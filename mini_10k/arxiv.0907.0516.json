{"id": "http://arxiv.org/abs/0907.0516v1", "guidislink": true, "updated": "2009-07-03T02:21:36Z", "updated_parsed": [2009, 7, 3, 2, 21, 36, 4, 184, 0], "published": "2009-07-03T02:21:36Z", "published_parsed": [2009, 7, 3, 2, 21, 36, 4, 184, 0], "title": "Adaptation and Self-Organization in Evolutionary Algorithms", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0907.2336%2C0907.4923%2C0907.1432%2C0907.5194%2C0907.3624%2C0907.1819%2C0907.4694%2C0907.0707%2C0907.0473%2C0907.3731%2C0907.4391%2C0907.0140%2C0907.4176%2C0907.4220%2C0907.1966%2C0907.3528%2C0907.4841%2C0907.2258%2C0907.0133%2C0907.5057%2C0907.1046%2C0907.3627%2C0907.0305%2C0907.4270%2C0907.1850%2C0907.4475%2C0907.3647%2C0907.3613%2C0907.5368%2C0907.5505%2C0907.4945%2C0907.3373%2C0907.1226%2C0907.1338%2C0907.0724%2C0907.3929%2C0907.3014%2C0907.4915%2C0907.0935%2C0907.0177%2C0907.3129%2C0907.4153%2C0907.2461%2C0907.2094%2C0907.4670%2C0907.3324%2C0907.3609%2C0907.4919%2C0907.3504%2C0907.1037%2C0907.0158%2C0907.2608%2C0907.0267%2C0907.1290%2C0907.4765%2C0907.0447%2C0907.4551%2C0907.3773%2C0907.2477%2C0907.0766%2C0907.0591%2C0907.1271%2C0907.2896%2C0907.1192%2C0907.0234%2C0907.4410%2C0907.3402%2C0907.3346%2C0907.1895%2C0907.1778%2C0907.3551%2C0907.3138%2C0907.4157%2C0907.4331%2C0907.2764%2C0907.1225%2C0907.2651%2C0907.5146%2C0907.3042%2C0907.3964%2C0907.4468%2C0907.3349%2C0907.1395%2C0907.1211%2C0907.1532%2C0907.1813%2C0907.1310%2C0907.3564%2C0907.4885%2C0907.5363%2C0907.2062%2C0907.4347%2C0907.1087%2C0907.2499%2C0907.3170%2C0907.4076%2C0907.3709%2C0907.4680%2C0907.0516%2C0907.4063%2C0907.2259&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adaptation and Self-Organization in Evolutionary Algorithms"}, "summary": "Abbreviated Abstract: The objective of Evolutionary Computation is to solve\npractical problems (e.g. optimization, data mining) by simulating the\nmechanisms of natural evolution. This thesis addresses several topics related\nto adaptation and self-organization in evolving systems with the overall aims\nof improving the performance of Evolutionary Algorithms (EA), understanding its\nrelation to natural evolution, and incorporating new mechanisms for mimicking\ncomplex biological systems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0907.2336%2C0907.4923%2C0907.1432%2C0907.5194%2C0907.3624%2C0907.1819%2C0907.4694%2C0907.0707%2C0907.0473%2C0907.3731%2C0907.4391%2C0907.0140%2C0907.4176%2C0907.4220%2C0907.1966%2C0907.3528%2C0907.4841%2C0907.2258%2C0907.0133%2C0907.5057%2C0907.1046%2C0907.3627%2C0907.0305%2C0907.4270%2C0907.1850%2C0907.4475%2C0907.3647%2C0907.3613%2C0907.5368%2C0907.5505%2C0907.4945%2C0907.3373%2C0907.1226%2C0907.1338%2C0907.0724%2C0907.3929%2C0907.3014%2C0907.4915%2C0907.0935%2C0907.0177%2C0907.3129%2C0907.4153%2C0907.2461%2C0907.2094%2C0907.4670%2C0907.3324%2C0907.3609%2C0907.4919%2C0907.3504%2C0907.1037%2C0907.0158%2C0907.2608%2C0907.0267%2C0907.1290%2C0907.4765%2C0907.0447%2C0907.4551%2C0907.3773%2C0907.2477%2C0907.0766%2C0907.0591%2C0907.1271%2C0907.2896%2C0907.1192%2C0907.0234%2C0907.4410%2C0907.3402%2C0907.3346%2C0907.1895%2C0907.1778%2C0907.3551%2C0907.3138%2C0907.4157%2C0907.4331%2C0907.2764%2C0907.1225%2C0907.2651%2C0907.5146%2C0907.3042%2C0907.3964%2C0907.4468%2C0907.3349%2C0907.1395%2C0907.1211%2C0907.1532%2C0907.1813%2C0907.1310%2C0907.3564%2C0907.4885%2C0907.5363%2C0907.2062%2C0907.4347%2C0907.1087%2C0907.2499%2C0907.3170%2C0907.4076%2C0907.3709%2C0907.4680%2C0907.0516%2C0907.4063%2C0907.2259&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Abbreviated Abstract: The objective of Evolutionary Computation is to solve\npractical problems (e.g. optimization, data mining) by simulating the\nmechanisms of natural evolution. This thesis addresses several topics related\nto adaptation and self-organization in evolving systems with the overall aims\nof improving the performance of Evolutionary Algorithms (EA), understanding its\nrelation to natural evolution, and incorporating new mechanisms for mimicking\ncomplex biological systems."}, "authors": ["James M Whitacre"], "author_detail": {"name": "James M Whitacre"}, "author": "James M Whitacre", "arxiv_comment": "PhD Thesis", "links": [{"href": "http://arxiv.org/abs/0907.0516v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0907.0516v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0907.0516v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0907.0516v1", "journal_reference": null, "doi": null, "fulltext": "School of Chemical Sciences and Engineering\nThe University of New South Wales\n\nAdaptation and Self-Organization in\nEvolutionary Algorithms\n\nby\nJames M. Whitacre\nsubmitted in partial fulfillment of\nthe requirements for the degree of\n\nDoctor of Philosophy\nJuly 2007\n\n\fACKNOWLEDGEMENTS\n\nThere are a number of very important people whom I would like to acknowledge. First, I\nwould like to sincerely thank my supervisor, Associate Professor Q. Tuan Pham for his\nguidance, support, and encouragement. His critical eye in the earlier drafts of this thesis\nhelped to greatly strengthen the final version.\nI would like to thank Dr Ruhul Sarker for the support and advice he has provided over the\nlast two years. I also want to thank him for taking an interest in my initial research ideas\nand for his willingness to take on a co-supervisory role in this research.\nI want to acknowledge a number of people who have provided fruitful discussions and\nadvice over the last few years including Hussein Abbass, Alan Blair, David Green, and Eric\nWhitacre. I also want to thank Hartmut Pohlheim for providing a number of the fitness\nlandscape graphics used in this thesis.\nSpecial thanks to my special gal Alice, especially for putting up with my sometimes long\nwork hours.\n\nFinally, I would like to thank my wonderful family for their support,\n\nencouragement and love (especially my mom and sister).\n\nii\n\n\fABSTRACT\nThe objective of Evolutionary Computation is to solve practical problems (e.g.\noptimization, data mining) by simulating the mechanisms of natural evolution. This thesis\naddresses several topics related to adaptation and self-organization in evolving systems\nwith the overall aims of improving the performance of Evolutionary Algorithms (EA),\nunderstanding its relation to natural evolution, and incorporating new mechanisms for\nmimicking complex biological systems.\nPart I of this thesis presents a new mechanism for allowing an EA to adapt its behavior in\nresponse to changes in the environment.\n\nUsing the new approach, adaptation of EA\n\nbehavior (i.e. control of EA design parameters) is driven by an analysis of population\ndynamics, as opposed to the more traditional use of fitness measurements. Comparisons\nwith a number of adaptive control methods from the literature indicate substantial\nimprovements in algorithm performance for a range of artificial and engineering design\nproblems.\nPart II of this thesis involves a more thorough analysis of EA behavior based on the\nmethods derived in Part I. In particular, several properties of EA population dynamics are\nmeasured and compared with observations of evolutionary dynamics in nature. The results\ndemonstrate that some large scale spatial and temporal features of EA dynamics are\nremarkably similar to their natural counterpart. Compatibility of EA with the Theory of\nSelf-Organized Criticality is also discussed.\nPart III proposes fundamentally new directions in EA research which are inspired by the\nconclusions drawn in Part II. These changes involve new mechanisms which allow selforganization of the EA to occur in ways which extend beyond its common convergence in\nparameter space. In particular, network models for EA populations are developed where\nthe network structure is dynamically coupled to EA population dynamics. Results indicate\nstrong improvements in algorithm performance compared to cellular Genetic Algorithms\nand non-distributed EA designs. Furthermore, topological analysis indicates that the\npopulation network can spontaneously evolve to display similar characteristics to the\ninteraction networks of complex biological systems.\niii\n\n\fTABLE OF CONTENTS\n\nACKNOWLEDGEMENTS......................................................................... II\nABSTRACT ................................................................................................ III\nTABLE OF CONTENTS............................................................................IV\nLIST OF FIGURES ..................................................................................... X\nLIST OF TABLES ................................................................................XXIV\nABBREVIATIONS ............................................................................XXVIII\nCHAPTER 1\n1.1\n\nINTRODUCTION ............................................................. 1\n\nAIMS AND OUTLINE ................................................................................................ 2\n\nCHAPTER 2\n\nGENERAL BACKGROUND OF EVOLUTIONARY\n\nALGORITHMS............................................................................................. 5\n2.1\n\nOPTIMIZATION FRAMEWORK ................................................................................... 5\n\n2.1.1\n\nReconciling Optimization Research in a World of \"No Free Lunches\" ........................9\n\n2.2\n\nJUSTIFICATION OF EA RESEARCH.......................................................................... 10\n\n2.3\n\nEVOLUTIONARY ALGORITHMS .............................................................................. 12\n\n2.3.1\n\nA Brief History .............................................................................................................12\n\n2.3.2\n\nGeneral Description.....................................................................................................13\n\n2.3.3\n\nSelection Methods ........................................................................................................15\n\n2.3.3.1\n\nProportional Selection......................................................................................................16\n\n2.3.3.2\n\nLinear Ranking Selection.................................................................................................16\n\n2.3.3.3\n\nExponential Ranking Selection........................................................................................17\n\n2.3.3.4\n\nTournament Selection ......................................................................................................17\n\n2.3.3.5\n\nTruncation Selection ........................................................................................................18\n\n2.3.3.6\n\nModified Tournament Selection ......................................................................................18\n\n2.3.3.7\n\nCharacterization and Comments ......................................................................................19\n\n2.3.4\n\nSearch Operators and Variation ..................................................................................19\n\n2.3.4.1\n\nCharacterization ...............................................................................................................20\n\niv\n\n\f2.3.4.2\n\nMultiple Operators...........................................................................................................22\n\n2.3.4.3\n\nSearch Operator Probabilities ..........................................................................................22\n\n2.3.4.4\n\nLocal Search and Expert Search Hybrids.........................................................................23\n\n2.3.5\n\nConstraint Handling.....................................................................................................23\n\n2.3.5.1\n\nRejection of infeasible Solutions .....................................................................................24\n\n2.3.5.2\n\nConstraint Handling by Repair ........................................................................................24\n\n2.3.5.3\n\nPenalty Functions.............................................................................................................24\n\n2.3.5.4\n\nStochastic Ranking ..........................................................................................................25\n\n2.3.6\n\nParameter Encoding ....................................................................................................26\n\n2.3.6.1\n\n2.3.7\n\nGene Expression Research...............................................................................................27\n\nInteraction Constraints in EA Populations ..................................................................28\n\n2.3.7.1\n\nCrowding and Niche Preserving Methods .......................................................................29\n\n2.3.7.2\n\nSpatially Distributed Populations.....................................................................................30\n\n2.3.7.3\n\nOther Restrictions ............................................................................................................32\n\n2.3.8\n\nPerformance Metrics and Analysis ..............................................................................33\n\n2.3.8.1\n\nTime Dependency ............................................................................................................33\n\n2.3.8.2\n\nDefining Performance......................................................................................................34\n\n2.3.9\n\nUses and Applications of EA........................................................................................35\n\n2.3.9.1\n\nWhen EA is used..............................................................................................................35\n\n2.3.9.2\n\nWhere EA is used ............................................................................................................37\n\nCHAPTER 3\n3.1\n\nADAPTATION OF EA DESIGN................................... 39\n\nAPPROACHES TO ADAPTATION: LITERATURE REVIEW ......................................... 40\n\n3.1.1\n\nImpetus for EA design adaptation research .................................................................40\n\n3.1.2\n\nAdjustable Parameters .................................................................................................41\n\n3.1.3\n\nEA Parameter Control Techniques ..............................................................................42\n\n3.1.3.1\n\nDeterministic Methods.....................................................................................................42\n\n3.1.3.2\n\nSelf-Adaptive Methods ....................................................................................................43\n\n3.1.3.3\n\nSupervisory Methods .......................................................................................................44\n\n3.1.4\n\nSupervisory Adaptation of search operator probabilities............................................44\n\n3.1.4.1\n\nOperator Quality ..............................................................................................................44\n\n3.1.4.2\n\nOperator Probability Setting ............................................................................................45\n\n3.1.4.3\n\nDefining Operator Rewards .............................................................................................46\n\n3.1.4.4\n\nEvent Measurement .........................................................................................................48\n\n3.1.4.5\n\nInterpretation....................................................................................................................49\n\n3.1.4.6\n\nOther Approaches ............................................................................................................51\n\n3.2\n3.2.1\n\nMEASURING POPULATION DYNAMICS FOR ADAPTIVE CONTROL .......................... 52\nWhy is Objective Function a Standard measure for fitness? .......................................53\n\nv\n\n\f3.2.1.1\n\nThe Hill-Climbing Assumption .......................................................................................53\n\n3.2.1.2\n\nSearch Bias Assumption ..................................................................................................53\n\n3.2.1.3\n\nEmpirical Bias..................................................................................................................54\n\n3.2.2\n\nMeasuring Impact on Population Dynamics: The Event Takeover Value (ETV) ........55\n\n3.2.2.1\n\nMultiple Parents and Genetic Dominance........................................................................57\n\n3.2.2.2\n\nHitchhiking ......................................................................................................................57\n\n3.2.2.3\n\nETV Calculation Procedure .............................................................................................59\n\n3.2.2.4\n\nComputational Costs of ETV Calculation........................................................................60\n\n3.2.2.5\n\nRelated Research..............................................................................................................61\n\n3.2.3\n\nETV Analysis ................................................................................................................62\n\n3.2.3.1\n\nFitness as a predictor of ETV...........................................................................................62\n\n3.2.3.2\n\nETV Distribution .............................................................................................................65\n\n3.2.4\n\nInterpreting ETV measurements ..................................................................................66\n\n3.2.4.1\n\nOutlier Calculation...........................................................................................................67\n\nEXPERIMENTS ....................................................................................................... 69\n\n3.3\n3.3.1\n\nExperimental Setup ......................................................................................................69\n\n3.3.1.1\n\nCore EA Design ...............................................................................................................70\n\n3.3.1.2\n\nSearch Operator Control ..................................................................................................72\n\n3.3.2\n\nResults and Discussion.................................................................................................75\n\n3.3.2.1\n\nGeneral Performance Statistics ........................................................................................75\n\n3.3.2.2\n\nOperator Probability Profile Analysis..............................................................................78\n\n3.3.2.3\n\nPerformance Results on Artificial Test Functions ...........................................................94\n\n3.3.2.4\n\nPerformance Results on Engineering Design Problems.................................................100\n\n3.3.3\n\nDiscussion and Conclusions ......................................................................................105\n\nCHAPTER 4\n\nLARGE SCALE FEATURES OF EA POPULATION\n\nDYNAMICS\n\n110\n\n4.1\n\nANALYSIS OF EA DYNAMICS USING ETV............................................................ 111\n\n4.1.1\n\nExperimental Setup ....................................................................................................111\n\n4.1.1.1\n\nPanmictic EA designs ....................................................................................................111\n\n4.1.1.2\n\nSpatially Distributed Populations...................................................................................112\n\n4.1.2\n\nETV Size Results.........................................................................................................114\n\n4.1.2.1\n\nImpact of EA design ......................................................................................................114\n\n4.1.2.2\n\nFitness Landscape Dependencies...................................................................................117\n\n4.1.2.3\n\nImpact of time length of evolution.................................................................................120\n\n4.1.2.4\n\nOther Experimental Conditions .....................................................................................123\n\n4.1.3\n\nETV Age Results.........................................................................................................123\n\nvi\n\n\f4.1.3.1\n\n4.1.4\n\n4.2\n\nCaveats...........................................................................................................................127\n\nConclusions ................................................................................................................127\n\nDISCUSSION: COMPARISONS BETWEEN EA AND NATURE .................................... 128\n\n4.2.1\n\nExtinction Sizes ..........................................................................................................129\n\n4.2.2\n\nSpecies Lifetime Distributions....................................................................................131\n\n4.2.3\n\nFractal Taxonomic Structures....................................................................................132\n\n4.2.4\n\nSummary of Conclusions............................................................................................134\n\n4.3\n\nSELF-ORGANIZED CRITICALITY .......................................................................... 135\n\n4.3.1\n\nSOC Definition ...........................................................................................................135\n\n4.3.2\n\nCompatibility of EA with SOC ...................................................................................136\n\n4.4\n\nRELEVANCE TO EA RESEARCH ............................................................................ 138\n\n4.4.1\n\nImpetus for SOTEA Chapter ......................................................................................139\n\nCHAPTER 5\n\nSELF-ORGANIZING TOPOLOGY EVOLUTIONARY\n\nALGORITHMS......................................................................................... 141\n5.1\n\nCRITICAL REVIEW OF PREVIOUS WORK .............................................................. 142\n\n5.1.1\n\nInteraction Constraints ..............................................................................................142\n\n5.1.1.1\n\n5.1.2\n\nPopulation Networks for Evolutionary Algorithms .......................................................142\n\nStructural Characteristics of Complex Networks.......................................................144\n\n5.1.2.1\n\nProperties of real networks ............................................................................................144\n\n5.1.2.2\n\nTopological Property Metrics ........................................................................................145\n\n5.1.3\n\nNetwork Evolution Models.........................................................................................148\n\n5.1.3.1\n\nThe BA Model: ..............................................................................................................148\n\n5.1.3.2\n\nThe Duplication and Divergence Model ........................................................................149\n\n5.1.3.3\n\nThe Fitness Model: ........................................................................................................151\n\n5.2\n\nMOTIVATIONS AND AIMS .................................................................................... 152\n\n5.3\n\nSOTEA MODEL I................................................................................................. 153\n\n5.3.1\n\nModel Description......................................................................................................154\n\n5.3.1.1\n\nSOTEA and cGA Network Dynamics ...........................................................................154\n\n5.3.1.2\n\nSOTEA and cGA State Dynamics .................................................................................157\n\n5.3.2\n\nExperimental Setup ....................................................................................................160\n\n5.3.2.1\n\nNK Landscape Test Function.........................................................................................160\n\n5.3.2.2\n\nCore EA Design .............................................................................................................162\n\n5.3.3\n\nResults ........................................................................................................................164\n\n5.3.3.1\n\nTopological Characteristics of Interaction Networks.....................................................164\n\n5.3.3.2\n\nGenetic Diversity ...........................................................................................................165\n\n5.3.3.3\n\nPerformance Results ......................................................................................................166\n\nvii\n\n\f5.3.3.4\n\nImpact of Ruggedness....................................................................................................167\n\n5.3.3.5\n\nImpact of Epistasis.........................................................................................................168\n\n5.3.3.6\n\nThe Impact of SOTEA model parameters......................................................................171\n\n5.3.4\n\nDiscussion ..................................................................................................................173\n\n5.3.4.1\n\nSOTEA Guidelines ........................................................................................................173\n\n5.3.4.2\n\nThe NK Model as an optimization research tool............................................................173\n\n5.3.5\n\n5.4\n\nConclusions ................................................................................................................174\n\nSOTEA MODEL II............................................................................................... 175\n\n5.4.1\n\nModel Description......................................................................................................175\n\n5.4.1.1\n\nDriving Forces ...............................................................................................................176\n\n5.4.1.2\n\nMechanics of Network Rewiring ...................................................................................178\n\n5.4.2\n\nExperimental Setup ....................................................................................................179\n\n5.4.2.1\n\nAlgorithm Designs.........................................................................................................179\n\n5.4.2.2\n\nEngineering Design Case Studies ..................................................................................183\n\n5.4.3\n\nResults ........................................................................................................................183\n\n5.4.3.1\n\nEngineering Design Performance Results......................................................................185\n\n5.4.3.2\n\nArtificial Test Function Results .....................................................................................192\n\n5.4.3.3\n\nStructural Analysis.........................................................................................................197\n\n5.4.4\n\nDiscussion ..................................................................................................................204\n\n5.4.5\n\nFuture Work ...............................................................................................................205\n\n5.4.6\n\nConclusions ................................................................................................................206\n\nCHAPTER 6\n\nSUMMARY OF FINDINGS......................................... 208\n\nREFERENCES.......................................................................................... 210\nAPPENDIX A TEST FUNCTION DEFINITIONS ............................ 228\nA.1\n\nARTIFICIAL TEST FUNCTIONS .............................................................................. 228\n\nA.1.1\n\nMinimum Tardy Task Problem (MTTP).....................................................................228\n\nA.1.2\n\nError Correcting Code Problem (ECC).....................................................................230\n\nA.1.3\n\nMassively Multimodal Deceptive Problem (MMDP).................................................231\n\nA.1.4\n\nFrequency Modulation ...............................................................................................231\n\nA.1.5\n\nQuadratic Function....................................................................................................231\n\nA.1.6\n\nGeneralized Rosenbrock's Function ..........................................................................232\n\nA.1.7\n\nRastrigin's Function...................................................................................................232\n\nA.1.8\n\nSchwefel's Function ...................................................................................................232\n\nA.1.9\n\nGriewangk's Function................................................................................................232\n\nA.1.10\n\nBohachevsky's Function.........................................................................................233\n\nviii\n\n\fA.1.11\n\nWatson's Function..................................................................................................233\n\nA.1.12\n\nColville's Function .................................................................................................233\n\nA.1.13\n\nSystem of linear equations .....................................................................................234\n\nA.1.14\n\nAckley's Function ...................................................................................................234\n\nA.1.15\n\nNeumaier's Function #2.........................................................................................234\n\nA.1.16\n\nHyper Ellipsoid ......................................................................................................235\n\nA.2\n\nENGINEERING DESIGN TEST PROBLEMS .............................................................. 235\n\nA.2.1\n\nTurbine Power Plant ..................................................................................................235\n\nA.2.2\n\nAlkylation Process......................................................................................................236\n\nA.2.3\n\nHeat Exchanger Network Design...............................................................................238\n\nA.2.4\n\nPressure Vessel ..........................................................................................................239\n\nA.2.5\n\nCoello's Welded Beam Design...................................................................................240\n\nA.2.6\n\nTension Compression Spring .....................................................................................242\n\nA.2.7\n\nGear Train Design .....................................................................................................243\n\nAPPENDIX B SEARCH OPERATORS .............................................. 244\nB.1\n\nSINGLE POINT RANDOM MUTATION.................................................................... 244\n\nB.2\n\nCREEP ................................................................................................................. 244\n\nB.3\n\nRAISE .................................................................................................................. 245\n\nB.4\n\nSWAP .................................................................................................................. 245\n\nB.5\n\nUNIFORM CROSSOVER......................................................................................... 245\n\nB.6\n\nSINGLE POINT CROSSOVER ................................................................................. 246\n\nB.7\n\nBLX- \u0391 CROSSOVER ........................................................................................... 246\n\nB.8\n\nWRIGHT'S HEURISTIC CROSSOVER ..................................................................... 246\n\nB.9\n\nEXTENDED LINE CROSSOVER .............................................................................. 247\n\nB.10\n\nDIFFERENTIAL EVOLUTION OPERATOR ............................................................... 247\n\nAPPENDIX C ADDITIONAL RESULTS FROM CHAPTER 5 ...... 248\nC.1\nENGINEERING DESIGN PROBLEMS: PERFORMANCE COMPARISONS WITH THE\nLITERATURE .................................................................................................................... 248\nC.2\n\nPANMICTIC EA PERFORMANCE TABLES .............................................................. 251\n\nPUBLICATIONS ...................................................................................... 252\n\nix\n\n\fLIST OF FIGURES\nFigure 2-1 Pseudocode for a basic Evolutionary Algorithm design .................................... 14\nFigure 2-2 Pseudocode for Stochastic Ranking procedure where U(0,1) is a uniform\nrandom number generator, \u03c6k and Fk are the total constraint violation and objective\nfunction value (resp.) for individual k, N is the number of sweeps through the\npopulation, \u03bb is the population size, and Pf is the probability that infeasible solutions\nwill be evaluated based on objective function values. The original description\nprovided in [64] recommends N > \u03bb and Pf = 0.45. ................................................. 26\nFigure 2-3 Pseudo code for Deterministic Crowding. Distance (either genotype or\nphenotype) is given as d(), fitness is given as f(), and N is the population size....... 30\nFigure 2-4 Pseudocode of a synchronous cGA ................................................................... 32\nFigure 3-1: Framework for a supervisory adaptive system. Here, an adaptive system\nreceives measurement data as a result of its interactions with the environment.\nThese measurements are then interpreted or assessed for relevance. Once\ninterpreted, the data is then allowed to drive internal changes to the system. The\nmechanics for internal change are not shown in the figure but would consist of\nmechanisms such as the Quality function and the Probability Matching Strategy. . 47\nFigure 3-2 Operator tree for an individual where only crossover (Cr) and mutation (Mu)\nevents occur. The root node in the tree, Cr, is the search operator that was used to\ngenerate the individual that is storing the operator tree. .......................................... 52\nFigure 3-3: Visualizing an individual's impact on population dynamics using genealogical\ngraphs. An individual's impact for a given generation (horizontal axis) is defined as\nthe number of paths leading from the measured node to the current generation. This\nis referred to as ETVgen and can be calculated for the \"Event Measured\" in the graph\nabove by counting the number of nodes on the dotted vertical line for a given\ngeneration. As the population moves from one generation to the next, one can see\nthat the number of individuals in the population that are descendants of the \"Event\nMeasured\" will change with each new generation. In other words, the ETVgen value\nis dynamic. To clarify this point, ETVgen values are calculated for the \"Event\nMeasured\" and are shown at the top of the graph. The maximum impact an event\nhas on the population is the maximum ETVgen value that is observed. This graphical\nillustration assumes a generational population updating strategy such that an\nindividual exists in a single generation only. This is done to simplify the illustration\nhowever other updating strategies could be used in which case some nodes would\nbe stretched across multiple generations in the graph.............................................. 56\nFigure 3-4: Genetic Hitchhiking in EA population dynamics. Considering ETVgen\nmeasurements based on the current generation, one can easily see that all nodes to\nx\n\n\fthe left of the white node will have the same ETVgen value (i.e. they all have the\nsame number of paths leading to the current population). However, these nodes are\nassigned their ETVgen values only because of a single important descendant (the\nwhite node). These linear structures in the genealogical branching process are a\nsign of genetic hitchhiking and can be seen in several different places in the graph\nabove (seven genetic hitchhiking occurrences in total). .......................................... 58\nFigure 3-5: Transfer of Historical Data. Each individual holds historical information in\naddition to genetic information. The historical information represents the direct line\nof ancestry for an individual. Examples of historical data lists are shown above for\nParent 1 (ID=P21) and Parent 2 (ID=P23) and their meaning is demonstrated by the\ngenealogical graph on the right. A new offspring only takes historical information\nfrom the parent that is genetically most similar (i.e. genetically dominant). In this\nexample, Parent 1 is assumed to be the genetically dominant parent. In addition, the\noffspring creates a new ID to indicate its placement in the genealogical tree. ........ 59\nFigure 3-6: Box plots of the size of an individual's impact on population dynamics (ETV)\nas a function of the individual's rank within the population where a rank of 1\nrepresents the worst individual and a rank of N represents the best individual (based\non objective function value). The data set was generated from a series of\nexperiments involving a number of test functions listed in Appendix A. The EA\nused to generate the results was a real-coded, pseudo steady state EA design using\nbinary tournament selection (without replacement) and a population size of N = 50.\nResults shown are a random sample of 5000 data points taken from a data set of\n300,000. The box plots have the standard meaning with the bottom line in the box\nrepresenting the first quartile, the middle line representing the median, and the\nupper line representing the third quartile. The symbol \u2733 is used to represent outlier\ndata points. ............................................................................................................... 63\nFigure 3-7 Box plots of the size of an individual's impact on population dynamics (ETV) as\na function of the individual's ranking within the population. Top: Results from\nrunning an EA on the Massively Multimodal Deceptive Problem (MMDP).\nBottom: Results from running an EA on the Quadratic Test Function. Both test\nfunctions are defined in Appendix A. The EA used to generate the results was a\nreal-coded, pseudo steady state EA design using binary tournament selection\n(without replacement) and a population size of N = 50. Results shown for each\ngraph are a random sample of 5000 data points taken from a data set of\napproximately 15,000. The box plots have the standard meaning with the bottom\nline in the box representing the first quartile, the middle line representing the\nmedian, and the upper line representing the third quartile. The symbol \u2733 is used to\nrepresent outlier data points. .................................................................................... 64\nFigure 3-8 ETV probability distribution from running an EA for 20,000 generations on the\n30-D Hyper Ellipsoid test function. The EA design has a population size N=200,\nsteady state population updating, and uses truncation selection. The solid line\nrepresents a power law with exponent 2.2. .............................................................. 65\n\nxi\n\n\fFigure 3-9: p\u03b1 calculation curves for sample sizes Mi=5 (- - -), Mi =10 (- -), and Mi =20\n(--). ...................................................................................................................... 69\nFigure 3-10 Pseudocode of EA design................................................................................ 71\nFigure 3-11 Pseudocode for standard search operator probability adaptation.................... 73\nFigure 3-12 Pseudocode for search operator probability adaptation using ETV. ETV is\ndefined in Section 3.2.2 and IOutlier is defined in Section 3.2.4. ............................... 74\nFigure 3-13 General algorithm performance for both adaptive and non-adaptive EA\ndesigns shown as a function of the number of generations (Gen) of evolution. In\norder to aggregate performance data from different test functions, it was necessary\nto deal with differences in fitness scaling. This was addressed by using the\nfollowing ranking procedure. Each algorithm is run 20 times on each test function\nlisted in Table 3-2. At a given generation, every EA run is ranked among all runs\nconducted on that test function (with a higher ranking being better). The median\nrank of each EA design is then calculated for each test function. Finally, these\nmedian ranks are averaged over all test functions and plotted against the number of\ngenerations (Gen). .................................................................................................... 77\nFigure 3-14 Rosenbrock fitness landscape shown in a two dimensional parameter space.\nThe two bottom graphs are shown for variable 1 and variable 2 varying over the\nentire parameter range [-2,2]. Graphs on the top focus on the parameter region\ncontaining the global optimum. The two graphs on the left show a restricted range\nof objective function values (vertical axis) to help in visualizing the fitness\nlandscape. Images were kindly provided by Hartmut Pohlheim and were generated\nusing the GEATbx toolbox in Matlab\u00ae [168]. Low resolution images can also be\nfound at http://www.geatbx.com/docu/fcnindex-01.html#P85_2637...................... 79\nFigure 3-15 Performance of adaptive and non-adaptive EA designs on the Rosenbrock test\nfunction. The global optimal solution is F=0. The optimal F value can not be\nshown due to log scaling on the F axis so performance profiles are seen to terminate\nwhen the global optima is reached. .......................................................................... 80\nFigure 3-16 Search operator probability profiles for adaptive methods I(median)-Pursuit,\nI(median), I(parent)-Pursuit, and I(parent) on the Rosenbrock test function.\nProbability values are shown on a logarithmic scale over the first 2000 generations\nof evolution. ............................................................................................................. 81\nFigure 3-17 Search operator probability profiles for adaptive methods I(rank)-Pursuit,\nI(rank), ETV-Outlier, and ETV on the Rosenbrock test function. Probability values\nare shown on a logarithmic scale over the first 2000 generations of evolution....... 82\nFigure 3-18 Schwefel fitness landscape shown in two dimensions of parameter space. The\nlandscape is shown for variable 1 and variable 2 varying over different parameter\nranges. The entire range is shown in the bottom graph with each parameter varying\nover [-500,500].\nThe vertical axis shows the objective function value\n(minimization) with the global optimal solution located at the origin of parameter\nspace. Images were kindly provided by Hartmut Pohlheim and were generated\nxii\n\n\fusing the GEATbx toolbox in Matlab\u00ae [168]. Low resolution images can also be\nfound at http://www.geatbx.com/docu/fcnindex-01.html#P85_2637...................... 83\nFigure 3-19 Performance of adaptive and non-adaptive EA designs on the Schwefel test\nfunction. The global optimal solution is at F=0....................................................... 84\nFigure 3-20 Search operator probability profiles for adaptive methods I(median)-Pursuit,\nI(median), I(parent)-Pursuit,\nand I(parent) on the Schwefel test function.\nProbability values are shown on a logarithmic scale over the first 2000 generations\nof evolution. ............................................................................................................. 85\nFigure 3-21 Search operator probability profiles for adaptive methods I(rank)-Pursuit,\nI(rank), ETV-Outlier, and ETV on the Schwefel test function. Probability values\nare shown on a logarithmic scale over the first 2000 generations of evolution....... 86\nFigure 3-22 Griewangk fitness landscape shown in two dimensions of parameter space. a)\nThe landscape is shown for variable 1 and variable 2 varying over their complete\nrange [-500,500]. b) The landscape is shown for variable 1 and variable 2 varying\nover the range [-50,50]. c) The landscape is shown for variable 1 and variable 2\nvarying over the range [-8,8]. The vertical axis shows the objective function value\n(minimization) with the global optimal solution located at the origin of parameter\nspace. Images were kindly provided by Hartmut Pohlheim and were generated\nusing the GEATbx toolbox in Matlab\u00ae [168]. Low resolution images can also be\nfound at http://www.geatbx.com/docu/fcnindex-01.html#P85_2637...................... 87\nFigure 3-23 Performance of adaptive and non-adaptive EA designs on the Griewangk test\nfunction. The global optimal solution is at F=0...................................................... 88\nFigure 3-24 Search operator probability profiles for adaptive methods I(median)-Pursuit,\nI(median), I(parent)-Pursuit, and I(parent) on the Griewangk test function.\nProbability values are shown on a logarithmic scale over the first 2000 generations\nof evolution. ............................................................................................................. 89\nFigure 3-25 Search operator probability profiles for adaptive methods I(rank)-Pursuit,\nI(rank), ETV-Outlier, and ETV on the Griewangk test function. Probability values\nare shown on a logarithmic scale over the first 2000 generations of evolution....... 90\nFigure 3-26 Ackley's fitness landscape shown in two dimensions of parameter space. a)\nThe landscape is shown for variable 1 and variable 2 varying over their complete\nrange [-30,30]. b) The landscape is shown for variable 1 and variable 2 varying\nover the range [-2,2]. The vertical axis shows the objective function value\n(minimization) with the global optimal solution located at the origin of parameter\nspace. Images were kindly provided by Hartmut Pohlheim and were generated\nusing the GEATbx toolbox in Matlab\u00ae [168]. Low resolution images can also be\nfound at http://www.geatbx.com/docu/fcnindex-01.html#P85_2637...................... 91\nFigure 3-27 Performance of adaptive and non-adaptive EA designs on Ackley's Path\nFunction. The global optimal solution is at F=0. The optimal F value can not be\nshown due to log scaling on the F axis so performance profiles are seen to terminate\nwhen the global optima is reached. .......................................................................... 92\nxiii\n\n\fFigure 3-28 Search operator probability profiles for adaptive methods I(median)-Pursuit,\nI(median), I(parent)-Pursuit, and I(parent) on Ackley's test function. Probability\nvalues are shown on a logarithmic scale over the first 2000 generations of evolution.\n.................................................................................................................................. 93\nFigure 3-29 Search operator probability profiles for adaptive methods I(rank)-Pursuit,\nI(rank), ETV-Outlier, and ETV on Ackley's test function. Probability values are\nshown on a logarithmic scale over the first 2000 generations of evolution............. 94\nFigure 3-30 Performance of adaptive and non-adaptive EA designs on the System of\nLinear Equations test function. The global optimal solution is at F=0................... 96\nFigure 3-31 Performance of adaptive and non-adaptive EA designs on the Quadratic test\nfunction. The global optimal solution is at F=0...................................................... 96\nFigure 3-32 Performance of adaptive and non-adaptive EA designs on Watson's test\nfunction. The global optimal solution is at F=2.288E-3. ......................................... 97\nFigure 3-33 Performance of adaptive and non-adaptive EA designs on Neumaier's function\n#2. The global optimal solution is unknown (see Appendix A). ............................ 97\nFigure 3-34 Performance of adaptive and non-adaptive EA designs on Colville's test\nfunction. The global optimal solution is at F=0...................................................... 98\nFigure 3-35 Performance of adaptive and non-adaptive EA designs on Bohachevsky's test\nfunction. The global optimal solution is at F=0...................................................... 98\nFigure 3-36 Performance of adaptive and non-adaptive EA designs on the Rastrigin test\nfunction. The global optimal solution is at F=0...................................................... 99\nFigure 3-37 Performance of adaptive and non-adaptive EA designs on the 30-D Hyper\nEllipsoid test function. The global optimal solution is at F=0................................ 99\nFigure 3-38 Performance of adaptive and non-adaptive EA designs on the Massively\nMultimodal Deceptive Problem (MMDP). The global optimal solution is at F=0.\n................................................................................................................................ 100\nFigure 3-39 Performance of adaptive and non-adaptive EA designs on the Turbine Power\nPlant Problem. The global optimal solution is at F=3.05. .................................... 102\nFigure 3-40 Performance of adaptive and non-adaptive EA designs on the Welded Beam\nDesign problem. The global optimal solution is unknown. .................................. 102\nFigure 3-41 Performance of adaptive and non-adaptive EA designs on the Tension\nCompression Spring problem. The global optimal solution is unknown.............. 103\nFigure 3-42 Performance of adaptive and non-adaptive EA designs on the Gear Train\nDesign problem. The global optimal solution is F=2.70 x10-12............................ 103\n\nxiv\n\n\fFigure 3-43 Performance of adaptive and non-adaptive EA designs on the Minimum Tardy\nTask Problem (MTTP). The global optimal solution is F=0. ............................... 104\nFigure 3-44 Performance of adaptive and non-adaptive EA designs on the Frequency\nModulation problem. The global optimal solution is F=0. ................................... 104\nFigure 3-45 Performance of adaptive and non-adaptive EA designs on the Error Correcting\nCode (ECC) problem. The global optimal solution is F=0................................... 105\nFigure 4-1 ETV size distributions for a number of panmictic EA designs. a) EA designs\nwith population size N=200, generational population updating (Gen), and selection\nmethods Tournament (Tour), Truncation (Trun) and Random (Rand) selection.\nSolid line represents a power law with exponent 2.5. b) EA designs with population\nsize N=200, steady state (SS) population updating, and selection methods\nTournament (Tour), Truncation (Trun) and Random (Rand) selection. Solid line\nrepresents a power law with exponent 2.3. Results from each EA design are taken\nover 20,000 generations of evolution on the 30-D Hyper Ellipsoid test function. 115\nFigure 4-2 ETV size distributions for a number of spatially distributed EA designs. a)\nCellular Genetic Algorithm (cGA) designs with population sizes (N=100, N=200),\nand neighborhood radius (R=1, R=5, R=30). Solid line represents a power law with\nexponent 2.2. b) Cellular Genetic Algorithm (cGA) designs with random selection\n(Rand), population size (N=200), and neighborhood radius (R=1, R=5, R=30).\nSolid line represents a power law with exponent 2.5. Results from each EA design\nare taken over 20,000 generations of evolution on the 30-D Hyper Ellipsoid test\nfunction. ................................................................................................................. 116\nFigure 4-3 ETV distributions shown primarily for spatially distributed EA designs. All EA\ndesigns have population size N=200. Cellular Genetic Algorithm (cGA) designs\nvary in the use of crowding and the neighborhood radius size (R=1, R=5, R=30).\nResults from using Deterministic Crowding (DC) are also presented in the inset.\nSolid line represents a power law with exponent 2.2. Results from each EA design\nare taken over 20,000 generations of evolution on the 30-D Hyper Ellipsoid test\nfunction. ................................................................................................................. 117\nFigure 4-4 ETV distributions shown for selected EA designs on a range of test functions\ntaken from Appendix A. Evolution occurred over 2000 generations and results\nshown are averages taken over 10 runs. To help in viewing results from a large\nnumber of test functions, data is grouped into bins. a) Results for an EA design\nusing steady state (SS) population updating, truncation selection (Trun), and\npopulation size N=200. Solid line represents a power law with exponent 2.2. b)\nResults for an EA design using generational (Gen) population updating, tournament\nselection (Tour), and population size N=200. Solid line represents a power law with\nexponent 2.2. .......................................................................................................... 119\nFigure 4-5 ETV distributions shown for selected EA designs on a range of test functions\ntaken from Appendix A. Evolution occurred over 2000 generations and results\nshown are averages taken over 10 runs. To help in viewing results from a large\nnumber of test functions, data is grouped into bins. a) Results for a distributed EA\nxv\n\n\fdesign (cGA) using neighborhood radius R=1, and population size N=200. Solid\nline represents a power law with exponent 2.2. b) Results for a distributed EA\ndesign (cGA) using neighborhood radius R=30, and population size N=200. Solid\nline represents a power law with exponent 2.2. ..................................................... 120\nFigure 4-6 ETV distribution results as a function of the time span of evolution. a) Results\nfor an EA design using steady state (SS) population updating, truncation selection\n(Trun), and population size N=200. Solid line represents a power law with\nexponent 2.2. b) Results for a distributed EA design (cGA) using neighborhood\nradius R=1, and population size N=200. Solid line represents a power law with\nexponent 2.5. Data sets are labeled by a number which indicates the number of\nETV measurements that are used to generate the distribution. For each EA run, the\nfirst 100 events are given to the first data set, the next 500 are given to the next data\nset and so on. Results for each EA design are averages over ten runs.................. 121\nFigure 4-7 Record ETV statistics for cellular Genetic Algorithms (cGA) with population\nsize N=100 and neighborhood radius (R=1, R=5, R=30). ETV(Max) is the largest\nETV found in every 200 events. Values are averages over 10 experimental\nreplicates. ............................................................................................................... 122\nFigure 4-8 ETV distributions with varying amounts of historical uncoupling in EA\npopulation dynamics. Experiments are conducted with a steady state EA using\ntruncation selection and population size N=100. Evolution took place over 20,000\ngenerations on the 30-D Hyper Ellipsoid test function. When conducting the\nstandard ETV calculation, historical event information is copied from the\ngenetically dominant parent to its offspring. In these experiments, the step of\nhistorical transfer is skipped with probability Pnew. The solid line in the graph\nrepresents a power law with exponent = 2.1.......................................................... 123\nFigure 4-9 ETV age distributions shown primarily for spatially distributed EA designs.\nThe age of an ETV is defined by the number of generations from the initial event to\nthe completion of the ETV calculation. All EA designs have population size\nN=200. Cellular Genetic Algorithm (cGA) designs vary in the use of crowding and\nthe neighborhood radius size (R=1, R=5, R=30). Results from using Deterministic\nCrowding (DC) are also provided for a population size of N=200. Solid line\nrepresents a power law with exponent 3.2. ............................................................ 125\nFigure 4-10 ETV age distributions for several EA designs. The age of an ETV is defined\nby the number of generations from the initial event to the completion of the ETV\ncalculation. a) EA designs with population sizes (N=200, N=50), generational\npopulation updating (Gen) and Tournament selection (Tour). Solid line represents a\npower law with exponent 3. b) EA designs with population sizes (N=200, N=50),\nsteady state population updating (SS), and Tournament selection (Tour). Solid line\nrepresents a power law with exponent 2.5. c) EA designs with population sizes\n(N=200, N=50), steady state population updating (SS), and Truncation selection\n(Trun). Solid line represents a power law with exponent 3.5. .............................. 127\nFigure 4-11 Probability of an extinction event as a function of the fraction of all species\nkilled. The distribution is derived based on a best of fit kill curve (see [170]) using\nxvi\n\n\ffossil data of marine species from the Paleozoic era. Reprinted by permission from\nthe Royal Society (Proceedings: Biological Sciences) [171], copyright (1996).... 130\nFigure 4-12 Local lifetime distributions for species based on North American bird\npopulations. a) Lifetime distributions for data taken over different timescales.\nPower law deviations are clearly present. b) Lifetime distributions with rescaling of\ndata to account for finite size effects. Data is now well approximated by a power\nlaw. Reprinted by permission from Macmillan Publishers Ltd: (NATURE) [173],\ncopyright (1998)..................................................................................................... 132\nFigure 4-13 log-log plots of the frequency of a selected taxon with different numbers of\nsub-taxa. a) Frequency of genera with different numbers of species for birds. The\nfrequency is given on the vertical axis and the number of bird species within the\ngenera is given on the horizontal axis. b) Frequency of orders with different\nnumbers of families for animals. The frequency is given on the vertical axis and the\nnumber of animal families within the order is given on the horizontal axis. Data\npoints with frequencies f=1 are omitted. Similar distributions for other data sets are\npresented in [179]. Reprinted by permission from Elsevier: (J. theor. Biol.) [179],\ncopyright (1990)..................................................................................................... 133\nFigure 5-1: Examples of interaction networks. The networks on the top represent\ncommonly used EA population structures and are known as (from left to right)\nPanmictic, island model, and cellular population structures. Networks at the bottom\nhave been developed with one or more characteristics of complex biological\nnetworks and are classified as (from left to right) Self-Organizing Networks\n(presented here), Hierarchical Networks [194], and Small World Networks [195].\nFigure 1e is reprinted with permission from AAAS. ............................................. 144\nFigure 5-2: Reproduction rules that change the population structure for SOTEA and the\ncellular GA. a) SOTEA Reproduction: When an offspring is created (by asexual\nreproduction), a new node (shown in black) is added to the network through a\nconnection to its parent (shown in gray). Each of the parent's connections are then\ninherited by the offspring (black dotted line) with probability Padd followed by each\nof the inherited connections being lost by the parent (gray dotted line) with\nprobability Premove. Unless stated otherwise, the parameters are set as Padd = Premove\n= 10%. This particular rule is loosely based on established models for genome\ncomplexification [203]. b) cellular GA Reproduction: When an offspring is\ncreated, a new node (shown in black) is added to the network and connected to its\nparent (shown in gray). One of the parent's connections is then transferred to the\noffspring, which allows the network to maintain a ring topology. ........................ 155\nFigure 5-3: Competition rules that change the population structure for SOTEA and the\ncellular GA. The details of the competition rule are the same for SOTEA and the\ncGA, however examples are given for both EA designs in this figure. Competition\nrule: The first step is to select an individual at random. This individual then\ndecides to compete for survival with its least fit neighbor. When these two\nindividuals compete for survival such as the nodes shown in black and gray, the less\nfit individual is killed. The winning individual (shown in black) inherits all\nconnections from the losing individual (shown in gray) that weren't already in the\nxvii\n\n\fwinning individual's neighborhood. Finally, the losing individual is removed from\nthe network............................................................................................................. 156\nFigure 5-4: This figure shows how structural changes from SOTEA's competition rule\ndepend on the fitness of individuals in the network. Starting with the network at the\ntop, the individual represented by the black node must decide which of its neighbors\nit will try to kill. The networks at the bottom show what would happen if neighbor\n1, 2, or 3 had been the least fit in the black node's neighborhood. Each of the\nchoices creates a new structure that is different from the other choices. Notice that\nfor the networks on the bottom, the black node has been changed to gray. This is to\nindicate that either the black node or the white neighbor could have won the\ncompetition (the structure is the same in either case). ........................................... 157\nFigure 5-5: This figure shows how the epistatic fitness (Fitepi) defined by (5-9) causes the\nfitness of an individual to depend on its local neighborhood. Parts a-c of the figure\nshow a population of five individuals defined on a network. The Objective\nFunction Value (Obj) and epistatic fitness defined by (5-9) are provided in the top\nand bottom (resp.) of each node (i.e. individual). For the top two individuals in part\na), an arrow is drawn towards the individual on the left to indicate it has the lower\nepistatic fitness. The top left individual's epistatic fitness is 2/3 because its\nobjective function value is better than 2 of its 3 neighbors. In part b), a new\nconnection has been added to the network causing the epistatic fitness values for the\ntwo top individuals to now be equal. Finally in part c), a connection has been\nremoved from the network, causing the top left individual to have an epistatic\nfitness that is now higher than the top right node. If the top two nodes were to\ncompete for survival based on epistatic fitness, it should now be clear that the\ndecision of who survives (i.e. who is more fit) will depend on the neighborhoods of\nthe individuals. ....................................................................................................... 159\nFigure 5-6: Pseudocode for SOTEA and cellular GA network dynamics. ....................... 160\nFigure 5-7: An example of the fitness lookup tables for determining the fitness contribution\nfi from bit xi. Given an NK landscape with NNK =8 and KNK =2, f3(x3, z1(3), z2(3)) is\nthe fitness contribution for x3. z1(3) and z2(3) are the two bits that epistatically\ninteract with x3. As shown in the figure, they have been selected as the bits to the\nleft and right of x3 (i.e. z1(3) = x2 and z2(3) = x4). The lookup table consists of 2^(\nKNK +1) entries, each associated with a unique combination of bit states for x3, z1(3)\nand z2(3). Each entry in the lookup table is a number between [0,1] drawn from a\nuniform distribution. .............................................................................................. 161\nFigure 5-8: Genetic Diversity Results are shown over 4000 generations for Panmictic GA,\nSOTEA, and cellular GA. Diversity for each EA is an average over 10 runs with\ndiversity calculated from (5-12) using the entire population (top graph) or the 20%\nbest individuals in the population (bottom graph). Experiments are conducted on\nNK models with NNK =30, KNK =14. For each EA design the population size is set\nto N=100 and epistatic fitness is used as defined by (5-9)..................................... 166\nFigure 5-9: Performance results are shown over 5000 generations for Panmictic GA,\nSOTEA, and cellular GA each operating with Epistatic Fitness. Performance for\nxviii\n\n\feach EA is an average over 10 runs with performance calculated as the best\nobjective function value in a run. Experiments are conducted on NK models with\nNNK =30, KNK =14. For each EA design the population size is set to N=100 and\nepistatic fitness is used as defined by (5-9)............................................................ 167\nFigure 5-10: Genetic diversity results are shown for different amounts of landscape\nruggedness for the Panmictic GA, SOTEA, and the cellular GA. Diversity is an\naverage of calculations using (5-12) that are taken at every 20 generations (up to\n1000 generations) from the 20% best individuals in the population. This measure\nthen also averaged over 5 runs. Experiments are conducted on NK models with\nNNK =30, and KNK varying as shown in graph. Increasing KNK indicates increasing\nlevels of landscape ruggedness. For each EA design, the population size is set to\nN=100 and epistatic fitness is used as defined by (5-9)......................................... 168\nFigure 5-11: Genetic diversity results are shown over 4000 generations for Panmictic GA,\nSOTEA, and cellular GA each operating without epistatic fitness. Diversity for\neach EA is an average over 10 runs with diversity calculated from (5-12) using the\nentire population (top graph) or the 20% best individuals in the population (bottom\ngraph). Experiments are conducted on NK models with NNK =30, KNK =14. For\neach EA design, the population size is set to N=100 and fitness is defined as the\nObjective Function Value. The results shown here for the Panmictic GA are\nidentical to results shown in Figure 5-8. This is because the fitness rankings of\nindividuals in a fully connected population are the same regardless of whether\nepistatic fitness (5-9) is used or the Objective Function Value is used. Because the\nfitness rankings are the same, the outcome of competitions will also be the same\n(hence no change to EA behavior). ........................................................................ 169\nFigure 5-12: Selective pressure patterns in the SOTEA network with (top) and without\n(bottom) epistasis. Selective pressure in the network is shown with arrows in black\nfor pressure directed away from the network center and green for other directions of\npressure. Selective pressure directions have only been calculated for nodes located\nnear the network center. The arrows are drawn by selecting a node and drawing an\narrow from this node to its worst neighbor. The worst fit neighbor is determined by\nepistatic fitness (5-9) for the top graph and by the Objective Function Value for the\nbottom graph. ......................................................................................................... 171\nFigure 5-13 SOTEA networks evolved using different parameter settings for the\nreproduction rule. In the reproduction rule, a parent's connections are inherited by\nits offspring with probability Padd followed by each of the inherited connections\nbeing lost by the parent with probability Premove. Population interaction networks\nwere evolved for a) Padd = Premove = 0.0%, b) Padd = Premove = 10%, c) Padd = Premove =\n20%. ....................................................................................................................... 172\nFigure 5-14 Adaptive Network Rules: A selected node N1 will attempt to add, remove or\ntransfer its connections based on the satisfaction of constraints and the improvement\nof properties. Add Rule: The dotted line represents a feasible new connection in\nthe network assuming nodes N1 and N3 both would like to increase their number of\nconnections. Remove Rule: The gray dotted line represents a feasible connection\nto remove in the network assuming nodes N1 and N2 both have an excess of\nxix\n\n\fconnections. Transfer Rule: The connection between N1 and N2 (gray dotted line)\nbeing transferred to now connect N1 and N3 (black dotted line) represents a feasible\ntransfer assuming this action results in an overall improvement to local clustering.\n................................................................................................................................ 179\nFigure 5-15 Performance results for the Pressure Vessel design problem are shown over\n3000 generations for SOTEA with different settings of Kmax, and for cellular GA\nwith different values of the neighborhood radius R. Performance for each EA is an\naverage over 20 runs of the best fitness (objective function) value in the population.\nInfeasible solutions are neglected from the calculations, however all runs obtained\nfeasibility within the first 100 generations. The global optimal solution has a fitness\nof 5850.38. ............................................................................................................. 185\nFigure 5-16 Performance results for the Alkylation Process design problem are shown over\n3000 generations for SOTEA with different settings of Kmax, and for cellular GA\nwith different values of the neighborhood radius R. Performance for each EA is an\naverage over 20 runs of the best fitness (objective function) value in the population.\nInfeasible solutions are neglected from the calculations, however all runs obtained\nfeasibility within the first 1400 generations. Several instances can be observed\nwhere fitness values momentarily decrease. This is the result of EA runs turning\nfrom infeasible to feasible where the new feasible solution is lower than the average\nperformance for that EA design and generation. The global optimal solution has a\nfitness of 1772.77. .................................................................................................. 185\nFigure 5-17 Performance results for the Heat Exchanger Network design problem are\nshown over 3000 generations for SOTEA with different settings of Kmax, and for\ncellular GA with different values of the neighborhood radius R. Performance for\neach EA is an average over 20 runs of the best fitness (objective function) value in\nthe population. Infeasible solutions are neglected from the calculations, however all\nruns obtained feasibility within the first 100 generations. The global optimal\nsolution has a fitness of 7049.25. ........................................................................... 186\nFigure 5-18 Performance results for the Gear Train Design design problem are shown over\n3000 generations for SOTEA with different settings of Kmax, and for cellular GA\nwith different values of the neighborhood radius R. Performance for each EA is an\naverage over 20 runs of the best fitness (objective function) value in the population.\nInfeasible solutions are neglected from the calculations, however all runs obtained\nfeasibility within the first 50 generations. The global optimal solution is unknown,\nhowever the best result previous to this work, is reported in [220] as 2.70E-12. .. 186\nFigure 5-19 Performance results for the Tension Compression Spring Design design\nproblem are shown over 3000 generations for SOTEA with different settings of\nKmax, and for cellular GA with different values of the neighborhood radius R.\nPerformance for each EA is an average over 20 runs of the best fitness (objective\nfunction) value in the population. Infeasible solutions are neglected from the\ncalculations, however all runs obtained feasibility within the first 50 generations.\nThe global optimal solution is unknown, however the best result previous to this\nwork, is reported in [221] as 0.01270. ................................................................... 187\nxx\n\n\fFigure 5-20 Performance results for the Welded Beam Design design problem are shown\nover 3000 generations for SOTEA with different settings of Kmax, and for cellular\nGA with different values of the neighborhood radius R. Performance for each EA is\nan average over 20 runs of the best fitness (objective function) value in the\npopulation. Infeasible solutions are neglected from the calculations, however all\nruns obtained feasibility within the first 50 generations. The global optimal solution\nis unknown, however the best result previous to this work, is reported in [222] as\n1.7255..................................................................................................................... 187\nFigure 5-21 Final performance results for the Pressure Vessel (Left), Alkylation Process\n(Middle) and Heat Exchanger Network (Right) design problems are shown with box\nplots of performance data grouped by Panmictic EA, cellular GA, and SOTEA. The\nbox plots represent final algorithm performance (after 3000 generations) over 20\nruns for all cGA, SOTEA, and Panmictic EA designs. This includes data from the\nfour cGA designs (with different parameter settings for neighborhood radius R), the\nfour SOTEA designs (with different parameter settings for KMax), and the eight\nPanmictic EA designs described in Section 5.4.2.1. Insets are provided for the cGA\nand SOTEA box plots to highlight the difference in results between these two\nalgorithms. Also notice that the Pressure Vessel and Heat Exchanger Network\nproblems are Minimization problems while the Alkylation Problem is a\nMaximization problem. .......................................................................................... 188\nFigure 5-22 Final performance results for the Gear Train (Left), Tension Compression\nSpring (Middle) and Welded Beam (Right) design problems are shown with box\nplots of performance data grouped by Panmictic EA, cellular GA, and SOTEA. The\nbox plots represent final algorithm performance (after 3000 generations) over 20\nruns for all cGA, SOTEA, and Panmictic EA designs. This includes data from the\nfour cGA designs (with different parameter settings for neighborhood radius R), the\nfour SOTEA designs (with different parameter settings for KMax), and the eight\nPanmictic EA designs described in Section 5.4.2.1. When necessary, insets are\nprovided for the cGA and SOTEA box plots (with data shifted and plotted on a log\nscale) to highlight the difference in results between these two algorithms. All three\ndesign problems are Minimization problems......................................................... 190\nFigure 5-23 Performance results for the Frequency Modulation problem are shown over\n3000 generations for SOTEA with different settings of Kmax, and for cellular GA\nwith different values of the neighborhood radius R. Performance for each EA is an\naverage over 20 runs of the best fitness (objective function) value in the population.\nThe global optimal solution is 0............................................................................. 192\nFigure 5-24 Performance results for the error correcting code (ECC) problem are shown\nover 3000 generations for SOTEA with different settings of Kmax, and for cellular\nGA with different values of the neighborhood radius R. Performance for each EA is\nan average over 20 runs of the best fitness (objective function) value in the\npopulation. The global optimal solution is 0.067416. Results are shifted so that\nglobal optima is 0. .................................................................................................. 192\nFigure 5-25 Performance results for the system of linear equations problem are shown over\n3000 generations for SOTEA with different settings of Kmax, and for cellular GA\nxxi\n\n\fwith different values of the neighborhood radius R. Performance for each EA is an\naverage over 20 runs of the best fitness (objective function) value in the population.\nThe global optimal solution is 0............................................................................. 193\nFigure 5-26 Performance results for the Rastrigin function are shown over 3000 generations\nfor SOTEA with different settings of Kmax, and for cellular GA with different values\nof the neighborhood radius R. Performance for each EA is an average over 20 runs\nof the best fitness (objective function) value in the population. The global optimal\nsolution is 0. ........................................................................................................... 193\nFigure 5-27 Performance results for the Griewangk function are shown over 3000\ngenerations for SOTEA with different settings of Kmax, and for cellular GA with\ndifferent values of the neighborhood radius R. Performance for each EA is an\naverage over 20 runs of the best fitness (objective function) value in the population.\nThe global optimal solution is 0............................................................................. 194\nFigure 5-28 Performance results for Watson's function are shown over 3000 generations for\nSOTEA with different settings of Kmax, and for cellular GA with different values of\nthe neighborhood radius R. Performance for each EA is an average over 20 runs of\nthe best fitness (objective function) value in the population. The global optimal\nsolution is 0.01714. ................................................................................................ 194\nFigure 5-29 Final performance results for the Frequency Modulation (Left), Error\nCorrecting Code (Middle) and System of Linear Equations (Right) test functions are\nshown with box plots of performance data grouped by Panmictic EA, cellular GA,\nand SOTEA. The box plots represent final algorithm performance (after 3000\ngenerations) over 20 runs for all cGA, SOTEA, and Panmictic EA designs. This\nincludes data from the four cGA designs (with different parameter settings for\nneighborhood radius R), the four SOTEA designs (with different parameter settings\nfor KMax), and the eight Panmictic EA designs described in Section 5.4.2.1. When\nnecessary, insets are provided for the cGA and SOTEA box plots (with data plotted\non a log scale) to highlight the difference in results between these two algorithms.\nAll three design problems are Minimization problems.......................................... 195\nFigure 5-30 Final performance results for the Rastrigin (Left), Griewangk (Middle) and\nWatson (Right) test functions are shown with box plots of performance data\ngrouped by Panmictic EA, cellular GA, and SOTEA. The box plots represent final\nalgorithm performance (after 3000 generations) over 20 runs for all cGA, SOTEA,\nand Panmictic EA designs. This includes data from the four cGA designs (with\ndifferent parameter settings for neighborhood radius R), the four SOTEA designs\n(with different parameter settings for KMax), and the eight Panmictic EA designs\ndescribed in Section 5.4.2.1. When necessary, insets are provided for the cGA and\nSOTEA box plots (with data shifted and plotted on a log scale) to highlight the\ndifference in results between these two algorithms. All three design problems are\nMinimization problems. ......................................................................................... 196\nFigure 5-31 Topological properties for SOTEA with different values of KMax and\npopulation sizes of N = 50 (\u2666), 100(\u25fc), and 200(\u25b2). Characteristics include a) the\ncharacteristic path length (L), b) the correlation between c and k (c-k), c) the slope\nxxii\n\n\fof the degree correlation (\u03c5), d) the average clustering coefficient cave and e) the\ndegree average kave. ................................................................................................ 198\nFigure 5-32 SOTEA Network Visualizations with KMax = 7 for population sizes N = 50, N =\n100, and N = 200. Network visuals were created using Pajek Software.............. 202\nFigure 5-33 SOTEA Network Visualizations with KMax = 5 for population sizes N = 50, N =\n100, and N = 200. Network visuals were created using Pajek Software............... 203\nFigure A-1 Simplified diagram of an alkylation process (recreated from [224]) ............. 236\nFigure A-2 Diagram of the Heat Exchanger Network Design Problem involving 1 cold\nstream that exchanges heat with three hot streams. Parameters to optimize include\nheat exchange areas (x1, x2, x3) and stream temperatures (x4, x5, x6, x7, x8). .......... 238\nFigure A-3 Pressure Vessel Drawing. Parameters of the problem include the thickness of\nthe shell Ts, the thickness of the head Th, the inner radius of the vessel R and the\nlength of the cylindrical section of the vessel L. This figure is taken out of [221]\nand is reprinted with permission from IEEE (\u00a9 1999 IEEE). ............................... 239\nFigure A-4: Diagram of a welded beam. The beam load is defined as P with all other\nparameters shown in the diagram defining dimensional measurements relevant to\nthe problem. This figure is taken out of [221] and is reprinted with permission from\nIEEE (\u00a9 1999 IEEE).............................................................................................. 240\nFigure A-5 Diagram of Tension Compression Spring. Parameters of the problem include\nthe mean coil diameter D, the wire diameter d and the number of active coils N\nwhich is represented by the number of loops of wire in the diagram. Forces acting\non the spring are shown as P. This figure is taken out of [221] and is reprinted with\npermission from IEEE (\u00a9 1999 IEEE). ................................................................. 242\n\nxxiii\n\n\fLIST OF TABLES\nTable 3-1 Partial list of methods that have been used to adapt search operator probabilities.\nParameters that are not specified in the method are listed as (-), parameters that are\nnot applicable are listed as (*) , and parameters that were varied in experiments are\nlisted as the range of values tested. Parameter \u03b1 in column two is the memory\nparameter given in (3-1), \u03b2 in column three is a parameter specific to the adaptive\npursuit strategy and is defined by (3-3), \u03c4 in column four is the adaptation cycle\nlength which defines the number of generations between the updating of search\noperator probabilities, PMin in column five is the lower bound on the allowed range\nof probability values, the \"Event Measurement\" in column six is described in\nSection 3.1.4.4, the \"Interpretation\" in column seven is the interpretation of event\nmeasurements as described in Section 3.1.4.5, and Nops in column eight is the\nnumber of search operators being adapted. The ETV event measurement and\nOutlier interpretation in the bottom two rows of the table are new event\nmeasurement and interpretation methods (resp.) proposed in this thesis and are\ndescribed in Sections 3.2.2 and 3.2.4. \"His. Credit\" refers to Historical Credit\nAssignment which is described below in Section 3.1.4.6. ....................................... 51\nTable 3-2 List of test functions used in experiments. Problem definitions, parameter\nsettings, fitness landscape characteristics, and problem descriptions (for design\nproblems) are provided in Appendix A.................................................................... 70\nTable 3-3: List of search operators used in EA designs. Full descriptions of each search\noperator are provided in Appendix B....................................................................... 71\nTable 3-4 Details of the adaptive methods used for adapting search operator probabilities\nare listed. Column one provides the label used to refer to each adaptive method.\nThe second column indicates whether the adaptive method uses the adaptive pursuit\nstrategy (Y) or the probability matching strategy (N). The measurement of an event\nis given in column three as either the fitness (F) or the Event Takeover Value\n(ETV). The interpretation of event measurements is either one of those listed in\nSection 3.1.4.5 or the Outlier method of Section 3.2.4.1. For the \"ETV\" adaptive\nmethod, the interpretation is equivalent to the ETV value. Each adaptive method\nhas the task of setting the operator probabilities for the 10 search operators listed in\nTable 3-3. Each adaptive method uses parameter settings \u03b1 = 0.8, PMin=0.02 and \u03c4\n=10. The adaptive pursuit strategy also has \u03b2 = 0.8. No attempt was made to tune\nthese parameters and the values were chosen largely to maintain consistency with\nprevious research in this topic. Preliminary testing indicated that the results are not\nstrongly sensitive to the setting of \u03b1 and \u03c4. .............................................................. 75\nTable 3-5 Overall performance statistics for each of the adaptive and non-adaptive EA\ndesigns. Column two measures the percentage of problems where an EA design\nwas the best EA design (comparisons based on median objective function value).\nColumn three measures the percentage of problems where an EA design was able to\n\nxxiv\n\n\ffind the best solution at least one time. The best solution is defined as the best\nfound in these experiments and is not necessarily the global optimal solution. ...... 76\nTable 3-6 Overall performance statistics for each of the adaptive and non-adaptive EA\ndesigns run on the artificial test functions. Column two measures the percentage of\nproblems where an EA design was the best EA design (comparisons based on\nmedian objective function value). Column three measures the percentage of\nproblems where an EA design was able to find the best solution at least one time.\nThe best solution is defined as the best found in these experiments and is not\nnecessarily the global optimal solution. Results for the non-adaptive EA designs are\nshown in the bottom two rows while the rows labeled as ETV and ETV-Outlier\nshow results for the new adaptive methods developed in this thesis. ...................... 95\nTable 3-7 Overall performance statistics for each of the adaptive and non-adaptive EA\ndesigns run on the engineering design problems. Column two measures the\npercentage of problems where an EA design was the best EA design (comparisons\nbased on median objective function value). Column three measures the percentage\nof problems where an EA design was able to find the best solution at least one time.\nThe best solution is defined as the best found in these experiments and is not\nnecessarily the global optimal solution. ................................................................. 100\nTable 4-1 Names of the seven search operators used in the cellular GA and Panmictic EA\ndesigns are listed below. More information on each of the search operators can be\nfound in Appendix B.............................................................................................. 113\nTable 5-1: Topological Characteristics for the interaction networks of the Panmictic GA,\ncellular GA, and SOTEA. For comparison, common topological characteristics of\ncomplex networks are also provided (taken from [198], and references therein). L\nis the characteristic path length, k is the node degree, kave is the average node\ndegree, N is the population size, and R is a correlation coefficient for the stated\nproportionalities. .................................................................................................... 165\nTable 5-2: Names of the seven search operators used in the cellular GA, SOTEA, and\nselected Panmictic EA designs are listed below. More information on each of the\nsearch operators can be found in Appendix B. ...................................................... 182\nTable 5-3 Overall performance statistics for the Panmictic EA, the cellular GA, and\nSOTEA. Column two measures the percentage of runs where the optimal solution\nwas found. The optimal solution is defined as the best solution found in these\nexperiments. Column three measures the percentage of runs where the solution\nranks in the top 5% of solutions from all EA designs. In column four, \"p\" indicates\nthe p value for the Mann-Whitney U-test where the hypothesis is that the given EA\ndesign class is superior to the other two EA design classes. Column five measures\nthe percentage of problems where the best EA design belonged to a particular\ndesign class. Column six measures the percentage of problems where an EA design\nclass was able to find the best solution at least one time. Statistics in columns 1-3\nare an average value over all test problems. .......................................................... 184\n\nxxv\n\n\fTable 5-4: Topological characteristics for the interaction networks of the Panmictic EA,\ncellular GA, and SOTEA. SOTEA networks are averages taken over all settings for\nKMax as described elsewhere. For comparison, common topological characteristics\nof several biological systems are also provided (taken from [196] and references\ntherein). Characteristics include the characteristic path length L, the degree average\nkave, the linkage distribution (k dist.), the average clustering coefficient cave,\ncorrelation between c and k (c-k), and degree correlations (k-kNN). For the k\ndistribution, \u03b3 refers to the exponent for k distributions that fit a power law. Two\nvalues for \u03b3 are given for the metabolic network and refer to the in/out-degree\nexponents (due to this being a directed network). Results for degree correlations are\ngiven as the slope \u03c5 of kNN vs k. N is the population size, and R is a correlation\ncoefficient for the stated proportionalities. ............................................................ 199\nTable A-1 Artificial Test Function Characteristics Table. Epi: Epistasis or Tight Linkage\n(i.e. Non-Separable), Con = Continuous, n = problem dimensionality (* indicates n\nis a parameter of the problem), Ref. = reference to problem description used, MM =\nmultimodal fitness landscape, Params = parameters of the problem. .................... 228\nTable B-1: List of search operators used in experiments. Details provided in this table\ninclude the search operator name, other common name, reference for description,\nand parameter settings if different from reference................................................. 244\nTable C-1 Comparison of results for the alkylation process design problem (maximization\nproblem). Results from other authors were reported in [224]. The best solution\nfound in these experiments was (F, x1, x2, x3, x4, x5, x6, x7) = (1772.77, 1698.18,\n54.73, 3029.65, 90.25, 95, 1035, 153.53) with constraints (g1, g2, g3, g4, g5, g6, g7,\ng8, g9, g10, g11, g12, g13, g14) = (0, 0, 4.70E-11, 0, 0, 3.72E-11, 9.98E-8, -0, 0, 0, 0, 0,\n0, 0). ....................................................................................................................... 248\nTable C-2 Comparison of results for the heat exchanger network design problem\n(minimization problem). Results from other authors were reported in [224]. The\nbest solution found in these experiments was (F, x1, x2, x3, x4, x5) = (7049.25,\n579.19, 1360.13, 5109.92, 182.01, 295.60) with constraints (g1, g2, g3) = (-2.06E-3,\n-6.22E-3, -4.60E-3). ............................................................................................... 249\nTable C-3 Comparison of results for the pressure vessel design problem (minimization\nproblem). Results from other authors were reported in [223]. Results are also\nreported for [222] however their solution violates integer constraints for the 3rd and\n4th parameters making their final solution infeasible. It should also be mentioned\nthat equations for defining the problem have errors in [221] and [223]. Previous\nstudies have used different bounds for the solution parameters in this problem which\nare stated in Column 4. These bounds can change the location of the optimal\nsolution making it hard to compare experimental results from different authors. The\nbest solution found in these experiments was (F, x1, x2, x3, x4) = (5850.37, 38.8601,\n221.365, 12, 6) with constraints (g1, g2, g3, g4) = (-7.00E-8, -4.27E-3, -0.53, -18.66).\n................................................................................................................................ 249\nTable C-4 Comparison of results for the welded beam design problem (minimization\nproblem). Results from other authors were reported in [222]. The best solution\nxxvi\n\n\ffound in these experiments was (F, x1, x2, x3, x4) = (1.72485, 0.20572973978,\n3.47048651338, 9.0366239103, 0.2057296397) with constraints (g1, g2, g3, g4, g5,\ng6, g7) = (0, 0, -9.99E-8, 0, 0, 0, 0)......................................................................... 250\nTable C-5 Comparison of results for the tension compression spring problem (minimization\nproblem). Results from other authors were reported in [221]. The best solution\nfound in these experiments was (F, x1, x2, x3) = (0.0126652303, 0.051838,\n0.360318, 11.081416) with constraints (g1, g2, g3, g4) = (-3.16E-5, 1.47E-5, -4.06, 0.725). .................................................................................................................... 250\nTable C-6 Comparison of results for the gear train design problem (minimization problem).\nResults from other authors were reported in [220]. The best solution found in these\nexperiments was (F, x1, x2, x3, x4) = (2.70 x10-12, 19, 16, 43, 49). ......................... 250\nTable C-7: Final performance results for eight Panmictic Evolutionary Algorithms run for\n3000 generations with algorithm designs varying by the use of generational (Gen) or\npseudo steady state (SS) population updating, the use of binary tournament selection\n(Tour) or truncation selection (Trun), and the number of search operators (Nops).\nPerformance is presented as the single best objective function value found in 20\nruns FBest as well as the average objective function value over 20 runs FAve. None of\nthe Evolutionary Algorithms listed below failed to obtain a feasible solution within\n3000 generations. The single best fitness values found for each problem are in bold.\n................................................................................................................................ 251\n\nxxvii\n\n\fABBREVIATIONS\n\nBA\n\nBarabasi-Albert Network Model (Section 5.1.3.1)\n\ncGA\n\ncellular Genetic Algorithm (Section 2.3.7.2)\n\nDC\n\nDeterministic Crowding (Section 2.3.7.1)\n\nDD\n\nDuplication and Divergence Network Model (Section 5.1.3.2)\n\nDE\n\nDifferential Evolution (Section 2.3.4)\n\ndGA\n\ndistributed Genetic Algorithm (Section 2.3.7.2)\n\nEA\n\nEvolutionary Algorithm (Section 2.3)\n\nEC\n\nEvolutionary Computation (Section 2.3)\n\nECC\n\nError Correcting Code Problem (Appendix A)\n\nETV\n\nEvent Takeover Value (Section 3.2.2)\n\nGA\n\nGenetic Algorithm (Section 2.3)\n\nMMDP\n\nMassively Multimodal Deceptive Problem (Appendix A)\n\nMTTP\n\nMinimum Tardy Task Problem (Appendix A)\n\nRCGA\n\nReal-Coded Genetic Algorithm (Section 2.3.6)\n\nSOC\n\nSelf-Organized Criticality (Section 4.3)\n\nSOTEA\n\nSelf-Organized Topology Evolutionary Algorithm (Chapter 5)\n\nxxviii\n\n\fChapter 1: Introduction\n\nChapter 1\n\nIntroduction\n\nEvolutionary Algorithms (EA) are a class of stochastic optimization methods which loosely\nfollow principles of natural selection in order to solve challenging problems. Over the\nyears, a strong track record (e.g. see [1], [2]) has brought them popularity in academia and\nhas also started to bring acceptance from industry [3], [4]. Today a number of companies\nwhich specialize in providing optimized business solutions are now using EA techniques\n[5], [6], [7], [8], [9], [10], [11].\nAlthough Evolutionary Algorithms have achieved impressive performance in many\napplication domains, these achievements are partly the result of careful algorithm design\nwhich often involves substantial efforts in defining a problem's representation and/or the\ncareful design of an EA's genetic operators.\n\nThese significant design efforts are a\n\nreflection of the fact that an EA is presently not able to robustly adapt its search behavior to\nfit a particular optimization problem. One promising avenue for addressing this problem is\nlearn how open-ended adaptability and robustness occurs in natural evolutionary processes\nand to incorporate these mechanisms into an EA.\nTo achieve such a goal, it is expected that a number of key features of natural evolution will\nneed to be integrated into an EA, some of which are not yet fully understood.1 Although\nwe still lack a complete understanding of evolution, the post-genomic era has provided a\nnumber of important insights into complex biological systems as well as a better\nunderstanding of the evolutionary processes that created these systems. With these recent\ndevelopments in mind, it was agreed upon at a recent workshop that a concerted effort\nshould now be made to integrate the latest understanding of evolutionary processes into EA\ndesign [22].\nIf such efforts bear fruit over the coming years, it is anticipated that EA will become a more\nflexible, autonomous, and robust algorithm for solving today's learning, control, design,\n\n1\n\nExamples of important features of natural evolution that should be of particular interest to EC research are\ndiscussed in [12], [13], [14], [15], [16], [17] and studied in [18], [19], [20], [21].\n\n1\n\n\fChapter 1: Introduction\n\nand scheduling tasks. As the gap between natural evolution and EA behavior is narrowed\neven further, it is anticipated that EA research could also become of strategic importance\nfor a number of frontier technologies where optimization methods are required but are not\npresently capable of providing viable solutions.\n\n1.1 Aims and Outline\nThe overarching aim of this research is to make progress in narrowing the gap between EA\nand natural evolution. Hence, the research questions raised in this thesis are aimed at\nincorporating (or understanding) some of the non-trivial aspects of natural evolution that\nare missing in Evolutionary Algorithms. The key aims and research questions raised in this\nthesis are described below.\nUnderstanding and Designing an Adaptive System: The effectiveness of an adaptive\nsystem can be measured by its ability to maintain competitiveness in a changing\nenvironment.\n\nNatural adaptive systems have ingrained within them an ability to\n\nadvantageously change internal components when exposed to changing external forces.\nHowever, it is not completely understood what features are required to make an adaptive\nprocess effective in Evolutionary Algorithms. An example of such an adaptive process is\nobserved in the adaptive methods used for the automated control of EA design parameters.\nWithin this context, this thesis aims to answer how the interactions between such adaptive\nsystems and their environment can be translated into useful information for driving internal\nchanges to these adaptive systems.\nTo date, research into methods for adapting EA design parameters has focused on the use of\nfitness measurements for controlling adaptive behavior. Chapter 3 presents an alternative\nmeasurement, called ETV (Event Takeover Value), which is derived from empirical\nevidence of an individual's impact on population dynamics. The ETV is able to measure an\nindividual's impact on EA population dynamics through an analysis of EA genealogical\ngraphs.\nDuring a preliminary analysis of population dynamics (using ETV), an unexpected\nbehavior is uncovered in Evolutionary Algorithms. It is found that there is a surprising\nscarcity of individuals in an EA population that cause even moderate changes to population\n2\n\n\fChapter 1: Introduction\n\ndynamics. Instead, most individuals actually have a negligible impact on the system. After\nthorough testing (mostly presented in Chapter 4), it is concluded that this is a fundamental\nproperty of EA dynamics and that most interactions between the EA population and its\nenvironment are effectively neutral and non-informative.\nBased on this conclusion and through the use of statistical arguments, the new adaptive\nsystem is modified to filter out data from individuals that have a small impact on\npopulation dynamics. As a result, only important interactions between the adaptive system\nand its environment are used to drive adaptive changes in the system.\n\nExperiments\n\nconducted on a number of artificial test functions and engineering design problems indicate\nthat the new method for adapting EA design parameters is superior to a number of adaptive\nmethods selected from the literature.\nUnderstanding EA Population Dynamics: The aim of Chapter 4 is to gain a better\nunderstanding of the population dynamics of Evolutionary Algorithms using the ETV\nmeasurement derived in Chapter 3. One important observation from this chapter is that the\nprobability distribution of an individual's impact on population dynamics fits a power law\nwith most individuals having a negligible impact. This chapter investigates this feature of\nEA dynamics more closely with the goal of determining what experimental conditions lead\nto power laws and what conditions lead to deviations from a power law.\nBy knowing what aspects of EA design can impact this characteristic of EA population\ndynamics, it is expected that this information can be used to improve EA robustness, in\npart, by driving EA behavior towards a more accurate reflection of natural evolution. After\ncomparisons are made between EA results and somewhat related observations from natural\nevolution, it is concluded that some aspects of the two systems share similar patterns of\nbehavior but only when certain conditions are met. In particular, the population topology is\nfound to be a significant factor in the ETV results and it is found that EA populations that\nare fully connected (i.e. Panmictic) are unable to mimic the spatial properties of natural\nevolutionary dynamics.\nThe experimental results from this chapter also provide evidence that the spatial properties\nof EA dynamics and its genealogy are self-organized and possible explanations for this\nbehavior are given based on the Theory of Self-Organized Criticality.\n3\n\n\fChapter 1: Introduction\n\nMimicking the Structural Organization of Complex Systems: Although the majority of\nexperimental factors tested in Chapter 4 do not significantly influence EA population\ndynamics, one factor which did alter its behavior was the introduction of spatial restrictions\nwithin the population. Interaction restrictions also occur in biological systems, however it\nis well known that network approximations of these systems have very different topological\nproperties compared to current spatially distributed EA populations.\nStructure is an emergent property of complex biological systems and plays a fundamental\nrole in the robustness and general behavior of these systems.\n\nChapter 5 reviews\n\ncontemporary understanding of how structure emerges in nature with the goal of\ndetermining how similar structural organization can be integrated with EA design in order\nto improve its robustness and behavior.\nHence, one of the primary aims of this chapter is to determine how an EA population\nstructure can self-organize to exhibit topological characteristics similar to complex\nbiological systems. This aim is achieved by modifying the population topology using\nlocalized rules that are coupled to EA population dynamics. Two different models of\ntopological organization are studied and each is found to display interesting behaviors. In\nparticular, the first model is found to generate non-random selection pressure patterns\nwithin the population topology and also is able to sustain very high levels of genetic\ndiversity. The second model is intentionally designed to evolve population structures with\nhigh levels of modularity. Results from testing this algorithm on a suite of test problems\nindicate that the new EA design strongly outperforms a number of other algorithms\nincluding cellular Genetic Algorithms and several non-distributed EA designs.\nThe following chapter provides general background material for this thesis. The chapter\nstarts with a brief introduction to optimization including a discussion of what conditions\nmake optimization challenging and why nature-inspired optimization methods are useful\nwithin certain contexts. A justification is also provided for the specific focus of this thesis\non Evolutionary Algorithms as opposed to other nature-inspired methods.\n\n4\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nChapter 2\n\nGeneral Background of Evolutionary\n\nAlgorithms\nThis chapter reviews (briefly) the background concepts and ideas that underlie the work\nconducted in this thesis. Each subsequent chapter also introduces and critically reviews\nmaterial that is relevant to the research questions being addressed within that chapter. The\nintention is to allow each chapter to be largely self-contained in order to improve clarity of\nthe material and to keep terminology and concepts fresh in the reader's memory as they are\nintroduced and subsequently explored.\n\n2.1 Optimization Framework\nIn general, optimization problems involve setting a vector x of free parameters of a system\nin order to optimize (maximize or minimize) some objective function F(x). A solution to a\nproblem can also be subject to the satisfaction of inequality constraints g(x), equality\nconstraints h(x), as well as upper and lower bounds on the range of allowable parameter\nvalues. Given a minimization problem consisting of n parameters, q inequality constraints,\nand r equality constraints, the problem can be defined as shown below.\nMin F (x ), x = ( x1 ,K, xn )\n\n(2-1)\n\nSubject to:\ng k ( x ) \u2264 0, k \u2208 {1,..., q}\nh j ( x ) = 0, j \u2208 {1,..., r}\n\n(2-2)\n(2-3)\n\nxiL \u2264 xi \u2264 xiU , i \u2208 {1,..., n}\n\n(2-4)\n\nThis is the basic structure of the single objective optimization problems considered in this\nthesis. There are no specific conditions attached to the variable type and the function\ncharacteristics although many of the problems tested have multimodal fitness landscapes\nand significant levels of parameter epistasis.\n\nOther conditions which are commonly\n\naddressed in optimization research but will not be specifically addressed here include\ndynamic objective functions and multiple conflicting objectives.\n5\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nOptimization problems are typically broken down into classes based on fitness landscape\ncharacteristics.\n\nFor some combinations of characteristics, search algorithms can be\n\ndesigned with a search bias that can effectively exploit these landscape features and allow\nthe problem to be solved to optimality (or near optimality) with relatively little\ncomputational effort.\n\nExamples of such simplifying characteristics include linear\n\nseparability, convex feasible spaces, and smooth unimodal landscapes.\nHowever, many real world optimization problems have characteristics which are not as\nsusceptible to simplifying assumptions. A list of arguably the most important of these\ncharacteristics is given below. It is important to note that many of the characteristics do not\nnecessarily pose a significant challenge when they occur in isolation, however the presence\nof several of these conditions can make a problem very difficult to solve.\nCharacteristics which make optimization problems challenging\n\nDimensionality\n\nUncertainty\n\nMultimodality\n\nComputational Costs\n\nComplex Constraints on Feasibility\n\nDynamic Objectives\n\nEpistasis\n\nMultiple Objectives\n\nDeception\nDimensionality: The more parameters that must be varied in order to optimize a problem,\n\nthe larger the dimensionality of the solution space. This can result in the problem size\nincreasing by orders of magnitude. However, the importance of dimensionality greatly\ndepends on the existence of parameter epistasis. If the additional parameters can be solved\nseparately from the other parameters in the problem then the increase in complexity will\nonly be additive and for the most part negligible.\nMultimodality: Multimodality refers to fitness landscapes that contain multiple fitness\n\npeaks (i.e. locally optimal solutions). These peaks play a critical role in the performance of\nalmost all optimization methods. Their prevalence through many problems of interest has\n\n6\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nbeen the primary impetus for research into alternatives to deterministic directed search and\ngradient-based search algorithms.\nComplex Constraints on Feasibility:\n\nConstraints determine which solutions are\n\nconsidered feasible within the solution space and they can play a significant role in the\ndifficulty of an optimization problem. When constraints are nonlinear, they often result in a\npatchwork of feasible solutions where isolated islands are surrounded by infeasible solution\nspace. The location of the optimal solution within this patchwork can be an important\nfactor in dictating how difficult the problem is to solve for a given algorithm.\nNonlinear constraints add to problem difficulty in a way that is somewhat similar to the\ninclusion of multiple objective functions however they also introduce unique difficulties in\na problem.\n\nUnlike the objective function where \"almost optimal\" is generally good\n\nenough, almost feasible is rarely accepted.\nEpistasis: Epistasis is a term used to indicate the degree of interaction between parameters\n\nin an objective function (or in constraint functions). Problems lacking epistatic interactions\nare completely separable (i.e. decomposable) meaning that each of the parameters can be\nsolved in isolation. Many real-world problems have at least some degree of epistasis.\nEpistasis is also a common contributor to multimodality and general problem difficulty.\nThe impact that epistasis has on problem difficulty and on EA behavior is a significant\ntopic of investigation in this thesis which is dealt with in more detail in Chapter 5.\nDeception:\n\nDeception is traditionally a term used to describe a feature of fitness\n\nlandscapes that make them difficult to solve using Evolutionary Algorithms. This difficulty\nis due to the challenge of maintaining building blocks of genetic material that are needed\nlater in the search process in order to find the optimal solution. However, the concept of\ndeceptiveness can be generalized to apply to any algorithm where the search bias ingrained\nin the algorithm makes the optimal solution more difficult to find as the search progresses.\nIn other words, deception is the result of an algorithm's search bias being fundamentally\ninappropriate for searching the given fitness landscape. A common form of deceptiveness\nis when hill climbing in a fitness landscape consistently drives the search away from the\noptimal solution.\n\n7\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nUncertainty:\n\nUncertainty refers to a lack of confidence that the fitness landscape\n\ngenerated by an objective function accurately reflects the true landscape of the problem\nbeing solved. Uncertainty in a problem can come in many different forms. For instance,\nmany real world problems are representations of a physical system where a model is\ndeveloped using a number of assumptions and simplifications. In this case, uncertainty can\ncome from the accuracy of the governing equations that are used to model the physical\nproblem. Another source of uncertainty is discretisation (i.e. granularity) of the parameter\nspace which is necessary for numerical optimization using computers. Discretisation can\nsignificantly reduce the size of the parameter space however this can also eliminate any\nchance of sampling the best parameter combinations. Depending on parameter sensitivity,\ngranularity can also play a role in defining prominent characteristics of a fitness landscape\nwhich in turn could alter the dynamics and performance of a search process.\nUncertainty can also come from noise in the objective function evaluation which can be\ninherent in the system, caused by measurement errors, or caused by numerical errors. A\nreview of additional types of uncertainty that are experienced in optimization research and\nhow they are addressed in Evolutionary Algorithms is provided in [23].\nComputational Costs: Many real world optimization problems have large computational\n\ncosts associated with the objective function and/or constraint evaluation. These costs are\ngenerally due to simulation of a real system. Metamodels such as Kriging models (e.g., see\n[24]) can be used to approximate the fitness landscape so that the more computationally\nexpensive simulations are needed less frequently.\n\nHowever, increasing computational\n\nefficiency in this way will also add uncertainty to the evaluation of the objective function.\nThis tradeoff means there are limits to the amount of increased computational efficiency\nthat can be afforded by metamodels. Another possibility is to increase the granularity of\nthe search space however there are tradeoffs with this approach as well which were\npreviously discussed.\nDynamic Objectives: Dynamic objective functions involve fitness landscapes that can\n\nchange over time. When presented with a dynamic objective function, it is generally\nassumed that it is not possible to know how the problem definition will change or how this\nwill impact the fitness landscape being searched. In this context, it is no longer sufficient to\ndesign a solver that can effectively search a given landscape. Instead, a solver must also\n8\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nmaintain some degree of search robustness and flexibility which can allow the search\nprocess to quickly account for changes in the fitness landscape. In some respects, the\ndesirable search features for a dynamic optimization problem are more inline with the\nfeatures observed in natural evolution and which are demanded by the natural environment\nand less similar to the conditions treated in traditional optimization (e.g. mathematical\nprogramming).\nThis is not to say that traditional optimization techniques are not used in this domain,\nhowever they commonly assume that dynamical uncertainty can be represented using\nprobabilistic models (e.g. stochastic programming). In most cases however, such models\nare unable to account for the emergent phenomena that is present in dynamic optimization\nproblems involving complex systems (e.g. social systems, climate change, warfare, and\norganizational dynamics).\nMultiple Objectives:\n\nobjective.\n\nMost real world problems are not defined as having a single\n\nInstead there are often multiple conflicting objectives which can not be\n\ncombined into a single metric. Common examples of such objectives include various\nmeasures of cost, performance, efficiency, risk, and heuristic objectives based on human\nexperience.\n\nThis complicates a search process because we are generally no longer\n\npresented with a problem containing a single optimal solution or a single selection pressure\n(i.e. driving force) for searching through the solution space. Such conditions can introduce\nunique challenges but also unique opportunities, especially for population based search\nprocesses.\n\nFor more information on multi-objective problem characteristics and\n\nEvolutionary Algorithms designed for this problem domain, we refer the reader to [25],\n[26].\n\n2.1.1\n\nReconciling Optimization Research in a World of \"No Free\n\nLunches\"\nPresented with the challenges listed above, it is important to ask whether an algorithm can\nbe designed to effectively deal with all of these characteristics simultaneously and in all of\ntheir varied forms. In other words, is it possible to create an effective general purpose\noptimization algorithm?\n9\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nAn important development along this line of questioning was the No Free Lunch Theorems\nfor Optimization (NFL) [27]. Given some basic assumptions (e.g. see [28]), NFL states\nthat no optimization algorithm is better than any other when its performance is averaged\nover all possible problems. If one assumes that this equality holds true for the subset of\nreal-world problems, then NFL would place severe limitations on the amount of progress\nthat is possible in optimization research.\nHowever, experience over the years suggests that NFL has only a partial bearing on the real\nworld. On the one hand, experience has shown that real-world problems cover a broad\nrange of problem types and that even for problems which appear to be similar, the best\napproach to solving them can often be very different. In short, empirical evidence supports\nthe notion that no best approach to optimization exists.\nOn the other hand, most real-world problems do display some basic similarities in fitness\nlandscape features such as the presence of correlated landscapes2 (also see [30]).\nExperience also has shown that not all optimization algorithms are equal and in fact some\nappear to be quite good at solving a reasonable range of problems (also see [30]). In\nsummary, NFL should act as a guide when conducting optimization research however the\ngoal of developing more effective optimization algorithms can be a reasonable aim if\nsufficient justification is provided.\n\n2.2 Justification of EA Research\nFollowing from the previous discussion, it appears that a strong argument should be given\nto justify research that focuses on advancing a particular class of optimization algorithms.\nA common and certainly valid justification would be one that is based on empirical\nevidence of strong algorithm performance. Indeed many nature-inspired algorithms and\nparticularly Evolutionary Algorithms (EA), have been found to be effective in a number of\n\n2\n\nOne common feature of almost all real-world problems is the existence of correlated landscapes which is to\nsay that one can expect (on average) that similarities between solutions in parameter space will produce\nsimilarities in objective function value. For correlation metrics and a review, see [29].\n\n10\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nimportant niche applications. As a result, the use of these methods in solving real world\nproblems has steadily grown over the years.\nToday, a number of nature-inspired optimization methods exist. Examples include Ant\nColonies, Immune Systems, Particle Swarms, and Simulated Annealing.\n\nEach are\n\ninteresting as topics of investigation in their own right, and deserve further study.\nHowever, the decision to use EA as the algorithmic framework for this research was not a\ndecision that was taken lightly nor was it a decision based solely on current empirical\nevidence. The decision to study Evolutionary Algorithms was instead largely based on the\ndesirable qualities of its natural counterpart.\nEvolution and Optimization: Many biological systems in nature are viewed as having\n\npowerful problem solving abilities.\n\nNature-inspired optimization methods attempt to\n\nmimic these behaviors, however few of the systems being mimicked have a clear relation to\noptimization. On the other hand, natural evolution has a number of important similarities to\noptimization that are now well recognized.\nThe first link between optimization and evolution was made in relating the natural\nenvironment to a fitness landscape which was suggested by Sewall Wright back in 1932\n[31]. He postulated that the adaptation of species was similar to climbing up a fitness\nlandscape which occurs due to genetic mutations and is driven by natural selection. It is\nquite simple (although not strictly accurate) to also think of the genome as a\nparameterization of life and to think of the thriving of a species as being due to its success\nin accomplishing some set of objectives.\nLooking at the diversity of life forms and the diversity of environments where life has\nflourished suggests that, although individual species are great specialists, the forces driving\nevolution are a powerful generalist. This ability to continually adapt and evolve new\nspecialized behaviors is not possible in today's optimization algorithms although it is a\nhighly desirable feature. In short, EA was selected as the topic of investigation because\nnatural evolution has a capacity to robustly \"solve\" a range of problems in the natural\nenvironment which are well outside the capacity of today's algorithms.\nOther natural systems, such as the behavior of ants or the immune system, are viewed as\nvery capable but highly specialized systems that have come about as a result of\n11\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nevolutionary processes. Hence, it is doubtful whether the overall potential of these other\nalgorithmic frameworks is comparable with the potential from mimicking natural evolution.\nBased on the premise that natural evolution has unique and advanced problem solving\ncapabilities, this thesis focuses on ways to mimic natural evolution in an artificial\nenvironment for purposes of optimization. This thesis tackles this topic in a multifaceted\napproach looking at issues such as i) building an effective feedback adaptive process ii)\ncomparing the dynamical behavior between EA and natural evolution and iii) creating\nmodels for the emergence of nature-inspired EA population structures. It is hoped that this\nwork will help others to look at EA from a different perspective and will help to generate\nmore effective algorithms for exploiting the power of natural evolutionary processes.\nHaving now provided the motivation and justification for this thesis, the remainder of this\nchapter provides a basic review of Evolutionary Algorithms for optimization.\n\n2.3 Evolutionary Algorithms\n\n2.3.1\n\nA Brief History\n\nThe term Evolutionary Algorithms is used to describe a range of stochastic optimization\nmethods which employ principles of natural selection and reproduction in biology to evolve\nsolutions to problems. Research in the field of Evolutionary Computation (EC) started as\nearly as the late 1950s [32], [33], [34], although much of the fundamental work, which is\ngenerally recognized as the origins of EC research, took place several years later.\nThree of the algorithmic frameworks developed in the early days of EC research are still in\nactive use today and include Genetic Algorithms (GA) [35], [36], Evolutionary\nProgramming (EP) [37], [38] and Evolution Strategies (ES) [39], [40]. Although there are\ndifferences between each of the algorithms, their similarities are much more striking and\nmost research using one algorithm class is generally applicable to the others.\nInstead of reviewing each of these algorithmic classes, the following review of EA reflects\nthe scope of the research presented in this thesis which deals primarily with the topic of\n\n12\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nparameter optimization using population-based search heuristics. For a more thorough\nreview of EC research we refer the reader to [41].\n\n2.3.2\n\nGeneral Description\n\nTaking terminology from genetics, an Evolutionary Algorithm initially starts with a\npopulation of individuals, with each individual representing a solution to the problem being\nsolved. Each individual has a chromosome made up of genes or parameters, and the set of\nall possible combinations of these genes makes up the genotypic space (i.e. solution space\nor parameter space). The individuals within a population are selected to reproduce and\nparticipate in the next generation in a process similar to the Darwinian principle of survival\nof the fittest. New individuals (referred to as offspring) are generated from selected parents\nusing what has become a library of reproduction operators (i.e. search or variation\noperators); some of which are similar to genetic mutation and recombination.\nThe selection of individuals is based on their fitness or phenotype which is typically\ndefined by the objective function value and is calculated using the genes of the individual.\nThis fitness then impacts an individual's chances of survival and/or procreation.\n\nBy\n\ncreating a bias toward selecting the best solutions for populating the next generation, the\nalgorithm is often able to exploit information contained in these more fit solutions in order\nto reach an optimal or near optimal solution.\nA more concrete understanding of Evolutionary Algorithms is possible using the\npseudocode in Figure 2-1 which loosely follows the framework outlined in [42]. For this\npseudocode, the parent population of size \u03bc at generation t is represented by P(t). For each\nnew generation, an offspring population P`(t) of size \u03bb is created using reproduction\noperators and evaluated to determine the objective function values for each offspring. The\nparent population for the next generation is then selected from P`(t) and Q, where Q is a\nsubset of P(t). Q is derived from P(t) by selecting those in the parent population with an\nage less than \u03ba.\n\n13\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nt=0\nInitialize P(t)\nEvaluate P(t)\nDo\nP`(t) = Variation(P(t))\nEvaluate P`(t)\nP(t+1) = Select(P`(t) \u222a Q)\nt=t+1\nLoop until termination criteria\nFigure 2-1 Pseudocode for a basic Evolutionary Algorithm design\n\nAlthough some EA designs do not fit the framework listed above, many common designs\ndo. For instance, a non-elitist generational EA design refers to conditions where \u03ba=1 and \u03bb\n> \u03bc, a steady state EA design refers to conditions where \u03ba=\u221e and \u03bb =1, a generation gap\nEA has 1< \u03ba < \u221e, and a pseudo steady state EA design typically involves \u03ba=\u221e and \u03bb = \u03bc.\nAlso, when an EA design is used with elitism, this simply means that the best individual in\na population is given its own value for \u03ba which is set to \u03ba=\u221e.\nBefore reviewing variation and selection schemes that are commonly used in EA design, it\nis important to first provide a clearer understanding of the parameters \u03bb, \u03bc, and \u03ba. To do\nthis, it is helpful to temporarily neglect the mechanisms used for selecting parents and for\ncreating new offspring. By doing this, search with an EA can be understood through its\nrelation to a simple branching process.\nExtending this analogy, active nodes describe points in the branching process from which\nnew branches can potentially be grown and for an EA, the number of active nodes is\ncontrolled by \u03bc. The parameter \u03bc can also be thought of as providing an upper bound on\nthe memory or the amount of genetic material present in the system. In a single time step\nor generation, the total number of new branches is controlled by \u03bb. Only active nodes have\nthe capacity to influence where new branches occur and the composition of the new nodes\n(thereby making the branching process a Markov Chain). Furthermore, active nodes are\nforced to become inactive after \u03ba time steps. This limits the amount of time that a node can\ndirectly influence the creation of new nodes.\nIn short, these three parameters impact the algorithm by constraining the dynamics to meet\ncertain conditions of this branching process. For instance, a constant value for \u03bc means that\n\n14\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nthe size (or memory) of the system can not change while setting \u03ba=\u221e means an individual\nhas the potential to directly influence future dynamics for arbitrarily long periods of time.\nThe actual influence of these parameters on other qualities of the system ultimately depends\non how selection and variation procedures are executed in the algorithm although some\ngeneral comments can be made. For instance, decreasing \u03ba generally causes the search\nprocess to become more influenced by basins of attraction in the fitness landscape while\nincreasing \u03ba causes the search to be more influenced by point attractors (i.e. local optima).\nIncreasing \u03bc can increase the amount of parallel search behavior that can potentially take\nplace in an EA however the actual amount of parallelism depends greatly on other aspects\nof EA design. Increasing \u03bb can increase the amount of innovation or changes to the\nmakeup of the population that can potentially occur, but again the actual amount of\ninnovation depends on other genetic operators.\nThe remaining sections of this chapter introduce each aspect of EA design in more detail.\nThe next section discusses different selection methods that have been devised for selecting\nP. Section 2.3.4 discusses variation methods which are also referred to as search operators\nor reproduction operators. Constraint handling is discussed in Section 2.3.5 (an important\ntopic in fitness evaluation) while options for parameter encoding are discussed in Section\n2.3.6. Some advanced features in EA design are presented in Section 2.3.7 with a focus on\ninteraction constraints between EA population members. Section 2.3.8 presents ways in\nwhich performance can be measured in Evolutionary Algorithms and Section 2.3.9\ndiscusses the uses and applications of Evolutionary Algorithms.\n\n2.3.3\n\nSelection Methods\n\nSelection methods have the task of deciding how much each individual in the population\nwill act to bias future search steps that are taken by the algorithm. There are a number of\nselection methods that have been considered in EA research and some of the more common\nschemes are briefly described in this section. In general, selection simply involves the\ncreation of one population P` by selecting individuals from another population P. Selection\nthat is done with replacement means that individuals in P can be selected multiple times so\nthat multiple copies of an individual can exist in P`. On the other hand, selection without\n15\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nreplacement means that, at most, only one copy of an individual from P can exist in P`.\nSeveral common methods for selecting individuals are described below.\n\n2.3.3.1\n\nProportional Selection\n\nProportional selection selects an individual from a population with a probability\nproportional to its fitness. Given a population size of N, the probability pi that individual i\nwith fitness fi is selected in a single selection event is defined by (2-5). To use proportional\nselection with minimization problems, it is necessary to define a scaling function. Scaling\nfunctions are also generally needed when proportional selection is used in order to address\nthis method's selection pressure sensitivity to a population's distribution of phenotypes.\nDue to the necessity of problem-specific scaling functions, proportional selection is\ndifficult to implement in practice.\n\npi =\n\nN\n\n\u2211f\nj =1\n\n2.3.3.2\n\n(2-5)\n\nfi\nj\n\nLinear Ranking Selection\n\nLinear ranking selection is an alternative selection procedure which does not have the\nscaling problems present in proportional selection. In linear ranking selection, solutions are\nranked from most fit (Rank=1) to worst fit (Rank=N) and are selected with a probability\nthat is linearly proportional to its ranking. In this way, an individual's probability of\nselection is based on how its fitness ranks among others in the population instead of being\nbased on the magnitude of the fitness value. Given a population size of N and parameters\n\n\u03b7+ and \u03b7- which control the overall selection pressure, the probability that individual i is\nselected in a single selection event is given by (2-6). In cases where linear ranking is used\nin this thesis and no values are specified for \u03b7+ and \u03b7-, it is assumed that \u03b7+ =1 and \u03b7- =0.\nOther formulations for defining linear ranking are possible such as the original definition\nwhich is given in [43], [44].\n\n16\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\npi =\n\n\u03b7 \u2212 + (\u03b7 + \u2212 \u03b7 \u2212 )( N \u2212 Rank i )\nN\n\n\u2211\u03b7\nj =1\n\n2.3.3.3\n\n\u2212\n\n(\n\n(2-6)\n\n)\n\n+ \u03b7 + \u2212 \u03b7 \u2212 (N \u2212 Rank j )\n\nExponential Ranking Selection\n\nExponential ranking is sometimes used to introduce a stronger selection pressure than is\npossible with linear ranking. As the name suggests, the probability of selection follows an\nexponential function of rank so that there is a much greater chance of being selected if a\nmember has a high ranking in the population. Selection pressure is controlled by c so that\nas c \u2192 1, the difference in selection probability between the best and worst solutions is lost\nand as c \u2192 0, selection probability differences become increasingly larger and follow an\nexponential curve along the ranked solutions.\npi =\n\nc Ranki\nN\n\n\u2211c\n\n(2-7)\n\nRank j\n\nj =1\n\n2.3.3.4\n\nTournament Selection\n\nTournament selection works by randomly sampling a subset of the population with sample\nsize q and then selecting the best individual from that sample. The size of q will impact the\nselection pressure from this method. A commonly used form of tournament selection is\nbinary tournament selection where q=2.\nTournament selection and tournament-based variants have a number of desirable properties\nthat make them a good choice when designing an Evolutionary Algorithm. Tournament\nselection is simple to use and many advanced features in an EA can be implemented using\ntournaments such as the use of crowding and age restrictions. Furthermore, tournaments do\nnot require global information in order to make selection decisions thereby making these\ndesigns more efficient to execute when run in a physically parallel environment. Finally,\nthe selection pressure of this method can be easily tuned by changing the tournament size q.\n\n17\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\n2.3.3.5\n\nTruncation Selection\n\nTruncation selection works by selecting with equal probability from among a fraction T, T\n\u2208 [0,1] of the best individuals in the population. With a population of size N sorted based\n\non rank, the selection probability is given by (2-8). If selection is conducted without\nreplacement (as is done throughout this thesis), then each of the T*N best individuals are\nselected one time only thereby causing the selection method to be deterministic.\n\u23a7 T1N , if 1 \u2264 i \u2264 TN\npi = \u23a8\nelse\n\u23a9 0,\n\n2.3.3.6\n\n(2-8)\n\nModified Tournament Selection\n\nThe experimental work in this thesis uses a modified form of tournament selection that is\ndefined by the pseudocode below.\nModified Tournament Pseudocode\n\n\u2022 starting with \u03bb + \u03bc individuals, conduct \u03bb tournaments\n\u2022 for each tournament, select the worst individual in the tournament and remove it from\nthe \u03bb + \u03bc population\n\n\u2022 after \u03bb tournaments, we are left with a new parent population of size \u03bc\n\u2022 randomly select from the parent population to generate \u03bb new offspring\nThis procedure is essentially equivalent to a canonical GA using elitism and tournament\nselection without replacement. The major difference with the canonical GA is in the\napplication of elitism: the surviving elite are chosen statistically (by tournament) rather than\ndeterministically. Furthermore, conventional elitism could be seen to over-favor the fitter\nmembers, which have a larger share of offspring per generation and survive more\ngenerations. In the modified tournament selection this favoritism happens naturally: a fitter\nmember has more offspring simply by surviving longer - a phenomenon observed in many\nspecies.\n\n18\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\n2.3.3.7\n\nCharacterization and Comments\n\nSelection schemes introduce bias into a search process primarily by selecting points in\nsolution space that are of relatively high fitness. However, selection schemes can also bias\na search in other ways and so it is important to be able to quantify different aspects of the\nsearch bias present in a selection scheme.\nOne aspect of a search bias is the loss of genetic diversity which can be approximated by\nmeasuring the proportion of individuals that are not selected during a selection phase [45].\nSelection schemes are also commonly characterized by their selection pressure which\nindicates the extent to which the scheme is biased towards preferring more fit individuals.\nFor example, the selection intensity (as defined in [46]) measures the increase in mean\nfitness resulting from a selection phase. Another possibility is to measure the takeover time\nwhich is the time required for the best individual to take over a population (restricted\nexperimental conditions apply, e.g. see [47]). Additional ways to characterize selection\nmethods have also been proposed, some of which can be found in [48], [45], and [49].\nA number of commonly used selection schemes were briefly described in this section and\ninclude proportional, linear ranking, exponential ranking, tournament, and truncation\nselection. Although there are differences in these selection schemes, their similarities seem\nto be much more striking. Most have a parameter which (to a rough approximation) tunes\nthe selection pressure in a manner similar to the others. Most are also global selection\nmethods where selection is based only on an individual's fitness.\nMore advanced selection methods do exist which take into account other factors such as\nage, genealogy, spatial locality and genotype in order to encourage different forms of\ndiversity or parallel search behavior. Some of these advanced methods are reviewed in\nSection 2.3.7.\n\n2.3.4\n\nSearch Operators and Variation\n\nSearch operators work by taking information from one or more individuals in the\npopulation as a basis for sampling new points in solution space.\n\nEarly studies of\n\nEvolutionary Algorithms involved the use of crossover and/or mutation however many of\n19\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nthe algorithms in use today employ a diverse range of search operators. Although an\nexceedingly large number of search operators have been introduced in the literature over\nthe years, only a few have been able to find traction with the broader EC community.\nSome of the more popular search operators are found in Differential Evolution [50],\nCovariance Matrix Adaptation [51], and Estimation of Distribution Algorithms [52]. These\noperators generally employ the use of multiple parents and are highly successful when\nassumptions about the fitness landscape are met.\n\n2.3.4.1\n\nCharacterization\n\nA set of ten search operators are used in experiments throughout this thesis and are\ndescribed in Appendix B. As an alternative to reviewing each of these search operators in\ndetail, as well as others that have been introduced in the literature, it is possibly more\nilluminating to discuss the search operators in more general terms based on their behavior\nand intended usage. A few directions along these lines are provided below.\nIntent and Search Bias: Probably the most important questions to ask about an operator\n\nare; what sort of search bias is created by the operator and what sort of landscape is the\noperator expecting to search. A number of standard search operators have been developed\nover the years which can be better understood by attempting to answer these questions.\nAs an example, gene swapping operators like single point crossover and uniform crossover\n(defined in Appendix B) are expecting that a partial decomposition of the problem exists\n(which does occur to some extent in many problems). However, these operators also\nexpect that the problem can be separated along the specified dimensions of the parameter\nspace (e.g. without the need for linear transformation) which is less often the case. These\noperators also expect to have access to a population that adequately samples each of the\nsub-problems or so-called building blocks.\nAnother good example are search operators with hill-climbing characteristics.\n\nThese\n\noperators expect that the fitness landscape will be somewhat smooth in the region in which\nthe population is distributed in parameter space.\n\nOne example is Wright's heuristic\n\ncrossover [53] which generates solutions by linear interpolation between two parents or\n\n20\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nextended line crossover [46], [54] which generates solutions by linear extrapolation (both\ndefined in Appendix B).\nExploitation and Exploration: Another important search operator property is the extent to\n\nwhich a search operator creates offspring that are biased to reflect the genetic material of\nthe parents. In discrete spaces this can be approximated by counting how many of the\ngenes in the final offspring are identical to the genes in the parents. In continuous spaces,\nsuch an assessment can be more difficult to make. Search operators which create offspring\nthat largely reflect the parents are often labeled as exploitive. This label is also used for\noperators that are biased to predominately reflect the features of the more fit parent. In\neither case, the operators are \"exploiting\" a landscape feature that is common in many\noptimization problems, namely that similarities in genotype tend to correspond with\nsimilarities in phenotype (i.e. correlated landscapes).\nThe opposite of this is exploration which generally is used to describe offspring that are\ndifferent from their parents. However, it is worth pointing out that this definition of\nexploration does not mean that an algorithm is necessarily capable of exploring new\nregions of the solution space.\n\nFrom a population perspective, what really defines\n\nexploration is the ability to create genetic material that is not only different from the\nparents, but is also different from other individuals in the population. Moving to a global\nperspective, an accurate definition of exploration should actually change based on the\nactions and history of the search process. However, because EA is a memory-less search\nprocess, this latter definition of exploration can not be measured or enforced meaning that\nsearch by an EA can become trapped in regions of parameter space for extended periods of\ntime.\nStochasticity: The execution of most search operators involves a random variable whose\n\nvalue is drawn from some predefined distribution. This allows the operator to display a\nrange of behaviors and greatly reduces the chances that the same inputs (i.e. parents) will\ngenerate the same output (i.e. offspring).\n\nThis can potentially help to improve the\n\nrobustness of a search process compared to deterministic operators which always give the\nsame output when given identical inputs.\n\n21\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\n2.3.4.2\n\nMultiple Operators\n\nAnother option for improving the robustness of a search process is to introduce multiple\nsearch operators, each containing a unique search bias. Recent studies have indicated that\nthe presence of multiple operators can help improve general performance of Evolutionary\nAlgorithms [55], [56].\nOn the other hand, the addition of multiple search biases (or the addition of stochastic\ncomponents to a search operator) are only advised if it is not possible to determine what the\nmost effective search bias is for a given problem. If such a bias can be obtained or learned\nthen the algorithm will perform substantially better on that problem. As a simple example,\nthe search bias obtained from using a gradient based search operator should be greatly\npreferred over a more robust operator like random search when searching a smooth\nunimodal fitness landscape.\nHowever, for many complex problems, it is expected that one particular search bias may be\ninsufficient for effectively searching throughout the entire fitness landscape. In these\nconditions, multiple search operators may be more effective. Since EA is often applied to\ncomplex problems with poorly understood fitness landscapes, it is expected that\nEvolutionary Algorithms should generally be designed using multiple search operators.\n\n2.3.4.3\n\nSearch Operator Probabilities\n\nTo develop an effective search bias in an EA design, it is necessary to select a set of search\noperators for traversing the fitness landscape as well as select the usage probabilities for\nexecuting those operators.3 Since probability parameters can take on values equal to (or\nclose to) zero, the task of selecting appropriate search operators and tuning the probability\nparameters can be thought of as similar tasks.\nSetting these parameters is often done by trial and error or by using an efficient design of\nexperiments. However, instead of running the algorithm many times in order to establish\n\n3\n\nThis can be more or less important depending on the amount of effort given to parameter encoding, which is\na complementary aspect of EA design that alters the fitness landscape.\n\n22\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nan effective EA design, it is worth considering whether an appropriate search bias can be\nlearned for a problem while the algorithm is being run. One promising option is to develop\nmechanisms that allow the probability parameter settings to adapt to the environment so\nthat the overall search bias reflects what has so far been useful for traversing that particular\nfitness landscape.\nNotice that in this case, the adapted parameter settings used in the EA design are no longer\ngenerally robust but instead become specialized for the particular problem being solved.\nSince adapting and tuning search operator probabilities is a major topic of investigation in\nthis thesis, a more detailed review will be presented in Chapter 3.\n\n2.3.4.4\n\nLocal Search and Expert Search Hybrids\n\nSearch operators can also involve more than one objective function evaluation. Such\nsearch operators are generally classified as local search or hill climbing methods and are\ndesigned to exploit local features of the fitness landscape. These methods often resemble\nclassic directed search methods, gradient-based search methods or use expert knowledge in\norder to intelligently select new solutions to evaluate.\nLocal search operators are implemented in a lot of different ways depending on their\npurpose. In some cases they are used only on the best solution at the end of an EA run as a\nmeans to fine tune the final solution more quickly than is otherwise possible using standard\nEA search operators. Other times local search is used on all individuals throughout an EA\nrun with the intention of modifying the fitness landscape from the perspective of the rest of\nthe algorithm. In this case, the local search operators are sometimes used to only modify\nthe phenotype (i.e. Baldwinian Evolution) instead of altering both the genotype and\nphenotype (i.e. Lamarckian Evolution). EA designs that are hybridized with local search\noperators are often referred to as Memetic Algorithms and are reviewed in [57].\n\n2.3.5\n\nConstraint Handling\n\nMany real-world optimization problems require a set of constraints to be placed on the\nparameters being optimized. Constraints can be simple bounds on the values a parameter is\n23\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nallowed to take but they can also be complex nonlinear relationships between multiple\nparameters which can fragment the feasible solution space. Finding feasible solutions can\nbe a challenging problem in itself and so the manner in which feasibility is treated is crucial\nto the effectiveness and efficiency of an optimization algorithm. In this section, several\nconstraint handling techniques are presented and discussed.\n\n2.3.5.1\n\nRejection of infeasible Solutions\n\nA rather na\u00efve approach to constraint handling in non-convex feasible spaces is to treat\nconstraints as they are typically treated in linear, convex problems; that is, to strictly\nenforce the constraints by requiring feasibility for every solution generated. For cases\nwhere it is not possible to directly solve the system of nonlinear constraints, this approach\ncan result in a significant computational burden.\n\n2.3.5.2\n\nConstraint Handling by Repair\n\nConstraint handling by repair involves the development of a procedure for turning\ninfeasible solutions into feasible solutions and can sometimes be as difficult to solve as the\noriginal problem.\n\nIn some instances where expert knowledge is available, repairing\n\ninfeasible solutions can be relatively straightforward and prove quite useful.\n\n2.3.5.3\n\nPenalty Functions\n\nThe most common and popular method for handling constraints is to incorporate penalty\nfunctions into the objective function. Solutions that violate one or more constraints will be\npenalized by altering their objective function value so that it represents a less fit solution.\nA few alternatives to static penalty functions are also available such as creating a dynamic\npenalty function that responds to changes in the population [58] or one that changes by a\nfixed schedule [59]. It is also worth noting that constraints can also be treated as additional\nobjectives [60] or as pseudo-objectives to an optimization problem [61]. A review of\nconstraint handling techniques that have been used with Evolutionary Algorithms is\nprovided in [62] and [63].\n24\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\n2.3.5.4\n\nStochastic Ranking\n\nAlmost all constraint handling methods work by applying a selection pressure that aims to\ndrive the population toward regions of the solution space that are both of high fitness and\nfeasible. For most EA selection methods (other than proportional selection) the selection\npressure is based on fitness rankings and not the relative difference between fitness values.\nHence, the majority of constraint handling techniques (e.g. penalty functions) can be\nunderstood as altering selection pressure by altering the rankings of individuals.\nHowever, as discussed in [64], the use of penalty functions is prone to over-dominance or\nunder-dominance. In the case of over-dominance, the penalty is too strong and all feasible\nsolutions are preferred over infeasible solutions. In the case of under-dominance, the\npenalty for infeasibility is not strong enough to impact the rankings of individuals such that\nthe objective function value is the only driver of population dynamics. Selecting the\nappropriate penalty weights is not only hard but the optimal penalty is also likely to be\ndynamic due to the non-stationary distribution of fitness values and the non-stationary\ndistribution of constraint violations within the population.\nAn effective alternative is provided by the Stochastic Ranking method of Runarsson and\nYao [64] and is used throughout this thesis.\n\nStochastic Ranking works by ranking\n\npopulation members using a stochastic sorting procedure that considers both the objective\nfunction and constraint violations. A pseudocode for Stochastic Ranking is provided in\nFigure 2-2. The decision to swap adjacent individuals (when at least one is infeasible)\noccurs based on the objective function with a probability Pf and otherwise is based on the\nextent of constraint violation. As Pf AE 0, the ranking of population members becomes\ndominated by the goal of attaining feasibility and as Pf AE 1, ranking becomes completely\nbased on the objective function. The parameter Pf allows for direct control over the extent\nof ranking changes that occur within a population due to infeasibility. Hence, this approach\nto constraint handling eliminates any problems with fitness scaling that plague penalty\nfunction methods (e.g. over-dominance and under-dominance).\n\n25\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nFor j = 1 To N\nFor k = 1 To \u03bb - 1\nsample u \u220a U(0,1)\nIf (\u03c6k = \u03c6k+1 = 0) or (u < Pf) Then\nIf (Fk > Fk+1 ) Then\nswap(k, k+1)\nEnd If\nElse\nIf (\u03c6k > \u03c6k+1) Then\nswap(k, k+1)\nEnd If\nEnd If\nNext k\nIf (no swap) Then Exit For\nNext j\nFigure 2-2 Pseudocode for Stochastic Ranking procedure where U(0,1) is a uniform random number generator, \u03c6k\nand Fk are the total constraint violation and objective function value (resp.) for individual k, N is the number of\nsweeps through the population, \u03bb is the population size, and Pf is the probability that infeasible solutions will be\nevaluated based on objective function values. The original description provided in [64] recommends N > \u03bb and Pf =\n0.45.\n\nIt is worth mentioning that approaches have also been proposed in [65] that can deal with\nthe issue of over-dominance and under-dominance when using penalty functions. In this\ncase, mock competitions take place between members in an EA population in order to\ndetermine how large the penalty function must be in order to balance the competing forces\nfrom the objective function and the penalty function.\n\n2.3.6\n\nParameter Encoding\n\nGenetic encoding (also referred to as parameter encoding) is a term that is used to describe\nhow solutions are represented in the chromosome. Genetic encoding methods are generally\nclassified as either direct or indirect encodings. With direct encoding, each gene in the\nchromosome is a parameter that is directly used without alteration for objective function\nevaluation. In other words, binary parameters of the optimization problem are represented\nby binary valued genes, integer parameters are presented by integer valued genes, and\ncontinuous parameters are represented by floating point numbers, also called real-coded\ngenes.\n\nSince many design problems involve continuous variables, real number\n\nrepresentation is very common in EA research. A review and analysis of real-coded GAs\n(RCGA) is provided in [66].\n\n26\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nIndirect encoding, on the other hand, involves a mapping process where genes must be\ntransformed or processed in some fashion before fitness evaluation can occur. A simple\nindirect encoding scheme is provided in the canonical GA. Here the chromosome is\nrepresented by a binary string which is broken up into n equal length segments\ncorresponding to n genes. Each binary segment is decoded to an integer value which is\nthen rescaled to fit within the boundary constraints of the corresponding parameter.\nAlthough arguments based on the Schema Theorem [36] have been given for preferring the\nbinary encoding scheme of the canonical GA, direct encoding is more commonly used in\nEA designs today.\n\nHowever, some specialized indirect encoding schemes have been\n\nsuccessfully applied such as that seen in Genetic Programming [67], [68] and in the\nevolution of artificial neural networks [69], [70], [71]. Similar success has yet to be seen\nfor parameter optimization problems, however some interesting studies have occurred in\nrecent years [72], [73].\n\n2.3.6.1\n\nGene Expression Research\n\nIndirect gene encoding can be seen as an analogue to the genotype-phenotype mapping\nprocess in living systems and is an important open topic in EA research. Indirect gene\nencodings are of great importance because they can influence features of the fitness\nlandscape potentially making a problem more or less difficult for a particular algorithm to\nsearch.\nObservations from Nature: Several details of the genotype-phenotype mapping process\n\nin nature have been uncovered over the years and this should help to guide future directions\nof artificial gene expression research. Some features of the mapping process are discussed\nbelow however this is not meant to be an exhaustive review of the topic.\nOne important feature of the mapping process is the dominating presence of canalizing\nfunctions.\n\nThis ubiquitous feature acts to dampen perturbations (e.g. from the\n\nenvironment) to the genotype-phenotype mapping process allowing it to quickly return to\nits intended dynamical trajectory. Canalization can also be understood as an important\nsource of dynamical robustness. The term dynamical robustness is used in reference to the\nstability of phenotypic expression in the face of environmental perturbations. Evidence of\n27\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\ndynamical robustness is seen for example in the yeast cell-cycle [74]. Artificial models\nhave also displayed this property to some degree as seen for instance in [75].\nA feature that is similar to dynamical robustness is mutational robustness which is also\nknown as genetic neutrality.\n\nMutational robustness can be observed in the fitness\n\nlandscape of natural evolving systems and is created (in part) by a many to one mapping\nfrom genotype to phenotype. There is evidence that high levels of neutrality are present in\nnatural evolution and this feature is believed to have a dramatic impact on evolutionary\ndynamics as was first theorized by Kimura [76]. Most studies of neutrality have so far\nfocused on RNA folding as seen for instance in [77], [78].\nIt has also been proposed in [79] that with sufficient neutrality, neutral networks of single\npoint mutations can percolate throughout genotype space meaning that a large portion of\nthe space can be reached without requiring changes in fitness. Such landscape features are\nless likely to force population convergence to a static region of genotype space and could\nplay an important role in maintaining population diversity as well as improving\nevolvability. In fact, they suggest in [80] that the presence of neutral networks can cause\nentropic barriers to replace fitness barriers meaning that adaptive improvements become\nless a question of \"if\" and more a question of \"when\".\nIt is worth mentioning that key features of the mapping process such as canalizing functions\nwere shown to be easily created in Boolean networks [81] however similar mechanisms\nhave not yet been considered in any indirect encoding schemes for EA. Also in [82], it was\nfound that genetic robustness or neutrality is prevalent in some classes of distributed\ndynamical systems.\n\n2.3.7\n\nInteraction Constraints in EA Populations\n\nTraditionally, EA population dynamics occur without restrictions or constraints on which\nindividuals in the population can interact (e.g. through selection and reproduction). As a\nconsequence, the population is tightly coupled and can become stuck or stalled for many\nfitness landscapes of interest. In order to provide for a more robust and parallel search\nprocess, restrictions in competition and/or mating are sometimes used.\n\n28\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nThis section reviews a number of advanced aspects of EA design which can be grouped\nunder heading of interaction constraints within EA populations. The history and intent of\nthe methods discussed below are often very different but they share a commonality that\nmakes them an important development in EA research.\nEach of the topics reviewed below introduces a new form of diversity or heterogeneity into\nan Evolutionary Algorithm. In many cases, these changes to the algorithm have resulted in\nsubstantial improvements in the robustness of EA performance. In addition to issues of\ndiversity in genotypes, these methods also offer other types of diversity such as diversity in\nphenotypes, ages, genetic operators, and selection pressures. In some cases, the age-old\nexploration-exploitation tradeoff no longer applies due to the fact that a range of exploitive\nand explorative behaviors can now be displayed within a single system.\n\n2.3.7.1\n\nCrowding and Niche Preserving Methods\n\nOne set of methods for restricting interactions in an EA are crowding methods which are\ngenerally thought of as a subclass of niching methods. Crowding methods work by forcing\nindividuals to compete for survival and/or reproduce with others in the population that are\nsimilar. Similarity is generally defined based on the genotype or phenotype however\nhistorical (i.e. genealogical) similarity is also sometimes used. An example of the later\nwould be restricting an offspring to only compete with its parents. A number of crowding\nmethods have been developed including the original \"standard crowding\" proposed in [83],\nDeterministic Crowding [84], restricted tournament selection [85] and probabilistic\ncrowding [86].\nDeterministic Crowding (DC) is an interesting extension to standard crowding in that the\nmethod guarantees that each individual in the population, or a direct descendent of the\nindividual, will survive to the next generation. From a genealogical perspective, this is an\ninteresting change to the algorithm because stochastic effects are eliminated which\n\n29\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\notherwise would cause a continual loss of lineages from the population.4 The result is a\nsubstantially parallel search process. Pseudo code for DC is presented in Figure 2-3.\nFor each generation\nDo N/2 times\nSelect 2 parents, p1 and p2 (randomly, no replacement)\nCreate 2 offspring, c1 and c2\nIf (d(p1, c1) + d(p2, c2)) < (d(p1, c2) + d(p2, c1))\nIf f(c1) > f(p1) replace p1 with c1\nIf f(c2) > f(p2) replace p2 with c2\nElse\nIf f(c2) > f(p1) replace p1 with c2\nIf f(c1) > f(p2) replace p2 with c1\nEnd If\nLoop\nNext\nFigure 2-3 Pseudo code for Deterministic Crowding. Distance (either genotype or phenotype) is given as d(),\nfitness is given as f(), and N is the population size.\n\nThe more general classification of niching is used to described strategies which allow an\nEA population to converge on multiple optima but do not necessarily involve a clear\nrestriction of interactions within an EA population. For instance, Sharing Methods [87] are\na form of globally-controlled niching where individuals are forced to share their fitness\nwith other individuals based on distances in genotype (or phenotype) space. Because an\nintimate knowledge of the fitness landscape is needed to appropriately establish the correct\nsharing strategy and parameter settings, sharing methods are not easily used.\n\nOther\n\nniching methods also exist such as Clearing which was proposed in [88] and is similar to\nFitness Sharing.\n\n2.3.7.2\n\nSpatially Distributed Populations\n\nAnother approach to restricting interactions in an EA is to define the population on a graph\nso that operators such as selection and reproduction are only able act within localized\nregions defined by the graph topology.\n\nEA designs where the population is spatially\n\ndistributed in some manner are referred to as distributed Evolutionary Algorithms (dEA).\n\n4\n\nThis is not to say that population convergence can no longer occur. Convergence is also be driven by search\noperators.\n\n30\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nThere are two general approaches to introducing spatial restrictions in an EA population\nwhich are referred to in this thesis as island models and network models.5 Reviews on\ndistributed EA designs as well as physically parallel implementations of Evolutionary\nAlgorithms are provided in [89] and [90].\nIsland Models: Island models work by breaking the EA population into subpopulations or\n\nislands, with the dynamics of each subpopulation loosely coupled to the others. With\nIsland Models, genetic operators can only be used between individuals within the same\nisland. Occasionally, individuals are selected to move to a new island thereby allowing one\nisland to influence the dynamics of another.\n\nA directed graph topology is usually\n\nestablished for the islands so that only specified islands can pass individuals to other\nspecified islands.\n\nSome of the earliest work on island model EA populations was\n\nconducted in [91].\nNetwork Models: With network models, the population is defined on a graph where each\n\nnode represents an individual in the population. Network models are almost exclusively\ndefined on a cellular grid and are often referred to as cellular Genetic Algorithms (cGA).\nThese distributed EA designs work by having genetic operators such as selection and\nreproduction restricted to occur within local neighborhoods on the network. Some of the\nearliest work on network models for EA populations was conducted in [92] and [93].\nThe network topology is known to significantly impact the behavior and performance of a\ncGA as was demonstrated in [94]. Since population topology and its impact on EA\nbehavior is a major topic of investigation in this thesis, a more detailed review will be\npresented in Chapter 5.\nPopulation updating strategies also can influence the overall dynamics of the system as well\nas the selection pressure as seen in [95] and [96]. It is also worth noting that other\ndistributed dynamical systems have displayed sensitivity to the population updating\nstrategy as seen for instance in the related field of complex systems research [97], [98].\n\n5\n\nIn practice, combinations of the two classes are common and are referred to as Hierarchical Evolutionary\nAlgorithms.\n\n31\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nAn example of a cGA with synchronous updating is provided in the pseudocode below.\nFirst, a population P() of size N is defined on a graph (usually a 1-D or 2-D grid). For a\nsingle generation, each cell is subjected to standard genetic operators. For a given cell i,\nparents are selected from within its neighborhood, search operators are selected, and an\noffspring is created and evaluated. The better fit between the offspring and P(i) is stored in\n\nTemp(i) until all N cells are calculated. The grid is then updated (synchronously) for the\nnext generation by replacing P() with Temp(). This process repeats until some stopping\ncriteria is reached.\nInitialize P()\nEvaluate P()\nInitialize Population Topology\nDo\nFor i=1 to N\nSelect Parents from Neighborhood(P(i))\nSelect Search Operators\nCreate and Evaluate offspring\nTemp(i) = Best_of(offspring, P(i))\nNext i\nReplace P() with Temp()\nLoop until termination criteria\nFigure 2-4 Pseudocode of a synchronous cGA\n\n2.3.7.3\n\nOther Restrictions\n\nAge Restrictions: Other mechanisms for restricting interactions have also been considered\n\nsuch as age-based restrictions which constrain interactions to only occur between\nindividuals of similar age. The age in this case refers to the total age of a search path such\nthat offspring inherit the age of their parents + 1. An example of this approach is seen in\nthe Age-Layered Population Structure (ALPS) presented by Hornby in [99]. Hornby has\nfound that age restrictions can allow for an effective utilization of new genetic material\nwhen it is introduced to an EA population.\nEnvironmental Restrictions: As individuals in an EA population continue to internalize\n\nmore and more algorithmic features that originally were defined globally, more interaction\nrestrictions become possible. One option is to create heterogeneous island models where\nindividuals are grouped based on similarity of parameter space granularity, search operator\n\n32\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\ntype, or selection pressure [100].\n\nClassification of an island's exploitive or explorative\n\ncharacter can then be used to restrict the flow of information between islands.\n\n2.3.8\n\nPerformance Metrics and Analysis\n\nIn optimization research, there have been intermittent efforts aimed at developing standards\nfor performance evaluation (e.g. see [101] and references therein). This section reviews\nseveral ways that performance can be measured in Evolutionary Algorithms.\n\nThe\n\ndiscussion is restricted to only address issues that arise within the particular experiments\nconducted in this thesis and so some context and background is necessary. A review of\nother methods for analyzing the behavior and performance of an EA is available in [102].\nThe discussion of performance metrics changes depending on whether a priori knowledge\nis available about the problem such as knowledge of optimal genotype(s) or phenotype(s).\nHere it is assumed, as is often the case for many real world optimization problems, that no\n\na priori knowledge is provided. As a consequence, it is assumed that performance can not\nbe measured by how close an algorithm gets to reaching the optimal solution, how fast it\napproaches the optimal solution, or how often it finds the optimal solution. Furthermore, it\nis assumed that only one best solution exists for a problem and this discussion neglects any\nadditional considerations of tradeoff surfaces which occur when dealing with multiple\nobjectives.\n\n2.3.8.1\n\nTime Dependency\n\nDeveloping a useful performance metric requires careful treatment of an important tradeoff\nbetween short-term and long-term performance. It is of general interest in optimization\nresearch to understand how an algorithm performs over different time scales. Along with\nclassification of the problem being searched, knowing the performance at different time\nscales also provides clues as to the other types of problems or conditions where the\nalgorithm may prove useful.\nAs a consequence, algorithm performance is often represented as a function of time or\ncomputational effort. Time is typically measured as the number of function evaluations (or\n33\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nsome multiple, e.g. generations) with the implied and generally valid assumption that\ncomputational costs not associated with objective function evaluation are negligible.\nA common mistake when presenting results is to only present performance at a single point\nin time (namely the time when the algorithm stops running). This neglects performance at\ndifferent timescales and introduces a significant bias into any conclusions drawn from these\nresults. Although a single performance measure is sometimes more satisfying to the reader,\nit does not provide a good sense of the actual usefulness of an algorithm.\n\n2.3.8.2\n\nDefining Performance\n\nFor a given instance in time, it is necessary to define a metric that is able to capture the\nsalient details of algorithm performance. If an Evolutionary Algorithm performed exactly\nthe same way every time it was run then its performance could be defined by the best\nsolution found as a function of computational effort. However, Evolutionary Algorithms\nare a stochastic search method and their performance is sensitive to initial conditions. To\nobtain a measure of expected algorithm performance requires a sampling of experimental\nreplicates to be taken with different initial conditions. Hence for a given instance in time, it\nis necessary to develop a performance metric which measures some group property of a\nsample of solution quality values taken from a set of experimental replicates.\nMost experimental results are presented by simply comparing the mean or median solution\nquality between different optimization algorithms. This is particularly useful for comparing\nresults that are stated in different publications however this is not the best way to determine\nwhich algorithm within a set of experiments has superior performance.\nMaking comparisons using a group statistic like the median does not take into consideration\nthe other properties of the sample distribution (i.e. moments). This becomes particularly\nrelevant when the distributions deviate from normality or the sample variance is large (both\ncommon occurrences in performance distributions of EA solution quality results). This\nbias is rarely (if ever) acknowledged in the presentation of EA experimental results.\nFor making statements about the superiority of one algorithm over another, a suitable\nalternative is to compare algorithms based on the ranking of solution quality values that are\n34\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\ntaken from a set of experimental runs. Taking the median ranking of an algorithm's\nsolution quality (with each run of an algorithm ranked against the other algorithm's tested)\nprovides a robust measure of how strong an algorithm performed compared to the others.\nFor even better comparisons, one can use rank-based statistical tests (e.g. the MannWhitney U Test) to determine the confidence level for stating that one algorithm is superior\nto another based on ranking. This approach was used for example in [103] and [104].\nThe experimental results in this thesis have been presented using several of the approaches\ndescribed above so that the reader can obtain a well-rounded picture of algorithm\nperformance.\n\nSome performance metrics focus on final algorithm performance while\n\nothers present performance as a function of time (e.g. with performance profiles).\nStatistical tests are also used in order to gain a sense of which algorithms are superior to\nothers and the confidence with which such statements can be made.\n\n2.3.9\n\nUses and Applications of EA\n\nAlthough Evolutionary Algorithms provide a useful tool for studying natural evolution, the\npurpose of this research, and most EC research, is towards its application in solving\noptimization problems.\n\n2.3.9.1\n\nWhen EA is used\n\nThere is no indisputable set of conditions that dictate when an Evolutionary Algorithm\nshould be applied to an optimization problem however some fairly clear guidelines can be\nestablished.\nBenefits: Evolutionary Algorithms are often effective in poorly defined problems where\n\nlittle is known about the fitness landscape. They are also effective for problems with\nsubstantial levels of noise or other sources of uncertainty [105] and for problems containing\na significant level of parameter epistasis. Evolutionary Algorithms do not require gradient\ninformation which also makes them useful for non-differentiable problems.\nEvolutionary Algorithms are effective on many problems because they allow for a global\nsearch through parameter space while exploiting basic landscape features (e.g. partial\n35\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\ndecomposition) that are present in many problems. Their stochastic nature also provides an\ninherent robustness to the search process by introducing a search bias that is expected to be\ntrue on average but does not have to be strictly true for the algorithm to perform well.\nAnother related benefit is their capacity to allow for some degree of parallel search to take\nplace. A population of solutions allows the algorithm to search in multiple directions at a\ntime and reduces its chances of becoming trapped or stuck due to the presence of local\noptima. A population of solutions can also help the algorithm deal with fitness functions\nthat have multiple objectives [106] or dynamic objectives [105].\n\nThey are also less\n\nsensitive to numerical errors compared to gradient-based and direct search methods [107].\nDrawbacks: Although there are many benefits to using an EA, some cautionary notes are\n\nalso warranted. One drawback to using an EA is that it must be tailored to fit a specific\nproblem. Although most design issues do not require expert knowledge of the fitness\nlandscape, some expert knowledge is required in order to design search operators that\nmatch well with the problem being solved. Furthermore, this aspect of EA design is crucial\nto the behavior and performance of the final algorithm. The remaining aspects of EA\ndesign largely consist of establishing the correct parameter settings. Some have claimed\nthat the non-intuitive nature of setting these parameters can make Evolutionary Algorithms\nhard to implement in practice (by non-experts).\nAnother drawback of EA is that they generally do not scale well and so are rarely used on\nproblems containing a large number of parameters (e.g. 1000+). The stochastic aspects\nwhich make EA robust also make it perform poorly if some regularity within the fitness\nlandscape can be exploited (but is neglected during EA design/hybridization efforts). Even\nthe slightest improvement in search bias can make huge differences in performance as the\nscale of a problem increases. A common source of regularity is the presence of smoothness\nat different scales in a fitness landscape. When such approximately smooth features are\npresent, a more directed search process can sometimes be much more effective. On the\nother hand, it is quite common to integrate local search mechanisms or expert knowledge\ninto an EA design which can help to address this drawback of Evolutionary Algorithms.\nSimilarly, EA has generally not been used on problems where computational resources only\nallow for a small number of function evaluations. Since EA is primarily designed to be a\nglobal search heuristic, it tends to have relatively poor performance over short time scales\n36\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\ncompared to deterministic methods. However, it is worth noting that computational costs\nare steadily becoming less of an issue when considering whether to use an EA. This is due\nto the increased availability of parallel computing resources as well as the development of\nmore efficient surrogate models, both of which are conditions that EA is particularly suited\nto exploit.\nIn summary, Evolutionary Algorithms are not a panacea for solving all complex problems\nin the world however they do provide search characteristics that are important for solving\nmany challenging problems. In general, EA should be treated as an adaptable framework\nfor solving difficult problems instead of viewing them as a collection of ready-to-use\nalgorithms [108].\n\n2.3.9.2\n\nWhere EA is used\n\nPopular application domains for Evolutionary Algorithms include data mining,\nclassification, scheduling, planning, and design. Although these are active areas of applied\nEC research, it is also worth pointing out that there is a diverse range of problems being\nsolved by Evolutionary Algorithms in both academia and industry with new applications\ncontinually surfacing.\nTo get a sense of more specific applications where Evolutionary Algorithms are used, one\ncan simply look at workshops that have taken place at international EC conferences over\nthe years.\n\nFor instance, taking a look at the Genetic and Evolutionary Computation\n\nConference (GECCO), one will find workshops have dealt with applications related to the\npetroleum industry, medical applications, mechatronic design, fault tolerance, robotic\nvision, evolvable hardware machines, circuit design, sensor evolution, damage recovery in\nrobots, modeling financial markets, structural design, and software testing.\nThe popularity of EA in a particular application domain can also indicated by the presence\nof application-specific EA surveys and reviews. A non-exhaustive list of such surveys\nincludes EA applied to computer-aided molecular design [109], job-shop scheduling [110],\nproject scheduling [1], aerospace problems [111], data mining and knowledge discovery\n[112], control systems engineering [113], [114], chemistry applications [115],\n\n37\n\n\fChapter 2: General Background of Evolutionary Algorithms\n\nmacroeconomic models [116], and the modeling and control of combustion processes\n[117].\nAmple evidence of its versatility can also be found in the hundreds of unique industrydriven applications where EA has been successfully applied. A sampling of applications\ninclude breast cancer detection [118], design of the world's fastest race car [119],\noptimizing schedules for bringing new products to market [3], designing pharmaceutical\ndrugs [120], processing MRI brain images [121], and the optimization of IntensityModulated Radiation Therapy IMRT [122].\nEvolutionary Algorithms can also be an important tool in academic research and have been\nused to build improved models of atomic force fields [123], improve interpretation of mass\nspectrometry data [124], and for the design of better molecular scale catalysts [125]. Many\nother examples can be found in the Applications of Evolutionary Computation book series\n[126].\n\n38\n\n\fChapter 3: Adaptation of EA Design\n\nChapter 3\n\nAdaptation of EA design\n\nDesigning an Evolutionary Algorithm involves a number of activities, from the\ndevelopment of an appropriate genetic encoding and/or an appropriate set of search\noperators, to establishing the correct selection pressure. For some aspects of EA design,\nthis involves the setting of a number of parameters which control aspects of EA behavior.\nParameter tuning is an important task in EA design because the optimal parameter settings\nwill vary from one problem to the next and the use of poor parameter settings can\nsignificantly impact algorithm performance.\nThis chapter focuses on ways in which robust EA search behavior can be attained by\nallowing traditionally static EA design parameters to adapt to their environment. Section\n3.1 starts by reviewing adaptive control of EA design parameters. The review focuses\nparticularly on a general framework for the adaptation of search operator usage\nprobabilities.\nSection 3.2 presents a new adaptive control procedure that is driven by empirical measures\nof an individual's impact on population dynamics. The method follows a principle of\nempirical search bias which is presented in this thesis as an alternative to the fitness driven\nsearch bias present in most optimization algorithms.\nThrough a preliminary analysis of population dynamics, an unexpected behavior is also\nuncovered in Evolutionary Algorithms. It is found that there is a surprising scarcity of\nindividuals in an EA population that cause even moderate changes to population dynamics.\nInstead, most individuals actually have a negligible impact on the system. From tests using\na number of experimental conditions, it is concluded that this is a fundamental property of\nEA dynamics such that most interactions between the EA and its environment are\neffectively neutral and non-informative.\nBased on this conclusion and with the use of statistical arguments, the adaptive system is\nmodified to filter out data from individuals with little impact on population dynamics (i.e.\n39\n\n\fChapter 3: Adaptation of EA Design\n\nneutral interactions). Experiments in Section 3.3, which are conducted on a number of\nartificial test functions and engineering design problems, indicate that the new adaptive\ncontrol method is superior to several other adaptive methods in the literature.\n\n3.1 Approaches to Adaptation: Literature Review\nThis section reviews past research on the tuning and adaptive control of EA design\nparameters. The section starts off with a justification for such research efforts and then\npresents a number of design parameters that can be adjusted in order to tune EA behavior.\nDifferent classes of parameter control techniques are also introduced, and finally, a\nframework is presented for the adaptive control of search operator usage probabilities.\n\n3.1.1\n\nImpetus for EA design adaptation research\n\nDesigning an Evolutionary Algorithm involves a number of activities, from the\ndevelopment of an appropriate genetic encoding and/or an appropriate set of search\noperators, to establishing the correct selection pressure. For some aspects of EA design,\nthis involves setting a number of parameters which control different aspects of EA\nbehavior. The number of parameters is potentially very large and each can influence the\noptimal setting of the others. Furthermore, there is no reason to assume that static EA\nparameter settings are optimal, especially considering the non-stationary environment\ncaused by the search process.\nEarly attempts at resolving this problem were focused on determining the best static\nparameter settings for an EA [83], [127]. Over time, it has become well recognized that the\nbest parameter settings depend on the problem being solved. Other attempts have focused\non so-called competent EA designs where general guidelines are used to quickly determine\ngood parameter settings.\n\nThese approaches however generally rely on a number of\n\nsimplifications and only apply to very specific algorithm designs such as the canonical\nform of the Genetic Algorithm (e.g. see [128], [129]).\nIronically, the problem of parameter setting for EA design actually defines an optimization\nproblem. However the \"optimization\" of EA design is particularly difficult because we\n40\n\n\fChapter 3: Adaptation of EA Design\n\ngenerally do not have the computational resources to test a large number of algorithm\ndesigns. With this in mind, one alternative is to carry out a systematic investigation using\nan efficient Design Of Experiments (DOE) for finding the best parameter settings. In cases\nwhere algorithm design features are not as easily parameterized, the problem becomes one\nof selecting among a set of optimization algorithms. A number of methods have been\nproposed for algorithm design automation or the hybridization of multiple search\nalgorithms. These approaches often involve some iterated learning process as seen in\n[130], [131], [132], [133].\nFor many optimization problems, very little if any computational resources can be devoted\nto parameter tuning, or more generally, to algorithm design automation, since optimizing\nthe search method would take much more time than optimizing the problem at hand. Under\nthese circumstances, one promising option is to consider ways to adapt EA design at the\nsame time it is being used to solve a problem. It is not expected that an optimal EA design\ncan be created by this approach however its potential to improve EA performance and\nreduce human design efforts makes such research of practical interest for EA practitioners.\nEvolutionary Algorithms which automatically tune one or more parameters are referred to\nin this thesis as Adaptive Evolutionary Algorithms.\n\n3.1.2\n\nAdjustable Parameters\n\nParameters can be established for many aspects of EA design which are broken down in\nthis review into two general categories that are present in most optimization algorithms.\nThe first is that of Selection which determines how search paths are added or lost in a\nsearch process. EA parameters associated with selection that have been adapted in the past\ninclude population size [134], population structure [135], [136 2007)], selection pressure\n[137], [138], [139], and penalty weights for constraint handling [59], [58].\nThe second general category is that of search operations and simply deals with how the\nalgorithm moves from one point in parameter space to another. EA parameters associated\nwith search operations include adapting crossover points [140], [141], [142], [143] adapting\nmutation step sizes [144], [145], and adapting the probability of using different search\n\n41\n\n\fChapter 3: Adaptation of EA Design\n\noperators [146], [147], [148], [149], [150], [103], [104].\n\nReviews of adaptation and\n\nparameter control in Evolutionary Algorithms are available in [151], [152], [153], [154].\nAdapting Search Operator Probabilities: Adapting search operator probabilities is an\n\naspect of EA design which has been extensively studied due to the tedious nature of tuning\nthese parameters, particularly when considering more than two search operators (ten in this\nchapter). This parameter tuning problem is also of real practical interest since most EA\ndesigns used in industry incorporate search operators that are custom made for a particular\nproblem. Deciding which of these operators to use and how often to use them presents a\ndifficult design challenge that is often dealt with by trial and error. Due to its significance,\nthis chapter will focus on applying adaptive methods for controlling search operator\nprobabilities.\n\n3.1.3\n\nEA Parameter Control Techniques\n\nMethods for adapting EA design parameters can be broken down into three general classes\nwhich are known as Deterministic, Self-Adaptive, and Supervisory (or Feedback) adaptive\ncontrol methods. Each of these methods are briefly discussed below.\n\n3.1.3.1\n\nDeterministic Methods\n\nWith deterministic parameter control methods, parameters are adjusted by an external fixed\nschedule or by an heuristic based on EA properties during runtime. Although deterministic\nmethods are included as a class of adaptive methods, there is no actual algorithm response\nto its environment and so its classification as an adaptive method is rather hard to justify.\nThe success of deterministic methods is likely to be highly problem-specific and even runspecific and the issue of defining the best deterministic method becomes a challenging\nproblem possibly rivaling that of the original optimization problem.\n\nA well-known\n\nexample of a deterministic adaptive method is the cooling schedule used in the Simulated\nAnnealing algorithm [155].\n\n42\n\n\fChapter 3: Adaptation of EA Design\n\n3.1.3.2\n\nSelf-Adaptive Methods\n\nWith self-adaptive methods, information is encoded in the individual population members\nthereby allowing adaptation to occur while the EA is running. Research into self-adaptive\nEA originated with the self-adaptive mutation rates in Evolution Strategies [42].\nSelf-adaptive methods have some characteristics which are highly favorable.\n\nMost\n\nimportantly, this approach can potentially allow for a diverse range of algorithm behaviors\nto be present within a single population. Such diversity has the potential to provide an\noverall robustness to search behavior and appears to have important similarities to what\ntakes place in natural evolution.\nOne of the main challenges with self-adaptive methods is that current EA designs tend to\nhave difficulty sustaining diversity in the population. Instead, dominant individuals tend to\nspread their genetic material throughout the population which drives population\nconvergence. This process happens quite quickly in many cases and search behaviors\nwhich exploit local landscape characteristics can exacerbate the problem. Since diversity is\na precondition for adaptation, population convergence can limit the adaptive capacity of\nself-adaptive methods.\nAlthough it not typically classified as a self-adaptive method, a similar approach that\nshould be mentioned is that of competitive evolution which was first proposed in [147]. In\nthis approach, a set of subpopulations are created and computational resources are\ndistributed based on fitness and improvement rates within the subpopulations.\n\nThe\n\nframework naturally allows for different EA design parameters in the subpopulations or\neven different optimization algorithms altogether. The difference between this and selfadaptive methods is that with competitive evolution, selection occurs on a larger scale so\nthat multiple individuals (in a subpopulation) will have identical parameter settings.\nSomewhat similar ideas have also been used in the Hierarchical distributed Genetic\nAlgorithm [156] where subpopulations are given different EA design parameters and\ncommunication between populations is restricted in an intelligent manner.\n\n43\n\n\fChapter 3: Adaptation of EA Design\n\n3.1.3.3\n\nSupervisory Methods\n\nSupervisory adaptive methods (also referred to as feedback adaptation) use measurements\ntaken of EA performance during runtime in order to adapt or control parameter settings.\nUnlike self-adaptation where the adaptive mechanism is coupled to the EA population,\nsupervisory methods work usually at a higher level than individuals and are an external\nmechanism that is uncoupled from the search space of the optimization problem.\nSupervisory adaptation is used in this thesis for EA parameter control, however no claims\nare made regarding its superiority to self-adaptive methods. On the one hand, self-adaptive\nmethods are admittedly more \"nature-inspired\", however they can also be more sensitive to\nconvergence problems and the associated loss of variability in EA populations. In the\nwords of Darwin \"without variability, nothing can be effected\" which is to say that\nadaptation is simply not possible without variation [157].\n\n3.1.4\n\nSupervisory Adaptation of search operator probabilities\n\nThis section presents a detailed framework for supervisory adaptation which is presented\nwithin the context of adapting search operator probabilities. This framework is similar to\nothers such as that presented in [158].\n\n3.1.4.1\n\nOperator Quality\n\nGiven a set of Nops search operators with probabilities Pi, i = (1,...,Nops), an adaptive\nmethod has the task of setting P in order to optimally control the usage rates of the\noperators. When an operator i is used, a reward R is returned. Since the environment is\nnon-stationary during evolution, an estimate of the expected reward for each operator is\nonly reliable over a short span of time. This is addressed by introducing the operator\nquality Q, which is defined such that past rewards influence operator quality by an extent\nthat decays exponentially with time t as defined in (3-1). The \u03b1 term in this equation\ncontrols the memory of the adaptive method where \u03b1AE1 results in no memory but\nmaximum adaptation and \u03b1AE0 results in maximum memory but no adaptation. The initial\n\n44\n\n\fChapter 3: Adaptation of EA Design\n\nvalue for Q is Q(t=0)=0. These quality values are then directly used to define search\noperator probabilities.\n\nQi (t + 1) = Qi (t ) + \u03b1 [Ri (t ) \u2212 Qi (t )]\n\n3.1.4.2\n\n(3-1)\n\nOperator Probability Setting\n\nWhen adapting P, it is necessary to place a lower bound on the probability value in order to\nprevent it from reaching zero. Allowing the probability to reach zero prevents future\nassessment of the operator which is not advised considering that the environment is nonstationary. To address this, a lower bound PMin is used to define the minimum probability\nof operator usage. Since only one operator is used at a time, the sum of all probabilities\nmust equal one meaning an upper bound PMax can also be defined as PMax = 1-NOps*PMin. A\nstandard approach for setting search operator probabilities is the Probability Matching\n\nStrategy defined in (3-2) which sets probability values to be proportional to operator\nquality.\n\nFor all experimental conditions where search operator probability values are\n\nadapted in this thesis, it is assumed that all probabilities are initialized as Pi(t=0) = 1/Nops.\nProbability Matching\n\nQ (t )\nPi (t + 1) = PMin + (1 \u2212 N Ops \u22c5 PMin ) N Ops i\n\n(3-2)\n\n\u2211 Q (t )\nj =1\n\nj\n\nAn alternative to the probability matching strategy is the Adaptive Pursuit Strategy\nproposed in [158]. This method was developed based on a perceived weakness in the\nprobability matching strategy due to its sensitivity to Q scaling. As a simple example of\nthis, Thierens considers a case of two search operators where each operator has a stationary\nreward value. He demonstrates that, as the difference between operator rewards becomes\nsmall, the difference between search operator probabilities also becomes small. In his\nargument, Thierens suggests that if one of the operators is superior, it should be strongly\nfavored, even if the extent of its superiority is small. The adaptive pursuit strategy defined\nin (3-3) provides a straightforward method for allowing the distinction between search\noperators to be maximized. The extent that the best operator dominates the search process\n\n45\n\n\fChapter 3: Adaptation of EA Design\n\nis controlled in equation (3-3) through the parameter PMax while the rate at which\nprobability values change is controlled by \u03b2. In short, the adaptive pursuit strategy allows\nthe best operator to reach a set maximum value regardless of its relative performance\ncompared to other operators.\nAdaptive Pursuit\nPi (t + 1) = Pi (t ) + \u03b2 (PSet \u2212 Pi (t ))\n\n(3-3)\n\n(\n\n)\n\nif Qi (t ) = Max Q1 (t ),...,Q N Ops (t )\n\u23a7P\nPSet = \u23a8 Max\n\u23a9 PMin else\n\nImplicit in the rationale for the adaptive pursuit strategy is an assumption that no significant\ninteraction exists between search operators and their impact on algorithm performance (i.e.\nno epistatic interaction). However, recent studies in [55] and [56] have suggested that\nsignificant beneficial interactions do take place between search operators meaning that\nsome operator combinations are superior to any single operator used in isolation. As a\nresult, this puts into doubt whether our goal should be to overwhelmingly favor a single\nbest search operator as is intended with the adaptive pursuit strategy.\n\n3.1.4.3\n\nDefining Operator Rewards\n\nIn the description of supervisory adaptation presented thus far, the reward R is simply the\nresult of an operator's interaction with the environment. However the adaptive framework\npresented in this thesis draws a distinction between interactions with the environment and\nthe interpretation of those interactions as is shown graphically in Figure 3-1.\n\n46\n\n\fChapter 3: Adaptation of EA Design\n\nInteraction\nInteraction\nEvent\n\nMeasurement\nFeedback\n\nChangeable\nComponents\nA\nB\nC\n\nFigure 3-1: Framework for a supervisory adaptive system. Here, an adaptive system receives measurement data\nas a result of its interactions with the environment. These measurements are then interpreted or assessed for\nrelevance. Once interpreted, the data is then allowed to drive internal changes to the system. The mechanics for\ninternal change are not shown in the figure but would consist of mechanisms such as the Quality function and the\nProbability Matching Strategy.\n\nInteraction events between an operator and its environment (i.e. offspring creation) are\nreferred to simply as events and measurements of an event are labeled as F. To give\nmeaning to an event, it must be interpreted within a particular context.\n\nUsing this\n\ndistinction between event measurements F and their interpretation I, the reward for an\noperator i at time t can be defined in (3-4) as the average interpretation of a set of Mi\ninteractions with the environment. The Iarchive term in the equation simply stores the I\ncalculations from each operator.\n\nRi (t ) =\n\n1\nMi\n\nMi\n\n\u2211I\nj =1\n\narchive\n\n(i, j )\n\n(3-4)\n\nIt is important to note that the reward at a particular time t has now been redefined so that it\nrepresents multiple interactions between the adaptive system and its environment. Using\nthis particular setup, each increment of time t is now referred to as an adaptation cycle. The\nnumber of interactions that take place during an adaptation cycle is controlled by the\nadaptation cycle length \u03c4 which is the number of generations before operator probabilities\nare recalculated.\n\nThe meaning of each of these terms within the overall adaptive\n\nframework is demonstrated in the pseudocode below.\n\n47\n\n\fChapter 3: Adaptation of EA Design\n\nDo\n'add standard genetic operators here\nGen= Gen+1\nFor each offspring\ni = offspring's search operator\nCalculate F (defined in Section 3.1.4.4)\nCalculate I (defined in Section 3.1.4.5)\nMi = M i + 1\nIarchive(i, Mi)= I\nNext offspring\nIf Gen mod \u03c4 = 0 THEN\nt=t+1\nFor each operator i\nCalculate R (defined in Section 3.1.4.3)\nCalculate Q (defined in Section 3.1.4.1)\nCalculate P (defined in Section 3.1.4.2)\nMi = 0\nNext i\nEnd If\nLoop until termination Criteria\n\nThe only calculation steps in the pseudocode above that have not yet been presented are for\n\nF and I. The next section briefly discusses possibilities for event measurements F while\nSection 3.1.4.5 describes interpretation methods I that have been used in the literature.\n\n3.1.4.4\n\nEvent Measurement\n\nMeasurements of search operator events typically consist of the fitness of the offspring that\nwas created in an event. Fitness is generally based on the objective function value although\nfitness measurement can also take into account the feasibility of the offspring. Somewhat\nuncommon fitness measures based on the objective function are provided in [104].\nSince practically all previous studies have used offspring fitness as the measurement of\nchoice, event measurement F is assumed to be equivalent to the offspring fitness\nthroughout this thesis. Section 3.2 is devoted to a new type of event measurement which is\nnot based on standard measures of fitness.\n\n48\n\n\fChapter 3: Adaptation of EA Design\n\n3.1.4.5\n\nInterpretation\n\nThe interpretation of search operator events involves defining an event measurement F\nwithin a given context. As previously mentioned, this is then used to calculate the operator\nreward R as defined in (3-4). It should also be mentioned that the interpretations methods,\nas described below, assume the problem being solved is a Maximization problem.\n\n3.1.4.5.1 Parent Context\nOne common approach to interpreting a fitness measurement is to do so within the context\nof its parents. For instance, in I1 the interpretation of the offspring measurement simply\nindicates (with a binary variable) whether the offspring was superior to one of its parents.\nHere it is assumed that the best parent is always chosen for comparison. Another option is\nto measure the magnitude of improvement between an offspring and its parent as seen in I2.\n\n\u23a7 1 if FOffspring > FParent\nI1 = \u23a8\n\u23a90 else\n\n(3-5)\n\nI 2 = FOffspring \u2212 FParent\n\n(3-6)\n\nI 3 = Max(0, FOffspring \u2212 FParent )\n\n(3-7)\n\nThe interpretation I3 equals I2 if I2 >0 but otherwise is set to 0. The interpretation I3 was\nfirst defined in [159] and is described in [160], as a combination of the probability of\nimprovement and the expectation of improvement.\n\n3.1.4.5.2 Population Context\nIt is also common to consider a measurement within the context of an individual's\npopulation. This is seen for example in I4, which is an interpretation similar to I1 except the\ncontext being considered is the median population fitness FMedian instead of the parent\nfitness.\n\n\u23a7 1 if FOffspring > FMedian\nI4 = \u23a8\n\u23a90 else\n\n(3-8)\n\n49\n\n\fChapter 3: Adaptation of EA Design\n\nAlso similar to I2, interpretation I5 considers the size of measurement improvement\ncompared to the best individual in the population FBest.\n\nHowever I5 also scales the\n\ninterpretation to take into account the distribution of measurements. This measurement was\nfirst introduced in [161].\n\nI5 =\n\n(F\n\nOffspring\n\n\u2212 FBest )\n\n(3-9)\n\n(FBest \u2212 FMedian )\n\nInterpretations similar to I3 have also been considered within a population context. For\nexample, in [162], they created I6 which is the same as I3 except that FParent is replaced with\n\nFBest. Also, in [163] they created I7 which is the same as I3 except that FParent is replaced\nwith the F measurement in the population that represents the 90th percentile F90th (i.e. F90th\nis the F value that is greater than 90% of other F values in the EA population).\n\nI 6 = Max(0, FOffspring \u2212 FBest )\n\n(3-10)\n\nI 7 = Max(0, FOffspring \u2212 F90th )\n\n(3-11)\n\nFinally, another common interpretation is to simply rank an offspring's fitness F within the\nEA population of size N as seen in I8.\nN\n\nI 8 = \u2211 \u03c6 (Offspring , i )\n\n(3-12)\n\ni =1\n\n\u23a7 1 if FOffspring > Fi\n\u23a90 else\n\n\u03c6 (Offspring , i ) = \u23a8\n\nA number of past studies on the adaptation of search operator probabilities can be defined\nusing the adaptive framework that has been laid out in this chapter. Several of these are\npresented in Table 3-1.\n\n50\n\n\fChapter 3: Adaptation of EA Design\n\nTable 3-1 Partial list of methods that have been used to adapt search operator probabilities.\nParameters that are not specified in the method are listed as (-), parameters that are not applicable are\nlisted as (*) , and parameters that were varied in experiments are listed as the range of values tested.\nParameter \u03b1 in column two is the memory parameter given in (3-1), \u03b2 in column three is a parameter\nspecific to the adaptive pursuit strategy and is defined by (3-3), \u03c4 in column four is the adaptation cycle\nlength which defines the number of generations between the updating of search operator probabilities,\nPMin in column five is the lower bound on the allowed range of probability values, the \"Event\nMeasurement\" in column six is described in Section 3.1.4.4, the \"Interpretation\" in column seven is the\ninterpretation of event measurements as described in Section 3.1.4.5, and Nops in column eight is the\nnumber of search operators being adapted. The ETV event measurement and Outlier interpretation in\nthe bottom two rows of the table are new event measurement and interpretation methods (resp.)\nproposed in this thesis and are described in Sections 3.2.2 and 3.2.4. \"His. Credit\" refers to Historical\nCredit Assignment which is described below in Section 3.1.4.6.\n\n3.1.4.6\n\nReference\n\n\u03b1\n\n\u03b2\n\n\u03c4\n\nPMin\n\n[158]\n[164]\n[160], [149]\n[162]\n[165]\n[163]\n[104]\n[103]\n\n0.8\n*\n0.3\n0.001-0.5\n*\n0.001-0.1\n0.5\n0.5\n\n0.8\n*\n*\n*\n*\n*\n*\n*\n\n1\n1\n4\n1\n100/N\n1\n20\n20\n\n0.1\n\nEvent\nMeasurement\n-\n\n0.1\n0\n0.01\n0.02\n0.02\n\nF\nF\nHis. Credit\nF, His. Credit\nF\nF, ETV\n\nInterpretation Nops\nHis. Credit\nI3\nI6\nI4\nI6, I7\nI8, Outlier\nI8, Outlier\n\n5\n2\n3\n5\n10\n10\n\nOther Approaches\n\nHistorical Credit Assignment: An interesting alternative for defining operator rewards\n\nbased on a principle of historical credit assignment is presented in [146] and also used in\n[164], [165]. In this adaptive method, each individual stores a search operator tree as\nshown in Figure 3-2. The operator tree records which search operators were used to create\nthe ancestors of each individual. When a search operator event occurs, credit for the event\nis assigned backward to all search operators in the operator tree with the initial credit\ndefined by I4. To account for the diminished importance of past events, the actual credit a\nsearch operator receives is adjusted to be \u03b3L * I4 where L is the path length between the\ncurrent event and the search operator receiving credit in the operator tree. The parameter \u03b3\ncontrols how quickly credit decays with distance in the operator tree.\n\n51\n\n\fChapter 3: Adaptation of EA Design\n\nFigure 3-2 Operator tree for an individual where only crossover (Cr) and mutation (Mu) events occur. The root\nnode in the tree, Cr, is the search operator that was used to generate the individual that is storing the operator tree.\n\nAlthough this approach does not fit precisely within the \"event measurement/\ninterpretation\" adaptive framework used in this thesis, it represents the only method where\nevents are measured based on the success of future offspring making this method quite\nunique. Interestingly enough, this method has a number of similarities to the new adaptive\nmethods presented in the next section. It is important to emphasize however that the\nadaptive methods derived in this chapter were developed independently from the approach\npresented in [146].\n\nAlso, despite the similarities, there are a number of important\n\ndifferences in the approaches as will be seen as the new methods are introduced.\n\n3.2 Measuring Population Dynamics for Adaptive Control\nThe following section presents a new type of event measurement that is notably distinct\nfrom the standard objective function value.\n\nSection 3.2.1 begins by clarifying why\n\nobjective function values are a common form of event measurement used for driving\nparameter adaptation (and more generally used for guiding an optimization search process).\nSection 3.2.1 also provides some alternatives to fitness-based search including a newly\nproposed concept called Empirical Search Bias. The new event measurement is presented\nas an example of Empirical Search Bias in Section 3.2.2. Based on an analysis of the new\nmeasurement in Section 3.2.3, a new interpretation method is also developed which is\npresented in Section 3.2.4.\n\nThe concepts of event measurement and measurement\n\ninterpretation follow from the review in the last section and the framework outlined in\nSection 3.1.4.\n\n52\n\n\fChapter 3: Adaptation of EA Design\n\n3.2.1\n\nWhy is Objective Function a Standard measure for fitness?\n\nAs previously mentioned, adaptive methods typically use a measure of fitness based on an\nindividual's objective function value in order to assess the merits of different search\nbehaviors. To understand why objective function values are universally used as indicators\nof fitness requires an understanding of the assumptions implicit in any search process. The\nmost important of these assumptions is referred to in this thesis as the Hill-Climbing\n\nAssumption.\n\n3.2.1.1\n\nThe Hill-Climbing Assumption\n\nIn order to search for an optimal solution to a problem, it is necessary to make assumptions\nabout the fitness landscape of the problem being solved. One of the most common and\nsuccessfully applied assumptions is that a solution's objective function value (Fitness) can\napproximate a solution's usefulness in searching for more fit solutions. Following this to\nits logical conclusion, this implies highly fit solutions will ultimately be useful in finding\nthe optimal solution.6 This also implies that a solution's reproductive worth (i.e. usefulness\nas a point to search from) and the solution objective function value are roughly equivalent\nmeasures. For unimodal landscapes, this assumption is often sufficient for guaranteeing an\noptimal solution will be found consistently and in a reasonable amount of time. However,\nfor multimodal landscapes, the Hill-Climbing Assumption can fail to produce reliable or\nacceptable results.7\n\n3.2.1.2\n\nSearch Bias Assumption\n\nAn alternative approach is to look at solving the inverse problem which is that of\noptimizing search bias. For clarity, this will be called optimization based on the Search\n\nBias Assumption. Here the goal is to find solutions and search mechanisms that are most\n\n6\n\nSimilar arguments can also be applied to the 1st and 2nd derivatives of the objective function.\n\n7\n\nIn EA, most selection schemes involve relaxation of the Hill-Climbing Assumption. This involves treating\nthe Hill-Climbing Assumption as being true in the average sense but not strictly true (i.e. a probability of it\nbeing true).\n\n53\n\n\fChapter 3: Adaptation of EA Design\n\nlikely to reach the optimal solution reliably and in a small number of steps. Instead of\nassigning credit to a highly fit solution, we look to assign credit to solutions that participate\nin finding high fitness solutions. The underlying assumption made here is that a solution\nthat was helpful in finding good solutions has a chance of being helpful in finding even\nbetter solutions. Furthermore, we treat this assumption as if it can be successfully applied\nthroughout the fitness landscape all the way up to the globally optimal solution. One\nobvious result of this approach is that a distinction is drawn between the reproductive value\nand the objective function value of a solution which makes this markedly distinct from\noptimization under the Hill-Climbing Assumption. The Search Bias Assumption is difficult\nto implement in practice although some options for doing so are proposed in [103].\n\n3.2.1.3\n\nEmpirical Bias\n\nThe Hill-Climbing Assumption and the Search Bias Assumption represent opposite ends of\na spectrum of possible bias for driving an optimization search process. On the one hand,\nthe Hill-Climbing Assumption relies solely on the current states of the system in order to\nguide future search behaviors while the Search Bias Assumption places emphasis entirely\non the initial conditions. In between is a realm where an intermediate reliance on history or\nsearch experience occurs (i.e. where history/experience partially guides the search process).\nThis third option is referred to in this thesis as Empirical Bias and a proposal for\nimplementing Empirical Bias is provided in the following sections.\nFor the implementation considered, the core of the EA design remains unchanged so that\nthe overall search process is still driven by the Hill-Climbing Assumption. However,\nsearch operator usage rates are driven by empirical evidence of offspring importance as\nopposed to being driven by the fitness of these offspring.\nFor a population-based optimization algorithm like EA, an empirical measure of the\nimportance of an individual can be obtained by measuring the individual's impact on\npopulation dynamics. Looking at population dynamics on a small timescale such as a\nsingle generation, an individual will only impact the population through competition for\nsurvival and/or competition to reproduce. However, if longer timescales are considered, we\nwill find an individual's impact is largely a result of the survival and spread of its offspring.\n\n54\n\n\fChapter 3: Adaptation of EA Design\n\nThe next section describes a procedure for measuring an individual's impact on population\ndynamics. This new event measurement, combined with a new interpretation method will\nbe used to adapt search operator probabilities in the experimental work in this chapter.\n\n3.2.2\n\nMeasuring Impact on Population Dynamics: The Event\n\nTakeover Value (ETV)\nThis section describes the Event Takeover Value (ETV) which is used for measuring an\nindividual's impact on population dynamics. Throughout the discussion, the term event is\nused as before to describe the creation of a new individual. To help understand ETV,\nFigure 3-3 shows a directed graph which represents the family tree of an individual's\nlineage. Here different generations are indicated by positioning on the horizontal axis,\nnodes represent individuals created in a particular generation and the parents and offspring\nof an individual are indicated by connections to the left and right (resp.). Starting at the\nroot node on the far left of Figure 3-3, one can observe how this individual's genetic\nmaterial is able to spread through the population. At each generation, it is possible to count\nthe number of individuals in the population that are historically linked to the root node.\nThis can be thought of as an instantaneous measure of the individual's impact on\npopulation dynamics and is referred to as ETVgen. A more detailed description of ETVgen is\nprovided in the caption of Figure 3-3.\n\n55\n\n\fChapter 3: Adaptation of EA Design\n\nETVgen = 1\n\n2\n\n3\n\n5\n\n6\n\n6\n\n7\n\n3\n\n3\n\n1\n\n2\n\n3\n4\n5\nGeneration\n\n6\n\n7\n\n8\n\nEvent\nMeasured\n\n0\n\nFigure 3-3: Visualizing an individual's impact on population dynamics using genealogical graphs. An individual's\nimpact for a given generation (horizontal axis) is defined as the number of paths leading from the measured node\nto the current generation. This is referred to as ETVgen and can be calculated for the \"Event Measured\" in the\ngraph above by counting the number of nodes on the dotted vertical line for a given generation. As the population\nmoves from one generation to the next, one can see that the number of individuals in the population that are\ndescendants of the \"Event Measured\" will change with each new generation. In other words, the ETVgen value is\ndynamic. To clarify this point, ETVgen values are calculated for the \"Event Measured\" and are shown at the top of\nthe graph. The maximum impact an event has on the population is the maximum ETVgen value that is observed.\nThis graphical illustration assumes a generational population updating strategy such that an individual exists in a\nsingle generation only. This is done to simplify the illustration however other updating strategies could be used in\nwhich case some nodes would be stretched across multiple generations in the graph.\n\nObserving Figure 3-3, it appears that a reasonable calculation of an individual's impact on\npopulation dynamics would be to count the total number of descendants for a given\nindividual.\n\nThis is equivalent to summing up ETVgen for all generations where the\n\nindividual's lineage remains alive.\n\nThe problem with this measurement is that an\n\nindividual's lineage occasionally is able to spread throughout the entire population so that\nthe cumulative ETVgen value increases indefinitely. A useful alternative which is used in\nthis thesis is to define ETV as the largest ETVgen value observed. This value naturally has\nan upper bound equal to the population size of the system. For the example given in Figure\n3-3, the ETV value for the \"Event Measured\" would be ETV=7, which occurs in the sixth\ngeneration.\n\n56\n\n\fChapter 3: Adaptation of EA Design\n\n3.2.2.1\n\nMultiple Parents and Genetic Dominance\n\nFigure 3-3 shows how an individual can impact population dynamics through the spread of\nits genetic material, however it does not consider the fact that offspring are often created\nfrom multiple parents. Also, when using multi-parent search operations, offspring tend to\nbe genetically biased to be more similar to one parent than the other(s). An accurate\nmeasure of ETV should therefore account for the possibility of multiple parents as well as\naccount for the possibility of dominance by one of the parents.\nAs an alternative to assigning a weighted importance to each of the parents, a dominant\nparent is chosen instead so that (for ETV calculation purposes) the offspring is seen as\nhaving only a single (dominant) parent. By using dominance, it is no longer necessary to\naddress the issue of distributing credit among multiple parents meaning that Figure 3-3 is\nstill a valid representation of the ETV measurement process. This also helps to simplify\nimplementation of the ETV calculation steps as seen later.\nSeveral ways for selecting the dominant parent have been tested including random\nselection, phenotypic similarity, and genotypic similarity between parents and offspring. In\npreliminary studies (results not shown), random selection resulted in mediocre EA\nperformance as well as poor differentiation between search operator probabilities.\nSelecting the parent that was most genetically similar (by Normalized Euclidean Distance)\nto the offspring worked well while selecting the parent that was least genetically similar\nperformed even more poorly than random selection.\n\nNo significant difference in\n\nperformance was observed between using genetic similarity and phenotypic similarity. In\norder to maintain consistency with the ETV measurement definition, genetic dominance is\nused in ETV calculations.\n\n3.2.2.2\n\nHitchhiking\n\nThus far, the ETV measurement implicitly assumes that an individual's impact on future\ndynamics does not degrade with the passage of time. However the stochastic nature of an\nEA makes this time dependency true and unavoidable.\n\nAddressing time dependence in\n\ncredit assignment has previously been done using exponential decay functions in [146],\n\n57\n\n\fChapter 3: Adaptation of EA Design\n\n[164], [165], and [103]. Another possible approach is to set a time window beyond which\nan individual's impact on population dynamics can no longer be measured.\nThrough careful study of the genealogical branching process, it has been found that certain\nbranching structures can indicate exactly when confidence in the ETVgen measurement is\nlost. An example of these conditions is shown in Figure 3-4. Looking at the ancestors (i.e.\nnodes to the left) of the white node, it is noticed that all ancestors have the same ETVgen\nvalue and that this value is obtained solely due to their historical linkage to an important\nfuture event. Obtaining credit in this fashion is referred to in this thesis as Genetic\n\nHitchhiking.\nThis phenomenon actually happens quite often. If an important event occurs, it will likely\nspread quickly throughout much of the population.\n\nHowever, all events prior to the\n\nimportant event also spread because they are historically linked. Care must be taken then to\nmake sure an event has spread due to its own importance and not the importance of some\nlater event. To account for this, ETVgen measurements of hitchhikers are disregarded.\nCurrent Generation\n\nFigure 3-4: Genetic Hitchhiking in EA population dynamics. Considering ETVgen measurements based on the\ncurrent generation, one can easily see that all nodes to the left of the white node will have the same ETVgen value\n(i.e. they all have the same number of paths leading to the current population). However, these nodes are assigned\ntheir ETVgen values only because of a single important descendant (the white node). These linear structures in the\ngenealogical branching process are a sign of genetic hitchhiking and can be seen in several different places in the\ngraph above (seven genetic hitchhiking occurrences in total).\n\n58\n\n\fChapter 3: Adaptation of EA Design\n\n3.2.2.3\n\nETV Calculation Procedure\n\nTo calculate ETV, a procedure is needed for recording genealogical information. The first\nstep is to assign an ID to each event that uniquely identifies the offspring and indicates\nwhich search operator created it. Historical information in the form of these ID values is\nstored in each individual as an ordered list which represents the direct line of ancestry for\nthat individual. An example of these ordered lists and their meaning within a genealogical\ntree is provided in Figure 3-5. When a new offspring is created, it inherits the historical\nrecords of the genetically dominant parent, and a new ID (representing the offspring) is\nadded to the offspring's historical record.\nParent 1\n\nParent 2\n\nHistory\n\nHistory\n\nP40\n\nP40\n\nP33\n\nP36\n\nP26\n\nP31\n\nP21\n\nP28\n\nP36\n\nP31\n\nP28\nP23\n\nP23\n\nGenetic\nDominance\n\nP40\n\nOffspring\nHistory\nP40\n\nP33\n\nP33\nP26\nP21\n\nP26\nP21\n\nNew ID\n\nNew ID\n\nFigure 3-5: Transfer of Historical Data. Each individual holds historical information in addition to genetic\ninformation. The historical information represents the direct line of ancestry for an individual. Examples of\nhistorical data lists are shown above for Parent 1 (ID=P21) and Parent 2 (ID=P23) and their meaning is\ndemonstrated by the genealogical graph on the right. A new offspring only takes historical information from the\nparent that is genetically most similar (i.e. genetically dominant). In this example, Parent 1 is assumed to be the\ngenetically dominant parent. In addition, the offspring creates a new ID to indicate its placement in the\ngenealogical tree.\n\nBy going over the historical records that are stored in the individuals in the current\npopulation and counting the number of times that the ID of an event is observed, the ETVgen\nfor that event (and that generation) can be calculated. Given a maximum size Tobs for the\nhistorical records list, an EA population size N, and individual population members M, the\n\nETVgen measurement for event \"ID\" can be calculated using (3-13).\n\n59\n\n\fChapter 3: Adaptation of EA Design\n\nN TObs\n\nETV gen ( ID) = \u2211\u2211 \u03c6i , j\n\n(3-13)\n\ni =1 j =1\n\n\u23a7 1 if ID = M i ( ID j )\n\u23a90 else\n\n\u03c6i , j = \u23a8\n\nTo check for genetic hitchhiking, the ETVgen value of an event must be compared with one\nof its offspring. If they are equal then the parent's ETVgen value is set to zero. Given two\nevents ID1 and ID2, genetic hitchhiking can be defined by (3-14).\nIF (ID1 = M i (ID j )) AND (ID2 = M i (ID j \u22121 ))\n\nAND (ETV gen ( ID1 ) = ETV gen ( ID2 ) ) THEN ETV gen ( ID1 ) = 0\n\n(3-14)\n\nThe final step in the ETV calculation is to compare each ETVgen value with the archived\nETV value. If ETVgen is larger than the archived ETV, then the ETV value is updated,\notherwise the old value is retained. The ETV calculation for an event is completed when an\nevent's ETVgen is found to be zero (i.e. a hitchhiker event).\n\n3.2.2.4\n\nComputational Costs of ETV Calculation\n\nThe computational costs of the ETV calculations are reasonably small if properly\nimplemented. These costs come primarily from i) the size of the \"historical list\" in each\nindividual and ii) from the number of events that are being calculated at each generation\n(i.e. the size of the ETV archive).\nHistorical List Size: The size of the historical list Tobs establishes the maximum number of\n\nancestor events that can be stored in each individual in the population. If Tobs is too small,\nit effectively reduces the amount of time that an ETV can be measured however if Tobs is\ntoo large, it will negatively impact the computational costs of the procedure. The first step\nto improve computational efficiency was to determine how large Tobs must be in order to\ncalculate ETV. This was accomplished by running experiments with EA designs varying\nby population size N (N=30 to N=400), selection pressures (binary tournament selection\nand random selection), and population updating (generational and steady state) on a random\nsampling of test functions taken from Table 3-2. Tobs was set to 100, which is large enough\nto ensure ETV calculations are almost always finalized. For each event, the smallest Tobs\nwas recorded that would have allowed for the ETV to be calculated. Looking at the\n60\n\n\fChapter 3: Adaptation of EA Design\n\ncumulative distribution of these values, it was found that 99.9% (+0.04) of all ETV\ncalculations complete within Tobs=20. The completion time did not appear to be sensitive\nto any of the conditions varied in these experiments. For the 0.1% of ETV calculations that\ndo not complete with Tobs=20, the measured ETV is expected to be an undervaluation for\nthese events. However, from these tests it is clear that the required size of the historical\nlists is quite small (Tobs=20) and has little sensitivity to test conditions (including the\npopulation size).\nComputational Costs from ETV Archive: At each generation, an event's ETV must be\n\nchecked for a larger value and checked for evidence that the ETV calculation has\ncompleted. As a result, the computational costs will depend on the number of ETV that are\nactively being calculated at a single point in time.\nIn order to determine the computational cost from ETV, it was necessary to look at the\naverage size of the ETV archive8 as a function of time and as a function of population size.\nFirst looking at the time dependency, it was found that the ETV archive size always\nconverges to a stable value. Focusing on these stable values, the ETV archive size was\nfound to equal 11.4N with N being the population size.9\nAlso, to help understand ETV computational costs, experiments were run to determine the\naverage number of generations needed to complete an ETV calculation. This was found to\nbe 4.0 with no sensitivity to population size. From these tests, it was concluded that ETV\ncomputational costs scale linearly with population size.\n\n3.2.2.5\n\nRelated Research\n\nRecently, genealogical graphs have also been used to help understand the dynamics in\nArtificial Life systems [166]. However, to the author's knowledge, no previous work on\ngenealogical graphs has addressed the issue of genetic dominance in multi-parent\n\n8\n\nThe archive holds the ETV value and ID number for all events held in the historical lists of the population\n\n9\n\nRelationship between ETV archive size and N determined by linear regression (R2 = 0.999) with five tests\nconducted over the range 20 < N < 400.\n\n61\n\n\fChapter 3: Adaptation of EA Design\n\nreproduction nor has anyone previously used the concept of genetic hitchhiking when\nassessing the impact of an event in population dynamics.\n\n3.2.3\n\n3.2.3.1\n\nETV Analysis\n\nFitness as a predictor of ETV\n\nConsidering that ETV measures an individual's impact on population dynamics, it would\nbe useful to assess whether an individual's objective function is an accurate predictor of\nETV.\nExperimental Setup: To test this, EA runs were conducted where fitness-based ranking\n\nand ETV values are calculated and stored for each individual. To ensure the results were\nnot sensitive to experimental conditions, it was necessary to test a variety of EA designs on\na variety of test functions. A large number of ad hoc experiments have been conducted\nwith EA designs varying by selection pressure, population size, and the number of search\noperators. Few noticeable distinctions were observed in these preliminary tests and so the\nresults are only presented for the EA design described in Figure 3-6. Several test functions\nhave also been considered in preliminary tests with results suggesting the relationship\nbetween ETV and fitness is sensitive to the fitness landscape, however this sensitively is\nrather low.\nRelationship between ETV and Rank: As expected, the results shown in Figure 3-6\n\nindicate that almost all individuals with a large impact on population dynamics (i.e. large\nETV) are caused by individuals of high rank. However, while a trend exists between larger\nranks and larger ETV, this does not mean that low ranking individuals never have a strong\nimpact on population dynamics. Evidence for this is provided in the box plots in Figure 3-6\nwhere one can see that ETV measurements rarely ever reach ETV > 5 and yet a number of\nlow ranking individuals were able to obtain much larger ETV values.\n\n62\n\n\fChapter 3: Adaptation of EA Design\n\n50\n\n40\n\nETV\n\n30\n\n20\n\n10\n\n0\n10\n\n20\n\n30\nRank\n\n40\n\n50\n\nFigure 3-6: Box plots of the size of an individual's impact on population dynamics (ETV) as a function of the\nindividual's rank within the population where a rank of 1 represents the worst individual and a rank of N\nrepresents the best individual (based on objective function value). The data set was generated from a series of\nexperiments involving a number of test functions listed in Appendix A. The EA used to generate the results was a\nreal-coded, pseudo steady state EA design using binary tournament selection (without replacement) and a\npopulation size of N = 50. Results shown are a random sample of 5000 data points taken from a data set of\n300,000. The box plots have the standard meaning with the bottom line in the box representing the first quartile,\nthe middle line representing the median, and the upper line representing the third quartile. The symbol \u2733 is used\nto represent outlier data points.\n\nSensitivity to fitness landscape: To demonstrate how little the ETV-Rank relationship\n\nwas sensitive to the fitness landscape, Figure 3-7 shows data from a simple unimodal test\nfunction and a highly deceptive test function. Here it can be seen that the relationship\nbetween ETV and rank is very similar for these two starkly different fitness landscapes.\nThe only important difference comes in the location of extreme ETV outliers (e.g.\nETV>40). For the deceptive problem (MMDP), the extreme ETV outliers are occasionally\nfound in lower ranking individuals while for the simple unimodal test function (Quadratic\nFunction), the extreme ETV outliers are more tightly associated with the highest ranked\nindividuals.\n\n63\n\n\fChapter 3: Adaptation of EA Design\n\nMMDP\n50\n\n40\n\nETV\n\n30\n\n20\n\n10\n\n0\n10\n\n20\n\n30\nRank\n\n40\n\n50\n\n40\n\n50\n\nQuadratic Function\n50\n\n40\n\nETV\n\n30\n\n20\n\n10\n\n0\n10\n\n20\n\n30\nRank\n\nFigure 3-7 Box plots of the size of an individual's impact on population dynamics (ETV) as a function of the\nindividual's ranking within the population. Top: Results from running an EA on the Massively Multimodal\nDeceptive Problem (MMDP). Bottom: Results from running an EA on the Quadratic Test Function. Both test\nfunctions are defined in Appendix A. The EA used to generate the results was a real-coded, pseudo steady state\nEA design using binary tournament selection (without replacement) and a population size of N = 50. Results\nshown for each graph are a random sample of 5000 data points taken from a data set of approximately 15,000.\nThe box plots have the standard meaning with the bottom line in the box representing the first quartile, the middle\nline representing the median, and the upper line representing the third quartile. The symbol \u2733 is used to represent\noutlier data points.\n\nIn summary, fitness-based ranking does provide some small indication of an individual's\nchances for impacting future dynamics, however its overall ability to predict future\n\n64\n\n\fChapter 3: Adaptation of EA Design\n\nbehavior is marginal. In general, fitness rankings have a strong tendency to overvalue the\nactual importance of individuals in future population dynamics.\n\n3.2.3.2\n\nETV Distribution\n\nThe results from Figure 3-6 indicate that regardless of rank, very few individuals have a\nlarge impact on population dynamics.\n\nNotice that even for the 20% highest ranked\n\nindividuals (i.e. Box plot with the label Rank=50), the median ETV value is approximately\nfive which is only 10% of the maximum ETV value. In order to better understand EA\npopulation dynamics, the distribution of ETV measurements is provided in Figure 3-8. The\nlinearity of the ETV distribution on the log-log plot indicates the distribution fits a power\nlaw.\n\n1\n\nP(ETV)\n\n0.01\n\n0.0001\n\n0.000001\n1\n\n10\n\n100\n\n1000\n\nETV\n\nFigure 3-8 ETV probability distribution from running an EA for 20,000 generations on the 30-D Hyper Ellipsoid\ntest function. The EA design has a population size N=200, steady state population updating, and uses truncation\nselection. The solid line represents a power law with exponent 2.2.\n\nImportance of ETV Distribution: The existence of a power law ETV distribution (with\n\nexponent ~ 2) indicates that the large majority of individuals play a negligible role in\ninfluencing population dynamics whereas a vanishingly small number of individuals\ndominate population dynamics. This also indicates that most of the search is characterized\nby actions of questionable importance but is punctuated by the infrequent occurrence of\n\n65\n\n\fChapter 3: Adaptation of EA Design\n\nimportant new discoveries. The power law ETV distribution has been confirmed for a\nbroad range of EA designs which is presented and discussed in Chapter 4.\nThis result also has significant implications for how to best interpret the interactions\nbetween an adaptive system and its environment. For instance, consider the adaptive\nsystem that is used in this chapter, namely the adaptation of search operator probabilities.\nIt is now known that no matter how good a search operator is, most of the times it is used, it\nwill have little impact on population dynamics. This is not viewed as an indicator of poor\noperator performance but instead is understood as a reflection of the fundamental dynamics\nof the system. In other words, the majority of small impact interactions between a search\noperator and its environment are viewed as being not informative (effectively neutral\ninteractions) whereas the small number of interactions which do have a large impact on\npopulation dynamics are viewed as being very informative and should be treated as\nvaluable indicators of performance. The next section proposes a way to take into account\nthese findings.\n\n3.2.4\n\nInterpreting ETV measurements\n\nBased on the conclusions from the last section, the goal of this section is to interpret ETV\ndata so that only informative, high impact events are able to influence search operator\nprobabilities. This is accomplished by treating ETV measurements as being dominated by\nneutral measurements that fit some assumed distribution. Statistical arguments are then\nused to gauge whether an ETV measurement is important based on the extent that the ETV\nis an outlier of the neutral distribution. 10\nThe first step is to determine the properties of the neutral distribution based on the ETV\nmeasurements gathered during an adaptation cycle. This is accomplished by taking all of\nthe ETV measurements (gathered from the previous adaptation cycle) and calculating the\nmean and variance of the sample based on the assumption that neutral measurements\ndominate the data and fit a lognormal distribution. Other distribution assumptions have\nalso been tested (e.g. Normal) with fairly similar results.\n\n10\n\nFor more information on statistical tests, see [167]\n\n66\n\n\fChapter 3: Adaptation of EA Design\n\nAn ETV is an outlier when it is NOT expected statistically that the sample population\ncontains at least one such measurement or any larger measurement. In other words, each\nETV can be assigned a probability p\u03b1 that the ETV is an outlier of the sample distribution.\nA quantitative formulation of this definition will now be derived from statistics. Also note\nthat for the calculations below, the ETV measurements are first transformed to log ETV so\nthat neutral events can be assumed to fit a normal distribution.\n\n3.2.4.1\n\nOutlier Calculation\n\nAssuming a normal distribution for the data, individual measurements ETVj are tested\nagainst the sample mean \u03bc and one-sided p values, defined as pz in (3-16), are calculated\nusing a z statistic as defined in (3-15). The s term in equation (3-15) represents the sample\nstandard deviation.\n\nThis calculated pz value indicates the probability of observing a\n\nmeasurement of size ETVj or greater. Hence, this simple statistical test can be used to\ndetermine the extent that an ETV value is an outlier. However, one must also account for\nthe fact that the number of outliers observed for a given search operator will also depend on\nthe number of times that the search operator was used (i.e. the search operator sample size).\nThis is relevant because different operators will generally have different operator usage\nprobability values and therefore will have different ETV sample sizes.\nIf a search operator i has an ETV sample size Mi, the number of measurements \u03b1 that are\nof size ETVj or greater follows a binomial distribution that is given by (3-17).\n\nThe\n\nprobability p\u03b1 of NOT observing a measurement greater than or equal to ETVj after Mi\nobservations is therefore the probability that \u03b1 < 1 given by (3-18, which can be calculated\nby the binomial cumulative distribution function.\n\nzj =\n\nETV j \u2212 \u03bc\n\n(3-15)\n\ns\n\np z = P( z > z j )\n\n(3-16)\n\n\u03b1 = Bin( M i , p z )\n\n(3-17)\n\np\u03b1 = P(\u03b1 < 1)\n\n(3-18)\n\n67\n\n\fChapter 3: Adaptation of EA Design\n\nThe final result p\u03b1 indicates the extent to which an ETV is an outlier that can not be easily\naccounted for by the stated distribution and the number of points sampled. Summing these\n\np\u03b1 values over all events produced by a search operator indicates the extent that the\noperator can create exceptional offspring that have an unexpectedly large impact on\npopulation dynamics.\nMeasurement interpretation by the Outlier method is defined by (3-19). The reason that the\n\np\u03b1 value is multiplied by Mi in this equation is to allow this interpretation method to fit\nwithin the adaptive framework presented in Section 3.1.4. To be clear, this means that an\noperator's reward R is equal to the sum of its p\u03b1 values (and not the average) when using\nthis interpretation method. An average is not used because any sensitivity to the operator\nsample size Mi has already been accounted for in the statistical arguments above.\n\nI Outlier = M i p\u03b1 (ETV j ),\n\nj \u2208 Mi\n\n(3-19)\n\nImpact of operator sample sizes: Taking a hypothetical sample of ETV data that has\n\nbeen normalized using (3-15), Figure 3-9 illustrates how the calculation of p\u03b1 will interpret\nan ETV measurement for different ETV values and different operator sample sizes. For\neach of the sample sizes in Figure 3-9, the p\u03b1 calculation places almost no value on any\nmeasurements found below the sample mean (z = 0). Also notice that for very high\nmeasurements (z > 3), the p\u03b1 calculation approaches a value of 1 meaning it has high\nconfidence that the measurement is an outlier (i.e. that the event had a large impact on\npopulation dynamics). Finally, for ETV values that are large but their classification as\noutliers is less certain, the sample size from which the measurement is taken will strongly\ninfluence the interpretation of the measurement.\n\n68\n\n\fChapter 3: Adaptation of EA Design\n\nFigure 3-9: p\u03b1 calculation curves for sample sizes Mi=5 (- - -), Mi =10 (- -), and Mi =20 (--).\n\nAlthough the Outlier interpretation method described here may seem overly complicated\nfor those unfamiliar with statistical tests, it is really only a procedure for selectively using\nmeasurement outliers for adaptation. Many of the interpretation methods described in\nSection 3.1.4.5 are simple heuristics for placing emphasis on higher valued measurements\nand so in this way they are similar to the Outlier interpretation method. The difference with\nthe present approach is that it actually quantifies the degree to which each event exceeds\nthe average (in terms of probability of occurrence), and gives much more weight to \"true\"\noutliers.\n\n3.3 Experiments\n\n3.3.1\n\nExperimental Setup\n\nThis chapter has thus far reviewed a framework for adaptation of search operator\nprobabilities in an Evolutionary Algorithm and has presented an approach to adaptation\nbased on the ETV measurement and the Outlier interpretation method.\n\nThis section\n\nassesses the performance of the new supervisory adaptive method by testing it on a suite of\nartificial test functions and engineering design problems (listed in Table 3-2) and\ncomparing these results with a number of adaptive and non-adaptive EA designs. Details\nof the EA designs used in these experiments are described next.\n\n69\n\n\fChapter 3: Adaptation of EA Design\n\nTable 3-2 List of test functions used in experiments. Problem definitions, parameter settings, fitness landscape\ncharacteristics, and problem descriptions (for design problems) are provided in Appendix A.\n\nArtificial Test Functions\nBohachevsky's\nQuadratic\nRosenbrock's Valley\nRastrigin\nSchwefel\nGriewangk\nMassively Multimodal Deceptive\nProblem (MMDP)\nWatson's\nColville's\nSystem of linear equations\nAckley's Path Function\nNeumaier's Function #2\n30-D Hyper Ellipsoid\n\n3.3.1.1\n\nEngineering Design Problems\nTurbine Power Plant\nWelded Beam Design\nTension Compression Spring\nGear Train Design\nApplication-Inspired Problems\nMinimum Tardy Task Problem\n(MTTP)\nError Correcting Code Problem (ECC)\nFrequency Modulation\n\nCore EA Design\n\nThe EA designs used in these experiments are described below with the core of the EA\ndesign given by the pseudocode in Figure 3-10.\n\nFor each generation Gen, N new\n\nindividuals are generated to form the offspring population. Offspring are generated by\nselecting one of the ten search operators given in Table 3-3. The operators are selected\nprobabilistically in proportion to the operator probability value. Parents are then selected at\nrandom from the parent population, with the number of parents depending on the search\noperator used. The offspring is finally created and its objective function value is evaluated.\n\n70\n\n\fChapter 3: Adaptation of EA Design\n\nInitialize population\nEvaluate population\nDo\n'Reproduction\nFor i=1 to N\nselect a single search operator (based on probabilities)\nselect parents (at random)\nCreate offspring\nEvaluate offspring\nNext i\nGen=Gen+1\nIF (constrained problem) THEN define fitness by Stochastic Ranking\nIf Adapt THEN Adapt Operator Probabilities\n'Selection\nFor i=1 to N\nselect two individuals (at random from parents and offspring)\nkeep more fit individual\nNext i\nLoop until stopping criteria\nFigure 3-10 Pseudocode of EA design\n\nTable 3-3: List of search operators used in EA designs. Full descriptions of each search operator are provided in\nAppendix B.\n\nSearch Operators\nWright's Heuristic Crossover\nSimple Crossover\nExtended Line Crossover\nUniform Crossover\nBLX- \u03b1\nDifferential Evolution Operator (DE)\nSwap\nRaise\nCreep\nSingle Point Random Mutation\n\nSelection takes place by combining the parent and offspring populations and then\nrepeatedly selecting two individuals at random and removing the worse individual until the\ntotal population size is reduced to N. The selection procedure is very similar to binary\ntournament selection without replacement.\n\n71\n\n\fChapter 3: Adaptation of EA Design\n\nPopulations were randomly initialized (with EA experimental replicates using random\nnumber seeds for blocking11), the population size was set to N=30, and the stopping criteria\nwas set as a maximum of 3000 generations. Thus the final solution from each experiment\nis obtained after 90,000 objective function evaluations.\n\nGenes consisted of direct\n\nrepresentations of the parameters being optimized (i.e. real coding).\nFor optimization problems with nonlinear constraints, fitness is determined using the\nstochastic ranking method presented in Chapter 2 and also described in [64].\n\nPrevious\n\nexperience with this method has indicated that Stochastic Ranking works well for many\nproblems using the parameter settings specified in [64].\n\n3.3.1.2\n\nSearch Operator Control\n\nThe algorithms tested in these experiments differ only in the settings of the search operator\nprobability values. A number of methods for adapting the probability values were tested\nwhich are described in the pseudocode in Figure 3-11 and in Figure 3-12.\nThe standard adaptive procedure is described by the pseudocode in Figure 3-11 and works\nby taking the fitness measurement for each new offspring, interpreting the fitness using one\nof the interpretation formula provided in Section 3.1.4.5, and storing this interpretation with\nothers from the same search operator. Every \u03c4 generations, the stored data is averaged to\ncalculate the Reward R, as defined in (3-4), which in turn is used to calculate the Quality Q\nfor each operator as defined in (3-1). Finally the operator probability value P is calculated\nusing either the probability matching strategy defined in (3-2) or the adaptive pursuit\nstrategy defined in (3-3).\n\n11\n\nBlocking is a method for designing experiments in order to reduce the variability of results arising from\nsome unimportant factor. For these experiments, the unimportant factor is the sensitivity of EA performance\nto the initial conditions of the population. In this case, blocking occurs by using the same set of random\nnumber seeds for tests on each EA design. More information on blocking can be found in chapter five of\n[167].\n\n72\n\n\fChapter 3: Adaptation of EA Design\n\nFor each offspring\ni = offspring's search operator\nCalculate F (defined in Section 3.1.4.4)\nCalculate I (defined in Section 3.1.4.5)\nMi = Mi +1\nIarchive (i, Mi)= I\nNext offspring\nIF Gen mod \u03c4 =0 THEN\nt=t+1\nFor each operator i\nCalculate R (defined in Section 3.1.4.3)\nCalculate Q (defined in Section 3.1.4.1)\nCalculate P (defined in Section 3.1.4.2)\nMi = 0\nNext i\nEND IF\nFigure 3-11 Pseudocode for standard search operator probability adaptation\n\nThe order of calculation steps changes slightly when using ETV in place of the fitness\nmeasurement and so a separate pseudocode is provided in Figure 3-12. The change to the\npseudocode is due to the fact that the ETV measurement of an offspring can take several\ngenerations to calculate. More information on the ETV calculation steps is provided in\nSection 3.2.2.3. When the ETV measurement is used, it is interpreted using either the\nOutlier interpretation method described in Section 3.2.4.1 or no interpretation is used, in\nwhich case the interpretation I is set equal to ETV.\n\n73\n\n\fChapter 3: Adaptation of EA Design\n\nCalculate ETV\nIf Gen mod \u03c4 =0 Then\nt=t+1\nFor each completed ETV\ni = ETV's search operator\nF = ETV\nCalculate I\n'(Options: IOutlier or I=ETV)\nMi = Mi +1\nIarchive (i, Mi)= I\nNext ETV\nFor each operator i\nCalculate R (defined in Section 3.1.4.3)\nCalculate Q (defined in Section 3.1.4.1)\nCalculate P (defined in Section 3.1.4.2)\nMi = 0\nNext i\nEnd If\nFigure 3-12 Pseudocode for search operator probability adaptation using ETV. ETV is defined in Section 3.2.2\nand IOutlier is defined in Section 3.2.4.\n\nThe parameter settings and other design details of the adaptive methods tested in these\nexperiments are provided in Table 3-4. The adaptive methods chosen were done so in an\nattempt to sample a number of the design options described in the background material\n(Section 3.1.4) including measurement interpretations which use the parent context, the\npopulation context, and a standard ranking interpretation (see Section 3.1.4.5).\n\nAlso\n\nincluded are several adaptive methods that use the adaptive pursuit strategy and the\nprobability matching strategy for adjusting search operator probabilities (see Section\n3.1.4.2).\n\n74\n\n\fChapter 3: Adaptation of EA Design\n\nTable 3-4 Details of the adaptive methods used for adapting search operator probabilities are listed. Column one\nprovides the label used to refer to each adaptive method. The second column indicates whether the adaptive\nmethod uses the adaptive pursuit strategy (Y) or the probability matching strategy (N). The measurement of an\nevent is given in column three as either the fitness (F) or the Event Takeover Value (ETV). The interpretation of\nevent measurements is either one of those listed in Section 3.1.4.5 or the Outlier method of Section 3.2.4.1. For the\n\"ETV\" adaptive method, the interpretation is equivalent to the ETV value. Each adaptive method has the task of\nsetting the operator probabilities for the 10 search operators listed in Table 3-3. Each adaptive method uses\nparameter settings \u03b1 = 0.8, PMin=0.02 and \u03c4 =10. The adaptive pursuit strategy also has \u03b2 = 0.8. No attempt was\nmade to tune these parameters and the values were chosen largely to maintain consistency with previous research\nin this topic. Preliminary testing indicated that the results are not strongly sensitive to the setting of \u03b1 and \u03c4.\n\nAdaptive EA\ndesign name\nI(median)-Pursuit\nI(parent)-Pursuit\nI(rank)-Pursuit\nI(median)\nI(parent)\nI(rank)\nETV-Outlier\nETV\n\nAdaptive\nPursuit (Y/N)\nY\nY\nY\nN\nN\nN\nN\nN\n\nEvent\nMeasurement\nF\nF\nF\nF\nF\nF\nETV\nETV\n\nMeasurement\nInterpretation\nI4\nI1\nI8\nI4\nI1\nI8\nOutlier\nETV\n\nTwo EA designs which do not adapt search operator probabilities are also considered. The\nfirst, referred to as Static-Ops2, only uses uniform crossover with probability 0.98 and\nsingle point mutation with probability 0.02. All other search operators have probability\nvalues of zero. The second design, Static-Ops10, uses all ten search operators listed in\nTable 3-3 with equal probability (P = 0.1).\n\n3.3.2\n\n3.3.2.1\n\nResults and Discussion\n\nGeneral Performance Statistics\n\nThis section attempts to draw general conclusions about the performance of the adaptive\nEA designs tested in these experiments. The first statistic in column two of Table 3-5 states\nthe percentage of problems that an EA was found to be the best algorithm out of those\ntested. The second statistic in the third column states the percentage of problems where an\nEA found the best solution in at least one its runs. These two statistics evaluate final\nalgorithm performance, however it is also useful to make statements about performance at\nother timescales. To address this, Figure 3-13 presents general algorithm performance as a\nfunction of time.\n75\n\n\fChapter 3: Adaptation of EA Design\n\nTable 3-5 Overall performance statistics for each of the adaptive and non-adaptive EA designs. Column two\nmeasures the percentage of problems where an EA design was the best EA design (comparisons based on median\nobjective function value). Column three measures the percentage of problems where an EA design was able to find\nthe best solution at least one time. The best solution is defined as the best found in these experiments and is not\nnecessarily the global optimal solution.\n\nEA Design\nI(median)-Pursuit\nI(parent)-Pursuit\nI(rank)-Pursuit\nI(median)\nI(parent)\nI(rank)\nETV-Outlier\nETV\nStatic-Ops2\nStatic-Ops10\n\n% of problems where EA\nwas best design\nfound best\n10.4%\n40%\n2.9%\n35%\n7.9%\n40%\n4.5%\n55%\n12.9%\n45%\n9.5%\n45%\n27.0%\n90%\n15.4%\n35%\n3.0%\n15%\n6.6%\n45%\n\nBased on these general performance statistics, some important conclusions can be drawn.\nFirst, it is clear that the two operator EA design with static search operator probabilities\n(Static-Ops2) performs very poorly on almost every test function. This is a significant\nconclusion since the two operator non-adaptive EA is by far the most commonly used EA\ndesign.\n\nBy simply including more operators without even tuning (or adapting) the\n\nprobability parameters, it is found that substantial performance improvements occur.\nAdding adaptive mechanisms for tuning the probability parameters provides significant\nperformance improvements although the best adaptive method is problem specific. It is\nimportant to notice that while as a class, adaptive methods were more than twice as likely\nto be the best design for a given problem (compared to the non-adaptive EA designs), no\nsingle adaptive method was strongly favored over all others. However, it is still somewhat\nimpressive that the ETV-Outlier adaptive method is found to be the best design on 27% of\nthe test functions while the average for all other adaptive methods was 9.1%. It is also\nworth pointing out that the next best adaptive method, ETV, was the best design only\n15.4% of the time.\nIf one is more interested in an algorithm's ability to find good solutions over multiple runs,\nthen much stronger conclusions can be made from these results. From column three of\nTable 3-5, one can see that the ETV-Outlier method is able to find a best solution in 18 of\n\n76\n\n\fChapter 3: Adaptation of EA Design\n\nthe 20 problems tested while the second best algorithm, I(median), only finds a best\nsolution 55% of the time.\nFinally, if one is concerned with performance at different time scales, it is also clear that the\nETV-Outlier adaptive method exhibits strong performance throughout the 3000 generations\ntested and that the non-adaptive method, Static-Ops2, exhibits poor performance\nthroughout the 3000 generations tested. It is also interesting to note in Figure 3-13 that two\nof the three adaptive methods employing the adaptive pursuit strategy are no better (on\naverage) than the non-adaptive EA design, Static-Ops10.\n\nI(median)-Pursuit\n\n70\n\nI(parent)-Pursuit\n\nRank\n\nI(rank)-Pursuit\nI(median)\n\n50\n\nI(parent)\nI(rank)\nETV-Outlier\n\n30\n\nETV\nStatic-Ops2\nStatic-Ops10\n\n10\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\nGen\nFigure 3-13 General algorithm performance for both adaptive and non-adaptive EA designs shown as a function\nof the number of generations (Gen) of evolution. In order to aggregate performance data from different test\nfunctions, it was necessary to deal with differences in fitness scaling. This was addressed by using the following\nranking procedure. Each algorithm is run 20 times on each test function listed in Table 3-2. At a given generation,\nevery EA run is ranked among all runs conducted on that test function (with a higher ranking being better). The\nmedian rank of each EA design is then calculated for each test function. Finally, these median ranks are averaged\nover all test functions and plotted against the number of generations (Gen).\n\nMore detailed performance results for individual test functions are broken down into three\nparts. In Section 3.3.2.2, the results from a selected set of artificial test functions are\nanalyzed in detail with the goal of understanding the relationship between search operator\nprobability profiles and algorithm performance. In Section 3.3.2.3, performance on the\nremaining artificial test functions is presented and briefly described. Finally, Section\n3.3.2.4 looks at algorithm performance on a selected set of engineering design problems.\n\n77\n\n\fChapter 3: Adaptation of EA Design\n\n3.3.2.2\n\nOperator Probability Profile Analysis\n\nThe optimal features of a search operator probability profile are generally not known,\nhowever it is possible to venture a few educated guesses as to what such a profile might\nlook like. First, the adaptive method should recognize those search operators which are not\nat all effective as a means of searching the fitness landscape. For those operators, it is\ndesirable to keep their probability values very close to Pmin (i.e. the smallest allowed value).\nAlthough it is unknown which operators are suited for a particular problem, it is believed\nthat the range of search operators used in these experiments (listed in Table 3-3) is broad\nenough so that at least one or more operators are not suited for each problem.\nAlso, as evolution proceeds, one can expect that the ruggedness and other fitness landscape\ncharacteristics will change as the population occupies different regions of parameter space.\nHence one would expect that, on some problems, an adaptive method should modify which\nsearch operators it prefers as it adapts to changes in the environment. For some artificial\ntest functions, the fitness landscape is fairly well understood and this knowledge is used in\nthe following section to assess each of the adaptive methods.\nThe Rosenbrock, Schwefel, Griewangk, and Ackley test functions have been selected for\nthis analysis. Search operator probability profiles are taken from the same experiments\nused to generate the performance results. The probability values that are shown for each\nsearch operator represent the median value from 20 experimental replicates.\n\n3.3.2.2.1 Rosenbrock Test Function\nThe Rosenbrock test function is a smooth unimodal test function with the global optimum\nresiding inside a long and narrow parabolic shaped valley as seen in Figure 3-14. Since the\nlandscape is smooth and unimodal, finding the valley is trivial, however the curvature of\nthe valley makes convergence to the global optimum slow and difficult. In most runs the\npopulation tends to gather along the bottom of the valley then follow it towards the bottom.\nSearch operators which exhibit hill climbing behaviors are expected to be more effective in\nthis fitness landscape since the problem is unimodal.\n\nThe extended-line operator,\n\ndifferential evolution, and Wright's heuristic crossover all have hill climbing characteristics\n(similar to directed search) so one would suspect these to be favored over the other\n\n78\n\n\fChapter 3: Adaptation of EA Design\n\noperators. Gene swapping operators like single point crossover, uniform crossover, and\nswap are not expected to perform well.\n\na)\n\nb)\n\nc)\n\nd)\n\nFigure 3-14 Rosenbrock fitness landscape shown in a two dimensional parameter space. The two bottom graphs\nare shown for variable 1 and variable 2 varying over the entire parameter range [-2,2]. Graphs on the top focus on\nthe parameter region containing the global optimum. The two graphs on the left show a restricted range of\nobjective function values (vertical axis) to help in visualizing the fitness landscape. Images were kindly provided\nby Hartmut Pohlheim and were generated using the GEATbx toolbox in Matlab\u00ae [168]. Low resolution images\ncan also be found at http://www.geatbx.com/docu/fcnindex-01.html#P85_2637.\n\nFor many of the adaptive methods presented in the figures below, Wright's heuristic\ncrossover and extended line crossover are strongly favored over other search operators.\nThese operators act as interpolation and extrapolation search actions (resp.) which are\nbiased towards the more fit parent making them quite effective on smooth landscapes like\nRosenbrock. It is also noticed that the differential evolution operator and BLX are also\nfavored, although to a lesser extent.\nFor the adaptive methods I(rank) and ETV, there is very little difference between search\noperator probabilities. These are also the two worst adaptive methods for this problem as\n79\n\n\fChapter 3: Adaptation of EA Design\n\nseen in Figure 3-15. On the other hand, I(median) and ETV-Outlier find the greatest\ndifferences between search operator probabilities. These are also two of the three best\nadaptive methods for this problem. These results taken together (particularly methods ETV\nand ETV-Outlier) indicate that poor measurement interpretations can prevent adaptive\nmethods from making important distinctions between operators.\nIt is also worth noting that there exists a form of symmetry in the Rosenbrock fitness\nlandscape which causes the environment to be approximately stationary during most of\nevolution.\n\nAs a consequence, one would expect the same search operators to be\n\nconsistently preferred throughout the run. It is noticed however that for two of the three\nadaptive methods employing the adaptive pursuit strategy, very little consistency is present\nin the selection of search operators and in fact the operator profiles appear quite chaotic.\nThis behavior is a general characteristic of the adaptive pursuit strategy which is repeatedly\nseen in the other test functions analyzed in this section. These two methods also seem to\nsuffer in performance compared with their less chaotic cousin, I(median)-pursuit, as seen in\nFigure 3-15.\n\nRosenbrock\n0.01\n1E-05\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n1E-08\n\nI(median)-Pursuit\n\n1E-11\n\nI(parent)-Pursuit\n\n1E-14\n\nI(rank)-Pursuit\nI(median)\n\nF 1E-17\n1E-20\n\nI(parent)\n\n1E-23\n\nI(rank)\n\n1E-26\n\nETV-Outlier\n\n1E-29\n\nETV\n\n1E-32\n\nStatic-Ops2\n\n1E-35\nGen\n\nStatic-Ops10\n\nFigure 3-15 Performance of adaptive and non-adaptive EA designs on the Rosenbrock test function. The global\noptimal solution is F=0. The optimal F value can not be shown due to log scaling on the F axis so performance\nprofiles are seen to terminate when the global optima is reached.\n\n80\n\n\fChapter 3: Adaptation of EA Design\n\n1.00\n\n1.00\n\nI(median)-Pursuit\n\nI(median)\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1.00\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1500\n\n2000\n\n1.00\n\nI(parent)-Pursuit\n\nI(parent)\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\nSingle Point X\n\nExtended Lin X\n\nWright's X\n\nUniform X\n\nBLX-0.2\n\nDE\n\nSwap\n\nRaise\n\nCreep\n\nMutation\n\nFigure 3-16 Search operator probability profiles for adaptive methods I(median)-Pursuit, I(median), I(parent)Pursuit, and I(parent) on the Rosenbrock test function. Probability values are shown on a logarithmic scale over\nthe first 2000 generations of evolution.\n\n81\n\n\fChapter 3: Adaptation of EA Design\n\n1.00\n\n1.00\n\nI(rank)-Pursuit\n\nI(rank)\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1.00\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1000\n\n1500\n\n2000\n\n1.00\n\nETV-Outlier\n\nETV\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\nSingle Point X\n\nExtended Lin X\n\nWright's X\n\nUniform X\n\nBLX-0.2\n\nDE\n\nSwap\n\nRaise\n\nCreep\n\nMutation\n\nFigure 3-17 Search operator probability profiles for adaptive methods I(rank)-Pursuit, I(rank), ETV-Outlier, and\nETV on the Rosenbrock test function. Probability values are shown on a logarithmic scale over the first 2000\ngenerations of evolution.\n\n3.3.2.2.2 Schwefel Test Function\nThe Schwefel test function has a multimodal fitness landscape as seen in Figure 3-18.\nLocal optima are distributed throughout parameter space with many containing fitness\nvalues that are similar to the global optima making the problem challenging.\n\n82\n\n\fChapter 3: Adaptation of EA Design\n\na)\n\nb)\n\nc)\n\nd)\n\ne)\n\nFigure 3-18 Schwefel fitness landscape shown in two dimensions of parameter space. The landscape is shown for\nvariable 1 and variable 2 varying over different parameter ranges. The entire range is shown in the bottom graph\nwith each parameter varying over [-500,500]. The vertical axis shows the objective function value (minimization)\nwith the global optimal solution located at the origin of parameter space. Images were kindly provided by\nHartmut Pohlheim and were generated using the GEATbx toolbox in Matlab\u00ae [168]. Low resolution images can\nalso be found at http://www.geatbx.com/docu/fcnindex-01.html#P85_2637.\n\nFor the adaptive methods that can make clear distinctions between search operators, it\nappears that two general trends occur. In the first trend, which is observed in I(median),\nI(parent) and I(rank), it is found that the creep operator is consistently preferred throughout\nthe span of evolution. These methods also demonstrate poor performance on this test\nfunction.\n\n83\n\n\fChapter 3: Adaptation of EA Design\n\nThe second general trend is seen in I(rank)-pursuit, ETV-Outlier and to a lesser extent in\nI(median)-pursuit. For these methods, the creep operator is strongly favored initially but in\nthe later stages of evolution, operators with hill climbing characteristics (Wright's heuristic\ncrossover, extended line crossover, differential evolution) are found to be strongly favored.\nThis behavior is most clearly visible in the search operator probability profile for the ETVOutlier adaptive method. The three adaptive methods that exhibit this second trend in\nbehavior also have the best performance as seen in Figure 3-26.\n\nSchewfel\n0.1\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\nI(median)-Pursuit\nI(parent)-Pursuit\n\n0.01\n\nI(rank)-Pursuit\nI(median)\n\nF\n\nI(parent)\nI(rank)\n\n0.001\n\nETV-Outlier\nETV\nStatic-Ops2\n0.0001\nGen\n\nStatic-Ops10\n\nFigure 3-19 Performance of adaptive and non-adaptive EA designs on the Schwefel test function. The global\noptimal solution is at F=0.\n\n84\n\n\fChapter 3: Adaptation of EA Design\n\n1.00\n\n1.00\n\nI(median)\n\nI(median)-Pursuit\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n\n0\n\n500\n\n1000\n\n1500\n\n0\n\n2000\n\n500\n\n1000\n\n1500\n\n2000\n\n1500\n\n2000\n\n1.00\n\n1.00\n\nI(parent)\n\nI(parent)-Pursuit\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\nSingle Point X\n\nExtended Lin X\n\nWright's X\n\nUniform X\n\nBLX-0.2\n\nDE\n\nSwap\n\nRaise\n\nCreep\n\nMutation\n\nFigure 3-20 Search operator probability profiles for adaptive methods I(median)-Pursuit, I(median), I(parent)Pursuit, and I(parent) on the Schwefel test function. Probability values are shown on a logarithmic scale over the\nfirst 2000 generations of evolution.\n\n85\n\n\fChapter 3: Adaptation of EA Design\n\n1.00\n\n1.00\n\nI(rank)-Pursuit\n\nI(rank)\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1.00\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1.00\n\nETV-Outlier\n\nETV\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\n1500\n\nSingle Point X\n\nExtended Lin X\n\nWright's X\n\nUniform X\n\nBLX-0.2\n\nDE\n\nSwap\n\nRaise\n\nCreep\n\nMutation\n\n2000\n\nFigure 3-21 Search operator probability profiles for adaptive methods I(rank)-Pursuit, I(rank), ETV-Outlier, and\nETV on the Schwefel test function. Probability values are shown on a logarithmic scale over the first 2000\ngenerations of evolution.\n\n3.3.2.2.3 Griewangk Test Function\nFrom Figure 3-22a, one can see that the fitness landscape for the Griewangk test function is\nsmooth at large parameter scales. With the initial EA population randomly distributed\nthroughout parameter space, it is expected that interpolative actions would be particularly\nuseful in the early stages of evolution. However, as the population converges to a more\nlocalized region of parameter space, the landscape becomes very rugged as evidenced by\nthe peaks in Figure 3-22b. Under these conditions, less exploitive operators are expected to\nbe useful such as uniform crossover and BLX. Finally, when the population eventually\n86\n\n\fChapter 3: Adaptation of EA Design\n\nconverges to a single peak like one of those shown in Figure 3-22c, the landscape again\nbecomes smooth so that highly exploitive operators are expected to again be preferred.\n\na)\n\nb)\n\nc)\n\nFigure 3-22 Griewangk fitness landscape shown in two dimensions of parameter space. a) The landscape is shown\nfor variable 1 and variable 2 varying over their complete range [-500,500]. b) The landscape is shown for variable\n1 and variable 2 varying over the range [-50,50]. c) The landscape is shown for variable 1 and variable 2 varying\nover the range [-8,8]. The vertical axis shows the objective function value (minimization) with the global optimal\nsolution located at the origin of parameter space. Images were kindly provided by Hartmut Pohlheim and were\ngenerated using the GEATbx toolbox in Matlab\u00ae [168]. Low resolution images can also be found at\nhttp://www.geatbx.com/docu/fcnindex-01.html#P85_2637.\n\nAs anticipated, many of the adaptive methods initially prefer Wright's heuristic crossover\nwhich indicates an ability to exploit global landscape features (This is most notably seen in\nthe ETV-Outlier method). However, most adaptive methods are not able to distinguish\nbetween search operators throughout the rest of the run. But notice in Figure 3-23 that\nmost adaptive methods stop improving within the first 500 generations meaning that\noperator adaptation was only significant to performance during this initial phase of\nevolution.\nAlthough speculative, it is possible that the ETV-Outlier adaptive method was best able to\ninitially exploit global landscape characteristics but that this also helped to expedite\npopulation convergence and ultimately was detrimental to final performance of the\n87\n\n\fChapter 3: Adaptation of EA Design\n\nalgorithm. Considering that adaptation can only take into account performance data over\nshort time scales, one might suspect that for some problems, an effective adaptive\nmechanism can actually impair algorithm performance. For the Griewangk test function,\nsome of the least adaptive methods (e.g. Static-Ops10, I(rank), ETV) also have the best\nperformance which seems to support this claim.\n\nGriewangk\n0.1\n0.09\n\nI(median)-Pursuit\n\n0.08\n\nI(parent)-Pursuit\n\n0.07\n\nI(rank)-Pursuit\nI(median)\n\nF 0.06\n\nI(parent)\n\n0.05\n\nI(rank)\n\n0.04\n\nETV-Outlier\n\n0.03\n\nETV\nStatic-Ops2\n\n0.02\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-23 Performance of adaptive and non-adaptive EA designs on the Griewangk test function. The global\noptimal solution is at F=0.\n\n88\n\n\fChapter 3: Adaptation of EA Design\n\n1.00\n\n1.00\n\nI(median)\n\nI(median)-Pursuit\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n0\n\n2000\n\n500\n\n1000\n\n1500\n\n2000\n\n1500\n\n2000\n\n1.00\n\n1.00\n\nI(parent)\n\nI(parent)-Pursuit\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\nSingle Point X\n\nExtended Lin X\n\nWright's X\n\nUniform X\n\nBLX-0.2\n\nDE\n\nSwap\n\nRaise\n\nCreep\n\nMutation\n\nFigure 3-24 Search operator probability profiles for adaptive methods I(median)-Pursuit, I(median), I(parent)Pursuit, and I(parent) on the Griewangk test function. Probability values are shown on a logarithmic scale over\nthe first 2000 generations of evolution.\n\n89\n\n\fChapter 3: Adaptation of EA Design\n\n1.00\n\n1.00\n\nI(rank)\n\nI(rank)-Pursuit\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n0\n\n2000\n\n500\n\n1000\n\n1500\n\n2000\n\n1000\n\n1500\n\n2000\n\n1.00\n\n1.00\n\nETV\n\nETV-Outlier\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\nSingle Point X\n\nExtended Lin X\n\nWright's X\n\nUniform X\n\nBLX-0.2\n\nDE\n\nSwap\n\nRaise\n\nCreep\n\nMutation\n\nFigure 3-25 Search operator probability profiles for adaptive methods I(rank)-Pursuit, I(rank), ETV-Outlier, and\nETV on the Griewangk test function. Probability values are shown on a logarithmic scale over the first 2000\ngenerations of evolution.\n\n3.3.2.2.4 Ackley's Path Function\nWith Ackley's path function, the fitness landscapes is smooth on a global scale as seen in\nFigure 3-26a, however once within the region containing the global optima, the landscape\nbecomes increasingly rugged as seen in Figure 3-26b. As a result, one might expect some\nexploitive search operators would be initially beneficial, however as the basin of attraction\nfor the global optima is discovered, more explorative search operators would then become\nmore useful. Also, since a large attractor is located in the center of parameter space, one\n\n90\n\n\fChapter 3: Adaptation of EA Design\n\nwould expect that an interpolation operator like Wright's heuristic crossover will be\ninitially very effective.\n\na)\n\nb)\n\nFigure 3-26 Ackley's fitness landscape shown in two dimensions of parameter space. a) The landscape is shown\nfor variable 1 and variable 2 varying over their complete range [-30,30]. b) The landscape is shown for variable 1\nand variable 2 varying over the range [-2,2]. The vertical axis shows the objective function value (minimization)\nwith the global optimal solution located at the origin of parameter space. Images were kindly provided by\nHartmut Pohlheim and were generated using the GEATbx toolbox in Matlab\u00ae [168]. Low resolution images can\nalso be found at http://www.geatbx.com/docu/fcnindex-01.html#P85_2637.\n\nFor the three best adaptive methods shown in Figure 3-27, I(median)-pursuit, I(median),\nETV-Outlier, and to a lesser extend with I(rank)-pursuit, there appears to be a very brief\ninitial phase (20 to 100 generations) where an exploitive operator is preferred (either\ndifferential evolution or Wright's heuristic crossover).\n\nThe fact that this period is\n\nextremely brief is not surprising since all methods reach objective function values of F <\n10 in the first 50 generations which indicates that the population has already converged to\nthe parameter region shown in Figure 3-26b. Next, a slightly longer phase (200 to 400\ngenerations) is observed where more explorative gene swapping operators are preferred.\nFinally, for the last 500 to 700 generations before reaching the global optimal solution, the\nbest adaptive methods again prefer highly exploitive search operators (Wright's heuristic\ncrossover, extended line crossover, differential evolution).\n\nThese changes in search\n\noperator preferences during evolution are believed to accurately reflect changes in the\nenvironment and are most clearly seen in the adaptive methods I(median) and ETV-Outlier.\n\n91\n\n\fChapter 3: Adaptation of EA Design\n\nAckley's Function\n10\n0.1\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\nI(median)-Pursuit\n\n0.001\n\nI(parent)-Pursuit\n\n1E-05\n\nI(rank)-Pursuit\n\n1E-07\n\nI(median)\n\n1E-09\n\nI(parent)\n\n1E-11\n\nI(rank)\n\nF\n\nETV-Outlier\n\n1E-13\n\nETV\n1E-15\n\nStatic-Ops2\n\n1E-17\nGen\n\nStatic-Ops10\n\nFigure 3-27 Performance of adaptive and non-adaptive EA designs on Ackley's Path Function. The global optimal\nsolution is at F=0. The optimal F value can not be shown due to log scaling on the F axis so performance profiles\nare seen to terminate when the global optima is reached.\n\n92\n\n\fChapter 3: Adaptation of EA Design\n\n1.00\n\n1.00\n\nI(median)-Pursuit\n\nI(median)\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1500\n\n2000\n\n1.00\n\n1.00\n\nI(parent)\n\nI(parent)-Pursuit\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\n1000\n\nSingle Point X\n\nExtended Lin X\n\nWright's X\n\nUniform X\n\nBLX-0.2\n\nDE\n\nSwap\n\nRaise\n\nCreep\n\nMutation\n\nFigure 3-28 Search operator probability profiles for adaptive methods I(median)-Pursuit, I(median), I(parent)Pursuit, and I(parent) on Ackley's test function. Probability values are shown on a logarithmic scale over the first\n2000 generations of evolution.\n\n93\n\n\fChapter 3: Adaptation of EA Design\n\n1.00\n\n1.00\n\nI(rank)-Pursuit\n\nI(rank)\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1.00\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n1000\n\n1500\n\n2000\n\n1.00\n\nETV-Outlier\n\nETV\n\n0.10\n\n0.10\n\n0.01\n\n0.01\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n0\n\n500\n\nSingle Point X\n\nExtended Lin X\n\nWright's X\n\nUniform X\n\nBLX-0.2\n\nDE\n\nSwap\n\nRaise\n\nCreep\n\nMutation\n\nFigure 3-29 Search operator probability profiles for adaptive methods I(rank)-Pursuit, I(rank), ETV-Outlier, and\nETV on Ackley's test function. Probability values are shown on a logarithmic scale over the first 2000 generations\nof evolution.\n\n3.3.2.3\n\nPerformance Results on Artificial Test Functions\n\nFor many of the artificial test functions presented in this section, the ETV-Outlier adaptive\nmethod is found to perform strongly throughout the 3000 generations considered.\nPerformance statistics provided in Table 3-6 also demonstrate superior final performance\nfrom the ETV-Outlier method for this set of test functions. Even for problems where the\nmethod does not clearly dominate, it generally was able to perform at least as well as the\nother EA designs. It is interesting to note that ETV-Outlier shows its worst performance on\n\n94\n\n\fChapter 3: Adaptation of EA Design\n\nthe most deceptive problem, MMDP. Performance graphs are presented roughly in the\norder of best to worst performance for the ETV-Outlier method.\nThe non-adaptive EA with two search operators, Static-Ops2, is often found to be\nsignificantly worse than all other algorithms. Also, it is interesting to note that the adaptive\nmethod ETV has nearly identical performance to Static-Ops10 for every problem. This\nresult should not be surprising considering that ETV (without Outlier interpretation) has\nlittle ability to distinguish between search operators, as was indicated in the previous\nsection.\nTable 3-6 Overall performance statistics for each of the adaptive and non-adaptive EA designs run on the artificial\ntest functions. Column two measures the percentage of problems where an EA design was the best EA design\n(comparisons based on median objective function value). Column three measures the percentage of problems\nwhere an EA design was able to find the best solution at least one time. The best solution is defined as the best\nfound in these experiments and is not necessarily the global optimal solution. Results for the non-adaptive EA\ndesigns are shown in the bottom two rows while the rows labeled as ETV and ETV-Outlier show results for the\nnew adaptive methods developed in this thesis.\n\nEA Design\nI(median)-Pursuit\nI(parent)-Pursuit\nI(rank)-Pursuit\nI(median)\nI(parent)\nI(rank)\nETV-Outlier\nETV\nStatic-Ops2\nStatic-Ops10\n\n% of problems where EA\nwas best design\nfound best\n16.7%\n46.2%\n2.6%\n38.5%\n12.8%\n38.5%\n15.4%\n61.5%\n0.0%\n38.5%\n7.7%\n46.2%\n29.5%\n92.3%\n7.7%\n30.8%\n0.0%\n7.7%\n7.7%\n38.5%\n\n95\n\n\fChapter 3: Adaptation of EA Design\n\nSystem of Linear Equations\n100\n1\n0.01\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\nI(median)-Pursuit\nI(parent)-Pursuit\n\n0.0001\n\nI(rank)-Pursuit\n\n1E-06\n\nI(median)\n\n1E-08\n\nI(parent)\n\n1E-10\n\nI(rank)\n\nF\n\nETV-Outlier\n\n1E-12\n\nETV\n1E-14\n\nStatic-Ops2\n\n1E-16\n\nStatic-Ops10\n\nGen\n\nFigure 3-30 Performance of adaptive and non-adaptive EA designs on the System of Linear Equations test\nfunction. The global optimal solution is at F=0.\n\nQuadratic\n1E-26 0\n1E-50\n1E-74\n1E-98\n1E-122\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\nI(median)-Pursuit\nI(parent)-Pursuit\nI(rank)-Pursuit\nI(median)\n\nF 1E-146\n1E-170\n1E-194\n1E-218\n\nI(parent)\nI(rank)\nETV-Outlier\n\n1E-242\n1E-266\n1E-290\n\nETV\nStatic-Ops2\nGen\n\nStatic-Ops10\n\nFigure 3-31 Performance of adaptive and non-adaptive EA designs on the Quadratic test function. The global\noptimal solution is at F=0.\n\n96\n\n\fChapter 3: Adaptation of EA Design\n\nWatson's\n0.029\nI(median)-Pursuit\n\nF\n\n0.027\n\nI(parent)-Pursuit\n\n0.025\n\nI(rank)-Pursuit\nI(median)\n\n0.023\n\nI(parent)\nI(rank)\n\n0.021\n\nETV-Outlier\n0.019\n\nETV\nStatic-Ops2\n\n0.017\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-32 Performance of adaptive and non-adaptive EA designs on Watson's test function. The global optimal\nsolution is at F=2.288E-3.\n\nNeumaier's Function #2\n4\nI(median)-Pursuit\n\n3.99\n\nI(parent)-Pursuit\n3.98\n\nI(rank)-Pursuit\nI(median)\n\nF 3.97\n\nI(parent)\nI(rank)\n\n3.96\n\nETV-Outlier\n3.95\n\nETV\nStatic-Ops2\n\n3.94\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-33 Performance of adaptive and non-adaptive EA designs on Neumaier's function #2. The global optimal\nsolution is unknown (see Appendix A).\n\n97\n\n\fChapter 3: Adaptation of EA Design\n\nColville's\n1\n0.01 0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n0.0001\n\nI(median)-Pursuit\n\n1E-06\n\nI(parent)-Pursuit\n\n1E-08\n\nI(rank)-Pursuit\nI(median)\n\nF 1E-10\n\nI(parent)\n\n1E-12\n\nI(rank)\n\n1E-14\n\nETV-Outlier\n\n1E-16\n\nETV\n\n1E-18\n\nStatic-Ops2\n\n1E-20\n\nStatic-Ops10\n\nGen\n\nFigure 3-34 Performance of adaptive and non-adaptive EA designs on Colville's test function. The global optimal\nsolution is at F=0.\n\nBohachevsky's\n1\n0.01 0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n0.0001\n\nI(median)-Pursuit\n\n1E-06\n\nI(parent)-Pursuit\n\n1E-08\n\nI(rank)-Pursuit\nI(median)\n\nF 1E-10\n\nI(parent)\n\n1E-12\n\nI(rank)\n\n1E-14\n\nETV-Outlier\n\n1E-16\n\nETV\n\n1E-18\n\nStatic-Ops2\n\n1E-20\nGen\n\nStatic-Ops10\n\nFigure 3-35 Performance of adaptive and non-adaptive EA designs on Bohachevsky's test function. The global\noptimal solution is at F=0.\n\n98\n\n\fChapter 3: Adaptation of EA Design\n\nRastrigin\n1.E+01\n1.E+00\n1.E-01 0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\nI(median)-Pursuit\n\n1.E-02\n\nI(parent)-Pursuit\n\n1.E-03\n\nI(rank)-Pursuit\nI(median)\n\nF 1.E-04\n\nI(parent)\n\n1.E-05\n\nI(rank)\n\n1.E-06\n\nETV-Outlier\n\n1.E-07\n\nETV\n\n1.E-08\n\nStatic-Ops2\n\n1.E-09\n\nStatic-Ops10\n\nGen\n\nFigure 3-36 Performance of adaptive and non-adaptive EA designs on the Rastrigin test function. The global\noptimal solution is at F=0.\n\n30-D Hyper Ellipsoid\n1.E+02\n1.E+01\nI(median)-Pursuit\n\n1.E+00\n1.E-01 0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\nI(parent)-Pursuit\nI(rank)-Pursuit\n\n1.E-02\n\nI(median)\n\nF 1.E-03\n\nI(parent)\n\n1.E-04\n\nI(rank)\n\n1.E-05\n\nETV-Outlier\n\n1.E-06\n\nETV\n\n1.E-07\n\nStatic-Ops2\n\n1.E-08\nGen\n\nStatic-Ops10\n\nFigure 3-37 Performance of adaptive and non-adaptive EA designs on the 30-D Hyper Ellipsoid test function. The\nglobal optimal solution is at F=0.\n\n99\n\n\fChapter 3: Adaptation of EA Design\n\nMMDP\n6\nI(median)-Pursuit\n\n5\n\nI(parent)-Pursuit\n4\n\nI(rank)-Pursuit\nI(median)\n\nF3\n\nI(parent)\nI(rank)\n\n2\n\nETV-Outlier\n1\n\nETV\nStatic-Ops2\n\n0\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-38 Performance of adaptive and non-adaptive EA designs on the Massively Multimodal Deceptive\nProblem (MMDP). The global optimal solution is at F=0.\n\n3.3.2.4\n\nPerformance Results on Engineering Design Problems\n\nExperiments are also conducted on a number of engineering design problems (and other\napplication-inspired test functions) in order to gauge the effectiveness of the adaptive\nmethods on practical optimization problems. Each of these problems are described in detail\nin Appendix A.\nTable 3-7 Overall performance statistics for each of the adaptive and non-adaptive EA designs run on the\nengineering design problems. Column two measures the percentage of problems where an EA design was the best\nEA design (comparisons based on median objective function value). Column three measures the percentage of\nproblems where an EA design was able to find the best solution at least one time. The best solution is defined as\nthe best found in these experiments and is not necessarily the global optimal solution.\n\nEA Design\nI(median)-Pursuit\nI(parent)-Pursuit\nI(rank)-Pursuit\nI(median)\nI(parent)\nI(rank)\nETV-Outlier\nETV\nStatic-Ops2\nStatic-Ops10\n\n% of problems where EA\nwas best design\nfound best\n0.0%\n28.6%\n4.8%\n28.6%\n0.0%\n42.9%\n0.0%\n42.9%\n57.1%\n33.3%\n0.0%\n42.9%\n33.3%\n85.7%\n21.4%\n42.9%\n7.1%\n28.6%\n0.0%\n57.1%\n\n100\n\n\fChapter 3: Adaptation of EA Design\n\nThe results from these experiments are noticeably different from results on the artificial test\nfunctions. Comparing the \"best design\" column of Table 3-7 and Table 3-6, it is interesting\nto note that three of the four algorithms that perform best on the artificial test functions are\nnow the worst algorithms for the engineering problems. On the other hand I(parent), which\nis tied as the worst method in the artificial test functions, is now tied with ETV-Outlier as\nthe best method for the engineering problems. In the face of these strong reversals in\nalgorithm performance, it is worth noting that ETV-Outlier maintains its status as the best\nalgorithm in both sets of test problems. Similar behavior is observed with the performance\nmetric in the third column of the same tables, however the performance reversals are not as\npronounced in this case.\nIt is also worth mentioning that the non-adaptive methods faired better on the engineering\nproblems, especially in their ability to find the best solution to a problem at least one time\n(i.e. the \"found best\" metric). In fact, Static-Ops10 is tied for being the second best\nalgorithm for this performance metric.\n\n101\n\n\fChapter 3: Adaptation of EA Design\n\nTurbine Power Plant\n3.06\nI(median)-Pursuit\n\n3.058\n\nI(parent)-Pursuit\nI(rank)-Pursuit\n\n3.056\n\nI(median)\n\nF\n\nI(parent)\n\n3.054\n\nI(rank)\nETV-Outlier\n\n3.052\n\nETV\nStatic-Ops2\n\n3.05\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-39 Performance of adaptive and non-adaptive EA designs on the Turbine Power Plant Problem. The\nglobal optimal solution is at F=3.05.\n\nWelded Beam Design\n1.9\n1.88\n\nI(median)-Pursuit\n\n1.86\n\nF\n\nI(parent)-Pursuit\n\n1.84\n\nI(rank)-Pursuit\n\n1.82\n\nI(median)\n\n1.8\n\nI(parent)\n\n1.78\n\nI(rank)\n\n1.76\n\nETV-Outlier\n\n1.74\n\nETV\nStatic-Ops2\n\n1.72\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-40 Performance of adaptive and non-adaptive EA designs on the Welded Beam Design problem. The\nglobal optimal solution is unknown.\n\n102\n\n\fChapter 3: Adaptation of EA Design\n\nTension Compression Spring\n0.0135\n0.0134\n\nI(median)-Pursuit\n\n0.0133\n\nF\n\nI(parent)-Pursuit\n\n0.0132\n\nI(rank)-Pursuit\n\n0.0131\n\nI(median)\n\n0.013\n\nI(parent)\n\n0.0129\n\nI(rank)\n\n0.0128\n\nETV-Outlier\n\n0.0127\n\nETV\nStatic-Ops2\n\n0.0126\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-41 Performance of adaptive and non-adaptive EA designs on the Tension Compression Spring problem.\nThe global optimal solution is unknown.\n\nGear Train Design\n1.E-09\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\nI(median)-Pursuit\nI(parent)-Pursuit\nI(rank)-Pursuit\nI(median)\n\nF\n\nI(parent)\nI(rank)\nETV-Outlier\nETV\nStatic-Ops2\n1.E-10\nGen\n\nStatic-Ops10\n\nFigure 3-42 Performance of adaptive and non-adaptive EA designs on the Gear Train Design problem. The global\noptimal solution is F=2.70 x10-12.\n\n103\n\n\fChapter 3: Adaptation of EA Design\n\nMTTP\n100000\nI(median)-Pursuit\n\n10000\n\nI(parent)-Pursuit\nI(rank)-Pursuit\n\n1000\n\nI(median)\n\nF\n\nI(parent)\n\n100\n\nI(rank)\nETV-Outlier\n\n10\n\nETV\nStatic-Ops2\n\n1\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-43 Performance of adaptive and non-adaptive EA designs on the Minimum Tardy Task Problem\n(MTTP). The global optimal solution is F=0.\n\nFrequency Modulation\n20\n19\n\nI(median)-Pursuit\n\n18\n\nI(parent)-Pursuit\n\n17\n\nI(rank)-Pursuit\nI(median)\n\nF 16\n\nI(parent)\n\n15\n\nI(rank)\n\n14\n\nETV-Outlier\n\n13\n\nETV\nStatic-Ops2\n\n12\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-44 Performance of adaptive and non-adaptive EA designs on the Frequency Modulation problem. The\nglobal optimal solution is F=0.\n\n104\n\n\fChapter 3: Adaptation of EA Design\n\nECC\n0.005\n0.0045\n\nI(median)-Pursuit\n\n0.004\n\nI(parent)-Pursuit\nI(rank)-Pursuit\n\n0.0035\n\nI(median)\n\nF 0.003\n\nI(parent)\n\n0.0025\n\nI(rank)\n\n0.002\n\nETV-Outlier\nETV\n\n0.0015\n\nStatic-Ops2\n\n0.001\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nStatic-Ops10\n\nFigure 3-45 Performance of adaptive and non-adaptive EA designs on the Error Correcting Code (ECC) problem.\nThe global optimal solution is F=0.\n\n3.3.3\n\nDiscussion and Conclusions\n\nA number of important conclusions can be drawn from the results in this chapter which are\nhighlighted below and discussed within the context of EC research.\nKeys to Effective Adaptation of Search Operators: Some useful conclusions can be\n\ndrawn about the characteristics of an effective adaptive procedure based on the probability\nprofile analysis from Section 3.3.2.2. From this analysis, it is concluded that an effective\nadaptive method is one that 1) is able to select appropriate operators for exploiting the\nfeatures of a given fitness landscape, 2) changes only in response to environmental changes\nand 3) is able to resolve the \"right amount\" of difference in the search operator usage rates.\nSuch a description of an effective adaptive method is not surprising however what is\nsurprising is that very few of the adaptive methods were able to exhibit these behaviors.\nFor example, adaptive methods using the adaptive pursuit strategy almost always created\nerratic probability profiles with changes in operator preference that did not reflect changes\nin the environment. Although these methods generally failed at condition 2, they were still\nable (in most cases) to select appropriate operators for traversing the landscape (thereby\nsatisfying condition 1).\n\n105\n\n\fChapter 3: Adaptation of EA Design\n\nWhen the ETV adaptive method was used without Outlier interpretation, it was very poor at\nresolving differences between search operators (i.e. it does not satisfy condition 3). This is\nnot surprising given the ETV distribution results (e.g. see Figure 3-8) which have indicated\nthat the ETV measurement is, on average, very similar for each event.\n\nNor was it\n\nsurprising that for a number of problems, its performance was very similar to that observed\nwith Static-Ops10 (where all search operators are used with equal probability). However,\nwhen the ETV-Outlier method was used, large differences in probability values were\nresolved between the operators (satisfying condition 1). This adaptive method exhibited\nstrong responses to changes in the environment (satisfying condition 2) yet these transitions\nwere largely non-existent in many of the other adaptive methods. Finally, the superior\nperformance of the ETV-Outlier method provides evidence that it also satisfies condition 3.\nIt is argued that the ETV-Outlier method exhibited this set of behaviors because of its\nability to focus solely on important events when making decisions of which operators to\nprefer. This requires both an accurate measure of an event's importance (provided by\nETV) and the ability to filter out events that represent neutral interactions between the\nadaptive system and its environment (provided by Outlier interpretation).\nPerformance Sensitivity to Test Function and NFL Implications: Algorithm\n\nperformance depended strongly on the set of test functions being considered.\n\nMany\n\nadaptive methods performed well on the first set of problems but were very bad on the\nsecond set (and vice versa). This did not occur nearly as much with ETV-Outlier which\nwas unexpected.\n\nThis adaptive method performed significantly better than the other\n\nadaptive and non-adaptive methods, although there were rare instances where this was not\nthe case. It has been argued in this chapter that the ETV-Outlier method is an effective\nadaptive procedure which implies that it is able to exploit landscape features for short-term\nperformance gains.\nHowever, it is worth pointing out that short-term performance gains do not guarantee longterm performance gains for certain fitness landscapes, particularly those with deceptive\nfeatures. Indeed, it is speculated that deceptive landscape features can prevent an otherwise\neffective adaptive method from providing long term benefits to the performance of an\noptimization algorithm. Some evidence of this may actually be seen in the results on the\n\n106\n\n\fChapter 3: Adaptation of EA Design\n\nMassively Multimodal Deceptive Problem (MMDP) where the ETV-Outlier method's\nperformance is particularly poor.\nIn a sense, this suggested tradeoff between short and long-term performance could be\ndemonstrating some form of NFL-related limitations that are expected to occur with any\nsolver. However, it is also postulated here that the subset of real-world problems, similar to\nthe artificial ones selected for these experiments, are actually a biased sampling from the\nspace of all possible fitness landscapes and as such will display certain landscape features\nmore often than others. For example, the subset of real-world problems is expected to be\ndominated by correlated versus uncorrelated and deceptive landscape characteristics [30].\nUnder these assumptions, exploitive adaptive mechanisms such as the ETV-Outlier could\nbe expected to work fairly well in many optimization problems as is evidenced to some\ndegree in this thesis. On the other hand, a real-world problem's fitness landscape is not\nnecessarily defined as being either deceptive or not deceptive. Instead, it is expected to\nhave varying degrees of both features, which suggests that's a robust search process should\nbe able to maintain high levels of both explorative and exploitive behaviors within a single\nsearch process. Such behaviors are not explicitly accounted for within the ETV-Outlier\nadaptive method nor has this issue been adequately addressed elsewhere in the adaptive\nliterature.\nAs previously implied, it is possible that an adaptive method could actually be detrimental\nto a search algorithm within certain contexts. To adequately assess potential shortcomings,\nit is not only necessary to test the adaptive mechanism on a diverse set of fitness landscapes\nbut also important to test a diverse range of search operators.\n\nBy including highly\n\nexploitive search operators for instance, it is possible to see if exploitive behavior will work\nagainst the adaptive method (e.g. by encouraging premature convergence within\nmultimodal fitness landscapes). This was accounted for in this thesis by creating the highly\nexploitive swap and creep operators which were included in the list of search operators\nused by the adaptive methods within this chapter. A possibly better test of an adaptive\nmechanism's limitations might be to include local search operators which involve a greedy\nand more exploitive multi-step search within a single operator.\nAlthough the conditions tested in this thesis did not appear to expose particular weaknesses\nin the ETV-Outlier method, this does not mean that this adaptive method can fully address\n107\n\n\fChapter 3: Adaptation of EA Design\n\nthe tradeoff between short and long-term performance in all problems. Ultimately, this\ntradeoff can only be dealt with by using either an iterative search which learns about the\nfitness landscape or possibly by extending the timescale of fitness measurements which are\nused by the adaptive method (thereby making the timescale for short-term performance\ngains not as short).\n\nConsidering that the ETV measurement itself can take multiple\n\ngenerations to calculate (compared to a single function evaluation for other adaptive\nmethods) it is possible that this longer measurement time scale is a contributor to ETV's\nexceptional performance.\nComparing ETV and fitness measurements: The Event Takeover Value or ETV has\n\nbeen put forth as a method for measuring an individual's impact on population dynamics.\nComparisons between ETV and fitness-based ranking measurements have shown that a\ncorrelation does exist however the scaling and distribution of the measurements is\ndramatically different. Most importantly, it was found that very few individuals have any\nsignificant impact on population dynamics.\n\nThis was interpreted to mean that most\n\ninteractions between the adaptive system and its environment are effectively neutral. This\nconclusion fits well with observations of other adaptive systems in nature and it is possible\nthat power law scaling (see Figure 3-8) is a general feature of interactions between many\nadaptive systems and their environment. This phenomena may even be the motivating\nforce for the repeated emergence of threshold phenomena in biological systems (e.g. in\ngene regulation, neural activation).\nAs an alternative to defining an arbitrary threshold, statistical arguments were used in this\nchapter to quantify the importance of interactions between an adaptive system and its\nenvironment.\n\nThis eliminated any need for threshold tuning and provided strong\n\nperformance gains in the ETV-Outlier adaptive method.\nETV adaptation as a generic tool for optimization: There are a few important issues that\n\nstill need to be addressed before the ETV-Outlier adaptive method can be readily\nimplemented as a generic add-on tool for multi-search operator metaheuristics. First,\nalthough the results in this chapter were promising, it is necessary to test the adaptive\nmethod on additional problems, particularly application-inspired problems. There are many\nEA application domains where a large number of specialized search operators have been\nproposed in the literature (e.g. scheduling problems) and where it is not clear which search\n108\n\n\fChapter 3: Adaptation of EA Design\n\noperators should be used for which problem instances. These problems would provide an\nideal environment for testing the performance of an adaptive method in its ability to\nadvantageously select and control operator usage.\nThere are also some potential drawbacks with the ETV-Outlier method that should be\npointed out. One possible concern with the ETV-Outlier method is its computational\nefficiency. Although Section 3.2.2.4 indicates that memory costs are actually small and\nthat memory and computational costs scale linearly with population size, the method still\ncould be deemed to be somewhat computationally costly when used on simple test\nfunctions (which is where most EC research takes place). It is also worth pointing out that\nthe adaptive method is quite complicated in comparison to the elegance of the original GA,\nrequiring a substantial degree of record keeping and statistical tests. This does not make it\ndifficult to implement per se but could make the algorithm difficult to understand and act as\na potential deterrent to its use.\nFinal Remarks: It is generally understood that individuals in an EA population have a\n\nusefulness in the overall search process which extends beyond their individual genotype\nand phenotype. However, few if any previous attempts have been made in measuring how\nindividuals impact the search process or have considered ways in which this information\nmight be used to improve algorithm performance. This chapter attempted to make some\ninroads into this topic using metrics derived from genealogical graphs. It was also pointed\nout that this new ETV measurement involves a new type of search bias assumption that was\nlabeled as Empirical Bias and is notably distinct from the standard Hill Climbing\nAssumption. The experimental results provided evidence that the adaptive method ETVOutlier has many of the characteristics that are desired in an adaptive procedure and are\narguably missing in previous methods for adapting EA design parameters.\n\n109\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nChapter 4\n\nLarge Scale Features of EA Population\n\nDynamics\n\nThe previous chapter presented the Event Takeover Value (ETV) as a way to measure an\nindividual's impact on EA population dynamics. The ETV is able to approximate an\nindividual's impact on population dynamics through an analysis of EA genealogical graphs.\nFrom preliminary tests in Chapter 3, it was found that the ETV probability distribution fits\na power law with an exponent of approximately 2. This distribution indicates that a large\nproportion of individuals do not significantly impact EA population dynamics while a small\nminority of individuals dominate population dynamics.\nThe aim of this chapter is to gain a better understanding of the population dynamics of\nEvolutionary Algorithms using the ETV measurement derived in Chapter 3. In particular,\nthis chapter investigates what experimental conditions can significantly impact the ETV\ndistribution. After a broad range of conditions are tested in Section 4.1, it is concluded that\nonly i) the population topology and ii) the introduction of completely new (i.e. randomly\ncreated) individuals can result in significant changes to the ETV distribution. If the EA\npopulation topology is a fully connected graph or if no new individuals are inserted into the\npopulation then the ETV distribution is found to be well approximated by a power law.\nHowever, when these conditions are not met, the ETV displays power law deviations for\nlarge ETV sizes. From these power law deviations, it is concluded that these EA designs\nare not capable of being dominated by a small number of individuals and hence are able to\nexhibit a higher degree of parallel search behavior.\nSection 4.2 reviews and discusses several studies on the spatial and temporal properties of\nnatural evolutionary dynamics which are found to exhibit similarities to the results\npresented in this chapter. Although the actual form of the measurements used to study\nnatural evolution is not identical to the ETV measurements used here, these results do\nsuggest that power law behavior and scale-invariant properties are prevalent in evolution.\n\n110\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nSection 4.3 describes the Theory of Self-Organized Criticality and presents the theory as a\npossible explanation for the spatial and temporal patterns observed in EA and natural\nevolution. The chapter is concluded with Section 4.4 which discusses the relevance of\nthese results to EA research and provides some motivations for the final chapter of this\nthesis.\n\n4.1 Analysis of EA dynamics using ETV\nThis section studies EA population dynamics using the ETV measurement. Section 4.1.1\nfirst describes the experimental conditions that are used throughout this chapter. Section 0\nthen investigates the experimental conditions that affect the distribution of ETV sizes in EA\npopulation dynamics.\n\nSection 4.1.3 follows with an investigation of the conditions\n\naffecting the distribution of ETV ages where the age is the total amount of time that an\nindividual is able to influence EA population dynamics.\n\n4.1.1\n\nExperimental Setup\n\nThe experiments presented in this chapter were conducted using a number of artificial test\nproblems. Definitions and problem descriptions are provided in Appendix A. A number of\nEvolutionary Algorithm designs have also been used in these experiments as elaborated on\nbelow.\n\n4.1.1.1\n\nPanmictic EA designs\n\nThe Panmictic EA design refers to the standard EA design where spatial restrictions are not\nimposed on the population. A high level pseudocode is given below with the parent\npopulation of size \u03bc at generation t defined by P(t). For each new generation, an offspring\npopulation P`(t) of size \u03bb is created through variation of the parent population. The parent\npopulation for the next generation is then selected from P`(t) and Q, where Q is subset of\n\nP(t). Q is derived from P(t) by selecting those in the parent population with an age less\nthan or equal to \u03ba.\n\n111\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nPseudocode for Panmictic EA designs\nt=0\nInitialize P(t)\nEvaluate P(t)\nDo\nP`(t) = Variation(P(t))\nEvaluate (P`(t))\nP(t+1) = Select(P`(t) \u222a Q)\nt=t+1\nLoop until termination criteria\nPopulation updating: The generational (Gen) EA designs that were tested in these\n\nexperiments used elitism for retaining the best parent and parameter settings \u03bc=N/2, \u03bb=N,\n\n\u03ba=1 (\u03ba=\u221e for best individual). The steady state (SS) EA design that was used in these\nexperiments actually involves a pseudo steady state population updating strategy with\nparameter settings \u03bc=\u03bb=N, \u03ba=\u221e.\nSelection: Selection occurs by either binary tournament selection without replacement\n\n(Tour), truncation selection (Trun), or random selection (Rand). Random selection is\nimplemented in the same fashion as binary tournament selection except the winner of a\ntournament is chosen at random (without regard for fitness of the individuals).\nSearch Operators: For each EA design, an offspring is created by using a single search\n\noperator that is selected at random from the list in Table 4-1. Search operator descriptions\nare provided in Appendix B.\nCrowding: Crowding in Panmictic populations was implemented using Deterministic\n\nCrowding (DC) which is described in Chapter 2.\n\n4.1.1.2\n\nSpatially Distributed Populations\n\nAll distributed EA designs that are tested in this chapter involve a cellular Genetic\nAlgorithm (cGA) which is described in the pseudocode below. The algorithm starts by\ndefining the initial population P on a ring topology with each node connected to exactly\ntwo others. For a given generation t, each node in the population is subject to standard\ngenetic operators. Each node N1 is selected as a parent and a second parent N2 is selected\namong all neighbors within a radius R using linear ranking selection. An offspring is\n112\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\ncreated using the two parents plus a single search operator selected at random from the list\nin Table 4-1. The better fit between the offspring and N1 is then stored in a temporary list\nTemp(N1) while genetic operators are used on each of the remaining nodes in the\npopulation. To begin the next generation, the population is updated with the temporary list.\nThis process repeats until some stopping criteria is met.\nPseudocode for cGA\nt=0\nInitialize P(t) (at random)\nInitialize population topology (ring structure)\nEvaluate P(t)\nDo\nFor each N1 in P(t)\nSelect N1 as first parent\nSelect N2 from Neighborhood(N1,R)\nSelect Search Operator (at random)\nCreate and evaluate offspring\nTemp(N1) = Best_of(offspring, N1)\nNext N1\nt=t+1\nP(t) = Temp()\nLoop until stopping criteria\nCrowding: Distributed EA designs that include crowding procedures are modified so that\n\nthe offspring competes with the parent (N1 or N2) that is most similar in phenotype.\nTable 4-1 Names of the seven search operators used in the cellular GA and Panmictic EA designs are listed below.\nMore information on each of the search operators can be found in Appendix B.\n\nSearch Operator Names\nWright's Heuristic Crossover\nSimple Crossover\nExtended Line Crossover\nUniform Crossover\nBLX- \u03b1\nDifferential Operator\nSingle Point Random Mutation\n\n113\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\n4.1.2\n\nETV Size Results\n\nThis section is concerned with determining what experimental conditions can influence the\nETV distribution. There are many aspects of an EA design that have been modified or\nextended over the years meaning that any attempt at making broad statements about EA\npopulation dynamics requires a broad range of experimental conditions to be tested.\nBecause a large number of experiments were necessary, only selected results are presented\nbased on their capacity to illuminate system behavior.\n\nSection 4.1.2.1 looks at the impact\n\nthat EA design features have on the ETV distribution while Section 4.1.2.2 investigates the\nimpact of the fitness landscape. Section 4.1.2.3 follows up with an investigation of whether\nthe ETV distribution is sensitive to the amount of time that evolution is observed.\n\n4.1.2.1\n\nImpact of EA design\n\nThe first EA design factors tested consisted of selection methods and population updating\nstrategies for Panmictic EA designs, with results shown in Figure 4-1a and Figure 4-1b.\nSelection pressures varied from very weak (e.g. random selection) to very strong (e.g.\ntruncation selection) and the population updating strategy varied from infinite maximum\nlife spans (steady state) to single generation life spans (generational). The most remarkable\nconclusion from these results is that the ETV distribution has very little sensitivity to these\ndesign factors and consistently takes on a power law distribution. Particularly surprising\nwas the results using random selection, which has no sensitivity to the fitness landscape of\nthe test problem being used. When random selection is used, the ETV distribution appears\nto take on a slightly smaller distribution tail although a power law is still clearly observed.\nExperiments were also conducted to determine the impact of the population size. As seen\nin Figure 4-2a, EA designs which differ only in the value of N have nearly identical ETV\ndistributions. The insensitivity to N was also observed for the other EA designs tested in\nFigure 4-1a and Figure 4-1b with N varying from 50 to 400 (results not shown).\nThe results in Figure 4-2 present what was found to be the most important factor impacting\nthe ETV distribution.\n\nThese experiments, which were run using the cellular Genetic\n\nAlgorithm, found that spatial restrictions result in power law deviations for large ETV\nsizes. Furthermore, the extent of the deviation was clearly dependent upon the degree of\n114\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nspatial restrictions in the system. As seen in Figure 4-2b, the use of random selection\nchanges the exponent of the power law (that best approximates the data) however power\nlaw deviations are still present.\n\na)\n\n1\n\nGen-Tour-N200\n0.01\n\nGen-Trun-N200\n\nP(ETV)\n\nGen-Rand-N200\n\n0.0001\n\n0.000001\n1\n\n10\nETV\n\n100\n\nb)\n\nSS-Tour-N200\nSS-Trun-N200\n\n0.1\n\nP(ETV)\n\nSS-Rand-N200\n\n0.001\n\n0.00001\n\n0.0000001\n1\n\n10\n\nETV\n\n100\n\n1000\n\nFigure 4-1 ETV size distributions for a number of panmictic EA designs. a) EA designs with population size\nN=200, generational population updating (Gen), and selection methods Tournament (Tour), Truncation (Trun)\nand Random (Rand) selection.\nSolid line represents a power law with exponent 2.5. b) EA designs with\npopulation size N=200, steady state (SS) population updating, and selection methods Tournament (Tour),\nTruncation (Trun) and Random (Rand) selection. Solid line represents a power law with exponent 2.3. Results\nfrom each EA design are taken over 20,000 generations of evolution on the 30-D Hyper Ellipsoid test function.\n\n115\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\n1\n\na)\n\ncGA-R1-N200\ncGA-R5-N200\ncGA-R30-N200\n\n0.1\n\ncGA-R1-N100\ncGA-R5-N100\n\nP(ETV)\n\n0.01\n\ncGA-R30-N100\n\n0.001\n\n0.0001\n\n0.00001\n\n0.000001\n1\n\nb)\n\n10\n\nETV\n\n100\n\n1000\n\n1\ncGA-R1-N200-Rand\ncGA-R5-N200-Rand\ncGA-R30-N200-Rand\n\nP(ETV)\n\n0.01\n\n0.0001\n\n0.000001\n1\n\n10\n\nETV\n\n100\n\n1000\n\nFigure 4-2 ETV size distributions for a number of spatially distributed EA designs. a) Cellular Genetic Algorithm\n(cGA) designs with population sizes (N=100, N=200), and neighborhood radius (R=1, R=5, R=30). Solid line\nrepresents a power law with exponent 2.2. b) Cellular Genetic Algorithm (cGA) designs with random selection\n(Rand), population size (N=200), and neighborhood radius (R=1, R=5, R=30). Solid line represents a power law\nwith exponent 2.5. Results from each EA design are taken over 20,000 generations of evolution on the 30-D Hyper\nEllipsoid test function.\n\nGiven that spatial restrictions are so far the only EA design factor significantly influencing\nthe ETV distribution, it was decided to consider other mechanisms for restricting\ninteractions within an EA population. A common approach for restricting interactions are\nso called crowding methods where offspring are forced to compete with similar individuals\nin the population.\n\nThe results in Figure 4-3 show that crowding does have a significant\n\nimpact on the ETV distribution, but only for spatially distributed EA designs. For all but\n116\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nthe smallest ETV value (ETV=1), the use of crowding decreases the ETV's probability of\noccurrence by roughly an order of magnitude in the cGA. However, for Panmictic EA\ndesigns, the use of crowding did not appear to have a significant impact on the ETV\ndistribution which is demonstrated using Deterministic Crowding (see inset of Figure 4-3).\n\n1.E-01\n0.1\n\n1.E-03\n\nDC-N200\n\n1.E-05\n\nP(ETV)\n\n1.E-07\n1\n\n0.001\n\n10\n\n100\n\n1000\n\ncGA-R1-N200\n0.00001\n\ncGA-R5-N200\ncGA-R30-N200\ncGA-crowding-R1-N200\ncGA-crowding-R5-N200\ncGA-crowding-R30-N200\n\n0.0000001\n1\n\n10\n\nETV\n\n100\n\n1000\n\nFigure 4-3 ETV distributions shown primarily for spatially distributed EA designs. All EA designs have\npopulation size N=200. Cellular Genetic Algorithm (cGA) designs vary in the use of crowding and the\nneighborhood radius size (R=1, R=5, R=30). Results from using Deterministic Crowding (DC) are also presented\nin the inset. Solid line represents a power law with exponent 2.2. Results from each EA design are taken over\n20,000 generations of evolution on the 30-D Hyper Ellipsoid test function.\n\nAll results presented thus far have been taken with evolution occurring on the 30-D HyperEllipsoid test function. This test function was selected because each of the EA designs\nwere able to evolve for long periods of time (achieving 1 million to 10 million events)\nwhich allowed for greater clarity in the distribution results. The next section addresses the\nimpact of the fitness landscape.\n\n4.1.2.2\n\nFitness Landscape Dependencies\n\nThe fitness landscape that an EA population evolves on will obviously impact the trajectory\nthat the population takes through parameter space. Hence, it came as a surprise to find how\nlittle the fitness landscape influenced the ETV distribution results. Test functions were\nselected from Appendix A and include unimodal and multimodal functions, linear and\nnonlinear functions, functions with strong and weak epistasis, as well as deceptive and nondeceptive functions.\n\nResults shown in Figure 4-4 and Figure 4-5 demonstrate little\n117\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nsensitivity to the fitness landscape on which evolution occurs. Other Panmictic EA designs\nwere also tested with similar results (results not shown). For the distributed EA results,\nsome sensitivity to the fitness landscape was observed when strong spatial restrictions were\npresent in the EA population (e.g. see Figure 4-5a). However, the general conclusions from\nthe previous section remain unchanged; only spatial restrictions in the EA population result\nin significant changes to the ETV distribution.\nAnother way to test the influence of the fitness landscape on ETV results is to use a random\nselection pressure as was done in the previous section. The use of random selection in an\nEA design is similar to evolving on a completely flat fitness landscape. The use of\ndifferent search operators is also expected to have an impact that is similar to changing the\nfitness landscape. Some preliminary work has tested the use of different search operators\n(results not shown) and this was found to have a similar effect to varying the test function\nalthough these results displayed even less sensitivity (possibly due to the range of operators\nused).\n\n118\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\na)\n\nSS-Trun-N200\n1\n\nP(ETV)\n\n0.01\n\nMTTP\n\nECC\n\nMMDP\n\nFreq Mod\n\nQuadratic\n\nRosenbrock\n\nRastrigin\n\nSchwefel\n\nGriewangk\n\nSys. Lin. Eqs.\n\nAckley's\n\nNeumaier's #2\n\n0.0001\n\n0.000001\n1\n\nb)\n\n10\n\nETV\n\n100\n\n1000\n\nGen-Tour-N200\n1\nMTTP\nMMDP\nQuadratic\nRastrigin\nGriewangk\nAckley's\n\nP(ETV)\n\n0.01\n\nECC\nFreq Mod\nRosenbrock\nSchwefel\nSys. Lin. Eqs.\nNeumaier's #2\n\n0.0001\n\n0.000001\n1\n\n10\n\nETV\n\n100\n\n1000\n\nFigure 4-4 ETV distributions shown for selected EA designs on a range of test functions taken from Appendix A.\nEvolution occurred over 2000 generations and results shown are averages taken over 10 runs. To help in viewing\nresults from a large number of test functions, data is grouped into bins. a) Results for an EA design using steady\nstate (SS) population updating, truncation selection (Trun), and population size N=200. Solid line represents a\npower law with exponent 2.2. b) Results for an EA design using generational (Gen) population updating,\ntournament selection (Tour), and population size N=200. Solid line represents a power law with exponent 2.2.\n\n119\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\na\n\ncGA-R1-N200\n1\n\nP(ETV)\n\n0.01\n\nMTTP\n\nECC\n\nMMDP\n\nFreq Mod\n\nQuadratic\n\nRosenbrock\n\nRastrigin\n\nSchwefel\n\nGriewangk\n\nSys. Lin. Eqs.\n\nAckley's\n\nNeumaier's #2\n\n0.0001\n\n0.000001\n1\n\n10\n\nb\n)\n\nETV\n\n1000\n\ncGA-R30-N200\n0.1\n\nP(ETV)\n\n100\n\n0.001\n\nMTTP\n\nECC\n\nMMDP\n\nFreq Mod\n\nQuadratic\n\nRosenbrock\n\nRastrigin\n\nSchwefel\n\nGriewangk\n\nSys. Lin. Eqs.\n\nAckley's\n\nNeumaier's #2\n\n0.00001\n\n0.0000001\n1\n\n10\n\nETV\n\n100\n\n1000\n\nFigure 4-5 ETV distributions shown for selected EA designs on a range of test functions taken from Appendix A.\nEvolution occurred over 2000 generations and results shown are averages taken over 10 runs. To help in viewing\nresults from a large number of test functions, data is grouped into bins. a) Results for a distributed EA design\n(cGA) using neighborhood radius R=1, and population size N=200. Solid line represents a power law with\nexponent 2.2. b) Results for a distributed EA design (cGA) using neighborhood radius R=30, and population size\nN=200. Solid line represents a power law with exponent 2.2.\n\n4.1.2.3\n\nImpact of time length of evolution\n\nIn this section, tests were conducted with evolution taking place over different lengths of\ntime. As seen in Figure 4-6, during the initial stages of evolution, the ETV distribution\n\n120\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\ndisplays power law deviations for large ETV sizes however these deviations disappear as\nevolution is observed over longer periods of time.\n\na)\n\nSS-Trun-N200\n1\n100\n500\n2000\n10000\n100000\n\nP(ETV)\n\n0.01\n\n0.0001\n\n0.000001\n1\n\nb)\n\n10\n\nETV\n\n100\n\n1000\n\ncGA-R1-N200\n1\n100\n500\n2000\n10000\n100000\n\nP(ETV)\n\n0.01\n\n0.0001\n\n0.000001\n1\n\n10\nETV\n\n100\n\nFigure 4-6 ETV distribution results as a function of the time span of evolution. a) Results for an EA design using\nsteady state (SS) population updating, truncation selection (Trun), and population size N=200. Solid line\nrepresents a power law with exponent 2.2. b) Results for a distributed EA design (cGA) using neighborhood radius\nR=1, and population size N=200. Solid line represents a power law with exponent 2.5. Data sets are labeled by a\nnumber which indicates the number of ETV measurements that are used to generate the distribution. For each EA\nrun, the first 100 events are given to the first data set, the next 500 are given to the next data set and so on. Results\nfor each EA design are averages over ten runs.\n\nThe fact that ETV distribution results have only a brief transient where the distribution is\nsensitive to time, but is insensitive thereafter, indicates that the distribution approaches a\nstationary state. However, record statistics of ETV in Figure 4-7 provide evidence that\nmaximum ETV sizes have an initial time dependency. This could mean that the system\n121\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\ndoes not initially start with population dynamics being defined by a power law distribution\nbut instead that the system evolves to achieve that state over time.\n\nETV (Max)\n\n100\n\n10\n\ncGA-R1-N100\ncGA-R5-N100\ncGA-R20-N100\n\n1\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n10000\n\nEvents\nFigure 4-7 Record ETV statistics for cellular Genetic Algorithms (cGA) with population size N=100 and\nneighborhood radius (R=1, R=5, R=30). ETV(Max) is the largest ETV found in every 200 events. Values are\naverages over 10 experimental replicates.\n\nIt has been determined that the reason for this initial dynamical behavior is actually due to\nthe lack of a genealogical or a historical coupling between individuals in the initial\npopulation. To confirm this, Figure 4-8 shows ETV distribution results where each new\noffspring has a probability Pnew of being historically uncoupled from the rest of the\npopulation. Historical uncoupling is simply done by preventing offspring from inheriting\nhistorical data from their parents.\n\nFrom a population dynamics perspective, this is\n\nequivalent to an EA design which includes a steady introduction of new individuals into the\nEA population. As seen in Figure 4-8, a small amount of historical uncoupling can result in\npower law deviations for the largest ETV sizes. However, as Pnew is increased, the extent\nthat the distribution deviates from a power law is found to increase only slightly.\n\n122\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\n1\nP(new)=0.3\nP(new)=0.1\nP(new)=0.04\nP(new)=0.03\n\nP(ETV)\n\n0.01\n\n0.0001\n\n0.000001\nETV\n\n1\n\n10\n\n100\n\nFigure 4-8 ETV distributions with varying amounts of historical uncoupling in EA population dynamics.\nExperiments are conducted with a steady state EA using truncation selection and population size N=100.\nEvolution took place over 20,000 generations on the 30-D Hyper Ellipsoid test function. When conducting the\nstandard ETV calculation, historical event information is copied from the genetically dominant parent to its\noffspring. In these experiments, the step of historical transfer is skipped with probability Pnew. The solid line in the\ngraph represents a power law with exponent = 2.1\n\n4.1.2.4\n\nOther Experimental Conditions\n\nAdditional tests were also conducted (results not shown) to help ensure that the ETV\ndistribution results that have been presented so far in this chapter were not biased due to\nother experimental factors.\n\nThis included experiments on selected EA designs at\n\npopulation sizes up to N=500, running evolution up to 100,000 generations, and\nexperiments with the ETV calculation parameter Tobs set as high as 500.\n\nThese\n\nexperiments resulted in no observable changes to ETV distribution results.\n\n4.1.3\n\nETV Age Results\n\nIn addition to measuring the size of an individual's impact, one can also measure the\namount of time that an individual is able to impact population dynamics. This is measured\nby recording the number of generations required for an individual's ETV calculation to\nfinish, which is referred to in this thesis as the ETV age. This section investigates this\naspect of EA dynamics more closely, again with the aim of determining what experimental\nconditions impact the ETV age distribution.\n\n123\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nLooking at Figure 4-9 and Figure 4-10, these results demonstrate that the ETV age\nrepeatedly approximates a power law however different sensitivities to EA design\nconditions have emerged (compared with ETV size distribution results shown previously).\nAlthough there is still no sensitivity to the population size, these results reveal that the\nselection method and population updating strategy do have an impact on the ETV age\ndistribution for Panmictic populations. This is seen for instance in the results presented in\nFigure 4-10b where EA designs with steady state population updating and tournament\nselection are found to have a clear power law deviation for large ages. On the other hand,\nthe introduction of spatial restrictions to the EA population does not have any influence on\nthis characteristic of population dynamics as seen in Figure 4-9.\n\nThis is surprising\n\nconsidering the importance of spatial restrictions in the previous ETV size distribution\nresults. Also shown in Figure 4-9, the addition of crowding to the cGA has a completely\nunexpected impact on the age distribution and appears to result in an almost log-periodic\nbehavior that on average still tends toward a power law distribution. On the other hand, the\naddition of crowding in Panmictic Populations (e.g. Deterministic Crowding) was found to\nhave little influence on the age distribution. In summary, these results indicate that most\nETV age distributions are well approximated by power laws although changes to the\ndistribution shape do occur under certain conditions.\n\n124\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\ncGA-R1-N200\ncGA-R5-N200\ncGA-R30-N200\ncGA-crowding-R1-N200\ncGA-crowding-R5-N200\ncGA-crowding-R30-N200\nDC-N200\n\nP(Age)\n\n0.1\n\n0.001\n\n0.00001\n\n0.0000001\n1\n\n10\n\nAge\n\n100\n\n1000\n\nFigure 4-9 ETV age distributions shown primarily for spatially distributed EA designs. The age of an ETV is\ndefined by the number of generations from the initial event to the completion of the ETV calculation. All EA\ndesigns have population size N=200. Cellular Genetic Algorithm (cGA) designs vary in the use of crowding and the\nneighborhood radius size (R=1, R=5, R=30). Results from using Deterministic Crowding (DC) are also provided\nfor a population size of N=200. Solid line represents a power law with exponent 3.2.\n\nAs a final comment on these results, it is also worth mentioning that although the ETV has\na maximum size equal to N, the ETV age measured here is only constrained by the amount\nof time that the system is observed. For these experiments, evolution was observed for up\nto 20,000 generations and ETV ages were found approaching 1000 without any evidence of\npower law deviations at large ages.\n\nBased on the observed distributions, it is concluded\n\nthat the maximum age of events in EA dynamics is only limited by the amount of time that\nevolution is allowed to take place.12\n\n12\n\nThe maximum age can also be limited by the ETV calculation procedure, for instance by limits placed on\nthe size of the historical records kept in the EA population. In these experiments, the maximum record size\nwas set to Tobs=240 and under these conditions, it was found that roughly one in every 50,000 events failed to\nfinish the ETV calculation before reaching a maximum record position.\n\n125\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\na)\nGen-Tour-N50\n\n0.1\n\nGen-Tour-N200\n\nP(Age)\n\n0.001\n\n0.00001\n\n0.0000001\n1\n\n10\n\nAge\n\n100\n\n1000\n\nb)\nSS-Tour-N50\n\n0.1\n\nP(Age)\n\nSS-Tour-N200\n\n0.001\n\n0.00001\n\n0.0000001\n1\n\n10\n\nAge\n\n100\n\n1000\n\n126\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nc)\n\nSS-Trun-N50\n\n0.1\n\nP(Age)\n\nSS-Trun-N200\n\n0.001\n\n0.00001\n\n0.0000001\n1\n\n10\n\nAge\n\n100\n\n1000\n\nFigure 4-10 ETV age distributions for several EA designs. The age of an ETV is defined by the number of\ngenerations from the initial event to the completion of the ETV calculation. a) EA designs with population sizes\n(N=200, N=50), generational population updating (Gen) and Tournament selection (Tour). Solid line represents a\npower law with exponent 3. b) EA designs with population sizes (N=200, N=50), steady state population updating\n(SS), and Tournament selection (Tour). Solid line represents a power law with exponent 2.5. c) EA designs with\npopulation sizes (N=200, N=50), steady state population updating (SS), and Truncation selection (Trun). Solid line\nrepresents a power law with exponent 3.5.\n\n4.1.3.1\n\nCaveats\n\nIt should be mentioned that, despite considerable efforts, the experiments were not\nexhaustive and so it is possible that other EA designs and certain landscape characteristics\ncould result in ETV distributions which deviate from a power law or are otherwise different\nfrom what was presented here. As an example, EA designs which parameterize the amount\nof interaction between population subgroups (i.e. island model population structure) could\nbe one unaccounted for situation where power laws would only be observed with the\nappropriate parameter tuning.\n\n4.1.4\n\nConclusions\n\nA number of conclusions can be drawn from the results presented in this chapter. First, it\nwas found that the probability of an individual's impact on EA dynamics fits a power law\n(exponent between 2.2 and 2.5). This is a robust property of the system which is largely\ninsensitive to most experimental conditions including changes to population size, search\n\n127\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\noperators, fitness landscape, selection scheme, population updating strategy, and the\npresence of crowding mechanisms.\nTwo experimental conditions were however found to result in power law deviations for\nlarge ETV sizes. The first is the steady introduction of new individuals that have no\nrelation to others in the population (i.e. historically uncoupled). The second condition is\nthe introduction of spatial restrictions into an EA population.\n\nUsing either of these\n\nconditions effectively removes the possibility of single individuals dominating the\ndynamics of the entire population. The associated power law deviations can be understood\nas an indicator of parallel computation within the system.\nThe amount of time than an individual influences EA dynamics (i.e. ETV age) also was\nfound to fit a power law with most individuals influencing the system for only brief periods\nof time. However, as suggested by the power law relation, there is a non-negligible\nprobability that an individual will influence EA dynamics over very large time scales. This\nbehavior was found to be robust and was almost completely insensitive to all experimental\nconditions tested.\n\n4.2 Discussion: Comparisons between EA and nature\nFrom the last section it was concluded that EA population dynamics exhibit power laws in\nETV spatial and temporal properties with little sensitivity to experimental conditions.\nSome of the measurements that have been taken of the spatial and temporal properties of\nnatural evolution have also been found to exhibit power law relations. This section briefly\nreviews the results from natural evolution and compares and contrasts them the results from\nthis chapter.\nIt is important to point out that no known measurements of natural evolution are exactly\nequivalent to ETV and so strong conclusions about similarities or differences in behavior\nare not possible. Instead, this section is provided to simply review and discuss current\nevidence that spatial and temporal patterns in EA population dynamics are similar to those\nobserved in natural evolution.\n\n128\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nSection 4.2.1 reviews past studies on extinction size distributions that are derived from the\nfossil record and compares this with ETV size distribution results. Section 4.2.2 reviews\npast studies on the distribution of species life times and compares this with ETV age\ndistribution results.\n\nFinally, Section 4.2.3 looks at some topological properties of\n\ntaxonomic structure in natural evolution and compares this with the genealogical structure\nof EA populations.\n\n4.2.1\n\nExtinction Sizes\n\nThe first large scale feature of evolutionary dynamics that is discussed deals with the\ncharacterization of extinction event sizes. Extinction event sizes are measured as the\nnumber of species (or percentage of species) which become extinct over a predefined time\ninterval. Using fossil data that has been compiled by Sepkoski [169], several studies have\nanalyzed extinction records [170], [171] and have found the distribution of extinction sizes\nto be very broad and well approximated by a power law. This is shown for instance in\nFigure 4-11, which is taken from [171]. In this figure, a best fit \"kill curve\", which was\ndeveloped in [170], is used to create the extinction distribution for Paleozoic marine\nspecies. Using this model, a power law distribution is clearly observed for all but the very\nlargest extinction events.\n\n129\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nFigure 4-11 Probability of an extinction event as a function of the fraction of all species killed. The distribution is\nderived based on a best of fit kill curve (see [170]) using fossil data of marine species from the Paleozoic era.\nReprinted by permission from the Royal Society (Proceedings: Biological Sciences) [171], copyright (1996).\n\nComparing Extinction sizes and ETV: The ETV measurement is similar in some ways to\n\nthe measurement of extinction sizes in natural evolution however there are also some\nimportant differences.\n\nThe most significant similarity is that both the ETV and the\n\nextinction size are a measure of the magnitude of changes that are taking place within each\nof their respective systems. On the other hand, while ETV is measuring the spread of\ngenetic material in a population, the extinction size is measuring the removal of species\n(which can also be thought of as a loss of genetic material). Another important difference\nis that ETV measures the changes resulting from a particular event while extinction sizes\nfrom the fossil record look at changes occurring over a time window.\nComparing Results: Despite these difference, the broad degree distributions for ETV\n\nsizes is found to approximate a power law (exponent = 2.2 to 2.5) which is arguably similar\nto what has been observed in nature (exponent ~ 2) for extinction sizes. However, it does\nappear from Figure 4-11 that extinction sizes in natural evolutionary dynamics exhibit\npower law deviations for large extinctions while this is largely not the case in most of the\nETV distribution results.\nSome have suggested [172] that the power law deviations in the fossil record are a result of\nthe system being in a state of disequilibrium and that more time is needed before a power\nlaw is observed. This is similar to the argument in [173] and shown in Figure 4-12, that\npower law deviations can occur due to finite size effects suggesting that a larger time\nwindow is needed before the fossil record can display a clear power law. Assuming for the\n130\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nmoment that the power law behavior in EA and natural evolution has a similar origin, the\nETV results from this chapter would then seem to support the disequilibrium argument. As\ndemonstrated in Figure 4-6, running an EA for smaller periods of evolution results in ETV\ndistributions with power law deviations for large ETV sizes.\nHowever, it is possible that other factors contribute to power law deviations in the fossil\nrecord. In the experiments with EA, it was also found that spatial restrictions were a\nprimary cause of power law deviations. Obviously, some amount of spatial restriction is\npresent in natural evolution (e.g. geographical isolation) and it is speculated that this at least\ncontributes to the observed power law deviations for extinction sizes in nature.\n\n4.2.2\n\nSpecies Lifetime Distributions\n\nAnother way to characterize natural evolutionary dynamics is to measure the lifetimes of\nspecies (or other taxa) as shown for example in Figure 4-12. Several studies [173], [169]\nhave found a broad distribution of lifetimes however there is disagreement as to whether\nsome of the reported results fit an exponential function [174], [175] or a power law, [173],\n[176].\nThe difference actually has great relevance to our understanding of evolution. As explained\nin Section 4.1.2 of [177], an exponential distribution would indicate that the age of a\nspecies has no impact on its likelihood of survival. In other words, older species are not\nbetter adapted to their environment compared to newer species. However, a more broad\ndistribution such as a power law would indicate a correlation exists between age and\nextinction probability meaning that older species are better adapted compared to newer\nspecies.\n\nThe results taken from [173] and presented in Figure 4-12 indicate that a\n\ncorrelation does exist and that the age distribution does fit a power law.\n\n131\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\na)\n\nb)\n\nFigure 4-12 Local lifetime distributions for species based on North American bird populations. a) Lifetime\ndistributions for data taken over different timescales. Power law deviations are clearly present. b) Lifetime\ndistributions with rescaling of data to account for finite size effects. Data is now well approximated by a power\nlaw. Reprinted by permission from Macmillan Publishers Ltd: (NATURE) [173], copyright (1998).\n\nComparing Species Lifetimes and ETV ages: The ETV age measurement from Section\n\n4.1.3 and the lifetime of a species in natural evolution are both measurements of relevant\ntimescales of events in their respective systems. However, the two measurements are also\ndifferent for a number reasons. For instance, if one thinks of events in EA population\ndynamics as being speciation events, then the species lifetime measurement for an EA\nwould simply be the life time of individuals in the EA population. The ETV age, on the\nother hand, measures the total lifetime of all species that contain a strong genealogical link\nto an original speciation event. There are no known studies of the fossil record which have\nattempted an analogous metric of natural evolutionary dynamics.\nComparing Results: Despite these differences, it is still interesting to note that both ETV\n\nage distributions and species lifetime distributions are very broad and are well\napproximated by power laws. However the power law exponents are quite distinct with the\nETV age distribution having an exponent of 2.5 to 3.5 while the results on bird species\ntaken from [173] have an exponent of 1.6.\n\n4.2.3\n\nFractal Taxonomic Structures\n\nYet another way to characterize natural evolutionary dynamics is to measure the\ntopological properties of its taxonomic structure. As briefly reviewed in [177], several\nstudies have looked at the frequency distribution of the number of species within a genus\n132\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\n[178] as well as frequency distributions in higher taxa [179]. From these results, some of\nwhich are reproduced in Figure 4-13, it appears that the frequency distribution fits a power\nlaw with exponents reported to vary between 1.5 and 2.3.\n\na)\n\nb)\n\nFigure 4-13 log-log plots of the frequency of a selected taxon with different numbers of sub-taxa. a) Frequency of\ngenera with different numbers of species for birds. The frequency is given on the vertical axis and the number of\nbird species within the genera is given on the horizontal axis. b) Frequency of orders with different numbers of\nfamilies for animals. The frequency is given on the vertical axis and the number of animal families within the\norder is given on the horizontal axis. Data points with frequencies f=1 are omitted. Similar distributions for other\ndata sets are presented in [179]. Reprinted by permission from Elsevier: (J. theor. Biol.) [179], copyright (1990).\n\nComparing EA Genealogy and Evolutionary Taxonomy: To compare the topological\n\nproperties of taxonomic structure (in natural evolution) with the genealogical structure of\nEA populations, it is necessary to clarify exactly what topological properties are being\nmeasured in Figure 4-13.\nIf one thinks of taxonomic structure in terms of a branching process, then the number of\nsub-taxa within a taxon (the horizontal axis of Figure 4-13) is equivalent to the number of\nbranches that extend away from a node in the taxonomic tree. This is what is measured in\nFigure 4-13 with the addition of a few restrictions on the data used. In particular, data is\nrestricted to particular groupings (e.g. birds, animals) and particular levels of hierarchy\nwithin the taxonomic structure (e.g. species/genera, families/order).\nFigure 4-13a provides results for the number of species within a genera which are the\nlowest and second lowest levels of the taxonomic hierarchy (resp.). An equivalent measure\nin EA genealogical graphs would be that of the number of offspring created by a parent.\nThis aspect of EA genealogy has been studied in detail in [180] where the distribution was\n\n133\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nfound to fit a power law under a range of conditions.\n\nThey also found that spatial\n\nrestrictions in the EA population can result in power law deviations but only in cases where\nhigh levels of spatial restrictions were imposed.13\n\nComparisons at higher levels of EA\n\ngenealogy would be possible by reconstructing the genealogical graphs of an evolving EA\npopulation, however this was not considered here.\nIt is also interesting to note that similar studies have looked at human genealogy through an\nanalysis of surname distributions [181] (also see [182] and references therein). In these\nstudies, it has been determined that the surname distribution is also well approximated by a\npower law. In summary, the taxonomy of natural evolution has been found to have a fractal\nstructure (as evidenced by the stated power laws) which is also observed in both human\ngenealogies and the genealogical trees of Evolutionary Algorithms.\n\n4.2.4\n\nSummary of Conclusions\n\nThe ETV distribution results presented in this chapter were found to be similar to the\nextinction distributions in natural evolution. However the distribution of extinction sizes in\nnatural evolution also displays power law deviations for large extinction sizes. From the\nlimited sources of power law deviations in the ETV results with EA, it was speculated that\nthat these deviations could be caused by either an insufficient amount of time that evolution\nhas taken place or the presence of geographical isolation.\nThe ETV age distribution results were generally found to be similar to the species lifetime\ndistributions in natural evolution with the power law exponent of the distribution being the\nonly significant difference.\n\nFinally, a review of natural evolution taxonomy, human\n\ngenealogy, and EA genealogy has found that each displays fractal characteristics as\nevidenced by power law distributions of structural features.\n\nIn particular, fractal\n\ncharacteristics are found to be pervasive throughout natural evolution taxonomy.\n\n13\n\nNotice that similar conclusions have also been made in this chapter with ETV distribution results.\n\n134\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\n4.3 Self-Organized Criticality\nThe review from the previous section has indicated that a number of spatial and temporal\nproperties in natural evolution do not have a characteristic scale (as evidenced by a power\nlaw). The experiments conducted in this chapter also provide evidence that some spatial\nand temporal properties in Evolutionary Algorithms do not have a characteristic scale.\nThese macro scale features have great relevance to the behavior of these systems and to our\nunderstanding of evolution.\n\nFor example, a power law relation in species lifetimes\n\nprovides strong evidence that older species are better adapted (on average) than younger\nspecies. This information has also helped to improve our understanding EA behavior. For\ninstance, in the previous chapter the ETV size distribution provided convincing evidence\nthat most interactions between an EA and its environment are effectively neutral. This new\nunderstanding was used to build a more effective approach to the adaptation of EA design\nparameters.\nIt could potentially be of great benefit to understand how this behavior emerges in natural\nevolution and Evolutionary Algorithms. One contender for explaining the macro features\nof evolutionary dynamics is the Theory of Self-Organized Criticality (SOC). This theory is\nbriefly described next, followed by a set of conditions that any dynamical system is\nexpected to satisfy in order to be compatible with SOC theory. Section 4.3.2 provides\nevidence that Evolutionary Algorithms meet a number of these conditions, with evidence\nbased primarily on the ETV distribution results of this chapter.\n\n4.3.1\n\nSOC Definition\n\nThe Theory of Self-Organized Criticality was first put forth by Bak, Tang, and Wiesenfeld\n[183] and has been used to explain a range of physical phenomena such as flicker noise (1/f\nnoise) which is observed in the light emitted from quasars, the intensity of sunspots, current\nthrough resistors, sand flow in an hour glass, the flow of rivers, and stock exchange price\nindices (see [183] and references therein). The theory claims that some coupled dynamical\nsystems are driven or attracted to a critical state where the system displays self-similarity in\nboth space and time. This behavior is in contrast to other critical phenomena (e.g. phase\ntransitions) where an environmental parameter (e.g. temperature) must be tuned in order for\n135\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nthe system to reach a critical state. A more detailed discussion of critical phenomena is\nbeyond the scope of this thesis however an introduction to the topic can be found in [184].14\nConditions for SOC: Although there is no generally agreed upon litmus test for SOC\n\nbehavior, a number of basic conditions are expected. Given a system of loosely coupled\ncomponents, an SOC system will evolve to a (critical) stationary state where interactions at\na local level (i.e. localized disturbances) can propagate and reach any size (including the\nsize of the entire system) with a non-negligible probability. Such a state is popularly\nindicated through the presence of power laws in spatial and temporal properties.\nFor an SOC system, critical dynamics should not be fragile to experimental conditions;\notherwise this would indicate some sort of tuning is necessary. Hence, a broad range of\nexperimental conditions should be tested before any claims of SOC are made. Finally,\nsince SOC systems are attracted to a critical state, but do not start in one, it is expected that\nsome transient exists where the system is initially not critical (i.e. power law deviations\nexist during the transient). Using these general conditions as a guide, the next section\nconsiders whether Evolutionary Algorithms are compatible with SOC theory using\narguments based on the ETV results from this chapter.\n\n4.3.2\n\nCompatibility of EA with SOC\n\nEA populations are already known to be loosely coupled dynamical systems and so it is\nassumed that some form of self-organization will take place. The question is simply\nwhether the attractor for the system is a critical one. To provide support for this statement,\none must show that i) the distribution of disturbance sizes fits a power law and ii) the power\nlaw is a robust property that occurs under many conditions.\n\nDisturbances to an EA\n\npopulation can be measured by ETV as described below.\nDefining ETV as a measure of disturbance: First a brief explanation is needed of how\n\nETV is a measure of disturbance size in EA populations. The explanation given here for\n\n14\n\nCritical phenomena also has relevance to other aspects of evolution which are not covered in this thesis.\nThe interested reader can find studies on its relevance to evolvability and fitness landscapes in [17], and its\nrelevance to evolutionary dynamics on neutral networks in [80].\n\n136\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\ndescribing population dynamics on a graph is equivalent to the description provided for the\nmeasurement of ETV that is given in the previous chapter.\nThe spatio-temporal process of EA population dynamics can be represented by a sparse\ndirected graph where individuals are represented by nodes and directed connections\nbetween nodes indicate that one individual (with outgoing connection) has influenced the\ncreation of another (with incoming connection). As has been done in similar studies, weak\ninteractions (i.e. connections) in the system are ignored meaning in this case that only the\ndominant parent is considered to be connected to an offspring.15 This results in a graph\ntopology where each node has only one input connection. This graphical model of EA\ndynamics is identical to the EA genealogical tree shown in the last chapter.\nThe creation of each new node (i.e. individual) is assumed to represent a new disturbance to\nthe system (i.e. genotypic and phenotypic change to population makeup). To observe the\ngrowth of a disturbance, we look at the total number of nodes in the current state of the\nsystem (i.e. population members) that have a path leading to this node. This represents the\ncurrent size of the system that is affected by a disturbance at a given point in time (i.e.\ngeneration). The maximum impact of the disturbance would be calculated in an identical\nfashion to the ETV measurement. Therefore, one can see that disturbances to genotypic\nand phenotypic characteristics of the population can propagate from one node to another\nand the eventual size of the disturbance is precisely what ETV measures.\nEvidence that EA populations are self-organized to a critical state: Tests conducted in\n\nthis chapter under a broad range of experimental conditions have indicated that the ETV\nsize and age distributions are well approximated by power laws and that the power laws are\na robust property of the system. The experimental results shown in Figure 4-6 also indicate\nthat a short transient occurs before the population dynamics organize to a stationary power\nlaw distribution. During this transient, larger ETV are less likely to be observed. This\nindicates that disturbances initially remain localized and that the EA population starts in an\nordered state but is quickly driven to a critical one. In summary, the results from this\n\n15\n\nThe use of a threshold criteria for considering only large interactions in a dynamical system is a standard\napproach employed when one only wants to study the existence of interactions and not the relative strengths\nof interactions, the latter being more complicated. For example see [185].\n\n137\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nchapter demonstrate that Evolutionary Algorithms meet the conditions necessary for\ncompatibility with SOC theory.\nAlternative Explanations: Despite this apparent compatibility, it is also worth pointing\n\nout that both spatial and temporal properties of EA population dynamics were derived from\ngenealogical graphs which are a type of branching process. Furthermore, it is known that\nbranching processes exhibit criticality when the average death rate equals the average\ngrowth rate of new branches (e.g. see [186] and [187]). This condition is the same as the\nrequirement in static EA population sizes that the number of individuals removed from an\nEA population is equal to the number added to the population. Hence, it is likely that the\nuse of a static EA population size is an important contributing factor to the ETV results\npresented in this chapter.\nAlthough the underlying causes are not fully understood, the ETV results from this chapter\nallow for tentative statements to be made regarding the sufficient conditions for a system's\nhistorical coupling to self-organize to a critical state. In particular, this behavior has been\nshown to take place in a closed system that contains a reproducing Panmictic population\nwith a static population size.\n\nMore experimentation is needed to further expand our\n\nunderstanding of this aspect of EA dynamics. It would also be a significant contribution if\nthis form of dynamical behavior could be related back to the dynamics of an EA in\nparameter space.\n\n4.4 Relevance to EA research\nWhat is most remarkable from the results of this chapter is not that measurements of EA\npopulation dynamics (as measured by ETV) fit a power law. What is remarkable is how\nlittle sensitivity the measurement results displayed to the selection pressure, the fitness\nlandscape, or the medium (artificial or natural) in which evolution took place.\nFurthermore, many other complex systems that are unrelated to evolution, ranging from\nearthquakes to solar flares to turbulence, also spontaneously organize to display similar\nspatial and temporal patterns (see [185] and references therein). This is of importance to\nEvolutionary Computation research because it indicates that at least some properties of\n\n138\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\nnatural evolution are not reliant on the specific context of \"nature\" and instead are a\nconsequence of very general and easily reproduced conditions and driving forces.\n\n4.4.1\n\nImpetus for SOTEA Chapter\n\nIn recent years there has been growing evidence that many significant features of biology\nare not a consequence of natural selection but instead are a result of physical conditions and\nconstraints. For instance, genome complexification models have been developed which,\nwhen randomly evolved (without a particular selection pressure), are able to generate\ntopological characteristics that are similar to gene regulatory networks [188] and protein\ninteraction networks [189].\nIn another important development, characteristics such as modularity and hierarchy, which\nare heavily exploited in natural evolution, have been found to emerge from simple localized\nrules [190] and do not require the presence of natural selection.\n\nModels of genome\n\ncomplexification have also been proposed recently where modularity is expected to emerge\nwithout the influence of natural selection [20].\nThese findings suggests that the unique quality of life is not generated solely from natural\nselection in reproducing populations but is also heavily reliant on the physical laws and\nconstraints that are imposed on these evolving systems.\nIf our goals as EA researchers are to mimic the salient features of life in order to exploit it\nfor purposes of optimization, it behooves us to actively explore what other conditions\n(beyond Darwinian theory) are necessary to acquire the robust and adaptive properties of\nlife. Furthermore, with growing evidence that Darwinian principles only provide a partial\nexplanation for life, one could reasonably speculate that only so much progress is possible\nin EA research from tweaking the traditional controls of natural selection (e.g. through the\ndevelopment of search operators and selection pressures).\nOn the other hand, the results from this chapter have shown that other factors such as\nspatial restrictions can cause significant changes to large scale dynamical features of a\nsystem. These results have also indicated that some form of self-organization already\noccurs in EA population dynamics which is notably distinct from its well-known\n139\n\n\fChapter 4: Large Scale Features of EA Population Dynamics\n\norganization in parameter space. This raises the question as to what other self-organizing\nprocesses occur in nature and could be of benefit to EA performance. The final chapter of\nthis thesis focuses squarely on these issues.\n\n140\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nChapter 5\n\nSelf-Organizing Topology Evolutionary\n\nAlgorithms\nWithin the last several years, it was discovered that the interaction networks of complex\nbiological systems have evolved to take on several non-random topological characteristics,\nsome of which are believed to positively impact system robustness. A number of network\ngrowth models have also been discovered that can successfully recreate many of these\nstructural characteristics using simple rules and in the absence of a selection pressure.\nIn the previous chapter, it was shown that spatial constraints (i.e. population topology) have\na significant impact on EA population dynamics. This chapter focuses on ways in which\nthe population topology can self-organize to exhibit topological characteristics similar to\ncomplex biological systems. The aim of this chapter is not only to mimic the structural\ncharacteristics of biological systems but also to acquire some of the desirable qualities\nfound in these systems.\nThe next section presents a critical review of previous work related to the application,\ncharacterization, and evolution of interaction networks. This is followed by Section 5.2\nwhich presents the motivations and aims of this chapter. Section 5.3 then describes the first\nof two network models. The first model is designed to sustain population diversity in\nrugged fitness landscapes which it accomplishes, in part, by mimicking the process of\ngenome complexification in natural evolution. The second model is described in Section\n5.4 and is designed for the purpose of evolving important topological properties such as\nmodularity. This model is also designed with a focus on EA performance with tests\nconducted on a number of artificial test functions and engineering design problems. The\nresults from these experiments provide strong evidence that the new Self-Organizing\nTopology Evolutionary Algorithms (SOTEA) are able to exhibit robust search behavior\nwith strong performance over both short and long time scales.\n\n141\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.1 Critical Review of Previous Work\nThis section provides a review of topics that are relevant to the work presented in this\nchapter. It starts by reviewing approaches to constraining interactions in population-based\nsystems with a focus on spatially distributed systems that are defined on a network. The\nsection then reviews metrics for characterizing network topology. The section concludes\nby presenting a number of network models that have been successful in mimicking the\ntopological characteristics of complex biological systems.\n\n5.1.1\n\nInteraction Constraints\n\nIn recent years, there has been an increasing awareness of the importance of locality or\ninteraction constraints when modeling complex systems.\n\nRestricting interactions in\n\npopulation-based systems has been a key factor in topics such as robustness against\nparasitic invasion [191], [192], enabling speciation [193], sustaining population diversity in\nrugged landscapes [136 2007), 2007)], the emergence of cooperative behavior [18], and\nrobustness to random attack [75]. Furthermore, convincing arguments have been made for\nits role in natural evolution and in particular, its impact on system evolvability [17].\nParallel developments have also taken place in population based search heuristics such as\nEvolutionary Algorithms, where it has been recognized that restricting interactions between\npopulation members can result in significant changes to algorithm behavior. This has been\nobserved in several seemingly disparate topics such as the age restrictions present in the\nAge Layered Population Structure (ALPS) for Genetic Algorithms [99], the genealogical\nand phenotypic restrictions present in Deterministic Crowding [84], coarse-grained\nrestrictions in interactions between heterogeneous subpopulations [100], and explicit static\ntopologies for constraining interactions in the cellular Genetic Algorithm (cGA).\n\n5.1.1.1\n\nPopulation Networks for Evolutionary Algorithms\n\nThis chapter limits its focus to interaction constraints that are implemented by defining an\nEA population on a network. The use of explicitly defined interaction networks provides a\nuseful framework for understanding system constraints and their impact on system\n142\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\ndynamics. This is, in part, because network representations of systems can be probed using\na number of tools developed in statistical mechanics.\nDefining an EA population on a network impacts an EA through the localization of genetic\noperators. For instance, actions such as reproduction and selection only occur among\nindividuals directly connected (i.e. linked) or near each other in the network. The three\ntypes of population structures typically considered for EA populations are shown on the top\nrow of Figure 5-1.\nThe fully connected graph in Figure 5-1a represents the canonical EA design, which is\nreferred to in this thesis as the Panmictic GA. Here, each individual (represented by nodes\nin the graph) can interact with every other individual such that no definition of locality is\npossible. The network in Figure 5-1b represents a typical island model population structure\nwhere individuals exist in fully-connected subgroups which are largely isolated from other\npopulation subgroups.\n\nHere the large arrows represent interactions which take place\n\nbetween subgroups but occur at a time scale much greater than that of interactions within\nsubgroups. As a consequence of this setup, the locality of island model networks is defined\non a scale that is significantly larger than the individual. The final EA structure shown in\nFigure 5-1c represents a cellular EA population structure which is referred to in this thesis\nas the cellular Genetic Algorithm or cGA. Similar to cellular Automata, the network of\ninteractions takes on a lattice structure with interactions constrained by the dimensionality\nof the lattice space. With the cellular GA, each individual has a unique environment\ndefined by its own unique set of interactions which is referred to as a neighborhood.\n\n143\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-1: Examples of interaction networks. The networks on the top represent commonly used EA population\nstructures and are known as (from left to right) Panmictic, island model, and cellular population structures.\nNetworks at the bottom have been developed with one or more characteristics of complex biological networks and\nare classified as (from left to right) Self-Organizing Networks (presented here), Hierarchical Networks [194], and\nSmall World Networks [195]. Figure 1e is reprinted with permission from AAAS.\n\nThe ratio of neighborhood size (i.e. number of connections per node) to system size (i.e.\ntotal number of nodes) can be seen as a measure of locality and it is worth noting that this\nratio decreases in the EA population structures from left to right on the top row of Figure\n5-1. Although the three population structures clearly have different degrees of locality,\nthey also have some important similarities. For each population structure, the nodes within\nthe network each have the exact same number of interactions and the same type of\ninteractions (i.e. regular graphs). Furthermore, the networks for all three cases are static\nand predefined.\n\n5.1.2\n\n5.1.2.1\n\nStructural Characteristics of Complex Networks\n\nProperties of real networks\n\nMany natural and man made systems consist of a large number of dynamical interacting\ncomponents.\n\nExamples are seen in biology (metabolic networks, protein interaction\n\n144\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nnetworks, gene regulatory networks, food webs, neural networks), social systems\n(coauthorship, relationships) and man made systems (World Wide Web, Internet, power\ngrids).\nDespite the significant simplifications necessary to create network representations of these\nsystems and despite their inherent differences in scale, environmental context and\nfunctionality, most real networks have a great deal of similarity in their topological\nproperties. These similarities include 1) small characteristic path lengths, 2) high clustering\ncoefficients, 3) fat-tailed degree distributions (e.g. power law), 4) degree correlations, 5)\nlow average connectivity, as well as other properties reviewed in [196]. Each of these\nfeatures are notably distinct from random graphs and regular lattices. The next section\nreviews a number of topological properties that are commonly measured when studying\nnetworks. Comprehensive reviews of this topic are provided in [196], [197], and [198].\n\n5.1.2.2\n\nTopological Property Metrics\n\nTo help understand the interaction networks of complex systems, a few simple measures\nare introduced which are commonly used to assess network structural characteristics.\nThroughout this paper, networks are represented by an adjacency matrix J such that\nindividuals i and j are connected (not connected) when Jij=1 (Jij=0).\nCharacteristic Path Length: The path length is the shortest distance between two nodes\n\nin a network. The characteristic path length L is the average path length over all node pair\ncombinations in a network. Generally, L grows very slowly with increasing system size\n(e.g. population size) N in complex systems. For instance, networks exhibiting the \"Small\nWorld\" property, such as the network in Figure 5-1f, have L proportional to log N [199].\nDegree Average: The degree ki is the number of connections that node i has with other\n\nnodes in the network which is defined in (5-1). The degree average kave is simply k\naveraged over all nodes in the network. The degree average is expected to remain small,\neven for large networks, as reviewed in [196].\n\n145\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nN\n\nki = \u2211 J i, j\n\n(5-1)\n\nj =1\n\nDegree Distribution: The degree distribution has been found to closely approximate a\n\npower law distribution for biological complex systems with power law and exponential\ndistributions often fitting abiotic complex systems [198]. Networks which display a power\nlaw k distribution are often referred to as scale free networks in reference to the scale\ninvariance of k.\nClustering Coefficient: Many complex biological systems have high levels of modularity\n\nwhich is typically indicated by the clustering coefficient. The clustering coefficient ci is a\nmeasure of how well the neighbors of a given node are locally interconnected. More\nspecifically, this is defined in (5-2) as the ratio between the number of connections ei\namong the ki neighbors of node i and the maximum possible number of connections\nbetween these neighbors which is ki(ki-1)/2.\nci =\n\n2e i\nk i (k i \u2212 1)\n\n(5-2)\n\nAlthough in practice, more efficient calculation methods are used, ei can be formally\ndefined using the adjacency matrix J as shown in (5-3).\nN\nN\n\u239b\n\u239e\nei = \u2211 \u239c J ij \u2211 J ik J jk \u239f, i \u2260 j \u2260 k\nj =1 \u239d\nk =1\n\u23a0\n\n(5-3)\n\nClustering-Degree Correlations: A common feature of biological and social systems is\n\nthe existence of an hierarchical architecture. Such an architecture implies that sparsely\nconnected nodes form tight modular units or clusters and communication paths between\nthese modular units are maintained via the presence of a few highly connected hubs [199].\nFigure 5-1e shows a network with these hallmark signs of modularity and hierarchy which\nwas grown using the deterministic models presented in [194].\nThe existence of hierarchy in a network is typically measured by looking at the correlation\nbetween the clustering coefficient and the node degree. Based on the description given\nabove, an hierarchical network is expected to exhibit higher connectivity for nodes with\n146\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nlow clustering (i.e. hubs) and vice versa. Furthermore, for the feature of hierarchy to be a\nscale invariant property of the system, c should have a power law dependence on k.\nDegree-Degree Correlations: For many complex networks, there exist degree correlations\n\nsuch that the probability that a node of degree k is connected to another node of degree k`\ndepends on k. This correlation is typically measured by first calculating the average nearest\nneighbors degree kNN,i which is defined in (5-4).\nk NN ,i =\n\n1\nki\n\nN\n\n\u2211J\nj =1\n\ni, j\n\n(5-4)\n\nkj\n\nNetworks are classified as assortative if kNN increases with k or disassortative if kNN\ndecreases with k. Degree correlations are often reported as the value of the slope \u03c5 for kNN\nas a linear function of k.\nRandom Networks: Thus far, only qualitative statements have been given regarding the\n\ntopological properties of complex networks. In practically all cases, when topological\nproperties are mentioned as being large or small (as has been mentioned above), the\nstatements are referring to property values in relation to those values observed in random\ngraphs and particularly the models developed by Erd\u00f6s and R\u00e9nyi [200], [201].\n\nAs\n\nreviewed in [197], random graphs have i) a characteristic path length LRand similar to that\nobserved in complex networks and approximated by (5-5), ii) a Poisson degree distribution\n(as opposed to the fat tailed degree distribution in complex networks), and iii) a clustering\ncoefficient cRand given by (5-6) which is orders of magnitude smaller than what is typically\nseen in complex networks [195].\n\nRandom graphs also do not exhibit any degree\n\ncorrelations or correlations between the degree and the clustering coefficient.\n\nLRand \u2248\n\nln ( N )\nln (k Ave )\n\n(5-5)\n\nk Ave\nN\n\n(5-6)\n\nc Rand =\n\n147\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.1.3\n\nNetwork Evolution Models\n\nIn order to mimic complex systems, it is important to understand how they obtain their\ninteresting behaviors and properties. For both man-made and biological complex systems,\nit is generally understood that the development of interaction networks in these systems\noccurs through a process of constrained growth. Examples would include growth of the\nWorld Wide Web, the developmental process in multi-cellular organisms, and the\ncomplexification of the genome.\nOver the last decade, substantial progress has been made in the development of network\ngrowth models which can evolve to display characteristics similar to real systems.\nExemplars of this success can be seen in the Barabasi-Albert (BA) Model [202], the\nDuplication and Divergence (DD) Model [203], the intrinsic fitness models of [204] and the\nrandom walk models of [190]. Common to many successful models is the emergence of\nrelevant network characteristics, such as those previously mentioned (e.g. L ~ log N, Power\nlaw k distribution), through the use of simple, locally defined rules which constrain\nstructural dynamics (including, but not limited to, network growth). Furthermore, these\nstructural dynamics are driven by one or more state properties of the nodes. This simply\nmeans that connections in the network change and nodes are added or removed with a bias\nderived by property values assigned or calculated for each node. Properties that have been\nused in models include the degree of a node k [202], measures of node modularity [205], as\nwell as measures of node fitness [204]. The remainder of this section presents several\nnetwork evolution models that are important contributions to the field and have been\nparticularly important in development of the SOTEA models presented in this thesis.\n\n5.1.3.1\n\nThe BA Model:\n\nThe Barab\u00e1si-Albert (BA) model works on the basis of network growth and preferential\nattachment. These principles are inspired by experiences with real systems and they are\nprevalent in a number of large complex systems. Examples of systems driven by growth\ninclude the World Wide Web, collaboration networks, genome complexification and many\nmore. The concept of preferential attachment is also observed in many systems such as\n148\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\ncitation networks where a new manuscript is more likely to cite well-known and already\nwell-cited papers compared with its less-cited peer papers. Preferential attachment is also\nseen in a number of other social networks as mentioned in [202].\nModel Description: Starting with a small number (m0) of nodes, at every time step a new\n\nnode is added with m (< m0) links that connect the new node to m nodes already present in\nthe system. To incorporate preferential attachment, the probability PN1,N2 that a new node\nN1 is connected to an existing node N2 depends on the degree kN2 of node N2.\nFurthermore, it is assumed that this dependence is linear as expressed in (5-7).\nPN 1, N 2 =\n\n(5-7)\n\nkN2\nN\n\n\u2211k\ni =1\n\ni\n\nModel Characteristics: This model creates networks with power law k distributions with\n\nexponent similar to that observed in real systems. These networks also have a path length\nL~ log log N which obviously grows very slowly with increasing system size N. However,\nthese models produce networks with no correlation between k and c [199].\n\n5.1.3.2\n\nThe Duplication and Divergence Model\n\nGenome Complexification: As far as complex systems are concerned, the genome and its\n\nassociated expression represent the largest and most interesting case of network evolution.\nHence it is of great interest that large data sets of these systems are now available as well as\nthe tools necessary for probing their structural organization. For instance, recent analysis\nof protein interaction networks and metabolic networks have found them to be\ncharacterized as having power law degree distributions, high modularity, low characteristic\npath lengths, and a low degree average [206], [195], [207].\nEvolutionary processes associated with genome complexification are known to be initiated\nby a process of gene duplication and gene mutation, which is also referred to as Duplication\nand Divergence (DD). In the review presented in [208] , studies are cited which have found\nthat roughly 40% of the human genome can be confirmed as being derived from past\nduplication events (with even higher values observed in other species).\n\n149\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nDue to the nature of the divergence process, it is also likely that the 40% estimate is a\nconservative lower bound on the actual number of duplicated genes in the human genome.\nKnowing that gene divergence occurs by a series of random mutations, the process\napproximates a random walk which acts to reduce (over time) any ability to recognize two\ngenes as having a similar historical origin. Hence, it is speculated here that DD is the\npredominate cause of genome complexification.\nAlthough several duplication and divergence models are available from the literature, the\none presented in [189] has been selected due to its simplicity and its proven capacity to\ngenerate power laws for k.\nModel Description: Starting with an unspecified initial network size, at every time step a\n\nnode is randomly chosen and duplicated. Links of the duplicated node are removed with\nprobability \u03b4. New links are added to the duplicated node between itself and randomly\nselected nodes in the network with probability \u03b1. In this model, \u03b4 is set to 0.53 and \u03b1 is set\nto 0.06/N where N is the network size at a given time step.\nThe duplication step in the model represents gene duplication where the original and\nduplicated genes retain the same structural properties meaning they initially have an\nidentical set of interactions. The rewiring steps (involving probabilities \u03b4 and \u03b1) represent\nmutations in the duplicated gene which cause its set of interactions to diverge from those of\nthe original gene. The parameter settings listed above for \u03b4 and \u03b1 are set in [189] based on\nempirical observations of protein interactions networks in yeast and the interaction\nnetworks of other complex systems.\nSimplifying Assumptions:\n\nAlthough this model for genome complexification was\n\nselected in part due to its simplicity, it is still important to highlight the simplifying\nassumptions that have been made.\n\u2022 The first simplification is that it does not allow for the presence of multiple duplications\nat a single instance in time.\n\nThis is known to occur in natural evolution with\n\nduplication involving sizes up to and including the entire genome [209], [210]. A\nmodel considers multi-gene duplications is presented in [203].\n\n150\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n\u2022 Another difference between this DD model and nature is that gene divergence\nmechanisms can potentially take place for both the duplicated and original genes. Also\ndivergence occurs as a slow and possibly continuous process instead of happening in a\nsingle time step.\n\u2022 Finally the removal of genes from the genome is also neglected thereby implying that\nnatural selection pressures present in real genome complexification will have roughly\nthe same impact on topology as random rewiring of the duplicated gene.\nIt is not clear what impact these additional features would have on the model and its\nstructural characteristics, however it is interesting to notice (as indicated below) that this\nmodel does provide many similarities to complex biological systems without the presence\nof natural selection pressures or the other assumptions listed above.\nModel Characteristics: In [189], structural characteristics of this DD model are compared\n\nwith data available on the yeast proteome. Results indicate the model creates a network\nwith characteristics similar to yeast for values of kave, k distribution, cave, and L. Correlation\nmeasurements were not considered in this study.\n\n5.1.3.3\n\nThe Fitness Model:\n\nThe model presented in [204] proposes a \"good-get-richer\" mechanism for network\ndynamics where nodes of higher fitness are more likely to become highly connected. This\nis presented as an alternative to preferential attachment or the so called \"rich-get-richer\"\nschemes present in the BA and DD models where an historical bias in network connectivity\ndrives future connectivity.\nThe fitness model is based on the concept of mutual attraction. One example they provide\nof such a system is a sexual interaction network where it is assumed interactions take place\ndue to mutual attraction between two partners. They argue that knowledge of sexual\npromiscuity (i.e. knowledge of k which is a precondition for preferential attachment) would\nactually have the reverse impact on the probability of interactions in such networks.\nAnother example they provide is the protein interaction networks inside cells where\ninteractions are driven by chemical affinity. This second example is less convincing\n151\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nhowever because protein specificity is an evolved trait meaning that both historical (i.e.\nevolutionary) bias as well as principles of mutual attraction play a role for such systems.\nThe fitness model description given in [204] is provided below.\nModel Description: In this version of the fitness model, they neglect mechanisms for\n\nnetwork growth and instead start off with a fixed number of nodes N containing no links.\nFor every node, a fitness value xi is assigned randomly from a probability distribution \u03c1(x).\nFor every possible pair of nodes i, j, a link is created with probability f(xi, xj) which is given\nby (5-8) where xM is the maximum value possible for x.\nf (xi , x j ) =\n\n(x x )\ni\n\n(5-8)\n\nj\n\nx M2\n\nModel Characteristics: When \u03c1(x) takes an exponential form, this model creates networks\n\nwith power law k distributions with the power law exponent similar to that observed in real\nsystems. The resulting networks also have clear correlations in kNN-k, and in c-k [204].\n\n5.2 Motivations and Aims\nCurrently implemented network structures for EA populations have proved beneficial to\nEA performance, however the population structures do not actually resemble the interaction\nnetworks of complex biological systems as indicated in the previous review. This puts into\nquestion how \"nature-inspired\" these EA designs are and what additional benefits might be\nderived from more accurate representations of the structure and dynamics of complex\nsystems.\nOver the last several years, the interaction networks of many complex systems have been\nstudied.\n\nIt is now known that these systems display some interesting non-random\n\ncharacteristics that are similar among many biological and even manmade systems [197].\nThese characteristics are believed to be highly relevant to the behavior of these systems and\nparticularly important to emergent qualities such as robustness.\nA primary aim of this chapter is to improve upon the performance and behavior of\ndistributed Evolutionary Algorithms by mimicking the self-organizing processes of\n152\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\ncomplex systems within the population topology. Evolutionary Algorithms which have this\nbehavior are referred to in this thesis as Self-Organizing Topology Evolutionary\nAlgorithms (SOTEA).\nTo date, few have investigated the importance of dynamic population topologies for an EA.\nOne exception is seen in [135] where the grid shape of a cellular GA adapts in response to\nperformance data using a predefined adaptive strategy. In that work, structural changes are\nglobally controlled using statistics on system behavior. SOTEA algorithms, on the other\nhand, adapt to local conditions through a coevolution of network structure and population\ndynamics.\n\n5.3 SOTEA model I\nThis section describes the first of two SOTEA designs that are developed and tested in this\nthesis. As previously mentioned, a general aim of this chapter is to create EA population\nnetworks which are topologically similar to the interaction networks of complex biological\nsystems. This first SOTEA model also looks at how this can benefit EA behavior on\noptimization problems containing rugged fitness landscapes.\nSection 5.3.1 describes SOTEA and a cGA variant that is used for comparison purposes.\nSection 5.3.2 describes the experimental setup including a description of a tunable fitness\nlandscape that is used to test algorithm behavior when exposed to different amounts of\nlandscape ruggedness. The results are presented in Section 5.3.3 including algorithm\nperformance, analysis of population topology, and the impact of SOTEA design features\nand fitness landscape features on population diversity. From these experiments, it is found\nthat SOTEA exhibits strong performance and is able to sustain high levels of population\ndiversity for evolution on rugged fitness landscapes. The population topology for SOTEA\nis also found to have some similarities to known features of complex biological systems.\nThis is followed by a discussion of these results in Section 5.3.4 as well as conclusions in\nSection 5.3.5.\n\n153\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.3.1\n\nModel Description\n\nFor all EA designs, the population is defined on a network. Besides the trivial case where\nthe network is fully connected (i.e. Panmictic GA), two other network designs are used and\nare referred to as the cellular GA (cGA) and the Self-Organizing Topology Evolutionary\nAlgorithm (SOTEA). For the cGA and SOTEA, the population is initially defined in a ring\nstructure with each node connected to exactly two others (e.g. Figure 5-1c). A change to\nthe network structure (i.e. network dynamics) simply refers to the addition or removal of\nnodes or links. The rules for defining network dynamics are described next.\n\n5.3.1.1\n\nSOTEA and cGA Network Dynamics\n\nFor both the cGA and SOTEA, a node is only added to the network when a new offspring is\nadded to the population and a node is only removed when an individual dies. Network\nchanges due to offspring creation are referred to as reproduction rules and changes due to\ndeath of individuals are referred to as competition rules. The reproduction and competition\nrules define how network dynamics occur and are described next.\nReproduction Rule: The reproduction rule (described in Figure 5-2) is used in SOTEA\n\nand the cGA when a new offspring is created. The first step in the reproduction rule\ninvolves making a copy of a parent and then mutating that copy to create an offspring.16\nStructural changes from the reproduction rule involve the addition of a new node\n(offspring) to the network, connection of the offspring to its parent, and then (depending on\nthe EA design) the possibility of additional connections being added to the offspring node\nand the possibility of connections being removed from the parent node. Complete details\nof the addition and removal of connections in the reproduction rule are provided in Figure\n5-2.\n\n16\n\nNotice this means an offspring only has a single parent\n\n154\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-2: Reproduction rules that change the population structure for SOTEA and the cellular GA. a) SOTEA\nReproduction: When an offspring is created (by asexual reproduction), a new node (shown in black) is added to\nthe network through a connection to its parent (shown in gray). Each of the parent's connections are then\ninherited by the offspring (black dotted line) with probability Padd followed by each of the inherited connections\nbeing lost by the parent (gray dotted line) with probability Premove. Unless stated otherwise, the parameters are set\nas Padd = Premove = 10%. This particular rule is loosely based on established models for genome complexification\n[203]. b) cellular GA Reproduction: When an offspring is created, a new node (shown in black) is added to the\nnetwork and connected to its parent (shown in gray). One of the parent's connections is then transferred to the\noffspring, which allows the network to maintain a ring topology.\n\nIt is worth noting that the reproduction rule represents the only difference between SOTEA\nand the cellular GA. With SOTEA, the addition of new nodes causes changes to the\nnetwork topology (see Figure 5-2a). These changes to network structure turn out to be a\ncrucial source of structural innovation needed for the self-organization of the SOTEA\nnetwork.\nCompetition Rule: The competition rule (described in Figure 5-3) is the same for SOTEA\n\nand the cellular GA. With this rule, a randomly selected individual tries to kill its weakest\n(i.e. least fit) neighbor. If instead, the selected individual is worse than its worst neighbor,\nthen it will die. Structural changes from the competition rule involve removal of the dead\nindividual and the transfer of its connections to the individual that survived.\n\n155\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-3: Competition rules that change the population structure for SOTEA and the cellular GA. The details\nof the competition rule are the same for SOTEA and the cGA, however examples are given for both EA designs in\nthis figure. Competition rule: The first step is to select an individual at random. This individual then decides to\ncompete for survival with its least fit neighbor. When these two individuals compete for survival such as the nodes\nshown in black and gray, the less fit individual is killed. The winning individual (shown in black) inherits all\nconnections from the losing individual (shown in gray) that weren't already in the winning individual's\nneighborhood. Finally, the losing individual is removed from the network.\n\nThis rule is particularly important because it allows for structural changes to depend on\nnode states. Figure 5-4 is provided to help clarify this point. Notice that once a node has\nbeen selected for the competition rule, this node must decide who to compete with. The\ndecision of who to compete with depends on which of the nodes is worst in the\nneighborhood. As a result, structural changes are always driven towards those nodes with\nthe lowest fitness. Notice that if an individual decided to kill one of its neighbors at\nrandom then this decision would no longer depend on the node states and the network\nstructural dynamics would no longer depend on (i.e. be coupled to) the population\ndynamics.\n\n156\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-4: This figure shows how structural changes from SOTEA's competition rule depend on the fitness of\nindividuals in the network. Starting with the network at the top, the individual represented by the black node\nmust decide which of its neighbors it will try to kill. The networks at the bottom show what would happen if\nneighbor 1, 2, or 3 had been the least fit in the black node's neighborhood. Each of the choices creates a new\nstructure that is different from the other choices. Notice that for the networks on the bottom, the black node has\nbeen changed to gray. This is to indicate that either the black node or the white neighbor could have won the\ncompetition (the structure is the same in either case).\n\n5.3.1.2\n\nSOTEA and cGA State Dynamics\n\nTo mimic the interaction networks of complex systems, it is important to recognize that\nstate dynamics occurring on these networks play a significant role in the system's behavior.\nIn complex systems, the states of a node are (by definition) dependent upon the states of\nneighboring (i.e. connected) nodes.\n\nSignificant progress has taken place recently in\n\nunderstanding the state dynamics of complex systems. Some current directions of research\ninclude exploring the synchronization of component states [211], [212], robustness of\ndynamical expression [213], [75] and the coupled dynamics of states and network\nstructures [214], [215], [216].\nIn the previous section it was pointed out that the competition rule forces changes in\nnetwork structure to depend on the fitness (i.e. the state) of population members. As a\nconsequence, network structural dynamics are driven by population dynamics. This section\nconsiders how a reverse coupling of state and structural dynamics could be achieved. In\n\n157\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nother words, how can a node's state depend on the states of the other nodes it is connected\nto.\nAlong these lines, a measure of fitness called Epistatic Fitness is defined to be sensitive to\nthe fitness values of individuals in a neighborhood. The definition of epistatic fitness that is\nused in the SOTEA algorithm is provided in (5-9).\n\nEpistatic Fitness =\n\nk \u2212 Rank \u2212 1\nk\n\n(5-9)\n\nIn (5-9), Rank refers to the rank of an individual's objective function value among all of its\nk neighbors. Here the objective function is not a direct measure of fitness but only an\nintermediate value used to compute (epistatic) fitness. A rank of 1 indicates that the\nindividual is better than all its neighbors, resulting in epistatic fitness taking on its\nmaximum value of 1. A rank of k+1 indicates that the individual is worse than all its\nneighbors, resulting in epistatic fitness taking on its minimum value of 0. The term\nepistatic fitness is used in reference to the measure's similarity to genetic epistasis.17\nUsing epistatic fitness (5-9) results in the fitness of an individual being dependent on the\nnetwork structure. In other words, the fitness is contextual. Figure 5-5 provides an\nexample to help clarify how (5-9) causes an individual's fitness to be dependent upon the\nnetwork structure.\n\n17\n\nIn the Genome, genetic epistasis refers to interactions between genes which have a noticeable impact on the\nphenotype. Similarly, nodes in the population network will now interact in a way such that they impact each\nother's fitness values.\n\n158\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-5: This figure shows how the epistatic fitness (Fitepi) defined by (5-9) causes the fitness of an individual to\ndepend on its local neighborhood. Parts a-c of the figure show a population of five individuals defined on a\nnetwork. The Objective Function Value (Obj) and epistatic fitness defined by (5-9) are provided in the top and\nbottom (resp.) of each node (i.e. individual). For the top two individuals in part a), an arrow is drawn towards the\nindividual on the left to indicate it has the lower epistatic fitness. The top left individual's epistatic fitness is 2/3\nbecause its objective function value is better than 2 of its 3 neighbors. In part b), a new connection has been added\nto the network causing the epistatic fitness values for the two top individuals to now be equal. Finally in part c), a\nconnection has been removed from the network, causing the top left individual to have an epistatic fitness that is\nnow higher than the top right node. If the top two nodes were to compete for survival based on epistatic fitness, it\nshould now be clear that the decision of who survives (i.e. who is more fit) will depend on the neighborhoods of the\nindividuals.\n\nIt is important to mention that an interesting situation arises when SOTEA is used with\nepistatic fitness. In this case, the fitness values depend on network structure (due to\nepistatic fitness) and structural changes depend on fitness values (due to the competition\nrule). The result is a coupling of structural changes to states plus a coupling of state\ndefinitions to structure. It is believed that such a dual coupling is unique among existing\n\n159\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nnetwork evolution models.\n\nThe competition and reproduction rules that are used for\n\nSOTEA and the cellular GA are summarized in the pseudocodes in Figure 5-6.\n\nFigure 5-6: Pseudocode for SOTEA and cellular GA network dynamics.\n\n5.3.2\n\nExperimental Setup\n\nThis section presents the remaining aspects of the Evolutionary Algorithm designs as well\nas the test function generator used in these experiments.\n\n5.3.2.1\n\nNK Landscape Test Function\n\nThe NK landscape, originally developed by Kauffman in [81], is a test function generator\nwith a tunable amount of ruggedness and a tunable problem size.\n\nThe following\n\ndescription of the NK landscape has been adapted from [217]. The NK landscape is a\nfunction f: BN \u2192 R where B = [0,1], N is the bit string length, and K is the number of bits\nin the string that epistatically interact with each bit. To help reduce confusion with other\nnotation in this thesis, the N and K parameters of the NK landscape are relabeled as NNK and\nKNK (resp.)\n160\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-7: An example of the fitness lookup tables for determining the fitness contribution fi from bit xi. Given\nan NK landscape with NNK =8 and KNK =2, f3(x3, z1(3), z2(3)) is the fitness contribution for x3. z1(3) and z2(3) are the\ntwo bits that epistatically interact with x3. As shown in the figure, they have been selected as the bits to the left and\nright of x3 (i.e. z1(3) = x2 and z2(3) = x4). The lookup table consists of 2^( KNK +1) entries, each associated with a\nunique combination of bit states for x3, z1(3) and z2(3). Each entry in the lookup table is a number between [0,1]\ndrawn from a uniform distribution.\n\nEach bit xi provides a fitness contribution f i : B K NK +1 \u2192 R whose value depends on the\nstate of bit xi and the states of the KNK bits interacting with xi. The KNK bits interacting with\n)\n. NK Landscapes are stochastically generated with the\nxi are labeled as z1( i ) , z 2( i ) ,..., z K( i NK\n\nfitness contribution fi of bit xi being a number drawn from a uniform distribution in the\nrange [0,1]. To determine the fitness contribution fi, a lookup table is used such as the one\nshown in Figure 5-7. The final fitness value f(x) is an average of each of the fitness\ncontributions as defined below. For a given instance of the NK landscape, the maximum\nfitness value is not known however fitness values are bounded between [0,1].\nf (x ) =\n\n1\nN NK\n\n\u2211 f (x , z ( ) , z ( ) , ... , z ( ) ),\n\nN NK\ni =1\n\ni\n\ni\n\ni\n\n1\n\ni\n2\n\ni\nK NK\n\nxi \u2208 (0,1), i \u2208 {1,.., n}\n\n(5-10)\n\nIn the original description [81], the KNK bits that epistatically interact with xi are those\nadjacent to xi in the bit string as seen in Figure 5-7. In this work, each zi is randomly\nselected to be any of the bits (other than xi) and not just those adjacent or nearby. Notice\nthat without epistatic interactions (KNK =0), the problem is completely decomposable and\n161\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\ntrivial to solve. However, as KNK increases, so too does the phenotypic interdependence of\ngenes. Genetic encoding of the NK landscape is simple with each bit xi representing a\nbinary gene and NNK being the size of the genome.\nFor most of these experiments NNK = 30, KNK = 14. These parameters have been selected\nbased on a tradeoff between the problem size, degree of ruggedness, and memory costs of\nthe model which are proportional to N \u00d7 2 K NK . More detailed descriptions of the NK\nlandscape model and its properties can be found in [217], [218], [79].\n\n5.3.2.2\n\nCore EA Design\n\nA binary coded EA is used with population size N varying over the range [50,400]. Only\nasexual reproduction is considered via parent duplication plus mutation with a bit flip\nmutation rate of 2/NNK for NNK binary genes. Evolution occurs using a pseudo steady state\nupdating strategy where the parent population of size N is randomly uniformly sampled\n(with replacement) N times to generate N offspring. The parents + offspring then compete\nfor survival to the next generation. A high level pseudocode for each of the EA designs is\nprovided below.\n\n162\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nPseudocode for all three EA designs:\nInitialize population\nIf SOTEA or cellular GA: Connect individuals with ring topology\nLoop\nLoop N times\nRandomly select an individual i\nGenerate offspring by mutation\nIf SOTEA: apply SOTEA reproduction rule (Figure 5-6)\nIf cellular GA: apply cellular GA reproduction rule (Figure 5-6)\nEnd loop\nLoop N times\nRandomly select an individual i\nIf Panmitic GA: Select random neighbour\nIf SOTEA or cellular GA: Select worst neighbour\nEliminate worse of i or its chosen neighbour\nIf SOTEA or cellular GA: assign links of loser to winner\nEnd loop\nGen=Gen+1\nUntil maximum number of generations\n\nA few comments should be made about the similarities and differences between the three\nEA designs.\n\nFirst, it should be noted that the cGA and SOTEA only differ in the\n\nreproduction rule used, which is described in Figure 5-6. In particular, SOTEA uses a\nreproduction rule that is loosely based on genome duplication and divergence while the\ncGA uses a rule that ensures the ring topology is maintained.\nThe Panmictic GA also clearly differs from the distributed EA designs in that its population\nis defined on a static fully connected network. However there is another difference in the\nPanmictic GA that needs to be explained and justified. As demonstrated in the pseudocode\nabove, the Panmictic GA uses a selection method that is similar to binary tournament\nselection (without replacement). This selection method was used in order to provide the\nPanmictic GA with a better chance of maintaining genetic diversity. If the Panmictic GA\nincorporated the same selection procedure as the distributed EA designs, this would be\nequivalent to truncation selection. Truncation selection was not used with the Panmictic\nGA because preliminary results (not shown) have indicated that this selection method\ncauses poor performance and low genetic diversity when a Panmictic GA is run on the NK\nfitness landscape.\n\n163\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.3.3\n\nResults\n\nThe experimental results start off by assessing the topological characteristics of the EA\npopulations to determine if any of the EA designs are able to acquire the topological\ncharacteristics of complex biological systems. Experiments are also conduced to see if any\nother behavioral qualities of complex biological systems are acquired. One important\nquality that would be of great value in an EA design is the capacity to sustain diversity\nwithin a competitive environment. This section investigates whether any of the EA designs\ncan sustain high levels of genetic diversity and also investigates whether this provides a\nbenefit to algorithm performance.\n\n5.3.3.1\n\nTopological Characteristics of Interaction Networks\n\nThis section looks at the structural characteristics of the interaction networks for each of the\nEA designs and compares them to what is observed in complex systems. For each of the\nstructural characteristics presented in Table 5-1, only the SOTEA network was found to\nhave characteristics similar to that seen in complex systems.\nThe last column in Table 5-1 highlights the fact that every individual has the same\nneighborhood size k in the Panmictic GA and the cellular GA, however k takes on a\ndistribution of values for SOTEA. The distribution for k is fat tailed (closely fitting an\nexponential function), meaning that there is large heterogeneity in the neighborhood size.\nKeeping in mind that only neighbors can compete in a structured EA, the neighborhood\nsize k impacts the selection pressure within the population.\n\nSince there is large\n\nheterogeneity in neighborhood sizes for SOTEA, it is reasonable to suspect that there will\nalso be significant heterogeneity in selection pressure.\n\n164\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nTable 5-1: Topological Characteristics for the interaction networks of the Panmictic GA, cellular GA, and\nSOTEA. For comparison, common topological characteristics of complex networks are also provided (taken from\n[198], and references therein). L is the characteristic path length, k is the node degree, kave is the average node\ndegree, N is the population size, and R is a correlation coefficient for the stated proportionalities.\n\nSystem\nPanmictic GA\ncellular GA\nSOTEA\n\nComplex\nNetworks\n\n5.3.3.2\n\nL\nL=1\nL~N\nL ~ log N\n(R2=0.969)\nL ~ log N\n\nkave\nkave = N-1\nkave = 2\nkave ~ log log N\n(R2=0.989)\nkave << N\n\nk distribution\nk = N-1\nk=2\nExponential\n(R2=0.991)\nFat Tail (e.g. Power\nLaw, Exponential)\n\nGenetic Diversity\n\nThis section looks at the genetic diversity that is maintained in each of the EA designs.\nMeasuring genetic diversity of the population is done in a straightforward manner. Genetic\nDiversity is calculated as the average Hamming Distance between population members\ndivided by the average Hamming Distance between random points in solution space. For a\nsingle binary gene, two randomly selected gene values have a 50% chance of being\ndifferent making the Hamming Distance between random individuals of NNK genes equal to\nNNK /2. The Hamming Distance is defined in (5-11) as a summation of 1 minus the\nKronecker Delta function \u03b4(Xi,h , Xj,h). The Kronecker Delta function has a value of 1 if Xi,h\n= Xj,h and 0 otherwise. Xi,h and Xj,h represent the hth gene for individuals i and j (resp.).\nHam(i, j ) =\n\nN NK\n\n\u2211 (1 \u2212 \u03b4 ( X\nh =1\n\nN\n\nDiv =\n\ni ,h\n\n, X j ,h ) )\n\nN\n\n\u2211 \u2211 Ham(i, j )\n\n(5-11)\n\n(5-12)\n\ni =1 j =1, j \u2260 i\n\nN\nN ( N \u2212 1)\u239b\u239c NK \u239e\u239f\n2\u23a0\n\u239d\n\nDiversity results are shown in Figure 5-8 with each of the EA designs using epistatic\nfitness. Results are given for genetic diversity of the entire population as well as diversity\nfor the 20% best individuals in the population. It is useful to measure diversity for the top\n20% because it is often very difficult to maintain diversity among the best individuals in a\npopulation. As expected, the results demonstrate that the Panmictic GA is not able to\nsustain genetic diversity, particularly in the top 20% of the population. The cellular GA has\nmuch higher levels of diversity although this is significantly reduced in the top 20%.\n165\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nSOTEA exhibits sizeable improvements in diversity compared to the other EA designs,\nparticularly for 20% best individuals in the population.\n\nCellular GA\n\nTotal Population Diversity\n1\n\nSOTEA\nPanmictic GA\n\n0.8\n\nDiv\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n1000\n\n2000\nGeneration\n\n3000\n\n4000\nCellular GA\n\nTop 20% Population Diversity\n1\n\nSOTEA\nPanmictic GA\n\n0.8\n\nDiv\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n1000\n\n2000\nGeneration\n\n3000\n\n4000\n\nFigure 5-8: Genetic Diversity Results are shown over 4000 generations for Panmictic GA, SOTEA, and cellular\nGA. Diversity for each EA is an average over 10 runs with diversity calculated from (5-12) using the entire\npopulation (top graph) or the 20% best individuals in the population (bottom graph). Experiments are conducted\non NK models with NNK =30, KNK =14. For each EA design the population size is set to N=100 and epistatic fitness\nis used as defined by (5-9).\n\n5.3.3.3\n\nPerformance Results\n\nPerformance results are shown in Figure 5-9 with each of the EA designs using epistatic\nfitness. These results demonstrate that the Panmictic GA is not able to continually locate\n\n166\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nimproved solutions while SOTEA and the cellular GA both are able to make steady\nprogress throughout the 5000 generations considered. However, SOTEA was found to\nhave better performance than the cellular GA during the later stages of evolution.\n\n-0.215\n-0.225\n\nFitness\n\n-0.235\n-0.245\n-0.255\nPanmictic GA\n\n-0.265\n\nCellular GA\nSOTEA\n\n-0.275\n0\n\n1000\n\n2000\n3000\nGenerations\n\n4000\n\n5000\n\nFigure 5-9: Performance results are shown over 5000 generations for Panmictic GA, SOTEA, and cellular GA\neach operating with Epistatic Fitness. Performance for each EA is an average over 10 runs with performance\ncalculated as the best objective function value in a run. Experiments are conducted on NK models with NNK =30,\nKNK =14. For each EA design the population size is set to N=100 and epistatic fitness is used as defined by (5-9).\n\n5.3.3.4\n\nImpact of Ruggedness\n\nThis section considers the impact that landscape ruggedness has on genetic diversity of the\npopulation for each of the EA designs. Landscape ruggedness is varied by changing the\nKNK parameter of the NK model as shown in Figure 5-10. These results clearly show that\nas the NK landscape becomes completely smooth (i.e. KNK AE 0), each of the EA designs\nloses the capacity to sustain genetic diversity. However as ruggedness increases, each EA\ndesign approaches its own asymptotic limit indicating its maximum capacity for genetic\ndiversity. Notice that the asymptote for SOTEA was not observed over the range of KNK\nvalues tested. Larger values of KNK were not considered due to computational costs.\nKnowing that a diversity measure equal to 1 approximates a uniform distribution in\ngenotype space, the fact that SOTEA has diversity close to 0.8 among its top 20%\nindividuals indicates that SOTEA is able to distribute the search process across many\npromising regions of genotype space.\n\n167\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n0.8\nCellular GA\nSOTEA\n\nDiv\n\n0.6\n\nPanmictic GA\n\n0.4\n\n0.2\n\n0\n0\n\n5\n\nKKNK\n\n10\n\n15\n\nFigure 5-10: Genetic diversity results are shown for different amounts of landscape ruggedness for the Panmictic\nGA, SOTEA, and the cellular GA. Diversity is an average of calculations using (5-12) that are taken at every 20\ngenerations (up to 1000 generations) from the 20% best individuals in the population. This measure then also\naveraged over 5 runs. Experiments are conducted on NK models with NNK =30, and KNK varying as shown in\ngraph. Increasing KNK indicates increasing levels of landscape ruggedness. For each EA design, the population size\nis set to N=100 and epistatic fitness is used as defined by (5-9).\n\n5.3.3.5\n\nImpact of Epistasis\n\nAll results presented thus far have considered EA designs with individual fitness defined by\n(5-9) (i.e. epistatic fitness). Figure 5-11 extends the analysis of population diversity for\ncases where the individual fitness is defined in the standard way (as the raw objective\nfunction value). Compared to the results with epistatic fitness (see Figure 5-8), both\nSOTEA and cellular GA have significantly less diversity and are hard to distinguish from\nthe diversity present in the Panmictic GA. This result provides evidence that epistasis can\nplay an important role in sustaining diversity in structured populations including in the\ncellular GA.\n\n168\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nCellular GA\n\nTotal Population Diversity\n1\n\nSOTEA\nPanmictic GA\n\n0.8\n\nDiv\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n1000\n\n2000\nGeneration\n\n3000\n\n4000\nCellular GA\n\nTop 20% Population Diversity\n1\n\nSOTEA\nPanmictic GA\n\n0.8\n\nDiv\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n1000\n\n2000\nGeneration\n\n3000\n\n4000\n\nFigure 5-11: Genetic diversity results are shown over 4000 generations for Panmictic GA, SOTEA, and cellular\nGA each operating without epistatic fitness. Diversity for each EA is an average over 10 runs with diversity\ncalculated from (5-12) using the entire population (top graph) or the 20% best individuals in the population\n(bottom graph). Experiments are conducted on NK models with NNK =30, KNK =14. For each EA design, the\npopulation size is set to N=100 and fitness is defined as the Objective Function Value. The results shown here for\nthe Panmictic GA are identical to results shown in Figure 5-8. This is because the fitness rankings of individuals in\na fully connected population are the same regardless of whether epistatic fitness (5-9) is used or the Objective\nFunction Value is used. Because the fitness rankings are the same, the outcome of competitions will also be the\nsame (hence no change to EA behavior).\n\n169\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.3.3.5.1 Selection Pressure Patterns\nA better understanding of the impact of epistatic fitness on SOTEA is possible by observing\nits influence on the selection pressure within the SOTEA network. The networks in Figure\n5-12 are examples of SOTEA networks grown with and without epistatic fitness.\nTo represent selection pressure in the system, each node is selected in a mock competition\ntrial and arrows are drawn to its worst neighbor. Arrows are drawn in this way because, in\nSOTEA and the cellular GA, competition occurs by first selecting an individual and then\nhaving it compete against its worst neighbor. Arrows in black represent selection pressure\ndirected away from the network center, while arrows in green indicate selection pressure\nthat is not directed away from the center.\nFor networks evolved with epistatic fitness, selection pressure points away from the\nnetwork center but without epistatic fitness, selection pressure points both toward and away\nfrom the network center. It was also found that older and better fitness nodes tend to be\nlocated more towards the center of the network. Additional experiments are needed in\norder to better understand this behavior of SOTEA, however it is believed that the selection\npressure patterns shown here ultimately play an important role in explaining why genetic\ndiversity is maintained at such high levels in SOTEA.\nIt should also be mentioned that the two networks shown in Figure 5-12 are taken after 100\ngenerations of SOTEA evolution. Typically the amount of time required for the selforganization of network structure to take place was less than 100 generations however no\nattempt was made at determining the exact time when this transient was complete. Beyond\n100 generations, it was found that topological characteristics of the SOTEA network as\nwell as network visualizations were very consistent.\n\n170\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-12: Selective pressure patterns in the SOTEA network with (top) and without (bottom) epistasis.\nSelective pressure in the network is shown with arrows in black for pressure directed away from the network\ncenter and green for other directions of pressure. Selective pressure directions have only been calculated for nodes\nlocated near the network center. The arrows are drawn by selecting a node and drawing an arrow from this node\nto its worst neighbor. The worst fit neighbor is determined by epistatic fitness (5-9) for the top graph and by the\nObjective Function Value for the bottom graph.\n\n5.3.3.6\n\nThe Impact of SOTEA model parameters\n\nThe SOTEA model includes parameters Padd and Premove for controlling how much the\nconnections of an offspring are different from the connections of its parent.\n\nThese\n\nparameters are conceptually similar to a mutation rate for network topology and they will\n\n171\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\ncontrol the amount of structural innovation that is introduced to the network. This in turn is\nexpected to impact the range of topologies that are possible during evolution. To get a\nsense of the impact of these parameters, Figure 5-13 presents networks that were evolved\nwith different settings for Padd and Premove. As seen in Figure 5-13a, when no innovation is\npossible (Padd = Premove = 0), the network consistently takes on a structure that resembles a\nsimple branching process. When both parameters are increased to 0.1, as seen in Figure\n5-13b, some clustering begins to emerge. However, when the parameters are increased a\nlittle more to 0.2 (see Figure 5-13b), the structure changes dramatically and is dominated by\na single highly connected cluster.\n\na)\n\nb)\n\nc)\n\nFigure 5-13 SOTEA networks evolved using different parameter settings for the reproduction rule. In the\nreproduction rule, a parent's connections are inherited by its offspring with probability Padd followed by each of\nthe inherited connections being lost by the parent with probability Premove. Population interaction networks were\nevolved for a) Padd = Premove = 0.0%, b) Padd = Premove = 10%, c) Padd = Premove = 20%.\n\n172\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.3.4\n\n5.3.4.1\n\nDiscussion\n\nSOTEA Guidelines\n\nIt would be useful to be able to generalize the results shown in this work and develop a\nframework under which network dynamics would be beneficial to an EA population.\nAlong these lines, several aspects of SOTEA have been highlighted as important features of\nthe design.\nFirst, the most important precondition for network self-organization was to have changes in\nnetwork structure be driven by the same forces that drive population dynamics; namely a\nfitness-based selection pressure.\nThe SOTEA model also used a dual coupling between network states and structure which\nsubstantially improved the behavior of the system.\n\nIn particular, network dynamics\n\ndepended upon node states (due to the SOTEA competition rule) and node states depended\nupon network structure (due to the use of epistatic fitness). It is worth mentioning that the\ncoevolution of structure and states is a topic of great significance to the study of complex\nnetworks. To this author's knowledge, a dual coupling between states and structure is not\npresent in any other model of complex systems available in the literature.\nIt is also interesting to note that selecting the worst neighbor in the competition rule is also\nsimilar to the extremal dynamics used in most models of self-organized critical systems as\nreviewed in [177]. By eliminating the worst individual in a neighborhood, SOTEA may\nactually be using an important driving force for some self-organizing processes in nature.\nAdditional experimentation is needed to substantiate these claims.\n\n5.3.4.2\n\nThe NK Model as an optimization research tool\n\nThere are valid concerns about the extent to which the NK model, as currently defined,\nrepresents real optimization problems of interest. One concern is that the topological\nproperties of the NK landscape's gene interaction network are similar to a random graph\nand do not correspond with the topological properties of many real world systems.\n\n173\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nAs reviewed in this chapter, many complex systems have a number of similarities in their\ntopological properties and these properties are also clearly non-random.\n\nGiven an\n\nappropriate problem representation, it is speculated that this (approximate) universality\ncould extend to a large and interesting class of optimization problems based on the fact that\nmany problems of interest are large complex systems of interacting components. To test\nthis simple idea, it would be necessary to develop course-grained network approximations\nof fitness landscapes for a number of real-world optimization problems.\n\nReal world\n\nproblems could then be probed using tools developed in statistical mechanics in order to\ndetermine whether any common structural properties exist.\nUnder the reasonable assumption that some topological properties are repeated in many real\nworld problems, it should be possible to develop network evolution models which can\nevolve similar structures. Given the success of recent models in mimicking topological\nproperties of complex systems, this task should not be too difficult. The result of these\nefforts would be a fitness landscape generator, similar in principle to the NK landscape, but\none that has the capacity to generate problem instances with properties that are similar to\nreal world problems. It is also worth mentioning that this suggestion has some similarities\nto the proposal by Kauffman for probing gene regulatory networks [219].\n\n5.3.5\n\nConclusions\n\nThis work was intended as an initial investigation into the self-organization of interaction\nnetworks for an Evolutionary Algorithm. Motivating this research was a desire to acquire\nstructural characteristics of complex biological systems which are believed to be relevant to\ntheir behavior. In addition, this work also aimed to create an artificial system with a\ncapacity for sustainable coexistence of distinct components within a competitive\nenvironment (i.e. sustainable diversity).\nPopulation diversity was not imposed upon the EA as is traditionally done but instead\nemerges in the system as a natural consequence of population dynamics.\n\nThe\n\nenvironmental conditions which enable sustainable diversity are similar to what is observed\nin complex biological systems. These conditions involved a self-organizing interaction\nnetwork and a contextual definition of individual fitness which was referred to as epistatic\n\n174\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nfitness. In addition, high levels of diversity also required evolution to take place within a\nrugged fitness landscape.\n\n5.4 SOTEA Model II\nThe previous SOTEA model demonstrated that fitness was a natural property for tying\nnetwork structural dynamics to the evolutionary dynamics of an EA population. Different\nforms of fitness measurements were considered including a contextual form (called\nepistatic fitness) where an individual's fitness is defined by its own local environment.\nThe next SOTEA model focuses on the emergence of additional structural properties that\nwere not present in the first SOTEA model. One of these properties is modularity which is\na structural feature that is heavily exploited in natural evolution. To encourage modularity,\nthe second model uses both fitness and measures of community cohesion to drive network\ndynamics. The new SOTEA is also designed for use with multi-parent search operators\nwhich are standard in most EA designs and were missing in the first SOTEA model.\nSection 5.4.1 describes the new SOTEA model including new driving forces for network\ndynamics and new rules for implementing changes to network structure. Section 5.4.2\npresents the experimental setup including the remaining aspects of the core EA design.\nResults are presented in Section 5.4.3 and include both an analysis of performance and a\ncomparison of topological properties between SOTEA and complex biological systems.\nThe performance results provide evidence that the new SOTEA exhibits robust search\nbehavior with strong performance on many problems. Discussion and conclusion sections\nfinish the chapter in Sections 5.4.4 and 5.4.6.\n\n5.4.1\n\nModel Description\n\nThis section describes the new SOTEA network model used to couple the network topology\nto the population dynamics of an Evolutionary Algorithm. With the new model, network\ndynamics are driven by a measure of node fitness and by a measure of node modularity as\ndescribed in Section 5.4.1.1.\n\nThese dynamics are implemented by rewiring localized\n\nregions of the network as described in Section 5.4.1.2.\n175\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nAs previously mentioned, the network is described by an adjacency matrix J such that\nindividuals i and j are connected (not connected) when Jij=1 (Jij=0). This work only deals\nwith undirected networks such that Jij = Jji. The terms individual and node are used\ninterchangeably to refer to individual members of the EA population situated within the\npopulation network. Also, the terms links and connections are used interchangeably to\nrefer to directly connected nodes (i.e. individuals that are neighbors in the population).\n\n5.4.1.1\n\nDriving Forces\n\nTwo properties are used to drive network dynamics in this SOTEA model which are\ndescribed in this section.\n\n5.4.1.1.1 k Adaptation\nFor most real networks, the degree k is not constant (unlike what is seen in lattices) but\ninstead takes on a distribution of values often fitting exponential or power law distributions.\nHow nodes come to obtain k values that are higher than others depends on the system under\nstudy and could be historically motivated or could be motivated by some form of intrinsic\nnode fitness. The former has been theorized to take place in the process of genome\ncomplexification which has been modeled primarily by the previously mentioned DD\nmodel. Other popular models such as the BA model and its associated mechanism of\n\"preferential attachment\" also appear to derive k distributions using an historical bias. An\nalternative is the \"good get richer\" concept [204] introduced in the fitness model in Section\n5.1.3.3, where individuals of high fitness are driven to obtain higher k.\nIn this model, an adaptive set point KSet is used to define a node's desired number of links\nsuch that high fitness individuals in the population are encouraged to acquire a larger\nnumber of connections as defined in (5-13). Although conceptually similar to the work in\n[204], the node's fitness will have an ability to evolve due to the dynamics of the EA\npopulation which is something previously unexplored in network evolution models. The\nKSet parameter is defined in (5-14) as a quadratic function of rank which has a lower bound\nof KMin = 3 and an upper bound KMax which must be set by the user. The Rank term in\n(5-14) refers to an individual's fitness based ranking with Rank = 1 being the best\nindividual and Rank = N the worst individual in the population.\n176\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nMin\n\nk i \u2212 K Set ,i\n\n2\n\u239b\n\u239b N \u2212 Rank \u239e \u239e\u239f\nK Set ,i = K Min + \u239c (K Max \u2212 K Min )\u239c\n\u239f \u239f\n\u239c\nN\n\u239d\n\u23a0 \u23a0\n\u239d\n\n(5-13)\n(5-14)\n\n5.4.1.1.2 Weighted Clustering Coefficient\nAs mentioned in the review of network properties in Section 5.1.2, many complex\nbiological systems have a high level of modularity (as measured by the clustering\ncoefficient) and a clear hierarchical structure. Networks with hierarchical structure are\nexpected to have a clustering coefficient that is inversely related to a node's degree (i.e. c-k\ncorrelations). The second property used to drive network dynamics attempts to explicitly\naddress both observations.\nBased on the previously stated driving force for k adaptation, it is expected that nodes in\nSOTEA networks with higher fitness will also have higher k. To encourage high levels of\nmodularity as well as the inverse relationship between c and k (needed for hierarchy),\nnetwork rewiring is driven to maximize a weighted version of the clustering coefficient as\ndefined in (5-15).\n\nIn this new version of the clustering coefficient, a connection's\n\ncontribution to c is weighted to give less importance to connections involving nodes of\nhigher fitness. The weight W for each connection is defined in (5-17) which alters the ei\nterm of the clustering coefficient equation, as seen in (5-16). Notice the similarity between\n\nW and the intrinsic fitness measure which is defined in (5-8) and was first presented in\n[204].\n\nMax ci* =\n\nN\n\nN\n\nj =1\n\nk =1\n\n2ei*\nk i (k i \u2212 1)\n\nei* = \u2211 J ij \u2211 J ik J jk W jk , i \u2260 j \u2260 k\n\nW jk =\n\nRank j \u00d7 Rank k\nN\n\n(5-15)\n\n(5-16)\n\n(5-17)\n\n2\n\n177\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.4.1.2\n\nMechanics of Network Rewiring\n\nThe previous section described two properties which will act as driving forces for network\ndynamics. In particular, each node will be driven to obtain a specific value of k based on its\nfitness and defined in (5-14) and each node will be driven to maximize the value of a\nweighted clustering coefficient given in (5-15). To obtain these goals, changes to network\nstructure must take place involving the addition, removal, and local transfer of links in a\nnetwork. Although conceptually simple, these rules must satisfy a number of constraints in\naddition to and sometimes superseding the driving forces previously stated. These rules are\ndescribed next and are also demonstrated in Figure 5-14.\nThe \"Add Link Rule\" and the \"Remove Link Rule\" are two rewiring rules that have been\ncreated in order to allow the k value for each node to reach KSet.\nAdd Link Rule: Starting with a selected node N1, a two step random walk is taken,\n\nmoving from node N1 to node N2 to node N3. If N1 wants to increase its number of links\n(kN1 < KSet) and N3 wants to increase its number of links (kN3 < KSet) then add a link\nbetween N1 and N3.\nRemove Link Rule: For a selected node N1 with kN1 > KSet, a two step random walk is\n\ntaken, moving from node N1 to node N2 to node N3. If N3 is already connected to N1\n(JN1,N3 =1) and kN3 > KSet then remove the connection between N1 and N3. Notice the\npresence of N2 with JN2,N1 = JN2,N3 = 1 ensures that connections removed using this rule do\nnot result in network fragmentation.\nThe \"Transfer Link Rule\" allows for the improvement of clustering locally within the\nnetwork. However, this rule is not allowed to result in net violations to k adaptation.\nTransfer Link Rule: For a selected node N1 a two step random walk is taken, moving\n\nfrom node N1 to node N2 to node N3. If kN3 < KSet, then the connection between N1 and N2\nis transferred to now be between N1 and N3 (i.e. JN1,N2 = 1, JN1,N3 = 0 changes to JN1,N2 = 0,\n\nJN1,N3 = 1). To determine if the transfer will be kept, the local modularity is calculated\nusing (5-15) for N1, N2 and N3 both BEFORE and AFTER the connection transfer. If\n\n(c\n\n*\nN1\n\n+ c *N 2 + c *N 3 ) increases after the connection transfer then the transfer is kept, otherwise\n178\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nit is reversed. In this way connections are only added which strengthen the weighted\nclustering metric and don't cause a net increase in KSet violations.\n\nN3\n\nN1\nAdd\n\nN2\n\nTransfer\n\nRemove\n\nN3\n\nN1\n\nN2\n\nN3\n\nN1\n\nN2\n\nN3\n\nN1\n\nN2\n\nFigure 5-14 Adaptive Network Rules: A selected node N1 will attempt to add, remove or transfer its connections\nbased on the satisfaction of constraints and the improvement of properties. Add Rule: The dotted line represents\na feasible new connection in the network assuming nodes N1 and N3 both would like to increase their number of\nconnections. Remove Rule: The gray dotted line represents a feasible connection to remove in the network\nassuming nodes N1 and N2 both have an excess of connections. Transfer Rule: The connection between N1 and\nN2 (gray dotted line) being transferred to now connect N1 and N3 (black dotted line) represents a feasible transfer\nassuming this action results in an overall improvement to local clustering.\n\nSince there are several constraints that the random walks (in the rewiring rules) must\nsatisfy, up to a maximum of 10 random walks are conducted starting from N1 for each\ninstance of rule execution in an attempt to satisfy the conditions. An upper bound on the\nnumber of walks is needed because there is no guarantee that a random walk exists which\nsatisfies all conditions.\n\n5.4.2\n\n5.4.2.1\n\nExperimental Setup\n\nAlgorithm Designs\n\nSOTEA: A high level pseudocode for SOTEA is provided below. The algorithm starts by\n\ndefining the initial population P on a ring topology with each node connected to exactly\ntwo others (e.g. Figure 5-1c). For a given generation t, each node is subjected to both\nnetwork rewiring rules and standard genetic operators. For a given node N1, each of the\n\n179\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nthree network rewiring rules are executed (defined in Section 5.4.1.2). Afterwards, N1 is\nselected as a parent and a second parent N2 is selected by conducting a two step random\nwalk across the network. An offspring is then created using the parents and a single search\noperator selected at random from the list in Table 5-2.18\n\nThe better fit between the\n\noffspring and N1 is stored in a temporary list Temp(N1) while the network rewiring rules\nand genetic operators are repeated on each of the remaining nodes in the population. To\nbegin the next generation, the population is updated with the temporary list. This process\nrepeats until some stopping criteria is met. In this case, the stopping criteria is a maximum\nof 3000 generations.\nPseudocode for SOTEA\nt=0\nInitialize P(t) (at random)\nInitialize population topology (ring structure)\nEvaluate P(t)\nDo\nFor each N1 in P(t)\nAdd Link Rule(N1)\nRemove Link Rule(N1)\nTransfer Link Rule(N1)\nSelect N1 as a first parent\nSelect parent N2 by conducting a two step random walk from N1\nSelect Search Operator (at random)\nCreate and Evaluate offspring\nTemp(N1) = Best_of(offspring, N1)\nNext N1\nt=t+1\nP(t) = Temp()\nLoop until stopping criteria\n\ncellular GA: A cellular GA is also tested in these experiments which is identical to\n\nSOTEA except for two design changes. First, the cGA does not use any of the network\nrewiring rules (Add, Remove, Transfer) that are used in SOTEA. This means the cGA has\na static ring topology. Also, when creating an offspring, the second parent N2 is selected\n\n18\n\nNo parameter tuning was attempted and all search operators are used with equal probability.\n\n180\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\namong all neighbors within a radius R of N1 using linear ranking selection. A high level\npseudocode is for the cGA is provided below.\nPseudocode for cGA\nt=0\nInitialize P(t) (at random)\nInitialize population topology (ring structure)\nEvaluate P(t)\nDo\nFor each N1 in P(t)\nSelect N1 as first parent\nSelect N2 from Neighborhood(N1,R)\nSelect Search Operator (at random)\nCreate and evaluate offspring\nTemp(N1) = Best_of(offspring, N1)\nNext N1\nt=t+1\nP(t) = Temp()\nLoop until stopping criteria\nPanmictic EA: SOTEA is also compared against a number of Panmictic EA designs. The\n\ncore of the Panmictic EA is given by the pseudocode below. For this pseudocode, the\nparent population of size \u03bc at generation t is defined by P(t). For each new generation, an\noffspring population P`(t) of size \u03bb is created through variation (search) operators and is\nevaluated to determine fitness values for each offspring. The parent population for the next\ngeneration is then selected from P`(t) and Q, where Q is subset of P(t). Q is derived from\n\nP(t) by selecting those in the parent population with an age less than \u03ba.\nPseudocode for Panmictic EA\nt=0\nInitialize P(t)\nEvaluate P(t)\nDo\nP`(t) = Variation(P(t))\nEvaluate (P`(t))\nP(t+1) = Select(P`(t) \u222a Q)\nt=t+1\nLoop until stopping criteria\n\nEight EA designs are tested which vary by the use of generational (with elitism) vs. pseudo\nsteady state population updating, the use of binary tournament selection vs. truncation\n\n181\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nselection, and by the number of search operators. Details are given below for each of the\ndesign conditions.\nPopulation updating: The generational EA design (with elitism for retaining the best\n\nparent) has the parameter settings N=\u03bb=2\u03bc, \u03ba=1 (\u03ba=\u221e for best individual). The pseudo\nsteady state EA design has the parameter settings N=\u03bb=\u03bc, \u03ba=\u221e. Unless otherwise stated,\n\nN=50. Each experiment ran for 3000 generations meaning that 150,000 objective function\nevaluations are required to obtain a final solution in each experimental run.\nSelection: Selection occurs by either binary tournament selection (without replacement) or\n\nby truncation selection. Both selection methods are described in Chapter 2.\nSearch Operators: For each EA design, an offspring is created by using a single search\n\noperator. Two designs were considered: i) a seven search operator design and ii) a two\nsearch operator design. For the seven operator case, an offspring is created by an operator\nthat is selected at random from the list in Table 5-2 (no parameter tuning was attempted).\nFor the two operator case, uniform crossover is used with probability = 0.98 and single\npoint random mutation is used with probability = 0.02. Search operator descriptions are\nprovided in Appendix B.\nTable 5-2: Names of the seven search operators used in the cellular GA, SOTEA, and selected Panmictic EA\ndesigns are listed below. More information on each of the search operators can be found in Appendix B.\n\nSearch Operator Names\nWright's Heuristic Crossover\nSimple Crossover\nExtended Line Crossover\nUniform Crossover\nBLX- \u03b1\nDifferential Operator\nSingle Point Random Mutation\nConstraint Handling: Each of the engineering design case studies involve nonlinear\n\ninequality constraints meaning that solution feasibility must be addressed. Feasibility is\ndealt with by defining fitness using the Stochastic Ranking method presented in [64] as\nopposed to defining fitness by the objective function. The parameter settings for Stochastic\nRanking were taken from the suggestions in [64].\n\n182\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.4.2.2\n\nEngineering Design Case Studies\n\nExperiments for assessing SOTEA performance are conducted on six engineering design\nproblems and six artificial test problems taken from the literature (described in Appendix\nA) and compared against cellular Genetic Algorithms and Panmictic Evolutionary\nAlgorithm designs.\nThe first four engineering design problems were chosen due to a prior difficulty in solving\nthese problems using Panmictic Evolutionary Algorithms and the difficulty that others have\nhad in solving these problems in general. Some of these test problems are small enough\nthat mathematical programming techniques have been used to solve for the global optimal\nsolution (problems 1, 2, and 3) which has helped in the assessment of algorithm\nperformance. In preliminary experiments conducted on the last two engineering design\nproblems, some Panmictic EA designs were able to find solutions that were better than\nthose reported in the literature. These problems were included in this work to see if\nadditional improvements could be made using SOTEA.\n\n5.4.3\n\nResults\n\nGeneral Performance Statistics: This section attempts to draw general conclusions about\n\nthe three EA design classes (Panmictic EA, cellular GA, and SOTEA) tested in these\nexperiments. The first statistic shown in column two of Table 5-3 measures the percentage\nof runs that an EA design class was able to find the optimal solution (optimal defined as the\nbest solution value found in all experiments). This percentage is an average over all test\nproblems. The second statistic shown in column three measures the percentage of runs\nwhere an EA design class finds a solution that ranks in the top 5% of all solutions found in\nthese experiments. The third statistic shown in column four is a p value for the MannWhitney U-test where the statistical hypothesis is that the given EA design class is superior\nto the other two EA design classes. The fourth statistic shown in column five measures the\npercentage of problems where the best EA design belonged to a particular design class19.\n\n19\n\nNotice that eight Panmictic EA designs were used in these experiments while only four cellular GA and\nfour SOTEA designs were used. This should bias columns 3 and 6 of Table 5-3 to favor the Panmictic EA.\n\n183\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nThe sixth column looks at the percentage of problems where an EA design class was able to\nfind the best solution at least one time.\nFor each of the statistics in Table 5-3, SOTEA is found to be significantly better than the\nother EA design classes based on the 12 problems tested in these experiments. Particularly\nimpressive are the results in column five which indicate that the SOTEA design is the best\nEA design in about 80% of the problems tested.\nTable 5-3 Overall performance statistics for the Panmictic EA, the cellular GA, and SOTEA. Column two\nmeasures the percentage of runs where the optimal solution was found. The optimal solution is defined as the best\nsolution found in these experiments. Column three measures the percentage of runs where the solution ranks in\nthe top 5% of solutions from all EA designs. In column four, \"p\" indicates the p value for the Mann-Whitney Utest where the hypothesis is that the given EA design class is superior to the other two EA design classes. Column\nfive measures the percentage of problems where the best EA design belonged to a particular design class. Column\nsix measures the percentage of problems where an EA design class was able to find the best solution at least one\ntime. Statistics in columns 1-3 are an average value over all test problems.\n\nEA Design\nPanmictic EA\ncellular GA\nSOTEA\n\n% of runs where EA\nfound best was top 5%\n4.2%\n5.1%\n9.4%\n11.6%\n16.9%\n25.2%\n\nU-Test\np\n0.87\n0.50\n0.13\n\n% of problems where EA\nwas best design found best\n8.3%\n16.7%\n12.5%\n66.7%\n79.2%\n83.3%\n\n184\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.4.3.1\n\nEngineering Design Performance Results\n\nPressure Vessel\n5990\n\nF\n\n5970\n\nSOTEA-Kmax3\n\n5950\n\nSOTEA-Kmax5\nSOTEA-Kmax7\n\n5930\n\nSOTEA-Kmax9\ncGA-R1\n\n5910\n\ncGA-R4\n5890\n\ncGA-R8\ncGA-R12\n\n5870\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-15 Performance results for the Pressure Vessel design problem are shown over 3000 generations for\nSOTEA with different settings of Kmax, and for cellular GA with different values of the neighborhood radius R.\nPerformance for each EA is an average over 20 runs of the best fitness (objective function) value in the population.\nInfeasible solutions are neglected from the calculations, however all runs obtained feasibility within the first 100\ngenerations. The global optimal solution has a fitness of 5850.38.\n\nAlkylation Process\n1770\n1765\nSOTEA-Kmax3\n1760\n\nSOTEA-Kmax5\nSOTEA-Kmax7\n\nF 1755\n\nSOTEA-Kmax9\ncGA-R1\n\n1750\n\ncGA-R4\n1745\n\ncGA-R8\ncGA-R12\n\n1740\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-16 Performance results for the Alkylation Process design problem are shown over 3000 generations for\nSOTEA with different settings of Kmax, and for cellular GA with different values of the neighborhood radius R.\nPerformance for each EA is an average over 20 runs of the best fitness (objective function) value in the population.\nInfeasible solutions are neglected from the calculations, however all runs obtained feasibility within the first 1400\ngenerations. Several instances can be observed where fitness values momentarily decrease. This is the result of EA\nruns turning from infeasible to feasible where the new feasible solution is lower than the average performance for\nthat EA design and generation. The global optimal solution has a fitness of 1772.77.\n\n185\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nHeat Exchanger Network Design\n7068\nSOTEA-Kmax3\n7063\nF\n\nSOTEA-Kmax5\nSOTEA-Kmax7\n\n7058\n\nSOTEA-Kmax9\ncGA-R1\ncGA-R4\n\n7053\n\ncGA-R8\ncGA-R12\n\n7048\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-17 Performance results for the Heat Exchanger Network design problem are shown over 3000\ngenerations for SOTEA with different settings of Kmax, and for cellular GA with different values of the\nneighborhood radius R. Performance for each EA is an average over 20 runs of the best fitness (objective function)\nvalue in the population. Infeasible solutions are neglected from the calculations, however all runs obtained\nfeasibility within the first 100 generations. The global optimal solution has a fitness of 7049.25.\n\nGear Train Design\n1.E-09\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\nSOTEA-Kmax3\nSOTEA-Kmax5\nSOTEA-Kmax7\n\nF 1.E-10\n\nSOTEA-Kmax9\ncGA-R1\ncGA-R4\ncGA-R8\ncGA-R12\n\n1.E-11\n\nGen\nFigure 5-18 Performance results for the Gear Train Design design problem are shown over 3000 generations for\nSOTEA with different settings of Kmax, and for cellular GA with different values of the neighborhood radius R.\nPerformance for each EA is an average over 20 runs of the best fitness (objective function) value in the population.\nInfeasible solutions are neglected from the calculations, however all runs obtained feasibility within the first 50\ngenerations. The global optimal solution is unknown, however the best result previous to this work, is reported in\n[220] as 2.70E-12.\n\n186\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nTension Compression Spring\n0.012675\n0.012673\n\nSOTEA-Kmax3\nSOTEA-Kmax5\n\n0.012671\nF\n0.012669\n\nSOTEA-Kmax7\nSOTEA-Kmax9\ncGA-R1\ncGA-R4\n\n0.012667\n\ncGA-R8\ncGA-R12\n\n0.012665\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-19 Performance results for the Tension Compression Spring Design design problem are shown over 3000\ngenerations for SOTEA with different settings of Kmax, and for cellular GA with different values of the\nneighborhood radius R. Performance for each EA is an average over 20 runs of the best fitness (objective function)\nvalue in the population. Infeasible solutions are neglected from the calculations, however all runs obtained\nfeasibility within the first 50 generations. The global optimal solution is unknown, however the best result previous\nto this work, is reported in [221] as 0.01270.\n\nWelded Beam Design\n1.72495\n\nSOTEA-Kmax3\nSOTEA-Kmax5\n\nF\n\nSOTEA-Kmax7\nSOTEA-Kmax9\ncGA-R1\ncGA-R4\ncGA-R8\ncGA-R12\n\n1.72485\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-20 Performance results for the Welded Beam Design design problem are shown over 3000 generations for\nSOTEA with different settings of Kmax, and for cellular GA with different values of the neighborhood radius R.\nPerformance for each EA is an average over 20 runs of the best fitness (objective function) value in the population.\nInfeasible solutions are neglected from the calculations, however all runs obtained feasibility within the first 50\ngenerations. The global optimal solution is unknown, however the best result previous to this work, is reported in\n[222] as 1.7255.\n\n187\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-21 Final performance results for the Pressure Vessel (Left), Alkylation Process (Middle) and Heat\nExchanger Network (Right) design problems are shown with box plots of performance data grouped by Panmictic\nEA, cellular GA, and SOTEA. The box plots represent final algorithm performance (after 3000 generations) over\n20 runs for all cGA, SOTEA, and Panmictic EA designs. This includes data from the four cGA designs (with\ndifferent parameter settings for neighborhood radius R), the four SOTEA designs (with different parameter\nsettings for KMax), and the eight Panmictic EA designs described in Section 5.4.2.1. Insets are provided for the\ncGA and SOTEA box plots to highlight the difference in results between these two algorithms. Also notice that the\nPressure Vessel and Heat Exchanger Network problems are Minimization problems while the Alkylation Problem\nis a Maximization problem.\n\nResults for Pressure Vessel Design Problem: For the Pressure Vessel design problem, all\n\nbut one of the SOTEA algorithms outperformed all of the cellular GA designs as seen in\nFigure 5-15. Performance also tended to improve as network connectivity was reduced for\nboth SOTEA and the cGA. In light of this trend, it is not surprising to see the performance\nof the suite of Panmictic EA designs performed very poorly on this problem as seen in\nFigure 5-21. Comparing results between Figure 5-15 and Table C-7 (in Appendix C), the\nbest final solution for a Panmictic EA design is beaten by all SOTEA designs after only 300\ngenerations.\nComparisons to work from previous authors highlights the strong performance of both of\nthe distributed Evolutionary Algorithms. Of the 8 papers referenced in and including [223],\nonly one other algorithm has been able to reach the objective function values obtained by\n\n188\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nthe distributed EA designs employed here. Performance comparison tables are provided in\nAppendix C.\nResults for Alkylation: For the Alkylation Process design problem, all but one of the\n\nSOTEA algorithms outperformed the cellular GA designs as seen in Figure 5-16. In this\nproblem there was no clear trend between performance and network connectivity. It is also\nclear that many of the algorithms were able to find improvements throughout the run\nsuggesting that convergence did not occur within the 3000 generations considered. Hence,\nit is possible that the conclusions drawn here would change if evolution was considered\nover a larger time scale. The Panmictic EA designs again performed relatively poorly on\nthis problem as seen in Figure 5-21.\nComparisons to work from previous authors highlights the strong performance of the\ndistributed Evolutionary Algorithms. Of the stochastic search methods described in the 5\npapers referenced in [224] including their own Differential Evolution Algorithm, none\nreached the fitness values obtained by the distributed EA designs employed here.\nHowever, two \u03b1BB (Branch and Bound Non-Linear Programming) algorithms were cited\nwhich did find the global optimum.\n\nPerformance comparison tables are provided in\n\nAppendix C.\nResults for HEN: For the Heat Exchanger Network design problem, all of the SOTEA\n\nalgorithms outperformed the cellular GA designs as seen in Figure 5-17. Performance also\ntended to improve as network connectivity was increased for both SOTEA and the cGA.\nSuch a trend seems to suggest that interaction constraints are not needed for this problem\nwhich makes the poor performance of the Panmictic EA designs (see Figure 5-21) a little\nsurprising. Comparing results between Figure 5-17 and Table C-7 (in Appendix C), the\nbest final result for a Panmictic EA design is beaten by all SOTEA designs after only 400\ngenerations.\nComparisons to other work are less favorable in this case. For instance, in [224], they\nintroduce a Differential Evolution Algorithm that can find the optimal solution 100% of the\ntime in under 40,000 evaluations. None of the algorithms employed here were able to\nobtain that level of performance for this problem. In fact, the best algorithm (SOTEA with\n\nKmax = 7) was only able to find the optimal solution 65% of the time in 150,000 evaluations.\n\n189\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nTo make a fair comparison to the results in [224], the results from this thesis were also\nanalyzed at 40,000 evaluations and under these conditions only two of the SOTEA\nalgorithms (and none of the cellular GAs) were able to find an optimal solution in that\namount of time (with the optimal being found only 10% of the time).\n\nIt is worth\n\nmentioning that this was one of the simplest design problems tested with only a marginal\nlevel of epistasis between parameters (e.g. see problem definition in Appendix A).\n\nFigure 5-22 Final performance results for the Gear Train (Left), Tension Compression Spring (Middle) and\nWelded Beam (Right) design problems are shown with box plots of performance data grouped by Panmictic EA,\ncellular GA, and SOTEA. The box plots represent final algorithm performance (after 3000 generations) over 20\nruns for all cGA, SOTEA, and Panmictic EA designs. This includes data from the four cGA designs (with different\nparameter settings for neighborhood radius R), the four SOTEA designs (with different parameter settings for\nKMax), and the eight Panmictic EA designs described in Section 5.4.2.1. When necessary, insets are provided for\nthe cGA and SOTEA box plots (with data shifted and plotted on a log scale) to highlight the difference in results\nbetween these two algorithms. All three design problems are Minimization problems.\n\nResults for Gear Train Design Problem: For the gear train design problem, there was no\n\nclear distinction in performance between the cellular GA and SOTEA. One of the cellular\nGA designs (R=12) was found to have better average performance than any of the SOTEA\ndesigns as seen in Figure 5-18 however comparison of end performance between the\ncellular GA, SOTEA, and the Panmictic EA shows very little difference as seen in Figure\n\n190\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5-22. Of the two papers referenced in and including [220], one previous method has been\nable to find the solutions achieved in this work.\nResults for Tension Compression Spring Design Problem: For the tension compression\n\nspring design problem, all but one of the distributed EA designs were found to converge to\nnearly identical values as seen in Figure 5-19. The SOTEA design with KMax=5 was found\nto have worse performance than the other designs.\n\nHowever, comparison of end\n\nperformance as shown in Figure 5-22 shows SOTEA did have a better median performance\ncompared to the cellular GA.\nComparisons to work from previous authors highlights the strong performance of both of\nthe distributed Evolutionary Algorithms. Of the three papers referenced in and including\n[221], no previous method has been able to find the solutions achieved in this work.\nResults for Welded Beam Design Problem: For the welded beam design problem, each\n\nof the distributed EA designs were found to converge to nearly identical values as seen in\nFigure 5-20. Both distributed EA designs strongly outperformed the Panmictic EA as seen\nin Figure 5-22.\nComparisons to work from previous authors highlights the strong performance of both of\nthe distributed Evolutionary Algorithms. Of the 3 papers referenced in and including [222],\nno previous method has been able to find the solutions achieved in this work.\n\n191\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.4.3.2\n\nArtificial Test Function Results\n\nFrequency Modulation\n20\n19\n18\n\nSOTEA-Kmax3\n\n17\n\nSOTEA-Kmax5\nSOTEA-Kmax7\n\nF 16\n\nSOTEA-Kmax9\n\n15\n\ncGA-R1\n\n14\n\ncGA-R4\n\n13\n\ncGA-R8\ncGA-R12\n\n12\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-23 Performance results for the Frequency Modulation problem are shown over 3000 generations for\nSOTEA with different settings of Kmax, and for cellular GA with different values of the neighborhood radius R.\nPerformance for each EA is an average over 20 runs of the best fitness (objective function) value in the population.\nThe global optimal solution is 0.\n\nECC\n0.0050\n0.0048\n\nSOTEA-Kmax3\nSOTEA-Kmax5\n\n0.0046\n\nSOTEA-Kmax7\n\nF\n\nSOTEA-Kmax9\n\n0.0044\n\ncGA-R1\ncGA-R4\n\n0.0042\n\ncGA-R8\ncGA-R12\n\n0.0040\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-24 Performance results for the error correcting code (ECC) problem are shown over 3000 generations\nfor SOTEA with different settings of Kmax, and for cellular GA with different values of the neighborhood radius R.\nPerformance for each EA is an average over 20 runs of the best fitness (objective function) value in the population.\nThe global optimal solution is 0.067416. Results are shifted so that global optima is 0.\n\n192\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nSystem of Linear Equations\n1.E+02\n1.E-01\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\nSOTEA-Kmax3\n\n1.E-04\nF\n1.E-07\n\nSOTEA-Kmax5\nSOTEA-Kmax7\nSOTEA-Kmax9\ncGA-R1\n\n1.E-10\n\ncGA-R4\n\n1.E-13\n\ncGA-R8\ncGA-R12\n\n1.E-16\n\nGen\n\nFigure 5-25 Performance results for the system of linear equations problem are shown over 3000 generations for\nSOTEA with different settings of Kmax, and for cellular GA with different values of the neighborhood radius R.\nPerformance for each EA is an average over 20 runs of the best fitness (objective function) value in the population.\nThe global optimal solution is 0.\n\nRastrigin\n1.E+01\n1.E+00\n1.E-01\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\nSOTEA-Kmax3\n\n1.E-02\n\nSOTEA-Kmax5\nSOTEA-Kmax7\n\nF 1.E-03\n\nSOTEA-Kmax9\n\n1.E-04\n\ncGA-R1\n\n1.E-05\n\ncGA-R4\ncGA-R8\n\n1.E-06\n\ncGA-R12\n\n1.E-07\nGen\nFigure 5-26 Performance results for the Rastrigin function are shown over 3000 generations for SOTEA with\ndifferent settings of Kmax, and for cellular GA with different values of the neighborhood radius R. Performance for\neach EA is an average over 20 runs of the best fitness (objective function) value in the population. The global\noptimal solution is 0.\n\n193\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nGriewangk\n0.10\n0.09\nSOTEA-Kmax3\n\n0.08\n\nSOTEA-Kmax5\n\n0.07\n\nSOTEA-Kmax7\n\n0.06\n\nSOTEA-Kmax9\n\nF\n\ncGA-R1\n\n0.05\n\ncGA-R4\n\n0.04\n\ncGA-R8\ncGA-R12\n\n0.03\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-27 Performance results for the Griewangk function are shown over 3000 generations for SOTEA with\ndifferent settings of Kmax, and for cellular GA with different values of the neighborhood radius R. Performance for\neach EA is an average over 20 runs of the best fitness (objective function) value in the population. The global\noptimal solution is 0.\n\nWatson's\n0.020\n\nSOTEA-Kmax3\n\n0.019\n\nSOTEA-Kmax5\nSOTEA-Kmax7\n\nF\n\nSOTEA-Kmax9\n\n0.018\n\ncGA-R1\ncGA-R4\ncGA-R8\ncGA-R12\n\n0.017\n0\n\n500\n\n1000\n\n1500\nGen\n\n2000\n\n2500\n\n3000\n\nFigure 5-28 Performance results for Watson's function are shown over 3000 generations for SOTEA with different\nsettings of Kmax, and for cellular GA with different values of the neighborhood radius R. Performance for each EA\nis an average over 20 runs of the best fitness (objective function) value in the population. The global optimal\nsolution is 0.01714.\n\n194\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-29 Final performance results for the Frequency Modulation (Left), Error Correcting Code (Middle) and\nSystem of Linear Equations (Right) test functions are shown with box plots of performance data grouped by\nPanmictic EA, cellular GA, and SOTEA. The box plots represent final algorithm performance (after 3000\ngenerations) over 20 runs for all cGA, SOTEA, and Panmictic EA designs. This includes data from the four cGA\ndesigns (with different parameter settings for neighborhood radius R), the four SOTEA designs (with different\nparameter settings for KMax), and the eight Panmictic EA designs described in Section 5.4.2.1. When necessary,\ninsets are provided for the cGA and SOTEA box plots (with data plotted on a log scale) to highlight the difference\nin results between these two algorithms. All three design problems are Minimization problems.\n\nResults for Frequency Modulation: For the frequency modulation problem, SOTEA\n\ndesigns are found to be both the best and worst performers (compared to the cellular GA)\nthroughout the optimization runs as seen in Figure 5-23. The larger distribution of SOTEA\nperformance is also evident in the final performance results shown in Figure 5-29. Here it\ncan also see that while both SOTEA and the cellular GA have much better median\nperformance than the Panmictic EA, only SOTEA and one of the Panmictic EAs were able\nto find the global optimal solution.\nResults for ECC: For the error correcting code problem, both SOTEA and the cellular GA\n\ndesigns are able to make steady progress toward the optimal solution with little difference\nbetween the two designs as seen in Figure 5-24. However, in the final distribution of\n\n195\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nresults shown in Figure 5-29, only SOTEA and one of the Panmictic EAs were able to find\nthe global optimal solution.\nResults for System of Linear Equations: For the system of linear equations test function,\n\nSOTEA designs overwhelmingly outperform the cellular GA as seen in Figure 5-25 and\nFigure 5-29. Also seen in Figure 5-29, both distributed EA designs were able to strongly\noutperform the Panmictic EA designs.\n\nFigure 5-30 Final performance results for the Rastrigin (Left), Griewangk (Middle) and Watson (Right) test\nfunctions are shown with box plots of performance data grouped by Panmictic EA, cellular GA, and SOTEA. The\nbox plots represent final algorithm performance (after 3000 generations) over 20 runs for all cGA, SOTEA, and\nPanmictic EA designs. This includes data from the four cGA designs (with different parameter settings for\nneighborhood radius R), the four SOTEA designs (with different parameter settings for KMax), and the eight\nPanmictic EA designs described in Section 5.4.2.1. When necessary, insets are provided for the cGA and SOTEA\nbox plots (with data shifted and plotted on a log scale) to highlight the difference in results between these two\nalgorithms. All three design problems are Minimization problems.\n\nResults for Rastrigin: For the Rastrigin test function, SOTEA designs overwhelmingly\n\noutperform the cellular GA and the Panmictic EA as seen in Figure 5-26 and Figure 5-30.\nAlthough both distributed EA designs have significantly better median performance than\n\n196\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nthe Panmictic EA designs, there is some indication that the Panmictic EA can occasionally\nfind better quality solutions than the cellular GA as seen in Figure 5-30.\nResults for Griewangk: For the Griewangk test function, SOTEA designs are very similar\n\nin performance to the cellular GA as seen in Figure 5-27. Both distributed EA designs\nperform better than the Panmictic EA designs as seen in Figure 5-30. However, from\nFigure 5-30 it also appears that SOTEA can occasionally find better quality solutions than\nthe cellular GA.\nResults for Watson:\n\nFor Watson's test function, SOTEA designs overwhelmingly\n\noutperform the cellular GA as seen in Figure 5-28. Both distributed EA designs perform\nbetter than the Panmictic EA designs as seen in Figure 5-30.\n\n5.4.3.3\n\nStructural Analysis\n\nThis section presents the structural characteristics of SOTEA and compares this with the\ncellular GA, the Panmictic EA, and values observed in complex biological systems. These\nresults indicate that, unlike standard EA population topologies, SOTEA obtains several\ncharacteristics observed in complex biological systems.\nMethods for SOTEA Topological Analysis: Because network dynamics in SOTEA take\n\nplace due to changes in node fitness and because node fitness is constantly evolving (due to\npopulation dynamics), the SOTEA network never fully converges to a stable structure. In\norder to determine topological characteristics, measurements are taken every 50 generations\nfor SOTEA run 10 times over 1000 generations. To consider the impact of system size,\ntopological properties for population sizes of N = 50, 100 and 200 have been measured with\nresults shown in Figure 5-31. Here it is seen that most properties show little dependency on\nthe population size except for L which is generally smaller for smaller systems. Figure\n5-31 also indicates that the topological properties of SOTEA are sensitive to the setting of\n\nKMax which is the only extra parameter of the SOTEA design. The topological property\nvalues for SOTEA with N=50 are also reported in Table 5-4, which are taken as an average\nover all KMax settings considered in this work (KMax = 3, 5, 7, 9).\n\n197\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\na)\n\n40\n\nb)\n\n0\n\nc)\n\n30\n\n-2\n\n30\n\n20\n\nc-k\n\nL 20\n\n-4\n\nv\n\n-6\n\n10\n\n10\n\n-8\n\n0\n\n0\n\n-10\n\n0\n\n5\nKmax\n\n10\n\n0\n\n5\nKmax\n\nd)\n\n0.8\n\n10\n\n0\n\n5\nKmax\n\n10\n\ne)\n\n5\n\n4\nk\n(ave)\n\nc\n(ave)\n\n3\n\n2\n\n0.4\n0\n\n5\nKmax\n\n10\n\n0\n\n5\nKmax\n\n10\n\nFigure 5-31 Topological properties for SOTEA with different values of KMax and population sizes of N = 50 (\u2666),\n100(\u25fc), and 200(\u25b2). Characteristics include a) the characteristic path length (L), b) the correlation between c and\nk (c-k), c) the slope of the degree correlation (\u03c5), d) the average clustering coefficient cave and e) the degree average\nkave.\n\n198\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nTable 5-4: Topological characteristics for the interaction networks of the Panmictic EA, cellular GA, and SOTEA.\nSOTEA networks are averages taken over all settings for KMax as described elsewhere. For comparison, common\ntopological characteristics of several biological systems are also provided (taken from [196] and references\ntherein). Characteristics include the characteristic path length L, the degree average kave, the linkage distribution\n(k dist.), the average clustering coefficient cave, correlation between c and k (c-k), and degree correlations (k-kNN).\nFor the k distribution, \u03b3 refers to the exponent for k distributions that fit a power law. Two values for \u03b3 are given\nfor the metabolic network and refer to the in/out-degree exponents (due to this being a directed network). Results\nfor degree correlations are given as the slope \u03c5 of kNN vs k. N is the population size, and R is a correlation\ncoefficient for the stated proportionalities.\n\nSystem\n\nN\n\nPanmictic 50\nEA\ncellular\n50\nGA\nSOTEA\n50\n\nL\n\nkave\n\nk dist.\n\nL=1\n\nkave = N-1 k = N-1\n\ncave\n(crand)\n1 (1)\n\nc-k\n\nk-kNN\n\nno\n\nno\n\nL~N\n\nkave = 2\n\nk=2\n\n0 (0.04) no\n\nno\n\n5.97\n\n3.6\n\nPoisson\n\n0.687\n(0.07)\nComplex Large L ~ log N kave << N Power Law, cave\nNetworks\n2<\u03b3<3\n>>crand\n(Scale Free\nNetwork)\nProtein\n2,115 2.12\n6.80\nPower Law, 0.07\n\u03b3 = 2.4\n(0.003)\nMetabolic 778\n7.40\n3.2\nPower Law, 0.7\n\u03b3 = 2.2/2.1 (0.004)\n\nc = -4.75k\n\n\u03c5 = 11.8\n\nPower Law\neither\n(Hierarchical) \u03c5 > 0\nor \u03c5 < 0\nPower Law\n\n\u03c5<0\n\nPower Law\n\n\u03c5<0\n\n5.4.3.3.1 Topological Properties of SOTEA\nThis section briefly comments on some of the topological properties of SOTEA and the\nrelevance of these properties to algorithm behavior.\nCharacteristic Path Length L: The total distance genetic material must travel across the\n\nnetwork is always small as indicated by small L suggesting there is always a potential for\nany two nodes to influence each other over a relatively small time scale. However, it\nshould be mentioned that a small path length does not necessarily mean strong interactions\noccur between different regions of the network (as suggested below). Additional studies on\nthe population dynamics of these systems are needed to verify the impact of small L.\nClustering Coefficient: The high value of the average clustering coefficient is potentially\n\nvery important to population dynamics and algorithm behavior in general. For example,\nconsider the impact of clustering on the random walks used for reproduction in SOTEA.\nRandom walks starting from within highly clustered regions of the network are unlikely to\ntravel outside the cluster (due to high levels of interconnectivity among neighbors). Such a\n\n199\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\ntopological feature may act to reduce the amount of communication between clusters, a\nbehavior reminiscent of the island model GA.\nDegree Average:\n\nThe low value for kave suggests the SOTEA network maintains a\n\nsparsely connected architecture with high levels of locality similar to that of the cellular\nGA.\nDegree distribution: k approximates a Poisson distribution which is not similar to the fat\n\ntailed distributions observed in complex systems or the distributions observed in the first\nSOTEA algorithm developed in Section 5.2. The distributions results suggest relatively\nlittle heterogeneity in k is present such that the level of locality is roughly uniform within\nthe system.\nPrevious studies, as reviewed in [197], have indicated that placing upper bounds on k can\nresult in strong deviations from a power law. This SOTEA model introduces very tight\nconstraints on the values of k (e.g. upper and lower bounds, quadratic set point) so k\ndistribution results should not be surprising. Future work will try to allow for higher levels\nof connection heterogeneity in the system, which is expected to become increasingly\nrelevant to system behavior as larger population sizes are considered.\nDegree-Degree correlations: The assortative character of the SOTEA networks (\u03c5 > 0)\n\nsuggests high fitness nodes are driven to preferentially interact with other high fitness\nnodes. Such a population topology might provide a natural robustness to the search process\nallowing for the coexistence of explorative and exploitive behaviors within a single system.\nClustering-Degree correlations: The linear relation between c and k suggests that some\n\nmarginal levels of hierarchy exist within the network however its presence is unlikely to\npersist with larger population sizes. In the current SOTEA design, it is not clear what role\n(if any) that hierarchy would play in algorithm behavior however this could change if nodes\nwere able to take on a diverse range of behaviors and actions.\n\n5.4.3.3.2 SOTEA Scaling\nIt is also helpful to analyze networks visually to understand network structure. Figure 5-32\nshows SOTEA networks after 400 generations of evolution with varying population sizes\n(N=50, 100, 200) and KMax = 7. Figure 5-33 shows the same conditions but with KMax = 5.\n200\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nOne very noticeable consequence of the SOTEA model is that many nodes are found in\nfour neighborhood clusters and in particular, there appears to be a \"kite\" motif present in\nthe network.20 It is expected that this is in part due to the degree lower bound of KMin = 3 in\nthe SOTEA model.\nIn the network visualizations, node sizes are adjusted to reflect individual fitness with\nlarger nodes representing individuals with better fitness. It was disappointing to see that\nhigher fitness nodes did not clearly take network hub positions even though the network\nrewiring rules encourage high fitness nodes to acquire more connections and be less\nattracted to clusters. Also, one can notice that as population size increases, residual ringlike structures can still be observed in the network, even after 400 generations. This\nindicates that initial topological bias continues to impact the network structure over long\nperiods of time for larger systems. It is suspected that this structural bias can significantly\nimpact algorithm behavior which may be investigated in future work.\n\n20\n\nA motif refers to an over-represented sub-graph within a network. In other words, there exists a structural\npattern within the network that is repeated at a frequency that is unlikely to occur by chance alone.\n\n201\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-32 SOTEA Network Visualizations with KMax = 7 for population sizes N = 50, N = 100, and N = 200.\nNetwork visuals were created using Pajek Software.\n\n202\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nFigure 5-33 SOTEA Network Visualizations with KMax = 5 for population sizes N = 50, N = 100, and N = 200.\nNetwork visuals were created using Pajek Software.\n\nAnother interesting but potentially worrisome attribute of this SOTEA model can be\nobserved in the network with N=200 in Figure 5-33. Here one can see that, for the lower\nhalf of the network, the nodes are generally much larger indicating they are of relatively\n203\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nhigher fitness than the upper section. Under such conditions, the use of a global fitness\nranking to control KSet in the SOTEA algorithm could cause entire regions of the network to\nbecome highly connected while leaving the rest of the network with very little connectivity.\nThe possibility of this situation occurring could be mitigated by replacing the global fitness\nmeasure with one that is locally defined. This would not impact selection pressure since\nselection is based on local pair-wise comparisons and not based on the magnitude of fitness\nvalues. Furthermore, a localized fitness measure would remove the only global information\ncurrently used in SOTEA which would make SOTEA more efficient in physically\ndistributed implementations of the algorithm.\n\n5.4.4\n\nDiscussion\n\nSOTEA Network Model:\n\nThe model for network dynamics used in this SOTEA\n\nalgorithm was developed using several guiding principles. First, it was desired to have\ntopological changes be driven by, and enacted on, local regions of the network. This not\nonly occurs for many real-world complex systems, it also is a prerequisite for physically\nparallel implementations of the algorithm. This led to the use of network rewiring rules\nbased on short random walks as well as node property values which are almost completely\nderived from local information (except for fitness ranking).\nSecond, it was recognized (in both SOTEA models) that for many complex systems, selforganization is at least partly driven by component fitness or attractive forces between\nsystem components.\n\nClearly the concept of fitness also plays an important role in\n\noptimization. This made an individual's fitness a natural choice for coupling the structural\ndynamics of the network to the dynamics of the EA population.\nDistributed EA research: A large amount of research efforts have been devoted to the\n\nstudy of distributed Evolutionary Algorithms. These efforts include the study of finegrained (e.g. cellular grids), coarse-grained (e.g. island models), and hybrid structures (e.g.\nhierarchical). The highly modular topology of the second SOTEA model combined with\nshort random walk interactions within the system could create fuzzy or partial islands\nwithin the system where interactions within a cluster are much more likely to occur\ncompared to interactions between clusters. Quantifying the prevalence of this behavior\n\n204\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\ncould be accomplished by calculating the characteristic residence time of random walkers\non local regions of the network using methods outlined in Section 2.3 in [225]. Assuming\nthat clusters can become fairly isolated from other clusters, this would allow for a more\nnature-inspired approach to the integration of fine-grain and coarse-grain structures within\nan EA population (as opposed to explicitly defined hierarchical topologies).\n\n5.4.5\n\nFuture Work\n\nThere are other issues which have not been addressed here and will be left to future work.\nOne issue is that the network models are not directed so that information can flow in any\ndirection across the network. This is often not the case for many biological systems due to\nthermodynamic law and other irreversible processes.\nIt would also be interesting to investigate whether the structural bias in the initial\npopulation changes the algorithm's performance sensitivity to initial conditions. It is\nspeculated that the combination of genetic bias and structural bias (in the initial population)\ncould offer an extended range of flexibility to the algorithm. For example, combining these\ntwo features could force interactions between certain initial genotypes to take place with a\nfrequency that is much greater than would occur under other circumstances. In other\nwords, this provides some control over which regions in solution space are able to initially\ninteract.\n\nCombining this with a highly modular adaptive network like SOTEA could also\n\nallow for some control over the timing of future interactions between genetic material. In\nshort, optimizing structural bias in SOTEA could provide some limited capacity to control\nnot only which points in solution space are able to interact but also the timing in which\nthese interactions occur. This might also provide a viable path for mitigating the effects of\ndeleterious (e.g. deceptive) attractor basins within a particular fitness landscape.\nFinally, it should be mentioned that network models currently exist (e.g. see [190]) which\ncan acquire the structural characteristics of complex systems without the presence of\ndriving forcing that encourage these characteristics to emerge (e.g. the weighted clustering\ncoefficient in SOTEA). SOTEA would be significantly improved if it could allow for the\nemergence of important topological properties using a simplified model while still\nexhibiting robust performance on optimization problems.\n\n205\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\n5.4.6\n\nConclusions\n\nThe Self-Organizing Topology Evolutionary Algorithm or SOTEA is a distributed EA\ncontaining a population structure that coevolves with EA population dynamics. With the\npopulation defined on a network, rules are used to modify the network topology based on\nthe current states of the population.\nSOTEA Network Model:\n\nThe second SOTEA model presented in this chapter was\n\ndesigned with an emphasis on locality of network information and locality in network\ndynamics. Network dynamics were driven by i) an adaptive connectivity where higher\nfitness individuals were encouraged to obtain higher levels of connectivity and ii) an\nadaptive definition of community which attempted to encourage high levels of clustering in\nnodes of low fitness.\nTo this author's knowledge, this model was unique among network models in that it\nconsidered two driving forces instead of just one. Also, this SOTEA model was the first\nnetwork model which evolves due to a dynamic state value of the nodes (i.e. fitness). The\ndynamics of node fitness were a natural consequence of the dynamics of the EA population.\nTopological Analysis:\n\nThe second SOTEA model allows for a self-organization of\n\npopulation network topology resulting in a large degree of clustering, small characteristic\npath length, and correlations between the clustering coefficient and a node's degree. Each\nof these characteristics are similar to what is observed in complex biological systems.\nHowever, a number of topological properties observed in complex systems were not\nattained in the current model including properties achieved in the first SOTEA algorithm\nsuch as a fat-tailed degree distribution.\n\nFuture work will attempt to address these\n\nshortcomings as well as attempt to create a clearer framework for the integration of\nmultiple driving forces in an adaptive network. The network rewiring rules were also\ndeveloped in a somewhat ad hoc fashion and future work will look to develop a more\nintuitive framework for structural dynamics.\nPerformance: A number of engineering design problems and artificial test functions were\n\nselected to test the effectiveness of the new SOTEA algorithm against another distributed\ndesign, the cellular GA.\n\nResults indicate the SOTEA algorithm was able to provide\n206\n\n\fChapter 5: Self-Organizing Topology Evolutionary Algorithms\n\nimproved performance and more consistent results compared with the cGA. Both of the\ndistributed Evolutionary Algorithms strongly outperformed a suite of eight other\nEvolutionary Algorithms tested.\n\n207\n\n\fChapter 6: Summary of Findings\n\nChapter 6\n\nSummary of Findings\n\nThe primary goal of this thesis was to improve the performance and general robustness of\nEvolutionary Algorithms using principles inspired by nature. Contributions from this thesis\ninclude: i) reducing the practical difficulties associated with designing an Evolutionary\nAlgorithm by developing a more effective procedure for adapting EA design parameters, ii)\ndetermining the aspects of EA design which impact population dynamics and enable\nparallel search behavior, and iii) mimicking the structural self-organization in complex\nbiological systems as a means to obtain advanced behaviors and improved performance in\ndistributed Evolutionary Algorithms.\nDesigning an Effective Adaptive Process: Chapter 3 proposed mechanisms for making a\n\nmore effective adaptive process for supervisory control of EA design parameters. To make\nthe adaptive process more effective, two modifications were proposed.\n\nThe first\n\nmodification was to use an empirical measure of an individual's importance on future\npopulation dynamics instead of estimating its importance through the use of fitness\nmeasurements. The second modification was to reduce the influence of non-informative\ninteractions between the adaptive system and its environment. This was deemed to be\nparticularly relevant due to evidence presented in this chapter that non-informative\nmeasurements dominated the data received by the adaptive system.\n\nThis second\n\nmodification to the adaptive system was accomplished by using statistical arguments that\nquantified the importance of measurements.\nNot only did the new adaptive method outperform all other methods on the majority of\nproblems tested, it was also found to be much more robust compared to the other adaptive\nmethods. In particular, it's performance was not strongly sensitive to the class of problems\n(artificial test functions vs. engineering design problems) that it was tested on.\nThe impact of EA design on population dynamics: Chapter 4 started with the goal of\n\nunderstanding how EA design factors can influence EA population dynamics. It was\nconcluded that the probability distribution of an individual's impact on population\ndynamics fits a power law regardless of almost all experimental conditions. This result\n\n208\n\n\fChapter 6: Summary of Findings\n\nindicates that a small number of individuals are capable of driving EA population dynamics\nwhile most other individuals have only a small impact. The existence of power law\ndeviations in the probability of large impact sizes (i.e. large ETV) was seen as an indicator\nthat such systems were not capable of being driven by single individuals and instead were\nable to exhibit higher levels of parallel search behavior.\nThe most significant factor that enables parallel search behavior was the topology of the EA\npopulation.\n\nAs the population topology came closer to approximating a Panmictic\n\npopulation, the system became increasingly driven by only a select few individuals.\n\nOn\n\nthe other hand, as spatial restrictions were increased in the population, single individuals\nwere no longer capable of dominating the dynamics of the entire population.\n\nIt is\n\nspeculated that this could account for the strong and robust performance gains that have\nbeen repeatedly observed in distributed Evolutionary Algorithms over the years. Another\nfactor which was found to create a smaller degree of parallel search behavior was the\nintroduction of completely randomized new individuals into an EA population.\nThe Self-Organization of Interaction Networks for EA population topology: The aim\n\nof Chapter 5 was to create EA populations with topological features that are similar to those\nobserved in complex biological systems. It was speculated that mimicking this aspect of\nnature could provide additional improvements to algorithm behavior compared to those\nalready observed in distributed EA designs. This chapter has demonstrated that the selforganization of population topology can induce a number of interesting new behaviors in an\nEA and has the potential to significantly improve its performance on challenging\noptimization problems. It is hoped that this work will inspire others to investigate the use\nof network models for the self-organization of population structure and that these research\nefforts will help to narrow the gap between EA and natural evolutionary processes.\n\n209\n\n\fREFERENCES\n[1]\n\nR. Kolisch and S. Hartmann, \"Experimental Investigation of Heuristics for\nResource-Constrained Project Scheduling: An Update,\" European Journal of\nOperational Research, vol. 100, 2005.\n\n[2]\n\nS. G. G. Ahire, A. Gupta, and M. Terwilliger, \"Workforce constrained preventive\nmaintenance scheduling using evolution strategies,\" Decision Sciences, vol. 31, pp.\n833-859, 2000.\n\n[3]\n\nM. H. Bassett, L. L. Gardner, and K. Steele, \"Dow AgroSciences Uses SimulationBased Optimization to Schedule the New-Product Development Process,\"\nInterfaces, vol. 34, pp. 426-437, 2004.\n\n[4]\n\nP. P. Bonissone, R. Subbu, N. Eklund, and T. R. Kiehl, \"Evolutionary algorithms +\ndomain knowledge = real-world evolutionary computation,\" IEEE Transactions on\nEvolutionary Computation, vol. 10, pp. 256-280, 2006.\n\n[5]\n\nIcoSystem, http://icosystem.com/technology.htm. 2007.\n\n[6]\n\nBlue Kaizen, http://www.bluekaizen.com/. 2007.\n\n[7]\n\nEsteco, http://www.esteco.com/schedulers.jsp. 2007.\n\n[8]\n\nAdvanced\nComputational\ncomtech.co.uk/application.shtml. 2007.\n\n[9]\n\nBio-Comp, http://www.bio-comp.com/industrial/maximizeproduction.htm. 2007.\n\n[10]\n\nXpertRule, http://www.xpertrule.com/pages/case_ud.htm. 2007.\n\n[11]\n\nNuTech Solutions, http://nutechsolutions.com/index.asp. 2007.\n\n[12]\n\nG. P. Wagner and L. Altenberg, \"Complex adaptations and the evolution of\nevolvability,\" Evolution, vol. 50, pp. 967-976, 1996.\n\n[13]\n\nM. Bedau, \"Can unrealistic computer models illuminate theoretical biology,\" pp.\n20-23, 1999.\n\n[14]\n\nW. Banzhaf, \"Artificial Chemistries\u2013Towards constructive dynamical systems,\"\nNonlinear Phenomena in Complex Systems, vol. 5, pp. 318-324, 2002.\n\n[15]\n\nD. G. Green, D. Newth, and M. Kirley, \"Connectivity and catastrophe-towards a\ngeneral theory of evolution,\" Artificial Life VII: Proceedings of the Seventh\nInternational Conference, pp. 153-161, 2000.\n\n[16]\n\nG. P. Wagner, \"Homologues, Natural Kinds and the Evolution of Modularity,\"\nIntegrative and Comparative Biology, vol. 36, p. 36, 1996.\n\nTechnologies,\n\nhttp://www.ad-\n\n210\n\n\f[17]\n\nS. A. Kauffman, \"Requirements for evolvability in complex systems: orderly\ncomponents and frozen dynamics,\" Physica D, vol. 42, pp. 135\u2013152, 1990.\n\n[18]\n\nF. C. Santos and J. M. Pacheco, \"Scale-Free Networks Provide a Unifying\nFramework for the Emergence of Cooperation,\" Physical Review Letters, vol. 95, p.\n98104, 2005.\n\n[19]\n\nH. Sayama, L. Kaufman, and Y. Bar-Yam, \"Symmetry breaking and coarsening in\nspatially distributed evolutionary processes including sexual reproduction and\ndisruptive selection,\" Physical Review E, vol. 62, pp. 7065-7069, 2000.\n\n[20]\n\nA. Force, W. A. Cresko, F. B. Pickett, S. R. Proulx, C. Amemiya, and M. Lynch,\n\"The Origin of Subfunctions and Modular Gene Regulation,\" Genetics, vol. 170, pp.\n433-446, 2005.\n\n[21]\n\nJ. P. Crutchfield and O. G\u00f6rnerup, \"Objects that make objects: the population\ndynamics of structural complexity,\" Journal of The Royal Society Interface, vol. 3,\npp. 345-349, 2006.\n\n[22]\n\nW. Banzhaf, G. Beslon, S. Christensen, J. A. Foster, F. K\u00e9p\u00e8s, V. Lefort, J. F.\nMiller, M. Radman, and J. J. Ramsden, \"Guidelines: From artificial evolution to\ncomputational evolution: a research agenda,\" Nature Reviews Genetics, vol. 7, pp.\n729-735, 2006.\n\n[23]\n\nY. Jin and J. Branke, \"Evolutionary optimization in uncertain environments-a\nsurvey,\" IEEE Transactions on Evolutionary Computation, vol. 9, pp. 303-317,\n2005.\n\n[24]\n\nW. C. M. van Beers and J. P. C. Kleijnen, \"Kriging interpolation in simulation: a\nsurvey,\" in Proceedings of the 36th Conference on Winter Simulation, 2004, pp.\n113-121.\n\n[25]\n\nC. Coello, A. Carlos, D. A. Van Veldhuizen, and G. B. Lamont, Evolutionary\nalgorithms for solving multi-objective problems: New York: Kluwer Academic,\n2002.\n\n[26]\n\nK. Deb, Multi-Objective Optimization Using Evolutionary Algorithms: Wiley, 2001.\n\n[27]\n\nD. H. Wolpert and W. G. Macready, \"No free lunch theorems for optimization,\"\nIEEE Transactions on Evolutionary Computation, vol. 1, pp. 67-82, 1997.\n\n[28]\n\nJ. R. Woodward and J. R. Neil, \"No free lunch, program induction and\ncombinatorial problems,\" Proceeding of the 6th European Conference on Genetic\nprogramming, EuroGP, pp. 475\u2013484, 2003.\n\n[29]\n\nW. Hordijk, \"A Measure of Landscapes,\" Evolutionary Computation, vol. 4, pp.\n335-360, 1996.\n\n[30]\n\nS. Christensen and F. Oppacher, \"What can we learn from No Free Lunch? A First\nAttempt to Characterize the Concept of a Searchable Function,\" in Proceedings of\n211\n\n\fthe 3rd Annual Conference on Genetic and Evolutionary Computation, 2001, pp.\n1219\u20131226.\n[31]\n\nS. Wright, \"The roles of mutation, inbreeding, crossbreeding and selection in\nevolution,\" Proceedings of the Sixth International Congress on Genetics, vol. 1, pp.\n356\u2013366, 1932.\n\n[32]\n\nH. J. Bremermann, \"Optimization through evolution and recombination,\" SelfOrganizing Systems, pp. 93\u2013106, 1962.\n\n[33]\n\nR. M. Friedberg, \"A learning machine: Part I,\" IBM Journal of Research and\nDevelopment, vol. 2, pp. 2-13, 1958.\n\n[34]\n\nG. E. P. Box, \"Evolutionary Operation: A Method for Increasing Industrial\nProductivity,\" Applied Statistics, vol. 6, pp. 81-101, 1957.\n\n[35]\n\nJ. H. Holland, \"Outline for a Logical Theory of Adaptive Systems,\" Journal of the\nACM (JACM), vol. 9, pp. 297-314, 1962.\n\n[36]\n\nJ. H. Holland, \"Adaptation in Natural and Artificial System,\" Ann Arbor: The\nUniversity of Michigan Press, vol. 20, 1975.\n\n[37]\n\nL. J. Fogel, \"Autonomous automata,\" Industrial Research, vol. 4, pp. 14-19, 1962.\n\n[38]\n\nL. J. Fogel, A. J. Owens, and M. J. Walsh, Artificial Intelligence Through Simulated\nEvolution: Wiley, 1966.\n\n[39]\n\nI. Rechenberg, \"Cybernetic solution path of an experimental problem,\" Library\nTranslation, vol. 1122, 1964.\n\n[40]\n\nH. P. Schwefel, \"Evolutionsstrategie und numerische Optimierung,\" Technische\nUniversit\u00e4t Berlin, 1975.\n\n[41]\n\nT. Back, D. B. Fogel, and Z. Michalewicz, Handbook of Evolutionary Computation:\nIOP Publishing Ltd. Bristol, UK, 1997.\n\n[42]\n\nH. P. Schwefel, Numerical Optimization of Computer Models: John Wiley & Sons,\nInc. New York, NY, USA, 1981.\n\n[43]\n\nJ. E. Baker, \"Adaptive Selection Methods for Genetic Algorithms,\" Proceedings of\nthe 1st International Conference on Genetic Algorithms, pp. 101-111, 1985.\n\n[44]\n\nJ. E. Baker, \"Reducing bias and inefficiency in the selection algorithm,\"\nProceedings of the Second International Conference on Genetic Algorithms on\nGenetic algorithms and their application, pp. 14-21, 1987.\n\n[45]\n\nT. Blickle, \"Theory of Evolutionary Algorithms and Application to System\nSynthesis,\" Swiss Federal Institute of Technology, 1996.\n\n212\n\n\f[46]\n\nH. M\u00fchlenbein and D. Schlierkamp-Voosen, \"Predictive models for the breeder\ngenetic algorithm, I.: continuous parameter optimization,\" Evolutionary\nComputation, vol. 1, pp. 25-49, 1993.\n\n[47]\n\nD. E. Goldberg and K. Deb, \"A Comparative Analysis of Selection Schemes Used\nin Genetic Algorithms,\" Urbana, vol. 51, pp. 61801-2996.\n\n[48]\n\nT. Blickle and L. Thiele, \"A Comparison of Selection Schemes used in Evolutionary\nAlgorithms,\" Evolutionary Computation, vol. 4, pp. 361-394, 1996.\n\n[49]\n\nW. Wieczorek and Z. J. Czech, \"Selection Schemes in Evolutionary Algorithms,\"\nProceedings of the Symposium on Intelligent Information Systems (IIS'2002 ), pp.\n185-194, 2002.\n\n[50]\n\nR. Storn and K. Price, \"Differential Evolution-A Simple and Efficient Adaptive\nScheme for Global Optimization over Continuous Spaces,\" International Computer\nScience Institute, Berkeley 1995.\n\n[51]\n\nN. Hansen and A. Ostermeier, \"Completely Derandomized Self-Adaptation in\nEvolution Strategies,\" Evolutionary Computation, vol. 9, pp. 159-195, 2001.\n\n[52]\n\nH. M\u00fchlenbein and G. Paass, \"From recombination of genes to the estimation of\ndistributions I. Binary parameters,\" Lecture Notes in Computer Science, vol. 1141,\npp. 178\u2013187, 1996.\n\n[53]\n\nA. H. Wright, \"Genetic Algorithms for Real Parameter Optimization,\" in First\nWorkshop on the Foundations of Genetic Algorithms and Classifier Systems, 1990,\npp. 205-218.\n\n[54]\n\nQ. T. Pham, \"Dynamic optimization of chemical engineering processes by an\nevolutionary method,\" Computers and Chemical Engineering, vol. 22, pp. 10891097, 1998.\n\n[55]\n\nQ. T. Pham, \"Evolutionary optimization of dynamic control problems accelerated\nby progressive step reduction,\" in Proceedings of the 7th Annual Conference on\nGenetic and Evolutionary Computation, 2005, pp. 2181-2187.\n\n[56]\n\nF. Herrera, M. Lozano, and A. M. S\u00e1nchez, \"Hybrid crossover operators for realcoded genetic algorithms: an experimental study,\" Soft Computing-A Fusion of\nFoundations, Methodologies and Applications, vol. 9, pp. 280-298, 2005.\n\n[57]\n\nN. Krasnogor and J. Smith, \"A tutorial for competent memetic algorithms: model,\ntaxonomy, and design issues,\" IEEE Transactions on Evolutionary Computation,\nvol. 9, pp. 474-488, 2005.\n\n[58]\n\nJ. C. Bean and A. B. Hadj-Alouane, \"A dual genetic algorithm for bounded integer\nprograms,\" Department of Industrial and Operations Engineering, The University of\nMichigan Tr-92-53, 1992.\n\n213\n\n\f[59]\n\nZ. Michalewicz and N. Attia, \"Evolutionary optimization of constrained problems,\"\nin Proceedings of the 3rd Annual Conference on EP, Singapore, 1994, pp. 98-108.\n\n[60]\n\nT. Ray, T. Kang, and S. K. Chye, \"An Evolutionary Algorithm for Constrained\nOptimization,\" Proceedings of the Genetic and Evolutionary Computation\nConference (GECCO'2000), pp. 771\u2013777, 2000.\n\n[61]\n\nK. Deb, S. Agrawal, A. Pratap, and T. Meyarivan, \"A Fast Elitist Non-Dominated\nSorting Genetic Algorithm for Multi-Objective Optimization: NSGA-II,\"\nProceedings of the Parallel Problem Solving from Nature VI Conference, pp. 849858, 2000.\n\n[62]\n\nC. A. C. Coello, \"A survey of constraint handling techniques used with evolutionary\nalgorithms,\" Lania-RI-99-04, Laboratorio Nacional de Inform\u00e1tica Avanzada,\n1999.\n\n[63]\n\nC. Coello, \"Theoretical and numerical constraint-handling techniques used with\nevolutionary algorithms: a survey of the state of the art,\" Computer Methods in\nApplied Mechanics and Engineering, vol. 191, pp. 1245-1287, 2002.\n\n[64]\n\nT. P. Runarsson and X. Yao, \"Stochastic ranking for constrained evolutionary\noptimization,\" IEEE Transactions on Evolutionary Computation, vol. 4, pp. 284294, 2000.\n\n[65]\n\nR. Sarker, T. Runarsson, and C. Newton, \"Genetic Algorithms for Solving a Class\nof Constrained Nonlinear Integer Programs,\" International Transactions in\nOperational Research, vol. 8, pp. 61-74, 2001.\n\n[66]\n\nF. Herrera, M. Lozano, and J. L. Verdegay, \"Tackling Real-Coded Genetic\nAlgorithms: Operators and Tools for Behavioural Analysis,\" Artificial Intelligence\nReview, vol. 12, pp. 265-319, 1998.\n\n[67]\n\nL. Altenberg, \"The Evolution of Evolvability in Genetic Programming,\" Advances\nin Genetic Programming, pp. 47-74, 1994.\n\n[68]\n\nR. E. Keller and W. Banzhaf, \"Genetic Programming using Genotype-Phenotype\nMapping from Linear Genomes into Linear Phenotypes,\" in Proceedings of the\nFirst Annual Conference on Genetic Programming 1996, pp. 116-122.\n\n[69]\n\nM. Mandischer, \"Representation and Evolution of Neural Networks,\" in Artificial\nNeural Nets and Genetic Algorithms Proceedings of the International Conference at\nInnsbruck, Austria, A. R. F., R. C. R., and S. N. C., Eds.: Springer, 1993, pp. 643649.\n\n[70]\n\nK. O. Stanley and R. Miikkulainen, \"Competitive coevolution through evolutionary\ncomplexification,\" Journal of Artificial Intelligence Research, vol. 21, pp. 63-100,\n2004.\n\n[71]\n\nX. Yao, \"Evolving artificial neural networks,\" Proceedings of the IEEE, vol. 87, pp.\n1423-1447, 1999.\n214\n\n\f[72]\n\nA. S. Wu and I. Garibay, \"The Proportional Genetic Algorithm: Gene Expression in\na Genetic Algorithm,\" Genetic Programming and Evolvable Machines, vol. 3, pp.\n157-192, 2002.\n\n[73]\n\nI. Garibay, A. S. Wu, and O. Garibay, \"Emergence of genomic self-similarity in\nlocation independent representations,\" Genetic Programming and Evolvable\nMachines, vol. 7, pp. 55-80, 2006.\n\n[74]\n\nF. Li, T. Long, Y. Lu, Q. Ouyang, and C. Tang, \"The yeast cell-cycle network is\nrobustly designed,\" Proceedings of the National Academy of Sciences, vol. 101, pp.\n4781-4786, 2004.\n\n[75]\n\nJ. G\u00f3mez-Gardenes, Y. Moreno, and L. M. Flori\u00e1, \"On the robustness of complex\nheterogeneous gene expression networks,\" Biophysical Chemistry, vol. 115, pp.\n225-228, 2005.\n\n[76]\n\nM. Kimura, The Neutral Theory of Molecular Evolution: Cambridge University\nPress, 1983.\n\n[77]\n\nM. A. Huynen, P. F. Stadler, and W. Fontana, \"Smoothness within ruggedness: The\nrole of neutrality in adaptation,\" in Proceedings of the National Academy of\nSciences, 1996.\n\n[78]\n\nP. Schuster, W. Fontana, P. F. Stadler, and I. L. Hofacker, \"From Sequences to\nShapes and Back: A Case Study in RNA Secondary Structures,\" Proceedings:\nBiological Sciences, vol. 255, pp. 279-284, 1994.\n\n[79]\n\nM. E. J. Newman and R. Engelhardt, \"Effects of neutral selection on the evolution\nof molecular species,\" Proceedings of the Royal Society of London Series B, vol.\n256, pp. 1333-1338, 1998.\n\n[80]\n\nE. van Nimwegen and J. P. Crutchfield, \"Metastable evolutionary dynamics:\nCrossing fitness barriers or escaping via neutral paths?,\" Bulletin of Mathematical\nBiology, vol. 62, pp. 799-848, 2000.\n\n[81]\n\nS. A. Kauffman, The Origins of Order: Self-Organization and Selection in\nEvolution: Oxford University Press, 1993.\n\n[82]\n\nY. D. Nochomovitz and H. Li, \"Highly designable phenotypes and mutational\nbuffers emerge from a systematic mapping between network topology and dynamic\noutput,\" Proceedings of the National Academy of Sciences, vol. 103, pp. 4180-4185,\n2006.\n\n[83]\n\nK. A. De Jong, \"An Analysis of the Behavior of a Class of Genetic Adaptive\nSystems,\" University of Michigan, 1975.\n\n[84]\n\nS. W. Mahfoud, \"A Comparison of Parallel and Sequential Niching Methods,\"\nConference on Genetic Algorithms, vol. 136, p. 143, 1995.\n\n215\n\n\f[85]\n\nG. R. Harik, \"Finding multimodal solutions using restricted tournament selection,\"\nProceedings of the Sixth International Conference on Genetic Algorithms, pp. 2431, 1995.\n\n[86]\n\nO. J. Mengshoel and D. E. Goldberg, \"Probabilistic crowding: Deterministic\ncrowding with probabilistic replacement,\" Proceedings of the Genetic and\nEvolutionary Computation Conference, pp. 409\u2013416, 1999.\n\n[87]\n\nD. E. Goldberg and J. Richardson, \"Genetic algorithms with sharing for multimodal\nfunction optimization,\" Proceedings of the Second International Conference on\nGenetic Algorithms on Genetic algorithms and their application, pp. 41-49, 1987.\n\n[88]\n\nA. Petrowski, \"A clearing procedure as a niching method for genetic algorithms,\"\nProceedings of the IEEE International Conference on Evolutionary Computation,\npp. 798-803, 1996.\n\n[89]\n\nE. Cantu-Paz, \"A survey of parallel genetic algorithms,\" Calculateurs Paralleles,\nvol. 10, pp. 141-171, 1998.\n\n[90]\n\nE. Alba and M. Tomassini, \"Parallelism and evolutionary algorithms,\" IEEE\nTransactions on Evolutionary Computation, vol. 6, pp. 443-462, 2002.\n\n[91]\n\nJ. J. Grefenstette, \"Parallel adaptive algorithms for function optimization,\"\nVanderbilt University, Nashville, TN, Tech. Rep. CS-81-19 1981.\n\n[92]\n\nM. Gorges-Schleuter, \"ASPARAGOS an asynchronous parallel genetic\noptimization strategy,\" Proceedings of the Third International Conference on\nGenetic Algorithms, pp. 422-427, 1989.\n\n[93]\n\nB. Manderick and P. Spiessens, \"Fine-grained parallel genetic algorithms,\"\nProceedings of the Third International Conference on Genetic Algorithms, pp. 428433, 1989.\n\n[94]\n\nJ. Sarma and K. A. De Jong, \"An analysis of the effects of neighborhood size and\nshape on local selection algorithms,\" Parallel Problem Solving from Nature (PPSN\nIV), vol. 1141, pp. 236\u2013244, 1996.\n\n[95]\n\nB. Dorronsoro, E. Alba, M. Giacobini, and M. Tomassini, \"The influence of grid\nshape and asynchronicity on cellular evolutionary algorithms,\" CEC2004 Congress\non Evolutionary Computation, vol. 2, 2004.\n\n[96]\n\nM. Giacobini, M. Tomassini, A. G. B. Tettamanzi, and E. Alba, \"Selection intensity\nin cellular evolutionary algorithms for regular lattices,\" IEEE Transactions on\nEvolutionary Computation, vol. 9, pp. 489-505, 2005.\n\n[97]\n\nF. Greil and B. Drossel, \"Dynamics of critical Kauffman networks under\nasynchronous stochastic update,\" Physical Review Letters, vol. 95, p. 048701, 2005.\n\n[98]\n\nD. Cornforth, D. G. Green, and D. Newth, \"Ordered asynchronous processes in\nmulti-agent systems,\" Physica D, vol. 204, pp. 70-82, 2005.\n216\n\n\f[99]\n\nG. S. Hornby, \"ALPS: the age-layered population structure for reducing the\nproblem of premature convergence,\" in Proceedings of the 8th Annual Conference\non Genetic and Evolutionary Computation, 2006, pp. 815-822.\n\n[100] E. Alba, F. Luna, A. J. Nebro, and J. M. Troya, \"Parallel heterogeneous genetic\nalgorithms for continuous optimization \" Parallel Computing, vol. 30, pp. 699-719,\n2004.\n[101] E. D. Dolan and J. J. Mor\u00e9, \"Benchmarking optimization software with performance\nprofiles,\" Mathematical Programming, vol. 91, pp. 201-213, 2002.\n[102] H. G. Beyer, H. P. Schwefel, and I. Wegener, \"How to analyse evolutionary\nalgorithms,\" Theoretical Computer Science, vol. 287, pp. 101-130, 2002.\n[103] J. M. Whitacre, T. Q. Pham, and R. A. Sarker, \"Credit assignment in adaptive\nevolutionary algorithms,\" in Proceedings of the 8th Annual Conference on Genetic\nand Evolutionary Computation, 2006, pp. 1353-1360.\n[104] J. M. Whitacre, T. Q. Pham, and R. A. Sarker, \"Use of statistical outlier detection\nmethod in adaptive evolutionary algorithms,\" in Proceedings of the 8th Annual\nConference on Genetic and Evolutionary Computation, 2006, pp. 1345-1352.\n[105] L. Bianchi, M. Dorigo, L. M. Gambardella, and W. J. Gutjahr, \"Metaheuristics in\nStochastic Combinatorial Optimization: a Survey,\" TechReport: Dalle Molle\nInstitute for Artificial Intelligence, 2006.\n[106] C. M. Fonseca and P. J. Fleming, \"An Overview of Evolutionary Algorithms in\nMultiobjective Optimization,\" Evolutionary Computation, vol. 3, pp. 1-16, 1995.\n[107] Q. T. Pham, \"Effect of Numerical Errors on the Performance of Optimization\nMethods,\" in Proceedings of Chemeca Brisbane, Australia, 2005.\n[108] T. Back, U. Hammel, and H. P. Schwefel, \"Evolutionary computation: comments on\nthe history and current state,\" IEEE Transactions on Evolutionary Computation,\nvol. 1, pp. 3-17, 1997.\n[109] D. E. Clark and D. R. Westhead, \"Evolutionary algorithms in computer-aided\nmolecular design,\" Journal of Computer-Aided Molecular Design, vol. 10, pp. 337358, 1996.\n[110] R. Cheng, M. Gen, and Y. Tsujimura, \"A tutorial survey of job-shop scheduling\nproblems using genetic algorithms--I. Representation,\" Computers and Industrial\nEngineering, vol. 30, pp. 983-997, 1996.\n[111] Q. Ahmed, K. Krishnakumar, and J. Neidhoefer, \"Applications of evolutionary\nalgorithms to aerospace problems: A survey,\" in ECCOMAS conference on\nnumerical methods in engineering Paris, France, 1996, pp. 236-242.\n[112] A. A. Freitas, \"A survey of evolutionary algorithms for data mining and knowledge\ndiscovery,\" Advances in Evolutionary Computation, pp. 819-845, 2002.\n217\n\n\f[113] P. J. Fleming and R. C. Purshouse, \"Evolutionary algorithms in control systems\nengineering: a survey,\" Control Engineering Practice, vol. 10, pp. 1223-1241, 2002.\n[114] D. Zeidler, S. Frey, K. L. Kompa, and M. Motzkus, \"Evolutionary algorithms and\ntheir application to optimal control studies,\" Physical Review A, vol. 64, p. 23420,\n2001.\n[115] R. L. Johnston and H. M. Cartwright, Applications of Evolutionary Computation in\nChemistry: Springer, 2004.\n[116] J. Arifovic, \"Evolutionary Algorithms in Macroeconomic Models,\" Macroeconomic\nDynamics, vol. 4, pp. 373-414, 2000.\n[117] S. A. Kalogirou, \"Artificial intelligence for the modeling and control of combustion\nprocesses: a review,\" Progress in Energy and Combustion Science, vol. 29, pp. 515566, 2003.\n[118] H. A. Abbass, \"An evolutionary artificial neural networks approach for breast\ncancer diagnosis,\" Artificial Intelligence in Medicine, vol. 25, pp. 265-281, 2002.\n[119] K. Wloch and P. J. Bentley, \"Optimising the performance of a formula one car\nusing a genetic algorithm,\" Proceedings of Eighth International Conference on\nParallel Problem Solving From Nature, pp. 702\u2013711, 2004.\n[120] S. Kamphausen, N. H\u00f6ltge, F. Wirsching, C. Morys-Wortmann, D. Riester, R.\nGoetz, M. Th\u00fcrk, and A. Schwienhorst, \"Genetic algorithm for the design of\nmolecules with desired properties,\" Journal of Computer-Aided Molecular Design,\nvol. 16, pp. 551-567, 2002.\n[121] Y. Fan, T. Jiang, and D. J. Evans, \"Volumetric Segmentation of Brain Images Using\nParallel Genetic Algorithms,\" IEEE Transactions on Medical Imaging, vol. 21,\n2002.\n[122] Q. Hou, J. Wang, Y. Chen, and J. M. Galvin, \"Beam orientation optimization for\nIMRT by a hybrid method of the genetic algorithm and the simulated dynamics,\"\nMedical Physics, vol. 30, p. 2360, 2003.\n[123] A. Globus, M. Menon, and D. Srivastava, \"Enabling Computational\nNanotechnology through JavaGenes in a Cycle Scavenging Environment,\"\nSupercomputing, 2002.\n[124] J. M. Malard, A. Heredia-Langner, D. J. Baxter, K. H. Jarman, and W. R. Cannon,\n\"Constrained de novo peptide identification via multi-objective optimization,\"\nProceedings of the 18th International Parallel and Distributed Processing\nSymposium, 2004.\n[125] A. S. McLeod, M. E. Johnston, and L. F. Gladden, \"Development of a Genetic\nAlgorithm for Molecular Scale Catalyst Design,\" Journal of Catalysis, vol. 167, pp.\n279-285, 1997.\n\n218\n\n\f[126] \"Applications of Evolutionary Computation book series,\" Springer.\n[127] J. Grefenstette, \"Optimization of control parameters for genetic algorithms,\" IEEE\nTransactions on Systems, Man and Cybernetics, vol. 16, pp. 122-128, 1986.\n[128] P. M. Reed, B. S. Minsker, and D. E. Goldberg, \"The Practitioner's Role in\nCompetent Search and Optimization Using Genetic Algorithms,\" in World Water\nand Environmental Resources Congress, Washington, DC, 2001, pp. 0-7844.\n[129] G. R. Harik and F. G. Lobo, \"A parameter-less genetic algorithm,\" Proceedings of\nthe Genetic and Evolutionary Computation Conference, vol. 1, pp. 258\u2013265, 1999.\n[130] V. A. Cicirello, \"Boosting Stochastic Problem Solvers Through Online SelfAnalysis of Performance,\" Carnegie Mellon University, 2003.\n[131] H. Guo, \"Algorithm Selection for Sorting and Probabilistic Inference: a Machine\nLearning-Based Approach,\" Kansas State University, 2003.\n[132] C. P. Gomes and B. Selman, \"Algorithm portfolio design: Theory vs. practice,\"\nProceedings of UAI-97, pp. 190\u2013197, 1997.\n[133] L. S. Crawford, M. P. J. Fromherz, C. Guettier, and Y. Shang, \"A Framework for\nOn-line Adaptive Control of Problem Solving,\" Proceedings CP'01 Workshop on\nOn-line Combinatorial Problem Solving and Constraint Programming, 2001.\n[134] F. G. Lobo and C. F. Lima, \"Revisiting evolutionary algorithms with on-the-fly\npopulation size adjustment,\" in Proceedings of the 8th Annual Conference on\nGenetic and Evolutionary Computation, 2006, pp. 1241-1248.\n[135] E. Alba and B. Dorronsoro, \"The Exploration/Exploitation Tradeoff in Dynamic\nCellular Genetic Algorithms,\" IEEE Transactions on Evolutionary Computation,\nvol. 9, pp. 126-142, 2005.\n[136] J. M. Whitacre, R. A. Sarker, and Q. T. Pham, \"The Self-Organization of Interaction\nNetworks for Nature-Inspired Optimization,\" IEEE Transactions on Evolutionary\nComputation, (accepted March, 2007).\n[137] B. Li and W. Jiang, \"A novel stochastic optimization algorithm,\" IEEE\nTransactions on Systems, Man and Cybernetics, vol. 30, pp. 193-198, 2000.\n[138] C.-K. Ting, S.-T. Li, and C. Lee, \"On the harmonious mating strategy through tabu\nsearch,\" Information Science, vol. 156, pp. 189-214, 2003.\n[139] D. H. Cho, J. K. Kim, H. K. Jung, and C. G. Lee, \"Optimal design of permanentmagnet motor using autotuning niching genetic algorithm,\" IEEE Transactions on\nMagnetics, vol. 39, pp. 1265-1268, 2003.\n[140] G. R. Harik and D. E. Goldberg, \"Learning linkage through probabilistic\nexpression,\" Computer Methods in Applied Mechanics and Engineering, vol. 186,\npp. 295\u2013310, 2000.\n219\n\n\f[141] S. Yang, \"Adaptive Crossover in Genetic Algorithms Using Statistics Mechanism,\"\nin Proceedings of the Eighth International Conference on Artificial Life, 2002, pp.\n182-185.\n[142] J. Smith, \"On Appropriate Adaptation Levels for the Learning of Gene Linkage,\"\nGenetic Programming and Evolvable Machines, vol. 3, pp. 129-155, 2002.\n[143] S. Y. Ho, L. S. Shu, and J. H. Chen, \"Intelligent evolutionary algorithms for large\nparameter optimization problems,\" IEEE Transactions on Evolutionary\nComputation, vol. 8, pp. 522-541, 2004.\n[144] F. Herrera and M. Lozano, \"Two-Loop Real-Coded Genetic Algorithms with\nAdaptive Control of Mutation Step Sizes,\" Applied Intelligence, vol. 13, pp. 187204, 2000.\n[145] A. K. Swain and A. S. Morris, \"Performance improvement of self-adaptive\nevolutionary methods with a dynamic lower bound,\" Information Processing\nLetters, vol. 82, pp. 55-63, 2002.\n[146] L. Davis, Handbook of Genetic Algorithms: Van Nostrand Reinhold New York,\n1991.\n[147] Q. T. Pham, \"Competitive evolution: a natural approach to operator selection,\"\nProgress in Evolutionary Computation, Lecture Notes in Artificial Intelligence, vol.\n956, pp. 49-60, 1994.\n[148] M. A. Bedau and N. H. Packard, \"Evolution of evolvability via adaptation of\nmutation rates,\" Biosystems, vol. 69, pp. 143-162, 2003.\n[149] C. Igel, F. Friedrichs, and S. Wiegand, \"Evolutionary Optimization of Neural\nSystems: The Use of Strategy Adaptation. Trends and Applications in Constructive\nApproximation,\" International Series of Numerical Mathematics, vol. 151, pp. 103123, 2005.\n[150] F. P. Espinoza, B. S. Minsker, and D. E. Goldberg, \"Adaptive Hybrid Genetic\nAlgorithm for Groundwater Remediation Design,\" Journal of Water Resources\nPlanning and Management, vol. 131, pp. 14-24, 2005.\n[151] J. E. Smith and T. C. Fogarty, \"Operator and parameter adaptation in genetic\nalgorithms,\" Soft Computing-A Fusion of Foundations, Methodologies and\nApplications, vol. 1, pp. 81-87, 1997.\n[152] A. E. Eiben, R. Hinterding, and Z. Michalewicz, \"Parameter control in evolutionary\nalgorithms,\" IEEE Transactions on Evolutionary Computation, vol. 3, pp. 124-141,\n1999.\n[153] F. Herrera and M. Lozano, \"Fuzzy adaptive genetic algorithms: design, taxonomy,\nand future directions,\" Soft Computing-A Fusion of Foundations, Methodologies\nand Applications, vol. 7, pp. 545-562, 2003.\n\n220\n\n\f[154] F. G. Lobo, C. F. Lima, and Z. Michalewicz, Parameter Setting in Evolutionary\nAlgorithms: Springer, 2007.\n[155] S. Kirkpatrick, C. D. Gelatt Jr, and M. P. Vecchi, \"Optimization by Simulated\nAnnealing,\" Science, vol. 220, p. 671, 1983.\n[156] F. Herrera, M. Lozano, and C. Moraga, \"Hierarchical distributed genetic\nalgorithms,\" International Journal of Intelligent Systems, vol. 14, pp. 1099-1121,\n1999.\n[157] C. Darwin, \"The variation of plants and animals under domestication,\" J. Murray,\nLondon, 1868.\n[158] D. Thierens, \"An adaptive pursuit strategy for allocating operator probabilities,\" in\nProceedings of the 7th Annual Conference on Genetic and Evolutionary\nComputation, 2005, pp. 1539-1546.\n[159] A. Tuson and P. Ross, \"Adapting Operator Settings in Genetic Algorithms,\"\nEvolutionary Computation, vol. 6, pp. 161-184, 1998.\n[160] C. Igel and M. Kreutz, \"Operator Adaptation in Evolutionary Computation and Its\nApplication to Structure Optimization of Neural Networks,\" Neurocomputing vol.\n55, pp. 347-361, 2003.\n[161] M. Srinivas and L. M. Patnaik, \"Adaptive probabilities of crossover and mutation in\ngenetic algorithms,\" IEEE Transactions on Systems, Man and Cybernetics, vol. 24,\npp. 656-667, 1994.\n[162] F. G. Lobo and D. E. Goldberg, \"Decision making in a hybrid genetic algorithm,\"\nIEEE International Conference on Evolutionary Computation, pp. 121-125, 1997.\n[163] H. J. C. Barbosa and A. M. e S\u00e1, \"On Adaptive Operator Probabilities in Real\nCoded Genetic Algorithms,\" in Workshop on Advances and Trends in Artificial\nIntelligence for Problem Solving (SCCC'00) Santiago, Chile, 2000.\n[164] B. A. Julstrom, \"What Have You Done for Me Lately? Adapting Operator\nProbabilities in a Steady-State Genetic Algorithm,\" Proceedings of the 6th\nInternational Conference on Genetic Algorithms, pp. 81-87, 1995.\n[165] B. A. Julstrom, \"Adaptive operator probabilities in a genetic algorithm that applies\nthree operators,\" in ACM Symposium on Applied Computing (SAC '97) 1997, pp.\n233-238.\n[166] C. Salzberg, A. Antony, and H. Sayama, \"Visualizing Evolutionary Dynamics of\nSelf-Replicators: A Graph-Based Approach,\" Artificial Life, vol. 12, pp. 275-287,\n2006.\n[167] D. C. Montgomery, G. C. Runger, and N. F. Hubele, Engineering Statistics: Wiley\nNew York, 1998.\n\n221\n\n\f[168] H. Pohlheim, \"GEATbx - Genetic and Evolutionary Algorithm Toolbox for use with\nMatlab. http://www.geatbx.com/, ,\" 1994-2007.\n[169] J. J. Sepkoski, \"A compendium of fossil marine animal families,\" Contributions In\nBiology And Geology, vol. 83, pp. 1-156, 1992.\n[170] D. M. Raup, \"A Kill Curve For Phanerozoic Marine Species,\" Paleobiology, vol.\n17, pp. 37-48, 1991.\n[171] M. E. J. Newman, \"Self-Organized Criticality, Evolution and the Fossil Extinction\nRecord,\" Proceedings: Biological Sciences, vol. 263, pp. 1605-1610, 1996.\n[172] Yule, \"A Mathematical Theory of Evolution, Based on the Conclusions of Dr. J. C.\nWillis, F.R.S,\" Royal Society of London Philosophical Transactions Series B, vol.\n213, pp. 21-87, 1925.\n[173] T. H. Keitt and H. E. Stanley, \"Dynamics of North American breeding bird\npopulations,\" Nature, vol. 393, pp. 257-260, 1998.\n[174] R. V. Sol\u00e9 and J. Bascompte, \"Are Critical Phenomena Relevant to Large-Scale\nEvolution?,\" Proceedings: Biological Sciences, vol. 263, pp. 161-168, 1996.\n[175] N. C. Stenseth and J. M. Smith, \"Coevolution in Ecosystems: Red Queen Evolution\nor Stasis?,\" Evolution, vol. 38, pp. 870-880, 1984.\n[176] M. E. J. Newman and P. Sibani, \"Extinction, diversity and survivorship of taxa in\nthe fossil record,\" Proceedings of the Royal Society of London Series B, vol. 266,\npp. 1593-1593, 1999.\n[177] B. Drossel, \"Biological evolution and statistical physics,\" Advances in Physics, vol.\n50, pp. 209-295, 2001.\n[178] J. C. Willis, \"Age and Area,\" The Quarterly Review of Biology, vol. 1, pp. 553-571,\n1926.\n[179] B. Burlando, \"The fractal dimension of taxonomic systems,\" Journal of Theoretical\nBiology, vol. 146, pp. 99-114, 1990.\n[180] J. L. Payne and M. J. Eppstein, \"Emergent mating topologies in spatially structured\ngenetic algorithms,\" in Proceedings of the 8th Annual Conference on Genetic and\nEvolutionary Computation, 2006, pp. 207-214.\n[181] S. Miyazima, Y. Lee, T. Nagamine, and H. Miyajima, \"Power-law distribution of\nfamily names in Japanese societies,\" Physica A: Statistical Mechanics and its\nApplications, vol. 278, pp. 282-288, 2000.\n[182] S. C. Manrubia and D. H. Zanette, \"At the boundary between biological and cultural\nevolution: the origin of surname distributions,\" Journal of Theoretical Biology, vol.\n216, pp. 461-77, 2002.\n\n222\n\n\f[183] P. Bak, C. Tang, and K. Wiesenfeld, \"Self-organized criticality: An explanation of\nthe 1/f noise,\" Physical Review Letters, vol. 59, pp. 381-384, 1987.\n[184] J. S. S. Martins and P. M. C. Oliveira, \"Computer simulations of statistical models\nand dynamic complex systems,\" Brazilian Journal of Physics, vol. 34, pp. 10771101, 2004.\n[185] M. Paczuski, \"Networks as Renormalized Models for Emergent Behavior in\nPhysical Systems,\" Arxiv preprint physics/0502028, 2005.\n[186] A. Saichev and D. Sornette, \"Vere-Jones' self-similar branching model,\" Physical\nReview E, vol. 72, p. 56122, 2005.\n[187] C. Adami and J. Chu, \"Critical and near-critical branching processes,\" Physical\nReview E, vol. 66, p. 11907, 2002.\n[188] D. V. Foster, S. A. Kauffman, and J. E. S. Socolar, \"Network growth models and\ngenetic regulatory networks,\" Physical Review E, vol. 73, p. 31912, 2006.\n[189] R. V. Sol\u00e9, R. Pastor-Satorras, E. Smith, and T. B. Kepler, \"A Model of Large-Scale\nProteome Evolution,\" Advances in Complex Systems, vol. 5, pp. 43-54, 2002.\n[190] A. Vazquez, \"Growing network with local rules: Preferential attachment, clustering\nhierarchy, and degree correlations,\" Physical Review E, vol. 67, p. 56104, 2003.\n[191] H. Sayama, M. A. M. Aguiar, Y. Bar-Yam, and M. Baranger, \"Spontaneous pattern\nformation and genetic invasion in locally mating and competing populations,\"\nPhysical Review E, vol. 65, p. 51919, 2002.\n[192] M. C. Boerlijst and P. Hogeweg, \"Spiral wave structure in pre-biotic evolution:\nhypercycles stable against parasites,\" Physica D, vol. 48, pp. 17-28, 1991.\n[193] M. J. Eppstein, J. L. Payne, and C. Goodnight, \"Sympatric speciation by selforganizing barriers to gene flow in simulated populations with localized mating,\" in\nProceedings of the 8th Annual Conference on Genetic and Evolutionary\nComputation, Seattle, 2006.\n[194] E. Ravasz, A. L. Somera, D. A. Mongru, Z. N. Oltvai, and A. L. Barab\u00e1si,\n\"Hierarchical Organization of Modularity in Metabolic Networks,\" Science, vol.\n297, pp. 1551\u20131555, 2002.\n[195] D. J. Watts and S. H. Strogatz, \"Collective dynamics of 'small-world' networks,\"\nNature, vol. 393, pp. 409-10, 1998.\n[196] S. Boccaletti, V. Latora, Y. Moreno, M. Chavez, and D. U. Hwang, \"Complex\nnetworks: Structure and dynamics,\" Physics Reports, vol. 424, pp. 175-308, 2006.\n[197] R. Albert and A. L. Barab\u00e1si, \"Statistical mechanics of complex networks,\" Reviews\nof Modern Physics, vol. 74, pp. 47-97, 2002.\n\n223\n\n\f[198] M. E. J. Newman, \"The structure and function of complex networks,\" SIAM Review,\nvol. 45, pp. 167-256, 2003.\n[199] A. L. Barab\u00e1si and Z. N. Oltvai, \"Network biology: understanding the cell's\nfunctional organization,\" Nature Reviews Genetics, vol. 5, pp. 101-113, 2004.\n[200] P. Erd\u00f6s and A. R\u00e9nyi, \"On random graphs,\" Publ. Math. Debrecen, vol. 6, pp. 290297, 1959.\n[201] P. Erd\u00f6s and A. R\u00e9nyi, \"On the evolution of random graphs,\" Bulletin of the\nInstitute of International Statistics, vol. 38, pp. 343-347, 1961.\n[202] A. L. Barab\u00e1si and R. Albert, \"Emergence of Scaling in Random Networks,\"\nScience, vol. 286, p. 509, 1999.\n[203] A. Wagner, \"Evolution of Gene Networks by Gene Duplications: A Mathematical\nModel and its Implications on Genome Organization,\" Proceedings of the National\nAcademy of Sciences, vol. 91, pp. 4387-4391, 1994.\n[204] G. Caldarelli, A. Capocci, P. De Los Rios, and M. A. Mu\u00f1oz, \"Scale-Free Networks\nfrom Varying Vertex Intrinsic Fitness,\" Physical Review Letters, vol. 89, p. 258702,\n2002.\n[205] P. Pollner, G. Palla, and T. Vicsek, \"Preferential attachment of communities: The\nsame principle, but a higher level,\" Europhysics Letters, vol. 73, pp. 478-484, 2006.\n[206] H. Jeong, B. Tombor, R. Albert, Z. N. Oltvai, and A. L. Barab\u00e1si, \"The large-scale\norganization of metabolic networks,\" Nature, vol. 407, pp. 651-654, 2000.\n[207] D. A. Fell and A. Wagner, \"The small world of metabolism,\" Nature Biotechnology,\nvol. 18, pp. 1121\u20131122, 2000.\n[208] J. Zhang, \"Evolution by gene duplication: an update,\" Trends in Ecology and\nEvolution, vol. 18, p. 292, 2003.\n[209] K. H. Wolfe and D. C. Shields, \"Molecular evidence for an ancient duplication of\nthe entire yeast genome,\" Nature, vol. 387, pp. 708-713, 1997.\n[210] M. Kellis, B. W. Birren, and E. S. Lander, \"Proof and evolutionary analysis of\nancient genome duplication in the yeast Saccharomyces cerevisiae,\" Nature, vol.\n428, pp. 617-624, 2004.\n[211] R. E. Amritkar and S. Jalan, \"Coupled dynamics on networks,\" Physica A:\nStatistical Mechanics and its Applications, vol. 346, pp. 13-19, 2005.\n[212] S. Boccaletti, J. Kurths, G. Osipov, D. L. Valladares, and C. S. Zhou, \"The\nsynchronization of chaotic systems,\" Physics Reports, vol. 366, pp. 1\u2013101, 2002.\n[213] M. E. Wall, W. S. Hlavacek, and M. A. Savageau, \"Design of gene circuits: lessons\nfrom bacteria,\" Nature Reviews Genetics, vol. 5, pp. 34-42, 2004.\n\n224\n\n\f[214] S. Bornholdt and T. Rohlf, \"Topological Evolution of Dynamical Networks: Global\nCriticality from Local Dynamics,\" Physical Review Letters, vol. 84, pp. 6114-6117,\n2000.\n[215] P. Fronczak, A. Fronczak, and J. A. Holyst, \"Self-organized criticality and\ncoevolution of network structure and dynamics,\" Physical Review E, vol. 73, p.\n046117, 2006.\n[216] M. G. Zimmermann, V. M. Egu\u00edluz, and M. San Miguel, \"Coevolution of\ndynamical states and interactions in dynamic networks,\" Physical Review E, vol. 69,\np. 65102, 2004.\n[217] H. Aguirre and K. Tanaka, \"A Study on the Behavior of Genetic Algorithms on NKLandscapes: Effects of Selection, Drift, Mutation, and Recombination,\" IEICE\nTransactions on Fundamentals of Electronics, Communications and Computer\nSciences, vol. 86, pp. 2270-2279, 2003.\n[218] N. Geard, J. Wiles, J. Hallinan, B. Tonkes, and B. Skellett, \"A comparison of\nneutral landscapes-NK, NKp and NKq,\" Proceedings of the 2002 Congress on\nEvolutionary Computation (CEC'02), vol. 1, 2002.\n[219] S. Kauffman, \"A proposal for using the ensemble approach to understand genetic\nregulatory networks,\" Journal of Theoretical Biology, vol. 230, pp. 581-90, 2004.\n[220] Y. C. Lin, F. S. Wang, and K. S. Hwang, \"A hybrid method of evolutionary\nalgorithms for mixed-integer nonlinear optimization problems,\" Proceedings of the\n1999 Congress on Evolutionary Computation (CEC '99), vol. 3, 1999.\n[221] C. A. C. Coello, \"Self-adaptive penalties for GA-based optimization,\" Proceedings\nof the 1999 Congress on Evolutionary Computation (CEC '99), vol. 1, 1999.\n[222] S. Y. Zeng, L. X. Ding, and L. S. Kang, \"An evolutionary algorithm of contracting\nsearch space based on partial ordering relation for constrained optimization\nproblems,\" Proceedings of the Fifth International Conference on Algorithms and\nArchitectures for Parallel Processing, pp. 76-81, 2002.\n[223] Y. Li, L. Kang, H. De Garis, Z. Kang, and P. Liu, \"A Robust Algorithm for Solving\nNonlinear Programming Problems,\" International Journal of Computer\nMathematics, vol. 79, pp. 523-536, 2002.\n[224] B. V. Babu and R. Angira, \"Modified differential evolution(MDE) for optimization\nof non-linear chemical processes,\" Computers and Chemical Engineering, vol. 30,\npp. 989-1002, 2006.\n[225] A. Lesne, \"Complex Networks: from Graph Theory to Biology,\" Letters in\nMathematical Physics, vol. 78, pp. 235-262, 2006.\n[226] D. E. Goldberg, K. Deb, and J. Horn, \"Massive Multimodality, Deception, and\nGenetic Algorithms,\" Parallel Problem Solving from Nature, 2, pp. 37-46, 1992.\n\n225\n\n\f[227] H. M\u00fchlenbein, M. Schomisch, and J. Born, \"The parallel genetic alghorithm as\nfunction optimizer,\" Parallel computing, vol. 17, pp. 619-632, 1991.\n[228] E. Janka, Vergleich Stochastischer Verfahren zur Globalen Optimierung:\nDiplomarbeit, Mathematisches Inst., Universitat Wien, 1999. A shorter online\nversion\nin\nEnglish\nlanguage\nis\nat\nwww.mat.univie.ac.at/~neum/glopt/janka/gopteng.html, 1999.\n[229] D. R. Stinson, An Introduction to the Design and Analysis of Algorithms. Winnipeg,\nMB, Canada: Charles Babbage Research Centre, 1987.\n[230] S. Khuri, T. B\u00e4ck, and J. Heitk\u00f6tter, \"An evolutionary approach to combinatorial\noptimization problems,\" in Proceedings of the 22nd annual ACM computer science\nconference on Scaling up: meeting the challenge of complexity in real-world\ncomputing applications, 1994, pp. 66-73.\n[231] F. J. MacWilliams and N. J. A. Sloane, The Theory of Error-Correcting Codes:\nNorth-Holland Amsterdam, 1977.\n[232] R. N. Sauer, A. R. Colville, and C. W. Burwick, \"Computer Points Way to More\nProfits,\" Hydrocarbon Processing, vol. 84, 1964.\n[233] C. A. Floudas and P. M. Pardalos, A Collection of Test Problems for Constrained\nGlobal Optimization Algorithms: Springer, 1990.\n[234] R. Angira and B. V. Babu, \"Evolutionary Computation for Global Optimization of\nNon-Linear Chemical Engineering Processes,\" in Proceedings of International\nSymposium on Process Systems Engineering and Control (ISPSEC'03)-For\nProductivity Enhancement through Design and Optimization Mumbai, 2003, pp. 34.\n[235] E. Sandgren, \"Nonlinear integer and discrete programming in mechanical design\noptimization,\" Journal of Mechanical Design, vol. 112, pp. 223\u2013229, 1990.\n[236] L. J. Eshelman and J. D. Schaffer, \"Real-coded genetic algorithms and intervalschemata,\" in Foundations of Genetic Algorithms. vol. 2, 1993, pp. 187\u2013202.\n[237] J. Bracken and G. P. McCormick, Selected Applications of Nonlinear\nProgramming: Wiley New York, 1968.\n[238] C. D. Maranas and C. A. Floudas, \"Global optimization in generalized geometric\nprogramming,\" Computers and Chemical Engineering, vol. 21, pp. 351-369, 1997.\n[239] C. S. Adjiman, I. P. Androulakis, and C. A. Floudas, \"A Global Optimization\nMethod, BB, for General Twice-Differentiable Constrained NLPs: II.\nImplementation and Computational Results,\" Computers and Chemical\nEngineering, vol. 22, pp. 1159-1179, 1998.\n[240] T. F. Edgar, D. M. Himmelblau, and L. S. Lasdon, Optimization of Chemical\nProcesses: McGraw-Hill, 2001.\n226\n\n\f[241] J. Fu, R. G. Fenton, and W. L. Cleghorn, \"A mixed integer-discrete-continuous\nprogramming method and its application to engineering design optimization,\"\nEngineering optimization, vol. 17, pp. 263-280, 1991.\n[242] B. K. Kannan and S. N. Kramer, \"Augmented Lagrange multiplier based method for\nmixed integer discrete continuous optimization and its applications to mechanical\ndesign,\" ASME, vol. 65, pp. 103-112, 1993.\n[243] Y. J. Cao and Q. H. Wu, \"Mechanical design optimization by mixed-variable\nevolutionary programming,\" Proceedings of the IEEE International Conference on\nEvolutionary Computation, pp. 443\u20136, 1997.\n[244] A. Belegundu and J. Arora, \"A study of mathematical programming methods for\nstructural optimization. part i: theory,\" International Journal for Numerical\nMethods in Engineering, vol. 21, pp. 1583-1599, 1985.\n[245] J. S. Arora, Introduction to Optimum Design: McGraw-Hill, 1989.\n[246] K. Deb and M. Goyal, \"Optimizing Engineering Designs Using a Combined\nGenetic Search,\" Proceedings of the Sixth International Conference on Genetic\nAlgorithms (ICGA-97), pp. 521-528, 1997.\n\n227\n\n\fAPPENDIX A\nA.1\n\nTEST FUNCTION DEFINITIONS\n\nArtificial Test Functions\n\nTable A-1 Artificial Test Function Characteristics Table. Epi: Epistasis or Tight Linkage (i.e. Non-Separable),\nCon = Continuous, n = problem dimensionality (* indicates n is a parameter of the problem), Ref. = reference to\nproblem description used, MM = multimodal fitness landscape, Params = parameters of the problem.\n\nA.1.1\n\nName\nMTTP\nECC\n\nEpi\nYes\nYes\n\nCon\nNo\nNo\n\nn\n*\n*\n\nMM\nYes\nYes\n\nRef.\n[135]\n[135]\n\nMMDP\n\nYes\n\nNo\n\n*\n\nYes\n\n[226]\n\nFrequency Modulation\nNK Landscape\n\nYes\nvaries\n\nYes\nNo\n\n6\n*\n\nYes\nYes\n\n[56]\n[81]\n\nRosenbrock\nRastrigin\nSchwefel\nGriewangk\nBohachevsky's\nWatson's\nColville's\nSystem of linear\nequations\nAckley's Function\nNeumaier's Function #2\nHyper Ellipsoid\n\nYes\nNo\nNo\nYes\nYes\nNo\nYes\nNo\n\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nYes\n\n*\n*\n*\n*\n2\n5\n4\n10\n\nNo\nYes\nYes\nYes\nYes\nNo\nYes\nNo\n\n[56]\n[56]\n[227]\n[56]\n[56]\n[56]\n[56]\n[56]\n\nNo\nNo\nNo\n\nYes\nYes\nYes\n\n25\n4\n*\n\nYes\nYes\nNo\n\n[56]\n[228]\n[83]\n\nParams.\nn = 200\nn = M*N\nM = 24\nN = 12\nn = 6k\nk=20\nn=N\nN, K varies\nn=2\nn = 20\nn = 20\nn = 10\n\nn = 30\n\nMinimum Tardy Task Problem (MTTP)\n\nThe Minimum Tardy Task Problem (MTTP) [229] is a task scheduling problem where the\nobjective is to execute as many tasks as possible within the time constraints and precedence\nrelations. Each task Tj, j \u220a {1,2,...,n) has a time length Lj (time needed to execute task), a\ndeadline Dj (before which the task must be completed), and a weight Wj (indicating the\npenalty cost from not completing the task). L, D, and W all take on positive integer values\nand the scheduling tasks are executed in sequential order. The scheduling problem is then\nto find a subset S of T which executes within the allocated time and minimizes the sum of\nall penalties for tasks that were not completed. A penalty term P is added for infeasible\nsolutions involving tasks that are started but not finished within the allocated time. P is\ngiven as the sum of all task weights thereby ensuring that infeasible solutions are assigned a\nworse fitness than feasible solutions.\n\n228\n\n\fMin F ( x ) = P +\n\n\u2211W\n\nj\u03b5 T \u2212 S\n\nj\n\nP = \u2211W j\nj\u03b5T\n\nxi \u2208 (0,1)\nCandidate schedules S are represented as a binary vector x indicating which tasks are to be\nexecuted. A candidate schedule is therefore the set of all tasks where xj =1. As enforced by\nthe problem generation method, tasks are ordered in the candidate schedule by their\ndeadlines and are also executed in that order. (This is easy to see since Dj as defined below\nis a monotonically increasing function of j). The size of the problem n can be controlled\nusing the problem generation method specified below (and taken from [230]) where n is a\nmultiple of 5.\nProblem Generation Method\n\u23a7 L if j > 5\nLj = \u23a8 i\n\u23a9 3 j else\n\u23a7 D + 24m if j > 5\nDj = \u23a8 i\n\u23a95 j else\n\n\u23a7Wi (m + 1)\n\u23aa 60 if\n\u23aa\n\u23aa 40 if\nWj = \u23a8\n7 if\n\u23aa\n\u23aa 3 if\n\u23aa\n\u23a9 50 if\n\nif j > 5\nj =1\nj=2\nj =3\nj=4\nj =5\n\ni = (j Mod 5) + 1,\n\nm = j / 5,\n\nj =1,...,n\n\nOptimal Solution = 2n\n\n229\n\n\fMTTP as implemented in Code\nFunction F(x)\n\n'xj j\u220a{1,2,...,n) ' parameters to optimize\n\n'start time for a task\nT0 = 0\nFor j = 1 To n\n'Is task being executed?\nIf xj = 1 Then\nIf (T0 + Lj)< Dj Then 'can we complete task in time?\nT0 = T0 + Lj\nElse\nInfeasible = True\nCost = Cost + Wj\nEnd If\nElse\nCost = Cost + Wj\nEnd If\nNext i\nP = sum(Wj)\nIf Infeasible Then Cost = Cost + P\nF(x) = 2n \u2013 Cost\n'Fopt = 0\n\nA.1.2\n\nError Correcting Code Problem (ECC)\n\nThe Error Correcting Code Problem (ECC) [231] is a problem where we try to minimize\nthe error in reading coded messages (error due to input noise) by maximizing the distance\nbetween code words in the code parameter space. Given a set of M binary code words,\neach of length N, the objective is to maximize the Hamming distance d between any pair of\ncode words.\nMax F ( X ) =\n\n1\nM\n\nM\n\n1\n2\nj =1, j \u2260 i d ij\n\n\u2211\u2211\ni =1\n\nxi \u2208 (0,1), i \u2208 {1,..., n}\nn = M*N\n\nOptimal (for M=24, N=12, F = 0.067416)\n\n230\n\n\fA.1.3\n\nMassively Multimodal Deceptive Problem (MMDP)\nk\n\nMin F ( X ) = \u2211 y j +1 \u2212 k\ni =1\n\n6\n\nj = \u2211 xt , t = 6(i \u2212 1) + m\nm =1\n\nY = (y1, y2, y3, y4, y5, y6) = (1, 0, 0.360384, 0.640576, 0.360384, 0, 1)\nxi \u2208 (0,1), i \u2208 {1,..., n}\nn = 6k\n\nA.1.4\n\nFrequency Modulation\n\nThe problem is to specify six parameters of the frequency modulation sound model\nrepresented by y(t).\n\nOriginal Parameters: X = (x1, x2, x3, x4, x5, x6) = (a1, w1, a2, w2, a3, w3)\n100\n\nMin F ( X ) = \u2211 ( y ( X , t ) \u2212 y ( X 0 , t ))\n\n2\n\nt =0\n\ny ( X , t ) = x1 sin ( x 2 t\u03b8 + x3 sin ( x 4 t\u03b8 + x5 sin ( x6 t\u03b8 )))\n\u239b 2\u03c0 \u239e\n\u239f X0 = (1.0, 5.0, 1.5, 4.8, 2.0, 4.9)\n\u239d 100 \u23a0\n- 6.4 \u2264 x i \u2264 6.35, i \u2208 {1,..., n}\n\n\u03b8 =\u239c\n\nxi \u220a R , n = 6\nOptimal (F, x1, x2, x3, x4, x5, x6) = (0, 1.0, 5.0, 1.5, 4.8, 2.0, 4.9)\n\nA.1.5\n\nQuadratic Function\n\nMin F ( X ) = x12 + x 22 + x32\n- 5.12 \u2264 x i \u2264 5.12, i \u2208 {1,..., n}\nxi \u220a R , n = 3\nOptimal (F, x1, x2, x3) = (0, 0, 0, 0)\n\n231\n\n\fA.1.6\n\nGeneralized Rosenbrock's Function\nn \u22121\n\n(\n\n(\n\nMin F ( X ) = \u2211 100 xi +1 \u2212 xi2\ni =1\n\n) + (x\n2\n\ni\n\n\u2212 1)\n\n2\n\n)\n\n- 2 \u2264 x i \u2264 2, i \u2208 {1,..., n}\n\nxi \u220a R , n = 2\nOptimal (F, x1, x2) = (0, 1.0, 1.0)\n\nA.1.7\n\nRastrigin's Function\nn\n\n(\n\n)\n\nMin F ( X ) = 10n + \u2211 xi2 \u2212 cos(2\u03c0xi )\ni =1\n\n- 5.12 \u2264 x i \u2264 5.12, i \u2208 {1,..., n}\n\nxi \u220a R, n = 20\nOptimal (F, xi,...xn) = (0, 0,...,0)\n\nA.1.8\n\nSchwefel's Function\nn\n\n(\n\nMin F ( X ) = \u2211 \u2212 x(i ) Sin Abs( x(i ) )\ni =1\n\n2\n\n)\n\n- 500 \u2264 x i \u2264 500, i \u2208 {1,..., n}\n\nxi \u220a R, n = 20\nOptimal (F, xi,...xn) = (0, 0,...,0)\n\nA.1.9\n\nGriewangk's Function\n\nn\n\u239bx \u239e\n1 n 2\n\u2212\nx\ncos\u239c\u239c i \u239f\u239f + 1\n\u2211\n\u220f\ni\n4000 i =1\ni =1\n\u239d i\u23a0\n- 600 \u2264 x i \u2264 600, i \u2208 {1,..., n}\n\nMin F ( X ) =\n\nxi \u220a R, n = 10\nOptimal (F, xi,...xn) = (0, 0,...,0)\n\n232\n\n\fA.1.10 Bohachevsky's Function\n\nMin F ( X ) = x12 + 2 x 22 \u2212 0.3 cos(3\u03c0x1 ) cos(4\u03c0x 2 ) + 0.3\n- 50 \u2264 x i \u2264 50, i \u2208 {1,..., n}\nxi \u220a R , n = 2\nOptimal (F, xi, x2) = (0, 0, 0)\n\nA.1.11 Watson's Function\n2\n\n2\n\u239e\n\u239b 5\n\u23a1 6 j \u22121 \u23a4\nj \u22121\n\u239c\nMin F ( X ) = \u2211 \u2211 jai x j +1 \u2212 \u23a2\u2211 ai x j \u23a5 \u2212 1\u239f + x12\n\u239f\n\u239c j =1\ni =1\n\u23a3 j =1\n\u23a6\n\u23a0\n\u239d\ni \u22121\nai =\n29\n- 2 \u2264 x i \u2264 2, i \u2208 {1,..., n}\n\n(\n\n30\n\n)\n\nxi \u220a R , n = 6\nOptimal (F, xi,..., xn) = (2.288E-3, -0.0158, 1.012, -0.02329, 1.260, -1.513, 0.09928)\n\nA.1.12 Colville's Function\n\n(\n\nMin F ( X ) = 100 x 2 \u2212 x12\n\n(\n\n) + (1 \u2212 x )\n2\n\n2\n\n)\n\n1\n\n(\n\n+ 90 x 4 \u2212 x32\n\n) + (1 \u2212 x )\n2\n\n2\n\n3\n\n+\n\n10.1 ( x 2 \u2212 1) + ( x 4 \u2212 1) + 19.8( x 2 \u2212 1)( x 4 \u2212 1)\n- 10 \u2264 x i \u2264 10, i \u2208 {1,..., n}\n2\n\n2\n\nxi \u220a R , n = 4\nOptimal (F, xi,..., xn) = (0, 1,..., 1)\n\n233\n\n\fA.1.13 System of linear equations\nMin F ( X ) = \u2211\u2211 (aij x j ) \u2212 b j\nn\n\nn\n\ni =1 j =1\n\nAx = b is given by:\n\n- 9 \u2264 x i \u2264 9, i \u2208 {1,..., n}\n\nxi \u220a R, n = 10\nOptimal (F, xi,..., xn) = (0, 1,..., 1)\n\nA.1.14 Ackley's Function\nn\n\u239b\nMin F ( x ) = \u221220 exp\u239c \u2212 0.2 n \u22121 \u2211 xi2\n\u239c\ni =1\n\u239d\n- 32.768 \u2264 x i \u2264 32.768, i \u2208 {1,..., n}\n\nn\n\u239e\n\u239f \u2212 exp\u239b\u239c n \u22121 \u2211 cos(2\u03c0xi )\u239e\u239f + 20\n\u239f\ni =1\n\u239d\n\u23a0\n\u23a0\n\nxi \u220a R, n = 25\nOptimal (F, xi,..., xn) = (0, 0,..., 0)\n\nA.1.15 Neumaier's Function #2\nOriginal definition\nn\n\u239b\n\u239e\nMin F ( x ) = \u2211 \u239c bk \u2212 \u2211 xik \u239f\nk =1 \u239d\ni =1\n\u23a0\nb = (8, 18, 44, 114)\n0 \u2264 x i \u2264 n, i \u2208 {1,..., n}\nn\n\n2\n\nxi \u220a R , n = 4\nOptimal (F, x1, x2, x3, x4) = (0, 1, 2, 2, 3)\n\n234\n\n\fModified definition (used in all experiments)\nn\n\nMin F ( x ) = \u2211 (bk \u2212 \u03b1 k )\n\n2\n\nk =1\n\nk\n\n\u239b\n\n\u239e\n\nn\n\n\u03b1 k = \u2211 \u239c \u2211 xij \u239f\n\nj =1 \u239d i =1\n\u23a0\nb = (8, 18, 44, 114)\n0 \u2264 x i \u2264 n, i \u2208 {1,..., n}\n\nxi \u220a R , n = 4\nOptimal unknown\n\nA.1.16 Hyper Ellipsoid\n\n( )\n\nn\n\nMin F ( X ) = \u2211 xi2 i 2\ni =1\n\n- 1 \u2264 x i \u2264 1, i \u2208 {1,..., n}\n\nxi \u220a R, n = 30\nOptimal (F, x1,..., xn) = (0, 0,..., 0)\n\nA.2\n\nEngineering Design Test Problems\n\nA.2.1 Turbine Power Plant\nMin F ( X ) = x3 f 1 + x 4 g1\n\ng1 ( X ) = 0.8008 + 0.2031x 2 + 0.000916 x 22\ng 2 ( X ) = 0.7266 + 0.2256 x 2 + 0.000778 x 22\nf1 ( X ) = 1.4609 + 0.15186 x1 + 0.00145 x12\nf 2 ( X ) = 1.5742 + 0.1631x1 + 0.001358 x12\n\nSubject to:\nBFG = (1 \u2212 x3 ) f 2 + (1 \u2212 x 4 )g 2 \u2264 10.0\n(18, 14, 0, 0) < (x1, x2, x3, x4) < (30, 25, 1, 1)\nxi \u220a R , n = 4\n\nOptimal Solution (F, x1, x2, x3, x4) = (3.05, 30, 20, 0, 0.58)\n\n235\n\n\fA.2.2\n\nAlkylation Process\n\nFractionator\nIsobutane\nMake up\n\nReactor\nOlefin feed\nAlkylate Product\n\nFresh Acid\n\nSpent Acid\n\nFigure A-1 Simplified diagram of an alkylation process (recreated from [224])\n\nThe alkylation process design problem, originally defined in [232], has the goal of\nimproving the octane number of an olefin feed stream through a reaction involving\nisobutene and acid. The reaction product stream is distilled with the lighter hydrocarbon\nfraction recycled back to the reactor. The objective function considers maximizing alkylate\nproduction minus the material (ie feed stream) and operating (ie recycle) costs. Design\nparameters all take on continuous values and include the olefin feed rate x1 (barrels/day),\nacid addition rate x2 (thousands of pounds/day), alkylate yield x3 (barrels/day), acid strength\nx4 (wt. %), motor octane number x5, external isobutene to olefin ratio x6, and F-4\nperformance number x7.\n\n236\n\n\fMax F ( X ) = 1.715 x1 + 0.035 x1 x6 + 4.0565 x3 + 10.0 x 2 \u2212 0.063x3 x5\nSubject to:\ng1 ( X ) = 0.0059553571x 62 x1 + 0.88392857 x3 \u2212 0.1175625 x6 x1 \u2212 x1 \u2264 0\n\ng 2 ( X ) = 1.1088 x1 + 0.1303533x1 x 6 \u2212 0.0066033x1 x 62 \u2212 x3 \u2264 0\n\ng 3 ( X ) = 6.66173269 x62 + 172.39878 x5 \u2212 56.596669 x 4 \u2212 191.20592 x6 \u2212 10000 \u2264 0\n\ng 4 ( X ) = 1.08702 x6 + 0.32175 x 4 \u2212 0.03762 x62 \u2212 x5 + 56.85075 \u2264 0\ng 5 ( X ) = 0.006198 x7 x 4 x3 + 2462.3121x 2 \u2212 25.125634 x 2 x 4 \u2212 x3 x 4 \u2264 0\ng 6 ( X ) = 161.18996 x 4 x3 + 5000 x 2 x 4 \u2212 489510 x 2 \u2212 x3 x 4 x 7 \u2264 0\ng 7 ( X ) = 0.33 x7 \u2212 x5 + 44.333333 \u2264 0\ng 8 ( X ) = 0.022556 x5 \u2212 0.007595 x7 \u2212 1.0 \u2264 0\ng 9 ( X ) = 0.00061x3 \u2212 0.0005 x1 \u2212 1.0 \u2264 0\ng10 ( X ) = 0.819672 x1 \u2212 x3 + 0.819672 \u2264 0\ng11 ( X ) = 24500.0 x 2 \u2212 250.0 x 2 x 4 \u2212 x3 x 4 \u2264 0\ng12 ( X ) = 1020.4082 x 4 x 2 + 1.2244898 x3 x 4 \u2212 100000 x 2 \u2264 0\ng13 ( X ) = 6.25 x1 x6 + 6.25 x1 \u2212 7.625 x3 \u2212 100000 \u2264 0\ng14 ( X ) = 1.22 x3 \u2212 x6 x1 \u2212 x1 + 1.0 \u2264 0\n\n(1500, 1, 3000, 85, 90, 3, 145) < (x1, x2, x3, x4, x5, x6, x7) < (2000, 120, 3500, 93, 95, 12,\n162)\nxi \u220a R , n = 7\nOptimal Solution (F, x1, x2, x3, x4, x5, x6, x7) = (1772.77, 1698.18, 53.66, 3031.3, 90.11, 95,\n10.5, 153.53)\n\n237\n\n\fA.2.3\n\nHeat Exchanger Network Design\n\nFigure A-2 Diagram of the Heat Exchanger Network Design Problem involving 1 cold stream that exchanges heat\nwith three hot streams. Parameters to optimize include heat exchange areas (x1, x2, x3) and stream temperatures\n(x4, x5, x6, x7, x8).\n\nThe Heat Exchanger Network design problem, originally defined by [233], has the goal of\nminimizing the total heat exchange surface area for a network consisting of one cold stream\nand three hot streams.\n\nAs shown in Figure A-2, there are eight design parameters\n\nconsisting of the heat exchanger areas (x1, x2, x3), intermediate cold stream temperatures\n(x4, x5) and hot stream outlet temperatures (x6, x7, x8). The problem is presented below in a\nreformulated form taken from [234] where a variable reduction method has been used to\neliminate equality constraints.\nMin F ( X ) = x1 + x 2 + x3\nSubject to:\ng1 ( X ) = 100 x1 \u2212 x1 (400 \u2212 x 4 ) + 833.33252 x 4 \u2212 83333.333 \u2264 0\ng 2 ( X ) = x 2 x 4 \u2212 x 2 (400 \u2212 x5 + x 4 ) \u2212 1250 x 4 + 1250 x5 \u2264 0\ng 3 ( X ) = x3 x5 \u2212 x3 (100 + x5 ) \u2212 2500 x5 + 1250000 \u2264 0\n\n(100, 1000, 1000, 10, 10) < (x1, x2, x3, x4, x5) < (10000, 10000, 10000, 1000, 1000)\nxi \u220a R , n = 5\nOptimal Solution (F, x1, x2, x3, x4, x5) = (7049.25, 579.19, 1360.13, 5109.92, 182.01,\n295.60,)\nremaining parameters are calculated from equality constraints. Their optimal values are:\n(x6, x7, x8) = (217.9, 286.40, 395.60)\n\n238\n\n\fA.2.4\n\nPressure Vessel\n\nFigure A-3 Pressure Vessel Drawing. Parameters of the problem include the thickness of the shell Ts, the thickness\nof the head Th, the inner radius of the vessel R and the length of the cylindrical section of the vessel L. This figure\nis taken out of [221] and is reprinted with permission from IEEE (\u00a9 1999 IEEE).\n\nThe pressure vessel design problem, originally defined by [235], has the goal of minimizing\nthe cost of a pressure vessel as calculated based on material, forming and welding costs.\nThe design is subject to dimensional constraints which are set to meet ASME standards for\npressure vessels. As shown in Figure A-3, there are four design parameters to optimize\nconsisting of the thickness of the shell Ts, the thickness of the head Th, the inner radius R\nand the length of the cylindrical section of the vessel L. Ts and Th take on integer values\nindicating the number of rolled steel plates (where each steel plate is 0.0625 inches thick)\nand R and L are continuous variables.\n\n239\n\n\fOriginal Parameters: X = (R, L, Th , Ts ) = ( x1 , x 2 , x3 , x 4 )\n\nMin F ( X ) = 0.6224 x1 x 2 (0.0625 x3 ) + 1.7781x12 (0.0625 x 4 )\n\n+ 3.1661x 2 (0.0625 x3 ) + 19.84 x1 (0.0625 x3 )\n2\n\n2\n\nSubject to:\ng1 ( X ) = \u22120.0625 x3 + 0.0193x1 \u2264 0\ng 2 ( X ) = \u22120.0625 x4 + 0.00954 x1 \u2264 0\n\ng 3 ( X ) = \u2212\u03c0x12 x 2 \u2212 43 \u03c0x13 + 1,296,000 \u2264 0\ng 4 ( X ) = x 2 \u2212 240 \u2264 0\n\n(1, 1, 1, 1) < (x1, x2, x3, x4) < (100, 400, 20, 20)\nx1, x2 \u220a R, x3, x4 \u220a Z, n = 4\nOptimal Solution unknown\n\nA.2.5\n\nCoello's Welded Beam Design\n\nFigure A-4: Diagram of a welded beam. The beam load is defined as P with all other parameters shown in the\ndiagram defining dimensional measurements relevant to the problem. This figure is taken out of [221] and is\nreprinted with permission from IEEE (\u00a9 1999 IEEE).\n\nThe Welded beam design problem has the goal of minimizing the cost of a weight bearing\nbeam subject to constraints on shear stress \u03c4, bending stress \u03c3, buckling load on the bar Pc,\nand dimensional constraints [221]. There are four design parameters to optimize consisting\nof the dimensional variables h, l, t, and b shown in Figure A-4.\nThe original formulation of the problem can be found in {Rekalitis, #336}. A change to\nthe Pc term in the problem formulation (stated below) appears to have occurred in [221] and\n240\n\n\fsome publications have implemented this new problem definition including the work\npresented in this thesis and others cited in Appendix C.\nOriginal Parameters: X = (h, l , t , b ) = ( x1 , x 2 , x3 , x 4 )\nMin F ( X ) = 1.10471x12 x 2 + 0.04811x3 x 4 (14 + x 2 )\n\nSubject to:\ng1 ( X ) = \u03c4 ( X ) \u2212 \u03c4 max \u2264 0\ng 2 ( X ) = \u03c3 ( X ) \u2212 \u03c3 max \u2264 0\ng 3 ( X ) = x1 \u2212 x 4 \u2264 0\n\ng 4 ( X ) = 0.10471x12 + 0.04811x3 x 4 (14 + x 2 ) \u2212 5 \u2264 0\ng 5 ( X ) = 0.125 \u2212 x1 \u2264 0\ng 6 ( X ) = \u03b4 ( X ) \u2212 \u03b4 max \u2264 0\ng 7 ( X ) = P \u2212 Pc ( X ) \u2264 0\n\nWhere:\n\n\u03c4 (X ) =\n\n\u03c4\u2032 =\n\n(\u03c4 \u2032)2 + 2\u03c4 \u2032\u03c4 \u2032\u2032 x2 + (\u03c4 \u2032\u2032)2\n\n2R\nx \u239e\nP\nMR\n\u239b\n, \u03c4 \u2032\u2032 =\n, M = P\u239c L + 2 \u239f\n2\u23a0\nJ\n2 x1 x 2\n\u239d\n\n2\n\u23a7\u23aa\n\u23a1 x 22 \u239b x1 + x3 \u239e 2 \u23a4 \u23ab\u23aa\nx 22 \u239b x1 + x3 \u239e\n+\u239c\n=\n+\u239c\nJ\n2\n2\nx\nx\n\u239f \u23a5\u23ac\n\u239f\n\u23a8\n1 2\u23a2\n4 \u239d 2 \u23a0\n12\n\u239d 2 \u23a0 \u23a6\u23a5 \u23aa\u23ad\n\u23aa\u23a9\n\u23a3\u23a2\n6 PL\n4 PL3\n(\n)\n\u03c3 (X ) =\n\u03b4\n,\nX\n=\nEx33 x 4\nx 4 x32\n\nR=\n\nx32 x 46\nx\nE \u239e\n36 \u239b\u239c\n\u239f\n1\u2212 3\nPc ( X ) =\n2\n\u239f\n\u239c\nL\n\u239d 2 L 4G \u23a0\nP = 6000 lb, L = 14 in, \u03b4max = 0.25 in, E = 30 x 106 psi, G = 12 x 106 psi,\n\u03c4max = 13,600 psi, \u03c3max = 30,000 psi\n4.013E\n\n(0.1, 0.1, 0.1, 0.1) < (x1, x2, x3, x4) < (2, 10, 10 ,2)\nxi \u220a R , n = 4\nOptimal Solution unknown\n\n241\n\n\fA.2.6\n\nTension Compression Spring\n\nFigure A-5 Diagram of Tension Compression Spring. Parameters of the problem include the mean coil diameter\nD, the wire diameter d and the number of active coils N which is represented by the number of loops of wire in the\ndiagram. Forces acting on the spring are shown as P. This figure is taken out of [221] and is reprinted with\npermission from IEEE (\u00a9 1999 IEEE).\n\nThe Tension Compression Spring problem, shown in Figure A-5, has the goal of\nminimizing the weight of a tension/compression spring subject to constraints on minimum\ndeflection, shear stress, surge frequency, and dimensional constraints [221]. There are\nthree design parameters to optimize consisting of the mean coil diameter D, the wire\ndiameter d and the number of active coils N.\nOriginal Parameters: X = (d , D, N ) = ( x1 , x 2 , x3 )\nMin F ( X ) = (N + 2 )Dd 2\n\nSubject to:\n\nD3 N\n\u22640\ng1 ( X ) = 1 \u2212\n71785d 4\n4 D 2 \u2212 dD\n1\ng 2 (X ) =\n+\n\u22121 \u2264 0\n3\n4\n12566 Dd \u2212 d\n5108d 2\n140.45d\n\u22640\ng3 (X ) = 1 \u2212\nD2 N\nD+d\ng 4 (X ) =\n\u22121 \u2264 0\n1.5\n\n(\n\n)\n\n(0.05, 0.25, 2) < (x1, x2, x3) < (2, 1.3, 15)\nx1, x2 \u220a R, x3 \u220a Z, n = 3\nOptimal Solution unknown\n\n242\n\n\fA.2.7\n\nGear Train Design\n\nThe gear train design problem was originally defined by [235] consists of optimizing a gear\ntrain such that the gear ratio approach as close as possible to 1/6.931. There are four design\nparameters consisting of integer values for the number of teeth for each gear.\nOriginal Parameters: X = (Td , Tb , Ta , T f ) = ( x1 , x 2 , x3 , x 4 )\n\u239b 1\nxx \u239e\n\u2212 1 2 \u239f\u239f\nMin F ( X ) = \u239c\u239c\n\u239d 6.931 x3 x 4 \u23a0\n\n2\n\n12 \u2264 x i \u2264 60, i \u2208 {1,..., n}\n\nx i \u220a Z, n = 4\nOptimal Solution (F, x1, x2, x3, x4) = (2.70 x10-12, 19, 16, 43, 49)\n\n243\n\n\fAPPENDIX B\n\nSEARCH OPERATORS\n\nFor the search operator descriptions, the offspring is defined as H = (h1,...hn) and the jth\nparent is defined as Gj = (gij,...gnj). For all search operators, the first parent is always the\nbest of the selected parents.\nAlso in the description, variables randomly assigned over a Uniform distribution with upper\nand lower bounds U and L are stated as Uniform(U, L), variables randomly assigned over a\nNormal distribution with mean \u03bc and variance \u03c32 are stated as N(\u03bc, \u03c32).\nTable B-1: List of search operators used in experiments. Details provided in this table include the search operator\nname, other common name, reference for description, and parameter settings if different from reference.\nSearch Operator Name\nWright's Heuristic Crossover\nSimple Crossover\nExtended Line Crossover\nUniform Crossover\nBLX- \u03b1\nDifferential Evolution\nSwap\nRaise\nCreep\nSingle Point Random Mutation\n\nB.1\n\nOther Name\nInterpolation\nSingle point\nCrossover\nExtrapolation\nDiscrete Crossover\n\nParameter Settings\nr = 0.5\n-\n\nReference\n[53]\n[66]\n\n\u03b1=0.3\n-\n\u03b1=0.2\n-\n-\nA = 0.01\nA = 0.001\n-\n\n[46]\n[46]\n[236]\n[50]\n[104]\n[104]\n[54]\n[66]\n\nSingle Point Random Mutation\n\nThis operator is described in [66] and requires a single parent G1. The ith gene is defined\nby:\n\u23a7 r if i = k\nhi = \u23a8\n1\n\u23a9 g i else\nk = Uniform[0, n], k \u2208 \u0396\n\n(\n\nr = Uniform hiMAX , hiMIN\n\nB.2\n\n)\n\nCreep\n\nThe creep operator is a variant of Gaussian mutation and was originally described in [54].\nCreep requires a single parent G1. The ith gene is defined by:\n\n244\n\n\f\u23a7 g i1 + r if i = k\nhi = \u23a8\n1\n\u23a9 g i else\nk = Uniform[0, n], k \u2208 \u0396\n\n(\n\n)\n\nr = N 0, \u03c3 2\nhiMAX \u2212 hiMIN\n\u03c3=\n1000\n\nB.3\n\nRaise\n\nThis operator is described in [104] and requires a single parent G1. The ith gene is defined\nby:\nhi = g i + ri\n\n(\n\nri = N 0, \u03c3 i2\n\n\u03c3i =\n\nB.4\n\n)\n\n\u2212 hiMIN\n100\n\nMAX\ni\n\nh\n\nSwap\n\nThis operator is described in [104] and requires two parents, G1 and G2. Defining G1 as the\nmore fit parent, the ith gene is defined by:\n\u23a7 g i1 if Ri > \u03b1\nhi = \u23a8 2\n\u23a9 g i else\nRi = Rank ( g i1 \u2212 g i2 )\n\nThe function Rank() gives the ranking of the absolute difference between gene values of\ntwo parents (Rank = 1 being the greatest absolute difference).\nParameter Specifications: In this work, \u03b1 = 1. This means only the single most similar\ngene between the parents will be swapped.\n\nB.5\n\nUniform Crossover\n\nThis operator was originally described by [46] and requires two parents, G1 and G2. The ith\ngene is defined by:\n245\n\n\f\u23a7 g i1 if ri > 0.5\nhi = \u23a8 2\n\u23a9 g i else\nri = Uniform[0,1]\n\nB.6\n\nSingle Point Crossover\n\nThis operator requires two parents, G1 and G2. The offspring H is then defined by:\nH = (g11, g21,..., gi1, gi+12,..., gn2)\ni = Uniform[0, n], i \u2208 \u0396\n\nB.7\n\nBLX- \u03b1 Crossover\n\nThis operator was originally described by [236] and requires two parents, G1 and G2. The ith\ngene is defined by:\nhi = Uniform[g min \u2212 I \u2217 \u03b1 , g max + I \u2217 \u03b1 ]\n\n(\n)\n= Min(g , g )\n\ng min = Max g i1 , g i2\n1\ng max\ni\nI = g max \u2212 g min\n\n2\ni\n\nSpecifications: The parameter \u03b1 must be set by the user. In this work, \u03b1 = 0.2\n\nB.8\n\nWright's Heuristic Crossover\n\nThis operator was originally described in [53] and requires two parents, G1 and G2.\nDefining G1 as the more fit parent, the ith gene is defined by:\n\n(\n\n)\n\nhi = r g i1 \u2212 g i2 + g i1\nr = Uniform[0,1]\n\nModifications: In this work, r is a static value set at 0.5. This operator has been modified\nto create a single offspring instead of two offspring by defining G1 as the more fit parent.\n\n246\n\n\fB.9\n\nExtended Line Crossover\n\nThis operator was originally described in [46] and requires two parents, G1 and G2.\nDefining G1 as the more fit parent, the ith gene is defined by:\n\n(\n\n)\n\nhi = r g i2 \u2212 g i1 + g i1\nr = Uniform[\u2212 0.25,1.25]\n\nModifications: In this work, r is a static value set at 0.3. This operator has been modified\nto create a single offspring instead of two offspring by defining G1 as the more fit parent.\n\nB.10\n\nDifferential Evolution Operator\n\nThis operator was originally described in [50] and requires four parents G1, G2, G3, and G4.\nThe ith gene is defined by:\n\u23a7 g 2 + \u03b1 (g i3 \u2212 g i4 ) \u2192 \u03b2 > r\nhi = \u23a8 i\ng i1 \u2192 else\n\u23a9\nr = Uniform[0,1]\n\nSpecifications: \u03b1 and \u03b2 are parameters that must be set. In this work, \u03b1 = 1 and \u03b2 = 0.5\n\n247\n\n\fAPPENDIX C\nC.1\n\nADDITIONAL RESULTS FROM\nCHAPTER 5\n\nEngineering Design Problems: Performance\nComparisons with the Literature\n\nTables are provided below which compare solution results from experiments in Chapter 5\nwith other results stated in the literature. The first column lists the authors (with reference),\nthe second column states the reported fitness values, and the third column provides the\nnumber of objective function evaluations used to obtain the fitness values reported in\ncolumn two. It is important to keep in mind that the results taken from the literature\nrepresent the best solution among all algorithms tested in that reference. Also, some studies\nimplement different requirements for constraint feasibility making some of the results\ndifficult to compare. For the constraint function values reported in this section, negative\nvalues are used to indicate the satisfaction of inequality constraints.\nTable C-1 Comparison of results for the alkylation process design problem (maximization problem). Results from\nother authors were reported in [224]. The best solution found in these experiments was (F, x1, x2, x3, x4, x5, x6, x7) =\n(1772.77, 1698.18, 54.73, 3029.65, 90.25, 95, 1035, 153.53) with constraints (g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, g12,\ng13, g14) = (0, 0, 4.70E-11, 0, 0, 3.72E-11, 9.98E-8, -0, 0, 0, 0, 0, 0, 0).\n\nReference\nBracken and McCormick, 1968\n[237]\nMaranas and Floudas, 1997 [238]\nAdjiman et al., 1998 [239]\nEdgar and Himmelblau, 2001 [240]\nBabu and Angira, 2006 [224]\nSOTEA (This Thesis)\ncGA (This Thesis)\nPanmictic EA (This Thesis)\n\nFitness\n1769\n\n1772.77\n1772.77\n1768.75\n1766.36\n1772.77\n1772.77\n1771.35\n\nObjective Function Evaluations\nnot reported\n\nnot reported\nnot reported\nnot reported\n92287 (average value)\n150,000\n150,000\n150,000\n\n248\n\n\fTable C-2 Comparison of results for the heat exchanger network design problem (minimization problem). Results\nfrom other authors were reported in [224]. The best solution found in these experiments was (F, x1, x2, x3, x4, x5) =\n(7049.25, 579.19, 1360.13, 5109.92, 182.01, 295.60) with constraints (g1, g2, g3) = (-2.06E-3, -6.22E-3, -4.60E-3).\n\nReference\nAngira and Babu, 2003 [234]\nBabu and Angira, 2006 [224]\nSOTEA (This Thesis)\ncGA (This Thesis)\nPanmictic EA (This Thesis)\n\nFitness\n7049.25\n7049.25\n7049.25\n7049.25\n7053.47\n\nObjective Function Evaluations\n36620\n31877\n150,000\n150,000\n150,000\n\nTable C-3 Comparison of results for the pressure vessel design problem (minimization problem). Results from\nother authors were reported in [223]. Results are also reported for [222] however their solution violates integer\nconstraints for the 3rd and 4th parameters making their final solution infeasible. It should also be mentioned that\nequations for defining the problem have errors in [221] and [223]. Previous studies have used different bounds for\nthe solution parameters in this problem which are stated in Column 4. These bounds can change the location of\nthe optimal solution making it hard to compare experimental results from different authors. The best solution\nfound in these experiments was (F, x1, x2, x3, x4) = (5850.37, 38.8601, 221.365, 12, 6) with constraints (g1, g2, g3, g4) =\n(-7.00E-8, -4.27E-3, -0.53, -18.66).\n\nReference\n\nFitness\n\nSandgren, 1990 [235]\nFu et. al., 1991 [241]\nKannan and Kramer, 1994 [242]\nCao and Wu, 1997 [243]\nLin et. al., 1999 [220]\nCoello, 1999 [221]\n\n8129.80\n8084.62\n7198.04\n7108.62\n6370.70\n6288.74\n\nObjective\nFunction\nEvaluations\nnot reported\nnot reported\nnot reported\nnot reported\n50,000\n900,000\n\nZeng et al., 2002 [222]\n\n5804.39\n\nnot reported\n\nLi et al., 2002 [223]\nSOTEA (This Thesis)\ncGA (This Thesis)\nPanmictic EA (This Thesis)\n\n5850.38\n5850.37\n5850.37\n5857.39\n\nnot reported\n150,000\n150,000\n150,000\n\nParameter Bounds\n\nnot reported\nnot reported\nnot reported\nnot reported\nnot reported\n1 < x1 < 99,\n1 < x2 < 99,\n10.0000 < x3 < 200.0000,\n10.0000 < x4 < 200.0000\n0 < x1 < 10,\n0 < x2 < 10,\n0 < x3 < 100,\n0 < x4 < 240\nnot reported\n1 < x1 < 20,\n1 < x2 < 20,\n1 < x3 < 100,\n1 < x4 < 400\n\n249\n\n\fTable C-4 Comparison of results for the welded beam design problem (minimization problem). Results from other\nauthors were reported in [222]. The best solution found in these experiments was (F, x1, x2, x3, x4) = (1.72485,\n0.20572973978, 3.47048651338, 9.0366239103, 0.2057296397) with constraints (g1, g2, g3, g4, g5, g6, g7) = (0, 0, 9.99E-8, 0, 0, 0, 0).\n\nReference\nCoello, 1999 [221]\nZeng et al. 2002 [222]\nSOTEA (This Thesis)\ncGA (This Thesis)\nPanmictic EA (This Thesis)\n\nFitness\n1.74830941\n1.72553637\n1.72485217\n1.72485217\n1.72485218\n\nObjective Function Evaluations\n900,000\nnot reported\n150,000\n150,000\n150,000\n\nTable C-5 Comparison of results for the tension compression spring problem (minimization problem). Results\nfrom other authors were reported in [221]. The best solution found in these experiments was (F, x1, x2, x3) =\n(0.0126652303, 0.051838, 0.360318, 11.081416) with constraints (g1, g2, g3, g4) = (-3.16E-5, 1.47E-5, -4.06, -0.725).\n\nReference\nBelegundu,1982 [244]\nArora, 1989 [245]\nCoello, 1999 [221]\nSOTEA (This Thesis)\ncGA (This Thesis)\nPanmictic EA (This Thesis)\n\nFitness\n0.0128334375\n0.0127302737\n0.0127047834\n0.0126652303\n0.0126652303\n0.0126652593\n\nObjective Function Evaluations\nnot reported\nnot reported\n900,000\n150,000\n150,000\n150,000\n\nTable C-6 Comparison of results for the gear train design problem (minimization problem). Results from other\nauthors were reported in [220]. The best solution found in these experiments was (F, x1, x2, x3, x4) = (2.70 x10-12,\n19, 16, 43, 49).\n\nReference\nFu et. al., [241]\nCao and Wu, 1997 [243]\nDeb and Goyal, 1997 [246]\nLin et al. 1999 [220]\nSOTEA (This Thesis)\ncGA (This Thesis)\nPanmictic EA (This Thesis)\n\nFitness\n4.5 x10-6\n2.36 x10-9\n2.70 x10-12\n2.70 x10-12\n2.70 x10-12\n2.70 x10-12\n2.70 x10-12\n\nObjective Function Evaluations\nnot reported\nnot reported\nnot reported\n50,000\n150,000\n150,000\n150,000\n\n250\n\n\fC.2\n\nPanmictic EA Performance Tables\n\nTable C-7: Final performance results for eight Panmictic Evolutionary Algorithms run for 3000 generations with\nalgorithm designs varying by the use of generational (Gen) or pseudo steady state (SS) population updating, the\nuse of binary tournament selection (Tour) or truncation selection (Trun), and the number of search operators\n(Nops). Performance is presented as the single best objective function value found in 20 runs FBest as well as the\naverage objective function value over 20 runs FAve. None of the Evolutionary Algorithms listed below failed to\nobtain a feasible solution within 3000 generations. The single best fitness values found for each problem are in\nbold.\n\nGen\n\nSel\n\nNops\n\nSS\nSS\nGen\nGen\nSS\nSS\nGen\nGen\n\nTour\nTrun\nTour\nTrun\nTour\nTrun\nTour\nTrun\n\n7\n7\n7\n7\n2\n2\n2\n2\n\nSS\nSS\nGen\nGen\nSS\nSS\nGen\nGen\n\nTour\nTrun\nTour\nTrun\nTour\nTrun\nTour\nTrun\n\n7\n7\n7\n7\n2\n2\n2\n2\n\nSS\nSS\nGen\nGen\nSS\nSS\nGen\nGen\n\nTour\nTrun\nTour\nTrun\nTour\nTrun\nTour\nTrun\n\n7\n7\n7\n7\n2\n2\n2\n2\n\nSS\nSS\nGen\nGen\nSS\nSS\nGen\nGen\n\nTour\nTrun\nTour\nTrun\nTour\nTrun\nTour\nTrun\n\n7\n7\n7\n7\n2\n2\n2\n2\n\nPressure Vessel\nFBest\n6059.70\n6059.73\n5953.06\n5964.23\n5867.87\n5857.39\n6144.69\n6188.86\n\nFAve\n6190.31\n6214.31\n6123.22\n6174.55\n6382.61\n6449.57\n6340.23\n6391.15\n\nGear Train\nFBest\nFAve\n2.70E-12 2.62E-10\n2.70E-12 7.70E-10\n2.70E-12 2.70E-12\n2.70E-12 1.09E-11\n2.70E-12 1.12E-09\n2.31E-11 1.81E-09\n2.70E-12 4.74E-12\n2.70E-12 2.70E-12\nFrequency\nModulation\nFBest\nFAve\n15.36\n0.00\n6.69\n18.28\n23.07\n26.95\n22.87\n25.97\n8.98\n15.87\n0.55\n16.49\n23.35\n26.33\n21.95\n26.77\nRastigrin\nFBest\nFAve\n1.25E-10 1.65E-06\n4.24E-02 1.26E-01\n6.33E-01 9.17E-01\n8.82E-02 1.96E-01\n3.10E-02 6.92E-02\n1.64E-01 2.83E-01\n7.82\n10.51\n4.89\n7.53\n\nHeat Exchanger\nNetwork\nFBest\nFAve\n7109.20\n7053.47\n7056.09\n7179.02\n7116.72\n7213.38\n7186.97\n7250.82\n7070.57\n7233.18\n7093.12\n7269.02\n7235.69\n7412.11\n7184.51\n7398.23\nTension Compression\nSpring\nFBest\nFAve\n0.012665 0.012758\n0.012665 0.012778\n0.012679 0.012710\n0.012687 0.012725\n0.012701 0.013861\n0.012804 0.015078\n0.012739 0.013035\n0.012694 0.012864\n\nAlkylation Process\nFBest\n1771.35\n1760.77\n1711.00\n1641.47\n1756.00\n1748.95\n1621.77\n1501.24\n\nFAve\n1750.38\n1630.90\n1667.34\n1495.13\n1708.38\n1661.17\n1510.93\n1343.48\n\nWelded Beam\nFBest\nFAve\n1.74602\n1.72485\n1.72494\n1.80945\n1.75465\n1.77920\n1.76485\n1.79732\n1.73570\n1.96193\n1.73060\n2.06087\n1.83742\n1.93124\n1.75302\n1.88472\nSystem of Linear\nError Correcting Code\nEquations\nFBest\nFAve\nFBest\nFAve\n3.53E-03 4.32E-03\n2.12E-05\n8.53E-14\n3.68E-03 4.29E-03\n3.16E-05\n1.32\n2.47E-03 3.75E-03\n10.90\n14.58\n3.44E-03 4.13E-03\n2.45\n5.27\n1.67\n3.54\n2.70E-07 3.84E-03\n3.43E-03 3.96E-03\n4.26\n5.90\n4.18E-03 4.77E-03\n50.21\n74.11\n35.69\n51.75\n2.70E-07 3.17E-03\nGriewangk\nWatson\nFBest\nFAve\nFBest\nFAve\n0.052\n0.012\n1.716E-02 2.025E-02\n0.049\n0.158\n1.728E-02 2.922E-02\n0.615\n0.751\n1.778E-02 1.941E-02\n0.348\n0.508\n1.730E-02 1.828E-02\n0.131\n0.216\n1.804E-02 4.887E-02\n0.154\n0.366\n1.829E-02 4.369E-02\n1.476\n2.729\n2.444E-02 5.673E-02\n1.474\n2.199\n2.205E-02 4.111E-02\n\n251\n\n\fPUBLICATIONS\nJournals\nWhitacre, J. M., Sarker, R. A., and Pham, T. Q., \"The Self-Organization of Interaction\nNetworks for Nature-Inspired Optimization.\" IEEE Transactions on Evolutionary\nComputation, (Accepted March, 2007)\nhttp://www.ceic.unsw.edu.au/staff/Tuan_Pham/Whitacre_SOTEA_2007.pdf\nWhitacre, J. M., Sarker, R. A., and Pham, T. Q., \"The Self-Organized Criticality of\nPopulation Dynamics and its Relevance to Adaptive Evolutionary Algorithms.\" IEEE\nTransactions on Evolutionary Computation, (Submitted November, 2006)\nWhitacre, J. M., Sarker, R. A., and Pham, T. Q., \"The influence of population topology\nand historical coupling on Evolutionary Algorithm population dynamics.\" Applied Soft\nComputing, (Submitted September, 2007)\nWhitacre, J. M., Sarker, R. A., and Pham, T. Q., \"A Self-Organizing Topology for\n\ndistributed Evolutionary Algorithms based on fitness-driven community structures.\"\nIEEE Transactions on Evolutionary Computation, (Submitted September, 2007)\n\nConference Proceedings\nWhitacre, J. M., Pham, T. Q., and Sarker, R. A., \"Use of statistical outlier detection method in\nadaptive evolutionary algorithms.\" In Proceedings of the 8th Annual Conference on Genetic\nand Evolutionary Computation (Seattle, Washington, USA, July 08 - 12, 2006). GECCO '06.\nACM Press, New York, NY, 1345-1352, 2006.\nwww.ceic.unsw.edu.au/staff/Tuan_Pham/fp122-whitacre.pdf\nWhitacre, J. M., Pham, T. Q., and Sarker, R. A., \"Credit assignment in adaptive evolutionary\nalgorithms.\" In Proceedings of the 8th Annual Conference on Genetic and Evolutionary\nComputation (Seattle, Washington, USA, July 08 - 12, 2006). GECCO '06. ACM Press, New\nYork, NY, 1353-1360, 2006. www.ceic.unsw.edu.au/staff/Tuan_Pham/fp123-whitacre.pdf\n\n252\n\n\f"}