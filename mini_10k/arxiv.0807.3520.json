{"id": "http://arxiv.org/abs/0807.3520v6", "guidislink": true, "updated": "2010-11-01T17:40:33Z", "updated_parsed": [2010, 11, 1, 17, 40, 33, 0, 305, 0], "published": "2008-07-22T17:05:35Z", "published_parsed": [2008, 7, 22, 17, 5, 35, 1, 204, 0], "title": "Testing the Nullspace Property using Semidefinite Programming", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0807.3045%2C0807.1446%2C0807.0213%2C0807.2496%2C0807.5074%2C0807.1818%2C0807.2401%2C0807.2250%2C0807.0581%2C0807.1510%2C0807.2653%2C0807.0442%2C0807.3645%2C0807.3436%2C0807.1815%2C0807.3568%2C0807.4604%2C0807.3346%2C0807.2922%2C0807.3793%2C0807.0330%2C0807.2754%2C0807.3791%2C0807.4673%2C0807.1855%2C0807.2139%2C0807.0712%2C0807.1199%2C0807.0395%2C0807.2009%2C0807.3965%2C0807.2839%2C0807.0938%2C0807.0503%2C0807.3212%2C0807.1576%2C0807.3091%2C0807.4154%2C0807.0129%2C0807.0340%2C0807.1547%2C0807.3802%2C0807.0286%2C0807.4798%2C0807.4437%2C0807.1415%2C0807.4688%2C0807.4480%2C0807.0033%2C0807.1665%2C0807.2774%2C0807.2066%2C0807.4414%2C0807.3939%2C0807.3111%2C0807.1550%2C0807.1719%2C0807.3217%2C0807.4022%2C0807.2210%2C0807.3173%2C0807.4018%2C0807.0931%2C0807.3512%2C0807.2867%2C0807.1142%2C0807.4235%2C0807.1984%2C0807.1800%2C0807.1539%2C0807.3116%2C0807.1540%2C0807.3872%2C0807.4453%2C0807.4148%2C0807.3986%2C0807.0999%2C0807.3115%2C0807.2101%2C0807.4082%2C0807.1143%2C0807.3384%2C0807.4458%2C0807.2219%2C0807.0483%2C0807.3520%2C0807.1862%2C0807.4354%2C0807.2761%2C0807.0103%2C0807.0163%2C0807.2695%2C0807.1698%2C0807.4287%2C0807.3565%2C0807.3168%2C0807.0110%2C0807.3667%2C0807.3714%2C0807.2038%2C0807.2057&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Testing the Nullspace Property using Semidefinite Programming"}, "summary": "Recent results in compressed sensing show that, under certain conditions, the\nsparsest solution to an underdetermined set of linear equations can be\nrecovered by solving a linear program. These results either rely on computing\nsparse eigenvalues of the design matrix or on properties of its nullspace. So\nfar, no tractable algorithm is known to test these conditions and most current\nresults rely on asymptotic properties of random matrices. Given a matrix A, we\nuse semidefinite relaxation techniques to test the nullspace property on A and\nshow on some numerical examples that these relaxation bounds can prove perfect\nrecovery of sparse solutions with relatively high cardinality.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0807.3045%2C0807.1446%2C0807.0213%2C0807.2496%2C0807.5074%2C0807.1818%2C0807.2401%2C0807.2250%2C0807.0581%2C0807.1510%2C0807.2653%2C0807.0442%2C0807.3645%2C0807.3436%2C0807.1815%2C0807.3568%2C0807.4604%2C0807.3346%2C0807.2922%2C0807.3793%2C0807.0330%2C0807.2754%2C0807.3791%2C0807.4673%2C0807.1855%2C0807.2139%2C0807.0712%2C0807.1199%2C0807.0395%2C0807.2009%2C0807.3965%2C0807.2839%2C0807.0938%2C0807.0503%2C0807.3212%2C0807.1576%2C0807.3091%2C0807.4154%2C0807.0129%2C0807.0340%2C0807.1547%2C0807.3802%2C0807.0286%2C0807.4798%2C0807.4437%2C0807.1415%2C0807.4688%2C0807.4480%2C0807.0033%2C0807.1665%2C0807.2774%2C0807.2066%2C0807.4414%2C0807.3939%2C0807.3111%2C0807.1550%2C0807.1719%2C0807.3217%2C0807.4022%2C0807.2210%2C0807.3173%2C0807.4018%2C0807.0931%2C0807.3512%2C0807.2867%2C0807.1142%2C0807.4235%2C0807.1984%2C0807.1800%2C0807.1539%2C0807.3116%2C0807.1540%2C0807.3872%2C0807.4453%2C0807.4148%2C0807.3986%2C0807.0999%2C0807.3115%2C0807.2101%2C0807.4082%2C0807.1143%2C0807.3384%2C0807.4458%2C0807.2219%2C0807.0483%2C0807.3520%2C0807.1862%2C0807.4354%2C0807.2761%2C0807.0103%2C0807.0163%2C0807.2695%2C0807.1698%2C0807.4287%2C0807.3565%2C0807.3168%2C0807.0110%2C0807.3667%2C0807.3714%2C0807.2038%2C0807.2057&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Recent results in compressed sensing show that, under certain conditions, the\nsparsest solution to an underdetermined set of linear equations can be\nrecovered by solving a linear program. These results either rely on computing\nsparse eigenvalues of the design matrix or on properties of its nullspace. So\nfar, no tractable algorithm is known to test these conditions and most current\nresults rely on asymptotic properties of random matrices. Given a matrix A, we\nuse semidefinite relaxation techniques to test the nullspace property on A and\nshow on some numerical examples that these relaxation bounds can prove perfect\nrecovery of sparse solutions with relatively high cardinality."}, "authors": ["Alexandre d'Aspremont", "Laurent El Ghaoui"], "author_detail": {"name": "Laurent El Ghaoui"}, "author": "Laurent El Ghaoui", "arxiv_comment": "Some typos corrected in Section 4.2", "links": [{"href": "http://arxiv.org/abs/0807.3520v6", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0807.3520v6", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0807.3520v6", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0807.3520v6", "journal_reference": null, "doi": null, "fulltext": "arXiv:0807.3520v6 [math.OC] 1 Nov 2010\n\nTesting the Nullspace Property\nusing Semidefinite Programming\nAlexandre d'Aspremont\u2217\n\nLaurent El Ghaoui\u2020\n\nOctober 24, 2018\n\nAbstract\nRecent results in compressed sensing show that, under certain conditions, the sparsest solution to an underdetermined set of linear equations can be recovered by solving a linear program. These results either rely on computing sparse eigenvalues of the design matrix or on\nproperties of its nullspace. So far, no tractable algorithm is known to test these conditions and\nmost current results rely on asymptotic properties of random matrices. Given a matrix A, we\nuse semidefinite relaxation techniques to test the nullspace property on A and show on some\nnumerical examples that these relaxation bounds can prove perfect recovery of sparse solutions\nwith relatively high cardinality.\n\nKeywords: Compressed sensing, nullspace property, semidefinite programming, restricted\nisometry constant.\n\n1 Introduction\nA recent stream of results in signal processing have focused on producing explicit conditions under\nwhich the sparsest solution to an underdetermined linear system can be found by solving a linear\nprogram. Given a matrix A \u2208 Rm\u00d7n with n > m and a vector v \u2208 Rm , writing kxk0 = Card(x)\nthe number of nonzero coefficients in x, this means that the solution of the following (combinatorial) l0 minimization problem:\nminimize kxk0\n(1)\nsubject to Ax = v,\nin the variable x \u2208 Rn , can be found by solving the (convex) l1 minimization problem:\nminimize kxk1\nsubject to Ax = v,\n\u2217\n\u2020\n\n(2)\n\nIn alphabetical order. ORFE Dept., Princeton University, Princeton, NJ 08544. aspremon@princeton.edu\nEECS Dept., U.C. Berkeley, Berkeley, CA 94720. elghaoui@eecs.berkeley.edu\n\n1\n\n\fin the variable x \u2208 Rn , which is equivalent to a linear program.\nBased on results by Vershik and Sporyshev (1992) and Affentranger and Schneider (1992),\nDonoho and Tanner (2005) show that when the solution x0 of (1) is sparse with Card(x0 ) = k\nand the coefficients of A are i.i.d. Gaussian, then the solution of the l1 problem in (2) will always\nmatch that of the l0 problem in (1) provided k is below an explicitly computable strong recovery\nthreshold kS . They also show that if k is below another (larger) weak recovery threshold kW , then\nthese solutions match with an exponentially small probability of failure.\nUniversal conditions for strong recovery based on sparse extremal eigenvalues were derived\nin Cand\u00e8s and Tao (2005) and Cand\u00e8s and Tao (2006) who also proved that certain (mostly random) matrix classes satisfied these conditions with an exponentially small probability of failure.\nSimpler, weaker conditions which can be traced back to Donoho and Huo (2001), Zhang (2005) or\nCohen et al. (2009) for example, are based on properties of the nullspace of A. In particular, if we\ndefine\n\u03b1k =\nmax\nmax\ny T x,\n{Ax=0, kxk1 =1} {kyk\u221e =1, kyk1 \u2264k}\n\nthese references show that \u03b1k < 1/2 guarantees strong recovery.\nOne key issue with the current sparse recovery conditions in Cand\u00e8s and Tao (2005) or Donoho and Huo\n(2001) is that except for explicit recovery thresholds available for certain types of random matrices,\ntesting these conditions on generic matrices is potentially harder than solving the combinatorial l0 norm minimization problem in (1) for example as it implies either solving a combinatorial problem\nto compute \u03b1k , or computing sparse eigenvalues. Semidefinite relaxation bounds on sparse eigenvalues were used in d'Aspremont et al. (2008) or Lee and Bresler (2008) for example to test the\nrestricted isometry conditions in Cand\u00e8s and Tao (2005) on arbitrary matrices. In recent independent results, Juditsky and Nemirovski (2008) provide an alternative proof of some of the results in\nDonoho and Huo (2001), extend them to the noisy case and produce a linear programming (LP)\nrelaxation bound on \u03b1k with explicit performance bounds.\nIn this paper, we derive a semidefinite relaxation bound on \u03b1k , study its tightness and performance. By randomization, the semidefinite relaxation also produces lower bounds on the objective\nvalue as a natural by-product of the solution. Overall, our bounds are slightly better than LP ones\nnumerically but both relaxations share the same asymptotic performance limits. However, because\nit involves solving a semidefinite program, the complexity of the semidefinite relaxation derived\nhere is significantly higher than that of the LP relaxation.\nThe paper is organized as follows. In Section 2, we briefly recall some key results in Donoho and Huo\n(2001) and Cohen et al. (2009). We derive a semidefinite relaxation bound on \u03b1k in Section 3, and\nstudy its tightness and performance in Section 4. Section 5 describes a first-order algorithm to solve\nthe resulting semidefinite program. Finally, we test the numerical performance of this relaxation\nin Section 6.\nNotation To simplify notation here, for a matrix X \u2208 Rm\u00d7n , we write its columns Xi , kXk1 the\nsum of absolute values of its coefficients (not the l1 norm of its spectrum) and kXk\u221e the largest\ncoefficient magnitude. More classically, kXkF and kXk2 are the Frobenius and spectral norms.\n\n2\n\n\f2 Sparse recovery & the null space property\nGiven a coding matrix A \u2208 Rm\u00d7n with n > m, a sparse signal x0 \u2208 Rn and an information vector\nv \u2208 Rm such that\nv = Ax0 ,\nwe focus on the problem of perfectly recovering the signal x0 from the vector v, assuming the\nsignal x0 is sparse enough. We define the decoder \u22061 (v) as a mapping from Rm \u2192 Rn , with\n\u22061 (v) ,\n\nargmin\n{x\u2208Rn : Ax=v}\n\nkxk1 .\n\n(3)\n\nThis particular decoder is equivalent to a linear program which can be solved efficiently. Suppose\nthat the original signal x0 is sparse, a natural question to ask is then: When does this decoder\nperfectly recover a sparse signal x0 ? Recent results by Cand\u00e8s and Tao (2005), Donoho and Tanner\n(2005) and Cohen et al. (2009) provide a somewhat tight answer. In particular, as in Cohen et al.\n(2009), for a given coding matrix A \u2208 Rm\u00d7n and k > 0, we can quantify the l1 error of a decoder\n\u2206(v) by computing the smallest constant C > 0 such that\nkx \u2212 \u2206(Ax)k1 \u2264 C\u03c3k (x)\nfor all x \u2208 Rn , where\n\n\u03c3k (x) ,\n\nmin\n\n{z\u2208Rn : Card(z)=k}\n\n(4)\n\nkx \u2212 zk1\n\nis the l1 error of the best k-term approximation of the signal x and can simply be computed as the\nl1 norm of the n \u2212 k smallest coefficients of x \u2208 Rn . We now define the nullspace property as in\nDonoho and Huo (2001) or Cohen et al. (2009).\nDefinition 1 A matrix A \u2208 Rm\u00d7n satisfies the null space property in l1 of order k with constant Ck\nif and only if\n(5)\nkzk1 \u2264 Ck kzT c k1\n\nholds for all z \u2208 Rn with Az = 0 and index subsets T \u2282 [1, n] of cardinality Card(T ) \u2264 k, where\nT c is the complement of T in [1, n].\n\nCohen et al. (2009) for example show the following theorem linking the optimal decoding quality\non sparse signals and the nullspace property constant Ck .\nTheorem 2 Given a coding matrix A \u2208 Rm\u00d7n and a sparsity target k > 0. If A has the nullspace\nproperty in (5) of order 2k with constant C/2, then there exists a decoder \u22060 which satisfies (4)\nwith constant C. Conversely, if (4) holds with constant C then A has the nullspace property at the\norder 2k with constant C.\nProof. See (Cohen et al., 2009, Corollary 3.3).\nThis last result means that the existence of an optimal decoder staisfying (4) is equivalent to A\nsatisfying (5). Unfortunately, this optimal decoder \u22060 (v) is defined as\n\u22060 (v) ,\n\nargmin \u03c3k (z)\n{z\u2208Rn : Az=v}\n\n3\n\n\fhence requires solving a combinatorial problem which is potentially intractable. However, using\ntighter restrictions on the nullspace property constant Ck , we get the following result about the\nlinear programming decoder \u22061 (v) in (3).\nTheorem 3 Given a coding matrix A \u2208 Rm\u00d7n and a sparsity target k > 0. If A has the nullspace\nproperty in (5) of order k with constant C < 2, then the linear programming decoder \u22061 (y) in (3)\nsatisfies the error bounds in (4) with constant 2C/(2 \u2212 C) at the order k.\nProof. See steps (4.3) to (4.10) in the proof of (Cohen et al., 2009, Theorem 4.3).\nTo summarize the results above, if there exists a C > 0 such that the coding matrix A satisfies\nthe nullspace property in (5) at the order k then there exists a decoder which perfectly recovers\nsignals x0 with cardinality k/2. If, in addition, we can show that C < 2, then the linear programming based decoder in (3) perfectly recovers signals x0 with cardinality k. In the next section, we\nproduce upper bounds on the constant Ck in (5) using semidefinite relaxation techniques.\n\n3 Semidefinite Relaxation\nGiven A \u2208 Rm\u00d7n and k > 0, we look for a constant Ck \u2265 1 in (5) such that\nkxT k1 \u2264 (Ck \u2212 1)kxT c k1\nfor all vectors x \u2208 Rn with Ax = 0 and index subsets T \u2282 [1, n] with cardinality k. We can\nrewrite this inequality\nkxT k1 \u2264 \u03b1k kxk1\n(6)\n\nwith \u03b1k \u2208 [0, 1). Because \u03b1k = 1 \u2212 1/Ck , if we can show that \u03b1k < 1 then we prove that A\nsatisfies the nullspace property at order k with constant Ck . Furthermore, if we prove \u03b1k < 1/2,\nwe prove the existence of a linear programming based decoder which perfectly recovers signals x0\nwith at most k errors. By homogeneity, the constant \u03b1k can be computed as\n\u03b1k =\n\nmax\n\nmax\n\n{Ax=0, kxk1 =1} {kyk\u221e =1, kyk1 \u2264k}\n\ny T x,\n\n(7)\n\nwhere the equality kxk1 = 1 can, without loss of generality, be replaced by kxk1 \u2264 1. We now\nderive a semidefinite relaxation for problem (7) as follows. After a change of variables\n\u0013\n\u0013 \u0012\n\u0012\nxxT xy T\nX ZT\n,\n=\nyxT yy T\nZ Y\nwe can rewrite (7) as\nmaximize Tr(Z)\nsubject to AXAT = 0, kXk1 \u2264 1,\n2\nkY\n\u0012 1 \u2264 k,T \u0013\n\u0012 k\u221e \u2264T1,\u0013kY k1 \u2264 k , kZk\nX Z\nX Z\n= 1,\n\u0017 0, Rank\nZ Y\nZ Y\n4\n\n(8)\n\n\fin the variables X, Y \u2208 Sn , Z \u2208 Rn\u00d7n , where all norms should be understood componentwise. We\nthen simply drop the rank constraint to form a relaxation of (7) as\nmaximize Tr(Z)\nsubject to AXAT = 0, kXk1 \u2264 1,\n2\nkY\n\u0012 k\u221e \u2264 T1,\u0013kY k1 \u2264 k , kZk1 \u2264 k,\nX Z\n\u0017 0,\nZ Y\n\n(9)\n\nwhich is a semidefinite program in the variables X, Y \u2208 Sn , Z \u2208 Rn\u00d7n . Note that the contraint\nkZk1 \u2264 k is redundant in the rank one problem but not in its relaxation. Because all constraints\nare linear here, dropping the rank constraint is equivalent to computing a Lagrangian (bidual)\nrelaxation of the original problem and adding redundant constraints to the original problem often\ntightens these relaxations. The dual of program (9) can be written\nminimize \u0012\nkU1 k\u221e + k 2 kU2 k\u221e + kU3 k1 + \u0013\nkkU4 k\u221e\nU1 \u2212 AT W A \u2212 21 (I + U4 )\nsubject to\n\u0017 0,\n\u2212 21 (I + U4T )\nU2 + U3\nwhich is a semidefinite program in the variables U1 , U2 , U3 , W \u2208 Sn and U4 \u2208 Rn\u00d7n . For any\nfeasible point of this program, the objective kU1 k\u221e + k 2 kU2 k\u221e + kU3 k1 + kkU4 k\u221e is an upper\nbound on the optimal value of (9), hence on \u03b1k . We can further simplify this program using\nelimination results for LMIs. In fact, (Boyd et al., 1994, \u00a72.6.2) shows that this last problem is\nequivalent to\nminimize \u0012\nkU1 k\u221e + k 2 kU2 k\u221e + kU3 k1 +\u0013kkU4 k\u221e\nU1 \u2212 wAT A \u2212 21 (I + U4 )\n(10)\nsubject to\n\u0017 0,\n1\nT\n\u2212 2 (I + U4 )\nU2 + U3\n\nwhere the variable w is now scalar. In fact, using the same argument, letting P \u2208 Rn\u00d7p be an\northogonal basis of the nullspace of A, i.e. such that AP = 0 with P T P = I, we can rewrite the\nprevious problem as follows\nminimize kU\nk 2 kU2 k\u221e + kU3 k1 + kkU\u0013\n4 k\u221e\n\u0012 1 k\u221e +\nT\nP U1 P\n\u2212 21 P T (I + U4 )\n\u0017 0,\nsubject to\n\u2212 12 (I + U4T )P\nU2 + U3\n\n(11)\n\nwhich is a (smaller) semidefinite program in the variables U1 , U2 , U3 \u2208 Sn and U4 \u2208 Rn\u00d7n . The\ndual of this last problem is then\nmaximize Tr(QT2 P )\nsubject to kP Q1 P T k1 \u2264 1, kP QT2 k1 \u2264 k\nkQ3 k\u221e \u2264 1,\u0013kQ3 k1 \u2264 k 2\n\u0012\nQ1 QT2\n\u0017 0,\nQ2 Q3\n5\n\n(12)\n\n\fwhich is a semidefinite program in the matrix variables Q1 \u2208 Sp , Q2 \u2208 Rp\u00d7n , Q3 \u2208 Sn , whose\nobjective value is equal to that of problem (9).\nNote that adding any number of redundant constraints in the original problem (8) will further\nimprove tightness of the semidefinite relaxation, at the cost of increased complexity. In particular,\nwe can use the fact that when\nkxk1 = 1, kyk\u221e = 1, kyk1 \u2264 k,\nand if we set Y = yy T and Z = yxT , we must have\nn\nX\ni=1\n\nand\n\n|Yij | \u2264 ktj , |Yij | \u2264 tj , 1T t \u2264 k, t \u2264 1,\n\nn\nX\ni=1\n\n|Zij | \u2264 krj , |Zij | \u2264 rj , 1T r \u2264 k,\n\nfor i, j = 1, . . . , n,\n\nfor i, j = 1, . . . , n,\n\nfor r, t \u2208 Rn . This means that we can refine the constraint kZk1 \u2264 k in (9) to solve instead\nmaximize Tr(Z)\nT\nsubject to AXA\n= 0, kXk1 \u2264 1,\nPn\nT\nPni=1 |Yij | \u2264 ktj , |Yij | \u2264 tj , 1 Tt \u2264 k, t \u2264 1,\n\u0013 krj , |Zij | \u2264 rj , 1 r \u2264 1, for i, j = 1, . . . , n,\n\u0012 i=1 |ZijT| \u2264\nX Z\n\u0017 0,\nZ Y\n\n(13)\n\nwhich is a semidefinite program in the variables X, Y \u2208 Sn , Z \u2208 Rn\u00d7n and r, t \u2208 Rn . Adding these\ncolumnwise constraints on Y and Z significantly tightens the relaxation. Any feasible solution to\nthe dual of (13) with objective value less than 1/2 will then be a certificate that \u03b1k < 1/2.\n\n4 Tightness & Limits of Performance\nThe relaxation above naturally produces a covariance matrix as its output and we use randomization\ntechniques as in Goemans and Williamson (1995) to produce primal solutions for problem (7).\nThen, following results by A. Nemirovski (private communication), we bound the performance of\nthe relaxation in (9).\n\n4.1 Randomization\nHere, we show that lower bounds on \u03b1k can be generated as a natural by-product of the relaxation.\nWe use solutions to the semidefinite program in (9) and generate feasible points to (7) by randomization. These can then be used to certify that \u03b1k > 1/2 and prove that a matrix does not satisfy\nthe nullspace property. Suppose that the matrix\n\u0013\n\u0012\nX ZT\n(14)\n\u0393=\nZ Y\n6\n\n\fsolves problem (9), because \u0393 \u0017 0, we can generate Gaussian variables (x, y) \u223c N (0, \u0393). Below,\nwe show that after proper scaling, (x, y) will satisfy the constraints of problem (7) with high\nprobability, and use this result to quantify the quality of these randomized solutions. We begin\nby recalling classical results on the moments of kxk1 and kxk\u221e when x \u223c N (0, X) and bound\ndeviations above their means using concentration inequalities on Lipschitz functions of Gaussian\nvariables.\nLemma 1 Let X \u2208 Sn , x \u223c N (0, X) and \u03b4 > 0, we have\nP\n(\n\np\n\n2/\u03c0 +\n\n\u221a\n\nkxk1\n\n2 log \u03b4)\n\nPn\n\ni=1\n\n(Xii )\n\n1/2\n\n\u22651\n\n!\n\n\u2264\n\n1\n\u03b4\n\n(15)\n\nProof. Let P be the square root of X and ui \u223c N (0, 1) be independent Gaussian variables, we\nhave\nn\nn\nX\nX\nPij uj\nkxk1 =\ni=1\n\nj=1\n\nPn\n\nhence, because each term | j=1 Pij uj | is a Lipschitz continuous function of the variables u with\nP\nP\nconstant ( nj=1 Pij2 )1/2 = (Xii )1/2 , kxk1 is Lipschitz with constant L = ni=1 (Xii )1/2 . Using the\nconcentration inequality by Ibragimov et al. (1976) (see also Massart (2007) for a general discussion) we get for any \u03b2 > 0\n\u0013\n\u0012\n\u0012\n\u0013\nt2\nkxk1\nE[kxk1 ] + t\n\u2264 exp \u2212 2\nP\n\u2265\n\u03b2\n\u03b2\n2L\np\n\u221a\nP\nwith E[kxk1 ] = 2/\u03c0 ni=1 (Xii )1/2 . Picking t = 2 log \u03b4L and \u03b2 = E[kxk1 ] + t yields the\ndesired result.\nWe now recall another classic result on the concentration of kyk\u221e , also based on the fact that\nkyk\u221e is a Lipschitz continuous function of independent Gaussian variables.\nLemma 2 Let Y \u2208 Sn , y \u223c N (0, Y ) and \u03b4 > 0 then\n\u0012\n\u0013\nkyk\u221e\n1\n\u221a\nP \u221a\n\u2265\n1\n\u2264\n\u03b4\n( 2 log 2n + 2 log \u03b4) maxi=1,...,n (Yii )1/2\n\n(16)\n\nProof. (Massart, 2007, Theorem 3.12) shows that kyk\u221e is a Lipschitz function of independent\nGaussian random variables with constant maxi=1,...,n (Yii )1/2 , hence a reasoning similar to that in\nlemma 1 yields the desired result.\nUsing union bounds, the lemmas above show that if we pick 3/\u03b4 < 1 and (x, y) \u223c N (0, \u0393),\nthe scaled sample points\n\u0012\n\u0013\nx\ny\n,\ng(X, \u03b4) h(Y, n, k, \u03b4)\n7\n\n\fwill be feasible in (7) with probability at least 1 \u2212 3/\u03b4 if we set\ng(X, \u03b4) = (\n\np\n\n2/\u03c0 +\n\np\n\n2 log \u03b4)\n\nn\nX\n\n(Xii )1/2\n\n(17)\n\ni=1\n\nand\n(\n\nh(Y, n, k, \u03b4) = max (\n\np\n\np\n(\n2 log 2n + 2 log \u03b4) max (Yii )1/2 ,\ni=1,...,n\n\np\n\n2/\u03c0 +\n\n\u221a\n\n2 log \u03b4)\nk\n\nPn\n\n1/2\ni=1 (Yii )\n\n)\n\n(18)\nThe randomization technique is then guaranteed to produce a feasible point of (7) with objective\nvalue\nq\n{1\u22123/\u03b4}\n\ng(X, \u03b4)h(Y, n, k, \u03b4)\nwhere q{1\u22123/\u03b4} is the 1\u22123/\u03b4 quantile of xT y when (x, y) \u223c N (0, \u0393). We now compute a (relatively\ncoarse) lower bound on the value of that quantile.\nLemma 3 Let \u01eb, \u03b4 > 3 and (x, y) \u223c N (0, \u0393), with \u0393 defined as in (14), then\n!\n\u221a\nn\nX\n3\n3\nP\nxi yi \u2265 Tr(Z) \u2212 \u221a\n\u03c3 \u2265\n\u03b4\n\u03b4\u22123\ni=1\n\n(19)\n\nwhere\n\u03c3 2 = kZk2F + Tr(XY ).\nProof. Let S \u2208 R2n\u00d72n be such that \u0393 = S T S and (x, y) \u223c N (0, \u0393), we have\nh\n\u0002\n\u0003\n\u00012 i Pn\nT\nT\n= i,j=1 E (SiT w)(Sn+i\nw)(SjT w)(Sn+j\nw)\nE yT x\n\nwhere w is a standard normal vector of dimension 2n. Wick's formula implies\n\uf8eb\nXii Zii Xij Zij\n\uf8ec\n\u0002 T\n\u0003\nZii Yii Zij Yij\nT\nT\nE (Si w)(Sn+i\nw)(SjT w)(Sn+j\nw) = Haf \uf8ec\n\uf8ed Xij Zij Xjj Zjj\nZij Yij Zjj Yjj\n= Zii Zjj + Zij2 + Xij Yij ,\n\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n\nwhere Haf (X) is the Hafnian of the matrix X (see Barvinok (2007) for example), which means\n\u0002\n\u0003\nE (y T x)2 = (Tr(Z))2 + kZk2F + Tr(XY ).\n\nBecause E[y T x] = E[Tr(xy T )] = Tr(E[xy T ]) = Tr(Z), we then conclude using Cantelli's\ninequality, which gives\n!\nn\nX\n1\nP\nxi yi \u2264 Tr(Z) \u2212 t\u03c3 \u2264\n1 + t2\ni=1\n8\n\n\fhaving set t =\n\n\u221a \u221a\n3/ \u03b4 \u2212 3.\n\nWe can now combine these results to produce a lower bound on the objective value achieved\nby randomization.\nTheorem 4 Given A \u2208 Rm\u00d7n , \u01eb > 0 and k > 0, writing SDPk the optimal value of (9), we have\nSDPk \u2212 \u01eb\n\u2264 \u03b1k \u2264 SDPk\ng(X, \u03b4)h(Y, n, k, \u03b4)\nwhere\n\n(20)\n\n3(kZk2F + Tr(XY ))\n.\n\u01eb2\nn\nX\np\np\ng(X, \u03b4) = ( 2/\u03c0 + 2 log \u03b4)\n(Xii )1/2\n\u03b4 =3+\n\ni=1\n\nand\n(\n\nh(Y, n, k, \u03b4) = max (\n\np\n\np\n(\n2 log 2n + 2 log \u03b4) max (Yii )1/2 ,\ni=1,...,n\n\np\n\n2/\u03c0 +\n\n\u221a\n\n2 log \u03b4)\nk\n\n1/2\ni=1 (Yii )\n\nPn\n\n)\n\nProof. If \u0393 solves (9) and the vectors (x, y) are sampled according to (x, y) \u223c N (0, \u0393), then\nE[(Ax)(Ax)T ] = E[AxxT AT ] = AXAT = 0,\nmeans that we always have Ax = 0. When \u03b4 > 3, Lemmas 1 and 2 show that\n\u0013\n\u0012\ny\nx\n,\ng(X, \u03b4) h(Y, n, k, \u03b4)\nwill be feasible in (7) with probability at least 1 \u2212 3/\u03b4, hence we can get a feasible point for (7)\nby sampling enough variables (x, y). Lemma 3 shows that if we set \u03b4 as above, the randomization\nprocedure is guaranteed to reach an objective value y T x at least equal to\nTr(Z) \u2212 \u01eb\ng(X, \u03b4)h(Y, n, k, \u03b4)\nwhich is the desired result.\nNote that because \u0393 \u0017 0, we have Zij2 \u2264 Xii Yjj , hence kZk2F \u2264 Tr(X) Tr(Y ) \u2264 k 2 . We also\nhave Tr(XY ) \u2264 kXk1 kY k1 \u2264 k 2 hence\n\u03b4 \u22643+\n\n9\n\n6k 2\n.\n\u01eb2\n\n\fP\nP\nand the only a priori unknown terms controlling tightness are ni=1 (Xii )1/2 , ni=1 (Yii )1/2 and\nmaxi=1,...,n (Yii )1/2 . Unfortunately, while the third term is bounded by one, the first two can become\nquite large, with trivial bounds giving\nn\nX\n\n(Xii )\n\n1/2\n\ni=1\n\n\u2264\n\n\u221a\n\nn and\n\nn\nX\ni=1\n\n(Yii )1/2 \u2264\n\n\u221a\n\nn,\n\nwhich means that, in the worst case, our lower bound will be off by a factor 1/n. However, we\nwill observe in Section 6 that, when k = 1, these terms are sometimes much lower than what the\nworst-case bounds seem to indicate. The expression for the tightness coefficient \u03b3 in (14) also\nhighlights the importance of the constraint kZk1 \u2264 k. Indeed, the positive semidefinitess of 2 \u00d7 2\nprincipal submatrices means that Zij2 \u2264 Xii Yjj , hence\n!\n! n\nn\nX\nX\n(Yii )1/2 ,\nkZk1 \u2264\n(Xii )1/2\ni=1\n\ni=1\n\nso controlling kZk1 potentially tightens the relaxation. This is confirmed in numerical experiments:\nthe relaxation including the (initially) redundant norm constraint on Z is significantly tighter on\nmost examples. Finally, note that better lower bounds on \u03b1k can be obtained (numerically) by\nsampling kxT k1 /kxk1 in (6) directly, or as suggested by one of the referees, solving\nmaximize cT x\nsubject to Ax = 0, kxk1 \u2264 1,\nin x \u2208 Rn for various random vectors c \u2208 {\u22121, 0, 1}n with at most k nonzero coefficients. In both\ncases unfortunately, the moments cannot be computed explicitly so studying performance is much\nharder.\n\n4.2 Performance\nFollowing results by A. Nemirovski (private communication), we can derive precise bounds on the\nperformance of the relaxation in (9).\nLemma 4 Suppose (X, Y, Z) solve the semidefinite program in (9), then\nTr(Z) = \u03b11\nand the relaxation is tight for k = 1.\nProof. First, notice that when the matrices (X, Y, Z) solve (9), AX = 0 with\n\u0013\n\u0012\nX ZT\n\u00170\nZ Y\nmeans that the rows of Z also belong\nP to the nullspace of A. If A satisfies the nullspace property\nin (6), we must have |Zii | \u2264 \u03b11 nj=1 |Zij | for i = 1, . . . , n, hence Tr(Z) \u2264 \u03b11 kZk1 \u2264 \u03b11 . By\nconstruction, we always have Tr(Z) \u2265 \u03b11 hence Tr(Z) = \u03b11 when Z solves (9) with k = 1.\n10\n\n\fAs in Juditsky and Nemirovski (2008), this also means that if a matrix A satisfies the restricted\nisometry property at cardinality O(m) (as\n\u221a Gaussian matrices do for example), then the relaxation\nin (9) will certify \u03b1k < 1/2 for k = O( m). Unfortunately, the results that follow show that this\nis the best we can hope for here.\nWithout loss of generality, we can assume that n = 2m (if n \u2265 2m, the problem is harder). Let\nQ be an orthoprojector on a (n\u2212m)-dimensional subspace\n\u221a of the nullspace of A, with Rank(Q) =\nn \u2212 m = m. By construction, kQk1 \u2264 nkQk2 = n m, 0 \u0016 Q \u0016 I and of course\n\u221a AQ = 0.\nWe can use\u221athis matrix to \u221a\nconstruct a feasible solution\n\u221a to problem (13) when k = n. We set\nX = Q/(n m), Y = Q/ n, Z = Q/n, tj = 1/ n and rj = 1/n for j = 1, . . . , n. We then\nhave\nkQi k1\nkYi k1 = \u221a \u2264 kQi k2 \u2264 1 \u2264 kti , i = 1, . . . , n,\nn\n\u221a\nand kYi k\u221e \u2264 kYi k2 \u2264 1/ n with 1T t \u2264 k. We also get\nkZi k1 =\nWith\n\nkQi k2\nkQi k1\n\u2264 \u221a \u2264 kri ,\nn\nn\n\u0012\n\nn\u22121 m\u22121/2 n\u22121\nn\u22121\nn\u22121/2\n\n\u0013\n\ni = 1, . . . , n.\n\n\u0017 0,\n\nthe matrices we have defined above form a feasible point of problem (13). Because, Tr(Z) =\nTr(Q)/n = 1/2, this\n\u221a feasible point proves that the optimal value of (13) is larger than 1/2 when\nn = 2m and k = n. This means that the relaxation\n\u221a in (13) can prove that a matrix satisfies the\nnullspace property for cardinalities at most k \u221a\n= O( n) and this performance bound is tight since\nwe have shown that it achieves this rate of O( n) for good matrices.\nThis counter example also produces bounds on the performance of another relaxation for testing\nsparse recovery. In fact, if we set X = Q/m with Q defined as above, we have Tr(X) = 1 with\nX \u0017 0 and\n\u221a\nkQk1\n\u22642 m\nkXk1 =\nm\nand X is an optimal solution of the problem\nminimize Tr(XAAT\u221a\n)\nsubject to kXk1 \u2264 2 2m\nTr(X) = 1, X \u0017 0,\nwhich is a semidefinite relaxation used in d'Aspremont et al. (2007) and d'Aspremont et al. (2008)\nto bound the restricted isometry constant \u03b4k (A). Because Tr(XAAT ) = 0 by\u221aconstruction, we\nknow that this last relaxation will fail to show \u03b4k (A) < 1 whenever k = O( m). Somewhat\nstrikingly, this means that the three different tractable tests for sparse recovery conditions, derived\nin d'Aspremont et al. (2008), Juditsky and Nemirovski\n(2008) and this paper, are all limited to\n\u221a\nshowing recovery at the (suboptimal) rate k = O( m).\n\n11\n\n\f5 Algorithms\nSmall instances of the semidefinite program in (11) and be solved efficiently using solvers such as\nSEDUMI (Sturm, 1999) or SDPT3 (Toh et al., 1999). For larger instances, it is more advantageous\nto solve (11) using first order techniques, given a fixed target for \u03b1. We set P \u2208 Rn\u00d7p to be an\northogonal basis of the nullspace of the matrix A in (6), i.e. such that AP = 0 with P T P = I.\nWe also let \u1fb1 be a target critical value for \u03b1 (such as 1/2 for example), and solve the following\nproblem\n\u0013\n\u0012\nP T U1 P\n\u2212 12 P T (I + U4 )\nmaximize \u03bbmin\n\u2212 21 (I + U4T )P\nU2 + U3\n(21)\n2\nsubject to kU1 k\u221e + k kU2 k\u221e + kU3 k1 + kkU4 k\u221e \u2264 \u1fb1\nin the variables U1 , U2 , U3 \u2208 Sn and U4 \u2208 Rn\u00d7n . If the objective value of this last problem is\ngreater than zero, then the optimal value of problem (11) is necessarily smaller than \u1fb1, hence\n\u03b1 \u2264 \u1fb1 in (7).\nBecause this problem is a minimum eigenvalue maximization problem over a simple compact\n(a norm ball in fact), large-scale instances can be solved efficiently using projected gradient algorithms or smooth semidefinite optimization techniques (Nesterov, 2007; d'Aspremont et al., 2007).\nAs we show below, the complexity of projecting on this ball is quite low.\nLemma 5 The complexity of projecting (x0 , y0 , z0 , w0 ) \u2208 R3n on\nkxk\u221e + k 2 kyk\u221e + kzk1 + kkwk\u221e \u2264 \u03b1\nis bounded by O(n log n log2 (1/\u01eb)), where \u01eb is the target precision in projecting.\nProof. By duality, solving\nminimize kx \u2212 x0 k2 + ky \u2212 y0 k2 + kz \u2212 z0 k2 + kw \u2212 w0 k2\nsubject to kxk\u221e + k 2 kyk\u221e + kzk1 + kkwk\u221e \u2264 \u03b1\nin the variables x, y, z \u2208 Rn is equivalent to solving\nmax min k(x, y, z, w) \u2212 (x0 , y0, z0 , w0 )k2 + \u03bbkxk\u221e + \u03bbk 2 kyk\u221e + \u03bbkzk1 + \u03bbkkwk\u221e \u2212 \u03bb\u03b1\n\u03bb\u22650 x,y,z,w\n\nin the variable \u03bb \u2265 0. For a fixed \u03bb, we can get the derivative w.r.t. \u03bb by solving four separate\npenalized least-squares problems. Each of these problems can be solved explicitly in at most\nO(n log n) (by shrinking the current point) so the complexity of solving the outer maximization\nproblem up to a precision \u01eb > 0 by binary search is O(n log n log2 (1/\u01eb))\nWe can then implement the smooth minimization algorithm detailed in (Nesterov, 2005, \u00a75.3)\nto a smooth approximation of problem (21) as in Nesterov (2007) or d'Aspremont et al. (2007) for\nexample. Let \u03bc > 0 be a regularization parameter. The function\n\u0012\n\u0012 \u0013\u0013\nX\n(22)\nf\u03bc (X) = \u03bc log Tr exp\n\u03bc\n12\n\n\fsatifies\n\u03bbmax (X) \u2264 f\u03bc (X) \u2264 \u03bbmax (X) + \u03bc log n\n\nfor any X \u2208 Sn . Furthermore, f\u03bc (X) is a smooth approximation of the function \u03bbmax (X), and\n\u2207f\u03bc (X) is Lipschitz continuous with constant log n/\u03bc. Let \u01eb > 0 be a given target precision, this\nmeans that if we set \u03bc = \u01eb/(2 log n) then\n\u0013\n\u0012\n1 T\nP (I + U4 )\n\u2212P T U1 P\n2\nwhere U = (U1 , U2 , U3 , U4 ),\n(23)\nf (U) \u2261 \u2212f\u03bc 1\n(I + U4T )P \u2212(U2 + U3 )\n2\nwill be an \u01eb/2 approximation of the objective function in (21). Whenever kUkF \u2264 1, we must\nhave\n\u0012\n\u0013 2\n\u2212P T U1 P\nP T U4 /2\n\u2264 kP T U1 P k22 + kU2 + U3 k22 + kP T U4 k22 \u2264 4,\nU4T P/2 \u2212(U2 + U3 )\n2\nhence, following (Nesterov, 2007, \u00a74), the gradient of f (U) is Lipschitz continuous with respect\nto the Frobenius norm, with Lipschitz constant given by\nL=\n\n8 log(n + p)\n,\n\u01eb\n\nWe then define the compact, convex set Q as\n\b\nQ \u2261 (U1 , U2 , U3 , U4 ) \u2208 S3n : kU1 k\u221e + k 2 kU2 k\u221e + kU3 k1 + kkU4 k\u221e \u2264 \u1fb1 ,\n\nand define a prox function d(U) over Q as d(U) = kUk2F /2, which is strongly convex with constant\n\u03c3 = 1 w.r.t. the Frobenius norm. Starting from U0 = 0, the algorithm in Nesterov (2005) for\nsolving\nmaximize f (U)\nsubject to U \u2208 Q,\nwhere f (U) is defined in (23), proceeds as follows.\nRepeat:\n1. Compute f (Uj ) and \u2207f (Uj )\n\n2. Find Yj = arg minY \u2208Q h\u2207f (Uj ), Y i + 21 LkUi \u2212 Y k2F\nn\no\nPi j+1\nLd(W )\n3. Find Wj = arg minW \u2208Q\n+ j=0 2 (f (Uj ) + h\u2207f (Uj ), W \u2212 Uj i)\n\u03c3\n\n4. Set Uj+1 =\n\n2\nWj\nj+3\n\n+\n\nj+1\nY\nj+3 j\n\nUntil gap \u2264 \u01eb.\nStep one above computes the (smooth) function value and gradient. The second step computes\nthe gradient mapping, which matches the gradient step for unconstrained problems (see (Nesterov,\n2003, p.86)). Step three and four update an estimate sequence see (Nesterov, 2003, p.72) of f\nwhose minimum can be computed explicitly and gives an increasingly tight upper bound on the\nminimum of f . We now present these steps in detail for our problem.\n13\n\n\fStep 1 The most expensive step in the algorithm is the first, the computation of f and its gradient.\nThis amounts to computing the matrix exponential in (22) at a cost of O(n3 ) (see Moler and Van Loan\n(2003) for details).\nStep 2\n\nThis step involves solving a problem of the form\n1\nargmin h\u2207f (U), Y i + LkU \u2212 Y k2F ,\n2\nY \u2208Q\n\nwhere U is given. The above problem can be reduced to an Euclidean projection on Q\nargmin kY \u2212 V kF ,\n\n(24)\n\nkY k\u2208Q\n\nwhere V = U+L\u22121 \u2207f\u03bc (U) is given. According to Lemma 5, this can be solved O(n log n log2 (1/\u01eb))\nopearations.\nStep 3 The third step involves solving an Euclidean projection problem similar to (24), with V\ndefined here by:\ni\n\u03c3 Xj+1\n\u2207f\u03bc (Uj ).\nV =\nL j=0 2\nStopping criterion We stop the algorithm when the duality gap is smaller than the target precision \u01eb. The dual of the binary optimization problem (21) can be written\nminimize \u1fb1 max{kP G11 P T k1 , kGk222 k1 , kG22 k\u221e , kP Gk12 k1 } \u2212 Tr(P G12 )\nsubject to Tr(G) = 1, G \u0017 0,\n\n(25)\n\nin the block matrix variable G \u2208 Sn+p with blocks Gij , i, j = 1, 2. Since the gradient \u2207f (U)\nproduces a dual feasible point by construction, we can use it to compute a dual objective value and\nbound the duality gap at the current point U.\nComplexity According to Nesterov (2007), the total worst-case complexity to solve (21) with\nabsolute accuracy less than \u01eb is then given by\n\u0013\n\u0012 4\u221a\nn log n\nO\n\u01eb\n3\nEach iteration of the algorithm\n\u221a requires computing a matrix exponential at a cost of O(n ) and\nthe algorithm requires O(n log n/\u01eb) iterations to reach a target precision of \u01eb > 0. Note that\nwhile this smooth optimization method can be used to produce reasonable complexity bounds for\nchecking if the optimal value of (21) is positive, i.e. if \u03b1k \u2264 \u1fb1, in practice the algorithm is relatively\nslow and we mostly use interior point solvers on smaller problems to conduct experiments in the\nnext section.\n\n14\n\n\f6 Numerical Results\nIn this section, we illustrate the numerical performance of the semidefinite relaxation detailed in\nsection 3.\n\n6.1 Illustration\nWe test the semidefinite relaxation in (11) on a sample of ten random Gaussian matrices A \u2208 Rp\u00d7n\n\u221a\nwith Aij \u223c N (0, 1/ p), n = 30 and p = 22. For each of these matrices, we solve problem (11)\nfor k = 2, . . . , 5 to produce upper bounds on \u03b1k , hence on Ck in (5), with \u03b1k = 1 \u2212 1/Ck . From\nDonoho and Huo (2001), we know that if \u03b1k < 1 then we can bound the decoding error in (4), and\nif \u03b1k < 1/2 then the original signal can be recovered exactly by solving a linear program. We also\nplot the randomized values for y T x with k = 1 together with the semidefinite relaxation bound.\n3000\n\nrag replacements\n\n0.8\n\nNumber of samples\n\nBounds on \u03b1k\n\n1\n\nl0 recovery\n\n0.6\n\n0.4\n\nl1 recovery\nPSfrag replacements\n\n0.2\n\n0\n1\n\n2\n\n3\n\n4\n\n2500\n2000\n1500\n1000\n500\n0\n0\n\n5\n\nSDP\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n\u03b11\n\nCardinality\n\nFigure 1: Bounds on \u03b1k . Left: Upper bounds on \u03b1k obtained by solving (11) for various\nvalues of k. Median bound over ten samples (solid line), dotted lines at pointwise minimum\nand maximum. Right: Lower bound on \u03b11 obtained by randomization (red dotted line)\ncompared with semidefinite relaxation bound (SDP dashed line).\n\n\u221a\nNext, in Figure 2, we use a Gaussian matrix A \u2208 Rp\u00d7n with Aij \u223c N (0, 1/ p), n = 36\nand p = 27 and, for each k, we sample fifty information vectors v = Ax0 where x0 is uniformly\ndistributed and has cardinality k. On the left, we plot the probability of recovering the original\nsparse signal x0 using the linear programming decoder in (3). On the right, we plot the mean l1\nrecovery error kx \u2212 x0 k1 using the linear programming decoder in (3) and compare it with the\nbound induced by Theorem 3.\n\n15\n\n\f10\n\n0.8\n\n10\n\n2\n\nMean l1 error\n\nProbability of recovery\n\nrag replacements\n\n3\n\n1\n\n0.6\n\n0.4\n\n1\n\n10\n\n0\n\n10\n\n\u22121\n\n0.2\n\n10\n\nPSfrag replacements\n0\n0\n\n10\n\n20\n\n30\n\n\u22122\n\n10\n\n40\n\n0\n\n5\n\nCardinality\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\nCardinality\n\nFigure 2: Sparse Recovery. Left: Empirical probability of recovering the original sparse\nsignal using the LP decoder in (3). The dashed line is at the strong recovery threshold.\nRight: Empirical mean l1 recovery error kx\u2212x0 k1 using the LP decoder (circles) compared\nwith the bound induced by Theorem 3 (squares).\n\n6.2 Performance on compressed sensing matrices\nIn tables 1, 2 and 3, we compare the performance of the linear programming relaxation bound\non \u03b1k derived in Juditsky and Nemirovski (2008) with that of the semidefinite programming bound\ndetailed in Section 3. We test these bounds for various matrix shape ratios \u03c1 = m/n, target cardinalities k on matrices with Fourier, Bernoulli or Gaussian coefficients using SDPT3 by Toh et al.\n(1999) to solve problem (11). We show median bounds computed over ten sample matrices for\neach type, hence test a total of 600 different matrices. We compare these relaxation bounds with the\nupper bounds produced by sequential convex optimization as in Juditsky and Nemirovski (2008,\n\u00a74.1). In the Gaussian case, we also compare these relaxation bounds with the asymptotic thresholds on strong and weak (high probability) recovery discussed in Donoho and Tanner (2008). The\nsemidefinite bounds on \u03b1k always match with the LP bounds in Juditsky and Nemirovski (2008)\nwhen k = 1 (both are tight), and are often smaller than LP bounds whenever k is greater than 1 on\nGaussian or Bernoulli matrices. The semidefinite upper bound on \u03b1k was smaller than the LP one\nin 563 out of the 600 matrices sampled here, with the difference ranging from 4e-2 to -9e-4. Of\ncourse, this semidefinite relaxation is significantly more expensive than the LP based one and that\nthese experiments thus had to be performed on very small matrices.\n\n6.3 Tightness\nSection 4 shows that the tightness of the semidefinite relaxation is explicitly controlled by the\nfollowing quantity\n\u03bc = g(X, \u03b4)h(Y, n, k, \u03b4),\n16\n\n\fRelaxation\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\n\n\u03c1\n0.5\n0.5\n0.5\n0.6\n0.6\n0.6\n0.7\n0.7\n0.7\n0.8\n0.8\n0.8\n\n\u03b11\n0.21\n0.21\n0.05\n0.16\n0.16\n0.04\n0.12\n0.12\n0.04\n0.10\n0.10\n0.04\n\n\u03b12\n0.38\n0.38\n0.10\n0.31\n0.31\n0.09\n0.25\n0.25\n0.09\n0.20\n0.20\n0.07\n\n\u03b13\n0.57\n0.57\n0.16\n0.46\n0.46\n0.15\n0.39\n0.39\n0.14\n0.30\n0.30\n0.13\n\n\u03b14\n0.82\n0.82\n0.24\n0.61\n0.61\n0.20\n0.50\n0.50\n0.18\n0.38\n0.38\n0.17\n\n\u03b15\n0.98\n0.98\n0.32\n0.82\n0.82\n0.31\n0.62\n0.62\n0.22\n0.48\n0.48\n0.23\n\nUpper bound\n2\n2\n2\n3\n3\n3\n4\n4\n4\n6\n6\n6\n\nTable 1: Given ten sample Fourier matrices of leading dimension n = 40, we list median\nupper bounds on the values of \u03b1k for various cardinalities k and matrix shape ratios \u03c1,\ncomputed using the linear programming (LP) relaxation in Juditsky and Nemirovski (2008)\nand the semidefinite relaxation (SDP) detailed in this paper. We also list the upper bound on\nstrong recovery computed using sequential convex optimization and the lower bound on \u03b1k\nobtained by randomization using the SDP solution (SDP low.). Values of \u03b1k below 1/2, for\nwhich strong recovery is certified, are highlighted in bold.\n\nwhere g and h are defined in (17) and (18) respectively. In Figure 3, we plot the histogram of values\nof \u03bc for all 600 sample matrices computed above, and plot the same histogram on a subset of these\nresults where the target cardinality k was set to 1. We observe that while the relaxation performed\nquite well on most of these examples, the randomization bound on performance often gets very\nlarge whenever k > 1. This can probably be explained by the fact that we only control the mean in\nLemma 3, not the quantile. We also notice that \u03bc is highly concentrated when k = 1 on Gaussian\nand Bernoulli matrices (where the results in Tables 2 and 3 are tight), while the performance is\nmarkedly worse for Fourier matrices.\nFinally, Tables 2 and 3 show that lower bounds on \u03b11 obtained by randomization for Gaussian\nare always tight (the solution of the SDP was very close to rank one), while performance on higher\nvalues of k and Fourier matrices is much worse. On 6 of these experiments however, the SDP\nrandomization lower bound was higher than 1/2, which proved that \u03b15 > 1/2, hence that the\nmatrix did not satisfy the nullspace property at order 5.\n\n6.4 Numerical complexity\nWe implemented the algorithm of Section (5) in MATLAB and tested it on random matrices.\nWhile the code handles matrices with n = 500, it is still considerably slower than similar firstorder algorithms applied to sparse PCA problems for example (see d'Aspremont et al. (2007)). A\npossible explanation for this gap in performance is perhaps that the DSPCA semidefinite relaxation\n17\n\n\fRelaxation\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\n\n\u03c1\n0.5\n0.5\n0.5\n0.6\n0.6\n0.6\n0.7\n0.7\n0.7\n0.8\n0.8\n0.8\n\n\u03b11\n0.27\n0.27\n0.27\n0.22\n0.22\n0.22\n0.20\n0.20\n0.20\n0.15\n0.15\n0.15\n\n\u03b12\n0.49\n0.49\n0.31\n0.41\n0.41\n0.29\n0.34\n0.34\n0.27\n0.26\n0.26\n0.23\n\n\u03b13\n0.67\n0.65\n0.33\n0.57\n0.56\n0.31\n0.47\n0.46\n0.31\n0.37\n0.37\n0.28\n\n\u03b14\n0.83\n0.81\n0.32\n0.72\n0.70\n0.32\n0.60\n0.59\n0.35\n0.48\n0.48\n0.33\n\n\u03b15\n0.97\n0.94\n0.35\n0.84\n0.82\n0.36\n0.71\n0.70\n0.38\n0.58\n0.58\n0.38\n\nStrong k\n2\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n\nWeak k\n11\n11\n11\n12\n12\n12\n14\n14\n14\n16\n16\n16\n\nTable 2: Given ten sample Gaussian matrices of leading dimension n = 40, we list median\nupper bounds on the values of \u03b1k for various cardinalities k and matrix shape ratios \u03c1,\ncomputed using the linear programming (LP) relaxation in Juditsky and Nemirovski (2008)\nand the semidefinite relaxation (SDP) detailed in this paper. We also list the asymptotic\nupper bound on both strong and weak recovery computed in Donoho and Tanner (2008)\nand the lower bound on \u03b1k obtained by randomization using the SDP solution (SDP low.).\nValues of \u03b1k below 1/2, for which strong recovery is certified, are highlighted in bold.\nRelaxation\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\nLP\nSDP\nSDP low.\n\n\u03c1\n0.5\n0.5\n0.5\n0.6\n0.6\n0.6\n0.7\n0.7\n0.7\n0.8\n0.8\n0.8\n\n\u03b11\n0.25\n0.25\n0.25\n0.21\n0.21\n0.21\n0.17\n0.17\n0.17\n0.14\n0.14\n0.14\n\n\u03b12\n0.45\n0.45\n0.28\n0.38\n0.38\n0.26\n0.32\n0.32\n0.24\n0.26\n0.26\n0.21\n\n\u03b13\n0.64\n0.63\n0.29\n0.55\n0.54\n0.29\n0.46\n0.46\n0.29\n0.38\n0.37\n0.27\n\n\u03b14\n0.82\n0.80\n0.29\n0.69\n0.68\n0.33\n0.58\n0.58\n0.33\n0.47\n0.47\n0.33\n\n\u03b15\n0.97\n0.94\n0.34\n0.83\n0.81\n0.34\n0.70\n0.69\n0.37\n0.57\n0.57\n0.38\n\nUpper bound\n2\n2\n2\n3\n3\n3\n4\n4\n4\n5\n5\n5\n\nTable 3: Given ten sample Bernoulli matrices of leading dimension n = 40, we list median\nupper bounds on the values of \u03b1k for various cardinalities k and matrix shape ratios \u03c1,\ncomputed using the linear programming (LP) relaxation in Juditsky and Nemirovski (2008)\nand the semidefinite relaxation (SDP) detailed in this paper. We also list the upper bound on\nstrong recovery computed using sequential convex optimization and the lower bound on \u03b1k\nobtained by randomization using the SDP solution (SDP low.). Values of \u03b1k below 1/2, for\nwhich strong recovery is certified, are highlighted in bold.\n\n18\n\n\f45\n70\n\nrag replacements\n\nNumber of samples\n\nNumber of samples\n\n40\n35\n30\n25\n20\n15\n10\n\n60\n50\n40\n30\n20\n10\n\n5\n0\n0\n\nPSfrag replacements\n10\n\n20\n\n30\n\n\u03bc\n\n40\n\n50\n\n60\n\n70\n\n0\n\n20\n\n40\n\n60\n\n80\n\n\u03bc\n\nFigure 3: Tightness. Left: Histogram of \u03bc = g(X, \u03b4)h(Y, n, k, \u03b4) defined in (17) and (18),\ncomputed for all sample solution matrices in the experiments above when k > 1. Right:\nIdem using only examples where the target cardinality is k = 1, for Gaussian and Bernoulli\nmatrices (light grey) or Fourier matrices (dark grey).\n\nn\n50\nCPU time 00 h 01 m\n\n100\n00 h 10 m\n\n200\n500\n01 h 38 m 37 h 22 m\n\nTable 4: CPU time to show \u03b11 < 1/2, using the algorithm of Section 5 on Gaussian\nmatrices with shape ratio \u03c1 = .7 for various values of n.\n\nis always tight (in practice at least) hence iterates near the solution tend to be very close to rank one.\nThis is not the case here as the matrix in (9) is very rarely rank one and the number of significant\neigenvalues has a direct impact on actual convergence speed. To illustrate this point, Figure 4\nshows a Scree plot of the optimal solution to (9) for a small Gaussian matrix (obtained by IP\nmethods with a target precision of 10\u22128 ), while Table 4 shows, as a benchmark, total CPU time for\nproving that \u03b11 < 1/2 on Gaussian matrices, for various values of n. We set the accuracy 1e \u2212 2\nand stop the code whenever positive objective values are reached. Unfortunately, performance\nfor larger values of k is typically much worse (which is why we used IP methods to run most\nexperiments in this section) and in many cases, convergence is hard to track as the dual objective\nvalues computed using the gradient in (25) produces a relatively coarse gap bounds as illustrated\nin Figure 4 for a small Gaussian matrix.\n\n19\n\n\f0\n\n3\n\n10\n\n2.5\n\n10\n\n\u22122\n\n2\n\n\u22124\n\n10\n\n\u03b1k\n\n1.5\n\u22126\n\n10\n1\n\n\u22128\n\n10\n0.5\n\n\u221210\n\n10\n\n0\n\nrag replacements\n\n\u22120.5\n0\n\n1000\n\n2000\n\nPSfrag\nreplacements\n4000\n5000\n\n3000\n\n\u221212\n\n10\n\n0\n\n5\n\n10\n\n15\n\n20\n\nEigenvalues\n\nIterations\n\nFigure 4: Complexity. Left: Primal and dual bounds on the optimal solution (computed\nusing interior point methods) using the algorithm of Section 5 on a small Gaussian matrix.\nRight: Scree plot of the optimal solution to (9) for a small Gaussian matrix (obtained by\ninterior point methods with a target precision of 10\u22128 ).\n\n7 Conclusion & Directions for Further Research\nWe have detailed a semidefinite relaxation for the problem of testing if a matrix satisfies the\nnullspace property defined in Donoho and Huo (2001) or Cohen et al. (2009). This relaxation is\ntight for k = 1 and matches (numerically) the linear programming relaxation in Juditsky and Nemirovski\n(2008). It is often slightly tighter (again numerically) for larger values of k. We can also remark\nthat the matrix A only appears in the relaxation (10) in \"kernel\" format AT A, where the constraints\nare linear in the kernel matrix AT A. This means that this relaxation might allow sparse experiment\ndesign problems to be solved, while maintaining convexity.\nOf course, these small scale experiments do not really shed light on the actual performance\nof both relaxations on larger, more realistic problems. In particular, applications in imaging and\nsignal processing would require solving problems where both n and k are several orders of magnitude larger than the values considered in this paper or in Juditsky and Nemirovski (2008) and\nthe question of finding tractable relaxations or algorithms that can handle such problem sizes remains open. Finally, the three different tractable tests for sparse recovery conditions, derived in\nd'Aspremont et al. (2008), Juditsky and Nemirovski\n(2008) and this paper, are all limited to show\u221a\ning recovery at the (suboptimal) rate k = O( m). Finding tractable test for sparse recovery at\ncardinalities k closer to the optimal rate O(m) also remains an open problem.\n\n20\n\n\fAcknowledgements\nThe authors are grateful to Arkadi Nemirovski (who suggested in particular the columnwise redundant constraints in (13) and the performance bounds) and Anatoli Juditsky for very helpful\ncomments and suggestions. Would like to thank Ingrid Daubechies for first attracting our attention\nto the nullspace property. We are also grateful to two anonymous referees for numerous comments and suggestions. We thank Jared Tanner for forwarding us his numerical results. Finally,\nwe acknowledge support from NSF grant DMS-0625352, NSF CDI grant SES-0835550, a NSF\nCAREER award, a Peek junior faculty fellowship and a Howard B. Wentz Jr. junior faculty award.\n\nReferences\nF. Affentranger and R. Schneider. Random projections of regular simplices. Discrete and Computational\nGeometry, 7(1):219\u2013226, 1992.\nA. Barvinok. Integration and optimization of multivariate polynomials by restriction onto a random subspace. Foundations of Computational Mathematics, 7(2):229\u2013244, 2007.\nStephen Boyd, Laurent El Ghaoui, E. Feron, and V. Balakrishnan. Linear Matrix Inequalities in System and\nControl Theory. SIAM, 1994.\nE. J. Cand\u00e8s and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51\n(12):4203\u20134215, 2005.\nE.J. Cand\u00e8s and T. Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406\u20135425, 2006.\nA. Cohen, W. Dahmen, and R. DeVore. Compressed sensing and best k-term approximation. Journal of the\nAMS, 22(1):211\u2013231, 2009.\nA. d'Aspremont, L. El Ghaoui, M.I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA\nusing semidefinite programming. SIAM Review, 49(3):434\u2013448, 2007.\nA. d'Aspremont, F. Bach, and L. El Ghaoui. Optimal solutions for sparse principal component analysis.\nJournal of Machine Learning Research, 9:1269\u20131294, 2008.\nD. L. Donoho and J. Tanner. Sparse nonnegative solutions of underdetermined linear equations by linear\nprogramming. Proc. of the National Academy of Sciences, 102(27):9446\u20139451, 2005.\nD.L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Transactions on\nInformation Theory, 47(7):2845\u20132862, 2001.\nD.L. Donoho and J. Tanner. Counting the Faces of Randomly-Projected Hypercubes and Orthants, with\nApplications. Arxiv preprint arXiv:0807.3590, 2008.\nM.X. Goemans and D.P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. J. ACM, 42:1115\u20131145, 1995.\n\n21\n\n\fIA Ibragimov, VN Sudakov, and BS Tsirelson. Norms of Gaussian sample functions. Proceedings of the\nThird Japan USSR Symposium on Probability theory, Lecture Notes in Math, 550:20\u201341, 1976.\nA. Juditsky and A.S. Nemirovski. On verifiable sufficient conditions for sparse signal recovery via l1\nminimization. ArXiv:0809.2650, 2008.\nK. Lee and Y. Bresler. Computing performance guarantees for compressed sensing. In IEEE International\nConference on Acoustics, Speech and Signal Processing, 2008. ICASSP 2008, pages 5129\u20135132, 2008.\nP. Massart. Concentration inequalities and model selection. Ecole d'Et\u00e9 de Probabilit\u00e9s de Saint-Flour\nXXXIII, 2007.\nC. Moler and C. Van Loan. Nineteen dubious ways to compute the exponential of a matrix, twenty-five\nyears later. SIAM Review, 45(1):3\u201349, 2003.\nY. Nesterov. Introductory Lectures on Convex Optimization. Springer, 2003.\nY. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127\u2013152,\n2005.\nY. Nesterov. Smoothing technique and its applications in semidefinite optimization. Mathematical Programming, 110(2):245\u2013259, 2007.\nJ. Sturm. Using SEDUMI 1.0x, a MATLAB toolbox for optimization over symmetric cones. Optimization\nMethods and Software, 11:625\u2013653, 1999.\nK. C. Toh, M. J. Todd, and R. H. Tutuncu. SDPT3 \u2013 a MATLAB software package for semidefinite programming. Optimization Methods and Software, 11:545\u2013581, 1999.\nAM Vershik and PV Sporyshev. Asymptotic behavior of the number of faces of random polyhedra and the\nneighborliness problem. Selecta Math. Soviet, 11(2):181\u2013201, 1992.\nY. Zhang. A simple proof for recoverability of l1 -minimization: Go over or under. Rice University CAAM\nTechnical Report TR05-09, 2005.\n\n22\n\n\f"}