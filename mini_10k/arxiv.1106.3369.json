{"id": "http://arxiv.org/abs/1106.3369v1", "guidislink": true, "updated": "2011-06-16T23:05:50Z", "updated_parsed": [2011, 6, 16, 23, 5, 50, 3, 167, 0], "published": "2011-06-16T23:05:50Z", "published_parsed": [2011, 6, 16, 23, 5, 50, 3, 167, 0], "title": "Evaluation of Fiji National University Campus Information Systems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1106.5874%2C1106.0251%2C1106.4724%2C1106.4096%2C1106.2630%2C1106.3087%2C1106.5570%2C1106.1563%2C1106.3170%2C1106.4333%2C1106.3806%2C1106.2894%2C1106.1792%2C1106.1544%2C1106.4682%2C1106.1830%2C1106.4793%2C1106.5445%2C1106.5796%2C1106.0409%2C1106.2766%2C1106.5825%2C1106.1764%2C1106.5288%2C1106.0373%2C1106.2561%2C1106.1619%2C1106.0288%2C1106.0390%2C1106.4953%2C1106.3080%2C1106.6000%2C1106.5325%2C1106.3184%2C1106.5300%2C1106.0881%2C1106.0370%2C1106.2679%2C1106.4519%2C1106.1055%2C1106.0042%2C1106.0774%2C1106.6228%2C1106.1178%2C1106.2584%2C1106.0585%2C1106.4998%2C1106.2644%2C1106.2659%2C1106.5460%2C1106.4147%2C1106.1487%2C1106.6334%2C1106.2599%2C1106.1315%2C1106.5888%2C1106.5865%2C1106.1951%2C1106.0201%2C1106.5031%2C1106.6279%2C1106.2146%2C1106.3369%2C1106.2957%2C1106.5953%2C1106.3911%2C1106.5488%2C1106.4967%2C1106.4826%2C1106.0623%2C1106.2673%2C1106.3078%2C1106.2875%2C1106.4088%2C1106.3436%2C1106.4605%2C1106.3662%2C1106.0307%2C1106.1482%2C1106.5803%2C1106.2390%2C1106.5916%2C1106.1939%2C1106.4214%2C1106.0923%2C1106.0653%2C1106.0933%2C1106.0162%2C1106.1637%2C1106.2955%2C1106.3725%2C1106.4174%2C1106.5230%2C1106.2401%2C1106.4917%2C1106.5307%2C1106.0851%2C1106.4843%2C1106.3318%2C1106.2704%2C1106.2204&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Evaluation of Fiji National University Campus Information Systems"}, "summary": "Fiji National University (FNU) has been encountering many difficulties with\nits current campus administrative systems. These difficulties include\naccessibility, scalability, performance, flexibility and integration. In order\nto address these difficulties, we developed a thin client web based campus\ninformation system. The newly designed system allows the students, academic and\nadministration staff of the university to handle their day to day affairs with\nthe university online. In this paper we describe three types of evaluation\ncarried out to determine the suitability of newly developed system for FNU\nenvironment. User interface evaluation was carried out to assess user interface\non a set of usability principles, usability evaluation to see the ease at which\nusers can use the system and finally performance evaluation to verify and\nvalidate user response time required to complete various tasks. The result of\neach of these evaluations were analysed and the system was rectified as part of\niterative design process.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1106.5874%2C1106.0251%2C1106.4724%2C1106.4096%2C1106.2630%2C1106.3087%2C1106.5570%2C1106.1563%2C1106.3170%2C1106.4333%2C1106.3806%2C1106.2894%2C1106.1792%2C1106.1544%2C1106.4682%2C1106.1830%2C1106.4793%2C1106.5445%2C1106.5796%2C1106.0409%2C1106.2766%2C1106.5825%2C1106.1764%2C1106.5288%2C1106.0373%2C1106.2561%2C1106.1619%2C1106.0288%2C1106.0390%2C1106.4953%2C1106.3080%2C1106.6000%2C1106.5325%2C1106.3184%2C1106.5300%2C1106.0881%2C1106.0370%2C1106.2679%2C1106.4519%2C1106.1055%2C1106.0042%2C1106.0774%2C1106.6228%2C1106.1178%2C1106.2584%2C1106.0585%2C1106.4998%2C1106.2644%2C1106.2659%2C1106.5460%2C1106.4147%2C1106.1487%2C1106.6334%2C1106.2599%2C1106.1315%2C1106.5888%2C1106.5865%2C1106.1951%2C1106.0201%2C1106.5031%2C1106.6279%2C1106.2146%2C1106.3369%2C1106.2957%2C1106.5953%2C1106.3911%2C1106.5488%2C1106.4967%2C1106.4826%2C1106.0623%2C1106.2673%2C1106.3078%2C1106.2875%2C1106.4088%2C1106.3436%2C1106.4605%2C1106.3662%2C1106.0307%2C1106.1482%2C1106.5803%2C1106.2390%2C1106.5916%2C1106.1939%2C1106.4214%2C1106.0923%2C1106.0653%2C1106.0933%2C1106.0162%2C1106.1637%2C1106.2955%2C1106.3725%2C1106.4174%2C1106.5230%2C1106.2401%2C1106.4917%2C1106.5307%2C1106.0851%2C1106.4843%2C1106.3318%2C1106.2704%2C1106.2204&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Fiji National University (FNU) has been encountering many difficulties with\nits current campus administrative systems. These difficulties include\naccessibility, scalability, performance, flexibility and integration. In order\nto address these difficulties, we developed a thin client web based campus\ninformation system. The newly designed system allows the students, academic and\nadministration staff of the university to handle their day to day affairs with\nthe university online. In this paper we describe three types of evaluation\ncarried out to determine the suitability of newly developed system for FNU\nenvironment. User interface evaluation was carried out to assess user interface\non a set of usability principles, usability evaluation to see the ease at which\nusers can use the system and finally performance evaluation to verify and\nvalidate user response time required to complete various tasks. The result of\neach of these evaluations were analysed and the system was rectified as part of\niterative design process."}, "authors": ["Bimal Aklesh Kumar"], "author_detail": {"name": "Bimal Aklesh Kumar"}, "author": "Bimal Aklesh Kumar", "arxiv_comment": "vol. 2 no. 3 2011", "links": [{"href": "http://arxiv.org/abs/1106.3369v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1106.3369v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.HC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1106.3369v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1106.3369v1", "journal_reference": null, "doi": null, "fulltext": "ISSN No. 0976-5697\n!\n$#$\n'\n\n(\n\n\"\n\n#\n\n!% & &$\n))) *\n\nEvaluation of Fiji National University Campus Information Systems\nBimal Aklesh Kumar\nDepartment of Computer Science and Information Systems\nFiji National University\nFiji Islands\nbimal.kumar@fnu.ac.fj\nAbstract: Fiji National University (FNU) has been encountering many difficulties with its current campus administrative systems. These\ndifficulties include accessibility, scalability, performance, flexibility and integration. In order to address these difficulties, we developed a thin\nclient web based campus information system. The newly designed system allows the students, academic and administration staff of the\nuniversity to handle their day to day affairs with the university online. In this paper we describe three types of evaluation carried out to\ndetermine the suitability of newly developed system for FNU environment. User interface evaluation was carried out to assess user interface on a\nset of usability principles, usability evaluation to see the ease at which users can use the system and finally performance evaluation to verify and\nvalidate user response time required to complete various tasks. The result of each of these evaluations were analysed and the system was\nrectified as part of iterative design process.\nKeywords: software evaluation, heuristics, usability, performance evaluation\n\nI.\n\nINTRODUCTION\n\nFiji National University (FNU) was established in 2010\nwith the merger of six government owned tertiary institutions.\nIt is a national institution, supporting the national effort for a\nstable economy and a literate population that is able to establish\nitself in the global community, while understanding and\nresponding to the aspirations of individuals. FNU has a\nnetwork of thirteen campuses throughout the country. The\nobjective of the FNU is to promote research and academic\nexcellence for the welfare and needs of the communities in Fiji\nas well as communities in the region and abroad who wish to\nreceive tertiary education of high quality at affordable cost [3].\nPrior to the merger and formation of the FNU and due to\nthe autonomous operations of these colleges, at least three\ndifferent campus information systems existed [4]. The\nuniversity faced considerable amount of difficulties with these\nsystems. These difficulties included accessibility, scalability,\nperformance, flexibility and integration. In order to address\nthese difficulties, we developed thin client web-based campus\ninformation systems. It was built using open source products\nand tools on modern code base with modern databases. FNUCIS has relatively clean separation between presentation,\nbusiness logic, and data access layers, with solid data\narchitectures and a well defined set of business processes. It is\neasily accessible to all the students and the staff of FNU\nthrough the local intranet or via World Wide Web. The design\nis such where subsequent modification is limited as possible to\nleast cost effect components and would not result in chain\nreaction of compensating modification hence making it easier\nto add more functionality in future [3]. The system easily\nintegrates with other systems such as finance and human\nresource used by the university.\nIn order to assess the suitability of newly developed system\nfor FNU we believe that three types of evaluation would be\nrequired; user interface evaluation to assess the user interface\nbased on a set of usability principles, usability evaluation to see\nthe ease at which users can use the system and finally\nperformance evaluation to verify the turnaround time required\nto complete various tasks. This paper describes in detail three\n\u00a9 2010, IJARCS All Rights Reserved\n\ntypes of evaluation carried out and presents the results of this\nevaluation.\nII.\n\nRELATED WORK\n\nIn literature there are very few studies reporting evaluation of\ninformation systems used by universities. Whyte and Bythway\n(1996) proposed a holistic approach to IS evaluation by\nspecifying three core elements to a system: Product that is\nhardware, software and training provided to the users; service\nthat is how users are responded to and process by which\nproduct and services are provided. Gemmell and Pagano\n(2003) used product-service-process grid to analyze the\nstudent information systems at Salford University, UK. The\nattributes associated with each element was then evaluated by\nthe users and finally gap approach was taken to the\nmeasurement of those system attributes (importance and\nperformance). It has been recognized in literature that user\nsatisfaction significantly affects the success or failure of any\ninformation systems [2]. Davis (2006), According to the\ntechnology acceptance model (TAM), simplicity, perceived\nease of use and efficiency are three fundamental determinants\nof user satisfaction. Therefore, we believe that user interface\nevaluation, usability evaluation and performance evaluation\nwould help as measure user satisfaction and success of our\nnewly developed system. This paper advances the body of\nsoftware evaluation knowledge at higher education sector.\nIII.\n\nUSER INTERFACE EVALUATION\n\nThere are basically four ways to evaluate a user interface:\nformally by some analysis technique, automatically by a\ncomputerized procedure, empirically by experiments with test\nusers, and heuristically by simply looking at the interface and\npassing judgment according to ones own opinion. Formally\nanalysis models are currently under extensive research but they\nhave not reached the stage where they can be generally applied\nin real software development projects. Automatic evaluation is\ncompletely infeasible excerpt for a very few primitive checks.\nTherefore current practice is to do empirical evaluations if one\nwants a good and thorough evaluation of a user interface.\n1\n\n\fBimal Aklesh Kumar et al , International Journal of Advanced Research in Computer Science, 2 (3), May-June, 2011,01-05\n\nUnfortunately in most practical cases people actually do not\nconduct empirical evaluations because they lack time,\nexpertise, inclination or simply the tradition to do so. In real\nlife most user interface evaluations are heuristic evaluations\n[8].\nHeuristic evaluation is usability engineering method for\nfinding the usability problems in a user interface design so that\nthey can be attended to as part of iterative design process [8].\nThe process requires that a small set of testers (evaluators)\nexamine the interface and judge its compliance with recognized\nusability principles (heuristics) [13]. The goal is the\nidentification of any usability issues, so that they can be\naddressed to as part of the iterative design process. Heuristic\nevaluation is popular in web development circles because it\nrequires few resources in terms of money, time or expertise, so\nany developer can enjoy the following benefits of heuristic\nevaluation [12].\n\u2022 This method provides quick and relatively cheap feed\nback for designers, the results would generate good\nideas for improving user interfaces. The development\nteam will also receive a good estimate how much the\nuser interface can be improved.\n\u2022 There is general acceptance that the design feedback\nprovided by the method is valid and useful. It can also\nbe obtained early on in the design process, whilst\nchecking conformity to established guidelines helps\npromote compatibility with other systems.\n\u2022 This method can seem overly critical as designers may\nonly get a feed back on the problematic aspects of the\ninterface as the method is normally used for the\nidentification of good aspects.\n\u2022 Usability problems found are normally restricted to\naspects of the interface that are reasonably easy to\ndemonstrate: use of colors, lay-out and information\nstructuring, consistency of the terminology,\nconsistency of the interaction mechanism.\nThere are three phases to carrying out heuristic evaluation,\nplanning, running and report [14]. In planning a panel of\nexperts is established with materials and equipments for\nevaluations. The experts work with the system while a list of\nproblems and recommendations are created in the third.\nA. Planning\nThe panel of experts must be established in good time for\nthe evaluation. The materials and the equipment for the\ndemonstration should also be in place. All the analysts need to\nhave sufficient time to become familiar with the product along\nwith the intended task scenarios. They should operate by an\nagreed set of evaluation criteria [8]. Our system would have\nthree primary users' students, academic and administration\nstaff. We chose a set of five evaluators from each group to\nexamine the interface and judge its compliance with a set of\nrecognized usability principles given on table I. These\nheuristics are general rules that described the common\nproperties of usable interfaces.\nTable I.\nNo.\n1\n2\n3\n4\n5\n6\n7\n8\n10\n\nSystem check list was produced based on the above\nheuristics for evaluators to use as a guide. Evaluators were\nrequired to identify problems and provide recommendations\nbased on the severity ratings. Severity rating is allocated to\neach problem which indicates the most serious problems. The\nfollowing 5 scale severity given on table II was used.\nTable II.\nScale\n0\n1\n2\n3\n4\n\n\u00a9 2010, IJARCS All Rights Reserved\n\nDescription\nI don't agree that this is a usability problem at all\nCosmetic problem only: need not be fixed unless extra\ntime is available on project\nMinor usability problem: fixing this should be given low\npriority\nMajor usability problem: important to fix, so should be\ngiven high priority\nUsability catastrophe: imperative to fix this before\nproduct can be released\n\nB. Running\nThe experts should be aware of any relevant contextual\ninformation relating to the intended user group, tasks and usage\nof the product. Heuristic evaluation is performed by having\neach evaluator inspect the interface alone. Only after all\nevaluations have been completed are the evaluators allowed to\ncommunicate and have their findings aggregated, this\nprocedure is important in order to ensure independent and\nunbiased evaluations from each evaluator [14]. The results of\nthe evaluation can be recorded as written reports these reports\nhave the advantage of presenting a formal record of evaluation\nbut require additional effort by the evaluator and to read and\naggregated by the evaluation manager. During the evaluation\nsession, the evaluators went through the interfaces several\ntimes and inspected the various elements and compared them\nwith the list of heuristics.\nC. Report\nA list of identified problems which may be prioritized with\nregards to the severity rating and safety critical is produced. A\nreport detailing the identified problems is written and provided\nas feedback to the development team. Heuristic evaluation does\nnot provide a systematic way to generate fixes to the usability\nproblems or a way to assess the probable quality of any\nredesigns [15]. However because heuristic evaluation aims at\nexplaining each observed usability problem with reference to\nestablished usability principles, it will often be fairly easy to\ngenerate a revised design according to guidelines provided by\nthe violated principle for good interactive systems [13]. Also\nmany usability problems have fairly obvious fixes once they\nhave been identified. There were four problems with a severity\nof three and above which is of high priority and it is important\nto fix. Other problems had a severity rating of one and two.\nThese were minor problems. Table III, discusses the\nrecommendation of the problems with severity rating of three\nand all of the problems found by the evaluators were\nimplemented.\nTable III. Recommendations\n\nUsabilty Principles\n\nHeuristics\nSimple and natural dialogue\nSpeak the users language\nMinimize user memory Load\nBe consistent\nProvide feedback\nProvide clearly marked exists\nProvide shortcuts\nGive Error messages\nHelp and documentation\n\nSeverity Ratings\n\nProblem No.\n2\n5\n8\n\n10\n\nRecommendation\nProvide explanation on technical jargons used.\nDisplay appropriate messages while processing is\ntaking place in the background.\nFields should be checked before passing information to\ndatabase. Apply error checking of fields and if there is\nan error display a pertinent explanation.\nImplement the help feature\n\n2\n\n\fBimal Aklesh Kumar et al , International Journal of Advanced Research in Computer Science, 2 (3), May-June, 2011,01-05\n\nIV.\n\nUSABILITY EVALUATION\n\nUsability testing requires number of users to perform a set\nof pre-defined tasks [5]. During the testing evaluators assess\nhow the users interact with the system and identify the usability\nissues of the system. Usability is one of the most important\nsuccess factors in system quality in particular for web sites.\nTesting the web usability appears to be more difficult than\ntesting traditional systems [7]. This is for the two reasons firstly\nthe web users are located all over the world but they access it\nconcurrently and secondly different types of hardware and\nsoftware are used in order to access the web. The usability of\nweb-based systems has a great impact on users on a daily basis,\nusers are unlikely to revisit a site if they encounter difficulties\nin using the site, where alternative sites are available.\nThe limitations to usability testing are; firstly that testing is\nalways an artificial situation which lack realistic circumstances\nand secondly participants do not fully represent the targeted\nweb site audience [10]. There are various methods for usability\nevaluation. Model/Metrics based use model of tool to generate\nusability measure. Inquiry based communicates with users to\ngain insights into usability problems. Inspection method\nreviews the user interface and tries it out to find problems.\nTesting method collects data to be analyzed while a user uses\nthe system [7]. In our study we believe that testing method\nwould be appropriate. The following steps are required to carry\nout evaluation [10]:\n\u2022 Test Plan - document what you will test and how.\nInclude major goals of the test.\n\u2022 Product/Prototype - the product must be bug free as\nappropriate for the goals for test. Pilot test the\nprototype by trying out the test scenarios. Consider\nall parts of the system.\n\u2022 Experimental Design - basic usability test of a single\nproduct usually an informal design. Competitive or\ncomparative studies.\n\u2022 Test Participants - participants to the level of the\ndevelopment. Number of participants depends on the\nlevel of the test.\n\u2022 Scenarios - these are usually end user \"use cases\" for\ntesting the product.\n\u2022 Test Setup - fidelity of test setup based on goals of\ntest. Common laboratory set up.\n\u2022 Test Procedures - typical test consider limits of\nattention in test time\n\u2022 Measures - perform measures. Secondary task\nperformance\n\u2022 Analysis - statistical comparison of competitive tasks.\nFor the purpose of usability evaluation we did a\ncomparative study by evaluating user satisfaction, working\nwith the various features of FNU-CIS and one of the existing\ncampus management systems PREMIUM. Usability is not a\nsingle one dimensional property; it has multiple components\nwith various attributes associated with user interface. These\nmeasures are the standard ones for determining how \"usable\"\nan interactive the system is, and allows us to make judgments\non the suitability of the interface for the tasks being carried out.\nEfficiency was measured in terms of ease to use the system.\nErrors are any action that prevents successful occurrence of\ndesired result and since some errors escalate the users' time, its\neffect is measured in the efficiency of use. Learn-ability and\nsatisfaction was a subjective measure assigned by each\nparticipant in the experiment. Interface memorability is rarely\ntested as thoroughly as other attributes but having the\n\n\u00a9 2010, IJARCS All Rights Reserved\n\ncomparison and post test questionnaires of both systems made\nit feasible to some extent.\nSet of task list was prepared for the evaluators to work on\nboth the systems. Participants with an equal mixture of\nstudents, academic and administration staff, volunteered for our\nusability study of FNU-CIS and Premium System. We\nidentified 45 participants to carry out a set of task on two\nsystems. After completion of the assigned tasks, participants\nanswered the post-test questionnaire and ranked the systems in\norder of preference. Every participant undertook all these tasks\naccording to the task lists and used the two systems across a\nnumber of sessions. After completing tasks, participants were\nasked to fill out a post-test questionnaire that contained a\nsubjective rating assigned to each tested characteristic of the\nsystem.\nThe post-test questionnaire consisted of a 5-point rating\nscale to gauze each characteristic of both applications. The\nrating scale ranged from 1 to 5 where 1 is \"Strongly Disagree\"\nand 5 is \"Strongly Agree\". There were also open ended\nquestions to gain user feedback. The tested features include:\nlogin, enrollment, generating class list, course adjustments,\nretrieving course work, accessing student details and student\ngrades. The bar chart in figure 1 presents the average ratings for\nthe tested features on efficiency like wise bar chart on figure 2\npresents the average rating on preference.\n5\n4.5\n4\n3.5\n3\nFNU-CIS\n2.5\nPREMIUM\n2\n1.5\n1\n0.5\n0\nStudents\n\nAcademic Staff\n\nAdministration Staff\n\nFigure 1. Usabilty test results on effeciency.\n\nAll three groups of participants found that FNU-CIS is\nmore efficient than Premium system.\n45\n40\n35\n30\n25\n\nFNU-CIS\nP RE M IUM\n\n20\n15\n10\n5\n0\nS tudents\n\nA c adem ic S taff\n\nA dm inis tration S taff\n\nFigure 2. Usabilty test results on preference.\n\nAll three groups of participants favored to use FNU-CIS\nover Premium system.\n\n3\n\n\fBimal Aklesh Kumar et al , International Journal of Advanced Research in Computer Science, 2 (3), May-June, 2011,01-05\n\nV.\n\nPERFORMANCE EVALUATION\n\nPerformance evaluation determines how fast some aspects\nof a system perform under a particular work load. It can also\nserve to validate and verify other quality attributes of the\nsystem [11]. Performance evaluation can serve different\npurposes. It can demonstrate that the system meets performance\ncriteria, can be used to compare two systems to find out which\none performs better or measure what parts of the system\nperforms badly [17]. Performing evaluation has various subgenres, we believe that load testing and configuration testing\nwould be ideal to test the performance of our system.\nA. Load Testing\nIt is conducted to understand the behavior of the application\nunder a specific expected load [16]. This load can be expected\nconcurrent number of users on the application performing a\nspecific number of transactions with in the set duration. This\ntest will give out the response times of all the important\nbusiness critical transactions and also point towards any bottle\nneck in the system.\nB. Configuration Testing\nIt is a variation of traditional performance testing. Rather\nthan testing from the perspective load you are testing the\neffects of configuration changes in the application performance\nbehavior.\nThere are seven phases of carrying out performance\nevaluation: identify the test environment, identify performance\nacceptance criteria, plan and design tests, configure the test\nenvironment, implement the test design, execute the test and\nanalyze results and tune.\n\u2022 Identify the test environment \u2013 identify the physical\ntest environment and the tools and resources available\nto the test team.\n\u2022 Identify performance acceptance criteria \u2013 identify the\nresponse time, throughput and resource utilization\ngoals and constraints. Response time is a user concern,\nthroughput is a business concern and resource\nutilization is a system concern.\n\u2022 Plan and design tests \u2013 identify key scenarios,\ndetermine variability among representative users and\nhow to simulate the variability, define test data and\nestablish metrics to be collected.\n\u2022 Configure the test Environment \u2013 prepare the test\nenvironment, tools and resources necessary to execute\neach strategy as features ad components become\navailable for test.\n\u2022 Implement the test design \u2013 develop the performance\ntest in accordance with the test design.\n\u2022 Execute the test \u2013 run and monitor your tests. Validate\nthe tests, test data and results collection, and execute\nvalidated tests for analysis while monitoring the tests\nand test environment.\n\u2022 Analyze Results and Tune \u2013 analyze consolidate and\nshare result data.\nThe test was set up in the computing lab at FNU which\nallowed us to use FNU network. There is no industry standard\nfor web application performance, in such an absence we\ndepended on our won judgment how fast is fast enough for our\napplication [14].We designed a set of tasks to be completed in\norder to measure the performance of the system.\nThe tests were performed using computers that have\nminimum hardware and software requirements to run our\ndesigned system. Software tool used to simulate task and create\n\u00a9 2010, IJARCS All Rights Reserved\n\nnumber of virtual users where necessary. Data was collected\nfor the response time and was later analyzed.\nFirstly we try to test the response time for our system\nagainst the existing student management system (PREMIUM).\nThe features which are similarly available in both the systems\nwere tested for response time. The tests were taken for eight\ndifferent business critical tasks such as login, enrollment,\ncourse adjustments, generating class list, student details etc.\nwhere the features are quite similar. The result below is given\nfor comparative test of our system against the premium system\ncurrently used by FNU.\nTable IV. Results of Comparative Testing\nTask\n\nResponse delay time\nwith FNU-CIS (ms)\n\n1. Login\n2. Enrollment\n3. Generate Class list\n4. Course Adjustment\n5. Retrieve Coursework\n6. Student Details\n7. Student Grades\n8.Update Coursework\n\n3960\n1637\n2232\n2386\n1669\n1967\n1669\n1372\n\nResponse delay\ntime with\nPREMIUM (ms)\n8018\n4002\n12281\n3437\n3669\n9007\n7950\n3867\n\nThe results of all eight tests favored FNU-CIS, thus it can\nbe stated that it is much faster then existing systems used by\nFNU.\nSecondly we performed load testing for our system using\nweb load testing tool Apache JMETER. Using the software tool\nwe simulated the tasks users will be performing on our system\nand tried this out with 500 virtual users which is the maximum\nexpected load of our system at any given time. These tests were\ncarried out on two different types of client machines.\nConfiguration 1 had the minimum hardware and software\nrequirements (256 MB RAM 1.2 GHZ processor) where as\nconfiguration 2 were PC's with 1 GB RAM and Intel dual core\nprocessors.\nTable V.\nTask\n\nLogin\nEnrollment\nCourse Adjustment\nRetrieve Coursework\nStudent Details\nClass List\n\nResults of Load Tesing\n\nAvg. response delay\ntime for\nConfiguration 1 (ms)\n4663\n4221\n3998\n3774\n2889\n2204\n\nAvg. response delay\ntime for\nConfiguration 2 (ms)\n4889\n4332\n4116\n4223\n3556\n3365\n\nThe result of load testing verifies that there is not much\ntime variance for the increase load with initial results of\ncomparison testing. It also shows that there is also not much\ndifference in the response time for running the application\nusing different client machines.\nVI.\n\nACKNOWLEDGMENT\n\nI would like to thank Dr. Sharlene Dai, senior lecturer at\nUniversity of the South Pacific for supervising this research\nproject and Fiji National University for financial support to\ncarry out this research.\nVII. CONCLUSIONS\nIn this paper we described three kinds of experiments done\non our FNU-CIS to assess user interface, usability and\nperformance of our system. Through heuristic evaluation, a set\n4\n\n\fBimal Aklesh Kumar et al , International Journal of Advanced Research in Computer Science, 2 (3), May-June, 2011,01-05\n\nof problems were found and rectified. Usability and\nperformance evaluation were based on comparative study of\ntwo systems. The results mainly favored FNU-CIS, users are\nvery much satisfied with use of FNU-CIS and have indicated\nfor wide use therefore we can justify that FNU-CIS is highly\nsuitable to be used at Fiji National University\nVIII. REFERENCES\n[1] Hallikainen P and Chen L (2005) \"A Holistic Framework\non Information Systems Evaluation with a Case\nAnalysis\", The Electronic Journal of Information Systems\nEvaluation, vol. 9 issue 2 pp. 57-64.\n[2] Gemmell M and Pagano R (2003) \"A Post\nImplementation Evaluation of a Student Information\nSystem in the UK Higher Education Sector\", The\nElectronic Journal of Information Systems Evaluation,\nvol. 6 issue 2 pp. 95-106.\n[3] Kumar B (2011) \"Thin Client Web-Based Campus\nInformation Systems for Fiji National University\",\nInternational Journal of Software Engineering and\nApplications, vol.2, no.1 pp. 13 -26.\n[4] Dai X and Kumar B (2010) \"Comparing and Contrasting\nCampus Information Systems in South Pacific Regional\nUniversities\", 2010 International Conference on\nComputational and Information Sciences (ICCIS 2010),\npublished by IEEE Computer Society pp. 721-724.\n[5] Chaudhary K, Dai X and Grundy K (2010) \"Experiences\nin Developing a Micro-payment System for Peer-to-Peer\nNetworks\", International Journal of Information\nTechnology and Web Engineering (IJITWE), vol. 5 issue\n1, pp. 23-42.\n\n\u00a9 2010, IJARCS All Rights Reserved\n\n[6] Carcary M (2009) \"ICT Evaluation in the Irish Higher\nEducation Sector\",The Electronic Journal of Information\nSystems Evaluation, vol. 12, issue 2 pp. 129-140.\n[7] Alshamari M and Mayhew P (2011) \"Technical Review:\nCurrent Issues of Usabilty Testing\", IETE Technical\nReview, vol. 26, issue 6 pp. 402-406.\n[8] Nielsen J and Molich R (1990) \"Heuristic Evaluation of\nUser Interfaces\" CHI' 90 Proceedings ACM pp 249-256.\n[9] The Basics of conduction a usabilty evaluiation, Melanie\nWright. Duke University Medical Center March 26 2004.\n[10] Gaffeney G \"What is Uasbilty Testing?\" 1999.\nInformation and Design.\n[11] S Barber (2010), How fast does a website need to Be?,\nunpublished.\n[12] Nielsen J \"How to Conduct a Heuristic Evaluation\"\nhttp://www.useit.com/papers/heuristic/heuristic_evaluatio\nn.html (2011).\n[13] (2011) Heuristic Evaluation \u2013 a Step by Step Guide,\nhttp://article.stiepoint.com/print/heuristic-evlautaion\nguide.html.\n[14] (2011) How to Conduct a Heuristic Evaluation. Jakob\nNeilsen.\nhttp://www.useit.com/papers/heuristic_evlautaion.html.\n[15] (2011) Heusrictic Evaluation,\nhttp://www.usabiltynet.org/tools/expertheuristic.html.\n[16] (2011) High Performance Testing,\nhttp://www.logigear.com/ 333-highperformancetesting.html\n[17] (2011) Software Performnace Testing,\nhttp://en.wikipedia.org/software_performnace_testing.\n\n5\n\n\f"}