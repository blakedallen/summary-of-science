{"id": "http://arxiv.org/abs/1201.0226v1", "guidislink": true, "updated": "2011-12-31T05:32:48Z", "updated_parsed": [2011, 12, 31, 5, 32, 48, 5, 365, 0], "published": "2011-12-31T05:32:48Z", "published_parsed": [2011, 12, 31, 5, 32, 48, 5, 365, 0], "title": "Towards Cost-Effective Storage Provisioning for DBMSs", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.0305%2C1201.2419%2C1201.4266%2C1201.0204%2C1201.1294%2C1201.1047%2C1201.4796%2C1201.6166%2C1201.3205%2C1201.0516%2C1201.3744%2C1201.0620%2C1201.6347%2C1201.2712%2C1201.3887%2C1201.4264%2C1201.3881%2C1201.6182%2C1201.6445%2C1201.5601%2C1201.3683%2C1201.3661%2C1201.1909%2C1201.0643%2C1201.5686%2C1201.1191%2C1201.5625%2C1201.6644%2C1201.4722%2C1201.6096%2C1201.5887%2C1201.5261%2C1201.0618%2C1201.0151%2C1201.4352%2C1201.5377%2C1201.0747%2C1201.2210%2C1201.3718%2C1201.6465%2C1201.0205%2C1201.6158%2C1201.2477%2C1201.0197%2C1201.3940%2C1201.3893%2C1201.1702%2C1201.0184%2C1201.2432%2C1201.2067%2C1201.6331%2C1201.1196%2C1201.0544%2C1201.2754%2C1201.0202%2C1201.1150%2C1201.5274%2C1201.1372%2C1201.1779%2C1201.1877%2C1201.3416%2C1201.4794%2C1201.0169%2C1201.5414%2C1201.3949%2C1201.5920%2C1201.4777%2C1201.4327%2C1201.4507%2C1201.5184%2C1201.3774%2C1201.0201%2C1201.1634%2C1201.1741%2C1201.1448%2C1201.0226%2C1201.2709%2C1201.3986%2C1201.2786%2C1201.3164%2C1201.1896%2C1201.3224%2C1201.6500%2C1201.2795%2C1201.5919%2C1201.3208%2C1201.1153%2C1201.2031%2C1201.0113%2C1201.1822%2C1201.5259%2C1201.2678%2C1201.4256%2C1201.6565%2C1201.1713%2C1201.0944%2C1201.2717%2C1201.4902%2C1201.3239%2C1201.5463%2C1201.4110&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Towards Cost-Effective Storage Provisioning for DBMSs"}, "summary": "Data center operators face a bewildering set of choices when considering how\nto provision resources on machines with complex I/O subsystems. Modern I/O\nsubsystems often have a rich mix of fast, high performing, but expensive SSDs\nsitting alongside with cheaper but relatively slower (for random accesses)\ntraditional hard disk drives. The data center operators need to determine how\nto provision the I/O resources for specific workloads so as to abide by\nexisting Service Level Agreements (SLAs), while minimizing the total operating\ncost (TOC) of running the workload, where the TOC includes the amortized\nhardware costs and the run time energy costs. The focus of this paper is on\nintroducing this new problem of TOC-based storage allocation, cast in a\nframework that is compatible with traditional DBMS query optimization and query\nprocessing architecture. We also present a heuristic-based solution to this\nproblem, called DOT. We have implemented DOT in PostgreSQL, and experiments\nusing TPC-H and TPC-C demonstrate significant TOC reduction by DOT in various\nsettings.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.0305%2C1201.2419%2C1201.4266%2C1201.0204%2C1201.1294%2C1201.1047%2C1201.4796%2C1201.6166%2C1201.3205%2C1201.0516%2C1201.3744%2C1201.0620%2C1201.6347%2C1201.2712%2C1201.3887%2C1201.4264%2C1201.3881%2C1201.6182%2C1201.6445%2C1201.5601%2C1201.3683%2C1201.3661%2C1201.1909%2C1201.0643%2C1201.5686%2C1201.1191%2C1201.5625%2C1201.6644%2C1201.4722%2C1201.6096%2C1201.5887%2C1201.5261%2C1201.0618%2C1201.0151%2C1201.4352%2C1201.5377%2C1201.0747%2C1201.2210%2C1201.3718%2C1201.6465%2C1201.0205%2C1201.6158%2C1201.2477%2C1201.0197%2C1201.3940%2C1201.3893%2C1201.1702%2C1201.0184%2C1201.2432%2C1201.2067%2C1201.6331%2C1201.1196%2C1201.0544%2C1201.2754%2C1201.0202%2C1201.1150%2C1201.5274%2C1201.1372%2C1201.1779%2C1201.1877%2C1201.3416%2C1201.4794%2C1201.0169%2C1201.5414%2C1201.3949%2C1201.5920%2C1201.4777%2C1201.4327%2C1201.4507%2C1201.5184%2C1201.3774%2C1201.0201%2C1201.1634%2C1201.1741%2C1201.1448%2C1201.0226%2C1201.2709%2C1201.3986%2C1201.2786%2C1201.3164%2C1201.1896%2C1201.3224%2C1201.6500%2C1201.2795%2C1201.5919%2C1201.3208%2C1201.1153%2C1201.2031%2C1201.0113%2C1201.1822%2C1201.5259%2C1201.2678%2C1201.4256%2C1201.6565%2C1201.1713%2C1201.0944%2C1201.2717%2C1201.4902%2C1201.3239%2C1201.5463%2C1201.4110&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Data center operators face a bewildering set of choices when considering how\nto provision resources on machines with complex I/O subsystems. Modern I/O\nsubsystems often have a rich mix of fast, high performing, but expensive SSDs\nsitting alongside with cheaper but relatively slower (for random accesses)\ntraditional hard disk drives. The data center operators need to determine how\nto provision the I/O resources for specific workloads so as to abide by\nexisting Service Level Agreements (SLAs), while minimizing the total operating\ncost (TOC) of running the workload, where the TOC includes the amortized\nhardware costs and the run time energy costs. The focus of this paper is on\nintroducing this new problem of TOC-based storage allocation, cast in a\nframework that is compatible with traditional DBMS query optimization and query\nprocessing architecture. We also present a heuristic-based solution to this\nproblem, called DOT. We have implemented DOT in PostgreSQL, and experiments\nusing TPC-H and TPC-C demonstrate significant TOC reduction by DOT in various\nsettings."}, "authors": ["Ning Zhang", "Junichi Tatemura", "Jignesh M. Patel", "Hakan Hac\u0131g\u00fcm\u00fc\u015f"], "author_detail": {"name": "Hakan Hac\u0131g\u00fcm\u00fc\u015f"}, "author": "Hakan Hac\u0131g\u00fcm\u00fc\u015f", "arxiv_comment": "VLDB2012", "links": [{"href": "http://arxiv.org/abs/1201.0226v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1201.0226v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1201.0226v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1201.0226v1", "journal_reference": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  274-285 (2011)", "doi": null, "fulltext": "Towards Cost-Effective Storage Provisioning for DBMSs\n\narXiv:1201.0226v1 [cs.DB] 31 Dec 2011\n\nNing Zhang #1 , Junichi Tatemura \u22172 , Jignesh M. Patel #3 , Hakan Hac\u0131g\u00fcm\u00fc\u015f \u22174\n#\nComputer Sciences Department, University of Wisconsin-Madison, USA\n1\nnzhang@cs.wisc.edu 3 jignesh@cs.wisc.edu\n\u2217\nNEC Laboratories America, USA\n2\ntatemura@sv.nec-labs.com 4 hakan@sv.nec-labs.com\nABSTRACT\n\nintroduction of flash solid state drives (SSDs). Thus it is\ncommon for DCs to have servers that have a rich I/O subsystem with a mix of traditional hard disk drives (HDDs)\ntypically in some RAID configuration, and some SSDs. To\nmake matters worse, since the price and the performance\ncharacteristics of these I/O devices vary widely, it is not\nuncommon to find server configurations that have a diverse\nI/O subsystems with various types of storage devices. For\nexample, a server may have a RAID HDD subsystem, and\na high-end fast but expensive SSD (e.g. Fusion IO), and a\nlow-end slow but cheaper SSD (e.g. a Crucial or Intel SSD).\nDC operators have to make the decision to purchase the\nserver boxes right upfront, and later have to deal with provisioning these resources on (ever changing) workloads. In\naddition, multiple different workloads may share resources\non the same physical box and provisioning the workload requires taking into account physical constraints such as storage capacity constraints. One dilemma that the DC operator faces in this setting is what resources to provision for\nspecific workloads given this rich (I/O) ecosystem.\nThe problem that we define and address in this paper is\nas follows: The DC has a cluster of servers each with a rich\nI/O subsystem on which a set of customer workloads must\nbe provisioned. Service Level Agreements (SLAs) between\nthe DC provider and the customers provide a contract in\nterms of what each customer can expect. Typical SLAs (via\nService Level Objectives embedded in the SLAs) describe\ncharacteristics such as expected performance [14] and expected data availability (e.g. SQL Azure's SLA [4]). Given\nthe SLAs, the goal of the DC provider is to provision enough\nresources to meet the SLAs, while minimizing the total operating cost, so as to maximize their profit.\nNotice that the objective here is to minimize the total operating cost (TOC). In this paper, we consider the TOC to\ninclude the amortized hardware cost (incurred during the\ninitial purchase and amortized over the expected lifespan of\nthat hardware), and the run-time energy costs incurred in\npowering that hardware when running the workload. Extending our work to include other components to TOC such\nas the management costs and amortized cost of the actual\nDC facility is fairly straight-forward (see the extended version of this paper [3] for more details).\nNow consider the impact of heterogeneous I/O hardware\non the TOC. Different I/O devices have different initial\ncosts, storage capacities, performance, and run-time energy\ncosts. SSDs generally run cooler than HDDs (the energy\nsavings is often an order of magnitude with SSDs), but cost\nmore (often more than 10X for the same storage). SSDs\n\nData center operators face a bewildering set of choices when\nconsidering how to provision resources on machines with\ncomplex I/O subsystems. Modern I/O subsystems often\nhave a rich mix of fast, high performing, but expensive SSDs\nsitting alongside with cheaper but relatively slower (for random accesses) traditional hard disk drives. The data center\noperators need to determine how to provision the I/O resources for specific workloads so as to abide by existing Service Level Agreements (SLAs), while minimizing the total\noperating cost (TOC) of running the workload, where the\nTOC includes the amortized hardware costs and the run\ntime energy costs. The focus of this paper is on introducing this new problem of TOC-based storage allocation, cast\nin a framework that is compatible with traditional DBMS\nquery optimization and query processing architecture. We\nalso present a heuristic-based solution to this problem, called\nDOT. We have implemented DOT in PostgreSQL, and experiments using TPC-H and TPC-C demonstrate significant\nTOC reduction by DOT in various settings.\n\n1.\n\nINTRODUCTION\n\nThe move towards cloud computing for data intensive\ncomputing presents unique opportunities and challenges for\ndata center (DC) operators. One key challenge that DC operators now face is how to provision resources in the DC for\nspecific customer workloads. The focus of this paper is on\none aspect of this vast problem \u2013 namely how to provision\nresources in the I/O subsystem. We note that I/O subsystems are often the most expensive components of high-end\ndata processing systems. For example, in the current highest\nperforming Oracle TPC-C configuration [2], the cost of the\nstorage subsystem is $23.9 million compared to $5.2 million\nfor the remaining server.\nTo fully understand the challenge, consider the dilemma\nof a modern DC operator, again only focusing on the I/O\nsubsystem. I/O subsystems have gotten incredibly complicated over the last few years primarily due to the disruptive\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Articles from this volume were invited to present\ntheir results at The 38th International Conference on Very Large Data Bases,\nAugust 27th - 31st 2012, Istanbul, Turkey.\nProceedings of the VLDB Endowment, Vol. 5, No. 4\nCopyright 2011 VLDB Endowment 2150-8097/11/12... $ 10.00.\n\n274\n\n\fTOC/GB/hour (cents)\nSequential Read (ms/IO)\nRandom Read (ms/IO)\nSequential Write (ms/row)\nRandom Write (ms/row)\n\nHDD\n3.47 \u00d7 10\u22124\n0.072 (0.174)\n13.32 (8.903)\n0.012 (0.039)\n10.15 (8.124)\n\nHDD Raid 0\n8.19 \u00d7 10\u22124\n0.049 (0.096)\n12.19 (2.712)\n0.011 (0.034)\n11.55 (3.770)\n\nL-SSD\n7.65 \u00d7 10\u22123\n0.036 (0.053)\n1.759 (1.468)\n0.020 (0.341)\n62.01 (37.45)\n\nL-SSD Raid 0\n9.51 \u00d7 10\u22123\n0.021 (0.037)\n1.570 (0.826)\n0.013 (0.082)\n21.14 (17.71)\n\nH-SSD\n1.69 \u00d7 10\u22121\n0.016 (0.013)\n0.091 (0.024)\n0.009 (0.025)\n0.928 (0.986)\n\nTable 1: The Cost and I/O profiles of different storage classes under 1 and 300 degree of concurrency: (1) The first\nrow lists five different storage types/classes that we use in our experiments. These storage types are discussed in\nmore detail in Section 4.1. L-SSD stands for Low-end SSD, H-SSD stands for High-end SSD. (2) The second row\nshows the storage cost in terms of cents per GB per hour, calculated using the method described in Section 2.1. (3)\nThe remaining four rows show the performance of the storage types on four typical I/O access patterns. In each cell,\nthe boldfaced number is for a workload with a single DB thread, whereas the number in the parentheses is the I/O\nperformance with 300 concurrent DB threads. See Section 3.5 for details about the concurrency parameter.\nhave far better random I/O performance. However, the sequential I/O performance of SSDs is comparable to HDDs\n(which are often setup in RAID configuration), or could be\nlower than the sequential performance of HDDs for the same\ncost [23]. Within the context of our problem statement, if\nwe measure the (TOC) for each byte of storage for each unit\ntime of usage, then different I/O devices have different costs,\nas shown in the first row of Table 1.\nThus, provisioning the I/O storage subsystem to minimize the TOC is an optimization problem that considers\nthe range of available I/O devices, examines the capacity\nconstraints for each device, and the performance characteristics of each workload and the I/O devices, to compute\na data layout that minimizes the TOC, while meeting the\nSLAs. In this paper, we propose, implement, and evaluate\na technique to address this problem.\nThe contributions of this paper are as follows:\n1. We formulate the problem of data placement to minimize the TOC for cloud-hosted DBMSs.\n\nextensions to our work. Related work is presented in Section 6, and Section 7 contains our concluding remarks.\n\n2. PROBLEM DEFINITION\nTo illustrate the problem of TOC-based storage provisioning, consider the following motivating scenario: Given a\ndata center with many database workloads, a data center\nadministrator needs to build a database server configuration\nthat consists of various storage devices. A critical question\nis how to choose the storage devices and how to place data\non these devices for each workload. Although it is said that\na high-end SSD performs much better than a hard disk drive\n(HDD), the administrator is not sure if it pays off in terms of\nthe (TOC) cost. The administrator wants to achieve better\ncost-performance while the performance (e.g., response time)\nmeets the given requirements as set by individual SLAs.\n\n2.1 Cost Model\nWe note that coming up with a cost (price) model of a\nstorage device is a complex problem as it depends on various\nfactors, such as vendor agreements and volume discounts. In\nthis work, we focus on the following simple model:\nStorage price (cent/GB/hour): For each storage, the\namortized storage cost is calculated and amortized by space\nand time (cent/GB/hour). Table 1 show our calculated storage prices for five actual devices: (1) HDD, (2) HDD RAID\n0, (3) L-SSD, (4) L-SSD RAID 0 and (5) H-SSD. In this calculation, the purchase cost of the I/O device is distributed\nover 36 months, and the energy cost is computed using a\ncost of 0.07 per kWh of energy consumption [16].\nWe model the available storage classes as D = {d1 , .., dM },\nwhere each di is a specific storage class (e.g. HDDs in RAID\n0 configuration). The price of dj is denoted by pj , and the\nprice vector P = {p1 , .., pM }.\nLayout cost (cent/hour): Assume that a database is laid\nout on D, taking Sj GB space for each class dj (Sj \u2265 0).\nNow, let L denote this particular layout. (We describe how\nto compute the layout L in Section 3.4). Then, the cost\nper hour for this layout L, denoted as C(L), is computed as\nC(L) = \u03a3dj \u2208D (pj \u2217 Sj ).\nWorkload cost (TOC) (cent/task): Assume that the\ndatabase with layout L achieves a throughput T (L, W ) (measured in tasks/hour) for a given workload W . Then, the\nworkload cost is defined as C(L, W ) = C(L)/T (L, W ) (more\ndetails are below in Section 2.3). In this paper, we refer to\nthis workload cost as TOC.\nOur problem is to find a layout L over D that minimizes\nC(L, W ) for a given workload W under the price model P\n\n2. We present a practical solution, called DOT, for the\ndata placement problem that can be incorporated in\nexisting DBMSs. The DOT method extends the DBMS\nquery optimizer's cost estimation component to consider the I/O speeds of various storage devices. DOT\nthen exploits the ability of most modern DBMSs to\noutput query plans (without actually executing the\nplan) that are then fed its TOC optimizing component. DOT's TOC optimizing module uses a novel\nheuristic to compute a desirable data placement.\n3. Finally, we have implemented the DOT method in\nPostgreSQL, and using TPC-H and TPC-C based workloads to verify its effectiveness, showing that in many\ncases, the data layout recommended by DOT is up to\n5X more cost-effective than other simple layouts.\nOn a cautionary note, we acknowledge that in this initial paper, we only focus on a small part of the problem\nof minimizing TOC in DCs. For example, we focus only\nthe I/O subsystem, we have focused on relatively simple\nworkloads, ignored multi-tenancy, and we do not consider\ndynamic workload migration. The area of minimizing TOC\nin DBMSs is fairly new, and there are many open unsolved\nproblems \u2013 we hope that this work seeds other work in this\narea to examine and solve these many open problems.\nThe remainder of this paper is organized as follows: Section 2 introduces our cost model and the problem definition.\nThe DOT method is described in Section 3. Section 4 contains experimental results, and Section 5 describes various\n\n275\n\n\fFigure 1: Allocating objects to storage classes\nwith constraints on storage capacity and workload performance, as described in the remainder of this section.\n\nFigure 2: Overview of the DOT method\n\n2.2 Data Layout and Capacity Constraints\n\nwe can demonstrate various cases of cost-performance tradeoffs, and compare them to the \"best\" performing case.\nWe also note that our framework can be generalized to\nallow a broader definition of performance constraints, including capturing a general distribution of the performance\nmetric (e.g. must be faster than x seconds in 95% of the\ncases, and faster than 1.5x seconds in 99% of the cases).\nSuch extensions are part of future work.\n\nWe assume that the storage system provides M different\nclasses of storage D = {d1 , * * * , dM }. A storage could be an\nindividual device, or a RAID group, and we use cj to denote\nthe capacity of the storage class dj .\nA database instance consists of a set of objects O =\n{o1 , * * * , oN }, such as individual tables, indices, temporary\nspaces or logs, that must be placed on one of the storage\nclasses in D. We use si to denote the size of the data object\noi . (In this paper, we do not consider partitioning or replication of objects, which are important considerations and\npromising directions for future research.)\nA data layout L is defined as a mapping from O to\nD, where L(o) indicates the storage mapping for object o.\nLet Oj denote a set of objects laid out on dj , i.e., Oj =\n{o|L(o) = dj , o \u2208 O}. A valid data layout must conform to\nthe capacity constraint of each storage, i.e., \u03a3oi \u2208Oj si < cj\n(j = 1, * * * , M ). Figure 1 illustrates a sample layout.\n\n2.5 Problem Formulation\nOur problem can be formally stated as follows:\nInput: (1) Database objects O = {o1 , * * * , oN }, (2) Storage classes D = {d1 , * * * , dM } with price (TOC/GB/hour)\nP = {p1 , * * * , pM } and capacity C = {c1 , * * * , cM }, (3) Query\nworkload W = {[q11 , * * * , qn1 ], * * * , [q1c , * * * , qnc ]} with performance constraints T = {tji }.\nOutput: A layout L : O \u2192 D that minimizes the TOC\nC(L, W ) = C(L) \u2217 t(L, W ) for a given W where\n\n2.3 Workloads\n\nC(L) = \u03a3dj \u2208D (pj \u2217 Sj )\n\nWe model a workload, W , as a set of query sequences,\n{[q11 , * * * , qn1 ], * * * , [q1c , * * * , qnc ]}, where each qij is a database\nquery, and c denotes the concurrency of the workload W .\nLet t(L, W ) be the execution time of W under layout L.\nThen, the workload cost (TOC) is C(L, W ) = C(L)\u2217t(L, W ).\n\nunder the capacity constraints, \u03a3oi \u2208Oj si < cj (j = 1, * * * ,\nM ), and performance constraints T = {tji }.\n\n2.4 Performance Constraints\n\nA naive way to solve the layout optimization problem (formulated in Section 2.5) is to enumerate and validate each\npossible layout candidate. However, this approach is computationally expensive, since with M different storage classes\nand N data objects, the number of all possible data layouts\nis exponentially large, i.e. M N .\nOur method to compute a Data layout Optimized to reduce the TOC, called DOT, is shown in Figure 2. There\nare four steps/phases in our solution: profiling, optimization, validation and refinement.\nThe technique starts by profiling the workloads on some\nbaseline layouts, L1 , * * * , Lk , to generate a number of workload profiles that are then used in an optimization phase.\nBriefly, a workload profile models the I/O behavior of the\nworkload when it runs on a baseline layout (e.g. for the\nquery select count(*) from Ai where id > A and id <\nB, it estimates how many random and sequential read I/Os\nare incurred on the table Ai when the table Ai and its indices\nare placed using some specific layout.) We discuss the profiling phase and the baseline layouts in detail in Section 3.4.\nThen, in the optimization phase, we employ an heuristic\napproach that makes use of the workload profiles and the\nworkload performance estimates from an extended DBMS\n\n3. THE DOT METHOD\n\nIn our model, we assume that there are performance related SLA constraints associated with the queries (so there\nis some limit on the query performance degradation that\ncan be tolerated). These performance constraints T can be\nmodeled as the upper bound of each query execution time,\nT = {tji }, where tji is the response time cap for query qij .\nWhile the framework above uses query response time as\nthe performance metric, this framework can be adapted to\nconsider other performance metrics, such as throughput rate.\nIn fact, in Section 4, we use response time constraints for\nindividual queries for the TPC-H DSS workload, and use\nthroughput constraints for the TPC-C OLTP workload.\nIn this paper, rather than using an absolute performance\nconstraint, we define the performance constraint as a ratio\nthat is relative to the best performance (similar to the way\nthe measure degradation limit that was used in [26]). For\ninstance, the performance constraint 1/3 means that the\nworkload can be up to 3 times slower than the best case\n(e.g., when all the objects are placed on a high-end SSD,\nwhen a high-end SSD is one of the available storage classes).\nUsing this method of defining the performance constraint,\n\n276\n\n\fProcedure 1 Optimization Phase of DOT\nInput:DOT input < O, D, P, C, W, T >, workload profile X\nOutput:Layout L\u2217\nL \u2190 L0 , L\u2217 \u2190 L\nc\u2217 \u2190 estimateT OC(W, L)\n\u2206 \u2190 enumerateM oves(O, D, P, X)\nfor i = 0 \u2192 |\u2206| do\nm \u2190 \u2206[i], Lnew \u2190 m(L)\n(c, T \u2032 ) \u2190 estimateT OC(W, Lnew )\nif f easible({Lnew , C}, {T \u2032 , T }) then\nL \u2190 Lnew\nif c < c\u2217 then\nL\u2217 \u2190 L, c\u2217 \u2190 c\nend if\nend if\nend for\n\na sequence of object moves. For each iteration, a move m\nin \u2206 is applied to a layout L, resulting in a new layout\nm(L). Here, as a heuristic, we want to apply a more beneficial move (i.e., larger TOC reduction) earlier. The subprocedure enumerateM oves should generate move candidates in such a promising order (we provide the pseudocode\nlater in Procedure 2), which we achieve by using a heuristic function (Section 3.3) to assign a priority score for each\nmove. This function considers the impact of a move that\ncomprises of a layout cost reduction and a workload performance penalty. The performance penalty is estimated based\non the estimated I/O time over the objects. After sorting\nthe move candidates in terms of their priority scores, we\napply them in sequence to generate new candidate layouts.\nA simple method to generate a set of move candidates is\nto move an object o \u2208 O to a storage class s \u2208 D one by\none, as was done in [10]. In this case, the sub-procedure\nenumerateM oves would generate M moves for each object.\nBy applying the moves one by one, DOT would investigate\nO(M N ) layouts. However, this approach has a serious limitation as it ignores the interactions between the objects.\nSince the move of one object can significantly change the\nI/O access pattern of another object, by ignoring the interaction between the objects, this simple approach ignores the\nchange in performance (e.g. the amount of I/O time) and\nindirectly affects the calculations of priority scores.\nA notable example of such interaction between objects is\nthe interaction between a table and its index: Assume that\na table has an index (e.g. B+ tree) on its primary key, and\na query wants to retrieve records in a given range on its\nprimary key (e.g., select * from table A where A.id >\n10 and A.id < 1000). Now, consider placing the index on\neither an SSD or a HDD, and the performance of these two\nplacement strategies. In this case, the performance of the\ntwo placement methods will depend on the the placement of\nthe base table. For instance, when the table is on the HDD,\nthe query planner may choose to only use a sequential scan\non the table to execute the query. In this case, the placement\nof the index has no impact to the I/O cost since it is not\naccessed at all. However, if the table is placed on the SSD,\nplacing the index on the SSD may let the query planner\nchoose an index scan to access both the table and the index\nfor greater performance, as this plan leverages the SSD's\nfaster random I/O speed. Thus, we should not ignore the\ninteraction between objects, e.g. a table and its index.\nOur heuristic approach is to put objects into groups, referred to as object groups, and consider interaction only\nwithin a group: we put a table and its indices in a group\nand consider all the combinations of their placements on different storage classes. For example, in the case of a table\nwith one index, and only two devices, a HDD and a SSD,\nwe consider (1) placing both the table and its index on the\nHDD device; (2) placing the table on the SSD device and the\nindex on the HDD device; (3) placing the table on the HDD\ndevice and the index on the SSD device, and (4) placing\nboth the table and its index on SSD device.\nOn the other hand, in our heuristic approach, we assume\nindependence between objects across different groups.\nProcedure 2 shows the pseudo code of enumerateM oves,\nwhich employs the idea of object groups. The high-level\ndescription of the procedure is as follows (see Section 3.2 for\ndetails): Data objects O are classified into groups G. For\neach group g in G, all the placement combinations of objects\n\nquery optimizer to explore the space of possible data layouts. This optimization phase outputs a recommended layout L\u2217 that satisfies all the constraints (See Section 2.5).\nWe describe this heuristic optimization approach in Section\n3.1. The extended DBMS query optimizer has a new cost\nestimation module that considers the different I/O speeds of\nstorage devices to give more precise estimates. We discuss\nhow to extend the query optimizer in Section 3.5.\nThe (heuristic) method used in the optimization phase is\nnot guaranteed to output a feasible layout; and rather than\nreturning a recommended layout, it may return an answer\nmarked as \"infeasible,\" which may mean that the process\nmissed a feasible layout that exists (i.e., false negative), or\nthat there is no feasible layout since the performance constraints are too strict. In either case, the performance constraints must be relaxed in order to compute a layout. The\nthird phase, namely the validation phase, checks if the recommended layout really confirms to the performance constraints through a test run of the workloads on the recommended layout. If the test run \"fails\", then the system goes\nto the refinement phase. This refinement phase uses real\nruntime statistics (such as the actual numbers of I/O incurred in the test run, buffer usage statistics, etc.), and uses\nthose as the input (instead of going to the profiling phase)\nto redo the optimization phase. In the interest of space, we\ndo not discuss the refinement phase in detail in this paper.\n\n3.1 The Heuristic Approach used in DOT\nThe pseudocode for the heuristic optimization module in\nDOT is shown in Procedure 1. This procedure enumerates\nthe layout candidates and returns the layout, L\u2217 , that has\nthe minimum estimated TOC (i.e., C(W, L)) amongst all\nthe candidates. The challenge here is how to enumerate a\npromising subset of the possible layouts.\nOur basic approach is to (1) start from a layout L0 that\nplaces all the objects on the most expensive storage class\n(say, d1 ), and (2) gradually move objects from d1 to other\nless expensive storage classes as long as the new layout Lnew\nand its estimated performance T \u2032 satisfies the capacity constraints C and the SLA constraints T (checked by procedure\nf easible in the pseudocode). Notice that, in our approach,\nthe move candidates, \u2206, are generated only once at the beginning of the procedure and are applied one by one, yielding\n|\u2206| layouts to be investigated.\nThe key component of this procedure is to generate \u2206,\n\n277\n\n\fProcedure 2 enumerateMoves: Enumeration of moves\nInput:< O, D, P, X >\nOutput:a list of moves \u2206\nG \u2190 grouping(O), \u2206 \u2190 \u03c6, \u03a3 \u2190 \u03c6\nfor all g \u2208 G do\nfor all p \u2208 D|g| do\nm \u2190 move(g, p)\ns \u2190 score(m, X, D, P )\n\u2206 \u2190 append(\u2206, m), \u03a3 \u2190 append(\u03a3, s)\nend for\nend for\n\u2206 \u2190 sort(\u2206, \u03a3)\n\nWe use the following four types of I/Os to model the typical DBMS query I/O access pattern [10]: sequential read\n(SR), random read (RR), sequential write (SW) and random\nwrite (RW). Now, let R denote the set of these I/O types.\nAs shown in Table 1, we are provided with the time of one\nI/O operation \u03c4rd for each type r \u2208 R and storage d \u2208 D.\nFrom this information, we need to estimate the accumulated\nnumber of I/O operations on o.\nWe use the profiling phase to estimate the number of I/O\noperations for each object (Section 3.4). As we have discussed above, the number of I/O operations on a specific\nobject can be very different depending on the placement of\nnot only this object but also other objects in the same group.\nThus, we estimate \u03c7pr [o], the number of I/O of type r on o\nwhen the group g is placed in a specific placement p.\nBased on the workload profiles X = {\u03c7pr [o]}, we estimate\nthe I/O time share of an object group g when it is placed\nXX p\nin p:\n\u03c7r [o] \u2217 \u03c4rp[o]\nT p [g] =\n(1)\n\nin g over storage classes D are considered, and a move m is\ngenerated for each combination. \u2206 is a list of such moves\nsorted in the order of priority.\nNext, we describe how move candidates are enumerated\nbased on object groups (Section 3.2), the priority score of\nmove candidates (Section 3.3), and workload profiles that\nare used to calculate the priority score (Section 3.4).\n\no\u2208g r\u2208R\n\nHere, p[o] is the storage class assigned by the placement p\nfor the object o.\nThen, the performance penalty of a move m(g, p) from the\ninitial layout L0 can be defined as follows:\n\n3.2 Object Groups\nWe divide the database objects in O into object groups\nso that interactions between objects in a group is higher\nfor objects within a group than objects in different groups.\nWe assume that any performance gain (or loss) due to a\nmove (from one storage class to another) is independent\nbetween objects in different groups. Let us represent a\ngroup of objects as a vector g = (o1 , * * * , oK ). Then, the\nplacement of a group can also be represented as a vector\np = (d1 , * * * , dK ) \u2208 DK . The number of possible placements\nof a group is O(M K ), where K is the size of the group.\nThe move of a group g to p is denoted as m(g, p). As\nshown in Procedure 2, enumerateM oves considers all the\npossible moves m(g, p). The size of \u2206 is thus O(GM K )\nwhere G is the number of groups and K is the size of a\ngroup (N = GK).\nWhile in the current version of DOT, a group consists\nof the table and its indices, in general, we could introduce\nother grouping to capture further interactions. However, we\nneed to carefully choose a grouping scheme so that the size\nK does not become too large. Notice that, if we put all the\nobjects in one group to consider all interactions, our algorithm becomes an exhaustive search method to enumerate\nall the O(M N ) layouts.\nIn the current grouping scheme, G is the number of tables\nand K is as large as the number of indices on each table.\nSince in many practical cases K is likely to be far smaller\nthan G, so the number of layout candidates O(GM K ) in\nDOT is much smaller than O(M KG ) (i.e., exhaustive search).\n\n\u03b4time [m] = T p [g] \u2212 T 0 [g]\n\n(2)\n\nNow consider the component the layout cost saving, which\nestimates the impact of a move m on the layout cost C(L).\nLet m(L) be the layout given by applying m to L. Then,\nthe cost saving of a move m is:\n\u03b4cost [m] = C(L0 ) \u2212 C(m(L0 ))\n\n(3)\n\nThe definition of C(L) is given in Section 2.1.\nFinally, the priority score of a move m, denoted as \u03c3[m],\nis defined by considering both the performance penalty and\nthe layout cost saving, and is calculated as:\n\u03c3[m] = \u03b4time [m]/\u03b4cost [m]\n\n(4)\n\nThe procedure enumerateM oves sorts all possible moves\nm(g, p) by their scores in the ascending order.\n\n3.4 Workload Profiles\nThe objective of the profiling phase is to measure the I/O\nbehavior of the workload when an object group g is laid out\nusing the placement p. This phase produces several workload profiles, where each profile corresponds to a specific\nplacement. As discussed above, the placement p of an object group can impact the optimizer's choice of query plans,\nresulting in very different I/O costs/profiles. Thus, when we\nprofile the workload, we consider these object interactions\nby enumerating all possible placements of an object group.\nA lightweight method to enumerate all possible placements of the object groups is to use a small set of layouts,\nreferred as baseline layouts. For instance, consider a case\nwhere each table has only one index on the primary key.\nThen we have a set of object groups of size 2 (i.e., K = 2).\nFor each group, we want to measure I/O profiles for all\nthe M 2 placement patterns. To do this, we use the M 2\nbaseline layouts {L(i,j) : 1 \u2264 i, j \u2264 M } defined as follows:\nL(i,j) places all the tables on di and all the indices on dj .\nThat is, each group object has the same placement p, where\np = (di , dj ). In general, we have O(M K ) baseline layouts\nwhere K is the (maximum) size of an object group. Compare\n\n3.3 Priority Score\nIn Procedure 2, a priority score s for a move m is calculated using workload profile X and storage information\n(D, P ). The priority score is derived from two components:\nperformance penalty and layout cost saving.\nFirst, we describe the notion of a performance penalty that\nestimates the impact of a move m relative to the workload\nperformance. The performance penalty is described using a\nterm called the I/O time share, which is the accumulated\nI/O time over objects o in g.\n\n278\n\n\fHDD\nL-SSD\nH-SSD\nBrand &\nWD Caviar\nImation\nFusion IO\nmodel\nBlack M-Class 2.5\"\nioDrive\nFlash type\nN/A\nMLC\nSLC\nCapacity\n500GB\n128GB\n80GB\nInterface\nSATA II\nSATA II PCI-Express\nRPM\n7200\nN/A\nN/A\nCache Size\n32MB\n64MB\nN/A\nPurchase cost\n$34\n$253\n$3550\nPower\n8.3 Watts\n2.5 Watts 10.5 Watts\n\nto the number of all possible layouts that cover all the combinations amongst different groups (which is again O(M N )),\nprofiling the workloads on the baseline layouts has a lower\ntotal complexity when K \u226a N . Notice that, by using only\nthe baseline layouts in this manner, we assume independence\nof the placements across different groups, which is the same\nassumption we made for our heuristics.\nA workload profile on a baseline layout, Lp , consists of the\nnumber of I/O in terms of the I/O types and the data objects. Here, \u03c7pr [o] is given as the number of I/Os of type r on\nobject o when the workload is executed over Lp . The workload profiles can be calculated either through (a) an estimate\ncomputed by our extended query optimizer as described in\nSection 3.5, or (b) a sample test run of the workload on Lp .\n(We see both cases in the results described in Sections 4.4\nand 4.5 respectively.)\nWe also notice that there is an opportunity to prune the\nbaseline layouts that are being profiled. If we can infer that\nthe query optimizer will choose the same plans on layouts\nLp and Lq , we only need to profile one of these. In Section\n4.5, we show a special case where only one layout is profiled.\nA general pruning method, however, is an open issue.\n\nTable 2: Storage class specifications\nInstead of using the I/O performance numbers of the devices published by the manufacturer or as seen from the OS\nlevel, we benchmark the effective I/O performance of each\nI/O request as observed by the DBMS, since: (1) with this\napproach, various overheads (e.g. latch overhead) and benefits (e.g. DB buffers) are incorporated. (2) we can model\nthe influence of concurrent DB queries on I/O performance.\nHere we use the term degree of concurrency to refer to\nthe number of concurrent DBMS query processing threads/\nprocesses, and use this concept to model how the I/O subsystem behaves when there are concurrent queries.\n\n3.5 Extended Query Optimizer\nThe heuristic step in DOT (described above in Section 3.1)\nestimates the TOC and then checks the performance constraint for a candidate layout by calling the query optimizer's estimation module to estimate the performance of\nthe workload for that layout. To enable this check, the query\noptimizer should support, or has to be extended to support:\n(1) query plan optimization that is aware of the I/O profiles\nof different storage classes; (2) execution time estimation of\nthe derived query plan. In this paper, we have extended the\nopen source RDBMS, PortgreSQL, to accommodate these\nrequirements of the DOT framework.\nA typical RDBMS such as PostgreSQL does not consider\ndifferent I/O performances for heterogeneous storage classes.\nHowever, as we have discussed, the best query plan can depend on the specific data layout. For example, the choice\nbetween a nested-loop join using an index (indexed NLJ)\nand hash join (HJ) given specific selectivities depends on\nthe random versus the sequential I/O performance characteristics of the different storage classes. In other words, if\nwe change the data layout, the cheapest query plan may\nalso change, and we need make the optimizer aware of this\ninteraction. To do that, we incorporate I/O profiles (as seen\nin Table 1) into the query plan cost estimation module.\nNext, we introduce a module that estimates the query response time. The PostgreSQL optimizer can output a query\nplan without actually executing the query. This plan includes statistics, such as the query plan cost, the number of\nI/Os for a scan and the number of rows processed by query\noperators (e.g., hashing or sorting). We utilize these statistics to estimate the I/O time associated with executing a\nquery, and use the CPU time estimates already provided by\nthe query optimizer to approximate the query response time\nas the sum of these two components. Methods for estimating the CPU time in this setting are well known [26], and\nhere we only focus on estimating the I/O time.\nFor simplicity, we do not analyze the effect of cached data\nin the buffer pool, which can significantly reduce the number of actual I/O in the query. We also ignore the cost of\nactually outputting the results.\n\n3.5.1 Benchmarking the I/O Characteristics\nIn general, our method of benchmarking the storage classes\nfollows the profiling method used in [10]. However, we generalize their method for benchmarking the storage class under\ncertain concurrency: we concurrently run K threads that\nissue queries over their own tables, i.e., thread i issues a\nquery to table Ai . Each table has a primary key id which\nis indexed with a B+ Tree.\nRead I/O: For read queries, we use a count(*) query\nso as to minimize the costs associated with producing the\noutput. Each thread issues the following queries:\n\u2022 Sequential Read (SR): Each thread issues the following query: select count(*) from Ai .\n\u2022 Random Read (RR): Each thread issues a sequence\nof queries, using the template: select count(*) from\nAi where id = ?, with randomly selected id values.\nThe time for each I/O is calculated by dividing the total\nelapsed time of running all queries by the total number of\nread I/O requests.\nWrite I/O: We benchmark the write I/O characteristics\nbased on the I/O characteristics that is observed end-to-end\nfrom inside the DBMS, and is estimated as follows:\n\u2022 Sequential Write (SW): The SW performance is\nmeasured by having each thread issue a large number\nof insertion queries, where each query inserts a single\nrow using the template: insert....into Ai .\n\u2022 Random Write (RW): To measure the RW performance, each thread issues a sequence of update queries\nusing the template update Ai set a = ? where id\n= ? with randomly selected id values. Notice that\nan update query consists of random read and random\nwrite. To estimate RW from update queries, we subtract the RR I/O time (as estimated above) from the\ntotal RW execution time. See [3] for more details.\n\n279\n\n\f(a) Box 1\n\n(b) Box 2\nFigure 4: Data layouts with relative SLA = 0.5 and\nthe original TPC-H workload.\n\nFigure 3: The original TPC-H workload with relative SLA = 0.5. The number in parenthesis associated with each label indicates the PSR value (%).\n\nmonths) of its purchase cost (including the RAID controller\nif needed) and the $0.07kWh data center energy cost [16].\nThe power dissipation in Table 2 is derived from the average\nvalues for read and write operations for each storage device.\nAlso, the power surcharge of the RAID controller is 8.25W.\nThe server runs CentOS (Linux kernel 2.6.18) and PostgreSQL 9.0.1, with our extended query optimizer (as discussed in Section 3.5) and the I/O profiling table shown\nin the Tables 1. We set the PostgreSQL shared buffer to\n4GB. In addition to the storage subsystems described above,\neach machine had an additional 500GB disk that holds the\nOS, DBMS binaries, and the database log files. Finally, OS\ncaching is turned off for both the log files and the data disks.\n\nAgain, the time for each write request (i.e. per row) is\ncalibrated by dividing the total elapsed time by the total\nnumber of queries.\nTable 1 shows the results from running this benchmark\non each storage class that we use in our evaluation (below),\nwith degree of concurrency values of 1 and 300. In our experiments described below, we use values with concurrency\n1 for the DSS workloads and 300 for the OLTP workloads.\nWe note that our DOT framework simply needs some\nmethod to characterize the I/O devices. The method described above is simply what we used in our evaluation in\nthis paper, and can be substituted with any other method\nwithout impacting the generality of our DOT framework.\n\n4.\n\n4.2 Simple Layouts\n\nEXPERIMENTAL RESULTS\n\nWe use the following \"simple\" layouts to compare with\nthe layouts that are recommended by DOT:\n\nIn this section, we experimentally evaluate our layout\ntechnique using an implementation of DOT in PostgreSQL,\nand demonstrate the effectiveness of our methods using both\nthe TPC-H benchmark (to represent a DSS workload) and\nthe TPC-C benchmark (to represent an OLTP workload).\n\n\u2022 All H-SSD: All objects placed in the H-SSD (i.e., L0 )\n\n\u2022 All L-SSD RAID 0: All objects placed in the L-SSD\nRAID 0\n\n4.1 Hardware and Software Specifications\n\n\u2022 All L-SSD: All objects placed in the L-SSD\n\n\u2022 All HDD RAID 0: All objects placed in the HDD\nRAID 0\n\nOur experimental platform is a server system with a 2.26\nGHz Intel(R) Xeon CPU E5520 with 8 cores and 64GB ECC\nmemory. To allow experiments in parallel and to avoid having to swap I/O devices for each experiment, we actually\nused two machines that were identical (same CPU, motherboard, memory, etc.), but had separate storage subsystems.\nThese two storage subsystems are:\n\u2022 Box 1: one HDD RAID 0, one L-SSD and one H-SSD.\n\n\u2022 All HDD: All objects placed in the HDD\n\n\u2022 Index H-SSD Data L-SSD: Indexes in the H-SSD\nand Data in the L-SSD.\nWe have also implemented the Object Advisor (OA) [10]\nmethod in PostgreSQL, as OA is the closest previously known\nmethod to DOT. We note that OA optimizes only for workload performance and not the TOC.\n\n\u2022 Box 2: one HDD, one L-SSD RAID 0, and one H-SSD.\n\nDOT is performed for each box individually, resulting in\ntwo separate recommendations. For instance, DOT on Box1\nrecommends a layout given the 3 storage classes HDD RAID\n0, L-SSD, and H-SSD as part of the input.\nThe specifications of the HDD, the L-SSD and the H-SSD\nis shown in Table 2. RAID 0 is implemented using two identical storage devices and a Dell SAS6/iR RAID controller.\nThis controller costs $110 and has a 256MB onboard cache.\nThe storage price for these storage classes, shown earlier\nin Table 1, is calculated from the amortized cost (over 36\n\n4.3 Performance Metrics\nAs discussed in Section 2.4, following the methodology\nin [26], as a performance measure we use a metric called the\n\"relative SLA\", which is the performance for a workload\nwith a given data layout compared to the performance of\nthat workload with all the data on the H-SSD (which is\ntypically the highest performing case). For instance, relative\nSLA = 0.5 implies that the target performance SLA is half\nof the performance with all the data on the H-SSD. For the\n\n280\n\n\f(a) Box 1\n\n(b) Box 2\n\nFigure 5: The modified TPC-H workload with relative SLA = 0.5. The number in parenthesis associated with each label indicates the PSR value (%).\n\nFigure 6: The advised data layouts with relative\nSLA = 0.5 and the modified TPC-H workload.\n\ntarget performance metrics, we use the response time of each\nquery for the TPC-H workload and the total throughput for\nthe TPC-C workload.\nNotice that a simple layout, which is not aware of SLA,\ncan fail to meet the target performance. We need an overall\nmeasure to indicate the degree of SLA violation of such a\nlayout. For the TPC-H workload, we measure the fraction\nof the queries that don't meet their relative SLA, using a\nratio called the performance satisfaction ratio (PSR). For\nexample, a PSR value of 75% means that 75% of queries in\nthe workload meet their relative SLAs and 25% of them do\nnot. For the TPC-C workload, we do not need an additional\nmeasure since the throughput performance itself serves as\nsuch an indicator.\n\nperiments, we use a smaller workload (for exhaustive search\nto be tractable) and vary the capacity limits (to make it\nmore challenging for the DOT heuristics).\nFor all the TPC-H experiments, a 30GB TPC-H database\nis generated (scale factor 20) and all the tables are randomly\nreshuffled so that they are not clustered on the primary keys.\n\n4.4.1 The Original TPC-H Workload\nFigure 3 shows the cost/performance comparison amongst\nthe different layouts when the relative SLA is set to 0.5. The\nresponse time is the time to complete the workload and the\ncost is the measured TOC. The corresponding PSR values\nare shown in parenthesis in the figure. So, for example, the\nPSR value for the All L-SSD case is 95%.\nFrom Figure 3, we make the following observations: First,\nour heuristic layouts on Box 1 and Box 2 produce significant\nsavings - more than 3X - in terms of the TOC against the\nAll H-SSD layout. Second, our heuristic layouts outperform\nthe ones produced by OA, especially on Box 1. Looking\nat the PSR values (shown in parenthesis in Figure 3), we\nalso notice that OA's PSR is only 95% and 90% on Box 1\nand Box 2 respectively, while DOT achieves a PSR of 100%\nin both cases. Third, all the other simple layouts (except\nthe All H-SSD case) have a lower TOC, but lead to longer\nresponse times. Finally, looking at the PSR numbers in Figure 3 (shown in the parenthesis) for these simple layouts, we\nobserve that these layout (except All H-SSD) have PSR values that are less than 100% \u2013 meaning that some queries in\nthese layouts don't meet the required performance targets.\nFigure 4 (a) and (b) shows our heuristic layouts for the\nBox 1 and Box 2 configurations. In these figures, the primary index associated with a table is denoted by appending\nthe suffix \" pkey\" to the table name (e.g. partsupp has an\nprimary index file called partsupp key).\nFrom Figure 4, we observe that some table objects (e.g.\nlineitem) that tend to be accessed frequently with the SR\nI/O requests, are placed on the HDD RAID 0 in Box 1 and\non the L-SSD RAID 0 in Box 2. RAID 0 systems are very\ncost-effective for SR I/O patterns as seen in Table 1: The\nSSD RAID 0 achieves SR I/O performance comparable to\nH-SSD (x1.3) with significantly lower storage cost (x0.056).\nThe HDD RAID 0 can be similarly compared with the L-\n\n4.4 TPC-H Experiments\nFor experiments on the DSS workloads, we used two flavors of the TPC-H workloads. These workloads are:\nThe original TPC-H workload: Following the methodology in [22], we use 66 queries generated from the original\n22 TPC-H query templates as this workload. Thus in this\nworkload, each TPC-H query occurs three times in the mix.\nThe workload is executed sequentially with the SR I/O as\nthe dominating I/O type.\nA modified TPC-H workload: We use the exact five\nTPC-H query templates (Query # 2, 5, 9, 11, 17) that were\nused in [10]. These five queries are modified in the same\nmanner as in [10] to simulate an Operational Data Store\nenvironment. The modifications to these queries is to add\nmore predicates (on the part key, order key and/or the\nsupplier key) to the \"where\" clause of the queries, so as to\nreduce the number of rows that are returned. As a result,\nthis workload now has a mix of random and sequential read\nI/O (Mixed I/O). This workload has a total of 5 query templates that are executed sequentially 20 times, to produce\na workload with a 100 queries. The actual queries that we\nused in this experiment can be found in [3].\nIn these experiments, we vary the relative SLA (to values\nof 0.5 and 0.25) without setting any capacity limits on the\nstorage classes.\nIn addition, we evaluate the heuristics in DOT by comparing it with an exhaustive search approach. For these ex-\n\n281\n\n\fThe DOT layouts for in this case are omitted here (but\ncan be found in [3]). We observed that compared to the case\nwhen the relative SLA = 0.5 (Figure 6), now some bulk data\n(e.g. lineitem) moves to the cheaper storage classes, such\nas L-SSD RAID 0 on Box 2, and HDD RAID 0 on Box 1.\nAnother interesting observation across the two different\nrelative SLAs of 0.5 and 0.25 above, comes from taking a\ncloser look at the ratio of Indexed NLJ (% INLJ) that are\nused in each case with the DOT layouts on both box configurations. As noted above, with a relative SLA value of 0.5\nabout 50% of the join operations are INLJs. Looking at the\nquery plans for the case when the relative SLA value is 0.25,\nwe observe that only 33% of the query plans in the DOT configurations (in both box configurations) are now INLJs. As\nthe SLA constraint loosens, DOT moved the data around\nand switched query plans to use more hash join algorithms\n(rather than INLJ) to achieve the target SLA. This observation demonstrates the need to consider query optimization\nalong with data layout optimization.\n\nFigure 7: The modified TPC-H workload with relative SLA = 0.25. The number in parenthesis associated with each label indicates the PSR value (%).\n\n4.4.3 Heuristics Versus Exhaustive Search\n\nSSD (x1.36 faster at only x0.107 of the storage cost). DOT\nleverages these RAID 0 systems to save on the TOC with\nonly a small (and acceptable) performance penalty.\nNotice in Figure 4 that some tables (e.g. partsupp) and\ntheir primary key indices are still placed on the H-SSD. In\nfact, some queries (e.g., Query #2) involves RR I/O. Since\nthe performance gap between the H-SSD and the RAID 0\nsystem is large for RR I/O, we still need to put these objects\non the H-SSD to meet the (relative) SLA requirements.\nWe have also repeated the experiment above with the relative SLA value set to 0.25. The heuristic layouts are similar\nas the ones when the relative SLA is 0.5. and we omit these\nresults in the interest of space.\n\nIn this experiment, we compare the heuristics (introduced\nin Section 3.1) with exhaustive search algorithms in terms of\nthe TOC and performance of the layouts that each method\nrecommends. The Exhaustive Search (ES) method explores\nall possible layouts and evaluates each one of them using the\nsame TOC and performance estimation as DOT.\nTo allow the ES method to complete, we use a smaller\nworkload in this experiment. This workload consists of 33\nTPC-H queries generated from the 11 TPC-H query templates, which are a subset of the original 22 TPC-H queries\ntemplate. These queries are: Q1, Q3, Q4, Q6, Q12, Q13,\nQ14, Q17, Q18, Q19, Q22. The reason why we use a subset\nof queries is that ES explores an exponential number (i.e.,\nM N ) of layouts. If we use the whole TPC-H data set (that\ncontains 16 objects), the number of all possible layout is 43\nmillion, which we estimate would take about 3,500 hours\nfor ES to compute. To make the ES method run in a reasonable amount of time, we use eight TPC-H data objects\n(lineitem, orders, customer, part and their indices) and\na subset of TPC-H original queries for this experiment.\nIn this experiment, we fix the relative SLA to 0.5 and vary\nthe capacity limits on the storage classes to compare the\nperformance of DOT and ES. Adding capacity constraints\nmakes the feasible search space more challenging for the\ngreedy heuristics to explore.\nWe enforce capacity limits on the HDD RAID 0 and the\nHDD storage devices. As described in Section 4.4.1, the\nH-SSD and the L-SSD devices are not heavily used in the\noriginal TPC-H queries, so we do not add capacity limits to\nthese devices. We ran a test run of ES on both configurations\nand found the space that it needs on the HDD Raid 0 device\nin Box 1 and the HDD device in Box 2, which was 27GB\nand 8.8GB respectively. Then, we set the capacity limits\nfor these devices to be around these limits, to 24GB and\n8GB on Box 1 and Box 2 respectively, and then decrease\nthis limit by half each time.\nFrom this experiment, we observed that DOT's performance (both in terms of the TOC and the response time)\nis comparable to that of ES \u2013 DOT's response time in these\nexperiments was within 9% of ES in all cases, and its TOC\nwas within 16% of ES in most cases. In the interest of space\nwe omit the detailed graphs, and refer the reader to [3].\n\n4.4.2 The Modified TPC-H Workload\nFigure 5 shows the cost/performance comparison for the\ndifferent layouts on the modified TPC-H workload, when\nthe relative SLA is 0.5. From the PSR values in this figure\n(shown in parenthesis in the figure), we observe that all the\nsimple layouts (except the ALL H-SSD case) fail to achieve\nthe target SLA, resulting in low PSR values.\nFigure 6 illustrates the layout created by DOT when the\nrelative SLA is 0.5. Observe the difference from the original TPC-H workload experiments (shown in Figure 4): now\nDOT places most of the data objects on the H-SSD device\nin both the Box 1 and 2 configurations. For this modified\nworkload we now have more selective predicates, and the\nquery optimizer has more opportunities to exploit the highperformance RR I/O characteristics of the H-SSD device by\nusing indexed NLJ (INLJ). In fact, we have observed that\non the DOT layouts (across both box configurations), with\nthis modified TPC-H workload (and relative SLA = 0.5),\n50% of the joins in the query plans for this workload are\nINLJ, whereas only 11% of the joins in the original TPC-H\nworkload (discussed in Section 4.4.1) were INLJ.\nAlthough DOT has to use mostly the H-SSD device to\nmeet the SLA, Figure 5 shows that DOT still saves on the\nTOC compared to the All H-SSD layout.\nNow, we relax the relative SLA to 0.25. The results for\nthis experiment are shown in Figure 7. From this figure, we\nobserve that the TOC with DOT is 5X lower than the TOC\nwith the All H-SSD layout, while achieving a 100% PSR.\n\n282\n\n\fHDD\n\nL-SSD\nRAID 0\nH-SSD\n\nFigure 8: TPC-C results\nAlso, DOT computes the layouts orders of magnitude\nfaster than ES taking about 9 seconds in each case, compared to 1,400 seconds for ES.\n\nSLA 0.5\npk warehouse\npk customer\norders\nitem, pk item\npk district\npk orders\ni orders\n\ni customer\ncustomer\npk stock\npk order line\ndistrict\norder line\nwarehouse\nstock\nhistory\nnew order\npk new order\n\nSLA 0.25\npk warehouse\nitem, pk item\npk orders\npk district\nhistory\npk customer\nnew order\ni orders\norders\npk new order\ni customer\ndistrict\npk order line\npk stock\norder line\nwarehouse\ncustomer\nstock\n\nSLA 0.125\npk warehouse\nitem, pk item\npk customer\npk district\nnew order\npk new order\norders\npk orders\npk order line\ni orders\ncustomer\ni customer\nwarehouse\ndistrict\npk stock\nhistory\norder line\nstock\n\nTable 3: DOT layouts under different relative SLAs\non Box 2 for the TPC-C workload.\n\n4.5 TPC-C Experiments\n\nthe TOC with DOT decreases as the relative SLA is relaxed:\nDOT on Box1 with the relative SLA = 0.125 has about 3X\nsmaller TOC compared to the All H-SSD case.\nFrom the data layout on Box 2 configuration shown in\nTable 3, we observe that as the relative SLA is relaxed,\nmore objects are shifted from the expensive storage classes\nto the cheaper ones. (We observed a similar trend for Box\n1 \u2013 see [3] for details.)\nAn interesting observation is we saw is that the L-SSD\ndevice in Box 1 is seldom used (see [3] for details), since the\nL-SSD device has poor random write (RW) performance, as\nseen from Table 1. Even though the L-SSD device is faster\nthan the HDD RAID 0 device for RR I/O, the difference is\nnot big enough to overcome the L-SSD's poor RW I/O and\nexpensive TOC. Therefore, most objects are laid out on the\nHDD RAID 0 and the H-SSD devices in Box 1. However,\nfor the layout on Box 2 (see Table 3), the customer object\nis placed on the L-SSD RAID 0 device when the relative\nSLA = 0.125 even though it is accessed frequently using\nRW I/O (Table 3). The reason for this behavior is that the\nRAID 0 device can improve random write performance by\ndistributing the write evenly over the two disks. Coupled\nwith RAID 0, the L-SSD device can still be utilized in the\nTPC-C workloads.\nOverall, these results indicate that even with the TPC-C\nlike workload, DOT can produce TOC-efficient data layouts.\n\nFor the TPC-C experiments, we measured and compared\nthe performance of the different layouts on two metrics, the\nNew-Order transactions per minute(tpmC) and the TOC.\nFor this experiment, we used the Database Test Suite 2 [1]\n(DBT2), which is a fair implementation of the TPC-C benchmark, and populated a 30GB (scale factor 300) TPC-C\ndatabase. DBT2 provides various workload parameters, such\nas terminals/warehouse, DB connections and think time. In\nour experiment, we choose 300 DB connnections, 1 terminals/warehouse, no think time, set the measurement period\nof TPC-C workload to 1 hour, and use two minutes to rampup the database.\n\n4.5.1 Workload Profiling\nWe observed that most I/O patterns in the TPC-C workload are random accesses, even when all the data objects\nare placed on the HDD. From this observation, we estimate\nthat the query plans will not change (from random access\nto sequential access) even if the data objects are moved to\nHDD. Thus, in this experiment, we only need one simple\nlayout: namely, the All H-SSD case.\nTo generate the workload profiles (see Section 3.4), we use\na test run instead of the estimates from the query optimizer,\nsince the TPC-C queries have short latencies, and the test\nrun can give actual I/O statistics. After a 5-minute test run,\nour layout technique uses the workload profiles and the I/O\nprofiles (estimated under 300 degree of concurrency) to get\na TOC-effective layout.\n\n4.5.3 Heuristics Versus Exhaustive Search\nWe also compared the DOT heuristics introduced in Section 3.1 to the Exhaustive Search (ES) for the TPC-C workload. In this experiment, we use the entire TPC-C benchmark, and set the relative SLA = 0.25. We also vary the\ncapacity constraints (as we did for the comparison with ES\nin the TPC-H case described in Section 4.4.3). In this experiment, we enforce capacity constraints only on the H-SSD,\nsince this device are often the most capacity constrained.\nThe specific capacity constraint values that we use for the\nH-SSD are: No Limit and 21GB.\nNotice that, given the stringent constraints both on the\ncapacity and the performance, there may be no feasible so-\n\n4.5.2 Performance Results\nFirst, we evaluate the effect on the TOC when using DOT\nwith varying performance constraints. We ran the TPC-C\nworkload on both Box 1 and Box 2 with relative SLA values\nof 0.5, 0.25, 0.125, without capacity limits on any of the\nstorage classes. Here, relative SLA = 0.5 means that the\nobserved tpmC should be higher half of the tpmC that can\nbe achieved with all the data on the H-SSD.\nFigure 8 shows the effectiveness of each layout in terms of\nthe tpmC and the TOC. From this figure, we observe that\n\n283\n\n\f5.2 Discrete-sized Storage Cost Model\n\n(a) No Limit\n\nIn Section 2.1, we define the layout cost as C(L) =\n\u03a3dj \u2208D (pj \u2217 Sj ), where C(L) is in linear relationship with\nthe actual space usage Sj on dj . However, the storage devices are generally bought in discrete-sized units (e.g. 40GB,\n80GB, 120GB) so C(L) may not vary linearly with Sj . To\nadapt to this discrete-sized case, we generalize our layout\ncost definition as follows.\nLayout cost (cent/hour): Assume that a database is laid\nout on D, taking Sj GB space for each storage class dj (Sj \u2265\n0). The price and capacity of dj are pj and cj respectively.\nNow, let L denote this particular layout. Then, the cost per\nhour for this layout L, denoted as C(L), is computed as:\n\n(b) 21GB (rel. SLA 0.13)\n\nFigure 9: ES vs DOT with different capacity limits\non Box 2 for the TPC-C workload.\nlution. In such a case, we slightly relax the relative SLA\nand repeat the optimization as illustrated in Figure 2. This\nprocess stops when ES finds a feasible solution.\nThe result for this experiment for Box 2 is shown in Figure 9. Each graph is associated with the capacity limit on\nthe H-SSD device, and the final relative SLA value. ES and\nDOT achieve almost same result (tpmC and TOC). In this\ncase, DOT computes the layouts in 3 seconds compared to\n800 seconds for ES.\nWe have also run the experiment above with relative SLA\nvalues of 0.5 and 0.125, on both Box 1 and Box 2, and capacity limits of 18GB, 15GB and 12GB. The results presented\nabove are representative of the results in these other cases\n(with DOT and ES having nearly the same TOC and tpmC\nperformance).\n\n5.\n\nC(L) = \u03a3dj \u2208D [\u03b1 \u2217 (pj \u2217 cj ) + (1 \u2212 \u03b1) \u2217 (Sj /cj ) \u2217 (pj \u2217 cj )]\nAs seen from the above formula, the layout cost C(L) is\ncomposed of two parts: (pj \u2217 cj ) and (Sj /cj ) \u2217 (pj \u2217 cj ). The\nfirst part, namely (pj \u2217 cj ), is the discrete cost determined\nby the number of identical devices in a certain storage class.\nThe discrete cost has to be paid no matter how much space\nis used in that storage class. On the other hand, (Sj /cj ) \u2217\n(pj \u2217 cj ) is the linear cost determined by the proportional\nspace usage. The variable \u03b1 is a tunable parameter that can\nadjust the weights between the discrete and the linear costs.\nIn the extended version of this paper [3], we include more\ndiscussions on this cost model and present experimental results demonstrating that our DOT method still works with\nthis storage cost model.\n\nDISCUSSIONS\n\nIn this section, we discuss two possible extensions of this\nwork: namely, (1) other possible problem definitions, and\n(2) the discrete-sized storage cost model. In the extended\nversion of this paper [3], we run extensive experiments to\nverify our DOT method can work well with these extensions.\n\n6. RELATED WORK\nThe problem of data placement involves assigning N data\nobjects to M storage devices with the objective of improving\nthe workload performance. A recent work on this topic by\nKoltsidas et al. [18] examines the optimal data page placement between a SSD and a traditional HDD. They propose\na family of online buffer pool replacement algorithm so that\npages are placed on the right devices for better workload\nperformance. Canim et al. [10] propose an Object Advisor\nto place database objects (e.g. tables or indices) on either\nSSDs or HDDs. Their method first collects the I/O statistics\nof a workload and then uses a greedy algorithm to decide\nthe placement of the tables and indices. Our work differs\nfrom this work in many aspects. First, their goal is to maximize the workload performance by using a limited capacity\non the SSDs, while our goal is to minimize the TOC that is\nincurred when running that workload. Second, their query\noptimizer is not aware of the specific characteristics of the\nSSDs, so they miss the interactions between the query plans\nand data layouts. In contrast, we design and employ an\nextended query optimizer in Section 3.5 to make the query\noptimizer aware of different storage classes, and our method\nis able to update the cheapest query plan dynamically as\nthe data layout is changed.\nThe virtual machine placement problem, as proposed in [7,\n12, 17], is to deploy the virtual machines on the most suitable physical hosts for either better resource utilization of\nthe hosts, lower operating costs, or load balancing among\nhosts. The connection between the virtual machine placement problem and our work is that the problem they want\nto solve is to find a mapping of a virtual machine onto physical hosts, while our problem is to find a mapping (or data\n\n5.1 Other Problem Formulations\nAn interesting complementary problem to the one that\nwe use in this paper is to pick the \"right\" server hardware\nfrom a range of options, for a pre-defined workload. In the\nfollowing, we formally define this problem. In [3], we experimentally show that given some options about the storage configurations, our DOT method is able to recommend\nthe TOC-efficient storage configuration and the data layout,\nwhile guaranteeing the SLAs of the input workloads.\n\n5.1.1 The Generalized Provisioning Problem\nInput: (1) Database objects O = {o1 , * * * , oN }, (2) Storage Configurations Options F = {f1 , * * * , fX }, where each\nfi uses the storage classes Di = {di1 , * * * , diM } with price\n(TOC/GB/hour) Pi = {pi1 , * * * , piM } and capacity Ci =\n{ci1 , * * * , ciM }, (3) Query workload W = {[q11 , * * * , qn1 ], * * * ,\n[q1c , * * * , qnc ]} with performance constraints T = {tji }.\nOutput: A storage configuration fk with the data layout\nLk on fk : O \u2192 Dk that minimizes the TOC C(Lk , W ) =\nC(Lk ) \u2217 t(Lk , W ) for a given W where\nC(Lk ) = \u03a3dk \u2208Dk (pkj \u2217 Sjk )\nj\n\nunder the capacity constraints, \u03a3oi \u2208O k si < ckj (j = 1, * * * ,\nj\n\nM ), and performance constraints T = {tji }.\nThe extended version of this paper [3], contains experimental results that show how the DOT framework can be\nextended to solve this generalized provisioning problem.\n\n284\n\n\f9. REFERENCES\n\nlayout) between data objects and storage classes. The commonality at a high level is that both consider performance\nconstraints (e.g. SLA) and that the naive solution space\nsize in both cases is exponential and impractical to explore\nall possible solutions. But besides that high-level similarity,\ntheir problem and our problem are very different in terms\nof the goal and solution.\nAnother branch of related work is index advisor, or alternatively, physical design tuning [6,8,9,13]. The only similarity between the index advisor problem and our problem is\nwe both consider the impact of indexes on query execution\nwithin the storage bounds. However, the differences between these two problems are significant. First, their problem statement is: given all possible index selection choices,\ndetermine which indexes should be selected to create. While\nour problem assumes that the indexes are already selected\nand we consider which storage class each index and data\nshould be placed on. Other important difference is that their\nobjective is to maximize the query/workload performance,\nwhile our goal is to minimize the total operation cost, while\nmaintaining a certain level of the query performance.\nWith the maturity of SSDs, substantial research has focused on improving the DBMS performance by using SSDs\nincluding revisiting the five-minute rule based [15], examining methods for improving various DBMS internals such as\nquery processing techniques [25, 27], index structure [5, 21,\n24], bufferpool extension [11], page layout [19] and temporary space [20]. These methods are complementary to our\nwork here, as these efforts allow the DBMS to use SSDs\nmore effectively \u2013 hence, these methods can be easily used\nalong with our method in a DBMS that is tuned for SSDs.\n\n7.\n\n[1] Database test suite. http://osdldbt.sourceforge.net/.\n[2] Oracle sparc supercluster with t3-4 servers, tpc-c 5.11.0,\nretrieved on 19-may-2011. http://www.tpc.org/results/\nindividual results/Oracle/Oracle SPARC SuperCluster with\nT3-4s TPC-C ES 120210.pdf.\n[3] Towards cost-effective storage provisioning for DBMSs:\nAddendum \u2013 extended version. http://pages.cs.wisc.edu/\n\u223cnzhang/pubs/vldb extended.pdf.\n[4] SQL azure service level agreement (SLA), retrieved on october\n27, 2010. http://go.microsoft.com/fwlink/?LinkId=159706.\n[5] D. Agrawal, D. Ganesan, R. K. Sitaraman, Y. Diao, and\nS. Singh. Lazy-adaptive tree: An optimized index structure for\nflash devices. PVLDB, 2(1):361\u2013372, 2009.\n[6] S. Agrawal, E. Chu, and V. R. Narasayya. Automatic physical\ndesign tuning: workload as a sequence. In SIGMOD\nConference, pages 683\u2013694, 2006.\n[7] N. Bobroff, A. Kochut, and K. A. Beaty. Dynamic placement of\nvirtual machines for managing SLA violations. In Integrated\nNetwork Management, pages 119\u2013128, 2007.\n[8] N. Bruno and S. Chaudhuri. Automatic physical database\ntuning: A relaxation-based approach. In SIGMOD Conference,\npages 227\u2013238, 2005.\n[9] N. Bruno and S. Chaudhuri. An online approach to physical\ndesign tuning. In ICDE, pages 826\u2013835, 2007.\n[10] M. Canim, B. Bhattacharjee, G. A. Mihaila, C. A. Lang, and\nK. A. Ross. An object placement advisor for DB2 using solid\nstate storage. PVLDB, 2(2):1318\u20131329, 2009.\n[11] M. Canim, G. A. Mihaila, B. Bhattacharjee, K. A. Ross, and\nC. A. Lang. SSD bufferpool extensions for database systems.\nPVLDB, 3(2), 2010.\n[12] S. Chaisiri, B.-S. Lee, and D. Niyato. Optimal virtual machine\nplacement across multiple cloud providers. In APSCC, pages\n103\u2013110, 2009.\n[13] S. Chaudhuri and V. R. Narasayya. Self-tuning database\nsystems: A decade of progress. In VLDB, pages 3\u201314, 2007.\n[14] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati,\nA. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and\nW. Vogels. Dynamo: amazon's highly available key-value store.\nIn SOSP, pages 205\u2013220, 2007.\n[15] G. Graefe. The five-minute rule twenty years later, and how\nflash memory changes the rules. In DaMoN, page 6, 2007.\n[16] J. R. Hamilton. Cooperative expendable micro-slice servers\n(cems): Low cost, low power servers for internet-scale services.\nIn CIDR, 2009.\n[17] C. Hyser, B. McKee, R. Gardner, and B. J. Watson. Autonomic\nvirtual machine placement in the data center. HPL-2007-189,\n2008.\n[18] I. Koltsidas and S. Viglas. Flashing up the storage layer.\nPVLDB, 1(1):514\u2013525, 2008.\n[19] S.-W. Lee and B. Moon. Design of flash-based DBMS: an\nin-page logging approach. In SIGMOD Conference, pages\n55\u201366, 2007.\n[20] S.-W. Lee, B. Moon, C. Park, J.-M. Kim, and S.-W. Kim. A\ncase for flash memory ssd in enterprise database applications.\nIn SIGMOD Conference, pages 1075\u20131086, 2008.\n[21] Y. Li, B. He, J. Yang, Q. Luo, and K. Yi. Tree indexing on\nsolid state drives. PVLDB, 3(1):1195\u20131206, 2010.\n[22] O. Ozmen, K. Salem, J. Schindler, and S. Daniel.\nWorkload-aware storage layout for database systems. In\nSIGMOD Conference, pages 939\u2013950, 2010.\n[23] M. Polte, J. Simsa, and G. Gibson. Enabling enterprise solid\nstate disks performance. In Workshop on Integrating\nSolid-state Memory into the Storage Hierarchy, 2009.\n[24] K. A. Ross. Modeling the performance of algorithms on flash\nmemory devices. In DaMoN, pages 11\u201316, 2008.\n[25] M. A. Shah, S. Harizopoulos, J. L. Wiener, and G. Graefe. Fast\nscans and joins using flash drives. In DaMoN, pages 17\u201324,\n2008.\n[26] A. A. Soror, U. F. Minhas, A. Aboulnaga, K. Salem,\nP. Kokosielis, and S. Kamath. Automatic virtual machine\nconfiguration for database workloads. In SIGMOD Conference,\npages 953\u2013966, 2008.\n[27] D. Tsirogiannis, S. Harizopoulos, M. A. Shah, J. L. Wiener,\nand G. Graefe. Query processing techniques for solid state\ndrives. In SIGMOD Conference, pages 59\u201372, 2009.\n\nCONCLUSIONS AND FUTURE WORK\n\nThis paper has introduced a new problem of provisioning\nI/O resources for a workload to minimize the total operating\ncost that is incurred when running that workload. This paper has also presented the design of a solution, called DOT,\nfor this problem. DOT extends the query optimization components that are already present in a modern DBMS, and\nhence is a practical solution. We have implemented DOT\nin PostgreSQL, and have presented extensive evaluation of\nDOT using various DSS and OLTP workloads. These results\ndemonstrate that DOT can produce significant reductions in\ntotal operation costs, while meeting performance SLAs.\nThere is a wide range of future work that is possible, including examining every aspect of database query processing, database query optimization, physical database design,\netc., from the new perspective of minimizing the total operating cost while meeting traditional performance targets set\nin SLAs. Other future work includes extending the DOT\nframework to help make purchasing and capacity planning\ndecisions; for example, by running DOT iteratively to determine the TOC and SLA performance of different hardware\nconfigurations under consideration.\n\n8.\n\nACKNOWLEDGMENTS\n\nThe work by the first author was primarily done while\nthe author was at the NEC Laboratories of America. This\nresearch was supported in part by a grant from the National\nScience Foundation under grant IIS-0963993, and a gift donation from the NEC Laboratories of America.\n\n285\n\n\f"}