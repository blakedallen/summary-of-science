{"id": "http://arxiv.org/abs/1011.0953v1", "guidislink": true, "updated": "2010-11-03T17:38:23Z", "updated_parsed": [2010, 11, 3, 17, 38, 23, 2, 307, 0], "published": "2010-11-03T17:38:23Z", "published_parsed": [2010, 11, 3, 17, 38, 23, 2, 307, 0], "title": "Overcoming Problems in the Measurement of Biological Complexity", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.5999%2C1011.1299%2C1011.5807%2C1011.2844%2C1011.3352%2C1011.3306%2C1011.4202%2C1011.3128%2C1011.3927%2C1011.1403%2C1011.2226%2C1011.1210%2C1011.5766%2C1011.3253%2C1011.4934%2C1011.6255%2C1011.4357%2C1011.3869%2C1011.2367%2C1011.6461%2C1011.4098%2C1011.0130%2C1011.1990%2C1011.2440%2C1011.1922%2C1011.2419%2C1011.0369%2C1011.5787%2C1011.3558%2C1011.0896%2C1011.1770%2C1011.1754%2C1011.4054%2C1011.6272%2C1011.0953%2C1011.2674%2C1011.2927%2C1011.2426%2C1011.2877%2C1011.5140%2C1011.5836%2C1011.3651%2C1011.5164%2C1011.0385%2C1011.1082%2C1011.0740%2C1011.1766%2C1011.6182%2C1011.0625%2C1011.5818%2C1011.2757%2C1011.0578%2C1011.4329%2C1011.3713%2C1011.0443%2C1011.1247%2C1011.6000%2C1011.5843%2C1011.3248%2C1011.5063%2C1011.3774%2C1011.2671%2C1011.5911%2C1011.1603%2C1011.0270%2C1011.3917%2C1011.0685%2C1011.4796%2C1011.1593%2C1011.6624%2C1011.6361%2C1011.4923%2C1011.6332%2C1011.1392%2C1011.4152%2C1011.6552%2C1011.3699%2C1011.6663%2C1011.0530%2C1011.0568%2C1011.5637%2C1011.4596%2C1011.3172%2C1011.0731%2C1011.1217%2C1011.3967%2C1011.2147%2C1011.1699%2C1011.4460%2C1011.4909%2C1011.3469%2C1011.5398%2C1011.6673%2C1011.1346%2C1011.5097%2C1011.6218%2C1011.5145%2C1011.6432%2C1011.3528%2C1011.4724%2C1011.4808&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Overcoming Problems in the Measurement of Biological Complexity"}, "summary": "In a genetic algorithm, fluctuations of the entropy of a genome over time are\ninterpreted as fluctuations of the information that the genome's organism is\nstoring about its environment, being this reflected in more complex organisms.\nThe computation of this entropy presents technical problems due to the small\npopulation sizes used in practice. In this work we propose and test an\nalternative way of measuring the entropy variation in a population by means of\nalgorithmic information theory, where the entropy variation between two\ngenerational steps is the Kolmogorov complexity of the first step conditioned\nto the second one. As an example application of this technique, we report\nexperimental differences in entropy evolution between systems in which sexual\nreproduction is present or absent.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.5999%2C1011.1299%2C1011.5807%2C1011.2844%2C1011.3352%2C1011.3306%2C1011.4202%2C1011.3128%2C1011.3927%2C1011.1403%2C1011.2226%2C1011.1210%2C1011.5766%2C1011.3253%2C1011.4934%2C1011.6255%2C1011.4357%2C1011.3869%2C1011.2367%2C1011.6461%2C1011.4098%2C1011.0130%2C1011.1990%2C1011.2440%2C1011.1922%2C1011.2419%2C1011.0369%2C1011.5787%2C1011.3558%2C1011.0896%2C1011.1770%2C1011.1754%2C1011.4054%2C1011.6272%2C1011.0953%2C1011.2674%2C1011.2927%2C1011.2426%2C1011.2877%2C1011.5140%2C1011.5836%2C1011.3651%2C1011.5164%2C1011.0385%2C1011.1082%2C1011.0740%2C1011.1766%2C1011.6182%2C1011.0625%2C1011.5818%2C1011.2757%2C1011.0578%2C1011.4329%2C1011.3713%2C1011.0443%2C1011.1247%2C1011.6000%2C1011.5843%2C1011.3248%2C1011.5063%2C1011.3774%2C1011.2671%2C1011.5911%2C1011.1603%2C1011.0270%2C1011.3917%2C1011.0685%2C1011.4796%2C1011.1593%2C1011.6624%2C1011.6361%2C1011.4923%2C1011.6332%2C1011.1392%2C1011.4152%2C1011.6552%2C1011.3699%2C1011.6663%2C1011.0530%2C1011.0568%2C1011.5637%2C1011.4596%2C1011.3172%2C1011.0731%2C1011.1217%2C1011.3967%2C1011.2147%2C1011.1699%2C1011.4460%2C1011.4909%2C1011.3469%2C1011.5398%2C1011.6673%2C1011.1346%2C1011.5097%2C1011.6218%2C1011.5145%2C1011.6432%2C1011.3528%2C1011.4724%2C1011.4808&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In a genetic algorithm, fluctuations of the entropy of a genome over time are\ninterpreted as fluctuations of the information that the genome's organism is\nstoring about its environment, being this reflected in more complex organisms.\nThe computation of this entropy presents technical problems due to the small\npopulation sizes used in practice. In this work we propose and test an\nalternative way of measuring the entropy variation in a population by means of\nalgorithmic information theory, where the entropy variation between two\ngenerational steps is the Kolmogorov complexity of the first step conditioned\nto the second one. As an example application of this technique, we report\nexperimental differences in entropy evolution between systems in which sexual\nreproduction is present or absent."}, "authors": ["Manuel Cebrian", "Manuel Alfonseca", "Alfonso Ortega"], "author_detail": {"name": "Alfonso Ortega"}, "author": "Alfonso Ortega", "arxiv_comment": "4 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/1011.0953v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.0953v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "nlin.AO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.PE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.0953v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.0953v1", "journal_reference": null, "doi": null, "fulltext": "Overcoming Problems in the Measurement of Biological Complexity\nManuel Cebrian, Manuel Alfonseca, and Alfonso Ortega\n\narXiv:1011.0953v1 [cs.CE] 3 Nov 2010\n\nDepartment of Computer Science, Universidad Aut\u00f3noma de Madrid, 28033 Madrid, Spain\nIn a genetic algorithm, fluctuations of the entropy of a genome over time are interpreted as\nfluctuations of the information that the genome's organism is storing about its environment, being\nthis reflected in more complex organisms. The computation of this entropy presents technical\nproblems due to the small population sizes used in practice. In this work we propose and test\nan alternative way of measuring the entropy variation in a population by means of algorithmic\ninformation theory, where the entropy variation between two generational steps is the Kolmogorov\ncomplexity of the first step conditioned to the second one. As an example application of this\ntechnique, we report experimental differences in entropy evolution between systems in which sexual\nreproduction is present or absent.\n\nI.\n\nINTRODUCTION\n\nThe evolution over time of the entropy of a genome\nwithin a population is currently an interesting problem\nwhich is conjectured to be connected to the evolution\nof the complexity of organisms in a genetic algorithm\n[1, 2]. The complexity of the genome of an organism\nis considered to be the amount of information about its\nenvironment it stores. That is, evolution would cause\nthe appearance of more complex sequences, which correspond to more complex phenotypes. This hypothesis\nstates that natural selection acts as a Maxwell demon,\naccepting only those changes which adapt better to the\nenvironment and give rise to more complex individuals\nwith genomes of lower entropy. This idea was tested by\nthe simulation of a very simple system of asexual individuals in fixed environmental conditions.\nHowever, it is well known that that the computation\nof the entropy as\nX\nH(genome) = \u2212\np(x) log p(x)\nx\u2208genomes in the population\n\n(1)\nhas technical complications, due to the large size of the\nsample needed to estimate it with accuracy [1, 3, 7]. In\npractice, it is usually estimated as\nH(genome) =\n\nX\n\nH(i),\n\n(2)\n\ni=1..size(genome)\n\ni.e., as the sum of the entropy contributions of each locus in the genome. This estimation misses the entropy\ncontributions due to epistatic effects. Some sophisticated\nstatistical methods can be used to remedy this (see Appendix in [2]), although we will not deal with them in\nthis work.\nAn still unexplored way to overcome this problem is\nto estimate the entropy of a genome as its average Kolmogorov complexity\nhK(genome)igenomes in the population \u2192 H(genome) (3)\n(see [5, 8, 9]). However, this result only holds for in-\n\nfinitely long sequences, and therefore it cannot be applied\nto finite (sometimes short) genomes.\nIf we are only interested in the entropy evolution of\nthe genome, and not in the particular value estimation,\nwe can resort to the following trick: the genetic algorithm can be modelled as a thermodynamic system which\nevolves over time, where every population is just a measurement by an observer, i.e. the system is modeled as\na statistical ensemble of genomes and each measurement\nis just a sample from that ensemble.\nNow we can measure the entropy evolution of the system from two different viewpoints. The first is the system itself, where the entropy is calculated a la Shannon\n(equation 2) by estimating the probabilities of the loci\nalleles, using the frequencies of the ensemble sample.\nThe second way of measuring the entropy is from the\nviewpoint of the observer, where a measurement is made\nof the population at each time step, and the information\nabout the system is updated, i.e., the observer measures\nthe system at time t and stores this information St . At\ntime t + 1 the observer makes another measurement and\nsubstitutes St by St+1 . The entropy variation due to\nthis substitution can be calculated for both equilibrium\nand non-equilibrium thermodynamic systems [4, 13, 14].\nSince evolution cannot be modeled as a system in equilibrium, the second case applies: the mutation and generational replacement operators may increase or decrease\nthe entropy of the system [10, 11].\nThus the entropy variation from the observer viewpoint is K(St ) \u2212 K(St+1 ) bits. As K(*) is an incomputable measure, we estimate it by using the Lempel-Ziv\nalgorithm [12] as limn\u2192\u221e (1/n)|LZ(x)| = (1/n)K(x|n),\nwhere x is an infinite string and |LZ(x)| is the size of the\nsame string compressed [5]. Now our measurement of the\nsystem at time t, |St | is much larger than with equation\n3 (just the genome), so the estimation becomes possible.\n\nII.\n\nEXPERIMENTS\n\nWe want to test whether the evolution of\nK(population sample) can help in the study of the\nevolution of H(genome). Both measurements have their\n\n\f2\nK(population)\n5\n\n3000\n\n4.5\n\nbits\n\n2000\n4\n\n1000\n\n3\n\n0\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n10000\n\n8000\n\n10000\n\n2.5\n\nH(genome)\n2\n\n0.8\n\n1.5\n\n0.6\n\nbits\n\nfrecuency\n\n3.5\n\n1\n0.5\n0\n0.75\n\n0.4\n0.2\n\n0.8\n\n0.85\n\n0.9\n\n0.95\n\n0\n\n1\n\n0\n\n2000\n\ncorrelation coefficients\n\nFIG. 2: Example of continuous evolutionary chase without\ngenetic differentiation obtained with parameters Bmax = 5,\nPopt = 0.2, \u03bc = 5.0 \u00d7 10\u22125 , \u03b1 = 0.05, sc = 1.02, 2 loci, phenotypic model: additive. Experimental correlation coefficient:\n0.9956.\nK(population)\n15000\n\n10000\n\n5000\n\n0\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n10000\n\n8000\n\n10000\n\nH(genome)\n6\n4\n\nbits\n\nlimitations, but their agreement would provide evidence\nthat the entropy evolution is being studied correctly.\nWe have evaluated this experimentally, using the genetic algorithm proposed by [6], which is able to reproduce sexual behaviour in a vary detailed way, because it includes several features absent in the [2] experiments, such as sexual reproduction, different interlocus and intra-locus interactions across the genotypic or\nphenotipyc distance, and the evolutionary mechanisms of\nmutation and natural selection.\nWe have implemented and run the same simulation\nproposed by Hayashi et al. The model's sexual dynamics can be summarized as follows: there are two different\nsexes (male and female). The likelihood that a female\nwith trait a will2 mate with a male with trait b is defined\nby \u03c8(d) = e\u2212\u03b1d , where d is the genetic or phenotypic distance measuring compatibility between the sexes, and \u03b1\nis a parameter that represents the compatibility between\ntraits. The value of \u03b1 used in the simulations is 0.005.\nThe overall number of offspring produced by a female\n2\nin each coupling is given by Wf = Bmax e\u2212sc(P \u2212Popt ) ,\nwhere P is the proportion of males with which the female has been able to mate. The parameter Popt (which\ncan take the values {0.2, 0.4, 0.6, 0.8}) defines the fertility\nof the female. The parameter sc (ranging from 1.02 to\n4 \u00d7 1.02) stands for sexual conflict selection in females.\nBmax is the maximum possible number of offspring (5).\nThe sex and the father are randomly chosen for each offspring, and the number of males each female encounters\nis n = 20.\nTwenty two different experiments have been performed. K(population sample) and H(genome) have\nbeen estimated for each generation step with the methods\ndescribed above. The system evolved for 10,000 generations with a population size of 1,000 individual genomes.\nIn all our results, both measures were highly correlated\n(see fig. 1), giving evidence that the dual measurement\n\n6000\n\ngenerations\n\nbits\n\nFIG. 1: Histogram of the 22 experimental correlation coefficients.\n\n4000\n\n2\n0\n\n0\n\n2000\n\n4000\n\n6000\n\ngenerations\n\nFIG. 3: Example of differentiation without speciation, obtained with parameters Bmax = 5, Popt = 0.8, \u03bc = 5.0\u00d710\u22125 ,\n\u03b1 = 0.05, sc = 1.02, 8 loci, phenotypic model: co-dominance.\nExperimental correlation coefficient: 0.9741.\n\nof the evolution of entropy by means of Kolmogorov complexity confirms the use of the Shannon entropy.\n\nIII.\n\nDISCUSSION\n\nWhen sexual dynamics are introduced in the system,\nthe increase of complexity observed by [2] is not present\nanymore.\nThe typical result observed in our experiments is a\nchaotic behavior of the entropy (fig. 2). Only large\n\n\f3\ntropy evolution via Kolmogorov Complexity. This overcomes many limitations that arise from epistatic effects\nbetween loci and provide an easy way to do study the dynamics without resorting to complex mathematical trick-\n\nK(population)\n\n4\n\n3\n\nx 10\n\nbits\n\n2\n\nH(genome) female\n\n1\n\nbits\n\n0\n\n1\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n0.5\n\n10000\n0\n\nH(genome)\n\nbits\n\n2000\n\n6000\n\n8000\n\n10000\n\n8000\n\n10000\n\n8000\n\n10000\n\n8000\n\n10000\n\nbits\n\n0.5\n0\n\n10\n\n0\n\n2000\n\n2\n\nbits\n\n5\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\ngenerations\n\ngenome lengths or high female mating rates (Popt ) escape from this (the other parameters seem to have little\nimportance). The effect of this is to increase the autocorrelation of the entropy time series and provoke the\nrise of a few entropy bumps during a small number of\ngenerations (figs. 3 and 4).\nOur hypothesis for this behavior of the entropy is the\nabsence of natural selection in the Hayashi et al. [6]\nmodel, which could explain the similarities between male\nand female evolution (fig. 5). Without natural selection,\nthe environment for females is reduced to random boundary conditions (mutations). On the other hand, males are\nselected by females as mating partners. In this way, females can be considered to become the environment for\nmales, since they determine the way in which the entropy\nof the males evolves. On the other hand, females have no\nenvironment to adapt to. Perhaps if the pressure of natural selection was applied both to male and female (not\nnecessarily in the same way), more complex patterns in\nthe behavior of their entropies would appear. The fact\nthat natural selection is not taken into account may be\nthe cause of the differences in entropy evolution between\nthe models by [2] and [6], and the reason why global decreases in entropy are not observed in the latter.\nCONCLUSSIONS AND FUTURE WORK\n\n0\n\n2000\n\nbits\n\n6000\n\n4000\n\n6000\n\nK(population) male\n\n4\n\n2\n\nFIG. 4: Example of genetic differentiation without coevolutionary chase or simpatryc speciation, obtained with parameters Bmax = 5, Popt = 0.2, \u03bc = 5.0 \u00d7 10\u22125 , \u03b1 = 0.05,\nsc = 1.02, 32 loci, phenotypic model: dominance. Experimental correlation coefficient: 0.8571.\n\nx 10\n\n1\n0\n\n10000\n\n4000\n\nK(population) female\n\n4\n\n0\n\n4000\n\n1\n\n15\n\nIV.\n\n0\n\nH(genome) male\n\n20\n\nx 10\n\n1\n0\n\n0\n\n2000\n\n4000\n\n6000\n\ngenerations\n\nFIG. 5: Same parameters as in fig. 2 with entropy calculation\ndecomposed by sex.\n\neries. We also use this methodology to study what is the\neffect of sexual reproduction in terms of the evolution of\ncomplexity, as a decrease of entropy. We show that when\nsexual reproduction is present the population enters in\na chaotic regime of complexity driven by the complexity drifts of the female organisms. We suggest that this\nmight be cause for female organism evolving chaotically\nwithout natural selection, and male organisms evolving\nwith females as the boundary conditions, which gives an\noverall chaotic evolution of complexity. The next immediate step is to introduce natural selection in the experiments and study whether this will change the evolution\nof complexity. We plan to do it by implementing the typical natural selection operators from genetic algorithms\nsuch as tournament selection, steady state-selection and\nso forth. We conjecture that introducing natural selection will remove the chaotic complexity dynamics and\nmight probably get closer to what Hayasi et. al. formerly\nreported: an increase in complexity by removing genetic\nmutations that do not improve the fitness function.\n\nAcknowledgements\n\nStudying a genetic algorithm from the observer point\nof view allows us to have large-scale estimates of the en-\n\nWe would like to thank Carlos Casta\u00f1eda for his help\nin the implementation and simulations.\n\n[1] C. Adami and N. J. Cerf. Physical complexity of symbolic\nsequences. Phys. D, 137(1-2):62\u201369, 2000.\n\n[2] C. Adami, C. Ofria, and T.C. Collier. Special feature:\n\n\f4\n\n[3]\n\n[4]\n[5]\n[6]\n[7]\n\n[8]\n\nEvolution of biological complexity. Proceedings of the\nNational Academy of Sciences, 97(9):4463\u20134468, 2000.\nG.P. Basharin. On a Statistical Estimate for the Entropy\nof a Sequence of Independent Random Variables. Theory\nof Probability and its Applications, 4:333, 1959.\nC.H. Bennett. The thermodynamics of computation. International Journal of Theoretical Physics, 21(12):905\u2013\n940, 1982.\nT.M. Cover and J.A. Thomas. Elements of information\ntheory. Wiley New York, 1991.\nT.I. Hayashi, M.D. Vose, and S. Gavrilets. Genetic differentiation by sexual conflict. Evolution, 61:516\u2013529(14),\nMarch 2007.\nH. Herzel, W. Ebeling, and A.O. Schmitt. Entropies of\nbiosequences: The role of repeats. Physical Review E, 50\n(6):5061\u20135071, 1994.\nA.N. Kolmogorov. Three approaches to the quantitative\n\n[9]\n[10]\n[11]\n[12]\n\n[13]\n[14]\n\ndefinition of information. International Journal of Computer Mathematics, 2(1):157\u2013168, 1968.\nM. Li and P. Vit\u00e1nyi. An Introduction to Kolmogorov\nComplexity and Its Applications. Springer, 1997.\nM.D. Vose. The Simple Genetic Algorithm: Foundations\nand Theory. MIT Press, 1999.\nA.H. Wright.\nFoundations of genetic algorithms.\nSpringer, 2005.\nJ. Ziv and A. Lempel. Compression of individual sequences via variable-rate coding. Information Theory,\nIEEE Transactions on, 24(5):530\u2013536, Sept. 1978.\nW. H. Zurek. Thermodynamic cost of computation, algorithmic complexity and the information metric. Nature,\n341:119\u2013124, Sept. 1989.\nW.H. Zurek. Algorithmic randomness and physical entropy. Physical Review A, 40(8):4731\u20134751, 1989.\n\n\f"}