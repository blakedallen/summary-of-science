{"id": "http://arxiv.org/abs/0803.0924v3", "guidislink": true, "updated": "2010-02-19T01:47:02Z", "updated_parsed": [2010, 2, 19, 1, 47, 2, 4, 50, 0], "published": "2008-03-06T17:50:07Z", "published_parsed": [2008, 3, 6, 17, 50, 7, 3, 66, 0], "title": "What Can We Learn Privately?", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0803.3653%2C0803.0090%2C0803.4295%2C0803.2199%2C0803.4096%2C0803.2601%2C0803.2115%2C0803.2224%2C0803.3470%2C0803.1420%2C0803.2182%2C0803.2236%2C0803.0503%2C0803.1758%2C0803.0818%2C0803.4277%2C0803.4462%2C0803.3043%2C0803.2696%2C0803.1414%2C0803.2727%2C0803.4162%2C0803.1365%2C0803.3752%2C0803.0632%2C0803.1043%2C0803.0119%2C0803.1245%2C0803.1901%2C0803.2506%2C0803.2275%2C0803.4175%2C0803.0189%2C0803.0924%2C0803.1572%2C0803.0001%2C0803.3120%2C0803.1708%2C0803.0575%2C0803.3059%2C0803.3905%2C0803.1730%2C0803.0361%2C0803.0228%2C0803.3518%2C0803.0305%2C0803.1209%2C0803.2672%2C0803.4498%2C0803.1326%2C0803.3982%2C0803.3255%2C0803.2344%2C0803.0182%2C0803.3855%2C0803.3128%2C0803.2588%2C0803.3347%2C0803.2328%2C0803.2386%2C0803.1597%2C0803.1521%2C0803.0908%2C0803.2796%2C0803.0449%2C0803.3615%2C0803.2931%2C0803.3668%2C0803.0826%2C0803.0930%2C0803.0292%2C0803.0226%2C0803.0431%2C0803.3988%2C0803.0172%2C0803.1591%2C0803.0142%2C0803.1754%2C0803.4360%2C0803.1845%2C0803.4147%2C0803.3742%2C0803.0098%2C0803.0023%2C0803.0811%2C0803.3873%2C0803.0087%2C0803.1814%2C0803.1212%2C0803.0495%2C0803.2697%2C0803.4016%2C0803.4088%2C0803.1102%2C0803.2221%2C0803.4412%2C0803.1510%2C0803.2907%2C0803.2005%2C0803.0533%2C0803.1042&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "What Can We Learn Privately?"}, "summary": "Learning problems form an important category of computational tasks that\ngeneralizes many of the computations researchers apply to large real-life data\nsets. We ask: what concept classes can be learned privately, namely, by an\nalgorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that\nsatisfy differential privacy, a notion that provides strong confidentiality\nguarantees in contexts where aggregate information is released about a database\ncontaining sensitive information about individuals. We demonstrate that,\nignoring computational constraints, it is possible to privately agnostically\nlearn any concept class using a sample size approximately logarithmic in the\ncardinality of the concept class. Therefore, almost anything learnable is\nlearnable privately: specifically, if a concept class is learnable by a\n(non-private) algorithm with polynomial sample complexity and output size, then\nit can be learned privately using a polynomial number of samples. We also\npresent a computationally efficient private PAC learner for the class of parity\nfunctions. Local (or randomized response) algorithms are a practical class of\nprivate algorithms that have received extensive investigation. We provide a\nprecise characterization of local private learning algorithms. We show that a\nconcept class is learnable by a local algorithm if and only if it is learnable\nin the statistical query (SQ) model. Finally, we present a separation between\nthe power of interactive and noninteractive local learning algorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0803.3653%2C0803.0090%2C0803.4295%2C0803.2199%2C0803.4096%2C0803.2601%2C0803.2115%2C0803.2224%2C0803.3470%2C0803.1420%2C0803.2182%2C0803.2236%2C0803.0503%2C0803.1758%2C0803.0818%2C0803.4277%2C0803.4462%2C0803.3043%2C0803.2696%2C0803.1414%2C0803.2727%2C0803.4162%2C0803.1365%2C0803.3752%2C0803.0632%2C0803.1043%2C0803.0119%2C0803.1245%2C0803.1901%2C0803.2506%2C0803.2275%2C0803.4175%2C0803.0189%2C0803.0924%2C0803.1572%2C0803.0001%2C0803.3120%2C0803.1708%2C0803.0575%2C0803.3059%2C0803.3905%2C0803.1730%2C0803.0361%2C0803.0228%2C0803.3518%2C0803.0305%2C0803.1209%2C0803.2672%2C0803.4498%2C0803.1326%2C0803.3982%2C0803.3255%2C0803.2344%2C0803.0182%2C0803.3855%2C0803.3128%2C0803.2588%2C0803.3347%2C0803.2328%2C0803.2386%2C0803.1597%2C0803.1521%2C0803.0908%2C0803.2796%2C0803.0449%2C0803.3615%2C0803.2931%2C0803.3668%2C0803.0826%2C0803.0930%2C0803.0292%2C0803.0226%2C0803.0431%2C0803.3988%2C0803.0172%2C0803.1591%2C0803.0142%2C0803.1754%2C0803.4360%2C0803.1845%2C0803.4147%2C0803.3742%2C0803.0098%2C0803.0023%2C0803.0811%2C0803.3873%2C0803.0087%2C0803.1814%2C0803.1212%2C0803.0495%2C0803.2697%2C0803.4016%2C0803.4088%2C0803.1102%2C0803.2221%2C0803.4412%2C0803.1510%2C0803.2907%2C0803.2005%2C0803.0533%2C0803.1042&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Learning problems form an important category of computational tasks that\ngeneralizes many of the computations researchers apply to large real-life data\nsets. We ask: what concept classes can be learned privately, namely, by an\nalgorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that\nsatisfy differential privacy, a notion that provides strong confidentiality\nguarantees in contexts where aggregate information is released about a database\ncontaining sensitive information about individuals. We demonstrate that,\nignoring computational constraints, it is possible to privately agnostically\nlearn any concept class using a sample size approximately logarithmic in the\ncardinality of the concept class. Therefore, almost anything learnable is\nlearnable privately: specifically, if a concept class is learnable by a\n(non-private) algorithm with polynomial sample complexity and output size, then\nit can be learned privately using a polynomial number of samples. We also\npresent a computationally efficient private PAC learner for the class of parity\nfunctions. Local (or randomized response) algorithms are a practical class of\nprivate algorithms that have received extensive investigation. We provide a\nprecise characterization of local private learning algorithms. We show that a\nconcept class is learnable by a local algorithm if and only if it is learnable\nin the statistical query (SQ) model. Finally, we present a separation between\nthe power of interactive and noninteractive local learning algorithms."}, "authors": ["Shiva Prasad Kasiviswanathan", "Homin K. Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "author_detail": {"name": "Adam Smith"}, "author": "Adam Smith", "arxiv_comment": "35 pages, 2 figures", "links": [{"href": "http://arxiv.org/abs/0803.0924v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0803.0924v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0803.0924v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0803.0924v3", "journal_reference": "SIAM Journal of Computing 40(3) (2011) 793-826", "doi": null, "fulltext": "What Can We Learn Privately?\u2217\nShiva Prasad Kasiviswanathan\u2020\n\nHomin K. Lee\u2021\n\nSofya Raskhodnikova\u00b6\n\nKobbi Nissim\u00a7\n\nAdam Smith\u00b6\n\narXiv:0803.0924v3 [cs.LG] 19 Feb 2010\n\nFebruary 13, 2013\n\nAbstract\nLearning problems form an important category of computational tasks that generalizes many of the\ncomputations researchers apply to large real-life data sets. We ask: what concept classes can be learned\nprivately, namely, by an algorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that satisfy differential privacy, a\nnotion that provides strong confidentiality guarantees in contexts where aggregate information is released\nabout a database containing sensitive information about individuals.\nOur goal is a broad understanding of the resources required for private learning in terms of samples,\ncomputation time, and interaction. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in\nthe cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (non-private) algorithm with polynomial sample complexity\nand output size, then it can be learned privately using a polynomial number of samples. We also present\na computationally efficient private PAC learner for the class of parity functions. This result dispels the\nsimilarity between learning with noise and private learning (both must be robust to small changes in\ninputs), since parity is thought to be very hard to learn given random classification noise.\nLocal (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms.\nWe show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Therefore, for local private learning algorithms, the similarity to learning with\nnoise is stronger: local learning is equivalent to SQ learning, and SQ algorithms include most known\nnoise-tolerant learning algorithms. Finally, we present a separation between the power of interactive\nand noninteractive local learning algorithms. Because of the equivalence to SQ learning, this result also\nseparates adaptive and nonadaptive SQ learning.\n\n1\n\nIntroduction\n\nThe data privacy problem in modern databases is similar to that faced by statistical agencies and medical\nresearchers: to learn and publish global analyses of a population while maintaining the confidentiality of the\n\u2217\n\nA preliminary version of this paper appeared in 49th Annual IEEE Symposium on Foundations of Computer Science [37].\nCCS-3, Los Alamos National Laboratory. Part of this work done while a student at Pennsylvania State University and supported\nby NSF award CCF-072891.\n\u2021\nDepartment of Computer Science, Columbia University, hkl7@columbia.edu\n\u00a7\nDepartment of Computer Science, Ben-Gurion University, kobbi@cs.bgu.ac.il. Supported in part by the Israel Science Foundation (grant 860/06), and by the Frankel Center for Computer Science.\n\u00b6\nDepartment of Computer Science and Engineering, Pennsylvania State University, {sofya,asmith}@cse.psu.edu. S.R. and A.S.\nare supported in part by NSF award CCF-0729171.\n\u2020\n\n1\n\n\fparticipants in a survey. There is a vast body of work on this problem in statistics and computer science.\nHowever, until recently, most schemes proposed in the literature lacked rigorous analysis of privacy and\nutility.\nA recent line of work [29, 26, 11, 24, 22, 21, 47, 25, 44, 7, 48, 14, 27] , initiated by Dinur and Nissim [20]\nand called private data analysis, seeks to place data privacy on firmer theoretical foundations and has been\nsuccessful at formulating a strong, yet attainable privacy definition. The notion of differential privacy [24]\nthat emerged from this line of work provides rigorous guarantees even in the presence of a malicious adversary with access to arbitrary auxiliary information. It requires that whether an individual supplies her actual\nor fake information has almost no effect on the outcome of the analysis.\nGiven this definition, it is natural to ask: what computational tasks can be performed while maintaining\nprivacy? Research on data privacy, to the extent that it formalizes precise goals, has mostly focused on\nfunction evaluation (\"what is the value of f (z)?\"), namely, how much privacy is possible if one wishes to\nrelease (an approximation to) a particular function f , evaluated on the database z. (A notable exception is the\nrecent work of McSherry and Talwar, using differential privacy in the design of auction mechanisms [44]).\nOur goal is to expand the utility of private protocols by examining which other computational tasks can be\nperformed in a privacy-preserving manner.\nPrivate Learning. Learning problems form an important category of computational tasks that generalizes\nmany of the computations researchers apply to large real-life data sets. In this work, we ask what can be\nlearned privately, namely, by an algorithm whose output does not depend too heavily on any one input or\nspecific training example. Our goal is a broad understanding of the resources required for private learning\nin terms of samples, computation time, and interaction. We examine two basic notions from computational\nlearning theory: Valiant's probabilistically approximately correct (PAC) learning [51] model and Kearns'\nstatistical query (SQ) model [39].\nInformally, a concept is a function from examples to labels, and a class of concepts is learnable if for any\ndistribution D on examples, one can, given limited access to examples sampled from D labeled according\nto some target concept c, find a small circuit (hypothesis) which predicts c's labels with high probability\nover future examples taken from the same distribution. In the PAC model, a learning algorithm can access\na polynomial number of labeled examples. In the SQ model, instead of accessing examples directly, the\nlearner can specify some properties (i.e., predicates) on the examples, for which he is given an estimate, up\nto an additive polynomially small error, of the probability that a random example chosen from D satisfies\nthe property. PAC learning is strictly stronger than the SQ learning [39].\nWe model a statistical database as a vector z = (z1 , * * * , zn ), where each entry has been contributed by\nan individual. When analyzing how well a private algorithm learns a concept class, we assume that entries\nzi of the database are random examples generated i.i.d. from the underlying distribution D and labeled by\na target concept c. This is exactly how (not necessarily private) learners are analyzed. For instance, an\nexample might consist of an individual's gender, age, and blood pressure history, and the label, whether this\nindividual has had a heart attack. The algorithm has to learn to predict whether an individual has had a heart\nattack, based on gender, age, and blood pressure history, generated according to D.\nWe require a private algorithm to keep entire examples (not only the labels) confidential. In the scenario\nabove, it translates to not revealing each participant's gender, age, blood pressure history, and heart attack\nincidence. More precisely, the output of a private learner should not be significantly affected if a particular example zi is replaced with arbitrary zi0 , for all zi and zi0 . In contrast to correctness or utility, which\nis analyzed with respect to distribution D, differential privacy is a worst-case notion. Hence, when we\nanalyze the privacy of our learners we do not make any assumptions on the underlying distribution. Such as-\n\n2\n\n\fsumptions are fragile and, in particular, would fall apart in the presence of auxiliary knowledge (also called\nbackground knowledge or side information) that the adversary might have: conditioned on the adversary's\nauxiliary knowledge, the distribution over examples might look very different from D.\n\n1.1\n\nOur Contributions\n\nWe introduce and formulate private learning problems, as discussed above, and develop novel algorithmic\ntools and bounds on the sample size required by private learning algorithms. Our results paint a picture of\nthe classes of learning problems that are solvable subject to privacy constraints. Specifically, we provide:\n(1) A Private Version of Occam's Razor. We present a generic private learning algorithm. For any concept\nclass C, we give a distribution-free differentially-private agnostic PAC learner for C that uses a number\nof samples proportional to log |C|. This is a private analogue of the \"cardinality version\" of Occam's\nrazor, a basic sample complexity bound from (non-private) learning theory. The sample complexity\nof our version is similar to that of the original, although the private algorithm is very different. As in\nOccam's razor, the learning algorithm is not necessarily computationally efficient.\n(2) An Efficient Private Learner for Parity. We give a computationally efficient, distribution-free differentially private PAC learner for the class of parity functions1 over {0, 1}d . The sample and time\ncomplexity are comparable to that of the best non-private learner.\n(3) Equivalence of Local (\"Randomized Response\") and SQ Learning. We precisely characterize the\npower of local, or randomized response, private learning algorithms. Local algorithms are a special\n(practical) class of private algorithms and are popular in the data mining and statistics literature [53, 2,\n1, 3, 52, 29, 45, 36]. They add randomness to each individual's data independently before processing the\ninput. We show that a concept class is learnable by a local differentially private algorithm if and only if\nit is learnable in the statistical query (SQ) model. This equivalence relates notions that were conceived\nin very different contexts.\n(4) Separation of Interactive and Noninteractive Local Learning. Local algorithms can be noninteractive, that is, using one round of interaction with individuals holding the data, or interactive, that is, using\nmore than one round (and in each receiving randomized responses from individuals). We construct a\nconcept class, called masked-parity, that is efficiently learnable by interactive local algorithms under the\nuniform distribution on examples, but requires an exponential (in the dimension) number of samples to\nbe learned by a noninteractive local algorithm. The equivalence (3) of local and SQ learning shows that\ninteraction in local algorithms corresponds to adaptivity in SQ algorithms. The masked-parity class thus\nalso separates adaptive and nonadaptive SQ learning.\n1.1.1\n\nImplications\n\n\"Anything\" learnable is privately learnable using few samples. The generic agnostic learner (1) has an\nimportant consequence: if some concept class C is learnable by any algorithm, not necessarily a private one,\nwhose output length in bits is polynomially bounded, then C is learnable privately using a polynomial number of samples (possibly in exponential time). This result establishes the basic feasibility of private learning:\nit was not clear a priori how severely privacy affects sample complexity, even ignoring computation time.\n1\nWhile the generic learning result (1) extends easily to \"agnostic\" learning (defined below), the learner for parity does not. The\nlimitation is not surprising, since even non-private agnostic learning of parity is at least as hard as learning parity with random\nnoise.\n\n3\n\n\fx1\n\nx1\nx.2\n\nA\n\n..\n\nxn\n\nx2\n\nUser\n\n..\n.\n\nxn\n\n(a)\n\nR\n\nUser\n\nR\nR\n\nA\n\n(b)\n\nFigure 1: Two basic models for database privacy: (a) the centralized model, in which data is collected by a trusted\nagency that publishes aggregate statistics or answers users' queries; (b) the local model, in which users retain their\ndata and run a randomization procedure locally to produce output which is safe for publication. The dotted arrows\nfrom users to data holders indicate that protocols may be completely noninteractive: in this case there is a single\npublication, without feedback from users.\n\nLearning with noise is different from private learning. There is an intuitively appealing similarity between learning from noisy examples and private learning: algorithms for both problems must be robust to\nsmall variations in the data. This apparent similarity is strengthened by a result of Blum, Dwork, McSherry\nand Nissim [11] showing that any algorithm in Kearns' statistical query (SQ) model [39] can be implemented in a differentially private manner. SQ was introduced to capture a class of noise-resistant learning\nalgorithms. These algorithms access their input only through a sequence of approximate averaging queries.\nOne can privately approximate the average of a function with values in [0, 1] over the data set of n individuals to within additive error O(1/n) (Dwork and Nissim [26]). Thus, one can simulate the behavior of an SQ\nalgorithm privately, query by query.\nOur efficient private learner for parity (2) dispels the similarity between learning with noise and private\nlearning. First, SQ algorithms provably require exponentially many (in the dimension) queries to learn\nparity [39]. More compellingly, learning parity with noise is thought to be computationally hard, and has\nbeen used as the basis of several cryptographic primitives (e.g., [13, 35, 4, 49]).\nLimitations of local (\"randomized response\") algorithms. Local algorithms (also referred to as randomized response, input perturbation, Post Randomization Method (PRAM), and Framework for HighAccuracy Strict-Privacy Preserving Mining (FRAPP)) have been studied extensively in the context of privacypreserving data mining, both in statistics and computer science (e.g., [53, 2, 1, 3, 52, 29, 45, 36]). Roughly,\na local algorithm accesses each individual's data via independent randomization operators. See Figure 1,\np. 4.\nLocal algorithms were introduced to encourage truthfulness in surveys: respondents who know that\ntheir data will be randomized are more likely to answer honestly. For example, Warner [53] famously\nconsidered a survey technique in which respondents are asked to give the correct answer to a sensitive\n(true/false) question with probability 2/3 and the incorrect answer with probability 1/3, in the hopes that\nthe added uncertainty would encourage them to answer honestly. The proportion of \"true\" answers in the\npopulation is then estimated using a standard, non-private deconvolution. The accepted privacy requirement\nfor local algorithms is equivalent to imposing differential privacy on each randomization operator [29].\nLocal algorithms are popular because they are easy to understand and implement. In the extreme case, users\ncan retain their data and apply the randomization operator themselves, using a physical device [53, 46] or a\ncryptographic protocol [5].\nThe equivalence between local and SQ algorithms (3) is a powerful tool that allows us to apply results\n4\n\n\ffrom learning theory. In particular, since parity is not learnable with a small number of SQ queries [39]\nbut is PAC learnable privately (2), we get that local algorithms require exponentially more data for some\nlearning tasks than do general private algorithms. Our results also imply that local algorithms are strictly\nless powerful than (non-private) algorithms for learning with classification noise because subexponential\n(non-private) algorithms can learn parity with noise [13].\nAdaptivity in SQ algorithms is important. Just as local algorithms can be interactive, SQ algorithms\ncan be adaptive, that is, the averaging queries they make may depend on answers to previous queries. The\nequivalence of SQ and local algorithms (3) preserves interaction/adaptivity: a concept class is nonadaptively\nSQ learnable if and only if it is noninteractively locally learnable. The masked parity class (4) shows that\ninteraction (resp., adaptivity) adds considerable power to local (resp., SQ) algorithms.\nMost of the reasons that local algorithms are so attractive in practice, and have received such attention,\napply only to noninteractive algorithms (interaction can be costly, complicated, or even impossible-for\ninstance, when statistical information is collected by an interviewer, or at a polling booth).\nThis suggests that further investigating the power of nonadaptive SQ learners is an important problem.\nFor example, the SQ algorithm for learning conjunctions [42] is nonadaptive, but SQ formulations of the\nperceptron and k-means algorithms [11] seem to rely heavily on adaptivity.\nUnderstanding the \"price\" of privacy for learning problems. The SQ result of Blum et al. [11] and our\nlearner for parity (2) provide efficient (i.e., polynomial time) private learners for essentially all the concept\nclasses known (by us) to have efficient non-private distribution-free learners. Finding a concept class that\ncan be learned efficiently, but not privately and efficiently, remains an interesting and important question.\nOur results also lead to questions of optimal sample complexity for learning problems of practical importance.\nThe private simulation of SQ algorithms due to Blum et al. [11] uses a factor of approximately\n\u221a\nt/\u000f more data points than the na\u0131\u0308ve non-private implementation, where t is the number of SQ queries and\n\u000f is the parameter of differential privacy (typically a small constant). In contrast, the generic agnostic learner\n(1) uses a factor of at most 1/\u000f more samples than the corresponding non-private learner. For parity, our\nprivate learner uses a factor of roughly 1/\u000f more samples than, and about the same computation time as,\nthe non-private learner. What, then, is the additional cost of privacy when learning practical concept classes\n(half-planes, low-dimensional curves, etc)? Can the theoretical sample bounds of (1) be matched by (more)\nefficient learners?\n1.1.2\n\nTechniques\n\nOur generic private learner (1) adapts the exponential sampling technique of McSherry and Talwar [44],\ndeveloped in the context of auction design. Our use of the exponential mechanism inspired an elegant\nsubsequent result of Blum, Liggett, and Roth [14] (BLR) on simultaneously approximating many different\nfunctions.\nThe efficient private learner for parity (2) uses a very different technique, based on sampling, running a\nnon-private learner, and occasionally refusing to answer based on delicately calibrated probabilities. Running a non-private learner on a random subset of examples is a very intuitive approach to building private\nalgorithms, but it is not private in general. The private learner for parity illustrates both why this technique\ncan leak private information and how it can sometimes be repaired based on special (in this case, algebraic)\nstructure.\n\n5\n\n\fThe interesting direction of the equivalence between SQ and local learners (3) is proved via a simulation\nof any local algorithm by a corresponding SQ algorithm. We found this simulation surprising since local\nprotocols can, in general, have very complex structure (see, e.g., [29]). The SQ algorithm proceeds by a\ndirect simulation of the output of the randomization operators. For a given input distribution D and any\noperator R, one can sample from the corresponding output distribution R(D) via rejection sampling. We\nshow that if R is differentially private, the rejection probabilities can be approximated via low-accuracy SQ\nqueries to D.\nFinally, the separation between adaptive and nonadaptive SQ (4) uses a Fourier analytic argument inspired by Kearns' SQ lower bound for parity [39].\n1.1.3\n\nClasses of Private Learning Algorithms\n\nPPAC\u2217 = PAC\u2217\nLI\u2217 = SQ\u2217\n\nPARITY\nMASKED-PARITY\n\nLNI\u2217 = NASQ\u2217\nFigure 2: Relationships among learning classes taking into account sample complexity, but not computational efficiency.\n\nWe can summarize our results via a complexity-theoretic picture of learnable and privately learnable\nconcept classes (more precisely, the members of the classes are pairs of concept classes and example distributions). In order to make asymptotic statements, we measure complexity in terms of the length d of the\nbinary description of examples.\nWe first consider learners that use a polynomial (in d) number of samples and output a hypothesis that is\ndescribed using a polynomial number of bits, but have unlimited computation time. Let PAC\u2217 denote the set\nof concept classes that are learnable by such algorithms ignoring privacy, and let PPAC\u2217 denote the subset\nof PAC\u2217 learnable by differentially private2 algorithms.\nSince we restrict the learner's output to a polynomial number of bits, the hypothesis classes of the\nalgorithms are de facto limited to have size at most exp(poly(d)). Thus, the generic private learner (point\n(1) in the introduction) will use a polynomial number of samples, and PAC\u2217 = PPAC\u2217 .\nWe can similarly interpret the other results above. Within PAC\u2217 , we can consider subsets of concepts\nlearnable by SQ algorithms (SQ\u2217 ), nonadaptive SQ algorithms (NASQ\u2217 ), local interactive algorithms (LI\u2217 )\nand local noninteractive algorithms (LNI\u2217 ). We obtain the following picture (see page 6):\nLNI\u2217 = NASQ\u2217\n\n(\n\nLI\u2217 = SQ\u2217\n\n(\n\nPPAC\u2217 = PAC\u2217 .\n\nThe equality of LI\u2217 and SQ\u2217 , and of LNI\u2217 and NASQ\u2217 , follow from the SQ simulation of local algorithms\n(Theorem 5.14). The parity and masked-parity concept classes separate PPAC\u2217 from SQ\u2217 and SQ\u2217 from\nNASQ\u2217 , respectively (Corollaries 5.15 and 5.17). (Note: The separation of PPAC\u2217 from SQ\u2217 holds even\nfor distribution-free learning; in contrast, the separation of SQ\u2217 from NASQ\u2217 holds for learnability under a\n2\n\nDifferential privacy is quantified by a real parameter \u000f > 0. To make qualitative statements, we look at algorithms where\n\u000f \u2192 0 as d \u2192 \u221e. Taking \u000f = 1/dc for any constant c > 0 would yield the same class.\n\n6\n\n\fspecific distribution on examples, since the adaptive SQ learner for MASKED-PARITY requires a uniform\ndistribution on examples.)\nWhen we take computational efficiency into account, the picture changes. The relation between local\nand SQ classes remain the same modulo a technical restriction on the randomization operators (Definition 5.13). SQ remains distinct from PPAC since parity is efficiently learnable privately. However, it is\nan open question whether concept classes which can be efficiently learned can also be efficiently learned\nprivately.\n\n1.2\n\nRelated Work\n\nPrior to this work, the literature on differential privacy studied function approximation tasks (e.g. [20,\n26, 11, 24, 47, 7]), with the exception of the work of McSherry and Talwar on mechanism design [44].\nNevertheless, several of these prior results have direct implications to machine learning-related problems.\nBlum et al. [11] considered a particular class of learning algorithms (SQ), and showed that algorithms in the\nclass could be simulated using noisy function evaluations. In an independent, unpublished work, Chaudhuri,\nDwork, and Talwar considered a version of private learning in which privacy is afforded only to input\nlabels, but not to examples. Other works considered specific machine learning problems such as mining\nfrequent itemsets [29], k-means clustering [11, 47], learning decision trees [11], and learning mixtures of\nGaussians [47].\nAs mentioned above, a subsequent result of Blum, Ligett and Roth [14] on approximating classes of\nlow-VC-dimension functions was inspired by our generic agnostic learner. We discuss their result further\nin Section 3.1. Since the original version of our work, there have also been several results connecting\ndifferential privacy to more \"statistical\" notions of utility, such as consistency of point estimation and density\nestimation [50, 23, 54, 56].\nOur separation of interactive and noninteractive protocols in the local model (3) also has a precedent:\nDwork et al. [24] separated interactive and noninteractive private protocols in the centralized model, where\nthe user accesses the data via a server that runs differentially private algorithms on the database and sends\nback the answers. That separation has a very different flavor from the one in this work: any example of a\ncomputation that cannot be performed noninteractively in the centralized model must rely on the fact that the\ncomputational task is not defined until after the first answer from the server is received. (Otherwise, the user\ncan send an algorithm for that task to the server holding the data, thus obviating the need for interaction.) In\ncontrast, we present a computational task that is hard for noninteractive local algorithms \u2013 learning masked\nparity \u2013 yet is defined in advance.\nIn the machine learning literature, several notions similar to differential privacy have been explored\nunder the rubric of \"algorithmic stability\" [19, 40, 16, 43, 28, 9]. The most closely related notion is changeone error stability, which measures how much the generalization error changes when an input is changed\n(see the survey [43]). In contrast, differential privacy measures how the distribution over the entire output\nchanges-a more complex measure of stability (in particular, differential privacy implies change-one error\nstability). A different notion, stability under resampling of the data from a given distribution [10, 9], is connected to the sample-and-aggregate method of [47] but is not directly relevant to the techniques considered\nhere. Finally, in a different vein, Freund, Mansour and Schapire [31] used a weighted averaging technique\nwith the same weights as the sampler in our generic learner to reduce generalization error (see Section 3.1).\n\n7\n\n\f2\n\nPreliminaries\n\nWe use [n] to denote the set {1, 2, . . . , n}. Logarithms base 2 and base e are denoted by log and ln, respectively. Pr[*] and E[*] denote probability and expectation, respectively. A(x) is the probability distribution\nover outputs of a randomized algorithm A on input x. The statistical difference between distributions P and\nQ on a discrete space D is defined as maxS\u2282D | P (S) \u2212 Q(S)|.\n\n2.1\n\nDifferential Privacy\n\nA statistical database is a vector z = (z1 , . . . , zn ) over a domain D, where each entry zi \u2208 D represents\ninformation contributed by one individual. Databases z and z0 are neighbors if zi 6= zi0 for exactly one\ni \u2208 [n] (i.e., the Hamming distance between z and z0 is 1). All our algorithms are symmetric, that is, they do\nnot depend on the order of entries in the database z. Thus, we could define a database as a multi-set in D,\nand use symmetric difference instead of the Hamming metric to measure distance. We adhere to the vector\nformulation for consistency with the previous works.\nA (randomized) algorithm (in our context, this will usually be a learning algorithm) is private if neighboring databases induce nearby distributions on its outcomes:\nDefinition 2.1 (\u000f-differential privacy [24]). A randomized algorithm A is \u000f-differentially private if for all\nneighboring databases z, z0 , and for all sets S of outputs,\nPr[A(z) \u2208 S] \u2264 exp(\u000f) * Pr[A(z0 ) \u2208 S].\nThe probability is taken over the random coins of A.\nIn [24], the notion above was called \"indistinguishability\". The name \"differential privacy\" was suggested by Mike Schroeder, and first appeared in Dwork [21].\nDifferential privacy composes well (see, e.g., [22, 47, 44, 38]):\nClaim 2.2 (Composition and Post-processing). If a randomized algorithm A runs k algorithms A1 , ..., Ak ,\nwhere each Ai is \u000fi -differentially private, and outputs a functionP\nof the results (that is, A(z) = g(A1 (z),\nA2 (z), ..., Ak (z)) for some probabilistic algorithm g), then A is ( ki=1 \u000fi )-differentially private.\nOne method for obtaining efficient differentially private algorithms for approximating real-valued functions is based on adding Laplacian noise to\u221athe true answer. Let Lap(\u03bb) denote the Laplace probability\n1 \u2212|x|/\u03bb\ndistribution with mean 0, standard deviation 2\u03bb, and p.d.f. f (x) = 2\u03bb\ne\n.\nTheorem 2.3 (Dwork et al. [24]). For a function f : Dn \u2192 R, define its global sensitivity GSf =\nmaxz,z0 |f (z) \u2212 f (z0 )| where the maximum is over all neighboring databases z, z0 . Then, an algorithm\nthat on input z returns f (z) + \u03b7 where \u03b7 \u223c Lap(GSf /\u000f) is \u000f-differentially private.\n\n2.2\n\nPreliminaries from Learning Theory\n\nA concept is a function that labels examples taken from the domain X by the elements of the range Y .\nA concept class C is a set of concepts. It comes implicitly with a way to represent concepts; size(c) is\nthe size of the (smallest) representation of c under the given representation scheme. The domain and the\nrange of the concepts in C are understood to be ensembles X = {Xd }d\u2208N and Y = {Yd }d\u2208N , where the\nrepresentation of elements in Xd , Yd is of size at most d. We focus on binary classification problems, in\nwhich the label space Yd is {0, 1} or {+1, \u22121}; the parameter d thus measures the size of the examples\n8\n\n\fin Xd . (We use the parameter d to formulate asymptotic complexity notions.) The concept classes are\nensembles C = {Cd }d\u2208N where Cd is the class of concepts from Xd to Yd . When the size parameter is clear\nfrom the context or not important, we omit the subscript in Xd , Yd , Cd .\nLet D be a distribution over labeled examples in Xd \u00d7 Yd . A learning algorithm is given access to D (the\nmethod for accessing D depends on the type of learning algorithm). It outputs a hypothesis h : Xd \u2192 Yd\nfrom a hypothesis class H = {Hd }d\u2208N . The goal is to minimize the misclassification error of h on D,\ndefined as\nerr(h) = Pr [h(x) 6= y] .\n(x,y)\u223cD\n\nThe success of a learning algorithm is quantified by parameters \u03b1 and \u03b2, where \u03b1 is the desired error\nand \u03b2 bounds the probability of failure to output a hypothesis with this error. Error measures other than\nmisclassification are considered in supervised learning (e.g., L22 ). We study only misclassification error\nhere, since for binary labels it is equivalent to the other common error measures.\nA learning algorithm is usually given access to an oracle that produces i.i.d. samples from D. Equivalently, one can view the learning algorithm's input as a list of n labeled examples, i.e., z \u2208 Dn where\nD = Xd \u00d7 Yd . PAC learning and agnostic learning are described in Definitions 2.4 and 2.5. Another\ncommon method of access to D is via \"statistical queries\", which return the approximate average of a function over the distribution. Algorithms that work in this model can be simulated given i.i.d. examples. See\nSection 5.\nPAC learning algorithms are frequently designed assuming a promise that the examples are labeled\nconsistently with some target concept c from a class C: namely, c \u2208 Cd and y = c(x) for all (x, y) in the\nsupport of D. In that case, we can think of D as a distribution only over examples Xd . To avoid ambiguity,\nwe use X to denote a distribution over Xd . In the PAC setting, err(h) = Prx\u223cX [h(x) 6= c(x)].\nDefinition 2.4 (PAC Learning). A concept class C over X is PAC learnable using hypothesis class H if\nthere exist an algorithm A and a polynomial poly(*, *, *) such that for all d \u2208 N, all concepts c \u2208 Cd ,\nall distributions X on Xd , and all \u03b1, \u03b2 \u2208 (0, 1/2), given inputs \u03b1, \u03b2 and z = (z1 , * * * , zn ), where n =\npoly(d, 1/\u03b1, log(1/\u03b2)), zi = (xi , c(xi )) and xi are drawn i.i.d. from X for i \u2208 [n], algorithm A outputs a\nhypothesis h \u2208 H satisfying\nPr[err(h) \u2264 \u03b1] \u2265 1 \u2212 \u03b2.\n\n(1)\n\nThe probability is taken over the random choice of the examples z and the coin tosses of A.\nClass C is (inefficiently) PAC learnable if there exists some hypothesis class H and a PAC learner A such\nthat A PAC learns C using H. Class C is efficiently PAC learnable if A runs it time polynomial in d, 1/\u03b1,\nand log(1/\u03b2).\nRemark: Our definition deviates slightly from the standard one (see, e.g., [42]) in that we do not take into\nconsideration the size of the concept c. This choice allows us to treat PAC learners and agnostic learners\nidentically. One can change Definition 2.4 so that the number of samples depends polynomially also on the\nsize of c without affecting any of our results significantly.\nAgnostic learning [32, 41] is an extension of PAC learning that removes assumptions about the target\nconcept. Roughly speaking, the goal of an agnostic learner for a concept class C is to output a hypothesis\nh \u2208 H whose error with respect to the distribution is close to the optimal possible by a function from C. In\nthe agnostic setting, err(h) = Pr(x,y)\u223cD [h(x) 6= y].\n\n9\n\n\fDefinition 2.5 (Agnostic Learning). (Efficiently) agnostically learnable is defined identically to (efficiently)\nPAC learnable with two exceptions: (i) the data are drawn from an arbitrary distribution D on Xd \u00d7 Yd ;\n(ii) instead of Equation 1, the output of A has to satisfy:\nPr[err(h) \u2264 OP T + \u03b1] \u2265 1 \u2212 \u03b2,\nwhere OP T = minf \u2208Cd {err(f )} . As before, the probability is taken over the random choice of z, and the\ncoin tosses of A.\nDefinitions 2.4 and 2.5 capture distribution-free learning, in that they do not assume a particular form\nfor the distributions X or D. In Section 5.3, we also consider learning algorithms that assume a specific\ndistribution D on examples (but make no assumption on which concept in C labels the examples). When we\ndiscuss such algorithms, we specify D explicitly; without qualification, \"learning\" refers to distribution-free\nlearning.\nEfficiency Measures. The definitions above are sufficiently detailed to allow for exact complexity statements (e.g., \"A learns C using n(\u03b1, \u03b2) examples and time O(t)\"), and the upper and lower bounds in this\npaper are all stated in this language. However, we also focus on two broader measures to allow for qualitative\nstatements: (a) polynomial sample complexity is the default notion in our definitions. With the novel restriction of privacy, it is not a priori clear which concept classes can be learned using few examples even if we\nignore computation time. (b) We use the term efficient private learning to impose the additional restriction\nof polynomial computation time (which implies polynomial sample complexity).\n\n3\n\nPrivate PAC and Agnostic Learning\n\nWe define private PAC learners as algorithms that satisfy definitions of both differential privacy and PAC\nlearning. We emphasize that these are qualitatively different requirements. Learning must succeed on\naverage over a set of examples drawn i.i.d. from D (often under the additional promise that D is consistent\nwith a concept from a target class). Differential privacy, in contrast, must hold in the worst case, with no\nassumptions on consistency.\nDefinition 3.1 (Private PAC Learning). Let d, \u03b1, \u03b2 be as in Definition 2.4 and \u000f > 0. Concept class C is\n(inefficiently) privately PAC learnable using hypothesis class H if there exists an algorithm A that takes\ninputs \u000f, \u03b1, \u03b2, z, where n, the number of labeled examples in z, is polynomial in 1/\u000f, d, 1/\u03b1, log(1/\u03b2), and\nsatisfies\na. [Privacy] For all \u000f > 0, algorithm A(\u000f, *, *, *) is \u000f-differentially private (Definition 2.1);\nb. [Utility] Algorithm A PAC learns C using H (Definition 2.4).\nC is efficiently privately PAC learnable if A runs in time polynomial in d, 1/\u000f, 1/\u03b1, and log(1/\u03b2).\nDefinition 3.2 (Private Agnostic Learning). (Efficient) private agnostic learning is defined analogously to\n(efficient) private PAC learning with Definition 2.5 replacing Definition 2.4 in the utility condition.\nEvaluating the quality of a particular hypothesis is easy: one can privately compute the fraction of the\ndata it classifies correctly (enabling cross-validation) using the sum query framework of [11]. The difficulty\nof constructing private learners lies in finding a good hypothesis in what is typically an exponentially large\nspace.\n10\n\n\f3.1\n\nA Generic Private Agnostic Learner\n\nIn this section, we present a private analogue of a basic consistent learning result, often called the cardinality\nversion of Occam's razor3 . This classical result shows that a PAC learner can weed out all bad hypotheses\ngiven a number of labeled examples that is logarithmic in the size of the hypothesis class (see [42, p. 35]).\nOur generic private learner is based on the exponential mechanism of McSherry and Talwar [44].\nLet q : Dn \u00d7 Hd \u2192 R take a database z and a candidate hypothesis h, and assign it a score q(z, h) =\n\u2212|{i : xi is misclassified by h, i.e., yi 6= h(xi )}| . That is, the score is minus the number of points in z\nmisclassified by h. The classic Occam's razor argument assumes a learner that selects a hypothesis with\nmaximum score (that is, minimum empirical error). Instead, our private learner A\u000fq is defined to sample a\nrandom hypothesis with probability dependent on its score:\n\u0010\n\u0011\nA\u000fq (z) : Output hypothesis h \u2208 Hd with probability proportional to exp \u000fq(z,h)\n.\n2\nSince the score ranges from \u2212n to 0, hypotheses with low empirical error are exponentially more likely to\nbe selected than ones with high error.\nAlgorithm A\u000fq fits the framework of McSherry and Talwar, and so is \u000f-differentially private. This follows\nfrom the fact that changing one entry zi in the database z can change the score by at most 1.\nLemma 3.3 (following [44]). The algorithm A\u000fq is \u000f-differentially private.\nA similar exponential weighting algorithm was considered by Freund, Mansour and Schapire [31] for\nconstructing binary classifiers with good generalization error bounds. We are not aware of any direct connection between the two results. Also note that, except for the case where |Hd | is polynomial, the exponential\nmechanism A\u000fq (z) does not necessarily yield a polynomial time algorithm.\nTheorem 3.4 (Generic Private Learner). For all d \u2208 N, any concept class Cd whose cardinality is at most\nexp(poly(d)) is privately agnostically learnable using Hd = Cd . More precisely, the learner uses n =\n1\nO((ln |Hd | + ln \u03b21 ) * max{ \u000f\u03b1\n, \u03b112 }) labeled examples from D, where \u000f, \u03b1, and \u03b2 are parameters of the\nprivate learner. (The learner might not be efficient.)\nProof. Let A\u000fq be as defined above. The privacy condition in Definition 3.1 is satisfied by Lemma 3.3.\nWe now show that the utility condition is also satisfied. Consider the event E = {A\u000fq (z) = h with err(h) >\n\u03b1 + OP T }. We want to prove that Pr[E] \u2264 \u03b2. Define the training error of h as\nerrT (h) = {i \u2208 [n] | h(xi ) 6= yi } /n = \u2212q(z, h)/n .\nBy Chernoff-Hoeffding bounds (see Theorem A.2 in Appendix A),\n\u0002\n\u0003\nPr |err(h) \u2212 errT (h)| \u2265 \u03c1 \u2264 2 exp(\u22122n\u03c12 )\nfor all hypotheses h \u2208 Hd . Hence,\n\u0002\n\u0003\nPr |err(h) \u2212 errT (h)| \u2265 \u03c1 for some h \u2208 Hd \u2264 2|Hd | exp(\u22122n\u03c12 ).\n3\n\nWe discuss the relationship to the \"compression version\" of Occam's razor at the end of this section.\n\n11\n\n\fWe now analyze A\u000fq (z) conditioned on the event that for all h \u2208 Hd , |err(h) \u2212 errT (h)| < \u03c1. For every\nh \u2208 Hd , the probability that A\u000fq (z) = h is\nexp(\u2212 2\u000f * n * errT (h))\nP\n\u000f\n0\nh0 \u2208Hd exp(\u2212 2 * n * errT (h ))\n\n\u0001\nexp \u2212 2\u000f * n * errT (h)\n\u2264\nmaxh0 \u2208Hd exp(\u2212 2\u000f * n * errT (h0 ))\n\u0012\n\u0013\n\u000f\n0\n= exp \u2212 * n * (errT (h) \u2212 min\nerrT (h ))\nh0 \u2208Hd\n2\n\u0011\n\u0010 \u000f\n\u2264 exp \u2212 * n * (errT (h) \u2212 (OP T + \u03c1)) .\n2\n\nHence, the probability that A\u000fq (z) outputs a hypothesis h \u2208 Hd such that errT (h) \u2265 OP T + 2\u03c1 is at most\n|Hd | exp(\u2212\u000fn\u03c1/2).\nNow set \u03c1 = \u03b1/3. If err(h) \u2265 OP T + \u03b1 then |err(h) \u2212 errT (h)| \u2265 \u03b1/3 or errT (h) \u2265 OP T + 2\u03b1/3.\n2\nThus\n\u0010 Pr[E] \u2264 |Hd |(2 exp(\u22122n\u03b1\u0011/9) + exp(\u2212\u000fn\u03b1/6)) \u2264 \u03b2 where the last inequality holds for n \u2265\n1\n6 (ln |Hd | + ln \u03b21 ) * max{ \u000f\u03b1\n, \u03b112 } .\nRemark: In the non-private agnostic case, the standard Occam's razor bound guarantees that O((log |Cd | +\nlog(1/\u03b2))/\u03b12 ) labeled examples suffice to agnostically learn a concept class Cd . The bound of Theorem 3.4\ndiffers by a factor of O( \u03b1\u000f ) if \u03b1 > \u000f, and does not differ at all otherwise. For (non-agnostic) PAC learning,\nthe dependence on \u03b1 in the sample size for both the private and non-private versions improves to 1/\u03b1. In\nthat case the upper bounds for private and non-private learners differ by a factor of O(1/\u000f). Finally, the\ntheorem can be extended to settings where Hd 6= Cd , but in this case using the same sample complexity the\nlearner outputs a hypothesis whose error is close to the best error attainable by a function in Hd .\nImplications of the Private Agnostic Learner The private agnostic learner has the following important\nconsequence: If some concept class Cd is learnable by any algorithm A, not necessarily a private one,\nand A's output length in bits is polynomially bounded, then there is a (possibly exponential time) private\nalgorithm that learns Cd using a polynomial number of samples. Since A's output is polynomially long, A's\nhypothesis class Hd must have size at most 2poly(d) . Since A learns Cd using Hd , class Hd must contain a\ngood hypothesis. Thus, our private learner will learn Cd using Hd with sample complexity linear in log |Hd |.\nThe \"compression version\" of Occam's razor It is most natural to state our result as an analogue of\nthe cardinality version of Occam's razor, which bounds generalization error in terms of the size of the\nhypothesis class. However, our result can be extended to the compression version, which captures the\ngeneral relationship between compression and learning (we borrow the \"cardinality version\" terminology\nfrom [42]). This latter version states that any algorithm which \"compresses\" the data set, in the sense that it\nfinds a consistent hypothesis which has a short description relative to the number of samples seen so far, is\na good learner (see [15] and [42, p. 34]).\nCompression by itself does not imply privacy, because the compression algorithm's output might encode\na few examples in the clear (for example, the hyperplane output by a support vector machine is defined\nvia a small number of actual data points). However, Theorem 3.4 can be extended to provide a private\nanalogue of the compression version of Occam's razor. If there exists an algorithm that compresses, in\nthe sense above, then there also exists a private PAC learner which does not have fixed sample complexity,\nbut uses an expected number of samples similar to that of the compression algorithm. The private learner\nproceeds in rounds: at each round it requests twice as many examples as in the previous round, and uses a\n12\n\n\frestricted hypothesis class consisting of sufficiently concise hypotheses from the original class H. We omit\nthe straightforward details.\n\n3.2\n\nPrivate Learning with VC dimension Sample Bounds\n\nIn the non-private case one can also bound the sample size of a PAC learner in terms of the VapnikChervonenkis (VC) dimension of the concept class.\nDefinition 3.5 (VC dimension). A set S \u2286 Xd is shattered by a concept class Cd if Cd restricted to S\ncontains all 2|S| possible functions from S to {0, 1}. The VC dimension of Cd , denoted V CDIM (Cd ), is\nthe cardinality of a largest set S shattered by Cd .\nWe can extend Theorem 3.4 to classes with finite VC dimension, but the resulting sample complexity\nalso depends logarithmically on the size of the domain from which examples are drawn. Recent results\nof Beimel et al. [8] show that for \"proper\" learning, the dependency is in fact necessary; that is, the VC\ndimension alone is not sufficient to bound the sample complexity of proper private learning. It is unclear if\nthe dependency is necessary in general.\nCorollary 3.6. Every concept class Cd is privately agnostically learnable using hypothesis class Hd = Cd\n1\nwith n = O((V CDIM (Cd ) * ln |Xd | + ln \u03b21 ) * max{ \u000f\u03b1\n, \u03b112 }) labeled examples from D. Here, \u000f, \u03b1, and \u03b2\nare parameters of the private agnostic learner, and V CDIM (Cd ) is the VC dimension of Cd . (The learner\nis not necessarily efficient.)\nProof. Sauer's lemma (see, e.g., [42]) implies that there are O(|Xd |V CDIM (Cd ) ) different labelings of Xd\nby functions in Cd . We can thus run the generic learner of the previous section with a hypothesis class of\nsize |Hd | = O(|Xd |V CDIM (Cd ) ). The statement follows directly.\nOur original proof of the corollary used a result of Blum, Ligget and Roth [14] (which was inspired, in\nturn, by our generic learning algorithm) on generating synthetic data. The simpler proof above was pointed\nout to us by an anonymous reviewer.\nRemark: Computability Issues with Generic Learners In their full generality, the generic learning\nresults of the previous sections (Theorems 3.4 and 3.6) produce well-defined randomized maps, but not necessarily \"algorithms\" in the sense of \"functions uniformly computable by Turing machines\". This is because\nthe concept class and example domain may themselves not be computable (nor even recognizable) uniformly\n(imagine, for example, a concept class indexed by elements of the halting problem). It is commonly assumed\nin the learning literature that elements of the concept class and domain can be computed/recognized by a\nTuring machine and some bound on the length of their binary representations is known. In this case, the\ngeneric learners can be implemented by randomized Turing machines with finite expected running time.\n\n4\n\nAn Efficient Private Learner for PARITY\n\nLet PARITY be the class of parity functions cr : {0, 1}d \u2192 {0, 1} indexed by r \u2208 {0, 1}d , where cr (x) =\nr x denotes the inner product modulo 2. In this section, we present an efficient private PAC learning\nalgorithm for PARITY. The main result is stated in Theorem 4.4.\nThe standard (non-private) PAC learner for PARITY [33, 30] looks for the hidden vector r by solving a\nsystem of linear equations imposed by examples (xi , cr (xi )) that the algorithm sees. It outputs an arbitrary\n13\n\n\fvector consistent with the examples, i.e., in the solution space of the system of linear equations. We want\nto design a private algorithm that emulates this behavior. A major difficulty is that the private learner's\nbehavior must be specified on all databases z, even those which are not consistent with any single parity\nfunction. The standard PAC learner would simply fail in such a situation (we denote failure by the output\n\u22a5). In contrast, the probability that a private algorithm fails must be similar for all neighbors z and z0 .\nWe first present a private algorithm A for learning PARITY that succeeds only with constant probability.\nLater we amplify its success probability and get a private PAC learner A\u2217 for PARITY. Intuitively, the reason\nPARITY can be learned privately is that when a new example (corresponding to a new linear constraint) is\nadded, the space of consistent hypotheses shrinks by at most a factor of 2. This holds unless the new\nconstraint is inconsistent with previous constraints. In the latter case, the size of the space of consistent\nhypotheses goes to 0. Thus, the solution space changes drastically on neighboring inputs only when the\nalgorithm fails (outputs \u22a5). The fact that algorithm outputs \u22a5 on a database z and a valid (non \u22a5) hypothesis\non a neighboring database z0 might lead to privacy violations. To avoid this, our algorithm always outputs\n\u22a5 with probability at least 1/2 on any input (Step 1).\nA PRIVATE LEARNER FOR PARITY, A(z, \u000f)\n1. With probability 1/2, output \u22a5 and terminate.\n2. Construct a set S by picking each element of [n] independently with probability p = \u000f/4.\n3. Use Gaussian elimination to solve the system of equations imposed by examples, indexed by S:\nnamely, {xi r = cr (xi ) : i \u2208 S}. Let VS denote the resulting affine subspace.\n4. Pick r\u2217 \u2208 VS uniformly at random and output cr\u2217 ; if VS = \u2205, output \u22a5.\nThe proof of A's utility follows by considering all the possible situations in which the algorithm fails to\nsatisfy the error bound, and by bounding the probabilities with which these situations occur.\nLemma 4.1 (Utility of A). Let X be a distribution over X = {0, 1}d . Let z = (z1 , . . . , zn ), where for all\n8\ni \u2208 [n], the entry zi = (xi , c(xi )) with xi drawn i.i.d. from X and c \u2208 PARITY. If n \u2265 \u000f\u03b1\n(d ln 2 + ln 4)\nthen\n1\nPr[A(z, \u000f) = h with error (h) \u2264 \u03b1] \u2265 .\n4\n\u0012\n\u0013\n1\n1\nProof. By standard arguments in learning theory [42], |S| \u2265\nd ln 2 + ln\nlabeled examples are\n\u03b1\n\u03b2\nsufficient for learning PARITY with error \u03b1 and failure probability \u03b2. Since A adds each element of [n] to\nS independently with probability p = \u000f/4, the expected size of S is pn = \u000fn/4. By the Chernoff bound\n(Theorem A.1), |S| \u2265 \u000fn/8 with probability at least 1 \u2212 e\u2212\u000fn/16 . We set \u03b2 = 14 and pick n such that\n\u000fn/8 \u2265 \u03b11 (d ln 2 + ln 4).\nWe now bound the overall success probability. A(z, \u000f) = h with err(h) \u2264 \u03b1 unless one of the following\nbad events happens: (i) A terminates in Step 1, (ii) A proceeds to Step 2, but does not get enough examples:\n|S| < \u03b11 (d ln 2 + ln 4)), (iii) A gets enough examples, but outputs a hypothesis with error greater than \u03b1.\nThe first bad event occurs with probability 1/2. If the lower bound on the database size n is satisfied then\nthe second bad event occurs with probability at most e\u2212\u000fn/16 /2 \u2264 1/8. The last inequality follows from the\nbound on n and the fact that \u03b1 \u2264 1/2. Finally, by our choice of parameters, the last bad event occurs with\nprobability at most \u03b2/2 = 1/8. The claimed bound on the success probability follows.\n14\n\n\fLemma 4.2 (Privacy of A). Algorithm A is \u000f-differentially private.\nAs mentioned above, the key observation in the following proof is that including of any single point in\nthe sample set S increases the probability of a hypothesis being output by at most 2.\nProof. To show that A is \u000f-differentially private, it suffices to prove that any output of A, either a valid\nhypothesis or \u22a5, appears with roughly the same probability on neighboring databases z and z0 . In the\nremainder of the proof we fix \u000f, and write A(z) as shorthand for A(z, \u000f). We have to show that\nPr[A(z) = h] \u2264 e\u000f * Pr[A(z0 ) = h] for all neighbors z, z0 \u2208 Dn and all hypotheses h \u2208 PARITY; (2)\nPr[A(z) =\u22a5] \u2264 e\u000f * Pr[A(z0 ) =\u22a5]\n\nfor all neighbors z, z0 \u2208 Dn .\n\n(3)\n\nWe prove the correctness of Eqn. (2) first. Let z and z0 be neighboring databases, and let i denote the entry\non which they differ. Recall that A adds i to S with probability p. Since z and z0 differ only in the ith entry,\nPr[A(z) = h | i \u2208\n/ S] = Pr[A(z0 ) = h | i \u2208\n/ S].\nNote that if Pr[A(z0 ) = h | i \u2208\n/ S] = 0, then also Pr[A(z) = h | i \u2208\n/ S] = 0, and hence Pr[A(z) = h] =\n0 because adding a constraint does not add new vectors to the space of solutions. Otherwise, Pr[A(z0 ) =\nh|i\u2208\n/ S] > 0. In this case, we rewrite the probability on z as follows:\nPr[A(z) = h] = p * Pr[A(z) = h | i \u2208 S] + (1 \u2212 p) * Pr[A(z) = h | i \u2208\n/ S],\nand apply the same transformation to the probability on z0 . Then\nPr[A(z) = h]\nPr[A(z0 ) = h]\n\n=\n\u2264\n=\n\np * Pr[A(z) = h | i \u2208 S] + (1 \u2212 p) * Pr[A(z) = h | i \u2208\n/ S]\n0\n0\np * Pr[A(z ) = h | i \u2208 S] + (1 \u2212 p) * Pr[A(z ) = h | i \u2208\n/ S]\np * Pr[A(z) = h | i \u2208 S] + (1 \u2212 p) * Pr[A(z) = h | i \u2208\n/ S]\n0\np * 0 + (1 \u2212 p) * Pr[A(z ) = h | i \u2208\n/ S]\np\nPr[A(z) = h | i \u2208 S]\n*\n+1\n1 \u2212 p Pr[A(z) = h | i \u2208\n/ S]\n\n(4)\n\nWe need the following claim:\nClaim 4.3.\n\nPr[A(z) = h | i \u2208 S]\n\u2264 2, for all z \u2208 Dn and all hypotheses h \u2208 PARITY.\nPr[A(z) = h | i \u2208\n/ S]\n\nThis claim is proved below. For now, we can plug it into Eqn. (4) to get\nPr[A(z) = h]\n2p\n\u2264\n+ 1 \u2264 \u000f + 1 \u2264 e\u000f .\n0\nPr[A(z ) = h]\n1\u2212p\nThe first inequality holds since p = \u000f/4 and \u000f \u2264 1/2. This establishes Eqn. (2). The proof of Eqn. (3) is\nsimilar:\nPr[A(z) =\u22a5]\nPr[A(z0 ) =\u22a5]\n\n=\n\u2264\n=\n\np * Pr[A(z) =\u22a5 | i \u2208 S] + (1 \u2212 p) * Pr[A(z) =\u22a5 | i \u2208\n/ S]\np * Pr[A(z0 ) =\u22a5 | i \u2208 S] + (1 \u2212 p) * Pr[A(z0 ) =\u22a5 | i \u2208\n/ S]\np * 1 + (1 \u2212 p) * Pr[A(z) =\u22a5 | i \u2208\n/ S]\n0\np * 0 + (1 \u2212 p) * Pr[A(z ) =\u22a5 | i \u2208\n/ S]\np\n2p\n+1\u2264\n+ 1 \u2264 \u000f + 1 \u2264 e\u000f .\n0\n(1 \u2212 p) * Pr[A(z ) =\u22a5 | i \u2208\n/ S]\n1\u2212p\n\nIn the last line, the first inequality follows from the fact that on any input, A outputs \u22a5 with probability at\nleast 1/2. This completes the proof of the lemma.\n15\n\n\fWe now prove Claim 4.3.\nProof of Claim 4.3. The left hand side\nP\nPr[A(z) = h | i \u2208 S]\nT \u2286[n]\\{i} Pr[A(z) = h | S = T \u222a {i}] * Pr[A selects T from [n] \\ {i}]\nP\n=\n.\nPr[A(z) = h | i \u2208\n/ S]\nT \u2286[n]\\{i} Pr[A(z) = h | S = T ] * Pr[A selects T from [n] \\ {i}]\nPr[A(z) = h | S = T \u222a {i}]\n\u2264 2 for each T \u2286 [n] \\ {i}.\nPr[A(z) = h | S = T ]\nRecall that VS is the space of solutions to the system of linear equations {hxi , ri = cr (xi ) : i \u2208 S}. Recall\nalso that A picks r\u2217 \u2208 VS uniformly at random and outputs h = cr\u2217 . Therefore,\n\u001a\n1/|VS | if r\u2217 \u2208 VS ,\nPr[A(z) = cr\u2217 | S] =\n0\notherwise.\n\nTo prove the claim, it is enough to show that\n\nIf Pr[A(z) = h | S = T ] = 0 then Pr[A(z) = h | S = T \u222a {i}] = 0 because a new constraint does not add\nnew vectors to the space of solutions. If Pr[A(z) = h | S = T \u222a {i}] = 0, the required inequality holds. If\nneither of the two probabilities is 0,\n1/|VT \u222a{i} |\n|VT |\nPr[A(z) = h | S = T \u222a {i}]\n=\n=\n\u2264 2.\nPr[A(z) = h | S = T ]\n1/|VT |\n|VT \u222a{i} |\nThe last inequality holds because in Z2 (the finite field with 2 elements where arithmetic is performed\nmodulo 2), adding a consistent linear constraint either reduces the space of solutions by a factor of 2 (if the\nconstraint is linearly independent from VT ) or does not change the solutions space (if it is linearly dependent\non the previous constraints). The constraint indexed by i has to be consistent with constraints indexed by T ,\nsince both probabilities are not 0.\nIt remains to amplify the success probability of A. To do so, we construct a private version of the\nstandard (non-private) algorithm for amplifying a learner's success probability. The standard amplification\nalgorithm generates a set of hypotheses by invoking A multiple times on independent examples, and then\noutputs a hypothesis from the set with the least training error as evaluated on a fresh test set (see [42] for\ndetails). Our private amplification algorithm differs from the standard algorithm only in the last step: it\nadds Laplacian noise to the training error to obtain a private version of the error, and then uses the perturbed\ntraining error instead of the true training error to select the best hypothesis from\u221athe set. 4 Recall that\nLap(\u03bb) denotes the Laplace probability distribution with mean 0, standard deviation 2\u03bb, and p.d.f. f (x) =\n1 \u2212|x|/\u03bb\n.\n2\u03bb e\n\n4\n\nAlternatively, we could use the generic learner from Theorem 3.4 to select among the candidate hypotheses; the resulting\nalgorithm has the same asymptotic behavior as the algorithm we discuss here. We chose the algorithm that we felt was simplest.\n\n16\n\n\fA MPLIFIED PRIVATE PAC LEARNER FOR PARITY, A\u2217 (z, \u000f, \u03b1, \u03b2)\nl\n\u0010 \u0011m\n1. \u03b2 0 \u2190 \u03b22 ; \u03b10 \u2190 \u03b15 ; k \u2190 log 3 \u03b210 ; n0 \u2190\n4\n\ncd\n\u000f\u03b10 ;\n\ns\u2190\n\nc0 k\n\u03b10 \u000f\n\nlog\n\n\u0010 \u0011\nk\n\u03b20\n\n(where c, c0 are constants).\n\n2. If n \u2264 kn0 + s, stop and return \"insufficient samples\".\n3. Divide z = (z1 , . . . , zn ) into two parts, training set z\u0304 = (z1 , . . . , zkn0 ) and test set \u1e91 =\n(zkn0 +1 , . . . , zkn0 +s ).\n4. Divide z\u0304 into k equal parts each of size n0 , let z\u0304j = (z(j\u22121)n0 +1 , . . . , zjn0 ) for j \u2208 [k].\n5. For j \u2190 1 to k\nhj \u2190 A(z\u0304j , \u000f);\nc T (hj ) =\nset perturbed training error of hj to err\n\n{zi \u2208 \u1e91 : hj (xi ) 6= c(xi )}\n+ Lap\ns\n\n\u0012\n\n\u0013\nk\n.\ns\u000f\n\nc T (hj )}.\n6. Output h\u2217 = hj \u2217 where j \u2217 = argminj\u2208[k] {err\nTheorem\n4.4.\u0011 Algorithm A\u2217 efficiently and privately PAC learns PARITY (according to Definition 3.1) with\n\u0010\nsamples.\nO d log(1/\u03b2)\n\u000f\u03b1\nThe theorem follows from Lemmas 4.5 and 4.6 that, respectively, prove privacy and utility of A\u2217 .\nLemma 4.5 (Privacy of A\u2217 ). Algorithm A\u2217 is \u000f-differentially private.\nProof. We prove that even if A\u2217 released all hypotheses hj , computed in Step 5, together with the correc T (hj ), it would still be \u000f-differentially private. Since the output of A\u2217\nsponding perturbed error estimates err\ncan be computed solely from this information, Claim 2.2 implies that A\u2217 is \u000f-differentially private.\nBy Lemma 4.2, algorithm A is \u000f-differentially private. Since A is invoked on disjoint parts of z to\ncompute hypotheses hj , releasing all these hypotheses would also be \u000f-differentially private.\nDefine the training error of hypothesis hj on \u1e91 as errT (hj ) = |{zi \u2208 \u1e91 : hj (xi ) 6= c(xi )}|/s. The\nglobal sensitivity of the errT function is 1/s because |errT (z)\u2212errT (z0 )| \u2264 1/s for every pair of neighboring\nc T (hj ) for one j, would be \u000f/k-differentially private,\ndatabases z, z0 . Therefore, by Theorem 2.3, releasing err\nand by Claim 2.2, releasing all k of them would be \u000f-differentially private. Since hypotheses hj and their\nc T (hj ) are computed on disjoint parts of the database z, releasing all that information\nperturbed errors err\nwould still be \u000f-differentially private.\nLemma 4.6 (Utility of A\u2217 ). A\u2217 (*, \u000f, *, *) PAC learns PARITY with sample complexity n = O( d log(1/\u03b2)\n).\n\u000f\u03b1\nProof. Let X be a distribution over X = {0, 1}d . Recall that z = (z1 , . . . , zn ), where for all i \u2208 [n],\nthe entry zi = (xi , c(xi )) with xi drawn i.i.d. from X and c \u2208 PARITY. Assume that \u03b2 < 1/4, and\nfor a constant C to be determined. We wish to prove that Pr[err(h\u2217 ) \u2264 \u03b1] \u2265 1 \u2212 \u03b2, where\nn \u2265 C d log(1/\u03b2)\n\u000f\u03b1\n\u2217\nh is the hypothesis output by A\u2217 .\nConsider the set of candidate hypotheses {h1 , ..., hk } output by the invocations of A inside of A\u2217 . We\ncall a hypothesis h good if err(h) \u2264 \u03b15 = \u03b10 . We call a hypothesis h bad if err(h) \u2265 \u03b1 = 5\u03b10 . Note that\ngood and bad refer to a hypothesis' true error rate on the underlying distribution.\nWe will show:\n1. With probability at least 1 \u2212 \u03b2 0 , one of the invocations of A outputs a good hypothesis.\n17\n\n\f2. Conditioned on any particular outcome {h1 , ..., hk } of the invocations of A, with probability at least\n1 \u2212 \u03b2 0 , both:\n(a) Every good hypothesis hj in {h1 , ..., hk } has training error errT (hj ) \u2264 2\u03b10 .\n(b) Every bad hypothesis hj in {h1 , ..., hk } has training error errT (hj ) \u2265 4\u03b10 .\n3. Conditioned on any particular hypotheses {h1 , ..., hk } and training errors errT (h1 ), ..., errT (hk ), with\nc T (hj ) \u2212 errT (hj )| < \u03b10 .\nprobability at least 1 \u2212 \u03b2 0 , for all j simultaneously, |err\nSuppose the events described in the three claims above all occur. Then some good hypothesis has\nperturbed training error less than 3\u03b10 , yet all bad hypotheses have perturbed training error greater than 3\u03b10 .\nc T (hj \u2217 ) is not bad, that is, has true error at most\nThus, the hypothesis hj \u2217 with minimal perturbed error err\n\u03b1. By the claims above, the probability that all three events occur is at least 1 \u2212 3\u03b2 0 = 1 \u2212 \u03b2, and so the\nlemma holds. We now prove the claims.\nFirst, by the utility guarantee of A, each invocation of A inside A\u2217 outputs a good hypothesis with\nprobability at least 41 as long as the constant c > 8(ln 2 + ln 4) (since in that case n0 , the size of each z\u0304j ,\nis large enough to apply Lemma 4.1). The k invocations of the algorithm A are on independent samples,\n\u0001k\nso the probability that none of h1 , . . . , hk is good is at most 43 . Setting k \u2265 log 3 \u03b210 ensures that with\n4\nprobability at least 1 \u2212 \u03b2 0 , at least one of h1 , . . . , hk has error at most \u03b10 .\nSecond, fix a particular sequence of candidate hypotheses h1 , ..., hk . For each j, the training error\nerrT (hj ) is the average of s Bernouilli trials, each with success probability err(hj ). (Crucially, the training\nset \u1e91 is independent of the data z\u0304 used to find the candidate hypotheses). To bound the training error, we\napply the multiplicative Chernoff bound (Theorem A.1) with n = s and p = err(hj ). Here, p \u2264 \u03b10 if hj is\ngood, and p \u2265 5\u03b10 if hj is bad.\nBy the multiplicative Chernoff bound (Theorem A.1) if s \u2265 \u03b1c10 ln \u03b2k0 (for appropriate constant c1 ), then\n\u0002\n\u0003\n\u03b20\nPr errT (hj ) \u2265 2\u03b10 hj is good \u2264 Pr[Binomial(s, \u03b10 ) \u2265 2\u03b10 s] \u2264\n, and\nk\n\u0002\n\u0003\n\u03b20\nPr errT (hj ) \u2264 4\u03b10 hj is bad \u2264 Pr[Binomial(s, 5\u03b10 ) \u2264 4\u03b10 s] \u2264 .\nk\nBy a union bound, all the training errors are (simultaneously) approximately correct, with probability at\n0\nleast 1 \u2212 k * \u03b2k = 1 \u2212 \u03b2 0 .\nFinally, we prove the third claim. Consider a particular candidate hypothesis hj . If s \u2265 c\u03b120k\u000f ln \u03b2k0 (for\nappropriate constant c2 ), then (by using the c.d.f.5 of the Laplacian distribution)\n\u0014\n\u0012 \u0013\n\u0015\n\u0002\n\u0003\n\u03b20\nk\n0\n0\nc T (hj )| < \u03b1 = Pr Lap\nPr |errT (hj ) \u2212 err\n\u2265\u03b1 \u2264 .\ns\u000f\nk\nBy a union bound, all k perturbed estimates are within \u03b10 of their correct value with probability at least\n0\n1 \u2212 k * \u03b2k = 1 \u2212 \u03b2 0 . This probability is taken over the choice of Laplacian noise, and so the bound holds\nindependently of the particular hypotheses or their training error estimates.\nRemark: In the non-private case O((d+ln(1/\u03b2))/\u03b1) labels are sufficient for learning PARITY. Theorem 4.4\nshows that the upper bounds on the sample size of private and non-private learners differ only by a factor of\nO(ln(1/\u03b2)/\u000f).\n5\n\nThe cumulative distribution function of the Laplacian distribution Lap(\u03bb) is F (x) =\nif x \u2265 0.\n\n18\n\n1\n2\n\nexp\n\nx\n\u03bb\n\n\u0001\n\nif x < 0 and 1 \u2212 21 exp \u2212 \u03bbx\n\n\u0001\n\n\f5\n\nLocal Protocols and SQ learning\n\nIn this section, we relate private learning in the local model to the SQ model of Kearns [39]. We first define\nthe two models precisely. We then prove their equivalence (Section 5.1), and discuss the implications for\nlearning (Section 5.2). Finally, we define the concept class MASKED-PARITY and prove that it separates\ninteractive from noninteractive local learning (Section 5.3).\nLocal Model. We start by describing private computation in the local model. Informally, each individual\nholds her private information locally, and hands it to the learner after randomizing it. This is modeled by\nletting the local algorithm access each entry zi in the input database z = (z1 , . . . , zn ) \u2208 Dn only via local\nrandomizers.\nDefinition 5.1 (Local Randomizer). An \u000f-local randomizer R : D \u2192 W is an \u000f-differentially private\nalgorithm that takes a database of size n = 1. That is, Pr[R(u) = w] \u2264 e\u000f Pr[R(u0 ) = w] for all u, u0 \u2208 D\nand all w \u2208 W . The probability is taken over the coins of R (but not over the choice of the input).\nNote that since a local randomizer works on a data set of size 1, u and u0 are neighbors for all u, u0 \u2208 D.\nThus, this definition is consistent with our previous definition of differential privacy.\nDefinition 5.2 (LR Oracle). Let z = (z1 , . . . , zn ) \u2208 Dn be a database. An LR oracle LRz (*, *) gets an\nindex i \u2208 [n] and an \u000f-local randomizer R, and outputs a random value w \u2208 W chosen according to the\ndistribution R(zi ). The distribution R(zi ) depends only on the entry zi in z.\nDefinition 5.3 (Local algorithm). An algorithm is \u000f-local if it accesses the database z via the oracle LRz\nwith the following restriction: for all i \u2208 [n], if LRz (i, R1 ), . . . , LRz (i, Rk ) are the algorithm's invocations\nof LRz on index i, where each Rj is an \u000fj -local randomizer, then \u000f1 + * * * + \u000fk \u2264 \u000f.\nLocal algorithms that prepare all their queries to LRz before receiving any answers are called noninteractive; otherwise, they are interactive.\nBy Claim 2.2, \u000f-local algorithms are \u000f-differentially private.\nSQ Model. In the statistical query (SQ) model, algorithms access statistical properties of a distribution\nrather than individual examples.\nDefinition 5.4 (SQ Oracle). Let D be a distribution over a domain D. An SQ oracle SQD takes as input a\nfunction g : D \u2192 {+1, \u22121} and a tolerance parameter \u03c4 \u2208 (0, 1); it outputs v such that:\n|v \u2212 E [g(u)]| \u2264 \u03c4.\nu\u223cD\n\nThe query function g does not have to be Boolean. Bshouty and Feldman [17] showed that given access\nto an SQ oracle which accepts only boolean query functions, one can simulate an oracle that accepts realvalued functions g : D \u2192 [\u2212b, b], and outputs Eu\u223cD [g(u)] \u00b1 \u03c4 using O(log(b/\u03c4 )) nonadaptive queries to\nthe SQ oracle and similar processing time.\nDefinition 5.5 (SQ algorithm). An SQ algorithm accesses the distribution D via the SQ oracle SQD . SQ\nalgorithms that prepare all their queries to SQD before receiving any answers are called nonadaptive;\notherwise, they are called adaptive.\nNote that we do not restrict g() to be efficiently computable. We will distinguish later those algorithms\nthat only make queries to efficiently computable functions g().\n19\n\n\f5.1\n\nEquivalence of Local and SQ Models\n\nBoth the SQ and local models restrict algorithms to access inputs in a particular manner. There is a significant difference though: an SQ oracle sees a distribution D, whereas a local algorithm takes as input a fixed\n(arbitrary) database z. Nevertheless, we show that if the entries of z are chosen i.i.d. according to D, then\nthe models are equivalent. Specifically, an algorithm in one model can simulate an algorithm in the other\nmodel. Moreover, the expected query complexity is preserved up to polynomial factors. We first present\nthe simulation of SQ algorithms by local algorithms (Section 5.1.1). The simulation in the other direction is\nmore delicate and is presented in Section 5.1.2.\n5.1.1\n\nSimulation of SQ Algorithms by Local Algorithms\n\nBlum et al. [11] used the fact that sum queries can be answered privately with little noise to show that any\nefficient SQ algorithm can be simulated privately and efficiently. We show that it can be simulated efficiently\neven by a local algorithm, albeit with slightly worse parameters.\nLet g : D \u2192 [\u2212b, b] be the SQ query we want to simulate. By Theorem 2.3, since the global sensitivity\nof g is 2b, the algorithm Rg (u) = g(u) + \u03b7 where \u03b7 \u223c Lap(2b/\u000f) is an \u000f-local randomizer. We construct\na local algorithm Ag that, given n and \u000f, and access to a database z via oracle LRz , invokes LRz for every\ni \u2208 [n] with the randomizer Rg and outputs the average of the responses:\nA LOCAL ALGORITHM Ag (n, \u000f, LRz ) THAT SIMULATES AN SQ QUERY g : D \u2192 [\u2212b, b]\n1. Output\n\n1\nn\n\nPn\n\ni=1 LRz (i, Rg )\n\nwhere Rg (u) = g(u) + \u03b7 and \u03b7 \u223c Lap\n\n2b\n\u000f\n\n\u0001\n\n.\n\n\u0001\n\u0001\n\u0001\nP\nP\nNote that Ag outputs n1 ni=1 g(zi ) + n1 ni=1 \u03b7i , where the \u03b7i are i.i.d. from Lap 2b\n\u000f . This algorithm is \u000f-local (since it applies a single \u000f-local randomized to each entry of z), and therefore \u000f-differentially\nprivate. The following lemma shows that when the input database z is large enough, Ag simulates the desired\nSQ query g with small error probability.\n2\n\nLemma 5.6. If, for sufficiently large constant c, database z has n \u2265 c * log(1/\u03b2)b\nentries sampled i.i.d.\n\u000f2 \u03c4 2\nfrom a distribution D on D then algorithm Ag approximates Eu\u223cD [g(u)] within additive error \u00b1\u03c4 with\nprobability at least 1 \u2212 \u03b2.\nProof. Let v = Eu\u223cD [g(u)] denote the true mean. By the Chernoff-Hoeffding bound for real-valued variables (Theorem A.2),\n\u0010 2 \u0011\n\u0002 P\n\u0003\nPr n1 ni=1 g(ui ) \u2212 v \u2265 \u03c42 \u2264 2 exp \u2212 \u03c48bn2 .\n\u0010\n\u0011\n2\nTherefore, in the absence of additive Laplacian random noise, O ln(1/\u03b2)b\nexamples are enough to ap2\n\u03c4\nproximate Eu\u223cD [g(u)] within additive error \u00b1 \u03c42 with probability at least 1 \u2212 \u03b22 . (Note that the number of\nexamples is smaller than the lower bound on n in the lemma by a factor of O(\u000f\u22122 )).\n2b\nThe effect of the Laplace\n\u0011 also be bounded via a standard tail inequality: setting \u03bb = \u000f in\n\u0010 noise can\nLemma A.3, we get that O\n\n[\u2212 \u03c42 , \u03c42 ]\n\nwith probability at\nprobability at least 1 \u2212 \u03b2.\n\nln(1/\u03b2)b2\n\u000f2 \u03c4 2\nmost \u03b22 . It\n\nsamples are sufficient to ensure that the average of \u03b7i 's lies outside\n\nfollows that Ag estimates Eu\u223cD [g(u)] within additive error \u00b1\u03c4 with\n\n20\n\n\fSimulation. Lemma 5.6 suggests a simple simulation of a nonadaptive (resp. adaptive) SQ algorithm by\na noninteractive (resp. interactive) local algorithm as follows. Assume the SQ algorithm makes at most t\nqueries to an SQ oracle SQD . The local algorithm simulates each query (g, \u03c4 ) by running Ag (n0 , \u000f, LRz )\n0 )b2\nwith parameters \u03b2 0 = \u03b2t and n0 = c * log(1/\u03b2\non a previously unused portion of the database z containing\n\u000f2 \u03c4 2\nn0 entries.\nTheorem 5.7 (Local simulation of SQ). Let ASQ be an SQ algorithm that makes at most t queries to an\nSQ oracle SQD , each with tolerance at least \u03c4 . The simulation above is \u000f-differentially private. If, for\n2\nsufficiently large constant c, database z has n \u2265 c * t log(t/\u03b2)b\nentries sampled i.i.d. from the distribution D\n\u000f2 \u03c4 2\nthen the simulation above gives the same output as ASQ with probability at least 1 \u2212 \u03b2.\nFurthermore, the simulation is noninteractive if the original SQ algorithm ASQ is nonadaptive. The\nsimulation is efficient if ASQ is efficient.\nProof. Each query is simulated with a fresh portion of z, and hence privacy is preserved as each entry is\nsubjected to a single application of the \u000f-local randomizer R. By the union bound, the probability of any\nof the queries not being approximated within additive error \u03c4 is bounded by \u03b2. If ASQ is nonadaptive, all\nqueries to LRz can be prepared in advance.\n5.1.2\n\nSimulation of Local Algorithms by SQ Algorithms\n\nLet z be a database containing n entries drawn i.i.d. from D. Consider a local algorithm making t queries to\nLRz . We show how to simulate any local randomizer invoked by this algorithm by using statistical queries\nto SQD . Consider one such randomizer R : D \u2192 W applied to database entry zi . To simulate R we need to\nsample w \u2208 W with probability p(w) = Przi \u223cD [R(zi ) = w] taken over choice of zi \u223c D and random coins\nof R. (For interactive algorithms, it is more complicated, as the outputs of different randomizers applied to\nthe same entry zi have to be correlated.)\nA brief outline. The idea behind the simulation is to sample from a distribution pe(*) that is within small\nstatistical distance of p(*). We start by applying R to an arbitrary input (say, 0) in the domain D and obtaining\na sample w \u223c R(0). Let q(w) = Pr[R(0) = w] (where the probability is taken only over randomness in R).\nSince R is \u000f-differentially private, q(w) approximates p(w) within a multiplicative factor of e\u000f . To sample\nw from p(*) we use the following rejection sampling algorithm: (i) sample w according to q(*); (ii) with\np(w)\nprobability q(w)e\n\u000f , output w; (iii) with the remaining probability, repeat from (i).\nTo carry out this strategy, we must be able to estimate p(w), which depends on the (unknown) distribution D, using only SQ queries. The rough idea is to express p(w) as the expectation, taken over z \u223c D,\nof the function h(z) = Pr[R(z) = w] (where the probability is taken only over the coins of R). We can\nuse h as the basis of an SQ query. In fact, to get a sufficiently accurate approximation, we must rescale the\nfunction h somewhat, and keep careful track of the error introduced by the SQ oracle. We present the details\nin the proof of the following lemma:\nLemma 5.8. Let z be a database with entries drawn i.i.d. from a distribution D. For every noninteractive (resp. interactive) local algorithm A making t queries to LRz , there exists a nonadaptive (resp.\nadaptive) statistical query algorithm B that in expectation makes O(t * e\u000f ) queries to SQD with accuracy\n\u03c4 = \u0398(\u03b2/(e2\u000f t)), such that the statistical difference between B's and A's output distributions is at most \u03b2.\nProof. We split the simulation over Claims 5.9 and 5.10. In the first claim we simulate noninteractive local\nalgorithms using nonadaptive SQ algorithms. In the second claim we simulate interactive local algorithms\nusing adaptive SQ algorithms.\n21\n\n\fClaim 5.9. For every noninteractive local algorithm A making t nonadaptive queries to LRz , there exists\na nonadaptive statistical query algorithm B that in expectation makes t * e\u000f queries to SQD with accuracy\n\u03c4 = \u0398(\u03b2/(e2\u000f t)), such that the statistical difference between B's and A's output distributions is at most \u03b2.\nProof. We show how to simulate an \u000f-local randomizer R using statistical queries to SQD . Because the\nlocal algorithm is non-interactive, we can assume without loss of generality that it accesses each entry zi\nonly once. (Otherwise, one can combine different operators, used to access zi , by combining their answers\ninto a vector). Given R : D \u2192 W , we want to sample w \u2208 W with probability:\np(w) = Pr [R(zi ) = w].\nzi \u223cD\n\nTwo notes regarding our notation: (i) As zi is drawn i.i.d. from D we could omit the index i. We leave\nthe index i in our notation to emphasize that we actually simulate the application of a local randomizer R to\nentry i. (ii) The semantics of Pr changes depending on whether it appears with the subscript zi \u223c D or not.\nPrzi \u223cD denotes probability that is taken over the choice of zi \u223c D and the randomness in R, whereas when\nthe subscript is dropped zi is fixed and the probability is taken only over the randomness in R. Using this\nnotation, Przi \u223cD [R(zi ) = w] = Ezi \u223cD Pr[R(zi ) = w].\nWe construct an algorithm BR,\u000f that given t, \u03b2, and access to the SQ oracle, outputs w \u2208 W , such that\nthe statistical difference between the output probability distributions of BR,\u000f and the simulated randomizer\nR is at most \u03b2/t. Because the local algorithm makes t queries, the overall statistical distance between the\noutput distribution of the local algorithm and the distribution resulting from the simulation is at most \u03b2, as\ndesired.\nA N SQ ALGORITHM BR,\u000f (t, \u03b2, SQD ) THAT SIMULATES AN \u000f- LOCAL RANDOMIZER R : D \u2192 W .\n1. Sample w \u223c R(0). Let q(w) = Pr[R(0) = w].\n2. Define g : D \u2192 [\u22121, 1] by g(zi ) =\n\nPr[R(zi ) = w] \u2212 q(w)\n, and let \u03c4 =\nq(w)(e\u000f \u2212 e\u2212\u000f )\n\n\u03b2\n.\n3e2\u000f t\n\n3. Query the SQ oracle v = SQD (g, \u03c4 ), and let pe(w) = vq(w)(e\u000f \u2212 e\u2212\u000f ) + q(w).\n4. With probability\n\npe(w)\n,\n\u03b2\nq(w)(1+ 3t\n)e\u000f\n\noutput w.\n\nWith the remaining probability, repeat from Step 1.\nWe now show that the statistical distance between the output of BR,\u000f (t, \u03b2, SQD ) and the distribution p(*)\nis at most \u03b2/t. As mentioned above, our initial approximation pe(*) of p(*) in Step 1 is obtained by applying\nR to some arbitrary input (namely, 0) in the domain D and sampling w \u223c R(0). Since R is \u000f-differentially\nprivate, q(w) = Pr[R(0) = w] approximates p(w) within a multiplicative factor of e\u000f .\nHowever, to carry out the rejection sampling strategy, we need to get a much better estimate of p(w).\nSteps 2 and 3 compute such an estimate, pe(w), satisfying (with probability 1)\npe(w) \u2208 (1 \u00b1 \u03c6) p(w) where \u03c6 =\n\n\u03b2\n3t\n\n.\n\n(5)\n\nWe establish the inclusion (5) below. For now, assume it holds on every iteration. Step 4 is a rejection\nsampling step which ensures that the output will follow a distribution close to pe(*). Inclusion (5) guarantees\npe(w)\nthat\nis at most 1, so the probability in Step 4 is well defined. The difficulty is that the quantity\n\u03b2\n\u000f\nq(w)(1+ 3t )e\n\n22\n\n\fpe(w) is not a well-defined function of w: it depends on the SQ oracle and may vary, for the same w, from\niteration to iteration.\nNevertheless, pe is fixed for any given iteration of the algorithm. In the given iteration, any particular\npe(w)\npe(w)\nelement w gets output with probability q(w)\u00d7 q(w)(1+\u03c6)e\n\u000f = (1+\u03c6)e\u000f . The probability that the given iteration\nP pe(w)\n1\u00b1\u03c6\nterminates (i.e., outputs some w) is then pterminate = w (1+\u03c6)e\n\u000f . By (5), this probability is in (1+\u03c6)e\u000f .\nThus, conditioned on the iteration terminating, element w is output with probability\n1\u00b1\u03c6\n1\u00b1\u03c6\n\npe(w)\n(1+\u03c6)*e\u000f *pteminate\n\n\u2208\n\n* p(w). Since \u03c6 \u2264 1/3, we can simplify this to get\n\u0002\n\u0003\nPr w output in a given iteration iteration produces output \u2208 (1 \u00b1 3\u03c6)p(w) .\n\nThis implies that no matter which iteration produces output, the statistical difference between the distribution\nof w and p(*) will be at most 3\u03c6 = \u03b2t , as desired.\n1\u2212\u03c6\n* e\u2212\u000f , the expected number of\nMoreover, since each iteration terminates with probability at least 1+\u03c6\n\u000f\n\u000f\niterations is at most 1+\u03c6\n1\u2212\u03c6 * e \u2264 2e . Thus, the total expected SQ query complexity of the simulation is\nO(t * e\u000f ).\nIt remains to prove the correctness of (5). To estimate p(w) given w, we set up the statistical query g(zi ).\nThis is a valid query since Pr[R(zi ) = w] is a function of zi , and furthermore g(zi ) \u2208 [\u22121, 1] for all zi as\nPr[R(zi ) = w]/ Pr[R(0) = w] \u2208 e\u00b1\u000f . The SQ query result v lies within Ezi \u223cD [g(zi )] \u00b1 \u03c4 , where \u03c4 is the\ntolerance parameter for the statistical query, and so\n\nE [g(zi )] =\n\nzi \u223cD\n\np(w) \u2212 q(w)\nEzi \u223cD Pr[R(zi ) = w] \u2212 q(w)\n=\n.\n\u000f\n\u2212\u000f\nq(w)(e \u2212 e )\nq(w)(e\u000f \u2212 e\u2212\u000f )\n\nPlugging in the bounds for v and q(w) we get that pe(w) \u2208 (1 \u00b1 \u03c4 0 )p(w) where \u03c4 0 = e2\u000f \u03c4 =\nestablishes (5) and concludes the proof.\n\n\u03b2\n3t .\n\nThis\n\nClaim 5.10. For every interactive local algorithm A making t queries to LRz , there exists an adaptive statistical query algorithm B that in expectation makes O(t * e\u000f ) queries SQD with accuracy \u03c4 = \u0398(\u03b2/(e2\u000f t)),\nsuch that the statistical difference between B's and A's output distributions is at most \u03b2.\nProof. As in the previous claim, we show how to simulate the output of the local randomizers during the run\nof the local algorithm. A difference, however, is that because an entry zi may be accessed multiple times, we\nhave to condition our sampling on the outcomes of previous (simulated) applications of local randomizers\nto zi .\nMore concretely, let R1 , R2 , ... be the sequence of randomizers that access the entry zi . To simulate\nRk (zi ), we must take into account the answers a1 , . . . , ak\u22121 given by the simulations of R1 (zi ), . . . , Rk\u22121 (zi ).\nWe show how to do this using adaptive statistical queries to SQD . The notation is the same as in Claim 5.9.\nWe want to output w \u2208 W with probability\np(w) = Pr [Rk (zi ) = w | Rk\u22121 (zi ) = ak\u22121 , Rk\u22122 (zi ) = ak\u22122 , . . . , R1 (zi ) = a1 ],\nzi \u223cD\n\nwhere Rj (1 \u2264 j \u2264 k \u2212 1) denotes the jth randomizer applied to zi .\nAs before, we start by sampling w \u223c R(0). Let q(w) = Pr[Rk (0) = w]. Note that q(w) approximates p(w) within a multiplicative factor of e\u000f because R1 , . . . , Rk are respectively \u000f1 -,. . . , \u000fk -differentially\n\n23\n\n\fprivate, and \u000f1 + . . . + \u000fk \u2264 \u000f. Hence, we can use the rejection sampling algorithm as in Claim 5.9.\nRewrite p(w):\np(w) =\n=\n\nPrzi \u223cD [Rk (zi ) = w \u2227 Rk\u22121 (zi ) = ak\u22121 \u2227 * * * \u2227 R1 (zi ) = a1 ]\nPrzi \u223cD [Rk\u22121 (zi ) = ak\u22121 \u2227 * * * \u2227 R1 (zi ) = a1 ]\n[Pr[R\nEzi \u223cD\nk (zi ) = w \u2227 Rk\u22121 (zi ) = ak\u22121 \u2227 * * * \u2227 R1 (zi ) = a1 ]]\nEzi \u223cD [Pr[Rk\u22121 (zi ) = ak\u22121 \u2227 * * * \u2227 R1 (zi ) = a1 ]]\n\nConditioned on a particular value of zi , the probabilities in the last expression depend only the coins of\nthe randomizers. The outputs of the randomizers are independent conditioned on zi , and therefore we can\nsimplify the expression above:\ni\nh\nQk\u22121\nEzi \u223cD Pr[Rk (zi ) = w] * j=1 Pr[Rj (zi ) = aj ]\ni\nhQ\np(w) =\nk\u22121\nEzi \u223cD\nj=1 Pr[Rj (zi ) = aj ]\nLet p1 and p2 denote the numerator and denominator, respectively, in the right hand side of the equation\nabove. Let r1 (zi ) and r2 (zi ) denote the values inside the expectations that define p1 and p2 , respectively.\nNamely,\nr1 (zi ) = Pr[Rk (zi ) = w] *\n\nk\u22121\nY\n\nPr[Rj (zi ) = aj ]\n\nand\n\nr2 (zi ) =\n\nj=1\n\nk\u22121\nY\n\nPr[Rj (zi ) = aj ] .\n\nj=1\n\nFor estimating p1 = Ezi \u223cD [r1 (zi )] we use the statistical query g1 (zi ), and for estimating p2 = Ezi \u223cD [r2 (zi )]\nwe use the statistical query g2 (zi ) defined as follows:\ng1 (zi ) =\n\nr1 (zi ) \u2212 r1 (0)\nr1 (0)(e\u000f \u2212 e\u2212\u000f )\n\nand\n\ng2 (zi ) =\n\nr2 (zi ) \u2212 r2 (0)\n.\nr2 (0)(e\u000f \u2212 e\u2212\u000f )\n\nAs in Claim 5.9, one can estimate p1 and p2 to within a multiplicative factor of (1 \u00b1 \u03c4 0 ) where \u03c4 0 = e2\u000f \u03c4\nand \u03c4 is the accuracy of the statistical queries. The ratio of the estimates for p1 and p2 gives an estimate\np\u0303(w) for p(w) to within a multiplicative factor (1 \u00b1 3\u03c4 0 ), for \u03c4 0 \u2264 31 . The estimate p\u0303(w) can then be used\nwith rejection sampling to sample an output of the randomizer.\n\u03b2\nLet t be the number of queries made by A. Setting \u03c4 0 \u2264 3t\nguarantees that the statistical difference\n\u03b2\nbetween distributions p and pe is at most t , and hence the statistical difference between B's and A's output\ndistributions is at most \u03b2. As in Claim 5.9, the expected number of SQ queries for rejection sampling is\nO(t * e\u000f ).\nClaims 5.9 and 5.10 imply Lemma 5.8.\nNote that the efficiency of the constructions in Lemma 5.8 depends on the efficiency of computing the\nfunctions submitted to the SQ oracle, e.g., the efficiency of computing the probability Pr[R(zi ) = w]. We\ndiscuss this issue in the next section.\n\n5.2\n\nImplications for Local Learning\n\nIn this section, we define learning in the local and SQ models. The equivalence of the two models follows\nfrom the simulations described in the previous sections. An immediate but important corollary is that local\nlearners are strictly less powerful than general private learners.\n24\n\n\fDefinition 5.11 (Local Learning). Locally learnable is defined identically to privately PAC learnable (Definition 3.1), except for the additional requirement that for all \u000f > 0, algorithm A(\u000f, *, *, *) is \u000f-local and\ninvokes LRz at most poly(d, size(c), 1/\u000f, 1/\u03b1, log(1/\u03b2)) times. Class C is efficiently locally learnable if\nboth: (i) the running time of A and (ii) the time to evaluate each query that A makes are bounded by some\npolynomial in d, size(c), 1/\u000f, 1/\u03b1, and log(1/\u03b2).\nLet X be a distribution over an input domain X. Let SQc,X denote the statistical query oracle that takes\nas input a function g : X \u00d7 {+1, \u22121} \u2192 {+1, \u22121} and a tolerance parameter \u03c4 \u2208 (0, 1) and outputs v such\nthat: |v \u2212 Ex\u223cX [g(x, c(x))]| \u2264 \u03c4 .\nDefinition 5.12 (SQ Learning6 ). SQ learnable is defined identically to PAC learnable (Definition 2.4), except\nthat instead of having access to examples z, an SQ learner A can make poly(d, size(c), 1/\u03b1, log(1/\u03b2))\nqueries to oracle SQc,X with tolerance \u03c4 \u2265 1/poly(d, size(c), 1/\u03b1, log(1/\u03b2)). Class C is efficiently SQ\nlearnable if both: (i) the running time of A and (ii) the time to evaluate each query that A makes are\nbounded by some polynomial in d, 1/\u03b1, and log(1/\u03b2).\nIn order to state the equivalence between SQ and local learning, we require the following efficiency\ncondition for a local randomizer.\nDefinition 5.13 (Transparent Local Randomizer). Let R : D \u2192 W be an \u000f-local randomizer. The randomizer is transparent if both: (i) for all inputs u \u2208 D, the time needed to evaluate R; and (ii) for all inputs\nu \u2208 D and outputs w \u2208 W the time taken to compute the probability Pr[R(u) = w], are polynomially\nbounded in the size of the input and 1/\u000f.\nAs stated, this definition requires exact computation of probabilities. This may not make sense on a\nfinite-precision machine, since for many natural randomizers the transition probabilities are irrational. One\ncan relax the requirement to insist that relevant probabilities are computable with additive error at most \u03c6 in\ntime polynomial in log( \u03c61 ).\nAll local protocols that have appeared in the literature [29, 3, 2, 1, 29, 45, 36] are transparent, at least in\nthis relaxed sense.\nIn the equivalences of the previous sections, transparency of local randomizers corresponds directly to\nefficient computability of the function g in an SQ query. To see why, consider first the simulation of SQ\nalgorithms by local algorithms: if the original SQ algorithm is efficient (that is, query g can be evaluated in\npolynomial time) then the local randomizer R(u) = g(u) + \u03b7 can also be evaluated in polynomial time for\nall u \u2208 D. Furthermore, it is simple to estimate for all inputs u \u2208 D and outputs w \u2208 W the probability\nPr[R(u) = w] since R(u) is a Laplacian random variable with known parameters. Second, in the SQ\ni )=w]\u2212q(w)\nsimulation of a local algorithm, the functions g(zi ) = Pr[R(z\nthat are constructed can be evaluated\nq(w)(e\u000f \u2212e\u2212\u000f )\nefficiently precisely when the local randomizers are transparent.\nWe can now state the main result of this section, which follows from Lemmas 5.6 and 5.8, along with\nthe correspondence between transparent randomizers and efficient SQ queries.\nTheorem 5.14. Let C be a concept class over X. Let X be a distribution over X. Let z = (z1 , . . . , zn )\ndenote a database where every zi = (xi , c(xi )) with xi drawn i.i.d. from X and c \u2208 C. Concept class C is\n6\n\nThe standard definition of SQ learning does not allow for any probability of error in the learning algorithm (that is, \u03b2 = 0). Our\ndefinition allows for a small failure probability \u03b2. This enables cleaner equivalence statements and clean modeling of randomized\nSQ algorithms. One can show that differentially private algorithms must have some non-zero probability of error, so a relaxation\nalong these lines is necessary for our results.\n\n25\n\n\flocally learnable using H by an interactive local learner with inputs \u03b1, \u03b2, and with access to LRz if and\nonly if C is SQ learnable using H by an adaptive SQ learner with inputs \u03b1, \u03b2, and access to SQc,X .\nFurthermore, the simulations guarantee the following additional properties: (i) an efficient SQ learner\nis simulatable by an efficient local learner that uses only transparent randomizers; (ii) an efficient local\nlearner that uses only transparent randomizers is simulatable by an efficient SQ learner; (iii) a nonadaptive\nSQ (resp. noninteractive local) learner is simulatable by a noninteractive local (resp. nonadaptive SQ)\nlearner.\nNow we can use lower bounds for SQ learners for PARITY (see, e.g., [39, 12, 55]) to demonstrate\nlimitations of local learners. The lower bound of [12] rules out SQ learners for PARITY that use at most\n2d/3 queries of tolerance at least 2\u2212d/3 , even (a) allowing for unlimited computing time, (b) under the\nrestriction that examples be drawn from the uniform distribution and (c) allowing a small probability of\nerror (see Footnote 6). Since PARITY is (efficiently) privately learnable (Theorem 4.4), and since local\nlearning is equivalent to SQ learning, we obtain:\nCorollary 5.15. Concept classes learnable by local learners are a strict subset of concept classes PAC\nlearnable privately. This holds both with and without computational restrictions.\n\n5.3\n\nThe Power of Interaction in Local Protocols\n\nTo complete the picture of locally learnable concept classes, we consider how interaction changes the power\nof local learners (and, equivalently, how adaptivity changes SQ learning). As mentioned in the introduction,\ninteraction is very costly in typical applications of local algorithms. We show that this cost is sometimes necessary, by giving a concept class that an interactive algorithm can learn efficiently with a polynomial number\nof examples drawn from the uniform distribution, but for which any noninteractive algorithm requires an\nexponential number of examples under the same distribution.\nLet MASKED-PARITY be the class of functions cr,a : {0, 1}d \u00d7 {0, 1}log d \u00d7 {0, 1} \u2192 {+1, \u22121}\nindexed by r \u2208 {0, 1}d and a \u2208 {0, 1}:\n(\n(\u22121)r x+a if b = 0,\ncr,a (x, i, b) =\n(\u22121)ri\nif b = 1,\nwhere r x denotes the inner product of r and x modulo 2, and ri is the ith bit of r. This concept class\ndivides the domain into two parts (according to the last bit, b). When b = 0, the concept cr,a behaves either\nlike the PARITY concept indexed by r, or like its negation, according to the bit a (the \"mask\"). When b = 1,\nthe concept essentially ignores the input example and outputs some bit of the parity vector r.\nBelow, we consider the learnability of MASKED-PARITY = {cr,a } when the examples are drawn from\nthe uniform distribution over the domain {0, 1}d+log d+1 . In Section 5.3.1, we give a adaptive SQ learner for\nMASKED-PARITY under the uniform distribution. The adaptive learner uses two rounds of communication\nwith the SQ oracle: the first, to learn r from the b = 1 half of the input, and the second, to retrieve the bit a\nfrom the b = 0 half of the input via queries that depend on r.\nIn Section 5.3.2, we show that no nonadaptive SQ learner which uses 2o(d) examples can consistently\nproduce a hypothesis that labels significantly more than 3/4 of the domain correctly. The intuition is that\nas the queries are prepared nonadaptively, any information about r gained from the b = 1 half of the inputs\ncannot be used to prepare queries to the b = 0 half. Since information about a is contained only in the\nb = 0 half, in order to extract a, the SQ algorithm is forced to learn PARITY, which it cannot do with\n\n26\n\n\ffew examples. Our separation in the SQ model directly translates to a separation in the local model (using\nTheorem 5.14).\nThe following theorem summarizes our results.\nTheorem 5.16.\n1. There exists an efficient adaptive SQ learner for MASKED-PARITY over the uniform distribution.\n2. No nonadaptive SQ learner can learn MASKED-PARITY (with a polynomial number of queries)\neven under the uniform distribution on examples. Specifically, there is an SQ oracle O such that any\nnonadaptive SQ learner that makes t queries to O over the uniform distribution, all with tolerance\nat least 2\u2212d/3 , satisfies the following: if the concept cr\u0304,\u0101 is drawn uniformly at random from the\nt\nset of MASKED-PARITY concepts, then, with probability at least 21 \u2212 2d/3+2\nover cr\u0304,\u0101 , the output\n1\nhypothesis h of the learner has err(cr\u0304,\u0101 , h) \u2265 4 .\nCorollary 5.17. The concept classes learnable by nonadaptive SQ learners (resp. noninteractive local\nlearners) under the uniform distribution are a strict subset of the concept classes learnable by adaptive\nSQ learners (resp. interactive local learners) under the uniform distribution. This holds both with and\nwithout computational restrictions.\nWeak vs. Strong Learning. The learning theory literature distinguishes between strong learning, in which\nthe learning algorithm is required to produce hypotheses with arbitrarily low error (as in Definition 2.4,\nwhere the parameter \u03b1 can be arbitrarily small), and weak learning, in which the learner is only required\nto produce a hypothesis with error bounded below 1/2 by a polynomially small margin. The separation\nproved in this section (Theorem 5.16) applies only to strong learning: although no nonadaptive SQ learner\ncan produce a hypothesis with error much better than 1/4, it is simple to design a nonadaptive weak SQ\nlearner for MASKED-PARITY under the uniform distribution with error exactly 1/4.\nIn fact, it is impossible to obtain an analogue of our separation for weak learning. The characterization of\nSQ learnable classes in terms of \"SQ dimension\" by Blum et al. [12] implies that adaptive and nonadaptive\nSQ algorithms are equivalent for weak learning. This is not explicit in [12], but follows from the fact that the\nweak learner constructed for classes with low SQ dimension is non-adaptive. (Roughly, the learner works\nby checking if the concept at hand is approximately equal to one of a polynomial number of alternatives;\nthese alternatives depend on the input distribution and the concept class, but not on the particular concept at\nhand.)\nDistribution-free vs Distribution-specific Learning The results of this section concern the learnability of MASKED-PARITY under the uniform distribution. The class MASKED-PARITY does not separate adaptive from nonadaptive distribution-free learners, since MASKED-PARITY cannot be learned by\nany SQ learner under the distribution which is uniform over examples with b = 0 (in that case, learning\nMASKED-PARITY is equivalent to learning PARITY under the uniform distribution). Separating adaptive\nfrom nonadaptive distribution-free SQ learning remains an open problem.\n5.3.1\n\nAn Adaptive Strong SQ Learner for MASKED-PARITY over the Uniform Distribution\n\nOur adaptive learner for MASKED-PARITY uses two rounds of communication with the SQ oracle: first,\nto learn r from the b = 1 half of the input, and second, to retrieve the bit a from the b = 0 half of the input\nvia queries that depend on r. Theorem 5.16, part (1), follows from the proposition below.\n27\n\n\fA DAPTIVE SQ L EARNER AMP FOR MASKED-PARITY OVER THE U NIFORM D ISTRIBUTION\n1. For j = 1, . . . , d (in parallel)\n(a) Define gj : D \u2192 {0, 1} by\ngj (x, i, b, y) = (i = j) \u2227 (b = 1) \u2227 (y = \u22121) ,\nwhere x \u2208 {0, 1}d , i \u2208 {0, 1}log d , b \u2208 {0, 1}, and y = cr,a (x, i, b) \u2208 {+1, \u22121}.\n(\n1\n1 if answerj > 4d\n;\n1\n(b) answerj \u2190 SQD (gj , \u03c4 ), where \u03c4 = 4d+1 , and r\u0302j \u2190\n0 otherwise.\n2.\n\n(a) r\u0302 \u2190 r\u02c61 . . . r\u02c6d \u2208 {0, 1}d\n(b) Define gd+1 : D \u2192 {0, 1} by\ngd+1 (x, i, b, y) = (b = 0) \u2227 (y 6= (\u22121)r\u0302\n\nx\n\n).\n\nwhere x \u2208 {0, 1}d , i \u2208 {0, 1}log d , b \u2208 {0, 1}, and y = cr,a (x, i, b) \u2208 {+1, \u22121}.\n(\n1 if answerd+1 > 14 ;\n(c) answerd+1 \u2190 SQD (gd+1 , 51 )., and \u00e2 \u2190\n0 otherwise.\n(d) Output cr\u0302,\u00e2 .\nProposition 5.18 (Theorem 5.16, part (1), in detail). The algorithm AMP efficiently learns MASKED-PARITY\n(with probability 1) in 2 rounds using d+1 SQ queries computed over the uniform distribution with minimum\n1\ntolerance 4d+1\n.\nProof. Consider the d queries in the first round. If rj = 1, then\nE\n\n(x,i,b,y)\u2190D\n\n[gj (x, i, b, y)] =\n\nPr\n\n[(i = j) \u2227 (b = 1)] =\n\ni\u2208u {0,1}log d ,b\u2208u {0,1}\n\n1\n.\n2d\n\n1\nIf rj = 0, then E[gj (x, i, b, y)] = 0. Since the tolerance \u03c4 is less than 4d\n, each query gj reveals the jth bit\nof r exactly. Thus, the estimate r\u0302j is exactly rj , and r\u0302 = r.\nGiven that r\u0302 is correct, the second round query gd+1 is always 0 if a = 0. If a = 1, then gd+1 is 1 exactly\nwhen b = 0. Thus E[gd+1 (x, i, b, y)] = a2 (where a \u2208 {0, 1}). Since the tolerance is less than 14 , querying\ngd+1 reveals a: that is, \u00e2 = a, and so the algorithm outputs the target concept.\nNote that the functions g1 , . . . , gd+1 are all computable in time O(d), and the computations performed\nby AMP can be done in time O(d), so the SQ learner is efficient.\n\n5.3.2\n\nImpossibility of non-adaptive SQ learning for MASKED-PARITY\n\nThe impossibility result (Theorem 5.16, part (2)) for nonadaptive learners uses ideas from statistical query\nlower bounds (see, e.g., [39, 12, 55]).\nProof of Theorem 5.16, part (2). Recall that the distribution D is uniform over D = {0, 1}d+log(d)+1 . For\nfunctions f, h : {0, 1}d+log d+1 \u2192 {+1, \u22121}, recall that err(f, h) = Prx\u223cD [f (x) 6= h(x)]. Define the inner\n28\n\n\fproduct of f and h as:\nhf, hi =\n\n1 X\nf (x)h(x) = E [f (x)h(x)].\n|D|\nx\u223cD\nx\u2208D\n\nThe quantity hf, hi = Prx\u223cD [f (x) = h(x)] \u2212 Prx\u223cD [f (x) 6= h(x)] = 1 \u2212 2 * err(f, h) measures the\ncorrelation between f and h when x is drawn from the uniform distribution D.\nLet the target function cr\u0304,\u0101 be chosen uniformly at random from the set {cr,a }. Consider a nonadaptive\nSQ algorithm that makes t queries g1 , . . . , gt . The queries g1 , . . . , gt must be independent of r\u0304 and \u0101 since\nthe learner is nonadaptive. The only information about \u0101 is in the outputs associated with the b = 0 half of\nthe inputs (recall that cr\u0304,\u0101 (x, i, b) = (\u22121)ri when b = 1).\nThe main technical part of the proof follows the lower bound on SQ learning of PARITY. Using Fourier\nanalysis, we split the true answer to a query into three components: a component that depends on the query\ng but not the pair (r\u0304, \u0101), a component that depends on g and r\u0304 (but not \u0101), and a component that depends on\ng, r\u0304, and \u0101 (see Equation (7) below). We show that for most target concepts cr\u0304,\u0101 the last component can be\nignored by the SQ oracle. That is, a very close approximation to the correct output to the SQ queries made\nby the learner can be computed solely based on g and r\u0304. Consequently, for most target concepts cr\u0304,\u0101 , the SQ\noracle can return answers that are independent of \u0101, and hence \u0101 cannot be learned.\nConsider a statistical query g : {0, 1}d \u00d7 {0, 1}log d \u00d7 {0, 1} \u00d7 {+1, \u22121} \u2192 {+1, \u22121}. For some\n(x, i, b) \u2208 D, the value of g(x, i, b, *) depends on the label (i.e., (g(x, i, b, +1) 6= g(x, i, b, \u22121))) and\notherwise g(x, i, b, *) is insensitive to the label (i.e., (g(x, i, b, +1) = g(x, i, b, \u22121))). Every statistical\nquery g(*, *, *, *) can be decomposed into a label-independent and label-dependent part. This fact was first\nimplicitly noted by Blum et al. [12] and made explicit by Bshouty and Feldman [17] (Lemma 30). We adapt\nthe proof presented in [17] for our purpose.\nLet\nfg (x, i, b) =\n\ng(x, i, b, 1) \u2212 g(x, i, b, \u22121)\n2\n\nand\n\nCg =\n\n1\nE[g(x, i, b, 1) + g(x, i, b, \u22121)] .\n2\n\nWe can rewrite the expectation of g on any concept cr\u0304,\u0101 in terms of these quantities:\nE[g(x, i, b, cr\u0304,\u0101 (x, i, b))] = Cg + hfg , cr\u0304,\u0101 i .\nNote that Cg depends on the statistical query g, but not on the target function. We now wish to analyze\nthe second term, hfg , cr\u0304,\u0101 i, more precisely. To this end, we define the following functions parameterized by\ns \u2208 {0, 1}:\n\u001a\n\u001a\n0\nif b 6= s,\n0\nif b 6= s,\ns\ns\ncr\u0304,\u0101 (x, i, b) =\nand fg (x, i, b) =\n(6)\ncr\u0304,\u0101 (x, i, b) if b = s,\nfg (x, i, b) if b = s.\nRecall that hfg , cr\u0304,\u0101 i is a sum over tuples (x, i, b). We can separate the sum into two pieces: one with\ntuples where b = 0 and the other with tuples where b = 1. Using the functions csr\u0304,\u0101 , fgs just defined, we can\nwrite hfg , cr\u0304,\u0101 i = hfg0 , c0r\u0304,\u0101 i + hfg1 , c1r\u0304,\u0101 i. Hence,\n0 0\n1 1\nE[g(x, i, b, cr\u0304,\u0101 (x, i, b))] = Cg + hfg , cr\u0304,\u0101 i + hfg , cr\u0304,\u0101 i.\n\n(7)\n\nThe inner product hfg1 , c1r\u0304,\u0101 i depends on the statistical query g and on r\u0304, but not on \u0101. Thus only the\nmiddle term on the righthand side of (7) depends on \u0101.\n\n29\n\n\fConsider an SQ oracle O = Ocr\u0304,\u0101 ,D that responds to every query (g, \u03c4 ) as follows (recall that D is the\nuniform distribution):\n\u001a\nCg + hfg1 , c1r\u0304,\u0101 i\nif |hfg0 , c0r\u0304,\u0101 i| < \u03c4,\nOcr\u0304,\u0101 ,D (g, \u03c4 ) =\nE[g(x, i, b, cr\u0304,\u0101 (x, i, b))] otherwise.\nIf the condition |hfg0 , c0r\u0304,\u0101 i| < \u03c4 is met for all the queries (g, \u03c4 ) made by the learner, then the SQ oracle O\nnever replies with a quantity that depends on \u0101. We now show that this is typically the case.\nExtend the definition of csr\u0304,\u0101 (Equation 6) to any (r, a) \u2208 {0, 1}d \u00d7 {0, 1} by defining\n\u001a\n0\n0\n\u0001 if b = 1,\ncr,a (x, i, b) =\nhr,xi+a\ncr,a (x, i, b) = (\u22121)\nif b = 0.\nNote that for r, r0 \u2208 {0, 1}d and a \u2208 {0, 1},\nhc0r,a , c0r0 ,a i =\n\n\u001a\n\n1/2 if r = r0 ,\n0\nif r 6= r0 .\n\nWe get that {c0r,0 }r\u2208{0,1}d is an orthogonal set of functions, and similarly with {c0r,1 }r\u2208{0,1}d . The `2 norm\nq\n\u221a\n\u221a\nof c0r,0 is kc0r,0 k = hc0r,0 , c0r,0 i = 1/ 2, so the set { 2 * c0r,0 }r\u2208{0,1}d is orthonormal. A similar argument\n\u221a\nholds for { 2 * c0r,1 }r\u2208{0,1}d .\n\u221a\nExpanding the function fg0 in the orthonormal set { 2 * c0r,0 }r\u2208{0,1}d , we get:\nX\n\n\u221a\nhfg0 , 2 * c0r,0 i2 \u2264 kfg0 k2 = hfg0 , fg0 i \u2264 1/2 .\n\nr\u2208{0,1}d\n\n\u221a\n(The first inequality is loose in general because the set { 2 * c0r,0 }r\u2208{0,1}d spans a subset of dimension 2d\nwhereas fg0 is taken from a space of dimension 2d+log d+1 ). Similarly,\nX\n\u221a\nhfg0 , 2 * c0r,1 i2 \u2264 kfg0 k2 = hfg0 , fg0 i \u2264 1/2.\nr\u2208{0,1}d\n\nSumming the two previous equations, we get\nX\n\n2 * hfg0 , c0r,a i2 \u2264 1 .\n\n(r,a)\u2208{0,1}d \u00d7{0,1}\n\nHence, at most 22d/3\u22121 functions cr,a can have |hfg0 , c0r,a i| \u2265 1/2d/3 . Since r\u0304, \u0101 was chosen uniformly\nat random we can restate this: for any particular query g, the probability that c0r\u0304,\u0101 has inner product more\nthan 1/2d/3 with fg0 is at most 22d/3\u22121 /2d+1 = 2\u2212d/3 . This is true regardless of a: since c0r,0 = \u2212c0r,0 ,\nwe have |hfg0 , c0r,0 i| = |hfg0 , c0r,1 i|, so the event that |hfg0 , c0r\u0304,\u0101 i| \u2265 1/2d/3 happens with probability at most\n2\u2212d/3 over r\u0304, for \u0101 = 0, 1.\nRecall that the learner makes t queries, g1 , . . . , gt . Let Good be the event that |hfg0i , cr\u0304,\u0101 i| \u2264 1/2d/3 for\nall i \u2208 [t] (i.e., the oracle can answer each of the queries independently of \u0101). Taking a union bound over\nqueries, we have Pr[Good] \u2265 1 \u2212 t/2d/3+2 (where the probability is taken only over r\u0304).\nWe argued above that there is a valid SQ oracle which, conditioned on Good, can be simulated using r\u0304 but without knowledge of \u0101, as long as all queries are made with tolerance \u03c4 \u2265 1/2d/3 (as in\n30\n\n\fthe theorem statement). To conclude the proof, we now argue that no nonadaptive strong learner exists for MASKED-PARITY over the uniform distribution. For that we concentrate on the b = 0 half of\nthe inputs, where the outcome of cr\u0304,\u0101 (*) depends on a. Let h be the output hypothesis of the learner.\nFor any input (x, i, 0) we have cr\u0304,0 (x, i, 0) = \u2212cr\u0304,1 (x, i, 0). Thus either cr\u0304,0 (x, i, 0) 6= h(x, i, 0) or\ncr\u0304,1 (x, i, 0) 6= h(x, i, 0), and so some choice of \u0101 causes the error of h to be at least 1/4.\nLet A be the event that err(h, cr\u0304,\u0101 ) \u2265 1/4. Because Good depends only on r\u0304, we can think of \u0101 as being\nselected after the learner's hypothesis h whenever Good occurs. Thus, Pr[A | Good] \u2265 1/2. Using Good to\ndenote the complement of the event Good, we get\nPr[A] = Pr[A \u2227 Good] + Pr[A \u2227 Good]\n1\n\u2265 Pr[A | Good] Pr[Good] + 0 \u2265 (1 \u2212 t/2d/3+2 ).\n2\nTherefore, Pr[err(h, cr\u0304,\u0101 ) \u2265 1/4] \u2265 12 (1 \u2212 t/2d/3+2 ), as desired.\n\nAcknowledgments\nWe thank Enav Weinreb for many discussions related to the local model, Avrim Blum and Rocco Servedio\nfor discussions about related work in learning theory, and Katrina Ligett and Aaron Roth for discussions\nabout [14]. We also thank an anonymous reviewer for useful comments on the paper and, in particular, for\nthe simple proof of Theorem 3.6.\n\nReferences\n[1] AGRAWAL , D., AND AGGARWAL , C. C. On the design and quantification of privacy preserving data\nmining algorithms. In PODS (2001), ACM, pp. 247\u2013255.\n[2] AGRAWAL , R., AND S RIKANT, R. Privacy-preserving data mining. In SIGMOD (2000), vol. 29(2),\nACM, pp. 439\u2013450.\n[3] AGRAWAL , S., AND H ARITSA , J. R. A framework for high-accuracy privacy-preserving mining. In\nICDE (2005), IEEE Computer Society, pp. 193\u2013204.\n[4] A LEKHNOVICH , M. More on average case vs approximation complexity. In FOCS (2003), IEEE,\npp. 298\u2013307.\n[5] A MBAINIS , A., JAKOBSSON , M., AND L IPMAA , H. Cryptographic randomized response techniques.\nIn PKC (2004), vol. 2947 of LNCS, Springer, pp. 425\u2013438.\n[6] A NGLUIN , D., AND VALIANT, L. G. Fast probabilistic algorithms for hamiltonian circuits and matchings. J. Comput. Syst. Sci. 18, 2 (1979), 155\u2013193.\n[7] BARAK , B., C HAUDHURI , K., DWORK , C., K ALE , S., M C S HERRY, F., AND TALWAR , K. Privacy,\naccuracy, and consistency too: a holistic solution to contingency table release. In PODS (2007), ACM,\npp. 273\u2013282.\n[8] B EIMEL , A., K ASIVISWANATHAN , S. P., AND N ISSIM , K. Bounds on the sample complexity for private learning and private data release. In Theory of Cryptography Conference (TCC) (2010), D. Micciancio, Ed., LNCS, Springer.\n31\n\n\f[9] B EN -DAVID , S., P \u00c1L , D., AND S IMON , H.-U. Stability of k-means clustering. In COLT (2007),\nLNCS, pp. 20\u201334.\n[10] B EN -DAVID , S., VON L UXBURG , U., AND P \u00c1L , D. A sober look at clustering stability. In COLT\n(2006), LNCS, Springer, pp. 5\u201319.\n[11] B LUM , A., DWORK , C., M C S HERRY, F., AND N ISSIM , K. Practical privacy: The SuLQ framework.\nIn PODS (2005), ACM, pp. 128\u2013138.\n[12] B LUM , A., F URST, M. L., JACKSON , J., K EARNS , M. J., M ANSOUR , Y., AND RUDICH , S. Weakly\nlearning DNF and characterizing statistical query learning using Fourier analysis. In STOC (1994),\nACM, pp. 253\u2013262.\n[13] B LUM , A., K ALAI , A., AND WASSERMAN , H. Noise-tolerant learning, the parity problem, and the\nstatistical query model. J. ACM 50, 4 (2003), 506\u2013519.\n[14] B LUM , A., L IGETT, K., AND ROTH , A. A learning theory approach to non-interactive database\nprivacy. In STOC (2008), ACM, pp. 609\u2013618.\n[15] B LUMER , A., E HRENFEUCHT, A., H AUSSLER , D., AND WARMUTH , M. K. Occam's razor. Inf.\nProcess. Lett. 24, 6 (1987), 377\u2013380.\n[16] B OUSQUET, O., AND E LISSEEFF , A. Stability and generalization. Journal of Machine Learning\nResearch 2 (2002), 499 \u2013 526.\n[17] B SHOUTY, N. H., AND F ELDMAN , V. On using extended statistical queries to avoid membership\nqueries. Journal of Machine Learning Research 2 (2002), 359\u2013395.\n[18] C HERNOFF , H. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of\nobservations. Ann. Math. Statist. 23 (1952), 493\u2013507.\n[19] D EVROYE , L., AND WAGNER , T. Distribution-free performance bounds for potential function rules.\nIEEE Transactions on Information Theory 25, 5 (1979), 601\u2013604.\n[20] D INUR , I., AND N ISSIM , K. Revealing information while preserving privacy. In PODS (2003), ACM,\npp. 202\u2013210.\n[21] DWORK , C. Differential privacy. In ICALP (2006), LNCS, pp. 1\u201312.\n[22] DWORK , C., K ENTHAPADI , K., M C S HERRY, F., M IRONOV, I., AND NAOR , M. Our data, ourselves:\nPrivacy via distributed noise generation. In EUROCRYPT (2006), LNCS, Springer, pp. 486\u2013503.\n[23] DWORK , C., AND L EI , J. Differential privacy and robust statistics. In Symposium on the Theory of\nComputing (STOC) (2009).\n[24] DWORK , C., M C S HERRY, F., N ISSIM , K., AND S MITH , A. Calibrating noise to sensitivity in private\ndata analysis. In TCC (2006), LNCS, Springer, pp. 265\u2013284.\n[25] DWORK , C., M C S HERRY, F., AND TALWAR , K. The price of privacy and the limits of lp decoding.\nIn STOC (2007), ACM, pp. 85\u201394.\n\n32\n\n\f[26] DWORK , C., AND N ISSIM , K. Privacy-preserving datamining on vertically partitioned databases. In\nCRYPTO (2004), LNCS, Springer, pp. 528\u2013544.\n[27] DWORK , C., AND Y EKAHNIN , S. On lower bounds for noise in private analysis of statistical\ndatabases. Presentation at BSF/DIMACS/DyDan Workshop on Data Privacy, February 2008.\n[28] E LISSEEFF , A., E VGENIOU , T., AND P ONTIL , M. Stability of randomized learning algorithms.\nJournal of Machine Learning Research 6 (2005), 55\u201379.\n[29] E VFIMIEVSKI , A., G EHRKE , J., AND S RIKANT, R. Limiting privacy breaches in privacy preserving\ndata mining. In PODS (2003), ACM, pp. 211\u2013222.\n[30] F ISCHER , P., AND S IMON , H.-U. On learning ring-sum-expansions. SIAM Journal on Computing\n21, 1 (1992), 181\u2013192.\n[31] F REUND , Y., M ANSOUR , Y., AND S CHAPIRE , R. E. Generalization bounds for averaged classifiers.\nAnnals of Statistics 32, 4 (2004), 1698\u20131722.\n[32] H AUSSLER , D. Decision theoretic generalizations of the PAC model for neural net and other learning\napplications. Information and Computation 100, 1 (1992), 78\u2013150.\n[33] H ELMBOLD , D., S LOAN , R., AND WARMUTH , M. K. Learning integer lattices. SIAM Journal on\nComputing 21, 2 (Apr. 1992), 240\u2013266.\n[34] H OEFFDING , W. Probability inequalities for sums of bounded random variables. Journal of the\nAmerican Statistical Association 58, 301 (1963), 13\u201330.\n[35] H OPPER , N. J., AND B LUM , M. Secure human identification protocols. In ASIACRYPT (2001),\nvol. 2248 of LNCS, Springer, pp. 52\u201366.\n[36] JANK , W., AND S HMUELI , G. Statistical Methods in eCommerce Research. Wiley & Sons, 2008.\n[37] K ASIVISWANATHAN , S. P., L EE , H. K., N ISSIM , K., R ASKHODNIKOVA , S., AND S MITH , A. What\ncan we learn privately? In FOCS (2008), pp. 559\u2013569.\n[38] K ASIVISWANATHAN , S. P., AND S MITH , A. A note on differential privacy: Defining resistance to\narbitrary side information. CoRR arXiv:0803.39461 [cs.CR] (2008).\n[39] K EARNS , M. Efficient noise-tolerant learning from statistical queries. Journal of the ACM 45, 6\n(1998), 983\u20131006. Preliminary version in proceedings of STOC'93.\n[40] K EARNS , M., AND RON , D. Algorithmic stability and sanity-check bounds for leave-one-out crossvalidation. Neural Computation 11, 6 (1999), 1427 \u2013 1453.\n[41] K EARNS , M. J., S CHAPIRE , R. E., AND S ELLIE , L. M. Toward efficient agnostic learning. Machine\nLearning 17, 2-3 (1994), 115\u2013141.\n[42] K EARNS , M. J., AND VAZIRANI , U. V. An Introduction to Computational Learning Theory. MIT\npress, Cambridge, Massachusetts, 1994.\n[43] K UTIN , S., AND N IYOGI , P. Almost-everywhere algorithmic stability and generalization error. In\nUAI (2002), pp. 275\u2013282.\n33\n\n\f[44] M C S HERRY, F., AND TALWAR , K. Mechanism design via differential privacy. In FOCS (2007), IEEE,\npp. 94\u2013103.\n[45] M ISHRA , N., AND S ANDLER , M. Privacy via pseudorandom sketches. In PODS (2006), ACM,\npp. 143\u2013152.\n[46] M ORAN , T., AND NAOR , M. Polling with physical envelopes: A rigorous analysis of a human-centric\nprotocol. In EUROCRYPT (2006), LNCS, Springer, pp. 88\u2013108.\n[47] N ISSIM , K., R ASKHODNIKOVA , S., AND S MITH , A. Smooth sensitivity and sampling in private data\nanalysis. In STOC (2007), ACM, pp. 75\u201384.\n[48] R ASTOGI , V., H ONG , S., AND S UCIU , D. The boundary between privacy and utility in data publishing. In VLDB (2007), pp. 531\u2013542.\n[49] R EGEV, O. On lattices, learning with errors, random linear codes, and cryptography. In STOC (2005),\npp. 84\u201393.\n[50] S MITH , A. Efficient, differentially private point estimators. CoRR abs/0809.4794 (2008).\n[51] VALIANT, L. G. A theory of the learnable. Communications of the ACM 27 (1984), 1134\u20131142.\n[52] VAN DEN H OUT, A., AND VAN DER H EIJDEN , P. Randomized response, statistical disclosure control\nand misclassification: A review. International Statistical Review 70 (2002), 269\u2013288.\n[53] WARNER , S. L. Randomized response: A survey technique for eliminating evasive answer bias.\nJournal of the American Statistical Association 60, 309 (1965), 63\u201369.\n[54] WASSERMAN , L., AND Z HOU , S. A statistical framework for differential privacy.\narXiv:0811.2501v1 [math.ST] (2008).\n\nArXiv.org,\n\n[55] YANG , K. New lower bounds for statistical query learning. Journal of Computer and System Sciences\n70, 4 (2005), 485\u2013509.\n[56] Z HOU , S., L IGETT, K., AND WASSERMAN , L. Differential privacy with compression. ArXiv.org,\narXiv:0901.1365v1 [stat.ML] (2009).\n\nA\n\nConcentration Bounds\n\nWe need several standard tail bounds in this paper.\nTheorem A.1 (Multiplicative Chernoff Bounds (e.g. [18, 6])). Let X1 , . . . , Xn be i.i.d. Bernoulli random\nvariables with Pr[Xi = 1] = \u03bc. Then for every \u03c6 \u2208 (0, 1],\n\u0014P\n\u0015\n\u0012 2 \u0013\n\u03c6 \u03bcn\ni Xi\nPr\n\u2265 (1 + \u03c6)\u03bc \u2264 exp \u2212\nn\n3\nand\n\n\u0014P\nPr\n\ni Xi\n\nn\n\n\u03c62 \u03bcn\n\u2264 (1 \u2212 \u03c6)\u03bc \u2264 exp \u2212\n2\n\u0015\n\n34\n\n\u0012\n\n\u0013\n.\n\n\fTheorem A.2 (Real-valued Additive Chernoff-Hoeffding Bound [34]). Let X1 , . . . , Xn be i.i.d. random\nvariables with E[Xi ] = \u03bc and a \u2264 Xi \u2264 b for all i. Then for every \u03b4 > 0,\n\u0014 P\n\u0013\n\u0015\n\u0012\n\u22122\u03b4 2 n\ni Xi\nPr\n.\n\u2212 \u03bc \u2265 \u03b4 \u2264 2 exp\nn\n(b \u2212 a)2\nLemma A.3 (Sums of Laplace Random Variables). \u0010Let X\u0011\n1 , ..., Xn be i.i.d. random variables drawn from\n|x|\n1\nLap(\u03bb) (i.e., with probability density h(x) = 2\u03bb exp \u2212 \u03bb ). Then for every \u03b4 > 0,\n\u0014 Pn\nPr\n\ni=1 Xi\n\nn\n\n\u0012 2 \u0013\n\u03b4 n\n\u2265 \u03b4 = exp \u2212 2 .\n4\u03bb\n\u0015\n\nThe proof of this lemma is standard; we include it here since we were unable to find an appropriate\nreference.\nP\nProof. Let S = ni=1 Xi . By the Markov inequality, for all t > 0,\nPr[S > \u03b4n] = Pr[etS > et\u03b4n ] \u2264\n\nmS (t)\nE[etS ]\n= t\u03b4n ,\nt\u03b4n\ne\ne\n\nwhere mS (t) = E[etS ] is the moment generating function of S. To compute mS (t), note that the moment\n1\n1\ngenerating function of X \u223c Lap(\u03bb) is mX (t) = E[etX ] = 1\u2212(\u03bbt)\n2 , defined for 0 < t < \u03bb . Hence\nmS (t) = (mX (t))n = (1 \u2212 (\u03bbt)2 )\u2212n < exp(n(\u03bbt)2 ), where the last inequality holds for (\u03bbt)2 < 21 . We\n\u03b4 2\nget that Pr[S > \u03b4n] \u2264 exp(n((\u03bbt)2 \u2212 t\u03b4)). To complete the proof,\n2 \u03bb (note\n\u0010 \u0010set t \u0001=\n\u0011\u0011 that if\u0010\u03b4 < 21 and\n\u0011\n2\n\u03b4\n1\n\u03b42\n\u03b4\n2\n2\n\u03bb > 1 then (\u03bbt) = ( 2 \u03bb) < 2 ). We get that Pr[S > \u03b4n] \u2264 exp n 2 \u03bb \u2212 2 \u03bb\n= exp \u2212n \u03b44 \u03bb2 ,\nas desired.\n\n35\n\n\f"}