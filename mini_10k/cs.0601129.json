{"id": "http://arxiv.org/abs/cs/0601129v1", "guidislink": true, "updated": "2006-01-30T22:02:47Z", "updated_parsed": [2006, 1, 30, 22, 2, 47, 0, 30, 0], "published": "2006-01-30T22:02:47Z", "published_parsed": [2006, 1, 30, 22, 2, 47, 0, 30, 0], "title": "Instantaneously Trained Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0408004%2Ccs%2F0408026%2Ccs%2F0408013%2Ccs%2F0408066%2Ccs%2F0408003%2Ccs%2F0408025%2Ccs%2F0408014%2Ccs%2F0408015%2Ccs%2F0408042%2Ccs%2F0408010%2Ccs%2F0408028%2Ccs%2F0408033%2Ccs%2F0408020%2Ccs%2F0408034%2Ccs%2F0408049%2Ccs%2F0408062%2Ccs%2F0408045%2Ccs%2F0408016%2Ccs%2F0408057%2Ccs%2F0408030%2Ccs%2F0408053%2Ccs%2F0408056%2Ccs%2F0408063%2Ccs%2F0408027%2Ccs%2F0408017%2Ccs%2F0408051%2Ccs%2F0408069%2Ccs%2F0408002%2Ccs%2F0408052%2Ccs%2F0408047%2Ccs%2F0408061%2Ccs%2F0408039%2Ccs%2F0408041%2Ccs%2F0408006%2Ccs%2F0408021%2Ccs%2F0408038%2Ccs%2F0408005%2Ccs%2F0408024%2Ccs%2F0408068%2Ccs%2F0408040%2Ccs%2F0408055%2Ccs%2F0408060%2Ccs%2F0408050%2Ccs%2F0408043%2Ccs%2F0408022%2Ccs%2F0408054%2Ccs%2F0408064%2Ccs%2F0408044%2Ccs%2F0408048%2Ccs%2F0408065%2Ccs%2F0408012%2Ccs%2F0408009%2Ccs%2F0408046%2Ccs%2F0408058%2Ccs%2F0408059%2Ccs%2F0408008%2Ccs%2F0408023%2Ccs%2F0408032%2Ccs%2F0408029%2Ccs%2F0601004%2Ccs%2F0601011%2Ccs%2F0601093%2Ccs%2F0601119%2Ccs%2F0601116%2Ccs%2F0601040%2Ccs%2F0601060%2Ccs%2F0601058%2Ccs%2F0601103%2Ccs%2F0601129%2Ccs%2F0601069%2Ccs%2F0601067%2Ccs%2F0601021%2Ccs%2F0601018%2Ccs%2F0601092%2Ccs%2F0601044%2Ccs%2F0601012%2Ccs%2F0601079%2Ccs%2F0601128%2Ccs%2F0601074%2Ccs%2F0601045%2Ccs%2F0601020%2Ccs%2F0601015%2Ccs%2F0601072%2Ccs%2F0601130%2Ccs%2F0601010%2Ccs%2F0601026%2Ccs%2F0601088%2Ccs%2F0601095%2Ccs%2F0601003%2Ccs%2F0601087%2Ccs%2F0601075%2Ccs%2F0601096%2Ccs%2F0601023%2Ccs%2F0601099%2Ccs%2F0601112%2Ccs%2F0601101%2Ccs%2F0601076%2Ccs%2F0601085%2Ccs%2F0601084%2Ccs%2F0601051%2Ccs%2F0601025&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Instantaneously Trained Neural Networks"}, "summary": "This paper presents a review of instantaneously trained neural networks\n(ITNNs). These networks trade learning time for size and, in the basic model, a\nnew hidden node is created for each training sample. Various versions of the\ncorner-classification family of ITNNs, which have found applications in\nartificial intelligence (AI), are described. Implementation issues are also\nconsidered.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0408004%2Ccs%2F0408026%2Ccs%2F0408013%2Ccs%2F0408066%2Ccs%2F0408003%2Ccs%2F0408025%2Ccs%2F0408014%2Ccs%2F0408015%2Ccs%2F0408042%2Ccs%2F0408010%2Ccs%2F0408028%2Ccs%2F0408033%2Ccs%2F0408020%2Ccs%2F0408034%2Ccs%2F0408049%2Ccs%2F0408062%2Ccs%2F0408045%2Ccs%2F0408016%2Ccs%2F0408057%2Ccs%2F0408030%2Ccs%2F0408053%2Ccs%2F0408056%2Ccs%2F0408063%2Ccs%2F0408027%2Ccs%2F0408017%2Ccs%2F0408051%2Ccs%2F0408069%2Ccs%2F0408002%2Ccs%2F0408052%2Ccs%2F0408047%2Ccs%2F0408061%2Ccs%2F0408039%2Ccs%2F0408041%2Ccs%2F0408006%2Ccs%2F0408021%2Ccs%2F0408038%2Ccs%2F0408005%2Ccs%2F0408024%2Ccs%2F0408068%2Ccs%2F0408040%2Ccs%2F0408055%2Ccs%2F0408060%2Ccs%2F0408050%2Ccs%2F0408043%2Ccs%2F0408022%2Ccs%2F0408054%2Ccs%2F0408064%2Ccs%2F0408044%2Ccs%2F0408048%2Ccs%2F0408065%2Ccs%2F0408012%2Ccs%2F0408009%2Ccs%2F0408046%2Ccs%2F0408058%2Ccs%2F0408059%2Ccs%2F0408008%2Ccs%2F0408023%2Ccs%2F0408032%2Ccs%2F0408029%2Ccs%2F0601004%2Ccs%2F0601011%2Ccs%2F0601093%2Ccs%2F0601119%2Ccs%2F0601116%2Ccs%2F0601040%2Ccs%2F0601060%2Ccs%2F0601058%2Ccs%2F0601103%2Ccs%2F0601129%2Ccs%2F0601069%2Ccs%2F0601067%2Ccs%2F0601021%2Ccs%2F0601018%2Ccs%2F0601092%2Ccs%2F0601044%2Ccs%2F0601012%2Ccs%2F0601079%2Ccs%2F0601128%2Ccs%2F0601074%2Ccs%2F0601045%2Ccs%2F0601020%2Ccs%2F0601015%2Ccs%2F0601072%2Ccs%2F0601130%2Ccs%2F0601010%2Ccs%2F0601026%2Ccs%2F0601088%2Ccs%2F0601095%2Ccs%2F0601003%2Ccs%2F0601087%2Ccs%2F0601075%2Ccs%2F0601096%2Ccs%2F0601023%2Ccs%2F0601099%2Ccs%2F0601112%2Ccs%2F0601101%2Ccs%2F0601076%2Ccs%2F0601085%2Ccs%2F0601084%2Ccs%2F0601051%2Ccs%2F0601025&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper presents a review of instantaneously trained neural networks\n(ITNNs). These networks trade learning time for size and, in the basic model, a\nnew hidden node is created for each training sample. Various versions of the\ncorner-classification family of ITNNs, which have found applications in\nartificial intelligence (AI), are described. Implementation issues are also\nconsidered."}, "authors": ["Abhilash Ponnath"], "author_detail": {"name": "Abhilash Ponnath"}, "author": "Abhilash Ponnath", "arxiv_comment": "13 pages", "links": [{"href": "http://arxiv.org/abs/cs/0601129v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0601129v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0601129v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0601129v1", "journal_reference": null, "doi": null, "fulltext": "Instantaneously Trained Neural Networks\nAbhilash Ponnath\nAbstract: This paper presents a review of instantaneously trained neural networks\n(ITNNs). These networks trade learning time for size and, in the basic model, a new\nhidden node is created for each training sample. Various versions of the cornerclassification family of ITNNs, which have found applications in artificial intelligence\n(AI), are described. Implementation issues are also considered.\n\n1 Introduction\nThe human brain, the most complex known living structure in the universe, has the nerve\ncell or neuron as its fundamental unit. The number of neurons and connections between\nthe neurons is enormous; this ensemble enables the brain to surpass the computational\ncapacity of supercomputers in existence today. Artificial neural networks (ANNs) are\nmodels of the brain, which implement the mapping, \u0192: X \u2192 Y such that the task is\ncompleted in a \"certain\" sense. These networks are able to learn training samples as well\nas generalize them, which makes them an interesting research area. Unfortunately, it is\ndoubtful that current ANN models will ever have learning capacity that will match the\nperformance of living systems. Furthermore, the number of iterations required for\ngeneralization is often excessive and this motivates us to look into network designs where\nlearning is fast.\nFor comparison with biological systems, it should be noted that memory is not\nstored in a single area of the brain but distributed. Also, different parts of the brain are\nrequired for processing different kinds of memories. The hippocampus, parahippocampal\nregion and areas of the cerebral cortex (including prefrontal cortex) support declarative or\ncognitive memory and the amygdala, striatum, and cerebellum support the nondeclarative or behavioral memory. Declarative knowledge can be classified into working,\nepisodic, semantic memories and requires processing in the medieval temporal region and\nparts of the thalamus (Fig. 1). Non-declarative knowledge requires the processing of the\nbasil ganglia. Semantic memory represents the individual's knowledge in general,\nwhereas episodic memory structures individual's experiences.\nCreating ANNs that can learn and generalize information seems to be the first\nstep in the design of connectionist intelligent machines, and this is done by several ANN\nmodels. However, the learning and generalization time for the most popular models can\nbe very large. The ability to store information quickly not only provides a capacity that is\ncommon in biological systems, it has obvious applications in computational systems.\nScholarly opinion is divided on whether AI machines can have real intelligence. It\nis not clear why complexity alone in connectionist [17-19] or rule-based [6,20-22,26,30]\nmachines would suddenly create self-referring intelligence [14]. If classical computing\n\n\fmodels (and these include ANNs) are deficient, then such intelligence may be a\nconsequence of quantum information processing in the brain, but current models of\nquantum information processing suffer from their own shortcomings [e.g.9,10,12,13,15],\nand we will not consider them in this paper (we list them here to give a flavor of the\nissues involved both in respect of speed of computation as well as storage of\ninformation).\n\nFig 1: Different regions of the brain\nThis paper is written to provide an introduction to the field of instantaneously\ntrained neural networks (ITNN), which matches the biological capacity of fast learning.\nThe paper is organized as follows: Section 2 provides a brief historical background to\nresearch in neural networks. ITNNs are described in Section 3, applications are presented\nin Section 4 and hardware implementations of a specific type of ITNN are presented in\nSection 5.\n2 Neural network models\nThe first model of a neuron was proposed in 1943 by Warren McCulloch and Walter\nPitts, who showed that a network built with sufficient number of the neurons with proper\nweights, was capable of any feasible computation. The network suffered from the\nlimitation that it lacked the capacity to learn. D.Hebb gave the first neural network with\nlearning capability [3], based on the correlation principle that if neuron x was repeatedly\nstimulated by neuron y at times neuron x is active, then neuron x will become more\nsensitive to stimuli from neuron y. This lead to the notion of adjustable synaptic weights\nwhich is incorporated into most neural networks we know today. Frank Rosenblatt\nproposed a class of neural networks called perceptrons.\nIt was believed that neural networks could perform any function until 1969 when\nMinsky and Papert pointed out certain computations limitations of the perceptrons [21].\nSpecifically, they showed that they could not perform tasks like Exclusive-XOR. The\nfield lay dormant until mid 80s when the backpropagation (BP) algorithm was\nintroduced. Considered a breakthrough in area of neural networks, this algorithm suffers\n\n\ffrom the drawback that it may not converge [1,16,19]. The ADALINE (ADAptive LInear\nNEuron) was invented by Widrow and his group. An adaptive learning algorithm was\nused to adjust the weights by minimizing the mean square error. MADALINE was\nconsisted of many ADALINE networks in parallel. The output was either 1 or 0\ndepending on the output from individual ADALINE units.\nNetworks without a feedback path are called feedforward networks .Two different\nconfigurations exist in this category they are single-layer feedforward networks and\nmulti-layer feedforward networks.\n2.1 Radial basis function networks\nRadial basis function (RBF) networks have their roots in older pattern recognition\ntechniques such as functional approximation and mixture models [19]. The RBF is a two\nlayer neural network in which each hidden unit implements a radial activated function.\nThe output units implement a weighted sum of the hidden unit outputs. RBFs possess\nnonlinear approximation properties and hence can model complex mappings. The\nmapping for the input is nonlinear and linear to the output. The commonly used RBFs are\nGaussian, piecewise-linear approximation, cubic approximation, multiquadratic function,\nand inverse-multiquadratic function. RBF's convergence of weights is faster and less\nsensitive to interference compared to a BP network.\nNeural networks have two important capabilities: (a) learning (b) generalization.\nThe task at hand for a neural network is to learn a model in which it is embedded;\nobservations are made and pooled to form the training samples to train the network. The\nobservations pooled are either labeled or unlabeled depending weather the sample is an\ninput-output pair or not. Learning can be classified as supervised or unsupervised\ndepending weather the sample is labeled or unlabeled.\n2.2 WIZARD\nWIZARD consists of RAM chips which has look up tables which implement the neuron\nfunctions. The training is done by updating the contents of the RAM chips. The input\nand output must be digitized into binary vectors hence a back draw of this network and\ncannot be used for time series prediction. The schematic diagram of a RAM node is\nshown below. The node performs logical functions and returns a value to the given input\nvector.\nA discriminator is formed by arranging a group of k units .The output of a\ndiscriminator is the sum of the individual RAM nodes. The network consists of\ndiscriminators in parallel with each discriminator trained to recognize a different class of\npattern. Initially all the k2N locations of the discriminator are set to zero and training is\ndone by applying a input pattern at the input and the RAM nodes are set to value 1.\nIf the same input pattern is presented to the discriminator, the RAM node is set to\nread mode and previously written locations will all be accessed and RAM nodes will be\n\n\fresponded with 1. If a noisy version is presented then the output will be a function of\nnumber of previously written locations that are accessed by the noisy input. This is\nproportional to the similarity of the input pattern without noise to input pattern with\nnoise.\n\nFig 2: A RAM node\n\nFig 3: A discriminator\n\n2.3 Probabilistic neural network (PNN)\nA probabilistic neural network (PNN) may be viewed as a normalized RBF network in\nwhich there is a hidden unit for each training value [27]. These hidden units are called\n\"kernels\" and they are typically probability density functions such as the Gaussian. The\nhidden-to-output weights are usually 1 or 0; for each hidden unit, a weight of 1 is used for\nthe connection to the output which is true for it, and all other connections are given\nweights of 0. The other possibility is to assign weights for the prior probabilities of each\nclass. The weights that need to be learned are the widths of the RBF units.\nPNN training is not iterative in the same sense as backpropagation, but it requires\nestimation of kernel bandwidth, and it may require the processing of all training\ninformation. In reality, therefore, it is not instantaneous.\n\n\fThe principle of operation of PNN is based on statistical technique, combining\nboth the bayes strategy and a nonparametric estimation technique. The estimator has a\nGaussian probability distribution function:\n\n[\n\nf k (x) = (2\u03a0) \u03c3 R S\nR/ 2\n\n] \u2211exp \u239b\u239c\u239c \u2212 (X \u2212 X2\u03c3) (X \u2212 X ) \u239e\u239f\u239f\n\u23a0\n\u239d\nT\n\n\u22121\n\nki\n\nki\n\n2\n\ni=1,S\n\nA PNN for a three-class problem is now described. The inputs are R-dimensional\ncontinuous valued vectors normalized to unit length. The input units receive the inputs\nand feed the to the pattern units, each of which form a dot product with a weight vector\nand perform a nonlinear operation on the dot product and feeds to the summation unit its\nactivation level. The nonlinear operation performed is an exponential function of the\nform exp[(zi-1)/\u03c32]. \u03c3 is called smoothing parameter. For good performance \u03c3 takes\nvalues between 0.2 and 0.3 [3].\n\nFig 4: A PNN with 3 output classes\nThe weight vector in each of the pattern is set equal to one of the vectors in the\ntraining set and then connecting the pattern unit output to the appropriate summation unit\nwith a connection weight of one. Using Bayes decision theory a test pattern can be\nassigned to category k if\n\nhk l k f k ( x) > hq l q f q ( x)\n\n\u2200q \u2260 k\n\nhk is a priori probability of occurrence of patterns from category k\nlk is loss associated with classifying a test pattern into category other than k when in\nreality belongs to k. The output unit receives the output from each summation multiplied\n\n\fhk lk\nand then determines the category into which the vector must be classified. This\nnk\nnetwork has the drawback that it cannot be used for applications involving function\napproximation. We need to use the generalized regression neural networks (GRNN) if the\napplication involves function approximation.\n\nby\n\nWe note that WIZARD and PNN do not possess instantaneous learning\ncapabilities although that is claimed for them by some people,\n3 Instantaneous neural networks\n\nThe principal ITNN is the corner classification neural network (CCNN) that includes its\nvariant FC neural network (FCNN). These networks are an attempt to model biological\nmemory. The FC networks are not purely instantaneous, and they need some learning.\nHowever, this learning could be done very quickly under certain conditions. Their\ngeneralization capacity seems to be almost as good as that of backpropagation networks\n[23, 28, 29].\nMemory may be divided into three types, sensory, short-term and long-term\nmemory. The duration for which information can be retained is shortest for sensory\nmemory and longest for greatest for long-term memory, short-term memory stands in\nbetween the sensory and long-term memory.\nShort-time memory also called working memory involves the ruminate thoughts\nwe have just encountered, the information fades approximately after twenty seconds if it\nis not renewed through rehearsal. Short-term memory needs to be protected from\noverloading by sensory stimulation, two cognitive processes that help in preventing\noverloading are sensory gating and selective attention. Sensory gating is the process by\nwhich certain channels are turned on while others are turned off. Selective attention is the\nprocess of culling information received by one channel i.e. abate information 'x'\nreceived by a channel in favor to information 'y' entering the same channel. The amount\nof information that short-term memory can hold is limited but it can be extended by\n\"grouping\" information.\n\n3.1 Corner classification neural networks (CCNN)\n\nThe corner classification (CC) network is based on the idea of phonological loop and the\nvisio-spatial sketchpad [2,11]. It was proposed by Kak in 1992 in three variations [7,8].\nThese and its more advanced variants are also known as the class of Kak neural networks.\nThe concept of radius of generalization was introduced in CC3 and thus this\nneural network overcame the generalization problem that plagued the earlier CC2\nnetwork. The Hamming distance was used for classification between binary vectors, i.e.\nany test vector whose Hamming distance from a training vector is smaller than the radius\nof generalization of the network is classified in the same output class as that training\n\n\fvector. A unique neuron is associated with each training sample and each node in the\nnetwork acts as a filter for the training sample. The filter is realized by making it act as a\nhyper plane to separate the corner of the n-dimensional cube represented by the training\nvector and hence the name corner-classification (CC) technique.\nThere are four versions of the CC technique, represented by CC1 through CC4.\nThe CC4 is shown to be better than the other networks in the CC category [5]. The\nnumber of input and output neurons is equal to the length of input and output patterns or\nvectors. The number of hidden neurons is equal to number of training samples the\nnetwork requires. The last node of the input layer is set to one to act as a bias to the\nhidden layer. The binary step function is used as activation function for both the hidden\nand output neurons. The output of the function is 1 if summation is positive and zero\notherwise.\n\nFig 5: General CC4 architecture\nThe hidden neuron receives a -1 if the input vector is \u20131 and 1 if the input is 0 and\n+1. The weight of the link from base node to a hidden neuron is r\u2013s+1, r is the radius of\ngeneralization and s is the number of ones in the input sequence. The weights in the\noutput layer are equal to 1 if the output value is 1 and \u20131 if the output value is 0. This\namounts to learning both the input class and its complement and thus instantaneous. The\nradius of generalization, r can be seen by considering the all-zero input vectors for which\nwn+1 = r + 1. The choice of r will depend on the nature of generalization sought. This\nnetwork also suffers from the draw back that the input output data must be digitized.\n\n3.2 FC neural network\n\nHuman decision making has the element of uncertainty in it, for example when we\ninterpret temperature we can only express the degree of hotness or coldness, this is not\nlike a truth statement which might be true or false. One might say it is hot, another might\nsay its or very hot etc. The principle of fuzzy classification networks depend on the\n\n\fconcept of \"nearest neighbor\", it consists of three layers-an input layer, a hidden layer\nand an output layer. The FC acronym has been seen either to stand for fuzzy\nclassification or fast classification [29].\nThe input data is normalized and presented as input vector x .The hidden neuron\nis represented by the weight vector wi and its elements are represented by wi, j ,i=(1,2,...S)\nand j=(1,2,...,R). The output is the dot product of the vectors \u03bc and u. This network can\nbe trained with just two passes of the samples, the first pass assigns the synaptic weights\nand the second pass determines the radius of generalization for each training sample. By\nfuzzification of the location of the each training sampler and by assigning fuzzy\nmembership functions of output classes to new input vectors.\n\nFig 6: FC network architecture\nThe network behaves as a 1NN classifier and a kNN classifier according to\nweather the input vector falls within the radius of generalization of a training vector or\nnot hence the radius of generalization acts as a switch between the 1NN classifier and the\nkNN classifier. The FC network meets the specifications set by traditional function\napproximation that every data point is covered in the given training sample and also\nCover's theorem on separability of patterns. In practical case k values is determined by\nthe sample size and be a fraction of the sample size. If k=S then the FC network operating\nas a kNN classifier can be viewed as a RBF network provided the membership function is\nchosen to be a Gaussian distributed, moreover if the weighting function is chosen to be\nthe membership function the FC network can be considered as a kernel regression.\n\n4 Applications of ITNNs\n\nWe speak of three broad applications that may be listed as (1) function approximation (2)\ntime series prediction (3) pattern classification and data fusion for metasearch engines.\nEach of these areas naturally covers many specific applications. For example, time series\n\n\fprediction may be applied to financial or signal data, and pattern recognition includes\napplication to engineering, military, or medical systems.\n4.1 Function approximation\n\nThe problem in function approximation is to implement a function, F(x), which\napproximates an unknown function, f(x), with a priori as the input-output pairs in a\nEuclidian sense for all inputs.\nThe input-output pairs form the training samples. The error can be made as small\nas possible by increasing the size of the sample space.\n\nFig 7: System identification\nSystem Identification and Inverse system modeling are two areas where ITNNs can\nbe useful. The schematic block diagrams for both the areas are shown in Figures 7 and 8\nalong with the neural network block.\n\nFig 8: Inverse system modeling\nSpecific applications in representation and control of systems need to be investigated\nfurther.\n4.2 Time series prediction\n\nTime series prediction is widely used in financial data for prediction of stocks, currency\nand interest rates and engineering such as electric load demand. The network is trained by\n\n\fhistorical data set with time index and the network predicts the future values based on\npast values. Two time series examples that have been investigated are Henon map and\nMackey-Glass time series [28,29], which are both chaotic time series and, therefore,\ntheir statistical properties remain unchanged.\nWe don't know how ITNNs perform for real world data, which is characterized by\ndifferent modes, as compared to other neural network techniques.\n4.3 Pattern classification\n\nOur brain is highly developed and can recognized different visual, audio and olfactive\npatterns. Neural networks can be trained in order to recognize the patterns. The term\n\"pattern classification\" refers to the process by which the network learns from a given set\nof training values the mapping between the given input-output pairs and attempts to\nassign new input patterns to one of the output classes predefined by the training classes.\nStudies show that ITNNs do almost as well as backpropagation networks in basic pattern\nclassification problems [23,28].\nMetasearch engines combine results to queries submitted to different search engines after\ndiscarding redundant results. ITNNs can be used to fuse data in this application. In a\nmethod proposed by Shu and Kak [25] the network was built using the keywords from\nweb pages. Individual key words were set either to 1 or 0 depending weather they were in\ntop or bottom of a list of search results.\n5 Implementation of ITNNs\n\nImplementation of an algorithm speaks of its success in the commercial arena. The CC4\nalgorithm was implemented using reconfigurable computing and to design an optical\nneural network .The FC network was implemented on FPGAs.\nZhu and Sutton [5] implemented Kak's FC network on FPGAs. The Celoxica\nRC2000 board with Xilinx XC2V6000 Virtex-II chip was used and JHDL hardware\ndescription language was used for design and simulation for hardware implementation.\nThe hidden neuron circuit was based on Euclidian distance in Kak's FC network but such\na implementation in a FPGA would require a computational complexity of O(mn)\n\"square operations\" where m is the number of neurons and n is the number of elements\nin the weight vector. Hence, the two different distance metrics were proposed, the city\nblock distance and the box distance. It should be noted that a general distance metric is\np\ndefined as d i = xi \u2212 wi and the value of the parameter p gives the distance different\nnames, as shown below for case p=1, 2, \u221e.\n\n\fn\n1\n\u23a7\n1\n\u2212\n=\n\u2212\nx\nw\nx\nw\n\u2211\ni\ni\ni\nj\ni\nj\n,\n,\n\u23aa\nj =1\n\u23aa\nn\n2\n2\n\u23aa\ndi = \u23a8 xi \u2212 wi = \u2211 xi , j \u2212 wi , j\nj =1\n\u23aa\n\u23aa xi \u2212 wi \u221e = Max xi , j \u2212 wi , j\n\u23aa\n\u23a9\n\n(\n\n)\n\np =1\n\nthe city block distance\n\np=2\n\nthe Euclidian distance\n\np=\u221e\n\nthe box distance\n\nThe use of the two different distance metrics is justified by the experiments done\nby Estlick which gave acceptable results [1]. The activation circuit was implemented as\nan n-bit constant comparator as the radius of generalization is constant after training.\nBitonic selection network is used to implement the kNN circuit. Implementation on\nFPGA requires that the algorithm be simple, modular and highly parallel and the FC\nnetworks possess these characteristics. The ease of implementation of the FC networks\non a FPGA justifies the claim that ITNNS can be modeled into hardware and can be\ncommercialized.\nZhu and Milne [31] showed that Kak's CC4 is hardware implementable in\nreconfigurable computing using fine grained parallelism. Shortt, Keating, Moulinier,\nPannell [24] made an optical implementation the Kak neural network using a bipolar\nmatrix vector multiplier, but suitable modifications to the structure and training algorithm\nwere required to build an optical neural network implementing N-parity.\n6 Conclusions\n\nThis paper provides a review of neural networks with an emphasis on ITNNs. The corner\nclassification network learns by creating a new hidden node for each training sample. Its\nadvantage in speed (for training) is a consequence of the price that is being paid in size of\nthe network. While these hidden neurons can be pruned, the size will remain much larger\nthan that of a backpropagation network. In applications, where the size does not matter,\nthese networks are an attractive alternative to backpropagation. Although, they have been\nevaluated for various kinds of chaotic time series, the performance of these networks on\nvarious benchmark data remains to be checked.\nReferences\n\n[1] M. Estlick, \"Algorithmic transformations in the implementation of K- means\nclustering on reconfigurable hardware\", in Proceedings of the Ninth ACM\nInternational Symposium on Field-Programmable Gate Arrays., ACM SIGDA:\nCalifornia. 2001, p. 103-110.\n[2] M. S. Gazzaniga, The Cognitive Neurosciences, MIT Press, Cambridge, Mass., 1995.\n[3] D. Hebb, The Organization of Behavior. Wiley, New York, 1949.\n[4] J.H. Holland, Adaptation in Natural and Artificial Systems. Univ of Michigan Press,\nAnn Arbor, 1975.\n\n\f[5] Z. Jihan, P. Sutton \"An FPGA implementation of Kak's instantaneously-trained, fastclassification neural networks\" Proceedings of the 2003 IEEE International\nConference on Field-Programmable Technology (FPT), December 2003.\n[6] A.C. Kak and M. Slaney, Principles of Computerized Tomographic Imaging, IEEE\nPress, 1988.\n[7] S. Kak, New algorithms for training feedforward neural networks. Pattern\nRecognition Letters 15, 1994, pp.295-298.\n[8] S. Kak, \"On generalization by neural networks,\" Information Sciences 111, 1998, pp.\n293-302.\n[9] S. Kak, \"Quantum information in a distributed apparatus.\" Found. Phys. 28 ,1998, pp.\n1005-1012; arXiv: quant-ph/9804047.\n[10] S. Kak, \"The initialization problem in quantum computing.\" Found. Phys. 29, pp.\n267-279, 1999; arXiv: quant-ph/9805002.\n[11] S. Kak, \"Better web searches and faster prediction using instantaneously trained\nneural networks.\" IEEE Intelligent Systems, vol. 14(6), pp. 78-81, 1999.\n[12] S. Kak, \"Statistical constraints on state preparation for a quantum computer.\"\nPramana, 57 (2001) 683-688; arXiv: quant-ph/0010109.\n[13] S. Kak, \"General qubit errors cannot be corrected.\" Information Sciences, 152, 195202 (2003); arXiv: quant-ph/0206144.\n[14] S. Kak, \"Artificial and biological intelligence.\" ACM Ubiquity, vol. 6, number 42,\n2005, pp. 1-20; arXiv: cs.AI/0601052.\n[15] S. Kak, \"The information complexity of quantum gates.\" Int. J. of Theoretical\nPhysics, 45, 2006; arXiv: quant-ph/0506013\n[16] S. V. Kartaloppoulos, \"Understanding neural networks and fuzzy logic: Basic\nConcepts and Applications\", IEEE Press, New York, 1995.\n[17] C. T. C. Lin and C. S. G. Lee, Fuzzy Neural Systems. Prentice Hall, Upper Saddle\nRiver, NJ, May 1996.\n[18] C.G. Looney, Pattern Recognition Using Neural Networks. Oxford University\nPress, Oxford, 1997.\n[19] M. Meng and A. C. Kak, \"Mobile Robot Navigation using Neural Networks and\nNonmetrical Environment Models,\" IEEE Control Systems, pp. 30-39, October 1993.\n[20] M. Minsky and S. Papert, Perceptrons. MIT Press, Cambridge, 1969.\n[21] M. Minsky, The Society of Mind. Simon and Schuster, New York, 1987.\n[22] E.A. Patrick, Fundamentals of Pattern Recognition. Prentice Hall, Englewood\nCliffs, 1972.\n[23] P. Raina, \"Comparison of learning and generalization capabilities of the Kak and\nthe backpropagation algorithms.\" vol. 81, 1981, pp. 261-274.\n[24] A. Shortt , J. G. Keating, L. Moulinier, C. N. Pannell , \"Optical implementation of\nthe Kak neural network\" Information Sciences 171, 2005, p.273-287.\n[25] B. Shu and S. Kak \"A neural network-based intelligent metasearch engine.\"\nInformation Sciences, vol. 120, 1999, p. 1-11.\n[26] H.A. Simon, The Sciences of the Artificial. MIT Press, Cambridge, 1969.\n[27] D. F. Specht, \"Probabilistic neural networks and general regression neural\nnetworks.\" In Chen, C.H., ed., 1996. Fuzzy Logic and Neural Network Handbook,\nMcGraw-Hill, New York, Chapter 3.\n\n\f[28] K. W. Tang and S. Kak, \"A new corner classification approach to neural network\ntraining,\" Circuits, Systems Signal Processing 17, 1998, p. 459-469.\n[29] K. W. Tang and S. Kak, \"Fast classification networks for signal processing,\"\nCircuits, Systems Signal Processing 21, 2002, 207- 224.\n[30] P.H. Winston, Artificial Intelligence. Addison-Wesley, Reading, 1992.\n[31] J. Zhu and G. Milne, \"Implementing Kak neural networks on a reconfigurable\ncomputing platform,\" In FPL 2000, LNCS 1896, R.W. Hartenstein and H.\nGruenbacher (eds.), Springer-Verlag, 2000, p. 260-269.\n\n\f"}