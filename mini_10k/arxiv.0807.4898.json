{"id": "http://arxiv.org/abs/0807.4898v5", "guidislink": true, "updated": "2009-04-23T22:17:37Z", "updated_parsed": [2009, 4, 23, 22, 17, 37, 3, 113, 0], "published": "2008-07-30T17:23:48Z", "published_parsed": [2008, 7, 30, 17, 23, 48, 2, 212, 0], "title": "Random matrices: Universality of ESDs and the circular law", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0807.0594%2C0807.0917%2C0807.3233%2C0807.4748%2C0807.4948%2C0807.1676%2C0807.4658%2C0807.3286%2C0807.2879%2C0807.0597%2C0807.2998%2C0807.3339%2C0807.0502%2C0807.3597%2C0807.2080%2C0807.4316%2C0807.2107%2C0807.5046%2C0807.3513%2C0807.4770%2C0807.3480%2C0807.3012%2C0807.1284%2C0807.1361%2C0807.3891%2C0807.1865%2C0807.1055%2C0807.4652%2C0807.0547%2C0807.1749%2C0807.3176%2C0807.1096%2C0807.2844%2C0807.4898%2C0807.2910%2C0807.2604%2C0807.2322%2C0807.3970%2C0807.1151%2C0807.4318%2C0807.4128%2C0807.1503%2C0807.0255%2C0807.3612%2C0807.0988%2C0807.0093%2C0807.2181%2C0807.2078%2C0807.3302%2C0807.0367%2C0807.4539%2C0807.0767%2C0807.3200%2C0807.0124%2C0807.3114%2C0807.3964%2C0807.5124%2C0807.2029%2C0807.2332%2C0807.1634%2C0807.0542%2C0807.2054%2C0807.2984%2C0807.2989%2C0807.1336%2C0807.1183%2C0807.4025%2C0807.2692%2C0807.1356%2C0807.0972%2C0807.2501%2C0807.0363%2C0807.2361%2C0807.3308%2C0807.4606%2C0807.4467%2C0807.2441%2C0807.3585%2C0807.0588%2C0807.3814%2C0807.3544%2C0807.5082%2C0807.1267%2C0807.3873%2C0807.0216%2C0807.0210%2C0807.3360%2C0807.0766%2C0807.2019%2C0807.1968%2C0807.3721%2C0807.0391%2C0807.4190%2C0807.2997%2C0807.4456%2C0807.4630%2C0807.2641%2C0807.0471%2C0807.1307%2C0807.1742%2C0807.1663&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Random matrices: Universality of ESDs and the circular law"}, "summary": "Given an $n \\times n$ complex matrix $A$, let\n  $$\\mu_{A}(x,y):= \\frac{1}{n} |\\{1\\le i \\le n, \\Re \\lambda_i \\le x, \\Im\n\\lambda_i \\le y\\}|$$ be the empirical spectral distribution (ESD) of its\neigenvalues\n  $\\lambda_i \\in \\BBC, i=1, ... n$.\n  We consider the limiting distribution (both in probability and in the almost\nsure convergence sense) of the normalized ESD $\\mu_{\\frac{1}{\\sqrt{n}} A_n}$ of\na random matrix $A_n = (a_{ij})_{1 \\leq i,j \\leq n}$ where the random variables\n$a_{ij} - \\E(a_{ij})$ are iid copies of a fixed random variable $x$ with unit\nvariance. We prove a \\emph{universality principle} for such ensembles, namely\nthat the limit distribution in question is {\\it independent} of the actual\nchoice of $x$. In particular, in order to compute this distribution, one can\nassume that $x$ is real of complex gaussian. As a related result, we show how\nlaws for this ESD follow from laws for the \\emph{singular} value distribution\nof $\\frac{1}{\\sqrt{n}} A_n - zI$ for complex $z$. As a corollary we establish\nthe Circular Law conjecture (in both strong and weak forms), that asserts that\n$\\mu_{\\frac{1}{\\sqrt{n}} A_n}$ converges to the uniform measure on the unit\ndisk when the $a_{ij}$ have zero mean.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0807.0594%2C0807.0917%2C0807.3233%2C0807.4748%2C0807.4948%2C0807.1676%2C0807.4658%2C0807.3286%2C0807.2879%2C0807.0597%2C0807.2998%2C0807.3339%2C0807.0502%2C0807.3597%2C0807.2080%2C0807.4316%2C0807.2107%2C0807.5046%2C0807.3513%2C0807.4770%2C0807.3480%2C0807.3012%2C0807.1284%2C0807.1361%2C0807.3891%2C0807.1865%2C0807.1055%2C0807.4652%2C0807.0547%2C0807.1749%2C0807.3176%2C0807.1096%2C0807.2844%2C0807.4898%2C0807.2910%2C0807.2604%2C0807.2322%2C0807.3970%2C0807.1151%2C0807.4318%2C0807.4128%2C0807.1503%2C0807.0255%2C0807.3612%2C0807.0988%2C0807.0093%2C0807.2181%2C0807.2078%2C0807.3302%2C0807.0367%2C0807.4539%2C0807.0767%2C0807.3200%2C0807.0124%2C0807.3114%2C0807.3964%2C0807.5124%2C0807.2029%2C0807.2332%2C0807.1634%2C0807.0542%2C0807.2054%2C0807.2984%2C0807.2989%2C0807.1336%2C0807.1183%2C0807.4025%2C0807.2692%2C0807.1356%2C0807.0972%2C0807.2501%2C0807.0363%2C0807.2361%2C0807.3308%2C0807.4606%2C0807.4467%2C0807.2441%2C0807.3585%2C0807.0588%2C0807.3814%2C0807.3544%2C0807.5082%2C0807.1267%2C0807.3873%2C0807.0216%2C0807.0210%2C0807.3360%2C0807.0766%2C0807.2019%2C0807.1968%2C0807.3721%2C0807.0391%2C0807.4190%2C0807.2997%2C0807.4456%2C0807.4630%2C0807.2641%2C0807.0471%2C0807.1307%2C0807.1742%2C0807.1663&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Given an $n \\times n$ complex matrix $A$, let\n  $$\\mu_{A}(x,y):= \\frac{1}{n} |\\{1\\le i \\le n, \\Re \\lambda_i \\le x, \\Im\n\\lambda_i \\le y\\}|$$ be the empirical spectral distribution (ESD) of its\neigenvalues\n  $\\lambda_i \\in \\BBC, i=1, ... n$.\n  We consider the limiting distribution (both in probability and in the almost\nsure convergence sense) of the normalized ESD $\\mu_{\\frac{1}{\\sqrt{n}} A_n}$ of\na random matrix $A_n = (a_{ij})_{1 \\leq i,j \\leq n}$ where the random variables\n$a_{ij} - \\E(a_{ij})$ are iid copies of a fixed random variable $x$ with unit\nvariance. We prove a \\emph{universality principle} for such ensembles, namely\nthat the limit distribution in question is {\\it independent} of the actual\nchoice of $x$. In particular, in order to compute this distribution, one can\nassume that $x$ is real of complex gaussian. As a related result, we show how\nlaws for this ESD follow from laws for the \\emph{singular} value distribution\nof $\\frac{1}{\\sqrt{n}} A_n - zI$ for complex $z$. As a corollary we establish\nthe Circular Law conjecture (in both strong and weak forms), that asserts that\n$\\mu_{\\frac{1}{\\sqrt{n}} A_n}$ converges to the uniform measure on the unit\ndisk when the $a_{ij}$ have zero mean."}, "authors": ["Terence Tao", "Van Vu", "Manjunath Krishnapur"], "author_detail": {"name": "Manjunath Krishnapur"}, "author": "Manjunath Krishnapur", "arxiv_comment": "45 pages, 8 figures, submitted, Acta Math. The main article is by Tao\n  and Vu, the appendix is by Krishnapur, and the figures are by Phillip Wood. A\n  simplified proof of the replacement principle added; some other corrections", "links": [{"href": "http://arxiv.org/abs/0807.4898v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0807.4898v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "15A52", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0807.4898v5", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0807.4898v5", "journal_reference": null, "doi": null, "fulltext": "RANDOM MATRICES:\nUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\nTERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\narXiv:0807.4898v5 [math.PR] 23 Apr 2009\n\nAbstract. Given an n \u00d7 n complex matrix A, let\n1\n|{1 \u2264 i \u2264 n, Re\u03bbi \u2264 x, Im\u03bbi \u2264 y}|\nn\nbe the empirical spectral distribution (ESD) of its eigenvalues \u03bbi \u2208\nC, i = 1, . . . n.\nWe consider the limiting distribution (both in probability and in\nthe almost sure convergence sense) of the normalized ESD \u03bc \u221a1 An\n\u03bcA (x, y) :=\n\nn\n\nof a random matrix An = (aij )1\u2264i,j\u2264n where the random variables\naij \u2212 E(aij ) are iid copies of a fixed random variable x with unit\nvariance. We prove a universality principle for such ensembles,\nnamely that the limit distribution in question is independent of\nthe actual choice of x. In particular, in order to compute this\ndistribution, one can assume that x is real of complex gaussian.\nAs a related result, we show how laws for this ESD follow from\nlaws for the singular value distribution of \u221a1n An \u2212 zI for complex\nz.\nAs a corollary we establish the Circular Law conjecture (both\nalmost surely and in probability), that asserts that \u03bc \u221a1 An conn\nverges to the uniform measure on the unit disk when the aij have\nzero mean.\n\n1. Introduction\n1.1. Empirical spectral distributions. This paper is concerned with\nthe convergence of empirical spectral distributions of random matrices,\nboth in the sense of convergence in probability and in the almost sure\nsense.\nDefinition 1.2 (Modes of convergence). For each n, let Fn be a random\nvariable taking values in some Hausdorff topological space X, and let\nF be another element of X.\n\u2022 We say that Fn converges in probability to F if for every neighbourhood V of F , we have limn\u2192\u221e P(Fn \u2208 V ) = 1.\n\u2022 We say that Fn converges almost surely to F if we have P(limn\u2192\u221e Fn =\nF ) = 1.\n1\n\n\f2\n\nTERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nSimilarly, if Xn is a scalar random variable, we say that Xn is bounded\nin probability if we have\nlim lim inf P(|Xn | \u2264 C) = 1\n\nC\u2192\u221e n\u2192\u221e\n\nand almost surely bounded if we have\nP(lim sup |Xn | < \u221e) = 1.\nn\n\nLet Mn (C) denote the set of n \u00d7 n complex matrices. For A \u2208 Mn (C),\nwe let\n1\n\u03bcA (s, t) := |{1 \u2264 i \u2264 n, Re\u03bbi \u2264 s, Im\u03bbi \u2264 t}|\nn\nbe the empirical spectral distribution (ESD) of its eigenvalues \u03bbi \u2208\nC, i = 1, . . . n. This is a discrete probability measure on C.\nNow suppose that An \u2208 Mn (C) is a random matrix ensemble (i.e. a\nprobability distribution on Mn (C)), and let \u03bc\u221e be a probability measure on C. We give the space of probability measures on C the usual\nvague topology,\nthus a sequence\nR\nR of deterministic measures \u03bcn converges\nto \u03bc if C f d\u03bcn converges to C f d\u03bc for every test function (i.e. continuous and compactly supported function) f : C \u2192 R. Thus, by\nDefinition 1.2, we see that \u03bc \u221a1 An converge in probability to \u03bc\u221e if for\nn\nevery continuous and compactly supported function f : C \u2192 R, the\nexpression\nZ\nZ\nf (z) d\u03bc \u221a1 An (z) \u2212 f (z) d\u03bc\u221e\n(1)\nn\n\nC\n\nC\n\nconverges to zero in probability, thus\nZ\nZ\nlim P(| f (z) d\u03bc \u221a1 An (z) \u2212 f (z) d\u03bc\u221e | \u2265 \u03b5) = 0\nn\u2192\u221e\n\nC\n\nn\n\nC\n\nfor every \u03b5 > 0. Similarly, \u03bc \u221a1 An converges almost surely to \u03bc\u221e if with\nn\nprobability 1, the expression (1) converges to zero for all f : C \u2192 R.\nRemark 1.3. In practice, our matrices An will have bounded entries\non the average, which suggests (by the Weyl comparision inequality,\n\u221a\nsee Lemma A.2) that their eigenvalues should be of size about O( n);\nthus the normalization by \u221a1n is natural.\n1.4. Universality. A fundamental problem in the theory of random\nmatrices is to determine the limiting distribution of the ESD of a random matrix ensemble (either in probability or in the almost sure sense),\nas the size of the random matrix tends to infinity.\nThe situation with this problem, so far, is that the analysis depends\nvery much on which ensemble one is dealing with. In some cases such as\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n3\n\nwhen the entries have gaussian distribution, powerful group-theoretic\nstructure (e.g. invariance under the orthogonal group O(n) or unitary\ngroup U (n)) plays an essential role, as one can use it to derive an explicit formula for the joint distribution of the eigenvalues. The limiting\ndistribution can then be computed directly from this formula. In the\nmajority of cases, however, there is little symmetry, and such a formula\nis not available. Consequently, the problem becomes much harder and\nits analysis typically requires tools from various areas of mathematics.\nOn the other hand, there is a well-known intuition behind this problem (and many others concerning random matrices), the universality\nphenomenon, that asserts that the limiting distribution should not depend on the particular distribution of the entries. This phenomenon\nmotivates many theorems and conjectures in the area. In the following, we mention two famous examples, Wigner's semi-circle law and\nthe Circular Law conjecture.\nWigner's semi circle law. In the 1950's, motivated by numerical experiments, Wigner [28] proved that the ESD of an n \u00d7 n hermitian\nmatrix with (upper diagonal) entries being iid gaussian random variables converge to the semi-circle law F whose density is given by\n( \u221a\n1\n4 \u2212 x2 , |x| \u2264 2\n\u03c1(x) = 2\u03c0\n0,\n|x| > 2.\nWigner's result (which holds for both modes of convergence) was later\nextended to many other ensembles. The most general form only requires the mean and variance of the entries [16, 2]:\nTheorem 1.5. Let An be the n \u00d7 n hermitian random matrix whose\nupper diagonal entries are iid complex random variables with mean 0\nand variance 1. Then the ESD of \u221a1n An converges (both in probability\nand in the almost sure sense) to the semi-circle distribution.\nCircular Law Conjecture. The well-known Circular Law conjecture\ndeals with non-hermitian matrices.\nConjecture 1.6. Let An be the n \u00d7 n random matrix whose entries\nare iid complex random variables with mean 0 and variance 1. Then\nthe ESD of \u221a1n An converges (both in probability and in the almost sure\nsense) to the uniform distribution on the unit disk.\nSimilarly to Wigner's law, this conjecture was posed, based on numerical evidence, in the 1950's. The case when the entries have complex\n\n\f4\n\nTERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\ngaussian distribution was verified by Mehta [14] in 1967, using Ginibre's formula for the joint density function of the eigenvalues of An\n(see, for example, [2, Chapter 10]):\n\np(\u03bb1 , . . . , \u03bbn ) = cn\n\nY\ni<j\n\n2\n\n|\u03bbi \u2212 \u03bbj | exp(\u2212n\n\nn\nX\n\n|\u03bbi |2 ).\n\n(2)\n\ni=1\n\nAnother case where such a formula is available is when the entries\nhave real gaussian distribution, and for this case the conjecture was\nconfirmed by Edelman [6]. For the general case when there is no formula, the problem appears much harder. Important partial results\nwere obtained by Girko [7, 8], Bai [1, 2], and more recently G\u00f6tzeTikhomirov [9, 10], Pan-Zhou [15] and the authors [26]. These results\nestablish the conjecture (in almost sure or in probability forms) under\nadditional assumptions on the distribution x. The strongest result in\nthe previous literature is from [26, 10] in which the almost sure and\nin probability forms of the conjecture respectively were shown under\nthe extra assumption that the entries have finite (2 + \u000f)-th moment\nfor any positive constant \u000f. An attempt to remove this extra \u000f (and\nthus proving Conjecture 1.6 in full generality) was a motivation for this\npaper.\nA demonstration of the circular law for the Bernoulli and the Gaussian\ncase appears in Figure 1.\nIn both the semi-circular law and the circular law, we observe that\nonly the mean and variance of the entries play a role in the limiting distribution. This is a common situation, in fact, for many other\nconjectures in random matrix theory, such as Dyson's conjecture [14,\nChapter 1], and this phenomenon sometimes referred to as universality\nin the literature.\nIn this paper, we rigorously prove the universality phenomenon for the\nESD of random matrices. More precisely, we show that the limiting\ndistribution of the ESD of a random matrix ensemble An depends only\nthe mean and variance of its entries, under a mild size condition on the\nmean EAn , and under the assumption that the matrix An \u2212 EAn has\niid entries.\nFor any matrix A, we define the Hilbert-Schmidt norm kAk2 by the\nformula kAk := trace(AA\u2217 )1/2 = trace(A\u2217 A)1/2 .\nTheorem 1.7 (Universality principle). Let x and y be complex random\nvariables with zero mean and unit variance. Let Xn = (xij )1\u2264i,j\u2264n and\nYn := (yij )1\u2264i,j\u2264n be n\u00d7n random matrices whose entries xij , yij are iid\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\nBernoulli\n\n5\n\nGaussian\n\n%\"!\n\n%\"!\n\n!\"$\n\n!\"$\n\n!\"&\n\n!\"'\n\n!\"#\n\n!\"#\n\n!\"'\n\n!\"&\n\n!\"!\n\n!\"!\n\n!!\"'\n\n!!\"&\n\n!!\"#\n\n!!\"#\n\n!!\"&\n\n!!\"'\n\n!!\"$\n\n!!\"$\n\n!%\"!\n\n!%\"!\n!\"!\n\n!\"'\n\n!\"#\n\n!\"&\n\n!\"$\n\n%\"!\n\n%\"'\n\n%\"#\n\n%\"&\n\n%\"$\n\n'\"!\n\n!\"!\n\n!\"&\n\n!\"#\n\n!\"'\n\n!\"$\n\n%\"!\n\n%\"&\n\n%\"#\n\n%\"'\n\n%\"$\n\n&\"!\n\nFigure 1. Eigenvalue plots of two randomly generated\n5000 by 5000 matrices. On the left, each entry was an\niid Bernoulli random variable, taking the values +1 and\n\u22121 each with probability 1/2. On the right, each entry\nwas an iid Gaussian normal random variable, with prob1\nability density function is \u221a2\u2217\u03c0\nexp(\u2212x2 /2). (These two\ndistributions were shifted by adding the identity matrix,\nthus the circles are centered at (1, 0) rather than at the\norigin.)\ncopies of x and y, respectively. For each n, let Mn be a deterministic\nn \u00d7 n matrix satisfying\nsup\nn\n\n1\nkMn k22 < \u221e.\nn2\n\n(3)\n\nLet An := Mn +Xn and Bn := Mn +Yn . Then \u03bc \u221a1 An \u2212\u03bc \u221a1 Bn converges\nn\nn\nin probability to zero. If furthermore we make the additional hypothesis\nthat the ESDs\n\u03bc( \u221a1\n\nn\n\nMn \u2212zI)( \u221a1n Mn \u2212zI)\u2217\n\n(4)\n\nconverge to a limit for almost every z, then \u03bc \u221a1 An \u2212 \u03bc \u221a1 Bn converges\nn\nn\nalmost surely to zero.\nRemark 1.8. The theorem still holds if we restrict the size of the matrices to an infinite subsequence n1 < n2 < . . . of positive integers.\nThis freedom to pass to a subsequence is useful for technical reasons\ninvolving compactness arguments.\nThe condition (3) has the following useful consequence, which we shall\nuse repeatedly:\n\n\f6\n\nTERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nLemma 1.9 (Tightness of ESDs). Let M\nAn be as in Theorem\nR n and\n1\n2\n2\n1.7. Then the quantities n2 kAn k2 and C |z| d\u03bc \u221a1 An (z) are almost\nn\nsurely bounded (and hence also bounded in probability).\nProof. By the Weyl comparison inequality (Lemma A.2) it suffices to\nshow that n12 kAn k22 is almost surely bounded. By (3) and the triangle\ninequality it suffices to show that n12 kXn k22 is almost surely bounded.\nBut this follows from the finite second moment of x and the strong law\nof large numbers.\n\u0003\nAs an immediate corollary of Theorem 1.7, we have\nCorollary 1.10 (Universality principle). Let x, y be complex random\nvariables with zero mean and unit variance. Let Xn and Yn be n \u00d7 n\nrandom matrices whose entries are iid copies of x and y, respectively.\nFor each n, let Mn be a deterministic n \u00d7 n matrix satisfying (3).\nLet An := Mn + Xn and Bn := Mn + Yn . Then if \u03bc \u221a1 Bn converges\nn\nin probability to a limiting measure \u03bc, then \u03bc \u221a1 An also converges in\nn\nprobability to \u03bc. If furthermore we make the additional hypothesis that\nthe ESDs (4) converge to a limit for almost every z, then we can replace\n\"in probability\" by \"almost surely\" in the previous sentence.\nA demonstration of this corollary appears in Figure 2.\nRemark 1.11. One consequence of Corollary 1.10 (in the case when (4)\nconverges to a limit) is that the ESD \u03bc \u221a1 An behaves asymptotically\nn\n\ndeterministically1 in the sense that there exists a deterministic measure\n\u03bcn for each n such that \u03bc \u221a1 An \u2212 \u03bcn converges almost surely to zero.\nn\nIndeed, one can simply take \u03bcn to be an instance of \u03bc \u221a1 Bn , where the\nn\nBn are selected independently of the An , and the claim will hold almost\nsurely. The question remains as to whether \u03bcn itself converges to some\nlimit as n \u2192 \u221e; we partially address this issue in Theorem 1.23 below.\n1.12. The Circular Law Conjecture. Thanks to Corollary 1.10, we\ncan reduce the problem of computing the limiting distribution to the\ncase when the entries are gaussian2 (or having any special distribution\nsatisfying the variance bound). In particular, since the Circular Law is\nverified for random matrices with complex gaussian entries (see [14]), it\nfollows that this law (both in probability and in the almost sure sense)\nholds in full generality. In other words, we have shown\n1The\n\nauthors thank Oded Schramm for this observation.\nidea of establishing a limiting law by first replacing a general random\nvariable with a gaussian one is sometimes referred to as the \"Lindberg trick\" in the\nliterature.\n2The\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\nBernoulli\n\n7\n\nGaussian\n\n&\n\n&\n\n%#$\n\n%#$\n\n%\n\n%\n\n!%#$\n\n!%#$\n\n!&\n\n!&\n% %#\"$\n\n&\n\n&#'$ \"\n\n\"#$\n\n( (#\"$\n\n!\n\n%\n\n%\n\n!\"#\n\n!\"#\n\n!\n\n!\n\n!!\"#\n\n!!\"#\n\n!%\n\n% %#\"$\n\n&\n\n&#'$ \"\n\n! !\"()\n\n%\n\n%\"'$ (\"!)\n\n\"#$\n\n( (#\"$\n\n!\n\n!%\n! !\"()\n\n%\n\n%\"'$ (\"!)\n\n(\"* &\n\n&\"#$\n\n$\n\n(\"* &\n\n&\"#$\n\n$\n\nFigure 2. Eigenvalue plots of randomly generated n\nby n matrices of the form Dn + Mn , where n =\n5000. In left column, each entry of Mn was an iid\nBernoulli random variable, taking the values +1 and\n\u22121 each with probability 1/2, and in the right column, each entry was an iid Gaussian normal random variable, with probability density function is\n\u221a1 exp(\u2212x2 /2).\nIn the first row, Dn is the de2\u03c0\nterministic matrix diag(1, 1, . . . , 1, 2.5, 2.5, . . . , 2.5), and\nin the second row Dn is the deterministic matrix\ndiag(1, 1, . . . , 1, 2.8, 2.8, . . . , 2.8) (in each case, the first\nn/2 diagonal entries are 1's, and the remaining entries\nare 2.5 or 2.8 as specified).\nTheorem 1.13 (Circular Law). Let Xn be the n \u00d7 n random matrix\nwhose entries are iid complex random variables with mean 0 and variance 1. Then the ESD of \u221a1n Xn converges (both in probability and in\nthe almost sure sense) to the uniform distribution on the unit disk.\nRemark 1.14. In [26] (see also [10] for an alternate proof for the in\nprobability sense), this theorem was proven with the extra assumption\nthat the entries have finite (2 + \u03b5)-th moment for any fixed \u03b5 > 0;\nearlier related results are appear in [7, 8, 1, 2, 9].\nNotice that in Theorem 1.13, we set Mn to be the all zero matrix\n(for which the boundedness and convergence hypotheses are trivial).\nIn [12], explicit distributions were computed for the case when Mn is\nan arbitrary diagonal matrix and Xn has iid gaussian entries. The\nformula for the limiting distribution is somewhat technical, but its\nis easy to describe: it is exactly the set of z \u2208 C for which\nRsupport \u22122\n|z \u2212 x| d\u03bc(x) \u2265 1 where \u03bc is the limiting distribution of the ESD of\n\n\f8\n\nTERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nMn . (In the case Mn is all zero, \u03bc has all its mass at the origin, and so\nthe set of z is the unit disk.)\nThe proof of Theorem 1.7 actually shows that if Mn and Mn0 both\nobey (3) and have the property that the difference between the ESD\n(4) and the counterpart for Mn0 converges to zero for almost every z,\nthen Theorem 1.7 holds with An := Mn + Xn and Bn := Mn0 + Yn (see\nRemark B.3).\nThis has the following interesting consequence. Assume that Mn is\na matrix with low rank, say o(n). In this case, it is easy to see that\nthe ESD (4) concentrates at |z|2 , since the matrix involved here is a\nself-adjoint low rank perturbation of |z|2 I. Thus, we can replace Mn\nby the zero matrix and obtain\nCorollary 1.15. (Circular Law for shifted matrices) Let Xn be the\nn \u00d7 n random matrix whose entries are iid complex random variables\nwith mean 0 and variance 1 and Mn be a deterministic matrix with rank\no(n) and obeying (3). Let An := Mn + Xn . Then the ESD of \u221a1n An\nconverges (in either sense) to the uniform distribution on the unit disk.\nIn particular, it shows that Theorem 1.13 still holds if the entries have\n(the same) non-zero mean. This extends a result of Chafa\u0131\u0308 [5], which\nin addition assumed that the entries had finite fourth moment.\n1.16. Extensions. We can extend Theorem 1.7 in several ways. First,\nby conditioning, we can obtain a theorem for Mn being a random matrix.\nTheorem 1.17 (Universality from a random base matrix). Let x and\ny be complex random variables with zero mean and unit variance. Let\nXn = (xij )1\u2264i,j\u2264n and Yn = (yij )1\u2264i,j\u2264n be n\u00d7n random matrices whose\nentries are iid copies of x and y, respectively. For each n, let Mn be\na random n \u00d7 n matrix, independent of Xn or Yn , such that n12 kMn k22\nis bounded in probability (see Definition 1.2). Let An := Mn + Xn\nand Bn := Mn + Yn . Then \u03bc \u221a1 An \u2212 \u03bc \u221a1 Bn converges in probability to\nn\n\nn\n\nzero. If we furthermore assume that n12 kMn k22 is almost surely bounded,\nand (4) converges almost surely to some limit for almost every z, then\n\u03bc \u221a1 An \u2212 \u03bc \u221a1 Bn converges almost surely to zero.\nn\n\nn\n\nWe can also address a more general form of random matrices (cf.\n[8]). Let Kn , Ln be two sequences of matrices. Define An := Mn +\nKn Xn Ln and Bn := Mn + Kn Yn Ln . We can show that under some\nmild assumptions on Mn , Kn , Ln , Theorem 1.7 still holds:\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\nBernoulli\n\nGaussian\n\n*\n\n*\n\n)\n\n)\n\n#\n\n#\n\n!\n\n!\n\n!#\n\n!#\n\n!)\n\n!)\n\n!*\n!!\"% !\"#\n\n#\n\n)\n\n*\n\n&\n\n%\n\n9\n\n(\n\n$\n\n$\"'\n\n'\"%\n\n!*\n!!\"% !\"#\n\n#\n\n)\n\n*\n\n&\n\n%\n\n(\n\n$\n\n$\"'\n\n'\"%\n\nFigure 3. Eigenvalue plots of two randomly generated\n5000 by 5000 matrices of the form A + BMn B, where A\nandB are diagonal matrices having n/2 entries with the\nvalue 1 followed by n/2 entries with the value 5 (for D)\nand the value 2 (for X). On the left, each entry of Mn\nwas an iid Bernoulli random variable, taking the values\n+1 and \u22121 each with probability 1/2. On the right, each\nentry of Mn was an iid Gaussian normal random variable,\n1\nwith probability density function is \u221a2\u2217\u03c0\nexp(\u2212x2 /2).\nTheorem 1.18. Let x and y be complex random variables with zero\nmean and unit variance. Let Xn and Yn be n\u00d7n random matrices whose\nentries are iid copies of x and y, respectively. Let Mn , Kn , Ln be random\nn \u00d7 n matrices (independent of Xn , Yn ) and let An := Mn + Kn Xn Ln\nand Bn := Mn + Kn Yn Ln . Assume that the expressions\n1\n1\n1\n1\n2\nkAn k22 + 2 kBn k22 + 2 kKn\u22121 Mn L\u22121\nkK \u22121 L\u22121 k2\n(5)\nn k2 +\n2\nn\nn\nn\nn n n 2\nare bounded in probability. If furthermore we assume that (5) is almost\nsurely bounded, and that for almost every z the ESDs\n\u03bc( \u221a1\n\nn\n\n\u22121\n\u22121\n\u22121\n\u22121 \u22121\n\u22121 \u22121 \u2217\n1\nKn\nMn L\u22121\nn \u2212zKn Ln )( \u221an Kn Mn Ln \u2212zKn Ln )\n\n(6)\n\nconverge almost surely to a limit, then \u03bc \u221a1 An \u2212\u03bc \u221a1 Bn converges almost\nn\nn\nsurely to zero.\nNote that Theorem 1.17 is the special case of Theorem 1.18 in which\nKn = Ln = I. It seems of interest to see whether the hypotheses on\n(5) can be verified for various natural random or deterministic matrices\nMn , Kn , Ln , normalised appropriately by a suitable power of n. We do\nnot pursue this matter here.\nA demonstration of the above theorem for the Bernoulli and the\nGaussian case appears in Figure 3.\nThe proofs of these extensions are discussed in Section 7.\n\n\f10 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nAnother direction for generalization is to consider random matrices\nwhose entries are independent, but not necessarily identically distributed.\nMost of the tools used in this paper (e.g. law of large numbers, Talagrand's inequality, and the least singular value bound from [26]) extend\nwithout difficulty to this setting. Furthermore, Krishnapur pointed out\nthat one can also prove a \"universal\" version of Theorem B.1. This\nleads to a generalization in Appendix C (written by Krishnapur).\nFor similar reasons, one expects to be able to extend the above results to the case when Xn and Yn are sparse iid random matrices; for\ninstance, the least singular value bounds from [26] extend to this case,\nand the circular law for sparse iid matrices is already known in several\ncases [9], [26]. We, however, will not pursue these matters here.\n\n1.19. Computing the ESD of a random non-hermitian matrix\nvia the ESD of a hermitian one. Theorem 1.7 provides one useful way to compute the (limiting distribution of) ESD of a random\nnon-hermitian matrix, namely that one can restrict to any particular\ndistribution (such as complex gaussian) of the entries. The proof of\nthis theorem (with some modification) also provides another way to\ndeal with this problem, namely that one can reduce the problem of\ncomputing the ESD of \u221a1n An to that of ( \u221a1n An \u2212 zI)( \u221a1n An \u2212 zI)\u2217 , for\nfixed z \u2208 C. More precisely, we have the following equivalences.\nTheorem 1.20 (Equivalences for convergence). Let An be as in Theorem 1.7, and let R\u03bc be a probability measure on C with the second\nmoment condition |z|2 d\u03bc(z) < \u221e. Then the following are equivalent:\n(i) The ESD \u03bc \u221a1\n\nn\n\nAn\n\nof\n\n\u221a1 An\nn\n\nconverges in probability to \u03bc.\n\n(ii) For almost every complex number z, n1 log | det( \u221a1n An \u2212zI)| conR\nverges in probability to C log |w \u2212 z| d\u03bc(w).\n(iii) For almost every complex number z, there exists a sequence \u03b5n >\n0 of positive numbers converging to zero such that n1 log det((( \u221a1n An \u2212\nR\nzI)+\u03b5n I)( \u221a1n An \u2212zI)\u2217 +\u03b5n I) converges in probability to 2 C log |w\u2212\nz| d\u03bc(w).\nIf furthermore the ESDs (4) converge to a limit for almost every z, then\nwe can replace convergence in probability by almost sure convegence in\nthe above equivalences.\nWe prove this result in Section 8. As a corollary, we have a criterion\nfor when \u221a1n An converges to a distribution \u03bc:\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n11\n\nCorollary 1.21. Let An be as in Theorem 1.7, and let\na probability\nR \u03bc be\n2\nmeasure on C with the second moment condition |z| d\u03bc(z) < \u221e.\nSuppose that for almost every complex number z, the ESD of ( \u221a1n An \u2212\nzI)( \u221a1n An \u2212 zI)\u2217 converges in probability to a limiting distribution \u03b7z\nR\non [0, +\u221e) such\nthat\nthe\nintegral\nlog t d\u03b7z (t) is absolutely convergent\nC\nR\nand equal to 2 C log |w \u2212 z| d\u03bc(w). Then the ESD of \u221a1n An converges\nin probability to \u03bc. If the ESDs (4) converge to a limit for almost\nevery z, then we can replace convergence in probability by almost sure\nconvergence in the above implication.\nProof. We verify the claim for almost sure convergence only; the proof\nfor convergence in probability is similar and is left as an exercise to the\nreader.\nBy Lemma 1.9, we see that for fixed z, | n1 trace( \u221a1n An \u2212 zI)( \u221a1n An \u2212\nzI)\u2217 | is also almost surely bounded. Taking limits, we conclude that\nZ\nt d\u03b7z (t) < \u221e.\nC\n\nWe then see from the dominated convergence theorem that for any\n\u03b5 > 0, n1 log det((( \u221a1n An \u2212 zI) + \u03b5I)( \u221a1n An \u2212 zI)\u2217 + \u03b5I) converges almost\nR\nsurely to C log(t + \u03b5) d\u03b7z (t). From this we obtain hypothesis (iii) of\nTheorem 1.20 (if \u03b5n is chosen to decay to zero sufficiently slowly), and\nthe claim follows.\n\u0003\nSince the eigenvalues of ( \u221a1n An \u2212 zI)( \u221a1n An \u2212 zI)\u2217 are the squares\nof the singular values of \u221a1n An \u2212 zI, we can also say that Theorem\n1.20 reduces the problem of computing the limiting distribution of the\neigenvalues of \u221a1n An to that of the singular values of \u221a1n An \u2212 zI.\nThe big gain here is that the matrix ( \u221a1n An \u2212 zI)( \u221a1n An \u2212 zI)\u2217 is\nhermitian. (Random matrices of this type are often called sample covariance matrices in the literature.) This allows one to use standard\ntools such as truncation, Wigner's moment method and Stieljes transform (see, for instance, the proof of Theorem 1.5 in [2, Chapter 2]), or\nresults such as Theorem B.1; techniques from free probability are also\nvery powerful for such problems. These methods cannot be applied to\nnon-hermitian matrices for various reasons (see [2, Chapter 10] for a\ndiscussion) and their failure has been the main difficulty in attacking\nproblems such as the Circular Law conjecture.\nOne can use Corollary 1.21 to give another proof of Theorem 1.13,\nwithout relying on explicit formulas such as (2). We omit the details.\n\n\f12 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\n1.22. Existence of the limit. The results in the previous chapters\nprovide two different ways to compute (explicitly) the limiting measure\nof the ESD of random matrices. In fact there is a simple compactness\nargument that guarantees the existence of the limit, assuming of course\nthat the deterministic ESDs (4) already converge, although the argument does not provide too much information on what the limit actually\nis. More precisely, we have\nTheorem 1.23. Let x be a complex random variable with zero mean\nand unit variance. Let Xn be the n \u00d7 n random matrix whose entries\nare iid copies of x. For each n, let Mn be a deterministic n \u00d7 n matrix\nsatisfying\n1\nsup 2 kMn k22 < \u221e.\n(7)\nn n\nAssume furthermore that the ESD (4) converges for almost every z \u2208\nC. Then the ESD of \u221a1n An , where An := Mn + Xn , converges (in both\nsenses) to a limiting measure \u03bc.\nProof. We let f1 , f2 , f3 , . . . be an enumeration of a sequence of test\nfunctions which is dense in the uniform topology (such a sequence exists thanks to the Stone-Weierstrass theorem and the compact support of test functions). By applying the Bolzano-Weierstrass theorem\nonce for each function in this sequence and then using the Arzel\u00e1Ascoli\ndiagonalization argument, we can refine the subsequence so that\nR\nf (z) d\u03bc \u221a1 An (z) converges in probability to some limit for each j,\nC j\nn\nR\nand hence by a limiting argument C g(z) d\u03bc \u221a1 An (z) converges in probn\nability to a limit for each test function g. By the Riesz representation\nfunction we conclude that along this subsequence, \u03bc \u221a1 An converges in\nn\nprobability to some limit \u03bc, which is also a probability measure by the\ntightness bounds in Lemma 1.9.\nApplying Theorem 1.20, we conclude that for almost every z, the\nexpression\n1\n1\n1\nlog det((( \u221a An \u2212 zI) + \u03b5n I)(( \u221a An \u2212 zI)\u2217 + \u03b5n I))\n(8)\nn\nn\nn\nR\nconverges in probability to 2 C log |w \u2212 z| d\u03bc(w) along this sequence,\nfor some \u03b5n converging to zero. On the other hand, from the hypotheses\nand the theorem of Dozier and Silverstein (see Theorem B.1) we know\nthat for almost every z, the expression (8) has a almost sure limit for\nthe entire sequence of n. Combining the two facts we\nR see that for almost\nevery z, (8) in fact converges almost surely to 2 C log |w \u2212 z| d\u03bc(w)\nfor all n. The claim now follows from another application of Theorem\n1.20.\n\u0003\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n13\n\n1.24. Notation. The asymptotic notation is used under the assumption that n \u2192 \u221e, holding all other parameters fixed. Thus for instance,\nif we say that a quantity az,n depending on n and another parameter\nz is equal to o(1), this means that az,n converges to zero as n \u2192 \u221e\nfor fixed z, but this convergence need not be uniform in z. As another\nexample, the condition (3) is equivalent to asserting that kMn k = O(n)\nas n \u2192 \u221e.\n2. The replacement principle\nThe first step toward Theorem 1.7 is the following result that gives\na general criterion for two random matrix ensembles \u221a1n An , \u221a1n Bn to\nconverge to the same limit.\nTheorem 2.1 (Replacement principle). Suppose for each n that An , Bn \u2208\nMn (C) are ensembles of random matrices. Assume that\n(i) The expression\n1\n1\nkAn k22 + 2 kBn k22\n(9)\n2\nn\nn\nis bounded in probability (resp. almost surely).\n(ii) For almost all complex numbers z,\n1\n1\n1\n1\nlog | det( \u221a An \u2212 zI)| \u2212 log | det( \u221a Bn \u2212 zI)|\nn\nn\nn\nn\nconverges in probability (resp. almost surely) to zero. In particular, for each fixed z, these determinants are non-zero with\nprobability 1 \u2212 o(1) for all n (resp. almost surely non-zero for\nall but finitely many n).\nThen \u03bc \u221a1 An \u2212 \u03bc \u221a1 Bn converges in probability (resp. almost surely) to\nn\nn\nzero.\nWe would like to remark here that we do not need to require independence among the entries of An and Bn . The proof of this theorem\nis rather \"soft\" in nature, relying primarily on the Stieltjes transform\ntechnique (following Girko [7]) that analyses the ESD \u03bc \u221a1 An in terms of\nn\n\nthe log-determinants n1 log | det( \u221a1n An \u2212 zI)|, combined with tools from\nclassical real analysis such as the dominated convergence theorem (see\nLemma 3.1 for the precise version of this theorem that we need). The\ndetails are given in Section 3.\nIn view of Lemma 1.9, we see that Theorem 1.7 follows immediately\nfrom Theorem 2.1 and the following proposition.\n\n\f14 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nProposition 2.2 (Converging determinant). Let x and y be complex\nrandom variables with zero mean and unit variance. Let Xn and Yn be\nn \u00d7 n random matrices whose entries are iid copies of x and y, respectively. For each n, let Mn be a deterministic n \u00d7 n matrix satisfying\n(3). Set An := Mn + Xn and Bn := Mn + Yn . Then for every fixed\nz \u2208 C,\n1\n1\n1\n1\nlog | det( \u221a An \u2212 zI)| \u2212 log | det( \u221a Bn \u2212 zI)|\n(10)\nn\nn\nn\nn\nconverges in probability to zero. If furthermore we assume that (4)\nconverges to a limit for this value of z, then (10) converges almost\nsurely to zero.\nFor any square matrix A of size n, let \u03bbi (A) and si (A) be the eigenvalues and singular values of A. Furthermore, let di (A) be the distance\nfrom the ith row vector of A to the subspace formed by the first i \u2212 1\nrow vectors. From linear algebra, we have the fundamental identity\n\n| det A| =\n\nn\nY\ni=1\n\n|\u03bbi (A)| =\n\nn\nY\ni=1\n\nsi (A) =\n\nn\nY\n\ndi (A).\n\n(11)\n\ni=1\n\nWe will need to study the singular values and distances of \u221a1n An \u2212 zI\nand \u221a1n Bn \u2212 zI in order to estimate their determinants. The proof of\nProposition 2.2, which occupies Sections 4, 5 and 6, is the heart of the\npaper. This proof relies on the following three ingredients:\n\u2022 A result by Dozier and Silverstein [3] that compares the ESD of\nthe singular values of the matrices \u221a1n An \u2212 zI and \u221a1n Bn \u2212 zI.\nThis will let us handle all the rows from 1 to (1 \u2212 \u03b4)n for some\nsmall \u03b4 > 0.\n\u2022 A lower tail estimate for the distance between a random vector\nand a fixed subspace of relatively large co-dimension, using a\nconcentration inequality of Talagrand [13]. This will handle the\ncontribution of the rows between (1 \u2212 \u03b4)n and (say) n \u2212 n0.99 .\n\u2022 A polynomial lower bound for the least singular value of \u221a1n An \u2212\nzI and \u221a1n Bn \u2212zI from [26, 27]. This bound enables us to handle\nthe contribution of the last n0.99 rows.\n3. The replacement principle\nThe purpose of this section is to establish Theorem 2.1. We begin\nwith a version of the dominated convergence theorem.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n15\n\nLemma 3.1 (Dominated convergence). Let (X, \u03bd) be a finite measure\nspace. For each integer n \u2265 1, let fn : X \u2192 R be a random functions\nwhich are jointly measurable with respect to X and the underlying probability space. Assume that\nR\n(i) (Uniform integrability) There exists \u03b4 > 0 such that X |fn (x)|1+\u03b4 d\u03bd\nis bounded in probability (resp. almost surely).\n(ii) (Pointwise convergence in probability) For \u03bd-almost every x \u2208\nX, fn (x) converges in probability (resp. almost surely) to zero.\nThen\nzero.\n\nR\nX\n\nfn (x) d\u03bd(x) converges in probability (resp. almost surely) to\n\nProof. We first prove the claim for convergence in probability. We can\nnormalise \u03bd to be a probability measure. Let \u03b5 > 0 be arbitrary. It\nsuffices to show that\nZ\nfn (x) d\u03bd(x) = O(\u03b5)\nX\n\nwith probability 1 \u2212 O(\u03b5) \u2212 o(1).\nBy hypothesis (i), we already know that with probability 1 \u2212 O(\u03b5) \u2212\no(1), that\nZ\n|fn (x)|1+\u03b4 d\u03bd(x) \u2264 C\u03b5\nX\n\nfor some C\u03b5 depending on \u03b5. This implies that\nZ\nfn (x)I(|fn (x)| \u2265 M ) d\u03bd(x) \u2264 C\u03b5 /M \u03b4\nX\n\nfor any M > 0, where I(E) denotes the indicator of an event E. In\nparticular, for M large enough we have\nZ\nfn (x)I(|fn (x)| \u2265 M ) d\u03bd(x) \u2264 \u03b5,\nX\n\nwith probability 1 \u2212 O(\u03b5) \u2212 o(1), and so it will suffice to show that\nZ\nfn (x)I(|fn (x)| \u2264 M ) d\u03bd(x) = O(\u03b5)\n(12)\nX\n\nwith probability 1 \u2212 o(1).\nFix M . By hypothesis, we have limn\u2192\u221e P(|fn (x)| \u2265 \u03b5) = 0 for \u03bdalmost every x \u2208 X. By the dominated convergence theorem, we\nconclude that\nZ\nP(|fn (x)| \u2265 \u03b5) d\u03bd(x) = o(1).\nX\n\n\f16 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nBy Fubini's theorem, we conclude that\nZ\nE\nI(|fn (x)| \u2265 \u03b5) d\u03bd(x) = o(1)\nX\n\nand so by Markov's inequality, we have\nZ\nI(|fn (x)| \u2265 \u03b5) d\u03bd(x) = O(\u03b5/M )\nX\n\nwith probability 1 \u2212 o(1). The claim (12) easily follows.\nNow we prove the claim for almost sure convergence. Again we let\n\u03bd be a probability measure and \u03b5 > 0 be arbitrary. With probability\n1 \u2212 O(\u03b5) we have\nZ\n|fn (x)|1+\u03b4 d\u03bd(x) \u2264 C\u03b5\n\nX\n\nfor all sufficiently large n, and some C\u03b5 depending on n. Also, with\nprobability 1, fn (x) converges to zero for almost every x. The claim now\nfollows by invoking (the deterministic special case of) the convergence\nin probability version of the lemma that we have just proven.\n\u0003\nNow we begin the proof of Theorem 2.1. We thus assume that An , Bn\nare as in that theorem. We shall first prove the claim for convergence\nin probability, and indicate later how to modify the proof to obtain the\nprinciple for almost sure convergence.\nFrom the boundedness in probability of (9) and Weyl's comparison\ninequality (Lemma A.2) we see that for every \u03b5 > 0 there exists C\u03b5 > 0\nsuch that for each n, the eigenvalues \u03bb1 , . . . , \u03bbn of An obey the bound\nn\nX\n1\n|\u03bb |2 \u2264 C\u03b5\n(13)\n2 j\nn\nj=1\nor equivalently that\nZ\n\n|z|2 d\u03bc \u221a1\n\nC\n\nn\n\nAn (z)\n\n\u2264 C\u03b5\n\nwith probability 1 \u2212 O(\u03b5) \u2212 o(1). Similarly we have\nZ\n|z|2 d\u03bc \u221a1 Bn (z) \u2264 C\u03b5 .\nC\n\nn\n\nIn particular, for each n we see that with probability 1 \u2212 O(\u03b5) \u2212 o(1)\nwe have the tightness bounds\n\u03bc \u221a1\n\nAn {z\n\n\u2208 C : |z| \u2265 R} \u2264 C\u03b5 /R2\n\n(14)\n\n\u03bc \u221a1\n\nBn {z\n\n\u2208 C : |z| \u2265 R} \u2264 C\u03b5 /R2\n\n(15)\n\nn\n\nand\nn\n\nfor all R > 0.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n17\n\nWe now take the standard step of passing from the ESDs \u03bc \u221a1\n\nn\n\nAn , \u03bc \u221a1n Bn\n\nto the characteristic functions m \u221a1 An , m \u221a1 Bn : R2 \u2192 C, which are\nn\nn\ndefined by the formulae\nZ\nm \u221a1 An (u, v) :=\neiuRe(z)+ivIm(z) d\u03bc \u221a1 An (z)\nn\nn\nZC\neiuRe(z)+ivIm(z) d\u03bc \u221a1 Bn (z)\nm \u221a1 Bn (u, v) :=\nn\n\nn\n\nC\n\nthus the functions m \u221a1 An , m \u221a1 Bn are continuous and are bounded unin\nn\nformly in magnitude by 1.\nThanks to the tightness bounds (14)-(15), we can easily pass back and\nforth between convergence of ESDs and convergence of characteristic\nfunctions:\nLemma 3.2. Let the notation and assumptions be as above. Then the\nfollowing are equivalent:\n(i) \u03bc \u221a1 An \u2212 \u03bc \u221a1 Bn converges in probability.\nn\nn\n(ii) For almost every u, v, m \u221a1 An (u, v) \u2212 m \u221a1 Bn (u, v) converges in\nn\nn\nprobability.\nProof. We first show that (i) implies (ii). Fix u, v, and let \u03b5 > 0 be\narbitrary. From (14), (15) we can find an R depending on C\u03b5 and \u03b5\nsuch that\n\u03bc \u221a1\n\nn\n\nAn ({z\n\n\u2208 C : |z| \u2265 R}) + \u03bc \u221a1\n\nn\n\nBn ({z\n\n\u2208 C : |z| \u2265 R}) \u2264 \u03b5\n\nwith probability 1 \u2212 O(\u03b5) \u2212 o(1). In particular, with probability 1 \u2212\nO(\u03b5) \u2212 o(1) we have\nZ\nm \u221a1 Bn (u, v)\u2212m \u221a1 An (u, v) = \u03c8(z/R)eiuRe(z)+ivIm(z) [d\u03bc \u221a1 Bn (z)\u2212d\u03bc \u221a1\nn\n\nn\n\nn\n\nwhere \u03c8 is any smooth compactly supported function that equals one\non the unit ball. But since \u03bc \u221a1 Bn \u2212 \u03bc \u221a1 An converges in probability,\nn\nn\nthe integral here converges to zero in probability. The claim follows.\nNow we prove that (ii) implies (i). Since continuous compactly supported functions are the uniform limit\nof smoothR compactly supported\nR\nfunctions, it suffices to show that C f d\u03bc \u221a1 An \u2212 C f d\u03bc \u221a1 Bn converges\nn\nn\nin probability to zero for every smooth compactly supported function\nf : C \u2192 C.\n\nn\n\nAn (u, v)(z)]+O(\u03b5)\n\n\f18 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nNow fix a smooth compactly supported function f : C \u2192 C. By\nFourier analysis, we can write\nZ\nZ\nZ Z\nf d\u03bc \u221a1 An \u2212 f d\u03bc \u221a1 Bn =\nf\u02c6(u, v)(m \u221a1 An (u, v)\u2212m \u221a1 Bn (u, v)) dudv\nn\n\nC\n\nC\n\nn\n\nR\n\nR\n\nn\n\nn\n\n(16)\n\u02c6\nfor some smooth, rapidly decreasing function f . In particular, the measure d\u03bd = f\u02c6(u, v) dudv is finite. The claim now follows from dominated\nconvergence (Lemma 3.1); note that the function m \u221a1 An \u2212 m \u221a1 Bn is\nn\nn\nbounded and so clearly obeys the moment condition required in that\nlemma.\n\u0003\nIn view of the above lemma, it suffices to show that m \u221a1 An (u, v) \u2212\nn\nm \u221a1 Bn (u, v) converges in probability to zero for almost every u, v \u2208 R.\nn\n\nFix u, v. Since we can exclude a set of measure zero, we can assume\nthat u, v are non-zero. We allow all implied constants in the arguments\nbelow to depend on u, v.\nFollowing Girko [7], we now proceed via the Stieltjes-like transform\ng \u221a1 An : C \u2192 R, defined almost everywhere by the formula\nn\nZ\nz\u2212w\ng \u221a1 An (z) := 2Re\nd\u03bc \u221a1 (w)\n2\nn\nn\nC |z \u2212 w|\n1\n(17)\nn\n2 X z \u2212 \u221a n \u03bbj\n= Re\n;\nn\n|z \u2212 \u221a1n \u03bbj |2\nj=1\nobserve that this is a locally integrable function on C, and that\n\u2202\n1\n2\ng \u221a1 An (z) =\nlog | det( \u221a An \u2212 zI)|\nn\n\u2202Re(z) n\nn\nfor all but finitely many z.\n\n(18)\n\nWe have the following fundamental identity:\nLemma 3.3 (Girko's identity). [7] For every non-zero u, v we have\nZ Z\nu2 + v 2\nm \u221a1 An (u, v) =\n( g \u221a1 An (s + it)eius+ivt dt)ds,\nn\n4\u03c0iu R R n\nwhere the inner integral is absolutely integrable for almost every s, and\nthe outer integral is absolutely convergent.\nProof. We argue as in [2, Lemma 3.1]. Since\nn\n1 X i(uRe( \u221a1n \u03bbj )+vIm( \u221a1n \u03bbj ))\nm \u221a1 An (u, v) =\ne\nn\nn j=1\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n19\n\nit suffices from (17) to show that\nZ Z\nu2 + v 2\nRe(s + it \u2212 w) ius+ivt\ni(uRe(w)+vIm(w))\ne\n=\n(\ne\ndt)ds\n2\u03c0iu R R |s + it \u2212 w|2\nfor each complex number w, with an absolutely convergent inner integral and outer integral. But standard contour integration shows that\nZ\nRe(s + it \u2212 w) ius+ivt\ne\ndt = \u03c0 sgn(s \u2212 Re(w))e\u2212v|s\u2212Re(w)| eius eivIm(w)\n2\n|s\n+\nit\n\u2212\nw|\nR\n(19)\nfor every s 6= Re(w), and the claim follows by an elementary integration.\n\u0003\nWe can of course define g \u221a1 Bn similarly, with analogous identities. To\nn\nconclude the proof of Theorem 2.1, it thus suffices to show that for any\n\u03b5 > 0 and any n, we have\nZ Z\n( (g \u221a1 An (s + it) \u2212 g \u221a1 Bn (s + it))eius+ivt dt)ds = O(\u03b5)\n(20)\nR\n\nR\n\nn\n\nn\n\nwith probability 1 \u2212 O(\u03b5) \u2212 o(1).\nFix \u03b5 > 0. By (14), (15), we can find an R > 1 large enough that with\nprobability 1 \u2212 O(\u03b5),\n\u03bc \u221a1\n\nn\n\nAn ({z\n\n\u2208 C : |z| \u2265 R}) + \u03bc \u221a1\n\nn\n\nBn ({z\n\n\u2208 C : |z| \u2265 R}) \u2264 \u03b5.\n\n(21)\n\nWe now condition on the event that (21) holds.\nWe now smoothly localize the z variable to a compact set as follows.\nLet \u03c8 : R \u2192 R+ be a smooth cutoff function which equals 1 on [\u22121, 1]\nand is supported on [\u22122, 2].\nLemma 3.4 (Truncation in s, t). Let w \u2208 C.\n(i) The integral\nZ Z\nRe(w \u2212 (s + it)) ius+ivt\n|\ne\ndt|(1 \u2212 \u03c8(s/R2 )) ds\n2\nR\nR |w \u2212 (s + it)|\nis of size O(1), and (if R is large enough) is of size O(\u03b5) when\n|w| \u2264 R.\n(ii) The integral\nZ Z\nRe(w \u2212 (s + it)) ius+ivt\n|\ne\n(1 \u2212 \u03c8(t/R2 )) dt|\u03c8(s/R2 ) ds (22)\n2\n|w\n\u2212\n(s\n+\nit)|\nR\nR\nis of size O(1), and (if R is large enough) is of size O(\u03b5) when\n|w| \u2264 R.\n\n\f20 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nProof. The claim (i) follows easily from (19), so we turn to (ii). We\nfirst verify the claim that (22) is bounded. Replacing everything by\nabsolute values one sees that\nZ\nRe(w \u2212 (s + it)) ius+ivt\n|\ne\n(1 \u2212 \u03c8(t/R2 )) dt| = O(1)\n2\nR |w \u2212 (s + it)|\n(in fact one can obtain an explicit upper bound of \u03c0), so we can dispose of the region of integration in which s = Re(w) + O(1). For the\nremaining values of s, we use repeated integration by parts, integrating\nthe eivt term and differentiating the others. After two such integrations\nwe obtain the bound\nZ\nRe(w \u2212 (s + it)) ius+ivt\n|\ne\n(1\u2212\u03c8(t/R2 )) dt| = O((R\u22122 +|s\u2212Re(w)|\u22121 )2 ).\n2\n|w\n\u2212\n(s\n+\nit)|\nR\nThe claim then follows.\nFinally, if |w| \u2264 R, then one easily verifies (by repeated integration\nby parts) that\nZ\nRe(w \u2212 (s + it)) ius+ivt\ne\n(1 \u2212 \u03c8(t/R2 )) dt = O(1/R4 )\n2\n|w\n\u2212\n(s\n+\nit)|\nR\n(say), and so the final claim of (ii) follows.\n\n\u0003\n\nFrom this lemma and (17), the triangle inequality and (21) we conclude that\nZ Z\n( g \u221a1 An (s + it)eius+ivt dt)(1 \u2212 \u03c8(s/R2 ))ds = O(\u03b5).\n(23)\nR\n\nR\n\nand\nZ Z\n( g \u221a1\nR\n\nR\n\nn\n\nn\n\nAn (s\n\n+ it)eius+ivt (1 \u2212 \u03c8(t/R2 )) dt)\u03c8(s/R2 )ds = O(\u03b5). (24)\n\nFrom (23), (24) (and their counterparts for g \u221a1 Bn ) and the triangle\nn\ninequality, we thus see that to prove (20), it suffices to show that\nZ Z\n(g \u221a1 An (s + it) \u2212 g \u221a1 Bn (s + it))eius+ivt \u03c8(t/R2 )\u03c8(s/R2 ) dtds (25)\nR\n\nR\n\nn\n\nn\n\nconverges in probability to zero for every fixed R \u2265 1. Note that the\nintegrands here are now jointly absolutely integrable in t, s, and so we\nmay now freely interchange the order of integration.\nFix R. Using (18) and integration by parts in the s variable, we can\nrewrite (25) in the form\nZ Z\nfn (s, t)\u03c6u,v,R (s, t) dsdt\nR\n\nR\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n21\n\nwhere\nfn (s, t) :=\n\n1\n1\n1\n1\nlog | det( \u221a An \u2212 zI)| \u2212 log | det( \u221a Bn \u2212 zI)|\nn\nn\nn\nn\n\nand\n\n\u2202 ius+ivt\n(e\n\u03c8(t/R2 )\u03c8(s/R2 )).\n\u2202s\n(Note that there are finitely many values of t for which the integration\nby parts is not justified due to singularities in g \u221a1 An or g \u221a1 Bn , but\nn\nn\nthese values of t clearly give a zero contribution at the end of the day.)\nThus it will suffice to show that\nZ Z\n|fn (s, t)||\u03c6u,v,R (s, t)| dsdt\n\u03c6u,v,R (s, t) := \u2212\n\nR\n\nR\n\nconverges in probability to zero.\nFrom (11) we have\nn\n\n1\n1X\n1\n1\nlog | det( \u221a An \u2212 zI)| =\nlog | \u221a \u03bbj \u2212 (s + it)|\nn\nn j=1\nn\nn\n\n(26)\n\nand similarly for Bn . From the boundedness and compact support of\n\u03c6u,v,R we observe that\nZ Z\n1\n1\nlog | \u221a \u03bb \u2212 (s + it)|2 |\u03c6u,v,R (s, t)| dsdt \u2264 O\u03c6u,v,R (1 + |\u03bb|2 )\nn\nn\nR R\nfor all \u03bb \u2208 C; from this, (26), (13), and the triangle inequality we see\nthat\nZ Z\n|fn (s, t)|2 |\u03c6u,v,R (s, t)| dsdt\n(27)\nR\n\nR\n\nis bounded uniformly in n. Since by hypothesis fn (s, t) converges in\nprobability to zero for almost every s, t, the claim now follows from\ndominated convergence (Lemma 3.1). The proof of Theorem 2.1 is\nnow complete in the case of convergence in probability.\n3.5. The almost sure convergence case. We now indicate how to\nadapt the above arguments to the case of almost sure convergence.\nFirstly, since (9) is now almost surely bounded instead of just bounded\nin probability, we can now say that for every \u03b5 > 0 there exists C\u03b5 > 0\nsuch that with probability 1 \u2212 O(\u03b5), (14), (15) holds for all sufficiently\nlarge n (as opposed to these bounds holding with probability 1\u2212O(\u03b5)\u2212\no(1) for each n separately).\nNext, we observe the (well-known) fact that Lemma 3.2 continues\nto hold when convergence in probability is replaced by almost sure\nconvergence throughout. Indeed the implication of (ii) from (i) is nearly\nidentical and is left as an exercise to the reader. To deduce (i) from\n\n\f22 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\n(ii) in the almost sure case, observe from the separability of the space\nof smooth compactly supported functions in the uniform topology that\nit suffices to show that (16) converges almost surely to zero for each\nf . On the other hand, from (ii) and Fubini's theorem we know that\nwith probability 1, that m \u221a1 An (u, v) \u2212 m(u, v) converges to zero for\nn\nalmost every u, v, and the claim follows from the (ordinary) dominated\nconvergence theorem.\nOnce again we use Girko's identity, Lemma 3.3, and reduce to showing\nthat for every \u03b5 > 0, one has with probability 1 \u2212 O(\u03b5) that (20) holds\nfor all but finitely many n. From our bounds on (14), (15) we see that\nwith probability 1 \u2212 O(\u03b5), that (21) holds for all but finitely many n.\nWe apply Lemma 3.4 (which is deterministic) and reduce to showing\nthat (25) converges almost surely to zero for each fixed R \u2265 1. The\nrest of the argument proceeds as in the convergence in probability case.\n3.6. An alternate argument. There is an alternate derivation3 of\nTheorem 2.1 that avoids Fourier analysis, and is instead based on the\nobservation that for any complex polynomial P (z), the distributional\nLaplacian \u2206 log |P (z)| of the logarithm of the magnitude of P is equal\nto the counting measure of the zeroes of P (counting multiplicity). In\nparticular, we see from Green's theorem that\nZ\nZ\n1\n1\n1\n1\nf d(\u03bc \u221a1 An \u2212\u03bc \u221a1 Bn ) =\n(\u2206f (z)) log | det( \u221a An \u2212zI)|\u2212 log | det( \u221a Bn \u2212zI)|\nn\nn\n2\u03c0n C\nn\nn\nn\nC\nfor any smooth, compactly supported f . Applying Lemma 3.1 we can\nthen get convergence of this integral (either in probability or in the\nalmost sure sense, as appropriate); the uniform integrability required\ncan be established by repeating the computations used to bound (27).\nOne can then easily take limits to replace smooth compactly supported\nf to continuous compactly supported f ; we omit the details.\n4. Proof of Proposition 2.2\nIn this section we present the proof of Proposition 2.2, modulo several\nkey lemmas. Let\n\u221a x, y, Mn , An , Bn , z be as in that proposition. By\nshifting Mn by nzI if necessary we can assume z = 0. Our task is\nnow to show that\n1\n1\n1\n1\nlog | det( \u221a An )| \u2212 log | det( \u221a Bn )|\nn\nn\nn\nn\nconverges in probability to zero, and also almost surely to zero if\n\u03bc 1 Mn Mn\u2217 converges.\nn\n\n3We\n\nthank Manjunath Krishnapur for this simpler argument.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n23\n\nLet us first remark that the almost sure convergence claim implies the\nconvergence in probability claim. Indeed, suppose that convergence in\nprobability failed, then there would exist an \u03b5 > 0 such that\n\u0012\n\u0013\n1\n1\n1\n1\nP\nlog | det( \u221a An )| \u2212 log | det( \u221a Bn )| \u2265 \u03b5 \u2265 \u03b5\n(28)\nn\nn\nn\nn\nfor a subsequence of n. By vague sequential compactness one can pass\nto a further subsequence along which \u03bc 1 Mn Mn\u2217 converges, and hence by\nn\nhypothesis one has almost sure (and hence in probability) convergence\nto zero along this sequence, contradicting (28). Thus it suffices to\nestablish almost sure convergence assuming the convergence of \u03bc 1 Mn Mn\u2217 .\nn\n\nLet Z1 , . . . , Zn be the rows of Mn . By assumption (3) we have\nn\nX\n\nkZi k2 = O(n2 ).\n\nj=1\n\n\u221a\nIn particular, at least half of the Zi have norm O( n). By permuting\nthe rows of Mn , An , Bn if necessary, we may assume that it the last half\nof the rows have this property, thus\n\u221a\nkZi k = O( n) for all n/2 \u2264 i \u2264 n.\n(29)\nLet \u03c31 (A) \u2265 . . . \u2265 \u03c3n (A) \u2265 0 denote the singular values of a matrix\nA. We have the following fundamental lower bound:\nLemma 4.1 (Least singular value bound). With probability 1, we have\n\u03c3n (An ), \u03c3n (Bn ) \u2265 n\u2212O(1)\n\n(30)\n\nfor all but finitely many n. In particular, with probability 1, An and\nBn are invertible for all but finitely many n.\nProof. This follows immediately from [26, Theorem 2.1] or [27, Theorem 4.1] and the Borel-Cantelli lemma, noting from (3) of Proposition\n2.2 that the operator norm of Mn is of polynomial size nO(1) . There\nare previous results in [17], [24], [18], [25], which handled special cases\nwith more assumptions on Mn and the underlying distributions x, y\n(for instance, in some of the prior results Mn was assumed to vanish,\nor x, y were assumed to be integer-valued or to have finite higher moments). One can obtain explicit bounds on the tail probability and on\nthe exponent O(1); see [27]. However, for our applications the above\nbounds will suffice.\n\u0003\nWe also have with probability 1 the crude upper bound\n\u03c31 (An ), \u03c31 (Bn ) \u2264 nO(1)\n\n(31)\n\n\f24 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nfor all but finitely many n, which follows easily from the polynomial\nsize of Mn the bounded second moment of x, y, and the Borel-Cantelli\nlemma. Again, much sharper bounds are available, especially if x and\ny have finite fourth moment, but we will not need these bounds here.\nLet X1 , . . . , Xn be the rows of An , and for each 1 \u2264 i \u2264 n let Vi be\nthe i \u2212 1-dimensional space generated by X1 , . . . , Xi\u22121 . From (11) we\nhave\nn\n1\n1\n1\n1X\nlog | det( \u221a An )| =\nlog dist( \u221a Xi , Vi )\nn\nn i=1\nn\nn\nand similarly\nn\n\n1\n1\n1X\n1\nlog | det( \u221a Bn )| =\nlog dist( \u221a Yi , Wi )\nn\nn i=1\nn\nn\nwhere Y1 , . . . , Yn are the rows of \u221a1n Bn , and Wi is spanned by Y1 , . . . , Yi\u22121 .\nOur task is then to show that\nn\n1X\n1\n1\nlog dist( \u221a Xi , Vi ) \u2212 log dist( \u221a Yi , Wi )\nn i=1\nn\nn\nconverges almost surely to zero.\nFrom (30), (31) and Lemma A.4 we almost surely obtain the bound\n1\n1\nlog dist( \u221a Xi , Vi ), log dist( \u221a Yi , Wi ) = O(log n)\nn\nn\nfor all but finitely many n. Thus it suffices to show that\nX\n1\n1\n1\nlog dist( \u221a Xi , Vi ) \u2212 log dist( \u221a Yi , Wi )\nn\nn\nn\n0.99\n1\u2264i\u2264n\u2212n\n\n(say) converges almost surely to zero. This follows immediately from\nthe following two lemmas.\nLemma 4.2 (High-dimensional contribution). For every \u03b5 > 0 there\nexists 0 < \u03b4 < 1/2 such that with probability 1, one has\nX\n1\n1\n| log dist( \u221a Xi , Vi )| = O(\u03b5)\nn\nn\n0.99\n(1\u2212\u03b4)n\u2264i\u2264n\u2212n\n\nfor all but finitely many n. Similarly with dist( \u221a1n Xi , Vi ) replaced by\ndist( \u221a1n Yi , Wi ).\nLemma 4.3 (Low-dimensional contribution). For every \u03b5 > 0 there\nexists 0 < \u03b4 < 1/2, such that with probability 1 \u2212 O(\u03b5), one has\nX\n1\n1\n1\nlog dist( \u221a Xi , Vi ) \u2212 log dist( \u221a Yi , Wi ) = O(\u03b5)\nn\nn\nn\n1\u2264i\u2264(1\u2212\u03b4)n\n\nfor all but finitely many n.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n25\n\nThe next two sections will be devoted to the proofs of these two lemmas.\n5. Proof of Lemma 4.2\nWe now prove Lemma 4.2. We can of course take n to be large depending on all fixed parameters. Let 0 < \u03b4 < 1/2 be a small number\ndepending on \u03b5 to be chosen later.\nClearly it suffices to prove this lemma for dist( \u221a1n Xi , Vi ). We first\nprove the (much easier) bound for the positive component of the logarithm. By the Borel-Cantelli lemma it suffices to show that\n\u221e\nX\n\nP(\n\nn=1\n\n1\nn\n\n1\nmax(log dist( \u221a Xi , Vi ), 0) \u2265 \u03b5) < \u221e.\nn\n0.99\n\nX\n\n(1\u2212\u03b4)n\u2264i\u2264n\u2212n\n\nTo establish this, we use the crude bound\n1\n1\nmax(log dist( \u221a Xi , Vi ), 0) \u2264 max(log \u221a kXi k, 0)\nn\nn\nand thus\nX\n1\nn\n\n\u221e\nX\n1\n1\n\u221a\nmax(log dist(\nXi , Vi ), 0) \u2264 O(\nn\nn\n0.99\nm=0\n\n(1\u2212\u03b4)n\u2264i\u2264n\u2212n\n\n\u221a\nI(kXi k \u2265 2m n)).\n\nX\n(1\u2212\u03b4)n\u2264i\u2264n\u2212n0.99\n\n(32)\nThus if the left-hand side of (32) exceeds \u03b5, we must have\nX\n\u221a\n1\nI(kXi k \u2265 2m n) \u2265 \u03b5/(100 + m)2\nn\n0.99\n(1\u2212\u03b4)n\u2264i\u2264n\u2212n\n\n(say) for some m \u2265 0. On the other hand, \u221afrom (29) and the second\nmoment method we see that P(kXi k \u2265 2m n) = O(2\u22122m ), and thus\nby Hoeffding's inequality we have\nX\n\u221a\n1\nP(\nI(kXi k \u2265 2m n) \u2265 \u03b5/(100+m)2 ) \u2264 C exp(\u2212cn\u22120.01 \u2212cm\u22120.01 )\nn\n0.99\n(1\u2212\u03b4)n\u2264i\u2264n\u2212n\n\n(say) for some constants C, c > 0 depending on \u03b5, if \u03b4 is chosen sufficiently small depending on \u03b5. The claim follows.\nIt remains to establish the bound for the negative component of the\nlogarithm. By the Borel-Cantelli lemma it suffices to show that\n\u221e\nX\nn=1\n\nP(\n\n1\nn\n\nX\n\n1\nmax(\u2212 log dist( \u221a Xi , Vi ), 0) \u2265 \u03b5) < \u221e.\nn\n0.99\n\n(1\u2212\u03b4)n\u2264i\u2264n\u2212n\n\nThis will follow from the union bound and the following estimate.\n\n\f26 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nProposition 5.1 (Lower tail bound). Let 1 \u2264 d \u2264 n \u2212 n0.99 and\n0 < c < 1, and let W be a (deterministic) d-dimensional subspace of\nCn . Let X be a row of An (the exact choice of row is not important).\nThen\n\u221a\nP(dist(X, W ) \u2264 c n \u2212 d) = O(exp(\u2212n0.01 )).\n(The implied constant of course depends on c.)\nIndeed, since Xi and Vi are independent of each other, the proposition\nimplies that\n1\n1 \u221a\nn\u2212i+1\ndist( \u221a Xi , Vi ) \u2265 \u221a\nn\n2 n\n(say) for each (1 \u2212 \u03b4)n \u2264 i \u2264 n \u2212 n0.99 , with probability 1 \u2212 O(n\u221210 )\n(say). Setting \u03b4 sufficiently small (compared to \u000f), taking logarithms\nand summing in i and n one obtains the claim.\nIt remains to prove the proposition. Similar lower bounds concerning\nthe distance of a random vector to a fixed subspace have appeared in\n[22], [18], [19]. Here, however, we have the complication that the coefficients of X have non-zero mean and have no higher moment bounds\nthan the second moment; in particular, they can be unbounded.\nWe first eliminate the problem that X has non-zero mean. Write\nX = v + X 0 , where v := E(X) is a deterministic vector (which could\nbe quite large) and X 0 has mean zero. Then we have dist(X, W ) \u2265\ndist(X 0 , span(W, v)). Thus Proposition 5.1 follows from the mean zero\ncase (after making the harmless change of incrementing d to d + 1, and\nadjusting the parameters slightly to suit this).\nHenceforth we assume that X has mean zero, thus X = (x1 , . . . , xn )\nfor some iid copies x1 , . . . , xn of x. Now we deal with the problem that\nthe x1 , . . . , xn can be unbounded. By Chebyshev's inequality, we have\nP(|xi | \u2265 n0.1 ) = O(n\u22120.2 ) for all 1 \u2264 i \u2264 n. The event |xi | \u2265 n0.1\nare jointly independent in i. By Chernoff inequality (see, for instance,\n[23, Chapter 1]), we can show that with probability 1\u2212O(exp(\u2212n0.01 )),\nthat there are at most n0.9 indices i for which |xi | \u2265 n0.1 . (One can also\nverify this directly using binomial coefficients and Sterling's formula.)\nBy conditioning on the various possible sets of indices for which |xi | \u2265\nn0.1 , we see that it suffices to show that\n\u221a\nP(dist(X, W ) \u2264 c n \u2212 d|EI ) = O(exp(\u2212n0.01 ))\nfor each I \u2282 {1, . . . , n} of cardinality at most n0.9 , where EI is the\nevent that I = {1 \u2264 i \u2264 n : |xi | \u2265 n0.1 }.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n27\n\nWithout loss of generality we can take I = {n0 + 1, . . . , n} for some\nn \u2212 n0.9 \u2264 n0 \u2264 n. We then observe that\ndist(X, W ) \u2265 dist(\u03c0(X), \u03c0(W ))\n0\n\nwhere \u03c0 : Cn \u2192 Cn is the orthogonal projection. By conditioning on\nthe coordinates xn0 +1 , . . . , xn and making the minor change of replacing\nn with n0 (and adjusting c slightly), we may thus reduce to the case\nwhen I is empty, thus it suffices to show that\n\u221a\nP(dist(X, W ) \u2264 c n \u2212 d||xi | < n0.1 for all i) = O(exp(\u2212n0.01 )).\nLet x\u0303 be the random variable x conditioned to the event |x| < n0.1 , and\nlet X\u0303 = (x\u03031 , . . . , x\u0303n ) be a vector consisting of iid copies of x\u0303. It then\nsuffices to show that\n\u221a\nP(dist(X\u0303, W ) \u2264 c n \u2212 d) = O(exp(\u2212n0.01 )).\n(33)\nNote that x\u0303 might have a non-zero mean, but this can be easily dealt\nwith by the same trick used before, subtracting Ex\u0303 from x\u0303 to make\nX to have zero mean. Since x had variance 1, we see from monotone\nconvergence that x\u0303 has variance 1 \u2212 o(1).\nTo prove (33), we recall the following inequality of Talagrand.\nTheorem 5.2 (Talagrand's inequality). Let D be the unit disk {z \u2208\nC, |z| \u2264 1}. For every product probability \u03bc on Dn , every convex 1Lipschitz function F : Cn \u2192 R, and every r \u2265 0,\n\u03bc(|F \u2212 M (F )| \u2265 r) \u2264 4 exp(\u2212r2 /8),\nwhere M (F ) denotes the median of F .\nProof. This is the complex version of [13, Corollary 4.10], in which D\nwas replaced by the unit interval [0, 1]. The proof is the same, with\na slight modification that implies a worse the constant (1/8 instead of\n1/4) in the exponent.\n\u0003\nWe apply this theorem with \u03bc equal to the distribution of X\u0303/n0.1\nand F : Cn \u2192 R equal to the convex 1-Lipschitz function F (v) :=\ndist(v, W ), and conclude that\nP(| dist(X\u0303, W ) \u2212 M (dist(X\u0303, W ))| \u2265 n0.1 r) \u2264 4 exp(\u2212r2 /8)\n\n(34)\n\nfor every r > 0. On the other hand, we can easily compute the second\nmoment (cf. [22, Lemma 2.5]):\nLemma 5.3. We have\nE(dist(X\u0303, W )2 ) = (1 \u2212 o(1))(n \u2212 d).\n\n\f28 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nProof. Let \u03c0 = (\u03c0ij )1\u2264i,j\u2264n be the orthogonal projection matrix to W .\nP P\nObserve that dist(X\u0303, W )2 = ni=1 nj=1 x\u0303i \u03c0ij x\u0303j . Since the x\u0303i are iid\nwith mean zero, we thus have\n2\n\n2\n\nE(dist(X\u0303, W ) ) = (Ex\u0303 )\n\nn\nX\n\n\u03c0ii .\n\ni=1\n\nP\nBut ni=1 \u03c0ii = trace(\u03c0) is equal to \u00f1. Since x\u0303 had variance 1 \u2212 o(1),\nthe claim follows.\n\u0003\nSince n \u2212 d \u2265 n0.99 and c < 1, the claim (33) from follows from (34)\nand the above lemma. The proof of Lemma 4.2 is now complete.\n\n6. Proof of Lemma 4.3\nWe now begin the proof of Lemma 4.3. Fix \u03b5, and assume that \u03b4 is\nsufficiently small depending on \u03b5. Write n0 := b(1 \u2212 \u03b4)nc. Observe that\nQn 0\n0\n\u221a1\ni=1 dist( n Xi , Vi ) is the n -dimensional volume of the parallelepiped\nspanned by X1 , . . . , Xn0 , which is also equal to det( n1 An,n0 A\u2217n,n0 )1/2 ,\nwhere An,n0 is the n0 \u00d7 n matrix with rows X1 , . . . , Xn0 . Expressing\nthis determinant as the product of singular values, we conclude the\nidentity\n1\nn\n\nn0\n\nX\n1\u2264i\u2264(1\u2212\u03b4)n\n\n1\n1X\nlog dist( \u221a Xi , Vi ) =\nlog\nn i=1\nn\n\n\u0012\n\n\u0013\n1\n\u221a \u03c3i (An,n0 ) .\nn\n\nSimilarly for Yi , Wi , and Bn,n0 (the matrix generated by Y1 , . . . , Yn0 .\nThus it suffices to show that with probability 1 \u2212 O(\u03b5), one has\n\u0012\n\u0013\n\u0012\n\u0013\nn0\n1 X\n1\n1\nlog \u221a \u03c3i (An,n0 ) \u2212 log \u221a \u03c3i (Bn,n0 ) = O(\u03b5)\nn0 i=1\nn\nn\nfor all but finitely many n. We rewrite (35) as\nZ \u221e\nlog t d\u03bdn,n0 (t) = O(\u03b5)\n\n(36)\n\n0\n\nwhere d\u03bdn,n0 is the difference of two ESDs:\nd\u03bdn,n0 = \u03bc 10 An,n0 A\u2217\nn\n\nn,n0\n\n\u2212 \u03bc 10 Bn,n0 B \u2217 0 .\nn\n\n(35)\n\nn,n\n\nWe control (35) by dividing the range of t into several parts.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n29\n\n6.1. The region of very large t. We now control the region where\nt \u2265 R\u03b5 for some large R\u03b5 .\nFrom Lemma A.2 we have that\nn0\n\nn0\n\n1X 1\n1X 1\n( \u221a \u03c3i (An,n0 ))2 ,\n( \u221a \u03c3i (Bn,n0 ))2\nn i=1 n\nn i=1 n\nis almost surely bounded, and thus\nZ \u221e\nt|d\u03bdn,n0 (t)|\n0\n\nis also almost surely bounded. Thus, with probability 1 \u2212 O(\u03b5), we\nhave\nZ \u221e\nt|d\u03bdn,n0 (t)| \u2264 C\u03b5\n0\n\nfor all but finitely many n, and some C\u03b5 independent of n, which implies\nthat\nZ\n\u221e\n\n| log t||d\u03bdn,n0 (t)| \u2264 \u03b5\n\n(37)\n\nR\u03b5\n\nfor all but finitely many n, and some R\u03b5 depending only on \u03b5.\n6.2. The region of intermediate t. We now control the region \u03b54 \u2264\nt \u2264 R\u03b5 .\nLemma 6.3. Let \u03c8 be a smooth function which equals 1 on [\u03b54 , R\u03b5 ] and\nis supported on [\u03b54 /2, 2R\u03b5 ]. Then with probability 1, we have\nZ \u221e\n\u03c8(t) log td\u03bdn,n0 (t) = O(\u03b5),\n(38)\n0\n\nif \u03b4 is sufficiently small depending on \u03b5 and \u03c8.\nProof. From the interlacing property (Lemma A.1), we see that\nZ \u221e\nZ \u221e\n\u03c8(t) log td\u03bdn,n0 (t) =\n\u03c8(t) log td\u03bdn,n (t) + O(\u03b5)\n0\n\n0\n\nif \u03b4 is sufficiently small depending on \u03b5 and \u03c8.\nWe now apply the recent result in [3, Theorem 1.1]. For the reader's\nconvenience, we restate this result in the Appendix; see Theorem B.1.\nThis result asserts under the above hypotheses that the ESDs d\u03bc 1 An A\u2217n\nn\nand d\u03bc 1 Bn Bn\u2217 converge almost surely to the same limit (in fact, this\nn\nlimit is given explicitly in terms of the limiting distribution of \u03bc 1 Mn Mn\u2217\nn\nvia the inverse Stieltjes transform of (47)). In particular, \u03bdn,n converges\nalmost surely to zero, and the claim follows.\n\u0003\n\n\f30 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nRemark 6.4. Note that for the convergence in probability case of Proposition 2.2, we need to apply Theorem B.1 to a subsequence of n rather\nthan to all n, thanks to the subsequence extraction performed at the\nbeginning of Section 4.\n6.5. The region of moderately small t. We now control the region\n\u03b4 2 \u2264 t \u2264 \u03b54 . For this we need some bounds on the low singular values\nof An,n0 and Bn,n0 .\nLemma 6.6. With probability 1, we have\nn0\n\n1X 1\n( \u221a \u03c3i (An,n0 ))\u22122 = O(1)\nn i=1 n\n\n(39)\n\nfor all but finitely many n, and similarly with An,n0 replaced by Bn,n0 .\nProof. Clearly it suffices to establish the claim for An,n0 . Using Proposition 5.1 and the Borel-Cantelli lemma, we see that with probability\n1, we have\n1\n1\u221a\ndist( \u221a Xi , span(X1 , . . . , Xi\u22121 , Xi+1 , . . . , Xn0 )) \u2265\n\u03b4n\n2\nn\nfor all but finitely many n, and all 1 \u2264 i \u2264 n0 . The claim then follows\nfrom Lemma A.4.\n\u0003\nSince the \u03c3i (An,n0 ) are decreasing in i, and n0 = b(1 \u2212 \u03b4)nc, we see\nthat the above lemma implies that with probability 1, we have\n1\n\u221a \u03c3b(1\u22122\u03b4)nc (An,n0 ) \u2265 c\u03b4\nn\nfor all but finitely many n, and some absolute constant c > 0. We can\ngeneralize this lower bound to handle higher singular values also:\nLemma 6.7. There exists an absolute constant c > 0 such that with\nprobability 1, we have\nn0 \u2212 i\n1\n\u221a \u03c3i (An,n0 ) \u2265 c\n(40)\nn\nn\nfor all but finitely many n, and all 1 \u2264 i \u2264 (1 \u2212 2\u03b4)n, and similarly\nwith An,n0 replaced by Bn,n0 .\nProof. Clearly it suffices to establish the claim for An,n0 . Using Proposition 5.1 and the Borel-Cantelli lemma, we see that with probability\n1, we have\n1\u221a\n1\ndist( \u221a Xi , span(X1 , . . . , Xi\u22121 , Xi+1 , . . . , Xn00 )) \u2265\nn \u2212 n00\n2\nn\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n31\n\nfor all but finitely many n, and all 1 \u2264 i \u2264 n00 and n/2 \u2264 n00 \u2264 n0 .\nApplying Lemma A.4, we conclude that we almost surely have\nn00\n\n1X 1\nn\n( \u221a \u03c3i (An,n00 ))\u22122 = O(\n)\nn i=1 n\nn \u2212 n00\nfor all but finitely many n, and all n/2 \u2264 n00 \u2264 n0 . Using the crude\nbound\nn00\nX\n1\n1\n( \u221a \u03c3i (An,n00 ))\u22122 \u2265 (n \u2212 n00 )( \u221a \u03c32n00 \u2212n (An,n00 ))\u22122\nn\nn\ni=1\nwe conclude that we almost surely have\n1\nn \u2212 n00\n\u221a \u03c32n00 \u2212n (An,n00 ) \u2265 c0\nn\nn\nfor all but finitely many n, all n/2 \u2264 n00 \u2264 n0 , and some absolute\nconstant c0 > 0. The claim now follows from the Cauchy interlacing\nproperty (Lemma A.1).\n\u0003\nRemark 6.8. If one assumes stronger moment assumptions (e.g subgaussian) on x, then more precise bounds are known, especially in the\nMn = 0 case: see [19], [20].\nFrom this lemma we can now bound the relevant contribution to (35):\nLemma 6.9. With probability 1, and if \u03b4 is sufficiently small depending\non \u03b5, we have\nZ \u03b54\n| log t||d\u03bdn,n0 (t)| = O(\u03b5)\n(41)\n\u03b42\n\nfor all but finitely many n.\nProof. By the triangle inequality and symmetry it suffices to show that\nwith probability 1, we have\nZ \u03b54\n| log t|d\u03bc 10 An,n0 A\u2217 0 (t) = O(\u03b5)\nn\n\n\u03b42\n\nn,n\n\nfor all but finitely many n. We rewrite the left-hand side as\nn0\n\n1X\n1\nf ( \u221a \u03c3i (An,n0 ))\nn i=1\nn\nwhere f (t) := | log t|I(\u03b4 2 \u2264 t2 \u2264 \u03b54 ). Since f cannot exceed | log \u03b4|, we\nsee that the contribution of the case i \u2265 (1 \u2212 2\u03b4)n is acceptable if \u03b4 is\nsmall enough, so it suffices to show that we almost surely have\nX\n1\n1\nf ( \u221a \u03c3i (An,n0 )) = O(\u03b5)\nn\nn\n1\u2264i\u2264(1\u22122\u03b4)n\n\n\f32 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nfor all but finitely many n.\nBy Lemma 6.7, we may assume that n is such that (40) holds. As a\nconsequence, we see that the only terms in the above sum which are\nnon-vanishing are those for which i = (1 \u2212 O(\u03b52 ))n. But then if we\napply (40) and crudely estimate f (t) \u2264 \u2212 log t we obtain the claim. \u0003\n6.10. The contribution of very small t. Finally, we need to control\nthe contribution when t \u2264 \u03b4.\nLemma 6.11. With probability 1, and if \u03b4 is sufficiently small depending on \u03b5, we have\nZ \u03b42\n| log t||d\u03bdn,n0 (t)| = O(\u03b5)\n(42)\n0\n\nfor all but finitely many n.\nProof. By arguing as in the proof of Lemma 6.9, it suffices to show that\nwe almost surely have\nn0\n\n1X\n1\ng( \u221a \u03c3i (An,n0 )) = O(\u03b5)\nn i=1\nn\nfor all but finitely many n, where g(t) := | log t|I(t2 \u2264 \u03b4 2 ).\nBy Lemmas 6.6, we may assume n is such that (39) holds. On the\nother hand, if \u03b4 is small enough, we have the bound g(t) \u2264 \u03b5t\u22122 . The\nclaim now follows from (39).\n\u0003\nPutting together (37), (38), (41), (42) we see that with probability\n1\u2212O(\u03b5), we have (36) for all but finitely many n, and the claim follows.\n7. Extensions\n7.1. Proof of Theorem 1.17. The theorem in the case of almost sure\nconvergence follows immediately from Theorem 1.7 by conditioning on\nMn , so it remains to verify the theorem in the case of convergence in\nprobability.\nLet fix a test function f (as in (1)) and a positive \u03b5. By the boundedness in probability of n12 kM k22 , we can find a C = C\u03b5 such that\nP(Mn \u2208 \u03a9n ) \u2265 1 \u2212 \u03b5, where\n\u03a9n := {M \u2208 Mn (C) :\n\n1\nkM k22 \u2264 C}.\nn2\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n33\n\nLet Mnf be the matrix in \u03a9n which maximizes4 the quantity\nZ\nZ\nP(| f (z) d\u03bc \u221a1 (Mnf +Xn (z) \u2212 f (z) d\u03bc \u221a1 (Mnf +Yn (z)| \u2265 \u03b5).\nn\n\nC\n\nC\n\nn\n\nApplying Theorem 1.7 to the sequence Mnf + Xn and Mnf + Yn , we see\nthat this quantity is o(1).\nTheorem 1.17 follows by integrating over all possible values of Mn\nusing the definition of Mnf , as well as the fact that P(\u03a9n ) \u2265 1 \u2212 \u03b5, and\nthen letting \u03b5 \u2192 0.\n7.2. Proof of Theorem 1.18. We first verify the claim for convergence in probability.\nThe condition (i) of Theorem 2.1 is satisfied thanks to the boundedness\nin probability of (5). In order to complete the proof, one needs to check\n(ii). Notice that\n\n1\n1\n\u22121 \u22121\ndet( \u221a An \u2212 zI) = det( \u221a (Kn\u22121 Mn L\u22121\nn + Xn ) \u2212 zKn Ln ) det Ln Kn .\nn\nn\nThe term det Ln Kn also appears in det( \u221a1n Bn \u2212 zI) and becomes additive (and thus cancels) after taking logarithm. Therefore, one only\nneeds to show that\n\n\u0011\n|\n+ Xn ) \u2212 zKn\u22121 L\u22121\nn\n\u0011\n\u0010\n\u22121 \u22121\n\u2212 n1 log | det \u221a1n (Kn\u22121 Mn L\u22121\n|\nL\n+\nY\n)\n\u2212\nzK\nn\nn\nn\nn\n1\nn\n\nlog | det\n\n\u0010\n\n\u221a1 (K \u22121 Mn L\u22121\nn\nn\nn\n\nconverges in probability to zero.\nOne can obtain this by repeating the proof of Proposition 2.2. The\nslight change here is that zI is replaced by zKn\u22121 L\u22121\nn , but this has no\nsignificant impact, except that we need to show\n1\n\u22121 \u22121\nFn := \u221a (Kn\u22121 Mn L\u22121\nn \u2212 zKn Ln )\nn\nsatisfies\n4If\n\nthe maximum is not attained, one can instead choose Mnf to be a matrix\nwhich maximizes this quantity to within a factor of two (say).\n\n\f34 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\n1\n1\ntrace Fn Fn\u2217 = 2 kFn k22 = O(1)\n2\nn\nn\nalmost surely (in order to guarantee (3)). But this is a consequence of\nthe boundedness in probability of (5).\nThe proof of the almost sure convergence is established similarly, with\nthe obvious changes (e.g. replacing boundedness in probability with\nalmost sure boundedness). We omit the details.\n\n8. Proof of Theorem 1.20\nWe first prove that (ii) implies (i) for almost sure convergence. Let An\nand \u03bc be as in Theorem 1.20. Construct a diagonal matrix Bn0 \u221awhose\ndiagonal entries are independent samples from \u03bc and let Bn := nBn0 .\nWe wish to invoke Theorem 2.1. We first need to verify the almost sure\nboundedness of (9). The bound for An follows from Lemma 1.9, and\nthe bound for Bn follows from the second moment hypothesis on \u03bc and\nthe (strong) law of large numbers. By Theorem 2.1, the problem now\nreduces to showing that for almost all complex numbers z,\n1\n1\n1\n1\nlog | det( \u221a An \u2212 zI)| \u2212 log | det( \u221a Bn \u2212 zI)|\nn\nn\nn\nn\nconverges almost surely to zero. The right hand side is easy to compute:\nPn\nlog |\u03bbi \u2212 z|\n1\n1\n1\n0\nlog | det( \u221a Bn \u2212 zI)| = log | det(Bn \u2212 zI)| = i=1\n,\nn\nn\nn\nn\nwhere \u03bbi are iid samples\nfrom \u03bc. On the other hand, from Fubini's\nR\ntheorem we see that C log |w \u2212 z| d\u03bc(w) is locally integrable in z, and\nthus\nZ\nlog |w \u2212 z| d\u03bc(w) < \u221e\n\n(43)\n\nC\n\nfor almost every z. If z is such P\nthat (43) holds, then by the strong law\nn\ni=1 log |\u03bbi \u2212z|\nof\nlarge\nnumbers,\nwe\nsee\nthat\nconverges almost surely to\nn\nR\nlog\n|w\n\u2212\nz|\nd\u03bc(w).\nThis\nshows\nthat\n(ii)\nimplies (i) for almost sure\nC\nconvergence. The proof for convergence in probability is identical and\nis left as an exercise to the reader.\nNow we show that (iii) implies (ii) for almost sure convergence. Let z\nbe such thatP\n(43) and (iii) hold. To show (ii), it suffices\nfrom (11) to\nR\nn\n1\nshow that n i=1 log \u03c3i converges almost surely to C log |w \u2212z| d\u03bc(w),\nwhere \u03c3i = \u03c3i ( \u221a1n An \u2212 zI) are the singular values of \u221a1n An \u2212 zI. On\np\nP\nthe other hand, from (iii) we already know that n1 ni=1 log \u03c3i2 + \u03b5n\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n35\n\nR\nconverges almost surely to C log |w \u2212z| d\u03bc(w). Thus it suffices to show\nthat\nn\nq\n1X\nlog \u03c3i2 + \u03b5n \u2212 log \u03c3i\n(44)\nn i=1\nconverges almost surely to zero.\nFrom Lemma 1.9, we know that n12 kAn k22 is almost surely bounded,\nand so for each z\nn\n1X 2\n1 1\n\u03c3i = k \u221a An \u2212 zIk22\nn i=1\nn\nn\nis almost surely bounded also. From this we easily see that\nq\nX\n1\nlog \u03c3i2 + \u03b5n \u2212 log \u03c3i\nn 1\u2264i\u2264n:\u03c3 \u2265\u03b4\ni\n\nn\n\nconverges almost surely to zero for some sequence \u03b4n (depending on\n\u03b5n ) converging sufficiently slowly to zero. To conclude the almost sure\nconvergence of (44) to zero, it thus suffices to show that\nX\n1\n1\nlog\nn 1\u2264i\u2264n:\u03c3 \u2264\u03b4\n\u03c3i\ni\n\nn\n\nconverges almost surely to zero. Using Lemma 4.1, we almost surely\nhave supi log \u03c31i \u2264 O(log n) for all but finitely many n, so it suffices to\nshow that\nX\n1\n1\nlog .\nn\n\u03c3i\n0.99\n1\u2264i\u2264n\u2212n\n\n:\u03c3i <\u03b4n\n\nconverges almost surely to zero. To do this, it suffices by the union\nbound and the Borel-Cantelli lemma to show that\ni\nP(\u03c3n\u2212i \u2264 c ) = O(exp(\u2212n0.01 )).\n(45)\nn\nfor all 1 \u2264 i \u2264 n \u2212 n0.99 and some c > 0 independent of n.\n0\nFor this we argue as in the proof of Lemma 6.7.\n\u221a Fix i. Let An be the\nmatrix form by the first n \u2212 k rows of An \u2212 z nI with k := i/2 and\n\u03c3j0 , 1 \u2264 j \u2264 n \u2212 k be the singular values of A0n (in decreasing order, as\nusual). By the interlacing law (Lemma A.1) and re-normalizing,\n\n1 0\n.\n\u03c3n\u2212i \u2265 \u221a \u03c3n\u2212i\nn\nBy Lemma A.4, we have that\n\u22122\n0\u22122\n\u03c310\u22122 + * * * + \u03c3n\u2212k\n= dist\u22122\n1 + * * * + distn\u2212k ,\n\n(46)\n\n\f36 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nwhere distj is the distance from the jth row of A0n to the subspace\nspanned by the remaining rows.\nAs shown in the proof of Lemma 4.2,\n1\u2212exp(\u2212n\u22120.01 ),\n\u221a with probability\n\u221a\ndistj is bounded from below by \u03a9( k) = \u03a9( i) for all j. Thus, with\nthis probability, the right hand side in the above identity is O(n/i).\nOn the other hand, as the \u03c3j0 are ordered decreasingly, the left hand\nside is at least\n\ni 0\u22122\n0\u22122\n(i \u2212 k)\u03c3n\u2212i\n= \u03c3n\u2212i\n.\n2\nIt follows that with probability 1 \u2212 exp(\u2212n\u22120.01 ),\n\ni\n0\n\u03c3n\u2212i\n= \u03a9( \u221a ).\nn\nThis and (46) complete the proof of (45), and so (44) converges almost\nsurely to zero.\nAs previously observed, the convergence of (44) to zero shows that (ii)\nimplies (iii) for almost sure convergence. An inspection of the argument\nshows the convergence of (44) to zero also lets us deduce (iii) from (ii).\nThe claim for convergence in probability follows similarly. To conclude\nthe proof of Theorem 1.20, it thus suffices to show that (i) implies (ii).\nAgain we start with the almost sure convergence case. Assume\n\u221a that\n(i) holds, and let z be such that (43) holds. By shifting A by nzI if\nnecessary we may take z to be zero. Let \u03bb1 , . . . , \u03bbn P\ndenote the eigenvalues of \u221a1n An . By (11), it suffices to show that n1 nj=1 log |\u03bbj | conR\nverges almost surely to C log |w| d\u03bc(w). From (13) we know that\nPn\n1\n|\u03bbj |2 is almost surely bounded. From this and (i) we conclude\nj=1P\nn\nR\nthat n1 nj=1 log(|\u03bbj |+\u03b5) converges almost surely to C (log |w|+\u03b5) d\u03bc(w)\nfor any fixed \u03b5 > 0. Combining\nthis with (43) and dominated conP\nvergence, we see that n1 nj=1 log(|\u03bbj | + \u03b5n ) converges almost surely to\nR\nlog |w| d\u03bc(w) for some sequence \u03b5n > 0 converging sufficiently slowly\nC\nto zero. It thus suffices to show that\nn\n\n1X\nlog(|\u03bbj | + \u03b5n ) \u2212 log |\u03bbj |\nn j=1\nconverges almost surely to zero.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n37\n\nBy repeating the arguments used to establish the almost sure convergence of (44) to zero, it suffices to show that\nX\n1\n1\nlog\nn\n|\u03bbi |\n1\u2264i\u2264n:|\u03bbi |\u2264\u03b4n\n\nconverges almost surely to zero.\nLet us order the eigenvalues \u03bbi so that |\u03bb1 | \u2265 . . . \u2265 |\u03bbn |. From Lemma\n4.1 and (45) (and the Borel-Cantelli lemma) we know that we almost\nsurely have\nX\n1\n1\n1\nlog \u2264 O(\u03ba log )\nn\n\u03c3i\n\u03ba\n(1\u2212\u03ba)n<i\u2264n\n\nfor all but finitely many n for any fixed 0 < \u03ba < 1/2, and hence by\nWeyl's comparison inequality (Lemma A.3) that we almost surely have\nX\n1\n1\n1\n\u2264 O(\u03ba log )\nlog\nn\n|\u03bbi |\n\u03ba\n(1\u2212\u03ba)n<i\u2264n\n\nfor all but finitely many n also. Since the left-hand side is bounded\n1\nfrom below by \u03ba log |\u03bbb(1\u2212\u03ba)nc\nwe almost surely conclude a lower bound\n|\nof the form\n|\u03bbb(1\u2212\u03ba)nc | \u2265 \u03baO(1)\nfor all but finitely many n. In particular (by setting \u03b4 to be a suitable\npower of \u03ba) this implies that almost surely\nX\n1\n1\nlog\n\u2264 O(\u03b4 c )\nn\n|\u03bbi |\n1\u2264i\u2264n:|\u03bbi |\u2264\u03b4\n\nfor all but finitely many n for any fixed 0 < \u03b4 \u001c 1 and some absolute\nconstant c > 0, and the claim follows. The analogous implication for\nconvergence in probability is similar. The proof of Theorem 1.20 is now\ncomplete.\nAppendix A. Linear algebra inequalities\nIn this appendix we record some elementary identities and inequalities\nregarding the eigenvalues and singular values of matrices.\nLemma A.1 (Cauchy's interlacing law). Let A be an n\u00d7n matrix with\ncomplex entries and A0 be the submatrix formed by the first m := n \u2212 k\nrows. Let \u03c31 (A) \u2265 . . . \u2265 \u03c3n (A) \u2265 0 denote the singular values of A,\nand similarly for A0 . Then we have\n\u03c3i (A) \u2265 \u03c3i (A0 ) \u2265 \u03c3i+k (A)\nfor every 1 \u2264 i \u2264 n \u2212 k.\n\n\f38 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nProof. The claim follows easily from the minimax characterization\n\u03c3i (A) = sup\n\ninf\n\nVi \u2282Cn v\u2208Vi :kvk=1\n\nkAvi k\n\nand\n\u03c3i (A0 ) =\n\nsup\n\ninf\n\nVi \u2282Cn\u2212k v\u2208Vi :kvk=1\n\nkAvi k\n\nof the singular values, where Vi range over i-dimensional complex subspaces.\n\u0003\nLemma A.2 (Weyl comparison inequality for second moment). Let\nA = (aij )1\u2264i,j\u2264n \u2208 Mn (C) have generalized eigenvalues \u03bb1 , . . . , \u03bbn \u2208 C\nand singular values \u03c31 (A) \u2265 . . . \u2265 \u03c3n (A) \u2265 0. Then\nn\nX\n\n2\n\n|\u03bbj | \u2264\n\nj=1\n\nn\nX\n\n2\n\n\u03c3j (A) =\n\nkAk22\n\n=\n\nj=1\n\nn X\nn\nX\n\n|aij |2 .\n\ni=1 j=1\n\nProof. The two equalities here are clear, so it suffices to prove the inequality. By the Jordan normal form we can write A = BU B \u22121 for\nsome upper-triangular U and invertible B. By the QR factorization\nwe can write B = QR for some orthogonal Q and upper triangular R.\nWe conclude that A = QV Q\u22121 for some upper triangular V . Conjugating by Q, we thus reduce to the case when A is an upper triangular\nmatrix, in which case the eigenvalues are simply the diagonal entries\na11 , . . . , ann and the claim is clear.\n\u0003\nWe also have the following (stronger) variant of the above inequality:\nLemma A.3 (Weyl comparison inequality for products). Let A =\n(aij )1\u2264i,j\u2264n \u2208 Mn (C) have generalized eigenvalues \u03bb1 , . . . , \u03bbn \u2208 C, ordered so that |\u03bb1 | \u2264 . . . \u2264 |\u03bbn |, and singular values \u03c31 (A) \u2265 . . . \u2265\n\u03c3n (A) \u2265 0. Then we have\nJ\nY\nj=1\n\nand\n\nn\nY\nj=J\n\n|\u03bbj | \u2264\n\nJ\nY\n\n\u03c3j (A)\n\nj=1\n\n\u03c3j (A) \u2264\n\nn\nY\n\n|\u03bbj |\n\nj=J\n\nfor all 0 \u2264 J \u2264 n.\nProof. It suffices to prove the former claim, as the latter then follows\nfrom (11). By arguing as in Lemma A.2 we may assume that A is\nupper triangular, so that the diagonal entries are some permutation of\n\u03bb1 , . . . , \u03bbn . Consider the symmetric minor A0 of A formed by the rows\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n39\n\nand columns corresponding to the entries \u03bb1 , . . . , \u03bbJ . The determinant\nof this matrix is then \u03bb1 . . . \u03bbJ , and thus by (11) we have\nJ\nY\n\n0\n\n\u03c3j (A ) =\n\nj=1\n\nJ\nY\n\n|\u03bbj |.\n\nj=1\n\nThe claim then follows from the Cauchy interlacing inequality (Lemma\nA.1).\n\u0003\nNow we record a useful identity for the negative second moment of a\nrectangular matrix.\nLemma A.4 (Negative second moment). Let 1 \u2264 n0 \u2264 n, and let A be\na full rank n0 \u00d7 n matrix with singular values \u03c31 (A) \u2265 . . . \u2265 \u03c3n0 (A) >\n0 and rows X1 , . . . , Xn0 \u2208 Cn . For each 1 \u2264 i \u2264 n0 , let Wi be the\nhyperplane generated by the n0 \u2212 1 rows X1 , . . . , Xi\u22121 , Xi+1 , . . . , Xn0 .\nThen\nn0\nn0\nX\nX\n\u22122\n\u03c3j (A) =\ndist(Xj , Wj )\u22122 .\nj=1\n\nj=1\n\nProof. Observe that the n0 \u00d7 n0 matrix (AA\u2217 )\u22121 has eigenvalues\n\u03c31 (A)\u22122 , . . . , \u03c3n0 (A)\u22122 .\nTaking traces, we conclude that\n0\n\nn\nX\nj=1\n\n0\n\n\u22122\n\n\u03c3j (A)\n\nn\nX\n=\n(AA\u2217 )\u22121 ej * ej\nj=1\n0\n\nwhere e1 , . . . , en0 is the standard basis of Cn . But if vj := (AA\u2217 )\u22121 ej =\n(vj,1 , . . . , vj,n0 ), then A\u2217 vj = vj,1 X1 + . . . + vj,n0 Xn0 is orthogonal to\nA\u2217 ei = Xi for i 6= j (and thus orthogonal to Wj ), and has an inner\nproduct of 1 with A\u2217 ej = Xj . Taking inner products of A\u2217 vj with the\northogonal projection of Xj to Wj , we conclude that\nvj,j dist(Xj , Wj )2 = 1.\nSince vj,j = vj * ej = (AA\u2217 )\u22121 ej * ej , the claim follows.\n\n\u0003\n\nAppendix B. A result of Dozier and Silverstein\nHere we reproduce Theorem 1.1 of [3] which we used in the end of\nSection 6.\nTheorem B.1. [3, Theorem 1.1] Let c be a positive constant and x\nbe a random variable with variance one. Let Xn be an n \u00d7 r random\nmatrix whose entries are iid copies of x, where r = (c + o(1))n. Let Mn\n\n\f40 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nbe a random n \u00d7 r matrix independent from Xn such that the ESD of\nMn Mn\u2217 converges to a limiting distribution H. Define Cn := nc (Mn +\nXn )(Mn + Xn )\u2217 . Then the ESD of Cn converges almost surely (and\nhence also in probability)\nR 1 to a limiting distribution F , whose Stieljes\ntransform m(z) := \u03bb\u2212z\ndF (\u03bb) satisfies the integral equation\nZ\ndH(t)\nm=\n(47)\nt\n\u2212 (1 + cm)z + (1 \u2212 c)\n1+cm\nfor any z \u2208 C.\nRemark B.2. The theorem still holds if we restrict the size n of the\nmatrices to an infinite subsequence n1 < n2 < . . . of positive integers.\nOne can show this by, for example, artificially filling in the missing\nindices or repeat the proof of Theorem B.1 under this restriction.\nRemark B.3. In (47), H appears, but the actual definition of Mn is\nirrelevant. Thus, one can conclude that if Mn and Mn0 are such that\nthe ESD's of Mn Mn\u2217 and Mn0 Mn0\u2217 tend to the same limit, then the ESDs\nof nc (Mn + Xn )(Mn + Xn )\u2217 and nc (Mn0 + Xn )(Mn0 + Xn )\u2217 also tend to\nthe same limit.\nRemark B.4. It was mentioned by Speicher [21] and also Krishnapur\n(private communication) that Theorem B.1 can be proved using free\nprobability, which is different from the approach in [3].\n\nAppendix C. Using a Hermitian invariance principle\n(by Manjunath Krishnapur)\nThe authors have shown invariance principles for ESDs of several nonHermitian matrix models. As in earlier papers, the proof goes through\nHermitian matrices, but does not need rates of convergence of the Hermitian ESDs, thanks to new ideas such as Lemma 4.2. However, because of the use of Theorem B.1, it may appear that a limiting result\nfor the associated Hermitian matrices is necessary to carry the program\nthrough. In this appendix, we point out how one may obtain a weak\ninvariance principle for ESDs of non-Hermitian matrices by using an\ninvariance principle for Hermitian matrices due to Chatterjee [4], in\ncases where a convergence result such as Theorem B.1 is not available.\nAs mentioned earlier, other parts of the proof do not require the entries\nare iid. Thus, as a consequence, we can obtain a weak invariance principle for a random matrix model with independent but not identically\ndistributed entries.\nWe need the following definition from [26, Section 2].\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n41\n\nDefinition C.1 (Controlled second moment). Let \u03ba \u2265 1. A complex\nrandom variable x is said to have \u03ba-controlled second moment if one\nhas the upper bound\nE|x|2 \u2264 \u03ba\n(in particular, |Ex| \u2264 \u03ba1/2 ), and the lower bound\nERe(zx \u2212 w)2 I(|x| \u2264 \u03ba) \u2265\n\n1\nRe(z)2\n\u03ba\n\n(48)\n\nfor all complex numbers z, w.\nExample. The Bernoulli random variable (P(x = +1) = P(x = \u22121) =\n1/2) has 1-controlled second moment. The condition (48) asserts in\nparticular that x has variance at least \u03ba1 , but also asserts that a significant portion of this variance occurs inside the event |x| \u2264 \u03ba, and also\ncontains some more technical phase information about the covariance\nmatrix of Re(x) and Im(x).\n\u0011\n\u0010\n\u0011\n\u0010\n(n)\n(n)\nbe constant\nTheorem C.2. Let Mn = \u03bci,j\nand Cn = \u03c3i,j\ni,j\u2264n\n\ni,j\u2264n\n\n(i.e. deterministic) matrices satisfying\n(1) supn n\u22122 kMn k22 < \u221e,\n(n)\n(2) a \u2264 \u03c3i,j \u2264 b for all n, i, j for some 0 < a < b < \u221e.\nGiven a matrix X = (xi,j )i,j\u2264n set\n\u0011\n1 \u0010 (n)\n1\n(n)\n.\nAn (X) = \u221a (Mn + Cn * X) = \u221a \u03bci,j + \u03c3i,j xi,j\nn\nn\ni,j\u2264n\n(here \"*\" denotes Hadamard product).\n(n)\n\nNow suppose that xi,j are independent complex-valued random vari(n)\n(n)\n(n)\nables with E[xi,j ] = 0 and E[|xi,j |2 ] = 1 and that yi,j are independent\nrandom variables, also having zero mean and unit variance.\n(n)\n\n(n)\n\nAssume furthermore that both xij and yij have \u03ba-controlled second\nmoment for some constant \u03ba > 0.\nAssume also Pastur's condition\nn\n\u221a i\n1 X h (n) 2 (n)\nE\n|x\n|\nI|x\n|\n\u2265\n\u000f\nn \u2212\u2192 0\ni,j\ni,j\nn2 i,j=1\nand the same for Y in place of X. Then,\n\u03bcAn (X) \u2212 \u03bcAn (Y) \u2192 0\nin the sense of probability.\n\nfor all \u000f > 0.\n\n(49)\n\n\f42 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\nSome remarks.\n\n(n)\n\n(n)\n\n(1) If we assume that xi,j are i.i.d. and yi,j are i.i.d then Pastur's\ncondition is obviously satisfied. Further, the condition of \u03bacontrolled second moment is also not necessary (see the first\nstep in the proof sketch).\n(2) Although the weak invariance principle in the paper uses only\nsubsequential limits (see Remark 6.4), it does use Theorem B.1\nto say that subsequential limits are the same for X as for Y.\nHence we need some changes in the proof in order to establish\nTheorem C.2, which we do in this appendix.\n(3) This highlights the important new ideas of the paper, such as\nLemma 4.2, which eliminate the need for rates of convergence\nof ESDs of the Hermitian matrices (An \u2212 zI)\u2217 (An \u2212 zI). This\nis unlike all earlier papers in the subject that followed Bai's\napproach and required such rates (eg., [1],[26],[9],[15]). The\nneed for rates made it impossible to use the invariance principle\nfor Hermitian matrices as we shall do now.\n(4) Take Cn = J (all ones matrix) and Mn = 0. Then Pastur's\ncondition (49) implies almost sure convergence of the ESD of\nAn (X)\u2217 An (X) (see [2, Theorem 3.9]). For general Cn , since\nwe use Chatterjee's invariance principle which assumes Pastur's\ncondition but only gives weak invariance, we are able to assert\nonly weak invariance for the non-Hermitian ESDs also. Thus,\nthere is some room for improvement here, namely, to strengthen\nthe conclusion of Theorem C.2 to almost sure convergence.\n(5) Does ESD of An (X) converge? Perhaps so, provided the singular values of Cn \u2212 zI have a limiting measure for every z. In\n[12] we have discussed some easy-to-check sufficient conditions\non Cn which implies convergence.\n\nThe following lemma is a \"Wishart\" analogue of the computations in\nsection 2 of [4] which considers Wigner matrices. As in that paper, the\nidea is to consider the Stieltjes transform of the ESD of An (X)\u2217 An (X)\nas a function of X. However a slight twist is needed as compared to\nWigner matrices, because the entries of An (X)\u2217 An (X) are quadratic in\nX whereas the invariance principle we invoke requires bounds on the\nsup-norm of derivatives of the Stieltjes transform.\nLemma C.3. Let X and Y be as in Theorem C.2. Let \u03bdnX and \u03bdnY\nbe the ESDs of An (X)\u2217 An (X) and An (Y)\u2217 An (Y). Then \u03bdnX \u2212 \u03bdnY \u2192 0\nweakly as n \u2192 \u221e.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n43\n\nProof. Let\n\u0014\nHn (X) =\n\n0\nAn (X)\n\u2217\nAn (X)\n0\n\n\u0015\n\nhave ESD \u03b8nX . The eigenvalues of Hn (X) are exactly the positive and\nnegative square roots of the eigenvalues of An (X)\u2217 An (X). Thus we\nmust show that \u03b8nX \u2212 \u03b8nY \u2192 0 weakly, in probability. Fix any \u03b1 in\n1\nthe upper half plane and let f (X) := 2n\nTr(Hn (X) \u2212 \u03b1I)\u22121 . The proof\nis complete if we show that E[f (X)] \u2212 E[f (Y)] \u2192 0 for any \u03b1 with\nIm{\u03b1} > 0. This can be done by following the same calculations as in\n[4]. It works because the entries of Hn (X) are linear in X and hence\nthe first partial derivative of Hn with respect to any xi,j is a constant\nmatrix. One must also use the upper bound on \u03c3i,j to bound the\nderivatives of f .\n\u0003\nRemark: Obviously the same conclusion holds for An \u2212 zI, just by\nabsorbing zI into Mn .\nProof of Theorem C.2. The conditions on Mn and Cn show that the\nfirst condition of Theorem 2.1 is satisfied (where the two matrices An\nand Bn are now An (X) and An (Y)).\nThus we only need to show an analogue of Proposition 2.2 (only the\nweak part). We sketch the modifications needed.\n(1) Lemma 4.1 can be proved under independence and \u03ba-controlled\nsecond moment without i.i.d. assumption (see [26, Theorem\n2.5]). If we make i.i.d. assumption, then Lemma 4.1 is itself\napplicable, which explains the first remark after the statement\nof the theorem.\nThe upper bounds on singular values in (31) are very general\nand hold in our setting for the same reasons. Hence we reduce\nto Lemma 4.2 and Lemma 4.3 as in the paper.\n(2) The high-dimensional contribution (analogue of Lemma 4.2) is\nproved almost the same way. In the proof of the lower tail bound\n(n)\n(Proposition 5.1) use the bounds on \u03c3i,j appropriately. In particular, we get a lower bounds of a2 (n\u2212d) for the second moment\nof dist(X, W ) in Lemma 5.3, and in applying Theorem 5.2 we\nget a Lipschitz constant of b for F (X) = dist(X, W ).\n(3) In the low-dimensional contribution (Lemma 4.3), the calculations in sections 6.1, 6.5 and 6.10 are exactly as before (in\nsection 6.5, we use the concentration result already outlined in\nthe previous step).\n\n\f44 TERENCE TAO, VAN VU, AND MANJUNATH KRISHNAPUR (APPENDIX)\n\n(4) That leaves section 6.2, which is the only step that is differently handled. Here we apply Lemma C.3 instead of quoting\nTheorem B.1.\n\u0003\nAcknowledgements. The first author is supported by a grant from the\nMacarthur Foundation and by NSF grant DMS-0649473. The second\nauthor is supported by an NSF Career Grant. The authors would like\nto thank M. Krishnapur for useful discussions and his careful reading of\nan early draft, and Ken Miller, Ricky, and weiyu for further corrections.\nWe also like to thank P. Matchett Wood for providing the figures in\nthe introduction.\n\nReferences\n[1] Z. D. Bai, Circular law, Ann. Probab. 25 (1997), 494\u2013529.\n[2] Z. D. Bai and J. Silverstein, Spectral analysis of large dimensional random\nmatrices, Mathematics Monograph Series 2, Science Press, Beijing 2006.\n[3] R. Dozier, J. Silverstein, On the empirical distribution of eigenvalues of\nlarge dimensional information-plus-noise-type matrices, J. Multivar. Anal. 98\n(2007), 678\u2013694.\n[4] S. Chatterjee, A simple invariance principle. [arXiv:math/0508213]\n[5] D. Chafai, Circular law for non-central random matrices, preprint.\n[6] A. Edelman, Eigenvalues and condition numbers of random matrices. SIAM\nJ. Matrix Anal. Appl. 9 (1988), no. 4, 543\u2013560.\n[7] V. L. Girko, Circular law, Theory Probab. Appl. (1984), 694\u2013706.\n[8] V. L. Girko, The strong circular law. Twenty years later. II. Random Oper.\nStochastic Equations 12 (2004), no. 3, 255\u2013312.\n[9] F. G\u00f6tze, A.N. Tikhomirov, On the circular law, preprint\n[10] F. G\u00f6tze, A.N. Tikhomirov, The Circular Law for Random Matrices, preprint\n[11] J. Ginibre, Statistical Ensembles of Complex, Quaternion, and Real Matrices,\nJournal of Mathematical Physics 6 (1965), 440-449.\n[12] M. Krishnapour and V. Vu, manuscript in preparation.\n[13] M. Ledoux, The concentration of measure phenomenon, Mathematical survey\nand monographs, volume 89, AMS 2001.\n[14] M.L. Mehta, Random Matrices and the Statistical Theory of Energy Levels,\nAcademic Press, New York, NY, 1967.\n[15] G. Pan and W. Zhou, Circular law, Extreme singular values and potential\ntheory, preprint.\n[16] L. A Pastur, On the spectrum of random matrices, Teoret. Mat. Fiz. 10,\n102-112 (1973).\n[17] M. Rudelson, Invertibility of random matrices: Norm of the inverse. Annals of\nMathematics, to appear.\n[18] M. Rudelson and R. Vershynin, The Littlewood-Offord problem and the condition number of random matrices, Advances in Mathematics, to appear.\n[19] M. Rudelson, R. Vershynin, The smallest singular value of a rectangular random matrix, preprint.\n\n\fUNIVERSALITY OF ESDS AND THE CIRCULAR LAW\n\n45\n\n[20] M. Rudelson, R. Vershynin, The least singular value of a random square matrix\nis O(n\u22121/2 ), preprint.\n[21] R. Speicher, survey in preparation.\n[22] T. Tao and V. Vu, On random \u00b11 matrices: Singularity and Determinant,\nRandom Structures Algorithms 28 (2006), no. 1, 1\u201323.\n[23] T. Tao, V. Vu, Additive combinatorics, Cambridge University Press, 2006.\n[24] T. Tao and V. Vu, Inverse Littlewood-Offord theorems and the condition number of random discrete matrices, Annals of Mathematics, to appear.\n[25] T. Tao and V. Vu, The condition number of a randomly perturbed matrix,\nSTOC 2007.\n[26] T. Tao and V. Vu, Random Matrices: The circular Law, Communications in\nContemporary Mathematics, 10 (2008), 261\u2013307.\n[27] T. Tao and V. Vu, Random matrices: A general approach for the least singular\nvalue problem, preprint.\n[28] P. Wigner, On the distribution of the roots of certain symmetric matrices, The\nAnnals of Mathematics 67 (1958) 325-327.\n\nDepartment of Mathematics, UCLA, Los Angeles CA 90095-1555\nE-mail address: tao@@math.ucla.edu\n\nDepartment of Mathematics, Rutgers University, Piscataway NJ 088548019\nE-mail address: vanvu@@math.rutgers.edu\n\nDepartment of Mathematics, U. Toronto, Toronto Canada, MS5 2E4\nE-mail address: manju@@math.toronto.edu\n\n\f"}