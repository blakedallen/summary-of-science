{"id": "http://arxiv.org/abs/cond-mat/0210415v1", "guidislink": true, "updated": "2002-10-18T16:15:47Z", "updated_parsed": [2002, 10, 18, 16, 15, 47, 4, 291, 0], "published": "2002-10-18T16:15:47Z", "published_parsed": [2002, 10, 18, 16, 15, 47, 4, 291, 0], "title": "Information transfer through disordered media by diffuse waves", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0210690%2Ccond-mat%2F0210110%2Ccond-mat%2F0210251%2Ccond-mat%2F0210658%2Ccond-mat%2F0210394%2Ccond-mat%2F0210193%2Ccond-mat%2F0210389%2Ccond-mat%2F0210498%2Ccond-mat%2F0210703%2Ccond-mat%2F0210584%2Ccond-mat%2F0210524%2Ccond-mat%2F0210097%2Ccond-mat%2F0210214%2Ccond-mat%2F0210129%2Ccond-mat%2F0210100%2Ccond-mat%2F0210200%2Ccond-mat%2F0210605%2Ccond-mat%2F0210684%2Ccond-mat%2F0210255%2Ccond-mat%2F0210286%2Ccond-mat%2F0210659%2Ccond-mat%2F0210145%2Ccond-mat%2F0210140%2Ccond-mat%2F0210369%2Ccond-mat%2F0210190%2Ccond-mat%2F0210517%2Ccond-mat%2F0210585%2Ccond-mat%2F0210714%2Ccond-mat%2F0210301%2Ccond-mat%2F0210040%2Ccond-mat%2F0210378%2Ccond-mat%2F0210386%2Ccond-mat%2F0210189%2Ccond-mat%2F0210250%2Ccond-mat%2F0210277%2Ccond-mat%2F0210090%2Ccond-mat%2F0210230%2Ccond-mat%2F0210581%2Ccond-mat%2F0210418%2Ccond-mat%2F0210105%2Ccond-mat%2F0210648%2Ccond-mat%2F0210231%2Ccond-mat%2F0210217%2Ccond-mat%2F0210407%2Ccond-mat%2F0210515%2Ccond-mat%2F0210403%2Ccond-mat%2F0210541%2Ccond-mat%2F0210618%2Ccond-mat%2F0210035%2Ccond-mat%2F0210438%2Ccond-mat%2F0210372%2Ccond-mat%2F0210535%2Ccond-mat%2F0210271%2Ccond-mat%2F0210691%2Ccond-mat%2F0210225%2Ccond-mat%2F0210052%2Ccond-mat%2F0210637%2Ccond-mat%2F0210242%2Ccond-mat%2F0210026%2Ccond-mat%2F0210074%2Ccond-mat%2F0210426%2Ccond-mat%2F0210009%2Ccond-mat%2F0210573%2Ccond-mat%2F0210487%2Ccond-mat%2F0210710%2Ccond-mat%2F0210138%2Ccond-mat%2F0210415%2Ccond-mat%2F0210233%2Ccond-mat%2F0210558%2Ccond-mat%2F0210552%2Ccond-mat%2F0210455%2Ccond-mat%2F0210429%2Ccond-mat%2F0210656%2Ccond-mat%2F0210631%2Ccond-mat%2F0210348%2Ccond-mat%2F0210589%2Ccond-mat%2F0210600%2Ccond-mat%2F0210121%2Ccond-mat%2F0210709%2Ccond-mat%2F0210653%2Ccond-mat%2F0210664%2Ccond-mat%2F0210321%2Ccond-mat%2F0210089%2Ccond-mat%2F0210173%2Ccond-mat%2F0210174%2Ccond-mat%2F0210319%2Ccond-mat%2F0210080%2Ccond-mat%2F0210491%2Ccond-mat%2F0210395%2Ccond-mat%2F0210549%2Ccond-mat%2F0210704%2Ccond-mat%2F0210354%2Ccond-mat%2F0210574%2Ccond-mat%2F0210687%2Ccond-mat%2F0210342%2Ccond-mat%2F0210081%2Ccond-mat%2F0210410%2Ccond-mat%2F0210185%2Ccond-mat%2F0210532%2Ccond-mat%2F0210178%2Ccond-mat%2F0210694&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Information transfer through disordered media by diffuse waves"}, "summary": "We consider the information content h of a scalar multiple-scattered, diffuse\nwave field $\\psi(\\vec{r})$ and the information capacity C of a communication\nchannel that employs diffuse waves to transfer the information through a\ndisordered medium. Both h and C are shown to be directly related to the\nmesoscopic correlations between the values of $\\psi(\\vec{r})$ at different\npositions $\\vec{r}$ in space, arising due to the coherent nature of the wave.\nFor the particular case of a communication channel between two identical linear\narrays of $n \\gg 1$ equally-spaced transmitters/receivers (receiver spacing a),\nwe show that the average capacity $<C> \\propto n$ and obtain explicit analytic\nexpressions for $<C>/n$ in the limit of $n \\to \\infty$ and $k \\ell \\to \\infty$,\nwhere $k= 2\\pi/ \\lambda$, $\\lambda$ is the wavelength, and $\\ell$ is the mean\nfree path. Modification of the above results in the case of finite but large n\nand $k \\ell$ is discussed as well.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0210690%2Ccond-mat%2F0210110%2Ccond-mat%2F0210251%2Ccond-mat%2F0210658%2Ccond-mat%2F0210394%2Ccond-mat%2F0210193%2Ccond-mat%2F0210389%2Ccond-mat%2F0210498%2Ccond-mat%2F0210703%2Ccond-mat%2F0210584%2Ccond-mat%2F0210524%2Ccond-mat%2F0210097%2Ccond-mat%2F0210214%2Ccond-mat%2F0210129%2Ccond-mat%2F0210100%2Ccond-mat%2F0210200%2Ccond-mat%2F0210605%2Ccond-mat%2F0210684%2Ccond-mat%2F0210255%2Ccond-mat%2F0210286%2Ccond-mat%2F0210659%2Ccond-mat%2F0210145%2Ccond-mat%2F0210140%2Ccond-mat%2F0210369%2Ccond-mat%2F0210190%2Ccond-mat%2F0210517%2Ccond-mat%2F0210585%2Ccond-mat%2F0210714%2Ccond-mat%2F0210301%2Ccond-mat%2F0210040%2Ccond-mat%2F0210378%2Ccond-mat%2F0210386%2Ccond-mat%2F0210189%2Ccond-mat%2F0210250%2Ccond-mat%2F0210277%2Ccond-mat%2F0210090%2Ccond-mat%2F0210230%2Ccond-mat%2F0210581%2Ccond-mat%2F0210418%2Ccond-mat%2F0210105%2Ccond-mat%2F0210648%2Ccond-mat%2F0210231%2Ccond-mat%2F0210217%2Ccond-mat%2F0210407%2Ccond-mat%2F0210515%2Ccond-mat%2F0210403%2Ccond-mat%2F0210541%2Ccond-mat%2F0210618%2Ccond-mat%2F0210035%2Ccond-mat%2F0210438%2Ccond-mat%2F0210372%2Ccond-mat%2F0210535%2Ccond-mat%2F0210271%2Ccond-mat%2F0210691%2Ccond-mat%2F0210225%2Ccond-mat%2F0210052%2Ccond-mat%2F0210637%2Ccond-mat%2F0210242%2Ccond-mat%2F0210026%2Ccond-mat%2F0210074%2Ccond-mat%2F0210426%2Ccond-mat%2F0210009%2Ccond-mat%2F0210573%2Ccond-mat%2F0210487%2Ccond-mat%2F0210710%2Ccond-mat%2F0210138%2Ccond-mat%2F0210415%2Ccond-mat%2F0210233%2Ccond-mat%2F0210558%2Ccond-mat%2F0210552%2Ccond-mat%2F0210455%2Ccond-mat%2F0210429%2Ccond-mat%2F0210656%2Ccond-mat%2F0210631%2Ccond-mat%2F0210348%2Ccond-mat%2F0210589%2Ccond-mat%2F0210600%2Ccond-mat%2F0210121%2Ccond-mat%2F0210709%2Ccond-mat%2F0210653%2Ccond-mat%2F0210664%2Ccond-mat%2F0210321%2Ccond-mat%2F0210089%2Ccond-mat%2F0210173%2Ccond-mat%2F0210174%2Ccond-mat%2F0210319%2Ccond-mat%2F0210080%2Ccond-mat%2F0210491%2Ccond-mat%2F0210395%2Ccond-mat%2F0210549%2Ccond-mat%2F0210704%2Ccond-mat%2F0210354%2Ccond-mat%2F0210574%2Ccond-mat%2F0210687%2Ccond-mat%2F0210342%2Ccond-mat%2F0210081%2Ccond-mat%2F0210410%2Ccond-mat%2F0210185%2Ccond-mat%2F0210532%2Ccond-mat%2F0210178%2Ccond-mat%2F0210694&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider the information content h of a scalar multiple-scattered, diffuse\nwave field $\\psi(\\vec{r})$ and the information capacity C of a communication\nchannel that employs diffuse waves to transfer the information through a\ndisordered medium. Both h and C are shown to be directly related to the\nmesoscopic correlations between the values of $\\psi(\\vec{r})$ at different\npositions $\\vec{r}$ in space, arising due to the coherent nature of the wave.\nFor the particular case of a communication channel between two identical linear\narrays of $n \\gg 1$ equally-spaced transmitters/receivers (receiver spacing a),\nwe show that the average capacity $<C> \\propto n$ and obtain explicit analytic\nexpressions for $<C>/n$ in the limit of $n \\to \\infty$ and $k \\ell \\to \\infty$,\nwhere $k= 2\\pi/ \\lambda$, $\\lambda$ is the wavelength, and $\\ell$ is the mean\nfree path. Modification of the above results in the case of finite but large n\nand $k \\ell$ is discussed as well."}, "authors": ["S. E. Skipetrov"], "author_detail": {"name": "S. E. Skipetrov"}, "author": "S. E. Skipetrov", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1103/PhysRevE.67.036621", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cond-mat/0210415v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cond-mat/0210415v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "REVTeX 4, 12 pages, 7 figures", "arxiv_primary_category": {"term": "cond-mat.dis-nn", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat.dis-nn", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.mes-hall", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.optics", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cond-mat/0210415v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cond-mat/0210415v1", "journal_reference": "Phys. Rev. E 67, 036621 (2003)", "doi": "10.1103/PhysRevE.67.036621", "fulltext": "Information transfer through disordered media by diffuse waves\nS.E. Skipetrov\u2217\n\narXiv:cond-mat/0210415v1 [cond-mat.dis-nn] 18 Oct 2002\n\nLaboratoire de Physique et Mod\u00e9lisation des Milieux Condens\u00e9s,\nCNRS, 38042 Grenoble, France\n(Dated: November 1, 2018)\nWe consider the information content h of a scalar multiple-scattered, diffuse wave field \u03c8(r) and\nthe information capacity C of a communication channel that employs diffuse waves to transfer the\ninformation through a disordered medium. Both h and C are shown to be directly related to the\nmesoscopic correlations between the values of \u03c8(r) at different positions r in space, arising due\nto the coherent nature of the wave. For the particular case of a communication channel between\ntwo identical linear arrays of n \u226b 1 equally-spaced transmitters/receivers (receiver spacing a), we\nshow that the average capacity hCi \u221d n and obtain explicit analytic expressions for hCi /n in the\nlimit of n \u2192 \u221e and kl \u2192 \u221e, where k = 2\u03c0/\u03bb, \u03bb is the wavelength, and l is the mean free path.\nModification of the above results in the case of finite but large n and kl is discussed as well. If the\nsignal to noise ratio S/N exceeds some critical value (S/N )c , hCi /n is a non-monotonic function of\na, exhibiting maxima at ka = m\u03c0 (m = 1, 2, . . .). For smaller S/N , ka = m\u03c0 correspond to local\nminima, while the absolute maximum of hCi /n is reached at some ka \u223c (S/N )1/2 < \u03c0. We define\nthe maximum average information capacity hCimax as hCi maximized over the receiver spacing a\nand the optimal normalized receiver spacing (ka)opt as the spacing maximizing hCi. Both hCimax /n\nand (ka)opt scale as (S/N )1/2 for S/N < (S/N )c , while (ka)opt = m\u03c0 and hCimax /n \u223c log(S/N )\nfor S/N > (S/N )c .\n\nI.\n\nINTRODUCTION\n\nTransport of coherent waves in disordered media have\nbeen extensively studied during the last decades [1, 2,\n3, 4, 5, 6]. Remarkably similar, diffusion behavior of\nmultiple-scattered electronic wave functions at low temperatures [1, 2, 3], coherent electromagnetic [4] (optical [5] and microwave [7]), acoustic [8], and elastic [9]\nwaves has permitted impressive advances of the field [6].\nRecently, multiple-scattered seismic waves in the Earth\ncrust have been demonstrated to behave in a similar way\n[10, 11]. In the context of these studies, the main quantities of interest are the transport coefficients of disordered\nsamples, such as, e.g., the transmission coefficient T or\nthe conductance g. The average values, fluctuations, full\nprobability distributions, angular, spatial, and temporal\ncorrelation functions of T and g have been studied both\ntheoretically and experimentally (see, e.g., Ref. 4 for a review). These quantities are, without any doubt, very important, since they describe the transport of wave energy\nthrough a disordered sample and hence can be measured\nexperimentally. On the other hand, in practical applications one rarely uses multiple-scattered waves with a primary purpose of energy transmission. Much more often,\nwaves are used to transfer the information. Readily available examples are microwave communications (portable\ntelephony) in cities, indoor wireless local-area networks in\nbuilding with complex structure, and underwater acoustic communication systems. In fact, one of the main\nmotivations to study the multiple scattering of waves in\ndisordered media is their possible use for transmission or\n\n\u2217 Sergey.Skipetrov@grenoble.cnrs.fr\n\nprocessing of information in electronic devices or wireless\ncommunications.\nIn the same way as the properties of a disordered sample with respect to transmission of energy are described\nby the transmission coefficient T or conductance g (depending on the details of the specific experiment), its\nproperties with respect to transmission of information are\ncharacterized by the information capacity C. The latter\ngives the maximum rate of error-free information transfer\nthrough a disordered sample using a given type of waves\n(acoustic, electromagnetic, etc.) and a given transmitter/receiver configuration (see Refs. 12, 13, 14 for a more\nrigorous definition of C). Information capacity of communication channels in disordered media has recently received considerable attention [15, 16, 17, 18, 19, 20, 21].\nThe most interesting and important result concerns a\ncommunication system consisting of multiple transmitters and receivers [15, 16, 17, 19, 20, 21]: it has been\nfound that the capacity of such a communication system\nscales linearly with the number of receivers n (as long\nas the number of transmitters m is of the same order).\nThe authors of Refs. 16, 19, 20, 21 have also studied\nthe effect of correlations between the signals received by\ndifferent receivers on C. More specifically, the communication channel between m transmitters and n receivers\nis described by a n \u00d7 m Green matrix G (G\u03b1i is the signal at the receiver \u03b1 due to a unit signal emitted by the\ntransmitter i). Due to the multiple scattering of waves,\nin a disordered medium the entries of G are random variables with certain correlations between them. These correlations are often termed \"mesoscopic\" [4] because they\noriginate from the fact that the phase coherence length of\nthe considered wave exceeds the length of the path that\nthe wave travels inside the disordered medium. Mesoscopic correlations complicate significantly the theoreti-\n\n\f2\ncal calculation of capacity which otherwise (i.e., for uncorrelated G\u03b1i ) is relatively straightforward [15, 19, 20].\nAlthough it follows from Refs. 16 and 20 that nonzero\ncorrelations between G\u03b1i reduce the information capacity of the communication channel under particular conditions considered in that papers, no systematic study of\nthe role of mesoscopic correlations in the context of information transfer is available at the time of writing. Mesoscopic correlations have been extensively studied during\nthe last decade (see, e.g., Refs. 2, 4, 7 and references\ntherein), but their role in the context of the information\ntransfer has been completely ignored until very recently\n[16, 19, 20]. Meanwhile, understanding the relation between mesoscopic correlations and information capacity\nhas not only the practical importance but also a fundamental significance, since the correlations are usually\naffected by the symmetries of the problem (translational\nsymmetry, time-reversal symmetry, etc.) and it might\nbe interesting to study the influence of these symmetries\non the information-theoretic quantities, such as the information capacity.\nThe purpose of the present paper is to consider the\nmultiple-scattered wave field from the point of view of\nthe information theory, to quantify its information content, and, finally, to provide a comprehensive study of\nthe information capacity of a communication channel in\na disordered medium with a proper account for mesoscopic correlations arising from the coherent nature of\nmultiple-scattered waves that carry the information. To\nbe specific, we limit our consideration to identical linear arrays of n equally-spaced transmitters/receivers (see\nFig. 1). Such a geometry is common for microwave\n[7] and acoustic [8] experiments with multiple-scattered\nwaves in disordered media [28]. Besides, it is also widely\nconsidered in connection with the time reversed acoustics\n[22] and the BLAST architecture for efficient communication over fading wireless channels [23]. We note that\nthe time-reversal techniques seems to be very promising\nfor wireless communications in disordered and/or chaotic\nenvironments [24].\nThe paper is organized as follows. We start by a study\nof the information content of the multiple-scattered wave\nfield in Sec. II. The information content of a random\nfield is quantified by its differential entropy. We show\nthat mesoscopic correlations reduce the differential entropy, reducing the information content of the multiplescattered wave field and leading, in practice, to a smaller\nvolume of computer memory required for its storage. After this somewhat introductory part of the paper, in Sec.\nIII we study the information capacity C of a communication channel between two identical linear arrays of n\ntransmitters/receivers in a disordered medium (see Fig.\n1). Under certain approximations, we obtain analytic expressions for the average information capacity hCi. The\nlatter expressions provide a reasonable estimate of hCi in\nreal experimental situations, although one can do better\nusing numerical methods. Next, we show that mesoscopic\ncorrelations reduce the information capacity (as found in\n\nFIG. 1: We consider the transfer of information between identical arrays of n transmitters/receivers placed in a disordered\nmedium. We denote the transmitter/receiver spacing by a\nand the distance between the arrays by L. A wave emitted\nby some transmitter i experiences multiple scattering in the\ndisordered medium before reaching the receiver \u03b1. The signal measured by the receiver \u03b1 due to the transmitter i is\ngiven by the Green function G\u03b1i . The Green functions G\u03b1i\n(\u03b1, i = 1, . . . , n) form a Green matrix G.\n\nRef. 16) only if the signal power is strong enough. If the\nsignal is weak, correlations play a positive role and allow\na higher capacity as compared to that in the absence of\ncorrelations. Finally, we define the maximum capacity as\na capacity maximized over the transmitter/receiver spacing and the optimal transmitter/receiver spacing as the\nspacing maximizing the capacity. Simple analytic expressions are found for these two quantities. We summarize\nthe main results of the paper in Sec. IV. Derivations\nof some important equations used in the main text but\nnot essential for its understanding are provided in the\nAppendices A and B.\nII. INFORMATION CONTENT OF THE\nMULTIPLE-SCATTERED WAVE FIELD\n\nWe start our analysis by considering a problem which\nis somewhat simpler than the one depicted in Fig. 1.\nNamely, we assume that there is only one transmitter\n(located, say, at r0 ) that emits a monochromatic scalar\nspherical wave, and analyze the information content of\nthe multiple-scattered wave field \u03c8(r). As a consequence\nof multiple scattering, \u03c8(r) is a random function of position r (\"speckle pattern\"). Since \u03c8(r) is a complex\ncontinuous random field, it contains an infinite amount\nof information or, in other words, an infinite volume of\ncomputer memory would be required to store its values at\nall r with absolute accuracy. In reality, however, one usually measures \u03c8(r) at some finite number n of positions\nr\u03b1 (\u03b1 = 1, . . . , n). In the following, we identify the information content of such a measurement with the information content of \u03c8(r), keeping in mind that this is strictly\ntrue only for n \u2192 \u221e. A measurement of \u03c8 in n points\n\n\f3\nIt also can be shown [13] that the Gaussian density function (2) maximizes the differential entropy for a given\ncovariance matrix K. Recalling that K\u03b1\u03b2 is the correlation function of the multiple-scattered wave field, we can\nreadily compute K in the ladder approximation [see the\ndiagram (b) of Fig. 2 with ri = rj = r0 or Ref. 25]:\n\u0013\n\u0012\n\u2206r\u03b1\u03b2\nsin(k\u2206r\u03b1\u03b2 )\n,\n(4)\nexp \u2212\nK\u03b1\u03b2 =\nk\u2206r\u03b1\u03b2\n2l\n\nFIG. 2: (a) The diagram contributing to the average intensity hI(r\u03b1 )i for a point source of radiation at r0 . The shaded\nrectangle is the ladder propagator, the solid lines denote the\nretarded and advanced average Green functions. (b) The\ndiagram for the correlation function of the Green functions\nG\u03b1i G\u2217\u03b2j .\n\nr\u03b1 results in a random complex vector y = {y\u03b1 }, where\n1/2\ny\u03b1 = \u03c8(r\u03b1 )/ hIi\nand we normalize the field\nE\nD \u03c8(r) by\nthe square root of the average intenisty hIi = |\u03c8(r\u03b1 )|2\n\ngiven by the diagram (a) of Fig. 2 and assumed to be\nthe same for all n receivers. From here on the angular brackets denote averaging over disorder. If p(y) is a\nprobability density function of the random vector y, the\ndifferential entropy of y is defined as [12, 13]\nZ\nh(y) = \u2212 p(y) log p(y)dn y.\n(1)\n\nThe logarithm is to base 2 and h is hence measured in\nbits. The differential entropy h is a measure of uncertainty of the random vector y and reflects the average\namount of information that one receives when some value\nof y is observed. The larger the differential entropy, the\nlarger the information content of the vector y, and we\nwill therefore use h to quantify the information content\nof y and, consequently, of the multiple-scattered wave\nfield \u03c8(r).\nUnder conditions of strong multiple scattering and provided that |r\u03b1 \u2212 r0 | \u226b l for all \u03b1 = 1, . . . , n and that\nkl \u226b 1 (where k = 2\u03c0/\u03bb, \u03bb is the wavelength, and l\nis the mean free path), the wave field \u03c8(r) can be considered Gaussian to a good accuracy, and hence y is a\ncircularly symmetric complex Gaussian random vector\ndescribed by a Gaussian probability density function\n\u0001\np(y) = det(\u03c0K)\u22121 exp \u2212y+ K \u22121 y ,\n(2)\nE\nD\nwhere K is a covariance matrix: K\u03b1\u03b2 = y\u03b1 y\u03b2\u2217 . The\nintegration in Eq. (1) can be then carried out analytically,\nyielding\nh(y) = log det(\u03c0eK).\n\n(3)\n\nwhere \u2206r\u03b1\u03b2 = |r\u03b1 \u2212 r\u03b2 |. We now restrict ourselves\nto the case of measurement points r\u03b1 arranged in a\nline with a constant distance a between r\u03b1 and r\u03b1+1 .\nAs we already mentioned in Sec. I, such a measurement geometry is common for both microwave [7] and\nacoustic [8, 22] experiments. The covariance matrix\nK is then Toeplitz: K\u03b1\u03b2 = K\u03b1\u2212\u03b2 , where K\u03b3 =\nsin[\u03b3ka]/[\u03b3ka] exp[\u2212 |\u03b3| ka/(2l)], and hence in the limit\nn \u2192 \u221e the density of its eigenvalues tends to a limit,\nwhich is the spectrum of y. The differential entropy rate\ncan be then expressed through the power spectral density\nof y [26]\nf (\u03bc) =\n\n\u221e\nX\n\nK\u03b3 exp(i\u03b3\u03bc).\n\n(5)\n\n\u03b3=\u2212\u221e\n\nThe resulting h(y) appears to scale linearly with n and\nhence it is convenient to consider the differential entropy\nrate or the differential entropy per receiver\nZ 2\u03c0\nh(y)\n1\nH(y) = lim\n= log(\u03c0e) +\nlog f (\u03bc)d\u03bc. (6)\nn\u2192\u221e n\n2\u03c0 0\nEquation (4) allows us to compute f (\u03bc) explicitly:\n\u001a\nsin(ka + \u03bc)\n1\narctan\nf (\u03bc) = 1 +\nka\nexp [ka/(2kl)] \u2212 cos(ka + \u03bc)\n\u001b\nsin(ka \u2212 \u03bc)\n.\n(7)\n+ arctan\nexp [ka/(2kl)] \u2212 cos(ka \u2212 \u03bc)\nThis expression simplifies greatly in the limit kl \u2192 \u221e:\nf (\u03bc) =\n\n\u03c0\n(m+ + m\u2212 + 1) ,\nka\n\n(8)\n\nwhere m\u00b1 denotes the greatest integer not larger than\n(ka \u00b1 \u03bc)/(2\u03c0). We show the differential entropy rate following from Eqs. (6\u20138) in Fig. 3 by solid lines for kl = 10,\n100, and kl \u2192 \u221e. In order to demonstrate the convergence of the exact formula (3) to the asymptotic Eq.\n(6), we also show the curves following from Eq. (3) for\nn = 50 (dashed lines) and the same values of kl. It is\nworthwhile to note that the entropy rate H reaches its\nmaximum value log(\u03c0e) at ka = m\u03c0 (where m is a positive integer) and that for ka > \u03c0 the variations of H\nwith ka are extremely weak. Exact analytic result for\nH can be obtained in the limit kl \u2192 \u221e by substituting f (\u03bc) given by Eq. (8) into Eq. (6) and performing\nthe integration. We find H \u2192 \u2212\u221e for ka < \u03c0 and\n\n\f4\ncontained in the signal measured by a given receiver and\nis a direct consequence of the fact that in the absence of\nexponential damping of correlation in Eq. (4), the correlation range is infinite and the decrease of h(y) with n is\nfaster than linear.\n\nIII. INFORMATION CAPACITY OF A\nCOMMUNICATION CHANNEL IN A\nDISORDERED MEDIUM\nA.\n\nFIG. 3: Differential entropy rate H as a function of normalized receiver spacing for the multiple-scattered wave field measured by a linear array of n \u2192 \u221e (solid lines) and n = 50\n(dashed lines) receivers at kl = 10, 100, and kl \u2192 \u221e.\n\nH = ka/\u03c0 + log[(\u03c0e/2)(\u03c0/ka)] for \u03c0 < ka < 2\u03c0. At\nka > 2\u03c0, H shows only weak deviations from its maximum value log(\u03c0e).\nLet us now discuss the implication of Fig. 3 for experimental measurements. In an experiment, the values of y\u03b1\ncannot be measured with an absolute accuracy. A m-bit\nquantization of Rey\u03b1 and Imy\u03b1 with a quantization step\n\u2206 \u223c 2\u2212m is a common procedure [29] and the measured\nquantized y\u03b1\u2032 can take \u223c \u2206\u22122 discrete values. The vector y \u2032 = {y\u03b1\u2032 } can therefore take \u223c \u2206\u22122n different values.\nOne can show that the entropy H(y \u2032 ) of y \u2032 is approximately h(y) \u2212 2n log \u2206 for \u2206 \u2192 0. The number of bits\nrequired on average to describe a given component y\u03b1\u2032 of\nthe random vector y \u2032 is then H(y \u2032 )/n \u223c H(y) \u2212 2 log \u2206.\nIt can be now easily seen that the smaller differential entropy rate H(y) means that less bits will be required in an\nexperiment to record all the relevant information about\nthe multiple-scattered wave field. As follows from Fig.\n3, at large receiver spacing a (ka > \u03c0), H is very close\nto its maximum value. At such a large receiver spacing, the signals measured by different receivers are only\nweakly correlated and hence each signal contains a relatively large amount of information. One therefore needs\na relatively large number of bits per receiver H(y \u2032 )/n\nto record the speckle pattern \u03c8(r) in this case. In contrast, at small a (ka < \u03c0), H decreases, since the signals measured by different receivers become significantly\ncorrelated and hence the information contained in each\nof the signals on average, H(y \u2032 )/n, is smaller than at\nka > \u03c0. If kl or n is finite, the value of H at ka < \u03c0 can\nbe small but remains finite. If, in contrast, we take the\nlimit of kl \u2192 \u221e and n \u2192 \u221e, H \u2192 \u2212\u221e at ka < \u03c0. This\nmeans that an infinitely small amount of information is\n\nGeneral definitions\n\nAfter having considered the information content of the\nmultiple-scattered wave field, we are now in a position\nto analyze the central problem of the present paper. We\nconsider a communication channel between two identical\nlinear arrays of n equally-spaced transmitters/receivers\nshown in Fig. 1. The vector of emitted signals x and the\nvector of received signals y are related by y = Gx + z,\nwhere G is a n \u00d7 n complex Green matrix (G\u03b1i gives\nthe signal measured at the receiver \u03b1 due to a unit signal\nemitted by the transmitter i), and z is a noise vector. We\nconsider scalar waves and assume that the noises z\u03b1 at\ndifferent receivers are statistically independent,Dnormally\nE\ndistributed random variables with power N : z\u03b1 z\u03b2\u2217 =\nN \u03b4\u03b1\u03b2 . Before defining the information capacity of such\na communication channel, we first remind the definitions\nof the conditional differential entropy [13]\nZ Z\nh(y|x) = \u2212\np(x, y) log p(y|x)dn x dn y,\n(9)\nand mutual information between two random vectors x\nand y [13]:\nI(x, y) = h(y) \u2212 h(y|x),\n\n(10)\n\nwhere p(x, y) and p(y|x) are the joint and conditional\nprobability density functions of x and y, respectively. In\nour case, y = Gx + z, and one finds h(y|x) = h(z) and\n\u0014\n\u0015\n1\nI(x, y) = log det In + G+ QG .\n(11)\nN\nHere In is the n \u00d7 n unit matrix and we assume that x is\na circularly symmetric complex Gaussian\nrandom\nvector\n\u0003\n\u0002\nwith a covariance matrix Q: Qij = E xi x\u2217j , where E[. . .]\ndenotes the averaging over all possible emitted signals\nx and should be contrasted from the disorder averaging\nthat we denote by the angular brackets.\nIf we assume that the Green matrix G is known at\nboth transmitters and receivers, the Shannon information capacity C (or simply capacity for brevity) of the\ncommunication channel is found by maximizing the mutual information (10) over all possible distributions p(x)\nof emitted signals x. The fundamental importance of\nC is that it gives the largest information transfer rate\n\n\f5\nR that can be realized for a given information channel in principle with infinitely small probability of error\n[12, 13]. Although no general procedure exists to realize R = C in practice, in many real situations one can\nachieve information transfer rates that are quite close to\nC [30]. Since the Gaussian distribution maximizes the\ndifferential entropy h (and hence the mutual information\nI) at a given Q (see Sec. II and Ref. 13), maximizing\nEq. (10) over p(x) amounts to maximize Eq. (11) over\nQ. In practice, however, G is often known only at the\nreceivers, but not at the transmitters, where only statistical information about G is available. In this case the\noptimal matrix Q is that maximizing the average mutual information hI(x, y)i. To accomplish the averaging, we need to specify the statistical properties of the\nGreen matrix G. In the considered case of strong multiple scattering, provided that the distance L between the\narrays of transmitters and receivers is much larger than\nthe mean free path l and that kl \u226b 1, G\u03b1i is a circularly\nsymmetric complex Gaussian random variable with zero\nmean\nandEcovariance given by the diagram (b) of Fig. 2:\nD\nG\u03b1i G\u2217\u03b2j\n\n= hIi K\u03b1\u03b2 Kij , where K\u03b1\u03b2 is defined in Eq.\nE\nD\n2\nis assumed to be independent\n(4) and hIi = |G\u03b1i |\nof \u03b1 and i. In the following we adopt the total emitted\npower constraint TrQ \u2264 n and introduce a normalized\nGreen matrix G = G/(hIi n)1/2 . In the present paper\nwe will only be interested in the average capacity hCi,\nalthough it should be kept in mind that C exhibits random fluctuations as disorder is varied. As follows from\nthe above reasoning, the average capacity is\n\u0014\n\u001c\n\u0015\u001d\nS +\nhCi = max log det In + G QG ,\nQ\nN\n\n(12)\n\nwhere S/N plays the role of the signal to noise ratio\nand S = n hIi is the average power received by each receiver assuming independent signals from transmitters.\nWhen Eq. (12) is applied to a real situation, it gives the\nmaximum amount of information that can be transferred\nthrough the considered communication channel per second using a unit frequency bandwidth [12, 13]. The units\nof hCi are therefore bits per second per Hertz (or bps/Hz\nfor brevity).\n\nB.\n\nAverage capacity at n \u226b 1\n\nFor a small number of transmitters/receivers n \u223c 1,\nthe averaging and maximization over Q in Eq. (12) can\nbe carried out by a numerical simulation (see, e.g., Ref.\n20 for n = 2 and Ref. 21 for n = 2 and 3). At large n\nsuch an approach becomes inadequate. It appears, however, that analytic methods can be applied to estimate\nthe asymptotic behavior of capacity for n \u226b 1 (see Appendix A and Refs. 16, 19, 27). In the limit of large n\n\nthe capacity per receiver is given by\nn\n\nh\ni\n1X\nhCi\nlog (S/N )\u22121/2 + \u03bai qi u\n=\nn\nn i=1\nn\n\nh\ni\n1X\nlog (S/N )\u22121/2 + \u03bai v\nn i=1\n\n+\n\n\u2212 uv/ ln 2 + log(S/N ),\n\n(13)\n\nwhere \u03bai are the eigenvalues of the matrix K, while the\nauxiliary variables u, v, and p \u2264 n nonzero eigenvalues\nqi of the matrix Q are solutions of the following system\nof equations (see Appendix A for derivations):\nn\n\nu =\n\n\u03bai\n1X\n,\nn i=1 (S/N )\u22121/2 + \u03bai v\n\n(14)\n\nn\n\n\u03bai qi\n1X\n,\nn i=1 (S/N )\u22121/2 + \u03bai qi u\n\u03bai u\n, i = 1, . . . , p,\n\u03c6 =\n(S/N )\u22121/2 + \u03bai qi u\np\nX\nqi .\nn =\nv =\n\n(15)\n(16)\n(17)\n\ni=1\n\nIn principle, the above equations are sufficient for the calculation of the average capacity at given n \u226b 1, S/N , and\nka. It should be noted, however, that the total number of\nequations is p + 3 (with p that can be as large as n), and\nthat the equations are nonlinear. Hence, the numerical\nsolution requires considerable computational resources at\nlarge n. Besides, the interpretation of numerical results\nis known to be a rather difficult task. Below we show that\nin the limit of n \u2192 \u221e, Eqs. (13)\u2013(17) can be significantly\nsimplified and even that simple analytic expressions for\nhCi can be obtained in certain cases.\nC.\n\nAverage capacity at n \u2192 \u221e\n\nAs we already mentioned in Sec. II, the covariance matrix K is Toeplitz and hence for n \u2192 \u221e one can use the\nlimit theorems known for this class of matrices [26]. Of\nparticular use for us is the fundamental eigenvalue distribution theorem of Szeg\u00f6 that states that if K is a n \u00d7 n\nHermitian Toeplitz matrix and F (x) is some continuous\nfunction then (under certain conditions fulfilled in our\ncase)\nn\n\n1X\n1\nF (\u03bai ) =\nn\u2192\u221e n\n2\u03c0\ni=1\nlim\n\nZ\n\n2\u03c0\n\nF [f (\u03bc)] d\u03bc,\n\n(18)\n\n0\n\nwhere f (\u03bc) is the power spectral density defined in Eq.\n(5). We now admit that the right-hand sides of Eqs.\n(13)\u2013(15) can be readily simplified using Eq. (18). After\nsome algebra this leads to the following set of equations\n\n\f6\nfor the auxiliary variables u and v:\n#\u22121\n\"\u0012 \u0013\nZ 2\u03c0\n\u22121/2\n1\nS\nu =\n+ vf (\u03bc)\nd\u03bc, (19)\nf (\u03bc)\n2\u03c0 0\nN\nZ 2\u03c0\n1 1\nv =\nu 2\u03c0 0 f (\u03bc)>v/\u221aS/N\n#\n\"\n\u0012 \u0013\u22121/2\nv\nS\nd\u03bc,\n(20)\n1\u2212\nN\nf (\u03bc)\nwhile the expression (13) for the average capacity per\nreceiver becomes\n\u0014\n\u0015\nZ 2\u03c0\nhCi\nf (\u03bc)\n1\nlog\nd\u03bc\n=\nn\n2\u03c0 0 f (\u03bc)>v/\u221aS/N\nv\n#\n\"\u0012 \u0013\nZ 2\u03c0\n\u22121/2\nS\n1\n+ vf (\u03bc) d\u03bc\nlog\n+\n2\u03c0 0\nN\n!\n\u0012 \u0013\nZ 2\u03c0\nS\n1\n+ log\nd\u03bc\n1\u2212\nN\n4\u03c0 0 f (\u03bc)<v/\u221aS/N\nuv\n,\n(21)\n\u2212\nln 2\nwhere the integral in Eq. (20) and the first integral in Eq.\n(21) are over the part of the interval (0, 2\u03c0) where f (\u03bc) >\n(S/N )\u22121/2 v, while the last integral in Eq. (21) is over\nthe part of the same interval where f (\u03bc) < (S/N )\u22121/2 v.\nOnce f (\u03bc) is known, Eqs. (19)\u2013(21) allow one to calculate\nthe average capacity per receiver, hCi /n, in the limit\nn \u2192 \u221e.\n1.\n\nAverage capacity at n \u2192 \u221e and kl \u2192 \u221e\n\nWe first consider the limit kl \u2192 \u221e. Although particularly simple results can be obtained in this case, we\nwill show later that this limit can serve as a good approximation to real situations with large but finite kl. If\n0 < ka < \u03c0, we find (see Appendix B)\n\u0015\n\u0014\n\u03c6\nka\nS \u03c0 1\nhCi\n\u2212\n=\nlog\n,\n(22)\nn\n\u03c0\nN ka \u03c6\nln 2\nwhere\nka\n1\n\u03c6=\n\u2212\n\u03c0\n2(S/N )\n\n\u0012\n\nka\n\u03c0\n\n#\n\u00133 \"r\nS \u0010 \u03c0 \u00112\n\u2212 1 . (23)\n1+4\nN ka\n\nAnalytic results for hCi /n can also be obtained at ka >\n\u03c0, but the resulting expressions are rather cumbersome\nand lengthy and we present them in Appendix B. In Fig.\n4 we show the average capacity per receiver hCi /n, as a\nfunction of normalized receiver spacing ka/\u03c0 at a fixed\nsignal to noise ratio S/N = 100. The solid line shows our\nanalytic result, corresponding to the limit n \u2192 \u221e [Eq.\n(22) at 0 < ka < \u03c0 and more lengthy but analytic formulas given in Appendix B at ka > \u03c0], while the dashed\n\nFIG. 4: Average information capacity per receiver of a communication channel between two identical linear arrays of n\nequally-spaced transmitters/receivers as a function of normalized receiver spacing for kl \u2192 \u221e and n \u2192 \u221e (solid line),\nn = 100 (dashed line), and n = 10 (dotted line). The signal\nto noise ratio is S/N = 100. The inset is a zoom of the main\nplot.\n\nand dotted lines are obtained by a numerical solution of\nEqs. (13)\u2013(17) at finite number of receivers n (n = 100\nand 10, respectively). As follows from our analysis, at\nkl \u2192 \u221e and n \u2192 \u221e, the derivative of hCi with respect\nto ka exhibits a discontinuity at ka = m\u03c0 (m = 1, 2, . . .).\nAlthough the discontinuity disappears at finite n, the\noverall agreement between the results corresponding to\nfinite n and n \u2192 \u221e remains satisfactory even for n as\nsmall as n = 10. The analytic results corresponding to\nthe limit of n \u2192 \u221e can therefore serve as a reasonable\napproximation in real situations with large but finite n.\n\n2.\n\nAverage capacity at n \u2192 \u221e and finite kl\n\nAlthough kl is large in the experiments performed in\nthe diffusion regime [7, 8, 22, 23], its value remains finite\nand it is therefore of interest to consider its effect on the\naverage capacity. At finite kl the power spectral density f (\u03bc) is given by Eq. (7) and Eqs. (19)\u2013(21) can be\nsolved only numerically. The solution is, however, quite\nsimplified by the fact that the result corresponding to\nkl \u2192 \u221e is known analytically (see Appendix B) and can\nbe used as a good starting point for the numerical algorithm. The average capacity per receiver obtained from\nEqs. (19)\u2013(21) at kl = 100 and 10 is shown in Fig. 5\nby dashed and dotted lines, respectively. The solid line\nshows the kl \u2192 \u221e result, the same as in Fig. 4. As follows from Fig. 5, at finite kl the capacity is somewhat\nhigher than at kl \u2192 \u221e. This is explained by a lower de-\n\n\f7\n\nFIG. 5: Average information capacity per receiver of a communication channel between two identical linear arrays of n\nequally-spaced transmitters/receivers as a function of normalized receiver spacing for n \u2192 \u221e and kl \u2192 \u221e (solid line),\nkl = 100 (dashed line), and kl = 10 (dotted line). The signal\nto noise ratio is S/N = 100. The inset is a zoom of the main\nplot.\n\ngree of correlation between the entries of the Green matrix G for smaller kl [see Eq. (4)]. Also, the derivative\nof hCi with respect to ka exhibits no jumps at ka = m\u03c0\n(m = 1, 2, . . .) when kl is finite. In general, however, the\naverage capacity is only slightly affected by the finiteness\nof kl as long as kl remains much larger than unity.\n\nD.\n\nMaximum capacity and optimal receiver spacing\n\nIt follows from Figs. 4 and 5 that as long as n \u226b 1 and\nkl \u226b 1, the average capacity per receiver is very close to\nits value for n \u2192 \u221e and kl \u2192 \u221e. We therefore limit\nthe rest of this subsection to the discussion of the latter limiting case, assuming that the behavior of hCi /n\nremains similar at finite but large n and kl. The typical behavior of the average capacity per receiver shown\nin Figs. 4 and 5 can be summarized as follows: hCi /n\nhas its absolute minimum at ka \u2192 0 while it reaches its\nmaxima at ka = m\u03c0 (m = 1, 2, . . .) and becomes almost\nindependent of ka for ka > \u03c0. It appears, however, that\nsuch a behavior is typical only for large signal to noise ratios S/N > (S/N )c , where (S/N )c is some critical value\nof S/N that we define below. To study the capacity as\na function of ka at various values of S/N , we plot the\ncapacity normalized to its value at ka = m\u03c0 in Fig. 6.\nAs follows from Fig. 6, at large S/N (S/N = 10 and\nS/N \u2192 \u221e) the behavior of capacity with ka is similar\nto that shown in Figs. 4 and 5. Interestingly, in the\nlimit S/N \u2192 \u221e we find a very simple result: hCi /n =\n\nFIG. 6: Average capacity of a communication channel between two identical linear arrays of n transmitters/receivers,\nnormalized to its value at ka = m\u03c0, is shown as a function of\nthe normalized receiver spacing for five different values of the\nsignal to noise ratio S/N . We assume kl \u2192 \u221e and n \u2192 \u221e\nfor this plot.\n\n(ka/\u03c0) log(S/N ) at ka < \u03c0 and hCi /n = log(S/N ) at\nka > \u03c0, i.e. the capacity grows linearly with ka for ka <\n\u03c0 and then remains constant for ka > \u03c0. At finite but\nlarge S/N the behavior of capacity is less simple but is\nqualitatively very similar: hCi /n first shows a monotonic\nincrease with ka for ka < \u03c0 and then oscillates weakly\nwith ka for ka > \u03c0 (see also Figs. 4 and 5). As we\nnoted above, such a behavior is typical only for S/N >\n(S/N )c . At smaller values of the signal to noise ratio,\nhCi /n exhibits a non-monotonic behavior with ka for\nka < \u03c0. More precisely, it reaches a maximum at some\nka < \u03c0, as can be seen from Fig. 6 for S/N = 0.05, 0.1,\nand 1. We call the value of ka maximizing the average\ninformation capacity optimal and denote it by (ka)opt .\nIn addition, we define the maximum capacity hCimax as\nthe capacity maximized over ka:\nhCimax\n\n\u001c\n\n\u0014\n\nS\n= max max log det In + G + QG\nQ\nka\nN\n\n\u0015\u001d\n\n. (24)\n\nIt follows from Eq. (22) that (ka)opt and hCimax depend\non the signal to noise ratio S/N in a very simple way:\n\nhCimax\nn\n\n\u001a\n\np\nA1 S/N , S/N < (S/N )c ,\n(25)\nm\u03c0,\nS/N > (S/N )c ,\n\uf8f1 p\n\uf8f2 A2 S/N ,\nS/N < (S/N )c ,\n=\nlog [(S/N )/\u03c6max ] \u2212 \u03c6max / ln 2, (26)\n\uf8f3\nS/N > (S/N )c ,\n\n(ka)opt =\n\nwhere (S/N )c \u2243 3.35, A1 \u2243 1.72 and A2 \u2243 0.92 are\n\n\f8\n\nFIG. 7: Normalized optimal receiver spacing, maximizing the\ninformation capacity, as a function of the signal to noise ratio. The inset shows the maximum average information capacity per receiver hCimax /n. The dashed line is hCi /n\nat ka = m\u03c0 (m = 1, 2, . . .), coinciding with hCimax /n\nfor S/N > (S/N )c \u2243 3.35 but smaller than the latter for\nS/N < (S/N )c . The vertical dotted line is S/N = (S/N )c .\n\nnumerical constants, and\n\u03c6max = 1 \u2212\n\n\u0011\n1 \u0010p\n1 + 4S/N \u2212 1 .\n2S/N\n\n(27)\n\nThe normalized optimal receiver spacing (ka)opt is\nshown in Fig. 7, while the maximum capacity hCimax\nis shown, in the inset. For comparison, we also show the\ncapacity corresponding to ka = m\u03c0 (the dashed line in\nthe inset of Fig. 7) which coincides with the maximum\ncapacity for S/N > (S/N )c but is smaller than the latter if S/N < (S/N )c . It is interesting to note that at\nka = m\u03c0 the average capacity scales linearly with S/N\nfor S/N \u2192 0, while the average capacity maximized over\nka is proportional to the square root of S/N in the same\nlimit. Hence, the latter can exceed the former significantly for small signal to noise ratios.\nQualitatively different behavior of the average capacity at small and large signal to noise ratios, illustrated\nin Figs. 6 and 7, can be understood without any lengthy\ncalculations. We first remind that at ka < \u03c0, according to Eq. (4), smaller ka means stronger correlation\nbetween the Green functions G\u03b1i . Next, we map the\nquite complicated communication channel shown in Fig.\n1 onto an equivalent set of n \u2032 independent communication channels, the capacity of each equivalent channel\nbeing \u223c log(1 + S \u2032 /N \u2032 ), where S \u2032 and N \u2032 denote the\nsignal and noise powers in each of n \u2032 independent channels. Obviously, the number n \u2032 of equivalent independent\nchannels grows with decreasing the correlations between\nthe entries of the Green matrix G. Consequently, since\n\nthe total capacity is n \u2032 log(1+S \u2032 /N \u2032 ), it seems that having zero correlations (and hence ka = m\u03c0) should always\nmaximize the capacity because it ensures the largest n \u2032 .\nThis reasoning, however, is not correct, since the signal\npower S \u2032 is also sensitive to the correlations between the\nentries of the Green matrix G. Indeed, partially correlated Green functions G\u03b1i lead to a constructive interference of scattered waves at the receivers, thus increasing\nthe power of the received signal, while the noise power\nN \u2032 remains unchanged. Therefore, when changing the\nreceiver spacing ka from 0 to \u03c0, one gradually switches\nfrom a small number of equivalent independent channels\nn \u2032 with relatively large signal to noise ratio S \u2032 /N \u2032 to\na larger number of equivalent independent channels n \u2032\nwith weaker S \u2032 /N \u2032 . If S \u2032 /N \u2032 is large (which is only possible if S/N is large), the capacity of each independent\nchannel log(1 + S \u2032 /N \u2032 ) \u2248 log(S \u2032 /N \u2032 ) depends on S \u2032 /N \u2032\nonly logarithmically, while the dependence of the total\ncapacity on n \u2032 is linear. To achieve the maximum capac\u2032\nity one therefore needs to choose the largest n \u2032 = nmax\n(which corresponds to ka = m\u03c0), the decrease of capacity of each equivalent channel being negligible due to its\nweak (logarithmic) dependence on the signal to noise ratio. In contrast, if S \u2032 /N \u2032 is small (which corresponds\nto small S/N ), the capacity of each equivalent channel\nlog(1 + S \u2032 /N \u2032 ) \u2248 S \u2032 /N \u2032 depends linearly on the signal to noise ratio. In this case, the maximum capacity\nis achieved by choosing some optimal number of inde\u2032\npendent equivalent channels n \u2032 < nmax\n(and hence some\noptimal value of ka < \u03c0), which is large enough, but less\n\u2032\nthan nmax\nto ensure a reasonable value of S \u2032 /N \u2032 in each\n\u2032\nof n equivalent channels. This explains the origin of the\noptimal receiver spacing and its behavior shown in Fig.\n7.\n\nIV.\n\nCONCLUSION\n\nIn the present paper we study the information content\nof coherent multiple-scattered wave fields in disordered\nmedia and the capacity of multiple-scattered waves to\ntransfer the information through a disordered medium.\nWe show that the information-theoretic quantities, such\nas the differential entropy h of the multiple-scattered\nwave field and the information capacity C of a communication channel in a disordered medium, are directly\nrelated to the mesoscopic correlations between the scattered waves. Mesoscopic correlations reduce the information content of the coherent, multiple-scattered wave\nfield in a disordered medium, leading to a smaller number of bits required to store all the relevant information about it. To consider the transfer of information\nby multiple-scattered waves, we limit ourselves to the\ncase of communication between two identical linear arrays of n equally-spaced transmitters/receivers (receiver\nspacing a). The average information capacity hCi of such\na communication channel is shown to scale linearly with\nn, analytic expressions for hCi /n are obtained in the\n\n\f9\nlimit n \u2192 \u221e and kl \u2192 \u221e. For finite but large values\nof n and kl the capacity per receiver hCi /n is somewhat\ngreater than at n \u2192 \u221e, kl \u2192 \u221e, but the latter limiting case proves to be a fairly good approximation as\nlong as n \u226b 1 and kl \u226b 1. Our analysis shows that if\nthe signal to noise ratio S/N exceeds the critical value\n(S/N )c \u2243 3.35, hCi /n grows monotonically with a as\nlong as ka < \u03c0 and then oscillates slightly below its\nmaximum value achieved at ka = m\u03c0 (m = 1, 2, . . .). If\nS/N < (S/N )c , the behavior of the average capacity per\nreceiver hCi /n is not monotonic for 0 < ka < \u03c0 and an\nabsolute maximum of hCi /n is reached at some ka < \u03c0.\nWe define the maximum average capacity hCimax as the\naverage capacity maximized over the receiver spacing\na and the normalized optimal receiver spacing (ka)opt\nas the spacing maximizing the average capacity. Both\nhCimax /n and (ka)opt are proportional to (S/N )1/2 for\nS/N < (S/N )c . At S/N > (S/N )c , we find (ka)opt = m\u03c0\nand hCimax /n \u221d log(S/N ).\nAcknowledgments\n\nand integration on the right-hand side of the equation:\nZ\nF (\u03b3) = (S/N )n\u03b3 d\u03bc(X, Y)\n\"\n\u03b3\nX\n\u0001\n1\n+\nX+\n\u00d7 exp \u2212 (S/N )\u22121/2\nm Xm + Ym Ym\n2\nm=1\n\uf8f9\n\u03b3\n\u0001\n1 X\n+\n\uf8fb,\n\u2212\nYm\nAYl X+\n(A2)\nl BXm\n4n\nm,l=1\n\nD\n\nE\n\u2032\n\u2032\u2217\nwhere G\u03b1i\nG\u03b2j\n= (1/n)A\u03b1\u03b2 Bij . The last term in Eq.\n(A2) precludes the direct integration over X and Y. We\ncan, however, reduce it to a sum of quadratic terms by\ndefining \u03b3 \u00d7\u03b3 complex matrices U and V and introducing\nintegrations along appropriate contours in the complex\nplain:\nZ\nF (\u03b3) = (S/N )n\u03b3 d\u03bc(X, Y)d\u03bc(U )d\u03bc(V )\n\u0012\n\u0013\n1\n\u00d7 exp \u2212 S ,\n(A3)\n2\n\nwhere\n\nThe author is grateful to R. Maynard and B.A. van\nTiggelen for numerous discussions. A.M. Sengupta is acknowledged for useful comments on the preprint [20].\n\nS = (S/N )\u22121/2\n+\n\n\u03b3\nX\n\n\u03b3\nX\n\n+\nX+\nm Xm + Ym Ym\n\nm=1\n\n\u0001\n\n+\nYm\nAYl Uml + Vml X+\nm BXl\n\nm,l=1\n\nAPPENDIX A: AVERAGE CAPACITY AT n \u226b 1\n\nIn this Appendix we follow Refs. 16 and 19 to calculate the average capacity hCi in the large-n limit.\nThe idea of the calculation stems from the fact that\nthe moment generating function of the random variable C = ln det[In + (S/N )G + QG], F (\u03b3) = hexp(\u2212\u03b3C)i,\nwrites F (\u03b3) \u2243 exp(\u2212\u03b3 hCi) in the limit of \u03b3 \u2192 0+ . We\nstart therefore by a calculation of F (\u03b3), keeping in mind\nthat taking the limit \u03b3 \u2192 0+ will allow us to obtain\nthe average\nD capacity hCi = hCi / lnE2. We admit that\n+\n\nF (\u03b3) = [det(In + (S/N )G QG)]\n\n\u2212\u03b3\n\n, and that for inte-\n\nger \u03b3 we can represent [det(In + (S/N )G + QG)]\n\n\u2212\u03b3\n\nas\n\nZ\n\u0003\u2212\u03b3\nn\u03b3\ndet(In + (S/N )G QG)\n= (S/N )\nd\u03bc(X, Y)\n\"\n\u03b3\nX\n\u0001\n1\n+\nX+\n\u00d7 exp \u2212 (S/N )\u22121/2\nm Xm + Ym Ym\n2\nm=1\n#\n\u03b3\nX\n\u0001\n1\n\u2032+\n\u2212\n(A1)\nY+ G \u2032 Xm \u2212 X+\nYm ,\nmG\n2 m=1 m\n\u0002\n\n+\n\nwhere G \u2032 = Q1/2 G and we introduce 2\u03b3 auxiliary complex\nvectors Xm and Ym (m = 1, . . . , \u03b3) - the procedure\nknown as the \"replica trick\", - and d\u03bc(X, Y) is the\nappropriate integration measure. We now average Eq.\n(A1) over disorder, interchanging the order of averaging\n\n\u2212 nUml Vlm ) .\n\n(A4)\n\nWe now perform the integrals over X and Y in Eq. (A3)\nand are left with\nZ\nF (\u03b3) = (S/N )n\u03b3 d\u03bc(U )d\u03bc(V )\nn\nh\ni\n\u00d7 exp \u2212 ln det (S/N )\u22121/2 + AU\nh\ni\no\n\u2212 ln det (S/N )\u22121/2 + BV + nTr(U V ) , (A5)\n\nwhere AU and BV are the outer products of matrices.\nIntroducing the eigenvalues \u03bei and \u03b7i of the matrices A\nand B, respectively, we can rewrite the exponent in Eq.\n(A5) as\nn n\nh\ni\nX\nln det (S/N )\u22121/2 + \u03bei U\ni=1\n\nh\nio\n+ ln det (S/N )\u22121/2 + \u03b7i V\n\u2212 nTr(U V ). (A6)\n\nIn the limit n \u2192 \u221e the integrations in Eq. (A5) can\nbe performed using the saddle point method. Assuming\nthat the replica symmetry is not broken at the saddle\npoint, we have U = uI\u03b3 and V = vI\u03b3 with I\u03b3 the \u03b3 \u00d7 \u03b3\nunit matrix. Equation (A6) then becomes\nn n h\ni\nX\nln (S/N )\u22121/2 + \u03bei u\n\u03bc\ni=1\n\nh\nio\n+ ln (S/N )\u22121/2 + \u03b7i v \u2212 \u03bcnuv.\n\n(A7)\n\n\f10\nAt the saddle point the partial derivatives of Eq. (A7)\nwith respect to u and v should be zero. This yields the\nfollowing equations for u and v:\nn\n\nu =\n\n1X\n\u03b7i\n,\n\u22121/2\nn i=1 (S/N )\n+ \u03b7i v\n\n(A8)\n\nn\n\nv =\n\n1X\n\u03bei\n.\nn i=1 (S/N )\u22121/2 + \u03bei u\n\n(A9)\n\n1.\n\nIf 0 < ka < \u03c0, the power spectral density takes a\nparticularly simple form [see Eq. (8)]: f (\u03bc) = \u03c0/(ka) for\n0 < \u03bc < ka or 2\u03c0 \u2212 ka < \u03bc < 2\u03c0, while f (\u03bc) = 0 for\nka < \u03bc < 2\u03c0 \u2212 ka. Integrations in Eqs. (19) and (20)\ncan now be easily performed, since f (\u03bc) > (S/N )\u22121/2 v\nat 0 < \u03bc < ka or 2\u03c0 \u2212 ka < \u03bc < 2\u03c0 and f (\u03bc) = 0 <\n(S/N )\u22121/2 v at ka < \u03bc < 2\u03c0 \u2212 ka. This yields\n#\u22121\n\u03c0\nu =\n+v\n,\nka\n\"\n\u0012 \u0013\u22121/2 #\nka 1\nka\nS\nv =\n1\u2212\n.\nv\n\u03c0 u\n\u03c0\nN\n\"\u0012\n\nRelaxing the condition of integer \u03b3, we take the limit \u03b3 \u2192\n0+ and put Eq. (A5) in the form F (\u03b3) \u2243 exp(\u2212\u03b3 hCi)\nwith\nhCi =\n\nn n h\ni\nX\nln (S/N )\u22121/2 + \u03bei u\ni=1\n\nh\nio\n+ ln (S/N )\u22121/2 + \u03b7i v\n\u2212 nuv + n ln(S/N ).\n\n(A10)\n\nThe variance of capacity can also be found in a similar\nway (see Ref. 19 for details), but we do not consider it\nhere. Changing variables back from G \u2032 to G we see that\nA = Q1/2 KQ1/2 and B = K yielding \u03bei = \u03bai qi and\n\u03b7i = \u03bai with \u03bai and qi the eigenvalues of the matrices\nK and Q, respectively. Eqs. (A8)\u2013(A10) then reduce to\nEqs. (13)\u2013(15) of Sec. III.\nTo maximize the average capacity hCi over the ensemble of covariance matrices Q, we consider an infinitesimal\nvariation \u03b4Q of the maximizing matrix Q and require\n\u03b4 hCi \u2264 0. This leads to Tr(\u03a6\u03b4Q) \u2264 0, where\nD \u0002\n\u0003\u22121 + E\n.\n\u03a6 = G In + (S/N )G + QG\nG\n\nS\nN\n\n\u0013\u22121/2\n\n(B1)\n(B2)\n\nWe now solve these equations with respect to u, v and\nsubstitute the solution into Eq. (21), where, again, the\nintegrations are readily performed. This gives\n\u0015\n\u0014\nhCi\n\u03c6\nka\nS \u03c0 1\n\u2212\n=\nlog\n,\n(B3)\nn\n\u03c0\nN ka \u03c6\nln 2\nwhere\n\u0012 \u00133\nka\n1\nka\n\u2212\n\u03c6 =\n\u03c0\n2(S/N ) \u03c0\n\"r\n#\nS \u0010 \u03c0 \u00112\n\u00d7\n1+4\n\u22121 ,\nN ka\n\n(B4)\n\nwhich are Eqs. (22) and (23) of Sec. III.\n\n(A11)\n\nThe allowed variations \u03b4Q of the covariance matrix\nshould keep Q + \u03b4Q positive definite and should not\nchange the total emitted power: Tr\u03b4Q = 0. If all eigenvalues of Q are positive, the same is true for Q + \u03b4Q (provided that \u03b4Q is small), and Tr(\u03a6\u03b4Q) = 0 is achieved by\n\u03a6 = \u03c6In , where \u03c6 is a scalar: Tr(\u03a6\u03b4Q) = \u03c6Tr\u03b4Q = 0.\nThis can be also shown to remain true if some eigenvalues of Q are zero [19]. It then follows from Eq. (A11)\nthat for p nonzero eigenvalues qi of the matrix Q one has\nEq. (16) of Sec. III. Finally, Eq. (17) is simply the total\nemitted power constraint TrQ = n.\n\n0 < ka < \u03c0\n\n2.\n\n\u03c0 < ka < 2\u03c0\n\nIn this case, f (\u03bc) = \u03c0/(ka) for 0 < \u03bc < 2\u03c0 \u2212 ka or\nka < \u03bc < 2\u03c0 and f (\u03bc) = 2\u03c0/(ka) for 2\u03c0 \u2212 ka < \u03bc < ka.\nWe now should distinguish two cases: (a) S/N > (S/N )1\nand (b) S/N \u2264 (S/N )1 , where\n\u0013\n\u0012 \u00132\nS\n1 ka\n=\nN 1\n4 \u03c0\n(p\n)\n(ka/\u03c0)[26 \u2212 7(ka/\u03c0)] \u2212 15\n\u00d7\n\u22121 .\n3 \u2212 ka/\u03c0\n\n\u0012\n\n(B5)\n\nIn the case (a), f (\u03bc) > (S/N )\u22121/2 v for all \u03bc \u2208 (0, 2\u03c0)\nand Eqs. (19), (20) become\nAPPENDIX B: AVERAGE CAPACITY AT n \u2192 \u221e\nAND kl \u2192 \u221e\n\nIn this Appendix we derive analytic expressions for the\naverage information capacity in the limit of n \u2192 \u221e and\nkl \u2192 \u221e. We consider separately the cases of 0 < ka <\n\u03c0, \u03c0 < ka < 2\u03c0, and 2\u03c0 < ka < 3\u03c0. At ka > 3\u03c0\ncalculations can be performed in a similar way.\n\n2 \u2212 ka/\u03c0\nv + (S/N )\u22121/2 ka/\u03c0\nka/\u03c0 \u2212 1\n+\n,\nv + (S/N )\u22121/2 ka/(2\u03c0)\n\"\n\u0012 \u0013\u22121/2 \u0012\n\u0013#\nka S\nka\n1\n1\u2212\n.\nv 3\u2212\nv =\nu\n2\u03c0 N\n\u03c0\n\nu =\n\n(B6)\n(B7)\n\n\f11\nThe values of u and v found from the two above equations\nare to be substituted into Eq. (21) that reduces to\n\u0013\n\u001a \u0014\n\u0015\u001b\n\u0012\n\u03c0\nhCi\n\u03c0\n1\nka\n+\nlog\n= 2\u2212\nn\n\u03c0\nka v(S/N )1/2\nka\n\u0014\n\u0015\u001b\n\u0013\n\u001a\n\u0012\n1\n2\u03c0\n2\u03c0\nka\n\u2212 1 log\n+\n+\n\u03c0\nka v(S/N )1/2\nka\nuv\nS\n\u2212\n+ log .\n(B8)\nln 2\nN\nIn the case (b), f (\u03bc) > (S/N )\u22121/2 v only for 2\u03c0 \u2212 ka <\n\u03bc < ka. Eq. (B6) remains the same, while Eqs. (B7) and\n(B8) become\n#\n\u0012\n\u0013\"\n\u0012 \u0013\u22121/2\n1 ka\nka\nS\nv =\n,\n(B9)\nv\n\u22121 1\u2212\nu \u03c0\nN\n2\u03c0\n(\u0012 \u0013\n\"\n#)\n\u0013\n\u0013\u22121/2\n\u0012\n\u22121/2 \u0012\nhCi\nS\nS\n\u03c0\nka\nlog\n+v\n= 2\u2212\nn\n\u03c0\nN\nN\nka\n\u0014\n\u0015\u001b\n\u0013\n\u001a\n\u0012\n1\n2\u03c0\n2\u03c0\nka\n\u2212 1 log\n+\n+\n1/2\n\u03c0\nka v(S/N )\nka\nuv\nS\n\u2212\n+ log .\n(B10)\nln 2\nN\n3.\n\n2\u03c0 < ka < 3\u03c0\n\nWe proceed as in the two previous subsections. f (\u03bc) =\n3\u03c0/(ka) for 0 < \u03bc < ka \u2212 2\u03c0 or 4\u03c0 \u2212 ka < \u03bc < 2\u03c0,\nwhile f (\u03bc) = 2\u03c0/(ka) for ka \u2212 2\u03c0 < \u03bc < 4\u03c0 \u2212 ka. If\nS/N > (S/N )2 [case (a)], where\n\u0012 \u0013\nS\nN 2\n\nf (\u03bc) > (S/N )\u22121/2 v for all \u03bc \u2208 (0, 2\u03c0) and Eqs. (19)\u2013(21)\nreduce to\n\nu =\n+\nv =\nhCi\n=\nn\n+\n\u2212\n\n3(ka/\u03c0 \u2212 2)\n3v + (S/N )\u22121/2 ka/\u03c0\n3 \u2212 ka/\u03c0\n,\n(B12)\nv + (S/N )\u22121/2 ka/(2\u03c0)\n\"\n\u0012 \u0013\u22121/2 \u0012\n\u0013#\nka S\n1\nka\n1\u2212\n, (B13)\nv 5\u2212\nu\n6\u03c0 N\n\u03c0\n\u0014\n\u0015\u001b\n\u0012\n\u0013\n\u001a\n1\nka\n3\u03c0\n3\u03c0\n\u2212 2 log\n+\n\u03c0\nka v(S/N )1/2\nka\n\u0012\n\u0013\n\u001a\n\u0014\n\u0015\u001b\nka\n2\u03c0\n1\n2\u03c0\n3\u2212\nlog\n+\n\u03c0\nka v(S/N )1/2\nka\nuv\nS\n+ log .\n(B14)\nln 2\nN\n\nIf S/N < (S/N )2 [case (b)], f (\u03bc) > (S/N )\u22121/2 v for\n0 < \u03bc < ka \u2212 2\u03c0 or 4\u03c0 \u2212 ka < \u03bc < 2\u03c0 only and we have\ninstead of Eqs. (B13) and (B14)\n\"\n\u0012 \u0013\u22121/2 \u0012\n\u0013#\nka S\nka\n2 ka\nv\n\u22121\u2212\n\u2212 1 , (B15)\nv =\nu 2\u03c0\n3\u03c0 N\n2\u03c0\n\u0014\n\u0015\u001b\n\u0012\n\u0013\n\u001a\n1\nka\n3\u03c0\n3\u03c0\nhCi\n=\n\u2212 2 log\n+\nn\n\u03c0\nka v(S/N )1/2\nka\n\u0012\n\u0014\n\u0013\n\u001a\n\u0015\u001b\nka\n2\u03c0\n\u22121/2\n\u22121/2\n+ 3\u2212\n(S/N )\n+\nlog (S/N )\nv\n\u03c0\nka\nuv\nS\n\u2212\n+ log .\n(B16)\nln 2\nN\n\n(ka/\u03c0)2 [ka/(2\u03c0) \u2212 1]\n=p\n, (B11)\n(ka/\u03c0)(32 \u2212 5ka/\u03c0) \u2212 35 + 5 \u2212 ka/\u03c0\n\n[1] Mesoscopic Phenomena in Solids, edited by B.L. Altshuler, P.A. Lee, and R.A. Webb (Elsevier, Amsterdam,\n1991).\n[2] R. Berkovits and S. Feng, Phys. Rep. 238, 135 (1994).\n[3] S. Datta, Electronic transport in mesoscopic systems\n(Cambridge University Press, Cambridge, 1995).\n[4] M.C.W. van Rossum and Th.M. Nieuwenhuizen, Rev.\nMod. Phys. 71, 313 (1999).\n[5] A. Lagendijk and B.A. van Tiggelen, Phys. Rep. 270,\n143 (1996).\n[6] Waves and Imaging through Complex Media, edited by\nP. Sebbah (Kluwer, Dordrecht, 2001).\n[7] P. Sebbah, R. Pnini, and A.Z. Genack, Phys. Rev. E 62,\n7348 (2000); P. Sebbah, B. Hu, A.Z. Genack, R. Pnini,\nand B. Shapiro, Phys. Rev. Lett. 88, 123901 (2002).\n[8] A. Derode, A. Tourin, and M. Fink, Phys. Rev. E 64,\n036605 (2001); ibid. 64, 036606 (2001).\n[9] N.P. Tr\u00e9gour\u00e8s and B.A. van Tiggelen, Phys. Rev. E 66,\n036601 (2002); Waves in Random Media 12, 21 (2002).\n\n[10] R. Hennino, N. Tr\u00e9gour\u00e8s, N.M. Shapiro, L. Margerin,\nM. Campillo, B.A. van Tiggelen, and R.L. Weaver, Phys.\nRev. Lett. 86, 3447 (2001).\n[11] N. Tr\u00e9gour\u00e8s, R. Hennino, C. Lacombe, N.M. Shapiro,\nL. Margerin, M. Campillo, and B.A. van Tiggelen, Ultrasonics 40, 269 (2002).\n[12] C.E. Shannon, Bell. Syst. Tech. J. 27, 379 & 623 (1948).\n[13] T.M. Cover and J.A. Thomas, Elements of Information\nTheory (Wiley, New York, 1991).\n[14] R.M. Gray, Entropy and Information Theory (SpringerVerlag, Berlin, 1990).\n[15] G.J. Foschini and M.J. Gans, Wireless Personal Communications 6, 311 (1998).\n[16] A.L. Moustakas, H.U. Baranger, L. Balents, A.M. Sengupta, S.H. Simon, Science 287, 287 (2000).\n[17] S.H. Simon, A.L. Moustakas, M. Stoytchev, and H. Safar,\nPhys. Today 54(9), 38 (2001).\n[18] M.R. Andrews, P.P. Mitra, and R. deCarvalho, Nature\n409, 316 (2001).\n\n\f12\n[19] A.M. Sengupta and P.P. Mitra, physics/0010081.\n[20] S.E. Skipetrov, physics/0207071.\n[21] A.L. Moustakas, S.H. Simon, and A.M. Sengupta, submitted (available at http://mars.bell-labs.com/).\n[22] M. Fink, Phys. Today 50, 34 (1997); M. Fink, D.\nCassereau, A. Derode, C. Prada, P. Roux, M. Tanter,\nJ.-L. Thomas, and F. Wu, Rep. Prog. Phys. 63, 1933\n(2000); M. Fink and J. de Rosny, Nonlinearity 15, R1\n(2002).\n[23] BLAST or \"Bell Labs Layered Space-Time\" is an\narchitecture for realizing very high data rates over\nfading wireless channels,\nsee http://www1.belllabs.com/project/blast/ for details and related references.\n[24] M.G. Heinemann, A. Lazzara, and K.B. Smith, Appl.\nPhys. Lett. 80, 694 (2002).\n[25] B. Shapiro, Phys. Rev. Lett. 57, 2168 (1986).\n[26] A. B\u00f6ttcher and B. Silbermann, Introduction to Large\nTruncated Toeplitz Matrices (Springer, New York, 1999);\n\n[27]\n[28]\n\n[29]\n\n[30]\n\nR.M. Gray, Toeplitz and Circular Matrices: A Review\n(http://www-ee.stanford.edu/\u223cgray/toeplitz.html).\nA.M. Sengupta and P.P. Mitra, Phys. Rev. E 60, 3389\n(1999).\nTo reduce the price of the experimental setup, one often uses a single mobile receiver to scan the multiple\nscattered speckle pattern. As long as the speckle pattern\nis stationary, this is equivalent to performing a simultaneous measurement with multiple receivers at different\npositions.\nOnly real quantities are, of course, measured in experiments. The signal has, however, two degrees of freedom:\nthe amplitude A = [(Rey\u03b1 )2 + (Imy\u03b1 )2 ]1/2 and the phase\n\u03c6 = arctan(Imy\u03b1 /Rey\u03b1 ). Measuring A and \u03c6 is equivalent to measuring the complex variable y\u03b1 .\nFor example, the transmission rate used in the telephony\nis about 90% of the information capacity of a typical\ntelephone line [13].\n\n\f"}