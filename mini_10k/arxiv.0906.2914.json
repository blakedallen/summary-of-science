{"id": "http://arxiv.org/abs/0906.2914v2", "guidislink": true, "updated": "2009-06-18T19:20:05Z", "updated_parsed": [2009, 6, 18, 19, 20, 5, 3, 169, 0], "published": "2009-06-16T12:33:25Z", "published_parsed": [2009, 6, 16, 12, 33, 25, 1, 167, 0], "title": "Efficient Multi-site Data Movement Using Constraint Programming for Data\n  Hungry Science", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.0457%2C0906.4596%2C0906.0260%2C0906.0875%2C0906.5444%2C0906.4580%2C0906.4573%2C0906.0587%2C0906.5093%2C0906.0305%2C0906.2528%2C0906.1667%2C0906.1997%2C0906.1865%2C0906.2426%2C0906.0626%2C0906.1562%2C0906.0336%2C0906.2301%2C0906.3621%2C0906.5394%2C0906.3846%2C0906.4907%2C0906.1232%2C0906.0066%2C0906.0236%2C0906.4823%2C0906.5028%2C0906.0037%2C0906.3489%2C0906.0189%2C0906.5304%2C0906.2499%2C0906.4148%2C0906.3394%2C0906.2914%2C0906.4477%2C0906.1677%2C0906.0439%2C0906.1163%2C0906.0573%2C0906.1294%2C0906.1572%2C0906.4840%2C0906.3574%2C0906.4528%2C0906.4922%2C0906.0560%2C0906.0495%2C0906.2881%2C0906.4287%2C0906.3333%2C0906.3423%2C0906.5074%2C0906.2925%2C0906.5486%2C0906.3086%2C0906.1199%2C0906.5155%2C0906.2682%2C0906.2106%2C0906.2888%2C0906.0477%2C0906.0528%2C0906.0395%2C0906.3255%2C0906.4642%2C0906.4270%2C0906.2603%2C0906.3950%2C0906.5021%2C0906.2173%2C0906.0941%2C0906.3337%2C0906.3450%2C0906.3377%2C0906.2009%2C0906.0358%2C0906.3750%2C0906.2351%2C0906.1656%2C0906.0760%2C0906.1917%2C0906.1805%2C0906.4128%2C0906.4680%2C0906.0235%2C0906.5173%2C0906.2474%2C0906.0563%2C0906.5612%2C0906.5314%2C0906.5004%2C0906.0538%2C0906.0002%2C0906.4811%2C0906.5101%2C0906.2575%2C0906.1076%2C0906.5184%2C0906.2576&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Efficient Multi-site Data Movement Using Constraint Programming for Data\n  Hungry Science"}, "summary": "For the past decade, HENP experiments have been heading towards a distributed\ncomputing model in an effort to concurrently process tasks over enormous data\nsets that have been increasing in size as a function of time. In order to\noptimize all available resources (geographically spread) and minimize the\nprocessing time, it is necessary to face also the question of efficient data\ntransfers and placements. A key question is whether the time penalty for moving\nthe data to the computational resources is worth the presumed gain. Onward to\nthe truly distributed task scheduling we present the technique using a\nConstraint Programming (CP) approach. The CP technique schedules data transfers\nfrom multiple resources considering all available paths of diverse\ncharacteristic (capacity, sharing and storage) having minimum user's waiting\ntime as an objective. We introduce a model for planning data transfers to a\nsingle destination (data transfer) as well as its extension for an optimal data\nset spreading strategy (data placement). Several enhancements for a solver of\nthe CP model will be shown, leading to a faster schedule computation time using\nsymmetry breaking, branch cutting, well studied principles from job-shop\nscheduling field and several heuristics. Finally, we will present the design\nand implementation of a corner-stone application aimed at moving datasets\naccording to the schedule. Results will include comparison of performance and\ntrade-off between CP techniques and a Peer-2-Peer model from simulation\nframework as well as the real case scenario taken from a practical usage of a\nCP scheduler.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.0457%2C0906.4596%2C0906.0260%2C0906.0875%2C0906.5444%2C0906.4580%2C0906.4573%2C0906.0587%2C0906.5093%2C0906.0305%2C0906.2528%2C0906.1667%2C0906.1997%2C0906.1865%2C0906.2426%2C0906.0626%2C0906.1562%2C0906.0336%2C0906.2301%2C0906.3621%2C0906.5394%2C0906.3846%2C0906.4907%2C0906.1232%2C0906.0066%2C0906.0236%2C0906.4823%2C0906.5028%2C0906.0037%2C0906.3489%2C0906.0189%2C0906.5304%2C0906.2499%2C0906.4148%2C0906.3394%2C0906.2914%2C0906.4477%2C0906.1677%2C0906.0439%2C0906.1163%2C0906.0573%2C0906.1294%2C0906.1572%2C0906.4840%2C0906.3574%2C0906.4528%2C0906.4922%2C0906.0560%2C0906.0495%2C0906.2881%2C0906.4287%2C0906.3333%2C0906.3423%2C0906.5074%2C0906.2925%2C0906.5486%2C0906.3086%2C0906.1199%2C0906.5155%2C0906.2682%2C0906.2106%2C0906.2888%2C0906.0477%2C0906.0528%2C0906.0395%2C0906.3255%2C0906.4642%2C0906.4270%2C0906.2603%2C0906.3950%2C0906.5021%2C0906.2173%2C0906.0941%2C0906.3337%2C0906.3450%2C0906.3377%2C0906.2009%2C0906.0358%2C0906.3750%2C0906.2351%2C0906.1656%2C0906.0760%2C0906.1917%2C0906.1805%2C0906.4128%2C0906.4680%2C0906.0235%2C0906.5173%2C0906.2474%2C0906.0563%2C0906.5612%2C0906.5314%2C0906.5004%2C0906.0538%2C0906.0002%2C0906.4811%2C0906.5101%2C0906.2575%2C0906.1076%2C0906.5184%2C0906.2576&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "For the past decade, HENP experiments have been heading towards a distributed\ncomputing model in an effort to concurrently process tasks over enormous data\nsets that have been increasing in size as a function of time. In order to\noptimize all available resources (geographically spread) and minimize the\nprocessing time, it is necessary to face also the question of efficient data\ntransfers and placements. A key question is whether the time penalty for moving\nthe data to the computational resources is worth the presumed gain. Onward to\nthe truly distributed task scheduling we present the technique using a\nConstraint Programming (CP) approach. The CP technique schedules data transfers\nfrom multiple resources considering all available paths of diverse\ncharacteristic (capacity, sharing and storage) having minimum user's waiting\ntime as an objective. We introduce a model for planning data transfers to a\nsingle destination (data transfer) as well as its extension for an optimal data\nset spreading strategy (data placement). Several enhancements for a solver of\nthe CP model will be shown, leading to a faster schedule computation time using\nsymmetry breaking, branch cutting, well studied principles from job-shop\nscheduling field and several heuristics. Finally, we will present the design\nand implementation of a corner-stone application aimed at moving datasets\naccording to the schedule. Results will include comparison of performance and\ntrade-off between CP techniques and a Peer-2-Peer model from simulation\nframework as well as the real case scenario taken from a practical usage of a\nCP scheduler."}, "authors": ["Michal Zerola", "J\u00e9r\u00f4me Lauret", "Roman Bart\u00e1k", "Michal \u0160umbera"], "author_detail": {"name": "Michal \u0160umbera"}, "author": "Michal \u0160umbera", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1088/1742-6596/219/6/062069", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0906.2914v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0906.2914v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "To appear in proceedings of Computing in High Energy and Nuclear\n  Physics 2009", "arxiv_primary_category": {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0906.2914v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0906.2914v2", "journal_reference": null, "doi": "10.1088/1742-6596/219/6/062069", "fulltext": "arXiv:0906.2914v2 [cs.DC] 18 Jun 2009\n\nEfficient Multi-site Data Movement Using Constraint\nProgramming for Data Hungry Science\nMichal Zerola\nNuclear Physics Institute, Academy of Sciences of the Czech Republic, Czech Republic\nE-mail: michal.zerola@ujf.cas.cz\n\nJ\u00e9r\u00f4me Lauret\nBrookhaven National Laboratory, Upton, USA\nE-mail: jlauret@bnl.gov\n\nRoman Bart\u00e1k\nFaculty of Mathematics and Physics, Charles University, Czech Republic\nE-mail: roman.bartak@mff.cuni.cz\n\nMichal \u0160umbera\nNuclear Physics Institute, Academy of Sciences of the Czech Republic, Czech Republic\nE-mail: sumbera@ujf.cas.cz\nAbstract. For the past decade, HENP experiments have been heading towards a distributed\ncomputing model in an effort to concurrently process tasks over enormous data sets that have\nbeen increasing in size as a function of time. In order to optimize all available resources\n(geographically spread) and minimize the processing time, it is necessary to face also the question\nof efficient data transfers and placements. A key question is whether the time penalty for moving\nthe data to the computational resources is worth the presumed gain. Onward to the truly\ndistributed task scheduling we present the technique using a Constraint Programming (CP)\napproach. The CP technique schedules data transfers from multiple resources considering all\navailable paths of diverse characteristic (capacity, sharing and storage) having minimum user's\nwaiting time as an objective. We introduce a model for planning data transfers to a single\ndestination (data transfer) as well as its extension for an optimal data set spreading strategy\n(data placement). Several enhancements for a solver of the CP model will be shown, leading\nto a faster schedule computation time using symmetry breaking, branch cutting, well studied\nprinciples from job-shop scheduling field and several heuristics. Finally, we will present the\ndesign and implementation of a corner-stone application aimed at moving datasets according\nto the schedule. Results will include comparison of performance and trade-off between CP\ntechniques and a Peer-2-Peer model from simulation framework as well as the real case scenario\ntaken from a practical usage of a CP scheduler.\n\n\f1. Introduction\nParamount to a distributed computing model (Grid or Cloud) is the key problem of processing\nin the most efficient manner vast amount of data in a minimum time. Since the beginning\nof the decade, High Energy and Nuclear Physics (HENP) communities have been tackling\nthis challenging problem with an ultimate focus to optimize all of their available resources\n(geographically spread) and thus minimize the processing time it takes to go over their steadily\ngrowing data sets.\nOne of such communities is the computationally challenging experiment STAR at RHIC\n(Relativistic Heavy Ion Collider [1]) located at the Brookhaven National Laboratory (USA). In\naddition to a typical Peta-byte scale storage requirements and large computational need this\nexperiment as a running experiment acquires a new set of valuable experimental data every year,\nintroducing other dimension of safe data transfer to the problem. From the yearly data sets,\nthe experiment may produce many physics-ready derived data sets which differ in accuracy as\nthe problem is better understood and as time passes.\nThe user's task is typically embarrassingly parallel; that is, a single program can run N times\non a fraction of the whole data set split into N sub-parts with usually no impact on science\nreliability, accuracy, or reproducibility. For a computer scientist, the issue then becomes how to\nsplit the embarrassingly parallel task into N jobs in the most efficient manner while knowing\nthe data set is spread over the world and/or how to spread 'a' dataset and place best the data\nfor maximal efficiency and fastest processing of the task.\nRather than trying to solve the whole complex issue including optimal data placement\nstrategy (distribution of centrally acquired data to other processing sites) with an emphasis\non efficient further processing we split the problem into several stages. In this paper we focus\non one block of this complex task which is of immediate need by the physicists: \"how to\nbring the desired dataset to a single destination in the shortest time?\" By isolating the data\ntransfer/placement and the computational challenges from each other, we get an opportunity\nto study the behavior of both sets of constraints separately. The paper summarize the work\nfrom [2] addressed to more theoretically based audience from automated planning community. In\naddition we present extensions for other real-life requirements in 4.1 and propose an architecture\nfor further implementation in 6.1.\n2. Related works\nThe needs of large-scale data intensive projects arising out of several fields such as bio-informatics\n(BIRN, BLAST), astronomy (SDSS) or HENP communities (STAR, ALICE) have been the\nbrainteasers for computer scientists for years. Whilst the cost of storage space rapidly decreases\nand computational power allows scientists to analyze more and more acquired data, appetite for\nefficiency in Data Grids becomes even more of a prominent need.\nDecoupling of job scheduling from data movement was studied by Ranganathan and Foster in\n[3]. The authors discussed combinations of replication strategies and scheduling algorithms, but\nnot considering the performance of the network. The nature of high-energy physics experiments,\nwhere data are centrally acquired, implies that replication to geographically spread sites is a\nmust in order to process data distributively. Intention to access large-scale data remotely over\nwide-area network has turned out to be highly ineffective and a cause of often poorly traceable\ntroubles.\nThe authors of [4] proposed and implemented improvements to Condor, a popular clusterbased distributed computing system. The presented data management architecture is based on\nexploiting the workflow and utilizing data dependencies between jobs through study of related\nDAGs. Since the workflow in high-energy data analysis is typically simple and embarrassingly\nparallel without dependencies between jobs these techniques don't lead to a fundamental\noptimization in this field.\n\n\fSato et al. in [5] and authors of [6] tackled the question of replica placement strategies\nvia mathematical constraints modeling an optimization problem in the Grid environment. The\nsolving approach in [5] is based on integer linear programming while [6] uses a Lagrangian\nrelaxation method [7]. The limitation of both models is a characterization of data transfers\nwhich neglects possible transfer paths and fetching data from a site in parallel via multiple links\npossibly leading to better network utilization.\nWe focus on this missing component considering wide-area network data transfers pursuing\nmore efficient data movement. An initial idea of our presented model originates from Simonis\n[8] and the proposed constraints for the traffic placement problem were expanded primarily\non link throughputs and consequently on follow-up transfer allocations in time. The solving\napproach is based on the Constraint Programming technique [9], used in artificial intelligence\nand operations research. One of the immense advantages of the constrained based approach is\na gentle augmentation of the model with additional real-life rules.\n3. Formal model\nThe input of the problem consists of two parts. The first part represents the (Grid) network and\nfile origins. The network, formally a directed weighted graph, consists of a set of nodes N (sites)\nand a set of directed edges E (links). The weight of an edge describes the number of time units\nneeded to transfer a file of one size unit. Information about files' origins is a mapping of each\nfile to a set of sites where the file is available. The second part of the input is a user request,\nnamely the set of files that need to be transferred to a common destination site. The solving\nprocess is composed of two stages:\n\u2022 a transfer path for each file, i.e., one origin and a valid path from the origin to the\ndestination, is selected (planning)\n\u2022 for each file and its selected transfer path, the particular transfers via links are scheduled\nin time such that the resulting plan has minimal makespan (scheduling)\nThe goal of the scheduling stage is to evaluate the path configuration in the sense of the\nrequired makespan. Essentially, it works as an objective function because the realization of the\nschedule will not depend on particular transfer times calculated in this phase, as we will show\nin section 6.\nBoth stages iterate until the plan of transfers with the minimal makespan is found (see Alg.\n1). As we can see, the phases are not strictly separated, while planning function takes makespan\nas an argument. This allows to prune the search space already during generating transfer paths\nusing constraint 6 explained latterly. In [2] we identified that about 90% of overall time is spent\nin the planning stage hence we put our effort to improve this stage. The following formalism\nAlgorithm 1 Pseudocode for a search procedure.\nmakespan \u2190 sup\nplan \u2190 Planner.getFirstPlan()\nwhile plan != null do\nschedule \u2190 Scheduler.getSchedule(plan, makespan) {Branch-and-Bound on makespan}\nif schedule.getMakespan() < makespan then\nmakespan \u2190 schedule.getMakespan() {better schedule found}\nend if\nPlanner.getNextPlan(makespan) {next feasible plan with cut constraint}\nend while\nis used to define a constraint model describing the planning sub-problem. The set OUT(n)\n\n\fconsists of all edges leaving node n, the set IN(n) of all edges leading to node n. Input received\nfrom a user is a set of demands D needed at the destination site dest. In our notation demands\nrepresent file requests and we will use this symbolism in the following text. For every demand\nd \u2208 D we have a set of sources orig(d) - sites where the demanded file d is already available.\nWe will present the link-based approach for modeling planning constraints. Another approach\ncalled path-based can be found in [2].\nThe essential idea of the link-based approach is using one decision {0, 1} variable Xde for\neach demand and link of the network, denoting whether demand d is routed over edge e or not.\nConstraints (1-3), ensure that if all decision variables have assigned values then the resulting\nconfiguration contains transfer paths. These constraints alone allow isolated loops along with\nthe valid paths and therefore precedence constraints (4) are used to eliminate such loops.\nX\n\n\u2200d \u2208 D :\n\nXde = 1,\n\ne\u2208\u222aOUT(n|n\u2208orig(d))\n\nX\n\n\u2200d \u2208 D :\n\nX\n\nXde\n\nXde = 0\n\nX\n\nXde = 1\n\ne\u2208IN(n)\n\n(2)\n\ne\u2208IN(dest(d))\n\n\u2200d \u2208 D, \u2200n \u2208\n/ orig(d) \u222a {dest(d)} :\nX\nX\nX\n\u2264 1,\nXde \u2264 1,\nXde =\nXde\n\ne\u2208OUT(n)\n\n(1)\n\ne\u2208\u222aIN(n|n\u2208orig(d))\n\nXde = 0,\n\ne\u2208OUT(dest(d))\n\nX\n\ne\u2208OUT(n)\n\n(3)\n\ne\u2208IN(n)\n\nPrecedence constraints (4) use non-decision positive integer variables Pde representing possible\nstart times of transfer for demand d over edge e. Let durde be the constant duration of transfer\nof d over edge e. Then constraint\nX\nX\n\u2200d \u2208 D \u2200n \u2208 N :\nXde * (Pde + durde ) \u2264\nXde * Pde\n(4)\ne\u2208IN(n)\n\ne\u2208OUT(n)\n\nensures a correct order between transfers for every demand, thus restricting loops. Unfortunately,\nconstraints (4) do not restrict the domains of Pde until the values Xde are known and therefore\nwe suggest using a redundant constraint (5) to estimate better the lower bound for each Pde .\nLet start be the start vertex of e not containing demand d (start \u2208\n/ orig(d)):\nmin\n\nf \u2208IN(start)\n\n(Pdf + durdf ) \u2264 Pde\n\n(5)\n\nVariables Pde can be used not only to break cycles but also to estimate the makespan of the\nplan. The idea is that according to the number of currently assigned demands per some link and\ntheir possible starting times, we can determine the lower bound of the makespan for the schedule\nthat will be computed later in the scheduling stage. Hence if we have some upper bound for the\nmakespan (typically obtained as the best solution from the previous iteration of planning and\nscheduling) we can restrict plans in next iterations by the following constraint:\n\u2200e \u2208 E : min(Pde ) +\nd\u2208D\n\nX\n\nXde * durde + SPe < makespan,\n\nd\u2208D\n\nwhere SPe stands for the value of the shortest path from the ending site of e to dest.\n\n(6)\n\n\fstart(Td,inc )\n\nend(Td,out )\n\nFree space\nsize(d)\n\nFigure 1. For a demand d passing a site with limited space (via the links inc and out) a new\ntask is created with the starting and ending times set according to file transfers Td,inc and Td,out .\n\n4. Search heuristics\nThe constraint model needs to be accompanied by a clever branching strategy to achieve good\nruntimes. A clever branching strategy is a key ingredient of any constraint satisfaction approach,\nespecially as the problem is NP-hard (as we have proven in [2]).\nAccording to the measurements shown in [2], the majority of time was spent in the planning\nphase, hence we proposed an improved variable selection heuristic that exploits better the actual\ntransfer times by using information from variables Pde . In particular, the heuristic, called\nMinPath, suggests to instantiate first variable Xde such that the following value is minimal:\ninf Pde + durde + SPe ,\n\n(7)\n\nwhere inf Pde means the smallest value in the current domain of Pde .\nConcerning the value selection heuristics, both variants were tested, particularly Increasing\n(assign first 0, then 1) and Decreasing (assign first 1, then 0) value iteration order.\nIn the scheduling phase two approaches were considered. First one, called SetTimes, is based\non determining Pareto-optimal trade-offs between makespan and resource peak capacity. A\ndetailed description and explanation of this approach can be found in [10]. The second one\nis a texture-based heuristic called SumHeight, using ordering tasks on unary resources. The\nimplementation used originates from [11] and supports Centroid sequencing of the most critical\nactivities.\n4.1. Additional real-life constraints\nThe constraint model, formally presented in the previous sections, covers mostly fundamental\nand essential attributes required in order to solve efficient data transfers. However, reality can\nprovide us with further restrictions we have to deal with, as far as we intend to tighten the gap\nbetween simulation and real life production.\n4.1.1. Storage space capacity. One of the facts we have suppressed during scheduling file\ntransfers via selected paths is a storage space limitation at sites. If a file is going to be transferred\nthrough a site, there must be guaranteed that from the start of a transfer over incoming link\ntill the end of a transfer over the outgoing link, enough storage space is available at the relevant\nsite. In fact, due to the current disk price, there will hardly be the limitations of holding the\nbulk of files at intermediate sites for a quite short amount of time, but the model is capable to\ndeal with it as well.\nThis restriction can be easily achieved by introducing a cumulative resource for each site with\nthe capacity equal to the free space of the site. If the demand d enters the site via the link inc\nand leaves it via the link out we create a new task assigned to this cumulative resource. The\nstart time of the task will be equal to the start time of the task Td,inc while the finish time to\nthe finish time of the task Td,out , as depicted in Fig. 1.\n\n\f1\u00d7\n\nShared\nslowdown 1\u00d7\n\n0\u00d7\n\n2\u00d7\n\n1\u00d7\n\n4\u00d7\n\n3\u00d7\n\n1\u00d7\ndummy vertex and edge\n\nFigure 2. An illustration of a dummy vertex and edge insertion into the graph. Original\nslowdown factors of links affected are reduced by 1, corresponding in this case to a limitation of\na sharing resource.\n\n4.1.2. Shared links. So far, we have assumed that all links incoming or outgoing from any site\nhave their own bandwidth (slowdown factor) that is not affected by others. Nevertheless, in\nreality this is not always feasible, since several links leading to a site usually share the same\nrouter and/or physical fiber having throughput less than the sum of their own values. Hence,\none can't use such links simultaneously at their maximum bandwidths.\nWe express this restriction by adding dummy vertices and edges to the network graph and by\nmodifying relevant slowdown factors. For a site where the restriction exists due to the shared\nfiber or router, a new dummy vertex is added together with a dummy edge, a connection with\nthe original site vertex. The slowdown factor of the added edge is set to the real limitation and\nslowdown factors of affected shared links are reduced by this amount.\n5. Comparative studies\nWe have implemented and compared performance of alternatives of the model, namely using linkbased and path-based approaches. Several combinations of heuristics were tried and in addition\na comparison with a simulated Peer-2-Peer method is shown.\nFor implementation of the solver we use Choco 1 , a Java based library for constraint\nprogramming. The Java based platform allows us an easier integration with already existing\ntools in the STAR environment.\n5.1. Peer-2-Peer (P2P) simulator\nThe P2P model is well known and successfully used in areas such as file sharing,\ntelecommunication or media streaming. P2P model doesn't allow file transfers via paths, only\nby direct connections. We implemented a P2P simulator by creating the following work-flow:\na) put an observer for each link leading from an origin to the destination; b) if an observer\ndetects the link is free, it picks up the file at his site (link starting node), initiates the transfer,\nand waits until the transfer is done. We introduced a heuristic for picking up a file as typically\ndone for P2P. The link observer picks up a file that is available at the smallest number of sites.\nIf there are more files available with the same cardinality of orig(n), it randomly picks any\nof them. After each transfer, the file record is removed from the list of possibilities over all\nsites. This process is typically resolved using distributed hash table (DHT) [12], however in our\nsimulator only simple structures were used. Finally an algorithm terminates when all files reach\nthe destination, thus no observer has any more work to do.\n5.2. Data sets\nRegarding the data input part, the realistic-like network graph consists of 5 sites, denoted as\nBNL, LBNL, MIT, KISTI, and Prague and all requested files are supposed to be transferred to\nthe Prague node. The distribution of file origins, i.e. amount of files available at one particular\n1\n\nhttp://choco.sourceforge.net\n\n\fHeuristics performance for 150 files\n150\n\nFastestLink\nMinPath\nPeer-2-Peer\n\nMakespan (units)\n\nMakespan (units)\n\nHeuristics performance for 50 files\n50\n48\n46\n44\n42\n40\n38\n36\n34\n32\n\nFastestLink\nMinPath\nPeer-2-Peer\n\n145\n140\n135\n130\n125\n120\n115\n110\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\nTime (sec)\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\nTime (sec)\n\nFigure 3. Convergence of makespan during the search process for FastestLink and MinPath.\n\nFiles\n25\n50\n100\n150\n200\n\nSolution\nFastestLink\n3.862\n26.508\n8.627\n16.52\n26.167\n\ntime\nMinPath\n1.431\n27.556\n3.176\n14.618\n14.031\n\nMakespan\nFastestLink MinPath\n14\n14\n36\n32\n73\n73\n111\n110\n146\n146\n\nP2P\n24\n40\n80\n120\n160\n\nTable 1. Comparison of heuristics with emphasis on time when the best solution was found\nand the makespan.\n\nsite, is following: 100% of files are available at a central repository at BNL, LBNL holds 60%,\nMIT 1%, and KISTI 5% of all files. Implemented demands feeder generates the requested\nnumber of demands and for each demand decides with a probability whether it is available at a\nparticular site or not, respecting the given distribution.\n5.3. Experiments\nOur experiments are designed to focus on evaluation of proposed alternatives of the model and\ndetecting the most suitable combination of search heuristics. Understanding the performance\nand limitation of the studied techniques in a simulated realistic environment is a necessary step\nprior to further software deployment. All presented experiments were performed on Intel Core2\nDuo CPU@1.6GHz with 2GB of RAM, running a Debian GNU Linux operating system.\nWe compared the performance of the FastestLink and MinPath heuristics and a Peer-2-Peer\nmodel [13] that is currently the most frequently used approach to solve the problem.\nFigure 3 shows that convergence of the new MinPath heuristic is faster than the FastestLink\nand both heuristics achieve better makespan than the P2P approach. Table 1 shows similar\ncomparison of heuristics and the P2P model including the time when the best solution was\nfound for several input instances.\nAs stated above, in reality the network characteristic is dynamic and fluctuates in time.\nHence, trying to create a plan for 1000 or more files that will take several hours to execute is\nneedless, as after the time elapsed the computed plan does not have to be valid anymore. Our\nintended approach is to work with batches of files, that gives us another benefit of implementing\nfair-share mechanism in a multi user environment as well. Particularly, the requests coming\n\n\ffrom users are queued and differ in size and priorities of users. The availability to pick demands\nfrom waiting requests into actual batch within reasonably short intervals is very convenient for\nachieving fair-shareness. The experiments give us an estimate on the number of files per batch.\n6. Schedule execution\nIn this section we will briefly discuss the applicability of our model and approach in a reallife network and protocol mechanism and explain the suggested schedule execution in such an\nenvironment. In order to achieve a full link speed between two points in a wide area network (to\nfully saturate the bandwidth) one has to understand the basic principles of the TCP/IP protocol\ncommunication. An operation of transferring a single data file itself consists of splitting the data\ncontent into packets of a given size (defined by window scaling parameter of the protocol) and\nsending them one-by-one from the source to the destination.\nSince the reception of each packet has to be acknowledged by the receiver to achieve both data\nintegrity and delivery guarantee, the time for the acknowledged packet to travel from source to\ndestination and back, so called round-trip time (RTT) plays an important role. In a wide area\nnetwork the RTT is usually significant, and to overcome such delays, current data transfer tools\nuse threads to handle several TCP streams in parallel in an attempt to smooth or minimize the\nintrinsic delays associated with TCP. Another standard approach in HENP communities is the\nexecution of several data transfers in parallel (multiple sender nodes per link) to increase the\nbandwidth usage. With this last approach, any one instance downtime would be compensated\nby other active senders transfers. Both approaches are typically combined for best results and\nit has been experimentally shown and taken as a standard assumption that a flat transfer rate\ncould be achieved across long distance.\nThe presented model in previous sections assumes a single file transfer at any time on a link\nusing unary resources. Trying to explicitly model the real network and packets behavior would\nhardly lead to any optimization (following all network peculiarities would cause the model to be\nbarely realizable). The mechanism of how the computed schedule will be executed in the real\nnetwork is following:\n\u2022 every link is supplied by one LinkManager that is responsible for transferring files over the\nlink if the link is part of their computed transfer path\n\u2022 as soon as a file becomes available, the corresponding LinkManager initiates another\ninstance of the transfer, respecting a maximum allowed simultaneous parallel instances\nSo rather than following the exact schedule, the implementation considers just the plan - the\ncomputed transfer paths, because there is no due-time limitation and executes transfers in a\ngreedy manner. However, to allow this, one has to be sure that the computed time to complete the\nschedule would not differ substantially from the real execution of the transfers. Consequently we\ndeveloped the realistic network simulator along the above facts and comparison of the makespans\nshowed results consistent with each other within a 3% margin, which is negligible. Hence\nthe experiment confirmed that presented model provides a fairly accurate estimate of the real\nmakespan.\n6.1. Architecture\nWe will briefly sketch out the proposed architecture of the automated data transfer planning\nand executing. The requests from the users are collected and recorded in a relational database.\nOne convenient approach is to use a web interface backed by a framework such as Django 2\nthat handles forms and templates and offers plugins to several common relational databases.\nThe planner, a standalone central component (the brain of the system) selects the batch of\n2\n\nhttp://www.djangoproject.com\n\n\fFigure 4. The scheme of a proposed architecture.\n\nrequested files from the database and computes the plan for it. The selection process depends\non the fair-share objectivity function and allows us to modularly implement and test various fairshare preferred factors (either from user perspective or from resource usage). The plan (transfer\npaths) are recorded back to the database indicating to Link Managers that files are available for\ntransfers. As proposed above, the link manager supplies a particular link, using the back-end\ndata mover. As soon as the file appears available at the site and is planned to be transferred via\nthe link, the data mover instance is executed, respecting the maximum simultaneously allowed\ntransfers. The status of the transfer is recorded back to the database, allowing users to see the\nprogress. The workflow is depicted in Fig. 4.\n7. Conclusions\nIn this paper we tackle the complex problem of efficient data movements on the network within a\ndistributed environment. The problem itself arises from the real-life needs of the running nuclear\nphysics experiment STAR and its peta-scale requirements for data storage and computational\npower as well. We presented the two stage constraint model, coupling path planning and transfer\nscheduling phase for data transfers to the single destination, with two alternative approaches\nfor planning transfer paths inspired by Simonis [8]. We proposed and implemented several\nsearch heuristics for both stages and performed sets of experiments with realistic data input for\nevaluating their applicability. Comparison of the results and trade-off between the schedule of a\nconstraint solver and a Peer-2-Peer simulator indicates that it is promising to continue with the\nwork, thus bringing improvements over the current techniques to the community. The execution\nof the schedule in a real environment and the architecture of the system is proposed. In the\nnearest future we want to concentrate on the integration of the solver with real data transfer\nback-ends, consequently executing tests in the real environment.\nAcknowledgments\nThe investigations have been partially supported by the IRP AVOZ 10480505, by the Grant\nAgency of the Czech Republic under Contract No. 202/07/0079 and 201/07/0205, by the grant\nLC07048 of the Ministry of Education of the Czech Republic and by the U.S. Department Of\nEnergy.\nReferences\n[1] J Adams e a 2005 Nuclear Physics A 757 102\u2013183\n[2] Zerola M, Lauret J, Bart\u00e1k R and \u0160umbera M 2009 (Submitted to ICAPS 2009)\n[3] Ranganathan K and Foster I 2002 vol 0 (Los Alamitos, CA, USA: IEEE Computer Society) pp 352\u2013258 ISSN\n1082-8907\n\n\f[4] Shankar S and DeWitt D J 2007 HPDC '07: Proceedings of the 16th international symposium on High\nperformance distributed computing (New York, NY, USA: ACM) pp 127\u2013136 ISBN 978-1-59593-673-8\n[5] Sato H, Matsuoka S, Endo T and Maruyama N 2008 GRID (IEEE) pp 250\u2013257\n[6] Rahman R M, Barker K and Alhajj R 2007 CCGRID (IEEE Computer Society) pp 171\u2013178\n[7] Fisher M L 1981 Management Science 27 1\u201318\n[8] Simonis H 2006 Handbook of Constraint Programming ed Rossi F, van Beek P and Walsh T (Elsevier) chap 25,\npp 875\u2013903\n[9] Marriott K and Stuckey P 1998 Programming with Constraints (Cambridge, Massachusetts: MIT Press)\n[10] Pape C L, Couronne P, Vergamini D and Gosselin V 1994 Proceedings of the Thirteenth Workshop of the\nU.K. Planning Special Interest Group\n[11] Beck J C, Davenport A J, Sitarski E M and Fox M S 1997 Proceedings of the Fourteenth National Conference\non Artificial Intelligence (AAAI-97) (AAAI Press) pp 241\u2013248\n[12] Naor M and Wieder U 2003 in Kaashoek and Stoica [13] pp 88\u201397\n[13] Kaashoek M F and Stoica I (eds) 2003 Peer-to-Peer Systems II, Second International Workshop, IPTPS\n2003, Berkeley, CA, USA, February 21-22,2003, Revised Papers (Lecture Notes in Computer Science vol\n2735) (Springer) ISBN 3-540-40724-3\n\n\f"}