{"id": "http://arxiv.org/abs/1202.5432v1", "guidislink": true, "updated": "2012-02-24T12:22:33Z", "updated_parsed": [2012, 2, 24, 12, 22, 33, 4, 55, 0], "published": "2012-02-24T12:22:33Z", "published_parsed": [2012, 2, 24, 12, 22, 33, 4, 55, 0], "title": "On the asymptotic behavior of the Nadaraya-Watson estimator associated\n  with the recursive SIR method", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.0945%2C1202.5016%2C1202.1997%2C1202.3545%2C1202.3499%2C1202.3519%2C1202.0248%2C1202.2177%2C1202.1491%2C1202.2733%2C1202.5605%2C1202.3032%2C1202.5113%2C1202.0929%2C1202.1708%2C1202.5206%2C1202.1546%2C1202.3967%2C1202.4706%2C1202.1957%2C1202.1130%2C1202.3031%2C1202.3250%2C1202.2496%2C1202.0955%2C1202.5041%2C1202.1479%2C1202.1792%2C1202.0745%2C1202.1339%2C1202.4299%2C1202.2263%2C1202.6037%2C1202.1294%2C1202.1217%2C1202.5737%2C1202.2305%2C1202.5891%2C1202.5595%2C1202.3055%2C1202.0108%2C1202.5492%2C1202.2491%2C1202.2409%2C1202.4658%2C1202.5961%2C1202.5205%2C1202.1291%2C1202.2336%2C1202.5701%2C1202.3538%2C1202.2112%2C1202.4527%2C1202.4753%2C1202.6140%2C1202.2819%2C1202.3612%2C1202.0612%2C1202.6415%2C1202.5245%2C1202.2859%2C1202.5623%2C1202.1350%2C1202.4360%2C1202.0354%2C1202.5283%2C1202.4432%2C1202.5148%2C1202.5966%2C1202.0291%2C1202.6532%2C1202.2015%2C1202.1665%2C1202.4952%2C1202.0203%2C1202.4423%2C1202.4524%2C1202.5072%2C1202.2995%2C1202.1337%2C1202.6525%2C1202.5068%2C1202.1592%2C1202.1785%2C1202.0948%2C1202.2101%2C1202.5954%2C1202.3701%2C1202.1635%2C1202.4647%2C1202.5311%2C1202.2629%2C1202.3943%2C1202.1945%2C1202.2280%2C1202.4914%2C1202.3251%2C1202.5432%2C1202.0348%2C1202.4544%2C1202.1241&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On the asymptotic behavior of the Nadaraya-Watson estimator associated\n  with the recursive SIR method"}, "summary": "We investigate the asymptotic behavior of the Nadaraya-Watson estimator for\nthe estimation of the regression function in a semiparametric regression model.\nOn the one hand, we make use of the recursive version of the sliced inverse\nregression method for the estimation of the unknown parameter of the model. On\nthe other hand, we implement a recursive Nadaraya-Watson procedure for the\nestimation of the regression function which takes into account the previous\nestimation of the parameter of the semiparametric regression model. We\nestablish the almost sure convergence as well as the asymptotic normality for\nour Nadaraya-Watson estimator. We also illustrate our semiparametric estimation\nprocedure on simulated data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.0945%2C1202.5016%2C1202.1997%2C1202.3545%2C1202.3499%2C1202.3519%2C1202.0248%2C1202.2177%2C1202.1491%2C1202.2733%2C1202.5605%2C1202.3032%2C1202.5113%2C1202.0929%2C1202.1708%2C1202.5206%2C1202.1546%2C1202.3967%2C1202.4706%2C1202.1957%2C1202.1130%2C1202.3031%2C1202.3250%2C1202.2496%2C1202.0955%2C1202.5041%2C1202.1479%2C1202.1792%2C1202.0745%2C1202.1339%2C1202.4299%2C1202.2263%2C1202.6037%2C1202.1294%2C1202.1217%2C1202.5737%2C1202.2305%2C1202.5891%2C1202.5595%2C1202.3055%2C1202.0108%2C1202.5492%2C1202.2491%2C1202.2409%2C1202.4658%2C1202.5961%2C1202.5205%2C1202.1291%2C1202.2336%2C1202.5701%2C1202.3538%2C1202.2112%2C1202.4527%2C1202.4753%2C1202.6140%2C1202.2819%2C1202.3612%2C1202.0612%2C1202.6415%2C1202.5245%2C1202.2859%2C1202.5623%2C1202.1350%2C1202.4360%2C1202.0354%2C1202.5283%2C1202.4432%2C1202.5148%2C1202.5966%2C1202.0291%2C1202.6532%2C1202.2015%2C1202.1665%2C1202.4952%2C1202.0203%2C1202.4423%2C1202.4524%2C1202.5072%2C1202.2995%2C1202.1337%2C1202.6525%2C1202.5068%2C1202.1592%2C1202.1785%2C1202.0948%2C1202.2101%2C1202.5954%2C1202.3701%2C1202.1635%2C1202.4647%2C1202.5311%2C1202.2629%2C1202.3943%2C1202.1945%2C1202.2280%2C1202.4914%2C1202.3251%2C1202.5432%2C1202.0348%2C1202.4544%2C1202.1241&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We investigate the asymptotic behavior of the Nadaraya-Watson estimator for\nthe estimation of the regression function in a semiparametric regression model.\nOn the one hand, we make use of the recursive version of the sliced inverse\nregression method for the estimation of the unknown parameter of the model. On\nthe other hand, we implement a recursive Nadaraya-Watson procedure for the\nestimation of the regression function which takes into account the previous\nestimation of the parameter of the semiparametric regression model. We\nestablish the almost sure convergence as well as the asymptotic normality for\nour Nadaraya-Watson estimator. We also illustrate our semiparametric estimation\nprocedure on simulated data."}, "authors": ["Bernard Bercu", "Thi Mong Ngoc Nguyen", "Jerome Saracco"], "author_detail": {"name": "Jerome Saracco"}, "author": "Jerome Saracco", "links": [{"href": "http://arxiv.org/abs/1202.5432v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.5432v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.5432v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.5432v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "ON THE ASYMPTOTIC BEHAVIOR OF THE\nNADARAYA-WATSON ESTIMATOR ASSOCIATED WITH THE\nRECURSIVE SIR METHOD\n\narXiv:1202.5432v1 [math.ST] 24 Feb 2012\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\nAbstract. We investigate the asymptotic behavior of the Nadaraya-Watson estimator for the estimation of the regression function in a semiparametric regression\nmodel. On the one hand, we make use of the recursive version of the sliced inverse\nregression method for the estimation of the unknown parameter of the model.\nOn the other hand, we implement a recursive Nadaraya-Watson procedure for the\nestimation of the regression function which takes into account the previous estimation of the parameter of the semiparametric regression model. We establish the\nalmost sure convergence as well as the asymptotic normality for our NadarayaWatson estimator. We also illustrate our semiparametric estimation procedure on\nsimulated data.\n\n1. INTRODUCTION\nThe goal of this paper is to investigate the asymptotic behavior of the NadarayaWatson estimator of the regression function f in the semiparametric regression\nmodel given, for all n \u2265 1, by\n(1.1)\n\nYn = f (\u03b80 Xn ) + \u03b5n\n\nwhere (Xn ) is a sequence of independent and identically distributed random vectors\nof Rp and the driven noise (\u03b5n ) is a real martingale difference sequence independent\nof (Xn ). We assume in all the sequel that the unknown p-dimensional parameter\n\u03b8 6= 0. On the one hand, we make use of the recursive version of the sliced inverse\nregression (SIR) method, originally proposed by Li [11] and Duan and Li [7], in order\nto estimate \u03b8. On the other hand, we estimate the unknown regression function f\nvia a recursive Nadaraya-Watson estimator which takes into account the previous\nestimation of the parameter \u03b8. Our purpose is precisely to investigate the asymptotic\nbehavior of the recursive Nadaraya-Watson estimator of f .\nOne can find a wide range of literature on nonparametric estimation of a regression\nfunction. We refer the reader to [6], [13], [19], [21] for some excellent books on density\nand regression function estimation. In the classical situation without any parameter\n\u03b8, the almost sure convergence of the Nadaraya-Watson estimator [12], [22] was\nproved by Noda [15] and its asymptotic normality was established by Schuster [18].\nMoreover, Choi, Hall and Rousson [4] propose three data-sharpening versions of the\n2000 Mathematics Subject Classification. Primary: 62H12 Secondary: 62G05, 60F05, 62L12.\nKey words and phrases. Semi-parametric regression, recursive estimation, Nadaraya-Watson\nestimator, Sliced inversion regression.\n1\n\n\f2\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\n\nNadaraya-Watson estimator in order to reduce the asymptotic variance in the central\nlimit theorem. In our situation, we propose to make use of a recursive NadarayaWatson estimator [8] of f which takes into account the previous estimation of the\nparameter \u03b8. It is given, for all x \u2208 Rp , by\nPn\nWk (x)Yk\nb\n(1.2)\nfn (x) = Pk=1\nn\nk=1 Wk (x)\nwith\n0\nXn \u0011\n1 \u0010 x \u2212 \u03b8bn\u22121\nK\nhn\nhn\nwhere the kernel K is a chosen probability density function and the bandwidth (hn )\nis a sequence of positive real numbers decreasing to zero, such that nhn tends to\ninfinity. For the sake of simplicity, we propose to make use of hn = 1/n\u03b1 with\n\u03b1 \u2208 ]0, 1[. The main difficulty arising here is that we have to deal with the recursive\nSIR estimator \u03b8bn of \u03b8 inside the kernel K.\n\nWn (x) =\n\nThe paper is organized as follows. Section 2 is devoted to the recursive SIR estimator\n\u03b8bn . Our main results on the asymptotic behavior of fbn are given in Section 3.\nUnder standard regularity assumptions on the kernel K, we establish the almost\nsure pointwise convergence of fbn together with its asymptotic normality. Section\n4 contains some numerical experiments on simulated data, illustrating the good\nperformances of our semiparametric estimation procedure. All the technical proofs\nare postponed in Appendices A and B.\n2. ON THE RECURSIVE SIR METHOD\nFrom the seminal work of Li [11] and Duan and Li [7] devoted to the SIR theory,\nwe know that the eigenvector associated with the maximum eigenvalue of the matrix\n\u03a3\u22121 \u0393 is collinear with \u03b8 where \u03a3 = V(Xn ) is positive definite, \u0393 = V(E(Xn |T (Yn ))\nand T is a slicing of the range of Yn into H non overlapping slices s1 , * * * , sH . One can\nobserve that since the link function f is unknown in the semiparametric regression\nmodel (1.1), the parameter \u03b8 is not entirely identifiable. Only its direction can be\nidentified without assuming additional constraints. Li [11] called effective dimension\nreduction (EDR), any direction collinear with \u03b8. Moreover, the SIR theory mainly\nrelies on the so-called linear condition (LC) which imposes that for all b \u2208 Rp ,\nE[b0 Xn |\u03b80 Xn ] is linear in \u03b80 Xn . It means that one can find \u03b1, \u03b2 \u2208 R such that\n(LC)\n\nE[b0 Xn |\u03b80 Xn ] = \u03b1 + \u03b2\u03b80 Xn .\n\nThis condition is required to only hold for the true parameter \u03b8. Since \u03b8 is unknown, it is not possible in practice to verify it a priori. Hence, we can assume that\n(LC) holds for all possible values of \u03b8, which is equivalent to elliptical symmetry of\nthe distribution of the identically distributed sequence (Xn ). Finally, Hall and Li\n[10] mentioned that (LC) is not a severe restriction because (LC) holds to a good\napproximation in many problems as the dimension p of the regression vector Xn\nincreases. Chen and Li [3] or Cook and Ni [5] also provide interesting discussions\non the linear condition.\n\n\fON THE ASYMPTOTIC BEHAVIOR OF THE NADARAYA-WATSON ESTIMATOR\n\n3\n\nIn order to obtain a recursive version of an EDR direction estimated with SIR\napproach, we need an analytic expression of the maximum eigenvector of \u03a3\u22121 \u0393. It\nis easily tractable when the range of Yn is divided into two non overlapping slices s1\nand s2 . Hereafter we shall assume that H = 2. In this special case, it is not hard\nto see that \u0393 = p1 z1 + p2 z2 where ph = P (Yn \u2208 sh ) and zh = E[Xn |Yn \u2208 sh ] \u2212 E[Xn ]\nwith ph 6= 0 for h = 1, 2. Moreover, it is straightforward to show that the eigenvector\nassociated to the maximum eigenvalue of \u03a3\u22121 \u0393 can be written as\n\u03b8e = \u03a3\u22121 (z1 \u2212 z2 ).\nThis vector \u03b8e is therefore an EDR direction. For the sake of simplicity, we identify\nin all the sequel the EDR direction \u03b8e with \u03b8. Our purpose is now to propose an\nestimator of the EDR direction \u03b8. First of all, let us recall the non recursive SIR\nestimator \u03b8en of \u03b8 given by Nguyen and Saracco [14]. The estimator \u03b8en can be\neasily obtained from the sample (X1 , Y1 ), . . . , (Xn , Yn ) by substituting the theoritical\nmoments by their sample couterparts. More precisely, \u03b8en is given by\n\u03b8en = \u03a3\u22121\nn (z1,n \u2212 z2,n )\n\n(2.1)\nwhere\nn\n\n(2.2)\n\n1X\n\u03a3n =\n(Xk \u2212 X n )(Xk \u2212 X n )0 ,\nn k=1\n\nn\n\n1X\nXn =\nXk\nn k=1\n\nand, for h = 1, 2, zh,n = mh,n \u2212 X n where\n(2.3)\n\nmh,n =\n\nn\n1 X\n\nnh,n\n\nk=1\n\nXk I{Yk \u2208sh } ,\n\nnh,n =\n\nn\nX\n\nI{Yk \u2208sh } .\n\nk=1\n\nNext, we focus our attention on the recursive SIR estimator \u03b8bn of \u03b8 proposed by\nBercu, Nguyen and Saracco [1], [14]. We split the sample into two parts: the\nsubsample of the first (n \u2212 1) observations (X1 , Y1 ), . . . , (Xn\u22121 , Yn\u22121 ), and the new\nobservation (Xn , Yn ). On the one hand, the inverse of the matrix \u03a3n given by (2.4)\nmay be recursively calculated via the Riccati equation [8],\n(2.4)\n\n\u03a3\u22121\nn =\n\nn\nn\n\u03a3\u22121\n\u03a3\u22121 \u03a6n \u03a60n \u03a3\u22121\nn\u22121 \u2212\nn\u22121\nn\u22121\n(n \u2212 1)(n + \u03c1n ) n\u22121\n\nwhere \u03c1n = \u03a60n \u03a3\u22121\nn\u22121 \u03a6n and \u03a6n = Xn \u2212 X n\u22121 . On the other hand, we can also obtain\nthe recursive form of zh,n . As a matter of fact, we have for h = 1, 2,\n\uf8f1\n1\n1\n\uf8f4\n\uf8f4 zh\u2217 ,n\u22121 \u2212 \u03a6n +\n\u03a6h\u2217 ,n if h = h\u2217 ,\n\uf8f2\nn\nnh\u2217 ,n\u22121 + 1\n(2.5)\nzh,n =\n\uf8f4\n1\n\uf8f4\n\uf8f3 zh,n\u22121 \u2212 \u03a6n\notherwise,\nn\n\n\f4\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\n\nwhere h\u2217 denotes the slice containing the observation Yn and \u03a6h\u2217 ,n = Xn \u2212 mh\u2217 ,n\u22121 .\nWe deduce from (2.4) and (2.5) that the recursive SIR estimator \u03b8bn is given by\n\u0012\n\u0013\nn\nn\n0 b\n\u03b8bn =\n\u03b8bn\u22121 \u2212\n\u03a3\u22121\nn\u22121 \u03a6n \u03a6n \u03b8n\u22121\nn\u22121\n(n \u2212 1)(n + \u03c1n )\n(2.6)\n\u0013\n\u0012\nh\u2217\n1\n(\u22121) n\n0 \u22121\n\u22121\n\u22121\n\u2212\n\u03a3n\u22121 \u2212\n\u03a3 \u03a6n \u03a6n \u03a3n\u22121 \u03a6h\u2217 ,n .\n(nh\u2217 ,n\u22121 + 1)(n \u2212 1)\nn + \u03c1n n\u22121\nThe SIR estimators \u03b8en and \u03b8bn share the same asymptotic properties, previously\nestablished in [14], under the following classical hypothesis.\n(H1 ) The random vectors (Xn ) are square integrable, independent and identically\ndistributed and (X1 , Y1 ), . . . , (Xn , Yn ) are independently drawn from (1.1).\nLemma 2.1. Assume that (LC ) and (H1 ) hold. Then, \u03b8bn converges a.s. to \u03b8,\n\u0012\n\u0013\nlog(log\nn)\n2\n(2.7)\n||\u03b8bn \u2212 \u03b8|| = O\na.s.\nn\nIn addition, we also have the asymptotic normality\n\u221a\nL\n(2.8)\nn(\u03b8bn \u2212 \u03b8) \u2212\u2192 N (0, \u2206)\nwhere the limiting covariance matrix \u2206 may be explicitely calculated.\n3. MAIN RESULTS\nOur purpose is to investigate the asymptotic properties of the recursive NadarayaWatson estimator fbn of the link function f given by (1.2). First of all, we assume\nthat the kernel K is a positive symmetric function, bounded with compact support,\ntwice differentiable with bounded derivatives, satisfying\nZ\nZ\nK 2 (x) dx = \u03bd 2 .\nK(x) dx = 1\nand\nR\n\nR\n\nMoreover, it is necessary to add the following standard hypothesis.\n(H2 ) The probability density function g associated with (Xn ) is continuous, positive on all Rp , twice differentiable with bounded derivatives.\n(H3 ) The link function f is Lipschitz.\nOur first result deals with the almost sure convergence of the estimator fbn .\nTheorem 3.1. Assume that (LC ) and (H1 ) to (H3 ) hold. In addition, suppose that\nthe sequence (Xn ) has a finite moment of order a > 2. Then, for any x \u2208 R, we\nhave\n(3.1)\nlim fbn (x) = f (x)\na.s.\nn\u2192\u221e\n\nMore precisely, if the bandwidth (hn ) is given by hn = 1/n\u03b1 with 0 < \u03b1 < 1/3,\nr\n\u0010\n\u0001\nlog(log n) \u0011\n(3.2)\nfbn (x) \u2212 f (x) = O n\u2212\u03b1 + O n1/a\na.s.\nn\n\n\fON THE ASYMPTOTIC BEHAVIOR OF THE NADARAYA-WATSON ESTIMATOR\n\n5\n\nwhile, if 1/3 \u2264 \u03b1 < 1,\n(3.3)\n\nfbn (x) \u2212 f (x) = O\n\n\u0010\u221a\n\n\u0011\n\n\u0010\nn\u03b1\u22121 log n + O n1/a\n\nr\n\nlog(log n) \u0011\nn\n\nProof. The proof is given Appendix A.\n\na.s.\n\u0003\n\nRemark 3.1. In the particular case where (Xn ) is a sequence of independent random\nvectors of Rp sharing the same N (m, \u03a3) distribution where the covariance matrix \u03a3\nis positive definite, we can replace n1/a by log n into (3.2) and (3.3). Consequently,\nfor any x \u2208 R, we obtain that if 0 < \u03b1 < 1/3,\n\u0001\na.s.\nfbn (x) \u2212 f (x) = O n\u2212\u03b1\nwhile, if 1/3 \u2264 \u03b1 < 1,\nfbn (x) \u2212 f (x) = O\n\n\u0010\u221a\n\nn\u03b1\u22121\n\nlog n\n\n\u0011\n\na.s.\n\nThe asymptotic normality of the estimator fbn is as follows.\nTheorem 3.2. Assume that (LC ) and (H1 ) to (H3 ) hold. In addition, suppose that\nthe sequence (Xn ) has a finite moment of order a = 6 and that the sequence (\u03b5n )\nhas a finite conditional moment of order b > 2. Then, as soon as the bandwidth\n(hn ) satisfies hn = 1/n\u03b1 with 1/3 < \u03b1 < 1, we have for any x \u2208 R, the pointwise\nasymptotic normality\n\u0011\n\u0010\np\n\u03c32\u03bd 2\nL\nb\n(3.4)\nnhn (fn (x) \u2212 f (x)) \u2212\u2192 N 0,\n(1 + \u03b1)h(\u03b8, x)\nwhere h(\u03b8, x) stands for the probability density function associated with (\u03b80 Xn ).\nProof. The proof is given Appendix B.\n\n\u0003\n\n4. NUMERICAL SIMULATIONS\nThe goal of this Section is to illustrate via some numerical experiments the theoretical results of Section 3. We will provide the numerical behavior of our recursive\nestimators combining the recursive Nadaraya-Watson estimator of the link function\nf together with the recursive SIR estimator of the parameter \u03b8. First of all, we describe in Section 4.1 the simulated model used in the numerical study and we present\nthe estimation procedure, in particular the choice of the bandwidth parameter \u03b1 by\na cross-validation criterion. Then, we illustrate in Sections 4.2 and 4.3 the almost\nsure convergence and the asymptotic normality of our recursive Nadaraya-Watson\nestimator of f .\n4.1. Simulated model and estimation procedures. We consider the semiparametric regression model given, for all n \u2265 1, by\n(M)\n\nYn = f (\u03b80 Xn ) + \u03b5n\n\nwhere the link function f is defined, for all x \u2208 R, by\n\u0010 3x \u0011\nf (x) = x exp\n.\n4\n\n\f6\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\n\n25\n20\n15\n\nY\n10\n5\n0\n\n0\n\n5\n\n10\n\nY\n\n15\n\n20\n\n25\n\nThe parameter \u03b8 belongs to Rp with p = 10 and is given by\n\u0011\n1 \u0010\n\u03b8=\u221a\n1, 2, \u22122, \u22121, 0, . . . , 0 .\n10\nMoreover, (Xn ) is a sequence of independent random vectors of Rp sharing the same\nN (0, Ip ) distribution, while (\u03b5n ) is a sequence of independent random variables with\nstandard N (0, 1) distribution, independent of (Xn ). In Figure 4.1, we present two\nscatterplots for a sample of size n = 1000 generated from model (M). On the left side,\none can observe the data in the \"true\" reduction subspace, that is the scatterplot of\n(\u03b80 X1 , Y1 ), . . . , (\u03b80 Xn , Yn ) based on the \"true\" EDR direction \u03b8. On the right side,\nwe plot the data obtained from the estimated EDR direction \u03b8bn calculated via our\nrecursive SIR procedure, that is the scatterplot of (\u03b8bn0 X1 , Y1 ), . . . , (\u03b8bn0 Xn , Yn ). One\ncan clearly notice that the EDR direction has been well estimated.\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\nTrue index\n\nScatterplots of\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\nEstimated index\n\nFigure 4.1.\nand (\u03b8bn0 X1 , Y1 ), . . . , (\u03b8bn0 Xn , Yn ).\n\n(\u03b80 X1 , Y1 ), . . . , (\u03b80 Xn , Yn )\n\nFor the recursive Nadaraya-Watson estimator fbn of f , we have chosen the well-known\nEpanechnikov kernel\n3\nK(x) = (1 \u2212 x2 )I{|x|\u22641}\n4\nand the bandwidth hn = 1/n\u03b1 with 0 < \u03b1 < 1. We now need to evaluate an optimal\nvalue for the smoothing parameter \u03b1. The problem of deciding how much to smooth\nis of great importance in nonparametric regression. We propose to make use of the\noptimal data-driven bandwidth \u03b1 which minimizes the cross-validation criterion\nn\nX\n0\nCV (\u03b1) =\n(Yk \u2212 Ybk,\u03b1 )2\nwhere\nYbk,\u03b1 = fbk\u22121 (\u03b8bk\u22121\nXk ).\nk=p+1\n\nWe can observe by simulations that the CV (\u03b1) functions are all convex and the\ncorresponding optimal data-driven bandwidth \u03b1 lies into the interval [0.33, 0.38].\nConsequently, in all Section 4, we have chosen the optimal value \u03b1 = 0.35.\n\n\fON THE ASYMPTOTIC BEHAVIOR OF THE NADARAYA-WATSON ESTIMATOR\n\n7\n\n12\n\n12\n10\n\n10\n8\n\n8\n6\n\n6\n4\n\n4\n2\n\n2\n0\n\n0\n-2\n\n-2\n\n-2\n\n-2\n\n0\n\n0\n\n2\n\n2\n\n4\n\n4\n\n6\n\n6\n\n8\n\n8\n\n10\n\n10\n\n12\n\n12\n\n4.2. Almost sure convergence. The good numerical performances of the recursive SIR estimator \u03b8bn were perviously illustrated in [1], [14]. In order to keep this\nsection brief, we only focus our attention on the almost sure convergence of fbn . We\ngenerate N = 1000 samples of different sizes n = 200, 500, 1000, 2000 from model\n(M) with p = 10. For each sample, we calculate the estimation fbn (\u03b8bn0 x) of f (\u03b80 x) for\n10 different values of x \u2208 Rp . The boxplots of the fbn (\u03b8bn0 x)'s are given in Figure 4.2.\nThe circle point in each boxplot represents the true value f (\u03b80 x) to easily judge the\nquality of the estimations. One can observe that the dispersion of the fbn (\u03b8bn0 x)'s\nare small and the mean is very close to the true value f (\u03b80 x). One can also notice\nthat the larger is the sample size n, the greater is the quality measure. As it was\nexpected, the quality of the estimation decreases for large values of f (\u03b80 x) since the\nnumber of observations around x decreases, see the scatterplots of Figure 4.1 to be\nconvinced.\nn = 200\nn = 500\n\nx1\n\nx2\n2\n\nx3\n\nx4\n4\n\nx5\n\nx6\n6\n\nx7\n\nx8\n8\n\nx9\n\nx10\n\nx1\n\n10\n\nx2\n2\n\nx3\n\n4\n\nx5\n\nx6\n6\n\nx7\n\nx8\n\nx9\n\nx10\n\nx8\n\nx9\n\nx10\n\n8\n\n10\n\n12\n10\n\n10\n8\n\n8\n6\n\n6\n4\n\n4\n2\n\n2\n0\n\n0\n-2\n\n-2\n\n-2\n\n-2\n\n0\n\n0\n\n2\n\n2\n\n4\n\n4\n\n6\n\n6\n\n8\n\n8\n\n10\n\n10\n\n12\n\n12\n\nn = 2000\n\n12\n\nn = 1000\n\nx4\n\nx1\n\nx2\n2\n\nx3\n\nx4\n4\n\nx5\n\nx6\n6\n\nx7\n\nx8\n8\n\nx9\n\nx10\n10\n\nx1\n\nx2\n2\n\nx3\n\nx4\n4\n\nx5\n\nx6\n6\n\nx7\n\n8\n\nFigure 4.2.\nAlmost sure convergence of fbn (\u03b8bn0 x) to f (\u03b80 x) for 10 different values of x.\n\n10\n\n\f8\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\n\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\n\n0.0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n4.3. Asymptotic normality. In order to illustrate the asymptotic normality of our\nrecursive Nadaraya-Watson estimator, we generate N = 1000 realizations of fbn (\u03b8bn0 x)\nfor n = 1000 from model (M) with p = 10. In Figure 4.3, we plot the histogram of\nthe standardized values of the fbn (\u03b8bn0 x)'s for 2 different values of x \u2208 Rp . We add\nthe density of the standard normal density on each histogram. One can clearly see\nthat the normal density coincides pretty well with all the histograms, which visually\nillustrates the asymptotic normality of our recursive Nadaraya-Watson estimator fbn\nof f .\n\n-3\n\n-2\n\n-3\n\n-1\n\n-2\n\n0\n\n-1\n\n1\n\n0\n\n2\n\n1\n\n2\n\n3\n\n-3\n\n3\n\n-2\n\n-3\n\n-1\n\n-2\n\n-1\n\n0\n\n0\n\n1\n\n1\n\n2\n\n2\n\n3\n\n3\n\nFigure 4.3.\nAsymptotic normality of fbn (\u03b8bn0 x) to f (\u03b80 x) for 2 different values of x.\n\nAppendix A\nPROOF OF THEOREM 3.1\n\nIn order to prove the almost sure pointwise convergence of Theorem 3.1, we shall\ndenote for all x \u2208 R\nPn (x) =\n\nn\nX\n\nWk (x)\u03b5k ,\n\nk=1\n\nNn (x) =\n\nn\nX\n\nWk (x),\n\nk=1\n\nand\nQn (x) =\n\nn\nX\n\nWk (x)(f (\u03a6k ) \u2212 f (x))\n\nk=1\n0\n\nwhere \u03a6n = \u03b8 Xn . We clearly obtain from (1.1) the main decomposition\n(A.1)\n\nPn (x) + Qn (x)\nfbn (x) \u2212 f (x) =\n.\nNn (x)\n\n\fON THE ASYMPTOTIC BEHAVIOR OF THE NADARAYA-WATSON ESTIMATOR\n\n9\n\nWe shall establish the asymptotic behavior of each sequence (Pn (x)), (Qn (x)) and\n(Nn (x)). Let (Fn ) be the filtration given by Fn = \u03c3(X1 , . . . , Xn , Y1 , . . . , Yn ). First\nof all, we can split Nn (x) into two terms,\nNn (x) = Mn(N ) (x) + Rn(N ) (x)\n\n(A.2)\nwhere\nMn(N ) (x)\n\n=\n\nn \u0010\nX\n\nWk (x) \u2212 E[Wk (x)|Fk\u22121 ]\n\n\u0011\n\nRn(N ) (x)\n\nand\n\nk=1\n\n=\n\nn\nX\n\nE[Wk (x)|Fk\u22121 ].\n\nk=1\n\nOn the one hand, we have\n1\nE[Wn (x)|Fn\u22121 ] =\nhn\n\nZ\nK\n\n\u0010 x \u2212 \u03b8b0\n\nn\u22121 xn\n\nhn\n\nRp\n\n\u0011\n\ng(xn ) dxn .\n\nWe can assume without loss of generality that, for n large enough, at least one\ncomponent of \u03b8bn is different from zero a.s. As a matter of fact, we already saw from\nLemma 2.1 that \u03b8bn converges a.s. to \u03b8 which is different from zero. For the sake of\nsimplicity, suppose that the first component \u03b8bn\u22121,1 6= 0 a.s. We can make the change\nof variables\n0\nx \u2212 \u03b8bn\u22121\nxn\nz=\nhn\nand z2 = xn,2 , . . . , zp = xn,p . The Jacobian of this linear transformation is given by\nJ =\u2212\n\nhn\n.\nb\n\u03b8n\u22121,1\n\nConsequently, we obtain that\nZ\n(A.3)\n\nK(z)h(\u03b8bn\u22121 , x \u2212 zhn )dz\n\nE[Wn (x)|Fn\u22121 ] =\nR\n\nwhere\nh(\u03b8bn\u22121 , x) =\n\n1\n|\u03b8bn\u22121,1 |\n\nZ\ng\nRp\u22121\n\n1\n\n\u0010\n\n\u03b8bn\u22121,1\n\np\n\u0010\n\u0011\n\u0011\nX\nx\u2212\n\u03b8bn\u22121,k zk , z2 , . . . , zp dz2 . . . dzp .\nk=2\n\nOne can observe that h(\u03b8, x) is exactly the probability density function associated\nwith the identically distributed sequence (\u03b80 Xn ). Therefore, as the probability density function g is continuous, twice differentiable with bounded derivatives, we deduce from (A.3) togheter with Taylor's formula that\nZ\n\u0010\nE[Wn (x)|Fn\u22121 ] =\nK(z) h(\u03b8bn\u22121 , x) \u2212 zhn h0 (\u03b8bn\u22121 , x)\nR\n\n\u0011\nz 2 h2n 00 b\nh (\u03b8n\u22121 , x \u2212 zhn \u03be) dz,\n2\nZ\nh2n\nb\n= h(\u03b8n\u22121 , x) +\nz 2 K(z)h00 (\u03b8bn\u22121 , x \u2212 zhn \u03be)dz\n2 R\n+\n\n\f10\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\n\nwhere 0 < \u03be < 1. Consequently, for n large enough,\nE[Wn (x)|Fn\u22121 ] \u2212 h(\u03b8bn\u22121 , x) \u2264 Mh \u03c4 2 h2n\n\n(A.4)\n\na.s.\n\nwhere\n00\n\nMh = sup h (\u03b8bn\u22121 , x)\n\n1\n\u03c4 =\n2\n2\n\nand\n\nx\u2208R\n\nZ\n\nx2 K(x)dx.\n\nR\n\nHence, we find from (A.4) that\nn\nX\n\nE[Wk (x) | Fk\u22121 ] \u2212 h(\u03b8bk\u22121 , x) = O\n\nn\n\u0010X\n\nk=1\n\nh2k\n\n\u0011\n\na.s.\n\nk=1\n\nIt follows from the continuity of h together with the fact that \u03b8bn converges to \u03b8 a.s.\nand hn goes to zero that\nn\n\n1X\nE[Wk (x)|Fk\u22121 ] = h(\u03b8, x)\nlim\nn\u2192\u221e n\nk=1\n\n(A.5)\n\na.s.\n\nwhich of course immediately implies that for all x \u2208 R\n(N )\n\nRn (x)\n= h(\u03b8, x)\nlim\nn\u2192\u221e\nn\n\n(A.6)\n\na.s.\n\n(N )\n\nOn the other hand, (Mn (x)) is a square integrable martingale difference sequence\nwith predictable quadratic variation given by\n<M\n\n(N )\n\n(x) >n =\n\nn\nX\n\n(N )\n\n(N )\n\nE[(Mk (x) \u2212 Mk\u22121 (x))2 |Fk\u22121 ],\n\nk=1\n\n=\n\nn \u0010\nX\n\n\u0011\nE[Wk2 (x)|Fk\u22121 ] \u2212 E2 [Wk (x)|Fk\u22121 ] .\n\nk=1\n\nVia the same change of variables as in (A.3), we obtain that\nZ\n1\n2\nE[Wn (x)|Fn\u22121 ] =\nK 2 (z)h(\u03b8bn\u22121 , x \u2212 zhn )dz,\nhn R\nZ\n\u0010\n1\n2\n=\nK (z) h(\u03b8bn\u22121 , x) \u2212 zhn h0 (\u03b8bn\u22121 , x)\nhn R\n\u0011\nz 2 h2n 00 b\n+\nh (\u03b8n\u22121 , x \u2212 zhn \u03be) dz\n2\nwhere 0 < \u03be < 1. Consequently, for n large enough,\n(A.7)\n\nE[Wn2 (x)|Fn\u22121 ] \u2212\n\n\u03bd2 b\nh(\u03b8n\u22121 , x) \u2264 Mh \u03bc2 hn\nhn\n\na.s.\n\nwhere\n2\n\nZ\n\n\u03bd =\n\n2\n\nK (x)dx\nR\n\nand\n\n1\n\u03bc =\n2\n2\n\nZ\nR\n\nx2 K 2 (x)dx.\n\n\fON THE ASYMPTOTIC BEHAVIOR OF THE NADARAYA-WATSON ESTIMATOR\n\n11\n\nHence, (A.7) ensures that\nn\nX\n\nn\n\nE[Wk2 (x)\n\nk=1\n\n\u0010X \u0011\n\u03bd2 b\n| Fk\u22121 ] \u2212 h(\u03b8k\u22121 , x) = O\nhk\nhk\nk=1\n\na.s.\n\nHowever, it is not hard to see that\nlim\n\nn\u2192\u221e\n\n1\nn1+\u03b1\n\nn\nX\n1\n1\n=\n.\nhk\n1+\u03b1\nk=1\n\nTherefore, it follows from (A.7) together with the almost sure convergence of h(\u03b8bn , x)\nto h(\u03b8, x) and Toeplitz's lemma that\n(A.8)\n\nn\nX\n\n1\n\nlim\n\nn1+\u03b1\n\nn\u2192\u221e\n\nE[Wk2 (x) | Fk\u22121 ] =\n\nk=1\n\n\u03bd2\nh(\u03b8, x)\n1+\u03b1\n\na.s.\n\nFurthermore, we also have from (A.4) that\nn\n\n(A.9)\n\n1X 2\nlim\nE [Wk (x)|Fk\u22121 ] = h2 (\u03b8, x)\nn\u2192\u221e n\nk=1\n\na.s.\n\nConsequently, we deduce from (A.8) and (A.9) that for all x \u2208 R,\n< M (N ) (x) >n\n\u03bd2\n=\nh(\u03b8, x)\nn\u2192\u221e\nn1+\u03b1\n1+\u03b1\n\n(A.10)\n\nlim\n\na.s.\n\nWe are now in position to make use of the strong law of large numbers for martingales\ngiven e.g. by Theorem 1.3.15 of [8]. As the probability density function g is positive\non its support, we have for all x \u2208 R, h(\u03b8, x) > 0, which implies that < M (N ) (x) >n\n(N )\ngoes to infinity a.s. Hence, for any \u03b3 > 0, (Mn (x))2 = o(n1+\u03b1 (log n)1+\u03b3 ) a.s. which\nleads to\nMn(N ) (x) = o(n)\n\n(A.11)\n\na.s.\n\nThen, we obtain from (A.2), (A.6) and (A.11) that for all x \u2208 R\nNn (x)\n= h(\u03b8, x)\nn\u2192\u221e\nn\n\n(A.12)\n\nlim\n\na.s.\n\nWe shall now investigate the asymptotic behavior of the sequence (Pn (x)). Since\n(Xn ) and (\u03b5n ) are independent, (Pn (x)) is a square integrable martingale difference\nsequence with predictable quadratic variation given by\n< P (x) >n =\n\nn\nX\n\nE[(Pk (x) \u2212 Pk\u22121 (x))2 |Fk\u22121 ] = \u03c3 2\n\nk=1\n\nn\nX\n\nE[Wk2 (x)|Fk\u22121 ].\n\nk=1\n\nThen, it follows from convergence (A.8) that\n(A.13)\n\n< P (x) >n\n\u03c32\u03bd 2\n=\nh(\u03b8, x)\nn\u2192\u221e\nn1+\u03b1\n1+\u03b1\nlim\n\na.s.\n\n\f12\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\n\nConsequently, we obtain from the strong law of large numbers for martingales that\nfor any \u03b3 > 0 and that for all x \u2208 R,\n\u0010p\n\u0011\n1+\u03b1\n1+\u03b3\n(A.14)\nPn (x) = o\nn (log n)\n= o(n)\na.s.\nIt remains to study the asymptotic behavior of the sequence (Qn (x)). We can split\nQn (x) into two terms,\n(A.15)\n\nQn (x) = \u03a3n (x) + \u2206n (x)\n\n0\nb n = \u03b8bn\u22121\nwhere \u03a6\nXn ,\n\n\u03a3n (x) =\n\nn\nX\n\nb k ))\nWk (x)(f (\u03a6k ) \u2212 f (\u03a6\n\nand\n\n\u2206n (x) =\n\nk=1\n\nn\nX\n\nb k ) \u2212 f (x)).\nWk (x)(f (\u03a6\n\nk=1\n\nThe right-hand side of (A.15) is easy to handle. As a matter of fact, the kernel K\nis compactly supported which means that one can find a positive constant A such\nthat K vanishes outside the interval [\u2212A, A]. Thus, for all n \u2265 1 and all x \u2208 R,\nWn (x) =\n\n0\nXn \u0011\n1 \u0010 x \u2212 \u03b8bn\u22121\nI{|\u03b8b0 Xn \u2212x|\u2264Ahn } .\nK\nn\u22121\nhn\nhn\n\nIn addition, the function f is Lipschitz, so it exists a positive constant Cf such that\nfor all n \u2265 1\n(A.16)\n\nb n ) \u2212 f (x)| \u2264 Cf |\u03a6\nb n \u2212 x| \u2264 Cf |\u03b8b0 Xn \u2212 x|.\n|f (\u03a6\nn\u22121\n\nConsequently, we obtain from (A.16) that for all x \u2208 R\n|\u2206n (x)| \u2264 Cf\n\nn\nX\n\n0\nWk (x)|\u03b8bk\u22121\nXk \u2212 x|,\n\nk=1\nn\nX\n\n\u2264 ACf\n\n(A.17)\n\nhk Wk (x).\n\nk=1\n\nMoreover, via the same lines as in the proof of (A.5), we find that\n(A.18)\n\nlim\n\nn\u2192\u221e\n\n1\nn1\u2212\u03b1\n\nn\nX\n\nhk E[Wk (x)|Fk\u22121 ] =\n\nk=1\n\n1\nh(\u03b8, x)\n1\u2212\u03b1\n\nFurthermore, denote\nMn(\u2206) (x) =\n\nn\nX\nk=1\n\n\u0010\n\u0011\nhk Wk (x) \u2212 E[Wk (x)|Fk\u22121 ] .\n\na.s.\n\n\fON THE ASYMPTOTIC BEHAVIOR OF THE NADARAYA-WATSON ESTIMATOR\n\n13\n\n(\u2206)\n\nOne can observe that (Mn (x)) is a square integrable martingale difference sequence\nwith bounded increments and predictable quadratic variation given by\nn\nX\n(\u2206)\n(\u2206)\n(\u2206)\n< M (x) >n =\nE[(Mk (x) \u2212 Mk\u22121 (x))2 |Fk\u22121 ],\nk=1\n\n=\n\nn\nX\n\n\u0010\n\u0011\nh2k E[Wk2 (x)|Fk\u22121 ] \u2212 E2 [Wk (x)|Fk\u22121 ] .\n\nk=1\n\nHence, it follows from (A.4) and (A.7) together with the almost sure convergence of\nh(\u03b8bn , x) to h(\u03b8, x) and Toeplitz's lemma that\n\u03bd2\n< M (\u2206) (x) >n\nh(\u03b8, x)\na.s.\n=\nn\u2192\u221e\nn1\u2212\u03b1\n1\u2212\u03b1\nConsequently, we obtain from the strong law of large numbers for martingales that\n\u00112\n\u0010\n\u0010\n\u0011\n(A.20)\nMn(\u2206) (x) = O n1\u2212\u03b1 log n\na.s.\n(A.19)\n\nlim\n\nThen, we infer from the conjunction of (A.17), (A.18) and (A.20) that for all x \u2208 R\n\u0010\n\u0011\n(A.21)\n|\u2206n (x)| = O n1\u2212\u03b1\na.s.\nThe left-hand side of (A.15) is much more difficult to handle. We can use once again\nthe assumption that the function f is Lipschitz to deduce that it exists a positive\nconstant Cf such that for all n \u2265 1\nb n ) \u2212 f (\u03a6n )| \u2264 Cf |\u03c0n |\n|f (\u03a6\n\n(A.22)\n\nwhere \u03c0n = (\u03b8bn\u22121 \u2212 \u03b8)0 Xn . Hence, it immediately follows from (A.22) that for all\nx\u2208R\nn\nX\nWk (x)|\u03c0k |.\n(A.23)\n|\u03a3n (x)| \u2264 Cf\nk=1\n\nDenote\nn\no\n0\nAn = |\u03b8bn\u22121\nXn \u2212 x| \u2264 Ahn\n\nand\n\nn\no\nBn = |\u03b80 Xn \u2212 x| \u2264 Ahn + bn\n\nwhere (bn ) is a sequence of positive real numbers which will be explicitely given\nlater. On the one hand, we immediately have from the triangle inequality that on\nthe set An \u2229 Bn ,\n|\u03c0n | \u2264 2Ahn + bn .\nOn the other hand, we also have on the set An \u2229 Bn ,\n0\nAhn + bn < |\u03b80 Xn \u2212 x| \u2264 |\u03c0n | + |\u03b8bn\u22121\nXn \u2212 x| \u2264 |\u03c0n | + Ahn\n\nwhich implies that |\u03c0n | > bn . Consequently, we obtain from (A.23) that\n(A.24) |\u03a3n (x)| \u2264 2ACf\n\nn\nX\nk=1\n\nhk Wk (x)+Cf\n\nn\nX\nk=1\n\nbk Wk (x)+Cf\n\nn\nX\nk=1\n\nWk (x)|\u03c0k |I{|\u03c0k |>bk } .\n\n\f14\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\n\nWe already saw from (A.21) that\nn\nX\n\n(A.25)\n\n\u0010\n\u0011\nhk Wk (x) = O n1\u2212\u03b1\n\na.s.\n\nk=1\n\nMoreover, it is assumed that the sequence (Xn ) has a finite moment of order a > 2\nwhich ensures that\nsup ||Xk || = o(n1/a )\n\na.s.\n\n1\u2264k\u2264n\n\nConsequently, we find from Lemma (2.1) that\n|\u03c0n | = o(bn )\n\n(A.26)\n\na.s.\n\nwhere we can choose\nr\nbn = n1/a\n\nlog(log n)\n.\nn\n\nTherefore, we clearly have\nn\nX\n\n(A.27)\n\nWk (x)|\u03c0k |I{|\u03c0k |>bk } < +\u221e\n\na.s.\n\nk=1\n\nFurthermore, it is not hard to see that\nn\nX\n\n\u0010\n\u0011\np\n1/a\nbk = O n\nn log(log n) .\n\nk=1\n\nHence, via the same lines as in the proof of (A.21), we obtain that\n(A.28)\n\nn\nX\n\n\u0010\n\u0011\np\nbk Wk (x) = O n1/a n log(log n)\n\na.s.\n\nk=1\n\nThen, we deduce from the conjunction of (A.24), (A.25), (A.27), and (A.28) that\n\u0011\n\u0010\n\u0011\n\u0010\np\n1\u2212\u03b1\n1/a\nn log(log n)\na.s.\n(A.29)\n|\u03a3n (x)| = O n\n+O n\nConsequently, we infer from (A.21) and (A.29) that for all x \u2208 R\n\u0010\n\u0011\n\u0010\n\u0011\np\n(A.30)\nQn (x) = O n1\u2212\u03b1 + O n1/a n log(log n)\na.s.\n\na.s.\n\nFinally, we can conclude from (A.1) together with (A.12), (A.14) and (A.30) that\nlim fbn (x) = f (x)\n\nn\u2192\u221e\n\na.s.\n\nwith the almost sure rates of convergence given by (3.2) and (3.3), which completes\nthe proof of Theorem 3.1.\n\n\fON THE ASYMPTOTIC BEHAVIOR OF THE NADARAYA-WATSON ESTIMATOR\n\n15\n\nAppendix B\nPROOF OF THEOREM 3.2\n\nWe already saw that (Pn (x)) is a square integrable martingale difference sequence\nwith predictable quadratic variation satisfying\n< P (x) >n\n\u03c32\u03bd 2\nh(\u03b8, x)\na.s.\n=\nn\u2192\u221e\nn1+\u03b1\n1+\u03b1\nIn order to establish the asymptotic normality of Theorem 3.2, it is necessary to\nprove that the sequence (Pn (x)) satisfies the Lindeberg condition, that is for all\n\u03b5 > 0,\nn\ni\n1 X h\nP\n(B.1)\nPn (x) = 1+\u03b1\nE |\u2206Pk (x)|2 I{|\u2206Pk (x)|\u2265\u03b5\u221an1+\u03b1 } |Fk\u22121 \u2212\u2192 0\nn\nk=1\nlim\n\nwhere \u2206Pn (x) = Pn (x) \u2212 Pn\u22121 (x). We have assumed that the sequence (\u03b5n ) has a\nfinite conditional moment of order b > 2 which means that\nsup E[|\u03b5n |b |Fn\u22121 ] < +\u221e\n\na.s.\n\nn\u22650\n\nConsequently, for all \u03b5 > 0, we have\nn\n1 X\nE[|\u2206Pk (x)|b |Fk\u22121 ],\nPn (x) \u2264 b\u22122 c\n\u03b5 n k=1\n\u2264\n(B.2)\n\n\u2264\n\nn\nX\n\n1\n\u03b5b\u22122 nc\n1\n\nE[Wkb (x)|Fk\u22121 ]E[|\u03b5k |b |Fk\u22121 ],\n\nk=1\nb\n\n\u03b5b\u22122 nc\n\nsup E[|\u03b5k | |Fk\u22121 ]\n1\u2264k\u2264n\n\nn\nX\n\nE[Wkb (x)|Fk\u22121 ]\n\nk=1\n\nwhere c = b(1 + \u03b1)/2. In addition, via the same lines as in the proof of (A.8), we\nobtain that\nn\nX\n1\n\u03beb\n(B.3)\nlim 1+\u03b1(b\u22121)\nh(\u03b8, x)\na.s.\nE[Wkb (x) | Fk\u22121 ] =\nn\u2192\u221e n\n1 + \u03b1(b \u2212 1)\nk=1\nwhere\nb\n\nZ\n\n\u03be =\n\nK b (x) dx.\n\nR\n\nTherefore, we deduce from (B.1) together with (B.2) and (B.3) that, for all \u03b5 > 0,\nPn (x) = O(nd )\n\na.s.\n\nwhere d = (2 \u2212 b)(1 \u2212 \u03b1)/2. We recall that b > 2 which means that d < 0. It ensures\nthat the Lindeberg condition is satisfied. Hence, it follows from the central limit\ntheorem for martingales given e.g. by Corollary 2.1.10 of [8] that for all x \u2208 R,\n\u0010 \u03c32\u03bd 2\n\u0011\nPn (x) L\n\u221a\n\u2212\u2192 N 0,\n(B.4)\nh(\u03b8, x) .\n1+\u03b1\nn1+\u03b1\n\n\f16\n\nBERNARD BERCU, THI MONG NGOC NGUYEN, AND JEROME SARACCO\n\nFurthermore, as soon as a \u2265 6 and 1/3 < \u03b1 < 1, we clearly obtain from (A.30) that\nQn (x)\nlim \u221a\n=0\na.s.\nn\u2192\u221e\nn1+\u03b1\nFinally, we find from (A.1) together with (A.12), (B.4), (B.5) and Slutsky's lemma\nthat, for all x \u2208 R,\n\u0010\n\u0011\np\n\u03c32\u03bd 2\nL\nnhn (fbn (x) \u2212 f (x)) \u2212\u2192 N 0,\n(1 + \u03b1)h(\u03b8, x)\n\n(B.5)\n\nwhich acheives the proof of Theorem 3.2.\nAcknowledgements. The first author would like to thanks Bruno Portier for\nhelpful remarks made on a preliminary version of the paper.\nReferences\n[1] Bercu, B., Nguyen, T.M.N., and Saracco, J., A new approach on recursive and non-recursive\nSIR methods, Journal of the Korean Statistical Society, 41(1), pp. 17-36, 2012.\n[2] Bercu, B., and Portier, B., Kernel density estimation and goodness-of-fit test in adaptive\ntracking, SIAM J. Control Optim. 47, pp. 2440-2457, 2008.\n[3] Chen, C.-H., and Li, K.-C., Can SIR be as popular as multiple linear regression?, Statistica\nSinica, 8(2), pp. 289-316, 1998.\n[4] Choi, E., Hall, P., and Rousson, V., Data sharpening methods for bias reduction in nonparametric regression, Ann. Statist. 28, pp. 1339-1355, 2000.\n[5] Cook, R. D.,and Ni, L., Sufficient dimension reduction via inverse regression: a minimum\ndiscrepancy approach, Journal of American Statistical Association, 100, pp. 410-418, 2005.\n[6] Devroye, L., and Lugosi, G., Combinatorial methods in density estimation, Springer Series in\nStatistics. Springer-Verlag, New York, 2001.\n[7] Duan, N., and Li, K.-C., Slicing regression: A link-free regression method, Ann. Stat. 19(2),\npp. 505-530,1991.\n[8] Duflo, M., Random Iterative Models, Springer-Verlag, Berlin, 1997.\n[9] Hall, P., and Heyde, C. C., Martingale limit theory and its application, Academic Press Inc.\nNew York, 1980.\n[10] Hall, P., and Li, K.-C., On almost linearity of low-dimensional projections from highdimensional data, Annals of Statistics, 21(2), pp. 867-889, 1993.\n[11] Li, K.-C., Sliced inverse regression for dimension reduction (with discussions), J. Am. Stat.\nAssoc. 86(414), pp. 316-342, 1991.\n[12] Nadaraja, E. A., On a regression estimate, Teor. Verojatnost. i Primenen. 9, pp. 157-159,\n1964.\n[13] Nadaraya, E. A., Nonparametric estimation of probability densities and regression curves,\nMathematics and its Applications, Kluwer Academic Publishers Group, Dordrecht, 1989.\n[14] Nguyen, T.M.N., and Saracco, J., Recursive estimation for sliced inverse regression (in French),\nJournal de la Soci\u00e9t\u00e9 Fran\u00e7aise de Statistique, Vol. 151(2), pp. 19-46, 2010.\n[15] Noda, K., Estimation of a regression function by the Parzen kernel-type density estimators,\nAnn. Inst. Statist. Math. 28, pp. 221-234, 1976.\n[16] Parzen, E., On estimation of a probability density function and mode, Ann. Math. Statist.\n33, pp. 1065-1076, 1972.\n[17] Rosenblatt, M., Remarks on some nonparametric estimates of a density function. Ann. Math.\nStatist. 27, pp. 832-837, 1956.\n[18] Schuster, E. F., Joint asymptotic distribution of the estimated regression function at a finite\nnumber of distinct points. Ann. Math. Statist. 43, pp. 84-88, 1972.\n\n\fON THE ASYMPTOTIC BEHAVIOR OF THE NADARAYA-WATSON ESTIMATOR\n\n17\n\n[19] Silverman, B. W., Density estimation for statistics and data analysis, Monographs on Statistics\nand Applied Probability. Chapman and Hall, London, 1986.\n[20] Stout, W. F. , Almost sure convergence, Academic Press, New York, 1974.\n[21] Tsybakov, A. B., Introduction \u00e0 l'estimation non-param\u00e9trique, Math\u00e9matiques et Applications, Springer-Verlag, Berlin, 2004.\n[22] Watson, G. S., Smooth regression analysis, Sankhya Ser. A 26, pp. 359-372, 1964.\nE-mail address: Bernard.Bercu@math.u-bordeaux1.fr\nE-mail address: Nguyen@math.unistra.fr\nE-mail address: Jerome.Saracco@math.u-bordeaux1.fr\nUniversit\u00e9 de Bordeaux, Institut de Math\u00e9matiques de Bordeaux, UMR CNRS\n5251, 351 cours de la lib\u00e9ration, 33405 Talence cedex, France.\nUniversit\u00e9 de Strasbourg, Institut de Recherche Math\u00e9matique Avanc\u00e9e, UMR\nCNRS 7501, 7 rue Ren\u00e9 Descartes 67084 Strasbourg cedex, France.\nUniversit\u00e9 de Bordeaux, Institut de Math\u00e9matiques de Bordeaux, UMR CNRS\n5251, 351 cours de la lib\u00e9ration, 33405 Talence cedex, France.\n\n\f"}