{"id": "http://arxiv.org/abs/0807.1996v3", "guidislink": true, "updated": "2008-11-01T19:28:22Z", "updated_parsed": [2008, 11, 1, 19, 28, 22, 5, 306, 0], "published": "2008-07-12T20:12:44Z", "published_parsed": [2008, 7, 12, 20, 12, 44, 5, 194, 0], "title": "A multiphysics and multiscale software environment for modeling\n  astrophysical systems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0807.0411%2C0807.4175%2C0807.2489%2C0807.2216%2C0807.0256%2C0807.1098%2C0807.4541%2C0807.2853%2C0807.5070%2C0807.1425%2C0807.2635%2C0807.0609%2C0807.3280%2C0807.1058%2C0807.3201%2C0807.4260%2C0807.5137%2C0807.0359%2C0807.3656%2C0807.1048%2C0807.4585%2C0807.1733%2C0807.2239%2C0807.3808%2C0807.4380%2C0807.2483%2C0807.2927%2C0807.0217%2C0807.1972%2C0807.4484%2C0807.3963%2C0807.0185%2C0807.4276%2C0807.4706%2C0807.4872%2C0807.4553%2C0807.0764%2C0807.2836%2C0807.0759%2C0807.3522%2C0807.1233%2C0807.3016%2C0807.4932%2C0807.4272%2C0807.2358%2C0807.4666%2C0807.1072%2C0807.3826%2C0807.4864%2C0807.2284%2C0807.3320%2C0807.1618%2C0807.1020%2C0807.3755%2C0807.2601%2C0807.2046%2C0807.4213%2C0807.4628%2C0807.1001%2C0807.0869%2C0807.2906%2C0807.3474%2C0807.0757%2C0807.3158%2C0807.1238%2C0807.1279%2C0807.1277%2C0807.0635%2C0807.1773%2C0807.2040%2C0807.3943%2C0807.1251%2C0807.2400%2C0807.1996%2C0807.2296%2C0807.1761%2C0807.1107%2C0807.0681%2C0807.2592%2C0807.1101%2C0807.4910%2C0807.1497%2C0807.4151%2C0807.3869%2C0807.0586%2C0807.3868%2C0807.2524%2C0807.2302%2C0807.2776%2C0807.4754%2C0807.1408%2C0807.3336%2C0807.1113%2C0807.3442%2C0807.4231%2C0807.0427%2C0807.4852%2C0807.1121%2C0807.4946%2C0807.1860%2C0807.0360&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A multiphysics and multiscale software environment for modeling\n  astrophysical systems"}, "summary": "We present MUSE, a software framework for combining existing computational\ntools for different astrophysical domains into a single multiphysics,\nmultiscale application. MUSE facilitates the coupling of existing codes written\nin different languages by providing inter-language tools and by specifying an\ninterface between each module and the framework that represents a balance\nbetween generality and computational efficiency. This approach allows\nscientists to use combinations of codes to solve highly-coupled problems\nwithout the need to write new codes for other domains or significantly alter\ntheir existing codes. MUSE currently incorporates the domains of stellar\ndynamics, stellar evolution and stellar hydrodynamics for studying generalized\nstellar systems. We have now reached a \"Noah's Ark\" milestone, with (at least)\ntwo available numerical solvers for each domain. MUSE can treat multi-scale and\nmulti-physics systems in which the time- and size-scales are well separated,\nlike simulating the evolution of planetary systems, small stellar associations,\ndense stellar clusters, galaxies and galactic nuclei.\n  In this paper we describe three examples calculated using MUSE: the merger of\ntwo galaxies, the merger of two evolving stars, and a hybrid N-body simulation.\nIn addition, we demonstrate an implementation of MUSE on a distributed computer\nwhich may also include special-purpose hardware, such as GRAPEs or GPUs, to\naccelerate computations. The current MUSE code base is publicly available as\nopen source at http://muse.li", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0807.0411%2C0807.4175%2C0807.2489%2C0807.2216%2C0807.0256%2C0807.1098%2C0807.4541%2C0807.2853%2C0807.5070%2C0807.1425%2C0807.2635%2C0807.0609%2C0807.3280%2C0807.1058%2C0807.3201%2C0807.4260%2C0807.5137%2C0807.0359%2C0807.3656%2C0807.1048%2C0807.4585%2C0807.1733%2C0807.2239%2C0807.3808%2C0807.4380%2C0807.2483%2C0807.2927%2C0807.0217%2C0807.1972%2C0807.4484%2C0807.3963%2C0807.0185%2C0807.4276%2C0807.4706%2C0807.4872%2C0807.4553%2C0807.0764%2C0807.2836%2C0807.0759%2C0807.3522%2C0807.1233%2C0807.3016%2C0807.4932%2C0807.4272%2C0807.2358%2C0807.4666%2C0807.1072%2C0807.3826%2C0807.4864%2C0807.2284%2C0807.3320%2C0807.1618%2C0807.1020%2C0807.3755%2C0807.2601%2C0807.2046%2C0807.4213%2C0807.4628%2C0807.1001%2C0807.0869%2C0807.2906%2C0807.3474%2C0807.0757%2C0807.3158%2C0807.1238%2C0807.1279%2C0807.1277%2C0807.0635%2C0807.1773%2C0807.2040%2C0807.3943%2C0807.1251%2C0807.2400%2C0807.1996%2C0807.2296%2C0807.1761%2C0807.1107%2C0807.0681%2C0807.2592%2C0807.1101%2C0807.4910%2C0807.1497%2C0807.4151%2C0807.3869%2C0807.0586%2C0807.3868%2C0807.2524%2C0807.2302%2C0807.2776%2C0807.4754%2C0807.1408%2C0807.3336%2C0807.1113%2C0807.3442%2C0807.4231%2C0807.0427%2C0807.4852%2C0807.1121%2C0807.4946%2C0807.1860%2C0807.0360&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present MUSE, a software framework for combining existing computational\ntools for different astrophysical domains into a single multiphysics,\nmultiscale application. MUSE facilitates the coupling of existing codes written\nin different languages by providing inter-language tools and by specifying an\ninterface between each module and the framework that represents a balance\nbetween generality and computational efficiency. This approach allows\nscientists to use combinations of codes to solve highly-coupled problems\nwithout the need to write new codes for other domains or significantly alter\ntheir existing codes. MUSE currently incorporates the domains of stellar\ndynamics, stellar evolution and stellar hydrodynamics for studying generalized\nstellar systems. We have now reached a \"Noah's Ark\" milestone, with (at least)\ntwo available numerical solvers for each domain. MUSE can treat multi-scale and\nmulti-physics systems in which the time- and size-scales are well separated,\nlike simulating the evolution of planetary systems, small stellar associations,\ndense stellar clusters, galaxies and galactic nuclei.\n  In this paper we describe three examples calculated using MUSE: the merger of\ntwo galaxies, the merger of two evolving stars, and a hybrid N-body simulation.\nIn addition, we demonstrate an implementation of MUSE on a distributed computer\nwhich may also include special-purpose hardware, such as GRAPEs or GPUs, to\naccelerate computations. The current MUSE code base is publicly available as\nopen source at http://muse.li"}, "authors": ["Simon Portegies Zwart", "Steve McMillan", "Stefan Harfst", "Derek Groen", "Michiko Fujii"], "author_detail": {"name": "Michiko Fujii"}, "author": "Michiko Fujii", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.newast.2008.10.006", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0807.1996v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0807.1996v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "24 pages, To appear in New Astronomy Source code available at\n  http://muse.li", "arxiv_primary_category": {"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0807.1996v3", "affiliation": "Tokyo", "arxiv_url": "http://arxiv.org/abs/0807.1996v3", "journal_reference": null, "doi": "10.1016/j.newast.2008.10.006", "fulltext": "arXiv:0807.1996v3 [astro-ph] 1 Nov 2008\n\nA multiphysics and multiscale software\nenvironment for modeling astrophysical\nsystems\nSimon Portegies Zwart a Steve McMillan b Stefan Harfst a\nDerek Groen a Michiko Fujii c Breannd\u00e1n \u00d3 Nuall\u00e1in a\nEvert Glebbeek d Douglas Heggie e James Lombardi f Piet Hut g\nVangelis Angelou a Sambaran Banerjee h Houria Belkus i\nTassos Fragos j John Fregeau j Evghenii Gaburov a Rob Izzard d\nMario Juri\u0107 g Stephen Justham k Andrea Sottoriva a\nPeter Teuben l Joris van Bever m Ofer Yaron n Marcel Zemp o\na University\n\nof Amsterdam, Amsterdam, The Netherlands\n\nb Drexel\n\nUniversity Philadelphia, PA, USA\n\nc University\nd Utrecht\n\nof Tokyo, Tokyo, Japan\n\nUniversity Utrecht, the Netherlands\n\ne University\n\nof Edinburgh Edinburgh, UK\n\nf Allegheny\n\nCollege Meadville, PA, USA\n\ng Institute\nh Tata\n\nfor Advanced Study Princeton, USA\n\nInstitute of Fundamental Research India\n\ni Vrije\n\nUniversiteit Brussel Brussel, Belgium\n\nj Northwestern\n\nUniversity Evanston, IL, USA\n\nk University\nl University\nm Saint\nn Tel\no University\n\nof Oxford Oxford, UK\n\nof Maryland College Park, MD, USA\n\nMary's University Halifax, Canada\nAviv University Tel Aviv, Israel\n\nof California Santa Cruz Santa Cruz, CA, USA\n\nAbstract\nWe present MUSE, a software framework for combining existing computational\ntools for different astrophysical domains into a single multiphysics, multiscale application. MUSE facilitates the coupling of existing codes written in different languages\nby providing inter-language tools and by specifying an interface between each module and the framework that represents a balance between generality and computa-\n\nPreprint submitted to Elsevier PreprintAccepted 2008 ???. Received 2008 ???; in original form 2008 ???\n\n\ftional efficiency. This approach allows scientists to use combinations of codes to solve\nhighly-coupled problems without the need to write new codes for other domains or\nsignificantly alter their existing codes. MUSE currently incorporates the domains of\nstellar dynamics, stellar evolution and stellar hydrodynamics for studying generalized stellar systems. We have now reached a \"Noah's Ark\" milestone, with (at least)\ntwo available numerical solvers for each domain. MUSE can treat multi-scale and\nmulti-physics systems in which the time- and size-scales are well separated, like simulating the evolution of planetary systems, small stellar associations, dense stellar\nclusters, galaxies and galactic nuclei. In this paper we describe three examples calculated using MUSE: the merger of two galaxies, the merger of two evolving stars,\nand a hybrid N -body simulation. In addition, we demonstrate an implementation of\nMUSE on a distributed computer which may also include special-purpose hardware,\nsuch as GRAPEs or GPUs, to accelerate computations. The current MUSE code\nbase is publicly available as open source at http://muse.li.\n\n1\n\nIntroduction\n\nThe Universe is a multi-physics environment in which, from an astrophysical\npoint of view, Newton's gravitational force law, radiative processes, nuclear\nreactions and hydrodynamics mutually interact. The astrophysical problems\nwhich are relevant to this study generally are multi-scale, with spatial and\ntemporal scales ranging from 104 m and 10\u22123 seconds on the small end to\n1020 m and 1017 s on the large end. The combined multi-physics, multi-scale\nenvironment presents a tremendous theoretical challenge for modern science.\nWhile observational astronomy fills important gaps in our knowledge by harvesting ever-wider spectral coverage with continuously increasing resolution\nand sensitivity, our theoretical understanding lags behind these exciting developments in instrumentation.\nIn many ways, computational astrophysics lies intermediate between observations and theory. Simulations generally cover a wider range of physical phenomena than observations with individual telescopes, whereas purely theoretical studies are often tailored to solving sets of linearized differential equations.\nAs soon as these equations show emergent behavior in which the mutual coupling of non-linear processes result in complex behavior, the computer provides\nan enormous resource for addressing these problems. Furthermore simulations\ncan support observational astronomy by mimicking observations and aiding\ntheir interpretation by enabling parameter-space studies. They can elucidate\nthe often complex consequences of even simple physical theories, like the nonlinear behavior of many-body gravitational systems. But in order to deepen\nour knowledge of the physics, extensive computer simulations require large\nprogramming efforts and a thorough fundamental understanding of many aspects of the underlying theory.\n2\n\n\fFrom a management perspective, the design of a typical simulation package\ndiffers from construction of a telescope in one very important respect. Whereas\nmodern astronomical instrumentation is generally built by teams of tens or\nhundreds of people, theoretical models are usually one-person endeavors. Pure\ntheory lends itself well to this relatively individualistic approach, but scientific\ncomputing is in a less favorable position. So long as the physical scope of a\nproblem remains relatively limited, the necessary software can be built and\nmaintained by a single numerically educated astronomer or scientific programmer. However, these programs are often \"single-author, single-use\", and thus\nsingle-purpose: recycling of scientific software within astronomy is still rare.\nMore complex computer models often entail non-linear couplings between\nmany distinct, and traditionally separate, physical domains. Developing a simulation environment suitable for multi-physics scientific research is not a simple task. Problems which encompass multiple temporal or spatial scales are\noften coded by small teams of astronomers. Many recent successful projects\nhave been carried out in this way; examples are GADGET (Springel et al.,\n2001), and starlab (Portegies Zwart et al., 2001). In all these cases, a team\nof scientists collaborated in writing a large-scale simulation environment. The\nresulting software has a broad user base, and has been applied to a wide variety of problems. These packages, however, each address one quite specific\ntask, and their use is limited to a rather narrow physical domain. In addition,\nconsiderable expertise is needed to use them and expanding these packages\nto allow the addition of a new physical domain is hampered by early design\nchoices.\nIn this paper we describe a software framework that targets multi-scale, multiphysics problems in a hierarchical and internally consistent implementation.\nIts development is based on the philosophy of \"open knowledge\" 1 . We call\nthis environment MUSE, for MUltiphysics Software Environment.\n\n2\n\nThe concept of MUSE\n\nThe development of MUSE began during the MODEST-6a 2 workshop in\nLund, Sweden (Davies et al., 2006), but the first lines of code were written\nduring MODEST-6d/e in Amsterdam (the Netherlands). The idea of Noah's\nArk (see \u00a7 2.1) was conceived and realized in 2007, during the MODEST-7f\n1\n\nSee for example http://www.artcompsci.org/ok/.\nMODEST stands for MOdeling DEnse STellar Systems; the term was coined\nduring the first MODEST meeting in New York (USA) in 2001. The MODEST web\npage is http://www.manybody.org/modest; see also Hut et al. (2003); Sills et al.\n(2003).\n2\n\n3\n\n\fMUSE\nFlow control layer (scripting language)\nInterface layer (scripting and high level languages)\nGas/hydro\ndynamics\n\nhydro\nlegacy code\n\nRadiative\ntransfer\n\nRT\nlegacy code\n\nStellar\nevolution\n\nstellar/binary\nevolution\nlegacy code\n\nGravitational\ndynamics\n\nstellar\ndynamics\nlegacy code\n\nFig. 1. Basic structure design of the framework (MUSE). The top layer (flow control)\nis connected to the middle (interface layer) which controls the command structure\nfor the individual applications. In this example only an indicative selection of numerical techniques is shown for each of the applications.\n\nworkshop in Amsterdam and the MODEST-7a meeting in Split (Croatia).\nThe development of a multi-physics simulation environment can be approached\nfrom a monolithic or from a modular point of view. In the monolithic approach\na single numerical solver is systematically expanded to include more physics.\nBasic design choices for the initial numerical solver are generally petrified in\nthe initial architecture. Nevertheless, such codes are sometimes successfully\nredesigned to include two or possibly even three solvers for a different physical phenomenon (see FLASH, where hydrodynamics has been combined with\nmagnetic fields Fryxell et al., 2000)). Rather than forming a self-consistent\nframework, the different physical domains in these environments are made to\nco-exist. This approach is prone to errors, and the resulting large simulation\npackages often suffer from bugs, redundancy in source code, sections of dead\ncode, and a lack of homogeneity. The assumptions needed to make these codes\ncompile and operate without fatal errors often hampers the science. In addition, the underlying assumptions are rarely documented, and the resulting\nscience is often hard to interpret.\nWe address these issues in MUSE by the development of a modular numerical\nenvironment, in which independently developed specialized numerical solvers\nare coupled at a meta level, resulting in the coherent framework as depicted\n4\n\n\fin Fig. 1. Modules are designed with well defined interfaces governing their\ninteraction with the rest of the system. Scheduling of, and communication\namong modules is managed by a top-level \"glue\" language. In the case of\nMUSE, this glue language is Python, chosen for its rich feature set, ease of\nprogramming, object-oriented capabilities, large user base, and extensive userwritten software libraries. However, we have the feeling that Python is not\nalways consistent and of equally high quality in all places. The objective of\nthe glue code is to realize the interoperation between different parts of the\ncode, which may be realized via object-relational mapping, in which individual\nmodules are equipped with instruction sets to exchange information with other\nmodules.\nThe modular approach has many advantages. Existing codes which have been\nwell tuned and tested within their own domains can be reused by wrapping\nthem in a thin interface layer and incorporating them into a larger framework.\nThe identification and specification of suitable interfaces for such codes allows\nthem to be interchanged easily. An important element of this design is the\nprovision of documentation and exemplars for the design of new modules and\nfor their integration into the framework. A user can \"mix and match\" modules\nlike building blocks to find the most suitable combination for the application\nat hand, or to compare them side by side. The first interface standard between stellar evolution and stellar dynamics goes back to Hut et al. (2003).\nThe resulting software is also more easily maintainable, since all dependencies\nbetween a module and the rest of the system are well defined and documented.\nA particular advantage of a modular framework is that a motivated scholar\ncan focus attention on a narrower area, write a module for it, then integrate it\ninto the framework with knowledge of only the bare essentials of the interface.\nFor example, it would take little extra work to adapt the results of a successful\nstudent project into a separate module, or for a researcher working with a code\nin one field of physics to find out how the code interacts in a multi-physics\nenvironment. The shallower learning curve of the framework significantly lowers the barrier for entry, making it more accessible and ultimately leading to\na more open and extensible system.\nThe only constraint placed on a new module is that it must be written (or\nwrapped) in a programming language with a Foreign Function Interface that\ncan be linked to a contemporary Unix-like system. As in the high-level language Haskell, a Foreign Function Interface provide a mechanism by which\na program written in one language can call routines from another language.\nSupported languages include low-level (C, C++ and Fortran) as well as other\nhigh-level languages such as C#, Java, Haskell, Python and Ruby. Currently,\nindividual MUSE modules are written in Fortran, C, and C++, and are interfaced with Python using f2py or swig. Several interfaces are written almost\nentirely in Python, minimizing the programming burden on the legacy pro5\n\n\fgrammer. The flexibility of the framework allows a much broader range of\napplications to be prototyped, and the bottom-up approach makes the code\nmuch easier to understand, expand and maintain. If a particular combination\nof modules is found to be particularly suited to an application, greater efficiency can be achieved by hard coding the interfaces and factoring out the\nglue code, thus effectively ramping up to a specialized monolithic code.\n\n2.1 Noah's Ark\nInstead of writing all new code from scratch, in MUSE we realized a software\nframework in which the glue language provides an object-relational mapping\nvia a virtual library which is used to bind a wide collection of diverse applications.\nMUSE consists of a hierarchical component architecture that encapsulates\ndynamic shared libraries for simulating stellar evolution, stellar dynamics and\ntreatments for colliding stars. As part of the MUSE specification, each module\nmanages its own internal (application-specific) data, communicating through\nthe interface only the minimum information needed for it to interoperate with\nthe rest of the system. Additional packages for file I/O, data analysis and\nplotting are included. Our objective is eventually to incorporate treatments\nof gas dynamics and radiative transfer, but at this point these are not yet\nimplemented.\nWe have so far included at least two working packages for each of the domains\nof stellar collisions, stellar evolution and stellar dynamics, in what we call the\n\"Noah's Ark\" milestone. The homogeneous interface that connects the kernel\nmodules enables us to switch packages at runtime via the scheduler. The goal\nof this paper is to demonstrate the modularity and interchangeability of the\nMUSE framework. In Tab. 1 we give an overview of the currently available\nmodules in MUSE.\n\n2.1.1 Stellar dynamics\nTo simulate gravitational dynamics (e.g. between stars and/or planets), we\nincorporate packages to solve Newton's equations of motion by means of gravitational N-body solvers. Several distinct classes of N-body kernels are currently available. These are based on the direct force evaluation methods or\ntree codes.\nCurrently four direct N-body methods are incorporated, all of which are based\non the fourth-order Hermite predictor-corrector N-body integrator, with block\ntime steps (Makino & Aarseth, 1992). Some of them can benefit from special6\n\n\fTable 1\nModules currently present (or in preparation) in MUSE. The codes are identified\nby their acronym, which is also used on the MUSE repository at http://muse.li,\nfollowed by a short description. Some of the modules mentioned here are used in\n\u00a7 3. Citations to the literature are indicated in the second column by their index\n1:Eggleton et al. (1989), 2:Eggleton (2006), 3:Hut et al. (1995), 4:Makino & Aarseth\n(1992), 5:Harfst et al. (2007), 6:Barnes & Hut (1986), 7:Lombardi et al. (2003);\n8:Rycerz et al. (2008b,a); 9:Fregeau et al. (2002, 2003); 10:Fujii et al. (2007). For a\nnumber of modules the source code is currently not available within MUSE because\nthey are not publicly available or still under development. Those are the Henyey\nstellar evolution code EVTwin (Eggleton, 1971, 2006), the Monte-Carlo dynamics\nmodule cmc (Joshi et al., 2000; Fregeau et al., 2003), the hybrid N -body integrator\nBRIDGE (Fujii et al., 2007, used in \u00a7 3.3) and the Monte-Carlo radiative transfer code\nMCRT.\nMUSE module\n\nref.\n\nlanguage\n\nbrief description\n\nEFT89\n\n1\n\nC\n\nParameterized stellar evolution\n\nEVTwin\n\n2\n\nF77/F90\n\nHenyey code to evolve stars\n\nHermite0\n\n3\n\nC++\n\nDirect N -body integrator\n\nNBODY1h\n\n4\n\nF77\n\nDirect N -body integrator\n\nphiGRAPE\n\n5\n\nF77\n\n(parallel) direct N -body integrator\n\nBHTree\n\n6\n\nC++\n\nBarnes-Hut tree code\n\nSmallN\n\nC++\n\nDirect integrator for systems of few bodies\n\nSticky Spheres\n\nC++\n\nAngular momentum and energy conserving collision treatment\n\nF77\n\nEntropy sorting for merging two stellar structures\n\nMCRT\n\nC++\n\nMonte-Carlo Radiative Transfer\n\nGlobus support\n\nPython\n\nSupport for performing simulations on distributed resources\n\nHLA\n\nSupport for performing simulations on distributed resources\n\nScheduler\n\nPython\n\nSchedules the calling sequence between modules\n\nUnit module\n\nPython\n\nUnit conversion\n\nXML parser\n\nPython\n\nPrimitive parser for XML formatted data\n\nmmas\n\nHLA grid support\n\n7\n\n8\n\ncmc\n\n9\n\nC\n\nMonte Carlo stellar dynamics module\n\nBRIDGE\n\n10\n\nC++\n\nHybrid direct N -body with Barnes-Hut Tree code\n\n7\n\n\fpurpose hardware such as GRAPE (Makino & Taiji, 1998; Makino, 2001) or\na GPU (Portegies Zwart et al., 2007; Belleman et al., 2008). Direct methods\nprovides the high accuracy needed for simulating dense stellar systems, but\neven with special computer hardware they lack the performance to simulate\nsystems with more than \u223c 106 particles. The Barnes-Hut tree-codes (Barnes\n& Hut, 1986) are included for use in simulations of large-N systems. Two of\nthe four codes are also GRAPE/GPU-enabled.\n\n2.1.2 Stellar evolution\nMany applications require the structure and evolution of stars to be followed\nat various levels of detail. At a minimum, the dynamical modules need to\nknow stellar masses and radii as functions of time, since these quantities feed\nback into the dynamical evolution. At greater levels of realism, stellar temperatures and luminosities (for basic comparison with observations), photon\nenergy distributions (for feedback on radiative transfer), mass loss rates, outflow velocities and yields of various chemical elements (returned to the gas in\nthe system), and even the detailed interior structure (to follow the outcome of\na stellar merger or collision), are also important. Consequently the available\nstellar evolution modules should ideally include both a very rapid but approximate code for applications where speed (enabling large numbers of stars) is\nparamount (e.g. when using the Barnes-Hut tree code to follow the stellar dynamics) as well as a detailed (but much slower) structure and evolution code\nwhere accuracy is most important (for example when studying specific objects\nin relatively small but dense star clusters).\nCurrently two stellar evolution modules are incorporated into MUSE. One,\ncalled EFT89, is based on fits to pre-calculated stellar evolution tracks (Eggleton et al., 1989), the other solves the set of coupled partial differential equations that describe stellar structure and evolution following the Henyey code\noriginally designed by Eggleton (1971). The latter code, called EVTwin is a\nfully implicit stellar evolution code that solves the stellar structure equations\nand the reaction-diffusion equations for the six major isotopes concurrently on\nan adaptive mesh (Glebbeek et al., 2008). EVTwin is designed to follow in detail the internal evolution of a star of arbitrary mass. The basic code, written\nin Fortran 77/90, operates on a single star-that is, the internal data structures (Fortran common blocks) describe just one evolving object. The EVTwin\nvariant describes a pair of stars, the components of a binary, and includes\nthe possibility of mass transfer between them. A single star is modeled as a\nprimary with a distant, non-evolving secondary. The lower speed of EVTwin is\ninconvenient, but the much more flexible description of the physics allows for\nmore realistic treatment of unconventional stars, such as collision products.\nOnly two EVTwin functions-the \"initialize\" and \"evolve\" operations-are ex8\n\n\fposed to the MUSE environment. The F90 wrapper also is minimal, providing\nonly data storage and the commands needed to swap stellar models in and\nout of EVTwin and to return specific pieces of information about the stored\ndata. All other high-level control structures, including identification of stars\nand scheduling their evolution, is performed in a python layer that forms the\nouter shell of the EVTwin interface. The result is that the structure and logic of\nthe original code is largely preserved, along with the expertise of its authors.\n\n2.1.3 Stellar collisions\nPhysical interactions between stars are currently incorporated into MUSE\nby means of one of two simplified hydrodynamic solvers. The simpler of the\ntwo is based on the \"sticky sphere\" approximation, in which two objects\nmerge when their separation becomes less than the sum of their effective\nradii. The connection between effective and actual radius is calibrated using\nmore realistic SPH simulations of stellar collisions. The second is based on the\nmake-me-a-star (MMAS) package 3 (Lombardi et al., 2003) and its extension\nmake-me-a-massive-star 4 (MMAMS, Gaburov et al. (2008)). MMA(M)S\nconstructs a merged stellar model by sorting the fluid elements of the original\nstars by entropy or density, then recomputing their equilibrium configuration,\nusing mass loss and shock heating data derived from SPH calculations. Ultimately, we envisage inclusion of a full SPH treatment of stellar collisions into\nthe MUSE framework.\nMMAS (and MMAMS) can be combined with full stellar evolution models,\nas they process the internal stellar structure in a similar fashion to the stellar\nevolution codes. The sticky sphere approximation only works with parameterized stellar evolution, as it does not require any knowledge of the internal\nstellar structure.\n\n2.1.4 Radiative transfer\nAt this moment one example module for performing rudimentary radiative\ntransfer calculations is incorporated in MUSE. The module uses a discrete\ngrid of cells filled with gas or dust which is parameterized in aR local density \u03c1\nand an opacity \u03ba, with which we calculate the optical depth ( \u03c1\u03badx). A star,\nthat may or may not be embedded in one of the grid cells emits L photons,\neach of which is traced through the medium until it is absorbed, escapes or\nlands in the camera. In each cloud cell or partial cell a photon has a finite\nprobability that it is scattered or absorbed. This probability is calculated by\nsolving the scattering function f , which depends on the angles and the Stokes\n3\n4\n\nSee http://webpub.allegheny.edu/employee/j/jalombar/mmas/\nSee http://modesta.science.uva.nl/\n\n9\n\n\fparameter. We adopt electron scattering for gas and Henyey & Greenstein\n(1941) for dust scattering (see (Ercolano et al., 2005) for details).\nSince this module is in a rather experimental stage we only present two images\nof its working, rather than a more complete description in \u00a7 3. In Fig. 2 we\npresent the result of a cluster simulation using 1024 stars which initially were\ndistributed in a Plummer (1911) sphere with a virial radius of 1.32 pc and\nin which the masses of the stars ware selected randomly from the Salpeter\n(Salpeter, 1955) mass function between 1 and 100 M\u2299 , totaling the cluster mass\nto about 750 M\u2299 . These parameters ware selected to mimic the Pleiades cluster\n(Portegies Zwart et al., 2001). The cluster was scaled to virial equilibrium\nbefore we started its evolution. The cluster is evolved dynamically using the\nBHTree package and the EFT89 module is used for evolving the stars.\nWe further assumed that the cluster was embedded in a giant molecular cloud\n(Williams et al., 2000). The scattering parameters were set to simulate visible\nlight. The gas and dust was distributed in a homogeneous cube with 5pc on\neach side which was divided into 1000 \u00d7 1000 \u00d7 100 grid cells with a density\nof 102 H2 particles/cm3 .\nIn Fig. 2 we present the central 5 pc of the cluster at an age of 120 Myr. The\nluminosity and position of the stars are observed from the z-axis, i.e. they are\nprojected on the xy-plane. In the left panel we present the stellar luminosity\ncolor coded, and the size of the symbols reflects the distance from the observer,\ni.e., there it gives an indication of how much gas is between the star and the\nobserver. The right image is generated using the MCRT module in MUSE and\nshows the photon-packages which were traced from the individual stars to the\ncamera position. Each photon-package represents a multitude of photons.\n2.2 Units\nA notorious pitfall in combining scientific software is the failure to perform\ncorrect conversion of physical units between modules. In a highly modular\nenvironment such as MUSE, this is a significant concern. One approach to\nthe problem could have been to insist on a standard set of units for modules\nincorporated into MUSE but this is neither practical nor in keeping with the\nMUSE philosophy.\nInstead, in the near future, we will provide a Units module in which information about the specific choice of units the conversion factors between them\nand certain useful physical constants are collected. When a module is added\nto MUSE, the programmer adds a declaration of the units it uses and expects.\nWhen several modules are imported into a MUSE experiment, the Units module then ensures that all external values passed to each module are properly\n10\n\n\fFig. 2. Radiative transfer module applied to a small N = 1024 particle Plummer\nsphere. Left image shows the intrinsic stellar luminosity at an age of 120 Myr, the\nright image the image after applying the radiative transfer module for the cluster\nin a molecular cloud using a total of 107 photon-packages. The bar to the right of\neach frame indicates the logarithm of the luminosity of the star (left image) and the\nlogarithm of the number of photons-packages that arrived in that particular pixel.\n\nconverted.\nNaturally, the flexibility afforded by this approach also introduces some overhead. However, this very flexibility is MUSE's great advantage; it allows an\nexperimenter to easily mix and match modules until the desired combination\nis found. At that point, the dependence on the Units module can be removed\n(if desired), and conversion of physical units performed by explicit code. This\nleads to more efficient interfaces between modules, while the correctness of the\nmanual conversion can be checked at any time against the Units module.\n\n2.3 Performance\nLarge scale simulations, and in particular the multiscale and multiphysics\nsimulations for which our framework is intended, require a large number of\nvery different algorithms, many of which achieve their highest performance\non a specific computer architecture. For example, the gravitational N-body\nsimulations are best performed on a GRAPE enabled PC, the hydrodynamical\nsimulations may be efficiently accelerated using GPU hardware, while the\ntrivially parallel simultaneous modeling of a thousand single stars is best done\non a Beowulf cluster or grid computer.\nThe top-level organization of where each module should run is managed using\na resource broker, which is grid enabled (see \u00a7 2.4). We include a provision for\nindividual packages to indicate the class of hardware on which they operate\noptimally. Some modules are individually parallelized using the MPI library,\n11\n\n\fwhereas others (like stellar evolution) are handled in a master-slave fashion\nby the top-level manager.\n\n2.4 MUSE on the grid\n\nDue to the wide range in computational characteristics of the available modules, we generally expect to run MUSE on a computational grid containing a\nnumber of specialized machines. In this way we reduce the run time by adopting computers which are best suited to each module. For example, we might\nselect a large GRAPE cluster in Tokyo for a direct N-body calculation, while\nthe stellar evolution is calculated on a Beowulf cluster in Amsterdam. Here\nwe report on our preliminary grid interface, which allows us to use remote\nmachines to distribute individual MUSE modules.\nThe current interface uses the MUSE scheduler as the manager of grid jobs and\nreplaces internal module calls with a job execution sequence. This is implemented with PyGlobus, an application programming interface to the Globus\ngrid middleware written in Python. The execution sequence for each module\nconsists of:\n\u2022 Write the state of a module, such as its initial conditions, to a file,\n\u2022 transfer the state file to the destination site\n\u2022 construct a grid job definition using the Globus resource specification language\n\u2022 submit the job to the grid; the grid job subsequently\n- reads the state file,\n- executes the specified MUSE module,\n- writes the new state of the module to a file, and\n- copies the state file back to the MUSE scheduler\n\u2022 then read the new state file and resume the simulation.\nThe grid interface has been tested successfully using DAS-3 5 , which is a fivecluster wide-area (in the Netherlands) distributed system designed by the\nAdvanced School for Computing and Imaging (ASCI). We executed individual invocations of stellar dynamics, stellar evolution, and stellar collisions on\nremote machines.\n\n5\n\nsee http://www.cs.vu.nl/das3/\n\n12\n\n\fFig. 3. Time evolution of the distance between two black holes, each of which initially\nresides in the center of a \"galaxy,\" made up of 32k particles, with total mass 100\ntimes greater than the black hole mass. Initially the two galaxies were far apart.\nThe curves indicate calculations with the direct integrator (PP), a tree code (TC),\nand using the hybrid method in MUSE (PP+TC). The units along the axes are\ndimensionless N -body units (Heggie & Mathieu, 1986).\n\n3\n\nMUSE examples\n\n3.1 Temporal decomposition of two N-body codes\n\nHere we demonstrate the possibility of changing the integration method within\na MUSE application during runtime. We deployed two integrators to simulate\nthe merging of two galaxies, each containing a central black hole. The final\nstages of such a merger, with two black holes orbiting one another, can only\nbe integrated accurately using a direct method. Since this is computationally\nexpensive, the early evolution of such a merger is generally ignored and these\ncalculations are typically started some time during the merger process, for\nexample when the two black holes form a hard bound pair inside the merged\ngalaxy.\nThese rather arbitrary starting conditions for the binary black hole merger\nproblem can be improved by integrating the initial merger between the two\ngalaxies. We use the BHTree code to reduce the computational cost of sim13\n\n\fulating this merger process. At a predetermined distance between the two\nblack holes, when the tree code is unlikely to produce accurate results, the\nsimulation is continued using the direct integration method for all particles.\nOverall this results in a considerable reduction in runtime while still allowing\nan accurate integration of the close black hole interaction.\nFig. 3 shows the results of such a simulation. The initial conditions are two\nPlummer spheres, each consisting of 32k equal-mass particles. At the center of\neach \"galaxy\" we place a black hole with mass 1% that of the galaxy. The two\nstellar systems are subsequently set on a collision orbit, but at a fairly large\nseparation. We use two stellar dynamics modules (see \u00a72.1): BHTree (Barnes\n& Hut, 1986), with a fixed shared time step, and phiGRAPE (Harfst et al.,\n2007), a direct method using hierarchical block time steps. Both modules\nare GRAPE-enabled and we make use of this to speed up the simulation\nsignificantly. The calculation is performed three times, once using phiGRAPE\nalone, once using only BHTree, and once using the hybrid method. In the latter\ncase the equations of motion are integrated using phiGRAPE if the two black\nholes are within \u223c 0.55 N-body units 6 (indicated by the horizontal dashed\nline in Fig. 3); otherwise we use the tree code. Fig. 3 shows the time evolution\nof the distance between the two black holes.\nThe integration with only the direct phiGRAPE integrator took about 250 minutes, while the tree code took about 110 minutes. As expected, the relative\nerror in the energy of the direct N-body simulation (< 10\u22126 ) is orders of magnitude smaller than the error in the tree code (\u223c 1%). The hybrid code took\nabout 200 minutes to finish the simulation, with an energy error a factor of\n\u223c 10 better than that of the tree code. If we compare the time evolution of\nthe black hole separation for the tree and the hybrid code we find that the\nhybrid code reproduces the results of the direct integration (assuming it to\nbe the most \"correct\" solution) quite well. In summary, the hybrid method\nseems to be well suited to this kind of problem, as it produces more accurate\nresults than the tree code alone and it is also faster than the direct code. The\ngain in performance is not very large (only \u223c 20%) for the particular problem\nstudied here, but as the compute time for the tree code scales with N log N\nwhereas the direct method scales with N 2 ; a better gain is to be expected for\nlarger N. In addition, the MUSE implementation of the tree code is rather\nbasic, and both its performance and its accuracy could be improved by using\na variable block time step.\n\n6\n\nsee http://en.wikipedia.org/wiki/Natural units#N-body units.\n\n14\n\n\fFig. 4. Evolution of a merger product formed by a collision between a 10M\u2299 star at\nthe end of its main-sequence lifetime and a 7M\u2299 star of the same age (filled circles),\ncompared to the track of normal star of the same mass (15.7 M\u2299 ) (triangles). A\nsymbol is plotted every 5 \u00d7 104 yr. Both stars start their evolution at the left of the\ndiagram (around Teff \u2243 3 \u00d7 104 K).\n\n3.2 Stellar mergers in MUSE\n\nHydrodynamic interactions such as collisions and mergers can strongly affect\nthe overall energy budget of a dense stellar cluster and even alter the timing of\nimportant dynamical phases such as core collapse. Furthermore, stellar collisions and close encounters are believed to produce a number of non-canonical\nobjects, including blue stragglers, low-mass X-ray binaries, recycled pulsars,\ndouble neutron star systems, cataclysmic variables and contact binaries. These\nstars and systems are among the most challenging to model and are also among\nthe most interesting observational markers. Predicting their numbers, distributions and other observable characteristics is essential for detailed comparisons\nwith observations.\nWhen the stellar dynamics module in MUSE identifies a collision, the stellar\nevolution module provides details regarding the evolutionary state and structure of the two colliding stars. This information is then passed on to the stellar\ncollision module, which calculates the structure of the merger remnant, returning it to the stellar evolution module, which then continues its evolution. This\ndetailed treatment of stellar mergers requires a stellar evolution module and\n15\n\n\fa collision treatment which resolve the internal structure of the stars; there is\nno point in using a sticky-sphere approach in combination with a Henyey-type\nstellar evolution code.\nFig. 4 presents the evolutionary track of a test case in which EVTwin (Eggleton,\n1971) (generally the more flexible TWIN code is used, which allows the evolution\nof two stars in a binary) was used to evolve the stars in our experiment. A\n10 M\u2299 star near the end of its main-sequence collided with a 7 M\u2299 star of\nthe same age. The collision itself was resolved using MMAMS. The evolution\nof the resulting collision product continued using EVTwin, which is presented\nas the solid curve in Fig. 4. For comparison we also plot (dashed curve) the\nevolutionary track of a star with the same mass as the merger product. The\nevolutionary tracks of the two stars are quite different, as are the timescales on\nwhich the stars evolve after the main sequence and through the giant branch.\nThe normal star becomes brighter as it follows an ordinary main-sequence\ntrack, whereas the merged star fades dramatically as it re-establishes thermal equilibrium shortly after the collision. The initial evolution of the merger\nproduct is numerically difficult, as the code attempts to find an equilibrium\nevolutionary track, which is hard because the merger product has no hydrogen\nin its core. As a consequence, the star leaves the main-sequence almost directly\nafter it establishes equilibrium, but since the core mass of the star is unusually\nsmall (comparable to that of a 10 M\u2299 star at the terminal-age main sequence)\nit is under luminous compared to the normal star. The slight kink in the evolutionary track between log10 Teff = 4.2 and 4.3 occurs when the merger product\nstarts to burn helium in its core. The star crosses the Hertzsprung gap very\nslowly (in about 1 Myr), whereas the normal star crosses within a few 10,000\nyears. This slow crossing is caused by the small core of the merger product,\nwhich first has to grow to a mass to be consistent with a \u223c 15.7 M\u2299 star\nbefore it can leave the Hertzsprung gap. The episode of extended Hertzsprung\ngap lifetime is interesting as observing an extended lifetime Hertzsprung gap\nstar is much more likely than witnessing the actual collision. Observing a star\non the Hertzsprung gap with a core too low in mass for its evolutionary phase\nwould therefore provide indirect evidence for the collisional history of the star\n(regretfully one would probably require some stellar stethoscope to observe\nthe stellar core in such a case).\n\n3.3 Hybrid N-body simulations with stellar evolution\n\nDense star clusters move in the potential of a lower density background. For\nglobular clusters this is the parent's galaxy halo, for open star clusters and\ndense young clusters it is the galactic disc, and nuclear star clusters are embedded in their galaxy's bulge. These high-density star clusters are preferably\n16\n\n\fmodeled using precise and expensive direct-integration methods. For the relatively low density regimes, however, a less precise method would suffice; saving\na substantial amount of compute time and allowing a much larger number of\nparticles to simulate the low-density host environment. In \u00a7 3.1 we described\na temporal decomposition of a problem using a tree code O(N log(N)) and a\ndirect N-body method. Here we demonstrate a spatial domain decomposition\nusing the same methods.\nThe calculations performed in this \u00a7 are run via a MUSE module which is\nbased on BRIDGE (Fujii et al., 2007). Within BRIDGE a homogeneous and\nseamless transition between the different numerical domains is possible with a\nsimilar method as is used in the mixed-variable symplectic method (Kinoshita\net al., 1991; Wisdom & Holman, 1991), in which the Hamiltonian is divided\ninto two parts: an analytic Keplerian part and the individual interactions\nbetween particles. The latter are used to perturb the regular orbits. In our\nimplementation the accurate direct method, used to integrate the high-density\nregions, is coupled to the much faster tree-code, which integrates the lowdensity part of the galaxies. The stars in the high-density regions are perturbed\nby the particles in the low-density environment.\nThe method implemented in MUSE and presented here uses an accurate direct\nN-body solver (like Hermite0) for the high density regime whereas the rest of\nthe system is integrated using BHTree. In principle, the user is free to choose\nthe integrator used in the accurate part of the integration; in our current\nimplementation we adopt Hermite0, but the tree-code is currently petrified in\nthe scheduler. This version of BRIDGE is currently not available in the public\nversion of MUSE.\nTo demonstrate the working of this hybrid scheme we simulate the evolution\nof a star cluster orbiting within a galaxy. The star cluster is represented by\n8192 particles with a Salpeter (Salpeter, 1955) mass function between 1 and\n100 M\u2299 distributed according to a W0 = 10 King model (King, 1966) density\nprofile. This cluster is located at a distance of 16 pc from the center of the\ngalaxy, with a velocity of 65 km s\u22121 in the transverse direction. The galaxy\nis represented by 106 equal-mass particles in a W0 = 3 King model density\ndistribution. The stars in the star cluster are evolved using the MUSE stellar\nevolution module EFT89, the galaxy particles have all the same mass of 30 M\u2299\nand were not affected by stellar evolution.\nThe cluster, as it evolves internally, spirals in towards the galactic center\ndue to dynamical friction. While the cluster spirals in, it experiences core\ncollapse. During this phase many stars are packed together in the dense cluster\ncore and stars start to collide with each other in a collision runaway process\n(Portegies Zwart et al., 1999). These collisions are handled internally in the\ndirect part of BRIDGE. Throughout the core collapse of the cluster about a\n17\n\n\fFig. 5. Results of the hybrid N -body simulation using a 4th-order Hermite scheme\nfor the particles integrated directly and a Barnes-Hut tree algorithm for the others.\nThe top left panel: distance from the cluster to the Galactic center, top right:\nevolution of the cluster core radius, bottom left: bound cluster mass, bottom right:\nevolution of the mass of a few cluster stars that happen to experience collisions.\nThe two crosses in the bottom right panel indicate the moment that two collision\nproducts coalesce with the runaway merger.\n\ndozen collisions occur with the same star, causing it to grow in mass to about\n700 M\u2299 . Although the stellar evolution of such collision products is highly\nuncertain (Belkus et al., 2007; Suzuki et al., 2007), we assume here that the\nmassive star collapses to a black hole of intermediate mass.\nThe direct part as well as the tree-part of the simulation was performed on a\nfull 1 Tflops GRAPE-6 (Makino et al., 2003), whereas the tree-code was run\non the host PC. The total CPU time for this simulation was about half a day,\nwhereas without BRIDGE the run would have taken years to complete. The\nmajority (\u223c 90%) of the compute time was spent in the tree code, integrating the 106 particles in the simulated galaxy. (Note again that this fraction\ndepends on the adopted models and the use of special-purpose hardware to\naccelerate the direct part of the integration.) Total energy was conserved to\nbetter than 2 \u00d7 10\u22124 (initial total energy was -0.25).\nThe results of the simulation are presented in Fig. 5. Here we see how the\ncluster (slightly) spirals in, due to dynamical friction with the surrounding\n(tree-code) stars, toward the galactic center before dissolving at an age of\nabout 4 Myr. By that time, however, the runaway collision has already resulted\n18\n\n\fin a single massive star of more than 700 M\u2299 .\nThe description of stellar evolution adopted in this calculation is rather simple\nand does not incorporate realistic mass loss, and it is expected that the collision runaway will have a mass of \u223c 50M\u2299 by the time it collapses to a black\nhole in a supernova explosion. The supernova itself may be unusually bright\n(possibly like SN2006gy (Portegies Zwart & van den Heuvel, 2007)) and may\ncollapse to a relatively massive black hole (Portegies Zwart et al., 2004). Similar collision runaway results were obtained using direct N-body simulations\nusing starlab (Portegies Zwart & McMillan, 2002) and NBODY (Baumgardt\net al., 2004), and with Monte-Carlo (G\u00fcrkan et al., 2004; Freitag et al., 2006)\nstellar dynamics simulations.\n\n3.4 Direct N-body dynamics with live stellar evolution\n\nWhile MUSE contains many self-contained dynamical modules, all of the stellar evolution modules described thus far have relied on simple analytical formulations or lookup formulae. Here we present a new simulation combining\na dynamical integrator with a \"live\" stellar evolution code, following the detailed internal evolution of stars in real time as the dynamics unfolds. A similar\napproach has been undertaken by Ross Church, in his PhD thesis. The novel\ningredient in this calculation is a MUSE interface onto the EVTwin stellar evolution program (Eggleton, 2006), modified for use within MUSE (see \u00a7 3.2 for\na description).\nIn keeping with the philosophy of not rewriting existing working code, in\nincorporating EVTwin into MUSE, we have made minimal modifications to the\nprogram's internal structure. Instead, we wrap the program in a F90 datamanagement layer which maintains a copy of the stellar data for each star\nin the system. Advancing a system of stars simply entails copying the chosen\nstar into the program and telling it to take a step. EVTwin proceeds with the\ntask at hand, blissfully unaware that it is advancing different stellar models\nat every invocation (see \u00a7 3.2).\nFigure 6 shows four snapshots during the evolution of a 1024-body system, carried out within MUSE using EVTwin and the simple shared-timestep hermite0\ndynamical module. Initially the stars had a mass function dN/dm \u221d m\u22122.2 for\n0.25M\u2299 < m < 15M\u2299 , for a mean mass of 0.92M\u2299 and were distributed according to a Plummer density profile with a dynamical time scale of 10 Myr,\na value chosen mainly to illustrate the interplay between dynamics and stellar\nevolution. (The initial cluster half-mass radius was \u223c 15 pc.) The initial halfmass relaxation time of the system was 377 Myr. The four frames show the\nstate of the system at times 0, 200, 400, and 600 Myr, illustrating the early\n19\n\n\fFig. 6. Evolution of a 1024-body cluster, computed using the hermite0 and EVTwin\nMUSE modules. The four rows of images show the physical state of the cluster (left)\nand the cluster H\u2013R diagram (right) at times (top to bottom) 0, 200, 400, and 600\nMyr. Colors reflect stellar temperature, and radii are scaled by the logarithm of the\n20\nstellar radius.\n\n\fmass segregation and subsequent expansion of the system as stars evolve and\nlose mass.\nThe integrator was kept deliberately simple, using a softened gravitational potential to avoid the need for special treatment of close encounters, and there\nwas no provision for stellar collisions and mergers. Both collisions and close\nencounters will be added to the simulation, and described in a future paper.\nWe note that, although the hermite0 module is the least efficient member of\nthe MUSE dynamical suite, nevertheless the CPU time taken by the simulation was roughly equally divided between the dynamical and stellar modules.\nEven without hardware acceleration (by GRAPE or GPU), a more efficient\ndynamical integrator (such as one of the individual block time step schemes\nalready installed in MUSE) would run at least an order of magnitude faster,\nunderscoring the need for careful load balancing when combining modules in\na hybrid environment.\n\n4\n\nDiscussion\n\nThe Multiscale Software Environment presented in this paper provides a diverse and flexible framework for numerical studies of stellar systems. Now\nthat the Noah's Ark milestone has been reached, one can ask what new challenges MUSE has to offer. Many of the existing modules have been adapted\nfor grid use and, as demonstrated in \u00a7 2.4, MUSE can be used effectively to\nconnect various computers around the world. However, there are currently a\nnumber of limitations in its use, and in its range of applications, which will be\naddressed in the future. Most of the current application modules remain unsuitable for large-scale scientific production simulations. The stellar dynamics\ncodes do not yet efficiently deal with close binaries and multiples, although\nmodules are under development, and external potentials, though relatively\neasy to implement, have not yet been incorporated. Binary evolution is not\nimplemented, and the diagnostics available to study the output of the various\nmodules remain quite limited.\nMany improvements can be made to the environment, and we expect to include\nmany new modules, some similar to existing ones, others completely different\nin nature. The current framework has no method for simulating interstellar\ngas, although this would be an extremely valuable addition to the framework,\nenabling study of gas-rich star clusters, galaxy collisions, colliding-wind binary\nsystems, etc. In addition, radiation transfer is currently not implemented, nor\nare radiative feedback mechanisms between stars and gas. Both would greatly\nincrease the applicability base of the framework. However, both are likely to\nchallenge the interface paradigm on which MUSE is based.\n21\n\n\fThe current MUSE setup, in which the individual modules are largely decoupled, has a number of attractive advantages over a model in which we allow\ndirect memory access. The downside is that MUSE in its present form works\nefficiently only for systems in which the various scales are well separated. Communication between the various modules, even of the same type, is currently\nall done via the top interface layer. For small studies, this poses relatively\nlittle overhead, but for more extensive calculations, or those in which more\ndetailed data must be shared, it is desirable to minimize this overhead. One\nway to achieve this would be by allowing direct data access between modules.\nHowever, for such cases, the unit conversion modules could not be used, and\nconsistency in the units between the modules cannot be guaranteed. As a result, each module would be required to maintain consistent units throughout,\nwhich may be hard to maintain and prone to bugs. In addition, the general\nproblem of sharing data structures between modules written in different languages, currently resolved by the use of the glue language, resurfaces.\n\nAcknowledgments\n\nWe are grateful to Atakan G\u00fcrkan, Junichiro Makino, Stephanie Rusli and Dejan Vinkovi\u0107 for many discussions. Our team meetings have been supported\nby the Yukawa Institute for Theoretical Physics in Kyoto, the International\nSpace Science Institute in Bern, the department of astronomy of the university of Split in Split, the Institute for Advanced Study in Princeton and the\nAstronomical Institute 'Anton Pannekoek' in Amsterdam. This research was\nsupported in part by the Netherlands Organization for Scientific Research\n(NWO grant No. 635.000.001 and 643.200.503), the Netherlands Advanced\nSchool for Astronomy (NOVA), the Leids Kerkhoven-Bosscha fonds (LKBF),\nthe ASTROSIM program of the European Science Foundation, by NASA ATP\ngrants NNG04GL50G and NNX07AH15G, by the National Science Foundation under grants AST-0708299 (S.L.W.M.) and PHY-0703545 (J.C.L.), by the\nSpecial Coordination Fund for Promoting Science and Technology (GRAPEDR project), the Japan Society for the Promotion of Science (JSPS) for Young\nScientists, Ministry of Education, Culture, Sports, Science and Technology,\nJapan and DEISA. Some of the calculations were done on the LISA cluster\nand the DAS-3 wide-area computer in the Netherlands. We are also grateful\nto SARA computing and networking services, Amsterdam for their support.\n\nReferences\nBarnes, J., Hut, P. 1986, Nat , 324, 446\nBaumgardt, H., Makino, J., Ebisuzaki, T. 2004, ApJ , 613, 1143\n22\n\n\fBelkus, H., Van Bever, J., Vanbeveren, D. 2007, ApJ , 659, 1576\nBelleman, R. G., B\u00e9dorf, J., Portegies Zwart, S. F. 2008, New Astronomy, 13,\n103\nDavies, M. B., Amaro-Seoane, P., Bassa, C., Dale, J., de Angeli, F., Freitag,\nM., Kroupa, P., Mackey, D., Miller, M. C., Portegies Zwart, S. 2006, New\nAstronomy, 12, 201\nEggleton, P. 2006, Evolutionary Processes in Binary and Multiple Stars, ISBN\n0521855578, Cambridge University Press.\nEggleton, P. P. 1971, MNRAS , 151, 351\nEggleton, P. P., Fitchett, M. J., Tout, C. A. 1989, ApJ , 347, 998\nErcolano, B., Barlow, M. J., Storey, P. J. 2005, MNRAS , 362, 1038\nFregeau, J. M., G\u00fcrkan, M. A., Joshi, K. J., Rasio, F. A. 2003, ApJ , 593,\n772\nFregeau, J. M., Joshi, K. J., Portegies Zwart, S. F., Rasio, F. A. 2002, ApJ ,\n570, 171\nFreitag, M., G\u00fcrkan, M. A., Rasio, F. A. 2006, MNRAS , 368, 141\nFryxell, B., Olson, K., Ricker, P., Timmes, F. X., Zingale, M., Lamb, D. Q.,\nMacNeice, P., Rosner, R., Truran, J. W., Tufo, H. 2000, ApJS , 131, 273\nFujii, M., Iwasawa, M., Funato, Y., Makino, J. 2007, Publ. Astr. Soc. Japan\n, 59, 1095\nGaburov, E., Lombardi, J. C., Portegies Zwart, S. 2008, MNRAS , 383, L5\nGlebbeek, E., Pols, O. R., Hurley, J. R. 2008, A&A , 488, 1007\nG\u00fcrkan, M. A., Freitag, M., Rasio, F. A. 2004, ApJ , 604, 632\nHarfst, S., Gualandris, A., Merritt, D., Spurzem, R., Portegies Zwart, S.,\nBerczik, P. 2007, New Astronomy, 12, 357\nHeggie, D. C., Mathieu, R. D. 1986, LNP Vol. 267: The Use of Supercomputers\nin Stellar Dynamics, in P. Hut, S. McMillan (eds.), Lecture Not. Phys 267,\nSpringer-Verlag, Berlin\nHenyey, L. G., Greenstein, J. L. 1941, ApJ , 93, 70\nHut, P., Makino, J., McMillan, S. 1995, ApJL , 443, L93\nHut, P., Shara, M. M., Aarseth, S. J., Klessen, R. S., Lombardi, Jr., J. C.,\nMakino, J., McMillan, S., Pols, O. R., Teuben, P. J., Webbink, R. F. 2003,\nNew Astronomy, 8, 337\nJoshi, K. J., Rasio, F. A., Portegies Zwart, S. 2000, ApJ , 540, 969\nKing, I. R. 1966, AJ , 71, 64\nKinoshita, H., Yoshida, H., Nakai, H. 1991, Celestial Mechanics and Dynamical\nAstronomy, 50, 59\nLombardi, J. C., Thrall, A. P., Deneva, J. S., Fleming, S. W., Grabowski, P. E.\n2003, MNRAS , 345, 762\nMakino, J. 2001, in S. Deiters, B. Fuchs, A. Just, R. Spurzem, R. Wielen\n(eds.), ASP Conf. Ser. 228: Dynamics of Star Clusters and the Milky Way,\np. 87\nMakino, J., Aarseth, S. J. 1992, Publ. Astr. Soc. Japan , 44, 141\nMakino, J., Fukushige, T., Koga, M., Namura, K. 2003, Publ. Astr. Soc.\nJapan , 55, 1163\n23\n\n\fMakino, J., Taiji, M. 1998, Scientific simulations with special-purpose computers : The GRAPE systems, Scientific simulations with special-purpose computers : The GRAPE systems /by Junichiro Makino & Makoto Taiji. Chichester ; Toronto : John Wiley & Sons, c1998.\nPlummer, H. C. 1911, MNRAS , 71, 460\nPortegies Zwart, S. F., Baumgardt, H., Hut, P., Makino, J., McMillan, S. L. W.\n2004, Nat , 428, 724\nPortegies Zwart, S. F., Belleman, R. G., Geldof, P. M. 2007, New Astronomy,\n12, 641\nPortegies Zwart, S. F., Makino, J., McMillan, S. L. W., Hut, P. 1999, A&A ,\n348, 117\nPortegies Zwart, S. F., McMillan, S. L. W. 2002, ApJ , 576, 899\nPortegies Zwart, S. F., McMillan, S. L. W., Hut, P., Makino, J. 2001, MNRAS\n, 321, 199\nPortegies Zwart, S. F., van den Heuvel, E. P. J. 2007, Nat , 450, 388\nRycerz, K., Bubak, M., Sloot, P. 2008a, in Computational Science ICCS 2008\n8th International Conference (eds. M. Bubak, G.D.v. Albada, J. Dongarra,\nP. Sloot), Krakow, Poland, Lecture Notes of Computer Science, Springer\n(2008), Vol. 5102, p. 217\nRycerz, K., Bubak, M., Sloot, P. 2008b, in Parallel Processing and Applied Mathematics 7th International Conference, PPAM 2007, (eds. R.\nWyrzykowski, J. Dongarra, K. Karczewski, J. Wasniewski), Gdansk, Poland,\nLecture Notes of Computer Science, Springer (2008), Vol. 4957, p. 780\nSalpeter, E. E. 1955, ApJ , 121, 161\nSills, A., Deiters, S., Eggleton, P., Freitag, M., Giersz, M., Heggie, D., Hurley,\nJ., Hut, P., Ivanova, N., Klessen, R. S., Kroupa, P., Lombardi, Jr., J. C.,\nMcMillan, S., Portegies Zwart, S., Zinnecker, H. 2003, New Astronomy, 8,\n605\nSpringel, V., Yoshida, N., White, S. D. M. 2001, New Astronomy, 6, 79\nSuzuki, T. K., Nakasato, N., Baumgardt, H., Ibukiyama, A., Makino, J.,\nEbisuzaki, T. 2007, ApJ , 668, 435\nWilliams, J. P., Blitz, L., McKee, C. F. 2000, Protostars and Planets IV, 97\nWisdom, J., Holman, M. 1991, AJ , 102, 1528\n\n24\n\n\f"}