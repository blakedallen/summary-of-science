{"id": "http://arxiv.org/abs/astro-ph/0612370v1", "guidislink": true, "updated": "2006-12-14T14:27:06Z", "updated_parsed": [2006, 12, 14, 14, 27, 6, 3, 348, 0], "published": "2006-12-14T14:27:06Z", "published_parsed": [2006, 12, 14, 14, 27, 6, 3, 348, 0], "title": "Multiscale Methods", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=astro-ph%2F0612547%2Castro-ph%2F0612108%2Castro-ph%2F0612412%2Castro-ph%2F0612656%2Castro-ph%2F0612669%2Castro-ph%2F0612016%2Castro-ph%2F0612736%2Castro-ph%2F0612372%2Castro-ph%2F0612385%2Castro-ph%2F0612156%2Castro-ph%2F0612136%2Castro-ph%2F0612459%2Castro-ph%2F0612752%2Castro-ph%2F0612466%2Castro-ph%2F0612449%2Castro-ph%2F0612210%2Castro-ph%2F0612323%2Castro-ph%2F0612144%2Castro-ph%2F0612310%2Castro-ph%2F0612489%2Castro-ph%2F0612002%2Castro-ph%2F0612720%2Castro-ph%2F0612014%2Castro-ph%2F0612401%2Castro-ph%2F0612434%2Castro-ph%2F0612626%2Castro-ph%2F0612543%2Castro-ph%2F0612363%2Castro-ph%2F0612756%2Castro-ph%2F0612708%2Castro-ph%2F0612004%2Castro-ph%2F0612045%2Castro-ph%2F0612537%2Castro-ph%2F0612357%2Castro-ph%2F0612370%2Castro-ph%2F0612492%2Castro-ph%2F0612022%2Castro-ph%2F0612036%2Castro-ph%2F0612766%2Castro-ph%2F0612071%2Castro-ph%2F0612686%2Castro-ph%2F0612438%2Castro-ph%2F0612172%2Castro-ph%2F0612113%2Castro-ph%2F0612152%2Castro-ph%2F0612689%2Castro-ph%2F0612165%2Castro-ph%2F0612038%2Castro-ph%2F0612673%2Castro-ph%2F0612188%2Castro-ph%2F0612041%2Castro-ph%2F0612589%2Castro-ph%2F0612389%2Castro-ph%2F0612714%2Castro-ph%2F0612755%2Castro-ph%2F0612246%2Castro-ph%2F0612320%2Castro-ph%2F0612697%2Castro-ph%2F0612249%2Castro-ph%2F0612398%2Castro-ph%2F0612265%2Castro-ph%2F0612765%2Castro-ph%2F0612226%2Castro-ph%2F0612495%2Castro-ph%2F0612435%2Castro-ph%2F0612394%2Castro-ph%2F0612067%2Castro-ph%2F0612441%2Castro-ph%2F0612193%2Castro-ph%2F0612267%2Castro-ph%2F0612773%2Castro-ph%2F0612610%2Castro-ph%2F0612430%2Castro-ph%2F0612735%2Castro-ph%2F0612209%2Castro-ph%2F0612170%2Castro-ph%2F0612690%2Castro-ph%2F0612287%2Castro-ph%2F0612399%2Castro-ph%2F0612479%2Castro-ph%2F0612216%2Castro-ph%2F0612085%2Castro-ph%2F0612130%2Castro-ph%2F0612778%2Castro-ph%2F0612187%2Castro-ph%2F0612134%2Castro-ph%2F0612049%2Castro-ph%2F0612160%2Castro-ph%2F0612604%2Castro-ph%2F0612042%2Castro-ph%2F0612470%2Castro-ph%2F0612373%2Castro-ph%2F0612650%2Castro-ph%2F0612148%2Castro-ph%2F0612327%2Castro-ph%2F0612408%2Castro-ph%2F0612775%2Castro-ph%2F0612452%2Castro-ph%2F0612198%2Castro-ph%2F0612606%2Castro-ph%2F0612579&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Multiscale Methods"}, "summary": "The lecture introduces a trous wavelet transforms, explains how to estimate\nthe spatial density for galaxy distributions, and, finally, how to describe the\nmorphology of cosmological density fields. An example application of these\nmethods to the 2dFGRS gives unexpected results.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=astro-ph%2F0612547%2Castro-ph%2F0612108%2Castro-ph%2F0612412%2Castro-ph%2F0612656%2Castro-ph%2F0612669%2Castro-ph%2F0612016%2Castro-ph%2F0612736%2Castro-ph%2F0612372%2Castro-ph%2F0612385%2Castro-ph%2F0612156%2Castro-ph%2F0612136%2Castro-ph%2F0612459%2Castro-ph%2F0612752%2Castro-ph%2F0612466%2Castro-ph%2F0612449%2Castro-ph%2F0612210%2Castro-ph%2F0612323%2Castro-ph%2F0612144%2Castro-ph%2F0612310%2Castro-ph%2F0612489%2Castro-ph%2F0612002%2Castro-ph%2F0612720%2Castro-ph%2F0612014%2Castro-ph%2F0612401%2Castro-ph%2F0612434%2Castro-ph%2F0612626%2Castro-ph%2F0612543%2Castro-ph%2F0612363%2Castro-ph%2F0612756%2Castro-ph%2F0612708%2Castro-ph%2F0612004%2Castro-ph%2F0612045%2Castro-ph%2F0612537%2Castro-ph%2F0612357%2Castro-ph%2F0612370%2Castro-ph%2F0612492%2Castro-ph%2F0612022%2Castro-ph%2F0612036%2Castro-ph%2F0612766%2Castro-ph%2F0612071%2Castro-ph%2F0612686%2Castro-ph%2F0612438%2Castro-ph%2F0612172%2Castro-ph%2F0612113%2Castro-ph%2F0612152%2Castro-ph%2F0612689%2Castro-ph%2F0612165%2Castro-ph%2F0612038%2Castro-ph%2F0612673%2Castro-ph%2F0612188%2Castro-ph%2F0612041%2Castro-ph%2F0612589%2Castro-ph%2F0612389%2Castro-ph%2F0612714%2Castro-ph%2F0612755%2Castro-ph%2F0612246%2Castro-ph%2F0612320%2Castro-ph%2F0612697%2Castro-ph%2F0612249%2Castro-ph%2F0612398%2Castro-ph%2F0612265%2Castro-ph%2F0612765%2Castro-ph%2F0612226%2Castro-ph%2F0612495%2Castro-ph%2F0612435%2Castro-ph%2F0612394%2Castro-ph%2F0612067%2Castro-ph%2F0612441%2Castro-ph%2F0612193%2Castro-ph%2F0612267%2Castro-ph%2F0612773%2Castro-ph%2F0612610%2Castro-ph%2F0612430%2Castro-ph%2F0612735%2Castro-ph%2F0612209%2Castro-ph%2F0612170%2Castro-ph%2F0612690%2Castro-ph%2F0612287%2Castro-ph%2F0612399%2Castro-ph%2F0612479%2Castro-ph%2F0612216%2Castro-ph%2F0612085%2Castro-ph%2F0612130%2Castro-ph%2F0612778%2Castro-ph%2F0612187%2Castro-ph%2F0612134%2Castro-ph%2F0612049%2Castro-ph%2F0612160%2Castro-ph%2F0612604%2Castro-ph%2F0612042%2Castro-ph%2F0612470%2Castro-ph%2F0612373%2Castro-ph%2F0612650%2Castro-ph%2F0612148%2Castro-ph%2F0612327%2Castro-ph%2F0612408%2Castro-ph%2F0612775%2Castro-ph%2F0612452%2Castro-ph%2F0612198%2Castro-ph%2F0612606%2Castro-ph%2F0612579&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The lecture introduces a trous wavelet transforms, explains how to estimate\nthe spatial density for galaxy distributions, and, finally, how to describe the\nmorphology of cosmological density fields. An example application of these\nmethods to the 2dFGRS gives unexpected results."}, "authors": ["E. Saar"], "author_detail": {"name": "E. Saar"}, "author": "E. Saar", "arxiv_comment": "39 pages, 22 figures, to be published in the proceedings of the\n  summer school \"Data Analysis in Cosmology\", Valencia, 2004", "links": [{"href": "http://arxiv.org/abs/astro-ph/0612370v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/astro-ph/0612370v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/astro-ph/0612370v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/astro-ph/0612370v1", "journal_reference": null, "doi": null, "fulltext": "Multiscale Methods\n\narXiv:astro-ph/0612370v1 14 Dec 2006\n\nEnn Saar1,2\n1\n\n2\n\nTartu Observatoorium, T\u00f5ravere, 61602 Tartumaa, Estonia,\nsaar@aai.ee\nObservatori Astron\u00f2mic, Universitat de Val\u00e8ncia, Apartat de Correus 22085,\nE-46071 Val\u00e8ncia, Spain,\nEnn.Saar@uv.es\n\nBernard Jones told you about multiresolution analysis in his wavelet lectures.\nThis is a pretty well formalized and self-contained area of wavelet analysis.\nMultiscale methods form a wider and less well defined area of tools and approaches; the term is used, e.g., in numerical analysis, but the main range of\nmultiscale methods are based on application of wavelets.\nIn this this lecture I shall explain how to carry out multiscale morphological\nanalysis of cosmological density fields. For that, we have to use wavelets to\ndecompose the data into different frequency bands, to calculate densities, and\nto describe morphology.\nLet us start with wavelet transforms.\n\n1 Wavelet Transforms\nThe most popular wavelet transforms are the orthogonal fast transforms, described by Bernard Jones. For morphological analysis, we need different transforms. The easiest way to understand wavelets is to start with continuous\nwavelet transforms.\n1.1 Continuous Wavelet Transform\nThe basics of wavelets are most easily understood in the case of one-dimensional\nsignals (time series or data along a line). The most commonly used decomposition of such a signal (f(x)) into contributions from different scales is the\nFourier decomposition:\nZ \u221e\nf\u02c6(k) =\nf (x) exp(\u2212i kx) dx .\n\u2212\u221e\n\nThe Fourier amplitudes f (k) describe the frequency content of a signal. They\nare not very intuitive, however, as they depend on the behaviour of a signal\n\n\f2\n\nEnn Saar\n\nas a whole; e.g., if the signal is a density distribution along a line, then all\nthe regions of the universe where this line passes through, contribute to it.\nFourier modes are homeless.\nFor analyzing the texture of images and fields, both scales and positions\nare important. The right tools for that are continuous wavelets. A wavelet\ntransform of our signal f (x) is\n\u0013\n\u0012\nZ \u221e\n1\nx\u2212b\ndx ,\nW (a, b) = \u221a\nf (x)\u03c8\na\na \u2212\u221e\nwhere \u03c8(x; a, b) is a wavelet profile. Here the argument b ties the wavelet to\na particular location, and a is the scale factor.\nIn order to be interesting (and different from the Fourier modes), typical\nwavelet profiles have a compact support. Two popular wavelets are shown in\nFig. 1 \u2013 the Morlet wavelet, and the Mexican hat wavelet (see the formulae\nin Bernard Jones lecture). The Morlet wavelet is a complex wavelet.\n0.4\n\n1\n\n0.3\n\n0.8\n\n0.2\n\n0.6\n\n0.1\n\n0.4\n\n0\n\n0.2\n\n-0.1\n\n0\n\n-0.2\n\n-0.2\n\n-0.3\n\n-0.4\n\n-0.4\n\n-0.6\n-4\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n-4\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n4\n\nFig. 1. Left \u2013 Morlet wavelet (solid line shows its real part, dashed line \u2013 the\nimaginary part), right \u2013 (a real-valued ) Mexican hat wavelet\n\nContinuous wavelets are good for finding sharp edges and singularities of\nfunctions. An example of that is given in Fig. 2. The brightness-coded wavelet\namplitudes (mexican-hat wavelet) for the upper-panel skyline show features\nat all scales.\nThis example shows us also the information explosion inherent to continuous wavelets \u2013 a function f (x) gives rise to a two-argument wavelet amplitude\nW (a, b); the collection of continuous wavelet amplitudes is heavily redundant.\nIf the wavelet is well-behaved (2), the wavelet amplitudes can be used to\nrestore the original function:\n\u0013\n\u0012\nZ \u221eZ \u221e\n1\nx \u2212 b da db\n1\n\u22c6\n\u221a W (a, b)\u03c8\nf (x) =\n,\n(1)\nC\u03c8 0\na\na2\na\n\u2212\u221e\n\n\fMultiscale Methods\n\n3\n\nFig. 2. Example (continuous mexican hat wavelet)\n\nwhere\nC\u03c8 =\n\nZ\n\n\u221e\n\n0\n\n\u03c8\u0302 \u22c6 (\u03bd)\u03c8\u0302(\u03bd)\nd\u03bd\n\u03bd\n\n\u22c6\n\n(\u03c8 is a complex conjugate of \u03c8, and \u03c8\u0302 is the Fourier transform of \u03c8). This\nconstant exists (C\u03c8 < \u221e), when\nZ \u221e\n\u03c8(x) dx = 0 .\n(2)\n\u03c8\u0302(0) = 0 \u21d2\n\u2212\u221e\n\nThis is the only requirement that a continuous-transform wavelet has to satisfy. Equation (1) shows that we need to know the wavelet amplitudes of all\nscales to reconstruct the original function. We also see that large-scale wavelet\namplitudes are heavily downweighted in this reconstruction.\n1.2 Dyadic Wavelet Transform\nIt is clear that the information explosion inherent in application of continuous\nwavelets has to be restrained. The obvious way to proceed is to consider\ncomputational restrictions.\nFirst, computations need to be carried out on a discrete coordinate grid\nbi . Second, if we look at Fig. 2 we see that the wavelet amplitudes change,\n\n\f4\n\nEnn Saar\n\nin general, smoothly with scale. This suggests that a discrete grid of scales\ncould suffice to analyze the data. While the obvious choice for the coordinate\ngrid is uniform, the scale grid is usually chosen logarithmic, in hope to better\ncatch the scale-dependent behaviour. This discretisation generates much less\ndata than the continuous transform did, only N \u00d7 J, where N is the size of\nthe original data (the number of grid points), and J is the number of different\nscales. The most popular choice of the scale grid is where neighbouring scales\ndiffer by a factor of two. Other choices are possible, of course, but this choice\nis useful in several respects, as we shall see below.\nSuch a wavelet transform is called dyadic. As we saw above, any compact\nfunction with the zero mean could be used as an analyzing wavelet. In the\ncase of a discrete wavelet transform, we, however, have a problem \u2013 can we\nrestore the original signal, given all the wavelet amplitudes obtained? It is\nnot clear at first sight, as the restoration integral for a continuous wavelet (1)\ncontains contributions of all wavelet scales.\nThe answer to that is yes, given that the frequency axis is completely\ncovered by the wavelets of dyadic scales (the wavelets form a so-called frame).\nThis requirement is referred to as the perfect reconstruction condition; I shall\nshow a specific example below.\nLet us rewrite now the wavelet transform, for finite grids. As we have heard\nabout the multiresolution analysis already, we shall try to introduce both the\nsmooth and the detail part of the transform. Let the initial data be a0 (m)\n(the index 0 shows the transform order, and the argument m \u2013 the grid point).\nThe smoothing operation and the wavelet transform proper are convolutions,\nso they can be written\nX\nX\naj+1 (m) =\nhj (k)aj (m + k) ,\ndj+1 (m) =\ngj (k)aj (m + k) , (3)\nk\n\nk\n\nwhere the filters hj (k) and gj (k) are different from zero for a few values of k\naround 0 only (the wavelet transform is local). Applying recursively this rule,\nwe can find wavelet transforms of all orders, starting from the data (a0 (*)).\nNow, we should like to be able to restore the signal, knowing the aj (*) and\ndj (*). As the previous filters were linear, restoration should also be linear, and\nwe demand:\nX\nX\naj (m) =\nh\u0303j (k)aj+1 (m + k) +\ng\u0303j (k)dj+1 (m + k) .\n(4)\nk\n\nk\n\nIt can be proved that these rules will work for any dyadic wavelet that satisfies\nthe so-called unit gain condition, if the filters are upsampled (see next section).\nIf you recall (or look up) Bernard Jones' lecture, you will notice that\nthe rules (3,4) look the same as the rules for bi-orthogonal wavelets. The only\ndifference is that for bi-orthogonal wavelets the filters have to satisfy an extra,\nso-called aliasing cancellation condition, arising because of the downsampling\nof the grid.\n\n\fMultiscale Methods\n\n5\n\n1.3 \u00c0 Trous Transform\nThis downsampling leads us to the next problem. The change of the frequency\n(scale) between wavelet orders when bi-orthogonal or orthogonal wavelet\ntransforms are used, is achieved by downsampling \u2013 choosing every other\ndata point. Applying the same wavelet filter on the downsampled data set is\nequivalent to using the twice as wide filter on the original data. Now, in our\ncase the grid is not diluted, and all points participate for all wavelet orders.\nThe obvious solution here is to upsample the filter \u2013 to introduce zeros between the filter points. This doubles the filter width, and it is also useful for\nthe computational point of view, as the operations count for the convolutions\ndoes not depend now on the wavelet order. Because of these zeros (holes), such\na dyadic transform is called \"\u00e0 trous\" ('with holes' in French; the transform\nwas introduced by French mathematicians, Holschneider et al. ([5])).\nIn Fourier language, such an upsampling halves the frequency:\n\u01251 (\u03c9) = \u01250 (2\u03c9) ,\n\n\u0125j (\u03c9) = \u0125(2j \u03c9) ,\n\nwhere \u03c9 stands for the frequency, and h(k) \u2261 h0 (k), \u0125(\u03c9) \u2261 \u01250 (\u03c9). Although\nwe describe localized transforms, it is useful to use Fourier transforms, occasionally \u2013 convolutions are frequent in wavelet transforms, and convolutions\nare converted to simple multiplications in Fourier space. Now, in coordinate\nspace, the explicit expression for the upsampled filter h(*) is:\nhj (k \u2032 ) = h(k)\u03b4(k \u2032 \u2212 k * 2j ) ,\nsaying simply that the only non-zero components of the smoothing filter for\nthe order j are those that have the index k * 2j , and they are determined by\nthe original filter values h(k) \u2261 h0 (k). Let us write now the smoothing sum\nagain:\nX\nX\naj+1 (m) =\nhj (k \u2032 )aj (m + k \u2032 ) =\nh(k)aj (m + 2j k) ,\n(5)\nk\u2032\n\nk\n\nwhere we retained only the non-zero terms in the last sum. The last equality\nis of the form usually used in \u00e0 trous transforms; the holes are returned to the\ndata space again. However, the procedure is different from the bi-orthogonal\ntransform, as we find the scaling and wavelet amplitudes for every grid point\nm, not for the downsampled sets only. The wavelet rule with holes reads\nX\ndj+1 (m) =\ng(k)aj (m + 2j k) ,\n(6)\nk\n\nwhere, obviously, g(k) \u2261 g0 (k).\nLet us now construct a particular \u00e0 trous transform, starting backwards,\nfrom the reconstruction rule (4). In multiresolution language, this rule tells\nus that the approximation (sub)space Vj where the \"smooth functions\" live,\n\n\f6\n\nEnn Saar\n\nis a direct sum of two orthogonal subspaces of the next order (the formula is\nfor projections, but it follows from this fact). Let us take it in a much simpler\nway, literally, and demand:\naj (m) = aj+1 (m) + dj+1 (m) ,\n\n(7)\n\nor h\u0303(k) = g\u0303(k) = \u03b40k . This is a very good choice, as applying it recursively\nwe get\nj=1\nX\ndj (m) ,\n(8)\na0 (m) = aJ (m) +\nj=J\n\nmeaning that the data is decomposed into a simple sum of contributions of\ndifferent details (wavelet orders) and the most smooth picture. As there are no\nextra weights, these detail spaces have a direct physical meaning, representing\nthe life in the full data space at a given resolution.\nThe condition (7) gives us at once the formula for the wavelet transform\nX\ndj+1 (m) = aj (m) \u2212 aj+1 (m) =\n[\u03b40k \u2212 h(k)] aj (m + k) ,\n(9)\nk\n\nor g(k) = \u03b40k \u2212 h(k) .\nSo far, so good. Now we have only to choose the filter h(k) to specify\nthe transform. As the filter is defined by the scaling function \u03c6(x) via the\ntwo-scale equation\nX\n\u03c6(x/2) = 2\nh(k)\u03c6(x \u2212 k) ,\n(10)\nk\n\nmeaning simply that the scaling function of the next order (note that f (x/2)\nis only half as fast as f (x)) has to obey the smoothing rule in (5) exactly (the\nspace where it lives is a subspace of the lower order space). Different normalizations are used; the coefficient 2 appears here if we omit extra coefficients\nfor the convolution (10) in the Fourier space:\n\u03c6\u0302(2\u03c9) = \u0125(\u03c9)\u03c6\u0302(\u03c9)\n(recall that the coordinate space counterpart of f\u02c6(2\u03c9) is f (x/2)/2). Obviously,\nnot all functions satisfy the two-scale equation (10), but a useful class of\nfunctions that do are box splines.\n1.4 Box Splines\nBox splines are easy to obtain \u2013 an n-th degree box spline Bn (x) is the result\nof n+1 convolutions of the box profile B0 (x) = 1, x \u2208 [0, 1] with itself. Some\nauthors like to shift it, some not; we adopt the condition that the convolution\nresult is centred at 0 when n is odd, and at x = 1/2 when n is even. This\nconvention gives a simple expression for the Fourier transform of the spline:\n\n\fMultiscale Methods\n\nB\u02c6n (\u03c9) =\n\n\u0012\n\nsin(\u03c9/2)\n\u03c9/2\n\n\u0013\n\nexp (\u2212i \u03b5\u03c9/2) ,\n\n7\n\n(11)\n\nwhere \u03b5 = 1, if n is even, and 0 otherwise. You can easily derive the formula\nyourselves, recalling that the Fourier transform of a [\u22121, 1] box is the sinc(*)\nfunction, and using the rule for the argument shifts.\nBox splines have, just to start, several very useful properties. First, they\nare compact; in fact, they are the most compact polynomials for a given degree\n(Bn (x) is a polynomial of degree n). Second, they are interpolating,\nX\nBj (x \u2212 k) = 1 ,\n(12)\nk\n\na necessary condition for a scaling function. And box splines satisfy the twoscale equation (10), with\n\u0012 \u0013\n\u2212(n+1) n\n,\n(13)\nh(k) = 2\nk\nwhere n is the degree of the spline (see de Boor ([2]). This formula is written\nfor unshifted box splines, and here the index k ranges from 0 to n. It is easy to\nmodify (13) for centred splines; e.g., for centred box-splines of an odd degree\nn the index k ranges from \u2212(n + 1)/2 to (n + 1)/2, and we have to replace k\nat the right-hand side of (13) by k + (n + 1)2.\nI do not intend to be original, and shall choose the B3 box spline for the\nscaling function. This is the most beloved box spline in astronomical community; see, e.g., the monograph by Jean-Luc Starck and Fion Murtagh ([14])\nfor many examples. As any spline, this can be given by different polynomials\nin different coordinate intervals; fortunately, a compact expression exists:\n\u0001\n1\n|x \u2212 2|3 \u2212 4|x \u2212 1|3 + 6|x|3 \u2212 4|x + 1|3 + |x + 2|3 .\n12\n(14)\nThis function is identically zero outside the interval [\u22122, 2]. Formula (13) gives\nus the filter h(k):\n\u03c6(x) = B3 (x) =\n\nh(k) = (1/16, 1/4, 3/8, 1/4, 1/16) ,\n\nk \u2208 [\u22122, 2] .\n\n(15)\n\nIn order to obtain the associated wavelet \u03c8(*), we have to return to our recipe\nfor calculating the wavelet coefficients (9). These coefficients are, in principle,\nconvolutions (multiplications in Fourier space):\nd\u02c6i+1 (\u03c9) = \u03c8\u0302(\u03c9)\u00e2i (\u03c9) ,\n\n(16)\n\n\f8\n\nEnn Saar\n\nSo, (9) gives us\n\u03c8\u0302(\u03c9)\u00e2i (\u03c9) = \u03c6\u0302(\u03c9/2)\u00e2i (\u03c9) \u2212 \u03c6\u0302(\u03c9)\u00e2i (\u03c9) ,\nor,\n\u03c8\u0302(\u03c9) = \u03c6\u0302(\u03c9/2) \u2212 \u03c6\u0302(\u03c9) .\n\n(17)\n\nFor coordinate space, the above expression transforms to\n\u03c8(x) = 2\u03c6(2x) \u2212 \u03c6(x) .\n\n(18)\n\n0.7\n0.6\n0.5\n\u03c6\n\n0.4\n0.3\n\n\u03c8\n\n0.2\n0.1\n0\n-0.1\n-0.2\n-0.3\n-2\n\n-1\n\n0\n\n1\n\n2\n\nx\n\nFig. 3. The B3 scaling function (\u03c6) and its associated wavelet (\u03c8)\n\nBoth the B3 box spline (the scaling function \u03c6(x) and its associated wavelet\n\u03c6(x) are shown in Fig. 3.\nI cannot refrain from noting that the last exercise was, strictly speaking,\nunnecessary. We could have proceeded with our wavelet transform after obtaining the filter h(k) (15). But is nice to know the wavelet by face and to get\na feeling what our algorithms really do.\nNow I can also show you the Fourier transform of the wavelet (17), to\nreassure you that such a wavelet can be built and that it does not leave gaps\nin the frequency axis. We see, first, that the filter peaks at \u03c9 \u2248 \u03c0, giving for\nits characteristic wavelength \u03bb = 2\u03c0/\u03c9 = 2 (grid units). Second, we see that\nneighbouring wavelet orders overlap in frequency. The reason for that is that\nour wavelets are not orthogonal. So we loose a bit in frequency separation,\nbut gain in spatial resolution.\nTwo points more: first, about normalization. Although our scaling function \u03c6(x) is normalized in the right way (its integral is 1), the coefficient\nin the two-scale equation (10 is different from the \u221a\nconventional one, and, as\na result, the filter coefficients hk sum to 1, not to 2. The integral over the\nprofile is zero (this ensures that \u03c8(x) is really a wavelet), but its norm\nRwavelet\n\u03c8 2 (x) dx \u2248 0.2345; normally it is chosen to be 1. What matters, really, is\n\n\fMultiscale Methods\n\n9\n\n0.5\n0.45\n0.4\n0.35\nFT(\u03c8)\n\n0.3\n0.25\n0.2\n0.15\n0.1\n0.05\n0\n-10\n\n-5\n\n0\n\n5\n\n10\n\n\u03c9\n\nFig. 4. The Fourier transforms of the three subsequent orders of the B3 (*)-associated\nwavelet. The transforms fully cover the frequency axis, but the overlap between\ndifferent orders is substantial\n\nthat it is not zero and does not diverge; welcome to the wavelet world of free\nnormalization.\nSecond, about initial data. We can recursively apply the transformation\nrules (5, 6) only if we assume that the data a0 (m) belongs to the class of\nsmooth functions (those obtained by convolution with the scaling function).\nSo, if the raw data comes in ticking at grid points (regular times), we should\nsmooth it once before starting our transform chain. If the raw data is given at\npoints x that do not coincide with the grid, the right solution is to distribute it\nto the grid m with the scaling weights \u03c6(x\u2212m). This procedure was christened\n'extirpolation' (inverse interpolation) by Press et al. ([9]); a strange fact is that\nN-body people extirpolate all the time in their codes, but nobody wants to\nuse the term.\nSo, we have specified all the recipes needed to perform the \u00e0 trous transform. Before we do that, we have to answer the question \u2013 why? Bernard Jones\ndemonstrated us how well orthogonal wavelet transforms work. An orthogonal wavelet transform changes a signal (picture) of N pixels into exactly N\nwavelet amplitudes, while an \u00e0 trous transform expands it into N \u00d7J pictures;\nwhy bother?\nThe reason is called 'translational invariance'. As many of the wavelet\namplitudes of an orthogonal transform do not have an exact home, then,\nwhen shifting the data, these amplitudes change in strange ways. Sure, we\ncan always use them to reconstruct the shifted picture, but it makes no sense\nto compare the wavelet amplitudes of the original and shifted pictures. All\nthe \u00e0 trous transforms, in the contrary, keep their amplitudes, these move\ntogether with the grid. This is 'translational invariance', and it is important\nin texture studies, where we want to see different scales of a picture at exactly\nthe same grid point. And cosmic texture is the main subject of this lecture.\nA point to note \u2013 Fourier transforms are not translation invariant, too.\n\n\f10\n\nEnn Saar\n\n1.5 Multi-Dimensional \u00c0 Trous\nAll the above discussion was devoted to one-dimensional wavelets. This is\ncustomary in wavelet literature, as the step into multi-dimensions is simple\n\u2013 we form direct products of independent one-dimensional wavelets, one for\nevery coordinate. This has been the main approach up to now, although it\ndoes not work well everywhere. An important example is a sphere, where\nspecial spherical wavelets have to be constructed (I suppose that these will be\nexplained in the CMB-lectures).\nSo, two-dimensional wavelets are introduced by defining the 2-D scaling\nfunction\n\u03c6(x, y) = \u03c6(x)\u03c6(y) ,\n(19)\nand three-dimensional wavelets \u2013 by the 3-D scaling function\n\u03c6(x, y, z) = \u03c6(x)\u03c6(y)\u03c6(z) .\n\n(20)\n\nA little bit extra care has to be taken to define wavelets; we have to step into\nFourier space for a while for that. Recalling (17), we have to write for two\ndimensions\n\u03c8\u0302(\u03c91 , \u03c92 ) = \u03c6\u0302(\u03c91 /2)\u03c6\u0302(\u03c92 /2) \u2212 \u03c6\u0302(\u03c91 )\u03c6\u0302(\u03c92 )\n(the direct products (19, 20) look exactly the same in the Fourier space). For\ncoordinate space, it gives\n\u03c8(x, y) = 4\u03c6(2x)\u03c6(2y) \u2212 \u03c6(x)\u03c6(y) ,\nand for three dimensions, respectively\n\u03c8(x, y, x) = 8\u03c6(2x)\u03c6(2y)\u03c6(2z) \u2212 \u03c6(x)\u03c6(y)\u03c6(z) ,\nI show the B3 -associated wavelets in Figs. 5 and 6. Because of their definition, the wavelet profiles are symmetric, but not isotropic, right? A big\nsurprise is that both the B3 scaling functions and the wavelets are practically\nisotropic, as the figures hint at. Let us define the isotropic part of the 2-D\nwavelet as\nZ 2\u03c0\n1\n\u03c8(r) =\n\u03c8(r cos \u03b1, r sin \u03b1) d\u03b1 ,\n2\u03c0 0\nand estimate the deviation from isotropy by\nZ 2Z 2\np\n\u01eb=\n|\u03c8(x, y) \u2212 \u03c8( x2 + y 2 )| dx dy .\n\u22122\n\n\u22122\n\nComparing \u01eb with the integral over the absolute value of our wavelet itself\n(about 4/9), we find that the difference is about 2%. For three dimensions,\nthe difference is a bit larger, up to 5%.\nThis isotropy is important for practical applications; it means that our\nchoice of specific coordinate directions does not influence the results we get.\n\n\fMultiscale Methods\n\n11\n\n\u03c8(x,y)\n1.5\n1\n0.5\n0\n1\n0\n\n-1\ny\n\n0\n1\n\n-1\n\nFig. 5. Two-dimensional B3 -associated wavelet\n\nFig. 6. Three-dimensional B3 -associated wavelet\n\nAnd, as an example, I show a sequence of transforms in Fig. 7. Those are\nslices of a 3-D B3 -associated \u00e0 trous transform sequence for the gravitational\npotential image of a N-body simulation. The data slice is at the upper left; the\nleft column shows the scaling solutions, higher orders up. The right column\nshows the wavelet amplitudes; the data can be restored by taking the lower\nleft image, and by adding to it all the wavelet images from the right column.\nSimple, is it?\n\n\f12\n\nEnn Saar\n\nFig. 7. \u00c1 trous potential (slices) for a N-body model (linear scale)\n\n\fMultiscale Methods\n\n13\n\n2 Cosmological Densities\nIn cosmology, the matter density distribution is a very important quantity\nthat, first, determines the future dynamics of structure, and, second, may\ncarry traces of the very early universe (initial conditions). We, however, cannot\nobserve it. The data we get, after enormous effort, gives us galaxy positions\nin (redshift) space; even if we learn to associate a proper piece of dark matter\nwith a particular galaxy, we have a point process, not a continuous density\nfield.\nThere are several ways to deal with that. First, we have to introduce a\ncoordinate grid; an extra discrete entity, but necessary when using continuous\nfields in practical computations.\n2.1 N-Body Densities\nA similar density estimation problem arises in N-body calculations. The fundamental building blocks there are mass points; these points are moved around\nevery timestep, and in order to get the accelerations for the next timestep,\nthe positions of the points have to be translated to the mass density on the\nunderlying grid.\nThe simplest density assignment scheme is called NGP (nearest grid point),\nwhere the grid coordinate i is found by rounding the point coordinate x (to\nkeep formulae simple, consider one-dimensional case and grid step one; generalization is trivial). Thus, the NGP assignment law is i = floor(x + 0.5). This\nscheme is pretty rough, but I have seen people using an even worse scheme,\ni = floor(x).\nA bit more complex scheme is CIC (cloud in cell) where the mass point is\ndressed in a cubic cloud of the size of the grid cell, and vertices's are dressed in\na similar region of influence. The part of the cloud that intersects this region\nof influence is assigned to the vertex. Sounds complicated, but as the mass\nweights are obtained by integration over the constant-density cloud, it is, in\nfact, only linear extirpolation (in 3-D, three linear).\nThe most complex single-cloud scheme used is TSC (triangular shaped\ncloud), where the density of the cloud changes linearly from a maximum in\nthe centre, to zero at the borders. The mass integration needed ensures that\nthis scheme is quadratic extirpolation.\nNote that the last two mass assignment schemes are, if fact, centred Bspline assignments \u2013 the vertex region is B0 , the CIC density is B0 , too, and\nthe TSC cloud is B1 . The weights are obtained by convolution of those density\nprofiles with the vertex profile, so, finally, NGP is a B0 extirpolation scheme\n(no density law to be convolved with), CIC becomes a B1 extirpolation scheme\nand TSC \u2013 a B2 scheme.\nToday's mass assignment schemes are mostly adaptive variations of those\nlisted above, where more dense grids are built in regions of higher density. But\nthe N-body mass assignment schemes share one property \u2013 mass conservation.\n\n\f14\n\nEnn Saar\n\nNo matter what scheme is used, the total mass assigned to all vertexes of the\ngrid is equal to the total mass of the mass points. No surprise in that \u2013 box\nsplines are interpolating (12).\n2.2 Statistical Densities\nThis class of density assignments (estimators) does not care about mass conservation. The underlying assumption is that we observe a sample of events\nthat are governed by an underlying probability density, and have to estimate\nthis density. In cosmology, there really is no difference between the two densities, spatial mass density and probability density.\nThe basic model of galaxy distribution adopted by cosmological statistics\nis that of the Cox point process. It says that, first, the universe is defined by\na realization of a random process that ascribes a probability density \u03bb(x) in\nspace. Then, a Poisson point process gets to work, populating the universe\nwith galaxies, where the probability to have a galaxy at x is given by the Poisson law with the parameter \u03bb(x) fixed by the initial random-field realization.\nNeat, right? And as we are dealing with random processes, no conservation is\nrequired.\nStatisticians have worked seriously on probability density estimation problem (see Silverman [13] for a review). The most popular density estimators\nare the kernel estimators:\nX\n\u033ai =\nK(xn \u2212 i)\n(21)\nn\n\n(recall that i are the grid coordinates and xn are the galaxy coordinates). The\nkernel K is a symmetric distribution:\nZ\nZ\nK(x)dx = 1 ,\nxK(x) = 0 .\nMuch work has been done on the choice of kernels, with the result that the\nexact shape of the kernel does not matter much, but its width does. The best\nkernel is said to be the Epanechikov kernel:\nKE (x) = A(1 \u2212 x2 /R2 ) ,\n\nx2 \u2264 R2 ,\n\n0\n\notherwise .\n\n(22)\n\n(I wrote it for multidimensional case to stress that this is not a direct-product,\nbut an isotropic kernel; A is, of course, a normalization constant). The Gaussian kernel comes close behind:\nKG (x) = \u221a\n\n\u0001\n1\nexp \u2212x2 /2\u03c3 2 .\n2\u03c0\u03c3\n\n(23)\n\nThis is the only kernel where the direct product is isotropic, too. The ranking\nof kernels is done by deciding how close the estimated probability density f \u0303(x)\nis to the true density f (x), by measuring the MSE (mean standard error):\n\n\fMultiscale Methods\n\nh\ni2\n\u0010\n\u0011 h\n\u0010\n\u0011i2\nMSE = E f \u0303(x) \u2212 f (x) = Var f \u0303(x) + Bias f \u0303(x)\n.\n\n15\n\n(24)\n\nNote that statisticians minimize the MSE, not only the variance, as cosmologists frequently tend to do. As usual in statistics, the results are asymptotic,\ntrue for a very big number of galaxies N . As I have tested, in the usual\ncosmological case where there are about 10 galaxies inside the Epanechikov\nkernel, the total mass over all vertexes is only a half of the galaxy number N ;\nEpanechikov cheats. It starts working properly for about 100 points inside the\nkernel. In this respect, the B3 kernel we used above, is is a very good candidate\nfor a density estimation kernel. It is smooth (meaning its Fourier transform\ndecays fast), and it is compact, no wide wings as the Gaussian kernel has.\nAnd it is interpolating, guaranteeing that not a single galaxy is lost.\nKernel density estimators allow a natural generalization for the case of\nextremely different density amplitudes and scales, as seen in cosmology.\nConstant-width kernels tend to over-smooth the sharp peaks of the density,\nif these exist. The solution is using adaptive kernels, by varying the kernel\nwidth h(*) from place to place. There are, basically, two different ways to do\nthat. The balloon or scatter estimators say:\nX \u0012 xn \u2212 i \u0013\n;\n(25)\n\u033ai =\nK\nh(i)\nn\nhere the kernels sit on the grid points i. The second type of estimators is called\nsample point, sandbox, or gather estimators:\nX \u0012 xn \u2212 i \u0013\n.\n(26)\n\u033ai =\nK\nh(xn )\nn\nHere the kernel width depends on the sample point. The most difficult problem\nfor adaptive kernels is how to choose the right kernel widths. The usual way\nis to estimate the density with a constant kernel first, and to select the adaptive kernel widths proportional to some fractional power of the local density\nobtained in the first pass (\u22121/5 is a recommended choice).\nBoth estimators are used in cosmology (the terms scatter and gather come\nfrom the SPH cosmological hydrodynamics codes). The lore says that the balloon estimators (25) work best in low-probability regions (voids in cosmology),\nand the sandbox estimators \u2013 where densities are high.\n2.3 Equal-Mass Densities\nA popular density estimator is based on k-d trees. These trees are formed by\nrecursive division of the sample space into two equal-probability halves (having\nthe same number of galaxies). It is a spatial version of adaptive histograms\n(an equal number of events per bin). Of course, k-d trees give more than\njust density estimates, they also imprint a tree structure on (or reveal the\n\n\f16\n\nEnn Saar\n\nstructure of the geometry of) the density field. An application of k-d trees for\nestimating densities appeared in astro-ph during the school, and has already\nbeen published by the time of writeup of the lecture (Ascasibar & Binney [1]).\nAnother popular equal-mass density estimators are kNN (k nearest neighbours) kernels. The name speaks for itself \u2013 the local kernel size is chosen to\ninclude k particles in the kernel volume. This estimator uses isotropic kernels.\nThe SPH gather algorithm uses, in fact, the kNN ideology. There is a separate free density estimation tool based on that algorithm ('smooth'), written\nby Joachim Stadel and available from the Washington University N-body\nshop3 . Try it; the only problems are that you have to present the input data\nin the 'tipsy' format, and that you get the densities at particle positions, not\non a grid. Should be easy to modify, if necessary.\n2.4 Wavelet Denoising\nWavelet denoising is a popular image processing methodology. The basic assumption is that noise in an image is present at all scales. Once we accept that\nassumption, the way to proceed is clear: decompose the image into separate\nscales (wavelet orders; orthogonal wavelet transforms are the best here), estimate the noise at each wavelet order, eliminate it somehow, and reconstruct\nthe image.\nThis course of actions includes two difficult points \u2013 first, estimating the\nnoise. The properties of the basically unknown noise are, ahem, unknown, and\nwe have to make assumptions about them. Gaussian and Poisson noise are the\nmost popular assumptions; this leaves us with the problem of relative noise\namplitudes (variances) between different wavelet orders. A popular method is\nto model the noise. Modelling is started by assuming that all the first-order\nwavelet data is noise (interesting, is it?) and processing that for the noise variance. After that, noise of that variance is modelled, wavelet transformed, and\nits properties found for every wavelet order. After that, we face a a common\ndecision theory problem, at which p-value have to set the noise limit? If we\ncut the noise at too low amplitude, we leave much of it in the final image, and\nif we take the cut too high, we eliminate part of the real signal, too. Once we\nhave selected that level, we can quantify it in the terms of the limiting wavelet\namplitude k\u03c3j , where \u03c3j2 is the modelled noise variance for the level j.\nThe second problem is how to suppress the noisy amplitudes. The first\napproach is called 'hard thresholding' and it is simple: the processed wavelet\namplitudes w\u0303j are\nw\u0303j = wj ,\n\nif |wj | > k\u03c3j ,\n\n0 otherwise .\n\n(27)\n\nThis thresholding leaves an empty trench around 0 in the wavelet amplitude\ndistribution.\n3\n\nttp://www-pcc.astro.washington.edu/\n\n\fMultiscale Methods\n\n17\n\nAnother approach is 'soft thresholding':\nw\u0303j = wj \u2212 sgn(wj ) k\u03c3j ,\n\nif\n\n|wj | > k\u03c3j ,\n\n0\n\notherwise .\n\n(28)\n\nThis thresholding takes out the same trench, first, but fills it up then, diminishing all the remaining amplitudes.\nDavid Donoho, who was the first to introduce soft thresholding, has also\nproposed an universal formula for the threshold level:\np\nk\u03c3j = 2 log(n) \u03c3j ,\n\nwhere n is the number of pixels in the image. This level corresponds to 3\u03c3\nfor n = 90, and to 4\u03c3 for n = 3000. Of course, astronomers complain \u2013 3000\npixels is a very small size for an astronomical image, but 4\u03c3 is a very high cutoff level; we can cut off much of the information in the image. Astronomical\ninformation is hard to obtain, and we do not want to waste even a bit. So we\nbetter keep our pictures noisy? Fortunately, more approaches to thresholding\nhave appeared in recent years; consult the new edition of the Starck and\nMurtagh book ([14]).\nAnyway, wavelet denoising has met with resounding success in image processing, no doubts about it. And image processing is an industry these days,\nso the algorithms that are used are being tested in practise every day. Now,\nimage processing is 2-D business; wavelet denoising of a 3-D (spatial) density\nis a completely different story. The difference is that the density contrasts\nare much bigger in 3-D than in 2-D \u2013 there simple is more space for the\nsignal to crowd in. And as wavelets follow the details, they might easily overamplify the contrasts. I know, we have spent the last year trying to develop\na decent wavelet denoising algorithm for the galaxy distributions (well, 'we'\nmeans mainly Jean-Luc Starck). Fig. 8 shows how the denoising might go awry\n(right panel), but shows at the same time that good recipes are also possible\n(left panel). The denoising procedure for the right panel has over-amplified\nthe contrasts, and has generated deep black (zero density) holes close to white\nhigh density peaks. So, in order to do a decent denoising job, one has to be\ncareful.\nThe details of the algorithm we used are too tedious to describe in full,\nthey can be found in Starck and Murtagh ([14]).. The main points are:\n1. We used the \u00e0 trous algorithm, not an orthogonal one. The reason for\nthat is that we needed to discriminate between the positions of significant\nwavelet amplitudes (the multiresolution support) and non-significant amplitudes at the last stage, and when speaking of positions, orthogonal\nwavelet transforms cannot be used.\n2. We hard thresholded the solution and iterated it, reconstructing and transforming again, to obtain a situation where the final significant wavelet\ncoefficients would cover exactly the original multiresolution support.\n3. Finally, we smooth the solution by imposing a smoothness constraint,\nrequiring that the sum of the absolute values of wavelet amplitudes of\n\n\f18\n\nEnn Saar\n\nFig. 8. Wavelet denoising of a (model) galaxy distribution (left \u2013 a successful attempt, right \u2013 over-denoising)\n\na given order would be minimal, while keeping the corrected amplitudes\nthemselves within a given tolerance (\u03c3j /2) of the original noisy ones. We\nconsider here only the amplitudes in the multiresolution support; this\npoint required using a translation invariant wavelet transform.\nFig. 9 compares the results of our 3-D wavelet denoising. We started with a\n3-D galaxy distribution, applied our algorithm to it, and, for comparison, built\ntwo Gaussian-smoothed density distributions. It is clearly seen that the typical\ndetails, in the case of Gaussian smoothing, are of uniform size, while the\nwavelet-denoised density distribution is adaptive, showing details of different\nscales.\n2.5 Multiscale Densities\nWe have made an implicit assumption during all this section, namely, that a\ntrue density field exists. Is that so certain? Our everyday experience tells us\nthat it is. But look at the numbers that stand behind this experience: even\none cubic centimetre of air has 6 \u00d7 1023 /22.4 \u00d7 103 \u2248 3 \u00d7 1019 particles. In our\nsurveys, one gigantic cosmological 'cubic centimetre' of 10 Mpc size contains\nabout 10 galaxies. Can we speak about their true spatial density?\nOne answer is that we can, but there are regions where the density estimates are extremely uncertain; statistics can tell us what the expected variances are. Another answer is that even if there is a true density, it is not\nalways a useful physical quantity, especially for the largest scales we study.\nOne of the reasons we measure the cosmological density field is to find its state\nof evolution and traces of initial conditions in it. The theory of the dynamics\nof perturbations in an expanding universe predicts that structure evolves at\ndifferent rates, slowly at large scales and much more rapidly at galactic scales.\nObservations show that cosmological fields are multiscale objects; the recently\n\n\fMultiscale Methods\n\n19\n\nFig. 9. Density fields for a model galaxy distribution. Left \u2013 Gaussian \u03c3 = 1 Mpc/h\nsmoothing, middle \u2013 wavelet denoising, right \u2013 Gaussian \u03c3 = 3 Mpc/h smoothing\n\ndetermined power spectra span scales from about 600 Mpc/h (k = 0.01 h/Mpc\nto 10 Mpc/h (k = 0.6 h/Mpc. Thus, should not these fields be studied in multiscale fashion, scale by scale? A true adaptive density mixes effects from different scales and scale separation could give us a cleaner look at the dynamics\nof large-scale structure.\nIn case of our everyday densities, this separation of scales can be done\nsafely later, analyzing the true density. For galaxy distributions, it is wiser to\nprescribe a scale (range) and to obtain that density directly from the observed\ngalaxy positions. One advantage is in accuracy, another in that there are places\n(voids), where, e.g., small-scale densities simply do not exist.\nAnd this is the point where the first part of the lecture (wavelets) connects with the second part (density fields). The representation of the observed\ndensity fields by a sum of the densities of different characteristic scales (7) is\njust what we are looking for. True, there is a pretty large frequency overlap\nbetween the neighbouring bands (Fig. 10 shows you their power spectra), but\nthat is possibly the best we can do, while keeping translational invariance.\nThe figure shows also the power spectra of Gaussians of the same scales that\nare sometimes used to select different scales. As we see, Gaussian frequency\nbands are heavily correlated, the overlap of the smaller frequency band with\n\n\f20\n\nEnn Saar\n\nthat of the higher one is total, and that is natural \u2013 smoothing destroys signals of high frequency, but it does not separate frequency bands. So Gaussians\nshould not be used in this business, but, alas, they frequently are.\n0.25\n\nwavelet\ngaussian\n\n0.2\n\nP(\u03c9)\n\n0.15\n\n0.1\n\n0.05\n\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n\u03c9\n\nFig. 10. Power spectra for two neighbouring scales, the B3 -associated \u00e0 trous bands\n(solid lines), and for Gaussian smoothing of the same scales /(dotted lines). Only\nthe positive half on the frequency axis is shown\n\nFigs. 11 and 12 show the \u00e0 trous density slices for a Gaussian cube. This is\na 2563 realization of a Gaussian random field with a power spectrum approximating that of our universe. In real universe, the size of the cube would be\n256 Mpc/h. The slices are taken from the same height; The images are in gray\ncoding, black shows the densest regions. As in Fig. 7, the scaling solutions\nform the left column and the wavelets \u2013 the right column; transform orders\ngrow downwards. In height, the wavelet orders are placed between the scaling\norder that produced them. The scaling solution of order three is repeated in\nFig. 12 to keep the scaling\u2013wavelet alignment. Enjoy.\n\n3 Minkowski Functionals\nPeter Coles explains in his lecture why it is useful to study morphology of\ncosmological fields. In short, it is useful because it is sort of a perpendicular\napproach to the usual moment methods. Our present cosmological paradigm\nsays that the initial perturbation field was a realization of a Gaussian random\nfield. The most direct test of that would be to measure all n-point joint amplitude distributions, starting from the 1-point distribution. Well, we know\nthat even this is not Gaussian, but we know why (gravitational dynamics of a\npositive density field inevitably skews this distribution), and we can model it.\nAs cosmological densities are pretty uncertain, the more uncertain are their\n\n\fMultiscale Methods\n\n21\n\nFig. 11. \u00c1 trous density (slices) of a Gaussian density field. (linear scale). The first\norders (0\u20133 for the scaling solutions, 1\u20133 for the wavelets)\n\n\f22\n\nEnn Saar\n\nFig. 12. \u00c1 trous density (slices) of a Gaussian density field. (linear scale). The\nlarge-scale orders (3\u20136 for the scaling solutions, 4\u20136 for the wavelets)\n\n\fMultiscale Methods\n\n23\n\nmany-point joint distributions. So this direct check does not work, at least\npresently.\nAnother possibility to check for Gaussianity is to estimate higher order\ncorrelation functions and spectra. For Gaussian realizations, odd-order correlations and power spectra should be zero, and even-order moments should be\ndirectly expressible via the second-order moments, the usual two-point correlation function and the power spectrum. Their dynamical distortions can also\nbe modelled, and this is an active area of research.\nMorphological studies provide an independent check of Gaussianity. Morphology of (density) fields depends on all correlation functions at once, is\nscale-dependent, but local, and can also be predicted and its change caused\nby dynamical evolution can be modelled. This lecture is, finally, about measuring morphology of cosmological fields.\nAn elegant description of morphological characteristics of density fields\nis given by Minkowski functionals [8]. These functionals provide a complete\nfamily of morphological measures \u2013 all additive, motion invariant and conditionally continuous4 functionals defined for any hypersurface are linear combinations of its Minkowski functionals.\nThe Minkowski functionals (MF for short) describe the morphology of isodensity surfaces, and depend thus on the specific density level5 . Of course,\nwhen the original data are galaxy positions, the procedure chosen to calculate\ndensities (smoothing) will also determine the result. The usual procedure used\nin this business is to calculate kernel densities with wide Gaussian kernels; the\nrecipes say that the width of the kernel (standard deviation) should be either\nthe mean distance between galaxies or their correlation length, whichever is\nlarger. Although this produces nice smooth densities, the recipe is bad, it\ndestroys the texture of the density distribution; I shall show it later.\nWe shall use wavelets to produce densities, and shall look first at the texture of a true (wavelet-denoised) density, and then at the scale-dependent\nmultiscale texture of the galaxy density distribution. We could also start directly from galaxies themselves, as Minkowski functionals can be defined for\na point process, decorating the points with spheres of the same radius, and\nstudying the morphology of the resulting surface. This approach does not refer to a density and we do not use it here. Although it is beautiful, too, the\nbasic model that it describes is a (constant-density) Poisson process; a theory\n4\n\n5\n\nNote added during final revision: We submitted recently a paper on multiscale\nMinkowski functionals, and the referee wondered what does 'conditionally continuous' mean. So, now I know \u2013 they are continuous if the hypersurfaces are\ncompact and convex, and we can approximate any decent hypersurface by unions\nof such.\nIn fact, Minkowski functionals depend on a surface, that is why they are called\nfunctionals (functions of functions). When we specify the family of iso-density\nsurfaces, the functionals will depend, suddenly, only on a number, the value of\nthe density level, and are downgraded to simple functions, at least in cosmological\napplications.\n\n\f24\n\nEnn Saar\n\nfor that case exists, and analytical expressions for Minkowski functionals are\nknown. Alas, as the galaxy distribution is strongly correlated, this reference\nmodel does not help us much. The continuous density case has a reference\nmodel, too, and that is a Gaussian random field, so this is more useful.\nFor a d-dimensional space, one can find d + 1 different Minkowski functionals. We shall concentrate on usual 3-D space; for that, the Minkowski\nfunctionals are defined as follows. Consider an excursion set F\u03c60 of a field\n\u03c6(x) in 3-D (the set of all points where density is larger than a given limit,\n\u03c6(x \u2265 \u03c60 )). Then, the first Minkowski functional (the volume functional) is\nthe volume of this region (the excursion set):\nZ\nV0 (\u03c60 ) =\nd3 x .\n(29)\nF\u03c60\n\nThe second MF is proportional to the surface area of the boundary \u03b4F\u03c6 of the\nexcursion set:\nZ\n1\nV1 (\u03c60 ) =\ndS(x) ,\n(30)\n6 \u03b4F\u03c60\n(but not the area itself, notice the constant). The third MF is proportional to\nthe integrated mean curvature of the boundary:\n\u0013\n\u0012\nZ\n1\n1\n1\ndS(x) ,\n(31)\n+\nV2 (\u03c60 ) =\n6\u03c0 \u03b4F\u03c60 R1 (x) R2 (x)\nwhere R1 (x) and R2 (x) are the principal curvatures of the boundary. The\nfourth Minkowski functional is proportional to the integrated Gaussian curvature (the Euler characteristic) of the boundary:\nZ\n1\n1\ndS(x) .\n(32)\nV3 (\u03c60 ) =\n4\u03c0 \u03b4F\u03c60 R1 (x)R2 (x)\nThe last MF is simply related to the genus that was the first morphological\nmeasure used in cosmology; all these papers bear titles containing the word\n'topology'. Well, the topological Euler characteristic \u03c7 for a surface in 3D can\nbe written as\nZ\n1\n\u03c7=\n\u03ba dS ,\n(33)\n2\u03c0 S\nwhere \u03ba is the Gaussian curvature, so\nV3 =\n\n1\n\u03c7.\n2\n\n(34)\n\nBear in mind, though, that the Euler characteristic (33) describes the topology\nof a given iso-density surface, not of the full 3-D density distribution; the\ntopology of the latter is, hopefully, trivial.\n\n\fMultiscale Methods\n\n25\n\nThe first topology papers concentrated on the genus G that is similar to\nV3 :\n\u03c7 = 2(1 \u2212 G) ,\n\nV3 = 1 \u2212 G .\n\n(35)\n\nThe functional V3 is a bit more comfortable to use \u2013 it is additive, while G\nis not, and in the case our surface breaks up into several isolated balls, V3 is\nequal to the number of balls. If the excursion set contains only a few isolated\nempty regions (bubbles), V3 gives their number. In a general case\nV3 = #-of-balls + #-of-bubbles \u2212 #-of-tunnels ,\nwhere only these tunnels that are open at both ends, count.\nI have to warn you about a possible confusion with the genus relations\n(33\u201335) \u2013 the coefficient 2 (or 1/2) occupies frequently a wrong position.\nThe confusion is due to a fact that two topological characteristics can be\ndefined for an excursion set \u2013 one for its surface, another for the set itself.\nThe relation between these depends on the dimensionality of the space; for\n3-D the topological characteristic for the excursion set is half of that for the\nsurface, and if we mix them up, our formulae become wrong. I know, we have\npublished a wrong formula, too (even twice), but the formulae are right in our\nbook [6]. So, bear in mind that the Minkowski functionals are calculated for\nsurfaces, and use only the relations above (33\u201335). When in doubt, consult\nthe classical paper by Mecke et al. [8], and use the Crofton's formula below\n(42) for a single cubic cell \u2013 it gives you V3 = 1 \u21d2 G = 0.\nFig. 13 shows a Gaussian cube (a realization of a Gaussian random process)\nfor two different smoothing widths (the left pair and the right pair of columns,\nrespectively), and for three volume fractions. You can see that the solid figures\ninside the isodensity surface are awash with handles, especially at the middle\n50% density level. Of course, the larger the smoothing, the less the number of\nhandles. You can also see that Gaussian patterns are symmetric \u2013 the filled\nregions are exact lookalikes of the empty regions, for a symmetric change of\nvolume fractions.\nGalaxy densities are more asymmetrical, as seen in Fig. 14. This figure\nshows a model galaxy distribution from a N-body simulation, in a smaller\ncube. The 50% density volumes differ, showing asymmetry in the density\ndistribution, and the 5% \u2013 95% symmetry, evident for the Gaussian cube, is\nnot so perfect any more.\nInstead of the functionals, their spatial densities Vi are frequently used:\nvi (f ) = Vi (f )/V ,\n\ni = 0, . . . , 3 ,\n\nwhere V is the total sample volume. The densities allow us to compare the\nmorphology of different data samples.\n3.1 Labelling the Isodensity Surfaces\nThe original argument of the functionals, the density level \u033a0 , can have different amplitudes for different fields, and the functionals are difficult to compare.\n\n\f26\n\nEnn Saar\n\nFig. 13. A Gaussian cube of 2563 pixels for different Gaussian smoothing filters.\nThe left two columns show isodensity surfaces for \u03c3 = 3 pixels, the right two columns\n\u2013 for \u03c3 = 8 pixels. To better delineate isodensity surfaces, we show two sides of the\nsurface in column pairs, where the left column shows high-density regions, and the\nright column \u2013 low-density regions for the same isodensity surface. The rows are for\nconstant volume fractions (7%, 50%, and 93%), starting from below\n\nBecause of that, normalized arguments are usually used; the simplest one is\nthe volume fraction fv , the ratio of the volume outside of the excursion set\nto the total volume of the region where the density is defined (the higher the\ndensity level, the closer this ratio is to 1). Another, similar argument is the\nmass fraction fm , which is very useful for real, positive density fields, but\nis cumbersome to apply for realizations of Gaussian fields, where the density may be negative. But when we describe the morphology of single objects\n(superclusters, say), the mass fraction is the most natural argument. It is\nalso defined to approach 1 for the highest density levels (and for the smallest\nmasses inside the isodensity surface).\nThe most widely used argument in this business is the Gaussianized volume\nfraction \u03bd, defined as\nZ \u221e\n1\nfv = \u221a\nexp(\u2212t2 /2) dt .\n(36)\n2\u03c0 \u03bd\n\n\fMultiscale Methods\n\n27\n\nFig. 14. A galaxy sample (603 pixels) for a 3-pixel smoothing scale. The left column\nshows high-density regions, and the right column \u2013 low-density regions for the same\nisodensity surface. The rows are for constant volume fractions (7%, 50%, and 93%),\nstarting from below\n\nThis argument was introduced already in the first topology paper by Gott\n[4], in order to eliminate the first trivial effect of gravitational clustering, the\ndeviation of the 1-point pdf from the (supposedly) Gaussian initial pdf. Notice\nthat using this argument, the first Minkowski functional is trivially Gaussian\nby definition.\nFor a Gaussian random field, \u03bd is the density deviation from the mean,\ndivided by the standard deviation. We can define a similar argument for any\nfield:\n\u033a\u2212\u033a\n.\n\u03bd\u03c3 =\n\u03c3(\u033a)\nI show different Minkowski functionals versus different arguments in\nFigs. 15 and 16. They are calculated for the model galaxy density distribution shown in the previous figure (Fig. 14). Note how much the shape of\nthe same function(al)s depends on the arguments used.\n\n\f28\n\nEnn Saar\n1\n\n1\n\n0.9\n\n0.9\n\n0.8\n\n0.8\n\n0.7\n\n0.7\nV0(vf)\n\n0.6\nV0\n\nV0\n\n0.6\n0.5\n0.4\n\nV0(\u03bdG)\n\n0.5\n0.4\n\nV0(mf)\n\n0.3\n\nV0(\u03bd\u03c3)\n\n0.3\n\n0.2\n\n0.2\n\n0.1\n\n0.1\n\n0\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5 0.6\nmf , vf\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n2\n\n3\n\n4\n\n\u03bd\u03c3 , \u03bdG\n\n2.5\n\n2.5\n\n2\n\n2\nV1(vf)\nV1(\u03bdG)\n\nV1\n\n1.5\n\nV1\n\n1.5\n\nV1(mf)\n\n1\n\n1\n\n0.5\n\n0.5\n\n0\n\n0\n\nV1(\u03bd\u03c3)\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5 0.6\nmf , vf\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\u03bd\u03c3 , \u03bdG\n\nFig. 15. The first two Minkowski functionals for N-body model galaxies. Here vf is\nthe volume fraction, mf \u2013 the mass fraction, \u03bd\u03c3 \u2013 the normalized volume fraction,\nand \u03bdG \u2261 \u03bd \u2013 the Gaussianized volume fraction from (36). The dotted line in the\nright panels shows the predicted Minkowski functionals for a Gaussian random field\nMF\n\n3.2 Gaussian Densities\nAll the Minkowski functionals have analytic expressions for iso-density slices\nof realizations of Gaussian random fields. For three-dimensional space they\nare:\n\u0013\n\u0012\n1 1\n\u03bd\nv0 = \u2212 \u03a6 \u221a\n,\n(37)\n2 2\n2\n\u0012 2\u0013\n2 \u03bb\n\u03bd\nv1 = \u221a exp \u2212\n,\n(38)\n3 2\u03c0\n2\n\u0012 2\u0013\n2 \u03bb2\n\u03bd\nv2 = \u221a \u03bd exp \u2212\n,\n(39)\n3 2\u03c0\n2\n\u0012 2\u0013\n\u03bb3\n\u03bd\nv3 = \u221a (\u03bd 2 \u2212 1) exp \u2212\n,\n(40)\n2\n2\u03c0\nwhere \u03a6(*) is the Gaussian error integral, and \u03bb is determined by the correlation function \u03be(r) of the field:\n\u03bb2 =\n\n1 \u03be \u2032\u2032 (0)\n.\n2\u03c0 \u03be(0)\n\nThe dimension of \u03bb is inverse length.\n\n(41)\n\n\fMultiscale Methods\n15\n\n29\n\n15\n10\n\n10\n\nV2(\u03bd\u03c3)\n\nV2(mf)\n\n5\n\nV2(\u03bdG)\n\nV2\n\nV2\n\n5\nV2(vf)\n\n0\n\n0\n-5\n-5\n\n-10\n\n-10\n\n-15\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5 0.6\nmf , vf\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n-3\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n2\n\n3\n\n4\n\n\u03bd\u03c3 , \u03bdG\n\n150\n\n150\nV3(mf)\n\n100\n\n100\n\n50\n\n50\n\n0\n\n0\nV3\n\nV3\n\n-2\n\n-50\nV3(vf)\n\n-100\n\nV3(\u03bd\u03c3)\n\nV3(\u03bdG)\n\n-50\n-100\n\n-150\n\n-150\n\n-200\n\n-200\n\n-250\n\n-250\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5 0.6\nmf , vf\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\u03bd\u03c3 , \u03bdG\n\nFig. 16. The last two Minkowski functionals for N-body model galaxies. Here vf is\nthe volume fraction, mf \u2013 the mass fraction, \u03bd\u03c3 \u2013 the normalized volume fraction,\nand \u03bdG \u2261 \u03bd \u2013 the Gaussianized volume fraction from (36). The dotted line in the\nright panels shows the predicted Minkowski functionals for a Gaussian random field\nMF\n\nThis expression allows to predict all the Minkowski functionals for a known\ncorrelation function (or power spectrum). We can also take a more empirical\napproach and to determine \u03bb2 on the basis of the observed density field itself,\nusing the relations \u03be(0) = h\u033a2 i and \u03be \u2032\u2032 (0) = h\u033a,2i i, where \u033a,i is the density\nderivative in one coordinate direction.\nThe expected form of these functionals is shown in Fig. 17.\nIn practice, it is easy to obtain good estimates of the Minkowski functionals\nfor periodic fields. The real data, however, is always spatially limited, and the\nlimiting surfaces cut the iso-density surface. An extremely valuable property of\nMinkowski functionals is that such cuts can be corrected for \u2013 the data volume\nmask is another body, and Minkowski functionals of intersecting bodies can\nbe calculated. Moreover, if we can assume homogeneity and isotropy for the\npattern, we can correct for border effects of large surveys. This is too technical\nfor the lecture, so I refer to our forthcoming paper [10].\n3.3 Numerical Algorithms\nSeveral algorithms are used to calculate the Minkowski functionals for a given\ndensity field and a given density threshold. We can either try to follow exactly the geometry of the iso-density surface, e.g., using triangulation, or to\n\n\f30\n\nEnn Saar\n1\n\n0.3\n\n0.9\n0.25\n\n0.8\n0.7\n\n0.2\nMF1\n\nMF0\n\n0.6\n0.5\n\n0.15\n\n0.4\n0.1\n\n0.3\n0.2\n\n0.05\n\n0.1\n0\n\n0\n-4\n\n-3\n\n-2\n\n-1\n\n0\n\u03bd\n\n1\n\n2\n\n3\n\n4\n\n-4\n\n0.2\n\n-3\n\n-2\n\n-1\n\n0\n\u03bd\n\n1\n\n2\n\n3\n\n4\n\n0\n\n1\n\n2\n\n3\n\n4\n\n0.2\n\n0.15\n\n0.1\n\n0.1\n0\nMF3\n\nMF2\n\n0.05\n0\n-0.05\n\n-0.1\n\n-0.2\n\n-0.1\n-0.3\n\n-0.15\n-0.2\n\n-0.4\n-4\n\n-3\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n-4\n\n-3\n\n-2\n\n-1\n\n\u03bd\n\n\u03bd\n\nFig. 17. Gaussian predictions for Minkowski functionals \u03bb = 1\n\napproximate the excursion set on a simple cubic grid. The first algorithms\nwere designed to calculate the genus only; the algorithm that was proposed\nby Gott, Dickinson, and Melott [4] uses a decomposition of the field into filled\nand empty cells, and another class of algorithms (Coles, Davies, and Pearson [3]) uses a grid-valued density distribution. The grid-based algorithms are\nsimpler and faster, but are believed to be not as accurate as the triangulation\ncodes.\nI advocate a simple grid-based algorithm, based on integral geometry\n(Crofton's intersection formula), proposed by Schmalzing and Buchert ([12]).\nIt is similar to that of [3], and for V3 (genus) coincides with it.\nTo start with, we find the density thresholds for given filling fractions f by\nsorting the grid densities. This is a common step for all grid-based algorithms.\nVertexes with higher densities than the threshold form the excursion set. This\nset is characterized by its basic sets of different dimensions \u2013 points (vertexes),\nedges formed by two neighbouring points, squares (faces) formed by four edges,\nand cubes formed by six faces. The algorithm counts the numbers of elements\nof all basic sets, and finds the values of the Minkowski functionals as\nV0 = a3 N3 ,\n\u0012\n\u0013\n2\n2\n2\nV1 = a\nN2 \u2212 N3 ,\n9\n3\n\u0013\n\u0012\n4\n2\n2\nN1 \u2212 N2 + N3 ,\nV2 = a\n9\n9\n3\nV3 = N0 \u2212 N1 + N2 \u2212 N3 ,\n\n(42)\n\n\fMultiscale Methods\n\n31\n\nwhere a is the grid step, N0 is the number of vertexes, N1 is the number of\nedges, N2 is the number of squares (faces), and N3 is the number of basic\ncubes in the excursion set.\nThis algorithm described above is simple to program, and is very fast,\nallowing the use of Monte-Carlo simulations for error estimation 6 .\nThe first, and most used, algorithm was 'CONTOUR' and its derivations; this\nalgorithm was written by David Weinberg and was probably one of the first\ncosmological public-domain algorithms [15]. However, there is a noticeable\ndifference between the results that our grid algorithm produces and those of\n'CONTOUR'. indexalgorithm:CONTOUR You can see it yourself, comparing the\nMinkowski functional figures in this lecture with Fig. 4 in Peter Coles' chapter.\nAll genus curves ever found by 'CONTOUR' look like that there, very jaggy.\nHow much I have tried, using Gaussian smoothing and changing smoothing\nlengths, I have never been able to reproduce these jaggies by our algorithm.\nAt the same time, the calculated genus curves always follow the Gaussian\nprediction, so there is no bias in either algorithm. As the genus (V3 ) counts\nobjects (balls, holes, handles), it should, in principle, have mainly unit jumps,\nand only occasionally a larger jump. That is what we see in our genus (V3 )\ngraphs. The only conclusion that I can derive is that the algorithm we use is\nmore stable, with a much smaller variance.\n3.4 Shapefinders\nAs Minkowski functionals give a complete morphological description of surfaces, it should be possible to use them to numerically describe the shapes\nof objects, right? For example, to differentiate between fat (spherical) objects\nand thin (cylindrical objects), banana-like superclusters and spiky superclusters. This hope has never died, and different shape descriptors have been proposed (a selection of them is listed in our book [6]). The set of shape descriptors that use Minkowski functionals was proposed by Sahni, Sathyaprakash\nand Shandarin [11], and is called 'shapefinders'. Now, shapefinder definitions\nhave a habit of changing from paper to paper and it is not easy to follow\nthe changes, so I shall give here a careful derivation of the last version of\nshapefinders. Shapefinders are defined as ratios of numbers that are similar to\nMinkowski functionals, but only close. in 3-D, the chain of Minkowski functionals Vi , from V0 to V3 , has gradually diminishing dimensions, from L3 to\nL0 . So, the ratios of neighbours in this chain have a dimension of length; these\nratios make up the first set of shapefinders. The proportionality coefficients\nare chosen to normalize all these ratios to R for a sphere of the radius R.\nTake a surface that is delimiting (shaping) a three-dimensional object (e.g., a\nsupercluster). The definitions of the first three Minkowski functionals (29\u201331)\ncan be rewritten as\n6\n\nA thorough analysis of the algorithm and its application to galaxy distributions\nwill be available, by the time this book will be published ([10]).\n\n\f32\n\nEnn Saar\n\n4\u03c0 3\nR ,\n3\nS = 6V1 = 4\u03c0R2 ,\nC = 3\u03c0V2 = R ,\n\nV = V0 =\n\nwhere the second equalities stand for a sphere of a radius R, V is the volume,\nS \u2013 the surface, and C \u2013 the mean integrated radius of curvature:\n\u0012\n\u0013\nZ\n1\n1\n1\ndS .\n(43)\n+\nC=\nR1 (x) R2 (x)\nS 2\nWhatever formulae you see in the literature, do not use them before checking\nwith the definition below \u2013 these are the true shapefinders:\n3V\n1 V0\n, thickness ,\n=\nS\n2 V1\nS\n2 V1\nH2 =\n, breadth ,\n=\nC\n\u03c0 V2\nC\n3\nH3 =\n= V2 , length .\n4\u03c0\n4\nH1 =\n\n(44)\n(45)\n(46)\n(47)\n\nOnly this normalization gives you Hi = R for a sphere; check it. The descriptive names you see were given by the authors. There is a fourth shapefinder \u2013\nthe genus has been given the honour to stand for it, directly. As genus counts\n\"minus\" things (minus the number of isolated objects, minus the number of\nholes), the fourth Minkowski functional V3 should be a better candidate.\nThe first set of shapefinders is accompanied by the 'second order' shapefinders\nH 2 \u2212 H1\n,\nH 2 + H1\nH 2 \u2212 H1\nK2 =\n,\nH 2 + H1\n\nK1 =\n\nplanarity ,\n\n(48)\n\nfilamentarity .\n\n(49)\n(50)\n\nThese five (six, if you count the genus) numbers describe pretty well the shape\nof smooth (ellipsoidal) surfaces. In this case, the ratios K1 and K2 vary nicely\nfrom 0 to 1, and another frequently used ratio, K1 /K2 , has definite trends\nwith respect to the shape of the object. But things get much more interesting\nwhen you start calculating the shapefinders of real superclusters; as shapes\nget complex, simple meanings disappear. Also, as shapefinders are defined as\nratios of observationally estimated numbers (or ratios of quadratic combinations, as Hi work out in terms of Vi ), they are extremely noisy. So, in case\nof serious use, a procedure should be developed to estimate the shapefinders\n\n\fMultiscale Methods\n\n33\n\ndirectly, not by the ratio rules (44, 48); such a procedure does not exist yet.\nBut, for the moment, the shapefinders are probably the best shape description\ntool (for cosmology) we have.\n3.5 Morphology of Wavelet-Denoised Density\nSo much for the preliminaries. Now we have all our tools (wavelets, densities,\nMinkowski functionals). Let us use them and see what is the morphology of\nthe real galaxy density distribution.\nThis question has been asked and answered about a hundred times, starting from the first topology paper by Gott et al [4]. The first data set was a\ncube from the CfA I sample and contained 123 galaxies, if I remember right.\n(Imagine estimating a spatial density on the basis of a hundred points; this\nis the moment that Landau's definition of a cosmologist is appropriate: \"Cosmologists are often wrong, but never in doubt\".) The answer was \u2013 Gaussian ! ;\nten points for courage. The same optimistic answer has been heard about a\nhundred times since (in all the papers published), with slight corrections in\nlater times, as data is getting better. These corrections have been explained\nby different observational and dynamical effects, and peace reigns.\nBut \u2013 all these studies have carefully smoothed the galaxy catalogues by\nnice wide Gaussian kernels to get a proper density. Doubts have been expressed\nthat one Gaussianity could lead to another (by Peter Coles, for example), and\nthe nice Gaussian behaviour we get is exactly that we have built in in the\ndensity field. So, let us wavelet-denoise the density field (a complex recipe,\nbut without any Gaussians), and estimate the Minkowski functionals. The\nnext two figures are from our recent work ([7]).\nThe data are the 2dFGRS Northern galaxies; we chose a maximum-volume\none-magnitude interval volume-limited (constant density) sample and cut a\nmaximum-volume brick from it (interesting, every time I say 'maximum', the\nsample gets smaller). Constant density is necessary, otherwise typical density\nscales will change with distance, and a brick was cut in order to avoid border\ncorrections (these can bring additional difficulties for wavelet denoising). We\ncarefully wavelet-denoised the density; Fig. 18 shows its fourth Minkowski\nfunctional.\nFirst, we see that our wavelet.denoised density is never close to Gaussian.\nSecondly, the specially built N-body catalogues (mocks) do not have Gaussian\nmorphology, too; although they deviate from the real data, they are closer to\nit than to the example Gaussian. We see also that the same galaxies, smoothed\nby a yet very narrow Gaussian kernel (\u03c3 = 2 Mpc/h), show an almost Gaussian\nmorphology. Thus, the clear message of this figure is that the morphology of\na good (we hope the best) adaptive galaxy density is far from Gaussian; and\nthe Gaussianity-confirming results obtained so far are all the consequence of\nthe Gaussianity input by hand \u2013 Gaussian smoothing. This figure exhibits a\nlittle non-Gaussianity yet; the next one (Fig. 19) almost does not. The filter\n\n\f34\n\nEnn Saar\n600\n\n2dF North, denoised\n400\n\n200\n\nV\n\n3\n\n0\nGrf&2dF, \u03c3=2\n\n-200\n\n-400\n\n-600\n-800\n-2\n\n-1\n\n0\n\n\u03bd\n\n1\n\n2\n\n3\n\nFig. 18. The Minkowski functional V3 for the 2dF brick, for the wavelet denoised\ndata set (thick solid line). The variability range of the wavelet denoised mocks is\nshown with bars. We show also the 95% confidence limits for 1300 realizations of\ntheoretical Gaussian density fields (dashed lines), and the V3 data curve (thin solid\nline), all obtained for the Gaussian \u03c3 = 2 Mpc/h smoothing\n\nused there is still narrow (\u03c3 = 4 Mpc/h); the filter widths used in most papers\nare around twice that value.\nI am pretty sure that Gaussian smoothing is the culprit here. We built\ncompletely non-Gaussian density distributions (Voronoi walls and networks),\nGaussian smoothed them using popular recipes about the kernel size, and\nobtained perfectly Gaussian Minkowski functionals.\nOne reason for that, as Vicent Mart\u0131\u0301nez has proposed, is that the severe\nsmoothing used changes a density field into Poissonian, practically. Try it \u2013\nsmooth a density field with a Gaussian of \u03c3 = r0 , where r0 is the correlation\nlength, and you get a density field with a very flat low-amplitude correlation\nfunction, almost Poissonian. And the Minkowski functionals of Poissonian\ndensity fields7 are Gaussian; we tested that.\nAnother reason that turns Minkowski functionals Gaussian even for small\n\u03c3, must be the extended wings of Gaussian kernels. Although they drop pretty\nfast, they are big enough to add to a small extra ripple on the main density\nfield. As Minkowski functionals are extremely sensitive to small density variations, then all they see is that ripple. This is especially well seen in initially\nempty regions. Usually, one uses a FFT-based procedure for Gaussian smooth7\n\nTo be more exact, here is the recipe \u2013 take a Poissonian point process of N points\nin a volume V and smooth it with a Gaussian kernel of \u03c3 = d, where d = (V /N )1/3\nis the mean distance between the points.\n\n\fMultiscale Methods\n\n40\n\n35\n\n2dF North, \u03c3 = 4\n\n20\n\nV3\n\n0\n\n-20\n\n-40\n\n-60\n\n-80\n-2\n\n-1\n\n0\n\n\u03bd\n\n1\n\n2\n\n3\n\nFig. 19. The Minkowski functional V3 for the 2dF brick, Gaussian smoothed with\n\u03c3 = 4 Mpc/h (thick solid line). The variability range of the mocks is shown with\nbars. We show also the 95% confidence limits for 1300 realizations of theoretical\nGaussian density fields (dashed lines).\n\ning, as that is at least a hundred times faster (for present catalogue volumes)\nthat direct convolution in real space. This procedure generates a wildly fluctuating small-amplitude density field in empty regions, and we did not realize\nfor a long time at first where those giant-amplitude ghost MF-s came from.\nGaussian smoothing and FFT, that was their address.\n3.6 Multiscale Morphology\nSo, the galaxy density, similar to that we have accustomed to find in our\neveryday experience, is decidedly non-Gaussian. But is that a problem? Cosmological dynamics tells us that structure evolves at different rate at different\nscales; a true density mixes these scales all together and is not the best object\nto search for elusive traces of initial conditions. Good. Multiscale densities to\nthe rescue.\nThe results that will end this chapter did not exist at the time of the\nsummer school. But we live in the present, and time is short, so I will include\nthem. A detailed account is already accepted for publication ([10]), I shall\nshow only a collection of Minkowski functionals here.\nAs our basic data set, we took the 2dFGRS volume-limited samples for\nthe [\u221220, \u221219] magnitude interval; they have the highest mean density among\nsimilar one-magnitude interval samples. We did not cut bricks this time, but\ncorrected for sample boundaries; we have learnt that by now. We waveletdecomposed the galaxy density fields and found the Minkowski functionals;\n\n\f36\n\nEnn Saar\n\nas simple as that. Although wavelet decompositions are linear and should not\nadd anything to the morphology of the fields, we checked that on simulated\nGaussian density fields. Right, they do not add any extra morphological signal.\nThe results (for the 2dFGRS North) are shown in Figs. 20\u201322. As the\namplitudes of the (densities of) Minkowski functionals vary in a large interval,\nwe use a sign-aware logarithmic mapping:\nlogn(x; a) = sgn(x) log(1 + |x|/a) .\nThis mapping accepts arguments of both signs (log(x) does not), is linear for\n|x| << a and logarithmic for |x| >> a. As the figures show, for the scales\npossible to study, the morphology of the galaxy density distribution is decidedly not Gaussian. The deviations are not too large for the second Minkowski\nfunctional v2 (watch how the maxima shift around), but are \u221a\nclearly seen for\nthe two others. The maximum wavelet order \u221a\nhere is 3 for a 2 Mpc/h grid,\nthat corresponds to a characteristic scale of 23 2 \u2248 11.3 Mpc/h. As the mean\nthickness of the 2dFGR North slice is about 40\u201350 Mpc/h, we cannot go much\nfurther \u2013 the higher order wavelet slices would be practically two-dimensional.\nThe 2dFGRS Southern dataset has similar size limitations. So, as 10 Mpc/h is\na scale where cosmological dynamics might have slight morphological effects,\nthe question if the original morphology of the cosmological density field was\nGaussian, is not answered yet. But we shall find it out soon, when the SDSS\nwill finally fill its full planned volume.\n2\n1.8\n1.6\n\nlogn(v1 x 100)\n\n1.4\n1.2\n1\n0.8\n0.6\n0.4\n0.2\n0\n-2\n\n-1\n\n0\n\u03bd\n\n1\n\n2\n\nFig. 20. Summary of the densities of the second MF v1 for the data and all wavelet\norders for the 2dFN19 sample, in the logn mapping. Thick lines show reference\nGaussian\npredictions. Full lines stand for the 1 Mpc/h grid, dotted lines \u2013 for the\n\u221a\n2 Mpc/h grid\n\n\fMultiscale Methods\n\n37\n\n5\n4\n3\n\n4\n\nlogn(v2 x 10 )\n\n2\n1\n0\n-1\n-2\n-3\n-4\n-5\n-2\n\n-1\n\n0\n\n1\n\n2\n\n\u03bd\n\nFig. 21. Summary of the densities of the third MF v2 for the data and all wavelet\norders for the 2dFN19 sample, in the logn mapping. Thick lines show reference\nGaussian\npredictions. Full lines stand for the 1 Mpc/h grid, dotted lines \u2013 for the\n\u221a\n2 Mpc/h grid\n6\n4\n\n5\n\nlogn(v3 x 10 )\n\n2\n0\n-2\n-4\n-6\n-8\n-2\n\n-1\n\n0\n\u03bd\n\n1\n\n2\n\nFig. 22. Summary of the densities of the fourth MF v3 for the data and all wavelet\norders for the 2dFN19 sample, in the logn mapping. Thick lines show reference\nGaussian\npredictions. Full lines stand for the 1 Mpc/h grid, dotted lines \u2013 for the\n\u221a\n2 Mpc/h grid\n\n\f38\n\nEnn Saar\n\nThese are the results, for the moment. The morphology of the galaxy\ndensity field is far from Gaussian, in contrary to practically every earlier result.\nHave all these results only confirmed that if we take a serious smoothing effort,\nwe are able to smooth any density field to Poissonian? An indecent thought.\nBut do not be worried, results come and go, this is the nature of research.\nMethods, on the contrary, stay a little longer, and this school was all about\nmethods.\n\nRecommended Reading\nI was surprised that Bernard Jones recommended a wavelet bookshelf that\nis almost completely different from mine (the only common book is that of\nI. Daubechies'). So, read those of Bernard's choice, and add mine:\n\u2022\n\n\u2022\n\n\u2022\n\nfive books:\n1. Stephane Mallat, A Wavelet Tour of Signal Processing, 2nd ed., Academic Press, London, 1999,\n2. C. Sidney Burrus, Ramesh A. Gopinath, Haitao Gao, Introduction to\nWavelets and Wavelet Transforms, Prentice Hall, NJ, 1997,\n3. Ingrid Daubechies, Ten lectures on wavelets, SIAM, Philadelphia, 2002,\n4. Jean-Luc Starck, Fionn Murtagh, \"Astronomical Image and Data Analysis\", 2nd ed., Springer, 2006 (application of wavelets and many other\nwonderful image processing methods in astronomy),\n5. Bernard W. Silverman, Density Estimation for Statistics and Data\nAnalysis, Chapman & Hall / CRC Press, Boca Raton, 1986 (the classical text on density estimation),\nthree articles:\n1. Mark J. Shensa, The Discrete wavelet Transform: Wedding the \u00c0 Trous\nand Mallat Algorithms, IEEE Transactions on Signal Processing, 40,\n2464\u20132482, 1992 (the title explains it all),\n2. K.R. Mecke, T. Buchert, H. Wagner, Robust morphological measures\nfor large-scale structure in the Universe, Astron. Astrophys. 288, 697\u2013\n704, 1994 (introducing Minkowski functionals in cosmology),\n3. Jens Schmalzing, Thomas Buchert, Beyond Genus Statistics: A Unified\nApproach to the Morphology of Cosmic Structure, Ap. J. Letts 482,\nL1, 1997, (presentation of two grid algorithms).\ntwo web pages:\n1. a wavelet tutorial by Jean-Luc Starck at (http://jstarck.free.fr),\n2. the wavelet pages by David Donoho at http://www-stat.stanford.\nedu/~donoho/ (look at lectures and reports).\n\n\fMultiscale Methods\n\n39\n\nAcknowledgements\nI was introduced to wavelets in about 1990 by Ivar Suisalu (he was my PhD\nstudent then). As that happened in NORDITA, I asked advice from Bernard\nJones soon, and started on a wavelet road, together with Vicent Mart\u0131\u0301nez\nand Silvestre Paredes; these early wavelets were continuous. Much later, I\nhave returned to wavelets, and have learnt much from Bernard, who is using\nwavelets in the real world, and from Vicent, Jean-Luc Starck, and David\nDonoho, the members of our multiscale morphology group. I thank them all\nfor pleasant collaboration and knowledge shared. All the results presented\nhere belong to our morphology group.\nMy present favourites are the \u00e0 trous wavelets, as you have noticed.\nMy research has been supported in Estonia by the Estonian Science Foundation grant 6104, and by the Estonian Ministry of Education research project\nTO-0060058S98. In Spain, I have been supported by the University of Valencia via a visiting professorship, and by the Spanish MCyT project AYA200308739-C02-01 (including FEDER).\n\nReferences\n1.\n2.\n3.\n4.\n5.\n\n6.\n7.\n8.\n9.\n10.\n11.\n12.\n13.\n14.\n15.\n\nY. Ascasibar, J. Binney, MNRAS 356, 872\u2013882, 2005\nC. de Boor, A Practical Guide to Splines. Springer-Verlag, New York, 1978\nP. Coles, A.G. Davies, R.C. Pearson, MNRAS 281, 1375, 1996\nJ.R. Gott, M. Dickinson, A.L. Melott, ApJ. 306, 341 1986\nM. Holtschneider, R. Kronland-Martinet, J. Morlet, P. Tchamitchian. In:\nWavelets, Time-Frequency Methods and Phase Space, Springer-Verlag, Berlin,\n289\u2013297, 1989\nV.J. Mart\u0131\u0301nez, E. Saar, Statistics of the Galaxy Distribution, Chapman & Hall\n/CRC Press, Boca Raton, 2002\nV.J. Mart\u0131\u0301nez, J.-L. Starck, E. Saar, D.L. Donoho, S.C. Reynolds, P. de la Cruz,\nS. Paredes, ApJ. 634, 744\nK.R. Mecke, T. Buchert, H. Wagner, A&A 288, 697\u2013704, 1994\nW.H. Press, B.P. Flannery, S.A. Teukolsky, W.T. Vetterling, Numerical Recipes\nin C: The Art of Scientific Computing, CUP, Cambridge, 1992\nE. Saar, V.J. Mart\u0131\u0301nez, J.-L. Starck, D. Donoho, MNRAS (accepted), astroph/0610958, 2006\nV. Sahni, B.S. Sathyaprakash, S.F. Shandarin, ApJ. Lett. 495, L5\u2013L8, 1998\nJ. Schmalzing, T. Buchert, ApJ. Lett. 482, L1, 1997\nB.W. Silverman, Density Estimation for Statistics and Data Analysis, Chapman\n& Hall / CRC Press, Boca Raton, 1986\nJ.-L. Starck, F. Murtagh, Astronomical Image and Data Analysis, 2nd ed.,\nSpringer, NY, 2006\nD.H. Weinberg, PASP 100, 1373\u20131385, 1988\n\n\f"}