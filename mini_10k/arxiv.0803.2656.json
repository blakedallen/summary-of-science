{"id": "http://arxiv.org/abs/0803.2656v1", "guidislink": true, "updated": "2008-03-18T15:22:25Z", "updated_parsed": [2008, 3, 18, 15, 22, 25, 1, 78, 0], "published": "2008-03-18T15:22:25Z", "published_parsed": [2008, 3, 18, 15, 22, 25, 1, 78, 0], "title": "A New Central Limit Theorem under Sublinear Expectations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0803.4314%2C0803.0537%2C0803.1304%2C0803.0476%2C0803.0943%2C0803.0143%2C0803.2278%2C0803.3787%2C0803.3505%2C0803.1560%2C0803.0726%2C0803.3548%2C0803.3183%2C0803.1044%2C0803.3819%2C0803.2869%2C0803.3159%2C0803.1095%2C0803.4208%2C0803.0326%2C0803.1357%2C0803.2659%2C0803.1697%2C0803.1346%2C0803.0551%2C0803.2484%2C0803.4154%2C0803.1909%2C0803.2489%2C0803.0787%2C0803.1839%2C0803.0932%2C0803.1784%2C0803.3408%2C0803.1317%2C0803.1872%2C0803.3970%2C0803.1348%2C0803.0569%2C0803.4481%2C0803.2967%2C0803.3253%2C0803.3738%2C0803.2927%2C0803.3272%2C0803.2942%2C0803.3024%2C0803.0262%2C0803.4493%2C0803.0280%2C0803.1966%2C0803.1793%2C0803.1316%2C0803.0285%2C0803.2860%2C0803.0381%2C0803.1036%2C0803.1273%2C0803.0472%2C0803.0304%2C0803.3608%2C0803.2458%2C0803.4296%2C0803.3362%2C0803.0369%2C0803.1429%2C0803.4299%2C0803.4123%2C0803.0196%2C0803.3915%2C0803.3674%2C0803.4370%2C0803.1287%2C0803.4264%2C0803.0187%2C0803.3340%2C0803.3901%2C0803.3008%2C0803.4335%2C0803.2845%2C0803.0290%2C0803.3395%2C0803.0752%2C0803.0748%2C0803.1238%2C0803.2164%2C0803.3830%2C0803.2445%2C0803.3874%2C0803.2269%2C0803.2121%2C0803.3551%2C0803.3356%2C0803.3956%2C0803.0795%2C0803.3019%2C0803.0038%2C0803.3919%2C0803.4442%2C0803.1089%2C0803.2656&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A New Central Limit Theorem under Sublinear Expectations"}, "summary": "We describe a new framework of a sublinear expectation space and the related\nnotions and results of distributions, independence. A new notion of\nG-distributions is introduced which generalizes our G-normal-distribution in\nthe sense that mean-uncertainty can be also described. W present our new result\nof central limit theorem under sublinear expectation. This theorem can be also\nregarded as a generalization of the law of large number in the case of\nmean-uncertainty.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0803.4314%2C0803.0537%2C0803.1304%2C0803.0476%2C0803.0943%2C0803.0143%2C0803.2278%2C0803.3787%2C0803.3505%2C0803.1560%2C0803.0726%2C0803.3548%2C0803.3183%2C0803.1044%2C0803.3819%2C0803.2869%2C0803.3159%2C0803.1095%2C0803.4208%2C0803.0326%2C0803.1357%2C0803.2659%2C0803.1697%2C0803.1346%2C0803.0551%2C0803.2484%2C0803.4154%2C0803.1909%2C0803.2489%2C0803.0787%2C0803.1839%2C0803.0932%2C0803.1784%2C0803.3408%2C0803.1317%2C0803.1872%2C0803.3970%2C0803.1348%2C0803.0569%2C0803.4481%2C0803.2967%2C0803.3253%2C0803.3738%2C0803.2927%2C0803.3272%2C0803.2942%2C0803.3024%2C0803.0262%2C0803.4493%2C0803.0280%2C0803.1966%2C0803.1793%2C0803.1316%2C0803.0285%2C0803.2860%2C0803.0381%2C0803.1036%2C0803.1273%2C0803.0472%2C0803.0304%2C0803.3608%2C0803.2458%2C0803.4296%2C0803.3362%2C0803.0369%2C0803.1429%2C0803.4299%2C0803.4123%2C0803.0196%2C0803.3915%2C0803.3674%2C0803.4370%2C0803.1287%2C0803.4264%2C0803.0187%2C0803.3340%2C0803.3901%2C0803.3008%2C0803.4335%2C0803.2845%2C0803.0290%2C0803.3395%2C0803.0752%2C0803.0748%2C0803.1238%2C0803.2164%2C0803.3830%2C0803.2445%2C0803.3874%2C0803.2269%2C0803.2121%2C0803.3551%2C0803.3356%2C0803.3956%2C0803.0795%2C0803.3019%2C0803.0038%2C0803.3919%2C0803.4442%2C0803.1089%2C0803.2656&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We describe a new framework of a sublinear expectation space and the related\nnotions and results of distributions, independence. A new notion of\nG-distributions is introduced which generalizes our G-normal-distribution in\nthe sense that mean-uncertainty can be also described. W present our new result\nof central limit theorem under sublinear expectation. This theorem can be also\nregarded as a generalization of the law of large number in the case of\nmean-uncertainty."}, "authors": ["Shige Peng"], "author_detail": {"name": "Shige Peng"}, "author": "Shige Peng", "links": [{"href": "http://arxiv.org/abs/0803.2656v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0803.2656v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "60H10, 60H05, 60H30", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0803.2656v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0803.2656v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:0803.2656v1 [math.PR] 18 Mar 2008\n\nA New Central Limit Theorem under Sublinear\nExpectations\nShige PENG\u2217\nInstitute of Mathematics\nShandong University\n250100, Jinan, China\npeng@sdu.edu.cn\nversion: March 18, 2008\n\nAbstract\nWe describe a new framework of a sublinear expectation space and the\nrelated notions and results of distributions, independence. A new notion of\nG-distributions is introduced which generalizes our G-normal-distribution\nin the sense that mean-uncertainty can be also described. W present our\nnew result of central limit theorem under sublinear expectation. This\ntheorem can be also regarded as a generalization of the law of large number\nin the case of mean-uncertainty.\n\n1\n\nIntroduction\n\nThe law of large numbers (LLN) and central limit theorem (CLT) are long and\nwidely been known as two fundamental results in the theory of probability and\nstatistics. A striking consequence of CLT is that accumulated independent and\nidentically distributed random variables tends to a normal distributed random\nvariable whatever is the original distribution. It is a very useful tool in finance\nsince many typical financial positions are accumulations of a large number of\nsmall and independent risk positions. But CLT only holds in cases of model\ncertainty. In this paper we are interested in CLT with mean and varianceuncertainty.\nRecently problems of model uncertainties in statistics, measures of risk and\nsuperhedging in finance motivated us to introduce, in [13] and [14] (see also\n[11], [12] and references herein), a new notion of sublinear expectation, called\n\"G-expectation\", and the related \"G-normal distribution\" (see Def. 4.5) from\nwhich we were able to define G-Brownian motion as well as the corresponding\n\u2217 The author thanks the partial support from The National Basic Research Program of\nChina (973 Program) grant No. 2007CB814900 (Financial Risk).\n\n1\n\n\fstochastic calculus. The notion of G-normal distribution plays the same important rule in the theory of sublinear expectation as that of normal distribution\nin the classic probability theory. It is then natural and interesting to ask if\nwe have the corresponding LLN and CLT under a sublinear expectation and,\nin particular, if the corresponding limit distribution of the CLT is a G-normal\ndistribution. This paper gives an affirmative answer. We will prove that the\naccumulated risk positions converge 'in law' to the corresponding G-normal distribution, which is a distribution under sublinear expectation. In a special case\nwhere the mean and variance uncertainty becomes zero, the G-normal distribution becomes the classical normal distribution. Technically we introduce a new\nmethod to prove a CLT under a sublinear expectation space. This proof of our\nCLT is short since we borrow a deep interior estimate of fully nonlinear PDE in\n[5]. The assumptions of our CLT can be still improved.\nThis paper is organized as follows: in Section 2 we describe the framework\nof a sublinear expectation space. The basic notions and results of distributions,\nindependence and the related product space of sublinear will be presented in\nSection 3. In Section 4 we introduce a new notion of G-distributions which\ngeneralizes our G-normal-distribution in the sense that mean-uncertainty can\nbe also described. Finally, in Section 5, we present our main result of CLT under\nsublinear expectation. For reader's convenience we present some basic results\nof viscosity solutions in the Appendix.\nThis paper is a new and generalized version of [15] in which only variance\nuncertainty was considered for random variables instead random random vectors. Our new CLT theorem can be applied to the case where both meanuncertainty and variance-uncertainty cannot be negligible. This theorem can\nbe also regarded as a new generalization of LLN. We refer to [9] and [10] for the\ndevelopments of LLN with non-additive probability measures.\n\n2\n\nBasic settings\n\nFor a given positive integer n we will denote by hx, yi the scalar product of x,\ny \u2208 Rn and by |x| = (x, x)1/2 the Euclidean norm of x. We denote by S(n) the\ncollection of n \u00d7 n symmetric matrices and by S+ (n) the non negative elements\nin S(n). We observe that S(n) is an Euclidean space with the scalar product\nhP, Qi = tr[P Q].\nIn this paper we will consider the following type of spaces of sublinear expectations: Let \u03a9 be a given set and let H be a linear space of real functions\ndefined on \u03a9 such that if X1 , * * * , Xn \u2208 H then \u03c6(X1 , * * * , Xn ) \u2208 H for each\n\u03c6 \u2208 Cl.Lip (Rn ) where Cl.Lip (Rn ) denotes the linear space of (local Lipschitz)\nfunctions \u03c6 satisfying\n|\u03c6(x) \u2212 \u03c6(y)| \u2264 C(1 + |x|m + |y|m )|x \u2212 y|, \u2200x, y \u2208 Rn ,\nfor some C > 0, m \u2208 N depending on \u03c6.\n\nH is considered as a space of \"random variables\". In this case we denote X =\n(X1 , * * * , Xn ) \u2208 Hn .\n2\n\n\fRemark 2.1 In particular, if X, Y \u2208 H, then |X|, X m \u2208 H are in H. More\ngenerally \u03c6(X)\u03c8(Y ) is still in H if \u03c6, \u03c8 \u2208 Cl.Lip (R).\nHere we use Cl.Lip (Rn ) in our framework only for some convenience of technicalities. In fact our essential requirement is that H contains all constants and,\nmoreover, X \u2208 H implies |X| \u2208 H. In general Cl.Lip (Rn ) can be replaced by\nthe following spaces of functions defined on Rn .\n\u2022 L\u221e (Rn ): the space bounded Borel-measurable functions;\n\u2022 Cb (Rn ): the space of bounded and continuous functions;\n\u2022 Cbk (Rn ): the space of bounded and k-time continuously differentiable functions with bounded derivatives of all orders less than or equal to k;\n\u2022 Cunif (Rn ): the space of bounded and uniformly continuous functions;\n\u2022 Cb.Lip (Rn ): the space of bounded and Lipschitz continuous functions.\nDefinition 2.2 A sublinear expectation \u00ca on H is a functional \u00ca : H 7\u2192 R\nsatisfying the following properties: for all X, Y \u2208 H, we have\n(a) Monotonicity:\n(b) Constant preserving:\n(c) Sub-additivity:\n(d) Positive homogeneity:\n\nIf X \u2265 Y then \u00ca[X] \u2265 \u00ca[Y ].\n\u00ca[c] = c.\n\u00ca[X] \u2212 \u00ca[Y ] \u2264 \u00ca[X \u2212 Y ].\n\u00ca[\u03bbX] = \u03bb\u00ca[X], \u2200\u03bb \u2265 0.\n\n(In many situation (c) is also called property of self\u2013domination). The triple\n(\u03a9, H, \u00ca) is called a sublinear expectation space (compare with a probability space (\u03a9, F , P)). If only (c) and (d) are satisfied \u00ca is called a sublinear\nfunctional.\nRemark 2.3 Just as in the framework of a probability space, a sublinear expectation space can be a completed Banach space under its natural norm k*k =\u00ca[|*|]\n(see [11]-[16]) and by using its natural capacity \u0109(*) induced via \u00ca[|*|] (see [4]\nand [3]). But the results obtained in this paper do not need the assumption of\nthe space-completion.\nLemma 2.4 Let E be a sublinear functional defined on (\u03a9, H), i.e., (c) and\n(d) hold for E. Then there exists a family Q of linear functional on (\u03a9, H) such\nthat\nE[X] := sup E[X], \u2200E \u2208 Q.\nE\u2208Q\n\nand such that, for each X \u2208 H, there exists a E \u2208 Q such that E[X] := E[X].\nIf we assume moreover that (a) holds (resp. (a), (b) hold) for E, then (a)\nalso holds (resp. (a), (b) hold) for each E \u2208 Q.\n3\n\n\fProof. Let Q be the family of all linear functional dominated by E, i.e., E[X] \u2264\nE[X], for all X \u2208 H, E \u2208 Q. We first prove that Q is non empty. For a given\nX \u2208 H, we denote L = {aX : a \u2208 R} which is a subspace of H. We define\nI : L \u2192 R by I[aX] = aE[X], \u2200a \u2208 R, then I[*] forms a linear functional on\nL and I \u2264E on L. Since E[*] is sub-additive and positively homogeneous, by\nHahn-Banach theorem (see e.g. [19]pp102) there exists a linear functional E\non H such that E = I on L and E \u2264 E on H. Thus E is a linear functional\ndominated by E such that E[X] := E[X]. We now define\nEQ [X] , sup E[X].\nE\u2208Q\n\nIt is clear that EQ =E.\nIf (a) holds for E, then for each non negative element X \u2208 H, for each\nE \u2208 Q, E[X] = \u2212E[\u2212X] \u2265 \u2212E[\u2212X] \u2265 0, thus (a) also holds for E. If\nmoreover (b) holds for E, then for each c \u2208 R, \u2212E[c] = E[\u2212c] \u2264 E[\u2212c] = \u2212c\nand E[c] \u2264 E[c] = c, we get E[c] = c. The proof is complete.\nExample 2.5 For some \u03c6 \u2208 Cl.Lip (R), \u03be \u2208 H, let \u03c6(\u03be) be a gain value favorable to a banker of a game. The banker can choose among a set of distribution\n{F (\u03b8, A)}A\u2208B(R),\u03b8\u2208\u0398 of a random variable \u03be. In this situation the robust expectation of the risk for a gamblers opposite to the banker is:\nZ\n\u00ca[\u03c6(\u03be)] := sup \u03c6(x)F (\u03b8, dx).\n\u03b8\u2208\u0398\n\n3\n\nR\n\nDistributions, independence and product spaces\n\nWe now consider the notion of the distributions of random variables under\nsublinear expectations. Let X = (X1 , * * * , Xn ) be a given n-dimensional random\nvector on a sublinear expectation space (\u03a91 , H1 , \u00ca). We define a functional on\nCl.Lip (Rn ) by\nF\u0302X [\u03c6] := \u00ca[\u03c6(X)] : \u03c6 \u2208 Cl.Lip (Rn ) 7\u2192 (\u2212\u221e, \u221e).\n\n(1)\n\nThe triple (Rn , Cl.Lip (Rn ), F\u0302X [*]) forms a sublinear expectation space. F\u0302X is\ncalled the distribution of X.\nDefinition 3.1 Let X1 and X2 be two n\u2013dimensional random vectors defined\nrespectively in sublinear expectation spaces (\u03a91 , H1 , \u00ca1 ) and (\u03a92 , H2 , \u00ca2 ). They\nd\n\nare called identically distributed, denoted by X1 = X2 , if\n\u00ca1 [\u03c6(X1 )] = \u00ca2 [\u03c6(X2 )],\nd\n\n\u2200\u03c6 \u2208 Cl.Lip (Rn ).\n\nIt is clear that X1 = X2 if and only if their distributions coincide.\n\n4\n\n\fRemark 3.2 If the distribution F\u0302X of X \u2208 H is not a linear expectation, then\nX is said to have distributional uncertainty. The distribution of X has the\nfollowing four typical parameters:\n\u03c3\u0304 2 := \u00ca[X 2 ], \u03c3 2 := \u2212\u00ca[\u2212X 2 ].\n\n\u03bc\u0304 := \u00ca[X], \u03bc := \u2212\u00ca[\u2212X],\n\nThe subsets [\u03bc, \u03bc\u0304] and [\u03c3 2 , \u03c3\u0304 2 ] characterize the mean-uncertainty and the varianceuncertainty of X. The problem of zero-mean uncertainty have been studied in\n[P3], [P4]. In this paper the mean uncertainty will be in our consideration.\nThe following simple property is very useful in our sublinear analysis.\nProposition 3.3 Let X, Y \u2208 H be such that \u00ca[Y ] = \u2212\u00ca[\u2212Y ], i.e. Y has no\nmean uncertainty. Then we have\n\u00ca[X + Y ] = \u00ca[X] + \u00ca[Y ].\nIn particular, if \u00ca[Y ] = \u00ca[\u2212Y ] = 0, then \u00ca[X + Y ] = \u00ca[X].\nProof. It is simply because we have \u00ca[X + Y ] \u2264 \u00ca[X] + \u00ca[Y ] and\n\u00ca[X + Y ] \u2265 \u00ca[X] \u2212 \u00ca[\u2212Y ] = \u00ca[X] + \u00ca[Y ].\nThe following notion of independence plays a key role:\nDefinition 3.4 In a sublinear expectation space (\u03a9, H, \u00ca) a random vector Y =\n(Y1 , * * * , Yn ), Yi \u2208 H is said to be independent to another random vector X =\n(X1 , * * * , Xm ), Xi \u2208 H under \u00ca[*] if for each test function \u03c6 \u2208 Cl.Lip (Rm \u00d7 Rn )\nwe have\n\u00ca[\u03c6(X, Y )] = \u00ca[\u00ca[\u03c6(x, Y )]x=X ].\nRemark 3.5 In the case of linear expectation, this notion of independence is\njust the classical one. It is important to note that under sublinear expectations\nthe condition \"Y is independent to X\" does not implies automatically that \"X\nis independent to Y \".\nExample 3.6 We consider a case where X, Y \u2208 H are identically distributed\nand \u00ca[X] = \u00ca[\u2212X] = 0 but \u03c3\u0304 2 = \u00ca[X 2 ] > \u03c3 2 = \u2212\u00ca[\u2212X 2 ]. We also assume\nthat \u00ca[|X|] = \u00ca[X + + X \u2212 ] > 0, thus \u00ca[X + ] = 21 \u00ca[|X| + X] = 21 \u00ca[|X|] > 0. In\nthe case where Y is independent to X, we have\n\u00ca[XY 2 ] = \u00ca[X + \u03c3\u0304 2 \u2212 X \u2212 \u03c3 2 ] = (\u03c3\u0304 2 \u2212 \u03c3 2 )\u00ca[X + ] > 0.\nBut if X is independent to Y we have\n\u00ca[XY 2 ] = 0.\n\n5\n\n\fThe independence property of two random vectors X, Y involves only the\njoint distribution of (X, Y ). The following construction tells us how to construct\nrandom vectors with given sublinear distributions and with joint independence.\nDefinition 3.7 Let (\u03a9i , Hi , \u00cai ), i = 1, 2 be two sublinear expectation spaces.\nWe denote by\nH1 \u00d7 H2 := {Z(\u03c91 , \u03c92 ) = \u03c6(X(\u03c91 ), Y (\u03c92 )) : (\u03c91 , \u03c92 ) \u2208 \u03a91 \u00d7 \u03a92 ,\n\n(X, Y ) \u2208 (H1 )m \u00d7 (H2 )n , \u03c6 \u2208 Cl.Lip (Rm \u00d7 Rn ), m, n = 1, 2, * * * },\n\nand, for each random variable of the above form Z(\u03c91 , \u03c92 ) = \u03c6(X(\u03c91 ), Y (\u03c92 )),\n(\u00ca1 \u00d7 \u00ca2 )[Z] := \u00ca1 [\u03c6\u0304(X)], where \u03c6\u0304(x) := \u00ca2 [\u03c6(x, Y )], x \u2208 Rm .\nIt is easy to check that the triple (\u03a91 \u00d7 \u03a92 , H1 \u00d7 H2 , \u00ca1 \u00d7 \u00ca2 ) forms a sunlinear expectation space. We call it the product space of sublinear expectation of\n(\u03a91 , H1 , \u00ca1 ) and (\u03a92 , H2 , \u00ca2 ). In this way we can define the product space of\nsublinear expectation\nn\nn\nn\nY\nY\nY\n( \u03a9i , Hi , \u00cai )\ni=1\n\ni=1\n\ni=1\n\nof any given sublinear expectation spaces (\u03a9i , Hi , \u00cai ), i = 1, 2, * * * , n. In particular, when (\u03a9i , Hi , \u00cai ) = (\u03a91 , H1 , \u00ca1 ) we have the product space of the form\n\u2297n\n\u2297n\n(\u03a9\u2297n\n1 , H1 , \u00ca1 ).\nThe following property is easy to check.\nProposition 3.8 Let Xi be ni -dimensional random vectors in sublinear expectation spaces (\u03a9i , Hi , \u00cai ), for i = 1, * * * , n, respectively. We denote\nYi (\u03c91 , * * * , \u03c9n ) := Xi (\u03c9i ), i = 1, * * * , n.\nThen Yi , i = 1, * * * , n are random variables in the product space of sublinear\nn\nn\nn\nY\nY\nY\nd\nexpectation ( \u03a9i , Hi , \u00cai ). Moreover we have Yi = Xi and Yi+1 is indei=1\n\ni=1\n\ni=1\n\npendent to (Y1 , * * * , Yi ), for each i.\nMoreover, if (\u03a9i , Hi , \u00cai ) = (\u03a91 , H1 , \u00ca1 ) and Xi = X1 , for all i, then we also\nd\n\nhave Yi = Y1 . In this case Yi ia called independent copies of Y1 for i = 2, * * * , n.\n\nThe situation \"Y is independent to X\" often appears when Y occurs after\nX, thus a very robust sublinear expectation should take the information of X\ninto account. We consider the following example: Let Y = \u03c8(\u03be, \u03b8), \u03c8 \u2208 Cb (R2 ),\nwhere \u03be and X are two bounded random variables in a classical probability space\n(\u03a9, F , P ) and \u03b8 is a completely unknown parameter valued in a given interval\n[a, b]. We assume that \u03be is independent of X under P in the classical sense.\n\n6\n\n\fOn the space (\u03a9, H) with H := {\u03c6(X, Y ) : \u03c6 \u2208 Cl.Lip (R2 )}, we can define the\nfollowing three robust sublinear expectations:\nE1 [\u03c6(X, Y )] = sup EP [\u03c6(X, \u03c8(\u03be, \u03b8)], E2 [\u03c6(X, Y )] = EP [ sup \u03c6(X, \u03c8(\u03be, \u03b8)],\n\u03b8\u2208[a,b]\n\n\u03b8\u2208[a,b]\n\nE3 [\u03c6(X, Y )] = EP [{ sup EP [\u03c6(x, \u03c8(\u03be, \u03b8)]}x=X ].\n\u03b8\u2208[a,b]\n\nBut it is seen that only under the sublinear expectation E3 that Y is independent\nto X.\nRemark 3.9 It is possible that the above parameter \u03b8 is in fact a function of X\nand \u03be: \u03b8 = \u0398(X, \u03be) where \u0398 is a completely unknown function valued in [a, b],\nthus Y = \u03c8(\u03be, \u0398(X, \u03be)) is dependent to X in the classical sense. But since \u0398 is\na completely unknown function a robust expectation is E3 .\n\u221e\n\nDefinition 3.10 A sequence of d-dimensional random vectors {\u03b7i }i=1 in H is\nsaid to converge\nin distribution under \u00ca if for each \u03c6 \u2208 Cb (Rn ) the sequence\nn\no\n\u00ca[\u03c6(\u03b7i )]\n\n4\n\n\u221e\n\ni=1\n\nconverges.\n\nG-distributed random variables\n\nGiven a pair of d-dimensional random vectors (X, Y ) in a sublinear expectation\nspace (\u03a9, H, \u00ca), we can define a function\n1\nG(p, A) := \u00ca[ hAX, Xi + hp, Y i],\n2\n\n(p, A) \u2208 S(d) \u00d7 Rd\n\n(2)\n\nIt is easy to check that G : Rd \u00d7 S(d) 7\u2192 R is a sublinear function monotonic in\nA \u2208 S(d) in the following sense: For each p, p\u0304 \u2208 Rd and A, \u0100 \u2208 S(d)\n\uf8f1\n\uf8f2 G(p + p\u0304, A + \u0100) \u2264 G(p, A) + G(p\u0304, \u0100),\nG(\u03bbp, \u03bbA) = \u03bbG(p, A), \u2200\u03bb \u2265 0,\n(3)\n\uf8f3\nG(p, A) \u2265 G(p, \u0100), if A \u2265 \u0100.\n\nG is also a continuous function.\nThe following property is classic. One can also check it by using Lemma 2.4.\n\nProposition 4.1 Let G : Rd \u00d7 S(d) 7\u2192 R be a sublinear function monotonic\nin A \u2208 S(d) in the sense of (3) and continuous in (0, 0). Then there exists a\nbounded subset \u0398 \u2208 Rd \u00d7 Rd\u00d7d such that\nG(p, A) =\n\n1\nsup [ tr[AQQT ] + hp, qi],\n(q,Q)\u2208\u0398 2\n\n\u2200(p, A) \u2208 Rd \u00d7 S(d).\n\nThe classical normal distribution can be characterized through the notion\nof stable distributions introduced by P. L\u00e9vy [6] and [7]. The distribution of\n\n7\n\n\fa d-dimensional random vector X in a sublinear expectation space (\u03a9, H, \u00ca) is\ncalled stable if for each a, b \u2208 Rd , there exists c \u2208Rd and d \u2208 R such that\nd\n\nha, Xi + b, X\u0304 = hc, Xi + d,\nwhere X\u0304 is an independent copy of X.\nThe following G-normal distribution plays the same role as normal distributions in the classical probability theory:\nProposition 4.2 Let G : Rd \u00d7 S(d) 7\u2192 R be a given sublinear function monotonic in A \u2208 S(d) the sense of (3) and continuous in (0, 0). Then there exists\na pair of d-dimensional random vectors (X, Y ) in some sublinear expectation\nspace (\u03a9, H, \u00ca) satisfying (2) and the following condition:\np\nd\n(aX + bX\u0304, a2 Y + b2 \u0232 ) = ( a2 + b2 X, (a2 + b2 )Y ), \u2200a, b \u2265 0,\n(4)\nwhere (X\u0304, \u0232 ) is an independent copy of (X, Y ). The distribution of (X, Y ) is\nuniquely determine by G.\nExample 4.3 For the sublinear function \u1e20 : Rd 7\u2192 R defined by \u1e20(p) :=\nG(p, 0), p \u2208 Rd , we can concretely construct a d-dimensional random vector\nY in some sublinear expectation space (\u03a9, H, \u00ca) satisfying\n\u1e20(p) := \u00ca[hp, Y i],\n\np \u2208 Rd\n\n(5)\n\nand the following condition:\nd\n\na2 Y + b2 \u0232 = (a2 + b2 )Y,\n\n\u2200a, b \u2208 R,\n\n(6)\n\nwhere Y is an independent copy of Y . In fact we can take \u03a9 = Rd , H =\nCl.Lip (Rd ) and Y (\u03c9) = \u03c9. To define the corresponding sublinear expectation \u00ca,\nwe apply Proposition 4.1 to find a subset \u0398\u0304 \u2208 Rd such that\n\u1e20(p) = sup hp, qi ,\nq\u2208\u0398\u0304\n\np \u2208 Rd .\n\n(7)\n\nThen for each \u03be \u2208 H of the form \u03be(\u03c9) = \u03c6(\u03c9), \u03c6 \u2208 Cl.Lip (Rd ). \u03c9 \u2208 Rd we set\n\u00ca[\u03be] = sup \u03c6(\u03c9).\n\n(8)\n\n\u03c9\u2208\u0398\u0304\n\nIt is easy to check that the distribution of Y satisfies (5) and (6). It is the\nso-called worst case distribution with respect to the subset of mean uncertainty\n\u0398\u0304. We denote this distribution by U(\u0398\u0304).\nExample 4.4 For the sublinear and monotone function \u011c : S(d) 7\u2192 R defined\nby \u011c(A) := G(0, A), A \u2208 S(d) the d-dimensional random vector X in Proposition 4.2 satisfies\n1\n(9)\n\u011c(A) := \u00ca[hAX, Xi], p \u2208 Rd\n2\n8\n\n\fand the following condition:\nd\n\naX + bX\u0304 =\n\np\na2 + b2 X,\n\n\u2200a, b \u2208 R,\n\n(10)\n\nwhere X\u0304 is an independent copy\nXi\n\u221a\n\u221a of X. In particular, for each components\nof X and X\u0304i of X\u0304, we have 2\u00ca[Xi ] = \u00ca[Xi + X\u0304i ] = 2\u00ca[Xi ] and 2\u00ca[\u2212Xi ] =\n\u00ca[\u2212Xi \u2212 X\u0304i ] = 2\u00ca[\u2212Xi ] it follows that X has no mean uncertainty:\n\u00ca[Xi ] = \u00ca[\u2212Xi ] = 0, i = 1, * * * , d.\nOn the other hand, by Proposition 4.1 we can find a bounded subset \u0398\u0302 \u2208 S+ (d)\nsuch that\n1\n1\n\u00ca[hAX, Xi] = \u011c(A) = sup tr[AQ], A \u2208 S(d).\n(11)\n2\n2 Q\u2208\u0398\u0304\nIf \u0398\u0302 is a singleton \u0398\u0302 = {Q}, then X is a classical zero-mean normal distributed\nwith covariance Q. In general \u0398\u0302 characterizes the covariance uncertainty of X.\nDefinition 4.5 (G-distribution) The pair of d-dimensional random vectors\n(X, Y ) in the above proposition is called G-distributed. X is said to be \u011c-normal\nd\n\ndistributed. We denote the distribution of X by X = N (0, \u0398\u0302).\nProposition 4.8 and Corollary 4.9 show that a G-distribution is a uniquely\ndefined sublinear distribution on (R2d , Cl.Lip (R2d )). We will show that a pair\nof G-distributed random vectors is characterized, or generated, by the following\nparabolic PDE defined on [0, \u221e) \u00d7 Rd \u00d7 Rd :\n\u2202t u \u2212 G(Dy u, Dx2 u) = 0,\n\n(12)\n\nwith Cauchy condition u|t=0 = \u03c6, where Dy = (\u2202yi )di=1 , Dx2 = (\u2202x2i ,xj )di,j=1 .\n(12) is called the G-heat equation.\nRemark 4.6 We will use the notion of viscosity solutions to the generating\nheat equation (12). This notion was introduced by Crandall and Lions. For the\nexistence and uniqueness of solutions and related very rich references we refer to\nCrandall, Ishii and Lions [2] (see Appendix for the uniqueness). We note that,\nin the situation where \u03c3 2 > 0, the viscosity solution (12) becomes a classical\n\u03b1\nC 1+ 2 ,2+\u03b1 -solution (see [5] and the recent works of [1] and [18]). Readers can\nunderstand (12) in the classical meaning.\nDefinition 4.7 A real-valued continuous function u \u2208 C([0, T ] \u00d7 Rd ) is called\na viscosity subsolution (respectively, supersolution) of (12) if, for each function\n\u03c8 \u2208 Cb3 ((0, \u221e) \u00d7 Rd \u00d7 Rd ) and for each minimum (respectively, maximum) point\n(t, x, y) \u2208 (0, \u221e) \u00d7 Rd \u00d7 Rd of \u03c8 \u2212 u, we have\n\u2202t \u03c8 \u2212 G(Dy \u03c8, Dx2 \u03c8) \u2264 0 (respectively, \u2265 0).\nu is called a viscosity solution of (12) if it is both super and subsolution.\n9\n\n\fProposition 4.8 Let (X, Y ) be G-distributed. For each \u03c6 \u2208 Cl.Lip (Rd \u00d7 Rd )\nwe define a function\n\u221a\nu(t, x, y) := \u00ca[\u03c6((x + tX, y + tY )], (t, x) \u2208 [0, \u221e) \u00d7 R.\nThen we have\nu(t + s, x, y) = \u00ca[u(t, x +\n\n\u221a\nsX, y + sY )], s \u2265 0.\n\n(13)\n\nWe also have the estimates: For each T > 0 there exist constants C, k > 0 such\nthat, for all t, s \u2208 [0, T ] and x, y \u2208 R,\n|u(t, x, y) \u2212 u(t, x\u0304, \u0233)| \u2264 C(1 + |x|k + |y|k + |x\u0304|k + |\u0233|k )|x \u2212 y|\n\n(14)\n\n|u(t, x, y) \u2212 u(t + s, x + y)| \u2264 C(1 + |x|k + |y|k )(s + |s|1/2 ).\n\n(15)\n\nand\nMoreover, u is the unique viscosity solution, continuous in the sense of (14) and\n(15), of the generating PDE (12).\nProof. Since\n\u221a\n\u221a\ntX, y + tY )] \u2212 \u00ca[\u03c6(x\u0304 + tX, \u0233 + tY )]\n\u221a\n\u221a\n\u2264 \u00ca[\u03c6(x + tX, y + tY ) \u2212 \u03c6(x\u0304 + tX, \u0233 + tY )]\n\nu(t, x, y) \u2212 u(t, x\u0304, \u0233) = \u00ca[\u03c6(x +\n\n\u2264 \u00ca[C1 (1 + |X|k + |Y |k + |x|k + |y|k + |x\u0304|k + |\u0233|k )]\n\u00d7 (|x \u2212 x\u0304| + |y \u2212 \u0233|)\n\n\u2264 C(1 + |x|k + |y|k + |x\u0304|k + |\u0233|k )(|x \u2212 x\u0304| + |y \u2212 \u0233|).\n\nWe then have (14). Let (X\u0304, \u0232 ) be an independent copy of (X, Y ). Since (X, Y )\nis G-distributed, then\n\u221a\nu(t + s, x, y) = \u00ca[\u03c6(x + t + sX, y + (t + s)Y )]\n\u221a\n\u221a\n= \u00ca[\u03c6(x + sX + tX\u0304, y + sY + t\u0232 )]\n\u221a\n\u221a\n= \u00ca[\u00ca[\u03c6(x + se\nx + tX\u0304, y + se\ny + t\u0232 )](ex,ey)=(X,Y ) ]\n\u221a\n= \u00ca[u(t, x + sX, y + sY )].\nWe thus obtain (13). From this and (14) it follows that\n\u221a\n\nsX, y + sY ) \u2212 u(t, x)]\n\u221a\n\u2264 \u00ca[C1 (1 + |x| + |y| + |X| + |Y | )( s|X| + s|Y |)].\n\nu(t + s, x, y) \u2212 u(t, x, y) = \u00ca[u(t, x +\nk\n\nk\n\nk\n\nk\n\nThus we obtain (15). Now, for a fixed (t, x, y) \u2208 (0, \u221e) \u00d7 Rd \u00d7 Rd , let \u03c8 \u2208\nCb1,3 ([0, \u221e) \u00d7 Rd \u00d7 Rd ) be such that \u03c8 \u2265 u and \u03c8(t, x, y) = u(t, x, y). By (13)\n\n10\n\n\fand Taylor's expansion it follows that, for \u03b4 \u2208 (0, t),\n\u221a\n0 \u2264 \u00ca[\u03c8(t \u2212 \u03b4, x + \u03b4X, y + \u03b4Y ) \u2212 \u03c8(t, x, y)]\n\n\u2264 C\u0304(\u03b4 3/2 + \u03b4 2 ) \u2212 \u2202t \u03c8(t, x, y)\u03b4\n\u221a\n1\n+ \u00ca[hDx \u03c8(t, x, y), Xi \u03b4 + hDy \u03c8(t, x, y), Y i \u03b4 +\nDx2 \u03c8(t, x, y)X, X \u03b4]\n2\n1\nDx2 \u03c8(t, x, y)X, X ]\u03b4 + C\u0304(\u03b4 3/2 + \u03b4 2 )\n= \u2212\u2202t \u03c8(t, x, y)\u03b4 + \u00ca[hDy \u03c8(t, x, y), Y i +\n2\n= \u2212\u2202t \u03c8(t, x, y)\u03b4 + \u03b4G(Dy \u03c8, Dx2 \u03c8)(t, x, y) + C\u0304(\u03b4 3/2 + \u03b4 2 ).\n\nFrom which it is easy to check that\n[\u2202t \u03c8 \u2212 G(Dy \u03c8, Dx2 \u03c8)](t, x, y) \u2264 0.\nThus u is a viscosity supersolution of (12). Similarly we can prove that u is a\nviscosity subsolution of (12).\nCorollary 4.9 If both (X, Y ) and (X\u0304, \u0232 ) are G-distributed with the same G,\ni.e.,\n1\n1\nG(p, A) := \u00ca[ hAX, Xi+hp, Y i] = \u00ca[ AX\u0304, X\u0304 + p, \u0232 ], \u2200(p, A) \u2208 S(d)\u00d7Rd .\n2\n2\nd\n\nd\n\nthen (X, Y ) = (X\u0304, \u0232 ). In particular, X = \u2212X.\nProof. For each \u03c6 \u2208 Cl.Lip (Rd \u00d7 Rd ) we set\n\u221a\nu(t, x, y) := \u00ca[\u03c6(x + tX, y + tY )],\n\u221a\n\u016b(t, x, y) := \u00ca[\u03c6(x + tX\u0304, y + t\u0232 )], (t, x) \u2208 [0, \u221e) \u00d7 R.\nBy the above Proposition, both u and \u016b are viscosity solutions of the G-heat\nequation (12) with Cauchy condition u|t=0 = \u016b|t=0 = \u03c6. It follows from the\nuniqueness of the viscosity solution that u \u2261 \u016b. In particular\n\u00ca[\u03c6(X, Y )] = \u00ca[\u03c6(X\u0304, \u0232 )].\nd\n\nThus (X, Y ) = (X\u0304, \u0232 ).\nCorollary 4.10 Let (X, Y ) be G-distributed. For each \u03c8 \u2208 Cl.Lip (Rd ) we define\na function\n\u221a\nv(t, x) := \u00ca[\u03c8((x + tX + tY )], (t, x) \u2208 [0, \u221e) \u00d7 Rd .\nThen v is the unique viscosity solution of the following parabolic PDE\n\u2202t v \u2212 G(Dx v, Dx2 v) = 0,\n\nv|t=0 = \u03c8.\n\n(16)\n\nMoreover we have v(t, x + y) \u2261 u(t, x, y), where u is the solution of the PDE\n(12) with initial condition u(t, x, y)|t=0 = \u03c8(x + y).\n11\n\n\f4.1\n\nProof of Proposition 4.2\n\nWe now proceed to prove Proposition 4.2. Let u = u\u03c6 be the unique viscosity soe = R2d , H\ne=\nlution of the G-heat equation (12) with u\u03c6 |t=0 = \u03c6, Then we take \u03a9\n2d\n2d\ne\nCl.Lip (R ), \u03c9\ne = (x, y) \u2208 R . The corresponding sublinear expectation E[*] is\ndefined by, for each \u03be \u2208 H of the form \u03be(\u03c9) = (\u03c6(x, y))(x,y)\u2208R2d \u2208 Cl.Lip (R2d ),\ne = u\u03c6 (1, 0). The monotonicity and sub-linearity of u\u03c6 with respect to \u03c6 are\nE[\u03be]\nknown in the theory of viscosity solution. For reader's convenience we provide\na new and simple proof in the Appendix (see Corollary 6.4 and Corollary 6.5).\ne is easy to be checked.\nThe positive homogeneity of E[*]\ne Ye )(\u03c9) = (x, y).\nWe now consider a pairs of d-dimensional random vectors (X,\nWe have\ne Ye )] = u\u03c6 (1, 0), \u2200\u03c6 \u2208 Cl.Lip (R2d ).\n\u00ca[\u03c6(X,\nIn particular, just set \u03c60 (x, y) =\n\n1\n2\n\nhAx, xi + hp, yi, we can check that\n\nu\u03c60 (t, x, y) := G(p, A)t +\n\n1\nhAx, xi + hp, yi .\n2\n\nWe thus have\nD\nE D\nE\ne 1 AX,\ne X\ne + p, Ye ] = u\u03c60 (t, 0)|t=1 = G(p, A), (p, A) \u2208 Rd \u00d7 S(n).\nE[\n2\n\ne Ye ) satisfies condition (4), we follow\nTo prove that the distribution of (X,\nDefinition 3.7 to construct a product space of sublinear expectation\ne \u00d7 \u03a9,\ne H\ne \u00d7 H,\ne E\ne \u00d7 E)\ne\n(\u03a9, H, \u00ca) = (\u03a9\n\nand introduce two pair of random vectors\n\n(X, Y )(\u03c91 , \u03c92 ) = \u03c91 , (X\u0304, \u0232 )(\u03c91 , \u03c92 ) = \u03c92 ,\n\ne \u00d7 \u03a9.\ne\n(\u03c91 , \u03c92 ) \u2208 \u03a9\n\nd\ne Ye ) and (X\u0304, \u0232 ) is an independent copy of\nBy Proposition 3.8 both (X, Y ) = (X,\n2d\n2d\n(X, Y ). For each \u03c6 \u2208 Cl.Lip (R ) and for each fixed\n\u221a \u03bb > 0, (x\u0304, \u0233) \u2208 R , since\n\u03c6\nthe function v defined by v(t, x, y) := u (\u03bbt, x\u0304 + \u03bbx, \u0233 + \u03bby) solves exactly the\nsame equation (12) but with Cauchy condition\n\u221a\nv|t=0 = \u03c6(x\u0304 + \u03bb \u00d7 *, \u0233 + \u03bb \u00d7 *).\n\nThus\n\u00ca[\u03c6(x\u0304 +\n\n\u221a\n\u03bbX, \u0233 + \u03bbY )] = v(t, x\u0304, \u0233)|t=1\n= u\u03c6(\n\n\u221a\n\u03bb\u00d7*,\u03bb\u00d7*)\n\n(t, x\u0304, \u0233)|t=1 = u\u03c6 (\u03bb, x\u0304, \u0233).\n\nBy the definition of \u00ca, for each t > 0 and s > 0,\n\u221a\n\u221a\n\u221a\n\u221a\ne E[\u03c6(\ne\n\u00ca[\u03c6( tX + sX\u0304, tY + s\u0232 )] = E[\ntx + sX\u0304, ty + s\u0232 )](x,y)=(X,Y ) ]\n\u03c6\n\n= uu\n\n(s,*,*)\n\n(t, 0, 0) = u\u03c6 (t + s, 0, 0)\n\n\u221a\n= \u00ca[\u03c6( t + sX, (t + s)Y )].\n12\n\n\f\u221a\n\u221a\nd \u221a\nNamely ( tX + sX\u0304, tY + s\u0232 ) = ( t + sX, (t + s)Y ). Thus the distribution\nof (X, Y ) satisfies condition (4).\ne : Cl.Lip (R2d ) 7\u2192 R forms a\nIt remains to check that the functional E[*]\nsublinear expectation, i.e., (a)-(d) of Definition 2.2 are satisfied. Indeed, (a)\nis simply the consequence of comparison theorem, or the maximum principle of\nviscosity solution (see [CIL], the prove of this comparison theorem as well as the\nsub-additivity (c) are given in the Appendix of [P6]). It is also easy to check\nthat, when \u03c6 \u2261 c, the unique solution of (12) is also u \u2261 c; hence (b) holds true.\n(d) also holds since u\u03bb\u03c6 = \u03bbu\u03c6 , \u03bb \u2265 0. The proof is complete.\n\n5\n\nCentral Limit Theorem\n\u221e\n\nTheorem 5.1 (Central Limit Theorem) Let a sequence {(Xi , Yi )}i=1 of Rd \u00d7\nd\n\nRd -valued random variables in (H, \u00ca). We assumed that (Xi+1 , Yi+1 ) = (Xi , Yi )\nand (Xi+1 , Yi+1 ) is independent to {(X1 , Y1 ), * * * , (Xi , Yi )} for each i = 1, 2, * * * .\nWe assume furthermore that,\n\u00ca[X1 ] = \u00ca[\u2212X1 ] = 0,\nThen the sequence {S\u0304n }\u221e\nn=1 defined by\nS\u0304n :=\n\nn\nX\nXi\nYi\n(\u221a + )\nn\nn\ni=1\n\nconverges in law to \u03be + \u03b6:\ne\nlim \u00ca[\u03c6(S\u0304n )] = E[\u03c6(\u03be\n+ \u03b6)],\n\nn\u2192\u221e\n\n(17)\n\nfor all functions \u03c6 \u2208 C(Rd ) satisfying a polynomial growth condition, where\n(\u03be, \u03b6) is a pair of G-distributed random vectors and where the sublinear function\nG : S(d) \u00d7 Rd 7\u2192 R is defined by\nG(p, A) := \u00ca[hp, Y1 i +\nPn\n\n1\nhAX1 , X1 i], A \u2208 S(d), p \u2208 Rd .\n2\n\nXi\n\u221a\nn\n\nconverges in law to N (0, \u0398\u0302), where the subset\nP\n\u0398\u0302 \u2282 S+ (d) is defined in (11) for \u011c(A) = G(0, A), A \u2208 S(d). The sum ni=1 Yni\nconverges in law to U(\u0398\u0304), where the subset \u0398\u0304 \u2282 Rd is defined in (7) for \u1e20(p) =\nG(p, 0), p \u2208 Rd . If we take in particular \u03c6(y) = d\u0398\u0304 (y) = inf{|x \u2212 y| : x \u2208 \u0398\u0304},\nthen by (8) we have the following generalized law of large number:\nCorollary 5.2 The sum\n\ni=1\n\nn\nX\nYi\nlim \u00ca[d\u0398\u0304 (\n)] = sup d\u0398\u0304 (\u03b8) = 0.\nn\u2192\u221e\nn\n\u03b8\u2208\u0398\u0304\ni=1\n\n13\n\n(18)\n\n\fRemark 5.3 If Yi has no mean-uncertainty, or in other words, \u0398\u0304 is a singleton:\n\u0398\u0304 = {\u03b8\u0304} then (18) becomes\nlim \u00ca[|\n\nn\u2192\u221e\n\nn\nX\nYi\ni=1\n\nn\n\n\u2212 \u03b8\u0304|] = 0.\n\nTo our knowledge, the law of large numbers with non-additive probability measures have been investigated with a quite different framework and approach from\nours (see [9], [10]).\nTo prove this theorem we first give\nLemma 5.4 We assume the same condition as Theorem 5.1. We assume\nfurthermore that there exists \u03b2 > 0 such that, for each A, \u0100 \u2208 S(d) with A \u2265 \u0100,\nwe have\n(19)\n\u00ca[hAX1 , X1 i] \u2212 \u00ca[ \u0100X1 , X1 ] \u2265 \u03b2tr[A \u2212 \u0100].\nThen (17) holds.\nProof. We first prove (17) for \u03c6 \u2208 Cb.Lip (Rd ). For a small but fixed h > 0, let\nV be the unique viscosity solution of\n\u2202t V + G(DV, D2 V ) = 0, (t, x) \u2208 [0, 1 + h] \u00d7 Rd , V |t=1+h = \u03c6.\n\n(20)\n\nSince (\u03be, \u03b6) is G-distributed we have\ne\nV (h, 0) = E[\u03c6(\u03be\n+ \u03b6)], V (1 + h, x) = \u03c6(x)\n\n(21)\n\nSince (20) is a uniformly parabolic PDE and G is a convex function thus, by\nthe interior regularity of V (see Krylov [5], Example 6.1.8 and Theorem 6.2.3),\nwe have\nkV kC 1+\u03b1/2,2+\u03b1 ([0,1]\u00d7Rd ) < \u221e, for some \u03b1 \u2208 (0, 1).\nWe set \u03b4 =\n\n1\nn\n\nand S0 = 0. Then\n\nV (1, S\u0304n ) \u2212 V (0, 0) =\n=\n=\n\nn\u22121\nX\ni=0\n\n{V ((i + 1)\u03b4, S\u0304i+1 ) \u2212 V (i\u03b4, S\u0304i )}\n\nn\u22121\nX\n\n{[V ((i + 1)\u03b4, S\u0304i+1 ) \u2212 V (i\u03b4, S\u0304i+1 )] + [V (i\u03b4, S\u0304i+1 ) \u2212 V (i\u03b4, S\u0304i )]}\n\ni=0\n\n\b i\nI\u03b4 + J\u03b4i\n\ni=0\nn\u22121\nX\n\nwith, by Taylor's expansion,\nJ\u03b4i = \u2202t V (i\u03b4, S\u0304i )\u03b4+\n\nE\nD\n\u221a\n1\nD2 V (i\u03b4, S\u0304i )Xi+1 , Xi+1 \u03b4+ DV (i\u03b4, S\u0304i ), Xi+1 \u03b4 + Yi+1 \u03b4\n2\n14\n\n\fI\u03b4i\n\nZ\n\n=\n\n1\n\n[\u2202t V ((i + \u03b2)\u03b4, S\u0304i+1 ) \u2212 \u2202t V (i\u03b4, S\u0304i+1 )]d\u03b2\u03b4 + [\u2202t V (i\u03b4, S\u0304i+1 ) \u2212 \u2202t V (i\u03b4, S\u0304i )]\u03b4\n\n0\n\n1\n1\nD2 V (i\u03b4, S\u0304i )Xi+1 , Yi+1 \u03b4 3/2 +\nD2 V (i\u03b4, S\u0304i )Yi+1 , Yi+1 \u03b4\n2\n2\nZ 1Z 1D\nE\n\u221a\n\u221a\n+\n\u0398i\u03b2\u03b3 (Xi+1 \u03b4 + Yi+1 \u03b4), Xi+1 \u03b4 + Yi+1 \u03b4 \u03b3d\u03b2d\u03b3\n\n+\n\n0\n\n0\n\nwith\n\n\u221a\n\u0398i\u03b2\u03b3 = D2 V (i\u03b4, S\u0304i + \u03b3\u03b2(Xi+1 \u03b4 + Yi+1 \u03b4) \u2212 D2 V (i\u03b4, S\u0304i ).\n\nThus\n\u00ca[\n\nn\u22121\nX\ni=0\n\nJ\u03b4i ] \u2212 \u00ca[\u2212\n\nn\u22121\nX\ni=0\n\nI\u03b4i ] \u2264 \u00ca[V (1, S\u0304n )] \u2212 V (0, 0) \u2264 \u00ca[\n\nn\u22121\nX\n\nJ\u03b4i ] + \u00ca[\n\nn\u22121\nX\n\nI\u03b4i ]. (22)\n\ni=0\n\ni=0\n\nPn\u22121\n\nWe now prove that \u00ca[ i=0 J\u03b4i ] = 0. For the 3rd term of J\u03b4i we have:\nD\nD\n\u221a E\n\u221a E\n\u00ca[ DV (i\u03b4, S\u0304i ), Xi+1 \u03b4 ] = \u00ca[\u2212 DV (i\u03b4, S\u0304i ), Xi+1 \u03b4 ] = 0.\nFor the second term, we have, from the definition of the function G,\n\u00ca[J\u03b4i ] = \u00ca[\u2202t V (i\u03b4, S\u0304i ) + G(DV (i\u03b4, S\u0304i ), D2 V (i\u03b4, S\u0304i ))]\u03b4.\nWe then combine the above two equalities with \u2202t V + G(DV, D2 V ) = 0 as well\nas the independence of (Xi+1 , Yi+1 ) to {(X1 , Y1 ), * * * , (Xi , Yi )}, it follows that\n\u00ca[\n\nn\u22121\nX\n\nJ\u03b4i ] = \u00ca[\n\ni=0\n\nn\u22122\nX\ni=0\n\nJ\u03b4i ] = * * * = 0.\n\nThus (22) can be rewritten as\n\u2212\u00ca[\u2212\n\nn\u22121\nX\ni=0\n\nI\u03b4i ] \u2264 \u00ca[V (1, S\u0304n )] \u2212 V (0, 0) \u2264 \u00ca[\n\nn\u22121\nX\n\nI\u03b4i ].\n\ni=0\n\nBut since both \u2202t V and D2 V are uniformly \u03b1-h\u00f6lder continuous in x and\nh\u00f6lder continuous in t on [0, 1] \u00d7 R, we then have\n\n\u03b1\n2-\n\n|I\u03b4i | \u2264 C\u03b4 1+\u03b1/2 [1 + |Xi+1 |2+\u03b1 + |Y1 |2+\u03b1 ].\nIt follows that\n\u00ca[|I\u03b4i |] \u2264 C\u03b4 1+\u03b1/2 (1 + \u00ca[|X1 |2+\u03b1 + |Y1 |2+\u03b1 ]).\nThus\n1\n\u2212C( )\u03b1/2 (1 + \u00ca[|X1 |2+\u03b1 + |Y1 |2+\u03b1 ]) \u2264 \u00ca[V (1, S\u0304n )] \u2212 V (0, 0)\nn\n1\n\u2264 C( )\u03b1/2 (1 + \u00ca[|X1 |2+\u03b1 + |Y1 |2+\u03b1 ]).\nn\n15\n\n\fAs n \u2192 \u221e we have\n\nlim \u00ca[V (1, S\u0304n )] = V (0, 0).\n\nn\u2192\u221e\n\n(23)\n\nOn the other hand, for each t, t\u2032 \u2208 [0, 1 + h] and x \u2208 Rd , we have\np\n|V (t, x) \u2212 V (t\u2032 , x)| \u2264 C( |t \u2212 t\u2032 |. + |t \u2212 t\u2032 |).\n,\n\u221a\nThus |V (0, 0) \u2212 V (h, 0)| \u2264 C( h + h) and, by (23),\n\n\u221a\n|\u00ca[V (1, S\u0304n )] \u2212 \u00ca[\u03c6(S\u0304n )]| = |\u00ca[V (1, S\u0304n )] \u2212 \u00ca[V (1 + h, S\u0304n )]| \u2264 C( h + h).\n\nIt follows form (21) and (23) that\n\u221a\ne\nlim sup |\u00ca[\u03c6(S\u0304n )] \u2212 E[\u03c6(\u03be\n+ \u03b6)]| \u2264 2C( h + h).\nn\u2192\u221e\n\nSince h can be arbitrarily small we thus have\n\ne\nlim \u00ca[\u03c6(S\u0304n )] = E[\u03c6(\u03be)].\n\nn\u2192\u221e\n\nWe now give\nProof of Theorem 5.1. For the case when the uniform Elliptic condition 19\ndoes not hold, we first follow an idea of Song [17] to introduce a perturbation\nto prove the above convergence for \u03c6 \u2208 Cb.Lip (Rd ). According to Definition 3.7\nand Proposition 3.8 we can construct a sublinear expectation space (\u03a9\u0304, H\u0304, \u0112)\nand a sequence of three random vectors {(X\u0304i , \u0232i , \u03b7\u0304i )}\u221e\ni=1 such that, for each n =\nd\n\n1, 2, * * * , {(X\u0304i , \u0232i )}ni=1 = {(Xi , Yi )}ni=1 and (X\u0304n+1 , \u0232n+1 , \u03b7\u0304n+1 ) is independent to\n{(X\u0304i , \u0232i , \u03b7\u0304i )}ni=1 and, moreover,\nZ\n|x|2\n1\n\u0112[\u03c8(Xi , Yi , \u03b7i )] = \u221a\n\u00ca[\u03c8(Xi , Yi , x)]e\u2212 2 dx, \u2200\u03c8 \u2208 Cl.Lip (R3\u00d7d ).\n2\u03c0d Rd\n\nWe then use the following perturbation Xi\u03b5 = Xi + \u03b5\u03b7i for a fixed \u03b5 > 0. It is\nseen that the sequence {(X\u0304i\u03b5 , \u0232i )}\u221e\ni=1 satisfies all conditions in the above CLT,\nin particular\nG\u03b5 (p, A) := \u0112[\n\n1\n\u03b52\nAX\u03041\u03b5 , X\u03041\u03b5 + p, \u02321 ] = G(p, A) + tr[A].\n2\n2\n\nThus it is strictly elliptic. We then can apply Lemma 5.4 to\nS\u0304n\u03b5 :=\nand obtain\n\nn\nn\nX\nX\n\u03b7\nYi\nX\u03b5\n\u221ai\n( \u221a i + ) = S\u0304n + \u03b5Jn , Jn =\nn\nn\nn\ni=1\ni=1\n\ne\nlim \u00ca[\u03c6(S\u0304n\u03b5 )] = E[\u03c6(\u03be\n+ \u03b6 + \u03b5\u03b7)].\n\nn\u2192\u221e\n\n16\n\n\fwhere (\u03be, \u03b6) is G-distributes and\nZ\n\u2212|x|2\n1\ne\ne\nE[\u03c8(\u03be + \u03b6, \u03b7)] = \u221a\nE[\u03c8(\u03be\n+ \u03b6, x)]e 2 dx, \u03c8 \u2208 Cl.Lip (R2d ).\n2\u03c0d Rd\nThus (\u03be + \u03b5\u03b7, \u03b6) is G\u03b5 -distributed. But we have\n|\u00ca[\u03c6(S\u0304n )] \u2212 \u00ca[\u03c6(S\u0304n\u03b5 )]| = |\u00ca[\u03c6(S\u0304n )] \u2212 \u00ca[\u03c6(S\u0304n + \u03b5Jn )]|\n\u2264 \u03b5C \u00ca[|Jn |] \u2264 C\u03b5\n\ne\ne\nand similarly |E[\u03c6(\u03be)]\n\u2212 E[\u03c6(\u03be\n+ \u03b5\u03b7)]| \u2264 C\u03b5. Sine \u03b5 can be arbitrarily small, it\nfollows that\ne\nlim \u00ca[\u03c6(S\u0304n )] = E[\u03c6(\u03be\n+ \u03b6)], \u2200\u03c6 \u2208 Cb.Lip (Rd ).\nn\u2192\u221e\n\ne + \u03b6|] < \u221e. We\nOn the other hand, it is easy to check that supn \u00ca[|S\u0304n |] + E[|\u03be\nthen can apply the following lemma to prove that the above convergence holds\nfor the case where \u03c6 in C(Rd ) with a polynomial growth condition. The proof\nis complete.\ne H,\ne E)\ne be two sublinear expectation space and\nLemma 5.5 Let (\u03a9\u0302, \u0124, \u00ca) and (\u03a9,\ne\nlet \u03b6 \u2208 \u0124 and \u03b6n \u2208 H, n = 1, 2, * * * , be given. We assume that, for a given p \u2265 1\ne |p ] \u2264 C. If the convergence limn\u2192\u221e \u00ca[\u03c6(Yn )] =\nwe have supn \u00ca[|Yn |p ] + E[|Y\ne\nE[\u03c6(Y )] holds for each \u03c6 \u2208 Cb.Lip (Rd ), then it also holds for all function \u03c6 \u2208\nC(Rd ) with growth condition |\u03c6(x)| \u2264 C(1 + |x|p\u22121 ).\nProof. We first prove that the above convergence holds for \u03c6 \u2208 Cb (Rd ) with\na compact support. In this case, for each \u03b5 > 0, we can find a \u03c6\u0304 \u2208 Cb.Lip (Rd )\nsuch that supx\u2208Rd |\u03c6(x) \u2212 \u03c6\u0304(x)| \u2264 2\u03b5 . We have\ne\ne\ne \u03c6\u0304(Y )]|\n|\u00ca[\u03c6(Yn )] \u2212 E[\u03c6(Y\n)]| \u2264 |\u00ca[\u03c6(Yn )] \u2212 \u00ca[\u03c6\u0304(Yn )]| + |E[\u03c6(Y\n)] \u2212 E[\ne \u03c6\u0304(Yn )]| \u2264 \u03b5 + |\u00ca[\u03c6\u0304(Yn )] \u2212 E[\ne \u03c6\u0304(Y )]|.\n+ |\u00ca[\u03c6\u0304(Yn )] \u2212 E[\n\ne\nThus lim supn\u2192\u221e |\u00ca[\u03c6(Yn )] \u2212 E[\u03c6(Y\n)]| \u2264 \u03b5. The convergence must hold since \u03b5\ncan be arbitrarily small.\nNow let \u03c6 be an arbitrary Cb (Rn )-function. For each N > 0 we can find\n\u03c61 , \u03c62 \u2208 Cb (Rd ) such that \u03c6 = \u03c61 + \u03c62 where \u03c61 has a compact support and\n\u03c62 (x) = 0 for |x| \u2264 N , and |\u03c62 (x)| \u2264 |\u03c6(x)| for all x. It is clear that\n|\u03c62 (x)| \u2264\n\nC\u0304(1 + |x|p )\n, \u2200x, where C\u0304 = sup |\u03c6(x)|.\nN\nx\u2208Rd\n\nThus\ne\ne 1 (Y ) + \u03c62 (Y )]|\n|\u00ca[\u03c6(Yn )] \u2212 E[\u03c6(Y\n)]| = |\u00ca[\u03c61 (Yn ) + \u03c62 (Yn )] \u2212 E[\u03c6\ne 1 (Y )]| + \u00ca[|\u03c62 (Yn )|] + E[|\u03c6\ne 2 (Y )|]\n\u2264 |\u00ca[\u03c61 (Yn )] \u2212 E[\u03c6\ne 1 (Y )]| + C\u0304 (\u00ca[|Yn | + E[|Y\ne |])\n\u2264 |\u00ca[\u03c61 (Yn )] \u2212 E[\u03c6\nN\ne 1 (Y )]| + C\u0304C .\n\u2264 |\u00ca[\u03c61 (Yn )] \u2212 E[\u03c6\nN\n17\n\n\fe\nWe thus have lim supn\u2192\u221e |\u00ca[\u03c6(Yn )]\u2212E[\u03c6(Y\n)]| \u2264\ne\nlarge thus \u00ca[\u03c6(Yn )] must converge to E[\u03c6(Y )].\n\n6\n\nC\u0304C\nN .\n\nSince N can be arbitrarily\n\nAppendix: some basic results of viscosity solutions\n\nWe will use the following well-known result in viscosity solution theory (see\nTheorem 8.3 of Crandall Ishii and Lions [2]).\nTheorem 6.1 Let ui \u2208USC((0, T ) \u00d7 Qi ) for i = 1, * * * , k where Qi is a locally\ncompact subset of RNi . Let \u03c6 be defined on an open neighborhood of (0, T ) \u00d7\nQ1 \u00d7 * * * \u00d7 Qk and such that (t, x1 , * * * , xk ) is once continuously differentiable in\nt and twice continuously differentiable in (x1 , * * * , xk ) \u2208 Q1 \u00d7 * * * \u00d7 Qk . Suppose\nthat t\u0302 \u2208 (0, T ), x\u0302i \u2208 Qi for i = 1, * * * , k and\nw(t, x1 , * * * , xk ) := u1 (t, x1 ) + * * * + uk (t, xk ) \u2212 \u03c6(t, x1 , * * * , xk )\n\u2264 w(t\u0302, x\u03021 , * * * , x\u0302k )\n\nfor t \u2208 (0, T ) and xi \u2208 Qi . Assume, moreover that there is an r > 0 such that\nfor every M > 0 there is a C such that for i = 1, * * * , k,\nbi \u2264 C whenever (bi , qi , Xi ) \u2208 P 2,+ ui (t, xi ),\n|xi \u2212 x\u0302i | + |t \u2212 t\u0302| \u2264 r and |ui (t, xi )| + |qi | + kXi k \u2264 M.\n\n(24)\n\nThen for each \u03b5 > 0, there are Xi \u2208 S(Ni ) such that\n2,+\n(i) (bi , Dxi \u03c6(t\u0302, x\u03021 , * * * , x\u0302k ), Xi ) \u2208 P ui (t\u0302, x\u0302i ), i = 1, * * * , k;\n(ii)\n\uf8ee\n\uf8f9\nX1 * * *\n0\n1\n\uf8ef\n.. \uf8fa \u2264 A + \u03b5A2\n..\n\u2212( + kAk) \u2264 \uf8f0 ...\n.\n. \uf8fb\n\u03b5\n0 * * * Xk\n(iii) b1 + * * * + bk = \u2202t \u03c6(t\u0302, x\u03021 , * * * , x\u0302k )\nwhere A = D2 \u03c6(x\u0302) \u2208 S(N1 + * * * + Nk ).\nObserve that the above conditions (24) will be guaranteed by having ui be\nsubsolutions of parabolic equations given in the following two theorems, which\nis an improved version of the one in the Appendix of [16].\nTheorem 6.2 (Domination Theorem) ui \u2208USC([0, T ] \u00d7 RN ) be subsolutions of\n\u2202t u \u2212 Gi (t, x, u, Du, D2 u) = 0,\n\ni = 1, * * * , k,\n\non (0, T )\u00d7RN such that, for given constants \u03b2i > 0, i = 1, * * * , k,\n0, uniformly as |x| \u2192 \u221e. We assume that\n(i) The functions\nGi : [0, T ] \u00d7 RN \u00d7 RN \u00d7 S(N ) 7\u2192 R,\n18\n\n(25)\n\u0011+\n\u2192\nu\n(t,\nx)\ni\ni=1\n\n\u0010P\nk\n\ni = 1, * * * , k,\n\n\fare continuous in the following sense: for each t \u2208 [0, T ), v1 , v2 \u2208 R, x, y, p,\nq \u2208 RN and Y \u2208 S(N ),\n[Gi (t, x, v, p, X) \u2212 Gi (t, y, v, p, X)]\u2212\n\n\u2264 \u03c9\u0304(1 + (T \u2212 t)\u22121 + |x| + |y| + |v|)\u03c9(|x \u2212 y| + |p| * |x \u2212 y|)\n\nwhere \u03c9, \u03c9\u0304 : R+ 7\u2192 R+ are given continuous functions with \u03c9(0) = 0.\n(ii) Given constants \u03b2i > 0, i = 1, * * * , k, the following domination condition\nholds for Gi :\nk\nX\n\u03b2i Gi (t, x, vi , pi , Xi ) \u2264 0,\n(26)\ni=1\n\nfor each (t, x) \u2208 (0, T ) \u00d7 RN and (vi , pi , Xi ) \u2208 R \u00d7 RN \u00d7 S(N ) such that\nPk\nPk\nPk\ni=1 \u03b2i vi \u2265 0,\ni=1 \u03b2i pi = 0,\ni=1 \u03b2i Xi \u2264 0.\nThen\na\nsimilar\ndomination\nalso\nholds for the solutions: If the sum of initial\nPk\nvalues\n\u03b2\nu\n(0,\n*)\nis\na\nnon-positive\nand continuous function on RN , then\ni\ni\ni=1\nPk\ni=1 \u03b2i ui (t, *) \u2264 0, for all t > 0.\n\nProof. We first observe that for \u03b4\u0304 > 0 and for each 1 \u2264 i \u2264 k, the functions\ndefined by \u0169i := ui \u2212 \u03b4\u0304/(T \u2212 t) is a subsolution of:\n\u2202t \u0169i \u2212 G\u0303i (t, x, \u0169i + \u03b4\u0304/(T \u2212 t), D\u0169i , D2 \u0169i ) \u2264 \u2212\n\n\u03b4\u0304\n(T \u2212 t)2\n\nwhere G\u0303i (t, x, v, p, X) := Gi (t, x, v + \u03b4\u0304/(T \u2212 t), p, X). It is easy to check that\nPk\nthe functions G\u0303i satisfy the same conditions as Gi . Since i=1 \u03b2i ui \u2264 0 follows\nPk\nfrom i=2 \u03b2i \u0169i \u2264 0 in the limit \u03b4\u0304 \u2193 0, it suffices to prove the theorem under the\nadditional assumptions\n\u2202t ui \u2212 Gi (t, x, ui , Dui , D2 ui ) \u2264 \u2212c, c := \u03b4\u0304/T 2 ,\nand limt\u2192T ui (t, x) = \u2212\u221e,\nuniformly in [0, T ) \u00d7 RN .\n\n(27)\n\nTo prove the theorem, we assume to the contrary that\nsup\n\nk\nX\n\n\u03b2i ui (t, x) = m0 > 0\n\n(s,x)\u2208[0,T )\u00d7RN i=1\n\nWe will apply Theorem 6.1 for x = (x1 , * * * , xk ) \u2208 Rk\u00d7N and\nk\nX\n\nk\u22121\n\u03b1X\n\u03b2i ui (t, xi ), \u03c6(x) = \u03c6\u03b1 (x) :=\n|xi+1 \u2212 xi |2 .\nw(t, x) :=\n2\ni=1\ni=1\n\nFor each large \u03b1 > 0 the maximum of w \u2212 \u03c6\u03b1 achieved at some (t\u03b1 , x\u03b1 ) inside\na compact subset of [0, T ) \u00d7 Rk\u00d7N . Indeed, since\nM\u03b1 =\n\nk\nX\ni=1\n\n\u03b1\n\u03b2i ui (t\u03b1 , x\u03b1\ni ) \u2212 \u03c6\u03b1 (x ) \u2265 m0 ,\n\n19\n\n\fthus t\u03b1 must be inside an interval [0, T0 ], T0 < T and x\u03b1 must be inside a\ncompact set {x \u2208 \u00d7Rk\u00d7N : supt\u2208[0,T0 ] w(t, x) \u2265 m20 }. We can check that (see\n[2] Lemma 3.1)\n\uf8f1\n\u03b1\n\uf8f4\n\uf8f4 (i) lim\u03b1\u2192\u221e \u03c6\u03b1 (x ) = 0.\n\uf8f2\n\u03b1\n\u03b1\n(ii) lim\u03b1\u2192\u221e M\u03b1 = lim\u03b1\u2192\u221e \u03b21 u1 (t\u03b1 , x\u03b1\n1 ) + * * * + \u03b2k uk (t , xk ))\n(28)\n= sup(t,x)\u2208[0,T )\u00d7RN [\u03b21 u1 (t, x) + * * * + \u03b2k uk (t, x)]\n\uf8f4\n\uf8f4\n\uf8f3\n= [\u03b21 u1 (t\u0302, x\u0302) + * * * + \u03b2k uk (t\u0302, x\u0302)] = m0 .\n\nwhere (t\u0302, x\u0302) is a limit point of (t\u03b1 , x\u03b1\n1 ). Since ui \u2208 USC, for sufficiently large \u03b1,\nwe have\nm0\n\u03b1\n\u03b1\n\u03b21 u1 (t\u03b1 , x\u03b1\n.\n1 ) + * * * + \u03b2k uk (t , xk ) \u2265\n2\nPk\nPk\nIf t\u0302 = 0, we have lim sup\u03b1\u2192\u221e i=1 \u03b2i ui (t\u03b1 , x\u03b1\ni ) =\ni=1 \u03b2i ui (0, x\u0302) \u2264 0. We\nknow that t\u0302 > 0 and thus t\u03b1 must be strictly positive for large \u03b1. It follows\nfrom Theorem 6.1 that, for each \u03b5 > 0 there exists b\u03b1\ni \u2208 R, Xi \u2208 S(N ) such that\n\u03b1\n\u03b1\n\u03b1\n \u03042;+\n(b\u03b1\ni , Dxi \u03c6(x ), Xi ) \u2208 JQi (ui )(t , xi ),\n\nk\nX\ni=1\n\n\u03b2i b \u03b1\ni = 0, for i = 1, * * * , k,\n\n(29)\n\nand such that\n\uf8eb\n\n\u03b21 X 1\n\uf8ec\n.\n1\n\uf8ec .\n\u2212 ( + kAk)I \u2264 \uf8ec .\n\u03b5\n\uf8ed 0\n0\n\n...\n0\n..\n..\n.\n.\n. . . \u03b2k\u22121 Xk\u22121\n...\n0\n\n0\n..\n.\n0\n\u03b2k X k\n\nwhere A = D2 \u03c6\u03b1 (x\u03b1 ) \u2208 S(kN ) is explicitly given by\n\uf8eb\nIN\n\u2212IN 0 . . .\n\uf8ec ..\n..\n\uf8ec\n.\nA = \u03b1JkN , where JkN = \uf8ec .\n\uf8ed 0\n0 . . . \u2212 IN\n\u2212IN\n0...0\nThe second inequality of (30) implies\n\nPk\n\ni=1\n\n\uf8f6\n\n\uf8f7\n\uf8f7\n\uf8f7 \u2264 A + \u03b5A2 ,\n\uf8f8\n\n0\n..\n.\nIN\n\u2212IN\n\n\uf8f6\n\u2212IN\n.. \uf8f7\n. \uf8f7\n\uf8f7.\n\u2212IN \uf8f8\nIN\n\n\u03b2i Xi \u2264 0. Setting\n\n\u22121\n\u03b1\n\u03b1\n\u03b1\n\u03b1\np\u03b1\n1 = Dx1 \u03c6\u03b1 (x ) = \u03b21 \u03b1(2x1 \u2212 x3 \u2212 x2 ),\n..\n.\n\u22121\n\u03b1\n\u03b1\n\u03b1\n\u03b1\np\u03b1\nk = Dxk \u03c6\u03b1 (x ) = \u03b2k \u03b1(2xk \u2212 xk\u22121 \u2212 x1 ).\n\nThus\n\nPk\n\ni=1\n\n\u03b2i p \u03b1\ni = 0. This with (29) and (27) it follows that\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\nb\u03b1\ni \u2212 Gi (t , xi , ui (t , xi ), pi , Xi ) \u2264 \u2212c, i = 1, * * * , k.\n\n20\n\n(30)\n\n\f\u03b1\n\u03b1\nBy (28)-(i) we also have lim\u03b1\u2192\u221e |p\u03b1\ni | * |xi \u2212 x1 | \u2192 0. This, together with the\nthe domination condition (26) of Gi , implies\nk\nX\n\n\u2212kc = \u2212\n\nk\nX\n\n\u03b2i b \u03b1\ni \u2212 kc \u2265 \u2212\n\n\u2265\u2212\n\nk\nX\n\n\u03b1\n\u03b1\n\u03b1\n\u03b2i Gi (t\u03b1 , x\u03b1\n1 , ui (t , xi ), pi , Xi )\n\ni=1\n\ni=1\n\ni=1\n\n\u2212\n\u2265\u2212\n\n\u03b1\n\u03b1\n\u03b1\n\u03b2i Gi (t\u03b1 , x\u03b1\ni , ui (t , xi ), pi , Xi )\n\nk\nX\ni=1\n\nk\nX\ni=1\n\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u2212\n\u03b2i [Gi (t\u03b1 , x\u03b1\ni , ui (t , xi ), pi , Xi ) \u2212 Gi (t , x1 , ui (t , xi ), pi , Xi )]\n\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b1\n\u03b2i \u03c9\u0304(1 + (T \u2212 T0 )\u22121 + |x\u03b1\n1 | + |xi | + |ui (t , xi )|)\u03c9(|xi \u2212 x1 | + |pi | * |xi \u2212 x1 |)\n\nThe right side tends to zero as \u03b1 \u2192 \u221e, which induces a contradiction. The\nproof is complete.\nTheorem 6.3 (Domination Theorem) Let polynomial growth functions ui \u2208USC([0, T ]\u00d7\nRN ) be subsolutions of\n\u2202t u \u2212 Gi (u, Du, D2 u) = 0,\n\ni = 1, * * * , k,\n\n(31)\n\non (0, T ) \u00d7 RN . We assume that Gi : R \u00d7 RN \u00d7 S(N) 7\u2192 R, i = 1, * * * , k, are\ngiven continuous functions satisfying the following conditions: There exists a\npositive constant C, such that\n(i) for all \u03bb \u2265 0\n\u03bbGi (v, p, Y ) \u2264 CGi (\u03bbv, \u03bbp, \u03bbY );\n(ii) Lipschitz condition:\n|Gi (v1 , p, X) \u2212 Gi (v2 , q, Y )| \u2264 C(|v1 \u2212 v2 | + |p \u2212 q| + kX \u2212 Y k),\n\u2200v1 , v2 \u2208 R, \u2200p, q \u2208 RN and X, Y \u2208 S(N ),\n\n(iii) domination condition for Gi : for fixed constants \u03b2i > 0, i = 1, * * * , k,\nk\nX\ni=1\n\n\u03b2i Gi (vi , pi , Xi ) \u2264 0, for all vi \u2208 R, pi \u2208 RN , Xi \u2208 S(N ),\n\nsuch that\n\nk\nX\ni=1\n\n\u03b2i vi \u2265 0,\n\nk\nX\n\n\u03b2i pi = 0,\n\ni=1\n\ni=1\n\nThen the following domination holds: If\ncontinuous function, then we have\nk\nX\ni=1\n\n\u03b2i ui (t, x) \u2264 0,\n\nk\nX\n\nPk\n\n\u03b2i Xi \u2264 0.\n\ni=1\n\n\u03b2i ui (0, *) is a non-positive\n\n\u2200(t, x) \u2208 (0, T ) \u00d7 RN .\n\n21\n\n\fProof. We set \u03be(x) := (1 + |x|2 )l/2 and\n\u0169i (t, x) := ui (t, x)e\u03bbt \u03be \u22121 (x), i = 1, * * * , k,\n\nPk\nwhere l is chosen large enough so that\ni=1 |\u0169i (t, x)| \u2192 0 uniformly. From\ncondition (i) it is easy to check that for each i = 1, * * * , \u0169i is a subsolution of\n\u2202t \u0169i \u2212 G\u0303i (x, \u0169i , D\u0169i , D2 \u0169i ) = 0,\n\n(32)\n\nwhere\nG\u0303i (x, v, p, X) := \u2212\u03bbv\n\n+ e\u03bbt CGi (v, p + v\u03b7(x), X + p \u2297 \u03b7(x) + \u03b7(x) \u2297 p + v\u03ba(x)).\n\nHere\n\u03b7(x) := \u03be \u22121 (x)D\u03be(x) = k(1 + |x|2 )\u22121 x,\n\n\u03ba(x) := \u03be \u22121 (x)D2 \u03be(x) = k(1 + |x|2 )\u22121 I \u2212 k(k \u2212 2)(1 + |x|2 )\u22122 x \u2297 x.\nSince \u03b7 and \u03ba are uniformly bounded, one can choose a fixed but large enough\n\u03bb > 0 such that G\u0303i (x, v, p, X) satisfies all conditions of Gi , i = 1, * * * , k in\nTheorem 6.2. The proof is complete by directly applying this theorem.\nWe have the following Corollaries which are basic in this paper:\nCorollary 6.4 (Comparison Theorem) Let F1 , F2 : RN \u00d7 S(N ) 7\u2192 R be given\nfunctions satisfying similar conditions (i) and (ii) of Theorem 6.3. We also\nassume that, for each p, q \u2208 RN and X, Y \u2208 S(N ) such that X \u2265 Y , we have\nF1 (p, X) \u2265 F2 (p, Y ).\nLet vi \u2208LSC((0, T ) \u00d7 RN ), i = 1, 2, be respectively a viscosity supersolution of\n\u2202t v \u2212 Fi (Dv, D2 v) = 0 such that v1 (0, *) \u2212 v2 (0, *) is a non-negative continuous\nfunction. Then we have v1 (t, x) \u2212 v2 (t, x) \u2265 0 for all (t, x) \u2208 [0, \u221e) \u00d7 RN .\nProof. We set \u03b21 = \u03b22 = 1, G1 (p, X) := \u2212F1 (\u2212p, \u2212X) and G2 = F2 (p, X).\nIt is observed that u1 := \u2212v1 \u2208USC((0, T ) \u00d7 RN ) is a viscosity subsolution of\n\u2202t u \u2212 G1 (Du, D2 u) = 0. For each p1 , p2 \u2208 RN and X1 , X2 \u2208 S(N ) such that\np1 + p2 = 0 and X1 + X2 \u2264 0, we also have\nG1 (p1 , X1 ) + G2 (p2 , X2 ) = F2 (p2 , X2 ) \u2212 F1 (p2 , \u2212X1 ) \u2264 0\nWe thus can apply Theorem 6.3 to get \u2212u1 + u2 \u2264 0. The proof is complete.\nCorollary 6.5 (Domination Theorem) Let Fi : RN \u00d7 S(N ) 7\u2192 R, i = 0, 1,\ngiven functions satisfying similar conditions (i) and (ii) of Theorem 6.3. Let\nvi \u2208LSC((0, T ) \u00d7 RN ) be viscosity supersolutions of \u2202t v \u2212 Fi (Dv, D2 v) = 0\n\n22\n\n\frespectively for i = 0, 1 and let v2 \u2208USC((0, T ) \u00d7 RN ) be a viscosity subsolution\nof \u2202t v \u2212 F1 (Dv, D2 v) = 0. We assume that:\nF1 (p, X) \u2212 F1 (q, Y ) \u2264 F0 (p \u2212 q, Z),\n\n\u2200p, q \u2208 RN , X, Y, Z \u2208 S(N ) such that X \u2212 Y \u2264 Z.\n\nThen the following domination holds: If v0 (0, *)+v1 (0, *)\u2212v2 (0, *) is a continuous\nand non-negative function then v0 (t, *) + v1 (t, *) \u2212 v2 (t, *) \u2265 0 for all t > 0.\nProof. We denote\nGi (p, X) := \u2212Fi (\u2212p, \u2212X), i = 0, 1, and G2 (p, X) := F1 (p, X).\nObserve that ui = \u2212vi \u2208USC((0, T ) \u00d7 RN ), i = 0, 1, are viscosity subsolutions\nof \u2202t u \u2212 Gi (Du, D2 u) = 0, i = 0, 1. We thus have, for each X0 + X1 + X2 \u2264 0,\np0 + p1 + p2 = 0,\nG0 (p0 , X0 ) + G1 (p1 , X1 ) + G2 (p2 , X2 )\n= \u2212F0 (\u2212p0 , \u2212X0 ) \u2212 F1 (\u2212p1 , \u2212X1 ) + F1 (p2 , X2 ) \u2264 0.\nP\nTheorem 6.3 can be applied, for the case \u03b2i = 1, to get ui \u2264 0, or v0 +v1 \u2212v2 \u2265\n0.\nAnother co-product of Theorem 6.3 is:\nCorollary 6.6 (Concavity) Let F : RN \u00d7 S(N ) 7\u2192 R be a given function satisfying similar conditions (i) and (ii) of Theorem 6.3. We assume that F is\nmonotone in X, i.e. F (p, X) \u2265 F (p, Y ) if X \u2265 Y , and that F is concave: for\neach fixed \u03b1 \u2208 (0, 1),\n\u03b1F (p, X)+(1\u2212\u03b1)F (q, Y ) \u2264 F (\u03b1p+(1\u2212\u03b1)q, \u03b1X+(1\u2212\u03b1)Y ), \u2200p, q \u2208 RN , X, Y \u2208 S(N ).\nLet vi \u2208USC((0, T ) \u00d7 RN ), i = 0, 1, be respectively viscosity subsolutions of\n\u2202t v \u2212 F (Dv, D2 v) = 0 and let v \u2208LSC((0, T ) \u00d7 RN ) be viscosity supersolution of\n\u2202t v\u2212F (Dv, D2 v) = 0 such that \u03b1v1 (0, *)+(1\u2212\u03b1)v2 (0, *)\u2212v(0, *) is a non-positive\ncontinuous function. Then for all t \u2265 0 \u03b1v1 (t, *) + (1 \u2212 \u03b1)v2 (t, *) \u2212 v(t, *) \u2265 0.\nProof. We set \u03b21 = \u03b1, \u03b22 = (1 \u2212 \u03b1), \u03b23 = 1 and denote\nG1 (p, X) = G2 (p, X) := F (p, X), G3 (p, X) = \u2212F (\u2212p, \u2212X).\nObserve that ui = vi \u2208USC((0, T ) \u00d7 RN ), i = 1, 2, are viscosity subsolutions\nof \u2202t u \u2212 Gi (Du, D2 u) = 0, u3 = \u2212v \u2208 USC is a viscosity subsolution of \u2202t u \u2212\nG3 (Du, D2 u) = 0. Since F is concave, thus for each pi \u2208 RN and Xi \u2208 S(N )\nsuch that \u03b21 X1 + \u03b22 X2 + \u03b23 X3 \u2264 0, \u03b21 p1 + \u03b22 p2 + \u03b23 p3 = 0, we have\n\u03b21 G1 (p1 , X1 ) + \u03b22 G2 (p2 , X2 ) + \u03b23 G3 (p3 , X3 ) \u2264 F (\u03b21 p1 + \u03b22 p2 , \u03b21 X1 + \u03b22 X2 ) \u2212 F (\u2212p3 , \u2212X3 )\n\u2264 F (\u2212p3 , \u03b21 X1 + \u03b22 X2 ) \u2212 F (\u2212p3 , \u2212X3 )\n\u2264 0.\n\nTheorem 6.3 can be applied to prove that \u03b1v1 (t, *) + (1 \u2212 \u03b1)v2 (t, *) \u2264 v(t, *). The\nproof is complete.\n23\n\n\fReferences\n[1] X. Cabre and L.A. Caffarelli, Fully nonlinear elliptic partial di erential\nequations, American Math. Society (1997).\n[2] Crandall, M., Ishii, H., and Lions, P.-L. (1992) User'S Guide To Viscosity\nSolutions Of Second Order Partial Differential Equations, Bulletin Of The\nAmerican Mathematical Society, 27(1), 1-67.\n[3] L. Denis, M. Hu and S. Peng, Function spaces and capacity related to\na Sublinear Expectation: application to G-Brownian Motion Pathes, see\narXiv:0802.1240v1 [math.PR] 9 Feb 2008.\n[4] L. Denis and C. Martini, A Theoretical Framework for the Pricing of Contingent Claims in the Presence of Model Uncertainty, The Annals of Applied Probability, vol. 16, No. 2, pp 827-852, 2006.\n[5] Krylov, N.V. Nonlinear Parabolic and Elliptic Equations of the Second Order, Reidel Publishing Company, 1987 (Original Russian version by Nauka,\nMoscow, 1985).\n[6] P. L\u00e9vy, Calcul d\u00e9s Probabilit\u00e9s, Gautier-Villars, 1925.\n[7] P. L\u00e9vy, Processus Stochastic et Mouvement Brownian, Jacques Gabay,\n2\u00e8me \u00e9dition, Gautier-Villars, 1965.\n[8] Maxwell, James Clerk, On the dynamic theory of gases, Brit. Assoc. Rep.,\n1859 (pt.2) p.9. in 1859;\n[9] Marinacci, M. Limit laws for non-additive probabilities and their frequentist\ninterpretation, Journal of Economic Theory 84, 145-195 1999.\n[10] Maccheroni, F. and Marinacci, M. A strong law of large numbers for capacities, Preprint, 2004.\n[11] Peng, S. (2004) Filtration Consistent Nonlinear Expectations and Evaluations of Contingent Claims, Acta Mathematicae Applicatae Sinica, English\nSeries 20(2), 1\u201324.\n[12] Peng, S. (2005) Nonlinear expectations and nonlinear Markov chains, Chin.\nAnn. Math. 26B(2), 159\u2013184.\n[13] Peng, S. (2006) G\u2013Expectation, G\u2013Brownian Motion and Related\nStochastic Calculus of It\u00f4's type, preprint (pdf-file available in\narXiv:math.PR/0601035v1 3Jan 2006), to appear in Proceedings of the 2005\nAbel Symposium.\n[14] Peng, S. (2006) Multi-Dimensional G-Brownian Motion and Related\nStochastic Calculus under G-Expectation, in arXiv:math.PR/0601699v1\n28Jan 2006.\n\n24\n\n\f[15] Peng, S. (2007) Law of large numbers and central limit theorem under\nnonlinear expectations, in arXiv:math.PR/0702358v1 13 Feb 2007.\n[16] Peng, S. (2007) G\u2013Brownian Motion and Dynamic Risk Measure under\nVolatility Uncertainty, in arXiv:0711.2834v1 [math.PR] 19 Nov 2007.\n[17] Song, Y. (2007) A general central limit theorem under Peng's G-normal\ndistribution, Preprint.\n[18] L. Wang, (1992) On the regularity of fully nonlinear parabolic equations:\nII, Comm. Pure Appl. Math. 45, 141-178.A general central limit theorem\nunder\n[19] K. Yosida, Functional Analysis, Sixth Edition, Springer-Verlag.\n\n25\n\n\f"}