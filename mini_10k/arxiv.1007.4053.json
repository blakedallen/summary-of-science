{"id": "http://arxiv.org/abs/1007.4053v1", "guidislink": true, "updated": "2010-07-23T06:50:52Z", "updated_parsed": [2010, 7, 23, 6, 50, 52, 4, 204, 0], "published": "2010-07-23T06:50:52Z", "published_parsed": [2010, 7, 23, 6, 50, 52, 4, 204, 0], "title": "AstroGrid-D: Grid Technology for Astronomical Science", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.1039%2C1007.0126%2C1007.1764%2C1007.1737%2C1007.1198%2C1007.1090%2C1007.2052%2C1007.4772%2C1007.1412%2C1007.3213%2C1007.1053%2C1007.0678%2C1007.5255%2C1007.2417%2C1007.3015%2C1007.0712%2C1007.3184%2C1007.3052%2C1007.0837%2C1007.3264%2C1007.0496%2C1007.2716%2C1007.4622%2C1007.4220%2C1007.4902%2C1007.3454%2C1007.4331%2C1007.5332%2C1007.4309%2C1007.3334%2C1007.2046%2C1007.0538%2C1007.0860%2C1007.4190%2C1007.3350%2C1007.0331%2C1007.4301%2C1007.4032%2C1007.5188%2C1007.1635%2C1007.3443%2C1007.3623%2C1007.0558%2C1007.0829%2C1007.5502%2C1007.2318%2C1007.0735%2C1007.1716%2C1007.2582%2C1007.3724%2C1007.1937%2C1007.1535%2C1007.4362%2C1007.0748%2C1007.3152%2C1007.3018%2C1007.4642%2C1007.4140%2C1007.1269%2C1007.2521%2C1007.1762%2C1007.0065%2C1007.5123%2C1007.5311%2C1007.5370%2C1007.3260%2C1007.2951%2C1007.5433%2C1007.1005%2C1007.2248%2C1007.3245%2C1007.5194%2C1007.3535%2C1007.0759%2C1007.1350%2C1007.5298%2C1007.0041%2C1007.4053%2C1007.3040%2C1007.5109%2C1007.3931%2C1007.3077%2C1007.0445%2C1007.1982%2C1007.3473%2C1007.2066%2C1007.4325%2C1007.3741%2C1007.2408%2C1007.3530%2C1007.1252%2C1007.3905%2C1007.4263%2C1007.2182%2C1007.1817%2C1007.0777%2C1007.5386%2C1007.5149%2C1007.1985%2C1007.1705%2C1007.1012&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "AstroGrid-D: Grid Technology for Astronomical Science"}, "summary": "We present status and results of AstroGrid-D, a joint effort of\nastrophysicists and computer scientists to employ grid technology for\nscientific applications. AstroGrid-D provides access to a network of\ndistributed machines with a set of commands as well as software interfaces. It\nallows simple use of computer and storage facilities and to schedule or monitor\ncompute tasks and data management. It is based on the Globus Toolkit middleware\n(GT4). Chapter 1 describes the context which led to the demand for advanced\nsoftware solutions in Astrophysics, and we state the goals of the project. We\nthen present characteristic astrophysical applications that have been\nimplemented on AstroGrid-D in chapter 2. We describe simulations of different\ncomplexity, compute-intensive calculations running on multiple sites, and\nadvanced applications for specific scientific purposes, such as a connection to\nrobotic telescopes. We can show from these examples how grid execution improves\ne.g. the scientific workflow. Chapter 3 explains the software tools and\nservices that we adapted or newly developed. Section 3.1 is focused on the\nadministrative aspects of the infrastructure, to manage users and monitor\nactivity. Section 3.2 characterises the central components of our architecture:\nThe AstroGrid-D information service to collect and store metadata, a file\nmanagement system, the data management system, and a job manager for automatic\nsubmission of compute tasks. We summarise the successfully established\ninfrastructure in chapter 4, concluding with our future plans to establish\nAstroGrid-D as a platform of modern e-Astronomy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.1039%2C1007.0126%2C1007.1764%2C1007.1737%2C1007.1198%2C1007.1090%2C1007.2052%2C1007.4772%2C1007.1412%2C1007.3213%2C1007.1053%2C1007.0678%2C1007.5255%2C1007.2417%2C1007.3015%2C1007.0712%2C1007.3184%2C1007.3052%2C1007.0837%2C1007.3264%2C1007.0496%2C1007.2716%2C1007.4622%2C1007.4220%2C1007.4902%2C1007.3454%2C1007.4331%2C1007.5332%2C1007.4309%2C1007.3334%2C1007.2046%2C1007.0538%2C1007.0860%2C1007.4190%2C1007.3350%2C1007.0331%2C1007.4301%2C1007.4032%2C1007.5188%2C1007.1635%2C1007.3443%2C1007.3623%2C1007.0558%2C1007.0829%2C1007.5502%2C1007.2318%2C1007.0735%2C1007.1716%2C1007.2582%2C1007.3724%2C1007.1937%2C1007.1535%2C1007.4362%2C1007.0748%2C1007.3152%2C1007.3018%2C1007.4642%2C1007.4140%2C1007.1269%2C1007.2521%2C1007.1762%2C1007.0065%2C1007.5123%2C1007.5311%2C1007.5370%2C1007.3260%2C1007.2951%2C1007.5433%2C1007.1005%2C1007.2248%2C1007.3245%2C1007.5194%2C1007.3535%2C1007.0759%2C1007.1350%2C1007.5298%2C1007.0041%2C1007.4053%2C1007.3040%2C1007.5109%2C1007.3931%2C1007.3077%2C1007.0445%2C1007.1982%2C1007.3473%2C1007.2066%2C1007.4325%2C1007.3741%2C1007.2408%2C1007.3530%2C1007.1252%2C1007.3905%2C1007.4263%2C1007.2182%2C1007.1817%2C1007.0777%2C1007.5386%2C1007.5149%2C1007.1985%2C1007.1705%2C1007.1012&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present status and results of AstroGrid-D, a joint effort of\nastrophysicists and computer scientists to employ grid technology for\nscientific applications. AstroGrid-D provides access to a network of\ndistributed machines with a set of commands as well as software interfaces. It\nallows simple use of computer and storage facilities and to schedule or monitor\ncompute tasks and data management. It is based on the Globus Toolkit middleware\n(GT4). Chapter 1 describes the context which led to the demand for advanced\nsoftware solutions in Astrophysics, and we state the goals of the project. We\nthen present characteristic astrophysical applications that have been\nimplemented on AstroGrid-D in chapter 2. We describe simulations of different\ncomplexity, compute-intensive calculations running on multiple sites, and\nadvanced applications for specific scientific purposes, such as a connection to\nrobotic telescopes. We can show from these examples how grid execution improves\ne.g. the scientific workflow. Chapter 3 explains the software tools and\nservices that we adapted or newly developed. Section 3.1 is focused on the\nadministrative aspects of the infrastructure, to manage users and monitor\nactivity. Section 3.2 characterises the central components of our architecture:\nThe AstroGrid-D information service to collect and store metadata, a file\nmanagement system, the data management system, and a job manager for automatic\nsubmission of compute tasks. We summarise the successfully established\ninfrastructure in chapter 4, concluding with our future plans to establish\nAstroGrid-D as a platform of modern e-Astronomy."}, "authors": ["Harry Enke", "Matthias Steinmetz", "Hans-Martin Adorf", "Alexander Beck-Ratzka", "Frank Breitling", "Thomas Bruesemeister", "Arthur Carlson", "Torsten Ensslin", "Mikael Hoegqvist", "Iliya Nickelt", "Thomas Radke", "Alexander Reinefeld", "Angelika Reiser", "Tobias Scholl", "Rainer Spurzem", "Juergen Steinacker", "Wolfgang Voges", "Joachim Wambsganss", "Steve White"], "author_detail": {"name": "Steve White"}, "author": "Steve White", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.newast.2010.07.005", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1007.4053v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1007.4053v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "14 pages, 12 figures Subjects: data analysis, image processing,\n  robotic telescopes, simulations, grid. Accepted for publication in New\n  Astronomy", "arxiv_primary_category": {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "astro-ph.IM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1007.4053v1", "affiliation": "AIP", "arxiv_url": "http://arxiv.org/abs/1007.4053v1", "journal_reference": "New Astron.16:79-93,2011", "doi": "10.1016/j.newast.2010.07.005", "fulltext": "AstroGrid-D: Grid Technology for Astronomical Science\nHarry Enkea , Matthias Steinmetza , Hans-Martin Adorfb , Alexander Beck-Ratzkac , Frank Breitlinga , Thomas\nBr\u00fcsemeisterd , Arthur Carlsonf , Torsten Ensslinb , Mikael H\u00f6gqviste , Iliya Nickelta , Thomas Radkec , Alexander\nReinefelde , Angelika Reiserg , Tobias Schollg , Rainer Spurzemd,h , J\u00fcrgen Steinackerd , Wolfgang Vogesf , Joachim\nWambsganssd , Steve Whitea\na Astrophysikalisches\n\nInstitut Potsdam AIP, Potsdam, Germany\nf\u00fcr Astrophysik MPA, Garching, Germany\nc Max-Planck-Institut f\u00fcr Gravitationsphysik (Albert-Einstein Institut) AEI, Potsdam, Germany\nd Astronomisches Recheninstitut am Zentrum f\u00fcr Astronomie Heidelberg ZAH, Heidelberg, Germany\ne Konrad-Zuse-Zentrum f\u00fcr Informationstechnik Berlin ZIB, Berlin, Germany\nf Max-Planck-Institut f\u00fcr extraterrestrische Physik MPE, Garching, Germany\ng Technische Universit\u00e4t M\u00fcnchen, Institut f\u00fcr Informatik TUM, Garching, Germany\nNational Astronomical Observatories of China, Chinese Academy of Sciences NAOC/CAS, 20A Datun Rd., Chaoyang District, Beijing\n100012, China\nb Max-Planck-Institut\n\narXiv:1007.4053v1 [cs.DC] 23 Jul 2010\n\nh\n\nAbstract\nWe present status and results of AstroGrid-D, a joint effort of astrophysicists and computer scientists to employ grid\ntechnology for scientific applications. AstroGrid-D provides access to a network of distributed machines with a set of\ncommands as well as software interfaces. It allows simple use of computer and storage facilities and to schedule or\nmonitor compute tasks and data management. It is based on the Globus Toolkit middleware (GT4).\nChapter 1 describes the context which led to the demand for advanced software solutions in Astrophysics, and we\nstate the goals of the project.\nWe then present characteristic astrophysical applications that have been implemented on AstroGrid-D in chapter\n2. We describe simulations of different complexity, compute-intensive calculations running on multiple sites (2.1), and\nadvanced applications for specific scientific purposes (2.2), such as a connection to robotic telescopes (2.2.3). We can\nshow from these examples how grid execution improves e.g. the scientific workflow.\nChapter 3 explains the software tools and services that we adapted or newly developed. Section 3.1 is focused on the\nadministrative aspects of the infrastructure, to manage users and monitor activity. Section 3.2 characterises the central\ncomponents of our architecture: The AstroGrid-D information service to collect and store metadata, a file management\nsystem, the data management system, and a job manager for automatic submission of compute tasks.\nWe summarise the successfully established infrastructure in chapter 4, concluding with our future plans to establish\nAstroGrid-D as a platform of modern e-Astronomy.\nKeywords: methods: data analysis, methods: numerical, techniques: image processing, telescopes\nPACS: 95.75.-z, 95.75.Pq, 89.20.Ff, 95.80.+p\n\n1. Introduction\nAstrophysical research is an intent driver for advances in\ncomputer science, especially so for high performance computing and data intensive calculations. We are used to the\ncontinuous increase of processor power which increases the\npotential of computer based analysis. Even faster is the\nrise of sensor size and storage capacity, both of which in\nrecent years have grown even stronger than Moore's Law\nwould predict. Unfortunately, this trend of growing data\nvolumes also increases the complexity of the data management, as well as the processing, analysis and visualisation.\nAbove a certain level, new methods have to be applied,\ne.g. the management of data becomes a task that is no\nlonger trivial enough for a file system alone. This challenge affects many other domains outside of Astrophysics\nPreprint submitted to Elsevier\n\nin the same way, and it is an important challenge to find\nanswers, since in several research areas further progress\ndepends on the successful processing of data volumes in\nthe high Terabyte or Petabyte scale.\nOne solution for improved data management is the recent success in meta data standardisation and advanced\ncorresponding protocols. In astrophysics this approach has\nled to the international \"Virtual Observatory\" initiative,\nwhich now allows for a fast search within extensive volumes of diverse stored data.\nBut computer science itself has also researched ways to\nimprove infrastructure usage and simplify the processing of\ninformation. The most compelling answer of recent years\nwas the massive development in Grid computing, where\na new software layer is used to connect distributed information infrastructures like clusters, storage servers and\nOctober 24, 2018\n\n\fdesktops to a loose network (see: (Foster, 2008)).\nSeveral research grid infrastructures were successfully\nset up in the past years. The most impressive example\nis the US \"TeraGrid\", funded since 2001 by the National\nScience Foundation. It offers over a petaflop of total compute capabilities and many different services and gateways\nto thousands of US scientists. Like the Open Science Grid,\nTeraGrid is based on the Globus Toolkit, enlarged by an\nauxilary software package set.\nThe European enterprise EGEE (\"Enabling Grids for\nE-SciencE\") was started 2004 as a EU project, sponsored\nfrom the European Union's research framework. EGEE\nwas at the beginning mostly driven by the CERN's new\nlarge Hadron Collider and its demand for compute power.\nIt currently combines about 40.000 CPUs and will in 2010\nbe transferred into a new body called EGI (European Grid\nInitiative). It will then focus mostly on the role to coordinate the collaboration of the national grid initiatives with\nsupported middlewares limited to gLite, UNICORE and\nARC.\nThe German national Grid initiative was inaugurated\nin 2004 by the Federal Research ministry. It has seen two\nmain stages: D-Grid 1 (2005-2008) focussed on Grid application for fundamental sciences, whereas D-Grid 2 (20072010) mostly researched Grid use in applied sciences and\nindustry.\nThe AstroGrid-D project was part of the first D-Grid\ninitiative and started in 2005. Five major German astronomy institutes participated: AIP, AEI, MPA, MPE, and\nZAH, together with computer science groups from the ZIB\nSupercomputer center and TUM. They collaborated on the\ncommon project goal: To establish a collaborative working\nenvironment for astronomy which provides the users with\nthe powerful and reliable software tools and allows easy\naccess to compute and storage facilities for their scientific\nwork.\nTo achieve this the projected aimed to:\n\nWe hereby present our experiences and results in some\ndetail. The paper is grouped into two main chapters: First\nthe astrophysical applications (2) and secondly our developments in information technology (3). In the summary\n(4) we give an outlook on our future plans.\n2. Astronomy and Grid: Astronomical Use Cases\nrunning on the project network\nMost areas of Astronomical research can profit from eScience concepts and grid technology in particular.\nIn the course of the project, a total of twenty selected astronomic pilot applications were modified for grid use and\nimplemented. Use cases ranged from compute-intensive\nsimulations running on clusters, task-farming jobs to explore large parameter spaces, analyzing programs accessing astronomical databases, to complex and specific applications as described below. These use cases also served to\ndefine the requirements for AstroGrid-D components.\nWhen considering a grid implementation for a given application, it is decisive to compare how time-consuming\nand complex the task will be compared to the benefits,\nsuch as speed gain. Before we describe examples in detail\nwe will state general experiences for different application\nclasses.\nFor large simulations, e.g. from cosmology (Mare Nostrum, WebLinks (2010)), a grid environment is ideal to\nreduce typical obstacles. In a grid infrastructure, a unified\nand standardised interface is provided to access the gridenabled resources of a high performance computing center.\nThe Grid offers a common way to execute calculations and\nmanage resulting data. Also many details, such as efficient\ndata transfer, are handled by the Grid middleware. The\nneed to learn details about a specific center is minimised.\nTaskfarming jobs benefit from the grid infrastructure\nsince there now is a multitude of resources available to\nthem, as shown for the Geo600-example (2.1.3). Especially\napplications with limited requirements can gain immensely\nfrom a grid implementation, where many hundreds of instances can be executed concurrently.\nRobotic telescopes (2.2.3) serve as an example for special scientific hardware. When combined to a worldwide\nnetwork on the basis of grid middleware, this brings important advantages to coordinated observations. Typical\ntasks for such a network are multi-wavelength campaigns\nor the continuous monitoring of transient astronomical objects. A grid based network simplifies coordination and infrastructure management, since grid devices such as storage servers and databases are easy to connect. Moreover,\nglobal grid schedulers can automatically coordinate and\noptimise the observations.\nFor large data sets like the Sloan Digital Sky Survey\n(SDSS, WebLinks (2010)) or the Millenium simulation\narchive (Springel et al., 2005), efficient processing poses\na huge problem. The data often have inconsistent formats and interfaces, and the methods still vary how to\n\n\u2022 set up a grid-based infrastructure for astronomical\nand astrophysical research\n\u2022 embed existing computational facilities, astronomical\nsoftware applications, data archives and instruments\n\u2022 integrate this grid infrastructure into the national\nD-Grid environment\n\u2022 provide support for other astronomical groups to join\n\u2022 strengthen international partnerships\nAstroGrid-D has reached these goals in its setup phase\nwhich ended early 2009. The most important results\nwere the first Virtual Organisation management, now the\nD-Grid-standard (see 3.1.1), integration of special hardware D-Grid (2.1.2, 2.2.3) and the production run of one\nof the most compute intensive scientific grid application to\ndate (2.1.3).\n2\n\n\fdefine subsets and correlate them, or even run algorithms\nagainst them. To select data, the scientist needs access to\na given database and, in most cases, also access to additional data files. Corresponding results must be stored in\nsome accessible device. Since the data volumes are growing\nlarge and the catalogues may be distributed, techniques for\ndata discovery searching, and transmission (data streaming) are applied, combined with mechanisms for parallelisation and load-balancing for the computing processes. At\nthis point, Grid data processing overcomes the limits of the\ncentralised data processing approach where so far large\nvolumes of data are transferred to the application that\nrequests them. The alternative is to distribute the data\nprocessing within the grid and the use of storage facilities accessible via grid methods. Whenever possible the\napplication is executed at the location of the data.\nMany solutions and design decisions, such as described\nin the last paragraph, rely on the work and standards of\nthe Virtual Observatory. Hence AstroGrid-D collaborates\nclosely with the German Astrophysical Virtual Observatory (GAVO), for example when using GAVOs easy-to-use\ndata access interface to N -body simulations. Via GAVO's\nparticipation in the IVOA activities, AstroGrid-D also participated from the developments where grid middleware is\nused to provide VObs services. We will continue the collaboration between AstroGrid-D and GAVO in the creation\nof a virtual data center for astronomy.\nTo support users in the deployment of their application,\nwe compiled an application-to-grid guide that illustrates\nthe steps to grid-enable simple applications (App2Grid,\nWebLinks (2010)).\n\nThe scientific problem for this example is derived from\nthe field of Magneto-Hydro-Dynamics. Rotation and turbulence in stars, accretion disks, and galaxies produce a\nmagnetic field by the dynamo effect. In the case shown\nhere the numerical simulation solves the induction equation with a turbulent electromotive force (alpha tensor).\nThe general parameter dependence as well as the time development of a given set are studied, with special focus on\nthe \"flip-flop\"-phenomenon of star spots (see Elstner and\nKorhonen, 2005).\nFor grid task farming with varying input sets the script\nreads in any number of input directories, each of which\ncontains different data. Together with the executable, the\njob is then submitted iteratively to grid resources specified\nin a list and executed there. Intermediate output can be\nretrieved on the fly; a visualisation example is shown in\nFig. 1.\n\n2.1. Compute-Intensive Generic Applications\nMany compute-intensive applications can be subdivided\ninto multiple small parallel tasks that can run independently, e.g. on multiple grid resources. This can usually\nbe achieved by partitioning the physical properties of the\nrelevant parameter space. In the following, we will discuss\nthree such compute-intensive grid applications, namely the\ntask-farming use case Dynamo, NBDOY6++ as an example use case with little I/O, and the gravitation wave analysis tool GEO600.\nWe have found that a grid implementation for this application type can be very beneficial and achieved within\na manageable timeframe.\n2.1.1. Dynamo\nThe Dynamo package shows how to use the advantages\nof grid computing without complex programming. Grid\nimplementation is achieved by a shell script, that is lean,\nrelatively simple to understand and easy to configure. It\nprovides a grid connection for the purpose of task farming\nof serial programs, i.e. the launching of many instances\nof scientific software where the input differs for each run.\nWe call this type of application atomic, since as a serial\ncalculation it requires no further communication until the\nresults are produced.\n\nFigure 1: Example output of a Dynamo run, showing real time results\nof four different grid resources\n\nThis solution is currently being applied to a similar use\ncase for GAVO. Upgrades of the software would properly\ninclude GridSphere and improve the stage-in process. The\nscript package can be downloaded from the AstroGrid-D\nuse case web pages (Dynamo, WebLinks (2010)). Users\nwith a demand for atomic, serial jobs should find this solution easy to implement within AstroGrid-D or within\nsimilar, Globus-based grids.\n3\n\n\f2.1.2. NBODY6++ and \u03c6GRAPE\nNBODY6++ and \u03c6GRAPE are two variants of a family of high-order accurate direct N -body simulation codes,\nwhich are built upon the development of a series of\nearlier versions (1-6) of NBODY codes (Aarseth, 1999).\n\u03c6GRAPE is the only parallel code of this type to use\nspecial purpose GRAPE6 hardware (Harfst et al., 2007)\nbased on GRAPE which has been designed by the University of Tokyo to accelerate gravitational force computations between particles (Makino et al., 2003; Fukushige\net al., 2005). While \u03c6GRAPE is just a plain direct parallel NBODY code using a 4th order Hermite integrator\nwith hierarchical block time steps, NBODY6++ is a parallel version of NBODY6 (with regularisation of close encounters, Ahmad-Cohen neighbour scheme, and other features), which is optimised for parallel general purpose supercomputers (Spurzem, 1999).\nExamples for applications where gravitational forces between many bodies have to be calculated are globular clusters, young forming star clusters or central dense star clusters in galactic nuclei. Recent typical research using direct N -body simulations includes, e.g., models of galactic\nstar clusters with many binaries (Hurley et al., 2007) or\nmassive binary black holes embedded in dense stellar systems leading to coalescence and gravitational wave emission (Berczik et al., 2005, 2006; Berentzen et al., 2009).\nNBODY6++ and \u03c6GRAPE are use cases of\nAstroGrid-D which supports deployment and execution of these as jobs on its resources using single and\nparallel hardware, as well as parallel hardware with\nspecial purpose GRAPE cards. The ZAH offers the 32\nnode GRACE cluster (GRACE, WebLinks (2010)) as a\nresource of AstroGrid-D, with reconfigurable specialised\nhardware to a total peak speed of 4 Teraflop/s (Harfst\net al., 2007; Spurzem et al., 2007, 2008). Another resource\nwith GRAPE hardware integrated in the AstroGrid-D\nis a cluster at the Main Astronomical Observatory in\nKiev, Ukraine (MAOKIEV, WebLinks (2010)), also an\nexample of collaboration made possible on the basis of a\ngrid Virtual Organisation.\nSubmission of an NBODY job starts with a shell script\npreparing an XML-based job description which is then\nstaged and transported through the AstroGrid-D Globus\nmiddleware. Input data, output data and files go along\nwith the job submission process. Future goals are to allow the submission of NBODY jobs through a portlet under the AstroGrid-D web portal and an integration of the\nAstroGrid-D file management system to allow handling of\nlarge datasets independent of the job staging process, see\ndeployment instructions and tutorial (NBODY6++, WebLinks (2010)).\n\nEinstein@Home is an ideal candidate for a grid application because of multi-platform support, well tested software base, simple resource requirements, built-in checkpoint and recovery methods, adjustable run time, and linear scaling with node number. Within the AstroGrid-D\nproject we developed the software for grid deployment,\njob statistics and the details for constant production mode\nruns, such as restart after a regular job end and cleanup\nof recoverable errors.\nThe deployment is triggered by a script which is invoked\nin a Web Service Grid Resource Allocation and Management (WS\u2013GRAM) job to all grid machines on which the\nGEO600 jobs should run. As prerequisites on the target\nresource only Subversion (to retrieve the GEO600 source\ncode) and a Perl interpreter are necessary. All other required software is installed during the deployment.\nDepending on the number of currently pending and active tasks, the submission script will automatically determine when to submit new tasks to a grid resource. To\nestablish a continuous submission scheme it is therefore\nsufficient to invoke the script periodically on the target.\n\nFigure 2: GEO600 CPU Time November 2008, taken from (GEO600statistics, WebLinks (2010)) with x-axis days of month, y-axis CPU\nhours consumed (sum over all used Grid resources)\n\nThe intermediate data are stored on the execution hosts\nsince a central server approach would significantly slow\ndown the job transfer rates. The submission of the\nGEO600 jobs can be controlled from a single workstation,\nfrom which the execution hosts are contacted directly. We\nplan to use the AstroGrid-D scheduler Gridway for the\ndistributions of the GEO600 jobs in a future update. Furthermore it is foreseen to extend the GEO600 use case to\ngrids based on other grid middlewares than Globus, such as\ngLite or Uniform Interface to Computing Resources (Unicore). This would allow a further distribution of the Einstein@Home jobs in the grids available.\n\n2.1.3. GEO600\nThe GEO600 use case is a task farming application. It\nuses the Einstein@Home application for analysing the data\nof the GEO600 Laser Interferometer near Hannover, in\norder to find signals of gravitational waves.\n\nThe GEO600 use case has been running in production mode for more than a year, and it consumes around\n100 000 CPU hours a day on D-Grid resources (see Fig. 2).\n4\n\n\f2.2. Advanced Applications\n\nthe installation and compilation on the resource, and an\n\"environment\" suite ensures that the necessary files and\nconnections are available on any resource.\nThe logistics of performing Clusterfinder calculations on\nthe grid involves splitting the calculation into jobs that can\nrun in parallel, identifying grid hosts with the capacity to\naccept a job at the given time, reassembling the individual\nresults into a coherent whole, and documenting the internal and external conditions under which the calculation\nwas carried out. A single calculation is then submitted\nas a globus job and calculates a likelihood map with a\ngiven set of parameters. The results are collected using\neither the post-staging capabilities of Globus or by direct\ngrid transfer using the globus-url-copy command. In the\ncase of Clusterfinder, special consideration has been given\nto the input data. The SDSS and ROSAT all sky survey (RASS, WebLinks (2010)) catalogues are too large to\ncopy the complete data set to a grid node. Therefore the\nmakefile controlling the Clusterfinder workflow is set up to\nrequest just the data needed from these catalogues.\nA demonstration version of Clusterfinder is available as\na portal application. The user can input coordinates and\nretrieve the corresponding likelihood map. It is planned to\nextend this portal to provide a production version of Clusterfinder as a grid service, including control over all the\ninput parameters and even the files for the cosmological\nmodel.\n\nSpecial purpose astrophysical applications and complex\ntool environments can also benefit from a grid infrastructure. We have chosen four relatively different use cases\nto represent this class of astrophysical applications and to\nshow how we approach an implementation.\nFirst, Clusterfinder is a use case involving both the deployment and performance of a typical compute-intense\ndata analysis application and the extensive use of distributed data resources. Cactus shows how monitoring\nand steering methods for parallel numerical simulations in\nthe grid can be generalised, and how a web portal can provide user-friendly assessment of grid jobs and visualisation.\nAccess to robotic telescopes as a grid resource represents\na unique approach to a grid with heterogeneous elements.\nFinally, the Planck Process Coordinator Workflow Engine\nProC has been grid enabled to demonstrate the power of\ngrid computing when applied to the complex workflow of\nprocessing the data product of a satellite mission. It is a\nuseful example for the handling of observation which may\nexceed the local capabilities or must be organised to suit\nthe demands of a locally distributed working group.\n2.2.1. Clusterfinder\nClusterfinder is an example for the deployment of a\ncompute-intense astrophysical application that uses distributed data, and its increase in performance. The scientific purpose of Clusterfinder is to reliably identify clusters of galaxies. It correlates the signature of X-ray images\nwith that in catalogues of optical observations in order to\nstudy the large scale structure of the universe. Scanning\nat optical wavelengths to look for areas with an unusually\nlarge number of galaxies is not an unambiguous method to\nidentify large clusters, as the galaxies may be spread out\nalong the line of sight. Also the observation of the X-ray\nemission of the hot gas between galaxies will result in some\nfalse identifications as there are many other X-ray sources.\nIn order to combine both sources of information, the theory of point processes is applied to calculate the statistical\nlikelihood of a cluster at any point in space, and peaks in\nthe combined likelihood are extracted into a catalogue of\ngalaxy clusters (Clusterfinder, WebLinks (2010)).\nData retrieval and the calculations can easily be parallelised as the algorithm for any point in the sky depends\nonly on data from nearby points, making Clusterfinder\nwell-suited for grid implementation. Input of the Clusterfinder program consists of a cosmology and galaxy cluster model, together with the grid of sky coordinates and\nredshifts on which the likelihood is to be calculated. Scanning the available data consumes about 20,000 CPU- hours\nper model. This entails over two years on a single processor or only several days when the resources of AstroGrid-D\nand D-Grid are used. An exploratory calculation on a\nsmaller area can be executed on the grid in one night.\nTo implement Clusterfinder for a grid environment, two\nsoftware tools were developed: A \"grid-module\" handles\n\n2.2.2. Cactus\nThe Cactus Computational ToolKit (CCTK) (Cactus,\nWebLinks (2010)) is an open source, general purpose software framework designed to solve large-scale systems of\npartial differential equations on supercomputers using finite differencing techniques. In the Astrophysics science\ncommunity Cactus is used to numerically simulate extremely massive bodies, such as neutron stars and black\nholes, and analyse the gravitational wave signal patterns\nemitted by these objects as predicted by Einstein's theory\nof General Relativity.\nIn AstroGrid-D we have developed application-specific\ntechniques for Cactus which enable scientists to manage\ntheir simulations more efficiently and in a more collaborative context. Many of these methods make use of standard\ngrid technology internally (Deliv. 6.6, WebLinks (2010)).\nAs an example for online application monitoring and\nsteering, users can connect to a running Cactus simulation just like any standard secure Hypertext Transfer Protocol (HTTP) web service, with a browser of their favorite\nchoice. User authentication and authorisation is based on\nX.509 grid certificates (see Section 3.1.1). When logged\nin, users can query an up-to-date status of the simulation\n(e.g. the physical simulation time or stdout/stderr log\noutput). Built-in online visualisation methods are available to analyse intermediate simulation data graphically\nvia dynamic generation of 1D line or 2D surface plots, thus\nallowing users to evaluate the quality of the simulation\nwhile the application is still running. Once authorised,\n5\n\n\fthey can also steer the simulation by interactively changing parameters, triggering a checkpoint to be written, or\nby terminating the job gracefully.\nEach Cactus simulation submitted to some supercomputer or grid resource can also announce itself at startup to\nthe AstroGrid-D information service, by sending an RDF\ndocument with metadata uniquely describing the simulation. The information service is then able to keep a history\nof all simulations submitted by Cactus users. To access\nand search that simulation database we provide a Cactus\nportlet, based on GridSphere (see Section 3.1.4) as a standardised web interface. After logging into the portal, users\ncan query the list of Cactus runs and filter it by owner, execution host, specific parameter settings etc. Queries are\nimplemented as Cactus-specific GridSphere portlets (Deliv. 7.5, WebLinks (2010)), allowing the user to easily\nnavigate through the list of simulations and browse individual query results. Also available in the portal are the\nresults of nightly Cactus integration tests, which are performed automatically on various machines in the grid, in\norder to verify the correctness of the latest development\nversion of the code.\n\nthe Heterogeneous Telescope Network (HTN) (Allan et al.,\n2006) serves as the protocol for observation requests.\nThe OpenTel Tools package provides programs for the\ntasks of observation (job) submission, cancellation, and\nstatus queries. The programs are based on commands of\nthe Globus Toolkit and are executed from the command\nline. Further details are described in (Deliv 5.3, WebLinks\n(2010)) and in the package documentation.\nSeveral user interfaces have been developed to simplify\noperation management: the OpenTel Tools, the Telescope\nMap, the Telescope Timeline, a broker, and a scheduler.\nThe Telescope Map is an interactive user interface shown in\nFig. 3. It is an extension of the AstroGrid-D Resource Map\n(section 3.1.3) for displaying geographic locations of telescopes and their properties such as available filters. Also\ndisplayed are day and night regions as well as weather information.\n\n2.2.3. Robotic Telescopes\nIn recent years a growing number of ground-based\nrobotic telescopes have been comissioned in astronomy,\ndue to their increased technical reliability. Robotic astronomy allows observations from sites which may be astronomically favourable, but are otherwise remote or even\nhostile for human operators, e.g. Antarctica.\nWith more robotic telescopes becoming operational,\nthere has been increasing interest in interconnecting them.\nSuch a telescope network can accomplish new types of\nobervations. Examples are an uninterrupted observational\ncampaign over many hours independent of day time and\nweather as required in astro-seismology, and rapid multiwavelength observations in case of transient events.\nAstroGrid-D contributes to this development with the\n(OpenTel, WebLinks (2010)) software package. OpenTel achieves the integration of robotic telescopes into the\nAstroGrid-D infrastructure and implements a telescope\nnetwork based on grid middleware. Each telescope thus\nacts as an individual grid resource with its own grid certificate. One immediate advantage provided by grid technology is the direct connection to compute and storage\nresources for data analysis and archiving. Additionally,\ngrid user and virtual organisation management provides a\ngood solution for the central management of access rights.\nThe metadata management relies on Stellaris (cf. section 3.2.1) and the (Usage Record format WebLinks\n(2010)) of the Global Grid Forum transformed into RDF.\nThe metadata is retrieved from Stellaris using Simple Protocol and RDF Query Language (SPARQL) queries. The\nmonitoring of observations is similar to the observation\nof jobs described below in section 3.1.3. The Robotic\nTelescope Markup Language (RTML) (Hessman, 2006) of\n\nFigure 3: The Telescope Map is an interactive user interface for the\nselection of telescopes. Daytime, weather conditions as well as the\ngeographic location and properties of the telescopes are displayed.\n\nThe Telescope Timeline is another interactive user interface useful for monitoring (Deliv. 2.7, WebLinks (2010)).\nIt is an extension of the AstroGrid-D Timeline (3.1.3) and\ndisplays information about executed observations with an\nappearance similar to Fig. 7.\nThe broker achieves an automatic selection of telescopes\nbased on the requirements of an observation (Deliv. 5.5,\nWebLinks (2010)). Filters and geographic coordinates but\nalso the dynamic data such as the current weather conditions are examples for selection criteria.\nThe network scheduler generates observation schedules\nof the desired duration (Deliv. 5.8, WebLinks (2010)).\nWhenever necessary, an observation is handed over to be\ncontinued by another telescope of the network. An example for a 24 h observation of the star Gliese 586A (Gl586A)\nin the small network of Fig. 3 is shown in Fig. 4.\nThe OpenTel software has been tested with the AIP's\nrobotic telescope STELLA-I (Strassmeier et al., 2004) and\nsimulated networks. It is available at (OpenTel, WebLinks\n(2010)).\n6\n\n\f90\nAmadeus.aip.de 02:30:00 - 06:50:00\nFaulkesTelescopeNorth.faulkes-telescope.com 06:40:00 - 10:20:00\nFaulkesTelescopeSouth.faulkes-telescope.com 10:10:00 - 17:50:00\nSTELLA-I.aip.de 19:40:00 - 02:40:00\nRoboTel.aip.de 17:40:00 - 19:50:00\n\n80\n70\n\nAlt [deg.]\n\n60\n50\n40\n30\n20\n10\n0\n0\n\n5\n\n10\n\n15\n\n20\n\ntime since 2008-11-01T02:40:00 [h]\n\nObservation: Gl586A (J2000 15.46 -9.34)\nFigure 4: Altitudes versus observation time for the network schedule\nof a simulated 24 h observation of Gl586A. The plot is produced\nby the OpenTel scheduler. The intersections of the altitude curves\nprovide the time intervals for the observations by the different telescopes. Schedules are optimised for object altitude.\n\nFigure 5: ProC supported simulation: Galaxy collision calculated\nwith the GADGET-simulation package steered by the ProC sampling\ncontrol element.\n\n2.2.4. The ProC workflow engine for scientific gridcomputing\nThe Process Coordinator (ProC) is a scientific workflow\nengine. It was originally developed as an integral component of the software infrastructure for the Planck Surveyor\nsatellite mission of the European Space Agency (Bennett\net al., 2000).\nCurrently, two sets of scientific programs are being executed using the ProC, each forming a problem-domain\nspecific toolbox. One is the simulation and data analysis package required for the Planck mission and cosmic microwave background (CMB) research (Reinecke\net al., 2006). The other is a post-processing package\nfor GADGET-simulations of cosmic structure formation\n(Springel, 2005), shown in Fig. 5. Both cosmological research areas are expected to benefit strongly from the parallel computing resources now being accessible for parameter space sampling problems via the grid-enabled ProC.\nThe ProC software package consists of three components: a graphical workflow editor, a graphical user interface for workflow execution, and a workflow engine,\nequipped with an application programing interface (API)\nand a versatile command line interface for expert users.\nThe ProC is implemented platform independently in Java\nand uses the extensible markup language (XML).\nOne of the advantages of using the ProC, compared to\nsimple scripting, is its ability to automatically recognize\nopportunities for reusage of previously generated computational results and for parallel execution of computational\nunits. This latter capability can exploit multiple cores on\na single processor, multiple processors cooperating in a local cluster, or the hundreds of compute elements offered\nby a dispersed grid.\nWith the help of the ProC Pipeline Editor the user is\nable to compose and modify scientific workflows consisting of programs, data flows, and control elements of the\n\nProC library. Strong data-typing assures that only valid\nconnections between modules can be made. The ProC's\nfeature set includes typical control elements (e.g. loops),\na fork/join mechanism, and specialised \"sampling\" elements for the investigation of high-dimensional parameter\nspaces via various algorithms. These elements permit usercontrolled parallel execution of the same program segment\non different data.\nWithin AstroGrid-D the ProC was grid-enabled with\nthe help of the Grid Application Toolkit (GAT, cf. subsection 3.1.4). In sample runs we used 200 compute elements simultaneously on a remote grid node. The need\nto deploy non-portable scientific code to a large number\nof grid nodes entailed the development of a comprehensive\npackage of environment modules.\nUpon request the ProC package is available free of\ncharge for scientific computing purposes.\n3. The AstroGrid-D Services\nIn this section we describe the architecture of our grid\nimplementation and explain the role of several of its components and services.\nWe decided to base the astrophysical community grid\non a recent version of Globus Toolkit (GT4) as a most\nwidespread and advanced middleware solution. However,\ngrid middleware capabilities are only generic functions and\nneed enhancements to be of actual use. In more general\nterms the middleware serves as an abstraction layer or\ntranslation interface. It connects the resource (the individual hardware and its operating system) with the grid resource API (application programming interface) and with\na set of uniform commands and applications, called the\nmiddleware API. The last interface is the one presented to\nthe grid users and grid applications. An operational grid\nthus in some ways resembles a nonlocal operating system\n7\n\n\fwith enhanced capabilities, such as distributed storage or\naccess to connected clusters and their batch systems.\nIn a second step we then modified or added architecture\nelements as necessary for Astronomical applications. The\nresult is shown in Fig. 6. At the resource level we find\ncompute elements (CE), storage elements (SE), and instruments. While compute and storage elements are common to all grids and can be properly managed by the basic middleware, the inclusion of instruments (e.g. robotic\ntelescopes) is one of the additions made by AstroGrid-D.\nAnother addition is AstroGrid-D's central information service Stellaris (3.2.1) which stores metadata of components,\nservices and data (yellow block in Fig. 6).\nWe further extended the middleware capabilities for job\nand file management (green block in Fig. 6) by adding\ndata stream management (3.2.3). Other components were\nenhanced: Monitoring and steering were attached to the\nStellaris information service (blue block in Fig. 6). With\nour Virtual Organisation management we achieved user\nand group management based on the GT4 security layer\n(red block in Fig. 6), to implement a grid that can easily\nbe used by collaborations to share access rights and data.\n\nkey encryption, for this purpose. At least one Grid Certification Authority per country provides such certificates\nfor users, resources and services. With this certificate it\nis possible to log onto other grid resources from any grid\nenabled workstation.\nThe following subsections describe some details of grid\nand resource work. The subsection about VOrg Management shows how the collaborative concept of virtual organisations and the security layer are tied together. Then, a\nbrief overview of the procedures for integrating a resource\ninto the grid is provided. The last paragraphs introduce\ndifferent interfaces provided by AstroGrid-D.\n3.1.1. VO Management\nVirtual Organisations (VOrgs, often somewhat confusingly called VO's) are a central element of any grid. In\nsome aspects they are the grid representation of the more\nfamiliar \"group\" concept of an operating system. A VOrg\nis formed by any number of users with a common intention\nto share resources, data and access rights in a grid.\nIn AstroGrid-D any user is authenticated by an individual X.509 certificate. However, the certificate itself does\nnot allow access to resources of AstroGrid-D or D-Grid,\nsince that right is restricted to members of our main VOrg\n\"AstroGrid-D\". Thus each user must also register for membership to this VOrg.\nTo improve the registration process and administer the\nmembers, AstroGrid-D uses a service written by Fermilab,\nthe Virtual Organisation Membership Registration Service,\n(VOMRS, WebLinks (2010)). The registration service itself is only accessible with the user's certificate installed\nin the web browser. During the registration process some\nof the user's work details are collected, such as name and\ninstitution. The user also has to choose which of the available VOrgs he wants to belong to. Upon verification by\nthe user's institute, the VOrg Administrator will grant the\nmembership status.\nAdditionally to the main VOrg, in AstroGrid-D currently four smaller VOrgs exist. These Sub-Organisations\nare used by specific institutes for internal grids, for our\nrobotic telescope resources and our collaboration with\nGAVO.\nTo connect the VOrg member database of AstroGrid-D\nwith each resource, we developed a separate service. At\neach resource this service regularly queries the central\nVOrg database for changes, and the resulting user list is\napplied to the resource's local access management. When\nan accepted VOrg member then logs on to the resource\nand is properly authenticated by the Globus Toolkit, he is\nmapped to an individual, local UNIX user account.\nOur extension to the VOMRS offers a number of options\nfor local resource administrators, e.g. to import only specific VOrgs or white- or blacklist single users. The system\nalso supports OGSA-DAI (see Section 3.2.3) and Unicore\nuser formats and cluster options. Individuals who change\ntheir \"distinguished name\" string, e.g. due to a change\nof institution, can be mapped back to their former grid\n\nFigure 6: Sketch of AstroGrid-D architecture, showing the layers\nand services that are involved in a grid application, and the pathes\nof interaction.\n\nFig. 6 only illustrates the architecture components. Not\nshown is the underlying, interconnecting network and the\nsecurity layer.\n3.1. Working with AstroGrid-D Resources\nEach system with built-in security requires the user and\neven services (hosts, databases etc.) to authenticate themselves. The grid uses X509-certificates, i.e. public/private\n8\n\n\faccount. Even if there is in general no guarantee for user\ndata to persist in the grid, it is often useful to re-gain an\nexisting environment of local settings and libraries.\nAstroGrid-D established the VORMS based solution in\n2006. Since then it was stable in operation, managing the\nabout 100 users of AstroGrid-D. The successful concept\nwas then also adopted by the German D-Grid where it\nbecame the standard form of user management.\n\nis displayed. The information is obtained via SPARQL\nqueries from (Stellaris, WebLinks (2010)), after it has\nbeen extracted from MDS, converted into RDF and uploaded to Stellaris. The Resource Map can be accessed at\nAGResourceMap, (WebLinks, 2010)). The software can\nbe obtained from the AstroGrid-D web page.\nJob monitoring is based on globus' audit logging. Audit\nlogging writes job status information into a database. This\ninformation is translated into RDF/XML and transferred\nto Stellaris.\nThe AstroGrid-D timeline was developed as a plain user\ninterface to job information. It is based on the (simile\ntimeline WebLinks (2010)). Jobs are represented by horizontal lines of length proportional to the job duration. A\ncolour code represents the status. For each job, additional\ninformation such as user ID and name of executable can be\ndisplayed. The search for information can be limited with\nkeywords and in the public area the details are strongly\nreduced for privacy reasons.\n\n3.1.2. Resource integration\nAstroGrid-D is currently comprised of about 20 grid resources provided by its member institutions: computer\nclusters, workstations, data storage servers, as well as a\ntelescope server.\nGerman astronomers apply for inclusion of a computer\nresource into AstroGrid-D on an individual basis; all German academic institutions are eligible by default. Resources of an Ukrainian institution have also been included\nfor collaboration.\nIdeally, to bring a resource onto the grid takes about\nfifteen hours for an experienced administrator. In practice more time may be required, due to complications in\nnetworking, retrieving certificates, and operating system\npeculiarities. Why would an institute invest that work\nand put their valuable computer resources on the grid?\nFirst, there is anyway considerable overhead for sharing\nof resources between institutions: accounts have to be set\nup, ports opened for special communications, etc. These\nproblems are solved by bringing resources onto the grid\nand using the tools and standard solutions it provides.\nSecond, on the grid, a resource has a much wider group of\nusers and can be used to full potential.\nAll steps required to bring hosts on-line as AstroGrid-D\nresources are described at (AGD-Globus, WebLinks\n(2010)).\n3.1.3. Monitoring\nIn a distributed, diverse grid environment, the monitoring of its parts and processes is of central importance for\nusers and administrators. Monitoring can in principle be\ndivided in two categories: resource and job monitoring.\nResource monitoring for compute and storage resources\nis realised in AstroGrid-D through the Monitoring and\nDiscovery System (MDS) of the Globus Toolkit. MDS is a\nsuite of web services to monitor and discover resources and\nservices on Grids. The gathered information is displayed\non the AstroGrid-D resources overview web page (MDS,\nWebLinks (2010)). An independent monitoring mechanism has been developed for telescope resources, which\nhandles telescope-specific information such as weather.\nAs a complementary interface to the resource list view,\nAstroGrid-D has developed a resource map as an advanced\nuser interface for displaying collected resource information\ntopographically. The Telescope Map in Fig. 3, discussed\nin Section 2.2.3, is a specialisation for telescopes. Both\nare based on the Google maps API. When a resource is\nselected, additional information about its load and usage\n\nFigure 7: The Timeline is an interactive user interface for displaying\nstatus, progress and general information of grid jobs. The top area\ndisplays hours, each line displaying the duration of a job and an\nidentifyer. A mouseclick opens up an information window (inset),\ndisplaying indepth information about the job. In the below areas\nthe scope of display is days and months.\n\nFurther details about monitoring can be found in (Deliv.\n5.9, WebLinks (2010)).\n3.1.4. User and Developer Interfaces\nIn AstroGrid-D there are four different ways available\nfor actual grid use. The middleware itself provides a\ncommandline interface as well as an API for software.\nThe Grid Application Tool (GAT) provides an alternative\nAPI which hides the underlying grid middleware and\nmakes its use transparent.\nAnd finally, GridSphere\nenables developers to quickly develop portlets for grid\napplications. Both GAT and GridSphere do not require\nthe installation of a grid middleware on the submission\n9\n\n\fhost, and it is also possible to use them on Windows\nmachines.\n\ning all the basic functionality, such as profile personalisation, layout customisation and administrative use.\nThe GridSphere AstroGrid-D portal offers a portlet for\nClusterfinder, and a Cactus Portlet is available at AEI.\n\nThe Globus Commandline and API\nThe AstroGrid-D resources are grid enabled by Globus\nmiddleware. They can thus be accessed via the command\nline interface of Globus. This interface allows data transfers and submission of jobs to the grid and provides many\nmore operations. For applications, Globus offers a rich\nAPI for each component of the middleware.\nThe Grid Application Toolkit (GAT, WebLinks (2010))\nis an API which offers grid access irrespective of the middleware which connects the resource to the grid. The GAT\nEngine and preliminary adaptors have been developed\nas part of the EU funded (Gridlab, WebLinks (2010)).\nWithin the AstroGrid-D project the Java implementation\nof JavaGAT is used. AstroGrid-D added adaptors for\nSGE, PBS, WS\u2013GRAM and gLite, and recently also a\nUNICORE adaptor (UNICORE 6) was contributed by the\nDGI\u20132 project. JavaGAT currently features adaptors to\nall the grid middlewares, which are used in D-Grid. JavaGAT uses the security layers of the middleware.\n\n3.2. Components of the AstroGrid-D Architecture\nThe middleware of the AstroGrid-D builds on existing\ngrid tools to integrate diverse types of resources. To accommodate the specific requirements of the AstroGrid-D\ncommunity, existing components were extended or substituted by newly developed ones. However, to let other\ncommunities benefit from these developments, we aim at\ngeneric solutions wherever possible.\nThe following subsections describe (1.) the information\nservice Stellaris, for central storage of all metadata and\nstatus information (2.) enhanced data storage capabilities\nof the grid, (3.) grid access to data sources, efficient data\ntransport and data streams, and (4.) options for job submission.\n3.2.1. Information Service\nThe goal of the AstroGrid-D information service, Stellaris (H\u00f6gqvist et al., 2007), is to provide a uniform framework for storage and querying of grid related information\nand metadata. Typical usage scenarios result in questions\nsuch as: Was data-set X already analysed with program Y\nand parameter set Z? Where is the output data from August 12th last year? Why did my last grid job fail? Who\ncreated the data producing the graph from the latest number of Science and where can I find it?\nWithin AstroGrid-D, we distinguish between four different types of metadata: (1) resource metadata describes\nproperties of the shared resources (e.g: for a telescope the\naperture, filters, ccd, capabilities), (2) activity state reflects the current and logged state of activities in the grid\nsuch as the location and characteristics of jobs and file\ntransfers (e.g. user, name of telescope, its location, start\nand end of observation, priority), (3) application metadata describes the program and its input parameters (e.g.:\nRA/Dec of the target, requested filters, etc.), and (4) scientific metadata, which includes information about the\nprovenance of data-sets which are used (science project,\ntype of data (image, table), provenance, references, etc.).\nIn order to respond to the previously stated example questions we will often need to query metadata of more than\none of the information types. Therefore, the integration of\nmetadata from many different sources is a strong requirement on the information service. We solve this problem\nby using the common metadata model (RDF, WebLinks\n(2010)) for all the information types.\nThe information system architecture in AstroGrid-D\n(see Fig. 9) consists of three main components; Stellaris,\nthe information service, data producers (applications, grid\nresources, and services) and data consumers (applications,\nservices and users). The Stellaris service itself is designed\naround two World Wide Web Consortium (W3C) standards: RDF for metadata representation and (SPARQL,\n\nFigure 8: JavaGAT Architecture\n\nThe availability of \"local adaptors\" enables the programmer to develop the application logic without a\nconnection to the grid. The developed application has\nthen access to all grid middlewares for which JavaGAT\nadaptors are available.\nGridSphere\nLike GAT, (GridSphere, WebLinks (2010)) was developed as part of Gridlab in 2002. The main goal of the portal related work focused on building a reliable, structured\nweb interface to support the European and global grid\ncommunity. A portal application can store the specifics\nof a grid job and run it from any standard Web browser.\nGridSphere is JSR 168 compliant and thus portlets running in GridSphere can run as well in other portal frameworks.\nGridSphere comes with a variety of core portlets provid10\n\n\fRDF Consumer\n(services,\napplications,\nServices\nServices\nusers,\n...)\nStorage\ninstance\nRDF\nInformation\nRDF\nInformation\nProducers\nProducers\nQueryInterface\nInterface\nQuery\nQuery Interface\n\nInformation Service Storage\nRDF\nStorage\n\nInformationManagement\nManagement\nInformation\nInformation\nInterface Management\nInterface\nInterface\n\nQuery Interface\nInformation Management\nInterface\n\ntranslate(metadata)\n\nRDF\nInformation\nInformation\nInformation\nProducers\nProducers\nProducers\n\nInformation\nInformation\nInformation\nProducers\nProducers\nProducers\n\nFigure 10: The AstroGrid-D database management. Users can access\nthe data sets both interactively and with batch jobs. The actual\nnodes on which the data sets reside is kept transparently.\n\nFigure 9: The AstroGrid-D information service framework\n\nAstroGrid-D considers it a major task to develop database\ntechnology further for building scalable data management\ninfrastructures. We are motivated by a growing number of\nusers and especially the expected data rates of forthcoming projects, such as the Panoramic Survey Telescope and\nRapid Response System (Pan-STARRS) or LOFAR.\nDue to the distributed nature of data sets and research\ngroups, using a grid-based approach is a natural choice\nfor the astrophysics community. The Open Grid Services Architecture-Data Access and Integration (OGSADAI, WebLinks (2010)) services enable the integration\nof databases in grid environments and they are part of\nthe Globus grid middleware. Therefore we chose OGSADAI to provide database data on resources within the\nAstroGrid-D and D-Grid infrastructure. Fig. 10 gives an\noverview of the AstroGrid-D database management.\nIn order to reduce the network traffic induced by\ndistributed queries on various data sources and to\nachieve load balancing within the community grid, various load balancing techniques have been tested and evaluated (Scholl et al., 2007a,b, 2009a,b).\nEspecially data-centric applications, such as the Clusterfinder use case (Section 2.2.1), benefit from the increased throughput introduced by load-balancing techniques for their database accesses (in the case of Clusterfinder to the SDSS and ROSAT databases). The\ndatabase relations have a fixed schema, which is also available via the metadata of the database system used. Data\naccess and manipulation is performed via the standardised\nquery language SQL. In future we also plan to support the\nVirtual Observatory Query Language (VOQL, formerly\nADQL, WebLinks (2010))), a specialised query language\nfor astronomical data based on SQL and an important\neffort by the International Virtual Observatory Alliance\n(IVOA).\nAnother prevalent processing model for e-Science data\nare data streams. Sensor sources (e.g., telescopes, satellites) continuously generate such data output. Due to\nthe fundamental importance of these sensors within astrophysics, we investigate efficient data stream processing\nmodels within AstroGrid-D. An important initial processing step of data streams is data filtering. Existing middleware structures do not offer such a processing model\n\nWebLinks (2010)) which is used for querying the information service. Thereby, we can benefit from existing\ntools for e.g. data integration and visualisation developed\nby the web-community at large. The (Stellaris software,\nWebLinks (2010)) was developed within the AstroGrid-D\nproject and is made available under the Apache Open\nSource license.\n3.2.2. File Management\nThe AstroGrid-D Data Management (ADM) has been\ndeveloped as a tool for distributed file management. It offers access to the user's files through the concept of a virtual file system via the command line, a web interface, or\na programming interface. Globus contains a software tool\ndenoted as Globus Replica Location Service (RLS), which\nallows to manage file replicas across the grid resources. We\nfound the latter to be somewhat difficult to use with job\nsubmission through the GridWay service to an execution\nhost, whose selection is not directly controlled by the user.\nOur ADM system delivers proper software tools to identify\nfiles and tag them with metadata independent of the original job execution environment. This is especially useful\nif the user needs to deploy data files required for job start\nand to access files after a job execution for post-processing.\nADM uses a relational database to store a unique file descriptor, i.e. a logical file identifier for each file, plus meta\ndata for each file or directory, e.g. the owner and a timestamp to log when the entry has been registered with the\nfilesystem. While file ownership and creation timestamp\nare mandatory, and ADM transparently cares for their\nmaintenance, meta data and individual files can be endowed with custom (user- defined) properties. ADM provides the command line client adm, including a C-library,\nwhich offers an easy-to-use access to the stored files. Furthermore, ADM ships with a web interface which permits\nto browse the virtual filesystem graphically.\n3.2.3. Data base access and data stream management\nAccess to databases storing observational and simulation\ndata has become an important part of daily astronomical\nwork. Depending on the various application requirements\nand data characteristics, databases store the actual raw\nmeasurements, results and / or the according metadata.\n11\n\n\fFigure 11: The AstroGrid-D data stream management. Users can\npublish and subscribe to data streams and share their stream-enabled\noperators using operator repositories. Internally, the data stream\nservices provide optimisation capabilities. An example of an operator would be a function performing a RA/DEC transformation into\nvarious coordinate systems. Another example operator would be a\nJava program listening for specific data from a data stream of an\ninstrument source.\n\nFigure 12: Flowchart of Steps to Submit NBODY Job via GridWay\n\n(yet).\nXML or XML-based protocols are the de-facto communication standard for web services and as well many astronomical IVOA protocols. Therefore, AstroGrid-D uses\nXML-based processing of data streams that are published\nby data sources and scientific applications can subscribe\nto. In order to increase the reusability of data streams for\nmultiple subscriptions, the query processing is performed\nby installing individual processing steps (operators) within\nthe grid network.\nRunning a data stream management within astrophysics\nrequires means to define and commonly share scientific\noperators based on already implemented functionality. A\nreusable operator is e.g. a chi-squared filter for configurable thresholds for quality assurement. Mobile operator\nrepositories enable researchers to provide these operators\nvia their own institution (e.g., personal web page) and\nto describe the operators with appropriate metadata in\nthe information service (Section 3.2.1). This considerably\nfacilitates collaborating researchers to discover and reuse\nsuch existing operators. Signing the operators with the\nauthor's grid certificate allows users to verify the trustworthiness of the operator's source.\nTechniques such as early filtering and early aggregation lead to good results, especially in the context of\nmulti-subscription optimisation (Kuntschke et al., 2005;\nKuntschke and Kemper, 2006a,b). The AstroGrid-D\ndata stream management (see Fig. 11) is available on all\nAstroGrid-D resources.\nBy developing data stream processing techniques for\ngrid environments, we moreover support the conversion\nfrom persistent data sets to streams. A combined, integrated processing of persistent and streaming data, as\nrequired by applications such as SED classification, is possible and results in better performance (Kuntschke et al.,\n2006).\n\nthe independently developed (GridWay WebLinks (2010))\nMetascheduler on top of the standard globus middleware\nlayer. As a metascheduler, GridWay enables large-scale,\nreliable and efficient sharing of computing resources managed by different Local Resource Management (LRM) systems, such as the Portable Batch System (PBS), the Sun\nGrid Engine (SGE), or the LSF, within a single organisation (enterprise grid) or scattered across several administrative domains. In the second case GridWay can interact\nalso with other grid middleware than Globus, such as e.g.\nUnicore or gLite. GridWay is meanwhile fully integrated\ninto the globus open source project, adheres to Globus philosophy and guidelines for collaborative development and\nso welcomes code and support contributions. GridWay has\nits own set of line mode commands, such as e.g. gwsubmit, gwstat or gwhosts to control the available resources\nand one's own jobs. GridWay can serve as a comfortable\nuser interface to the entire grid, similar in style to a local\nresource management system (LRM, queue system). Note\nthat resource informations have to be provided through\nthe Globus MDS information service and middleware to\nthe GridWay server. The LRM \"Fork\" means that single\nprocessor jobs are accepted to be started by a Unix process fork. Another LRM available is PBS (portable batch\nsystem) for parallel jobs. Fig. 12 illustrates three stages\nof a job run, for the example of an NBODY calculation\n(2.1.2). The first step is the deployment which delivers an\nXML based job description as described in Section 2.1.2.\nSuch XML jobs can be submitted through the standard\nGlobus GRAM job submission interface and middleware\nto the Gridway host rather than directly to the LRM of\nan execution host.\nGridway then receives this job through Globus and the\nGridway Job Manager acts as a broker and scheduler. It\nselects an available execution host through a matchmaking\nprocess and submits the job to it by Globus GRAM. At\npresent we have implemented a simple round robin strategy for single fork jobs; the GridWay software in principle\n\n3.2.4. Job Management\nAstroGrid-D has implemented job management through\n12\n\n\fallows to implement more complex scheduling algorithms\nincluding user defined parameters. It is always possible to\nsubmit jobs targeted to a certain resource through GridWay, though this is not the desired mode of operation.\nThe third step is the execution and postprocessing stage,\nduring which it has to be ensured that the build process\nproperly works on the target resource and that the user\nreceives the simulation results for postprocessing.\nThe two- step submission procedure with two Globus\nGRAM jobs connected by the GridWay server is denoted\nas GridGateWay. Note that it is also possible for the user\nto directly logon to the GridWay host and use it for job\nsubmission directly.\n\nin the grid. The integration of databases and data streams\nis also provided by AstroGrid-D. Special attention is paid\nto optimising techniques that guarantee good performance\nresults as well for throughput as for response time. Many\nof the services summarised above are addressed in close\ncollaboration with GAVO, whose focus is more on the side\nof the scientific user, whereas AstroGrid-D is solving the\ntechnical and infrastructural aspects.\nMost of the German community grids, except the High\nEnergy Physics community, employ the Globus Middleware. On EU level, gLite developed by EGEE is dominating all grid efforts, whereas internationally, the split is\nequal between EGEE/gLite and Globus. A lot of effort\ngoes into interoperability of these different middlewares,\nbut sometimes there still are barriers. AstroGrid-D is collaborating with both EGEE/EGI as well as the Open Science Grid (OSG).\n\n4. Summary and Outlook\nSummary\nAstroGrid-D established a nation-wide pool of compute,\ndata, and instrument resources accessible for astronomers.\nIt also integrated special hardware compute resources like\nclusters of GRAPE6 boards into the grid. The use case\nNBODY6++ shows impressively it's exploit in a grid environment. Well documented procedures explaining how to\nbring a resource into the grid are available. Authentication\nand authorisation for the use of the grid resources is managed by the Virtual Organisation. Moreover, the resources\nof AstroGrid-D were integrated in D-Grid, which in turn\nprovides access to the resources of the whole D-Grid for\nthe AstroGrid-D members. Robotic telescopes were also\nintegrated into the grid as a special hardware resource, so\nthey can be accessed like any other compute node.\nA variety of typical astronomical applications was\nbrought to the grid. We investigated simple but compute intensive task farming applications like Dynamo or\nGEO600 and showed that it is very easy to run them on\nthe grid without the need of complex reprogramming. We\nalso looked into more complex and data intensive tasks\nlike e.g. the Clusterfinder and ported them to the grid.\nThe Clusterfinder program, e.g., is now able to scan the\nentire available data for one model parameter set within\nseveral days, whereas it would need more than two years\non a single processor.\nWe developed a set of high-level services: Programmers\ncan now make use of an information service to handle meta\ndata and to monitor jobs and resources. Also, they can\nabstract from interfacing a specific grid middleware and\nuse GAT instead. Moreover, GridSphere enables a user\nfriendly grid access with any web browser. The ProC workflow engine supports the composition of scientific workflows and their parallel grid execution. Resource brokering and job scheduling is augmented in AstroGrid-D by the\nGridWay Metascheduler. Thereby more complex scheduling algorithms can be implemented. The AstroGrid-D\nData Management ADM handles file staging in combination with the job submission via GridWay. It thus provides an easy-to-use access to stored files and their replica\n\nOutlook\nThe important next step is to enlarge the community\nof grid users. For this purpose, the consulting and the\nsupport of new users has to be professionalised. We are\nable to offer considerable resources in compute power and\nstorage to the scientific community.\nThere are some infrastructure elements that we would\nlike to improve, e.g. our methods for resource brokering\nand job scheduling.\nProper and efficient handling of large amounts of data is\na key feature that the grid offers. Upcoming projects such\nas LOFAR, PanStarrs or LSST will produce immense data\nvolumes whose storage, administration, and processing can\nno longer be handled by local institutions. Moreover, this\ndata is in many cases processed in distributed, international working groups. Grid technology is an appropriate\nanswer to these new challenges. Due to the parallelisation\npotential and the security layers of the grid, administration and access can be achieved even in a complexity where\ncentral processing hits its limit. For this purpose we need a\npowerful data management component to enable handling\nfiles, data bases, and data streams in a coherent framework.\nAstroGrid-D established a solid basis to cope with these\nfuture challenges arising from forthcoming scientific needs.\nWe are looking forward to establish our solutions as a cornerstone of German e-Astronomy.\nAcknowledgments\nThis work is supported by the German Federal Ministry\nof Education and Research within the D-Grid initiative\nunder contracts 01AK804[A-G]. AIP acknowledges support by EFRE, grant No. 9053 ARI-ZAH acknowledges\nsupport of the GRACE project by Volkswagen Foundation grant No. I/80 041-043 (Project 'GRACE') and by\nthe Ministry of Science, Research and the Arts of BadenW\u00fcrttemberg (Az: 823.219-439/30 and /36). We acknowledge the special memorandum of understanding between\n13\n\n\fAstrogrid-D and the astronomical segment of Ukrainian\nAcademic GRID Network. We thank Ignacio Llorente,\nRuben Montero, and Tino V\u00e1zquez of Universidad Complutense Madrid, Spain, for help and support in installation and operation of the GridWay service.\n\np2p infrastructures. In: Proc. of the Intl. Conf. on Very Large\nData Bases (demo). Trondheim, Norway, pp. 1259\u20131262.\nMakino, J., Fukushige, T., Koga, M., Namura, K., Dec. 2003.\nGRAPE-6: Massively-Parallel Special-Purpose Computer for Astrophysical Particle Simulations. PASJ 55, 1163\u20131187.\nReinecke, M., Dolag, K., Hell, R., Bartelmann, M., Ensslin, T. A.,\nJan. 2006. A simulation pipeline for the Planck mission. AA 445,\n373\u2013373.\nScholl, T., Bauer, B., Gufler, B., Kuntschke, R., Reiser, A., Kemper, A., Mar. 2009a. Scalable community-driven data sharing in\ne-science grids. Future Generation Computer Systems 25 (3), 290\u2013\n300, http://dx.doi.org/10.1016/j.future.2008.05.006.\nScholl, T., Bauer, B., Gufler, B., Kuntschke, R., Weber, D., Reiser,\nA., Kemper, A., Sep. 2007a. HiSbase: Histogram-based P2P Main\nMemory Data Management. In: Proc. of the Intl. Conf. on Very\nLarge Data Bases (demo). Vienna, Austria, pp. 1394\u20131397.\nScholl, T., Bauer, B., M\u00fcller, J., Gufler, B., Reiser, A., Kemper, A.,\nMar. 2009b. Workload-Aware Data Partitioning in CommunityDriven Data Grids. In: Proc. of the Intl. Conf. on Extending\nDatabase Technology (EDBT). Saint-Petersburg, Russia, (to appear).\nScholl, T., Kuntschke, R., Reiser, A., Kemper, A., Dec. 2007b. Community Training: Partitioning Schemes in Good Shape for Federated Data Grids. In: Proc. of the IEEE Intl. Conf. on e-Science\nand Grid Computing. Bangalore, India, pp. 195\u2013203.\nSpringel, V., Dec. 2005. The cosmological simulation code\nGADGET-2. MNRAS 364, 1105\u20131134.\nSpringel, V., White, S. D. M., Jenkins, A., Frenk, C. S., Yoshida, N.,\nGao, L., Navarro, J., Thacker, R., Croton, D., Helly, J., Peacock,\nJ. A., Cole, S., Thomas, P., Couchman, H., Evrard, A., Colberg,\nJ., Pearce, F., Jun. 2005. Simulations of the formation, evolution\nand clustering of galaxies and quasars. Nature 435, 629\u2013636.\nSpurzem, R., Sep. 1999. Direct N-body Simulations. Journal of Computational and Applied Mathematics 109, 407\u2013432.\nSpurzem, R., Berczik, P., Berentzen, I., Merritt, D., Nakasato, N.,\nAdorf, H. M., Br\u00fcsemeister, T., Schwekendiek, P., Steinacker,\nJ., Wambsganss, J., Martinez, G. M., Lienhart, G., Kugel, A.,\nM\u00e4nner, R., Burkert, A., Naab, T., Vasquez, H., Wetzstein, M.,\nJul. 2007. From Newton to Einstein N-body dynamics in galactic nuclei and SPH using new special hardware and astrogrid-D.\nJournal of Physics Conference Series 78 (1), 012071\u2013+.\nSpurzem, R., Berentzen, I., Berczik, P., Merritt, D., Amaro-Seoane,\nP., Harfst, S., Gualandris, A., 2008. Parallelization, Special Hardware and Post-Newtonian Dynamics in Direct N - Body Simulations. In: Aarseth, S. J., Tout, C. A., Mardling, R. A. (Eds.),\nLecture Notes in Physics, Berlin Springer Verlag. Vol. 760 of Lecture Notes in Physics, Berlin Springer Verlag. pp. 377\u2013+.\nStrassmeier, K. G., Granzer, T., Weber, M., Woche, M., Andersen,\nM. I., Bartus, J., Bauer, S.-M., Dionies, F., Popow, E., Fechner, T., Hildebrandt, G., Washuettl, A., Ritter, A., Schwope, A.,\nStaude, A., Paschke, J., Stolz, P. A., Serre-Ricart, M., de la Rosa,\nT., Arnay, R., Oct. 2004. The STELLA robotic observatory. Astronomische Nachrichten 325, 527\u2013+.\nWebLinks,\n2010.\nhttp://www.astrogrid-d.org/\nproject-documents/Posters/publications/citations-na.html.\n\nReferences\nAarseth, S. J., Nov. 1999. From NBODY1 to NBODY6: The Growth\nof an Industry. PASP 111, 1333\u20131346.\nAllan, A., Hessman, F., Bischoff, K., Burgdorf, M., Cavanagh, B.,\nChristian, D., Clay, N., Dickens, R., Economou, F., Fadavi, M.,\nFraser, S., Granzer, T., Grosvenor, S., Jenness, T., Koratkar, A.,\nLehner, M., Mottram, C., Naylor, T., Saunders, E., Solomos, N.,\nSteele, I., Tuparev, G., Vestrand, T., White, R., Yost, S., Sep.\n2006. A protocol standard for heterogeneous telescope networks.\nAstronomische Nachrichten 327, 744\u2013+.\nBennett, K., Pasian, F., Sygnet, J.-F., Banday, A. J., Bartelmann,\nM., Gispert, R., Hazell, A., O'Mullane, W., Vuerli, C., Jun. 2000.\nSharing data, information, and software for the ESA Planck mission: the IDIS prototype. In: Kibrick, R. I., Wallander, A. (Eds.),\nSociety of Photo-Optical Instrumentation Engineers (SPIE) Conference Series. Vol. 4011 of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series. pp. 2\u201310.\nBerczik, P., Merritt, D., Spurzem, R., Nov. 2005. Long-Term Evolution of Massive Black Hole Binaries. II. Binary Evolution in\nLow-Density Galaxies. APJ 633, 680\u2013687.\nBerczik, P., Merritt, D., Spurzem, R., Bischof, H.-P., May 2006.\nEfficient Merger of Binary Supermassive Black Holes in Nonaxisymmetric Galaxies. APJL 642, L21\u2013L24.\nBerentzen, I., Preto, M., Berczik, P., Merritt, D., Spurzem, R.,\nApr. 2009. Binary Black Hole Merger in Galactic Nuclei: PostNewtonian Simulations. Astrophysical Journal 685, 455\u2013+.\nElstner, D., Korhonen, H., Apr. 2005. Flip-flop phenomenon: observations and theory. Astronomische Nachrichten 326, 278\u2013282.\nFoster, I. T., 2008. Service Oriented Computing. ICSOC 2008, LNCS\n5364 5364/2008, 3ff.\nFukushige, T., Makino, J., Kawai, A., Dec. 2005. GRAPE-6A: A\nSingle-Card GRAPE-6 for Parallel PC-GRAPE Cluster Systems.\nPASJ 57, 1009\u20131021.\nHarfst, S., Gualandris, A., Merritt, D., Spurzem, R., Portegies\nZwart, S., Berczik, P., Jul. 2007. Performance analysis of direct\nN-body algorithms on special-purpose supercomputers. New Astronomy 12, 357\u2013377.\nHessman, F. V., Sep. 2006. Remote Telescope Markup Language\n(RTML). Astronomische Nachrichten 327, 751\u2013+.\nH\u00f6gqvist, M., R\u00f6blitz, T., Reinefeld, A., May 2007. Stellaris: An\nRDF-based Information Service for AstroGrid-D. In: German eScience Conference. Baden-Baden, Germany.\nHurley, J. R., Aarseth, S. J., Shara, M. M., Aug. 2007. The Core\nBinary Fractions of Star Clusters from Realistic Simulations. APJ\n665, 707\u2013718.\nKuntschke, R., Kemper, A., Mar. 2006a. Data Stream Sharing. In:\nCurrent Trends in Database Technology \u2013 EDBT 2006, EDBT\n2006 Workshop PhD, DataX, IIDB, IIHA, ICSNW, QLQP, PIM,\nPaRMa, and Reactivity on the Web, Munich, Germany, March\n26-31, 2006, Revised Selected Papers. Vol. 4254 of Lecture Notes\nin Computer Science (LNCS). Springer Verlag, pp. 769\u2013788.\nKuntschke, R., Kemper, A., Nov. 2006b. Matching and Evaluation\nof Disjunctive Predicates for Data Stream Sharing. In: Proc. of\nthe ACM Intl. Conf. on Information and Knowledge Management\n(CIKM). Arlington, VA, USA, pp. 832\u2013833.\nKuntschke, R., Scholl, T., Huber, S., Kemper, A., Reiser, A.,\nAdorf, H.-M., Lemson, G., Voges, W., Dec. 2006. Grid-based Data\nStream Processing in e-Science. In: Proc. of the IEEE Intl. Conf.\non e-Science and Grid Computing. Amsterdam, The Netherlands,\np. 30.\nKuntschke, R., Stegmaier, B., Kemper, A., Reiser, A., Aug. 2005.\nStreamglobe: Processing and sharing data streams in grid-based\n\n14\n\n\f"}