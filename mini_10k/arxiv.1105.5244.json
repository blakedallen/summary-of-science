{"id": "http://arxiv.org/abs/1105.5244v1", "guidislink": true, "updated": "2011-05-26T09:59:59Z", "updated_parsed": [2011, 5, 26, 9, 59, 59, 3, 146, 0], "published": "2011-05-26T09:59:59Z", "published_parsed": [2011, 5, 26, 9, 59, 59, 3, 146, 0], "title": "Statistical Challenges of Global SUSY Fits", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1105.2066%2C1105.0640%2C1105.5402%2C1105.0526%2C1105.2251%2C1105.3281%2C1105.4111%2C1105.2919%2C1105.4248%2C1105.3587%2C1105.3911%2C1105.2024%2C1105.1117%2C1105.4090%2C1105.0222%2C1105.0164%2C1105.2085%2C1105.3106%2C1105.2592%2C1105.2798%2C1105.2641%2C1105.3662%2C1105.1389%2C1105.6360%2C1105.1473%2C1105.2129%2C1105.4771%2C1105.1708%2C1105.1413%2C1105.3507%2C1105.3162%2C1105.0086%2C1105.3134%2C1105.1658%2C1105.0227%2C1105.1785%2C1105.0738%2C1105.1830%2C1105.2118%2C1105.1500%2C1105.2403%2C1105.3208%2C1105.4686%2C1105.6200%2C1105.1766%2C1105.0380%2C1105.6073%2C1105.5244%2C1105.0445%2C1105.3937%2C1105.6149%2C1105.2377%2C1105.4477%2C1105.2748%2C1105.4585%2C1105.0020%2C1105.3446%2C1105.5573%2C1105.2045%2C1105.5204%2C1105.3854%2C1105.3906%2C1105.6241%2C1105.5612%2C1105.2988%2C1105.4878%2C1105.6156%2C1105.4160%2C1105.2874%2C1105.3315%2C1105.6286%2C1105.2749%2C1105.4303%2C1105.2709%2C1105.4670%2C1105.1075%2C1105.3439%2C1105.4373%2C1105.4150%2C1105.0690%2C1105.0594%2C1105.4595%2C1105.2724%2C1105.5525%2C1105.1938%2C1105.2554%2C1105.2398%2C1105.3734%2C1105.3516%2C1105.4063%2C1105.1303%2C1105.2506%2C1105.5980%2C1105.0743%2C1105.2962%2C1105.1068%2C1105.5978%2C1105.4530%2C1105.4978%2C1105.4759%2C1105.1177&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Statistical Challenges of Global SUSY Fits"}, "summary": "We present recent results aiming at assessing the coverage properties of\nBayesian and frequentist inference methods, as applied to the reconstruction of\nsupersymmetric parameters from simulated LHC data. We discuss the statistical\nchallenges of the reconstruction procedure, and highlight the algorithmic\ndifficulties of obtaining accurate profile likelihood estimates.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1105.2066%2C1105.0640%2C1105.5402%2C1105.0526%2C1105.2251%2C1105.3281%2C1105.4111%2C1105.2919%2C1105.4248%2C1105.3587%2C1105.3911%2C1105.2024%2C1105.1117%2C1105.4090%2C1105.0222%2C1105.0164%2C1105.2085%2C1105.3106%2C1105.2592%2C1105.2798%2C1105.2641%2C1105.3662%2C1105.1389%2C1105.6360%2C1105.1473%2C1105.2129%2C1105.4771%2C1105.1708%2C1105.1413%2C1105.3507%2C1105.3162%2C1105.0086%2C1105.3134%2C1105.1658%2C1105.0227%2C1105.1785%2C1105.0738%2C1105.1830%2C1105.2118%2C1105.1500%2C1105.2403%2C1105.3208%2C1105.4686%2C1105.6200%2C1105.1766%2C1105.0380%2C1105.6073%2C1105.5244%2C1105.0445%2C1105.3937%2C1105.6149%2C1105.2377%2C1105.4477%2C1105.2748%2C1105.4585%2C1105.0020%2C1105.3446%2C1105.5573%2C1105.2045%2C1105.5204%2C1105.3854%2C1105.3906%2C1105.6241%2C1105.5612%2C1105.2988%2C1105.4878%2C1105.6156%2C1105.4160%2C1105.2874%2C1105.3315%2C1105.6286%2C1105.2749%2C1105.4303%2C1105.2709%2C1105.4670%2C1105.1075%2C1105.3439%2C1105.4373%2C1105.4150%2C1105.0690%2C1105.0594%2C1105.4595%2C1105.2724%2C1105.5525%2C1105.1938%2C1105.2554%2C1105.2398%2C1105.3734%2C1105.3516%2C1105.4063%2C1105.1303%2C1105.2506%2C1105.5980%2C1105.0743%2C1105.2962%2C1105.1068%2C1105.5978%2C1105.4530%2C1105.4978%2C1105.4759%2C1105.1177&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present recent results aiming at assessing the coverage properties of\nBayesian and frequentist inference methods, as applied to the reconstruction of\nsupersymmetric parameters from simulated LHC data. We discuss the statistical\nchallenges of the reconstruction procedure, and highlight the algorithmic\ndifficulties of obtaining accurate profile likelihood estimates."}, "authors": ["Roberto Trotta", "Kyle Cranmer"], "author_detail": {"name": "Kyle Cranmer"}, "author": "Kyle Cranmer", "arxiv_comment": "7 pages, 3 figures. To appear in Proceedings of PHYSTAT11", "links": [{"href": "http://arxiv.org/abs/1105.5244v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1105.5244v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "hep-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "hep-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1105.5244v1", "affiliation": "NYU", "arxiv_url": "http://arxiv.org/abs/1105.5244v1", "journal_reference": null, "doi": null, "fulltext": "Statistical Challenges of Global SUSY Fits\nRoberto Trotta1 & Kyle Cranmer2\n1 Imperial College London. 2 New York University\n\narXiv:1105.5244v1 [hep-ph] 26 May 2011\n\nAbstract\nWe present recent results aiming at assessing the coverage properties of Bayesian\nand frequentist inference methods, as applied to the reconstruction of supersymmetric parameters from simulated LHC data. We discuss the statistical\nchallenges of the reconstruction procedure, and highlight the algorithmic difficulties of obtaining accurate profile likelihood estimates.\n1 Introduction\nExperiments at the Large Hadron Collider (LHC) have already started testing many models of particle\nphysics beyond the Standard Model (SM), and particular attention is being paid to the Minimal Supersymmetric SM (MSSM) and to other scenarios involving softly-broken supersymmetry (SUSY).\nIn the last few years, parameter inference methodologies have been developed, applying both Frequentist and Bayesian statistics (see e.g., [1\u20136]). While the efficiency of Markov Chain Monte Carlo\n(MCMC) techniques has allowed for a full exploration of multidimensional models, the likelihood function from present data is multimodal with many narrow features, making the exploration task with conventional MCMC methods challenging. A powerful alternative to classical MCMC has emerged in the\nform of Nested Sampling [7], a Monte Carlo method whose primary aim is the efficient calculation\nof the Bayesian evidence, or model likelihood. As a by-product, the algorithm also produces samples\nfrom the posterior distribution. Those same samples can also be used to estimate the profile likelihood.\nM ULTI N EST [8], a publicly available implementation of the nested sampling algorithm, has been shown\nto reduce the computational cost of performing Bayesian analysis typically by two orders of magnitude\nas compared with basic MCMC techniques. M ULTI N EST has been integrated in the SuperBayeS code1\nfor fast and efficient exploration of SUSY models.\nHaving implemented sophisticated statistical and scanning methods, several groups have turned\ntheir attention to evaluating the sensitivity to the choice of priors [4, 9, 10] and of scanning algorithms\n[11]. Those analyses indicate that current constraints are not strong enough to dominate the Bayesian\nposterior and that the choice of prior does influence the resulting inference. While confidence intervals\nderived from the profile likelihood or a chi-square have no formal dependence on a prior, there is a sampling artifact when the contours are extracted from samples produced from Bayesian sampling schemes,\nsuch as MCMC or M ULTI N EST [10].\nGiven the sensitivity to priors and the differences between the intervals obtained from different\nmethods, it is natural to seek out a quantitative assessment of their performance, namely their coverage:\nthe probability that an interval will contain (cover) the true value of a parameter. The defining property\nof a 95% confidence interval is that the procedure adopted for its estimation should produce intervals that\ncover the true value 95% of the time; thus, it is reasonable to check if the procedures have the properties\nthey claim. While Bayesian techniques are not designed with coverage as a goal, it is still meaningful\nto investigate their coverage properties. Moreover, the frequentist intervals obtained from the profile\nlikelihood or chi-square functions are based on asymptotic approximations and are not guaranteed to\nhave the claimed coverage properties.\nHere we report on recent studies investigating the coverage properties of both Bayesian and Frequentist procedures commonly used in the literature. We also highlight the numerical and sampling\nchallenges that have to be met in order to obtain a sufficienlty high-resolution mapping of the profile\n1\n\nAvailable from: www.superbayes.org\n\n\flikelihood when adopting Bayesian algorithms (which are typically designed to map out the posterior\nmass, instead).\nFor the sake of example, we consider in the following the so-called mSUGRA or Constrained Minimal Supersymmetric Standard Model (CMSSM), a model with fairly strong universality assumptions\nregarding the SUSY breaking parameters, which reduce the number of free parameters to be estimated\nto just five, denoted by the symbol \u0398: common scalar (m0 ), gaugino (m1/2 ) and tri\u2013linear (A0 ) mass\nparameters (all specified at the GUT scale) plus the ratio of Higgs vacuum expectation values tan \u03b2 and\nsign(\u03bc), where \u03bc is the Higgs/higgsino mass parameter whose square is computed from the conditions\nof radiative electroweak symmetry breaking (EWSB).\n2 Coverage study of the CMSSM\n2.1\n\nAccelerated inference from neural networks\n\nCoverage studies require extensive computational expenditure, which would be unfeasible with standard\nanalysis techniques. Therefore, in Ref. [12] a class of machine learning devices called Artificial Neural\nNetworks (ANNs) was used to approximate the most computationally intensive sections of the analysis\npipeline.\nInference on the parameters of interest \u0398 requires relating them to observable quantities, such as\nthe sparticle mass spectrum at the LHC, denoted by m, over which the likelihood is defined. This is\nachieved by evolving numerically the Renormalization Group Equations (RGEs) using publicly available codes, which is however a computationally intensive procedure. One can view the RGEs simply\nas a mapping from \u0398 \u2192 m, and attempt to engineer a computationally efficient representation of the\nfunction. In [12], an adequate solution was provided by a three-layer perceptron, a type of feed-forward\nneural network consisting of an input layer (identified with \u0398), a hidden layer and an output layer (identified with the value of m(\u0398) that we are trying to approximate). The weight and biases defining the\nnetwork were determined via an appropriate training procedure, involving the minimization of a loss\nfunction (here, the discrepancy between the value of m(\u0398) predicted by the network and its correct\nvalue obtained by solving the RGEs) defined over a set of 4000 training samples. A number of tests on\nthe accuracy and noise of the network were performed, showing a correlation in excess of 0.9999 between the approximated value of m(\u0398) and the value obtained by solving the RGEs for an independent\nsample. A second classification network was employed to distinguish between physical and un-physical\npoints in parameter space (i.e., values of \u0398 that do not lead to physically viable solutions to the RGEs).\nThe final result of replacing the computationally expensive RGEs with the ANNs is presented in Fig. 1,\nwhich shows that the agreement between the two methods is excellent, within numerical noise. By using\nthe neural network, a speed-up factor of about 3 \u00d7 104 compared with scans using the explicit spectrum\ncalculator was observed.\n2.2\n\nCoverage results for the ATLAS benchmark\n\nWe studied the coverage properties of intervals obtained for the so-called \"SU3\" benchmark point. To\nthis end, we need the ability to generate pseudo-experiments with \u0398 fixed at the value of the benchmark.\nWe adopted a parabolic approximation of the log-likelihood function (as reported in Ref. [13]), based on\nthe measurement of edges and thresholds in the invariant mass distributions for various combinations of\nleptons and jets in final state of the selected candidate SUSY events, assuming an integrated luminosity\nof 1 fb\u22121 for ATLAS. Note that the relationship between the sparticle masses and the directly observable\nmass edges is highly non-linear, so a Gaussian is likely to be a poor approximation to the actual likelihood\nfunction. Furthermore, these edges share several sources of systematic uncertainties, such as jet and\nlepton energy scale uncertainties, which are only approximately communicated in Ref. [13]. Finally, we\nintroduce the additional simplification that the likelihood is also a multivariate Gaussian with the same\ncovariance structure. We constructed 104 pseudo-experiments and analyzed them with both MCMC\n2\n\n\fBridges et al (2011)\n\nBridges et al (2011)\n\n300\n250\n\n68%, 95% contours\nBlack: SuperBayeS pdf\nBlue: Neural Network\ntrue value\n\n50\n40\ntan\u03b2\n\n200\n\n0\n\nm (GeV)\n\n60\n\n68%, 95% contours\nBlack: SuperBayeS pdf\nBlue: Neural Network\ntrue value\n\n30\n\n150\n\n20\n100\n\n10\n50\n\n280\nm\n\n300\n(GeV)\n\n\u22122\n\n320\n\n1/2\n\n0\n2\nA0 (TeV)\n\n4\n\nFig. 1: Comparison of Bayesian posteriors obtained by solving the RGEs fully numerically (black lines, giving\n68% and 95% regions) and neural networks (blue lines and corresponding filled regions), from simulated ATLAS\ndata. The red diamond gives the true value for the benchmark point adopted. From [12].\n\n(using a Metropolis-Hastings algorithm) and M ULTI N EST. Altogether, our neural network MCMC runs\nhave performed a total of 4\u00d71010 likelihood evaluations, in a total computational effort of approximately\n2 \u00d7 104 CPU-minutes. We estimate that the solving the RGEs fully numerically would have taken\nabout 1100-CPU years, which is at the boundary of what is feasible today, even with a massive parallel\ncomputing effort.\nThe results are shown in Fig. 2, where it can be seen that the methods have substantial overcoverage for 1-d intervals, which means that the resulting inferences are conservative. While it is difficult\nto unambiguously attribute the over-coverage to a specific cause, the most likely cause is the effect of\nboundary conditions imposed by the CMSSM. When \u0398 is composed of parameters of interest, \u03b8, and\nnuisance parameters, \u03c8, the profile likelihood ratio is defined as\n\u03bb(\u03b8) \u2261\n\n\u02c6\nL(\u03b8, \u03c8\u0302)\nL(\u03b8\u0302, \u03c8\u0302)\n\n.\n\n(1)\n\n\u02c6\nwhere \u03c8\u0302 is the conditional maximum likelihood estimate (MLE) of \u03c8 with \u03b8 fixed and \u03b8\u0302, \u03c8\u0302 are the\nunconditional MLEs. When the fit is performed directly in the space of the weak-scale masses (i.e.,\nwithout invoking a specific SUSY model and hence bypassing the mapping \u0398 \u2192 m), there are no\nboundary effects, and the distribution of \u22122 ln \u03bb(m) (when m is true) is distributed as a chi-square with\na number of degrees of freedom given by the dimensionality of m. Since the likelihood is invariant under\nreparametrizations, we expect \u22122 ln \u03bb(\u03b8) to also be distributed as a chi-square. If the boundary is such\n\u02c6 6= m\u0302,\n\u02c6 then the resulting interval will modified. More importantly, one\nthat m(\u03b8\u0302, \u03c8\u0302) 6= m\u0302 or m(\u03b8, \u03c8\u0302)\nexpects the denominator L(\u03b8\u0302, \u03c8\u0302) < L(m\u0302) since m is unconstrained, which will lead to \u22122 ln \u03bb(\u03b8) <\n\u22122 ln \u03bb(m). In turn, this means more parameter points being included in any given contour, which leads\nto over-coverage.\nThe impact of the boundary on the distribution of the profile likelihood ratio is not insurmountable. It is not fundamentally different than several common examples in high-energy physics where an\nunconstrained MLE would lie outside of the physical parameter space. Examples include downward\n3\n\n\fBridges et al (2011)\n\n1.1\n\n0.8\n0.7\n\n0.9\n0.8\n0.7\n\n0.6\nm1/2\n\nA0\n\ntan \u03b2\n\n0.9\n0.8\n0.7\n\n0.6\nm0\n\nProfile\u2212likelihood intervals\n\n1\nCoverage\n\n0.9\n\nBridges et al (2011)\n\n1.1\n\nShortest intervals\n\n1\nCoverage\n\n1\nCoverage\n\nBridges et al (2011)\n\n1.1\n\nEqual\u2212tail intervals\n\n0.6\nm0\n\nm1/2\n\nA0\n\ntan \u03b2\n\nm0\n\nm1/2\n\nA0\n\ntan \u03b2\n\nFig. 2: Coverage for various types of intervals for the CMSSM parameters, from 104 realizations, employing MCMC for the reconstruction (each pseudo-experiment is reconstructed with 106 samples). Green/circles\n(red/squares) is for the 68% (95%) error. From [12].\n\nfluctuations in event-counting experiments when the signal rate is bounded to be non-negative. Another\ncommon example is the measurement of sines and cosines of mixing angles that are physically bounded\nbetween [\u22121, 1], though an unphysical MLE may lie outside this region. The size of this effect is related\nto the probability that the MLE is pushed to a physical boundary. If this probability can be estimated, it is\npossible to estimate a corrected threshold on \u22122 ln \u03bb. For a precise threshold with guaranteed coverage,\none must resort to a fully frequentist Neyman Construction. A similar coverage study (but without the\ncomputational advantage provided by ANNs) has been carried out for a few CMSSM benchmark points\nfor simulated data from future direct detection experiments [14]. Their findings indicate substantial\nunder-coverage for the resulting intervals, especially for certain choices of Bayesian priors. Both works\nclearly show the timeliness and importance of evaluating the coverage properties of the reconstructed\nintervals for future data sets.\n3 Challenges of profile likelihood evaluation\nFor highly non-Gaussian problems like supersymmetric parameter determination, inference can depend\nstrongly on whether one chooses to work with the posterior distribution (Bayesian) or profile likelihood\n(frequentist) [4, 10, 15]. There is a growing consensus that both the posterior and the profile likelihood\nought to be explored in order to obtain a fuller picture of the statistical constraints from present-day and\nfuture data. This begs the question of the algorithmic solutions available to reliably explore both the\nposterior and the profile likelihood in the context of SUSY phenomenology.\nThe profile likelihood ratio defined in Eq. (1) is an attractive choice as a test statistics, for under certain regularity conditions, Wilks [16] showed that the distribution of \u22122 ln \u03bb(\u03b8) converges to a\nchi-square distribution with a number of degrees of freedom given by the dimensionality of \u03b8. Clearly,\nfor any given value of \u03b8, evaluation of the profile likelihood requires solving a maximisation prob\u02c6\nlem in many dimensions to determine the conditional MLE \u03c8\u0302. While posterior samples obtained with\nM ULTI N EST have been used to estimate the profile likelihood, the accuracy of such an estimate has\nbeen questioned [11]. As mentioned above, evaluating profile likelihoods is much more challenging than\nevaluating posterior distributions. Therefore, one should not expect that a vanilla setup for M ULTI N EST\n(which is adequate for an accurate exploration of the posterior distribution) will automatically be optimal for profile likelihoods evaluation. In Ref. [17] the question of the accuracy of profile likelihood\nevaluation from M ULTI N EST was investigated in detail. We report below the main results.\nThe two most important parameters that control the parameter space exploration in M ULTI N EST\nare the number of live points nlive \u2013 which determines the resolution at which the parameter space is\n4\n\n\fglobal best-fit\nCOA best-fit\n1-\u03c3\n2-\u03c3\n\n1\n\u03bb\n\n0.8\n0.6\n0.4\n0.2\n0\n0\n\n1\n\n2\nm0 (TeV)\n\n3\n\n2\nm0 (TeV)\n\n3\n\n4\n\n0\n\n0.5\n\n1\nm1/2 (TeV)\n\n1.5\n\n1\nm1/2 (TeV)\n\n1.5\n\n2\n\n-4\n\n-3\n\n-2\n\n-1 0\n1\nA0 (TeV)\n\n2\n\n-1 0\n1\nA0 (TeV)\n\n2\n\n3\n\n4\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n40\n\n50\n\n60\n\ntan\u03b2\n\n16\n\n\u2206\u03c72\n\n12\n8\n4\n0\n0\n\n1\n\n4\n\n0\n\n0.5\n\n2\n\n-4\n\n-3\n\n-2\n\n3\n\n4\n\n10\n\n20\n\n30\ntan\u03b2\n\nFig. 3: 1-D profile likelihoods from present-day data for the CMSSM parameters normalized to the global bestfit point. The red solid and blue dotted vertical lines represent the global best-fit point (\u03c72 = 9.26, located in\nthe focus point region) and the best-fit point found in the stau co-annihilation region (\u03c72 = 11.38) respectively.\nThe upper and lower panel show the profile likelihood and \u2206\u03c72 values, respectively. Green (magenta) horizontal\nlines represent the 1\u03c3 (2\u03c3) approximate confidence intervals. M ULTI N EST was run with 20,000 live points and\ntol = 1 \u00d7 10\u22124 (a configuration deemed appropriate for profile likelihood estimation), requiring approximately 11\nmillion likelihood evaluations. From [17].\n\nexplored \u2013 and a tolerance parameter tol, which defines the termination criterion based on the accuracy\nof the evidence. Generally, a larger number of live points is necessary to explore profile likelihoods\naccurately. Moreover, setting tol to a smaller value results in M ULTI N EST gathering a larger number of\nsamples in the high likelihood regions (as termination is delayed). This is usually not necessary for the\nposterior distributions, as the prior volume occupied by high likelihood regions is usually very small and\ntherefore these regions have relatively small probability mass. For profile likelihoods, however, getting\nas close as possible to the true global maximum is crucial and therefore one should set tol to a relatively\nsmaller value. In Ref. [17] it was found that nlive = 20, 000 and tol = 1 \u00d7 10\u22124 produce a sufficiently\naccurate exploration of the profile likelihood in toy models that reproduce the most important features of\nthe CMSSM parameter space.\nIn principle, the profile likelihood does not depend on the choice of priors. However, in order\nto explore the parameter space using any Monte Carlo technique, a set of priors needs to be defined.\nDifferent choices of priors will generally lead to different regions of the parameter space to be explored\nin greater or lesser detail, according to their posterior density. As a consequence, the resulting profile\nlikelihoods might be slightly different, purely on numerical grounds. We can obtain more robust profile\nlikelihoods by simply merging samples obtained from scans with different choices of Bayesian priors.\nThis does not come at a greater computational cost, given that a responsible Bayesian analysis would estimate sensitivity to the choice of prior as well. The results of such a scan are shown in Fig. 3, which was\nobtained by tuning M ULTI N EST with the above configuration, appropriate for an accurate profile likelihood exploration, and by merging the posterior samples from two different choices of priors (see [17] for\ndetails). This high-resolution profile likelihood scan using M ULTI N EST compares favourably with the\nresults obtained by adopting a dedicated Genetic Algorithm technique [11], although at a slightly higher\ncomputational cost (a factor of \u223c 4). In general, an accurate profile likelihood evaluation was about an\norder of magnitude more computationally expensive than mapping out the Bayesian posterior.\n\n5\n\n\f4 Conclusions\nAs the LHC impinges on the most anticipated regions of SUSY parameter space, the need for statistical\ntechniques that will be able to cope with the complexity of SUSY phenomenology is greater than ever.\nAn intense effort is underway to test the accuracy of parameter inference methods, both in the Frequentist\nand the Bayesian framework. Coverage studies such as the one presented here require highly-accelerated\ninference techniques, and neural networks have been demonstrated to provide a speed-up factor of up to\n30, 000 with respect to conventional methods. A crucial improvement required for future coverage investigations is the ability to generate pseudo-experiments from an accurate description of the likelihood.\nBoth the representation of the likelihood function and the ability to generate pseudo-experiments are now\npossible with the workspace technology in RooFit/RooStats [18]. We encourage future experiments to\npublish their likelihoods using this technology. Finally, an accurate evaluation of the profile likelihood\nremains a numerically challenging task, much more so than the mapping out of the Bayesian posterior.\nParticular care needs to be taken in tuning appropriately Bayesian algorithms targeted to the exploration\nof posterior mass (rather than likelihood maximisation). We have demonstrated that the M ULTI N EST\nalgorithm can be succesfully employed for approximating the profile likelihood functions, even though it\nwas primarily designed for Bayesian analyses. In particular, it is important to use a termination criterion\nthat allows M ULTI N EST to explore high-likelihood regions to sufficient resolution.\nAcknowledgements: We would like to thank the organizers of PHYSTAT11 for a very interesting\nworkshop. We are grateful to Yashar Akrami, Jan Conrad, Joakim Edsj\u00f6, Louis Lyons and Pat Scott for\nmany useful discussions.\n\nReferences\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n\n[14]\n[15]\n[16]\n[17]\n[18]\n\nE. A. Baltz and P. Gondolo, JHEP 10 (2004) 052.\nB. C. Allanach and C. G. Lester, Phys. Rev. D73 (2006) 015013.\nR. Ruiz de Austri, R. Trotta, and L. Roszkowski, JHEP 05 (2006) 002.\nB. C. Allanach, K. Cranmer, C. G. Lester, and A. M. Weber, JHEP 0708 (2007) 023.\nO. Buchmueller, R. Cavanaugh, A. De Roeck, J. R. Ellis, H. Flacher, S. Heinemeyer, G. Isidori,\nK. A. Olive et al., Eur. Phys. J. C64, 391-415 (2009).\nL. Roszkowski, R. Ruiz de Austri, and R. Trotta, JHEP 07 (2007) 075.\nJ. Skilling, Nested Sampling, in American Institute of Physics Conference Series (R. Fischer,\nR. Preuss, and U. V. Toussaint, eds.), pp. 395\u2013405, (2004).\nF. Feroz, M. P. Hobson, and M. Bridges, Mon. Not. R. Astron. Soc. 398 (2009) 1601\u20131614.\nR. Lafaye, T. Plehn, M. Rauch, and D. Zerwas, European Physical Journal C 54 (2008) 617\u2013644.\nR. Trotta, F. Feroz, M. Hobson, L. Roszkowski, and R. Ruiz de Austri, JHEP 12 (2008) 24.\nY. Akrami, P. Scott, J. Edsjo, J. Conrad, and L. Bergstrom, JHEP 04 (2010) 057.\nM. Bridges, K. Cranmer, F. Feroz, M. Hobson, R. R. de Austri, R. Trotta, JHEP 1103, 012 (2011).\nThe ATLAS Collaboration: G. Aad, E. Abat, B. Abbott, J. Abdallah, A. A. Abdelalim, A. Abdesselam, O. Abdinov, B. Abi, M. Abolins, H. Abramowicz, and et al., ArXiv e-prints (Dec., 2009)\n[http://xxx.lanl.gov/abs/0901.0512].\nY. Akrami, C. Savage, P. Scott, J. Conrad, and J. Edsj\u00f6, (2010), pre-print:\n[http://xxx.lanl.gov/abs/1011.4297].\nP. Scott, J. Conrad, J. Edsj\u00f6, L. Bergstr\u00f6m, C. Farnier, and Y. Akrami, JCAP 1001, 031 (2010).\nS. Wilks, Ann. Math. Statist. 9 (1938) 60\u20132.\nF. Feroz, K. Cranmer, M. Hobson, R. Ruiz de Austri, R. Trotta, (2011), pre-print:\n[http://xxx.lanl.gov/abs/1101.3296].\nL. Moneta, K. Belasco, K. Cranmer, A. Lazzaro, D. Piparo, et al., The RooStats Project, Proceed-\n\n6\n\n\fings of Science (2010) Proceedings of the 13th International Workshop on Advanced Computing\nand Analysis Techniques in Physics Research, India, [http://xxx.lanl.gov/abs/arXiv:1009.1003].\n\n7\n\n\f"}