{"id": "http://arxiv.org/abs/0812.4581v1", "guidislink": true, "updated": "2008-12-25T00:32:45Z", "updated_parsed": [2008, 12, 25, 0, 32, 45, 3, 360, 0], "published": "2008-12-25T00:32:45Z", "published_parsed": [2008, 12, 25, 0, 32, 45, 3, 360, 0], "title": "Feature Dynamic Bayesian Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0812.3762%2C0812.3885%2C0812.3556%2C0812.2424%2C0812.2429%2C0812.0900%2C0812.1629%2C0812.1931%2C0812.2864%2C0812.4460%2C0812.4526%2C0812.0538%2C0812.1853%2C0812.4928%2C0812.2396%2C0812.2131%2C0812.4642%2C0812.2373%2C0812.1851%2C0812.5033%2C0812.4173%2C0812.2854%2C0812.1359%2C0812.1312%2C0812.5022%2C0812.0712%2C0812.4793%2C0812.2957%2C0812.4581%2C0812.0243%2C0812.1147%2C0812.0001%2C0812.3684%2C0812.4703%2C0812.1090%2C0812.0417%2C0812.1839%2C0812.3208%2C0812.2924%2C0812.0470%2C0812.3571%2C0812.4267%2C0812.0919%2C0812.3031%2C0812.2835%2C0812.4767%2C0812.4411%2C0812.2761%2C0812.0309%2C0812.3698%2C0812.0127%2C0812.1336%2C0812.3411%2C0812.4056%2C0812.3787%2C0812.5070%2C0812.3958%2C0812.4230%2C0812.2254%2C0812.4628%2C0812.3155%2C0812.1986%2C0812.3980%2C0812.0812%2C0812.0939%2C0812.4935%2C0812.3639%2C0812.1743%2C0812.4887%2C0812.4547%2C0812.3422%2C0812.1310%2C0812.0804%2C0812.4502%2C0812.4335%2C0812.1703%2C0812.1965%2C0812.0182%2C0812.3896%2C0812.0522%2C0812.4416%2C0812.2966%2C0812.2706%2C0812.2662%2C0812.4488%2C0812.2393%2C0812.4208%2C0812.4058%2C0812.4782%2C0812.0800%2C0812.3263%2C0812.1783%2C0812.2223%2C0812.3960%2C0812.3906%2C0812.0130%2C0812.0188%2C0812.2134%2C0812.2292%2C0812.3065%2C0812.2849&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Feature Dynamic Bayesian Networks"}, "summary": "Feature Markov Decision Processes (PhiMDPs) are well-suited for learning\nagents in general environments. Nevertheless, unstructured (Phi)MDPs are\nlimited to relatively simple environments. Structured MDPs like Dynamic\nBayesian Networks (DBNs) are used for large-scale real-world problems. In this\narticle I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost\ncriterion that allows to automatically extract the most relevant features from\nthe environment, leading to the \"best\" DBN representation. I discuss all\nbuilding blocks required for a complete general learning algorithm.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0812.3762%2C0812.3885%2C0812.3556%2C0812.2424%2C0812.2429%2C0812.0900%2C0812.1629%2C0812.1931%2C0812.2864%2C0812.4460%2C0812.4526%2C0812.0538%2C0812.1853%2C0812.4928%2C0812.2396%2C0812.2131%2C0812.4642%2C0812.2373%2C0812.1851%2C0812.5033%2C0812.4173%2C0812.2854%2C0812.1359%2C0812.1312%2C0812.5022%2C0812.0712%2C0812.4793%2C0812.2957%2C0812.4581%2C0812.0243%2C0812.1147%2C0812.0001%2C0812.3684%2C0812.4703%2C0812.1090%2C0812.0417%2C0812.1839%2C0812.3208%2C0812.2924%2C0812.0470%2C0812.3571%2C0812.4267%2C0812.0919%2C0812.3031%2C0812.2835%2C0812.4767%2C0812.4411%2C0812.2761%2C0812.0309%2C0812.3698%2C0812.0127%2C0812.1336%2C0812.3411%2C0812.4056%2C0812.3787%2C0812.5070%2C0812.3958%2C0812.4230%2C0812.2254%2C0812.4628%2C0812.3155%2C0812.1986%2C0812.3980%2C0812.0812%2C0812.0939%2C0812.4935%2C0812.3639%2C0812.1743%2C0812.4887%2C0812.4547%2C0812.3422%2C0812.1310%2C0812.0804%2C0812.4502%2C0812.4335%2C0812.1703%2C0812.1965%2C0812.0182%2C0812.3896%2C0812.0522%2C0812.4416%2C0812.2966%2C0812.2706%2C0812.2662%2C0812.4488%2C0812.2393%2C0812.4208%2C0812.4058%2C0812.4782%2C0812.0800%2C0812.3263%2C0812.1783%2C0812.2223%2C0812.3960%2C0812.3906%2C0812.0130%2C0812.0188%2C0812.2134%2C0812.2292%2C0812.3065%2C0812.2849&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Feature Markov Decision Processes (PhiMDPs) are well-suited for learning\nagents in general environments. Nevertheless, unstructured (Phi)MDPs are\nlimited to relatively simple environments. Structured MDPs like Dynamic\nBayesian Networks (DBNs) are used for large-scale real-world problems. In this\narticle I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost\ncriterion that allows to automatically extract the most relevant features from\nthe environment, leading to the \"best\" DBN representation. I discuss all\nbuilding blocks required for a complete general learning algorithm."}, "authors": ["Marcus Hutter"], "author_detail": {"name": "Marcus Hutter"}, "author": "Marcus Hutter", "arxiv_comment": "7 pages", "links": [{"href": "http://arxiv.org/abs/0812.4581v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0812.4581v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0812.4581v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0812.4581v1", "journal_reference": "Proc. 2nd Conf. on Artificial General Intelligence (AGI 2009)\n  pages 67-73", "doi": null, "fulltext": "Feature Dynamic Bayesian Networks\nMarcus Hutter\n\narXiv:0812.4581v1 [cs.AI] 25 Dec 2008\n\nRSISE @ ANU and SML @ NICTA\nCanberra, ACT, 0200, Australia\nmarcus@hutter1.net\nwww.hutter1.net\n\n24 December 2008\nAbstract\n\nFeature MDPs [Hut09]. Concrete real-world problems can often be modeled as MDPs. For this purpose,\na designer extracts relevant features from the history\n(e.g. position and velocity of all objects), i.e. the history\nht = a1 o1 r1 ...at\u22121 ot\u22121 rt\u22121 ot is summarized by a feature\nvector st := \u03a6(ht ). The feature vectors are regarded as\nstates of an MDP and are assumed to be (approximately)\nMarkov.\nArtificial General Intelligence (AGI) [GP07] is concerned with designing agents that perform well in a very\nlarge range of environments [LH07], including all of the\nmentioned ones above and more. In this general situation, it is not a priori clear what the useful features are.\nIndeed, any observation in the (far) past may be relevant\nin the future. A solution suggested in [Hut09] is to learn\n\u03a6 itself.\nIf \u03a6 keeps too much of the history (e.g. \u03a6(ht ) = ht ),\nthe resulting MDP is too large (infinite) and cannot be\nlearned. If \u03a6 keeps too little, the resulting state sequence\nis not Markov. The Cost criterion I develop formalizes this\ntradeoff and is minimized for the \"best\" \u03a6. At any time\nn, the best \u03a6 is the one that minimizes the Markov code\nlength of s1 ...sn and r1 ...rn . This reminds but is actually\nquite different from MDL, which minimizes model+data\ncode length [Gr\u00fc07].\n\nFeature Markov Decision Processes (\u03a6MDPs)\n[Hut09] are well-suited for learning agents in general environments. Nevertheless, unstructured\n(\u03a6)MDPs are limited to relatively simple environments. Structured MDPs like Dynamic Bayesian\nNetworks (DBNs) are used for large-scale realworld problems. In this article I extend \u03a6MDP\nto \u03a6DBN. The primary contribution is to derive a\ncost criterion that allows to automatically extract\nthe most relevant features from the environment,\nleading to the \"best\" DBN representation. I discuss all building blocks required for a complete\ngeneral learning algorithm.\nKeywords:\nReinforcement learning; dynamic\nBayesian network; structure learning; feature\nlearning; global vs. local reward; explore-exploit.\n\n1\n\nIntroduction\n\nAgents. The agent-environment setup in which an Agent\ninteracts with an Environment is a very general and prevalent framework for studying intelligent learning systems\n[RN03]. In cycles t = 1,2,3,..., the environment provides\na (regular) observation ot \u2208 O (e.g. a camera image) to\nthe agent; then the agent chooses an action at \u2208 A (e.g. a\nlimb movement); finally the environment provides a realvalued reward rt \u2208IR to the agent. The reward may be very\nscarce, e.g. just +1 (-1) for winning (losing) a chess game,\nand 0 at all other times [Hut05, Sec.6.3]. Then the next\ncycle t+1 starts. The agent's objective is to maximize his\nreward.\n\nDynamic Bayesian networks. The use of \"unstructured\" MDPs [Hut09], even our \u03a6-optimal ones, is clearly\nlimited to relatively simple tasks. Real-world problems\nare structured and can often be represented by dynamic\nBayesian networks (DBNs) with a reasonable number of\nnodes [DK89]. Bayesian networks in general and DBNs\nin particular are powerful tools for modeling and solving complex real-world problems. Advances in theory and\nEnvironments. For example, sequence prediction is con- increase in computation power constantly broaden their\ncerned with environments that do not react to the agents range of applicability [BDH99, SDL07].\nactions (e.g. a weather-forecasting \"action\") [Hut03], plan- Main contribution. The primary contribution of this\nning deals with the case where the environmental function work is to extend the \u03a6 selection principle developed in\nis known [RPPCd08], classification and regression is for [Hut09] for MDPs to the conceptually much more demandconditionally independent observations [Bis06], Markov ing DBN case. The major extra complications are approxDecision Processes (MDPs) assume that ot and rt only imating, learning and coding the rewards, the dependence\ndepend on at\u22121 and ot\u22121 [SB98], POMDPs deal with Par- of the Cost criterion on the DBN structure, learning the\ntially Observable MDPs [KLC98], and Dynamic Bayesian DBN structure, and how to store and find the optimal\nNetworks (DBNs) with structured MDPs [BDH99].\nvalue function and policy.\n1\n\n\fAlthough this article is self-contained, it is recom- The idea is that \u03a6 shall extract the \"relevant\" aspects\nof the history in the sense that \"compressed\" history\nmended to read [Hut09] first.\nsar1:n \u2261 s1 a1 r1 ...sn an rn can well be described as a sample\n2 Feature Dynamic Bayesian Networks\nfrom some MDP (S,A,T,R) = (state space, action space,\ntransition probability, reward function).\n(\u03a6DBN)\n(\u03a6) Dynamic Bayesian Networks are structured\n(\u03a6)MDPs. The state space is S = {0,1}m, and each state\ns \u2261 x \u2261 (x1 ,...,xm ) \u2208 S is interpreted as a feature vector\nx = \u03a6(h), where xi = \u03a6i (h) is the value of the ith binary\nfeature. In the following I will also refer to xi as feature\ni, although strictly speaking it is its value. Since nonbinary features can be realized as a list of binary features,\nI restrict myself to the latter.\nGiven xt\u22121 = x, I assume that the features (x1t ,...,xm\nt )=\n\u2032\nx at time t are independent, and that each x\u2032i depends\nonly on a subset of \"parent\" features ui \u2286 {x1 ,...,xm }, i.e.\nthe transition matrix has the structure\nm\nY\na\n\u2032\nPa (x\u2032i |ui ) (1)\nTxx\u2032 = P(xt = x |xt\u22121 = x, at\u22121 = a) =\n\nIn this section I recapitulate the definition of \u03a6MDP from\n[Hut09], and adapt it to DBNs. While formally a DBN\nis just a special case of an MDP, exploiting the additional\nstructure efficiently is a challenge. For generic MDPs, typical algorithms should be polynomial and can at best be\nlinear in the number of states |S|. For DBNs we want\nalgorithms that are polynomial in the number of features\nm. Such DBNs have exponentially many states (2O(m) ),\nhence the standard MDP algorithms are exponential, not\npolynomial, in m. Deriving poly-time (and poly-space!)\nalgorithms for DBNs by exploiting the additional DBN\nstructure is the challenge. The gain is that we can handle\nexponentially large structured MDPs efficiently.\nNotation. Throughout this article, log denotes the binary logarithm, and \u03b4x,y = \u03b4xy = 1 if x = y and 0 else is\nthe Kronecker symbol. I generally omit separating commas if no confusion arises, in particular in indices. For\nany z of suitable type (string,vector,set),\nI define\nS string\nP\nz = z1:l = z1 ...zl , sum z+ = j zj , union z\u2217 = j zj , and\nvector z \u2022 = (z1 ,...,zl ), where j ranges over the full range\n{1,...,l} and l = |z| is the length or dimension or size of\nz. \u1e91 denotes an estimate of z. The characteristic function 11B = 1 if B=true and 0 else. P(*) denotes a probability over states and rewards or parts thereof. I do\nnot distinguish between random variables Z and realizations z, and abbreviation P(z) := P[Z = z] never leads to\nconfusion. More specifically, m \u2208 IN denotes the number of features, i \u2208 {1,...,m} any feature, n \u2208 IN the current time, and t \u2208 {1,...,n} any time. Further, in order\nnot to get distracted at several places I gloss over initial conditions or special cases where inessential. Also\n0\u2217undefined=0\u2217infinity:=0.\n\ni=1\n\nThis defines our \u03a6DBN model. It is just a \u03a6MDP with\nspecial S and T . Explaining \u03a6DBN on an example is\neasier than staying general.\n\n3\n\n\u03a6DBN Example\n\nConsider an instantiation of the simple vacuum world\n[RN03, Sec.3.6]. There are two rooms, A and B, and a\nvacuum Robot that can observe whether the room he is\nin is Clean or Dirty; M ove to the other room, Suck, i.e.\nclean the room he is in; or do N othing. After 3 days a\nroom gets dirty again. Every clean room gives a reward\n1, but a moving or sucking robot costs and hence reduces\nthe reward by 1. Hence O={A,B}\u00d7{C,D}, A={N,S,M },\nR = {\u22121,0,1,2}, and the dynamics Env() (possible histories) is clear from the above description.\n\nDynamics as a DBN. We can model the dynamics by\na DBN as follows: The state is modeled by 3 features.\nFeature R \u2208 {A,B} stores in which room the robot is, and\n\u03a6MDP definition. A \u03a6MDP consists of a 7 tufeature A/B \u2208{0,1,2,3} remembers (capped at 3) how long\npel (O,A,R,Agent,Env,\u03a6,S) = (observation space, action\nago the robot has cleaned room A/B last time, hence S =\nspace, reward space, agent, environment, feature map,\n{0,1,2,3}\u00d7{A,B}\u00d7{0,1,2,3}. The state/feature transition\nstate space). Without much loss of generality, I assume\nis as follows:\nthat A and O are finite and R \u2286 IR. Implicitly I assume\nA to be small, while O may be huge.\nif (xR= A and a = S) then x\u2032A= 0 else x\u2032A= min{xA+1, 3};\nAgent and Env are a pair or triple of interlocking func- if (xR= B and a = S) then x\u2032B = 0 else x\u2032B = min{xB+1, 3};\ntions of the history H := (O\u00d7A\u00d7R)\u2217 \u00d7O:\nif a = M (if xR= B then x\u2032R= A else x\u2032R= B) else x\u2032R= xR ;\nEnv : H \u00d7 A \u00d7 R \u2740 O, on = Env(hn\u22121 an\u22121 rn\u22121 ),\nA DBN can be viewed as a two-layer Bayesian network\nAgent : H \u2740 A,\nan = Agent(hn ),\n[BDH99]. The dependency structure of our example is\nEnv : H \u00d7 A \u2740 R,\nrn = Env(hn an ).\ndepicted in the right diagram.\nt\nt\u22121\nEach feature consists of a (left,right)- \u2713\u270f\u2713\u270f\nwhere \u2740 indicates that mappings \u2192 might be stochasA \u2732 A\u2032\ntic. The informal goal of AI is to design an Agent() that pair of nodes, and a node i \u2208 {1,2,3 = \u2712\u2711\n\u2743\u2712\u2711\n\u271a\n\u271a\nb\non the right is connected to \u2713\u270f\n\u2713\u270f\nachieves high (expected) reward over the agent's lifetime m}={A,R,B}\n\u271a\ni\n\u2732\nall\nand\nonly\nthe\nparent\nfeatures\nu\non\nthe\nR\nin a large range of Env()ironments.\nR\u2032\n\u2712\u2711\n\u2712\u2711\n\u2769\nleft.\nThe\nreward\nis\nThe feature map \u03a6 maps histories to states\n\u2713\u270f\n\u2769\n7\u2713\u270f\n\u2769\n\u2732\nB\nB\u2032\n\u03a6 : H \u2192 S, st = \u03a6(ht ), ht = oar1:t\u22121 ot \u2208 H\nr = 11xA <3 + 11xB <3 \u2212 11a6=N\n\u2712\u2711\u2712\u2711\nx\nx\u2032\n2\n\n\fThe features map \u03a6 = (\u03a6A ,\u03a6R ,\u03a6B ) can also be written The length of the Shannon-Fano code of x1:n is just the\ndown explicitly. It depends on the actions and observa- logarithm of this expression. We alsopneed to code each\nia i\nnon-zero count nia\ntions of the last 3 time steps.\nui xi to accuracy O(1/ n u + ), which each\n1\nia\nneeds 2 log(nui + ) bits. Together this gives a complete code\nDiscussion. Note that all nodes x\u2032i can implicitly also deof length\npend on the chosen action a. The optimal policies are repX\netitions of action sequence S,N,M or S,M,N . One might\nCL(x1:n |a1:n ) =\nCL(nia\n(3)\nui \u2022 )\nA/B\nthink that binary features x\n\u2208 {C,D} are sufficient,\ni,ui ,a\nbut this would result in a POMDP (Partially Observable\nMDP), since the cleanness of room A is not observed while The rewards are more complicated.\nthe robot is in room B. That is, x\u2032 would not be a (probaa\nReward structure. Let Rxx\n\u2032 be (a model of) the obbilistic) function of x and a alone. The quaternary feature\nserved reward when action a in state x results in state\nA\nx \u2208{0,1,2,3} can easily be converted into two binary feax\u2032 . It is natural to assume that the structure of the retures, and similarly xB . The purely deterministic exama\na\nwards Rxx\n\u2032 is related to the transition structure Txx\u2032 . Inple can easily be made stochastic. For instance, Sucking\ndeed, this is not restrictive, since one can always consider\nand M oving may fail with a certain probability. Possible,\na DBN with the union of transition and reward depenbut more complicated is to model a probabilistic transidencies. Usually it is assumed that the \"global\" reward\ntion from Clean to Dirty. In the randomized versions the\nia\nis a sum of \"local\" rewards Ru\ni x\u2032i , one for each feature\nagent needs to use its observations.\ni [KP99]. For simplicity of exposition I assume that the\nlocal reward Ri only depends on the feature value x\u2032i and\n4 \u03a6DBN Coding and Evaluation\nnot on ui and a. Even this is not restrictive and actually\nI now construct a code for s1:n given a1:n , and for r1:n may be advantageous as discussed in [Hut09] for MDPs.\ngiven s1:n and a1:n , which is optimal (minimal) if s1:n r1:n So I assume\ngiven a1:n is sampled from some MDP. It constitutes our\nm\nX\na\ncost function for \u03a6 and is used to define the \u03a6 selection\nRxi \u2032i =: R(x\u2032 )\nRxx\n\u2032 =\nprinciple for DBNs. Compared to the MDP case, reward\ni=1\ncoding is more complex, and there is an extra dependence\nFor instance, in the example of Section 2, two local rewards\non the graphical structure of the DBN.\nA\nB\n\u2032\nRecall [Hut09] that a sequence z1:n with counts n = (Rx\u2032A =11x\u2032A <3 and Rx\u2032B =11x\u2032B <3 ) depend on x only, but\nR\nthe\nthird\nreward\ndepends\non\nthe\naction\n(R\n=\n\u22121\n1a6=N ).\n(n1 ,...,nm ) can within an additive constant be coded in\nOften it is assumed that the local rewards are directly\n\u2032\nCL(n) := n H(n/n) + m 2\u22121 logn if n>0 and 0 else (2) observed or known [KP99], but we neither want nor can\ndo this here: Having to specify many local rewards is an\nbits, where n = n+ = n1 +...+nm and m\u2032 = |{i : ni > 0}| \u2264 extra burden for the environment (e.g. the teacher), which\nmP\nis the number of non-empty categories, and H(p) := preferably should be avoided. In our case, it is not even\nm\n\u2212 i=1 pi logpi is the entropy of probability distribution p. possible to pre-specify a local reward for each feature, since\nThe code is optimal (within +O(1)) for all i.i.d. sources. the features \u03a6i themselves are learned by the agent and\nState/Feature Coding. Similarly to the \u03a6MDP case, are not statically available. They are agent-internal and\nwe need to code the temporal \"observed\" state=feature not part of the \u03a6DBN interface. In case multiple rewards\nsequence x1:n . I do this by a frequency estimate of the are available, they can be modeled as part of the regular\nstate/feature transition probability. (Within an additive observations o, and r only holds the overall reward. The\nconstant, MDL, MML, combinatorial, incremental, and agent must and can learn to interpret and exploit the local\nBayesian coding all lead to the same result). In the fol- rewards in o by himself.\nlowing I will drop the prime in (ui ,a,x\u2032i ) tuples and related situations if/since it does not lead to confusion. Let\nTuiai xi = {t \u2264 n : ut\u22121 = ui ,at\u22121 = a,xit = xi } be the set of\ntimes t\u2212 1 at which features that influence xi have values ui , and action is a, and which leads to feature i havia\ni+\ning value xi . Let nia\nui xi = |Tui xi | their number (n++ = n\n\u2200i). I estimate each feature probability separately by\nia\nP\u0302a (xi |ui ) = nia\nui xi /nui + . Using (1), this yields\nP\u0302(x1:n |a1:n ) =\n=\n\n... =\n\nn\nY\n\nt\u22121\nT\u0302xat\u22121\nxt =\n\nt=1\n\nexp\n\nn Y\nm\nY\n\nLearning the reward function. In analogy to the MDP\ncase for R and the\nfor T above it is tempting to\nPDBN case\n\u2032\nestimate Rxi i by r\u2032 r\u2032 n+irxi /n+i+xi but this makes no sense.\na\nFor instance if rt = 1 \u2200t, then R\u0302xi i \u2261 1, and R\u0302xx\n\u2032 \u2261 m is\na gross mis-estimation of rt \u2261 1. The localization of the\nglobal reward is somewhat more complicated. The goal is\nto choose Rx1 1 ,...,Rxmm such that rt = R(xt ) \u2200t.\nWithout loss we can set R0i \u2261 0, since we can subtract a\nconstant from each local reward and absorb them into an\noverall constant w0 . This allows us to write\n\nP\u0302at\u22121 (xit |uit\u22121 )\n\nt=1 i=1\n\u0012 ia \u0013\u0015\nnui \u2022\nia\nnu i + H\nia\nn\nui +\ni,ui ,a\n\n\u0014 X\n\nR(x) = w0 x0 + w1 x1 + ... + wm xm = w\u22a4x\nwhere wi := R1i and x0 :\u2261 1.\n3\n\n\fIn practice, the \u03a6DBN model will not be perfect, and 5 DBN Structure Learning & Updating\nan approximate solution, e.g. a least squares fit, is the best\nThis section briefly discusses minimization of (6) w.r.t. G\nwe can achieve. The square loss can be written as\ngiven \u03a6 and even briefer minimization w.r.t. \u03a6. For the\nn\nmoment\nregard \u03a6 as given and fixed.\nX\nLoss(w) :=\n(R(xt ) \u2212 rt )2 = w\u22a4Aw \u2212 2b\u22a4w + c (4)\nCost and DBN structure. For general structured local\nt=1\nia\nrewards Ru\ni x\u2032i , (3) and (5) both depend on G, and (6)\nn\nn\nn\nX\nX\nX\nrepresents\na\nnovel DBN structure learning criterion that\nAij :=\nxit xjt , bi :=\nrt xit , c :=\nrt2\nincludes the rewards.\nt=1\nt=1\nt=1\nFor our simple reward model Rxi i , (5) is independent\nNote that Aij counts the number of times feature i and j of G, hence only (3) needs to be considered. This is a\nare \"on\" (=1) simultaneously, and bi sums all rewards for standard MDL criterion, but I have not seen it used in\nwhich feature i is on. The loss is minimized for\nDBNs before. Further, the features i are independent in\nthe sense that we can search for the optimal parent sets\n\u0175 := arg min Loss(w) = A\u22121 b,\nR\u0302(x) = \u0175\u22a4x\nPai \u2286 {1,...,m} for each feature i separately.\nw\n\nComplexity of structure search. Even in this case,\nwhich involves an inversion of the (m+1)\u00d7(m+1) matrix finding the optimal DBN structure is generally hard. In\nA. For singular A we take the pseudo-inverse.\nprinciple we could rely on off-the-shelf heuristic search\nReward coding. The quadratic loss function suggests a methods for finding good G, but it is probably better to\nuse or develop some special purpose optimizer. One may\nGaussian model for the rewards:\neven restrict the space of considered graphs G to those for\nwhich (6) can be minimized w.r.t. G efficiently, as long as\n2\n2 n/2\nP(r1:n |\u0175, \u03c3) := exp(\u2212Loss(\u0175)/2\u03c3 )/(2\u03c0\u03c3 )\nthis restriction can be compensated by \"smarter\" \u03a6.\nA brute force exhaustive search algorithm for Pai is to\nMaximizing this w.r.t. the variance \u03c3 2 yields the maximum\nconsider allP\n2m subsets of {1,...,m} and select the one that\nlikelihood estimate\nia\nminimizes\nui ,a CL(nui \u2022 ). A reasonable and often emne\nployed assumption is to limit the number of parents to\n\u2212 log P(r1:n |\u0175, \u03c3\u0302) = n2 log(Loss(\u0175)) \u2212 n2 log 2\u03c0\nsome small value p, which reduces the search space size to\np\n2\nO(m\n).\nwhere \u03c3\u0302 = Loss(\u0175)/n. Given \u0175 and \u03c3\u0302 this can be reIndeed,\nsince the Cost is exponential in the maximal\ngarded as the (Shannon-Fano) code length of r1:n (there\nnumber of parents of a feature, but only linear in n, a\nare actually a few subtleties here which I gloss over). Each\n\u221a\nweight \u0175k and \u03c3\u0302 need also be coded to accuracy O(1/ n), Cost minimizing \u03a6 can usually not have more than a log1\nwhich needs (m+2) 2 logn bits total. Together this gives a arithmic number of parents, which leads to a search space\nthat is pseudo-polynomial in m.\ncomplete code of length\nHeuristic structure search. We could also replace the\n(5) well-founded criterion (3) by some heuristic. One such\nheuristic has been developed in [SDL07]. The mutual inn\nne\nlog\nn\n\u2212\nlog\n= n2 log(Loss(\u0175)) + m+2\nformation is another popular criterion for determining the\n2\n2\n2\u03c0\ndependency of two random variables, so we could add j\nas\na parent of feature i if the mutual information of xj\n\u03a6DBN evaluation and selection is similar to the MDP\n\u2032i\ncase. Let G denote the graphical structure of the DBN, and x is2above a certain threshold. Overall this takes\ni.e. the set of parents Pai \u2286 {1,...,m} of each feature i. time O(m ) to determine G. An1 MDL inspired threshold\n(Remember ui are the parent values). Similarly to the for binary random variables is 2n logn. Since the mutual\ninformation treats parents independently, T\u0302 has to be esMDP case, the cost of (\u03a6,G) on hn is defined as\ntimated accordingly, essentially as in naive Bayes classification [Lew98] with feature selection, where x\u2032i represents\nCost(\u03a6, G|hn ) := CL(x1:n |a1:n ) + CL(r1:n |x1:n , a1:n ),\ni\n(6) the class label and u are the features selected x. The\nimproved Tree-Augmented naive Bayes (TAN) classifier\nand the best (\u03a6,G) minimizes this cost.\n[FGG97] could be used to model synchronous feature dependencies (i.e. within a time slice). The Chow-Liu [CL68]\n(\u03a6best , Gbest ) := arg min{Cost(\u03a6, G|hn )}\n\u03a6,G\nminimum spanning tree algorithm allows determining G in\ntime O(m3 ). A tree becomes a forest if we employ a lower\nA general discussion why this is a good criterion can be\nthreshold for the mutual information.\nfound in [Hut09]. In the following section I mainly highlight the difference to the MDP case, in particular the \u03a6 search is even harder than structure search, and remains an art. Nevertheless the reduction of the complex\nadditional dependence on and optimization over G.\nCL(r1:n |x1:n a1:n ) =\n\n4\n\n\f(ill-defined) reinforcement learning problem to an internal feature search problem with well-defined objective is a\nclear conceptual advance.\nIn principle (but not in practice) we could consider\nthe set of all (computable) functions {\u03a6 : H \u2192 {0,1}}.\nWe then compute Cost(\u03a6|h) for every finite subset \u03a6 =\n{\u03a6i1 ,...,\u03a6im } and take the minimum (note that the order\nis irrelevant).\nMost practical search algorithms require the specification of some neighborhood function, here for \u03a6. For instance, stochastic search algorithms suggest and accept\na neighbor of \u03a6 with a probability that depends on the\nCost reduction. See [Hut09] for more details. Here I will\nonly present some very simplistic ideas for features and\nneighborhoods.\nAssume binary observations O = {0,1} and consider the\nlast m observations as features, i.e. \u03a6i (hn ) = on\u2212i+1 and\n\u03a6(hn )=(\u03a61 (hn ),...,\u03a6m (hn ))=on\u2212m+1:n . So the states are\nthe same as for \u03a6m MDP in [Hut09], but now S = {0,1}m\nis structured as m binary features. In the example here,\nm = 5 lead to a perfect \u03a6DBN. We can add a new feature\non\u2212m (m \u2740 m+1) or remove the last feature (m \u2740 m\u22121),\nwhich defines a natural neighborhood structure.\nNote that the context trees of [McC96, Hut09] are more\nflexible. To achieve this flexibility here we either have to\nuse smarter features within our framework (simply interpret s = \u03a6S (h) as a feature vector of length m = \u2308log|S|\u2309)\nor use smarter (non-tabular) estimates of Pa (xi |ui ) extending our framework (to tree dependencies).\nFor general purpose intelligent agents we clearly\nneed more powerful features. Logical expressions or\n(non)accepting Turing machines or recursive sets can map\nhistories or parts thereof into true/false or accept/reject\nor in/out, respectively, hence naturally represent binary\nfeatures. Randomly generating such expressions or programs with an appropriate bias towards simple ones is a\nuniversal feature generator that eventually finds the optimal feature map. The idea is known as Universal Search\n[Gag07].\n\n6\n\nture of the DBN. They are usually complex functions of\nthe (exponentially many) states, which cannot even be\nstored, not to mention computed [KP99]. It has been suggested that the value can often be approximated well as a\nsum of local values similarly to the rewards. Such a value\nfunction can at least be stored.\nModel-based learning. The default quality measure\nfor the approximate value is the \u03c1-weighted squared difference, where \u03c1 is the stationary distribution.\nEven for a fixed policy, value iteration does not converge to the best approximation, but usually converges to\na fixed point close to it [BT96]. Value iteration requires \u03c1\nexplicitly. Since \u03c1 is also too large to store, one has to approximate \u03c1 as well. Another problem, as pointed out in\n[KP00], is that policy iteration may not converge, since different policies have different (misleading) stationary distributions. Koller and Parr [KP00] devised algorithms for\ngeneral factored \u03c1, and Guestrin et al. [GKPV03] for maxnorm, alleviating this problem. Finally, general policies\ncannot be stored exactly, and another restriction or approximation is necessary.\nModel-free learning. Given the difficulties above, I suggest to (re)consider a very simple class of algorithms, without suggesting that it is better. The above model-based\nalgorithms exploit T\u0302 and R\u0302 directly. An alternative is to\nsample from T\u0302 and use model-free \"Temporal Difference\n(TD)\" learning algorithms based only on this internal virtual sample [SB98]. We could use TD(\u03bb) or Q-value variants with linear value function approximation.\nBeside their simplicity, another advantage is that neither the stationary distribution nor the policy needs to be\nstored or approximated. Once approximation Q\u0302\u2217 has been\nobtained, it is trivial to determine the optimal (w.r.t. Q\u0302\u2217 )\naction via an+1 = argmaxa Q\u2217a\nxn+1 for any state of interest\n(namely xn+1 ) exactly.\nExploration. Optimal actions based on approximate\nrather than exact values can lead to very poor behavior due to lack of exploration. There are polynomially\noptimal algorithms (Rmax,E3,OIM) for the explorationexploitation dilemma.\nFor model-based learning, extending E3 to DBNs is\nstraightforward, but E3 needs an oracle for planning in\na given DBN [KK99]. Recently, Strehl et al. [SDL07] accomplished the same for Rmax. They even learn the DBN\nstructure, albeit in a very simplistic way. Algorithm OIM\n[SL08], which I described in [Hut09] for MDPs, can also\nlikely be generalized to DBNs, and I can imagine a modelfree version.\n\nValue & Policy Learning in \u03a6DBN\n\nGiven an estimate \u03a6\u0302 of \u03a6best , the next step is to determine a good action for our agent. I mainly concentrate on\nthe difficulties one faces in adapting MDP algorithms and\ndiscuss state of the art DBN algorithms. Value and policy\nlearning in known finite state MDPs is easy provided one is\nsatisfied with a polynomial time algorithm. Since a DBN\nis just a special (structured) MDP, its (Q) Value function\nrespects the same Bellman equations [Hut09, Eq.(6)], and\nthe optimal policy is still given by an+1 := argmaxa Q\u2217a\nxn+1 .\nNevertheless, their solution is now a nightmare, since the 7 Incremental Updates\nstate space is exponential in the number of features. We\nAs discussed in Section 5, most search algorithms are loneed algorithms that are polynomial in the number of feacal in the sense that they produce a chain of \"slightly\"\ntures, i.e. logarithmic in the number of states.\nmodified candidate solutions, here \u03a6's. This suggests a\nValue function approximation. The first problem is potential speedup by computing quantities of interest inthat the optimal value and policy do not respect the struc- crementally.\n5\n\n\f\u2022 It may be necessary to impose and exploit structure\non the conditional probability tables P a (xi |ui ) themselves [BDH99].\n\u2022 Real-valued observations and beliefs suggest to extend the binary feature model to [0,1] interval valued\nfeatures rather than coding them binary. Since any\ncontinuous semantics that preserves the role of 0 and\n1 is acceptable, there should be an efficient way to\ngeneralize Cost and Value estimation procedures.\n\u2022 I assumed that the reward/value is linear in local rewards/values. Is this sufficient for all practical purposes? I also assumed a least squares and Gaussian\nmodel for the local rewards. There are efficient algorithms for much more flexible models. The least we\ncould do is to code w.r.t. the proper covariance A.\n\u2022 I also barely discussed synchronous (within time-slice)\ndependencies.\n\u2022 I guess \u03a6DBN will often be able to work around too\nrestrictive DBN models, by finding features \u03a6 that are\nmore compatible with the DBN and reward structure.\n\u2022 Extra edges in the DBN can improve the linear value\nfunction approximation. To give \u03a6DBN incentives to\ndo so, the Value would have to be included in the Cost\ncriterion.\n\u2022 Implicitly I assumed that the action space A is small.\nIt is possible to extend \u03a6DBN to large structured\naction spaces.\n\u2022 Apart from the \u03a6-search, all parts of \u03a6DBN seem\nto be poly-time approximable, which is satisfactory\nin theory. In practice, this needs to be improved to\nessentially linear time in n and m.\n\u2022 Developing smart \u03a6 generation and smart stochastic\nsearch algorithms for \u03a6 are the major open challenges.\n\u2022 A more Bayesian Cost criterion would be desirable: a\nlikelihood of h given \u03a6 and a prior over \u03a6 leading to a\nposterior of \u03a6 given h, or so. Monte Carlo (search) algorithms like Metropolis-Hastings could sample from\nsuch a posterior. Currently probabilities (=2\nb \u2212CL ) are\nassigned only to rewards and states, but not to observations and feature maps.\n\nCost. Computing CL(x|a) in (3) takes at most time\nO(m2k |A|), where k is the maximal number of parents\nof a feature. If we remove feature i, we can simply remove/subtract the contributions from i in the sum. If we\nadd a new feature m+1, we only need to search for the best\nparent set um+1 for this new feature, and add the corresponding code length. In practice, many transitions don't\noccur, i.e. nia\nui xi =0, so CL(x|a) can actually be computed\nmuch faster in time O(|{nia\nui xi > 0}|), and incrementally\neven faster.\nRewards. When adding a new feature, the current local\nreward estimates may not change much. If we reassign a\nfraction \u03b1 \u2264 1 of reward to the new feature xm+1 , we get\nthe following ansatz1 .\nR\u0302(x1, ..., xm+1 ) = (1\u2212\u03b1)R\u0302(x)+wm+1 xm+1 =: v\u22a4\u03c8(x)\nv := (1\u2212\u03b1, wm+1 )\u22a4, \u03c8 := (R\u0302(x), xm+1 )\u22a4\nPn\nm+1\n1\nMinimizing\n)\u2212 rt )2 w.r.t. v analogous\nt=1 (R\u0302(xt ...xt\nto (4) just requires a trivial 2\u00d72 matrix inversion. The\nminimum \u1e7d results in an initial new estimate w\u0303 = ((1\u2212\n\u03b1\u0303)\u01750 ,...,(1 \u2212 \u03b1\u0303)\u0175m ,w\u0303m+1 )\u22a4, which can be improved by\nsome first order gradient decent algorithm in time O(m),\ncompared to the exact O(m3 ) algorithm. When removing\na feature, we simply redistribute its local reward to the\nother features, e.g. uniformly, followed by improvement\nsteps that cost O(m) time.\nValue. All iteration algorithms described in Section 6 for\ncomputing (Q) Values need an initial value for V or Q. We\ncan take the estimate V\u0302 from a previous \u03a6 as an initial\nvalue for the new \u03a6. Similarly as for the rewards, we can\nredistribute a fraction of the values by solving relatively\nsmall systems of equations. The result is then used as\nan initial value for the iteration algorithms in Section 6.\nA further speedup can be obtained by using prioritized\niteration algorithms that concentrate their time on badly\nestimated parameters, which are in our case the new values\n[SB98].\nSimilarly, results from time t can be (re)used as initial\nestimates for the next cycle t+1, followed by a fast improvement step.\n\nSummary. In this work I introduced a powerful framework (\u03a6DBN) for general-purpose intelligent learning\nagents, and presented algorithms for all required building\n\u03a6DBN leaves much more questions open and room for\nblocks. The introduced cost criterion reduced the informal\nmodifications and improvements than \u03a6MDP. Here are a\nreinforcement learning problem to an internal well-defined\nfew.\nsearch for \"relevant\" features.\n\u2022 The cost function can be improved by integrating out\nthe states analogous to the \u03a6MDP case [Hut09]: The References\nlikelihood P(r1:n |a1:n ,\u00db ) is unchanged, except that [BDH99] C. Boutilier, T. Dean, and S. Hanks. Decisiontheoretic planning: Structural assumptions and com\u00db \u2261 T\u0302 R\u0302 is now estimated locally, and the complexity\nputational leverage. Journal of Artificial Intelligence\npenalty becomes 12 (M +m+2)logn, where M is (esResearch, 11:1\u201394, 1999.\nsentially) the number of non-zero counts nia\n,\nbut\nui x i\n[Bis06] C. M. Bishop. Pattern Recognition and Machine\nan efficient algorithm has yet to be found.\n\n8\n\nOutlook\n\nLearning. Springer, 2006.\n\n1 An\n\nAnsatz is an initial mathematical or physical model\nwith some free parameters to be determined subsequently.\n[http://en.wikipedia.org/wiki/Ansatz]\n\n[BT96]\n\n6\n\nD. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic\nProgramming. Athena Scientific, Belmont, MA, 1996.\n\n\f[CL68]\n\nC. K. Chow and C. N. Liu.\nApproximating\ndiscrete probability distributions with dependence\ntrees. IEEE Transactions on Information Theory, IT14(3):462\u2013467, 1968.\n\n[RN03]\n\nS. J. Russell and P. Norvig. Artificial Intelligence. A\nModern Approach. Prentice-Hall, Englewood Cliffs,\nNJ, 2nd edition, 2003.\n\n[RPPCd08] S. Ross, J. Pineau, S. Paquet, and B. Chaib-draa.\nOnline planning algorithms for POMDPs. Journal\nof Artificial Intelligence Research, 2008(32):663\u2013704,\n2008.\n\n[DK89] T. Dean and K. Kanazawa. A model for reasoning\nabout persistence and causation. Computational Intelligence, 5(3):142\u2013150, 1989.\n[FGG97] N. Friedman, D. Geiger, and M. Goldszmid. Bayesian\nnetwork classifiers. Machine Learning, 29(2):131\u2013163,\n1997.\n\n[SB98]\n\n[Gag07] M. Gaglio.\nUniversal search.\n2(11):2575, 2007.\n\n[SDL07] A. L. Strehl, C. Diuk, and M. L. Littman. Efficient structure learning in factored-state MDPs. In\nProc. 27th AAAI Conference on Artificial Intelligence, pages 645\u2013650, Vancouver, BC, 2007. AAAI\nPress.\n\nScholarpedia,\n\n[GKPV03] C. Guestrin, D. Koller, R. Parr, and S. Venkataraman. Efficient solution algorithms for factored MDPs.\nJournal of Artificial Intelligence Research (JAIR),\n19:399\u2013468, 2003.\n[GP07]\n\n[SL08]\n\nB. Goertzel and C. Pennachin, editors. Artificial General Intelligence. Springer, 2007.\n\n[Gr\u00fc07] P. D. Gr\u00fcnwald. The Minimum Description Length\nPrinciple. The MIT Press, Cambridge, 2007.\n[Hut03] M. Hutter. Optimality of universal Bayesian prediction for general loss and alphabet. Journal of Machine\nLearning Research, 4:971\u20131000, 2003.\n[Hut05] M. Hutter. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability.\nSpringer, Berlin, 2005.\n300 pages,\nhttp://www.hutter1.net/ai/uaibook.htm.\n[Hut09] M. Hutter. Feature Markov decision processes. In Artificial General Intelligence (AGI'09). Atlantis Press,\n2009.\n[KK99] M. Kearns and D. Koller. Efficient reinforcement\nlearning in factored MDPs. In Proc. 16th International Joint Conference on Artificial Intelligence\n(IJCAI-99), pages 740\u2013747, San Francisco, 1999.\nMorgan Kaufmann.\n[KLC98] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra.\nPlanning and acting in partially observable stochastic\ndomains. Artificial Intelligence, 101:99\u2013134, 1998.\n[KP99]\n\nD. Koller and R. Parr. Computing factored value\nfunctions for policies in structured MDPs,. In Proc.\n16st International Joint Conf. on Artificial Intelligence (IJCAI'99), pages 1332\u20131339, Edinburgh, 1999.\n\n[KP00]\n\nD. Koller and R. Parr. Policy iteration for factored\nMDPs. In Proc. 16th Conference on Uncertainty in\nArtificial Intelligence (UAI-00), pages 326\u2013334, San\nFrancisco, CA, 2000. Morgan Kaufmann.\n\n[Lew98] D. D. Lewis. Naive (Bayes) at forty: The independence assumption in information retrieval. In\nProc. 10th European Conference on Machine Learning (ECML'98), pages 4\u201315, Chemnitz, DE, 1998.\nSpringer.\n[LH07]\n\nS. Legg and M. Hutter. Universal intelligence: A\ndefinition of machine intelligence. Minds & Machines,\n17(4):391\u2013444, 2007.\n\n[McC96] A. K. McCallum.\nReinforcement Learning with\nSelective Perception and Hidden State. PhD thesis, Department of Computer Science, University of\nRochester, 1996.\n\n7\n\nR. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA,\n1998.\n\nI. Szita and A. L\u00f6rincz. The many faces of optimism:\na unifying approach. In Proc. 12th International Conference (ICML 2008), volume 307, Helsinki, Finland,\nJune 2008.\n\n\f"}