{"id": "http://arxiv.org/abs/1103.0996v2", "guidislink": true, "updated": "2011-11-08T06:15:51Z", "updated_parsed": [2011, 11, 8, 6, 15, 51, 1, 312, 0], "published": "2011-03-04T23:51:47Z", "published_parsed": [2011, 3, 4, 23, 51, 47, 4, 63, 0], "title": "Communication with Disturbance Constraints", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1103.1079%2C1103.2668%2C1103.5960%2C1103.2473%2C1103.2779%2C1103.0287%2C1103.1155%2C1103.4695%2C1103.5780%2C1103.3534%2C1103.3651%2C1103.1679%2C1103.3110%2C1103.6032%2C1103.1527%2C1103.5591%2C1103.2433%2C1103.4260%2C1103.6060%2C1103.0132%2C1103.1451%2C1103.4542%2C1103.0119%2C1103.1324%2C1103.2154%2C1103.5065%2C1103.4505%2C1103.0942%2C1103.4597%2C1103.1091%2C1103.0587%2C1103.0742%2C1103.5860%2C1103.2175%2C1103.3970%2C1103.6148%2C1103.6250%2C1103.5390%2C1103.4493%2C1103.5546%2C1103.3358%2C1103.4789%2C1103.1687%2C1103.2147%2C1103.3456%2C1103.0118%2C1103.5693%2C1103.0899%2C1103.0541%2C1103.0873%2C1103.5891%2C1103.5900%2C1103.4851%2C1103.3321%2C1103.3272%2C1103.1581%2C1103.6286%2C1103.2036%2C1103.2994%2C1103.5280%2C1103.5636%2C1103.3062%2C1103.5078%2C1103.0573%2C1103.2318%2C1103.3237%2C1103.4971%2C1103.3314%2C1103.2796%2C1103.0952%2C1103.2090%2C1103.5032%2C1103.3667%2C1103.4027%2C1103.1703%2C1103.1213%2C1103.3218%2C1103.0597%2C1103.0004%2C1103.4131%2C1103.5504%2C1103.0739%2C1103.5924%2C1103.5830%2C1103.0621%2C1103.1961%2C1103.1311%2C1103.4745%2C1103.3396%2C1103.2871%2C1103.0996%2C1103.2627%2C1103.3689%2C1103.1063%2C1103.3391%2C1103.5491%2C1103.0163%2C1103.3532%2C1103.5550%2C1103.5260%2C1103.3143&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Communication with Disturbance Constraints"}, "summary": "Motivated by the broadcast view of the interference channel, the new problem\nof communication with disturbance constraints is formulated. The\nrate-disturbance region is established for the single constraint case and the\noptimal encoding scheme turns out to be the same as the Han-Kobayashi scheme\nfor the two user-pair interference channel. This result is extended to the\nGaussian vector (MIMO) case. For the case of communication with two disturbance\nconstraints, inner and outer bounds on the rate-disturbance region for a\ndeterministic model are established. The inner bound is achieved by an encoding\nscheme that involves rate splitting, Marton coding, and superposition coding,\nand is shown to be optimal in several nontrivial cases. This encoding scheme\ncan be readily applied to discrete memoryless interference channels and\nmotivates a natural extension of the Han-Kobayashi scheme to more than two user\npairs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1103.1079%2C1103.2668%2C1103.5960%2C1103.2473%2C1103.2779%2C1103.0287%2C1103.1155%2C1103.4695%2C1103.5780%2C1103.3534%2C1103.3651%2C1103.1679%2C1103.3110%2C1103.6032%2C1103.1527%2C1103.5591%2C1103.2433%2C1103.4260%2C1103.6060%2C1103.0132%2C1103.1451%2C1103.4542%2C1103.0119%2C1103.1324%2C1103.2154%2C1103.5065%2C1103.4505%2C1103.0942%2C1103.4597%2C1103.1091%2C1103.0587%2C1103.0742%2C1103.5860%2C1103.2175%2C1103.3970%2C1103.6148%2C1103.6250%2C1103.5390%2C1103.4493%2C1103.5546%2C1103.3358%2C1103.4789%2C1103.1687%2C1103.2147%2C1103.3456%2C1103.0118%2C1103.5693%2C1103.0899%2C1103.0541%2C1103.0873%2C1103.5891%2C1103.5900%2C1103.4851%2C1103.3321%2C1103.3272%2C1103.1581%2C1103.6286%2C1103.2036%2C1103.2994%2C1103.5280%2C1103.5636%2C1103.3062%2C1103.5078%2C1103.0573%2C1103.2318%2C1103.3237%2C1103.4971%2C1103.3314%2C1103.2796%2C1103.0952%2C1103.2090%2C1103.5032%2C1103.3667%2C1103.4027%2C1103.1703%2C1103.1213%2C1103.3218%2C1103.0597%2C1103.0004%2C1103.4131%2C1103.5504%2C1103.0739%2C1103.5924%2C1103.5830%2C1103.0621%2C1103.1961%2C1103.1311%2C1103.4745%2C1103.3396%2C1103.2871%2C1103.0996%2C1103.2627%2C1103.3689%2C1103.1063%2C1103.3391%2C1103.5491%2C1103.0163%2C1103.3532%2C1103.5550%2C1103.5260%2C1103.3143&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Motivated by the broadcast view of the interference channel, the new problem\nof communication with disturbance constraints is formulated. The\nrate-disturbance region is established for the single constraint case and the\noptimal encoding scheme turns out to be the same as the Han-Kobayashi scheme\nfor the two user-pair interference channel. This result is extended to the\nGaussian vector (MIMO) case. For the case of communication with two disturbance\nconstraints, inner and outer bounds on the rate-disturbance region for a\ndeterministic model are established. The inner bound is achieved by an encoding\nscheme that involves rate splitting, Marton coding, and superposition coding,\nand is shown to be optimal in several nontrivial cases. This encoding scheme\ncan be readily applied to discrete memoryless interference channels and\nmotivates a natural extension of the Han-Kobayashi scheme to more than two user\npairs."}, "authors": ["Bernd Bandemer", "Abbas El Gamal"], "author_detail": {"name": "Abbas El Gamal"}, "author": "Abbas El Gamal", "arxiv_comment": "25 pages, 10 figures; added vector Gaussian case, outer bound for the\n  deterministic channel with two disturbance constraints, and a new example;\n  submitted for publication to IEEE Transactions on Information Theory", "links": [{"href": "http://arxiv.org/abs/1103.0996v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1103.0996v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1103.0996v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1103.0996v2", "journal_reference": null, "doi": null, "fulltext": "1\n\nCommunication with\nDisturbance Constraints\n\nAbstract\nMotivated by the broadcast view of the interference channel, the new problem of communication with disturbance constraints is formulated. The rate\u2013disturbance region is established\nfor the single constraint case and the optimal encoding scheme turns out to be the same as\nthe Han\u2013Kobayashi scheme for the two user-pair interference channel. This result is extended\nto the Gaussian vector (MIMO) case. For the case of communication with two disturbance\nconstraints, inner and outer bounds on the rate\u2013disturbance region for a deterministic model\nare established. The inner bound is achieved by an encoding scheme that involves rate splitting,\nMarton coding, and superposition coding, and is shown to be optimal in several nontrivial cases.\nThis encoding scheme can be readily applied to discrete memoryless interference channels and\nmotivates a natural extension of the Han\u2013Kobayashi scheme to more than two user pairs.\n\nI. I NTRODUCTION\nAlice wishes to communicate a message to Bob while causing the least disturbance to nearby\nDick, Diane, and Diego, who are not interested in the communication from Alice. Assume a\ndiscrete memoryless broadcast channel p(y, z1 , . . . , zK |x) between Alice X, Bob Y , and their\npreoccupied friends Z1 , . . . , ZK as depicted in Figure 1. We measure the disturbance at side\nreceiver Zj by the amount of undesired information rate (1/n)I(X n ; Zjn ) originating from the\nsender X, and require this rate not to exceed Rd,j in the limit. The problem is to determine the\noptimal trade-off between the message communication rate R and the disturbance rates Rd,j .\nM\n\nEncoder\n\nFigure 1.\n\nXn\n\np(y, z1 , . . . , zK |x)\n\narXiv:1103.0996v2 [cs.IT] 8 Nov 2011\n\nBernd Bandemer and Abbas El Gamal\nInformation Systems Laboratory, Stanford University,\n350 Serra Mall, Stanford, CA 94305, USA\nEmail: bandemer@stanford.edu, abbas@ee.stanford.edu\n\nYn\n\nDecoder\n\nM\u0302\n\nZ1n\n\n1\nn\nn\nn I(X ; Z1 )\n\n\u2264 Rd,1\n\nn\nZK\n\n1\nn\nn\nn I(X ; ZK )\n\n\u2264 Rd,K\n\nCommunication system with disturbance constraints.\n\nThis communication with disturbance constraints problem is motivated by the broadcast side\nof the interference channel in which each sender wishes to communicate a message only to\none of the receivers while causing the least disturbance to the other receivers. However, in this\npaper, which is an extended version of [1], we focus on studying the problem of communication\nwith disturbance constraints itself. The application of the coding scheme developed in this paper\nto deterministic interference channels with more than two user pairs is discussed in [2].\nThis work is partially supported by DARPA ITMANET. Bernd Bandemer is supported by an Eric and Illeana Benhamou\nStanford Graduate Fellowship.\n\n\f2\n\nFor a single disturbance constraint, we show that the optimal encoding scheme is rate splitting\nand superposition coding, which is the same as the Han\u2013Kobayashi scheme for the two user-pair\ninterference channel [3, 4]. This motivates us to study communication with more than one\ndisturbance constraint with the hope of finding good coding schemes for interference channels\nwith more than two user pairs. To this end, we establish inner and outer bounds on the rate\u2013\ndisturbance region for the deterministic channel model with two disturbance constraints that are\ntight in some nontrivial special cases. In the following section we provide needed definitions and\npresent an extended summary of our results. The proofs are presented in subsequent sections,\nwith some parts deferred to the Appendix.\nII. D EFINITIONS AND MAIN RESULTS\nConsider the discrete memoryless communication system with K disturbance constraints\n(henceforth referred to as DMC-K-DC) depicted in Figure 1. The channel consists of K + 2\nfinite alphabets X , Y, Zj , j \u2208 [1:K], and a collection of conditional pmfs p(y, z1 , . . . , zK |x).\nA (2nR , n) code for the DMC-K-DC consists of the message set [1:2nR ], an encoding function\nxn : [1:2nR ] \u2192 X n , and a decoding function m\u0302 : Y n \u2192 [1:2nR ]. We assume that the message\nM is uniformly distributed over [1:2nR ]. A rate\u2013disturbance tuple (R, Rd,1 , . . . , Rd,K ) \u2208 RK+1\n+\nis achievable for the DMC-K-DC if there exists a sequence of (2nR , n) codes such that\nlim P(M\u0302 6= M ) = 0,\n\nn\u2192\u221e\n\nlim sup (1/n)I(X n ; Zjn ) \u2264 Rd,j ,\n\nj \u2208 [1:K].\n\nn\u2192\u221e\n\nThe rate\u2013disturbance region R of the DMC-K-DC is the closure of the set of all achievable\ntuples (R, Rd,1 , . . . , Rd,K ).\nRemark 1. Like the message rate R, the disturbance rates Rd,j , for j \u2208 [1:K], are measured in\nunits of bits per channel use. (We use logarithms of base 2 throughout.)\nRemark 2. The measure of disturbance (1/n)I(X n ; Zjn ) can be expanded as (1/n)H(Zjn ) \u2212\n(1/n)H(Zjn | X n ). The first term is the entropy rate of the received signal Zj and is caused by\nboth the transmission itself and by noise inherent to the channel. Subtracting the second term\nseparates out the noise part. (For channels with additive white noise, e.g., the Gaussian case,\nthe second term is exactly the differential entropy of each noise sample.)\nRemark 3. Our results remain essentially true if disturbance is measured by (1/n)H(Zjn ) instead.\nIf the channel is deterministic, the two measures coincide.\nRemark 4. The disturbance constraint (1/n)I(X n ; Zjn ) \u2264 Rd,j is reminiscent of the information\nleakage rate constraint for the wiretap channel [5, 6], that is, (1/n)I(M ; Zjn ) \u2264 Rleak . Replacing\nM with X n , however, dramatically changes the problem and the optimal coding scheme. In the\nwiretap channel, the key component of the optimal encoding scheme is randomized encoding,\nwhich helps control the leakage rate (1/n)I(M ; Zjn ). Such randomization reduces the achievable\ntransmission rate for a given disturbance constraint, hence is not desirable in our setting.\nThe rate\u2013disturbance region is not known in general. In this paper we establish the following\nresults.\nA. Rate\u2013disturbance region for a single disturbance constraint\nConsider the case with a single disturbance constraint, i.e., K = 1, and relabel Z1 as Z and\nRd,1 as Rd . We fully characterize the rate\u2013disturbance region for this case.\n\n\f3\n\nTheorem 1. The rate\u2013disturbance region R of the DMC-1-DC is the set of rate pairs (R, Rd )\nsuch that\nR \u2264 I(X; Y ),\nRd \u2265 I(X; Z | U ),\nR \u2212 Rd \u2264 I(X; Y | U ) \u2212 I(X; Z | U ),\nfor some pmf p(u, x) with |U| \u2264 |X | + 1.\nLet R(U, X) be the rate region defined by the rate constraints in the theorem for a fixed\njoint pmf (U, X) \u223c p(u, x). This rate region is illustrated in Figure 2. The rate\u2013disturbance\nregion is simply the union of these regions over all p(u, x) and is convex without the need for\na time-sharing random variable.\nRd\n\nA\n\nR(U, X)\n45\u25e6\nI(X; Z|U )\n\nB\n\nR\n\nI(X; Y |U ) I(X; Y )\nFigure 2.\n\nExample of R(U, X), the constituent region of R.\n\nThe proof of Theorem 1 is given in Subsections III-A and III-B. Achievability is established\nusing rate splitting and superposition coding. Receiver Y decodes the satellite codeword while\nreceiver Z distinguishes only the cloud center. Note that this encoding scheme is identical to\nthe Han\u2013Kobayashi scheme for the two user-pair interference channel [3, 4].\nWe now consider three interesting special cases.\n1) Deterministic channel: Assume that Y and Z are deterministic functions of X. We show\nthat the rate\u2013disturbance region in Theorem 1 reduces to the following.\nCorollary 1. The rate\u2013disturbance region for the deterministic channel with one disturbance\nconstraint is the set of rate pairs (R, Rd ) such that\nR \u2264 H(Y ),\nR \u2212 Rd \u2264 H(Y | Z),\nfor some pmf p(x).\nClearly, this region is convex. Alternatively, the region can be written as the set of rate pairs\n(R, Rd ) such that\nR \u2264 H(Y | Q),\nRd \u2265 I(Y ; Z | Q),\nfor some joint pmf p(q, x) with |Q| \u2264 2. Corollary 1 and the alternative description of the\nregion are established by substituting U = Z in the region of Theorem 1 and simplifying the\nresulting region as detailed in Subsection III-C.\nRemark 5. Consider the injective deterministic interference channel with two user pairs depicted\nin Figure 3. Here, gij is a function that models the link from transmitter i to receiver j, for\n\n\f4\n\ni, j \u2208 {1, 2}. The combining functions fj are assumed to be injective in each argument. This\nsetting is a special case of the channel investigated in [7]. This can be seen by merging g11\nand f1 of Figure 3 into a function f10 that maps (X1 , Z2 ) to Y1 . Likewise, define the function\nf20 as the merger of g22 and f2 . The modified combining functions f10 and f20 are injective\nin Z2 and Z1 , respectively, and therefore satisfy the assumptions in [7]. It follows that the\nHan\u2013Kobayashi scheme where the transmitters use superposition codebooks generated according\nto p(z1 )p(x1 |z1 ) and p(z2 )p(x2 |z2 ) achieves the capacity region of the channel in Figure 3.\nOn the other hand, Corollary 1 shows that the same encoding scheme achieves the disturbanceconstrained capacity for the channels X1 \u2192 (Y10 , Z1 ) and X2 \u2192 (Y20 , Z2 ), shown as dashed\nboxes in Figure 3. Here, Y10 and Y20 are the desired receivers, and Z1 and Z2 are the side\nreceivers associated with disturbance constraints. Note that decodability of the desired messages\nat receivers Y1 and Y2 in the interference channel certainly implies decodability at Y10 and Y20\nin the channels with disturbance constraint, respectively.\nM1 \u2192 X1\n\ng11\n\nY10\n\ng12\n\nFigure 3.\n\ng22\n\nY1 \u2192 M\u03021\n\nf2\n\nY2 \u2192 M\u03022\n\nZ2\nZ1\n\ng21\nM2 \u2192 X2\n\nf1\n\nY20\n\nInjective deterministic interference channel with two user pairs.\n\nExample 1. Consider the deterministic channel depicted in Figure 4(a) and its rate\u2013disturbance\nregion in Figure 4(b). Note that rates R \u2264 1 can be achieved with zero disturbance rate by\nrestricting the transmission to input symbols {0, 1} (or {2, 3}), which map to different symbols\nat Y , but are indistinguishable at Z. On the other hand, for sufficiently large Rd , the disturbance\nconstraint becomes inactive and R is bounded only by the unconstrained capacity log(3). In\naddition to the optimal region achieved by superposition coding, the figure also shows the\nstrictly suboptimal region achieved by simple non-layered random codes.\n2) Gaussian channel: Consider the problem of communication with one disturbance constraint\nfor the Gaussian channel\nY = X + W1 ,\nZ = X + W2 ,\nwhere the noise is W1 \u223c N (0, 1) and W2 \u223c N (0, N ). Assume an average power constraint P\non the transmitted signal X.\nThe case N \u2264 1 is not interesting, since then Y is a degraded version of Z and the disturbance\nrate is simply given by the data rate R. If N > 1, Z is a degraded version of Y , and the\nrate\u2013disturbance region reduces to the following.\nCorollary 2. The rate\u2013disturbance region of the Gaussian channel with parameters P > 0 and\nN > 1 is the set of rate pairs (R, Rd ) such that\nR \u2264 C(\u03b1P ),\nRd \u2265 C(\u03b1P/N ),\nfor some \u03b1 \u2208 [0, 1], where C(x) = (1/2) log(1 + x) for x \u2265 0.\nAchievability is proved using Gaussian codes with power \u03b1P . The converse follows by\ndefining \u03b1? \u2208 [0, 1] such that R = C(\u03b1? P ) and applying the vector entropy power inequality\nto Z n = Y n + W\u03032n , where W\u03032 \u223c N (0, N \u2212 1) is the excess noise. The details are given in\n\n\f5\n\nM \u2192X\n\n{0, 1, 2, 3}\n\n0\n1\n2\n3\n\n0\n\n0\n1\n2\n3\n\n0\n1\n2\n\n{0, 1}\n\n1\n\nZ\n\n{0, 1, 2}\n\nY \u2192 M\u0302\n\n(a) Channel block diagram\n\nRd\n1.0\n0.8\n0.6\n0.4\n0.2\n\nSuperposition codebooks\nSingle-user codebooks\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\nR\n\n(b) Rate\u2013disturbance region\nFigure 4.\n\nDeterministic example with one disturbance constraint.\n\nSubsection III-D. Note that this is a degenerate form of the Han\u2013Kobayashi scheme because the\nconstraint from the multiple access side of the interference channel is not taken into consideration.\n3) Vector Gaussian channel: Now consider the vector Gaussian channel with one disturbance\nconstraint\nY = X + W1 ,\nZ = X + W2 ,\nn\n\nwhere X \u2208 R and the noise W1 \u223c N (0, K1 ) and W2 \u223c N (0, K2 ) for some positive\nsemidefinite covariance matrices K1 , K2 \u2208 Rn\u00d7n . Assume an average transmit power constraint\ntr(Kx ) \u2264 P , where Kx = E(XX T ) is the covariance matrix of X. This case is not degraded\nin general.\nTheorem 2. The rate\u2013disturbance region of the Gaussian vector channel with parameters P ,\nK1 , and K2 is the convex hull of the set of pairs (R, Rd ) such that\n|Ku + Kv + K1 |\n,\n|K1 |\n|Kv + K1 | |K2 |\nR \u2212 Rd \u2264 12 log\n,\n|Kv + K2 | |K1 |\n|Kv + K2 |\nRd \u2265 21 log\n.\n|K2 |\nR\u2264\n\n1\n2\n\nlog\n\nfor some positive semidefinite matrices Ku , Kv \u2208 Rn\u00d7n with tr(Ku + Kv ) \u2264 P .\nAchievability of this rate\u2013disturbance region is shown by applying Theorem 1. Using the\ndiscretization procedure in [8], it can be shown that the theorem continues to hold with the\npower constraint additionally applied to the set of permissible input distributions. The claimed\n\n\f6\n\nregion then follows by considering the special case where the input distribution p(u, x) is jointly\nGaussian. To prove the converse, we use an extremal inequality in [9] to show that Gaussian\ninput distributions are sufficient. The details of the proof are given in Subsection III-E.\nB. Inner and outer bounds for the deterministic channel with two disturbance constraints\nThe correspondence between optimal encoding for the channel with one disturbance constraint\nand the Han\u2013Kobayashi scheme for the interference channel suggests that the optimal coding\nscheme for K disturbance constraints may provide an efficient (if not optimal) scheme for the\ninterference channel with more than two user pairs. This is particularly the case for extensions\nof the two user-pair injective deterministic interference channel for which Han\u2013Kobayashi is\noptimal [7] (see Remark 5). As such, we restrict our attention to the deterministic version of\nthe DMC-2-DC.\nFirst, we establish the following inner bound on the rate\u2013disturbance region.\nTheorem 3 (Inner bound). The rate\u2013disturbance region R of the deterministic channel with\ntwo disturbance constraints is inner-bounded by the set of rate triples (R, Rd,1 , Rd,2 ) such that\nR \u2264 H(Y ),\n\n(1)\n\nRd,1 + Rd,2 \u2265 I(Z1 ; Z2 | U ),\n\n(2)\n\nR \u2212 Rd,1 \u2264 H(Y | Z1 , U ),\n\n(3)\n\nR \u2212 Rd,2 \u2264 H(Y | Z2 , U ),\n\n(4)\n\nR \u2212 Rd,1 \u2212 Rd,2 \u2264 H(Y | Z1 , Z2 , U ) \u2212 I(Z1 ; Z2 | U ),\n\n(5)\n\n2R \u2212 Rd,1 \u2212 Rd,2 \u2264 H(Y | Z1 , Z2 , U ) + H(Y | U )\n\u2212 I(Z1 ; Z2 | U ),\n\n(6)\n\nfor some pmf p(u, x).\nThe inner bound is convex. The expression I(Z1 ; Z2 | U ) appears in three of the inequalities.\nAs in Marton coding for the 2-receiver broadcast channel with a common message, it is\nthe penalty incurred in encoding independent messages via correlated sequences. The region\nR(U, X) defined by the inequalities in the theorm for a fixed p(u, x) is illustrated in Figure 5.\nR\n(1)\n(3)\n\nRd,2\n\n(6)\n\n(5)\n(2)\n\nFigure 5.\n\n(4)\n\nRd,1\n\nRegion R(U, X) for Theorem 3. Each face is annotated by the inequality that defines it.\n\nRemark 6. The right-hand side of condition (6) can be equivalently expressed as\nH(Y | Z1 , Z2 , U ) + H(Y | U ) \u2212 I(Z1 ; Z2 | U )\n= H(Y | Z1 , U ) + H(Y | Z2 , U ) \u2212 I(Z1 ; Z2 | U, Y ),\nThis shows that the condition is stricter than the sum of conditions (3) and (4).\nThe encoding scheme for Theorem 3 involves rate splitting, Marton coding, and superposition\ncoding. The analysis of the probability of error, however, is complicated by the fact that receiver\n\n\f7\n\nY wishes to decode all parts of the message as detailed in Subsection IV-A. Receivers Z1 and\nZ2 each observe a satellite codeword from a superposition codebook.\nRemark 7. The encoding scheme underlying the inner bound of Theorem 3 can be readily\nextended to the general (non-deterministic) DMC-2-DC.\nTo complement the inner bound, we establish the following outer bound on the rate\u2013disturbance\nregion of the deterministic channel with two disturbance constraints.\nTheorem 4 (Outer bound). If a rate triple (R, Rd,1 , Rd,2 ) is achievable for the deterministic\nchannel with two disturbance constraints, then it must satisfy the conditions\nR \u2264 H(Y | Q),\nRd,1 \u2265 I(Y ; Z1 | Q),\nRd,2 \u2265 I(Y ; Z2 | Q),\nfor some pmf p(q, x) with |Q| \u2264 3.\nThe proof of this outer bound is given in Subsection IV-B. Note that this outer bound is very\nsimilar in form to the alternative description of Corollary 1 for the single-constraint deterministic\ncase.\nThe inner bound in Theorem 3 and the outer bound in Theorem 4 coincide in some special\ncases. To discuss these, we introduce the following notation. Since all channel outputs are\nfunctions of X, they can be equivalently thought of as set partitions of the input alphabet X . Set\npartitions form a partially ordered set (poset) under the refinement relation. Since this poset is a\ncomplete lattice [10], the following concepts are well-defined. For two set partitions (functions)\nf and g, let f 4 g denote that f is a refinement of g (equivalently, g is degraded with respect\nto f ), let f \u2227 g be the intersection of the two set partitions (the function that returns both f\nand g), and let f \u2228 g denote the finest set partition of which both f and g are refinements (the\nG\u00e1cs\u2013K\u00f6rner\u2013Witsenhausen common part of f and g, cf. [11, 12]).\nThe inner bound of Theorem 3 coincides with the outer bound of Theorem 4 if Z1 or Z2 is\na degraded version of Y \u2227 (Z1 \u2228 Z2 ), i.e., if the output Y together with the common part of Z1\nand Z2 determine Z1 or Z2 completely.\nTheorem 5. The rate\u2013disturbance region R of the deterministic channel with two disturbance\nconstraints is given by the outer bound of Theorem 4 if\nY \u2227 (Z1 \u2228 Z2 ) 4 Z1 ,\n\nor\n\nY \u2227 (Z1 \u2228 Z2 ) 4 Z2 .\nThe theorem is proved by specializing Theorem 3 as detailed in Subsection IV-C. In the\ncase where Z1 or Z2 is a degraded version of Y alone, achievability follows by setting U = \u2205\nin Theorem 3. Otherwise, we let U = Z1 \u2228 Z2 . This is intuitive, since U corresponds to the\ncommon-message step in the Marton encoding scheme.\nExample 2. Consider the deterministic channel depicted in Figure 6. The desired receiver output\nY is a refinement of both side receiver outputs Z1 and Z2 , and hence, Theorem 5 applies.\nFigure 7(a) depicts the rate\u2013disturbance region, numerically approximated by evaluating each\ngrid point in a regular grid over the distributions p(x) and subsequently taking the convex\nhull. Figure 7(b) contrasts the single-constraint case (where Rd,2 is set to infinity, and thus\ninactive) with the case where both side receivers are under the same disturbance rate constraint\n(Rd,1 = Rd,2 ). As expected, imposing an additional disturbance constraint can significantly\nreduce the achievable message rate. Finally, Figure 7(c) illustrates the trade-off between the\ndisturbance rates Rd,1 and Rd,2 at the two side receivers, for a fixed data rate R.\nWe conclude this section by considering another case in which we can fully characterize the\nrate\u2013disturbance region of the deterministic channel with two disturbance constraints. If Z1 is a\n\n\f8\n\nM \u2192X\nFigure 6.\n\n0\n1\n2\n3\n\n0\n1\n2\n\n0\n1\n2\n3\n\n0\n1\n2\n\n{0, 1, 2}\n\n{0, 1, 2}\n\n{0, 1, 2, 3}\n\nZ2\n\nZ1\nY \u2192 M\u0302\n\nDeterministic channel with two disturbance constraints (Example 2).\n\ndegraded version of Z2 (or vice versa), the region R of Theorem 3 is optimal and simplifies to\nthe following.\nCorollary 3. The rate\u2013disturbance region R of the deterministic channel with two disturbance\nconstraints with Z1 4 Z2 or Z2 4 Z1 is the set of rate triples (R, Rd,1 , Rd,2 ) such that\nR \u2264 H(Y ),\nR \u2212 Rd,1 \u2264 H(Y | Z1 ),\nR \u2212 Rd,2 \u2264 H(Y | Z2 ).\nfor some pmf p(x).\nAchievability follows as a special case of Theorem 3. The encoding scheme underlying the\ntheorem carefully avoids introducing an ordering between the side receiver signals Z1 and Z2 ,\nbut such ordering is naturally given by the channel here. Consequently, the corollary follows by\nsetting the auxiliary U equal to the output at the degraded side receiver. This turns the encoding\nscheme into superposition coding with three layers. The details are given in Subsection IV-D.\nNote that the region of Corollary 3 is akin to the deterministic case with one disturbance\nconstraint in Corollary 1. In both cases, the side receiver signals need not be degraded with\nrespect to Y .\n\n\f9\n\nR\n\nRd,2\nRd,1\n(a) Rate\u2013disturbance region\n\nRd\n2.0\n\nSingle disturbance constraint\nSymmetric disturbance constraints\n\n1.5\n\n1.0\n\n0.5\n\n0.5\n\n1.0\n\n1.5\n\nR\n\n2.0\n\n(b) Single disturbance constraint (Rd,1 = Rd , Rd,2 = \u221e) and symmetric\ndisturbance constraint (Rd,1 = Rd,2 = Rd ).\n\nRd,2\n\nR = 2.0\n\n1.5\n\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\n1.1\n\n1.0\n\n0.5\n\n0.5\n\n1.0\n\n1.5\n\nRd,1\n\n(c) Contour lines of the rate\u2013disturbance region at constant rate R.\nFigure 7.\n\nRate\u2013disturbance region for Example 2.\n\n\f10\n\nIII. P ROOFS FOR A SINGLE DISTURBANCE CONSTRAINT\nA. Achievability proof of Theorem 1\nAchievability is proved as follows.\nCodebook generation. Fix a pmf p(u, x).\n1) Split the message M into two independent messages M0 and M1 with rates R0 and R1 ,\nrespectively. Hence R = R0 + R1 .\nnR0\n2) For\n], independently generate a sequence un (m0 ) according to\nQn each m0 \u2208 [1 : 2\ni=1 p(ui ).\n3) For each (m0Q\n, m1 ) \u2208 [1:2nR0 ] \u00d7 [1:2nR1 ], independently generate a sequence xn (m0 , m1 )\nn\naccording to i=1 p(xi | ui (m0 )).\nEncoding. To send message m = (m0 , m1 ), transmit xn (m0 , m1 ).\nDecoding. Upon receiving y n , find the unique (m\u03020 , m\u03021 ) such that (un (m\u03020 ), xn (m\u03020 , m\u03021 ), y n ) \u2208\n(n)\nT\u03b5 (U, X, Y ).\nAnalysis of the probability of error. We are using a superposition code over the channel from X\nto Y . Using the law of large numbers and the packing lemma in [8], it can be shown that the\nprobability of error tends to zero as n \u2192 \u221e if\nR1 < I(X; Y | U ) \u2212 \u03b4(\u03b5),\nR0 + R1 < I(X; Y ) \u2212 \u03b4(\u03b5).\n\n(7)\n(8)\n\nAnalysis of disturbance rate. We analyze the disturbance rate averaged over codebooks C.\nI(X n ; Z n | C) \u2264 H(Z n , M0 | C) \u2212 H(Z n | X n , C)\n= H(M0 ) + H(Z n | M0 , C) \u2212 H(Z n | X n )\n(a)\n\n\u2264 nR0 + H(Z n | U n ) \u2212 nH(Z | X)\n\u2264 nR0 + nH(Z | U ) \u2212 nH(Z | X, U )\n= nR0 + nI(X; Z | U )\n\u2264 nRd ,\n\n(9)\n\nwhere (a) follows since U n is a function of the codebook C and M0 . Substituting R = R0 + R1\nand using Fourier\u2013Motzkin elimination on inequalities (7), (8), and (9) completes the proof of\nachievability.\nB. Converse of Theorem 1\n(n)\n\nConsider a sequence of codes with Pe \u2192 0 as n \u2192 \u221e and the joint pmf that it induces\non (M, X n , Y n , Z n ) assuming M \u223c Unif[1:2nR ]. Define the time-sharing random variable\nn\nQ \u223c Unif[1:n], independent of everything else. We use the identification U = (Q, YQ+1\n, Z Q\u22121 ),\nand let X = XQ , Y = YQ , and Z = ZQ . Note that (X, Y, Z) is consistent with the channel.\nThen\nR \u2264 I(X; Y ) + \u03b5n ,\n\n\f11\n\nas in the converse proof for point-to-point channel capacity, which uses the same identifications\nof random variables. On the other hand,\nnRd \u2265 I(X n ; Z n )\n= H(Z n ) \u2212 H(Z n | X n )\nn\nX\n\u0001\n=\nH(Zi | Z i\u22121 ) \u2212 H(Zi | Xi )\ni=1\n\n\u2265\n\nn\nX\n\nn\nH(Zi | Z i\u22121 , Yi+1\n) \u2212 nH(Z | X)\n\ni=1\n\n= nH(Z | U ) \u2212 nH(Z | X, U )\n= nI(X; Z | U ).\nFinally,\nn(Rd \u2212 R)\n\u2265 I(X n ; Z n ) \u2212 nR\n(a)\n\n\u2265 H(Z n ) \u2212 H(Z n | X n ) \u2212 I(M ; Y n ) \u2212 n\u03b5n\nn\n\u0001\n(b) X\nn\n=\nH(Zi | Z i\u22121 ) \u2212 I(M ; Yi | Yi+1\n) \u2212 nH(Z | X) \u2212 n\u03b5n\ni=1\n\n=\n\nn\nX\n\nn\nn\nH(Zi | Z i\u22121 , Yi+1\n) + I(Yi+1\n; Zi | Z i\u22121 )\n\ni=1\n\n\u0001\nn\nn\n\u2212H(Yi | Yi+1\n) + H(Yi | M, Yi+1\n) \u2212 nH(Z | X) \u2212 n\u03b5n\nn\n(c) X\nn\nn\nH(Zi | Z i\u22121 , Yi+1\n) + I(Yi ; Z i\u22121 | Yi+1\n)\n=\ni=1\n\n\u0001\nn\n\u2212H(Yi | Yi+1\n) + H(Yi | Xi ) \u2212 nH(Z | X) \u2212 n\u03b5n\nn\nX\nn\nn\n=\nH(Zi | Z i\u22121 , Yi+1\n) \u2212 H(Yi | Z i\u22121 , Yi+1\n)\ni=1\n\n\u0001\nn\n+H(Yi | Xi , Z i\u22121 , Yi+1\n) \u2212 nH(Z | X) \u2212 n\u03b5n\nn\nX\n\u0001\nn\nn\nH(Zi | Z i\u22121 , Yi+1\n) \u2212 I(Xi ; Yi | Z i\u22121 , Yi+1\n)\n=\ni=1\n\n\u2212 nH(Z | X) \u2212 n\u03b5n\n(d)\n\n= nH(Z | U ) \u2212 nI(X; Y | U ) \u2212 nH(Z | X, U ) \u2212 n\u03b5n\n\n= nI(X; Z | U ) \u2212 I(X; Y | U ) \u2212 n\u03b5n ,\nwhere (a) uses Fano's inequality, (b) single-letterizes the noise term H(Z n | X n ) with equality\ndue to memorylessness of the channel, (c) applies Csisz\u00e1r's sum identity to the second term and\nchannel memorylessness to the fourth term, and (d) uses the previous definitions of auxiliary\nrandom variables. Finally, the cardinality bound on U is established using the convex cover\nmethod in [8].\n\n\f12\n\nC. Proof of Corollary 1\nUsing the deterministic nature of the channel, the region in Theorem 1 reduces to the set of\nrate pairs (R, Rd ) such that\nR \u2264 H(Y ),\n\n(10)\n\nRd \u2265 H(Z | U ),\n\n(11)\n\nRd \u2265 R + H(Z | U ) \u2212 H(Y | U ),\n\n(12)\n\nfor some pmf p(u, x). Now fixing a rate R and a pmf p(x) and varying p(u|x) to minimize\nRd , the right hand sides of (11) and (12) are lower bounded by\nH(Z | U ) \u2265 0,\nand\nR + H(Z | U ) \u2212 H(Y | U )\n= R + H(Z | U ) \u2212 H(Y, Z | U ) + H(Z | Y, U )\n= R \u2212 H(Y | Z, U ) + H(Z | Y, U )\n\u2265 R \u2212 H(Y | Z).\nNote that the particular choice U = Z simultaneously achieves both lower bounds with equality\nand is therefore sufficient. The rate\u2013disturbance region thus reduces to Corollary 1.\nFor a fixed pmf p(x), this region has exactly two corner points: P1 = (H(Y |Z), 0) and\nP2 = (H(Y ), I(Y ; Z)). As we vary p(x), there is one corner point P1 that dominates all other\nP1 points. The pmf p(x) for this dominant P1 can be constructed by maximizing H(Y |Z)\nas follows. For each z \u2208 Z, define Yz \u2286 Y to be the set of y symbols that are compatible\nwith z. Let z ? be a symbol that maximizes |Yz |. For each element of Yz? , pick exactly one\nx that is compatible with it and z ? . Finally, place equal probability mass on each of these\nx values, and zero mass on all others. This pmf on X yields the dominant corner point P1 ,\nnamely (log(|Yz? |), 0). Moreover, for this distribution, P2 coincides with P1 . Therefore, the\nnet contribution (modulo convexification) of each pmf p(x) to the rate\u2013disturbance region\namounts to its corner point P2 . This implies the alternative description of the region. Lastly, the\ncardinality bound on Q in the alternative description is follows from the convex cover method\nin [8].\nD. Proof of Corollary 2\nAchievability is straightforward using a random Gaussian codebook with power control, and\nupper-bounding the disturbance rate at receiver Z by white Gaussian noise. The converse can\nbe seen as follows. Clearly, R \u2264 C(P ). Let \u03b1? \u2208 [0, 1] be such that R = C(\u03b1? P ). Then\nn C(\u03b1? P ) = nR \u2264 I(X n ; Y n ) + n\u03b5n\n= h(Y n ) \u2212 h(Y n | X n ) + n\u03b5n ,\nand therefore,\nh(Y n ) \u2265\n=\n\nn\n2\nn\n2\n\nlog(2\u03c0e) + n C(\u03b1? P ) \u2212 n\u03b5n\nlog (2\u03c0e(1 + \u03b1? P )) \u2212 n\u03b5n\n\nSince N < 1, we can write the physically degraded form of the channel as Y = X + W1 ,\nZ = Y + W\u03032 , where W\u03032 \u223c N (0, N \u2212 1) is the excess noise that receiver Z experiences in\n\n\f13\n\naddition to receiver Y . Applying the vector entropy power inequality to Z n = Y n + W\u03032n , we\nconclude\n\u0010 2\n\u0011\nn\nn\n2\nn\n1\n1\nn h(Y ) + 2 n h(W\u03032 )\nh(Z\n)\n\u2265\nlog\n2\nn\n2\n\u0001\n\u2265 21 log 2\u22122\u03b5n * 2\u03c0e(1 + \u03b1? P ) + 2\u03c0e(N \u2212 1)\n\u2265\n\n1\n2\n\nlog (2\u03c0e(N + \u03b1? P )) \u2212 \u03b5n ,\n\nand finally,\nRd \u2265\n=\n\u2265\n\nn\nn\n1\nn I(X ; Z )\nn\n1\n1\nn h(Z ) \u2212 2 log(2\u03c0eN )\nC(\u03b1? P/N ) \u2212 \u03b5n .\n\nE. Proof of Theorem 2\nRecall the shape of R(U, X) depicted in Figure 2. The coordinates of the corner points A\nand B are given by\nA(U, X) :\n\nR = h(X + W1 ) \u2212 h(W1 ),\n\n(13)\n\nRd = h(X + W2 | U ) + h(X + W1 ) \u2212 h(X + W1 | U ) \u2212 h(W2 ),\nB(U, X) :\n\n(14)\n\nR = h(X + W1 | U ) \u2212 h(W1 ),\n\n(15)\n\nRd = h(X + W2 | U ) \u2212 h(W2 ).\n\n(16)\n\nProof of achievability: We specialize Theorem 1. Consider the specific p(u, x) constructed\nas follows. For given positive semidefinite matrices Ku , Kv \u2208 Rn\u00d7n with tr(Ku + Kv ) \u2264 P ,\nlet\nU \u223c N (0, Ku ),\nV \u223c N (0, Kv ),\nX = U + V,\nwhere U and V are independent. Then, the terms in Theorem 1 evaluate to\n|Ku + Kv + K1 |\n,\n|K1 |\n|Kv + K1 |\nI(X; Y | U ) = h(Y | U ) \u2212 h(W1 ) = 21 log\n,\n|K1 |\n|Kv + K2 |\nI(X; Z | U ) = h(Z | U ) \u2212 h(W2 ) = 21 log\n.\n|K2 |\nI(X; Y ) = h(Y ) \u2212 h(W1 ) =\n\n1\n2\n\nlog\n\nSimplifying the right hand sides and introducing time-sharing leads to the desired result.\nFor completeness, the coordinates of A and B for given matrices Ku , Kv are\n|Ku + Kv + K1 |\n,\n|K1 |\n|Kv + K2 | |Ku + Kv + K1 |\nRd = 21 log\n,\n|K2 |\n|Kv + K1 |\n|Kv + K1 |\nB(Ku , Kv ) : R = 21 log\n,\n|K1 |\n|Kv + K2 |\nRd = 12 log\n.\n|K2 |\nA(Ku , Kv ) :\n\nR=\n\n1\n2\n\nlog\n\nThe constituent region R(U, X) for fixed Ku and Kv is depicted in Figure 8.\n\n(17)\n(18)\n(19)\n(20)\n\n\f14\n\nRd\n\nA\n\nR(U, X)\n45\u25e6\n1\n2\n\nlog\n\n|Kv +K2 |\n|K2 |\n\nB\n\nR\n1\n2\n\nFigure 8.\n\nlog\n\n|Kv +K1 |\n|K1 |\n\n1\n2\n\nlog\n\n|Ku +Kv +K1 |\n|K1 |\n\nConstituent region for Gaussian superposition codebook with parameters Ku and Kv .\n\nProof of converse: The converse proof of Theorem 1 continues to hold and we only\nneed to show that Gaussian input distributions are sufficient. We proceed as follows. Since\nthe rate\u2013disturbance region is convex, its boundary can be fully characterized by maximizing\nR \u2212 \u03bbRd for each \u03bb > 0. We write\nR \u2212 \u03bbRd \u2264\n\nmax\n(R,Rd )\u2208R\n\n= max\n\n{R \u2212 \u03bbRd }\nmax\n\n(U,X) (R,Rd )\u2208R(U,X)\n\n{R \u2212 \u03bbRd } ,\n\nwhere the outer optimization is over the joint distribution of (U, X) and the inner optimization\nis over the region achieved by that distribution. The inner optimization can be solved explicitly\nas follows. For ease of presentation, assume for the moment that the power constraint is of the\nform Kx \u0016 S for some positive semidefinite matrix S. (That is, valid Kx are precisely those\nthat result in the matrix S \u2212 Kx being positive semidefinite.)\nFirst, consider \u03bb \u2264 1. For any distribution (U, X) \u223c p(u, x), point A(U, X) achieves a value\nof the inner optimization at least as large as point B(U, X), or any point on the line between\nthem. Using the coordinates of A(U, X) in (13) and (14), we can write\nR \u2212 \u03bbRd \u2264 max {\u03bb (h(X + W1 | U ) \u2212 h(X + W2 | U ))\n(U,X)\n\n+ (1 \u2212 \u03bb)h(X + W1 ) \u2212 h(W1 ) + \u03bbh(W2 )}\n(a)\n\n\u2264 \u03bb * max {h(X + W1 | U ) \u2212 h(X + W2 | U )}\n(U,X)\n\n+ (1 \u2212 \u03bb) * max {h(X + W1 )} \u2212 h(W1 ) + \u03bbh(W2 )\n(U,X)\n\u001a\n\u001b\n(b)\n\b\n|Kx + K1 |\n\u2264 \u03bb * max 12 log\n+ (1 \u2212 \u03bb) * max 12 log ((2\u03c0e)n |Kx + K1 |)\nKx \u0016S\nKx \u0016S\n|Kx + K2 |\nn\nn\n1\n\u03bb\n\u2212 2 log ((2\u03c0e) |K1 |) + 2 log ((2\u03c0e) |K2 |) .\nIn (a), the two maximizations are taken independently. In step (b), the first maximization is\nachieved by a Gaussian X that is independent of U , due to a theorem proved by Liu and\nViswanath [9, Thm. 8]. The optimization is now only over covariances matrices. Let K ? be an\noptimizer of this first maximization. The second maximization is also achieved by a Gaussian\n\n\f15\n\nX, and is optimized by Kx = S since f (Kx ) = |Kx + K1 | is matrix monotone. It follows that\n|K ? + K1 | 1\u2212\u03bb\n+ 2 log ((2\u03c0e)n |S + K1 |)\n|K ? + K2 |\n\u2212 12 log ((2\u03c0e)n |K1 |) + \u03bb2 log ((2\u03c0e)n |K2 |)\n|S + K1 | \u03bb\n|K ? + K2 | |S + K1 |\n\u2212 2 log ?\n.\n= 21 log\n|K1 |\n|K + K1 | |K2 |\n\nR \u2212 \u03bbRd \u2264\n\n\u03bb\n2\n\nlog\n\nBut this upper bound is achieved with equality by Gaussian superposition codebooks, namely\nthrough the point A(Ku , Kv ) as specified by equations (17) and (18), with Ku = S \u2212 K ? and\nKv = K ? .\nNow, consider \u03bb > 1. The argument proceeds analogously to the previous case. For\ncompleteness' sake, the details are as follows. We can write the inner optimization explicitly\nusing the coordinates of B(U, X) in (15) and (16) as\nR \u2212 \u03bbRd \u2264 max {h(X + W1 | U ) \u2212 \u03bbh(X + W2 | U )} + \u03bbh(W2 ) \u2212 h(W1 )\n(U,X)\n\n(a)\n\n1\nlog ((2\u03c0e)n |Kx + K1 |) \u2212 \u03bb2 log ((2\u03c0e)n |Kx\nKx \u0016S 2\n+ \u03bb2 log ((2\u03c0e)n |K2 |) \u2212 21 log ((2\u03c0e)n |K1 |) .\n\n\u2264 max\n\n\b\n\n+ K2 |)\n\nThe optimum in (a) is achieved by a Gaussian X (independent of U ) by virtue of [9, Thm. 8],\nwhile the other two terms are independent of the optimization variable. Let K ? be an optimizer.\nThen\n|K ? + K1 | \u03bb\n|K ? + K2 |\n\u2212 2 log\n.\nR \u2212 \u03bbRd \u2264 21 log\n|K1 |\n|K2 |\nThis upper bound is achieved with equality by Gaussian superposition codebooks through the\npoint B(Ku , Kv ) as given by equations (19) and (20) with Ku = 0 and Kv = K ? . This is a\npower control strategy, similar to the scalar Gaussian case.\nWe have thus shown that under a power constraint Kx \u0016 S, Gaussian superposition codes\nare optimal. The conclusion extends to the sum power constraint tr(Kx ) \u2264 P by observing that\n[\n{Kx : tr(Kx ) \u2264 P } =\n{Kx : Kx \u0016 S}.\nS: S\u00170\ntr(S)\u2264P\n\nIn other words, the sum power constraint can be expressed as a union of constraints of the type\nKx \u0016 S, for each of which Gaussian superposition codes are optimal. Therefore, a Gaussian\nsuperposition code must be optimal overall, too.\nIV. P ROOFS FOR TWO DISTURBANCE CONSTRAINTS\nA. Proof of Theorem 3\nCodebook generation. Fix a pmf p(u, x). Split the rate as R = R0 + R1 + R2 + R3 . Define the\nauxiliary rates R\u03031 \u2265 R1 and R\u03032 \u2265 R2 , let \u03b50 > 0, and define the set partitions\n[1:2nR\u03031 ] = L1 (1) \u222a * * * \u222a L1 (2nR1 ),\n[1:2nR\u03032 ] = L2 (1) \u222a * * * \u222a L2 (2nR2 ),\nwhere L1 (*) and L2 (*) are indexed sets of size 2n(R\u03031 \u2212R1 ) and 2n(R\u03032 \u2212R2 ) , respectively.\nQn\n1) For each m0 \u2208 [1:2nR0 ], generate un (m0 ) according to i=1 p(ui ).\nQn\n2) For each l1 \u2208 [1:2nR\u03031 ], generate z1n (m0 , l1 ) according to i=1 p(z1i | ui (m0 )). Likewise,\nQn\nfor each l2 \u2208 [1:2nR\u03032 ], generate z2n (m0 , l2 ) according to i=1 p(z2i | ui (m0 )).\n\n\f16\n\n3) For each (m0 , m1 , m2 ), let S(m0 , m1 , m2 ) be the set of all pairs (l1 , l2 ) from the product\n(n)\nset L1 (m1 ) \u00d7 L2 (m2 ) such that (z1n (m0 , l1 ), z2n (m0 , l2 )) \u2208 T\u03b50 (Z1 , Z2 | un (m0 )).\nnR3\nn\n4) For each (m0 , l1 , l2 ) and m3 \u2208 [1:2\n], generate x (m0 , l1 , l2 , m3 ) according to\nn\nY\n\np(xi | ui (m0 ), z1i (l1 ), z2i (l2 ))\n\ni=1\n\nif (l1 , l2 ) \u2208 S(m0 , m1 , m2 ). Otherwise, we draw from Unif(X n ).\n(m ,m ,m ) (m ,m ,m )\n5) Choose (l1 0 1 2 , l2 0 1 2 ) uniformly from S(m0 , m1 , m2 ). If S(m0 , m1 , m2 ) is\nempty, choose (1, 1).\nEncoding. To send message m = (m0 , m1 , m2 , m3 ), transmit the sequence\n(m0 ,m1 ,m2 )\n\nxn (m0 , l1\n\n(m0 ,m1 ,m2 )\n\n, l2\n\n, m3 ).\n\nDecoding. Let \u03b5 > \u03b50 . Upon receiving y n , define the tuple\nT (m0 , m1 , m2 , m3 )\n\u0010\n(m ,m ,m )\n(m ,m ,m )\n= un (m0 ), z1n (m0 , l1 0 1 2 ), z2n (m0 , l2 0 1 2 ),\n(m0 ,m1 ,m2 )\n\nxn (m0 , l1\n\n(m0 ,m1 ,m2 )\n\n, l2\n\n, m3 ), y n\n\n\u0011\n\nDeclare that m\u0302 = (m\u03020 , m\u03021 , m\u03022 , m\u03023 ) has been sent if it is the unique message such that\nT (m\u03020 , m\u03021 , m\u03022 , m\u03023 ) \u2208 T\u03b5(n) (U, Z1 , Z2 , X, Y ).\nAnalysis of the probability of error. Without loss of generality, assume that m0 = m1 = m2 =\nm3 = 1 is transmitted. Define the following events.\nEe1 : S(1, 1, 1) is empty,\nEe2 : S(1, 1, 1) contains two distinct pairs with\nequal first or second component,\nEi : {T (m0 , m1 , m2 , m3 ) \u2208 T\u03b5(n) (U, Z1 , Z2 , X, Y ) for\nsome (m0 , m1 , m2 , m3 ) \u2208 Mi },\n\ni \u2208 {0, . . . , 5},\n\nwhere the message subsets Mi are specified in Table 1. Defining the \"encoding error\" event\nEe = Ee1 \u222a Ee2 and the \"decoding error\" event Ed = E0c \u222a E1 \u222a E2 \u222a E3 \u222a E4 \u222a E5 , the probability\nof error can be upper-bounded as\nP(E) \u2264 P(Ee \u222a Ed ) \u2264 P(Ee ) + P(Ed | Eec ).\nThe motivation for introducing Ee2 as an \"error\" is to simplify the analysis of the second\nprobability term.\nWe bound P(Ee ) by the following lemma. Let r1 = R\u03031 \u2212 R1 and r2 = R\u03032 \u2212 R2 .\nLemma 1. P(Ee ) \u2192 0 as n \u2192 \u221e if\nr1 + r2 > I(Z1 ; Z1 | U ) + \u03b4(\u03b50 ),\n\n(21)\n\n0\n\n(22)\n\n0\n\n(23)\n\nr1 /2 + r2 < I(Z1 ; Z2 | U ) \u2212 \u03b4(\u03b5 ),\nr1 + r2 /2 < I(Z1 ; Z2 | U ) \u2212 \u03b4(\u03b5 ).\n\nProof sketch: First, consider Ee1 . As in the proof of Marton's inner bound for the broadcast\nchannel, the mutual covering lemma [8] implies P(Ee1 ) \u2192 0 as n \u2192 \u221e if (21) holds.\nNow consider Ee2 , for which we need to control the number of typical pairs that can occur\nin the same \"row\" or \"column\" of the product set L1 (m1 ) \u00d7 L2 (m2 ), i.e., for the same l1 or l2\ncoordinate. The probability P(Ee2 ) tends to zero provided that (22) and (23) hold.\n\n\f17\n\nMessage subset\nM0\nM1\nM2\nM3\nM4\nM5\nTable 1.\n\nm0\n1\n1\n1\n1\n1\n6= 1\n\nm1\n1\n1\n6= 1\n1\n6= 1\nany\n\nm2\n1\n1\n1\n6= 1\n6= 1\nany\n\nm3\n1\n6= 1\nany\nany\nany\nany\n\nMessage subsets for decoding error events.\n\nThis is akin to the birthday problem [13], where k samples are drawn uniformly and\nindependently from [1:N ], and the interest is in samples that have the same value (collisions).\nIt is well-knownpthat for the probability of collision to\u221abe pc , the number of samples required\nis roughly k \u2248 \u22122N ln(1 \u2212 pc ), which scales with N . In our case, the number of samples\nis the cardinality of the set S(m0 , m1 , m2 ), which is roughly k = 2n(r1 +r2 \u2212I(Z1 ;Z2 | U )) . The\nsamples are categorized into N1 = 2nr1 and N2 = 2nr2 classes along rows and columns,\nrespectively.\nachieve a probability of collision pc \u2192 0 along both dimensions, we need\n\u221a To \u221a\nk \u001c min{ N1 , N2 }, which yields exactly the conditions (22) and (23).\nA rigorous proof is given in Appendix A.\nBefore we proceed to bound the probability of decoding error, we need the following lemma,\nwhich is proved in Appendix B.\nLemma 2 (Independence lemma). Consider a finite set A and a subset A0 \u2282 A. Let pA be an\narbitrary pmfQover A. Let the random vector An be distributed proportionally to the product\nn\ndistribution l=1 pA (al ), restricted to the support set {an : ak \u2208 A0 for some k}. Let I be\ndrawn uniformly from {i : Ai \u2208 A0 }. Let J = ((I + s \u2212 1) mod n) + 1 for some integer\ns \u2208 [1:(n \u2212 1)]. Then, the random variables AI and AJ are independent.\nWe bound the probability P(Ed | Eec ) by the following lemma.\nLemma 3. P(Ed | Eec ) \u2192 0 as n \u2192 \u221e if\nR3 < H(Y | Z1 , Z2 , U ) \u2212 \u03b4(\u03b5),\n\n(24)\n\nR\u03031 + R3 < H(Y | Z2 , U ) + I(Z1 ; Z2 | U ) \u2212 \u03b4(\u03b5),\n\n(25)\n\nR\u03032 + R3 < H(Y | Z1 , U ) + I(Z1 ; Z2 | U ) \u2212 \u03b4(\u03b5),\n\n(26)\n\nR\u03031 + R\u03032 + R3 < H(Y | U ) + I(Z1 ; Z2 | U ) \u2212 \u03b4(\u03b5),\nR0 + R\u03031 + R\u03032 + R3 < H(Y ) + I(Z1 ; Z2 | U ) \u2212 \u03b4(\u03b5).\n\n(27)\n(28)\n\nProof sketch: The events of which Ed is composed are illustrated in Figure 9, which also\ndepicts the structure of the codebook for m0 = 1. The product sets L1 (m1 ) \u00d7 L2 (m2 ), for each\n(m1 , m2 ), are represented by shaded squares. In each product set, the sequence pair selected\nin step 5 of the codebook generation procedure is shown with its superposed xn codewords,\nas created in step 4. The correct codeword xn (1, 1, 1, 1) is shown as a white circle which is\nconnected to the received sequence y n . The codewords that may be mistakenly detected at the\nreceiver are shown as black circles. The product sets associated with decoding error events E1 ,\nE2 , E3 , and E4 are labeled 1, 2, 3, and 4, respectively.\nWe bound the probability of each sub-event of Ed . First, note that by the conditional typicality\nlemma in [8], P(E0c ) \u2192 0 as n \u2192 \u221e (this relies on \u03b50 < \u03b5). The probabilities of the events\nE1 through E5 conditioned on Eec tend to zero as n \u2192 \u221e under conditions (24) through (28),\ncorrespondingly.\nThe events E2 and E3 require the most careful analysis, since the true codeword xn (1, 1, 1, 1)\nand the codewords with which it may be confused can share the same z1n or z2n sequence (see\ndashed line and circles on it in Figure 9). Moreover, even when the chosen pairs in two different\n\n\f18\n(1,1)\n\nz2n (1, l2\n\n)\n\n(1,1)\n\nz1n (1, l1\n\nm1 = 1\n\nz1n (1, 1)\nz1n (1, 2)\n\n)\nxn (1, 1, 1, 1)\n\nm1 = 2\n\nz1n (1, 2n(R\u03031 \u2212R1 ) )\n\nyn\n\nm2 = 1\nFigure 9.\n\nm2 = 2\n\nm2 = 3\n\nIllustration of decoding error events, for m0 = 1.\n\nproduct sets do not share one of the two coordinates (see the chosen pairs for (m1 , m2 ) = (1, 1)\nand (2, 1) in Figure 9), correlation could potentially be caused by the selection procedure in\nstep 5 of codebook generation. We use the independence lemma (Lemma 2) to show that the\nevent Eec prevents this correlation leakage from occurring. The application of the lemma is what\ndistinguishes this analysis from the conventional Marton inner bound for broadcast channels [14,\n15]. There, analysis of the selection process can be altogether avoided since each receiver\ndecodes only one of the two coordinates.\nA detailed proof for the event E3 is given in Appendix C, the other events follow likewise.\nAnalysis of disturbance rate. When viewed by receiver Z1 , the codeword for message m =\n(m ,m ,m )\n(m0 , m1 , m2 , m3 ) appears as z1n (m0 , l1 0 1 2 ). We can pessimistically assume that all\nn\nsequences z1 (m0 , l1 ) as created in step 2 of codebook generation can be seen at the receiver for\nsome message m. Therefore, the number of possible sequences at Z1 , and thus its disturbance\nrate, is upper-bounded by H(Z1n ) \u2264 n(R0 + R\u03031 ). Applying the same argument for Z2 , the\nproposed scheme achieves\nR0 + R\u03031 \u2264 Rd,1 ,\n\n(29)\n\nR0 + R\u03032 \u2264 Rd,2 .\n\n(30)\n\nConclusion of the proof. Collecting inequalities (21) through (30), recalling R = R0 + R1 +\nR2 + R3 , and using the Fourier-Motzkin procedure to eliminate R0 , R1 , R2 , and R3 leads to\nthe (R, Rd,1 , Rd,2 ) region claimed in the theorem.\nFinally, the statement of Remark 6 follows from\n\u2212 I(Z1 ; Z2 | U ) + I(Z1 ; Z2 | U, Y )\n= \u2212H(Z2 | U ) + H(Z2 | U, Z1 ) + H(Z2 | U, Y ) \u2212 H(Z2 | U, Y, Z1 )\n= \u2212I(Y ; Z2 | U ) + I(Y ; Z2 | U, Z1 ),\nwhich leads to the equality\nH(Y | Z1 , Z2 , U ) + H(Y | U ) \u2212 I(Z1 ; Z2 | U ) + I(Z1 ; Z2 | U, Y )\n= H(Y | Z1 , Z2 , U ) + H(Y | U ) \u2212 I(Y ; Z2 | U ) + I(Y ; Z2 | U, Z1 )\n= H(Y | Z1 , U ) + H(Y | Z2 , U ).\n\n\f19\n\nB. Proof of Theorem 4\nFirst, consider\nnR \u2264 I(X n ; Y n ) + n\u03b5n\nn\nX\n=\nI(X n ; Yi | Y i\u22121 ) + n\u03b5n\ni=1\n\n=\n\nn\nX\n\nI(Xi ; Yi | Y i\u22121 ) + n\u03b5n\n\ni=1\n\n= nI(X; Y | Q)\n= nH(Y | Q).\nFurthermore,\nnRd,1 \u2265 I(X n ; Z1n )\n\u2265 I(Y n ; Z1n )\nn\nX\nI(Yi ; Z1n | Y i\u22121 )\n=\ni=1\n\n\u2265\n\nn\nX\n\nI(Yi ; Z1i | Y i\u22121 )\n\ni=1\n\n= nI(Y ; Z1 | Q),\nwhere Y = YT , Z1 = Z1T , and Q = (Y T \u22121 , T ) with T \u223c Unif[1:n]. The same argument leads\nto\nnRd,2 \u2265 nI(Y ; Z2 | Q),\nwith the same random variable identifications, and the additional Z2 = Z2T . Finally, the\ncardinality bound on Q follows from the convex cover method in [8].\nC. Proof of Theorem 5\nFirst, we specialize Theorem 3 as follows.\nCorollary 4. The rate\u2013disturbance region R of the deterministic channel with two disturbance\nconstraints is inner-bounded by the set of rate triples (R, Rd,1 , Rd,2 ) such that\nR \u2264 H(Y ),\n\n(31)\n\nRd,1 \u2265 I(Y ; Z1 , U ),\n\n(32)\n\nRd,2 \u2265 I(Y ; Z2 , U ),\n\n(33)\n\nRd,1 + Rd,2 \u2265 I(Y ; Z1 , Z2 , U ) + I(Y ; U ) + I(Z1 ; Z2 | U )\n= I(Y ; Z1 , U ) + I(Y ; Z2 , U ) + I(Z1 ; Z2 | U, Y ),\n\n(34)\n\nfor some pmf p(u, x).\nThe two equivalent expressions in (34) originate from Remark 6. An example of the constituent\nregions of Corollary 4 for fixed p(u, x) is depicted in Figure 10. The figure also illustrates how\nthe corollary follows from Theorem 3: Each constituent region of the corollary is a strict subset\nof the constituent region of the theorem, for the same p(u, x).\n\n\f20\n\nR\n(31)\n\nRd,2\n\n(32)\n(34)\n\n(33)\n\nRd,1\nFigure 10. Constituent region for Corollary 4, for a fixed p(u, x). Each face is annotated by the\ninequality that defines it. For comparison, the constituent region of Theorem 3 is shown with dashed lines\n(see Figure 5).\n\nProof of Corollary 4: In Theorem 3, consider the case where (1) is met with equality, i.e.,\nR = H(Y ). This yields a subset region which is still achievable. It simplifies to\nRd,1 + Rd,2 \u2265 I(Z1 ; Z2 | U ),\n\n(35)\n\nRd,1 \u2265 I(Y ; Z1 , U ),\n\n(36)\n\nRd,2 \u2265 I(Y ; Z2 , U ),\n\n(37)\n\nRd,1 + Rd,2 \u2265 I(Y ; Z1 , Z2 , U ) + I(Z1 ; Z2 | U ),\n\n(38)\n\nRd,1 + Rd,2 \u2265 I(Y ; Z1 , Z2 , U ) + I(Y ; U ) + I(Z1 ; Z2 | U )\n= I(Y ; Z1 , U ) + I(Y ; Z2 , U ) + I(Z1 ; Z2 | U, Y ).\n\n(39)\n\nClearly, conditions (35) and (38) are dominated by inequality (39), and the desired result follows.\nProof of achievability for Theorem 5: We further specialize Corollary 4. We choose\nU = Z1 \u2228 Z2 , i.e., the common part of Z1 and Z2 . This implies that condition (34) can be\nomitted, since I(Z1 ; Z2 | U, Y ) = 0 for all p(u, x) by assumption. Furthermore, U can be\ndropped from conditions (32) and (33) by virtue of being a function of Z1 and Z2 . We conclude\nthat\nR \u2264 H(Y ),\n\n(40)\n\nRd,1 \u2265 I(Y ; Z1 ),\n\n(41)\n\nRd,2 \u2265 I(Y ; Z2 ),\n\n(42)\n\nis achievable for all p(x). Adding a time-sharing random variable Q completes the proof.\nNote that in the special case where Y 4 Z1 or Y 4 Z2 , the same conclusion holds with the\nchoice U = \u2205.\nD. Proof of Corollary 3\nProof of achievability: We prove the result for Z1 4 Z2 , the other case follows by symmetry.\nWe specialize the achievable region of Theorem 3 by choosing U = Z2 . The rate\u2013disturbance\nconstraints are\nR \u2264 H(Y ),\nRd,1 + Rd,2 \u2265 0,\n\n(43)\n(44)\n\nR \u2212 Rd,1 \u2264 H(Y | Z1 ),\n\n(45)\n\nR \u2212 Rd,2 \u2264 H(Y | Z2 ),\n\n(46)\n\nR \u2212 Rd,1 \u2212 Rd,2 \u2264 H(Y | Z1 ),\n\n(47)\n\n2R \u2212 Rd,1 \u2212 Rd,2 \u2264 H(Y | Z1 ) + H(Y | Z2 ).\n\n(48)\n\n\f21\n\nClearly, (44) is vacuous. Furthermore, (47) is dominated by (45), and (48) is dominated by the\nsum of (45) and (46). This completes the proof.\nProof of converse: The first inequality follows from Fano's inequality as\nnR \u2264 I(X n ; Y n ) + n\u03b5n\n= H(Y n ) + n\u03b5n\n\u2264 nH(Y ) + n\u03b5n ,\nwhere Y = YQ and Q \u223c Unif[1:n]. The other two inequalities follow as\nn(R \u2212 Rd,1 ) \u2264 nR \u2212 I(X n ; Z1n )\n\u2264 H(Y n ) \u2212 H(Z1n ) + n\u03b5n\n\u2264 H(Y n , Z1n ) \u2212 H(Z1n ) + n\u03b5n\n= H(Y n | Z1n ) + n\u03b5n\n\u2264 nH(Y | Z1 ) + n\u03b5n ,\nwith Z1 = Z1Q , and likewise for n(R \u2212 Rd,2 ).\nV. ACKNOWLEDGMENTS\nThe authors would like to thank Pramod Viswanath and Yeow-Khiang Chia for helpful\ndiscussions, and gratefully acknowledge the Information Theoretic Inequalities Prover (Xitip) [16,\n17], which was used as a verification tool in some of our derivations.\nR EFERENCES\n[1] B. Bandemer and A. El Gamal, \"Communication with disturbance constraints,\" in Proceedings of\nISIT 2011, St. Petersburg, Russia, Aug. 2011.\n[2] --, \"An achievable rate region for the 3-user-pair deterministic interference channel,\" in Proceedings\nof Allerton 2011, Monticello, IL, Sep. 2011, (invited).\n[3] T. S. Han and K. Kobayashi, \"A new achievable rate region for the interference channel,\" IEEE\nTrans. Inf. Theory, vol. 27, no. 1, pp. 49\u201360, Jan. 1981.\n[4] H.-F. Chong, M. Motani, H. K. Garg, and H. El Gamal, \"On the Han-Kobayashi region for the\ninterference channel,\" IEEE Trans. Inf. Theory, vol. 54, no. 7, pp. 3188\u20133195, Jul. 2008.\n[5] A. D. Wyner, \"The wire-tap channel,\" Bell System Technical Journal, vol. 54, no. 8, pp. 1355\u20131387,\nOct. 1975.\n[6] I. Csisz\u00e1r and J. K\u00f6rner, \"Broadcast channels with confidential messages,\" IEEE Trans. Inf. Theory,\nvol. 24, no. 3, pp. 339\u2013348, May 1978.\n[7] A. A. El Gamal and M. H. M. Costa, \"The capacity region of a class of deterministic interference\nchannels,\" IEEE Trans. Inf. Theory, vol. 28, no. 2, pp. 343\u2013346, Mar. 1982.\n[8] A. El Gamal and Y.-H. Kim, Network Information Theory. Cambridge University Press, 2011.\n[9] T. Liu and P. Viswanath, \"An extremal inequality motivated by multiterminal information-theoretic\nproblems,\" IEEE Trans. Inf. Theory, vol. 53, no. 5, pp. 1839\u20131851, May 2007.\n[10] R. P. Stanley, Enumerative Combinatorics, 2nd ed. Cambridge University Press, 2011, vol. 1.\n[Online]. Available: http://www-math.mit.edu/~rstan/ec/\n[11] P. G\u00e1cs and J. K\u00f6rner, \"Common information is far less than mutual information,\" Problems of\nControl and Information Theory, vol. 2, no. 2, pp. 149\u2013162, 1973.\n[12] H. S. Witsenhausen, \"On sequences of pairs of dependent random variables,\" SIAM Journal of\nApplied Mathematics, vol. 28, no. 1, pp. 100\u2013113, Jan. 1975.\n[13] R. von Mises, \"\u00dcber Aufteilungs- und Besetzungs-Wahrscheinlichkeiten,\" Revue de la Facult\u00e9 des\nSciences de l'Universit\u00e9 d'Istanbul, vol. 4, pp. 145\u2013163, 1939, reprinted in \"Selected Papers of\nRichard von Mises\", vol. 2 (Ed. P. Frank, S. Goldstein, M. Kac, W. Prager, G. Szeg\u0151, and G. Birkhoff).\nProvidence, RI: American Mathematical Society, pp. 313-334, 1964.\n[14] K. Marton, \"A coding theorem for the discrete memoryless broadcast channel,\" IEEE Trans. Inf.\nTheory, vol. 25, no. 3, pp. 306\u2013311, May 1979.\n[15] A. El Gamal and E. C. van der Meulen, \"A proof of Marton's coding theorem for the discrete\nmemoryless broadcast channel,\" IEEE Trans. Inf. Theory, vol. 27, no. 1, pp. 120\u2013122, Jan. 1981.\n[16] R. Pulikkoonattu. (2008, Jan.) Information theoretic inequalities prover \"Xitip\". [Online]. Available:\nhttp://xitip.epfl.ch/\n\n\f22\n\n[17] R. W. Yeung and Y.-O. Yan. Information theoretic inequality prover. [Online]. Available:\nhttp://user-www.ie.cuhk.edu.hk/~ITIP/\n\nA PPENDIX\nA. Proof of Lemma 1\nThe product bin (m1 , m2 ) = (1, 1) for m0 = 1 contains lm sequence pairs, where l = 2nr1\nand m = 2nr2 . Each pair (Z1n (1, l1 ), Z2n (1, l2 )), for l1 \u2208 [1:l] and l2 \u2208 [1:m], has probability\n.\np = 2\u2212nI(Z1 ;Z2 | U ) to be jointly typical. Now fix one coordinate, say l1 = 1. The corresponding\n\"row\" of the bin contains m sequences Z2n (1, l2 ), each of which has an independent probability\nof p to be jointly typical with Z1n (1, 1). Let K be the total number of typical sequences in this\nrow. Then\nP(K = 0) = (1 \u2212 p)m ,\nP(K = 1) = mp(1 \u2212 p)m\u22121 ,\nP(K \u2265 2) = 1 \u2212 (1 \u2212 p + mp) (1 \u2212 p)m\u22121\n| {z }\n\u22651\u2212(m\u22121)p\n\n\u2264 m2 p2 .\nWe have thus upper-bounded the probability to encounter two or more typical pairs in a single\nrow. Consequently, the probability of two or more typical pairs occurring in any row is upper\nbounded by lm2 p2 . Substituting definitions leads to the desired inequality. The same argument\ncan be made for columns of the bin.\nB. Proof of independence lemma (Lemma 2)\nWe prove the lemma for s = 1, the remaining cases follow by symmetry. For ease of notation,\ndefine the specialized modulo operator JxK = 1 + ((x \u2212 1) mod n), the indicator function\n1A0 (a) = 1 if a \u2208 A0 and 0 otherwise, and the shorthand notations Y = AI and Z = AJ .\nNotice that\n( Q\nn\n1\npA (al ) if ak \u2208 A0 for some k \u2208 [1:n]\np(an ) = c l=1\n0\notherwise,\nwhere c is a normalization constant, the exact value of which is not relevant. Further,\n(\n1\nPn\nif ai \u2208 A0\nk=1 1A0 (ak )\np(i | an ) =\n0\notherwise.\nThe joint distribution of (An , I, J, Y, Z) is then\n(\nn\nPn p(a )\nif ai \u2208 A0 , ai = y, aj = z, and j = Ji + 1K\nn\n1\n0 (ak )\nA\nk=1\np(a , i, j, y, z) =\n0\notherwise.\nPartially marginalizing, it follows that\np(y, z) =\n\nn\nX\n\nX\n\np(an )\n.\n0\nk=1 1A (ak )\n\nPn\ni=1 an : ai \u2208A0\nai =y\naJi+1K =z\n\nIt is clear that p(y, z) = p(y)p(z) = 0 if y \u2208\n/ A0 . On the other hand, for y \u2208 A0 , we have\nQn\nn\nX\nX\npA (al )\nPl=1\np(y, z) =\n.\nn\n0\nc\nk=1 1A (ak )\ni=1 n\na : ai =y\naJi+1K =z\n\n\f23\n\nThe fraction under the sum is invariant under permutations of an . Therefore,\nQn\nn\np (a )\n1X X\nPnl=1 A l\np(y, z) =\n0\nc i=1 n\n1\nk=1 A (ak )\na : a1 =y\na2 =z\n\nn\n=\nc\n=\n\nX\nan =(y,z,an\n3)\n\nQn\np (a )\nPnl=1 A l\n0\n1\nk=1 A (ak )\n\nn pA (y) pA (z)\nc\n\nQn\n\nX\n\npA (al )\nPn\n,\n1 + 1A0 (z) + k=3 1A0 (ak )\nl=3\n\nn\u22122\nan\n3 \u2208A\n\nan3\n\nwhere\nare the last n \u2212 2 components of an . Observe that p(y, z) separates into a function\nof z and a function of y. Independence is thus established.\nC. Proof of Lemma 3, exemplified for E3\nWe analyze the probability of E3 as follows.\n\b\n(1,1,m2 )\n(1,1,m2 )\nE3 = U n (1), Z1n (1, L1\n), Z2n (1, L2\n),\n\u0001\n(1,1,m2 )\n(1,1,m2 )\nn\nn\nX (1, L1\n, L2\n, m3 ), Y\n\u2208 T\u03b5(n) ,\nfor some m2 6= 1, m3\n\u2286\n\n\b\n\n(1,1,m2 )\n\nU n (1), Z1n (1, L1\n\n), Z2n (1, l2 ),\n\u0001\n(1,1,m2 )\nX n (1, L1\n, l2 , m3 ), Y n \u2208 T\u03b5(n) ,\nfor some m2 6= 1, m3 , l2 \u2208\n/ L2 (1) ,\n\n(1,1,m )\n\n(1,1,1)\n\n2\nDefine the event Eeq = {L1\n= L1\n}, which allows us to write P(E3 | Eec ) = P(E3 \u2229\nc\nc\nc\nEeq | Ee ) + P(E3 \u2229 Eeq | Ee ). We consider both terms separately.\n\b\n(1,1,1)\nE3 \u2229 Eeq \u2286 U n (1), Z1n (1, L1\n), Z2n (1, l2 ),\n\u0001\n(1,1,1)\nX n (1, L1\n, l2 , m3 ), Y n \u2208 T\u03b5(n) ,\n\nfor some l2 \u2208\n/ L2 (1), m3 .\nThus,\nP(E3 \u2229 Eeq | Eec )\n\u0011\nX \u0010\n(1,1,1)\nP U n (1) = un , Z1n (1, L12\n) = z1n , Y n = y n | Eec\n\u2264\n(n)\n\n(un ,z1n ,y n )\u2208T\u03b5\n\n*\n\nX\n\nnR3\n2X\n\nP (un , z1n , Z2n (1, l2 ),\n\nl2 \u2208L\n/ 2 (1) m3 =1\n\n(1,1,1)\n\nX n (1, L1\n\n, l2 , m3 ), y n ) \u2208 T\u03b5(n) | Eec\n\n\u0001\n\n\u2264 2n(R\u03032 +R3 ) P ? ,\nwhere P ? is shorthand for the last P(*) expression. Continue with\n\u0010\nX\n(1,1,1)\nP? =\nP Z2n (1, l2 ) = z2n , X n (1, L1\n, l2 , m3 ) = xn\n(z2n ,xn )\u2208T\u03b5(n) (\nZ2 ,X | un ,z1n ,y n )\n(a)\n\n(1,1,1)\n\nU n (1) = un , Z1n (L1\n\nX\n\n=\n\n(z2n ,xn )\u2208T\u03b5(n) (\nZ2 ,X | un ,z1n ,y n )\n\n.\n\n|\n\n{z\n\n}\n\n= 2nH(X,Z2 |Z1 ,Y,U )\n\np(z2n | un )\n| {z }\n.\n= 2\u2212nH(Z2 |U )\n\n) = z1n , Y n = y n , Eec\n\np(xn | z1n , z2n , un )\n|\n{z\n}\n.\n= 2\u2212nH(X|Z1 ,Z2 ,U )\n\n\u0011\n\n\f24\n\n\u2264 2n(H(X,Z2 |Z1 ,Y,U )\u2212H(Z2 |U )\u2212H(X|Z1 ,Z2 ,U )+\u03b4(\u03b5))\n= 2n(\u2212H(Y |Z1 ,U )\u2212I(Z1 ;Z2 |U )+\u03b4(\u03b5)) .\nIn step (a), we have used the fact that l2 \u2208\n/ L2 (1), and therefore, Z2n (1, l2 ) relates to a bin other\nthan the first one. It is independent of the conditions Y n = y n and Eec , both of which relate\nonly to the (1, 1) bin for m0 = 1. A similar argument applies to the second term.\nSubstituting back in the previous chain of inequalities implies that P(E3 \u2229 Eeq | Eec ) \u2192 0 as\nn \u2192 \u221e if inequality (26) holds.\nNext, consider\n\b\nc\nE3 \u2229 Eeq\n\u2286 U n (1), Z1n (1, l1 ), Z2n (1, l2 ), X n (1, l1 , l2 , m3 ),\n\u0001\n(1,1,1)\nY n \u2208 T\u03b5(n) , for some l1 \u2208 L1 (1) \\ {L1\n},\nl2 \u2208\n/ L2 (1), m3 .\nWe argue\nc\nP(E3 \u2229 Eeq\n| Eec )\nX\nX\n\u2264\n***\nP (U n (1) = un , Y n = y n | Eec )\n(n)\n\n(1,1,1)\n\n(un ,y n )\u2208T\u03b5\n\nl1 \u2208L1 (1)\\{L1\n\n}\n\n2nR3\n\n***\n\nX\n\nX\n\nP (un , Z1n (1, l1 ), Z2n (1, l2 ),\n\nl2 \u2208L\n/ 2 (1) m3 =1\n\nX n (1, l1 , l2 , m3 ), y n ) \u2208 T\u03b5(n) | U n (1) = un , Y n = y n , Eec\n\n\u0001\n\n\u2264 2n(R\u03031 \u2212R1 +R\u03032 +R3 ) P ? ,\nwhere P ? represents the last P(*) expression. Finally,\nX\nP? =\nP Z1n (1, l1 ) = z1n , Z2n (1, l2 ) = z2n ,\n(z1n ,z2n ,xn )\u2208T\u03b5(n) (\nZ1 ,Z2 ,X | un ,y n )\n\nX n (1, l1 , l2 , m3 ) = xn |\nU n (1) = un , Y n = y n , Eec\n\nX\n\nX\n\n(z1n ,z2n ,xn )\u2208T\u03b5(n) (\nZ1 ,Z2 ,X | un ,y n )\n\nz2n (l20 ), for\nall l20 \u2208L2 (1)\n\n=\n\n\u0001\n\nP Z2n (1, l20 ) = z2n (l20 ) for\n\u0001\nall l20 \u2208 L2 (1) | Eec\n\n* P Z1n (1, l1 ) = z1n , Z2n (1, l2 ) = z2n ,\nX n (1, l1 , l2 , m3 ) = xn |\nU n (1) = un , Y n = y n , Z2n (1, l20 ) = z2n (l20 )\n\u0001\nfor all l20 \u2208 L2 (1), Eec\n(a)\n\n\u2264\n\nX\n\n(z1n ,z2n ,xn )\u2208T\u03b5(n) (\nZ1 ,Z2 ,X | un ,y n )\n\n|\n.\n\n{z\n\n}\n\np(z1n | un , Eec ) p(z2n | un ) p(xn | z1n , z2n , un )\n{z\n}\n|\n{z\n} | {z } |\n.\n.\n(b)\n= 2\u2212nH(Z2 |U ) = 2\u2212nH(X|Z1 ,Z2 ,U )\n.\n= 2\u2212nH(Z1 |U )\n\n= 2nH(X,Z1 ,Z2 |Y,U )\nn(H(X,Z1 ,Z2 |Y,U )\u2212H(Z1 |U )\u2212H(Z2 |U )\u2212H(X|Z1 ,Z2 ,U )+\u03b4(\u03b5))\n\n\u22642\n\n= 2n(\u2212H(Y |U )\u2212I(Z1 ;Z2 |U )+\u03b4(\u03b5)) .\nHere, (a) uses uses the fact that for the l1 indices in question, Z1n (1, l1 ) is independent of\n(1,1,1)\nY n . This is a consequence of independence between the selected Z1n (1, L1\n) and the other\nn\n(non-selected) Z1 (1, l1 ) due to Lemma 2. The lemma applies because the event is conditioned\n(1,1,1)\n(1) on Eec , which ensures that picking L1\nis uniform as required by the lemma, and (2) on\nZ2n (1, l20 ) for all l20 \u2208 L2 (1), which provides for the qualifying set A0 of the lemma.\n\n\f25\n\nStep (b) follows from\np(Eec | un , z1n )\np(Eec | un )\n1\n\u2264 p(z1n | un ) *\np(Eec | un )\n1\n\u2264 p(z1n | un ) *\n1 \u2212 2\u2212\u03b4n\n0\n\u2264 2\u2212n(H(Z1 |U )\u2212\u03b5) * 2n\u03b4\n\np(z1n | un , Eec ) = p(z1n | un ) *\n\n0\n\n\u2264 2\u2212n(H(Z1 |U )\u2212\u03b5\u2212\u03b4 ) .\nHere, \u03b4 is the minimum slack of the three conditions for Eec in Lemma 1. Note that for any\n\u03b4, \u03b4 0 > 0, we can find an N0 such that\n\u2200n \u2265 N0 :\n\n0\n1\n\u2264 2n\u03b4 .\n\u2212\u03b4n\n1\u22122\n\nc\nWe conclude that P(E3 \u2229 Eeq\n| Eec ) \u2192 0 as n \u2192 \u221e if\n\nR\u03031 \u2212 R1 + R\u03032 + R3 \u2264 H(Y |Q) + I(X1 ; X2 |Q) \u2212 \u03b4(\u03b5).\nThis is an implication of (27) which stems from analyzing E4 , and may thus be omitted.\n\n\f"}