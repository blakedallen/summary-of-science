{"id": "http://arxiv.org/abs/0909.0184v1", "guidislink": true, "updated": "2009-09-01T13:50:20Z", "updated_parsed": [2009, 9, 1, 13, 50, 20, 1, 244, 0], "published": "2009-09-01T13:50:20Z", "published_parsed": [2009, 9, 1, 13, 50, 20, 1, 244, 0], "title": "Robust nearest-neighbor methods for classifying high-dimensional data", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0909.2589%2C0909.3142%2C0909.0383%2C0909.1805%2C0909.2034%2C0909.1970%2C0909.4297%2C0909.5407%2C0909.4319%2C0909.4857%2C0909.2732%2C0909.3658%2C0909.5626%2C0909.1851%2C0909.3147%2C0909.4759%2C0909.2554%2C0909.0341%2C0909.2149%2C0909.0033%2C0909.1793%2C0909.2878%2C0909.3273%2C0909.3011%2C0909.3674%2C0909.5057%2C0909.2940%2C0909.2046%2C0909.5180%2C0909.0427%2C0909.3950%2C0909.0433%2C0909.1601%2C0909.0429%2C0909.0985%2C0909.1604%2C0909.3521%2C0909.3817%2C0909.4176%2C0909.2731%2C0909.1495%2C0909.2292%2C0909.0215%2C0909.0432%2C0909.3338%2C0909.3231%2C0909.1876%2C0909.5265%2C0909.1265%2C0909.4912%2C0909.5513%2C0909.2750%2C0909.2025%2C0909.3035%2C0909.4110%2C0909.2506%2C0909.2007%2C0909.1169%2C0909.4190%2C0909.0159%2C0909.0439%2C0909.0674%2C0909.4054%2C0909.0371%2C0909.1528%2C0909.0184%2C0909.1543%2C0909.1889%2C0909.4913%2C0909.4040%2C0909.5417%2C0909.1296%2C0909.3195%2C0909.4827%2C0909.4555%2C0909.3802%2C0909.2947%2C0909.3986%2C0909.5251%2C0909.4175%2C0909.1410%2C0909.0596%2C0909.2489%2C0909.0897%2C0909.4292%2C0909.3883%2C0909.5529%2C0909.3277%2C0909.0652%2C0909.0501%2C0909.4157%2C0909.1885%2C0909.2426%2C0909.2395%2C0909.4183%2C0909.5304%2C0909.1132%2C0909.5671%2C0909.4831%2C0909.0087%2C0909.4191&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Robust nearest-neighbor methods for classifying high-dimensional data"}, "summary": "We suggest a robust nearest-neighbor approach to classifying high-dimensional\ndata. The method enhances sensitivity by employing a threshold and truncates to\na sequence of zeros and ones in order to reduce the deleterious impact of\nheavy-tailed data. Empirical rules are suggested for choosing the threshold.\nThey require the bare minimum of data; only one data vector is needed from each\npopulation. Theoretical and numerical aspects of performance are explored,\npaying particular attention to the impacts of correlation and heterogeneity\namong data components. On the theoretical side, it is shown that our truncated,\nthresholded, nearest-neighbor classifier enjoys the same classification\nboundary as more conventional, nonrobust approaches, which require finite\nmoments in order to achieve good performance. In particular, the greater\nrobustness of our approach does not come at the price of reduced effectiveness.\nMoreover, when both training sample sizes equal 1, our new method can have\nperformance equal to that of optimal classifiers that require independent and\nidentically distributed data with known marginal distributions; yet, our\nclassifier does not itself need conditions of this type.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0909.2589%2C0909.3142%2C0909.0383%2C0909.1805%2C0909.2034%2C0909.1970%2C0909.4297%2C0909.5407%2C0909.4319%2C0909.4857%2C0909.2732%2C0909.3658%2C0909.5626%2C0909.1851%2C0909.3147%2C0909.4759%2C0909.2554%2C0909.0341%2C0909.2149%2C0909.0033%2C0909.1793%2C0909.2878%2C0909.3273%2C0909.3011%2C0909.3674%2C0909.5057%2C0909.2940%2C0909.2046%2C0909.5180%2C0909.0427%2C0909.3950%2C0909.0433%2C0909.1601%2C0909.0429%2C0909.0985%2C0909.1604%2C0909.3521%2C0909.3817%2C0909.4176%2C0909.2731%2C0909.1495%2C0909.2292%2C0909.0215%2C0909.0432%2C0909.3338%2C0909.3231%2C0909.1876%2C0909.5265%2C0909.1265%2C0909.4912%2C0909.5513%2C0909.2750%2C0909.2025%2C0909.3035%2C0909.4110%2C0909.2506%2C0909.2007%2C0909.1169%2C0909.4190%2C0909.0159%2C0909.0439%2C0909.0674%2C0909.4054%2C0909.0371%2C0909.1528%2C0909.0184%2C0909.1543%2C0909.1889%2C0909.4913%2C0909.4040%2C0909.5417%2C0909.1296%2C0909.3195%2C0909.4827%2C0909.4555%2C0909.3802%2C0909.2947%2C0909.3986%2C0909.5251%2C0909.4175%2C0909.1410%2C0909.0596%2C0909.2489%2C0909.0897%2C0909.4292%2C0909.3883%2C0909.5529%2C0909.3277%2C0909.0652%2C0909.0501%2C0909.4157%2C0909.1885%2C0909.2426%2C0909.2395%2C0909.4183%2C0909.5304%2C0909.1132%2C0909.5671%2C0909.4831%2C0909.0087%2C0909.4191&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We suggest a robust nearest-neighbor approach to classifying high-dimensional\ndata. The method enhances sensitivity by employing a threshold and truncates to\na sequence of zeros and ones in order to reduce the deleterious impact of\nheavy-tailed data. Empirical rules are suggested for choosing the threshold.\nThey require the bare minimum of data; only one data vector is needed from each\npopulation. Theoretical and numerical aspects of performance are explored,\npaying particular attention to the impacts of correlation and heterogeneity\namong data components. On the theoretical side, it is shown that our truncated,\nthresholded, nearest-neighbor classifier enjoys the same classification\nboundary as more conventional, nonrobust approaches, which require finite\nmoments in order to achieve good performance. In particular, the greater\nrobustness of our approach does not come at the price of reduced effectiveness.\nMoreover, when both training sample sizes equal 1, our new method can have\nperformance equal to that of optimal classifiers that require independent and\nidentically distributed data with known marginal distributions; yet, our\nclassifier does not itself need conditions of this type."}, "authors": ["Yao-ban Chan", "Peter Hall"], "author_detail": {"name": "Peter Hall"}, "author": "Peter Hall", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/08-AOS591", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0909.0184v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0909.0184v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/08-AOS591 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62H30 (Primary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0909.0184v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0909.0184v1", "journal_reference": "Annals of Statistics 2009, Vol. 37, No. 6A, 3186-3203", "doi": "10.1214/08-AOS591", "fulltext": "The Annals of Statistics\n2009, Vol. 37, No. 6A, 3186\u20133203\nDOI: 10.1214/08-AOS591\nc Institute of Mathematical Statistics, 2009\n\narXiv:0909.0184v1 [math.ST] 1 Sep 2009\n\nROBUST NEAREST-NEIGHBOR METHODS FOR\nCLASSIFYING HIGH-DIMENSIONAL DATA\nBy Yao-ban Chan and Peter Hall\nUniversity of Melbourne\nWe suggest a robust nearest-neighbor approach to classifying\nhigh-dimensional data. The method enhances sensitivity by employing a threshold and truncates to a sequence of zeros and ones in\norder to reduce the deleterious impact of heavy-tailed data. Empirical rules are suggested for choosing the threshold. They require the\nbare minimum of data; only one data vector is needed from each\npopulation. Theoretical and numerical aspects of performance are explored, paying particular attention to the impacts of correlation and\nheterogeneity among data components. On the theoretical side, it\nis shown that our truncated, thresholded, nearest-neighbor classifier\nenjoys the same classification boundary as more conventional, nonrobust approaches, which require finite moments in order to achieve\ngood performance. In particular, the greater robustness of our approach does not come at the price of reduced effectiveness. Moreover,\nwhen both training sample sizes equal 1, our new method can have\nperformance equal to that of optimal classifiers that require independent and identically distributed data with known marginal distributions; yet, our classifier does not itself need conditions of this\ntype.\n\n1. Introduction. In classification problems where sample size is much\nsmaller than dimension, nearest-neighbor methods, after truncation to reduce noise, can enjoy particularly good performance. They have the potential to be highly adaptive, not least because they do not require explicit\nassumptions about marginal distributions.\nHowever, in very high-dimensional settings, conventional nearest-neighbor\nmethods can be adversely affected by \"noise\" from vector components that\ndo not carry useful information for classification. Moreover, they are not\nReceived August 2007; revised January 2008.\nAMS 2000 subject classification. 62H30.\nKey words and phrases. Classification boundary, detection boundary, false discovery\nrate, heterogeneous components, higher criticism, optimal classification, threshold, zero\u2013\none data.\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2009, Vol. 37, No. 6A, 3186\u20133203. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nY.-B. CHAN AND P. HALL\n\nrobust against outliers. In particular, they can be influenced considerably by\nheavy-tailed features of sampling distributions and can fail to give accurate\nclassification when marginal distributions do not enjoy finite variance. Their\nsensitivity to correlation among data components, particularly in very highdimensional contexts, is not well understood. And their performance in highdimensional, highly heterogeneous cases, where the tails of distributions can\nvary from very light to very heavy within the same data vector, is largely\nunknown.\nThese phenomena occur often in the area of gene microarray analysis.\nEach microarray represents thousands of gene expression levels, but the\nsample size is typically small. Furthermore, the underlying distributions of\nthe gene expressions levels are generally unknown and are likely to be heterogeneous, heavy-tailed and significantly dependent upon each other. With\nthese features, conventional nearest-neighbor methods for analysis are likely\nto be ineffective.\nIn this paper, we shall suggest a robust nearest-neighbor classifier, where\nthresholding and truncation to zeros and ones are used to increase performance and, in particular, to remove sensitivity to heavy-tailed behavior.\nChoosing the threshold appropriately is the key to good classification accuracy. Threshold selection must adapt both to distribution type and to the\nways in which populations differ from one another. We suggest a simple\nand practicable approach to selecting the threshold. Unlike cross-validation,\nour technique gives good performance even when there is only one training\ndata-vector from each population.\nWe shall use theoretical arguments and numerical simulation to show that\nour technique is relatively insensitive to dependence among vector components, and that it enjoys good classification accuracy in high-dimensional,\nhighly heterogeneous cases. In settings such as these, the performance of\ntruncated nearest-neighbor classifiers can surpass that of competitors, such\nas methods based on extrema or on false-discovery rate (FDR) ideas. The\nlatter two approaches are often identical; see Jin [14] and Donoho and Jin [5].\nNearest-neighbor methods are popular because of the wide variety of data\ntypes for which they are appropriate. Their implementation requires only\na measure of distance and, in particular, is not founded on distributional\nproperties of the data. Therefore, nearest-neighbor classifiers enjoy a high\ndegree of acceptance in settings involving complex data, for example, in\npattern recognition. See Dasarathy [3] and Shakhnarovich, Darrell and Indyk\n[17], for instance.\nProperties of nearest-neighbor classifiers in classical settings, where dimension is small relative to sample size, are quite well understood. See, in\nparticular, Devroye, Gy\u00f6rfi and Lugosi [4]. Chapter 5 of that monograph is\nan excellent guide to the literature. There is a very large number of papers\non nearest-neighbor methods in other settings, and it is possible to mention\n\n\fROBUST NEAREST-NEIGHBOR METHODS\n\n3\n\nonly a few of them here. Early contributions include those of Cover and Hart\n[2] and Cover [1], who gave upper bounds to risk, and Wagner [18] and Fritz\n[7], who derived convergence properties of the error rate. Psaltis, Snapp and\nVenkatesh [16] extended Cover's [1] results to higher dimensional settings,\nbut still with the number of dimensions much less than sample size. Kulkarni\nand Posner [15] and Holst and Irle [10] discussed the case of dependent data\nvectors.\nIn these relatively classical treatments, it is common to regard the order,\nk, of a nearest-neighbor classifier as a tuning parameter and, perhaps, to\nattempt to optimize over it. However, in a variety of contemporary applications the number of data in each sample is so small, especially relative\nto dimension, that there is little point in taking k larger than 1. We argue\nthat, in such cases, the information that is critical to good performance is\naccumulated not through the number of data, but through the many components of each data vector. With that in mind, in this paper we shall optimize\nperformance in a way that is sensitive to dimension, rather than to sample\nsize.\n2. Methodology.\n2.1. Sparsity and truncation. Assume we observe random p-vectors\nX1 , . . . , Xm and Y1 , . . . , Yn , drawn from X- and Y -populations, respectively.\nWe wish to construct a classifier, for the purpose of ascribing a new p-vector,\nZ say, to either population.\nSuppose it is known that the respective components of X and Y distributions are similar, except that one of them has, for a potentially sparsely\narrayed sequence of component indices, generally higher mean than the\nother. We can formalize at least part of this assumption, by asking that,\nif X = (X (1) , . . . , X (p) )T and Y = (Y (1) , . . . , Y (p) )T , then,\n(2.1)\n\nfor each k, (a) X (k) \u2212E(X (k) ) and Y (k) \u2212E(Y (k) ) have similar distributions, and (b) E(Y (k) ) \u2265 E(X (k) ); and, for a potentially sparsely\ndistributed sequence of indices k, (c) E(Y (k) ) > E(X (k) ).\n\nThe one-sided nature of parts (b) and (c) of (2.1) motivates a one-sided\nclassifier. Alternative methodology and theory, very similar to that which\nwe shall develop below, are available in the two-sided case.\nIn view of the possible sparsity, it seems reasonable to truncate components of the data vectors by deleting those that do not attain a threshold,\nt say. This has the effect of reducing the amount of noise that is present in\ncoordinate values that convey little or no information for classification.\nThere are a variety of ways of implementing a procedure such as this.\nFor example, we may do it by replacing each component by 0 if it is less\n\n\f4\n\nY.-B. CHAN AND P. HALL\n(k)\n\n(k)\n\n(k)\n\n\u2212 K (k) )2 ,\n\n(k)\n\nthan t, and by 1 otherwise. That is, defining Ii = I(Xi > t), Ji =\n(k)\n(1)\n(p)\n(1)\n(p)\nI(Yi > t), Ii = (Ii , . . . , Ii )T and Ji = (Ji , . . . , Ji )T , we may build\nthe classifier using the indicator data vectors I1 , . . . , Im , J1 , . . . , Jn . Alterna\u2032 , Y \u2032 , . . . , Y \u2032 , where X (k) \u2032 =\ntively, we could base the classifier on X1\u2032 , . . . , Xm\nn\n1\ni\n(1)\n(p)\n(k)\n(k)\n(k)\n(k)\n(k)\nXi I(Xi > t), Yi \u2032 = Yi I(Yi > t), Xi\u2032 = (Xi \u2032 , . . . , Xi \u2032 )T and Yi\u2032 =\n(1)\n(p)\n(Yi \u2032 , . . . , Yi \u2032 )T .\nUsing the indicator data, we could conclude that Z came from the X\npopulation if\n(2.2)\n\nmin\n\n1\u2264i\u2264m\n\np\nX\n\n(k)\n\n(Ii\n\n\u2212 K (k) )2 \u2264 min\n\n1\u2264i\u2264n\n\nk=1\n\np\nX\n\n(Ji\n\nk=1\n\nwhere K (k) = I(Z (k) > t); and that Z was from the Y population otherwise.\nAlternatively, in place of (2.2) we could use the criterion\n(2.3)\n\nmin\n\n1\u2264i\u2264n\n\np\nX\n\n(k) \u2032\n\n(Xi\n\n\u2212 Z (k)\u2032 )2 \u2264 min\n\n1\u2264i\u2264m\n\nk=1\n\np\nX\n\n(k) \u2032\n\n(Yi\n\n\u2212 Z (k)\u2032 )2 ,\n\nk=1\n\nwhere Z (k)\u2032 = Z (k) I(Z (k) > t). In this case, if (2.3) were true, then we would\nconclude that Z was from the X population. However, relative to methods\nbased on (2.2), this approach would suffer more from stochastic variability and, hence, be less robust, in cases where X and Y had heavy-tailed\ndistributions.\n2.2. Empirical choice of t. We suggest a method based on thresholding,\nas follows. Let iX and iY denote the respective values of i at which the minima on the left- and right-hand sides of (2.2) are achieved. In this notation,\n(2.2) is equivalent to T \u2264 0, where\n(2.4)\n\nT = T (t) =\n\np\nX\n\n(k)\n\n(k)\n\n(IiX \u2212 JiY )(1 \u2212 2K (k) ).\n\nk=1\n\nLet \u03bep denote a sequence diverging to infinity; put\n(2.5)\n\nzp = \u03bep log p,\n\ndenoting a threshold; let\n2\n\n2\n\nS = S(t) =\n\n(2.6)\n\np\nX\n\n(k)\n\n(k)\n\n(IiX + JiY )\n\nk=1\n\nand define t = \u03b8 by:\n(2.7)\n\n\u03b8 is the infimum of values t \u2265 0 such that |T (t)|/S(t) > zp ; or, if\nno such t exists, take t to be a default value, for example, t = 0 or\nt = \u2212\u221e.\n\n\f5\n\nROBUST NEAREST-NEIGHBOR METHODS\n\nIn the case of independent components, it is feasible to use a smaller\nthreshold, defining zp by\nzp = \u03bep (log p)1/2 ,\n\n(2.8)\n\nwhere \u03bep \u2192 \u221e, instead of by (2.5). Nevertheless, (2.5) is also appropriate in\nthe case of independence. If, in (2.7), it were necessary to pass to the default\nvalue, then we would conclude that the classification problem was marginal.\nThat is, there was insufficient information to solve the problem reliably.\nWith t = \u03b8 given by (2.7), the classifier suggested by (2.2) is as follows:\n(2.9)\n\nClassify Z as coming from the X population, if T (\u03b8) \u2264 0 and as\ncoming from the Y population otherwise.\n\nOur theoretical justification for (2.5) will be based on the assumption\nthat the components X (k) and Y (k) are produced by a generalized form of\nan infinite-order moving average. The generalization permits marginal distributions to vary extensively from one component to the next, so that they\nare heavy-tailed for some indices k but light-tailed for others. Alternative\nmodels for weak dependence, for example based on autoregressive processes,\ncan be shown to also lead to the threshold choice at (2.5).\nFrom at least a theoretical viewpoint, exact choice of \u03bep is largely unimportant. Any sequence, for example, \u03bep = log p, which diverges more slowly\nthan any polynomial is appropriate. In this way, the sensitivity of the tuningparameter selection problem is greatly reduced; we pass from the parameter\nt, to which the classifier is very sensitive, to \u03bep , to which the classifier is\nlargely insensitive. Practical, empirical choices of \u03bep will be discussed in\nSection 4.\nMotivation for a threshold-based approach to choosing t can be provided\nas follows. Neglecting, for the moment, the fact that iX and iY at (2.4)\n(k)\n(l)\nare random variables; taking the components IiX and JiY to be completely\nindependent, for each k and l, and conditioning on the new data vector Z;\nthe random variable T , at (2.4), is seen to have variance equal to\n(2.10)\n\np\nX\n\n(k)\n\n(k)\n\nvar(IiX \u2212 JiY )(1 \u2212 2K (k) )2 =\n\nk=1\n\np\nX\n\n(k)\n\n(k)\n\n{var(IiX ) + var(JiY )},\n\nk=1\n\nwhere the identity follows from the independence assumed earlier in this\nparagraph and from the fact that 1 \u2212 2K (k) = \u00b11. Under the assumptions,\n(k)\n(k)\n(k)\n(k)\nvar(IiX ) = (EIiX )(1 \u2212 EIiX ) \u2264 E(IiX ), with an analogous result holding\n(k)\n\nfor var(IiY ). Therefore, S 2 , at (2.6), tends to overestimate the right-hand\nside of (2.10):\nE(S 2 ) \u2265\n\np\nX\n\n(k)\n\n(k)\n\n{var(IiX ) + var(JiY )}.\n\nk=1\n\n\f6\n\nY.-B. CHAN AND P. HALL\n\nThis slight conservatism, and the log factor in the threshold zp , provide\nopportunities for repairing errors that arise from failure of the independence\nassumption.\nIn the argument above, we defined T to be the difference between the two\nsides of (2.2), rather than between the two sides of (2.3). Indeed, it can be\nawkward to estimate the variance of T if we use (2.3) and do not have a good\nmodel for the distributions of X (k) and Y (k) . There are ways of overcoming\nthis difficulty, but we do not find them as attractive as working with (2.2).\nAn alternative approach to selecting t could be based on standard crossvalidation, taking \u03b8 to be the infimum of values t that minimize the error-rate\nestimator,\nCV(t) =\n\nm\n1 X\nI min kXi\u20321 \u2212 Xi\u2032 k > min kYj\u2032 \u2212 Xi\u2032 k\n1\u2264j\u2264n\nm i=1 i1 6=i\n\n\u0012\n\n\u0013\n\nn\n1X\n+\nI min kY \u2032 \u2212 Yj\u2032 k > min kXi\u2032 \u2212 Yj\u2032 k .\n1\u2264i\u2264m\nn j=1 j1 6=j j1\n\n\u0012\n\n\u0013\n\nHowever, this technique has the disadvantage that it works only when m\nand n both exceed 1. Moreover, in most problems there is a continuum of\nvalues of t that minimize CV(t), and so cross-validation does not give an\nexplicit answer to the tuning-parameter choice problem.\n2.3. Example of mixed light- and heavy-tailed components. When both\nlight- and heavy-tailed data components are present in each data vector, and\nonly a very small proportion of the components differ through perturbations,\nit can be particularly difficult to achieve good classification using standard\ndistance-based methods, such as support vector machines. In the case of\nthese approaches, the accumulation of noise from irrelevant components can\ndrown out the signal in those few components that convey information for\nclassification. Methods such as FDR, based on extrema, can bring substantial improvements in performance. However, when data distributions are\nheterogeneous, those techniques too can have difficulty.\nTo illustrate this point, assume for the sake of simplicity that all vector\ncomponents are mutually independent. Suppose that X consists of just p1\u2212\u03b2\ncomponents with standard normal distributions, where \u03b2 \u2208 (0, 1), and p \u2212\np1\u2212\u03b2 components having exponential distributions, for which P (X (k) > x) =\ne\u2212x when x \u2265 0. Construct the Y variable by adding \u03bc = r log p, where r > 0,\nto just p1\u2212\u03b2 of the components of X, leaving the others unaltered.\nIf these special p1\u2212\u03b2 components are among those that have an exponential distribution, then we can write\n(2.11)\n(2.12)\n\nmax X (k) = Q1 + log p + op (1),\n\n1\u2264k\u2264p\n\nmax Y (k) = max{Q1 + log p, Q2 + (1 \u2212 \u03b2 + r) log p} + op (1),\n\n1\u2264k\u2264p\n\n\fROBUST NEAREST-NEIGHBOR METHODS\n\n7\n\nwhere Q1 and Q2 are asymptotically independent and have the extremevalue distribution function exp(\u2212e\u2212x ). In both these expansions, we can\nconsider Q1 + log p to equal the maximum of the p \u2212 2p1\u2212\u03b2 components\nof X that have an exponential distribution and which exclude the p1\u2212\u03b2\ncomponents to which the perturbation \u03bc is added to form Y , and Q2 +\n(1 \u2212 \u03b2 + r) log p to be the maximum of the p1\u2212\u03b2 components of Y that are\nobtained by perturbing components of X.\nIt follows from (2.11) and (2.12) that if r > \u03b2, then maxk Yk \u2212 maxk Xk \u2192\n\u221e. More specifically, when r > \u03b2, the maximum of the components of a new\nvector Z can be used to obtain asymptotically correct classification. This\nresult does not hold if r \u2264 \u03b2.\nOn the other hand, if the perturbation \u03bc is added to each of the p1\u2212\u03b2\ncomponents of X that have a normal N (0, 1) distribution, and if none of the\nexponentially distributed components of X is perturbed, then\nmax Y (k) = max[Q1 + log p, {2(1 \u2212 \u03b2)(log p)}1/2 + r log p] + op (1)\n\n1\u2264k\u2264p\n\nand P (maxk Xk = maxk Yk ) \u2192 1, unless r \u2265 1. In particular, in this case, only\nwhen r \u2265 1 is it possible to discriminate between the X and Y populations\nusing extrema or FDR.\nBy way of contrast, we shall show in Section 3 that, no matter where the\nperturbations are added, the nearest-neighbor method produces asymptotically correct classification whenever r > 2\u03b2 \u2212 1. Since 1 > \u03b2 > 2\u03b2 \u2212 1, then\nthe nearest-neighbor classifier enjoys greater sensitivity than the method\nbased on extrema or FDR, no matter whether the perturbations are added\nto light- or heavy-tailed data components.\n2.4. Discussion of nearest-neighbor methods. The versatility, performance\nand simplicity of NN classifiers are important factors in their popularity.\nAs we show in this paper, NN methods also have significant potential for\n\"robustification\" and for fine-tuning through thresholding; both of these\nmodifications lead to further improvements in performance. Nevertheless,\nwell-known caveats about NN techniques should be mentioned.\nNearest-neighbor algorithms are most clearly suited to problems where\nthe major departures among distributions are the results of differences in\nmeans, rather than differences in variances. To appreciate why NN classifiers\ncan face challenges when the differences are principally in terms of variance,\nconsider the elementary case where the variables X (k) , for 1 \u2264 k \u2264 p, are\n2 ;\nindependent and identically distributed with zero mean and variance \u03c3X\n2 < \u03c32 .\nthe Y (k) 's are likewise i.i.d., with zero mean and variance \u03c3Y2 ; and \u03c3X\nY\nIf Z comes from population X then, as p increases, the probability that the\ninequality\np\n\n(2.13)\n\np\n\n1 X (k)\n1 X (k)\n(X \u2212 Z (k) )2 <\n(Y \u2212 Z (k) )2\np k=1\np k=1\n\n\f8\n\nY.-B. CHAN AND P. HALL\n\nholds tends to 1, since the left-hand side and right-hand side are, respectively,\n2 + o (1) and \u03c3 2 + \u03c3 2 + o (1). The probability that (2.13) holds\nequal to 2\u03c3X\np\np\nX\nY\nwhen Z is from population Y also converges to 1, since in this setting the two\n2 + \u03c3 2 + o (1) and 2\u03c3 2 + o (1). Therefore,\nsides of (2.13) are, respectively, \u03c3X\np\np\nY\nY\nno matter what population Z is from, the simple NN classifier will, with\nprobability converging to 1 as p \u2192 \u221e, assign Z to the population with\nsmaller variance, that is, to population X. This will hold true for samples\nof any sizes m and n, provided those quantities are kept fixed as p diverges.\nThe result is quite different if the two populations have equal variances\nbut unequal means. There, the probability that Z is correctly allocated by a\nNN classifier typically converges to 1 as p \u2192 \u221e, if there are sufficiently many\nsufficiently large differences among means. Although in Section 3 we shall\npermit distributions to take very different forms among components, the\ndifferences with real leverage for classification will be those among means.\nThe process of thresholding, which converts continuous measurements into\nzero\u2013one data, tends to remove problems caused by differences among variances, although to some extent it converts differences among means into\ndifferences among variances; recall that a zero\u2013one variable with mean q has\nvariance q(1 \u2212 q). However, as we shall show in Section 3, this does not cause\nsignificant difficulty.\n3. Theoretical properties.\n3.1. Summary. The models that we shall use to describe the X and\nY vectors will differ through perturbations (location changes), \u03bc(k) , added\nto individual components. The models will be constructed so as to admit\nconsiderable heterogeneity among the distributions, as well as to allow dependence; see Section 3.6 for discussion of the latter. In Sections 3.2 and\n2.3 we shall describe the density, size and scalability of the perturbations\nand marginal distributions. Classification boundaries will be discussed in\nSection 3.4. The principles introduced there will dictate the context of the\nmain theoretical results given in Sections 3.5 and 3.6. These results will\nreflect difficult classification problems, where configurations are close to optimal classification boundaries. Our main theorems will be stated under the\nassumption that the number of dimensions, p, diverges, while the sample\nsizes, m and n, are held fixed.\n3.2. Relationship between marginal distributions of X and Y . For sequences bp and cp depending on p, we write bp \u224d cp to mean that the ratio\nbp /cp is bounded above zero and below infinity, as p diverges. Given a sequence ap diverging to infinity, and a constant \u03b2 \u2208 ( 12 , 1), we shall say that\n(3.1)\n\nthe sequence \u03bc(1) , . . . , \u03bc(p) \"has asymptotic density p\u2212\u03b2 and is on\nthe scale ap ,\" if (i) the number, Np say, of nonzero \u03bc(k) 's satisfies\nNp \u224d p1\u2212\u03b2 and (ii) none of the nonzero \u03bc(k) 's is less than ap .\n\n\fROBUST NEAREST-NEIGHBOR METHODS\n\n9\n\nThe perturbations \u03bc(k) will be added to the respective components of X\nto create a vector with the distribution of Y . Therefore, our model for the\nway in which the marginal distributions of X and Y are related will be that\n(3.2)\n\nfor 1 \u2264 k \u2264 p, Y (k) is distributed as X (k) + \u03bc(k) , where the sequence\n\u03bc(1) , . . . , \u03bc(p) has asymptotic density p\u2212\u03b2 and is on the scale ap , with\n\u03b2 \u2208 ( 21 , 1).\n\nCondition (3.2) relates only to the number of \u03bc(k) 's that are different from\nzero, not to the order of the nonzero values in the sequence \u03bc(1) , . . . , \u03bc(p) . In\nparticular, the assumption is much less stringent than it would be if it were\nsupposed that the indices of the nonzero \u03bc(k) 's were distributed according\nto a particular random process. The latter constraint is implicit whenever\nmixture models are assumed.\nIn (3.1) and (3.2), we choose \u03b2 \u2208 ( 12 , 1) since classification in the case \u03b2 \u2264 21\nis relatively easy (indeed, root-n consistent estimation is generally possible\nwhen \u03b2 < 12 ), and since nontrivial, asymptotically correct classification is\nimpossible if \u03b2 = 1.\n3.3. Scalability. We shall use the phrase, \"the marginal distributions of\nX are continuous and scalable,\" to mean that, for each r \u2208 (0, 1), the equation\n(3.3)\n\np\nX\n\nP (X (k) > ap ) = p1\u2212r\n\nk=1\n\nhas a unique solution ap = ap (r), and that for each \u03b5 \u2208 (0, r) there exists\nC = C(\u03b5) \u2208 (0, 1) such that, for all sufficiently large p,\n(3.4)\n\np\nX\n\nP (X (k) > Cap ) \u2264 p1\u2212r+\u03b5 .\n\nk=1\n\nIn particular, if the X (k) 's are identically distributed as X (0) , then the\ncommon distribution is scalable if, when ap is defined by P (X (0) > ap ) = p\u2212r ,\nfor each \u03b5 \u2208 (0, r) there exists C \u2208 (0, 1), such that P (X (0) > Cap ) \u2264 p1\u2212r+\u03b5 .\nScalable distributions include the normal, and other exponentially decreasing distributions such as the Subbotin, with probability density function f\ngiven by\n(3.5)\n\nf (x) = C\u03b3\u22121 exp(\u2212|x|\u03b3 /\u03b3),\n\nwhere \u03b3 > 0 and C\u03b3 = 2\u0393(1/\u03b3)\u03b3 (1/\u03b3)\u22121 . See Donoho and Jin [5] for an account of the interest in, and applications of, the Subbotin distribution. Scalable distributions also include regularly varying distributions such as the\nPareto, for which\n(3.6)\n\nP (X (k) > x) = x\u2212\u03b3 ,\n\n\f10\n\nY.-B. CHAN AND P. HALL\n\nwhen x > 1, where \u03b3 > 0. Nonscalable distributions have extremely light\nupper tails, for example, the extreme-value distribution for which P (X >\nx) = exp(\u2212ex ).\nOf course, scalability of the marginal distributions of X does not require\nthe X (k) 's to be identically distributed. A particularly simple, nonidentically distributed example is that where N1 (p) of the components X (k) are\ndistributed as X (0) , say; the other N2 (p) = p \u2212 N1 (p) components have distribution functions that dominate that of X (0) , in the sense that P (X (k) \u2264\nt) \u2265 P (X (0) \u2264 t) for all 1 \u2264 k \u2264 p and all t \u2265 t0 , say; the distribution of X (0)\nis scalable, in the sense described in the previous paragraph; and N1 (p) \u223c p\nas n \u2192 \u221e. This model, and Theorems 1 and 2 below, permit a rigorous\naccount of performance of the nearest-neighbor classifier in the context of\nthe examples discussed in Section 2.3.\n3.4. Detection and classification boundaries. In this subsection, we assume that all the marginal distributions of X are identical to that of X (0) ,\nsay, and we take each of the p1\u2212\u03b2 nonzero values of \u03bc(k) to equal ap , defined\nby P (X (0) > ap ) = p\u2212r , where \u03b2 \u2208 ( 12 , 1) and r \u2208 (0, 1). Theorems 1 and 2,\nbelow, imply that in this case the robust nearest-neighbor classifier defined\nby (2.9) will asymptotically correctly classify data, provided that\n(3.7)\n\n1 \u2212 2\u03b2 + r > 0.\n\nThat is, if (3.7) holds, and even when m = n = 1 (i.e., when there is only one\ntraining data value from each population), the probability that the classifier\nat (2.9) correctly assigns Z, no matter whether it comes from the X or the\nY population, converges to 1 as p \u2192 \u221e.\nConversely, if (\u03b2, r) lies strictly below the boundary described by the line\n(3.8)\n\n1 \u2212 2\u03b2 + r = 0,\n\nthen the probability of correct classification fails to converge to 1. Moreover,\nthe same boundary plays the same role (i.e., as the border that separates\nclassifiable and nonclassifiable cases) if we use a truncated standard nearestneighbor method. The latter technique requires the data distributions to\nhave several finite moments, whereas the approach suggested in our paper\nis far more robust than conventionally truncated nearest-neighbor methods.\nIt is significant that the boundaries are identical in the cases of robust and\nnonrobust nearest-neighbor methods. In particular, the greater robustness\nof our approach does not come at the price of reduced effectiveness.\ntr =\nTo define standard truncated nearest-neighbor classifiers, let Xij\ntr\ntr\nXij I(Xij > t), Yij = Yij I(Yij > t) and Zj = Zj I(Zj > t), respectively, where\nt denotes the truncation point. The corresponding truncated vectors are\ntr ), Y tr = (Y tr ) and Z tr = (Z tr ). We apply the standard nearestXitr = (Xij\nj\nij\ni\ntr } and {Y tr , . . . , Y tr },\nneighbor classifier to the truncated datasets {X1tr , . . . , Xm\nn\n1\n\n\fROBUST NEAREST-NEIGHBOR METHODS\n\n11\n\ninstead of to the original data. That is, we assign Z to the X population\nif Z tr is nearer to at least one of Xitr 's than it is to any of the Yitr 's, and\nwe assign it to the Y population otherwise. Assume that the random variables Xi1 j1 \u2212 E(Xi1 j1 ) and Yi2 j2 \u2212 E(Yi2 j2 ) are all independent and identically distributed, with the distribution of U , say, and that the scalability condition holds. It can be proved that if q = p1\u2212\u03b2 ; if the truncation\npoint t does not exceed \u03bd; if \u03bd = ap , where ap satisfies (3.3) [or equivalently,\nP (U > ap ) = p\u2212r ]; and if (\u03b2, r) lies strictly below the boundary given by\n(3.8); then pE{U 4 I(U > t)}/(q\u03bd 2 )2 is bounded away from zero. Moreover,\nit is shown by Hall, Pittelkow and Ghosh [8] that if, along a subsequence\nof values of p, pE{U 4 I(U > t)}/(q\u03bd 2 )2 does not converge to zero, then the\nprobability of correct classification fails to converge to 1. Similarly, if (\u03b2, r)\nlies above the boundary, then the probability of correct classification converges to 1. This establishes the implications of the boundary in the case of\nstandard truncated nearest-neighbor classifiers, and its implications for our\ntruncated form are similar.\nIn some problems, and when m = n = 1, the boundary at (3.8) is identical\nto that for an optimal classifier, implying that the robust nearest-neighbor\napproach has asymptotically optimal performance. However, the classifiers\nfor which this boundary is known require the marginal distribution to be\nknown; our truncated, thresholded nearest-neighbor approach is not subject\nto that requirement.\nFor example, in the Subbotin case represented by (3.5), with 0 < \u03b3 \u2264 1;\nand in the Pareto case given by (3.6), when \u03b3 > 0; it is known [5, 14] that the\nboundary represented by (3.8) is the optimal boundary for signal detection.\nIt can be proved from this result that it is also the optimal boundary for\nclassification, when m = n = 1. In the Subbotin case where \u03b3 > 1, alternative\nmethods, such as Donoho and Jin's [5] higher-criticism method and the\napproaches suggested by Ingster [11, 12, 13], give a lower optimal boundary\neven when m = n = 1 and, hence, permit classification in cases where robust\nnearest-neighbor methods do not.\n3.5. Case of independent components. The error rates of the classifier at\n(2.8) are defined to be the probability that Z is misclassified as coming from\nY when it is really from X and the probability of misclassification of Z as\ncoming from X when it is actually from Y .\nNote that, if t is sufficiently small, then it is possible to have P (X (k) >\nt) = P (Y (k) > t) = 1, uniformly in 1 \u2264 k \u2264 p and in p. In this case, the ratio\nT (t)/S(t) is not well defined. To remove pathologies such as this, we modify\nthe definition of \u03b8, at (2.7), by insisting that, for some fixed t0 sufficiently\nlarge, only values t \u2265 t0 be considered. In the theorem below, we hold m and\nn fixed and let p increase without bound.\n\n\f12\n\nY.-B. CHAN AND P. HALL\n\nTheorem 1. If the components of X are independent, and the components of Y are independent; if the marginal distributions are related by (3.2),\nand are continuous and scalable; if, for r \u2208 (0, 1), the quantity ap = ap (r),\ndefined by (3.3), diverges to infinity but at a rate no faster than pD for some\nD > 0, as p increases; if the pair (\u03b2, r) is above the classification boundary,\nin the sense that (3.7) holds; and if zp is given by (2.8), where \u03bep diverges\nmore slowly than p\u03b5 for each \u03b5 > 0; then, as p \u2192 \u221e for fixed m and n, the\nerror rates of the classifier at (2.9) converge to zero.\nThe assumption in Theorem 1 that ap = O(pD ), for some D > 0, is satisfied if, for example, supk E(|X (k) |\u03b5 ) < \u221e for some \u03b5 > 0.\n3.6. Case of dependent components. As in (3.2), we take the distributions of the components of Y to be translations of those of the respective\ncomponents of X. In particular, given stochastic processes U1 , . . . , Up and\nU1# , . . . , Up# , each with the same p-variate distribution, we define\n(3.9)\n\nX (k) = Uk + \u03bdk ,\n\nY (k) = Uk# + \u03bdk + \u03bc(k) .\n\nThe challenge is to model the degree of dependence among marginals and,\nat the same time, to permit the marginal distributions to vary in shape, as\nwell as location, from one component to another. This is done through an\nexponentiated moving average process, defined in part (a) of (3.10):\n\u03b1k\n, where the nonnegative random variables\n(a) Uk = j\u22651 \u03c9j Wj+k\nWj are independent and identically distributed as W ; (b) for all w,\nP (W \u2264 w) < 1; (c) for some c > 0, E(W c ) < \u221e; (d) the distribution\nof W has a bounded probability density; (e) the constants \u03b1k are\npermitted to be functions of p as well as k, and for some C > 1, for\nall p and for all 1 \u2264 k \u2264 p, C \u22121 \u2264 \u03b1k \u2264 C; (f) for some C > 0, for\nsome \u03c9 \u2208 (0, 1) and for all j \u2265 1, |\u03c9j | \u2264 C\u03c9 j ; and (g) at least one \u03c9j\nis strictly positive.\n\nP\n\n(3.10)\n\nThe \u03bdk 's are taken to be uniformly bounded, and the \u03bc(k) 's to have properties similar to those at (3.2):\n\n(3.11)\n\n(a) \u03bdk and \u03bc(k) are functions of p as well as k; (b) for a fixed constant C > 0, |\u03bdk | \u2264 C for all p and for all 1 \u2264 k \u2264 p; and (c) given\nr \u2208 (0, 1) and \u03b2 \u2208 ( 21 , 1) and with ap defined by (3.3), the sequence\n\u03bc(1) , . . . , \u03bc(k) has asymptotic density p\u2212\u03b2 and is on the scale ap .\n\nThe \"continuity\" part of the assumption, in Theorem 1, that the marginal\ndistributions of X are continuous and scalable, is taken care of by (3.10)(d).\nHowever, we also need scalability, as well as a version of that condition in the\n\n\fROBUST NEAREST-NEIGHBOR METHODS\n\n13\n\ncase of logarithmically spaced marginals. For the latter, (3.12) is sufficient:\ndefining \u03c0k (t) = P (X (k) \u2265 t), we ask that\n(3.12)\n\nfor each B, \u03b5 > 0, there exists t\u2032 = t\u2032 (B,\n\u03b5) such that, if lp deP\nnotes the integer part of B log p, then p\u03b5 0\u2264k\u2264(p\u2212h)/lp \u03c0klp +h (t) \u2265\nP\n\u2032\n1\u2264k\u2264p \u03c0k (t) for all 0 \u2264 h \u2264 lp and all t \u2265 t .\n\nIn assumption (3.10), parts (a) and (e) imply that the Uk process is a generalized moving average with geometrically decaying coefficients. The generalization, through raising Wj+k to the power \u03b1k , allows the distribution of\nUk to be varied substantially from one component to another. In particular,\nthe tail weights can be very different; smaller \u03b1k 's give distributions with\nlighter tails.\nTo interpret parts (b) and (g) of (3.10), note that if P (Uk \u2264 C) = 1 for\nsome C > 0 and for all k, then the problem of discriminating between X\nand Y , on the basis of location shifts to the right, is relatively simple. Part\n(b), which asserts that the upper tail of the distribution of W is unbounded,\n\u03b1k\nto Uk\ntogether with (g), which asks that at least one contribution \u03c9j Wj+k\nbe positive, permit us to avoid this degeneracy. Part (c) of (3.10) is a very\nweak moment assumption and, in particular, permits the distribution of W\nto be so heavy tailed that it lies in the domain of attraction of a stable law.\nIn (3.11), parts (a) and (b) permit the \u03bdk 's to vary quite generally, subject\nonly to being bounded. Condition (3.12) holds true trivially if the marginal\ndistributions are all identical and can be shown to be valid under other\nheterogeneous models.\nTheorem 2, below, is a version of Theorem 1 for dependent data. As in\nthe case of Theorem 1, we modify the definition of \u03b8, at (2.7), by considering\nonly values t \u2265 t0 , for t0 fixed but sufficiently large.\nTheorem 2. If the joint distributions of the components of X and Y\nare given by (3.9), with the quantities there generated as described by (3.10)\nand (3.11); if the marginal distributions of X (k) are scalable, and satisfy\n(3.12); if, for r \u2208 (0, 1), the quantity ap = ap (r), defined by (3.3), diverges\nno faster than pD for some D > 0, as p increases; if the pair (\u03b2, r) lies above\nthe classification boundary, in the sense that (3.7) holds; and if zp is given by\n(2.5), where \u03bep diverges more slowly than p\u03b5 for each \u03b5 > 0; then, as p \u2192 \u221e\nfor fixed m and n, the error rates of the classifier at (2.9) converge to zero.\n4. Numerical properties.\n4.1. Microarray data. As a practical example, we compared the performance of the thresholded method with the nearest-neighbor method on the\nBRCA dataset [6, 9], which we obtained from http://www.nejm.org/general/\n\n\f14\n\nY.-B. CHAN AND P. HALL\n\ncontent/supplemental/hedenfalk/index.html. This dataset contains microarray data from patients with breast cancer, caused by two different types of\nmutations, labelled BRCA1 and BRCA2. The expression level of each of\n3226 genes was measured in each patient, and there are 7 patients with\nBRCA1 and 8 patients with BRCA2.\nThis dataset (and indeed many gene microarray datasets) is very suited to\nour thresholded method. For a start, it is a dataset with very high dimension\nand low sample size. Furthermore, it is expected that only a few genes will\nbe differentially expressed between the two types of cancer, so the difference\nbetween the populations is sparse. Lastly, the underlying distributions of\nthe gene expressions are likely to be both heavy-tailed and with significant\ndependence among genes, which nearest-neighbor traditionally does poorly\nat, especially in comparison with the thresholded method.\nWe tested the two methods on this dataset by calculating the crossvalidation performance, where we classify each patient according to all the\nother patients and calculate the classification rate. For the nearest-neighbor\nmethod, cross-validation correctly classified 11 out of the 15 patients. Our\nthresholded method did a lot better; with zp = 0.5(ln p)1/2 , all 15 patients\nwere classified correctly under cross-validation. In fact, this happened when\nwe set the coefficient of (ln p)1/2 in zp to be anywhere between 0.35 and 0.5.\n4.2. Simulated data. As an additional test, we also compared the thresholded method with the nearest-neighbor method for simulated data. We\ncompare the two methods in the area of the \u03b2\u2013r plane where classification\nis possible (r > 2\u03b2 \u2212 1), but not easy (\u03b2 < 12 or r > 1). Overall, we found\nthat in cases where standard nearest-neighbor does not perform well, the\nthresholded method improves on it. We look at some of these cases.\n4.2.1. Independent heavy-tailed marginal distributions. Nearest-neighbor\nmethods do not do very well when the marginal distributions of the components of X (and Y ) are heavy-tailed (i.e., go to 0 slower than a normal distribution). We compared the methods for simple models where m = n = 1 and\neach of the components of X are independent and have identical Student's-t\ndistributions. By varying the degrees of freedom, we can observe the behavior of the methods relative to the heaviness of the tails.\nFor this case, if we are given a threshold t, the success rate of the algorithm\ncan be approximated very accurately, for any \u03b2 and r, by looking at the\ncontribution of each dimension to T (t). By varying t we can calculate the\noptimal threshold, which we call the a priori optimal threshold, and also\nthe best possible performance of the classifier. However, we are not usually\ngiven the threshold, so this is an upper limit on the success rate. Instead,\nwe compare the classifiers with empirically chosen thresholds, on simulated\ndata with p up to 20,000.\n\n\fROBUST NEAREST-NEIGHBOR METHODS\n\n15\n\nWe found that, for sufficiently heavy tails, the thresholded method dominates standard nearest-neighbor in all areas of the \u03b2\u2013r plane. In fact, the\nsuccess rate of the thresholded method actually improves for heavier tails.\nAs the tails get lighter (the d.f. gets larger), the success rate declines, and\nnearest-neighbor does better in a small area in the plane, which grows and\nmoves around as the tail weight decreases. For small d.f., this area occurs\nat high \u03b2 and r [see Figure 1(a)]; for larger d.f., this area occurs at low \u03b2\nand r neither high nor low [see Figure 1(b)].\nThe thresholded method also dominates nearest-neighbor if we use the\na priori optimal thresholds, for sufficiently heavy tails. If the tails are not\nheavy enough, nearest-neighbor works better for low \u03b2 and r.\nWe found that the best performance of the thresholded method is achieved\nwhen we take zp in (2.7) to be c(ln p)1/2 , where c is a constant. The value of\nc, which maximizes the success rate, lies between 0.3 and 0.9, depending on\n\u03b2 and r. However, the best success rate achieved with an empirically chosen\nthreshold is worse than that achieved with the a priori threshold, because\nthe empirical threshold is not constant for constant zp . Figure 2 estimates\nthe distribution of the chosen threshold for various cases when zp is close to\noptimal. Figure 3 shows how the value of the threshold affects the success\nrate, while Figure 4 shows how the value chosen for zp affects the success rate.\nIn both of these figures, the curves represent the thresholded method, while\nthe horizontal lines show the performance of the nearest-neighbor method\nfor comparison.\n4.2.2. Dependent normal marginal distributions. Another case where standard nearest-neighbor methods perform badly is when the components of X\nare dependent on each other. We compared the methods for varying degrees\n\n(a)\n\n(b)\n\nFig. 1. Areas where the two methods perform better, for heavy-tailed distributions. The\nnearest-neighbor method performs better in the shaded area; otherwise, the thresholded\nmethod is better. (a) t distributions, d.f. = 4, (b) t distributions, d.f. = 10.\n\n\f16\n\nY.-B. CHAN AND P. HALL\n\nFig. 2. Estimated distribution of thresholds produced with t distributions at\nzp = 0.55(ln p)1/2 , p = 20,000, at various (\u03b2, r) and degrees of freedom.\n\nand types of dependence; for example, when the components of X are moving averages of independent standard normal variables, or weighted moving\naverages, or an autoregressive process X (i+1) = \u03b1X (i) + (1 \u2212 \u03b1)N (i) , where\nN is a sequence of independent standard normal variables.\nAgain, we found that for sufficient levels of dependence, the thresholded\nmethod dominates nearest-neighbor for all (\u03b2, r). For weaker levels of dependence, the nearest-neighbor method works better in a small area at small\n\u03b2 and r neither small nor large (see Figure 5), and this region grows with\ndecreasing dependence. We found that the strength of the dependence [e.g.,\ncov(X (i) , X (i+1) )] affects the size of this region more than the length of the\ndependence (the number of components of X dependent on a given component).\n\nFig. 3. Success rate vs.\np = 20,000, (\u03b2, r) = (0.7, 0.4).\n\nthreshold\n\n(as\n\na\n\nproportion\n\nof\n\nshift\n\namount)\n\nfor\n\n\fROBUST NEAREST-NEIGHBOR METHODS\n\nFig. 4.\n\n17\n\nSuccess rate vs. c for p = 20,000, (\u03b2, r) = (0.7, 0.4), where zp = c(ln p)1/2 .\n\nAs with the heavy-tailed case, taking zp = c(log p)1/2 optimizes the success\nrate, with c taking similar values as before. However, the overall success rate\nof the thresholded method is worse than for an equivalent independent case.\nThe behavior of the chosen threshold, and its effect on the success rate, is\nsimilar to its behavior for heavy-tailed distributions.\n4.2.3. Independent normal marginal distributions. For comparison, we\nalso looked at the case where the components of X were independent and\nnormally distributed. Here, the thresholded method does not dominate nearestneighbor, which works better for low \u03b2 (approximately \u03b2 < 0.65). This is\nconsistent with heavy-tailed distributions as the tails get lighter. The behavior of the chosen threshold, and its effect on the success rate, is again\nsimilar to its behavior for heavy-tailed distributions. The overall success\n\nFig. 5. Areas where the two methods work best, for moving averages of 5 normal random\nvariables.\n\n\f18\n\nY.-B. CHAN AND P. HALL\n\nrate is worse than for heavy-tailed distributions, but better than that for\ndependent distributions.\n4.2.4. Larger samples. The above scenarios all involved m = n = 1. As\nthe sample sizes m and n increase, but are kept equal, the classification\nsuccess rate of both methods increase. As m and n increase, the thresholded\nmethod outperforms the nearest-neighbor method for a greater range of the\n\u03b2\u2013r plane, although the difference is slight up to m = n = 10 (the upper\nlimit of our testing).\nWhen the sample sizes are not equal, the thresholded method performs\nbetter when m is smaller, if m + n is kept constant. In fact, although increasing m or n while keeping the other fixed generally increases the classification rate, it is possible to decrease the classification rate by increasing m while keeping n fixed (e.g., when n = 1). As the effectiveness of the\nnearest-neighbor method stays largely the same, the thresholded method\noutperforms the nearest-neighbor method for much larger areas of the \u03b2\u2013r\nplane, when m < n, and is much less effective for m > n.\nREFERENCES\n[1] Cover, T. M. (1968). Rates of convergence for nearest neighbor procedures. In\nProceedings of the Hawaii International Conference on System Sciences (B. K.\nKinariwala and F. F. Kuo, eds.) 413\u2013415. Univ. Hawaii Press, Honolulu.\n[2] Cover, T. M. and Hart, P. E. (1967). Nearest neighbor pattern classification.\nIEEE Trans. Inform. Theory 13 21\u201327.\n[3] Dasarathy, B. V. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification\nTechniques. IEEE Computer Society, Los Alamitos, CA.\n[4] Devroye, L., Gy\u00f6rfi, L. and Lugosi, G. (1996). A Probabilistic Theory of Pattern\nRecognition. Springer, New York. MR1383093\n[5] Donoho, D. J. and Jin, J. (2004). Higher criticism for detecting sparse heterogeneous mixtures. Ann. Statist. 32 962\u2013994. MR2065195\n[6] Efron, B. (2004). Large-scale simultaneous hypothesis testing: The choice of a null\nhypothesis. J. Amer. Statist. Assoc. 99 96\u2013104. MR2054289\n[7] Fritz, J. (1975). Distribution-free exponential error bound for nearest neighbor pattern classification. IEEE Trans. Inform. Theory 21 552\u2013557. MR0395379\n[8] Hall, P., Pittelkow, Y. and Ghosh, M. (2008). Theoretical measures of relative\nperformance of classifiers for high-dimensional data with small sample sizes. J.\nRoy. Statist. Soc. Ser. B 70 159\u2013173.\n[9] Hedenfalk, I., Duggan, D., Chen, Y., Radmacher, M., Bittner, M., Simon,\nR., Meltzer, P., Gusterson, B., Esteller, M., Raffeld, M., Yakhini,\nZ., Ben-Dor, A., Dougherty, E., Kononen, J., Bubendorf, L., Fehrle,\nW., Pittaluga, S., Gruvberger, S., Loman, N., Johannsson, O., Olsson,\nH., Wilfond, B., Sauter, G., Kallioniemi, O.-P., Borg, A. and Trent,\nJ. (2001). Gene expression profiles in hereditary breast cancer. N. Engl. J. Med.\n344 539\u2013548.\n[10] Holst, M. and Irle, A. (2001). Nearest neighbor classification with dependent training sequences. Ann. Statist. 29 1424\u20131442. MR1873337\n\n\fROBUST NEAREST-NEIGHBOR METHODS\n\n19\n\n[11] Ingster, Y. I. (1999). Minimax detection of a signal for ln -balls. Math. Methods\nStatist. 7 401\u2013428. MR1680087\n[12] Ingster, Y. I. (2001). Adaptive detection of a signal of growing dimension. I. Meeting\non mathematical statistics. Math. Methods Statist. 10 395\u2013421. MR1887340\n[13] Ingster, Y. I. (2002). Adaptive detection of a signal of growing dimension. II. Math.\nMethods Statist. 11 37\u201368. MR1900973\n[14] Jin, J. (2002). Detection boundary for sparse mixtures. Unpublished manuscript.\n[15] Kulkarni, S. R. and Posner, S. E. (1995). Rates of convergence of nearest neighbor\nestimation under arbitrary sampling. IEEE Trans. Inform. Theory 41 1028\u2013\n1039. MR1366756\n[16] Psaltis, D., Snapp, R. R. and Venkatesh, S. S. (1994). On the finite sample\nperformance of the nearest neighbor classifier. IEEE Trans. Inform. Theory 40\n820\u2013837.\n[17] Shakhnarovich, G., Darrell, T. and Indyk, P. (2006). Nearest-Neighbor Methods\nin Learning and Vision. MIT Press, Boston.\n[18] Wagner, T. J. (1971). Convergence of the nearest neighbor rule. IEEE Trans. Inform. Theory 17 566\u2013571. MR0298829\nDepartment of Mathematics and Statistics\nUniversity of Melbourne\nParkville, VIC 3010\nAustralia\nE-mail: y.chan@ms.unimelb.edu.au\nP.Hall@ms.unimelb.edu.au\n\n\f"}