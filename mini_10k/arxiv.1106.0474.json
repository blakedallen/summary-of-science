{"id": "http://arxiv.org/abs/1106.0474v1", "guidislink": true, "updated": "2011-06-02T17:08:36Z", "updated_parsed": [2011, 6, 2, 17, 8, 36, 3, 153, 0], "published": "2011-06-02T17:08:36Z", "published_parsed": [2011, 6, 2, 17, 8, 36, 3, 153, 0], "title": "Restricted Collapsed Draw: Accurate Sampling for Hierarchical Chinese\n  Restaurant Process Hidden Markov Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1106.4886%2C1106.0643%2C1106.4081%2C1106.1290%2C1106.1844%2C1106.2223%2C1106.4306%2C1106.5755%2C1106.1311%2C1106.1668%2C1106.1808%2C1106.3307%2C1106.4596%2C1106.6037%2C1106.3176%2C1106.6021%2C1106.1613%2C1106.1050%2C1106.0845%2C1106.6328%2C1106.6161%2C1106.4536%2C1106.3278%2C1106.3918%2C1106.4491%2C1106.3680%2C1106.2450%2C1106.1762%2C1106.0235%2C1106.1335%2C1106.5630%2C1106.5140%2C1106.4866%2C1106.2791%2C1106.4140%2C1106.5819%2C1106.4994%2C1106.0474%2C1106.3705%2C1106.1469%2C1106.3311%2C1106.1800%2C1106.2362%2C1106.1865%2C1106.5799%2C1106.6108%2C1106.3404%2C1106.0265%2C1106.4388%2C1106.3120%2C1106.4136%2C1106.1536%2C1106.3562%2C1106.0882%2C1106.0166%2C1106.4709%2C1106.1620%2C1106.0099%2C1106.5601%2C1106.1402%2C1106.1017%2C1106.0466%2C1106.4999%2C1106.1359%2C1106.0609%2C1106.6118%2C1106.3212%2C1106.5863%2C1106.6190%2C1106.0985%2C1106.1466%2C1106.1651%2C1106.2293%2C1106.0062%2C1106.4544%2C1106.6247%2C1106.5553%2C1106.4753%2C1106.1393%2C1106.1829%2C1106.2774%2C1106.3054%2C1106.5130%2C1106.1986%2C1106.4732%2C1106.2096%2C1106.4045%2C1106.3727%2C1106.3035%2C1106.3624%2C1106.5948%2C1106.0710%2C1106.5883%2C1106.1584%2C1106.4037%2C1106.5435%2C1106.0301%2C1106.2676%2C1106.2985%2C1106.1294%2C1106.2043&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Restricted Collapsed Draw: Accurate Sampling for Hierarchical Chinese\n  Restaurant Process Hidden Markov Models"}, "summary": "We propose a restricted collapsed draw (RCD) sampler, a general Markov chain\nMonte Carlo sampler of simultaneous draws from a hierarchical Chinese\nrestaurant process (HCRP) with restriction. Models that require simultaneous\ndraws from a hierarchical Dirichlet process with restriction, such as infinite\nHidden markov models (iHMM), were difficult to enjoy benefits of \\markerg{the}\nHCRP due to combinatorial explosion in calculating distributions of coupled\ndraws. By constructing a proposal of seating arrangements (partitioning) and\nstochastically accepts the proposal by the Metropolis-Hastings algorithm, the\nRCD sampler makes accurate sampling for complex combination of draws while\nretaining efficiency of HCRP representation. Based on the RCD sampler, we\ndeveloped a series of sophisticated sampling algorithms for iHMMs, including\nblocked Gibbs sampling, beam sampling, and split-merge sampling, that\noutperformed conventional iHMM samplers in experiments", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1106.4886%2C1106.0643%2C1106.4081%2C1106.1290%2C1106.1844%2C1106.2223%2C1106.4306%2C1106.5755%2C1106.1311%2C1106.1668%2C1106.1808%2C1106.3307%2C1106.4596%2C1106.6037%2C1106.3176%2C1106.6021%2C1106.1613%2C1106.1050%2C1106.0845%2C1106.6328%2C1106.6161%2C1106.4536%2C1106.3278%2C1106.3918%2C1106.4491%2C1106.3680%2C1106.2450%2C1106.1762%2C1106.0235%2C1106.1335%2C1106.5630%2C1106.5140%2C1106.4866%2C1106.2791%2C1106.4140%2C1106.5819%2C1106.4994%2C1106.0474%2C1106.3705%2C1106.1469%2C1106.3311%2C1106.1800%2C1106.2362%2C1106.1865%2C1106.5799%2C1106.6108%2C1106.3404%2C1106.0265%2C1106.4388%2C1106.3120%2C1106.4136%2C1106.1536%2C1106.3562%2C1106.0882%2C1106.0166%2C1106.4709%2C1106.1620%2C1106.0099%2C1106.5601%2C1106.1402%2C1106.1017%2C1106.0466%2C1106.4999%2C1106.1359%2C1106.0609%2C1106.6118%2C1106.3212%2C1106.5863%2C1106.6190%2C1106.0985%2C1106.1466%2C1106.1651%2C1106.2293%2C1106.0062%2C1106.4544%2C1106.6247%2C1106.5553%2C1106.4753%2C1106.1393%2C1106.1829%2C1106.2774%2C1106.3054%2C1106.5130%2C1106.1986%2C1106.4732%2C1106.2096%2C1106.4045%2C1106.3727%2C1106.3035%2C1106.3624%2C1106.5948%2C1106.0710%2C1106.5883%2C1106.1584%2C1106.4037%2C1106.5435%2C1106.0301%2C1106.2676%2C1106.2985%2C1106.1294%2C1106.2043&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose a restricted collapsed draw (RCD) sampler, a general Markov chain\nMonte Carlo sampler of simultaneous draws from a hierarchical Chinese\nrestaurant process (HCRP) with restriction. Models that require simultaneous\ndraws from a hierarchical Dirichlet process with restriction, such as infinite\nHidden markov models (iHMM), were difficult to enjoy benefits of \\markerg{the}\nHCRP due to combinatorial explosion in calculating distributions of coupled\ndraws. By constructing a proposal of seating arrangements (partitioning) and\nstochastically accepts the proposal by the Metropolis-Hastings algorithm, the\nRCD sampler makes accurate sampling for complex combination of draws while\nretaining efficiency of HCRP representation. Based on the RCD sampler, we\ndeveloped a series of sophisticated sampling algorithms for iHMMs, including\nblocked Gibbs sampling, beam sampling, and split-merge sampling, that\noutperformed conventional iHMM samplers in experiments"}, "authors": ["Takaki Makino", "Shunsuke Takei", "Issei Sato", "Daichi Mochihashi"], "author_detail": {"name": "Daichi Mochihashi"}, "author": "Daichi Mochihashi", "links": [{"href": "http://arxiv.org/abs/1106.0474v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1106.0474v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1106.0474v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1106.0474v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:1106.0474v1 [stat.ML] 2 Jun 2011\n\nRestricted Collapsed Draw: Accurate Sampling for\nHierarchical Chinese Restaurant Process Hidden Markov Models\n\nAbstract\nWe propose a restricted collapsed draw (RCD)\nsampler, a general Markov chain Monte Carlo\nsampler of simultaneous draws from a hierarchical Chinese restaurant process (HCRP) with\nrestriction. Models that require simultaneous\ndraws from a hierarchical Dirichlet process with\nrestriction, such as infinite Hidden markov models (iHMM), were difficult to enjoy benefits of\nthe HCRP due to combinatorial explosion in\ncalculating distributions of coupled draws. By\nconstructing a proposal of seating arrangements\n(partitioning) and stochastically accepts the proposal by the Metropolis-Hastings algorithm, the\nRCD sampler makes accurate sampling for complex combination of draws while retaining efficiency of HCRP representation. Based on the\nRCD sampler, we developed a series of sophisticated sampling algorithms for iHMMs, including blocked Gibbs sampling, beam sampling, and\nsplit-merge sampling, that outperformed conventional iHMM samplers in experiments.\n\n1 Introduction\nExisting sampling algorithms for infinite hidden Markov\nmodels (iHMMs, also known as the hierarchical Dirichlet\nprocess HMMs) [??] do not use a hierarchical Chinese\nrestaurant process (HCRP) [?], which is a way of representing the predictive distribution of a hierarchical Dirichlet process (HDP) by collapsing, i.e. integrating out, the underlying distributions of the Dirichlet process (DP). While\nan HCRP representation provides efficient sampling for\nmany other models based on an HDP [??] through reducing the dimension of sampling space, it has been considered rather \"awkward\" [?] to use an HCRP for iHMMs,\ndue to the difficulty in handling coupling between random\nvariables. In the simplest case, consider step-wise Gibbs\nsampling from an iHMM defined as \u03c0 k \u223c DP(\u03b2, \u03b10 ) and\n\nxi-1\n\nxi\n\nxi+1\n\nxi+2\n\nyi-1\n\nyi\n\nyi+1\n\nyi+2\n\nx'i\n\nx'i+1\n\nxi-1\nyi-1\n\nyi\n\nxi+1\n\nxi+2\n\nyi+1\n\nyi+2\n\nFigure 1: Step-wise Gibbs sampling in iHMM. Since the\nDirichlet process prior is posed on transitions in iHMM,\nresampling xi involves taking two transitions, xi\u22121 \u2192 xi\nand xi \u2192 xi+1 , simultaneously. In this case, we consider\ndistribution of two draws (x\u2032i , x\u2032i+1 ) with restriction that the\ndraws are consistent with remaining sequence, i.e., x\u2032i+1 =\nxi+1 .\n\u03b2 \u223c GEM(\u03b3). Given x1 , . . . , xi\u22121 , xi+1 , . . . , xT , resampling hidden state xi at time step i actually consists of two\ndraws (Figure 1), x\u2032i \u223c \u03c0 xi\u22121 and x\u2032i+1 \u223c \u03c0x\u2032i , under the\nrestriction (x\u2032i , x\u2032i+1 ) \u2208 C that these draws are consistent\nwith the following sequence, i.e., C = {(x\u2032i , x\u2032i+1 )|x\u2032i+1 =\nxi+1 }. Under the HCRP, the two draws are coupled even\nif xi\u22121 6= x\u2032i , because distributions \u03c0 xi\u22121 , \u03c0 x\u2032i as well as\nthe base measure \u03b2 are integrated out in an HCRP, and coupling complicates sampling from the restricted distribution.\nTo generalize, the main part of the difficulty is to obtain\na sample from a restricted joint distribution of simultaneous draws from collapsed distributions, which we call\nrestricted collapsed draw (RCD). Consider resampling L\ndraws simultaneously, x = (xj1 i1 , . . . , xjL iL ), from the\nrespective restaurants j = (j1 , . . . , jL ), when we have a\nrestriction C such that x \u2208 C. Step-wise Gibbs sampling\nfrom iHMM can be fitted into RCD with L = 2 by allowing\n\n\frestaurant index j2 to be dependent on the preceding draw\nxj1 i1 .\nIn this paper, we point out that it is not enough to consider\nthe distribution of draws. Since the HCRP introduces an\nadditional set of latent variables s that accounts for the seating arrangements of the restaurants, we have to compute an\nexact distribution of s as well, under the restriction. We\nwant to perform sampling from the following conditional\ndistribution,\n1\np(x, s|C) =\nI[ x \u2208 C ] p(x, s) ,\n(1)\nZC\nwhere ZC is a normalization constant and I is the indicator\nfunction, whose value is 1 if the condition is true and 0 otherwise. Although non-restricted probability p(x, s) can be\neasily calculated for a given x and s, calculating the normalization constant ZC leads to a combinatorial explosion\nin terms of L.\nTo solve this issue, we propose the restricted collapsed\ndraw (RCD) sampler, which provides accurate distributions of simultaneous draws and seating arrangements from\nHCRP. The RCD sampler constructs a proposal of seating\narrangements using a given proposal of draws, and the pair\nof proposals are stochastically accepted by the MetropolisHastings algorithm [?]. Since the RCD sampler can handle any combination of restricted collapsed draws simultaneously, we were able to develop a series of sampling\nmethod for HCRP-HMM, including a blocked collapsed\nGibbs sampler, a collapsed beam sampler, and a splitmerge sampler for HCRP-HMM. Through experiments we\nfound that our collapsed samplers outperformed their noncollapsed counterparts.\n\n2 HCRP representation for iHMM\n\nAn infinite hidden Markov model (iHMM) [??] is defined\nover the following HDP:\nGk \u223c DP(\u03b10 , G0 )\n\n,\n\n(2)\n\nTo see the relation of this HDP to the transition matrix \u03c0,\nconsider the explicit representation of parameters:\nG0 =\n\n\u221e\nX\n\nk\u2032 =1\n\n\u03b2k\u2032 \u03c6k\u2032\n\nGk =\n\n\u221e\nX\n\n\u03c0k\u2032 k \u03c6k\u2032\n\n2.2 HCRP-HMM\nAs another way of representing HDP in iHMM (Eq. 2), we\nintroduce a hierarchical Chinese restaurant process (HCRP,\nalso known as the Chinese restaurant franchise), which\ndoes not need to sample the transition distribution \u03c0 and\nits base measure \u03b2 in Eq. (4):\nkjt |\u03b3 \u223c CRP(\u03b3)\nxji = kjtji\n\ntji |\u03b10 \u223c CRP(\u03b10 )\nyji |xji , \u03c6 \u223c F (\u03c6xji )\n\n\u03c6k \u223c H\n\n(6)\n(7)\n. (8)\n\nUsing the Chinese restaurant metaphor, we say that customer i of restaurant j sits at table tji , which has a dish of\nan index kjtji .\nTo understand connection between HDP and HCRP, consider a finite model of grouped observations xji , in which\neach group j choose a subset of M mixture components\nfrom a model-wide set of K mixture components:\n\u03b2|\u03b3 \u223c Dir(\u03b3/K, . . . , \u03b3/K)\nkjt |\u03b2 \u223c \u03b2\n\u03c4 j |\u03b10 \u223c Dir(\u03b10 /M, . . . , \u03b10 /M ) tji |\u03c4 j \u223c \u03c4 j\n\n(9)\n(10)\n\nAs K \u2192 \u221e and M \u2192 \u221e, the limit of this model is HCRP;\nhence the infinite limit of this model is also HDP. Equation (6) is derived by taking the infinite limit of K and M\nafter integrating out \u03b2 and \u03c4 in Eqs. (9) and (10). The\ndistribution \u03c0 j in Eq. 4 can be derived from \u03c4 j and kj as\nfollows:\nX\n\u03c0j =\n(11)\n\u03c4jt \u03b4kjt .\nt\n\n2.1 Infinite HMM\n\nG0 \u223c DP(\u03b3, H)\n\nGiven an HDP and initial state x0 , we can construct an\ninfinite HMM by extracting a sequence of draws xi as\nxi = xxi\u22121 i , and corresponding observations yi = yxi i .\nFigure 2 shows a graphical representation of the iHMM.\n\n,\n\n(3)\n\nk\u2032 =1\n\nwhere transition probability \u03c0k is given as \u03c0 k \u223c\nDP(\u03b10 , \u03b2), \u03b2 \u223c GEM(\u03b3) is the stick-breaking construction of DPs [?], and \u03c6k \u223c H.\nA formal definition for the HDP based on this representation is:\n\u03b2|\u03b3 \u223c GEM(\u03b3) \u03c0j |\u03b10 , \u03b2 \u223c DP(\u03b10 , \u03b2)\nxji |(\u03c0 k )\u221e\n\u03c6k \u223c H yji |xji \u223c F (\u03c6xji ) ,\nk=1 \u223c \u03c0 j\n\n(4)\n(5)\n\nTo consider sampling of xji using HCRP (Eqs. 7 and 8),\nwe use count notation njtk as the number of customers in\nrestaurant j at table t serving the dish of the k-th entry, and\nmjk as the number of tables in restaurants the j serving the\ndish of the k-thPentry. We also use dots for marginal counts\n(e.g., m*k =\nj mjk ). Then, we sample table index tji\nfrom the following distribution:\nnjt*\n(12)\np(tji = t|tj1 , . . . , tj,i\u22121 ) =\nnj** +\u03b10\n\u03b10\np(tji = tnew |tj1 , . . . , tj,i\u22121 ) =\n.\n(13)\nnj** +\u03b10\nWhen tji = tnew (i.e., the customer sits at a new table), we\nneed to sample kjtnew , whose distribution is:\nm*k\np(kjt = k|k11 , . . .) =\n(14)\nm** +\u03b3\n\u03b3\np(kjt = k new |k11 , . . .) =\n.\n(15)\nm** +\u03b3\nThese variables determine the new sample xji = kjtji .\nSince xji does not uniquely determine the state of the\nHCRP model, we need to keep latent variables tji and\n\n\f\u03b3\nH\n\n\u03b10\n\u03b2\n\n\u031fxt\u22121\n\u221e\n\nx0\n\nx1\n\nx2\n\nx3\n\nxT\n\ny1\n\ny2\n\ny3\n\nyT\n\nFxt\n\u221e\n\nFigure 2: Graphical Representation of iHMM.\nkjt for subsequent sampling. We will denote s(j) =\n(tj1 , tj2 , . . .) as the seating arrangement in restaurant j,\ns(0) = (k11 , k12 , . . . , k21 , . . .) as the seating arrangement\nin the root restaurant, and s as the collection of all seating\narrangements, corresponding to the sampled model state.\nIn Bayesian inference based on sampling, we need a procedure to sample the latent variables, given the value of new\ndraw xji and the seating arrangements for other draws s,\nwhich is called as addCustomer.\nConstruction of HCRP-HMM is the same as iHMM, i.e.,\nextracting a sequence of draws xi given x0 as xi = xxi\u22121 i ,\nand corresponding observations yi = yxi i .\n\n3 Restricted Collapsed Draw Sampler\nWhat we want is a sampling algorithm for HCRP-HMM.\nAs described in the Introduction, the problem can be reduced to an algorithm for sampling from p(x, s|C), i.e.,\nthe distribution of restricted collapsed draw with seating\narrangements (Eq. 1).\nOur idea is to apply the Metropolis-Hastings algorithm\n[?] to the seating arrangements, which stochastically accepts the proposal distribution of seating arrangements. Although it is hard to directly give proposal distribution q(s)\nof seating arrangements, our method constructs q(s) by\ncombining qx (x) with qs (s|x), another proposal of seating arrangements given the proposed draws, which is based\non the addCustomer procedure that is standardly used in\nGibbs sampling of HCRP.\n3.1 Overall sampling\nThe Metropolis-Hastings algorithm provides a way of constructing an MCMC sampler using unnormalized probability value p\u0303(z). After sampling z \u2217 from proposal distribution q(z \u2217 |z), the algorithm computes acceptance probability R:\n\u0013\n\u0012\np\u0303(z \u2217 ) q(z old |z \u2217 )\n.\n(16)\nR = min 1,\np\u0303(z old ) q(z \u2217 |z old )\n\nThen the result z new = z \u2217 with probability R, and\nz new = z old otherwise. Repeating this process constitutes an MCMC sampler from required distrubution p(z) \u221d\np\u0303(z),\nWithin the context of HCRP, sample space z consists of\ndraws x and seating arrangement s. From Eq. (1), we can\nuse the non-restricted probability of draws p(x, s) as unnormalized probability value p\u0303(z), but it is not easy to provide a proposal for joint distribution q(x\u2217 , s\u2217 ).\nOur idea is to factorize the proposal distribution as:\nq(x\u2217 ,s\u2217 |s0 ) = qx (x\u2217 |s0 ) * qs (s\u2217 |x\u2217 , s0 ) .\n\n(17)\n\nFirst factor qx is the proposal distribution of the draws.\nSecond factor qs is the proposal distribution of the seating arrangements given the proposal draws. We use the\nresult of the addCustomer procedure, which stochastically\nupdates the seating arrangements, as the proposal distribution of the seating arrangements.\n3.2 Computing Factors\nThe following describes each factor in R and its computation.\nTrue Probability p(x, s) in Eq. (1) is the joint probability of all draws xji :\nY\np(s(j) ) * p(s(0) )\n(18)\np(x, s) =\nj\n\nwhere\n\nY\n\u0393(\u03b10 )\nm\n* \u03b10 j* *\n\u0393(njt* )\n\u0393(\u03b10 + nj** )\nt\nY\n\u0393(\u03b3)\n* \u03b3K *\np(s(0) ) =\n\u0393(m*k ) ,\n\u0393(\u03b3 + m** )\nt\np(s(j) ) =\n\n(19)\n(20)\n\nand \u0393 is the Gamma function. This is the product of the\nprobabilities of seating arrangements (Eqs. 12 to 15) for\neach customer.\nIn practice, we only need to calculate probabilities that account for the change in seating from s0 , because the prob-\n\n\fability for unchanged customers is cancelled out through\nreducing the fraction in R. Let s0 be the seating arrangement for the unchanged customers, then\np(s\u2217 )\np(s\u2217 |s0 )\n=\np(sold )\np(sold |s0 )\n\u2217\n\n.\n\n(21)\n\n\u2217\n\nIn fact, p(x , s |s0 ) is easily calculated along with addCustomer operations:\np(x\u2217 , s\u2217 |s0 ) =\np(x\u22171 , s\u22171 |s0 ) p(x\u22172 , s\u22172 |s\u22171 ) * * * p(x\u2217L , s\u2217 |s\u2217L\u22121 ) .\n\n(22)\n\nHere, p(xl , sl |sl\u22121 ) is probability p(xjl il , tjl il |jl , sl\u22121 )\nof obtaining seating arrangement sl as a result of drawing\na sample from restaurant j:\n1\n\u00d7\np(xji = k, tji = t|j, s) =\nn\nj** + \u03b10\n\uf8f1\n\uf8f4\nnjtk\nnjt* \u2265 1\n\uf8f4\n\uf8f4\n\uf8f2\nm*k\n\u03b10 *\nnjt* = 0, m*k \u2265 1\nm** + \u03b3\n\uf8f4\n\u03b3\n\uf8f4\n\uf8f4\n\uf8f3 \u03b10 *\nnjt* = 0, m*k = 0\nm** + \u03b3\n\np(xi |s0 ) .\n\n(23)\n\n(24)\n\nWe will again discuss the proposal distribution of draws for\nthe HCRP-HMM case in Section 4.\nProposal Distribution of Seating Arrangements\nqs (s\u2217 |x, s0 ), is the product of the probabilities for each\noperation of adding a customer:\nqs (s\u2217 |x\u2217 , s0 ) = qs (s\u22171 |x\u22171 , s0 ) qs (s\u22172 |x\u22172 , s\u22171 )\n(25)\n\nHere, qs (sl |xl , sl\u22121 ) = p(tjl il |xjl il , jl , sl\u22121 ), i.e., the\nprobability of obtaining seating arrangement sl as a result\nof the addCustomer(xjl il , jl , sl\u22121 ) operation.\np(tji = t|xji = k, j, s) =\n\uf8f1\nnjtk\nnjt* \u2265 1 \u2227 kjt = k\n\uf8f4\n\uf8f4\n\uf8f4\nn\n+\n\u03b10 mm***k\n\uf8f4\n+\u03b3\n\uf8f2 j*k\nm*k\n\u03b10 m** +\u03b3\nnjt* = 0 \u2227 m*k > 0\nm\n\uf8f4\n\uf8f4\n\uf8f4 nj*k + \u03b10 m***k\n+\u03b3\n\uf8f4\n\uf8f3\n1\nnjt* = 0 \u2227 m*k = 0\n\np(x\u2217l , sl |sl\u22121 )\nqs (sl |x\u2217l , sl\u22121 )\n\nrlold =\n\np(xold\nl , sl |sl\u22121 )\nqs (sl |xold\nl , sl\u22121 )\n\n(28)\n\nto simplify the calculation of R as:\n\u0013\n\u0012\np(s\u2217 ) qs (sold |xold , s0 ) qx (xold )\nR = min 1,\np(sold ) qs (s\u2217 |x\u2217 , s0 ) qx (x\u2217 )\n\u0013\n\u0012\nr(x\u2217 , s\u2217 |s0 ) q(xold )\n,\n(29)\n= min 1,\nr(xold , sold |s0 ) q(x\u2217 )\np(x\u2217 , s\u2217 |s0 )\nqs (s\u2217 |x\u2217 , s0 )\np(x\u2217L , sL |sL\u22121 )\np(x\u22171 , s1 |s0 )\n*\n*\n*\n=\nqs (s1 |x\u22171 , s0 )\nqs (sL |x\u2217L , sL\u22121 )\n\u2217\n= r1\u2217 * r2\u2217 * * * rL\n\ni=1\n\n* * * qs (s\u2217 |x\u2217l , s\u2217l\u22121 ) .\n\nrl\u2217 =\n\nr(x\u2217 , s\u2217 |s0 ) =\n\nProposal Distribution of Draws q(x) can be anything\nas long as it is ergodic within restriction C. To increase\nthe acceptance probability, however, it is preferable for the\nproposal distribution to be close to the true distribution.\nWe suggest that a good starting point would be to use a\njoint distribution composed of the predictive distributions\nof each draw, as has been done in the approximated Gibbs\nsampler [?]:\nL\nY\n\nPaying attention to the fact that both Eqs. (23) and (27) are\ncalculated along a series of addCustomer calls, we introduce factors\n\nwhere\n\nThe same applies to the calculation of p(sold |s0 ), which\ncan be done along with removeCustomer operations.\n\nqx (x) = I[x \u2208 C]\n\n3.3 Simplification\n\n(26)\n\n. (27)\n\n.\n\n(30)\n\nSurprisingly, assigning Eqs. (23) and (27) into Eq. (28) reveals that rl\u2217 is equal to p(xjl il = x\u2217l |s\u2217l\u22121 ), i.e., the probability of new customer xjl il at restaurant jl eating dish\nx\u2217l :\np(xji = k|s) =\np(xji = k new |s) =\n\n*k\nnj*k + \u03b10 mm\n*k +\u03b3\n\nnj** + \u03b10\n\u03b10 m*k\u03b3+\u03b3\nnj** + \u03b10\n\n.\n\n(31)\n(32)\n\nIn other words, calculation of the accept ratio does not use\ntji (the table index of each customer), despite the fact that\nthe values of tji are being proposed; tji will indirectly affect the accept ratio by changing subsequent draw probabilities p(x\u2217l+1 |s\u2217l ), p(x\u2217l+2 |s\u2217l+1 ), . . . through modifying\nnjtk and mjk , i.e., the number of customers and tables.\nIt is now clear that, as done in some previous work [?], we\ncan save storage space by using an alternative representation for seating arrangements s, in which the table indices\nof each customer tji are forgotten but only the numbers of\ncustomers njt* , kjt and mjk are retained. The only remaining reference to tji in the removeCustomer procedure can\nbe safely replaced by sampling.\nHowever, it should be noted that we have to revert to original seating assignment sold whenever the proposal is rejected. Putting the old draws xold back into s0 by using\nthe addCustomer procedure again will lead sampling to\nan incorrect distribution of seating assignments, and consequently, an incorrect distribution of draws.\nAlgorithm 1 is the one we propose othat obtains new samples xnew drawn simultaneously from restaurants indexed\nby j and associated seating arrangement snew , given previous samples xold and sold .\n\n\fAlgorithm 1 MH-RCDSampler(j, xold , sold ): Metropolis-Hastings sampler for restricted collapsed draw\nold\nsold\nL = s\nfor l = L downto 1 do\nold\nold\nold\nold\nsold\n{ Remove customers for xold\n}\n1 , . . . , xm sequentially from s\nl\u22121 = removeCustomer(xl , jl , sl )\nold\nold old\nrl = p(xl , sl\u22121 )\n{ Calculate factors for accept ratio }\nend for\ns\u22170 = s0 = sold\n0\nx\u2217 \u223c qx (x; s0 )\n{ Draw x\u2217 from proposal distribution q(x) of draws. }\nfor l = 1 to L do\nrl\u2217 = p(x\u2217l , s\u2217l\u22121 )\n{ Calculate factors for accept ratio }\ns\u2217l = addCustomer(x\u2217l , jl , s\u2217l\u22121 )\n{ Add customers for x\u22171 , . . . , x\u2217m sequentially to s\u22170 }\nend for\ns\u2217 = s\u2217L\n{ Obtain proposal seating s\u2217 }\n\u0012\n\u0013\nL\nqx (sold ) Y rl\u2217\n13: R = min 1,\n{ Calculate acceptance probability }\nqx (s\u2217 )\nrold\nl=1\n( l\nhx\u2217 , s\u2217 i\nwith probability R\n14: return hxnew , snew i =\n{ Accept/reject proposed sample }\nhxold , sold i otherwise.\n\n1:\n2:\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n12:\n\nThe first half of this sampler is similar to a sampler\nfor a single draw; it consists of removing old customers (line 3), choosing a new sample (line 7), and\nadding the customers again (line 10). The main difference is that there are L times of iteration for each call\nremoveCustomer/addCustomer, and the calculation of r,\nwhich is later used for acceptance probability R.\n\n4 Gibbs sampler for HCRP-HMM\nThis section describes a series of samplers for HCRPHMM. First, we present the step-wise Gibbs sampler as the\nsimplest example. After that, we describe a blocked Gibbs\nsampler using a forward-backward algorithm. We also explain the HCRP version of the beam sampler [?] as well as\nthe split-merge sampler [?] for iHMM.\n4.1 Step-wise Gibbs sampler\nA step-wise Gibbs sampler for HCRP-HMM is easily constructed using an RCD sampler (Algorithm 5 in the Appendix describes one Gibbs sweep). We slightly modified\nthe proposal distribution q(xt ) from that suggested in Section 3.2, in order to ensure that xt+1 is proposed with nonzero probability even when no table in s0 serves dish xt+1 :\n\u0013\n\u0013\n\u0012\n\u0012\n\u03b10 \u03b3\n(x\n)\n\u03b4xt+1\nqx (xt ) \u221d p(xt |s0 t\u22121 ) +\n(\u03b10 + nxt\u22121 ** )(\u03b3 + m** )\n(xt )\n\n* p(xt+1 |s0\n\n(xt )\n\n) * p(yt |F0\n\n)\n\n.\n\n(33)\n\n4.2 Blocked Gibbs sampler\nWe can construct an alternate sampling scheme under\nthe framework of RCD sampler that resamples a block\n\nof hidden states simultaneously, based on the forwardbackward sampler [?]. The idea is that we run the forwardbackward sampler with a predictive transition distribution\nfrom HCRP-HMM, and use the result as a proposal of restricted collapsed draw.\nFor iHMM, the forward-backward sampling algorithm [?]\ncannot be directly used, because the forward probability\nvalues for an infinite number of states have to be stored for\neach time step t [?]. This is not the case for HCRP-HMM,\nbecause predictive transition probability \u03c0\u0302 from given seating assignment s0 , which is given as Eqs. (31) and (32),\nonly contains transition probability for finite number K of\nstates plus one for k new . Thus we only need to store K + 1\nforward probability for each time step t.\nResult x\u0304 of the forward-backward sampler, however, cannot be used directly as the proposal; the i-th state of the\nproposal x\u2217i is equal to x\u0304i when x\u0304i 6= k new , but we need\nto assign new state indices to x\u2217i whenever x\u0304i = k new . In\nparticular, when k new has appeared W \u2265 2 times, all appearances of k new may refer either to the same new state,\nor to W different states, or to anything in between the two,\nin which some appearances of k new share a new state.\nTo achieve this purpose, we prepare special CRP Q\u2217 that\naccounts for the previously unseen states, marked by k new\nin the result of the forward-backward sampler. Specifically,\neach table in Q\u2217 has a dish with an unused state index, and\neach appearance of k new is replaced with a draw from Q\u2217 .\nThis construction ensures that every state sequence is proposed with a non-zero probability, and allows the proposal\nprobability to be easily calculated. The concentration parameter of Q\u2217 is set as equal to \u03b3. To handle the case where\nsome of the new states are equal to xtb+1 , i.e., index of the\nstate that succeeds to the resampling block, we add to Q\u2217\n\n\fan extra customer that correponds to xtb+1 when xtb+1 does\nnot appear in s0 ,\nResulting proposal probability is:\n\nslice\n\nprobability obtained by forward-backward sampling\n\n\u2217\n\nqx (x ) =\nL\nY\n\nl=0\n\n\u03c0\u0302x\u0304l+1 x\u0304l *\n\ncause beam sampling satisfies the detailed balance equation, which ensures that the ratio of proposal probability\nq\u2217\nis always equal to the ratio of the\nwith beam sampling qslice\nold\n\nL\nY\n\nl=1\n\n!\n\nFx\u0304l (yl )\n\n*\n\nY\n\nq\u2217\nqold .\n\np(x\u2217l |Q\u2217 ) ,\n\nl:x\u0304l =knew\n\n4.4 Split-Merge Sampling\n(34)\n\nwhere the first factor accounts for the forward probability\nof the sequence, and the second factor accounts for probability of the new state assignment.\nNote also that, to make a sensible proposal distribution, we\ncannot resample the whole state sequence simultaneously.\nWe need to divide the state sequence into several blocks,\nand resample each block given the other blocks. The size\nof a block affects efficiency, because blocks that are too\nlarge have lower accept probability, while with blocks that\nare too small, the algorithm has little advantage over stepwise Gibbs sampling.\nAlgorithm 8 in the Appendix describes one sweep of a\nblocked Gibbs sampler for an HCRP-HMM.\n4.3 Beam sampling\nBeam sampling for HDP-HMM [?] is a sampling algorithm that uses slice sampling [?] for transition probability\nto extract a finite subset from the state space. Although the\npossible states are already finite in HCRP-HMM, the same\ntechnique may benefit sampling of HCRP-HMM by improving efficiency from the reduced number of states considered during one sampling step.\nWe just need replace the call to ForwardBackwadSampling\nin Algorithm 8 with the call to BeamSampling to use beam\nsampling with HCRP-HMM. A brief overview of the beam\nsampling is:\n1. Sample auxiliary variables u = (u0 , . . . , uL ) as ul \u223c\nUniform(0, \u03c0xl xl\u22121 ),\n2. For l = 1, . . . , L, calculate forward probability\nq(x\u2032l = k \u2032 ) using a P\nslice of transition probability\nq(x\u2032l = k \u2032 ) = Fk\u2032 (yl ) k I(\u03c0k\u2032 k > ul\u22121 )q(x\u2032l\u22121 = k),\n3. For l = L, . . . , 1, sample the states x\u2032l backwardly,\ni.e. p(x\u2032l = k) \u221d I(\u03c0x\u2032l+1 k > ul ).\n\nWe can integrate the split-merge sampling algorithm,\nwhich is another sampling approach to Dirichlet process\nmixture models [?], into HCRP-HMM using the RCD sampler. A split-merge sampler makes a proposal move that\ntries to merge two mixture components into one, or to split\na mixture component into two; the sampler then uses a\nMetropolis-Hastings step to stochastically accept the proposal. Based on the RCD framework, we can extend the\nsplit-merge sampler into HCRP, which can be applied to\nHCRP-HMM. Within the context of HMM, the sampler\ncorresponds to merge two state indices into one, or to split\na state index into two.\nOur implementation is based on an improved version of\nhte split-merge sampler, called the sequentially-allocated\nmerge-split sampler [?], which produces a split proposal\nwhile sequentially allocating components in random order.\nTo deal with temporal dependency in HMM, we identify\nfragments of state sequences to be resampled within the\nstate sequence, and perform blocked Gibbs sampling for\neach fragment in random order.\nWe added one important optimization to the split-merge\nsampling algorithm. Since a merge move is proposed much\nmore frequently than a split move, and the move has a relatively low accept probability, it is beneficial if we have a\nway of determining whether a merge move is rejected or\nnot earlier. Let us point out that, when proposal probability for a merge move is calculated, the accept probability is monotonically decreasing. Consequently we sample\nRthr , the threshold of accept probability, at the beginning\nof the algorithm and stop further calculation when R becomes less than Rthr . Algorithm 9 in the Appendix is the\nsplit-merge sampling algorithm for HCRP-HMM.\nSplit-merge sampling allows faster mixing when it is interleaved with other sampling strategies. We examine splitmerge sampling with each of the samplers we have presented in this paper.\n\nFor details, refer to the original paper [?].\nSome remarks may be needed for the calculation of qx\u2217 ,\ni.e., the proposal probability for the state sequence. Although beam sampling has a different proposal distribution\nfrom forward-backward sampling, we can use the same calculation of proposal probability used in acceptance probability as that of forward-backward sampling. This is be-\n\n5 Experiments and Discussion\nThis section presents two series of experiments, the first\nwith small artificial sequences and the second with a sequence of natural language words.\n\n\f5.1 Settings\nWe put gamma prior Gamma(1, 1) on \u03b10 and \u03b3, and\nsampled between every sweep using an auxiliary variable\nmethod [?] in all the experiments. We introduced HCRP\nas a prior of emission distributions as well, and its hyperparameters were also sampled in the same way.\nThe initial state sequence given to the sampler is the result\nof a particle filter with 100 particles.\n\nT\n\u2212t\nX\n1\n(xi \u2212 \u03bc)(xi+t \u2212 \u03bc)\n(T \u2212 t)\u03c3 2 i=1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\nG\n\nH\n\nA\n\n(35)\n\nFigure 3: Automaton that generates Sequence 2. Circles\ndenote hidden states, and the same alphabet emissions are\nobserved from states within an oval group. A dashed arrow denotes transition with probability 0.8, a bold arrow\ndenotes transition with probability 0.84, and a solid arrow\ndenotes emission with probability 1/3.\n\n(36)\n\n5.2 Artificial data\n\nWe measured autocorrelation time (ACT) to evaluate mixing. Given a sequence of values x = x1 , x2 , . . . , xT , its\nmean \u03bc and variance \u03c3 2 , ACT (x) are defined as follows:\nACFt (x) =\n\nH\n\n1\n\n\u221e\n\nACT (x) =\n\n1 X\nACFt (x)\n+\n2 t=1\n\n.\n\nSince with larger t, ACFi (x) is expected to converge to\nzero, we used ACFi (x) for t \u2264 1000.\nFor artificial sequence, we evaluated mutual information\nbetween the ht , hidden state used in sequence generation\nand xt , inferred states as follows:\nMI =\n\nXX\nh\n\nx\n\np(x, h) log\n\np(x, h)\np(x)p(h)\n\n.\n\n(37)\n\nFor natural language text, the inferred model is evaluated\nby multiple runs of a particle filter on a given test sequence\nof length Ttest . We specifically construct a particle filter\nwith Z = 100 particles for each sampled model state sz ,\nand evaluate likelihood l(yi |sz ) for each emission. Finally,\nwe calculate the perplexity (the reciprocal geometric mean\nof the emission probabilities) of the test sequence:\n\u0013\n\u0012\n1 X\n\u02c6\nlog l(yi )\n(38)\nP P L = exp \u2212\nTtest\nwhere\n\nl\u0302(yi ) =\n\nZ\n1 X\nl(yi |sz ) .\nZ z=1\n\n(39)\n\nThe samplers we chose for comparison are the step-wise\nGibbs sampler with direct assignment representation [?],\nwhich uses stick-breaking for the root DP and CRP for the\nother DPs, the step-wise Gibbs sampler with stick-breaking\nconstruction, and the beam sampler with stick-breaking\nconstruction [?]. For fair comparison between different algorithms, we collected samples to evaluate the autocorrelation time and perplexity on a CPU-time basis (excluding\nthe time used in evaluation). All the algorithms were implemented with C++ and tested on machines with an Intel\nXeon E5450 at 3 GHz.\n\nThe first series of experiments are performed with two\nsmall artificial sequences. Sequence 1 consists of repeating sequence of symbols A-B-C-D-B-C-D-E-... for length\nT = 500, and we run the sampler 30 s for burn-in, and after that, a model state is sampled every 2 s until a total of\n300 s is reached. Sequence 2 is generated from the simple\nfinite state automaton in Figure 3 for length T = 2500, and\nwe use 60 s for burn-in and total 600 s. We evaluated the\nmutual information between the inferred hidden states and\nthe true hidden states.\nFigure 4 shows the distribution of mutual information for\n100 trials after 300 s. We can see that some of the samplers\nbased on the proposed method achieved a better mutual information compared to existing samplers. The improvement depends on the type of sequence and the samplers.\nFor Sequence 1, we can see that split-merge sampling\nyields better results compared to other samplers. Although\nHMM with eight hidden states can completely predict the\nsequence, the samplers tend to be trapped in a local optimum with five states in the initial phase, because our selected prior of \u03b3 poses a larger probability on a smaller\nnumber of hidden states, Detailed investigations (Figure 5)\nconfirmed this analysis.\nFor Sequence 2, on the other hand, blocked samplers\nworked very efficiently. Step-wise samplers generally\nworked poorly on the sequence, because the strong dependency on temporally adjacent states impedes mixing.\nStill, step-wise Gibbs sampler for HCRP-HMM outperformed the beam sampler with the stick-breaking process.\nThe blocked Gibbs sampler had inferior performance due\nto its heavy computation for a large number of states, but\nthe beam sampler for HCRP-HMM was efficient and performed well. Combination with a small number of splitmerge samplers increases the performance (more splitmerge sampling leads to lower performance by occupying\n\n\fcomputational resource for the beam sampler). From averages statistics of samplers (Table 1), we can see that (1) the\nincrease of mutual information cannot be described only\nby the increase of the number of states; (2) The accept ratio\nfor the Gibbs trial has a very high accept rate; (3) Splitmerge samplers have a very low accept rate, but still make\nimprovement for mutual information.\n5.3 Natural language text\nWe also tested the samplers using a sequence of natural\nlanguage words from Alice's Adventure in Wonderland. We\nconverted the text to lower case, removed punctuation, and\nplaced a special word EOS after every sentence to obtain\na corpus with 28, 120 words; we kept the last 1,000 words\nfor test corpus and learned on a sequence with length T =\n27120. We introduce a special word UNK (unknown) to\nreplace every word that occurred only once, resulting in\n|\u03a3| = 1, 487 unique words in the text. We took 10,000 s\nfor burn-in, and sampled a model state for every 120 s, until\nthe total of 172,800 s. Table 2 summarize the averaged\nstatistics for 18 trials.\nWe found that step-wise sampling outperformed blocked\nsampling (including beam sampling). The reason for this\nmay be the nature of the sequence, which has a lower temporal dependency. Blocked Gibbs sampling, in particular,\nconsumes too much time for one sweep to be of any practical use. We also found that split-merge sampling had a\nvery low accept rate and thus made little contribution to the\nresult.\nYet, we can see the advantage of using HCRP representation over stick-breaking representation. The direct assignment (DA) algorithm showed a competitively good\nperplexity, reflecting the fact that DA uses stick-breaking\nfor only the root DP and uses the CRP representation for\nthe other DP. Though step-wise Gibbs sampling and its\nslice sampling version seems outperforming DA slightly,\nwe need to collect more data to show that the difference is\nsignificant. At least, however, we can say that now many\nsampling algorithms are available for inference, and we can\nchoose a suitable one depending on the nature of the sequence.\n\n6 Conclusion and Future Work\nWe have proposed a method of sampling directly from constrained distributions of simultaneous draws from a hierarchical Chinese restaurant process (HCRP). We pointed\nout that, to obtain a correct sample distribution, the seating arrangements (partitioning) must be correctly sampled\nfor restricted collapsed draw, and we thus proposed applying the Metropolis-Hastings algorithm to the seating arrangements. Our algorithm, called the Restricted Collapsed\nDraw (RCD) sampler, uses a na\u0131\u0308ve sampler to provide a\n\nproposal distribution for seating arrangements. Based on\nthe sampler, we developed various sampling algorithms\nfor HDP-HMM based on HCRP representation, including\nblocked Gibbs sampling, beam sampling, and split-merge\nsampling.\nThe applications of the RCD sampler, which is at the heart\nof our algorithms, are not limited to HCRP-HMM. The\nexperimental results revealed that some of the proposed algorithms outperform existing sampling methods, indicating\nthat the benefits of using a collapsed representation exceed\nthe cost of rejecting proposals.\nThe main contribution of this study is that it opens a way\nof developing more complex Bayesian models based on\nCRPs. Since the RCD sampler is simple, flexible, and\nindependent of the particular structure of a hierarchy, it\ncan be applied to any combination or hierarchical structure of CRPs. Our future work includes using this algorithm to construct new Bayesian models based on hierarchical CRPs, which are hard to implement using a noncollapsed representation. Planned work includes extending\nHDP-IOHMM [?] with a three-level hierarchical DP (e.g.,\nthe second level could correspond to actions, and the third\nlevel, to input symbols).\n\n\f(a) Sequence 1\n\n(b) Sequence 2\n\nFigure 4: Average mutual information of sampled hidden states\n\n100\n80\n60\n40\n20\n0\n\n100\n80\n60\n40\n20\n0\n2\n\n2.2\n\n2.4\n\n2.6\n\n2.8\n\n3\n\n100\n80\n60\n40\n20\n0\n2\n\n(a) HDP-HMM (SB) SGibbs\n100\n80\n60\n40\n20\n0\n\n2.2\n\n2.4\n\n2.6\n\n2.8\n\n3\n\n(b) HDP-HMM (DA) SGibbs\n100\n80\n60\n40\n20\n0\n\n2\n\n2.2\n\n2.4\n\n2.6\n\n2.8\n\n(d) HCRP-HMM Beam\n\n3\n\n2\n\n2.2\n\n2.4\n\n2.6\n\n2.8\n\n3\n\n(c) HDP-HMM (SB) Beam\n100\n80\n60\n40\n20\n0\n\n2\n\n2.2\n\n2.4\n\n2.6\n\n2.8\n\n(e) HCRP-HMM Beam\nSplit-Merge 3/sweep\n\n3\n\n2\n\n2.2\n\n2.4\n\n2.6\n\n2.8\n\n3\n\n(f) HCRP-HMM Beam\nSplit-Merge 25/sweep\n\nFigure 5: Distribution of mutual information for Sequence 1. X-axis shows mutual information and Y-axis shows frequency. Block size \u2248 6 for HCRP-HMM Beam sampling.\n\n\fname\nHDP-HMM (DA) SGibbs\nHDP-HMM (SB) Beam\nHCRP-HMM SGibbs\nHCRP-HMM SGibbs +SM=2\nHCRP-HMM SGibbs +SM=13\nHCRP-HMM SSlice\nHCRP-HMM SSlice +SM=2\nHCRP-HMM SSlice +SM=13\nHCRP-HMM BGibbs\nHCRP-HMM BGibbs +SM=2\nHCRP-HMM BGibbs +SM=13\nHCRP-HMM Beam\nHCRP-HMM Beam +SM=2\nHCRP-HMM Beam +SM=13\n\nTable 1: Experimental results for Sequence 2\nMI ACT #states #states secs/sweep Gibbs accept rate\n2.92\n0.527 14.910\n0.044\n-\n3.04\n0.640 14.720\n0.032\n-\n3.18\n0.719 16.210\n0.026\n0.999666\n3.28\n0.615 17.190\n0.030\n0.999632\n3.19\n0.493 16.830\n0.044\n0.999619\n2.82\n0.820 15.950\n0.009\n0.999847\n2.86\n0.705 17.330\n0.013\n0.999822\n2.59\n0.604 16.400\n0.030\n0.999830\n3.01\n0.317 14.900\n0.206\n0.995525\n3.12\n0.513 16.270\n0.237\n0.995135\n3.18\n0.637 16.530\n0.265\n0.994715\n3.21\n0.866 15.180\n0.016\n0.997233\n3.37\n0.898 16.910\n0.019\n0.996369\n3.28\n0.875 17.070\n0.034\n0.996316\n\nSM accept rate\n-\n-\n-\n0.000631\n0.000650\n-\n0.000827\n0.000910\n-\n0.000985\n0.000715\n-\n0.000497\n0.000532\n\nDA: Direct Assignment\nSB: Stick-Breaking construction\nMI: Mutual Information\nACT: Auto-correlation time, samples collected for every 0.1 s\n#states: number of states SGibbs: step-wise Gibbs\nSSlice: step-wise Gibbs with slice sampling (beam sampling with block size=1)\nBGibbs: blocked Gibbs (block size \u2248 8)\nBeam: beam sampling (block size \u2248 8 for HCRP-HMM, T for stick-breaking)\nSM: Split-Merge sampler (+SM=n denotes SM trials per Gibbs sweep)\n\nTable 2: Experiments on Natural language text\nSampler\nPerplexity\n# states sec/sweep Gibbs accept rate\nHDP-HMM (DA) SGibbs\n134.22\n313.056\n10.017\n-\n151.10\n242.389\n38.045\n-\nHDP-HMM (SB) SGibbs\nHDP-HMM (SB) Beam\n178.59\n68.444\n16.126\n-\nHCRP-HMM SGibbs\n133.31\n379.833\n7.027\n0.999861\nHCRP-HMM SGibbs+SM=130\n131.66\n386.833\n7.664\n0.999857\nHCRP-HMM SGibbs+SM=5400\n135.94\n336.278\n31.751\n0.999880\n131.17\n422.000\n0.469\n0.999986\nHCRP-HMM SSlice\nHCRP-HMM SSlice+SM=130\n131.67\n409.833\n0.976\n0.999993\n152.76\n254.722\n36.617\n0.999993\nHCRP-HMM SSlice+SM=5400\nHCRP-HMM BGibbs\n199.14 1840.833 29380.261\n0.992748\nHCRP-HMM Beam\n141.77\n603.333\n80.681\n0.995627\n142.69\n567.278\n72.217\n0.995612\nHCRP-HMM Beam+SM=130\nHCRP-HMM Beam+SM=5400\n141.07\n495.667\n84.925\n0.995554\nFor HCRP-HMM, the block size \u2248 10.\n\nSM accept rate\n-\n-\n-\n-\n0.000052\n0.000050\n-\n0.000052\n0.000056\n-\n-\n0.000124\n0.000101\n\n\fA\n\nMiscellaneous Algorithms\n\nAlgorithm 2 getProb(j, k, s): Calculate p(xji = k|s)\nif m*k = 0 then\n\u03b3\nreturn\nelse\nreturn\nend if\n\n\u03b10 m\n\n*k +\u03b3\n\nnj** +\u03b10\n\nm*k\n*k +\u03b3\n\nnj*k +\u03b10 m\n\nnj** +\u03b10\n\nAlgorithm 3 addCustomer(j, k, sold ): Adds new customer eating dish k to restaurant j.\ns := sold\nWith probabilities proportional to: njtk (t =\n1, . . . , mj* ): Increment njtk (the customer sits at t-th\nnew\ntable) \u03b10 mm***k\nserv+\u03b3 : sit customer at a new table t\ning dish k in restaurant j (njtnew k := 1, kjtnew := k,\nincrement mjk )\nreturn updated s\nAlgorithm 4 removeCustomer(j, k, sold ): Removes existing customer eating dish k from restaurant j.\ns := sold\nSample tji in proportional to njtji k\nDecrement njtji k (the customer at tji -th table is removed)\nif njtji k becomes zero then\nRemove the unoccupied table tji from restaurant j,\ndecrement mjk\nend if\nreturn updated s\n\nB Step-wise Gibbs sampler\nTo manipulate emission probability F (xi ) with a conjugate\nprior, we introduced a similar notation to HCRP, which can\nbe intuitively understood.\n\n\fAlgorithm 5 Step-wise Gibbs sweep for HCRP-HMM\nInput: y1 , . . . , yi : observed emissions\nx1 , . . . , xi : previously inferred states\nsold : set of CRP seating arrangements\nF old : set of emission distributions\nfor i = 1, . . . , T , in random order do\ns1 = removeCustomer(xi+1 , xt , sold )\nr3old = getProb(xi+1 , xt , s1 )\nF 0 = removeCustomer(yi , xi , F old )\nr2old = getProb(yi , xt , s1 )\ns0 = removeCustomer(xi , xi\u22121 , s1 )\nr1old = getProb(xi , xi\u22121 , s0 )\nSample x\u2217i \u0010\nin proportion to q(xt ) where\nq(xt ) \u221d\n\n\u03b10 \u03b3\n(\u03b10 +nxt\u22121 ** )(\u03b3+m** ) \u03b4xt+1\n(x )\n\n(xt\u22121 )\n\n+ p(xt |S0\n\n* p(yt |Fxt ) * p(xt+1 |S0 t )\n\u2217\nr1 = getProb(x\u2217t , xt\u22121 , s0 )\ns1 = addCustomer(x\u2217t , xt\u22121 , s0 )\nr2\u2217 = getProb(yt , xt , F 0 )\nF \u2217 = addCustomer(yt , xt , F 0 )\nr3\u2217 = getProb(xt+1 , x\u2217t , s1 )\n\u2217\ns\u2217 = addCustomer(x\nt+1 , xt , s1 ) \u0013\n\u0012\n\u2217\n\u2217\n\u2217\nr1 r2 r3 q(xt )\nR := min 1, old\n\u2217\nold old\nr\n( 1 r2 r3 q(xt )\nhx\u2217i , s\u2217 , F \u2217 i\nwith probability R\nhxt , s, F i :=\nhxt , sold , F old i otherwise\nend for\n\nC\n\n\u0011\n)\n\nBlocked Gibbs sampler\n\nFor details on the ForwardBackwardSampling routine,\nplease refer to the literature [?].\nAlgorithm 6 removeSeq(i0 , i1 , x, sold , F old ): remove\ncustomers for a part of state sequence (xi0 , . . . , xi1 )\nL = i1 \u2212 i0 \u2212 1\nsL = removeCustomer(xi0 +L , xi1 , sold )\nold\nrL\n= p(xji = k|s)\nF L = F old\nfor l = L \u2212 1 downto 0 do\nsl = removeCustomer(xib +l , xib +l\u22121 , sl+1 )\nF l = removeCustomer(yib +l , xib +l , F l+1 )\nrlold\n=\ngetProb(xib +l , xib +l\u22121 , sl ) *\ngetProb(yib +l , xib +l , F l )\nend for\nQL\nreturn hs0 , F 0 , l=0 rlold i\n\nAlgorithm 7 addSeq(i0 , i1 , x, s0 , F 0 ): add customers for\na part of state sequence (xi0 , . . . , xi1 )\nL = i1 \u2212 i0 \u2212 1\nfor l = 0 to L \u2212 1 do\nrl\u2217\n=\ngetProb(xib +l , xib +l\u22121 , sl ) *\ngetProb(yib +l , x\u2217ib +l , F l )\nsl+1 = addCustomer(x\u2217ib +l , x\u2217ib +l\u22121 , sl )\nF l+1 = addCustomer(yib +l , x\u2217ib +l , F l )\nend for\n\u2217\nrL\n= getProb(xi1 , x\u2217i1 \u22121 , s\u2217L )\n\u2217\ns = addCustomer(xi1 +L , x\u2217i1 \u22121 , s\u2217L ); F \u2217 = FL\u2217\nQL\nreturn hs\u2217 , F \u2217L , l=0 rl\u2217 i\nAlgorithm 8 Blocked Gibbs sweep for HCRP-HMM\nInput: y1 , . . . , yi : observed emissions\nx = x1 , . . . , xT : previously inferred states\ns: set of CRP seating arrangements\nF : set of emission distributions\nB: number of blocks\nChoose block boundaries i1 , . . . , iB\u22121 \u2208 {2, . . . , T };\ni0 := 1, iB = T\nfor b = 0, . . . , B \u2212 1, in random order do\nhs0 , F 0 , rold i = removeSeq(ib , ib+1 \u2212 ib , x, s, F , 0);\nx\u2217i = xi for all t < ib or t \u2265 tb+1\n(x\u2217ib , . . . , x\u2217ib +L\u22121 ) =\nFBSampler(\u03c0\u0302|S0 , F0 , yib :ib +L\u22121 , xib \u22121 , xib +L )\nCalculate q old = q(xib , . . . , xib +L\u22121 ) and q \u2217 =\nq(x\u2217ib , . . . , x\u2217ib +L\u22121 )\nQold = CRP(\u03b3, H)\nQ\u2217 = CRP(\u03b3, H)\nif xtb+1 refers to a new state in s0 then\nQ\u2217 := addCustomer(xtb+1 , Q\u2217 )\nQold := addCustomer(xtb+1 , Qold )\nend if\nfor t = tb to tb+1 \u2212 1 do\nif xi refers to a new state in s0 then\nq old := q old \u2217 getProb(xi , Qold )\nQold := addCustomer(xi , Qold )\nend if\nif x\u2217i = k new then\nsample s \u223c Q\u2217 ; x\u2217i := s\nq \u2217 := q \u2217 \u2217 getProb(x\u2217i , Q\u2217 )\nQ\u2217 := addCustomer(xi , Q\u2217 )\nend if\nend for\nS0\u2217 = S0 ; F0\u2217 = F0\n\u2217\nhs\u2217 , F \u2217 , r\u0012\n= addSeq(i0 , L, \u0013\nx, s0 , F 0 )\nq old old \u2217\nR := min 1, \u2217 * r * r\n(q\nhx\u2217 , s\u2217 , F \u2217 i\nwith probability R\nhx, s, F i :=\nold\nold old\nhx , s , F i otherwise\nend for\n\n\fD\n\nSplit-Merge sampler\n\nAlgorithm 9 Split-Merge Sampler for an HCRP-HMM\nInput: y1 , . . . , yT : observed emissions\nx1 , . . . , xT : previously inferred states\nsold : set of CRP seating arrangements\nF old : set of emission distributions\nthr\nR \u223c Uniform(0, 1)\nChoose distinct t1 , t2 \u2208 {1, . . . , T }\n/ {t1 , t2 }, and not contained in other\nIdentify all fragments (bi , ei ) s.t. for all t \u2208 (bi , . . . , ei ), xi \u2208 {xt1 , xt2 } \u2227 t \u2208\nfragments\nPermute fragments randomly\nLet U be the number of fragments\nsU+1 = sold , F U+1 = F old\nif xt1 = xt2 then\n{ Try split move }\nfor i = U downto 1 do\nhsi , F i , riold i = removeSeq(bi , ei , x, si+1 , F i+1 )\nqiold = 1\nend for\nx\u2217t2 = new k index\nelse\n{ Try merge move }\nfor i = U downto 1 do\nhsi , F i , riold i = removeSeq(xbi :ei , si+1 , F i+1 )\nSeqProb(\u03c0\u0302|si+1 , Fi , ybi :ei , xbi \u22121:ei +1 )\nqiold =\nForwardProb(\u03c0\u0302|si+1 , Fi , ybi :ei , xbi \u22121 , xei +1 ; {xt1 , xt2 })\nend for\nx\u2217t2 = xt1\nend if\n{ Remove customers that accounts for transitions around xold\nt2 }\nF 0 = removeCustomer(yt2 , xt2 , F 1 )\npold\n0 = p(yt2 |F xt2 )\ns0 := s1\nif t2 \u2212 1 is not in any fragment then\ns0 := removeCustomer(xt2 , xt2 \u22121 , s0 ))\nr0old \u2217 = getProb(xt2 , xt2 \u22121 , s1 ))\nend if\nif t2 + 1 is not in any fragment then\ns0 := removeCustomer(xt2 + 1, xt2 , s0 ))\nr0old \u2217 = getProb(xt2 +1 xt2 , s0 ))\nend if\nq0old = q0\u2217 = 1\n(continue to Algorithm 10)\n\n\fAlgorithm 10 Split-Merge Sampler for an HCRP-HMM (continued)\n{ Add customers that accounts for transitions around x\u2217t2 }\n\u2217\n\u2217\np0 = p(yt2 |F x\u2217t )\n2\nF \u22171 = addCustomer(yt2 , x\u2217t2 , F \u22170 )\ns\u22171 := s0\nif t2 + 1 is not in any fragment then\nr0\u2217 \u2217 = getP rob(xt2 +1 xt2 , s\u22171 ))\ns\u22171 := addCustomer(xt2 + 1, xt2 , s\u22171 ))\nend if\nif t2 \u2212 1 is not in any fragment then\nr0\u2217 \u2217 = getP rob(xt1 |xt2 \u22121 , s\u22171 ))\ns\u22171 := addCustomer(xt2 , xt2 \u2212 1, s\u22171 ))\nend if\nif xt1 = xt2 then\n{ Try split move }\nfor i = 1 to U do\nx\u2217bi , . . . , x\u2217bi +Li \u22121 = LimitedFBSampler(\u03c0\u0302|s\u2217i+1 , Fi\u2217 , ybi :bi +Li \u22121 , x\u2217bi \u22121 , x\u2217bi +L ; {x\u2217t1 , x\u2217t2 })\nSeqProb(\u03c0\u0302|s\u2217i+1 , Fi\u2217 , ybi :ei , x\u2217bi \u22121:ei +1 )\nqi\u2217 =\nForwardProb(\u03c0\u0302|s\u2217i+1 , Fi\u2217 , ybi :ei , x\u2217bi \u22121 , x\u2217ei +1 ; {x\u2217t1 , x\u2217t2 })\n\u2217\nhsi+2 , F \u2217i+1 , ri\u2217 i = addSeq(bi , ei , x\u2217 , s\u2217i+1 , F \u2217i )\nend for\nelse\n{ Try merge move }\nfor i = 1 to U do\nQ\nri\u2217 QI riold\nRcur = i\u22121\ni\u2032 =0 \u2217 *\ni=0 old\nqi\nri\nthr\ncur\nif R\n\u2265R\nthen\nrejection determined, exit loop\nend if\nx\u2217bi , . . . , x\u2217ei = xt1\nqi\u2217 = 1\nhs\u2217i , F \u2217i\u22121 , ri\u2217 i = addSeq(bi , ei , x, si+1 , F i )\nend for\nend if\nQI\nri\u2217 QI qiold\n* i=1 \u2217\nR = i=0 old\nqi\nr(\ni\nhx\u2217 , s\u2217 , F \u2217 i\nRthr < R\nhx, s, F i =\nhxold , sold , F old i otherwise\n\n\f"}