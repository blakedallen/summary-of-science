{"id": "http://arxiv.org/abs/0708.3517v1", "guidislink": true, "updated": "2007-08-27T00:04:03Z", "updated_parsed": [2007, 8, 27, 0, 4, 3, 0, 239, 0], "published": "2007-08-27T00:04:03Z", "published_parsed": [2007, 8, 27, 0, 4, 3, 0, 239, 0], "title": "Sparse inverse covariance estimation with the lasso", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0708.2878%2C0708.2372%2C0708.4371%2C0708.3517%2C0708.3885%2C0708.0136%2C0708.0058%2C0708.4108%2C0708.4324%2C0708.2769%2C0708.1958%2C0708.3467%2C0708.3631%2C0708.3305%2C0708.0057%2C0708.3667%2C0708.2882%2C0708.2379%2C0708.4073%2C0708.1296%2C0708.4095%2C0708.2805%2C0708.4196%2C0708.0296%2C0708.3330%2C0708.2422%2C0708.3565%2C0708.1989%2C0708.3660%2C0708.4119%2C0708.1904%2C0708.2270%2C0708.2874%2C0708.2185%2C0708.0496%2C0708.1083%2C0708.2683%2C0708.1449%2C0708.3532%2C0708.1477%2C0708.1077%2C0708.0360%2C0708.3523%2C0708.2887%2C0708.1330%2C0708.2509%2C0708.2767%2C0708.0799%2C0708.2721%2C0708.2175%2C0708.1623%2C0708.2566%2C0708.3415%2C0708.0468%2C0708.2477%2C0708.1201%2C0708.3572%2C0708.0941%2C0708.3873%2C0708.2886%2C0708.0419%2C0708.3027%2C0708.3417%2C0708.2060%2C0708.3424%2C0708.2300%2C0708.1239%2C0708.2034%2C0708.1762%2C0708.3778%2C0708.0645%2C0708.2813%2C0708.1309%2C0708.1404%2C0708.2540%2C0708.3854%2C0708.0139%2C0708.1299%2C0708.3608%2C0708.1251%2C0708.3585%2C0708.1613%2C0708.3373%2C0708.2119%2C0708.2583%2C0708.1209%2C0708.3336%2C0708.3239%2C0708.0286%2C0708.0045%2C0708.4383%2C0708.0366%2C0708.1822%2C0708.2685%2C0708.0728%2C0708.1058%2C0708.0044%2C0708.2297%2C0708.4040%2C0708.1780%2C0708.3945&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Sparse inverse covariance estimation with the lasso"}, "summary": "We consider the problem of estimating sparse graphs by a lasso penalty\napplied to the inverse covariance matrix. Using a coordinate descent procedure\nfor the lasso, we develop a simple algorithm that is remarkably fast: in the\nworst cases, it solves a 1000 node problem (~500,000 parameters) in about a\nminute, and is 50 to 2000 times faster than competing methods. It also provides\na conceptual link between the exact problem and the approximation suggested by\nMeinhausen and Buhlmann (2006). We illustrate the method on some cell-signaling\ndata from proteomics.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0708.2878%2C0708.2372%2C0708.4371%2C0708.3517%2C0708.3885%2C0708.0136%2C0708.0058%2C0708.4108%2C0708.4324%2C0708.2769%2C0708.1958%2C0708.3467%2C0708.3631%2C0708.3305%2C0708.0057%2C0708.3667%2C0708.2882%2C0708.2379%2C0708.4073%2C0708.1296%2C0708.4095%2C0708.2805%2C0708.4196%2C0708.0296%2C0708.3330%2C0708.2422%2C0708.3565%2C0708.1989%2C0708.3660%2C0708.4119%2C0708.1904%2C0708.2270%2C0708.2874%2C0708.2185%2C0708.0496%2C0708.1083%2C0708.2683%2C0708.1449%2C0708.3532%2C0708.1477%2C0708.1077%2C0708.0360%2C0708.3523%2C0708.2887%2C0708.1330%2C0708.2509%2C0708.2767%2C0708.0799%2C0708.2721%2C0708.2175%2C0708.1623%2C0708.2566%2C0708.3415%2C0708.0468%2C0708.2477%2C0708.1201%2C0708.3572%2C0708.0941%2C0708.3873%2C0708.2886%2C0708.0419%2C0708.3027%2C0708.3417%2C0708.2060%2C0708.3424%2C0708.2300%2C0708.1239%2C0708.2034%2C0708.1762%2C0708.3778%2C0708.0645%2C0708.2813%2C0708.1309%2C0708.1404%2C0708.2540%2C0708.3854%2C0708.0139%2C0708.1299%2C0708.3608%2C0708.1251%2C0708.3585%2C0708.1613%2C0708.3373%2C0708.2119%2C0708.2583%2C0708.1209%2C0708.3336%2C0708.3239%2C0708.0286%2C0708.0045%2C0708.4383%2C0708.0366%2C0708.1822%2C0708.2685%2C0708.0728%2C0708.1058%2C0708.0044%2C0708.2297%2C0708.4040%2C0708.1780%2C0708.3945&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider the problem of estimating sparse graphs by a lasso penalty\napplied to the inverse covariance matrix. Using a coordinate descent procedure\nfor the lasso, we develop a simple algorithm that is remarkably fast: in the\nworst cases, it solves a 1000 node problem (~500,000 parameters) in about a\nminute, and is 50 to 2000 times faster than competing methods. It also provides\na conceptual link between the exact problem and the approximation suggested by\nMeinhausen and Buhlmann (2006). We illustrate the method on some cell-signaling\ndata from proteomics."}, "authors": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "author_detail": {"name": "Robert Tibshirani"}, "author": "Robert Tibshirani", "arxiv_comment": "submitted", "links": [{"href": "http://arxiv.org/abs/0708.3517v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0708.3517v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "C5C60", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0708.3517v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0708.3517v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:0708.3517v1 [stat.ME] 27 Aug 2007\n\nSparse inverse covariance estimation with the\nlasso\nJerome Friedman \u2217\nTrevor Hastie \u2020\nand Robert Tibshirani\u2021\nOctober 23, 2018\n\nAbstract\nWe consider the problem of estimating sparse graphs by a lasso\npenalty applied to the inverse covariance matrix. Using a coordinate\ndescent procedure for the lasso, we develop a simple algorithm that is\nremarkably fast: in the worst cases, it solves a 1000 node problem (\u223c\n500, 000 parameters) in about a minute, and is 50 to 2000 times faster\nthan competing methods. It also provides a conceptual link between\nthe exact problem and the approximation suggested by Meinshausen\n& B\u00fchlmann (2006). We illustrate the method on some cell-signaling\ndata from proteomics.\n\n1\n\nIntroduction\n\nIn recent years a number of authors have proposed the estimation of sparse\nundirected graphical models through the use of L1 (lasso) regularization.\nThe basic model for continuous data assumes that the observations have a\nmultivariate Gaussian distribution with mean \u03bc and covariance matrix \u03a3. If\nthe ijth component of \u03a3\u22121 is zero, then variables i and j are conditionally\n\u2217\n\nDept. of Statistics, Stanford Univ., CA 94305, jhf@stanford.edu\nDepts. of Statistics, and Health, Research & Policy, Stanford Univ., CA 94305,\nhastie@stanford.edu\n\u2021\nDepts. of Health, Research & Policy, and Statistics, Stanford Univ, tibs@stanford.edu\n\u2020\n\n1\n\n\findependent, given the other variables. Thus it makes sense to impose an L1\npenalty for the estimation of \u03a3\u22121 .\nMeinshausen & B\u00fchlmann (2006) take a simple approach to this problem:\nthey estimate a sparse graphical model by fitting a lasso model to each variable, using the others as predictors. The component \u03a3\u0302\u22121\nij is then estimated\nto be non-zero if either the estimated coefficient of variable i on j, or the\nestimated coefficient of variable j on i, is non-zero (alternatively they use an\nAND rule). They show that asymptotically, this consistently estimates the\nset of non-zero elements of \u03a3\u22121 .\nOther authors have proposed algorithms for the exact maximization of\nthe L1 -penalized log-likelihood; both Yuan & Lin (2007) and Banerjee et al.\n(2007) adapt interior point optimization methods for the solution to this\nproblem. Both papers also establish that the simpler approach of Meinshausen & B\u00fchlmann (2006) can be viewed as an approximation to the exact\nproblem.\nWe use the development in Banerjee et al. (2007) as a launching point,\nand propose a simple, lasso-style algorithm for the exact problem. This new\nprocedure is extremely simple, and is substantially faster than the interior\npoint approach in our tests. It also bridges the \"conceptual gap\" between\nthe Meinshausen & B\u00fchlmann (2006) proposal and the exact problem.\n\n2\n\nThe proposed method\n\nSuppose we have N multivariate normal observations of dimension p, with\nmean \u03bc and covariance \u03a3. Following Banerjee et al. (2007), let \u0398 = \u03a3\u22121 ,\nand let S be the empirical covariance matrix, the problem is to maximize the\nlog-likelihood\nlog det \u0398 \u2212 tr(S\u0398) \u2212 \u03c1||\u0398||1,\n\n(1)\n\nwhere tr denotes the trace and ||\u0398||1 is the L1 norm- the sum of the absolute\nvalues of the elements of \u03a3\u22121 . Expression (1) is the Gaussian log-likelihood\nof the data, partially maximized with respect to the mean parameter \u03bc.\nYuan & Lin (2007) solve this problem using the interior point method for\nthe \"maxdet\" problem, proposed by Vandenberghe et al. (1998). Banerjee\net al. (2007) develop a different framework for the optimization, which was\nthe impetus for our work.\n2\n\n\fBanerjee et al. (2007) show that the problem (1) is convex and consider\nestimation of \u03a3 (rather than \u03a3\u22121 ), as follows. Let W be the estimate of \u03a3.\nThey show that one can solve the problem by optimizing over each row and\ncorresponding column of W in a block coordinate descent fashion. Partitioning W and S\n\u0013\n\u0013\n\u0012\n\u0012\nS11 s12\nW11 w12\n,\n(2)\n, S=\nW =\nT\nsT12 s22\nw12\nw22\nthey show that the solution for w12 satisfies\n\u22121\n\u017512 = argminy {y T W11\ny : ||y \u2212 s12 ||\u221e \u2264 \u03c1}.\n\n(3)\n\nThis is a box-constrained quadratic program which they solve using an interior point procedure. Permuting the rows and columns so the target column\nis always the last, they solve a problem like (3) for each column, updating\ntheir estimate of W after each stage. This is repeated until convergence. Using convex duality, Banerjee et al. (2007) go on to show that (3) is equivalent\nto the dual problem\n1/2\n\nmin\u03b2 ||W11 \u03b2 \u2212 b||2 + \u03c1||\u03b2||1,\n\n(4)\n\n\u22121/2\n\nwhere b = W11 s12 /2. This expression is the basis for our approach.\nFirst we note that it is easy to verify the equivalence between the solutions\nto (1) and (4) directly. The sub-gradient equation for maximization of the\nlog-likelihood (1) is\nW \u2212 S \u2212 \u03c1 * \u0393 = 0,\n(5)\nusing the fact that the derivative of log det \u0398 equals \u0398\u22121 = W , given in e.g\nBoyd & Vandenberghe (2004), page 641. Here \u0393ij \u2208 sign(\u0398ij ); i.e. \u0393ij =\nsign(\u0398ij ) if \u0398ij 6= 0, else \u0393ij \u2208 [\u22121, 1] if \u0398ij = 0.\nNow the upper right block of equation (5) is\nw12 \u2212 s12 \u2212 \u03c1 * \u03b312 = 0,\n\n(6)\n\nusing the same sub-matrix notation as in (2).\nOn the other hand, the sub-gradient equation from (4) works out to be\n2W11 \u03b2 \u2212 s12 + \u03c1 * \u03bd = 0,\nwhere \u03bd \u2208 sign(\u03b2) element-wise.\n3\n\n(7)\n\n\fNow suppose (W, \u0393) solves (5), and hence (w12 , \u03b312 ) solves (6). Then\n\u22121\n\u03b2 = 12 W11\nw12 and \u03bd = \u2212\u03b312 solves (7). The equivalence of the first two\nterms is obvious. For the sign terms, since W11 \u03b812 + w12 \u03b822 = 0, we have\n\u22121\nthat \u03b812 = \u2212\u03b822 W11\nw12 (partitioned-inverse formula). Since \u03b822 > 0, then\n\u22121\nsign(\u03b812 ) = \u2212sign(W11\nw12 ) = \u2212sign(\u03b2).\nNow to the main point of this paper. Problem (4) looks like a lasso (L1 regularized) least squares problem. In fact if W11 = S11 , then the solutions\n\u03b2\u0302 are easily seen to equal one-half of the lasso estimates for the pth variable\non the others, and hence related to the Meinshausen & B\u00fchlmann (2006)\nproposal. As pointed out by Banerjee et al. (2007), W11 6= S11 in general\nand hence the Meinshausen & B\u00fchlmann (2006) approach does not yield\nthe maximum likelihood estimator. They point out that their block-wise\ninterior-point procedure is equivalent to recursively solving and updating\nthe lasso problem (4), but do not pursue this approach. We do, to great\nadvantage, because fast coordinate descent algorithms (Friedman et al. 2007)\nmake solution of the lasso problem very attractive.\nIn terms of inner products, the usual lasso estimates for the pth variable\non the others take as input the data S11 and s12 . To solve (4) we instead use\nW11 and s12 , where W11 is our current estimate of the upper block of W . We\nthen update w and cycle through all of the variables until convergence.\nNote that from (5), the solution wii = sii + \u03c1 for all i, since \u03b8ii > 0, and\nhence \u0393ii = 1. Here is our algorithm in detail:\nCovariance Lasso Algorithm\n1. Start with W = S +\u03c1I. The diagonal of W remains unchanged in what\nfollows.\n2. For each j = 1, 2, . . . p, 1, 2, . . . p, . . ., solve the lasso problem (4), which\ntakes as input the inner products W11 and s12 . This gives a p \u2212 1\nvector solution \u03b2\u0302. Fill in the corresponding row and column of W\nusing w = 2W11 \u03b2\u0302.\n3. Continue until convergence\nNote again that each step in step (2) implies a permutation of the rows\nand columns to make the target column the last. The lasso problem in step\n(2) above can be efficiently solved by coordinate descent (Friedman et al.\n4\n\n\f(2007),Wu & Lange (2007)). Here are the details. Letting V = W11 , then\nthe update has the form\nX\nVkj \u03b2\u0302k , \u03c1)/(2Vjj )\n(8)\n\u03b2\u0302j \u2190 S(s12j \u2212 2\nk6=j\n\nfor j = 1, 2, . . . p, j = 1, 2, . . . p, . . ., where S is the soft-threshold operator:\nS(x, t) = sign(x)(|x| \u2212 t)+ .\n\n(9)\n\nWe cycle through the predictors until convergence.\nNote that \u03b2\u0302 will typically be sparse, and so the computation w = 2W11 \u03b2\u0302\nwill be fast: if there are r non-zero elements, it takes rp operations.\nFinally, suppose our final estimate of \u03a3 is \u03a3\u0302 = W , and store the estimates\n\u03b2\u0302 from the above in the rows and columns of a p \u00d7 p matrix B\u0302 (note that\nthe diagonal of B\u0302 is not determined). Then we can obtain the pth row (and\ncolumn) of \u0398\u0302 = \u03a3\u0302\u22121 = W \u22121 as follows:\n1\nP\n\n\u0398\u0302pp =\n\u0398\u0302kp\n\nWpp \u2212 2 k6=p B\u0302kp Wkp\n= \u22122\u0398\u0302pp B\u0302kp ; k 6= p\n\n(10)\n\nInterestingly, if W = S, these are just the formulas for obtaining the inverse\nof a partitioned matrix. That is, if we set W = S and \u03c1 = 0 in the above\nalgorithm, then one sweep through the predictors computes S \u22121 , using a\nlinear regression at each stage.\n\n3\n\nTiming comparisons\n\nWe simulated Gaussian data from both sparse and dense scenarios, for a\nrange of problem sizes p. The sparse scenario is the AR(1) model taken from\nYuan & Lin (2007): \u03b2ii = 1, \u03b2i,i\u22121 = \u03b2i\u22121,i = 0.5, and zero otherwise. In\nthe dense scenario, \u03b2ii = 2,\u03b2ii\u2032 = 1 otherwise. We chose the the penalty\nparameter so that the solution had about the actual number of non-zero\nelements in the sparse setting, and about half of total number of elements\nin the dense setting. The convergence threshold was 0.0001. The covariance\nlasso procedure was coded in Fortran, linked to an R language function. All\ntimings were carried out on a Intel Xeon 2.80GH processor.\n5\n\n\fp\n100\n100\n200\n200\n400\n400\n\nProblem\nType\nsparse\ndense\nsparse\ndense\nsparse\ndense\n\n(1) Covariance\nLasso\n.018\n.038\n.070\n.324\n.601\n2.47\n\n(2) Approx (3) COVSEL\n.007\n.018\n.027\n.146\n.193\n.752\n\n34.67\n2.17\n> 205.35\n16.87\n> 1616.66\n313.04\n\nRatio of\n(3) to (1)\n1926.1\n57.1\n> 2933.6\n52.1\n> 2690.0\n126.5\n\nTable 1: Timings (seconds) for covariance lasso, Meinhausen-Buhlmann approximation, and COVSEL procedures.\nWe compared the covariance lasso to the COVSEL program provided by\nBanerjee et al. (2007). This is a Matlab program, with a loop that calls a C\nlanguage code to do the box-constrained QP for each column of the solution\nmatrix. To be as fair as possible to COVSEL, we only counted the CPU time\nspent in the C program. We set the maximum number of outer iterations to\n30, and following the authors code, set the the duality gap for convergence\nto 0.1.\nThe number of CPU seconds for each trial is shown in Table 1. In the\ndense scenarios for p = 200 and 400, COVSEL had not converged by 30\niterations. We see that the covariance Lasso is 50 to 2000 times faster than\nCOVSEL, and only about 3 times slower than the approximate method. Thus\nthe covariance lasso is taking only about 3 passes through the the columns\nof W on average.\nFigure 1 shows the number of CPU seconds required for the covariance\nlasso procedure, for problem sizes up to 1000. Even in the dense scenario, it\nsolves a 1000 node problem (\u223c 500, 000 parameters) is about a minute.\n\n4\n\nAnalysis of cell signalling data\n\nFor illustration we analyze a flow cytometry dataset on p = 11 proteins and\nn = 7466 cells, from Sachs et al. (2003). These authors fit a directed acyclic\ngraph (DAG) to the data, producing the network in Figure 2.\nThe result of applying the covariance Lasso to these data is shown in\nFigure 3, for 12 different values of the penalty parameter \u03c1. There is moderate\n6\n\n\f70\n60\n40\n30\n0\n\n10\n\n20\n\nNumber of seconds\n\n50\n\nsparse\ndense\n\n200\n\n400\n\n600\n\n800\n\n1000\n\nNumber of variables p\n\nFigure 1: Number of CPU seconds required for the covariance lasso procedure.\n\nRaf\nJnk\n\nMek\nPlcg\n\nP38\n\nPKC\n\nPIP2\n\nPIP3\n\nPKA\nAkt\n\nErk\n\nFigure 2: Directed acylic graph from cell-signaling data, from Sachs et al. (2003).\n\n7\n\n\fRaf\n\nRaf\nJnk\n\nMek\nPlcg\n\nP38\n\nPKC\n\nPIP2\n\nPIP3\n\nPKA\n\nPlcg\n\nPKC\n\nPIP2\n\nPIP3\n\nPKA\n\nL1 norm= 2.06117\n\nPlcg\n\nPIP3\n\nPKA\n\nPKC\n\nPIP2\n\nPIP3\n\nL1 norm= 0.04336\n\nRaf\n\nPKA\n\nPlcg\n\nPKC\n\nPIP2\n\nPIP3\n\nPKA\n\nL1 norm= 0.02199\n\nPlcg\n\nP38\n\nPKC\n\nPIP2\n\nPIP3\n\nAkt\n\nErk\n\nPKA\n\nL1 norm= 0.01228\n\nRaf\nJnk\n\nMek\nPlcg\n\nPKC\n\nPIP2\n\nPIP3\n\nRaf\nJnk\n\nMek\nP38\n\nPKA\n\nPlcg\n\nPKC\n\nPIP2\n\nPIP3\n\nPKA\n\nL1 norm= 0.00927\n\nPlcg\n\nP38\n\nPKC\n\nPIP2\n\nPIP3\n\nAkt\n\nErk\n\nPKA\n\nL1 norm= 0.00496\n\nRaf\nJnk\n\nMek\nPlcg\n\nPKC\n\nPIP2\n\nPIP3\n\nPKA\nAkt\n\nL1 norm= 0.00372\n\nRaf\nJnk\n\nMek\nP38\n\nPlcg\n\nPKC\n\nPIP2\n\nPIP3\n\nPKA\nAkt\n\nL1 norm= 0.00297\n\nJnk\n\nMek\nP38\n\nErk\n\nAkt\n\nErk\n\nL1 norm= 0.00687\n\nRaf\n\nJnk\n\nMek\nP38\n\nAkt\n\nAkt\n\nErk\n\nL1 norm= 0.01622\n\nRaf\n\nJnk\n\nMek\nP38\n\nAkt\n\nAkt\n\nErk\n\nJnk\n\nMek\nP38\n\nErk\n\nPKC\n\nPIP2\n\nRaf\nJnk\n\nErk\n\nP38\n\nL1 norm= 0.09437\n\nRaf\nMek\n\nErk\n\nPlcg\n\nAkt\n\nErk\n\nJnk\n\nMek\nP38\n\nAkt\n\nErk\n\nRaf\nJnk\n\nMek\n\nPlcg\n\nP38\n\nPKC\n\nPIP2\n\nPIP3\n\nPKA\nErk\n\nAkt\n\nL1 norm= 7e\u221205\n\nFigure 3: Cell-signaling data: undirected graphs from covariance lasso with different values of the penalty parameter \u03c1.\n\n8\n\n\fagreement between, for example, the graph for L1 norm = 0.00496 and the\nDAG: the former has about half of the edges and non-edges that appear in\nthe DAG. Figure 4 shows the lasso coefficients as a function of total L1 norm\nof the coefficient vector.\nIn the left panel of Figure 5 we tried two different kinds of 10-fold crossvalidation for estimation of the parameter \u03c1. In the \"Regression\" approach,\nwe fit the covariance-lasso to nine-tenths of the data, and used the penalized\nregression model for each protein to predict the value of that protein in the\nvalidation set. We then averaged the squared prediction errors over all 11\nproteins. In the \"Likelihood\" approach, we again applied the covariancelasso to nine-tenths of the data, and then evaluated the log-likelihood (1)\nover the validation set. The two cross-validation curves indicate that the\nunregularized model is the best, not surprising give the large number of\nobservations and relatively small number of parameters. However we also see\nthat the likelihood approach is far less variable than the regression method.\nThe right panel compares the cross-validated sum of squares of the exact\ncovariance lasso approach to the Meinhausen-Buhlmann approximation. For\nlightly regularized models, the exact approach has a clear advantage.\n\n5\n\nDiscussion\n\nWe have presented a simple and fast algorithm for estimation of a sparse\ninverse covariance matrix using an L1 penalty. It cycles through the variables,\nfitting a modified lasso regression to each variable in turn. The individual\nlasso problems are solved by coordinate descent.\nThe speed of this new procedure should facilitate the application of sparse\ninverse covariance procedures to large datasets involving thousands of parameters.\nFortran and R language routines for the proposed methods will be made\nfreely available.\nAcknowledgments\nWe thank the authors of Banerjee et al. (2007) for making their COVSEL\nprogram publicly available, and Larry Wasserman for helpful discussions.\nFriedman was partially supported by grant DMS-97-64431 from the National\nScience Foundation. Hastie was partially supported by grant DMS-0505676\n9\n\n\fRaf\u2212Erk\n\n0.0\n\nRaf\u2212PKC\n\n\u22120.2\n\nPKC\u2212Jnk\nPlcg\u2212PIP2\n\nbeta\n\nPKC\u2212P38\n\n\u22120.6\n\n\u22120.4\n\nErk\u2212Akt\n\nRaf\u2212Mek\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\n2.0\n\nL1 norm\n\nFigure 4: Cell-signaling data: profile of coefficients as the total L1 norm of the coefficient vector increases, that is, as \u03c1 decreases. Profiles for the largest coefficients\nare labeled with the corresponding pair of proteins.\n\n10\n\n\f1e\u221202\n\n100\n\n200\n\ncv error\n1e\u221204\n\n300\n\n400\n\n100\n80\n60\n\ncv error\n\n40\n\nRegression\nLikelihood\n\nExact\nApproximate\n\n1e+00\n\n1e\u221204\n\nlog L1 norm\n\n1e\u221202\n\n1e+00\n\nlog L1 norm\n\nFigure 5: Cell-signaling data. Left panel shows tenfold cross-validation using both\nRegression and Likelihood approaches (details in text). Right panel compares the regression sum of squares of the exact covariance lasso approach to the MeinhausenBuhlmann approximation.\n\n11\n\n\ffrom the National Science Foundation, and grant 2R01 CA 72028-07 from\nthe National Institutes of Health. Tibshirani was partially supported by\nNational Science Foundation Grant DMS-9971405 and National Institutes of\nHealth Contract N01-HV-28183.\n\nReferences\nBanerjee, O., Ghaoui, L. E. & d'Aspremont, A. (2007), 'Model selection\nthrough sparse maximum likelihood estimation', To appear, J. Machine\nLearning Research 101.\nBoyd, S. & Vandenberghe, L. (2004), Convex Optimization, Cambridge University Press.\nFriedman, J., Hastie, T. & Tibshirani, R. (2007), 'Pathwise coordinate optimization', Annals of Applied Statistics, to appear .\nMeinshausen, N. & B\u00fchlmann, P. (2006), 'High dimensional graphs and variable selection with the lasso', Annals of Statistics 34, 1436\u20131462.\nSachs, K., Perez, O., Pe'er, D., Lauffenburger, D. & Nolan, G. (2003),\n'Causal protein-signaling networks derived from multiparameter singlecell data', Science (308 (5721)), 504\u20136.\nVandenberghe, L., Boyd, S. & Wu, S.-P. (1998), 'Determinant maximization with linear matrix inequality constraints', SIAM Journal on Matrix\nAnalysis and Applications 19(2), 499\u2013533.\n*citeseer.ist.psu.edu/vandenberghe98determinant.html\nWu, T. & Lange, K. (2007), Coordinate descent procedures for lasso penalized\nregression.\nYuan, M. & Lin, Y. (2007), 'Model selection and estimation in the gaussian\ngraphical model', Biometrika 94(1), 19\u201335.\n\n12\n\n\f"}