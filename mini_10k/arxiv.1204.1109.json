{"id": "http://arxiv.org/abs/1204.1109v2", "guidislink": true, "updated": "2012-07-10T06:58:13Z", "updated_parsed": [2012, 7, 10, 6, 58, 13, 1, 192, 0], "published": "2012-04-05T02:08:47Z", "published_parsed": [2012, 4, 5, 2, 8, 47, 3, 96, 0], "title": "Crossing Statistic: Reconstructing the Expansion History of the Universe", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.3274%2C1204.2396%2C1204.5384%2C1204.0489%2C1204.2674%2C1204.1569%2C1204.0188%2C1204.2450%2C1204.2140%2C1204.1686%2C1204.2242%2C1204.3254%2C1204.5038%2C1204.4004%2C1204.4975%2C1204.1716%2C1204.2615%2C1204.5238%2C1204.4118%2C1204.1842%2C1204.4311%2C1204.0472%2C1204.3597%2C1204.3154%2C1204.6269%2C1204.2013%2C1204.4769%2C1204.5752%2C1204.2655%2C1204.1691%2C1204.3487%2C1204.4042%2C1204.2182%2C1204.0081%2C1204.0863%2C1204.1841%2C1204.0083%2C1204.0838%2C1204.1109%2C1204.0446%2C1204.4581%2C1204.6503%2C1204.0361%2C1204.5405%2C1204.1623%2C1204.3721%2C1204.2472%2C1204.2624%2C1204.2360%2C1204.1900%2C1204.5589%2C1204.5419%2C1204.4277%2C1204.4657%2C1204.1660%2C1204.1170%2C1204.3175%2C1204.0922%2C1204.2925%2C1204.0089%2C1204.2557%2C1204.0575%2C1204.6455%2C1204.1548%2C1204.3116%2C1204.0641%2C1204.1279%2C1204.3894%2C1204.0823%2C1204.4809%2C1204.4040%2C1204.2095%2C1204.1882%2C1204.4055%2C1204.0548%2C1204.5157%2C1204.0120%2C1204.4874%2C1204.2503%2C1204.3059%2C1204.3369%2C1204.4399%2C1204.0882%2C1204.3947%2C1204.0231%2C1204.5084%2C1204.3714%2C1204.4540%2C1204.3919%2C1204.3720%2C1204.1521%2C1204.0761%2C1204.5105%2C1204.3992%2C1204.2550%2C1204.5971%2C1204.2566%2C1204.3164%2C1204.1014%2C1204.3961%2C1204.6005&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Crossing Statistic: Reconstructing the Expansion History of the Universe"}, "summary": "We present that by combining Crossing Statistic and Smoothing method one can\nreconstruct the expansion history of the universe with a very high precision\nwithout considering any prior on the cosmological quantities such as the\nequation of state of dark energy. We show that the presented method performs\nvery well in reconstruction of the expansion history of the universe\nindependent of the underlying models and it works well even for non-trivial\ndark energy models with fast or slow changes in the equation of state of dark\nenergy. Accuracy of the reconstructed quantities along with independence of the\nmethod to any prior or assumption gives the proposed method advantages to the\nother non-parametric methods proposed before in the literature. Applying on the\nUnion 2.1 supernovae combined with WiggleZ BAO data we present the\nreconstructed results and test the consistency of the two data sets in a model\nindependent manner. Results show that latest available supernovae and BAO data\nare in good agreement with each other and spatially flat LCDM model is in\nconcordance with the current data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.3274%2C1204.2396%2C1204.5384%2C1204.0489%2C1204.2674%2C1204.1569%2C1204.0188%2C1204.2450%2C1204.2140%2C1204.1686%2C1204.2242%2C1204.3254%2C1204.5038%2C1204.4004%2C1204.4975%2C1204.1716%2C1204.2615%2C1204.5238%2C1204.4118%2C1204.1842%2C1204.4311%2C1204.0472%2C1204.3597%2C1204.3154%2C1204.6269%2C1204.2013%2C1204.4769%2C1204.5752%2C1204.2655%2C1204.1691%2C1204.3487%2C1204.4042%2C1204.2182%2C1204.0081%2C1204.0863%2C1204.1841%2C1204.0083%2C1204.0838%2C1204.1109%2C1204.0446%2C1204.4581%2C1204.6503%2C1204.0361%2C1204.5405%2C1204.1623%2C1204.3721%2C1204.2472%2C1204.2624%2C1204.2360%2C1204.1900%2C1204.5589%2C1204.5419%2C1204.4277%2C1204.4657%2C1204.1660%2C1204.1170%2C1204.3175%2C1204.0922%2C1204.2925%2C1204.0089%2C1204.2557%2C1204.0575%2C1204.6455%2C1204.1548%2C1204.3116%2C1204.0641%2C1204.1279%2C1204.3894%2C1204.0823%2C1204.4809%2C1204.4040%2C1204.2095%2C1204.1882%2C1204.4055%2C1204.0548%2C1204.5157%2C1204.0120%2C1204.4874%2C1204.2503%2C1204.3059%2C1204.3369%2C1204.4399%2C1204.0882%2C1204.3947%2C1204.0231%2C1204.5084%2C1204.3714%2C1204.4540%2C1204.3919%2C1204.3720%2C1204.1521%2C1204.0761%2C1204.5105%2C1204.3992%2C1204.2550%2C1204.5971%2C1204.2566%2C1204.3164%2C1204.1014%2C1204.3961%2C1204.6005&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present that by combining Crossing Statistic and Smoothing method one can\nreconstruct the expansion history of the universe with a very high precision\nwithout considering any prior on the cosmological quantities such as the\nequation of state of dark energy. We show that the presented method performs\nvery well in reconstruction of the expansion history of the universe\nindependent of the underlying models and it works well even for non-trivial\ndark energy models with fast or slow changes in the equation of state of dark\nenergy. Accuracy of the reconstructed quantities along with independence of the\nmethod to any prior or assumption gives the proposed method advantages to the\nother non-parametric methods proposed before in the literature. Applying on the\nUnion 2.1 supernovae combined with WiggleZ BAO data we present the\nreconstructed results and test the consistency of the two data sets in a model\nindependent manner. Results show that latest available supernovae and BAO data\nare in good agreement with each other and spatially flat LCDM model is in\nconcordance with the current data."}, "authors": ["Arman Shafieloo"], "author_detail": {"name": "Arman Shafieloo"}, "author": "Arman Shafieloo", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1088/1475-7516/2012/08/002", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1204.1109v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.1109v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "12 pages, 4 figures, discussions extended, 1 figure added, results\n  unchanged, accepted for publication in JCAP", "arxiv_primary_category": {"term": "astro-ph.CO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "astro-ph.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.1109v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.1109v2", "journal_reference": "JCAP 08 (2012) 002", "doi": "10.1088/1475-7516/2012/08/002", "fulltext": "Preprint typeset in JHEP style - HYPER VERSION\n\narXiv:1204.1109v2 [astro-ph.CO] 10 Jul 2012\n\nCrossing Statistic: Reconstructing the\nExpansion History of the Universe\n\nArman Shafieloo\nInstitute for the Early Universe, Ewha Womans University\nSeoul, 120-750, South Korea\nE-mail: arman@ewha.ac.kr\n\nAbstract: We present that by combining Crossing Statistic [1, 2] and Smoothing\nmethod [3\u20135] one can reconstruct the expansion history of the universe with a very\nhigh precision without considering any prior on the cosmological quantities such as\nthe equation of state of dark energy. We show that the presented method performs\nvery well in reconstruction of the expansion history of the universe independent of\nthe underlying models and it works well even for non-trivial dark energy models\nwith fast or slow changes in the equation of state of dark energy. Accuracy of\nthe reconstructed quantities along with independence of the method to any prior\nor assumption gives the proposed method advantages to the other non-parametric\nmethods proposed before in the literature. Applying on the Union 2.1 supernovae\ncombined with WiggleZ BAO data we present the reconstructed results and test the\nconsistency of the two data sets in a model independent manner. Results show that\nlatest available supernovae and BAO data are in good agreement with each other\nand spatially flat \u039bCDM model is in concordance with the current data.\nKeywords: Supernovae, dark energy, cosmological parameter estimation.\n\n\fContents\n1. Introduction\n\n1\n\n2. Method and Analysis\n2.1 Smoothing Method\n2.2 Crossing Statistic and Reconstructing Dark Energy\n2.3 Real Current SN Ia and BAO Data\n\n2\n2\n3\n6\n\n3. Conclusion\n\n8\n\n1. Introduction\nReconstructing the expansion history of the universe and properties of dark energy\nis one of the main goals of today's cosmology to understand our universe and its\ncomponents. There have been many approaches in last decade proposed to do the\nreconstruction of the expansion history and one can generalize them in two categories of parametric and non-parametric methods. Parametric methods are viable\napproaches if we know the actual class-form of the phenomena we are studying and\nwe can use them to put constraints on the parameters of the model. See [6\u201310] for\ndetails of data analysis and methods of parametric reconstruction of the properties\nof dark energy using supernovae data. However dealing with phenomena that we\nhave no clear idea about its nature and behavior, using parametric methods can be\nmisleading since the underlying actual model might not be covered by the assumed\nparametric form. Dealing with uncertainties in the dispersion of the data adds another complication to the analysis and leave us with no clear way to find this fact\nthat we might have chosen an inappropriate parametric form. This raises the importance of the non-parametric and model independent approaches to find out the\nbehavior of the phenomena in a more direct way by avoiding parametrizing cosmological quantities [3\u20135, 11\u201321]. However non-parametric approaches have their own\nshortcomings. For instance, estimation of the errors can be a tricky task in many\ncases since in some methods one cannot easily assign the degree of freedom in the\nlikelihood analysis. For a review over this subject look at [22].\nSmoothing method was proposed to reconstruct the expansion history of the universe in a completely model independent way as a top-down approach starting with\nthe data to reconstruct the cosmological quantities step by step from the luminosity\n\n\u20131\u2013\n\n\fdistance dL (z) to Hubble parameter h(z) and the deceleration parameter q(z). This\nmethod has been used broadly since its proposition and has passed different tests\nand has shown its strength. Nevertheless, while smoothing method is a promising\napproach to reconstruct the underlying expansion history of the universe and probably the best way to find a feature in a dataset, estimation of the error-bars on the\nreconstructed quantities have been an issue since it is not possible to define degrees\nof freedom in this approach. Error amplification because of taking the derivatives\n(moving from smooth dL (z) to h(z) and then q(z)) and at the same time controlling\nthe bias in the reconstruction are another important related issues in the smoothing\nmethod. In this paper we use the idea of Bayesian interpretation of Crossing Statistic [1, 2] to estimate the uncertainties of the reconstructed cosmological quantities\nin a well defined and robust statistical manner. Bayesian interpretation of Crossing\nStatistic [2] was introduced recently for the purpose of model selection and falsifying\ncosmological models in a purely model independent manner while we have no information about the underlying model of the universe. In this paper by combining the\ntwo methods, error-sensitive smoothing method and the Crossing Statistic, we reconstruct the expansion history of the universe and estimate the redshift evolution of\nsome cosmological quantities such as deceleration parameter q(z) and Om diagnostic\nOm(z).\nIn the following we will explain first briefly about the smoothing method and\nhow it can be used for the reconstruction of the properties of dark energy. Next\nwe explain about the Crossing Statistic and its Bayesian interpretation and then\nwe introduce how we can combine the two methods to reconstruct the expansion\nhistory of the universe and set the confidence limits. Efficiency of the method will be\ntested using simulated data and it will be shown how precisely it can reconstruct the\nexpansion history of the universe. In fact it is not unfair to claim that the proposed\nmethod works with considerably higher precision in comparison to other available\nnon-parametric methods while here we are not even using any priors in the analysis.\nThen I apply the method on the current supernovae and Baryon Acoustic Oscillation\n(BAO) data. At the end I summarize and highlight the advantages of this approach\nto other available methods.\n\n2. Method and Analysis\n2.1 Smoothing Method\nSmoothing method is a completely model independent approach to derive the dL (z)\nrelation directly from the data, without any assumptions other than the introduction\nof a smoothing scale. The only parameter used in the smoothing method is the\nsmoothing width \u2206, which is constrained only by the quality and quantity of the\ndata, and has nothing to do with any cosmological model. The smoothing method\n\n\u20132\u2013\n\n\fis an iterative procedure with each iteration typically giving a better fit to the data.\nIt has been shown in [3\u20135] that the final reconstructed results are independent of the\nassumed initial guess, dL (zi )g below.\nThe modified smoothing method (error-sensitive) can be summarized by the following equation [5]:\n\nln dL (z, \u2206)s = ln dL (z)g\n\"\n\u0001#\ni\nX [ln dL (zi ) \u2212 ln dL (zi )g ]\nln2 1+z\n1+z\n+N (z)\nexp \u2212\n,\n2\n2\n\u03c3\n2\u2206\ndL (zi )\ni\n\"\n\u0001#\n2 1+zi\nX\nln\n1\n1+z\nN (z)\u22121 =\nexp \u2212\n.\n2\n2\n2\u2206\n\u03c3dL (zi )\ni\n\n(2.1)\n\nwhere dL (z) is the data, N (z) is the normalization factor, dL (zi )g is the initial\nguess model and \u2206 is the width of smoothing.\nThe absolute brightness of the supernovae is degenerate with H0 since the observed quantity is the distance modulus \u03bc(z). The outcome of the smoothing method\nis therefore H0 dL (z)/c \u2261 drec\nL (z) = (1 + z)D(z). In this paper I choose \u2206 = 0.30\nwhich is similar to the value being used in [5]. Complete explanation of the relations\nbetween the \u2206, the number of data points, quality of the data and the reconstructed\nresults can be found in [3, 4]. It has been shown before that smoothing method is\na promising approach to reconstruct the expansion history of the universe however,\nsetting the confidence limits have been an issue and previously bootstrap approach\nwas being used to set the confidence limits. In this paper, the reconstructed form of\nthe dL (z) will be used as a mean function in the full reconstruction process which includes the idea of Bayesian interpretation of Crossing Statistic as it will be explained\nin the next section.\n2.2 Crossing Statistic and Reconstructing Dark Energy\nThe main idea behind this work lies on this fact that the actual model of the universe\nand the reconstructed models using smoothing method would have one or two mild\ncrossings (the distance modules of the actual model of the universe and the reconstructed \u03bc(z) from smoothing method would cross each other at one or two points\nin the data redshift range) and Crossing functions multiplied by the mean functions\ngenerated by the smoothing method would cover the actual model of the universe\nto an indistinguishable level. It is in fact combining a non-parametric method with\na parametric method to define and set the confidence limits. Crossing function is\ndefined by Chebichev polynomials [2]:\nTII (C1 , C2 , z) = 1 + C1 (\n\nz\nzmax\n\n\u20133\u2013\n\n) + C2 [2(\n\nz\nzmax\n\n)2 \u2212 1],\n\n(2.2)\n\n\fII\nand we fit \u03bcTSmooth\n(z) = \u03bcSmooth (z) \u00d7 TII (C1 , C2 , z) to the data and find the\nbest\nbest\nbest fit point C1 , C2 in the hyperparameter space and also C1 , C2 points related\nto the 1\u03c3, 2\u03c3 and 3\u03c3 confident limits. Each TII (C1 , C2 , z) (where C1 , C2 pairs are\nwithin hyperparameter confidence contours) multiplied to \u03bcSmooth (z) represents a\nreconstruction of the expansion history of the universe consistent to the data.\n\nWe should note that though the reconstructed luminosity distances are model\nindependent, the derived uncertainties do depend on the number of Chebichev terms\nincluded. In fact depends on the number of Chebichev terms, the \u2206\u03c72 (with respect\nto the best fit point) for different confidence limits can vary and this can affect the\nwidth of the error-band. In this paper to estimate the errors we have included the\nChebichev polynomials up to the second order and this selection is based on an\nargument we have discussed in [2]. The fact is that within the range of the data\nup to redshift of around z = 2, we cannot expect to have more than two crossings\nbetween different assumed models. This is even valid for models with significant\ndifferences, e.g SCDM and LCDM models that though they are so different in their\npredictions for the expansion history of the universe they do not cross each other\nin more than two points (for different values of matter density). This is due to the\nfact that the observables, \u03bc(z) are integrated quantities over the inverse of Hubble\nparameter (expansion history) and they monotonically increases by redshift. In [2]\nwe have discussed this issue in more details that the data is not sensitive to more\nthan two crossing between models (equivalently we can include up to the second order\nof Chebichev polynomials for error estimation). However, this is true that in some\ncases even assuming one crossing term (using Chebichev polynomial of the first term\nonly) would be sufficient for the purpose of error estimation but it is always safer to\nconsider all unexpected possibilities. In other words, having a broader error-bands\nwhich contains the fiducial model with a very high certainty would be more reliable\nand acceptable than having a very narrow error-band for the reconstructed results\nbut with possibilities of missing the fiducial model at some redshift ranges. As we\nwill see in the results, in all assumed cases for different models of dark energy (even\nfor the non trivial kink model) the reconstruction method works pretty well and the\nfiducial models are covered by the error-bands at all redshift ranges.\nTo test the method we apply first the procedure to the simulated data. I assume\nthree sets of simulated data based on three dark energy models (all spatially flat\nuniverses). First model is an evolving dark energy model or Kink model with the\nsame parameters used in [2, 24]. Equation of state of dark energy in this particular\nmodel is given by:\n\nw(z) = w0 + (wm \u2212 w0 )\n\n\u22121\n1 + exp(\u2206\u22121\nt (1 + zt ) )\n1 \u2212 exp(\u2206\u22121\nt )\n\n\u20134\u2013\n\n(2.3)\n\n\f\u0015\n\u22121\n\u22121\nexp(\u2206\u22121\nt ) + exp(\u2206t (1 + zt ) )\n\u00d7 1\u2212\n,\n\u22121\n\u22121\n\u22121\nexp(\u2206\u22121\nt (1 + z) ) + exp(\u2206t (1 + zt ) )\n\u0014\n\nwith the constants having the values w0 = \u22121.0, wm = \u22120.5, zt = 0.5, \u2206t = 0.05.\nI assume \u03a9m = 0.27 in the simulations. Second model is based on a dark energy\nmodel with constant equation of state of w(z) = 0.9 and with \u03a9m = 0.27 and finally\nthe third simulated data is based on \u039bCDM model with w(z) = \u22121 and \u03a9m = 0.27.\nSimulated data are based on future space based supernovae data with 2298 data\npoints in the range of 0.015 < z < 1.7 with intrinsic dispersion of \u03c3int = 0.13 [25].\nThe Kink model I used in this analysis has a special form of the equation of state of\ndark energy that at low redshifts it converges to w(z) = \u22121 and at higher redshifts\nit smoothly changes to w = \u22120.5. This evolving equation of state of dark energy is\nnot reducible to the common form of w(z) = w0 + wa z/(1 + z) [26] or many other\ndark energy parametrizations. Hence using usual parameterizations results to wrong\nreconstruction of dark energy equation of state if the data is based on the Kink model\n(look at upper-right panel of figure. 6 in [24]).\nDeriving dL (z) we can accordingly derive h(z), Om(z) and q(z):\n\u0014 \u0012\n\u0013\u0015\u22121\nd dL (z)\nH(z) =\n(2.4)\ndz 1 + z\nH 0 (z)\nq(z) = (1 + z)\n\u22121\n(2.5)\nH(z)\nh2 (z) \u2212 1\n(2.6)\nOm(z) =\n(1 + z)3 \u2212 1\nwhere derivatives are respect to redshift and h(z) = H(z)/H0 . To derive all these\nquantities we do not need to know the value of the matter density and in particular\nOm(z) can be used directly to falsify the cosmological constant which requires Om(z)\nshould be a constant at all redshifts [15].\nWe should note that the observables \u03bc(z) that are tightly related to the luminosity distances dL (z), are integrated properties that are functions of the evolution\nof the universe and there might be different dark energy models (assuming different\nmatter density and curvature) that result to the same observables. The concept of\ncosmographic degeneracy does exist and make it almost impossible to distinguish\nbetween some certain dark energy models if we have no tight constraints on the curvature and matter density [27]. However, in this paper we try to avoid this issue\nand we work on h(z) and q(z) directly. The reason is that unlike the equation of\nstate of dark energy w(z), these quantities h(z) and q(z) are direct derivatives of\nthe reconstructed distance results and they are independent of the actual value of\nthe matter density. We have also assumed a flat spatial curvature through out the\npaper.\nTo have an idea of the reconstruction process, in figure. 1 we show a set of\nsimulated data and the reconstructed \u03bc(z) and its error bands. The simulated data\n\n\u20135\u2013\n\n\f0.6\n\nSimulated Data\n3\u03c3 CL\n2\u03c3 CL\n1\u03c3 CL\nFiducial Model\n\n0.4\n\n\u2206\u03bc(z)\n\n0.2\n\n0\n\n-0.2\n\n-0.4\n\n-0.6\n0.2\n\n0.4\n\n0.6\n\n0.8\nz\n\n1\n\n1.2\n\n1.4\n\n1.6\n\nFigure 1: A set of simulated data and the reconstructed \u03bc(z) and its error bands. The\nsimulated data is based on a spatially flat LCDM model and the actual model is subtracted\nfrom the data and the reconstructed results for better clarification.\n\nis based on a spatially flat LCDM model and the fiducial model is subtracted from\nthe data and the reconstructed results for better clarification. In this paper by 1\u03c3,\n2\u03c3 and 3\u03c3 we mean 68%, 95% and 99% confidence limits respectively.\nIn figure. 2 we see the reconstructed h(z), Om(z) and q(z) for the three assumed\nmodels. One can clearly see that the reconstruction procedure performs perfectly\nwell in reconstructing the cosmological quantities. One should notice that here we\nare not assuming any particular prior on any cosmological quantity which gives it an\nadvantage to other reconstruction processes.\n2.3 Real Current SN Ia and BAO Data\nNow we can apply the method on the current available data. We use Union2.1\nsupernovae compilation [28] that is the most recent compilation of the supernovae\ndata along with WiggleZ baryon acoustic oscillation data [29]. Using BAO data we\nconsider the dz ratios all scaled to dz at z = 0.106 to make the BAO data independent\nof the knowledge of the early universe (look at table. 1). dz is given by:\ndz =\n\nrs (zCM B )\nDV (z)\n\n(2.7)\n\nwhere rs (zCM B ) is the sound horizon at the epoch when CMB photons decouple\nfrom baryons and DV (z) is the dilation-scale distance,\ndL (z)2\nDV (z) =\n(1 + z)2\n\u0014\n\n\u0012\n\ncz\nH(z)\n\n\u0013\u00151/3\n(2.8)\n\nIn fact by using the dz ratios we remove the rS (zCM B ) from the equations and\nminimize the effects of the assumptions of the early universe from our analysis.\nSimilar to the procedure we used in the previous section we apply first the smoothing\nmethod on the supernovae data to reconstruct the mean functions and then we use\n\n\u20136\u2013\n\n\fFigure 2: Reconstructed h(z), Om(z) and q(z) with their 1\u03c3 (green), 2\u03c3 (blue) and 3\u03c3\n(magenta) confidence limits for three sets of the simulated JDEM data based on a kink\ndark energy model on the top, \u039bCDM in the middle and w = \u22120.9 in the bottom, all\nwith \u03a90m = 0.27. Strength of the method in reconstructing the cosmological quantities is\nobvious. Black double-dot lines represents spatially flat \u039bCDM model with \u03a90m = 0.27 for\ncomparison.\n\nthese mean functions along with Crossing functions fitting supernovae and BAO\ndata to find out the Crossing hyper parameters and define the confidence limits.\nReconstructed Om(z) and q(z) using Union 2.1 data is shown in figure. 3 upper\npanels. In the lower panels we can see the same quantities using combination of\nUnion 2.1 supernovae and WiggleZ BAO data. One can clearly see that the standard\nflat \u039bCDM model has a proper consistency to the data. In figure. 4 we see the 1\u03c3\nand 2\u03c3 confidence contours of C1 , C2 hyperparameters using supernovae data and\nBAO data individually. One should remember that these are the coefficients of the\nCrossing function around the mean function generated by smoothing method using\nsupernovae data. So it is expected that the C1 , C2 contours using supernovae data be\ncentered around 0, 0 point. This plot shows that the BAO and supernovae data are\nin proper consistency with each other without assuming any cosmological model. It\n\n\u20137\u2013\n\n\fInverse Covariance Matrix\ndz=0.106\ndz=0.20\n\ndz=0.106 /dz=0.20\ndz=0.106 /dz=0.35\ndz=0.106 /dz=0.44\ndz=0.106 /dz=0.60\ndz=0.106 /dz=0.73\n\ndz=0.106\ndz=0.35\n\n284.709 \u2212103.845\n\u2212103.845 91.940\n\u22124.773\n\u22122.552\n\u22129.600\n\u22125.133\n\u22126.961\n\u22123.722\n\ndz=0.106\ndz=0.44\n\ndz=0.106\ndz=0.60\n\ndz=0.106\ndz=0.73\n\nMean Values\n\n\u22124.773\n\u22122.552\n14.546\n\u22129.575\n2.275\n\n\u22129.600\n\u22125.133\n\u22129.575\n30.340\n\u221210.771\n\n\u22126.961\n\u22123.722\n2.275\n\u221210.771\n12.954\n\n1.764 \u00b1 0.097\n3.063 \u00b1 0.170\n3.668 \u00b1 0.328\n4.628 \u00b1 0.299\n5.676 \u00b1 0.398\n\nTable 1: WiggleZ baryon acoustic oscillation distance ratios scaled to dz=0.106 and their\ninverse covariance matrix [30]. Using the BAO distance ratios one can reduce the effects\nfrom the early universe on the distance measurements.\n\nalso shows that supernovae data is still a much more powerful probe of the expansion\nhistory of the universe in comparison to BAO data (notice the size of the confidence\ncontours). One reason lies on the fact that supernovae data covers a very large\nredshift range (from as low as z = 0.015 up to about z = 1.5) and also the number\nof data points are considerably larger in the supernovae compilations.\n\n3. Conclusion\nIn this paper we present a robust and easy to use method of model independent\nreconstruction of the expansion history of the universe which performs perfectly well\nfor any dark energy model. Combining the smoothing method and Crossings Statistic results in an approach which can be easily used to reconstruct the expansion\nhistory of the universe and the properties of dark energy without setting any prior\non the cosmological quantities. This is an important advantage over other available reconstruction methods in which one has to set some priors on the equation of\nstate of dark energy or some other cosmological parameters. The presented approach\nalso resolves the problem of defining the confidence limits around the reconstructed\ncosmological quantities using smoothing method. It has been shown before that\nsmoothing method is a very powerful approach to reconstruct the expansion history\nof the universe but because of its non-parametric nature, it was not possible to define the confidence limits in a straightforward way. In this work, using the idea of\nBayesian interpretation of Crossing Statistic this problem is resolved and one can\nset proper confidence limits on the reconstructed quantities. We have shown the\nstrength of the method in reconstruction of the expansion history of the universe\nusing simulated data where we could reconstruct different dark energy models, even\nsome non trivial ones, with a very high precision. We have also used the method to\ntest the consistency of the two important cosmological data sets, Union 2.1 supernovae data and WiggleZ BAO data in a completely model independent way. These\ntwo data sets are pretty well consistent with each other, however, at the current\n\n\u20138\u2013\n\n\fFigure 3: Reconstructed Om(z) and q(z) with their 1\u03c3 (green), 2\u03c3 (blue) and 3\u03c3 (magenta) confidence limits using Union2.1 supernovae data (on the top panels) and using\ncombination of Union2.1 supernovae and WiggleZ BAO data (on the bottom panels). One\ncan see that including BAO data into the analysis does not change the reconstructed results significantly. At the current status of the cosmological observations supernovae data\ncontain much more information on the expansion history of the universe in comparison to\nBAO data.\n\nstatus of cosmological observations supernovae data has much more power in reconstruction of the expansion history of the universe. Based on our analysis spatially\nflat \u039bCDM model has a proper concordance to the given Union 2.1 supernovae and\nWiggleZ BAO data, even though, we cannot still rule out many other dark energy\nmodels including other two models discussed in this paper.\n\nAcknowledgments\nAS thanks Eric Linder and Chris Blake for useful discussions. This work has been\nsupported by World Class University grant R32-2009-000-10130-0 through the National Research Foundation, Ministry of Education, Science and Technology of Korea.\n\nReferences\n[1] A. Shafieloo, T. Clifton and P. Ferreira, JCAP 08, 017 (2011).\n[2] A. Shafieloo, JCAP 05, 024 (2012).\n\n\u20139\u2013\n\n\fFigure 4: Confidence contours of the Crossing hyperparameters using Union 2.1 supernovae data (1\u03c3 CL in magenta, 2\u03c3 CL in blue) and WiggleZ BAO data (1\u03c3 CL in green, 2\u03c3\nCL in red). There is a clear consistency between the two data sets without assuming any\nparticular cosmological model. One can also see that supernovae data are more powerful\nin putting constraints on the expansion history of the universe in comparison to BAO data\nat the current status of cosmological observations.\n[3] A. Shafieloo, U. Alam, V. Sahni and A. A. Starobinsky, Mon. Not. Roy. Astron. Soc.\n366, 1081 (2006).\n[4] A. Shafieloo, Mon. Not. Roy. Astron. Soc. 380, 1573 (2007).\n[5] A. Shafieloo and C. Clarkson, Phys. Rev. D 81, 083537 (2010).\n[6] T. M. Davis, Astrophys. J. 666, 716 (2007).\n[7] P. Serra, A. Heavens and A. Melchiorri, Mon. Not. Roy. Astron. Soc. 379, 169\n(2007).\n[8] E. Linder and R. Miquel, Int. J. Mod. Phys. D 17, 2315 (2008).\n[9] J. Sollerman, Astrophys. J. 703 1374 (2009).\n[10] M. Kilbinger et al. Mon. Not. Roy. Astron. Soc. 405, 2381 (2010).\n[11] R. A. Daly and S. G. Djorgovski, Astrophys. J. 597, 9 (2003).\n[12] D. Huterer and G. Starkman, Phys. Rev. Lett. 90 031301 (2003).\n[13] Y. Wang and P. Mukherjee, Astrophys. J. 606, 654 (2004).\n\n\u2013 10 \u2013\n\n\f[14] Y. Wang and M. Tegmark, Phys. Rev. D 71, 103513 (2005).\n[15] V. Sahni, A. Shafieloo and A. A. Starobinsky, Phys. Rev. D 78, 103502 (2008).\n[16] C. Zunckel and C. Clarkson, Phys. Rev. Lett 101, 181301 (2008).\n[17] A. Shafieloo, V. Sahni and A. A. Starobinsky, Phys. Rev. D 80 R, 101301 (2009).\n[18] C. Clarkson and C. Zunckel, Phys. Rev. Lett. 104, 211301 (2010).\n[19] S. Nesseris and A. Shafieloo, Mon. Not. Roy. Astron. Soc. 408, 1879 (2010).\n[20] T. Holsclaw et. al, Phys.Rev. Lett 105, 241302 (2010).\n[21] R. Crittenden et al, arXiv:1112.1693 (2011).\n[22] V. Sahni and A. A. Starobinsky, Int. J. Mod. Phys. D 15, 2105 (2006).\n[23] R. J. Barlow, Statistics, John Wiley & Sons Ltd, (1989).\n[24] T. Holsclaw et. al, Phys. Rev. D 82, 103502 (2010).\n[25] http://jdem.lbl.gov/\n[26] M. Chevallier and D. Polarski, Int. J. Mod. Phys. D 10, 213 (2010); E.V. Linder,\nPhys. Rev. Lett. 90, 091301 (2003).\n[27] A. Shafieloo and E. Linder, Phys. Rev. D 84, 063519 (2011).\n[28] N. Suzuki et al., arXiv:1105.3470.\n[29] C. Blake et al., , Mon. Not. Roy. Astron. Soc. 418, 1725B (2011)\n[30] Chris Blake, private communication.\n\n\u2013 11 \u2013\n\n\f"}