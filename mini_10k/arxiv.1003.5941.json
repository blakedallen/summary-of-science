{"id": "http://arxiv.org/abs/1003.5941v1", "guidislink": true, "updated": "2010-03-30T22:26:32Z", "updated_parsed": [2010, 3, 30, 22, 26, 32, 1, 89, 0], "published": "2010-03-30T22:26:32Z", "published_parsed": [2010, 3, 30, 22, 26, 32, 1, 89, 0], "title": "A lower bound for distributed averaging algorithms", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1003.1411%2C1003.5983%2C1003.2668%2C1003.6071%2C1003.5354%2C1003.3421%2C1003.2154%2C1003.1913%2C1003.2152%2C1003.4850%2C1003.0726%2C1003.3363%2C1003.1260%2C1003.0469%2C1003.4567%2C1003.0097%2C1003.5060%2C1003.1406%2C1003.0335%2C1003.2718%2C1003.5941%2C1003.1786%2C1003.4219%2C1003.5709%2C1003.2684%2C1003.2999%2C1003.1679%2C1003.1794%2C1003.0084%2C1003.4690%2C1003.4083%2C1003.5747%2C1003.5304%2C1003.0740%2C1003.0504%2C1003.0333%2C1003.1628%2C1003.0947%2C1003.3709%2C1003.1466%2C1003.6060%2C1003.1347%2C1003.6029%2C1003.4637%2C1003.4136%2C1003.5210%2C1003.5871%2C1003.3650%2C1003.3248%2C1003.1382%2C1003.2323%2C1003.5244%2C1003.3750%2C1003.5932%2C1003.3965%2C1003.5097%2C1003.0314%2C1003.5710%2C1003.3245%2C1003.0419%2C1003.0392%2C1003.4169%2C1003.1863%2C1003.1891%2C1003.5092%2C1003.6112%2C1003.3887%2C1003.2142%2C1003.1199%2C1003.0544%2C1003.2309%2C1003.1197%2C1003.4154%2C1003.3419%2C1003.1405%2C1003.5175%2C1003.4390%2C1003.5099%2C1003.4621%2C1003.4400%2C1003.2760%2C1003.0418%2C1003.0393%2C1003.0939%2C1003.2845%2C1003.1044%2C1003.1807%2C1003.3675%2C1003.4938%2C1003.3823%2C1003.1483%2C1003.5952%2C1003.3435%2C1003.4792%2C1003.2939%2C1003.4537%2C1003.1348%2C1003.0593%2C1003.4432%2C1003.1230%2C1003.0082&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A lower bound for distributed averaging algorithms"}, "summary": "We derive lower bounds on the convergence speed of a widely used class of\ndistributed averaging algorithms. In particular, we prove that any distributed\naveraging algorithm whose state consists of a single real number and whose\n(possibly nonlinear) update function satisfies a natural smoothness condition\nhas a worst case running time of at least on the order of $n^2$ on a network of\n$n$ nodes. Our results suggest that increased memory or expansion of the state\nspace is crucial for improving the running times of distributed averaging\nalgorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1003.1411%2C1003.5983%2C1003.2668%2C1003.6071%2C1003.5354%2C1003.3421%2C1003.2154%2C1003.1913%2C1003.2152%2C1003.4850%2C1003.0726%2C1003.3363%2C1003.1260%2C1003.0469%2C1003.4567%2C1003.0097%2C1003.5060%2C1003.1406%2C1003.0335%2C1003.2718%2C1003.5941%2C1003.1786%2C1003.4219%2C1003.5709%2C1003.2684%2C1003.2999%2C1003.1679%2C1003.1794%2C1003.0084%2C1003.4690%2C1003.4083%2C1003.5747%2C1003.5304%2C1003.0740%2C1003.0504%2C1003.0333%2C1003.1628%2C1003.0947%2C1003.3709%2C1003.1466%2C1003.6060%2C1003.1347%2C1003.6029%2C1003.4637%2C1003.4136%2C1003.5210%2C1003.5871%2C1003.3650%2C1003.3248%2C1003.1382%2C1003.2323%2C1003.5244%2C1003.3750%2C1003.5932%2C1003.3965%2C1003.5097%2C1003.0314%2C1003.5710%2C1003.3245%2C1003.0419%2C1003.0392%2C1003.4169%2C1003.1863%2C1003.1891%2C1003.5092%2C1003.6112%2C1003.3887%2C1003.2142%2C1003.1199%2C1003.0544%2C1003.2309%2C1003.1197%2C1003.4154%2C1003.3419%2C1003.1405%2C1003.5175%2C1003.4390%2C1003.5099%2C1003.4621%2C1003.4400%2C1003.2760%2C1003.0418%2C1003.0393%2C1003.0939%2C1003.2845%2C1003.1044%2C1003.1807%2C1003.3675%2C1003.4938%2C1003.3823%2C1003.1483%2C1003.5952%2C1003.3435%2C1003.4792%2C1003.2939%2C1003.4537%2C1003.1348%2C1003.0593%2C1003.4432%2C1003.1230%2C1003.0082&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We derive lower bounds on the convergence speed of a widely used class of\ndistributed averaging algorithms. In particular, we prove that any distributed\naveraging algorithm whose state consists of a single real number and whose\n(possibly nonlinear) update function satisfies a natural smoothness condition\nhas a worst case running time of at least on the order of $n^2$ on a network of\n$n$ nodes. Our results suggest that increased memory or expansion of the state\nspace is crucial for improving the running times of distributed averaging\nalgorithms."}, "authors": ["Alex Olshevsky", "John N. Tsitsiklis"], "author_detail": {"name": "John N. Tsitsiklis"}, "author": "John N. Tsitsiklis", "links": [{"href": "http://arxiv.org/abs/1003.5941v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1003.5941v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1003.5941v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1003.5941v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "A lower bound for distributed averaging algorithms\n\narXiv:1003.5941v1 [math.OC] 30 Mar 2010\n\nAlex Olshevsky, John N. Tsitsiklis\n\nAbstract- We derive lower bounds on the convergence speed\nof a widely used class of distributed averaging algorithms. In\nparticular, we prove that any distributed averaging algorithm\nwhose state consists of a single real number and whose (possibly\nnonlinear) update function satisfies a natural smoothness condition has a worst case running time of at least on the order of\nn2 on a network of n nodes. Our results suggest that increased\nmemory or expansion of the state space is crucial for improving\nthe running times of distributed averaging algorithms.\n\narguments of fi,G(t) in some arbitrary order; we assume\nthat this order does not change, i.e. if G(t1 ) = G(t2 ), then\nthe message coming from the same neighbor of agent i is\nmapped to the same argument of fi,G(t) for t = t1 and\nt = t2 . It is desired that\n\nI. I NTRODUCTION\n\nfor every i, for every sequence of graphs G(t) having the\nproperty that\n\nThe goal of this paper is to analyze the fundamental\nlimitations of a class of distributed averaging algorithms.\nThese algorithms are message-passing rules for a collection\nof agents (which may be sensors, nodes of a communication\nnetwork, or UAVs), each beginning with a real number, to\nestimate the average of these numbers using only nearest\nneighbor communications. Such algorithms are interesting\nbecause a number of sophisticated network coordination\ntasks can be reduced to averaging (see [13], [25], [1], [2], [6],\n[8], [9], [20], [23]), and also because they can be designed\nto be robust to frequent failures of communication links.\nA variety of such algorithms are available (see [22], [10],\n[18], [24], [15], [16], [17], [12], [26], [21]). However, many\nof these algorithms tend to suffer from a common disadvantage: even when no link failures occur, their convergence\ntimes do not scale well in the number of agents. Our aim\nin this paper is to show that this is, in fact, unavoidable\nfor a common class of such algorithms; namely, that any\ndistributed averaging algorithm that uses a single scalar state\nvariable at each agent and satisfies a natural \"smoothness\"\ncondition will have this property.\nWe next proceed to define distributed averaging algorithms\nand informally state our result.\nA. Background and basic definitions.\nDefinition of local averaging algorithms: Agents 1, . . . , n\nbegin with real numbers x1 (0), . . . , xn (0) stored in memory.\nAt each round t = 0, 1, 2, . . ., agent i broadcasts xi (t) to\neach of its neighbors in some undirected graph G(t) =\n({1, . . . , n}, E(t)), and then sets xi (t + 1) to be some\nfunction of xi (t) and of the values xi\u2032 (t), xi\u2032\u2032 (t), . . . it has\njust received from its own neighbors:\nxi (t + 1) = fi,G(t) (xi (t), xi\u2032 (t), xi\u2032\u2032 (t), . . .).\n\n(1)\n\nWe require each fi,G(t) to be a differentiable function. Each\nagent uses the incoming messages xi\u2032 (t), xi\u2032\u2032 (t), . . . as the\nThe authors are with the Laboratory for Information and Decision\nSystems, Massachusetts Institute of Technology, Cambridge, MA, USA.\nEmail: {alex o,jnt}@mit.edu. This research was supported by the\nNSF under grants ECCS-0701623 and CMMI-0856063.\n\nn\n\nlim xi (t) =\n\nt\u2192\u221e\n\n1X\nxi (0),\nn i=1\n\n(2)\n\nthe graph ({1, . . . , n}, \u222as\u2265t E(s)) is connected for every t,\n(3)\nand for every possible way for the agents to map incoming\nmessages to arguments of fi,G(t) .\nIn words, as the number of rounds t approaches infinity,\niteration (1) must converge to the average of the numbers\nx1 (0), . . . , xn (0). Note that the agents have no control\nover the communication graph sequence G(t), which is\nexogenously provided by \"nature.\" However, as we stated\npreviously, every element of the sequence G(t) must be\nundirected: this corresponds to bidirectional models of communication between agents. Moreover, the sequence G(t)\nmust satisfy the mild connectivity condition of Eq. (3), which\nsays that the network cannot become disconnected after a\nfinite period.\nLocal averaging algorithms are useful tools for information\nfusion due to their efficient utilization of resources (each\nagent stores only a single number in memory) as well as\ntheir robustness properties (the sequence of graphs G(t)\nis time-varying, and it only needs to satisfy the relatively\nweak connectivity condition in Eq. (3) for the convergence\nin Eq. (2) to hold). As far as the authors are aware, no other\nclass of schemes for averaging (e.g., flooding, fusion along\na spanning tree) is known to produce similar results under\nthe same assumptions.\nRemark: As can be seen from the subscripts, the update\nfunction fi,G(t) is allowed to depend on the agent and on the\ngraph. Some dependence on the graph is unavoidable since\nin different graphs an agent may have a different number\nof neighbors, in which case nodes will receive a different\nnumber of messages, so that even the number of arguments\nof fi,G(t) will depend on G(t). It is often practically desired\nthat fi,G(t) depend only weakly on the graph, as the entire\ngraph may be unknown to agent i. For example, we might\nrequire that fi,G(t) be completely determined by the degree\nof i in G(t). However, since our focus is on what distributed\nalgorithms cannot do, it does not hurt to assume the agents\n\n\fhave unrealistically rich information; thus we will not assume\nany restrictions on how fi,G(t) depends on G(t).\nRemark: We require the functions fi,G(t) to be smooth, for\nthe following reason. First, we need to exclude unnatural\nalgorithms that encode vector information in the infinitely\nmany bits of a single real number. Second, although we\nmake the convenient technical assumption that agents can\ntransmit and store real numbers, we must be aware that in\npractice agents will transmit and store a quantized version\nof xi (t). Thus, we are mostly interested in algorithms that\nare not disrupted much by quantization. For this reason, we\nmust prohibit the agents from using discontinuous update\nfunctions fi,G(t) . For technical reasons, we actually go a\nlittle further, and prohibit the agents from using non-smooth\nupdate functions fi,G(t) .\nB. Examples.\nIn order to provide some context, let us mention just a few\nof the distributed averaging schemes that have been proposed\nin the literature:\n1) The max-degree method [18] involves picking \u01eb(t)\nwith the property \u01eb(t) \u2264 1/(d(t) + 1), where d(t) is\nthe largest degree of any agent in G(t), and updating\nby\nX\n(xj (t) \u2212 xi (t)) .\nxi (t + 1) = xi (t) + \u01eb(t)\ni\u2208Ni (t)\n\nHere we use Ni (t) to denote the set of neighbors\nof agent i in G(t). In practice, a satisfactory \u01eb(t)\nmay not be known to all of the agents, because this\nrequires some global information. However, in some\ncases a satisfactory choice for \u01eb(t) may be available,\nfor example when an a priori upper bound on d(G(t))\nis known.\n2) The Metropolis method [24] involves setting \u01ebij (t) to\nsatisfy \u01ebij (t) \u2264 min(1/(di (t) + 1), 1/(dj (t) + 1)),\nwhere di (t), dj (t) are the degrees of agents i and j\nin G(t), and updating by\nX\n\u01ebij (t) (xj (t) \u2212 xi (t)) .\nxi (t + 1) = xi (t) +\nj\u2208Ni (t)\n\n3) The load-balancing algorithm of [17] involves updating\nby\nX\naij (t) (xj (t) \u2212 xi (t)) ,\nxi (t + 1) = xi (t) +\ni\u2208Ni (t)\n\nwhere aij (t) is determined by the following rule: each\nagent selects exactly two neighbors, the neighbor with\nthe largest value above its own and with the smallest\nvalue below its own. If i, j have both selected each\nother, then aij (t) = 1/3; else aij (t) = 0. The intuition\ncomes from load-balancing: agents think of xi (t) as\nload to be equalized among their neighbors; they try\nto offload on their lightest neighbor and take from their\nheaviest neighbor.\n\nWe remark that the above load-balancing algorithm is not\na \"local averaging algorithm\" according to our definition\nbecause xi (t + 1) does not depend only on xi (t) and its\nneighbors; for example, agents i and j may not match up\nbecause j has a neighbor k with xk (t) > xj (t). By contrast,\nthe max-degree and Metropolis algorithm are indeed \"local\naveraging algorithms.\"\nFor each of the above algorithms, it is known that Eq. (2)\nholds provided the connectivity condition in Eq. (3) holds. A\nproof of this fact for the load-balancing algorithm is implicit\nin [17], and for the others it follows from the results of [14],\n[3].\nC. Our contribution\nOur goal is to study the worst-case convergence time\nover all graph sequences. This convergence time may be\narbitrarily bad since one can insert arbitrarily many empty\ngraphs into the sequence G(t) without violating Eq. (3). To\navoid this trivial situation, we require that there exist some\ninteger B such that the graphs\n(k+1)B\n\n({1, . . . , n}, \u222ai=kB E(k))\n\n(4)\n\nare connected for every integer k.\nLet x(t) be the vector in Rn whose ith component is xi (t).\nWe define the convergence time T (n, \u01eb) of a local averaging\nalgorithm as the time until \"sample variance\"\n\uf8f62\n\uf8eb\nn\nn\nX\nX\n1\n\uf8edxi (t) \u2212\nxj (0)\uf8f8\nV (x(t)) =\nn\nj=1\ni=1\n\npermanently shrinks by a factor of \u01eb, i.e., V (x(t)) \u2264\n\u01ebV (x(0)) for all t \u2265 T (n, \u01eb), for all possible n-node graph\nsequences satisfying Eq. (4), and all initial vectors x(0) for\nwhich not all xi (0) are equal; T (n, \u01eb) is defined to be the\nsmallest number with this property. We are interested in how\nT (n, \u01eb) scales with n and \u01eb.\nCurrently, the best available upper bound for the convergence time is obtained with the load-balancing algorithm; in\n[17] it was proven that\n1\nT (n, \u01eb) \u2264 Cn2 B log ,\n\u01eb\nfor some absolute constant1 C.We are primarily interested in\nwhether its possible to improve the scaling with n to below\nn2 . Are there nonlinear update functions fi,G(t) which speed\nup the convergence time?\nOur main result is that the answer to this question is\n\"no\" within the class of local averaging algorithms. For such\nalgorithms we prove a general lower bound of the form\n1\nT (n, \u01eb) \u2265 cn2 B log ,\n\u01eb\nfor some absolute constant c. Moreover, this lower bound\nholds even if we assume that the graph sequence G(t) is the\nsame for all t; in fact, we prove it for the case where G(t)\nis a fixed \"line graph.\"\n1 By \"absolute constant\" we mean that C does not depend on the problem\nparameters n, B, \u01eb.\n\n\fII. F ORMAL\n\nSTATEMENT AND PROOF OF MAIN RESULT\n\nWe next state our main theorem. The theorem begins by\nspecializing our definition of local averaging algorithm to the\ncase of a fixed line graph, and states a lower bound on the\nconvergence time in this setting.\nWe will use the notation 1 to denote the vector in Rn\nwhose entries are all ones, and 0 to denote the vector\nwhose entries are all 0. The average of the initial values\nx1 (0), . . . , xn (0) will be denoted by x\u0304.\n\nLet f (without a subscript) be the mapping from Rn to\nitself that maps x(t) to x(t + 1) according to Eq. (5).\nLemma 1: f (a1) = a1, for any a \u2208 R.\nProof: Suppose that x(0) = a1. Then, the initial\naverage is a, so that\na1 = lim x(t) = lim x(t + 1) = lim f (x(t)).\nt\n\nt\n\nt\n\nWe use the continuity of f to get\na1 = f (lim x(t)) = f (a1).\nt\n\nTheorem 1: Let f1 , fn be two differentiable functions\nfrom R2 to R, and let f2 , f3 , . . . , fn\u22121 be differentiable\nfunctions from R3 to R. Consider the dynamical system\nx1 (t + 1) =\nxi (t + 1) =\nxn (t + 1) =\n\ni (0)\nFor i, j = 1, . . . , n, we define aij = \u2202f\u2202x\n, and the matrix\nj\n\uf8eb\n\uf8f6\na11 a12 0\n0\n***\n0\nf1 (x1 (t), x2 (t)),\n\uf8ec a21 a22 a23 0\n***\n0 \uf8f7\n\uf8ec\n\uf8f7\nfi (xi (t), xi\u22121 (t), xi+1 (t)), i = 2, . . . , n \u2212 1,\n\uf8ec 0 a32 a33 a34\n\u2032\n***\n0 \uf8f7\nA = f (0) = \uf8ec\n\uf8f7.\n\uf8ec ..\n..\n..\n..\n..\n.. \uf8f7\nfn (xn\u22121 (t), xn (t)).\n(5)\n\uf8ed .\n.\n.\n.\n.\n. \uf8f8\n\nSuppose that there exists a function \u03c4 (n, \u01eb) such that\n\n0\n\nkx(t) \u2212 x\u03041k2\n< \u01eb,\nkx(0) \u2212 x\u03041k2\n\n0\n\n0\n\nan,n\u22121\n\nann\n\nLemma 2: For any integer k \u2265 1,\n\nfor all n and \u01eb, all t \u2265 \u03c4 (n, \u01eb), and all initial conditions\nx1 (0), . . . , xn (0) for which not all xi (0) are equal. Then,\n1\nn2\nlog ,\n\u03c4 (n, \u01eb) \u2265\n30\n\u01eb\nfor all \u01eb > 0 and n \u2265 3.\n\n***\n\n(6)\n\nRemark: The dynamical system described in the theorem\nstatement is simply what a local averaging algorithm looks\nlike on a line graph. The functions f1 , fn are the update\nfunctions at the left and right endpoints of the line (which\nhave only a single neighbor), while the update functions\nf2 , f3 , . . . , fn\u22121 are the ones used by the middle agents\n(which have two neighbors). As a corollary, the convergence\ntime of any local averaging algorithm must satisfy the lower\nbound T (n, \u01eb) \u2265 (1/30)n2 log(1/\u01eb).\nRemark: Fix some n \u2265 3. A corollary of our theorem\nis that there are no \"local averaging algorithms\" which\ncompute the average in finite time. More precisely, there\nis no local averaging algorithm which, starting from initial\nconditions x(0) in some ball around the origin, always results\nin x(t) = x\u03041 for all times t larger than some T . We will\nsketch a proof of this after proving Theorem 1. By contrast,\nthe existence of such algorithms in slightly different models\nof agent interactions was demonstrated in [7] and [19].\nA. Proof of Theorem 1.\nWe first briefly sketch the proof strategy. We will begin by\npointing out that 0 must be an equilibrium of Eq. (5); then,\nwe will argue that an upper bound on the convergence time\nof Eq. (5) would imply a similar convergence time bound\non the linearization of Eq. (5) around the equilibrium of 0.\nThis will allow us to apply a previous \u03a9(n2 ) convergence\ntime lower bound for linear schemes, proved by the authors\nin [21].\n\nkf k (x) \u2212 Ak xk2\n= 0,\nx\u21920\nkxk2\nlim\n\nwhere f k refers to the k-fold composition of f with itself.\nProof: The fact that f (0) = 0 implies by the chain rule\nthat the derivative of f k at x = 0 is Ak . The above equation\nis a restatement of this fact.\nLemma 3: Suppose that xT 1 = 0. Then,\nlim Am x = 0.\n\nm\u2192\u221e\n\nProof: Let B be a ball around the origin such that for\nall x \u2208 B, with x 6= 0, we have\n1\nkf k (x) \u2212 Ak xk2\n\u2264 ,\nkxk2\n4\n\nfor k = \u03c4 (n, 1/2).\n\nSuch a ball can be found due to Lemma 2. Since we can\nscale x without affecting the assumptions or conclusions of\nthe lemma we are trying to prove, we can assume that x \u2208 B.\nIt follows that that for k = \u03c4 (n, 1/2), we have\nkAk xk2\nkxk2\n\n=\n\u2264\n\u2264\n\u2264\n\nkAk x \u2212 f k (x) + f k (x)k2\nkxk2\n1 kf k (x)k2\n+\n4\nkxk2\n1 1\n+\n4 2\n3\n.\n4\n\nSince this inequality implies that Ak x \u2208 B, we can apply\nthe same argumet argument recursively to get\nlim (Ak )m x = 0,\n\nm\u2192\u221e\n\nwhich implies the conclusion of the lemma.\nLemma 4: A1 = 1.\n\n\fProof: We have\nh1\nf (0 + h1) \u2212 f (0)\n= lim\n= 1,\nh\u21920 h\nh\nwhere we used Lemma 1.\nLemma 5: For every vector x \u2208 Rn ,\nA1 = lim\n\nh\u21920\n\nlim Ak x = x\u03041,\n\nk\u2192\u221e\n\nPn\nwhere x\u0304 = ( i=1 xi )/n.\nProof: Every vector x can be written as\nx = x\u03041 + y,\nwhere y T 1 = 0. Thus,\nlim Ak x = lim Ak (x\u03041 + y) = x\u03041 + lim Ak y = x\u03041,\n\nk\u2192\u221e\n\nk\u2192\u221e\n\nk\u2192\u221e\n\nwhere we used Lemmas 3 and 4.\nLemma 6: The matrix A has the following properties:\n1) aij = 0 whenever |i \u2212 j| > 1.\n2) The graph G = ({1, . . . , n}, E), with E =\n{(i, j) | aij 6= 0}, is strongly connected.\n3) A1 = 1 and 1T A = 1.\n4) An eigenvalue of A of largest modulus has modulus 1.\n5) A has an eigenvector v, with real eigenvalue \u03bb \u2208 (1 \u2212\n6\nT\nn2 , 1), such that v 1 = 0.\nProof:\n1) True because of the definitions of f and A.\n2) Suppose not. Then, there is a nonempty set S \u2282\n{1, . . . , n} with the property that aij = 0 whenever\ni \u2208 S and j \u2208 S c . Consider the vector x with\nxi = 0Pfor i \u2208 S, and xj = 1 for j \u2208 S c . Clearly,\n(1/n) i xi > 0, but (Ak x)i = 0 for i \u2208 S. This\ncontradicts Lemma 5.\n3) The first equality was already proven in Lemma 4. For\nthe second, let b = 1T A. Consider the vector\nz = lim Ak ei ,\nk\u2192\u221e\n\n(7)\n\nwhere ei is the ith unit vector. By Lemma 5,\nz=\n\n1\n1T e i\n1 = 1.\nn\nn\n\nOn the other hand,\nlim Ak ei = lim Ak+1 ei = lim Ak (Aei ).\n\nk\u2192\u221e\n\nk\u2192\u221e\n\nk\u2192\u221e\n\nApplying Lemma 5 again, we get\nT\n\n1 * (Aei )\nbi\n1 = 1,\nn\nn\nwhere bi is the ith component of b. We conclude that\nbi = 1; since no assumption was made on i, this\nimplies that b = 1, which is what we needed to show.\n4) We already know that A1 = 1, so that an eigenvalue with modulus 1 exists. Now suppose there is\nan eigenvalue with larger modulus, that is, there is\nsome vector x \u2208 Cn such that Ax = \u03bbx and\n|\u03bb| > 1. Then limk kAk xk2 = \u221e. By writing\nz=\n\nx = xreal + iximaginary , we immediately have that\nAk x = Ak xreal + iAk ximaginary . But by Lemma 5\nboth Ak xreal and Ak ximaginary approach some finite\nmultiple of 1 as k \u2192 \u221e, so kAk xk2 is bounded above.\nThis is a contradiction.\n5) The following fact is a combination of Theorems 4.1\nand 6.1 in [21]: Consider an n \u00d7 n matrix A such\nthat aij = 0 whenever |i \u2212 j| > 1, and such that the\ngraph with edge set {(i, j) | aij 6= 0} is connected.\nLet \u03bb1 , \u03bb2 , . . . be its eigenvalues in order of decreasing\nT\nmodulus. Suppose that \u03bb1 = 1, A1\nP = 1, and \u03c0 A =\nT\n\u03c0 , for some vector \u03c0 satisfying i \u03c0i = 1, and \u03c0i \u2265\n1/(Cn) for some positive C and for all i. Then, A\nhas a real eigenvalue in2 (1 \u2212 6C/n2, 1). Furthermore,\nthe corresponding eigenvector is orthogonal to 1, since\nright-eigenvectors of a matrix are orthogonal to lefteigenvectors with different eigenvalues.\nBy parts 1-4, all the assumptions of the result from [21]\nare satisfied with \u03c0 = 1/n and C = 1, thus completing\nthe proof of the lemma.\n\nRemark: An alternative proof of part 5 is possible. One can\nargue that parts 1 and 3 force A to be symmetric, and that\nLemma 5 implies that the elements aij must be nonnegative.\nOnce these two facts are established, the results of [4] will\nthen imply an eigenvalue has to lie in (1 \u2212 c/n2 , 1), for a\ncertain absolute constant c.\nProof of Theorem 1: Let v be an eigenvector of A with the\nproperties in part 5 of Lemma 6. Fix a positive integer k.\nLet \u01eb > 0 and pick x 6= 0 to be a small enough multiple of\nv so that\nkf k (x) \u2212 Ak (x)k2\n\u2264 \u01eb.\nkxk2\nThis is possible by Lemma 2. Then, we have\n\u0013k\n\u0012\nkf k (x)k2\nkAk xk2\n6\n\u2265\n\u2212\u01eb\u2265 1\u2212 2\n\u2212 \u01eb.\nkxk2\nkxk2\nn\nUsing the orthogonality property xT 1 = 0, we have x\u0304 = 0.\nSince we placed no restriction on \u01eb, this implies that\n\u0013k\n\u0012\nkf k (x) \u2212 x\u03041k2\nkf k (x)k2\n6\ninf\n= inf\n\u2265 1\u2212 2\nx6=0\nx6=0\nkx \u2212 x\u03041k2\nkxk2\nn\nPlugging k = \u03c4 (n, \u01eb) into this equation, we see that\n\u0013\u03c4 (n,\u01eb)\n\u0012\n6\n\u2264 \u01eb.\n1\u2212 2\nn\nSince n \u2265 3, we have 1 \u2212 6/n2 \u2208 (0, 1), and\n\u03c4 (n, \u01eb) \u2265\n\n1\nlog \u01eb.\nlog(1 \u2212 6/n2 )\n\n2 The reference [21] proves that an eigenvalue lies in (1 \u2212 c C/n2 , 1)\n1\nfor some absolute constant c1 . By tracing through the proof, we find that\nwe can take c1 = 6.\n\n\fNow using the bound log(1\u2212\u03b1) \u2265 5(\u03b1\u22121) for \u03b1 \u2208 [0, 2/3),\nwe get\nn2\n1\n\u03c4 (n, \u01eb) \u2265\nlog .\n30\n\u01eb\nq.e.d.\nRemark: We now sketch the proof of the claim we made\nearlier that a local averaging algorithm cannot average in\nfinite time. Fix n \u2265 3. Suppose that for any x(0) in some\nball B around the origin, a local averaging algorithm results\nin x(t) = x\u03041 for all t \u2265 T .\nThe proof of Theorem 1 shows that given any k, \u01eb > 0,\none can pick a vector v(\u01eb) so that if x(0) = v(\u01eb) then\nV (x(k))/V (x(0)) \u2265 (1 \u2212 6/n2)k \u2212 \u01eb. Moreover, the vectors\nv(\u01eb) can be chosen to be arbitrarily small. One simply picks\nk = T and \u01eb < (1\u22126/n2)k to get that x(T ) is not a multiple\nof 1; and furthermore, picking v(\u01eb) small enough in norm\nto be in B results in a contradiction.\nRemark: Theorem 1 gives a lower bound on how long we\nmust wait for the 2-norm kx(t) \u2212 x\u03041k2 to shrink by a factor\nof \u01eb. What if we replace the 2-norm with other\u221anorms,\nfor example with the \u221e-norm? Since B\u221e (0, r/ n) \u2282\nB2 (0, r) \u2282 B\u221e (0, r), it follows that if the \u221e-norm shrinks\nby a factor of \u01eb, then the 2-norm must shrink by at least\n\u221a\nn\u01eb. Since \u01eb only enters the lower bound of Theorem 1\nlogarithmically, the answer only changes by a factor of log n\nin passing to the \u221e-norm. A similar argument shows that,\nmodulo some logarithmic factors, it makes no difference\nwhich p-norm is used.\nIII. C ONCLUSIONS\nWe have proved a lower bound on the convergence time\nof local averaging algorithms which scales quadratically in\nthe number of agents. This lower bound holds even if all the\ncommunication graphs are equal to a fixed line graph. Our\nwork points to a number of open questions.\n1) Is it possible to loosen the definition of local averaging\nalgorithms to encompass a wider class of algorithms?\nIn particular, is it possible to weaken the requirement\nthat each fi,G(t) be smooth, perhaps only to the requirement that it be piecewise-smooth or continuous,\nand still obtain a \u03a9(n2 ) lower bound?\n2) Does the worst-case convergence time change if we\nintroduce some memory and allow xi (t + 1) to depend\non the last k sets of messages received by agent i?\nAlternatively, there is the broader question of how\nmuch is there to be gained if every agent is allowed to\nkeep track of extra variables. Some positive results in\nthis direction were obtained in [11].\n3) What if each node maintains a small number of update\nfunctions, and is allowed to choose which of them\nto apply? Our lower bound does not apply to such\nschemes, so it is an open question whether its possible\nto design practical algorithms along these lines with\nworst-case convergence time scaling better than n2 .\n\nR EFERENCES\n[1] M. Alighanbari, J.P. How, \"Unbiased Kalman Consensus Algorithm,\"\nProceedings of the 2006 American Control Conference, Minneapolis,\nMinnesota, USA, June 14-16, 2006\n[2] L. Brunet, H. L. Choi, J. P. How, \"Consensus-based auction approaches\nfor decentralized task assignment,\" AIAA Guidance, Navigation, and\nControl Conference, Honolulu, Hawaii, Aug. 2008.\n[3] V. D. Blondel, J. M. Hendrickx, A. Olshevsky, and J. N. Tsitsiklis,\n\"Convergence in multiagent coordination, consensus, and flocking,\"\nin Proceedings of the Joint 44th IEEE Conference on Decision and\nControl and European Control Conference (CDC-ECC'05), Seville,\nSpain, December 2005.\n[4] S. Boyd, P. Diaconis, J. Sun, and L. Xiao, \"Fastest mixing Markov\nchain on a path,\" The American Mathematical Monthly, 113(1):70-74,\nJanuary 2006.\n[5] M. Cao, A.S. Morse, B.D.O Anderson, \"Reaching a Consensus in a\nDynamically Changing Environment: Convergence Rates, Measurement\nDelays, and Asynchronous Events,\" SIAM Journal on Control and\nOptimization, 47(2):601-623, 2008.\n[6] R. Carli, A. Chiuso, L. Schenato, S. Zampieri, \"Distributed Kalman\nfiltering based on consensus strategies,\" IEEE Journal on Selected Areas\nin Communications, 26(4):622-633, 2008.\n[7] J. Cortes, \"Finite-time convergent gradient flows with applications to\nnetwork consensus, \" Automatica, Vol. 42(11):1993-2000, 2006.\n[8] C. Gao, J. Cortes, F. Bullo, \"Notes on averaging over acyclic digraphs\nand discrete coverage control,\" Automatica, 44(8):2120-2127, 2008.\n[9] N. Hayashi, T. Ushio, \"Application of a consensus problem to fair multiresource allocation in real-time systems, \" Proceedings of the 47th IEEE\nConference on Decision and Control, Cancun, Mexico, 2008.\n[10] A. Jadbabaie, J. Lin, A.S. Morse, \"Coordination of groups of mobile\nautonomous agents using nearest neighbor rules,\" IEEE Transactions\non Automatic Control, 48(6):988-1001, 2003.\n[11] K. Jung, D. Shah, J. Shin, \"Distributed averaging via lifted Markov\nchains, \" preprint, 2008.\n[12] A. Kashyap, T. Ba\u015far, R. Srikant, \"Quantized consensus,\" Automatica,\n43(7):1192-1203, 2007.\n[13] Q. Li, D. Rus, \"Global clock synchronization for sensor networks, \"\nProceedings of Infocom, Hong Kong, March 2004.\n[14] S. Li and H. Wang, \"Multi-agent coordination using\nnearest-neighbor rules: revisiting the Vicsek model,\" 2004;\nhttp://arxiv.org/abs/cs.MA/0407021.\n[15] L. Moreau, \"Stability of multiagent systems with time-dependent communication links,\" IEEE Transactions on Automatic Control, 50(2):169182, 2005.\n[16] C. C. Moallemi and B. Van Roy, \"Consensus propagation,\" IEEE\nTransactions on Information Theory, 52(11):4753-4766, 2006.\n[17] A. Nedic, A. Olshevsky, A. Ozdaglar, and J. N. Tsitsiklis. \"On\ndistributed averaging algorithms and quantization effects,\" IEEE Transactions on Automatic Control, 54(11):2506-2517, 2009.\n[18] R. Olfati-Saber and R. M. Murray.\"Consensus problems in networks\nof agents with switching topology and time-delays,\" IEEE Trans. on\nAutomatic Control, 49(9):1520-1533, Sep., 2004.\n[19] S. Sundaram, C.N. Hadjicostis, \"Finite-time distributed consensus in\ngraphs with time-invariant topologies,\" Proceedings of the American\nControl Conference, New York, NY, July 2007.\n[20] M. Schwager, J.-J. Slotine, D. Rus, \"Consensus learning for distributed coverage control,\" Proceedings of International Conference\non Robotics an Automation, Pasadena, CA, May 2008.\n[21] A. Olshevsky, J.N. Tsitsiklis, \"Convergence speed in distributed consensus and averaging,\" SIAM Journal on Control and Optimization,\nVolume 48(1):33-55, 2009.\n[22] J. N. Tsitsiklis, D. P. Bertsekas and M. Athans, \"Distributed asynchronous deterministic and stochastic gradient optimization algorithms,\"\nIEEE Transactions on Automatic Control, 31(9):803-812, 1986.\n[23] F. Wuhid, R. Stadler, M. Dam, \"Gossiping for threshold detection,\n\" Proceedings of the 11th IFIP/IEEE international conference on\nSymposium on Integrated Network Management, 2009.\n[24] L. Xiao and S. Boyd, \"Fast linear iterations for distributed averaging,\n\" Systems and Control Letters, 53:65-78, 2004.\n[25] L. Xiao, S. Boyd and S. Lall, \"A Scheme for robust distributed sensor\nfusion based on average consensus, \" Proceedings of International\nConference on Information Processing in Sensor Networks, April 2005,\np63-70, Los Angeles, 2005.\n[26] M. Zhu, S. Martinez, \"On the convergence time of asynchronous\ndistributed quantized averaging algorithms,\" preprint, 2008.\n\n\f"}