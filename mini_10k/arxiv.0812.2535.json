{"id": "http://arxiv.org/abs/0812.2535v1", "guidislink": true, "updated": "2008-12-13T09:21:31Z", "updated_parsed": [2008, 12, 13, 9, 21, 31, 5, 348, 0], "published": "2008-12-13T09:21:31Z", "published_parsed": [2008, 12, 13, 9, 21, 31, 5, 348, 0], "title": "Pattern Recognition and Memory Mapping using Mirroring Neural Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0812.1515%2C0812.2133%2C0812.2109%2C0812.4359%2C0812.3146%2C0812.4244%2C0812.3953%2C0812.0771%2C0812.3540%2C0812.0273%2C0812.3002%2C0812.1528%2C0812.4409%2C0812.3323%2C0812.2980%2C0812.1339%2C0812.4456%2C0812.3088%2C0812.3904%2C0812.2876%2C0812.4101%2C0812.1491%2C0812.3873%2C0812.0212%2C0812.4338%2C0812.3533%2C0812.3578%2C0812.2066%2C0812.0435%2C0812.1854%2C0812.3217%2C0812.1429%2C0812.3843%2C0812.1253%2C0812.2830%2C0812.1925%2C0812.2006%2C0812.3661%2C0812.4316%2C0812.1234%2C0812.4650%2C0812.2368%2C0812.1295%2C0812.0046%2C0812.4748%2C0812.0501%2C0812.1315%2C0812.3789%2C0812.2083%2C0812.1151%2C0812.1168%2C0812.4525%2C0812.2209%2C0812.4398%2C0812.1830%2C0812.1737%2C0812.2472%2C0812.0569%2C0812.1110%2C0812.4139%2C0812.0129%2C0812.1736%2C0812.0545%2C0812.2169%2C0812.0856%2C0812.2067%2C0812.1676%2C0812.4886%2C0812.4865%2C0812.2958%2C0812.2496%2C0812.0517%2C0812.4097%2C0812.1092%2C0812.2933%2C0812.2062%2C0812.1081%2C0812.3966%2C0812.2535%2C0812.0214%2C0812.0342%2C0812.4180%2C0812.0013%2C0812.1650%2C0812.3455%2C0812.1366%2C0812.2986%2C0812.0991%2C0812.0654%2C0812.3776%2C0812.4829%2C0812.0347%2C0812.1531%2C0812.4659%2C0812.2953%2C0812.0362%2C0812.4787%2C0812.3352%2C0812.1402%2C0812.4327%2C0812.2489&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Pattern Recognition and Memory Mapping using Mirroring Neural Networks"}, "summary": "In this paper, we present a new kind of learning implementation to recognize\nthe patterns using the concept of Mirroring Neural Network (MNN) which can\nextract information from distinct sensory input patterns and perform pattern\nrecognition tasks. It is also capable of being used as an advanced associative\nmemory wherein image data is associated with voice inputs in an unsupervised\nmanner. Since the architecture is hierarchical and modular it has the potential\nof being used to devise learning engines of ever increasing complexity.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0812.1515%2C0812.2133%2C0812.2109%2C0812.4359%2C0812.3146%2C0812.4244%2C0812.3953%2C0812.0771%2C0812.3540%2C0812.0273%2C0812.3002%2C0812.1528%2C0812.4409%2C0812.3323%2C0812.2980%2C0812.1339%2C0812.4456%2C0812.3088%2C0812.3904%2C0812.2876%2C0812.4101%2C0812.1491%2C0812.3873%2C0812.0212%2C0812.4338%2C0812.3533%2C0812.3578%2C0812.2066%2C0812.0435%2C0812.1854%2C0812.3217%2C0812.1429%2C0812.3843%2C0812.1253%2C0812.2830%2C0812.1925%2C0812.2006%2C0812.3661%2C0812.4316%2C0812.1234%2C0812.4650%2C0812.2368%2C0812.1295%2C0812.0046%2C0812.4748%2C0812.0501%2C0812.1315%2C0812.3789%2C0812.2083%2C0812.1151%2C0812.1168%2C0812.4525%2C0812.2209%2C0812.4398%2C0812.1830%2C0812.1737%2C0812.2472%2C0812.0569%2C0812.1110%2C0812.4139%2C0812.0129%2C0812.1736%2C0812.0545%2C0812.2169%2C0812.0856%2C0812.2067%2C0812.1676%2C0812.4886%2C0812.4865%2C0812.2958%2C0812.2496%2C0812.0517%2C0812.4097%2C0812.1092%2C0812.2933%2C0812.2062%2C0812.1081%2C0812.3966%2C0812.2535%2C0812.0214%2C0812.0342%2C0812.4180%2C0812.0013%2C0812.1650%2C0812.3455%2C0812.1366%2C0812.2986%2C0812.0991%2C0812.0654%2C0812.3776%2C0812.4829%2C0812.0347%2C0812.1531%2C0812.4659%2C0812.2953%2C0812.0362%2C0812.4787%2C0812.3352%2C0812.1402%2C0812.4327%2C0812.2489&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper, we present a new kind of learning implementation to recognize\nthe patterns using the concept of Mirroring Neural Network (MNN) which can\nextract information from distinct sensory input patterns and perform pattern\nrecognition tasks. It is also capable of being used as an advanced associative\nmemory wherein image data is associated with voice inputs in an unsupervised\nmanner. Since the architecture is hierarchical and modular it has the potential\nof being used to devise learning engines of ever increasing complexity."}, "authors": ["Dasika Ratna Deepthi", "K. Eswaran"], "author_detail": {"name": "K. Eswaran"}, "author": "K. Eswaran", "links": [{"href": "http://arxiv.org/abs/0812.2535v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0812.2535v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0812.2535v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0812.2535v1", "arxiv_comment": null, "journal_reference": "Paper No 336, IEEE, ICETiC 2009, International Conference on\n  Emerging Trends in Computing", "doi": null, "fulltext": "Pattern Recognition and Memory Mapping using Mirroring Neural Networks\nName(s) Dasika Ratna Deepthi (1) and K. Eswaran (2)\nAddress: (1) Osmania University, Hyderabad - 500 007\n(2) Sreenidhi Institute of Science and Technology, Yamnampet, Ghatkesar, Hyderabad - 501 30, India\nEmail addresses radeep07@gmail.com, kumar.e@gmail.com\n(Paper #336, accepted in IEEE International Conf. on Emerging Trends in Computing, ICETiC Jan. 2009)\n\nAbstract\nIn this paper, we present a new kind of learning\nimplementation to recognize the patterns using the\nconcept of Mirroring Neural Network (MNN) which\ncan extract information from distinct sensory input\npatterns and perform pattern recognition tasks. It is\nalso capable of being used as an advanced associative\nmemory wherein image data is associated with voice\ninputs in an unsupervised manner. Since the\narchitecture is hierarchical and modular it has the\npotential of being used to devise learning engines of\never increasing complexity.\nKEY WORDS\nMirroring Neural Network, sensory input patterns,\npattern recognition, associative memory, learning\nengines.\n\nlearning and in its form is an imitation of the basic\nnatural neural system [9]. We follow the bottom-up\napproach that resembles the architecture used by\nDileep George and Jeff Hawkins [10] and [11] to\nrecognize the patterns by organizing the MNN\nmodules (approximating cortical regions of the\ncortical system) of different levels using connections\n[12]. Our conviction is that the proposed architecture\ncan be used as a part of an unsupervised hierarchical\nlearning machine to be used for complex real-world\npattern recognition problem, an aspect discussed in\nsection 3.\nIn this paper, we deal with usage of MNN concept for:\n\u2022\n\u2022\n\u2022\n\nFeature extraction of patterns\nMapping the extracted features of the patterns\nConstruction of the pattern recognition\narchitecture\n\n1: INTRODUCTION\n\n2: PATTERN RECOGNITION AND MEMORY\nMAPPING\n\nIn this paper, we introduce an algorithm using\nMirroring Neural Networks (MNN) which performs a\ndimension reduction of input data followed by\nmapping, to recognize patterns. There have been many\ninvestigations done on pattern recognition, a few of\nwhich deal with geometric feature extraction [1],\nmanifold learning [2] and non-linear dimensional\nreduction [3] and [4] etc.,. In addition to pattern\nrecognition through data reduction, the neural network\napproach can also be used to resolve high dimensional\nproblems in clustering [5] and to study complex\nneuronal properties [6].\nWe in our approach, develop an architecture\nwhich does non-linear data reduction associated with\nmapping using a special type of neural network called\nMNN (refer [7] and [8] for details). The architecture is\nhierarchical; possesses the ability of unsupervised\n\nWe construct a software architecture which does\nfeature extraction coupled with memory mapping for a\n\"pattern recognizer\". This architecture is hierarchical\nand is constructed out of several modules each of these\nmodules is a \"Mirroring Neural Network\" [7]. The\ntask of the latter is to take a set of data and then reduce\n(compress) the data in such a manner that information\nis not lost, the compressed data is then passed on to\nthe next module. The additional purpose of the pattern\nrecognizer is to perform automatic memory maps, by\nthis we mean: given a set of data which is connected to\nanother set of data, the pattern recognizer associates\nthe former set with the latter. For example, suppose we\nare given a set of data which represents the digitized\ndata (say a .wav file) of the spoken word \"Alice\nSimmons\" and another set of data which is a digitized\n\n\fimage of the person, \"Alice Simmons\", then the\npattern recognizer can be taught to associate the\nvoiced data to her image. The pattern recognizer can\nthen learn to associate any other speech signal\ncontaining the spoken word \"Alice Simmons\" to any\nother image of \"Alice Simmons\". The type of pattern\nrecognizer described in this example can be utilized to\nconstruct an intelligent device which associates a\nspoken word to an image.\nThe main characteristics of the architecture are:\n(i) its input independence (ii) its hierarchical structure\n(iii) its modularity, (being built up of individual\nmodules or \"cells\") and (iv) the attribute that each of\nthese modules actually implements a single common\nalgorithm (the MNN algorithm). Since the architecture\nhas these 4 characteristics, it can be easily generalized\nand enlarged to solve a large variety of problems of\nincreasing complexity and size. The single common\nalgorithm that is used by the MNN which perform a\ncompression (feature extraction) and a memory\nassociation of the data, details are below.\n2.1. Overview of the proposed architecture\nIn order to design our architecture which has the\nabove mentioned 4 characteristics, we have imitated\nsome features of the neo-cortex [11] in the human\nbrain, which seem to possess these characteristics. The\nhuman brain can encode the data of different sensory\ninput patterns and not only recognize the patterns but\nis also able to effectively store these in very efficient\nmemory maps and is capable of self-learning and\ndecision making. Of course, our architecture can at\nbest be said to be a crude copy of the complicated\narchitecture of the neo-cortex whose details are yet\nunknown and are a subject of intense research among\nneuroscientists.\nIn our approach the hierarchical architecture is\nbuilt up of modules connected in a tree-like structure\neach module is a MNN, the bottommost level is used\nto accept the sensory input data and extract those\nfeatures that best capture the patterns which are to be\nrecognized. At this level, each of the MNNs processes\na category of sensory patterns. The extracted features\nof dissimilar sensory patterns from the lower level\nMNNs are used by the upper level MNNs so that the\nlatter can function as associative memory maps. That\nis, in the upper level each MNN maps the extracted\nfeatures of patterns belong to a group of sensory input\nto the features of the corresponding pattern group of a\ndifferent sensory input. (For example, a sensory input\nrepresenting the spoken word 'face\" is associated with\nits corresponding visual sensory input i.e., \"face\nimage\"). This process of feature extraction and the\nassociated mapping can be repeated for each of the\n\ndistinguishable sensory patterns. And we can say that\nour architecture performs a combination of learning\nand mapping thus simulating the processes of\nautomatic learning, memorization and recognition as is\ndone by the cortical system. An illustrated pictorial\ndiagram (Figure 1) of the proposed architecture with\nexplanation on sample implementation is given below.\nLet us, for the sake of simplicity, consider only\nthe two categories of sensory input patterns out of N\ncategories (an example case, for the human brain the\ndistinct categories of sensory patterns belong to\nSound, Image, Odor, Taste, Sense/Touch) in our\narchitecture, it will become apparent that there is no\nloss of generality in choosing only two categories.\nOne of the two input patterns is a voice (sound)\npattern and the other is an image pattern and they may\nbe termed Category I and Category II respectively\n(this simplified situation corresponds to N=2, in Fig\n1). Further, we assume that each of these categories\ncontain 3 distinguishable pattern groups in it, say\n\"Face\", \"Window\" and \"Garden\" (this corresponds to\nk=3, in Fig. 1). Then the bottommost level MNNs i.e.,\nMNN I and MNN II are trained to correspondingly\nextract the best low-dimensional features of voice\npatterns ('Face', 'Window' and 'Garden') and image\npatterns ('Face', 'Window' and 'Garden'). These\nextracted features are classified into the 3\ndistinguishable groups by Forgy's clustering algorithm\n[13] and resulting groups are given to the MNNs at the\nupper level. These upper level MNNs are then trained\nto map the input sound to the corresponding image\nfrom left to right at Level II. For example if the input\ndata (in a .wav file) is the sound \"window\" it is\nautomatically associated with the image of a\n\"window\". In this paper the first level of training is\nsupervised in the sense that there is a training session\nwherein data is fed in pairs, one containing a spoken\nword and the other an image. For example, a spoken\nword \"garden\" (from a .wav file) is presented to the\nMNN I along with the image of a \"garden\" (from a\n.bmp file) to MNN II at level I. During the training\nmany such pairs of data (speech-cum-image) belong to\ndifferent groups (in our example, there are 3 groups)\nare presented, after that both the MNNs, MNN I and\nMNN II learn to classify the speech and images\nrespectively. At level II these speech data and image\ndata are associated by another set of MNNs - one\nMNN for each group.\nAfter successful training, the pattern recognizing\narchitecture will perform as follows: If a word (say a\n\"window\") is sent as input (to MNN I), then the\nnetwork architecture will automatically associate this\nword with the image \"window\" and the .bmp file\ncontaining that image is displayed. Thus the\narchitecture maps speech data to visual data.\n\n\fControl flow\nData flow\nControl flow\nalong with data\n\n-N1\n\nMNN I1-II1\n\n-N2\nMNN I2-II2\n-N3\n\nMNN I3-II3\n\nLevel II\n\n.\n.\n.\n\n.\n\n.\n.\n.\n1\n\n2\n\n3\n\n.\n.\n...\n\nk\n\nForgy's algorithm to\ncluster feature set I\n(among 'k' groups)\n\nLevel I\n\nMNN I\n\n-Nk\nMNN Ik-IIk\n\nk\n\n...\n\n3\n\n2\n\n1\n\nForgy's algorithm to\ncluster feature set II\n(among 'k' groups)\n\nk\n\n.........\n\n...\n\n3\n\n2\n\n1\n\nForgy's algorithm to\ncluster feature set N\n(among 'k' groups)\n\nMNN N\n\nMNN II\n\n.........\n\nCategory I Patterns belong\nto 'K' different groups\n\nCategory II Patterns belong to\n'K' different groups\n\nSensory Input category I\n\nSensory Input category II\n\n.......\n\nCategory N Patterns belong\nto 'K' different groups\n\nSensory Input category N\n\nFigure 1. Illustrated pictorial diagram of the proposed architecture\n\n2.2. The MNN concept\nIn this paragraph we briefly describe the functions\nof the Mirroring Neural Network (MNN), details are\ngiven in [7] and [8]. The MNN module is essentially a\nconverging-diverging type of artificial neural network\narchitecture. The MNN compresses the received input\nand gives an output. The idea is to train the MNN so\nthat the output is as close as possible to the input. If\nthis is successful for a particular class of patterns then\nthe particular MNN has \"learnt\" the pattern, the output\nof the lowest dimensional hidden layer becomes the\nfeature set characterizing the pattern. This feature set\ncan then be used to distinguish it from other patterns\n\nand classify it. So we see an MNN does the following\ntasks: (i) compresses the input data, (ii) extracts a\nsuitable feature set characterizing the input pattern and\n(iii) has the property to reconstruct the original data\ngiven the compressed data. It may be noted, one MNN\ncan be used to recognize either one pattern or a\nparticular pattern from a set of patterns. We use a\nMNN as a module of our pattern recognition\narchitecture.\nTo summarize this section we can say that we\nimplement the MNN's feature extraction (at level I) on\ndifferent kinds of patterns i.e., voice samples besides\nimage patterns. In addition to usual feature extraction,\nat the upper level, the MNN concept is to carry out a\n\n\fmemory map by connecting two categories of sensory\npatterns which are associated with each other. And we\nconstruct the architecture for pattern recognition with\nthe combination of lower and upper level MNNs\nwhich perform dimensional reduction and memory\nmapping respectively.\n2.3. Demonstration of the proposed architecture\nIn this section, we describe the procedure\nfollowed for training the pattern recognition network\nand present the results on an actual example.\n2.3.1. Inputs to Level I. The inputs to the architecture\nare pairs of voice samples and their corresponding\nimages. The voice samples are the actual spoken\nwords repeatedly uttered by a particular speaker.\nActually we took 150 samples of the word \"face\"( i.e.,\nthe word \"face\" repeatedly uttered 150 times)\nsimilarly 150 samples of the word \"window\" and the\n150 samples of the word \"garden\" are taken. Each of\nthese words is paired with a corresponding image data\nand this data pair is fed to the architecture as Sensory\ninput I (voice) and sensory input II (image).\nThe procedure to obtain the digitized voice\nsamples is as follows: each voice sample is digitized at\n2000 samples per second and re-sampled to 510\nequally spaced points using Matlab [14]. These 510\nvalues of the word samples are considered as\nrepresenting the sensory input I (input vector of 510\ndimensions to MNN I).\nFor inputting the data for the images the\nprocedure is as follows: A total of 450 different\nimages were considered in this study, all the images\nthough different, fall into one of three classes viz., a\nface (faces are from the Yale face database [15]), a\nwindow or a garden. Each of these images is resized to\na fixed size 17 X 30, containing 510 pixels. The\ngrayscales (intensity levels of each pixel) whose range\nis from 0 to 255 are rescaled [16] so that they all lie\nbetween -1 to +1, these 510 intensity values constitute\nthe input vectors for each sample image and are given\nas input vectors to MNN II.\n2.3.2. Classification at Level I. During the training\nperiod when pairs of (voice, image) data are fed to\nlevel I, both MNN I and MNN II learn (i) to reduce the\ninput data and then (ii) to independently classify their\nrespective inputs.\nThe data reduction is done from 510 to 20 units\nby using a 3 layer MNN architecture, 510-20-510\nprocessing elements. The reduced inputs are then used\nto classify the inputs. For example the 20 features\nextracted by the MNN I classifies its input words,\nusing Forgy's algorithm into 3 groups (k=3 in figure)\n\nfor the 3 types of words \"face\", \"window\" and\n\"garden\". Similarly, and MNN II reduces its image\ndata to 20 features using again a 510-20-510\narchitecture and classifies the inputs into 3 groups one\nfor a \"face image\", another for \"window image\" and\nthe third for the \"garden image\". See[7] and [8] for\nmore details on data reduction by MNN and its use of\nForgy's algorithm.\n2.3.3. Training of MNN's in Level II. In the\nfollowing example the Level I MNNs are first trained\nand their inputs classified and then the level II MNN's\nare trained for associating words with the\ncorresponding images.\nBut, it is to be mentioned here that it is quite\npossible to envisage both the levels to be trained\nsimultaneously, under the assumption of simultaneity\nby using temporal information [10] and [11]. If we\nassume that the sensory input I is related to Sensory\ninput II, this will happen if the word face is presented\nsimultaneously with the image of a face, then MNN I\nand MNN II can then classify their inputs and put\nthem in the same group (say group 1 for face),\nsimultaneously MNN I1-II1 in Level II will be trained\nsuch that the reduced input given to MNN I1-II1\n(from the MNN I in Level I) is mapped (matched) to\nthe reduced input of MNN II at level I. If this is done\nproperly, then the architecture will train itself to\nassociate the word face with the image of a face and\nthe word window with the image of a window etc.,.\nThe purpose of Level II is to associate each group\nof input from Sensory I to its appropriate group from\nSensory II. That is, if the word \"face\" is invoked as\ninput, then the architecture should associate a \"face\nimage\" to this input. Since there are 3 groups (k=3) in\nour data, there will be 3 MNNs in Level II each of\nthese have to be trained with the appropriate inputs.\nWe have chosen a 20-20-20 architecture for the 3\nMNNs in Level II.\nThe procedure to train these level II MNNs is as\nfollows: the reduced input from MNN I (in this case it\nis a 20 dimensional vector), becomes the input to the\nappropriate MNN in Level II. Example, if the input\n(garden word, garden image) is fed as data to level I\nMNNs then the 20 feature vector (of garden word)\nfrom MNN I is given as input to MNN I3-II3 in Level\nII so that its output is equal to the 20 dimensional\nfeature vector of the garden-image, obtained by data\nreduction using MNN II in Level I.\nAfter the training of the Level II MNNs, the\nwhole architecture can be considered as trained. The\nfunctioning of this trained architecture can be now\ntested. Simply by inputting a voice sample and finding\nout if the architecture maps this voice input to its\n\n\fcorresponding image and give it as an output. The\nflow chart of the architecture is shown in Figure 2.\n(i)\n\n(ii)\n\n(iii)\n\nTable 1: Pattern recognition and memory mapping\nusing MNN\n\nMNN I for\nfeature extraction\n\nInput\nto the\nsystem\n\nForgy's clustering\nalgorithm\n\nFeature\nset I1\n\nMNN\nI1-II1\n\nFeature\nset I2\n\nMNN\nI2-II2\n\nFeature\nset II1\n\nFeature\nset II2\n\nFeature\nset I3\n\nMNN\nI3-II3\n\nFeature\nset II3\n\nMNN II for\nreconstruction\n\n(a)\n\n(b)\n\ninto their appropriate group is found to be 91.6% and\n95.3% respectively (using only the reduced input\nvector of 20 dimensions).\nThe overall efficiency of recognition, that is, the\nrate of correct prediction of a voice input to its\nappropriate image output is found to be 91.6%.\n\n(c)\n\nFigure 2. Pictorial representation of pattern\nrecognition and memory mapping exemplified on\ntypical samples from each group: (i) word 'face'\nmapped to image 'face' (a); (ii) word 'window'\nmapped to image 'widow' (b); (iii) word 'garden'\nmapped to image 'garden' (c).\n\n2.3.4 Results. The experiment is conducted by using a\ndata set of 450 (word, image) pairs, for training\npurposes 300 pairs are used and then the remaining\n150 pairs are used for testing (evaluation). It is\nemphasized here that the pairs that were used for\ntesting the architecture were new pairs of data and\nwere NOT used while training the architecture. Table\n1 summarizes our results.\nAt Level I, the efficiencies of the MNN I and\nMNN II in classifying the voice and image patterns\n\nVoice\nsamples\n\nOutput\nfrom\nthe\nsystem\nImage\nsamples\n\nSuccess\nrate of\nMNN I\n\nSuccess\nrate of\nMNN II\n\nOverall\nefficiency\n\n91.6%\n\n95.3%\n\n91.6%\n\n3: CONCLUSIONS AND FUTURE WORK\nWe have demonstrated the successful functioning\nof an unsupervised learning algorithm which has the\nfollowing features: (i) It is hierarchical and modular\n(ii) each module runs on a common algorithm, (iii)\ncapable of automatic data reduction and feature\nextraction and (iv) provides an efficient associative\nmemory map. Because of these features it is capable\nof being enlarged and used for more complex learning\ntasks. For example, its ability to associate a voice\npattern with an image pattern makes it a good\ncandidate for devising learning machines which can\nassociate memories of two simultaneous events e.g.,\nthe image of a train moving with the whistle blowing,\nthe sight of fire with its heat. This kind of learning\nmachine could provide a method of associating\nmemories of two different events separated temporally\nand thus learn to recognize cause and effect. It is\nhoped that architecture is flexible enough to be\ndeployed, in the future, for even more complex pattern\nrecognition tasks as those performed by neo-cortex in\nthe human brain.\nREFERENCES\n[1] C. J. C. Burges, Geometric methods for feature\nextraction and dimensional reduction, In L. Rokach\nand O. Maimon, editors, Data Mining and Knowledge\nDiscovery Handbook: A Complete Guide for\nPractitioners and Researchers (Kluwer Academic\nPublishers, 2005).\n\n\f[2] J. Zhang, S. Z. Li and J. Wang, Manifold learning\nand applications in recognition, In Intelligent\nMultimedia Processing with Soft Computing,\nSpringer-Verlag, Heidelberg, 2004.\n[3] Hiu Chung Law, Clustering, Dimensionality\nReduction and Side Information (Michigan State\nUniversity, 2006).\n[4] G. E. Hinton & R. R. Salakhutdinov, Reducing the\nDimensionality of Data with Neural Networks,\nScience, 313, July 2006, 504-507.\n[5] Alex L. P. Tay, Jacek M. Zurada, Lai-Ping Wong,\nand Jian Xu, The Hierarchical Fast Learning Artificial\nNeural Network (HieFLANN)-An Autonomous\nPlatform\nfor\nHierarchical\nNeural\nNetwork\nConstruction, IEEE Transactions on Neural Networks,\nThis article has been accepted for inclusion in a future\nissue of this journal, (2007).\n[6] Brian Lau, Garrett B. Stanley, Yang Dan,\nComputational subunits of visual cortical neurons\nrevealed by artificial neural networks. Proc Natl Acad\nSci USA 99(13), 2002, 8974\u20138979.\n[7] Dasika Ratna Deepthi, Sujeet Kuchibholta, K.\nEswaran, Dimentionality reduction and reconstruction\nusing mirroring neural networks and object\nrecognition based on reduced dimension characteristic\nvector, Advances in Computer Vision and Information\nTechnology (ACVIT-07) on Artificial Intelligence,\nNovember 2007.\n[8] Dasika Ratna Deepthi, G.R. Aditya Krishna, K.\nEswaran, automatic pattern classification by\nunsupervised learning using dimensionality reduction\nof data with mirroring neural networks, Advances in\nComputer Vision and Information Technology\n(ACVIT-07) on Artificial Intelligence, November\n2007.\n[9] Marcus Kaiser, Brain architecture: A design for\nnatural computation, Royal Society, 2008, pp 1-13.\n[10] Dileep George and Jeff Hawkins, A Hierarchical\nBayesian Model of Invariant Pattern Recognition in\nthe Visual Cortex.\n[11] Jeff Hawkins and Dileep George, Numenta\n(2007) in Hierarchical Temporal Memory, Concepts,\nTheory, and Terminology, (Numenta Inc), 2007, pp 120\n\n[12] D. C. Van Essen, C. H. Anderson, and D. J.\nFelleman. Information processing in the primate visual\nsystem: an integrated systems perspective, Science,\n255(5043) :419\u2013423, Jan 24 1992. LR: 20041117;\nJID:0404511; RF: 38\n[13] Earl Gose, Richard Johnsonbaugh & Steve Jost,\nPattern recognition and image analysis (New Delhi:\nPrentice Hall of India, 2000), pp 211-213\n[14] http://en.wikipedia.org/wiki/MATLAB\n[15]\nhttp://cvc.yale.edu/projects/yalefaces/yalefaces.html\n[16] R. C Gonzalez and R. E. Woods, Digital image\nprocessing (Englewood Cliffs, NJ: Prentice-Hall,\n2002).\n\n\f"}