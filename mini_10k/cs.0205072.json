{"id": "http://arxiv.org/abs/cs/0205072v1", "guidislink": true, "updated": "2002-05-29T17:48:48Z", "updated_parsed": [2002, 5, 29, 17, 48, 48, 2, 149, 0], "published": "2002-05-29T17:48:48Z", "published_parsed": [2002, 5, 29, 17, 48, 48, 2, 149, 0], "title": "Unsupervised Learning of Morphology without Morphemes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0205011%2Ccs%2F0205006%2Ccs%2F0205050%2Ccs%2F0205003%2Ccs%2F0205061%2Ccs%2F0205010%2Ccs%2F0205018%2Ccs%2F0205023%2Ccs%2F0205046%2Ccs%2F0205059%2Ccs%2F0205043%2Ccs%2F0205067%2Ccs%2F0205022%2Ccs%2F0205066%2Ccs%2F0205020%2Ccs%2F0205038%2Ccs%2F0205049%2Ccs%2F0205079%2Ccs%2F0205017%2Ccs%2F0205036%2Ccs%2F0205007%2Ccs%2F0205025%2Ccs%2F0205027%2Ccs%2F0205060%2Ccs%2F0205016%2Ccs%2F0205008%2Ccs%2F0205065%2Ccs%2F0205013%2Ccs%2F0205012%2Ccs%2F0205034%2Ccs%2F0205041%2Ccs%2F0205039%2Ccs%2F0205072%2Ccs%2F0205021%2Ccs%2F0205056%2Ccs%2F0205062%2Ccs%2F0205053%2Ccs%2F0205040%2Ccs%2F0205024%2Ccs%2F0205075%2Ccs%2F0205029%2Ccs%2F0205002%2Ccs%2F0205042%2Ccs%2F0205026%2Ccs%2F0205044%2Ccs%2F0205070%2Ccs%2F0205076%2Ccs%2F0205037%2Ccs%2F0205058%2Ccs%2F0205035%2Ccs%2F0205005%2Ccs%2F0205057%2Ccs%2F0205045%2Ccs%2F0205004%2Ccs%2F0205074%2Ccs%2F0205077%2Ccs%2F0205073%2Ccs%2F0205063%2Ccs%2F0205030%2Ccs%2F0205055%2Ccs%2F0205019%2Ccs%2F0205001%2Ccs%2F0205015%2Ccs%2F0205071%2Ccs%2F0205064%2Ccs%2F0205051%2Ccs%2F0205033%2Ccs%2F0205028%2Ccs%2F0205009%2Ccs%2F0205069%2Ccs%2F0205054%2Ccs%2F0205032%2Ccs%2F0205048%2Ccs%2F0205031%2Ccs%2F0205052%2Ccs%2F0404011%2Ccs%2F0404029%2Ccs%2F0404037%2Ccs%2F0404006%2Ccs%2F0404043%2Ccs%2F0404047%2Ccs%2F0404058%2Ccs%2F0404005%2Ccs%2F0404012%2Ccs%2F0404042%2Ccs%2F0404032%2Ccs%2F0404045%2Ccs%2F0404010%2Ccs%2F0404053%2Ccs%2F0404041%2Ccs%2F0404050%2Ccs%2F0404035%2Ccs%2F0404031%2Ccs%2F0404054%2Ccs%2F0404048%2Ccs%2F0404001%2Ccs%2F0404040%2Ccs%2F0404025%2Ccs%2F0404024%2Ccs%2F0404030%2Ccs%2F0404004&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Unsupervised Learning of Morphology without Morphemes"}, "summary": "The first morphological learner based upon the theory of Whole Word\nMorphology Ford et al. (1997) is outlined, and preliminary evaluation results\nare presented. The program, Whole Word Morphologizer, takes a POS-tagged\nlexicon as input, induces morphological relationships without attempting to\ndiscover or identify morphemes, and is then able to generate new words beyond\nthe learning sample. The accuracy (precision) of the generated new words is as\nhigh as 80% using the pure Whole Word theory, and 92% after a post-hoc\nadjustment is added to the routine.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0205011%2Ccs%2F0205006%2Ccs%2F0205050%2Ccs%2F0205003%2Ccs%2F0205061%2Ccs%2F0205010%2Ccs%2F0205018%2Ccs%2F0205023%2Ccs%2F0205046%2Ccs%2F0205059%2Ccs%2F0205043%2Ccs%2F0205067%2Ccs%2F0205022%2Ccs%2F0205066%2Ccs%2F0205020%2Ccs%2F0205038%2Ccs%2F0205049%2Ccs%2F0205079%2Ccs%2F0205017%2Ccs%2F0205036%2Ccs%2F0205007%2Ccs%2F0205025%2Ccs%2F0205027%2Ccs%2F0205060%2Ccs%2F0205016%2Ccs%2F0205008%2Ccs%2F0205065%2Ccs%2F0205013%2Ccs%2F0205012%2Ccs%2F0205034%2Ccs%2F0205041%2Ccs%2F0205039%2Ccs%2F0205072%2Ccs%2F0205021%2Ccs%2F0205056%2Ccs%2F0205062%2Ccs%2F0205053%2Ccs%2F0205040%2Ccs%2F0205024%2Ccs%2F0205075%2Ccs%2F0205029%2Ccs%2F0205002%2Ccs%2F0205042%2Ccs%2F0205026%2Ccs%2F0205044%2Ccs%2F0205070%2Ccs%2F0205076%2Ccs%2F0205037%2Ccs%2F0205058%2Ccs%2F0205035%2Ccs%2F0205005%2Ccs%2F0205057%2Ccs%2F0205045%2Ccs%2F0205004%2Ccs%2F0205074%2Ccs%2F0205077%2Ccs%2F0205073%2Ccs%2F0205063%2Ccs%2F0205030%2Ccs%2F0205055%2Ccs%2F0205019%2Ccs%2F0205001%2Ccs%2F0205015%2Ccs%2F0205071%2Ccs%2F0205064%2Ccs%2F0205051%2Ccs%2F0205033%2Ccs%2F0205028%2Ccs%2F0205009%2Ccs%2F0205069%2Ccs%2F0205054%2Ccs%2F0205032%2Ccs%2F0205048%2Ccs%2F0205031%2Ccs%2F0205052%2Ccs%2F0404011%2Ccs%2F0404029%2Ccs%2F0404037%2Ccs%2F0404006%2Ccs%2F0404043%2Ccs%2F0404047%2Ccs%2F0404058%2Ccs%2F0404005%2Ccs%2F0404012%2Ccs%2F0404042%2Ccs%2F0404032%2Ccs%2F0404045%2Ccs%2F0404010%2Ccs%2F0404053%2Ccs%2F0404041%2Ccs%2F0404050%2Ccs%2F0404035%2Ccs%2F0404031%2Ccs%2F0404054%2Ccs%2F0404048%2Ccs%2F0404001%2Ccs%2F0404040%2Ccs%2F0404025%2Ccs%2F0404024%2Ccs%2F0404030%2Ccs%2F0404004&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The first morphological learner based upon the theory of Whole Word\nMorphology Ford et al. (1997) is outlined, and preliminary evaluation results\nare presented. The program, Whole Word Morphologizer, takes a POS-tagged\nlexicon as input, induces morphological relationships without attempting to\ndiscover or identify morphemes, and is then able to generate new words beyond\nthe learning sample. The accuracy (precision) of the generated new words is as\nhigh as 80% using the pure Whole Word theory, and 92% after a post-hoc\nadjustment is added to the routine."}, "authors": ["Sylvain Neuvel", "Sean A. Fulop"], "author_detail": {"name": "Sean A. Fulop"}, "author": "Sean A. Fulop", "arxiv_comment": "10 pages, to appear in Proceedings of the Workshop on Morphological\n  and Phonological Learning 2002, ACL Publications", "links": [{"href": "http://arxiv.org/abs/cs/0205072v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0205072v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.6", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0205072v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0205072v1", "journal_reference": null, "doi": null, "fulltext": "Unsupervised Learning of Morphology Without Morphemes\nSean A. Fulop\nDepts. of Linguistics\nand Computer Science\nsfulop@uchicago.edu\n\narXiv:cs/0205072v1 [cs.CL] 29 May 2002\n\nSylvain Neuvel\nDept. of Linguistics\nsneuvel@uchicago.edu\n\nThe University of Chicago\n\nAbstract\nThe first morphological learner based\nupon the theory of Whole Word Morphology (Ford et al., 1997) is outlined,\nand preliminary evaluation results are presented. The program, Whole Word Morphologizer, takes a POS-tagged lexicon\nas input, induces morphological relationships without attempting to discover or\nidentify morphemes, and is then able to\ngenerate new words beyond the learning\nsample. The accuracy (precision) of the\ngenerated new words is as high as 80% using the pure Whole Word theory, and 92%\nafter a post-hoc adjustment is added to the\nroutine.\nThe aim of this project is to develop a computational model employing the theory of whole word\nmorphology (Ford et al., 1997) capable on the one\nhand of identifying morphological relations within a\nlist of words from any one of a wide variety of languages and, on the other, of putting that knowledge\nto use in creating previously unseen word forms.\nA small application called Whole Word Morphologizer which does just this is outlined and discussed.\nIn particular, this approach is set against the literature on computational morphology as an entirely\ndifferent way of doing things which has the potential\nto be generalized to all known varieties of morphology in the world's languages, a feature not shared by\nprevious methods. As it is based on a model of the\nmental lexicon in which all entries are entire, fully\nfledged words, this project also serves as an empirical demonstration that a word-based morphological\n\ntheory that rejects the notion of morpheme as minimal unit of form and meaning (and/or grammatical\nproperties) is viable from the point of view of acquisition as well as generation.\n\n1 Morphological learning\nSince its inception in the mid 1950s, the field of\ncomputational morphology has been characterized\nby a paucity of procedures for generation. Notwithstanding the impressive body of literature on the\nshortcomings of traditional Paninian morphology,\nmost computational research projects also rely on a\ntraditional notion of the morpheme and ignore all\nnon-compositional aspects of morphology. These\nobservations are obviously not unrelated and are in\npart inherited from the field of computational syntax\nwhere applications traditionally were designed to assign a syntactic structure to a given string of words,\nthough this is less true today.\n1.1\n\nSegmentation and morpheme identification\n\nWord formation and the population of the lexicon,\nwhile central to morphological theory, are noticeably absent from the field of computational morphology. Most computational work in the field\nof morphology has focused on the identification of\nmorphemes or morphological parsing while paying\nlittle or no attention to generation. While these applications find a common goal in the automatic acquisition of morphology, it is helpful to distinguish\nbetween two types of analysis in light of the often\nvery different results sought by various morphological learners.\nOn the one hand, some applications focus exclusively on the segmentation of words or longer\nstrings into smaller units. In other words, their\n\n\ffunction is to identify morpheme boundaries within\nwords and, as such, they only indirectly identify\nmorphemes as linguistic units. Zellig Harris's (Harris, 1955; Harris, 1967) pioneering work suggests\nthat morpheme boundaries can be determined by\ncounting the number of letters that follow a given\nsubstring within a corpus (v. (Hafer and Weiss,\n1974) for a further development of Harris's ideas).\nJanssen (1992) and Flenner (1994; 1995) also work\ntowards segmenting words but use training corpora\nin which morpheme boundaries have been manually\ninserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised\nlearning techniques to generate a set of segmentation rules that can further be applied to previously\nunseen words.\nOn the other hand, some computational morphological applications are designed solely to identify\nmorphemes based on a training corpus and not to\nprovide a morphological analysis for each word of\nthat corpus. Brent (1993), for example, aims at finding the right set of suffixes from a corpus, but the\nalgorithm cannot double as a morphological parser.\nMore recently, efforts have been developing\nwhich identify morphemes and perform some sort of\nanalysis. Schone and Jurafsky (2001) employ a great\nmany sophisticated post-hoc adjustments to obtain\nthe right conflation sets for words by pure corpus\nanalysis without annotations. Their procedure uses\na morpheme-based model, provides an analysis of\nthe words, and does in a sense discover morphological relations. Goldsmith (2001b; 2001a), inspired by\nde Marcken's (1995) thesis on minimum description\nlength, attempts to provide both a list of morphemes\nand an analysis of each word in a corpus. Also, Baroni (2000) aims at finding a set of prefixes from a\ncorpus, together with an affix-stem parse of each of\nthe words.\nWhile they might differ in their methods or objectives, all of the above morphological applications\nshare a common characteristic in that they are learners designed exclusively for the acquisition of morphological facts from corpora and do not generate\nnew words based on the information they acquire.\n1.2\n\nParsing and generation\n\nOnly a handful of programs can both parse and generate words. Once again, these programs fall into\n\ntwo very distinct categories. In view of the disparity between these programs, it is useful to distinguish between genuine morphological learners able\nto generate from acquired knowledge and generators/parsers that implement a man-made analysis.\nThe latter group is perhaps the most well known, so\nlet us begin with them.\nKimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen,\n1994) can provide a morphological analysis of the\nwords in a corpus and generate new words based on\na set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the\nAttribute-Logic Engine (Carpenter, 1992). Some of\nthese systems (e.g. (Karttunen et al., 1987)) have\na front-end that compiles more traditional linearly\nordered morphological rules into the finite-state automata of two-level morphology. Once again, these\napplications require a set of man-made lexical rules\nto function. While the practical uses of such applications as PC-Kimmo are incontestable, it is clear that\nthey are part of a different endeavour, and should not\nbe confused with genuine morphological learners.\nThe other relevant group of computational applications can, as mentioned, both acquire morphological knowledge from corpora and generate new\nwords based on that knowledge. Albright and Hayes\n(2001a; 2001b) tackle the wider task of acquiring morphology and (morpho)phonology based on\na small paradigm list and their learner is able to generate particular inflected forms given a related word.\nD\u017eeroski and Erjavec (1997) work towards learning\nmorphological rules for forming particular inflectional forms given a lemma (a set of related words).\nTheir learner produces a set of rules relating all the\nmembers of a paradigm to a base form. The program\ncan then produce a member of that paradigm on\ncommand given the base form. While the methods\nused by Albright and Hayes and D\u017eeroski and Erjavec radically differ, both use a form of supervised\nlearning which significantly reduces the amount of\ninformation their learner has to acquire. Albright\nand Hayes train their program using a paradigm list\nin which each entry contains, for example, both the\npresent and past tense forms of an English verb.\n\n\fSimilarly, the training data used by D\u017eeroski and Erjavec similarly has a base form, or lexeme, associated to each and every word so that all the words\nof a given paradigm share a common label. The\ndistinctions between the two methods are immaterial, what matters is that both learners are being told\nwhich words are related to which and are left with\nthe task of describing that relation in the form a rule.\nIn other words, the algorithms they use cannot discover that words are morphologically related.\n1.3\n\nWhat's morphology?\n\nIn the above algorithms, the task of determining\nwhether one word is related to another in a morphological sense is most frequently left to the linguist,\nas this information has to be encoded in the training data for these algorithms. (Some of the most\nrecent work such as (Schone and Jurafsky, 2001)\nand (Goldsmith, 2001b) are notable exceptions to\nthis paradigm.) This is perhaps not surprising, since\nno serious attempt at defining a morphological relation has been made in the last few decades. American structuralists of the forties and fifties proposed\nwhat have been referred to as discovery procedures\n(v. (Nida, 1949), for example) for the identification\nof morphemes but since the mid fifties (Chomsky,\n1955), it has been customary for morphological theory to ignore this aspect of morphology and relegate\nit to studies on language acquisition. But, since a\nmorphological learner like that presented here is designed to model the acquisition of morphology, it\nseems that it should above all be able to determine\nfor itself whether two words are morphologically related or not, whether there is anything morphological to acquire at all.\nAnother important thing to note about the vast\nmajority of computational morphology learners is\ntheir reliance on a traditional notion of the morpheme as a lexical unit and their exclusive focus on concatenative morphology. There is a\npanoply of recent publications devoted to the empirical shortcomings of traditional so-called \"Itemand-Arrangement\" morphology (Hockett, 1954;\nBochner, 1993; Ford and Singh, 1991; Anderson,\n1992; Ford et al., 1997), and the list of phenomena\nthat fall out of reach of a compositional approach\nis rather impressive: zero-morphs, ablaut-like processes, templatic morphology, class markers, partial\n\nsuppletion, etc. Still, seemingly every documented\nmorphological learner relies on a Bloomfieldian notion of the morpheme and produces an Item-andArrangement analysis; this description applies to all\nof the computational papers cited above.\n\n2 An alternative theory\nWhole Word Morphologizer (henceforth WWM) is\nthe first implementation of the theory of Whole\nWord Morphology. The theory, developed by Alan\nFord and Rajendra Singh at Universit\u00e9 de Montr\u00e9al,\nseeks to account for morphological relations in a\nminimalist fashion. Ford and Singh published a series of papers dealing with various aspects of the theory between 1983 and 1990. Drawing on these papers, they published a full outline of it in 1991 (Ford\nand Singh, 1991) and an even fuller defense of it\nin 1997 (Ford et al., 1997). Since then, aspects of it\nhave been taken up in a series of publications by Agnihotri, Dasgupta, Ford, Neuvel, Singh, and various\ncombinations of these authors. The central mechanism of the theory, the Word Formation Strategy\n(WFS), is a sort of non-decomposable morphological transformation that relates full words with full\nwords (or helps one fashion a full word from another full word) and parses any complex word into\na variable and a non-variable component. Neuvel\nand Singh (In press) offer a strict definition of morphological relatedness and, based on this definition,\nsuggest guidelines for the acquisition of Word Formation Strategies.\nIn Whole-Word Morphology, any morphological\nrelation can be represented by a rule of the following\nform:\n(1)\n\n|X |\u03b1 \u2194 |X \u2032 |\u03b2\n\nin which the following conditions and notations are\nemployed:\n1. |X |\u03b1 and |X \u2032 |\u03b2 are statements that words of the\nform X and X \u2032 are possible in the language,\nand X and X \u2032 are abbreviations of the forms of\nclasses of words belonging to categories \u03b1 and\n\u03b2 (with which specific words belonging to the\nright category can be unified in form);\n2. \u2032 represents all the form-related differences between X and X \u2032 ;\n\n\f3. \u03b1 and \u03b2 are categories that may be represented\nas feature-bundles;\n4. \u2194 represents a bi-directional implication;\n5. X \u2032 and X are semantically related.\nThere are several ramifications of (1). First, there\nis only one morphology; no distinction, other than\na functional one, is made between inflection and\nderivation. Second, morphology is relational and not\ncompositional. The program thus makes no reference to theoretical constructs such as 'root', 'stem',\nand 'morpheme', or devices such as 'levels' and\n'strata' and relies exclusively on the notion of morphological relatedness. And since its objective is\nnot to assign a probability to a given word or string,\nit must rely on a strict formal definition of a morphological relation. Ultimately, the theory takes the\nSaussurean view that words are defined by the differences amongst them and argues that some of these\ndifferences, namely those that are found between\ntwo or more pairs of words, constitute the domain\nof morphology. In other words, two words of a lexicon are morphologically related if and only if all the\ndifferences between them are found in at least one\nother pair of words of the same lexicon.\n\nas a data structure. Currently, it works on orthographic representations. This means it would as easily work on phonemic transcriptions, but it will require empirical evaluation to see whether the results\nfrom these can improve upon those obtained using\nspellings, and we have not yet gone through such an\nexercise. It starts on either the left or right edge of\nthe words if the two words share their first (few) segments or their last (few) segments, respectively (the\nforward version is presented in Algorithm 2 in the\nnext section). This is just a simple-minded way of\naligning the similar parts of the words for the comparison; a more sophisticated implementation in the\nfuture could use a more general sequence alignment\nprocedure. The segments are placed in one of two\nlists in the comparison structure (differences or similarities) based on whether or not they are identical.\nEach comparison structure also contains the categories of both words, and is kept in a large list of all\ncomparison structures found from analyzing the entire corpus. The example below shows the information in the comparison structure produced from the\nEnglish words receive and reception. It includes the\ndifferences and similarities between the two words,\nfrom the perspective of each word in turn, as well as\nthe lexical categories of the words.\n\n3 Overview of the method\nUnder the assumption that the morphology of a language resides exclusively in differences that are exploited in more than one pair of words within its lexicon, WWM (Algorithm 1 in the next section) compares every word of a small lexicon and determines\nthe segmental differences found between them. The\ninput to the current version of the program is a small\ntext file that contains anywhere from 1000 to 5000\nwords. Each word appears in orthographic form and\nis followed by its syntactic and morphological categories, as in the example below:\n(2)\n\ncat,\ncatch,\ncatches,\n\nNs\nV\nV3s\n\ndecided,\n\nVp\n\n(Noun, singular)\n(Verb, (pres.) 3rd pers.\nsing.)\n(Verb, past)\n\nThe algorithm simply compares each letter from\nword A to the corresponding one from word B to\nproduce a comparison record, which can be viewed\n\n(3)\n\nDifferences\nFirst word Second word\n####iveV\n\n####ptionNs\n\nSimilarities\nFirst\nSecond\nrece###\n\nrece#####\n\nMatching character sequences in the difference\nsection are replaced with a variable. The result is then set against comparisons generated by\nother pairs of words and duplicate differences are\nrecognized. In the example below, the comparisons produced by the pairs receive/reception, conceive/conception and deceive/deception are shown.\n\n\f(4)\n\nDifferences\nFirst word Second word\nX iveV\nX iveV\nX iveV\n\nX ptionNs\nX ptionNs\nX ptionNs\n\nSimilarities\nFirst\nSecond\nrece###\nconce###\ndece###\n\nrece#####\nconce#####\ndece#####\n\nThe three comparisons in (4) share the same formal and grammatical differences, and so the theory\nindicates they should be merged into one morphological strategy. Since the differences are the same,\nit is only the similarities that are actually merged.\nEach new morphological strategy is also restricted\nto apply in as narrow an environment as possible.\nNeuvel and Singh (Neuvel and Singh, In press) suggest that any morphological strategy must be maximally restricted at all times; this is accomplished by\nspecifying as constant all the similarities found, not\nbetween words, but between the similarities found\nbetween words. In (4), all three sets of similarities\nend with the sequence of letters \"ce.\" These similarities between similarities are specified as constant in\neach strategy and the length of each word is also factored in. The merge routine called in Algorithm 2\ncarries out this procedure; we don't show it because\nit is tedious but not especially interesting. The restricted morphological strategy relating the words in\n(4) is as follows:\n(5)\n\nDifferences\nFirst word Second word\nX iveV\n\nX ptionNs\n\nSimilarities\nFirst\nSecond\n\u2217##ce###\n\n\u2217##ce#####\n\nFor the sake of clarity, we can represent the information contained in (5) in a more familiar fashion\nusing the formalism described in (1). The vertical\nbrackets '|*|' are used for orthographic forms so as\nnot to confuse them with phonemic representations.\n(6)\n\n|\u2217##ceive|V \u2194 |\u2217##ception|Ns\n\nThe '#' signs in the above representations stand\nfor letters that must be instantiated but are not specified; the '\u2217' symbol stands for a letter that is not\nspecified and that may or may not be instantiated.\nStrategy (6) can therefore be interpreted as follows:\n(6\u2032 ) If there is a verb that ends with the sequence\n\"ceive\" preceded by no less than two and\nno more than three characters, there should\nalso be a singular noun that ends with the sequence \"ception\" preceded by the same two\nor three characters.\nAfter performing the comparisons and merging,\nWWM extracts a list of morphological strategies,\nwhich are those comparison structures whose count\nis more than some fixed threshold. Table 1 contains a few strategies found from the first few\nchapters of Moby Dick. These strategies result\nfrom merging comparison structures which have the\nsame differences-merging the similarities of several unifiable word pairs, and so many have no specified letters at all.\nWWM then goes through the lexicon word by\nword and attempts to unify each word in form and\ncategory with the left or right side of this strategy.\nIf it succeeds, WWM replaces all the segments fully\nspecified on the side of the strategy the word is unified with, with the segments fully specified on the\nother side. For example, given the noun perception\nin the corpus and strategy (6), WWM will map the\nword onto the right hand side of (6), take out the sequence \"ception\" from the end and replace it with\nthe sequence \"ceive\" to produce the new word perceive. The category of the word will also be changed\nfrom singular noun to verb. New words can thus be\ngenerated in a rather obvious fashion by taking each\nword in the original lexicon and applying any strategies that can be applied, i.e. whose orthographic\nform and part of speech can be unified with the word\nat hand. Algorithm 3 shows the basic generation\nprocedure; once again the routines called unify\nand create which implement the nitty-gritty details of the above description are not given because\nthey are more tedious than interesting, and will certainly need to be changed in more general future\nversions of WWM. Table 2 gives some of the new\nwords WWM creates using text from Le petit prince\nas its base lexicon.\n\n\fTable 1: Word-formation strategies discovered from Moby Dick\nDifferences\n1st word 2nd word\nXdPP\nXV\nXedPP\nXV\nXsNp\nXNs\nXingGER XedPP\nXingGER XsV3s\nXnessNs XADJ\nXlyADV\nXADJ\nXestADJ\nXADJ\nXsV3s\nXV\nXerADJ\nXADJ\nXlessADJ XNs\nXingGER XyADJ\nXedPP\nXsV3s\nXingsNp XV\n\nSimilarities\n1st word\n2nd word\n\u2217\u2217\u2217\u2217####e#\n\u2217\u2217\u2217\u2217####e\n\u2217########\n\u2217######\n\u2217\u2217\u2217\u2217\u2217\u2217#####\n\u2217\u2217\u2217\u2217\u2217\u2217####\n\u2217\u2217\u2217\u2217\u2217\u2217####### \u2217\u2217\u2217\u2217\u2217\u2217######\n\u2217\u2217\u2217\u2217\u2217#######\n\u2217\u2217\u2217\u2217\u2217#####\n\u2217\u2217\u2217\u2217######### \u2217\u2217\u2217\u2217\u2217\u2217\u2217\u2217#####\n\u2217\u2217\u2217\u2217\u2217\u2217######\n\u2217\u2217\u2217\u2217\u2217\u2217####\n\u2217#######\n\u2217####\n\u2217\u2217\u2217#####\n\u2217\u2217\u2217####\n\u2217######\n\u2217####\n\u2217########\n\u2217####\n\u2217#######\n\u2217#####\n\u2217\u2217######\n\u2217\u2217#####\n\u2217\u2217\u2217#########\n\u2217\u2217\u2217#####\n\nTable 2: Words generated from Le petit prince\ndrames\ndress\u00e9e\ndresser\ndressa\ndressais\ndresse\ndressent\ndressez\ndressait\ndroits\ndroites\n\nNp\nPF\nINF\nVp3\nVi2\nV3\nV6\nV5\nVi3\nAMP\nAFP\n\ndroitement\ndr\u00f4les\ndr\u00f4lement\ndunes\ndurerait\nd\u00e9cid\u00e9e\nd\u00e9cider\nd\u00e9cida\nd\u00e9cide\nd\u00e9coiff\u00e9\nd\u00e9concentr\u00e9s\n\nADV\nAIP\nADV\nNp\nVc3\nPF\nINF\nVp3\nV3\nAM\nAMP\n\nThe output from the algorithm is a list of words,1\nmuch as in Table 2, which are generated from the input corpus using the morphological relations (strategies) discovered. The method described above will\nclearly force WWM to create words that were already part of its original lexicon; in fact, each and\nevery word involved in licensing the discovery of\na morphological strategy will be duplicated by the\nprogram. Generated words that were not part of\nWWM's original lexicon are then added to a sepa1 By word we mean an orthographic form together with the\npart of speech. Further work in this vein would add meanings\nas well.\n\nExamples\nbaked/bake, charged/charge\ndirected/direct\nhelmets/helmet, rabbits/rabbit\nwalking/walked, talking/talked\nwalking/walks, talking/talks\nshort/shortness\neasy/easily, quick/quickly\nhardest/hard, shortest/short\njumps/jump, plays/play\nharder/hard, louder/loud\npainless/pain, childless/child\nraining/rainy, running/runny\nplayed/plays\npaintings/paint\n\nrate word list containing only new words. If desired,\nthis new word list can be merged with the original\nlexicon for another round of discovery to formulate new strategies based on a larger dataset. Additionally, each of the new words can simply be put\nthrough another cycle of word creation by applying\nthe same strategies as before a second time.\n\n4 Implementation\nThis section contains some pseudocode showing\nseveral basic components of the Whole Word Morphologizer. Algorithm 1 shows the main procedure,\nwhich takes a POS-tagged lexicon as input and outputs a list of all words that are possible given the\nmorphological relations present in the lexicon.\nThe two procedures compforward and compbackward are symmetrical, so Algorithm 2 shows\njust the first of these. This algorithm provides the\ndata structure which includes the differences and\nsimilarities between each pair of words in the lexicon, in similar fashion to the examples in the preceding section. In practice, only those pairs of words\nwhich are by some heuristic sufficiently similar in\nthe first place are compared. Additionally, the two\nsimilarities sequences for each word pair are actually represented as one sequence which encodes the\ninformation found in the two sequences of the examples in the preceding; this is just for convenience of\n\n\fAlgorithm 1 WWM(lexicon)\nRequire: lexicon to be a list of POS-tagged\nwords.\nEnsure: a list newwords is generated\nfor all tagged words wi do\nfor all tagged words w j do\nif wi and w j share a beginning sequence\nthen\ncompforward(wi , w j )\nelse if wi and w j share an ending sequence\nthen\ncompbackward(wi , w j )\nend if\nend for\nend for\nfor all comparison structures in the list do\nif count(comparison) > Threshold then\nappend comparison to the list\nstrategies\ngenerate(lexicon, strategies)\nend if\nend for\n\nstorage and computation.\nAlgorithm 3 shows the outline of the final stage,\nwhich generates an output list of words from the input lexicon and the morphological strategies. The\nstrategy list is simply a list of all comparison structures that occurred more frequently than some arbitrary threshold number.\n\n5 Accomplishments and prospects\n5.1\n\nInitial results\n\nWhole Word Morphologizer has been tested on a\nlimited basis using English and French lexicons of\napproximately 3000 entries, garnered from the POStagged versions of Le petit prince and Moby Dick.\nThe program initially, without any post-hoc corrections, achieved between 70% and 82% accuracy in\ngeneration; these figures measure the percentage of\nthe new words beyond the original lexicon that are\npossible words of the language. The figures thus\nmeasure a kind of precision value, in terms of the\nprecision/recall tradeoff, and are fair values in that\nthey do not include the generated words that are already in the lexicon.\n\nAlgorithm 2 compforward(w1 , w2 )\nRequire: w1 and w2 to be (word, category) pairs.\nEnsure: a data structure comparison documenting the different and similar letters between w1\nand w2 is merged into the global list of comparisons. comparison is a structure of 5 lists\nw1 dif, w1 cat, w2 dif, w2 cat, sim.\nfor x = 1 to length(w2 ) do\nif characters w1 (x) = w2 (x) then\nappend w1 (x) to list sim\nif list w1 dif does not end with 'X' then\nappend 'X' to both lists w1 dif and w2 dif\nelse\nappend w1 (x) to w1 dif,\nappend w2 (x) to w2 dif, append '#' to sim\nend if\nend if\nend for\nfor x = length(w2 ) + 1 to length(w1 ) do\nappend w1 (x) to w1 dif\nend for\nif dif lists and categories match a comparison already in the list comps then\nmerge\ncomparisons\nand\nincrement\ncount(comparison)\nelse\nappend comparison to comps\ncount(comparison) \u2190 1\nend if\n\nA satisfactory recall metric seems impossible to\nthink of in its usual sense here. First of all, there are\ngenerally an indefinite number of possible words in a\nlanguage. One therefore cannot give a precise set of\nwords that we wish the system could generate from\na specific lexicon, so there seems to be no way to\nmeasure the percentage of \"desired words\" that are\nin fact generated. Even if we were to make such a\nlist by hand from the current small corpora to use as\na gold standard (which has been suggested by a referee), it must also be remembered that WWM discovers strategies (morphological relations) for creating new words from given ones. It cannot be expected to discover strategies that are not evident in a\ncorpus. Indeed, WWM will never discover that, for\nexample, 'am' and 'be' are related, because according to the theory of morphology being applied these\n\n\fAlgorithm 3 generate(lexicon, strategies)\nEnsure: a list newwords is generated using\nlexicon and strategies\nfor all words in lexicon do\nfor all strategies do\nif unify(lexicon[x], strategies[x])\nsays the word and strategy match with either\nleft or right alignment then\nnewword\n\u2190\ncreate(lexicon[x],\nstrategies[x])\nif newword is not in the lexicon or the list\nnewwords then\nappend newword to newwords list\nend if\nend if\nend for\nend for\nwords are only related by convention, not by morphology. \"Nonproductive morphology\" is not really\nmorphology.\nThe real point is that we do not want to hold\nWWM's performance up against our own ideas\nabout morphological relations among words, since\nit would be practically impossible to determine not\nmerely a large set of possible words that linguists\nthink are related to those in the corpus, but rather a\nset of possible words that WWM ought to generate\naccording to its theory. This would amount to trying to beat WWM at its own game in pursuit of a\ngold standard, which could only be obtained using a\nbetter implementation of WWM's theory. A perfect\nimplementation of Whole Word Morphology would\nhave perfect recall, in view of our eventual goal of\nusing this theory to inform us about the morphology\nof a language-about what ought to be recalled. We\nare not trying to learn something that we feel is already known.\n5.2\n\nWhat's learning?\n\nIt is worth considering the endeavor of learning morphology in terms of formal learning theory, as presented in Osherson et al. (1986) or Kanazawa (1998)\nfor example. In the classical framework, the problem of learning a language from positive example data is approached by considering the successive guesses at the target language that a purported\n\nlearner makes when presented with some sequentially increasing learning sample drawn from that\nlanguage. Considering just morphology, it seems\nthat the target language is the set of all possible\nwords of the natural language at hand, a possibly\ninfinite (or at least indefinite) set. WWM's output\nis a list of generated words subsuming the corpus,\nwhich are supposed to be all the words creatable by\napplying its idea of morphology to that corpus. It\ncan thus be viewed as making a guess about the target language, given a certain learning sample. If the\nlearning sample is increased, its guess increases in\nsize also. The errors in precision of course mean\nthat at the current corpus sizes its guesses are for the\nmoment not even subsets of the target language.\nAccording to one classic paradigm, a system\nwould be held to be a successful learner if it could\nbe proven to home in on the target language as the\nlearning sample increased in size indefinitely. This\nis Gold's (1967) criterion of identification in the\nlimit. In this framework, an empirical analysis cannot be used to decide the adequacy of a learner, and\nwe would like to deemphasize the importance of the\nempirical results for this purpose. That said, the empirical results are for now all we have to show, but\neventually we hope to produce a mathematical proof\nof just what WWM can learn, and just what kinds of\nlexicons are learnable in Gold's sense.\nTo our knowledge, it has never been proven\nwhether the total lexicon of a natural language is\nidentifiable in the limit from the sort of data we provide (i.e. POS-tagged words), using in particular the\ntheory of Whole Word Morphology in a perfect fashion. Still, it is interesting that nothing about this language learning paradigm says anything about morphological analysis. The current crop of true morphological learners, e.g. (Goldsmith, 2001b), endeavor to learn to analyze the morphology of the\nlanguage at hand in the manner of a linguist. Goldsmith has even called his Linguistica system a \"linguist in a box.\" This is perhaps an interesting and\nworthwhile endeavor, but it is not one that is undertaken here. WWM is instead attempting to learn\nthe target language in a more direct way from the\ndata, without first constructing the intermediary of\na traditional morphological analysis. We are thus\nnot learning the linguist's notion of morphology but\nrather the result of morphology, i.e. the word forms\n\n\fof the language together with the other information\nthat goes into a word.2\n5.3\n\nPost-hoc fixes and future developments\n\nA significant proportion of errors in generation result from the application of competing ambiguous\nmorphological strategies. For example, when using the (French) text of Le petit prince as its base\nlexicon, WWM produces two strategies relating 2nd\nperson verb forms to their infinitives. Given the verb\nconjugues 'conjugate,' pres. 2nd sing., one strategy\nproduces the correct -er class infinitive conjuguer\nwhile the other creates the non-word *conjuguere,\nbased on the relation among -re verb forms like\nfais/faire 'do' and vends/vendre 'sell.' This is because of an inherent ambiguity among various word\npairs which do not fully indicate the paradigms of\nwhich they are a part. WWM then adds to its lexicon, not only the correct form, but all the outputs\nwarranted by its grammar.\nTo try to correct this problem, a form of lexical\nblocking has been implemented in the current version of the program. WWM creates every possible\nword, including different strategies giving the same\none, and lets lexical lookup take precedence over\nproductive morphology. The knowledge WWM possesses about its lexicon increases considerably during the creation of morphological strategies. The\nprogram learns not only which strategies are licensed by a given lexicon, but also which words\nof its lexicon are related to one another. WWM\ncan assign a number to every lexical entry and give\nthe same \"paradigm\" number to related words. Before adding a newly created word to its lexicon, the\nprogram looks for an existing word with the same\nparadigm number and category. For example, if\nWWM maps the word decoction, which was assigned to, say, paradigm 489 onto a strategy creating\nplural nouns, it will look for a plural noun belonging\nto paradigm 489 in its lexicon before it adds decoctions to the list of new words.\nPreliminary results are encouraging, with WWM\nreaching up to 92% accuracy in generation after\n2 In\n\nthis theory, a word's form cannot be usefully divorced\nfrom the other information that allows its proper use, and in our\nimplementation the POS tags (poor substitutes for what should\nbe a richer database of information) are crucial to the discovery\nof the strategies.\n\nthe blocking modification. Obviously the program\nneeds to be systematically tested on multiple lexica\nfrom different languages, but these results strongly\nsuggest that it is possible to model the acquisition\nof morphology as a component of learning to generate language directly, rather than to treat computational learning as the acquisition of linguistic theory\nas several current approaches do, e.g. (Goldsmith,\n2001b).\nAlthough the principles of whole word morphology allow one to contemplate versions of WWM that\nwould work on templatic morphologies, polysynthetic languages, and a host of other recalcitrant phenomena, the current instantiation of the program is\nnot so ambitious. The comparison algorithm detailed in the previous section compares words letter\nby letter, either from left to right or from right to\nleft. No other possible alignments between words\nare considered and WWM is in its current state only\ncapable of grasping prefixal and suffixal morphology. We are currently developing a more sophisticated sequence alignment routine which will allow the program to handle infixing, circumfixing,\nand templatic morphologies of the Semitic type, as\nwell as word-internal changes typified by Germanic\nstrong verb ablaut.\n\nReferences\nAdam Albright and Bruce Hayes.\n2001a.\nAn\nautomated learner for phonology and morphology.\nhttp://www.linguistics.ucla.edu/people/hayes/\nlearning/index.htm.\nAdam Albright and Bruce Hayes.\n2001b.\nBurnt\nand\nsplang:\nSome\nproblems\nof\ngenerality\nin\nphonological\nlearning.\nhttp://www.linguistics.ucla.edu/people/hayes/learning/\nindex.htm.\nStephen R. Anderson. 1992. A-Morphous Morphology.\nCambridge University Press.\nEvan L. Antworth. 1990. PC-KIMMO: a two-level processor for morphological analysis. Occasional Publications in Academic Computing 16, Summer Institute\nof Linguistics, Dallas, TX.\nMarco Baroni. 2000. An automated distribution-driven\nprefix learner. Presented at the 9th International Morphology Meeting, Vienna, Austria, February.\nH. Bochner. 1993. Simplicity in Generative Morphology.\nMouton de Gruyter, Berlin.\n\n\fMichael Brent. 1993. Minimal generative models: A\nmiddle ground between neurons and triggers. In Proceedings of the 15th Annual Conference of the Cognitive Science Society, pages 28\u201336. Lawrence Erlbaum\nAssociates.\nBob Carpenter. 1992. The Logic of Typed Feature Structures, volume 32 of Cambridge Tracts in Theoretical\nComputer Science. Cambridge University Press.\nNoam Chomsky. 1955. The logical structure of linguistic\ntheory. Unpublished manuscript. Published as a book\nwith a new introduction in 1975 by Plenum Press.\nCarl de Marcken. 1995. Unsupervised Language Acquisition. Ph.D. thesis, MIT.\nSa\u0161o D\u017eeroski and Toma\u017e Erjavec. 1997. Induction\nof Slovene nominal paradigms. In Nada Lavrac and\nSa\u0161o D\u017eeroski, editors, Inductive Logic Programming,\n7th International Workshop, volume 1297 of Lecture\nNotes in Computer Science. Springer.\n\nCharles Hockett. 1954. Two models of grammatical description. Word, 10:210\u2013231.\nAxel Janssen. 1992. Segmentierung franz\u00f6sischer Wortformen in Morphe ohne Verwendung eines Lexikons.\nIn Ursula Klenk, editor, Computatio Linguae, pages\n74\u201395. Steiner Verlag, Stuttgart.\nMakoto Kanazawa. 1998. Learnable Classes of Categorial Grammars. Studies in Logic, Language and Information. CSLI Publications and the European Association for Logic, Language and Information.\nLauri Karttunen, Kimmo Koskenniemi, and Ronald M.\nKaplan. 1987. A compiler for two-level phonological\nrules. Technical Report CSLI-87-108, Center for the\nStudy of Language and Information, Palo Alto.\nLauri Karttunen, Ronald M. Kaplan, and Annie Zaenen.\n1992. Two-level morphology with composition. In\nProceedings of the 15th International Conference on\nComputational Linguistics, volume I, pages 141\u2013148,\nNantes, France.\n\nGudrun Flenner. 1994. Ein quantitatives Morphsegmentierungssystem f\u00fcr spanische Wortformen. In Ursula\nKlenk, editor, Computatio Linguae II, pages 31\u201362.\nSteiner Verlag, Stuttgart.\n\nLauri Karttunen. 1993. Finite state constraints. In\nJohn A. Goldsmith, editor, The Last Phonological\nRule, pages 173\u2013194. University of Chicago Press.\n\nGudrun Flenner.\n1995.\nQuantitative Morphsegmentierung im Spanischen auf phonologisher Basis.\nSprache und Datenverarbeitung, 19(2):63\u201379.\n\nLauri Karttunen. 1994. Constructing lexical transducers.\nIn Proceedings of the 15th International Conference\non Computational Linguistics, volume I, pages 406\u2013\n411.\n\nA. Ford and R. Singh. 1991. Prop\u00e9deutique morphologique. Folia Linguistica, 25(3\u20134):549\u2013575.\nA. Ford, R. Singh, and G. Martohardjono. 1997. Pace\nPanini. Peter Lang, New York.\nE. M. Gold. 1967. Language identification in the limit.\nInformation and Control, 10:447\u2013474.\nJohn A. Goldsmith. 2001a. Linguistica: An automatic\nmorphological analyzer. In Arika Okrent and John\nBoyle, editors, CLS 36: The Main Session, volume 361. Chicago Linguistic Society, Chicago.\n\nDimitar Kazakov and Suresh Manandhar. 1998. A hybrid approach to word segmentation. In David Page,\neditor, Proceedings of Inductive Logic Programming98, volume 1446 of Lecture Notes in Computer Science. Springer.\nKimmo Koskenniemi. 1983. Two-level morphology: a\ngeneral computational model for word-form recognition and production. Technical Report 11, Dept. of\nGeneral Linguistics, University of Helsinki.\nS. Neuvel and R. Singh. In press. Vive la diff\u00e9rence!\nWhat morphology is about. Folia Linguistica.\n\nJohn A. Goldsmith. 2001b. Unsupervised learning of\nthe morphology of a natural language. Computational\nLinguistics, 27(2):153\u2013198.\n\nEugene Nida. 1949. Morphology. The descriptive analysis of words. University of Michigan Press, Ann Arbor, MI.\n\nM. A. Hafer and S. F. Weiss. 1974. Word segmentation\nby letter successsor varieties. Information Storage and\nRetrieval, 10(371\u2013385).\n\nDaniel N. Osherson, Michael Stob, and Scott Weinstein.\n1986. Systems that Learn. The MIT Press, Cambridge, MA.\n\nZellig Harris. 1955. From phoneme to morpheme. Language, 31:190\u2013222.\n\nPatrick Schone and Daniel Jurafsky. 2001. Knowledgefree induction of inflectional morphologies. In 2nd\nMeeting of the North American Chapter of the ACL,\npages 183\u2013191. Association for Computational Linguistics, Morgan Kaufman.\n\nZellig Harris. 1967. Morpheme boundaries within\nwords: Report on a computer test. In Transformations\nand Discourse Analysis Papers, volume 73.\n\n\f"}