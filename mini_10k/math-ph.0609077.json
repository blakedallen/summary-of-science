{"id": "http://arxiv.org/abs/math-ph/0609077v1", "guidislink": true, "updated": "2006-09-27T10:21:34Z", "updated_parsed": [2006, 9, 27, 10, 21, 34, 2, 270, 0], "published": "2006-09-27T10:21:34Z", "published_parsed": [2006, 9, 27, 10, 21, 34, 2, 270, 0], "title": "An amended MaxEnt formulation for deriving Tsallis factors, and\n  associated issues", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math-ph%2F0611050%2Cmath-ph%2F0611012%2Cmath-ph%2F0611014%2Cmath-ph%2F0611081%2Cmath-ph%2F0611057%2Cmath-ph%2F0611002%2Cmath-ph%2F0611049%2Cmath-ph%2F0611058%2Cmath-ph%2F0611035%2Cmath-ph%2F0611078%2Cmath-ph%2F0611083%2Cmath-ph%2F0611041%2Cmath-ph%2F0611071%2Cmath-ph%2F0611046%2Cmath-ph%2F0611039%2Cmath-ph%2F0611017%2Cmath-ph%2F0611085%2Cmath-ph%2F0611069%2Cmath-ph%2F0611043%2Cmath-ph%2F0611056%2Cmath-ph%2F0611044%2Cmath-ph%2F0611079%2Cmath-ph%2F0611023%2Cmath-ph%2F0611059%2Cmath-ph%2F0611011%2Cmath-ph%2F0101034%2Cmath-ph%2F0101031%2Cmath-ph%2F0101033%2Cmath-ph%2F0101011%2Cmath-ph%2F0101038%2Cmath-ph%2F0101025%2Cmath-ph%2F0101014%2Cmath-ph%2F0101023%2Cmath-ph%2F0101001%2Cmath-ph%2F0101020%2Cmath-ph%2F0101030%2Cmath-ph%2F0101009%2Cmath-ph%2F0101029%2Cmath-ph%2F0101018%2Cmath-ph%2F0101016%2Cmath-ph%2F0101015%2Cmath-ph%2F0101017%2Cmath-ph%2F0101006%2Cmath-ph%2F0101022%2Cmath-ph%2F0101036%2Cmath-ph%2F0101010%2Cmath-ph%2F0101007%2Cmath-ph%2F0101012%2Cmath-ph%2F0101035%2Cmath-ph%2F0101002%2Cmath-ph%2F0101003%2Cmath-ph%2F0101013%2Cmath-ph%2F0101026%2Cmath-ph%2F0101005%2Cmath-ph%2F0101024%2Cmath-ph%2F0101004%2Cmath-ph%2F0101032%2Cmath-ph%2F0101037%2Cmath-ph%2F0101021%2Cmath-ph%2F0101027%2Cmath-ph%2F0101019%2Cmath-ph%2F0101028%2Cmath-ph%2F0101008%2Cmath-ph%2F0609086%2Cmath-ph%2F0609034%2Cmath-ph%2F0609061%2Cmath-ph%2F0609059%2Cmath-ph%2F0609012%2Cmath-ph%2F0609013%2Cmath-ph%2F0609029%2Cmath-ph%2F0609041%2Cmath-ph%2F0609001%2Cmath-ph%2F0609020%2Cmath-ph%2F0609022%2Cmath-ph%2F0609038%2Cmath-ph%2F0609032%2Cmath-ph%2F0609040%2Cmath-ph%2F0609050%2Cmath-ph%2F0609066%2Cmath-ph%2F0609088%2Cmath-ph%2F0609070%2Cmath-ph%2F0609069%2Cmath-ph%2F0609048%2Cmath-ph%2F0609051%2Cmath-ph%2F0609074%2Cmath-ph%2F0609033%2Cmath-ph%2F0609006%2Cmath-ph%2F0609018%2Cmath-ph%2F0609027%2Cmath-ph%2F0609058%2Cmath-ph%2F0609031%2Cmath-ph%2F0609084%2Cmath-ph%2F0609047%2Cmath-ph%2F0609008%2Cmath-ph%2F0609021%2Cmath-ph%2F0609039%2Cmath-ph%2F0609052%2Cmath-ph%2F0609079%2Cmath-ph%2F0609083%2Cmath-ph%2F0609077%2Cmath-ph%2F0609045&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "An amended MaxEnt formulation for deriving Tsallis factors, and\n  associated issues"}, "summary": "An amended MaxEnt formulation for systems displaced from the conventional\nMaxEnt equilibrium is proposed. This formulation involves the minimization of\nthe Kullback-Leibler divergence to a reference $Q$ (or maximization of Shannon\n$Q$-entropy), subject to a constraint that implicates a second reference\ndistribution $P\\_{1}$ and tunes the new equilibrium. In this setting, the\nequilibrium distribution is the generalized escort distribution associated to\n$P\\_{1}$ and $Q$. The account of an additional constraint, an observable given\nby a statistical mean, leads to the maximization of R\\'{e}nyi/Tsallis\n$Q$-entropy subject to that constraint. Two natural scenarii for this\nobservation constraint are considered, and the classical and generalized\nconstraint of nonextensive statistics are recovered. The solutions to the\nmaximization of R\\'{e}nyi $Q$-entropy subject to the two types of constraints\nare derived. These optimum distributions, that are Levy-like distributions, are\nself-referential. We then propose two `alternate' (but effectively computable)\ndual functions, whose maximizations enable to identify the optimum parameters.\nFinally, a duality between solutions and the underlying Legendre structure are\npresented.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math-ph%2F0611050%2Cmath-ph%2F0611012%2Cmath-ph%2F0611014%2Cmath-ph%2F0611081%2Cmath-ph%2F0611057%2Cmath-ph%2F0611002%2Cmath-ph%2F0611049%2Cmath-ph%2F0611058%2Cmath-ph%2F0611035%2Cmath-ph%2F0611078%2Cmath-ph%2F0611083%2Cmath-ph%2F0611041%2Cmath-ph%2F0611071%2Cmath-ph%2F0611046%2Cmath-ph%2F0611039%2Cmath-ph%2F0611017%2Cmath-ph%2F0611085%2Cmath-ph%2F0611069%2Cmath-ph%2F0611043%2Cmath-ph%2F0611056%2Cmath-ph%2F0611044%2Cmath-ph%2F0611079%2Cmath-ph%2F0611023%2Cmath-ph%2F0611059%2Cmath-ph%2F0611011%2Cmath-ph%2F0101034%2Cmath-ph%2F0101031%2Cmath-ph%2F0101033%2Cmath-ph%2F0101011%2Cmath-ph%2F0101038%2Cmath-ph%2F0101025%2Cmath-ph%2F0101014%2Cmath-ph%2F0101023%2Cmath-ph%2F0101001%2Cmath-ph%2F0101020%2Cmath-ph%2F0101030%2Cmath-ph%2F0101009%2Cmath-ph%2F0101029%2Cmath-ph%2F0101018%2Cmath-ph%2F0101016%2Cmath-ph%2F0101015%2Cmath-ph%2F0101017%2Cmath-ph%2F0101006%2Cmath-ph%2F0101022%2Cmath-ph%2F0101036%2Cmath-ph%2F0101010%2Cmath-ph%2F0101007%2Cmath-ph%2F0101012%2Cmath-ph%2F0101035%2Cmath-ph%2F0101002%2Cmath-ph%2F0101003%2Cmath-ph%2F0101013%2Cmath-ph%2F0101026%2Cmath-ph%2F0101005%2Cmath-ph%2F0101024%2Cmath-ph%2F0101004%2Cmath-ph%2F0101032%2Cmath-ph%2F0101037%2Cmath-ph%2F0101021%2Cmath-ph%2F0101027%2Cmath-ph%2F0101019%2Cmath-ph%2F0101028%2Cmath-ph%2F0101008%2Cmath-ph%2F0609086%2Cmath-ph%2F0609034%2Cmath-ph%2F0609061%2Cmath-ph%2F0609059%2Cmath-ph%2F0609012%2Cmath-ph%2F0609013%2Cmath-ph%2F0609029%2Cmath-ph%2F0609041%2Cmath-ph%2F0609001%2Cmath-ph%2F0609020%2Cmath-ph%2F0609022%2Cmath-ph%2F0609038%2Cmath-ph%2F0609032%2Cmath-ph%2F0609040%2Cmath-ph%2F0609050%2Cmath-ph%2F0609066%2Cmath-ph%2F0609088%2Cmath-ph%2F0609070%2Cmath-ph%2F0609069%2Cmath-ph%2F0609048%2Cmath-ph%2F0609051%2Cmath-ph%2F0609074%2Cmath-ph%2F0609033%2Cmath-ph%2F0609006%2Cmath-ph%2F0609018%2Cmath-ph%2F0609027%2Cmath-ph%2F0609058%2Cmath-ph%2F0609031%2Cmath-ph%2F0609084%2Cmath-ph%2F0609047%2Cmath-ph%2F0609008%2Cmath-ph%2F0609021%2Cmath-ph%2F0609039%2Cmath-ph%2F0609052%2Cmath-ph%2F0609079%2Cmath-ph%2F0609083%2Cmath-ph%2F0609077%2Cmath-ph%2F0609045&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "An amended MaxEnt formulation for systems displaced from the conventional\nMaxEnt equilibrium is proposed. This formulation involves the minimization of\nthe Kullback-Leibler divergence to a reference $Q$ (or maximization of Shannon\n$Q$-entropy), subject to a constraint that implicates a second reference\ndistribution $P\\_{1}$ and tunes the new equilibrium. In this setting, the\nequilibrium distribution is the generalized escort distribution associated to\n$P\\_{1}$ and $Q$. The account of an additional constraint, an observable given\nby a statistical mean, leads to the maximization of R\\'{e}nyi/Tsallis\n$Q$-entropy subject to that constraint. Two natural scenarii for this\nobservation constraint are considered, and the classical and generalized\nconstraint of nonextensive statistics are recovered. The solutions to the\nmaximization of R\\'{e}nyi $Q$-entropy subject to the two types of constraints\nare derived. These optimum distributions, that are Levy-like distributions, are\nself-referential. We then propose two `alternate' (but effectively computable)\ndual functions, whose maximizations enable to identify the optimum parameters.\nFinally, a duality between solutions and the underlying Legendre structure are\npresented."}, "authors": ["Jean-Fran\u00e7ois Bercher"], "author_detail": {"name": "Jean-Fran\u00e7ois Bercher"}, "author": "Jean-Fran\u00e7ois Bercher", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1063/1.2423305", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/math-ph/0609077v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math-ph/0609077v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Presented at MaxEnt2006, Paris, France, july 10-13, 2006", "arxiv_primary_category": {"term": "math-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.other", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.MP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "PACS: 05.30.-d; 05.20.-y; 05.70.Ce; 05.90.+m", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math-ph/0609077v1", "affiliation": "MOSIM", "arxiv_url": "http://arxiv.org/abs/math-ph/0609077v1", "journal_reference": null, "doi": "10.1063/1.2423305", "fulltext": "arXiv:math-ph/0609077v1 27 Sep 2006\n\nAn amended MaxEnt formulation for deriving\nTsallis factors, and associated issues\nJean-Fran\u00e7ois Bercher\n\u00c9quipe Signal et Information,\nDept. Mod\u00e9lisation et Simulation\nESIEE, Noisy-le-Grand, France\nAbstract.\nAn amended MaxEnt formulation for systems displaced from the conventional MaxEnt\nequilibrium is proposed. This formulation involves the minimization of the Kullback-Leibler\ndivergence to a reference Q (or maximization of Shannon Q-entropy), subject to a constraint that\nimplicates a second reference distribution P1 and tunes the new equilibrium. In this setting, the\nequilibrium distribution is the generalized escort distribution associated to P1 and Q. The account\nof an additional constraint, an observable given by a statistical mean, leads to the maximization\nof R\u00e9nyi/Tsallis Q-entropy subject to that constraint. Two natural scenarii for this observation\nconstraint are considered, and the classical and generalized constraint of nonextensive statistics\nare recovered. The solutions to the maximization of R\u00e9nyi Q-entropy subject to the two types\nof constraints are derived. These optimum distributions, that are Levy-like distributions, are selfreferential. We then propose two 'alternate' (but effectively computable) dual functions, whose\nmaximizations enable to identify the optimum parameters. Finally, a duality between solutions and\nthe underlying Legendre structure are presented.\nKey Words: R\u00e9nyi entropy, Levy distributions, optimization, nonextensive thermodynamics,\nduality\n\nINTRODUCTION\nThe formalism of nonextensive statistical mechanics [1, 2] leads to a generalized\nBoltzmann factor in the form of a Tsallis distribution (or factor) that depends on an\nentropic index and recovers the classical Boltzmann factor as a special limit case [1].\nThis distribution is of high interest in many physical systems since it enables to model\npower-law phenomena. In a wide variety of fields, experiments, numerical results and\nanalytical derivations fairly agree with the description by a Tsallis distribution.\nTsallis' distributions (sometimes called Levy distributions) are derived by\nmaximization of Tsallis entropy [3], under suitable constraints. The present formulation\nis as follows: maximize Tsallis' entropy\n\u0014Z\n\u0015\n1\n\u03b1\nP (x) dx \u2212 1 ,\n(1)\nT\u03b1 (P ) =\n1\u2212\u03b1\n0\n\nThis work was presented at MaxEnt2006, the 26th International Workshop on Bayesian Inference and\nMaximum Entropy Methods in Science and Engineering CNRS, Paris, France, July 8-13, 2006.\n\n\fsubject to\nm=\n\nZ\n\nxP \u2217 (x)dx with P \u2217 (x) = R\n\nP (x)\u03b1\n,\nP (x)\u03b1 dx\n\n(2)\n\nwhere the mean constraint is called a 'generalized' mean constraint in the nonextensive\nlitterature, and P \u2217 (x) is called the 'escort' distribution. This formulation\nwas preferred\nR\nto the simple maximization with a classical mean constraint m = xP (x)dx because of\nmathematical difficulties. The solution is given in the litterature as\n1\nP (x) =\nZ\n\n1\n\u0012\n\u0013 1\u2212\u03b1\n(1 \u2212 \u03b1)\u03b2\n1\u2212\n,\n(x \u2212 m)\nZ 1\u2212\u03b1\n\n(3)\n\nwhere Z is a partition function.\nOf course, these distributions do not coincide with those derived by conventionnal\nMaxEnt and consequently will not be justified from a probabilistic point of view, because\nof the uniqueness of the rate function in the large deviations theory [4, 5]. Furthermore,\nthe status and interest of generalized expectations and of escort distributions is unclear.\nLast, it is apparent that the expression of distribution (3) is implicit, so that both its\nmanipulation and determination of its parameter \u03b2 will be difficult.\nHowever, in view of the success of nonextensive statistics, there should exist a\nprobabilistic setting that provides a justification for the maximization of Tsallis entropy.\nThere are now several indications that results of nonextensive statistics are physically\nrelevant for partially equilibrated or nonequilibrated systems, with a stationary state\ncharacterized by fluctuations of an intensive parameter [6, 7]; for instance, the Tsallis\nfactor is obtained from the Boltzmann-Gibbs' if the inverse of temperature fluctuates\naccording to a gamma distribution.\nIn this paper, I present a framework for the maximization of R\u00e9nyi/Tsallis Q\u2212entropy,\nthat leads to the so-called Levy distribution (or Tsallis factor). The R\u00e9nyi information\ndivergence, the opposite of R\u00e9nyi Q-entropy, is given by\nZ\n1\nD\u03b1 (P ||Q) =\nlog P (x)\u03b1 Q(x)1\u2212\u03b1 dx,\n(4)\n\u03b1\u22121\nwhere \u03b1 is a real parameter called the entropic index. Using L'Hospital's rule, the\nKullback-Leibler divergence is recovered for \u03b1 \u2192 1\nZ\nP (x)\ndx.\n(5)\nD(P ||Q) = P (x) log\nQ(x)\nIts opposite is the Shannon Q\u2212entropy, the correct, coordinate invariant, extension of the\nclassical Shannon entropy to the continuous case [8]. This divergence can be interpreted\nas a \"distance\" between two distributions. R\u00e9nyi and Tsallis Q-entropies are related by\na simple monotonic function. Therefore, their maximization under the same constraint\nlead to the same distribution.\nIn the following, I propose an amended MaxEnt formulation for systems with a\ndisplaced equilibrium, find that the relevant entropy in this setting is the R\u00e9nyi entropy,\n\n\finterpret the mean constraints, derive the correct form of solutions, propose numerical\nprocedures for estimating the parameters of the Tsallis factor and characterize the\nassociated entropies. I will also indicate a duality between the solutions associated with\nclassical and generalized mean constraint. Finally I will discuss the underlying Legendre\nstructure of generalized thermodynamics associated to this setting.\n\nTHE AMENDED MAXENT FORMULATION\nA key for the apparition of Levy distributions and a probabilistic justification might\nbe that it seems to appear in the case of modified, perturbated, or displaced classical\nBoltzmann-Gibbs equilibrium. This means that the original MaxEnt formulation \"find\nthe closest distribution to a reference under a mean constraint\" may be amended by\nintroducing for instance a new constraint that displaces the equilibrium. The partial\nor displaced equilibrium may be imagined as an equilibrium characterized by two\nreferences, say P1 and Q. Instead of selecting the nearest distribution to a reference under\na mean constraint, we may look for a distribution P \u2217 simultaneously close to two distinct\nreferences: such a distribution will be localized somewhere 'between' the two references\nP1 and Q. For instance, we may consider a global system composed of two subsystems\ncharacterized by two prior reference distributions. The global equilibrium is attained for\nsome intermediate distribution, and the observable may be, depending on the viewpoint\nor on the experiment, either the mean under the distribution of the global system or\nunder the distribution of one subsystem. This can model a fragmentation process: a\nsystem \u03a3(A, B) fragments into A, with distribution P1 , and B with distribution Q, and\nthe whole system is viewed with distribution P \u2217 that is some intermediate between P1\nand Q. This can also model a phase transition: a system leaves a state Q toward P1 and\npresents an intermediate distribution P \u2217 .\nThis can be stated as: find P \u2217 such that the Kullback-Leibler divergence to Q,\nD(P ||Q) is minimum (or equivalently the Shannon Q-entropy is maximum), but under\nthe constraint that D(P ||Q) = D(P ||P1) + \u03b8, where \u03b8 can be expressed as a loglikelihood. The problem simply writes\n(\nR\nP (x)\ndx\nminP D(P ||Q) = minP P (x) log Q(x)\nR\n(6)\nP1 (x)\ns.t \u03b8 = D(P ||Q) \u2212 D(P ||P1) = P (x) log Q(x) dx\nand its solution was given by Kullback [9, page 39] as an illustration of his general\ntheorem on constrained minimization of D(P ||Q):\nP \u2217(x) = R\n\nP1 (x)\u03b1 Q(x)1\u2212\u03b1\n,\nP1 (x)\u03b1 Q(x)1\u2212\u03b1 dx\n\n(7)\n\nwhich is nothing else but the escort distribution (2) of nonextensive statistics [10]\n(although it is generalized here with reference Q). The parameter \u03b1 is simply the\nLagrange parameter associated to the constraint, and it can be shown that necessarily\n\u03b1 \u2264 1. Clearly, distribution P \u2217 which is the geometric mean between P1 and Q realizes\n\n\fa trade-off, governed by \u03b1, between the two references. By dual attainment, we have\n\u0012\n\u0012Z\n\u0013\u0013\n\u001a\nminP D(P ||Q)\n\u03b1\n1\u2212\u03b1\nP1 (x) Q(x) dx\n. (8)\n= sup \u03b1\u03b8 \u2212 log\ns.t \u03b8 = D(P ||Q) \u2212 D(P ||P1)\n\u03b1\nR\n\u0001\nIn this last relation, the term log P1 (x)\u03b1 Q(x)1\u2212\u03b1 dx is directly proportional to the\nR\u00e9nyi divergence (4).\n\nObservable mean values\nObservable values are as usual the statistical mean under some distributions.\nDepending on the viewpoint, the observable may be a mean under distribution P1 ,\nthe distribution of an isolated subsystem, or under P \u2217, the equilibrium distribution\nbetween P and Q. Hence, the problem will be completed by an additionnal constraint,\nand a possible approach would be to select distribution P1 by further minimizing the\nKullback-Leibler information divergence D(P ||Q), but over P1 (x) and subject to the\nmean constraint. So, the whole problem writes\n\uf8f1\n(\nR\nP (x)\n\uf8f4\ndx\nminP D(P ||Q) = minP P (x) log Q(x)\n\uf8f2\nR\nminP1\nP\n(x)\n1\n,\n(9)\nK=\nsubject to: \u03b8 = P (x) log Q(x) dx\n\uf8f4\n\uf8f3\nsubject to: m = EP1 [X] or m = EP \u2217 [X]\nR\nwhere EP [X] represents the statistical mean under distribution P : EP [X] = xP (x)dx.\nThis may be tackled in two steps: first minimize with respect to P taking into account\nthe mean log-likelihood constraint, and obtain (7), and second, minimize with respect to\nP1 . Taking into account (8), problem (9) becomes\n\u0014\n\u001a\n\u0015\nmaxP1 (\u03b1 \u2212 1)D\u03b1 (P1 ||Q)\n(10)\nK = sup \u03b1\u03b8 \u2212\nsubject to: m = EP1 [X] or m = EP \u2217 [X]\n\u03b1\nand amounts to the extremization of R\u00e9nyi information divergence under a mean\nconstraint. Therefore, we find that the amended MaxEnt formulation leads to the\nmaximization of R\u00e9nyi (or equivalently Tsallis) entropy subject to a statistical mean\nconstraint. We can note that the second constraint, m = EP \u2217 [X] is nothing else but the\n'generalized expectation' of nonextensive statistics that has here a clear interpretation.\nIt is important to note that the minimization of Kullback-Leibler divergence with\nrespect to P and P1 , subject to the two constraints, may not always reduce to the twosteps procedure above.\n\nSOLUTIONS TO THE MAXIMIZATION OF R\u00c9NYI Q-ENTROPY\nWe now consider the maximization of R\u00e9nyi Q-entropy subject to the classical mean\nconstraint (C) m = EP1 [X] and the generalized mean constraint (G) m = EP \u2217 [X] as we\nobtained in (10). We first begin by some results on a general 'Tsallis' distribution, that\nsimplify the derivation of exact solutions (proofs are omitted to save space).\n\n\fPreliminary results\nDefinition 1 Distribution P\u03bd# (x) is defined by:\n#\n\nP\u03bd# (x) = [\u03b3(x \u2212 x) + 1]\u03bd Q(x)eD\u03b1 (P\u03bd\n\n||Q)\n\n,\n\n(11)\n\non domain D = DQ \u2229D\u03b3 , where DQ = {x : Q(x) \u2265 0} and D\u03b3 = {x : \u03b3(x \u2212 x) + 1 \u2265 0} .\nIn this expression, x is either (a) a fixed parameter, say m, and P\u03bd# (x) is a two\nparameters distribution, (b) or some statistical mean with respect to P\u03bd# (x), e.g.\nits \"classical\" or \"generalized\" mean, and as such a function of \u03b3. Observe that\ndistribution P\u03bd# (x) is not necessarily normalized to one. Associated with P\u03bd# (x), we\nalso define a partition function\nZ\n(12)\nZ\u03bd (\u03b3, x) = [\u03b3(x \u2212 x) + 1]\u03bd Q(x)dx.\nD\n\nNotation 2 We will denote by E\u03bd [X] the statistical mean with respect to the probability\n(\u03b1)\ndistribution associated with P\u03bd# (x), and by E\u03bd [X] the generalized \u03b1\u2212mean. One can\n(\u03b1)\nobserve that in the case of the Levy distribution (11), we have E\u03bd [X] = E\u03b1\u03bd [X] . In the\n(\u03b1)\n\u03b1\n.\nspecial case \u03bd = \u00b1\u03be, we obtain E\u00b1\u03be [X] = E\u00b1(\u03be+1) [X] , because \u03be\u03b1 = (\u03be + 1) = \u03b1\u22121\nTheorem 3 The Levy distribution P\u03be# (x) with exponent \u03bd = \u03be, is normalized\nto one if and only if x = E\u03be [x] , the statistical mean of the distribution, and\nD\u03b1 (P\u03be# ||Q) = \u2212 log Z\u03be+1 (\u03b3, x) = \u2212 log Z\u03be (\u03b3, x).\n#\nIn the same way, the Levy distribution P\u2212\u03be\n(x) with exponent \u03bd = \u2212\u03be, is normalized\n(\u03b1)\nto one if and only if x = E\u2212\u03be\u22121 [x] = E\u2212\u03be [x] , the generalized \u03b1\u2212expectation\n#\nof the distribution, and D\u03b1 (P\u2212\u03be\n||Q) = \u2212 log Z\u2212(\u03be+1) (\u03b3, x) = \u2212 log Z\u2212\u03be (\u03b3, x), with\n\u03b1\u03be = (\u03be + 1).\n\nWhen x is a fixed parameter m, this will be only true for a special value \u03b3 \u2217 of \u03b3 such\n(\u03b1)\nthat E\u03be [x] = m or E\u2212\u03be [x] = m, respectively in the first and second case.\nRemark 4 Here takes place an important remark on the mapping x \u2194 \u03b3. Consider the\nnormalized distribution P\u03be# (x) with x = E\u03be [x] . This distribution depends on the sole\nparameter \u03b3, and x is a function of \u03b3. But contrary to the intuition, the mapping x \u2194 \u03b3\nis not necessarily one to one. This means that a specified value of the mean x = m may\ncorrespond to several values of \u03b3, and conversely a specified value of \u03b3 may give several\ndifferent means x. This can be illustrated through numerical examples.\nLemma 5 Partition functions Z\u03be+1 (\u03b3, m) and Z\u2212\u03be (\u03b3, m) are convex functions of \u03b3.\n\nSolutions\nThe solutions to the maximization of R\u00e9nyi Q-entropy subject to the classical mean\nconstraint (C) m = EP1 [X] and the generalized mean constraint (G) m = EP \u2217 [X] are\n\n\ffound using standard Lagrangian techniques The optimum solution, see for instance\n[11], is a saddle point of the Lagrangian and we may proceed in two steps: first minimize\nthe Lagrangian in P (x), and thus obtain a solution in terms of the Lagrange parameters,\nand then maximize the resulting Lagrangian, the dual function, in order to exhibit\nthe optimum Lagrange parameters. Taking into account the normalization conditions\ndescribed above, these solutions are easily derived and simplified:\n\n(C) PC (x) =\n\n[\u03b3(x \u2212 x) + 1]\u03be\nQ(x), with x = EPC [X] = E\u03be [X]\nZ\u03be (\u03b3, x)\n\n(1 + \u03b3(x \u2212 x))\u2212\u03be\n(G) PG (x) =\nQ(x) with x = EPG [X] = E\u2212(\u03be+1) [X]\nZ\u2212\u03be (\u03b3, x)\n\n(13)\n(14)\n\n1\n, and Z\u03bd (\u03b3, x) is the partition function. It is important to emphasize that x\nwhere \u03be = \u03b1\u22121\nin (13) is the statistical mean with respect to PC (x), x in (14) is the generalized \u03b1-mean\nwith respect to PG (x), and as such a function of \u03b3. It is a common mistake in the large\nmajority of reported results and calculations to improperly take for x the fixed value m\nof the constaint, which is only correct for the optimum value of the Lagrange parameter.\nThese optimum distributions appear to be self-referential, since their expressions\ninvolve their statistical mean. Therefore, the direct determination of their parameters\nis difficult, if not intractable.\n\nAlternate dual functions\nFrom the Lagrangian theory, one should maximize the dual function in order\nto obtain the remaining Lagrange parameter. But in the present cases, the dual\nfunctions are implicitely defined. Thus, in order to identify the value of the natural\nparameter associated to the mean constraints, I propose two 'alternate' (but effectively\ncomputable) dual functions, whose numerical maximizations enable to exhibit the\noptimum parameters.\nFor the classical mean, I just sketch the procedure. At the optimum, we have D(\u03b3 \u2217 ) =\n\u2217\ne\nsup\u03b3 sup\u03bc inf P L(P, \u03b3, \u03bc). For any value \u03bc\ne of \u03bc, letting D(\u03b3)\n= L(P\u03b3,e\ne), we have\n\u03bc , \u03b3, \u03bc\n\u2217\n\u2217\n\u2217\n\u2217\n\u2217\ne\ne ) = D(\u03b3 ) for the optimum \u03b3 , then D(\u03b3\ne ) will be a\nD(\u03b3 ) \u2265 D(\u03b3).\nThus, if D(\u03b3\ne\nmaximum of D(\u03b3)\nand the maximization of the dual function can be carried equivalently\ne\ne \u2217 ) = D(\u03b3 \u2217 ) is achieved with \u03bc\nvia the maximization of D(\u03b3).\nCondition D(\u03b3\ne(\u03b3) =\n\u2212 (\u03be + 1) (1 \u2212 \u03b3m) . Then, after some algebra, we obtain the very simple form\ne C (\u03b3) = \u2212 log Z\u03be+1 (\u03b3, m)\nD\n\n(15)\n\nthat is simply the expression of the divergence from P\u03be# to Q, D\u03b1 (P\u03be# ||Q). We know that\nZ\u03be+1(\u03b3, m) is a convex function. Thus, if Z\u03be+1(\u03b3, m) is defined on a continuous domain,\ne C (\u03b3) has an only maximum for \u03b3 = \u03b3 \u2217 . If Z\u03be+1 (\u03b3, m) is defined (and convex) on\nD\ne C (\u03b3) may have a maximum on each of these intervals, and one has to\nseveral intervals, D\n\n\fselect the minimum of these maxima (that is the maximum associated with the minimum\ndivergence). Hence, the identification of the optimum parameter \u03b3 \u2217 simply amounts to\nthe unconstrained maximization of an unimodal functional, possibly in several intervals.\nFor the generalized mean, the rationale for an alternate dual function is as follows.\n#\nWe know that D\u03b1 (P\u2212\u03be\n||Q) = \u2212 log Z\u2212\u03be (\u03b3, m) when the generalized mean constraint\nd log Z\n\n(\u03b3,m)\n\nZ\n\n(\u03b3,m)\n\n\u2212\u03be\nis satisfied. Since\n= \u2212\u03be (x \u2212 m) Z\u2212\u03be\u22121\n, \u2212 log Z\u2212\u03be (\u03b3, m) is maximum\nd\u03b3\n\u2212\u03be (\u03b3,m)\nwhen the constraint x = m is satisfied. Hence, the search of the optimum Lagrange\nparameter can be carried using the very simple alternate dual function\n\ne G (\u03b3) = \u2212 log Z\u2212\u03be (\u03b3, m).\nD\n\n(16)\n\nThe partition function Z\u2212\u03be (\u03b3, m) is a convex function for \u03b1 \u2264 1. If it is defined on a\ne G (\u03b3) has an only maximum that is simply reached for \u03b3 \u2217 such that\ncontinuous domain, D\nm = E\u2212\u03be\u22121 [x], the generalized \u03b1-mean. If the domain is given by several intervals, then\ne G (\u03b3) may present several maxima, and the minimum of these maxima, associated with\nD\n#\nthe minimum divergence D\u03b1 (P\u2212\u03be\n||Q), has to be selected. We thus obtain two practical\nnumerical schemes for the identification of the distributions parameters, and it is also\npossible to study the behaviour of entropies associated with some particular references\nQ. We come to a close to this presentation by considering the relationship between the\ntwo minimization problems and an underlying Legendre structure.\n\nDUALITY AND LEGENDRE STRUCTURE\nThe \u03b1 \u2194 1/\u03b1 duality\nThe dual functions associated to the two problems are \u2212 log Z\u03be1 +1 (\u03b3, m) and\n\u2212 log Z\u2212\u03be2 (\u03b3, m). Thus, we will have pointwise equality of dual functions, and of\ncourse of the optima, if \u03be1 + 1 = \u2212\u03be2 , that is if indexes \u03b11 and \u03b12 satisfy \u03b11 = 1/\u03b12 . We\ncan also remark that with \u2212\u03be2 = \u03be1 + 1 = \u03b11 \u03be1 , we have the following relations between\nthe two optimum probability density functions:\nPG =\n\nPG\u03b12 Q1\u2212\u03b12\nPC\u03b11 Q1\u2212\u03b11\nand\nP\n=\n, with \u03b12 = 1/\u03b11 ,\nC\n1\n2\nZ\u03be1\u2212\u03b1\nZ\u03be1\u2212\u03b1\n1\n1\n\n(17)\n\nand using the fact that Z\u03be1 +1 (\u03b3, m) = Z\u03be1 (\u03b3, m) for the optimum value of \u03b3. It means that\nPG is the escort distribution of PC with index \u03b11 and that PC is the escort distribution\nassociated with PG and index \u03b12 . It can be checked in the general case that always\nhave the equality D 1 (P \u2217 ||Q) = D\u03b1 (P1 ||Q) between the 1/\u03b1 R\u00e9nyi divergence of the\n\u03b1\nescort distribution to Q and the standard \u03b1 divergence Hence, the minimization of the\n\u03b1 R\u00e9nyi divergence subject to the generalized mean constraint is exactly equivalent to\nthe minimization of the 1/\u03b1 R\u00e9nyi divergence subject to the classical mean constraint\nso that generalized and classical mean constraints can always be swapped, provided the\nindex \u03b1 is changed into 1/\u03b1, as was argued in [12, 13].\n\n\fThe Legendre structure\nIn the study of alternative entropies, considerable efforts have been directed to the\nanalysis of associated thermodynamics. The concave entropies corresponding to our two\n\u03bb\nproblems are SC = log Z\u03be+1 (\u2212 (\u03be+1)\n, x), and SG = log Z\u2212\u03be (\u03bb/\u03be, x). Let us consider the\ngeneral form S = log Z\u03bd+1 (\u03b3, x).\nIn terms of the Lagrange multiplier \u03bb, it can be shown that\ndS dS d\u03b3\ndx\n=\n= \u2212\u03b3 (\u03bd + 1) .\nd\u03bb\nd\u03b3 d\u03bb\nd\u03bb\n\n(18)\n\nSpecializing the result to the two entropies, we obtain in both cases the Euler formula:\ndx\ndS\n=\u03bb .\nd\u03bb\nd\u03bb\n\n(19)\n\nNext, the derivative of the entropy with respect to the mean is simply\ndS\ndS d\u03bb\ndx d\u03bb\n=\n=\u03bb\n= \u03bb.\ndx\nd\u03bb dx\nd\u03bb dx\n\n(20)\n\nLet us now introduce the Massieu potential \u03c6(\u03bb) = S \u2212 \u03bbx (or equivalently the free\nenergy). Derivations with respect to the Lagrange parameter and to the mean give\nd\u03c6\nd\u03bb\nd\u03c6\n= \u2212x, and\n= \u2212x .\nd\u03bb\ndx\ndx\n\n(21)\n\nThese four relations show that S and \u03c6 are conjugated with variables x and \u03bb : S [x] \u21cb \u03c6\n[\u03bb] , so that the basic Legendre structure of thermodynamics is preserved (but care must\nbe taken for interpretations, for instance a valid definition of temperature requires that \u03bb\nalways remains positive).\n\nREFERENCES\n1.\n2.\n3.\n4.\n5.\n6.\n7.\n\nC. Tsallis, \"Nonextensive statistics: Theoretical, experimental and computational evidences and\nconnections,\" Brazilian Journal of Physics, vol. 29, pp. 1\u201335, March 1999.\nC. Tsallis, \"Entropic nonextensivity: a possible measure of complexity,\" Chaos, Solitons,& Fractals,\nvol. 13, pp. 371\u2013391, 2002.\nC. Tsallis, \"Possible generalization of Boltzmann-Gibbs statistics,\" Journal of Statistical Physics,\nvol. 52, no. 1-2, pp. 479\u2013487, 1988.\nB. R. La Cour and W. C. Schieve, \"Tsallis maximum entropy principle and the law of large numbers,\"\nPhys. Rev. E, vol. 62, pp. 7494 \u2013 7496, November 2000.\nJ. Grendar, M. and M. Grendar, \"Maximum entropy method with non-linear moment constraints:\nchallenges,\" in Bayesian inference and maximum entropy methods in science and engineering\n(G. Erickson, ed.), AIP, 2004.\nC. Beck, \"Generalized statistical mechanics of cosmic rays,\" Physica A, vol. 331, pp. 173\u2013181,\njanuary 2004.\nG. Wilk and Z. Wodarczyk, \"Interpretation of the nonextensitivity parameter q in some applications\nof Tsallis statistics and L\u00e9vy distributions,\" Physical Review Letters, vol. 84, pp. 2770\u20132773, March\n2000.\n\n\f8.\n9.\n10.\n11.\n12.\n13.\n\nE. T. Jaynes, Statistical Physics, ch. Information Theory and Statistical Mechanics, pp. 181\u2013218.\nBenjamin, New York, 1963.\nS. Kullback, Information Theory and Statistics. Wiley, New York, 1959.\nC. Beck and F. Schloegl, Thermodynamics of Chaotic Systems. Cambridge University Press, 1993.\nS. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 1st ed., March\n2004. ISBN: 0521833787.\nG. A. Raggio, \"On equivalence of thermostatistical formalisms.\" http://arxiv.org/abs/condmat/9909161, 1999.\nJ. Naudts, \"Dual description of nonextensive ensembles,\" Chaos, Solitons, and Fractals, vol. 13,\nno. 3, pp. 445\u2013450, 2002.\n\n\f"}