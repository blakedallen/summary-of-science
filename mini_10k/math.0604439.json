{"id": "http://arxiv.org/abs/math/0604439v1", "guidislink": true, "updated": "2006-04-20T06:46:48Z", "updated_parsed": [2006, 4, 20, 6, 46, 48, 3, 110, 0], "published": "2006-04-20T06:46:48Z", "published_parsed": [2006, 4, 20, 6, 46, 48, 3, 110, 0], "title": "Regular variation in the branching random walk", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0604427%2Cmath%2F0604317%2Cmath%2F0604544%2Cmath%2F0604513%2Cmath%2F0604077%2Cmath%2F0604473%2Cmath%2F0604511%2Cmath%2F0604522%2Cmath%2F0604590%2Cmath%2F0604389%2Cmath%2F0604195%2Cmath%2F0604264%2Cmath%2F0604116%2Cmath%2F0604554%2Cmath%2F0604439%2Cmath%2F0604165%2Cmath%2F0604420%2Cmath%2F0604147%2Cmath%2F0604221%2Cmath%2F0604479%2Cmath%2F0604572%2Cmath%2F0604359%2Cmath%2F0604017%2Cmath%2F0604326%2Cmath%2F0604481%2Cmath%2F0604041%2Cmath%2F0604156%2Cmath%2F0604069%2Cmath%2F0604038%2Cmath%2F0604061%2Cmath%2F0604603%2Cmath%2F0604367%2Cmath%2F0604553%2Cmath%2F0604355%2Cmath%2F0604398%2Cmath%2F0604246%2Cmath%2F0604121%2Cmath%2F0604335%2Cmath%2F0604089%2Cmath%2F0604192%2Cmath%2F0604196%2Cmath%2F0604409%2Cmath%2F0604559%2Cmath%2F0604025%2Cmath%2F0604029%2Cmath%2F0604136%2Cmath%2F0604076%2Cmath%2F0604414%2Cmath%2F0604123%2Cmath%2F0604273%2Cmath%2F0604178%2Cmath%2F0604625%2Cmath%2F0604537%2Cmath%2F0604218%2Cmath%2F0604469%2Cmath%2F0604545%2Cmath%2F0604105%2Cmath%2F0604244%2Cmath%2F0604283%2Cmath%2F0604519%2Cmath%2F0604127%2Cmath%2F0604394%2Cmath%2F0604539%2Cmath%2F0604189%2Cmath%2F0604179%2Cmath%2F0604091%2Cmath%2F0604397%2Cmath%2F0604026%2Cmath%2F0604604%2Cmath%2F0604382%2Cmath%2F0604117%2Cmath%2F0604605%2Cmath%2F0604112%2Cmath%2F0604484%2Cmath%2F0604005%2Cmath%2F0604181%2Cmath%2F0604070%2Cmath%2F0604437%2Cmath%2F0604331%2Cmath%2F0604453%2Cmath%2F0604617%2Cmath%2F0604626%2Cmath%2F0604527%2Cmath%2F0604376%2Cmath%2F0604023%2Cmath%2F0604520%2Cmath%2F0604396%2Cmath%2F0604313%2Cmath%2F0604065%2Cmath%2F0604251%2Cmath%2F0604250%2Cmath%2F0604446%2Cmath%2F0604412%2Cmath%2F0604360%2Cmath%2F0604458%2Cmath%2F0604372%2Cmath%2F0604155%2Cmath%2F0604364%2Cmath%2F0604489%2Cmath%2F0604457%2Cmath%2F0604468&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Regular variation in the branching random walk"}, "summary": "Let $\\{\\mm_n, n=0,1,...\\}$ be the supercritical branching random walk\nstarting with one initial ancestor located at the origin of the real line. For\n$n=0,1,...$ let $W_n$ be the moment generating function of $\\mm_n$ normalized\nby its mean. Denote by $AW_n$ any of the following random variables: maximal\nfunction, square function, $L_1$ and a.s. limit $W$, $\\su |W-W_n|$, $\\su\n|W_{n+1}-W_n|$. Under mild moment restrictions and the assumption that\n$\\rP\\{W_1>x\\}$ regularly varies at $\\infty$ it is proved that $\\rP\\{AW_n>x\\}$\nregularly varies at $\\infty$ with the same exponent. All the proofs given are\nnon-analytic in the sense that these do not use Laplace-Stieltjes transforms.\nThe result on the tail behaviour of $W$ is established in two distinct ways.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0604427%2Cmath%2F0604317%2Cmath%2F0604544%2Cmath%2F0604513%2Cmath%2F0604077%2Cmath%2F0604473%2Cmath%2F0604511%2Cmath%2F0604522%2Cmath%2F0604590%2Cmath%2F0604389%2Cmath%2F0604195%2Cmath%2F0604264%2Cmath%2F0604116%2Cmath%2F0604554%2Cmath%2F0604439%2Cmath%2F0604165%2Cmath%2F0604420%2Cmath%2F0604147%2Cmath%2F0604221%2Cmath%2F0604479%2Cmath%2F0604572%2Cmath%2F0604359%2Cmath%2F0604017%2Cmath%2F0604326%2Cmath%2F0604481%2Cmath%2F0604041%2Cmath%2F0604156%2Cmath%2F0604069%2Cmath%2F0604038%2Cmath%2F0604061%2Cmath%2F0604603%2Cmath%2F0604367%2Cmath%2F0604553%2Cmath%2F0604355%2Cmath%2F0604398%2Cmath%2F0604246%2Cmath%2F0604121%2Cmath%2F0604335%2Cmath%2F0604089%2Cmath%2F0604192%2Cmath%2F0604196%2Cmath%2F0604409%2Cmath%2F0604559%2Cmath%2F0604025%2Cmath%2F0604029%2Cmath%2F0604136%2Cmath%2F0604076%2Cmath%2F0604414%2Cmath%2F0604123%2Cmath%2F0604273%2Cmath%2F0604178%2Cmath%2F0604625%2Cmath%2F0604537%2Cmath%2F0604218%2Cmath%2F0604469%2Cmath%2F0604545%2Cmath%2F0604105%2Cmath%2F0604244%2Cmath%2F0604283%2Cmath%2F0604519%2Cmath%2F0604127%2Cmath%2F0604394%2Cmath%2F0604539%2Cmath%2F0604189%2Cmath%2F0604179%2Cmath%2F0604091%2Cmath%2F0604397%2Cmath%2F0604026%2Cmath%2F0604604%2Cmath%2F0604382%2Cmath%2F0604117%2Cmath%2F0604605%2Cmath%2F0604112%2Cmath%2F0604484%2Cmath%2F0604005%2Cmath%2F0604181%2Cmath%2F0604070%2Cmath%2F0604437%2Cmath%2F0604331%2Cmath%2F0604453%2Cmath%2F0604617%2Cmath%2F0604626%2Cmath%2F0604527%2Cmath%2F0604376%2Cmath%2F0604023%2Cmath%2F0604520%2Cmath%2F0604396%2Cmath%2F0604313%2Cmath%2F0604065%2Cmath%2F0604251%2Cmath%2F0604250%2Cmath%2F0604446%2Cmath%2F0604412%2Cmath%2F0604360%2Cmath%2F0604458%2Cmath%2F0604372%2Cmath%2F0604155%2Cmath%2F0604364%2Cmath%2F0604489%2Cmath%2F0604457%2Cmath%2F0604468&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Let $\\{\\mm_n, n=0,1,...\\}$ be the supercritical branching random walk\nstarting with one initial ancestor located at the origin of the real line. For\n$n=0,1,...$ let $W_n$ be the moment generating function of $\\mm_n$ normalized\nby its mean. Denote by $AW_n$ any of the following random variables: maximal\nfunction, square function, $L_1$ and a.s. limit $W$, $\\su |W-W_n|$, $\\su\n|W_{n+1}-W_n|$. Under mild moment restrictions and the assumption that\n$\\rP\\{W_1>x\\}$ regularly varies at $\\infty$ it is proved that $\\rP\\{AW_n>x\\}$\nregularly varies at $\\infty$ with the same exponent. All the proofs given are\nnon-analytic in the sense that these do not use Laplace-Stieltjes transforms.\nThe result on the tail behaviour of $W$ is established in two distinct ways."}, "authors": ["Aleksander Iksanov", "Sergey Polotskiy"], "author_detail": {"name": "Sergey Polotskiy"}, "author": "Sergey Polotskiy", "arxiv_comment": "submitted", "links": [{"href": "http://arxiv.org/abs/math/0604439v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0604439v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "Primary: 60G42; 60J80; Secondary: 60E99", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0604439v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0604439v1", "journal_reference": null, "doi": null, "fulltext": "Regular variation in the branching random walk\n\narXiv:math/0604439v1 [math.PR] 20 Apr 2006\n\nAleksander Iksanov\u2217and Sergey Polotskiy\u2020\nFaculty of Cybernetics, National T.Shevchenko University of Kiev ,\n01033 Kiev, Ukraine\n\nAbstract\nLet {Mn , n = 0, 1, . . .} be the supercritical branching random walk starting with\none initial ancestor located at the origin of the real line. For n = 0, 1, . . . let Wn\nbe the moment generating function of Mn normalized by its mean. Denote by AWn\nany of the following random variables: maximal function, square function, L1 and a.s.\nlimit W , sup|W \u2212 Wn |, sup|Wn+1 \u2212 Wn |. Under mild moment restrictions and the\nn\u22650\n\nn\u22650\n\nassumption that P{W1 > x} regularly varies at \u221e it is proved that P{AWn > x}\nregularly varies at \u221e with the same exponent. All the proofs given are non-analytic\nin the sense that these do not use Laplace-Stieltjes transforms. The result on the tail\nbehaviour of W is established in two distinct ways.\nMSC: Primary: 60G42; 60J80; Secondary: 60E99\nKeywords: branching random walk; nonnegative martingale; maximal function;\nsquare function; regular variation; perpetuity\n\n1\n\nAn introduction, notation and results\n\nLet M be a point process on R, i.e. random, locally finite counting measure. Explicitly,\nJ(\u03c9)\n\nM(A)(\u03c9) :=\n\nX\n\n\u03b4Xi (\u03c9) (A),\n\ni=1\n\nwhere J := M(R), {Xi : i = 1, J } are the points of M, A is any Borel subset of R and \u03b4x\nis the Dirac measure concentrated at x. We assume that M has no atom at +\u221e, and the\nJ may be deterministic or random, finite or infinite with positive probability.\nLet {Mn , n = 0, 1, . . .} be the branching random walk (BRW), i.e. the sequence of\npoint processes which, for any Borel set B \u2286 R, are defined as follows: M0 (B) = \u03b40 (B),\nX\nMn+1 (B) :=\nMn,r (B \u2212 An,r ), n = 0, 1, . . . ,\nr\n\nwhere {An,r } are the points of Mn , and {Mn,r } are independent copies of M. More\ndetailed definition of the BRW can be found in, for example, [3, 17, 22].\n\u2217\n\u2020\n\ne-mail address: iksan@unicyb.kiev.ua\ne-mail address: pilot ser@mail.ru\n\n1\n\n\fIn the case when P{J < \u221e} = 1 we assume that EJ > 1. In the contrary case\nthe condition holds automatically. Thus we only consider the supercritical BRW. As a\nconsequence, P{Mn (R) > 0 for all n} > 0.\nIn what follows we use the notation that is generally accepted in the literature on the\nBRW: Au denotes the position on R of a genericPpoint u = i1 . . . in ; the record |u| = n\nmeans that the u is a point of Mn ; the symbol |u|=n denotes the summation over all\npoints of Mn ; Fn = \u03c3(M1 , . . . , Mn ) denotes the \u03c3-field generated by {Mk , k = 1, . . . , n};\nF0 is the trivial \u03c3-field.\nDefine the function\nZ\nX\neyAu \u2208 (0, \u221e], y \u2208 R,\nm(y) := E eyx M(dx) = E\nR\n\n|u|=1\n\nand assume that there exists a \u03b3 > 0 such that m(\u03b3) < \u221e. Set Yu := e\u03b3Au /m|u| (\u03b3) and\nZ\nX\n\u2212n\nWn := m(\u03b3)\ne\u03b3x Mn (dx) =\nYu .\nR\n\n|u|=n\n\nAs is well-known (see, for example, [12]), the sequence {(Wn , Fn ), n = 0, 1, . . .} is a nonnegative martingale. Notice that W0 = EWn = 1.\nLet {dn , n = 1, 2, . . .} be the martingale difference sequence, i.e.\nWn = 1 +\n\nn\nX\n\ndk , n = 1, 2, . . .\n\nk=1\n\nThe square function S and maximal function W \u2217 are defined by\n!1/2\n\u221e\nX\nand W \u2217 := supWn .\nd2k\nS := 1 +\nn\u22650\n\nk=1\n\nSet also\nSn :=\n\n1+\n\nn\nX\nk=1\n\nd2k\n\n!1/2\n\n, n = 1, 2, . . . and \u2206 := sup|dn |.\nn\u22651\n\nRecall that since Wn is a non-negative martingale all the defined variables are a.s. finite\n(for finiteness of S for general L1 -bounded martingales we refer to [1] or to Theorem 2 on\np.390 [11]).\nWhen the martingale Wn is uniformly integrable, we denote by W\u221e = W its L1 and\na.s. limit, and then define\nM := sup|W \u2212 Wn | = sup|\nn\u22650\n\nn\u22650\n\n\u221e\nX\n\nk=n+1\n\ndk |.\n\nLemma 1 [21] (see also [2] for a slightly different proof in the case J < \u221e a.s.) states\nthat there exist r \u2208 (0, 1) and \u03b8 = \u03b8(r) > 1 such that whenever t > 1\nP{W > t} \u2264 P{W \u2217 > t} \u2264 \u03b8P{W > rt}.\n2\n\n(1)\n\n\f\u2217\nThis suggests that the\nPntail behaviours of W and W are quite similar.\nLet now {fn := k=0 gk , n = 0, 1, . . .} be any martingale. P\nIt is well-known that the\n2 1/2 functions are\ndistributions of maximal f \u2217 := sup|fn | and square S(f ) := ( \u221e\nk=0 gk )\nn\u22650\n\nclose in many respects. The evidence in favor of such a statement is provided by, for\nexample, the (moment) Burkholder-Gundy-Davis inequality (Theorem 1.1 [10]) or the\ndistribution function inequalities like (34) of this paper. From [9] and [10] and many other\nsubsequent works it follows that there exist a subset H of the set of all martingales and a\nclass A of operators on martingales such that the distributions of A1 h and A2 h are close\nin an appropriate sense whenever Ai \u2208 A and h \u2208 H. Often can it be possible to express\nthis closeness via moment or distribution function inequalities like those mentioned above.\nKeeping this in mind, it would not be an unrealistic conjecture that the regular variation\nof P{A1 h > x} is equivalent to that of P{A2 h > x}, where Ai and h belong to some\nsubsets of operators and martingales respectively that may be different from A and H.\nOn the other hand, let us notice that as far as we know the conjecture does not follow\nfrom previously known results on martingales.\nThe aim of this paper is to prove a variant of the conjecture for the martingales Wn and\noperators Ai , i = 1, 5 given as follows: A1 W = W \u2217 , A2 W = \u2206, A3 W = S, A4 W = W\u221e ,\nA5 W = M .\nIn addition to the notation introduced above, other frequently used notation and conventions include: L(t) denotes a function that slowly varies at infinity; 1A denotes the inf (t)\ndicator function of the set A; f (t) \u223c g(t) is abbreviation of the limit relation lim\n= 1;\nt\u2192\u221e g(t)\nx+ := max(x, 0); x \u2227 y = min(x, y); x \u2228 y = max(x, y); we write Pn {*} instead of P{*|Fn },\nand En {*} instead of E{*|Fn }; the record \"const\" denotes a constant whose value is of no\nimportance and may be different on different appearances.\nNow we are ready to state our result.\nTheorem 1.1. Assume that there exist \u03b2 > 1 and \u01eb > 0 such that\nX\nX\nk\u03b2 := E\nYu\u03b2 < 1, E\nYu\u03b2+\u01eb < \u221e and\n|u|=1\n\n(2)\n\n|u|=1\n\nP{W1 > x} \u223c x\u2212\u03b2 L(x).\n\n(3)\n\nThen\n(I) P{W \u2217 > x} \u223c P{\u2206 > x} \u223c P{S > x} \u223c (1 \u2212 k\u03b2 )\u22121 P{W1 > x};\n(II) Wn converges almost surely and in mean to a random variable W and\nP{W > x} \u223c (1 \u2212 k\u03b2 )\u22121 P{W1 > x};\n\n(4)\n\nP{M > x} \u223c (1 \u2212 k\u03b2 )\u22121 P{W1 > x}.\nRemark 1.1. We are not aware of any papers on branching processes which investigate the\ntail behaviour of random variables like \u2206, M or S. [21] is the only paper we know of that\ndeals with the tail behaviour of random variables like W \u2217 .\n3\n\n\fRemark 1.2. When \u03b3 = 0 and J < \u221e a.s., Wn reduces to the (supercritical) normalized\nGalton-Watson process. In this case (4) was proved in [5] for non-integer \u03b2 and in [13]\nfor integer \u03b2. When \u03b3 > 0, J < \u221e a.s. and M(\u2212\u221e, \u2212\u03b3 \u22121 log m(\u03b3)) = 0 a.s., W can be\nviewed as a limit random variable in the Crump-Mode branching process. In this case (4)\nwas established in [6] for non-integer \u03b2. The technique used in the last three cited works\nis purely analytic (based on using the Laplace-Stieltjes transforms and Abel-Tauberian\ntheorems) and completely different from ours. On the other hand, let us notice that the\nabove mentioned analytic approach was successfully employed and further developed by\nthe second-named author. In 2003, in an unpublished diploma paper he proved (4) for\nnon-integer \u03b2 for the general case treated here.\nOur desire to find a non-analytic proof of (4) was a starting point for the development\nof this paper. In the course of writing two different (non-analytic) proofs were found.\nOne of these proofs given in Section 2 falls within the general scope of the paper. The\nsecond given in Section 3 continues a line of research initiated in [17], [22], [18]. Here an\nunderlying idea is that the martingale Wn and so called perpetuities have many features\nin common. In particular, several non-trivial results on perpetuities (however, it seems,\nonly those related to perpetuities with not all moments finite) can be effectively exploited\nto obtain similar results on the limiting behaviour of Wn . Maybe we should recall that,\nin modern probability, by a perpetuity is meant a random variable\nB1 +\n\n\u221e\nX\nk=2\n\nA1 A2 * * * Ak\u22121 Bk ,\n\nprovided the latter series absolutely converges, and where {(Ak , Bk ) : k = 1, 2, . . .} are\nindependent identically distributed random vectors.\nThe paper is structured as follows. In Section 2 we prove Theorem 1.1. Here an\nessential observation is that, given Fn , Wn+1 looks like a weighted sum of independent\nidentically distributed random variables. This allows us to exploit the well-known result\n[25] on the tail behaviour of such sums under the regular variation assumption. The second\nkey ingredient of the proof is using the distribution function inequalities for martingales.\nIn Section 3 we give another proof of (4) which rests on a relation between the BRW and\nperpetuities. Here availability of Grincevi\u010dius-Grey [16] result on the tail behaviour of\nperpetuities is crucial. Finally, in Section 4 we discuss applicability of Theorem 1.1 to\nseveral classes of point processes. The section closes with two remarks which show that\n(2) and (3) are not necessary conditions for the regular variation of the tails of W \u2217 , W\nand a related random variable.\n\n2\n\nProof of Theorem 1.1\n\n(I) We will prove the result for W \u2217 and \u2206 simultaneously. To this end, let Q and Q\u0303\nbe independent identically distributed random variables whose distribution is supported\nby (a, \u221e), a > \u2212\u221e. Assume that P{Q > x} \u223c x\u2212\u03b2 L(x) for \u03b2 > 1. In particular, this\nassumption ensures that E|Q| < \u221e and P{|Q| > x} \u223c P{Q > x}. With a slight abuse of\n4\n\n\fnotation, set Qs := |Q| \u2212 |Q\u0303|. Then\n1 \u2212 F (x) := P{|Qs | > x} \u223c 2x\u2212\u03b2 L(x).\n\n(5)\n\nR\u221e\nIndeed, 1 \u2212 F (x) = 2 0 (1 \u2212 G(x + y))dG(y), where G(x) = P{|Q| \u2264 x}, x \u2265 0. Now\n(5) follows from monotonicity of 1 \u2212 G, the relation 1 \u2212 G(x + y) \u223c 1 \u2212 G(x), y \u2208 R and\nFatou's lemma.\nThe equality\nX\nEt(Z) = E\nYu t(Yu ),\n|u|=1\n\nwhich is assumed to hold for all bounded Borel functions t, defines the distribution of a\nrandom variable Z. More generally,\nX\nEt(Z1 * * * Zn ) = E\nYu t(Yu ),\n(6)\n|u|=n\n\nwhere Z1 , Z2 , . . . are independent copies of the Z. Notice that we can permit for (6) to\nhold for any Borel function t. In that case we assume that if the right-hand side is infinite\nor does not exist, the same is true for the left-hand side.\nP\nUnder the assumptions of the theorem, the function kx := E |u|=1 Yux is log-convex\nfor x \u2208 (1, \u03b2), k1 = 1 and k\u03b2 < 1. Therefore,\nk\u03b2\u2212\u01eb < 1 for all \u01eb \u2208 (0, \u03b2 \u2212 1).\n\n(7)\n\nAlso we can pick a \u03b4 \u2208 (0, \u03b2 \u2212 1) such that k\u03b2+\u03b4 < 1. By using these facts and equality\n(6) we conclude that with this \u03b4\nX\nX\nn\nn\n< 1.\n(8)\n< 1 and E\nYu\u03b2+\u03b4 = k\u03b2+\u03b4\nE\nYu\u03b2\u2212\u03b4 = k\u03b2\u2212\u03b4\n|u|=n\n\n|u|=n\n\nLet us notice, for later needs, that we can choose \u03b4 as small as needed. Among other\nthings, (8) implies that for x \u2208 [1, \u03b2 + \u03b4]\nX\nYux < \u221e a.s.\n(9)\n|u|=n\n\nUntil further notice, we fix an arbitrary n \u2208 N. Put\nX\nX\nTn := |\nYu Qu | and Xn :=\nYu |Qu |.\n|u|=n\n\n|u|=n\n\nGiven Fn , let {Qu : |u| = n} and {Qsu : |u| = n} be conditionally independent copies of\nthe random variables Q and Qs respectively. In view of (9), an appeal to Lemma A3.7[25]\nallows us to conclude that\nX\nPn {Tn > x} \u223c\nYu\u03b2 P{|Q| > x} a.s.\n(10)\n|u|=n\n\n5\n\n\fThe cited lemma assumes that each term of the series on the left-hand side has zero mean,\nbut this condition is not needed in the proof of the result used here.\nFn\nn\nDenote by \u03bcF\nn the conditional on Fn median of Xn , i.e. \u03bcn is a random variable that\nsatisfies\nFn\nn\nPn {Xn \u2212 \u03bcF\nn \u2265 0} \u2265 1/2 \u2264 Pn {Xn \u2212 \u03bcn \u2264 0} a.s.\n\nn\nLet also \u03bcn denote the usual median of Xn . Since \u03bcF\nn \u2265 0 a.s., we have from (10)\n\nlim sup\nx\u2192\u221e\n\nn\nX\nPn {Tn > x + \u03bcF\nn }\n\u2264\nYu\u03b2 a.s.\nP{|Q| > x}\n\n|u|=n\n\nIf we could prove that for large x\nn\nPn {Tn > x + \u03bcF\nn }\n\u2264 Un a.s.\nP{|Q| > x}\n\nand EUn < \u221e,\n\n(11)\n\nwhere Un is a random variable, then using Fatou's lemma yielded\nlim supE\nx\u2192\u221e\n\nn\nX\nPn {Tn > x + \u03bcF\nP{Tn > x + \u03bcn }\n(6)\nn }\n= lim sup\n\u2264E\nYu\u03b2 = k\u03b2n .\nP{|Q| > x}\nP{|Q| > x}\nx\u2192\u221e\n\n(12)\n\n|u|=n\n\nSince P{|Q| > x + \u03bcn } \u223c P{|Q| > x}, (12) implied that\nlim sup\nx\u2192\u221e\n\nP{Tn > x}\n\u2264 k\u03b2n .\nP{|Q| > x}\n\nOn the other hand, by using (10) and Fatou's lemma the reverse inequality for the lower\nlimit follows easily. Therefore, as soon as (11) is established, we get\nP{Tn > x} \u223c k\u03b2n P{|Q| > x}.\n\n(13)\n\nWe now intend to show that (11) holds with\n\uf8eb\n\uf8f6\nX\nX\nYu\u03b2\u2212\u03b4 +\nYu\u03b2+\u03b4 \uf8f8 ,\nUn = const \uf8ed\n|u|=n\n\n(14)\n\n|u|=n\n\nfor appropriate small \u03b4 that satisfies (8). Notice that\n\nn\nn\n) < \u221e.\n+ k\u03b2+\u03b4\nEUn = const(k\u03b2\u2212\u03b4\n\n(15)\n\nBy the triangle inequality and conditional symmetrization inequality\nX\nFn\nn\nYu Qsu | > x}.\n(1/2)Pn {Tn > x + \u03bcF\nn } \u2264 (1/2)Pn {Xn > x + \u03bcn } \u2264 Pn {|\n|u|=n\n\nLet us show that for x > 0\nPn {|\n\nX\n\n|u|=n\n\nYu Qsu | > x} \u2264\n6\n\n(16)\n\n\f\uf8eb\n\n\u2264 Pn { sup Yu |Qsu | > x} + x\u22122 En \uf8ed\n|u|=n\n\nX\n\n|u|=n\n\n\uf8f6\n\nYu2 (Qsu )2 1{Yu |Qsu |\u2264x} \uf8f8 := I1 (n, x) + I2 (n, x). (17)\n\nLet {Y (k)Qs (k) : k = 1, 2, . . .} be any enumeration\nof the set {Yu Qsu : |u| = n}. The inP\nequality E|Q| < \u221e implies that the series |u|=n Yu Qu is absolutely convergent. Therefore\nP\nP\u221e\ns\n|u|=n Yu Qu =\nk=1 Y (k)Q (k). Define\n\u03c4x :=\n\n\uf8f1\n\uf8f2inf{k \u2265 1 : Y (k)|Qs (k)| > x}, if\n\nsupY (k)|Qs (k)| > x;\nk\u22651\n\n\uf8f3+\u221e, otherwise .\n\nFor any fixed m \u2208 N and x > 0\n\nPn {|\n\nm\nX\nk=1\n\nY (k)Qs (k)| > x} \u2264\n\n\u2264 Pn {\u03c4x \u2264 m \u2212 1} + Pn {|\n\u2264 Pn {\n\nsup\n\n1\u2264k\u2264m\u22121\n\nm\nX\nk=1\n\nY (k)Qs (k)| > x, \u03c4x \u2265 m} \u2264\n\nY (k)|Qs (k)| > x} + Pn {|\n\n(by Markov inequality)\n\u2264 Pn {\n\ns\n\nsup\n\nY (k)|Q (k)| > x} + x\n\n1\u2264k\u2264m\u22121\n\n\u22122\n\nEn\n\n\u03c4X\nx \u2227m\nk=1\n\nm\nX\n\nY (k)Qs (k)| > x} \u2264\n\ns\n\nY (k)Q (k)1{\u03c4x \u2265k}\n\nk=1\n\n!2\n\n\u2264\n\n(En Qs (k) = 0 and, given Fn , Qs (k) and 1{\u03c4x \u2265k} are independent)\n\u2264 Pn {\n\nsup\n\n1\u2264k\u2264m\u22121\n\nY (k)|Qs (k)| > x} + x\u22122 En\n\nm\nX\n\nY 2 (k)(Qs (k))2 1{Y (k)|Qs (k)|\u2264x} .\n\nk=1\n\nIf the distribution of Qs is continuous, sending m \u2192 \u221e then completes the proof of (17).\nAssume now that the distribution of Qs has atoms. Let R be a random variable with\nuniform distribution on [\u22121, 1], which is independent of Qs . Given Fn , let {Ru : |u| = n}\nbe conditionally independent copies of R which are also independent of {Qsu : |u| = n}.\nSince for all t > 0\nP{|Qs | > t} \u2264 2P{|Qs ||R| > t/2},\nwe have by Theorem 3.2.1[23]\nX\nX\nPn {|\nYu Qsu | > t} \u2264 4Pn {|\nYu Qsu Ru | > t/4},\n|u|=n\n\n|u|=n\n\n7\n\n(18)\n\n\fand the distribution of Qs R is (absolutely) continuous. Now we can apply the already\nestablished part of (17) to the right-hand side of (18). Strictly speaking, when the distribution of Qs has atoms, (17) should be written in a modified form: additional constants\nshould be added, and Qsu should be replaced with Qsu Ru . On the other hand, a perusal of\nthe subsequent proof reveals that only the regular variation of P{|Qs | > x} plays a crucial\nrole. Therefore, for ease of notation we prefer to keep (17) in its present form. This does\nnot cause any mistakes as P{|Qs R| > x} \u223c E|R|\u03b2 P{|Qs | > x}.\nAssume\nthat 1 \u2212 F (x) regularly varies with index \u2212\u03b2, \u03b2 \u2208 (1, 2). Set\nR x temporarily\n2\nT (x) := 0 y dF (y). By Theorem 1.6.4 [7]\nT (x) \u223c\n\n\u03b2\n\u03b2\nx2 (1 \u2212 F (x)) \u223c\nx2\u2212\u03b2 L1 (x).\n2\u2212\u03b2\n2\u2212\u03b2\n\nAlso by Theorem 1.5.3 [7] there exists a non-decreasing S(x) such that\nT (x) \u223c S(x).\n\n(19)\n\nFor any Ai > 0 and \u03b4 defined in (8) there exists an xi > 0 such that whenever x \u2265 xi , i =\n1, 2, 3\nx\u03b2+\u03b4 (1 \u2212 F (x)) \u2265 1/A1 ;\n(20)\nx\u03b2\u22122+\u03b4 S(x) \u2265 1/A2 ;\nT (x) \u2264 (A3 +\n\n\u03b2\n)x2 (1 \u2212 F (x)) := Bx2 (1 \u2212 F (x)).\n2\u2212\u03b2\n\n(21)\n(22)\n\nAlso for any Ai > 1 and the same \u03b4 as above there exists an xi > 0 such that whenever\nx \u2265 xi and ux \u2265 xi , i = 4, 5, 6\n1 \u2212 F (ux)\n\u2264 A4 (u\u2212\u03b2+\u03b4 \u2228 u\u2212\u03b2\u2212\u03b4 );\n1 \u2212 F (x)\n\n(23)\n\nT (ux)\n\u2264 A5 (u2\u2212\u03b2+\u03b4 \u2228 u2\u2212\u03b2\u2212\u03b4 );\nT (x)\n\n(24)\n\nT (ux)\nS(ux)\n\u2264 A6\n.\nT (x)\nS(x)\n\n(25)\n\n(23) and (24) follows from Potter's bound (Theorem 1.5.6 (iii)[7]); (25) is implied by (19).\nSet x0 := max xi and assume that x0 > 1.\n1\u2264i\u22646\n\nTo check (11) and (14), we consider three cases: (a) \u03b2 \u2208 (1, 2); (b) \u03b2 > 2, \u03b2 6= 2n , n \u2208 N;\n(c) \u03b2 = 2n , n \u2208 N.\n(a) For any fixed x \u2265 x0 and y > 0\nX Pn {Yu |Qs | > x/y}\nX 1 \u2212 F (x(yYu )\u22121 )\nI1 (n, x/y)\nu\n\u2264\n=\n=\n1 \u2212 F (x)\n1 \u2212 F (x)\n1 \u2212 F (x)\n|u|=n\n\n|u|=n\n\n8\n\n\f=\n\nX\n\n|u|=n\n\n* * * 1{yYu >x/x0 } +\n\nX\n\n|u|=n\n\n* * * 1{yYu \u2264x/x0 } =: I11 (n, x, y) + I12 (n, x, y).\n\nSince\n(yYu )\u03b2+\u03b4 \u2265 (yYu )\u03b2+\u03b4 1{yYu >x/x0 } \u2265 (x/x0 )\u03b2+\u03b4 1{yYu >x/x0 } ,\nwe get\n\u03b2+\u03b4\n\nI11 (n, x, y) \u2264 (x0 y)\n\n\u03b2+\u03b4\n(20)\n|u|=n Yu\n\u2264\nx\u03b2+\u03b4 (1 \u2212 F (x))\n\nP\n\nA1 x0\u03b2+\u03b4 y \u03b2+\u03b4\n\nX\n\nYu\u03b2+\u03b4 .\n\n|u|=n\n\nFurther\n(23)\n\nI12 (n, x, y) \u2264 A4\n\nX\n\n\uf8eb\n\n(yYu )\u03b2\u2212\u03b4 \u2228 (yYu )\u03b2+\u03b4 \u2264 A4 \uf8edy \u03b2\u2212\u03b4\n\n|u|=n\n\nX\n\nYu\u03b2\u2212\u03b4 + y \u03b2+\u03b4\n\nX\n\n|u|=n\n\n|u|=n\n\n\uf8f6\n\nYu\u03b2+\u03b4 \uf8f8 ;\n\nR\n\u22121\nX Yu2 x(yYu ) z 2 dF (z) (22)\nX Y 2 T (x(yYu )\u22121 )\nI2 (n, x/y)\nu\n2\n2\n0\n=y\n=\n\u2264\nBy\n1 \u2212 F (x)\nx2 (1 \u2212 F (x))\nT (x)\n|u|=n\n\n\uf8eb\n\n= By 2 \uf8ed\n\nX\n\n|u|=n\n\n|u|=n\n\n* * * 1{yYu >x/x0 } +\n\nX\n\n|u|=n\n\n\uf8f6\n\n* * * 1{yYu \u2264x/x0 } \uf8f8 =: By 2 (I21 (n, x, y) + I22 (n, x, y)).\n\nX Y 2 S(x(yYu )\u22121 )\n(25)\nu\n1{yYu >x/x0 } \u2264\nI21 (n, x, y) \u2264 A6\nS(x)\n|u|=n\n\n\u03b2\u22122+\u03b4\n\n\u2264 A6 (x0 y)\n\nS(x0 )\n\nX\n\n|u|=n\n\nX\n(21)\nYu\u03b2+\u03b4\n\u03b2\u22122+\u03b4\n\u2264\nA\nA\n(x\ny)\nS(x\n)\nYu\u03b2+\u03b4 ;\n2\n6\n0\n0\nx\u03b2\u22122+\u03b4 S(x)\n|u|=n\n\nX\n(24)\nYu2 ((yYu )\u03b2\u22122\u2212\u03b4 \u2228 (yYu )\u03b2\u22122+\u03b4 ) \u2264\nI22 (n, x, y) \u2264 A5\n|u|=n\n\n\uf8eb\n\n\u2264 A5 \uf8edy \u03b2\u22122\u2212\u03b4\n\nX\n\nYu\u03b2\u2212\u03b4 + y \u03b2\u22122+\u03b4\n\nX\n\n|u|=n\n\n|u|=n\n\n\uf8f6\n\nYu\u03b2+\u03b4 \uf8f8 .\n\nThus according to (17) we have proved that for x \u2265 x0 and y > 0\n\uf8f6\n\uf8eb\nP\nX\nX\nPn {| |u|=n Yu Qsu | > x/y}\nYu\u03b2+\u03b4 \uf8f8 .\nYu\u03b2\u2212\u03b4 + y \u03b2+\u03b4\n\u2264 const \uf8edy \u03b2\u2212\u03b4\nP{|Qs | > x}\n|u|=n\n\n(26)\n\n|u|=n\n\nIn particular, since P{|Qs | > x} \u223c 2P{|Q| > x} then setting in (26) y = 1 and using (16)\nleads to (11) with Un being a multiple of the right-hand side of (26).\nIn the remaining cases we only investigate the situation when \u03b2 \u2208 (2, 4) and \u03b2 = 2.\nFor other values of \u03b2 the inequality (11) with Un satisfying (14) follows by induction.\n9\n\n\f(b) \u03b2 \u2208 (2, 4). Given Fn , let {\u00d1 , Nu : |u| = n} be conditionally independent copies of\na random variable N with normal (0, 1) distribution. Using the approach similar to that\nexploited to obtain (18)(this fruitful argument has come to our attention from [25]) allows\nus to conclude that for x > 0 and appropriate positive constants c1 , c2\nX\nPn {|\nYu Qsu | > x} \u2264\n|u|=n\n\n\u2264 c1 Pn {|\n\nX\n\n|u|=n\n\nYu Nu Qsu | > c2 x} = c1 Pn\n\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\n\uf8eb\n\n|\u00d1 | \uf8ed\n\nX\n\n|u|=n\n\n\uf8f61/2\n\nYu2 (Qsu )2 \uf8f8\n\n> c2 x\n\n\uf8fc\n\uf8f4\n\uf8fd\n\n.\n\n(27)\n\n\uf8f4\n\uf8fe\n\nNotice that P{(Qs )2 > x} regularly varies with index \u2212\u03b2/2 \u2208 (\u22122, \u22121). Also it is obvious\nthat, if needed, we can reduce \u03b4 in (8) to ensure that k\u03b2\u22122\u03b4 < 1 and k\u03b2+2\u03b4 < 1. Therefore\nwe can use (26) with Yu replaced with Yu2 , and Qsu replaced with (Qsu )2 which gives after\n1/2\na little manipulation that for x \u2265 x0\n\uf8eb\n\uf8f6\nP\nX\nX\nPn {| |u|=n Yu Qsu | > x}\nYu\u03b2+2\u03b4 E|N |\u03b2+2\u03b4 +\nYu\u03b2\u22122\u03b4 E|N |\u03b2\u22122\u03b4 \uf8f8 .\n\u2264 const \uf8ed\nP{|Qs | > x}\n|u|=n\n\n|u|=n\n\nBy using the same argument as in the case (a) we can check that (11) and (14) have been\nproved.\n(c) \u03b2 = 2. In the same manner as we have established (26) it can be proved that for\ny > 0 and large x\n\uf8f6\n\uf8eb\nP\nX\nX\nPn { |u|=n Yu2 (Qsu )2 > x/y}\nYu2+2\u03b4 \uf8f8 .\nYu2\u22122\u03b4 + y 2+2\u03b4\n\u2264 const \uf8edy 2\u22122\u03b4\nP{(Qs )2 > x}\n|u|=n\n\n|u|=n\n\nHence an appeal to (27) assures that (11) and (14) hold in this case too.\nWe have a representation\nX\n(u)\nWn+1 =\nY u W1 ,\n\n(28)\n\n|u|=n\n\n(u)\n\nwhere, given Fn , W1 are (conditionally) independent copies of W1 . Each element of the\n(u)\nset {W1 : |u| = n} is constructed in the same way as W1 , the only exception being that\n(u)\nwhile W1 is defined on the whole family tree, W1 is defined on the subtree with root u.\nWe only give a complete proof for the \u2206. The analysis of the W \u2217 is similar but\nsimpler, and hence omitted. From (28) we conclude that |dn+1 | is the same as Tn with\n(u)\nQu = W1 \u2212 1. Hence by (10)\nX\nX\n1{ max |dk |\u2264x} Pn {|dn+1 | > x} \u223c\nYu\u03b2 P{|W1 \u2212 1| > x} \u223c\nYu\u03b2 P{W1 > x} a.s., (29)\n1\u2264k\u2264n\n\n|u|=n\n\n|u|=n\n\n10\n\n\fand by (13)\nP{|dn+1 | > x} \u223c k\u03b2n P{|W1 \u2212 1| > x} \u223c k\u03b2n P{W1 > x}.\n\n(30)\n\nRecall that\nP{\u2206 > x} = P{|d1 | > x} +\n= P{|d1 | > x} + E\n\n\u221e\nX\n\n\u221e\nX\n\nn=1\n\nn=1\n\n1{\n\nP{ max |dk | \u2264 x, |dn+1 | > x} =\n1\u2264k\u2264n\n\nmax |dk |\u2264x} Pn {|dn+1 |\n\n> x}.\n\n1\u2264k\u2264n\n\nUsing this, (29) and applying Fatou's lemma twice allows us to conclude that\n1{ max |dk |\u2264x} Pn {|dn+1 | > x}\n\u221e\nX\nP{\u2206 > x}\n1\u2264k\u2264n\nlim inf\nElim inf\n\u22651+\n\u2265\nx\u2192\u221e P{W1 > x}\nx\u2192\u221e\nP{W1 > x}\nn=1\n\u22651+\n\n\u221e\nX\n\nn=1\n\n\uf8eb\n\nE\uf8ed\n\nX\n\n|u|=n\n\n\uf8f6\n\nYu\u03b2 \uf8f8 = (1 \u2212 k\u03b2 )\u22121 .\n\nTo complete the proof for \u2206 we must calculate the corresponding upper limit. For this\nit suffices to check that for large x and large n \u2208 N\nP{|dn+1 | > x}\n\u2264 Cn and Cn is a summable sequence,\nP{W1 > x}\n\n(31)\n\nand use the dominated convergence theorem. Taking the expectation in (11) allows us to\nconclude that for n = 1, 2, . . . and large x\nP{|dn+1 | > x + \u03bcn }\n\u2264 constEUn ,\nP{W1 > x}\nP\n(u)\nwhere \u03bcn is the median of Vn := |u|=n Yu |W1 \u2212 1|, and EUn is given by (15). The\nfamily of distributions of Vn is tight. In view of (30),\nP{|dn+1 | > x + y} \u223c P{|dn+1 | > x} locally uniformly in y.\nTherefore, (31) holds with Cn = constEUn and the result for \u2206 has been proved.\nFor later needs let us notice here that in the same way as above we can prove that for\nfixed n \u2208 N\nP{ sup Wm > x} \u223c k\u03b2n (1 \u2212 k\u03b2 )\u22121 P{W1 > x}.\n(32)\nm\u2265n\n\nConsider now the square function S. Since S \u2265 \u2206 a.s., and we have already proved\nthat P{\u2206 > x} \u223c (1 \u2212 k\u03b2 )\u22121 P{W1 > x} then\nlim inf\nx\u2192\u221e\n\n1\nP{S > x}\n\u2265\n.\nP{W1 > x}\n1 \u2212 k\u03b2\n11\n\n\fTherefore we must only calculate the upper limit. We begin with showing that for any\nn\u2208N\nn\u22121\nX\nP{Sn > x}\nlim sup\nk\u03b2m .\n(33)\n\u2264\nx\u2192\u221e P{W1 > x}\nm=0\n\nWe use induction on n.\n(1) If n = 1 then S1 \u2264 W1 and (33) is obvious.\n(2) Assume that (33) holds for n = j and show that it holds for n = j + 1. For every x > 0\nand \u01eb \u2208 (0, 1)\nP{Sj+1 > x} \u2264 P{Sj2 > (1 \u2212 \u01eb)x2 } + P{d2j+1 > (1 \u2212 \u01eb)x2 } + P{Sj2 > \u01ebx2 , d2j+1 > \u01ebx2 } =\nwrite the latter P as EPj and use Fj -measurability of Sj to get\n= P{Sj > (1 \u2212 \u01eb)1/2 x} + P{|dj+1 | > (1 \u2212 \u01eb)1/2 x} + E1{Sj >\u01eb1/2 x} Pj {|dj+1 | > \u01eb1/2 x}.\n(u)\n\nAccording to (10) with Qu replaced by W1\nlim 1{Sj >\u01eb1/2 x}\n\nx\u2192\u221e\n\n\u2212 1,\n\nPj {|dj+1 | > \u01eb1/2 x}\n= 0 a.s.,\nP{W1 > x}\n\nand there exists a \u03b41 > 0 such that for large x\n1{Sj >\u01eb1/2 x}\n\nX\nPj {|dj+1 | > \u01eb1/2 x}\nYu\u03b2 + \u03b41 a.s.\n\u2264 \u01eb\u2212\u03b2/2\nP{W1 > x}\n|u|=n\n\nTherefore, by the dominated convergence\nlim E1{Sj >\u01eb1/2 x}\n\nx\u2192\u221e\n\nPj {|dj+1 | > \u01eb1/2 x}\n= 0.\nP{W1 > x}\n\nBy the inductive assumption and (30)\nlim sup\nx\u2192\u221e\n\nj\nj\u22121\nX\nX\nP{Sj+1 > x}\nk\u03b2m .\nk\u03b2m + (1 \u2212 \u01eb)\u2212\u03b2/2 k\u03b2j = (1 \u2212 \u01eb)\u2212\u03b2/2\n\u2264 (1 \u2212 \u01eb)\u2212\u03b2/2\nP{W1 > x}\nm=0\n\nm=0\n\nSending \u01eb \u2192 0 proves (33).\nFor m = 0, 1, . . . and fixed n \u2208 N set W\u0303m := Wm\u2228n and F\u0303m ; = Fm\u2228n . Choose\n\u221a\n2\u03c12 \u03b2+1\n\u03c1 \u2208 (0, 3) so small that \u03bd :=\n2\n\u2208 (0, 1). Applying Theorem 18.2 [8] (in the\n3 \u2212 \u03c12\nnotation of that paper take \u03b2 = 2 and \u03b4 = \u03c1) to the non-negative martingale (W\u0303m , F\u0303m )\ngives\nP{(\n\n\u221e\nX\n\nm=n+1\n\nd2m )1/2\n\n> 2x} \u2264 P{ sup W\u0303m > \u03c1x} + P{(\nm\u2265n\n\n\u221e\nX\n\nm=n+1\n\n12\n\nd2m )1/2 > 2x, sup W\u0303m \u2264 \u03c1x} \u2264\nm\u2265n\n\n\f\u2264 P{ sup Wm > \u03c1x} +\nm\u2265n\n\n\u221e\nX\n2\u03c12\nd2m )1/2 > x}.\nP{(\n3 \u2212 \u03c12\n\n(34)\n\nm=n+1\n\nP{W1 > x}\n\u2264 2\u03b2+1 for x \u2265 y. Set\nP{W1 > 2x}\nP{ sup Wm > \u03c1x}\n\u0012 \u0013\u03b2\nk\u03b2n\n2\nm\u2265n\n. In view of (32), A(y) < \u221e and lim A(x) =\n.\nA(y) := sup\nx\u2192\u221e\nP{W\n>\n2x}\n1\n\u2212\nk\n\u03c1\n1\n\u03b2\nx\u2265y\nNow we have for x \u2265 y\nP\nP\n2 1/2 > 2x}\n2 1/2 > x}\nP{( \u221e\nP{( \u221e\nm=n+1 dm )\nm=n+1 dm )\n\u2264 A(y) + \u03bd\n.\nP{W1 > 2x}\nP{W1 > x}\n\nBy Potter's bound we can take y > 0 such that\n\nIterating the latter inequality gives that for k = 0, 1, . . .\nP\n2 1/2 > x}\nP{( \u221e\nm=n+1 dm )\nsup\n\u2264\nP{W1 > x}\nx\u2208[2k y,2k+1 y]\n\u2264 A(y)(1 + \u03bd + . . . + \u03bd\n\nk\u22121\n\nP\n2 1/2 > x}\nP{( \u221e\nm=n+1 dm )\n.\n) + \u03bd sup\nP{W1 > x}\nx\u2208[y,2y]\nk\n\nLet k \u2192 \u221e to obtain\nP\n2 1/2 > x}\nP{( \u221e\nm=n+1 dm )\nlim sup\n\u2264 A(y)(1 \u2212 \u03bd)\u22121 .\nP{W1 > x}\nx\u2192\u221e\nNow sending y \u2192 \u221e gives\nP\n\u0012 \u0013\u03b2\n2 1/2 > x}\nk\u03b2n\nP{( \u221e\n2\nm=n+1 dm )\n= const k\u03b2n .\n\u2264\nlim sup\nP{W1 > x}\n(1 \u2212 k\u03b2 )(1 \u2212 \u03bd) \u03c1\nx\u2192\u221e\nFor any \u03bb \u2208 (0, 1) and any n \u2208 N\nP{S > x} \u2264 P{Sn > (1 \u2212 \u03bb)1/2 x} + P{(\n\n\u221e\nX\n\nd2k )1/2 > \u03bb1/2 x}.\n\nk=n+1\n\nTherefore\nn\u22121\nX\nP{S > x} (33),(35)\n\u2212\u03b2/2\nlim sup\nk\u03b2m + const \u03bb\u2212\u03b2/2 k\u03b2n .\n\u2264\n(1 \u2212 \u03bb)\nx\u2192\u221e P{W1 > x}\nm=0\n\nLet n \u2192 \u221e and then \u03bb \u2192 0 to get the desired bound for the upper limit\nlim sup\nx\u2192\u221e\n\nP{S > x}\n1\n\u2264\n.\nP{W1 > x}\n1 \u2212 k\u03b2\n13\n\n(35)\n\n\fThis completes the proof for S.\n(II) From the already proved relation\nP{W \u2217 > x} \u223c (1 \u2212 k\u03b2 )\u22121 P{W1 > x} \u223c (1 \u2212 k\u03b2 )\u22121 x\u2212\u03b2 L(x),\n\n(36)\n\nit follows that EW \u2217 < \u221e which in turn ensures the uniform integrability of Wn .\nLet us now prove (4). Since W \u2217 \u2265 W a.s., E(W \u2217 \u2212 x)+ \u2265 E(W \u2212 x)+ for any x \u2265 0.\n(36) together with Proposition 1.5.10 [7] implies that\nE(W \u2217 \u2212 x)+ \u223c (\u03b2 \u2212 1)\u22121 (1 \u2212 k\u03b2 )\u22121 xP{W1 > x}.\nTherefore,\nlim sup\nx\u2192\u221e\n\nE(W \u2212 x)+\n1\n\u2264\n.\nxP{W1 > x}\n(\u03b2 \u2212 1)(1 \u2212 k\u03b2 )\n\n(37)\n\nFor each x \u2265 1 define the stopping time \u03bdx by\n\u001a\ninf{n \u2265 1 : Wn > x}, if W \u2217 > x;\n\u03bdx :=\n+\u221e,\notherwise.\nThe random variable W closes the martingale Wn . Hence, for each x \u2265 1\nE(W \u2212 x)1{\u03bdx <\u221e} = E(W\u03bdx \u2212 x)1{\u03bdx <\u221e} ,\nand hence\nE(W \u2212 x)+ \u2265 E(W\u03bdx \u2212 x)+ 1{\u03bdx <\u221e} .\nWe now transform the right-hand side into a more tractable form\n+\n\nE(W\u03bdx \u2212 x) 1{\u03bdx <\u221e} =\n=E\n\n\u221e\nX\nk=1\n\n\u221e\nX\nk=1\n\n+\n\nE(Wk \u2212 x) 1{\u03bdx =k} = E\n+\n\n1{\u03bdx \u2265k} Ek\u22121 (Wk \u2212 x) = E\n\n\u221e\nX\nk=1\n\n\u03bdx\nX\nk=1\n\nEk\u22121 ((Wk \u2212 x)+ 1{\u03bdx \u2265k} ) =\n\nEk\u22121 (Wk \u2212 x)+ .\n\nFrom (28) and (10) with Q replaced by W1 it follows that for k = 2, 3, . . .\nX\nPk\u22121 {Wk > y} \u223c\nYu\u03b2 P{W1 > y} a.s.\n|u|=k\u22121\n\nAn appeal to Proposition 1.5.10 [7] gives that for k = 2, 3, . . .\nX\nYu\u03b2 yP{W1 > y} a.s.\nEk\u22121 (Wk \u2212 y)+ \u223c (\u03b2 \u2212 1)\u22121\n|u|=k\u22121\n\nSince lim \u03bdx = +\u221e a.s., using Fatou's lemma allows us to conclude that\nx\u2192\u221e\n\nE\nE(W \u2212 x)+\n\u2265 lim inf\nlim inf\nx\u2192\u221e\nx\u2192\u221e xP{W1 > x}\n14\n\nP\u03bd x\n\n\u2212 x)+\n=\nxP{W1 > x}\n\nk=1 Ek\u22121 (Wk\n\n\f\uf8eb\n\uf8f6\uf8f6\n\uf8eb\n\u221e\nX\nX\n1 \uf8ed\n1\n=\nYu\u03b2 \uf8f8\uf8f8 =\nE\uf8ed\n.\n1+\n\u03b2\u22121\n(\u03b2 \u2212 1)(1 \u2212 k\u03b2 )\nk=2\n\n|u|=k\u22121\n\nCombining the latter inequality with (37) yields\n\nE(W \u2212 x)+ \u223c (\u03b2 \u2212 1)\u22121 (1 \u2212 k\u03b2 )\u22121 xP{W1 > x},\nwhich by the monotone density theorem (see Theorem 1.7.2 [7]) implies (4).\nThe result for M immediately follows from\nP{W > x} \u223c P{W \u2217 > x} \u223c (1 \u2212 k\u03b2 )\u22121 P{W1 > x},\nas |W \u2212 1| \u2264 M \u2264 W \u2217 a.s. The proof of the theorem is finished.\n\n3\n\nThe second proof of (4) in the case \u03b2 > 2\n\nAssume that the assumptions of Theorem 1.1 hold with \u03b2 > 2. By (36) and Theorem\n1.6.5 [7],\nEW \u2217 (W \u2217 \u2212 x)+ \u223c \u03b2(\u03b2 \u2212 1)\u22121 (\u03b2 \u2212 2)\u22121 (1 \u2212 k\u03b2 )\u22121 x2\u2212\u03b2 L(x).\nSince for each x > 0 EW \u2217 (W \u2217 \u2212 x)+ \u2265 EW (W \u2212 x)+\nlim sup\nx\u2192\u221e\n\nEW (W \u2212 x)+\n\u03b2\n\u2264\n.\nx2\u2212\u03b2 L(x)\n(\u03b2 \u2212 1)(\u03b2 \u2212 2)(1 \u2212 k\u03b2 )\n\n(38)\n\nLyons [24] constructed a probability space with probability measure Q and proved the\nfollowing equality\nn\nX\nEQ (Wn |G) = 1 +\n\u03a0k\u22121 (Sk \u2212 1)\nQ a.s.,\nk=1\n\nwhere EQ is expectation with respect to Q; \u03a00 := 1, \u03a0k := M1 M2 * * * Mk , k = 1, 2, . . .;\n{(Mk , Sk ) : k = 1, 2, . . .} are Q-independent copies of a random vector (M, S) whose\ndistribution is defined by the equality\nX\nX\nE\nYu h(Yu ,\nYv ) = Eh(M, S),\n(39)\n|u|=1\n\n|v|=1\n\nwhich is assumed to hold for any nonnegative Borel bounded function h(x, y); G is the\n\u03c3-field that can be explicitly described (we only note that \u03c3((Mk , Sk ), k = 1, 2, . . .) \u2282 G).\nAlso for any Borel function r with obvious convention when the right-hand side is infinite\nor does not exist\nEQ r(Wn ) := EWn r(Wn ) and EQ r(W ) := EW r(W ).\n\n(40)\n\nLyons explains his clever argument in a quite condensed form. More details clarifying his\nway of reasoning can be found in [4] and [22].\n15\n\n\f2\nSince\nP P{W12 > x} regularly varies with exponent \u2212\u03b2, \u03b2 > 2 then EW1 < \u221e. Also by\n(7) E |u|=1 Yu < 1. By Proposition 4 [17] the last two inequalities together ensure that\nEW 2 < \u221e. In view of (40) EQ W = EW 2 and hence EQ W < \u221e. In Lemma 4.1 [22] it was\nproved that (1) holds with P replaced by Q. This implies that EQ W \u2217 < \u221e iff EQ W < \u221e.\nTherefore we have checked that EQ W \u2217 < \u221e which by the dominated convergence theorem\nimplies that\n\u221e\nX\nEQ (W |G) = 1 +\n\u03a0k\u22121 (Sk \u2212 1) =: R\nQ a.s.\nk=1\n\nBy Jensen's inequality, for any convex function g\n\nEQ (g(W )|G) \u2265 g(EQ (W |G)) = g(R)\n\nQ a.s.\n\nSetting g(u) := (u \u2212 x)+ , x > 0 and taking expectations yields\n\n(40)\nEW (W \u2212 x)+ = EQ (W \u2212 x)+ \u2265 EQ (R \u2212 x)+ .\n\n(41)\n\nFrom (39) it follows that EQ M \u03b2\u22121 = k\u03b2 < 1, EQ M \u03b2\u22121+\u01eb = k\u03b2+\u01eb < \u221e and\nZ \u221e\nydP{W1 \u2264 y}.\nQ{S \u2212 1 > t} =\nt+1\n\nUsing the latter equality and Theorem 1.6.5 [7] leads to Q{S\u22121 > t} \u223c \u03b2(\u03b2\u22121)\u22121 t1\u2212\u03b2 L(t).\nTherefore Theorem 1 [16] applies to the perpetuity R which gives\nQ{R > t} \u223c \u03b2(\u03b2 \u2212 1)\u22121 (1 \u2212 EQ M \u03b2\u22121 )\u22121 t1\u2212\u03b2 L(t).\nBy Proposition 1.5.10 [7]\n+\n\nEQ (R \u2212 x) =\n\nZ\n\n\u221e\nx\n\nQ{R > t}dt \u223c\n\n\u03b2\nx2\u2212\u03b2 L(x).\n(\u03b2 \u2212 1)(\u03b2 \u2212 2)(1 \u2212 k\u03b2 )\n\nAn appeal to (41) now results in\nlim inf\nx\u2192\u221e\n\nEW (W \u2212 x)+\n\u03b2\n\u2265\n.\n2\u2212\u03b2\nx\nL(x)\n(\u03b2 \u2212 1)(\u03b2 \u2212 2)(1 \u2212 k\u03b2 )\n\nCombining this with (38) yields\n(40)\nEQ (W \u2212 x)+ = EW (W \u2212 x)+ \u223c\n\nBy the monotone density theorem\nQ{W > x} \u223c\nSince Q{W > x} =\n\nR\u221e\n\n\u03b2\nx2\u2212\u03b2 L(x).\n(\u03b2 \u2212 1)(\u03b2 \u2212 2)(1 \u2212 k\u03b2 )\n\n\u03b2\nx1\u2212\u03b2 L(x).\n(\u03b2 \u2212 1)(1 \u2212 k\u03b2 )\n\nydP{W \u2264 y}, integrating by parts gives\nZ \u221e\nx\nxP{W > x}\n=1\u2212\ny \u22122 Q{W > y}dy.\nQ{W > x}\nQ{W > x} x\nx\n\nBy Proposition 1.5.10 [7] the right-hand side tends to (\u03b2 \u2212 1)\u03b2 \u22121 when x \u2192 \u221e. Therefore,\nP{W > x} \u223c (\u03b2 \u2212 1)(\u03b2x)\u22121 Q{W > x} \u223c (1 \u2212 k\u03b2 )\u22121 x\u2212\u03b2 L(x) as desired.\n16\n\n\fRemark 3.1. This argument seems not to work as just described when \u03b2 \u2208 (1, 2]. If\n\u03b2 \u2208 (1, 2) we can get a bound for the upper limit\nlim sup\nx\u2192\u221e\n\nEW (W \u2227 x)\n\u03b2\n\u2264\n.\n2\u2212\u03b2\nx\nL(x)\n(\u03b2 \u2212 1)(2 \u2212 \u03b2)(1 \u2212 k\u03b2 )\n\nHowever, we do not know how the corresponding lower limit could be obtained. In fact,\nwe have not been able to find any random variable \u03be with appropriate tail behaviour and\nsuch that W \u2265 \u03be in some strong or weak sense.\n\n4\n\nMiscellaneous comments\n\nWe begin this section with discussing the following problem: which classes of point processes satisfy both (2) and (3) and which do not.\nLet h : [0, \u221e) \u2192 [0, \u221e) be a nondecreasing and right-continuous function with h(+0) >\n0.\nExample 4.1. Let {\u03c40 := 0, \u03c4i , i \u2265 1} be the renewal times of an ordinary renewal process.\nIn addition to the conditions on h imposed above, assume that h(0) is finite. Consider the\n\u22121\npoint process\n.}, where \u03b3 > 0 is chosen so\nP\u221e M with points {Ai = \u03b3 log h(\u03c4i ), i = 1, 2, . .P\nthat E i=1 h(\u03c4i ) = 1. According to Theorem 1 [14] W1 = \u221e\ni=1 h(\u03c4i ) has exponentially\ndecreasing tail. Thus while we can find h and {\u03c4i } such that (2) holds, (3) always fails.\n\nThe situation when P{W1 > x} \u223c x\u2212\u03b2 L(x) and EW1\u03b2 < \u221e is not terribly interesting.\nHowever, if this is the case Theorem 1.1 implies the one-way implication of a well-known\nmoment result (see [17] and [21])\nX\nE\nYu\u03b2 < 1, EW1\u03b2 < \u221e \u21d4 EW \u03b2 < \u221e, E(W \u2217 )\u03b2 < \u221e.\n|u|=1\n\nIn the subsequent examples in addition to (2) and (3) we require that EW1\u03b2 = \u221e. Examples 4.2 and 4.3 essentially shows that when the number of points in a point process is\ninfinite, and the points are independent or constitute an (inhomogeneous) Poisson flow,\nP\nE |u|=1 Yu\u03b2 < \u221e implies EW1\u03b2 < \u221e, \u03b2 > 1.\n\nExample 4.2. Assume that M is a point process with independent points {Ci }, and\nP\nP\u221e\n\u03b2\n\u03b3Ci and \u03b3 > 0. Then\nE \u221e\ni=1 Yi = 1 and for some \u03b2 > 1 E\ni=1 Yi < \u221e, where Yi = e\nEW1\u03b2 < \u221e.\nP\nP\u221e\n\u03b2\nIn this case W1 = \u221e\ni=1 Yi . Hence we must check that E( i=1 Yi ) < \u221e. By using\nthe c\u03b2 -inequality let us write the (formal) inequality\n\u221e\n\u221e\n\u221e\n\u221e\n\u221e\nX\nX\nX\nX\nX\nX\nYi )\u03b2\u22121 ).\nYi )E(\nYi (Yi +\nYi\u03b2 + (E\nYi )\u03b2 = E\nYk )\u03b2\u22121 \u2264 (2\u03b2\u22122 \u2228 1)(E\nE(\ni=1\n\ni=1\n\ni=1\n\nk6=i\n\ni=1\n\ni=1\n\n(42)\nFor any \u03b2 > 1 there existsPn \u2208 N such that \u03b2 \u2208 P\n(n, n + 1]. We will use induction\n\u221e\n\u03b2\u22121 < \u221e. Hence by (42)\non n. If \u03b2 \u2208 (1, 2] then E \u221e\nY\n<\n\u221e\nimplies\nE(\ni=1 i\ni=1 Yi )\n17\n\n\fP\n\u03b2\nE( \u221e\nthat the conclusion is true for \u03b2 \u2208 (n, n + 1] and prove it for\ni=1 Yi ) < \u221e. AssumeP\nP\u221e \u03b2\u22121\nP\u221e \u03b2\n<\u221e\n<\n\u221e\nwe\nhave\nE\n\u03b2 \u2208 (n + 1, n + 2]. Since E \u221e\nY\ni=1 Yi\ni=1 Yi < \u221e and E\ni=1\ni\nP\u221e\n\u03b2\u22121\nwhich, by the assumption of induction, implies E( i=1 Yi )\n< \u221e. It remains to apply\n(42).\nExample 4.3. Let {\u03c4i , i \u2265 1} be the arrival times of a Poisson process with intensity\n\u03bb > 0. Consider a Poisson point process M with points {Bi } and assume that for any\na \u2208 R M(a, \u221e) \u2265 1 a.s. Then there exists a function h as described at the beginning of\n\u03b3Bi and\nthis sectionPthat additionally\nR \u221e satisfies h(0) = \u221e, and \u03b3 > 0 such that h(\u03c4i ) = e\n\u221e\nEW1 = E i=1 h(\u03c4i ) = \u03bb 0 h(u)du = 1. The distribution of W1 is infinitely divisible\nwith zero shift and L\u00e9vy measure \u03bd given\n= \u03bbh\u2190 (dx), where h\u2190 is a\nP\u221eas \u03b2follows: R\u03bd(dx)\n\u221e \u03b2\ngeneralized inverse function. Since \u03bbE i=1 h (\u03c4i ) = 0 x \u03bd(dx),\nR \u221e and as is well-known\nfrom the general theory of infinitely divisible distributions, 1 x\u03b2 \u03bd(dx) < \u221e implies\nP\n\u03b3\u03b2Bi < \u221e implies EW \u03b2 < \u221e.\nEW1\u03b2 < \u221e, we conclude that E \u221e\ni=1 e\n1\n\nIn the next example we point out a class of point processes which satisfy (2) and (3),\nand EW1\u03b2 = \u221e.\n\nExample 4.4. Let K be a nonnegative integer-valued random variable with P{K > x} \u223c\nx\u2212\u03b2 L(x), \u03b2 > 1, and {Di , i \u2265 1} be independent identically distributed random variables\nwhich are independent of K. If P\nM is a point processPwith points {Di , i = 1, K} and\nK\n\u03b3Di = 1 and E\n\u03b3\u03b2Di < 1 then according to\nthere exists a \u03b3 > 0 such that E K\ni=1 e\ni=1 e\n\u03b3\u03b2D\n\u2212\u03b2\n1x\nProposition 4.3 [15] we have P{W1 > x} \u223c Ee\nL(x).\nWe conclude with two remarks that fit the context of the present paper.\n\nRemark 4.1. The tail behaviour of P{W > x} and P{W \u2217 > x} has been investigated in [17]\nP\nand [21]. In particular, from those works it follows that when the condition E |u|=1 Yu\u03b2 <\n1 fails, the condition P{W1 > x} \u223c x\u2212\u03b2 L(x) is not a necessary one for either P{W > x} \u223c\nx\u2212\u03b2 L(x) or P{W \u2217 > x} \u223c x\u2212\u03b2 L(x) to hold.\nRemark 4.2. Let {Xi } be the points of a point process, and let V be a random variable\nsatisfying the following distributional equality\nd\n\nV =\n\n\u221e\nX\n\nXi Vi ,\n\ni=1\n\nwhere V1 , V2 , . . . are conditionally on {Xi } independent copies of V . The distribution\nof V is called a fixed point of the smoothing transform (see [17] for more details, and\n[19] and [20] for an interesting particular case). It is known and can be easily checked\nthat the distribution of W is a fixed point of the smoothing transform with {Xi : i =\n1, 2, . . .} = {Yu : |u| = 1}. Thus (4) could be reformulated as a result on the tail behaviour\nof the fixed points with finite mean. The tail behaviour of fixed points with infinite mean\ndeserves\na special mention. Typically, their tails regularly vary with index \u03b1 \u2208 (0, 1), or\nRx\n0 P{V > y}dy slowly varies. This follows from Proposition 1(b) [17] and Proposition\n8.1.7[7].\n\n18\n\n\fReferences\n[1] Austin, D. G. (1966). A sample function property of martingales. Ann. Math. Stat. 37, 1396-1397.\n[2] Biggins, J. D. (1979). Growth rates in the branching random walk. Z. Wahrscheinlichkeitsth. 48,\n17-34.\n[3] Biggins, J. D. and Kyprianou, A. E. (1997). Seneta-Heyde norming in the branching random\nwalk. Ann. Prob. 25, 337-360.\n[4] Biggins, J. D. and Kyprianou, A. E. (2004). Measure change in multitype branching.\nAdv.Appl.Prob.36, 544-581.\n[5] Bingham, N. H. and Doney, R. A. (1974). Asymptotic properties of supercritical branching processes I: The Galton-Watson process. Adv.Appl.Prob. 6, 711-731.\n[6] Bingham, N. H. and Doney, R. A., (1975). Asymptotic properties of supercritical branching\nprocesses II: Crump-Mode and Jirina processes. Adv.Appl.Prob. 7, 66-82.\n[7] Bingham, N. H., Goldie, C. M. and Teugels, J.L. (1989) Regular variation, 1st paperback edn.\nCambridge Univ. Press, Cambridge.\n[8] Burkholder, D. L. (1973). Distribution function inequalities for martingales. Ann.Prob. 1, 19-42.\n[9] Burkholder, D. L. and Gundy, R. F. (1970). Extrapolation and interpolation of quasi-linear\noperators on martingales. Acta Math. 124, 249-304.\n[10] Burkholder, D. L., Davis, B. J. and Gundy, R. F. (1972). Integral inequalities for convex\nfunctions of operators on martingales. Proc. Sixth Berkeley Symp. Math. Statist. Prob. 2, 223-240.\n[11] Chow, Y. S. and Teicher, H. (1992). Probability theory: independence, interchangeability, martingales, 2nd edn. Springer-Verlag, New York.\n[12] Kingman, J. F. C. (1975). The first birth problem for an age-dependent branching process. Ann.\nProb. 3, 790-801.\n[13] de Meyer, A. (1982). On a theorem of Bingham and Doney. J.Appl.Prob. 19, 217-220.\n[14] Doney, R. A. and O'Brien, G. L. (1991). Loud shot noise. Ann.Appl.Prob.1, 88-103.\n[15] Fay, G., Gonz\u00e1lez-Ar\u00e9valo, B., Mikosch, T. and Samorodnitsky, G. (2005). Modeling teletraffic arrivals by a Poisson cluster process. Technical report, Cornell University.\n[16] Grey, D. R. (1994). Regular variation in the tail behaviour of solutions of random difference equations. Ann. Appl. Prob. 4, 169-183.\n[17] Iksanov, A. M. (2004). Elementary fixed points of the BRW smoothing transforms with infinite\nnumber of summands. Stoch.Proc.Appl. 114, 27-50.\n[18] Iksanov, A. M. (2006). On the rate of convergence of a regular martingale related to the branching\nrandom walk (in Ukrainian). Ukr.Math.J. 58, 326-342.\n[19] Iksanov, A. M. and Jurek Z. J. (2002). On fixed points of Poisson shot noise transforms.\nAdv.Appl.Prob. 34, 798-825.\n[20] Iksanov, A. M. and Kim C. S. (2004). On a Pitman-Yor problem. Stat. Prob. Lett. 68, 61-72.\n\n19\n\n\f[21] Iksanov, O. and Negadajlov, P. (2006). On the supremum of a martingale related to the branching\nrandom walk. Theor.Probab. Math. Stat., in press.\n[22] Iksanov, A. M. and R\u00f6sler, U. (2006). Some moment results about the limit of a martingale\nrelated to the supercritical branching random walk and perpetuities. Ukr.Math.J., in press.\n[23] Kwapie\u0144, S. and Woyczy\u0144ski, W. A. (1992). Random series and stochastic integrals: Single and\nmultiple, 2nd edn. Birkh\u00e4user, Basel and Boston.\n[24] Lyons, R. (1997). A simple path to Biggins's martingale convergence for branching random walk.\nIn Classical and Modern Branching Processes, eds K.B. Athreya and P. Jagers, Springer, Berlin,\npp.217-221.\n[25] Mikosch, T. and Samorodnitsky, G. (2000). The supremum of a negative drift random walk with\ndependent heavy-tailed steps. Ann.Appl.Probab. 10, 1025-1064.\n\n20\n\n\f"}