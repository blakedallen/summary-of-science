{"id": "http://arxiv.org/abs/0710.1520v3", "guidislink": true, "updated": "2009-02-09T07:53:07Z", "updated_parsed": [2009, 2, 9, 7, 53, 7, 0, 40, 0], "published": "2007-10-08T11:52:27Z", "published_parsed": [2007, 10, 8, 11, 52, 27, 0, 281, 0], "title": "Multicolor urn models with reducible replacement matrices", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0710.1872%2C0710.5051%2C0710.1140%2C0710.5508%2C0710.3612%2C0710.1669%2C0710.3106%2C0710.3695%2C0710.5110%2C0710.4714%2C0710.2959%2C0710.5101%2C0710.0372%2C0710.1570%2C0710.0270%2C0710.4347%2C0710.5141%2C0710.1379%2C0710.4765%2C0710.0897%2C0710.3571%2C0710.0124%2C0710.2771%2C0710.5251%2C0710.0131%2C0710.1482%2C0710.2498%2C0710.0847%2C0710.0957%2C0710.2653%2C0710.1115%2C0710.5200%2C0710.5176%2C0710.0986%2C0710.3775%2C0710.4858%2C0710.2688%2C0710.0575%2C0710.0420%2C0710.0465%2C0710.2602%2C0710.0591%2C0710.3105%2C0710.4326%2C0710.0859%2C0710.3726%2C0710.4299%2C0710.4890%2C0710.1272%2C0710.3303%2C0710.0094%2C0710.0168%2C0710.3547%2C0710.0456%2C0710.3404%2C0710.3330%2C0710.4193%2C0710.4100%2C0710.5082%2C0710.1880%2C0710.4941%2C0710.1603%2C0710.1584%2C0710.0799%2C0710.5336%2C0710.2593%2C0710.0702%2C0710.2024%2C0710.0113%2C0710.5894%2C0710.0647%2C0710.3079%2C0710.2918%2C0710.5310%2C0710.1576%2C0710.3611%2C0710.5037%2C0710.1182%2C0710.4902%2C0710.2787%2C0710.4144%2C0710.1496%2C0710.0477%2C0710.5057%2C0710.1580%2C0710.2838%2C0710.4357%2C0710.2025%2C0710.4173%2C0710.1520%2C0710.4649%2C0710.5780%2C0710.4429%2C0710.2841%2C0710.3724%2C0710.0748%2C0710.4337%2C0710.4415%2C0710.2261%2C0710.4532%2C0710.5701&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Multicolor urn models with reducible replacement matrices"}, "summary": "Consider the multicolored urn model where, after every draw, balls of the\ndifferent colors are added to the urn in a proportion determined by a given\nstochastic replacement matrix. We consider some special replacement matrices\nwhich are not irreducible. For three- and four-color urns, we derive the\nasymptotic behavior of linear combinations of the number of balls. In\nparticular, we show that certain linear combinations of the balls of different\ncolors have limiting distributions which are variance mixtures of normal\ndistributions. We also obtain almost sure limits in certain cases in contrast\nto the corresponding irreducible cases, where only weak limits are known.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0710.1872%2C0710.5051%2C0710.1140%2C0710.5508%2C0710.3612%2C0710.1669%2C0710.3106%2C0710.3695%2C0710.5110%2C0710.4714%2C0710.2959%2C0710.5101%2C0710.0372%2C0710.1570%2C0710.0270%2C0710.4347%2C0710.5141%2C0710.1379%2C0710.4765%2C0710.0897%2C0710.3571%2C0710.0124%2C0710.2771%2C0710.5251%2C0710.0131%2C0710.1482%2C0710.2498%2C0710.0847%2C0710.0957%2C0710.2653%2C0710.1115%2C0710.5200%2C0710.5176%2C0710.0986%2C0710.3775%2C0710.4858%2C0710.2688%2C0710.0575%2C0710.0420%2C0710.0465%2C0710.2602%2C0710.0591%2C0710.3105%2C0710.4326%2C0710.0859%2C0710.3726%2C0710.4299%2C0710.4890%2C0710.1272%2C0710.3303%2C0710.0094%2C0710.0168%2C0710.3547%2C0710.0456%2C0710.3404%2C0710.3330%2C0710.4193%2C0710.4100%2C0710.5082%2C0710.1880%2C0710.4941%2C0710.1603%2C0710.1584%2C0710.0799%2C0710.5336%2C0710.2593%2C0710.0702%2C0710.2024%2C0710.0113%2C0710.5894%2C0710.0647%2C0710.3079%2C0710.2918%2C0710.5310%2C0710.1576%2C0710.3611%2C0710.5037%2C0710.1182%2C0710.4902%2C0710.2787%2C0710.4144%2C0710.1496%2C0710.0477%2C0710.5057%2C0710.1580%2C0710.2838%2C0710.4357%2C0710.2025%2C0710.4173%2C0710.1520%2C0710.4649%2C0710.5780%2C0710.4429%2C0710.2841%2C0710.3724%2C0710.0748%2C0710.4337%2C0710.4415%2C0710.2261%2C0710.4532%2C0710.5701&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Consider the multicolored urn model where, after every draw, balls of the\ndifferent colors are added to the urn in a proportion determined by a given\nstochastic replacement matrix. We consider some special replacement matrices\nwhich are not irreducible. For three- and four-color urns, we derive the\nasymptotic behavior of linear combinations of the number of balls. In\nparticular, we show that certain linear combinations of the balls of different\ncolors have limiting distributions which are variance mixtures of normal\ndistributions. We also obtain almost sure limits in certain cases in contrast\nto the corresponding irreducible cases, where only weak limits are known."}, "authors": ["Arup Bose", "Amites Dasgupta", "Krishanu Maulik"], "author_detail": {"name": "Krishanu Maulik"}, "author": "Krishanu Maulik", "links": [{"title": "doi", "href": "http://dx.doi.org/10.3150/08-BEJ150", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0710.1520v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0710.1520v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.3150/08-BEJ150 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0710.1520v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0710.1520v3", "journal_reference": "Bernoulli 2009, Vol. 15, No. 1, 279-295", "doi": "10.3150/08-BEJ150", "fulltext": "arXiv:0710.1520v3 [math.PR] 9 Feb 2009\n\nBernoulli 15(1), 2009, 279\u2013295\nDOI: 10.3150/08-BEJ150\n\nMulticolor urn models with reducible\nreplacement matrices\nARUP BOSE* , AMITES DASGUPTA** and KRISHANU MAULIK\u2020\nStatistics and Mathematics Unit, Indian Statistical Institute, 202 B.T. Road, Kolkata 700108,\nIndia. E-mail: * abose@isical.ac.in; ** amites@isical.ac.in; \u2020 krishanu@isical.ac.in\nConsider the multicolored urn model where, after every draw, balls of the different colors are\nadded to the urn in a proportion determined by a given stochastic replacement matrix. We\nconsider some special replacement matrices which are not irreducible. For three- and four-color\nurns, we derive the asymptotic behavior of linear combinations of the number of balls. In particular, we show that certain linear combinations of the balls of different colors have limiting\ndistributions which are variance mixtures of normal distributions. We also obtain almost sure\nlimits in certain cases in contrast to the corresponding irreducible cases, where only weak limits\nare known.\nKeywords: martingale; reducible stochastic replacement matrix; urn model; variance mixture of\nnormal\n\n1. Introduction\nConsider an urn model with balls of K colors. The row vector C0 will denote the number\nof balls of each color we start with. (By abuse of terminology, we shall allow the number\nof balls to be any non-negative real number.) The vector C0 will be taken to be a\nprobability vector, that is, each coordinate is non-negative and the coordinates add up\nto 1. Suppose R = ((rij )) is a K \u00d7 K non-random stochastic (i.e., each row sum is one)\nreplacement matrix. The results of this paper extend to non-random replacement matrices\nwith constant (not necessarily one) row sums by an obvious rescaling. Let Cn be the row\nvector giving the number of balls of each color after the nth trial. At the nth trial, a ball\nis drawn at random, and so a ball of ith color appears with probability Ci,n\u22121 /n. If a\nball of ith color appears, then the number of balls of jth color is increased by rij . If R\nequals the identity matrix, then it is well known (see, e.g., [3]) that Cn /(n + 1) converges\nalmost surely to a Dirichlet random vector with parameters given by the starting vector\nC0 .\nLet 1 or 0 stand respectively for the column vector of relevant dimension with all\ncoordinates 1 or 0. For any vector \u03be, \u03be 2 will be the vector whose coordinates are the\nsquare of those of \u03be.\nThis is an electronic reprint of the original article published by the ISI/BS in Bernoulli,\n2009, Vol. 15, No. 1, 279\u2013295. This reprint differs from the original in pagination and\ntypographic detail.\n1350-7265\n\nc\n\n2009 ISI/BS\n\n\f280\n\nA. Bose, A. Dasgupta and K. Maulik\n\nIn Section 2, we consider two color models (K = 2). If the replacement matrix R is\nnot the identity matrix, then it has two right eigenvectors, 1 and \u03be corresponding to the\nprincipal eigenvalue 1 and the non-principal eigenvalue \u03bb, respectively, with |\u03bb| < 1. If R\nis irreducible, the asymptotic properties of Cn 1 and Cn \u03be are well known in the literature,\nsee Proposition 2.1.\nWhen the replacement matrix R is reducible but not the identity matrix, then, after\npossibly interchanging the names of the colors, R is an upper triangular matrix\n\u0012\n\u0013\ns 1\u2212s\nR=\n,\n(1)\n0\n1\nfor 0 < s < 1. Here the non-principal eigenvalue is s with the corresponding eigenvector\n\u03be = (1, 0)\u2032 . The asymptotic behavior of the linear combinations is given in Proposition 2.2.\nIn this case, Cn \u03be/ns = Wn /ns converges almost surely for all values of s in contrast to the\nirreducible case. See also Theorems 1.3(v), 1.7, 1.8 and 8.8 of [7], where the distribution\nof the limiting random variable was identified using methods from the branching process.\nIn the multicolor case, when R is irreducible, the weak/strong laws corresponding\nto different linear combinations are completely known, see [1, 6]. Gouet [4] considered\n(reducible) replacement matrices that are block diagonal, with all but the last block irreducible. The last block was taken to be block upper triangular, which cannot be converted\ninto a block diagonal one, and each diagonal subblock of the last block was assumed to be\na multiple of some irreducible stochastic matrix. He showed (cf. Theorem 3.1 of [4]) that\nthe proportions of colors converge almost surely to a constant vector where the non-zero\ncoordinates correspond to all but the last diagonal block and the last diagonal subblock\nof the last diagonal block. We call the corresponding colors dominant. To avoid trivial\nsituations, we shall always assume positive contribution to at least one non-dominant\ncolor in the initial vector C0 .\nWe shall consider three- and four-color urn models with block upper triangular replacement matrices that are not block diagonal. The diagonal blocks will be taken to be\nirreducible and we shall extend the result obtained in [4] by obtaining the limiting results\nfor linear combinations corresponding to a complete set of linearly independent vectors.\nSpecifically, in Section 3, we consider three colors \u2013 white, black and green \u2013 and the\n3 \u00d7 3 replacement matrix\n!\n1\u2212s\nsQ\n(2)\nR=\n1\u2212s ,\n0 0\n1\nwhere 0 < s < 1, and Q is a 2 \u00d7 2 irreducible aperiodic stochastic matrix with stationary\ndistribution \u03c0Q . Here green alone is the dominant color and we assume that W0 + B0 > 0.\na.s.\nWe show in Theorem 3.1(iv) that (Wn , Bn )/ns \u2192 \u03c0 Q V , where P (V > 0) = 1 and V is\nnon-degenerate. If \u03be is the eigenvector corresponding to the non-principal eigenvalue \u03bb of\nQ, weak/strong laws for (Wn , Bn )\u03be are also provided in Theorem 3.1. If \u03bb \u2264 1/2, then the\nweak limit is a variance mixture of normal, in contrast to the irreducible model, where\nthe weak limit is normal.\n\n\fMulticolor urn models\n\n281\n\nIn Section 4, we consider another type of reducible replacement matrix with two dominant colors:\n\u0012\n\u0013\ns (1 \u2212 s)p\nR=\n,\n(3)\n0\nP\nwhere P is a 2 \u00d7 2 irreducible stochastic matrix, p is a row probability vector and\n0 < s < 1. If the eigenvalues of P are \u03bb and 1, then s, \u03bb and 1 are eigenvalues of R. Clearly\n(1, 0, 0)\u2032 is the eigenvector corresponding to s and the behavior of the corresponding linear\ncombination, Wn , follows directly from Proposition 2.2.\nNow consider the eigenvalue \u03bb. If R is diagonalizable, then the weak/strong law of\nthe linear combination given by the eigenvector corresponding to \u03bb is summarized in\nTheorem 4.1. If R is not diagonalizable, then one of the eigenvalues is repeated, namely\n\u03bb = s, and the repeated eigenvalue has eigenspace of dimension 1, spanned by (1, 0, 0)\u2032 .\nConsider the Jordan decomposition of R, RT = T J , where T is non-singular and\n\uf8eb\n\uf8f6\ns 1 0\nJ = \uf8ed0 s 0\uf8f8.\n(4)\n0 0 1\n\nThe first and the third columns of T can be chosen as (1, 0, 0)\u2032 and 1 respectively. For the\nlinear combination corresponding to the middle column, we get weak/strong law. The\nconvergence is in the almost sure sense, whenever \u03bb \u2265 1/2, unlike the irreducible and\nthe diagonalizable reducible cases. For \u03bb = 1/2, in the irreducible and the diagonalizable\nreducible\nfor the irreducible\np cases, we have weak convergence only. Also, the scaling\n\u221a\n3\ncase is n log n and for the diagonalizable reducible\np case is n log n, unlike the nondiagonalizable reducible case, where the scaling is n log2 n.\nOther interesting reducible three-color urn models have been considered in the literature. For example, [2] and [9] consider three-color urn models with triangular replacement\nmatrices. Our emphasis is on replacement matrices with block triangular structure, given\nby (2) and (3). Note that Q in (2) is assumed to be a stochastic matrix. However, our\ntechniques do not have a direct extension to the case where Q does not have a constant\nrow sum. In another related work, Pouyanne [8] allows eigenvalues of the replacement\nmatrix to be complex and obtains interesting results for appropriate linear combinations.\nFor example, in his Theorems 3.5 and 3.6, rates are given for the linear combinations\ncorresponding to the eigenvalue with the second largest real part, where it is bigger than\n1/2. In our setup, all the eigenvalues are real and we obtain the rates for all possible\nlinear combinations.\nThe results for three-color urn models are extended to four-color (white, black, green\nand yellow) urns with the reducible replacement matrix given by\n\u0012\n\u0013\nsQ E\nR=\n,\n(5)\n0 P\nwhere each component is a 2 \u00d7 2 matrix and furthermore P and Q are irreducible stochastic matrices, 0 < s < 1. The results are summarized in Propositions 4.2\u20134.5. An interesting\n\n\f282\n\nA. Bose, A. Dasgupta and K. Maulik\n\nphenomenon is observed in Proposition 4.5, where the replacement matrix is not diagonalizable and the repeated eigenvalue is zero. Unlike the behavior of the corresponding\nlinear combination in other cases, where it remains a constant, we get a weak limit of\nvariance mixture of normal distribution in this case.\nBefore proceeding with the details, note that the proofs are based on studying the\nbehavior of appropriate martingales with the filtration Fn being the natural filtration of\nthe sequence {Cn }.\n\n2. Two-color urn models\nDefine\n\u03a0n (\u03bb) =\n\nn\u22121\nY\u0012\n\n1+\n\nj=0\n\n\u0013\n\u03bb\n.\nj +1\n\n(6)\n\nRecall that Euler's formula for gamma function gives\n\u03a0n (\u03bb) \u223c n\u03bb /\u0393(\u03bb + 1),\n\n\u03bb not a negative integer.\n\n(7)\n\nThis will be used at several places later.\nWe first mention the asymptotic behavior in two-color irreducible urn models. The\nfollowing results are well known. See, for example, [1, 6].\nProposition 2.1. In a two-color urn model with irreducible replacement matrix R,\nCn a.s.\n\u2192 \u03c0R,\nn+1\n\n(8)\n\nwhere \u03c0 R is the stationary distribution of R. Further, we have:\n\u221a\n\u03bb2\n(i) If \u03bb < 1/2, then Cn \u03be/ n \u21d2 N (0, 1\u22122\u03bb\n\u03c0 R \u03be2 ).\n\u221a\n(ii) If \u03bb = 1/2, then Cn \u03be/ n log n \u21d2 N (0, \u03bb2 \u03c0R \u03be 2 ).\n(iii) If \u03bb > 1/2, then Cn \u03be/\u03a0n (\u03bb) is an L2 -bounded martingale and converges almost\nsurely, as well as in L2 , to a non-degenerate random variable.\nRemark 2.1. Since Wn + Bn = n + 1, we have from (8),\na.s.\n\n(Wn , Bn )/(Wn + Bn ) \u2192 \u03c0R .\n\n(9)\n\na.s.\n\nRemark 2.2. From (8), we see that Cn \u03be/(n + 1) \u2192 \u03c0R \u03be. However \u03c0 R \u03be = \u03c0R R\u03be =\n\u03bb\u03c0 R \u03be, and since \u03bb 6= 1, we have \u03c0 R \u03be = 0. This explains the appropriate scaling up of\nCn \u03be/(n + 1) to obtain the weak laws for the proportions above.\nRemark 2.3. When \u03bb > 1/2, using (7), Cn \u03be/n\u03bb converges almost surely, as well as in\nL2 , to a non-degenerate random variable. Thus, the scalings in Proposition 2.1(i) and\n\n\fMulticolor urn models\n\n283\n\n(iii) are different. Also, in (iii), the distribution of the limit random variable depends on\nthe starting value (W0 , B0 ) unlike in (i) and (ii). Furthermore, as in Remark 2.1, we can\nconclude that, when \u03bb > 1/2, (Wn , Bn )\u03be/(Wn + Bn )\u03bb converges almost surely, as well as\nin L2 , to a non-degenerate random variable.\nRemark 2.4. If \u03bb = 0, both rows of R equal \u03c0 R . Since \u03c0 R \u03be = 0 and clearly Cn =\nC0 + n\u03c0R , we have Cn \u03be = C0 \u03be for all n.\nNext, we consider the almost sure limit behavior of the two-color urn model with an\nupper triangular reducible replacement matrix given by (1).\nProposition 2.2. In a two-color urn model with an upper triangular replacement matrix\ngiven by (1), we have:\n(i) Cn 1/(n + 1) = 1.\na.s.\n(ii) Cn /(n + 1) \u2192 (0, 1).\n(iii) Cn \u03be/\u03a0n (s) = Wn /\u03a0n (s) is an L2 -bounded martingale, where \u03a0n (s) is given\nby (6). Further, Wn /ns converges to a non-degenerate, positive random variable\nalmost surely, as well as in L2 .\nProof. Statement (i) is trivial. Statement (ii) is same as that of (8) in Proposition 2.1\nand a proof can be obtained from Proposition 4.3 of [4].\nFor (iii), observe that the number of white balls evolves as\nWn+1 = Wn + s\u03c7n+1 ,\nwhere \u03c7n is the indicator of a white ball in nth trial. Define the martingale sequence\nVn = Wn /\u03a0n (s), n \u2265 1. We shall show that {Vn } is an L2 -bounded martingale and hence\nconverges almost surely, as well as in L2 . Also, the variance of Vn increases to that of the\nlimit and hence the limit is non-degenerate. The proposition then follows from (7), the\ndistribution of V and the fact that it is almost surely positive, all of which have been\nestablished using branching process techniques in Theorem 1.3(v) of [7].\nClearly, we have\n\u0012\n\u0013\nWn\ns\n\u03c7n+1 \u2212\n,\nVn+1 \u2212 Vn =\n\u03a0n+1 (s)\nn+1\nand further, using Vn+1 = Vn + (Vn+1 \u2212 Vn ) and the martingale property, there exists N\n(non-random), such that for all n \u2265 N ,\n\u0014\n\u0015\nWn\nWn2\ns2\n2\n2\n\u2212\nE[Vn+1 |Fn ] = Vn + 2\n\u03a0n+1 (s) n + 1 (n + 1)2\n\u2264 Vn2 +\n\nVn\n(n + 1)\u03a0n (s)\n\n\u2264 Vn2 + \u0393(s + 1)\n\n\u0015\n\u0014\n\u0393(s + 1)\n\u0393(s + 1)\n1 + Vn2\n2\n+\n=\nV\n1\n+\n.\nn\n(n + 1)s+1\n(n + 1)s+1\n(n + 1)s+1\n\n\f284\n\nA. Bose, A. Dasgupta and K. Maulik\n\nThe last inequality holds for n \u2265 N and follows from the fact that Vn \u2264 (1 + Vn2 )/2\nand (7). Taking further expectation and adding 1 to both sides, we have, for n \u2265 N ,\n\u0014\n\u0015\n\u0393(s + 1)\n2\nE[Vn+1 ] + 1 \u2264 1 +\n(E[Vn2 ] + 1).\n(n + 1)s+1\nIterating, we get for n \u2265 N ,\n2\nE[Vn+1\n]+1\n\n\u2264 (E[VN2 ] + 1)\n\nn \u0014\nY\n\nj=N\n\n\u0393(s + 1)\n1+\n(j + 1)s+1\n\n\u0015\n\nand since s > 0, we further have for all n > N ,\nE[Vn2 ]\n\n\u2264\n\n(E[VN2 ] + 1) exp\n\n\u221e\nX\n\u0393(s + 1) j \u2212(s+1)\n\nwhich shows {Vn } is L2 -bounded as required.\n\n0\n\n!\n\n< \u221e,\n\u0003\n\n3. One dominant color, K = 3\nNow we are ready to consider the three-color urn model with only one dominant color,\nsay green. We shall denote the row subvector corresponding to the non-dominant colors\n(Wn , Bn ) as Sn . We collect the results in the following theorem.\nTheorem 3.1. Consider a three-color urn model with a reducible replacement matrix\nR given by (2). Suppose the non-principal eigenvalue of Q is \u03bb and the corresponding\neigenvector is \u03be. Then the following hold:\n(i) Cn 1/(n + 1) = 1.\na.s.\n(ii) Cn /(n + 1) \u2192 (0, 0, 1).\n(iii) Sn 1/(n + 1)s converges almost surely, as well as in L2 , to a non-degenerate positive random variable U .\na.s.\n(iv) Sn /(n + 1)s \u2192 \u03c0 Q U .\ns2 \u03bb2\nU \u03c0Q \u03be2 ).\n(v) If \u03bb < 1/2, then Sn \u03be/ns/2 \u21d2 N (0, s(1\u22122\u03bb)\n\u221a s\n(vi) If \u03bb = 1/2, then Sn \u03be/ n log n \u21d2 N (0, s2 \u03bb2 U \u03c0Q \u03be2 ).\n(vii) If \u03bb > 1/2, then Sn \u03be/\u03a0n (s\u03bb) is an L2 -bounded martingale and almost surely, as\nwell as in L2 , Sn \u03be/ns\u03bb \u2192 V , where V is a non-degenerate random variable.\nThe random variable U in (iv), (v) and (vi) is the same limiting random variable obtained\nin (iii). The distributions of U and V depend on the initial value S0 .\nRemark 3.1. Note that the eigenvalues of R are 1, s and s\u03bb with corresponding eigenvectors 1, (1, 1, 0)\u2032 and (\u03be \u2032 , 0)\u2032 , respectively, yielding the linear combinations Cn 1, Sn 1\nand Sn \u03be.\n\n\fMulticolor urn models\n\n285\n\nProof of Theorem 3.1. Statement (i) is immediate. Statement (ii) follows from Theorem 3.1 and Proposition 4.3 of [4].\nNote that Sn 1 = Wn + Bn . From the structure of R, the pair (Wn + Bn , Gn ) yields a\ntwo-color model with reducible replacement matrix\n\u0012\n\u0013\ns 1\u2212s\n.\n0\n1\nStatement (iii) then follows from Proposition 2.2. The distribution of U has been identified in Theorem 1.3(v) of [7].\nConsider the successive times \u03c4k when either a white or a black ball is observed. Due to\nthe assumed special structure of the matrix R, it is only at these times that more white\nor black balls are added and the total number added is the constant s. Thus S\u03c4k /S\u03c4k 1 are\nthe proportions from the evolution of a two-color urn model governed by the irreducible\nreplacement matrix Q. Hence by the two-color urn result (9), it converges almost surely\nto \u03c0 Q . Note that at all other n, \u03c4k < n < \u03c4k+1 , the vector Sn = S\u03c4k and hence the ratio\na.s.\nis unchanged. Moreover, from the statement (iii), we have Sn 1/ns \u2192 V . Now combining\nall of the above, the proof of the statement (iv) is complete.\nFor (vii), let \u03c7n be the row vector, which takes values \u03c7n = (1, 0), (0, 1) or (0, 0)\naccordingly as the white, black or green balls are observed in nth trial and consider the\nmartingale Tn = Sn \u03be/\u03a0n (s\u03bb). Then the martingale difference is\n\u0012\n\u0013\nSn \u03be\ns\u03bb\n\u03c7n \u03be \u2212\nTn+1 \u2212 Tn =\n\u03a0n+1 (s\u03bb)\nn+1\nand hence\n2\nE[Tn+1\n]\n\n\u00132 \u0014\n\u00132 \u0015\n\u0012\nSn \u03be 2\nSn \u03be\n=\nE\n\u2212\nn+1\nn+1\n\u00132\n\u0015 \u0012\n\u0015\n\u0014\n\u0014\n1\ns\u03bb\n(s\u03bb)2\nSn \u03be 2\n2\n+\n.\nE\n= E[Tn ] 1 \u2212\n(n + 1)2 (1 + s\u03bb/(n + 1))2\n\u03a0n+1 (s\u03bb) (n + 1)1\u2212s\n(n + 1)s\nE[Tn2 ] +\n\n\u0012\n\ns\u03bb\n\u03a0n+1 (s\u03bb)\n\nThe first term is bounded by E[Tn2 ]. From statement (iii), Sn 1/(n + 1)s is L2 -bounded\nand hence L1 -bounded. So Sn \u03be 2 /(n + 1)s is also L1 -bounded. Thus, using (7), the second\nterm is bounded by a constant multiple of n\u2212(1+s(2\u03bb\u22121)) , which is summable as \u03bb > 1/2.\nThus {Tn } is an L2 -bounded martingale and hence converges almost surely as well as in\nL2 .\nFor (v) and (vi), we start with the case \u03bb < 1/2. Call Xn = Sn \u03be/ns/2 . We have the\nevolution equation for Sn \u03be given by\nSn+1 \u03be = Sn \u03be + s\u03c7n+1 Q\u03be = Sn \u03be + \u03bbs\u03c7n+1 \u03be.\n\n(10)\n\nWe now use the decomposition of Xn+1 into a conditional expectation and a martingale\ndifference\nXn+1 = E(Xn+1 |Fn ) + {Xn+1 \u2212 E(Xn+1 |Fn )}.\n\n\f286\n\nA. Bose, A. Dasgupta and K. Maulik\n\nUsing (10) and the fact that (1 + 1/n)\u2212s/2 = (1 \u2212 s/2n) + O(1/n2 ) we then get\n\u03bbs\nSn \u03be\nSn \u03be\n(1 + 1/n)\u2212s/2 +\nns/2\n(n + 1)s/2 n + 1\n\u0012\n\u0012\n\u0013\u2212s/2\n\u0012 \u0013\u0013\ns\n1\n1\n1\n= Xn 1 \u2212\n+ \u03bbsXn 1 +\n+O 2\n2n\nn\nn\nn+1\n\u0012\n\u0013\ns(1/2 \u2212 \u03bb)\n= Xn 1 \u2212\n+ Xn O(n\u22122 ).\nn\n\nE(Xn+1 |Fn ) =\n\n(11)\n\nOn the other hand, the martingale difference is really\nMn+1 := Xn+1 \u2212 E(Xn+1 |Fn ) =\n\n\u0013\n\u0012\n\u03bbs\nSn\n\u03be,\n\u03c7\n\u2212\nn+1\nn+1\n(n + 1)s/2\n\n(12)\n\nso that\n\u0012\n\u0013\ns(1/2 \u2212 \u03bb)\n+ Xn O(n\u22122 ) + Mn+1 .\nXn+1 = Xn 1 \u2212\nn\n\n(13)\n\nIterating the equation above, we get\n\u0013 X\n\u0013\nn \u0012\nn \u0012\nn\nY\nY\ns(1/2 \u2212 \u03bb)\ns(1/2 \u2212 \u03bb)\n\u22122\n1\u2212\n1\u2212\nXn+1 = X1\nXj O(j )\n+\ni\ni\ni=1\ni=j+1\nj=1\n\u0013\nn \u0012\nY\ns(1/2 \u2212 \u03bb)\n.\n1\u2212\nMj+1\n+\ni\ni=j+1\nj=1\nn\nX\n\n(14)\n\nSince \u03bb < 1/2, we have \u03a0n (\u2212s(1/2 \u2212 \u03bb)) \u223c n\u2212s(1/2\u2212\u03bb) /\u0393(1 \u2212 s(1/2 \u2212 \u03bb)) \u2192 0, and hence\nthe first term above converges to 0 for every sample point. The continued product in\nthe second term is bounded by 1. Since the coordinates of Sn /(n + 1) are bounded by\n1, we have that |Xn |/n1\u2212s/2 is bounded for every sample point.\nthe sum of the\nPThus\n\u221e\nelements of the second term above is bounded by a multiple of 1 j \u2212(1+s/2) , which is\nfinite; and individually each element tends to zero since each infinite product diverges to\nzero. Hence the second term of (14) tends to zero for every sample point.\nNow we turn to the third term of (14),\n\u0013\nn \u0012\nY\ns(1/2 \u2212 \u03bb)\n1\u2212\nMj+1\n.\nZn+1 =\ni\ni=j+1\nj=1\nn\nX\n\n(15)\n\nWe verify the conditional Lyapunov condition and compute the conditional variance as\nn \u2192 \u221e. The conditional Lyapunov condition demands that for some k > 2,\nn\nX\nj=1\n\nE(|Mj+1 |k |Fj )\n\n\u0013k\nn \u0012\nY\ns(1/2 \u2212 \u03bb)\na.s.\n1\u2212\n\u2192 0.\ni\ni=j+1\n\n\fMulticolor urn models\n\n287\n\nSince each coordinate of \u03c7n+1 and Sn /(n + 1) is bounded by 1, the martingale difference\ndefined in (12) is bounded by a constant multiple of (n + 1)\u2212s/2 . Thus the above sum is\nbounded by a constant multiple of\n\u0013k\nn \u0012\nn\nY\nX\ns(1/2 \u2212 \u03bb)\n1\u2212\nj \u2212ks/2\n,\ni\ni=j+1\nj=1\nwhich tends to zero by the bounded convergence theorem, provided we choose k > 2/s.\nNow we compute the conditional variance. An exact computation and the statement\n(iv) yields, with probability 1,\n\u00132 \u0015\n\u0014\n\u0012\n(\u03bbs)2\n(\u03bbs)2 Sn \u03be2\nSn \u03be\n2\n\u223c\nE(Mn+1 |Fn ) =\n\u2212\nU \u03c0Q \u03be2 .\n(n + 1)s n + 1\nn+1\nn+1\nQn\n) = \u03a0n (\u2212s(1/2 \u2212 \u03bb))/\u03a0j (\u2212s(1/2 \u2212 \u03bb)) and using (7),\nThen, writing i=j+1 (1 \u2212 s(1/2\u2212\u03bb)\ni\nthe sum of the conditional variances satisfies, on a set of probability 1,\n\u00132\nn \u0012\nn\nn\nY\nX\ns(1/2 \u2212 \u03bb)\n(\u03bbs)2 U \u03c0 Q \u03be 2 X\n1\n2\n1\u2212\nE(Mj+1 |Fj )\n\u223c\n,\ns(1\u22122\u03bb)\n1\u2212s(1\u22122\u03bb)\ni\nn\nj\ni=j+1\nj=1\nj=1\nwhich converges almost surely to (\u03bbs)2 U \u03c0Q \u03be2 /s(1 \u2212 2\u03bb). Thus, by martingale central\nlimit theorem (see Corollary 3.1 of [5]), the limiting distribution of Zn+1 , and hence\nXn+1 is the required variance mixture of normal.\nSince the analysis for the statement (vi) is similar, we omit\n\u221a the details and provide\nonly a brief sketch of the arguments. We start with Xn = Sn \u03be/ ns log n. The following is\nthe relevant martingale decomposition now. To express the decomposition, the following\nstraightforward approximations are used:\n(1 + 1/n)\u2212s/2 = 1 \u2212\n\ns\n+ O(n\u22122 )\n2n\n\nand\n\nlog n\nlog n\n=\nlog(n + 1) log n + 1/n + O(n\u22122 )\n\ntogether give\n\u0012\n\nn\nn+1\n\n\u0013s/2 s\n\n\u0013\u0013\n\u0012\n\u0013\u0012\n\u0012\nlog n\ns\n1\n1\n= 1\u2212\n+ O(n\u22122 ) 1 \u2212\n+O 2\nlog(n + 1)\n2n\n2n log n\nn log n\n=1\u2212\n\n1\ns\n\u2212\n+ O(n\u22122 ).\n2n 2n log n\n\nUsing \u03bb = 1/2 carefully, the conditional expectation becomes\nE(Xn+1 |Fn ) = Xn [1 \u2212 (2n log n)\u22121 ] + Xn O(n\u22122 )\nand, for the martingale difference, we get,\n\u0013\n\u0012\ns\nSn\nMn+1 := Xn+1 \u2212 E(Xn+1 |Fn ) = p\n\u03be.\n\u03c7n+1 \u2212\nn+1\n2 (n + 1)s log(n + 1)\n\n\f288\n\nA. Bose, A. Dasgupta and K. Maulik\n\nThese together give us the recursion on Xn as\nXn+1 = Xn [1 \u2212 (2n log n)\u22121 ] + Xn O(n\u22122 ) + Mn+1 ,\na decomposition similar to (13). The rest of the proof follows as before with appropriate\nchanges.\n\u0003\nRemark 3.2. Theorem 3.1 gives the scaling for all the linear combinations except when\nP\n\u03bb = 0, in which case (v) applies and we obtain Sn \u03be/ns/2 \u2192 0. However, as discussed in\nRemark 2.4, Q has both rows the same as \u03c0Q , which satisfies \u03c0 Q \u03be = 0. Since Sn changes\nonly when a white or black ball appears, we have Sn = S0 + (Wn + Bn )\u03c0 Q and hence\nSn \u03be = S0 \u03be for all n.\n\n4. Two dominant colors, K = 3, 4\nWe now consider the three-color case with two dominant colors. The replacement matrix\nR, given by (3), is\n\u0012\n\u0013\ns (1 \u2212 s)p\nR=\n,\n0\nP\nwhere p is a probability vector and P is a 2 \u00d7 2 irreducible stochastic matrix. Thus\n1 is always an eigenvalue of P with the corresponding eigenvector 1. We shall denote\nthe other eigenvalue of P as \u03bb with corresponding eigenvector \u03be. Then s and 1 are two\neigenvalues of R with corresponding eigenvectors (1, 0, 0)\u2032 and 1, respectively. Observe\nthat Cn (1, 0, 0)\u2032 = Wn . The results for this linear combination follow from two-color urn\nmodel results, and we summarize them below. We shall denote the stationary distribution\nof P by \u03c0P .\nProposition 4.1. Consider a three-color urn model with two dominant colors and the\nreplacement matrix given by (3). Then:\n(i) Cn 1/(n + 1) = 1.\na.s.\n(ii) Cn /n \u2192 (0, \u03c0P ).\ns\n(iii) Wn /n \u2192 V almost surely, as well as in L2 .\nIn (iii), if we start with the initial vector C0 = (W0 , B0 , G0 ), then V has the same\ndistribution as the limit random variable in Theorem 3.1(vii) with the initial vector\n(W0 , B0 + G0 ).\nProof. Statement (i) is trivial. The proof of (ii) is given in Theorem 3.1 or Proposition 4.3\nof [4]. For the remaining part, consider the two-color urn model (Wn , Bn + Gn ) obtained\nby collapsing the last two colors. This will have the replacement matrix as in (1) and the\nresults will follow from Proposition 2.2.\n\u0003\n\n\fMulticolor urn models\n\n289\n\nHowever, the one remaining linear combination is more subtle. The choice of the linear\ncombination depends on whether R is similar to a diagonal matrix or, equivalently, has\na complete set of eigenvectors. Suppose \u03bb 6= s. Then R is diagonalizable and v 2 = (c, \u03be \u2032 )\u2032\nis an eigenvector of R corresponding to \u03bb, with c = (1 \u2212 s)p\u03be/(\u03bb \u2212 s). If \u03bb = s, then R\nis diagonalizable if and only if p\u03be = 0. In that case, (0, \u03be\u2032 )\u2032 is another eigenvector of R\ncorresponding to s independent of (1, 0, 0)\u2032 . Also note that, in that case, p is orthogonal\nto \u03be and since p is a probability vector we have p = \u03c0 P . In the diagonalizable case with\n\u03bb = s, we denote this remaining vector (0, \u03be \u2032 )\u2032 by v 2 and consider the corresponding\nlinear combination. The following theorem summarizes the results.\nTheorem 4.1. Consider a three-color urn model with replacement matrix R given by (5),\nwhere R is diagonalizable. Then the following weak/strong laws hold:\n\u221a\n\u03bb2\n(i) If \u03bb < 1/2, then Cn v 2 / n \u21d2 N (0, 1\u22122\u03bb\n\u03c0 P \u03be 2 ).\n\u221a\n(ii) If \u03bb = 1/2, then Cn v 2 / n log n \u21d2 N (0, \u03bb2 \u03c0P \u03be2 ).\n(iii) If \u03bb > 1/2, then Cn v 2 /\u03a0n (\u03bb) is an L2 -bounded martingale and Cn v 2 /n\u03bb converges almost surely to a non-degenerate random variable.\nProof. The proofs of (i) and (ii) are similar to those of (v) and (vi) of Theorem 3.1, so\nwe omit them.\nDefine \u03c7n as the row vector that takes values (1, 0, 0), (0, 1, 0) and (0, 0, 1) accordingly\nas white, black or green balls appear in nth trial. Also define Zn = Cn v 2 /\u03a0n (\u03bb). It is\nsimple to check that {Zn } is a martingale. Note that,\n\u0012\n\u0013\n\u03bb\nCn\nZn+1 \u2212 Zn =\n\u03c7n+1 \u2212\nv2 ,\n\u03a0n+1 (\u03bb)\nn+1\nwhich gives us\n\u03bb2\n\nCn v 22\nE[(Zn+1 \u2212 Zn ) |Fn ] = 2\n\u2212\n\u03a0n+1 (\u03bb) n + 1\n2\n\n\u0014\n\n\u0012\n\nCn v 2\nn+1\n\n\u00132 \u0015\n\n.\n\nAlso, Cn /(n + 1) is bounded by 1 for each coordinate. Hence, the conditional\nPn expectation\n\u22122\u03bb\n2\nabove is bounded by a constant multiple of nP\n. So we get E[Zn+1\n] = i=1 {E[(Zi+1 \u2212\n\u221e\nZi )2 } is bounded by a constant multiple of 1 i\u22122\u03bb , which is finite, as \u03bb > 1/2. Thus,\n2\n\u0003\n{Zn } is L -bounded. The rest of the statement (iii) follows from (7).\nIf R is not diagonalizable, then a complete set of eigenvectors is not available and\none of the eigenvalues must be repeated, which gives s = \u03bb and p 6= \u03c0 P . So we consider\nthe Jordan decomposition RT = T J , where J is given by (4). We can choose the first\nand third columns of T as t1 = (1, 0, 0)\u2032 and t3 = 1. Also the subvector of the lower two\ncoordinates of t2 is an eigenvector of P corresponding to s. We shall denote it by \u03be as\nwell. The behavior of Cn t2 is substantially different from the irreducible case given in\nTheorem 3.15 of [6] or the diagonalizable case in Theorem 4.1 above.\n\n\f290\n\nA. Bose, A. Dasgupta and K. Maulik\n\nTheorem 4.2. Consider a three-color urn model with replacement matrix R given by (5),\nwhere R is not diagonalizable. Then, we have:\n\u221a\ns2\n(i) If s < 1/2, then Cn t2 / n \u21d2 N (0, 1\u22122s\n\u03c0P \u03be2 ).\ns\n(ii) If s \u2265 1/2, then Cn t2 /n log n converges to V almost surely, as well as in L2 ,\nwhere V is the almost sure limit random variable obtained in Proposition 4.1(iii).\n\u221a\nProof. We first consider the case when s < 1/2. Call Xn = Cn t2 / n. Define the row\nvector \u03c7n as in the proof of Theorem 4.1. We shall split Xn+1 into conditional expectation\nand martingale difference parts as in the proof of Theorem 3.1(v). From the Jordan\ndecomposition of R and the form (4) of J , the evolution equation for Cn is given by\nCn+1 t2 = Cn t2 + s\u03c7n+1 t2 + \u03c7n+1 t1 .\nHence the conditional expectation becomes\n\u0013\n\u0012\nCn t2\n1\ns\nE(Xn+1 |Fn ) = \u221a\n+\n1+\nCn t1\nn+1\n(n + 1)3/2\nn+1\n\u0013\n\u0012\n1\n1/2 \u2212 s\n+ Xn O(n\u22122 ) +\nWn ,\n= Xn 1 \u2212\nn+1\n(n + 1)3/2\nsince Cn t1 = Wn . Using the notation st = t1 + st2 , the martingale difference term becomes\n\u0013\n\u0012\nCn\ns\nt.\n\u03c7n+1 \u2212\nMn+1 := Xn+1 \u2212 E(Xn+1 |Fn ) = \u221a\nn+1\nn+1\nPutting this together, we get a recursion on Xn as\n\u0012\n\u0013\n1/2 \u2212 s\nWn\nXn+1 = Xn 1 \u2212\n+ Xn O(n\u22122 ) +\n+ Mn+1 ,\nn\n(n + 1)3/2\nand iterating we get,\n\u0013 X\n\u0013\nn \u0012\nn\nn \u0012\nY\nY\n1/2 \u2212 s\n1/2 \u2212 s\n\u22122\n1\u2212\nXj O(j )\n+\n1\u2212\nXn+1 = X1\ni\ni\ni=j+1\nj=1\ni=1\n+\n\nn\nX\nj=1\n\n\u0013 X\n\u0013\nn \u0012\nn \u0012\nn\nY\nY\n1/2 \u2212 s\nWj\n1/2 \u2212 s\n1\n\u2212\n1\n\u2212\nM\n+\n,\nj+1\ni\ni\n(j + 1)3/2 i=j+1\ni=j+1\nj=1\n\nwhich is similar to the decomposition (14), except for the additional third term. Further\nanalysis is similar to that done for Theorem 3.1(v), except for the contribution of the\nthird term, which we now show to be negligible with probability 1. By Proposition 4.1(iii),\nQ\n) = \u03a0n (\u2212(1/2\u2212s))/\u03a0j (\u2212(1/2\u2212s)) and using (7), the third term\nwriting ni=j+1 (1\u2212 1/2\u2212s\ni\n\n\fMulticolor urn models\n\n291\n\nis of the order of\n1\nn1/2\u2212s\n\nn\nX\nj=1\n\nV\n\n1\n\nj 3/2\u2212s\n\nj \u2212(1/2\u2212s)\n\n\u223c\n\nV log n\n\u21920\nn1/2\u2212s\n\nalmost surely, since s < 1/2.\nUsing Proposition 4.1(ii), the structure of the vectors t1 and t2 and the fact \u03c0 P \u03be = 0,\nthe conditional variance term is\n2\nE(Mn+1\n|Fn ) =\n\n\u0014\n\u00132 \u0015\n\u0012\ns2\ns2\nCn t2\nCn t\n\u223c\n\u2212\n\u03c0P \u03be2 ,\nn+1 n+1\nn+1\nn+1\n\nwhich gives the required variance for the limiting normal distribution.\nNow we consider the other situation, where s \u2265 1/2. Using the form (4) of J in the\nJordan decomposition of R, we again have Rt2 = t1 + st2 = st. Thus\nCn+1 t2 = Cn t2 + \u03c7n+1 Rt2 = Cn t2 + s\u03c7n+1 t,\nwhich implies\n\u0012\nE[Cn+1 t2 |Fn ] = Cn t2 1 +\n\ns\nn+1\n\n\u0013\n\n+\n\nCn\nt1 .\nn+1\n\nThis gives us the martingale\nn\u22121\n\nXn =\n\nX 1\nCj t1\nCn t2\n\u2212\n.\n\u03a0n (s) j=1 j + 1 \u03a0j+1 (s)\n\n(16)\n\nCn\n)t/\u03a0n+1 (s), which\nThe martingale difference is then given by Xn+1 \u2212 Xn = s(\u03c7n+1 \u2212 n+1\nyields\n\u0014\n\u00132 \u0015\n\u0012\ns2\nCn t2\nCn t\n2\nE[(Xn+1 \u2212 Xn ) |Fn ] = 2\n,\n(17)\n\u2212\n\u03a0n+1 (s) n + 1\nn+1\n\nand using the fact that each coordinate of Cn /(n + 1) is bounded by 1 and Euler's\nformula for gamma function, the conditional second moment above is bounded by a\n2\nconstant multiple of n\u22122s . P\nTaking expectation and adding, we get E[Xn+1\n] is bounded\nn \u22122s\n2\nby a \u221a\nconstant multiple of 0 i . This implies, {Xn } is L -bounded if s > 1/2 and,\n{Xn / log n} is L2 -bounded if s = 1/2. Thus, for s > 1/2, Xn / log n \u2192 0 almost surely,\nas well as in L2 . For s = 1/2, Xn / log n \u2192 0 in L2 .\nWe now show the convergence is almost sure also, when s = 1/2. For this, consider the\nrandom variables Zn = Xn / log n and get\nZn+1 \u2212 Zn =\n\n\u0014\n\u0015\nXn+1 \u2212 Xn\nlog n\nXn\n1\u2212\n.\n\u2212\nlog(n + 1)\nlog n\nlog(n + 1)\n\n(18)\n\n\f292\nSince [1 \u2212\n\nbounded),\n\nhence\n\nA. Bose, A. Dasgupta and K. Maulik\n\u221a\n\u221a\n3/2\nlog n\nn and Xn / log n\nlog(n+1) ]/ log n \u223c 1/n log\nPn\nlog k\n}] is\nwe have E[ k=2 \u221a|Xk | \u221a 1 {1 \u2212 log(k+1)\nlog k\nlog k\n\nis L2 -bounded (and hence L1 bounded uniformly over n and\n\nn\nX\n\n\u0015\n\u0014\nXk\nlog k\n1\n\u221a\n\u221a\n1\u2212\nlog(k + 1)\nlog k log k\nk=2\nconverges absolutely almost surely. On the other hand, the first term of (18) is a\nmartingale difference and, using (17) for s = 1/2, the conditional variance E[(Xn+1 \u2212\nXn )2 /log2 (n + 1)|Fn } is bounded by a constant\nmultiple of [(n + 1) log2 (n + 1)]\u22121 ,\nPn\nwhich is summable. Hence, the martingale { k=1 (Xk+1 \u2212 Xk )/log(k + 1)} is L2 -bounded\nand thus converges almost surely. Combining the two observations above we get that\nZn = Xn / log n converges almost surely.\nThus Xn / log n converges to 0 almost surely, and in L2 , for all s \u2265 1/2. Hence,\nfrom (16), we have\nn\u22121\n\nCj t1\nCn t2\n1 X 1\nXn\n=\n\u2212\nlog n log n\u03a0n (s) log n j=0 j + 1 \u03a0j+1 (s)\n\n(19)\n\nconverges to 0 almost surely, as well as in L2 . But using (7) and Proposition 4.1(iii), we\nknow that Cn t1 /\u03a0n (s) \u223c \u0393(s + 1)Wn /ns \u2192 \u0393(s + 1)V almost surely, as well as in L2 .\nHence the second term in (19) converges to \u0393(s + 1)V almost surely, as well as in L2 .\nThus,\nCn t2\n1\nCn t2\n\u223c\nns log n \u0393(s + 1) \u03a0n (s) log n\nconverges to V almost surely, as well as in L2 .\n\n\u0003\n\nRemark 4.1. As in the case of one dominant color, we have the correct scaling for all\nthe linear combinations except when \u03bb = 0 < s. (This situation arises only in the case of\ndiagonalizable replacement matrix.) But v 2 being an eigenvector of R corresponding to\n\u03bb = 0, we have Rv 2 = 0. Thus Cn v 2 = C0 v 2 for all n.\nThe three-color urn model with two dominant colors can be easily extended to certain\nfour-color models. We consider the reducible replacement matrix given in (5),\n\u0012\n\u0013\nsQ E\nR=\n,\n0 P\nwhere P and Q are 2 \u00d7 2 irreducible stochastic matrices, 0 < s < 1. The eigenvalues of\nQ are \u03bb and 1, with |\u03bb| < 1. The eigenvalues of P are \u03b2 and 1, with |\u03b2| < 1. Then s\u03bb,\ns, \u03b2 and 1 are all eigenvalues of R. If \u03be is an eigenvector of Q corresponding to \u03bb, then\nv 1 = (1\u2032 , 0\u2032 )\u2032 , v 2 = (\u03be \u2032 , 0\u2032 )\u2032 and v 4 = 1 are eigenvectors of R corresponding to s, s\u03bb and\n1 respectively.\n\n\fMulticolor urn models\n\n293\n\nIf R is diagonalizable, then there is another eigenvector v 3 corresponding to \u03b2. If R\nis not diagonalizable, then one of its eigenvalues must repeat, namely \u03b2 must equal s or\ns\u03bb and we denote the other by \u03b1. In this case, we consider the Jordan decomposition\nRT = T J , where T is nonsingular. The fourth column t4 of T can be chosen as v 4 . The\nfirst two columns t1 and t2 of T can be chosen as the eigenvectors of R corresponding\nto \u03b1 and \u03b2. However, the third column t3 of T will not be an eigenvector of R, yet\nthe two-dimensional vector \u03bd formed by the lower half of t3 will be an eigenvector of P\ncorresponding to \u03b2. We shall only study Cn t3 separately in the non-diagonalizable case.\nThe following three Propositions are suitable extensions of the three-color results of\nthis section. The proofs are suitable modifications as well.\nProposition 4.2. Consider a four-color urn model with the replacement matrix given\nby (5). Then:\n(i)\n(ii)\n(iii)\n(iv)\n(v)\n\nCn 1/(n + 1) = 1.\na.s.\nCn /n \u2192 (0, 0, \u03c0P ).\na.s.\n(Wn , Bn )/ns \u2192 \u03c0 Q U .\ns\nCn v 1 /n \u2192 U almost surely, as well as in L2 .\ns2 \u03bb2\nIf \u03bb < 1/2, then Cn v 2 /ns/2 \u21d2 N (0, s(1\u22122\u03bb)\nU \u03c0 Q \u03be 2 ).\n\u221a s\n(vi) If \u03bb = 1/2, then Cn v 2 / n log n \u21d2 N (0, s2 \u03bb2 U \u03c0 Q \u03be 2 ).\n(vii) If \u03bb > 1/2, then Cn v 2 /ns\u03bb \u2192 V almost surely, as well as in L2 .\nIf we start with the initial vector (W0 , B0 , G0 , Y0 ), then U and V have the same distribution as the limit random variable in Theorem 3.1(iii) and the positive random variable\nin Theorem 3.1(vii), respectively, starting with initial vector (W0 , B0 , G0 + Y0 ).\nNext we consider the linear combination Cn v 3 in the diagonalizable case.\nProposition 4.3. In the four-color urn model with replacement matrix R given by (5),\nassume that all the eigenvalues of R are distinct. Then the following weak/strong laws\nhold for Cn v 3 :\n\u221a\n\u03b22\n(i) If \u03b2 < 1/2, then Cn v 3 / n \u21d2 N (0, 1\u22122\u03b2\n\u03c0 P \u03bd 2 ).\n\u221a\n(ii) If \u03b2 = 1/2, then Cn v 3 / n log n \u21d2 N (0, \u03b2 2 \u03c0 P \u03bd 2 ).\n(iii) If \u03b2 > 1/2, then Cn v 3 /\u03a0n (\u03b2) is an L2 -bounded martingale and Cn v 3 /n\u03b2 converges almost surely to a non-degenerate random variable.\nFinally, we consider the case when the replacement matrix R is not diagonalizable. As\nin the three-color urn model with a non-diagonalizable replacement matrix, the evolution\nof the linear combination Cn t3 depends on the eigenvector of R corresponding to the\neigenvalue \u03b2. When \u03b2 < 1/2, the effect of the contribution of the linear combination of\nthis eigenvector is negligible. However, for \u03b2 \u2265 1/2, this provides the main contribution\nand the almost sure limit random variable depends on whether \u03b2 equals s or s\u03bb. To\n\n\f294\n\nA. Bose, A. Dasgupta and K. Maulik\n\ndenote the limit random variable in a unified way, we define the random variable\n\u001a\nU,\nwhen \u03b2 = s,\nW=\nV,\nwhen \u03b2 = s\u03bb,\n\n(20)\n\nwhere U and V are the random variables defined in Proposition 4.2. Suppose \u03b2 \u2265 1/2. If\n\u03b2 = s, then t2 = v 1 and, by Proposition 4.2(iv), Cn t2 /(n + 1)\u03b2 = Cn v 1 /(n + 1)s \u2192 U = W\nalmost surely, as well as in L2 . If \u03b2 = s\u03bb, then t2 = v 2 . Also s < 1 implies \u03bb > 1/2. Hence\nby Proposition 4.2(vii), Cn t2 /(n + 1)\u03b2 = Cn v 2 /(n + 1)s\u03bb \u2192 V = W almost surely, as well\nas in L2 . So, for non-diagonalizable R and \u03b2 \u2265 1/2, we conclude Cn t2 /(n + 1)\u03b2 \u2192 W\nalmost surely, as well as in L2 .\nProposition 4.4. Consider the four-color urn model with replacement matrix R given\nby (5), where R is not diagonalizable. Then, we have:\n\u221a\n\u03b22\n(i) If \u03b2 < 1/2, then Cn t3 / n \u21d2 N (0, 1\u22122\u03b2\n\u03c0 P \u03bd 2 ).\n\u03b2\n(ii) If \u03b2 \u2265 1/2, then Cn t3 /n log n converges to W almost surely, as well as in L2 ,\nwhere W is as defined in (20).\nRemark 4.2. As in two- and three-color urn models, we have correct scalings for all\nlinear combinations except when \u03bb or \u03b2 becomes zero. If \u03bb = 0, Proposition 4.2(v) gives\nP\nCn v 2 /ns/2 \u2192 0. However, considering the three-color urn model (Wn , Bn , Gn + Yn ), we\nget, from Remark 3.2, Cn v 2 = C0 v 2 .\nIn the case of the diagonalizable replacement matrix, if \u03b2 = 0, we have a similar situation for the linear combination Cn v 3 in Proposition 4.3(i). However, as in Remark 4.1,\nv 3 being an eigenvector of R corresponding to \u03b2 = 0, we have Rv 3 = 0 and Cn v 3 = C0 v 3 .\nThe situation becomes more interesting when \u03b2 = 0 and the replacement matrix is\nnot diagonalizable. Thus, we necessarily have \u03b2 = s or \u03b2 = \u03bbs, but \u03b2 being zero and\ns being positive, only the second alternative is possible and further \u03b2 = \u03bb = 0. In this\n\u221a P\ncase, Proposition 4.4(i) gives Cn t3 / n \u2192 0. The correct rate is given in the following\nproposition.\nProposition 4.5. Consider the four-color urn model with the replacement matrix R\ngiven by (5), which is not diagonalizable. Further assume the repeated eigenvalue of R to\nbe zero. Then\nCn t3 /ns/2 \u21d2 N (0, \u03c0Q \u03be2 U/s),\nwhere U is the limit random variable corresponding to (Wn + Bn )/ns obtained in Proposition 4.2(iv).\nProof. Let \u03c7n be the row vector as in the proof of Theorem 3.1(vii). Using RT = T J\nand the fact \u03b2 = 0, we get Rt3 = t2 + \u03b2t3 = t2 . Hence using t2 = (\u03be \u2032 , 0\u2032 )\u2032 , the evolution\n\n\fMulticolor urn models\n\n295\n\nequation for Cn t3 is given by\nCn+1 t3 = Cn t3 + \u03c7n+1 \u03be.\nThe rest of the proof is similar to that of Theorem 4.2(i) and is omitted.\n\n\u0003\n\nAcknowledgement\nWe thank the referee for an extremely careful reading of the manuscript and highly\nconstructive comments.\n\nReferences\n[1] Bai, Z.-D. and Hu, F. (2005). Asymptotics in randomized urn models. Ann. Appl. Probab.\n15 914\u2013940. MR2114994\n[2] Flajolet, P., Dumas, P. and Puyhaubert, V. (2006). Some exactly solvable models of urn\nprocess theory. Discrete Math. Theor. Comput. Sci. AG 59\u2013118 (electronic).\n[3] Freedman, D.A. (1965). Bernard Friedman's urn. Ann. Math. Statist. 36 956\u2013970.\nMR0177432\n[4] Gouet, R. (1997). Strong convergence of proportions in a multicolor P\u00f3lya urn. J. Appl.\nProbab. 34 426\u2013435. MR1447347\n[5] Hall, P. and Heyde, C.C. (1980). Martingale Limit Theory and Its Application. New York:\nAcademic Press Inc. MR0624435\n[6] Janson, S. (2004). Functional limit theorems for multitype branching processes and generalized P\u00f3lya urns. Stochastic Process. Appl. 110 177\u2013245. MR2040966\n[7] Janson, S. (2006). Limit theorems for triangular urn schemes. Probab. Theory Related Fields\n134 417\u2013452. MR2226887\n[8] Pouyanne, N. (2008). An algebraic approach to P\u00f3lya processes. Ann. Inst. H. Poincar\u00e9\nProbab. Statist. 44 293\u2013323.\n[9] Puyhaubert, V. (2005). Mod\u00e8les d'urnes et ph\u00e9nom\u00e8nes de seuil en combinatoire analytique.\nPh.D. thesis, \u00c9cole Polytechnique, Palaiseau, France.\n\n\f"}