{"id": "http://arxiv.org/abs/1010.2968v1", "guidislink": true, "updated": "2010-10-14T16:27:13Z", "updated_parsed": [2010, 10, 14, 16, 27, 13, 3, 287, 0], "published": "2010-10-14T16:27:13Z", "published_parsed": [2010, 10, 14, 16, 27, 13, 3, 287, 0], "title": "On a probabilistic definition of time", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1010.1032%2C1010.5393%2C1010.1457%2C1010.2968%2C1010.1777%2C1010.4020%2C1010.6085%2C1010.1836%2C1010.3954%2C1010.5934%2C1010.0097%2C1010.4879%2C1010.5380%2C1010.0613%2C1010.0907%2C1010.0780%2C1010.4000%2C1010.4318%2C1010.1322%2C1010.4288%2C1010.6158%2C1010.0474%2C1010.5925%2C1010.1579%2C1010.3784%2C1010.4925%2C1010.3933%2C1010.3220%2C1010.0095%2C1010.3715%2C1010.0784%2C1010.0154%2C1010.3708%2C1010.3932%2C1010.4941%2C1010.5663%2C1010.5001%2C1010.5803%2C1010.4133%2C1010.4324%2C1010.0376%2C1010.3751%2C1010.2104%2C1010.3409%2C1010.1115%2C1010.5256%2C1010.1854%2C1010.2499%2C1010.6138%2C1010.1920%2C1010.5785%2C1010.1622%2C1010.5622%2C1010.3727%2C1010.2188%2C1010.3372%2C1010.0615%2C1010.0676%2C1010.4828%2C1010.2355%2C1010.3590%2C1010.2099%2C1010.1507%2C1010.1345%2C1010.0094%2C1010.0679%2C1010.5924%2C1010.5664%2C1010.4438%2C1010.2039%2C1010.0807%2C1010.3910%2C1010.5631%2C1010.2532%2C1010.6022%2C1010.5501%2C1010.0164%2C1010.1499%2C1010.2895%2C1010.0040%2C1010.1659%2C1010.6179%2C1010.2370%2C1010.5423%2C1010.2177%2C1010.1731%2C1010.5739%2C1010.0208%2C1010.5865%2C1010.2258%2C1010.2146%2C1010.4596%2C1010.1480%2C1010.5843%2C1010.3533%2C1010.3677%2C1010.1974%2C1010.5498%2C1010.5861%2C1010.5578%2C1010.3388&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On a probabilistic definition of time"}, "summary": "A mechanism is proposed that allows to interpret the temporal evolution of a\nphysical system as a result of the inability of an observer to record its whole\nstate and a simple example is given. It is based on a review of the concepts of\ninformation, entropy and order. It is suggested that the temporal evolution and\nthe choice of the \"initial state\" depend on the way the observer \"compresses\"\ninformation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1010.1032%2C1010.5393%2C1010.1457%2C1010.2968%2C1010.1777%2C1010.4020%2C1010.6085%2C1010.1836%2C1010.3954%2C1010.5934%2C1010.0097%2C1010.4879%2C1010.5380%2C1010.0613%2C1010.0907%2C1010.0780%2C1010.4000%2C1010.4318%2C1010.1322%2C1010.4288%2C1010.6158%2C1010.0474%2C1010.5925%2C1010.1579%2C1010.3784%2C1010.4925%2C1010.3933%2C1010.3220%2C1010.0095%2C1010.3715%2C1010.0784%2C1010.0154%2C1010.3708%2C1010.3932%2C1010.4941%2C1010.5663%2C1010.5001%2C1010.5803%2C1010.4133%2C1010.4324%2C1010.0376%2C1010.3751%2C1010.2104%2C1010.3409%2C1010.1115%2C1010.5256%2C1010.1854%2C1010.2499%2C1010.6138%2C1010.1920%2C1010.5785%2C1010.1622%2C1010.5622%2C1010.3727%2C1010.2188%2C1010.3372%2C1010.0615%2C1010.0676%2C1010.4828%2C1010.2355%2C1010.3590%2C1010.2099%2C1010.1507%2C1010.1345%2C1010.0094%2C1010.0679%2C1010.5924%2C1010.5664%2C1010.4438%2C1010.2039%2C1010.0807%2C1010.3910%2C1010.5631%2C1010.2532%2C1010.6022%2C1010.5501%2C1010.0164%2C1010.1499%2C1010.2895%2C1010.0040%2C1010.1659%2C1010.6179%2C1010.2370%2C1010.5423%2C1010.2177%2C1010.1731%2C1010.5739%2C1010.0208%2C1010.5865%2C1010.2258%2C1010.2146%2C1010.4596%2C1010.1480%2C1010.5843%2C1010.3533%2C1010.3677%2C1010.1974%2C1010.5498%2C1010.5861%2C1010.5578%2C1010.3388&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A mechanism is proposed that allows to interpret the temporal evolution of a\nphysical system as a result of the inability of an observer to record its whole\nstate and a simple example is given. It is based on a review of the concepts of\ninformation, entropy and order. It is suggested that the temporal evolution and\nthe choice of the \"initial state\" depend on the way the observer \"compresses\"\ninformation."}, "authors": ["Alberto Bicego"], "author_detail": {"name": "Alberto Bicego"}, "author": "Alberto Bicego", "arxiv_comment": "11 pages, 3 figures", "links": [{"href": "http://arxiv.org/abs/1010.2968v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1010.2968v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "physics.gen-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.gen-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1010.2968v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1010.2968v1", "journal_reference": null, "doi": null, "fulltext": "On a probabilistic definition of time\nAlberto Bicego\n(albertobicego@libero.it)\nA mechanism is proposed that allows to interpret the temporal evolution of a\nphysical system as a result of the inability of an observer to record its whole state\nand a simple example is given. It is based on a review of the concepts of\ninformation, entropy and order. It is suggested that the temporal evolution and the\nchoice of the \"initial state\" depend on the way the observer \"compresses\"\ninformation.\n\nIntroduction.\nCarlo Rovelli has recently returned [1] on an issue repeatedly addressed in the past [2\u22125],\nnamely on the advisability to introduce the concept of time in quantum gravity. His answer is clear:\n\"The answer I defend is that we must forget the notion of time altogether, and build a quantum\ntheory of gravity where this notion does not appear at all.\"\nThe \"problem of time\" is central for any attempt to combine general relativity with quantum\nmechanics, as is clear at least since Wheeler-DeWitt's equation was proposed, but the positions on\nits possible solution are very different [6, 7]. However, even if one does not want to adhere to the\nposition of those who consider time an indispensable ingredient in any physical theory [8], it is\nobviously necessary that this concept can be found at least as a result of some process of limit \u2212\nsimilar to the classical limits of relativity or quantum mechanics \u2212 or may emerge in some\nparticular situation. The mechanism proposed by Rovelli to recover the notion of time is a\nthermodynamic origin [1, 2]; Julian Barbour [9, 10] suggests that it is our brain, through certain\nstructures of the universe that he calls time capsules, to give us the illusion of a passing time, just\nlike a movie projector from a set of frames.\nIn addition, in an attempt to address some unsolved problems, such as the collapse of the\nwave function, Rovelli reaches a reformulation of quantum mechanics based on information theory\n[11, 12].\nI fully endorse the opinion that time is not something flowing someway outside the universe,\nbut it is the result of an interaction between the universe and our brain. I am also convinced that\ninformation theory should play an important role in physics not only, for example, in order to obtain\nquantum computers, but at a more fundamental level; an opinion, moreover, already expressed even\nmore categorically, amongst others, by J.A. Wheeler [13]. I can't say, of course, whether Rovelli's\nor Barbour's ideas will actually lead eventually to a consistent and complete theory of quantum\ngravity and I will not deal with this problem. I'll just show how, in some cases, a notion of time can\nbe derived from the probabilistic nature of information recording. According to Anderson's\nclassification [7], I should say that, rather than the \"problem of time mismatch\", I'll address the\nproblem of the \"arrow of time\".\n\nEntropy.\nI must begin by saying that I will use the terms entropy and information in Shannon's\noriginal meaning [14], although there is still a vigorous debate on their meaning and in particular on\nthe possible relationship between different definitions of entropy in various fields. [15\u221217].\nSo, consider an event that can be realized in N different ways xi, each with probability pi.\nShannon defines entropy of the set p1, ..., pN\n1\n\n\fN\n\n(1)\n\nH = \u2212 k \u2211 pi log 2 pi\ni =1\n\nwhere k is arbitrary and I have chosen 2 as base of the logarithm, to have the bit as information unit.\nIf we have no knowledge at all about the event, we shall give to all the results the same probability\np = 1 / N and therefore\n(2)\nH = k log 2 N\nLet us now consider a physical system S. The system S can be known by an observer O\nthrough a series of measures that determine its state. The observer O can consider S as the set of\npossible outcomes of the measurement process.1 Since we are interested in some kind of record of\nthe state of S, we consider a state of S as specified by a certain number, call it s, of bits (the number\nof possible states of S will therefore be N = 2s ).\nWe shall call measure any physical interaction between S and O because of which S will be in a\nstate x and O in a state X. Ultimately we can consider a measure as a function m : S \u2192 O from S to\nO.\nIn principle, if the number of bits available to O to carry out the measure is at least equal to s, a\nmeasure could be a bijection, so that a state of S is uniquely specified by one of O's, but it is clear\nthat in most cases it will not be so: think of a gas, whose state is specified by the knowledge (at\nleast) of the positions and moments of all particles, while measures allow us to know only the\nmacroscopic parameters. 2 We'll call micro-states the states x of S and macrostates the states X of\nO (also O will be the thought of as the set of its states). No ambiguity will arise if we call\nmacrostate also the set of microstates corresponding to the same state of O, i.e. m\u22121(X); we can thus\nsay that x \u2208 X and call microstates belonging to the same macrostate equivalent. The way in which\nO takes its measurements allows us therefore to build in a natural way a partition on S.\nSuppose now that O addresses the problem to predict the outcome of a measure: he will give\neach state a probability p(x) of showing up. x is therefore a random variable and it is possible to\nintroduce for it an entropy in Shannon's sense:\nH ( x) = \u2212 k \u2211 p ( x) log 2 p ( x)\n(3)\nS\n\nwhere the sum is extended to all states of S.\nIf O does not know anything about the system S, he will have no reason to assign different\nprobabilities to the various microstates, so in the absence of information about the system S,\n(4)\nH ( x) = k log 2 N = ks\nIt should be noted (as Shannon says explicitly) that H, in this approach, is not in any way a function\nof the state x, it is simply a number describing the amount of information that the observer O has on\nthe system S: in the case of minimum information, it is equal, up to a constant, to the number of bits\nused to describe a state of S.\nAssuming now that the result of a measurement has been X, O will be interested to know in which\nmicrostate S is. Again, in the absence of other information he will assign to all microstates\ncorresponding to X the same probability\n\uf8f11 / w( X ) if x \u2208 X\n(5)\np( x X ) = \uf8f2\notherwise\n\uf8f30\n\n1\n\nOne may wonder if S is actually something more than this, i.e. whether it has an existence independent of O'\nobservation (\"The notion rejected here is the notion of absolute, or observer-independent, state of a system;\nequivalently, the notion of observer-independent values of physical quantities.\" Rovelli in [11]), but we will not deal\nwith this problem.\n2\nIn fact, the existence of a much larger number of states of the system than is possible to measure, is supposed by us on\nthe assumption that the gas is made up of particles; it is therefore shareable the definition given by Rovelli [11], who\nregards the state of a system as the maximum amount of information one can get from it.\n\n2\n\n\fwhere I called p( x X ) the probability that the state of S is x conditioned by the fact that the\nmeasurement result is X and w, according to custom, the multiplicity of the macrostate X. Therefore\nH ( x X ) = k log w( X )\n(6)\nIt is (6), not (2) that must be interpreted as Boltzmann's equation. A value of H is therefore\nassociated with each macrostate X, but we can also associate this value with each state x \u2208 X , so\nthat we can consider H as a function defined on S, although not bijective. We shall therefore write\nH(x). This function tells us that the higher the multiplicity of the macrostate X to which x belongs,\nthe lower the information on the system S that O has gained by the measurement: only if w = 1\ndoes O know with certainty the state of S.\nThe meaning of H is here precisely that of information theory, since S can be regarded as a signal\nsource, O the destination and the measuring instrument the channel; so that O, receiving the\nmessage X, will raise the question of what the original content x is.\nIt should be noted that in this approach the definition of entropy makes sense only in\nreference to a specific observer and has no meaning for the system itself. Different observers can,\nin principle, construct different entropy functions in relation to different ways of grouping states\ntogether.\n\nTime evolution.\nIdentifying the time evolution of a system means, first of all, being able to order its states,\ni.e., given two states, say which comes first. All the classic examples make use of an idea like this:\none imagines a camera that records the state of the system and notes that he can easily tell whether\nthe movie runs forward or backward.\nIt must therefore be possible to establish a function which associates with each state of the system a\nvalue t of a real parameter. In fact, the function may not be injective, since all we can measure are\nthe macrostates. It is unavoidable the temptation to nominate entropy itself as a possible \"time\" of\nthe system; but any increasing function of H will do. That solves the problem of ordering the states\nof S, but still it is not enough.\nIt is usally said that a physical system evolves naturally over time so as to increase its\nentropy as it passes from less probable states to more probable ones. Actually we are referring, of\ncourse, to macrostates, the only ones we can really observe.\nThe idea underlying this explanation of the temporal evolution is that all microstates are equally\nlikely and so it seems natural that the temporal evolution goes in the direction of increasing\nmultiplicity. On closer reflection, however, this view appears to be an undue extension of the\nfundamental axiom of statistical mechanics. The axiom of microstates equiprobability concerns\nonly states of equilibrium and excludes the possibility of temporal evolution; indeed, statistical\nmechanics completely ignores the idea of time. \"Statistical mechanics, however, does not describe\nhow a system approaches equilibrium, nor does it determine whether a system can ever be found to\nbe in equilibrium. It merely states what the equilibrium situation is, for a given system\" (Kerson\nHuang in [18]).\nBesides, it does not justify the observation that evolution is progressive: if we observe that a system\nat a given time is in a state with low multiplicity (not likely), we can also expect that at a later\nmoment the system state belongs to a macrostate with higher multiplicity, but this hardly explains\nwhy the steady state is reached gradually. If all microstates are equally likely, why is the system not\nin a state of high multiplicity immediately after the first measurement?\nIt seems to me that a system could present a time evolution only if its states are not equally\nprobable. It's impossible to attribute a time evolution to a system, if, at any time, it has the same\nprobability of being in any microstate; the idea of time evolution is inextricably linked to a gradual\nchange.\nSo, let's see how we can introduce a time in S.\n3\n\n\fWe will need the following ingredients:\n\n\u2022\n\n(7)\n\na partition of S into macrostates, as described above, with these features:\nthere is a subset O 'of O in which different macrostates have different multiplicity, so\nthat\nXi \u2260 X j \u2192 H(Xi) \u2260 H(X j )\n\nthere is a state X 0 \u2208 O ' , which we'll call initial (macro)state, having\nH(X0) = 0\n\n(8)\n\nand thus multiplicity 1. We'll call initial (micro)state also x0 = m \u22121 ( X 0 ) ;\n\u2022\n\na bijection t : O \u2192 I \u2282 R + which is an increasing function of H;\nfrom (6) follows that t takes its minimum value in X0: we'll set\nt( X 0 ) = 0\n\n\u2022\n\na function p : O \u00d7 I \u2192 [0;1] with these features:\nfor each value of t, p is a probability function defined on O; in particular we'll have\ntherefore:\nfor each fixed t (the sum is over all the macrostates of O);\n\u2211 p( X , t ) = 1\n\n(9)\n\n(10)\n\nO\n\n(11)\n\ninitially the system is certainly in x0:\np ( X 0 ,0 ) = 1\nfor each fixed X the function p ( X , t ) has one maximum in t m = t ( X ) :\n\n(12)\n\nmax ( p ( X , t ) ) = t m \u2194 t m = t ( X )\n\nIn this case we'll say that t is a time for the observer O in relation to the set of states O'.\nIt may seem strange that t is a function defined on O and not on S, but remember that the\nstates of S are inaccessible to O: all he can know are the macrostates X.\nt can anyway be traced back to be a function on S as follows:\n(13)\nt ( x) = t (m( x) )\nOf course t (x) is not an injective function.\nAlso p can be made to be a function on S \u00d7 I , defining\n1\n(14)\np ( x, t ) =\np ( X , t ) with X = m(x)\nw( X )\nIn this way the probabilities of two microstates belonging to the same macrostate are the same and\neach one reaches the maximum at the same t of the corresponding p ( X , t ) .\nWe can therefore speak of a time for S, but we must not forget that this makes sense only in\nreference to an observer O: an observer that constructs different macrostates might observe a\ndifferent time evolution.\n\nAn example.\nIt may seem that the proposed mechanism is artificial and complicated, but we'll see an\nexample in which the above procedure reveals simple and effective.\nConsider a system S whose state x is specified by a number s of bits. We can imagine a state of the\nsystem as a series of boxes s filled with 0 and 1. Suppose we are not able to record a full state x, but\nonly the number n of zeros it contains. The number n specifies then the macrostates of the system.\nThe multiplicity of n is\ns!\nw(n) =\n(15)\nn!( s \u2212 n)!\n4\n\n\fand the corresponding entropy\ns!\n(16)\nH (n) = k log 2\nn!( s \u2212 n)!\nO is, for us, the set of values of n. Observe that there are different values of n with the same\nmultiplicity, e.g. n = 0 and n = s. But if we consider O' = {n | n < s / 2}, then all the macrostates have\ndifferent multiplicity and H is an increasing function of n.\nLet's introduce now the function\nn\uf8f6\n\uf8eb\n(17)\nwhere b = 1 \u2212 2 p\nt = log b \uf8ec1 \u2212 2 \uf8f7\ns\uf8f8\n\uf8ed\nand p is an arbitrary parameter between 0 and 1/2 (excluding extremes). Let's introduce, besides,\nthe function3\n(18)\np ( n ,0 ) = \u03b4 n 0\nif\nt=0\n\n(1 + (1 \u2212 2 p) ) (1 \u2212 (1 \u2212 2 p) )\np(n, t ) = w(n)\nt s\u2212n\n\nt n\n\nif\nt>0\n2s\nIt is easily verified (see Appendix 1) that (7\u221212) are satisfied.\nNote that S does not have, per se, any intrinsic notion of time, but the way in which O stores\nthe states of S, i.e. the way in which he makes a measurement, authorizes us to consider the function\nt a time for O. He will observe that by making a measurement on S he will have a different\nprobability of obtaining the various states while the \"time\" t is changing. As an example, I show in\nFigure 1 some values of p (n, t ) . The system, therefore, according to O, \"evolves\" from the initial\nstate in which all the bits have value 1 to a final state in which half of the bits (as a limit value,\nsince for n = 10 the curve has no maximum) are 0.\n0.4\n\n0.35\n\n0.3\nn=0\nn=1\nn=2\nn=3\nn=4\nn=5\nn=6\nn=7\nn=8\nn=9\nn=10\n\n0.25\n\np(n,t)\n\n0.2\n\n0.15\n\n0.1\n\n0.05\n\n0\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n-0.05\nt\n\nFigure 1:\n\nThe probability for O of finding n zeros depends on the \"time\". The system thus \"evolves\" from an\ninitial state where all the bits are 1 to a final state in which half of the bits are 0. In this graph\nI have set p = 0.06 and s = 20, with s number of zeros and p is a scaling factor for t.\n\nThe \"trajectory\" of S, as seen by O, can be obtained simply by inverting (17):\ns (1 \u2212 b t )\n(19)\nn(t ) =\n2\n3\n\nOne might wonder where these functions come from. I derived them for an earlier work that was not published. This\ndoes not matter at all here: we shall take (17) and (18) as simple definitions: their adequacy will be justified by the\nresults we obtain.\n\n5\n\n\fThe parameter p plays the role of scale factor for t, and determines, therefore, its unit.\nOne might think that this is an entirely abstract contruction and that S does not evolve at all,\nhowever, this model describes just one of the examples that are used to explain the meaning of\nentropy and time evolution: the urn model, proposed in 1907 by P. and T. Ehrenfest [19]. This is a\nbox divided into two parts; initially only one of them contains a number s of particles; the box is\nshaken and every second a particle has a probability p of jumping on the opposite side. Assignining\nto the compartment which initially contains the particles the value 1 and to the other 0, the system is\ndescribed by s bits that specify the position of each particle. The time evolution is precisely that\ndescribed above, which we derived considering solely how O stores the information about S.\n\nOrder and reversibility.\nLet's try to understand better the relationship between information and temporal evolution.\nIt is usually said that a system evolves spontaneously from order to disorder. States of low entropy\nare considered ordered and states of high entropy disordered. You can take the example of a deck\nof playing cards, originally prepared \"in order\" and then mixed repeatedly. Everyone knows that,\nwhile in theory possible, in practice there is no hope of finding the initial state again. We need an\nexternal intervention to prepare the system in this state, after which the system, if we let it \"evolve\"\nspontaneously, will roll away from it relentlessly. Similar considerations were applied to the\nuniverse as a whole, inducing cosmologists to ask: \"What forced the entropy of our world to be so\nlow in the past? (...) To produce a universe similar to that in which we live, the Creator would have\nto aim at an absurdly small volume of phase space of possible universes\" (Roger Penrose in [20]).\nHowever, returning to our example of the deck, we can ask why the initial state should be so\nspecial. Suppose that from an initial state, which we call x0, completely \"ordered\", shuffling a large\nnumber of times we have come to a state fully \"disordered\"; in what are x0 and x1 different? All in\nall, if we mix, we will not expect to find x1 more than to find x0. Imagine turning the cards, which\nare in the state x1, face down and write on each one a progressive number; they appear now\ncompletely \"ordered\", but if we restore x0 they will appear completely \"disordered\". Why are we\nnot willing to consider x1 as the initial state without writing the numbers on the back? The answer is\nthat mixing from x1 we would be unable to distinguish any evolution, because the states are all\nalike, or, more simply, because it is not easy to memorize x1.\n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n(a)\n0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n(b)\n0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0\n(c)\nFigure 2: A configuration in which few details emerge from a background, such as (a) or (b) is easier to memorize\nthan one in which they become numerous, probably because in the process of recording we use a sort of\n\"compression algorithm\", rather that store all the individual bits.\n\nAs another example, consider Figure 2: it shows three different configurations of a\nhypothetical system specified by 20 bits. Try to look at each one for a few seconds and then try to\nrecall them. It is quite obvious that while the first (a) is recorded immediately, the second (b)\nrequires greater attention, while the third (c) is more difficult to remember. Generally we find it\neasier to recognize configurations in which few elements emerge from a homogeneous background,\n6\n\n\fwhile, if their number grows too large, they tend to merge and become themselves an\nindistinguishable background.\nThe reason is probably that in the process of storing our brain takes some strategy of \"compression\"\nto reduce the number of bits needed to reproduce the figure. We can, for example, imagine a\nstrategy of this kind: we record the number of consecutive 0, starting from the first box, then the\nnumber of consecutive 1, then the number of 0 ... and so on, until the end of the boxes. You can see\n(Appendix 2) that this method is efficient when the 0s are \"compact\", but it requires an\nexaggeratedly high number of bits when they are distributed evenly. It is then possible that our\nbrain gives up a faithful storage, losing information on the system. We saw in the previous example\nthat incomplete recording of the state of a system can lead to attribute temporal evolution.\nNow imagine an object on a segment of length L; let l << L be the smallest interval that we\ncan appreciate, so that we can imagine our segment as a set of boxes that are occupied or empty; to\nfix ideas we will say that each box can be white or black. Suppose that our object is long nl\n( l << nl << L ) and fills n boxes. Then a state of our system can be described by s = L / l bits putting, for example, 0 where the box is black - n of which are consecutive zeros. Of course we can\nuse the compression algorithm described above, saving a large amount of memory. The\nrepresentation of data, in this case is lossless, entropy cannot be defined, nor a time evolution for\nthe system in the way described above. But if our object can be separated into particles of length l,\nthe restriction that the zeros are consecutive disappears and data compression quickly becomes\ninefficient for configurations where the particles are finely distributed. It may be convenient then to\ngive up a faithful memorization and use a probabilistic representation. For example, rather then\nreferring to individual white or black boxes, we could build groups and consider, instead of the\nactual configuration of each group, the \"degree of gray\", specified by the number of black boxes in\nthe group. It is clear that the reconstruction of the system state will be approximate and we can only\n\"guess\" what it really is. In this case it should be possible, somehow, to introduce a time function\nfor the system.\nThis would suggest that in those which are referred to as irreversible phenomena there is a\nloss of information due to failure to record completely the state of the system, while the ability to\nknow fully the state of the system leads to reversible phenomena, for which it is not possible to\nidentify any temporal order.\n\nFinal remarks.\nThe observer O in our example, just because he stores information about S in a certain way,\nis led to introduce an order in the set of the states of S. He can associate with each state a number t\nand, choosing to use t as a time for the system, he will find that by making a measurement at a\ngiven time t, he will have a maximum probability of finding some (macro)state. It is easy to see,\nwith a numerical simulation, that the maximum of Figure 1 become very sharp increasing the\nnumber of bits s, so that the temporal evolution of the system is well defined.\nThis mechanism poses some problems, a few of which I will try to highlight.\n\n\u0001\n\nAmong the ingredients to build a time evolution I put a function t : O \u2192 I \u2282 R + . It may\nseem that this means introducing a parameter from the outside, as in Newtonian physics and\nquantum mechanics. Actually, the central role is played by the function p ( X , t ) : it is\nmaximizing it that you can identify the function t. But how can we find p ( X , t ) ? In the\nexample it was found, in fact, so as to reproduce the correct time evolution of the system\nunder consideration. One may ask whether there is a general way to determine it. The\nproblem seems somewhat similar to that of finding the action or Hamilton's characteristic\nfunction: it is fairly easy, in some cases where one already knows the dynamics, to find\n\n7\n\n\fthese functions, but it is not possible to find them in general, in order to derive the dynamic\nas a result.\n\n\u0001\n\nI suggested that the temporal evolution, as a result of the grouping of states of S into\nmacrostates, is due to the inability of our brain to record all the information contained in S.\nCan we hazard a psychological mechanism by which the feeling of time evolution takes\nplace?\nSuppose we have a hundred strings like those shown in Figure 2, which we are required to\nreproduce after observing them for some time and that we get a reward for each\nconfiguration correctly reproduced. Where shall we start from? No doubt from those, such\nas a and b, which we are able to memorize easily and with certainty, while we shall be\ninduced to leave last those for which we can only make attempts. Can we accept that there\nbe some sort of advantage in paying greater attention to unmistakable configurations and\nthat we can interpret this as considering preceding? In this case Barbour would be right in\nsaying that \"it is our brain that plays the movie\" [10], giving us the impression of passing\ntime.\n\n\u0001\n\nI insisted that the time evolution of a system depends on how an observer records the\nsystem. Consider a configuration in which the bits have alternate values: 010101010 ... Of\ncourse if we wanted to use the compression algorithm described above it would be a bad\ndeal, but everybody will agree that this can be considered a \"highly ordered\" state, since it is\nvery easy to memorize. It can easily be traced back to the case where the first half of the bit\nis 0, simply changing the order in which one counts the boxes. If we associate, as before, a\nblack box to 0 and white one to 1, we have a something that can roughly represent the\ndiffusion of two liquids of different colors; it follows that if it is possible to identify for it a\ntime evolution, it should also be possible to have a \"reversed\" evolution, starting from a\nsituation in which the molecules of the two liquids are evenly distributed. But we must\nbelieve that this way of counting is not spontaneous, or it is not convenient for our brain.\n\n8\n\n\fAppendix 1.\nI'll show that the example proposed satisfies the conditions (7\u221212).\nSince the initial macrostate is n = 0, (7\u22129) are immediate.\ns \u2212n\nn\ns\ns\u2212n\nn\n1 + bt\n1 \u2212 bt\n1 s \uf8ebs \uf8f6\n(10) becomes \u2211 w(n)\n= s \u2211 \uf8ec\uf8ec \uf8f7\uf8f7 1 + b t\n1 \u2212 b t = 1 for the binomial formula.\ns\n2\n2 n =0 \uf8ed n \uf8f8\nn =0\n\n(\n\n) (\n\n)\n\n(\n\n) (\n\n)\n\n(\n\n) (\n\n)\n\ns\u2212n\n\nn\n\nTo prove (12) it suffices to annul the derivative respect to t of 1 + b t\n1 \u2212 b t (or its logarithm)\nand note that (17) follows. H is an increasing function of t because w is an increasing function of n\n(for n < s/2) and n is an increasing function of t, as can be seen from (19) (remember that b < 1); its\nexpression follows from (16) e (19):\ns!\nH (t ) = k log 2\nt\n\uf8eb s 1 \u2212 b \uf8f6 \uf8eb s 1 + bt \uf8f6\n\uf8ec\uf8ec\n\uf8f7\uf8f7!\uf8ec\uf8ec\n\uf8f7\uf8f7!\n\uf8ed 2 \uf8f8\uf8ed 2 \uf8f8\n\n(\n\n)\n\n(\n\n)\n\nAppendix 2.\nWe want to memorize the state of a system of s bit.\nConsider the following \"compression algorithm\":\n\u2022\n\u2022\n\nread the first bit: if it is 0 write 0, if it is \u00e8 1, write 1; this requires 1 bit of memory\ncount the n0 consecutive bits equal to the first, write n0; since 1 \u2264 n0 \u2264 s (including the first\n\n\u2022\n\nbit), this requires about log 2 s bits\ncount the next n1 consecutive bits different from the first one, write n1; since 0 \u2264 n1 \u2264 s \u2212 n0\n\n\u2022\n\ncount the next n2 consecutive bits equal to the first one, write n2; since 0 \u2264 n2 \u2264 s \u2212 n0 \u2212 n1\n\n\u2022\n\nthis requires about log 2 (s \u2212 n0 ) bits\n\nthis requires about log 2 (s \u2212 n0 \u2212 n1 ) bits\nproceed until you finish the s bits\n\nWe have used in this way a number of bits\n(A2.1)\n\nc\n\nc\n\nk =0\n\ni=k\n\nn = 1 + \u2211 log 2 \u2211 ni\nc\n\nwhere c is the number of changes in the value of two contiguous bits and\n\n\u2211n\ni =0\n\nc\n\nIn the sum (A2.1) the most important terms are those for which\n\n\u2211n\ni=k\n\ni\n\ni\n\n= s.\n\n\u2248 s , therefore roughly\n\n(A2.2)\nn \u2248 1+ a log 2 s\nwhere a is a number close to the number of groups of bits that are small.\nIt is clear that this algorithm works well if the values are grouped into large homogeneous groups,\notherwise it fails. We can find a limit setting in (A2.2) n \u2264 s and solving for a; ignoring 1\ns\n.\ncompared to s, we find a \u2264\nlog 2 s\n9\n\n\fWe can therefore say that when the bit value changes more than\n\ns\ntimes, the compression\nlog 2 s\n\nmethod becomes useless, because we need more than s bits.\nOf course I do not want to maintain that the method described is that used by our brain to\nstore information, but it is likely that it implements a compression algorithm of some kind and that\nwhen the space required becomes excessive, it prefers, as they say in computer science, a lossy\nprocedure, reaching only a probabilistic knowledge of the system it is observing.\n\nAppendix 3.\nThe system S considered in the example has no other feature than to be described by s bits. It could\ntherefore represent any physical system. We can amuse ourselves by deriving a \"cosmological\"\nresult, imagining that S represents the entire universe. We will base ourselves on two assumptions\n(not so easy to accept, indeed):\n\u2022 the multiplicity of the states represents somehow the \"volume\" of the universe;\n\u2022 our perception of time is somehow analogous to that which comes from knowledge of the\n\"number of zeros\" of the universe, so that we can use (15\u221219).\nFrom (15, 16) we have\n(A3.1)\nw = 2H /k\nWe can obtain an approximate expression of H using Stirling' formula, ln x!\u2248 x ln x \u2212 x , that holds\nwhen n and s \u2212 n are large. We get:\nss\n(A3.2)\nH \u2248 k log 2 n\nn ( s \u2212 n) ( s \u2212 n )\nand therefore\nss\n(A3.3)\nw(t ) \u2248 n\nn ( s \u2212 n) ( s \u2212 n )\nwhere n is given by (19).\n1.4E+30\n1.2E+30\n\nw(t)\n\n1E+30\n8E+29\n6E+29\n4E+29\n2E+29\n0\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n120\n\nt\n\nFigure 3:\n\nUniverse's \"volume\" versus time (in arbitrary units). The universe undergoes a growth of many orders\nof magnitude, which brings it rapidly close to the maximum size, then slows down to become\nasymptotically stationary. Here the number s of bits that describe the universe is set equal to 100 and\nthe parameter p, which determines the time scale, equal to 0.035; s determines the asymptotic value of\nthe volume.\n\nFigure 3 is a graph of w(t). Curiously, in this model the universe would undergo a period of\ninflation, during which it grows many orders of magnitude, reaching almost the maximum volume,\nand then slow its expansion becoming asymptotically stationary. The instant when deceleration\nbegins can be found annulling the second derivative of (A3.3), which is\n\n10\n\n\fn\ns\nn \uf8f9\n\uf8ebs\n\uf8f6\uf8ee\n\uf8ebs\n\uf8f6\n\uf8ebs\n\uf8f6\n&& = w ln 2 b\uf8ec \u2212 n \uf8f7 \uf8efln\nw\n\u2212 \uf8ec \u2212 n\uf8f7\n+ \uf8ec \u2212 n \uf8f7 ln 2\ns \u2212 n \uf8fa\uf8fb\n\uf8ed2\n\uf8f8\uf8f0 s \u2212 n \uf8ed 2\n\uf8f8 n( s \u2212 n) \uf8ed 2\n\uf8f8\nwhere w is given by (A3.3) and n by (19). Annulling the term in parentheses one gets an equation\nthat cannot be solved exactly in R.\nThough with this mechanism inflation arises naturally, probably it cannot be considered\nsatisfactory, because, as Liddle says in [21], \"it must come to an end early enough that the big bang\nsuccesses are not threatened\".\n\n(A3.4)\n\n___________________________\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]\n\nC Rovelli (2008), Forget time, arXiv:quant-ph/9609002v2\nC Rovelli, Statistical mechanics of gravity and the thermodynamical origin of time, Class.\nQuantum Grav. 10 (1993) 1549-1566\nC Rovelli (1999), Quantum spacetime: what do we know?, arXiv:gr-qc/9903045v1\nM Montesinos , C Rovelli (2001), Statistical mechanics of generally covariant quantum\ntheories: A Boltzmann-like approach, arXiv:gr-qc/0002024v2\nC Rovelli (1997), Strings, loops and others: a critical survey of the present approaches to\nquantum gravity, arXiv:gr-qc/9803024v3\nC J Isham (1992), Canonical Quantum Gravity and the Problem of Time, arXiv:grqc/9210011v1\nE Anderson (2010), The Problem of Time in Quantum Gravity arXiv:1009.2157v1 is a\ncomprehensive review, in particular, of the latest developments\nS M Carrol, What if Time Really Exists?, http://www.fqxi.org/data/essay-contestfiles/Carroll_fqxitimecontest.pdf\nJ Barbour (2008), The nature of time, http://www.fqxi.org/data/essay-contestfiles/Barbour_The_Nature_of_Time.pdf\nJ Barbour, The End of Time, Oxford University Press, New York (2000)\nC Rovelli (1998), \"Incerto tempore, incertisque loci\":Can we compute the exact time at\nwhich a quantum measurement happens?, arXiv:quant-ph/9802020v3\nC Rovelli (2008), Relational Quantum Mechanics, arXiv:quant-ph/9609002v2\nJ A Wheeler (1990). Information, physics, quantum: The search for links. In (W. Zurek, ed.)\nComplexity, Entropy, and the Physics of Information. Redwood City, CA: Addison-Wesley.\nCited in D J Chalmers, (1995) Facing up to the Hard Problem of Consciousness, Journal of\nConsciousness Studies 2(3): 200-19.\nCE Shannon (1949), The mathematical theory of communication, University of Illinois\nPress, http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf\nS K Lin (1999), Diversity and Entropy, http://www.mdpi.com/1099-4300/1/1/1\nP C Mariju\u00e0n (2003), Foundations of Information Science, http://www.mdpi.com/10994300/5/2/214/\nJ G Roederer (2003), On the Concept of Information and Its Role in Nature,\nhttp://www.mdpi.com/1099-4300/5/1/3/\nK Huang (1987), Statistical Mechanics, John Wiley & Sons\nP and T Ehrenfest (1907), \u01d7ber zwei bekannte Einw\u00e4nde gegen das Boltzmannsche HTheorem, Phys. Zeit. 8, 311-314\nR Penrose (1989), TheEmperor's New Mind, Oxford University Press\nA R Liddle (1999), An introduction to cosmological inflation,\nhttp://arxiv.org/PS_cache/astro-ph/pdf/9901/9901124v1.pdf\n\n11\n\n\f"}