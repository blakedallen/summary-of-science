{"id": "http://arxiv.org/abs/physics/0310059v1", "guidislink": true, "updated": "2003-10-13T20:12:26Z", "updated_parsed": [2003, 10, 13, 20, 12, 26, 0, 286, 0], "published": "2003-10-13T20:12:26Z", "published_parsed": [2003, 10, 13, 20, 12, 26, 0, 286, 0], "title": "Comparing the ensemble mean and the ensemble standard deviation as\n  inputs for probabilistic medium-range temperature forecasts", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0510011%2Cphysics%2F0510236%2Cphysics%2F0510219%2Cphysics%2F0510136%2Cphysics%2F0510134%2Cphysics%2F0510050%2Cphysics%2F0510131%2Cphysics%2F0510162%2Cphysics%2F0510112%2Cphysics%2F0510044%2Cphysics%2F0510045%2Cphysics%2F0510233%2Cphysics%2F0510127%2Cphysics%2F0510087%2Cphysics%2F0510207%2Cphysics%2F0510203%2Cphysics%2F0510256%2Cphysics%2F0510260%2Cphysics%2F0510218%2Cphysics%2F0510192%2Cphysics%2F0510223%2Cphysics%2F0510237%2Cphysics%2F0510155%2Cphysics%2F0510252%2Cphysics%2F0510140%2Cphysics%2F0510068%2Cphysics%2F0310104%2Cphysics%2F0310009%2Cphysics%2F0310074%2Cphysics%2F0310118%2Cphysics%2F0310127%2Cphysics%2F0310090%2Cphysics%2F0310031%2Cphysics%2F0310110%2Cphysics%2F0310014%2Cphysics%2F0310111%2Cphysics%2F0310013%2Cphysics%2F0310121%2Cphysics%2F0310006%2Cphysics%2F0310001%2Cphysics%2F0310040%2Cphysics%2F0310008%2Cphysics%2F0310049%2Cphysics%2F0310069%2Cphysics%2F0310025%2Cphysics%2F0310096%2Cphysics%2F0310022%2Cphysics%2F0310076%2Cphysics%2F0310027%2Cphysics%2F0310087%2Cphysics%2F0310081%2Cphysics%2F0310077%2Cphysics%2F0310041%2Cphysics%2F0310054%2Cphysics%2F0310017%2Cphysics%2F0310093%2Cphysics%2F0310119%2Cphysics%2F0310026%2Cphysics%2F0310007%2Cphysics%2F0310117%2Cphysics%2F0310148%2Cphysics%2F0310100%2Cphysics%2F0310064%2Cphysics%2F0310050%2Cphysics%2F0310157%2Cphysics%2F0310012%2Cphysics%2F0310088%2Cphysics%2F0310036%2Cphysics%2F0310072%2Cphysics%2F0310138%2Cphysics%2F0310020%2Cphysics%2F0310133%2Cphysics%2F0310144%2Cphysics%2F0310106%2Cphysics%2F0310010%2Cphysics%2F0310149%2Cphysics%2F0310082%2Cphysics%2F0310122%2Cphysics%2F0310016%2Cphysics%2F0310143%2Cphysics%2F0310131%2Cphysics%2F0310091%2Cphysics%2F0310094%2Cphysics%2F0310059%2Cphysics%2F0310085%2Cphysics%2F0310018%2Cphysics%2F0310155%2Cphysics%2F0310067%2Cphysics%2F0310145%2Cphysics%2F0310063%2Cphysics%2F0310051%2Cphysics%2F0310161%2Cphysics%2F0310160%2Cphysics%2F0310034%2Cphysics%2F0310033%2Cphysics%2F0310058%2Cphysics%2F0310109%2Cphysics%2F0310048%2Cphysics%2F0310060%2Cphysics%2F0310003%2Cphysics%2F0310021&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Comparing the ensemble mean and the ensemble standard deviation as\n  inputs for probabilistic medium-range temperature forecasts"}, "summary": "We ask the following question: what are the relative contributions of the\nensemble mean and the ensemble standard deviation to the skill of a\nsite-specific probabilistic temperature forecast? Is it the case that most of\nthe benefit of using an ensemble forecast to predict temperatures comes from\nthe ensemble mean, or from the ensemble spread, or is the benefit derived\nequally from the two? The answer is that one of the two is much more useful\nthan the other.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0510011%2Cphysics%2F0510236%2Cphysics%2F0510219%2Cphysics%2F0510136%2Cphysics%2F0510134%2Cphysics%2F0510050%2Cphysics%2F0510131%2Cphysics%2F0510162%2Cphysics%2F0510112%2Cphysics%2F0510044%2Cphysics%2F0510045%2Cphysics%2F0510233%2Cphysics%2F0510127%2Cphysics%2F0510087%2Cphysics%2F0510207%2Cphysics%2F0510203%2Cphysics%2F0510256%2Cphysics%2F0510260%2Cphysics%2F0510218%2Cphysics%2F0510192%2Cphysics%2F0510223%2Cphysics%2F0510237%2Cphysics%2F0510155%2Cphysics%2F0510252%2Cphysics%2F0510140%2Cphysics%2F0510068%2Cphysics%2F0310104%2Cphysics%2F0310009%2Cphysics%2F0310074%2Cphysics%2F0310118%2Cphysics%2F0310127%2Cphysics%2F0310090%2Cphysics%2F0310031%2Cphysics%2F0310110%2Cphysics%2F0310014%2Cphysics%2F0310111%2Cphysics%2F0310013%2Cphysics%2F0310121%2Cphysics%2F0310006%2Cphysics%2F0310001%2Cphysics%2F0310040%2Cphysics%2F0310008%2Cphysics%2F0310049%2Cphysics%2F0310069%2Cphysics%2F0310025%2Cphysics%2F0310096%2Cphysics%2F0310022%2Cphysics%2F0310076%2Cphysics%2F0310027%2Cphysics%2F0310087%2Cphysics%2F0310081%2Cphysics%2F0310077%2Cphysics%2F0310041%2Cphysics%2F0310054%2Cphysics%2F0310017%2Cphysics%2F0310093%2Cphysics%2F0310119%2Cphysics%2F0310026%2Cphysics%2F0310007%2Cphysics%2F0310117%2Cphysics%2F0310148%2Cphysics%2F0310100%2Cphysics%2F0310064%2Cphysics%2F0310050%2Cphysics%2F0310157%2Cphysics%2F0310012%2Cphysics%2F0310088%2Cphysics%2F0310036%2Cphysics%2F0310072%2Cphysics%2F0310138%2Cphysics%2F0310020%2Cphysics%2F0310133%2Cphysics%2F0310144%2Cphysics%2F0310106%2Cphysics%2F0310010%2Cphysics%2F0310149%2Cphysics%2F0310082%2Cphysics%2F0310122%2Cphysics%2F0310016%2Cphysics%2F0310143%2Cphysics%2F0310131%2Cphysics%2F0310091%2Cphysics%2F0310094%2Cphysics%2F0310059%2Cphysics%2F0310085%2Cphysics%2F0310018%2Cphysics%2F0310155%2Cphysics%2F0310067%2Cphysics%2F0310145%2Cphysics%2F0310063%2Cphysics%2F0310051%2Cphysics%2F0310161%2Cphysics%2F0310160%2Cphysics%2F0310034%2Cphysics%2F0310033%2Cphysics%2F0310058%2Cphysics%2F0310109%2Cphysics%2F0310048%2Cphysics%2F0310060%2Cphysics%2F0310003%2Cphysics%2F0310021&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We ask the following question: what are the relative contributions of the\nensemble mean and the ensemble standard deviation to the skill of a\nsite-specific probabilistic temperature forecast? Is it the case that most of\nthe benefit of using an ensemble forecast to predict temperatures comes from\nthe ensemble mean, or from the ensemble spread, or is the benefit derived\nequally from the two? The answer is that one of the two is much more useful\nthan the other."}, "authors": ["Stephen Jewson"], "author_detail": {"name": "Stephen Jewson"}, "author": "Stephen Jewson", "links": [{"href": "http://arxiv.org/abs/physics/0310059v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/physics/0310059v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "physics.ao-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.ao-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/physics/0310059v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/physics/0310059v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:physics/0310059v1 [physics.ao-ph] 13 Oct 2003\n\nComparing the ensemble mean and the ensemble standard\ndeviation as inputs for probabilistic medium-range temperature\nforecasts\nStephen Jewson\u2217\nRMS, London, United Kingdom\nSeptember 18, 2018\n\nAbstract\nWe ask the following question: what are the relative contributions of the ensemble mean and the\nensemble standard deviation to the skill of a site-specific probabilistic temperature forecast? Is it the\ncase that most of the benefit of using an ensemble forecast to predict temperatures comes from the\nensemble mean, or from the ensemble spread, or is the benefit derived equally from the two? The\nanswer is that one of the two is much more useful than the other.\n\n1\n\nIntroduction\n\nWe consider medium range probabilistic site-specific forecasts of temperatures. There are a number of\nmethods that can be used to produce such forecasts. These methods differ in terms of a) the underlying\nnumerical forecast model, b) the predictors that are taken from the forecast model and c) the statistical\nmethods used to transform these predictors into predictions. For instance, the underlying model could be\nthe ECMWF or the NCEP model (inter alia); the underlying predictors could be the ensemble mean and\nthe standard deviation or just the ensemble mean (inter alia) and the statistical transform used could\nbe based on the rank histogram or spread regression (inter alia).\nMuch work has gone into trying to understand the sources of medium range forecast skill, and to improve\nmedium range forecasts. Some of this has aimed at determining which of the various numerical models\nproduce the best predictors (point \"a\" above). There has also been some investigation into which of the\nvarious statistical transforms are the most appropriate (point \"c\" above). We will now address a third\nquestion but closely related question: what is the relative importance of the different predictors that\ncan be derived from the numerical models (point \"b\" above). In particular, we will consider the relative\nimportance of the mean and the standard deviation of ensemble forecast temperatures, when used to\nmake a site-specific probabilistic forecast. We consider only the mean and the standard deviation, rather\nthan all the individual ensemble members, since, for the dataset we will use, it has been shown by Jewson\n(2003b) that there seems to be no useful information in the ensemble beyond the mean and the spread.\nThe statistical models we use to address this question are all derived from the spread regression model\nof Jewson et al. (2003). This model takes the mean and the standard deviation from an ensemble forecast\nand converts them into a probabilistic forecast. This model is particularly useful for addressing the\nquestion of relative importance of the different predictors since it allows us to turn each of them on and\noff rather easily.\nWe do not think that it is a priori obvious which of the ensemble mean and ensemble spread are more\nuseful for making a probabilistic forecast. For instance, one can imagine dynamical systems for which\nthe expectation varies very little, but for which the uncertainty varies a lot. Similarly one can imagine\ndynamical systems for which the expectation varies a lot, but for which the uncertainty varies little. In the\nfirst of these it is likely that forecasts of the expectation are more important, while in the latter it is likely\nthat forecasts of the uncertainty would be more important. We see, therefore, that our question is, in\npart, a question about the dynamics of the atmosphere. One can also imagine a particular forecast system\nthat, for whatever reason, does very well in predicting the expectation, but very poorly in predicting the\nuncertainty, or vice versa. From this we see that our question is also a question about specific forecast\nsystems.\n\u2217 Correspondence\n\naddress: RMS, 10 Eastcheap, London, EC3M 1AJ, UK. Email: x@stephenjewson.com\n\n\fIn section 2 we describe the data we use for this study. In section 3 we describe the methodology and the\nstatistical models we will use to address this question. In section 4 we present the results of our analysis\nand in section 5 we discuss the implications.\n\n2\n\nData\n\nWe will base our analyses on one year of ensemble forecast data for the weather station at London's\nHeathrow airport, WMO number 03772. The forecasts are predictions of the daily average temperature,\nand the target days of the forecasts run from 1st January 2002 to 31st December 2002. The forecast was\nproduced from the ECMWF model (Molteni et al., 1996) and downscaled to the airport location using\na simple interpolation routine prior to our analysis. There are 51 members in the ensemble. We will\ncompare these forecasts to the quality controlled climate values of daily average temperature for the same\nlocation as reported by the UKMO.\nThere is no guarantee that the forecast system was held constant throughout this period, and as a result\nthere is no guarantee that the forecasts are in any sense stationary, quite apart from issues of seasonality.\nThis is clearly far from ideal with respect to our attempts to build statistical interpretation models on\npast forecast data but is, however, unavoidable: this is the data we have to work with.\nThroughout this paper all equations and all values are in terms of double anomalies (have had both the\nseasonal mean and the seasonal standard deviation removed). Removing the seasonal standard deviation\nremoves most of the seasonality in the forecast error statistics, and partly justifies the use of non-seasonal\nparameters in the statistical models for temperature that we propose.\n\n3\n\nModels\n\nWe will address the question of whether the useful information in ensemble forecasts comes to a greater\nextent from the ensemble mean or from the ensemble spread by considering the site-specific forecasts for\ntemperature at London Heathrow described above. Our forecasts are produced by four ensemble interpretation models, which is the name we give to the statistical models that take the non-probabilistic predictors\nfrom the ensemble forecasts and convert them into probability density functions for the predicted temperatures. Such models are essential if one wants to produce probability forecasts from ensemble output.\nThe models we choose are ideally suited for answering the question at hand because they allow us to turn\non and off the various sources of information in the ensemble very easily and see what impact that has\non the skill of the resulting forecast.\nThe simplest model we consider consists of the use of linear regression between a single member of the\nensemble and the observed temperature. The regression model serves to correct biases, to adjust the\namplitude of the variability, and to predict the uncertainty. We will write this model as:\nTi \u223c N (\u03b1 + \u03b2xi , \u03b3)\n\n(1)\n\nwhere Ti is the double anomaly temperature on day i and xi is the ensemble member on day i. The skill\nof this model gives an indication of the ability of single integrations of the numerical model to predict\nthe distribution of future atmospheric states. It is a baseline that the ensemble forecast should beat if it\nhas any value.\nOur second model is the same as the first, but the predictor is the ensemble mean, rather than a single\nensemble member. We write this model as:\nTi \u223c N (\u03b1 + \u03b2mi , \u03b3)\n\n(2)\n\nwhere mi is the ensemble mean on day i. This model should hopefully perform better than the first\nmodel.\nOur third model is an extension of the first model to include the ensemble standard deviation as a\npredictor of the uncertainty:\nTi \u223c N (\u03b1 + \u03b2xi , \u03b3 + \u03b4si )\n\n(3)\n\nwhere si is the ensemble standard deviation.\nFinally our fourth model combines the second and the third models by using the ensemble mean as a\npredictor for the temperature, and the ensemble standard deviation as a predictor for the uncertainty\naround the temperature. This is the spread regression model of Jewson et al. (2003).\n\n\fTi \u223c N (\u03b1 + \u03b2mi , \u03b3 + \u03b4si )\n\n(4)\n\nThese models have been designed to allow us to answer the question at hand. For instance, comparison\nof the results from model one and model two, or between model three and model four, gives the benefit\nof using the ensemble mean versus a single integration. Comparison between model one and model three,\nor between model two and model four, gives the benefit of using the ensemble spread versus only using\npast forecast error statistics.\nWe fit all of these models using the standard maximum likelihood method from classical statistics. More\ndetails of this method as applied to this problem are given in Jewson (2003c).\n\n3.1\n\nScoring the models\n\nIn order to compare the results from our four models we need a measure for how good the resulting\nprobabilistic forecast is. To our knowledge only two scores have been suggested in the literature for\ncomparing continuous probabilistic forecasts: the \"ignorance\" by Roulston and Smith (2002), and the\nlikelihood, by Jewson et al. (2003). It turns out that these two are, for this case, more or less the same\nthing. We will therefore use this score to compare our forecasts, in the form of the log-likelihood.\n\n4\n4.1\n\nResults\nParameter values\n\nWe first look at the parameter values for our various statistical models. Figure 1 shows the values of\nalpha for the four models. The two models that are based on the single ensemble member have very\nsimilar values of alpha, while the two models that are based on the ensemble mean also have similar\nvalues. The values from the models based on the single ensemble member are somewhat smaller.\nFigure 2 shows the values of beta for the four models. The values from the four models pair up in the\nsame way as for the values of alpha, with the values for the single model based forecasts being much\nlower than the values for the ensemble mean based forecasts.\nWe can understand these values of beta as follows. We can consider a single integration of the numerical\nmodel to consist of the ensemble mean plus a noise term. The noise terms are at least somewhat different\nfor the different members of the ensemble. Thus, in forming the ensemble mean the noise terms tend to\ncancel out and the overall noise is smaller. When we regress a single member onto observed temperatures,\nthe regression coefficient is rather small because of the large noise term. When we regress the ensemble\nmean onto observed temperatures, the regression coefficient is much larger, because the noise level is\nlower. The mathematics of this effect have been given in detail by Jewson and Ziehmann (2002).\nThe value of alpha can be understood in terms of the values of beta. Alpha arises due to a combination\nof bias, and the beta term. We would expect the bias to be the same for a single ensemble member and\nthe ensemble mean: the differences we see can be explained as being due to the differences in the beta.\nFigure 3 shows the values of gamma for the four models. Because gamma plays a slightly different role\nin the different models, there is no single physical interpretation of what it means. In the two regression\nmodels gamma represents the uncertainty: we see that the uncertainty is greater in the model that\nonly uses the single ensemble member, as would be expected. In the spread regression models, gamma\nrepresents only a part of the total uncertainty, and can only be interpreted in combination with delta,\nwhich is shown in figure 4. Confidence intervals for the value of delta in the spread regression model\nare given by Jewson et al. (2003), and show that there is significant sampling uncertainty about these\nestimates. This is not surprising since delta is related to the second moment of the second moment of\nobserved temperatures. This sampling uncertainty is reflected in figure 4 by the jaggedness of the lines.\nNevertheless we can see a general trend towards lower values of delta at high leads for both models.\n\n4.2\n\nTotal Uncertainty\n\nComparing the gamma and delta parameter values for the four statistical models is somewhat dissatisfactory since, as we have seen, gamma does not have consistent interpretation across the four models. It\nmakes more sense to compare the total uncertainty predicted by the four models. In figures 5 and figure 6\nwe compare the mean and the standard deviation of this predicted uncertainty respectively. In figure 5\nwe see that the two models based on the ensemble mean have much lower mean uncertainty than the two\nmodels based on the single integration, as would be expected. The ensemble mean gives significantly better forecasts than individual ensemble members, especially at longer leads. Figure 6 shows the variability\n\n\fin the predicted uncertainty for the two spread regression models. The two regression models, both of\nwhich ignore the ensemble spread, are not shown because they predict constant levels of uncertainty. We\nsee that the two spread regression models predict roughly the same levels of variability in the uncertainty.\nThere are some differences between the two, but we suspect these differences are mostly due to sampling\nerror.\n\n4.3\n\nLikelihood scores\n\nFinally we come to the results that answer the question that we set out to address: what are the relative\ncontributions of the ensemble mean and the ensemble standard deviation to the skill of the final forecast?\nWe present the comparisons in terms of the negative of the log-likehood, for which small values represent\na good forecast. We compare the models in pairs to isolate individual effects, as described in section 3.\nFigure 7, top left panel, shows a comparison between models one and two. This comparison is designed\nto illustrate the benefit of using the ensemble mean over using a single ensemble member. We see a very\nlarge benefit at all lead times. For instance, the ensemble mean based forecast at lead 6 is roughly as\ngood as the single member based forecast at lead 4. We are left in no doubt that use of the ensemble\nmean is very beneficial for our probabilistic forecast. The lower left panel shows a comparison between\nmodels three and four. This comparison also shows the benefit of using the ensemble mean over using\na single ensemble member, but now for the models that also use the ensemble spread. The size of the\nbenefit is roughly the same as before.\nThe right hand panels show the effects of using the ensemble spread. The top right panel shows the\nbenefit of using the ensemble spread in the models that do not use the ensemble mean, and the lower\nright panel shows the benefit of using the ensemble spread in the models that do use the ensemble mean.\nThe results are roughly the same: in both cases we see only a very marginal benefit from using the\nensemble spread.\nFigure 8 now shows the differences that we see in the benefit due to the ensemble mean and the ensemble\nspread. There are a number of ways one could try and quantify this effect: we choose to look at the\nnumerical differences in the log-likelihood. The top left hand panel shows the differences in the loglikelihood caused by the use of the ensemble mean as a predictor (from comparing models 1 and 2).\nThe values lie between 10 and 70. The top right hand panel shows the differences in the log-likelihood\ncaused by the use of the ensemble spread as a predictor (from comparing models 2 and 4). The values lie\nbetween zero and 10. The lower panels show the ratios of these log-likelihood differences, which quantify\nhow much more useful the ensemble mean is as a predictor than the ensemble spread. We see the values\nfor this ratio lie between four and 100, and increase at longer lead times. The ensemble spread is relatively\nmost useful at the shortest lead times, while the ensemble mean is relatively most useful at the longest\nlead times.\n\n5\n\nDiscussion\n\nWe have investigated the relative contributions of the ensemble mean and the ensemble standard deviation\nto the skill of probabilistic temperature forecasts. This was done by using four different statistical models\nto produce the probabilistic forecasts. These models differ in terms of whether they use the ensemble\nmean or a single ensemble member, and whether they use the ensemble spread or predict uncertainty\npurely using past forecast error statistics.\nThe results are very clear: we see that the ensemble mean is much more important than the ensemble\nspread. The difference is least at short leads, where the ensemble mean is only about four times as\nuseful as the ensemble spread (using the particular measure that we have chosen). At medium leads the\nensemble mean is between 10 and 20 times as useful as the ensemble spread, and at the longest lead the\nensemble mean is roughly 100 times more useful than the ensemble spread.\nThere are a number of caveats to this study. In particular, we have only used data from one station.\nDifferent results would probably be obtained at other stations. We have also only used one year of\nforecast data, and we have seen that this leads to rather noisy estimates for some of our parameters. Our\nestimates would be more accurate if we had more data, as long as that data were stationary. Having said\nthat, our main result, which is that the ensemble mean is much more useful than the ensemble standard\ndeviation, seems to be so clear that we doubt whether it would change even if we did repeat this analysis\nfor a number of other stations and with more data.\nFinally, we note that have performed all of our analysis in-sample. This is not ideal: if possible, out of\nsample results are to be preferred. However, Jewson (2003a) has shown that out of sample one cannot\ndetect any beneficial effects of ensemble spread whatsoever using this data-set, and so it would not be\n\n\fpossible to perform this study if we were to do it out of sample. By using an in-sample analysis, and\nusing statistical models with small numbers of parameters to minimize overfitting and artificial skill, we\nhave been able to shed light on a question that could not be otherwise addressed.\n\n\f1.0\n0.8\n0.6\n0.4\n0.0\n\n0.2\n\nalpha\n\n2\n\n4\n\n6\n\n8\n\n10\n\nlead\n\nFigure 1: The values of alpha for the four models described in the text. Model 1 (solid line), model 2\n(dashed line), model 3 (dotted line) and model 4 (dot-dashed line).\n\n\f1.0\n0.8\n0.6\n0.0\n\n0.2\n\n0.4\n\nbeta\n\n2\n\n4\n\n6\n\n8\n\n10\n\nlead\n\nFigure 2: The values of beta for the four models described in the text. Model 1 (solid line), model 2\n(dashed line), model 3 (dotted line) and model 4 (dot-dashed line).\n\n\f1.2\n1.0\n0.8\n0.6\n0.0\n\n0.2\n\n0.4\n\ngamma\n\n2\n\n4\n\n6\n\n8\n\n10\n\nlead\n\nFigure 3: The values of gamma for the four models described in the text. Model 1 (solid line), model 2\n(dashed line), model 3 (dotted line) and model 4 (dot-dashed line).\n\n\f1.2\n1.0\n0.8\n0.6\n0.0\n\n0.2\n\n0.4\n\ndelta\n\n2\n\n4\n\n6\n\n8\n\n10\n\nlead\n\nFigure 4: The values of delta for two of the four models described in the text. Model 2 (solid line) and\nmodel 4 (dashed line).\n\n\f1.2\n1.0\n0.8\n0.6\n0.4\n0.0\n\n0.2\n\nmean uncertainty\n\n2\n\n4\n\n6\n\n8\n\n10\n\nlead\n\nFigure 5: The mean level of uncertainty predicted by the four models described in the text. Model 1\n(solid line), model 2 (dashed line), model 3 (dotted line) and model 4 (dot-dashed line).\n\n\f0.20\n0.15\n0.10\n0.05\n0.00\n\nsd of uncertainty\n\n2\n\n4\n\n6\n\n8\n\n10\n\nlead\n\nFigure 6: The standard deviation of the level of uncertainty predicted by two of the four models described\nin the text. Model 2 (solid line) and model 4 (dashed line).\n\n\f2\n\n4\n\n4\n\n6\n\n6\n\n8\n\n8\n\n10\n\n10\n\n2\n\n2\n\n4\n\n4\n\n6\n\n6\n\n8\n\n8\n\n10\n\n10\n\nFigure 7: Comparisons between the minus log-likelihood scores from the four models described in the\ntext.\n\n2\n\n500\n400\n300\n200\n500\n400\n300\n200\n\n1 vs 3\n2 vs 4\n\n500\n400\n300\n200\n500\n400\n300\n200\n\n1 vs 2\n2 vs 3\n\n\f10\n8\n6\n4\n0\n\n2\n\nbenefit due to ens sd\n\n60\n40\n\nbenefit due to ens mean\n\n20\n\n4\n\n6\n\n8\n\n10\n\n2\n\n4\n\n6\n\n8\n\n10\n\n2\n\n4\n\n6\n\n8\n\n10\n\n2\n\n4\n\n6\n\n8\n\n10\n\n0\n\n20 40 60 80\n\nratio\n\n10\n0\n\n5\n\nratio\n\n15\n\n20\n\n2\n\nFigure 8: Comparisons between the minus log-likelihood scores from the four models described in the\ntext. The top left panel shows the differences in the log-likelihood between models 1 and 2, while the\ntop right panel shows the differences between models 2 and 4. The lower panels show the ratio of these\ndifferences (with different vertical scales).\n\nReferences\nS Jewson. Closed-form expressions for the pricing of weather derivatives: Part 1 - the expected payoff.\nSSRN, 2003a.\nS Jewson. Comparing the ensemble mean and the ensemble standard deviation as inputs for probabilistic\ntemperature forecasts. Arxiv, 2003b.\nS Jewson. Moment based methods for ensemble assessment and calibration. Arxiv, 2003c.\nS Jewson, A Brix, and C Ziehmann. A new framework for the assessment and calibration of ensemble\ntemperature forecasts. ASL, 2003. Submitted.\nS Jewson and C Ziehmann. Weather swap pricing and the optimum size of medium range forecast\nensembles. Weather and Forecasting, 2002.\nF Molteni, R Buizza, T Palmer, and T Petroliagis. The ECMWF ensemble prediction system: Methodology and validation. Q. J. R. Meteorol. Soc., 122:73\u2013119, 1996.\nM Roulston and L Smith. Evaluating probabilistic forecasts using information theory. Mon. Wea. Rev.,\n130:1653\u20131660, 2002.\n\n\f"}