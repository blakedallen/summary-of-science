{"id": "http://arxiv.org/abs/cs/0306072v1", "guidislink": true, "updated": "2003-06-13T18:57:35Z", "updated_parsed": [2003, 6, 13, 18, 57, 35, 4, 164, 0], "published": "2003-06-13T18:57:35Z", "published_parsed": [2003, 6, 13, 18, 57, 35, 4, 164, 0], "title": "The EU DataGrid Workload Management System: towards the second major\n  release", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0306070%2Ccs%2F0306009%2Ccs%2F0306097%2Ccs%2F0306127%2Ccs%2F0306047%2Ccs%2F0306022%2Ccs%2F0306038%2Ccs%2F0306085%2Ccs%2F0306062%2Ccs%2F0306094%2Ccs%2F0306010%2Ccs%2F0306080%2Ccs%2F0306076%2Ccs%2F0306102%2Ccs%2F0306130%2Ccs%2F0306107%2Ccs%2F0306131%2Ccs%2F0306136%2Ccs%2F0306045%2Ccs%2F0306052%2Ccs%2F0306029%2Ccs%2F0306128%2Ccs%2F0306055%2Ccs%2F0306110%2Ccs%2F0306091%2Ccs%2F0306090%2Ccs%2F0306061%2Ccs%2F0306124%2Ccs%2F0306030%2Ccs%2F0306065%2Ccs%2F0306036%2Ccs%2F0306093%2Ccs%2F0306037%2Ccs%2F0306088%2Ccs%2F0306082%2Ccs%2F0306120%2Ccs%2F0306066%2Ccs%2F0306075%2Ccs%2F0306024%2Ccs%2F0306084%2Ccs%2F0306032%2Ccs%2F0306060%2Ccs%2F0306039%2Ccs%2F0306048%2Ccs%2F0306072%2Ccs%2F0306064%2Ccs%2F0306017%2Ccs%2F0306014%2Ccs%2F0306083%2Ccs%2F0306112%2Ccs%2F0306031%2Ccs%2F0306028%2Ccs%2F0306106%2Ccs%2F0306098%2Ccs%2F0306074%2Ccs%2F0306049%2Ccs%2F0306095%2Ccs%2F0306096%2Ccs%2F0306135%2Ccs%2F0306057%2Ccs%2F0306113%2Ccs%2F0306108%2Ccs%2F0306034%2Ccs%2F0306081%2Ccs%2F0306086%2Ccs%2F0306105%2Ccs%2F0306033%2Ccs%2F0306114%2Ccs%2F0306125%2Ccs%2F0306011%2Ccs%2F0306005%2Ccs%2F0306043%2Ccs%2F0306021%2Ccs%2F0306089%2Ccs%2F0306092%2Ccs%2F0306053%2Ccs%2F0306059%2Ccs%2F0306035%2Ccs%2F0306058%2Ccs%2F0306115%2Ccs%2F0306025%2Ccs%2F0306018%2Ccs%2F0607100%2Ccs%2F0607082%2Ccs%2F0607049%2Ccs%2F0607021%2Ccs%2F0607020%2Ccs%2F0607036%2Ccs%2F0607059%2Ccs%2F0607096%2Ccs%2F0607045%2Ccs%2F0607057%2Ccs%2F0607136%2Ccs%2F0607129%2Ccs%2F0607128%2Ccs%2F0607113%2Ccs%2F0607023%2Ccs%2F0607043%2Ccs%2F0607145%2Ccs%2F0607005%2Ccs%2F0607014&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The EU DataGrid Workload Management System: towards the second major\n  release"}, "summary": "In the first phase of the European DataGrid project, the 'workload\nmanagement' package (WP1) implemented a working prototype, providing users with\nan environment allowing to define and submit jobs to the Grid, and able to find\nand use the ``best'' resources for these jobs. Application users have now been\nexperiencing for about a year now with this first release of the workload\nmanagement system. The experiences acquired, the feedback received by the user\nand the need to plug new components implementing new functionalities, triggered\nan update of the existing architecture. A description of this revised and\ncomplemented workload management system is given.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0306070%2Ccs%2F0306009%2Ccs%2F0306097%2Ccs%2F0306127%2Ccs%2F0306047%2Ccs%2F0306022%2Ccs%2F0306038%2Ccs%2F0306085%2Ccs%2F0306062%2Ccs%2F0306094%2Ccs%2F0306010%2Ccs%2F0306080%2Ccs%2F0306076%2Ccs%2F0306102%2Ccs%2F0306130%2Ccs%2F0306107%2Ccs%2F0306131%2Ccs%2F0306136%2Ccs%2F0306045%2Ccs%2F0306052%2Ccs%2F0306029%2Ccs%2F0306128%2Ccs%2F0306055%2Ccs%2F0306110%2Ccs%2F0306091%2Ccs%2F0306090%2Ccs%2F0306061%2Ccs%2F0306124%2Ccs%2F0306030%2Ccs%2F0306065%2Ccs%2F0306036%2Ccs%2F0306093%2Ccs%2F0306037%2Ccs%2F0306088%2Ccs%2F0306082%2Ccs%2F0306120%2Ccs%2F0306066%2Ccs%2F0306075%2Ccs%2F0306024%2Ccs%2F0306084%2Ccs%2F0306032%2Ccs%2F0306060%2Ccs%2F0306039%2Ccs%2F0306048%2Ccs%2F0306072%2Ccs%2F0306064%2Ccs%2F0306017%2Ccs%2F0306014%2Ccs%2F0306083%2Ccs%2F0306112%2Ccs%2F0306031%2Ccs%2F0306028%2Ccs%2F0306106%2Ccs%2F0306098%2Ccs%2F0306074%2Ccs%2F0306049%2Ccs%2F0306095%2Ccs%2F0306096%2Ccs%2F0306135%2Ccs%2F0306057%2Ccs%2F0306113%2Ccs%2F0306108%2Ccs%2F0306034%2Ccs%2F0306081%2Ccs%2F0306086%2Ccs%2F0306105%2Ccs%2F0306033%2Ccs%2F0306114%2Ccs%2F0306125%2Ccs%2F0306011%2Ccs%2F0306005%2Ccs%2F0306043%2Ccs%2F0306021%2Ccs%2F0306089%2Ccs%2F0306092%2Ccs%2F0306053%2Ccs%2F0306059%2Ccs%2F0306035%2Ccs%2F0306058%2Ccs%2F0306115%2Ccs%2F0306025%2Ccs%2F0306018%2Ccs%2F0607100%2Ccs%2F0607082%2Ccs%2F0607049%2Ccs%2F0607021%2Ccs%2F0607020%2Ccs%2F0607036%2Ccs%2F0607059%2Ccs%2F0607096%2Ccs%2F0607045%2Ccs%2F0607057%2Ccs%2F0607136%2Ccs%2F0607129%2Ccs%2F0607128%2Ccs%2F0607113%2Ccs%2F0607023%2Ccs%2F0607043%2Ccs%2F0607145%2Ccs%2F0607005%2Ccs%2F0607014&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In the first phase of the European DataGrid project, the 'workload\nmanagement' package (WP1) implemented a working prototype, providing users with\nan environment allowing to define and submit jobs to the Grid, and able to find\nand use the ``best'' resources for these jobs. Application users have now been\nexperiencing for about a year now with this first release of the workload\nmanagement system. The experiences acquired, the feedback received by the user\nand the need to plug new components implementing new functionalities, triggered\nan update of the existing architecture. A description of this revised and\ncomplemented workload management system is given."}, "authors": ["G. Avellino", "S. Barale", "S. Beco", "B. Cantalupo", "D. Colling", "F. Giacomini", "A. Gianelle", "A. Guarise", "A. Krenek", "D. Kouril", "A. Maraschini", "L. Matyska", "M. Mezzadri", "S. Monforte", "M. Mulac", "F. Pacini", "M. Pappalardo", "R. Peluso", "J. Pospisil", "F. Prelz", "E. Ronchieri", "M. Ruda", "L. Salconi", "Z. Salvet", "M. Sgaravatto", "J. Sitera", "A. Terracina", "M. Vocu", "A. Werbrouck"], "author_detail": {"name": "A. Werbrouck"}, "author": "A. Werbrouck", "arxiv_comment": "Talk from the 2003 Computing in High Energy and Nuclear Physics\n  (CHEP03), La Jolla, Ca, USA, March 2003", "links": [{"href": "http://arxiv.org/abs/cs/0306072v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0306072v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "H.3.4", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0306072v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0306072v1", "journal_reference": null, "doi": null, "fulltext": "CHEP 2003, UC San Diego, March 24-28, 2003\n\n1\n\nThe EU DataGrid Workload Management System: towards the second\nmajor release\nG. Avellino, S. Beco, B. Cantalupo, A. Maraschini, F. Pacini, A. Terracina\nDATAMAT S.p.A\nS. Barale, A. Guarise, A. Werbrouck\nINFN, Sezione di Torino\nD. Colling\nImperial College London\nF. Giacomini, E. Ronchieri\nINFN, CNAF\nA. Gianelle, R. Peluso, M. Sgaravatto\nINFN, Sezione di Padova\nD. Kouril , A. Krenek, L. Matyska, M. Mulac, J. Pospisil, M. Ruda, Z. Salvet, J. Sitera, M. Vocu\nCESNET\nM. Mezzadri, F. Prelz\nINFN, Sezione di Milano\nS. Monforte, M. Pappalardo,\nINFN, Sezione di Catania\nL. Salconi\nINFN, Sezione di Pisa\n\nIn the first phase of the European DataGrid project, the 'workload management' package (WP1) implemented a working\nprototype, providing users with an environment allowing to define and submit jobs to the Grid, and able to find and use the\n\"best\" resources for these jobs. Application users have now been experiencing for about a year with this first release of the\nworkload management system. The experiences acquired, the feedback received by the user and the need to plug new\ncomponents implementing new functionalities, triggered an update of the existing architecture. A description of this revised\nand complemented workload management system is given.\n\n\u2022\n\n1. INTRODUCTION\nThe European DataGrid project (EDG) [1] is a project\nfunded by the European Union, with the aim to design and\nimplement a Grid computing infrastructure, providing\naccess to large sets of distributed computational and data\nresources, and suitable for the needs of widely distributed\nscientific communities. In the context of the EDG project,\nWork Package 1 [2] was mandated to build a suitable\nsystem for scheduling and resource management in a Grid\nenvironment.\nDuring the first phase of the project, a Grid Workload\nManagement System (WMS) was designed and\nimplemented (also by integrating existing technologies),\nand deployed in the DataGrid testbed.\nThis first WMS, described in [3], has then been\nreviewed and complemented. In short the objectives to\nreview the architecture of the WMS, discussed in this\npaper, were:\n\u2022\n\nto address the shortcomings that emerged in the first\nDataGrid testbed, in particular some scalability and\nreliability problems;\n\nMOAT007\n\n\u2022\n\nto make it easy to plug-in new components\nimplementing new functionalities;\nto favor the interoperability with other Grid\nframeworks.\n\nThe principles and the lessons learned when evaluating the\nfirst system on the DataGrid testbed, were applied when\nreviewing the architecture of the WMS: these are\ndiscussed in more detail in another CHEP 2003 paper [4].\nIn section 2 the new Workload Management System is\npresented. Section 3 describes the most significant\nimprovements of this new WMS with respect to the first\nsystem, while section 4 discusses about some of the new\nintroduced functionalities. In section 5 it is discussed\nabout the foreseen future activities. Section 6 concludes\nthe paper.\n\n2. THE NEW WORKLOAD MANAGEMENT\nSYSTEM ARCHITECTURE\nThe new revised Workload Management System\narchitecture is represented in Figure 1.\n\n\fCHEP 2003, UC San Diego, March 24-28, 2003\n\n2\n\nFigure 1: The new WMS architecture\nAs in the first release of the Workload Management\nSystem, the User Interface (UI) is the component that\nallows users to access the functionalities offered by the\nWMS. In particular, via the UI, users are allowed to\nsubmit jobs, which also includes the staging of some input\nfiles (the so-called input sandbox files) from the file\nsystem of the UI machine to the worker node where the\nexecution will take place, to control them (cancel them,\nmonitor their status), to retrieve the output files produced\nby the job (the so-called output sandbox files), etc.\n\nMOAT007\n\nCharacteristics, requirements and preferences of jobs are\nspecified via a Job Description Language (JDL), based on\nthe Condor ClassAd language [5].\nThe Network Server is a generic network daemon,\nresponsible for accepting incoming requests from the UI\n(e.g. job submission, job removal), which, if valid, are\nthen passed to the Workload Manager. For this purpose the\nNetwork Server uses Protocol, to check if the incoming\nrequests conform to the agreed protocol.\nThe Workload Manager is the core component of the\nWorkload Management System. Given a valid request, it\nhas to take the appropriate actions to satisfy it. To do so, it\nmay need support from other components, which are\nspecific to the different request types. All these\n\n\fCHEP 2003, UC San Diego, March 24-28, 2003\ncomponents that offer support to the Workload Manager\nprovide a class whose interface is inherited from a Helper\nclass, which consists of a single method (resolve()).\nEssentially the Helper, given a JDL expression, returns a\nmodified one, which represents the output of the required\naction. For example, if the request was to find a suitable\nresource for a job, the input JDL expression will be the\none specified by the user at submission time, and the\noutput will be the JDL expression augmented with the\nresource choice.\nThe Resource Broker (RB) or MatchMaker is one of\nthese classes offering support to the Workload Manager. It\nis responsible to perform the matchmaking between the\nresource requirements (specified in the job JDL\nexpression) and the status of the Grid. So, given a job\nsubmission request, the RB is responsible to find the\nresources that best match the request. For this purpose the\nRB has to interact with the Information Services, and also\nwith the EDG Data Management services to resolve data\nrequirements. The Resource Broker can be \"decomposed\"\nin three sub-modules:\n\u2022\n\u2022\n\n\u2022\n\na sub-module responsible for performing the\nmatchmaking, therefore returning all the\nresources suitable for that JDL expression;\na sub-module responsible for performing the\nranking of matches resources, therefore returning\njust the \"best\" resource suitable for that JDL\nexpression;\na sub-module implementing the chosen\nscheduling strategy, easily pluggable and\nreplaceable with other ones implementing\ndifferent scheduling strategies.\n\nWithin this architecture, the Resource Broker is\ntherefore re-cast as a module, implementing the Helper\ninterface, which can be \"plugged\" and used also in\nframeworks other than the EDG Workload Management\nSystem.\nThe Job Adapter is responsible for making the final\n\"touches\" to the JDL expression for a job, before it is\npassed to CondorG for the actual submission. So, besides\npreparing the CondorG submission file, this module is also\nresponsible for creating the wrapper script: in fact the user\njob is wrapped within a script, which is responsible for\ncreating the appropriate execution environment in the CE\nworker node (this includes the transfer of the input and of\nthe output sandboxes).\nCondorG [6] is the module responsible for performing\nthe actual job management operations (job submission, job\nremoval, etc.), issued on request of the Workload\nManager. The CondorG framework is exploited for\nvarious reasons:\n\u2022\n\nthe reliable two-phase commit protocol used by\nCondorG for job management operations, along\nwith other provisions to increase the scalability of\nthe GRAM protocol;\n\nMOAT007\n\n\u2022\n\u2022\n\n\u2022\n\u2022\n\n3\n\nthe persistency: CondorG keeps a persistent (crash\nproof) queue of jobs;\nthe logging system: CondorG logs all the relevant\nevents (e.g. job started its execution, job execution\ncompleted, etc.) concerning the managed jobs: this\nis useful to increase the reliability of the whole\nsystem;\nthe increased openness of the CondorG framework;\nthe need for interoperability with the US Grid\nprojects, of which CondorG is an important\ncomponent.\n\nThe Log Monitor is responsible for \"watching\" the\nCondorG log file, intercepting interesting events\nconcerning active jobs, that is events affecting the job state\nmachine (e.g. job done, job cancelled, etc.), and therefore\ntriggering appropriate actions.\nFor what concerns the Logging and Bookkeeping (LB)\nservice, it stores logging and bookkeeping information\nconcerning events generated by the various components of\nthe WMS. Using this information, the LB service keeps a\nstate machine view of each job. In the new WMS the LB is\nessentially the only job repository information. The\ndependencies between this component and the other\nmodules of the Workload Management System (UI\naccessing the LB service to get status and logging\ninformation on jobs, and the various modules pushing\nevents concerning jobs to the LB) are not represented in\nthe figure, just for increased simplicity.\n\n3. IMPROVEMENTS OF THE WORKLOAD\nMANAGEMENT SYSTEM\nVarious improvements were applied when designing the\nnew WMS architecture, applying the lessons learnt while\nevaluating the first Workload Management System in the\nEDG testbed.\nFirst of all the duplication of persistent information\nrelated to jobs (which was difficult to keep coherent, and\nwhich caused various problems) was avoided. As already\nmentioned, in the new WMS the LB service is essentially\nthe only repository for job information. The drawback is\nthat the reliability and scalability of this service is now\nmuch more important and critical than in the past. To\naddress this issue, besides various improvements in the\ndesign and implementation, the new WMS has been\ndesigned to make possible relying on multiple LB servers\nper single WM, in order to distribute the load among\nmultiple servers and therefore avoiding bottlenecks.\nAnother major improvement was the introduction of\nvarious techniques and capabilities to quickly recover\nfrom failures (e.g. process or system crashes). For\nexample, the communication among the various\ncomponents of the new WMS is now much more reliable,\nsince it is done via persistent queues implemented in the\nfile system.\nIn the new Workload Management System, moreover,\nmonolithic long-lived processes were avoided. Instead,\n\n\fCHEP 2003, UC San Diego, March 24-28, 2003\nsome functionalities (e.g. the matchmaking) have been\ndelegated to pluggable modules. This also helps reducing\nthe exposure to memory leaks, coming not only from the\nEDG software, but also from the software linked with the\nWMS software.\nHaving the RB-Matchmaker as a pluggable module also\nincreases the flexibility of the whole system, and the\ninteroperability with other Grid frameworks. In fact it is\nnow much more feasible to exploit the Resource Broker\nalso \"outside\" the EDG Workload Management System.\nMoreover it is much more easier to implement and \"plug\"\nin the system the module implementing the chosen\nscheduling strategies, defined according to the one's own\nneeds and requirements. Interoperability is also favored by\nthe compliance of the new WMS to the Glue schema [7]\n[8], the common schema for information services agreed\nbetween European and US High Energy and Nuclear\nPhysics Grid projects.\nOther enhancements in design and implementation were\napplied to all the services, addressing the various\nshortcomings seen with the first release of the WMS.\nImprovements are also due to enhancements in the\nunderlying software, such as the ones coming from the\nGlobus [9] and the Condor [10] projects.\n\n4. NEW FUNCTIONALITY\nThe new Workload Management System also\nimplements some new functionalities, not available in the\nfirst release of the software.\nGangmatching is one of the new functionalities provided\nby the Resource Broker. It allows to take into account both\ncomputational and storage resources information in the\nmatchmaking. So, for example, a user could specify that\nhis jobs must be executed on a computational resource\n\"close\" to a storage system where there is \"enough\" free\nspace available.\nJob checkpointing is another of these new\nfunctionalities. Instead of addressing the classic\ncheckpointing problem, that is saving somewhere all the\ninformation related to a process (process's data and stack\nsegments, information about open files, pending signals,\nCPU state, etc.) as it is addressed in other projects (e.g.\nCondor [11]), the idea was providing users with a \"trivial\",\nor logical checkpointing service: through a proper API, a\nuser can save, at any moment during the execution of a\njob, the state of this job. So users can insert in the code for\ntheir applications some specific function calls to save,\nfrom time to time, the state of their jobs. A checkpointable\napplication must be able, of course, to restart itself from a\npreviously saved state. In this \"trivial\" checkpointing\nservice a state is defined by the user, and it is represented\nby a list of <var, value> pairs. They must allow to\nrepresent exactly what that job has done until that moment,\nand they must be chosen by the user in such a way that,\nrelying on them, the job can restart later its processing\nMOAT007\n\n4\n\nfrom this intermediate state. This checkpointing\nframework is useful when a job is aborted because of an\n\"external\" problem (e.g. a machine crash), and in these\ncases the job is automatically rescheduled (possibly on a\nresource different than the one where the problem\nhappened) and resubmitted. If a state for that job was\nsaved in its previous execution, the job doesn't need to\nstart from the beginning, but it can start from the \"point\"\ncorresponding to the last saved state. Since it is not always\nso straightforward to \"automatically\" (by the Grid\nmiddleware) understand when a job ends in an \"abnormal\"\nway, it was also foreseen to allow the user to retrieve an\nintermediate state for a job (usually the last saved one),\nand explicitly resubmit the job, pointing out that it must\nstart using this intermediate state.\nFor what concerns the architecture of the checkpointing\nframework, the functionality of persistently saving the\nstate of a job, and of retrieving a previously saved state, is\nprovided by the LB service.\nThe DataGrid Accounting System (DGAS) is another\nnew functionality offered by the revised WMS. It is a\nclosed economy based Grid accounting framework where\nusers and resources are seen as entities capable of\nexchanging \"virtual credits\". For example, when a user\nsubmits a job to a Grid resource, the user pays to the\nresource a well-defined amount of credits in order to get\nthe job executed. Generally a user receives the amount of\ncredits needed to perform his computations by the\nmanagement of the research group he belongs to. Research\ngroups have their own Grid resources, and these resources\nearn credits by executing user jobs. These credits can then\nbe redistributed among the users belonging to that group.\nDGAS has two main purposes:\n\u2022\n\n\u2022\n\nAccounting for Grid Users and Resources\nIt is possible to easily take tracks about resources\nused by the various users, and about the usage of\nthe available Grid resources.\nEconomic Brokering\nHelp the Resource Broker in choosing the most\nsuitable resource for a given job. In fact, once a\nvalid price setting policy has been established, the\nmodel should lead to a state of nearly stable\nequilibrium able to satisfy the needs of both\nresource providers and consumers.\n\nAs first step, only the first functionality is provided by\nthe new Workload Management System.\nThe new WMS also allows the execution of interactive\njobs. This was done by integrating the Condor bypass\nsoftware [12], making available a channel for the standard\nstreams (stdin, stdout, stderr) from the worker node where\nthe execution takes place to a remote machine, typically\nthe User Interface machine, where the user can 'control\"\nthe job.\n\n\fCHEP 2003, UC San Diego, March 24-28, 2003\nOne of the most interesting new features of the WMS is\nthe new extended querying capability of the Logging and\nBookkeeping service. Users are allowed to define and\nmark jobs via user tags, and can then specify queries on\nthese user tags and on the other \"standard\" fields. Just as\nexample, a user could ask to get the status of all his jobs\nreferring to production 'xyz' (user tag) and running on\nresource X or on resource Y.\nIn the new Workload Management System it is also\npossible to submit parallel (MPI) jobs, considering the\nMPICH implementation, a widely used, freely available,\nportable implementation of MPI.\nLast but not least, it should be mentioned that in the new\nsoftware release, it is possible to access the Workload\nManagement functionalities not only via a python\ncommand line interface (as it was the case for the first\nrelease of the system), but also via C++ and Java API, and\nalso via a Graphical User Interface (GUI).\n\n5. FUTURE WORK\nThe new WMS also provides hooks for some other new\nfunctionalities that will be implemented and integrated\nlater (actually the development of most of this software is\nalready in good progress).\nAs already mentioned, the Economic Brokering, that is\nthe integration of Grid Accounting with the Resource\nBroker (so that the most suitable resource for a given job\nis chosen according to the current price of resources and a\npre-defined economic policy), is one of these new future\ncapabilities of the WMS.\nAnother new functionality that will be provided is the\nsupport of inter-job dependencies, which can be defined\nby Directed Acyclic Graphs (DAGs), whose nodes are\nprogram executions (jobs), and whose arcs represent\ndependencies between them.\nWithin the Workload\nManagement System, a DAG will be managed by a metascheduler, called DAGMan (DAG Manager), whose main\npurpose is to navigate the graph, determine which nodes\nare free of dependencies, and follow the execution of the\ncorresponding jobs. DAGMan is a product originally\ndeveloped within the Condor project [10]. DAGMan can\ntherefore be seen as an iterator through the nodes of a\nDAG, looking for free nodes (i.e. nodes without\ndependencies). The corresponding jobs can then be\nsubmitted for execution. Before doing this, it is of course\nnecessary to choose the resource where to submit the job,\nand this will be done considering a lazy scheduling model,\nthat is a job (node) is bound to a resource just before that\njob is ready to be submitted.\nJob partitioning is another functionality that will be\nintroduced in the Workload Management System\nframework. Job partitioning takes place when a job has to\nprocess a large set of \"independent elements\", as it often\nhappens in many applications, such as most HENP\nMOAT007\n\n5\n\napplications. In these cases it may be worthwhile to\n\"decompose\" the job into smaller sub-jobs (each one\nresponsible for processing just a sub-set of the original\nlarge set of elements), in order to reduce the overall time\nneeded to process all these elements through \"trivial\"\nparallelisation, and to optimize the usage of all available\nGrid resources. The proposed approach is to address the\njob partitioning problem in the context of the logical job\ncheckpointing framework described above: the processing\nof a job could be described as a set of independent\nsteps/iterations, and this characteristic can be exploited,\nconsidering different, simultaneous, independent sub-jobs,\neach one taking care of a step or of a sub-set of steps, and\nwhich can be executed in parallel. The partial results (that\nare the results of the various sub-jobs) can be represented\nby job states (the final job states of the various sub-jobs),\nwhich can then be merged together by a job aggregator,\nwhich must start its execution when the various sub-jobs\nhave terminated their execution.\nImmediate or advance reservation of resources, which\ncan be heterogeneous in type and implementation and\nindependently controlled and administered, is another new\nfunctionality that will be supported, to allow the use of\nend-to-end quality of service (QoS) services in emerging\nnetwork-based applications. The Workload Management\nSystem will provide a generic framework to support\nreservation of resources, based on concepts that have\nemerged and been widely discussed in the Global Grid\nForum. In its implementation it is foreseen to address at\nleast computing, network and storage resources, provided\nthat adequate support exists from the local management\nsystems.\n\n6. CONCLUSIONS\nThe first Workload Management System, which was\nimplemented in the first phase of the DataGrid project, and\nevaluated in the DataGrid testbed also in some quasiproduction experiment activities, has been reviewed. The\nobject was in particular to address some of the existing\nproblems and shortcomings, and to support some new\nfunctionality. Moreover in the new WMS the hooks\nneeded to implement some new functionalities, to be\nprovided later, were implemented.\nThe preliminary results of the new Workload\nManagement System, in terms of reliability, stability and\nperformance are very encouraging. A more comprehensive\nevaluation will be possible when real test activities\nperformed by real users on the large scale DataGrid\ntestbed will be performed (at the time of writing, the new\nWMS is being integrated in this testbed).\n\n\fCHEP 2003, UC San Diego, March 24-28, 2003\n\nAcknowledgments\n\n[5]\n\nDataGrid is a project funded by the European\nCommission under contract IST-2000-25182.\nWe also acknowledge the national funding agencies\nparticipating to DataGrid for their support of this work.\n\n[6]\n\nReferences\n[1]\n[2]\n[3]\n\n[4]\n\nHome page for the DataGrid project\nhttp://www.eu-datagrid.org\nHome page for the Grid Workload Management\nWork Package of the DataGrid project\nhttp://www.infn.it/workload-grid\nDataGrid WP1 members (C. Anglano et al.),\n\"Integrating Grid tools to build a Computing\nResource Broker: activities of DataGrid WP1\",\nCHEP 2001 Conference, Beijing (p. 708 in the\nproceedings)\nDataGrid WP1 members (G. Avellino et al.), \"The\nfirst deployment of workload management services\non the EU DataGrid testbed: feedback on design\nand implementation\", also presented at CHEP 2003\nConference, San Diego\n\nMOAT007\n\n[7]\n[8]\n\n[9]\n[10]\n[11]\n[12]\n\n6\n\n\"Classified Advertisements\" Home Page\nhttp://www.cs.wisc.edu/condor/classad\nCondor-G Home Page\nhttp://www.cs.wisc.edu/condor/condorg\nHome page for the Glue Schema effort\nhttp://www.hicb.org/glue/glue-schema/schema.htm\nS. Andreozzi, M. Sgaravatto, C. Vistoli, \"A\nconceptual model of grid resources and services\",\nalso presented at CHEP 2003 Conference, San\nDiego\nHome page for the Globus project\nhttp://www.globus.org\nHome page for the Condor project\nhttp://www.cs.wisc.edu/condor\nHome page for Condor checkpointing\nhttp://www.cs.wisc.edu/condor/checkpointing.html\nHome page for the Condor bypass software\nhttp://www.cs.wisc.edu/condor/bypass\n\n\f"}