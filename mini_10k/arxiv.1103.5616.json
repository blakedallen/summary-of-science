{"id": "http://arxiv.org/abs/1103.5616v1", "guidislink": true, "updated": "2011-03-29T12:09:44Z", "updated_parsed": [2011, 3, 29, 12, 9, 44, 1, 88, 0], "published": "2011-03-29T12:09:44Z", "published_parsed": [2011, 3, 29, 12, 9, 44, 1, 88, 0], "title": "To Parallelize or Not to Parallelize, Speed Up Issue", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1103.4664%2C1103.0581%2C1103.4622%2C1103.2381%2C1103.1840%2C1103.0535%2C1103.4356%2C1103.1176%2C1103.2746%2C1103.3874%2C1103.4753%2C1103.0900%2C1103.4268%2C1103.1973%2C1103.4300%2C1103.2205%2C1103.2594%2C1103.5291%2C1103.1024%2C1103.3598%2C1103.4912%2C1103.4418%2C1103.5431%2C1103.6124%2C1103.6093%2C1103.1940%2C1103.5185%2C1103.1430%2C1103.5207%2C1103.3800%2C1103.6234%2C1103.0922%2C1103.6205%2C1103.3508%2C1103.2106%2C1103.5432%2C1103.3952%2C1103.4467%2C1103.5209%2C1103.5489%2C1103.4115%2C1103.5916%2C1103.5221%2C1103.6127%2C1103.1549%2C1103.0716%2C1103.4014%2C1103.2404%2C1103.3626%2C1103.1839%2C1103.3286%2C1103.1094%2C1103.2634%2C1103.3292%2C1103.0345%2C1103.2283%2C1103.5299%2C1103.4361%2C1103.5214%2C1103.2996%2C1103.5639%2C1103.0582%2C1103.5616%2C1103.3042%2C1103.4227%2C1103.6123%2C1103.5700%2C1103.5227%2C1103.5342%2C1103.3480%2C1103.1621%2C1103.5985%2C1103.5206%2C1103.3230%2C1103.0998%2C1103.3522%2C1103.1869%2C1103.5197%2C1103.4771%2C1103.2347%2C1103.0897%2C1103.3103%2C1103.4470%2C1103.2266%2C1103.2926%2C1103.4201%2C1103.1259%2C1103.2982%2C1103.6011%2C1103.0645%2C1103.0624%2C1103.2683%2C1103.0914%2C1103.1361%2C1103.1371%2C1103.1744%2C1103.0631%2C1103.0461%2C1103.0870%2C1103.2001%2C1103.4654&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "To Parallelize or Not to Parallelize, Speed Up Issue"}, "summary": "Running parallel applications requires special and expensive processing\nresources to obtain the required results within a reasonable time. Before\nparallelizing serial applications, some analysis is recommended to be carried\nout to decide whether it will benefit from parallelization or not. In this\npaper we discuss the issue of speed up gained from parallelization using\nMessage Passing Interface (MPI) to compromise between the overhead of\nparallelization cost and the gained parallel speed up. We also propose an\nexperimental method to predict the speed up of MPI applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1103.4664%2C1103.0581%2C1103.4622%2C1103.2381%2C1103.1840%2C1103.0535%2C1103.4356%2C1103.1176%2C1103.2746%2C1103.3874%2C1103.4753%2C1103.0900%2C1103.4268%2C1103.1973%2C1103.4300%2C1103.2205%2C1103.2594%2C1103.5291%2C1103.1024%2C1103.3598%2C1103.4912%2C1103.4418%2C1103.5431%2C1103.6124%2C1103.6093%2C1103.1940%2C1103.5185%2C1103.1430%2C1103.5207%2C1103.3800%2C1103.6234%2C1103.0922%2C1103.6205%2C1103.3508%2C1103.2106%2C1103.5432%2C1103.3952%2C1103.4467%2C1103.5209%2C1103.5489%2C1103.4115%2C1103.5916%2C1103.5221%2C1103.6127%2C1103.1549%2C1103.0716%2C1103.4014%2C1103.2404%2C1103.3626%2C1103.1839%2C1103.3286%2C1103.1094%2C1103.2634%2C1103.3292%2C1103.0345%2C1103.2283%2C1103.5299%2C1103.4361%2C1103.5214%2C1103.2996%2C1103.5639%2C1103.0582%2C1103.5616%2C1103.3042%2C1103.4227%2C1103.6123%2C1103.5700%2C1103.5227%2C1103.5342%2C1103.3480%2C1103.1621%2C1103.5985%2C1103.5206%2C1103.3230%2C1103.0998%2C1103.3522%2C1103.1869%2C1103.5197%2C1103.4771%2C1103.2347%2C1103.0897%2C1103.3103%2C1103.4470%2C1103.2266%2C1103.2926%2C1103.4201%2C1103.1259%2C1103.2982%2C1103.6011%2C1103.0645%2C1103.0624%2C1103.2683%2C1103.0914%2C1103.1361%2C1103.1371%2C1103.1744%2C1103.0631%2C1103.0461%2C1103.0870%2C1103.2001%2C1103.4654&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Running parallel applications requires special and expensive processing\nresources to obtain the required results within a reasonable time. Before\nparallelizing serial applications, some analysis is recommended to be carried\nout to decide whether it will benefit from parallelization or not. In this\npaper we discuss the issue of speed up gained from parallelization using\nMessage Passing Interface (MPI) to compromise between the overhead of\nparallelization cost and the gained parallel speed up. We also propose an\nexperimental method to predict the speed up of MPI applications."}, "authors": ["Alaa Ismail Elnashar"], "author_detail": {"name": "Alaa Ismail Elnashar"}, "author": "Alaa Ismail Elnashar", "links": [{"href": "http://arxiv.org/abs/1103.5616v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1103.5616v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1103.5616v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1103.5616v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "International Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\nTO PARALLELIZE OR NOT TO\nPARALLELIZE, SPEED UP ISSUE\nAlaa Ismail El-Nashar\nFaculty of Science, Computer Science Department, Minia University, Egypt\nAssistant professor, Department of Computer Science, College of Computers and\nInformation Technology, Taif University, Saudi Arabia\n\nemail: nashar_al@yahoo.com\nAbstract\nRunning parallel applications requires special and expensive processing resources to obtain the required\nresults within a reasonable time. Before parallelizing serial applications, some analysis is recommended\nto be carried out to decide whether it will benefit from parallelization or not. In this paper we discuss the\nissue of speed up gained from parallelization using Message Passing Interface (MPI) to compromise\nbetween the overhead of parallelization cost and the gained parallel speed up. We also propose an\nexperimental method to predict the speed up of MPI applications.\n\nKey words\nParallel programming, Message Passing Interface, Speed up\n\n1. INTRODUCTION\nExecution time reduction is one of the most challenging goals of parallel programming.\nTheoretically, adding extra processors to a processing system leads to a smaller execution time\nof a program compared with its execution time using a fewer processors system or a single\nmachine[9]. Practically, when a program is executed in parallel, the hypothesis that the parallel\nprogram will run faster is not always satisfied. If the main goal of parallelizing a serial program\nis to obtain a faster run then the main criterion to be considered is the speedup gained from\nparallelization.\nSpeed up is defined as the ratio of serial execution time to the parallel execution time [2], it is\nused to express how many times a parallel program works faster than its serial version used to\nsolve the same problem. Many conflicting parameters such as parallel overhead, hardware\narchitecture, programming paradigm, programming style may negatively affect the execution\ntime of a parallel program making its execution time larger than that of the serial version and\nthus any parallelization gain will be lost. In order to obtain a faster parallel program, these\nconflicted parameters need to be well optimized.\nVarious parallel programming paradigms can be used to write parallel programs such as\nOpenMP [7], Parallel Virtual Machine (PVM) [21], and Message Passing Interface (MPI) [23].\nMPI is the most commonly used paradigm in writing parallel programs since it can be\nemployed not only within a single processing node but also across several connected ones. MPI\nenables the programmer to control both data distribution and process synchronization. MPICH2\n[22] is an MPI implementation that is working well on a wide range of hardware platforms and\nalso supports using of C/C++ and FORTRAN programming languages.\nIn this paper we discuss some of the parameters that affect the parallel programs performance\nas a parallelization gain issue and also propose an experimental method to predict the speed up\nof MPI applications. We focus on the parallel programs written by MPI paradigm using\n\nDOI : 10.5121/ijdps.2011.2202\n\n14\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\nMPICH2 implementation. This may be considered as a guide to decide before parallelizing\nserial applications, whether it will benefit from parallelization or not.\nThe paper is organized as follows: section 2 includes the related work. Section 3 covers the\nMPI parallelization methodologies. In section 4, we present some different parameters that\naffect parallel programs performance. Section 5 focuses on the performance limitations of MPI\nprograms. In section 6, we propose an experimental method to predict the speed up of MPI\nprograms.\n\n2. RELATED WORK\nReducing program execution time is one of the advantages that application programmers hope\nto achieve. Converting sequential programs into parallel ones is a costly duty; it requires special\nhardware and software equipments. It is preferable to virtually anticipate the speed up gained\nfrom parallelism before executing the application on a real parallel environment.\nSeveral systems have been developed for analyzing the performance of parallel programs.\nThese systems are either model or trace based.\nPetrini et al. [8] introduced a model based system to predict the performance of programs on\nmachines prior to their construction, and to identify the causes of performance variations from\nthe predictions. These methods pick up the slight variations in a program execution that arise at\nruntime that cannot be modeled by examining the static code.\nVampir [10] and Dimemas [18] are two trace based analysis tools that predict parallel programs\nperformance. These models use a trace file and the user's selection of network parameters that\nis used in the communication model to simulate the program execution.\nMPE (Multi-Processing Environment) library and jumpshot [1] that are distributed with\nMPICH [22] implementation provide graphical performance analysis for message passing\ninterface programs.\nIn this paper we introduce an experimental approach to predict the speed up of message passing\nprograms. Our approach is based on executing the parallel program several times on a single\nphysical processor with different numbers of virtual MPI processes.\n\n3. PARALLELIZATION WITH MPI\nIn message passing paradigm, several separate processes used to complete the overall\ncomputation. In this scheme, many concurrent processes are created, and all of the data\ninvolved in the calculation is distributed among them using different ways. There is no shared\ndata; when a process needs data held by another one, the second process must send it to the first\nprocess. An MPI message passing protocol describes the internal methods and policies an MPI\nimplementation employs to accomplish message delivery. There are two common message\npassing protocols, eager and rendezvous [13], [17]. Eager protocol is an asynchronous protocol\nthat allows a send operation to complete without acknowledgement from a matching receive.\nRendezvous protocol is a synchronous protocol which requires an acknowledgement from a\nmatching receive in order to complete the send operation. Since MPI enables the programmer to\ncontrol both data distribution and process synchronization, problem decomposition and inter\nprocess communication represent two challenges in writing MPI parallel programs. Unless they\nare coded carefully, program performance will be negatively affected.\n\n3.1 Problem decomposition\nThe first challenge in writing MPI programs is how to divide the concerned problem into\nsmaller sub problems. Problem decomposition has two types, data parallelism and task\nparallelism.\n\n15\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\nData partitioning challenge concerns with the manner in which the data can be divided among\nthe available processors. Data are divided into pieces of approximately the same size and then\nmapped to different processors or MPI processes depending on the process ID. Each\nprocessor/process then operates only on the portion of the data that is assigned to it. This\nstrategy can be efficiently used in solving the iterative problems in which processors can\noperate independently on large portions of data, communicating only the much smaller data\npieces at each iteration. The processes may need to communicate periodically in order to\nexchange data. This approach implies that the program needs to keep track of date pieces\nrequired by a process at any time instance.\nTask parallelism focuses on the computation that is to be performed rather than on the data\nmanipulated by the computation. The problem is decomposed according to the work that must\nbe done. Each task then performs a portion of the overall work.\n\n3.2 Processes Communication\nInter process communication challenge concerns with the manner in which the running\nprocesses can be fully controlled. This implies that explicit send and receive data operations\nmust be executed whenever data needs to move from one process to another. Two approaches\ncan be used to implement data distribution and communication activities among processors,\nnamely, \"point-to-point communication\" and \"collective communication\".\n\n3.2.1 Point-to-Point Communication\nMPI point-to-point operations enable message passing between only two different MPI\nprocesses. In this scheme, one process performs the message send operation and the other one\nperforms the matching receive operation. Send and receive operations work in two modes,\nblocking and non-blocking. In blocking mode, A blocking send routine will only return after it\nis safe to modify the application send buffer. This implies a handshaking with the receive\nprocess to confirm a safe send. A blocking receive only returns after the data has arrived and is\nready for use by the program. In case of non-blocking mode, both send and receive routines\nreturn immediately and do not wait for any communication events to complete, such as message\ncopying from user memory to system buffer space or the actual arrival of message. In this\nmode, non-blocking operations request the MPI library to perform the operation when it is able.\nIt is unsafe to modify the application buffer until the requested non-blocking operation was\nactually performed by the library. There are \"wait\" routines used to do this task. Non-blocking\ncommunications are primarily used to overlap computation with communication and exploit\npossible performance gains [20].\n\n3.2.2 Collective Communication\nIn general, all data movement among processes can be accomplished using MPI send and\nreceive routines. More over, a set of standard collective communication routines [20] are\ndefined in MPI. Each collective communication routine has a parameter called a communicator,\nwhich identifies the group of participating processes. The collective communication routines\nallow data movement among all processors or just a specified set of processors.\nThe function MPI_Barrier blocks the caller processes until all members in the communicator\ncall the routine. The function MPI_Bcast broadcasts a message from the root process to all\nprocesses in the communicator. The routines MPI_Gather and MPI_Gatherv allow each process\nin the communicator to send data to one process, while MPI_Scatter allows one process to send\na different message to each other process. The routines MPI_ Allgather and MPI_Allgatherv\ngather fixed and variable sized information, respectively, from all processes and puts the results\nto all processes. The function MPI_Alltoall is a generalization of MPI_Allgather, it allows\ndifferent messages to be sent to different processes. The most general form of all-to-all\ncommunication is MPI_Alltoallv, which allows general many-to-many or one-to-many\n16\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\ncommunications to be performed by carefully selecting the input arguments. Finally,\nMPI_Reduce performs global reduction operations using an operation, such as sum, maximum\nor minimum, which is then sent to the root process [4].\n\n4. PERFORMANCE METRICS\nThree metrics are commonly used to measure the performance of MPI programs, execution\ntime, speedup and efficiency. Several factors such as the number of processors used, the size of\nthe data being processed and inter-processor communications influence parallel program's\nperformance\n\n4.1 Execution time\nA parallel program's execution time is a common performance indicator. It is defined as the\ntime elapsed from that instance at which the first processor starts program execution to that\ninstance at which the last processor completes it. MPI enables the programmer to measure the\nexecution time of his code or a part of it by calling the function MPI_wtime( ). This function\ncall returns the wall clock time in seconds represented as double precision value on the calling\nprocessor. The part of code to be timed is enclosed between two timing calls, the difference\nbetween the two time values that generated from timing calls is the execution time of this part\nof code. The execution time T is given by:\n\nT = TComp + TComm + Tidle\n\n(1)\n\nwhere TComp is the computation time, TComm is the communication time consumed by processor\nto send and/or receive messages, and Tidle is the time a process spends waiting for data from\nother processors.\n\n4.2 Speed up and Efficiency\nConsidering execution time only as a performance metric may be insufficient, specially if we\nneed to study how the number of processors and problem size can affect a program\nperformance.\nSpeed up is another performance metric that takes processors number p, and problem size n,\ninto account. In terms of problem size and processors number, the total parallel execution time\nof a program that solves an n size problem on p processors is given by:\n\nT parallel = \u03c3 (n) +\n\n\u03c6 (n)\np\n\n+ \u03ba ( n, p )\n\n(2)\n\nWhere \u03c3 (n) is the program's serial part execution time, \u03c6 (n) is the program's parallel part\nexecution time, and \u03ba (n, p ) is the communication time.\nSpeed up is the ratio of the time taken to solve a problem on a single processor to the time\nrequired to solve the same problem on a parallel computer with multiple processors [24]. The\nspeedup metric for solving an n-size problem using P processors is expressed by:\n\n\u03c8 ( n, p ) \u2264\n\nTserial\nT parallel\n\n(3)\n\nAmdahl's Law [7] is one way of predicting the maximum achievable speedup for a given\nprogram. The law assumes that a fraction f of a program's execution time was infinitely\nparallelizable with no overhead, while the remaining fraction, 1-f, was totally serial [15].\nAccording to this law, the speedup of n-size problem on p processors is governed by\n\n\u03c8 ( n, p ) \u2264\n\n1\n, 0 \u2264 f \u22641\nf + (1 \u2212 f ) / p\n\n(4)\n\n17\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\nAmdahl's law treats problem size as a constant and hence the execution time decreases as\nnumber of processors increases. Gustafson law [12] gives another formula for predicting\nmaximum achievable speedup which is described by\n(5)\n\u03c8 (n, p ) \u2264 p + (1 \u2212 p ) s\nwhere s is the fraction of total execution time spent in serial code. The two laws ignore the\ncommunication cost ; they overestimate the speed up value [3].\nEfficiency is the ratio of speed up obtained to the number of processors used [2]. It measures\nprocessors utilization. Parallel system efficiency of solving an n-size problem on P processors is\ngiven by\n\n0 \u2264 \u03b5 ( n, p ) \u2264\n\n\u03c8 ( n, p )\np\n\n\u22641\n\n(6)\n\n5. PERFORMANCE LIMITATIONS OF MPI PROGRAMS\nSeveral factors affect the performance of parallel MPI programs. The application programmer\nhave to adapt these variables to achieve the optimal performance.\n\n5.1 Effect of problem decomposition\nWhen dividing the data into processes the programmer have to pay attention to the amount of\nload being processed by each processor. Load balancing is the task of equally dividing work\namong the available processes. This is easy to be programmed when the same operations are\nbeing performed by all the processes on different pieces of data. Irregular load distribution\nleads to load imbalance which cause some processes to finish earlier than others. Load\nimbalance is one source of overhead, so all tasks should be mapped onto processes as evenly as\npossible so that all tasks complete in the shortest amount of time to minimize the processors'\nidle time which lead to a faster execution as equation 1 indicates.\n\n5.2 Effect of communication pattern\nThe cost of communication in the execution time can be measured in terms of latency and\nbandwidth. Latency is the time taken to set up the envelope for communication, where\nbandwidth is the actual speed of transmission. Regardless of the network hardware architecture\nthe communication pattern affects the performance of MPI programs. Using collective\ncommunication pattern is more efficient than using of point-to-point communication pattern\n[23], so the application programmer have to avoid using of the latter one as much as possible,\nspecially for large size problems, for the following reasons:\n1. Although point-to-point pattern is a simple way of specifying communication in parallel\nprograms; its use leads to large program size and complicated communication structure,\nwhich negatively affect the program performance.\n2. Send-receive does not offer fundamental performance advantages over collective\noperations. The latter offer efficient implementations without changing the applications.\n3. In practice, using the non-blocking versions of send-receive, MPI_Isend and MPI_Irecv,\noften lead to slower execution than the blocking version because of the extra\nsynchronization.\n\n5.3 Effect of message size\nMessage size can be a very significant contributor to MPI application performance. The effect\nof message size is also influenced by latency, communication pattern and number of processors\nused as described in equation 2 and equation 3. To achieve an optimal performance, the\napplication programmer should take the following considerations into account:\n\n18\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\n1. In most cases, increasing the message size will yield better performance. For\ncommunication intensive applications, the smaller message size reduces MPI application\nperformance because latency badly affects short messages..\n2. for smaller message size with less number of processors, it is better to implement\nbroadcasting in terms of non-blocking point-to-point communication whereas for other\ncases broadcasting using MPI_Bcast saves time significantly.\n\n5.4 Effect of message passing protocol\nMPI message passing protocols affect the program performance. The performance is\nimplementation dependent. So the application programmer has to consider the following\ncircumstances:\n1. In case of eager protocol, the receiving process is responsible for buffering the message\nupon its arrival, specially if the receive operation has not been posted [13]. This operation\nis based upon the implementation's guarantee of a certain amount of available buffer space\non the receive process. In this case, the application programmer has to pay attention to the\nfollowing requirements to achieve a reasonable performance\na. message sizes must be small.\nb. avoid using of intensive communication to decrease the time consumed by the\nreceive process side to pull messages from the network and/or copy the data into\nbuffer space.\n2. If the receiving process buffer space can't be allocated or the limits of the buffer are\nexceeded rendezvous protocol is used. In this protocol, sender process sends message\nenvelope to destination process which receives and stores that envelope. When buffer\nspace is available, destination process replies to sender that requested data can be sent,\nhence sender process receives reply from destination process and then sends data [17]. In\nthis case, the application programmer has to pay attention to the following requirements to\nachieve a reasonable performance\na. message sizes must be large enough to avoid the time consumed for handshaking\nbetween sender and receiver.\nb. Using non-blocking sends with waits/tests to prevent program from blocking while\nwaiting for a receiving confirmation from receive process.\n\n5.5 Effect of processors' number\nAdding extra processors to the system reduces the computation time but increases the\ncommunication time as described in equation 3. The increase in communication time may be\nlarger than the decrease in computation time which leads to a dramatic decreasing of\nperformance. Equation 4 assures that the speedup is usually less than the number of processors.\nIn practice, speed up does not increase linearly as the number of processors increases but tends\nto saturate and accordingly the efficiency drops as the number of processors increases [12].\nThe effect of processor's number is also influenced by the problem size. Speedup and\nefficiency increase as the problem size increases on the same number of processors. If\nincreasing the number of processors reduces efficiency, and increasing the problem size\nincreases efficiency, the application programmer should be able to keep efficiency constant by\nincreasing both simultaneously.\n\n5.6 Effect of processes' number\nMPI implementations allow the programmer to run his application using arbitrary number of\nprocesses and processors. The number of processes may be less than, equal to, or greater than\nthe number of processors. It is common to develop parallel applications with a small number of\nprocesses on a single processor. As the application becomes more fully developed and stable,\n19\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\nlarger testing runs can be conducted on actual clusters to check for scalability and performance\nbottlenecks.\nThe number of processes per processor affects the application performance so the application\nprogrammer has to be aware of the following considerations:\n1. In general, maximum performance is achieved when each process has its own processor.\nWhen the number of processes is less than or equal to the number of processors, the application will run at its peak performance. Since the total system is either underutilized (there\nare unused processors) or fully utilized (all processors are being used), the application is\nnot hindered by several parameters such as context switching, cache misses, or virtual\nmemory thrashing caused by other local processes [14].\n2. running too many processes, the processors will thrash, continually trying to give each\nprocess its fair share of run time.\n3. running too few processes may not enable the programmer to run meaningful data through\nhis application, or may not cause error conditions that occur with larger numbers of\nprocesses.\n\n6. EXPERIMENTAL SPEED UP PREDICTION\nIn some cases, the predicted performance may differs from that achieved experimentally. In this\nsection we present an experimental method to predict the speed up of MPI applications as a\nperformance measure. The proposed method is summarized in the following steps:\n\n1. Execute the serial version of MPI application on a single processor machine.\n2. Record the serial execution time, Ts .\n3. Execute the parallel MPI application on the same single processor machine repeatedly\nusing arbitrary number of MPI processes, 1,2,3,...,n.\n4. Record the parallel execution times, Tp1 , Tp 2 ,...., Tp n , for each run.\n\n5. Graph the obtained results as a two dimensional graph. The X-axis for MPI processes\nnumber and the Y-axis for the parallel execution times, Tp1 , Tp 2 ,...., Tp n .\n6. If the parallel execution time is rapidly increases as the number of MPI processes\nincreases, this implies that the MPI application will exhibit a poor speed up if it is run in\nparallel on multiple physical processors.\n7. If the parallel execution time remains constant or slowly increases as the number of MPI\nprocesses increases, this implies that the MPI application will exhibit a linear speed up if it\nis run in parallel on multiple physical processors.\nWe applied the proposed method on two MPI applications. The first one solves the concurrent\nwave equation and the second finds the number of primes and also the largest prime number\nwithin an interval of integers. The two applications are also executed in parallel on multiple\nphysical processors. The recorded serial execution time, Ts for both applications is used to find\nout their experimental speed up to be compared with the predicted ones.\n\n6.1 Experimental setup\nSince modern parallel machines are very costly and not easy to be access, we used an\nexperimental system consists of 8 DELL machines. Each of these machines consists of Intel\ni386 based P4-1.6GHz processor with 512MB memory running on Microsoft Windows XP\nProfessional Service Pack 2. These machines are connected via a Fast Ethernet 100Mbps\nswitch. These machines are not as powerful as the recent cluster machines in terms of the\nhardware and performance but they can reasonably perform for testing purposes and also for\n20\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\nsolving small and middle size parallel problems. The experiments programs was written in\nFortran 90 using MPICH2 version 1.0.6p1, as a message passing implementation.\n\n6.2 Experimented Problems\n6.2.1 Problem 1: Concurrent wave equation\nThe concurrent wave equation [6] is the partial differential equation that describes the\npropagation of waves. The one-dimensional wave equation that represents a flexible vibrating\nstring stretched between two points on the x-axis is expressed by\n\nv\n\n2\n\u2202 2u\n2 \u2202 u\n, where, c is\n=\nc\n\u2202t 2\n\u2202x 2\n\nthe speed of the wave's propagation and u = u ( p, t ) describes the wave's amplitude at position\nv\np at time t .\nThe numerical solution of this equation can be given by :\nu(i,t+1) = (2.0 * u(i,t)) - u(i,t-1) + (c *(u(i-1,t)-(2.0*u(i,t))+u(i+1,t)))\n\n(8)\n\nwhere i is the position index along the x axis at the time t. Equation 8 implies that the amplitude\nat each position index i and time t+1 depends on the previous time steps (t, t-1) and neighboring\npoints (i-1, i+1).This means that the parallel solution requires interprocess communication. The\nparallel solution is based on dividing the vibrating string into points. Each processor is\nrepeatedly responsible for updating the amplitude of a number of points over time. At each\niteration, each processor exchanges boundary points with their nearest neighbors. The parallel\nalgorithm that solve this equation is summarized as follows:\n\n1. Initialize MPI environment.\n2. Determine number of MPI processes and identities.\n3. Determine left and right neighbors.\n4. If Process_id=master then\n5.\nobtains input values from user.\n6.\nbroadcast time advance parameter, total points and time steps\n7. else\n8.\nreceive input values from master\n9. endif\n10.calculate initial values based on sine curve\n11. calculate new values using wave equation\n12. update their points a specified number of times\n13. update values for each point along string\n14. exchange data with \"left-hand\" neighbor\n15. exchange data with \"right-hand\" neighbor\n16. If Process_id <> master then\n17.\nsend the updated values to the master\n18. else\n19.\nreceives results from workers and prints\n20. endif\n21. Finalize MPI environment\n22. End\nFigure 1. Wave equation parallel algorithm solution\n\n21\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\n6.2.2 Problem 2: Prime numbers generator\nThere is no general \"formula\" for generating prime numbers. However there are some\napproximations and theorems predicting the number of prime numbers less than a particular\nupper bound [11]. Brute-force algorithm [5], shown in figure 2, which is also called \"na\u00efve\"\nalgorithm can be used in primality test .\n1. Na\u00efve (n:integer,prime:logical)\n2. /* Assume first four primes are counted\n3. if n > 10 then\n4. squareroot = int( n )\n5. do i=3,squareroot,2\n6.\nif n % i = 0 then\n7.\nprime = false\n8.\nreturn\n9.\nendif\n10. enddo\n11. prime = true\n12. return\n13. else\n14. prime =false\n15. return\n16. endif\n17. End na\u00efve\nFigure 2. Na\u00efve and Sieves algorithm\nThe simplest primality test for a given number n, is to check whether any integer m from 2 to\nn \u2212 1 divides n. If n is divisible by any m then n is composite, otherwise it is prime. Rather than\ntesting all m up to n \u2212 1 , \"na\u00efve and sieves\" algorithm [16] tests only m up to n , if n is\ncomposite then it can be factored into two values, at least one of which must be less than or\nequal to n . The algorithm efficiency can also be improved by skipping all even m except 2.\nA pseudo serial version and also the corresponding MPI parallel version that use this algorithm\nto find the number of primes and also the largest prime number within an interval of integers\nare shown in figure 3 and figure 4 respectively.\n\n1. Determine the upper LIMIT of integers interval.\n2. prime_counter = 4\n3. do n =11, LIMIT, 2\n4.\ncall na\u00efve(n, prime)\n5.\nif (prime) then\n6.\nprime_counter = prime_counter + 1\n7.\nprime_value = n\n8.\nendif\n\n9. Enddo\n10. print prime_value, prime_counter\n11. End\nFigure 3. Serial primes generator pseudo code.\n\n22\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\n1. Initialize MPI environment.\n2. Determine number of MPI processes, ntasks, and Identities, rank.\n3. Determine the upper LIMIT of integers interval.\n4. mystart = (rank*2) + 1; stride = ntasks*2\n5. prime_counter = 0; prime_value = 0\n6. do n=mystart, LIMIT, stride\n7.\ncall na\u00efve(n, prime)\n8.\nif (prime) then\n9.\nprime_counter = prime_counter + 1\n10.\nprime_value = n\n11.\nendif\n12. enddo\n13. Reduce(pc,pcsum,MPI_SUM,master)\n14. Reduce(prime_value, maxprime, MPI_MAX , master)\n15. If Process_id=master then\n16.\nprint maxprime,pcsum-4\n17. endif\n18. Finalize MPI environment\n19. End\n\nFigure 4. Parallel MPI primes generator pseudo code.\n\n6.3 Predicted versus experimental results\nThe parallel MPI applications that solve both wave equation and prime numbers generator\nproblems were executed on the hardware architecture described in section 5.1. Serial execution\ntime , parallel execution time on a single processor using multiple number of processes and also\nparallel execution time on multiple processors for both problems are shown in table 1.\nTable 1. Serial and parallel execution times for\nWave Equation and Primes Generator\nParallel execution\nProblem\n\nProblem 1\nWave\nEquation\n\nProblem 2\nPrimes\nGenerator\n\nSerial\nexecution\ntime\n\n0.80216\n\n55.625\n\nSingle physical\nMultiple physical\nprocessor\nprocessors\nMPI\nExecution Physical Execution\nprocesses\ntime\nprocessors\ntime\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n2\n4\n8\n10\n16\n20\n\n1.3561\n3.6942\n6.3833\n9.4002\n12.5629\n15.301\n18.1778\n21.5001\n24.1733\n27.3349\n55.5887\n55.464\n54.9653\n55.5158\n55.1428\n55.9213\n\n1\n2\n4\n8\n\n1.3561\n4.0952\n1.2112\n11.4501\n\n1\n2\n4\n6\n8\n\n57.625\n32.6704\n17.38331\n11.58861\n8.2103\n23\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\nApplying the proposed speed up prediction method to wave equation problem using 10 MPI\nprocesses on a single physical processor we predicted that the application will exhibit a poor\nspeed up if it is executed in parallel using multiple physical processors.\nOur prediction is based on that the execution time is rapidly increases as the number of MPI\nprocesses as shown in figure 5. To prove that our prediction was true, we executed the same\nMPI code on 8 physical processors. Knowing the execution time of the serial code version, the\nexperimental speed up was calculated. Figure 6 shows that the maximum speed up achieved by\n8 physical processors was only 0.66228534 and hence our prediction was true.\n\nExecution Time (seconds)\n\n30\n25\n20\n15\n10\n5\n0\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\nNumber of Processes\n\nFigure 5. Execution time using 10 processes on a single CPU for problem 1\n9\n\nExperimental\n\n8\n\nIdeal\n\nSpeed up\n\n7\n6\n5\n4\n3\n2\n1\n0\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nNumber of Processors\n\nFigure 6. Experimental speed up for problem 1\nTo be unbiased, we also re-executed the same parallel code using different number of processes\non the same 8 physical processors. Figure 7 shows that the execution time was negatively\naffected as the number of MPI processes increases except in case of running a small number of\nMPI processes using 8 physical processors. The experimental results shows that there is no\nsignificant speed up improvement as shown in figure 8. This also proves that our prediction was\ntrue.\nApplying the proposed method to prime numbers generator problem using 20 MPI processes on\na single physical processor, we predicted that the application will exhibit a linear speed up if it\nis executed in parallel using multiple physical processors.\n\n24\n\n\fExecution Time (seconds)\n\nInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\n30\n\n8 CPUs\n\n4 CPUs\n\n25\n\n2 CPUs\n\n1 CPU\n\n20\n15\n10\n5\n0\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\nNumber of MPI Processes\n\nFigure 7. Effect of processes number on execution time using 8 CPUs for problem1\n12\n\nIdeal\n4 CPUs\n1 CPU\n\nSpeed Up\n\n10\n\n8 CPUs\n2 CPUs\n\n8\n6\n4\n2\n0\n1\n\n3\n\n5\n\n7\n\n9\n\n11\n\nNumber of Processes\n\nFigure 8. Experimental vs. ideal speed up for problem 1\n\nEx ex c ution T im e (s ec onds )\n\nOur prediction is based on that the execution time is slowly increases or seems to be constant as\nthe number of MPI processes as shown in figure 9. Running the same MPI code on 8 physical\nprocessors achieved a linear speed up as shown figure 10 and hence our prediction was also\ntrue.\n\n60\n45\n30\n15\n0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nNumber of Processes\n\nFigure 9. Execution time using 20 processes on a single CPU for problem 2\n\n25\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n9\n8\n\nExperimental\n\nSpeed up\n\n7\n\nIdeal\n\n6\n5\n4\n3\n2\n1\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nNumber of Processors\n\nFigure 10. Experimental speed up for problem2\n\n7. CONCLUSION\nConcerning the issue of speed up gained from parallelization, the decision making to parallelize\nor not to parallelize the serial application is not a trivial task.\nIn this paper we studied the conflicting parameters that affect the parallel programs\nperformance, specially MPI applications, showing some recommendations to be followed to\nachieve a reasonable performance. The problem nature is one of the most important factors that\naffect the parallel program speed up. If The problem can be divided into independent subparts\nand no communication is required, except to split up the problem and combine the final results,\nthen there is a great parallelization opportunity, and the resultant parallel program will exhibit a\nlinear speed up. If the same instruction set are applied to all data and processes communication\nis synchronous , speed up will be directly proportional to the computation -communication\nratio. If there are different instruction sets to be applied to all data to solve a specific problem\nand the inter-process communication is asynchronous, this will reduce the parallelization\nopportunity. Speed up of the resultant parallel application will be negatively affected with extra\ncommunication overhead.\nWe also proposed an experimental method that aids in speed up prediction. The proposed\nmethod is based on running the MPI applications with several MPI processes using only one\nsingle processor machine. It gives an indication about the speed up behavior of MPI\napplications without using extra parallel hardware facilities, so it is recommended to be applied\nto MPI applications before running them on real powerful cluster machines or an expensive\nparallel systems. The proposed method was applied to predict the speed up of MPI applications\nthat solve wave equation and prime numbers generator problems. The predicted speed up was\nas the same as experimental speed up achieved when using multiple physical processors for\nboth applications.\n\nREFERENCES\n[1] A. Chan D. Ashton, R. Lusk, and W. Gropp, Jumpshot-4 Users Guide, Mathematics\nand Computer Science Division, Argonne National Laboratory July 11, 2007.\n[2] A. Grama, A. Gupta, and V. Kumar, \"Isoefficiency Function: A Scalability Metric for\nParallel Algorithms and Architectures\", IEEE Parallel and Distributed Technology,\nSpecial Issue on Parallel and Distributed Systems: From Theory to Practice, Volume 1,\nNumber 3, pp 12-21, August 1993.\n[3] A. H. Karp and H. Flatt, \"Measuring Parallel Processor Performance\", Communication\nof the ACM Volume 33 Number 5, May 1990.\n26\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\n[4] A. Karwande, X. Yuan, and D. K. Lowenthal, \" CC-MPI: A Compiled Communication\nCapable MPI Prototype for Ethernet Switched Clusters\", Journal of Parallel and\nDistributed Computing, Volume 65, Number 10, pp 1123-1133, 2005.\n[5] A. Mohammad , O. Saleh and R. A. Abdeen \"Occurrences Algorithm for String\nSearching Based on Brute-force Algorithm\", Journal of Computer Science, Volume 2,\nNumber 1, pp 82-85, 2006.\n[6] C. Geoffrey, Fox et al \"Solving problems on concurrent processors\", Prentice-Hall,\nInc. Upper Saddle River, NJ, USA, ISBN:0-13-823022-6 , 1988.\n[7] E. Gabriel, G. E. Fagg, G. Bosilca, T. Angskun, J. J. Dongarra, J. M. Squyres, V.\nSahay, P. Kambadur, B. Barrett, A. Lumsdaine, R. H. Castain, D. J. Daniel, R. L.\nGraham, and T. S. Woodall, \"Open MPI: Goals, Concept, and Design of a Next\nGeneration MPI Implementation\", In Proceedings, 11th European PVM/MPI Users'\nGroup Meeting, Budapest, Hungary, pp. 97\u2013104, September 2004.\n[8] F. Petrini, D. Kerbyson, and S. Pakin. The case of the missing supercomputer\nperformance: Achieving optimal performance on the 8,192 processors of ASCI Q. In\nProc. Supercomputing, Phoenix, AZ, Nov. 2003.\n[9] G. M. Amdahl, \"Validity of the Single Processor Approach to achieving Large Scale\nComputing Capabilities\", In Proceedings of the AFIPS Spring Joint Computer\nConference, pp 483\u2013485, April 1967.\n[10] H. Brunst, M. Winkler, W. E. Nagel and H.-C. Hoppe, Performance Optimization for\nLarge Scale Computing: The Scalable VAMPIR Approach,, International Conference\non Computational Science (ICCS2001) Workshop on Tools and Environments for\nParallel and Distributed Programming, San Francisco, CA, May 2001.\n[11]\nI. Aziz, N. Haron, L. Tanjung and W. W. dagang, \"Parallelization of Prime\nNumber Generation Using Message Passing Interface\", WSEAS Transactions on\nComputers, Volume 7, Number 4, pp 291-303, April 2008.\n[12]\nJ. Gustafson \"Reevaluating Amdahl's Law\", Communications of the ACM,\nVolume 31, Number 5, pp 532-533, 1988.\n[13] J. Liu, A. Vishnu, and D. K. Panda \"Building Multirail InfiniBand Clusters: MPILevel Design and Performance Evaluation\", In Proceedings of the ACM/IEEE SC2004\nConference, pp 33 \u2013 33, Nov. 2004.\n[14] J. M. Squyres , \"Processes, Processors, and MPI\", Cluster World, MPI Mechanic\nVolume 1 Number 2, pp 8-11, January 2004.\n[15] M. D. Hill and M. R. Marty, \"Amdahl's Law in the Multicore Era\", IEEE Computer\nSociety, Volume 41, Number 7, pp 33-38, 2008.\n[16] O. L. Atkin and D. J. Bernstein, \"Prime sieves using binary quadratic forms\",\nMathematics of Computation Volume 73, pp 1023\u20131030, 2004.\n[17] R. Brightwell, K. D. Underwood, \"Evaluation of an Eager Protocol Optimization for\nMPI\",10th European PVM/MPI Users' Group Meeting, Venice, Italy, pp 327-334,\nSeptember 29 - October 2, 2003.\n[18] R. M. Badia, J. Labarta, J. G., and F. Escal \u0301e. DIMEMAS: Predicting MPI\napplications behavior in grid environments. In Workshop on Grid Applications and\nProgramming Tools, 8th Global Grid Forum (GGF8), pages 50\u201360, Seattle, WA, June\n2003.\n[19] S. Gorlatch, \"Send-Receive Considered Harmful: Myths and Realities of Message\nPassing\", ACM Transactions on Programming Languages and Systems, Volume 26,\nNumber 1, pp 47\u201356, January 2004.\n[20] The MPI Forum. The MPI-2: Extensions to the Message Passing Interface, July 1997.\nAvailable at http://www.mpi-forum.org/docs/mpi-20-html/mpi2-report.html.\n[21] V. S. Sunderam, \"PVM: A framework for parallel distributed computing\",\nConcurrency: Practice & Experience, Volume 2, Number 4, pp 315\u2013339, Dec. 1990.\n27\n\n\fInternational Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011\n\n[22] W. Gropp, \"MPICH2: A New Start for MPI Implementations\", In Recent Advances in\nPVM and MPI: 9th European PVM/MPI Users' Group Meeting, Linz, Austria, Oct.\n2002.\n[23] Y. Aoyama J. Nakano \"Practical MPI Programming\", International Technical Support\nOrganization, IBM Coorporation SG24-5380-00, August 1999.\n[24] Y. Yan, X. Zhang, and Q. Ma, \"Software Support for Multiprocessor Latency\nMeasurement and Evaluation\", IEEE Transactions on Software Engineering, Volume\n23, Number1, pp 4-16, January 1997.\n\nAuthor\nAlaa I. Elnashar was born in Minia, Egypt, on November 5, 1967.\nHe received his B.Sc. and M.Sc. from Faculty of Science,\nDepartment of Mathematics (Math. & Comp. Science), and Ph.D.\nfrom Faculty of Science, Department of Computer Science, Minia\nUniversity, Egypt, in 1988, 1994 and 2005. He is a staff member in\nFaculty of Science, Computer Science Dept., Minia University,\nEgypt.\nDr. Elnashar was a postdoctoral fellow at Kanazawa University,\nJapan. His research interests are in the area of Software\nEngineering, Software Testing, Parallel programming and Genetic\nAlgorithms.\nNow, Dr Elnashar is an Assistant professor, Department of Computer Science, College of\nComputers and Information Technology, Taif University, Saudi Arabia\n\n28\n\n\f"}