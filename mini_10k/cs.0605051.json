{"id": "http://arxiv.org/abs/cs/0605051v1", "guidislink": true, "updated": "2006-05-11T14:21:36Z", "updated_parsed": [2006, 5, 11, 14, 21, 36, 3, 131, 0], "published": "2006-05-11T14:21:36Z", "published_parsed": [2006, 5, 11, 14, 21, 36, 3, 131, 0], "title": "A General Method for Finding Low Error Rates of LDPC Codes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0605077%2Ccs%2F0605098%2Ccs%2F0605133%2Ccs%2F0605144%2Ccs%2F0605073%2Ccs%2F0605116%2Ccs%2F0605129%2Ccs%2F0605140%2Ccs%2F0605135%2Ccs%2F0605056%2Ccs%2F0605143%2Ccs%2F0605035%2Ccs%2F0605136%2Ccs%2F0605075%2Ccs%2F0605037%2Ccs%2F0605105%2Ccs%2F0605121%2Ccs%2F0605087%2Ccs%2F0605079%2Ccs%2F0605120%2Ccs%2F0605005%2Ccs%2F0605132%2Ccs%2F0605104%2Ccs%2F0605038%2Ccs%2F0605011%2Ccs%2F0605068%2Ccs%2F0605130%2Ccs%2F0605106%2Ccs%2F0605060%2Ccs%2F0605062%2Ccs%2F0605114%2Ccs%2F0605127%2Ccs%2F0605123%2Ccs%2F0605008%2Ccs%2F0605128%2Ccs%2F0605111%2Ccs%2F0605036%2Ccs%2F0605103%2Ccs%2F0605013%2Ccs%2F0605085%2Ccs%2F0605045%2Ccs%2F0605099%2Ccs%2F0605134%2Ccs%2F0605032%2Ccs%2F0605029%2Ccs%2F0605074%2Ccs%2F0605026%2Ccs%2F0605054%2Ccs%2F0605113%2Ccs%2F0605024%2Ccs%2F0605027%2Ccs%2F0605047%2Ccs%2F0605019%2Ccs%2F0605086%2Ccs%2F0605025%2Ccs%2F0605040%2Ccs%2F0605007%2Ccs%2F0605028%2Ccs%2F0605030%2Ccs%2F0605082%2Ccs%2F0605053%2Ccs%2F0605090%2Ccs%2F0605004%2Ccs%2F0605094%2Ccs%2F0605095%2Ccs%2F0605084%2Ccs%2F0605012%2Ccs%2F0605100%2Ccs%2F0605081%2Ccs%2F0605022%2Ccs%2F0605052%2Ccs%2F0605069%2Ccs%2F0605110%2Ccs%2F0605057%2Ccs%2F0605088%2Ccs%2F0605109%2Ccs%2F0605145%2Ccs%2F0605102%2Ccs%2F0605112%2Ccs%2F0605009%2Ccs%2F0605023%2Ccs%2F0605078%2Ccs%2F0605059%2Ccs%2F0605101%2Ccs%2F0605126%2Ccs%2F0605016%2Ccs%2F0605010%2Ccs%2F0605071%2Ccs%2F0605117%2Ccs%2F0605137%2Ccs%2F0605051%2Ccs%2F0605108%2Ccs%2F0605091%2Ccs%2F0605018%2Ccs%2F0605142%2Ccs%2F0605119%2Ccs%2F0605006%2Ccs%2F0605003%2Ccs%2F0605141%2Ccs%2F0605034%2Ccs%2F0605080&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A General Method for Finding Low Error Rates of LDPC Codes"}, "summary": "This paper outlines a three-step procedure for determining the low bit error\nrate performance curve of a wide class of LDPC codes of moderate length. The\ntraditional method to estimate code performance in the higher SNR region is to\nuse a sum of the contributions of the most dominant error events to the\nprobability of error. These dominant error events will be both code and decoder\ndependent, consisting of low-weight codewords as well as non-codeword events if\nML decoding is not used. For even moderate length codes, it is not feasible to\nfind all of these dominant error events with a brute force search. The proposed\nmethod provides a convenient way to evaluate very low bit error rate\nperformance of an LDPC code without requiring knowledge of the complete error\nevent weight spectrum or resorting to a Monte Carlo simulation. This new method\ncan be applied to various types of decoding such as the full belief propagation\nversion of the message passing algorithm or the commonly used min-sum\napproximation to belief propagation. The proposed method allows one to\nefficiently see error performance at bit error rates that were previously out\nof reach of Monte Carlo methods. This result will provide a solid foundation\nfor the analysis and design of LDPC codes and decoders that are required to\nprovide a guaranteed very low bit error rate performance at certain SNRs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0605077%2Ccs%2F0605098%2Ccs%2F0605133%2Ccs%2F0605144%2Ccs%2F0605073%2Ccs%2F0605116%2Ccs%2F0605129%2Ccs%2F0605140%2Ccs%2F0605135%2Ccs%2F0605056%2Ccs%2F0605143%2Ccs%2F0605035%2Ccs%2F0605136%2Ccs%2F0605075%2Ccs%2F0605037%2Ccs%2F0605105%2Ccs%2F0605121%2Ccs%2F0605087%2Ccs%2F0605079%2Ccs%2F0605120%2Ccs%2F0605005%2Ccs%2F0605132%2Ccs%2F0605104%2Ccs%2F0605038%2Ccs%2F0605011%2Ccs%2F0605068%2Ccs%2F0605130%2Ccs%2F0605106%2Ccs%2F0605060%2Ccs%2F0605062%2Ccs%2F0605114%2Ccs%2F0605127%2Ccs%2F0605123%2Ccs%2F0605008%2Ccs%2F0605128%2Ccs%2F0605111%2Ccs%2F0605036%2Ccs%2F0605103%2Ccs%2F0605013%2Ccs%2F0605085%2Ccs%2F0605045%2Ccs%2F0605099%2Ccs%2F0605134%2Ccs%2F0605032%2Ccs%2F0605029%2Ccs%2F0605074%2Ccs%2F0605026%2Ccs%2F0605054%2Ccs%2F0605113%2Ccs%2F0605024%2Ccs%2F0605027%2Ccs%2F0605047%2Ccs%2F0605019%2Ccs%2F0605086%2Ccs%2F0605025%2Ccs%2F0605040%2Ccs%2F0605007%2Ccs%2F0605028%2Ccs%2F0605030%2Ccs%2F0605082%2Ccs%2F0605053%2Ccs%2F0605090%2Ccs%2F0605004%2Ccs%2F0605094%2Ccs%2F0605095%2Ccs%2F0605084%2Ccs%2F0605012%2Ccs%2F0605100%2Ccs%2F0605081%2Ccs%2F0605022%2Ccs%2F0605052%2Ccs%2F0605069%2Ccs%2F0605110%2Ccs%2F0605057%2Ccs%2F0605088%2Ccs%2F0605109%2Ccs%2F0605145%2Ccs%2F0605102%2Ccs%2F0605112%2Ccs%2F0605009%2Ccs%2F0605023%2Ccs%2F0605078%2Ccs%2F0605059%2Ccs%2F0605101%2Ccs%2F0605126%2Ccs%2F0605016%2Ccs%2F0605010%2Ccs%2F0605071%2Ccs%2F0605117%2Ccs%2F0605137%2Ccs%2F0605051%2Ccs%2F0605108%2Ccs%2F0605091%2Ccs%2F0605018%2Ccs%2F0605142%2Ccs%2F0605119%2Ccs%2F0605006%2Ccs%2F0605003%2Ccs%2F0605141%2Ccs%2F0605034%2Ccs%2F0605080&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper outlines a three-step procedure for determining the low bit error\nrate performance curve of a wide class of LDPC codes of moderate length. The\ntraditional method to estimate code performance in the higher SNR region is to\nuse a sum of the contributions of the most dominant error events to the\nprobability of error. These dominant error events will be both code and decoder\ndependent, consisting of low-weight codewords as well as non-codeword events if\nML decoding is not used. For even moderate length codes, it is not feasible to\nfind all of these dominant error events with a brute force search. The proposed\nmethod provides a convenient way to evaluate very low bit error rate\nperformance of an LDPC code without requiring knowledge of the complete error\nevent weight spectrum or resorting to a Monte Carlo simulation. This new method\ncan be applied to various types of decoding such as the full belief propagation\nversion of the message passing algorithm or the commonly used min-sum\napproximation to belief propagation. The proposed method allows one to\nefficiently see error performance at bit error rates that were previously out\nof reach of Monte Carlo methods. This result will provide a solid foundation\nfor the analysis and design of LDPC codes and decoders that are required to\nprovide a guaranteed very low bit error rate performance at certain SNRs."}, "authors": ["Chad A. Cole", "Stephen G. Wilson", "Eric. K. Hall", "Thomas R. Giallorenzi"], "author_detail": {"name": "Thomas R. Giallorenzi"}, "author": "Thomas R. Giallorenzi", "arxiv_comment": "Submitted Trans. Inf. Theory", "links": [{"href": "http://arxiv.org/abs/cs/0605051v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0605051v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0605051v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0605051v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:cs/0605051v1 [cs.IT] 11 May 2006\n\nA General Method for Finding Low Error Rates of LDPC\nCodes\nChad A. Cole\nStephen G. Wilson\n\nEric. K. Hall\nThomas R. Giallorenzi\n\nUniv. of Virginia\nCharlottesville, VA 22904\n\nL-3 Communications\nSalt Lake City, UT 84116\n\n\u2217\n\nMarch 19, 2018\n\nThis work is supported by L-3 Communications. This work has been submitted to the IEEE for possible\npublication. Copyright may be transferred without notice, after which this version may no longer be accessible.\n\u2217\n\n\fAbstract\nThis paper outlines a three-step procedure for determining the low bit error rate performance curve of a wide class of LDPC codes of moderate length. The traditional method to\nestimate code performance in the higher SNR region is to use a sum of the contributions\nof the most dominant error events to the probability of error. These dominant error events\nwill be both code and decoder dependent, consisting of low-weight codewords as well as\nnon-codeword events if ML decoding is not used. For even moderate length codes, it is not\nfeasible to find all of these dominant error events with a brute force search. The proposed\nmethod provides a convenient way to evaluate very low bit error rate performance of an\nLDPC code without requiring knowledge of the complete error event weight spectrum or\nresorting to a Monte Carlo simulation. This new method can be applied to various types of\ndecoding such as the full belief propagation version of the message passing algorithm or the\ncommonly used min-sum approximation to belief propagation. The proposed method allows\none to efficiently see error performance at bit error rates that were previously out of reach\nof Monte Carlo methods. This result will provide a solid foundation for the analysis and\ndesign of LDPC codes and decoders that are required to provide a guaranteed very low bit\nerror rate performance at certain SNRs.\nKeywords - LDPC codes, error floors, importance sampling.\n\n1\n\nIntroduction\n\nThe recent rediscovery of the powerful class of codes known as low-density parity-check (LDPC)\ncodes [1, 2] has sparked a flurry of interest in their performance characteristics. Certain applications for LDPC codes require a guaranteed very low bit error rate, and there is currently no\npractical method to evaluate the performance curve in this region. The most difficult task of determining the error 'floor' of a code (and decoder) in the presence of additive white Gaussian noise\n(AWGN) is locating the dominant error events that contribute most of the error probability at\nhigh SNR. Recently, a technique for solving this problem for the class of moderate length LDPC\ncodes [3] has been discovered. Since an ML detector is not commonly used (or even feasible)\nfor decoding LDPC codes, most of these error events are a type of non-codeword error called\ntrapping sets (TS) [4]. Since the error contribution of a TS is not given by a simple Q-function,\nas in the case of the two-codeword problem for an ML decoder, it is not clear how the list of\nerror events (mainly TS) returned by the search technique of [3], henceforth referred to as the\n'decoder search,' should be utilized to provide a complete picture of a code's low bit error rate\nperformance. This paper will present a three-pronged attack for determining the complete error\nperformance of a short to moderate length LDPC code for a variety of decoders. The first step\nis to utilize the decoder search to build a list of the dominant error events. Next, a deterministic\nnoise is directed along a line in n-dimensional space towards each of these dominant events and a\nEuclidean distance to the error boundary is found by locating the point at which the decoder fails\nto converge to the correct state. This step will be crucial in determining which of the error events\nin our initial list is truly dominating (i.e. nearest in n-dimensional decoding space to our reference\nall-zeros codeword). The final step involves an importance sampling (IS) [5, 6, 7] procedure for\ndetermining the low bit error rate performance of the entire code. The IS technique has been\napplied to LDPC codes before [8, 9, 10] with limited success. The method proposed in [8] does\n\n\fnot scale well with block length. The method in [9] uses IS to find the error contribution of each\nTS individually, and then a sum of these error contributions gives the total code performance.\nThis method is theoretically correct, but it has a tendency to underestimate the error curves since\ninevitably some important error events will not be known. The method proposed in this paper is\neffective for block length n < 10000 or so, and does not require that the initial list of dominant\nTS be complete, thus improving upon some of the limitations of previous methods of determining\nvery low bit error rates.\nThis paper is organized as follows: Section 2 introduces some terms and concepts necessary\nto understand the message passing decoding of LDPC codes and what causes their error floors.\nSection 3 gives a self-contained introduction to the decoder search procedure of [3]. Section 4\ndetails step two of our general procedure - the localized decoder search for the error boundary of\na given TS. Section 5 reviews the basics of importance sampling (step three of our procedure).\nSection 6 puts together all three steps of our low bit error performance analysis method and gives\na step-by-step example. Section 7 gives some results for different codes and decoders and shows\nthe significant performance differences in the low bit error region of different types of LDPC codes.\nThe final section summarizes the contribution offered in this paper.\n\n2\n\nPreliminaries\n\nLDPC codes, the revolutionary form of block coding that allows large block length codes to be\npractically decoded at SNR's close to the channel capacity limit, were first presented by Gallager\nin the early 1960's [1]. These codes have sparse parity check matrices, denoted by H, and can be\nconveniently represented by a Tanner graph [11], where each row of H is associated with a check\nnode ci , each column of H is associated with a variable node vj , and each position where Hij = 1\ndefines an edge connecting vi to ci in the graph. A regular {j, k} graph has j '1's per column and\nk '1's per row.\nThe iterative message passing algorithm (MPA), also referred to as Belief Propagation when\nusing full-precision soft data in the messages, is the method commonly used to decode LDPC codes.\nThis algorithm passes messages between variable and check nodes, representing the probability\nthat the variable nodes are '1' or '0' and whether the check nodes are satisfied. The following\nequations, representing the calculations at the two types of nodes, will be considered in the log\nlikelihood ratio (LLR) domain with notation taken from [12].\nWe will consider \u221a\nBPSK modulation on an AWGN memoryless channel, where each received\nchannel output yi = Es xi + ni is conditionally independent of any others. The transmitted bits,\nxi , are modulated by 0 \u2192 +1, 1 \u2192 \u22121 and are assumed to be equally likely. ni \u223d N(0, No /2)\nis the noise with two-sided PSD No /2. The a posteriori probability for bit xi , given the channel\ndata, yi , is given by\n\nP (xi |yi ) =\n\nP (yi |xi )P (xi )\nP (yi )\n\n(1)\n\nr(xi =0|yi )\nThe LLR of the channel data for the AWGN case is denoted Lci = log PP r(x\n= 4yi Es /No .\ni =1|yi )\nth\nth\nThe LLR message from the j check node to the i variable node is given by\n\n\f\uf8ee\n\nLrji = 2 tanh\u22121 \uf8f0\n\nY\n\nVj \\i\n\n\uf8f9\n\ntanh(Lqij /2)\uf8fb\n\n(2)\n\nThe set Vj is all of the variable nodes connected to the j th check node and Ci is all of the check\nnodes connected to the ith variable node. Vj \\i is the set Vj without the ith member, and Ci \\j is\nlikewise defined. The LLR message from the ith variable node to the j th check node is given by\nLqij =\n\nX\n\nLrji + Lci\n\n(3)\n\nCi \\j\n\nThe marginal LLR for the ith code bit, which is used to make a hard decision for the transmitted\ncodeword is\nLQi =\n\nX\n\nLrji + Lci\n\n(4)\n\nCi\n\nIf LQi > 0, then x\u0302i = 0, else x\u0302i = 1. The decoder continues to pass these messages in each\niteration until a preset maximum number of iterations is reached or the estimate x\u0302 \u2208 C, the set\nof all valid codewords. For a more detailed exposition of the MPA, see [12].\nAs with all linear codes, it is convenient to assume the all-zeros codeword as a reference vector\nfor study. If a subset of variable nodes and check nodes is considered, members of this subset will\nbe called the active nodes of a subgraph. By studying the local subgraph structure around each\nvariable node, arguments can be made about the global graph structure. For example, bounds\ncan be placed on the minimum distance of a code by only considering the local structure of a\ncode [11]. Small subsets of non-zero bits which do not form a valid codeword but still cause the\nMPA decoder problems are well-documented in the literature and typically referred to as trapping\nsets (TS) [4]. A TS, x, is a length-n bit vector denoted by a pair (a, b), where a is the Hamming\nweight of the bit vector and b is the number of unsatisfied checks, i.e. the Hamming weight of the\nsyndrome xHT . Alternatively, from a Tanner graph perspective, a TS could be defined as the a\nnonzero variable nodes of x and all of the check nodes connected by one edge to those a variable\nnodes. A valid codeword is a TS with b = 0. Two examples of TS, both extracted from a (96,48)\ncode on MacKay's website [13], are shown in Figure 1. The shaded check nodes, #4 for the (5, 1)\nTS and #2 and #11 for the (4, 2) TS, are the unsatisfied check nodes. A cycle of length c occurs\nwhen a path with c edges exists between a node and itself. There are no 4-cycles in this code,\nbut there are many 6-cycles within the subgraphs of Figure 1. The essential intuition pointing\nto these as problematic for the decoder is that if a bits have sufficient noise to cause them to\nindividually appear as soft 1's, while the others in the subgraph are soft 0's, the check nodes with\nwhich the a variable nodes connect will be satisfied, tending to reinforce the wrong state to the\nrest of the graph. Only the unsatisfied check(s) is a route through which messages come to reverse\nthe apparent (incorrect) state.\n\n\f(5,1) TS\n4\n\n5\n\n6\n\n22\n\n7\n\n28\n\n12\n\n87\n\n13\n\n14\n\n46\n\n93\n\n92\n\n(4,2) TS\n1\n\n2\n\n2\n\n11\n\n31\n\n3\n\n33\n\n23\n\n38\n\n39\n\n38\n\nFigure 1: A (5,1) and (4,2) TS subgraph\n\nThe number of edges within a TS, |ET S |, is determined by the number of variable nodes, a,\nand their degrees.\n|ET S | =\n\na\nX\n\ndvi\n\n(5)\n\ni=1\n\nThe number of check nodes participating in a dominant TS is most often given by\n|ET S | \u2212 b\n+b\n(6)\n2\nThis equation assumes that all unsatisfied check (USC) nodes are connected to one variable\nnode in the TS and all satisfied checks are connected to exactly two variable nodes from the TS.\nSubgraphs with these properties are referred to as elementary trapping sets in the literature [14].\nSince any odd number of connections to TS bits will cause a check to be unsatisfied and any\neven number will cause the check to be satisfied, it is not guaranteed that all dominant TS are\nelementary. However, for dominant TS, i.e. those with small a and much smaller b, it is evident\nthat given |ET S | edges to spend in creating an (a, b) TS, most edge permutations will produce\ndc = 2 satisfied checks and dc = 1 USC's. Empirical evidence also supports this observation as is\nseen in compiled tables of TS shown in Section 3.\nThe girth, g, of a graph is defined as the length of the shortest cycle and we assume this\nvalue to be g \u2265 6. This constraint is easily enforced when building low-density codes; 4-cycles\nare only present when two columns of H have 1's in more than one common row. Now consider\na tree obtained by traversing the graph breadth-first from a given variable node. This tree has\nalternating variable and check nodes in each tier of the tree. For this girth-constrained set of\nregular codes, a tree rooted at a variable node will guarantee all dv nodes in the first tier of\nvariable nodes, in this case dv (dc \u2212 1) = 15 nodes, will be distinct as illustrated in Figure 2. If\n|CT S | =\n\n\fthe root variable node in the tree is set to a '1', then to satisfy all of the check nodes in the first\ntier of check nodes, an odd number of variable nodes under each of the dc check nodes in the\nfirst tier of variable nodes must be a '1.' Since with high probability the dominant error events\nwill correspond to elementary TS, we assume that exactly one variable node associated with each\ncheck node in the first tier is a '1.' To enumerate all of these 1 + dv = 4-bit combinations we\nmust consider all (dc \u2212 1)dv = 125 combinations in one tree, and then take all n variable nodes as\nthe root of a tree, which entails n(dc \u2212 1)dv combinations for a general regular graph. The search\nmethod of Section 3 makes extensive use of these trees rooted at each variable node.\n\n1\n\n1\n\n0000\n00\n1111\n11\n0000\n1111\n00\n11\n0000\n1111\n00\n11\n0000\n1111\n00\n11\n0000\n1111\n00\n11\n0000\n1111\n00\n11\n0000\n1111\n00\n11\n0000\n1111\n00\n11\n0000\n00\n1111\n11\n0000\n1111\n00\n11\n0000\n1111\n00\n11\n\n2\n\n3\n\n2 3 4 5 6 7 8 9 1011 1213141516\nFigure 2: Tree showing first layer of check and variable nodes\n\n3\n\nTrapping Set Search Method (Step 1)\n\nThe key to an efficient search for problematic trapping sets (and low-weight codewords, which\ncan be considered as an (a, 0) TS and will thus further not be differentiated from other TS) lies\nin significantly reducing the entire n-dimensional search space to focus on the only regions which\ncould contain the dominant error events. The low-density structure of Tanner graphs for LDPC\ncodes allows one to draw conclusions about the code's global behavior by observing the local\nconstraints within a few edges of each node. This allows a search algorithm that searches the\nspace local to each variable node.\nTo see how the local graph structure can limit our search space for dominant error events,\nconsider two sets of length-n bit vectors with Hamming weight four. The first set, S1 , contains\nall such vectors: S1 = {x : wH (x) = 4}. The second set, S2 , consists of the constrained set of\nthose four-bit combinations which are contained within the union of a variable root node and one\nvariable node from each of the three branches in the first tier of variable nodes associated with\nthat root node. When we consider only satisfied check nodes connected to two active variable\n\n\fnodes, the number of active check or variable nodes in tier i is dv (dv \u2212 1)i\u22121 . For example, the\nnumber of nodes in the first tier of variable nodes |V1 |, and the first tier of check nodes |C1 |,\nfor {3, 6} codes, is |V1 | = |C1 | = 3. Now assign the variable nodes in the leftmost branch in\nthe ith tier to set Vi1 and label these from left to right for all variable node sets in the ith tier.\nFor example, in Figure 2, set V11 contains nodes 2-6, set V12 contains nodes 7-11, and set V13\ncontains nodes 12-16. We do the same for check node sets, and the first tier of check nodes would\nconsist of only theTset C11 containing checks 1-3. Using this notation, set S2 can be defined as:\nv\nwH (V1i ) = 1 j = 1, . . . , n}.\nS2 = {x : (xj = 1) di=1\nIn a regular code there are dc \u2212 1 elements satisfying wH (V1i ) = 1 for each i = 1, . . . , dv . Since\nwe choose these elements from each of the V1i independently, there are (dc \u2212 1)dv elements in S2 .\nFor a regular {3, 6} code, dv + 1 = 4, so the elements of S2 are all 4-bit combinations, and these\ncombinations of variable nodes will ensure that at least the three check nodes directly connected\nto the root variable node are satisfied. For example, if we choose the leftmost variable node in\neach branch of Figure 2, a member of the set S\u00122 would\n\u0013 be {v1 , v2 , v7 , v12 }.\n96\n= 3321960 and |S2 | = n(dc \u22121)dv = 12000.\nConsider a (96,48) {3, 6} code where |S1 | =\n4\nThis example illustrates the large reduction in the number of vectors belonging to |S2 | as opposed\nto |S1 |, and thus results in a correspondingly much smaller search space for dominant error events.\nNotice the gap between the sizes of these two sets gets even larger as block length increases.\nThe motivation for examining the smaller set of 4-bit combinations, |S2 |, above was to limit\nthe number of directions in n-dimensional decoding space necessary to search for dominant error\nevents. If a true ML decoder were available, a simple technique can be utilized to find the minimum\ndistance of a code [15], which is similar to our problem of finding the low-weight TS spectrum for\na code. The idea is to introduce a very unnatural noise, called an 'error impulse,' in a single bit\nposition as input to the ML decoder. Unfortunately, the single-bit error impulse method cannot\nbe used with the MPA to find dominant error events [16] mainly because the MPA's objective is to\nperform a bit ML decision rule and not the vector ML rule. Typically when a large error impulse\nis input to one bit, the decoder will correctly decode the error until the impulse reaches a certain\nsize where the channel input for that bit overrides the dv check messages and flips that bit while\nleaving all n \u2212 1 other bits alone. Instead of the single-bit impulse, it makes more sense to apply\na multi-bit error with a smaller impulse magnitude in each bit, as this would better simulate a\ntypical high SNR noise realization.\nThe choice of which bits to apply the impulse to is very important - they should be a subset\nof the bits of a minimum distance TS. A good candidate set of impulse bit locations is given\nby S2 . A multi-bit error impulse should appear as a more 'natural' noise to the MPA and the\n4-bit combinations of S2 will be likely to get error impulses into multiple bits of a dominant TS.\nFor example, suppose a minimum distance TS has a '1' in its first four bits. If an error impulse\n\u01eb1 were applied in all four of these positions, leaving the other n \u2212 4 bits alone (i.e. the noise\nis \u2212\u01eb1 [1, 1, 1, 1, 0, . . . , 0]), would the message passing algorithm decode to this minimum distance\nTS? It cannot be guaranteed, but if the code block length is not too long, based on extensive\nempirical evidence, the decoder will decode to this nearby TS for sufficiently large \u01eb1 . This MPA\ndecoding behavior leads to the following theorem.\nTheorem 1 If g \u2265 6, in a {3, 6}-regular code, every (a, b) TS with a > b must contain at least\none 4-bit combination from S2 among the a bits of the TS.\n\n\fProof 1 First notice that in a dominant TS, the TS variable nodes should not be connected to\nmore than one unsatisfied check (USC). If a dv = 3 variable node were, then the 2 or 3 'good'\nmessages coming to that variable node should be enough to flip the bit, thus creating a variable\nnode that is connected to 0 or 1 USC's. Thus, for a dominant TS, we can assume that all variable\nnodes in the TS connected to USC's are connected to only one USC. Now, take any variable node\nin the TS that is NOT connected to any USC's (there will be a \u2212 b of these) and use it as a root\nnode to unroll the graph to the first layer of variable nodes and notice that one of these (dc \u2212 1)dv\npossible combinations of 4-bits (i.e. an element of S2 ) will all be within the a TS bits.\nAs block length n increases, 4-bit impulses begin to behave like the single-bit impulses described\nabove, where there exists a threshold \u01ebt such that for all impulse magnitudes below this threshold\nthe decoder corrects the message and if \u01eb1 > \u01ebt , then the decoder outputs a '1' in the four bits\nwith the impulse and sets the other n \u2212 4 bits to '0'. For rate-1/2 {3, 6} codes this typically\nhappens around n = 2000. One modification to partially avoid this is to scale the other n \u2212 4 bits\nwith another parameter, say \u03b3. In other words instead of sending '1' in the n \u2212 4 noiseless bits,\nsend 0 < \u03b3 < 1. This allows the 'bad' information from our four impulse bits to more thoroughly\npropagate further out into the Tanner graph and simultaneously lessens the magnitude of the\n'good' messages coming in to correct the variable nodes where \u01eb1 was applied. This method also\nloses effectiveness as n increases past 5000.\nFor a still longer code, where say g \u2265 10, the 4-bit impulse method will generally fail unless\nmodified again. To see why, consider the tree of Figure 3, and we will use the convention that the\nall-zeros message is sent and the LLR has the probability of a bit being a zero in the numerator,\nthus a 'good' message which works to correct a variable node will have a (+) sign and a 'bad'\nmessage which works to reinforce the error state will have a (-) sign. For a {3, 6} code, the six\nvariable nodes at variable tier two of the tree (Vp in the Figure) will have four messages coming\nto them: the channel data Lc (+), Lr (\u2212) from checks connected to variable nodes which have an\n\u01eb1 error impulse input, and two Lr (+) messages coming from check nodes connected to noiseless\nvariable nodes. Since the minimum magnitude of the messages which are incoming to the three\ncheck nodes neighboring one of the six Vp variable nodes is equal to \u03b3, the three Lr messages will\nhave roughly the same magnitude with belief propagation and exactly the same magnitude with\nthe min-sum algorithm. Thus, the two positive Lr messages overpower the single negative Lr\nmessage, and the Lc = \u03b34Es /No message provides even more positive weight to our LQ marginal\nprobability calculation. To get around this problem and force the decoder to return dominant error\nevents, we find all possible v-nodes that are at variable tier two and connected to the dv (dv \u2212 1)\ncheck nodes at check tier two of the tree. This will be dv (dv \u2212 1)(dc \u2212 1) = 30 variable nodes\nfor a {3, 6} code. In these positions input an error impulse with a smaller magnitude than \u01eb1 ,\nbut larger than that of the outside parameter +\u03b3 values. Call this second error impulse value \u01eb2 .\nThis extra deterministic noise causes the negative Lr messages floating down to variable node tier\nthree of the tree to have a much larger magnitude than the positive Lr messages coming up, and\nthis stronger 'bad' information is more likely to cause the decoder to fail on a dominant TS.\nTo recap, for a {3, 6} code, the deterministic input to the decoder is now [1 \u2212 \u01eb1 , 1 \u2212 \u01eb1 , 1 \u2212\n\u01eb1 , 1 \u2212 \u01eb1 , 1 \u2212 \u01eb2 , . . . , 1 \u2212 \u01eb2 , \u03b3, . . . , \u03b3]. It is important to have a definition of a TS which takes into\naccount the entire history of the decoding process and not just the final state. This new definition\nwill eliminate ambiguities that arise from the previous definition of a TS, which was based solely\n\n\f1 \u2212 \u01eb1\n\n1 \u2212 \u01eb1\n\nLr (\u2212)\n\nVp\nLr (+)\n\n***\n\n1 \u2212 \u01eb2\n\nLr (+)\n\n***\n\nA total of\ndv (dv \u2212 1)(dc \u2212 1)\nvariable nodes will\nhave error impulses of\n\u01eb2 .\n\u03b3\n\nFigure 3: Tree for Deterministic Noise Input\n\non the combinatorial properties of a bit vector. Since the entire decoding process involves many\nMPA iterations, a formal definition is necessary to locate where in this dynamic process the TS\nstate was achieved. This definition will become important in Section 6.1.\nDefinition 1 During the decoding process, a history of the hard decision, x\u0302l , of the message\nestimate must be saved at each iteration l and if the maximum number of iterations Imax occurs and\nno valid codeword has been found, the TS will be defined as the x\u0302l which satisfies min wH (x\u0302l HT ),\nwhere l = 1, . . . , Imax .\n\nl\n\nA practical example highlighting the power of this search method can be seen with some long\n{3, 6} codes proposed by Takeshita [17]. In two rate-1/2 codes, with n = 8192 and n = 16384, the\nsearch found many codewords at dH of 52 and 56 respectively. This was a great improvement upon\nthe results returned from the Nearest-Nonzero Codeword Search of [16]. Table 1 gives the search\nparameters required to find error events for some larger codes. A total of 204 codewords with\nHamming weight 24 were found in the Ramanujan [18] code, 3775 codewords with Hamming weight\n52 in the Takeshita (8192,4096) code, and an estimated 4928 codewords with Hamming weight 56\nin the Takeshita (16384,8192) code. This last estimate was determined by only searching the first\n93 variable node trees, which took 12 hours, and then multiplying the number of codewords at\nHamming weight 56 found up to that point (28 in this case) by (16384/93). Thus the time of 2112\ncompute-hours, running on an AMD Athlon 2.2 GHz 64-bit processor with 1 GByte RAM, is an\n\n\fCode\n(4896,2448) (Ramanujan)\n(8192,4096) (Takeshita)\n(16384,8192)(Takeshita)\n\n\u01eb1\n5\n6.25\n6.25\n\n\u01eb2\n2\n4\n5\n\n\u03b3\n0.4\n0.45\n0.4\n\nTime (Hrs)\n24\n320\n2112*\n\nTable 1: Search Parameter Values, Eb /No = 6 dB, Max. 50 MPA Iterations\nestimate1 . This reasoning would not hold for most codes, but these algebraic constructions tend\nto have a regularity about them from the perspective of each local variable node tree. Although\nthe large compute-times necessary for this method may seem impractical, for codes of this length,\na simple Monte Carlo simulation would take much longer to find the error floor and it would not\ncollect the dmin TS as this method does.\nFor the three example codes above, the listed Hamming weights are believed to be their dmin .\nThere could be more codewords of these Hamming weights, and it is possible that codewords of\nsmaller weight exist, but this is unlikely and the multiplicity of these codewords found from our\nsearch is probably a tight lower bound on the true multiplicity. The argument here is the same\nused in [16], only this method appears to be more efficient for most types of low-density codes\nand has the advantage of also finding the dominant TS which are usually the cause of an LDPC\ncode error floor.\nAnother way to deal with longer codes is to grow the tree one level deeper, and apply the same\n\u01eb1 to each of the variable nodes at the root, variable tier one, and variable tier two in Figure 3.\nFor a {3, 6} code, this would give sets of 10 bits in which to apply the error impulse. The number\nof these 10-bit combinations for each root node is (dc \u2212 1)dv (dc \u2212 1)dv (dv \u22121) = 59 = 1953125, which\nclearly shows that this method is not nearly as efficient for long codes as it is for short codes. Still,\nthe method will find an error floor, or at least a lower bound on Pf , much faster than standard\nMonte Carlo simulation. It is possible to take variations of these sets of bits; for example, if the\ncode girth is at least 8, then all 6-bit combinations given by the root node, three variable nodes\nat tier one and the first two variable nodes at tier two (V21 , V22 ) are unique and make a good set\nof 6-bit error impulse candidates. The number of bits needed to form an error impulse capable of\nfinding dominant TS is a function of n, k and girth. For larger n, more impulse bits are generally\nrequired. This method has been applied to many rate-1/2 codes with n at least 10000, and there\nwas always a combination of parameters \u01eb1 , \u01eb2 , and \u03b3 which provided an enumeration of dominant\nTS and codewords, leading to the calculation of the error floor in much less time than what a\nstandard Monte Carlo simulation would require.\n\n3.1\n\nIrregular Codes\n\nIrregular codes can be constructed which require a lower Eb /No to reach the 'waterfall' threshold.\nUnfortunately these codes often suffer from higher error floors. The new search technique can\nefficiently determine what types of TS and codewords cause this bad high-SNR performance.\n1\n\nThroughout this paper, jobs requiring compute-times larger than 8-10 hours have been executed on a Linux\ncluster, where each node of the cluster has roughly the same computing power as the aforementioned platform.\nThus, when large compute-times are listed, this can be considered the number of equivalent hours for a desktop\ncomputer.\n\n\fThe method of taking each of the n variable nodes and growing a tree from which we apply\ndeterministic error impulses is the same. The major observation is that nearly all dominant TS\nand codewords in irregular codes contain most of their bits in the low-degree variable nodes. Most\nirregular code degree distributions contain many dv = 2 variable nodes, and these typically induce\nthe low error floor. So, it makes sense to order the n variable nodes from smallest dv to largest\nand perform the search on the smallest (i.e. dv = 2) variable nodes first. In fact, for all irregular\ncodes tested, the search for dominant TS can stop once trees have been constructed for all of the\nvariable nodes with the smallest two dv 's. Note that the number of bits which receive an error\nimpulse is dependent on dv and will be 1 + dv if the tree is only grown down to the first variable\nnode tier. The parameter values for \u01eb1 , \u01eb2 , and \u03b3 are also dependent on the variable node degree\nof the root in a given tree. For example, if the code has variable node degrees of [2 3 6 8], then\nthe associated \u03b3's might be [0.3 0.3 0.4 0.45], i.e. the highest degree variable node of dv = 8 would\nhave an error impulse in 9 bits, and thus it needs less help from the other n \u2212 9 bits to cause an\nerror, so its \u03b3 parameter can be set higher.\n\n3.2\n\nHigh-Rate Codes\n\nLDPC codes of high rate contain check node degrees considerably larger than their lower rate\ncounterparts. For example, in rate 0.8 regular {3, 15} codes, the 4-bit impulse method would\nrequire n(dc \u2212 1)dv = n143 = 2744n decodings, much higher than the 125n decodings for {3, 6}\ncodes. On a positive note, n can grow longer in these types of more densely-packed codes before\nthe search requires the help of the extra \u01eb2 noise. The search was applied to a group of codes\nproposed in [19] and succeeded in locating dominant TS and codewords. One code had column\nweights of 5 and 6 and row weights of 36. Instead of applying \u01eb1 to a variable node from each of\nthe V1i sets in variable tier one, which would require dv + 1 impulse bits, we instead pick a number\nless than dv , call it vnum , in this case 4, and choose all combinations of vnum variable node sets\namong the V1i sets. Assuming the check node degrees are all the same, the number of decodings\n|D| required using this method is given by (7).\n\n|D| =\n\n|dv |\nX\ni=1\n\n|dvi |\n\n\u0012\n\ndvi\nvnum\n\n\u0013\n\n(dc \u2212 1)vnum\n\n(7)\n\nwhere |dvi | denotes the number of variable nodes of degree i and |dv | denotes the number of\ndifferent variable node degrees.\n\n3.3\n\nSearch Parameter Selection\n\nThe choice of search parameters is very important in finding a sufficient list of dominant error\nevents in a reasonable amount of compute time. The purpose of this section is to illustrate how the\nsearch method depends on the magnitude of the error impulse, \u01eb1 , for the simple 4-bit impulse. Our\nexample code, the PEG (1008, 504) {3, 6} code [13] will require n(dc \u2212 1)dv = (1008)53 = 126000\ntotal decodings in the search. Each decoding will attempt to recover the all-zero's message from\nthe deterministic decoder input of \u03b3 = 0.6 and \u01eb1 varying over three values. The SNR parameter\n\n\fError Class\n(6,2)\n(4,2)\n(8,2)\n(10,2)\n(9,3)\n(7,3)\n(12,2)\n(5,3)\n(11,3)\n(10,4)\n(8,4)\n(6,4)\n\nMultiplicity\n5\n6\n3\n3\n27\n57\n1\n21\n2\n5\n8\n21\n\nd2E\n12.47\n12.43\n12.79\n14.29\n19.93\n22.27\n19.61\n29.32\n79.34\n70.70\n77.49\n55.89\n\n|T S|Elem\n5\n6\n3\n3\n27\n57\n1\n21\n2\n5\n7\n21\n\nTable 2: Dominant Error Event Table for (1008,504) PEG - Eb /No = 6 dB, \u01eb1 = 3.0, \u03b3 = 0.6, 50\niterations\nwill be Eb /No = 6 dB and a maximum of 50 BP iterations will be performed. The error impulse,\n\u01eb1 , will take on the values 3, 3.5, and 4. Increasing \u01eb1 increases the number of TS found while the\nmean number of iterations required for each decoding is also increased, which leads to the longer\ncompute times needed to run the search program for larger \u01eb1 . For example, if \u01eb1 = 3, it might\ntake 5 iterations on average to decode a message block. If \u01eb1 is increased to 4, it might take a\nmean of 10 iterations to decode, thus causing the program to take twice as long, even though the\ntotal number of decodings, 126000, stays the same. The base case compute-time for this example\ncode, with \u01eb1 = 3, is slightly under 40 minutes.\nIt appears that for most codes with n < 2000, there is an \u01eb1 , call it \u01eb\u22171 , such that when \u01eb1 is\nincreased above this level, few meaningful error events are discovered beyond those which would\nbe uncovered by using \u01eb\u22171 . So, by determining the probability of frame error contributed by those\nevents found by running the search program with \u01eb\u22171 , we should have a reasonably tight lower\nbound on Pf for the code. How do we best find this \u01eb\u22171 ? There is probably no practical analytical\nsolution to this question, but Tables 2 3 and 4 list the error events returned from the search using\nthree different values of \u01eb1 and will help illustrate the issue. The columns in the tables, from left\nto right, represent the TS class, multiplicity of that class, squared-Euclidean distance to the error\nthreshold found by a deterministic noise directed towards the TS (averaged over each member of\na specific TS class, this error threshold will be explained in Section 4), and the number of TS\nfrom this class that are elementary [14], meaning all unsatisfied checks have one edge connected\nto the TS bits.\nThe major point to observe from the tables is that the first three rows, representing the most\ndominant error events, for this code TS of classes (6, 2), (4, 2), and (8, 2), are unchanged for each\nof the four search executions. This robustness to an uncertain \u01eb\u22171 is important for this method\nto be a viable solution, since \u01eb\u22171 will have to be iteratively estimated for a given code. Extensive\nMonte Carlo simulations with the nominal noise density in the higher SNR region verify that\nindeed the first three rows include the error events most likely to occur.\n\n\fError Class\n(6,2)\n(4,2)\n(8,2)\n(10,2)\n(12,2)\n(9,3)\n(7,3)\n(14,2)\n(11,3)\n(5,3)\n(10,4)\n(8,4)\n(6,4)\n(12,4)\n(7,5)\n(9,5)\n(11,5)\n(13,3)\n(15,3)\n\nMultiplicity\nd2E\n|T S|Elem\n5 12.47\n5\n6 12.43\n6\n3 12.79\n3\n4 14.26\n4\n4 16.08\n4\n89 22.20\n88\n97 23.58\n97\n2 17.74\n2\n8 39.39\n8\n90 33.90\n90\n30 64.40\n29\n82 56.71\n82\n127 55.57\n127\n2 45.85\n0\n15 82.75\n15\n4 98.12\n4\n5 120.52\n4\n1 159.03\n0\n1 183.49\n1\n\nTable 3: Dominant Error Event Table for (1008,504) PEG - Eb /No = 6 dB, \u01eb1 = 3.5, \u03b3 = 0.6, 50\niterations\n\n\fError Class\n(6,2)\n(4,2)\n(8,2)\n(10,2)\n(12,2)\n(9,3)\n(7,3)\n(14,2)\n(11,3)\n(16,2)\n(13,3)\n(5,3)\n(10,4)\n(8,4)\n(12,4)\n(6,4)\n(9,5)\n(11,5)\n(14,4)\n(7,5)\n(15,3)\n(13,5)\n(15,5)\n\nMultiplicity\n5\n6\n3\n4\n6\n113\n110\n3\n49\n1\n9\n104\n124\n469\n12\n384\n104\n10\n3\n176\n3\n1\n1\n\nd2E\n|T S|Elem\n12.47\n5\n12.43\n6\n12.79\n3\n14.26\n4\n16.64\n6\n21.29\n111\n26.59\n109\n17.73\n3\n23.38\n48\n19.36\n1\n52.08\n9\n35.88\n104\n48.36\n116\n45.96\n466\n94.28\n5\n54.95\n383\n90.18\n100\n100.19\n7\n97.89\n1\n80.71\n172\n124.18\n0\n159.03\n1\n183.49\n0\n\nTable 4: Dominant Error Event Table for (1008,504) PEG - Eb /No = 6 dB, \u01eb1 = 4.0, \u03b3 = 0.6, 50\niterations\n\n\f4\n\nLocating TS Error Boundary (Step 2)\n\nOnce a list of potential dominant error events has been compiled, it is not a simple task to\ndetermine which of these bit vectors will cause the decoder the most trouble. For an ML decoder\nthis is not an issue because the Hamming weight, wp\nH , is enough information to determine the\ntwo-codeword error probability: P (x1 \u2192 x2 ) = Q( 2wH Es /No). For a TS, it is possible to\ndetermine the error contribution of a certain bit vector by either conditioning the probability of\nerror on the magnitude of the noise in the direction of the TS bits [4] or by using a mean-shifting\nIS procedure [9]. Both of these methods require a simulation with at least a few thousand noisy\nmessages per SNR to get an accurate measurement of the Pf contributed by the TS in question.\nThe idea proposed here is to send a deterministic noise in the direction of the TS bits and let the\ndecoder tell us the magnitude of noise necessary to cross from the correct decoding region into\nthe error region and use this information to quickly determine relative error performance between\ndifferent TS, not necessarily of different (a, b) type. This method only maps out the point of the\nerror boundary which is along the line connecting the all-ones point in n-dimensional signal space\nwith the point on the n-dimensional hypercube associated with the given TS. It requires only p\ndecodings to find this point of the error boundary with accuracy (lmax \u2212 lmin )/2p since a binary\nsearch, as described below, has complexity O(log L), where L = 2p is the number of quantization\nbins in between lmax , the largest magnitude in a dimension where a TS bit resides, and lmin = 1,\nwhich would be on the error boundary if the TS were an actual codeword. lmax = 3.5 is the value\nused in this research. The procedure to locate the error boundary is to first input the vector\ny = [1 \u2212 I(1)\u01eb, 1 \u2212 I(2)\u01eb, . . . , 1 \u2212 I(n)\u01eb] to the decoder, where I(i) is the indicator function for\nwhether the ith bit belongs to the TS and the magnitude of \u01eb, call it \u01eb1 , is (lmin + lmax )/2. If\nthe decoder corrects this deterministic error input, then for the second iteration, and in general\nfor the ith iteration, update \u01ebi = \u01ebi\u22121 + (lmax \u2212 lmin )/2i and apply this to the decoder input. If\nthe first input vector resulted in an error, then set \u01ebi = \u01ebi\u22121 \u2212 (lmax \u2212 lmin )/2i . This process will\nbe repeated p times, which tells us to within (lmax \u2212 lmin )/2p how close the error boundary is,\nrequiring only p decodings. p = 10 is used in this research and should be more than adequate for\nmost purposes.\nAll non-codeword TS, since they have at least one unsatisfied check (USC), should have a\ndistance to the error boundary that is at least as large as the distance to a wH = a codeword\nerror boundary. Finding the error contribution of a specific TS is analogous to the two-codeword\nproblem, except the error region is much more complicated than the half-space decision region\nresulting from the two-codeword problem. The situation is depicted in Figure 4 where we assume\na (4, 2) TS exists among the first 4 bits of the n-length bit vector. Consider the n \u2212 1 dimensional\nplane bisecting the line joining the 1 vector (all-zeros codeword) and the (-1,-1,-1,-1,1,...,1) TS;\nthis plane would represent a half-space boundary if the TS were actually a codeword and the\ndecoder were ML. The arrow starting at the point 1 (the signal space coordinates of codeword 0)\nand directed towards the TS shows where the error region begins. The shape of the error region\nis very complicated and this two-dimensional figure does not accurately represent its true shape,\nbut it makes intuitive sense that the nearest point in the error region should be in the direction\nof the bits involved in the TS.\nTable 5 lists the dominant TS found with the search for a (504,252) regular {3, 6} code with\ngirth eight [3]. The parameters of the search were set as follows: \u01eb1 = 3.6, \u03b3 = 0.8, 50 iterations,\n\n\fE\n\n(1,1,. . .,1)\n(1-\u01eb,1-\u01eb,1-\u01eb,1-\u01eb,1,. . .,1)\n\n\u221a\n\n(-1,-1,-1,-1,1,. . .,1)\n\u221a\nn\n\nn\n\n(0,0,. . .,0)\nFigure 4: Deterministic Error in TS Direction\n\nEb /No = 5 dB. The column labeled d2E denotes the average Euclidean-squared distance to the\nerror boundary for the TS class in that row. For example, if a deterministic noise impulse with\nmagnitude \u01eb were applied in each of the a bits of an (a, b) = (10, 2) TS and the decoder switched\nfrom correctly decoding to an error at \u01eb = 1.5, then d2E = a\u01eb2 = 10(1.5)2 = 22.5. The rows are\nordered by dominance, where the minimum d2E among all TS of a class determines dominance.\nFor example, the (11,3) TS on average have a larger d2E (23.3 in this case) than the (9,3) TS, which\nhas an average of 20.7, but the minimum d2E among the 20 (11,3) TS is less than the minimum\namong the 186 (9,3) TS. This behavior is in contrast to valid codewords, where all codewords\nwith a given wH will have the same error contribution in the two-codeword problem. The (10, 2)\n2\nTS class had a member with the smallest d2E among all TS found\np for this code. Knowing dE does\nnot tell us exactly what contribution a TS gives to Pf , but Q( 2d2E Es /No) does approximate this\ncontribution to within a couple orders of magnitude.\nOne way to get more confidence in the validity of using d2E as a criteria to establish which\nTS are most dominant is to simulate a large number of noisy message frames using the nominal\nGaussian noise density at a higher SNR and tabulating which TS classes these errors fall into. This\nis done for the PEG (1008,504) code at Eb /No = 4.0 dB and the results are in direct correlation\nwith what is expected from the d2E values computed with our deterministic noise impulse (See\nTable 4 for dominant TS of this code). This code has five (6,2) TS which dominate and two of\nthem have a d2E smaller than the rest (at 4.0 dB), and these specific TS are indeed much more\nlikely to occur with the nominal noise density. Two (8,2) TS are also very dominant. Table 6\nshows all of the TS which had more than one error with the nominal noise density. 6(10)8 trials\nat Eb /No = 4.0 dB were performed and a total of 85 errors were recorded. The value in column\ntwo of Table 6 denotes the number of times the five most dominant error events occur. All but\nfive of these 85 total errors were from TS that were represented in the list in Table 4 obtained\nwith the new search method.\n\n\fError Class\n(10,2)\n(12,2)\n(11,3)\n(9,3)\n(10,4)\n(13,3)\n(8,4)\n(7,3)\n(12,4)\n(6,4)\n(9,5)\n(7,5)\n(11,5)\n(5,3)\n\nMultiplicity\n22\n5\n20\n186\n46\n4\n303\n106\n3\n1178\n15\n41\n1\n24\n\nd2E\n15.01\n15.87\n23.29\n20.68\n28.64\n19.76\n27.78\n26.09\n41.87\n41.14\n27.99\n32.92\n26.82\n41.30\n\n|T S|Elem\n22\n5\n17\n186\n40\n3\n300\n106\n2\n1178\n13\n41\n0\n24\n\nTable 5: Dominant Error Event Table for (504,252) {3, 6} Code - Eb /No = 6 dB, \u01eb1 = 3.5, \u03b3 = 0.6,\n50 iterations\n\nTS\n(6, 2)1\n(6, 2)2\n(8, 2)1\n(8, 2)2\n(10, 2)1\n\n#Errors\n28\n20\n11\n8\n3\n\nd2E at 4.0 dB\n14.31\n14.36\n14.35\n14.46\n15.54\n\nTable 6: Monte Carlo Verification of d2E\n\n\f5\n\nImportance Sampling (IS) (Step 3)\n\nIn IS, we statistically bias the received realizations in a manner that produces more errors [5, 6, 7].\nInstead of incrementing by one for each error event (Ie ), as for a traditional Monte Carlo (MC)\nsimulation, a 'weight' is accumulated for each error to restore an unbiased estimate of Pf . This\nstrategy, if done correctly, will lead to a greatly reduced variance of the estimate compared to\nstandard MC.\nf \u2217 (y) denotes the (IS) biasing density and it is incorporated into the MC estimate as follows:\nPf , E[Ie (y)]\nZ\n=\nIe (y)f (y)dy\nRn\nZ\nf (y) \u2217\n=\nIe (y) \u2217\nf (y)dy\nf (y)\nRn\n= E\u2217 [Ie (y)w(y)]\nThis gives an alternate sampling estimator\nL\n\nP\u0302fIS\n\n1X\nIe (yl )w(yl )\n=\nL l=1\n\n(8)\n\nL realizations yl are generated according to f \u2217 (y), the biased density. If yl lands in the error\nl)\nregion then the weight function, w(yl ) = ff\u2217(y\n, is accumulated to find the estimate of Pf . MC\n(yl )\ncan be seen as a special case of this more general procedure, with f \u2217 (y) = f (y). P\u0302fIS is unbiased\nand has a variance given by\n\nV ar[P\u0302fIS ] = E\u2217 [\n\nL\n1 X\n(\nIe (yl )w(yl ))2 ] \u2212 Pf2\nL2 l=1\n\n1\n(LE\u2217 [Ie2 (y)w 2 (y)] + L(L \u2212 1)Pf2 ) \u2212 Pf2\nL2\nE\u2217 [Ie (y)w 2(y)] \u2212 Pf2\n=\nL\nE[Ie (y)w(y)] \u2212 Pf2\n=\nL\nR\nw(y)f (y)dy \u2212 Pf2\n=E\nL\n=\n\n(9)\n\nNotice that because f \u2217 (y) is in the denominator of w(y), it must be non-zero over the error\nregion E, else V ar[P\u0302fIS ] is unbounded. The key quantity here is the first term on the RHS of the\nlast line in (9). For the IS method to offer a smaller variance than MC, this quantity must be less\n\n\fthan Pf which appears as the first term on the RHS of V ar[P\u0302fM C ]. We will denote this term as\nV and estimate it on-line using MC, where the samples are taken from f \u2217 (y).\nV\u0302\n\n=\n\n1\nL\n\nL\nP\n\nw(yl )2\n\n(10)\n\nl=1\n\nBoth V\u0302 and P\u0302fIS rely on the same samples, yl , and this circular dependence means that if we\nare underestimating Pf , then we are likely underestimating V . V\u0302 can give us some confidence\nin P\u0302fIS , but the simulator must make sure that P\u0302fIS passes several consistency checks first. It\nwill most often be the case that P\u0302f is being underestimated. One check is employing the sphere\npacking bound [20], which can be used as a very loose lower bound that no code could possibly\nexceed. One benefit of tracking V\u0302 is that if it indicates a poor estimate, then P\u0302f is definitely not\naccurate, but the converse is not true.\nUsing an f \u2217 with the same properties as the nominal f , except for a shifted mean to center\nthe new density at the error boundary, has been shown to provide the smallest V ar[P\u0302fIS ] in the\ntwo-codeword problem [7, 5] and large deviations theory [21] also suggests that mean-shifting\nto the nearest error regions in n-dimensional signal space should provide the optimal f \u2217 . The\nf \u2217 proposed for determining error performance of LDPC codes is based on a weighted sum of\nmean-shifted fi\u2217 densities, where there are M nearby error events used to form f \u2217 .\nM\n1\n|y \u2212 \u03bci |2\n1 X\nexp\n\u2212\nf (y) =\nM i=1 (2\u03c0\u03c3 2 )n/2\n2\u03c3 2\n\u2217\n\n(11)\n\nThe \u03bci are n-bit vectors with zeros in all places except for ones in the a bits of an (a, b)\ndominant TS or the wH bits of a low-weight codeword. The choice of using a magnitude of '1' in\nthe a TS bit positions is probably not the most efficient. When shifting towards valid codewords,\n'1' is the optimal value for this mean-shifted f \u2217 [7]. The distance to the error boundary for a TS,\nas found in step two of our procedure, always has a magnitude \u2265 1 in the a bit positions of the\nTS. So, if mean-shifting to the boundary of the error region is the most efficient IS procedure,\nthen this shift value should be used instead of '1'. Since the two-codeword error regions of TS are\nnot in the shape of a half-space, this argument is not quite correct, so although a more efficient\nsimulation can be performed by increasing the shift value to have a magnitude larger than one,\ncare must be used to not over bias the shift point, which will return a P\u0302fIS which is too small\n[6]. This weighted-sum IS density should catch many of the 'inbred' TS which were not explicitly\ncaught in the initial TS search phase, but which share many bits with the TS vectors that were\nfound and thus are 'close by' in n-dimensional decoding space.\nThe weighted-sum f \u2217 IS density should work best at moderate SNR where many error events\ncontribute to the error floor. At the highest SNR's of interest, large deviations theory suggests\nthat only the nearest error events in n-dimensional decoding space contribute to Pf [22]. If there\nare a small number of these nearest events, then it is appropriate to break E up into \u03b1 regions\nEi , i = 1, . . . , \u03b1 corresponding to each of the minimum distance error events. \u03b1 is the total number\nof these minimum distance events. An example of this technique used on a {4, 8} code will be\ngiven in Section 7.\n\n\f6\n\nError Floor Estimation Procedure\n\nOur procedure consists of three steps which all make use of the decoding algorithm to map out the\nn-dimensional region of E and estimate Pf . Since this region is very dependent on the particular\ndecoding algorithm and its actual implementation details, e.g. fixed-point or full double-precision\nvalues for messages, it is imperative to make use of the specific decoder to determine E.\nThe first step uses the search method proposed in [3] and expanded upon in Section 3 to obtain\na list of dominate error events. This list is dependent on the decoding algorithm since certain\nTS might be more problematic for say the full belief propagation implementation as opposed to\nthe min-sum approximation. The size of the list is also a function of the parameters \u01eb1 , \u01eb2 , \u03b3, and\nEb /No . It is important to choose these parameters to find all of the dominant error events, while\nstill keeping the average number of iterations per decoding as small as possible to avoid wasting\ncomputing time. We will see in some examples that as long as all of the minimum distance error\nevents are included in this list, it is possible to miss some of the moderately dominant error events\nand still get an accurate estimate of Pf . This is an important benefit over the method which\nbreaks up the error region into separate pieces for each possible bit vector as proposed in [4, 9],\nwhere the estimate of Pf will almost assuredly be below the true value, as it is nearly impossible\nto guarantee all important error events have been accounted for, especially for longer and higher\nrate codes.\nTo expand on this, consider a simple toy example where there are six dominant error regions\nand our initial list contains all but one of these. Figure 5 shows the error region surrounding the\nall-zeros codeword. The small, grey filled circles between the all-ones point in n-dimensional space\nand the nearest error regions are the mean-shift points in f \u2217 . The single white-filled circle in front\nof E1 represents the dominant error event which was not accounted for in the initial list obtained\nwith the search from step one of the three-step procedure. Notice that the two mean-shift points\ntowards the regions E2 and E6 are 'close enough' in n-dimensional space to have a high probability\nof landing some f2\u2217 or f6\u2217 noise realizations in E1 . This will allow the E1 contribution to be included\nin the total Pf estimate. If, on the other hand, there are Ei such that no mean-shift points are near\nenough to these regions to have significant probability of producing noise realizations in them,\nthen P\u0302fIS will with high probability (essentially the same probability of not getting a hit in Ei )\nunderestimate Pf by the amount of the error probability that lies in Ei . So the paradox is that\neven though P\u0302fIS is unbiased, it can with high probability underestimate the true Pf when using\nthis particular f \u2217 .\nTo form an f \u2217 which adequately covers the error region without needlessly including too many\nshift points which are unlikely to offer any error region hits and only serve to complicate f \u2217 , we\nmake use of the d2E values returned from stage two. The third column of Table 5 lists the d2E values\nfor a (504,252) {3, 6} girth eight LDPC code [3]. From this column, it can be seen that the (10,2)\nand (12,2) TS are the most dominant error events. The (6,4) TS have a very large multiplicity of\nat least 1178, but with an average d2E of 41.14, they don't contribute much to Pf at higher SNR.\nStill, there are some (6,4) TS with much smaller d2E than the average over this class, so some of\nthese should be included in f \u2217 . Thus, a good strategy would be to order the entire list provided\nby the search and pick the M TS with the smallest d2E to include in f \u2217 for the third step of the\nprocedure. M will be based on the parameters n and k of the code as well as the size of the initial\nTS list. A larger M will provide a more accurate estimate of Pf , but will require more total noisy\n\n\fE3\nE2\n\n[1, * * * , 1]\n\nE1\n\nE4\n\nE5\nE6\n\nFigure 5: IS mean-shifting to cover error region\n\nmessage decodings if we use a fixed number of decodings P for each mean-shifted fi\u2217 .\nThe final step of our procedure takes the M error events with the smallest d2E and forms a\nweighted sum of mean-shift fi\u2217 densities for f \u2217 (11). Equation (12) below confirms that deterministically generating P realizations for each of our M fi\u2217 densities will form a valid, unbiased,\nP\u0302fIS .\n\nP\u0302fIS\n\nM\nP\n1 X 1 X Ie (y)f (y)\n=\nM\nM m=1 P l=1 1 P\nfp\u2217 (y)\nM\n\n(12)\n\np=1\n\nThe expected value is\nM\nM Z\n\u2217\n1 X\nIe (y)f (y)\n1 X\nf (y)fm\n(y)\nE[P\u0302fIS ] =\nE[ M\n]=\ndy\nM\nP\nP\nM m=1\nM m=1\n1\n1\n\u2217\n\u2217\nfp (y)\nfp (y)\nE M\nM\np=1\n\n1\n=\nM\n\nZ f (y)\nE\n\n1\nM\n\nM\nP\n\nm=1\nM\nP\n\np=1\n\np=1\n\n\u2217\nfm\n(y)\n\nfp\u2217 (y)\n\ndy =\n\nZ\n\nf (y)dy = Pf\n\nE\n\nOne implementation issue which must be addressed concerns the numerical accuracy of the\nweight function calculation. Any computer will evaluate ex = 0 when x < \u2212N for some positive\nN. When the block length is large or the SNR is high, all of the terms in f \u2217 could equate to\nzero, giving a weight of x0 = \u221e. To avoid this, a very small constant term can be added in both\n\n\fthe f (y) and f \u2217 (y) distributions which will, with high probability, ensure that |x| < N while not\naffecting the value of the weight function. The constant term \u03c8 is added in the second line of the\nfollowing equation:\n(2\u03c0\u03c3 2 )n/2\nw(y) =\n(2\u03c0\u03c3 2 )n/2\n\n1\nM\n\nm=1\n\n2\nexp \u2212 (y\u22121)\n2\n2\u03c3\n\nexp \u03c8\n=\n=\nM\nP\n(y\u22121+\u03bcm )2\n1\nexp \u03c8 M\nexp \u2212 2\u03c32\nm=1\n\nIf \u03c8 is chosen to be\n\nn\n,\n2\n\n2\n\n2\n\nexp \u2212 (y\u22121)\n2\u03c32\n=\nM\nP\n(y\u22121+\u03bcm )2\nexp \u2212 2\u03c32\n1\nM\n\n1\nM\n\n+ \u03c8)\nexp(\u2212 (y\u22121)\n2\u03c32\nM\nP\n2\nm)\nexp(\u2212 (y\u22121+\u03bc\n+ \u03c8)\n2\u03c32\n\nm=1\n\n2\n\nexp \u2212 (y\u22121)\n2\u03c32\nM\nP\n2\nm)\nexp \u2212 (y\u22121+\u03bc\n2\u03c32\n\nm=1\n\nthen the argument in the exponent of the P\nterm in the denominator\n\ncorresponding\nto the fi\u2217 centered about the ith shift point will be \u2212(\nP\nn\n2\ni=1 (ni )\n2\u03c32\n\n(13)\n\n\u03c32\n\nn\n2\ni=1 (ni )\n2\u03c32\n\n\u2212 n2 ). Since the\n\nz =\nterm is a scaled \u03c72 random variable with E[z] = n 2\u03c32 = n2 , all M terms in the\ndenominator of the weight function will be zero only if all of the \u03c72 random variables fall more\nthan N from their mean, which is highly unlikely, especially for large n and high SNR.\n\n6.1\n\nNew Error Events\n\nOne way to gain insight into how well the initial list contains important error events is to keep\ntrack of all errors which occur during the IS simulation, and call these 'hits.' When a hit occurs,\nwe see if this TS is the same as the bit string towards which we biased for that noise realization.\nIf it is, this will be called an 'intended hit.' As SNR increases, the number of intended hits should\napproach the number of hits, because the noise 'clouds' are more concentrated and less likely to\nstray from the error region near the shift point. If the decoder does not converge to the all-zeros\ncodeword after the maximum number of iterations and a new TS, as defined by Definition 1,\noccurs during the decoding process which is not among the initial M shift points, then add this\nnew error event to a cumulative list. Continue making this list of new error events and their\nfrequency of occurrence. The list of new error events will gauge how thorough the initial list of\nTS covers the error region. Because noise realizations have occurred in the new error regions\nassociated with the new error events, these regions are effectively considered in P\u0302fIS , even though\nthey weren't included in the M shift-points ahead of time. The new TS should contain many bits\nin common with some of the more dominant TS returned from the search procedure of step one.\nThe M shift points can be adaptively increased as new events with small d2E are discovered. If\nthe parameters used in the search of step one are chosen wisely, then the initial list of shift points\nshould adequately cover the error region and the list of new error events will be small.\nIt is common for the empirical variance, (10), to underestimate the true variance in the highSNR region where the noise clouds are small. This is because almost all of the hits are intended\nhits, so the noise realizations don't venture towards new error regions, where hits are likely to\ncause a large w(y). So, in the high-SNR region, when the initial TS list excludes some dominant\nerror events, these regions are being ignored in P\u02c6f IS and V\u0302 . For example, consider a standard\nMonte Carlo simulation. At high SNR, we explore the n-dimensional error region centered about\nthe all-ones point (all-zeros codeword) with 107 noisy messages. Let the true Pf be 10\u22128 at\n\n\f7\n\nthis SNR. Thus with probability (1 \u2212 10\u22128 )10 = 0.9048 we would not get any hits in 107 trials,\nresulting in an estimate of P\u02c6f M C = 0 and an empirical variance also equal to zero. Now, using\n(11), imagine placing the initial point of reference at one of the M shift points located between\nthe all-ones point and the dominant error event boundaries. Since most of the noise realizations\nfall much closer to the intended error region, there will be many more hits in this error region and\nthe error contribution of at least those TS and codewords among the list of M shift points will be\ncounted. Still, for all but relatively short codes, this list will be incomplete and a large V\u0302 could\nbe obtained by having one or more noise realizations land in error regions not included among\nthe M points in the initial list that are closer to the all-ones vector than any of our shift points.\nThis will produce a weight greater than one, which will significantly increase V\u0302 . Although this\nwill give a large V\u0302 , it is still better to know that this previously undiscovered error event exists.\nThe alternative situation, where no new error events are discovered, will produce a small V\u0302 , but\nthis is reminiscent of the Monte Carlo example above where the regions of E not associated with\nour M shift points are ignored.\nIS is no magical tool, and it really only helps when we know ahead of time (steps one and\ntwo of the procedure) where the nearest error regions are. What we gain from using (11) as our\nIS f \u2217 is a significant reduction in the number of samples needed to get a good estimate of Pf\ncompared to the method of finding the Pf contributed by each individual error event as detailed\nin [9, 4]. There is also a better chance of accounting for the Pf contributed by those error events\nwhich were not explicitly enumerated with the search of step one in our procedure, but are 'close\nby' in n-dimensional decoding space to some of the vectors that were in the list. Still, it must\nbe stressed that IS is a 'dumb' procedure that helps when we already have a very good list of\ndominant TS and codewords for the given code.\n\n6.2\n\nComplexity\n\nIt is difficult to attach a measure of complexity to our entire procedure. Traditionally, a 'gain'\nmetric is measured in an IS simulation, usually a ratio of the number of samples required to achieve\na certain variance for P\u02c6f using Monte Carlo versus applying IS. This metric is essentially useless\nwhen applying IS to the analysis of the error performance of large LDPC block codes. The online\nvariance estimator of (10) which is typically used to determine the number of samples required to\nachieve a variance comparable to Monte Carlo for a given SNR is, as outlined above, not reliable.\nAnother often overlooked aspect of using IS to simulate decoding errors is the increase in the\nmean number of iterations required to decode when f \u2217 causes most noise realizations to fall near\nand in the error region, thus requiring the decoder to 'work harder' to find a valid codeword.\nWhen the list of mean-shift candidates in f \u2217 is large, the calculation of the weight function is\nalso time consuming. So, ultimately, the best choice of metric for measuring complexity will be\nthe less elegant - but more accurate - total compute-time required to run the IS simulation. This\nwill incorporate the total number of noise samples, the extra iterations required for the MPA to\ndecode shifted-noise realizations, and the weight function calculation. To measure complexity of\nthe entire three-step process, also include the time required to search for and determine relative\ndominance of the initial list of TS from steps one and two.\n\n\f7\n\nSimulation Results\n\nWe now compare the error performance analysis results obtained from the three-step procedure\nwith a standard Monte Carlo estimate to see if the results concur. The Cole (504,252) code [3]\nwith girth eight and a progressive edge growth [23] code on the MacKay website [13] will be used\nas a test case. Figure 6 has Monte Carlo data points up to Eb /No = 5 dB for both codes. The\nhighest SNR point at 5 dB for the Cole code involved (3.15)109 trials and 13 errors were collected,\nso P\u0302fM C is not very accurate. It took about 6000 compute-hours on the cluster to obtain this\npoint. The IS data for both codes, while slightly underestimating the true Pf , only required 12\nminutes to obtain the dominant TS list in step one, negligible time to determine d2E for step two,\nand 2 hours for the actual IS simulation.\nPEG (504, 252) versus Cole (504,252)\n\n0\n\n10\n\nCole MC\nMacKay MC\nCole IS\nMacKay IS\n\u22125\n\nFER\n\n10\n\n\u221210\n\n10\n\n\u221215\n\n10\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nE /N (dB)\nb o\n\nFigure 6: Comparison of (504,252) codes\nThe phenomenon of underestimating Pf using the f \u2217 of step three is the most troublesome\nweakness in our procedure. Figure 7 shows three curves representing the IS estimate of the\n(1008,504) Cole code. All three use the same f \u2217 , but the number of samples per SNR varies\namong 39000, 390000, and (3.9)106. As more trials are performed, more of the error region gets\nexplored, thus increasing P\u02c6f IS . This effect is strongest at lower SNR where the noise clouds are\nlarger and our f \u2217 is less like the 'optimal' f \u2217 .\nIn all of the following results, a maximum of 50 MPA iterations were performed in decoding.\nTable 7 lists the parameters used in the search phase of our procedure for a number of different\ncodes. A value of 1 \u2212 \u03b3 in the \u01eb2 column means \u01eb2 was essentially not used and only the 4-bit\nimpulse with magnitude \u01eb1 was used, and the rest of the n \u2212 4 bits were scaled by \u03b3. \u01eb2 is usually\nonly required for larger codes.\nThe column labeled 'Mean d2E ' in Table 8 is calculated over all |EE| of the error events, not\njust the ones that fall below the threshold, d2E < d2ET . The 'Time' column in Table 9 is on an AMD\nAthlon 2.2 GHz processor with 1 GByte RAM.\n\n\fCode\n(504,252) (Cole)\n(504,252) PEG (Hu)\n(603,301) Irregular (Dinoi)\n(1008,504) (Cole)\n(1008,504) PEG (Hu)\n(2640,1320) (Margulis)\n(4896,2448) (Ramanujan)\n(1000,500) {4, 8} (MacKay)\n\n\u01eb1\n3.6\n3.6\n3.5\n4\n4\n5\n5\n2.5\n\n\u01eb2\n1\u2212\u03b3\n1\u2212\u03b3\n1\u2212\u03b3\n1\u2212\u03b3\n1\u2212\u03b3\n1\u2212\u03b3\n2\n1\u2212\u03b3\n\n\u03b3\n0.8\n0.8\n0.3\n0.7\n0.7\n0.3\n0.4\n0.4\n\nEb /No (dB) Time (Hrs)\n5\n0.2\n5\n0.2\n5\n1.5\n7\n1.2\n7\n1.2\n6\n8.2\n6\n24.0\n8\n1.7\n\nTable 7: Step one parameter values\n\nCode\n(504,252) (Cole)\n(504,252) PEG (Hu)\n(603,301) Irregular (Dinoi)\n(1008,504) (Cole)\n(1008,504) PEG (Hu)\n(2640,1320) (Margulis)\n(4896,2448) (Ramanujan)\n(1000,500) {4, 8} (MacKay)\n\n|EE|\n578\n1954\n10760\n750\n1700\n2640\n204\n119\n\nd2ET\n29\n60\n60\n20\n\n|EE|<d2E\nT\n578\n1954\n1499\n390\n1007\n2640\n204\n1\n\nMin d2E\n14.58\n11.04\n15.47\n21.45\n13.46\n31.97\n24.00\n17.16\n\nAvg d2E\n35.15\n30.68\n49.29\n57.20\n52.90\n32.00\n24.00\n17.16\n\nTable 8: Step two parameter values\n\nCode\n(504,252) (Cole)\n(504,252) PEG\n(603,301) Irregular (Dinoi)\n(1008,504) (Cole)\n(1008,504) PEG (10,2)\n(2640,1320) (Margulis)\n(4896,2448) (Ramanujan)\n(1000,500) {4, 8} (MacKay)\n\n# Trials/SNR\n195400\n115600\n29980\n3900000\n302100\n208600\n204000\n40000\n\n|EE|new Time (Hrs)\n209\n2.0\n5649\n2.0\n3525\n4.2\n1574\n78.8\n1842\n5.0\n1001\n19.5\n120\n60.0\n2\n2.0\n\nTable 9: Step three parameter values\n\n\fCole (1008, 504) IS Results\n\n0\n\n10\n\n39k Trials\n390k\n3.9M\n\n\u22125\n\nFER\n\n10\n\n\u221210\n\n10\n\n\u221215\n\n10\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nEb/No (dB)\n\nFigure 7: Number of trials effect on P\u0302fIS\nApplying the method to larger codes, an irregular code, and a {4, 8} code highlights the\ngenerality of this three-step IS method. The search step does not find all of the (14, 4) TS in the\nMargulis (2640,1320) code [24], but these are all supersets of (12, 4) TS which are all discovered.\nWhen keeping track of new error events in step three, many errors land in (14, 4) TS, thus the Pf\nassociated with these (14, 4) TS is accounted for in the total Pf . The IS results for the Margulis\ncode are shown in Figure 8(b). Step one found 204 codewords with a minimum wH of 24 in\nthe Ramanujan (4896, 2448) code [18], and while this agrees with the number found in [16], it's\npossible some were missed. The Ramanujan code has an error floor dominated by valid codewords,\nbut step three of our procedure does find many non-codeword TS that are not included among the\ninitial list of 204 mean-shift points. As seen in Figure 8(d) the total P\npf obtained from IS hovers\nabout an order of magnitude above the simple approximation 204Q( 2(24)Es /No). Step three\ncould be applied to the quadratic permutation polynomial (8192,4096) code [17] using the 3775\nweight-52 codewords found in step one as the shift points. However, since n is larger for this code\nand the 3775 mean-shift points create a slow weightpfunction calculation, a quick alternative to a\nfull IS simulation is to lower bound Pf by 3775Q( 2(52)Es /No ). If step three were\np performed,\n\u02c6\nthen a more accurate measurement of Pf could be obtained, i.e. Pf IS \u2265 3775Q( 2(52)Es /No).\nThese examples further highlight how step three of our procedure is a better strategy for finding\nthe Pf of a code than the previously proposed methods of considering only the Pf contributed by\neach individual error event and then summing these.\nFigure 8(c) shows how a MacKay {4, 8} (1000,500) code has a major advantage over the\ntraditionally considered {3, 6} codes in the error floor region. The MacKay (1000,500) code has\nsome 4-cycles and its dominant error event is a single (9,4) TS. No valid codewords were found.\nEven though the girth is only four, the {4, 8} code has an error floor significantly lower than the\ncomparably-sized (1008,504) {3, 6} code with a girth of six. So, although these extra cycles affect\nthe decoder's threshold region adversely, they do not degrade the high-SNR performance and in\n\n\fDinoi Irregular LDPC (603, 301)\n\n0\n\nMargulis (2640, 1320) IS Results\n\n0\n\n10\n\n10\nMonte Carlo\nIS\nUnion of low\u2212weight codewords\n\nMonte Carlo\nIS\n\n\u22125\n\n\u22125\n\n10\n\nFER\n\nFER\n\n10\n\n\u221210\n\n10\n\n\u221215\n\n\u221215\n\n10\n\n10\n\n\u221220\n\n10\n\n\u221210\n\n10\n\n\u221220\n\n1\n\n2\n\n3\n\n4\nEb/No (dB)\n\n5\n\n6\n\n10\n\n7\n\n1.5\n\n2\n\n2.5\n\n(a) Dinoi (603,301)\n\n3.5\n4\nEb/No (dB)\n\n4.5\n\n5\n\n5.5\n\n6\n\n(b) Margulis (2640,1320)\n\nMacKay 4,8 (1000, 500) IS Results\n\n0\n\n3\n\nRamanujan (4896, 2448) IS Results\n\n0\n\n10\n\n10\nMonte Carlo Pf\n10\n\nf\n\nMonte Carlo P\n\ns\n\no\n\n\u22124\n\n10\n\nMacKay 3,6 (1008,504) Pf\n\n\u22125\n\n204Q((2(24)E /N )0.5)\n\nb\n\nIS Pb\n10\n\nMonte Carlo\nIS\n\n\u22122\n\nIS P\n\n\u22126\n\nFER\n\nFER\n\n10\n\n\u22128\n\n10\n\n\u221210\n\n10\n\n\u221210\n\n10\n\n\u221212\n\n10\n\n\u221214\n\n10\n\u221215\n\n10\n\n\u221216\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\nEb/No (dB)\n\n4.5\n\n5\n\n(c) MacKay (1000,500)\n\n5.5\n\n6\n\n10\n\n1.5\n\n2\n\n2.5\n\n3\n3.5\nEb/No (dB)\n\n4\n\n(d) Ramanujan (4896, 2448)\n\nFigure 8: Simulation results\n\n4.5\n\n5\n\n\ffact the extra 33% of edges in the graph improves the error floor performance substantially.\nThe (603,301) irregular code developed in [25] has a low error floor and the new method is an\nefficient way to measure this code's performance. The first step returns a list of 10760 TS, and\nwe use all 1499 of them which have d2E < 29. Figure 8(a) shows P\u02c6f IS for this code. The curve\nmatches well with the results in [25] up to where the Monte Carlo data ends at Eb /No = 4 dB.\nThe\npsimulated curve extending into the higher SNR region stays above the known lower bound of\nQ( 2(15)Es /No) caused by the code's single weight-15 codeword. These two checks reinforce our\nconfidence in the validity of the high SNR performance results given by the three-step method\ndescribed in this paper.\nFinally, using (1008,504) {3, 6} codes, we also compare the performance of the full belief\npropagation MPA with the min-sum MPA. These surprising results are shown in Figure 9, where\nwe see that at higher SNR (above 3.5 dB in this case) the min-sum algorithm actually performs\nbetter than the much more complex belief propagation implementation! This behavior is evident\nfor many codes that have an error floor dominated by non-codeword TS. The degree to which minsum outperforms BP is code dependent, but for some codes it is very significant. This surprising\nresult is an example of the usefulness of the low BER analysis tools presented in this paper.\nPEG (1008, 504) versus Cole (1008,504)\n\n0\n\n10\n\nMacKay IS BP\nCole IS BP\nMacKay IS MS\nCole IS MS\n\u22125\n\nFER\n\n10\n\n\u221210\n\n10\n\n\u221215\n\n10\n\n3\n\n3.5\n\n4\nEb/No (dB)\n\n4.5\n\n5\n\nFigure 9: Comparison of (1008,504) codes\n\n8\n\nConclusions\n\nThe work presented here is very helpful in the analysis of LDPC code error floors. We developed a\nprocedure that first uses a novel search technique to find possibly dominant error events, then uses\na deterministic error impulse to determine which events in the initial list are truly dominant error\nevents, and then performs a traditional mean-shifting IS technique to determine code performance\nin the low bit error region. This novel and general result has applicability to the analysis of most\nclasses of regular and irregular LDPC codes and many decoders.\n\n\fThe procedure also provides the ability to accurately analyze and iteratively adjust the code\nand decoder behavior in the error floor region, a useful tool for applications that must have a\nguaranteed very low bit error rate at a given SNR. One of the byproducts of this research is the\nobservation that the min-sum decoding algorithm is just as good as, and in some cases better,\nthan the full belief propagation algorithm at sufficiently high SNR. In fact, this SNR is usually\njust slightly higher than where Monte Carlo simulations typically end, thus the result was always\nslightly out of reach of previous researchers. It was also shown that the class of regular {4, 8}\ncodes can provide a lower error floor than comparable-length {3, 6} codes.\nIt is our belief that the methods outlined in this paper, while not completely solving the\nproblem of finding the low-weight TS spectrum and error floor for the general class of long LDPC\ncodes, will still have a big impact on how researchers evaluate LDPC codes and decoders in the\nhigh SNR region. The work will provide a solid foundation for others to build upon to attack\neven longer LDPC codes.\n\nReferences\n[1] R.G.Gallager, Low-Density Parity-Check Codes. MIT Press, 1963.\n[2] D. MacKay, \"Good error correcting codes based on very sparse matrices,\" IEEE Trans. Inf.\nTheory, vol. 45, no. 3, pp. 399\u2013431, Mar 1999.\n[3] C. A. Cole, S. G. Wilson, E. K. Hall, and T. R. Giallorenzi, \"Analysis and design of moderate length regular LDPC codes with low error floors,\" Conf. on Information Sciences and\nSystems, Mar 2006.\n[4] T. Richardson, \"Error floors of LDPC codes,\" Allerton Conference, 2001.\n[5] R. Srinivasan, Importance Sampling - Applications in Communications and Detection.\nSpringer-Verlag, 2002.\n[6] P. J. Smith, M. Shafi, and H. Gao, \"Quick simulation: A review of importance sampling\ntechniques in communications systems,\" IEEE JSAC, vol. 15, no. 4, pp. 597\u2013613, May 1997.\n[7] X. Wu, \"IS - Block and Convolutional Codes,\" Master's thesis, University of Virginia, May\n1995.\n[8] B. Xia and W. Ryan, \"On importance sampling for linear block codes,\" in Proc. IEEE ICC,\nvol. 4, May 2003, pp. 2904\u20132908.\n[9] E. Cavus, C. Haymes, and B. Daneshrad, \"A highly efficient importance sampling method\nfor performance evaluation of LDPC codes at very low bit error rates,\" Submitted, IEEE\nTrans. Commun.\n[10] R. Holzlohner, A. Mahadevan, C. Menyuk, J. Morris, and J. Zweck, \"Evaluation of the very\nlow BER of FEC codes using dual adaptive importance sampling,\" IEEE Comm. Letters,\nvol. 2, Feb 2005.\n\n\f[11] R. M. Tanner, \"A recursive approach to low complexity codes,\" IEEE Trans. Inf. Theory,\nvol. 27, no. 5, pp. 533\u2013547, Sept 1981.\n\n[12] W. Ryan, \"LDPC tutorial,\" http://www.ece.arizona.edu/\u223c ryan/New%20Folder/ryan-crc-ldpc-chap.pdf.\n[13] D. Mackay, \"Mackay codes web site,\" http://www.inference.phy.cam.ac.uk/mackay/codes.\n[14] O. Milenkovic, E. Soljanin, and P. Whiting, \"Asymptotic spectra of trapping sets in regular\nand irregular LDPC code ensembles,\" Submitted IEEE Trans. Inf. Theory, 2005.\n[15] C. Berrou, S. Vaton, M. Jezequel, and C. Douillard, \"Computing the minimum distance of\nlinear codes by the error impulse method,\" IEEE GlobeComm'02, vol. 2, pp. 1017\u20131020, Nov\n2002.\n[16] X.-Y. Hu, M. P. C. Fossorier, and E. Eleftheriou, \"On the computation of the minimum\ndistance of LDPC codes,\" IEEE ICC'04, 2004.\n[17] O. Y. Takeshita, \"A new construction for LDPC codes using permutation polynomials over\ninteger rings,\" Submitted, IEEE Trans. Inf. Theory, 2005.\n[18] J. Rosenthal and P. Vontobel, \"Constructions of LDPC codes using Ramanujan graphs and\nideas from Margulis,\" in Proc. 38th Allerton Conf. Commun. (ICC). Monticello, Illinois,\nOctober 2000, pp. 248\u2013257.\n[19] S. Song, L. Lan, S. Lin, and K. Abdel-Ghaffar, \"Construction of quasi-cyclic LDPC codes\nbased on the primitive elements of finite fields,\" Conference on Info. Sciences and Systems,\nMar 2006.\n[20] C. E. Shannon, \"Probability of error for optimal codes in Gaussian channel,\" Bell Syst. Tech.\nJ., vol. 38, pp. 611\u2013656, 1959.\n[21] J. A. Bucklew, Introduction to Rare Event Simulation. Springer, 2004.\n[22] J. S. Sadowsky and J. A. Bucklew, \"On large deviation theory and asymptotically efficient\nMonte-Carlo estimation,\" IEEE Trans. Inf. Theory, vol. 36, no. 3, pp. 579\u2013588, May 1990.\n[23] X. Hu, E. Eleftheriou, and D. Arnold, \"Progressive edge-growth Tanner graphs,\" IEEE\nGlobeComm'01, vol. 2, pp. 995\u20131001, Nov 2001.\n[24] G. A. Margulis, \"Explicit constructions of graphs without short cycles and low-density codes,\"\nCombinatorica, vol. 2, no. 1, pp. 71\u201378, 1982.\n[25] L. Dinoi, F. Sottile, and S. Benedetto, \"Design of variable-rate irregular LDPC codes with\nlow error floors,\" in Proc. IEEE ICC, vol. 1, May 2005, pp. 647\u2013651.\n\n\f"}