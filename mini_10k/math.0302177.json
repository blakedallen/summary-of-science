{"id": "http://arxiv.org/abs/math/0302177v2", "guidislink": true, "updated": "2003-06-04T13:46:52Z", "updated_parsed": [2003, 6, 4, 13, 46, 52, 2, 155, 0], "published": "2003-02-14T21:01:20Z", "published_parsed": [2003, 2, 14, 21, 1, 20, 4, 45, 0], "title": "Random Weighting, Asymptotic Counting, and Inverse Isoperimetry", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0302230%2Cmath%2F0302133%2Cmath%2F0302035%2Cmath%2F0302144%2Cmath%2F0302344%2Cmath%2F0302028%2Cmath%2F0302034%2Cmath%2F0302323%2Cmath%2F0302010%2Cmath%2F0302245%2Cmath%2F0302130%2Cmath%2F0302307%2Cmath%2F0302167%2Cmath%2F0302281%2Cmath%2F0302229%2Cmath%2F0302320%2Cmath%2F0302222%2Cmath%2F0302006%2Cmath%2F0302219%2Cmath%2F0302139%2Cmath%2F0302225%2Cmath%2F0302125%2Cmath%2F0302304%2Cmath%2F0302131%2Cmath%2F0302211%2Cmath%2F0302177%2Cmath%2F0302334%2Cmath%2F0302264%2Cmath%2F0302187%2Cmath%2F0302189%2Cmath%2F0302076%2Cmath%2F0302208%2Cmath%2F0302149%2Cmath%2F0302163%2Cmath%2F0302296%2Cmath%2F0302077%2Cmath%2F0302030%2Cmath%2F0302285%2Cmath%2F0302273%2Cmath%2F0302293%2Cmath%2F0302071%2Cmath%2F0302057%2Cmath%2F0302016%2Cmath%2F0302196%2Cmath%2F0302153%2Cmath%2F0302250%2Cmath%2F0302287%2Cmath%2F0302166%2Cmath%2F0302103%2Cmath%2F0302104%2Cmath%2F0302180%2Cmath%2F0302036%2Cmath%2F0302335%2Cmath%2F0302119%2Cmath%2F0302094%2Cmath%2F0302117%2Cmath%2F0302011%2Cmath%2F0302343%2Cmath%2F0302284%2Cmath%2F0302342%2Cmath%2F0302060%2Cmath%2F0302327%2Cmath%2F0302338%2Cmath%2F0302257%2Cmath%2F0302266%2Cmath%2F0302258%2Cmath%2F0302027%2Cmath%2F0302226%2Cmath%2F0302290%2Cmath%2F0302195%2Cmath%2F0302325%2Cmath%2F0302294%2Cmath%2F0302145%2Cmath%2F0007120%2Cmath%2F0007126%2Cmath%2F0007199%2Cmath%2F0007063%2Cmath%2F0007204%2Cmath%2F0007139%2Cmath%2F0007210%2Cmath%2F0007176%2Cmath%2F0007016%2Cmath%2F0007137%2Cmath%2F0007006%2Cmath%2F0007039%2Cmath%2F0007021%2Cmath%2F0007205%2Cmath%2F0007209%2Cmath%2F0007195%2Cmath%2F0007095%2Cmath%2F0007130%2Cmath%2F0007018%2Cmath%2F0007053%2Cmath%2F0007044%2Cmath%2F0007003%2Cmath%2F0007092%2Cmath%2F0007033%2Cmath%2F0007211%2Cmath%2F0007014%2Cmath%2F0007149%2Cmath%2F0007110&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Random Weighting, Asymptotic Counting, and Inverse Isoperimetry"}, "summary": "For a family X of k-subsets of the set 1,...,n, let |X| be the cardinality of\nX and let Gamma(X,mu) be the expected maximum weight of a subset from X when\nthe weights of 1,...,n are chosen independently at random from a symmetric\nprobability distribution mu on R. We consider the inverse isoperimetric problem\nof finding mu for which Gamma(X,mu) gives the best estimate of ln|X|. We prove\nthat the optimal choice of mu is the logistic distribution, in which case\nGamma(X,mu) provides an asymptotically tight estimate of ln|X| as k^{-1}ln|X|\ngrows. Since in many important cases Gamma(X,mu) can be easily computed, we\nobtain computationally efficient approximation algorithms for a variety of\ncounting problems. Given mu, we describe families X of a given cardinality with\nthe minimum value of Gamma(X,mu), thus extending and sharpening various\nisoperimetric inequalities in the Boolean cube.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0302230%2Cmath%2F0302133%2Cmath%2F0302035%2Cmath%2F0302144%2Cmath%2F0302344%2Cmath%2F0302028%2Cmath%2F0302034%2Cmath%2F0302323%2Cmath%2F0302010%2Cmath%2F0302245%2Cmath%2F0302130%2Cmath%2F0302307%2Cmath%2F0302167%2Cmath%2F0302281%2Cmath%2F0302229%2Cmath%2F0302320%2Cmath%2F0302222%2Cmath%2F0302006%2Cmath%2F0302219%2Cmath%2F0302139%2Cmath%2F0302225%2Cmath%2F0302125%2Cmath%2F0302304%2Cmath%2F0302131%2Cmath%2F0302211%2Cmath%2F0302177%2Cmath%2F0302334%2Cmath%2F0302264%2Cmath%2F0302187%2Cmath%2F0302189%2Cmath%2F0302076%2Cmath%2F0302208%2Cmath%2F0302149%2Cmath%2F0302163%2Cmath%2F0302296%2Cmath%2F0302077%2Cmath%2F0302030%2Cmath%2F0302285%2Cmath%2F0302273%2Cmath%2F0302293%2Cmath%2F0302071%2Cmath%2F0302057%2Cmath%2F0302016%2Cmath%2F0302196%2Cmath%2F0302153%2Cmath%2F0302250%2Cmath%2F0302287%2Cmath%2F0302166%2Cmath%2F0302103%2Cmath%2F0302104%2Cmath%2F0302180%2Cmath%2F0302036%2Cmath%2F0302335%2Cmath%2F0302119%2Cmath%2F0302094%2Cmath%2F0302117%2Cmath%2F0302011%2Cmath%2F0302343%2Cmath%2F0302284%2Cmath%2F0302342%2Cmath%2F0302060%2Cmath%2F0302327%2Cmath%2F0302338%2Cmath%2F0302257%2Cmath%2F0302266%2Cmath%2F0302258%2Cmath%2F0302027%2Cmath%2F0302226%2Cmath%2F0302290%2Cmath%2F0302195%2Cmath%2F0302325%2Cmath%2F0302294%2Cmath%2F0302145%2Cmath%2F0007120%2Cmath%2F0007126%2Cmath%2F0007199%2Cmath%2F0007063%2Cmath%2F0007204%2Cmath%2F0007139%2Cmath%2F0007210%2Cmath%2F0007176%2Cmath%2F0007016%2Cmath%2F0007137%2Cmath%2F0007006%2Cmath%2F0007039%2Cmath%2F0007021%2Cmath%2F0007205%2Cmath%2F0007209%2Cmath%2F0007195%2Cmath%2F0007095%2Cmath%2F0007130%2Cmath%2F0007018%2Cmath%2F0007053%2Cmath%2F0007044%2Cmath%2F0007003%2Cmath%2F0007092%2Cmath%2F0007033%2Cmath%2F0007211%2Cmath%2F0007014%2Cmath%2F0007149%2Cmath%2F0007110&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "For a family X of k-subsets of the set 1,...,n, let |X| be the cardinality of\nX and let Gamma(X,mu) be the expected maximum weight of a subset from X when\nthe weights of 1,...,n are chosen independently at random from a symmetric\nprobability distribution mu on R. We consider the inverse isoperimetric problem\nof finding mu for which Gamma(X,mu) gives the best estimate of ln|X|. We prove\nthat the optimal choice of mu is the logistic distribution, in which case\nGamma(X,mu) provides an asymptotically tight estimate of ln|X| as k^{-1}ln|X|\ngrows. Since in many important cases Gamma(X,mu) can be easily computed, we\nobtain computationally efficient approximation algorithms for a variety of\ncounting problems. Given mu, we describe families X of a given cardinality with\nthe minimum value of Gamma(X,mu), thus extending and sharpening various\nisoperimetric inequalities in the Boolean cube."}, "authors": ["Alexander Barvinok", "Alex Samorodnitsky"], "author_detail": {"name": "Alex Samorodnitsky"}, "author": "Alex Samorodnitsky", "arxiv_comment": "The revision contains a new isoperimetric theorem, some other\n  improvements and extensions; 29 pages, 1 figure", "links": [{"href": "http://arxiv.org/abs/math/0302177v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0302177v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.MG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "05A16, 60C05, 60D05, 51F99, 68W20", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0302177v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0302177v2", "journal_reference": null, "doi": null, "fulltext": "arXiv:math/0302177v2 [math.CO] 4 Jun 2003\n\nRANDOM WEIGHTING, ASYMPTOTIC\nCOUNTING, AND INVERSE ISOPERIMETRY\n\nAlexander Barvinok and Alex Samorodnitsky\nJune 2003\nAbstract. For a family X of k-subsets of the set {1, . . . , n}, let |X| be the cardinality of X and let \u0393(X, \u03bc) be the expected maximum weight of a subset from X\nwhen the weights of 1, . . . , n are chosen independently at random from a symmetric\nprobability distribution \u03bc on R. We consider the inverse isoperimetric problem of\nfinding \u03bc for which \u0393(X, \u03bc) gives the best estimate of ln |X|. We prove that the\noptimal choice of \u03bc is the logistic distribution, in which case \u0393(X, \u03bc) provides an\nasymptotically tight estimate of ln |X| as k\u22121 ln |X| grows. Since in many important\ncases \u0393(X, \u03bc) can be easily computed, we obtain computationally efficient approximation algorithms for a variety of counting problems. Given \u03bc, we describe families\nX of a given cardinality with the minimum value of \u0393(X, \u03bc), thus extending and\nsharpening various isoperimetric inequalities in the Boolean cube.\n\n1. Introduction\nLet X be a family of k-subsets of the set {1, . . . , n}. Geometrically, we think of\nX as a set of points x = (\u03be1 , . . . , \u03ben ) in the Hamming sphere of radius k\n\u03be1 + . . . + \u03ben = k\n\nwhere\n\n\u03bei \u2208 {0, 1} for\n\ni = 1, . . . , n.\n\nWe also consider general families X of subsets of {1, . . . , n}, which we view as sets\nX \u2282 {0, 1}n of points in the Boolean cube.\nLet us fix a Borel probability measure \u03bc in R. We require \u03bc to be symmetric,\nthat is, \u03bc(A) = \u03bc(\u2212A) for any Borel set A \u2282 R, and to have finite variance.\nIn this paper, we relate two quantities associated with X. The first quantity is\nthe cardinality |X| of X. The second quantity \u0393(X, \u03bc) is defined as follows. Let\n1991 Mathematics Subject Classification. 05A16, 60C05, 60D05, 51F99, 68W20.\nKey words and phrases. combinatorial counting, Hamming metric, exponential measure, logistic distribution, polynomial time algorithms, isoperimetric inequalities, Boolean cube, inverse\nproblems, stochastic processes.\nThe research of the first author was partially supported by NSF Grant DMS 9734138. The\nresearch of the second author was partially supported by ISF Grant 039-7165.\nTypeset by AMS-TEX\n\n1\n\n\fus fix a measure \u03bc as above and let \u03b31 , . . . , \u03b3n be independent random variables\nhaving the distribution \u03bc. Then\nX\n\u03b3i .\n\u0393(X, \u03bc) = E max\nx\u2208X\n\ni\u2208x\n\nIn words: we sample weights of 1, . . . , n independently at random from the distribution \u03bc, define the weight of a subset x \u2208 X as the sum of the weights of its\nelements and let \u0393(X, \u03bc) be the expected maximum weight of a subset from X.\nOften, when the choice of \u03bc is clear from the context or not important, we write\nsimply \u0393(X).\nIt is easy to see that \u0393(X) is well defined, that \u0393(X) = 0 if X consists of a\nsingle point (recall that \u03bc is symmetric) and that \u0393(X) \u2265 \u0393(Y ) provided Y \u2282 X.\nThus, in a sense, \u0393(X) measures how large X is. In some respects, \u0393(X) behaves\nrather like ln |X|. For example, if X \u2282 {0, 1}n and Y \u2282 {0, 1}m , we can define\nthe direct product X \u00d7 Y \u2282 {0, 1}m+n. In this case, |X \u00d7 Y | = |X| * |Y | and\n\u0393(X \u00d7 Y ) = \u0393(X) + \u0393(Y ).\nOur goal can be stated (somewhat vaguely) as follows:\n(1.1) Problem. Find a measure \u03bc for which \u0393(X, \u03bc) gives the best estimate of\nln |X|.\nOur motivation comes from problems of efficient combinatorial counting. For\nmany interesting families X, given a set \u03b31 , . . . , \u03b3n of weights, we can easily find\nthe maximum weight of a subset x \u2208 X using well-known optimization algorithms.\nThe value of \u0393(X, \u03bc) can be efficiently computed through averaging of several sample maxima for randomly chosen weights \u03b31 , . . . , \u03b3n . At the same time, counting\nelements in X can be a hard and interesting problem. Thus, for such families,\n\u0393(X, \u03bc) provides a quick estimate for ln |X|. We give some examples in Section 2,\nwhere we also argue that the problems of optimization (computing \u0393(X, \u03bc)) and\ncounting (computing ln |X|) are asymptotically equivalent.\n(1.2) The logistic measure. Let X be a non-empty family of k-subsets of\n{1, . . . , n}. One of our main results is that there are measures \u03bc for which \u0393(X, \u03bc)\ngives an asymptotically tight estimate for ln |X| provided ln |X| grows faster than\na linear function of k. We obtain the best estimates when \u03bc = \u03bc0 is the logistic\nmeasure with density\n1\nfor \u03b3 \u2208 R.\ne\u03b3 + e\u2212\u03b3 + 2\nWe prove that for any \u03b1 > 1 there exists \u03b2 = \u03b2(\u03b1) > 0 such that\n\u03b2\u0393(X) \u2264 ln |X| \u2264 \u0393(X) provided\n\n|X| \u2265 \u03b1k\n\nand\n\u03b2(\u03b1) \u2212\u2192 1\n\nas\n2\n\n\u03b1 \u2212\u2192 +\u221e.\n\n\fMoreover, we prove that for t = k \u22121 \u0393(X) we have\nt \u2212 ln t \u2212 1 \u2264 k \u22121 ln |X| \u2264 t\nfor all sufficiently large t. Note that the bounds do not depend on n at all.\nThe existence of such measures \u03bc seems to contradict some basic geometric\nintuition. If we fix the cardinality |X| of a set X in the Hamming sphere, we would\nexpect \u0393(X) to be large if X is \"random\" and small if X is tightly packed. It\nturns out, however, that there are measures that manage to ignore, to some extent,\nthe difference between dense and sparse sets. In Sections 4 and 5 we prove some\ngeneral asymptotically tight bounds which allow one to obtain similar estimates\nfor a variety of measures \u03bc. For example, the measure with density |\u03b3|e\u2212|\u03b3| /2 also\nguarantees the asymptotic equivalence of ln |X| and \u0393(X).\nWe prove that the logistic distribution is, in a well-defined sense, optimal among\nall distributions \u03bc for which ln |X| \u2264 \u0393(X, \u03bc) for all non-empty X \u2282 {0, 1}n: given\na lower bound for \u0393(X, \u03bc), we get the best lower bound for ln |X| when \u03bc is the\nlogistic distribution.\nIn addition, we prove that the logistic distribution has an interesting extremal\nproperty: the inequality ln |X| \u2264 \u0393(X) turns into equality if X is a face (subcube)\nof the Boolean cube {0, 1}n.\nWe state our results in Section 3.\nThe problems we are dealing with have obvious connections to some central questions in probability and combinatorics, such as discrete isoperimetric inequalities\n(cf. [ABS98], [Le91], and [T95]) and estimates of the supremum of a stochastic\nprocess, see [T94]. In particular, in [T94], M. Talagrand considers the functional\n\u0393(X, \u03bc), where X is a family of subsets of the set {1, . . . , n} and \u03bc is the symmetric\nexponential distribution with density e\u2212|\u03b3| /2. He proves that ln |X| \u2264 c\u0393(X) for\nsome absolute constant c, see also [La97]. In Section 7, we prove that the optimal\nvalue of the constant is c = 2 ln 2 (the equality is obtained when X is a face of the\nBoolean cube {0, 1}n ). We also prove that ln |X| \u2264 \u0393(X) + k ln 2 provided X lies\nin the Hamming ball of radius k (the inequality is asymptotically sharp).\n(1.3) Isoperimetric inequalities. Suppose that \u03bc is the Bernoulli measure:\n\u03bc{1} = \u03bc{\u22121} =\n\n1\n.\n2\n\nThis case was studied in our paper [BS01]. It turns out that \u0393(X) has a simple\ngeometric interpretation: the value of 0.5n\u2212\u0393(X) is the average Hamming distance\nfrom a point x in the Boolean cube {0, 1}n to the subset X \u2282 {0, 1}n . The classical\nisoperimetric inequality in the Boolean cube, Harper's Theorem (see [Le91]), implies\nthat among all sets X of a given cardinality, the smallest value of \u0393(X) is attained\nwhen X is the sphere in the Hamming metric. More precisely, let us fix 0 < \u03b1 < ln 2.\nThen there exists \u03b2 = \u03b2(\u03b1), 0 < \u03b2 < 1/2, such that if Yn is the Hamming sphere\n3\n\n\fof radius \u03b2n + o(n) in {0, 1}n then we have ln |Yn | = \u03b1n + o(n) and for any set\nXn \u2282 {0, 1}n with ln |Xn | = \u03b1n + o(n), we have \u0393(Yn ) \u2264 \u0393(Xn ) + o(n). We\ndetermine \u03b2 from the equation\n\u03b2 ln\n\n1\n1\n+ (1 \u2212 \u03b2) ln\n=\u03b1\n\u03b2\n1\u2212\u03b2\n\nand note that \u0393(Yn ) = \u03b2n + o(n).\nIn Section 8, we construct sets Yn with asymptotically the smallest value of\n\u0393(Yn ) for an arbitrary symmetric probability measure \u03bc with finite variance. It is\nno longer true that Yn is a Hamming sphere in {0, 1}n . For example, if \u03bc{1} =\n\u03bc{\u22121} = \u03bc{0} = 1/3 then Yn has to be the direct product of two Hamming spheres.\nIt turns out that for any symmetric \u03bc with finite variance Yn can be chosen to be\nthe direct product of at most two Hamming spheres. More precisely, let us fix a\nsymmetric probability measure \u03bc and a number 0 < \u03b1 < ln 2. Then we construct\nnumbers \u03bbi = \u03bbi (\u03b1, \u03bc) \u2265 0 and 0 \u2264 \u03b2i = \u03b2i (\u03b1, \u03bc) \u2264 1/2 for i = 1, 2, such that\n\u03bb1 + \u03bb2 = 1 and the following holds: if Yn is the direct product of the Hamming\nsphere of radius \u03b21 n + o(n) in the Boolean cube of dimensions \u03bb1 n + o(n) and the\nHamming sphere of radius \u03b22 n + o(n) in the Boolean cube of dimension \u03bb2 n + o(n)\n(so that Yn is a subset of the Boolean cube of dimension n) then ln |Yn | = \u03b1n + o(n)\nand \u0393(Yn ) \u2264 \u0393(Xn ) + o(n) for any set Xn \u2282 {0, 1}n such that ln |Xn | = \u03b1n + o(n).\n(1.4) The inverse isoperimetric problem. It turns out that for the inequality\nln |X| \u2264 c\u0393(X, \u03bc) to hold with some constant c = c(\u03bc) for any non-empty family X\nof k-subsets of {1, . . . , n}, the measure \u03bc has to have an at least exponential tail,\nthat is, for the cumulative distribution function F of \u03bc we must have 1\u2212F (t) \u2265 e\u2212at\nfor some a > 0 and all sufficiently large t. On the other hand, for the lower bound\nof k \u22121 ln |X| in terms of k \u22121 \u0393(X, \u03bc) to be non-trivial (other than 0), \u03bc has to have\nan at most exponential tail. Thus for the solution of Problem 1.1, which we call\nthe inverse isoperimetric problem, we are interested in measures with exponential\ntails.\n2. Applications to Combinatorial Counting\nThis research is a continuation of [B97] and [BS01], where the idea to use optimization algorithms for counting problems was developed.\nFirst, we discuss how to compute \u0393(X) for many interesting families of subsets.\nLet us assume that the family X of subsets of {1, . . . , n} is given by its Optimization Oracle.\n(2.1) Optimization Oracle\nInput: Real vector c = (\u03b31 , . . . , \u03b3n )\nOutput: Real number\nw(X, c) = max\nx\u2208X\n\n4\n\nX\ni\u2208x\n\n\u03b3i .\n\n\fThus, we input real weights of the elements 1, . . . , n and output the maximum\nweight w(X, c) of a subset x \u2208 X in this weighting. As is discussed in [B97]\nand [BS01], for many interesting families X Optimization Oracle 2.1 can be easily\nconstructed. We provide two examples below.\n(2.2) Bases in matroids. Let A be a k \u00d7 n matrix of rank k over a field F. We\nassume that k < n. Let X = X(A) be the set of all k-subsets x of {1, . . . , n} such\nthat the columns of A indexed by the elements of x are linearly independent. Thus\nX is the set of all non-zero k \u00d7 k minors of A, or, in other words, the set of bases\nof the matroid represented by A. It is an interesting and apparently hard problem\nto compute or to approximate the cardinality of X, cf. [JS97].\nOn the other hand, it is very easy to construct the Optimization Oracle for\nX. Indeed, given real weights \u03b31 , . . . , \u03b3n , we construct a linearly independent set\nai1 , . . . , aik of columns of the largest total weight one-by-one. First, we choose\nai1 to be a non-zero column of A with the largest possible weight \u03b3i1 . Then we\nchoose ai2 to be a column of the maximum possible weight such that ai1 and ai2 are\nlinearly independent. We proceed as above, and finally select aik to be a column of\nthe maximum possible weight such that ai1 , . . . , aik are linearly independent; cf.,\nfor example, Chapter 12 of [PS98] for \"greedy algorithms\". Particular cases of this\nproblem include counting forests and spanning subgraphs in a given graph.\nLet A and B be k \u00d7 n matrices of rank k < n and let X be the set of all\nk-subsets x of {1, . . . , n} such that the columns of A indexed by the elements of\nx are linearly independent and the columns of B indexed by the elements of x\nare linearly independent. Then there exists a much more complicated than above,\nbut still polynomial time algorithm, which, given weights \u03b31 , . . . , \u03b3n , computes the\nlargest weight of a subset x from X, see Chapter 12 of [PS98].\n(2.3) Perfect matchings in graphs. Let G be a graph with 2k vertices and n\nedges. A collection of k pairwise disjoint edges in G is called a perfect matching\n(known to physicists as a dimer cover). It is a hard and interesting problem to\ncount perfect matchings in a given graph, see [JS97]. Recently, using the Markov\nchain approach, M. Jerrum, A. Sinclair and E. Vigoda constructed a polynomial\ntime approximation algorithm to count perfect matchings in a given bipartite graph\n[JSV01], but for general graphs no such algorithms are known.\nThere is a classical O(n3 ) algorithm for finding a perfect matching of the maximum weight in any given edge-weighted graph, see Section 11.3 of [PS98], so Oracle\n2.1 is readily available.\nFor any set X given by its Optimization Oracle 2.1, the value of \u0393(X) can be\nwell approximated by the sample mean of a moderate size.\n(2.4) Algorithm for computing \u0393(X, \u03bc)\nInput: A family X of subsets of {1, . . . , n} given by its Optimization Oracle 2.1;\n5\n\n\fOutput: A number w approximating \u0393(X, \u03bc);\nAlgorithm: Choose a positive integer m (see Section 2.5 for details). Sample\nindependently m random vectors ci from the product measure \u03bc\u2297n in Rn . For each\nvector ci , using Optimization Oracle 2.1, compute the maximum weight w(X, ci )\nof a subset from X. Output\nm\n\n1 X\nw(X, ci ).\nw=\nm i=1\n(2.5) Choosing the number of samples m. Let us consider the output\nw = w(X; c1 , . . . , cm )\nof Algorithm 2.4 as a random variable on the space\nRnm = Rn \u2295 . . . \u2295 Rn\n|\n{z\n}\nm times\n\nendowed with the product measure \u03bc\u2297mn . Clearly, the expectation of w is \u0393(X, \u03bc).\nLet D = E(\u03b3 2 ) be the variance of \u03bc. Using the estimates\n\u0010X\ni\u2208x\n\n\u03b3i\n\n\u00112\n\n\u2264\n\nn\n\u0010X\ni=1\n\n|\u03b3i |\n\n\u00112\n\n\u2264n\n\nn\nX\n\n\u03b3i2\n\nfor\n\nx \u2282 {1, . . . , n},\n\ni=1\n\nwe conclude that the variance of w does not exceed n2 D/m. Therefore, by Chebyshev's inequality, for the output w to satisfy |w \u2212 \u0393(X, \u03bc)| \u2264 \u01eb with probability at\nleast 2/3, we can choose m = \u23083\u01eb\u22122 n2 D\u2309.\nAs usual, to achieve a higher probability 1\u2212\u03b4 of success, we can run the algorithm\nO(ln \u03b4 \u22121 ) times and then find the median of the computed estimates.\nFor many measures \u03bc the bound for m can be essentially improved. In particular,\nwe are interested in the case of the logistic measure \u03bc with density (2 + e\u03b3 + e\u2212\u03b3 )\u22121 .\nTo obtain the desired estimate we use a concentration property of the symmetric\nexponential measure \u03bd with density e\u2212|\u03b3| /2, see Section 4.5 of [Led01].\nLet us define\n\u0001\n\u001a\n\u03b3 \u2212 ln 2 \u2212 e\u03b3\nif \u03b3 \u2264 0\n\u0001\n\u03c8(\u03b3) =\n\u2212\u03b3\n\u03b3 + ln 2 \u2212 e\nif \u03b3 > 0\n\nand\n\n\u03a8(c) = \u03c8(\u03b31 ), . . . , \u03c8(\u03b3n)\n\n\u0001\n\nfor\n\nc = (\u03b31 , . . . , \u03b3n ).\n\nThen \u03c8(\u03b3) has the logistic distribution \u03bc if \u03b3 has the exponential distribution \u03bd.\nThus we can write\n\u0001\n\u0393(X, \u03bc) = E w X; \u03a8(c1 ), . . . , \u03a8(cm ) ,\n6\n\n\fwhere vectors (c1 , . . . , cm ) are sampled from the exponential distribution \u03bd \u2297mn in\nRnm . If X is a family of k-subsets then the Lipschitz coefficient of\n\u0001\nf (c1 , . . . , cm ) = w X; \u03a8(c1 ), . . . , \u03a8(cm )\n\np\nwith respect to the l2 metric of Rnm does not exceed 2 k/m while the Lipschitz\ncoefficient with respect to the l1 metric does not exceed 2/m. Applying Proposition\n4.18 of [Led01], we conclude that for the output w of Algorithm 2.4 to satisfy\n|w \u2212 \u0393(X, \u03bc)| \u2264 \u01eb with probability at least 2/3, we can choose m = O(k\u01eb\u22122 ). Note\nthat the choice of m is independent of the size n of the ground set.\nWe observe that it is easy to sample a random weight \u03b3 from the logistic distribution provided sampling from the uniform distribution on the interval [0, 1] is\navailable (which is the case for many computer packages). Indeed, if \u03be is uniformly\ndistributed on the interval [0, 1], then \u03b3 = ln \u03be\u2212ln(1\u2212\u03be) has the logistic distribution.\nOur numerical experiments suggest that the choice of m = O(1) (for example,\nm = 5) is good enough and that in many cases m = 1 suffices.\n(2.6) Counting with multiplicities. Suppose that every element i of the ground\nset {1, . . . , n} has a positive integer multiplicity qi . Let X be a family of k-subsets\nof {1, . . . , n} and let\nXY\nqi .\npX (q1 , . . . , qn ) =\nx\u2208X i\u2208x\n\nIt may be of interest to compute or approximate pX .\nFor instance, let A = (aij ) be a 2k\u00d72k symmetric matrix of non-negative integers\naij . Let us construct an (undirected) graph G on 2k vertices {1, . . . , 2k} where the\nvertices i and j are connected by an edge if and only if aij > 0. We identify the\nedges of G with the set {1, . . . , n}. Let X be the set of all perfect matchings in G\nidentified with a family of k-subsets of {1, . . . , n}, see Example 2.3. If we assign\nmultiplicities aij to the edges of G, then the value of pX (aij ) is called the hafnian\nhaf A of A, a polynomial of a considerable interest which generalizes permanent.\nComputing pX (q1 , . . . , qn ) is reduced to counting in the following straightforward\nway. Let N = q1 + . . . + qn and let us view the set {1, . . . , N } as the multiset\nconsisting of q1 copies of 1, q2 copies of 2, . . . , qn copies of n. Let us construct a\nfamily\nY of k-subsets of {1, . . . , N } as follows: for each k-subset x \u2208 X we construct\nQ\ni\u2208x qi k-subsets y \u2208 Y by replacing every i \u2208 x by any of its qi copies. It is clear\nthat |Y | = pX (q1 , . . . , qn ).\nTo construct Optimization Oracle 2.1 for Y , we apply the oracle for X with the\ninput c = (\u03b31 , . . . , \u03b3n ), where \u03b3i is the maximum of qi weights assigned to the qi\ncopies of i. Moreover, Algorithm 2.4 is easily modified for computing \u0393(Y, \u03bc) instead\nof \u0393(X, \u03bc). We still work with the underlying family X, but instead of sampling\nweights from the distribution \u03bc, we sample the i-th weight \u03b3i from the distribution\n\u03bcqi of the maximum of qi independent random variables with the distribution \u03bc\n(note that \u03bcqi is not symmetric for qi > 1). Thus, if \u03bc is the logistic distribution,\nto sample \u03b3i , we sample \u03be from the uniform distribution on [0, 1] and let \u03b3i =\n7\n\n\f\u0001\n\u2212 ln \u03be \u22121/qi \u2212 1 . Luckily, for the logistic distribution the required number m of\ncalls to Oracle 2.1 does not depend on the size of the ground set, hence we use the\nsame number m of calls whether we consider counting with or without multiplicities.\nIn [BS01] we discuss how our approach fits within the general framework of\nthe Monte Carlo method. The estimates we get are not nearly as precise as those\nobtained by the Markov chain based Monte Carlo Method (see, for example, [JS97]),\nbut supply a non-trivial information and are easily computed for a wide variety of\nproblems. Even for the much-studied problem of counting perfect matchings in\ngeneral (non-bipartite) graphs our approach produces new theoretical results. For\nsome of the problems, such as counting bases in the intersection of two general\nmatroids (see Example 2.2), our estimates seem to be the only ones that can be\nefficiently computed at the moment. If X is a family of k-subsets of {1, . . . , n}\nand |X| = ek\u03bb for some \u03bb = \u03bb(X) then, in polynomial time, we estimate \u03bb(X)\nwithin a constant multiplicative factor as long as \u03bb(X) is separated from 0 and\nall sufficiently large \u03bb(X) are estimated with an additive error of 1 + ln \u03bb(X), see\nSection 3. Similar estimates hold for counting with multiplicities of Section 2.6. On\nthe other hand, the Markov chain approach, if successful, allows one to estimate the\ncardinality |X| within any prescribed relative error. We note that for truly large\nproblems the correct scale is logarithmic because |X| can be prohibitively large to\ndeal with. The Markov chain approach relies on the local structure of X (needed\nfor \"rapid mixing\"), whereas our method uses some global structure (the ability to\noptimize on X efficiently).\n(2.7) Asymptotic equivalence ofP\ncounting and optimization. One can view\nthe optimization functional maxx\u2208X i\u2208x \u03b3i as the \"tropical version\" of the polynomial pX (q1 , . . . , qn ) of Section 2.6: we get the former if we replace \"+\" with \"max\"\nand product with sum in the latter. Thus our results establish a weak asymptotic\nequivalence of the counting and optimization problems: if we can optimize, we\ncan estimate ln pX with a relative error which approaches 0 as k \u22121 ln pX grows.\nVice versa, if we can approximate ln pX , we can optimize (at least approximately):\nchoosing qi (t) = 2t\u03b3i , we get\nX\n\u0001\n\u03b3i .\nlim t\u22121 log2 pX q1 (t), . . . , qn (t) = max\n\nt\u2212\u2192+\u221e\n\nx\u2208X\n\ni\u2208x\n\nA. Yong [Y03] implemented our algorithms for some counting problems, such as\nestimating the number of forests in a given graph, computing the permanent and\nthe hafnian of a given non-negative integer matrix and performed a number of\nnumerical experiments. The algorithm produces the upper and lower bounds for\nthe logarithm of the cardinality of the family in question, see Section 3. The upper\nbound is attained when the family is \"tightly packed\" as a subset of the Boolean\ncube whereas the lower bound is attained on sparse families. It appears that there\nis some metric structure inherent to various families of combinatorially defined\n8\n\n\fsets. For example, when we applied our methods to estimate the logarithm of the\nnumber of spanning trees in a given connected graph, the exact value (which can\nbe easily computed by the matrix-tree formula) turns out to be very close to the\nupper bound obtained by our algorithm. Informally, spanning trees appear to be\n\"tightly packed\". On the other hand, when we estimated the logarithm of the\nnumber of perfect matchings in a graph, the true value (when we were able to find\nit by other methods) seems to lie close to the middle point between the upper and\nlower bounds.\n3. The Logistic Measure: Results\nLet us choose \u03bc0 with density\ne\u03b3\n\n1\n+ e\u2212\u03b3 + 2\n\nfor\n\n\u03b3 \u2208 R.\n\nThe cumulative distribution function F of \u03bc is given by\nF (\u03b3) =\n\n1\n1 + e\u2212\u03b3\n\nfor\n\n\u03b3 \u2208 R.\n\nThe variance of \u03bc0 is \u03c0 2 /3 [M85]. Our first main result is as follows.\n(3.1) Theorem.\n(1) For every non-empty set X \u2282 {0, 1}n , we have\nln |X| \u2264 \u0393(X);\n(2) Let\n\u0010\nsin \u03c0\u03b4 \u0011\nh(t) = sup \u03b4t + ln\n\u03c0\u03b4\n0\u2264\u03b4<1\n\nfor\n\nt \u2265 0.\n\nThen h(t) is a convex increasing function and for any non-empty family X\nof k-subsets of {1, . . . , n}, we have\nh(t) \u2264 k \u22121 ln |X|\n\nwhere\n\nt = k \u22121 \u0393(X).\n\nFrom the expansion\nln\n\nsin \u03c0x\n\u03c02\n= \u2212 x2 + O(x4 ) for\n\u03c0x\n6\n\nwe deduce that\nh(t) =\n\n3 2\nt + O(t4 ) for\n2\u03c0 2\n9\n\nx \u2248 0,\n\nt\u22480\n\n\f(we substitute \u03b4 = (3t/\u03c0 2 )). From the expansion\nln\n\nsin \u03c0(1 \u2212 x)\n= ln x + x + O(x2 ) for\n\u03c0(1 \u2212 x)\n\nx \u2248 0,\n\nwe deduce that\nh(t) \u2265 t \u2212 ln t \u2212 1\n\nas t \u2212\u2192 +\u221e\n\n(we substitute \u03b4 = 1 \u2212 t\u22121 ).\nA Maple plot of h(t) is shown on Figure 1 below.\n\nh(t)\n\n6\n4\n2\n0\n\n2\n\n4\n\n6\n\n8\n\nt\n\n10\n\nFigure 1\n\nWe obtain the following corollary.\n(3.2) Corollary. For any \u03b1 > 1 there exists \u03b2 = \u03b2(\u03b1) > 0 such that for any\nnon-empty family X of k-subsets of {1, . . . , n} with |X| \u2265 \u03b1k we have\n\u03b2\u0393(X) \u2264 ln |X| \u2264 \u0393(X).\nMoreover,\n\u03b2(\u03b1) \u2212\u2192 1\n\nas\n\n\u03b1 \u2212\u2192 +\u221e.\n\nProof. From Part (1) of Theorem 3.1, we have k \u22121 \u0393(X) \u2265 ln \u03b1. Since h(t) is\nconvex, we have h(t) \u2265 \u03b2t for some \u03b2 = \u03b2(\u03b1) > 0 and all t \u2265 ln \u03b1. The asymptotics\nof \u03b2(\u03b1) as \u03b1 \u2212\u2192 +\u221e follows from the asymptotics of h(t) as t \u2212\u2192 +\u221e.\n\u0003\nThus, using the logistic distribution allows us to estimate ln |X| within a constant\nfactor and the approximation factor approaches 1 as k \u22121 ln |X| grows.\n10\n\n\fWe note that the bound ln |X| \u2264 \u0393(X) is sharp. For example, if X is an mdimensional face of the Boolean cube then ln |X| = m ln 2 and one can show that\n\u0393(X) = m ln 2 as well. Indeed, because \u0393(X) is invariant under coordinate permutations, we may assume that X consists of the points (\u03be1 , . . . , \u03bem , 0, . . . , 0), where\n\u03bei \u2208 {0, 1} for i = 1, . . . , m. The set X can be written as the Minkowski sum\nX = X1 + . . . + Xm , where Xi consists of the origin and the i-th basis vector ei .\nHence \u0393(X) = m\u0393(X1 ) (cf. Section 4.1) and \u0393(X1 ) is computed directly as\nZ +\u221e\nx\n\u0393(X1 ) =\ndx = ln 2\nx\ne + e\u2212x + 2\n0\n(we substitute ex = y and then integrate by parts).\nIt turns out that the logistic measure is optimal in a well-defined sense.\n(3.3) Theorem. Let M be the set of all measures \u03bc such that\nln |X| \u2264 \u0393(X, \u03bc)\nfor any non-empty family X of k-subsets of {1, . . . , n}, any n \u2265 1, and any 1 \u2264\nk \u2264 n.\nFor a measure \u03bc \u2208 M and a number t > 0, let c(t, \u03bc) be the infimum of k \u22121 ln |X|\ntaken over all n \u2265 1, all 1 \u2264 k \u2264 n, and all non-empty families X of k-subsets\n{1, . . . , n} such that k \u22121 \u0393(X, \u03bc) \u2265 t. Then for all t > 0\nc(t, \u03bc) \u2264 c(t, \u03bc0 ),\nwhere \u03bc0 is the logistic distribution.\n(3.4) Discussion. Unless \u03bc is concentrated in 0, for X = {0, 1} we have \u0393(X, \u03bc) =\nc ln 2 for some c > 0 and hence \u0393(X, \u03bc) = c ln |X| if X is a face of the Boolean cube\n{0, 1}n , cf. Section 4.1. As we are looking for the best measure \u03bc in Problem 1.1, it\nis only natural to assume that \u0393(X, \u03bc) \u2265 c1 ln |X| for all X \u2282 {0, 1}n , which, after\nscaling, becomes \u0393(X, \u03bc) \u2265 ln |X|. This explains the definition of M.\nLet us choose \u03bc \u2208 M. Then any upper bound for \u0393(X, \u03bc) is automatically\nan upper bound for ln |X|. The function c(t, \u03bc) measures the quality of the lower\nbound estimate for ln |X| given a lower bound for \u0393(X, \u03bc).\nIncidentally, it follows from our proof that the logistic measure is the measure\nof the smallest variance in M.\nWe prove Theorems 3.1 and 3.3 in Section 6.\n4. General Estimates: the Upper Bound\nIt is convenient to think about families X geometrically, as subsets of the Boolean\ncube {0, 1}n \u2282 Rn . Let us fix a symmetric probability measure \u03bc on R with finite\nvariance and let \u03bc\u2297n be the product measure on Rn . For a finite set X \u2282 Rn we\nwrite\n\u0393(X) = E maxhc, xi,\nx\u2208X\n\nwhere c = (\u03b31 , . . . , \u03b3n ) is a random vector sampled from the distribution \u03bc\u2297n on\nRn and h*, *i is the standard scalar product in Rn .\n11\n\n\f(4.1) Preliminaries. It is easy to check that\n\u0393(X) \u2265 \u0393(Y )\n\nprovided\n\nY \u2282X\n\nand that\n\u0393(X) = 0\n\nif |X| = 1,\n\nthat is, if X is a point (\u03bc is symmetric). It follows that \u0393(X) \u2265 0 for any finite\nnon-empty subset X \u2282 Rn . Moreover\n\b\n\u0393(X + Y ) = \u0393(X) + \u0393(Y ) where X + Y = x + y : x \u2208 X, y \u2208 Y\n\nis the Minkowski sum of X and Y . In particular, \u0393(X + y) = \u0393(X) for any set X\nand any point y. We note that\n\b\n\u0393(\u03bbX) = |\u03bb|\u0393(X) where \u03bbX = \u03bbx : x \u2208 X\n\nis a dilation of X and that \u0393(X) is invariant under the action of the hyperoctahedral\ngroup, which permutes and changes signs of the coordinates.\nLet S(k, n) be the Hamming sphere of radius k centered at the origin, that is, the\nset of points x = (\u03be1 , . . . , \u03ben ) \u2208 {0, 1}n such that \u03be1 +. . .+\u03ben = k. Combinatorially,\nS(k, n) is the family of all k-subsets of {1, . . . , n}.\nLet F be the cumulative distribution function of \u03bc. In this section, we prove the\nfollowing main result.\n(4.2) Theorem. For a non-empty set X \u2282 {0, 1}n and a number \u03c4 > 0, let\nG(X, \u03c4 ) = ln |X| \u2212 \u03c4 \u0393(X).\nLet\n\u2212\u03c4 a\n\ng\u03c4 (a) = ln(1 + e\n\n)\u2212\u03c4\n\nZ\n\na\n\n+\u221e\n\n\u0001\n1 \u2212 F (t) dt\n\nfor\n\na \u2208 R.\n\n(1) For any non-empty set X \u2282 {0, 1}n , we have\nG(X, \u03c4 ) \u2264 n sup g\u03c4 (a);\na\u22650\n\n(2) Suppose that\nsup g\u03c4 (a) = g\u03c4 (a0 ) > 0\na\u22650\n\nfor some, necessarily finite,\n\na0 \u2265 0.\n\nThen there exists a sequence Xn = S(kn , n) \u2282 {0, 1}n of Hamming spheres\nsuch that\nG(Xn , \u03c4 )\nlim\n= g\u03c4 (a0 ).\nn\u2212\u2192+\u221e\nn\nAssuming that F is continuous and strictly increasing, we can choose kn =\n\u03b1n + o(n) for \u03b1 = 1 \u2212 F (a0 ).\nBefore we embark on the proof of Theorem 4.2, we summarize some useful properties of g\u03c4 (a).\n12\n\n\f(4.3) Properties of g\u03c4 . We observe that\ng\u03c4 (0) = ln 2 \u2212 \u03c4\n\nZ\n\n+\u221e\n\n\u0001\n\n1 \u2212 F (t) dt = ln 2 \u2212 \u03c4\n0\n\nZ\n\n+\u221e\n\nt dF (t).\n0\n\nFurthermore,\nlim\n\na\u2212\u2192+\u221e\n\ng\u03c4 (a) = 0,\n\nsince \u03bc has expectation. If F (t) is continuous then g\u03c4 is differentiable and\ng\u03c4\u2032 (a)\n\n\u0010 e\u03c4 a\n\u0011\n=\u03c4\n\u2212 F (a) .\n1 + e\u03c4 a\n\nIn particular, a is a critical point of g\u03c4 (a) if and only if a is a solution of the equation\ne\u03c4 a\n= F (a)\n1 + e\u03c4 a\nor, in other words, if\n\n\u0001\na\u03c4 = ln F (a) \u2212 ln 1 \u2212 F (a) .\n\nIn particular, a = 0 is always a critical point of g\u03c4 .\n\nWe prove Part (1) of Theorem 4.2 by induction on n, inspired by Talagrand's\nmethod [T95]. The induction is based on the following simple observation.\n(4.4) Lemma. Suppose that the cumulative distribution function F is continuous.\nFor a non-empty set X \u2282 {0, 1}n , n > 1, let\nn\no\nX1 = x \u2208 {0, 1}n\u22121 : (x, 1) \u2208 X\nand\nn\no\nX0 = x \u2208 {0, 1}n\u22121 : (x, 0) \u2208 X .\nThen, for any a \u2208 R we have\n\u0001\n\u0393(X) \u2265 1 \u2212 F (a) \u0393(X1 ) + F (a)\u0393(X0 ) +\n\nZ\n\n+\u221e\n\nt dF (t).\n\na\n\nProof. Let c = (c, \u03b3), where c \u2208 Rn\u22121 , \u03b3 \u2208 R, and let\nw(X, c) = maxhx, ci for\nx\u2208X\n\nc \u2208 Rn .\n\nClearly,\nw(X, c) \u2265 w(X1 , c) + \u03b3\n\nand\n13\n\nw(X, c) \u2265 w(X0 , c).\n\n\fTherefore,\n\u0393(X) =\n\nZ\n\nw(X, c) d\u03bc\u2297n (c)\n\nRn\n\n=\n\nZ\n\nw(X, c) d\u03bc\n\nZ\n\n\u0010\n\nRn :\u03b3>a\n\n\u2265\n\nRn :\u03b3>a\n\n\u2297n\n\n(c) +\n\nZ\n\nw(X, c) d\u03bc\u2297n (c)\n\nRn :\u03b3\u2264a\n\nZ\n\u0011\n\u2297n\nw(X1 , c) + \u03b3 d\u03bc (c) +\n\n= 1 \u2212 F (a)\n\nw(X0 , c) d\u03bc\u2297n (c)\n\nRn :\u03b3\u2264a\n\n\u0001\n\nZ\n\nw(X1 , c) d\u03bc\n\n\u2297n\u22121\n\n(c) +\n\nRn\u22121\n\nZ\n\n+\u221e\n\n\u03b3 dF (\u03b3)\n\na\n\nZ\n\nw(X0 , c) d\u03bc\u2297n\u22121 (c)\nRn\u22121\nZ +\u221e\n\u0001\n= 1 \u2212 F (a) \u0393(X1 ) + F (a)\u0393(X0 ) +\n\u03b3 dF (\u03b3),\n+ F (a)\n\na\n\nand the proof follows.\n\n\u0003\n\n(4.5) Lemma. Suppose that the cumulative distribution function F is continuous.\nFor a non-empty set X \u2282 {0, 1}n and a number \u03c4 > 0 let G(X, \u03c4 ) and g\u03c4 (a) be\ndefined as in Theorem 4.2. Then for any non-empty set X \u2282 {0, 1}n , n > 1, there\nexists a non-empty set Y \u2282 {0, 1}n\u22121 such that\nG(X, \u03c4 ) \u2264 G(Y, \u03c4 ) + sup g\u03c4 (a).\na\u22650\n\nProof. Let us construct X1 and X0 as in Lemma 4.4. We have\n|X1 | = \u03bb|X| and\n\n|X0 | = (1 \u2212 \u03bb)|X| for some\n\n0 \u2264 \u03bb \u2264 1.\n\nWithout loss of generality, we assume that 0 \u2264 \u03bb \u2264 1/2. Otherwise, we replace X\nby X \u2032 , where\nn\no\nX \u2032 = (\u03be1 , . . . , 1 \u2212 \u03ben ) : (\u03be1 , . . . , \u03ben ) \u2208 X .\n\nClearly, |X| = |X \u2032 | and by Section 4.1, \u0393(X) = \u0393(X \u2032 ).\nIf \u03bb = 0 we choose Y = X0 . Identifying Rn\u22121 with the hyperplane \u03ben = 0 in Rn ,\nwe observe that X = Y and so G(X, \u03c4 ) = G(Y, \u03c4 ). Since by Section 4.3 we have\nsup g\u03c4 (a) \u2265 0,\na\u22650\n\nthe result follows.\nThus we assume that 0 < \u03bb \u2264 1/2. Let Y \u2208 {X0 , X1 } be the set with the larger\nvalue of G(*, \u03c4 ), where the ties are broken arbitrarily. We have\n|X| =\n\n1\n1\n|X1 | and |X| =\n|X0 |.\n\u03bb\n1\u2212\u03bb\n14\n\n\fFor any a \u2265 0\n\u0001\nG(X, \u03c4 ) = ln |X| \u2212 \u03c4 \u0393(X) = 1 \u2212 F (a) ln |X| + F (a) ln |X| \u2212 \u03c4 \u0393(X)\n\u0001\n= 1 \u2212 F (a) ln |X1 | + F (a) ln |X0 |\n\u0001 1\n1\n\u2212 \u03c4 \u0393(X).\n+ (1 \u2212 F (a) ln + F (a) ln\n\u03bb\n1\u2212\u03bb\nBy Lemma 4.4 we conclude that\n\u0001\nG(X, \u03c4 ) \u2264 1 \u2212 F (a) ln |X1 | + F (a) ln |X0 |\n\u0001 1\n1\n+ (1 \u2212 F (a) ln + F (a) ln\n\u03bb\n1\u2212\u03bb\nZ +\u221e\n\u0001\n\u2212 1 \u2212 F (a) \u03c4 \u0393(X1 ) \u2212 F (a)\u03c4 \u0393(X0 ) \u2212 \u03c4\nt dF (t)\na\n\u0001\n= 1 \u2212 F (a) G(X1 , \u03c4 ) + F (a)G(X0 , \u03c4 )\nZ +\u221e\n\u0001 1\n1\nt dF (t)\n+ 1 \u2212 F (a) ln + F (a) ln\n\u2212\u03c4\n\u03bb\n1\u2212\u03bb\na\nZ +\u221e\n\u0001 1\n1\n\u2212\u03c4\nt dF (t).\n\u2264G(Y, \u03c4 ) + 1 \u2212 F (a) ln + F (a) ln\n\u03bb\n1\u2212\u03bb\na\nOptimizing in a, we choose\n(4.5.1)\nThen\n\n1 \u00101 \u2212 \u03bb\u0011\n,\na = ln\n\u03c4\n\u03bb\n1\n= 1 + e\u03c4 a\n\u03bb\n\nand\n\nso a \u2265 0.\n\n1\n1 + e\u03c4 a\n=\n.\n1\u2212\u03bb\ne\u03c4 a\n\nHence\n\u03c4a\n\nG(X, \u03c4 ) \u2264G(Y, \u03c4 ) + ln(1 + e ) \u2212 \u03c4 aF (a) \u2212 \u03c4\n\nZ\n\n+\u221e\n\nt dF (t)\n\na\n\n\u0001\n\n\u2212\u03c4 a\n\n=G(Y, \u03c4 ) + ln(1 + e\n\nZ\n\n+\u221e\n\n) + \u03c4 a 1 \u2212 F (a) + \u03c4\na\nZ +\u221e\n\u0001\n=G(Y, \u03c4 ) + ln(1 + e\u2212\u03c4 a ) \u2212 \u03c4\n1 \u2212 F (t) dt\n\nt d 1 \u2212 F (t)\n\n\u0001\n\na\n\n=G(Y, \u03c4 ) + g\u03c4 (a),\n\nas claimed.\n\n\u0003\n\nNow we are ready to prove Part (1) of Theorem 4.2.\nProof of Part (1) of Theorem 4.2. Without loss of generality, we may assume that\nthe cumulative distribution function F is continuous. The proof follows by induction\n15\n\n\fon n. For n = 1, there are two possibilities. If |X| = 1 then G(X, \u03c4 ) = 0 (see Section\n4.1) and the result holds since\nsup g\u03c4 (a) \u2265 0,\na\u22650\n\nsee Section 4.3. If |X| = 2 then X = {0, 1} and\nZ +\u221e\nG(X, \u03c4 ) = ln 2 \u2212 \u03c4\nt dF (t) = g\u03c4 (0),\n0\n\nso the inequality holds as well.\nThe induction step follows by Lemma 4.5.\n\n\u0003\n\nLet S(k, n) be the Hamming sphere of radius k, that is, the set of all k-subsets of\n{1, . . . , n}. Given weights \u03b31 , . . . , \u03b3n , the maximum weight of a subset x \u2208 S(k, n)\nis the sum of the first k largest weights among \u03b31 , . . . , \u03b3n .\nThe proof of Part (2) of Theorem 4.2 is based on the following lemma.\n(4.6) Lemma. Suppose that the cumulative distribution function F of \u03bc is strictly\nincreasing and continuous. Let us choose 0 < \u03b1 < 1 and let Xn be the Hamming\nsphere of radius \u03b1n + o(n) in {0, 1}n .\nThen\nZ +\u221e\n\u0393(Xn )\nt dF (t).\n=\nlim\nn\u2212\u2192+\u221e\nn\nF \u22121 (1\u2212\u03b1)\nProof. Let \u03b31 , . . . , \u03b3n be independent random variables with the distribution \u03bc and\nlet u1:n \u2264 u2:n \u2264 . . . \u2264 un:n be the corresponding order statistics, that is, the\npermutation of \u03b31 , . . . , \u03b3n in the increasing order. Then\nmax\n\nx\u2208Xn\n\nX\ni\u2208x\n\n\u03b3i =\n\nn\nX\n\num:n .\n\nm=n\u2212\u03b1n+o(n)\n\nConsequently, \u0393(Xn ) is the expectation of the last sum.\nThe corresponding asymptotics for the order statistics is well known, see, for\nexample, [S73].\n\u0003\nNow we are ready to complete the proof of Theorem 4.2.\nProof of Part (2) of Theorem 4.2. Without loss of generality, we assume that the\ncumulative distribution function F of \u03bc is continuous and strictly increasing. Let\nus choose \u03b1 and kn as described, so Xn \u2282 {0, 1}n is the Hamming sphere of radius\n\u03b1n + o(n) in {0, 1}n.\nAs is known (see, for example, Theorem 1.4.5 of [Li99]),\nln |Xn |\n1\n1\n=\u03b1 ln + (1 \u2212 \u03b1) ln\nn\u2212\u2192+\u221e\nn\n\u03b1\n1\u2212\u03b1\n\u0001\n1\n1\n= 1 \u2212 F (a0 ) ln\n+ F (a0 ) ln\n.\n1 \u2212 F (a0 )\nF (a0 )\n16\nlim\n\n\fMoreover, by Lemma 4.6,\n\u0393(Xn )\n=\nlim\nn\u2212\u2192+\u221e\nn\n\nZ\n\n+\u221e\n\nt dF (t).\na0\n\nHence\nG(Xn , \u03c4 )\nn\u2212\u2192+\u221e\nn\n\u0001\n=\n1 \u2212 F (a0 ) ln\nlim\n\n1\n1\n+ F (a0 ) ln\n\u2212\u03c4\n1 \u2212 F (a0 )\nF (a0 )\n\nZ\n\n+\u221e\n\nt dF (t).\n\na0\n\nOn the other hand,\nZ\n\n+\u221e\n\n\u0001\n1 \u2212 F (t) dt\na\nZ +\u221e\n\u0001\n\u2212\u03c4 a\nt dF (t).\n= ln(1 + e\n) + \u03c4 a 1 \u2212 F (a) \u2212 \u03c4\n\u2212\u03c4 a\n\ng\u03c4 (a) = ln(1 + e\n\n)\u2212\u03c4\n\na\n\nSince a0 is a critical point of g\u03c4 , we have\n\n\u0001\n\u03c4 a0 = ln F (a0 ) \u2212 ln 1 \u2212 F (a0 ) ,\n\ncf. Section 4.3. Therefore,\nZ\n\u0010\n\u0001\u0011\n\u0001\ng\u03c4 (a0 ) = \u2212 ln F (a0 ) + ln F (a0 ) \u2212 ln 1 \u2212 F (a0 )\n1 \u2212 F (a0 ) \u2212\u03c4\n\n+\u221e\n\nt dF (t)\n\na0\n\n1\n1\n= 1 \u2212 F (a0 ) ln\n+ F (a0 ) ln\n\u2212\u03c4\n1 \u2212 F (a0 )\nF (a0 )\n\u0001\n\nand the proof follows.\n\nZ\n\n+\u221e\n\nt dF (t)\na0\n\n\u0003\n\nSome remarks are in order.\n(4.7) Remarks.\n(4.7.1) Optimizing in a in Lemma 4.4, we substitute a = \u0393(X0 ) \u2212 \u0393(X1 ) and\nobtain the inequality\nZ +\u221e\n\u0001\nt dF (t)\n\u0393(X) \u2265 1 \u2212 F (a) \u0393(X1 ) + F (a)\u0393(X0 ) +\na\nZ\n\u0001\n=\u0393(X0 ) +\n1 \u2212 F (t) dt.\n\u0393(X0 )\u2212\u0393(X1 )\n\nThis inequality is harder to work with than with that of Theorem 4.2 but it sometimes leads to more delicate estimates, see Section 7.\n(4.7.2) M. Talagrand proved in [T94] that for every non-empty set X of subsets of\n{1, . . . , n} there is a \"shifted\" set X \u2032 of subsets of {1, . . . , n} such that |X \u2032 | = |X|,\n\u0393(X \u2032 ) \u2264 \u0393(X), X \u2032 is hereditary (that is, if x \u2208 X \u2032 and y \u2282 x then y \u2208 X \u2032 ) and\nleft-hereditary (that is, if x \u2208 X \u2032 , i \u2208 x, j \u2208\n/ x and j < i then the subset x \u222a {j} \\ {i}\nalso lies in X \u2032 ).\n17\n\n\f5. General Estimates: the Lower Bound\nLet us fix a symmetric probability measure \u03bc with the cumulative distribution\nfunction F . In this section, we prove the following main result.\n(5.1) Theorem. Assume that the moment generating function\n\u03b4x\n\nL(\u03b4, \u03bc) = L(\u03b4) = E e\n\n=\n\nZ\n\n+\u221e\n\ne\u03b4x d\u03bc(x)\n\u2212\u221e\n\nis finite in some neighborhood of \u03b4 = 0. Let\n\u0010\n\u0011\nh(t, \u03bc) = h(t) = sup \u03b4t \u2212 ln L(\u03b4)\n\nfor\n\nt \u2265 0.\n\n\u03b4\u22650\n\n(1) For any non-empty family X of k-subsets of {1, . . . , n}, we have\nk \u22121 ln |X| \u2265 h(t)\n\nfor\n\nt = k \u22121 \u0393(X);\n\n(2) For any t > 0 such that F (t) < 1 and for any 0 < \u01eb < 0.1 there exist\nk = k(t, \u01eb, \u03bc), n = n(k), and a family of k-subsets of the set {1, . . . , n} such\nthat\nk \u22121 \u0393(X) \u2265 (1 \u2212 \u01eb)t and k \u22121 ln |X| \u2264 h(t) + \u01eb.\nBefore proving Theorem 5.1, we summarize some properties of L(\u03b4) and h(t).\n(5.2) Preliminaries. Let f (\u03b4) = ln L(\u03b4). Thus we assume that f (\u03b4) is finite on\nsome interval in R, possibly on the whole line. It is known that f (\u03b4) is convex\nand continuous on the interval where it is finite, see, for example, Section 5.11 of\n[GS01]. Since \u03bc is symmetric, we have f (0) = 0 and from Jensen's inequality we\nconclude that f (\u03b4) \u2265 0 for all \u03b4.\nThe function h(t) is convex conjugate to f (\u03b4). Therefore, h(t) is finite on some\ninterval where it is convex, continuous and approaches +\u221e as t approaches a boundary point not in the interval. Besides,\nt2\nh(t) =\n+ O(t4 ) for\n2D\n\nt \u2248 0,\n\nwhere D is the variance of \u03bc. In particular, h(0) = 0 and h(t) is increasing for\nt \u2265 0, see Section 5.11 of [GS01].\nNow we are ready to prove Theorem 5.1.\nProof of Theorem 5.1.\n18\n\n\fLet us prove Part (1). Without loss of generality, we assume that \u0393(X) > 0. Let\nus choose a positive integer m, let N = nm, K = km and let\nY = X \u00d7 . . . \u00d7 X \u2282 {0, 1}N .\n|\n{z\n}\nm times\n\nLet us pick a point y = (x1 , . . . , xm ) from Y , where xi \u2208 X for i = 1, . . . , m. Thus\nsome K coordinates of y are 1's and the rest are 0's. Let us endow RN with the\nproduct measure \u03bc\u2297N and let \u03b31 , . . . , \u03b3K be independent random variables with\nthe distribution \u03bc. Then, for any t > 0\nK\nK\no\nnX\nn\no\nnX\nto\nN\n\u03b3i > K\n\u03b3i > mt = P\nP c \u2208 R : hc, yi > mt = P\n.\nk\ni=1\ni=1\n\nBy the Large Deviations Inequality (see, for example, Section 5.11 of [GS01])\nP\n\nK\nnX\n\n\u03b3i \u2265 K\n\ni=1\n\nTherefore,\nn\nP c \u2208 RN :\n\n\b\nto\n\u2264 exp \u2212Kh(t/k) .\nk\n\n\u0011m\no\n\u0010\n\b\n\b\nmaxhc, yi > mt \u2264 |Y | exp \u2212Kh(t/k) = |X| exp \u2212kh(t/k)\n.\ny\u2208Y\n\nSince a vector c \u2208 RN is an m-tuple c = (c1 , . . . , cm ) with ci \u2208 Rn and\nmaxhc, yi =\ny\u2208Y\n\nm\nX\ni=1\n\nmaxhci , xi,\nx\u2208X\n\nthe last inequality can be written as\nm\n\u0011m\no \u0010\n\b\n1 X\nmaxhci , xi > t \u2264 |X| exp \u2212kh(t/k)\n.\nm i=1 x\u2208X\n\nn\nP c1 , . . . , cm :\n\nHowever, by the Law of Large Numbers\nm\n\n1 X\nmaxhci , xi \u2212\u2192 \u0393(X) in probability\nm i=1 x\u2208X\nas m \u2212\u2192 +\u221e. Therefore, for any 0 < t < \u0393(X),\nn\nP c1 , . . . , cm :\n\nm\n\no\n1 X\nmaxhci , xi > t \u2212\u2192 1\nm i=1 x\u2208X\n19\n\nas m \u2212\u2192 +\u221e.\n\n\fTherefore, we must have\n|X| exp{\u2212kh(t/k)} \u2265 1 for every t < \u0393(X).\nHence\nk \u22121 ln |X| \u2265 h(t)\n\nfor every\n\nt < k \u22121 \u0393(X),\n\nand the proof follows by the continuity of h, cf. Section 5.2.\nLet us prove Part (2). Let \u03b31 , . . . , \u03b3k be independent random variables having\nthe distribution \u03bc. By the Large Deviations Theorem (see Section 5.11 of [GS01]),\nif k = k(\u01eb, t, \u03bc) is sufficiently large then\nP\n\nk\nnX\ni=1\n\no\n\n\b\n\u0001\n\u03b3i > kt \u2265 exp \u2212k h(t) + \u01eb/2 .\n\n\u0001\nWe make k large enough to ensure, additionally, that ln 3 + ln ln(1/\u01eb) /k \u2264 \u01eb/2.\nLet |X| be the largest integer not exceeding\n3 ln\n\n\b\n\u0001\n1\nexp k h(t) + \u01eb/2 ,\n\u01eb\n\nso k \u22121 ln |X| \u2264 h(t) + \u01eb, and let X consist of |X| pairwise disjoint k-subsets of\n{1, . . . , n} for a sufficiently large n = n(k).\nSuppose that c = (\u03b31 , . . . , \u03b3n ) is a random vector of independent\nweights with\nP\nthe distribution \u03bc. Since x \u2208 X are disjoint, the weights i\u2208x \u03b3i of subsets from\nX are independent random variables. Let w(X, c) be the largest weight of a subset\nx \u2208 X. We have\nn\nP c:\n\no \u0010\n\b\n\u0001 \u0011|X|\nw(X, c) \u2264 kt \u2264 1 \u2212 exp \u2212k h(t) + \u01eb/2\n\u2264 \u01eb/2.\n\nSimilarly (since \u03bc is symmetric):\nn\no\nP c : w(X, \u2212c) \u2264 kt \u2264 \u01eb/2,\nand, therefore,\n\nn\nP c:\n\no\nw(X, c) + w(X, \u2212c) \u2264 2kt \u2264 \u01eb.\n\nSince w(X, c)+w(X, \u2212c) is always non-negative, its expectation is at least (1\u2212\u01eb)2kt.\nOn the other hand, this expectation is 2\u0393(X). Hence we have constructed a family\nX of k-subsets such that\nk \u22121 \u0393(X) \u2265 (1 \u2212 \u01eb)t and\n\nk \u22121 ln |X| \u2264 h(t) + \u01eb.\n\u0003\n\n20\n\n\f(5.3) Remarks.\n(5.3.1) Using the convexity of h(t), one can extend the bound of Part (1) of\nTheorem 5.1 to families X of at most k-element subsets of {1, . . . , n}.\n(5.3.2) Suppose that the moment generating function L(\u03b4, \u03bc) is infinite for all \u03b4\nexcept for \u03b4 = 0. Let us choose t > 0 and 0 < \u01eb < 0.1. We claim that there exists\na family X of k-subsets of {1, . . . , n} such that\nk \u22121 \u0393(X) \u2265 (1 \u2212 \u01eb)t\n\nand\n\nk \u22121 ln |X| \u2264 \u01eb\n\n(in other words, we can formally take h(t) \u2261 0 in Part (2) of Theorem 5.1). Let \u03b3\nbe a random variable with the distribution \u03bc. For c > 0, let \u03b3c be the truncation\nof \u03b3:\n\u001a\n\u03b3, if |\u03b3| \u2264 c\n\u03b3c =\n0, if |\u03b3| > c.\nLet \u03bcc be the distribution of \u03b3c . It is not hard to see that \u0393(X, \u03bc) \u2265 \u0393(X, \u03bcc )\n(consider \u0393(X) as the expectation of 0.5w(X, c) + 0.5w(X, \u2212c), where w(X, c) is\nthe maximum weight of a subset x \u2208 X for the vector c = (\u03b31 , . . . , \u03b3n ) of weights).\nChoosing a sufficiently large c brings h(t, \u03bcc ) arbitrarily close to 0. Then we construct a set X as in Part (2) of Theorem 5.1.\n(5.3.3) Our proof of Part (2) of Theorem 5.1 seems to require n to be exponentially large in k. This is not so, since every suitable pair n, k can be rescaled to\na suitable pair N = nm, K = km for a positive integer m. Let X be a family of\nk-subsets of {1, . . . , n} constructed in the proof of Part (2) and let\nY = X \u00d7 . . . \u00d7 X \u2282 {0, 1}N .\n{z\n}\n|\nm times\n\nThen Y is a family of K-subsets of {1, . . . , N } and\nK \u22121 \u0393(Y ) \u2265 (1 \u2212 \u01eb)t and\n\nK \u22121 ln |Y | \u2264 h(t) + \u01eb.\n\n6. The Logistic Measure: Proofs\nIn this section, we prove Theorems 3.1 and 3.3.\nProof of Theorem 3.1. To prove Part (1), let us choose \u03c4 = 1 in Part (1) of Theorem\n4.2. We have\nZ +\u221e\ne\u2212t\n\u2212a\ng1 (a) = ln(1 + e ) \u2212\ndt = 0 for all a.\n1 + e\u2212t\na\nHence\nln |X| \u2264 \u0393(X)\nas claimed.\n21\n\n\fTo prove Part (2), we use Part (1) of Theorem 5.1. The moment generating\nfunction of the logistic distribution is given by\nL(\u03b4) =\n\nZ\n\n+\u221e\n\u2212\u221e\n\n\u03c0\u03b4\ne\u03b4x\ndx =\nx\n\u2212x\ne +e +2\nsin \u03c0\u03b4\n\nfor\n\n\u2212 1 < \u03b4 < 1,\n\nsee [M85]. Hence the formula for h(t) follows.\nIt follows from Section 5.2 that h is convex and increasing.\n\n\u0003\n\nNow we are ready to prove optimality of the logistic distribution.\nProof of Theorem 3.3. Let us choose \u03bc \u2208 M and let F\u03bc be the cumulative distribution function of \u03bc. We claim that F\u03bc (t) < 1 for all t \u2208 R. To see that, we let \u03c4 = 1\nin Theorem 4.2. If F\u03bc (t) = 1 then g1 (t) > 0 and, by Part (2) of Theorem 4.2, there\nis a set X \u2282 {0, 1}n with ln |X| > \u0393(X), which contradicts the definition of M.\nLet us assume first that the moment generating function L(\u03b4, \u03bc) is finite in some\nneighborhood of \u03b4 = 0. Then, by Theorem 5.1, we have c(t, \u03bc) = h(t, \u03bc) and hence\nwe must prove that h(t, \u03bc) \u2264 h(t, \u03bc0 ), where \u03bc0 is the logistic distribution.\nLet\nZ +\u221e\n\u0001\nT (a, \u03bc) =\n1 \u2212 F\u03bc (t) dt.\na\n\nWe can write\nZ +\u221e\nZ +\u221e\nZ +\u221e\n\u0001 1\n\u0001\n\u03b4x\n\u03b4x\ne d 1 \u2212 F\u03bc (x) = +\ne dF\u03bc (x) = \u2212\n\u03b4e\u03b4x 1 \u2212 F\u03bc (x) dx\n2\n0\n0\n0\nZ +\u221e\nZ +\u221e\n1\n1\n\u03b4e\u03b4x d(\u2212T (x, \u03bc)) = + \u03b4T (0, \u03bc) +\n\u03b4 2 e\u03b4x T (x, \u03bc) dx.\n= +\n2\n2\n0\n0\nSimilarly,\nZ\n\n0\n\u03b4x\n\ne\n\n\u2212\u221e\n\ndF\u03bc (x) =\n\nZ\n\n0\n\n+\u221e\n\u2212\u03b4x\n\ne\n\n1\ndF\u03bc (x) = \u2212 \u03b4T (0, \u03bc) +\n2\n\nTherefore,\nL(\u03b4, \u03bc) = 1 + \u03b4\n\n2\n\nZ\n\n+\u221e\n0\n\nZ\n\n+\u221e\n\n\u03b4 2 e\u2212\u03b4x T (x, \u03bc) dx.\n\n0\n\n\u0001\ne\u2212\u03b4x + e\u03b4x T (x, \u03bc) dx.\n\nSince ln |X| \u2264 \u0393(X), by Part (2) of Theorem 4.2 we conclude that\nT (a, \u03bc) \u2265 ln(1 + e\u2212a ) = T (a, \u03bc0 ) for all a \u2265 0.\nTherefore, L(\u03b4, \u03bc) \u2265 L(\u03b4, \u03bc0 ) and h(t, \u03bc) \u2264 h(t, \u03bc0 ) for all t \u2265 0, as claimed.\nSuppose now that the moment generating function L(\u03b4, \u03bc) is infinite for \u03b4 6= 0.\nThen, as follows from Remark 5.3.2, c(t, \u03bc) = 0 for all t > 0, which completes the\nproof.\n\u0003\n22\n\n\f7. The Exponential Measure\nLet us choose \u03bc to be the measure with density\n1 \u2212|\u03b3|\ne\n2\n\nfor\n\n\u03b3 \u2208 R.\n\nAs we have already mentioned, one of the results of [T94] is the estimate\nln |X| \u2264 c\u0393(X)\nfor some absolute constant c. In this section, we find the optimal value of c and\nestablish some general isoperimetric inequalities which, we believe, are interesting\nin their own right.\n(7.1) Theorem. Let \u03bc be the measure with density e\u2212|\u03b3| /2 for \u03b3 \u2208 R.\n(1) Let X \u2282 {0, 1}n be a non-empty subset of the Boolean cube. Then\nln |X| \u2264 (2 ln 2)\u0393(X);\n(2) Let X \u2282 {0, 1}n be a non-empty subset of the Boolean cube such that \u03be1 +\n. . . + \u03ben \u2264 k for every (\u03be1 , . . . , \u03ben ) \u2208 X. That is, X lies in the Hamming\nball of radius k and we may interpret X as a family of at most k-element\nsubsets of {1, . . . , n}. Then\nln |X| \u2264 \u0393(X) + k ln 2.\nBefore we prove Theorem 7.1, we note that c = 2 ln 2 is the best possible value\nin Part (1). If X is a m-dimensional face of the Boolean cube then ln |X| = m ln 2\nand we show that \u0393(X) = m/2, so the equality holds. As in Section 3, it suffices\nto check the formula for X = {0, 1}, in which case\n1\n\u0393(X) =\n2\n\nZ\n\n+\u221e\n\nxe\u2212x dx =\n\n0\n\n1\n.\n2\n\nThe inequality of Part (2) is asymptotically sharp: if X is the Hamming sphere of\nradius k = o(n) in {0, 1}n, then\n\u0393(X) = ln |X| \u2212 k ln 2 + o(k)\n\nas k \u2212\u2192 +\u221e,\n\ncf. Lemma 4.6.\nAs for the lower bound, using Part (1) of Theorem 5.1 one can show that for any\nnon-empty family X of k-subsets of {1, . . . , n}, we have\n\u0001\nk \u22121 ln |X| \u2265 h k \u22121 \u0393(X) ,\n23\n\n\fwhere\np\np\n\u0001\nh(t) = 1 + t2 + ln 1 + t2 \u2212 1 \u2212 2 ln t + ln 2 \u2212 1\n=t \u2212 ln t \u2212 O(1) for large t.\n\nThus the exponential distribution also allows us to estimate ln |X| up to a constant\nfactor. However, the estimates are not as good as for the logistic distribution.\nProof of Theorem 7.1. To prove Part (1), we use Part (1) of Theorem 4.2.\nThe function g\u03c4 (a) is given by\ng\u03c4 (a) = ln(1 + e\u2212\u03c4 a ) \u2212\n\n\u03c4 \u2212a\ne\n2\n\nfor\n\na \u2208 R.\n\nLet us consider the critical points of g\u03c4 .\nWe have\n\u03c4 \u0010 e(\u03c4 \u22121)a + e\u2212a \u2212 2 \u0011\ng\u03c4\u2032 (a) =\n.\n2\n1 + e\u03c4 a\nSince the numerator of the fraction is a linear combination of two exponential\nfunctions and a constant, it can have at most two real zeros. We observe that a = 0\nis a zero and that g\u03c4\u2032 (a) < 0 for small a > 0 provided \u03c4 < 2.\nHence for \u03c4 < 2 the function g\u03c4 has at most one critical point a > 0 which has\nto be a point of local minimum.\nTherefore\n\b\nsup g\u03c4 (a) = max g\u03c4 (0), 0\nfor all \u03c4 < 2.\na\u22650\n\nLet us choose \u03c4 = 2 ln 2. Then g\u03c4 (0) = 0 and we conclude that\nsup g\u03c4 (a) = 0.\na\u22650\n\nBy Part (1) of Theorem 4.2, we conclude that\nln |X| \u2264 \u03c4 \u0393(X) = (2 ln 2)\u0393(X).\nWe prove Part (2) by induction on n. If n = 1, there are two cases. If X\nconsists of a single point then \u0393(X) = 0, ln |X| = 0 and the inequality is satisfied.\nIf X = {0, 1} then k = 1 and \u0393(X) = 1/2, hence the inequality holds as well.\nSuppose that n > 1. Clearly, we can assume that k > 0. Without loss of\ngenerality, we may assume that X is hereditary, see Remark 4.7.2. Let us construct\nsets X0 , X1 \u2282 {0, 1}n\u22121 as in Lemma 4.4. We note that X0 lies in the Hamming ball\nof radius k and X1 lies in the Hamming ball of radius k \u2212 1. Since X is hereditary,\nX1 \u2282 X0 . Therefore,\n|X0 | \u2265 |X1 |\n\nand \u0393(X0 ) \u2265 \u0393(X1 ).\n24\n\n\fThe inequality of Remark 4.7.1 gives us\n\u0393(X) \u2265 \u0393(X0 ) +\n\n\b\n1\nexp \u0393(X1 ) \u2212 \u0393(X0 ) .\n2\n\nLet us consider a function\n\n1\nf (a, b) = a + eb\u2212a .\n2\nIt is easy to see that for every a the function is increasing in b and that for every b\nit is increasing on the interval a \u2265 b \u2212 ln 2.\nApplying the induction hypothesis to X0 and X1 , we conclude\n\u0001\n\u0001\nf \u0393(X0 ), \u0393(X1 ) \u2265f \u0393(X0 ), ln |X1 | \u2212 (k \u2212 1) ln 2\n\u0001\n\u2265f ln |X0 | \u2212 k ln 2, ln |X1 | \u2212 (k \u2212 1) ln 2 .\nTherefore,\n\u0011 \u0010 |X |\n|X1 | \u0010\n|X1 | + |X0 | \u0011\n1\n= ln |X| \u2212 k ln 2 +\n\u2212 ln\n|X0 |\n|X0 |\n|X0 |\n\u0010\n\u0011\n\u0001\n|X1 |\n= ln |X| \u2212 k ln 2 + t \u2212 ln(1 + t) for t =\n|X0 |\n\u2265 ln |X| \u2212 k ln 2.\n\n\u0393(X) \u2265 ln |X0 | \u2212 k ln 2 +\n\nThe proof now follows.\n\n\u0003\n\n8. An Asymptotic Solution to the Isoperimetric Problem\nIn this section, we discuss what sets Xn \u2282 {0, 1}n with the smallest ratio\n\u0393(Xn , \u03bc)/ ln |Xn | may look like. We claim that for any symmetric probability measure \u03bc with finite variance and for a sufficiently large n we can choose Xn to be the\nproduct of at most two Hamming spheres.\n(8.1) Theorem. Let us fix a symmetric probability measure \u03bc and a number\n0 < \u03b1 < ln 2.\nThen there exist numbers \u03b2i , \u03bbi , i = 1, 2, depending on \u03b1 and \u03bc only, such that\n0 \u2264 \u03b2i \u2264 \u03bbi\n\nfor\n\ni = 1, 2,\n\n\u03bb1 + \u03bb2 = 1\nand the following holds.\nLet Sni be the Hamming sphere of radius \u03b2i n + o(n) in the Boolean cube of\ndimension \u03bbi n + o(n), i = 1, 2, and let Yn = Sn1 \u00d7 Sn2 be the direct product of the\nspheres considered as a subset of the Boolean cube of dimension n.\n25\n\n\fThen\nln |Yn | = \u03b1n + o(n)\nand for any sequence of sets Xn \u2282 {0, 1}n such that\nln |Xn | = \u03b1n + o(n),\nwe have\n\u0393(Yn , \u03bc) \u2264 \u0393(Xn , \u03bc) + o(n).\nProof. Let F be the cumulative distribution function of \u03bc. Without loss of generality, we assume that F is continuous and strictly increasing. Given \u03bc and \u03b1, let\nus consider the function\n\u0001 Z +\u221e\n\u0001\n\u03b1 ln 1 + e\u2212\u03c4 x\n1 \u2212 F (t) dt\n+\nH(\u03c4, x) = \u2212\n\u03c4\n\u03c4\nx\nof two variables \u03c4 > 0 and x \u2265 0.\nBy Part (1) of Theorem 4.2, for any \u03c4 > 0,\n(8.1.1)\n\nn\u22121 \u0393(Xn ) \u2265 inf H(\u03c4, x) provided\nx\u22650\n\nln |Xn | = \u03b1n + o(n).\n\nWe claim that there exists 0 < \u03c40 < +\u221e such that\ninf H(\u03c40 , x) \u2265 inf H(\u03c4, x) for all \u03c4 \u2265 0.\n\nx\u22650\n\nx\u22650\n\nIndeed, since \u03b1 < ln 2,\ninf H(\u03c4, x) \u2212\u2192 \u2212\u221e\n\nx\u22650\n\nas \u03c4 \u2212\u2192 0 + .\n\nAlso,\ninf H(\u03c4, x) \u2212\u2192 0 as\n\nx\u22650\n\n\u03c4 \u2212\u2192 +\u221e.\n\nOn the other hand, choosing x1 > 0 such that\nZ\n\n+\u221e\nx1\n\n\u0001\n1 \u2212 F (t) dt = \u03b4 > 0\n\nand \u03c41 such that\n\u0001\n\u03b1 \u2212 ln 1 + e\u2212\u03c41 x1 > 0 and\n\n\u03c41\u22121 |\u03b1 \u2212 ln 2| < \u03b4\n\nwe observe that\ninf H(\u03c41 , x) > 0,\n\nx\u22650\n\n26\n\n\fwhich implies that there exists 0 < \u03c40 < +\u221e maximizing inf x\u22650 H(\u03c4, x).\nOur next goal is to show that one can find 0 \u2264 x1 , x2 \u2264 +\u221e such that\n(8.1.2)\n\nH(\u03c40 , x1 ) = H(\u03c40 , x2 ) = inf H(\u03c40 , x)\nx\u22650\n\nand such that\n(8.1.3)\n\n\u03c4 0 x1\n\u03c4\n0\ne x1 +\n\n\u0001\n+ ln 1 + e\u2212\u03c40 x1 \u2265 \u03b1 and\n1\n\u0001\n\u03c4 0 x2\n\u2212\u03c40 x2\n+\nln\n1\n+\ne\n\u2264 \u03b1.\ne\u03c40 x2 + 1\n\n(it is possible that x1 = x2 or that x2 = +\u221e).\nFor \u01eb in a small neighborhood of 0, we define x\u01eb \u2265 0 as a point such that\n\u0001\nH \u03c40 + \u01eb, x\u01eb = inf H(\u03c40 + \u01eb, x)\nx\u22650\n\n(possibly x\u01eb = +\u221e). We obtain x1 as a limit point of x\u01eb as \u01eb \u2212\u2192 0\u2212 and x2 as a\nlimit point of x\u01eb as \u01eb \u2212\u2192 0+. Clearly, (8.1.2) holds and it remains to show that\n(8.1.3) holds as well.\nIndeed,\n\u0001\n\u0001\n\u2202\nH(\u03c40 , xi ) \u2265H \u03c40 + \u01eb, x\u01eb = H(\u03c40 , x\u01eb ) + \u01eb H(\u03c4 , x\u01eb\n\u2202\u03c4\n\u0001\n\u2202\n\u2265H(\u03c40 , xi ) + \u01eb H(\u03c4 , x\u01eb\n\u2202\u03c4\nfor some \u03c4 between \u03c40 and \u03c40 + \u01eb and i = 1, 2.\nBesides,\n\u0011\n\u0001\n1 \u0010 \u03c4x\n\u2202\n\u2212\u03c4 x\nH(\u03c4, x) = 2\n+\nln\n1\n+\ne\n\u2212\n\u03b1\n,\n\u2202\u03c4\n\u03c4 1 + e\u03c4 x\n\nfrom which we deduce (8.1.3).\nAdditionally, from (8.1.2) we deduce that if 0 < xi < +\u221e, we must have\n\u2202\nH(\u03c40 , xi ) = 0,\n\u2202x\nthat is,\n(8.1.4)\n\n1\ne\u03c40 xi\n\n+1\n\n= 1 \u2212 F (xi ),\n\nfor\n\ni = 1, 2,\n\nwhich also holds for xi = 0 and xi = +\u221e.\nNow we are ready to define \u03bbi and \u03b2i . Namely, we write\nX \u0010 \u03c4 0 xi\n\u0001\u0011\nwhere \u03bb1 , \u03bb2 \u2265 0 and\n+ ln 1 + e\u2212\u03c40 xi\n\u03b1=\n\u03bbi \u03c40 xi\ne\n+\n1\ni=1,2\n27\n\n\u03bb1 + \u03bb2 = 1,\n\n\fcf. (8.1.3).\nNext, we define \u03b21 and \u03b22 by\n\u03b2i =\n\n\u03bbi\n+1\n\ne\u03c40 xi\n\nfor\n\ni = 1, 2.\n\nLet Sni be the Hamming sphere of dimension \u03bbi n + o(n) and radius \u03b2i n + o(n).\nUsing Theorem 1.4.5 of [Li99], we obtain\n\u0001\n\u0001\n1\n1\ne\u03c40 xi\ni\n\u03c40 xi\nln |Sn | = \u03c4 x\nln e\n+1 + \u03c4 x\nln 1 + e\u2212\u03c40 xi + o(1)\n\u03bbi n\ne 0 i +1\ne 0 i +1\n\u0001\n\u03c4 0 xi\n+ ln 1 + e\u2212\u03c40 xi + o(1).\n= \u03c40 xi\ne\n+1\nThus for Yn = Sn1 \u00d7 Sn2 , we have\nln |Yn | = \u03b1n + o(n),\nas claimed.\nBy Part (2) of Theorem 4.2 and (8.1.4)\n\u0393(Sni )\n= H(\u03c40 , xi ) + o(1).\n\u03bbi n\nUsing (8.1.2) we conclude that for Yn = Sn1 \u00d7 Sn2 , we have\n\u0393(Yn ) \u0393(Sn1 ) + \u0393(Sn2 )\n=\n= \u03bb1 H(\u03c40 , x1 ) + \u03bb2 H(\u03c40 , x2 ) + o(1).\nn\nn\n= inf H(\u03c40 , x) + o(1)\nx\u22650\n\nHence, by (8.1.1),\n\u0393(Yn ) \u2264 \u0393(Xn ) + o(n),\nwhich completes the proof.\n\n\u0003\nAcknowledgment\n\nThe authors are very grateful to Nathan Linial for many helpful discussions and\nencouragement, in particular, for his suggestion to look for an optimal solution to\nthe \"inverse isoperimetric problem\".\n28\n\n\fReferences\n[ABS98]\n[B97]\n[BS01]\n[GS01]\n[JS97]\n\n[JSV01]\n\n[La97]\n[Le91]\n\n[Led01]\n[Li99]\n[M85]\n[PS98]\n[S73]\n[T94]\n[T95]\n[Y03]\n\nN. Alon, R. Boppana, and J. Spencer, An asymptotic isoperimetric inequality, Geom.\nFunct. Anal. 8 (1998), 411\u2013436.\nA. Barvinok, Approximate counting via random optimization, Random Structures &\nAlgorithms 11 (1997), 187\u2013198.\nA. Barvinok and A. Samorodnitsky, The distance approach to approximate combinatorial counting, Geom. Funct. Anal. 11 (2001), 871\u2013899.\nG.R. Grimmett and D.R. Stirzaker, Probability and Random Processes. Third edition,\nThe Clarendon Press, Oxford University Press, New York, 2001.\nM. Jerrum and A. Sinclair, The Markov chain Monte Carlo method: an approach to\napproximate counting and integration, Approximation Algorithms for NP-hard Problems, D.S. Hochbaum, ed., PWS, Boston, 1997, pp. 483\u2013520.\nM. Jerrum, A. Sinclair, and E. Vigoda, A polynomial-time approximation algorithm for\nthe permanent of a matrix with non-negative entries, Proceedings of the Thirty-Third\nAnnual ACM Symposium on Theory of Computing, ACM Press, New York, NY, USA,\npp. 712\u2013721.\nR. Latala, Sudakov minoration principle and supremum of some processes, Geom.\nFunct. Anal. 7 (1997), 936\u2013953.\nI. Leader, Discrete isoperimetric inequalities, Probabilistic Combinatorics and its Applications (San Francisco, CA 1991), Proc. Sympos. Appl. Math., vol. 44, Amer. Math.\nSoc., Providence, RI, pp. 57\u201380.\nM. Ledoux, The Concentration of Measure Phenomenon, Mathematical Surveys and\nMonographs, vol. 89, American Mathematical Society, Providence, RI, 2001.\nJ.H. van Lint, Introduction to Coding Theory. Third edition, Graduate Texts in Mathematics, vol. 86, Springer-Verlag, Berlin, 1999.\nH.J. Malik, Logistic distribution, Encyclopedia of Statistical Sciences (S. Kotz, N.L.\nJohnson and C.B. Read, eds.), vol. 5, Wiley-Interscience, New York, pp. 123\u2013129.\nC.H. Papadimitriou and K. Steiglitz, Combinatorial Optimization: Algorithms and\nComplexity, Dover, NY, 1998.\nS.M. Stigler, The asymptotic distribution of the trimmed mean, Annals of Statistics 1\n(1973), 472\u2013477.\nM. Talagrand, The supremum of some canonical processes, Amer. Jour. Math. 116\n(1994), 283\u2013325.\nM. Talagrand, Concentration of measure and isoperimetric inequalities in product\nspaces, Inst. Hautes \u00c9tudes Sci. Publ. Math 81 (1995), 73\u2013205.\nA. Yong, Experimental C++ codes for estimating permanents, hafnians and the number of forests in a graph, available at http://www.math.lsa.umich.edu/\u223c barvinok/ papers.html (2003).\n\nDepartment of Mathematics, University of Michigan, Ann Arbor, MI 48109-1109,\nUSA\nE-mail address: barvinok@umich.edu\nDepartment of Computer Science, Hebrew University of Jerusalem, Givat Ram\nCampus, Jerusalem 91904, Israel\nE-mail address: salex@cs.huji.ac.il\n\n29\n\n\f"}