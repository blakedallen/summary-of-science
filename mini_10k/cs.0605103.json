{"id": "http://arxiv.org/abs/cs/0605103v8", "guidislink": true, "updated": "2007-04-28T23:13:34Z", "updated_parsed": [2007, 4, 28, 23, 13, 34, 5, 118, 0], "published": "2006-05-24T04:42:53Z", "published_parsed": [2006, 5, 24, 4, 42, 53, 2, 144, 0], "title": "A Better Alternative to Piecewise Linear Time Series Segmentation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0605077%2Ccs%2F0605098%2Ccs%2F0605133%2Ccs%2F0605144%2Ccs%2F0605073%2Ccs%2F0605116%2Ccs%2F0605129%2Ccs%2F0605140%2Ccs%2F0605135%2Ccs%2F0605056%2Ccs%2F0605143%2Ccs%2F0605035%2Ccs%2F0605136%2Ccs%2F0605075%2Ccs%2F0605037%2Ccs%2F0605105%2Ccs%2F0605121%2Ccs%2F0605087%2Ccs%2F0605079%2Ccs%2F0605120%2Ccs%2F0605005%2Ccs%2F0605132%2Ccs%2F0605104%2Ccs%2F0605038%2Ccs%2F0605011%2Ccs%2F0605068%2Ccs%2F0605130%2Ccs%2F0605106%2Ccs%2F0605060%2Ccs%2F0605062%2Ccs%2F0605114%2Ccs%2F0605127%2Ccs%2F0605123%2Ccs%2F0605008%2Ccs%2F0605128%2Ccs%2F0605111%2Ccs%2F0605036%2Ccs%2F0605103%2Ccs%2F0605013%2Ccs%2F0605085%2Ccs%2F0605045%2Ccs%2F0605099%2Ccs%2F0605134%2Ccs%2F0605032%2Ccs%2F0605029%2Ccs%2F0605074%2Ccs%2F0605026%2Ccs%2F0605054%2Ccs%2F0605113%2Ccs%2F0605024%2Ccs%2F0605027%2Ccs%2F0605047%2Ccs%2F0605019%2Ccs%2F0605086%2Ccs%2F0605025%2Ccs%2F0605040%2Ccs%2F0605007%2Ccs%2F0605028%2Ccs%2F0605030%2Ccs%2F0605082%2Ccs%2F0605053%2Ccs%2F0605090%2Ccs%2F0605004%2Ccs%2F0605094%2Ccs%2F0605095%2Ccs%2F0605084%2Ccs%2F0605012%2Ccs%2F0605100%2Ccs%2F0605081%2Ccs%2F0605022%2Ccs%2F0605052%2Ccs%2F0605069%2Ccs%2F0605110%2Ccs%2F0605057%2Ccs%2F0605088%2Ccs%2F0605109%2Ccs%2F0605145%2Ccs%2F0605102%2Ccs%2F0605112%2Ccs%2F0605009%2Ccs%2F0605023%2Ccs%2F0605078%2Ccs%2F0605059%2Ccs%2F0605101%2Ccs%2F0605126%2Ccs%2F0605016%2Ccs%2F0605010%2Ccs%2F0605071%2Ccs%2F0605117%2Ccs%2F0605137%2Ccs%2F0605051%2Ccs%2F0605108%2Ccs%2F0605091%2Ccs%2F0605018%2Ccs%2F0605142%2Ccs%2F0605119%2Ccs%2F0605006%2Ccs%2F0605003%2Ccs%2F0605141%2Ccs%2F0605034%2Ccs%2F0605080&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Better Alternative to Piecewise Linear Time Series Segmentation"}, "summary": "Time series are difficult to monitor, summarize and predict. Segmentation\norganizes time series into few intervals having uniform characteristics\n(flatness, linearity, modality, monotonicity and so on). For scalability, we\nrequire fast linear time algorithms. The popular piecewise linear model can\ndetermine where the data goes up or down and at what rate. Unfortunately, when\nthe data does not follow a linear model, the computation of the local slope\ncreates overfitting. We propose an adaptive time series model where the\npolynomial degree of each interval vary (constant, linear and so on). Given a\nnumber of regressors, the cost of each interval is its polynomial degree:\nconstant intervals cost 1 regressor, linear intervals cost 2 regressors, and so\non. Our goal is to minimize the Euclidean (l_2) error for a given model\ncomplexity. Experimentally, we investigate the model where intervals can be\neither constant or linear. Over synthetic random walks, historical stock market\nprices, and electrocardiograms, the adaptive model provides a more accurate\nsegmentation than the piecewise linear model without increasing the\ncross-validation error or the running time, while providing a richer vocabulary\nto applications. Implementation issues, such as numerical stability and\nreal-world performance, are discussed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0605077%2Ccs%2F0605098%2Ccs%2F0605133%2Ccs%2F0605144%2Ccs%2F0605073%2Ccs%2F0605116%2Ccs%2F0605129%2Ccs%2F0605140%2Ccs%2F0605135%2Ccs%2F0605056%2Ccs%2F0605143%2Ccs%2F0605035%2Ccs%2F0605136%2Ccs%2F0605075%2Ccs%2F0605037%2Ccs%2F0605105%2Ccs%2F0605121%2Ccs%2F0605087%2Ccs%2F0605079%2Ccs%2F0605120%2Ccs%2F0605005%2Ccs%2F0605132%2Ccs%2F0605104%2Ccs%2F0605038%2Ccs%2F0605011%2Ccs%2F0605068%2Ccs%2F0605130%2Ccs%2F0605106%2Ccs%2F0605060%2Ccs%2F0605062%2Ccs%2F0605114%2Ccs%2F0605127%2Ccs%2F0605123%2Ccs%2F0605008%2Ccs%2F0605128%2Ccs%2F0605111%2Ccs%2F0605036%2Ccs%2F0605103%2Ccs%2F0605013%2Ccs%2F0605085%2Ccs%2F0605045%2Ccs%2F0605099%2Ccs%2F0605134%2Ccs%2F0605032%2Ccs%2F0605029%2Ccs%2F0605074%2Ccs%2F0605026%2Ccs%2F0605054%2Ccs%2F0605113%2Ccs%2F0605024%2Ccs%2F0605027%2Ccs%2F0605047%2Ccs%2F0605019%2Ccs%2F0605086%2Ccs%2F0605025%2Ccs%2F0605040%2Ccs%2F0605007%2Ccs%2F0605028%2Ccs%2F0605030%2Ccs%2F0605082%2Ccs%2F0605053%2Ccs%2F0605090%2Ccs%2F0605004%2Ccs%2F0605094%2Ccs%2F0605095%2Ccs%2F0605084%2Ccs%2F0605012%2Ccs%2F0605100%2Ccs%2F0605081%2Ccs%2F0605022%2Ccs%2F0605052%2Ccs%2F0605069%2Ccs%2F0605110%2Ccs%2F0605057%2Ccs%2F0605088%2Ccs%2F0605109%2Ccs%2F0605145%2Ccs%2F0605102%2Ccs%2F0605112%2Ccs%2F0605009%2Ccs%2F0605023%2Ccs%2F0605078%2Ccs%2F0605059%2Ccs%2F0605101%2Ccs%2F0605126%2Ccs%2F0605016%2Ccs%2F0605010%2Ccs%2F0605071%2Ccs%2F0605117%2Ccs%2F0605137%2Ccs%2F0605051%2Ccs%2F0605108%2Ccs%2F0605091%2Ccs%2F0605018%2Ccs%2F0605142%2Ccs%2F0605119%2Ccs%2F0605006%2Ccs%2F0605003%2Ccs%2F0605141%2Ccs%2F0605034%2Ccs%2F0605080&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Time series are difficult to monitor, summarize and predict. Segmentation\norganizes time series into few intervals having uniform characteristics\n(flatness, linearity, modality, monotonicity and so on). For scalability, we\nrequire fast linear time algorithms. The popular piecewise linear model can\ndetermine where the data goes up or down and at what rate. Unfortunately, when\nthe data does not follow a linear model, the computation of the local slope\ncreates overfitting. We propose an adaptive time series model where the\npolynomial degree of each interval vary (constant, linear and so on). Given a\nnumber of regressors, the cost of each interval is its polynomial degree:\nconstant intervals cost 1 regressor, linear intervals cost 2 regressors, and so\non. Our goal is to minimize the Euclidean (l_2) error for a given model\ncomplexity. Experimentally, we investigate the model where intervals can be\neither constant or linear. Over synthetic random walks, historical stock market\nprices, and electrocardiograms, the adaptive model provides a more accurate\nsegmentation than the piecewise linear model without increasing the\ncross-validation error or the running time, while providing a richer vocabulary\nto applications. Implementation issues, such as numerical stability and\nreal-world performance, are discussed."}, "authors": ["Daniel Lemire"], "author_detail": {"name": "Daniel Lemire"}, "author": "Daniel Lemire", "arxiv_comment": "to appear in SIAM Data Mining 2007", "links": [{"href": "http://arxiv.org/abs/cs/0605103v8", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0605103v8", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "H.2.8", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0605103v8", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0605103v8", "journal_reference": null, "doi": null, "fulltext": "A Better Alternative to Piecewise Linear Time Series Segmentation\u2217\nDaniel Lemire\u2020\n\narXiv:cs/0605103v8 [cs.DB] 28 Apr 2007\n\nAbstract\nTime series are difficult to monitor, summarize and predict.\nSegmentation organizes time series into few intervals having\nuniform characteristics (flatness, linearity, modality, monotonicity and so on). For scalability, we require fast linear\ntime algorithms. The popular piecewise linear model can\ndetermine where the data goes up or down and at what rate.\nUnfortunately, when the data does not follow a linear model,\nthe computation of the local slope creates overfitting. We\npropose an adaptive time series model where the polynomial degree of each interval vary (constant, linear and so on).\nGiven a number of regressors, the cost of each interval is its\npolynomial degree: constant intervals cost 1 regressor, linear intervals cost 2 regressors, and so on. Our goal is to\nminimize the Euclidean (l2 ) error for a given model complexity. Experimentally, we investigate the model where intervals can be either constant or linear. Over synthetic random walks, historical stock market prices, and electrocardiograms, the adaptive model provides a more accurate segmentation than the piecewise linear model without increasing the\ncross-validation error or the running time, while providing\na richer vocabulary to applications. Implementation issues,\nsuch as numerical stability and real-world performance, are\ndiscussed.\n1 Introduction\nTime series are ubiquitous in finance, engineering, and science. They are an application area of growing importance in\ndatabase research [2]. Inexpensive sensors can record data\npoints at 5 kHz or more, generating one million samples every three minutes. The primary purpose of time series segmentation is dimensionality reduction. It is used to find frequent patterns [24] or classify time series [29]. Segmentation\npoints divide the time axis into intervals behaving approximately according to a simple model. Recent work on segmentation used quasi-constant or quasi-linear intervals [27],\nquasi-unimodal intervals [23], step, ramp or impulse [21], or\nquasi-monotonic intervals [10, 16]. A good time series segmentation algorithm must\n\u2022 be fast (ideally run in linear time with low overhead);\n\n\u2217 Supported\n\u2020 Universit\u00e9\n\nby NSERC grant 261437.\ndu Qu\u00e9bec \u00e0 Montr\u00e9al (UQAM)\n\n\u2022 provide the necessary vocabulary such as flat, increasing\nat rate x, decreasing at rate x, . . . ;\n\u2022 be accurate (good model fit and cross-validation).\n\n1250\nadaptive (flat,linear)\n\n1200\n1150\n1100\n1050\n1000\n950\n900\n850\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n500\n\n600\n\n1250\npiecewise linear\n\n1200\n1150\n1100\n1050\n1000\n950\n900\n850\n0\n\n100\n\n200\n\n300\n\n400\n\nFigure 1: Segmented electrocardiograms (ECG) with adaptive constant and linear intervals (top) and non-adaptive\npiecewise linear (bottom). Only 150 data points out of 600\nare shown. Both segmentations have the same model complexity, but we simultaneously reduced the fit and the leaveone-out cross-validation error with the adaptive model (top).\nTypically, in a time series segmentation, a single model\nis applied to all intervals. For example, all intervals are assumed to behave in a quasi-constant or quasi-linear manner.\nHowever, mixing different models and, in particular, constant and linear intervals can have two immediate benefits.\nFirstly, some applications need a qualitative description of\neach interval [48] indicated by change of model: is the tem-\n\n\fperature rising, dropping or is it stable? In an ECG, we need\nto identify the flat interval between each cardiac pulses. Secondly, as we will show, it can reduce the fit error without\nincreasing the cross-validation error. Intuitively, a piecewise\nmodel tells when the data is increasing, and at what rate, and\nvice versa. While most time series have clearly identifiable\nlinear trends some of the time, this is not true over all time\nintervals. Therefore, the piecewise linear model locally overfits the data by computing meaningless slopes (see Fig. 1).\nGlobal overfitting has been addressed by limiting the\nnumber of regressors [46], but this carries the implicit assumption that time series are somewhat stationary [38].\nSome frameworks [48] qualify the intervals where the slope\nis not significant as being \"constant\" while others look for\nconstant intervals within upward or downward intervals [10].\nPiecewise linear segmentation is ubiquitous and was one\nof the first applications of dynamic programming [7]. We\nargue that many applications can benefit from replacing it\nwith a mixed model (piecewise linear and constant). When\nidentifying constant intervals a posteriori from a piecewise\nlinear model, we risk misidentifying some patterns including\n\"stair cases\" or \"steps\" (see Fig. 1). A contribution of this\npaper is experimental evidence that we reduce fit without\nsacrificing the cross-validation error or running time for a\ngiven model complexity by using adaptive algorithms where\nsome intervals have a constant model whereas others have a\nlinear model. The new heuristic we propose is neither more\ndifficult to implement nor more expensive computationally.\nOur experiments include white noise and random walks as\nwell as ECGs and stock market prices. We also compare\nagainst the dynamic programming (optimal) solution which\nwe show can be computed in time O(n2 k).\nPerformance-wise, common heuristics (piecewise linear or constant) have been reported to require quadratic\ntime [27]. We want fast linear time algorithms. When the\nnumber of desired segments is small, top-down heuristics\nmight be the only sensible option. We show that if we allow the one-time linear time computation of a buffer, adaptive (and non-adaptive) top-down heuristics run in linear time\n(O(n)).\nData mining requires high scalability over very large\ndata sets. Implementation issues, including numerical stability, must be considered. In this paper, we present algorithms\nthat can process millions of data points in minutes, not hours.\n2 Related Work\nTable 1 summarizes the various common heuristics and algorithms used to solve the segmentation problem with polynomial models while minimizing the Euclidean (l2 ) error.\nThe top-down heuristics are described in section 7. When\nthe number of data points n is much larger than the number of segments k (n \u226b k), the top-down heuristics is particularly competitive. Terzi and Tsaparas [42] achieved a\n\nTable 1: Complexity of various segmentation algorithms\nusing polynomial models with k segments and n data points,\nincluding the exact solution by dynamic programming.\nAlgorithm\nDynamic Programming\nTop-Down\nBottom-Up\nSliding Windows\n\nComplexity\nO(n2 k)\nO(nk)\nO(n log n) [39] or O(n2 /k) [27]\nO(n) [39]\n\ncomplexity of O(n4/3 k 5/3 ) for the piecewise constant model\nby running (n/k)2/3 dynamic programming routines and using weighted segmentations. The original dynamic programming solution proposed by Bellman [7] ran in time O(n3 k),\nand while it is known that a O(n2 k)-time implementation is\npossible for piecewise constant segmentation [42], we will\nshow in this paper that the same reduced complexity applies for piecewise linear and mixed models segmentations\nas well.\nExcept for Pednault who mixed linear and quadratic\nsegments [40], we know of no other attempt to segment\ntime series using polynomials of variable degrees in the\ndata mining and knowledge discovery literature though there\nis related work in the spline and statistical literature [19,\n35, 37] and machine learning literature [3, 5, 8]. The\nintroduction of \"flat\" intervals in a segmentation model has\nbeen addressed previously in the context of quasi-monotonic\nsegmentation [10] by identifying flat subintervals within\nincreasing or decreasing intervals, but without concern for\nthe cross-validation error.\nWhile we focus on segmentation, there are many\nmethods available for fitting models to continuous variables, such as a regression, regression/decision trees, Neural Networks [25], Wavelets [14], Adaptive Multivariate\nSplines [19], Free-Knot Splines [35], Hybrid Adaptive\nSplines [37], etc.\n3 Complexity Model\nOur complexity model is purposely simple. The model\ncomplexity of a segmentation is the sum of the number of\nregressors over each interval: a constant interval has a cost\nof 1, a linear interval a cost of 2 and so on. In other words,\na linear interval is as complex as two constant intervals.\nConceptually, regressors are real numbers whereas all other\nparameters describing the model only require a small number\nof bits.\nIn our implementation, each regressor counted uses\n64 bits (\"double\" data type in modern C or Java). There\nare two types of hidden parameters which we discard (see\nFig. 2): the width or location of the intervals and the number\n\n\fof regressors per interval. The number of regressors per interval is only a few bits and is not significant in all cases. The\nwidth of the intervals in number of data points can be represented using \u03ba\u2308log m\u2309 bits where m is the maximum length\nof a interval and \u03ba is the number of intervals: in the experimental cases we considered, \u2308log m\u2309 \u2264 8 which is small\ncompared to 64, the number of bits used to store each regressor counted. We should also consider that slopes typically\nneed to be stored using more accuracy (bits) than constant\nvalues. This last consideration is not merely theoretical since\na 32 bits implementation of our algorithms is possible for the\npiecewise constant model whereas, in practice, we require\n64 bits for the piecewise linear model (see proposition 5.1\nand discussion that follows). Experimentally, the piecewise\nlinear model can significantly outperform (by \u2248 50%) the\npiecewise constant model in accuracy (see Fig. 11) and vice\nversa. For the rest of this paper, we take the fairness of our\ncomplexity model as an axiom.\n\nc\nax + b\n\nm1\n\nax + b\n\nm2\n\nm3\n\n4 Time Series, Segmentation Error and Leave-One-Out\nTime\nseries\nare\nsequences\nof\ndata\npoints\n(x0 , y0 ), . . . , (xn\u22121 , yn\u22121 ) where the x values, the \"time\"\nvalues, are sorted: xi > xi\u22121 . In this paper, both the x\nand y values are real numbers. We define a segmentation\nas a sorted set of segmentation indexes z0 , . . . , z\u03ba such that\nz0 = 0 and z\u03ba = n. The segmentation points divide the time\nseries into intervals S1 , . . . , S\u03ba defined by the segmentation\nindexes as Sj = {(xi , yi )|zj\u22121 \u2264 i < zj } . Additionally,\neach interval S1 , . . . , S\u03ba has a model (constant, linear,\nupward monotonic, and so on).\nP\u03ba In this paper, the segmentation error is computed from\nj=1 Q(Sj ) where the function Q is the square of the l2 rePzj \u22121\n(p(xr ) \u2212\ngression error. Formally, Q(Sj ) = minp r=z\nj\u22121\nyr )2 where the minimum is over the polynomials p of a given\ndegree. For example,\nif the interval Sj is said to be constant,\nP\nthen Q(Sj ) = zj \u2264l\u2264zj+1 (yl \u2212 \u0233)2 where \u0233 is the average,\nP\n\u0233 = zj\u22121 \u2264l<zj zj+1yl\u2212zj . Similarly, if the interval has a linear model, then p(x) is chosen to be the linear polynomial\np(x) = ax + b where a and b are found by regression. The\nsegmentation error can be generalized to other norms, such\nas\nP the maximum-error (l\u221e ) norm [10, 32] by replacing the\noperators by max operators.\nqPWhen reporting experimental error, we use the l2 error\n\u03ba\nj=1 Q(Sj ). We only compare time series having a fixed\nnumber of data points,\nbut otherwise, the mean square error\nq\nP\u03ba\n\nQ(Sj )\n\nj=1\n.\nn\nFigure 2: To describe an adaptive segmentation, you need should be used:\nIf\nthe\ndata\nfollows\nthe\nmodel over each interval,\nthe length and regressors of each interval.\nthen the error is zero. For example, given the time series (0, 0), (1, 0), (2, 0), (3, 1), (4, 2), we get no error when\nThe desired total number of regressors depends on do- choosing the segmentation indexes z = 0, z = 2, z = 5\n0\n1\n2\nmain knowledge and the application: when processing ECG with a constant model over the index interval [0, 2) and a\ndata, whether we want to have two intervals per cardiac pulse linear model over the index interval [2, 5). However, the\nor 20 intervals depends on whether we are satisfied with choice of the best segmentation is not unique: we also get\nthe mere identification of the general location of the pulses no error by choosing the alternative segmentation indexes\nor whether we desire a finer analysis. In some instances, z = 0, z = 3, z = 5.\n0\n1\n2\nthe user has no guiding principles or domain knowledge\nThere are two types of segmentation problem:\nfrom which to choose the number of intervals and a model\n\u2022 given a bound on the model complexity, find the segmenselection algorithm is needed. Common model selection\ntation minimizing the segmentation error;\napproaches such as Bayesian Information Criterion (BIC),\nMinimum Description Length (MDL) and Akaike Informa- \u2022 given a bound on the segmentation error, find a segmentation minimizing the model complexity.\ntion Criterion (AIC) suffer because the possible model complexity p is large in a segmentation problem (p = n) [11]. If we can solve efficiently and incrementally one problem\nMore conservative model selection approaches such as Risk type, then the second problem type is indirectly solved.\nInflation Criterion [17] or Shrinkage [14] do not directly Because it is intuitively easier to suggest a reasonable bound\napply because they assume wavelet-like regressors. Cross- on the model complexity, we focus on the first problem type.\nvalidation [18], generalized cross-validation [12], and leaveFor applications such as queries by humming [49], it\none-out cross-validation [45] methods are too expensive. is useful to bound the distance between two time series\nHowever, stepwise regression analysis [9] techniques such using only the segmentation data (segmentation points and\nas permutation tests (\"pete\") are far more practical [46]. In polynomials over each interval). Let k * k be any norm in\nthis paper, we assume that the model complexity is known a Banach space, including the Euclidean distance. Given\neither as an input from the user or through model selection.\n\n\ftime series y, y \u2032 , let s(y), s(y \u2032 ) be the piecewise polynomial\napproximations corresponding to the segmentation, then by\nthe triangle inequality ks(y)\u2212s(y \u2032 )k\u2212ks(y)\u2212yk\u2212ks(y \u2032)\u2212\ny \u2032 k \u2264 ky \u2212y \u2032 k \u2264 ks(y)\u2212s(y \u2032 )k+ks(y)\u2212yk+ks(y \u2032)\u2212y \u2032 k.\nHence, as long as the approximation errors are small, ks(y)\u2212\nyk < \u01eb and ks(y \u2032 ) \u2212 y \u2032 k < \u01eb, then we have that nearby\nsegmentations imply nearby time series (ks(y) \u2212 s(y \u2032 )k <\n\u01eb \u21d2 ky \u2212 y \u2032 k < 3\u01eb) and nearby time series imply nearby\nsegmentations (ky \u2212 y \u2032 k < \u01eb \u21d2 ks(y) \u2212 s(y \u2032 )k < 3\u01eb). This\nresult is entirely general.\nMinimizing the fit error is important. On the one\nhand, if we do not assume that the approximation errors are\nsmall, it is possible for the segmentation data to be identical ks(y) \u2212 s(y \u2032 )k = 0 while the distance between the time\nseries, ky \u2212 y \u2032 k, is large, causing false positives when identifying patterns from the segmentation data. For example, the\nsequences 100, \u2212100 and \u2212100, 100 can be both approximated by the same flat model (0, 0), yet they are far apart.\nOn the other hand, if the fit error is large, similar time series\ncan have different segmentations, thus causing false negatives. For example, the sequences \u2212100, 100, \u2212100.1 and\n\u2212100.1, 100, \u2212100 have the piecewise flat model approximations 0, 0, \u2212100.1 and \u2212100.1, 0, 0 respectively.\nBeside the data fit error, another interesting form of\nerror is obtained by cross-validation: divide your data points\ninto two sets (training and test), and measure how well your\nmodel, as fitted over the training set, predicts the test set. We\npredict a missing data point (xi , yi ) by first determining the\ninterval [zj\u22121 , zj ) corresponding to the data point (xzj1 <\nxi < xzj ) and then we compute p(xi ) where p is the\nregression polynomial over Sj . The error is |p(xi ) \u2212 yi |.\nWe opt for the leave-one-out cross-validation where the test\nset is a single data point and the training set is the remainder.\nWe repeat the cross-validation over all possible missing data\npoints, except for the first and last data point in the time\nseries, and compute the mean square error. If computing the\nsegmentation takes linear time, then computing the leaveone-out error in this manner takes quadratic time, which is\nprohibitive for long time series.\nNaturally, beyond the cross-validation and fit errors, a\nsegmentation should provide the models required by the\napplication. A rule-based system might require to know\nwhere the data does not significantly increase or decrease\nand if flat intervals have not been labelled, such queries are\nhard to support elegantly.\n\ninto quasi-polynomial intervals in optimal time, we must\ncompute fit errors in constant time (O(1)).\nP ROPOSITION 5.1. Given a time series {(xi , yi )}i=1,...,n , if\nwe allow the one-time O(n) computation of a prefix buffer,\nfinding the best polynomial fitting the data over the interval\n[xp , xq ] is O(1). This is true whether we use the Euclidean\ndistance (l2 ) or higher order norms (lr for \u221e > r > 2).\nProof. We prove the result using the Euclidean (l2 ) norm,\nthe proof is similar for higher order norms.\nWe begin by showing that polynomial regression can be\nreduced\nPN \u22121 to ja matrix inversion problem. Given a polynomial\nPq\nj=0 aj x , the square of the Euclidean error is\ni=p (yi \u2212\nPN \u22121\nj 2\nj=0 aj xi ) . Setting the derivative with respect to al\nto zero for l = 0, . . . , N \u2212 1,PgeneratesPa system of\nq\nN \u22121\nj+l\n=\nN equations and N unknowns,\ni=p xi\nj=0 aj\nPq\nl\ny\nx\nwhere\nl\n=\n0,\n.\n.\n.\n,\nN\n\u2212\n1.\nOn\nthe\nright-handi=p i i\nPq\nl\nside, we have a N dimensional vector (Vl =\ni=p yi xi )\nwhereas on the P\nleft-hand-side, we have the N \u00d7 N Toeplitz\nq\ni+l\nmatrix Al,i =\nmultiplied by the coefficients of\ni=p xi\nthe polynomial (a0 , . . . , aN \u22121 ). That is, we have the matrixPN \u22121\nvector equation i=0 Al,i ai = Vl .\nAs long as N \u2265 q \u2212 p, the matrix A is invertible. When\nN < q \u2212 p, the solution is given by setting N = q \u2212 p and\nletting ai = 0 for i > q \u2212 p. Overall, when N is bounded\na priori by a small integer, no expensive numerical analysis\nis needed. Only computing the matrix A and the vector V is\npotentially expensive because they involve summations over\na large number of terms.\nOnce the coefficients a0 , . . . , aN \u22121 are known, we compute the fit error using the formula:\nq\nX\ni=p\n\n\uf8eb\n\uf8ed\n\nN\n\u22121\nX\nj=0\n\n\uf8f62\n\naj xji \u2212 yi \uf8f8\n\n=\n\nN\n\u22121 N\n\u22121\nX\nX\n\naj al\n\nj=0 l=0\n\n\u2212 2\n\nN\n\u22121\nX\nj=0\n\naj\n\nq\nX\n\nxj+l\ni\n\ni=p\n\nq\nX\ni=p\n\nxji yi +\n\nq\nX\n\nyi2 .\n\ni=p\n\nAgain, only the summations are potentially expensive.\nHence, computing the best polynomial fitting some data\npoints over a specific range and computing the corresponding\nfit error in constant\nPqtime is equivalent to computing range\nsums of the form i=p xii yil in constant time for 0 \u2264 i, l \u2264\n2N . To do so, simply compute once all prefix sums Pqj,l =\nPq\n5 Polynomial Fitting in Constant Time\nj l\ni=0 xi yi and then use their subtractions to compute range\nThe naive fit error computation over a given interval takes queries Pq xj y l = P j,l \u2212 P j,l .\nq\np\u22121\ni=p i i\nlinear time\nP O(n): solve 2for the polynomial p and then\ncompute\ni (yi \u2212 p(xi )) . This has lead other authors\nPrefix sums speed up the computation of the range sums\nto conclude that top-down segmentation algorithm such as (making them constant time) at the expense of update time\nDouglas-Peucker's require quadratic time [27] while we will and storage: if one of the data point changes, we may have to\nshow they can run in linear time. To segment a time series recompute the entire prefix sum. More scalable algorithms\n\n\fTable 2: Accuracy of the polynomial fitting in constant time\nusing 32 bits and 64 bits floating point numbers respectively.\nWe give the worse percentage of error over 1000 runs using\nuniformly distributed white noise (n = 200). The domain\nranges from x = 0 to x = 199 and we compute the fit error\nover the interval [180, 185).\n\nN = 1 (y = b)\nN = 2 (y = ax + b)\nN = 3 (y = ax2 + bx + c)\n\n32 bits\n7 \u00d7 10\u22123 %\n5%\n240%\n\n64 bits\n1 \u00d7 10\u221211 %\n6 \u00d7 10\u22129 %\n3 \u00d7 10\u22123 %\n\nare possible if the time series are dynamic [31]. Computing\nthe needed prefix sums is only done once in linear time and\nrequires (N 2 + N + 1)n units of storage (6n units when\nN = 2). For most practical purposes, we argue that we will\nsoon have infinite storage so that trading storage for speed is\na good choice. It is also possible to use less storage [33].\nWhen using floating point values, the prefix sum approach causes a loss in numerical accuracy which becomes\nsignificant if x or y values grow large and N > 2 (see Table 2). When N = 1 (constant polynomials), 32 bits floating\npoint numbers are sufficient, but for N \u2265 2, 64 bits is required. In this paper, we are not interested in higher order\npolynomials and choosing N = 2 is sufficient.\n6 Optimal Adaptive Segmentation\n\n(see Algorithm 1). Once we have computed the r \u00d7 n + 1\nmatrix, we reconstruct the optimal solution with a simple\nO(k) algorithm (see Algorithm 2) using matrices D and P\nstoring respectively the best segmentation points and the best\ndegrees.\nAlgorithm 1 First part of dynamic programming algorithm\nfor optimal adaptive segmentation of time series into intervals having degree 0, . . . , N \u2212 1.\n1: INPUT: Time Series (xi , yi ) of length n\n2: INPUT: Model Complexity k and maximum degree N\n(N = 2 \u21d2 constant and linear)\n3: INPUT: Function E(p, q, d) computing fit error with\npoly. of degree d in range [xp , xq ) (constant time)\n4: R, D, P \u2190 k \u00d7 n + 1 matrices (initialized at 0)\n5: for r \u2208 {0, . . . , k \u2212 1} do\n6:\n{r scans the rows of the matrices}\n7:\nfor q \u2208 {0, . . . , n} do\n8:\n{q scans the columns of the matrices}\n9:\nFind a minimum of Rr\u22121\u2212d,p +E(p, q, d) and store\nits value in Rr,q , and the corresponding d, p tuple\nin Dr,q , Pr,q for 0 \u2264 d \u2264 min(r + 1, N ) and\n0 \u2264 p \u2264 q + 1 with the convention that R is \u221e\non negative rows except for R\u22121,0 = 0.\n10: RETURN cost matrix R, degree matrix D, segmentation points matrix P\n\nAlgorithm 2 Second part of dynamic programming algorithm for optimal adaptive segmentation.\n1: INPUT: k \u00d7 n + 1 matrices R, D, P from dynamic\nprogramming algo.\n2: x \u2190 n\n3: s \u2190 empty list\n4: while r \u2265 0 do\n5:\np \u2190 Pr,x\n6:\nd \u2190 Dr,x\n7:\nr \u2190r\u2212d+1\n8:\nappend interval from p to x having degree d to s\n9:\nx\u2190p\n10: RETURN optimal segmentation s\n\nAn algorithm is optimal, if it can find a segmentation with\nminimal error given a model complexity k. Since we can\ncompute best fit error in constant time for arbitrary polynomials, a dynamic programming algorithm computes the optimal adaptive segmentation in time O(n2 N k) where N is\nthe upper bound on the polynomial degrees. Unfortunately,\nif N \u2265 2, this result does not hold in practice with 32 bits\nfloating point numbers (see Table 2).\nWe improve over the classical approach [7] because we\nallow the polynomial degree of each interval to vary. In the\ntradition of dynamic programming [30, pages 261\u2013265], in a\nfirst stage, we compute the optimal cost matrix (R): Rr,p is\nthe minimal segmentation cost of the time interval [x0 , xp )\nusing a model complexity of r. If E(p, q, d) is the fit error\nof a polynomial of degree d over the time interval [xp , xq ), 7 Piecewise Linear or Constant Top-Down Heuristics\ncomputable in time O(1) by proposition 5.1, then\nComputing optimal segmentations with dynamic programming is \u03a9(n2 ) which is not practical when the size of the\nRr,q =\nmin\nRr\u22121\u2212d,p + E(p, q, d)\ntime series is large. Many efficient online approximate al0\u2264p\u2264q,0\u2264d<N\ngorithms are based on greedy strategies. Unfortunately, for\nwith the convention that Rr\u22121\u2212d,p is infinite when r \u2212 1 \u2212 small model complexities, popular heuristics run in quadratic\nd < 0 except for R\u22121,0 = 0. Because computing Rr,q time (O(n2 )) [27]. Nevertheless, when the desired model\nonly requires knowledge of the prior rows, Rr\u2032 ,* for r\u2032 < r, complexity is small, a particularly effective heuristic is the\nwe can compute R row-by-row starting with the first row top-down segmentation which proceeds as follows: starting\n\n\fwith a simple segmentation, we further segment the worst\ninterval, and so on, until we exhaust the budget. Keogh\net al. [27] state that this algorithm has been independently\ndiscovered in the seventies and is known by several name:\nDouglas-Peucker algorithm, Ramers algorithm, or Iterative\nEnd-Points Fits. In theory, Algorithm 3 computes the topdown segmentation, using polynomial regression of any degree, in time O(kn) where k is the model complexity, by\nusing fit error computation in constant time. In practice, our\nimplementation only works reliably for d = 0 or d = 1 using 64 bits floating point numbers. The piecewise constant\n(d = 0) and piecewise linear (d = 1) cases are referred to\nas the \"top-down constant\" and \"top-down linear\" heuristics\nrespectively.\nAlgorithm 3 Top-Down Heuristic.\nINPUT: Time Series (xi , yi ) of length n\nINPUT: Polynomial degree d (d = 0, d = 1, etc.) and\nmodel complexity k\nINPUT: Function E(p, q) computing fit error with poly.\nin range [xp , xq )\nS empty list\nS \u2190 (0, n, E(0, n))\nb\u2190k\u2212d\nwhile b \u2212 d \u2265 0 do\nfind tuple (i, j, \u01eb) in S with maximum last entry\nfind minimum of E(i, l) + E(l, j) for l = i + 1, . . . , j\nremove tuple (i, j, \u01eb) from S\ninsert tuples (i, l, E(i, l)) and (l, j, E(l, j)) in S\nb\u2190b\u2212d\nS contains the segmentation\n\n8 Adaptive Top-Down Segmentation\nOur linear time adaptive segmentation heuristic is based on\nthe observation that a linear interval can be replaced by two\nconstant intervals without model complexity increase. After\napplying the top-down linear heuristic from the previous\nsection (see Algorithm 3), we optimally subdivide each\ninterval once with intervals having fewer regressors (such\nas constant) but the same total model complexity. The\ncomputational complexity is the same, O((k + 1)n). The\nresult is Algorithm 4 as illustrated by Fig. 3. In practice, we\nfirst apply the top-down linear heuristic and then we seek to\nsplit the linear intervals into two constant intervals.\nBecause the algorithm only splits an interval if the fit\nerror can be reduced, it is guaranteed not to degrade the\nfit error. However, improving the fit error is not, in itself,\ndesirable unless we can also ensure we do not increase the\ncross-validation error.\nAn alternative strategy is to proceed from the top-down\nconstant heuristic and try to merge constant intervals into\nlinear intervals. We chose not to report our experiments with\n\nInitially, solve for piecewise\nlinear segmentation\n\nIntervals are further subdivided\ninto flat intervals.\n\nFigure 3: Adaptive Top-Down Segmentation: initially, we\ncompute a piecewise linear segmentation, then we further\nsubdivide some intervals into constant intervals.\n\nthis alternative since, over our data sets, it gives worse results\nand is slower than all other heuristics.\n9 Implementation and Testing\nUsing a Linux platform, we implemented our algorithms\nin C++ using GNU GCC 3.4 and flag \"-O2\". Intervals\nare stored in an STL list object. Source code is available\nfrom the author. Experiments run on a PC with an AMD\nAthlon 64 (2 GHZ) CPU and enough internal memory so\nthat no disk paging is observed.\nUsing ECG data and various number of data points, we\nbenchmark the optimal algorithm, using dynamic programming, against the adaptive top-down heuristic: Fig. 4 demonstrates that the quadratic time nature of the dynamic programming solution is quite prevalent (t \u2248 n2 /50000 seconds) making it unusable in all but toy cases, despite a C++\nimplementation: nearly a full year would be required to optimally segment a time series with 1 million data points! Even\nif we record only one data point every second for an hour, we\nstill generate 3,600 data points which would require about\n4 minutes to segment! Computing the leave-one-out error of\na quadratic time segmentation algorithm requires cubic time:\nto process the numerous time series we chose for this paper,\ndays of processing are required.\nWe observed empirically that the timings are not sensitive to the data source. The difference in execution time of\nthe various heuristics is negligible (under 15%): our implementation of the adaptive heuristic is not significantly more\nexpensive than the top-down linear heuristic because its additional step, where constant intervals are created out of lin-\n\n\fear ones, can be efficiently written as a simple sequential\nscan over the time series. To verify the scalability, we generated random walk time series of various length with fixed\nmodel complexity (k = 20), see Fig. 5.\n10 Random Time Series and Segmentation\n\n8\nadaptive top-down heuristic\noptimal\n\n7\n6\n5\ntime (s)\n\nAlgorithm 4 Adaptive Top-Down Heuristic.\nINPUT: Time Series (xi , yi ) of length n\nINPUT: Bound on Polynomial degree N and model complexity k\nINPUT: Function E(p, q, d) computing fit error with poly.\nin range [xp , xq )\nS empty list\nd\u2190N \u22121\nS \u2190 (0, n, d, E(0, n, d))\nb\u2190k\u2212d\nwhile b \u2212 d \u2265 0 do\nfind tuple (i, j, d, \u01eb) in S with maximum last entry\nfind minimum of E(i, l, d) + E(l, j, d) for l = i +\n1, . . . , j\nremove tuple (i, j, \u01eb) from S\ninsert tuples (i, l, d, E(i, l, d)) and (l, j, d, E(l, j, d)) in\nS\nb\u2190b\u2212d\nfor tuple (i, j, q, \u01eb) in S do\nfind minimum m of E(i, l, d\u2032 ) + E(l, j, q \u2212 d\u2032 \u2212 1) for\nl = i + 1, . . . , j and 0 \u2264 d\u2032 \u2264 q \u2212 1\nif m < \u01eb then\nremove tuple (i, j, q, \u01eb) from S\ninsert tuples (i, l, d\u2032 , E(i, l, d\u2032 )) and (l, j, q \u2212 d\u2032 \u2212\n1, E(l, j, q \u2212 d\u2032 \u2212 1)) in S\nS contains the segmentation\n\n4\n3\n2\n1\n0\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\nn\n\nFigure 4: Adaptive Segmentation Timings: Time in seconds\nversus the number of data points using ECG data. We\ncompare the optimal dynamic programming solution with\nthe top-down heuristic (k = 20).\n\nFigure 5: Heuristics timings using random walk data: Time\nin seconds versus the number of data points (k = 20).\n\nIntuitively, adaptive algorithms over purely random data are\nwasteful. To verify this intuition, we generated 10 sequences\nof Gaussian random noise (n = 200): each data point takes\non a value according to a normal distribution (\u03bc = 0, \u03c3 = 1).\nThe average leave-one-out error is presented in Table 3 (top)\ntop-down heuristics\noptimal\n14\nk=20\nwith model complexity k = 10, 20, 30. As expected, the\nk=30\n12\nadaptive heuristic shows a slightly worse cross-validation\n10\nerror. However, this is compensated by a slightly better fit\n8\nerror (5%) when compared with the top-down linear heuristic\n(Fig. 6). On this same figure, we also compare the dynamic\n6\nprogramming solution which shows a 10% reduction in fit\n4\nerror for all three models (adaptive, linear, constant/flat), for\n2\ntwice the running time (Fig. 4). The relative errors are not\n0\nsensitive to the model complexity.\nadapt. linear\nflat\nadapt. linear\nflat\nMany chaotic processes such as stock prices are sometimes described as random walk. Unlike white noise, the Figure 6: Average Euclidean (l2 norm) fit error over synvalue of a given data point depends on its neighbors. We gen- thetic white noise data.\nerated 10 sequences of Gaussian random walks (n = 200):\nstarting at the value 0, each data point takes on the value\n\n\ftop-down heuristics\n14\n\ntop-down heuristics\n\noptimal\n\n12\n\noptimal\n\n1.4\n\nk=20\nk=30\n\n1.2\n\n10\n\n1\n\n8\n\n0.8\n\n6\n\n0.6\n\n4\n\n0.4\n\n2\n\n0.2\n0\n\n0\nadapt. linear\n\nflat\n\nadapt. linear\n\nflat\n\nadapt. linear\n\nflat\n\nadapt. linear\n\nflat\n\nFigure 7: Average Euclidean (l2 norm) fit error over syn- Figure 8: Average Euclidean (l2 norm) fit error over 14 difthetic random walk data.\nferent stock prices, for each stock, the error was normalized\n(1 = optimal adaptive segmentation). The complexity is set\nat 20 (k = 20) but relative results are not sensitive to the\nTable 3: Leave-one-out cross-validation error for top-down complexity.\nheuristics on 10 sequences of Gaussian white noise (top)\nand random walks (bottom) (n = 200) for various model\ncomplexities (k)\nspite some controversy, technical analysis is a Data Mining\ntopic [44, 20, 26].\nwhite noise adaptive linear constant linear/adaptive\nWe segmented daily stock prices from dozens of compak = 10\n1.07\n1.05\n1.06\n98%\nnies [1]. Ignoring stock splits, we pick the first 200 trading\nk = 20\n1.21\n1.17\n1.14\n97%\ndays of each stock or index. The model complexity varies\nk = 30\n1.20\n1.17\n1.16\n98%\nk = 10, 20, 30 so that the number of intervals can range from\nrandom walks adaptive linear constant linear/adaptive 5 to 30. We compute the segmentation error using 3 topk = 10\n1.43\n1.51\n1.43\n106%\ndown heuristics: adaptive, linear and constant (see Table 4\nk = 20\n1.16\n1.21\n1.19\n104%\nfor some of the result). As expected, the adaptive heurisk = 30\n1.03\n1.06\n1.06\n103%\ntic is more accurate than the top-down linear heuristic (the\ngains are between 4% and 11%). The leave-one-out crossvalidation error is improved with the adaptive heuristic when\nthe model complexity is small. The relative fit error is not\nyi = yi\u22121 + N (0, 1) where N (0, 1) is a value from a normal sensitive to the model complexity. We observed similar redistribution (\u03bc = 0, \u03c3 = 1). The results are presented in Ta- sults using other stocks. These results are consistent with\nble 3 (bottom) and Fig. 7. The adaptive algorithm improves our synthetic random walks results. Using all of the historithe leave-one-out error (3%\u20136%) and the fit error (\u2248 13%) cal data available, we plot the 3 segmentations for Microsoft\nover the top-down linear heuristic. Again, the optimal al- stock prices (see Fig. 9). The line is the regression polynogorithms improve over the heuristics by approximately 10% mial over each interval and only 150 data points out of 5,029\nand the model complexity does not change the relative errors. are shown to avoid clutter. Fig. 8 shows the average fit error\nfor all 3 segmentation models: in order to average the results, we first normalized the errors of each stock so that the\n11 Stock Market Prices and Segmentation\nCreating, searching and identifying stock market patterns is optimal adaptive is 1.0. These results are consistent with the\nsometimes done using segmentation algorithms [48]. Keogh random walk results (see Fig. 7) and they indicate that the\nand Kasetty suggest [28] that stock market data is indistin- adaptive model is a better choice than the piecewise linear\nguishable from random walks. If so, the good results from model.\nthe previous section should carry over. However, the random\nwalk model has been strongly rejected using variance esti- 12 ECGs and Segmentation\nmators [36]. Moreover, Sornette [41] claims stock markets Electrocardiograms (ECGs) are records of the electrical voltage in the heart. They are one of the primary tool in screenare akin to physical systems and can be predicted.\nMany financial market experts look for patterns and ing and diagnosis of cardiovascular diseases. The resulting\ntrends in the historical stock market prices, and this approach time series are nearly periodic with several commonly idenis called \"technical analysis\" or \"charting\" [4, 6, 15]. If tifiable extrema per pulse including reference points P, Q, R,\nyou take into account \"structural breaks,\" some stock mar- S, and T (see Fig. 10). Each one of the extrema has some\nket prices have detectable locally stationary trends [13]. De- importance:\n\n\fTable 4: Euclidean segmentation error (l2 norm) and cross-validation error for k = 10, 20, 30: lower is better.\n\nGoogle\nSun Microsystems\nMicrosoft\nAdobe\nATI\nAutodesk\nConexant\nHyperion\nLogitech\nNVidia\nPalm\nRedHat\nRSA\nSandisk\nGoogle\nSun Microsystems\nMicrosoft\nAdobe\nATI\nAutodesk\nConexant\nHyperion\nLogitech\nNVidia\nPalm\nRedHat\nRSA\nSandisk\nGoogle\nSun Microsystems\nMicrosoft\nAdobe\nATI\nAutodesk\nConexant\nHyperion\nLogitech\nNVidia\nPalm\nRedHat\nRSA\nSandisk\n\nadaptative\n79.3\n23.1\n14.4\n15.3\n8.6\n9.8\n32.6\n39.0\n6.7\n13.4\n51.7\n125.2\n17.1\n13.6\n52.1\n13.9\n10.5\n8.5\n5.2\n6.5\n21.0\n26.0\n4.2\n9.1\n33.8\n77.7\n9.8\n9.0\n37.3\n11.7\n7.5\n6.2\n3.6\n4.7\n16.6\n18.9\n3.2\n6.9\n24.5\n58.1\n7.1\n6.5\n\nfit error\nlinear\n87.6\n26.5\n15.5\n16.4\n9.5\n10.9\n34.4\n41.4\n8.0\n15.2\n54.2\n147.3\n19.3\n15.6\n59.1\n16.5\n12.3\n9.4\n6.1\n7.2\n22.3\n29.6\n4.9\n10.7\n35.2\n88.2\n10.6\n10.6\n42.7\n13.2\n9.2\n6.8\n4.5\n5.5\n17.6\n21.5\n3.7\n8.6\n25.9\n65.2\n8.7\n7.6\n\nconstant\n88.1\n21.7\n15.5\n14.7\n8.1\n10.4\n32.6\n38.9\n6.3\n12.4\n48.2\n145.3\n15.1\n12.1\n52.2\n13.8\n11.1\n8.3\n5.1\n6.2\n21.8\n27.7\n4.2\n9.1\n31.8\n82.8\n10.9\n8.5\n39.5\n11.6\n8.4\n6.2\n3.7\n4.9\n15.9\n18.4\n3.1\n6.9\n22.5\n58.3\n7.3\n6.4\n\nlinear/adaptive\n110%\n115%\n108%\n107%\n110%\n111%\n106%\n106%\n119%\n113%\n105%\n118%\n113%\n115%\n113%\n119%\n117%\n111%\n117%\n111%\n106%\n114%\n117%\n118%\n104%\n114%\n108%\n118%\n114%\n113%\n123%\n110%\n125%\n117%\n106%\n114%\n116%\n125%\n106%\n112%\n123%\n117%\n\nadaptative\n2.6\n1.5\n1.1\n1.1\n0.8\n0.9\n1.7\n1.9\n0.7\n1.0\n1.9\n3.7\n1.3\n1.1\n2.3\n1.4\n1.0\n1.1\n0.7\n0.8\n1.4\n1.8\n0.7\n0.9\n1.9\n3.6\n1.2\n1.0\n2.2\n1.4\n1.0\n1.0\n0.7\n0.8\n1.4\n1.7\n0.7\n0.9\n1.8\n3.8\n1.2\n1.0\n\nleave one out error\nlinear\n2.7\n1.5\n1.1\n1.1\n0.9\n0.9\n1.7\n2.0\n0.8\n1.1\n2.0\n3.9\n1.3\n1.1\n2.4\n1.4\n1.1\n1.0\n0.7\n0.8\n1.4\n1.8\n0.7\n1.0\n1.9\n3.6\n1.1\n1.0\n2.2\n1.4\n1.0\n0.9\n0.7\n0.7\n1.4\n1.7\n0.6\n0.9\n1.8\n3.8\n1.2\n1.0\n\nconstant\n2.8\n1.4\n1.1\n1.2\n0.8\n0.9\n1.7\n1.7\n0.7\n1.0\n2.0\n3.7\n1.3\n1.0\n2.4\n1.4\n1.0\n1.1\n0.7\n0.8\n1.5\n1.7\n0.7\n1.0\n1.8\n3.5\n1.2\n0.9\n2.3\n1.4\n1.0\n1.0\n0.7\n0.8\n1.4\n1.6\n0.7\n0.9\n1.8\n3.6\n1.2\n0.9\n\nlinear/adaptive\n104%\n100%\n100%\n100%\n113%\n100%\n100%\n105%\n114%\n110%\n105%\n105%\n100%\n100%\n104%\n100%\n110%\n91%\n100%\n100%\n100%\n100%\n100%\n111%\n100%\n100%\n92%\n100%\n100%\n100%\n100%\n90%\n100%\n87%\n100%\n100%\n86%\n100%\n100%\n100%\n100%\n100%\n\n\f160\n\n160\n\n160\n\nadaptive (flat,linear)\n\npiecewise linear\n\npiecewise flat\n\n140\n\n140\n\n140\n\n120\n\n120\n\n120\n\n100\n\n100\n\n100\n\n80\n\n80\n\n80\n\n60\n\n60\n\n60\n\n40\n\n40\n\n40\n\n20\n\n20\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n20\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\nFigure 9: Microsoft historical daily stock prices (close) segmented by the adaptive top-down (top), linear top-down (middle),\nand constant top-down (bottom) heuristics. For clarity, only 150 data points out of 5,029 are shown.\nR\n\ntop-down heuristics\n90\n\nvoltage\n\nk=20\nk=30\n\n80\n70\n\nT\n\nP\n\noptimal\n\n60\n50\n\nQ\n\n40\n\nS\ntime\n\n30\n20\n\nFigure 10: A typical ECG pulse with PQRST reference\npoints.\n\n10\n0\nadapt. linear\n\nflat\n\nadapt. linear\n\nflat\n\nFigure 11: Average Euclidean (l2 norm) fit error over ECGs\n\u2022 a missing P extrema may indicate arrhythmia (abnormal for 6 different patients.\nheart rhythms);\n\u2022 a large Q value may be a sign of scarring;\nthe random walk and stock market data, the piecewise con\u2022 the somewhat flat region between the S and T points is\nstant model is no longer competitive in this case. The opcalled the ST segment and its level is an indicator of\ntimal solution, in this case, is far more competitive with an\nischemia [34].\nimprovement of approximately 30% of the heuristics, but the\nECG segmentation models, including the piecewise linear relative results are the same.\nmodel [43, 47], are used for compression, monitoring or\ndiagnosis.\nWe use ECG samples from the MIT-BIH Arrhythmia\nDatabase [22]. The signals are recorded at a sampling\nrate of 360 samples per second with 11 bits resolution.\nPrior to segmentation, we choose time intervals spanning\n300 samples (nearly 1 second) centered around the QRS\ncomplex. We select 5 such intervals by a moving window\nin the files of 6 different patients (\"100.dat\", \"101.dat\",\n\"102.dat\", \"103.dat\", \"104.dat\", \"105.dat\"). The model\ncomplexity varies k = 10, 20, 30.\nThe segmentation error as well as the leave-one-out error are given in Table 5 for each patient and they are plotted\nin Fig. 11 in aggregated form, including the optimal errors.\nWith the same model complexity, the adaptive top-down\nheuristic is better than the linear top-down heuristic (>5%),\nbut more importantly, we reduce the leave-one-out crossvalidation error as well for small model complexities. As\nthe model complexity increases, the adaptive model eventually has a slightly worse cross-validation error. Unlike for\n\n13 Conclusion and Future Work\nWe argue that if one requires a multimodel segmentation\nincluding flat and linear intervals, it is better to segment\naccordingly instead of post-processing a piecewise linear\nsegmentation. Mixing drastically different interval models\n(monotonic and linear) and offering richer, more flexible\nsegmentation models remains an important open problem.\nTo ease comparisons accross different models, we propose a simple complexity model based on counting the number of regressors. As supporting evidence that mixed models are competitive, we consistently improved the accuracy\nby 5% and 13% respectively without increasing the crossvalidation error over white noise and random walk data.\nMoreover, whether we consider stock market prices of ECG\ndata, for small model complexity, the adaptive top-down\nheuristic is noticeably better than the commonly used topdown linear heuristic. The adaptive segmentation heuristic\nis not significantly harder to implement nor slower than the\ntop-down linear heuristic.\n\n\fWe proved that optimal adaptive time series segmentations can be computed in quadratic time, when the model\ncomplexity and the polynomial degree are small. However,\nTable 5: Comparison of top-down heuristics on ECG data despite this low complexity, optimal segmentation by dy(n = 200) for various model complexities: segmentation namic programming is not an option for real-world time series (see Fig. 4). With reason, some researchers go as far\nerror and leave-one-out cross-validation error.\nas not even discussing dynamic programming as an alternative [27]. In turn, we have shown that adaptive top-down\nFit error for k = 10, 20, 30.\npatient\nadaptive linear constant linear/adaptive\nheuristics can be implemented in linear time after the linear\n100\n99.0\n110.0\n116.2\n111%\ntime computation of a buffer. In our experiments, for a small\n101\n142.2\n185.4\n148.7\n130%\nmodel complexity, the top-down heuristics are competitive\n102\n87.6\n114.7\n99.9\n131%\nwith the dynamic programming alternative which sometimes\n103\n215.5\n300.3\n252.0\n139%\noffer small gains (10%).\n104\n124.8\n153.1\n170.2\n123%\nFuture work will investigate real-time processing for\n105\n178.5\n252.1\n195.3\n141%\nonline applications such as high frequency trading [48] and\naverage\n141.3\n185.9\n163.7\n132%\nlive patient monitoring. An \"amnesic\" approach should be\n100\n46.8\n53.1\n53.3\n113%\ntested [39].\n101\n55.0\n65.3\n69.6\n119%\n102\n103\n104\n105\naverage\n100\n101\n102\n103\n104\n105\naverage\npatient\n100\n101\n102\n103\n104\n105\naverage\n100\n101\n102\n103\n104\n105\naverage\n100\n101\n102\n103\n104\n105\naverage\n\n42.2\n48.0\n50.2\n114%\n88.1\n94.4\n131.3\n107%\n53.4\n53.4\n84.1\n100%\n52.4\n61.7\n97.4\n118%\n56.3\n62.6\n81.0\n111%\n33.5\n34.6\n34.8\n103%\n32.5\n33.6\n40.8\n103%\n30.0\n32.4\n35.3\n108%\n59.8\n63.7\n66.5\n107%\n29.9\n30.3\n48.0\n101%\n35.6\n37.7\n60.2\n106%\n36.9\n38.7\n47.6\n105%\nLeave-one-out error for k = 10, 20, 30.\nadaptive linear constant linear/adaptive\n3.2\n3.3\n3.7\n103%\n3.8\n4.5\n4.3\n118%\n4.0\n4.1\n3.5\n102%\n4.6\n5.7\n5.5\n124%\n4.3\n4.1\n4.3\n95%\n3.6\n4.2\n4.5\n117%\n3.9\n4.3\n4.3\n110%\n2.8\n2.8\n3.5\n100%\n3.3\n3.3\n3.6\n100%\n3.3\n3.0\n3.4\n91%\n2.9\n3.1\n4.7\n107%\n3.8\n3.8\n3.6\n100%\n2.4\n2.5\n3.6\n104%\n3.1\n3.1\n3.7\n100%\n2.8\n2.2\n3.3\n79%\n2.9\n2.9\n3.6\n100%\n3.3\n2.9\n3.3\n88%\n3.7\n3.1\n4.4\n84%\n3.2\n3.2\n3.5\n100%\n2.1\n2.1\n3.4\n100%\n3.0\n2.7\n3.6\n90%\n\n14 Acknowledgments\nThe author wishes to thank Owen Kaser of UNB, Martin\nBrooks of NRC, and Will Fitzgerald of NASA Ames for their\ninsightful comments.\nReferences\n[1] Yahoo! Finance. last accessed June, 2006.\n[2] S. Abiteboul, R. Agrawal, et al. The Lowell Database\nResearch Self Assessment. Technical report, Microsoft,\n2003.\n[3] D. M. Allen. The Relationship between Variable Selection\nand Data Agumentation and a Method for Prediction. Technometrics, 16(1):125\u2013127, 1974.\n[4] S. Anand, W.-N. Chin, and S.-C. Khoo. Charting patterns on\nprice history. In ICFP'01, pages 134\u2013145, New York, NY,\nUSA, 2001. ACM Press.\n[5] C. G. F. Atkeson, A. W. F. Moore, and S. F. Schaal. Locally\nWeighted Learning. Artificial Intelligence Review, 11(1):11\u2013\n73, 1997.\n[6] R. Balvers, Y. Wu, and E. Gilliland. Mean reversion across\nnational stock markets and parametric contrarian investment\nstrategies. Journal of Finance, 55:745\u2013772, 2000.\n[7] R. Bellman and R. Roth. Curve fitting by segmented straight\nlines. J. Am. Stat. Assoc., 64:1079\u20131084, 1969.\n[8] M. Birattari, G. Bontempi, and H. Bersini. Lazy learning for\nmodeling and control design. Int. J. of Control, 72:643\u2013658,\n1999.\n[9] H. J. Breaux. A modification of Efroymson's technique for\nstepwise regression analysis. Commun. ACM, 11(8):556\u2013\n558, 1968.\n[10] M. Brooks, Y. Yan, and D. Lemire. Scale-based monotonicity\nanalysis in qualitative modelling with flat segments. In\nIJCAI'05, 2005.\n[11] K. P. Burnham and D. R. Anderson. Multimodel inference:\nunderstanding aic and bic in model selection. In Amsterdam\nWorkshop on Model Selection, 2004.\n\n\f[12] S. Chardon, B. Vozel, and K. Chehdi. Parametric blur estimation using the generalized cross-validation. Multidimensional\nSyst. Signal Process., 10(4):395\u2013414, 1999.\n[13] K. Chaudhuri and Y. Wu. Random walk versus breaking trend\nin stock prices: Evidence from emerging markets. Journal of\nBanking & Finance, 27:575\u2013592, 2003.\n[14] D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation\nby wavelet shrinkage. Biometrika, 81:425\u2013455, 1994.\n[15] E. F. Fama and K. R. French. Permanent and temporary\ncomponents of stock prices. Journal of Political Economy,\n96:246\u2013273, 1988.\n[16] W. Fitzgerald, D. Lemire, and M. Brooks. Quasi-monotonic\nsegmentation of state variable behavior for reactive control.\nIn AAAI'05, 2005.\n[17] D. P. Foster and E. I. George. The risk inflation criterion\nfor multiple regression. Annals of Statistics, 22:1947\u20131975,\n1994.\n[18] H. Friedl and E. Stampfer. Encyclopedia of Environmetrics,\nchapter Cross-Validation. Wiley, 2002.\n[19] J. Friedman. Multivariate adaptive regression splines. Annals\nof Statistics, 19:1\u2013141, 1991.\n[20] T. Fu, F. Chung, R. Luk, and C. Ng. Financial Time Series\nIndexing Based on Low Resolution Clustering. ICDM-2004\nWorkshop on Temporal Data Mining, 2004.\n[21] D. G. Galati and M. A. Simaan. Automatic decomposition of\ntime series into step, ramp, and impulse primitives. Pattern\nRecognition, 39(11):2166\u20132174, November 2006.\n[22] A. L. Goldberger, L. A. N. Amaral, et al. PhysioBank,\nPhysioToolkit, and PhysioNet. Circulation, 101(23):215\u2013\n220, 2000.\n[23] N. Haiminen and A. Gionis. Unimodal segmentation of\nsequences. In ICDM'04, 2004.\n[24] J. Han, W. Gong, and Y. Yin. Mining segment-wise periodic\npatterns in time-related databases. In KDD'98, 1998.\n[25] T. Hastie, R. Tibshirani, and J. Friedman. Elements of\nStatistical Learning: Data Mining, Inference and Prediction.\nSpringer-Verlag, 2001.\n[26] J. Kamruzzaman, RA Sarker, and I. Ahmad. SVM based\nmodels for predicting foreign currency exchange rates. ICDM\n2003, pages 557\u2013560, 2003.\n[27] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani. An online\nalgorithm for segmenting time series. In ICDM'01, pages\n289\u2013296, Washington, DC, USA, 2001. IEEE Computer\nSociety.\n[28] E. J. Keogh and S. F. Kasetty. On the Need for Time\nSeries Data Mining Benchmarks: A Survey and Empirical\nDemonstration. Data Mining and Knowledge Discovery,\n7(4):349\u2013371, 2003.\n[29] E. J. Keogh and M. J. Pazzani. An enhanced representation\nof time series which allows fast and accurate classification,\nclustering and relevance feedback. In KDD'98, pages 239\u2013\n243, 1998.\n[30] J. Kleinberg and \u00c9. Tardos.\nAlgorithm design.\nPearson/Addison-Wesley, 2006.\n[31] D. Lemire. Wavelet-based relative prefix sum methods for\nrange sum queries in data cubes. In CASCON 2002. IBM,\nOctober 2002.\n\n[32] D. Lemire, M. Brooks, and Y. Yan. An optimal linear time\nalgorithm for quasi-monotonic segmentation. In ICDM'05,\n2005.\n[33] D. Lemire and O. Kaser. Hierarchical bin buffering: Online\nlocal moments for dynamic external memory arrays. submitted in February 2004.\n[34] D. Lemire, C. Pharand, et al. Wavelet time entropy, t wave\nmorphology and myocardial ischemia. IEEE Transactions in\nBiomedical Engineering, 47(7), July 2000.\n[35] M. Lindstrom. Penalized estimation of free-knot splines.\nJournal of Computational and Graphical Statistics, 1999.\n[36] A. W. Lo and A. C. MacKinlay. Stock Market Prices\nDo Not Follow Random Walks: Evidence from a Simple\nSpecification Test. The Review of Financial Studies, 1(1):41\u2013\n66, 1988.\n[37] Z. Luo and G. Wahba. Hybrid adaptive splines. Journal of\nthe American Statistical Association, 1997.\n[38] G. Monari and G. Dreyfus. Local Overfitting Control via\nLeverages. Neural Comp., 14(6):1481\u20131506, 2002.\n[39] T. Palpanas, T. Vlachos, E. Keogh, D. Gunopulos, and\nW. Truppel. Online amnesic approximation of streaming time\nseries. In ICDE 2004, 2004.\n[40] E. Pednault. Minimum length encoding and inductive inference. In G. Piatetsky-Shapiro and W. Frawley, editors,\nKnowledge Discovery in Databases. AAAI Press, 1991.\n[41] D. Sornette. Why Stock Markets Crash: Critical Events\nin Complex Financial Systems. Princeton University Press,\n2002.\n[42] E. Terzi and P. Tsaparas. Efficient algorithms for sequence\nsegmentation. In SDM'06, 2006.\n[43] I. Tomek. Two algorithms for piecewise linear continuous\napproximations of functions of one variable. IEEE Trans. on\nComputers, C-23:445\u2013448, April 1974.\n[44] P. Tse and J. Liu. Mining Associated Implication Networks:\nComputational Intermarket Analysis. ICDM 2002, pages\n689\u2013692, 2002.\n[45] K.i Tsuda, G. R\u00e4tsch, S. Mika, and K.-R. M\u00fcller. Learning\nto predict the leave-one-out error. In NIPS*2000 Workshop:\nCross-Validation, Bootstrap and Model Selection, 2000.\n[46] K. T. Vasko and H. T. T. Toivonen. Estimating the number\nof segments in time series data using permutation tests. In\nICDM'02, 2002.\n[47] H. J. L. M. Vullings, M. H. G. Verhaegen, and H. B. Verbruggen. ECG segmentation using time-warping. In IDA'97,\npages 275\u2013285, London, UK, 1997. Springer-Verlag.\n[48] H. Wu, B. Salzberg, and D. Zhang. Online event-driven\nsubsequence matching over financial data streams. In SIGMOD'04, pages 23\u201334, New York, NY, USA, 2004. ACM\nPress.\n[49] Y. Zhu and D. Shasha. Query by humming: a time series\ndatabase approach. Proc. of SIGMOD, 2003.\n\n\f"}