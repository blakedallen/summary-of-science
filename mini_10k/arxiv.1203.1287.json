{"id": "http://arxiv.org/abs/1203.1287v1", "guidislink": true, "updated": "2012-03-06T19:12:24Z", "updated_parsed": [2012, 3, 6, 19, 12, 24, 1, 66, 0], "published": "2012-03-06T19:12:24Z", "published_parsed": [2012, 3, 6, 19, 12, 24, 1, 66, 0], "title": "A Finite Population Model of Molecular Evolution: Theory and Computation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.4398%2C1203.4619%2C1203.2167%2C1203.2961%2C1203.6737%2C1203.5509%2C1203.3971%2C1203.3014%2C1203.3141%2C1203.4736%2C1203.1851%2C1203.1083%2C1203.2223%2C1203.4394%2C1203.3000%2C1203.1100%2C1203.0828%2C1203.4811%2C1203.1899%2C1203.6391%2C1203.2041%2C1203.3687%2C1203.2800%2C1203.0321%2C1203.4063%2C1203.2078%2C1203.1766%2C1203.4874%2C1203.5256%2C1203.1858%2C1203.6541%2C1203.6471%2C1203.0571%2C1203.1839%2C1203.4430%2C1203.5684%2C1203.6875%2C1203.0476%2C1203.1254%2C1203.2603%2C1203.3145%2C1203.3557%2C1203.1317%2C1203.6165%2C1203.0393%2C1203.6756%2C1203.0491%2C1203.1138%2C1203.0824%2C1203.2287%2C1203.5584%2C1203.2491%2C1203.3900%2C1203.3214%2C1203.5163%2C1203.5593%2C1203.1287%2C1203.5860%2C1203.1298%2C1203.2474%2C1203.1661%2C1203.2693%2C1203.6130%2C1203.4969%2C1203.6060%2C1203.2614%2C1203.2908%2C1203.3329%2C1203.3743%2C1203.3004%2C1203.0155%2C1203.0739%2C1203.1945%2C1203.2741%2C1203.4053%2C1203.2809%2C1203.1892%2C1203.1799%2C1203.2569%2C1203.6174%2C1203.5801%2C1203.5934%2C1203.2129%2C1203.6218%2C1203.6003%2C1203.3725%2C1203.6504%2C1203.0532%2C1203.3396%2C1203.1355%2C1203.2699%2C1203.2870%2C1203.6740%2C1203.6399%2C1203.5278%2C1203.4163%2C1203.4492%2C1203.0931%2C1203.4670%2C1203.5113%2C1203.3304&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Finite Population Model of Molecular Evolution: Theory and Computation"}, "summary": "This paper is concerned with the evolution of haploid organisms that\nreproduce asexually. In a seminal piece of work, Eigen and coauthors proposed\nthe quasispecies model in an attempt to understand such an evolutionary\nprocess. Their work has impacted antiviral treatment and vaccine design\nstrategies. Yet, predictions of the quasispecies model are at best viewed as a\nguideline, primarily because it assumes an infinite population size, whereas\nrealistic population sizes can be quite small. In this paper we consider a\npopulation genetics-based model aimed at understanding the evolution of such\norganisms with finite population sizes and present a rigorous study of the\nconvergence and computational issues that arise therein. Our first result is\nstructural and shows that, at any time during the evolution, as the population\nsize tends to infinity, the distribution of genomes predicted by our model\nconverges to that predicted by the quasispecies model. This justifies the\ncontinued use of the quasispecies model to derive guidelines for intervention.\nWhile the stationary state in the quasispecies model is readily obtained, due\nto the explosion of the state space in our model, exact computations are\nprohibitive. Our second set of results are computational in nature and address\nthis issue. We derive conditions on the parameters of evolution under which our\nstochastic model mixes rapidly. Further, for a class of widely used fitness\nlandscapes we give a fast deterministic algorithm which computes the stationary\ndistribution of our model. These computational tools are expected to serve as a\nframework for the modeling of strategies for the deployment of mutagenic drugs.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.4398%2C1203.4619%2C1203.2167%2C1203.2961%2C1203.6737%2C1203.5509%2C1203.3971%2C1203.3014%2C1203.3141%2C1203.4736%2C1203.1851%2C1203.1083%2C1203.2223%2C1203.4394%2C1203.3000%2C1203.1100%2C1203.0828%2C1203.4811%2C1203.1899%2C1203.6391%2C1203.2041%2C1203.3687%2C1203.2800%2C1203.0321%2C1203.4063%2C1203.2078%2C1203.1766%2C1203.4874%2C1203.5256%2C1203.1858%2C1203.6541%2C1203.6471%2C1203.0571%2C1203.1839%2C1203.4430%2C1203.5684%2C1203.6875%2C1203.0476%2C1203.1254%2C1203.2603%2C1203.3145%2C1203.3557%2C1203.1317%2C1203.6165%2C1203.0393%2C1203.6756%2C1203.0491%2C1203.1138%2C1203.0824%2C1203.2287%2C1203.5584%2C1203.2491%2C1203.3900%2C1203.3214%2C1203.5163%2C1203.5593%2C1203.1287%2C1203.5860%2C1203.1298%2C1203.2474%2C1203.1661%2C1203.2693%2C1203.6130%2C1203.4969%2C1203.6060%2C1203.2614%2C1203.2908%2C1203.3329%2C1203.3743%2C1203.3004%2C1203.0155%2C1203.0739%2C1203.1945%2C1203.2741%2C1203.4053%2C1203.2809%2C1203.1892%2C1203.1799%2C1203.2569%2C1203.6174%2C1203.5801%2C1203.5934%2C1203.2129%2C1203.6218%2C1203.6003%2C1203.3725%2C1203.6504%2C1203.0532%2C1203.3396%2C1203.1355%2C1203.2699%2C1203.2870%2C1203.6740%2C1203.6399%2C1203.5278%2C1203.4163%2C1203.4492%2C1203.0931%2C1203.4670%2C1203.5113%2C1203.3304&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper is concerned with the evolution of haploid organisms that\nreproduce asexually. In a seminal piece of work, Eigen and coauthors proposed\nthe quasispecies model in an attempt to understand such an evolutionary\nprocess. Their work has impacted antiviral treatment and vaccine design\nstrategies. Yet, predictions of the quasispecies model are at best viewed as a\nguideline, primarily because it assumes an infinite population size, whereas\nrealistic population sizes can be quite small. In this paper we consider a\npopulation genetics-based model aimed at understanding the evolution of such\norganisms with finite population sizes and present a rigorous study of the\nconvergence and computational issues that arise therein. Our first result is\nstructural and shows that, at any time during the evolution, as the population\nsize tends to infinity, the distribution of genomes predicted by our model\nconverges to that predicted by the quasispecies model. This justifies the\ncontinued use of the quasispecies model to derive guidelines for intervention.\nWhile the stationary state in the quasispecies model is readily obtained, due\nto the explosion of the state space in our model, exact computations are\nprohibitive. Our second set of results are computational in nature and address\nthis issue. We derive conditions on the parameters of evolution under which our\nstochastic model mixes rapidly. Further, for a class of widely used fitness\nlandscapes we give a fast deterministic algorithm which computes the stationary\ndistribution of our model. These computational tools are expected to serve as a\nframework for the modeling of strategies for the deployment of mutagenic drugs."}, "authors": ["Narendra M. Dixit", "Piyush Srivastava", "Nisheeth K. Vishnoi"], "author_detail": {"name": "Nisheeth K. Vishnoi"}, "author": "Nisheeth K. Vishnoi", "links": [{"href": "http://arxiv.org/abs/1203.1287v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1203.1287v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "q-bio.PE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "q-bio.PE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1203.1287v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1203.1287v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:1203.1287v1 [q-bio.PE] 6 Mar 2012\n\nA Finite Population Model of Molecular Evolution:\nTheory and Computation\nNarendra M. Dixit\n\nPiyush Srivastava\n\nDepartment of Chemical Engineering\nIndian Institute of Science\nBangalore-560 012, India\nEmail: narendra@chemeng.iisc.ernet.in\n\nComputer Science Division\nUniversity of California at Berkeley\nBerkeley - 94720, CA, USA\nEmail: piyushsr@eecs.berkeley.edu\n\nNisheeth K. Vishnoi\nMicrosoft Research\nBangalore - 560 025, India\nEmail: nisheeth.vishnoi@gmail.com\n\nAbstract\nThis paper is concerned with the evolution of haploid organisms that reproduce asexually. In a seminal piece of work, Eigen and coauthors proposed the quasispecies model in an attempt to understand\nsuch an evolutionary process. Their work has impacted antiviral treatment and vaccine design strategies. Yet, predictions of the quasispecies model are at best viewed as a guideline, primarily because it\nassumes an infinite population size, whereas realistic population sizes can be quite small. In this paper\nwe consider a population genetics-based model aimed at understanding the evolution of such organisms\nwith finite population sizes and present a rigorous study of the convergence and computational issues\nthat arise therein. Our first result is structural and shows that, at any time during the evolution, as the\npopulation size tends to infinity, the distribution of genomes predicted by our model converges to that\npredicted by the quasispecies model. This justifies the continued use of the quasispecies model to derive\nguidelines for intervention. While the stationary state in the quasispecies model is readily obtained, due\nto the explosion of the state space in our model, exact computations are prohibitive. Our second set\nof results are computational in nature and address this issue. We derive conditions on the parameters\nof evolution under which our stochastic model mixes rapidly. Further, for a class of widely used fitness landscapes we give a fast deterministic algorithm which computes the stationary distribution of our\nmodel. These computational tools are expected to serve as a framework for the modeling of strategies\nfor the deployment of mutagenic drugs.\nTopics: Molecular Evolution, Quasispecies Theory.\n\n\fContents\n1\n\nIntroduction\n\n1\n\n2\n\nPreliminaries and Definitions\n2.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2 The Quasispecies Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3 The Error Threshold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n2\n2\n3\n3\n\n3\n\nA Finite Population Model and Our Main Results\n3.1 A Finite Population (RSM) Model . . . . . . . . . . . . . . .\n3.2 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2.1 Convergence of the Quasispecies and the RSM Model\n3.2.2 Computational Results in the RSM Model . . . . . . .\n3.3 Overview of Our Technical Contributions . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n4\n4\n6\n6\n6\n7\n\nDiscussion and Future Perspectives\n4.1 Previous Work . . . . . . . . . .\n4.2 Applications of the RSM Model\n4.3 Critique of the RSM Model . . .\n4.4 Open Problems . . . . . . . . .\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n9\n. 9\n. 10\n. 10\n. 11\n\nFormal Statements of Main Results\n5.1 Preliminaries and Definitions . . . . . . . . . . . . . . . . . . . . . . . .\n5.2 Convergence to the Quasispecies Model . . . . . . . . . . . . . . . . . .\n5.3 Computational Results . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3.1 Mixing Time Bounds on the RSM Process . . . . . . . . . . . . .\n5.3.2 Computing the Stationary Distribution in the Class Invariant Case\n5.3.3 Markov Chain Monte Carlo Methods . . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n4\n\n5\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\n.\n.\n.\n.\n\nA Starting State and Transition Matrix of the RSM Markov Chain\nB Proofs Omitted from Section 5\nB.1 Proofs Omitted from Section 5.1 . . . . . . . .\nB.1.1 Relationships between Error Thresholds\nB.2 Proof of Theorem 5.6 . . . . . . . . . . . . . .\nB.3 Proof of Corollary 5.7 . . . . . . . . . . . . . .\nB.4 Proof of Theorem 5.8 . . . . . . . . . . . . . .\nB.4.1 A Coupling for the RSM Process . . .\nB.5 Proof of Theorem 5.9 . . . . . . . . . . . . . .\nB.6 Proof of Theorem 5.10 . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n12\n12\n14\n14\n14\n15\n15\n19\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n20\n20\n21\n22\n25\n26\n26\n31\n32\n\n\f1\n\nIntroduction\n\nThe rapid genomic evolution of viruses such as HIV has made the design of drugs and vaccines with lasting\nactivity one of the most difficult challenges of our time. A novel intervention strategy which potentially\noutplays viruses in this evolutionary game was suggested by the pioneering work of Eigen and coworkers\n[Eig71, EMS89]. Eigen and coworkers considered the asexual evolution of a haploid organism and found\nthat when the mutation (or evolutionary) rate was small, the organism survived as a collection of closely\nrelated yet distinct genomes together termed the quasispecies. Viral populations in infected individuals are\nknown to exist as such quasispecies [LA10]. Remarkably, this quasispecies model predicted that when the\nmutation rate increased beyond a critical value, called the error threshold, the collection of genomes in the\nquasispecies ceased to be closely related; in fact, a completely random collection of genomic sequences\nwas predicted to emerge. This transition with increasing mutation rate thus induced a severe loss of genetic\ninformation in the quasispecies and has been referred to as an error catastrophe. The generic antiviral drug\nribavirin has been shown to act as a mutagen\u2013an agent that induces mutations\u2013against poliovirus and trigger\na severe loss of viral infectivity in culture [CCA01]. This strategy of enhancing the viral mutation rate thus\nappears promising and particularly advantageous because it is unlikely to be susceptible to failure through\nviral evolution-driven development of drug resistance. Mutagenic drugs that attempt to induce an error\ncatastrophe are thus being explored as a potential antiviral strategy [CCA01, GPLL+ 05, ADL04], and one\nsuch drug for HIV is currently under clinical trials [MHH+ 11].\nThe success of mutagenic strategies relies on accurate estimates of the error threshold of the pathogens\nunder consideration. Notwithstanding the tremendous insights into viral evolution the quasispecies model\nprovides, important gaps remain between the quasispecies model and the realistic evolution of viruses and\nother haploid asexual organisms. First, whereas the model assumes an infinite population size and, hence,\nadopts a deterministic approach, real populations are often small enough to lend themselves to substantial\nstochastic effects. For instance, the effective population size of HIV-1 in infected individuals is about 103 \u2212\n106 [KAB06, BSSD11], which is thought to underlie the strongly stochastic nature of HIV-1 evolution.\nSecond, the model assumes a single-peak fitness landscape, where one genomic sequence is assumed to\nbe the fittest and all other genomes are equally less fit. Realistic fitness landscapes are far more complex\n[HMC+ 11]. There have been significant efforts in the last 30 years to close these gaps [Wil05]. While\nmore general landscapes have been successfully considered in the quasispecies case [SH06], a rigorous\ntreatment of the finite population case has remained elusive (see Section 4). Importantly, it still remains to\nbe established whether the insights gained from the quasispecies model, such as the occurrence of an error\ncatastrophe, translate to the more realistic, finite population case.\nHere, we consider a finite population model of the asexual evolution of a haploid organism. Following\nstandard population genetics-based descriptions [HC06], the model considers evolution in discrete, nonoverlapping generations. Within each generation, genomes undergo reproduction (R), selection (S), and\nmutation (M), yielding progeny genomes for the next generation. We analyze this RSM model formally and\nestablish the following key results. We show that in the infinite population limit, the expected structure of\nthe quasispecies predicted by the RSM model converges to that of the quasispecies model. Thus, insights\nfrom the quasispecies model may be translated to the finite population scenario. Indeed, we show further\nthat the error threshold predicted by the RSM model also converges in the infinite population limit to that\nof the quasispecies model. Finite population models, such as the RSM model, appropriately tuned to mimic\nspecific details, such as the fitness landscape, of the pathogens under consideration may thus be employed\nto obtain realistic estimates of the error threshold.\nUnlike the quasispecies model, where the quasispecies is identified readily using black-box eigenvector\n\n1\n\n\fdetermination algorithms, identifying the expected quasispecies of the RSM model is computationally prohibitive even for the smallest realistic genome and population sizes. Monte Carlo sampling techniques are\ntherefore often resorted to [Wil05, AB05, BKP+ 11, GD10, TBVD12]. Here, going beyond the ideas of the\nquasispecies model, we examine the mixing properties of the RSM model. We establish constraints on the\nmodel parameters under which the RSM model exhibits rapid mixing and therefore allows fast estimation\nof the expected quasispecies structure. Finally, we suggest an algorithm that uses the Markov Chain Monte\nCarlo paradigm to estimate the error threshold in the RSM model. Our study thus serves as a framework for\nelucidating quantitative guidelines for the modeling of intervention strategies that employ mutagenic drugs.\nThe paper is organized as follows. In Section 2 we briefly describe the quasispecies model and the notion\nof the error threshold. In Section 3 we setup the finite population RSM model, present our main results and\noutline the techniques employed. In Section 4 we discuss our results in the context of previous studies and\nhighlight open problems arising from work. Formal statements of our results are presented in Section 5.\nDetailed proofs are contained in the Appendix.\n\n2\n\nPreliminaries and Definitions\n\n2.1\n\nPreliminaries\n\nWe consider the evolution of a population of haploid organisms that reproduce asexually. In this evolutionary process the genome of each individual is modeled as a string of nucleotides. During reproduction, the\ngenome is copied with possible mutations, which can be insertions, deletions, or substitutions. In applications, such as the modeling of viral evolution, it is often convenient to neglect insertions, deletions, and\nsubstitutions other than transitions (A to G or C to T , and vice versa). Under this assumption, a genome\nmay, without loss of generality, be represented as a binary string. We thus represent a genome as an L-bit\nstring \u03c3 = (\u03c31 , \u03c32 , . . . , \u03c3L ), where \u03c3i \u2208 {0, 1}.\nThe fitness of a genome is then modeled in terms of its propensity to produce copies of itself. Specifically, the fitness of the genome \u03c3 is defined by the number of copies a\u03c3 of itself that it produces in one\nround of replication (also called one generation). However, during replication, each bit of each of the a\u03c3\noffsprings is copied incorrectly with probability \u03bc (called the error or mutation rate), thus potentially giving\nrise to an L-bit string different from \u03c3 . The fittest genome, also termed the master sequence, is without loss\nof generality assumed to be 0 = (0, . . . , 0) so that a0 > a\u03c3 for all \u03c3 6= 0.\nThe primary cause of the complexity and diversity in the evolution of such organisms is the variety\nof possible fitness landscapes, which a priori can be arbitrary functions from {0, 1}L to the set of nonnegative integers. Several special classes of fitness landscapes have been employed in the literature and\nwe list the important ones below. We will assume that a\u03c3 \u2265 1. The case a\u03c3 = 0 for some \u03c3 's has been\nused [WK93, TH07, GD10], and will be discussed in Section 4.\n1. (General) Here the only condition is that a\u03c3 \u2265 1.\n2. (Class Invariant [SH06, TH07, PMnD10, BSSD11]) In a class invariant landscape a\u03c3 depends only\non the Hamming weight of \u03c3 .\n3. (Single Peak [Eig71, NS89, PMnD10]) Here, we have a0 > 1 and a\u03c3 = 1 for all \u03c3 6= 0.\ndef\n\n4. (Multiplicative [TH07, WH96]) These are parametrized by a1 , . . . , aL \u2265 1 so that for a given \u03c3 , a\u03c3 =\n\u220fLi=1,\u03c3i =0 ai .\n\n2\n\n\fOther landscapes such as the simpler additive or linear landscapes and more complex correlated landscapes\nhave also been employed in the literature [BS93, Wie97, vNCM99].\n\n2.2\n\nThe Quasispecies Model\n\nEigen and coworkers [Eig71, EMS89] gave the following differential equations for the time-evolution of the\nfractional population of the genome \u03c3 at time t, denoted by x\u03c3 (t):\ndx\u03c3 (t)\n= \u2211 x\u03c4 (t)a\u03c4 Q\u03c4\u03c3 \u2212 x\u03c3 (t)\u0100(t) for all \u03c3 .\ndt\n\u03c4\ndef\n\nHere, Q\u03c3 \u03c4 = \u03bc dH (\u03c3 ,\u03c4) (1 \u2212 \u03bc)L\u2212dH (\u03c3 ,\u03c4) is the probability that \u03c3 mutates to \u03c4 and dH (\u03c3 , \u03c4) is the Hamming\ndef\n\ndistance between \u03c3 and \u03c4. \u0100(t) is the average fitness \u2211\u03c3 a\u03c3 x\u03c3 (t) at time t. Defining A\u03c3 \u03c4 = a\u03c3 when \u03c3 = \u03c4\ndef\n\nand 0 otherwise, they showed that the vector of stationary frequencies, v\u03c3\u03bc = limt\u2192\u221e x\u03c3 (t), is the dominant\ndef\n\nright eigenvector of the value matrix QA at mutation rate \u03bc such that kv\u03bc k1 = \u2211\u03c3 v\u03c3\u03bc = 1.1 The collection of\ngenomes determined by this dominant eigenvector, which marks the culmination of the evolutionary process,\nis called the quasispecies. It is important to note that the vector v\u03bc is independent of the starting population\ndistribution.\nWe will mostly be concerned with the discrete time version of the quasispecies model. In the discrete\ntime case, t = {0, 1, . . .}, denoting the fraction of genomes of type \u03c3 at time t by mt\u03c3 , Eigen's equations can\nbe written as:\n\u03c4\n\u03c3 def \u2211\u03c4 mt a\u03c4 Q\u03c4\u03c3\nmt+1\n=\n.\n(1)\n\u2211\u03c4 mt\u03c4 a\u03c4\nIn vector notation, given the fractional population mt at time t, the fractional population mt+1 at time t + 1\nis given by mt+1 = r(mt ), where the \u03c3 co-ordinate r\u03c3 of the vector valued function r is defined as\ndef\n\nr\u03c3 (x) =\n\n(QAx)\u03c3\nQAx\n\u2211\u03c4 a\u03c4 Q\u03c4\u03c3 x\u03c4\n=\nand thus, r(x) =\n.\n||Ax||1\n||Ax||1\n\u2211\u03c4 a\u03c4 x\u03c4\n\n(2)\n\nAgain, mt can be shown to converge to v\u03bc irrespective of the starting population distribution as t goes to\ninfinity. However, at any finite t, mt depends on the initial state m0 .\n\n2.3\n\nThe Error Threshold\n\nWith the single peak landscape, Eigen et al. observed empirically that there is a critical value \u03bcc \u2264 0.5\nfor the mutation rate \u03bc such that for \u03bc \u001c \u03bcc , the quasispecies is dominated by the master sequence, i.e.\nv0\u03bc \u2265 v\u03c3\u03bc \u2200\u03c3 , whereas when 0.5 \u2265 \u03bc > \u03bcc the quasispecies is dispersed approximately uniformly. The\ncritical mutation rate \u03bcc is called the error threshold because the uniform dispersal for \u03bc > \u03bcc implies a\nsevere loss of representation in the quasispecies of the genetic information encoded by the master sequence.\ndef\nEvidently, this dispersal also decreases the mean fitness, A = \u2211\u03c3 a\u03c3 v\u03c3\u03bc .\nWe note here that despite the notion of the error threshold being widely recognized, no consensus exists on its definition; see Wilke [Wil05]. Since, in most cases, v\u03bc will never become exactly the uniform\ndistribution on {0, 1}L , it is clear that the goal is to find the smallest \u03bc such that v\u03bc is close to the uniform\ndistribution on all genomes, i.e., the vector with every coordinate equal to 2\u2212L , which we denote by U. To\n1 Throughout\n\nthis paper, we will be dealing with vectors over {0, 1}L . Vectors will be typeset in boldface. The components of a\nvector x will be denoted either by x\u03c3 or by x\u03c3 for \u03c3 \u2208 {0, 1}L , based on convenience of notation in the context of use.\n\n3\n\n\fdefine the error threshold we also need a function that measures closeness: e.g. kv\u03bc \u2212 Uk1 , kv\u03bc \u2212 Uk\u221e or\nthe difference in Shannon entropies of v\u03bc and U, namely |H(v\u03bc ) \u2212 L|. Hence, for a given distance function\nd which measures closeness of v\u03bc and U, and a bound on closeness \u03b5 > 0, we define\ndef\n\n\u03bccd (\u03b5) = min{\u03bc \u2208 (0, 1) : d(v\u03bc , U) \u2264 \u03b5}.\nFirst note that at \u03bc = 1/2, the steady state vector v\u03bc is exactly U. Hence, \u03bccd (\u03b5) \u2264 1/2 for all d, \u03b5 > 0.\nSecond, note that changing the distance function d will change the error threshold quantitatively. Eigen\nand coworkers presented a heuristic argument that the error threshold should scale as 1/L for the single-peak\nmodel without any rigorous proofs of its existence and without mentioning any closeness function.\n\n3\n\nA Finite Population Model and Our Main Results\n\nIn this section, we describe at an informal level the salient features of our work, which comprises a finite\npopulation model to capture molecular evolution, and our theoretical and computational results associated\nwith it. We give a high-level technical overview of the methods used to prove our results in Section 3.3,\nwhile precise definitions and formal statements of our results appear in Section 5. Proofs have been moved\nto the Appendix due to considerations of space.\n\n3.1\n\nA Finite Population (RSM) Model\n\nWe consider the following stochastic discrete time finite population model of evolution which we call the\nRSM model. The parameters are the same as in the quasispecies model: the genome length L, the sequence\nspace {0, 1}L , a per bit mutation rate \u03bc and the fitness landscape {a\u03c3 }\u03c3 \u2208{0,1}L with all a\u03c3 \u2265 1 and integers.\nAt any time t, let Nt\u03c3 be the number of genomes (a random variable) of type \u03c3 , and fix the total population\n\u2211\u03c3 Nt\u03c3 to be N. This fixing of the population size to N at each step is the key distinction from the quasispecies\nmodel and is a new parameter. In each time step, the ensuing evolution is then described in terms of the\nfollowing three steps.\n1. (Reproduction) First, in the reproduction step, each genome \u03c3 produces a\u03c3 copies of itself, giving\ndef\ndef\nrise to an intermediate population It = \u2211\u03c3 \u2208{0,1}L It\u03c3 , where It\u03c3 = a\u03c3 Nt\u03c3 .\n2. (Selection) Second, in the selection step, N genomes are chosen at random without replacement from\nthis intermediate population of size It , resulting in the selection of St\u03c3 genomes of type \u03c3 where\nSt\u03c3 \u2208 {0, 1, . . . , It\u03c3 } and \u2211\u03c3 \u2208{0,1}L St\u03c3 = N \u2264 It .\n3. (Mutation) Third, in the mutation step, each selected genome is mutated with probability \u03bc per bit,\n\u03c3 genomes of type \u03c3 , such that\n\u03c3 = N.\ngiving rise to the next generation of Nt+1\n\u2211\u03c3 \u2208{0,1}L Nt+1\nThe starting state is denoted by N0 which is typically ggiven by N00 = N and N0\u03c3 = 0 for all \u03c3 6= 0, but we\nwill often not use this assumption. This RSM model is best viewed as a Markov chain where the state space\nL\nis the set of functions f : {0, 1}2 7\u2192 {0, 1, . . . , N} such that \u2211\u03c3 f (\u03c3 ) = N. Thus, the number of states of this\n\u0001\nL\nL\nMarkov chain is N+2N \u22121 which is roughly N 2 . It can be shown (see Fact 5.1) that for any 0 < \u03bc < 1 the\ntransition matrix of this Markov chain has a unique stationary vector, denoted by \u03c0. \u03c0 is indexed by all f\nsatisfying the property above and \u2211 f \u03c0( f ) = 1, i.e., \u03c0 is a probability distribution over the state space of the\nRSM Markov chain.\n4\n\n\fdef\n\nLet Dt = (Nt\u03c3 /N)\u03c3 \u2208{0,1}L denote the random vector which captures the fractional population at time t.\nIt can also be shown that limt\u2192\u221e E [Dt |D0 ] exists and is independent of D0 . We denote this limit as E [D\u221e ]\nand it can be computed from the stationary vector \u03c0 as follows:\nN\n\ni\n* \u03c0( f ).\nN\ni=0 f : f (\u03c3 )=i\n\nE [D\u03c3\u221e ] = \u2211\n\n\u2211\n\nFinally, we will subscript Dt with parameters such as \u03bc, N when we want to highlight dependence on them,\ne.g. Dt,\u03bc,N . The key questions of interest, especially given the fact that computing \u03c0 would be prohibitive\neven for small values of L and N, are:\n1. For a fixed t, what does E [Dt |D0 ] converge to as N increases?\n2. Is there a notion of error threshold in the RSM model?\n3. How to obtain an estimate of E [D\u221e ] efficiently?\nWe present theoretical results that address all of the above questions. Note that if the answer to the first\nquestion is that E [Dt |D0 ] converges to the prediction of the quasispecies model mt with the same starting\nstates (m0 = D0 ), then it is important as we can leverage the significant understanding obtained from the\nstudy of the quasispecies model while incorporating stochastic effects with finite populations. For the second\nquestion, we need to first define a notion of the error threshold in the RSM model. We do so formally, given\na distance function d.\nDefinition 3.1. Let \u03b5 \u2265 0.\n\b\n\u0002\n\u0003\ndef\n\u03bccd (\u03b5, N) = min \u03bc \u2208 (0, 1) : d(E D\u221e,\u03bc,N , U) \u2264 \u03b5 ,\nwhere U is the uniform distribution over all genomes of length L.\nWhat we want to understand is limN\u2192\u221e \u03bccd (\u03b5, N). Again, if we can show that the answer here is \u03bcc (\u03b5), then\nwe can translate the insights from the quasispecies model to the RSM model.\nFor the third question, first note that if we want to estimate the error threshold, we need to be able\nto compute E [D\u221e ] . Secondly, we consider the standard notion of efficiency: the algorithm to estimate\nE [D\u221e ] should be polynomial in the input size. As we noted, the state space of the RSM Markov chain is\nprohibitively large and computing the stationary state is prohibitive. We employ the Markov Chain Monte\nCarlo method and run the RSM process for some time \u03c4 such that it is guaranteed that distribution from\nwhich D\u03c4 is drawn comes statistically close to the distribution from which D\u221e is drawn, irrespective of\nD0 . Simulating each step of the random walk can be done efficiently. Hence, we are led to the question of\nbounding the mixing time of the RSM Markov chain: the smallest time the finite time distribution needs to\ncome close to the steady state distribution for all starting configurations.\nThe issue of how the input is presented is also important and we briefly discuss it here. In one model,\none can be given all a\u03c3 which would require bit length about \u2211\u03c3 log a\u03c3 and can, in principle, be as large\nor even larger than 2L . Often, this is not the case and either the values a\u03c3 are given by a simple equation,\nor only some fixed number, say k \u001c 2L of the values a\u03c3 are strictly bigger than 1. In the latter case the\ninput has bit length roughly O(k log max\u03c3 a\u03c3 + log L + log 1/\u03bc ). Another case for the input is when a\u03c3 are\nclass invariant. In this case the input is of length roughly O(L log max\u03c3 a\u03c3 + log 1/\u03bc ). We now proceed to\nsummarize our results.\n5\n\n\f3.2\n\nOur Results\n\nWe now give informal statements of our main results, before describing the mathematical techniques employed in the proofs of these results. The formal statements of the theorems described here appear in Section\n5, after a discussion in Section 4, while the formal proofs are deferred to the Appendix.\n3.2.1\n\nConvergence of the Quasispecies and the RSM Model\n\nTheorem 3.2 (Convergence of the RSM and Quasispecies Models). Fix a fitness landscape A with positive\nentries and a mutation transition matrix Q. Consider the RSM process started with the initial state D0 and\nconsider the evolution of the quasispecies model started with the initial state m0 = D0 . Then for any fixed\ntime t,\nlim E [Dt |D0 ] = mt ,\n(3)\nN\u2192\u221e\n\nwhere mt is the fractional population vector at time t starting from m0 predicted by the quasispecies model.\nThe theorem shows that in the infinite population limit, the stochastic fluctuations of the RSM process\ndisappear, and the model converges to the quasispecies model. Informally, the main technical difficulty in\nproving the above theorem is to establish a result of the form limN\u2192\u221e E[Dt |Dt\u22121 ] = limN\u2192\u221e E [Dt |D0 ] with\nprobability 1, which would establish convergence to the quasispecies model. The full proof is deferred\nto the Appendix. This convergence result allows us to show that for any distance function d, a finitary\nversion of the error threshold, \u03bccd (\u03b5, N) as defined above, converges to the error threshold \u03bccd (\u03b5) of the\nquasispecies model, as the population size goes to infinity. These two results provide validation for the finite\npopulation RSM model by establishing that in the infinite population limit, the predictions from the RSM\nmodel converge to those of the quasispecies model. We now move on to problems concerning the mixing\ntime and other computational issues of the RSM model.\n3.2.2\n\nComputational Results in the RSM Model\n\nAs noted before, a primary computational question in both the quasispecies model and the RSM model is\nthe determination of the quasispecies, or the expected population profile at stationarity in the RSM model,\nwhich can then be used to estimate error thresholds (see Section 3.3 for an overview and Sections 5.3.2\nand 5.3.3 for details). For the quasispecies model, a satisfactory solution to this problem is obtained via\nthe observation that the quasispecies is the leading right eigenvector of the QA matrix. The QA matrix is of\ndimension 2L \u00d72L , and the above observation can thus be used to obtain efficient algorithms using black-box\neigenvector finding algorithms for moderate values of L. In the case of class-invariant fitness landscapes, it\nis known [SS82] that one only needs to find the leading eigenvector of an (L + 1) \u00d7 (L + 1) matrix.\nHowever, similar approaches are not as effective for the RSM model. In this case, the stationary distribution is the leading eigenvector of the transition matrix M of the RSM process which is of dimension\nL\nroughly N 2 . Using ideas similar to those referred to above, one can reduce the running time for computing\n2\nthe stationary distribution to N O(L ) .\nTheorem 3.3 (Computation of Steady State in the Class Invariant Case). For any class invariant fitness\n2\nlandscape A, there is an algorithm running in time T = O(N O(L ) ) which computes the steady state of the\nRSM process with population size N and the genome length L.\n\n6\n\n\fHowever, as mentioned before, in many applications, as in the case of HIV, for instance, where N \u223c\nand L \u223c 104 , the problem is still computationally prohibitive. In these cases, one typically resorts to Monte Carlo simulations of the RSM process for estimating the population profile at stationarity [TBVD12], and thus we are led to considering the mixing time of the RSM process. The following\ntheorem derives conditions on the parameters of evolution under which the RSM model mixes rapidly.\n\n103 \u2212 106\n\nTheorem 3.4 (Mixing Time of the RSM Process). Given a fitness landscape A, mutation rate \u03bc, the RSM\n1\n\u03c4 a\u03c4\nprocess exhibits fast mixing if (1 \u2212 2\u03bc) max\nmin\u03c4 a\u03c4 L + N < 1.\nHaving stated our results, we now highlight the techniques employed in the proofs.\n\n3.3\n\nOverview of Our Technical Contributions\n\nAs before, we will denote by Dt the random variable of fractional populations after t steps of the RSM\nprocess, and by St the random variable of the populations of genomes after the replication and selection\nsteps of the (t + 1)-the step of the RSM process.\nOur convergence result (Theorem 3.2). The starting point of the proof of our convergence result is to\nobserve that E [Dt+1 |Dt ] has the same functional form r (as a function of Dt ) as the evolution equation of\nthe discrete time quasispecies model, with r as defined in Equation (2): E [Dt+1 |Dt ] = r(Dt ). Our high\nlevel approach is to first show that Dt+1 is actually concentrated around E [Dt+1 |Dt ] . Using the Lipschitz\ncontinuity of the evolution function r, we can then chain these concentration results inductively to show\nthat the evolution of Dt is tightly concentrated around the evolution of the discrete time quasispecies model,\nwhich allows us to show that E [Dt ] converges to the quasispecies as N \u2192 \u221e. To illustrate the ideas involved,\nwe consider the case L = 1. Here the two genomes are {0, 1}. After the replication phase in the t-th step,\nthere are a0 Dt0 N copies of 0. For the i-th copy, let Ri denote the indicator variable for this copy being selected\na D0 N\n\n0 t\nin the selection phase, so that St0 = \u2211i=1\nRi . Since the Ri 's are not independent, we cannot directly apply\na Chernoff bound. However, since they are negatively correlated, one expects concentration to hold, and\nthis can indeed be shown using the so-called method of bounded differences. The same reasoning works\nfor St1 , and thus we get that given Dt , the intermediate population St after the replication and selection steps\nis concentrated around its expectation with high probability. We now look at the mutation step. Let Mi be\n0\nthe indicator variable for the ith genome being 0 after the mutation step. We then have NDt+1\n= \u2211Ni=1 Mi .\nSince the Mi 's are independent\u0002 random\u0003 variables, it can be shown using a Chernoff bound that given St ,\n0\n0 |S = 1/N (\u03bcS0 + (1 \u2212 \u03bc)S1 ). The two steps can then can be combined\nDt+1\nis concentrated around E Dt+1\nt\nt\n\u0002 \u0002 0t\n\u0003 \u0003\n\u0002\n\u0003\n0\nto show that given Dt , Dt+1 is concentrated around E E Dt+1\n|St |Dt = 1/N E \u03bcSt0 + (1 \u2212 \u03bc)St1 |Dt =\n\n(1\u2212\u03bc)a0 Dt0 +\u03bcDt1\n.\na0 Dt0 +Dt1\n\n1 .\nThe same reasoning works for Dt+1\nWith some more work, this argument can be generalized to work for arbitrary L. The concentration\nguarantee we obtain is of the following form: there are quantities \u03b5t and pt which are both oN (1) such that\ngiven Dt , |Dt+1 \u2212 E [Dt+1 |Dt ]| \u2264 \u03b5t with probability at least 1 \u2212 pt . In the next step, we chain these stepwise bounds inductively in order to remove the conditioning and show that for all t \u2264 t0 , Dt is concentrated\naround mt . An important component of the induction is the observation that r is Lipschitz continuous,\nwhich allows us to control the propagation of the errors \u03b5t0 in each step. By the induction hypothesis, we\nhave that |Dt \u2212 mt | \u2264 \u03b5t0 with probability at least 1 \u2212 pt0 , where \u03b5t0 and pt0 are both oN (1). Assuming the\nLipschitz constant of r to be K, this implies that E [Dt+1 |Dt ] = r(Dt ) is within distance K\u03b5t0 of mt+1 = r(mt )\nwith probability at least 1 \u2212 pt0 . Applying the convergence result from the first step, we then have that with\n0\n0\nprobability at least 1 \u2212 pt+1\n= 1 \u2212 pt0 \u2212 pt , |Dt+1 \u2212 mt+1 | \u2264 \u03b5t+1\n= K\u03b5t0 + \u03b5t of mt+1 . The quantities pt0 , \u03b5t0 for\n\n7\n\n\ft \u2264 t0 can be chosen to be oN (1), which is sufficient to show the required convergence. The details appear\nin Appendix B.2. We now give an overview of the proofs of our computational results.\nComputing the stationary distribution in the class invariant case (Theorem 3.3). Recall that the state\nL\nspace of the RSM Markov chain is roughly N 2 . However, if the fitness function is class invariant, we can\nshow that the number of distinct coordinates in the state space is about N L . To see this, first we define an\nequivalence relation on the states of the RSM Markov chain. We say that f , g, which are functions from\n{0, 1}L to {0, 1, . . . , N} satisfying \u2211\u03c3 f (\u03c3 ) = N and \u2211\u03c3 g(\u03c3 ) = N, are equivalent (denoted f \u2261 g) if they\nhave the same statistics for every Hamming class, i.e., for every 0 \u2264 i \u2264 N,\n\n\u2211\n\nf (\u03c3 ) =\n\n{\u03c3 \u2208{0,1}L |wH (\u03c3 )=i}\n\n\u2211\n\ng(\u03c3 ).\n\n{\u03c3 \u2208{0,1}L |wH (\u03c3 )=i}\n\nThus, the state space of the RSM Markov chain gets partitioned into about (N + 1)L+1 different classes.\nThen, due to the fact that the fitness function is class invariant, it can be shown that the transition probability\nof f to any other equivalence class is the same as that of g to the same class. Hence, one only needs\nto compute the transition probability from one equivalence class to another. This probability is a large\nbinomial sum and one has to be careful in its computation and keep track of the number of bits required to\nrepresent each entry of this Markov chain over the equivalence classes. Once we have the transition matrix\nof this Markov chain, one can compute its largest eigenvector which corresponds to the stationary state. We\n2\nshow that, if one does this carefully, one can compute the eigenvector in time roughly N O(L ) . The details\nappear in Section B.5.\nAlgorithm to compute the error threshold. Once we have the ability to either compute the stationary\nstate of the RSM process or derive independent samples from its stationary state (which allows us to estimate\nthe relative frequencies of the genomes at stationarity with a good precision by taking an average of the\nsampled states), the algorithm to estimate the error threshold is simple. The idea is to start with a small value\n(\u001c 1/L) of \u03bc, and to estimate/compute the stationary distribution of the RSM process for the current value of\n\u03bc. The algorithm then checks if the estimate of the stationary distribution is close to the uniform distribution\non the genomes in the measure of closeness of one's choice. If so, it stops and outputs the current value of\n\u03bc as an estimate of the error threshold. Else, it increases \u03bc by a very small amount and repeats the above\nsteps. In case direct computation of the stationary state of the RSM process is computationally prohibitive,\nindependent samples from the stationary distribution of the RSM process are derived by simulating the RSM\nprocess up to its mixing time. The number of samples required can be estimated from a simple application\nof Chernoff bound on the random variable corresponding to the stationary state distribution of the RSM\nprocess. Hence, to establish bonds on the running time of the error threshold estimating algorithm, it is\nimportant to be able to bound the time it takes for the RSM process so that Dt comes close to the stationary\nstate, D\u221e . Our next result is towards this.\nMixing time result (Theorem 3.4). Since the stationary distribution of the RSM chain is not very well\nunderstood, it is not clear how to apply conductance-based geometric tools or the canonical paths method\n(see, for example, [JS88]) in order to prove the mixing time result. We are thus led to more combinatorial\ncoupling based methods. Here one starts with an integer valued metric d on the state space of the Markov\nchain, and then one runs two copies Xt and Yt of the chain. To show fast mixing, it is then sufficient to prove\nthat Xt and Yt can be coupled so that E [d(Xt+1 ,Yt+1 )|(Xt ,Yt )] \u2264 \u03b1 < 1. In general, defining a coupling can\n8\n\n\fbe tricky because one needs to carefully argue that the marginals of the coupling agree with the original\nMarkov chain.\nWe define the coupling in two phases: the first phase includes the replication and selection steps and\nthe second phase includes only the mutation step. We begin with the easier mutation step. Let It and Jt\ndenote the state of the two RSM chains after the replication and selection steps. For most natural choices\nof the distance metric d, it is possible to couple the mutation step using the standard coupling for the\nrandom walk on the hypercube so that E [d(Xt+1 ,Yt+1 )|(It , Jt )] \u2264 (1 \u2212 2\u03bc)d(It , Jt ). The challenge however\nlies in controlling E [d(It , Jt )|(Xt ,Yt )] while coupling the replication and selection steps, since because of\nthe global nature of the replication and selection steps, E [d(It , Jt )|(Xt ,Yt )] can become quite large. We\ncontrol this increase by a careful choice of the metric d, and by appealing to the path coupling methods\nof Bubley and Dyer [BD97]. The path coupling theorem says that for integer valued d, it is sufficient to\nensure E [d(Xt+1 ,Yt+1 )|(Xt ,Yt )] \u2264 \u03b1 \u2264 1 only for states Xt and Yt satisfying d(Xt ,Yt ) = 1 in order to establish\nfast mixing. Our coupling is then defined as follows. Fix a permutation of the N genomes in the chain\nXt : d(Xt ,Yt ) is then the minimum over all possible permutations of the N genomes in Yt of the sum of the\nHamming distances between the genomes at the same positions in the two permutations. The main technical\nstep is to show that for this d, the replication and selection steps can be coupled in such a way that starting\nN max\u03c3 a\u03c3\nfrom Xt and Yt satisfying d(Xt ,Yt ) = 1, E [d(It , Jt )|(Xt ,Yt )] after these steps is at most N\u22121\nmin\u03c4 a\u03c4 L. It is in\nthis step that we use the form of the distance metric d crucially; the details of the coupling are somewhat\ntechnical and involve arguing carefully that the coupling is valid, and are given in Appendix B.4. We then\ncombine this with the coupling for the mutation step described above to show contraction in the expected\nN max\u03c3 a\u03c3\ndistance under the condition (1 \u2212 2\u03bc) N\u22121\nmin\u03c4 a\u03c4 L < 1.\n\n4\n4.1\n\nDiscussion and Future Perspectives\nPrevious Work\n\nThe notion of the quasispecies and the existence of an error threshold were recognized first by Eigen and his\ncoworkers in the 1970s and 1980s [Eig71, EMS89]. Translation of these ideas into intervention strategies\nrequires overcoming two key limitations of the quasispecies model. First, the model assumes an infinite\npopulation size, whereas realistic population sizes can be quite small. With HIV, for instance, the effective\npopulation size is estimated to be \u223c 103 \u2212 106 [KAB06, BSSD11]. Second, the theory assumes a singlepeak fitness landscape, whereas realistic landscapes can be far more complex [BCP+ 04, HMC+ 11]. Efforts\nover the last several decades have attempted to overcome these limitations of the quasispecies model [NS89,\nBS93, WH96, Wie97, AF98, SH06, TH07, PMnD10] (see Wilke [Wil05] for a recent review). The finite\npopulation case, however, has remained difficult to solve in full generality. Most studies resort to simulations\nor use approximate or heuristic approaches to describe the finite population case, and we discuss some of\nthese here.\nNowak and Schuster [NS89] used a birth-death process to model the underlying\n\u221a evolution in finite populations and using simulations predicted that the error threshold scales as 1/ N. Their model, however,\ndoes not converge to the quasispecies model as N goes to infinity. Alves and Fontanari [AF98] present a\nmodel which employs a two-stage sampling with replacement in the selection process: first sampling uniformly from the population, and then sampling from the obtained sample with biases proportional to the\nfitness. They note, however, that sampling with replacement destroys the negative correlation between the\nselection of two individuals of the same species induced by the finite population constraint when selection is implemented using sampling without replacement. They find that the error threshold scales as 1/N.\n\n9\n\n\fFurther, they only analyze a heuristic deterministic approximation to their model, and do not consider rigorously the question of convergence of their original model to the quasispecies model. The closest to our\nconvergence result is that by van Nimwegen et al. [vNCM99] who show convergence to a deterministic\nEigen-like dynamics but again employ sampling with replacement and use special cases of additive fitness\nlandscapes. With finite populations, there can be a significant statistical difference between sampling with\nand without replacement, the latter (which we employ) being more realistic. It is well known that as N \u2192 \u221e\nthe difference between sampling with and without replacement shrinks, but then as we prove, so does the\ndifference between our population genetics model and the quasispecies model. Their convergence proof has\na similar structure as ours but we are able to use Chernoff bound type inequalities which are much stronger\nthan the second moment inequalities used by them. Consequently, our convergence results are quantitatively\nstronger. We additionally prove convergence of the error threshold and fast mixing, questions not considered\nby [vNCM99]. More recently, Musso [Mus11] presented the transition matrix for sampling with replacement in the case L = 1 and also claimed convergence to the quasispecies model in the deterministic limit.\nNo attempt, however, is made in [Mus11] to make this latter claim rigorous. Another class of studies relies\non approximations and heuristics inspired from physics, and in particular statistical mechanics, to render the\nfinite population case mathematically tractable (e.g., [BK98, SRA08, PMnD10]).\nWhile previous studies have focused extensively on the fractional distribution of genomes at stationarity,\nlittle is known of the time to reach the stationary state. Campos and Fontanari [CF99] show that in the\nlimit of infinitely large genome lengths (L \u2192 \u221e) and population sizes (N \u2192 \u221e) and with the single peak\nfitness landscape, the timescale associated with the decline of the master sequence is 1/ln(qa) where q is\nthe probability that a genome is replicated without error, and a is the relative fitness\nof the master sequence.\n\u221a\nFurther, they show that with finite populations, this timescale is proportional to N. The mixing time when L\nand N are both finite and when the fitness landscape is more general than the single peak remains unknown.\nThe latter mixing time has practical significance in the modeling of the action of mutagenic drugs, as it\nrespresents the duration of therapy required to ensure completion of the transition to the error catastrophe.\nOur study presents conditions when the mixing is rapid and hence the transition to error catastrophe occurs\nquickly. Further, for computational studies that attempt to realize this transition in silico, our study presents\nan algorithm that allows efficient Monte Carlo sampling-based estimation of the error threshold.\n\n4.2\n\nApplications of the RSM Model\n\nThe motivation behind the RSM model and the algorithms discussed here is to get a basic framework for\nunderstanding the evolution of viruses of current interest such as HIV. Making concrete predictions relevant\nto the clinical setting requires super-imposing the specifics of the viruses of concern on the present framework. This often involves subtle modifications of the RSM process along with validation against data. For\nexample, in related recent work, two of the authors and their co-workers applied the RSM model to mimic\nthe within-host genomic evolution of HIV-1 [TBVD12]. It has been shown before [BSSD11] that these\nsimulations quantitatively capture data of the evolution of viral genomic diversity in patients over extended\ndurations (\u223c 10 years) following infection and the approach is extended in [TBVD12] to estimate the error\nthreshold of HIV-1. We envision that similar adaptation of our model will prove useful in elucidating the\nevolution and treatment guidelines for other asexual haploid organisms of interest.\n\n4.3\n\nCritique of the RSM Model\n\nWe note that our structural and computational results are independent of the nature of the fitness landscape\nso long as there are no lethal mutations (a\u03c3 6= 0 for any \u03c3 ). Our model, however, does not consider lethal\n10\n\n\fmutations. While letting some a\u03c3 be 0 does not affect the quasispecies model due to the constant rescaling involved, it introduces an absorbing state in the RSM Markov chain, thus making it non-ergodic, and\ncausing the population to eventually decrease to zero. While Wilke and others [Wil05, WK93, TH07] have\ncommented on the role of lethal mutations in extreme cases, establishing their full implications lies beyond\nthe scope of the present paper. Although lethal mutations do occur, it turns out that in many important\nscenarios such as the evolution of HIV-1, a Hamming class invariant landscape without lethal mutations appears to capture key features of the underlying fitness interactions [BCP+ 04], rendering our RSM framework\napplicable.\nFinally, we note that our assumption of a fixed population size, N, is consistent with the widely accepted\npopulation genetics-based models of evolution, where a constant effective population size is employed to\nquantify the strength of stochastic effects [HC06]. Note that allowing N to vary with time (generations),\ndoes not increase the complexity in our model. The distinction between an infinite population model and\na finite population model arises from the culling of the population in the latter model in order to maintain\na finite population size. A fixed N or varying N will only result in different extents of culling in different\ngenerations, but will not change the overall structure of the model. The advantage in keeping N constant for\nour present study is that it allows easier examination of the convergence to the quasispecies model.\n\n4.4\n\nOpen Problems\n\nOur study of the quasispecies and RSM models has revealed several interesting and important problems. We\nlist the main ones here.\nStructure of the Quasispecies. Perhaps the most attractive feature of finite population models as opposed\nto the quasispecies model is that they can be used to study the effect of random genetic drift on inter-patient\nvariations. Inter-patient variations in disease progression and response to treatments are known to be significant with HIV infection [NBS+ 98, GKB+ 05]. The collection of viral particles in an infected individual may\nbe thought of as one realization of the random viral evolutionary process, and limt\u2192\u221e Var [Dt\u03c3 ] then provides an estimate of inter-patient variations in viral evolution due to the effect of the finite population size.\nThus, in addition to the structure of the quasispecies in the finite population case, defined by the expected\nfrequencies limt\u2192\u221e E [Dt ] when N < \u221e, the variance of the frequencies limt\u2192\u221e Var [Dt\u03c3 ] as a function of the\npopulation size N is also an important quantity to be studied.\nError Threshold. In the quasispecies model with the single-peak fitness landscape, \u03bcc has been found,\nwithout a rigorous proof, to be O(1/L), so that an error catastrophe occurs for \u03bc \u001c 0.5 (e.g., see [EMS89]).\nFurther, the transition is sharp, so that a small increase in \u03bc from below to above \u03bcc induces a dramatic\nchange in the quasispecies structure. With other fitness landscapes, such as the multiplicative landscape,\nhowever, the quasispecies approaches the uniform distribution gradually as \u03bc approaches 0.5 [WH96]. Further, lethal mutations, where a\u03c3 = 0 for some \u03c3 's, appear to show the existence of an error threshold only\nif multiple mutations in a single replication are allowed [WK93, TH07, Wil05]. Thus, the conditions under\nwhich a sharp transition leading to an error catastrophe at \u03bcc \u001c 0.5 would occur remain to be established.\n\u221a\nSecond, the dependence of \u03bcc on N remains to be identified. While some simulations suggest a 1/ N dependence [NS89, BS93], others find the dependence to go as 1/N [AF98]. As pointed out before, knowledge\nof \u03bcc for finite N is important in the modeling of antiviral strategies based on mutagenic drugs.\n\n11\n\n\fMixing Time. The main outstanding question here is to get a tight bound on the mixing time of the RSM\nMarkov chain for a full range of evolutionary parameters. We notice that our result shows a good mixing\ntime bound only under certain conditions on the parameters. Though we conjecture that the chain is rapidly\nmixing for other values of the parameters too, we believe that novel methods would be needed to extend\nour results in this direction. Apart from being useful in determining the time required for simulations to\nproduce samples from the stationary distribution, the mixing time bounds also have biological significance.\nFor example, when modeling the effect of a mutagenic drug under the RSM model, the convergence rate\nwould models the minimum required duration of treatment before the error catastrophe occurs.\n\n5\n5.1\n\nFormal Statements of Main Results\nPreliminaries and Definitions\n\nIn this section we present rigorous statements of our results. Several definitions may be found repeated here\nin the interest of the readability of this section. We recall that genomes of length L are denoted by L-bit 0-1\nstrings. We will denote the Hamming distance between genomes \u03c3 and \u03c4 by dH (\u03c3 , \u03c4), and the Hamming\nweight of a genome \u03c3 by wH (\u03c3 ). A population is defined as a multiset of genomes of the same length.\nWhile discussing the RSM model, we will fix the size of the population to be N.\nThe Markov Chain for the RSM Model. We will denote the evolution of the RSM process using a time\u221e\nindexed sequence of vector valued random variables (Nt )t=0\n. The entries of Nt are indexed by genomes \u03c3 ,\nand the entry Nt\u03c3 denotes the number of genomes of type \u03c3 at time t. At every time t, \u2211\u03c3 \u2208{0,1}L Nt\u03c3 = N.\ndef\n\nThe random variables Dt\u03c3 = Nt\u03c3/N denote the fractional population of the genome \u03c3 at time t.\nReproduction Step and the Fitness Landscape In the reproduction step, each genome \u03c3 produces a\u03c3\ndef\ncopies of itself, so that the number of genomes of type \u03c3 after this step is It\u03c3 = a\u03c3 Nt\u03c3 , and the\ndef\n\ndef\n\ndef\n\ntotal number of genomes is It = \u2211\u03c3 a\u03c3 Nt\u03c3 . The matrix A defined by A\u03c3 \u03c3 = a\u03c3 and A\u03c3 \u03c4 = 0 for \u03c3 6= \u03c4\nis called the fitness landscape. The fitness landscape is said to be class-invariant if a\u03c3 depends only\non the Hamming weight of \u03c3 . By a slight abuse of notation, we will denote by ai the fitness of all\ngenomes with Hamming weight i in the class invariant case.\nSelection and Mutation Steps and the Mutation Rate In the selection step, N genomes are sampled without replacement from the genomes obtained after the reproduction step. In the mutation step, each bit\nof each of the N genomes obtained after the selection step is flipped with a probability \u03bc, called the\ndef\nmutation rate. The mutation transition matrix Q defined by Q\u03c3 \u03c4 = \u03bc dH (\u03c3 ,\u03c4) (1 \u2212 \u03bc)L\u2212dH (\u03c3 ,\u03c4) gives the\nprobability that a genome of type \u03c3 mutates to one of type \u03c4 in the mutation step.\nThe RSM process as described above is a Markov chain on the state space of functions f : {0, 1}L \u2212\u2192 N,\nsatisfying \u2211\u03c3 \u2208{0,1}L f (\u03c3 ) = N. The transition matrix M of this chain is described in Section A.\nFact 5.1. When the mutation rate \u03bc \u2208 (0, 1) and a\u03c3 > 0 for all \u03c3 , the Markov chain M corresponding to\nthe RSM process is ergodic, and hence has a unique stationary distribution.\nThis is a simple consequence of the fact that \u03bc and A are positive. See Section B.1 for a proof.\n\n12\n\n\fImportant Statistics of the RSM Process and the Projected RSM Process. The transition matrix M\n\u0001\n\u0001\nL\nL\nof the RSM Markov chain is of dimension N+2N \u22121 \u00d7 N+2N \u22121 . When the fitness landscape A is class\ninvariant, one can get a projected Markov chain with a significantly smaller state space which can still be\nused to compute the average fitness and the average population of each fitness class at stationarity. Consider\nequivalence classes indexed by functions h : [0, L] \u2212\u2192 N with \u2211Li=0 h(i) = N, such that a function f in the\nstate space of M is in the equivalence class [h] if and only if for every i \u2208 [0, L], \u2211{\u03c3 \u2208{0,1}L |wH (\u03c3 )=i} f (\u03c3 ) =\nh(i). We then have the following lemma the proof of which is in Section B.1.\nLemma 5.2. Let f , g belong to the same equivalence class h as defined above, and let [h0 ] be another\nequivalence class. We then have M ( f , [h0 ]) = M (g, [h0 ]).\nThus, we can consider the projected Markov chain Mw with state space\n(\n)\nL\n\n\u03a9w =\n\n[h]\n\n\u2211 h(i) = N\n\n.\n\ni=0\n\nNotice that |\u03a9w | =\n\nN+L\nL\n\n\u0001\n. Also, if \u03c0w is the stationary distribution of Mw , then by the projection property\n\u03c0w ([h]) =\n\n\u2211\n\n\u03c0( f ).\n\nf \u2208[h]\n\nThis property implies that the expected populations for every Hamming class of genomes and, hence, the\nexpected average fitness at stationarity, are the same for Mw and M .\nMixing Time. We will denote by \u03c0 the stationary distribution of the RSM process, and let N\u221e be a random\nvariable distributed according to \u03c0. We know that the distributions of the random variables Nt converge in\ntotal variation distance (and hence in distribution) to \u03c0, due to the ergodicity of the RSM process. We fix\nour notation for mixing times in this section.\nDefinition 5.3. The total variation distance between two probability distributions D1 and D2 on the sample\nspace \u03a9 is defined by kD1 \u2212 D2 kTV = maxA\u2286\u03a9 |D1 (A) \u2212 D2 (A)|.\nDefinition 5.4. Fix a Markov chain N on a state space S. We define\ndef\n\nd(t) = max kN t (\u03b1, *) \u2212 \u03c0kTV .\n\u03b1\u2208S\n\nFor 0 \u2264 \u03b5 \u2264 1/2, the mixing time of N is defined by\ndef\n\n\u03c4mix (\u03b5) = min{t : d(t) \u2264 \u03b5}.\nNotice that by the projection property, the mixing time of the projected RSM chain Mw is at most the mixing\ntime of the original RSM chain M .\nError Thresholds. In the following definition, we specifically emphasize the dependence of the random\nvariables Nt , Dt on \u03bc, N by denoting them as Nt,\u03bc,N and Dt,\u03bc,N . We denote by D\u221e,\u03bc,N a version of Dt,\u03bc,N\ndistributed according to the stationary distribution of the RSM process, and by U the uniform distribution\nover genomes. Given a distance function d, one can define the error threshold with respect to d as follows.\n13\n\n\fDefinition 5.5 (Error Threshold for the RSM Model). Let \u03b5 \u2265 0.\n\b\n\u0002\n\u0003\ndef\n\u03bccd (\u03b5, N) = min \u03bc \u2208 (0, 1) : d(E D\u221e,\u03bc,N , U) \u2264 \u03b5 ,\nwhere U is the uniform distribution over all genomes of length L.\nOf particular interest is the function d h : for any two distributions D1 and D2 over genomes, d h (D1 , D2 )\ndenotes |\u2211\u03c3 w(\u03c3 )(D1 (\u03c3 ) \u2212 D2 (\u03c3 ))|. The corresponding \u03bc will carry the superscript h.\n\n5.2\n\nConvergence to the Quasispecies Model\n\nOur first main result is that the RSM model converges to the quasispecies model.\nTheorem 5.6. Fix a fitness landscape A with positive entries and a mutation transition matrix Q. Consider\nthe RSM started with the initial state D0 and consider the evolution of the quasispecies model started with\nthe initial state m0 = D0 . Then for any fixed time t0 ,\nlim E [Dt0 |D0 ] = mt0 ,\n\nN\u2192\u221e\n\n(4)\n\nwhere mt0 is the state of evolution of the quasispecies model at time t0 starting from m0 .\nThe proof of the above theorem is relegated to Section B.2. As a corollary to the theorem above, one can\nshow that there is convergence of a finitary version of the error threshold \u03bcch (\u03b5, N) to the error threshold\n\u03bcch (\u03b5) for the quasispecies model, as the population size goes to infinity. Formally, we have the following:\nCorollary 5.7. Fix a mutation rate \u03bc \u2264 1/2 and an error parameter \u03b5. For every \u03b4 > 0, there exists a time\nt0 > 0 such that for t > t0 , one can find an N\u03b4 ,t such that for N > N\u03b4 ,t ,\n\u0002\n\u0003 \u0001\nd h E Dt,\u03bc,N , U \u2265 \u03b5 \u2212 \u03b4 , when \u03bc < \u03bcch (\u03b5), and\n\u0002\n\u0003 \u0001\nd h E Dt,\u03bc,N , U \u2264 \u03b5 + \u03b4\nwhen \u03bc = \u03bcch (\u03b5),\nHere we use the subscripts \u03bc and N to emphasize the dependence of the distribution of Dt on \u03bc and N.\nAlthough we will prove our results for the error threshold in terms of the average Hamming distance, it is\neasy to translate our results to other common dispersal measures as described in Section B.1.1. The proof\nof the above Corollary follows easily from Theorem 5.6 and is given in Section B.3. We note here that\nextending the above corollary to get convergence of finite population error thresholds depends upon proving\na strengthened version of our convergence result (Theorem 5.6), which we leave as an open problem. In\nfact, on the basis of simulation results, we conjecture that for fixed \u03b5, \u03bcch (\u03b5, N) monotonically increases to\n\u03bcch (\u03b5).\n\n5.3\n5.3.1\n\nComputational Results\nMixing Time Bounds on the RSM Process\n\nWe give a coupling argument in Section B.4 which allows us to prove the following theorem.\nTheorem 5.8. Fix 0 < \u03bc \u2264 1/2, and a fitness landscape A. Let\ndef\n\nK(A, \u03bc) = (1 \u2212 2\u03bc)\nWhen K(A, \u03bc) < 1, we have \u03c4mix (\u03b5) = O\n\n\u0010\n\nlog(NL/\u03b5)\nlog(1/K)\n\n\u0011\n.\n\n14\n\nN max\u03c3 a\u03c3\nL.\nN \u2212 1 min\u03c4 a\u03c4\n\n\f5.3.2\n\nComputing the Stationary Distribution in the Class Invariant Case\n\nTheorem 5.9. For every A which is class invariant, and \u03bc which can be represented using b bits, there is an\nalgorithm running in time T given by\n\u0012\n\u0013L(L+1) !\n\u00133\n\u00132 \u0012\n\u0012\n(N + L)L\n(N + L)L\nN\ndef\n3\n2\nT = \u00d5 bL +\n)\n(NbL + L + N max a\u03c3 ) + NbL\ne(1 +\nL!\nL!\nL(L + 1)\n2\n\nwhich computes \u03c0w for the Markov chain Mw described above. For fixed L, T = O(N O(L ) ).\nThe proof of the above theorem appears in Section B.5, and is based on the projected RSM process discussed\nin Section 5.1. The above theorem immediately gives a grid-search based algorithm that given a grid resolution \u03b4 and \u03b5 > 0, outputs a approximation \u03bc0 to the error threshold in time T * 1/2\u03b4 such that \u03bc0 \u2265 \u03bcch (\u03b5, N)\nand d h (D\u221e,\u03bc0 \u2212\u03b4 , U) > \u03b5. We now consider Markov Chain Monte Carlo based grid-search methods.\n5.3.3\n\nMarkov Chain Monte Carlo Methods\n\nThe general strategy for Monte Carlo based grid search methods for determining error thresholds is described\nin the algorithm E RRORT HRESHOLD in Figure 1 in the Appendix. We will denote the mixing time \u03c4mix (\u03b5)\nfor parameters L, N, A and \u03bc as \u03c4(L, N, A, \u03bc, \u03b5). We consider the projected chain Mw described above which\ncontains enough information to compute the average Hamming weight, and whose state can be maintained\nas a tuple in {0, 1, . . . , N}L+1 .\nTheorem 5.10. Let A be class invariant, and consider the error threshold \u03bcch (\u03b5, N). Suppose the algorithm\nE RRORT HRESHOLD is run with input grid resolution \u03b4 , accuracy parameter \u03b41 , and error probability \u03b42 .\nLet T be the maximum over k of the quantity \u03c4(L, N, A, k\u03b4 , \u03b41 /(2L)) where k \u2264 1/(2\u03b4 ) is a positive integer.\nThe algorithm E RRORT HRESHOLD runs in time T * s * \u00d5(d1/(2\u03b4 )e NL max\u03c3 a\u03c3 ), where\n\u0018 4\n\u0019\n8L\ns=\nlog (2 d1/(2\u03b4 )e (L + 1)/\u03b42 ) ,\n\u03b412\nand with probability at least 1 \u2212 \u03b42 , produces an output \u03bc0 satisfying \u03bc0 \u2265 \u03bcch (\u03b5 + \u03b41 /2) and\nd h (D\u221e,\u03bc0 \u2212\u03b4 , U) \u2265 \u03b5 \u2212 \u03b41 /2.\nThe proof of the above theorem appears in Section B.6, where we also point out some technical subtleties\nabout the definition of error thresholds.\nAcknowledgments. We thank Rajesh Balagam for helping us with several simulations based on which this\ntheoretical study was initiated. This work was initiated while Piyush Srivastava was an intern at Microsoft\nReseach, Bangalore.\n\nReferences\n[AB05]\n\nChristian L. Althaus and Sebastian Bonhoeffer. Stochastic Interplay between Mutation and\nRecombination during the Acquisition of Drug Resistance Mutations in Human Immunodeficiency Virus Type 1. Journal of Virology, 79(21):13572\u201313578, 2005.\n\n15\n\n\f[ADL04]\n\nJon P. Anderson, Richard Daifuku, and Lawrence A. Loeb. Viral error catastrophe by mutagenic nucleosides. Annual Review of Microbiology, 58(1):183\u2013205, 2004.\n\n[AF98]\n\nD. Alves and J. F. Fontanari. Error threshold in finite populations. Physical Review E,\n57(6):7008 \u2013 7013, June 1998.\n\n[BCP+ 04]\n\nSebastian Bonhoeffer, Colombe Chappey, Neil T. Parkin, Jeanette M. Whitcomb, and Christos J. Petropoulos. Evidence for positive epistasis in HIV-1. Science, 306(5701):1547\u20131550,\n2004.\n\n[BD97]\n\nR. Bubley and M. E. Dyer. Path coupling: a technique for proving rapid mixing in markov\nchains. In Proceedings of the 38th IEEE Symposium on the Foundations of Computer Science(FOCS), pages 223 \u2013 231, 1997.\n\n[BK98]\n\nD. Bonnaz and A. J. Koch. Stochastic model of evolving populations. Journal of Physics A:\nMathematical and General, 31(2):417, 1998.\n\n[BKP+ 11]\n\nRebecca Batorsky, Mary F. Kearney, Sarah E. Palmer, Frank Maldarelli, Igor M. Rouzine, and\nJohn M. Coffin. Estimate of effective recombination rate and average selection coefficient for\nHIV in chronic infection. Proceedings of the National Academy of Sciences, 108(14):5661\u2013\n5666, 2011.\n\n[BS93]\n\nSebastian Bonhoeffer and Peter F. Stadler. Error thresholds on correlated fitness landscapes.\nJournal of Theoretical Biology, 164(3):359 \u2013 372, 1993.\n\n[BSSD11]\n\nRajesh Balagam, Vasantika Singh, Aparna Raju Sagi, and Narendra M. Dixit. Taking multiple infections of cells and recombination into account leads to small within-host effectivepopulation-size estimates of HIV-1. PLoS ONE, 6(1):e14531, 01 2011.\n\n[CCA01]\n\nShane Crotty, Craig E. Cameron, and Raul Andino. RNA virus error catastrophe: Direct molecular test by using ribavirin. Proceedings of the National Academy of Sciences, 98(12):6895\u2013\n6900, 2001.\n\n[CF99]\n\nPRA Campos and JF Fontanari. Finite-size scaling of the error threshold transition in finite\npopulations. J. Phys A, 32:L1\u2013L7, 1999.\n\n[DP09]\n\nDevdatt P. Dubhashi and Alessandro Panconesi. Concentration of Measure for the Analysis of\nRandomized Algorithms. Cambridge University Press, 2009.\n\n[Eig71]\n\nM. Eigen. Selforganization of matter and the evolution of biological macromolecules. Die\nNaturwissenschaften, 58:456\u2013523, 1971.\n\n[EMS89]\n\nM. Eigen, J. McCaskill, and P. Schuster. The molecular quasi-species. Adv. Chem. Phys.,\n75:149\u2013263, 1989.\n\n[GD10]\n\nSaikrishna Gadhamsetty and Narendra M. Dixit. Estimating frequencies of minority\nnevirapine-resistant strains in chronically HIV-1-infected individuals naive to nevirapine by\nusing stochastic simulations and a mathematical model. J. Virol., 84(19):10230\u201310240, 2010.\n\n16\n\n\f[GKB+ 05] Enrique Gonzalez, Hemant Kulkarni, Hector Bolivar, Andrea Mangano, Racquel Sanchez,\nGabriel Catano, Robert J. Nibbs, Barry I. Freedman, Marlon P. Quinones, Michael J. Bamshad,\nKrishna K. Murthy, Brad H. Rovin, William Bradley, Robert A. Clark, Stephanie A. Anderson,\nRobert J. O'Connell, Brian K. Agan, Seema S. Ahuja, Rosa Bologna, Luisa Sen, Matthew J.\nDolan, and Sunil K. Ahuja. The influence of CCL3L1 gene-containing segmental duplications\non HIV-1/AIDS susceptibility. Science, 307(5714):1434\u20131440, 2005.\n[GPLL+ 05] Ana Grande-P\u00e9rez, Ester L\u00e1zaro, Pedro Lowenstein, Esteban Domingo, and Susanna C. Manrubia. Suppression of viral infectivity through lethal defection. Proceedings of the National\nAcademy of Sciences of the United States of America, 102(12):4448\u20134452, 2005.\n[HC06]\n\nDaniel L. Hartl and Andrew G. Clark. Principles of Population Genetics, Fourth Edition.\nSinauer Associates, Inc., 4th edition, December 2006.\n\n[HMC+ 11] Trevor Hinkley, Joao Martins, Colombe Chappey, Mojgan Haddad, Eric Stawiski, Jeannette M\nWhitcomb, Christos J Petropoulos, and Sebastian Bonhoeffer. A systems analysis of mutational\neffects in HIV-1 protease and reverse transcriptase. Nature Genetics, 43:487\u2013489, 2011.\n[JS88]\n\nMark Jerrum and Alistair Sinclair. Conductance and the rapid mixing property for markov\nchains: the approximation of the permanent resolved. In Proceedings of the 20th Annual ACM\nSymposium on Theory of Computing (STOC 1988), pages 235\u2013243, 1988.\n\n[KAB06]\n\nRoger D. Kouyos, Christian L. Althaus, and Sebastian Bonhoeffer. Stochastic or deterministic:\nwhat is the effective population size of HIV-1? Trends in Microbiology, 14(12):507 \u2013 511,\n2006.\n\n[LA10]\n\nAdam S. Lauring and Raul Andino. Quasispecies theory and the behavior of RNA viruses.\nPLoS Pathog, 6(7):e1001005, 07 2010.\n\n[MHH+ 11] James I. Mullins, Laura Heath, James P. Hughes, Jessica Kicha, Sheila Styrchak, Kim G.\nWong, Ushnal Rao, Alexis Hansen, Kevin S. Harris, Jean-Pierre Laurent, Deyu Li, Jeffrey H.\nSimpson, John M. Essigmann, Lawrence A. Loeb, and Jeffrey Parkins. Mutation of HIV-1\ngenomes in a clinical population treated with the mutagenic nucleoside kp1461. PLoS ONE,\n6(1):e15135, 01 2011.\n[Mus11]\n\nFabio Musso. A stochastic version of Eigen's model. Bulletin of Mathematical Biology, 73:151\n\u2013 180, 2011.\n\n[NBS+ 98]\n\nMonique Nijhuis, Charles A. B. Boucher, Pauline Schipper, Thomas Leitner, Rob Schuurman,\nand Jan Albert. Stochastic processes strongly influence HIV-1 evolution during suboptimal\nprotease-inhibitor therapy. Proceedings of the National Academy of Sciences, 95(24):14441\u2013\n14446, 1998.\n\n[NS89]\n\nM. Nowak and P. Schuster. Error thresholds of replication in finite populations-mutation frequencies and the onset of Muller's ratchet. J. Theor Biol, 137:375\u2013395, 1989.\n\n[PMnD10] Jeong-Man Park, Enrique Mu\u00f1oz, and Michael W. Deem. Quasispecies theory for finite populations. Phys. Rev. E, 81(1):011902, Jan 2010.\n\n17\n\n\f[SH06]\n\nDavid B. Saakian and Chin-Kun Hu. Exact solutions of the Eigen model with general fitness\nfunctions and degradation rates. PNAS, 103(13):4935 \u2013 4939, 2006.\n\n[SRA08]\n\nDavid B. Saakian, Olga Rozanova, and Andrei Akmetzhanov. Dynamics of the Eigen and the\nCrow-Kimura models for molecular evolution. Phys. Rev. E, 78(4):041908, Oct 2008.\n\n[SS82]\n\nJ. Swetina and P. Schuster. Self-replication with errors: a model for polynucleotide replication.\nBiophys. Chem., 16:329\u2013345, 1982.\n\n[TBVD12] Kushal Tripathi, Rajesh Balagam, Nisheeth K. Vishnoi, and Narendra M. Dixit. Stochastic\nsimulations suggest that HIV-1 survives close to its error threshold. Submitted, 2012.\n[TH07]\n\nNobuto Takeuchi and Paulien Hogeweg. Error-threshold exists in fitness landscapes with lethal\nmutants. BMC Evolutionary Biology, 7(1):15, 2007. A response to Claus Wilke: Quasispecies\ntheory in the context of population genetics, BMC Evol Biol 2005, 5:44.\n\n[vNCM99] Eric van Nimwegen, Japen P. Crutchfield, and Melanie Mitchell. Statistical dynamics of the\nroyal road genetic algorithm. Theoretical Computer Science, 229:41 \u2013 102, 1999.\n[WH96]\n\nG. Woodcock and PG Higgs. Population evolution on a multiplicative single-peak fitness\nlandscape. J. Theor. Biol., 179:61\u201373, 1996.\n\n[Wie97]\n\nThomas Wiehe. Model dependency of error thresholds: the role of fitness functions and contrasts between the finite and infinite sites models. Genetics Research, 69(02):127\u2013136, 1997.\n\n[Wil05]\n\nClaus Wilke. Quasispecies theory in the context of population genetics. BMC Evolutionary\nBiology, 5(1):44, 2005.\n\n[WK93]\n\nG. P. Wagner and P. Krall. What is the difference between models of error thresholds and\nMuller's ratchet? J. Math. Biol., 32:33\u201344, 1993.\n\n18\n\n\fA\n\nStarting State and Transition Matrix of the RSM Markov Chain\n\nAs stated before, the RSM Markov chain starts with the \"fittest\" possible population with all the weight\nconcentrated on the master sequence, so that N0M = N and N0\u03c3 = 0 for all \u03c3 6= M. We now proceed to set up\nsome notation for writing out the transition matrix M .\nDefinition A.1. (Multivariate Geometric distribution). Let g(\u03c3 ) denote the number of genomes of type\n\u03c3 in an urn. Consider the process of choosing, without replacement, N genomes from this urn. Then\nPhyp (g \u2192 f ; N) denotes the probability of obtaining f (\u03c3 ) genomes of type \u03c3 for each \u03c3 . We have,\ndef\n\nPhyp (g \u2192 f ; N) =\n\n\u220f\u03c3 \u2208{0,1}L\n\ng(i) \u0001\nf (i)\n\nhg,1i\u0001\nN\n\n(5)\n\nDefinition A.2. (Multivariate Binomial Distribution). Let f (\u03c3 ) denote the number of genomes of type\n\u03c3 . Consider a stochastic process in which each genome of type \u03c3 independently mutates into a genome \u03c4\n(possibly equal to \u03c3 ) with probability Q(\u03c3 , \u03c4). We denote by Pbin ( f \u2192 D; Q) the probability that D(\u03c3 , \u03c4)\ngenomes of type \u03c3 mutate to type \u03c4 under this process. We have\n\u0012\n\u0013\nf (\u03c3 )\ndef\nbin\nn\no\nP ( f \u2192 D; Q) = \u220f\nQ(\u03c3 , \u03c4)D(\u03c3 ,\u03c4)\n\u220f\nL\n{0,\nD(\u03c3\n,\n\u03c4)|\u03c4\n\u2208\n1}\n\u03c3 \u2208{0,1}L\n\u03c4\u2208{0,1}L\nWe can now write the entries of M . For f , g : {0, 1}L \u2212\u2192 N satisfying h f , 1i = hg, 1i = N, we denote by\nM ( f , g) the conditional probability of obtaining g starting from f in one step of the RSM process. Given a\nfunction f {0, 1}L \u2212\u2192 N, we denote by A f the function such that A f (\u03c3 ) = a\u03c3 f (\u03c3 ). Then, we have\nM ( f , g) =\n\n\u2211\n\nPhyp (A f \u2192 h; N)\n\n\u2211\nD:1D=g;D1=h\n\nh:hh,1i=N\n\nwhere Q and A are as defined above.\n\n19\n\nPbin (h \u2192 D; Q) ,\n\n\fB\n\nProofs Omitted from Section 5\n\nB.1\n\nProofs Omitted from Section 5.1\n\nProof Sketch of Fact 5.1. When \u03bc \u2208 (0, 1) and a\u03c3 > 0 for all \u03c3 , it can be verified easily that this chain is\nirreducible and also has a non-zero self-loop probability at every point in the state space. Thus, the chain\nis ergodic and hence by the Fundamental theorem of Markov chains, has a unique stationary distribution to\nwhich it converges as t \u2192 \u221e.\nWe now give a proof of Lemma 5.2.\nProof of Lemma 5.2. We will show that under class invariance, we can project the RSM Markov chain so\nthat its state space consists of equivalence classes indexed by functions h : [0, L] \u2212\u2192 N with \u2211Li=0 h(i) = N,\nsuch that a function f in the state space of M is in the equivalence class [h] if and only if for every i \u2208 [0, L],\nf (\u03c3 ) = h(i).\n\n\u2211\n\n{\u03c3 \u2208{0,1}L |wH (\u03c3 )=i}\nWe will find it convenient to consider the reproduction and selection phases separately from the mutation\nphase, show that the projection described above can be done for both of them, and then combine the two\nresults using the following general fact about projected Markov chains, the proof of which we include for\ncompleteness.\nFact B.1. Let P and R be the transition kernels of two Markov chains on the same state space \u03a9, and let S\ndenote the composition PR of the two chains. Suppose that there is a partition of \u03a9 into equivalence classes\n\u03a90 , such that for any f \u2261 f 0 , and any equivalence class [g], we have\nP( f , [g]) = P( f 0 , [g]) and R( f , [g]) = R( f 0 , [g]).\nThen, we also have S( f , [g]) = S( f 0 ([g]), for all f , f 0 and g as described above.\nProof. The proof is by direct computation. We have,\nS( f , [g]) =\n\n\u2211 P( f , q0 )R(q0 , [g])\n\nq0 \u2208\u03a9\n\n=\n\n\u2211 \u2211\n\nP( f , q0 )R(q0 , [g])\n\n[q]\u2208\u03a90 q0 \u2208[q]\n\n=\n\nR(q, [g])\n\n\u2211\n\nR(q, [g])P( f , [q])\n\n(6)\n\nR(q, [g])P( f 0 , [q])\n\n(7)\n\n[q]\u2208\u03a90\n\n=\n\nP( f , q0 )\n\n\u2211\n\n\u2211\n\nq0 \u2208[q]\n\n[q]\u2208\u03a9\n\n=\n\n\u2211\n[q]\u2208\u03a9\n\nJust as in the derivation of equation (6) above, we get S( f 0 , [g]) = \u2211[q]\u2208\u03a90 R(q, [g])P( f 0 , [q]), and hence, by\nequation (7), we have S( f 0 , [g]) = S( f , [g]), as claimed.\nIn order to use the last fact, we now decompose the matrix of the RSM process into the following two\nMarkov chains on \u03a9:\n20\n\n\f1. The Reproduce-Select Chain. We denote the transition matrix of this chain as P, such that P( f , g)\nis the probability of obtaining the state g starting from state f after the reproduction and selection\nphases. Notice that\nP( f , g) = Phyp (A f \u2192 g; N) .\nAssume that A is class invariant and let A(i) denote the reproduction rate for a genome of Hamming\nweight i. For an equivalence class [h] of \u03a9 as defined above, we consider the probability P( f , [h]),\nwith f \u2208 [h0 ]. By the definition of Phyp (\u2192; ), this is the probability of drawing h(i) genomes of Hamming weight i for 0 \u2264 i \u2264 L, when N genomes are drawn without replacement from a bag containing\nA(i) \u2211\u03c3 :wH (\u03c3 )=i f (\u03c3 ) = A(i)h0 (i) genomes of weight i. By definition, this probability depends only on\nh and the equivalence class h0 of f , and hence P( f , [h]) = P(g, [h]) when A is class invariant and f \u2261 g.\n2. The Mutation Chain. We will directly write down the entries R( f , [h0 ]) for the probability of obtaining a state in the equivalence class [h0 ] starting from a state f in the equivalence class [h]. We will\nshow now that we can write R( f , [h0 ]) in terms only of h and h0 , and hence R( f , [h0 ]) = R(g, [h0 ]) for\nf \u2261 g. Denote by Qi j the probability that a string \u03c3 of Hamming weight i transforms into some string\nof Hamming weight j in the mutation step, and notice that this probability is well defined because of\nthe definition of the mutation transition matrix Q. Since f \u2208 [h], there are h(i) strings of Hamming\nweight i initially, for 0 \u2264 i \u2264 L. Denote by di j the number of strings of weight i which transform into\nstrings of weight j in the mutation step. Then, we have\n\u0012\n\u0013\nh(i)\nd\n0\nR( f , [h ]) =\n(8)\n\u2211 \u220f \bdi j |0 \u2264 j \u2264 L \u220f Qi ji j .\n0\u2264 j\u2264L\nd:\u2211 di j =h(i) 0\u2264i\u2264L\nj\n\n\u2211i di j =h0 ( j)\n\nSince R( f , [h0 ]) depends only upon h and h0 , we get that R( f , [h0 ]) = R(g, [h0 ]) for f \u2261 g.\nCombining the above two discussions and using Fact B.1, we see that when A is class invariant, the transition\nmatrix M of the RSM process satisfies M ( f , [h0 ]) = M (g, [h0 ]) whenever f \u2261 g. This completes the proof\nof Lemma 5.2.\nB.1.1\n\nRelationships between Error Thresholds\n\nWe first define error thresholds according to various dispersal measures.\nDefinition B.2 (Error Thresholds). Let \u03b5 \u2265 0, and U be the uniform distribution over the set of genomes.\n\b\n\u0002\n\u0003\ndef\n1. \u03bccex,1 (\u03b5, N) = min \u03bc \u2208 (0, 1) : E D\u221e,\u03bc \u2212 U 1 \u2264 \u03b5 .\n\b\n\u0002\n\u0003\ndef\n2. \u03bcch (\u03b5, N) = min \u03bc \u2208 (0, 1) : \u2211\u03c3 wH (\u03c3 ) E D\u03c3\u221e,\u03bc \u2212 2\u2212L \u2211\u03c3 wH (\u03c3 ) \u2264 \u03b5 .\n\b\n\u0002\n\u0003\ndef\n3. \u03bccsh (\u03b5, N) = min \u03bc \u2208 (0, 1) : H(E D\u221e,\u03bc ) \u2212 H(U) \u2264 \u03b5 . Here H denotes the Shannon entropy,\nusing the base e.\nDefinition B.3 (Error Threshold for the Quasispecies Model). Let \u03bc \u2208 (0, 1) and let v\u03bc denote the the\nstationary expected fraction vector with `1 norm 1. The error thresholds are defined as follows.\ndef\n\n1. \u03bccex,1 (\u03b5) = min{\u03bc \u2208 (0, 1) : kv\u03bc \u2212 Uk1 \u2264 \u03b5}.\n\n21\n\n\fdef\n\n2. \u03bcch (\u03b5) = min{\u03bc \u2208 (0, 1) : | \u2211\u03c3 v\u03bc (\u03c3 )wH (\u03c3 ) \u2212 2\u2212L \u2211\u03c3 wH (\u03c3 )| \u2264 \u03b5}.\ndef\n\n3. \u03bccsh (\u03b5) = min{\u03bc \u2208 (0, 1) : |H(v\u03bc ) \u2212 H(U)| \u2264 \u03b5}, where H denotes the Shannon entropy, using the\nbase e.\nOur results are mostly stated in terms of the error threshold \u03bcch . However, we now describe how the different\ndefinitions above are related to each other. The following inequalities relate the different distance measures\nthat we have considered. The first of these follows from the definition of the `1 norm, while the second is\nthe well known Pinsker's inequality relating the `1 norm to the entropy.\n\"\n#\n\u0002\n\u0003\nwH (\u03c3 )\nE \u2211 wH (\u03c3 ) D\u03c3\u221e,\u03bc \u2212 E [\u03c3 \u2190 U] \u2211\n(9)\n\u2264 L E D\u221e,\u03bc \u2212 U 1\n|\u03a9|\n\u03c3\n\u03c3\nq\n\u0003\n\u0002\n\u0002\n\u0003\nE D\u221e,\u03bc \u2212 U 1 \u2264\n(10)\n2 H(E D\u221e,\u03bc ) \u2212 H(U)\nThis gives us the following relationship between the error-thresholds:\n\u03bcch (\u03b5, N) \u2264 \u03bccex,1 (\u03b5/L, N)\n\u03bccex,1 (\u03b5, N) \u2264 \u03bccsh (\u03b5 2 /2, N)\n\n(11)\n\nUsing the fact that the distributions involved are defined over a state space of size 2L , we can show the\nfollowing weak converse to inequality (10):\n\u0002\n\u0003\n\u0002\n\u0003\n2\nH(E D\u221e,\u03bc ) \u2212 H(U) \u2264 2L E D\u221e,\u03bc \u2212 U 1 .\nThis gives us a further relationship between the error thresholds:\n\u03bccex,1 (\u03b5, N) \u2265 \u03bccsh (2L \u03b5 2 , N)\nHowever, we notice that one cannot in general close the loop in inequalities (9) and (10) (and hence in\ninequalities (11)) above. To see this, consider for example the following two distributions P and Q for\nL > 1.\n1. P: puts total weight 1 \u2212 \u03b5 on weight 1 strings and weight \u03b5 on the string 0, so that the average\nHamming weight is 1 \u2212 \u03b5.\n2. Q: puts total weight (1 \u2212 \u03b5)/L on weight L strings, and weight 1 \u2212 (1 \u2212 \u03b5)/L the string 0, so that the\naverage Hamming weight is still 1 \u2212 \u03b5.\nThe average Hamming weight in both cases is 1 \u2212 \u03b5, so that in that metric, the distance between P and Q is\nzero. However, the total variation distance between P and Q is at least 1 \u2212 \u03b5.\n\nB.2\n\nProof of Theorem 5.6\n\nIn the rest of this section, we will use the following concentration inequalities about the multivariate hypergeometric distribution:\nFact B.4. Consider the hypergeometric distribution Phyp (g \u2192 f ; N) defined in equation (5) above. Let D\u03c3\nbe the random variable denoting the fraction of genomes of type \u03c3 which are drawn in the process starting\nwith g(\u03c4) genomes of each type \u03c4. We then have:\n22\n\n\f1. E [D\u03c3 ] =\n\ng(\u03c3 )\nhg,1i .\n\n2. The following concentration inequality holds for \u03b5 \u2265 0:\n\u0001\nP [|D\u03c3 \u2212 E [D\u03c3 ] | > \u03b5] \u2264 2 exp \u2212\u03b5 2 N .\nSimilarly for the multivariate binomial distribution, we have the following:\nFact B.5. Consider N genomes with f (\u03c3 ) genomes of each type \u03c3 . Let D\u03c3 be the random variable denoting\nthe fraction of genomes of type \u03c3 after a mutation step. Then\n1. E [D\u03c3 ] =\n\n1\nN\n\n\u2211\u03c3 f (\u03c4)Q\u03c4\u03c3 .\n\n2. The following concentration inequality holds for \u03b5 \u2265 0:\nP [|D\u03c3 \u2212 E [D\u03c3 ] | > \u03b5] \u2264 2 exp \u22122\u03b5 2 N\n\n\u0001\n\nFact B.4 is a consequence of Azuma's inequality, and a proof can be found in the book by Dubhashi and\nPanconesi [DP09]. Fact B.5 is essentially a restatement of the Chernoff-Hoeffding bound. Combining the\nabove bounds, we can deduce the following concentration inequality for each step of the RSM process:\nLemma B.6. Consider a state Nt of the RSM process. We then have\n\u0002 \u03c3\n\u0003\n\u03c3\n1. E Dt+1\n|Nt = (DhDt AQ)\n= r\u03c3 (Dt ), with r\u03c3 as defined in equation (2).\nt ,Ai\n2. Let \u03b51 and \u03b52 be arbitrary\npositive constants.\nThen with probability\n\u0001\n\u0001\n\u0002\n\u0003 (conditional on Nt ) at least\n\u03c3 \u2212 E D\u03c3 |N | \u2264 (\u03b5 + \u03b5 ) for every \u03c3 . In\n1 \u2212 22L+1 (exp \u2212\u03b512 N + exp \u22122\u03b522 N ), we have |Dt+1\n1\n2\nt+1 t\nparticular, choosing\u0001 \u03b51 = \u03b52 = \u03b5/2, we\u0002get that \u0003with probability (conditional on Nt ) at least 1 \u2212\n\u03c3 \u2212 E D\u03c3 |N | \u2264 \u03b5 for every \u03c3 .\n22L+2 exp \u2212\u03b5 2 N/4 , we have |Dt+1\nt+1 t\nProof. For ease of notation, let g\u03c3 = a\u03c3 Nt\u03c3 . Let I \u03c3 be the random variable denoting the fraction of genomes\nof type \u03c3 left after the selection step. Thus, we have\nE [I \u03c3 |Nt ] =\n\n\u0002 \u03c3\n\u0003\ng\u03c3\nand E Dt+1\n|I, Nt = \u2211 I \u03c4 Q\u03c4,\u03c3 .\nhg, 1i\n\u03c4\n\nUsing a union bound over all genome types\n\u0001 with the concentration inequality in Fact B.4, we get that\nwith probability at least 1 \u2212 2L+1 exp \u2212\u03b512 N conditioned on Nt , we have\nI\u03c3 \u2208\n\ng\u03c3\n\u2264 \u03b51 , for all \u03c3 .\nhg, 1i\n\nWe denote the above event by E . Now, we consider the concentration of Dt conditioned on I. Using a union\nbound over all genome types\n\u0001 along with the concentration inequality in Fact B.5, we get with probability at\nleast 1 \u2212 2L+1 exp \u22122\u03b522 N conditioned on I, we have\n\u03c3\nDt+1\n\u2212 \u2211 I \u03c4 Q\u03c4\u03c3 \u2264 \u03b52 , for all \u03c3 .\n\u03c4\n\n23\n\n\f\u0001\n\u0001\nWe denote the above event by F . With probability at least 1 \u2212 22L+1 (exp \u2212\u03b512 N + exp \u22122\u03b522 N ), conditioned on Nt , both E and F occur, and then we have, for all \u03c3 ,\n\u0002 \u03c3\n\u0003\n\u03c3\nDt+1\n\u2212 E Dt+1\n|Nt\n\n=\n\ng\u03c4\n\n\u03c3\n\u2212\n)\n\u2211 Q\u03c4\u03c3 (Dt+1\nhg, 1i\n\u03c4\n\n\u2264\n\n\u03c3\nDt+1\n\u2212 \u2211 I \u03c4 Q\u03c4\u03c3 + \u2211 Q\u03c4\u03c3 I \u03c4 \u2212\n\u03c4\n\n\u03c4\n\ng\u03c4\nhg, 1i\n\n= \u03b52 + \u03b51 \u2211 Q\u03c4\u03c3 = \u03b51 + \u03b52 ,\n\u03c4\n\nwhich is what we sought to prove.\nBefore proceeding, we need the following lemma:\nLemma B.7. Fix a fitness landscape A with positive entries and a mutation transition matrix Q with \u03bc < 1/2.\nThe functions r\u03c4 defined in equation (2) are Lipschitz with Lipschitz constant\n\u0001\nmax\u03c4 a\u03c4\n(1 \u2212 \u03bc)L \u2212 \u03bc L\nK=\nmin\u03c4 a\u03c4\non the set of probability distributions over genomes.\nProof. For a probability distribution x over genomes, we have\n0\n\n\u2202 r\u03c3 (x)\n\u2202 x\u03c3\n\n=\n\u2264\n\n0\na\u03c3\nQ\u03c3 \u03c3 0 \u2212 r\u03c3 (x)\n\u2211\u03c4 a\u03c4 x\u03c4\n\u0001\nmax\u03c4 a\u03c4\n(1 \u2212 \u03bc)L \u2212 \u03bc L ,\nmin\u03c4 a\u03c4\n0\n\nwhere the last line follows by noticing that fact that for all x and all \u03c3 0 , min\u03c3 ,\u03c4 Q\u03c3 \u03c4 \u2264 r\u03c3 (x) \u2264 max\u03c3 ,\u03c4 Q\u03c3 \u03c4 ,\nand min\u03c3 \u03c4 Q\u03c3 \u03c4 = \u03bc L , while max\u03c3 \u03c4 Q\u03c3 \u03c4 = (1 \u2212 \u03bc)L . Thus, by the mean value theorem, for any probability\ndistributions x and y over genomes, we get\n|r(x) \u2212 r(y)| \u2264 K ||x \u2212 y||1 .\n\nProof (of Theorem 5.6). Fix a time t0 . In the rest of the proof, we drop the conditioning on the initial state\nbeing concentrated on the master sequence for ease of notation. We will prove the following claim by\ninduction for 0 \u2264 t \u2264 t0 :\nClaim B.8. For every \u03c3 \u2208 {0, 1}L and 0 \u2264 t \u2264 t0 , there exist lt\u03c3 , ut\u03c3 and pt satisfying the conditions\ndef\n\n1. 0 \u2264 lt\u03c3 \u2264 ut\u03c3 \u2264 1, and st = max\u03c3 ut\u03c3 \u2212 lt\u03c3 and pt are oN (1). Also, mt\u03c3 lies in the interval [lt\u03c3 , ut\u03c3 ].\n2. With probability at least 1 \u2212 pt , Dt\u03c3 lies in the interval [lt\u03c3 , ut\u03c3 ] for all \u03c3 .\nWe first see how to finish the proof of Theorem 5.6 assuming Claim B.8. From item 2 in Claim B.8, and\nusing mt\u03c3 \u2208 [lt\u03c3 , ut\u03c3 ] ,we get\n\u0002 \u0003\n|E Dt\u03c30 \u2212 mt\u03c30 | \u2264 pt0 + (1 \u2212 pt0 )|ut\u03c30 \u2212 lt\u03c30 |, for all \u03c3 .\n(12)\nNow item 1 of the claim implies that the right hand side of equation (12) goes to 0 as N \u2192 \u221e, which\nconcludes the proof of Theorem 5.6, assuming Claim B.8.\n24\n\n\fWe now proceed to prove Claim B.8.\nProof of Claim B.8. At t = 0, we can set lt\u03c3 = ut\u03c3 = mt\u03c3 , and pt = 0. By the definition of the starting state,\nthis satisfies the conditions claimed in the claim. Now suppose that we have shown that with probability\n1 \u2212 pt , we have Dt\u03c3 \u2208 [lt\u03c3 , ut\u03c3 ] for all \u03c3 . We call the latter event Et . Recall that\n\u0002 \u03c3\n\u0003\nE Dt+1\n|Dt = r\u03c3 (Dt ),\nand define\nl 0t+1 =\n\u03c3\n\nmin\n\n{y|y\u03c3 \u2208[lt\u03c4 ,ut\u03c4 ]}\n\nr\u03c3 (y); u0t+1 =\n\u03c3\n\nmax\n\n{y|y\u03c4 \u2208[lt\u03c4 ,ut\u03c4 ]}\n\nr\u03c3 (y)\n\n\u0002\n\u0003\n\u03c3 \u2208 l 0 \u03c3 , u0 \u03c3\n\u03c3\nNotice that mt+1\nt+1\nt+1 . Also, because of the Lipschitz condition on the function r shown in\n\u03c3\n\u03c3\nL\n0\n0\nLemma B.7, we have u t+1 \u2212 l t+1 \u2264 2 Kst . Now, we condition on the event Et defined above, and in this\ncase, we have\n\u0002 \u03c3\n\u0003 \u0002 \u03c3\n\u03c3 \u0003\nE Dt+1\n|Et \u2208 l 0t+1 , u0t+1 , for all \u03c3 .\n\u03c3 = l 0 \u03c3 \u2212 \u03b5/2, u\u03c3 = u0 \u03c3 + \u03b5/2. Using the concentration\nChoose \u03b5(N) = N \u22121/3 = oN (1), and set lt+1\nt+1\nt+1\nt+1\nresult quoted in Lemma B.6, we get that conditioned on Et , with probability at least 1 \u2212 p(N) where p(N) =\nexp(\u2212\u03a9(N 1/3 )) = oN (1),\n\u0002\u03c3\n\u0003\n\u03c3\n\u03c3\nDt+1\n\u2208 lt+1\n, ut+1\n, for all \u03c3 .\n\nNow, we saw above that Et occurs with probability at least 1 \u2212 pt . Hence, by a union bound, we get that\nwith probability at least 1 \u2212 pt+1 , where pt+1 = pt + p(N),\n\u0002\u03c3\n\u0003\n\u03c3\n\u03c3\nDt+1\n\u2208 lt+1\n, ut+1\n, for all \u03c3 .\nThis proves the induction hypothesis, except that we need to make sure that st , pt are oN (1). We first consider\nst . From above, we have the following recurrence for st :\nst+1 \u2264 2L Kst + \u03b5(N); s0 = 0.\n\n(13)\n\nThis satisfies st = ON (\u03b5) = oN (1) for all t \u2264 t0 , by the choice of \u03b5. Similarly, we have pt = t p(N) = oN (1)\nfor t \u2264 to by the choice of p(N). This proves Claim B.8.\n\nB.3\n\nProof of Corollary 5.7\n\nWe begin by noticing that for 0 < \u03bc < 1/2, we can choose a time t0 such that for t > t0 , the state mt of the\nquasispecies model satisfies\nd h (mt , U) \u2212 d h (v\u03bc , U) \u2264 \u03b4 /2,\n(14)\nwhere v is the unique stationary vector of the quasispecies model. Now fix t > t0 . Since the distance function\nd h is continuous, Theorem 5.6 allows us to choose an N\u03b4 such that for N > N\u03b4 ,\n\u0002\n\u0003\nd h (mt , U) \u2212 d h (E Dt,\u03bc,N , U) \u2264 \u03b4 /2.\n(15)\nCombining equations (14) and (15), we get\n\u0002\n\u0003\nd h (v, U) \u2212 d h (E Dt,\u03bc,N , U) \u2264 \u03b4 .\nThus, when \u03bc < \u03bcch (\u03b5), we have\n\u0002\n\u0003 \u0001\nd h E Dt,\u03bc,N , U \u2265 \u03b5 \u2212 \u03b4 , when \u03bc < \u03bcch (\u03b5), and,\nand when \u03bc = \u03bcch (\u03b5),\n\n\u0002\n\u0003 \u0001\nd h E Dt,\u03bc,N , U \u2264 \u03b5 + \u03b4 when \u03bc = \u03bcch (\u03b5).\n25\n\n\fB.4\n\nProof of Theorem 5.8\n\nBefore proving Theorem 5.8, we first set up some notation for the coupling argument.\nDefinition B.9. A coupling of two probability distributions D1 and D2 is a pair of random variables (X,Y )\ndefined on a single probability space such that the marginal distribution of X is D1 and the marginal distribution of Y is D2 .\n\u221e\nDefinition B.10. A coupling of Markov chains with transition matrix M is defined to be a process (Xt ,Yt )t=0\nwith the property that both (Xt ) and (Yt ) are Markov chains with transition matrix M , although the two\nchains may possibly have different starting distributions.\n\nAny coupling of Markov chains with transition matrix M can be modified so that the two chains stay\ntogether at all times after their first simultaneous visit to a single state: more precisely, such that if Xs = Ys ,\nthen Xt = Yt for t \u2265 s. In the following, we only consider such couplings. The following well known facts\nare the basis of coupling based methods for proving mixing time bounds.\nTheorem B.11. Let {(Xt ,Yt )} be a coupling satisfying the definition above for which X0 = \u03b1 and Y0 = \u03b2 .\ndef\n\nLet \u03c4couple be the first time the chains meet: \u03c4couple = min{t : Xt = Yt }. Then\n\u0002\n\u0003\nkM t (\u03b1, *) \u2212 M t (\u03b2 , *)kTV \u2264 P \u03c4couple > t|X0 = \u03b1,Y0 = \u03b2 .\nLemma B.12 (Coupling Lemma). Let X,Y be random variables defined on a finite sample space \u03a9 and let\nC be any coupling of C and Y. Then\nmin PC [X 6= Y ] = kX \u2212Y kTV .\nC\n\nDefinition B.13. Let d : \u03a9 \u00d7 \u03a9 \u2212\u2192 R\u22650 be a distance metric on the state space \u03a9 of the two Markov chains\n{Xt }t and {Yt }t . Suppose C is a coupling such that for every t \u2265 0,\nE [d(Xt+1 ,Yt+1 )] \u2264 \u03b8 * E [d(Xt ,Yt )]\nfor every starting distributions X0 ,Y0 , then we call C a (\u03b8 , d) coupling. Note that this implies that\nE [d(Xt ,Yt )] \u2264 \u03b8 t * D,\nwhere D = max\u03c3 ,\u03c4\u2208\u03a9 d(\u03c3 , \u03c4).\nFix a integer valued distance function d. Let let {Xt }t be a realization of the Markov chain starting from X0\nand Yt be another realization starting from the stationary distribution \u03c0 of the Markov chain. If C is a (\u03b8 , d)\ncoupling, then it follows from the Coupling Lemma that\nCoupling\n\nMarkov\n\nd integral\n\n= P [d(Xt ,Yt )] \u2265 1] \u2264 E [d(Xt ,Yt )] \u2264 \u03b8 t * D.\n\u0010\n\u0011\n(D/\u03b5)\nThis implies that the mixing time \u03c4mix (\u03b5) = O log\nlog (1/\u03b8 ) when \u03b8 < 1.\nkXt \u2212 \u03c0kTV\n\nB.4.1\n\n\u2264\n\nP [Xt 6= Yt ]\n\nA Coupling for the RSM Process\n\nThe coupling C we will construct will have two independent parts C = (CS , CM ), CS for the ReproduceSelect phase and CM for the mutation part. We first describe the somewhat simpler mutation coupling.\n26\n\n\fMutation Coupling CM . Let Xt = {\u03c31 , . . . , \u03c3N } and Yt = {\u03bd1 , . . . , \u03bdN }. Let M denote an arbitrary permutation on [N] such that M(i) denotes the image of i \u2208 [N]. We define the distance between the states\nas\n(\n)\nN\n\u0001\ndef\ndmatch (Xt ,Yt ) = min \u2211 dH \u03c3i , \u03bdM(i) .\nM\n\ni=1\n\nThe mutation coupling follows the following algorithm, with M set to be the permutation which achieves\nthe minimum in the above definition.\n1. For i = 1, . . . , N\n(a) For j = 1, . . . , L\ni. Choose independently and uniformly at random r j from [0, 1].\nii. If \u03c3i ( j) = \u03bdM(i) ( j)\nA. Flip \u03c3i ( j) and \u03bdM(i) ( j) if and only if r j \u2265 1 \u2212 \u03bc.\niii. Else\ndef\ndef\nA. Let r j,1 = r j and r j,2 = 1 \u2212 r j .\nB. Flip \u03c3i ( j) if and only if r j,1 \u2265 1 \u2212 \u03bc.\nC. Flip \u03bdM(i) ( j) if and only if r j,2 \u2265 1 \u2212 \u03bc.\nLemma B.14. For \u03bc \u2264 1/2, CM is a ((1 \u2212 2\u03bc), dmatch ) coupling.\nProof. To prove that CM is a valid coupling one just needs to note that if r is distributed uniformly at random\nin [0, 1] then so is 1 \u2212 r. Hence, for i = 1, . . . , N and j = 1, . . . , L, each bit \u03c3i ( j) (respectively, \u03bdi ( j))) flips\nwith probability exactly \u03bc. Further, these flips are independent by construction.\nTo prove that CM is a ((1 \u2212 2\u03bc), dmatch ) coupling, let Xt ,Yt be the states of the two Markov chains with\ndef\n\ndistance d = dmatch (Xt ,Yt ). By definition of dmatch , there is some matching M ? which achieves d. Without\nloss of generality assume that M ? is identity, i.e., M ? (i) = i, for all 1 \u2264 i \u2264 N. Hence, \u2211Ni=1 dH (\u03c3i , \u03bdi ) = d.\ndef\n\ndef\n\nt+1\n) be the output\nof CM on input (\u03c31 , . . . , \u03c3N ) and\nLet Xt+1 = (\u03c31t+1 , . . . , \u03c3Nt+1 ) and Yt+1 = (\u03bd1t+1\n\u0002 ,N. . . , \u03bdN t+1\n\u0003\n(\u03bd1 , . . . , \u03bdN ) respectively. We will calculate E \u2211i=1 dH (\u03c3i , \u03bdit+1 ) and show that it is exactly (1 \u2212 2\u03bc) * d.\nHence, dmatch (Xt+1 ,Yt+1 ) \u2264 (1 \u2212 2\u03bc) * d, as dmatch is defined as the minimum over all possible matchings.\nBy linearity of expectation it is sufficient to show that for all i = 1, . . . , N,\n\u0003\n\u0002\nE dH (\u03c3it+1 , \u03bdi (t + 1) = (1 \u2212 2\u03bc) * dH (\u03c3i , \u03bdi ).\n\nAgain by linearity of expectation it is sufficient to show the following:\n\u0002\n\u0003\nEr j dH (\u03c3it+1 ( j), \u03bdit+1 ( j)) = (1 \u2212 2\u03bc) * dH (\u03c3i ( j), \u03bdi ( j)).\n\u0002 t+1\n\u0003\nt+1\nThis follows from observing\nthat\nif\n\u03c3\n(\nj)\n=\n\u03bd\n(\nj),\nthen\nP\n\u03c3\n(\nj)\n=\n\u03bd\n(\nj)\ni\ni\ni\ni\n\u0002\n\u0003\n\u0002\n\u0003 = 1, while if \u03c3i ( j) 6= \u03bdi ( j),\nthen, as \u03bc \u2264 1/2, P \u03c3it+1 ( j) = \u03bdit+1 ( j) = 2\u03bc. Hence, P \u03c3it+1 ( j) 6= \u03bdit+1 ( j) = 1 \u2212 2\u03bc. This completes the\nproof.\n\n27\n\n\fCoupling CS for the Selection Process. We again consider two states Xt = {\u03c31 , \u03c32 , . . . , \u03c3N } and Yt =\n{\u03bd1 , \u03bd2 , . . . , \u03bdN }. Our distance function is still dmatch defined above. We first note the dmatch (*, *) is actually\na metric.\nLemma B.15. dmatch (*, *) is a metric.\nProof. By construction dmatch (X,Y ) \u2265 0 with equality if and only if X = Y . Now consider states X =\n{\u03c3i }Ni=1 ,Y = {\u03c4i }Ni=1 and Z = {\u03bdi }Ni=1 in the state space \u03a9. Let \u03b1 and \u03b2 be permutations of [N] such that\nN\n\nN\n\ndmatch (X,Y ) = \u2211 dH \u03c3i , \u03c4\u03b1(i)\ni=1\n\n\u0001\n\nand dmatch (Y, Z) = \u2211 dH \u03c4i , \u03bd\u03b2 (i)\n\n\u0001\n\ni=1\n\nNow we have\nN\n\ndmatch (X, Z) \u2264\n\n\u2211 dH\n\n\u03c3i , \u03bd\u03b2 (\u03b1(i))\n\n\u0001\n\ni=1\nN\n\n\u2264\n\n\u2211\n\n\u0001\n\u0001\u0001\ndH \u03c3i , \u03c4\u03b1(i) + dH \u03c4\u03b1i + \u03bd\u03b2 (\u03b1(i))\n\ni=1\n\n= dmatch (X,Y ) + dmatch (Y, Z) .\n\nWe will use the following general path coupling result of Bubley and Dyer [BD97] to define the coupling\nCS .\nTheorem B.16 (Path Coupling [BD97]). Consider a Markov chain M on state space \u03a9 and a distance\nfunction d on \u03a9 such that d 0 (x, x) = 0 for all x \u2208 \u03a9. Consider a connected undirected graph G on \u03a9 such\nthat the length of each edge {x, y}, if present in G, is d(x, y), and let d 0 be the shortest path metric on G.\nSuppose there exists a coupling C for M such that for some \u03b1 < 1, and all Xt ,Yt \u2208 \u03a9 which are adjacent in\nG,\nEC [d(Xt+1 ,Yt+1 )|Xt ,Yt ] \u2264 \u03b1d(Xt ,Yt ).\nIf every edge of G is a shortest path under the metric d 0 described above, then the coupling C can be\nextended to a coupling C 0 such that\n\u0002\n\u0003\nEC 0 d 0 (Xt+1 ,Yt+1 |Xt ,Yt \u2264 \u03b1d 0 (Xt ,Yt ).\nfor all Xt ,Yt \u2208 \u03a9.\nWe will first show now that the path metric resulting from an application of the above theorem to dmatch\nis dmatch itself, since this is crucial for composing the CS coupling with the coupling CM described above.\nLemma B.17. Consider the state space \u03a9 of the RSM Markov chain M . Let G be the graph on \u03a9 in which\ntwo vertices X and Y are adjacent if and only if dmatch (X,Y ) = 1. Then the path metric d 0 constructed in\nTheorem B.16 is identical with d, and each edge in G is a shortest path.\nProof. For brevity we will denote dmatch (*, *) by d. Notice that since each edge is of length 1, it is also a\nshortest path by construction. Since d is a metric, we also have d 0 (X,Y ) \u2265 d(X,Y ) for all X,Y \u2208 \u03a9. We\nnow proceed by induction to show that d(X,Y ) \u2265 d 0 (X,Y ) for all X,Y \u2208 \u03a9. Notice that when d(X,Y ) = 1,\n28\n\n\fthis is true by definition of d 0 . Now, suppose that d(X,Y ) \u2264 k \u2212 1 implies d 0 (X,Y ) \u2264 d(X,Y ), and consider\nthe case d(X,Y ) = k > 1. We claim that there exists a Z such that d(X, Z) \u2264 k \u2212 1 and d(Y, Z) \u2264 1. The\nexistence of such a Z implies using the induction hypothesis that\nd 0 (X,Y ) \u2264 d 0 (X, Z) + d 0 (Z,Y )\n\u2264 d(X, Z) + d(Z,Y ) \u2264 k = d(X,Y ).\nIt only remains to construct such a Z. Let X = {\u03c3i }Ni=1 and Y = {\u03bdi }Ni=1 . Without loss of generality, we\nmay assume that d(X,Y ) = \u2211Ni=1 dH (\u03c3i , \u03bdi ). Since d(X,Y ) = k > 1, there exists a j such that dH (\u03c3 j , \u03bd j ) \u2265 1.\nLet s be a string obtained by flipping a single bit of \u03bd j such that dH (\u03c3 j , s) = dH (\u03c3 j , \u03bd j ) \u2212 1. Now let\nZ = {\u03c4i }Ni=1 , where \u03c4i = \u03bdi for i 6= j and \u03c4 j = s. By construction, d(Y, Z) \u2264 1 and d(X, Z) \u2264 k \u2212 1.\n\nClaims about the Coupling. Suppose we find a coupling C for the selection phase such that when\ndmatch (Xt ,Yt ) = 1, then the intermediate states I(Xt ) and I(Yt ) satisfy\nE [dmatch (I(Xt ), I(Yt )) |Xt ,Yt ] \u2264 \u03b1,\nthen using Theorem B.16 and the coupling for the mutation phase described above, we get\nE [dmatch (Xt+1 ,Yt+1 |Xt ,Yt )] \u2264 \u03b1(1 \u2212 2\u03bc)dmatch (Xt ,Yt ) .\nThis will give us fast mixing as long as \u03b1(1 \u2212 2\u03bc) < 1.\nWe now describe such a coupling for the selection process, for general Xt and Yt , which we will analyze\nonly in the simple but sufficient case when dmatch (Xt ,Yt ) = 1. Suppose that Xt and Yt contain, respectively,\nnx\u03c3 and ny\u03c3 genomes of type \u03c3 . After reproduction, the number of genomes of type \u03c3 in the two chains\nis a\u03c3 nx\u03c3 and a\u03c3 ny\u03c3 respectively. Let the total number of genomes be Mx = \u2211\u03c3 a\u03c3 nx\u03c3 and My = \u2211\u03c3 a\u03c3 ny\u03c3\nrespectively. Without loss of generality let us assume that Mx \u2265 My , and set M = Mx .\n\u0001\nWe now construct a bag of M balls as follows. For each \u03c3 , the bag has exactly a\u03c3 min nx\u03c3 , ny\u03c3 balls\nwith label (\u03c3 , (x, y)). If nx\u03c3 \u2265 ny\u03c3 , then the bag has exactly a\u03c3 (nx\u03c3 \u2212 ny\u03c3 ) balls with label (\u03c3 , x), otherwise it\nhas exactly a\u03c3 (ny\u03c3 \u2212 nx\u03c3 ) balls with label (\u03c3 , y). Thus the total number of balls in the bag is M.\nWe now take a random permutation of the M balls, and take the intermediate state IX (respectively, IY )\nto be the multiset of genomes given by the first N balls carrying the label (x, y) or x (respectively, (x, y) or\ny). Notice that a ball carrying a label (x, y) can contribute a genome to both IX and IY .\nClaim B.18. The above coupling is a valid Markovian coupling for the selection phase of the RSM chain.\nProof. Notice that sampling without replacement a objects from a set of b objects is equivalent to taking the\nfirst a elements from a uniform random permutation of the b objects. Also note that given a subset S of a set\nof b objects, and a uniform random permutation \u03b1 over the b objects, the restriction of \u03b1 to the elements of S\nis a uniformly random permutation of the elements of S. Now consider the set of M labeled balls constructed\nabove, and define SX (respectively, SY ) to be the set of balls carrying a (x, y) or x (respectively, x(x, y) or y)\nlabel. By the observations above, see that the set IX (respectively, IY ) has the same distribution as if it was\nsampled without replacement from SX (respectively, SY ). This proves the claim.\nLemma B.19. Suppose dmatch (Xt ,Yt ) = 1. Then under the above coupling, we have\nE [dmatch (IX , IY ) |Xt ,Yt ] \u2264\n\n29\n\n1 max a\u03c3\nL.\n1 \u2212 N1 min a\u03c3\n\n\fProof. For brevity, we will denote Xt and Yt as X = {\u03c3i }Ni=1 and Y = {\u03bdi }Ni=1 and dmatch (*, *) by d. Since\nd(X,Y ) = 1, we can assume without loss of generality that \u03c3i = \u03bdi for i > 1, and \u03c31 and \u03bd1 differ in exactly\none bit.\nWe now consider the coupling described above, and let S be the set of balls with label (x, y). Notice that\n|S| = \u22111<i\u2264N a\u03c3i . Notice that |S| \u2265 (N \u2212 1) min\u03c3 a\u03c3 . We assume without loss of generality that a\u03c31 > a\u03bd1 ,\nso that the number of balls M in the bag is |S| + a\u03c31 . Consider a random permutation \u03b1 of the M balls, and\nlet I be the (random) multiset of balls with label (x, y) occurring in the first N positions of \u03b1. Notice that\n|IX \u2229 IY | \u2265 |I|. We observe that if the intersection of IX and IY (seen as multisets) is of size at least |I|, then\ndmatch (IX , IY ) \u2264 L(N \u2212 |I|), hence we have\nE [dmatch (IX , IY ) |X,Y ] \u2264 L(N \u2212 E [|I| |X,Y ]).\n\n(16)\n\nNow, we have\n|S|\nN\n|S| + a\u03c31\n\u0012\n\u0013\n\u0012\n\u0013\na\u03c31\na \u03c31\n\u2265 N 1\u2212\n\u2265 N 1\u2212\n.\n|S|\n(N \u2212 1) min\u03c4 a\u03c4\n\nE [|I| |X,Y ] =\n\nPlugging this into equation (16), we get\nE [dmatch (IX , IY ) |X,Y ] \u2264\n\u2264\n\nLNa\u03c31\n(N \u2212 1) min\u03c4 a\u03c4\n1 max\u03c4 a\u03c4\nL,\n1 \u2212 N1 min\u03c4 a\u03c4\n\nwhich establishes our claim.\nUsing the above discussion, we get fast mixing under the condition that\n(1 \u2212 2\u03bc)\n\n1 max\u03c3 a\u03c3\nL < 1.\n1 \u2212 N1 min\u03c3 a\u03c3\n\nFormally, we have\nTheorem B.20. Fix a mutation rate \u03bc < 1/2, and a fitness landscape A. Define\nK(A, \u03bc) = (1 \u2212 2\u03bc)\nWhen K(A, \u03bc) < 1, we have \u03c4mix (\u03b5) = O\n\n\u0010\n\nlog(NL/\u03b5)\nlog(1/K)\n\n1 max\u03c3 a\u03c3\nL.\n1 \u2212 N1 min\u03c4 a\u03c4\n\n\u0011\n.\n\nWe note here that the main difficulty in designing coupling arguments for the RSM process is the distance\nexpansion property of the reproduction and selection phases. Given two states of the RSM process, the\nreproduction phase amplifies the distance between them, and the nature of the selection phase tends to\nkeep this distance intact. In this setting, the chain can be described as a noisy random walk on a boolean\nhypercube, and our bound reflects the intuition that when the noise is small, the fast mixing property of\nthe hypercube should able to enforce fast mixing of the RSM chain. We reemphasize that we consider that\nachieving a better understanding of the mixing properties of the RSM walk, in terms of both upper and lower\nbounds appears to be an interesting and challenging open problem.\n30\n\n\fB.5\n\nProof of Theorem 5.9\n\nWe now proceed to prove Theorem 5.9. We will use notation similar to that used in the proof of Lemma\n5.2 in Appendix B.1. By a slight abuse of notation, we will use P and R for the transition matrices of the\nprojected versions of the chains described in the proof\n\u0001 of Lemma 5.2 above. We note that for any numbers M\nM\n0\n0\n0\nand M1 , M2 , . . . , Ml summing up to M, M0 ,M0 ,...,M0 is of size at most O(M log M) bits and can be computed\n1\n2\nl\nin at most O(M) operations over numbers of size at most O(M log M). We start with an estimation of the\ntime required to compute the entries of P. Using the above observation, and the form of entries of P, we\nhave the following observation for P.\nObservation B.21. Each entry of P is of size at most \u00d5(N max a\u03c3 ), and can be computed in N max a\u03c3\noperations over integers of size \u00d5(N max a\u03c3 ).\nWe now proceed to estimate the complexity of computing entries of the matrix R. We first get a bound\non the time required to pre-compute the (L + 1) \u00d7 (L + 1) matrix Q defined in the description of the mutation\nchain in Appendix B.1.\nObservation B.22. Let b be the number of bits required to represent \u03bc. We have\nQst = (1 \u2212 \u03bc)L\n\n\u0012\n\n\u03bc\n1\u2212\u03bc\n\n\u0013s\u2212t\n\n\u00132x\n\u0012\n\u0013\u0012\n\u0013\u0012\nL\u2212s\ns\n\u03bc\n.\n\u2211 x\n1\u2212\u03bc\nt \u2212x\nx\n\nHence, each entry of Q is of size at most \u00d5(bL) and can be pre-computed in time O(L2 ) operations over\nnumbers of size \u00d5(bL).\nProof of Theorem 5.9. Notice that the number of terms in the sum in equation (8) for the computation of the\nentry of the matrix R([h], [h0 ]) is at most\nL\n\n\u0012\n\u0013\n\u0010 e \u0011L(L+1) L\nh(i) + L\n\u2264\n\u220f L\n\u220f(h(i) + L)L\nL\ni=0\ni=0\n\u0013\u0013L(L+1)\n\u0012 \u0012\nN\n, by the AM-GM inequality.\n\u2264\ne 1+\nL(L + 1)\nWe will use the shorthands S =\n\nN+L\nL\n\n\u0001\n\n\u2264\n\n(N+L)L\nL!\n\nand\n\n\u0012 \u0012\nG = e 1+\n\nN\nL(L + 1)\n\n\u0013\u0013L(L+1)\n.\n\nNotice that R is of dimension at most S \u00d7 S. Computing the products of all the Qi j 's in each of these terms\ntakes N multiplications on numbers of size at most \u00d5(bL), and hence produces a number of size \u00d5(NbL)\nin time at most \u00d5(NbL). Computing the products of all the multinomial coefficients in each of the terms\ntakes O(N) multiplications on integers of size log N, thus producing an integer of size O(N log N) in time\n\u00d5(N log N). The total size of each entry of R is thus at most s1 = \u00d5 (log G + N log N + NbL), and the entry\ncan be computed in time t1 = \u00d5(Gs1 ). All the entries of R can thus be computed in time S2t1 .\nNotice that Mw = PR, and given the above estimates on the sizes of the entries of P and R and the times\nrequired to compute their entries, each entry of Mw is of size at most s2 = O(log S + s1 + N max a\u03c3 ), and\n\n31\n\n\fcan be computed in time \u00d5(Ss2 ), given the entries of P and R. Thus, the time taken for the computation of\nMw , including the computation of R and P and the pre-computation of Q is\n\u0001\nT1 = \u00d5 bL3 + S3 s2 + S2 Gs1\nand each entry of Mw is of size\n\ns2 = \u00d5(NbL + N max a\u03c3 + L2 ).\n\nThe time required to compute an exact solution of Mw \u03c0w = \u03c0w with the restriction ||\u03c0w ||1 = 1 using Gaussian\nelimination is thus of the order \u00d5(S3 s2 ). Comparing this with T1 , we get that the total running time is \u00d5(T1 ),\nwhich is what we sought to prove.\n\nB.6\n\nProof of Theorem 5.10\n\nI NPUT: An initial state \u03b10 \u2208 \u03a9w , an \u03b5 > 0, grid resolution \u03b4 , accuracy parameter \u03b41 , and error probability\n\u03b42 , L, N and an A which is class invariant.\nG OAL : To estimate \u03bcch (\u03b5, N).\nO UTPUT: \u03bc0 such that\u03bc0 \u2265 \u03bcch (\u03b5 + \u03b41 /2) and d h (D\u221e,\u03bc0 \u2212\u03b4 , U) \u2265 \u03b5 \u2212 \u03b41 /2.\nl 4\nm\n\u00061\u0007\n\u03b41\n8L\n0\nLet \u03b5 = 2L2 (the distance from stationarity), c = 2\u03b4 and s = \u03b4 2 log(2c(L + 1)/\u03b42 ) (number of sam1\nples from the distribution).\nFor \u03b4 \u2264 \u03bc \u2264 1/2 in steps of \u03b4 ,\ndef\n\n1. Let \u03c4 = \u03c4(L, N, A, \u03bc, \u03b5 0 ).\n2. Let D\u03c4,k , for k = 1, . . . , s, denote s independent samples from the RSM process with parameters\nL, N, A and \u03bc starting from the initial state \u03b10 .\ndef\n\n3. Let Zs = 1s \u2211sk=1 D\u03c4,k .\n4. If d h (Zs ,U) \u2264 \u03b5 then Return \u03bc and Stop.\n5. Else \u03bc = \u03bc + \u03b4 .\nFigure 1: The Algorithm E RRORT HRESHOLD\nIn this section, we give a proof of Theorem 5.10. Recall that \u03c0w denotes the stationary distribution of the\nprojected RSM chain described in Section 5.1. Using the definition of the mixing time, and the ChernoffHoeffding bound, we have the following lemma in order to bound the number of samples s required in each\niteration of the algorithm:\nLemma B.23. Suppose we take s samples from independent realizations of the fully mixed projected RSM\nprocess, denoting the samples so obtained as D(i) for i = 1, 2, . . . , s. For any fixed genome \u03c3 , we then have\nE [D\u03c3 (i)] = E\u03c0w [D\u03c3 ] for all i. Now for every \u03b5 > 0\n#\n\"\n\u0001\n1 s \u03c3\nD (i) \u2212 E\u03c0w [D\u03c3 ] \u2265 \u03b5 \u2264 2 exp \u22122\u03b5 2 s .\nP\n\u2211\ns i=1\n32\n\n\fIn particular, if we run s independent realizations of the chain up to the time \u03c4(L, N, A, \u03bc, \u03b5/2), and let Di\ndenote the sample obtained by the ith realization, then\n\"\n#\n\u0001\n1 s \u03c3\nP\nD (i) \u2212 E\u03c0w [D\u03c3 ] \u2265 \u03b5 \u2264 2 exp \u2212\u03b5 2 s/2 .\n\u2211\ns i=1\nProof of Theorem 5.10. Consider the random variables Zs in any of the at most c iterations in E RRORT HRESH OLD . By the choice of \u03b5 0 and s, we get from the above lemma and a union bound that all the c different\nvariables Zs that we get across all the iterations of the loop satisfy\n||Zs \u2212 E\u03c0w [D]||\u221e \u2264 \u03b41 /(2L2 )\n\n(17)\n\nwith probability at least 1 \u2212 \u03b42 . In the rest of the proof, we condition on the above event occurring.\nSince the average Hamming distance can be at most L, and we are running the projected chains till\n\u03c4(L, N, A, \u03bc, \u03b41 /(2L2 )) for each \u03bc, we get that for every \u03bc considered by the algorithm\nd h (\u03c0w,\u03bc , U) \u2212 d h (Zs , U) \u2264 \u03b41 /2.\nWe therefore deduce that when the algorithm outputs a \u03bc, d h (\u03c0w,\u03bc , U) \u2264 \u03b5 + \u03b41 /2, using the conditioning\non the event in equation (17). Similarly, by noticing that the algorithm had d h (Zs , U) \u2265 \u03b5 for \u03bc \u2212 \u03b4 , we get\nthat d h (\u03c0w,\u03bc\u2212\u03b4 , U) \u2265 \u03b5 \u2212 \u03b41 /2. The estimate of the running time follows by noticing that assuming bits with\nbias \u03bc can be sampled in O(log(1/\u03bc)) time, it takes time O(NL max a\u03c3 log(1/\u03bc)) to simulate each step of\nthe Mw chain. The bound on the error probability follows from the conditioning used on the validity of\nequation (17).\nWe comment briefly on a technical point about the definition of the error threshold that has been used in\nthe literature (and that we use too). With this definition, there might exist a \u03bc satisfying 1/2 > \u03bc > \u03bcch (\u03b5) such\nthat d h (v, U) > \u03b5. An analogous condition might hold in the finite population case too. If we could preclude\nthe occurrence of such anomalous behavior of error-thresholds, we would be able to improve the guarantee\non the output \u03bc0 of the algorithm E RRORT HRESHOLD to be of the form \u03bcch (\u03b5 +\u03b41 , N) \u2264 \u03bc0 \u2264 \u03bcch (\u03b5 \u2212\u03b41 )+\u03b4 .\nWe observe, however, that somewhat surprisingly, even for the simpler case of the quasispecies model, to\nthe best of our knowledge, no attempts have been made to rigorously prove that such anomalous behavior\ncannot occur. We leave the resolution of this point for the finite population case as an open problem.\n\n33\n\n\f"}