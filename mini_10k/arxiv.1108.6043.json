{"id": "http://arxiv.org/abs/1108.6043v1", "guidislink": true, "updated": "2011-08-30T19:24:12Z", "updated_parsed": [2011, 8, 30, 19, 24, 12, 1, 242, 0], "published": "2011-08-30T19:24:12Z", "published_parsed": [2011, 8, 30, 19, 24, 12, 1, 242, 0], "title": "Optimal Data Split Methodology for Model Validation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.2689%2C1108.1230%2C1108.5267%2C1108.5240%2C1108.3091%2C1108.1172%2C1108.3457%2C1108.4452%2C1108.3745%2C1108.4177%2C1108.3995%2C1108.5127%2C1108.5054%2C1108.0839%2C1108.3117%2C1108.6226%2C1108.2012%2C1108.5632%2C1108.1521%2C1108.1749%2C1108.5877%2C1108.5328%2C1108.6043%2C1108.3120%2C1108.5319%2C1108.1088%2C1108.0077%2C1108.2874%2C1108.4613%2C1108.3228%2C1108.5132%2C1108.4298%2C1108.5302%2C1108.1256%2C1108.4668%2C1108.2834%2C1108.4490%2C1108.0939%2C1108.6320%2C1108.3910%2C1108.4650%2C1108.1097%2C1108.4783%2C1108.1994%2C1108.1514%2C1108.3181%2C1108.1987%2C1108.6019%2C1108.5085%2C1108.1308%2C1108.5711%2C1108.3762%2C1108.0341%2C1108.5283%2C1108.3363%2C1108.5438%2C1108.2286%2C1108.5423%2C1108.3153%2C1108.0788%2C1108.5737%2C1108.2484%2C1108.1816%2C1108.2166%2C1108.0205%2C1108.4917%2C1108.2664%2C1108.2125%2C1108.5294%2C1108.0155%2C1108.4004%2C1108.0434%2C1108.4290%2C1108.1698%2C1108.2952%2C1108.0461%2C1108.1547%2C1108.1704%2C1108.1840%2C1108.0866%2C1108.3735%2C1108.1812%2C1108.0696%2C1108.1101%2C1108.5377%2C1108.5905%2C1108.0532%2C1108.6210%2C1108.1495%2C1108.1700%2C1108.0977%2C1108.1896%2C1108.0812%2C1108.5940%2C1108.3124%2C1108.1091%2C1108.5653%2C1108.3454%2C1108.0264%2C1108.0404%2C1108.6066&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Optimal Data Split Methodology for Model Validation"}, "summary": "The decision to incorporate cross-validation into validation processes of\nmathematical models raises an immediate question - how should one partition the\ndata into calibration and validation sets? We answer this question\nsystematically: we present an algorithm to find the optimal partition of the\ndata subject to certain constraints. While doing this, we address two critical\nissues: 1) that the model be evaluated with respect to predictions of a given\nquantity of interest and its ability to reproduce the data, and 2) that the\nmodel be highly challenged by the validation set, assuming it is properly\ninformed by the calibration set. This framework also relies on the interaction\nbetween the experimentalist and/or modeler, who understand the physical system\nand the limitations of the model; the decision-maker, who understands and can\nquantify the cost of model failure; and the computational scientists, who\nstrive to determine if the model satisfies both the modeler's and decision\nmaker's requirements. We also note that our framework is quite general, and may\nbe applied to a wide range of problems. Here, we illustrate it through a\nspecific example involving a data reduction model for an ICCD camera from a\nshock-tube experiment located at the NASA Ames Research Center (ARC).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.2689%2C1108.1230%2C1108.5267%2C1108.5240%2C1108.3091%2C1108.1172%2C1108.3457%2C1108.4452%2C1108.3745%2C1108.4177%2C1108.3995%2C1108.5127%2C1108.5054%2C1108.0839%2C1108.3117%2C1108.6226%2C1108.2012%2C1108.5632%2C1108.1521%2C1108.1749%2C1108.5877%2C1108.5328%2C1108.6043%2C1108.3120%2C1108.5319%2C1108.1088%2C1108.0077%2C1108.2874%2C1108.4613%2C1108.3228%2C1108.5132%2C1108.4298%2C1108.5302%2C1108.1256%2C1108.4668%2C1108.2834%2C1108.4490%2C1108.0939%2C1108.6320%2C1108.3910%2C1108.4650%2C1108.1097%2C1108.4783%2C1108.1994%2C1108.1514%2C1108.3181%2C1108.1987%2C1108.6019%2C1108.5085%2C1108.1308%2C1108.5711%2C1108.3762%2C1108.0341%2C1108.5283%2C1108.3363%2C1108.5438%2C1108.2286%2C1108.5423%2C1108.3153%2C1108.0788%2C1108.5737%2C1108.2484%2C1108.1816%2C1108.2166%2C1108.0205%2C1108.4917%2C1108.2664%2C1108.2125%2C1108.5294%2C1108.0155%2C1108.4004%2C1108.0434%2C1108.4290%2C1108.1698%2C1108.2952%2C1108.0461%2C1108.1547%2C1108.1704%2C1108.1840%2C1108.0866%2C1108.3735%2C1108.1812%2C1108.0696%2C1108.1101%2C1108.5377%2C1108.5905%2C1108.0532%2C1108.6210%2C1108.1495%2C1108.1700%2C1108.0977%2C1108.1896%2C1108.0812%2C1108.5940%2C1108.3124%2C1108.1091%2C1108.5653%2C1108.3454%2C1108.0264%2C1108.0404%2C1108.6066&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The decision to incorporate cross-validation into validation processes of\nmathematical models raises an immediate question - how should one partition the\ndata into calibration and validation sets? We answer this question\nsystematically: we present an algorithm to find the optimal partition of the\ndata subject to certain constraints. While doing this, we address two critical\nissues: 1) that the model be evaluated with respect to predictions of a given\nquantity of interest and its ability to reproduce the data, and 2) that the\nmodel be highly challenged by the validation set, assuming it is properly\ninformed by the calibration set. This framework also relies on the interaction\nbetween the experimentalist and/or modeler, who understand the physical system\nand the limitations of the model; the decision-maker, who understands and can\nquantify the cost of model failure; and the computational scientists, who\nstrive to determine if the model satisfies both the modeler's and decision\nmaker's requirements. We also note that our framework is quite general, and may\nbe applied to a wide range of problems. Here, we illustrate it through a\nspecific example involving a data reduction model for an ICCD camera from a\nshock-tube experiment located at the NASA Ames Research Center (ARC)."}, "authors": ["Rebecca Morrison", "Corey Bryant", "Gabriel Terejanu", "Kenji Miki", "Serge Prudhomme"], "author_detail": {"name": "Serge Prudhomme"}, "author": "Serge Prudhomme", "arxiv_comment": "Submitted to International Conference on Modeling, Simulation and\n  Control 2011 (ICMSC'11), San Francisco, USA, 19-21 October, 2011", "links": [{"href": "http://arxiv.org/abs/1108.6043v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1108.6043v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1108.6043v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1108.6043v1", "journal_reference": null, "doi": null, "fulltext": "Optimal Data Split Methodology for Model\nValidation\n\narXiv:1108.6043v1 [physics.data-an] 30 Aug 2011\n\nRebecca Morrison, Corey Bryant, Gabriel Terejanu, Kenji Miki, Serge Prudhomme\n\nAbstract-The decision to incorporate cross-validation into\nvalidation processes of mathematical models raises an immediate question \u2013 how should one partition the data into calibration\nand validation sets? We answer this question systematically: we\npresent an algorithm to find the optimal partition of the data\nsubject to certain constraints. While doing this, we address two\ncritical issues: 1) that the model be evaluated with respect to\npredictions of a given quantity of interest and its ability to\nreproduce the data, and 2) that the model be highly challenged\nby the validation set, assuming it is properly informed by the\ncalibration set. This framework also relies on the interaction\nbetween the experimentalist and/or modeler, who understand\nthe physical system and the limitations of the model; the\ndecision-maker, who understands and can quantify the cost of\nmodel failure; and the computational scientists, who strive to\ndetermine if the model satisfies both the modeler's and decisionmaker's requirements. We also note that our framework is quite\ngeneral, and may be applied to a wide range of problems.\nHere, we illustrate it through a specific example involving a\ndata reduction model for an ICCD camera from a shock-tube\nexperiment located at the NASA Ames Research Center (ARC).\nIndex Terms-Model\nBayesian inference\n\nvalidation,\n\nquantity\n\nof\n\ninterest,\n\nI. I NTRODUCTION\nModel validation to assess the credibility of a given\nmodel is becoming a necessary activity when making critical\ndecisions based on the results of computer modeling and simulations. As stated in [1], validation requires that the model\naccurately capture the critical behavior of the system(s), and\nthat uncertainties due to random effects be quantified and\ncorrectly propagated through the model.\nThere are various approaches to model validation; here\nwe explore a procedure based on cross-validation. First,\none partitions the data into two sets: the calibration (or\ntraining) set and the validation set. Next, the calibration set\nis used to calibrate the model. Then the calibrated model\nproduces a set of predicted values to be compared with the\nvalidation set. A small discrepancy between predicted values\nand the validation set improves the credibility of the model\nwhile a large discrepancy may invalidate the model. We will\npresent a general framework based on these principles that\nincorporates our particular goals along with a detailed crossvalidation algorithm.\nMore specifically, we examine situations in which we want\nto predict values for which experimental data is not available,\nreferred to here as the prediction scenario. Experiments for\nthis scenario may be impractical or even impossible. Still,\nManuscript received June 30, 2011; revised August 13, 2011. This\nmaterial is based upon work supported by the Department of Energy\n[National Nuclear Security Administration] under Award Number [DEFC52-08NA28615].\nAll authors are located at the Institute for Computational Engineering\nand Sciences, The University of Texas at Austin; Austin, TX, 78712 e-mail:\n{rebeccam, cbryant, terejanu, kenji, serge}@ices.utexas.edu.\n\nas a computational scientist, one wants to predict a certain\nquantity of interest (QoI) at this scenario and assess the\nquality of this prediction. Often, the only experimental data\navailable comes from legacy experiments and may be incomplete. Furthermore, this QoI is seldom directly observable\nfrom the system, but requires some additional modeling.\nNumerous examples of this situation exist. Computational\nmodels aimed at predicting the reentry of space vehicles are\none such example. Some characteristics of the system can\nbe recreated in sophisticated wind tunnels, but experiments\nare expensive and may be unreliable. Another example is\nthe maintenance of nuclear stockpiles. Since experiments to\nassess environmental impact in case of failure are banned,\npredictive models must be used.\nBabu\u0161ka et al. present a systematic approach to assess\npredictions of this type using Bayesian inference and what\nthey call a validation pyramid [2]. Scenarios of varying\ncomplexity are available that suggest an obvious hierarchy\non which to validate the model. In their calibration phase,\nBayesian updating is used to condition the model on the\nobservations available at lower levels of the pyramid. The\nmodel's predictive ability is then assessed by further conditioning using the validation data at the higher levels. One\nadvantage of their approach is that the prediction metric is\ndirectly related to the QoI. This feature is maintained in our\nwork.\nIn the work described above, the authors employ a single\nsplit of the data into calibration and validation scenarios.\nWhile they argue that this partition of the data is made clear\nby the experimental set-up and validation pyramid, it is often\nthe case that all experiments provide an equal amount of\ninformation regarding the QoI. To avoid a subjective choice\nof the calibration set, we determine the set by a more rigorous\nand quantitative process.\nTo do this, we propose a cross-validation inspired methodology to partition the data into calibration and validation\nsets. In contrast to previous works, we do not immediately\nchoose a single partition as above, nor do we use averages\nor estimators over multiple splits (see [3]\u2013[5] and references\ntherein). Instead, our approach first considers all possible\nways to split the data into disjoint calibration and validation\nsets which satisfy a chosen set size. Then, by analyzing\nseveral splits, we methodically choose what we term the\n\"optimal\" split. Once the optimal split is found, we are then\nable to judge the validity of the model, and whether or not\nit should be used for predictive purposes.\nFirst, we argue that the model's ability to replicate the\nobservations must be assessed quantitatively. A model incapable of reproducing observations should not be used to\npredict unobservable quantities of interest. Specifically in\nthe case of Bayesian updating, the prior information and\nobserved data must be sufficient to produce a satisfactory\n\n\fposterior distribution for the model parameters.\nSecond, we address a fundamental issue of validation.\nWe can never fully validate a model; instead, we can only\ntry to invalidate it. Because of this, when we choose a\nvalidation set with which to test the model, it should be\nthe most challenging possible. In other words, we demand\nthat the model perform well on even the most challenging\nof validation sets; otherwise, we cannot be confident in its\nprediction of the QoI.\nWith these concepts in mind, we propose that the optimal\nsplit satisfy the following desiderata:\n(I) The model is sufficiently informed by the calibration\nset (and is thus able to reproduce the data).\n(II) The validation set challenges the model as much as\npossible with respect to the quantity of interest.\nUsing the optimal partition, we are then able to answer\nwhether the model should be used for prediction.\nIn the current work, we apply our framework to a data\nreduction model which converts photon counts received by an\nICCD camera to radiative intensities [6]. We chose this as an\nappropriate application because the resulting intensities are\nlater used to make higher-level decisions. Thus, the validity\nof the model should be thoroughly tested before the model is\ndeemed reliable. Moreover, the uncertainty in such a model\nshould be explicitly evaluated, as possible errors may be\npropagated through to other predicted quantities.\nThe paper is organized as follows. In section II, the general\nframework is detailed step by step. A concise algorithm is\nprovided. In section III, the approach is applied to the data\nreduction model. Finally, in section IV, short-comings and\nfuture work are discussed.\nII. VALIDATION F RAMEWORK\nFeedback Control\n\nCalibration\nData \u03c0 (Dc)\n\nModel\nis\nInvalid\n\nModel with\nCalibrated\n\n\u03c0 (\u03b8)\nc\n\nCalibration\nScenario\n\nSc\n\nParameter(s)\n\n\u03c3 (\u03b8)\nc\n\nEstimate\nQoI\nusing\n\nQc\n\nPrediction\nScenario\n\nD(Q c, Q v) < \u03b3\n\nSp\n\n\u03c0 (\u03b8)\nv\n\nValidation\nScenario\n\nSv\n\n\u03c3 (\u03b8)\nv\n\nModel with\nRe\u2212Calibrated\n\nSensitivity/\nUncertainty\nQuantification\n\nParameter(s)\n\nValidation\nData \u03c0 (Dv)\n\nFig. 1.\n\nNo\n\nQv\n\nYes\nModel not\nInvalid\nIncreased\nConfidence\n\nDecision\n\nThe calibration and validation cycle\n\nFigure 1 demonstrates the previously described framework\n[2], which applies when the quantity of interest is only\navailable as a prediction through the computational model,\nnot through direct observation. There, Bayesian updating is\nperformed on a calibration set, and then a prediction of the\nQoI is made using the updated model. A subsequent update is\nperformed using a validation set followed by an additional\nprediction with the newly updated model. Finally, the two\npredictions are compared to assess the model's predictive\ncapabilities.\n\nA. Prediction metric\nThe QoI driven model assessment developed in [2] requires a metric comparing predictions of the QoI obtained\nfrom the calibration and validation sets. In many instances\nthe QoI is determined by a decision-maker, who may not\nbe the computational scientist performing the analysis. Consequently, they must work together to develop a suitable\nmetric for the QoI, as well as a tolerance, which quantifies\nhow consistent the predictions must be (note that we cannot\nmeasure accuracy of the predictions because we have no\ntrue value). Ideally this metric would measure in the units\nof the QoI, or provide a relative error, allowing for easy\ninterpretation by decision-makers. Examples of such metrics\ninclude absolute error measures and percent error based on\na nominal value.\nIn the following sections of the paper we will denote the\nmetric used to measure the predictive performance of the\nmodel by MQ , highlighting the fact that it is determined by\nthe QoI. Likewise, we will denote the threshold, or tolerance,\n\u2217\nby MQ\n. We stress that generality is maintained since the rest\nof the procedure discussed below is flexible to the particular\nchoice of metric and threshold. What we do require is that the\nchoice of metric be appropriate for the application at hand.\nFor instance, when using Bayesian inference we require that\nthe metric be compatible with probabilistic inputs, since\npredictions are provided as probability or cumulative density\nfunctions of the QoI.\nB. Data reproduction\nAs discussed briefly in the introduction, one must also\nensure that the model is capable of reproducing the observed\ndata. This evaluation has been overlooked, or at least not\nemphasized, in previous works [2], [6]. Such an evaluation\nprovides confidence that the model and data provided are\nmutually suitable. Verifying the model's reproducibility of\nthe observables demonstrates that the parameters of the\nmodel are adequately informed through the inverse problem.\nAs in the case of the prediction metric, establishing a\nperformance criterion may require further knowledge of the\nphysical system. The analyst is likely not an expert in the\nsystem being modeled and should solicit a modeler's, or\nexperimentalist's, assistance in developing a performance\nmetric and tolerance. Doing so will certify that the correct\naspects of the system are being captured sufficiently by the\nmodel.\nWe use a similar notation as for the prediction metric.\n\u2217\nLet MD and MD\ndenote the data reproduction metric and\nthreshold respectively. We note again that these choices are\ndefined based on the model being considered and are not\nspecific to the framework we propose.\nC. Choice of calibration size\nAs with many validation schemes, the selection of the\ncalibration, or training, set size is important. From k-fold to\nleave-one-out cross validation, this set size can vary greatly\n[4], [5]. Here, we do not require a particular choice as the\nproposed framework will apply whatever this size may be.\nIndeed, various choices were considered in preparation of\nthis work; however, we do not discuss them further.\n\n\fWe will denote these partitions, or splits, by {sk }, where\nk = 1, 2, . . . , P . The computational impact of this becomes\neven more significant while performing the next step of the\nprocedure.\nD. Inversion for model parameters\nFor each admissible partition of the data, we solve an\ninverse problem using the calibration set, of size NC , as\ninput data. It is not hard to see why solving P inverse problems may be difficult, or even impossible, for complicated\nmodels. This is an area for improvement; approximations\nand alternative approaches to reduce the number of inverse\nproblems will be the subject of future work.\nAs discussed previously, we treat the inverse problem in a\nprobabilistic setting, using Bayesian updating to incorporate\nthe calibration data. As a result we obtain distributions of\nmodel parameters, and these in turn yield distributions for\nthe predicted quantities. At this point it becomes clear that\nthe definition of the metrics will depend on how the inverse\nproblems are solved. Note, of course, that a deterministic\napproach could also be used.\n\nWith the solutions obtained from the inverse problems we\nare now able to evaluate the model's performance. For each\ncalibration set, we compute the metrics as detailed above.\nOne can then visualize the data on a Cartesian grid where\nthe x and y axes correspond to the metrics MQ and MD ,\nrespectively, and each point corresponds to a single partition\nof the data into a calibration and validation set (figure 2).\nsk\n\nMD\n\nNote that it could be the case that each of the N observations\nin fact represents a set of observations, if, for instance,\nrepeated experimental measurements are taken at the same\nconditions.\nWe do not perform our analysis on a single partitioning\nof the data but instead consider all possible partitions of the\ndata respecting (1). The reason for this approach is two-fold.\nFirst, it reduces the sensitivity of the final outcome to any\nparticular set of data. Since each data point is equivalent\nunder partitioning, we do not bias the groupings in any\nsubjective way (once we have chosen the calibration set size).\nSecond, we envision an application where it is unclear\nwhich experiments relate more closely to the QoI scenario.\nConsidering all admissible partitions can provide insight as\nto which observations are most influential with respect to the\nQoI. As an example, consider a case of resonance where the\nQoI is associated with the resonant behavior of the system.\nWithout knowing a priori the resonance frequency of the\nsystem, one cannot say which frequencies will be important\nfor capturing the resonant behavior.\nHowever, the drawback of this approach is evident: we\nmust consider the model performance for all partitions of the\ndata. This yields a combinatorially large number of partitions,\nwhose exact number is given by the binomial formula:\n\u0012 \u0013\nN\nN!\n.\n(2)\nP =\n=\nNC !NV !\nNC\n\nE. Computation of the metrics\n\nMQ\n\nFig. 2.\n\nMetrics computed for each data split sk\n\nF. Optimal partition\nWe attempt to invalidate the model using the optimal split\ndetermined by (I) and (II).\nFor (I), performance in replicating the observables is\nmeasured using the data metric MD . Thus we only consider\nsplits, sk , of the data that satisfy\n\u2217\nMD (sk ) < MD\n.\n\nIf no points lie below this threshold, the model's ability of\nreproducing the observed data is unsatisfactory, and one must\nchange or improve the model, or the data, or perhaps both.\nNext, to satisfy (II), we select the partition s\u2217 which\nmaximizes the prediction metric,\ns\u2217 =\n\nargmax MQ (sk ).\n\n(3)\n\nsk ,\n\u2217\nMD (sk )<MD\n\nThe optimal partition for the results shown above is highlighted in figure 3.\nsk\ns*\nModel is incapable of reproducing experimental data\nMD\n\nWith that being said, we do recognize that the particular\nchoice could impact the final conclusion and must be made\nwith care. Of particular concern is providing enough data so\nthat all parameters of the model are sufficiently informed by\nthe inverse problem. If the model were to fail with respect to\nthe data metric, the issue may not be the model itself but too\nsmall a calibration set size. In this case one should perform\nfurther analysis to determine the source of this discrepancy\nand increase the calibration set size if necessary.\nGiven N observations, we denote the size of the calibration set by NC and the size of the validation set by NV .\nThat is,\nNC + NV = N.\n(1)\n\nM*D\n\nModel is capable of reproducing experimental data\n\nMQ\n\nFig. 3.\n\nIdentification of optimal split s\u2217\n\n\f\u2217\nG. Comparison of s\u2217 with MQ\n\nFinally we are able to assess the model's ability to predict\nthe QoI. After identifying the optimal partition we compare\nthe model's performance, in this \"worst case\" scenario,\n\u2217\nagainst the threshold MQ\n.\n\u2217\nIf s fails to satisfy the threshold then we conclude that\nthe model is invalid, given the observations available, and\nshould not be used to make predictions for the QoI. Figure\nsk\ns*\n\nMD\n\nModel is incapable of reproducing experimental data\n\nM*D\n\nModel is\nnot invalidated\nModel is incapable of predicting QoI\n\nM*Q\n\nFig. 4.\n\nMQ\n\n\u2217\nComparison of s\u2217 with MQ\n\n4 shows an example of exactly this case.\nIf s\u2217 does not violate the tolerance set by the decisionmaker we can only conclude that the model is not invalidated\ngiven the observations we have. This does not guarantee\nthat the model is valid, only that we cannot demonstrate\notherwise. This outcome warrants further observations to\ncontinue challenging the model. The process to obtain these\nadditional experimental results may be supplemented by\nperforming optimal experimental design [7]. If, however, we\ncannot obtain more data, then the process is complete, and\nwe conclude that the model has not been invalidated.\n\nOnce more we stress that the procedure described above\nis extremely general. This is an advantage of the approach\nand allows for its application to a wide range of problems.\nSpecifics will be discussed for one application below.\nIII. A PPLICATION TO THE DATA REDUCTION MODEL\nWe now turn to a specific implementation of the proposed\nalgorithm. A data reduction model is chosen for analysis.\nSince higher level models require predictions obtained from\nthis data reduction model, it is critical that we accurately\nassess the quality of its predictions. Moreover, we must\nconsider all partitions of the data because the model is\nsignificantly influenced by the data.\nThe inverse problem of calibrating the model parameters\nfrom the measurement data is solved using Markov chain\nMonte Carlo simulations. In our simulations, samples from\nthe posterior distribution are obtained using the statistical\nlibrary QUESO [8] equipped with the Hybrid Gibbs Transitional Markov Chain Monte Carlo method proposed by\nCheung and Beck [9].\nFirst, we briefly describe the experimental set-up, the\nmodel in question, and the quantity of interest. Next, we\ndescribe the application of the algorithm and present the\nresults.\nA. The ICCD camera\nThe example problem comes from a shock-tube experiment in which an ICCD camera measures photon counts [6].\nAs the opening time, or gate width, of the camera increases,\nso does the photon count. For sufficiently large gate widths,\nthis behavior is linear and simple to model. However, at very\nsmall gate widths, below the linear regime of the instrument,\nthe behavior becomes complicated and nonlinear. Figure 5\nshows a diagram of this behavior. It is in this nonlinear region\nwhere we wish to predict the photon count.\n\nH. Algorithm\nThe general algorithm can be summarized in 8 steps:\n\u2217\n1. elicit the data metric and threshold, MD and MD\n, from\nthe modeler and/or experimentalist\n\nCounts\n\nt (ns)\n\n\u2217\n2. elicit the QoI metric and threshold, MQ and MQ\n, from\nthe decision-maker\n\n3. given N , choose the calibration set size, NC , such that\nNC + NV = N\n4. generate all possible partitions of the data {sk }P\nk=1 ,\nwhere\n\u0012 \u0013\nN\nN!\nP =\n=\nNC\nNC !NV !\n5. solve P inverse problems for splits {sk }P\nk=1\n6. for each partition sk , compute MD (sk ) and MQ (sk )\n7. find optimal split\ns\u2217 =\n\nargmax MQ (sk )\n\nsk ,\n\u2217\nMD (sk )<MD\n\n\u2217\n8. compare MQ (s\u2217 ) with MQ\n\nFig. 5.\n\nPhoton count vs. time for a small gate width in the ICCD camera\n\nThe raw data are photon counts received by the camera\nat eleven different gate widths ranging from 0.5 to 10\nmicro-seconds. Given the photon count, N\u2206t , computing the\nquantity of interest is a simple post-processing step. The\nreciprocity \u03c1 is defined as\n\u0013 \u0012\n\u0013\n\u0012\nN\u2206t\nN10\n\u03c1(\u2206t) =\n/\n,\n(4)\n10\n\u2206t\nand the QoI is \u03c1(0.1). A data reduction model for the photon\ncount as a function of gate width, N\u2206t , is required because\nwe do not have the data N0.1 .\nIf we only wished to predict the photon count in the linear\nregime of the instrument, we could simply use:\nN\u2206t = \u03b2\u2206t.\n\n(5)\n\n\fBut in order to account for the nonlinearity encountered at\nvery small gate widths, several new parameters are introduced into the model:\n(\u03b11 \u2212 \u03b12 )\nN\u2206t = \u03b2(\u2206t + \u03b4) \u2212 \u03b2\n(1 \u2212 e\u2212\u03b11 (\u2206t+\u03b4) ). (6)\n(\u03b11 \u03b12 )\nHere, \u03b2 describes the linear term as before, and \u03b4 is a correction to the gate width \u2206t, in case it is reported incorrectly.\nAlso, \u03b11 and \u03b12 allow for differing opening and closing rates\nof the camera, respectively. These four parameters must later\nbe calibrated through the inverse problem.\nB. Application of the algorithm\nNow we apply the proposed algorithm to the example\nproblem. The steps are as follows:\n1. As mentioned above, predictions of the quantity of interest result in cumulative distribution functions (CDFs)\nin the units of the QoI. Thus, we compare the maximum\nhorizontal distance between the CDFs, with some cutoff\nat the tails [2]. In general, for two CDFs F and G, we\ndefine:\n|xF,u \u2212 xG,u |\nMQ = max\n\u000f\n\u000f\nu\u2208( 2 ,1\u2212 2 )\n\n3. Given 11 gate widths, we choose calibration set size\nNC = 7, leaving the validation set size as NV = 4.\nIn fact, as described earlier, multiple data points are\nprovided for each of the gate widths. However, these\npoints are not considered individually during the partioning process, but are kept together as a single unit.\n4. We generate all possible data sets for calibration:\n\u0012 \u0013\n11\n= 330.\n7\n5. We solve these 330 inverse problems. Again, this is done\nprobabilistically using uniform priors on all parameters\nand Bayesian updating.\n6. We compute MQ and MD for each of the 330 splits of\nthe data. Results are shown in figure 7.\n\u2217\n7. We find all points which satisfy MD (sk ) < MD\n. In\nthis example, all points satisfy this requirement. Next,\nwe find\ns\u2217 = argmax MQ (sk ).\nsk ,\n\u2217\nMD (sk )<MD\n\nThis is the right-most point on the plot, shown in blue\nin figure 7.\n\nwhere xF,u = min{x \u2208 R; F (x) \u2265 u}.\n\nsk\ns*\n0.4\n\nModel is incapable of reproducing experimental data\n0.3\n\nMD\n\nHere, F is the posterior CDF using the calibration data,\nG is the posterior CDF using all the data (including the\nvalidation data), and \u000f is a parameter between 0 and 1\ncontrolling the amount of cut-off of the tails. See figure\n6.\nUsing this metric, the tolerance for the quantity of\n\u2217\n= 2,\ninterest is set. In this example, the threshold is MQ\nwhich corresponds to the rather large percent error of\nabout 100%.\n\n0.5\n\n0.2\n\nModel is\nnot invalidated\n\n(iv)\n\n(iii)\n\n0.1\n\n(i)\n\nModel is incapable of predicting QoI\n\n(ii)\n\n0\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nMQ\n\nFig. 7.\n\nResults as described in step 7.\n\n\u2217\n\u2217\n,\n. Since MQ (s\u2217 ) > MQ\n8. We compare MQ (s\u2217 ) with MQ\nthen we must conclude that the model is incapable of\npredicting the QoI, and has thus been invalidated.\n\nFig. 6.\n\nthe metric for the QoI\n\n2. The model's performance with respect to the data is\ncharacterized using a normalized difference between\nthe true observations and the corresponding predicted\nvalues:\n\u0014q\n\u0015\nMD = E\n(X \u2212 D)T diag(D)\u22121 (X \u2212 D)\nwhere D is a vector of all the data, and X is a vector\nof the predicted values corresponding point-wise to D.\nNote that the entries of X are the predicted values\nof the observables using the calibrated model (on just\nthe calibration set), but for all values of gate widths\n(including those corresponding to both calibration and\nvalidation sets).\n\u2217\nUsing the above as the data metric, the tolerance MD\nis set to 0.2, which yields an average relative error of\n20%.\n\nC. Analysis of results\nLooking at figure 7, we see that the points, representing\nsplits, are grouped into four regions. First, we note that s\u2217\nbelongs to group (iv), and as group number increases, the\nsize of the group decreases. Now let us examine why s\u2217 is\nthe most challenging of the splits and why these groups have\nformed.\nFirst, recall that the data we received was N\u2206t , where\n\u2206t ranged from 0.5 to 10 micro-seconds; specifically \u2206t =\n0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, and 10 \u03bcs. As one might\nguess, the most challenging (optimal) data split, s\u2217 , contained\nthe four lowest gate widths in the validation set. In other\nwords, the data points \"closest\" to the QoI scenario were\nmissing from the calibration set. After calibration, the model\nmust first predict the QoI without any of the information\ncontained in the lowest gate widths. Then, using the validation points to recalibrate the model, there is a large distance\nbetween the resulting predictions (CDFs) for the QoI.\n\n\fThe calibration set for s\u2217 is missing 0.5, 0.6, 0.7, 0.8. The\nother points in group (iv) are those whose calibration set is\nmissing 0.5, 0.6, 0.7, but at least contains 0.8, making them\nslightly less challenging than s\u2217 . Next, group (iii) contains\nthose splits whose calibration set is missing 0.5 and 0.6.\nSimilarly, in group (ii), we see all the splits whose calibration\nset is missing 0.5. Finally, in group (i), the calibration sets\ncontain 0.5, and some collection of the remaining points.\nThe grouping of the data suggests a possible way to\ndecrease the number of inverse problems performed. By\nfinding representatives of the groups, we could search for\nthe optimal split without analyzing every one. Moreover, in\nthe case that the model is not invalidated, understanding these\ngroups could be helpful when planning further experiments,\nperhaps through experimental design.\nIV. C ONCLUSION\nComputationally, our approach is prohibitively expensive\nand requires significant improvement. Methods to reduce the\nnumber of inverse problems required are being investigated.\nAmong them is the use of mutual information to group\nobservations into sets containing similar data. These larger\nsets then require fewer inverse problems.\nWith experimental design one may be able to choose\nwhere to perform subsequent experiments that will challenge\nthe model further, finding a new optimal split. This newly\ndetermined split could render the model invalid, or provide\nincreased confidence in the model's predictive capability.\nThis paper has proposed and demonstrated a systematic\nframework for assessing a model's predictive performance\nwith respect to a particular quantity of interest. Extending the\nwork of Babu\u0161ka et al., the ability of the model to reproduce\nexperimental observations was also evaluated.\nA data reduction model was examined using our framework and ultimately deemed invalid. While the model was\ncapable of reproducing the observations at higher gate\nwidths, it failed based on its performance in predicting the\nquantity of interest. The analysis was carried out on all\npartitions of the data respecting a chosen size constraint. This\nallowed for the determination of an optimal set satisfying\ncalibration (I) and validation (II) requirements.\nR EFERENCES\n[1] R. Hills, M. Pilch, K. Dowding, J. Red-horse, T. Paez, I. Babu\u0161ka, and\nR. Tempone, \"Validation challenge workshop,\" Computer Methods in\nApplied Mechanics and Engineering, vol. 197, no. 29 - 32, pp. 2375\n\u2013 2380, 2008.\n[2] I. Babu\u0161ka, F. Nobile, and R. Tempone, \"A systematic approach to\nmodel validation based on bayesian updates and prediction related\nrejection criteria,\" Computer Methods in Applied Mechanics and\nEngineering, vol. 197, no. 29-32, pp. 2517 \u2013 2539, 2008, Validation\nChallenge Workshop.\n[3] A. Vehtari and J. Lampien, \"Bayesian model assessment and comparison using cross-validation predictive densities,\" Neural Computation,\nvol. 14, pp. 2439 \u2013 2468, 2002.\n[4] S. Arlot and A. Celisse, \"A survey of cross-validation procedures for\nmodel selection,\" Statistics Surveys, vol. 4, pp. 40 \u2013 79, 2010.\n[5] F. Alqallaf and P. Gustafson, \"On cross-validation of bayesian models,\"\nThe Canadian Journal of Statistics, vol. 29, no. 2, pp. 333\u2013340, 2001.\n[6] M. Panesi, K. Miki, and S. Prudhomme, \"On the validation of a\ndata reduction model with application to shock tube experiments,\"\nComputer Methods in Applied Mechanics and Engineering, 2011,\nunder review.\n[7] G. Terejanu, R. R. Upadhyay, and K. Miki, \"Bayesian Experimental\nDesign for the Active Nitridation of Graphite by Atomic Nitrogen,\"\nExperimental Thermal and Fluid Science, 2011, under review.\n\n[8] E. E. Prudencio and K. W. Schulz, \"The Parallel C++ Statistical\nLibrary 'QUESO': Quantification of Uncertainty for Estimation, Simulation and Optimization,\" Submitted to IEEE IPDPS, 2011.\n[9] S. H. Cheung and J. L. Beck, \"New bayesian updating methodology\nfor model validation and robust predictions of a target system based\non hierarchical subsystem tests.\" Computer Methods in Applied Mechanics and Engineering, 2009, accepted for publication.\n[10] J. Larsen and C. Goutte, \"On optimal data split for generalization\nestimation and model selection,\" in Neural Networks for Signal\nProcessing IX, 1999. Proceedings of the 1999 IEEE Signal Processing\nSociety Workshop, aug 1999, pp. 225 \u2013234.\n\n\f"}