{"id": "http://arxiv.org/abs/1104.4210v6", "guidislink": true, "updated": "2015-02-19T17:49:34Z", "updated_parsed": [2015, 2, 19, 17, 49, 34, 3, 50, 0], "published": "2011-04-21T09:14:33Z", "published_parsed": [2011, 4, 21, 9, 14, 33, 3, 111, 0], "title": "Curve registration by nonparametric goodness-of-fit testing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.5255%2C1104.1784%2C1104.3532%2C1104.3544%2C1104.2507%2C1104.2133%2C1104.1385%2C1104.1470%2C1104.3256%2C1104.4081%2C1104.0841%2C1104.2118%2C1104.2990%2C1104.1871%2C1104.1786%2C1104.3001%2C1104.0909%2C1104.4555%2C1104.4598%2C1104.2137%2C1104.3124%2C1104.3240%2C1104.2993%2C1104.5268%2C1104.1127%2C1104.1869%2C1104.3594%2C1104.1556%2C1104.2337%2C1104.5189%2C1104.1441%2C1104.3002%2C1104.3262%2C1104.1270%2C1104.4969%2C1104.1667%2C1104.1024%2C1104.3874%2C1104.1680%2C1104.3984%2C1104.2944%2C1104.3049%2C1104.1273%2C1104.4002%2C1104.0713%2C1104.2377%2C1104.4684%2C1104.3645%2C1104.5313%2C1104.3914%2C1104.4731%2C1104.5162%2C1104.5550%2C1104.4210%2C1104.5505%2C1104.2015%2C1104.2963%2C1104.0198%2C1104.2439%2C1104.4195%2C1104.2164%2C1104.2874%2C1104.3626%2C1104.0210%2C1104.0524%2C1104.4579%2C1104.1151%2C1104.2924%2C1104.0827%2C1104.1772%2C1104.1015%2C1104.2298%2C1104.4526%2C1104.0576%2C1104.0020%2C1104.2190%2C1104.2103%2C1104.4758%2C1104.3580%2C1104.4051%2C1104.2912%2C1104.3216%2C1104.5141%2C1104.2397%2C1104.1358%2C1104.3793%2C1104.1042%2C1104.4942%2C1104.1916%2C1104.3496%2C1104.4828%2C1104.3637%2C1104.0586%2C1104.5492%2C1104.5563%2C1104.0952%2C1104.3114%2C1104.0312%2C1104.4508%2C1104.5179%2C1104.2006&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Curve registration by nonparametric goodness-of-fit testing"}, "summary": "The problem of curve registration appears in many different areas of\napplications ranging from neuroscience to road traffic modeling. In the present\nwork, we propose a nonparametric testing framework in which we develop a\ngeneralized likelihood ratio test to perform curve registration. We first prove\nthat, under the null hypothesis, the resulting test statistic is asymptotically\ndistributed as a chi-squared random variable. This result, often referred to as\nWilks' phenomenon, provides a natural threshold for the test of a prescribed\nasymptotic significance level and a natural measure of lack-of-fit in terms of\nthe $p$-value of the $\\chi^2$-test. We also prove that the proposed test is\nconsistent, \\textit{i.e.}, its power is asymptotically equal to $1$. Finite\nsample properties of the proposed methodology are demonstrated by numerical\nsimulations. As an application, a new local descriptor for digital images is\nintroduced and an experimental evaluation of its discriminative power is\nconducted.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.5255%2C1104.1784%2C1104.3532%2C1104.3544%2C1104.2507%2C1104.2133%2C1104.1385%2C1104.1470%2C1104.3256%2C1104.4081%2C1104.0841%2C1104.2118%2C1104.2990%2C1104.1871%2C1104.1786%2C1104.3001%2C1104.0909%2C1104.4555%2C1104.4598%2C1104.2137%2C1104.3124%2C1104.3240%2C1104.2993%2C1104.5268%2C1104.1127%2C1104.1869%2C1104.3594%2C1104.1556%2C1104.2337%2C1104.5189%2C1104.1441%2C1104.3002%2C1104.3262%2C1104.1270%2C1104.4969%2C1104.1667%2C1104.1024%2C1104.3874%2C1104.1680%2C1104.3984%2C1104.2944%2C1104.3049%2C1104.1273%2C1104.4002%2C1104.0713%2C1104.2377%2C1104.4684%2C1104.3645%2C1104.5313%2C1104.3914%2C1104.4731%2C1104.5162%2C1104.5550%2C1104.4210%2C1104.5505%2C1104.2015%2C1104.2963%2C1104.0198%2C1104.2439%2C1104.4195%2C1104.2164%2C1104.2874%2C1104.3626%2C1104.0210%2C1104.0524%2C1104.4579%2C1104.1151%2C1104.2924%2C1104.0827%2C1104.1772%2C1104.1015%2C1104.2298%2C1104.4526%2C1104.0576%2C1104.0020%2C1104.2190%2C1104.2103%2C1104.4758%2C1104.3580%2C1104.4051%2C1104.2912%2C1104.3216%2C1104.5141%2C1104.2397%2C1104.1358%2C1104.3793%2C1104.1042%2C1104.4942%2C1104.1916%2C1104.3496%2C1104.4828%2C1104.3637%2C1104.0586%2C1104.5492%2C1104.5563%2C1104.0952%2C1104.3114%2C1104.0312%2C1104.4508%2C1104.5179%2C1104.2006&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The problem of curve registration appears in many different areas of\napplications ranging from neuroscience to road traffic modeling. In the present\nwork, we propose a nonparametric testing framework in which we develop a\ngeneralized likelihood ratio test to perform curve registration. We first prove\nthat, under the null hypothesis, the resulting test statistic is asymptotically\ndistributed as a chi-squared random variable. This result, often referred to as\nWilks' phenomenon, provides a natural threshold for the test of a prescribed\nasymptotic significance level and a natural measure of lack-of-fit in terms of\nthe $p$-value of the $\\chi^2$-test. We also prove that the proposed test is\nconsistent, \\textit{i.e.}, its power is asymptotically equal to $1$. Finite\nsample properties of the proposed methodology are demonstrated by numerical\nsimulations. As an application, a new local descriptor for digital images is\nintroduced and an experimental evaluation of its discriminative power is\nconducted."}, "authors": ["Olivier Collier", "Arnak S. Dalalyan"], "author_detail": {"name": "Arnak S. Dalalyan"}, "author": "Arnak S. Dalalyan", "links": [{"href": "http://arxiv.org/abs/1104.4210v6", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1104.4210v6", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1104.4210v6", "affiliation": "ENSAE, CREST", "arxiv_url": "http://arxiv.org/abs/1104.4210v6", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Curve registration by nonparametric goodness-of-fit testing\nOlivier Collier and Arnak S. Dalalyan\n\narXiv:1104.4210v6 [math.ST] 19 Feb 2015\n\nENSAE ParisTech/CREST/GENES\n3 avenue P. Larousse\n92245 Malakoff, FRANCE\n\nAbstract\nThe problem of curve registration appears in many different areas of applications ranging from neuroscience\nto road traffic modeling. In the present work1 , we propose a nonparametric testing framework in which we\ndevelop a generalized likelihood ratio test to perform curve registration. We first prove that, under the null\nhypothesis, the resulting test statistic is asymptotically distributed as a chi-squared random variable. This\nresult, often referred to as Wilks' phenomenon, provides a natural threshold for the test of a prescribed\nasymptotic significance level and a natural measure of lack-of-fit in terms of the p-value of the \u03c72 -test. We\nalso prove that the proposed test is consistent, i.e., its power is asymptotically equal to 1. Finite sample\nproperties of the proposed methodology are demonstrated by numerical simulations. As an application, a\nnew local descriptor for digital images is introduced and an experimental evaluation of its discriminative\npower is conducted.\nKeywords: nonparametric inference, hypotheses testing, Wilks' phenomenon, keypoint matching\n\nIntroduction\nBoosted by applications in different areas such as biology, medicine, computer vision and road traffic\nforecasting, the problem of curve registration and, more particularly, some aspects of this problem related\nto nonparametric and semiparametric estimation, have been explored in a number of recent statistical\nstudies. In this context, the model used for deriving statistical inference represents the input data as a finite\ncollection of noisy signals such that each input signal is obtained from a given signal, termed mean template\nor structural pattern, by a parametric deformation and by adding a white noise. Hereafter, we refer to\nthis as the deformed mean template model. The main difficulties for developing statistical inference in this\nproblem are caused by the nonlinearity of the deformations and the fact that not only the deformations but\nalso the mean template used to generate the observed data are unknown.\nWhile the problems of estimating the mean template and the deformations was thoroughly investigated\nin recent years, the question of the adequacy of modeling the available data by the deformed mean template\nmodel received little attention. By the present work, we intend to fill this gap by introducing a nonparametric\ngoodness-of-fit testing framework that allows us to propose a measure of appropriateness of a deformed mean\ntemplate model. To this end, we focus our attention on the case where the only allowed deformations are\ntranslations and propose a measure of goodness-of-fit based on the p-value of a chi-squared test.\nModel description\nWe consider the case of functional data, that is each observation is a function on a fixed interval, taken\nfor simplicity equal to [0, 1]. More precisely, assume that two independent samples, denoted {Xi }i=1,...,n and\n{Xi#}i=1,...,n# , of functional data are available such that within each sample the observations are independent\n1 This\n\npaper was presented in part at the AI-STATS 2012 conference.\n\nPreprint submitted to Elsevier\n\nNovember 2, 2018\n\n\fidentically distributed (i.i.d. ) drifted and scaled Brownian motions. Let f and f # be the corresponding\ndrift functions: f (t) = dE[X1 (t)]/dt and f # (t) = dE[X1# (t)]/dt. Then, for t \u2208 [0, 1],\nZ t\nZ t\nf # (u) du + s# Bl# (t),\nf (u) du + sBi (t), Xl# (t) =\nXi (t) =\n0\n\n0\n\n#\n\n(B1 , . . . , Bn , B1# , . . . , Bn# # )\n\nwhere s, s > 0 are the scaling parameters and\nare independent Brownian motions.\nSince we assume that the entire paths are observed, the scale parameters s and s# can be recovered with\narbitrarily small error using the quadratic variation. So, in what follows, these parameters are assumed to\nbe known (an extension to the setting of unknown noise level is briefly discussed in Section 3).\nThe goal of the present work is to provide a statistical testing procedure for deciding whether the curves\nof the functions f and f # coincide up to a translation. Considering periodic extensions of f and f # on the\nwhole real line, this is equivalent to checking the null hypothesis\nH0 :\n\n\u2203 (\u03c4 \u2217 , a\u2217 ) \u2208 [0, 1] \u00d7 R\n\nsuch that\n\nf (*) = f # (* + \u03c4 \u2217 ) + a\u2217 .\n\n(1)\n\nIf the null hypothesis is satisfied, we are in the set-up of a deformed mean template model, where f (*) plays\nthe role of the mean template and spatial translations represent the set of possible deformations.\nStarting from [23] and [31], semiparametric and nonparametric estimation in different instances of the\ndeformed mean template model have been intensively investigated [4, 6, 8\u201311, 14, 16, 20, 25, 41, 43, 44] with\napplications to image warping [5, 22]. However, prior to estimating the common template, the deformations\nor any other object involved in a deformed mean template model, it is natural to check its appropriateness,\nwhich is the purpose of this work.\nTo achieve this goal, we first note that the pair of sequences of complex-valued random variables Y =\n(Y0 , Y1 , . . .) and Y # = (Y0# , Y1# , . . .), defined by\n\u0003\n\u0002\nYj , Yj# =\n\nZ\n\n0\n\n1\n\ne\n\n2\u03c0ijt\n\n\u0015\n\u0014 X\nn\nn#\n1 X #\n1\nXi (t), #\nXl (t) ,\nd\nn i=1\nn\nl=1\n\nis a sufficient statistic in the model generated by observations (X1 , . . . , Xn ) and (X1# , . . . , Xn# # ). Therefore,\nwithout any loss of information, the initial (functional) data can be replaced by the transformed data\nR1\nR1\n(Y , Y # ). Let us denote by cj = 0 f (x) e2ij\u03c0x dx and c#j = 0 f # (x) e2ij\u03c0x dx the complex Fourier coefficients\nof the signals f and f # . Then, the first components of the observed sequences, (Y0 , Y0# ), can be written as\ns#\nY0# = c#0 + \u221a \u01eb#0 ,\nn#\n\ns\nY0 = c0 + \u221a \u01eb0 ,\nn\n\nwhere \u01eb0 and \u01eb#0 are two independent, real, standard Gaussian variables. Furthermore, for j \u2265 1, we have\ns#\nYj# = c#j + \u221a\n\u01eb#j ,\n2n#\n\ns\nYj = cj + \u221a \u01ebj ,\n2n\n\n(2)\n\nwhere the complex valued random variables \u01ebj , \u01eb#j are i.i.d. standard Gaussian: \u01ebj , \u01eb#j \u223c NC (0, 1), which\nmeans that their real and imaginary parts are independent N (0, 1) random variables. Moreover, (\u01eb0 , \u01eb#0 ) are\nindependent of {(\u01ebj , \u01eb#j ) : j \u2265 1}. In what follows, we will use boldface letters for denoting vectors or infinite\nsequences so that, for example, c and c# refer to {cj ; j = 0, 1, . . .} and {c#j ; j = 0, 1, . . .}, respectively.\nUnder the mild assumption that f and f # are squared integrable, the likelihood\n\u221a\n\u221a ratio of the Gaussian\nprocess Y \u2022,# = (Y , Y # ) is well defined. Using the notation c\u2022,# = (c, c# ), \u03c3 = s/ 2n and \u03c3 # = s# / 2n#,\nthe corresponding negative log-likelihood is given by\n\u0013\n\u0012\n|Yj# \u2212 c#j |2\n(Y0 \u2212 c0 )2\n(Y0# \u2212 c#0 )2 X |Yj \u2212 cj |2\nl(Y \u2022,# , c\u2022,# ) =\n.\n(3)\n+\n+\n+\n4\u03c3 2\n4\u03c3 #2\n2\u03c3 2\n2\u03c3 #2\nj\u22651\n\nIn the present work, we present a theoretical analysis of the penalized likelihood ratio test in the asymptotics\nof large samples, i.e., when both n and n# tend to infinity, or equivalently, when \u03c3 and \u03c3 # tend to zero. The\nfinite sample properties are examined through numerical simulations.\n2\n\n\fSome motivations\nEven if the shifted curve model is a very particular instance of the general deformed mean template\nmodel, it plays a central role in several applications. To cite a few of them:\nECG interpretation: An electro-cardiogram (ECG) can be seen as a collection of replica of nearly the\nsame signal, up to a time shift. Significant information about heart malformations or diseases can be\nextracted from the mean signal if we are able to align the available curves. For more details we refer\nto [43].\nRoad traffic forecast: In [33], a road traffic forecasting procedure is introduced. To this end, archetypes\nof the different types of road trafficking behavior on the Parisian highway network are built, using a\nhierarchical classification method. In each obtained cluster, the curves all represent the same events,\nonly randomly shifted in time.\nKeypoint matching: An important problem in computer vision is to decide whether two points in a same\nimage or in two different images correspond to the same real-world point. The points in images are\nthen usually described by the regression function of the magnitude of the gradient over the direction\nof the gradient of the image restricted to a given neighborhood (cf. [34]). The methodology we shall\ndevelop in the present paper allows to test whether two points in images coincide, up to a rotation\nand an illumination change, since a rotation corresponds to shifting the argument of the regression\nfunction by the angle of the rotation.\nRelation to previous work\nThe problem of estimating the parameters of the deformation is a semiparametric one, since the deformation involves a finite number of parameters that have to be estimated by assuming that the unknown\nmean template is merely a nuisance parameter. In contrast, the testing problem we are concerned with is\nclearly nonparametric. The parameter describing the probability distribution of the observations is infinitedimensional not only under the alternative but also under the null hypothesis. Surprisingly, the statistical\nliterature on this type of testing problems is very scarce. Indeed, while [27] analyzes the optimality and\nthe adaptivity of testing procedures in the setting of a parametric null hypothesis against a nonparametric\nalternative, to the best of our knowledge, the only papers concerned with nonparametric null hypotheses\nare [1, 2] and [21]. Unfortunately, the results derived in [1, 2] are inapplicable in our set-up since the\nnull hypothesis in our problem is neither linear nor convex. The set-up of [21] is closer to ours. However,\nthey only investigate the minimax rates of separation without providing the asymptotic distribution of the\nproposed test statistic, which generally results in an overly conservative testing procedure. Furthermore,\ntheir theoretical framework comprises a condition on the sup-norm-entropy of the null hypothesis, which is\nirrelevant in our set-up and may be violated.\nThere is also a relatively vast literature on the nonparametric comparison of several curves (see [35, 36, 38,\n42] and the references therein). The results developed in most of these papers concern the regression model\nwith random design and assume that a noisy version of each curve is observed. The tests proposed therein\nare mainly based on kernel smoothing, which are quite different from the tests analyzed in the present paper.\nIn particular, tests based on kernel smoothing may achieve optimal rates only for smoothness classes with\nregularity less than 1. Note also that when transposed to the model of Gaussian white noise, the problem\nof testing the equality of two functions boils down to that of testing that a function vanishes everywhere,\njust by computing the difference of observed noisy signals. Thus the null hypothesis becomes simple.\nIn a companion2 paper [12], the problem of curve registration by statistical hypotheses testing has\nbeen tackled from an asymptotic minimax point of view. In [12], as a complement of the present work,\nminimax rates of separation (up to log factors) are established and a smoothness-adaptive test is proposed\nunder some assumptions, which are substantially stronger than those required in this paper. Note also that\n2 The writing of the paper [12] being completed slightly later than the present one, it has been published earlier because of\nthe randomness of the reviewing process.\n\n3\n\n\fpreliminary versions of some results reported in the present manuscript have been presented at AI-STATS\nconference [15]. The present manuscript is a corrected, completed and significantly developed version of\n[15]. More precisely, Section 3 contains important extensions of the presented methodology to the case of\nmultidimensional signals and unknown noise variance, whereas Section 5 presents an original application to\nthe problem of keypoint matching in computer vision.\nOur contribution\nWe adopt, in this work, the approach based on the generalized likelihood ratio tests, cf. [18] for a\ncomprehensive account on the topic. The advantage of this approach is that it provides a general framework\nfor constructing testing procedures which asymptotically achieve the prescribed significance level for the type\nI error and, under mild conditions, have a power that tends to one. It is worth mentioning that in the context\nof nonparametric testing, the use of the generalized likelihood ratio leads to a substantial improvement upon\nthe likelihood ratio, very popular in parametric statistics. In simple words, the generalized likelihood allows\nto incorporate some prior information on the unknown signal in the test statistic which introduces more\nflexibility and turns out to be crucial both in theory and in practice, see [19].\nWe prove that under the null hypothesis the generalized likelihood ratio test statistic is asymptotically\ndistributed as a \u03c72 -random variable. This allows us to choose a threshold that makes it possible to asymptotically control the test significance level without being excessively conservative. Such results are referred\nto as Wilks' phenomena. In this relation, let us quote [18]: \"While we have observed the Wilks' phenomenon\nand demonstrated it for a few useful cases, it is impossible for us to verify the phenomenon for all nonparametric hypothesis testing problems. The Wilks' phenomenon needs to be checked for other problems that\nhave not been covered in this paper. In addition, most of the topics outlined in the above discussion remains\nopen and are technically and intellectually challenging. More developments are needed, which will push the\ncore of statistical theory and methods forward.\"\nIt is noteworthy that our results apply to the Gaussian sequence model (2), which is often seen as\na prototype of nonparametric statistical model. In fact, it is provably asymptotically equivalent to many\nother statistical models [7, 17, 24, 37, 40] and captures most theoretical difficulties of the statistical inference.\nFurthermore, using the aforementioned results on asymptotic equivalence, the main theoretical findings of\nthe present paper automatically carry over the nonparametric regression model, the density model, the\nergodic diffusion model, etc.\nFinally, we provide a detailed explanation of how the proposed methodology can be used for solving the\nproblem of keypoint matching in digital images. This leads to a new descriptor termed Localized Fourier\nTransform which is particularly well adapted for testing for rotation. The first experiments reported in this\nwork show the validity of our theoretical findings and the potential of the new descriptor.\nOrganization\nThe rest of the paper is organized as follows. After a brief presentation of the model, we introduce the\ngeneralized likelihood ratio framework in Section 1. The main results characterizing the asymptotic behavior\nof the proposed testing procedure, based on generalized likelihood ratio testing for a large variety of shrinkage\nweights, are stated in Section 2. Section 3 contains extensions of our results to the multidimensional setting\nand to the case of unknown noise magnitude. Some numerical examples illustrating the theoretical results\nare included in Section 4, while Section 5 is devoted to the application of the proposed methodology to\nthe problem of keypoint matching in computer vision. The resulting Localized Fourier Transform (LoFT)\ndescriptor is tested on a pair of real images degraded by white Gaussian noise. The proofs of the lemmas\nand of the theorems are postponed to the Appendix.\n1. Penalized Likelihood Ratio Test\nWe are interested in testing the hypothesis (1), which translates in the Fourier domain to\nH0 : there exists\n\n\u2217\n\n\u03c4\u0304 \u2217 \u2208 [0, 2\u03c0[ such that cj = e\u2212ij \u03c4\u0304 c#j\n4\n\nfor all j \u2265 1.\n\n\fIndeed, one easily checks that if (1) is true, then3\nZ 1\nZ\n#\n\u2217 2ij\u03c0t\n2ij\u03c0\u03c4\u2217\ncj =\nf (t \u2212 \u03c4 )e\ndt = e\n0\n\n1\n\n\u2217\n\nf (z)e2ij\u03c0z dz = e2ij\u03c0\u03c4 cj\n\n0\n\n\u2217\n\nand, therefore, the aforementioned relation holds with \u03c4\u0304 = 2\u03c0\u03c4 \u2217 . If no additional assumptions are imposed\non the functions f and f # , or equivalently on their Fourier coefficients c and c# , the nonparametric testing\nproblem has no consistent solution. A natural assumption widely used in nonparametric statistics is that\nc = (c0 , c1 , . . .) and c# = (c#0 , c#1 , . . .) belong to some Sobolev ball\n+\u221e\no\nn\nX\nj 2s |uj |2 \u2264 L2 ,\nFs,L = u = (u0 , u1 , . . .) :\nj=0\n\nwhere the positive real numbers s and L stand for the smoothness and the radius of the class Fs,L .\n\nSince we will also aim at establishing the (uniform) consistency of the proposed testing procedure, we\nneed to precise the form of the alternative. It seems that the most compelling form for the null and the\nalternative is\n(\n\u2217\nH0 : there exists \u03c4\u0304 \u2217 \u2208 [0, 2\u03c0[ such that cj = e\u2212ij \u03c4\u0304 c#j for all j \u2265 1.\n(4)\nP+\u221e\nH1 : inf \u03c4 j=1 |cj \u2212 e\u2212ij\u03c4 c#j |2 \u2265 \u03c1\n\nfor some \u03c1 > 0. In other terms, under H0 the graph of the function f # is obtained from that of f by a\ntranslation. To ease notation, we will use the symbol \u25e6 to denote coefficient-by-coefficient multiplication,\nalso known as the Hadamard product, and e(\u03c4 ) will stand for the sequence (e\u2212i\u03c4 , e\u22122i\u03c4 , . . .).\nTo present the penalized likelihood ratio test, which is a variant of the generalized likelihood ratio test,\nwe introduce a penalization in terms of weighted l2 -norm of c\u2022,# . In this context, the choice of the l2 norm penalization is mainly motivated by the fact that Sobolev regularity assumptions are made on the\nfunctions P\nf and f # . For a sequence of non-negative real numbers, \u03c9, P\nwe define the weighted l2 norm\n2\nkck\u03c9,2 = j\u22650 \u03c9j |cj |2 . We will also use the standard notation kukp = ( j |uj |p )1/p for any p > 0. Using\nthis notation, the penalized log-likelihood is given by\npl(Y \u2022,# , c\u2022,# ) =l(Y \u2022,# , c\u2022,# ) +\n\nkck2\u03c9,2\nkc# k2\u03c9,2\n+\n.\n2\n2\u03c3\n2\u03c3 #2\n\n(5)\n\nThe resulting penalized likelihood ratio test is based on the test statistic\n\u2206(Y \u2022,# ) =\n\nmin\n\nc\u2022,# :H0 is true\n\npl(Y \u2022,# , c\u2022,# ) \u2212 min\npl(Y \u2022,# , c\u2022,# ).\n\u2022,#\nc\n\n(6)\n\nIt is clear that \u2206(Y \u2022,# ) is always non-negative. Furthermore, it is small when H0 is satisfied and is large if\nH0 is violated. Therefore, \u2206(Y \u2022,# ) is a good test statistic for deciding whether or not the null hypothesis\nH0 should be rejected.\nLemma 1. The test statistic \u2206(Y \u2022,# ) can be written in the following form:\n\u2206(Y \u2022,# ) =\n\n+\u221e\nX\n|Yj \u2212 e\u2212ij\u03c4 Yj# |2\n1\n.\nmin\n2(\u03c3 2 + (\u03c3 # )2 ) \u03c4 \u2208[0,2\u03c0] j=1\n1 + \u03c9j\n\n(7)\n\nProof. We start by noting that the minimization of the quadratic functional (5) leads to\nmin\npl(Y \u2022,# , c\u2022,# ) =\n\u2022,#\nc\n\n1 X \u03c9j\n1 X \u03c9j\n2\n|Y\n|\n+\n|Y # |2 .\nj\n2\u03c3 2\n1 + \u03c9j\n2\u03c3 #2\n1 + \u03c9j j\nj\u22651\n\n(8)\n\nj\u22651\n\n3 We use here the change of the variable z = t \u2212 \u03c4 \u2217 and the fact that the integral of a 1-periodic function on an interval of\nlength one does not depend on the interval of integration.\n\n5\n\n\fLet us compute the statistic minc\u2022,# :H0 is true pl(Y \u2022,# , c\u2022,# ) that can be equivalently written in the form\nmin\u03c4 \u2208[0,2\u03c0] mincj =e\u2212ij\u03c4\u0304 \u2217 c#j ,j\u22651 pl(Y \u2022,# , c\u2022,# ). When c\u2022,# satisfies the null hypothesis, simple algebra yields\nthat the relation\n\u2217\n\u0013\nX \u0012 |Yj \u2212 cj |2\n|e\u2212ij \u03c4\u0304 Yj# \u2212 cj |2\nkck2\u03c9,2\nkck2\u03c9,2\n\u2022,# \u2022,#\npl(Y , c ) =\n+\n+\n+\n2\u03c3 2\n2\u03c3 #2\n2\u03c3 2\n2\u03c3 #2\nj\u22651\n\nis true for some \u03c4\u0304 \u2217 . We need to compute the minimum with respect to c of the right-hand side of the last\ndisplay. To ease notation, let us set \u03c3\u0304 2 = 1/( \u03c312 + \u03c31# 2 ) and Z = \u03c3 \u22122 Y + \u03c3 #\u22122 e(\u03c4\u0304 \u2217 ) \u25e6 Y # . Then, we get\npl(Y \u2022,# , c\u2022,# ) =\n\nX \u0012 |Yj |2\nj\u22651\n\n2\u03c3 2\n\n+\n\n\u0013\n|Yj# |2\n1 + \u03c9j\n2\n+\n|c\n|\n\u2212\nRe(\nZ\u0304\nc\n)\n.\nj\nj j\n2\u03c3 #2\n2\u03c3\u0304 2\n\nThe minimum of this expression is attained at cj =\nmin\n\nc\u2022,# :H0 is true\n\npl(Y\n\n\u2022,#\n\n,c\n\n\u2022,#\n\n)=\n\n\u03c3\u03042 Zj\n1+\u03c9j .\n\nX \u0012 |Yj |2\n2\u03c3 2\n\nj\u22651\n\nThis leads to\n\u0013\n|Yj# |2\n\u03c3\u0304 2\n2\n+\n\u2212\n|Zj | .\n2\u03c3 #2\n2(1 + \u03c9j )\n\n(9)\n\nCombining equations (8) and (9) we get\n\u2206(Y \u2022,# ) =\n\nX\n1 X |Yj |2\n\u03c3\u0304 2\n1 X |Yj# |2\n+\n\u2212\n|Zj |2 .\n2\u03c3 2 j 1 + \u03c9j\n2\u03c3 #2 j 1 + \u03c9j\n2(1\n+\n\u03c9\n)\nj\nj\n\nTo complete the proof, it suffices to replace Z by its definition and to use the identity \u03c3 \u22122 \u2212 \u03c3\u0304 2 \u03c3 \u22124 =\n\u03c3 #\u22122 \u2212 \u03c3\u0304 2 \u03c3 #\u22124 = (\u03c3 2 + \u03c3 #2 )\u22121 .\nFrom now on, it will be more convenient to use the notation \u03bdj = 1/(1 + \u03c9j ). The elements of the\nsequence \u03bd = {\u03bdj ; j \u2265 1} are hereafter referred to as shrinkage weights. They are allowed to take any value\nbetween 0 and 1. Even the value 0 will be authorized, corresponding to the limiting case when wj = +\u221e,\nor equivalently to our belief that the corresponding Fourier coefficient is 0. The test statistic can then be\nwritten as:\n1\nmin kY \u2212 e(\u03c4 ) \u25e6 Y # k22,\u03bd ,\n(10)\n\u2206(Y \u2022,# ) =\n2(\u03c3 2 + (\u03c3 # )2 ) \u03c4 \u2208[0,2\u03c0]\nand one of the goals is to find the asymptotic distribution of this quantity under the null hypothesis.\n2. Main results\nThe test based on the generalized likelihood ratio statistic involves a sequence \u03bd, which should be chosen\nby the user. However, we are able to provide theoretical guarantees only under some conditions on these\nweights. To state these conditions, we set \u03c3\u2217 = max(\u03c3, \u03c3 # ) and choose a positive integer N = N\u03c3\u2217 \u2265 2, which\nrepresents the number of Fourier coefficients involved in our testing procedure. In addition to requiring that\n0 \u2264 \u03bdj \u2264 1 for every j, we assume that:\n(A)\n\n\u03bd1 = 1,\n\nand\n\n\u03bdj = 0, \u2200j > N\u03c3\u2217 ,\n\n(B) for some positive constant c, it holds that\n\nX\nj\u22651\n\n\u03bdj2 \u2265 cN\u03c3\u2217 .\n\nMoreover, we will use the following condition in the proof of the consistency of the test:\n(C) \u2203 c > 0, such that min{j \u2265 0, \u03bdj < c} \u2192 +\u221e, as \u03c3\u2217 \u2192 0.\n6\n\n\fIn simple words, this condition implies that the number of terms \u03bdj that are above a given strictly positive\nlevel goes to +\u221e as \u03c3\u2217 converges to 0. If N\u03c3\u2217 \u2192 +\u221e as \u03c3\u2217 \u2192 0, then all the aforementioned conditions\nare satisfied for the shrinkage weights \u03bd of the form \u03bdj+1 = h(j/N\u03c3\u2217 ), where h : R \u2192 [0, 1] is an integrable\nfunction, supported on [0, 1], continuous in 0 and satisfying h(0) = 1. The classical examples of shrinkage\nweights include:\n\uf8f1\n\uf8f4\n1{j\u2264N\u03c3\u2217 } ,\n(projection weight)\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 \b\n\u0001\u03bc \u22121\n\u03bdj =\n1 + \u03baNj\u03c3\n1{j\u2264N\u03c3\u2217 } , \u03ba > 0, \u03bc > 1, (Tikhonov weight)\n(11)\n\u2217\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3 \b1 \u2212 j \u0001\u03bc ,\n\u03bc > 0.\n(Pinsker weight)\nN \u03c3\u2217\n\n+\n\nNote that condition (C) is satisfied in all these examples with c = 0.5, or any other value in (0, 1). Here on,\nwe write \u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) instead of \u2206(Y \u2022,# ) in order to stress its dependence on \u03bd and on \u03c3\u2217 .\n\nTheorem 1. Let c \u2208 F1,L and |c1 | > 0. Assume that the shrinkage weights \u03bdj are chosen to satisfy\n5/2\nconditions (A), (B), N\u03c3\u2217 \u2192 +\u221e and \u03c3\u22172 N\u03c3\u2217 log(N\u03c3\u2217 ) \u2192 0. Then, under the null hypothesis, the test\n\u2022,#\nstatistic \u2206\u03bd,\u03c3\u2217 (Y ) is asymptotically distributed as a Gaussian random variable:\n\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) \u2212 k\u03bdk1 D\n\u2212\u2212\u2212\u2212\u2192 N (0, 1).\n\u03c3\u2217 \u21920\nk\u03bdk2\n\n(12)\n\nThe main outcome of this result is a test of hypothesis H0 that is asymptotically of a prescribed significance level \u03b1 \u2208 (0, 1). Indeed, let us define the test that rejects H0 if and only if\n\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) \u2265 k\u03bdk1 + z1\u2212\u03b1 k\u03bdk2 ,\n\n(13)\n\nwhere z1\u2212\u03b1 is the (1 \u2212 \u03b1)-quantile of the standard Gaussian distribution.\n\nCorollary 1. The test of hypothesis H0 defined by the critical region (13) is asymptotically of significance\nlevel \u03b1.\nRemark 1. Let us consider the case of projection weights \u03bdj = 1(j \u2264 N\u03c3\u2217 ). One can reformulate the\nasymptotic relation stated in Theorem 1 by claiming that 2\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) is approximately N (2N\u03c3\u2217 , 4N\u03c3\u2217 )\ndistributed. Since the latter distribution approaches the chi-squared distribution (as N\u03c3\u2217 \u2192 \u221e), we get:\nD\n\n2\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) \u2248 \u03c722N\u03c3\u2217 ,\n\nas\n\n\u03c3\u2217 \u2192 0.\n\nIn the case of general shrinkage weights satisfying the assumptions stated in the beginning of this section,\nan analogous relation holds as well:\nD\n2k\u03bdk1\n\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) \u2248 \u03c722k\u03bdk2 /k\u03bdk2 ,\n1\n2\nk\u03bdk22\n\nas\n\n\u03c3\u2217 \u2192 0.\n\nThis type of results are often referred to as Wilks' phenomenon.\nThe proof of Theorem 1 is rather technical and, therefore, is deferred to the Appendix. Let us simply\nmention here that we present the proof in a slightly more general case c \u2208 Fs,L with a smoothness s \u2208\n(0, 1]. It appears from the proof that the convergence stated in (12) holds if s > 7/8, N\u03c3\u2217 \u2192 \u221e and\n\u22122s+9/2\nlog(N\u03c3\u2217 ) \u2192 0, as \u03c3\u2217 \u2192 0. We do not know whether the last condition on N\u03c3\u2217 can be avoided\n\u03c3\u22172 N\u03c3\u2217\nby using other techniques, but it seems that in our proof there is no room for improvement in order to relax\nthis assumption. At a heuristic level, it is quite natural to avoid choosing N\u03c3\u2217 too large. Indeed, large N\u03c3\u2217\nleads to undersmoothing in the problem of estimating the quadratic functional kc \u2212 e(\u03c4 ) \u25e6 c# k22 . Therefore,\nthe test statistic corresponds to registration of two curves in which the signal is dominated by noise, which\nis clearly not a favorable situation for performing curve registration.\n7\n\n\fRemark 2. The p-value of the aforementioned test based on the Gaussian or chi-squared approximation can\nbe used as a measure of the goodness-of-fit or, in other terms, as a measure of alignment for the pair of\ncurves under consideration. If the observed two noisy curves lead to the data y \u2022,# , then the (asymptotic)\np-value is defined as\n!\n\u0010\u2206\n\u2022,#\n) \u2212 k\u03bdk1\n\u03bd,\u03c3\u2217 (y\n\u2217\n,\n\u03b1 =\u03a6\nk\u03bdk2\nwhere \u03a6 stands for the c.d.f. of the standard Gaussian distribution.\nNote that a similar result in the model of regression has been established by [25][Theorem 3]. Their\nresults are based on another test statistics, involving kernel smoothing, and hold under more stringent\nassumptions, like the existence of a uniformly continuous second-order derivative of the regression function.\nRemark also that under these assumptions, the test presented in [25] is definitely not rate optimal in the\nsense of minimax rate of separation [30], while our test is minimax rate optimal up to logarithmic factors, as\nproved in the companion paper [12]. Finally, [25] does not provide any result on the power of the proposed\ntest statistics.\nSo far, we have only focused on the behavior of the test under the null without paying attention on what\nhappens under the alternative. The next theorem fills this gap by establishing the consistency of the test\ndefined by the critical region (13).\nTheorem 2. Let condition (C) be satisfied and let \u03c3\u22174 N\u03c3\u2217 tend to 0 as \u03c3\u2217 \u2192 0. Then the test statistic\n\nT\u03c3\u2217 =\n\n\u2206\u03bd,\u03c3\u2217 (Y \u2022,# )\u2212k\u03bdk1\nk\u03bdk2\n\nP\n\n\u2192 +\u221e as \u03c3\u2217 \u2192 0.\ndiverges under H1 , i.e., T\u03c3\u2217 \u2212\n\nIn other words, Theorem 2 establishes the convergence to one of the power of the test defined via (13)\nas the noise level \u03c3\u2217 tends to 0.\n\nThe previous theorem tells us nothing about the (minimax) rate of separation of the null hypothesis\nfrom the alternative. In other words, Theorem 2 does not provide the rate of divergence of T\u03c3\u2217 . Under more\nstringent assumptions on the weight sequence \u03bd and when \u03c3 = \u03c3 # , the minimax approach is developed in the\ncompanion paper [12]. Here, we focus our attention on studying other aspects of the previously introduced\nmethodology, more related to their applicability in the area of computer vision.\nRemark 3. It might be compelling to start the shifted-curve test by performing a test of equality of norms.\nIndeed, if there is enough evidence for rejecting the hypothesis kck2 = kc# k2 , then the hypothesis cj =\n\u2217\ne\u2212ij \u03c4\u0304 c#j , j \u2265 1, will be rejected as well. To perform the test of the equality of norms, one can use the\nprocedure proposed in [13], which is proved to achieve optimal rates of separation.\n3. Some extensions\nThis section presents two possible extensions of the methodology developed in foregoing sections. They\nstem from practical considerations and concern the case of multidimensional curves and the setting of\nunknown noise magnitude.\n3.1. Multidimensional signals\nTheorems 1 and 2 can be straightforwardly extended to the case of multidimensional curves f and f #\nfrom R \u2192 Rd , with an arbitrary integer d \u2265 1. More precisely, we may assume that the observations\n{X i , . . . , X n ; X #1 , . . . , X #n# } are Rd values random processes given by\nX i (t) =\n\nZ\n\n0\n\nt\n\nf (u) du + diag(S)B i (t),\n\nX #l (t)\n\n8\n\n=\n\nZ\n\n0\n\nt\n\nf # (u) du + diag(S # )B #l (t),\n\n\fwhere diag(S) and diag(S # ) are diagonal d \u00d7 d matrices with positive entries and the random processes\n{B 1 , . . . , B n , B #1 , . . . , B #n# } are independent d-dimensional Brownian motions. In order to test the null\nhypothesis\nH0 : \u2203 (\u03c4 \u2217 , a\u2217 ) \u2208 [0, 1] \u00d7 Rd\nsuch that\nf (*) = f # (* + \u03c4 \u2217 ) + a\u2217 ,\n(14)\nit suffices to compute the Fourier coefficients\n\u0002\n\u0003\nY j , Y #j =\n\nZ\n\n1\n\ne\n\n2\u03c0ijt\n\n0\n\n\u0015\n\u0014 X\nn\nn#\n1 X #\n1\nX i (t), #\nX l (t)\nd\nn i=1\nn\n\nj = 1, 2, . . .\n\nl=1\n\nand to evaluate the test statistic\n\u2206(Y \u2022,# ) =\n\nX\n\u0001\n1\nmin\n\u03bdj (diag(\u03c3)2 + diag(\u03c3 # )2 )\u22121/2 Y j \u2212 e\u2212ij\u03c4 Y #j\n2 \u03c4 \u2208[0,2\u03c0]\n\n2\n2\n\n,\n\nj\u2208N\n\n\u221a\n\u221a\nwhere \u03c3 = S/ 2n and \u03c3 # = S # / 2n# . One can easily check that if H0 is true, then under the assumptions\n\u221a\nof Theorem 1, as \u03c3\u2217 = max(k\u03c3k\u221e , k\u03c3# k\u221e ) \u2192 0, the random variable T\u03c3\u2217 = (\u2206(Y \u2022,# ) \u2212 dk\u03bdk1 )/( dk\u03bdk2 )\nconverges in distribution to a standard Gaussian random variable. Furthermore, under H1 , we have T\u03c3\u2217 \u2192 \u221e\nin probability provided that the assumptions of Theorem 2 are fulfilled.\n3.2. Unknown noise level\nIn several application it is not realistic to assume that the magnitude of noise, denoted by (\u03c3, \u03c3 # ) is\nknown in advance. In such a situation, the testing procedure defined by the critical region (13) cannot be\napplied, since the test statistic \u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) depends on \u03c3 2 + \u03c3 #2 . We describe below one possible approach\nto address this issue. Note that in order to make this setting meaningful, we assume that the noisy Fourier\ncoefficients Yj and Yj# are observable only for a finite number of indices j \u2208 {1, . . . , p}. Therefore, we assume\nin this section that Y and Y # are complex valued vectors of dimension p.\nAdopting the same strategy as before, we aim at defining a testing procedure based on the principle of\npenalized likelihood ratio evaluation. However, when the pair (\u03c3, \u03c3 # ) is unknown, the expression (3) for the\nnegative log-likelihood is not valid anymore. Instead, up to some irrelevant summands, we have\nl(Y \u2022,# , c\u2022,# , \u03c3 \u2022,# ) = p(log \u03c3 + log \u03c3 # ) +\n\nkY # \u2212 c# k22\nkY \u2212 ck22\n+\n.\n2\u03c3 2\n2\u03c3 #2\n\n(15)\n\nTherefore, given a vector of weights \u03c9 \u2208 Rp+ , the penalized log-likelihood is defined as\npl(Y \u2022,# , c\u2022,# , \u03c3 \u2022,# ) =l(Y \u2022,# , c\u2022,# , \u03c3 \u2022,# ) +\n\nkc# k2\u03c9,2\nkck2\u03c9,2\n+\n.\n2\u03c3 2\n2\u03c3 #2\n\n(16)\n\nThus, the test statistic to be used is the difference between the minimum of the penalized log-likelihood\nconstrained to H0 and the unconstrained minimum of the penalized log-likelihood, that is\n\u2206(Y \u2022,# ) = min\n\u2022,#\n\u03c3\n\nmin\n\nc\u2022,# :H0 is true\n\npl(Y \u2022,# , c\u2022,# , \u03c3 \u2022,# ) \u2212 min\nmin\npl(Y \u2022,# , c\u2022,# , \u03c3 \u2022,# ).\n\u2022,#\n\u2022,#\n\u03c3\n\nc\n\n(17)\n\nDenoting by \u03bd the vector (1/(1 + \u03c91 ), . . . , 1/(1 + \u03c9p ))\u22a4 and by 1 \u2212 \u03bd the vector (1 \u2212 \u03bd1 , . . . , 1 \u2212 \u03bdp )\u22a4 , and\nrestricting the minimization to \u03c3 = \u03c3 # , we get\n\u0012\n\u0013\nmin\u03c4 \u2208[0,2\u03c0[ kY \u2212 e(\u03c4 ) \u25e6 Y # k22,\u03bd\n\u2022,#\n\u2206(Y ) = p log 1 +\n.\n2(kY k22,1\u2212\u03bd + kY # k22,1\u2212\u03bd )\nLet \u03b1 \u2208 (0, 1) be a prescribed significance level and let us introduce the statistic\n \u0303 \u2022,# ) =\n\u2206(Y\n\nk1 \u2212 \u03bdk1\nmin kY \u2212 e(\u03c4 ) \u25e6 Y # k22,\u03bd .\nkY k22,1\u2212\u03bd + kY # k22,1\u2212\u03bd \u03c4 \u2208[0,2\u03c0[\n9\n\n(18)\n\n\f# 2\n1\n \u0303 \u2022,# ) can be seen as an estimator of\nIntuitively, this new test statistic \u2206(Y\n2(\u03c32 +\u03c3# 2 ) min\u03c4 kY \u2212 e(\u03c4 ) \u25e6 Y k2,\u03bd\nused in the setting of known noise level. Therefore, it is not so much a surprise that the critical region we\ndeduce from (18) is of the form\n \u0303 \u2022,# ) \u2265 C\u03bd,\u03b1 ,\n\u2206(Y\n\nwhere C\u03bd,\u03b1 is a given threshold. To propose a choice of this threshold that leads to a test of asymptotic\n \u0303 \u2022,# ) should be characterized under the null hypothesis. Let us\nlevel \u03b1, the asymptotic distribution of \u2206(Y\nintroduce the statistic\nT (Y \u2022,# ) =\n\n \u0303 \u2022,# ) \u2212 k\u03bdk1\n\u2206(Y\n.\nk\u03bdk2\n\n(19)\n\nIn order to simplify the presentation, we develop the subsequent arguments only in the case \u03c3 = \u03c3 # = \u03c3\u2217 .\n#\nWe further assume that the noisy Fourier coefficients Y and\n\u221a Y are generated by a process described in the\nIntroduction, cf. (2), which means that \u03c3\u2217 is equal to s/ 2n with a known parameter n and an unknown\nfactor s. The asymptotic setting \u03c3\u2217 \u2192 0 corresponds then to the standard \"large sample asymptotics\"\nn \u2192 \u221e. The main advantage of this setting is that it allows us to use the knowledge of n in the choice of\nthe sequence \u03bd.\nTheorem 3. Let c \u2208 F1,L and let the sequence \u03bd satisfy conditions (A) and (B) with an integer N\u03c3\u2217 that\ndepends only on n so that N\u03c3\u2217 \u2192 +\u221e and \u03c3\u2217 N\u03c3\u2217 = O(1) when \u03c3\u2217 tends to zero. Assume, in addition,\n3/2\nthat p \u2265 2N\u03c3\u2217 is large enough to satisfy (p \u2212 N\u03c3\u2217 )\u03c3\u22172 N\u03c3\u2217 \u2192 \u221e and that for some constant c\u2032 > 0,\n\u22122\n\u2032 \u22122\nmaxj\u22651 j (1 \u2212 \u03bdj ) \u2264 c N\u03c3\u2217 . Then, under the null hypothesis, if \u03c3 = \u03c3 # tends to zero, the test statistic\nT (Y \u2022,# ) satisfies\nT (Y \u2022,# ) \u2264\n\np\nX\n\u03bdj (|\u03bej |2 \u2212 2)\nj=1\n\n2k\u03bdk2\n\n+\n\np\n\u0001\nk\u03bdk1 X 1 \u2212 \u03bdj\nOP (1)\n,\n4 \u2212 |\u03bej |2 \u2212 |\u03bej# |2 +\n3/2\n2\n4k\u03bdk2 j=1 k1 \u2212 \u03bdk1\n(p \u2212 N\u03c3\u2217 )\u03c3\u2217 N\u03c3\u2217 \u2227 (p \u2212 N\u03c3\u2217 )1/2\n\nwhere {\u03bej ; \u03bej# } are i.i.d. NC (0, 1) random variables.\nThe proof of this theorem is postponed to the Appendix. Instead, we discuss here some relevant\nconsequences of it. First of all, note that a simple application of Lyapunov's P\ncentral limit theorem im\u03bdj\np\nplies that under the conditions N\u03c3\u2217 \u2192 \u221e and (p \u2212 N\u03c3\u2217 )2 /p \u2192 \u221e both sums j=1 2k\u03bdk\n(|\u03bej |2 \u2212 2) and\n2\n\u0001\nPp\n1\u2212\u03bdj\n2\n# 2\n\u221a\nconverge in distribution to the standard Gaussian distribution. Therefore,\nj=1 2 2k1\u2212\u03bdk2 4 \u2212 |\u03bej | \u2212 |\u03bej |\nthe test defined by the critical region\n\u0013\n\u0012\nk\u03bdk1 k1 \u2212 \u03bdk2\n(20)\nz1\u2212\u03b1+\u03b2\nT (Y \u2022,# ) \u2265 min z1\u2212\u03b2 + \u221a\n0<\u03b2<1\n2k\u03bdk2 k1 \u2212 \u03bdk1\nis asymptotically of level not larger than \u03b1. Note also that a more precise critical region\nP can\u03bdj be deduced\nfrom Theorem 3 without relying on the central limit theorem. Indeed, the main terms pj=1 2k\u03bdk\n(|\u03bej |2 \u2212 2)\n2\n\u0001\nPp\n1\u2212\u03bdj\n4 \u2212 |\u03bej |2 \u2212 |\u03bej# |2 have parameter free distributions, the quantiles of which can be\nand j=1 2\u221a2k1\u2212\u03bdk\n2\ndetermined numerically by means of Monte Carlo simulations. In the case of projection weights, one can\nalso use the quantiles of chi squared distributions.\nTo conclude this section, let us have a closer look at the assumptions of the last theorem. The conditions\nare satisfied for most weights used in practice. Thus, the most\n(A), (B) and maxj6=0 j \u22122 (1 \u2212 \u03bdj ) \u2264 c\u2032 N\u03c3\u22122\n\u2217\n2 3/2\nimportant conditions are (p \u2212 N\u03c3\u2217 )\u03c3\u2217 N\u03c3\u2217 \u2192 \u221e and \u03c3\u2217 N\u03c3\u2217 \u2192 0. The first of these two conditions ensures\nthat the error term coming from the estimation of the unknown noise level is small. The second one is a\n5/2\nweak version of the condition \u03c3\u2217 N\u03c3\u2217 log N\u03c3\u2217 = o(1) present in Theorem 1, which ensures that we do not\nuse a strongly undersmoothed test statistic. We manage here to obtain a condition on N\u03c3\u2217 which is weaker\nthan the corresponding condition in Theorem 1 because we do not establish the asymptotic distribution of\n10\n\n\fthe test statistic but just an upper bound of the latter. The choice of p and N\u03c3\u2217 is particularly important\nfor obtaining a test with a power close to one, especially in the case of alternatives that are close to the null.\nHowever, the investigation of this point is out of scope of the present work. Let us just mention that if we\nchoose N\u03c3\u2217 = \u03c3\u2217\u2212\u03b2 and p = 2\u03c3\u2217\u2212\u03b3 , the conditions of Theorem 3 are satisfied if \u03b2 \u2264 min(\u03b3, 1) and 2\u03b3 + 3\u03b2 \u2265 4.\nOf course, these conditions are closely related to the assumption that the unknown signal belongs to the\nsmoothness class of regularity 1.\n4. Numerical experiments\nWe have implemented the proposed testing procedures (13) and (20) in Matlab and carried out a certain\nnumber of numerical experiments on synthetic data. The aim of these experiments is merely to show that\nthe methodology developed in the present paper is applicable and to give an illustration of how the different\ncharacteristics of the testing procedure, such as the significance level and the power, depend on the noise\nvariance \u03c3\u22172 and on the shrinkage weights \u03bd. We also aimed at comparing the performance of the testing\nprocedure (20) with that of (13). In order to ensure the reproducibility, the Matlab code of the experiments\nreported in this section is made available on https://code.google.com/p/shifted-curve-testing/.\n4.1. Behavior of the Type I error rate\nIn order to illustrate the convergence of the test statistic of the procedure (13) when \u03c3\u2217 tends to zero\nand to assess the Type I error rate, we conducted the following experiment. We chose as function f the\nsmoothed version of the HeaviSine function, considered as a benchmark in the signal processing community,\nand computed its complex Fourier coefficients {cj ; j = 0, . . . , 106 }. More precisely, the jth Fourier coefficients\ncj of f is obtained by dividing by j the corresponding Fourier coefficient of the HeaviSine function.\nAsymptotics of true negative rate\n\nAsymptotics of true negative rate\n1.01\n\nPro jection weights\nTikhonov weights\nPinsker weights\n\n1\n0.99\n\nProportion of True Negatives\n\nProportion of True Negatives\n\n1.01\n\n0.98\n0.97\n0.96\n0.95\n0.94\n\n5\n\n10\n\n15\n\nlog2 \u03c3\n\n20\n\n1\n0.99\n0.98\n0.97\n\nPro jection weights\nTikhonov weights\nPinsker weights\n\n0.96\n0.95\n0.94\n\n5\n\n10\n\n15\n\n20\n\nlog2 \u03c3 \u22122\n\n\u22122\n\nFigure 1: The proportion of true negatives in the experiment described in Section 4.1 as a function of n = log2 \u03c3\u22122 for three\ndifferent shrinkage weights: projection (dashed line), Tikhonov (solid line) and Pinsker (dash-dotted line). One can observe\nthat for all the weights the proportion of true negatives converges to the nominal level 0.95. The left panel plots the results for\nthe test procedure using the true noise level, while the right panel plots the results for the procedure corresponding to unknown\nnoise level.\n\nFor each value of n taken from the set {nk = 20 \u00d7 2k , k = 1, . . . , 15}, we repeated 105 times the following\ncomputations:\n\u22121/2\n],\n\u2022 set \u03c3\u2217 = n\u22121/2 and4 N\u03c3\u2217 = [50\u03c3\u2217\n4 This\n\nvalue of N\u03c3\u2217 satisfies the assumptions required by our theoretical results.\n\n11\n\n\f\u2022 generate the noisy sequence {Yj ; j = 1, . . . , N\u03c3\u2217 } by adding to {cj } an i.i.d. NC (0, \u03c3\u22172 ) sequence {\u03bej },\n\n\u2022 randomly choose a parameter \u03c4 \u2217 uniformly distributed in [0, 2\u03c0], independent of {\u03bej },\n\u2217\n\n\u2022 generate the shifted noisy sequence {Yj# ; j = 1, . . . , N\u03c3\u2217 } by adding to {eij\u03c4 cj } an i.i.d. NC (0, \u03c3\u22172 )\nsequence {\u03bej# }, independent of {\u03bej } and of \u03c4 \u2217 ,\n\n\u2022 compute the three values of the test statistic \u2206\u03bd,\u03c3\u2217 corresponding to the classical shrinkage weights\ndefined by (11) and compare these values with the threshold for \u03b1 = 5%.\nTikh\nPinsk\n5\nWe denote by pproj\naccept (\u03c3\u2217 ), paccept (\u03c3\u2217 ) and paccept (\u03c3\u2217 ) the proportion of experiments (among 10 that were\nrealized) for which the value of the corresponding test statistic was lower than the threshold, i.e., the\nproportion of experiments leading to the non-rejection of the null hypothesis. We plotted in the left panel\nTikh\nPinsk\nof Fig. 1 the (linearly interpolated) curves k 7\u2192 pproj\naccept (nk ), k 7\u2192 paccept (nk ) and k 7\u2192 paccept (nk ). For the\nPinsker weights, we (somewhat arbitrarily) chose \u03bc = 2, while the parameters of the Tikhonov weights were\nchosen as follows: \u03ba = 1/2 and \u03bc = 2. The graphs plotted in the left panel of Figure 1 show that the\nproportion of true negatives is very close to the nominal level 0.95, irrespectively from the choice of the\nweight sequence. Furthermore, when \u03c3\u2217 goes to zero (that is when n becomes large) the empirical Type I\nerror rate converges to the nominal level. This is in line with the claim of Theorem 1.\n\nWe also carried out the same experiment for the test statistic (18) that does not require the knowledge\nof the\u221anoise levels \u03c3 and \u03c3 # . The data generation process was the same as before, except that \u03c3\u2217 was defined\nas s/ n, where s was drawn at random uniformly in the interval [1, 4]. The cut-off parameter N\u03c3\u2217 = Nn\nwas then set to [50n1/4 ], independently of the value of s. The number p of Fourier coefficients was chosen\nproportional to N 3/2 . It is interesting to observe that in this situation as well the observed proportion of\ntrue negatives is dominating the nominal level. This test is more conservative than the one based on the\nfull knowledge of the noise level and this is not surprising since the threshold used in (20) is not based on\nas asymptotic distribution of the test statistic but merely on an asymptotic upper bound. It should also\nbe noted the noise-level-adaptive procedure has a computational complexity which is slightly higher than\nthe one of the test procedure for known \u03c3\u2217 . This increase of computational complexity is due to the fact\nthat the estimation of \u03c3\u22172 requires computing the Fourier coefficients of the observed signals corresponding\nto high frequencies.\n4.2. Power of the tests\nIn the previous experiment, we illustrated the behavior of the penalized likelihood ratio test under the\nnull hypothesis. The aim of the second experiment is to show what happens under the alternative. To this\nend, we still use the smoothed HeaviSine function as signal f and define f # = f + \u03b3\u03c6, where \u03b3 is a real\nparameter. Two cases are considered: \u03c6(t) = c cos(4t) and \u03c6(t) = c/(1 + t2 ), where c is a constant ensuring\nthat \u03c6 has an L2 norm equal to that of f . For the sake of the conciseness, only the results obtained for the\nprojection weights are reported.\nIn the experiment described in this section, we chose n from {n(k) = 2k : k = 1, . . . , 4} and \u03b3 from\n{\u03b3(l) = 0.1l : l = 1, . . . , 15}. For each value of the pair (n(k), \u03b3(l)), we repeated 5000 times the following\ncomputations:\n\u22121/2\n\n\u2022 set \u03c3\u2217 = n(k)\u22121/2 and N\u03c3\u2217 = [50\u03c3\u2217\n\n],\n\n\u2022 compute the complex Fourier coefficients {cj ; j = 1, . . . , 106 } and {c#j ; j = 1, . . . , 106 } of f and f # ,\nrespectively,\n\u2022 generate the noisy sequence {Yj ; j = 1, . . . , N\u03c3\u2217 } by adding to {cj } an i.i.d. NC (0, \u03c3\u22172 ) sequence {\u03bej },\n\n\u2022 generate the shifted noisy sequence {Yj# ; j = 1, . . . , N\u03c3\u2217 } by adding to {c#j } an i.i.d. NC (0, \u03c3\u22172 ) sequence\n{\u03bej# }, independent of {\u03bej },\n\n\u2022 compute the value of the test statistic \u2206\u03bd,\u03c3\u2217 corresponding to the projection weights and compare this\nvalue with the threshold for \u03b1 = 5%.\n12\n\n\fTo demonstrate the behavior of the test under H1 when the distance between the null and the alternative\nvaries, we computed the proportion of true positives, also called the empirical power, among the 5000\nsimulated random samples. The results, plotted in Fig. 2 show that even for moderately small values of \u03b3,\nthe test succeeds in taking the correct decision. We also clearly observe the convergence of the test since the\ncurves corresponding to small values of \u03c3\u2217 (or, equivalently, large values of n) are at the left of the curves\ncorresponding to larger values of \u03c3\u2217 . Note also that the results for \u03c6(t) = c cos(4t) and \u03c6(t) = c/(1 + t2 )\nare quite comparable.\nTrue positive rate \u03c6(t) = c/(1 + t2 )\nProportion of True Positives\n\nProportion of True Positives\n\nTrue positive rate \u03c6(t) = c cos(4t)\n1\n0.8\n0.6\n0.4\n\nn=\nn=\nn=\nn=\n\n0.2\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\n2\n4\n8\n16\n1.4\n\n1\n0.8\n0.6\n0.4\n\nn=\nn=\nn=\nn=\n\n0.2\n0\n\n0.2\n\n0.4\n\n0.6\n\n\u03b3\n\n0.8\n\n1\n\n1.2\n\n2\n4\n8\n16\n1.4\n\n\u03b3\n\nFigure 2: The proportion of true positives in the experiment described in Section 4.2 as a function of the parameter \u03b3 measuring\nthe \"distance\" from the null. The parameter n is defined as \u03c3\u2217\u22122 , where \u03c3\u2217 is the noise level. We can observe that even for\nrelatively small values of \u03b3 the test has a power close to one.\n\nProportion of True Positives\n\nTo further investigate the power of the generalized likelihood ratio test described in previous sections, we\ncarried out the last experiment with a nonsmooth function \u03c6. More precisely, we chose as \u03c6 the unsmoothed\nversion of the function \u03c8(t) = 1/(1 + t2 ), in the sense that the Fourier coefficient cj of \u03c6 is equal to the\ncorresponding Fourier coefficient of \u03c8(*) multiplied by j, for j \u2265 1. The function \u03c6 obtained in this way is\nthen normalized to have a L2 -norm equal to that of f . Note that the nonsmoothness of \u03c6 implies that of\nf # . The results are plotted in Fig. 3. When compared with Fig. 2, this plot clearly shows that the power of\nthe test gets deteriorated in the nonsmooth case. Indeed, in order to achieve a behavior of the power similar\nto that of Fig. 2, we need to multiply \u03b3 by a factor close to 10.\nTrue positive rate\nThe non-smooth case\n\n1\n0.8\n0.6\n\nn=\nn=\nn=\nn=\n\n0.4\n0.2\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n2\n4\n8\n16\n12\n\n\u03b3\nFigure 3: The proportion of true positives in the experiment described in Section 4.2 as a function of the parameter \u03b3 measuring\nthe \"distance\" from the null. The \"perturbation\" function \u03c6 is the unsmoothed version of the function c/(1 + t2 ). Comparing\nthis plot to Fig. 2, we see that, for a given n, the values of \u03b3 leading to a power close to one are much larger.\n\nFinally, we performed the same experiment for the noise-level-adaptive procedure defined by (18), (19)\nand (20). The differences compared to the protocol described in the beginning of this subsection were that\nthe values of n were chosen among {n(k) = 10 \u00d7 2k : k = 1, . . . , 4} and \u03c3\u2217 and N\u03c3\u2217 were set to sn\u22121/2\n13\n\n\fand [50n1/4 ], respectively. As in the nonsmooth case, here also we observe that the rate of convergence gets\ndeteriorated. The noise-level-adaptive\n\u221a procedure has a power equivalent to that of the original procedure\nfor noise levels that are divided by 10. This is in part explained by the fact that the adaptive test is\nconservative (cf. the right panel of Fig. 1) but also by the fact that the substitution of the noise level by an\nestimator results in an increased stochastic error.\nTrue positive rate \u03c6(t) = c/(1 + t2 )\n\nTrue positive rate \u03c6(t) = c cos(4t)\n1\n\n0.8\n0.6\n0.4\n\nn\nn\nn\nn\n\n0.2\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n= 20\n= 40\n= 80\n= 160\n\n1.2\n\nProportion of True Positives\n\nProportion of True Positives\n\n1\n\n0.8\n0.6\n0.4\n\n0\n\n1.4\n\n\u03b3\n\nn\nn\nn\nn\n\n0.2\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n= 20\n= 40\n= 80\n= 160\n\n1.2\n\n1.4\n\n\u03b3\n\nFigure 4: The proportion of true positives in the experiment described in Section 4.2 as a function of the parameter \u03b3 measuring\nthe \"distance\" from the null. As opposed to Fig. 2, here we applied the noise-level-adaptive test defined by (18), (19) and (20).\nThe parameter n is defined as \u03c3\u2217\u22122 , where \u03c3\u2217 is the noise level. We can observe that in order to get plots similar to those of\nFig. 2, we used much smaller values of \u03c3\u2217 . This means that the convergence of the noise-level adaptive-test is slower than that\nof the original generalized likelihood ratio test.\n\n5. Application to keypoint matching\nAs mentioned in the introduction, the methodology developed in previous sections may be applied to\nthe problem of keypoint matching in computer vision. More specifically, for a pair of digital images I and\nI # representing the same 3D object, the task of keypoint matching consists in finding pairs of image points\n(x0 , x#0 ) in images I and I # respectively, corresponding to the same 3D point. For more details on this and\nrelated topics, we refer the interested reader to the book [26]. For our purposes here, we assume that we are\ngiven a pair of points (x0 , x#0 ) in images I and I # respectively and the goal is to decide whether they are the\nprojections of the same 3D point or not. In fact, we will tackle a slightly simpler5 problem corresponding\nto deciding whether a neighborhood of x0 in the image I coincides with a neighborhood of x#0 in the image\nI # up to a rotation. Of course, this problem is made harder by the fact that the images are contaminated\nby noise.\nThe plan of this section is as follows. As a first step, we present a new definition of a local descriptor\n(termed LoFT for Localized Fourier Transform) of a keypoint x0 in some image I. This local descriptor is\nbased on the Fourier coefficients of some mapping related to the local neighborhood of the image I around\nx0 . Therefore, it is particularly well suited for testing for rotation between two keypoints. In a second\nstep, we define a matching criterion: a {0, 1}-valued mapping that takes as input pairs (x0 , I) and (x#0 , I # )\nand outputs 1 if and only if x0 and x#0 are classified as matching points (i.e., corresponding to the same\n3D point). Finally, as a third step, we perform several experiments showing the potential of the proposed\napproach.\n5 Note here that using the state-of-the-art techniques of keypoint detection based on the differences of Gaussians, see [34],\none can recover a rather reliable value of the scale parameter for every keypoint. Using this scale parameter, the problem of\ntesting for a similarity transform reduces to the problem of testing for a rotation that we consider in this section.\n\n14\n\n\f(a)\n\u0001\n\n(b)\n\n(c)\n\nr\n\n200\n\n200\n\n3r\n2\n\n100\n\n100\n\n0\n0\n200\n\n2r\n2\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n100\n\nr\n2\n\n0\n200\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n100\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n50\n\n\u03c0\n\n3\u03c0\n2\n\n2\u03c0\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n0\n200\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n100\n\n0\n\n\u03c0\n2\n\n1\n\n100\n\n0\n\n100\n\n0\n\n0\n0\n200\n\n0\n0\n100\n50\n\n0\n\n0\n0\n\n(d)\n\n0\n\n(e)\n\n(f )\n\nFigure 5: Illustration of the construction of the LoFT descriptor of a keypoint x0 in an image I. Once a keypoint is chosen\nalong with the radius of the neighborhood to be considered (a,b), we split the corresponding subimage into 4 rings of equal areas\nrepresented in (c). In (d), we plot the pixel intensities as a function of polar coordinates of the pixel. Each band corresponds\nto a ring in (c). The four curves in (e) are obtained by averaging the pixel intensities corresponding to the same angle within\neach band of (d). Smoothed versions of these curves obtained by removing high frequencies are plotted in (f).\n\n5.1. LoFT descriptor\nWe start by describing the construction of the LoFT descriptor of a point x0 in a digital image I. For\nthe purpose of illustration we use color images, but all the experiments were conducted on grayscale images\nonly. As any other construction of local descriptor, it is necessary to choose a radius r > 0 that specifies the\nneighborhood around x0 . In all our experiments we used r = 32 pixels, which seems to lead to good results.\nThus, we restrict the image I to the disc D(x0 , r) with center x0 and radius r, as shown in Fig. 6 (a) and\n(b). Note that this disc contains approximately \u03c0r2 > 3000 pixels, but we will encode the restriction of I\nto D(x0 , r) by a vector of size 128.\nThe main idea consists in considering the function X : [0, 2\u03c0] \u2192 R4 defined by\n\uf8ee\n\uf8f9\nX1 (t)\nZ \u221alr/2\n\u0001\n\uf8ef .. \uf8fa\nX(t) = \uf8f0 . \uf8fb ,\nXl (t) = \u221a\nI x0 + u[sin(t), cos(t)] du, l = 1, . . . , 4.\n\n(21)\n\nl\u22121r/2\n\nX4 (t)\n\nIn other terms, each Xl (t) describes the behavior of I on some ring centered at x0 , cf. Fig. 5(c) for an\nillustration. As shown in Fig. 5(e), because of noise and textures present in the images, the functions Xi are\nhighly nonsmooth. Since the details are not necessarily very informative when matching two image regions,\nwe suggest to smooth out the functions Xl by removing high frequency Fourier coefficients, cf. Fig. 5(f).\nThe resulting descriptor is the vector composed of the first k Fourier coefficients\n1\nYj = \u221a\n2\u03c0\n\nZ\n\n2\u03c0\n\nX(t)eijt dt,\n\n0\n\n15\n\nj = 1, . . . , k.\n\n(22)\n\n\fTo get a descriptor of size 128 (a complex number is encoded as two real numbers corresponding to its real\nand imaginary parts), we chose k = 16. The computation of each element of the descriptor requires thus to\nevaluate an integral of the form\n1\nYj (l) = \u221a\n2\u03c0\n\nZ\n\n0\n\n2\u03c0\n\nZ\n\n\u221a\nlr/2\n\n\u221a\nl\u22121r/2\n\n\u0001\neijt I x0 + u[sin(t), cos(t)] du dt.\n\n(23)\n\nNote that we dispose only a regularly sampled version of the image I in the Cartesian coordinate system. This results in a nonregular sampling in the polar coordinate system, illustrated in Fig. 5(d). The\nintegrals are then approximated by the corresponding Riemann sums; the rings being chosen so that they\ncontain approximately the same number of sampled points, the qualities of these approximations are roughly\nequivalent.\n5.2. Matching criterion\nThe rationale for using this function X is the following. Assume that x0 and x#0 are two true matches\nand that the images are observed without noise. That is to say that x0 and x#0 are two points in I and I # ,\nrespectively, such that if we rotate I by some angle \u03c4 \u2217 around x0 then in the neighborhood of x0 of radius r\nthe image I coincides with I # in the neighborhood of x\u2032 . Or, mathematically speaking, \u2200(u, t) \u2208 [0, r]\u00d7[0, 2\u03c0],\n\u0001\n\u0001\n(24)\nI x0 + u[sin(t \u2212 \u03c4 \u2217 ), cos(t \u2212 \u03c4 \u2217 )] = I # x#0 + u[sin t, cos t] .\n\nThen, by simple integration and using (21) one checks that\nX(t \u2212 \u03c4 \u2217 ) = X # (t),\n\n\u2200t \u2208 [0, 2\u03c0],\n\n(25)\n\nwhere X # is defined in the same manner as X, that is by replacing in (21) I by I # and x0 by x#0 . Furthermore,\nsince these two functions are 2\u03c0-periodic, relation (25) holds for the smoothed versions of X and X # as\nwell.\nThis observation, depicted in Fig. 6, leads to the following criterion for keypoint matching based on\ntheir LoFT descriptors. Given a threshold \u03bb > 0 and a (estimated) noise level \u03c3, we declare that the LoFT\ndescriptors Y and Y # corresponding to the keypoints x0 and x#0 and defined by (23) match if and only if\n\u2206 :=\n\nk\nX\n1\nkY j \u2212 e\u2212ij\u03c4 Y #j k22 \u2264 \u03bb.\nmin\n4\u03c3 2 \u03c4 \u2208[0,2\u03c0] j=1\n\n(26)\n\nAccording to the theoretical results established in foregoing sections, under the null hypothesis (that is\nwhen the pair (x0 , x#0 ) is a true match) the test statistic \u2206 is asymptotically parameter free and the limiting\ndistribution is Gaussian with zero mean and a variance that can be easily computed. We carried out some\nexperiments, reported in the next subsection, which show that this property holds not only for small \u03c3, but\nalso for reasonably high values of it. Furthermore, substituting the true noise level by an estimated one\nyields sensibly similar results.\n5.3. Experimental evaluation\nTo empirically assess the properties of the LoFT descriptor and the matching criterion defined in (26), we\ncarried out some numerical experiments. All the codes and the images necessary to reproduce the results and\nthe figures reported in this section can be freely downloaded from the website http://imagine.enpc.fr/~dalalyan/LoFT.\nWe chose two grayscale images of resolution 300 \u00d7 450 that coincide up to a rotation by an angle \u03c0/2.\nWe degraded these images by adding two independent white Gaussian noises of variance \u03c3 2 . This resulted\nin two noisy images I and I # depicted in Fig. 7 (left panels). Then we chose at random L = 104 pairs of\ntruly matching points (xl , x#l ). The only restriction made on these points is that the distance between two\ndistinct points xl and xl\u2032 is at least of 5 pixel. Then we chose points x\u0303#l such that x\u0303#l = x#l + (10, 10) which\n16\n\n\f200\n\n200\n\n150\n\n150\n\n100\n\n100\n\n50\n\n50\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\n2\n\n3\n\n4\n\n5\n\n6\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n200\n\n150\n\n150\n\n100\n\n100\n\n50\n\n50\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\n200\n\n200\n\n150\n\n150\n\n100\n\n100\n\n50\n\n50\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n80\n\n80\n\n60\n\n60\n\n40\n\n40\n\n20\n\n20\n0\n\n(a)\n\n1\n\n200\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n(b)\n\n(c)\n\n(d)\n\nFigure 6: Rotated image regions correspond to shifted curves. (a) and (b) are an image region and the corresponding curves\nused in the definition of the LoFT descriptor. (c) is the image region (a) rotated by the angle \u03c0/4, the corresponding curves\nare depicted in (d). We clearly see that the four curves in (d) are obtained from those in (b) by shifting to left (with a shift\nequal to \u03c0/2 \u2248 1.57.\n\nwe used as false matches with xl 's. We computed the corresponding LoFT descriptors in each image. This\n#\nyielded three sets of vectors {Y l }, {Y #l } and {\u1ef8 l }. Finally, the values of the test statistic \u2206 were computed\n#\n\u2032\nfor the pairs (Y l , Y #l ) and (Y l , \u1ef8 l ). We obtained two samples \u03b41 , . . . , \u03b4L and \u03b41\u2032 , . . . , \u03b4L\n. The first sample\ncharacterizes the behavior of the test statistic under the null (i.e., for true matches), whereas the second\nsample characterizes the behavior of the test statistic under the alternative (false matches).\nThe parallel boxplots of these two samples, computed for several values of \u03c3, are plotted in the rightbottom panel of Fig. 7. Two scenarios were considered: known \u03c3 and unknown \u03c3. In the second scenario the\nestimator of \u03c3 proposed by [28] were used and injected in (26) instead of \u03c3. The results of the first scenario\nare plotted in the first row of the right-bottom panel of Fig. 7, while those of the second scenario are in the\nsecond row. One can note that the different values for the noise level considered in these experiments are\n\u03c3 \u2208 {5, 10, 30, 60}.\n\nIn the light of these figures, several observations can be made. Perhaps the most striking one is that\neven for a noise level as high as6 \u03c3 = 30, there is a clear separation between the two samples. Furthermore,\nthe top-right panel of Fig. 7 shows that the distribution of the test statistic under the null is extremely close\nto the Gaussian distribution, as proved in our theoretical results. Therefore, choosing as \u03bb any reasonable\nquantile (95%, 99%, 99.9%) of this distribution results in rejecting all the false matches. In other terms, the\np-values associated to the elements of the second sample, the one of false matches, are all below the level of\n0.1%.\nA second important observation is that the result is almost not impacted by the substitution of the true\nnoise variance by its estimated value. This may be very useful for applying LoFT descriptors to very noisy\nimages such as those encountered in medical imaging and astrophysics. A last observation is that when\n\u03c3 = 60, the noise is so strong that nearly 5% of false matches are classified as true matches, when the\nthreshold \u03bb in (26) is chosen equal to 2.22, which is the 99%-quantile of the distribution of the test statistic\nunder the null. This is not so surprising and shows the limits of the presented approach.\nTo close this section, let us stress that the primary aim here was to show the applicability and the\npotential of the proposed approach. A more comprehensive experimental evaluation of the discriminative\npower of the LoFT descriptor comparing it to other state-of-the-art descriptors is the subject of an ongoing\nwork.\n6 Note that the standard deviation of the image intensities in this example being equal to 40.25, the signal-to-noise ratio is\nvery small.\n\n17\n\n\f2\n1.8\n\nhist ogram of t he t est\nst at ist ic under H 0\n\n1.6\n\nnor mal densit y funct ion\n\n1.4\n1.2\n1\n0.8\n0.6\n0.4\n0.2\n0\n0.8\n\n1\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2\n\n2.2\n\n8\n\n8\n\n8\n\n8\n\n6\n\n6\n\n6\n\n6\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n0\n\n0\n1\n\n2\n\n0\n1\n\n\u03c3= 5\n\n2\n\n2\n\n1\n\n\u03c3 = 30\n\n8\n\n8\n\n8\n\n6\n\n6\n\n6\n\n6\n\n4\n\n4\n\n4\n\n4\n\n2\n\n2\n\n2\n\n2\n\n0\n1\n\n2\n\n\u03c3= 5\n\n0\n1\n\n2\n\n\u03c3 = 10\n\n2.8\n\n2\n\n\u03c3 = 60\n\n8\n\n0\n\n2.6\n\n0\n1\n\n\u03c3 = 10\n\n2.4\n\n0\n1\n\n2\n\n\u03c3 = 30\n\n1\n\n2\n\n\u03c3 = 60\n\nFigure 7: Experimental evaluation of the discriminative power of the LoFT descriptor. Top-left and bottom-left: two noisy\nimages used in our experiments that coincide up to a rotation by angle \u03c0/2. Top-right: the histogram of the test statistic \u2206 (cf.\n(26)) computed for 104 randomly chosen pairs of truly matching points. The true noise level is equal to 30, for image intensities\nranging from 0 to 255. The value of \u03c3 used in (26) is the estimated noise level computed by the procedure described in [28].\nOne can observe that the distribution is very close to a Gaussian one. Bottom-right: the boxplots of the logarithm of the test\nstatistic \u2206 for true matches and for false matches computed for different noise levels. In the first (top) row the true \u03c3 is used\nin (26), while in the second row we used the estimator provided by [28]. A remarkable property is that the boxplots under H0\nare almost not impacted by the change of \u03c3. It is also noteworthy that the boxplots of true matches are well separated from\nthose of false matches for all values of \u03c3 except for \u03c3 = 60.\n\n6. Conclusion\nIn the present work, we provided a methodological and theoretical analysis of the curve registration\nproblem from a statistical standpoint based on the nonparametric goodness-of-fit testing. In the case\nwhere the noise is white Gaussian and additive with a small variance we established that under the null\nhypothesis the penalized log-likelihood ratio statistic is asymptotically distribution free. This result is valid\nfor the weighted l2 -penalization under some mild assumptions on the weights. Furthermore, we proved\n18\n\n\fthat the test based on the Gaussian (or chi-squared) approximation of the penalized log-likelihood ratio\nstatistic is consistent. These results naturally carry over to other nonparametric models for which asymptotic\nequivalence (in the Le Cam sense) with the Gaussian white noise has been proven.\nIt can be interesting, however, to develop a direct inference in these models. In particular, the model of\nspatial Poisson processes (cf. [29]) can be of special interest because of its applications in image analysis.\nSome important issues closely related to the present work have been treated in the companion paper [12].\nIn that paper, focusing on the case of equal variances of noise and considering only projection weights,\nthe minimax rate of separation of the null hypothesis from the alternative is obtained (up to a log-factor)\nand an adaptive test is proposed. It should be mentioned, that although the procedure proposed in [12] is\nadaptive and achieves the asymptotically minimax rates of separation, it is not necessarily suitable for the\nreal applications because it is often overly conservative. This is due to the fact that the minimax procedures\nare essentially designed to be optimal in the worst cases, but can be outperformed in the favorable situations.\nFinally, we demonstrated that the main ideas introduced in the present work lead to a new local descriptor\nfor digital images tailored to testing for rotation between two image regions. The optimization of the\nimplementation of this descriptor and its more systematic evaluation on various benchmark datasets used\nin computer vision is another promising direction of future research.\nAppendix A. Proofs of the theorems\nThis and the following appendices contain the technical proofs of the theorems stated in previous sections.\nFor the sake of self-containedness, we provide all the details of the proofs although some of them-such as\nthe Berman theorem-have been already used in earlier references (see, for instance, [12]).\nThe proof of Wilks' phenomenon is divided into several parts. First we assume that H0 is true and study\nthe convergence of the pseudo-estimator \u03c4\u0302 (of the shift \u03c4\u0304 \u2217 ) defined as the maximizer of the log-likelihood\n\u2217\nover the interval [\u03c4\u0304 \u2217 \u2212 \u03c0, \u03c4\u0304 \u2217 + \u03c0]. Here, \u03c4\u0304 \u2217 is an element of [0, 2\u03c0[ such that cj = e\u2212ij \u03c4\u0304 c#j , for all j \u2265 1.\nAppendix A.1. Maximizer of the log-likelihood\nProposition 1. Let c \u2208 Fs,L for some s \u2208 (0, 1], L > 0 and let |c1 | > 0. If the shrinkage weights \u03bdj satisfy\nconditions (A) and (B), then the solution \u03c4\u0302 to the optimization problem\nX\n\u03c4\u0302 = arg max\nM (\u03c4 ),\nwith\nM (\u03c4 ) =\n\u03bdj Re(eij\u03c4 Yj Yj# )\n\u2217\n\u03c4 :|\u03c4 \u2212\u03c4\u0304 |\u2264\u03c0\n\nj\u22651\n\nsatisfies the asymptotic relation\n|\u03c4\u0302 \u2212 \u03c4\u0304 \u2217 | = \u03c3\u2217\n\np\n\u0001\nlog N\u03c3\u2217 N\u03c31\u2212s\n+ \u03c3\u2217 N\u03c33/2\nOP (1),\n\u2217\n\u2217\n\nas\n\n\u03c3\u2217 \u2192 0.\n\u2217\n\nProof of Proposition 1. Throughout this proof, we work under the null hypothesis H0 . If we set \u03b7j = e\u2212ij \u03c4\u0304 \u01ebj\nand \u03b7j# = \u01eb#j , we can write the decomposition\nM (\u03c4 ) \u2212 E[M (\u03c4 )] = \u03c3\u2217 S(\u03c4 ) + \u03c3\u03c3 # D(\u03c4 + \u03c4\u0304 \u2217 ),\nwhere\nS(\u03c4 ) =\n\nX\nj\u22651\n\n\u0011o\n\u0010\u03c3\nn\n\u03c3#\ncj \u03b7j + cj \u03b7j# ,\n\u03bdj Re eij\u03c4\n\u03c3\u2217\n\u03c3\u2217\n\nD(\u03c4 ) =\n\nX\nj\u22651\n\n\u0001\n\u03bdj Re eij\u03c4 \u03b7j \u03b7j# .\n\nP\nFurthermore, the expectation of M (\u03c4 ) is given by E[M (\u03c4 )] = j\u22651 \u03bdj |cj |2 cos[j(\u03c4 \u2212 \u03c4\u0304 \u2217 )]. In what follows,\nfor every function f : R \u2192 R, we denote by kf k\u221e the supremum over R of the function f .\n19\n\n\fOn the one hand, using the assumption |c1 | > 0 along with condition (A), we get that for every \u03c4 \u2208\n[\u03c4\u0304 \u2217 \u2212 \u03c0, \u03c4\u0304 \u2217 + \u03c0] it holds\n\u0002\n\u0003\n\u0002\n\u0003\n\u2217\nE M (\u03c4 ) \u2212 E M (\u03c4\u0304 \u2217 )\n2|c1 |\n2 1 \u2212 cos(\u03c4 \u2212 \u03c4\u0304 )\n\u2264\n\u2212\u03bd\n|c\n|\n\u2264 \u2212 2 , C < 0.\n1\n1\n\u2217\n2\n\u2217\n2\n(\u03c4 \u2212 \u03c4\u0304 )\n(\u03c4 \u2212 \u03c4\u0304 )\n\u03c0\nTherefore,\n\u0002\n\u0003\n\u0002\n\u0003\nM (\u03c4 ) \u2212 M (\u03c4\u0304 \u2217 ) = E[M (\u03c4 )] \u2212 E[M (\u03c4\u0304 \u2217 )] + \u03c3\u2217 S(\u03c4 ) \u2212 S(\u03c4\u0304 \u2217 ) + \u03c3\u03c3 # D(\u03c4 ) \u2212 D(\u03c4\u0304 \u2217 )\n\u2264 \u2212C |\u03c4 \u2212 \u03c4\u0304 \u2217 |2 + \u03c3\u2217 |\u03c4 \u2212 \u03c4\u0304 \u2217 | * kS \u2032 k\u221e + \u03c3\u22172 |\u03c4 \u2212 \u03c4\u0304 \u2217 | * kD\u2032 k\u221e\n\b\n= |\u03c4 \u2212 \u03c4\u0304 \u2217 | \u03c3\u2217 kS \u2032 k\u221e + \u03c3\u22172 kD\u2032 k\u221e \u2212 C|\u03c4 \u2212 \u03c4\u0304 \u2217 | .\n\nReplacing in this inequality \u03c4 by \u03c4\u0302 and using that M (\u03c4\u0302 ) \u2212 M (\u03c4\u0304 \u2217 ) \u2265 0, we get\n\b\n|\u03c4\u0302 \u2212 \u03c4\u0304 \u2217 | \u2264 C \u22121 \u03c3\u2217 kS \u2032 k\u221e + \u03c3\u22172 kD\u2032 k\u221e .\n\n(A.1)\n\nOn the other hand, we have\n\np\n\u0001\n\u03c32 + \u03c3# 2 X\nS (\u03c4 ) =\nj|cj |\u03bdj Re eij\u03c4 \u03b6j ,\n\u03c3\u2217\n\u2032\n\nj\u22651\n\nwhere \u03b6j are i.i.d. complex valued random variables, whose real and imaginary parts are independent\nstandard Gaussian random variables. Therefore, the large deviations of the sup-norm of S \u2032 can be controlled\nusing the following lemma.\nLemma 2. Let s = (s1 , . . . , sN ) be a vector from RN and let {\u03bej } and {\u03bej\u2032 } be two independent sequences\nPN\nof i.i.d. N (0, 1) random variables. The sup-norm of the function Z(t) = j=1 sj {cos(jt)\u03bej + sin(jt)\u03bej\u2032 },\nsatisfies\n2\nP(kZk\u221e \u2265 ksk2 x) \u2264 (N + 1)e\u2212x /2 ,\n\u2200x > 0.\nProof. See Appendix C.\nTo apply this result to Z(t) = S \u2032 (t), we choose sj =\n(s1 , . . . , sN\u03c3\u2217 ) with Euclidean norm\nksk22 =\n\n\u221a\n\n\u03c32 +\u03c3# 2\nj|cj |\u03bdj ,\n\u03c3\u2217\n\nwhich leads to a vector s =\n\n2 N\u03c3\n\u03c3 2 + \u03c3 # X\u2217 2 2 2\nL2 .\nj |cj | \u03bdj \u2264 2N\u03c32(1\u2212s)\n\u2217\n\u03c3\u22172\nj=1\n\nThe last inequality follows from the fact that c \u2208 Fs,L , \u03c3\u2217 = max(\u03c3, \u03c3 # ) and \u03bdj \u2208 [0, 1] for every j. Using\nthis bound, Lemma 2 and the fact that N\u03c3\u2217 \u2265 1, we get that the inequality\n\u0011 \u03b1\n\u0010\np\nlog(4N\u03c3\u2217 /\u03b1) \u2264\n(A.2)\nP kS \u2032 k\u221e \u2265 2LN\u03c31\u2212s\n\u2217\n2\n\nholds true for every \u03b1 \u2208 (0, 1).\n\nFinally, the large deviations of the term kD\u2032 k\u221e are controlled by the following lemma.\n\nLemma 3. Let N be some positive integer and let \u03b7j , \u03b7j# , j = 1, . . . , N be independent complex valued\nrandom variables such that their real and imaginary parts areP\nindependent standard\n\u0001 Gaussian variables. Let\nN\ns = (s1 , . . . , sN ) be a vector of real numbers. Denote Z(t) = j=1 sj Re eijt \u03b7j \u03b7j# for every t in [0, 2\u03c0] and\nkZk\u221e = supt\u2208[0,2\u03c0] |Z(t)|. Then,\nn\n\u221a\n\u0001o\n2\n2\nP kZk\u221e > 2x ksk2 + yksk\u221e \u2264 (N + 1)e\u2212x /2 + e\u2212y /2 ,\n20\n\n\u2200x, y > 0.\n\n\fProof. See Appendix C.\nIn order to bound the sup-norm of D\u2032 (*) using Lemma 3, we set N = N\u03c3\u2217 and sj = j\u03bdj for all j =\n3/2\n1, . . . , N\u03c3\u2217 . This yields ksk2 \u2264 N\u03c3\u2217 and ksk\u221e \u2264 N\u03c3\u2217 . Therefore,\nn\n\u221a\np\n\u0001o\n2\n2\nP kD\u2032 k\u221e > 2xN\u03c3\u2217\n\u2200x, y > 0.\n(A.3)\nN\u03c3\u2217 + y \u2264 (N\u03c3\u2217 + 1)e\u2212x /2 + e\u2212y /2 ,\n\np\np\nFor any \u03b1 \u2208 (0, 1), choosing x = 2 log(8N\u03c3\u2217 /\u03b1) and y = 2 log(4/\u03b1), we arrive at\nn\np\np\np\n\u0001o \u03b1\nP kD\u2032 k\u221e > 2N\u03c3\u2217 log(8N\u03c3\u2217 /\u03b1) N\u03c3\u2217 + log(4/\u03b1) \u2264 .\n2\n\n(A.4)\n\nInequalities (A.1), (A.2) and (A.4) imply that |\u03c4\u0302 \u2212\u03c4\u0304 \u2217 | is, in probability, at most of the order \u03c3\u2217\n3/2 \u0001\n\u03c3\u2217 N\u03c3\u2217 .\n\np\nlog N\u03c3\u2217 N\u03c31\u2212s\n+\n\u2217\n\nAppendix A.2. Proof of Theorem 1\n\nRecall that we present the proof in the case c \u2208 Fs,L for s > 7/8. The claim of Theorem 1 can be readily\nobtained by taking s = 1.\nOne can check that, under H0 ,\n\u2206\u03bd,\u03c3\u2217 (Y\n\n\u2022,#\n\n#\n\" +\u221e\nX\n1\n\u2212ij\u03c4 # 2\n\u03bdj Yj \u2212 e\nYj\nmin\n)=\n2(\u03c3 2 + (\u03c3 # )2 ) \u03c4 \u2208[0,2\u03c0[ j=1\n=\n\nwhere we have used the notation:\nD\u03c3\u2217 (\u03c4 ) =\n\n+\u221e\nX\nj=1\n\nC\u03c3\u2217 (\u03c4 ) =\n\n+\u221e\nX\nj=1\n\nP\u03c3\u2217 (\u03c4 ) =\n\n+\u221e\nX\nj=1\n\n2(\u03c3 2\n\n(A.5)\n\n\b\n1\nmin\nD\u03c3\u2217 (\u03c4 ) + 2C\u03c3\u2217 (\u03c4 ) + P\u03c3\u2217 (\u03c4 ) ,\n#\n2\n\u2217\n+ (\u03c3 ) ) |\u03c4 \u2212\u03c4\u0304 |\u2264\u03c0\n\n\u03bdj |cj |2 1 \u2212 e\u2212ij(\u03c4 \u2212\u03c4\u0304\n\n\u2217\n\n) 2\n\n,\n\n(A.6)\n\n(deterministic term)\n\n\u0002\n\u0001\u0003\n\u2217 \u0001\n\u03bdj Re cj 1 \u2212 e\u2212ij(\u03c4 \u2212\u03c4\u0304 ) \u03c3\u01ebj \u2212 e\u2212ij\u03c4 \u03c3 # \u01eb#j ,\n\n(cross term)\n\n2\n\n\u03bdj \u03c3\u01ebj \u2212 e\u2212ij\u03c4 \u03c3 # \u01eb#j .\n\n(principal term)\n\u2217\n\n(Since H0 is assumed satisfied, there exists \u03c4\u0304 \u2217 \u2208 [0, 2\u03c0[ such that cj = e\u2212ij \u03c4\u0304 c#j for all j \u2265 1.) We denote\nby \u03c4\u0302 the pseudo-estimator of \u03c4\u0304 \u2217 defined as the minimizer of the right-hand side of (A.5) over the interval\n[\u03c4\u0304 \u2217 \u2212 \u03c0, \u03c4\u0304 \u2217 + \u03c0] and study the asymptotic behavior of the terms D\u03c3\u2217 , C\u03c3\u2217 and P\u03c3\u2217 separately.\nFor the deterministic term, in view of Proposition 1, it holds that\n|D\u03c3\u2217 (\u03c4\u0302 )| \u2264\n=\n\n+\u221e\nX\nj=1\n\nN \u03c3\u2217\n\nj 2 \u03bdj |cj |2 (\u03c4\u0302 \u2212 \u03c4\u0304 \u2217 )2 \u2264 (\u03c4\u0302 \u2212 \u03c4\u0304 \u2217 )2\n\n(N\u03c32(1\u2212s)\n{\u03c3\u22172 N\u03c32(1\u2212s)\n\u2217\n\u2217\n\n+\n\nX\nj=1\n\nL2 (\u03c4\u0302 \u2212 \u03c4\u0304 \u2217 )2\nj 2 |cj |2 \u2264 N\u03c32(1\u2212s)\n\u2217\n\n\u03c3\u22172 N\u03c33\u2217 ) log N\u03c3\u2217 } Op (1).\n\nTherefore,\n7\n9\n|D\u03c3\u2217 (\u03c4\u0302 )|\n2\n2 \u22124s\n2 \u22122s\n\u2264\n{(N\n+\n\u03c3\nN\n) log N\u03c3\u2217 } Op (1).\n\u03c3\n\u03c3\n\u2217\n\u2217\n\u2217\n\u03c3\u22172 k\u03bdk2\n\n21\n\n(A.7)\n\n\fLet us turn now to the cross term. As C\u03c3\u2217 (\u03c4\u0304 \u2217 ) = 0, we have\n|C\u03c3\u2217 (\u03c4\u0302 )| \u2264 |\u03c4\u0302 \u2212 \u03c4\u0304 \u2217 | * kC\u03c3\u2032 \u2217 k\u221e .\nFurthermore, if we define \u03bej and \u03bej\u2032 as respectively the real and the imaginary parts of the random variable\n\u0001\n\u2217\nc\n\u221a j\n\u03c3\u01edj \u2212 eij \u03c4\u0304 \u03c3 # \u01eb \u0304#j , we get\n|c | \u03c32 +\u03c3# 2\nj\n\nC\u03c3\u2032 \u2217 (\u03c4\u0304 \u2217 + t) =\n\n+\u221e\np\nX\n\u0003\n\u0002\nj\u03bdj |cj | sin(jt)\u03bej + cos(jt)\u03bej\u2032 ,\n\u03c3 2 + \u03c3 #2\nj=1\n\nUsing Lemma 2, we check that kC\u03c3\u2032 \u2217 k\u221e is, in probability, of the order {\u03c3\u2217 N\u03c31\u2212s\n\u2217\nview of Proposition 1, it holds that\n\n\u2200t \u2208 R.\np\nlog N\u03c3\u2217 }. Therefore, in\n\nlog N\u03c3\u2217 } Op (1)\n)N\u03c31\u2212s\n+ \u03c3\u2217 N\u03c33/2\n|C\u03c3\u2217 (\u03c4\u0302 )| = {\u03c3\u22172 (N\u03c31\u2212s\n\u2217\n\u2217\n\u2217\nand, therefore,\n3\n|C\u03c3\u2217 (\u03c4\u0302 )|\n\u22122s\n= {(N\u03c32\u2217\n) log N\u03c3\u2217 } Op (1).\n+ \u03c3\u2217 N\u03c32\u2212s\n\u2217\n2\n\u03c3\u2217 k\u03bdk2\n\n(A.8)\n\nP+\u221e\n2\nLet us study the last term, P\u03c3\u2217 (\u03c4 ) = j=1 \u03bdj \u03c3\u01ebj \u2212 e\u2212ij\u03c4 \u03c3 # \u01eb#j , which will determine the asymptotic\n\u2217\nbehavior of the test statistic. Denoting \u03b7j = eij \u03c4\u0304 \u01ebj and \u03b7j# = \u01eb#j , we can rewrite this term as P\u03c3\u2217 (\u03c4 ) =\nP+\u221e\n\u2212ij(\u03c4 \u2212\u03c4\u0304 \u2217 ) # # 2\n\u03c3 \u03b7j . We wish to prove that under the null hypothesis H0 , if conditions (A),\nj=1 \u03bdj \u03c3\u03b7j \u2212 e\n5/2\n\n(B), N\u03c3\u2217 \u2192 +\u221e and \u03c3\u22172 N\u03c3\u2217 log(N\u03c3\u2217 ) = oP (1) are fulfilled, then\nH\u03c3\u2217 (\u03c4\u0302 ) =\n\nP\u03c3\u2217 (\u03c4\u0302 ) \u2212 2(\u03c3 2 + (\u03c3 # )2 )k\u03bdk1 D\n\u2212\u2212\u2212\u2212\u2192 N (0, 1).\n\u03c3\u2217 \u21920\n2(\u03c3 2 + (\u03c3 # )2 )k\u03bdk2\n\nTo check this property, we decompose the principal term as follows:\nH\u03c3\u2217 (\u03c4\u0302 ) = H\u03c3\u2217 (\u03c4\u0304 \u2217 ) + R\u03c3\u2217 (\u03c4\u0302 ),\n\nP\u03c3\u2217 (\u03c4\u0302 ) \u2212 P\u03c3\u2217 (\u03c4\u0304 \u2217 )\n.\n2(\u03c3 2 + (\u03c3 # )2 )k\u03bdk2\n\nwith\n\nR\u03c3\u2217 (\u03c4\u0302 ) =\n\nXj,\u03c3\u2217 =\n\n\u03bdj \u0010 \u03c3\u03b7j \u2212 \u03c3 # \u03b7j#\n2k\u03bdk2 (\u03c3 2 + (\u03c3 # )2 )1/2\n\nWe start by writing H\u03c3\u2217 (\u03c4\u0304 \u2217 ) as\nN \u03c3\u2217\n\u2217\n\nH\u03c3\u2217 (\u03c4\u0304 ) =\n\nX\n\nXj,\u03c3\u2217 ,\n\nwith\n\nj=1\n\n2\n\n\u0011\n\u22122 ,\n\nand, by applying the Berry-Esseen inequality [39, Theorem 5.4], which is valid since the Xj,\u03c3\u2217 's are independent random variables with mean 0 and finite third moment. Furthermore, we have\nN \u03c3\u2217\n\nB\u03c3\u2217 =\n\nX\nj=1\n\n\u0001\nVar Xj,\u03c3\u2217 = 1,\n\n\u22123\n\nL\u03c3\u2217 = B\u03c3\u22172\n\nand\n\nN \u03c3\u2217\n\nX\nj=1\n\n\u22121\n\nE|Xj,\u03c3\u2217 |3 \u2264 C N\u03c3\u22172 .\n\nTherefore, the Berry-Esseen inequality yields supx |F\u03c3\u2217 (x) \u2212 \u03a6(x)| \u2264 K L\u03c3\u2217 , where \u03a6 is the c.d.f. of the\n\u0001\n\u2212 1 PN\u03c3\u2217\nXj,\u03c3\u2217 < x and K is an absolute constant. Hence\nstandard Gaussian distribution, F\u03c3\u2217 (x) = P B\u03c3\u22172 j=1\nD\n\nH\u03c3\u2217 (\u03c4\u0304 \u2217 ) \u2212\u2212\u2212\u2212\u2192 N (0, 1).\n\u03c3\u2217 \u21920\n\n22\n\n\fIt remains to prove that R\u03c3\u2217 (\u03c4\u0302 ) tends to 0 in probability, which-in view of Slutski's lemma-will be\nsufficient for completing the proof. By the mean value theorem, there exists some real number t between \u03c4\u0302\nand \u03c4\u0304 \u2217 such that\nR\u03c3\u2217 (\u03c4\u0302 ) =\n\n=\n\n+\u221e\nX\n\u0001\n\u2217\n\u03bdj\n\u03c3\u03c3 #\nRe \u03b7j \u03b7j# (eij(\u03c4\u0302 \u2212\u03c4\u0304 ) \u2212 1)\n2\n#\n2\n\u03c3 + (\u03c3 ) j=1 k\u03bdk2\nN \u03c3\u2217\nX j\u03bdj (\u03c4\u0302 \u2212 \u03c4\u0304 \u2217 )\n\u0001\n\u03c3\u03c3 #\nRe eijt \u03b7j \u03b7j# .\n2\n#\n2\n\u03c3 + (\u03c3 ) j=1\nk\u03bdk2\n\nThen, by virtue of Lemma 3,\n|R\u03c3\u2217 (\u03c4\u0302 )| \u2264\n\nN \u03c3\u2217\nX\n\u0001\n\u03c3\u03c3 # |\u03c4\u0302 \u2212 \u03c4\u0304 \u2217 |\nj\u03bdj Re eijt \u03b7j \u03b7j#\nsup\n2\n#\n2\n(\u03c3 + (\u03c3 ) )k\u03bdk2 t\u2208[0,2\u03c0] j=1\n\n)N\u03c3\u2217 log N\u03c3\u2217 } * OP (1).\n+ \u03c3\u2217 N\u03c33/2\n= {\u03c3\u2217 (N\u03c31\u2212s\n\u2217\n\u2217\nCombining all these relations, we get\n(\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) \u2212 k\u03bdk1 )\nD\u03c3\u2217 (\u03c4\u0302 ) + 2C\u03c3\u2217 (\u03c4\u0302 )\n=\n+ H\u03c3\u2217 (\u03c4\u0304 \u2217 ) + R\u03c3\u2217 (\u03c4\u0302 )\nk\u03bdk2\n2(\u03c3 2 + \u03c3 #2 )k\u03bdk2\n7\n\n\u22124s\n\n= H\u03c3\u2217 (\u03c4\u0304 \u2217 ) + {N\u03c32\u2217\n\n9\n\n\u22122s\n\n+ \u03c3\u22172 N\u03c32\u2217\n\n3\n\n\u22122s\n\n+ N\u03c32\u2217\n\n5\n\n+ \u03c3\u22172 N\u03c32\u2217 }OP (log N\u03c3\u2217 ).\n+ \u03c3\u2217 N\u03c32\u2212s\n\u2217\n\n\u22122s+9/2\n\nlog N\u03c3\u2217 \u2192 0, as \u03c3\u2217 \u2192 0, we infer the relation\nUnder the assumptions s > 7/8, N\u03c3\u2217 \u2192 \u221e and \u03c3\u22172 N\u03c3\u2217\n(\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) \u2212 k\u03bdk1 )/k\u03bdk2 = H\u03c3\u2217 (\u03c4\u0304 \u2217 ) + OP (1). The application of the Slutsky lemma completes the proof.\nAppendix A.3. Power of the test\nThe aim of this section is to present a proof of Theorem 2. To this end, we study the test statistic\nT\u03c3\u2217 = (\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) \u2212 k\u03bdk1 )/k\u03bdk2 , and show that it tends to +\u221e in probability under H1 . Actually, the\nhypothesis H1 will be supposed to be satisfied throughout this section. It holds true that:\n\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) =\n\nX\n1\nmin\n\u03bdj Yj \u2212 e\u2212ij\u03c4 Yj#\n2\n\u03c3\u2217 \u03c4 \u2208[0,2\u03c0]\n\n2\n\nj\u22651\n\nX\n\u00012\n1\n\u03bdj cj \u2212 e\u2212ij\u03c4 c#j ) + (\u03c3\u01ebj \u2212 e\u2212ij\u03c4 \u03c3 # \u01eb#j\n= 2 min\n\u03c3\u2217 \u03c4 \u2208[0,2\u03c0]\nj\u22651\nnX\nnX\no\no\n1\n2\n\u2265 2 min\nmax\n\u03bdj |cj \u2212 e\u2212ij\u03c4 c#j | * (|\u01ebj | + |\u01eb#j |) .\n\u03bdj |cj \u2212 e\u2212ij\u03c4 c#j |2 \u2212\n\u03c3\u2217 \u03c4 \u2208[0,2\u03c0]\n\u03c3\u2217 \u03c4 \u2208[0,2\u03c0]\nj\u22651\n\nj\u22651\n\nLet us focus on the first term. Denoting \u03b4\u03c3\u2217 = min{j \u2265 1, \u03bdj < c}, we get by condition (C) that \u03b4\u03c3\u2217 \u2192 +\u221e,\nwhich implies\nmin\n\n\u03c4 \u2208[0,2\u03c0]\n\nX\nj\u22651\n\n\u03bdj |cj \u2212 e\u2212ij\u03c4 c#j |2 \u2265 c min\n\n\u03c4 \u2208[0,2\u03c0]\n\n\u2265c\n\n\u0010\n\nmin\n\n\u03b4\u03c3\u2217\nX\nj=1\n\n\u03c4 \u2208[0,2\u03c0]\n\n\u2265c \u03c1\u2212\n23\n\n|cj \u2212 e\u2212ij\u03c4 c#j |2\n\n+\u221e\nX\nj=1\n\n4L2 \u03b4\u03c3\u22122\n\u2217\n\n|cj \u2212 e\u2212ij\u03c4 c#j |2 \u2212 4L2 \u03b4\u03c3\u22122\n\u2217\n\u0001\n.\n\n\u0011\n\n\fFor the second term, we use that for every \u03c4 it holds that\nN \u03c3\u2217\n\nX\nj\u22651\n\n\u03bdj c j \u2212\n\ne\u2212ij\u03c4 c#j\n\n* \u01ebj \u2212\n\ne\u2212ij\u03c4 \u01eb#j\n\n\u2264\n\nmax\n\nj=1,...,N\u03c3\u2217\n\n(|\u01ebj | \u2228\n\n|\u01eb#j |)\n\nN \u03c3\u2217\n\nX\u0010\nj=1\n\n|cj | + |c#j |\n\n\u0011\n\n\u0011\nX\u0010\np\n= OP ( log N\u03c3\u2217 )\n|cj | + |c#j | .\nj=1\n\nFurthermore, using the Cauchy-Schwarz inequality, we get\n\u00131/2 \u0012 N\n\u03c3\u2217\n\u03c3\u2217\n\u0010\n\u00112 \u00131/2\n\u0011 \u0012N\nX\nX\nX\u0010\n#\n2\n\u22122\n#\n\u2264 3L.\nj |cj | + |cj |\nj\n|cj | + |cj | \u2264\n\nN \u03c3\u2217\n\nj=1\n\nj=1\n\nj=1\n\nTherefore,\nmax\n\n\u03c4 \u2208[0,2\u03c0]\n\nX\nj\u22651\n\np\n\u03bdj cj \u2212 e\u2212ij\u03c4 c#j * \u01ebj \u2212 e\u2212ij\u03c4 \u01eb#j = OP ( log N\u03c3\u2217 ).\n\nCombining all these estimates, we get\nT\u03c3\u2217\n\n\u0011\n\u0010 p\n\u2212 OP \u03c3\u2217 log N\u03c3\u2217 \u2212 \u03c3\u22172 N\u03c3\u2217 P\nc\u03c1 \u2212 4Lc\u03b4\u03c3\u22122\n\u2217\n\u2206\u03bd,\u03c3\u2217 (Y \u2022,# ) \u2212 k\u03bdk1\np\n\u2212\n\u2192 +\u221e\n\u2265\n=\nk\u03bdk2\n\u03c3\u22172 N\u03c3\u2217\n\nin view of the convergences \u03b4\u03c3\u2217 \u2192 \u221e, N\u03c3\u2217 \u2192 \u221e and \u03c3\u22172 N\u03c3\u2217 \u2192 0 as \u03c3\u2217 \u2192 0.\nAppendix B. Proof of Theorem 3\n\nThroughout this proof, we assume that H0 is fulfilled, that is for some \u03c4\u0304 \u2217 \u2208 [0, 2\u03c0[, the relation cj =\n\u2217\ne\u2212ij \u03c4\u0304 c#j holds for every j \u2208 N. We also recall that \u03c3 and \u03c3 # are assumed to be equal. One easily checks that\np\n\nkY \u2212 e(\u03c4 ) \u25e6 Y # k22,\u03bd\nkY \u2212 e(\u03c4\u0304 \u2217 ) \u25e6 Y # k22,\u03bd\n1X\n\u03bdj (|\u03bej |2 \u2212 2).\n\u2264\n= k\u03bdk1 +\n2\n#2\n2\n#2\n\u03c4 \u2208[0,2\u03c0[\n2(\u03c3 + \u03c3 )\n2(\u03c3 + \u03c3 )\n2 j=1\nmin\n\n\u221a\n\u2217\nwhere \u03bej = (\u01ebj \u2212 e\u2212ij \u03c4\u0304 \u01eb#j )/ 2 for every j \u2208 N. Therefore, using the notation \u00db\u03c3\u2217 =\nP\n\u03bdj\nZ\u03c3\u2217 = pj=1 2k\u03bdk\n(|\u03bej |2 \u2212 2), we get\n2\nkY \u2212 e(\u03c4 ) \u25e6 Y # k22,\u03bd\n\u00db\u03c3\u2217\nk\u03bdk1\nmin\n\u2212\nk\u03bdk2 \u03c4 \u2208[0,2\u03c0[\n4\u03c3\u22172\nk\u03bdk2\n\u0013\n\u0012\np\nk\u03bdk1\nk\u03bdk1 X \u03bdj\n2\n+\n(|\u03bej | \u2212 2) \u2212\n\u2264 \u00db\u03c3\u2217\nk\u03bdk2 j=1 2k\u03bdk2\nk\u03bdk2\n\u0012\n\u0013\n\u0001 k\u03bdk1\n= Z\u03c3\u2217 + \u00db\u03c3\u2217 \u2212 1\n+ Z\u03c3\u2217 .\nk\u03bdk2\n\n(B.1)\n\n4\u03c3\u22172 k1\u2212\u03bdk1\nkY k22,1\u2212\u03bd +kY # k22,1\u2212\u03bd\n\nand\n\nT (Y \u2022,# ) =\n\nSince the random variables Z\u03c3\u2217 tend in distribution to N (0, 1) as \u03c3\u2217 \u2192 0, and\n(the last inequality follows from condition (B)), we arrive at\nT (Y \u2022,# ) \u2264 Z\u03c3\u2217 + \u00db\u03c3\u2217 \u2212 1\n\n\u0001\n\u0001 k\u03bdk1\n) .\n1 + OP (N\u03c3\u22121/2\n\u2217\nk\u03bdk2\n\n24\n\n(B.2)\nk\u03bdk2\nk\u03bdk1\n\n\u2264\n\nk\u03bdk2\nk\u03bdk22\n\n\u2264 (cN\u03c3\u2217 )\u22121/2\n(B.3)\n\n\fTo complete the proof, we study the behavior of \u00db\u03c3\u2217 as \u03c3\u2217 \u2192 0. To this end, we define\n(2k1 \u2212 \u03bdk1 \u2212 k\u01ebk22,1\u2212\u03bd ) + (2k1 \u2212 \u03bdk1 \u2212 k\u01eb# k22,1\u2212\u03bd )\n,\n4k1 \u2212 \u03bdk2\nhc, \u01eb + e(\u03c4\u0304 \u2217 ) \u25e6 \u01eb# i1\u2212\u03bd\n\u221a\n.\n=\n2kck2,(1\u2212\u03bd)2\n\nZ\u03c3\u2032 \u2217 =\nZ\u03c3\u2032\u2032\u2217\nWe have\n\nkY k22,1\u2212\u03bd + kY # k22,1\u2212\u03bd \u2212 4\u03c3\u22172 k1 \u2212 \u03bdk1\n4\u03c3\u22172 k1 \u2212 \u03bdk1\nkck22,1\u2212\u03bd\nkck2,(1\u2212\u03bd)2 Z\u03c3\u2032\u2032\u2217\nk1 \u2212 \u03bdk2 \u2032\n=1\u2212\n.\n(B.4)\nZ\u03c3\u2217 + 2\n+ \u221a\nk1 \u2212 \u03bdk1\n2\u03c3\u2217 k1 \u2212 \u03bdk1\n2\u03c3\u2217 k1 \u2212 \u03bdk1\nP\n2\nTo evaluate the right-hand side, we first upper-bound kck22,1\u2212\u03bd =\nj (1 \u2212 \u03bdj )|cj | by the expression\n\u0002\n\u0003\nP\n2\n2\n\u2032\n\u22122\n\u22122\nj |cj | \u2264 c LN\u03c3\u2217 . Taking into account the facts that \u03c3\u2217 N\u03c3\u2217 = O(1), Z\u03c3\u2032\u2032\u2217 \u223c N (0, 2)\nmaxj\u22651 j (1\u2212\u03bdj )\nP j\n2\nand kck2,(1\u2212\u03bd)2 = j (1 \u2212 \u03bdj )2 |cj |2 \u2264 kck22,1\u2212\u03bd \u2264 c\u2032 LN\u03c3\u22122\n, we get\n\u2217\n=1+\n\u00db\u03c3\u22121\n\u2217\n\nk1 \u2212 \u03bdk2 \u2032\nO((\u03c3\u2217 N\u03c3\u2217 )\u22122 ) + OP ((\u03c3\u2217 N\u03c3\u2217 )\u22121 )\nZ\u03c3\u2217 +\nk1 \u2212 \u03bdk1\nk1 \u2212 \u03bdk1\nk1 \u2212 \u03bdk2 \u2032\nOP (1)\n=1\u2212\n.\nZ +\nk1 \u2212 \u03bdk1 \u03c3\u2217 (\u03c3\u2217 N\u03c3\u2217 )2 k1 \u2212 \u03bdk1\n\n=1\u2212\n\u00db\u03c3\u22121\n\u2217\n\nFinally, using the inequality k1 \u2212 \u03bdk1 \u2265 p \u2212 N\u03c3\u2217 , we get that\n=1\u2212\n\u00db\u03c3\u22121\n\u2217\n\nOP (1)\nk1 \u2212 \u03bdk2 \u2032\n.\nZ\u03c3\u2217 +\nk1 \u2212 \u03bdk1\n(p \u2212 N\u03c3\u2217 )(\u03c3\u2217 N\u03c3\u2217 )2\n\n(B.5)\n1/2\n\nOn the other hand, since \u03bdj \u2208 [0, 1] for every j and \u03bdj = 0 for j \u2265 N\u03c3\u2217 , we have k1 \u2212 \u03bdk2 \u2264 k1 \u2212 \u03bdk1 =\nk1\u2212\u03bdk1\n1/2\n. In particular, relation (B.5) in conjunction with the condition (p \u2212\n1/2 \u2264 k1 \u2212 \u03bdk1 /(p \u2212 N\u03c3\u2217 )\nk1\u2212\u03bdk1\n\n3/2\n\n\u2212 1| \u2264\nN\u03c3\u2217 )\u03c3\u22172 N\u03c3\u2217 \u2192 +\u221e implies that |\u00db\u03c3\u22121\n\u2217\n\nand, therefore, \u00db\u03c3\u2217 = 1 +\n\n\u22121/2\nOP (N\u03c3\u2217 ).\n\n= Z\u03c3\u2217\n= Z\u03c3\u2217\n\n\u22121/2\n\n|Z\u03c3\u2032 \u2217 | + oP (N\u03c3\u2217\n\n\u22121/2\n\n) = (p \u2212 N\u03c3\u2217 )\u22121/2 OP (1) + oP (N\u03c3\u2217\n\n)\n\nCombining this with (B.5) and (B.3), we arrive at\n\nT (Y \u2022,# ) \u2264 Z\u03c3\u2217 + \u00db\u03c3\u2217 1 \u2212 \u00db\u03c3\u22121\n\u2217\n= Z\u03c3\u2217\n\nk1\u2212\u03bdk2\nk1\u2212\u03bdk1\n\n\u0001 k\u03bdk1\n\n\u0001\n)\n1 + OP (N\u03c3\u22121/2\n\u2217\n\nk\u03bdk2\n\u0001\n\u0001\nk\u03bdk1\n)\n1 + OP (N\u03c3\u22121/2\n+ 1 \u2212 \u00db\u03c3\u22121\n\u2217\n\u2217\nk\u03bdk2\n\u0013\n\u0012\n\u0001\nk\u03bdk1\nOP (1)\nk1 \u2212 \u03bdk2 \u2032\n+\nZ\u03c3\u2217 +\n)\n1 + OP (N\u03c3\u22121/2\n\u2217\n2\nk1 \u2212 \u03bdk1\n(p \u2212 N\u03c3\u2217 )(\u03c3\u2217 N\u03c3\u2217 ) k\u03bdk2\n\u0013\n\u0012\nOP (1)\nOP (1)\nk\u03bdk1\nk1 \u2212 \u03bdk2 \u2032\n+\nZ\u03c3\u2217 +\n+\n1/2 k\u03bdk\n1/2\nk1 \u2212 \u03bdk1\n(p \u2212 N\u03c3\u2217 )(\u03c3\u2217 N\u03c3\u2217 )2\n2\n(p \u2212 N\u03c3\u2217 ) N\u03c3\u2217\n\n= Z\u03c3\u2217 +\n\nk\u03bdk1 k1 \u2212 \u03bdk2 \u2032\nOP (1)\nOP (1)\nZ +\n+\n.\nk\u03bdk2 k1 \u2212 \u03bdk1 \u03c3\u2217 (p \u2212 N\u03c3\u2217 )\u03c3\u22172 N\u03c33/2\n(p\n\u2212\nN\u03c3\u2217 )1/2\n\u2217\n\u2217\n\n\u2217\n\nThis result, combined with the obvious identity |\u01ebj |2 + |\u01eb#j |2 = 12 |\u01ebj + e\u2212ij \u03c4\u0304 \u01eb#j |2 + 21 |\u01ebj \u2212 e\u2212ij \u03c4\u0304 \u01eb#j |2 = |\u03bej |2 +\n|\u03bej# |2 , completes the proof of the theorem.\nAppendix C. Bounds for the maxima of random sums\nIn this section, we will gather some useful technical lemmas. They essentially characterize the stochastic\nbehavior of the maximum of the sum of independent random quantities, which are either \"simple\" Gaussian\n25\n\n\fprocesses [3] or scaled chi-squared random variables. Note that instead of Berman's formula, one could\nuse the combination of the Dudley theorem and Talagrand's inequality to control the supremum of the\nconsidered random process. Both approaches lead to optimal results. However, the Berman formula is much\neasier to apply since it avoids the computation of the covering numbers or other quantities of the same\nflavor. It should be however emphasized that this is made possible by the fact that the noise is assumed\nGaussian and the sample paths of the process to be maximized are continuously differentiable. In a more\ngeneral situation (non Gaussian noise, nonsmooth sample paths, etc.) one would most likely have no other\nchoice than applying the Dudley-Talagrand routine.\nP\nProposition 2 ([3]). Suppose that gj are continuously differentiable functions satisfying nj=1 gj (t)2 = 1\niid\n\nfor all t, and \u03bej \u223c N (0, 1). Then, for every x > 0, we have\n\n\u0013\n\u0012\nZ +\u221e \u2212 t2\nn\nX\nL0 \u2212 x22\ne 2\n\u221a dt,\ngj (t)\u03bej \u2265 x \u2264\nP sup\n+\ne\n2\u03c0\n2\u03c0\n[a,b] j=1\nx\n\nwith\n\nL0 =\n\nZ\n\na\n\nb\n\n\u0014X\nn\nj=1\n\ngj\u2032 (t)2\n\n\u00151/2\n\ndt.\n\nWe will also use the following fact about moderate deviations of the random variables that can be written\nas the sum of squares of independent centered Gaussian random variables.\nLemma 4. Let N be some positive integer and let \u03b7j# , j = 1, . . . , N be independent complex valued random\nvariables such that their real and imaginary parts are independent standard Gaussian variables. Let s =\n(s1 , . . . , sN ) be a vector of real numbers. For any y \u2265 0, it holds that\nN\no\nnX\n\u221a\n2\ns2j |\u03b7j# |2 \u2265 2ksk22 + 2 2ksk24 y + 2ksk2\u221e y 2 \u2264 e\u2212y /2 ,\nP\nj=1\n\nwith the standard notations ksk\u221e = max |sj | and kskqq =\nj=1,...,N\n\nProof. This is a direct consequence of [32, Lemma 1].\n\nPN\n\nj=1\n\n|sj |q .\n\nProof of Lemma 2. We apply Proposition 2 to the functions {gj (t)}j=1,...,2N defined on the interval [a, b] =\n[0, 2\u03c0] by gj (t) = [sj cos(jt)1(j \u2264 N )+ sj sin(jt)1(j > N )]/ksk2 . One easily checks that for every t \u2208 [0, 2\u03c0],\nP\n2\nwe have 2N\nj=1 gj (t) = 1. Therefore, applying Berman's result we get\n\u0010\n\u0011\n\u0011 \u0010L\nX\nX\n\u0001\n2\n0\nP kZk\u221e \u2265 ksk2 x = P sup\n+ 1 e\u2212x /2 .\ngj (t)\u03bej +\ngj (t)\u03bej\u2032 \u2265 x \u2264\n2\u03c0\nt\u2208[0,2\u03c0]\nj\u2264N\n\nj>N\n\nIn the present context, the constant L0 admits the following simple upper bound:\nL0 =\n\nZ\n\n0\n\n2\u03c0\n\nN\nhX\n\nj 2 s2j /ksk22\n\nj=1\n\ni1/2\n\ndt \u2264 2\u03c0N,\n\nwhich yields the desired result.\nProof of Lemma 3. First note that we can not directly apply Berman's formula, since the summands are not\nGaussian. However, they are conditionally Gaussian if the conditioning is done, for example, with respect to\nthe sequence {\u03b7j# }j=1,...,N . Indeed, one easily checks that conditionally to {\u03b7j# }j=1,...,N , the random processes\n\u03c4 7\u2192\n\nN\nX\n\nsj Re eij\u03c4 \u03b7j \u03b7j#\n\nj=1\n\n26\n\n\u0001\n\n\fand\n\u03c4 7\u2192\n\nN\nX\nj=1\n\nsj |\u03b7j# | cos(j\u03c4 )\u03bej \u2212 sin(j\u03c4 )\u03bej\u2032\n\n\u0001\n\niid\n\nwith \u03bej , \u03bej\u2032 \u223c N (0, 1)\n\nhave the same distributions. Therefore, it follows from Lemma 2 that\n\u0012\n\u0013\nN\nN\n\u0011 12\n\u0010X\nX\n\u0001\n2\ns2j |\u03b7j# |2\nsj Re eij\u03c4 \u03b7j \u03b7j# \u2265 x\nP sup\n{\u03b7j# }j=1,...,N \u2264 (N + 1)e\u2212x /2 .\n[0,2\u03c0] j=1\n\nj=1\n\nLet us now denote by \u03b6 the square root of the random variable\n\nPN\n\n2 # 2\nj=1 sj |\u03b7j | .\n\nIt is clear that for all a > 0,\n\n\u0001\n\u0001\n\u0001\nP kZk\u221e \u2265 ax = P kZk\u221e \u2265 ax; \u03b6 \u2264 a + P kZk\u221e \u2265 ax; \u03b6 > a\n\u0001\n\u0001\n\u2264 P kZk\u221e \u2265 x\u03b6 + P \u03b6 > a\n\u0001\n2\n\u2264 (N + 1)e\u2212x /2 + P \u03b6 > a .\n\n\u221a\nTo complete the proof, it suffices to replace a by 2(ksk2 + yksk\u221e ) and to\u221aapply Lemma 4 along with the\ninequalities ksk2 + ksk\u221e y = (ksk22 + 2ksk\u221e ksk2 y + ksk2\u221e y 2 )1/2 \u2265 (ksk22 + 2ksk24 y + ksk2\u221e y 2 )1/2 .\nAcknowledgments\nThis work has been partially supported by the ANR grant Callisto and the grant Investissements d'Avenir\n(ANR-11-IDEX-0003/Labex Ecodec/ANR-11-LABX-0047).\nReferences\nReferences\n[1] Baraud, Y., Huet, S., Laurent, B., 2003. Adaptive tests of linear hypotheses by model selection. Ann. Statist. 31 (1),\n225\u2013251.\n[2] Baraud, Y., Huet, S., Laurent, B., 2005. Testing convex hypotheses on the mean of a Gaussian vector. Application to\ntesting qualitative hypotheses on a regression function. Ann. Statist. 33 (1), 214\u2013257.\n[3] Berman, S. M., 1988. Sojourns and extremes of a stochastic process defined as a random linear combination of arbitrary\nfunctions. Comm. Statist. Stochastic Models 4 (1), 1\u201343.\n[4] Bigot, J., Gadat, S., 2010. A deconvolution approach to estimation of a common shape in a shifted curves model. Ann.\nStatist. 38 (4), 2422\u20132464.\n[5] Bigot, J., Gadat, S., Loubes, J.-M., 2009. Statistical M-estimation and consistency in large deformable models for image\nwarping. J. Math. Imaging Vision 34 (3), 270\u2013290.\n[6] Bigot, J., Gamboa, F., Vimond, M., 2009. Estimation of translation, rotation, and scaling between noisy images using the\nFourier-Mellin transform. SIAM J. Imaging Sci. 2 (2), 614\u2013645.\n[7] Brown, L. D., Low, M. G., 1996. Asymptotic equivalence of nonparametric regression and white noise. Ann. Statist. 24 (6),\n2384\u20132398.\n[8] Carroll, R. J., Hall, P., 1992. Semiparametric comparison of regression curves via normal likelihoods. Austral. J. Statist.\n34 (3), 471\u2013487.\n[9] Castillo, I., 2007. Semi-parametric second-order efficient estimation of the period of a signal. Bernoulli 13 (4), 910\u2013932.\n[10] Castillo, I., 2012. A semiparametric Bernstein-von Mises theorem for Gaussian process priors. Probab. Theory Related\nFields 152 (1-2), 53\u201399.\n[11] Castillo, I., Loubes, J.-M., 2009. Estimation of the distribution of random shifts deformation. Math. Methods Statist.\n18 (1), 21\u201342.\n[12] Collier, O., 2012. Minimax hypothesis testing for curve registration. Electron. J. Stat. 6, 1129\u20131154.\n[13] Comminges, L., Dalalyan, A. S., 2013. Minimax testing of a composite null hypothesis defined via a quadratic functional\nin the model of regression. Electron. J. Stat. 7, 146\u2013190.\n[14] Dalalyan, A. S., 2007. Stein shrinkage and second-order efficiency for semiparametric estimation of the shift. Math. Methods\nStatist. 16 (1), 42\u201362.\n[15] Dalalyan, A. S., Collier, O., 2012. Wilks' phenomenon and penalized likelihood-ratio test for nonparametric curve registration. In: Journal of Machine Learning Research - Proceedings Track 22. pp. 264\u2013272.\n\n27\n\n\f[16] Dalalyan, A. S., Golubev, G. K., Tsybakov, A. B., 2006. Penalized maximum likelihood and semiparametric second-order\nefficiency. Ann. Statist. 34 (1), 169\u2013201.\n[17] Dalalyan, A. S., Reiss, M., 2007. Asymptotic statistical equivalence for ergodic diffusions: the multidimensional case.\nProbab. Theory Related Fields 137 (1-2), 25\u201347.\n[18] Fan, J., Jiang, J., 2007. Nonparametric inference with generalized likelihood ratio tests. TEST 16 (3), 409\u2013444.\n[19] Fan, J., Zhang, C., Zhang, J., 2001. Generalized likelihood ratio statistics and Wilks phenomenon. Ann. Statist. 29 (1),\n153\u2013193.\n[20] Gamboa, F., Loubes, J.-M., Maza, E., 2007. Semi-parametric estimation of shifts. Electron. J. Stat. 1, 616\u2013640.\n[21] Gayraud, G., Pouet, C., 2005. Adaptive minimax testing in the discrete regression scheme. Probab. Theory Related Fields\n133 (4), 531\u2013558.\n[22] Glasbey, C. A., Mardia, K. V., 2001. A penalized likelihood approach to image warping. J. R. Stat. Soc. Ser. B Stat.\nMethodol. 63 (3), 465\u2013514.\n[23] Golubev, G. K., 1988. Estimation of the period of a signal with an unknown form against a white noise background.\nProblems Inform. Transmission 24 (4), 288\u2013299.\n[24] Grama, I., Nussbaum, M., 1998. Asymptotic equivalence for nonparametric generalized linear models. Probab. Theory\nRelated Fields 111 (2), 167\u2013214.\n[25] H\u00e4rdle, W., Marron, J. S., 1990. Semiparametric comparison of regression curves. Ann. Statist. 18 (1), 63\u201389.\n[26] Hartley, R., Zisserman, A., 2003. Multiple view geometry in computer vision, 2nd Edition. Cambridge University Press,\nCambridge.\n[27] Horowitz, J. L., Spokoiny, V. G., 2001. An adaptive, rate-optimal test of a parametric mean-regression model against a\nnonparametric alternative. Econometrica 69 (3), 599\u2013631.\n[28] Immerkaer, J., 1996. Fast noise variance estimation. Computer Vision and Image Understanding 64 (2), 300 \u2013 302.\n[29] Ingster, Y. I., Kutoyants, Y. A., 2007. Nonparametric hypothesis testing for intensity of the Poisson process. Math.\nMethods Statist. 16 (3), 217\u2013245.\n[30] Ingster, Y. I., Suslina, I. A., 2003. Nonparametric goodness-of-fit testing under Gaussian models. Vol. 169 of Lecture Notes\nin Statistics. Springer-Verlag, New York.\n[31] Kneip, A., Gasser, T., 1992. Statistical tools to analyze data representing a sample of curves. Ann. Statist. 20 (3), 1266\u2013\n1305.\n[32] Laurent, B., Massart., P., 2000. Adaptive estimation of a quadratic functional by model selection. Ann. Statist. 28 (5),\n1302\u20131338.\n[33] Loubes, J.-M., Maza, E., Lavielle, M., Rodr\u0131\u0301guez, L., 2006. Road trafficking description and short term travel time\nforecasting, with a classification method. Canad. J. Statist. 34 (3), 475\u2013491.\n[34] Lowe, D. G., 2004. Distinctive image features from scale-invariant keypoints. International journal of computer vision\n60 (2), 91\u2013110.\n[35] Munk, A., Dette, H., 1998. Nonparametric comparison of several regression functions: exact and asymptotic theory. Ann.\nStatist. 26 (6), 2339\u20132368.\n[36] Neumeyer, N., Dette, H., 2003. Nonparametric comparison of regression curves: an empirical process approach. Ann.\nStatist. 31 (3), 880\u2013920.\n[37] Nussbaum, M., 1996. Asymptotic equivalence of density estimation and Gaussian white noise. Ann. Statist. 24 (6), 2399\u2013\n2430.\n[38] Pardo-Fern\u00e1ndez, J. C., Van Keilegom, I., Gonz\u00e1lez-Manteiga, W., 2007. Testing for the equality of k regression curves.\nStatist. Sinica 17 (3), 1115\u20131137.\n[39] Petrov, V. V., 1995. Limit theorems of probability theory. Vol. 4 of Oxford Studies in Probability. The Clarendon Press\nOxford University Press, New York.\n[40] Reiss, M., 2008. Asymptotic equivalence for nonparametric regression with multivariate and random design. Ann. Statist.\n36 (4), 1957\u20131982.\n[41] R\u00f8nn, B. B., 2001. Nonparametric maximum likelihood estimation for shifted curves. J. R. Stat. Soc. Ser. B Stat. Methodol.\n63 (2), 243\u2013259.\n[42] Srihera, R., Stute, W., 2010. Nonparametric comparison of regression functions. J. Multivariate Anal. 101 (9), 2039\u20132059.\n[43] Trigano, T., Isserles, U., Ritov, Y., 2011. Semiparametric curve alignment and shift density estimation for biological data.\nIEEE Transactions on Signal Processing 59 (5), 1970\u20131984.\n[44] Vimond, M., 2010. Efficient estimation for a subclass of shape invariant models. Ann. Statist. 38 (3), 1885\u20131912.\n\n28\n\n\f\f"}