{"id": "http://arxiv.org/abs/1108.1042v1", "guidislink": true, "updated": "2011-08-04T10:43:17Z", "updated_parsed": [2011, 8, 4, 10, 43, 17, 3, 216, 0], "published": "2011-08-04T10:43:17Z", "published_parsed": [2011, 8, 4, 10, 43, 17, 3, 216, 0], "title": "On strong homogeneity of two global optimization algorithms based on\n  statistical models of multimodal objective functions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.2047%2C1108.3738%2C1108.2439%2C1108.1371%2C1108.0131%2C1108.2733%2C1108.3419%2C1108.4993%2C1108.4263%2C1108.4286%2C1108.0100%2C1108.1361%2C1108.0669%2C1108.4465%2C1108.0629%2C1108.3673%2C1108.2724%2C1108.2154%2C1108.6319%2C1108.5601%2C1108.4323%2C1108.4148%2C1108.3781%2C1108.6324%2C1108.2869%2C1108.3240%2C1108.5475%2C1108.2832%2C1108.0899%2C1108.4873%2C1108.4354%2C1108.6026%2C1108.3639%2C1108.5406%2C1108.2953%2C1108.3990%2C1108.2672%2C1108.0231%2C1108.0573%2C1108.2265%2C1108.3940%2C1108.4480%2C1108.2588%2C1108.1339%2C1108.0919%2C1108.3701%2C1108.2939%2C1108.2405%2C1108.3668%2C1108.3500%2C1108.5950%2C1108.5966%2C1108.2288%2C1108.3645%2C1108.1551%2C1108.0759%2C1108.2458%2C1108.1770%2C1108.3719%2C1108.1885%2C1108.0074%2C1108.2201%2C1108.5439%2C1108.0245%2C1108.0292%2C1108.0615%2C1108.1042%2C1108.2351%2C1108.4487%2C1108.4622%2C1108.4905%2C1108.1369%2C1108.1983%2C1108.5958%2C1108.2368%2C1108.5445%2C1108.3061%2C1108.1237%2C1108.4551%2C1108.3893%2C1108.0122%2C1108.3094%2C1108.2096%2C1108.5379%2C1108.5211%2C1108.3501%2C1108.1554%2C1108.0380%2C1108.1684%2C1108.3882%2C1108.3766%2C1108.3389%2C1108.3151%2C1108.1242%2C1108.1015%2C1108.2250%2C1108.2426%2C1108.0111%2C1108.4923%2C1108.1228%2C1108.1708&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On strong homogeneity of two global optimization algorithms based on\n  statistical models of multimodal objective functions"}, "summary": "The implementation of global optimization algorithms, using the arithmetic of\ninfinity, is considered. A relatively simple version of implementation is\nproposed for the algorithms that possess the introduced property of strong\nhomogeneity. It is shown that the P-algorithm and the one-step Bayesian\nalgorithm are strongly homogeneous.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.2047%2C1108.3738%2C1108.2439%2C1108.1371%2C1108.0131%2C1108.2733%2C1108.3419%2C1108.4993%2C1108.4263%2C1108.4286%2C1108.0100%2C1108.1361%2C1108.0669%2C1108.4465%2C1108.0629%2C1108.3673%2C1108.2724%2C1108.2154%2C1108.6319%2C1108.5601%2C1108.4323%2C1108.4148%2C1108.3781%2C1108.6324%2C1108.2869%2C1108.3240%2C1108.5475%2C1108.2832%2C1108.0899%2C1108.4873%2C1108.4354%2C1108.6026%2C1108.3639%2C1108.5406%2C1108.2953%2C1108.3990%2C1108.2672%2C1108.0231%2C1108.0573%2C1108.2265%2C1108.3940%2C1108.4480%2C1108.2588%2C1108.1339%2C1108.0919%2C1108.3701%2C1108.2939%2C1108.2405%2C1108.3668%2C1108.3500%2C1108.5950%2C1108.5966%2C1108.2288%2C1108.3645%2C1108.1551%2C1108.0759%2C1108.2458%2C1108.1770%2C1108.3719%2C1108.1885%2C1108.0074%2C1108.2201%2C1108.5439%2C1108.0245%2C1108.0292%2C1108.0615%2C1108.1042%2C1108.2351%2C1108.4487%2C1108.4622%2C1108.4905%2C1108.1369%2C1108.1983%2C1108.5958%2C1108.2368%2C1108.5445%2C1108.3061%2C1108.1237%2C1108.4551%2C1108.3893%2C1108.0122%2C1108.3094%2C1108.2096%2C1108.5379%2C1108.5211%2C1108.3501%2C1108.1554%2C1108.0380%2C1108.1684%2C1108.3882%2C1108.3766%2C1108.3389%2C1108.3151%2C1108.1242%2C1108.1015%2C1108.2250%2C1108.2426%2C1108.0111%2C1108.4923%2C1108.1228%2C1108.1708&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The implementation of global optimization algorithms, using the arithmetic of\ninfinity, is considered. A relatively simple version of implementation is\nproposed for the algorithms that possess the introduced property of strong\nhomogeneity. It is shown that the P-algorithm and the one-step Bayesian\nalgorithm are strongly homogeneous."}, "authors": ["Antanas Zilinskas"], "author_detail": {"name": "Antanas Zilinskas"}, "author": "Antanas Zilinskas", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.amc.2011.07.051", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1108.1042v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1108.1042v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "11 pages, 1 figure", "arxiv_primary_category": {"term": "cs.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "65C20", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "F.2.1", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1108.1042v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1108.1042v1", "journal_reference": null, "doi": "10.1016/j.amc.2011.07.051", "fulltext": "arXiv:1108.1042v1 [cs.NA] 4 Aug 2011\n\nOn strong homogeneity of two global optimization\nalgorithms based on statistical models of multimodal\nobjective functions\nAntanas \u017dilinskas\nVilnius University\nInstitute of Mathematics and Informatics,\nVilnius, Lithuania\nantanas.zilinskas@mii.vu.lt\n\nAbstract\nThe implementation of global optimization algorithms, using the arithmetic\nof infinity, is considered. A relatively simple version of implementation is\nproposed for the algorithms that possess the introduced property of strong\nhomogeneity. It is shown that the P-algorithm and the one-step Bayesian\nalgorithm are strongly homogeneous.\nKeywords: Arithmetic of infinity, Global optimization, Statistical models\n1. Introduction\nGlobal optimization problems are considered where the computation of\nobjective function values, using the standard computer arithmetic, is problematic because of either underflows or overflows. A perspective means for\nsolving such problems is the arithmetic of infinity [6, 7, 8]. Besides fundamentally new problems of minimization of functions whose computation\ninvolves infinite or infinitesimal values, the arithmetic of infinity can be also\nvery helpful for the cases where the computation of objective function values\nis challenging because of the involvement of numbers differing in many orders\nof magnitude. For example, in some problems of statistical inference [15, 16],\nthe values of operands, involved in the computation of objective functions,\ndiffer by more than a factor of 10200 .\nThe arithmetic of infinity can be applied to the optimization of challenging objective functions in two ways. First, the optimization algorithm\nPreprint submitted to Applied Mathematics and Computation\n\nNovember 7, 2018\n\n\fcan be implemented in the arithmetic of infinity. Second, the arithmetic of\ninfinity can be applied to scale the objective function values to be suitable\nfor processing by a conventionally implemented optimization algorithm. The\nsecond case is simpler to apply, since the arithmetic of infinity should be\napplied only to the scaling of function values. If both implementation versions of the algorithm perform identically with respect to the generation of\nsequences of points where the objective function values are computed, the\nalgorithm is called strongly homogeneous. In the present paper, we show\nthat both implementation versions - of the P-algorithm and of the one-step\nBayesian algorithm - are strongly homogeneous.\nTo be more precise, let us consider two objective functions f (x) and h(x),\nx \u2208 A \u2286 Rd differing only in scales of function values, i.e. h(x) = af (x) + b\nwhere a and b are constants that can assume not only finite but also infinite\nand infinitesimal values expressed by numerals introduced in [6, 7]. In its\nturn, f (x) is defined by using the traditional finite arithmetic. The sequences\nof points generated by an algorithm, when applied to these functions, are\ndenoted by xi , i = 1, 2, . . . , and vi , i = 1, 2, . . . , respectively. The algorithm\nthat generates the identical sequences xi = vi , i = 1, 2, . . . , is called strongly\nhomogeneous. A weaker property of algorithms is considered in [1, 9], where\nthe algorithms that generate the identical sequences for the functions f (x)\nand h(x) = f (x) + b are called homogeneous. Since the proper scaling of\nfunction values by translation alone is not always possible, in the present\npaper we consider invariance of the optimization results with respect to a\nmore general (affine) transformation of the objective function values.\n2. Description of the P-algorithm\nLet us consider the minimization problem\nmin f (x), A \u2286 Rd ,\nx\u2208A\n\n(1)\n\nwhere the multimodality of the objective function f (x) is expected. Although\nthe properties of the feasible region are not essential in a further analysis, for\nthe sake of explicitness, A is assumed to be a hyper-rectangle. For the arguments justifying the construction of global optimization algorithms using\nstatistical models of objective functions, we refer to [9, 12, 11]. Global optimization algorithms based on statistical models implement the ideas of the\ntheory of rational decision making under uncertainty [10]. The P-algorithm\n2\n\n\fis constructed in [13] stating the rationality axioms in the situation of selection of a point of current computation of the value of f (x); it follows from\nthe axioms that a point should be selected where the probability to improve\nthe current best value is maximal.\nTo implement the P-algorithm, Gaussian stochastic functions are used\nmainly because of their computational advantages; however such type of\nstatistical models is justified axiomatically and by the results of a psychometric experiment [10, 11, 13]. Application for a statistical model of a nonGaussian stochastic function would imply at least serious implementation\ndifficulties. Let \u03be(x) be the Gaussian stochastic function with mean value\n\u03bc, variance \u03c3 2 , and correlation function \u03c1(*, *). The choice of the correlation\nfunction normally is based on the supposed properties of the aimed objective functions, and the properties of the corresponding stochastic function,\ne.g. frequently used correlation functions are \u03c1(xi , xj ) = exp(\u2212c||xi \u2212 xj ||),\n\u03c1(xi , xj ) = exp(\u2212c||xi \u2212xj ||2 ). The parameters \u03bc and \u03c3 2 should be estimated\nusing a sample of the objective function values.\nLet yi = f (xi ) be the function values computed during the previous n\nminimization steps. By the P-algorithm [10, 13] the next function value is\ncomputed at the point of maximum probability to overpass the aspiration\nlevel yon :\nxn+1 = arg max P{\u03be(x) \u2264 yon |\u03be(xi ) = yi , i = 1, ..., n}.\nx\u2208A\n\n(2)\n\nSince \u03be(x) is the Gaussian stochastic function, the maximization in (2) can\nbe reduced to the maximization of\nyon \u2212 mn (x|xi , yi )\n,\nsn (x|xi , yi )\n\n(3)\n\nwhere mn (x|xi , yi ) and s2n (x|xi , yi ) denote the conditional mean and conditional variance of \u03be(x) with respect to \u03be(xi ) = yi , i = 1, ..., n,. The explicit\nformulae of mn (x|xi , yi ) and s2n (x|xi , yi ) are presented below since they will\nbe needed in a further analysis\nmn (x|xi , yi ) = \u03bc + (y1 \u2212 \u03bc, . . . , yn \u2212 \u03bc)\u03a3\u22121 \u03a5T ,\ns2n (x|xi , yi ) = \u03c3 2 (1 \u2212 \u03a5\u03a3\u22121 \u03a5T ),\n\uf8eb\n\uf8f6\n\u03c1(x1 , x1 ) . . . \u03c1(x1 , xn )\n\uf8f8 . (4)\n...\n...\n...\n\u03a5 = (\u03c1(x1 , x), . . . , \u03c1(xn , x)), \u03a3 = \uf8ed\n\u03c1(xn , x1 ) . . . \u03c1(xn , xn )\n3\n\n\f3. Evaluation of the influence of scaling on the search by the Palgorithm\nTo evaluate the influence of data scaling on the whole optimization process, two objective functions are considered: f (x) and \u03c6(x) = a * f (x) + b,\nwhere a and b are constants. Let us assume that the first n function values\nwere computed for both functions at the same points (xi , i = 1, ..., n). The\nnext points of computation of the values of f (*) and \u03c6(*) are denoted by xn+1\nand vn+1 . We are interested in the strong homogeneity of the P-algorithm,\ni.e. in the equality xn+1 = vn+1 .\nThe parameters of the stochastic function, estimated using the same\nmethod but different function values, normally are different. The estimates\nof \u03bc and \u03c3 2 , obtained using the data (xi , yi = f (xi ), i = 1, ..., n) and\n(xi , zi = \u03c6(xi ), i = 1, ..., n), are denoted as \u03bc\u0304, \u03c3\u0304 2 and \u03bc\u0303, \u03c3\u0303 2 , respectively.\nIt is assumed that \u03bc\u0303 = a\u03bc\u0304 + b and \u03c3\u0303 2 = a2 \u03c3\u0304 2 ; as shown below, this natural\nassumption is satisfied for the two most frequently used estimators.\nP\nObviously, the unbiased estimates of \u03bc and of \u03c3 2 , \u03bc\u0303 = k1 k1 zi , and \u03c3\u0303 2 =\nPk\n1\n2\n1 (\u03bc\u0303\u2212zi ) , satisfy the assumptions made. Although those estimates are\nk\u22121\nwell justified only for independent observations, they sometimes (especially\nwhen only a small number (k) of observations is available) are used also\nfor rough estimation of the parameters \u03bc and of \u03c3 2 despite the correlation\nbetween the {zi }.\nThe maximum likelihood estimates also satisfy the assumptions:\n\u0012\n\u0013\n1\n(y \u2212 \u03bcI)\u03a3\u22121 (y \u2212 \u03bcI)T\n2\n(\u03bc\u0303, \u03c3\u0303 ) = arg max\nexp \u2212\n,(5)\n\u03bc,\u03c3 2 (2\u03c0)n/2 |\u03a3|1/2 \u03c3 n\n2\u03c3 2\nwhere y = (y1 , . . . , yn )T , and I is the n dimensional unit vector.\nIt is easy to show that the maximum likelihood estimates implied by (5)\nare equal to\nPn Pn\nz \u03c1(x , x )\ni=1\nP Pi=1 i i j\n\u03bc\u0303 =\n(6)\n\u03c1(xi , xj )\n\u0001\n1\n(y \u2212 \u03bc\u0303I)\u03a3\u22121 (y \u2212 \u03bc\u0303I)T .\n(7)\n\u03c3\u0303 2 =\nn\nIt follows from (6) and (7) that \u03bc\u0303 = a\u03bc\u0304 + b, and \u03c3\u0303 2 = a2 \u03c3\u0304 2 correspondingly.\nThe aspiration levels are defined depending on the scales of function values: yon = min yi \u2212 \u03b5\u03c3\u0304, zon = min zi \u2212 \u03b5\u03c3\u0303.\ni=1,...,n\n\ni=1,...,n\n\n4\n\n\fTheorem 1. The P-algorithm, based on the Gaussian model with estimated\nparameters, is strongly homogeneous.\nProof. According to the definition of vk+1 the following equalities are valid\nvn+1 = arg max\nx\u2208A\n\nzon \u2212 mn (x|xi , zi )\nsn (x|xi , zi )\n\nmin zi \u2212 \u03b5\u03c3\u0303 \u2212 (\u03bc\u0303 + (z1 \u2212 \u03bc\u0303, . . . , zn \u2212 \u03bc\u0303)\u03a3\u22121 \u03a5T )\np\n= arg max\n.\nx\u2208A\n\u03c3\u0303 (1 \u2212 \u03a5\u03a3\u22121 \u03a5T )\ni=1,...,n\n\n(8)\n\nTaking into account the relation between zi and yi and the corresponding\nrelations between the estimates of \u03bc and \u03c3, equalities (8) can be extended as\nfollows\nvn+1\n\nmin ayi \u2212 a\u03b5\u03c3\u0304 \u2212 a(y1 \u2212 \u03bc\u0304, . . . , yn \u2212 \u03bc\u0304)\u03a3\u22121 \u03a5T\np\n= arg max\nx\u2208A\na\u03c3\u0304 (1 \u2212 \u03a5\u03a3\u22121 \u03a5T )\ni=1,...,n\n\nmin yi \u2212 \u03b5\u03c3\u0304 \u2212 (y1 \u2212 \u03bc\u0304, . . . , yn \u2212 \u03bc\u0304)\u03a3\u22121 \u03a5T\np\n= arg max\nx\u2208A\n\u03c3\u0304 (1 \u2212 \u03a5\u03a3\u22121 \u03a5T )\nyon \u2212 mn (x|xi , yi )\n= arg max\n= xn+1 .\nx\u2208A\nsn (x|xi , yi )\ni=1,...,n\n\n(9)\n\nThe equality between vn+1 and xn+1 means that the sequence of points generated by the P-algorithm is invariant with respect to the scaling of the\nobjective function values. The strong homogeneity of the P-algorithm is\nproven.\nAs shown in [2, 14], the P-algorithm and the radial basis function algorithm are equivalent under very general assumptions. Therefore the statement on the strong homogeneity of the P-algorithm is also valid for the radial\nbasis function algorithm.\n4. Evaluation of the influence of scaling on the search by the onestep Bayesian algorithm\nStatistical models of objective functions are also used to construct Bayesian\nalgorithms [4, 5]. Let a Gaussian stochastic function \u03be(x) be chosen for the\n\n5\n\n\fstatistical model as in Section 2. An implementable version of the Bayesian\nalgorithm is the so called one-step Bayesian algorithm defined as follows:\nxn+1 = arg max E{max(yon \u2212 \u03be(x), 0)|\u03be(xi ) = yi , i = 1, . . . , n}. (10)\nx\u2208A\n\nTheorem 2. The one-step Bayesian algorithm, based on the Gaussian model\nwith estimated parameters, is strongly homogeneous.\nProof. The value of the objective function is computed by the one-step\nBayesian algorithm at the point of maximum average improvement (10).\nThe formula of conditional mean in (10) can be rewritten as follows\nE{max(yon \u2212 \u03be(x), 0)|\u03be(xi ) = yi , i = 1, . . . , n} =\nZ yon\n(yon \u2212 t)p(t|mn (x|xi , yi ), s2n (x|xi , yi ))dt,\n=\n\n(11)\n\n\u2212\u221e\n\nwhere p(t|\u03bc, \u03c3 2 ) denotes the Gaussian probability density with the mean\nvalue \u03bc and variance \u03c3 2 . For simplicity, we use in this formula and hereinafter the traditional symbol \u221e. Obviously, when one starts to work in\nthe framework of the infinite arithmetic [6, 7], it should be substituted by\nan appropriate infinite number that has been defined a priori by the chosen\nstatistical model.\nIntegration by parts in (11) results in the following formula\nE{max(yon \u2212 \u03be(x), 0)|\u03be(xi ) = yi , i = 1, . . . , n} =\nZ yon \u2212mn (x|xi ,yi )\nsn (x|xi ,yi )\n= sn (x|xi , yi )\n\u03a0(t)dt,\n\n(12)\n\n\u2212\u221e\n\nwhere \u03a0(t) is the Laplace integral: \u03a0(t) =\nformulae (4), (9), the equalities\n\n1\n2\u03c0\n\nRt\n\u2212\u221e\n\n2\n\nexp(\u2212 \u03c42 )d\u03c4 . From the\n\nyon \u2212 mn (x|xi , yi )\nzon \u2212 mn (x|xi , zi )\n=\n,\nsn (x|xi , yi )\nsn (x|xi , zi )\ns2n (x|xi , zi ) = as2n (x|xi , yi ),\nfollow implying the invariance of the sequence x1 , x2 , . . . , generated by the\none-step Bayesian algorithm with respect to the scaling of values of the objective function. The strong homogeneity of the one-step Bayesian algorithm\nis proven.\n6\n\n\f5. Strong homogeneity is not a universal property of global optimization algorithms\nAlthough the invariance of the whole optimization process with respect\nto affine scaling of objective function values seems very natural, not all global\noptimization algorithms are strongly homogeneous. For example, the rather\npopular algorithm DIRECT [3] is not strongly homogeneous. We are not\ngoing to investigate in detail the properties of DIRECT related to the scaling\nof objective function values. Instead an example is presented contradicting\nthe necessary conditions of strong homogeneity.\nFor the sake of simplicity let us consider the one-dimension version of\nDIRECT. Let the feasible region (interval) be partitioned into subintervals [ai , bi ], i = 1, ..., n. The objective function values computed at the\npoints ci = (ai + bi )/2 are supposed positive, f (ci ) > 0; denote fmin =\nmin{f (c1 ), . . . , f (cn )}. A j-th subinterval is said to be potentially optimal if\nthere exists a constant L > 0 such that\nf (cj ) \u2212 L\u2206j \u2264 f (ci ) \u2212 L\u2206i , \u2200i = 1, ..., n,\nf (cj ) \u2212 L\u2206j \u2264 fmin \u2212 \u03b5|fmin |,\n\n(13)\n(14)\n\nwhere \u2206i = (bi \u2212 ai )/2, and \u03b5 is a constant defining the requested relative\nimprovement, 0 < \u03b5 < 1. All potentially optimal subintervals are subdivided\nat the current iteration.\nLet us consider the iteration where the potentially optimal j-th subinterval is not the longest one. Then f (cj ) \u2264 f (ci ) for all ci where \u2206j = \u2206i .\nOtherwise there exists a constant L such that\nf (cj ) \u2212 f (ci )\n,\ni: \u2206j >\u2206i\n\u2206j \u2212 \u2206i\nf (ci ) \u2212 f (cj )\nL \u2264 min\n,\ni: \u2206j <\u2206i\n\u2206i \u2212 \u2206j\nL \u2265 (f (cj ) \u2212 fmin + \u03b5|fmin |)/\u2206j .\nL \u2265\n\nmax\n\n(15)\n(16)\n(17)\n\nThe values f (ci ) and \u2206i corresponding to the minimum in (16) are denoted as f + and \u2206+ correspondingly, i.e.\nmin\n\ni: \u2206j <\u2206i\n\nf (ci ) \u2212 f (cj )\nf + \u2212 f (cj )\n=\n.\n\u2206i \u2212 \u2206j\n\u2206+ \u2212 \u2206j\n\n7\n\n(18)\n\n\fLet the values of the function \u03c6(x) = f (x) + \u03b4 be computed at the points\nci , and assume that the following inequality \u03b4 > \u03b4f /\u03b5 is valid, where\n\u03b4f = (f + \u2212 f (cj ))\n\n\u2206j\n\u2212 f (cj ) + (1 \u2212 \u03b5)fmin .\n\u2212 \u2206j\n\n\u2206+\n\n(19)\n\nFor the data related to \u03c6(x) the following inequality holds:\nL\u2212 = (\u03c6(cj ) \u2212 \u03c6min + \u03b5|\u03c6min |)/\u2206j\n> (f (cj ) \u2212 fmin )/\u2206j + (\u03b5fmin + \u03b4f /\u2206j )\nf + \u2212 f (cj )\n=\n= L+ ,\n\u2206+ \u2212 \u2206j\nand a constant L satisfying the inequalities L\u2212 \u2264 L \u2264 L+ can not exist.\nTherefore the j-th subinterval for the function \u03c6(x) is not potentially optimal\nbecause necessary conditions (analogous to (16) and (17) for the function\nf (x)) are not satisfied.\n6. Numerical Example\nTo demonstrate the strong homogeneity of the P-algorithm an example of one dimensional optimization is considered. For a statistical model\nthe stationary Gaussian stochastic function with correlation function \u03c1(t) =\nexp(\u22125t) is chosen. Let the values of the first objective function (say f (x))\ncomputed at the points (0, 0.2, 0.5, 0.9, 1) be equal to (-0.8, -0.9, -0.65, -0.85,\n-0.55), and the values of the second objective function (say \u03c6(x)) be equal to\n(0, -0.4, 0.6, -0.2, 0.99). The graphs of the conditional mean and conditional\nstandard deviation for both sets of data are presented in Figure 1. In the\nsection of Figure 1 showing the conditional means, the horizontal lines are\ndrawn at the levels yo4 and zo4 correspondingly.\nIn spite of the obvious difference in the data, the functions expressing\nthe probability of improvement for both cases coincide. Therefore, their\nmaximizers which define the next points of function evaluations also coincide.\nThis coincidence is implied by the strong homogeneity of the P-algorithm and\nthe following relation: \u03c6(x) = af (x) + b, where the values of a, b up to five\ndecimal digits are equal to a = 3.9765, b = 3.1804.\n\n8\n\n\fConditional Means\n1\n\n0.5\n\n0\n\n\u22120.5\n\n\u22121\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nConditional Standard Deviations\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nProbability of Improvement\n0.2\n\n0.15\n\n0.1\n\n0.05\n\n0\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nFigure 1: An example of data used for planning the current iteration of the P-algorithm\n\n9\n\n\f7. Conclusions\nBoth the P-algorithm and the one-step Bayesian algorithm are strongly\nhomogeneous. The optimization results by these algorithms are invariant\nwith respect to affine scaling of values of the objective function. The implementations of these algorithms using the conventional computer arithmetic\ncombined with the scaling of function values, using the arithmetic of infinity,\nare applicable to the objective functions with either infinite or infinitesimal\nvalues. The optimization results, obtained in this way, would be identical\nwith the results obtained applying the implementations of the algorithms in\nthe arithmetic of infinity.\n8. ACKNOWLEDGEMENTS\nThe valuable remarks of two unknown referees facilitated a significant\nimprovement of the presentation of results.\nReferences\n[1] Elsakov S.M., Shiryaev V.I. (2010) Homogeneous algorithms for multiextremal optimization, Computational Mathematics and Mathematical\nPhysics, vol.50(10), 1642-1654\n[2] Gutmann H.-G. (2001) A radial basis function method for global optimization, Journal of Global Optimization, vol.19, 201-227.\n[3] Jones D.R. et al. (1993) Lipschitzian optimization without the Lipschitz\nconstant, Journal of Optimization Theory and Applications, vol.79 (1),\n157181.\n[4] Mockus J. (1972) On Bayesian methods of search for extremum, Avtomatika i Vychislitelnaja Tekhnika, No.3, 53-62, (in Russian).\n[5] Mockus J. (1988) Bayesian Approach to Global Optimization, Kluwer\nAcademic Publishers, Dodrecht.\n[6] Sergeyev Ya.D. (2008) A new applied approach for executing computations with infinite and infinitesimal quantities, Informatica, vol.19(4),\n567-596.\n\n10\n\n\f[7] Sergeyev Ya.D. (2009) Numerical computations and mathematical modelling with infinite and infinitesimal numbers, Journal of Applied Mathematics and Computing, vol.29, 177-195.\n[8] Sergeyev Ya.D. (2010) Lagrange Lecture: Methodology of numerical\ncomputations with infinities and infinitesimals, Rendiconti del Seminario\nMatematico dell'Universit e del Politecnico di Torino, vol.68(2), 95113.\n[9] Strongin R., Sergeyev Ya.D. (2000) Global Optimization with Nonconvex Constraints, Kluwer Academic Publishers, Dodrecht.\n[10] T\u00f6rn A., \u017dilinskas A. (1989) Global optimization, Lecture Notes in Computer Science, vol.350, 1-255.\n[11] Zhigljavsky A., \u017dilinskas A. (2008) Stochastic Global Optimization,\nSpringer, N.Y.\n[12] \u017dilinskas A. (1982) Axiomatic approach to statistical models and their\nuse in multimodal optimization theory, Mathematical Programming,\nvol.22, 104-116.\n[13] \u017dilinskas A. (1985) Axiomatic characterization of a global optimization\nalgorithm and investigation of its search strategies, Operations Research\nLetters, vol.4, 35-39.\n[14] \u017dilinskas A. (2010) On similarities between two models of global optimization: statistical models and radial basis functions, Journal of Global\nOptimization, vol.48, 173-182.\n[15] \u017dilinskas A. (2011) Small sample estimation of parameters for Wiener\nprocess with noise, Communications in Statistics - Theory and Methods,\nvol. 40(16), 3020-3028.\n[16] \u017dilinskas A,. \u017dilinskas J. (2010) Interval arithmetic based optimization\nin nonlinear regression, Informatica, vol.21(1), 149-158.\n\n11\n\n\f"}