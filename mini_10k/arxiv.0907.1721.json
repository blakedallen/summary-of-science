{"id": "http://arxiv.org/abs/0907.1721v1", "guidislink": true, "updated": "2009-07-10T04:50:22Z", "updated_parsed": [2009, 7, 10, 4, 50, 22, 4, 191, 0], "published": "2009-07-10T04:50:22Z", "published_parsed": [2009, 7, 10, 4, 50, 22, 4, 191, 0], "title": "Distributed Function Computation in Asymmetric Communication Scenarios", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0907.4728%2C0907.4932%2C0907.5198%2C0907.0842%2C0907.1488%2C0907.1871%2C0907.0648%2C0907.1818%2C0907.3247%2C0907.3941%2C0907.0688%2C0907.2106%2C0907.5455%2C0907.4058%2C0907.1489%2C0907.3065%2C0907.5203%2C0907.3555%2C0907.4215%2C0907.5496%2C0907.0143%2C0907.1881%2C0907.1095%2C0907.5221%2C0907.5229%2C0907.1212%2C0907.0735%2C0907.4119%2C0907.5087%2C0907.3382%2C0907.4345%2C0907.1378%2C0907.5317%2C0907.1752%2C0907.0787%2C0907.1298%2C0907.0072%2C0907.4637%2C0907.0414%2C0907.4860%2C0907.5443%2C0907.2283%2C0907.1239%2C0907.5376%2C0907.4525%2C0907.1388%2C0907.0589%2C0907.1721%2C0907.0025%2C0907.0700%2C0907.2290%2C0907.4101%2C0907.5279%2C0907.4582%2C0907.5011%2C0907.2972%2C0907.3012%2C0907.4445%2C0907.0317%2C0907.3325%2C0907.4014%2C0907.0884%2C0907.0776%2C0907.3981%2C0907.0878%2C0907.4077%2C0907.1839%2C0907.2103%2C0907.5013%2C0907.4271%2C0907.5275%2C0907.3923%2C0907.0965%2C0907.5392%2C0907.0036%2C0907.1842%2C0907.3678%2C0907.4206%2C0907.3643%2C0907.0214%2C0907.0241%2C0907.4674%2C0907.2189%2C0907.2610%2C0907.2689%2C0907.5029%2C0907.1142%2C0907.0732%2C0907.3966%2C0907.4908%2C0907.1694%2C0907.2613%2C0907.1487%2C0907.0854%2C0907.3522%2C0907.2611%2C0907.1631%2C0907.2353%2C0907.1925%2C0907.5389%2C0907.0056&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Distributed Function Computation in Asymmetric Communication Scenarios"}, "summary": "We consider the distributed function computation problem in asymmetric\ncommunication scenarios, where the sink computes some deterministic function of\nthe data split among N correlated informants. The distributed function\ncomputation problem is addressed as a generalization of distributed source\ncoding (DSC) problem. We are mainly interested in minimizing the number of\ninformant bits required, in the worst-case, to allow the sink to exactly\ncompute the function. We provide a constructive solution for this in terms of\nan interactive communication protocol and prove its optimality. The proposed\nprotocol also allows us to compute the worst-case achievable rate-region for\nthe computation of any function. We define two classes of functions: lossy and\nlossless. We show that, in general, the lossy functions can be computed at the\nsink with fewer number of informant bits than the DSC problem, while\ncomputation of the lossless functions requires as many informant bits as the\nDSC problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0907.4728%2C0907.4932%2C0907.5198%2C0907.0842%2C0907.1488%2C0907.1871%2C0907.0648%2C0907.1818%2C0907.3247%2C0907.3941%2C0907.0688%2C0907.2106%2C0907.5455%2C0907.4058%2C0907.1489%2C0907.3065%2C0907.5203%2C0907.3555%2C0907.4215%2C0907.5496%2C0907.0143%2C0907.1881%2C0907.1095%2C0907.5221%2C0907.5229%2C0907.1212%2C0907.0735%2C0907.4119%2C0907.5087%2C0907.3382%2C0907.4345%2C0907.1378%2C0907.5317%2C0907.1752%2C0907.0787%2C0907.1298%2C0907.0072%2C0907.4637%2C0907.0414%2C0907.4860%2C0907.5443%2C0907.2283%2C0907.1239%2C0907.5376%2C0907.4525%2C0907.1388%2C0907.0589%2C0907.1721%2C0907.0025%2C0907.0700%2C0907.2290%2C0907.4101%2C0907.5279%2C0907.4582%2C0907.5011%2C0907.2972%2C0907.3012%2C0907.4445%2C0907.0317%2C0907.3325%2C0907.4014%2C0907.0884%2C0907.0776%2C0907.3981%2C0907.0878%2C0907.4077%2C0907.1839%2C0907.2103%2C0907.5013%2C0907.4271%2C0907.5275%2C0907.3923%2C0907.0965%2C0907.5392%2C0907.0036%2C0907.1842%2C0907.3678%2C0907.4206%2C0907.3643%2C0907.0214%2C0907.0241%2C0907.4674%2C0907.2189%2C0907.2610%2C0907.2689%2C0907.5029%2C0907.1142%2C0907.0732%2C0907.3966%2C0907.4908%2C0907.1694%2C0907.2613%2C0907.1487%2C0907.0854%2C0907.3522%2C0907.2611%2C0907.1631%2C0907.2353%2C0907.1925%2C0907.5389%2C0907.0056&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider the distributed function computation problem in asymmetric\ncommunication scenarios, where the sink computes some deterministic function of\nthe data split among N correlated informants. The distributed function\ncomputation problem is addressed as a generalization of distributed source\ncoding (DSC) problem. We are mainly interested in minimizing the number of\ninformant bits required, in the worst-case, to allow the sink to exactly\ncompute the function. We provide a constructive solution for this in terms of\nan interactive communication protocol and prove its optimality. The proposed\nprotocol also allows us to compute the worst-case achievable rate-region for\nthe computation of any function. We define two classes of functions: lossy and\nlossless. We show that, in general, the lossy functions can be computed at the\nsink with fewer number of informant bits than the DSC problem, while\ncomputation of the lossless functions requires as many informant bits as the\nDSC problem."}, "authors": ["Samar Agnihotri", "Rajesh Venkatachalapathy"], "author_detail": {"name": "Rajesh Venkatachalapathy"}, "author": "Rajesh Venkatachalapathy", "arxiv_comment": "10 pages, 6 figures, 2 tables", "links": [{"href": "http://arxiv.org/abs/0907.1721v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0907.1721v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0907.1721v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0907.1721v1", "journal_reference": null, "doi": null, "fulltext": "Distributed Function Computation in Asymmetric\nCommunication Scenarios\n(Extended Abstract)\nSamar Agnihotri\u2020 and Rajesh Venkatachalapathy\u00a7\n\u2020 CEDT,\n\n\u00a7 Systems\n\nIndian Institute of Science, Bangalore - 560012, India\nScience Graduate Program, Portland State University, Portland, OR 97207\nEmail: samar@cedt.iisc.ernet.in, venkatr@pdx.edu\n\narXiv:0907.1721v1 [cs.IT] 10 Jul 2009\n\nAbstract\nWe consider the distributed function computation problem in asymmetric communication scenarios, where the sink computes\nsome deterministic function of the data split among N correlated informants. The distributed function computation problem is\naddressed as a generalization of distributed source coding (DSC) problem. We are mainly interested in minimizing the number of\ninformant bits required, in the worst-case, to allow the sink to exactly compute the function. We provide a constructive solution\nfor this in terms of an interactive communication protocol and prove its optimality. The proposed protocol also allows us to\ncompute the worst-case achievable rate-region for the computation of any function. We define two classes of functions: lossy and\nlossless. We show that, in general, the lossy functions can be computed at the sink with fewer number of informant bits than the\nDSC problem, while computation of the lossless functions requires as many informant bits as the DSC problem.\n\nI. I NTRODUCTION\nLet us consider a distributed function computation scenario, where a sink node is interested in exactly computing some\ndeterministic function f = f (X) of data-vector X that is split among N correlated informants. The correlation in informants'\ndata is modeled by discrete and finite distribution P, known only to the sink (asymmetric communication, [1]). The sink and\ninformants interactively communicate with each other, with communication proceeding in rounds, as in [2]. We are concerned\nwith minimizing the number of bits that the informants send, in the worst-case, to allow the sink to compute the function.\nWe consider the distributed function computation problem as a generalization of distributed source coding (DSC) problem1.\nThe particular distributed function computation problem we consider is a generalization of DSC problem in asymmetric\ncommunication scenarios, we addressed in [3]. As for that work, the motivation for this work too comes from sensor networks,\nparticularly from our efforts to address the distributed function computation problem in single-hop data-gathering wireless sensor\nnetworks, while maximizing the worst-case operational lifetime of the network. In a typical data-gathering sensor network, it\nis reasonable to assume that the base-station has large resources of energy, computation, and communication as well as the\nknowledge of correlations in sensor data, whereas a sensor node is resource limited and only knows its sampled data-values.\nTherefore, we argue that in such communication scenarios, the onus should be on the base-station to bear most of the burden\nof computation and communication associated with function computation. Allowing interactive communication between the\nbase-station and sensor nodes lets us precisely do this: base-station forms and communicates efficient queries to sensor nodes,\nwhich they respond to with short and easily computable messages. This reduces the communication and computation effort at\nsensor nodes, hence enhancing their lifetime, which in turn leads to increased network lifetime.\nThe distributed function computation problem was first addressed by Yao in [2] and later by other researchers in different\nsetups, as we discuss in Section II. However, our work mainly differs from the extant work in one or more aspects as follows.\nFirst, we approach the distributed function computation problem as a generalization of DSC problem. This allows us to exploit\nthe correlation in informants' data to solve the function computation problem at the sink with fewer informant bits. Second,\nwe are concerned with asymmetric communication (only sink knows the joint distribution of informants' data) and asymmetric\ncomputation (only sink computes the function). Third, we are concerned with the worst-case analysis. Fourth, we are interested\nin distributed function computation with a single instance of data at informants (one-shot computation problem). Finally, we\nconsider a more powerful model of communication where the sink and informants interactively communicate with each other.\nOur work allows us to clearly delineate the roles played in optimally solving distributed function computation problem in\narbitrary networks by correlation in informants' data, the properties of the function to be computed, communication model,\nnetwork connectivity graph, and routing strategies. In this sense, our work acts as a fundamental building block to a general\ntheory of distributed function computation over arbitrary networks, which we expect to eventually develop.\nIn Section III, we revisit the notion of information ambiguity, an information measure we proposed in [4] for the worst-case\ninformation-theoretic analyses, and extend it to a form useful in the present context. In Section IV, we provide the details\nof the communication model we assume and formally introduce the variant of distributed function computation problem we\naddress in this paper. In the next section, we give a communication protocol to compute any given function at the sink, prove\n1 DSC\n\nproblem is a special case of distributed function computation problem where the function to be computed is identity map, idX .\n\n\fits optimality with respect to minimizing the number of informant bits, and provide the bounds on its performance. Finally in\nSection VI, we discuss some properties of distributed function computation problem and propose a classification scheme for\nfunctions, based on the number of informant bits required, in general, to compute those at the sink.\nII. R ELATED W ORK\nThere are three major existing approaches to address the distributed function computation problem, as follows:\nCommunication complexity: The seminal paper by Yao [2] introduced the problem of computing the minimum number of bits\nexchanged between two processors when both the processors compute a function of the input that is split between processors.\nVariants of this problem and numerous solution approaches have been explored in the field of communication complexity, [1].\nThis work provides insights into developing efficient communication protocols for function computation. However, it is mainly\ninterested in estimating the order-of-magnitude of the bounds on communication and computation costs. Also, it is not obvious\nhow to extend this work, when for example, one or more nodes in the network are interested in computing some function of\nsource nodes' data or the source data is split among more than two nodes and is possibly correlated.\nScaling laws: Recently in [5]\u2013[7], the distributed function computation problem has been addressed to find how the rate\nof function computation scales with network size. This approach however does not provide a simple framework to exploit\nthe correlation in source data and to incorporate stronger models of computation and communication, such as interactive\ncommunication, data-buffers, cooperating sources.\nInformation theory: Much before Yao introduced his formulation of distributed function computation problem, Slepian and\nWolf in [8] introduced the DSC problem. It was many years before distributed function computation problem was seriously\naddressed in information-theoretic setup, [9]\u2013[12]. Still, there is very little such work that comprehensively addresses the\ndistributed function computation problem over any given network, function, and model of communication and computation.\nIII. I NFORMATION A MBIGUITY FOR D ISTRIBUTED F UNCTION C OMPUTATION\nWe revise and generalize some relevant definitions and properties of information ambiguity, an information measure we\nintroduced in [4] for performing the worst-case information-theoretic analysis in certain communication scenarios. We then\nextend the notion of information ambiguity to a form useful for distributed function computation in this paper.\nNote: All the logarithms used in this paper are to the base 2, unless explicitly mentioned otherwise.\nLet us consider a N -tuple of random variables (X1 , . . . , XN ) \u223c P = p(x1 , . . . , xN ), Xi \u2208 X , i \u2208 {1, . . . , N }, where X is\ndiscrete and finite alphabet of size |X |. The support set of (X1 , . . . , XN ) is defined as:\ndef\n\nSX1 ,...,XN = {(x1 , . . . , xN )|p(x1 , . . . , xN ) > 0}\n\n(1)\n\nWe also call SX1 ,...,XN as the ambiguity set of (X1 , . . . , XN ). The cardinality of SX1 ,...,XN is called ambiguity of (X1 , . . . , XN )\nand denoted as \u03bcX1 ,...,XN = |SX1 ,...,XN |. So, the minimum number of bits required to describe an element of SX1 ,...,XN , in\nthe worst-case, is \u2308log \u03bcX1 ,...,XN \u2309.\nThe support set SXi of Xi , i \u2208 {1, . . . , N }, is the set\ndef\n\ndef\n\nSXi = {xi : for some x\u2212i , (x\u2212i , xi ) \u2208 SX1 ,...,XN }, with x\u2212i = {x1 , . . . , xN } \\ xi\n\n(2)\n\nof all possible Xi values. We also call SXi ambiguity set of Xi . The ambiguity of Xi is defined as \u03bcXi = |SXi |. The conditional\nambiguity set of (X1 , . . . , XN ), when random variable Xi takes the value xi , xi \u2208 SXi , is\ndef\n\nSX1 ,...,XN |Xi (xi ) = {(x1 , . . . , xN ) : (x1 , . . . , xN ) \u2208 SX1 ,...,XN and xi \u2208 SXi },\n\n(3)\n\nthe set of possible (X1 , . . . , XN ) values when Xi = xi . The conditional ambiguity in that case is \u03bcX1 ,...,XN |Xi (xi ) =\n|SX1 ,...,XN |Xi (xi )|, the number of possible values of (X1 , . . . , XN ) when Xi = xi . The maximum conditional ambiguity of\n(X1 , . . . , XN ) is\ndef\n(4)\n\u03bc\nbX1 ,...,XN |Xi = sup{\u03bcX1 ,...,XN |Xi (xi ) : xi \u2208 SXi },\n\nthe maximum number of (X1 , . . . , XN ) values possible with any value that Xi can take.\nIn fact, for any two subsets XA and XB of {X1 , . . . , XN }, such that XA \u222a XB \u2286 {X1 , . . . , XN } and XA \u2229 XB = \u03c6,\nwe can define for example, ambiguity set SXA of XA , conditional ambiguity set SXA |XB (xB ) of XA given the set xB of\nvalues that XB can take, and maximum conditional ambiguity set SXA |XB of XA for any set of values that XB can take,\nwith corresponding ambiguity, conditional ambiguity, and maximum conditional ambiguity given by \u03bcXA , \u03bcXA |XB (xB ), and\n\u03bc\nbXA |XB , respectively. However, for the sake of brevity, we do not develop the precise definitions of these quantities here.\nFurther, let us represent each of \u03bcXi values that random variable Xi can take in \u2308log \u03bcXi \u2309 bits as bi1 . . . bi\u2308log \u03bcX \u2309 . Let\ni\nbinaryj (xi ) represent the value of j th , 1 \u2264 j \u2264 \u2308log \u03bcXi \u2309, bit-location in the bit-representation of xi . Then, knowing that the\nvalue of j th bit-location is b, b \u2208 {0, 1}, we can define the set of possible values that Xi can take as\ndef\n\nSXi |bij (b) = {xi : xi \u2208 SXi and binaryj (xi ) = b},\n\n(5)\n\n\fTABLE I\nN OTATION USED FREQUENTLY IN THE PAPER\nN\nX\nP\nXi\nSX i\n\nSX1 ,...,XN\nSX1 ,...,XN |I\nSf\nSf |I\n#f\n#DSC\n\nnumber of informants\ndiscrete and finite alphabet set of cardinality |X |\nN -dimensional discrete probability distribution, P = p(x1 , . . . , xN ), xi \u2208 X\nrandom variable observed by ith informant. Xi \u2208 X\nambiguity set at the sink of ith informant's data, with corresponding ambiguity \u03bcXi = |SXi |\nambiguity set at the sink of all informants' data, with corresponding ambiguity \u03bcX1 ,...,XN = |SX1 ,...,XN |\nconditional ambiguity set at the sink of all informant's data, when sink has information I, with corresponding\nconditional ambiguity \u03bcXi |I = |SXi |I |. The exact nature of I will be obvious from the context\nambiguity set at the sink of the output values of function f , with corresponding ambiguity \u03bcf = |Sf |\nconditional ambiguity set at the sink of the output values of function f when sink has information I, with corresponding\nconditional ambiguity \u03bcf |I = |Sf |I |\nminimum number of informant bits required in the worst-case to compute the function f at the sink\nminimum number of informant bits required in the worst-case to solve the DSC problem at the sink\n\nwith corresponding cardinality denoted as \u03bcXi |bij (b). We can similarly define SXA |bij (b) with Xi \u2208 XA as\ndef\n\nSXA |bij (b) = {xA : xA \u2208 SXA and binaryj (xi ) = b},\n\n(6)\n\nwith corresponding cardinality denoted as \u03bcXA |bij (b). The definitions of conditional ambiguity sets in (5) and (6) can be easily\nextended to the situations where the values of one or more bit-locations in one or more random variable's bit-representation\nare known, but once more for the sake of brevity, we omit the details of such extended definitions.\nNext, we introduce the notion of the ambiguity set and ambiguity of the function output values. The support-set of output\nvalues of some function f , also called ambiguity set of function output values of function f , is defined as:\ndef\n\nSf = {f (x1 , . . . , xN ) : for some (x1 , . . . , xN ) \u2208 SX1 ,...,XN }\n\n(7)\n\nThe cardinality of Sf is called ambiguity of output values of function f and denoted as \u03bcf = |Sf |. So, the minimum number\nof bits required to describe an element in Sf is \u2308log \u03bcf \u2309. The conditional ambiguity set of function output values when\nXi = xi , xi \u2208 SXi , i \u2208 {1, . . . , N }, is defined as\ndef\n\nSf |Xi (xi ) = {f (x1 , . . . , xN ) : for some (x1 , . . . , xN ) \u2208 SX1 ,...,XN |Xi (xi )}\n\n(8)\n\nThe corresponding cardinality is called conditional ambiguity of function output values when Xi = xi and denoted as \u03bcf |Xi (xi ).\nWe can further define the maximum conditional ambiguity of function output values as\ndef\n\n\u03bc\nbf |Xi = sup{\u03bcf |Xi (xi ) : xi \u2208 SXi }\n\n(9)\n\nSf |bij (b) = {f (x1 , . . . , xN ) : (x1 , . . . , xN ) \u2208 SX1 ,...,XN and xi \u2208 SXi |bij (b)},\n\n(10)\n\nmaximum number of function output values possible over any value that Xi can take over SXi . The definitions in (8) and (9)\ncan be similarly extended to the situations where the conditioning is carried out over a subset XA of {X1 , . . . , XN }. We omit\nthe discussion of such extensions here.\nFurther, when the value of j th bit-location in the binary-representation of xi , xi \u2208 SXi , is known, that is bij = b, b \u2208 {0, 1},\nwe can define corresponding conditional ambiguity set of function output values as follows\ndef\n\nwith corresponding cardinality denoted as \u03bcf |bij (b).\nIf the function f is defined for every XA , XA \u2282 {X1 , . . . , XN }, then for a given support-set SX1 ,...,XN of data-vectors,\nthe functional \u2308log \u03bcf \u2309 is a valid information measure as it satisfies various axioms of such measures, such as expansibility,\nmonotonicity, symmetry, subadditivity, and additivity, [13]. We omit the details of proof for the sake of brevity .\nIn the Table I, we summarize the notation used frequently in this section and in the rest of the paper.\nIV. D ISTRIBUTED F UNCTION C OMPUTATION\n\nIN\n\nA SYMMETRIC C OMMUNICATION S CENARIOS\n\nLet us consider a distributed function computation scenario, where a sink computes some function of the data of N correlated\ninformants. We assume the asymmetric communication, where the joint distribution P of informants' data is known only to\nthe sink. The Figure 1 depicts this scenario for N = 2.\nProblem Statement: A sample X = (x1 , . . . , xN ) is drawn i.i.d. from a discrete and finite distribution P over N binary\nstrings, as in [14], [15]. The strings of X are revealed to N informants, with the string xi , i \u2208 {1, . . . , N }, being given to the ith\ninformant. The sink wants to exactly compute a deterministic function f = f (X) of informants' data X (one-shot computation\nproblem). Our objective is to minimize the total number of informant bits required, in the worst-case, to accomplish this.\nThe Problem Setting: We consider an asymmetric communication scenario [1]. Communication takes place over N binary,\nerror-free channels, where each channel connects an informant with the sink. An informant and the sink can interactively\n\n\fP ( knows X )\nX1\n1\n\nP ( knows X )\n2\nX2\nFig. 1.\n\n(\n\nknows p(x , x )\n1 2\nwants to compute f(x , x )\n1 2\n\n(\n\nP\n\nDistributed function computation problem for two informants in asymmetric communication scenarios.\n\ncommunicate over the channel connecting them by exchanging messages (finite sequences of bits determined by agreed\nupon, deterministic protocol). The informants cannot communicate directly with each other, though. We assume that the\ncommunication between the sink and the informants proceeds in rounds, as in [2]. In each round, depending on the information\nheld by the communicators, one or other communicator may send the first message. However, we assume, as in [16], that in\neach communication round, first the sink communicates to the informants and then, the informants respond with their messages.\nEach bit communicated over any channel is counted, as either a sink bit if sent by the sink or an informant bit if sent by an\ninformant.\nWe assume the informants to be memoryless in the sense that they do not remember the messages they send in different\nrounds. We assume that ith informant knows its support-set SXi , so that it represents the binary string xi , given to it, as\nbi1 . . . bi\u2308log \u03bcX \u2309 in \u2308log \u03bcXi \u2309 bits.\ni\nThe sink knows the distribution P and the corresponding support-sets: SX1 ,...,XN of data-vectors and Sf of function output\nvalues. So, every X, X \u2208 SX1 ,...,XN , can be uniquely described using \u2308log \u03bcX1 ,...,XN \u2309 bits and every f (X) can be uniquely\ndescribed using \u2308log \u03bcf \u2309 bits. This implies that to compute f (X) unambiguously, the sink must receive at least \u2308log \u03bcf \u2309 bits\nfrom the informants, in the worst-case.\nFor the design and analysis of efficient communication protocols for distributed function computation, we develop a problemencoding scheme as follows. Every informant data-vector X, X \u2208 SX1 ,...,XN , can also be uniquely described by concatenating\nPN\nthe bit-representations of all corresponding xi , 1 \u2264 i \u2264 N . That is, X can be represented at the receiver by i=1 \u2308log \u03bcXi \u2309 bits\nlong representation, constructed by concatenating ith informant's \u2308log \u03bcXi \u2309 bit-representation of xi , for each i \u2208 {1, . . . , N }.\nWith this encoding scheme, our distributed function computation problem reduces to minimizing the number #f of bit-locations\nin the concatenated bit-representation\nof X, whose values the sink needs to exactly compute f (X). It should be noted that\nP\ntrivially, \u2308log \u03bcf \u2309 \u2264 #f \u2264 N\n\u2308log\n\u03bc\nXi \u2309.\ni=1\nWe illustrate this problem-encoding scheme with an example support-set in Figure 2. Let the informants 1 and 2 observe\ntwo correlated random variables X1 and X2 , respectively, with (X1 , X2 ) derived from the support-set in first column. Let the\nfunction f being computed at the sink be 'bitwise OR' of the instance of (X1 , X2 ) revealed to the informants. For the given\nsupport-set, at least \u2308log \u03bcX1 ,X2 \u2309 = 4 bits are required to describe any element of SX1 ,X2 and at least \u2308log \u03bcf \u2309 = 3 bits are\nrequired to describe any element of Sf . Also, to individually describe any value assumed by X1 and X2 , it requires 3 bits.\nFor any given support-set of data-vectors, sink that knows the joint distribution P, can construct a problem-encoding as\nin Figure 2. It knows that one string, hitherto unknown, from the fourth column is drawn, with first \u2308log \u03bcX1 \u2309 bits given to\ninformant 1, next \u2308log \u03bcX2 \u2309 bits given to informant 2, and so on. We require the sink to exactly evaluate the given function\nf on this string, whose different parts are held by different informants, with the informants sending minimum total number of\nbits to the sink.\nNote on the terminology: We call a bit-location in the bit-string at an informant (as well as in the bit-representation of X\nin encoding scheme defined above) defined, if the sink knows its value unambiguously, otherwise itPis called undefined. For\nexample, until the sink learns of the actual X revealed to the informants, one or more bits in the N\ni=1 \u2308log \u03bcXi \u2309 bits long\nrepresentation of X, remain undefined. Similarly, a bit-location in the bit-representation of the output of the function f is\ncalled evaluated if the sink can unambiguously compute its value based on the values of one or more bits in informant strings.\nV. C OMMUNICATION P ROTOCOL\n\nFOR\n\nD ISTRIBUTED F UNCTION C OMPUTATION\n\nWe address the distributed function computation problem, introduced in the last section, in bit-serial communication scenarios,\nwhere in each communication round, only one informant can send only one bit to the sink. This is an example of scenarios\nwhere communication takes place over a channel with uplink throughput constrained to one bit per channel use. Our interest in\nthis communication model stems from it allowing us to compute the minimum number of informant bits (total and individual)\nrequired to compute f (X) at the sink when any number of rounds and sink bits can be used. In other words, this communication\nscenario enables us to compute the worst-case achievable rate-region for this problem, as we show later in this section.\nWe provide a constructive solution of the distributed function computation problem of the last section, based on interactive\ncommunication. The proposed protocol optimally solves this problem and computes the worst-case achievable rate-region. We\ncall the proposed protocol \"bit-serial function Computation (bSerfComp)\" protocol and describe it next.\n\n\fSupport set S X , X\n1 2\n\nX\n1\nX\n2\n\n1\n1\n\n2\n\n*\n\n2\n\n3\n\n*\n\n4\n\n5\n\n*\n*\n\n3\n\n4\n\n*\n*\n\n*\n\n5\n\n*\n*\n\n*\n\nElements of S X , X\n1 2\n\nBinary Representation\n\nConcatenated Binary\nRepresentation\n\nFunction Output for\n'bitwise OR'\n\n( 1, 1 )\n\n( 000, 000 )\n\n000000\n\n000\n\n( 1, 3 )\n\n( 000, 010 )\n\n000010\n\n010\n\n( 2, 2 )\n\n( 001, 001 )\n\n001001\n\n001\n\n( 2, 4 )\n\n( 001, 011 )\n\n001011\n\n011\n\n( 3, 1 )\n\n( 010, 000 )\n\n010000\n\n010\n\n( 3, 3 )\n\n( 010, 010 )\n\n010010\n\n010\n\n( 3, 5 )\n\n( 010, 100 )\n\n010100\n\n110\n\nS X = {1, 2, 3, 4, 5}\n\n( 4, 2 )\n\n( 011, 001 )\n\n011001\n\n011\n\nS X = {1, 2, 3, 4, 5}\n\n( 4, 4 )\n\n( 011, 011 )\n\n011011\n\n011\n\nS = {000, 001, 010, 011, 110}\nf\n\n( 5, 3 )\n\n( 100, 010 )\n\n100010\n\n110\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\n1\n\n2\n\nFig. 2. Example of problem encoding: (a) Support-sets: SX1 ,X2 , SX1 , SX2 , and Sf with \u03bcX1 ,X2 = 10, \u03bcX1 = \u03bcX2 = 5, \u03bcf = 5 (b) the members of\nSX1 ,X2 (c) binary representation of members of SX1 ,X2 (d) the concatenated binary representation. If the string '000010' is drawn, then '000' is given to\ninformant 1 and '010' is given to informant 2. (e) Function output values corresponding to X for 'bitwise OR'.\n\nA. The bSerfComp protocol\nIn bSerfComp protocol, in each communication round only one bit is sent by the informant chosen to communicate with\nthe sink. The chosen bit has the property that it divides the size of the current conditional ambiguity set of function output\nvalues, at the sink, closest to half2P\n. Formally, in terms of the problem statement and encoding introduced in the last section, if\nN\nU is the set of undefined bits in i=1 \u2308log \u03bcXi \u2309 bits long representation of X, then the bit chosen in lth , l \u2265 0, round is the\none that solves argminj\u2208U maxb(j)\u2208{0,1} \u03bclf |b(j) . The sink, after receiving the value of the chosen bit, recomputes the set of\nundefined bits U . This is carried out iteratively till all bits in \u2308log \u03bcf \u2309 bits long representation of f (X) are not evaluated.\nAlgorithm: bSerfComp\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\nl=0\nl\nLet SX\n= SX1 ,...,XN\n1 ,...,XN\nLet Sfl = Sf , \u03bclf = |Sfl |\nPN\nLet V = {1, . . . , i=1 \u2308log \u03bci \u2309}\nl\nLet U be the set of undefined bits in V , U \u2286 V , over all X \u2208 SX\n1 ,...,XN\nl\nwhile (\u03bcf > 1)\nK l+1 = argminj\u2208U maxb(j)\u2208{0,1} \u03bclf |b(j)\nChoose the bit-location corresponding to k l+1 , where k l+1 is a randomly chosen element of K l+1\nThe sink asks the informant corresponding to bit-location k l+1 to send the bit-value b(k l+1 )\nl+1\nl\n= SX\nSet SX\nl+1 )\n1 ,...,XN\n1 ,...,XN |b(k\nl+1\nl\nSet Sf = Sf |b(kl+1 )\nCompute U \u2282 V , the set of undefined bits\nl =l+1\n\nThe sink can perform the worst-case performance analysis of the bSerfComp protocol by selecting on the line 9, b\u2217 (k l+1 )\nthat solves:\nb\u2217 (k l+1 ) = argmax \u03bclf |b(kl+1 )=s\ns={0,1}\n\nNote that there are two versions of the bSerfComp protocol: in the online version, the sequence of queries from the sink\nto the informants is determined adaptively depending on the informant response in the previous rounds, while in the offline\nversion, for a given support-set of data-vectors the entire sequence of queries is determined before actual querying starts. For\nexample, the sequence of queries generated for the worst-case analysis of the protocol corresponds to the offline version.\n2 For\n\nn-ary representation of data-values, this will be 1/n.\n\n\fB. Optimality of bSerfComp protocol\nThe binary representations of the elements of Sf , as in Figure 2.e, can be arranged as the leaves of a binary tree, where\nambiguity set of function output values Sf forms the root and conditional ambiguity sets of function output values form\ninternal nodes and leaves. The set of function output values corresponding to a child node is obtained by conditioning the set\nof function output values corresponding to its parent node on the value b, b \u2208 {0, 1} of bij : j th bit-location in the binary string\nrevealed to ith informant, with 'b = 0' leading to the left subtree and 'b = 1' leading to the right subtree. Such a binary tree\nwith \u03bcf leaves will have a minimum-height of \u2308log \u03bcf \u2309, implying that at least \u2308log \u03bcf \u2309 bits are required to describe any leaf,\nin the worst-case.\nLemma 1: bSerfComp protocol computes all minimum-height binary trees corresponding to the given support-set to exactly\nevaluate a given function f .\nProof: Follows from the definition of minimum-height binary trees and the description of bSerfComp protocol.\nLemma 2: bSerfComp protocol computes bi , the minimum number of bits that the ith , i \u2208 {1, . . . , N }, informant must\nsend to let the sink exactly evaluate the function f .\nProof: The bSerfComp protocol exploits the bit-serial communication scenario where a bit queried from the chosen\ninformant maximally conditions the resultant ambiguity set of function output values at the sink. Also, to reduce the number\nof bits that an informant sends, the bSerfComp protocol can procrastinate querying the bits from the concerned informant\nuntil it can be postponed no more, thus maximally reducing the number of bits an informant sends. Combining these two\nobservations, proves the lemma.\nLemma 3: For a given support-set, each corner point of the worst-case achievable rate-region for computing function f\ncorresponds to at least one minimum-height binary tree, with height #f .\nProof: For the sake of contradiction, let us assume that there is a corner point of the worst-case achievable rate-region to\nwhich no minimum-height binary tree corresponds to. This implies that this corner point is outside the worst-case rate-region\ndefined by the set of all the corner points visited by the set of minimum-height binary trees. This further implies that at this\ncorner point at least one informant, say ith , sends fewer bits than bi (defined in the statement of Lemma 2 above). However,\nthis contradicts the definition of bi , that it is the minimum number of bits ith informant needs to send to let the sink exactly\nevaluate the function f . Thus, there cannot be any corner point outside the rate-region defined by the set of corner points\ncorresponding to the set of all minimum-height binary trees, hence proving the lemma.\nTheorem 1: For a given support-set, bSerfComp protocol computes the worst-case achievable rate-region for function f .\nProof: Combining the statements of Lemmas 1 and 3, we can state that bSerfComp protocol computes each corner\npoint of the worst-case achievable rate-region. Thus, bSerfComp protocol computes the worst-case achievable rate-region for\ncomputing function f .\nThe worst-case achievable rate-region for distributed computation of function f in asymmetric communication scenarios is\ngiven by the following corollary to Theorem 1. For the sake of notational simplicity, we state it only for N = 2.\nCorollary 1: For N = 2, if bi denotes the minimum number of bits that an informant i, 1 \u2264 i \u2264 2, sends over all solutions\nof bSerfComp protocol and #f denotes the total number of bits sent by all informants, then the worst-case achievable rate\nregion is given by:\nR1\nR2\nR1 + R2\n\n\u2265 b1\n\u2265 b2\n\u2265 #f\n\nProof: The proof follows from the worst-case optimality of bSerfComp protocol proven in Theorem 1.\nIn Figures 3-4, using bSerfComp protocol, we compute the worst-case achievable rate-regions for functions: 'bitwise OR',\n'bitwise AND', and 'bitwise XOR', evaluated at sink over two support-sets of data-vectors for two correlated informants3.\nC. Performance bounds for bSerfComp protocol\nTo compute the bounds on the performance of bSerfComp protocol, we make use of an interesting and important observation\nregarding the working of bSerfComp protocol to compute a given function f at the sink for a given support-set of data-vectors.\nObservation: A bit-location in the bit-representation of the function output values can be evaluated, without all bit-locations\nin the concatenated bit-representation of X being defined.\nLet #f and #DSC denote the minimum number of informant bits required, in the worst-case, to evaluate the function f\nand to solve the DSC problem, respectively, at the sink for a given support-set of data-vectors.\n3 In computer programming literature, it is well-know how to compute the bitwise functions over two binary strings, [17]. Let B(X , X ) denote the output\ni\nj\nof the bitwise function B evaluated over binary-strings corresponding to Xi and Xj , i, j \u2208 {1, . . . , N }, i 6= j. Define B(Xi ) = Xi . Then, the evaluation\nof B over any number N, N \u2265 2, of arguments can be recursively defined, for example, as: B(X1 , . . . , XN ) = B(B(X1 , . . . , XN\u22121 ), XN ).\n\n\fR\n\nX\n1\nX\n2\n\n1\n1\n\n*\n\n2\n\n3\n\n4\n\n5\n\n2\n\n5\n\n5\n\n4\n\n4\n\n4\n\n3\n\n3\n\n3\n\n2\n\n2\n\n2\n\n*\n*\n\n*\n\n(a)\n\n3\n\n4\n\n5 R\n\n0\n\n1\n\n1\n\n2\n\n(b)\n\n3\n\n4\n\n5 R\n\n0\n\n1\n\n1\n\nR 2\n\n2\n\n+\n\n1\n\n1\n\nR 1\n\n0\n\n*\n\n1\n\nR 2\n\n5\n\n*\n\n+\n\n1\n\n*\n\nR 1\n\n*\n\nR 2\n\n*\n\n+\n\n*\n\n4\n\nR\n\n2\n\n5\n\nR 1\n\n3\n\n2\n\nR\n\n2\n\n2\n\n(c)\n\n3\n\n4\n\n5 R\n\n1\n\n(d)\n\nFig. 3. Distributed function computation - I: support-set of data-vectors (a) and worst-case achievable rate-regions for 'bitwise OR' in (b), for 'bitwise AND'\nin (c), and for 'bitwise XOR' in (d)\n\nR\n\nX\n1\nX\n2\n\n1\n1\n\n2\n\n*\n\n4\n\n5\n\n5\n\n4\n\n4\n\n4\n\n3\n\n3\n\n3\n\n2\n\n2\n\n*\n\n(a)\n\n1\n\n2\n\n3\n\n4\n\n5 R\n\n0\n\n1\n\n1\n\n1\n\n2\n\n(b)\n\n3\n\n4\n\n5 R\n\n(c)\n\n0\n\n1\n\nR 2\n\n0\n\n+\n\n*\n\n*\n\nR 1\n\n5\n\n1\n\nR 2\n\n1\n\n*\n\n+\n\n*\n\nR 1\n\nR 2\n\n*\n\n+\n\n2\n\n3\n*\n\n2\n\n5\n\n*\n*\n\n4\n\nR\n\n2\n\n5\n\nR 1\n\n2\n\n3\n\nR\n\n2\n\n1\n\n2\n\n3\n\n4\n\n5 R\n\n1\n\n(d)\n\nFig. 4. Distributed function computation - II: support-set of data-vectors (a) and worst-case achievable rate-regions for 'bitwise OR' in (b), for 'bitwise\nAND' in (c), and for 'bitwise XOR' in (d)\n\nLoose Bounds: As we discussed before, #f is bounded from below by \u2308log \u03bcf \u2309, that is\n\u2308log \u03bcf \u2309 \u2264 #f\nLet \u03bcs be the number of data-vectors or informant strings which evaluate to the function output value s, s \u2208 Sf . Also, let\nus define \u03bcmin\n= mins\u2208Sf \u03bcs . Then, assuming that the function f evaluates to the output that corresponds to \u03bcmin\n, we obtain\ns\ns\na trivial but useful upper bound on #f as:\n#f \u2264 #DSC \u2212 \u2308log \u03bcmin\n\u2309\ns\nTherefore, combining above two bounds on #f , we can say that #f loosely satisfies:\n\u2308log \u03bcf \u2309 \u2264 #f \u2264 #DSC \u2212 \u2308log \u03bcmin\n\u2309\ns\n\n(11)\n\nTight bounds: For a given support-set of data-vectors, \u2308log \u03bcf \u2309 is the lower bound on minimum number of informant bits\nrequired to evaluate the output of function f . Let us assume that the sink has obtained \u2308log \u03bcf \u2309 informant bits using bSerfComp\nprotocol. Let assume that the size of conditional ambiguity set of data-vectors at the end of lth round, 1 \u2264 l \u2264 \u2308log \u03bcf \u2309, is\n1/21\u2212\u01ebl of its size at the beginning of this round. Define \u01ebmax = max{\u01eb1 , . . . , \u01eb\u2308log \u03bcf \u2309 }. Then, the size of conditional ambiguity\nset of data-vectors after \u2308log \u03bcf \u2309 informant bits are received satisfies:\n\u03bcX1 ,...,XN\n\u03bcX ,...,XN\n\u2264 (1\u2212\u01eb 1 )\u2308log\nP\u2308log \u03bcf \u2309\n\u03bcf \u2309\nmax\n2\n(1\u2212\u01ebl )\n2 l=1\nNow, if\n\n\u03bcX1 ,...,XN\n(1\u2212\u01eb\nmax )\u2308log \u03bcf \u2309\n2\n\n\u2264 \u03bcmin\ns\n\nthen, the function output evaluation finishes with \u2308log \u03bcf \u2309 to \u2308log \u03bcf \u2309 + \u2308log \u03bcmin\n\u2309 informant bits. So, we have\ns\n\u2308log \u03bcf \u2309 \u2264 #f \u2264 \u2308log \u03bcf \u2309 + \u2308log \u03bcmin\n\u2309\ns\nand in this case the lower bound in (11) is actually tight.\nOtherwise, that is, if\n\u03bcX1 ,...,XN\n> \u03bcmin\ns\n2(1\u2212\u01ebmax )\u2308log \u03bcf \u2309\n\n(12)\n\n\fm\nl\n\u03bc 1 ,...,XN\nbits, where \u03bc\u2217 is the size\nthen, the function computation finishes in \u2308log \u03bcf \u2309 + \u2308log \u03bc\u2217 \u2309 to \u2308log \u03bcf \u2309 + log (1\u2212\u01ebXmax\n)\u2308log \u03bcf \u2309\n2\nof smallest subset of function output values that satisfies\nX\n\u03bcX1 ,...,XN\n\u03bcs\n\u2264\n(1\u2212\u01eb\n)\u2308log\n\u03bc\n\u2309\nmax\nf\n2\ns\u2208S,S\u2286S\nf\n\n|S|=\u03bc\u2217\n\nTherefore, in this case we have:\n\nl\n\u2308log \u03bcf \u2309 + \u2308log \u03bc\u2217 \u2309 \u2264 #f \u2264 \u2308log \u03bcf \u2309 + log\n\nVI. S OME P ROPERTIES\n\nOF\n\n\u03bcX1 ,...,XN\n2(1\u2212\u01ebmax )\u2308log \u03bcf \u2309\n\nm\n\n(13)\n\nD ISTRIBUTED F UNCTION C OMPUTATION\n\nWe discuss some of the significant results, properties, and observations based on our work on distributed function computation\nproblem in asymmetric communication scenarios.\nA. Two Classes of Functions\nLet us consider two deterministic functions g = max{X1 , X2 } and h = X1X2 . For two or more data-vectors derived from\nany discrete and finite support-set, the function g may evaluate to same output value. On the other hand, the function h assigns,\nin general, a unique output value to each of its input pairs. Generalizing this to the functions of N, N \u2265 2, variables computed\nover corresponding discrete and and finite support-sets, there are various functions whose behavior is either like function g or\nlike function h above.\nThe common statistical functions, such as 'max', 'min', 'majority, 'mean', 'median', and 'mode' and logical functions, such\nas 'parity', 'bitwise OR', 'bitwise AND', and 'bitwise XOR' belong to a class\nPN of functions, which we call lossy functions.\nSimilarly, the functions such as 'identity function', 'iterated exponentiation', i6=j Xi eXj belong to a class of functions, which\nwe call lossless functions. Formally, for the lossy functions the cardinality of their range is smaller than the cardinality of their\ndomain, while for the lossless functions two cardinalities are equal. In fact, it is easy to prove that the equality of the sizes of\ndomain and range of a function is an equivalence relation and classes of lossy and lossless functions are equivalence classes.\nThe reason these two equivalence classes of functions are relevant in the discussion of distributed function computation is\nthat, in general, the computation of lossy functions at the sink requires fewer number of informant bits than computation of\nlossless functions. As DSC belongs to the equivalence class of lossless functions (DSC is distributed function computation with\nfunction to be computed being the identity map: idX ), this implies that, in general, for a given support-set the computation of\nlossless functions requires as many informant bits, in the worst-case, as the solution of DSC problem, while the computation\nof lossy functions requires fewer number of informant bits than DSC.\nThe bSerfComp protocol of the last section can be used to compute both, the lossy and lossless functions. However, as\nfor the lossless functions, the bSerfComp protocol reduces to much simpler bSerCom protocol of [3] for computing DSC\nin the corresponding communication scenario, the latter can be deployed at the sink for their computation in asymmetric\ncommunication scenarios.\nAlso, for the lossless functions the knowledge of function output allows us to uniquely determine the input data-vector\nrevealed to the informants (reversible function computation), while for the lossy functions this is not possible (irreversible\nfunction computation). This apparent loss of information accompanying the computation of lossy functions results in their\ncomputation with fewer number of informant bits, but at the cost of sacrificing our ability to recover the input data-vector\nfrom their output. For the lossless functions, there is no such information loss in their computation, allowing the unambiguous\nrecovery of the input data-vectors from their output, but at the cost of larger number of informant bits.\nTABLE II\nC OMPARISON OF lossy AND lossless FUNCTION COMPUTATION\nLossy Functions\n1. Examples: various common statistical and bitwise functions\n2. Range of the function is smaller than its domain\n3. Complex bSerfComp protocol is used for computation\n4. The worst-case rate-region is larger than DSC\n5. The sink cannot unambiguously recover the data-vector revealed\nto the informants from function output value\n\nLossless Functions\n1. Examples: DSC, iterated exponentiation\n2. Range of the function is of same size as its domain\n3. Simple bSerCom of [3] is used for computation\n4. The worst-case rate-region coincides with DSC\n5. The sink can unambiguously recover the data-vector revealed\nto the informants from function output value\n\nIt should be noted that above classification of functions holds true for any given support-set of data-vectors, in general.\nHowever, one can always concoct exceptions where the cardinality of the support-set of function output values for some lossy\nfunction is same as the cardinality of the corresponding support-set of data-vectors. Similarly, some exception for lossless\nfunctions can be constructed, where the cardinality of the support-set of function output values is smaller than the cardinality\n\n\fX\n1\nX\n2\n\nX\n1\n1\n\n1\n\n3\n\n2\n\n*\n\n2\n\n5\n\nX\n2\n1\n2\n\n*\n\n*\n*\n*\n\n5\n\n*\n\n3\n4\n\n4\n\n3\n\n*\n\n*\n\n4\n\n*\n\n\u03bc max = 5\n\n\u03bc\n\nDSC\n\n3\n\n2\n\n*\n\n*\n\n1\n\n*\n\n2\n\n*\n*\n\n*\n\n3\n\n2\n\n*\n\n\u03bc\n\nDSC\n\n*\n\n*\n\n(b)\n\n3\n\n2\n\n*\n\n*\n\nDSC\n\n\u03bc max = 5\n\n= 10\n\n*\n\n*\n\n5\n\u03bc\n\n5\n\n*\n\n*\n\n4\n\n*\n\n4\n\n*\n\n3\n\n*\n\n*\n*\n\n1\n\n2\n\n*\n\n\u03bc max = 5\n\n= 10\n\nX\n2\n1\n\n*\n\n4\n\n5\n\n4\n\n*\n\n5\n\n*\n\n(a)\n\n1\n\n3\n\n*\n\n\u03bc max = 5\n\n= 10\n\n4\n\n*\n\n*\n\n5\n\nX\n2\n\n1\n\n5\n\n*\n\nX\n1\n\nX\n1\n\n*\n\n\u03bc\n\nDSC\n\n=9\n\n(d)\n\n(c)\n\nFig. 5. 'max' computation for support-set in (a) requires #f = 4 informant bits, (b) requires #f = 3 informant bits, (c) requires #f = 4 informant bits,\nand (d) requires #f = 3 informant bits\n\nX\n1\nX\n2\n\nX\n1\n1\n\n2\n\n3\n\n4\n\n5\n\n1\n\n*\n\n*\n\n*\n\n*\n\n*\n\n2\n\nX\n2\n\nX\n1\n1\n\n2\n\n3\n\n4\n\n5\n\nX\n2\n\nX\n1\n1\n\n2\n\n3\n\n4\n\n5\n\nX\n2\n\n1\n\n2\n\n3\n\n4\n\n1\n\n*\n\n1\n\n*\n\n1\n\n*\n\n*\n\n2\n\n*\n\n2\n\n*\n\n2\n\n*\n\n3\n\n*\n\n3\n\n*\n\n3\n\n*\n\n3\n\n*\n\n4\n\n*\n\n4\n\n*\n\n4\n\n5\n\n*\n\n*\n\n5\n\n5\n\n\u03bc max = 5\n\n\u03bc\n\nDSC\n\n=9\n\n(a)\n\n*\n\n*\n\n\u03bc max = 1\n\n*\n\n(b)\n\n\u03bc\n\n*\nDSC\n\n=9\n\n*\n\n*\n\n*\n\n\u03bc max = 2\n\n(c)\n\n*\n\u03bc\n\n*\n\n*\nDSC\n\n=9\n\n4\n\n*\n\n5\n\n*\n\n*\n\n*\n\n\u03bc max = 2\n\n*\n\u03bc\n\n5\n\n*\n\n*\n\n*\nDSC\n\n= 11\n\n(d)\n\nFig. 6. 'max' computation for support-set in (a) requires #f = 6 informant bits, (b) requires #f = 0 or no informant bits, (c) requires #f = 2 informant\nbits, and (d) requires #f = 2 informant bits\n\nof corresponding support-set of data-vectors. We state without proof that the number of such instances of support-sets is small\nfor any given cardinality of the support-sets. Further, in all situations the following lemma always holds for any function f .\nLemma 4: If \u2308log \u03bcf \u2309 = \u2308log \u03bcDSC \u2309, then #f = #DSC . Also, if \u2308log \u03bcf \u2309 < \u2308log \u03bcDSC \u2309, then #f \u2264 #DSC .\nProof: Omitted for brevity.\nThis brings us to relating our work on function classification with Han and Kobayashi's work along similar lines, [9]. We\nestablish two equivalence classes of functions: lossy and lossless. Given that DSC problem belongs to the class of lossless\nfunctions, the worst-case achievable rate-region of lossless functions coincides with the worst-case rate-region of DSC problem,\nwhile for lossy functions it is correspondingly larger. In [9] too, the authors have introduced such dichotomy of functions of\ncorrelated sources: for one class of functions the achievable rate-region coincides with Slepian-Wolf rate-region and for another\nclass it does not. However, in spite of apparent similarities in results, there are some basic differences. First, we are interested\nin the worst-case information-theoretic analysis while authors in [9] are concerned with average-case analysis. Second, our\nresults pertain to one-shot function computation, while [9] deploys block-encoding.\nIt is interesting to ask if for a given communication scenario, we can always construct two or more equivalence classes\nof functions based on the communication cost of their computation. This appears to be a largely unexplored problem and a\nsystematic answer that also unifies various previous attempts to classify functions based on their communication costs, as in\n[5], [9], and this paper, warrants our attention. Further, proposed two classes of the functions can be further refined based on\nother finer details of the functions and we actually expect the classification structure to be richer than just the dichotomous\nclassification in the paper. As of now, our own work in these directions is in preliminary stage and we propose to address\nthese issues comprehensively in the near future.\nB. Dependence of #f on \u03bcf and \u03bcDSC\nIn subsection V-C, we established how for a given support-set of data-vectors #f , the minimum number of informant bits\nneeded to compute the function f in the worst-case, depends on \u03bcf , the ambiguity of function output values, and \u03bcX1 ,...,XN ,\nthe ambiguity of data-vectors. Now, let us consider how for a given function f , #f for two different support-set of data-vectors\ndepends on corresponding \u03bcf and \u03bcX1 ,...,XN .\nLet \u03bc1f and \u03bc2f denote the cardinality of the set of function output values for first and second support-set, respectively.\nLet \u03bc1DSC and \u03bc2DSC denote the cardinality of the set of data-vectors for first and second support-set, respectively.\n\n\fFinally, let #1f and #2f denote the minimum number of informant bits required to compute the function f for first and\nsecond support-set, respectively.\nIn this subsection, we state without proof the relation between #1f and #2f , given the relations between \u03bc1f and \u03bc2f , and \u03bc1DSC\nand \u03bc2DSC . We provide an exhaustive list of various possibilities and provide an example for each when the sink computes\nmax{X1 , X2 } over data values of two informants for a given support-set.\nProperty 1: \u03bc1f = \u03bc2f , \u03bc1DSC = \u03bc2DSC : #1f and #2f may or may not be equal, for example the support-sets in Figures 5.(a)-(b)\nfor which #1f 6= #2f . In this case, it is clear that ambiguities corresponding to function output values and data-vectors alone\ncannot be used to establish the relation between #1f and #2f . This deficiency of the notion of ambiguity is addressed in greater\ndetail in one of our related papers, [18].\nProperty 2: \u03bc1f = \u03bc2f , \u03bc1DSC 6= \u03bc2DSC =\u21d2 #1f and #2f follow the ordering of \u03bc1DSC and \u03bc2DSC . An illustration of this\ncase is given by the support-sets in Figures 5.(c)-(d).\nProperty 3: \u03bc1f 6= \u03bc2f , \u03bc1DSC = \u03bc2DSC =\u21d2 #1f and #2f follow the ordering of \u03bc1f and \u03bc2f . Figures 6.(a)-(b) illustrate this.\nProperty 4: \u03bc1f < \u03bc2f , \u03bc1DSC < \u03bc2DSC =\u21d2 #1f \u2264 #2f . Support-sets in Figures 6.(c) and 5.(c) illustrate this.\nProperty 5: \u03bc1f < \u03bc2f , \u03bc1DSC > \u03bc2DSC =\u21d2 #1f \u2264 #2f . Support-sets in figures 6.(d) and 5.(d) illustrate this.\nVII. C ONCLUSIONS\n\nAND\n\nF UTURE W ORK\n\nWe address the distributed function computation problem in asymmetric and interactive communication scenarios, where\nthe sink is interested in computing some deterministic function of input data that is split among N correlated informants and\nis derived from some discrete and finite distribution. We consider the distributed function computation as a generalization of\ndistributed source coding problem. We are mainly interested in computing #f , the minimum number of informant bits required\nin the worst-case, to allow the sink to exactly compute the given function. We provide bSerfComp protocol to optimally compute\nthe functions at sink for any given support-set of data-vectors and prove it computes the worst-case achievable rate-region for\ncomputing any given function, illustrating this with examples. Also, we provide a set of bounds on the performance of the\nproposed protocol.\nWe define two equivalence classes of functions: lossy and lossless. We show that the lossy functions can be computed, in\ngeneral, with fewer number of informant bits than lossless function, such as DSC. Further, we establish the dependence of\n#f , when the function f is computed over two different support-sets, on their respective ambiguities of function output values\nand data-vectors.\nIn future, we want to extend this work in three interesting directions. First, in this paper we have assumed that the sink and\ninformants directly communicate with each other. Allowing the sink and informants to indirectly communicate with each other\nover one or more intermediate nodes (as in multihop networks), offers many more opportunities of reducing the number of bits\ncarried over the network to compute a function at the sink. Second, allowing the sink to tolerate certain amount of error in the\ncomputation of the function may reduce the number of informant bits required. We want to address these directions formally\nin our setup. Finally, we want to come up with a generic framework to classify the functions based on the communication\ncosts of their computation over arbitrary networks with any given model of communication and computation.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n\nE. Kushilevitz and N. Nisan, Communication Complexity, Cambridge Univ. Press, Cambridge, UK, 1997.\nA. C. Yao, \"Some complexity questions related to distributed computing,\" Proc. ACM STOC 1979, Atlanta, GA, April-May 1979.\nS. Agnihotri and H. S. Jamadagni, \"Worst-case asymmetric distributed source coding,\" Proc. Allerton 2008, Monticello, IL, September 2008.\nS. Agnihotri and H. S. Jamadagni, \"Information Ambiguity,\" Proc. ISITA 2008, Auckland, NZ, December 2008.\nA. Giridhar and P. R. Kumar, \"Computing and communicating functions over sensor networks,\" IEEE JSAC, vol. 23, April 2005.\nN. Khude, A. Kumar, and A. Karnik, \"Time and energy complexity of distributed computation of a class of functions in wireless sensor networks,\"\nIEEE Trans. Mob. Comp., vol. 7, May 2008.\nS. Kamath and D. Manjunath, \"On distributed function computation in structure-free random networks,\" Proc. IEEE ISIT 2008, Toronto, Canada, July\n2008.\nD. Slepian and J. K. Wolf, \"Noiseless coding of correlated information sources,\" IEEE Trans. Inform. Theory, vol. IT-19, July 1973.\nT. S. Han and K. Kobayashi, \"A dichotomy of functions F (X, Y ) of correlated sources (X, Y ) from the viewpoint of the achievable rate region,\" IEEE\nTrans. Inform. Theory, vol. IT-33, January 1987.\nR. Gallager, \"Finding parity in a simple broadcast network,\" IEEE Trans. Inform. Theory, vol. IT-34, March 1988.\nL. J. Schulman, \"Coding for interactive communication,\" IEEE Trans. Inform. Theory, vol. IT-42, November 1996.\nA. Orlistky and J. R. Roche, \"Coding for computing,\" IEEE Trans. Inform. Theory, vol. IT-47, March 2001.\nG. J. Klir, Uncertainty and Information: Foundations of Generalized Information Theory, John Wiley & Sons, 2006.\nJ. Chou, D. Petrovic, and K. Ramchandran, \"A distributed and adaptive signal processing approach to exploiting correlation in sensor networks,\" Journal\nof Ad Hoc Networks, vol. 2, October 2004.\nM. Adler, \"Collecting correlated information from a sensor network,\" Proc. SODA 2005, Vancouver, Canada, January 2005.\nA. Orlitsky, \"Worst-case interactive communication I: Two messages are almost optimal,\" IEEE Trans. Inform. Theory, vol. IT-36, September 1990.\nB. W. Kernighan and D. M. Ritchie, The C Programming Language, 2nd ed., Prentice-Hall Inc., 1988.\nS. Agnihotri and V. Rajesh, \"Worst-case compressibility of discrete and finite distributions,\" Available as arXiv:0907.1723v1.\n\n\f"}