{"id": "http://arxiv.org/abs/1010.0842v1", "guidislink": true, "updated": "2010-10-05T11:06:15Z", "updated_parsed": [2010, 10, 5, 11, 6, 15, 1, 278, 0], "published": "2010-10-05T11:06:15Z", "published_parsed": [2010, 10, 5, 11, 6, 15, 1, 278, 0], "title": "Tuning Tempered Transitions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1010.2643%2C1010.1676%2C1010.2168%2C1010.5003%2C1010.6051%2C1010.0955%2C1010.5561%2C1010.1059%2C1010.0725%2C1010.2183%2C1010.5560%2C1010.2944%2C1010.3356%2C1010.2197%2C1010.0842%2C1010.1351%2C1010.2479%2C1010.4566%2C1010.5090%2C1010.5039%2C1010.0184%2C1010.2705%2C1010.0218%2C1010.3749%2C1010.6176%2C1010.3379%2C1010.2359%2C1010.3743%2C1010.3410%2C1010.1112%2C1010.3166%2C1010.4586%2C1010.6049%2C1010.0664%2C1010.2534%2C1010.2809%2C1010.0769%2C1010.3330%2C1010.6090%2C1010.0417%2C1010.5126%2C1010.1036%2C1010.1414%2C1010.2334%2C1010.2852%2C1010.1082%2C1010.4162%2C1010.5633%2C1010.4138%2C1010.5557%2C1010.3904%2C1010.2750%2C1010.4344%2C1010.3162%2C1010.3291%2C1010.1138%2C1010.3841%2C1010.0862%2C1010.4986%2C1010.4743%2C1010.0186%2C1010.5400%2C1010.5967%2C1010.5866%2C1010.3072%2C1010.5232%2C1010.2215%2C1010.3884%2C1010.2077%2C1010.2501%2C1010.0874%2C1010.1882%2C1010.1786%2C1010.5213%2C1010.0024%2C1010.2115%2C1010.1651%2C1010.5567%2C1010.5395%2C1010.3795%2C1010.3025%2C1010.2250%2C1010.4048%2C1010.2952%2C1010.2293%2C1010.0197%2C1010.1037%2C1010.2261%2C1010.5373%2C1010.4509%2C1010.0940%2C1010.0909%2C1010.2813%2C1010.1472%2C1010.3757%2C1010.2223%2C1010.5524%2C1010.2270%2C1010.5236%2C1010.3183%2C1010.3142&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Tuning Tempered Transitions"}, "summary": "The method of tempered transitions was proposed by Neal (1996) for tackling\nthe difficulties arising when using Markov chain Monte Carlo to sample from\nmultimodal distributions. In common with methods such as simulated tempering\nand Metropolis-coupled MCMC, the key idea is to utilise a series of\nsuccessively easier to sample distributions to improve movement around the\nstate space. Tempered transitions does this by incorporating moves through\nthese less modal distributions into the MCMC proposals. Unfortunately the\nimproved movement between modes comes at a high computational cost with a low\nacceptance rate of expensive proposals. We consider how the algorithm may be\ntuned to increase the acceptance rates for a given number of temperatures. We\nfind that the commonly assumed geometric spacing of temperatures is reasonable\nin many but not all applications.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1010.2643%2C1010.1676%2C1010.2168%2C1010.5003%2C1010.6051%2C1010.0955%2C1010.5561%2C1010.1059%2C1010.0725%2C1010.2183%2C1010.5560%2C1010.2944%2C1010.3356%2C1010.2197%2C1010.0842%2C1010.1351%2C1010.2479%2C1010.4566%2C1010.5090%2C1010.5039%2C1010.0184%2C1010.2705%2C1010.0218%2C1010.3749%2C1010.6176%2C1010.3379%2C1010.2359%2C1010.3743%2C1010.3410%2C1010.1112%2C1010.3166%2C1010.4586%2C1010.6049%2C1010.0664%2C1010.2534%2C1010.2809%2C1010.0769%2C1010.3330%2C1010.6090%2C1010.0417%2C1010.5126%2C1010.1036%2C1010.1414%2C1010.2334%2C1010.2852%2C1010.1082%2C1010.4162%2C1010.5633%2C1010.4138%2C1010.5557%2C1010.3904%2C1010.2750%2C1010.4344%2C1010.3162%2C1010.3291%2C1010.1138%2C1010.3841%2C1010.0862%2C1010.4986%2C1010.4743%2C1010.0186%2C1010.5400%2C1010.5967%2C1010.5866%2C1010.3072%2C1010.5232%2C1010.2215%2C1010.3884%2C1010.2077%2C1010.2501%2C1010.0874%2C1010.1882%2C1010.1786%2C1010.5213%2C1010.0024%2C1010.2115%2C1010.1651%2C1010.5567%2C1010.5395%2C1010.3795%2C1010.3025%2C1010.2250%2C1010.4048%2C1010.2952%2C1010.2293%2C1010.0197%2C1010.1037%2C1010.2261%2C1010.5373%2C1010.4509%2C1010.0940%2C1010.0909%2C1010.2813%2C1010.1472%2C1010.3757%2C1010.2223%2C1010.5524%2C1010.2270%2C1010.5236%2C1010.3183%2C1010.3142&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The method of tempered transitions was proposed by Neal (1996) for tackling\nthe difficulties arising when using Markov chain Monte Carlo to sample from\nmultimodal distributions. In common with methods such as simulated tempering\nand Metropolis-coupled MCMC, the key idea is to utilise a series of\nsuccessively easier to sample distributions to improve movement around the\nstate space. Tempered transitions does this by incorporating moves through\nthese less modal distributions into the MCMC proposals. Unfortunately the\nimproved movement between modes comes at a high computational cost with a low\nacceptance rate of expensive proposals. We consider how the algorithm may be\ntuned to increase the acceptance rates for a given number of temperatures. We\nfind that the commonly assumed geometric spacing of temperatures is reasonable\nin many but not all applications."}, "authors": ["Gundula Behrens", "Nial Friel", "Merrilee Hurn"], "author_detail": {"name": "Merrilee Hurn"}, "author": "Merrilee Hurn", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s11222-010-9206-z", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1010.0842v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1010.0842v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "To appear in Statistics and Computing", "arxiv_primary_category": {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1010.0842v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1010.0842v1", "journal_reference": null, "doi": "10.1007/s11222-010-9206-z", "fulltext": "Tuning Tempered Transitions\n\narXiv:1010.0842v1 [stat.CO] 5 Oct 2010\n\nGundula Behrens\u2217, Nial Friel\u2020and Merrilee Hurn\u2021\nOctober 24, 2018\n\nAbstract\nThe method of tempered transitions was proposed by Neal (1996) for tackling the difficulties arising\nwhen using Markov chain Monte Carlo to sample from multimodal distributions. In common with\nmethods such as simulated tempering and Metropolis-coupled MCMC, the key idea is to utilise a series\nof successively easier to sample distributions to improve movement around the state space. Tempered\ntransitions does this by incorporating moves through these less modal distributions into the MCMC\nproposals. Unfortunately the improved movement between modes comes at a high computational cost\nwith a low acceptance rate of expensive proposals. We consider how the algorithm may be tuned to\nincrease the acceptance rates for a given number of temperatures. We find that the commonly assumed\ngeometric spacing of temperatures is reasonable in many but not all applications.\n\nKeywords: Markov Chain Monte Carlo, Multimodality, Tempering, Thermodynamic Integration.\n\n1 Introduction to tempering ideas in MCMC\nIt is well known that standard Markov chain Monte Carlo (MCMC) methods, such as the MetropolisHastings algorithm or the Gibbs sampler, often have difficulties in moving around their target distribution.\nWhen a chain mixes poorly in this way, there is a danger that modes have been missed or that modes are\nnot represented in their right proportions, both of which may lead to bias in the statistical inference. To\novercome such mixing problems, various more sophisticated MCMC methods have been devised based on\na few key ideas. This paper concentrates on one of these key ideas, namely tempering.\nOne way to motivate tempering is to think of using importance sampling to estimate some expectation\nEp0 [h(X)] with respect to the target distribution p0 by sampling from some less modal distribution p1 .\n\u2217\n\nDepartment of Epidemiology and Preventive Medicine, University Hospital Regensburg 93042 Regensburg, Germany;\n\nGundula.Behrens@klinik.uni-regensburg.de\n\u2020\nSchool of Mathematical Sciences, University College Dublin, Belfield, Dublin 4, Republic of Ireland; Email: nial.friel@ucd.ie\n\u2021\nDepartment of Mathematical Sciences, University of Bath, Bath, BA2 7AY, UK; Email: M.A.Hurn@bath.ac.uk\n\n1\n\n\fOne possibility for generating a less modal distribution than p0 on the same support is to \"flatten\" it by\ntaking p1 (x) \u221d p0 (x)\u03b2 , \u2200x, with \u03b2 < 1. As \u03b2 \u2192 0, p1 (x) becomes closer to a uniform distribution\nand consequently becomes more amenable to sampling. For \u03b2 close to 1, there is far less benefit as p1\nmay not be that much less modal than p0 . Unfortunately as the two distributions become far enough apart\nthat the difficulty with modality is overcome, they may also become far enough apart so that many of the\nimportance weights will be very close to zero resulting in unstable estimates of Ep0 [h(X)]. The basis of all\nthe tempering methods is the introduction of a series of distributions bridging the gap between p0 and p1 .\nThe differences between the various approaches is in how these bridging distributions are included. We will\ndescribe various approaches to incorporating bridging distributions using the common form of tempering\nwhich involves powering up all or part of the unnormalised target distribution. The inclusion of other types\nof bridging distribution would also be possible, but the literature has generally restricted itself to this form.\nWe assume that the target distribution can be written as\np(x) \u221d \u03c0(x) exp(\u2212\u03b20 h(x)),\n\n(1)\n\nwhere h(x) may be known as the \"energy\" function and the parameter \u03b20 as the target \"inverse temperature\".\nSince we can write any positive function f (x) in exponential form f (x) = exp(\u2212\u03b20 h(x)) by setting\nh(x) = \u2212 \u03b210 log(f (x)) this class covers a wide range of applications. The tempered distributions are then\ndefined by\npi (x) \u221d \u03c0(x) exp(\u2212\u03b2i h(x)),\n\ni = 0, 1, . . . , n,\n\n(2)\n\nwhere 0 \u2264 \u03b2n \u2264 . . . \u2264 \u03b21 \u2264 \u03b20 , are the inverse temperatures characterising each distribution. The\nflexibility of potentially only tempering part of the target distribution is quite useful. In Bayesian problems\nit may be that one or other of the prior and likelihood contribute to the mixing problems.\nOne of the earlier suggestions for incorporating tempering into MCMC is to run n + 1 Markov chains in\nparallel, each sampling from one of the n + 1 tempered distributions. At each iteration, proposals are made\nto update each chain separately and additionally there is a proposal to swap the x values between chains\nthereby coupling them and giving rise to the name Metropolis-coupled MCMC (Geyer 1991). The state\nspace is the enlarged set of (n + 1) values for x and the target distribution is p0 \u2297 p1 \u2297 . . . \u2297 pn . The idea is\nthat large moves made under pn will filter back down to the lowest level p0 . The normalising constants for\nthe tempered distributions are not needed in this method as they appear only in the acceptance probabilities\nfor the coupling move where they cancel out. However the tempered distributions do need to be close in\norder that the swaps between them are not too infrequent. This may mean that n will have to be large and\nthere are then obvious consequences for storage and computational effort.\nA single chain alternative to Metropolis coupling is simulated tempering (Marinari and Parisi 1992,\n2\n\n\fGeyer and Thompson 1995) which runs a chain on the state space of x augmented by a variable i which takes\nthe values i = 0, 1, . . . , n with probabilities determined by a \"pseudo-prior\". The stationary distribution of\nx|i is pi (x), and updates are either of x|i or i|x with the latter effectively moving up or down the tempering\nsequence. Although again the normalising constants of the tempering distributions are not needed explicitly,\nin practice to get reasonable acceptance rates for the moves between temperatures, the pseudo-prior needs\nto be roughly proportional to these unknown normalising constants.\nTempered transitions (Neal 1996) is another single chain method but without the need to guesstimate the\nrelative normalising constants of the tempered distributions. It uses a deterministically ordered sweep up and\ndown the tempering distributions as a way of generating proposals for the main chain in a way which will be\ndescribed in more detail in the next section. The overwhelming cost of the algorithm is in the construction of\nthe proposals and therefore it is imperative that these are tuned carefully to maximise acceptance rates. Neal\n(1996) finds tempered transitions and simulated tempering to be of roughly equal computational cost. He\nalso compares tempered transitions, simulated tempering and Metropolis-coupled MCMC on other criteria\nsuch as storage requirements and the number of tempering levels required concluding that there is no overall\nwinner and that the choice of method may be problem and goal specific.\nClosely related methods which aim to make fuller use of the sampling at all temperature levels via importance sampling can be found in Neal (2001) and more recently in Gramacy, Samworth and King (2010).\nThe former has links to tempered transitions, effectively using just the second half of the complex proposal\nmechanism. The latter has stronger connections with simulated tempering and Metropolis coupled MCMC\nwhere samples from the different temperatures are stored. Other instances of ideas involving populations of\nsamples can be found in both the population-based MCMC and the Sequential Monte Carlo literature (see\nJasra, Stephens and Holmes (2007) for an overview).\nA common question arising across the algorithms involving tempering is the choice of the bridging distributions given by Equation (2). The general recommendation is to space the \u03b2s geometrically, that is so\nthat \u03b2i /\u03b2i+1 is a constant for all levels i (Neal 1996). Neal formulates this rule by considering sampling\nfrom a multivariate Gaussian using simulated tempering; the geometric spacing attempts to maximise the\nacceptance rates of swaps between neighbouring chains at all levels. Gelman and Meng (1998) also consider choices of bridging distribution, although in the context of the closely related question of estimating\nnormalising constants where they are trying to minimise the Monte Carlo error of path sampling estimates.\nOther work on rationales for choosing the \u03b2s can be found in Iba (2001) and Lefebvre, Steele and Vandal\n(2010). The former reviews the (largely physics) literature, comparing simulated tempering with exchange\nand ensemble Monte Carlo methods and aims to maximise the swapping rates between the bridging distributions using preliminary runs (to satisfy a theoretically derived optimality criterion). The latter is interested\n3\n\n\fin path sampling for estimating normalising constants and the related tuning of the bridging distributions;\nthey derive an expression for the symmetrised Kullback-Leibler divergence between pairs of distributions\nand use the minimisation of this as their criterion.\nIn this paper we consider tempered transitions with bridging distributions of the form given by Equation (2). Of the various tempering schemes, the choice of the {\u03b2i } seems to have been least well addressed\nfor tempered transitions. Our approach is largely computational and tries to answer the question \"For a given\nnumber n of tempering distributions, how best should they be spaced?\". We note that the question of how we\nshould choose n, for fixed computational time, is beyond the scope of this article. The approach we take is\nto use a small number of preliminary short runs to assess whether geometric spacing is likely to be adequate\nand, if not, we propose an optimal way of spacing them. In Section 2 we describe tempered transitions in\ndetail, building on many of the insights offered in Neal's paper. We provide a theoretical analysis which\noutlines when geometric temperature spacing is optimal and give a motivating example where geometric\nspacing is sub-optimal. We also draw some parallels with some of the other theoretical approaches outlined\nabove. In Section 3 we discuss the implementation details of applying our proposed approach to a slowly\nmixing MCMC application.\n\n2 How to tune tempered transitions?\n2.1 The tempered transitions algorithm\nWe begin by describing the algorithmic details of the tempered transitions algorithm and setting up the\nreasoning behind our tuning approach. Suppose the chain is currently in state x, then the algorithm generates\na proposal x\u2032 for the next state using a secondary chain which passes through all the auxiliary distributions\n{pi } first in ascending order of the \u03b2s (\"heating-up\") and then in descending order (\"cooling-down\") back\nto the target distribution p0 . To do this, it uses n pairs of MCMC transition kernels with the ith pair, Ti and\nTi\u2032 satisfying detailed balance with respect to pi\npi (x) Ti (x, x\u2032 ) = pi (x\u2032 ) Ti\u2032 (x\u2032 , x) \u2200x, x\u2032 , i = 1, . . . , n.\nStep 1 Set x0 = x.\nStep 2 Move up and down the tempered distributions using MCMC transitions\nGenerate x1 from x0 using transition kernel T1 .\nGenerate x2 from x1 using transition kernel T2 .\n..\n.\n4\n\n\fGenerate xn from xn\u22121 using transition kernel Tn .\nGenerate x\u2032n\u22121 from xn using transition kernel Tn\u2032 .\n..\n.\nGenerate x\u20321 from x\u20322 using transition kernel T2\u2032 .\nGenerate x\u20320 from x\u20321 using transition kernel T1\u2032 .\nStep 3 Accept x\u2032 = x\u20320 as the next state with probability\n(\n\n\u03b1(x, x\u2032 |x0 , x1 , . . . , xn , x\u2032n\u22121 , . . . , x\u20321 , x\u20320 ) = min 1,\n\n# \"n\u22121\n#)\n\"n\u22121\nY pi (x\u2032 )\nY pi+1 (xi )\ni\ni=0\n\npi (xi )\n\ni=0\n\npi+1 (x\u2032i )\n\n,\n\n(3)\n\notherwise, remain at state x.\nThere is no need for the normalising constants of the tempering distributions as they cancel in the acceptance\nprobability. Neal (1996) demonstrates that the algorithm satisfies detailed balance with respect to the target\np0 . However it is perhaps clear that the proposal is potentially computationally costly and tricky to tune.\nThere are two dependent aspects to the tuning. The first is that the {Ti , Ti\u2032 } should have reasonable\nacceptance rates and, at the later stages of the tempering, be able to make large moves in the state space\n(otherwise this expensive proposal makes little change). This is not quite the usual tuning problem for\nMCMC in that each successive level has a different target distribution. During the first half, these distributions are becoming progressively less modal, while in the second half the reverse is true. Obviously the\ncloser the consecutive distributions, ie the closer the consecutive \u03b2s, the less of an effect this will be.\nSubject to the individual {Ti , Ti\u2032 } working well, the second tuning aspect is that the overall acceptance\nrate of the entire tempered transition proposal should be as high as possible (although notice that if all the\nproposed changes in the tempering are rejected, the overall proposal will be accepted since x0 = x1 = . . . =\nx\u20321 = x\u20320 and so a high acceptance rate here can be slightly misleading if viewed in isolation). To gain more\ninsight into tuning the tempered transition acceptance rate, we follow Neal's lead and rewrite the acceptance\nprobability, Equation (3), using the form of tempering defined by Equation (2).\n\u03b1(x, x\n\n\u2032\n\n|x0 , x1 , . . . , x\u20321 , x\u20320 )\n\n(\n\n= min 1,\n\n# \"n\u22121\n#)\n\"n\u22121\nY exp(\u2212\u03b2i h(x\u2032 ))\nY exp(\u2212\u03b2i+1 h(xi ))\ni\n\nexp(\u2212\u03b2i h(xi ))\n\ni=0\n\n= min 1, exp(\u2212(F \u2032 \u2212 F ))\nwhere F\n\n=\n\nand F \u2032 =\n\nn\u22121\nX\n\n\b\n\ni=0\n\nexp(\u2212\u03b2i+1 h(x\u2032i ))\n\n(4)\n\n(\u03b2i \u2212 \u03b2i+1 )h(xi )\n\ni=0\nn\u22121\nX\n\n(\u03b2i \u2212 \u03b2i+1 )h(x\u2032i ).\n\ni=0\n\nThis expression has an interpretation related to estimating the ratio of normalising constants by thermodynamic integration. Let Z(\u03b2) denote the normalising constant of the distribution defined by Equation (2),\n5\n\n\fwhere for the moment we assume that \u03b2 takes continuous values in the interval [\u03b20 , \u03b2n ].\nZ(\u03b2) =\n\nZ\n\n\u03c0(x) exp(\u2212\u03b2h(x)) dx\n\nx\n\nthen\n\ndZ(\u03b2)\nd\u03b2\n\nd exp(\u2212\u03b2h(x))\ndx\nd\u03b2\nx\nZ\n\u03c0(x) exp(\u2212\u03b2h(x))\nZ(\u03b2) dx\n=\n(\u2212h(x))\nZ(\u03b2)\nx\n= \u2212Z(\u03b2)E\u03b2 [h(X)].\n=\n\nZ\n\n\u03c0(x)\n\n(5)\n\nSolving this differential equation for Z(\u03b2) gives the relation\nlog(Z(\u03b2n )) \u2212 log(Z(\u03b20 )) =\n\nZ\n\n\u03b20\n\n\u03b2n\n\nE\u03b2 [h(X)] d\u03b2.\n\n(6)\n\nThat is, the log of the ratio of the normalising constants at two values of \u03b2 can be expressed as the area under\nthe curve g(\u03b2) = E\u03b2 [h(X)] between them. Recall that the sequences {x0 , . . . xn\u22121 } and {x\u2032n\u22121 , . . . x\u20320 } are\ndrawn such that the xi have target distribution pi , while the x\u2032i have target distribution pi+1 . Figure 1\nillustrates a slightly idealised realisation of F (left) and F \u2032 (right) as the shaded areas constructed as the sum\nof rectangles with widths (\u03b2i \u2212\u03b2i+1 ) and heights h(xi ) (left) or h(x\u2032i ) (right). Both areas are approximations\nof the integral of g(\u03b2) between \u03b2n and \u03b20 . Different realisations of {x0 , x1 , . . . , x\u20320 } will obviously give\nquite different and usually rather messier pictures, with correspondingly quite varied values of F \u2032 \u2212 F .\n(In fact, Figure 1 was constructed using the average of several realisations to reduce this variability for\npresentation purposes.) Those realisations of x0 , x1 , . . . , x\u20321 , x\u20320 where the shaded area on the right (F \u2032 ) is\nsmaller than that on the left (F ) will be accepted since in that case exp(\u2212(F \u2032 \u2212 F )) > 1 in Equation (4).\nThose for which F \u2032 is slightly larger than F may be accepted, but we will almost certainly reject those for\nwhich (F \u2032 \u2212 F ) is large. We take this as a motivation for selecting the {\u03b2i } for fixed n.\n\n2.2 The proposed rationale for choosing {\u03b2i }\nGiven the cost of each tempered transition proposal, our motivation is to increase the number of proposals\naccepted. The value of \u03b20 is fixed by Equation (1), and we assume that the other extreme of the \u03b2s is also\ndetermined, possibly by the fact that it defines a distribution for which direct sampling is possible, certainly\nby the need to move around the state space freely under pn . What remains undetermined are n and the set\n{\u03b21 , . . . , \u03b2n\u22121 }.\nFigure 1 showed the F and F \u2032 associated with a realisation {x0 , x1 , . . . , x\u20320 }. If at each stage the transitions were able to reach their equilibrium distributions in the one step available, then E[h(xi )] = g(\u03b2i )\nand E[h(x\u2032i )] = g(\u03b2i+1 ); Figure 2 shows the corresponding approximations to the integral of g(\u03b2) (this is\nequivalent to Neal's Figure 1(a)). Denote this difference between the areas of these two step functions as a\n6\n\n\fRealisation of F\n3.5\n3.0\n2.5\n2.0\n0.0\n\n0.5\n\n1.0\n\n1.5\n\nh(x )\n\n1.5\n0.0\n\n0.5\n\n1.0\n\nh(x )\n\n2.0\n\n2.5\n\n3.0\n\n3.5\n\nRealisation of F'\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.2\n\n0.4\n\n\u03b2\n\n0.6\n\n0.8\n\n1.0\n\n\u03b2\n\nFigure 1: Two approximations to the integral of g(\u03b2). The breakpoints of the rectangles are given by\n\u03b2n \u2264 . . . \u2264 \u03b21 \u2264 \u03b20 . The shaded area on the left is F =\nF\u2032 =\n\nPn\u22121\ni=0\n\nPn\u22121\ni=0\n\n(\u03b2i \u2212 \u03b2i+1 )h(xi ), while that on the right is\n\n(\u03b2i \u2212 \u03b2i+1 )h(x\u2032i ). The overlaid curve is g(\u03b2) = E\u03b2 [h(X)].\n\nfunction of the \u03b2 values\nSn (\u03b20 , . . . , \u03b2n ) =\n=\n\nn\u22121\nX\n\n(\u03b2i \u2212 \u03b2i+1 )Ei+1 [h(X)] \u2212\n\ni=0\nn\u22121\nX\n\nn\u22121\nX\n\n(\u03b2i \u2212 \u03b2i+1 )Ei [h(X)]\n\ni=0\n\n(\u03b2i \u2212 \u03b2i+1 )g(\u03b2i+1 ) \u2212\n\ni=0\n\nn\u22121\nX\n\n(\u03b2i \u2212 \u03b2i+1 )g(\u03b2i ).\n\n(7)\n\ni=0\n\nSome results are well known for g(\u03b2) = E\u03b2 [h(X)]. Rewriting Equation (2) as\np\u03b2 (x) = \u03c0(x) exp(\u2212\u03b2 h(x) \u2212 K(\u03b2))\n\n(8)\n\nso that K(\u03b2) is the log of the normalising constant Z(\u03b2) and rearranging Equation (5),\n1 dZ(\u03b2)\nZ(\u03b2) d\u03b2\nd log(Z(\u03b2))\n= \u2212\nd\u03b2\n\u2032\n= \u2212K (\u03b2)\n\ng(\u03b2) = \u2212\n\nand g\u2032 (\u03b2) =\n\nZ\n\nh(x)\n\nd\np\u03b2 (x)dx\nd\u03b2\n7\n\n(9)\n\n\fExpected h(X) values\n3.5\n3.0\n2.5\n2.0\n0.0\n\n0.5\n\n1.0\n\n1.5\n\ng(\u03b2)\n\n1.5\n0.0\n\n0.5\n\n1.0\n\ng(\u03b2)\n\n2.0\n\n2.5\n\n3.0\n\n3.5\n\nExpected h(X') values\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.2\n\n0.4\n\n\u03b2\n\n0.6\n\n0.8\n\n1.0\n\n\u03b2\n\nFigure 2: The shaded area on the left is\n\nPn\u22121\ni=0\n\n(\u03b2i \u2212 \u03b2i+1 )g(\u03b2i ), while that on the right is\n\n\u03b2i+1 )g(\u03b2i+1 ). The difference between the two areas is Sn (\u03b20 , . . . , \u03b2n ).\n\n=\n=\n\nZ\n\nZ\n\nPn\u22121\ni=0\n\n(\u03b2i \u2212\n\nh(x)(\u2212h(x) \u2212 K \u2032 (\u03b2))\u03c0(x) exp(\u2212\u03b2 h(x) \u2212 K(\u03b2))dx\n(\u2212h(x)2 + h(x)g(\u03b2))p\u03b2 (x)dx\n\n= \u2212Var\u03b2 [h(X)] .\n\n(10)\n\nTherefore g\u2032 (\u03b2) < 0, for all \u03b2, showing that g(\u03b2) is a decreasing function of \u03b2. It is possible to examine\ng\u2032\u2032 (\u03b2) similarly, showing that the curve may be convex, concave, or a mixture of the two. The main point\nhere is that because g(\u03b2) is decreasing, we know that Sn (\u03b20 , . . . , \u03b2n ) \u2265 0.\nWe propose the minimisation of Sn (\u03b20 , . . . , \u03b2n ) over {\u03b2i } as our rule for choosing the tempered transition parameters. Obviously increasing n immediately reduces Sn . However our primary motivation here is\nthe most effective choice of the particular {\u03b21 , . . . , \u03b2n\u22121 for a fixed number of levels n and fixed values of\n\u03b20 and \u03b2n .\nNote that minimising Sn = E[F \u2032 \u2212 F ] is not directly equivalent to maximising the expected value of the\nacceptance probability, \u03b1 = min{1, exp(\u2212(F \u2032 \u2212 F ))}, however\nE[exp(\u2212(F \u2032 \u2212 F ))] = 1 \u2212 Sn +\n\nE[(F \u2032 \u2212 F )2 ]\n\u2212 ...\n2!\n\n(11)\n\nand so intuitively minimising Sn seems a reasonable start. Other possible criteria include, for example,\n8\n\n\fmaximising P(F \u2032 < F ) over the {\u03b2i } or, as suggested by one of the referees, examining the variance as\nwell as the expectation of F \u2032 \u2212 F since a high variance could perhaps improve mixing by generating big\nmoves more often than a low variance might. We have so far only considered looking at Sn .\n\n2.3 A motivating example\nTo motivate the tuning procedure, we study the one-dimensional two-parameter simplified Witch's Hat\ndistribution used by Geyer and Thompson (1995). Although this is quite a straightforward example, we\nshall see that it is one for which geometric spacing of the temperatures is not optimal. Geyer and Thompson\nattribute this distribution to Matthews (1993) who introduced it as a problem case for the Gibbs sampler:\np(x) \u221d 1 + bI[x\u2264a] , 0 \u2264 x \u2264 1\nwhere the parameters satisfy 0 < a < 1 and b \u2265 0. This apparently innocuous L-shaped distribution causes\nproblems for standard Metropolis-Hastings moves if a is small but b is large as it can be difficult to move\nbetween the intervals [0, a] and (a, 1]. The distribution can be expressed in a form suitable for tempering\npi (x) \u221d exp(\u03b2i log(1 + bI[x\u2264a] )), i = 0, . . . , n\nwhere \u03b20 = 1. In this case, g(\u03b2) = E\u03b2 [\u2212 log(1 + bI[x\u2264a] )] is available analytically as are its derivatives:\ng(\u03b2) =\ng\u2032 (\u03b2) =\ng \u2032\u2032 (\u03b2) =\n\n\u2212a(1 + b)\u03b2 log(1 + b)\na(1 + b)\u03b2 + (1 \u2212 a)\na(a \u2212 1)(1 + b)\u03b2 (log(1 + b))2\n(a(1 + b)\u03b2 + 1 \u2212 a)2\n\u2212a(a \u2212 1)(1 + b)\u03b2 (log(1 + b))3 \u0010\n(a(1 + b)\u03b2 + (1 \u2212 a))\n\n3\n\n\u0011\n\na(1 + b)\u03b2 \u2212 (1 \u2212 a)\n\n(12)\n\nThe second derivative shows that for \u03b2 \u2208 [0, 1], the curve g(\u03b2) may be convex (if a \u2208 [0.5, 1] and b \u2265 0),\nconcave (if a \u2208 (0, 0.5) and b \u2208 (0, 1/a \u2212 2]), or a mix of the two (otherwise). We propose to study one\ndistribution for which g(\u03b2) is convex (taking a = 0.5 and b = 7.5 \u00d7 108 ) and one for which it is concave\n(taking a = 10\u22124 and b = 9.5 \u00d7 103 ). The latter distribution is hard to sample with roughly half the mass\nconcentrated in the narrow [0, 9.5\u00d7 103 ] peak. The former distribution does not present a sampling problem,\nbut we are still interested in the effect of the shape of g(\u03b2) on tempering performance.\nGiven g(\u03b2), \u03b20 , \u03b2n and n, how do we minimise Sn over {\u03b21 , . . . , \u03b2n\u22121 }, Equation (7)? The (n \u2212 1)\npartial derivatives are available,\n\u2202Sn\n= (g(\u03b2i\u22121 ) \u2212 2g(\u03b2i ) + g(\u03b2i+1 )) + (\u03b2i\u22121 \u2212 2\u03b2i + \u03b2i+1 )g\u2032 (\u03b2i ), i = 1, . . . , n \u2212 1,\n\u2202\u03b2i\n\n(13)\n\nhowever, despite their relatively simple form, no analytic solution is readily available for the minimisation\nproblem. As a result, we perform the minimisation numerically using the built-in quasi-Newton optim with\n9\n\n\foption L-BFGS-B in R incorporating derivative information and the constraints that \u03b2n < \u03b2i < \u03b20 for\ni = 1, . . . , n \u2212 1 and fixed \u03b20 and \u03b2n . This function is not guaranteed to converge to a global maximum;\nin our experiments it was insensitive to starting points (we used geometric or uniform spacings) with the\nexception of occasional catastrophic convergence for large n to a non-ordered set of \u03b2s. In all cases we\nencountered, changing from geometric to uniform initial spacings, or vice versa, resolved this problem.\nFigure 3 illustrates the minimal Sn and the corresponding {\u03b2i } for the two examples of the Witch's hat\ndistribution when n = 4 and \u03b2n = 1/16, as well as the equivalent Sn for a geometric spacing of the {\u03b2i }.\nUnsurprisingly given the different shapes of the two curves, the change in the size of Sn achieved by the\noptimal scheme over the geometric one is more significant for the concave g(\u03b2) curve. However, even\nin the convex example it is clear that the values of the \u03b2s themselves, particularly \u03b21 , are quite different\nunder geometric spacing and the minimal Sn scheme. Table 1 shows the minimum values of Sn and the\ngeometric values of Sn for n = 2, 4, 8, 16, 32 and 64 and for both pairs of parameters a and b. For each\nn we performed 500000 iterations of tempered transitions where at each level i = 1, . . . , n we use direct\nsampling (by inversion) to draw from the tempered distribution pi . This direct sampling is only realistically\npossible, at least for values of \u03b2 close to \u03b20 , in a test example such as this, but it does allow us to separate\nthe effects of the different \u03b2 choices from the effects of slow mixing of the transitions at levels 1 to n. Table\n1 gives observed average acceptance rates together with the estimated integrated autocorrelation time of the\ntempering calculated with respect to the known theoretical mean. The Witch's Hat example is unusual in\nthat only moves between the regions 0 \u2264 x \u2264 a and a < x \u2264 1 are problematic while all within-region\nmoves are always accepted (giving rise to unusually high acceptance rates for a tempering problem). In\naddition, a high acceptance rate in tempered transitions can actually mask a lack of mixing and so integrated\nautocorrelation times are a useful diagnostic.\nFor both distributions, decreasing Sn by optimising the {\u03b2i }, increases the observed acceptance rate and\ndecreases the integrated autocorrelation time. These improvements are most noticeable when comparing the\ngeometric and optimal schemes for the hard sampling problem, a = 10\u22124 and b = 9.5 \u00d7 103 , where the\nchanges in Sn are most dramatic. Concentrating on n = 4 to tie in with Figure 3, for the easier convex\ng(\u03b2), optimising the {\u03b2i } made a small difference to the overall Sn albeit with noticeable changes to the\n{\u03b2i } themselves. Here the tuning has made only marginal improvements in acceptance rates and integrated\nautocorrelation times (although as noted earlier, this distribution is not hard to sample and there is little\nscope for improvement anyway). In the harder sampling problem, where g(\u03b2) was concave, the benefits of\ntuning the {\u03b2i } are very clear. In this example, the additional computational cost of tuning comes only from\nthe R optimisation stage. The benefits of tuning are greatest when n is small (as n increases, the geometric\nSn anyway decreases to zero).\n10\n\n\fg ( \u03b2)\n0.0\n\n0.4\n\n0.8\n\n0.0\n\n0.4\n\n0.8\n\na=10\u22124, b=9.5 \u00d7 103\n\na=10\u22124, b=9.5 \u00d7 103\n\ng(\u03b2)\n0.0\n\n0.4\n\n\u22124 \u22123 \u22122 \u22121\n\n0\n\n\u03b2optimal\n\n0\n\n\u03b2geometric\n\n\u22124 \u22123 \u22122 \u22121\n\ng(\u03b2)\n\n\u221220 \u221219 \u221218 \u221217 \u221216\n\na=0.5, b=7.5 \u00d7 108\n\n\u221220 \u221219 \u221218 \u221217 \u221216\n\ng ( \u03b2)\n\na=0.5, b=7.5 \u00d7 108\n\n0.8\n\n0.0\n\n\u03b2geometric\n\n0.4\n\n0.8\n\n\u03b2optimal\n\nFigure 3: Sn for the Witch's hat distribution when n = 4 using geometric {\u03b2i } spacing (left) or the optimal\n{\u03b2i } (right) overlaid by g(\u03b2) in black; on the top row, a = 0.5 and b = 7.5 \u00d7 108 , on the bottom row\na = 10\u22124 and b = 9.5 \u00d7 103 .\n\n2.3.1\n\nWhen is the geometric temperature placement optimal?\n\nAn interesting question raised by this example is under what circumstances will tuning of the {\u03b2i } be likely\nto make efficiency gains over the default geometric spacing for fixed n? Suppose the target distribution is\nthe d-dimensional multivariate Gaussian with mean \u03bc and variance \u03a3. Then, h(x) = 12 (x \u2212 \u03bc)T \u03a3\u22121 (x \u2212 \u03bc)\nand the tempered distributions are d-dimensional multivariate Gaussian with mean \u03bc and variance \u03b2i\u22121 \u03a3.\nMore importantly, g(\u03b2) =\nthe partial derivatives\n\n\u2202Sn\n\u2202\u03b2i\n\nd\n2\u03b2\n\nand g\u2032 (\u03b2) =\n\n\u2212d\n2\u03b2 2 .\n\nAs a result, when the {\u03b2i } are geometrically spaced, all\n\n= 0 in Equation (13), and so geometric spacing is also the optimal minimum Sn\n\nspacing. In fact we can go further: suppose the set of\n\n\u2202Sn\n\u2202\u03b2i\n\n11\n\nare all zero for a general g and for all n when the\n\n\fn=2\n\nn=4\n\nn=8\n\nn = 16\n\nn = 32\n\nn = 64\n\nOptimal\na = 0.5\n\nSn\n\n0.83386\n\n0.30241\n\n0.13214\n\n0.06218\n\n0.03023\n\n0.01492\n\nb = 7.5 \u00d7 108\n\n\u03b1\n\n0.78\n\n0.80\n\n0.84\n\n0.87\n\n0.91\n\n0.93\n\n(convex)\n\n\u03c4\n\n1.55\n\n1.48\n\n1.38\n\n1.28\n\n1.20\n\n1.14\n\nSn\n\n0.90444\n\n0.38612\n\n0.18454\n\n0.09122\n\n0.04548\n\n0.02272\n\n\u03b1\n\n0.78\n\n0.79\n\n0.82\n\n0.85\n\n0.89\n\n0.89\n\n\u03c4\n\n1.58\n\n1.51\n\n1.46\n\n1.36\n\n1.26\n\n1.26\n\nGeometric\n\nOptimal\na = 10\u22124\n\nSn\n\n1.46627\n\n0.63456\n\n0.29879\n\n0.14591\n\n0.07234\n\n0.03607\n\nb = 9.5 \u00d7 103\n\n\u03b1\n\n0.55\n\n0.63\n\n0.72\n\n0.80\n\n0.85\n\n0.90\n\n(concave)\n\n\u03c4\n\n7.05\n\n2.36\n\n1.75\n\n1.47\n\n1.33\n\n1.22\n\nSn\n\n3.34158\n\n2.20779\n\n1.25229\n\n0.64996\n\n0.32786\n\n0.16428\n\n\u03b1\n\n0.51\n\n0.51\n\n0.55\n\n0.61\n\n0.69\n\n0.78\n\n\u03c4\n\n591.36\n\n55.56\n\n9.13\n\n3.11\n\n1.91\n\n1.54\n\nGeometric\n\nTable 1: Results for the Witch's Hat problem under two settings of the parameters a and b and multiple\nchoices of n, the number of tempering levels. The minimal sum of squares, observed acceptance rates and\nestimated integrated autocorrelation times are shown for the geometric scheme and for the optimal scheme.\n\n{\u03b2i } are geometrically spaced, i.e. when\ng \u2032 (\u03b2i ) = \u2212\n\ng\n\n\u0010\n\n\u03b2i\ncn\n\n\u03b2i+1\n\u03b2i\n\n\u0011\n\n= cn where cn =\n\n\u2212 2g(\u03b2i ) + g(cn \u03b2i )\n\n\u03b2i\ncn\n\n\u2212 2\u03b2i + cn \u03b2i\n\n\u0010\n\n\u0011\n\u03b2n 1/n\n,\n\u03b20\n\n\u03b2n 6= 0, then\n\n, i = 1, . . . , n \u2212 1.\n\n(14)\n\nAs n \u2192 \u221e with fixed \u03b20 and \u03b2n , cn \u2192 1 and a repeated application of L'h\u00f4pital's rule yields the equation\n\u03b2g\u2032 (\u03b2) = \u2212(\u03b2 2 g\u2032\u2032 (\u03b2) + \u03b2g \u2032 (\u03b2))\nwhich has general solution g(\u03b2) =\n\nK1\n\u03b2\n\n(15)\n\n+ K2 for constants K1 and K2 . In other words, geometric spacing\n\nonly minimises Sn if the target distribution has this form of g(\u03b2). This is a wider class than just the Gaussian,\nfor example the exponential distribution has g(\u03b2) = \u03b21 . The result ties in with Figure 3 and Table 1.\n\n12\n\n\f2.4 An alternative perspective on the optimisation problem\nIn this section we provide an intuitive formalisation of what quantity Sn in Equation (7) represents. Since\npi (x) = \u03c0(x) exp(\u2212\u03b2i h(x))/Z(\u03b2i ), it follows that\nZ(\u03b2i+1 )\nZ(\u03b2i )\n\nexp(\u2212\u03b2i+1 h(x))\npi (x)\nexp(\u2212\u03b2i h(x))\npi+1 (x)\npi (x)\n= exp(\u2212(\u03b2i+1 \u2212 \u03b2i )h(x))\npi+1 (x)\n=\n\nTaking logarithms,\nZ(\u03b2i+1 )\nlog\nZ(\u03b2i )\n\u0012\n\npi+1 (x)\n= (\u03b2i \u2212 \u03b2i+1 )h(x) \u2212 log\n.\npi (x)\n\n\u0013\n\n\u0012\n\n\u0013\n\n(16)\n\nMultiplying both sides of the equation by pi+1 (x) and integrating with respect to pi+1 (x) leads to\nlog\nwhere KL[pi+1 , pi ] =\n\nR\n\n\u0012\n\nZ(\u03b2i+1 )\nZ(\u03b2i )\n\n\u0013\n\nx pi+1 (x) log\n\n= (\u03b2i \u2212 \u03b2i+1 )Ei+1 [h(X)] \u2212 KL [pi+1 , pi ] ,\n\n\u0010\n\npi+1 (x)\npi (x)\n\n\u0011\n\n(17)\n\n, is the Kullback-Leibler divergence between distributions\n\npi+1 and pi . Similarly, it can be shown, by multiplying both sides of Equation (16) by pi (x) and integrating\nwith respect to x, that\nZ(\u03b2i+1 )\nlog\nZ(\u03b2i )\n\u0012\n\n\u0013\n\n= (\u03b2i \u2212 \u03b2i+1 )Ei [h(X)] + KL[pi , pi+1 ].\n\n(18)\n\nSumming both Equations (17) and (18) over i indices leads to\nlog\n\n\u0012\n\nZ(\u03b2n )\nZ(\u03b20 )\n\n\u0013\n\n=\n\nX\n\n(\u03b2i \u2212 \u03b2i+1 )Ei+1 [h(X)] \u2212\n\ni\n\n=\n\nX\n\nX\n\nKL[pi+1 , pi ]\n\ni\n\n(\u03b2i \u2212 \u03b2i+1 )Ei [h(X)] +\n\ni\n\nX\n\nKL[pi , pi+1 ].\n\ni\n\nIt now follows directly that\nX\n1 n\u22121\n{KL[pi+1 , pi ] + KL[pi , pi+1 ]}\nSn (\u03b20 , . . . , \u03b2n ) =\n2 i=0\n\nThus our optimisation problem can be recast as one of finding temperatures {\u03b21 , . . . , \u03b2n\u22121 } to minimise\nthe sum of the symmetrised Kullback-Leibler distances between successive distributions pi and pi+1 . This\ninterpretation ties in with the recent work by Lefebvre, Steele and Vandal (2010) who consider this same\nsymmetrised Kullback-Leibler divergence in picking optimal schemes for path sampling. A similar perspective, but in the context of marginal likelihood estimation using the power posterior method of Friel and\nPettitt (2008), appears in Section 3.2 of that paper and also in Calderhead and Girolami (2009).\n\n13\n\n\f3 Application of the tuning to a non-toy problem\n3.1 A Bayesian mixture problem\nWe now turn to Bayesian mixture modelling, an application where tempered transitions has been advocated\nin the past as a possible solution to sampling problems (see, for example, Celeux, Hurn and Robert (2000)\nand Jasra, Holmes and Stephens (2005)). The benchmark for good MCMC mixing here is label-switching:\nIn the Bayesian treatment of a k\u2212component mixture model, the likelihood is invariant to the labelling\nof the components. This invariance is inherited by the posterior if, as is quite natural in many cases, the\npriors do not impose identifiability. The logical conclusion of such invariance is that a well-mixing MCMC\nsampler should visit all k! labellings of the components. Label-switching could be achieved trivially by\nincorporating a move type which permutes the component labels, however this may mask more significant\ndifficulties in moving around the state space. Certainly we can have greater confidence in the exploratory\npowers of a sampler which can swap component labels in the course of its other moves.\nWe use the much-studied galaxy data set for illustration, see for example Richardson and Green (1997),\nwhich comprises measurements on the velocities of 82 galaxies (Figure 4). Unlike in that paper though,\nwe fix the number of mixture components at three, the smallest number of components with non-negligible\nposterior probability according to Richardson and Green. Using a small number of components makes label\nswitching harder as there is no \"redundant\" component to move freely around the state space exchanging\nidentities with the less mobile components needed to explain the data if they pass sufficiently close.\nDenoting the 82 velocity measurements by y = {y1 , . . . , y82 }, we follow Richardson and Green (1997)\nin incorporating corresponding latent allocation variables z = {z1 , . . . , z82 }. Given zi = j, yi follows the\nj th of the three component Gaussian distributions of the mixture,\n1\n\nexp\nf (yi |zi = j, \u03bcj , \u03c3j2 ) = q\n2\u03c0\u03c3j2\n\n\u2212(yi \u2212 \u03bcj )2\n2\u03c3j2\n\n!\n\ni = 1, . . . , 82.\n\nFurther, conditional independence is assumed for the observations. We specify largely independent standard\nproper priors:\nP(zi = j) = wj , where\n\n3\nX\n\nwj = 1\n\nj=1\n\n{w1 , w2 , w3 } \u223c Dirichlet(1, 1, 1)\n\u03bcj\n\n\u223c N (0, 1000), j = 1, 2, 3\n\n\u03c3j2 \u223c InvGam(1, 1), j = 1, 2, 3.\n\n14\n\n\f0.10\n0.00\n\n0.05\n\nDensity\n\n0.15\n\n0.20\n\nHistogram of Galaxy Data\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\nVelocities in 1000 km/s\n\nFigure 4: The galaxy data used for as illustration of the mixture modelling.\n\nso that the posterior of interest is\nf (z, {wj , \u03bcj , \u03c3j2 }3j=1 |y) \u221d\n\n82\nY\n\nf (yi |zi , \u03bczi , \u03c3z2i )\u00d7 f ({wj })\u00d7\n\ni=1\n\n3\nY\n\nj=1\n\nf (\u03bcj )\u00d7\n\n3\nY\n\nf (\u03c3j2 )\u00d7\n\nj=1\n\n82\nY\n\nf (zi |{wj }) (19)\n\ni=1\n\nWe know that if label switching is taking place when sampling from Equation (19) that the marginal\nposterior distributions for the sets of parameters of the three Gaussian components should be identical.\nFigure 5 shows the output for the {\u03bcj } parameters using 100000 iterations of standard MCMC updates\nincluding a burn in of 10000 iterations (Gibbs updates for {wj }, {\u03bcj }, {\u03c3j2 } and a uniform Metropolis\nproposal to change the {zi } in turn); it is clear that label switching is not happening. Tempering the whole of\nthe posterior defined by Equation (19) is problematic as there is no guarantee that the tempered distributions\nwill remain proper. Instead, we follow Celeux, Hurn and Robert (2000) in tempering only the likelihood\ncontribution leaving the priors untempered. This approach generates proper tempered distributions provided\nthe priors are proper. In the notation of previous sections, we set \u03b20 = 1 and \u03b2n = 1/16 while\n\nh(x) =\n\n\uf8f6\n\n\uf8eb\n\n82\n3 \uf8ec\nX\nX\n\uf8f7\n\uf8ec nj log(\u03c3 2 ) + 1\n(yi \u2212 \u03bcj )2 \uf8f7\nj\n\uf8f8\n\uf8ed2\n2\n2\u03c3\nj\n\nj=1\n\nwhere x = {z, {wj , \u03bcj , \u03c3j2 }3j=1 } and nj =\n\ni=1\n\nzi =j\n\nP82\n\ni=1 I[zi =j] ,\n\nj = 1, 2, 3. Unlike in the motivating example\n\nof the previous section, the g(\u03b2) corresponding to this form of h(x) is not available analytically and so we\nmust now address the question of approximating it before we can optimise the {\u03b2i }.\n15\n\n\f10 20 30 40\n0\n\n\u03bc1\n\n0.0 0.4 0.8 1.2\n\nDensity\n\nTrace plot\n\n0\n\n10\n\n20\n\n30\n\n40\n\n0\n\n\u03bc1\n\n20000\n\n40000\n\n60000\n\n80000\n\nIterations after burn in\n\n0\n\n10 20 30 40\n\n\u03bc2\n\n0.15\n0.00\n\nDensity\n\n0.30\n\nTrace plot\n\n0\n\n10\n\n20\n\n30\n\n40\n\n0\n\n\u03bc2\n\n20000\n\n40000\n\n60000\n\n80000\n\nIterations after burn in\n\n10 20 30 40\n0\n\n\u03bc3\n\n0.0 0.4 0.8 1.2\n\nDensity\n\nTrace plot\n\n0\n\n10\n\n20\n\n30\n\n40\n\n0\n\n\u03bc3\n\n20000\n\n40000\n\n60000\n\n80000\n\nIterations after burn in\n\nFigure 5: Histograms and trace plots of the {\u03bcj } chains indicating a lack of mode swapping when \u03b2 = 1.\n\n3.2 Approximating g(\u03b2)\nThe difficulty in estimating g(\u03b2) = E\u03b2 [h(X)] and g \u2032 (\u03b2) = \u2212(E\u03b2 [h(X)2 ] \u2212 E\u03b2 [h(X)]2 ) for \u03b2n \u2264 \u03b2 \u2264 \u03b20\nis that sampling under p\u03b2 is difficult for \u03b2 close to \u03b20 (hence the need for tempered transitions!). We\npropose instead to estimate g(\u03b2) and g \u2032 (\u03b2) using importance sampling. The obvious importance distribution\nto use is p\u03b2n since we have already made an assumption that we can sample from this distribution quite\nfreely. However it may be a poor choice as an importance distribution for p\u03b2 when \u03b2 is close to \u03b20 because\nwhen the importance distribution is quite far from the target, the resulting estimates can be dominated\nby a handful of the samples (Robert and Casella 1999). As a compromise, we importance sample for\nexpectations under p\u03b2 by sampling under p\u03b2\u0303 for some \u03b2\u0303 where \u03b2n \u2264 \u03b2\u0303 < \u03b2 \u2264 \u03b20 , in which case the\nunnormalised importance weights are exp(\u2212(\u03b2\u0303 \u2212 \u03b2)h(x)). We note in passing that a standard result states\nthat the importance distribution which minimises the variance of the importance estimate of some function\n\n16\n\n\f\u03c8(x) is\nf \u2217 (x) \u221d |\u03c8(x)| p\u03b2 (x).\nWe turn this statement around to ask for what function \u03c8(x) is p\u03b2\u0303 (x) the optimal importance distribution?\n|\u03c8(x)| \u221d\n\np\u03b2\u0303 (x)\np\u03b2 (x)\n\n= exp(\u2212(\u03b2\u0303 \u2212 \u03b2)h(x))\n= 1 \u2212 (\u03b2\u0303 \u2212 \u03b2)h(x) + ((\u03b2\u0303 \u2212 \u03b2)h(x))2 /2 + . . .\nSo using p \u0303\u03b2 as an importance distribution would be optimal if we were trying to estimate exp(\u2212(\u03b2\u0303 \u2212\n\u03b2)h(x)). It is not optimal for estimating E\u03b2 [h(X)] and E\u03b2 [h(X)2 ], however it may be more reasonable for\nthis goal when (\u03b2\u0303 \u2212 \u03b2) is small, than if we were, say, trying to estimate E\u03b2 [X] or E\u03b2 [X 2 ].\nWe work with 20 uniformly spaced values of \u03b2\u0303 in the interval [\u03b2n , \u03b20 ]. As a compromise between the\ninadequate sampling for large \u03b2 and the risk of unreliable importance sampling for large \u03b2 \u2212 \u03b2\u0303, we generate\nrelatively small samples at each \u03b2\u0303 and use these samples to estimate g(\u03b2\u0303) and g\u2032 (\u03b2\u0303) both directly and indirectly by importance sampling using the next smallest of the 20 chosen values (with obvious modifications\nat the end points). Figure 6 shows the results when using 10000 samples at each \u03b2\u0303 and discarding the first\n1000 iterations as burn in. We propose to use the average of the two estimates for g(\u03b2\u0303) and g \u2032 (\u03b2\u0303) at each\npoint, with visual inspection recommended to check for major discrepancies. In this example, the estimated\ng(\u03b2) curve appears quite far from the geometric-friendly form g(\u03b2) =\n\nK1\n\u03b2\n\n+ K2 .\n\n3.3 Results\nGiven the importance sampling estimates of both curve g(\u03b2) and its derivative g\u2032 (\u03b2), we can minimise\nSn (\u03b20 , . . . , \u03b2n ) using Equation (13). As before, we use the R optimisation routine optim with linear interpolation used to evaluate g and g\u2032 between the 20 \u03b2\u0303 values. In order to assess the effects of the imperfect\nestimation of g(\u03b2) on the tuning procedure, we replicate the estimation process five times with each replicate\nbeing used to select {\u03b2i }. Figure 7 shows both the variability in estimated g and g \u2032 and how the optimised Sn\ndecreases with n for the five sets of estimates. By letting n become sufficiently large, it would be possible to\nreduce Sn below any positive threshold. (An upper bound on the minimum Sn is n1 (\u03b20 \u2212\u03b2n )(g(\u03b2n )\u2212g(\u03b20 )),\nachieved either by uniformly spacing \u03b21 , . . . , \u03b2n\u22121 or by uniformly spacing g(\u03b21 ), . . . , g(\u03b2n\u22121 ).) However\nas the computational cost of the tempering increases linearly in n, the curves show that costs grow quite\nrapidly for relatively small decreases in Sn .\nTurning to the tempered transitions themselves, we ran the algorithm for 100000 iterations including a\nburn-in of 10000 iterations. At each tempering stage, the same proposal types were used as in the importance\n17\n\n\f180\n160\n140\n100\n\n120\n\nEstimated g(\u03b2)\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n\u03b2\n\nFigure 6: Estimating g(\u03b2) and g\u2032 (\u03b2): Symbols indicate the \u03b2\u0303 at which samples are generated; red circles\nand red lines segments indicate direct sampling; blue triangles and blue line segments indicate importance\nsampling estimates; black lines are linear interpolations.\n\nsampling. For each of the five sets of importance estimates, we temper using n = 64, 128, 256, 512 and\ncompare the optimised \u03b2 results with those of geometric spacing. We know that if switching is taking place,\nthen the marginals for each component should be identical. This obviously implies that the three posterior\nexpected \u03bci should be equal, as should the expected weights and the expected variances. We propose to\nmonitor the mixing using the usual tool of the integrated autocorrelation times, however in estimating these\ndiagnostics we use the averages of each group of parameters over the three chains rather than the usual\nchain-wise average for each parameter. For example, if the labels swap regularly, the individual averages\nof each of the three \u03bci chains will be close to their overall average, and the non-centred autocorrelations\nwill not be much different from the standard centred ones. On the other hand, if the labels do not switch as\nwas the case with standard MCMC illustrated in Figure 5, the autocorrelations calculated around the overall\naverage will be greatly increased and this will be reflected in the modified integrated autocorrelation times.\nTable 2 summarises the results, showing the acceptance rates and the estimated integrated autocorrela18\n\n\fn = 64\n\nn = 128\n\nn = 256\n\nn = 512\n\n\u03b1=0.00013\n\n\u03b1=0.00065\n\n\u03b1=0.00187\n\n\u03b1=0.00853\n\n\u03c4\u0302 ({wj })\n\n***, ***, ***\n\n3253, 22910, 95995\n\n1649, 1382, 1916\n\n256, 249, 257\n\n\u03c4\u0302 ({\u03bcj })\n\n96042, ***, ***\n\n4505, 6938, 4552\n\n1003, 1457, 745\n\n322, 322, 269\n\n\u03c4\u0302 ({\u03c3j2 })\n\n***, 4970 5188\n\n477, 768, 1137\n\n710, 475, 799\n\n140, 132, 163\n\nTuned 1\n\n\u03b1=0.00062\n\n\u03b1=0.00316\n\n\u03b1=0.01366\n\n\u03b1=0.04105\n\n\u03c4\u0302 ({wj })\n\n9089, 4750, 5743\n\n1137, 721, 1117\n\n161, 148, 161\n\n50, 51, 45\n\n\u03c4\u0302 ({\u03bcj })\n\n6406, 3842, 2702\n\n781, 762, 952\n\n134, 165, 160\n\n45, 48, 49\n\n\u03c4\u0302 ({\u03c3j2 })\n\n3071, 1323, 5498\n\n321, 113, 441\n\n78, 88, 90\n\n31, 33, 28\n\nTuned 2\n\n\u03b1=0.00054\n\n\u03b1=0.00346\n\n\u03b1=0.01426\n\n\u03b1=0.04194\n\n\u03c4\u0302 ({wj })\n\n5481, 10701, 17267\n\n642, 1357, 952\n\n134, 161, 148\n\n51, 45, 49\n\n\u03c4\u0302 ({\u03bcj })\n\n13077, 6291, 20823\n\n893, 1235, 1309\n\n148, 123, 133\n\n48, 54, 51\n\n\u03c4\u0302 ({\u03c3j2 })\n\n6040, 8845, 2274\n\n249, 250, 239\n\n116, 72, 44\n\n38, 34, 26\n\nTuned 3\n\n\u03b1=0.00053\n\n\u03b1=0.00362\n\n\u03b1=0.01395\n\n\u03b1=0.04467\n\n\u03c4\u0302 ({wj })\n\n8610, 4050, 3609\n\n774, 1244, 813\n\n185, 166, 178\n\n49, 50, 49\n\n\u03c4\u0302 ({\u03bcj })\n\n6826, 19664, 25232\n\n1499, 827, 1133\n\n185, 167, 149\n\n46, 47, 51\n\n\u03c4\u0302 ({\u03c3j2 })\n\n5967, 2569, 2283\n\n268, 288, 156\n\n83, 91, 56\n\n34, 37, 23\n\nTuned 4\n\n\u03b1=0.00054\n\n\u03b1=0.00282\n\n\u03b1=0.00923\n\n\u03b1=0.03884\n\n\u03c4\u0302 ({wj })\n\n4668, 8875, 8538\n\n960, 1043, 862\n\n258, 253, 244\n\n51, 50, 55\n\n\u03c4\u0302 ({\u03bcj })\n\n6335, 3805, 16416\n\n2365, 751, 1928\n\n250, 197, 209\n\n51, 47, 53\n\n\u03c4\u0302 ({\u03c3j2 })\n\n3266, 4463, 1782\n\n411, 218, 376\n\n91, 115, 117\n\n43, 32, 39\n\nTuned 5\n\n\u03b1=0.00062\n\n\u03b1=0.00275\n\n\u03b1=0.00928\n\n\u03b1=0.02518\n\n\u03c4\u0302 ({wj })\n\n18057, 6343, 7629\n\n1065, 763, 1353\n\n293, 261, 210\n\n75, 77, 86\n\n\u03c4\u0302 ({\u03bcj })\n\n5513, 5965, 39982\n\n3743, 1671, 1966\n\n221, 272, 287\n\n91, 93, 87\n\n\u03c4\u0302 ({\u03c3j2 })\n\n1423, 1496, 538\n\n492, 506, 386\n\n118, 149, 118\n\n59, 48, 64\n\nGeometric\n\nTable 2: Results for the mixture problem using different numbers of tempering levels. Results are shown\nfor geometric spacing of the {\u03b2i } and for the tuned spacing from the five replicates of importance sampling.\n*** indicates that the estimates of integrated autocorrelation times did not converge reliably.\n\n19\n\n\f20\n\n180\n\n10\n\nMinimum Sn\n\n15\n\n160\n140\n100\n\n5\n\n120\n\nEstimated g(\u03b2)\n\n0.2\n\n0.6\n\n1.0\n\n10\n\n\u03b2\n\n30\n\n50\n\nn\n\nFigure 7: Left: five replicates of the estimated g(\u03b2) in black and g\u2032 (\u03b2) in red line segments with {\u03b2\u0303}\nindicated by circles. Right: the five corresponding minimum Sn against the number of tempering levels n.\n\ntions times for the three sets of parameters. The first point to note is how large n needs to be in order to\nachieve even low acceptance rates. This is not unexpected; Jasra, Holmes and Stephens (2005) describe\n\"huge rejection rates when sampling from the full posterior\" using tempered transitions. Although tempering moves may not be accepted very often, each one can make a large move in the state space and it is\ncommon practice to intersperse tempering moves with standard MCMC moves for improved local exploration. The fact that acceptance rates can be so low highlights the importance of any tuning. The worst case\nis geometric spacing when n = 64; here the actual number of acceptances is so low, just 13, that the estimates of integrated autocorrelation times fail to converge reliably (taken to mean that the estimate exceeded\na tenth of the total run length). As n increases, acceptance rates improve and integrated autocorrelation\ntimes decrease for all runs. There is some variability between the five replicates of the tuned spacings of\nthe {\u03b2i }, however at all of the n considered, all five outperform geometric spacing by some considerable\nmargin in terms of the acceptance rates and consequently the integrated autocorrelation times.\nThe cost of tuning the {\u03b2i } comprises the cost of the samples required to estimate g(\u03b2) and g\u2032 (\u03b2) using\nimportance sampling plus the optimisation costs for minimising Sn . Here we used 20 relatively short runs,\n20\n\n\fn = 64\n\nn = 128\n\nn = 256\n\nn = 512\n\nEstimating g and g \u2032\n\n8.35\n\n8.35\n\n8.35\n\n8.35\n\nOptimisation\n\n2.78\n\n16.97\n\n52.02\n\n191.00\n\nTuned tempering\n\n879.81\n\n1762.61\n\n3507.79\n\n7022.67\n\nTuned total\n\n890.94\n\n1787.93\n\n3568.16\n\n7222.02\n\nGeometric tempering\n\n874.12\n\n1758.88\n\n3494.94\n\n7002.63\n\n1.019\n\n1.017\n\n1.021\n\n1.031\n\nProportion increase\n\nTable 3: Time in user CPU time seconds for the geometrically spaced temperatures and for the tuned temperatures including a breakdown of the cost of tuning.\n\neach of only 10000 iterations, for the g(\u03b2) and g\u2032 (\u03b2) estimation; this stage is independent of n. The cost of\nthe deterministic minimisation of Sn in R increases with n but is of the order of a few minutes for n = 512.\nThis makes the cost of the tuning procedure a small fraction of the total cost. Full details are given in\nTable 3 showing that the additional cost of the tuning procedure, for this example, varied between 2% and\n4% extra CPU time compared to the untuned procedure. Combining this information with the integrated\nautocorrelation times in Table 2 indicates the tuned procedure gives substantial improvements in mixing\ncompared to the geometric temperature placement.\nIn this example, very little of the total computational effort was spent in estimating g(\u03b2) and g\u2032 (\u03b2).\nAlthough importance sampling cannot be guaranteed to be particularly good for this type of problem, we\nsuggest that this is a sensible strategy. The associated risk is either that the importance sampling fails to\nidentify a g(\u03b2) curve which is not suitable for geometric spacing or, conversely, that it identifies interesting\nfeatures which are not in fact present. In the former case, a visual inspection of the roughly estimated g(\u03b2)\nmay suggest that it is not worthwhile implementing any optimisation, reverting to the default geometric,\nand so the wasted CPU time is minimal. (The same argument is also reasonable when importance sampling\nworks well for estimating g(\u03b2).) On the other hand, if the estimated g(\u03b2) looks to be of the form where\ntuning may help, more computational effort could be put into improving the accuracy of its estimation,\nespecially if a discrepancy is noted between the values of the curve using direct and indirect sampling.\n\n4 Discussion\nIn this paper we have explored how to tune the expensive tempered transitions algorithm to make best\nuse of computational resources. We have shown that the geometric schedule will be optimal if the curve\n\n21\n\n\fg(\u03b2) = E\u03b2 [h(X)] is of a particular form, where the target distribution is p(x) \u221d \u03c0(x) exp(\u2212\u03b20 h(x)). The\ntuning itself is relatively cheap and examples have demonstrated that it can make a significant difference.\nAlthough we have not explicitly considered the question of choosing the number of tempering levels,\nwe have some purely anecdotal evidence that the tuning procedure may yield useful information regarding\nthe minimum number of tempering levels required. In our experience, tempered transitions does not seem\nto perform at all well with a {\u03b2i } for which Sn > 1. For example, in our mixture example the geometric Sn\nis approximately of the order 2, 1, 0.5 and 0.25 for n = 64, 128, 256, 512 respectively, while for the tuned\n{\u03b2i } it is of the corresponding approximate value 1.2, 0.6, 0.3 and 0.15. It also seems feasible that the tuning\napproach proposed here may also be relevant to some of the other MCMC algorithms which incorporate an\nelement of tempering. This is another topic for future research.\n\nAcknowledgements\nGundula Behrens thanks the Engineering and Physical Sciences Research Council and Evangelisches Studienwerk for financial support. Nial Friel's research was supported by a visit to Bath by the Bath Institute\nfor Complex Systems (EPSRC grant GR/S86525/01) and by a Science Foundation Ireland Research Frontiers Program grant, 09/RFP/MTH2199. We are grateful to the Associate Editor and the referees for their\ninsightful and helpful comments and to Dr Jey Sivaloganathan for advice on Equations (14) and (15).\n\nReferences\n[1] B. Calderhead and M. Girolami. Estimating Bayes factors via thermodynamic integration and population MCMC. Computational Statistics and Data Analysis, 53(12):4028\u20134045, 2009.\n[2] G. Celeux, M. Hurn, and C.P. Robert. Computational and inferential difficulties with mixture posterior\ndistributions. Journal of the American Statistical Association, 95(451):957\u2013970, 2000.\n[3] N. Friel and A.N. Pettitt. Marginal likelihood estimation via power posteriors. Journal of the Royal\nStatistical Society, Series B, 70:589\u2013607, 2008.\n[4] A. Gelman and X-L. Meng. Simulating normalizing constants: From importance sampling to bridge\nsampling to path sampling. Statistical Science, 13(2):163\u2013185, 1998.\n[5] C.J. Geyer. Markov chain Monte Carlo maximum likelihood. Computer Science and Statistics,\n23:156\u2013163, 1991.\n\n22\n\n\f[6] C.J. Geyer and E.A. Thompson. Annealing Markov chain Monte Carlo with applications to ancestral\ninference. Journal of the American Statistical Association, 90(431):909\u2013920, 1995.\n[7] R. Gramacy, R. Samworth, and R. King. Importance tempering. Statistics and Computing, 20:1\u20137,\n2010.\n[8] Y. Iba. Extended ensemble Monte Carlo. International Journal of Modern Physics C, 12(5):623\u2013656,\n2001.\n[9] A. Jasra, C.C. Holmes, and D.A. Stephens. Markov chain Monte Carlo methods and the label switching\nproblem in Bayesian mixture modelling. Statistical Science, 17(1):50\u201367, 2005.\n[10] A. Jasra, D.A. Stephens, and C.C. Holmes. On population-based simulation for static inference. Statistics and Computing, 17(3):263\u2013279, 2007.\n[11] G. Lefebvre, R.J. Steele, and A.C. Vandal. A path sampling identity for computing the KullbackLeibler and J-divergences. Computational Statistics and Data Analysis, 54(7):1719\u20131731, 2010.\n[12] E. Marinari and G. Parisi. Simulated tempering: a new Monte Carlo scheme. Europhysics Letters,\n19(6):451\u2013458, 1992.\n[13] P. Matthews. A slowly mixing Markov chain with implications for Gibbs sampling. Statistics and\nProbability Letters, 17:231\u2013236, 1993.\n[14] R.M. Neal. Sampling from multimodal distributions using tempered transitions. Statistics and Computing, 6:353\u2013366, 1996.\n[15] R.M. Neal. Annealed importance sampling. Statistics and Computing, 11:125\u2013139, 2001.\n[16] S. Richardson and P.J. Green. On Bayesian analysis of mixtures with an unknown number of components (with discussion). Journal of the Royal Statistical Society B, 59(4):731\u2013792, 1997.\n[17] C.P. Robert and G. Casella. Monte Carlo statistical methods. Springer, New York, 1999.\n\n23\n\n\f"}