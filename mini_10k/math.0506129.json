{"id": "http://arxiv.org/abs/math/0506129v2", "guidislink": true, "updated": "2009-01-13T09:27:28Z", "updated_parsed": [2009, 1, 13, 9, 27, 28, 1, 13, 0], "published": "2005-06-08T07:00:20Z", "published_parsed": [2005, 6, 8, 7, 0, 20, 2, 159, 0], "title": "Rate of Escape of the Mixer Chain", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0506447%2Cmath%2F0506195%2Cmath%2F0506222%2Cmath%2F0506020%2Cmath%2F0506581%2Cmath%2F0506217%2Cmath%2F0506465%2Cmath%2F0506609%2Cmath%2F0506358%2Cmath%2F0506262%2Cmath%2F0506444%2Cmath%2F0506283%2Cmath%2F0506363%2Cmath%2F0506144%2Cmath%2F0506276%2Cmath%2F0506472%2Cmath%2F0506218%2Cmath%2F0506198%2Cmath%2F0506133%2Cmath%2F0506353%2Cmath%2F0506016%2Cmath%2F0506369%2Cmath%2F0506367%2Cmath%2F0506285%2Cmath%2F0506569%2Cmath%2F0506451%2Cmath%2F0506482%2Cmath%2F0506064%2Cmath%2F0506424%2Cmath%2F0506435%2Cmath%2F0506450%2Cmath%2F0506199%2Cmath%2F0506203%2Cmath%2F0506365%2Cmath%2F0506211%2Cmath%2F0506034%2Cmath%2F0506563%2Cmath%2F0506215%2Cmath%2F0506077%2Cmath%2F0506410%2Cmath%2F0506417%2Cmath%2F0506026%2Cmath%2F0506130%2Cmath%2F0506057%2Cmath%2F0506023%2Cmath%2F0506025%2Cmath%2F0506392%2Cmath%2F0506566%2Cmath%2F0506185%2Cmath%2F0506173%2Cmath%2F0506274%2Cmath%2F0506027%2Cmath%2F0506591%2Cmath%2F0506421%2Cmath%2F0506404%2Cmath%2F0506528%2Cmath%2F0506426%2Cmath%2F0506152%2Cmath%2F0506097%2Cmath%2F0506166%2Cmath%2F0506557%2Cmath%2F0506414%2Cmath%2F0506161%2Cmath%2F0506478%2Cmath%2F0506332%2Cmath%2F0506258%2Cmath%2F0506329%2Cmath%2F0506501%2Cmath%2F0506613%2Cmath%2F0506382%2Cmath%2F0506187%2Cmath%2F0506147%2Cmath%2F0506206%2Cmath%2F0506584%2Cmath%2F0506469%2Cmath%2F0506171%2Cmath%2F0506612%2Cmath%2F0506244%2Cmath%2F0506151%2Cmath%2F0506362%2Cmath%2F0506319%2Cmath%2F0506117%2Cmath%2F0506234%2Cmath%2F0506409%2Cmath%2F0506452%2Cmath%2F0506190%2Cmath%2F0506509%2Cmath%2F0506216%2Cmath%2F0506416%2Cmath%2F0506282%2Cmath%2F0506067%2Cmath%2F0506129%2Cmath%2F0506287%2Cmath%2F0506614%2Cmath%2F0506432%2Cmath%2F0506544%2Cmath%2F0506503%2Cmath%2F0506212%2Cmath%2F0506272%2Cmath%2F0506002%2Cmath%2F0506297&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Rate of Escape of the Mixer Chain"}, "summary": "The mixer chain on a graph G is the following Markov chain. Place tiles on\nthe vertices of G, each tile labeled by its corresponding vertex. A \"mixer\"\nmoves randomly on the graph, at each step either moving to a randomly chosen\nneighbor, or swapping the tile at its current position with some randomly\nchosen adjacent tile.\n  We study the mixer chain on Z, and show that at time t the expected distance\nto the origin is t^{3/4}, up to constants. This is a new example of a random\nwalk on a group with rate of escape strictly between t^{1/2} and t.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0506447%2Cmath%2F0506195%2Cmath%2F0506222%2Cmath%2F0506020%2Cmath%2F0506581%2Cmath%2F0506217%2Cmath%2F0506465%2Cmath%2F0506609%2Cmath%2F0506358%2Cmath%2F0506262%2Cmath%2F0506444%2Cmath%2F0506283%2Cmath%2F0506363%2Cmath%2F0506144%2Cmath%2F0506276%2Cmath%2F0506472%2Cmath%2F0506218%2Cmath%2F0506198%2Cmath%2F0506133%2Cmath%2F0506353%2Cmath%2F0506016%2Cmath%2F0506369%2Cmath%2F0506367%2Cmath%2F0506285%2Cmath%2F0506569%2Cmath%2F0506451%2Cmath%2F0506482%2Cmath%2F0506064%2Cmath%2F0506424%2Cmath%2F0506435%2Cmath%2F0506450%2Cmath%2F0506199%2Cmath%2F0506203%2Cmath%2F0506365%2Cmath%2F0506211%2Cmath%2F0506034%2Cmath%2F0506563%2Cmath%2F0506215%2Cmath%2F0506077%2Cmath%2F0506410%2Cmath%2F0506417%2Cmath%2F0506026%2Cmath%2F0506130%2Cmath%2F0506057%2Cmath%2F0506023%2Cmath%2F0506025%2Cmath%2F0506392%2Cmath%2F0506566%2Cmath%2F0506185%2Cmath%2F0506173%2Cmath%2F0506274%2Cmath%2F0506027%2Cmath%2F0506591%2Cmath%2F0506421%2Cmath%2F0506404%2Cmath%2F0506528%2Cmath%2F0506426%2Cmath%2F0506152%2Cmath%2F0506097%2Cmath%2F0506166%2Cmath%2F0506557%2Cmath%2F0506414%2Cmath%2F0506161%2Cmath%2F0506478%2Cmath%2F0506332%2Cmath%2F0506258%2Cmath%2F0506329%2Cmath%2F0506501%2Cmath%2F0506613%2Cmath%2F0506382%2Cmath%2F0506187%2Cmath%2F0506147%2Cmath%2F0506206%2Cmath%2F0506584%2Cmath%2F0506469%2Cmath%2F0506171%2Cmath%2F0506612%2Cmath%2F0506244%2Cmath%2F0506151%2Cmath%2F0506362%2Cmath%2F0506319%2Cmath%2F0506117%2Cmath%2F0506234%2Cmath%2F0506409%2Cmath%2F0506452%2Cmath%2F0506190%2Cmath%2F0506509%2Cmath%2F0506216%2Cmath%2F0506416%2Cmath%2F0506282%2Cmath%2F0506067%2Cmath%2F0506129%2Cmath%2F0506287%2Cmath%2F0506614%2Cmath%2F0506432%2Cmath%2F0506544%2Cmath%2F0506503%2Cmath%2F0506212%2Cmath%2F0506272%2Cmath%2F0506002%2Cmath%2F0506297&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The mixer chain on a graph G is the following Markov chain. Place tiles on\nthe vertices of G, each tile labeled by its corresponding vertex. A \"mixer\"\nmoves randomly on the graph, at each step either moving to a randomly chosen\nneighbor, or swapping the tile at its current position with some randomly\nchosen adjacent tile.\n  We study the mixer chain on Z, and show that at time t the expected distance\nto the origin is t^{3/4}, up to constants. This is a new example of a random\nwalk on a group with rate of escape strictly between t^{1/2} and t."}, "authors": ["Ariel Yadin"], "author_detail": {"name": "Ariel Yadin"}, "author": "Ariel Yadin", "links": [{"href": "http://arxiv.org/abs/math/0506129v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0506129v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.GR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "60J10, 60B15", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0506129v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0506129v2", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:math/0506129v2 [math.PR] 13 Jan 2009\n\nRate of Escape of the Mixer Chain\nAriel Yadin\u2217\n\nAbstract\nThe mixer chain on a graph G is the following Markov chain. Place tiles on the\nvertices of G, each tile labeled by its corresponding vertex. A \"mixer\" moves randomly\non the graph, at each step either moving to a randomly chosen neighbor, or swapping\nthe tile at its current position with some randomly chosen adjacent tile.\nWe study the mixer chain on Z, and show that at time t the expected distance to\nthe origin is t3/4 , up to constants. This is a new example of a random walk on a group\nwith rate of escape strictly between t1/2 and t.\n\n1\n\nIntroduction\n\nLet G = (V, E) be a graph. On each vertex v \u2208 V , place a tile marked v. Consider the\n\nfollowing Markov chain, which we call the mixer chain. A \"mixer\" performs a random walk\non the graph. At each time step, the mixer chooses a random vertex adjacent to its current\nposition. Then, with probability 1/2 it moves to that vertex, and with probability 1/2 it\n\nremains at the current location, but swaps the tiles on the current vertex and the adjacent\nvertex. If G is the Cayley graph of a group, then the mixer chain turns out to be a random\nwalk on a different group.\nAside from being a canonical process, the mixer chain is interesting because of its rate of\nescape. For a random walk {Xt } on some graph G, we use the terminology rate of escape for\nthe limit\n\nlim\n\nt\u2192\u221e\n\nlog E[d(Xt , X0 )]\n,\nlog t\n\nwhere d(*, *) is the graphical distance. When restricting to random walks on groups, it is still\n\nopen what values in [0, 1] can be obtained by rates of escape. For example, if the group is\n\u2217 Faculty\n\nof Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100,\n\nIsrael. Email: ariel.yadin@weizmann.ac.il\n\n1\n\n\fZd then the rate of escape is 1/2. On a d-ary tree (free group) the rate of escape is 1. As\nfar as the author is aware, the only other examples known were given by Erschler in [1] (see\nalso [2]). Erschler iterates a construction known as the lamp-lighter (slightly similar to the\nmixer chain), and produces examples of groups with rates of escape 1 \u2212 2\u2212k , k = 1, 2, . . . ,.\nAfter formally defining the mixer chain on general groups, we study the mixer chain on Z.\nOur main result, Theorem 2.1, shows that the mixer chain on Z has rate of escape 3/4.\nIt is not difficult to show (perhaps using ideas from this note) that on transient groups the\nmixer chain has rate of escape 1. Since all recurrent groups are essentially Z and Z2 , it\nseems that the mixer chain on other groups cannot give examples of other rates of escape.\nAs for Z2 , one can show that the mixer chain has rate of escape 1. In fact, the ideas in this\nnote suggest that the distance to the origin in the mixer chain on Z2 is n log\u22121/2 (n) up to\nconstants.\nAfter introducing some notation, we provide a formal definition of the mixer chain, as random\nwalk on a Cayley graph. The generalization to general graphs is immediate.\nAcknowledgement. I wish to thank Itai Benjamini for suggesting this construction, and\nfor useful discussions.\n\n1.1\n\nNotation\n\nLet G be a group and U a generating set for G, such that if x \u2208 U then x\u22121 \u2208 U (U is\n\ncalled symmetric). The Cayley graph of G with respect to U is the graph with vertex set\n\b\nG and edge set {g, h} : g \u22121 h \u2208 U . Let D be a distribution on U . Then we can define\n\nthe random walk on G (with respect to U and D) as the Markov chain with state space G\n\b\nand transition matrix P (g, h) = 1 g \u22121 h \u2208 U D(g \u22121 h). We follow the convention that such\na process starts from the identity element in G.\n\nA permutation of G is a bijection from G to G. The support of a permutation \u03c3, denoted\nsupp(\u03c3), is the set of all elements g \u2208 G such that \u03c3(g) 6= g. Let \u03a3 be the group of\n\nall permutations of G with finite support (multiplication is composition of functions). By\n< g, h > we denote the transposition of g and h; that is, the permutation \u03c3 with support\n\n{g, h} such that \u03c3(g) = h, \u03c3(h) = g. By < g1 , g2 , . . . , gn > we denote the cyclic permutation\n\n\u03c3 with support {g1 , . . . , gn }, such that \u03c3(gj ) = gj+1 for j < n and \u03c3(gn ) = g1 .\n\nFor an element g \u2208 G we associate a canonical permutation, denoted by \u03c6g , defined by\n\n\u03c6g (h) = gh for all h \u2208 G. It is straightforward to verify that the map g 7\u2192 \u03c6g is a homomor2\n\n\fphism of groups, and so we use g to denote \u03c6g . Although g 6\u2208 \u03a3, we have that g\u03c3g \u22121 \u2208 \u03a3 for\nall \u03c3 \u2208 \u03a3.\n\nWe now define a new group, that is in fact the semi-direct product of G and \u03a3, with respect\nto the homomorphism g 7\u2192 \u03c6g mentioned above. The group is denoted by G \u22c9 \u03a3, and its\n\nelements are G \u00d7 \u03a3. Group multiplication is defined by:\ndef\n\n(g, \u03c3)(h, \u03c4 ) = (gh, g\u03c4 g \u22121 \u03c3).\nWe leave it to the reader to verify that this is a well-defined group operation. Note that the\nidentity element in this group is (e, id), where id is the identity permutation in \u03a3 and e is\nthe identity element in G. Also, the inverse of (g, \u03c3) is (g \u22121 , g \u22121 \u03c3 \u22121 g).\nWe use d(g, h) = dG,U (g, h) to denote the distance between g and h in the group G with\nQk\nrespect to the generating set U ; i.e., the minimal k such that g \u22121 h = j=1 uj for some\n\nu1 , . . . , uk \u2208 U . The generating set also provides us with a graph structure. g and h are\n\nsaid to be adjacent if d(g, h) = 1, that is if g \u22121 h \u2208 U . A path \u03b3 in G (with respect to the\n\ngenerating set U ) is a sequence (\u03b30 , \u03b31 , . . . , \u03b3n ). |\u03b3| denotes the length of the path, which is\n\ndefined as the length of the sequence minus 1 (in this case |\u03b3| = n).\n\n1.2\n\nMixer Chain\n\nIn order to define the mixer chain we require the following\nProposition 1.1. Let U be a finite symmetric generating set for G. Then,\n\u03a5 = {(u, id), (e, < e, u >) : u \u2208 U }\ngenerates G \u22c9 \u03a3. Furthermore, for any cyclic permutation \u03c3 =< g1 , . . . , gn >\u2208 \u03a3,\ndG\u22c9\u03a3,\u03a5 ((g1 , \u03c3), (g1 , id)) \u2264 5\n\nn\nX\n\nd(gj , \u03c3(gj )).\n\nj=1\n\nProof. Let D((g, \u03c3), (h, \u03c4 )) denote the minimal k such that (g, \u03c3)\u22121 (h, \u03c4 ) =\n\nQk\n\nj=1\n\n\u03c5j , for\n\nsome \u03c51 , . . . , \u03c5k \u2208 \u03a5, with the convention that D((g, \u03c3), (h, \u03c4 )) = \u221e if there is no such\n\nfinite sequence of elements of \u03a5. Thus, we want to prove that D((g, \u03c3), (e, id)) < \u221e for all\n\ng \u2208 G and \u03c3 \u2208 \u03a3. Note that by definition for any f \u2208 G and \u03c0 \u2208 \u03a3, D((g, \u03c3), (h, \u03c4 )) =\nD((f, \u03c0)(g, \u03c3), (f, \u03c0)(h, \u03c4 )).\n\n3\n\n\fA generator simple path in G is a finite sequence of generators u1 , . . . , uk \u2208 U such that for\nQk\nany 1 \u2264 l \u2264 k, j=l uj 6= e. By induction on k, one can show that for any k \u2265 1, and for\nany generator simple path u1 , . . . , uk ,\n(e, < e,\n\nk\nY\n\nj=1\n\nuj >) =\n\nk\u22121\nY\nj=1\n\n(e, < e, uj >)(uj , id) * (e, < e, uk >) *\n\nk\u22121\nY\n\n\u22121\n(e, < e, u\u22121\nk\u2212j >)(uk\u2212j , id).\n\nj=1\n\nIf d(g, h) = k then there exists a generator simple path u1 , . . . , uk such that h = g\nThus, we get that for any h \u2208 G,\n\n(1.1)\nQk\n\nj=1\n\nuj .\n\nD((e, < e, h >), (e, id)) \u2264 4d(h, e) \u2212 3.\nBecause g < e, g \u22121 h > g \u22121 =< g, h >, we get that if \u03c4 =< g, h > \u03c3 then\nD((g, \u03c4 ), (g, \u03c3)) = D((g, \u03c3)(e, < e, g \u22121 h >), (g, \u03c3)(e, id)) \u2264 4d(g \u22121 h, e) \u2212 3 = 4d(g, h) \u2212 3.\nThe triangle inequality now implies that D((h, \u03c4 ), (g, \u03c3)) \u2264 5d(g, h) \u2212 3.\nThus, if \u03c3 =< g1 , g2 , . . . , gn >, since \u03c3 =< g1 , g2 >< g2 , g3 > * * * < gn\u22121 , gn >, we get that\nD((g1 , \u03c3), (g1 , id)) \u2264 5\n\nn\u22121\nX\n\nd(gj , gj+1 ) + d(gn , g1 ).\n\n(1.2)\n\nj=1\n\nThe proposition now follows from the fact that any \u03c3 \u2208 \u03a3 can be written as a finite product\nof cyclic permutations.\n\n\u2293\n\u2294\n\nWe are now ready to define the mixer chain:\nDefinition 1.2. Let G be a group with finite symmetric generating set U . The mixer chain\non G (with respect to U ) is the random walk on the group G \u22c9 \u03a3 with respect to uniform\nmeasure on the generating set \u03a5 = {(u, id), (e, < e, u >) : u \u2208 U }.\nAn equivalent way of viewing this chain is viewing the state (g, \u03c3) \u2208 G \u22c9 \u03a3 as follows:\n\nThe first coordinate corresponds to the position of the mixer on G. The second coordinate\ncorresponds to the placing of the different tiles, so the tile marked x is placed on the vertex\n\u03c3(x). By Definition 1.2, the mixer chooses uniformly an adjacent vertex of G, say h. Then,\n\nwith probability 1/2 the mixer swaps the tiles on h and g, and with probability 1/2 it moves\nto h. The identity element in G \u22c9 \u03a3 is (e, id), so the mixer starts at e with all tiles on their\ncorresponding vertices (the identity permutation).\n4\n\n\f1.3\n\nDistance Bounds\n\nIn this section we show that the distance of an element in G \u22c9 \u03a3 to (e, id) is essentially\ngoverned by the sum of the distances of each individual tile to its origin.\nLet (g, \u03c3) \u2208 G \u22c9 \u03a3. Let \u03b3 = (\u03b30 , \u03b31 , . . . , \u03b3n ) be a finite path in G. We say that the path \u03b3\n\ncovers \u03c3 if supp(\u03c3) \u2282 {\u03b30 , \u03b31 , . . . , \u03b3n }. The covering number of g and \u03c3, denoted Cov(g, \u03c3),\nis the minimal length of a path \u03b3, starting at g, that covers \u03c3; i.e.\n\nCov(g, \u03c3) = min {|\u03b3| : \u03b30 = g and \u03b3 is a path covering \u03c3} .\nTo simplify notation, we denote D = dG\u22c9\u03a3,\u03a5 .\nProposition 1.3. Let (g, \u03c3) \u2208 G \u22c9 \u03a3. Then,\nD((g, \u03c3), (g, id)) \u2264 2Cov(g, \u03c3) + 5\n\nX\n\nd(h, \u03c3(h)).\n\nh\u2208supp(\u03c3)\n\nProof. The proof of the proposition is by induction on the size of supp(\u03c3). If |supp(\u03c3)| = 0,\nthen \u03c3 = id so the proposition holds. Assume that |supp(\u03c3)| > 0.\n\nLet n = Cov(g, \u03c3), and let \u03b3 be a path in G such that |\u03b3| = n, \u03b30 = g and \u03b3 covers \u03c3. Write\n\n\u03c3 = c1 c2 * * * ck , where the cj 's are cyclic permutations with pairwise disjoint non-empty\nsupports, and\n\nsupp(\u03c3) =\n\nk\n[\n\nsupp(cj ).\n\nj=1\n\nLet\nj = min {m \u2265 0 : \u03b3m \u2208 supp(\u03c3)} .\nSo, there is a unique 1 \u2264 l \u2264 k such that \u03b3j \u2208 supp(cl ). Let \u03c4 = c\u22121\nl \u03c3. Thus,\nsupp(\u03c4 ) =\n\n[\n\nsupp(cj ),\n\nj6=l\n\nand specifically, |supp(\u03c4 )| < |supp(\u03c3)|. Note that h \u2208 supp(\u03b3j\u22121 cl \u03b3j ) if and only if \u03b3j h \u2208\n\nsupp(cl ), and specifically, e \u2208 supp(\u03b3j\u22121 cl \u03b3j ). \u03b3j\u22121 cl \u03b3j is a cyclic permutation, so by Proposition 1.1, we know that\n\nD((\u03b3j , \u03c3), (\u03b3j , \u03c4 )) = D((\u03b3j , \u03c4 )(e, \u03b3j\u22121 cl \u03b3j ), (\u03b3j , \u03c4 )) = D((e, \u03b3j\u22121 cl \u03b3j ), (e, id))\nX\nX\nd(h, \u03c3(h)).\nd(\u03b3j\u22121 h, \u03b3j\u22121 cl (h)) = 5\n\u22645\nh\u2208supp(cl )\n\nh\u2208supp(cl )\n\n5\n\n(1.3)\n\n\fBy induction,\nD((\u03b3j , \u03c4 ), (\u03b3j , id)) \u2264 2Cov(\u03b3j , \u03c4 ) + 5\n\nX\n\nd(h, \u03c4 (h)).\n\n(1.4)\n\nh\u2208supp(\u03c4 )\n\nLet \u03b2 be the path (\u03b3j , \u03b3j+1 , . . . , \u03b3n ). Since \u03b3j is the first element in \u03b3 that is in supp(\u03c3), we\nget that supp(\u03c4 ) \u2282 supp(\u03c3) \u2286 {\u03b3j , \u03b3j+1 , . . . , \u03b3n }, which implies that \u03b2 is a path of length\nn \u2212 j that covers \u03c4 , so Cov(\u03b3j , \u03c4 ) \u2264 n \u2212 j. Combining (1.3) and (1.4) we get,\n\nD((g, \u03c3), (g, id)) \u2264 D((\u03b30 , \u03c3), (\u03b3j , \u03c3)) + D((\u03b3j , \u03c3), (\u03b3j , \u03c4 )) + D((\u03b3j , \u03c4 ), (\u03b3j , id)) + D((\u03b3j , id), (\u03b30 , id))\nX\nX\nd(h, \u03c3(h)) + 2(n \u2212 j) + j\nd(h, \u03c3(h)) + 5\n\u2264j+5\nh\u2208supp(\u03c4 )\n\nh\u2208supp(cl )\n\n= 2Cov(g, \u03c3) + 5\n\nX\n\nd(h, \u03c3(h)).\n\nh\u2208supp(\u03c3)\n\n\u2293\n\u2294\nProposition 1.4. Let (g, \u03c3) \u2208 G \u22c9 \u03a3 and let g \u2032 \u2208 G. Then,\nD((g, \u03c3), (g \u2032 , id)) \u2265\n\n1\n2\n\nX\n\nd(h, \u03c3(h)).\n\nh\u2208supp(\u03c3)\n\nProof. The proof is by induction on D = D((g, \u03c3), (g \u2032 , id)). If D = 0 then \u03c3 = id, and we are\ndone. Assume that D > 0. Let \u03c5 \u2208 \u03a5 be a generator such that D((g, \u03c3)\u03c5, (g \u2032 , id)) = D \u2212 1.\n\nThere exists u \u2208 U such that either \u03c5 = (u, id) or \u03c5 = (e, < e, u >). If \u03c5 = (u, id) then by\ninduction\n\nD \u2265 D((g, \u03c3)\u03c5, (g \u2032 , id)) \u2265\n\n1\n2\n\nX\n\nd(h, \u03c3(h)).\n\nh\u2208supp(\u03c3)\n\nSo assume that \u03c5 = (e, < e, u >). If \u03c3(h) 6\u2208 {g, gu}, then < g, gu > \u03c3(h) = \u03c3(h), and\n\b\n\b\nsupp(\u03c3) \\ \u03c3 \u22121 (g), \u03c3 \u22121 (gu) = supp(< g, gu > \u03c3) \\ \u03c3 \u22121 (g), \u03c3 \u22121 (gu) .\n\nSince d(g, gu) = 1,\nX\n\nd(h, \u03c3(h)) = d(g, \u03c3 \u22121 (g)) + d(gu, \u03c3 \u22121 (gu)) +\n\nh\u2208supp(\u03c3)\n\nX\n\nd(h, \u03c3(h))\n\nh6\u2208{\u03c3\u22121 (g),\u03c3\u22121 (gu)}\n\n\u2264 d(g, gu) + d(gu, \u03c3 \u22121 (g)) + d(gu, g) + d(g, \u03c3 \u22121 (gu))\nX\n+\nd(h, < g, gu > \u03c3(h))\nh6\u2208{\u03c3\u22121 (g),\u03c3\u22121 (gu)}\n\n\u22642+\n\nX\n\nd(h, < g, gu > \u03c3(h)).\n\nh\u2208supp(<g,gu>\u03c3)\n\n6\n\n\fSo by induction,\nD = 1 + D((g, < g, gu > \u03c3), (g \u2032 , id)) \u2265 1 +\n\u2265\n\n1\n2\n\nX\n\n1\n2\n\nX\n\nd(h, < g, gu > \u03c3(h))\n\nh\u2208supp(<g,gu>\u03c3)\n\nd(h, \u03c3(h)).\n\nh\u2208supp(\u03c3)\n\n\u2293\n\u2294\n\n2\n\nThe Mixer Chain on Z\n\nWe now consider the mixer chain on Z, with {1, \u22121} as the symmetric generating set. We\ndenote by {\u03c9t = (St , \u03c3t )}t\u22650 the mixer chain on Z.\n\nFor \u03c9 \u2208 Z\u22c9\u03a3 we denote by D(\u03c9) the distance of \u03c9 from (0, id) (with respect to the generating\nset \u03a5, see Definition 1.2). Denote by Dt = D(\u03c9t ) the distance of the chain at time t from\n\nthe origin.\nAs stated above, we show that the mixer chain on Z has rate of escape 3/4. In fact, we prove\nslightly stronger bounds on the distance to the origin at time t.\nTheorem 2.1. Let Dt be the distance to the origin of the mixer chain on Z. Then, there\nexist constants c, C > 0 such that for all t \u2265 0, ct3/4 \u2264 E[Dt ] \u2264 Ct3/4 .\nThe proof of Theorem 2.1 is in Section 3.\nFor z \u2208 Z, denote by Xt (z) = |\u03c3t (z) \u2212 z|, the distance of the tile marked z to its origin at\ntime t. Define\n\nXt =\n\nX\n\nXt (z),\n\nz\u2208Z\n\nwhich is a finite sum for any given t. As shown in Propositions 1.3 and 1.4, Xt approximates\nDt up to certain factors.\nFor z \u2208 Z define\nVt (z) =\n\nt\nX\n\n1{St = \u03c3t (z)}.\n\nj=0\n\nVt (z) is the number of times that the mixer visits the tile marked z, up to time t.\n\n7\n\n\f2.1\n\nDistribution of Xt (z)\n\nProposition 2.2. For \u03c3 \u2208 \u03a3 define \u03c3 \u2032 \u2208 \u03a3 by \u03c3 \u2032 (z) = \u2212\u03c3(\u2212z) for all z \u2208 Z. Then, for any\nt \u2265 1, ((S1 , \u03c31 ), . . . , (St , \u03c3t )) and ((\u2212S1 , \u03c31\u2032 ), . . . , (\u2212St , \u03c3t\u2032 )) have the same distribution.\n\nProof. Let \u03c6 be the permutation (not in \u03a3) defined by \u03c6(x) = \u2212x for all x \u2208 Z. Then,\n\n\u03c3 \u2032 = \u03c6\u03c3\u03c6. Since \u03c62 = id, we get that (\u03c3\u03c4 )\u2032 = \u03c3 \u2032 \u03c4 \u2032 , \u03c3 \u2032\u2032 = \u03c3 and (\u03c3 \u22121 )\u2032 = (\u03c3 \u2032 )\u22121 .\n\nThe proof is by induction on t. If t = 1, then (S1 , \u03c31 ) is uniformly distributed over the set\n\u03a5 = {(1, id), (\u22121, id), (0, < 0, 1 >), (0, < 0, \u22121 >)} .\nSince < 0, 1 >\u2032 =< 0, \u22121 >, and id\u2032 = id, the proposition is proved for t = 1.\nLet t > 1. Let (x, \u03c4 ) \u2208 Z \u22c9 \u03a3 be any element, and let \u03c5 = (y, \u03c1) \u2208 \u03a5 be a generator.\nWe have (x, \u03c4 )(y, \u03c1) = (x + y, x\u03c1(\u2212x)\u03c4 ). Since \u03c1 \u2208 {id, < 0, 1 >, < 0, \u22121 >}, we have that\n\n(x\u03c1(\u2212x)\u03c4 )\u2032 = (\u2212x)\u03c1\u2032 x\u03c4 \u2032 . Since y 6= 0 if and only if \u03c1 = id, we get that (\u2212y)\u03c1y = \u03c1, so\n(\u2212x, \u03c4 \u2032 )(\u2212y, \u03c1\u2032 ) = (\u2212(x + y), (\u2212x)\u03c1\u2032 x\u03c4 \u2032 ) = (\u2212(x + y), (x\u03c1(\u2212x)\u03c4 )\u2032 ).\nThus, we get that for any (z, \u03c3), (x, \u03c4 ) \u2208 Z \u22c9 \u03a3 and any (y, \u03c1) \u2208 \u03a5,\n(z, \u03c3) = (x, \u03c4 )(y, \u03c1)\n\nif and only if\n\n(\u2212z, \u03c3 \u2032 ) = (\u2212x, \u03c4 \u2032 )(\u2212y, \u03c1\u2032 ).\n\nIf (y, \u03c1) is uniformly distributed in \u03a5, then so is (\u2212y, \u03c1\u2032 ). Thus, since (St\u22121 , \u03c3t\u22121 )\u22121 (St , \u03c3t )\n\u2032\nis uniformly distributed in \u03a5, (\u2212St\u22121 , \u03c3t\u22121\n)\u22121 (\u2212St , \u03c3t\u2032 ) is also uniformly distributed in \u03a5.\n\u2032\n)), have the same\nBy induction, ((S1 , \u03c31 ), . . . , (St\u22121 , \u03c3t\u22121 )) and ((\u2212S1 , \u03c31\u2032 ), . . . , (\u2212St\u22121 , \u03c3t\u22121\n\ndistribution. The Markov property now implies the proposition.\n\n\u2293\n\u2294\n\nBy a lazy random walk on Z, we refer to the integer valued process Wt , such that Wt+1 \u2212 Wt\n\nare i.i.d. random variables with the distribution P[Wt+1 \u2212Wt = 1] = P[Wt+1 \u2212Wt = 1] = 1/4\n\nand P[Wt+1 \u2212 Wt = 0] = 1/2.\n\nLemma 2.3. Let t \u2265 0 and z \u2208 Z. Let k \u2265 1 such that P[Vt (z) = k] > 0. Then, conditioned\non Vt (z) = k, the distribution of \u03c3t (z) \u2212 z is the same as Wk\u22121 + B, where {Wk } is a lazy\nrandom walk on Z, and B is a random variable independent of {Wk } such that |B| \u2264 2.\n\nProof. Define inductively the following random times: T0 (z) = 0, and for j \u2265 1,\nTj (z) = inf {t \u2265 Tj\u22121 (z) + 1 : St = \u03c3t (z)} .\n\n8\n\n\fClaim 2.4. Let T = T1 (0). For all l such that P[T = l] > 0,\n\u0002\nP \u03c3T (0) = 1\n\nand\n\n\u0003\n\u0002\nT = l = P \u03c3T (0) = \u22121\n\u0002\nP \u03c3T (0) = 0\n\n\u0003\nT = l = 1/4,\n\n\u0003\nT = l = 1/2,\n\nProof. Note that |S1 \u2212 \u03c31 (0)| = 1 and that for all 1 \u2264 t < T , \u03c3t (0) = \u03c31 (0). Thus,\n\n\u03c3T \u22121 (0) = \u03c31 (0) and ST \u22121 = S1 . So we have the equality of events\n{T = l} =\n\nl\u22121\n\\\nt=1\n\n{St 6= \u03c3t (0)}\n\nHence, if we denote E =\n\n\\\n\nTl\u22121\nt=1\n\n{Sl\u22121 = S1 , \u03c3l\u22121 (0) = \u03c31 (0)}\n\n{St 6= \u03c3t (0)}\n\nT\n\n\\\n\n{Sl = \u03c31 (0) or \u03c3l (0) = S1 } .\n\n{Sl\u22121 = S1 , \u03c3l\u22121 (0) = \u03c31 (0)}, then\n\n\u0002\nP[T = l] = P[E] * P Sl = \u03c31 (0) or \u03c3l (0) = S1\n1\n= P[E] * .\n2\n\nE\n\n\u0003\n\n(2.1)\n\nSince the events {S1 = 0} and {\u03c31 (0) = 0} are disjoint and their union is the whole space,\n\nwe get that\n\nP[\u03c3T (0) = 0, T = l] = P[E, Sl = \u03c31 (0) = 0] + P[E, \u03c3l (0) = S1 = 0]\n\u0002\n\u0003\n= P [E, \u03c31 (0) = 0] * P Sl = \u03c31 (0) Sl\u22121 = S1 , \u03c3l\u22121 (0) = \u03c31 (0) = 0\n\u0003\n\u0002\n+ P [E, S1 (0) = 0] * P \u03c3l (0) = S1 Sl\u22121 = S1 = 0, \u03c3l\u22121 (0) = \u03c31 (0)\n1\n= P[E] * .\n4\n\n(2.2)\n\nCombining (2.1) and (2.2) we get that\n\nFinally, by Proposition 2.2,\n\n\u0002\n\u0003 1\nP \u03c3T (0) = 0 T = l = .\n2\n\nP [\u03c3T (0) = 1, T = l] = P [E , Sl = \u03c3l (0) = 1]\n= P [\u03c3T (0) = \u22121, T = l] .\nSince the possible values for \u03c3T (0) are \u22121, 0, 1, the claim follows.\nWe continue with the proof of Lemma 2.3.\nWe have the equality of events {Vt (z) = k} = {Tk (z) \u2264 t < Tk+1 (z)}.\n9\n\n\u2293\n\u2294\n\n\fLet t1 , t2 , . . . , tk , tk+1 be such that\nP[T1 (z) = t1 , . . . , Tk+1 (z) = tk+1 ] > 0,\nand condition on the event E = {T1 (z) = t1 , . . . , Tk+1 (z) = tk+1 }. Assume further that tk \u2264\n\nt < tk+1 , so that Vt (z) = k. Write\n\n\u03c3t (z) \u2212 z = \u03c3t (z) \u2212 \u03c3Tk (z) (z) +\n\nk\nX\nj=2\n\n\u03c3Tj (z) (z) \u2212 \u03c3Tj\u22121 (z) (z) + \u03c3T1 (z) (z) \u2212 z.\n\n(2.3)\n\nFor 1 \u2264 j \u2264 k \u22121 denote Yj = \u03c3Tj+1 (z) (z)\u2212\u03c3Tj (z) (z). By Claim 2.4 and the Markov property,\n\nconditioned on E, {Yj } are independent with the distribution P[Yj = 1|E] = P[Yj = \u22121|E] =\nPk\u22121\nj=1 Yj has the same distribution of Wk\u22121 .\n\n1/4 and P[Yj = 0|E] = 1/2. So conditioned on E,\n\nFinally, |\u03c3t (z) \u2212 \u03c3Tk (z) (z)| \u2264 1, and |\u03c3T1 (z) (z) \u2212 z| \u2264 1. Since conditioned on E, \u03c3t (z) \u2212\n\n\u03c3Tk (z) (z), and \u03c3T1 (z) (z) \u2212 z are independent of {Yj }, this completes the proof of the lemma.\n\n\u2293\n\u2294\n\nCorollary 2.5. There exist constants c, C > 0 such that for all t \u2265 0 and all z \u2208 Z,\np\np\nc E[ Vt (z)] \u2212 2 P[Vt (z) \u2265 1] \u2264 E[Xt (z)] \u2264 C E[ Vt (z)] + 2 P[Vt (z) \u2265 1].\n\nProof. Let {Wt } be a lazy random walk on Z. Note that {2Wt } has the same distribution as\n\n\u2032\n{S2t\n} where {St\u2032 } is a simple random walk on Z. It is well known (see e.g. [3]), that there\n\nexist universal constants c1 , C1 > 0 such that for all t \u2265 0,\n\n\u221a\n\u221a\n\u2032\nc1 t \u2264 E[|S2t\n|] = 2 E[|Wt |] \u2264 C1 t.\n\nBy Lemma 2.3, we know that for any k \u2265 0,\n\u0002\n\u0003\nE[|Wk |] \u2212 2 \u2264 E Xt (z) Vt (z) = k + 1 \u2264 E[|Wk |] + 2.\n\nThus, summing over all k, there exists constants c2 , C2 > 0 such that\n\np\np\nc2 E[ Vt (z)] \u2212 2 P[Vt (z) \u2265 1] \u2264 E[Xt (z)] \u2264 C2 E[ Vt (z)] + 2 P[Vt (z) \u2265 1].\nLemma 2.6. Let {St\u2032 } be a simple random walk on Z started at S0\u2032 = 0, and let\nLt (z) =\n\nt\nX\n\b\n1 Sj\u2032 = z .\nj=0\n\n10\n\n\u2293\n\u2294\n\n\fThen, for any z \u2208 Z, and any k \u2208 N,\nP[L2t (2z) \u2265 k] \u2264 P[Vt (z) \u2265 k].\np\np\nSpecifically, E[ L2t (2z)] \u2264 E[ Vt (z)].\n\nProof. Fix z \u2208 Z. For t \u2265 0 define Mt = St \u2212 \u03c3t (z) + z. Note that\nVt (z) =\n\nt\nX\n\n1{Mj = z},\n\nj=0\n\nso Vt (z) is the number of times {Mt } visits z up to time t.\n{Mt } is a Markov chain on Z with the following step distribution.\n\n\u0002\n\nP Mt+1\n\n\uf8f1\n\uf8f4\n1/2\nMt = z,\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n1/2 |Mt \u2212 z| = 1,\n\uf8f4\n\uf8f4\n\uf8f4\n\u0003 \uf8f2 1/4 |Mt \u2212 z| = 1,\n= Mt + \u03b5 Mt =\n\uf8f4\n\uf8f4 1/4 |Mt \u2212 z| = 1,\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n1/4 |Mt \u2212 z| > 1,\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n1/2 |Mt \u2212 z| > 1,\n\n\u03b5 \u2208 {\u22121, 1} ,\n\n\u03b5 = \u2212Mt + z,\n\u03b5 = 0,\n\n\u03b5 = Mt \u2212 z,\n\n\u03b5 \u2208 {\u22121, 1} ,\n\n\u03b5 = 0.\n\nSpecifically, {Mt } is simple symmetric when at z, lazy symmetric when not adjacent to z,\nand has a drift towards z when adjacent to z.\n\nDefine {Nt } to be the following Markov chain on Z: N0 = 0,\n\uf8f1\n\uf8f4\n\uf8f4 1/2 Nt = z,\n\u0003 \uf8f2\n\u0002\nP Nt+1 = Nt + \u03b5 Nt =\n1/2 Nt 6= z,\n\uf8f4\n\uf8f4\n\uf8f3 1/4 N 6= z,\nt\n\nand for all t \u2265 0,\n\u03b5 \u2208 {\u22121, 1} ,\n\n\u03b5 = 0,\n\n\u03b5 \u2208 {\u22121, 1} .\n\nSo {Nt } is simple symmetric at z, and lazy symmetric when not at z. Let\nVt\u2032 (z)\n\n=\n\nt\nX\n\n1{Nj = z},\n\nj=0\n\nbe the number of times {Nt } visits z up to time t.\nDefine inductively \u03c10 = \u03c1\u20320 = 0 and for j \u2265 0,\n\b\n\u03c1j+1 = min t \u2265 1 : M\u03c1j +t = z ,\nn\no\n\u03c1\u2032j+1 = min t \u2265 1 : N\u03c1\u2032j +t = z .\n11\n\n\fIf Nt \u2265 Mt > z then\n\u0002\n\u0003\n\u0002\n\u0003\nP Mt+1 = Mt + 1 Mt = P Nt+1 = Nt + 1 Nt ,\n\nand\n\n\u0002\n\u0003\n\u0002\n\u0003\nP Mt+1 = Mt \u2212 1 Mt \u2265 P Nt+1 = Nt \u2212 1 Nt .\n\nThus, we can couple Mt+1 and Nt+1 so that Mt+1 \u2264 Nt+1 . Similarly, if Nt \u2264 Mt < z\n\nthen Mt+1 moves towards z with higher probability than Nt+1 , and they both move away\nfrom z with probability 1/4. So we can couple Mt+1 and Nt+1 so that Mt+1 \u2265 Nt+1 . If\n\nNt = Mt = z then Mt+1 and Nt+1 have the same distribution, so they can be coupled so\n\nthat Nt+1 = Mt+1 .\nThus, we can couple {Mt } and {Nt } so that for all j \u2265 0, \u03c1j \u2264 \u03c1\u2032j a.s.\nLet {St\u2032 } be a simple random walk on Z. For x \u2208 Z, let\n\u2032\n\u03c4x = min {2t \u2265 2 : S2t\n= 2z , S0\u2032 = 2x} .\n\nThat is, \u03c4x is the first time a simple random walk started at 2x hits 2z (this is necessarily an\neven number). In [3, Chapter 9] it is shown that \u03c4x has the same distribution as \u03c42z \u2212 2|z \u2212 x|.\n\n\u2032\n\u2032\nNote that if Nt 6= z then S2t+2\n\u2212 S2t\nhas the same distribution as 2(Nt+1 \u2212 Nt ). Since\n\n|N\u03c1\u2032j\u22121 +1 \u2212 z| = 1, we get that for all j \u2265 2, \u03c1\u2032j has the same distribution as 12 (\u03c42z \u2212 2) + 1.\n\nAlso, \u03c1\u20321 has the same distribution as 21 \u03c40 if z 6= 0, and the same distribution as 12 (\u03c42z \u2212 2) + 1\nPk\n\u2032\nif z = 0. Hence, we conclude that for any k \u2265 1,\nj=1 \u03c1j has the same distribution as\nP\nk\n1\nj=1 \u03c1\u0303j , where {\u03c1\u0303j }j\u22651 are defined by\n2\nn\no\n\u03c1\u0303j+1 = min 2t \u2265 2 : S\u03c1\u0303\u2032 j +2t = 2z .\n\nPk\nPk\nFinally note that Vt (z) \u2265 k if and only if j=1 \u03c1j \u2264 t, Vt\u2032 (z) \u2265 k if and only if j=1 \u03c1\u2032j \u2264 t,\nPk\nand Lt (2z) \u2265 k if and only if j=1 \u03c1\u0303j \u2264 t. Thus, under the above coupling, for all t \u2265 0,\nVt (z) \u2265 Vt\u2032 (z) a.s. Also, Vt\u2032 (z) has the same distribution as L2t (2z). The lemma follows. \u2293\n\u2294\n\n2.2\n\nThe Expectation of Xt\n\nRecall that Xt =\n\nP\n\nz\n\nXt (z).\n\nLemma 2.7. There exists constants c, C > 0 such that for all t \u2265 0,\nct3/4 \u2264 E[Xt ] \u2264 Ct3/4 .\n12\n\n\fProof. We first prove the upper bound. For z \u2208 Z let A(z) be the indicator of the event that\n\nthe mixer reaches z up to time t; i.e. At (z) = 1{Vt (z) \u2265 1}. Note that (\u03c3t (z)\u2212z)(1\u2212At(z)) =\nP\n0. Also, by definition z Vt (z) = t. By Corollary 2.5, using the Cauchy-Schwartz inequality,\nE[Xt ] =\n\nX\nz\n\nX\n\nX\np\nE[ Vt (z)] + 2 E\nAt (z)\n\nX\n\nX\n\nE[Xt (z)] \u2264 C1\n\n\u2264 C1 E\n\ns\nX\nz\n\nVt (z) *\n\nz\n\nz\n\nAt (z) + 2 E\n\nz\n\nAt (z),\n\nz\n\nfor some constant C1 > 0. For any z \u2208 Z, if At (z) = 1, then there exists 0 \u2264 j \u2264 t such that\n\n|Sj \u2212 z| = 1. That is, At (z) = 1 implies that z \u2208 [mt \u2212 1, Mt + 1], where Mt = max0\u2264j\u2264t Sj\nP\nand mt = min0\u2264j\u2264t Sj . Thus, z A(z) \u2264 Mt \u2212 mt + 2. Since Mt \u2212 mt is just the number\n\u221a\nP\nof sites visited by a lazy random walk, we get (see e.g. [3]) E[ z At (z)] \u2264 C2 t, for some\n\nconstant C2 > 0. Hence, there exists some constant C3 > 0 such that\nq\n\u221a\n\u221a\nE[Xt ] \u2264 C1 t * C2 t + 2C2 t \u2264 C3 t3/4 .\nThis proves the upper bound.\n\nWe turn to the lower bound. Let {St\u2032 } be a simple random walk on Z started at S0\u2032 = 0, and\nlet\n\nLt (z) =\n\nt\nX\n\b\n1 Sj\u2032 = z .\nj=0\n\nLet\n\nT (z) = min {t \u2265 0 : St\u2032 = z} .\nBy the Markov property,\nP [L2t (z) \u2265 k] \u2265 P [T (z) \u2264 t] P [Lt (0) \u2265 k] ,\nso\np\np\nE[ L2t (2z)] \u2265 P [T (2z) \u2264 t] E[ Lt (0)].\np\nTheorem 9.3 of [3] can be used to show that E[ Lt (0)] \u2265 c1 t1/4 , for some constant c1 > 0.\nBy Corollary 2.5, and Lemma 2.6, there exists a constant c2 > 0 such that\nE[Xt ] \u2265 c2\n\nX\n\n\u2265 c1 t\n\nz\n\n1/4\n\nX\np\nE[ L2t (2z)] \u2212 2\nAt (z)\nz\n\n* c2 E\n\nX\nz\n\n\u221a\n1{T (2z) \u2264 t} \u2212 2C2 t.\n\n13\n\n\fLet Mt\u2032 = max0\u2264j\u2264t Sj\u2032 and m\u2032t = min0\u2264j\u2264t Sj\u2032 . Then,\nX\nz\n\n1{T (2z) \u2264 t} = [m\u2032t , Mt\u2032 ]\n\n\\\n\n2Z.\n\nSo for some constants c3 , c4 > 0,\nE[Xt ] \u2265 c3 t1/4 *\n\n\u221a\n1\nE[Mt\u2032 \u2212 m\u2032t \u2212 1] \u2212 2C2 t \u2265 c4 t3/4 .\n2\n\u2293\n\u2294\n\n3\n\nProof of Theorem 2.1\n\nProof. Recall that Cov(z, \u03c3) is the minimal length of a path on Z, started at z, that covers\nsupp(\u03c3). Let Mt = max0\u2264j\u2264t Sj and mt = min0\u2264j\u2264t Sj , and let It = [mt \u2212 1, Mt + 1]. Note\n\nthat supp(\u03c3t ) \u2282 It . So for any z \u2208 It , Cov(z, \u03c3t ) \u2264 2|It |. {St } has the distribution of a lazy\n\n\u2032\nrandom walk on Z, so {2St } has the same distribution as {S2t\n}, where {St\u2032 } is a simple random\n\nwalk on Z. It is well known (see e.g. [3, Chapter 2]) that there exist constants c1 , C1 > 0 such\n\u221a\n\u221a\n\u221a\nthat c1 t \u2264 E[|It |] \u2264 C1 t. Since St \u2208 It , we get that E[Cov(St , \u03c3t )] \u2264 2C1 t. Together\nwith Propositions 1.3 and 1.4, and with Lemma 2.7, we get that there exist constants c, C > 0\nsuch that for all t \u2265 0,\nct3/4 \u2264\n\n1\nE[Xt ] \u2264 E[Dt ] \u2264 2 E[Cov(St , \u03c3t )] + 5 E[Xt ] \u2264 Ct3/4 .\n2\n\u2293\n\u2294\n\nReferences\n[1] Erschler (Dyubina), A. On the Asymptotics of Drift. Journal of Mathematical Sciences 121 (2004), 2437\u20132440.\n[2] Revelle, D. Rate of Escape of Random Walks on Wreath Products and Related Groups.\nAnnals of Probability 31 (2003), 1917-1934.\n[3] R\u00e9v\u00e9sz, P. Random Walk in Random and Non-Random Environments. World Scientific\nPublishing Co., 2005.\n\n14\n\n\f"}