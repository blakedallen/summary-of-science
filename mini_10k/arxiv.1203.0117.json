{"id": "http://arxiv.org/abs/1203.0117v3", "guidislink": true, "updated": "2012-09-25T03:44:33Z", "updated_parsed": [2012, 9, 25, 3, 44, 33, 1, 269, 0], "published": "2012-03-01T08:34:30Z", "published_parsed": [2012, 3, 1, 8, 34, 30, 3, 61, 0], "title": "Learning a Common Substructure of Multiple Graphical Gaussian Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.4206%2C1203.3730%2C1203.1786%2C1203.2069%2C1203.0970%2C1203.1803%2C1203.4923%2C1203.6689%2C1203.1396%2C1203.0018%2C1203.6790%2C1203.3755%2C1203.2773%2C1203.0043%2C1203.2210%2C1203.6763%2C1203.6269%2C1203.3973%2C1203.4583%2C1203.1409%2C1203.5898%2C1203.1364%2C1203.0435%2C1203.0854%2C1203.3173%2C1203.3167%2C1203.6056%2C1203.6283%2C1203.3007%2C1203.2593%2C1203.0475%2C1203.3388%2C1203.5686%2C1203.0845%2C1203.3683%2C1203.3352%2C1203.4898%2C1203.6074%2C1203.2345%2C1203.0106%2C1203.0058%2C1203.4533%2C1203.5051%2C1203.2619%2C1203.1206%2C1203.3367%2C1203.4532%2C1203.6362%2C1203.3669%2C1203.5097%2C1203.3212%2C1203.1311%2C1203.1697%2C1203.4468%2C1203.0195%2C1203.2126%2C1203.6000%2C1203.1280%2C1203.0343%2C1203.6168%2C1203.3162%2C1203.0508%2C1203.1838%2C1203.3379%2C1203.6558%2C1203.1757%2C1203.5012%2C1203.6704%2C1203.2761%2C1203.2144%2C1203.3380%2C1203.4684%2C1203.2293%2C1203.0989%2C1203.4937%2C1203.0966%2C1203.4469%2C1203.3252%2C1203.0187%2C1203.5409%2C1203.6612%2C1203.0117%2C1203.5576%2C1203.4567%2C1203.1563%2C1203.3696%2C1203.3216%2C1203.4030%2C1203.1584%2C1203.5735%2C1203.6590%2C1203.5159%2C1203.5344%2C1203.3253%2C1203.1694%2C1203.5285%2C1203.0814%2C1203.2631%2C1203.0185%2C1203.3974%2C1203.2264&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Learning a Common Substructure of Multiple Graphical Gaussian Models"}, "summary": "Properties of data are frequently seen to vary depending on the sampled\nsituations, which usually changes along a time evolution or owing to\nenvironmental effects. One way to analyze such data is to find invariances, or\nrepresentative features kept constant over changes. The aim of this paper is to\nidentify one such feature, namely interactions or dependencies among variables\nthat are common across multiple datasets collected under different conditions.\nTo that end, we propose a common substructure learning (CSSL) framework based\non a graphical Gaussian model. We further present a simple learning algorithm\nbased on the Dual Augmented Lagrangian and the Alternating Direction Method of\nMultipliers. We confirm the performance of CSSL over other existing techniques\nin finding unchanging dependency structures in multiple datasets through\nnumerical simulations on synthetic data and through a real world application to\nanomaly detection in automobile sensors.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.4206%2C1203.3730%2C1203.1786%2C1203.2069%2C1203.0970%2C1203.1803%2C1203.4923%2C1203.6689%2C1203.1396%2C1203.0018%2C1203.6790%2C1203.3755%2C1203.2773%2C1203.0043%2C1203.2210%2C1203.6763%2C1203.6269%2C1203.3973%2C1203.4583%2C1203.1409%2C1203.5898%2C1203.1364%2C1203.0435%2C1203.0854%2C1203.3173%2C1203.3167%2C1203.6056%2C1203.6283%2C1203.3007%2C1203.2593%2C1203.0475%2C1203.3388%2C1203.5686%2C1203.0845%2C1203.3683%2C1203.3352%2C1203.4898%2C1203.6074%2C1203.2345%2C1203.0106%2C1203.0058%2C1203.4533%2C1203.5051%2C1203.2619%2C1203.1206%2C1203.3367%2C1203.4532%2C1203.6362%2C1203.3669%2C1203.5097%2C1203.3212%2C1203.1311%2C1203.1697%2C1203.4468%2C1203.0195%2C1203.2126%2C1203.6000%2C1203.1280%2C1203.0343%2C1203.6168%2C1203.3162%2C1203.0508%2C1203.1838%2C1203.3379%2C1203.6558%2C1203.1757%2C1203.5012%2C1203.6704%2C1203.2761%2C1203.2144%2C1203.3380%2C1203.4684%2C1203.2293%2C1203.0989%2C1203.4937%2C1203.0966%2C1203.4469%2C1203.3252%2C1203.0187%2C1203.5409%2C1203.6612%2C1203.0117%2C1203.5576%2C1203.4567%2C1203.1563%2C1203.3696%2C1203.3216%2C1203.4030%2C1203.1584%2C1203.5735%2C1203.6590%2C1203.5159%2C1203.5344%2C1203.3253%2C1203.1694%2C1203.5285%2C1203.0814%2C1203.2631%2C1203.0185%2C1203.3974%2C1203.2264&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Properties of data are frequently seen to vary depending on the sampled\nsituations, which usually changes along a time evolution or owing to\nenvironmental effects. One way to analyze such data is to find invariances, or\nrepresentative features kept constant over changes. The aim of this paper is to\nidentify one such feature, namely interactions or dependencies among variables\nthat are common across multiple datasets collected under different conditions.\nTo that end, we propose a common substructure learning (CSSL) framework based\non a graphical Gaussian model. We further present a simple learning algorithm\nbased on the Dual Augmented Lagrangian and the Alternating Direction Method of\nMultipliers. We confirm the performance of CSSL over other existing techniques\nin finding unchanging dependency structures in multiple datasets through\nnumerical simulations on synthetic data and through a real world application to\nanomaly detection in automobile sensors."}, "authors": ["Satoshi Hara", "Takashi Washio"], "author_detail": {"name": "Takashi Washio"}, "author": "Takashi Washio", "arxiv_comment": "47 pages, 6 figures, elsarticle.cls", "links": [{"href": "http://arxiv.org/abs/1203.0117v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1203.0117v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1203.0117v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1203.0117v3", "journal_reference": null, "doi": null, "fulltext": "arXiv:1203.0117v3 [stat.ML] 25 Sep 2012\n\nLearning a Common Substructure of Multiple Graphical\nGaussian Models\nSatoshi Haraa , Takashi Washioa\na\n\nInstitute of Scientific and Industrial Research (ISIR), Osaka University, Osaka,\n5670047, Japan\n\nAbstract\nProperties of data are frequently seen to vary depending on the sampled\nsituations, which usually changes along a time evolution or owing to environmental effects. One way to analyze such data is to find invariances, or\nrepresentative features kept constant over changes. The aim of this paper\nis to identify one such feature, namely interactions or dependencies among\nvariables that are common across multiple datasets collected under different\nconditions. To that end, we propose a common substructure learning (CSSL)\nframework based on a graphical Gaussian model. We further present a simple learning algorithm based on the Dual Augmented Lagrangian and the\nAlternating Direction Method of Multipliers. We confirm the performance\nof CSSL over other existing techniques in finding unchanging dependency\nstructures in multiple datasets through numerical simulations on synthetic\ndata and through a real world application to anomaly detection in automobile\nsensors.\nKeywords: Graphical Gaussian Model, Common Substructure, Dual\nAugmented Lagrangian, Alternating Direction Method of Multipliers\n1. Introduction\nIn several real world data, such as that from the stock market (Baillie and Bollerslev,\n1989), gene regulatory networks (Ahmed and Xing, 2009; Zhang et al., 2009),\nbiomedical measurements (Varoquaux et al., 2010), or sensors in engineering\nsystems (Id\u00e9 et al., 2009), there are dynamical properties over time evolutions or due to changes in the surrounding environments. Such effects cause\ndata to have different behaviors in each dataset collected under different conditions. One way to analyze such data is to explicitly include the change into\nPreprint submitted to Neural Networks\n\nSeptember 26, 2012\n\n\fthe model (Hamilton, 1994; Durbin et al., 2001), which usually requires detailed domain knowledge that is rarely available in most cases. Another way\nis to impose general and mild assumptions on the data. This kind of approach\nis especially common in the multi-task learning literatures (Caruana, 1997;\nTurlach et al., 2005), where the relationships among datasets are treated as\na clue for combining multiple tasks into a single problem. The scope of the\npresent paper is in the latter context where the relationship among datasets\nis the objective we want to analyze. For the purpose, we focus on invariance\nof the data against the underlying changes which provides partial yet important aspects of the data behaviors (von B\u00fcnau et al., 2009; Hara et al.,\n2012). We provide a technique for finding one of such invariance, specifically\nconstant interactions or dependencies among variables across several different\nconditions. An illustrative example is an engineering system where system errors are observed as dependency anomalies in sensor values (Id\u00e9 et al., 2009),\nwhich are usually caused by a fault in a subsystem. The invariance, which\nin this example is the remaining healthy subsystems, is captured by a steady\ndependency over the multiple datasets sampled before and after the error\nonset. Hence, we can use such information as a clue for finding erroneous\nsubsystems.\nGraphical modeling is a popular approach for analyzing dependencies in\nmultivariate data (Lauritzen, 1996). We adopt one of the most fundamental models, a graphical Gaussian model (GGM), as the basis of our framework. A GGM is a basic model representing linear dependencies among\ncontinuous random variables, and has been widely studied owing to the simple nature, that is, the dependency structure is represented by the zero\npatterns in an inverse covariance matrix. Identification of such zero patterns from data was first studied by Dempster (1972) as a Covariance Selection where the task is formulated as the combinatorial problem of optimizing the location of zeros in a matrix. Since classical algorithms for this\ndo not scale to high dimensional data, the scope of studies has shifted to\na relaxed setting (Meinshausen and B\u00fchlmann, 2006; Yuan and Lin, 2007;\nBanerjee et al., 2008), where Covariance Selection is formulated as a convex optimization problem using a l1 -regularization that induces zeros in the\nresulting matrix. Because of the effectiveness of the relaxed formulation, several related optimization techniques have also been studied (Friedman et al.,\n2008; Duchi et al., 2008a; Li and Toh, 2010; Scheinberg and Rish, 2010; Yuan,\n2009; Scheinberg et al., 2010; Hsieh et al., 2011).\nIn our context, the objective is not to estimate the structure of a GGM\n2\n\n\ffrom a single dataset, but to decompose the resulting GGMs from several\ndatasets into common and individual substructures, with the former representing the invariance we aim to detect. There are some prior studies on\nlearning a set of GGMs from multiple datasets. Varoquaux et al. (2010) and\nHonorio and Samaras (2010) imported the idea of Group-Lasso (Yuan and Lin,\n2006; Bach, 2008) and Multitask-Lasso (Turlach et al., 2005; Liu et al., 2009),\nand extended the framework of a single GGM setting. In both cases, the problem is formulated under the assumption that all matrices share the same\nzero patterns. Guo et al. (2011) considered a method to avoid this additional assumption, although the problem then loses convexity. Though these\napproaches achieved some success in improving the estimation accuracy of\ngraphical models, this does not necessarily mean that they are suitable for\nfinding commonness across datasets as we will see in the simulation. In\nthe context of common substructure detection, Zhang and Wang (2010) proposed using a Fused-Lasso (Tibshirani et al., 2005) type of technique to find\nan invariant pattern between two datasets. As a general framework for N\ndatasets situations, Chiquet et al. (2011) considered imposing sign coherence on the resulting structures, while Hara and Washio (2011) extended\nthe framework of Zhang and Wang (2010) to the general situation of N\ndatasets 1 . In the opposite context where the target is dynamics rather than\ninvariance, Zhou et al. (2010) proposed using weighted statistics to trace the\nevolution of a GGM. We note there are also several related studies in the\nbinary Markov random field literatures (Guo et al., 2007; Ahmed and Xing,\n2009). They also use l1 -regularization (Wainwright et al., 2007) and FusedLasso type techniques (Ahmed and Xing, 2009) for recovering temporal dependency structures, which are technically quite close to the ones of GGM.\nThe contribution of this paper is two folds. First, we introduce the novel\nCommon Substructure Learning (CSSL) framework that is applicable for a\ngeneral case of N datasets. Second, a sophisticated algorithm based on the\nDual Augmented Lagrangian (DAL) (Tomioka et al., 2011) and the Alternating Direction Method of Multipliers (ADMM) (Gabay and Mercier, 1976;\nBoyd et al., 2011) is proposed. In the proposed algorithm, the inner problems for each iterative update are simple and can be solved efficiently which\n1\n\nThis paper is an extension of Hara and Washio (2011) with more general settings, an\nefficient optimization algorithm, and exhaustive simulations on synthetic and real world\ndatasets.\n\n3\n\n\fTable 1: Mathematical Notation\n\nNotation Description\nkxkp\nkAkp\nkAkS\nkBk1,p\nA\u227b0\nsgn (a)\ndiag (x)\n\nd\n\n\u0010P\n\nd\ni=1\n\np\n\n\u0011 p1\n\nlp -norm of a vector x \u2208 R , kxkp =\n|xi |\nfor p \u2208 [1, \u221e) and kxk\u221e = max1\u2264i\u2264d |xi |\nvectorized lp -norm of a matrix A \u2208 Rd\u00d7d ,\nkAkp = (A11 , A12 , . . . , Add )\u22a4 p\nspectral norm of a matrix A \u2208 Rd\u00d7d ,\nkAkS = max1\u2264i\u2264d \u03c3i (A) where \u03c3i (A) is\nan ith singular value of A\nl1,p -norm of matrices B = {Bi ; Bi \u2208 Rd\u00d7d }N\ni=1 ,\nPd\n\u22a4\nkBk1,p = j,j \u2032=1 (B1,jj \u2032 , B2,jj \u2032 , . . . , BN,jj \u2032 ) p\na matrix A is symmetric and positive definite\nsign function on a scalar a, sgn (a) = 1 for a > 1,\nsgn (a) = \u22121 for a < 0 and sgn (a) = 0 for a = 0\nd \u00d7 d matrix with x \u2208 Rd on its diagonal\n\nresults in fast computation. We confirm the validity of the CSSL approach\nthrough simulations on synthetic datasets and on an anomaly detection task\nin real-world data.\nThe remainder of the paper is organized as follows. In Section 2, we briefly\nreview properties of GGMs and existing learning techniques. In Section 3,\nwe present the proposed framework and its theoretical properties. The optimization algorithm based on DAL-ADMM is introduced in Section 4. The\nvalidity of the proposed method is presented through synthetic experiments\nin Section 5. In Section 6, we apply the proposed method to an anomaly\ndetection task on sensor error data. Finally, we conclude the paper in Section 7.\n2. Structure Learning of Graphical Gaussian Model\nIn this section, we review the GGM estimation problem (Meinshausen and B\u00fchlmann,\n2006; Yuan and Lin, 2007; Banerjee et al., 2008) and some prior extensions\nto multiple datasets (Varoquaux et al., 2010; Honorio and Samaras, 2010;\nZhang and Wang, 2010).\nWe also summarize mathematical notations used throughout the paper\nin Table 1.\n\n4\n\n\f2.1. Graphical Gaussian Model\nIn multivariate analysis, covariance and correlation are commonly used\nas indicators for a relationship between two random variables. However, in\ngeneral, a covariance between two random variables xj and xj \u2032 is affected by\nother variables. Therefore, we need to remove such effects to estimate an essential dependency structure, which is available by searching for conditional\ndependency among random variables. In a general graphical model, we express these dependencies using a graph with vertices corresponding to each\nrandom variable and edges spanning random variables that are conditionally\ndependent.\nHere, we assume that a d-dimensional random variable x = (x1 , x2 , . . . , xd )\u22a4\nfollows a zero mean Gaussian distribution, that is, x \u223c N (0d , \u039b\u22121) for some\nsymmetric and strictly positive definite matrix \u039b \u2208 Rd\u00d7d . We refer to a\ngraphical model of Gaussian variables as graphical Gaussian model (GGM)\nNote that the zero mean assumption can be achieved without loss of generality by subtracting a sample mean from the dataset. Here, a covariance matrix\nis parameterized as the inverse of a precision matrix \u039b since this is a more\nprimitive parameter representing essential dependency among variables. A\nprecision matrix relates to the conditional expectation as\n\u039bjj \u2032 \u221d \u2212E [xj xj \u2032 |other variables] ,\nthat is, the (j, j \u2032 )th entry of \u039b is proportional to the covariance between xj\nand xj \u2032 with the remaining d \u2212 2 variables fixed. With this property, the\nconditional independence between Gaussian random variables is expressed\nas zero entries of \u039b:\n\u22a5 xj \u2032 | other variables\n\u039bjj \u2032 = 0 \u21d4 xj \u22a5\nwhere \u22a5\n\u22a5 denotes statistical independence. Because of this property, the edge\npatterns in a GGM correspond to the non-zero entries in a precision matrix\n\u039b. In a GGM, two vertices have an edge between them if and only if the\ncorresponding (j, j \u2032 )th entry of \u039b is non-zero. In the case that only few pairs\nof variables are dependent, most off-diagonal elements in \u039b are zeros and the\ncorresponding graph expression is sparse, which allows us to visually inspect\nthe underlying relations.\n\n5\n\n\f2.2. Sparse Estimation of GGM\nA naive way to estimate a precision matrix \u039b is a maximum likelihood\nestimation formulated as\n\u039b\u0302 = argmax l(\u039b; S) ,\n\u039b\u2208P\n\nl(\u039b; S) = log det \u039b \u2212 tr [S\u039b] .\n\n(1)\n\nHere, l(\u039b; S) is a log-likelihood of a Gaussian distribution (up to a constant),\nS is a sample covariance matrix and P is a set of symmetric positive definite\nmatrices P = {A \u2208 Rd\u00d7d ; A \u227b 0}. The positive definiteness constraint is\nimposed so that the resulting \u039b is a valid precision matrix. For a strictly\npositive definite matrix S, the solution to this problem is \u039b\u0302 = S \u22121 . However,\nin a finite sample case, even when the true parameter is zero, that is, \u039bjj \u2032 = 0,\nits maximum likelihood estimator \u039b\u0302jj \u2032 is non-zero with probability one. In\nthis situation, the resulting graphical model is a complete graph, which states\nthat every pairs of variables is conditionally dependent and the underlying\nintrinsic relationships are masked.\nThe major scope of GGM studies is how to avoid this unfavorable result\nfrom a maximum likelihood estimation and infer a sparse graph structure,\nwhich is referred to as Covariance Selection (Dempster, 1972). In classical\nstudies, some entries of a precision matrix \u039b are fixed as zeros and the remaining non-zero entries are estimated, where the zero pattern is optimized in a\ncombinatorial manner. However, this combinatorial problem is not feasible\nfor high-dimensional data.\nIn recent studies, the use of an l1 -regularization has been shown to\nbe practical for Covariance Selection. The first such study was conducted\nby Meinshausen and B\u00fchlmann (2006). In their approach, the solution is\nobtained by solving the Lasso (Tibshirani, 1996). Here, let us denote d\u0002\n\u0003\u22a4\ndimensional data with n data points using an n\u00d7d matrix X = x1 x2 . . . xn ,\nwith Xj as its jth column and X\\j as its remaining d \u2212 1 columns. For each\ncolumn, we solve the following Lasso:\n1\nmin\nXj \u2212 X\\j \u03b8\n\u03b8 2\n\n2\n2\n\n+ \u03c1 k\u03b8k1 ,\n\n(2)\n\nwhere \u03c1 \u2265 0 is a regularization parameter. We then set zero patterns of \u03b8 to\nthe jth column of \u039b. Meinshausen and B\u00fchlmann (2006) have also showed\nthe asymptotic convergence of their estimator to the true graph structure\n6\n\n\funder a proper condition. This approach was later reformulated as an l1 regularized maximum likelihood problem (Yuan and Lin, 2007; Banerjee et al.,\n2008):\nmax l(\u039b; S) \u2212 \u03c1 k\u039bk1 .\n\n(3)\n\n\u039b\u2208P\n\nWe refer to this problem as Sparse Inverse Covariance Selection (SICS) following Scheinberg et al. (2010). The resulting precision matrix of (3) has\nsome zero entries owing to the effect of an additional l1 -regularization term.\nSeveral efficient optimization techniques are available for solving this problem. Examples include GLasso (Friedman et al., 2008), PSM (Duchi et al.,\n2008a), IPM (Li and Toh, 2010), SINCO (Scheinberg and Rish, 2010), ADMM\n(Yuan, 2009; Scheinberg et al., 2010) and QUIC (Hsieh et al., 2011).\n2.3. Learning a Set of GGMs with Same Topological Patterns\nThe ordinary SICS problem (3) aims to learn one GGM from a single\ndataset. The extension of this framework to multiple datasets has been\nstudied by Varoquaux et al. (2010) and Honorio and Samaras (2010). The\ntask is to estimate N precision matrices \u039b1 , \u039b2 , . . . , \u039bN from N datasets\nwhere the sample covariance matrices for each dataset are S1 , S2 , . . . , SN . The\nobjective of this multi-task extension is to improve the estimation accuracy of\neach GGM by incorporating the similarity among datasets. In the framework\nof the above studies, GGMs from each dataset are assumed to have the same\ntopological patterns, that is, the same edge connection structures while the\nedge weights might be different for each GGM. They both introduced a l1,p norm of a set of N precision matrices {\u039bi }N\ni=1\n1\n!\np\nd\nN\nX\nX\n|\u039bi,jj \u2032 |p\nk\u039bk1,p =\n,\nj,j \u2032 =1\n\ni=1\n\nas a regularization term analogous to the Group-Lasso (Yuan and Lin, 2006;\nBach, 2008) and Multitask-Lasso (Turlach et al., 2005; Liu et al., 2009) with\np \u2208 [1, \u221e]. Varoquaux et al. (2010) has considered the case p = 2 while\nHonorio and Samaras (2010) used p = \u221e. These two choices are commonly\nadopted in many scenarios owing to the computational efficiency. The entire\nestimation problem is defined as\nmax\n\n{\u039bi ;\u039bi \u2208P}N\ni=1\n\nN\nX\ni=1\n\nti l(\u039bi ; Si ) \u2212 \u03c1 k\u039bk1,p ,\n7\n\n(4)\n\n\fwith non-negative weights t1 , t2 , . . . , tN .PWithout loss of generality, we can\nlimit ourselves to the normalized case N\ni=1 ti = 1 since the unnormalized\nversion is just a scaled objective function for some constant. The typical\nchoice of parameters is ti = PNni n where ni is the number of data points\ni=1 i\nin the ith dataset. We refer to problem (4) as Multitask Sparse Inverse\nCovariance Selection (MSICS) in the remainder of the paper.\nNote that the MSICS problem (4) involves the ordinary SICS (3) as a\nspecial case when p = 1 where the l1,1 -regularization term completely decouples into N individual l1 -regularizations. In the extended case for p > 1,\n\u0011 1p\n\u0010P\nN\np\nthe regularization term enforces the joint structure \u039b\u0303jj \u2032 =\ni=1 |\u039bi,jj \u2032 |\n\nto be sparse, with \u039b\u0303jj \u2032 = 0 indicating that the corresponding (j, j \u2032 )th entries\nare zeros across all N precision matrices.\n\n2.4. Learning Structural Changes between Two GGMs\nAlthough taking advantage of situations with multiple datasets using the\npreceding techniques is useful for improving the estimation performances of\nthe resulting GGMs, it only imposes joint zero patterns and does not indicate anything about the commonness of the non-zero entries. It is therefore not that helpful when comparing GGMs representing similar models\nwhere we expect that there may exist some common edges whose weights are\nclose to each other. Zhang and Wang (2010) considered the two datasets\ncase and constructed an algorithm using a Fused-Lasso type regularization (Tibshirani et al., 2005) to round these similar values to be exactly the\nsame allowing only significantly different edges between two GGMs to be\nextracted. Their approach follows the ideas of Meinshausen and B\u00fchlmann\n(2006) by connecting the update procedure (2) for two datasets X1 and X2\nthrough a new regularization term for the variation between two parameters\nk\u03b8 1 \u2212 \u03b8 2 k1 ,\nmin\n\u03b8 1 ,\u03b8 2\n\n2 \u001a\nX\n1\ni=1\n\n2\n\nXi,j \u2212\n\n2\nXi,\\j \u03b8 i 2\n\n+ \u03c1 k\u03b8 i k1\n\n\u001b\n\n+ \u03b3 k\u03b8 1 \u2212 \u03b8 2 k1 ,\n\n(5)\n\nwhere \u03b3 \u2265 0 is a regularization parameter for the variation. The new term\nenforces the variation of some elements in two parameters to shrink to zeros.\nThey also provided a coordinate descent-based optimization procedure for\nthe above problem.\n8\n\n\f3. Learning Common Patterns in Multiple GGMs\nThe preceding work by Zhang and Wang (2010) adopted the idea of the\nFused-Lasso type technique using the specific formulation of the two datasets\nsituation. In our study, we introduce a new framework, a Common Substructure Learning (CSSL), for finding invariant patterns in multiple dependency\nstructures that is applicable to the general case of N datasets.\n3.1. Common Substructure Learning Problem\nWe first formalize what invariance we are aiming to detect in multiple dependency structures. To begin with, we assume that the number of variables\nin each dataset is the same, so they are all d-dimensional. Also, the identities\nof each variable are the same. For instance, x1 is always a value from the\nsame sensor while its behavior may change across datasets. We then define\na common substructure for multiple GGMs as follows.\nDefinition 1 (Common Substructure of Multiple GGMs). Let \u039b1 , \u039b2 ,\n. . . , \u039bN be precision matrices corresponding to each GGM. Then, the common substructure of the GGMs is expressed by an adjacency matrix \u0398 \u2208 Rd\u00d7d\ndefined as\n\u001a\n\u039b1,jj \u2032 , if \u039b1,jj \u2032 = \u039b2,jj \u2032 = . . . = \u039bN,jj \u2032\n.\n(6)\n\u0398jj \u2032 =\n0,\notherwise\nNote this is a natural extension of the invariance notion adopted in the\nprior work by Zhang and Wang (2010) for the case of two datasets. With\nan ordinal sparsity assumption for GGMs, this definition leads the precision\nmatrices to simultaneously have sparseness and commonness. That is:\n\u2022 Sparseness: \u039bi,jj \u2032 = 0 for some 1 \u2264 i \u2264 N and 1 \u2264 j, j \u2032 \u2264 d,\n\u2022 Commonness: \u039b1,jj \u2032 = \u039b2,jj \u2032 = . . . = \u039bN,jj \u2032 for some 1 \u2264 j, j \u2032 \u2264 d.\nUnder the above commonness, the basic idea of our framework is to\nparametrize each precision matrix \u039bi using two components, a common substructure \u0398 and an individual substructure \u03a9i \u2208 Rd\u00d7d :\n\u039bi = \u0398 + \u03a9i .\n\n(7)\n\nHere, each individual substructure matrix \u03a9i is composed of non-zero entries\nthat are not common across the N precision matrices.\n9\n\n\fIn the preceding formulation (5), some entries in the two precision matrices are shrunk to the same value owing to the effect of the term k\u03b8 1 \u2212 \u03b8 2 k1 .\nIn the proposed parameterization, such commonness corresponds to the case\nwhen some entries of the individual substructures are simultaneously zero,\nthat is, \u03a91,jj \u2032 = \u03a92,jj \u2032 = . . . = \u03a9N,jj \u2032 = 0. Hence, the non-zero common value\nis expressed by a common substructure matrix \u0398. These facts motivate us\nto regularize the individual substructures through the grouped regularization k\u03a9k1,p . On the other hand, we expect a common substructure \u0398 to be\nsparse so that we can interpret it easily. To that end, we adopt an ordinary\nl1 -regularization k\u0398k1 and the overall problem is summarized as follows:\nmax\n\n\u0398,{\u03a9i }N\ni=1\n\nN\nX\ni=1\n\nti l(\u0398 + \u03a9i ; Si ) \u2212 \u03c1 k\u0398k1 \u2212 \u03b3 k\u03a9k1,p\n\ns.t. \u0398 + \u03a9i \u2208 P (1 \u2264 i \u2264 N) ,\n\n(8)\n\nwith regularization parameters \u03c1, \u03b3 \u2265 0. Since \u2212l(\u0398 + \u03a9i ; Si ), k\u0398k1 and\nk\u03a9k1,p are all convex, the entire formulation is again a convex optimization problem. We refer to this problem as Common Substructure Learning\n(CSSL). Note that in the above formulation, we have slightly relaxed the\ncondition of commonness to allow \u0398jj \u2032 and \u03a9i,jj \u2032 to become simultaneously\nnon-zeros which is contrary to Definition (6). We correct this point by applying the criterion (6) to the resulting precision matrices \u039b\u03021 , \u039b\u03022 , . . . , \u039b\u0302N in\nthe post processing stage to extract only truly common entries.\nHere, we list two important properties of the CSSL problem (8), a dual\nproblem and the bound on eigenvalues. We first present the dual problem,\nwhich plays an important role in constructing an efficient optimization algorithm in the next section.\nPropostion 1 (Dual of CSSL). The dual problem of CSSL (8) is\nmin\n\n{Wi ;Wi \u2208P}N\ni=1\n\ns.t.\n\nN\nX\ni=1\n\nN\nX\ni=1\n\n\u2212\n\nN\nX\ni=1\n\nti log det Wi \u2212 d ,\n\nti (Wi,jj \u2032 \u2212 Si,jj \u2032 ) \u2264 \u03c1 ,\n\ntqi |Wi,jj \u2032 \u2212 Si,jj \u2032 |q\n\n! 1q\n\n\u2264 \u03b3 (1 \u2264 j, j \u2032 \u2264 d) ,\n10\n\n(9)\n\n\fwhere q is a parameter satisfying p\u22121 + q \u22121 = 1. The resulting matrices of\nthe dual problem Wi\u2217 are related to the optimal precision matrices \u039b\u2217i through\nthe inverse, \u039b\u2217i = Wi\u2217 \u22121 .\nIn both the primal and dual formulations (8), (9), we enforced the positive\ndefiniteness constraints, \u039bi = \u0398 + \u03a9i \u2208 P and Wi \u2208 P so that the matrices\nare valid precision or covariance matrices. Here, we show that they can be\ntightened according to the next theorem.\nTheorem 1 (Bounds on Eigenvalues). The optimal precision matrices for\n1\nthe CSSL (8) \u039b\u22171 , \u039b\u22172 , . . . , \u039b\u2217N with 0 < \u03c1 < N p \u03b3 < \u221e have bounded eigen\u2217\nmax\nvalues \u03bbmin\nId , where the bounding parameters \u03bbmin\nand \u03bbmax\ni Id \u0016 \u039b i \u0016 \u03bb i\ni\ni\nare\n1\n\n\u03bbmin\ni\n\nti\nN p d2\n=\n, \u03bbmax\n=\n.\ni\nti kSi kS + d\u03b3\n\u03c1\n\nUsing this result, we can replace the constraint \u039bi \u2208 P with the tighter\nd\u00d7d\n\u039bi \u2208 P\u0303i = {A \u2208 Rd\u00d7d ; A \u0017 \u03bbmin\n;A \u0017\ni Id }, and similarly Wi \u2208 {A \u2208 R\n\u22121\n\u03bbmax\nI\n}.\nNote\nthat\nthis\nupdate\nis\npractically\nimportant\nwhen\nconstructing\nd\ni\nan optimization algorithm. Since the new constraint set P\u0303i is closed, we\ncan project points out of the constraint set onto the boundary, which is\nunavailable for the original open set P.\n3.2. Interpretations of CSSL\nThe proposed CSSL problem (8) can be interpreted as a generalization of\nan ordinary SICS problem (3) and its multi-task extension MSICS (4). In the\ncase that \u03b3 \u2192 \u221e, the solution to the CSSL is \u03a91 = \u03a92 = . . . = \u03a9N = 0d\u00d7d ,\nwhich means that all precision matrices are equal and are represented by a\nsingle matrix \u0398. Such \u0398 is available by solving the SICS problem (3) with\nP\n1\np\nS= N\ni=1 ti Si . On the other hand, if \u03c1 \u2265 N \u03b3, the common substructure \u0398\nbecomes zero. This fact follows from the relationship between the lp -norms:\n1\n\n\u03b3 k\u0398 + \u03a9i k1,p \u2264 N p \u03b3 k\u0398k1 + \u03b3 k\u03a9k1,p \u2264 \u03c1 k\u0398k1 + \u03b3 k\u03a9k1,p .\nSuppose that the common substructure is non-zero, that is, \u0398 6= 0d\u00d7d , then\nthe above inequality means that the update \u03a9i \u2190 \u0398 + \u03a9i and \u0398 \u2190 0d\u00d7d\nimproves the objective function value (8) without changing the resulting\nprecision matrix \u039bi = \u0398+\u03a9i , and thus the solution must be \u0398 = 0d\u00d7d . Under\n11\n\n\fthis situation, the CSSL problem (8) coincides with MSICS (4). For the\n1\nproper parameters \u03c1 < N p \u03b3 < \u221e, the CSSL problem (8) is the intermediate\nof those two problems.\nThe CSSL problem can also be interpreted from a distributional perspective. From the relationship between the Lagrangian expression and the\nconstrained optimization problem, the CSSL problem (8) is equivalent to\nsolving a set of N maximum likelihood estimation problems (1) under the\nadditional constraints\nk\u0398k1 \u2264 \u03b7 , k\u03a9k1,p \u2264 \u03b7 \u2032 ,\n\n(10)\n\nfor some properly chosen positive constants \u03b7, \u03b7 \u2032 . Moreover, we have\nmax k\u03a9i \u2212 \u03a9 k1 \u2264\n\n1\u2264i<i\u2032 \u2264N\n\ni\u2032\n\nmax\n\n1\u2264i<i\u2032 \u2264N\n\nd\nX\n\nj,j \u2032 =1\n\n(|\u03a9i,jj \u2032 | + |\u03a9i\u2032 ,jj \u2032 |)\n\n\u2264 2 k\u03a9k1,\u221e \u2264 2 k\u03a9k1,p ,\nwhere the second inequality\ncomes from the fact that exchanging the order\nP\nof max1\u2264i<i\u2032 \u2264N and dj,j \u2032=1 produces the upper bound. The last inequality\nis an ordinary relationship between lp -norms. These relations and the fact\nthat \u039bi \u2212 \u039bi\u2032 = \u03a9i \u2212 \u03a9i\u2032 lead to the bound\nmax k\u039bi \u2212 \u039bi\u2032 k1 \u2264 2\u03b7 \u2032 .\n\n1\u2264i<i\u2032 \u2264N\n\nHence, from the result of Honorio (2011, Lemma 23) and general matrix norm\nrules, the left-hand side of this inequality can be interpreted as the upper\nbound of the KL divergence between two distributions pi (x) = N (0d , \u039b\u22121\ni )\n\u22121\nand pi\u2032 (x) = N (0d , \u039bi\u2032 ). With these properties, we can interpret the second\nconstraint in (10) as a constraint on the similarity among distributions:\nmax\nDKL (pi (x)||pi\u2032 (x)) \u2264 2\u03b7 \u2032 max k\u039b\u22121\ni kS ,\n\u2032\n\n1\u2264i,i \u2264N\n\n1\u2264i\u2264N\n\nwhere DKL (pi (x)||pi\u2032 (x)) denotes a KL divergence between two distributions\npi (x) and pi\u2032 (x). From Theorem 1, the optimal parameters \u039b\u22171 , \u039b\u22172 , . . . , \u039b\u2217N\nhave bounded spectral norms for a finite \u03b3, and thus this upper bound on the\nKL divergence is always valid. Moreover, we can further extend this bound\ninto the extreme case \u03b3 \u2192 \u221e and \u03b7 \u2032 \u2192 0. As we have discussed before,\nthis is the case \u03a91 = \u03a92 = . . . = \u03a9N = 0d\u00d7d and the problem is equivalent\n12\n\n\fPN\nto solving a single SICS problem for \u0398 with S =\ni=1 Si . Hence, from\nBanerjee et al. (2008, Theorem 1), we can see that the resulting precision\nmatrices still have finite eigenvalues for \u03c1 > 0, and the right hand side of the\nabove inequality goes to zero. This means that the resulting distributions\nrepresented by precision matrices derived from CSSL (8) have to be similar\nto one another at some level and they can be even identical in the extreme\ncase. Note that MSICS (4) is a special case of CSSL when \u0398 = 0d\u00d7d and\nthus the same upper bound holds, although there is the significant distinction\nthat the parameter \u03b7 \u2032 in MSICS (4) also affects the sparsity of the resulting\nprecision matrices while CSSL (8) can control the sparsity through the other\nhyper-parameter \u03c1.\n3.3. Connection to Additive Sparsity Models\nIn this section, we discuss some connections of the CSSL problem (8)\nto Additive Sparsity Models (Jalali et al., 2010; Chandrasekaran et al., 2010;\nAgarwal et al., 2011; Cand\u00e8s et al., 2011; Obozinski et al., 2011). In general\nadditive sparsity models, the objective parameter we want to estimate is\nmodeled as the sum of two components, as in (7). Hence, these two parameters are estimated using sparsity inducing norms such as an l1 -norm and a\ntrace-norm. In this sense, CSSL can be seen as a specific example of additive\nsparsity models where we use the combination of an l1 -regularization and a\ngroup-wise regularization.\nHere, we point out two close works from Jalali et al. (2010) and Chandrasekaran et al.\n(2010). The former considers the multi-task least squares regression problem\nunder the combination of l1 , group-wise regularizations. Their basic idea is\nquite close to ours in that some regression parameters can be close to each\nother across datasets. They also prove the advantage of combining two regularizations over using only one theoretically and numerically. The latter\nstudy is on GGMs but with different sparsity assumptions from ours. They\nshow that the additive sparsity model naturally appears in GGM when there\nare latent variables. In such a situation, the first component in the additive\nsparsity model corresponds to the precision matrix between observed variables while the latter component is an interaction between latent variables.\nThis insight is also available for interpreting our model (7), that is, a common interaction among observed variables is contaminated by the effect of\nlatent variables which are different for each dataset.\n\n13\n\n\f4. Optimization via DAL-ADMM\nIn this section, we present the optimization algorithm for solving the\nCSSL problem (9). Our basic approach here is to adopt the Augmented\nLagrangian techniques (Hestenes, 1969; Powell, 1967). In a prior study,\nTomioka et al. (2011) have shown that solving a dual problem using the\nAugmented Lagrangian, which is referred to as Dual Augmented Lagrangian\n(DAL), is preferable for the case when the primal loss is badly conditioned.\nSee Tomioka et al. (2011, Table 3) and the discussion therein. This is actually the case we are faced with, as summarized in the next theorem.\nP\nTheorem 2. The Hessian matrix of the CSSL primal loss function N\ni=1 ti l(\u0398+\n\u03a9i ; Si ) is rank-deficient while the Hessian matrix of the CSSL dual loss funcP\n1\np\ntion \u2212 N\ni=1 ti log det Wi is always full rank for 0 < \u03c1 < N \u03b3 < \u221e.\nThis fact motivates us to solve the dual problem rather than the primal problem. To that end, we construct an algorithm based on the DAL approach.\n\n4.1. DAL-ADMM Algorithm\nThe basic structure of the proposed algorithm is based on the idea of\nDAL. However, while the original DAL requires solving the inner problem\nalmost exactly (Tomioka et al., 2011), we take an alternative approach using\nADMM (Gabay and Mercier, 1976; Boyd et al., 2011) that makes the entire\nprocedure dramatically simple.\nTo begin with, we rewrite the CSSL dual problem (9) in the following\nequivalent form:\nmin\n\n{Wi ,Yi ;Wi \u2208P}N\ni=1\n\n\u2212\n\nN\nX\n\nti log det Wi\n\ni=1\n\ns.t. ti Wi \u2212 Yi \u2212 ti Si = 0 (1 \u2264 i \u2264 N) ,\n! q1\nN\nN\nX\nX\n\u2264 \u03b3 (1 \u2264 j, j \u2032 \u2264 d) .\n|Yi,jj \u2032 |q\nYi,jj \u2032 \u2264 \u03c1 ,\ni=1\n\n(11)\n\ni=1\n\nBased on this expression, we define the following Augmented Lagrangian\n\n14\n\n\ffunction:\nL\u03b2 (W, Y, Z) = \u2212\n\nN\nX\n\nti log det Wi + \u03b4\u03c1 (Y ) + \u03b4\u0303\u03b3q (Y )\n\ni=1\n\n\u0002\n\u0003 \u03b2\n+ tr Z \u22a4 (T W \u2212 Y \u2212 T \u03a3) + kT W \u2212 Y \u2212 T \u03a3k22 ,\n2\n\n(12)\n\nwhere \u03b2 is a nonnegative parameter and \u03a3, W , Y and Z are the concatenated\n\u0002\n\u0003\u22a4\n\u0002\n\u0003\u22a4\nmatrices \u03a3 = S1 S2 . . . SN\n, W = W1 W2 . . . WN\n, Y =\n\u0003\u22a4\n\u0003\u22a4\n\u0002\n\u0002\nY1 Y2 . . . YN\n, and T is as the matrix\nand\u0001Z = Z1 Z2 . . . ZN\n\u22a4\nT = diag [t1 , t2 , . . . , tN ] \u2297 Id , where \u2297 denotes the Kronecker product and\nId is the d-dimensional identity matrix. We also defined the functions \u03b4\u03c1 (Y )\nand \u03b4\u0303\u03b3q (Y ) as\n(\nPN\n\u2032\n0 , if\ni=1 Yi,jj \u2032 \u2264 \u03c1 for 1 \u2264 j, j \u2264 d ,\n\u03b4\u03c1 (Y ) =\n\u221e , otherwise\n\uf8f1\n\u00111\n\u0010P\n\uf8f2\nq q\nN\n\u2032\n|\n0\n,\nif\n|Y\n\u2264 \u03b3 for 1 \u2264 j, j \u2032 \u2264 d .\nq\ni,jj\ni=1\n\u03b4\u0303\u03b3 (Y ) =\n\uf8f3 \u221e , otherwise\n\nIn the Augmented Lagrangian function (12), the optimal precision matrix \u039b\u2217i\nis represented by the optimal dual variable Zi\u2217 . This can be verified through\na simple calculation. We set the derivative of the unaugmented Lagrangian\nL0 (W, Y, Z) over Wi to zeros and find that\nWi\u2217 \u22121 = Zi\u2217 ,\nwhich implies that \u039b\u2217i = Zi\u2217 from Proposition 1. This follows since the\nsolution to (11) must be the saddle point of the unaugmented Lagrangian\nfunction L0 (W, Y, Z).\nWe solve problem (11) using ADMM by iteratively applying the following\nthree steps until convergence:\n\uf8f1 (k+1)\nW\n\u2208 argmin L\u03b2 (W, Y (k) , Z (k) )\n\uf8f4\n\uf8f4\n\uf8f2\n{Wi ;Wi \u2208P}N\ni=1\n(k+1)\n(k+1)\n.\nY\n\u2208\nargmin\nL\n, Y, Z (k) )\n\u03b2 (W\n\uf8f4\n\uf8f4\nY\n\u0001\n\uf8f3 (k+1)\nZ\n= Z (k) + \u03b2 T W (k+1) \u2212 Y (k+1) \u2212 T \u03a3\n15\n\n\fHence, using ADMM, convergence of the dual variable Z to the optimal parameter Z \u2217 is guaranteed as the number of iterations tends to infinity (Boyd et al.,\n2011, Section 3.2). This means we can find the optimal precision matrices\n\u039b\u22171 , \u039b\u22172 , . . . , \u039b\u2217N using DAL-ADMM. In the following two subsections, we give\nthe update procedures for W and Y .\n4.2. Inner Optimization Problem: Update of W\nThe update of W can be factorized into N independent problems where\neach problem defines an update of Wi :\nh\ni \u03b2\n2\n(k) \u22a4\n(k)\nmin \u2212ti log det Wi + ti tr Zi Wi +\nti Wi \u2212 Yi \u2212 ti Si .\nWi \u2208P\n2\n2\nBy setting the derivative over Wi to zero, we obtain\n\u0012\n\u0013\n1 (k)\n1 (k)\n1\nWi \u2212\nYi \u2212\nZ i + Si \u2212\nW \u22121 = 0d\u00d7d .\nti\n\u03b2ti\n\u03b2ti i\n(k)\n\n(k)\n\nNow, write the eigen-decomposition as t1i Yi \u2212 \u03b2t1i Zi + Si = P DP \u22a4 with\nD = diag (\u03c31 , \u03c32 , . . . , \u03c3d ) and P \u22a4 P = P P \u22a4 = Id . Then, the above matrix\nequation has a solution of the form Wi = P D\u0303P \u22a4 with D\u0303 = diag (\u03c3\u03031 , \u03c3\u03032 , . . . , \u03c3\u0303d ).\n\u22121\nThe equation for each eigenvalue is \u03c3\u0303m \u2212\u03c3m \u2212 \u03b2t1i \u03c3\u0303m\n= 0 (1 \u2264 m \u2264 d), which\nhas the analytic solution\nq\n2 + 4\n\u03c3m + \u03c3m\n\u03b2ti\n.\n\u03c3\u0303m =\n2\nNote the positive definiteness of Wi is automatically fulfilled since \u03c3\u0303m > 0\nfor \u03b2 > 0.\n4.3. Inner Optimization Problem: Update of Y\nThe update of Y is formulated as\nh\ni \u03b2\n\u22a4\nmin \u03b4\u03c1 (Y ) + \u03b4\u0303\u03b3q (Y ) \u2212 tr Z (k) Y +\nT W (k+1) \u2212 Y \u2212 T \u03a3\nY\n2\n\n2\n2\n\n,\n\nor equivalently, the projection Y = proj (Y0 , A) of Y0 = T W (k+1) + \u03b21 Z (k) \u2212T \u03a3\n\u001b\nn\n\u00111\n\u0010P\n\u0002\n\u0003\u22a4 PN\nq q\nN\n\u2032\nonto the set A = Y = Y1 Y2 . . . YN\n;\n\u2264 \u03b3 , \u2200j, j ,\ni=1 Yi,jj \u2032 \u2264 \u03c1 ,\ni=1 |Yi,jj \u2032 |\nwhere proj(\u2217, \u2217) is a projection function defined as\n1\nproj (V, B) = argmin kU \u2212 V k22 .\nU \u2208B 2\n16\n\n\fTable 2: Solutions to problem (13) for q = 1, 2 and \u221e: see the corresponding appendix\nfor further details. An operator T\u03b3 (\u2217) in y \u2208 \u2202C2 for q = \u221e is a thresholding for each y0,i ,\nthat is, yi = sgn (y0,i ) min(|y0,i |, \u03b3).\n\nq=1\n\nq=2\n\ny0 \u2208 C\ny \u2208 \u2202C1\ny \u2208 \u2202C2\ny \u2208 \u2202C3\n\nq=\u221e\n\ny = y0\n\u0001\n\u22a4\n1\u22a4\nN y 0 \u2212 \u03c1 sgn 1N y 0\ny = y0 \u2212\n1N (Appendix A.1)\nN\n\u03b3\nContinuous Quadratic\ny\ny=\ny = T\u03b3 (y 0 )\nky 0 k2 0\nKnapsack Problem\n(Appendix A.4)\n(Appendix A.2)\n(Appendix A.3)\nContinuous Quadratic\nContinuous Quadratic\nAnalytic Solution\nKnapsack Problem\nKnapsack Problem\n(Appendix A.6)\n(Appendix A.5)\n(Appendix A.7)\n\nWe can further decompose this problem into O(d2 ) problems over y =\n(Y1,jj \u2032 , Y2,jj \u2032 , . . . , YN,jj \u2032 )\u22a4 for each (j, j \u2032 )th entry. Hence, each problem is\ny = proj (y 0 , C) ,\n\n(13)\n\nwhere y 0 is an N-dimensional vector with the ith component equal to y0,i =\n(k)\n(k+1)\nti Wi,jj \u2032 + \u03b21 Zi,jj \u2032 \u2212 ti Si,jj \u2032 , and where the constraint set is C = {u \u2208\nRN ; |1\u22a4\nN u| \u2264 \u03c1, kukq \u2264 \u03b3} with 1N being an N-dimensional vector of ones.\nFor any q \u2208 [1, \u221e], problem (13) has a trivial solution y = y 0 if y 0 \u2208 C.\nIn the remaining cases, that is, |1\u22a4\nN y 0 | > \u03c1 or ky 0 kq > \u03b3, the solution is\non the boundary of the constraint set \u2202C = {u; |1\u22a4\nN u| = \u03c1, kukq \u2264 \u03b3} \u2229\n\u22a4\n{u; |1N u| \u2264 \u03c1, kukq = \u03b3} owing to the convexity of the objective function.\nThus, the problem can be reduced to a search of the boundary. However,\neven though the constraint set C is convex, it is an intersection of two sets\nand the shape of the boundary \u2202C is rather complicated. Therefore, we do\nnot search the boundary \u2202C directly, but solve a set of simpler problems\ninstead. The basic approach is to classify the boundary into three parts,\n\u22a4\n\u2202C1 = {u; |1\u22a4\n6 \u03c1, kukq = \u03b3} and\nN u| = \u03c1, kukq 6= \u03b3}, \u2202C2 = {u; |1N u| =\n\u22a4\n\u2202C3 = {u; |1N u| = \u03c1, kukq = \u03b3}. The problems we solve here are modified\nversions of (13), replacing the constraint with y \u2208 \u2202Cm for each m \u2208 {1, 2, 3}:\ny = proj (y 0 , \u2202Cm ) .\n\n(14)\n17\n\n\fNote that \u2202C1 and \u2202C2 involve infeasible solutions to the problem (13). For\nexample, a point y with kykq > \u03b3 is infeasible even if y \u2208 \u2202C1 , while these\nthree regions covers the entire boundary of the constraint set \u2202C \u2282 \u222a3m=1 \u2202Cm .\nThis guarantees that we can search the entire boundary \u2202C indirectly by\nsearching the sets \u2202Cm (m = 1, 2, 3) instead. Hence, if neither of the solutions\nto (14) for y \u2208 \u2202C1 and y \u2208 \u2202C2 are involved in C, the solution to (13) is\nin \u2202C3 . We can take advantage of this property to construct an efficient\nsolution procedure. We first solve problems (14) for y \u2208 \u2202C1 and y \u2208 \u2202C2 ,\nrespectively, and if neither of solutions is in C, then we solve (14) for y \u2208 \u2202C3 .\nIn this paper, we focus on the specific cases q = 1, 2 and \u221e, since efficient\nsolution procedures are available. In Table 2, we summarized the solutions\nto problem (13). For further details, see Appendix A.\n4.4. Convergence Criteria\nAlthough the asymptotic convergence of Z (k) as k \u2192 \u221e is theoretically\nguaranteed, in practice we need to stop the iteration at some point. A major\nstopping criterion is the duality-gap, the difference between the primal and\ndual objective function values. Let f (W ) be the objective function in (9)\nand let g(\u0398, \u03a9) be the one in (8). Then the duality-gap at the kth iteration\nis defined as\n\u2032\n\n\u2032\n\nduality-gap = f (W\u0303 (k) ) \u2212 max\ng(\u0398\u0303(k ) , \u03a9\u0303(k ) ) ,\n\u2032\n1\u2264k \u2264k\n\nwhere W\u0303 (k) , \u0398\u0303(k) and \u03a9\u0303(k) denote parameters estimated in the kth step after proper projections and transformations. We need these modifications of\nvariables since the estimators in intermediate steps are not necessarily feasible. For example, W (k) does not need to satisfy the constraints in (9) since\nthey are imposed only on a variable Y in the DAL-ADMM setting \u0010\n(11). The\n\u0011\n(k)\nprojected variable W\u0303 (k) is W\u0303 (k) = T \u22121 \u1ef8 (k) + \u03a3 where \u1ef8 (k) = proj Y0 , A\n(k)\n\nand Y0 = T (W (k) \u2212 \u03a3). The same goes for \u039b(k) = Z (k) . An estima(k)\ntor \u039bi is not\npositive definite, and thus we project them as\n\u0010 necessarily\n\u0011\n(k)\n(k)\n\u039b\u0303i = proj \u039bi , P\u0303i . This projection is available in the following man(k)\n\nner. Let \u039bi = P DP \u22a4 be an eigen-decomposition with a diagonal matrix\n(k)\nD = diag(\u03c31 , \u03c32 , . . . , \u03c3d ). Then the projected matrix is \u039b\u0303i = P D\u0303P \u22a4 , where\neach element of D\u0303 = diag(\u03c3\u03031 , \u03c3\u03032 , . . . , \u03c3\u0303d ) is \u03c3\u0303m = max(\u03c3m , \u03bbmin\ni ). For com(k)\n(k)\nputing the value of g(\u0398\u0303 , \u03a9\u0303 ), we need to further factorize \u039b\u0303(k) into \u0398\u0303(k)\n18\n\n\f(k)\n\nand \u03a9\u0303(k) . This can be computed in an element-wise manner. Let \u03b8 = \u0398\u0303jj \u2032 ,\n(k)\n\n(k)\n\n(k)\n\n(k)\n\n(k)\n\n\u03a9i,jj \u2032 = \u039b\u0303i,jj \u2032 \u2212 \u03b8 and \u03bb = (\u039b\u03031,jj \u2032 , \u039b\u03032,jj \u2032 , \u039b\u0303N,jj \u2032 )\u22a4 . Then the problem we need\nto solve is\nmin \u03c1|\u03b8| + \u03b3 k\u03bb \u2212 \u03b81N kp .\n\u03b8\n\nFor p = 1 and \u221e, this function is piecewise linear with breakpoints {0, \u03bb1, \u03bb2 , . . . , \u03bbN }\nmin \u03bb +max \u03bb\nand {0, i i 2 i\u2032 i\u2032 }, respectively. Hence, the optimal \u03b8 is one of these\nbreakpoints and can be found by searching the candidates. For the case\np = 2, the analytic solution is\n\uf8fc\n\uf8f1\nv\nu\n2\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n2\n2 \u03bb\u0303\n\uf8fd\n\u0010\n\u0011u\n\u03b3 2 (1\u22a4\nt \u22a4\nN \u03bb\u0303) \u2212 \u03c1\n1 \uf8f2 \u22a4\n\u22a4\n2\n2\n.\n1N \u03bb\u0303 \u2212 sgn 1N \u03bb\u0303\n(1N \u03bb\u0303) \u2212 N\n\u03b8=\n\uf8f4\nN\uf8f4\n\u03b3 2 N \u2212 \u03c12\n\uf8f4\n\uf8f4\n\uf8fe\n\uf8f3\nSome other useful gaps are provided by Boyd et al. (2011). The primalgap measures how much the equality constraints in (11) is fulfilled,\nprimal-gap = T W (k) \u2212 Y (k) \u2212 T \u03a3\n\n2\n\n,\n\nwhile the dual-gap is a degree of the feasibility condition of the solution,\ndefined as\ndual-gap = \u03b2 T (Y (k+1) \u2212 Y (k) )\n\n2\n\n.\n\nIn our simulations in Sections 5 and 6, we have evaluated both criteria.\nWe set two threshold parameters \u01ebgap and \u01ebpdgap , and evaluated the conditions\nduality-gap \u2264 \u01ebgap and max(primal-gap, dual-gap) \u2264 \u01ebpdgap in each iteration.\nIf one of two conditions is fulfilled, we regard the iteration as converged and\noutput the result. In the simulations in Sections 5 and 6, we set \u01ebgap = 10\u22125 d\nand \u01ebpdgap = 10\u22125 .\n4.5. Computational Complexity\nIn this section, we summarize the computational complexity of the proposed algorithm. In the W update step, the computational cost is dominated\nby the eigen-decomposition of a d \u00d7 d matrix, which requires O(d3 ) operations, so the overall complexity is O(Nd3 ) for the update of N matrices. In\nthe Y update step, we need a projection proj (Y0 , A) which is divided into\n19\n\n\fO(d2 ) subproblems. For both q = 1 and q = \u221e, the most computationally\nexpensive procedure is solving the continuous quadratic knapsack problem\nwhich requires sorting O(N) elements and has complexity O(N ln N) 2 . In\nthe case q = 2, the update is analytically available with O(N) complexity.\nThe overall complexity for the Y update is thus O((N ln N)d2 ) for q = 1, \u221e\nand O(Nd2 ) for q = 2. The complexity for the Z update is O(Nd2 ). In the\n(k)\nconvergence check, we need to calculate the projection proj(\u039bi , P\u0303i ) which\n3\nhas O(d3 )\u0010 complexity\n\u0011 or O(Nd ) for N matrices. We also need the projec(k)\ntion proj Y0 , A which is again O((N ln N)d2 ) for q = 1, \u221e and O(Nd2 )\nfor q = 2. Summarizing the above results, we conclude that the computational complexity of one update in DAL-ADMM is O(Nd3 + (N ln N)d2 ) for\nq = 1, \u221e and O(Nd3 ) for q = 2. In many practical situations, the number\nof datasets N is in the tens, while the dimensionality of the data d can be\na few hundred. In such cases, ln N \u226a d holds, and the entire complexity is\napproximately O(Nd3 ). We note this is the least necessary complexity. For\nan unregularized setting, the solution \u039b\u2217i is a maximum likelihood estimate\nSi\u22121 , which requires O(d3 ) complexity for a matrix inverse and O(Nd3 ) for\nN matrices.\nDespite the theoretical complexity, the choice of \u03b2 is of practical importance since it affects the number of iterations needed until convergence. We\npropose using the heuristic from Boyd et al. (2011). In this heuristic, we\nupdate the value of \u03b2 = \u03b2 (k) in every steps following the next rule:\n\uf8f1\n\uf8f2 2\u03b2 (k) , if primal-gap \u2265 10 \u2217 dual-gap\n(k+1)\n\u03b2\n=\n0.5\u03b2 (k) , if dual-gap \u2265 10 \u2217 primal-gap .\n\uf8f3\n\u03b2 (k) , otherwise\nWhile this does not give any theoretical guarantees on its performance, it\ndoes give us a pragmatic choice of \u03b2 and results in convergence with a smaller\nnumber of steps.\n4.6. Heuristic Choice of Hyper\u2013parameters\nIn the CSSL problem (8), the choice of hyper-parameters \u03c1 and \u03b3 affects\nthe resulting precision matrices. There are several approaches for choosing\nthese, such as cross-validation (Yuan and Lin, 2007; Guo et al., 2011) or the\n2\n\nSee Appendix A.2, Appendix A.5, and Appendix A.7.\n\n20\n\n\fBayesian information criterion (Guo et al., 2011). Apart from selection techniques, the following result gives us some insight into \u03c1 and \u03b3, and is helpful\nfor analyzing the data more intensively.\nPropostion 2. Let the bivariate common\nsubstructure\n\u0398\nsub\u0015\n\u0015\n\u0014 and individual\n\u0014\nu i \u03c9i\n0 \u03b8\n, and conand \u03a9i =\nstructures \u03a9i be in the forms \u0398 =\n\u03c9i vi\n\u03b8 0\nsider the following CSSL problem with regularizations only on off-diagonal\nentries:\nmax\n\n\u0398,{\u03a9i }N\ni=1\n\nN\nX\ni=1\n\nti l(\u0398 + \u03a9i ; Si ) \u2212 2\u03c1|\u03b8| \u2212 2\u03b3 k\u03c9kp\n\ns.t. \u0398 + \u03a9i \u2208 P (1 \u2264 i \u2264 N) ,\n\n(15)\n\nwhere \u03c9 = (\u03c91 , \u03c92 , . . . , \u03c9N )\u22a4 . Then the off-diagonal entries of the resulting\nprecision matrices \u03b8, \u03c9 have the following property:\nmax |ri | \u2264 \u03b3 and\n\n1\u2264i\u2264N\n\nN\nX\ni=1\n\nti ri \u2264 \u03c1 \u21d2 \u03b8 = 0, \u03c9 = 0N ,\n\nwhere ri is the off-diagonal entry of Si .\n\nAlthough the result is specific to the bivariate case, we can use this as\na guideline for choosing the hyper-parameters \u03c1 and \u03b3. It also shows that\n\u03c1 and \u03b3 are not independent of each other, but rather they should change\nPN\nsimultaneously proportional to max1\u2264i\u2264N |ri | and\ni=1 ti ri . In particular, if\neach matrix Si is multiplied by some positive constant c, the above condition\nindicates that \u03c1 and \u03b3 also need to be multiplied by c. Such scale invariance is\nmaintained only by a linear model between \u03c1 and \u03b3. Therefore, we construct\nthe following heuristic based on this linear model.\nPN\n1. Assume that the linear relation\ni=1 ti Si,jj \u2032 = s1 max1\u2264i\u2264N |Si,jj \u2032 |+s0\nholds for all entries 1 \u2264 j \u2264 j \u2032 \u2264 d for some s0 , s1 \u2208 R.\nn\no\nPN\n\u2032\nt\nS\n2. Estimate s0 , s1 with least squares regression using the tuples max1\u2264i\u2264N |Si,jj \u2032 |,\ni=1 i i,jj\n3. Parameterize \u03c1, \u03b3 as \u03c1 = max(s1 \u03b1 + s0 , 0) and \u03b3 = \u03b1 using a parameter\n\u03b1.\n\nThis procedure provides an efficient way of tuning \u03c1 and \u03b3 simultaneously\nthrough a single parameter \u03b1.\n21\n\n\f5. Simulation\nIn this section, we investigate the performance of the proposed CSSL approach in finding common substructures among datasets through numerical\nsimulations.\n5.1. Generation of Synthetic Data\nWe fist briefly summarize the data generation procedure for our simulations. For the synthetic data, we need N precision matrices with sparseness\nand commonness. We tackle this problem in a two-stage approach. We\nfirst generate a single sparse precision matrix, and then add some non-zero\nentries to make N matrices where the additional patterns are individual\nto each other 3 . After N precision matrices \u039b1 , \u039b2, . . . , \u039bN have been constructed, we generate N datasets from the corresponding Gaussian distributions N (0d , \u039b\u22121\ni ) for 1 \u2264 i \u2264 N.\n5.2. Baseline Methods and Evaluation Measurements\nIn the simulation, we adopt SICS (3) and MSICS (4) as baseline methods\nto compare with CSSL. Since neither method is designed for finding a common substructure, we apply a heuristic to extract the substructure \u0398\u0302 from\nthe estimated precision matrices \u039b\u03021 , \u039b\u03022 , . . . , \u039b\u0302N . Note that, in SICS, each \u039b\u0302i\nis estimated by solving (3) individually while the set of matrices is estimated\nsimultaneously in MSICS (4). Following is the heuristic criterion used:\n\u001a\n\u03b8\u0302jj \u2032 , if max1\u2264i<i\u2032 \u2264d |\u039b\u0302i,jj \u2032 \u2212 \u039b\u0302i\u2032 ,jj \u2032 | \u2264 \u01eb\n\u0398\u0302jj \u2032 =\n0 , otherwise\nwhere \u01eb is some given threshold. Here, to avoid selecting zero edges as parts of\na common substructure, we set \u03b8\u0302jj \u2032 to zero if \u039b\u03021,jj \u2032 = \u039b\u03022,jj \u2032 = . . . = \u039b\u0302N,jj \u2032 = 0\nand one otherwise. In our simulation, we select the threshold \u01eb from the\nresulting precision\nn matrices. Specifically, we ocompute variations of estimators\nfor each entry max1\u2264i<i\u2032 \u2264N |\u039b\u0302i,jj \u2032 \u2212 \u039b\u0302i\u2032 ,jj \u2032 |\n, and then set \u01eb as the\n1\u2264j\u2264j \u2032 \u2264d\n\n100\u01eb0 % quantile. This corresponds to considering the lower 100\u01eb0 % varied\nentries as common.\nIn our simulation, we evaluate the common substructure detection performance through precision, recall and the F-measure. While these values are\n3\n\nSee Appendix B for further details.\n\n22\n\n\fdefined based on the number of true positive, false positive and false negative\ndetections, we slightly modify these measurements. This is because finding\ncommon dependencies with higher amplitudes is much more important than\nfinding very small dependencies which can be approximated as zero in practice. To that end, we adopt following weighted measurements, namely WTP\n(weighted true positive), WFP (weighted false positive), and WFN (weighted\nfalse negative),\nWTP =\n\nd\nX\nj<j \u2032\n\nWFP =\n\nd\nX\nj<j \u2032\n\nJ \u0303c,jj \u2032 J \u0303p,jj \u2032 Jc,jj \u2032 max |\u039bi,jj \u2032 | ,\n1\u2264i\u2264N\n\nJ \u0303c,jj \u2032 J \u0303p,jj \u2032 (1 \u2212 Jc,jj \u2032 ) max |\u039bi,jj \u2032 | ,\n1\u2264i\u2264N\n\nd n\no\nX\nWFN =\nJ \u0303c,jj \u2032 (1 \u2212 J \u0303p,jj \u2032 ) + (1 \u2212 J \u0303c,jj \u2032 ) Jc,jj \u2032 max |\u039bi,jj \u2032 | ,\n1\u2264i\u2264N\n\nj<j \u2032\n\nwhere J \u0303c,jj \u2032 , J \u0303p,jj \u2032 and Jc,jj \u2032 are defined as\n\u0013\n\u0012\nmax |\u039b\u0302i,jj \u2032 \u2212 \u039b\u0302i\u2032 ,jj \u2032 | < \u01eb ,\nJ \u0303c,jj \u2032 = I\n1\u2264i<i\u2032 \u2264N\n\u0013\n\u0012\n \u0303\nJp,jj \u2032 = I max |\u039b\u0302i,jj \u2032 | > 0 ,\n1\u2264i\u2264N\n\u0013\n\u0012\nmax\n|\u039bi,jj \u2032 \u2212 \u039bi\u2032 ,jj \u2032 | = 0 .\nJc,jj \u2032 = I\n\u2032\n1\u2264i<i \u2264N\n\nHere, I(P ) is an indicator function that returns 1 for a true statement P and\n0 otherwise. The modified measurements in the simulation are defined using\nthese values as\nWTP\n,\nWTP + WFP\nWTP\n,\nRecall =\nWTP + WFN\nPrecision \u2217 Recall\nF-measure = 2\n.\nPrecision + Recall\nPrecision =\n\nIn the simulation, we also observe whether the zero pattern in the precision matrices is properly recovered using each method. We use the following\n23\n\n\fF-measure for this evaluation, which we refer to the \"F0 -measure\" to distinguish it from the one above:\nF0 -measure =\nTP =\n\nN X\nd\nX\ni=1\n\nFP =\n\nFN =\n\nI(\u039bi,jj \u2032 = 0)I(\u039b\u0302i,jj \u2032 = 0) ,\n\nj<j \u2032\n\nN X\nd\nX\ni=1\n\n2TP\n,\n2TP + FP + FN\n\nj<j \u2032\n\nN X\nd\nX\ni=1 j<j \u2032\n\nI(\u039bi,jj \u2032 6= 0)I(\u039b\u0302i,jj \u2032 = 0) ,\nI(\u039bi,jj \u2032 = 0)I(\u039b\u0302i,jj \u2032 6= 0) .\n\n5.3. Result\nWe conducted simulations for three cases with data dimensionality d =\n25, 50 and 100 where the number of datasets is fixed at N = 5. For each case,\nwe generate precision matrices \u039b1 , \u039b2, . . . , \u039bN to have 15% non-zero entries\non average. In the simulation, we randomly generate datasets 100 times and\napplied each method using several different hyper-parameters, where in each\nrun we set the number of data points in each dataset to be 5d. For CSSL,\nwe use the heuristic with a parameter \u03b1 varying from 10\u22122 to 10\u22120 over 41\nvalues. We also evaluate results for \u03c1 = \u03b1 and \u03b3 = \u221e to see the effect\nof \u03b3 in an extreme case. As discussed in\nPSection 3.2, this corresponds to\nsolving a single SICS problem with S = N\ni=1 ti Si and setting the result to\n\u039b\u03021 = \u039b\u03022 = . . . = \u039b\u0302N = \u039b\u0302. For SICS and MSICS, we set the value of \u03c1 as\n\u03c1 = \u03b1. For each method, we adopt the resulting precision matrices with 15%\nnon-zero entries among these 41 values of \u03b1. In SICS and MSICS, we also\nvary the thresholding parameter \u01eb0 between 0.5, 0.7 and 0.9.\nWe summarize the results in Table 3. From the table, we can see the\nclear advantage of CSSL for p = 2 and \u221e over the other methods. These two\nmethods show higher F-measures, which are from their higher precision and\nrecall. This contrasts with other methods, SICS and MSICS, which achieve\nhigh recall, but have relatively poor precision. This means that structure\ndetected by those methods involve not only true common substructure but\nalso many false detections. This shows the drawback of estimated precision\nmatrices derived through SICS and MSICS, that is, their estimators tend to\nbe highly varied even for true common entries while this is not the case for\n24\n\n\fCSSL. This phenomenon is especially significant in SICS, which can hardly\nfind common substructures owing to its highly varied estimators. The results\nfor MSICS under p = \u221e and \u01eb0 = 0.9 are still better than the others, although \u01eb0 = 0.9 means that 90% of estimated non-zero entries are considered\ncommon, which is too optimistic. Moreover, we can see that the improvement of the F-measure is achieved by the growth of recall by contrasting the\nresults with \u01eb0 = 0.5 and 0.9. This means that variations on the true common\nsubstructure mostly happens in between 50% and 90% of the entire variations of the estimated precision matrices, which are highly varied and can\nhardly be considered common. Note that despite the significant difference in\nthe common entry detection performance, all methods achieve comparable\nzero pattern identification performance as shown by the F0 -measure. This\nshows that finding common entries is a different problem from the ordinal\ngraphical model selection, and that only CSSL does well at both tasks.\nWe note that CSSL with p = 1 and \u03b3 = \u221e give two extreme results.\nIn the former setting, the resulting precision matrices achieve higher precision with lower recall, which is very conservative, while it is the opposite in\nthe latter setting. The first result is caused by the difference of a grouped\nregularization k\u03a9k1,p for p = 1 and p > 1. For p = 1, k\u03a9k1,p completely\ndecouples into ordinary l1 -regularizations and the resulting precision matrices do not necessarily have common zero entries in individual substructures. Intuitively speaking, the results for p = 1 have common zero entries\n\u03a91,jj \u2032 = \u03a92,jj \u2032 = . . . = \u03a9N,jj \u2032 = 0 only when it is strongly confident, which\nresults in a very conservative performance compared with p > 1. On the\nother hand, if \u03b3 = \u221e, the entire structures are considered to be common,\nwhich results in fewer false negatives and more false positives.\n6. Application to Anomaly Detection\nIn this section, we apply CSSL to an anomaly detection problem. The\ntask is to identify contributions of each variable to the difference between\ntwo datasets. Correlation anomalies (Id\u00e9 et al., 2009), or errors on dependencies between variables, are known to be difficult to detect using existing approaches, especially with noisy data. To overcome this problem, the\nuse of sparse precision matrices was proposed by Id\u00e9 et al. (2009), since the\nsparse approach reasonably suppresses the pseudo-correlation among variables caused by noise and improves the detection rate. Here, we propose\nusing CSSL. There is a clear indication that the proposed method can fur25\n\n\fther suppress the variation in the estimated matrices. In particular, we\nexpect that dependency structures among healthy variables are estimated to\nbe common, which reduces the risk that such variables are mis-detected and\nonly anomalies are enhanced.\n6.1. Anomaly Score\nWe adopt the measurement for correlation anomalies proposed by Id\u00e9 et al.\n(2009). This score is based on the KL-divergence between two conditional\ndistributions. Formally, let xA , xB \u2208 Rd be Gaussian random variables fol\u22121\n\u22121\nlowing N (0d , \u039bA ) and N (0d , \u039bB ), respectively. We measure the degree\nB\nof anomaly between their jth variables xA\nj and xj using a KL-divergence beA\nB B\nA\ntween their conditional distributions pA (xA\nj |x\\j ) and pB (xj |x\\j ), where x\\j\nand xB\n\\j are the remaining d \u2212 1 variables. To compute the score, we first\ndivide the precision matrix \u039bA and its inverse W A into a (d \u2212 1) \u00d7 (d \u2212 1)\ndimensional matrix, a d \u2212 1 dimensional vector, and a scalar,\n\u0014 A A \u0015\n\u0014 A A \u0015\nV\\j v \\j\nL\\j l\\j\nA\nA \u22121\nA\n,\n, W =\u039b\n=\n\u039b =\nA\nA\nvA\n\u03c3jA\nl\\j \u03bbj\n\\j\nwhere we have rotated the rows and columns of \u039bA and W A simultaneously\nso that their original jth rows and columns are located at the last rows and\ncolumns of the matrix. The matrices \u039bB and its inverse W B are also divided\nin a same manner. The score is then given as\nZ\nAB\nA\nA A\nB B\ndj = dxA\n\\j pA (x\\j ) DKL (pA (xj |x\\j )||pB (xj |x\\j ))\n\uf8f1\n\uf8fc\nB\u22a4 B B\nA\u22a4 A A \uf8fd\n\uf8f2\nl\\j V\\j l\\j\n1 l\\j V\\j l\\j\n\u22a4 A\nB\n= vA\n\u2212\n\\j (l\\j \u2212 l\\j ) +\nB\n\uf8fe\n2\uf8f3\n\u03bbj\n\u03bbA\nj\n(\n)\n\u03bbA\n1\nj\nB\n+\nln B + \u03c3jA (\u03bbA\n.\nj \u2212 \u03bbj )\n2\n\u03bbj\nHere, the KL-divergence is averaged over the remaining d \u2212 1 variables xA\n\\j .\nAB\nBA\nSince the KL-divergence is not symmetric and dj 6= dj holds in general,\nthe resulting anomaly score aj is decided as their maximum:\nBA\naj = max(dAB\nj , dj ) .\n\n26\n\n\f6.2. Simulation Setting\nWe evaluate the anomaly detection performance using sensor error data (Id\u00e9 et al.,\n2009). The dataset comprised 42 sensor values collected from a real car in\n79 normal states and 20 faulty states. The fault is caused by mis-wiring of\nthe 24th and 25th sensors, resulting in correlation anomalies. Since sample covariances are rank-deficient in some datasets, we added 10\u22123 on their\ndiagonal to avoid singularities.\nFor simulation, we randomly sample nn datasets from the normal states\nand nf datasets from the faulty states, and then estimate sparse precision\nmatrices using six methods, CSSL with p = 1, 2 and \u221e, SICS (3), and\nMSICS (4) with p = 2 and \u221e. For CSSL, we adopt the heuristic and set\n\u03c1 = max(s1 \u03b1 + s0 , 0) and \u03b3 = \u03b1 for a given \u03b1, and for SICS and MSICS, we\nset \u03c1 = \u03b1. We test each method for 11 different values of \u03b1 ranging from\n10\u22121.5 to 10\u22120.5 . The weight parameters ti in CSSL and MSICS are set as\nti = 2n1n for normal datasets and ti = 2n1 f for faulty datasets to balance the\neffects from the two states. Since the anomaly score is designed only for a\npair of datasets, we calculate anomaly scores for each of nn \u00d7 nf pairs of\ndatasets.\n6.3. Result\nWe repeated the above procedure 100 times for 4 different settings, [nn , nf ]\n= [4, 1], [12, 3], [20, 5] and [40, 10]. For each run, we evaluated the detection\nperformance of each method by drawing an ROC curve and measuring the\narea under the curve (AUC). In Table 4, we summarize the best median results for each method and setting. The table shows that CSSL with p = 2, \u221e\nand MSICS with p = \u221e achieve better detection performances than the others. In particular, CSSL with p = 2 and \u221e achieve AUC = 1 as their median\nperformance in some cases. This means that they detect faulty sensors perfectly for more than half of the simulation. To see further differences, we plot\nthe median anomaly scores derived from each method for [nn , nf ] = [20, 5] in\nFigure 1. From these graphs, we observe a clear distinction between successful methods and other methods on the significance of healthy sensors. The\n22nd and 28th sensors are relatively highly enhanced in SICS and MSICS\nwith p = 2, but are not in CSSL and MSICS with p = \u221e. We conjecture\nthat this is the major cause of performance differences. Interestingly, not\nonly the 22nd and 28th sensors but most of the other healthy sensors also\nhave the same tendencies. That is, CSSL and MSICS with p = \u221e reasonably\n27\n\n\f40\n\n10\n\n20\n30\nSensor ID\n\n40\n\nAnomaly Score\n\nAnomaly Score\n\n20\n30\nSensor ID\n\n(d) SICS\n\n40\n\n20\n30\nSensor ID\n\n40\n\n(c) CSSL (p = \u221e)\n\n(b) CSSL (p = 2)\n\n(a) CSSL (p = 1)\n\n10\n\n10\n\nAnomaly Score\n\n20\n30\nSensor ID\n\nAnomaly Score\n\nAnomaly Score\n\nAnomaly Score\n10\n\n10\n\n20\n30\nSensor ID\n\n(e) MSICS (p = 2)\n\n40\n\n10\n\n20\n30\nSensor ID\n\n40\n\n(f) MSICS (p = \u221e)\n\nFigure 1: Median anomaly scores for each method for [nn , nf ] = [20, 5] with best AUCs.\nEach plot is normalized so that the maximum is the same. Dotted lines denote true faulty\nsensors.\n\nsuppress their significance while keeping erroneous sensors enhanced. Moreover, although the differences are subtle, we can see that CSSL with p = 2\nand \u221e more successfully suppress the significance of sensors 1 to 21 and 33\nto 42 than does MSICS with p = \u221e. Thus, as we expected in the beginning,\nCSSL reduces the nuisance effects and highlights only those variables with\ncorrelation anomalies. The remaining peaks at some healthy variables are\ncaused by the effect of the two faulty sensors since their effects may propagate\nto other healthy yet highly related sensors.\n7. Conclusion\nIn this paper, we formulated the CSSL problem for multiple GGMs. We\nfurther provided a simple DAL-ADMM algorithm where each update step\ncan be solved in a very efficient manner. Numerical results on synthetic\ndatasets indicate the clear advantage of the CSSL approach, in that it can\nachieve high precision and recall at the same time, which existing GGM\nstructure learning methods can not achieve. We also applied the proposed\nCSSL technique to the anomaly detection task in sensor error data. Through\nthe simulation, we observed that CSSL could efficiently suppress nuisance\n\n28\n\n\feffects among variables in noisy sensors and successfully enhanced target\nfaulty sensors.\nSeveral future research topics have been indicated, including analyzing\nthe asymptotic property of the CSSL problem (8), and extending the current\nformulation to the adaptive Lasso (Zou, 2006; Fan et al., 2009) type one\nto guarantee the oracle property (Zou, 2006) of the estimator. Applying the\nnotion of commonness to more general dependency models, such as those with\nnon-linear relations or commonness based on higher-order moment statistics,\nis also important.\nAcknowledgments\nWe would like to acknowledge support for this project from the JSPS\nGrant-in-Aid for Scientific Research(B) #22300054. The authors would like\nto thank Tsuyoshi Id\u00e9 and his colleagues for providing sensor error datasets\nfor our simulation. We also received several helpful comments from Shohei\nShimizu.\nAppendix A. Solutions to (13) for q = 1, 2 and \u221e\nHere, we give detailed derivations of Table 2.\nAppendix A.1. The solution is in \u2202C1 .\nProblem (14) for y \u2208 \u2202C1 is formulated as follows:\n1\nmin ky \u2212 y 0 k22 s.t. |1\u22a4\nN y| = \u03c1 .\ny 2\n\n(A.1)\n\nNote that we have ignored the constraint kykq 6= \u03b3 because it holds for\ngeneral y 0 and \u03b3 with probability one. Hence, our interest is whether the\nsolution to (A.1) satisfies kykq \u2264 \u03b3 or not. The additional constraint is not\nimportant in this respect.\nThe problem (A.1) has two possible cases as its solution, 1\u22a4\nN y = \u03c1 and\n\u22a4\n1N y = \u2212\u03c1. For each case, we can solve the problem using Lagrange multipliers:\n1\nmin max ky \u2212 y 0 k22 + \u03bc(1\u22a4\nN y \u2212 \u03b6) ,\n\u03bc\ny\n2\n\n29\n\n\fwhere \u03b6 \u2208 {\u03c1, \u2212\u03c1}. By setting the derivative over y to zero, we get y =\ny 0 \u2212 \u03bc1N . Moreover, by substituting this result above, we derive the optimal \u03bc as \u03bc = N1 (1\u22a4\nN y 0 \u2212 \u03b6) and the resulting objective function value is\n\u00012\n1\n\u22a4\n1N y 0 \u2212 \u03b6 . The constraint \u03b6 = \u03c1 or \u03b6 = \u2212\u03c1 is chosen so that this\n2N\nobjective function value is minimized. Obviously, \u03b6 = \u03c1 is optimal for the\n\u22a4\ncase when 1\u22a4\nN y 0 \u2265 0, while \u03b6 = \u2212\u03c1 for 1N y 0 < 0. Thus, the overall solution\nto problem (A.1) is\n\u0001\n\u22a4\ny\ny\n\u2212\n\u03c1\nsgn\n1\n1\u22a4\n0\nN 0\ny = y0 \u2212 N\n1N .\nN\nAppendix A.2. The solution is in \u2202C2 for q = 1.\nWhen the solution is in \u2202C2 , the problem is formulated as\n1\nmin ky \u2212 y 0 k22 s.t. kykq = \u03b3 .\ny 2\n\n(A.2)\n\nHere, the shape of the constraint boundary changes according to the value\nof q. For general q \u2208 [1, \u221e], there exist several algorithms to solve this\nproblem (Boyd and Vandenberghe, 2004; Sra, 2011). Especially, for q = 1, 2\nand \u221e, solutions are available in a very efficient manner.\nFor q = 1, it has been shown by Honorio and Samaras (2010) that the\nproblem is equivalent to the following Continuous Quadratic Knapsack Problem:\nmin\nz\n\nN\nX\n1\ni=1\n\n2\n\n(zi \u2212 |y0,i |)2 s.t. z \u2265 0, 1\u22a4\nNz = \u03b3 ,\n\n(A.3)\n\nwhich relates to y by yi = sgn (y0,i ) zi . Honorio and Samaras (2010) have also\nprovided a solution technique for this problem. From the KKT condition, the\nsolution to (A.3) is zi (\u03bd) = max(|y0,i | \u2212 \u03bd, 0) for some constant \u03bd. Moreover,\n\u22a4\nthe optimal \u03bd satisfies 1\u22a4\nN z(\u03bd) = \u03b3. Since 1N z(\u03bd) is a decreasing piecewise\nlinear function with breakpoints |y0,i |, we can find a minimum breakpoint \u03bd0\nthat satisfies 1\u22a4\nN z(\u03bd0 ) \u2264 \u03b3 by sorting the N breakpoints. The optimal \u03bd is\nthen given as\nP\n|y0,i | \u2212 \u03b3\n,\n\u03bd = i\u2208I0\n|I0 |\n30\n\n\fwhere I0 = {i; |y0,i | \u2212 \u03bd0 \u2265 0}. Note that the complexity of this algorithm is\nO(N log N) since we conduct a sorting of N values 4 .\nAppendix A.3. The solution is in \u2202C2 for q = 2.\nThe solution to problem (A.2) for q = 2 is analytically available. We\nsolve the problem using Lagrange multipliers:\n1\n\u03bb\nmin max ky \u2212 y 0 k22 + (kyk22 \u2212 \u03b3 2 ) .\ny \u03bb 2\n2\nBy setting the derivative over y to zero, we get y =\nthe constraint kyk2 = \u03b3, the solution is\ny=\n\n1\ny .\n1+\u03bb 0\n\nMoreover, from\n\n\u03b3\ny .\nky 0 k2 0\n\nAppendix A.4. The solution is in \u2202C2 for q = \u221e.\nThe solution of (A.2) for the case q = \u221e is much simpler. The problem\nis just a box-constrained least squares, with solution\n\uf8f1\n(if y0,i > \u03b3)\n\uf8f2 \u03b3\ny0,i (if |y0,i | \u2264 \u03b3)\nyi =\n\uf8f3\n\u2212\u03b3 (if y0,i < \u2212\u03b3)\n\nwhich is equivalent to yi = sgn (y0,i ) min(|y0,i |, \u03b3).\n\nAppendix A.5. The solution is in \u2202C3 for q = 1.\nWe provide the solution procedure for (14) for y \u2208 \u2202C3 and q = 1 based\non the next theorem.\nTheorem 3. Let \u1ef9 be the solution to problem (14) for y \u2208 \u2202C1 , and suppose\nit is infeasible in the original problem (13). Then, the solution to (14) for\ny \u2208 \u2202C3 has same signs as \u1ef9, that is, \u1ef9i yi \u2265 0 for 1 \u2264 i \u2264 N.\n4\n\nWe can further reduce this to expected linear time complexity by introducing a randomized algorithm (Duchi et al., 2008b).\n\n31\n\n\fFrom this result, we can factorize the variable indices into two parts,\nI+ = {i; \u1ef9i \u2265 0} and I\u2212 =P\n{i; \u1ef9i < 0}. Using this\nthe objective\nP factorization,\n2\nfunction is expressed as 21 i\u2208I+ (yi \u2212y0,iP\n)2 + 12 i\u2208IP\n(y\n\u2212y\n)\n.\nThe equality\ni\n0,i\n\u2212\nconstraints can\ni\u2208I+ yi +\ni\u2208I\u2212 yi = \u03b6, with \u03b6 \u2208\nP also bePexpressed as\n{\u03c1, \u2212\u03c1} and i\u2208I+ yi \u2212 i\u2208I\u2212 yi = \u03b3. From these expressions, we derive two\nindependent problems:\nX\n\u00012\n1X +\n\u03b3+\u03b6\n+\nmin\nyi+ =\ny\n\u2212\ny\ns.t.\ny\n\u2265\n0\n,\n,\n0,i\ni\n+\n2\ny 2 i\u2208I\ni\u2208I\n+\n+\nX\n\u00012\n\u03b3\u2212\u03b6\n1X \u2212\n\u2212\n\u2212\ny\n+\ny\ny\n=\ns.t.\ny\n\u2265\n0\n,\n.\nmin\n0,i\ni\ni\n2\ny \u2212 2 i\u2208I\ni\u2208I\n\u2212\n\n\u2212\n\nThe solutions to these problems relate to y in that yi = yi+ for i \u2208 I+ and\nyi = \u2212yi\u2212 for i \u2208 I\u2212 . These problems are continuous quadratic knapsack\nproblems and the solution can be found by using the same algorithm as in\nproblem (A.3). We derive the final solution by solving these problems for the\ntwo cases \u03b6 = \u03c1 and \u03b6 = \u2212\u03c1, and choosing the one with the smaller objective\nfunction value in (14).\nAppendix A.6. The solution is in \u2202C3 for q = 2.\nThe solution in the case y \u2208 \u2202C3 and q = 2 is analytically available. We\nuse Lagrange multipliers:\n1\n\u03bb\n2\n2\nmin max ky \u2212 y 0 k22 + \u03bc(1\u22a4\nN y \u2212 \u03b6) + (kyk2 \u2212 \u03b3 ) ,\ny \u03bc,\u03bb 2\n2\nwhere \u03b6 \u2208 {\u03c1, \u2212\u03c1}. By setting the derivative over y to zero, we get y =\n1\u22a4N y 0\n1\n(y\n\u2212\n\u03bc1\n).\nIf\n\u03c1\n=\n0,\nwe\nhave\n\u03bc\n=\nfrom the constraint 1\u22a4\nN\n0\nN y = 0.\n1+\u03bb\nN\nHence, from kyk2 = \u03b3, we get the optimal y as\ny=\n\n\u03b3\n(y \u2212 \u03bc1N ) .\nky 0 \u2212 \u03bc1N k2\n\n\u03b6\nfrom the constraint 1\u22a4\nN y = \u03b6.\n1\u22a4N y 0 \u2212N \u03bc\nHence, we have a quadratic equation in \u03bc from the constraint kyk22 = \u03b3 2 :\n\nFor the case \u03c1 > 0, we have\n\n1\n1+\u03bb\n\n=\n\n2\n\u03c12 kv \u2212 \u03bc1N k22 = \u03b3 2 (1\u22a4\nN v \u2212 N\u03bc) .\n\n32\n\n\fSolving this equation gives the optimal y as\n\u221a\n\u03b6\n1 \b \u22a4\ny= \u22a4\n1N y 0 \u00b1 \u03c4\n(y 0 \u2212 \u03bc1N ) , \u03bc =\nN\n1N y 0 \u2212 N\u03bc\n2\n\n\u22a4\n\n2\n(1\u22a4\nN y0)\n\nwhere \u03c4 =\n\u2212N\n2\nky \u2212 y 0 k2 , we have\nky \u2212\n\ny 0 k22\n\n,\n\n\u03b3 2 (1N y 0 )2 \u2212\u03c12 ky 0 k\n\n2\n\n\u03b3 2 N \u2212\u03c12\n\n. By substituting this result into\n\n2\n\u00012 N ky 0 k22 \u2212 (1\u22a4\n\u221a \u00012\n1\nN y0)\n\u22a4\n=\n\u03b6 \u2212 1N y 0 +\n\u03b6\u00b1 \u03c4 .\nN\nN\u03c4\n\n2\nSince N ky 0 k22 \u2212 (1\u22a4\nN y 0 ) \u2265 0, the minimum\n\u0001 of this value\u22a4 is \u0001achieved by\n\u22a4\nchoosing \u03b6 and a sign in \u03bc as \u03b6 = sgn 1N y 0 \u03c1 and \u2212sgn 1N y 0 . Thus, the\noverall result is\n\u0001\n\u03c1\n(y 0 \u2212 \u03bc1N ) ,\ny = sgn 1\u22a4\ny\nN 0\n\u22a4\n1N y 0 \u2212 N\u03bc\n\u0001\u221a\n1 \b \u22a4\n\u03bc=\n1N y 0 \u2212 sgn 1\u22a4\n\u03c4 .\nN y0\nN\nAppendix A.7. The solution is in \u2202C3 for q = \u221e.\nThe solution for (14) with y \u2208 \u2202C3 and q = \u221e has two possible cases,\n1\u22a4\ny\n= \u03c1 and 1\u22a4\nN\nN y = \u2212\u03c1, where for each case the problem is:\n\nmin\ny\n\nN\nX\n1\ni=1\n\n2\n\n(yi \u2212 y0,i )2 s.t. 1\u22a4\nN y = \u03b6 , \u2212\u03b31N \u2264 y \u2264 \u03b31N ,\n\n(A.4)\n\nwith \u03b6 \u2208 {\u03c1, \u2212\u03c1}. Here, the constraint kyk\u221e = \u03b3 is relaxed to kyk\u221e \u2264 \u03b3.\nHowever, if the solution to (A.4) satisfies kyk\u221e < \u03c1, it has already been\nfound as a solution to (14) for y \u2208 \u2202C1 and therefore this relaxation does not\naffect the overall procedure.\nSince problem (A.4) is a variant of the continuous quadratic knapsack\nproblem, a similar strategy to (A.3) is available. From the KKT condition,\nthe solution to (A.4) is of the form yi (\u03bd) = sgn (y0,i \u2212 \u03bd) min(|y0,i \u2212 \u03bd|, \u03b3) for\n\u22a4\nsome constant \u03bd. Moreover, the optimal \u03bd satisfies 1\u22a4\nN y(\u03bd) = \u03b6. Since 1N y(\u03bd)\nis a decreasing piecewise linear function with breakpoints {y0,i \u2212\u03b3, y0,i +\u03b3}N\ni=1 ,\n\u22a4\nwe can find a minimum breakpoint \u03bd0 that satisfies 1N y(\u03bd0 ) \u2264 \u03b6 by sorting\nthe 2N breakpoints. The optimal \u03bd is then\n\uf8f1 P\n\uf8f2\ni\u2208I2 y0,i + \u03b3(|I1 | \u2212 |I3 |) \u2212 \u03b6\n(if I2 6= \u03c6)\n\u03bd=\n|I2 |\n\uf8f3\n\u03bd0\n(if I2 = \u03c6)\n33\n\n\fwhere I1 = {i; y0,i \u2212 \u03bd0 \u2265 \u03b3}, I2 = {i; \u2212\u03b3 \u2264 y0,i \u2212 \u03bd0 < \u03b3} and I3 =\n{i; y0,i \u2212 \u03bd0 < \u2212\u03b3}.\nAppendix B. Generation of Synthetic Precision Matrices\nHere, we present the detailed procedure used to generate the sparse precision matrices with a common substructure in Section 5. The procedure is\ncomposed of two sequential steps. We first generate a single precision matrix,\nwhich is the common substructure in the resulting N matrices. Then, we add\nsome non-zero entries to get a matrix \u039bi . This additional pattern is chosen\nto be unique for each matrix so that the resultant matrices \u039b1 , \u039b2 , . . . , \u039bN\nsatisfy the additive model assumption (7). In the following two subsections,\nwe explain the above steps.\nAppendix B.1. Generation of a Sparse Precision Matrix\nIn several previous studies, synthetic sparse precision matrices are generated in a naive manner, that is, just adding a properly scaled identity matrix\nto a sparse symmetric matrix so that the resulting matrix is sparse and positive definite (Banerjee et al., 2008; Wang et al., 2009; Li and Toh, 2010). In\nour simulations, we take a different approach to generating a sparse precision\nmatrix for compatibility with the next step.\nOur approach is based on an eigen-decomposition \u039b = V DV \u22a4 , where\nD is a matrix with eigenvalues on its diagonal and V is an orthonormal\nmatrix such that V \u22a4 V = V V \u22a4 = Id . Here, we use the fact that \u039b is sparse\nif V is sufficiently sparse and the problem can be reduced to generating a\nsparse orthonormal matrix V . This can be done easily by applying a Givens\nrotation (Golub and Van Loan, 1996) to an identity matrix Id . Formally, we\nlet V (0) = Id and apply the following procedure repeatedly until the desired\nsparsity is achieved.\n1. Randomly pick two indices j, j \u2032 from {1, 2, . . . , d}.\n2. Randomly generate \u03b8 from a uniform distribution U([0, 2\u03c0]).\n3. Update the (j, j), (j, j \u2032 ), (j \u2032 , j) and (j \u2032 , j \u2032)th entries of V (k) as\n\"\n#\n#\n\u0014\n\u0015 \" (k)\n(k+1)\n(k+1)\n(k)\nVjj\nVjj \u2032\nVjj Vjj \u2032\ncos \u03b8 \u2212 sin \u03b8\n\u2190\n.\n(k+1)\n(k+1)\n(k)\n(k)\nsin \u03b8 cos \u03b8\nVj \u2032 j\nVj \u2032 j \u2032\nVj \u2032 j Vj \u2032 j \u2032\n(k+1)\n\n4. Keep the remaining entries Vj0 j \u2032\n0\n(j \u2032 , j), (j \u2032 , j \u2032 ) }.\n\n34\n\n(k)\n\n\u2190 Vj0 j \u2032 for (j0 , j0\u2032 ) \u2208\n/ { (j, j), (j, j \u2032 ),\n0\n\n\fIn our simulations, we generated each eigenvalue from a uniform distribution\nU([0, 1]).\nAppendix B.2. Generation of Sparse Precision Matrices with a Common\nSubstructure\nHere, we turn to imposing commonness on the resulting precision matrices. To begin with, we generate small sparse precision matrices \u03a81 , \u03a82 , . . . , \u03a8a\nin the preceding manner and construct a sparse block-diagonal precision matrix \u039b0 = block \u2212 diag (\u03a81 , \u03a82 , . . . , \u03a8a ). We then add some non-zero entries\nto \u039b0 and generate N precision matrices \u039b1 , \u039b2 , . . . , \u039bN . At this stage, we\nkeep the original non-zero entries \u039b0 unchanged so they form a common substructure at the end. Note that the addition of non-zero entries can not be\ndone randomly since this might destroy the positive definiteness.\nWe describe the procedure for the case a = 2. Let the eigen-decompositions\nof \u03a81 and \u03a82 be \u03a81 = V1 D1 V1\u22a4 and \u03a82 = V2 D2 V2\u22a4 . Note that V1 and V2\nare sparse since\u0014 they are \u0015generated to be so. Now, let matrix \u039bi be of\n\u03a81 \u03a6i\nthe form \u039bi =\n. The objective is to generate a sparse non-zero\n\u03a6\u22a4\n\u03a82\ni\nmatrix \u03a6i while keeping the positive definiteness of \u039bi . This corresponds\nto keeping a determinant of \u039bi positive. Here, we choose \u03a6i of the form\n\u03a6i = \u1e7c1b \u039ei \u1e7c2b \u22a4 where \u039ei is a b \u00d7 b diagonal matrix and \u1e7c1b and \u1e7c2b are matrices composed\nof b columns in\u0003 V1 and V2\u0002, respectively. Specifically,\nwe\n\u0003\n\u0002\nlet V1 = v 1,1 v 1,2 . . . v 1,d1 and V2 = v 2,1 v 2,2 . . . v 2,d2 , where\nd1 and\u0002 d2 denote the dimensionality\nof each \u0002matrix. Then \u1e7c1b and \u1e7c2b are\n\u0003\n\u0003\n\u1e7c1b = v 1,\u03c01,1 v 1,\u03c01,2 . . . v 1,\u03c01,b and \u1e7c2b = v 2,\u03c02,1 v 2,\u03c02,2 . . . v 2,\u03c02,b ,\nrespectively, for some index sets {\u03c01,1 , \u03c01,2 , . . . , \u03c01,b } \u2286 {1, 2, . . . , d1 }, { \u03c02,1 ,\n\u03c02,2 , . . . , \u03c02,b } \u2286 {1, 2, . . . , d2 }. Then, from a general matrix property, we can\nexpress the determinant of \u039bi as\n\u0001\n\u22a4\ndet \u039bi = det \u03a81 \u2212 \u03a6i \u03a8\u22121\n2 \u03a6i\n\u0001\n= det D1 \u2212 V1\u22a4 \u03a6i V2 D2\u22121 V2\u22a4 \u03a6\u22a4\nV\n1\ni\n\u0012\n\u0013\nb\n2\nY\n\u03bei,m\n=\n\u03c31,\u03c01,m \u2212\n,\n\u03c3\n2,\u03c0\n2,m\nm=1\nwhere D1 = diag(\u03c31,1 , \u03c31,2 , . . . , \u03c31,d1 ), D2 = diag(\u03c32,1 , \u03c32,2 , . . . , \u03c32,d2 ) and \u039ei =\ndiag(\u03bei,1, \u03bei,2 , . . . , \u03bei,b ). Hence, the positive definiteness of \u039bi is guaranteed if\n2\n\u03bei,m\n< \u03c31,\u03c01,m \u03c32,\u03c02,m is satisfied for 1 \u2264 m \u2264 b. Moreover, this inequality\n35\n\n\fprovides us a guideline on choosing index sets. Since we want non-zero entries\nof \u03a6i to be larger, which can be achieved by larger |\u03bei,m |, we choose index sets\nso that \u03c31,\u03c01,m \u03c32,\u03c02,m large. This corresponds to choosing leading eigenvalues\nand eigenvectors of \u03a81 and \u03a82 . In our simulations, we pick b = 2 indices\nat random from those with eigenvalues in the top 1/3. We also generate\n\u221a\n\u03bei,m as \u03bei,m = \u03be0,i,m \u03c31,\u03c01,m \u03c32,\u03c02,m , where \u03be0,i,m follows a uniform distribution\nU([\u22120.8, \u22120.5] \u222a [0.5, 0.8]).\n(1)\nFor general a > 2 cases, we first construct a matrix \u039bi from \u03a81 and \u03a82 .\n(2)\n(1)\nWe then iteratively apply the above procedure to generate \u039bi from \u039bi and\n(3)\n(2)\n(a\u22121)\n\u03a83 , \u039bi from \u039bi and \u03a84 , until \u039bi = \u039bi\nis derived. In the simulations\nin Section 5, we set the number of modules to a = 2 for d = 25, a = 3 for\nd = 50 and a = 4 for d = 100.\nAppendix C. Proof of Theorems\nAppendix C.1. Proof of Proposition 1\nLet E and Fi be non-negative d\u00d7d matrices satisfying \u2212Ejj \u2032 \u2264 \u0398jj \u2032 \u2264 Ejj \u2032\nand \u2212Fi,jj \u2032 \u2264 \u03a9i,jj \u2032 \u2264 Fi,jj \u2032 , respectively, for all 1 \u2264 i \u2264 N and 1 \u2264 j, j \u2032 \u2264 d.\nThen, using Lagrange multipliers \u0393, \u03930 , and {\u2206i , \u22060,i }N\ni=1 , the CSSL problem\n(8) is expressed as\nmax\n\nmin\n\nN\n\u0398,E,{\u03a9i ,Fi }N\ni=1 \u0393,\u03930 ,{\u2206i ,\u22060,i }i=1\n\n\u2212\n\nN\nX\ni=1\n\nd\nX\n\nj,j \u2032 =1\n\nti {log det(\u0398 + \u03a9i ) \u2212 tr [Si (\u0398 + \u03a9i )]}\n\n\uf8f1\n\uf8f2\n\u03c1Ejj \u2032 + \u03b3\n\uf8f3\n\nN\nX\ni=1\n\np\nFi,jj\n\u2032\n\n! p1 \uf8fc\n\uf8fd\n\uf8fe\n\n\u2212 tr [\u0393\u0398] + tr [abs(\u0393)E] + tr [\u03930 E]\n\u2212\n\nN\nX\ni=1\n\n{tr [\u2206i \u03a9i ] \u2212 tr [abs(\u2206i )Fi ] \u2212 tr [\u22060,i Fi ]}\n\ns.t. \u03930,jj \u2032 \u2265 0 , \u22060,.i,jj \u2032 \u2265 0 (1 \u2264 i \u2264 N , 1 \u2264 j, j \u2032 \u2264 d) .\nBy changing the order of maximization and minimization above, we derive\nthe dual problem. Now, we optimize each variable \u0398, E, \u03a9i and Fi by setting\n\n36\n\n\feach derivative to zero:\nN\nX\n\b\nti (\u0398 + \u03a9i )\u22121 \u2212 Si \u2212 \u0393 = 0d\u00d7d ,\ni=1\n\n\u2212 \u03c11d 1\u22a4\nd + abs(\u0393) + \u03930 = 0d\u00d7d ,\n\b\nti (\u0398 + \u03a9i )\u22121 \u2212 Si \u2212 \u2206i = 0d\u00d7d (1 \u2264 i \u2264 N) ,\n! 1\u2212p\np\nN\nX\np\n\u2212\u03b3\nFi,jj \u2032\nFi,jj \u2032 + |\u2206i,jj \u2032 | + \u22060,i,jj \u2032 = 0\ni=1\n\n(1 \u2264 i \u2264 N , 1 \u2264 j, j \u2032 \u2264 d) .\n\nAs a result of these equations, we get\n\b\n\u2206i = ti (\u0398 + \u03a9i )\u22121 \u2212 Si ,\nN\nX\ni=1\n\n\u2206i,jj \u2032 \u2264 \u03c1 (1 \u2264 j, j \u2032 \u2264 d) ,\n\nN\nX\ni=1\n\n|\u2206i,jj \u2032 |q\n\n! 1q\n\n\u2264 \u03b3 (1 \u2264 j, j \u2032 \u2264 d) .\n\nand so the dual problem is given by (9) where we set Wi = (\u0398 + \u03a9i )\u22121 =\n1\n\u2206 + Si .\n\u0003\nti i\nAppendix C.2. Proof of Theorem 1\nWe first prove the lower-bound. Let Wi = t1i \u2206i + Si in the dual prob\u00111\n\u0010P\nPN\nN\nq q\n\u2032\n\u2264 \u03b3, and\n|\n|\u2206\n\u2264\n\u03c1\nand\n\u2206\nlem (9). Then we have\ni,jj\ni,jj\ni=1\ni=1\nhence\n1\n\u2206i + Si\nti\n\nS\n\n1\nk\u2206i kS + kSi kS\nti\nd\n|\u2206i,jj \u2032 | + kSi kS\n\u2264 max\nti j,j \u2032\nd\n\u2264 max max\n|\u2206i,jj \u2032 | + kSi kS\nti i j,j \u2032\nd\u03b3\n\u2264\n+ kSi kS ,\nti\n\n\u2264\n\n37\n\n\fwhere the last inequality comes from the general relationship between lp \u00111\n\u0010P\nN\nq q\n\u2032\nnorms maxi |\u2206i,jj \u2032 | \u2264\n|\n|\u2206\n. Since Wi\u2217 = t1i \u2206\u2217i + Si = \u039b\u2217i \u22121 holds\ni,jj\ni=1\nat the optimum, we have the lower-bound.\nWe now turn to proving the upper-bound. From strong duality, the\nduality-gap is zero at the optimal solution to the primal and the dual problems (8), (9), and we have\n\u2217\n\n\u2217\n\n\u03c1 k\u0398 k1 + \u03b3 k\u03a9 k1,p = d \u2212\n\nN\nX\n\nti tr [Si (\u0398\u2217 + \u03a9\u2217i )] .\n\ni=1\n\n1\n\nMoreover, from 0 < \u03c1 < N p \u03b3 < \u221e, tr [Si (\u0398\u2217 + \u03a9\u2217i )] \u2265 0 and the general\n\u00111\n\u0010P\nN\np p\n\u2217\n\u2265 maxi |\u03a9\u2217i,jj \u2032 |,\nlp -norm rule\ni=1 |\u03a9i,jj \u2032 |\n1\n\nk\u0398\u2217 k1 + N \u2212 p k\u03a9\u2217 k1,\u221e \u2264\n\nd\n\u03c1\n\n1\n\nholds. Since N p \u2265 1 for p \u2265 1, we get\n1\n\n\u2217\n\n\u2217\n\nk\u0398 k1 + k\u03a9 k1,\u221e\n\nN pd\n.\n\u2264\n\u03c1\n\nWe use this inequality to derive the upper-bound. From the definition, the\nprecision matrix factorizes as \u039b\u2217i = \u0398\u2217 + \u03a9\u2217i , and hence we have\nk\u039b\u2217i kS \u2264 k\u0398\u2217 kS + k\u03a9\u2217i kS\n\u2264 k\u0398\u2217 kS + d max\n|\u03a9\u2217i,jj \u2032 |\n\u2032\nj,j\n\n\u2217\n\n\u2264 k\u0398 kS + d max max\n|\u03a9\u2217i,jj \u2032 |\n\u2032\ni\n\n\u2217\n\nj,j\n\n\u2217\n\n\u2264 k\u0398 kS + d k\u03a9 k1,\u221e\n\u0011\n\u0010\n\u2264 d k\u0398\u2217 kS + k\u03a9\u2217 k1,\u221e\n\u0011\n\u0010\n\u2217\n\u2217\n\u2264 d k\u0398 k1 + k\u03a9 k1,\u221e\n1\n\nN p d2\n\u2264\n\u03c1\n\nHere, we have used the relationship k\u0398\u2217 kS \u2264 k\u0398\u2217 k2 \u2264 k\u0398\u2217 k1 .\n38\n\n\u0003\n\n\fAppendix C.3. Proof of Theorem 2\nP\nThe Hessian matrix of the CSSL primal loss N\ni=1 ti l(\u0398 + \u03a9i ; Si ) is given\nby\n\uf8f9\n\uf8ee PN\nt1 K1 t2 K2\n...\ntN KN\ni=1 ti Ki\n\uf8ef\n...\n0d2 \u00d7d2 \uf8fa\nt1 K1\nt1 K1 0d2 \u00d7d2\n\uf8fa\n\uf8ef\n..\n\uf8fa\n\uf8ef\nHprimal = \u2212 \uf8ef\n2\n2\n.\nt2 K2\n0d \u00d7d t2 K2\n\uf8fa,\n\uf8fa\n\uf8ef\n.\n.\n..\n\uf8f0\n..\n..\n.\n0d2 \u00d7d2 \uf8fb\n...\n0d2 \u00d7d2 tN KN\ntN KN\n0d2 \u00d7d2\n\nwhere Ki = (\u0398 + \u03a9i )\u22121 \u2297 (\u0398 + \u03a9i )\u22121 . It is easy to verify that 1N +1 \u2297 Id spans\na null space of Hprimal and thus Hprimal is always rank-deficient.\nP\nOn the other hand, the matrix of the CSSL dual loss \u2212 N\ni=1 ti log det Wi\nis the block-diagonal matrix\nHdual = block\u2013diag(t1 K\u03031 , t2 K\u03032 , . . . , tN K\u0303N ) ,\nwhere K\u0303i = Wi\u22121 \u2297 Wi\u22121 . From Theorem 1, we know that the CSSL solution\nhas bounded eigenvalues and thus the above Hessian matrix is always strictly\npositive definite for any feasible Wi .\n\u0003\nAppendix C.4. Proof of the Proposition 2\n\u0015\n\u0014\nai ri\n. Then we have an upperLet Si be the covariance matrix Si =\nri bi\nbound for (15) of\nN\nX\n\b\nti log(ui vi \u2212 (\u03b8 + \u03c9i )2 ) \u2212 (ai ui + bi vi + 2ri \u03b8 + 2ri \u03c9i )\ni=1\n\n\u2212 2\u03c1|\u03b8| \u2212 2\u03b3 k\u03c9kp\n\n\u2264\n\nN\nX\n\b\nti log(ui vi \u2212 (\u03b8 + \u03c9i )2 ) \u2212 (ai ui + bi vi ) \u2212 2(ri \u03c9i + \u03b3|\u03c9i |)\ni=1\n\n\u22122\n\nN\nX\ni=1\n\nti ri \u03b8 + \u03c1|\u03b8|\n\n!\n\n,\n\nP\nfrom the relationship N\ni=1 ti |\u03c9i | \u2264 k\u03c9k\u221e \u2264 k\u03c9kp . Moreover, this upperbound coincides with the original problem when \u03c9 = 0N . Therefore, if\n39\n\n\f\u03c9 = 0N is a maximizer of this upper-bound, it is also a maximizer of (15).\nFrom the derivative of the upper-bound over \u03c9i , we get that \u03c9i = 0 is a\nmaximizer if the following condition holds:\n\u2212(\u03b3 + ri ) \u2264\n\n\u03b8\n\u2264 (\u03b3 \u2212 ri ) .\nui vi \u2212 \u03b82\n\nThis is a sufficient condition for the original problem (15) to have \u03c9i = 0 as\nits solution. Under this condition, problem (15) can be expressed as\nmax\n\n\u03b8,\u0169,\u1e7d,ui ,vi\n\nlog(\u0169\u1e7d \u2212 \u03b82 ) \u2212 (\u00e3\u0169 + b\u0303\u1e7d) \u2212 2(r\u0303\u03b8 + \u03c1|\u03b8|)\n\ns.t. \u0169\u1e7d \u2212 \u03b82 > 0 ,\n\n\u03b8\n\u2264 (\u03b3 \u2212 ri ) (1 \u2264 i \u2264 N)\nui vi \u2212 \u03b82\nP\nfor some properly chosen \u00e3, b\u0303 and r\u0303 = N\ni=1 ti ri . Hence, since the additional\ncondition involves \u03b8 = 0 irrelevant to the value of ui and vi if max1\u2264i\u2264N |ri | \u2264\n\u03b3 holds, we have \u03b8 = 0 when |r\u0303| \u2264 \u03c1 from Id\u00e9 et al. (2009, Proposition 1). \u0003\n\u2212 (\u03b3 + ri ) \u2264\n\nAppendix C.5. Proof of Theorem 3\nLet h(y) = 21 ky \u2212 y 0 k22 and y \u2032 be one of the feasible solutions to the original problem (13). Moreover, since \u1ef9 is infeasible for the original problem (13),\nk\u1ef9kq > \u03b3 holds. Then, for y \u2032\u2032 = y \u2032 + \u01eb(\u1ef9 \u2212 y \u2032 ) with 0 < \u01eb \u2264 1, h(y \u2032\u2032 ) \u2264 h(y \u2032 )\nholds from the convexity of h. Therefore, y \u2032\u2032 is a better solution to prob\u2032\u2032\n\u2032\u2032\nlem (13) as long as the constraints |1\u22a4\nN y | \u2264 \u03c1 and ky kq \u2264 \u03b3 are satisfied.\n\u22a4\n\u22a4 \u2032\n\u2032\u2032\nThe first condition always holds because |1\u22a4\nN y | \u2264 (1\u2212\u01eb)|1N y |+\u01eb|1N \u1ef9| \u2264 \u03c1.\n\u0010P\n\u0011 q1\nN\n\u2032\u2032 q\nOn the other hand, the latter condition ky \u2032\u2032 kq =\n\u2264 \u03b3 is no\ni=1 |yi |\nlonger valid if ky \u2032 kq = \u03b3 and sgn (yi\u2032 ) = sgn (\u1ef9i \u2212 yi\u2032 ), which results in \u1ef9i yi\u2032 \u2265 0.\nThis is a necessary condition for the solution to (13). Otherwise, we can always improve the solution by the above procedure, which contradicts its\noptimality.\n\u0003\nReferences\nAgarwal, A., Negahban, S., Wainwright, M., 2011. Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions. Proceedings\nof the 28th International Conference on Machine Learning, 1129\u20131136.\n40\n\n\fAhmed, A., Xing, E. P., 2009. Recovering time-varying networks of dependencies in social and biological studies. Proceedings of the National Academy\nof Sciences 106 (29), 11878\u201311883.\nBach, F. R., 2008. Consistency of the group lasso and multiple kernel learning. The Journal of Machine Learning Research 9, 1179\u20131225.\nBaillie, R. T., Bollerslev, T., 1989. Common stochastic trends in a system of\nexchange rates. The Journal of Finance 44 (1), 167\u2013181.\nBanerjee, O., El Ghaoui, L., d'Aspremont, A., 2008. Model selection through\nsparse maximum likelihood estimation for multivariate gaussian or binary\ndata. The Journal of Machine Learning Research 9, 485\u2013516.\nBoyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., 2011. Distributed\noptimization and statistical learning via the alternating direction method\nof multipliers. Foundations and Trends in Machine Learning 3 (1), 1\u2013122.\nBoyd, S., Vandenberghe, L., 2004. Convex optimization. Cambridge University Press.\nCand\u00e8s, E. J., Li, X., Ma, Y., Wright, J., 2011. Robust principal component\nanalysis? Journal of the ACM 58 (3), 11:1\u201311:37.\nCaruana, R., 1997. Multitask learning. Machine Learning 28 (1), 41\u201375.\nChandrasekaran, V., Parrilo, P., Willsky, A., 2010. Latent variable graphical\nmodel selection via convex optimization. Arxiv preprint arXiv:1008.1290.\nChiquet, J., Grandvalet, Y., Ambroise, C., 2011. Inferring multiple graphical\nstructures. Statistics and Computing 21 (4), 537\u2013553.\nDempster, A. P., 1972. Covariance selection. Biometrics 28 (1), 157\u2013175.\nDuchi, J., Gould, S., Koller, D., 2008a. Projected subgradient methods for\nlearning sparse gaussians. Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence, 145\u2013152.\nDuchi, J., Shalev-Shwartz, S., Singer, Y., Chandra, T., 2008b. Efficient projections onto the l 1-ball for learning in high dimensions. Proceedings of\nthe 25th international conference on Machine learning, 272\u2013279.\n41\n\n\fDurbin, J., Koopman, S., Atkinson, A., 2001. Time series analysis by state\nspace methods. Vol. 15. Oxford University Press.\nFan, J., Feng, Y., Wu, Y., 2009. Network exploration via the adaptive lasso\nand scad penalties. The Annals of Applied Statistics 3 (2), 521.\nFriedman, J., Hastie, T., Tibshirani, R., 2008. Sparse inverse covariance\nestimation with the graphical lasso. Biostatistics 9 (3), 432\u2013441.\nGabay, D., Mercier, B., 1976. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers &\nMathematics with Applications 2 (1), 17\u201340.\nGolub, G., Van Loan, C., 1996. Matrix computations. Vol. 3. Johns Hopkins\nUniversity Press.\nGuo, F., Hanneke, S., Fu, W., Xing, E., 2007. Recovering temporally rewiring\nnetworks: A model-based approach. In: Proceedings of the 24th International Conference on Machine learning. ACM, pp. 321\u2013328.\nGuo, J., Levina, E., Michailidis, G., Zhu, J., 2011. Joint estimation of multiple graphical models. Biometrika 98 (1), 1\u201315.\nHamilton, J., 1994. Time series analysis. Vol. 2. Cambridge University Press.\nHara, S., Kawahara, Y., Washio, T., von B\u00fcnau, P., Tokunaga, T., Yumoto,\nK., 2012. Separation of stationary and non-stationary sources with a generalized eigenvalue problem. Neural Networks 33, 7\u201320.\nHara, S., Washio, T., 2011. Common substructure learning of multiple\ngraphical gaussian models. Machine Learning and Knowledge Discovery\nin Databases, 1\u201316.\nHestenes, M., 1969. Multiplier and gradient methods. Journal of Optimization Theory and Applications 4 (5), 303\u2013320.\nHonorio, J., 2011. Lipschitz parametrization of probabilistic graphical models. Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence, 347\u2013354.\n\n42\n\n\fHonorio, J., Samaras, D., 2010. Multi-task learning of gaussian graphical\nmodels. Proceedings of the 27th International Conference on Machine\nLearning, 447\u2013454.\nHsieh, C., Sustik, M., Dhillon, I., Ravikumar, P., 2011. Sparse inverse covariance matrix estimation using quadratic approximation. Advances in\nNeural Information Processing Systems 24, 2330\u20132338.\nId\u00e9, T., Lozano, A. C., Abe, N., Liu, Y., 2009. Proximity-based anomaly\ndetection using sparse structure learning. Proceedings of the 2009 SIAM\nInternational Conference on Data Mining, 97\u2013108.\nJalali, A., Ravikumar, P., Sanghavi, S., Ruan, C., 2010. A dirty model for\nmulti-task learning. Advances in Neural Information Processing Systems\n23, 964\u2013972.\nLauritzen, S., 1996. Graphical models. Oxford University Press, USA.\nLi, L., Toh, K., 2010. An inexact interior point method for l 1-regularized\nsparse covariance selection. Mathematical Programming Computation, 1\u2013\n25.\nLiu, H., Palatucci, M., Zhang, J., 2009. Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis\ndiscovery. Proceedings of the 26th International Conference on Machine\nLearning, 649\u2013656.\nMeinshausen, N., B\u00fchlmann, P., 2006. High-dimensional graphs and variable\nselection with the lasso. The Annals of Statistics 34 (3), 1436\u20131462.\nObozinski, G., Jacob, L., Vert, J., 2011. Group lasso with overlaps: the latent\ngroup lasso approach. Arxiv preprint arXiv:1110.0413.\nPowell, M., 1967. A method for non-linear constraints in minimization problems. Optimization, 283\u2013298.\nScheinberg, K., Ma, S., Goldfarb, D., 2010. Sparse inverse covariance selection via alternating linearization methods. Advances in Neural Information\nProcessing Systems 23, 2101\u20132109.\n\n43\n\n\fScheinberg, K., Rish, I., 2010. Learning sparse gaussian markov networks\nusing a greedy coordinate ascent approach. Machine Learning and Knowledge Discovery in Databases, 196\u2013212.\nSra, S., 2011. Fast projections onto l1,q -norm balls for grouped feature selection. Machine Learning and Knowledge Discovery in Databases, 305\u2013317.\nTibshirani, R., 1996. Regression shrinkage and selection via the lasso. Journal\nof the Royal Statistical Society: Series B 58 (1), 267\u2013288.\nTibshirani, R., Saunders, M., Rosset, S., Zhu, J., Knight, K., 2005. Sparsity\nand smoothness via the fused lasso. Journal of the Royal Statistical Society:\nSeries B 67 (1), 91\u2013108.\nTomioka, R., Suzuki, T., Sugiyama, M., 2011. Super-linear convergence of\ndual augmented lagrangian algorithm for sparsity regularized estimation.\nThe Journal of Machine Learning Research 12, 1537\u20131586.\nTurlach, B., Venables, W., Wright, S., 2005. Simultaneous variable selection.\nTechnometrics 47 (3), 349\u2013363.\nVaroquaux, G., Gramfort, A., Poline, J. B., Thirion, B., 2010. Brain covariance selection: better individual functional connectivity models using\npopulation prior. Advances in Neural Information Processing Systems 23,\n2334\u20132342.\nvon B\u00fcnau, P., Meinecke, F. C., Kir\u00e1ly, F. C., M\u00fcller, K. R., 2009. Finding\nstationary subspaces in multivariate time series. Physical Review Letters\n103 (21), 214101.\nWainwright, M., Ravikumar, P., Lafferty, J., 2007. High-dimensional graphical model selection using l1 -regularized logistic regression. Advances in\nNeural Information Processing Systems 19, 1465\u20131472.\nWang, C., Sun, D., Toh, K., 2009. Solving log-determinant optimization\nproblems by a newton-cg primal proximal point algorithm. SIAM Journal\non Optimization 20, 2994\u20133013.\nYuan, M., Lin, Y., 2006. Model selection and estimation in regression with\ngrouped variables. Journal of the Royal Statistical Society: Series B 68 (1),\n49\u201367.\n44\n\n\fYuan, M., Lin, Y., 2007. Model selection and estimation in the gaussian\ngraphical model. Biometrika 94, 19\u201335.\nYuan, X., 2009. Alternating direction methods for sparse covariance selection. Preprint available at http://www.optimizationonline.org/DB HTML/2009/09/2390.html.\nZhang, B., Li, H., Riggins, R. B., Zhan, M., Xuan, J., Zhang, Z., Hoffman,\nE. P., Clarke, R., Wang, Y., 2009. Differential dependency network analysis\nto identify condition-specific topological changes in biological networks.\nBioinformatics 25 (4), 526\u2013532.\nZhang, B., Wang, Y., 2010. Learning structural changes of gaussian graphical\nmodels in controlled experiments. Proceedings of the 26th Conference on\nUncertainty in Artificial Intelligence, 701\u2013708.\nZhou, S., Lafferty, J., Wasserman, L., 2010. Time varying undirected graphs.\nMachine Learning 80 (2), 295\u2013319.\nZou, H., 2006. The adaptive lasso and its oracle properties. Journal of the\nAmerican Statistical Association 101 (476), 1418\u20131429.\n\n45\n\n\fTable 3: Simulation results for three cases (d = 25, 50 and 100) with N = 5 datasets evaluated by weighted precision, recall and F-measure, denoted by \"Prec.\", \"Rec.\" and \"F\" in\nthe table, respectively. The \"F0 \" denotes the F0 -measure for zero pattern identification.\nEach simulation is conducted so that each dataset has 5d data points, and the measurements are averaged over 100 random realization of datasets. The numbers in brackets are\nstandard deviations of each measurement. Each of the three rows in SICS and MSICS\ncorresponds to results for \u01eb0 = 0.5, 0.7 and 0.9 from the top. We highlight the top three\nresults for each measurement in bold font (except for \"F0 \").\n\nPrec.\n\n.84 (.19) .70 (.16) .56 (.19) .48 (.20)\n\nRec.\n\n.45 (.32)\n\nF\n\n.56 (.22) .75 (.14) .66 (.17) .60 (.19)\n\nF0\n\n.92 (.02)\n\nPrec.\n\n.87 (.11) .69 (.14) .56 (.17)\n\nRec.\n\n.41 (.20)\n\nF\n\n.53 (.20) .75 (.12) .66 (.15) .61 (.15)\n\nF0\n\n.90 (.03)\n\nPrec.\n\nCSSL\n(\u03b3 = \u221e)\n\n.91 (.07) .78 (.10) .64 (.14)\n\nRec.\n\nCSSL\n(p = \u221e)\n\n.37 (.18)\n\nF\n\nCSSL\n(p = 2)\n\n.51 (.19) .79 (.10) .72 (.12) .67 (.12)\n\nF0\n\nd = 100\n\nd = 50\n\nd = 25\n\nCSSL\n(p = 1)\n\n.87 (.04)\n\n.82 (.14) .84 (.12) .86 (.11)\n\n.92 (.02)\n\n.92 (.02)\n\n.92 (.02)\n.47 (.17)\n\n.83 (.11) .85 (.10) .91 (.05)\n\n.90 (.02)\n\n.89 (.02)\n\n.89 (.03)\n.53 (.15)\n\n.81 (.11) .83 (.11) .95 (.02)\n\n.87 (.04)\n\n.87 (.03)\n\n46\n\n.87 (.03)\n\nSICS\n.14\n.20\n.33\n.07\n.23\n.80\n.09\n.21\n.45\n.92\n.10\n.13\n.27\n.04\n.10\n.50\n.05\n.10\n.34\n.89\n.09\n.10\n.22\n.03\n.06\n.24\n.05\n.07\n.22\n.87\n\n(.14)\n(.16)\n(.16)\n(.07)\n(.18)\n(.20)\n(.08)\n(.16)\n(.18)\n(.02)\n(.13)\n(.14)\n(.19)\n(.04)\n(.11)\n(.22)\n(.06)\n(.11)\n(.20)\n(.03)\n(.11)\n(.12)\n(.17)\n(.10)\n(.10)\n(.19)\n(.10)\n(.10)\n(.18)\n(.03)\n\nMSICS\n(p = 2)\n.38 (.21)\n.43 (.21)\n.41 (.19)\n.48 (.24)\n.74 (.19)\n.83 (.13)\n.41 (.21)\n.53 (.21)\n.53 (.19)\n.93 (.02)\n.24 (.20)\n.37 (.20)\n.42 (.18)\n.18 (.19)\n.51 (.21)\n.81 (.12)\n.20 (.19)\n.42 (.20)\n.54 (.17)\n.90 (.02)\n.17 (.14)\n.33 (.21)\n.46 (.18)\n.06 (.10)\n.25 (.21)\n.67 (.16)\n.08 (.11)\n.28 (.21)\n.54 (.17)\n.88 (.04)\n\nMSICS\n(p = \u221e)\n.54 (.23)\n.49 (.21)\n.45 (.19)\n.60 (.24)\n.74 (.19)\n.86 (.11)\n.55 (.23)\n.58 (.20)\n.58 (.18)\n.92 (.02)\n.58 (.19)\n.52 (.19)\n.47 (.18)\n.60 (.19)\n.72 (.16)\n.86 (.08)\n.58 (.19)\n.59 (.18)\n.60 (.16)\n.90 (.03)\n.68 (.15)\n.62 (.16)\n.55 (.16)\n.59 (.17)\n.67 (.15)\n.82 (.09)\n.63 (.16)\n.64 (.15)\n.65 (.14)\n.87 (.03)\n\n\fTable 4: Anomaly detection results: The simulation is conducted for 4 different settings,\n[nn , nf ] = [4, 1], [12, 3], [20, 5] and [40, 10]. For each method, we compute precision matrices\nfor 11 different values of \u03b1 ranging from 10\u22121.5 to 10\u22120.5 . The table shows the median\nof the best AUCs among these 11 results over 100 random realizations of datasets. The\nnumbers in brackets are 25% and 75% quantiles. The bold font represents the top three\nresults, which are CSSL (p = 2), CSSL (p = \u221e) and MSICS (p = \u221e) for all settings.\n\n[nn , nf ] = [4, 1]\nbest AUC\n\u03b1\nCSSL (p = 1)\n.975 (.950 / .987) 10\u22120.9\nCSSL (p = 2) .987 (.963 / 1.00) 10\u22120.9\nCSSL (p = \u221e) .987 (.963 / 1.00) 10\u22120.9\nSICS\n.975 (.938 / .987) 10\u22120.5\nMSICS (p = 2)\n.975 (.950 / .987) 10\u22120.8\nMSICS (p = \u221e) .987 (.963 / 1.00) 10\u22121.1\n[nn , nf ] = [20, 5]\nbest AUC\n\u03b1\nCSSL (p = 1)\n.975 (.950 / 1.00) 10\u22120.9\nCSSL (p = 2) 1.00 (.975 / 1.00) 10\u22120.8\nCSSL (p = \u221e) 1.00 (.987 / 1.00) 10\u22120.9\nSICS\n.975 (.950 / .987) 10\u22120.5\nMSICS (p = 2)\n.975 (.950 / .987) 10\u22121.0\nMSICS (p = \u221e) .987 (.975 / 1.00) 10\u22121.1\n\n47\n\n[nn , nf ] = [12, 3]\nbest AUC\n\u03b1\n.975 (.950 / 1.00) 10\u22120.9\n.987 (.963 / 1.00) 10\u22120.9\n1.00 (.987 / 1.00) 10\u22120.9\n.975 (.938 / .987) 10\u22120.5\n.975 (.950 / .987) 10\u22120.7\n.987 (.975 / 1.00) 10\u22121.2\n[nn , nf ] = [40, 10]\nbest AUC\n\u03b1\n\u22120.9\n.975 (.963 / 1.00) 10\n.987 (.963 / 1.00) 10\u22120.8\n1.00 (.987 / 1.00) 10\u22120.9\n.975 (.950 / .987) 10\u22120.5\n.975 (.950 / .987) 10\u22121.0\n.987 (.975 / 1.00) 10\u22120.9\n\n\f"}