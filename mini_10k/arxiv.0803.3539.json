{"id": "http://arxiv.org/abs/0803.3539v1", "guidislink": true, "updated": "2008-03-25T11:57:15Z", "updated_parsed": [2008, 3, 25, 11, 57, 15, 1, 85, 0], "published": "2008-03-25T11:57:15Z", "published_parsed": [2008, 3, 25, 11, 57, 15, 1, 85, 0], "title": "Reinforcement Learning by Value Gradients", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0803.1234%2C0803.1917%2C0803.3604%2C0803.2243%2C0803.4207%2C0803.3515%2C0803.0508%2C0803.4111%2C0803.3774%2C0803.1842%2C0803.3068%2C0803.0955%2C0803.1068%2C0803.2006%2C0803.1282%2C0803.4240%2C0803.3017%2C0803.3039%2C0803.4087%2C0803.2865%2C0803.3776%2C0803.3079%2C0803.1369%2C0803.1914%2C0803.1150%2C0803.1186%2C0803.1847%2C0803.4029%2C0803.0576%2C0803.3350%2C0803.3561%2C0803.4399%2C0803.1033%2C0803.2325%2C0803.0022%2C0803.1865%2C0803.2403%2C0803.2206%2C0803.1576%2C0803.1388%2C0803.3587%2C0803.1894%2C0803.4027%2C0803.3577%2C0803.4170%2C0803.3204%2C0803.2671%2C0803.2296%2C0803.2118%2C0803.3539%2C0803.4260%2C0803.2009%2C0803.0141%2C0803.3421%2C0803.3371%2C0803.2814%2C0803.3671%2C0803.4241%2C0803.1792%2C0803.1870%2C0803.1506%2C0803.0347%2C0803.2864%2C0803.2561%2C0803.0439%2C0803.1546%2C0803.3649%2C0803.2949%2C0803.4038%2C0803.1831%2C0803.0489%2C0803.1723%2C0803.2558%2C0803.0005%2C0803.1825%2C0803.3466%2C0803.0514%2C0803.0079%2C0803.0110%2C0803.4391%2C0803.0271%2C0803.0324%2C0803.2051%2C0803.2487%2C0803.1696%2C0803.3080%2C0803.1481%2C0803.3302%2C0803.1716%2C0803.2340%2C0803.2641%2C0803.2854%2C0803.1666%2C0803.3600%2C0803.2954%2C0803.3400%2C0803.2246%2C0803.4357%2C0803.0838%2C0803.0655%2C0803.1384&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Reinforcement Learning by Value Gradients"}, "summary": "The concept of the value-gradient is introduced and developed in the context\nof reinforcement learning. It is shown that by learning the value-gradients\nexploration or stochastic behaviour is no longer needed to find locally optimal\ntrajectories. This is the main motivation for using value-gradients, and it is\nargued that learning value-gradients is the actual objective of any\nvalue-function learning algorithm for control problems. It is also argued that\nlearning value-gradients is significantly more efficient than learning just the\nvalues, and this argument is supported in experiments by efficiency gains of\nseveral orders of magnitude, in several problem domains. Once value-gradients\nare introduced into learning, several analyses become possible. For example, a\nsurprising equivalence between a value-gradient learning algorithm and a\npolicy-gradient learning algorithm is proven, and this provides a robust\nconvergence proof for control problems using a value function with a general\nfunction approximator.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0803.1234%2C0803.1917%2C0803.3604%2C0803.2243%2C0803.4207%2C0803.3515%2C0803.0508%2C0803.4111%2C0803.3774%2C0803.1842%2C0803.3068%2C0803.0955%2C0803.1068%2C0803.2006%2C0803.1282%2C0803.4240%2C0803.3017%2C0803.3039%2C0803.4087%2C0803.2865%2C0803.3776%2C0803.3079%2C0803.1369%2C0803.1914%2C0803.1150%2C0803.1186%2C0803.1847%2C0803.4029%2C0803.0576%2C0803.3350%2C0803.3561%2C0803.4399%2C0803.1033%2C0803.2325%2C0803.0022%2C0803.1865%2C0803.2403%2C0803.2206%2C0803.1576%2C0803.1388%2C0803.3587%2C0803.1894%2C0803.4027%2C0803.3577%2C0803.4170%2C0803.3204%2C0803.2671%2C0803.2296%2C0803.2118%2C0803.3539%2C0803.4260%2C0803.2009%2C0803.0141%2C0803.3421%2C0803.3371%2C0803.2814%2C0803.3671%2C0803.4241%2C0803.1792%2C0803.1870%2C0803.1506%2C0803.0347%2C0803.2864%2C0803.2561%2C0803.0439%2C0803.1546%2C0803.3649%2C0803.2949%2C0803.4038%2C0803.1831%2C0803.0489%2C0803.1723%2C0803.2558%2C0803.0005%2C0803.1825%2C0803.3466%2C0803.0514%2C0803.0079%2C0803.0110%2C0803.4391%2C0803.0271%2C0803.0324%2C0803.2051%2C0803.2487%2C0803.1696%2C0803.3080%2C0803.1481%2C0803.3302%2C0803.1716%2C0803.2340%2C0803.2641%2C0803.2854%2C0803.1666%2C0803.3600%2C0803.2954%2C0803.3400%2C0803.2246%2C0803.4357%2C0803.0838%2C0803.0655%2C0803.1384&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The concept of the value-gradient is introduced and developed in the context\nof reinforcement learning. It is shown that by learning the value-gradients\nexploration or stochastic behaviour is no longer needed to find locally optimal\ntrajectories. This is the main motivation for using value-gradients, and it is\nargued that learning value-gradients is the actual objective of any\nvalue-function learning algorithm for control problems. It is also argued that\nlearning value-gradients is significantly more efficient than learning just the\nvalues, and this argument is supported in experiments by efficiency gains of\nseveral orders of magnitude, in several problem domains. Once value-gradients\nare introduced into learning, several analyses become possible. For example, a\nsurprising equivalence between a value-gradient learning algorithm and a\npolicy-gradient learning algorithm is proven, and this provides a robust\nconvergence proof for control problems using a value function with a general\nfunction approximator."}, "authors": ["Michael Fairbank"], "author_detail": {"name": "Michael Fairbank"}, "author": "Michael Fairbank", "links": [{"href": "http://arxiv.org/abs/0803.3539v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0803.3539v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0803.3539v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0803.3539v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "25-March-2008\n\nReinforcement Learning by Value-Gradients\n\narXiv:0803.3539v1 [cs.NE] 25 Mar 2008\n\nMichael Fairbank\n\nmichael.fairbank 'at' virgin.net\n\nc/o Gatsby Computational Neuroscience Unit,\nAlexandra House,\n17 Queen Square,\nLONDON, UK\n\nEditor: Leslie Pack Kaelbling\n\nAbstract\nThe concept of the value-gradient is introduced and developed in the context of reinforcement learning, for deterministic episodic control problems that use a function approximator and have a continuous state space. It is shown that by learning the valuegradients, instead of just the values themselves, exploration or stochastic behaviour is no\nlonger needed to find locally optimal trajectories. This is the main motivation for using\nvalue-gradients, and it is argued that learning the value-gradients is the actual objective\nof any value-function learning algorithm for control problems. It is also argued that learning value-gradients is significantly more efficient than learning just the values, and this\nargument is supported in experiments by efficiency gains of several orders of magnitude, in\nseveral problem domains.\nOnce value-gradients are introduced into learning, several analyses become possible.\nFor example, a surprising equivalence between a value-gradient learning algorithm and a\npolicy-gradient learning algorithm is proven, and this provides a robust convergence proof\nfor control problems using a value function with a general function approximator. Also,\nthe issue of whether to include 'residual gradient' terms into the weight update equations\nis addressed. Finally, an analysis is made of actor-critic architectures, which finds strong\nsimilarities to back-propagation through time, and gives simplifications and convergence\nproofs to certain actor-critic architectures, but while making those actor-critic architectures\nredundant.\nUnfortunately, by proving equivalence to policy-gradient learning, finding new divergence examples even in the absence of bootstrapping, and proving the redundancy of\nresidual-gradients and actor-critic architectures in some circumstances, this paper does\nsomewhat discredit the usefulness of using a value-function.\nKeywords: Reinforcement Learning, Control Problems, Value-gradient, Function approximators\n\n1. Introduction\nReinforcement learning (RL) algorithms frequently make use of a value function (Sutton and Barto,\n1998). On problem domains where the state space is large and continuous, the value function needs to be represented by a function approximator. In this paper, analysis is restricted\nto episodic control problems of this kind, with a known differentiable deterministic model.\nAs Sutton and Barto (1998) stated: \"The central role of value estimation is arguably\nthe most important thing we have learned about reinforcement learning over the last few\nc Michael Fairbank.\n\n\fFairbank\n\ndecades\". However the use of a value function introduces some major difficulties when\ncombined with a function approximator, concerning the lack of convergence guarantees to\nlearning. This problem has led to a major alternative RL approach which works without\na value function at all, i.e. policy-gradient learning (PGL) algorithms (Williams, 1992;\nBaxter and Bartlett, 2000; Werbos, 1990), which do have the desired convergence guarantees. In this paper, a surprising equivalence between these two seemingly different approaches is shown, and this provides the basis for convergence guarantees to variants of\nvalue function learning algorithms.\nIt is the central thesis of this paper that for value function methods, it is not the values\nthemselves that are important, but in fact the value-gradients (defined to be the gradient of\nthe value function with respect to the state vector). We distinguish between methods that\naim to learn a value function by explicitly updating value-gradients from those that don't\nby referring to them as value-gradient learning (VGL) and value-learning (VL), respectively.\nThe necessity of exploration to VL methods is demonstrated in section 1.3, which becomes\nvery apparent in our problem domains where all functions are deterministic. We call the\nlevel of exploration that searches immediately neighbouring trajectories as local exploration.\nThis requirement for local exploration is not necessary with VGL methods, since the valuegradient automatically provides awareness of any superior neighbouring trajectories. This\nis shown for a specific example in Section 1.3, and proven in the general case in Appendix\nA. It is then argued that VGL methods are an idealised form of VL methods, are easier to\nanalyse, and are more efficient (sections 1.4 and 1.5).\nThe VGL themselves are stated at the start of Section 2. One of these algorithms\n(Section 2.1) is proven to be equivalent to PGL. This is used as the basis for a VGL\nalgorithm in a continuous-time formulation with convergence guarantees (Section 2.2). It\nalso produces a tentative theoretical justification for the commonly used TD(\u03bb) weightupdate, which from the author's point of view has always been a puzzling issue.\nThe residual-gradients algorithm for VGL is given and analysed in Section 2.3, and new\nreasons are given for the ineffectiveness of residual-gradients in deterministic environments,\nboth with VGL and VL. In Section 3, actor-critic architectures are defined for VGL, and\nit is shown that the value-gradient analysis provides simplifications to certain actor-critic\narchitectures. This allows new convergence proofs, but at the expense of making the actorcritic architecture redundant.\nIn Section 4 experimental details are provided that justify the optimality claims, and\nthe efficiency claims (by several orders of magnitude). The problems we use include the\nToy Problem, defined in Section 1.1.1, which is simple enough to be able to analyse in\ndetail, and challenging enough to cause difficulties for VL. Examples showing diverging\nweights are given for all VL algorithms and some VGL algorithms in Section 4.3. Also a\nLunar-Lander neural-network experiment is included, which is a larger scale neural network\nproblem that seems to defeat VL. Finally, Section 5 gives conclusions and a discussion on\nVGL, highlighting the contributions of this paper.\nValue-gradients have already appeared in various forms in the literature. Dayan and Singh\n(1996) argue the importance of value-gradients over the values themselves, which is the central thesis of this paper.\nThe target value-gradient we define is closely related to the \"adjoint vector\" that appears\nin Pontryagin's maximum principle, as discussed further in Appendix A.\n2\n\n\fReinforcement Learning by Value-Gradients\n\nThe equations of Back-Propagation Through Time (BPTT, Werbos (1990)) and Differential Dynamic Programming (Jacobson and Mayne, 1970) implicitly contain references to\nthe target value-gradient, both with \u03bb = 1 (\u03bb is the bootstrapping parameter, defined in\nSection 1.1).\nIn continuous-time problem domains, Doya (2000) uses the value-gradient explicitly\nin the greedy policy, and Baird (1994) defines an \"advantage\" function that implicitly\nreferences the value-gradient. Both of these are discussed further in Section 2.2. A valuegradient appears in the Hamilton-Jacobi-Bellman Equation which is an optimality condition\nfor continuous-time value-functions; although here only its component parallel to the trajectory is used, and this component is not useful in obviating the need for local exploration.\nHowever, the most similar work on value-gradients is in a family of algorithms (Werbos,\n1998; White and Sofge, 1992, ch.13) referred to as Dual Heuristic Programming (DHP).\nThese are full VGL methods, but are based on actor-critic architectures specifically with\n\u03bb = 0, and are more focussed towards unknown stochastic models.\n1.1 Reinforcement Learning Problem Notation and Definitions\nState Space, S, is a subset of Rn . Each state in the state space is denoted by a column\nvector ~x. A trajectory is a list of states {~x0 , ~x1 , . . . , ~xF } through state space starting at a\ngiven point ~x0 . The trajectory is parametrised by real actions at for time steps t according\nto a model. The model is comprised of two known smooth deterministic functions f (~x, a)\nand r(~x, a). The first model function f links one state in the trajectory to the next, given\naction at , via the Markovian rule ~xt+1 = f (~xt , at ). The second model function, r, gives an\nimmediate real-valued reward rt = r(~xt , at ) on arriving at the next state ~xt+1 .\nAssume that each trajectory is guaranteed to reach a terminal state in some finite time\n(i.e. the problem is episodic). Note that in general, the number of time steps in a trajectory\nmay be dependent on the actions taken. For example, a scenario like this could be an aircraft\nwith limited fuel trying to land. For a particular trajectory label the final time step t = F ,\nso that ~xF is the terminal state of that trajectory. Assume each action at is a real number\nthat, for some problems, may be constrained to \u22121 \u2264 at \u2264 1.\nFor any trajectory starting at state ~x0 and following actions {a0 , a1 , . . . , aF \u22121 } until\nreaching a terminal state under the given model, the total reward encountered is given by\nthe function:\n\nR(~x0 , a0 , a1 , . . . , aF \u22121 ) =\n\nF\n\u22121\nX\n\nr(~xt , at )\n\nt=0\n\n= r(~x0 , a0 ) + R(f (~x0 , a0 ), a1 , a2 , . . . , aF \u22121 )\n\n(1)\n\nThus R is a function of the arbitrary starting state ~x0 and the actions, and this allows us\nto obtain the partial derivative \u2202R\n\u2202~\nx.\nPolicy. A policy is a function \u03c0(~x, w),\n~ parametrised by weight vector w,\n~ that generates\nactions as a function of state. Thus for a given trajectory generated by a given policy \u03c0,\nat = \u03c0(~xt , w).\n~ Since the policy is a pure function of ~x and w,\n~ the policy is memoryless.\n3\n\n\fFairbank\n\nIf a trajectory starts at state ~x0 and then follows a policy \u03c0(~x, w)\n~ until reaching a\nterminal state, then the total reward is given by the function:\nR\u03c0 (~x0 , w)\n~ =\n\nF\n\u22121\nX\n\nr(~xt , \u03c0(~xt , w))\n~\n\nt=0\n\n= r(~x0 , \u03c0(~x0 , w))\n~ + R\u03c0 (f (~x0 , \u03c0(~x0 , w)),\n~ w)\n~\nApproximate Value Function. We define V (~x, w)\n~ to be the real-valued output of a\nsmooth function approximator with weight vector w\n~ and input vector ~x.1 We refer to\nV (~x, w)\n~ simply as the \"value function\" over state space ~x, parametrised by weights w.\n~\nQ Value function. The Q Value function (Watkins and Dayan, 1992) is defined as\nQ(~x, a, w)\n~ = r(~x, a) + V (f (~x, a), w)\n~\n\n(2)\n\nTrajectory Shorthand Notation. For a given trajectory through states {~x0 , ~x1 , . . . , ~xF }\nwith actions {a0 , a1 , . . . , aF \u22121 }, and for any function defined on S (e.g. including V (~x, w),\n~\nG(~x, w),\n~ R(~x, a0 , a1 , . . . , aF \u22121 ), R\u03c0 (~x, w),\n~ r(~x, a), V \u2032 (~x, w)\n~ and G\u2032 (~x, w))\n~ we use a subscript\nof t on the function to indicate that the function is being evaluated at (~xt , at , w).\n~\nFor\n~ and Rt = R(~xt , at , at+1 , . . . , aF \u22121 ).\nexample, rt = r(~xt , at ), Gt = G(~xt , w),\n~ R\u03c0 t =R\u03c0 (~xt , w)\nNote that this shorthand does not mean that these functions are functions of t, as that\nwould break the Markovian condition.\nSimilarly, for any of these function's partial derivatives, we use brackets with a subscripted t to \u0001indicate that the partial derivative is to be evaluated at time step t. For\n\u2202G\n, i.e. the function \u2202G\nat (~xt , w).\n~ Also,\nexample, \u2202G\n\u2202w\n~ \u0010t is\u0011shorthand for \u2202 w\n~ (~\n\u2202w\n~ evaluated\nxt ,w)\n~ \u0010\n\u0011\n\u0001\n\u2202f\n\u2202R\nfor example, \u2202f\n; and similarly for other\n; \u2202R\n\u2202a t = \u2202a (~\nx t =\n\u2202~\nx (~\nxt ,at ,at+1 ,...,aF \u22121 )\nxt ,at ) \u0001 \u2202~\n\u0001\n\u03c0\n\u2202r\n\u2202R\npartial derivatives including \u2202~\nx t and\n\u2202w\n~ t.\nGreedy Policy. The greedy policy on V generates \u03c0(~x, w)\n~ such that\n\u03c0(~x, w)\n~ = arg max(Q(~x, a, w))\n~\na\n\n(3)\n\nsubject to the constraints (if present) that \u22121 \u2264 at \u2264 1. The greedy policy is a one-step\nlook-ahead that decides which action to take, based only on the model and V . A greedy\ntrajectory is one that has been generated by the greedy policy. Since for a greedy policy,\nthe actions are dependent on the value function and state, and V = V (~x, w),\n~ it follows that\n\u03c0 = \u03c0(~x, w).\n~ This means that any modification to the weight vector w\n~ will immediately\nchange V (~x, w)\n~ and move all greedy trajectories. Hence we say the value function and\ngreedy policy are tightly coupled.\nFor the greedy policy, when\u0010 the\u0011constraints \u22121 \u2264 at \u2264 1 are present, we say an action\nat is saturated if |at | = 1 and \u2202Q\n\u2202a t 6= 0. If either of these conditions is not met, or the\nconstraints are not present, then at is not saturated. We note two useful consequences of\nthis:\n1. This differs slightly from some definitions in the RL literature, which would refer to this use of the\nfunction V as an approximated value function for the greedy policy on V . To side-step this circularity in\ndefinition, we have treated V (~x, w)\n~ simply as a smooth function on which a greedy policy can be defined.\n\n4\n\n\fReinforcement Learning by Value-Gradients\n\nLemma 1 If at is not saturated, then\n\n\u0010\n\n\u0011\n\n\u2202Q\n\u2202a t\n\n= 0 and\n\n\u0010\n\n\u0011\n\n\u22022Q\n\u2202a2 t\n\n\u2264 0 (since it's a maximum).\n\n\u0001\n\u0001\nLemma 2 If at is saturated, then, whenever they exist, \u2202\u03c0\n= 0 and \u2202\u2202\u03c0\n= 0.\n\u2202~\nx\nw\n~\nt\nt\n\u0001\n\u0001\n\u2202\u03c0\n\u2202\u03c0\nNote that \u2202~x t and \u2202 w~ t may not exist, for example, if there are multiple joint maxima\nin Q(~x, a, w)\n~ with respect to a.\n\u01eb-Greedy Policy. In our later experiments we implement VL algorithms that require some\nexploration. Hence we make use of a slightly modified version of the greedy policy which\nwe refer to as the \u01eb-greedy policy:2\n\u03c0(~x, w)\n~ = arg max(Q(~x, a, w))\n~ + RN D(\u01eb)\na\n\nHere RN D(\u01eb) is defined to be a random number generator that returns a normally distributed random variable with mean 0 and standard deviation \u01eb.\nThe Value-Gradient - definition. The value-gradient function G(~x, w)\n~ is the derivative\n(~\nx,w)\n~\nof the value function V (~x, w)\n~ with respect to state vector ~x. Therefore G(~x, w)\n~ = \u2202V \u2202~\n.\nx\nSince V (~x, w)\n~ is defined to be smooth, the value-gradient always exists.\nTargets for V. For a trajectory found by a greedy policy \u03c0(~x, w)\n~ on a value function\n\u2032\nV (~x, w),\n~ we define the function V (~x, w)\n~ recursively as\n\u0001\nV \u2032 (~x, w)\n~ = r(~x, \u03c0(~x, w))\n~ + \u03bbV \u2032 (f (~x, \u03c0(~x, w)),\n~ w)\n~ + (1 \u2212 \u03bb)V (f (~x, \u03c0(~x, w)),\n~ w)\n~\n(4)\n\nwith V \u2032 (~xF , w)\n~ = 0 and where 0 \u2264 \u03bb \u2264 1 is a fixed constant. To calculate V \u2032 for a particular\npoint ~x0 in state space, it is necessary to run and cache a whole trajectory starting from\n~x0 under the greedy policy \u03c0(~x, w),\n~ and then work backwards along it applying the above\n\u2032\nrecursion; thus V (~x, w)\n~ is defined for all points in state space.\nUsing shorthand notation, the above equation simplifies to\n\u0001\nV \u2032 t = rt + \u03bbV \u2032 t+1 + (1 \u2212 \u03bb)Vt+1\n\n\u03bb is a \"bootstrapping\" parameter, giving full bootstrapping when \u03bb = 0 and none when\n\u03bb = 1, described by Sutton (1988). When \u03bb = 1, V \u2032 (~x, w)\n~ becomes identical to R\u03c0 (~x, w).\n~\n\u2032\nFor any \u03bb, V is identical to the \"\u03bb-return\", or the \"forward view of TD(\u03bb)\", described by\nSutton and Barto (1998). The use of V \u2032 greatly simplifies the analysis of value functions\nand value-gradients.\nWe refer to the values V \u2032 t as the \"targets\" for Vt . The objective of any VL algorithm\nis to achieve Vt = V \u2032 t for all t > 0 along all possible greedy trajectories. By Eq. 4 and for\nany \u03bb, this objective becomes equivalent to the deterministic and undiscounted case of the\nBellman Equation of dynamic programming (Sutton and Barto, 1998):\nVt = V \u2032 t \u2200t > 0\n\n\u21d0\u21d2\n\nVt = rt + Vt+1 \u2200t > 0\n\n(5)\n\nSince the Bellman Equation needs satisfying at all points in state space, it is sometimes\nreferred to as a global method, and this means VL algorithms always need to incorporate\nsome form of exploration.\n2. This differs from the definition of the \u01eb-greedy policy that Sutton and Barto (1998) use. Also in the\ndefinition we use, we have assumed the actions are unconstrained.\n\n5\n\n\fFairbank\n\nWe point out that since V \u2032 is dependent on the actions and on V (~x, w),\n~ it is not a simple\nmatter to attain the objective V \u2261 V \u2032 , since changing V infinitesimally will immediately\nmove the greedy trajectories (since they are tightly coupled), and therefore change V \u2032 ; these\ntargets are moving ones. However, a learning algorithm that continually attempts to move\nthe values Vt infinitesimally and directly towards the values V \u2032 t is equivalent to TD(\u03bb)\n(Sutton, 1988), as shown in Section 1.2.1. This further justifies Eq. 4.\nMatrix-vector notation. Throughout this paper, a convention is used that all defined\nvector quantities are columns, and any vector becomes transposed (becoming a row) if it\nappears in the numerator of a differential. Upper indices indicate the component of a\u0010 vector.\n\u0011\n\u03c0\u0001\n\u2202f\nis\na\ncolumn;\nFor example, ~xt is a column; w\n~ is a column; Gt is a column; \u2202R\n\u2202w\n\u2202a t is\n\u0010\n\u0011~ t \u0001\n\u0010 \u0011\nj\n\u2202f (~\nx,a)\na row; \u2202f\n; \u2202G\n\u2202~\nx t is a matrix with element (i, j) equal to\n\u2202w\n~ t is a matrix with\n\u2202~\nxi\nt\n\u0010 \u0011\n\u0010 j\u0011\nP \u0010 \u2202f i \u0011 i\n\u2202f\n.\nAn\nexample\nof\na\nproduct\nis\nelement (i, j) equal to \u2202G\nG\n=\nGt+1 .\nt+1\ni\ni \u2202a\n\u2202a\n\u2202w\n~\nt\n\n\u2032\n\nt\n\nt\n\n(~\nx,w)\n~\n~ = \u2202V \u2202~\nTarget Value-Gradient, (G\u2032 t ). We define G\u2032 (~x, w)\n. Expanding this, using\nx\nEq. 4, gives:\n\u0012 \u0013 \u0012 \u0013 \u0013 \u0012\u0012 \u0013\n\u0012 \u0013 \u0012 \u0013\u0013\n\u0012\u0012 \u0013\n\u0001\n\u2202\u03c0\n\u2202r\n\u2202f\n\u2202\u03c0\n\u2202f\n\u2202r\n\u2032\n\u03bbG\u2032 t+1 + (1 \u2212 \u03bb)Gt+1\n+\n+\n+\nGt =\n\u2202~x t\n\u2202~x t \u2202a t\n\u2202~x t\n\u2202~x t \u2202a t\n(6)\n\u2032\n~\n~ and\nwith G F = 0. To obtain this total derivative we have used the fact that at = \u03c0(~xt , w),\nthat therefore changing ~xt will immediately change all later actions and states.\nThis recursive formula takes a known target value-gradient at the end point of a trajectory (G\u2032 F = ~0), and works it backwards along the trajectory rotating and incrementing it as\nappropriate, to give the desired value-gradient at each time step. This is the central equation behind all VGL algorithms; the objective for any VGL algorithm is to attain Gt = G\u2032 t\nfor all t > 0 along a greedy trajectory. As with the target values, it should be noted that\nthis objective is not straightforward to achieve since the values G\u2032 t are moving targets and\nare highly dependent on w.\n~\nThe above objective G \u2261 G\u2032 is a local requirement that only needs satisfying along a\ngreedy trajectory, and is usually sufficient to ensure the trajectory is locally optimal. This\nis in stark contrast to the Bellman Equation for VL (Eq. 5) which is a global requirement.\nConsequently VGL is potentially much more efficient and effective than VL. This difference\nis justified and explained further in Section 1.3.\nAll terms of Eq. 6 are obtainable from knowledge of the model functions and the policy.\nFor obtaining the term \u2202\u03c0\n\u2202~\nx it is usually preferable to have the greedy policy written in\nanalytical form, as done in Section 2.2 and the experiments of Section 4. Alternatively,\nusing a derivation similar to that of Eq. 17, it can be shown that, when it exists,\n\n\u0012\n\n\u2202\u03c0\n\u2202~x\n\n\u0013\n\n=\nt\n\n(\n\n\u2212\n0\n\n\u0010\n\n\u0011 \u0010\n\n\u22022Q\n\u2202~\nx\u2202a t\n\n\u0011\u22121\n\u22022Q\n\u2202a2 t\n\nif at unsaturated and\nif at saturated\n\n\u0010\n\n\u0011\u22121\n\u22022Q\n\u2202a2 t\n\nexists\n\n\u2032\nIf \u03bb > 0 and if \u2202\u03c0\n\u2202~\nx does not exist at some time step, t0 , of the trajectory, then G t is not\ndefined for all t \u2264 t0 . In some common situations, such as the continuous-time formulations\n(Section 2.2), \u2202\u03c0\n\u2202~\nx is always defined so this is not a problem.\n\n6\n\n\fReinforcement Learning by Value-Gradients\n\na0\n\na1\n\nTarget\nPosition\n(x=0)\nx\n\nx0\n\nx1\n\nx2\n\nFigure 1: Illustration of 2-step Toy Problem\n\u2202\u03c0\n\u2032\nIn the special case where \u03bb = 0, the dependency of Eq. \u00106 on\n\u0011 \u2202~x disappears and so G t\n\u0001\n\u2202f\n\u2202r\n\u2032\nalways exists, and the definition reduces to G\u2032 t = \u2202~\nx t + \u2202~\nx t Gt+1 . In this case G t is\nequivalent to the target value-gradient that Werbos uses in the algorithms DHP and GDHP\n(Werbos, 1998, Eq. 18).\n\u0001\n\u03c0\u0001\n\u2202R\u03c0\nWhen \u03bb = 1, G\u2032 t becomes identical to \u2202R\n\u2202~\nx t.\n\u2202~\nx t appears explicitly in the equations\nof differential dynamic programming (Jacobson and Mayne, 1970), and implicitly in backpropagation through time (Eq. 15).\n\n1.1.1 Example - Toy Problem\nMany experiments in this paper make use of the n-step Toy Problem with parameter k.\nThis is a problem in which an agent can move along a straight line and must move towards\nthe origin efficiently in a given number of time steps, illustrated in Fig. 1, and defined\ngenerically now:\nState space is one-dimensional and continuous. The actions are unbounded. In this\nepisodic problem, we define the model functions differently at each time step, and each\ntrajectory is defined to terminate at time step t = n + 1. Strictly speaking, to satisfy the\nMarkovian requirement and achieve these time step dependencies, we should add one extra\ndimension to state space to hold t and adjust the model functions accordingly. However\nthis complication was omitted in the interests of keeping the notation simple. Under this\nsimplification, the model functions are:\n(\nxt + at if 0 \u2264 t < n\n(7a)\nf (xt , at ) =\nxt\nif t = n\n(\n\u2212kat 2 if 0 \u2264 t < n\n(7b)\nr(xt , at ) =\n\u2212xt 2\nif t = n\nwhere k is a real-valued non-negative constant to allow more varieties of problem types to\nbe specified. The (n + 1)th time step is present just to deliver a final reward based only on\nstate. The model functions in the time step t = n are independent of the action an , and\nso each trajectory is completely parametrised by just (x0 , a0 , a1 , . . . , an\u22121 ). This completes\nthe definition of the Toy Problem.\nNext we describe the optimal trajectories and optimal policy for the n-step Toy Problem.\nSince the total reward is\nR(x0 , a0 , a1 , . . . , an\u22121 ) = \u2212k(a0 2 + a1 2 + . . . + an\u22121 2 ) \u2212 (xn )2\n= \u2212k(a0 2 + a1 2 + . . . + an\u22121 2 ) \u2212 (x0 + a0 + a1 + . . . + an\u22121 )2\n7\n\n\fFairbank\n\nt\nx\nn\u22121\nn\u22122\nn\u22123\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\nFigure 2: The lines A to F are optimal trajectories for the Toy Problem. The trajectories\nare shown as continuous lines for illustration here, although time is defined to be\ndiscrete in this problem. The arrowheads of the trajectories show the position of\nthe terminal states.\n\nthen the actions that maximise this are all equal, and are given by,\nat = \u2212\n\nx0\nfor all 0 \u2264 t < n.\nn+k\n\nSince the optimal actions are all equal and directed oppositely to the initial state x0 ,\noptimal trajectories form straight lines towards the centre as shown in Figure 2. Each\nkx0\noptimal trajectory terminates at xn = n+k\n. The optimal policy, usually denoted \u03c0 \u2217 (~xt ), is\n\u03c0 \u2217 (xt ) = \u2212\n\nxt\nfor all 0 \u2264 t < n\nn\u2212t+k\n\n(8)\n\nThe optimal value function, usually denoted V \u2217 (~xt ), can be found for this problem by\nevaluating the total reward encountered on following the optimal policy until termination:\n(\nk(xt )2\n\u2212 n\u2212t+k\n\u2217\nV (xt ) =\n0\n\nif t \u2264 n\nif t = n + 1\n\n(9)\n\nFor a simple example of a value function and greedy policy, the reader should see Appendix B (equations 33 and 34).\n1.2 Value-Learning Methods\nIntroducing the targets V \u2032 simplifies the formulation of some learning algorithms. The\nobjective of all VL algorithms is to learn Vt = V \u2032 t for all t \u2265 1. We do not need to consider\nthe time step t = 0 since the greedy policy is independent of the value function at t = 0. A\nquick review of two common VL methods follows.\nAll learning algorithms in this paper are \"off-line\", that is any weight updates are applied\nonly after considering a whole trajectory. In all learning algorithms we take \u03b1 to be a small\npositive constant.\n8\n\n\fReinforcement Learning by Value-Gradients\n\n1.2.1 TD(\u03bb) Learning\nThe TD(\u03bb) algorithm (Sutton, 1988) attempts to achieve Vt = V \u2032 t for all t \u2265 1 by the\nfollowing weight update:\nX \u0012 \u2202V \u0013\n\u2206w\n~ =\u03b1\n(V \u2032 t \u2212 Vt )\n(10)\n\u2202w\n~ t\nt\u22651\n\nThe equivalence of this formulation to that used by Sutton (1988) is proven in Appendix\nC. This equivalence validates the V \u2032 notation. Although not originally defined for control\nproblems by Sutton (1988), it is shown in Appendix D that the TD(\u03bb) weight update can\nbe applied directly to control problems with a known model using the \u01eb-greedy policy, and\nis then equivalent to Sarsa(\u03bb) (Rummery and Niranjan, 1994).\nUnfortunately there are no convergence guarantees for this equation when used with a\ngeneral function approximator for the value function, and divergence examples abound.\nAs shown by Sutton and Barto (1998) TD learning becomes Monte-Carlo learning when\nthe bootstrapping parameter, \u03bb = 1.\n1.2.2 Residual Gradients\nWith the aim of improving the convergence guarantees\nof the previous method, the approach\nP\nof Baird (1995) is to minimize the error E = 21 t\u22651 (V \u2032 t \u2212 Vt )2 by gradient descent on the\nweights:\n\u0012\n\u0013\u0013\nX \u0012\u0012 \u2202V \u0013\n\u2202V \u2032\n\u2212\n(V \u2032 t \u2212 Vt )\n\u2206w\n~ =\u03b1\n\u2202w\n~ t\n\u2202w\n~ t\nt\u22651\n\nThe extra terms introduced by this method are referred to throughout this paper as\nthe \"residual gradient terms\". We extend the residual gradient method to value-gradients\nin Section 2.3, and extend it to work with a greedy policy that is tightly coupled with the\nvalue function.\n1.3 Motivation for Value-Gradients\n\nThe above VL algorithms need to use some form of exploration. The required exploration\ncould be implemented by randomly varying the start point in state space at each iteration,\na technique known as \"exploring starts\". Alternatively a stochastic model or policy could\nbe used to force exploration within a single trajectory. Exploration introduces inefficiencies\nwhich are discussed in Section 1.5.\nFigure 3 demonstrates why VL without exploration can lead to suboptimal trajectories,\nwhereas VGL will not. Understanding this is a very central point to this paper as it is the\ncentral motivation for using value-gradients. If a fuller explanation of either of these cases\nis required, then Appendix B gives details, and goes further in showing that the trajectory\nwith learned value-gradients will also be optimal.\nThe conclusion of this example, and Appendix B, is that without exploration, VL will\npossibly, and in fact be likely to, converge to a suboptimal trajectory. Exploration is\nnecessary since it enables the learning algorithm to become aware of the presence of any\nsuperior neighbouring trajectories (which is exactly the information that a value-gradient\ncontains). Without this awareness learning can terminate on a suboptimal trajectory, as\nthis counterexample shows.\n9\n\n\fFairbank\n\nt\n\u221210\n\n\u22128\n\n\u22126\nx\n\n\u22128\n\n\u22128\n\nn\u22121\n\n\u22128\n\n\u22128\n\nn\u22122\n\n\u22128\n\n\u22128\n\nn\u22123\n\n\u22128\n\n\u22128\nA\n\nFigure 3: Illustration of the problem of VL without exploration. In this diagram the floating\nnegative numbers in the bottom left quadrant represent value function spot values\n(for a value function which is constant throughout state space). Hence, in the\nToy Problem the greedy policy will choose zero actions, and the greedy trajectory\n(from A) is the straight line shown. The negative numbers above the x axis give\nthe final reward, rn . Since the intermediate rewards rt are zero for all t < n,\nwe have V \u2032 t = \u22128 for all t \u2264 n. So Vt = V \u2032 t = \u22128 for all t \u2264 n, and so VL is\ncomplete; yet the trajectory is not optimal (c.f. Fig. 2). This situation cannot\nhappen with VGL, since if the value-gradients were learned then there would be\na value-gradient perpendicular to the trajectory, and the greedy policy would not\nhave chosen the trajectory shown.\n\nNote that this requirement for exploration is separate from the issue of exploration that\nis sometimes also required to learn an unknown model.\nSimilar counterexamples can be constructed for other problem domains. For example,\nFigure 3 can be applied to any problem where all the reward occurs at the end of an episode.\nFurthermore, it is proven in Appendix A that in a general problem, learning the valuegradients along a greedy trajectory is a sufficient condition for the trajectory to be locally\nextremal, and also often ensures the trajectory is locally optimal (these terms are defined in\nthe same appendix). This contrasts with VL in that there has been no need for exploration.\nLocal exploration comes for free with value-gradients, since knowledge of the value-gradients\nautomatically provides knowledge of the neighbouring trajectories.\n1.4 Relationship of Value-Learning to Value-Gradient Learning\nLearning the value-gradients along a trajectory learns the relative values of the value function along it and all of its immediately neighbouring trajectories. We refer to all these\ntrajectories collectively as a tube. Any VL algorithm that exhibits sufficient exploration to\nlearn the target values fully throughout the entire tube would also achieve this goal, and\ntherefore also achieve locally optimal trajectories.\nThis shows the consistency of the two methods, and the equivalence of their objectives.\nWe believe VGL techniques represent an idealised form of VL techniques, and that VL\nis a stochastic approximation to VGL. Both have similar objectives, i.e. to achieve an\noptimal trajectory by learning the relative target values throughout a tube; but whereas\n10\n\n\fReinforcement Learning by Value-Gradients\n\nVL techniques rely on the scattergun approach of stochastic exploration to achieve this,\nVGL techniques go about it more methodically and directly.\nFigure 4 illustrates the contrasting approaches of the two methods.\nt\n\u221210\n\n\u22128\n\n\u22126\n\nt\n\u221210\n\nx\n\nA\n\n\u22128\n\n\u22126\n\nx\n\nA\n\nFigure 4: Diagrams contrasting VL by stochastic control (left figure) against deterministic\nlearning by VGL (right figure). In the VL case (left figure), stochastic control\nmakes the trajectory zigzag, and the target value passed backwards along the\ntrajectory (which in this case is approximately \u22127, whereas the deterministic\nvalue should be \u22128) will be passed back to the points in state space encountered\nalong the trajectory. In the VGL case (right figure), the value-gradient at the end\npoint (which will be a small vector pointing in the positive x direction) is passed\nbackwards, without any stochastic distortion, along the central trajectory and\ntherefore influences the value function along all three trajectories simultaneously.\nOnce the effects of exploration are averaged out in VL, the underlying weight update\nis often very similar to a VGL weight update; but possibly with some extra, and usually\nunwanted, terms. See Section 4.1 for an example analysis in a specific problem. Hence, we\nwould expect a large proportion of results obtained for one method to apply to the other.\nFor example, using a value-gradient analysis, in Section 4.3 we derive an example that\ncauses VGL to diverge, and then empirically find this example causes divergence in VL too.\nAlso, we would expect the analyses on residual gradients and actor-critic architectures to\nplace limitations on what can be achieved with these methods when used with VL (sections\n2.3 and 3).\n\u2202V \u2032\nIf the value-gradient is learned throughout the whole of state space, i.e. if \u2202V\n\u2202~\nx = \u2202~\nx for\nall ~x, then this forms a differential equation of which the Bellman Equation (V = V \u2032 for all\n~x; see Eq. 5) is a particular solution. This is illustrated in Equation 11.\nG(~x, w)\n~ = G\u2032 (~x, w)\n~ \u2200~x\n\n\u21d0\u21d2\n\nV (~x, w)\n~ = V \u2032 (~x, w)\n~ + c \u2200~x\n\n(11)\n\nOf course the arbitrary constant, c, is not important as it does not affect the greedy\npolicy. Hence we propose that the values themselves are not important at all; it is only\nthe value-gradients that are important. This extreme view is consistent with the fact that\nit is value-gradients, not values, that appear in the optimality proof (Appendix A), in the\nrelationship of value function learning to PGL (Section 2.1) and in the equation for \u2202\u2202\u03c0\nw\n~ (Eq.\n17).\n11\n\n\fFairbank\n\nThe directness of the VGL method means it is more open to theoretical analysis than the\nVL method. The greedy policy is dependent on the value-gradient but not on the values (see\nEq. 17 or Eq. 34), so it seems essential to consider value-gradients if we are to understand\nhow an update to w\n~ will affect a greedy trajectory. If we define a \"whole system\" to mean\nthe conjunction of a tightly coupled value function with a greedy policy, then it is necessary\nto consider value-gradients if we are to understand the overall convergence properties of any\n\"whole system\" weight-update.\n1.5 Efficiencies of learning Value-Gradients\nVGL introduces some significant efficiency improvements into value function learning.\n\u2022 The removal of the need for exploration should be a major efficiency gain. As learning\nalgorithms are already iterative, the need to explore neighbouring trajectories causes a\nnested layer of iteration. Also, the issue of exploration severely restricts standard algorithms, particularly those that work by learning the Q(~x, a, w)\n~ function (e.g. Sarsa(\u03bb),\nQ(\u03bb)-learning (Watkins, 1989)). For example, whenever an action at in a trajectory\nis exploratory, i.e. non-greedy, the target values backed up to all previous time steps\n(i.e. the values V \u2032 k for all k < t) will be changed. This effectively sends the wrong\nlearning targets back to the early time steps. This difficulty is dealt with in Sarsa(\u03bb)\nby making \u01eb slowly tend to zero in the \u01eb-greedy policy. This difficulty is dealt with\nin Q(\u03bb)-learning by forcing \u03bb = 0 or truncating learning beyond the exploratory time\nstep. Both of these solutions have performance implications (we show in Section 4.1\nthat as \u01eb \u2192 0, learning grinds to a halt).\n\u2022 Learning the value-gradients along a trajectory is similar to learning the value function's relative values along an entire group of immediately neighbouring trajectories\n(see Fig. 4). Thus the value-gradients encapsulate more relevant information than\nthe values, and therefore learning by value-gradients should be faster (provided the\nfunction approximator for V can be made to learn gradients efficiently, which is not\na trivial problem).\n\u2022 As described in Section 2.2, some VGL algorithms are doing true gradient ascent on\nR\u03c0 , so are viable to speed up through any of the fast neural-network optimisation\nalgorithms available.\nThese informal arguments are backed up by the experiments in Section 4.\n\n2. Learning Algorithms for Value-Gradients\nThe objective of any VGL algorithm is to ensure Gt = G\u2032 t for all t > 0 along a greedy\ntrajectory. As proven in Appendix A, this will be sufficient to ensure a locally extremal\ntrajectory (and often a locally optimal trajectory). This section looks at some learning\nalgorithms that try to achieve this objective. In Section 2.2 we derive a VGL algorithm\nthat is guaranteed to converge to a locally optimal trajectory.\n12\n\n\fReinforcement Learning by Value-Gradients\n\nDefine the sum-of-squares error function for value-gradients as\nE(~x0 , w)\n~ =\n\n1X\n(Gt \u2212 G\u2032 t )T \u03a9t (Gt \u2212 G\u2032 t )\n2\n\n(12)\n\nt\u22651\n\nfor a given greedy trajectory. \u03a9t is any arbitrarily chosen positive semi-definite matrix (as\nintroduced by Werbos, 1998), included for generality, and often just taken to be the identity\nmatrix for all t. A use for \u03a9t could be to allow us to compensate explicitly for any rescaling\nof the dimensions of state-space. If bootstrapping is used then \u03a9t should be chosen to be\nstrictly positive definite.\nOne approach for VGL is to perform gradient descent on the above error function (giving\nthe VGL counterpart to residual-gradients):\n\u2206w\n~ =\u03b1\n\n\u2202E\n\u2202w\n~\n\n(13)\n\nThis equation is analysed in Section 2.3. However a simpler weight update is to omit the\n\"residual gradient\" terms (giving the VGL counterpart to TD(\u03bb)):\nX \u0012 \u2202G \u0013\n(14)\n\u03a9t (G\u2032 t \u2212 Gt )\n\u2206w\n~ =\u03b1\n\u2202w\n~ t\nt\u22651\n\nIn the next section we prove that this equation, with \u03bb = 1, is equivalent to PGL, and\nshow it leads to a successful algorithm with convergence guarantees.\n\u0001\n\u2202G\nAny VGL algorithm is going to involve using the matrices \u2202G\n\u2202w\n~ t and/or \u2202~\nx which, for\nneural-networks, involves second order back-propagation. This is described by (White and Sofge,\n1992, ch. 10) and (Coulom, 2002, Appendix A). In fact, these matrices are only required\nwhen multiplied by a column vector, which can be implemented efficiently by extending the\ntechniques of Pearlmutter (1994) to this situation.\n2.1 Relationship to Policy-Gradient Learning\nWe now prove that the VGL update of Eq. 14, with \u03bb = 1 and a carefully chosen \u03a9t matrix,\nis equivalent to PGL on a greedy policy. It is this equivalence that provides convergence\nguarantees for Eq. 14. To make this demonstration clearest, it is easiest to start by\nconsidering a PGL weight update; although it should be pointed out that the discovery of\nthis equivalence occurred the opposite way around, since forms of Eq. 14 date back prior\nto Werbos (1998).\nPGL, sometimes also known as \"direct\" reinforcement learning, is defined to be gradient\nascent on R\u03c0 (~x0 , w)\n~ with respect to the weight vector w\n~ of the policy:\n\u0012 \u03c0\u0013\n\u2202R\n\u2206w\n~ =\u03b1\n\u2202w\n~ 0\nBack-propagation through time (BPTT) is merely an efficient implementation of this\nformula, designed for architectures where the policy \u03c0(~x, w)\n~ is provided by a neural-network\n(see Werbos, 1990). PGL methods will naturally find stationary points that are constrained\nlocally optimal trajectories (see Appendix A for optimality definitions).\n13\n\n\fFairbank\n\nFor PGL, we therefore have:\n\n\u21d2 \u2206w\n~ =\u03b1\n\n\u0012\n\n\u2202R\u03c0\n\u2202w\n~\n\n\u0013\n\n\u0012\n\n\u2202R\u03c0\n\u2202w\n~\n\n\u0013\n\n\u2202(r(~xt , \u03c0(~xt , w))\n~ + R\u03c0 (f (~xt , \u03c0(~xt , w)),\n~ w))\n~\n\u2202w\n~\n\u0012\n\u0013 \u0012\u0012 \u0013\n\u0012 \u0013 \u0012 \u03c0\u0013 \u0013 \u0012 \u03c0\u0013\n\u2202\u03c0\n\u2202r\n\u2202f\n\u2202R\n\u2202R\n=\n+\n+\n\u2202w\n~ t\n\u2202a t\n\u2202a t \u2202~x t+1\n\u2202w\n~ t+1\n\u0012 \u0013 \u0012 \u03c0\u0013 \u0013\nX \u0012 \u2202\u03c0 \u0013 \u0012\u0012 \u2202r \u0013\n\u2202f\n\u2202R\n= \u03b1\n+\n(15)\n\u2202w\n~ t\n\u2202a t\n\u2202a t \u2202~x t+1\n=\n\nt\n\n0\n\nt\u22650\n\nThis equation is identical to the weight update performed by BPTT. It is defined for a\ngeneral policy. We now switch to specifically\u0001consider PGL applied to a greedy policy.\n\u03c0\nInitially we only consider the case where \u2202R\n\u2202w\n~ 0 exists for a greedy trajectory, and hence\n\u0001\n\u2202\u03c0\n\u2202w\n~ t exists for all t. Now in the summation of Eq. 15, we\u0001 only need to consider the time\nsteps where at is not saturated, since for at saturated, \u2202\u2202\u03c0\nw\n~ t = 0 (by Lemma 2).\n\u0001\n\u0001\n\u2202\u03c0\n\u2202r\nThe summation involves terms \u2202 w~ t and \u2202a t which can be reinterpreted under the\ngreedy policy:\nLemma 3 The greedy policy implies, for an unsaturated action,\n\u0013\n\u0012 \u0013\n\u0012 \u0013 \u0012\n\u0013\n\u2202r\n\u2202f\n\u2202V\n\u2202Q\n=\n+\n=0\n\u2202a t\n\u2202a t\n\u2202a t \u2202~x t+1\n\u0012 \u0013\n\u0012 \u0013\n\u2202r\n\u2202f\n\u21d2\n= \u2212\nGt+1\n\u2202a t\n\u2202a t\n\u0012\n\nLemma 4 When\n0 therefore,\n0 =\n=\n=\n\u21d2\n\n\u0012\n\n\u2202\u03c0\n\u2202w\n~\n\n\u0013\n\n=\nt\n\n\u2202\u03c0\n\u2202w\n~ t\n\n\u0001\n\nexists for an unsaturated action at , the greedy policy implies\n\n(16)\n\n\u0010\n\n\u0011\n\n\u2202Q\n\u2202a t\n\n\u2261\n\n\u0012\n\u0013 \u0012\n\u0013\u0012\n\u0013\n\u0013\n\u2202\u03c0\n\u2202\n\u2202Q(~xt , at , w)\n~\n\u2202Q(~xt , \u03c0(~xt , w),\n~ w)\n~\n\u2202\n+\n=\n\u2202at\n\u2202w\n~\n\u2202w\n~ t \u2202at\n\u2202at\n\u0012\u0012 \u0013\n\u0012 \u0013\n\u0013 \u0012\n\u0013 \u0012 2 \u0013\n\u2202\n\u2202r\n\u2202f\n\u2202\u03c0\n\u2202 Q\n+\nG(~xt+1 , w)\n~ +\n\u2202w\n~\n\u2202a t\n\u2202a t\n\u2202w\n~ t \u2202a2 t\n\u0012\n\u0013 \u0012 \u0013T \u0012\n\u0013 \u0012 2 \u0013\n\u2202\u03c0\n\u2202G\n\u2202f\n\u2202 Q\n+\n\u2202w\n~ t+1 \u2202a t\n\u2202w\n~ t \u2202a2 t\n\u0012 2 \u0013\n\u0012\n\u0013 \u0012 \u0013T \u0012 2 \u0013\u22121\n\u2202 Q\n\u2202 Q\n\u2202G\n\u2202f\n, assuming\n6= 0\n(17)\n\u2212\n\u2202w\n~ t+1 \u2202a t \u2202a2 t\n\u2202a2 t\n\u2202\n\u2202w\n~\n\n\u0012\n\nIt now becomes possible to analyse the PGL weight update with a greedy\npolicy. Sub\u03c0\u0001\n\u2032\nstituting the results of the above lemmas (Eq. 16 and Eq. 17), and \u2202R\n\u2202~\nx t = G t with\n14\n\n\fReinforcement Learning by Value-Gradients\n\n\u03bb = 1 (see Eq. 6), into Eq. 15 gives:\n\u0013\n\u2202R\u03c0\n\u2206w\n~ = \u03b1\n\u2202w\n~ 0\n!\n\u0012\n\u0013 \u0012 \u0013T \u0012 2 \u0013\u22121 \u0012 \u0013\nX\n\u2202G\n\u2202f\n\u2202 Q\n\u2202f\n\u2032\n= \u03b1\n\u2212\n(\u2212Gt+1 + G t+1 )\n\u2202w\n~ t+1 \u2202a t \u2202a2 t\n\u2202a t\nt\u22650\nX \u0012 \u2202G \u0013\n= \u03b1\n\u03a9t (G\u2032 t+1 \u2212 Gt+1 )\n\u2202w\n~ t+1\n\u0012\n\n(18)\n\nt\u22650\n\nwhere\n\u03a9t = \u2212\n\n\u0012\n\n\u2202f\n\u2202a\n\n\u0013T \u0012\nt\n\n\u22022Q\n\u2202a2\n\n\u0013\u22121 \u0012\nt\n\n\u2202f\n\u2202a\n\n\u0013\n\n,\n\n(19)\n\nt\n\nand is positive semi-definite, by the greedy policy (Lemma 1).\nEquation 18 is identical to a VGL weight update equation (Eq. 14), with a carefully\n\u0010 2 \u0011\u22121\n\u0001\n\u0001\nexist for all t. If \u2202\u2202\u03c0\nchosen matrix for \u03a9t , and \u03bb = 1, provided \u2202\u2202\u03c0w~ t and \u2202\u2202aQ2\nw\n~ t\nt\n\n\u03c0\n\ndoes not exist, then \u2202R\n\u2202w\n~ is not defined either. Resolutions to these existence conditions are\nproposed at the end of this section.\nThis completes the demonstration of the equivalence of a VGL algorithm (with the\n\u03c0\nconditions stated above) to PGL (with greedy policy; when \u2202R\n\u2202w\n~ exists) . Unfortunately we\ncouldn't find a similar analysis for \u03bb < 1, and divergence examples in this case are given in\nSection 4.3.\nThis result for \u03bb = 1 was quite a surprise. It justifies the omission of the \"residual\ngradient terms\" when forming the weight update equation (Eq. 14). Omitting these residual\ngradient terms is not, as it may have seemed, a puzzling modification to \u2202E\n\u2202w\n~ ; it is really\n\u2202R\u03c0\n,\n(with\n\u03bb\n=\n1,\nand\nthe\ngiven\n\u03a9\n).\nThis\nmeans\nusing\nthe\n\u03a9\nterms\nensures\nan optimal\nt\nt\n\u2202w\n~\n\u03c0\nvalue of R is obtained, as shown in the experiment in Section 4.4. Also, it shows that\nVGL algorithms (and hence value function learning algorithms) are not that different from\nPGL algorithms after all. It was not known that a PGL weight update, when applied to\na greedy policy on a value function, would be doing the same thing as a value function\nweight update; even if both had \u03bb = 1. Of course they are usually not the same, unless this\nparticular choice of \u03a9t is chosen.\nThis also provides a tentative justification for the TD(\u03bb) weight update equation (Eq.\n10). From the point of view of the author, this previously had no theoretical justification. It\nwas seemingly chosen because it looks a bit like gradient descent on an error function, and\nthe Bellman Equation happens to be a fixed point of it. This has been a hugely puzzling\nissue. There are no convergence guarantees for it and numerous divergence examples (in\nSection 4.3, we show it can even diverge with \u03bb = 1). Our explanation for it is that it is a\nstochastic approximation to Eq. 14, which itself is an approximation to PGL when \u03bb = 1.\nAlso it is our understanding that this is a particularly good form of VGL weight update\nto make, since it has good convergence guarantees. If an alternative is chosen, e.g. by\nreplacing \u03a9t by the identity matrix, then it might be possible to get much more aggressive\n15\n\n\fFairbank\n\nlearning.3 TD(\u03bb), being a stachostic approximation to Eq. 14, is fixed to implicitly use an\nidentity matrix for \u03a9t . But this creates the unwanted problem of non-monotonic progress,\nin the same way that any aggressive modification to gradient ascent may do. It is also\npossible to get divergence in this case (see Section 4.3). It is our opinion that it is better\nto use a more theoretically justifiable acceleration method such as conjugate-gradients or\nRPROP.\nThis equivalence reduces the possible advantages of the value function architecture, in\nthe case of \u03bb = 1, down to being solely a sophisticated implementation of a policy function.\nThis sophisticated policy architecture may just be easier to train than other policies; just\nas some neural-network architectures are easier to train than others. It is not the actual\nlearning algorithm that is delivering any benefits.\nWe note that it also creates a difficulty in distinguishing between PGL and VGL. With\n\u03bb = 1 and \u03a9t as defined in Eq. 19, the equation can no longer be claimed as a new learning\nalgorithm, since it is the same as BPTT with a greedy policy. Therefore the experimental\nresults will be exactly the same as for BPTT. However, we will describe the above weight\nupdate as a VGL update; it is of the same form as Eq. 14. We also point out that forms of\nEq. 14 came first (see Werbos, 1998), before the connection to PGL was realised, and that\nit itself is an idealised form of the TD(\u03bb) weight update.\nThis equivalence proof is almost a convergence proof for Eq. 18 with \u03bb = 1, since for the\nmajority of learning iterations\nthere is smooth gradient ascent on R\u03c0 . The problem is that\n\u0001\nsometimes the terms \u2202\u2202\u03c0\nw\n~ t are not defined and then learning progress jumps discontinuously.\nOne solution to this problem could be to choose a function approximator for V such that\nthe function Q(~x, a, w)\n~ is everywhere strictly concave with respect to a, as is done in the\nToy Problem experiments of Section 4. A more general solution is given in the next section.\n\u0010 2 \u0011\u22121\nexists for all t.\nBoth of these solutions also satisfy the requirement that \u2202\u2202aQ2\nt\n\n2.2 Continuous-Time Formulation\nIn many problems time can be treated as a continuous variable, i.e. t \u2208 [0, F ], as considered\nby Doya (2000) and Baird (1994). With continuous-time formulations, some extra difficulties can arise for VL as described and solved by Baird (1994), but these do not apply to\nVGL, for reasons described further below. We describe a continuous-time formulation here,\nsince, in some circumstances the greedy policy \u03c0(~\n~ becomes a smooth function of ~x and\n\u0001 x, w)\n\u2202\u03c0\nw.\n~ This removes the problem of undefined \u2202 w~ t terms that was described in the previous\nsection, and leads to a VGL algorithm for control problems with a function approximator\nthat is guaranteed to converge.\nWe use bars over the previously defined functions to denote their continuous-time counterparts, so that f \u0304(~x, a) and r\u0304(~x, a) denote the continuous-time model functions. The\ntrajectory is generated from a given start point ~x0 by the differential equation (DE),\nd~xt\n= f \u0304(~xt , \u03c0(~xt , w))\n~\ndt\n3. In fact, in the continuous-time formulation of Eq. 23,\n\n\"\n\n\u22022Q\n\u2202a2\n\n(20)\n\n\"\u22121\nt\n\n= \u2212g \u2032\n\n\"\"\n\n\u2202 r\u0304 L\n\u2202a\n\n\"\n\nt\n\n+\n\n\"\n\n\u2202 f \u0304\n\u2202a\n\n\"\n\nt\n\n\"\nGt , and so\n\nsetting \u03a9t to the identity matrix is analogous to giving the derivative of a sigmoid function an artificial\nboost (see Eq. 19). This is like the trick proposed by Fahlman (1988) that is sometimes used to speed\nup supervised learning in artificial neural networks, but at the expense of robustness.\n\n16\n\n\fReinforcement Learning by Value-Gradients\n\nThe total reward for this trajectory is\n\u03c0\n\nR (~x0 , w)\n~ =\n\nZ\n\nF\n\nr\u0304(~xt , at )dt\n\n0\n\nThe continuous-time Q function is\nQ\u0304(~x, a, w)\n~ = r\u0304(~x, a) + f \u0304T (~x, a)G(~x, w)\n~ + V (~x, w)\n~\n\n(21)\n\nThis is closely related to the \"advantage function\" (Baird, 1994), that is \u0100(~x, a, w)\n~ =\nQ\u0304(~x, a, w)\u2212\n~ V (~x, w).\n~ The major difference of the VGL approach over advantage functions is\nthat \"advantage learning\" only learns the component of G that is parallel to the trajectory,\nand so it is similar to all VL algorithms in that constant exploration of the neighbouring\ntrajectories must take place. Also, as pointed out by Doya (2000), the problem of indistinguishable Q values that advantage-learning is designed to address is not relevant when\nusing the following policy:\nWe use the same policy as proposed by Doya (2000). The greedy policy does not need to\nlook ahead in the continuous-time formulation, and instead relies only on the value-gradient\nat the current time. We assume the model functions are linear in a (which is common in\nNewtonian models, since acceleration is proportional to the force), and then we introduce\nan extra \"action-cost\" non-linear term, r\u0304 C (~x, a), into the model's reward function, giving\nr\u0304(~x, a) = r\u0304 L (~x, a) + r\u0304 C (~x, a)\n\n(22)\n\nwhere r\u0304 L (~x, a) is the original linear reward function. The action-cost term has the effect\nof ensuring the action chosen by the greedy policy is bound to [\u22121, 1],Rand also that the\na\nactions at are smooth functions of Gt . A suitable choice is r\u0304 C (~x, a) = \u2212 0 g\u22121 (x)dx where\ng\u22121 is the inverse of g(x) = tanh(x/c), and c is a positive constant. This idea is explained\nmore fully by Doya (2000).\nUsing this choice of r\u0304 C (~x, a), and substituting Eq. 21 and Eq. 22 into the greedy policy\ngives:\n\u0013\n\u0013\n\u0012  \u0304\u0013\n\u2202f\n\u2202r\u0304 L\n+\nGt\nat = \u03c0(~xt , w)\n~ = g\n\u2202a t\n\u2202a t\n\u0012\n\u0013\u0012\n\u0013\n\u0012  \u0304\u0013\n\u0013 \u0012  \u0304\u0013T\n\u0012\u0012 L \u0013\n\u2202\u03c0\n\u2202G\n\u2202f\n\u2202f\n\u2202r\u0304\n\u2032\n\u21d2\n= g\n+\nGt\n\u2202w\n~ t\n\u2202a t\n\u2202a t\n\u2202w\n~ t \u2202a t\n\u0012\n\u0013\n\u0012\n\u0013\nT\n\u2202 f \u0304\n1 \u2212 at 2 \u2202G\n=\nc\n\u2202w\n~ t \u2202a t\n\u0012 \u0013\n\u0012  \u0304\u0013 \u0012\n\u0013 \u0012  \u0304\u0013T\n2\n\u2202\u03c0\n\u2202f\n\u2202G\n\u2202f\n1 \u2212 at\nand\n=\n\u2202~x t\nc\n\u2202~x t \u2202~x t \u2202a t\n\u0012\u0012\n\n(23)\n\nFor this policy, as c \u2192 0, the policy tends to \"bang-bang\" control. For c > 0, this\npolicy function meets the objective of producing bound actions that are smooth functions\nof Gt , and therefore, since the function approximator is assumed smooth, are also smooth\nfunctions of w.\n~ This solves the problem of discontinuities described in the previous section.\n17\n\n\fFairbank\n\nThe solution works for any c > 0, so can get arbitrarily close to the ideal of bang-bang\ncontrol.\nUsing this policy, the trajectory can be calculated via Eq. 20 using a suitable DE solver.\nFor small c, the actions can rapidly alternate and so the DE may be \"stiff\" and need an\nappropriate solver (see Press et al., 1992, ch.16.6).\nFor the learning equations in continuous-time, we use \u03bb\u0304 as the 'bootstrapping' parameter. This is related to the discrete time \u03bb by e\u2212\u03bb\u0304\u2206t = \u03bb where \u2206t is the discrete-time\ntime-step. This means \u03bb\u0304 = 0 gives no bootstrapping, and that bootstrapping increases as\n\u03bb\u0304 \u2192 \u221e, i.e. this is the opposite way around to the discrete-time parameter \u03bb.\nThe equations in the rest of this section were derived in a similar manner to the discretetime case, and by letting the discrete time-step \u2206t \u2192 0.\nThe continuous-time target-value has several different equivalent formulations:\n\u0013\n\u0012\nZ F\nZ F\n\u0001\n\u2202Vt\ndt + Vt0 =\ne\u2212\u03bb\u0304(t\u2212t0 ) r\u0304t + \u03bb\u0304Vt dt\nV \u2032 t0 =\ne\u2212\u03bb\u0304(t\u2212t0 ) r\u0304t +\n\u2202t\nt0\nt0\n\u2032\n\u0001\n\u2202V t\n= \u2212r\u0304t + \u03bb\u0304 V \u2032 t \u2212 Vt with boundary condition V \u2032 F = 0\n\u21d2\n\u2202t\nThe target value-gradient is given by:\n\u0012\n\u0013\n\u0012  \u0304\u0013\n\u0001\n\u0001\nDr\u0304\n\u2202G\u2032 t\nDf\n=\u2212\nG\u2032 t + \u03bb\u0304 G\u2032 t \u2212 Gt\n\u2212\n\u2202t\nD~x t\nD~x t\n\n(24)\n\n\u2202\n\u2202\u03c0 \u2202\nD\nwith boundary condition G\u2032 F = ~0, and where D~\nx \u2261 \u2202~\nx + \u2202~\nx \u2202a . This is a DE that needs\nsolving with equivalent care to which the trajectory was, and it may also be stiff. Note\nthat in this equation, r\u0304 is the full r\u0304 as defined in Eq. 22. Also, in the case of an episodic\nproblem where a final impulse of reward is given, an alternative boundary condition to this\none may be required-see Appendix E.1 for a discussion and example.\nFor this policy, model and \u03bb\u0304 = 0, the PGL weight update is:\n\u0012 \u03c0\u0013\n\u0013\nZ F\u0012\n\u2202R\n\u2202G\n(25)\n\u03a9\u0304t (G\u2032 t \u2212 Gt )dt\n\u2206w\n~ =\u03b1\n=\u03b1\n\u2202w\n~ 0\n\u2202\nw\n~\n0\nt\n\u0010  \u0304\u0011T \u0010  \u0304\u0011\n2\n\u2202f\n\u2202f\nt\nwhere \u03a9\u0304t = 1\u2212a\nc\n\u2202a t\n\u2202a t and is positive semi-definite.\nThis integral is the exact equation for gradient ascent on R\u03c0 . Therefore, if implemented\nprecisely, termination will occur at a constrained (with respect to w)\n~ locally optimal trajectory (see Appendix A for optimality definitions). However, numerical methods are required\nto evaluate this integral and the other DEs in this section. For example, the above integral\nis most simply approximated as:\n\n\u2206w\n~ =\u03b1\n\n\u0013\nF \u0012\nX\n\u2202G\nt=0\n\n\u2202w\n~\n\n\u03a9\u0304t (G\u2032 t \u2212 Gt )\u2206t\nt\n\nwhich is very similar to the discrete-time case (Eq. 18).\nThe fact that this algorithm is gradient ascent means it can be speeded up with any\nof the fast optimisers available, e.g. RPROP (Riedmiller and Braun, 1993), which becomes\nvery useful when c is small and therefore \u2206w\n~ becomes very small.\n18\n\n\fReinforcement Learning by Value-Gradients\n\nEq. 25 was derived for \u03bb\u0304 = 0 although it can, in theory, be applied to other \u03bb\u0304. In this\ncase, it is thought to be necessary to choose a full-rank version of \u03a9\u0304. However our results\nwith bootstrapping with a function approximator are not good (see Section 4.5).\n2.3 Residual Gradients\nIn this section we derive the formulae for full gradient descent on the value-gradient error\nfunction, according to Eq. 12 and Eq. 13. The particularly promising thing about this\napproach is that it has good convergence guarantees for any \u03bb. This kind of full gradient\ndescent method is known as residual gradients by Baird (1995) or as using the Galerkinized\nequations by Werbos (1998).\nTo calculate the total derivative of E, it is easier to first write E in recursive form:\n1\n~\nE(~xt , w)\n~ = (Gt \u2212 G\u2032 t )T \u03a9t (Gt \u2212 G\u2032 t ) + E(~xt+1 , w)\n2\nwith E(~xF , w)\n~ = 0 at the terminal time step. This gives a total derivative:\n\u0013\n\u0012\u0012\n\u0013\n\u0012 \u2032\u0013 \u0013\n\u0012\n\u0013 \u0012 \u0013 \u0012\n\u0013\n\u0012\n\u0013\n\u0012\n\u2202G\n\u2202G\n\u2202\u03c0\n\u2202f\n\u2202E\n\u2202E\n\u2202E\n\u2032\n=\n\u2212\n\u03a9t (Gt \u2212 G t ) +\n+\n\u2202w\n~ t\n\u2202w\n~ t\n\u2202w\n~ t\n\u2202w\n~ t \u2202a t \u2202~x t+1\n\u2202w\n~ t+1\nwith\n\u0012\n\n\u2202E\n\u2202~x\n\n\u2202E\n\u2202w\n~ F\n\n= ~0 and where\n\n\u0013\n\n\u0012\u0012\n\n\u0001\n\n=\nt\n\n\u2202G\n\u2202~x\n\n\u0013\n\n\u2212\nt\n\n\u0012\n\n\u2202E\n\u2202~\nx t+1\n\n\u2202G\u2032\n\u2202~x\n\n\u0001\n\n\u0013\u0013\n\nis found recursively by\n\u2032\n\n\u03a9t (Gt \u2212 G t ) +\n\nt\n\n\u0012\u0012\n\n\u2202\u03c0\n\u2202~x\n\n\u0013 \u0012\nt\n\n\u2202f\n\u2202a\n\n\u0013\n\n+\nt\n\n\u0012\n\n\u2202f\n\u2202~x\n\n\u0013 \u0013\u0012\nt\n\n\u2202E\n\u2202~x\n\n\u0013\n\nt+1\n\n\u0001\n~\nwith \u2202E\n\u2202~\nx F = 0. Note that this goes further than doing residual-gradients for VL in that\nthere is a consideration for \u2202E\n\u2202~\nx . This is necessary for true gradient-descent on E with respect\nto the weights, since in this paper we say the value function and greedy policy are tightly\ncoupled, and therefore updating w\n~ will immediately change the trajectory. This can be\nverified by evaluating \u2202E\nnumerically.\n\u2202w\n~\nAlthough this weight update performs relatively well in the experiments of Section 4,\nour general experience of this algorithm is that it often gets stuck in far-from-optimal local\nminima. This was quite puzzling and was not explained by previous criticisms of residual\ngradients (for example, see Baird, 1995), since these only applied to stochastic scenarios.\nIt seems that many of the local minima of E are not necessarily valid local maxima of\nR\u03c0 . We speculate that choosing to include the residual gradient terms is analogous to\nchoosing to maximise a function f (x) by gradient descent on (f \u2032 (x))2 . This makes it difficult\nto distinguish the maxima from the unwanted minima, inflections and saddle points, and\nalthough the situations are not identical, an effect like this may be reducing the effectiveness\nof the residual gradients weight update. This contrasts with the non-residual gradients\napproach (Eq. 14) where \u2206w\n~ = \u03b1 \u2202R\n\u2202w\n~ which is analogous to maximising f (x) directly (if\n\u03bb = 1; see Section 2.1). By the arguments of Section 1.4, we would expect this explanation\nto apply to VL with residual gradients too.\nTo illustrate this problem by a specific example, consider a variant of the 1-step Toy\nProblem with k = 0, modified so that the final reward is R(x1 ) = \u2212x1 2 + 4 cos(x1 ) instead\nof the usual R(x1 ) = \u2212x1 2 . Then the optimal policy is \u03c0 \u2217 (x0 ) = \u2212x0 . Let the function\n19\n\n\fFairbank\n\n10\n5\ny\n0\n-5\n-10\n\ny = R\u03c0 (w)\ny = E(w)\n-4 -2\n\n0\nw\n\n2\n\n4\n\nFigure 5: Graphs showing the spurious minima traps that can exist for Residual Gradient\nmethods compared to direct methods.\n\napproximator be V (x1 , w) = \u2212x1 2 + wx1 , so that the greedy policy with this model gives\n\u03c0(x0 , w) = w/2 \u2212 x0 . This gives R\u03c0 (x0 , w) = \u2212w2 /4 + 4 cos(w/2) which has just one\nmaximum, at w = 0, corresponding to the optimal policy. The residual value-gradient\nerror is E(x0 , w) = 12 (G1 \u2212 G\u2032 1 )2 = 21 (w \u2212 4 sin(w/2))2 which has many local minima (see\nFigure 5), only one of which corresponds to the optimal policy. The spurious minima in\nE correspond to points of inflection in R0\u03c0 , of which there are infinitely many. Therefore\ngradient descent on E is more likely than not to converge to an incorrect solution, whereas\ngradient ascent on R\u03c0 will converge to the correct solution.\n\n3. Actor-Critic Architectures\nThis section discusses the use of actor-critic architectures with VGL. It shows that in some\ncircumstances the actor-critic architecture can be shown to be equivalent to a simpler architecture. While this can be used to provide convergence guarantees for the actor-critic\narchitecture, it also makes the actor-critic architecture redundant in these circumstances.\nAn actor-critic architecture (Barto et al., 1983; Werbos, 1998; Konda and Tsitsiklis,\n2003) uses two neural-networks, or more generally two function approximators, in a control\nproblem. The first neural-network, parametrised by weight vector ~z, is the \"actor\" which\nprovides the policy, \u03c0(~x, ~z). In an actor-critic architecture the greedy policy is not used,\nsince the actor neural-network is the policy. The second neural-network, parametrised by a\nweight vector w,\n~ is the \"critic\" and provides the value function V (~x, w).\n~\nFor this section we extend the definition of V \u2032 and G\u2032 to apply to trajectories found by\npolicies other than the greedy policy. To define V \u2032 for an arbitrary policy \u03c0(~x, ~z) and value\nfunction V (~x, w)\n~ we use:\nV \u2032 (~x, w,\n~ ~z) = r(~x, \u03c0(~x, ~z)) + \u03bbV \u2032 (f (~x, \u03c0(~x, ~z)), w,\n~ ~z) + (1 \u2212 \u03bb)V (f (~x, \u03c0(~x, ~z)), w)\n~\n\u2032\n\n(26)\n\nx,w,~\n~ z)\nand for a given trajectory we can\nwith V \u2032 (~xF , w,\n~ ~z) = 0. This gives G\u2032 (~x, w,\n~ ~z) = \u2202V (~\n\u2202~\nx\n\u2032\n\u2032\n\u2032\n\u2032\n~ ~z).\n~ ~z) and G t = G (~xt , w,\nuse the shorthand V t = V (~xt , w,\nThe value-gradient version of the actor training equation is as follows (Werbos, 1998,\nEq. 11):\n\u0012 \u0013\n\u0013\nX \u0012 \u2202\u03c0 \u0013 \u0012\u0012 \u2202r \u0013\n\u2202f\n+\nGt+1\n(27)\n\u2206~z = \u03b1\n\u2202~z t\n\u2202a t\n\u2202a t\nt\u22650\n\n20\n\n\fReinforcement Learning by Value-Gradients\n\nwhere \u03b1 is a small positive learning-rate and Gt+1 is calculated by the critic. This equation is\nalmost identical\nto the PGL equation (BPTT, Eq. 15) except that Gt+1 has been substituted\n\u0001\n\u2202R\u03c0\nfor \u2202~x t+1 . Eq. 27 is a non-standard actor training equation, however in Appendix F we\nprove it is equivalent to at least one other actor training equation and demonstrate how it\nautomatically incorporates exploration.\nThe critic training equation would attempt to attain Gt = G\u2032 t for all t by some appropriate method, for example by Eq. 14, which is the only update we consider here.\nIn this section, it is useful to define the notion of a hypothetical ideal function approximator. This is a function approximator that can be assumed to have enough degrees of\nfreedom, and a strong enough learning algorithm, to attain its desired objectives exactly,\ne.g. Gt = G\u2032 t for all t, exactly. We also refer to an ideal critic and an ideal actor, which are\nbased on ideal function approximators.\nTraining an actor and a critic together gives several possibilities of implementation;\neither one could be fixed while training the other, or they could both be trained at the\nsame time. We only analyse the situations of keeping one fixed while training the other.\nDoing a long iterative process (training one) within another long iterative process (training\nthe other) is very bad for efficiency, which may make the cases analysed seem infeasible\nand therefore of little relevance. However, the analyses below show both of these situations\nhave equivalent architectures that are efficient and feasible to implement, and therefore are\nrelevant.\nIt is noted that the results in this section should also apply to VL actor-critic system,\nsince as discussed in Section 1.4, VL is a stochastic approximation to VGL.\n3.1 Keeping the Actor Fixed while training the Critic\nIn this scenario we keep the actor fixed while training an ideal critic fully to convergence,\nand then apply one iteration of actor training, and then repeat until both have converged.\nIt is shown that, if the critic is ideal, then this scenario is equivalent to back-propagation\nthrough time (BPTT) for any \u03bb.\nSince the actor is fixed, the trajectory is fixed; therefore an ideal critic will be able to\nattain the objective of\u0001 Gt = G\u2032 t for all t along the trajectory. Then since Gt = G\u2032 t for all\n\u03c0\nt, we have Gt = \u2202R\n\u2202~\nx t for all t (proof is in Appendix A, lemma 5). Therefore the actor's\nweight update equation (Eq. 27) becomes identical to Eq. 15. Therefore we could omit\n\u03c0\nthe critic and replace G with \u2202R\n\u2202~\nx in the actor training equation (i.e. we remove the inner\niterative process, and remove the actor-critic architecture), and we are left with BPTT.\nThis shows that this actor-critic is guaranteed to converge, since it is the same as BPTT.\nThe above argument assumed the critic was ideal. This may be an unrealistic assumption\nsince a real function approximator can have only finite flexibility. However, the objective\nof the function approximator is to learn Gt = G\u2032 t for all t, and this goal can be achieved\nexactly; simply by removing the critic. In effect, by removing the critic, a virtual ideal\nfunction approximator is obtained. It is assumed that there would be no advantage in using\na non-ideal critic.\nConclusion: BPTT is equivalent to the idealised version of this architecture, and therefore the idealised version of this architecture is guaranteed to converge to a constrained\n21\n\n\fFairbank\n\nlocally optimal trajectory (see Appendix A for optimality definitions). The idealised version of this architecture is attainable by removing the critic.\n3.2 Keeping the Critic Fixed while training the Actor\nIn this scenario we consider keeping the critic fixed while training an ideal actor fully to\nconvergence, and then apply one iteration of training to the critic and repeat.\nThe actor weight update equation (Eq. 27) tries to maximise Q(~xt , at , w)\n~ with respect\nto at at each time step t. If this is fully achieved, then the greedy policy will be satisfied.\nTherefore the ideal actor can be removed and replaced by the greedy policy, to get the same\nalgorithm. This again removes the innermost iterative step, and removes the actor-critic\narchitecture. Again, it is assumed that there would be no advantage in using a non-ideal\nactor.\nNow when it comes to the critic-training step (Eq. 14), do we allow for the fact that\nchanging the critic weights is going to change the actor in a predictable way? If so, then\nwe treat \u03a9t as defined in Eq. 19. Otherwise we are working as if the actor and critic are\nfully separated, and we are free to choose \u03a9t . Having made this choice and substitution,\nthe actor is redundant.\nConclusion: Keeping the critic fixed while training an idealised actor is equivalent to\nusing just the critic with a greedy policy. This idealised architecture can be efficiently\nattained by removing the actor.\n\n4. Experiments\nIn this section a comparison is made between the performance of several weight update\nstrategies on various tasks. The weight update formulae considered are summarised in\nTable 1.\nThe first few experiments are all based on the n-step Toy Problem defined in Section\n1.1.1 The choice of this problem domain was made because it is smooth, deterministic,\nconcave and possible to make the experiments easily describable and reproducible. Within\nthis choice, it makes a level playing field for comparison between VL and VGL algorithms.\nThe final experiment is a neural-network based experiment on a Lunar-Lander problem\nspecified in Appendix E. This is a new benchmark problem defined for this paper; the\nproblem with most current existing benchmark problems was that they are only defined for\ncontinuing tasks, discrete state-space tasks or tasks not well suited to local exploration.\nIn the Toy Problem experiments, all weight components were initialised with a uniform\nrandom distribution over a range from \u221210 to +10. The experiments were based on 1000\ntrials. The stopping criteria used for each trial were as follows: A trial was considered a\nsuccess when |wi \u2212 wi\u2217 | < 10\u22127 for all components i. A trial was considered a failure if\nany component of w\n~ became too large for the computer's accuracy, or when the number of\niterations in any trial exceeded 107 . A function RN D(\u01eb) is defined to return a normally\ndistributed random variable with mean 0 and standard deviation \u01eb. The variables c1 , c2 ,\nc3 are real constants specific to each experiment designed to allow further variation in the\nproblem specifications.\n22\n\n\fReinforcement Learning by Value-Gradients\n\nWeight Update Formula\nValue-Learning Update, TD(\u03bb), (Eq. 10).\nValue-Gradient Learning Update (Eq. 18), using full \u03a9t matrix from\nEq. 19.\nValue-Gradient Learning Update (Eq. 14), but using \u03a9t as the identity\nmatrix for all t.\nValue-Gradient Learning Update, residual gradients (Eq. 13), but using\n\u03a9t as the identity matrix for all t.\n\nAbbreviation\nVL(\u03bb)\nVGL\u03a9(\u03bb)\nVGL(\u03bb)\nVGLRG(\u03bb)\n\nTable 1: Weight update formulae considered.\n4.1 Experiment 1: One-step Toy Problem\nIn this experiment the one-step Toy Problem with k = 0 was considered from a fixed start\npoint of x0 = 0. A function approximator for V with just two parameters (w1 , w2 ) was\nused, and defined separately4 for the two time steps:\n\u001a\n\u2212(x1 \u2212 c1 )2 + w1 x1 + w2 if t = 1\nV (xt , w1 , w2 ) =\n0\nif t = 2\nwhere c1 is a real constant.\nUsing this model and function approximator definition, it is possible to calculate the\nfunctions Q(x, a, w),\n~ G(x, w)\n~ and the \u01eb-greedy policy \u03c0(x, w)\n~ (which again must be defined\ndifferently for each time step). These functions are listed in the left-hand column of Table\n2. Using these formulae, and the model functions again, the full trajectory is calculated in\nthe right-hand column of Table 2. Also in this right-hand column, V \u2032 , G\u2032 and \u03a9 have been\ncalculated for each time step using Eq. 4, Eq. 6 and Eq. 19 respectively. These formulae\nwould have to be evaluated sequentially from top to bottom.\nThe \u01eb-greedy policy was necessary for the VL experiments. When applying the weight\n\u2202G\nupdate formulae, expressions for \u2202V\n\u2202w\n~ and \u2202 w\n~ were calculated analytically from the functions\ngiven\nof Table\n\u0010\n\u0011in the left\n\u0011\n\u0010 column\n\u0010 2.\u0011 For example, for this function approximator we find\n\u2202V\n\u2202V\n\u2202G\n= x1 , \u2202w2 = 1 and \u2202w\n= 1, etc.\n\u2202w1\n1\n1\n\n1\n\n1\n\nNote that as w2 does not affect the trajectory, this component was not used as part of\nthe stopping criteria.\nResults for these experiments using the VL(\u03bb) and VGL(\u03bb) algorithms are shown in\nTable 3. This set of experiments verifies the failure of VL when exploration is removed;\nthe slowing down of VL when it is too low; and the blowing-up of VL when it is too high\n(in this case failure tended to occur because the size of weights exceeded the computer's\nrange). The efficiency and success rate of the VGL experiments is much better than for the\nVL experiments, and this is true for both values of c1 tested. In fact, the problem is trivially\neasy for the VGL(\u03bb) algorithm, but causes the VL(\u03bb) algorithm considerable problems.\nTo gain some further insight into the different behaviour of these two algorithms we can\nlook at the differential equations that the weights obey. For the VGL(\u03bb) system of this\n4. See Section 1.1.1 for an explanation on this abuse of notation.\n\n23\n\n\fFairbank\n\nFunction approximator and\n\u01eb-Greedy Policy\nTime step 1:\nV (x1 , w1 , w2 ) = \u2212(x1 \u2212 c1 )2 + w1 x1 + w2\n\u21d2 G(x1 , w1 , w2 ) = 2(c1 \u2212 x1 ) + w1\nQ(x0 , a0 , w)\n~ = \u2212(x0 + a0 \u2212 c1 )2\n+ w1 (x0 + a0 ) + w2\n\u03c0(x0 , w)\n~ = (2c1 \u2212 2x0 + w1 )/2 + RN D(\u01eb)\nTime step 2:\nV (x2 , w1 , w2 ) = 0\n\u21d2 G(x2 , w1 , w2 ) = 0\nOptimal Policy: \u03c0 \u2217 (x0 ) = \u2212x0\nOptimal Weights: w1 \u2217 = \u22122c1\n\nSequential trajectory\nequations\nx0 \u2190 0\na0 \u2190 (2c1 \u2212 2x0 + w1 )/2 + RN D(\u01eb)\nx 1 \u2190 x 0 + a0\nV \u2032 1 \u2190 \u2212x1 2\nG\u2032 1 \u2190 \u22122x1\nV1 \u2190 \u2212(x1 \u2212 c1 )2 + x1 w1 + w2\nG1 \u2190 2(c1 \u2212 x1 ) + w1\n\u03a90 \u2190 12\n\nTable 2: Functions and Trajectory Variables for Experiment 1.\n\nc1\n\n\u01eb\n\nSuccess\nrate\n\n0\n0\n0\n0\n10\n\n10\n1\n0.1\n0\n1\n\n66.4%\n100.0%\n100.0%\n0.0%\n99.4%\n\n0\n10\n\n0\n0\n\n100.0%\n100.0%\n\n\u03b1 = 0.01\n\u03b1 = 0.1\nIterations\nSuccess\nIterations\n(Mean)\n(s.d.)\nrate (Mean)\n(s.d.)\nResults for algorithm VL(\u03bb)\n1075.1 293.35\n0.0%\n1715.8 343.31\n87.6%\n163.52 31.948\n172445 31007\n89.5%\n17160 3033.6\n0.0%\n6048.5 270.86\n0.0%\nResults for algorithm VGL(\u03bb)\n1728.2 112.05 100.0%\n166.15 11.481\n1898.5 51.986 100.0%\n181.59 14.861\n\n\u03b1 = 1.0\nSuccess\nIterations\nrate (Mean)\n(s.d.)\n0.0%\n3.8%\n16.5%\n0.0%\n0.0%\n100.0%\n100.0%\n\n134.86\n1527.6\n\n59.643\n118.39\n\n1\n1\n\n0\n0\n\nTable 3: Results for Experiment 1. Note because this is a 1-step problem, \u03bb is irrelevant.\n\n24\n\n\fReinforcement Learning by Value-Gradients\n\nexperiment, by going through the equations of the right-hand column of Table 2 and the\nVGL(\u03bb) weight update equations, we can eliminate all variables except for the weights and\nconstants to obtain a self-contained pair of weight update equations:\n\u001a\n\u2206w1 = \u2212\u03b1 (2c1 + w1 )\n\u2206w2 = 0\nTaking \u03b1 to be sufficiently small, these become a pair of coupled differential equations. The\nsolution is a straight line across the w\n~ plane directly to the optimal solution w1\u2217 = \u22122c1 .\nDoing the same for the VL(\u03bb) system, and integrating over the random variable RN D(\u01eb)\nto average out the effects of exploration, gives a similar pair of coupled weight update\nequations:\n\uf8f1\n\u0011\n\u0001\u0010\n\uf8f2 h\u2206w1 i = \u2212\u03b1 c1 + w1 2\u01eb2 + c1 2 + 2c1 w1 + w1 2 + w2\n2\n2\n\u0011\n\u0010\n\uf8f3 h\u2206w2 i = \u2212\u03b1 c1 2 + 2c1 w1 + w1 2 + w2\n2\n\nThere is no known full analytical solution to this pair of equations. However it is clear\n\u0001\nthat the second equation is continually aiming to achieve w2 = \u2212 c1 2 + 2c1 w1 + w1 2 /2 .\nIn the case that this is achieved, both equations would then simplify to the VGL coupled\nequations, but with a magnitude proportional to \u01eb2 . This shows that if \u01eb = 0, the valuegradient part of these equations vanishes. It is also noted that in this case experiments show\nlearning fails. Hence it is speculated that none of the other terms in the VL(\u03bb) coupled\nequations are doing anything beneficial, and that it is unlikely they will ever do so even in\nmore complicated systems. Very informally, this example illustrates how VGL applies just\nthe \"important bits\" of a VL weight update (in this example at least).\n4.2 Experiment 2: Two-step Toy Problem, with Sufficiently Flexible Function\nApproximator\nIn this experiment the two-step Toy Problem with k = 1 is considered from a fixed start\npoint of x0 = 0. A function approximator is defined differently at each time step, by four\nweights in total:\n\uf8f1\n\uf8f2 \u2212c1 x1 2 + w1 x1 + w2 if t = 1\nV (xt , w1 , w2 , w3 , w4 ) =\n\u2212c2 x2 2 + w3 x2 + w4 if t = 2\n\uf8f3\n0\nif t = 3\n\nwhere c1 and c2 are real positive constants. The consequential functions and variables for\nthis experiment are found and presented in a similar manner as for Experiment 1, in Table\n4.\nFor ease of implementation of the residual-gradients algorithm, the expressions in the\nright-hand column of Table 4 for Gt and G\u2032 t were used to implement a sum-of-squares error\nfunction E(w)\n~ (Eq. 12), with \u03a9t = 1. Numerical differentiation on this function was then\nused to implement the gradient descent. For a larger scale system, it would be more efficient\nand accurate to use the recursive equations given in Section 2.3.\nResults for the experiments are given in Table 5. These results show all VGL experiments\nperforming significantly better than the corresponding VL experiments; in most cases by\naround two orders of magnitude. The results also show that for all of the VGL(\u03bb) algorithm\n25\n\n\fFairbank\n\nFunction approximator and\n\u01eb-Greedy Policy\nTime step 1:\nV (x1 , w1 , w2 ) = \u2212c1 x1 2 + w1 x1 + w2\n\u21d2 G(x1 , w1 , w2 ) = \u22122c1 x1 + w1\nQ(x0 , a0 , w)\n~ = \u2212ka0 2 \u2212 c1 (x0 + a0 )2\n+ w1 (x0 + a0 ) + w2\n1 \u22122c1 x0\n+ RN D(\u01eb)\n\u03c0(x0 , w)\n~ = w2(c\n1 +k)\nTime step 2:\nV (x2 , w3 , w4 ) = \u2212c2 x2 2 + w3 x2 + w4\n\u21d2 G(x2 , w3 , w4 ) = \u22122c2 x2 + w3\nQ(x1 , a1 , w)\n~ = \u2212ka1 2 \u2212 c2 (x1 + a1 )2\n+ w3 (x1 + a1 ) + w4\n3 \u22122c2 x1\n+ RN D(\u01eb)\n\u03c0(x1 , w)\n~ = w2(c\n2 +k)\nTime step 3:\nV (x3 ) = 0\n\u21d2 G(x3 ) = 0\n\nSequential trajectory\nequations\nx0 \u2190 0\n1 \u22122c1 x0\na0 \u2190 w2(c\n+ RN D(\u01eb)\n1 +k)\nx1 \u2190 x0 + a0\n3 \u22122c2 x1\n+ RN D(\u01eb)\na1 \u2190 w2(c\n2 +k)\nx2 \u2190 x1 + a1\nV \u2032 2 \u2190 \u2212x2 2\nG\u2032 2 \u2190 \u22122x2\nV2 \u2190 \u2212c2 x2 2 + w3 x2 + w4\nG2 \u2190 \u22122c2 x2 + w3\n\u03a91 \u2190 2(c21+k)\nV \u2032 1 \u2190 \u2212ka1 2 + \u03bbV \u2032 2 + (1 \u2212 \u03bb)V2\n\u2032 +(1\u2212\u03bb)G )\n2\n2\nG\u2032 1 \u2190 2c2 ka1 +k(\u03bbG\nc2 +k\nV1 \u2190 \u2212c1 x1 2 + w1 x1 + w2\nG1 \u2190 \u22122c1 x1 + w1\n\u03a90 \u2190 2(c11+k)\n\nOptimal Weights: w1 \u2217 = w3 \u2217 = 0\nTable 4: Functions and Trajectory Variables for Experiment 2.\nresults, increasing \u03b1 from 0.01 to 0.1 brings the number of iterations down by a factor\nof approximately 10, which hints that further efficiency of the VGL algorithms could be\nattained.\nThe optimal value function, denoted by V \u2217 , for this experiment is\n\u2217\n\nV (xt ) =\n\n\u001a\n\nk\nx1 2 if t = 1\n\u2212 1+k\n\u2212x2 2\nif t = 2\n\nFor this reason most experiments were done with c1 = 21 and c2 = 1. However the only\nnecessity is to have c1 > 0 and c2 > 0, since these are required to make the greedy policy\nproduce continuous actions; a problematic issue for all value function architectures.\n4.3 Experiment 3: Divergence of Algorithms With Two-step Toy Problem\nWe now study the Toy Problem to try to find a set of parameters that cause learning to\nbecome unstable. Surprisingly the two-step Toy Problem is sufficiently complex to provide\nexamples of divergence, both with and without bootstrapping. By the principles argued\nin Section 1.4, we would expect these examples that were found for VGL to also cause\ndivergence with the corresponding VL methods. This is confirmed empirically.\nIf we take the previous experiment and consider the VGL\u03a9(\u03bb) weight update then the\nonly two weights that change are w\n~ = (w1 , w3 )T . The weight update equation for these two\nweights can be found analytically by substituting all the equations of the right hand side of\n26\n\n\fReinforcement Learning by Value-Gradients\n\nWeight Update\nAlgorithm (\u03bb)\nVL(1)\nVL(1)\nVGL(1)\nVGL\u03a9(1)\nVGLRG(1)\nVL(0)\nVL(0)\nVGL(0)\nVGL\u03a9(0)\nVGLRG(0)\nVL(1)\nVGL(1)\nVL(1)\nVGL(1)\n\n\u01eb\n1\n0.1\n0\n0\n0\n1\n0.1\n0\n0\n0\n0.1\n0\n0.1\n0\n\nc1\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\n4\n4\n0.1\n0.1\n\nc2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\nSuccess\nrate\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n\n\u03b1 = 0.01\nIterations\n(Mean)\n(s.d.)\n244122 252234\n135588\n17360\n1596.8\n72.58\n6089.1 340.09\n794.6\n40.30\n244368 252114\n138073\n17630\n1743.7 103.41\n6516.5 375.99\n1252.4\n92.81\n228336\n60829\n5034.7\n340.6\n134443\n16614\n1516.2\n89.5\n\nSuccess\nrate\n91.3%\n100.0%\n100.0%\n100.0%\n100.0%\n91.6%\n99.9%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n\n\u03b1 = 0.1\nIterations\n(Mean)\n(s.d.)\n736030 743920\n21406.6\n8641\n152.5\n13.79\n600.7\n47.06\n72.2\n5.36\n734029 742977\n21918\n8664\n166.2\n12.81\n643.0\n39.12\n118.2\n11.63\n78364\n62085\n495.1\n35.7\n20974\n8569\n144.4\n13.8\n\nTable 5: Results for various algorithms on Experiment 2.\nTable 4 into the VGL\u03a9(\u03bb) weight update equation, and using \u01eb = 0, giving:\n\u2206w\n~ = \u03b1DED w\n~\n\u0013\n\u0001\n(k + \u03bb(1 + b)(b(k + 1) + 1) \u2212 bk) (\u03bb(k + 1)(b + 1) \u2212 k)\n, b = \u2202\u03c0\nwith E = \u22122\n\u2202x 1 =\n1 + b(k + 1)\n(k + 1)\n\u0012\n\u0013\n1/2(k + c1 ) 0\n\u2212c2\nc2 +k and D =\n0\n1/2(k + c2 )\nWe can consider more types of function approximator by defining the weight vector w\n~\nT\nto be linear system of two new weights ~p = (p1 , p2 ) such that w\n~ = F ~p and where F is a\n2 \u00d7 2 constant real matrix. If the VGL\u03a9(\u03bb) weight update equation is now recalculated for\nthese new weights then the dynamic system for p~ is:\n\u0012\n\n\u2206~\np = \u03b1(F T DEDF )~\np\n\n(28)\n\nTaking \u03b1 to be sufficiently small, then the weight vector p~ evolves according to a\ncontinuous-time linear dynamic system, and this system is stable if and only if the matrix product F T DEDF is stable (i.e. if the real part of every eigenvalue of this matrix\nproduct is negative).\nThe VGL(\u03bb) system weight update can also be derived and that system is identical to\nEq. 28 but with the leftmost D matrix omitted.\n\u0012\n\u0013\n10 1\n\u22121\nChoosing \u03bb = 0, with c1 = c2 = k = 0.01 and F = D\nleads to divergence\n\u22121 \u22121\nfor both the VGL(\u03bb) and VGL\u03a9(\u03bb) systems. Empirically, we found that these parameters\ncause the VL(0) algorithm to diverge too. This is a specific counterexample for the VL\n27\n\n\fFairbank\n\nAlgorithm (\u03bb)\nVL(1)\nVL(0)\nVGL(1)\nVGL(0)\nVGL\u03a9(1)\nVGL\u03a9(0)\n\nDivergence\nexample found\nYes\nYes\nYes\nYes\nNo\nYes\n\nProven\nto converge\nNo\nNo\nNo\nNo\nYes\nNo\n\nTable 6: Results for Experiment 3: Which algorithms can be made to diverge?\nsystem which is \"on-policy\" and equivalent to Sarsa. Previous examples of divergence for a\nfunction approximator with bootstrapping have usually been for \"off-policy\" learning (see\nfor example, Baird, 1995; Tsitsiklis and Roy, 1996b). Tsitsiklis and Roy (1996a) describe\nan \"on-policy\" counterexample for a non-linear function approximator, but this is not for\nthe greedy policy.\nAlso, perhaps surprisingly, it is possible to get instability with\u0012\u03bb = 1 with\u0013the VGL(\u03bb)\n\u22121 \u22121\nsystem. Substituting c2 = k = 0.01, c1 = 0.99 and F = D \u22121\nmakes the\n10 1\nVGL(1) system diverge. This result has been empirically verified to carry over to the VL(1)\nsystem too, i.e. this is a result where Sarsa(1) and TD(1) diverge. This highlights the\ndifficulty of control-problems in comparison to prediction tasks. A prediction task is easier,\nsince as Sutton (1988) showed, the \u03bb = 1 system is equivalent to gradient descent on the\nsum-of-squares error E = \u03a3t (V \u2032 t \u2212 Vt )2 , and so convergence is guaranteed for a prediction\ntask. However in a control problem, even when there is no bootstrapping, changing one\nvalue of V \u2032 t affects the others by altering the greedy actions. This problem is resolved by\nusing the VGL\u03a9(1) weight update.\nThe results of this section are summarised in Table 6.\n4.4 Experiment 4: Two-step Toy Problem, with Insufficiently Flexible\nFunction Approximator\nIn this experiment the two-step Toy Problem with k = 2 was considered from a fixed start\npoint of x0 = 0. A function approximator with just one weight component, (w1 ), was\ndefined differently at each time step:\n\uf8f1\nif t = 1\n\uf8f2 \u2212c1 x1 2 + w1 x1\nV (xt , w1 ) =\n\u2212c2 x2 2 + (w1 \u2212 c3 )x2 if t = 2\n\uf8f3\n0\nif t = 3\n\nHere c1 = 2, c2 = 0.1 and c3 = 10 are constants. These were designed to create some conflict\nfor the function approximator's requirements at each time step. The optimal actions are\na0 = a1 = 0, and therefore the function approximator would only be optimal if it could\n(x,w)\n~\n= 0 at x = 0 for both time steps. The presence of the c3 term makes this\nachieve \u2202V \u2202x\nimpossible, so a compromise must be made.\n28\n\n\fReinforcement Learning by Value-Gradients\n\nSequential equations\nx0 \u2190 0\nG\u2032 2 \u2190 \u22122x2\nw1 \u22122c1 x0\nG2 \u2190 \u22122c2 x2 + w1\na0 \u2190 2(c1 +k)\nx 1 \u2190 x 0 + a0\n3 \u22122c2 x1\na1 \u2190 w1 \u2212c\n2(c2 +k)\nx 2 \u2190 x 1 + a1\n\u03a90 \u2190 2(c11+k)\n\n\u2032\n\n2 +(1\u2212\u03bb)G2 )\nG\u2032 1 \u2190 2c2 ka1 +k(\u03bbG\nc2 +k\nG1 \u2190 \u22122c1 x1 + w1\n\u03a91 \u2190 2(c21+k)\n\nTable 7: Trajectory Variables for Experiment 4.\n\nOnly the value-gradient algorithms were considered in this experiment; hence there was\nno need for exploration or use of the \u01eb-greedy policy. The equations for the trajectory are\nvery similar as to Experiment 2, and so only the key results are listed in Table 7. A different\nstopping criterion was used in this experiment, since each algorithm converges to a different\nfixed point. The stopping condition used was |\u2206w1 | < (10\u22127 \u03b1). Once the fixed point had\nbeen reached we noted the value of R, the total reward for that trajectory, in Table 8. Each\nalgorithm experiment used \u03b1 = 0.01, attained 100% convergence, and produced the same\nvalue of R each run. In these trials, \u03b1 was not varied, and the iteration result columns are\nnot very meaningful, since it can be shown for each algorithm that \u2206w1 is a linear function\nof w1 . This indicates that any of the algorithms could be made arbitrarily fast by fine\ntuning \u03b1, and also confirms that there is only one fixed point for each algorithm.\nThe different values of R in the results show that the different algorithms have varying\ndegrees of optimality with respect to R. The first algorithm (VGL\u03a9(1)) is the only one\nthat is really optimal with respect to R (subject to the constraints imposed by the awkward\nchoice of function approximator), since it is equivalent to gradient ascent on R\u03c0 , as shown\nin section 2.1.\nIt is interesting that the other algorithms converge to different suboptimal points, compared to VGL\u03a9(1). This shows that introducing the \u03a9t terms and using \u03bb = 1 balances\nthe priorities between minimising (G\u2032 1 \u2212 G1 )2 and minimising (G\u2032 2 \u2212 G2 )2 appropriately so\nas to finish with an optimal value for R. Different values for the constants c1 and c2 were\nchosen to emphasise this point. The relative rankings of the other algorithms may change\nin other problem-domains, but it is expected that VGL\u03a9(1) would always be at the top.\nHence we use this algorithm as our standard choice of algorithm out of those listed in this\npaper; to be used in conjunction with a robust optimiser, e.g. RPROP.\n4.5 Experiment 5: One-Dimensional Lunar-Lander Problem\nThe objective of this experiment was to learn the \"Lunar-Lander\" problem described in\nAppendix E. The value-function was provided by a fully connected multi-layer perceptron\n(MLP) (see Bishop (1995) for details). The MLP had 3 inputs, one hidden layer of 6 units,\nand one output in the final layer. Additional shortcut connections connected all input units\ndirectly to the output layer. The activation functions were standard sigmoid functions in the\ninput and hidden layers, and an identity function (i.e. linear activation) in the output unit.\n29\n\n\fFairbank\n\nWeight Update\nAlgorithm (\u03bb)\nVGL\u03a9(1)\nVGLRG(1)\nVGL(1)\nVGL\u03a9(0)\nVGLRG(0)\nVGL(0)\n\nR\n\u22122.65816\n\u22122.68083\n\u22122.79905\n\u22122.82344\n\u22123.97316\n\u22125.76701\n\nIterations\n(Mean) (s.d.)\n2327\n247\n183\n19\n500\n51\n3532\n352\n256\n21\n1077\n87\n\nTable 8: Results for Experiment 4, ranked by R.\n\nThe input to the neural-network was (h/100, v/10, u/50) T , and the output was multiplied by\n100 to give the value function. Each weight in the neural-network was initially randomised\nto lie in [\u22121, 1], with uniform probability distribution.\nIn this section results are presented as pairs as diagrams. The left-hand diagrams show\na graph of the total reward for all trajectories versus the number of training iterations, and\ncompare performance to that of an optimal policy. The optimal policy's performance was\ncalculated by the theory described in Appendix E.2. That appendix also shows example\noptimal trajectories. The right-hand diagrams show a cross-section through state space,\nwith the y-axis showing height and the x-axis showing velocity. The final trajectories\nobtained are shown as curves starting at a diamond symbol and finishing at h = 0.\nAll algorithms used in this section were the continuous-time counterparts to those stated,\nand are described in Section 2.2. Also, all weight updates were combined with RPROP for\nacceleration. For implementing RPROP, the weight update for all trajectories was first\naccumulated, and then the resulting weight update was fed into RPROP at the end of each\niteration. RPROP was used with the default parameters defined by Riedmiller and Braun\n(1993).\nResults for the task of learning one trajectory from a fixed start point are shown in Figure\n6 for the VGL\u03a9 algorithm. The results were averaged over 10 runs. VGL\u03a9 worked better on\nthis task than VGL. VGL sometimes produced unstable or suboptimal solutions. However,\nboth could manage the task well with c = 1. It was not possible to get VL to work on this\ntask at all. VL failed with a greedy policy, as expected, as there is no exploration. However\nVL also failed on this task when using the \u01eb-greedy policy. We suspect the reason for this\nwas that random exploration was producing more unwanted noise than useful information;\nrandom exploration makes the spacecraft fly the wrong way and learn the wrong values.\nWe believe this makes a strong case for using value-gradients and a deterministic system\ninstead of VL with random exploration.\nThe kink in the left diagram of Figure 6 was caused because c was low, so the gradient\n\u2202R\u03c0\n\u2202w\n~ was tiny for the first few iterations. It seemed that RPROP would cause the weight\nchange to build up momentum quickly and then temporarily overshoot its target before\nbringing it back under control. It was very difficult to learn this task with such a small c\nvalue without RPROP.\nFigure 7 shows the performance of VGL and VL in a task of learning 50 trajectories from\nfixed start points. The VGL learning graph is clearly more smooth, more efficient and more\n30\n\n\fReinforcement Learning by Value-Gradients\n\n0\n\nR\n\n120\n100\n80\nH 60\n40\n20\n0\n\n-20\n-40\n-60\n-80\n-100\n\nVGL\u03a9\nOptimal Policy\n1\n\n10\n\n100\nIterations\n\n1000\n\n10000\n\n\u2666\nLearned Trajectory\nStart Point\n-6\n\n-4\n\n-2\n\n0\nVelocity\n\n2\n\n\u2666\n4\n\n6\n\nFigure 6: Learning performance for a single trajectory on the Lunar-Lander problem. Parameters: c = 0.01, \u2206t = 0.1, \u03bb\u0304 = 0. The algorithm used was VGL\u03a9. The\ntrajectory produced is very close to optimal (c.f. Fig. 9).\n\noptimal than the VL graph. VGL\u03a9 and VGL could both manage this task, achieving close\nto optimal performance each time, but only VGL\u03a9 could cope with the smaller c values in\nthe range [0.01, 1).\n\n-20\n-30\n-40\nR -50\n-60\n-70\n-80\n\n120\n100 \u2666\n\u2666\n\u2666\n\u2666 \u2666 \u2666 \u2666\u2666\n80\n\u2666 \u2666\n\u2666\n\u2666\n\u2666\nH 60\n\u2666\n\u2666\n\u2666\n\u2666 \u2666\u2666\n\u2666\n40 \u2666 \u2666\n\u2666\u2666\u2666\n\u2666 \u2666\u2666 \u2666\n\u2666\n\u2666\n\u2666\n\u2666\u2666\n\u2666 \u2666\n20 \u2666 \u2666 \u2666 \u2666\u2666\n\u2666\n\u2666 \u2666\n\u2666\n\u2666\n\u2666\n\u2666\n0\n-10\n-5\n0\n5\n10\nVelocity\n\nVGL\nVL\nOptimal Policy\n\n1\n\n10\n\n100\nIterations\n\n1000\n\n10000\n\nFigure 7: Learning performance for learning 50 trajectories simultaneously on LunarLander problem. Parameters: c = 1, \u2206t = 1, \u03bb\u0304 = 0. The algorithms used\nwere VL and VGL. Graphs were averaged over 20 runs. The averaging will have\nhad a smoothing effect on both curves. The right graph shows a set of final\ntrajectories obtained with one of the VGL trials, which are close to optimal.\n\nIt was difficult to get VL working well on this problem at all, and the parameters chosen\nwere those that appeared to be most favourable to VL in preliminary experiments. Having\nsuch a high c value makes the task much easier, but VL still could not get close to optimal,\nor stable, trajectories. No stochastic elements were used in either the policy or model.\nThe large number of fixed trajectory start points provided the exploration required by VL.\nThese gave a reasonable sampling of the whole of state space. Preliminary tests with the\n31\n\n\fFairbank\n\n\u01eb-greedy policy did not produce any successful learning. The policy used was the greedy\npolicy described in Appendix E.\nWe could not get bootstrapping to get to work on this task for either the VL or VGL\nalgorithms. With bootstrapping, the trajectories tended to continually oscillate. It was not\nclear why this was, but it is consistent with the lack of convergence results for bootstrapping.\nIt is desirable to have c as small as possible to get fuel-efficient trajectories (see for\nexample Figure 9), but a small c makes the continuous-time DE more stiff. However our\nexperience was that the limiting factor in choosing a small c was not the stiffness of the\n\u03c0\n~\nDE, but the fact that as c \u2192 0, \u2202R\n\u2202w\n~ \u2192 0 which made learning with VGL\u03a9 difficult. Using\nRPROP largely remedied this since it copes well with small gradients and can respond\nquickly when the gradient suddenly changes. We did not need to use a stiff DE solver, and\nfound the Euler method adequate to integrate the equations of Section 2.2.\n\n5. Conclusions\nThis section summarises some of the issues and benefits raised by the VGL approach. Also,\nthe contributions of this paper are highlighted.\n\u2022 Several VGL algorithms have been stated. These are algorithms VGL(\u03bb), VGL\u03a9(\u03bb)\nand VGLRG(\u03bb) (see Table 1), with their continuous time counterparts, and actorcritic learning; all defined for any 0 \u2264 \u03bb \u2264 1. Results on the Toy Problem and the\nLunar-Lander are better than the VL results by several orders of magnitude, in all\ncases.\n\u2022 The value-gradient analysis goes a large way to resolving the issue of exploration in\nvalue function learning. Local exploration comes for free with VGL. Other than the\nproblem of local versus global optimality, the problems of exploration are resolved,\nand value function learning is put onto an equal footing with PGL. For example, as\ndiscussed in Section 1.5, exploration had previously caused difficulties in Q(\u03bb)-learning\nand Sarsa(\u03bb).\n\u2022 In Appendix A, definitions of extremal and optimal trajectories and an optimality\nproof are given for learning by value-gradients. The proof refers to Pontryagin's\nMaximal Principle (PMP), but in the case of \"bang-bang\" control, the conclusion of\nthe proof goes slightly further than is implied solely by PMP.\n\u2022 The value-gradient analysis provides an overall view that links several different areas\nwithin reinforcement learning. For example, the connection between PGL and value\nfunction learning is proven in Section 2.1. This provides an explanation of what\nhappens when the \"residual gradient terms\" are missed off from the weight update\n\u2202R\u03c0\nequations (i.e. \u2202E\n\u2202w\n~ \u2192 \u2202w\n~ ; see Section 2.1), and a tentative justification for the TD(\u03bb)\nweight update equation (see Section 2.1). Also, the obvious similarity of form between\nEq. 15 and Eq. 27 provides connections between PGL and actor-critic architectures,\nas discussed in Section 3.\n\u2022 The use of a function approximator has been intrinsic throughout. This followed from\nthe definition of V in Section 1.1. This has led to a robust convergence proof, for a\n32\n\n\fReinforcement Learning by Value-Gradients\n\ngeneral function approximator and value function, in Section 2.1. The use of a general function approximator is an advancement over simplified function approximators\n(e.g. linear approximators), or function approximators that require a hand-picked\npartitioning of state space.\n\u2022 Most previous studies have separated the value function update from the greedy policy\nupdate, but this has been a severe limitation because RL does not work this way; in\npractice, it is necessary to alternate one with the other, otherwise the RL system\nwould never improve. However no previous convergence proof or divergence example\napplies to this \"whole system\". In this paper there has been a tight coupling of\nthe value function to the greedy policy, and this has made it possible to successfully\nanalyse what effect a value function update has to the greedy policy. We have found\nconvergence proofs and divergence examples for the whole system. We do not think\nit is possible to do this analysis without value-gradients, since the expression for \u2202\u2202\u03c0w~\nin Eq. 17 depends on \u2202G\n\u2202w\n~ . Hence value-gradients are necessary to understand valuefunction learning, whether by VL or VGL.\n\u2022 By considering the \"whole system\", a divergence example is found for VL (including\nSarsa(\u03bb) and TD(\u03bb)) and VGL with \u03bb = 1, in Section 4.3. This may be a surprising\nresult, since it is generally thought that the case of \u03bb = 1 is fully understood and\nalways converges, but this is not so for the whole system on a control problem.\n\u2022 It is proposed in Section 1.4 that the value-gradients approach is an idealised form\nof VL. We also believe that the approach of VL is not only haphazardly indirect,\nbut also introduces some extra unwanted terms into the weight update equation, as\ndemonstrated in the analysis at the end of Section 4.1.\n\u2022 The continuous-time policy stated in Section 2.2 (taken from Doya, 2000) is an exact\nimplementation of the greedy policy \u03c0(~x, w)\n~ that is smooth with respect to ~x and w\n~\nprovided that the function approximator used is smooth. This resolves one of the\ngreatest difficulties of value-function learning, namely that of discontinuous changes\nto the actions chosen by the greedy policy.\n\u2022 A new explanation is given about how residual gradient algorithms can get trapped in\nspurious local minima, as described in Section 2.3. We think this is the main reason\nwhy residual gradients often fails to work in practice, even in the case of VGL and\ndeterministic systems. Understanding this will hopefully save other researchers losing\ntoo much time exploring this possibility.\nIt is the opinion of the author that there are several problematic issues about reinforcement learning with an approximated value function, which the algorithm VGL\u03a9(1) resolves.\nThese problematic issues are:\n\u2022 Learning progress is far from monotonic, when measured by either E (Eq. 12) or\nR\u03c0 , or any other metric currently known. This problem is resolved by the proposed\nalgorithm, when used in conjunction with a policy such as the one in Section 2.2.\n33\n\n\fFairbank\n\n\u2022 When learning a value function with a function approximator, the objective is to\nobtain Gt = G\u2032 t for all t along a trajectory. In general, due to the nature of function\napproximation, this will never be attained exactly. However, even if this is very close\nto being attained, the value of R\u03c0 may be far from optimal. In short, minimising E\nand maximising R\u03c0 are not same thing unless E = 0 can be attained exactly. As\ndemonstrated in the experiment of Section 4.4, and the proofs in Section 2.1, this\nproblem is resolved by the proposed algorithm.\n\u2022 The success of learning can depend on how state space is scaled. The definition of \u03a9t\n(Eq. 19) resolves this problem. Other algorithms can become unstable without this.\n\u2022 Making successful experiments reproducible in VL is very difficult. There are no convergence guarantees, either with or without bootstrapping, and success often depends\nupon well-chosen parameter choices made by the experimenter. For example, the\nLunar-Lander problem in Section 4.5 seems to defeat VL with the given choices of\nstate space scaling and function approximator. With the proposed algorithm, convergence to some fixed point is assured; and so one major element of luck is removed.\nAll of the proposed algorithms are defined for any 0 \u2264 \u03bb \u2264 1. The results in Section\n3.1 and Appendix A are valid proofs for any \u03bb, but the main convergence result of this\npaper, Eq. 18, applies only to \u03bb = 1. Unfortunately divergence examples exist for \u03bb < 1,\nas described in Section 4.3.\nAlso by proving equivalence to policy-learning in the case of \u03bb = 1, and finding a lack\nof robustness and divergence examples for \u03bb < 1, the usefulness of the value-function is\nsomewhat discredited; both for VGL, and its stochastic relative, VL.\n\nAcknowledgments\nI am very grateful to Peter Dayan, Andy Barto, Paul Werbos and R\u00e9mi Coulom for their\ndiscussions, suggestions and pointers for research on this topic.\n\nAppendix A. Optimal Trajectories\nIn this appendix we define locally optimal trajectories and prove that if G\u2032 t = Gt for all t\nalong a greedy trajectory then that trajectory is locally extremal, and in certain situations,\nlocally optimal.\nLocally Optimal Trajectories. We define a trajectory parametrised by values {~x0 , a0 , a1 , a2 , . . .}\nto be locally optimal if R(~x0 , a0 , a1 , a2 , . . .) is at a local maximum with respect to the parameters {a0 , a1 , a2 , . . .}, subject to the constraints (if present) that \u22121 \u2264 at \u2264 1.\nLocally Extremal Trajectories (LET). We define a trajectory parametrised by values\n{~x0 , a0 , a1 , a2 , . . .} to be locally extremal if, for all t,\n\uf8f1\n\u0001\n\u2202R\n\uf8f4\n\uf8f2 \u2202a \u0001t = 0 if at is not saturated\n\u2202R\n(29)\n\u2202a t > 0 if at is saturated and at = 1\n\uf8f4\n\uf8f3 \u2202R \u0001\n\u2202a t < 0 if at is saturated and at = \u22121.\n34\n\n\fReinforcement Learning by Value-Gradients\n\nIn the case that all\u0001 the actions are unbounded, this criterion for a LET simplifies to that of\njust requiring \u2202R\n\u2202a t = 0 for all t. Having the possibility of bounded actions introduces the\nextra complication of saturated actions. The second condition in Eq. 29 incorporates the\nidea that if a saturated action at is fully \"on\", then we would normally like it to be on even\nmore (if that were possible). In fact, in this definition R is locally optimal with respect to\nany saturated actions. Consequently, if all of the actions are saturated (for example in the\ncase of \"bang-bang\" control), then this definition of a LET provides a sufficient condition\nfor a locally optimal trajectory.\nConcave Model Functions. We say a model has concave model functions if all locally\nextremal trajectories are guaranteed to be locally optimal. In other words, if we define \u2207a R\n\u2202R\n, and \u2207a \u2207a R to be the matrix with\nto be a column vector with ith element equal to \u2202a\ni\n2\n\nR\n(i, j)th element equal to \u2202a\u2202i \u2202a\n, then the model functions are concave if \u2207a R = 0 implies\nj\n\u2207a \u2207a R is negative definite.\n2\nFor example, for the two-step Toy \u0012\nProblem with\n\u0013 k = 1, since R(x0 , a0 , a1 ) = \u2212a0 \u2212\n\u22124 \u22122\na1 2 \u2212(x0 +a0 +a1 )2 , we have \u2207a \u2207a R =\n, which is constant and negative definite;\n\u22122 \u22124\nand so the two-step Toy Problem with k = 1 has concave model functions. It can also be\nshown that the n-step Toy Problem with any k \u2265 0, and any n \u2265 1, also has concave model\nfunctions.\nConstrained Locally Optimal Trajectories. The previous two optimality criteria were\nindependent of any policy. This weaker definition of optimality is specific to a particular\npolicy, and is defined as follows: A constrained (with respect to w)\n~ locally optimal trajectory\nis a trajectory parametrised by an arbitrary smooth policy function \u03c0(~xt , w),\n~ where w\n~ is the\nweight vector of some function approximator, such that R\u03c0 (~x0 , w)\n~ is at a local maximum\nwith respect to w.\n~\nIf we assume the function R\u03c0 (~x, w)\n~ is continuous and smooth everywhere with respect\nto w,\n~ then this kind of optimality is naturally achieved at any stationary point found by\ngradient ascent on R\u03c0 with respect to w,\n~ i.e. by any PGL algorithm.\n\nLemma 5 If G\u2032 t \u2261 Gt (for all t, and some\u0001 \u03bb) along a trajectory found by an arbitrary\n\u03c0\nsmooth policy \u03c0(~x, ~z), then G\u2032 t \u2261 Gt \u2261 \u2202R\n\u2202~\nx t for all t.\nThis lemma is for a general policy, and is required by section 3.1. Here we use the\nextended definitions of \u0010V \u2032 and G\u0011\u2032 that apply to any policy, given in Section 3 and Eq. 26.\n\u03c0 x,~\nz)\nFirst we note that \u2202R \u2202~(~\n\u2261 G\u2032 t with \u03bb = 1, since when \u03bb = 1 any dependency of\nx\nt\n\nG\u2032 (~xt , w,\n~ ~z) on w\n~ disappears. Also, by Eq. 6 and since G\u2032 t = Gt we get,\n\u0012 \u0013 \u0012 \u0013 \u0013 \u0012\u0012 \u0013\n\u0012 \u0013 \u0012 \u0013\u0013\n\u0012\u0012 \u0013\n\u2202\u03c0\n\u2202r\n\u2202f\n\u2202\u03c0\n\u2202f\n\u2202r\n\u2032\n+\n+\n+\nG\u2032 t+1\nGt=\n\u2202~x t\n\u2202~x t \u2202a t\n\u2202~x t\n\u2202~x t \u2202a t\n\u03c0\u0001\nTherefore G\u2032 t is independent of \u03bb and therefore G\u2032 t \u2261 Gt \u2261 \u2202R\n\u2202~\nx t for all t.\n\nLemma\n6 If \u0001G\u2032 t \u2261 Gt (for all t, and some \u03bb) along a greedy trajectory then G\u2032 t \u2261 Gt \u2261\n\u0001\n\u2202R\u03c0\n\u2202R\n\u2202~\nx t \u2261\n\u2202~\nx t for all t.\nThis is proved by induction. Note that this lemma differs from the previous lemma in\nthat it is specifically for the greedy policy, and the conclusion is stronger. By Eq. 6 and\n35\n\n\fFairbank\n\nsince G\u2032 t = Gt we get,\nGt =\n\n\u0012\n\n\u2202\u03c0\n\u2202~x\n\n\u0013 \u0012\nt\n\n\u2202Q\n\u2202a\n\n\u0013\n\n+\nt\n\n\u0012\n\n\u2202r\n\u2202~x\n\n\u0013\n\n+\nt\n\n\u0012\n\n\u2202f\n\u2202~x\n\n\u0013\n\nGt+1\nt\n\n\u0001\n~\nThe left term of this sum must be zero since the greedy policy implies either \u2202\u03c0\n\u2202~\nx t = 0 (in\n\u0010\n\u0011\n\u0001\n\u2202Q\nthe case that at is saturated and \u2202\u03c0\n\u2202~\nx t exists, by Lemma 2), or\n\u2202a t = 0 (in the case that\n\u0001\nat is not saturated, by Lemma 1). If \u2202\u03c0\nexist then it must be that \u03bb = 0, since\n\u2202~\nx t does not\n\u0010 \u0011\n\u2202Q\n\u2032\n\u2032\nG t exists, and when \u03bb = 0 the definition is G t = \u2202~x . Therefore in all cases,\nt\n\nGt =\n\n\u0012\n\n\u2202r\n\u2202~x\n\n\u0013\n\n+\nt\n\n\u0012\n\n\u2202f\n\u2202~x\n\n\u0013\n\nGt+1\nt\n\nAlso, differentiating Eq. 1 with respect to ~x gives\n\u0012\n\u0013\n\u0012 \u0013\n\u0012 \u0013 \u0012\n\u0013\n\u2202R\n\u2202r\n\u2202f\n\u2202R\n=\n+\n\u2202~x t\n\u2202~x t\n\u2202~x t \u2202~x t+1\n\n(30)\n\n\u0001\ndefinition. Also their values at the final time\nSo \u2202R\n\u2202~\nx t and Gt have the same recursive\n\u0001\n\u2202R\nstep t = F are the same, since \u2202~x F = GF = ~0. Therefore, by induction and lemma 5,\n\u0001\n\u0001\n\u2202R\u03c0\n\u2261\nG\u2032 t \u2261 Gt \u2261 \u2202R\n\u2202~\nx t\n\u2202~\nx t for all t.\n\nTheorem 7 Any greedy trajectory satisfying G\u2032 t = Gt (for all t) must be locally extremal.\nProof: Since the greedy policy maximises Q(~xt , at , w)\n~ with respect to at at each timestep t, we know at each t,\n\uf8f1\u0010 \u0011\n\u2202Q\n\uf8f4\n= 0 if at is not saturated\n\uf8f4\n\uf8f4\n\uf8f2\u0010 \u2202a \u0011t\n\u2202Q\n> 0 if at is saturated and at = 1\n(31)\n\uf8f4\n\u0010 \u2202a \u0011t\n\uf8f4\n\uf8f4\n\uf8f3 \u2202Q < 0 if at is saturated and at = \u22121.\n\u2202a\nt\n\nThese follow\u0001from Lemma 1 and the definition of saturated actions. Additionally, by Lemma\n6, Gt \u2261 \u2202R\n\u2202~\nx t for all t. Therefore since,\n\u0012\n\nwe have\n\n\u2202R\n\u2202a t\n\n\u0001\n\n\u2261\n\n\u0010\n\n\u0011\n\n\u2202Q\n\u2202a t\n\n\u2202R\n\u2202a\n\n\u0013\n\n=\nt\n\n\u0012\n\n\u2202r\n\u2202a\n\n\u0013\n\n+\n\n\u2202f\n\u2202a\n\n\u0013 \u0012\n\n\u2202R\n\u2202~x\n\n\u0013\n\u0012 \u0013t\n\u2202f\n\u2202r\n+\nGt+1\n=\n\u2202a t\n\u2202a t\n\u0012\n\u0013\n\u2202Q\n=\n\u2202a t\n\u0012\n\nt\n\n\u0012\n\n\u0013\n\nt+1\n\nfor all t. Therefore the consequences of the greedy policy (Eq.\n\n31) become equivalent to the sufficient conditions for a LET (Eq. 29), which implies the\ntrajectory is a LET.\n36\n\n\fReinforcement Learning by Value-Gradients\n\nCorollary 8 If, in addition to the conditions of Theorem 7, the model functions are concave, or if all of the actions are saturated (bang-bang control), then the trajectory is locally\noptimal.\nThis follows from the definitions given above of concave model functions and a LET.\nRemark: In practice we often do not need to worry about the need for concave model\nfunctions, since any algorithm that works by gradient ascent on R\u03c0 will tend to head\ntowards local maxima, not saddle-points or minima. This applies to all VGL algorithms\nlisted in this paper, except for residual-gradients.\nRemark: We point out that the proof of Theorem 7 could almost be replaced by use of\nPontryagin's maximum\nprinciple (PMP) (Bronshtein and Semendyayev, 1985), since Eq.\n\u0001\n\u2202R\n30 implies \u2202~x t is the \"costate\" (or \"adjoint\") vector of PMP, and Lemma 6 implies that\nthe greedy policy is equivalent to the maximum condition of PMP. PMP on its own is not\nsufficient for the optimality proof without use of Lemma 6. Use of PMP would obviate\nthe need for the bespoke definition of a LET that we have used. We did not use PMP\nbecause it is only described to be a \"necessary\" condition for optimality, and the way we\nhave formulated the proof allows us to derive the corollary's extra conclusion for bang-bang\ncontrol.\n\nAppendix B. Detailed counterexample of the failure of value-learning\nwithout exploration, compared to the impossibility of failure for\nvalue-gradient learning.\nThis section gives a more detailed example than that of Fig. 3, to show why exploration is\nnecessary to VL but not to VGL.\nWe consider the one-step Top Problem with k = 1. For this problem, the optimal policy\n(Eq. 8) simplifies to\n\u03c0 \u2217 (x0 ) = \u2212x0 /2\n\n(32)\n\nNext we define a value function on which a greedy policy can be defined. Let the value\nfunction be linear, for simplicity, and be approximated by just two parameters (w1 , w2 ),\nand defined separately for the two time steps.\n\nV (xt , w1 , w2 ) =\n\n\u001a\n\nw1 + w2 x1 if t = 1\n0\nif t = 2\n\n(33)\n\nFor the final time step, t = 2, we have assumed the value function is perfectly known, so\nthat V2 \u2261 R2 \u2261 0. At time step t = 0, it is not necessary to define the value function since\nthe greedy policy only looks ahead. Differentiating this value function gives the following\nvalue-gradient function:\n\u001a\nw2 if t = 1\nG(xt , w1 , w2 ) =\n0\nif t = 2\n37\n\n\fFairbank\n\nThe greedy policy on this value function gives\na0 =\u03c0(x0 , w)\n~\n= arg max (r(x0 , a, w)\n~ + V (f (x0 , a), w))\n~\na\n\u0001\n= arg max \u2212a2 + w1 + w2 (x0 + a)\na\n\nby Eqs. 2, 3\nby Eqs. 33, 7\n\n=w2 /2\n\n(34)\n\nHaving defined a value-function and found the greedy policy that acts on it, we next\nanalyse the situations in the VL and VGL cases, each without exploration. The value\nfunction defined above is used in the following examples.\nNote that the conclusions of the following examples cannot be explained by choice of\nfunction approximator for V . For example Fig. 3 shows a counterexample for a different\nfunction approximator, and similar counterexamples for VL can easily be found for any\nfunction approximator of a higher degree. A linear function approximator was chosen here\nsince it is the simplest type of approximator that can be made to learn an optimal trajectory\nin this problem, as is illustrated in the VGL example below.\nValue-Learning applied to Toy Problem (without exploration): Here the aim is\nto show that VL, without exploration, can be applied to the one-step Toy Problem (with\nk = 1) and converge to a sub-optimal trajectory.\nThe target for the value function at t = 1 is given by:\nV \u2032 1 = r(x1 , a1 ) + \u03bbV \u2032 2 + (1 \u2212 \u03bb)V2\n= \u2212x1\n\nby Eq. 4\n\n2\n\nby Eq. 7, and since V\n\n\u2032\n\n2\n\n= V2 = 0\n\nThe value function at t = 1 is given by:\nV1 = w1 + w2 x1\nA simple counterexample can be chosen to show that if VL is complete (i.e. if Vt = V \u2032 t\nfor all t > 0), then the trajectory may not be optimal. If x0 = 5, w1 = \u221225, w2 = 0\nthen the greedy policy (Eq. 34) gives a0 = w2 /2 = 0 and thus x1 = x0 = 5. Therefore\nV1 = V \u2032 1 = \u221225, and V2 = V \u2032 2 = 0, and so learning is complete. However the trajectory is\nnot optimal, since the optimal policy (Eq. 32) requires a0 = \u22125/2.\nValue-Gradient Learning applied to Toy Problem: The objective of VGL is to make\nthe value-gradients match their target gradients. For the one-step Toy Problem (with\nk = 1), we get:\n\u0012\u0012 \u0013\n\u0012 \u0013 \u0012 \u0013 \u0013\n\u2202r\n\u2202\u03c0\n\u2202r\nG\u2032 1 =\n+\n\u2202x 1\n\u2202x 1 \u2202a 1\n\u0012\u0012 \u0013\n\u0012 \u0013 \u0012 \u0013 \u0013\n\u0001\n\u2202f\n\u2202\u03c0\n\u2202f\n+\n\u03bbG\u2032 2 + (1 \u2212 \u03bb)G2\n+\nby Eq. 6\n\u2202x 1\n\u2202x 1 \u2202a 1\n\u0001\nby Eq. 7\n= (\u22122x1 + 0) + (1 + 0) \u03bbG\u2032 2 + (1 \u2212 \u03bb)G2\nsince G\u2032 2 = G2 = 0\n\n= \u2212 2x1\n\nThe value-gradient at t = 1 is given by G1 = w2 .\n38\n\n\fReinforcement Learning by Value-Gradients\n\nFor these to be equal, i.e. for Gt = G\u2032 t for all t > 0, we must have w2 = \u22122x1 . The\ngreedy policy (Eq. 34) then gives a0 = w2 /2 = \u2212x1 = \u2212(x0 + a0 ) \u21d2 a0 = \u2212x0 /2 which is\nthe same as the optimal policy (Eq. 32). Therefore if the value-gradients are learned, then\nthe trajectory will be optimal.\n\nAppendix C. Equivalence of V \u2032 notation in TD(\u03bb)\nThe formulation of TD(\u03bb) as presented in this paper (Eq. 10) uses the V \u2032 notation. This can\nbe proven to be equivalent to the formulation\nused by Sutton (1988) as follows. Expanding\nP\nthe recursion in Eq. 4 gives V \u2032 t = k\u2265t \u03bbk\u2212t (rk + (1 \u2212 \u03bb)Vk+1 ), so Eq. 10 becomes:\n\u2206w\n~ = \u03b1\n\nX \u0012 \u2202V \u0013\nt\u22651\n\n\u2202w\n~\n\nt\n\n\uf8eb\n\uf8f6\nX\n\uf8ed\n\u03bbk\u2212t (rk + (1 \u2212 \u03bb)Vk+1 ) \u2212 Vt \uf8f8\nk\u2265t\n\n\uf8eb\n\uf8f6\nX\nX \u0012 \u2202V \u0013 X\n\uf8ed\n\u03bbk\u2212t (rk + Vk+1 ) \u2212\n\u03bbk\u2212t \u03bbVk+1 \u2212 Vt \uf8f8\n= \u03b1\n\u2202w\n~ t\nt\u22651\nk\u2265t\nk\u2265t\n\uf8eb\n\uf8f6\nX \u0012 \u2202V \u0013 X\nX\n\uf8ed\n= \u03b1\n\u03bbk\u2212t (rk + Vk+1 ) \u2212\n\u03bbk\u2212t Vk \uf8f8\n\u2202w\n~ t\nt\u22651\n\n= \u03b1\n\nk\u2265t\n\nk\nXX\n\nk\u2265t\n\n(rk + Vk+1 \u2212 Vk ) \u03bbk\u2212t\n\nk\u22651 t=1\n\n= \u03b1\n\nX\n\n(rt + Vt+1 \u2212 Vt )\n\nt\u22651\n\nt\nX\n\nt\u2212k\n\n\u03bb\n\nk=1\n\n\u0012\n\n\u0012\n\n\u2202V\n\u2202w\n~\n\n\u2202V\n\u2202w\n~\n\n\u0013\n\n\u0013\n\nt\n\nk\n\nThis last line is a batch-update version of the weight update equation given by Sutton\n(1988). This validates the use of the notation V \u2032 .\n\nAppendix D. Equivalence of Sarsa(\u03bb) to TD(\u03bb) for control problems with\na known model\nSarsa(\u03bb) is an algorithm for control problems that learns to approximate the Q(~x, a, w)\n~\nfunction (Rummery and Niranjan, 1994). It is designed for policies that are dependent on\nthe Q(~x, a, w)\n~ function (e.g. the greedy policy or \u01eb-greedy policy).\nThe Sarsa(\u03bb) algorithm is defined for trajectories where all actions after the first are\nfound by the given policy; the first action a0 can be arbitrary. The function-approximator\nupdate is then:\nX \u0012 \u2202Q \u0013\n(Q\u2032 t \u2212 Qt )\n(35)\n\u2206w\n~ =\u03b1\n\u2202w\n~ t\nt\u22650\n\nwhere Q\u2032 t = rt + V \u2032 t+1 .\nSarsa(\u03bb) is designed to be able to work in problem domains where the model functions\nare not known, however we can also apply it to the Q function as defined in Eq. 2 that relies\nupon our known model functions. This means we can rewrite Eq. 35 in terms of V (~x, w)\n~\n39\n\n\fFairbank\n\nto become exactly the same as Eq. 10. For this reason, we are justified to work with the\nTD(\u03bb) weight update on control problems using the \u01eb-greedy policy in the experiments of\nthis paper.\n\nAppendix E. One-dimensional Lunar-Lander Problem\nIn this continuous-time problem, a spacecraft is constrained to move in a vertical line and\nits objective is to make a fuel-efficient gentle landing. The spacecraft is released from\nvertically above a landing pad in a uniform gravitational field, and has a single thruster\nthat can produce upward accelerations.\nThe state vector ~x has three components: height (h), velocity (v), and fuel remaining\n(u), so that ~xt \u2261 (ht , vt , ut )T . Velocity and height have upwards defined to be positive. The\nspacecraft can perform upward accelerations at with at \u2208 [0, 1].\nThe continuous-time model functions for this problem are:\nf \u0304((h, v, u)T , a) = (v, (a \u2212 kg ), \u2212a)T\nr\u0304((h, v, u)T , a) = \u2212(kf )a + r\u0304 C (a)\nkg \u2208 (0, 1) is a constant giving the acceleration due to gravity; the spacecraft can produce\ngreater acceleration than that due to gravity. kf is a constant giving fuel penalty. We used\nkg = 0.2 and kf = 2.\nTerminal states are where the spacecraft hits the ground (h = 0) or runs out of fuel\n(u = 0). In addition to the continuous-time reward r\u0304 defined above, a final impulse of\nreward equal to \u2212v 2 \u2212 2(kg )h is given as soon as the lander reaches a terminal state. The\nterms in this final reward represent kinetic and potential energy respectively, which means\nwhen the spacecraft runs out of fuel, it's as if it crashes to the ground by freefall.\nRa\n(as described in\nr\u0304 C (a) = \u2212 0.5 g\u22121 (x)dx is the action-cost term of the reward function\n\u0002\n\u0003a\nSection 2.2), where g(x) = 21 (tanh(x/c)+1) and therefore r\u0304 C (a) = c x arctanh(1 \u2212 2x) \u2212 12 ln(1 \u2212 x) 0.5 .\n\u0011\n\u0010\n \u0304\nThis means the continuous-time greedy policy is exactly at = g \u2212kf + \u2202\u2202af Gt and this ensures at \u2208 (0, 1).\nThe derivatives of these model functions are:\n\uf8eb\n\uf8f6\n\uf8eb \uf8f6\n0 0 0\n0\n \u0304\n\u2202f\n\u2202r\u0304\n\u2202 f \u0304\n= \uf8ed 1 0 0 \uf8f8,\n= \uf8ed 0 \uf8f8,\n=\n\u2202~x\n\u2202~x\n\u2202a\n0 0 0\n0\n\n0 1 \u22121\n\n\u0001 \u2202r\u0304\n,\n= \u2212kf \u2212 c arctanh(2a \u2212 1)\n\u2202a\n\nE.1 Discontinuity Corrections for Continuous-Time Formulations (Clipping)\nWith a continuous-time model and episodic task, care needs to be taken in calculating\ngradients at any terminal states or points where the model functions are not smooth. Figure\n8 illustrates this complication when the spacecraft reaches a terminal state.\nThis problem means G\u2032 changes discontinuously at the boundary of terminal states.\nSince the learning algorithms only use G\u2032 t for t < F , the last gradient we need is the\n40\n\n\fReinforcement Learning by Value-Gradients\n\nA\nB\n\nC\n\nD\n\nh=0\n\nFigure 8: The lines AB and CD are sections of two trajectories that approach a transition\nto a region of terminal states (the line h = 0, in this case). If the end A of AB is\nmoved down then the end B will move down. However if C moves down then D\nwill move left, due to the presence of the barrier. This difference is what we call\nthe problem of Discontinuity Corrections.\n\nlimiting one as t \u2192 F . This can be calculated by considering the following one-sided limit:\nlim\n\n\u2206t\u21920+\n\n\u0012\n\n\u2202R\u03c0\n\u2202~x\n\n\u0013\n\n=\nF \u2212\u2206t\n\nlim\n\nh\u21920+\n\n\u0012\n\n\u2202R\u03c0\n\u2202~x\n\n\u0013\n\nsince for small h, \u2206t \u2248 \u2212h/v\nF +h/v\n\n\u0012\n\u00132 !\n(a\n\u2212\nk\n)h\nh\ng\n= lim\n((kf )a \u2212 r\u0304 C (a)) \u2212 v \u2212\nxF +h/v\nv\nv\nh\u21920+ \u2202~\nF +h/v\n\uf8eb\n\uf8f6\n(kf )aF \u2212r\u0304 C (aF )\n+ 2(aF \u2212 kg )\nvF\n\uf8ec\n\uf8f7\n= \uf8ed \u22122vF\n\uf8f8\n0\n\u2202\n\nwhere aF = limt\u2192F \u2212 (at ).\n\u03c0\u0001\nSimilarly it can be shown that, limt\u2192F \u2212 \u2202R\n\u2202a t = 0 and therefore the boundary condition to use for the target-value gradient is given by\n\uf8eb\n\n(kf )aF \u2212r\u0304 C (aF )\nvF\n\n\uf8ec\nlim G\u2032 t = \uf8ed \u22122vF\n\u2212\nt\u2192F\n0\n\n+ 2(aF \u2212 kg )\n\n\uf8f6\n\uf8f7\n\uf8f8\n\n(36)\n\nThis limiting target value-gradient is the one to use instead of the boundary condition\ngiven in Eq. 24 or a value-gradient based solely on the final reward impulse. If this issue is\nignored then the first component in the above vector would be zero and learning would not\nfind optimal trajectories. A similar correction needs making in the case of the spacecraft\nrunning out of fuel.\nAlso, in the calculation of the trajectory (Eq. 20) by an appropriate numerical method,\nwe think it is best to use clipping in the final time step, so that the spacecraft cannot, for\nexample, finish with h < 0. The use of clipping ensures that the total reward is a smooth\nfunction of the weights and this should aid methods that work by local exploration.\n41\n\n\fFairbank\n\nE.2 Lunar-Lander Optimal Trajectories\nIt is useful to know the optimal trajectories, purely for comparison of the learned solutions.\nHere we only consider optimal trajectories where there is sufficient fuel to land gently. An\noptimal policy is found using Pontryagin's maximum principle: The adjoint vector ~p(t)\nsatisfies the differential equation (DE)\n\u2202~\npt\n=\u2212\n\u2202t\n\n\u0012\n\n\u2202r\u0304\n\u2202~x\n\n\u0013\n\n\u2212\nt\n\n\u0012  \u0304\u0013\n\u2202f\n(~\npt )\n\u2202~x t\n\nand the trajectory state evolution equation\n(i.e. the\n\u0010\n\u0011 model functions), and where the action\n\u2202 f \u0304\nat each instant is found by at = g \u2212kf + \u2202a pt . The trajectory has to be evaluated\nbackwards from the end point at a given vF and h = 0. The boundary condition for this\nend point is pF = limt\u2192F \u2212 G\u2032 t (see Eq. 36) with aF = g(\u2212kf \u2212 2vF ).\nSolving the adjoint vector DE, and substituting into the expression for at gives\n\np~t\n\u21d2 at\n\n\uf8f6\np~0F\n(kf )aF \u2212 r\u0304 C (aF )\n+ 2(aF \u2212 kg )\n= \uf8ed \u22122vF + (F \u2212 t)~\np0F \uf8f8 with p~0F =\nvF\n0\n\u0001\n= g \u2212kf \u2212 2vF + (F \u2212 t)~\np0F\n\uf8eb\n\n(37)\n\nNumerical integration of the model functions with Eq. 37 gives the optimal trajectory\nbackwards from the given end point. To find which vF produces the trajectory that passes\nthrough a given point (h0 , v0 ) is another problem that requires solving numerically. Some\noptimal trajectories found using this method are shown in Figure 9.\n\nc=1\n120\n100\n80\n60\n40\n20\n0\n\nc=0.01\n120\n100\n80\n60\n40\n20\n0\n\n\u2666\n\u2666\n\u2666\n\n\u2666\n-6\n\n-4\n\n-2\n\n0\n\n2\n\n4\n\n6\n\n\u2666\n\u2666\n\u2666\n\n\u2666\n-6\n\n-4\n\n-2\n\n0\n\n2\n\n4\n\n6\n\nFigure 9: Lunar-Lander optimal trajectories. Shows height (y-axis) vs. velocity (x-axis).\nFuel dimension of state-space is omitted. Trajectories start at diamond and finish\nat h = 0. As c \u2192 0, trajectories become more fuel-efficient, and the transition\nbetween the freefall phase and braking phase becomes sharper. It is most fuelefficient to freefall for as long as possible (shown by the upper curved sections)\nand then to brake as quickly as possible (shown by the lower curved sections).\n\n42\n\n\fReinforcement Learning by Value-Gradients\n\nAppendix F. Derivation of actor training equation\nThe actor training equation (Eq. 27) is non-standard. However it can be seen to be\nconsistent with the more standard equations, while automatically including exploration, as\nfollows. Barto et al. (1983) use the TD(0) error signal \u03b4t = (rt + Vt+1 \u2212 Vt ) to train the\nactor, and also specify that some stochastic behaviour is required to force exploration.\nWhen the domain of actions to choose from is continuous, the simplest technique to\nforce exploration is to add a small amount of random noise nt at time step t to the action\nchosen (as done by Doya, 2000) giving modified actions a\u2032 t = at + nt . The stochastic realvalued (SRV) unit algorithm (Gullapalli, 1990) is used to train the actor while efficiently\ncompensating for the added noise:\n\u0012 \u0013\n\u2202\u03c0\n(rt + Vt+1 \u2212 Vt )\n\u2206~z = \u03b1nt\n\u2202~z t\nMaking a first order Taylor series approximation to the above equation, by expanding the\nterms Vt+1 and rt about the values they would have had if there was no noise, gives\n\u0012 \u0013\n\u0012 \u0013\n\u0012\u0012 \u0013\n\u0013\n\u2202f\n\u2202\u03c0\n\u2202r\n\u2206~z = \u03b1nt\n(rt + Vt+1 + nt\n+\nGt+1 \u2212 Vt )\n\u2202~z t\n\u2202a t\n\u2202a t\nIntegrating with respect to nt to find the mean weight update, and assuming nt \u2208 [\u2212\u01eb, \u01eb] is\na uniformly distributed random variable over a small range centred on zero, gives\n\u0012 \u0013 \u0012\u0012 \u0013\n\u0012 \u0013\n\u0013\nZ \u01eb\n1\n\u01eb2 \u2202\u03c0\n\u2202r\n\u2202f\nh\u2206~zi =\n\u2206~zdnt = \u03b1\n+\nGt+1\n3 \u2202~z t\n\u2202a t\n\u2202a t\n\u2212\u01eb 2\u01eb\nwhich is equivalent to Eq. 27 when summed over t. This justifies the use of Eq. 27 and\nexplains how it automatically incorporates exploration.\n\nReferences\nL. C. Baird. Reinforcement learning in continuous time: Advantage updating. In Proceedings\nof the International Conference on Neural Networks, Orlando, FL, June, 1994.\nLeemon C. Baird. Residual algorithms: Reinforcement learning with function approximation. In International Conference on Machine Learning, pages 30\u201337, 1995.\nA. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can\nsolve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13:834\u2013846, 1983.\nJ. Baxter and P. L. Bartlett. Direct gradient-based reinforcement learning (invited). In\nProceedings of the International Symposium on Circuits and Systems, pages III\u2013271\u2013274,\n2000.\nChristopher M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press,\n1995.\n43\n\n\fFairbank\n\nI. N. Bronshtein and K. A. Semendyayev. Handbook of Mathematics, chapter 3.2.2, pages\n372\u2013382. Van Nostrand Reinhold Company, 3rd edition, 1985.\nR\u00e9mi Coulom. Reinforcement Learning Using Neural Networks, with Applications to Motor\nControl. PhD thesis, Institut National Polytechnique de Grenoble, 2002.\nPeter Dayan and Satinder P. Singh. Improving policies without measuring merits. In\nDavid S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in\nNeural Information Processing Systems, volume 8, pages 1059\u20131065. The MIT Press,\n1996.\nKenji Doya. Reinforcement learning in continuous time and space. Neural Computation, 12\n(1):219\u2013245, 2000.\nS. E. Fahlman. Faster-learning variations on back-propagation: An empirical study. In\nProceedings of the 1988 Connectionist Summer School, pages 38\u201351, San Mateo, CA,\n1988. Morgan Kaufmann.\nV. Gullapalli. A stochastic reinforcement learning algorithm for learning real-valued functions. Neural Networks, 3:671\u2013692, 1990.\nD. H. Jacobson and D. Q. Mayne. Differential Dynamic Programming. Elsevier, New York,\nNY, 1970.\nV. R. Konda and J. N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control\nand Optimization, 42(4):1143\u20131166, 2003.\nBarak A. Pearlmutter. Fast exact multiplication by the Hessian. Neural Computation, 6\n(1):147\u2013160, 1994.\nWilliam H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical recipes in C (2nd ed.): the art of scientific computing. Cambridge University Press,\nNew York, NY, USA, 1992. ISBN 0-521-43108-5.\nMartin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation\nlearning: The RPROP algorithm. In Proc. of the IEEE Intl. Conf. on Neural Networks,\npages 586\u2013591, San Francisco, CA, 1993.\nG. Rummery and M. Niranjan. On-line q-learning using connectionist systems. Tech.\nRep. Technical Report CUED/F-INFENG/TR 166, Cambridge University Engineering\nDepartment, 1994.\nRichard S. Sutton. Learning to predict by the methods of temporal differences. Machine\nLearning, 3:9\u201344, 1988.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The\nMIT Press, Cambridge, Massachussetts, USA, 1998.\nJohn N. Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with\nfunction approximation. Technical Report LIDS-P-2322, 1996a.\n44\n\n\fReinforcement Learning by Value-Gradients\n\nJohn N. Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic\nprogramming. Machine Learning, 22(1-3):59\u201394, 1996b.\nC. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University,\n1989.\nC. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279\u2013292, 1992.\nPaul J. Werbos. Backpropagation through time: What it does and how to do it. In\nProceedings of the IEEE, volume 78, No. 10, pages 1550\u20131560, 1990.\nPaul J. Werbos. Stable adaptive control using new critic designs. eprint arXiv:adaporg/9810001, 1998. URL http://xxx.lanl.gov/html/adap-org/9810001.\nWhite and D. Sofge, editors. Handbook of Intelligent Control. Van Nostrand, 1992.\nR. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229\u2013356, 1992.\n\n45\n\n\f"}