{"id": "http://arxiv.org/abs/cs/0612136v1", "guidislink": true, "updated": "2006-12-27T07:19:32Z", "updated_parsed": [2006, 12, 27, 7, 19, 32, 2, 361, 0], "published": "2006-12-27T07:19:32Z", "published_parsed": [2006, 12, 27, 7, 19, 32, 2, 361, 0], "title": "Experiments on predictability of word in context and information rate in\n  natural language", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0612002%2Ccs%2F0612113%2Ccs%2F0612078%2Ccs%2F0612024%2Ccs%2F0612035%2Ccs%2F0612067%2Ccs%2F0612080%2Ccs%2F0612004%2Ccs%2F0612051%2Ccs%2F0612033%2Ccs%2F0612036%2Ccs%2F0612110%2Ccs%2F0612048%2Ccs%2F0612052%2Ccs%2F0612040%2Ccs%2F0612077%2Ccs%2F0612003%2Ccs%2F0612139%2Ccs%2F0612097%2Ccs%2F0612045%2Ccs%2F0612074%2Ccs%2F0612087%2Ccs%2F0612125%2Ccs%2F0612063%2Ccs%2F0612114%2Ccs%2F0612054%2Ccs%2F0612020%2Ccs%2F0612090%2Ccs%2F0612037%2Ccs%2F0612031%2Ccs%2F0612050%2Ccs%2F0612010%2Ccs%2F0612137%2Ccs%2F0612017%2Ccs%2F0612041%2Ccs%2F0612083%2Ccs%2F0612099%2Ccs%2F0612030%2Ccs%2F0612042%2Ccs%2F0612091%2Ccs%2F0612100%2Ccs%2F0612005%2Ccs%2F0612071%2Ccs%2F0612116%2Ccs%2F0612108%2Ccs%2F0612098%2Ccs%2F0612046%2Ccs%2F0612044%2Ccs%2F0612075%2Ccs%2F0612016%2Ccs%2F0612084%2Ccs%2F0612142%2Ccs%2F0612043%2Ccs%2F0612140%2Ccs%2F0612070%2Ccs%2F0612106%2Ccs%2F0612058%2Ccs%2F0612061%2Ccs%2F0612059%2Ccs%2F0612029%2Ccs%2F0612128%2Ccs%2F0612034%2Ccs%2F0612115%2Ccs%2F0612039%2Ccs%2F0612119%2Ccs%2F0612011%2Ccs%2F0612093%2Ccs%2F0612027%2Ccs%2F0612131%2Ccs%2F0612082%2Ccs%2F0612068%2Ccs%2F0612126%2Ccs%2F0612012%2Ccs%2F0612085%2Ccs%2F0612094%2Ccs%2F0612112%2Ccs%2F0612009%2Ccs%2F0612105%2Ccs%2F0612021%2Ccs%2F0612056%2Ccs%2F0612006%2Ccs%2F0612018%2Ccs%2F0612133%2Ccs%2F0612141%2Ccs%2F0612136%2Ccs%2F0612076%2Ccs%2F0612088%2Ccs%2F0612073%2Ccs%2F0612102%2Ccs%2F0612092%2Ccs%2F0612064%2Ccs%2F0612023%2Ccs%2F0612008%2Ccs%2F0612123%2Ccs%2F0612109%2Ccs%2F0612022%2Ccs%2F0612015%2Ccs%2F0612129%2Ccs%2F0612026%2Ccs%2F0612096%2Ccs%2F0612120&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Experiments on predictability of word in context and information rate in\n  natural language"}, "summary": "Based on data from a large-scale experiment with human subjects, we conclude\nthat the logarithm of probability to guess a word in context (unpredictability)\ndepends linearly on the word length. This result holds both for poetry and\nprose, even though with prose, the subjects don't know the length of the\nomitted word. We hypothesize that this effect reflects a tendency of natural\nlanguage to have an even information rate.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0612002%2Ccs%2F0612113%2Ccs%2F0612078%2Ccs%2F0612024%2Ccs%2F0612035%2Ccs%2F0612067%2Ccs%2F0612080%2Ccs%2F0612004%2Ccs%2F0612051%2Ccs%2F0612033%2Ccs%2F0612036%2Ccs%2F0612110%2Ccs%2F0612048%2Ccs%2F0612052%2Ccs%2F0612040%2Ccs%2F0612077%2Ccs%2F0612003%2Ccs%2F0612139%2Ccs%2F0612097%2Ccs%2F0612045%2Ccs%2F0612074%2Ccs%2F0612087%2Ccs%2F0612125%2Ccs%2F0612063%2Ccs%2F0612114%2Ccs%2F0612054%2Ccs%2F0612020%2Ccs%2F0612090%2Ccs%2F0612037%2Ccs%2F0612031%2Ccs%2F0612050%2Ccs%2F0612010%2Ccs%2F0612137%2Ccs%2F0612017%2Ccs%2F0612041%2Ccs%2F0612083%2Ccs%2F0612099%2Ccs%2F0612030%2Ccs%2F0612042%2Ccs%2F0612091%2Ccs%2F0612100%2Ccs%2F0612005%2Ccs%2F0612071%2Ccs%2F0612116%2Ccs%2F0612108%2Ccs%2F0612098%2Ccs%2F0612046%2Ccs%2F0612044%2Ccs%2F0612075%2Ccs%2F0612016%2Ccs%2F0612084%2Ccs%2F0612142%2Ccs%2F0612043%2Ccs%2F0612140%2Ccs%2F0612070%2Ccs%2F0612106%2Ccs%2F0612058%2Ccs%2F0612061%2Ccs%2F0612059%2Ccs%2F0612029%2Ccs%2F0612128%2Ccs%2F0612034%2Ccs%2F0612115%2Ccs%2F0612039%2Ccs%2F0612119%2Ccs%2F0612011%2Ccs%2F0612093%2Ccs%2F0612027%2Ccs%2F0612131%2Ccs%2F0612082%2Ccs%2F0612068%2Ccs%2F0612126%2Ccs%2F0612012%2Ccs%2F0612085%2Ccs%2F0612094%2Ccs%2F0612112%2Ccs%2F0612009%2Ccs%2F0612105%2Ccs%2F0612021%2Ccs%2F0612056%2Ccs%2F0612006%2Ccs%2F0612018%2Ccs%2F0612133%2Ccs%2F0612141%2Ccs%2F0612136%2Ccs%2F0612076%2Ccs%2F0612088%2Ccs%2F0612073%2Ccs%2F0612102%2Ccs%2F0612092%2Ccs%2F0612064%2Ccs%2F0612023%2Ccs%2F0612008%2Ccs%2F0612123%2Ccs%2F0612109%2Ccs%2F0612022%2Ccs%2F0612015%2Ccs%2F0612129%2Ccs%2F0612026%2Ccs%2F0612096%2Ccs%2F0612120&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Based on data from a large-scale experiment with human subjects, we conclude\nthat the logarithm of probability to guess a word in context (unpredictability)\ndepends linearly on the word length. This result holds both for poetry and\nprose, even though with prose, the subjects don't know the length of the\nomitted word. We hypothesize that this effect reflects a tendency of natural\nlanguage to have an even information rate."}, "authors": ["Dmitrii Manin"], "author_detail": {"name": "Dmitrii Manin"}, "author": "Dmitrii Manin", "arxiv_comment": "14 pages, 3 figures", "links": [{"href": "http://arxiv.org/abs/cs/0612136v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0612136v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0612136v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0612136v1", "journal_reference": "Manin, D.Yu. 2006. Experiments on predictability of word in\n  context and information rate in natural language. J. Information Processes\n  (electronic publication, http://www.jip.ru), 6 (3), 229-236", "doi": null, "fulltext": "arXiv:cs/0612136v1 [cs.IT] 27 Dec 2006\n\nExperiments on predi tability of word in\nontext\nand information rate in natural language\u2217\nD. Yu. Manin\nJuly 26, 2018\n\nAbstra t\n\nBased on data from a large-s ale experiment with human subje ts,\nwe on lude that the logarithm of probability to guess a word in ontext (unpredi tability) depends linearly on the word length. This result holds both for poetry and prose, even though with prose, the\nsubje ts don't know the length of the omitted word. We hypothesize\nthat this e\u001be t re\u001de ts a tenden y of natural language to have an even\ninformation rate.\n\n1\n\nIntrodu tion\n\nIn this paper we report a parti ular result of an experimental study on\npredi tability of words in ontext. The experiment's primary motivation is the study of some aspe ts of poetry per eption, but the result\nreported here is, in the author's view, of a general linguisti interest.\nThe \u001crst study of natural text predi tability was performed by the\nfounder of information theory, C. E. Shannon [1\u2104. (We'll note that\neven in his groundbreaking work [2\u2104, Shannon brie\u001dy tou hed on the\nrelationship between literary qualities and redundan y by ontrasting\nhighly redundant Basi English with Joy e's \u0010Finnegan's Wake\u0011 whi h\n\u2217\n\nThis is a somewhat expanded version of the paper Manin, D.Yu. 2006.\n\npredi tability of word in\n\nontext and information rate in natural language.\n\nPro esses (ele troni publi ation, http://www.jip.ru), 6 (3), 229-236\n1\n\nExperiments on\n\nJ. Information\n\n\f\u0010enlarges the vo abulary and is alleged to a hieve a ompression of\nsemanti ontent\u0011.) Shannon presented his subje t with random passages from Je\u001berson's biography and had her guess the next letter until\nthe orre t guess was re orded. The number of guesses for ea h letter\nwas then used to al ulate upper and lower bounds for the entropy of\nEnglish, whi h turned out to be between 0.6 and 1.3 bits per hara ter (bp ), mu h lower than that of a random mix of the same letters.\nShannon's results also indi ated that onditional entropy de reases as\nmore and more text history be omes known to the subje t, up to at\nleast 100 letters.\nSeveral authors repeated Shannon's experiments with some modi\u001c ations. Burton and Li klider [3\u2104 used 10 di\u001berent texts of similar\nstyle, and fragment lengths of 1, 2, 4, ..., 128, 1000 hara ters. Their\non lusion was that, ontrary to Shannon, in reasing history doesn't\na\u001be t measured entropy when history length ex eeds 32 hara ters.\nF\u0001\nonagy [4\u2104 ompared predi tability of the next letter for three types\nof text: poetry, newspaper, and \u0010a onversation of two young girls\u0011.\nApparently, his te hnique involved only one guess per letter, so entropy estimates ould not be al ulated (see below), and results are\npresented in terms of the rate of orre t answers, poetry being mu h\nless predi table than both other types.\nKolmogorov reported the results of 0.9\u00151.4 bp for Russian texts in\nhis work [5\u2104 that laid the ground of algorithmi omplexity theory. The\npaper ontains no details on the very ingenious experimental te hniue,\nbut it is des ribed in the well-known monograph by Yaglom & Yaglom\n[6\u2104.\nCover and King [7\u2104 modi\u001ced Shannon's te hnique by having their\nsubje ts pla e bets on the next letter. They showed that the optimal betting poli y would be to distribute available apital among the\npossible out omes a ording to their probability and so if the subje ts\nplay in an optimal way (whi h is not self-evident though), the letter\nprobabilities ould be inferred from their bets. Their estimate of the\nentropy of English was al ulated at 1.3 bp . This work also ontains\nan extensive bibliography.\nMoradi et al [8\u2104 \u001crst used two di\u001berent texts (a textbook on digital\nsignal pro essing and a novel by Judith Krantz) to on\u001crm Burton\nand Li klider's results on the riti al history length (32 hara ters),\nthen added two more texts (\u0010101 Dalmatians\u0011 and a federal aviation\nmanual) to study the dependen e of entropy on text type and subje t\n(with somewhat in on lusive results).\n2\n\n\fA number of works were devoted to estimating entropy of natural\nlanguage by means of statisti al analysis, without using human subje ts. One of the \u001crst attempts is reported in [9\u2104, where 39 English\ntranslations of 9 lassi al Greek texts were used to study entropy dependen y on subje t matter, style, and period. A very rude entropy\nestimate by letter digram frequen y was used. For some of the more\nre ent developments, see [10\u2104, [11\u2104 and referen es therein. By the very\nnature of these methods they an't utilize meaning (and even syntax)\nof the text, but by the brute for e of ontemporary omputers they begin a hieving results that ome reasonably lose to those demonstrated\nby human language speakers.\nOur experimental setup di\u001bers from the previous work in two important aspe ts. First, we have subje ts guess whole words, and not\nindividual hara ters. Se ond, the words to be guessed ome (generally speaking) from the middle of a ontext, rather than at the end of\na fragment. In addition to \u001clling blanks, we present the subje ts with\ntwo other task types where authenti ity of a presented word is to be\nassessed. The reason for this is that while most of the previous studies\nwere eventually aimed at e\u001e ient text ompression, we are interested in\nliterary ( hie\u001dy, poeti ) texts as works of literature, and not as mere\nhara ter strings subje t to appli ation of ompression algorithms1 .\nOur goal in designing the experiment was to provide resear hers in the\n\u001celd of poeti s with hard data to ground some hypotheses that otherwise are unavoidably spe ulative. Guessing the next word in sequen e\nis not the best way to treat literary text, be ause even an ordinary\nsenten e like this one is not essentially a linear sequen e of words or\nhara ters, but a omplex stru ture with word asso iations running\nall over the pla e, both forward and ba kward. A poem, even more\nso, is a stru ture with strongly oordinated parts, whi h is not read\nsequentially, mu h less written sequentially. Also, pra ti e shows that\neven when guessing letter by letter, people almost always base their\nnext hara ter hoi e on a tentative word guess. This is why guessing\nwhole words in ontext was more appropriate for our purpose.\nHowever, the results we present here, as already mentioned, are not\nrelevant to poeti s proper, so we will not dwell on this further, and\nrefer the interested reader to [12\u2104.\n1 It\n\nshould be noted though that e\u001e ient ompression is important not only per se, but\nalso for ryptographi appli ations as pointed out in [11\u2104. In addition, language models\ndeveloped for the purpose of ompression are su essfully used in appli ations like spee h\nre ognition and OCR, allowing to disambiguate di\u001e ult ases and orre t errors.\n3\n\n\f2\n\nExperimental setup\n\nIn their Introdu tion to the spe ial issue on omputational linguisti s using large orpora, Chur h and Mer er [13\u2104 note that \u0010The 1990s\nhave witnessed a resurgen e of interest in 1950s-style empiri al and\nstatisti al methods of language analysis\u0011. They attribute this empiri al renaissan e primarily to the availability of pro essing power and\nof massive quantities of data. Of ourse, these fa tors favor statistial analysis of texts as hara ter strings. However, wide availability\nof omputer networks and intera tive Web te hnologies also made it\npossible to set up large-s ale experiments with human subje ts.\nThe experiment has the form of an online literary game in Russian2 .\nHowever, the players are also fully aware of the resear h side, have\nfree a ess to theoreti al ba kground and urrent experimental results,\nand an parti ipate in online dis ussions. The players are presented\nwith text fragments in whi h one of the words is repla ed with blanks\nor with a di\u001berent word. Any sequen e of 5 or more Cyrilli letters\nsurrounded by non-letters was onsidered a \u0010word\u0011. Words are sele ted\nfrom fragments randomly. There are three di\u001berent trial types:\ntype 1: a word is omitted, and is to be guessed.\ntype 2: a word is highlighted, and the task is to determine whether\nit is original or repla ed.\ntype 3: two words are displayed, and the subje t has to determine\nwhi h one is the original word.\nIn orre t guesses from trials of type 1 are used as repla ements in\ntrials of types 2 and 3.\nTexts are randomly drawn from a orpus of 3439 fragments of\nmostly poeti works in a wide range of styles and periods: from Avantgarde to mass ulture and from 18th entury to ontemporary. Three\nprosai texts are also in luded (two lassi novels, and a ontemporary\npoliti al essay).\nAs of this writing, the experiment has been running almost ontinuously for three years. Over 8000 people took part in it and olle tively\nmade almost 900,000 guesses, about a third of whi h is of type 1. The\ntraditional laboratory experiment ould have never a hieved this s ale.\nOf ourse, the te hnique has its own drawba ks, whi h are dis ussed in\n2 http://ygre\n\n.msk.ru\n\n4\n\n\fdetail in [12\u2104. But they are a small pri e to pay for statisti al relevan e,\nespe ially if it an't be a hieved in any other way.\n\n3\n\nResults\n\nThe spe i\u001c goal of the experiment is to dis over and analyze systemati di\u001beren es between di\u001berent ategories of texts from the viewpoint\nof how easy it is to a) re onstru t an omitted word, and b) distinguish\nthe original word from a repla ement. However here we'll onsider a\nparti ular property of the texts that turns out to be independent of\nthe text type and so probably hara terizes the language itself rather\nthan spe i\u001c texts. This property is the dependen y of word unpredi tability on its length.\nWe de\u001cne unpredi tability U as the negative binary logarithm of\nthe probability to guess a word, U = \u2212 log2 p1 , where p1 is the average\nrate of orre t answers to trials of type 1. For a single word, this is\nformally equivalent to Shannon's de\u001cnition of entropy, H . However,\nwhen multiple words are taken into a ount, entropy should be al ulated as the average logarithm of probability, and not as the logarithm\nof average probability,\n\nH=\u2212\n\nN\n1 X\nlog pi\nN i=1 2 1\n\n(1)\n\nN\n1 X\npi\nN i=1 1\n\n(2)\n\nU = \u2212 log2\n\nIndeed, the logarithm of probability to guess a word equals the\namount of information in bits required to determine the word hoi e.\nThus, it is this quantity that is subje t to averaging. When dealing\nwith experimental data, it is ustomary to use frequen ies as estimates\nof unobservable probabilities. However, there are always words that\nwere never guessed orre tly and have p1 = 0 for whi h logarithm is\nunde\u001cned (this is why Shannon's te hinque involves repeated guessing\nof the same letter until the orre t answer is obtained). Formally, if\nthere is one element in the sequen e with zero (very small, in fa t)\nprobability of being guessed, then the amount of information of the\nwhole sequen e may be determined solely by this one element.\nOn the other hand, unpredi tability as de\u001cned above is not sensitive to the exa t probability to guess su h words, but only on how\n5\n\n\fmany there are of them. While entropy hara terizes the number of\ntries required to guess a randomly sele ted word, unpredi tability hara terizes the portion of words that would be guessed on the \u001crst try.\nThey are equal, of ourse, if all words have the same entropy.\nOne way around the problem presented by never-guessed words\nwould be to assign some arbitrary \u001cnite entropy to them. We ompared unpredi tability with entropy al ulated under this approximation with two values of the onstant: 10 bits ( orresponding roughly to\nwild guessing using a frequen y di tionary) and 3 bits (the low bound).\nIn both ases, while H is not equal numeri ally to U , they turned out to\nbe in an almost monotoni , approximately linear orresponden e. This\nprobably means that the fra tion of hard-to-guess words o-varies with\nunpredi tability of the rest of the words. Be ause of this, we prefer\nto work in terms of unpredi tability, rather than introdu ing arbitrary\nhypotheses to al ulate an entropy value of dubious validity.\nUnpredi tability as a fun tion of word length al ulated over all\nwords of the same length a ross all texts is plotted in Fig. 1 and\nFig. 2 (where word length is measured in hara ters and syllables\nrespe tively). Con\u001cden e intervals on the graphs are al ulated based\non the standard deviation of the binomial distribution (sin e the data\nomes from a series of independent trials with two possible out omes\nin ea h: a guess may be orre t or in orre t).\nIn the range from 5 to 14 hara ters and from 1 to 5 syllables,\nan ex ellent linear dependen e is observed. Longer words are rare, so\nthe data for them is signi\u001c antly less statisti ally reliable. We'll only\ndis uss the linear dependen e in the range where it is de\u001cnitely valid.\n\n4\n\nDis ussion\n\nIt is very di\u001e ult, for the reasons mentioned above, to ompare our\nresults with previous studies. However, there are two points of omparison that an be made. First, we an roughly estimate the e\u001be t of\nword guessing in ontext as opposed to guessing the next word in sequen e. Re all that Shannon [1\u2104 estimated zeroth-order word entropy\nfor English based on Zipf's law to be 11.82 bits per word (bpw). Brown\net al [10\u2104 used a word trigram model to a hieve an entropy estimate\nof 1.72 bp , whi h translates to 7.74 bpw for average word length of\n4.5 hara ters in English. This means that trigram word probabilities\nontribute 11.82 \u2212 7.74 = 4.08 bpw for predi tion of word in sequen e.\n\n6\n\n\f7\n\nunpredi tability\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\nword length in\n\nFigure 1:\n\n12\n\n14\n\n16\n\n18\n\n20\n\nhara ters\n\nUnpredi tability as a fun tion of word length in\n\nhara ters, all\n\ntexts\n\n3.5\n\nunpredi tability\n\n3\n\n2.5\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n0\n\n1\n\n2\n\n3\n\nword length in\n\n4\n\n5\n\n6\n\n7\n\nyllables\n\nFigure 2: Unpredi tability as a fun tion of word length in syllables, all texts\n\n7\n\n\fBut word in ontext parti ipates in three trigrams at on e: as the last,\nthe middle and the \u001crst word of a trigram. Only the \u001crst trigram is\navailable when the model is predi ting the next word, but all three trigrams ould be used to \u001cll in an omitted word (this is a hypotheti al\nexperiment whi h was not a tually performed). Of ourse, they are\nnot statisti ally independent, and as a rough estimate we an assume\nthat the last trigram ontributes somewhat less information than the\n\u001crst one, while the middle trigram ontributes very little (sin e all of\nits words are already a ounted for). In other words, we ould expe t\nthis model to have about 4 bpw more information when guessing words\nin ontext, whi h is very signi\u001c ant.\nThe se ond point of omparison is provided by [14\u2104 (Fig. 13 there),\nwhere entropy is plotted for the n-th letter of ea h word, versus its\nposition n. Entropy was estimated using a Ziv\u0015Lempel type algorithm. It is well-known that guessing is least on\u001cdent at the word\nboundaries for both human subje ts and omputer algorithms, and\nthis hart quanti\u001ces the observation: the \u001crst letter has the entropy\nof 4 bp , whi h drops qui kly to about 0.6\u00150.7 bp for the 5th letter and then stays surprizingly onstant all the way through the 16th\nhara ter. This hart is pra ti ally the same for the original text and\na text with randomly permuted words, whi h gives a telling eviden e\nof the urrent language models' strengths and weaknesses. For the\npurposes of this dis ussion, the data allows to re onstru t the depenP\n(l)\n(w)\nden y of word entropy on the word length as hn = ni=1 hi , where\n(w)\n(l)\nhn is the entropy of words of length n, and hi is the entropy of\nthe i-th letter in a word. This dependen y, valid for the language\nmodel in [14\u2104, has a steep in rease from 1 through 5 hara ters, and\nthen an approximately linear growth with a mu h shallower slope of\n0.6\u00150.7 bp . This is very di\u001berent from our Fig. 1, and even though\nour data is on unpredi tability, rather than entropy, the di\u001beren e is\nprobably signi\u001c ant.\nIn fa t, our result may at \u001crst glan e seem trivial. Indeed, a ording to a theorem due to Shannon (Theorem 3 in [2\u2104), for a hara ter sequen e emitted by a stationary ergodi sour e, almost all subsequen es\nof length n have the same probability exponential in n: Pn = 2\u2212Hn for\nlarge enough length (H is the entropy of the sour e). However, this explanation is not valid here for several reasons. Even if we set aside the\nquestion of natural language ergodi ity, from the formal point of view,\nthe theorem requires that n is large enough so that all possible letter\ndigrams are likely to be en ountered more than on e (many times, in\n8\n\n\ffa t). Needless to say that the length of a single word is mu h less\nthan that. Pra ti ally, if this explanation were to be adopted, we'd\nexpe t the probability to guess a word to be on the order Pn , whi h\nis mu h smaller than the observed probability. In fa t, the only reason our subje ts are able to guess words in ontext is that the words\nare onne ted to the ontext and make sense in it, while under the\nassumptions of Shannon's theorem, the equiprobable subsequen es are\nasymptoti ally independent of the ontext.\nAnother tentative argument is to presume that the total number\nof words in the language (either in the vo abulary or in texts, whi h\nis not the same thing) of a given length in reases with length, whi h\nmakes longer words harder to guess due to sheer expansion of possibilities. If there had been exponential expansion of vo abulary with word\nlength, we ould argue that ontextual restri tions on word hoi e ut\nthe number of hoi es by a onstant fa tor (on the average), so the\nnumber of words satisfying these restri tions still grows exponentially\nwith word length. However, the data does not support this idea. Distribution of words by length, whether omputed from the a tual texts\nor from a di tionary (we used a Russian frequen y di tionary ontaining 32000 words [15\u2104), is not even monotoni , let alone exponentially\ngrowing. The number of di\u001berent words grows up to about 8 hara ters of length, then de reases. This behavior is in no way re\u001de ted\nin Figs 1, 2, so we an on lude that the total number of di tionary\nwords of a given length is not a fa tor in guessing su ess.\nIn fa t, the word length distribution ould have had a dire t e\u001be t\non unpredi tability only if the word length were known to the subje t.\nBut this is generally not the ase. Subje ts in our experiment are not\ngiven any external lue as to the length of the omitted word. Sin e\nRussian verse is for the most part metri , the syllabi length of a line\nis typi ally known, and this allows to predi t the syllabi length of the\nomitted word with a great deal of ertainty. However unpredi tability\ndepends on word length in exa tly the same way for poetry and prose\n(see Fig. 3), and in prose there are no external or internal lues for\nthe word length. 3\nThis leaves us with the only reasonable explanation for the ob3 It is also worth noting that average unpredi tability of words in poetry and prose\nis surprisingly lose. In poetry, it turns out, predi tability due to meter and rhyme is\nountera ted by in reased unpredi tability of semanti s and, possibly, grammar. Notably,\nthese two tenden ies almost balan e ea h other. This phenomenon and its signi\u001c an e is\ndis ussed at length in [12\u2104.\n\n9\n\n\f7\n\nunpredi tability\n\n6\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\nword length in\n\n12\n\n14\n\n16\n\n18\n\n20\n\nhara ters\n\nFigure 3: Unpredi tability as a fun tion of word length in\n\nhara ters, prose\n\nonly\n\nserved dependen y: in ourse of its evolution, the language tends to\neven out information rate, so that longer words arry proportionally\nmore information. This would be a natural assumption, sin e an uneven information rate is ine\u001e ient: some portions will underutilize the\nbandwidth of the hannel, and some will overutilize it and diminish\nerror- orre tion apabilities. In other words, as language hanges over\ntime, some words and grammati al forms that are too long will be\nshortened, and those that are too short will be expanded and reinfor ed.\nIt is interesting to note that this hypothesis was also proposed in\npassing by Chur h and Mer er in a di\u001berent ontext in [13\u2104. Dis ussing\nappli ations of trigram word-predi tion models to spee h re ognition,\nthey write (page 12):\nIn general, high-frequen y fun tion words like to and\nthe, whi h are a ousti ally short, are more predi table than\nontent words like resolve and important, whi h are longer.\nThis is onvenient for spee h re ognition be ause it means\nthat the language model provides more powerful onstraints\njust when the a ousti model is having the toughest time.\n10\n\n\fOne suspe ts that this is not an a ident, but rather a natural result of the evolution of spee h to \u001cll the human needs\nfor reliable ommuni ation in the presen e of noise.\nA feature that is \u0010 onvenient for spee h re ognition\u0011 is, indeed, not\nto be unexpe ted in natural language, and from our results it appears\nthat its extent is mu h broader than ould be suggested by Chur h and\nMer er's observation. Of ourse, this is only one of many me hanisms\nthat drive language hange, and it only a ts statisti ally, so any given\nlanguage state will have low-redundan y and high-redundan y po kets.\nThus, any Russian speaker knows how di\u001e ult it is to distinguish\nbetween mne nado 'I need' and ne nado 'please don't'. Moreover,\nit is likely that this hange typi ally pro eeds by va illations. As an\nexample onsider the evolution of negation in English a ording to [16\u2104\n(p. 175\u0015176):\nthe original Old English word of negation was ne, as\nin i ne wat, 'I don't know'. This ordinary mode of negation ould be reinfor ed by the hyperboli use of either wiht\n'something, anything' or nawiht 'nothing, not anything' [...\u2104.\nAs time progressed, the hyperboli for e of (na)wiht began\nto fade [...\u2104 and the form nawiht ame to be interpreted as\npart of a two-part, \u0010dis ontinuous\u0011 marker of negation ne\n... nawiht [...\u2104. But on e ordinary negation was expressed\nby two words, ne and nawiht, the stage was set for ellipsis\nto ome in and to eliminate the seeming redundan y. The\nresult was that ne, the word that originally had been the\nmarker of negation, was deleted, and not, the re\u001dex of originally hyperboli nawiht be ame the only marker of negation. [...\u2104 (Modern English has introdu ed further hanges\nthrough the introdu tion of the \u0010helping word\u0011 do.)\nThis looks very mu h like os illations resulting from an iterative\nsear h for the optimum length of a parti ular grammati al form. It's\nall the more amazing then, how this tenden y, despite its statisti al\nand non-stationary hara ter, beautifully manifests itself in the data.\nAddendum. After this paper was published in J. Information\nPro ., the author be ame aware of the following works in whi h e\u001be ts\nof the same nature was dis overed on di\u001berent levels:\nThe dis our e level. Genzel and Charniak [17\u2104 study the entropy\nof a senten e depending on its position in the text. They show that\n\n11\n\n\fthe entropy al ulated by a language model, whi h does not a ount\nfor semanti s, in reases somewhat in the initial portion of the text.\nThey on lude that the hypotheti al entropy value with a ount for\nsemanti s would be onstant, be ause the ontent of the pre eding text\nwould help predi ting the following text.\nThe senten e level. The authors of [18\u2104 onsider the English senten es with optional relativizer that. They demonstrate experimentally\nthat the speakers tend to utter the optional relativizer more frequently\nin those senten es where information density is higher thus \u0010diluting\u0011\nthem. This an be interpreted as a tenden y to homogenize information density.\nThe syllabe level Aulett and Turk [19\u2104 demonstrate that in spontaneous spee h, those syllables that are less predi table, are in reased\nin duration and prosodi prominen e. In this way, speakers tend to\nsmooth out the redundan y level.\nA knowledgements. I am grateful to D. Flitman for hosting\nthe website, to Yu. Manin, M. Verbitsky, Yu. Fridman, R. Leibov,\nG. Mints and many others for fruitful dis ussions. This work ould\nnot have happened without the generous support of over 8000 people\nwho generated experimental data by playing the game.\n\nReferen es\n[1\u2104 Shannon C.E. Predi tion and entropy of printed English. Bell\nSystem Te hni al Journal, 1951, vol. 30, pp. 50\u001564.\n[2\u2104 Shannon C.E. A mathemati al theory of ommuni ation. Bell System Te hni al Journal, 1948, vol. 27, pp. 379\u0015423.\n[3\u2104 Burton N.G., Li klider J.C.R. Long-range onstraints in the statisti al stru ture of printed English. Ameri an Journal of Psyhology, 1955, vol. 68, no. 4, pp. 650\u0015653\n[4\u2104 F\u0001onagy I. Informationsgehalt von wort und laut in der di htung. In: Poeti s. Poetyka. \u00cf\u00ee\u00fd\u00f2\u00e8\u00ea\u00e0. Warszawa: Pa\u0001\nnstwo\nWydawni two Naukowe, 1961, pp. 591\u0015605.\n[5\u2104 Kolmogorov A. Three approa hes to the quantitative de\u001cnition of\ninformation. Problems Inform. Transmission, 1965, vol. 1, pp. 1\u0015\n7.\n[6\u2104 Yaglom A.M. and Yaglom I.M. Probability and information Reidel, Dordre ht, 1983.\n12\n\n\f[7\u2104 Cover T.M., King R.C. A onvergent gambling estimate of the\nentropy of English. Information Theory, IEEE Transa tions on,\n1978, vol. 24, no. 4, pp. 413\u0015421.\n[8\u2104 Moradi H., Roberts J.A., Grzymala-Busse J.W. Entropy of English text: Experiments with humans and a ma hine learning\nsystem based on rough sets. Inf. S i., 1998, vol. 104, no. 1\u00152,\npp. 31\u001547.\n[9\u2104 Paisley W.J. The e\u001be ts of authorship, topi stru ture, and time\nof omposition on letter redundan y in English text. J. Verbal.\nBehav., 1966, vol. 5, pp. 28\u001534.\n[10\u2104 Brown P.F., Della Pietra V.J., Mer er R.L., Della Pietra S.A.,\nLai J.C. An estimate of an upper bound for the entropy of English.\nComput. Linguist., 1992, vol. 18, no. 1, pp. 31\u001540.\n[11\u2104 Teahan W.J., Cleary J.G. The entropy of English using PPMbased models. In: DCC '96: Pro eedings of the Conferen e on\nData Compression, Washington: IEEE Computer So iety, 1996,\npp. 53\u001562.\n[12\u2104 Leibov R.G., Manin D.Yu. An attempt at experimental poeti s\n[tentative title\u2104. To be published in: Pro . Tartu Univ. [in Russian\u2104, Tartu: Tartu University Press\n[13\u2104 Chur h K.W., Mer er R.L. Introdu tion to the spe ial issue on\nomputational linguisti s using large orpora. Comput. Linguist.,\n1993, vol. 19, no. 1, pp. 1\u001524.\n[14\u2104 T.S h\u0004\nurmann and P.Grassberger. Entropy estimation of symbol\nsequen es. Chaos, 1996, vol. 6, no. 3, pp. 414\u0015427.\n[15\u2104 Sharo\u001b S.,\nThe frequen y di tionary for\nhttp://www.artint.ru/proje ts/frqlist/frqlist-en.asp\n\nRussian.\n\n[16\u2104 Ho k H.H., Joseph B.D. Language History, Language Change,\nand Language Relationship. Berlin\u0015New York: Mouton de\nGruyter, 1996.\n[17\u2104 Genzel & Charniak, 2002. Entropy rate onstan y in text. Pro .\n40th Annual Meeting of ACL, 199\u0015206.\n[18\u2104 Anonymous authors (paper under review), 2006. Speakers optimize information density through synta ti redu tion. To be published.\n\n13\n\n\f[19\u2104 Aylett M. and Turk A., 2004. The Smooth Signal Redundan y\nHypothesis: A Fun tional Explanation for Relationships between\nRedundan y, Prosodi Prominen e, and Duration in Spontaneous\nSpee h. Language and Spee h, 47(1), 31\u001556.\n\n14\n\n\f"}