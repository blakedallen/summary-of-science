{"id": "http://arxiv.org/abs/1107.1831v1", "guidislink": true, "updated": "2011-07-10T04:32:30Z", "updated_parsed": [2011, 7, 10, 4, 32, 30, 6, 191, 0], "published": "2011-07-10T04:32:30Z", "published_parsed": [2011, 7, 10, 4, 32, 30, 6, 191, 0], "title": "Adjoints and Automatic (Algorithmic) Differentiation in Computational\n  Finance", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.5183%2C1107.0952%2C1107.0725%2C1107.0938%2C1107.1478%2C1107.3251%2C1107.4113%2C1107.3411%2C1107.4937%2C1107.5349%2C1107.4602%2C1107.3510%2C1107.1754%2C1107.1419%2C1107.5282%2C1107.1047%2C1107.4532%2C1107.2316%2C1107.3995%2C1107.5456%2C1107.3979%2C1107.1639%2C1107.4344%2C1107.1377%2C1107.1768%2C1107.2847%2C1107.2957%2C1107.1141%2C1107.1441%2C1107.5373%2C1107.5936%2C1107.0370%2C1107.4347%2C1107.0114%2C1107.5529%2C1107.0942%2C1107.0912%2C1107.4342%2C1107.5329%2C1107.5610%2C1107.1171%2C1107.1744%2C1107.5624%2C1107.0507%2C1107.3687%2C1107.3368%2C1107.2174%2C1107.5258%2C1107.2067%2C1107.4144%2C1107.1202%2C1107.1831%2C1107.2244%2C1107.1709%2C1107.4372%2C1107.5809%2C1107.2554%2C1107.0944%2C1107.2155%2C1107.5984%2C1107.0062%2C1107.0782%2C1107.4251%2C1107.2630%2C1107.4304%2C1107.2813%2C1107.1390%2C1107.0149%2C1107.4445%2C1107.4791%2C1107.5708%2C1107.2229%2C1107.0960%2C1107.0100%2C1107.3831%2C1107.0052%2C1107.2272%2C1107.5460%2C1107.3492%2C1107.2243%2C1107.4476%2C1107.1573%2C1107.5441%2C1107.4135%2C1107.4497%2C1107.4104%2C1107.3370%2C1107.1808%2C1107.2505%2C1107.1773%2C1107.1494%2C1107.5525%2C1107.4942%2C1107.3103%2C1107.3973%2C1107.4327%2C1107.2585%2C1107.0158%2C1107.3284%2C1107.4294%2C1107.4244&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adjoints and Automatic (Algorithmic) Differentiation in Computational\n  Finance"}, "summary": "Two of the most important areas in computational finance: Greeks and,\nrespectively, calibration, are based on efficient and accurate computation of a\nlarge number of sensitivities. This paper gives an overview of adjoint and\nautomatic differentiation (AD), also known as algorithmic differentiation,\ntechniques to calculate these sensitivities. When compared to finite difference\napproximation, this approach can potentially reduce the computational cost by\nseveral orders of magnitude, with sensitivities accurate up to machine\nprecision. Examples and a literature survey are also provided.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.5183%2C1107.0952%2C1107.0725%2C1107.0938%2C1107.1478%2C1107.3251%2C1107.4113%2C1107.3411%2C1107.4937%2C1107.5349%2C1107.4602%2C1107.3510%2C1107.1754%2C1107.1419%2C1107.5282%2C1107.1047%2C1107.4532%2C1107.2316%2C1107.3995%2C1107.5456%2C1107.3979%2C1107.1639%2C1107.4344%2C1107.1377%2C1107.1768%2C1107.2847%2C1107.2957%2C1107.1141%2C1107.1441%2C1107.5373%2C1107.5936%2C1107.0370%2C1107.4347%2C1107.0114%2C1107.5529%2C1107.0942%2C1107.0912%2C1107.4342%2C1107.5329%2C1107.5610%2C1107.1171%2C1107.1744%2C1107.5624%2C1107.0507%2C1107.3687%2C1107.3368%2C1107.2174%2C1107.5258%2C1107.2067%2C1107.4144%2C1107.1202%2C1107.1831%2C1107.2244%2C1107.1709%2C1107.4372%2C1107.5809%2C1107.2554%2C1107.0944%2C1107.2155%2C1107.5984%2C1107.0062%2C1107.0782%2C1107.4251%2C1107.2630%2C1107.4304%2C1107.2813%2C1107.1390%2C1107.0149%2C1107.4445%2C1107.4791%2C1107.5708%2C1107.2229%2C1107.0960%2C1107.0100%2C1107.3831%2C1107.0052%2C1107.2272%2C1107.5460%2C1107.3492%2C1107.2243%2C1107.4476%2C1107.1573%2C1107.5441%2C1107.4135%2C1107.4497%2C1107.4104%2C1107.3370%2C1107.1808%2C1107.2505%2C1107.1773%2C1107.1494%2C1107.5525%2C1107.4942%2C1107.3103%2C1107.3973%2C1107.4327%2C1107.2585%2C1107.0158%2C1107.3284%2C1107.4294%2C1107.4244&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Two of the most important areas in computational finance: Greeks and,\nrespectively, calibration, are based on efficient and accurate computation of a\nlarge number of sensitivities. This paper gives an overview of adjoint and\nautomatic differentiation (AD), also known as algorithmic differentiation,\ntechniques to calculate these sensitivities. When compared to finite difference\napproximation, this approach can potentially reduce the computational cost by\nseveral orders of magnitude, with sensitivities accurate up to machine\nprecision. Examples and a literature survey are also provided."}, "authors": ["Cristian Homescu"], "author_detail": {"name": "Cristian Homescu"}, "author": "Cristian Homescu", "arxiv_comment": "23 pages", "links": [{"href": "http://arxiv.org/abs/1107.1831v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.1831v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "q-fin.CP", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "q-fin.CP", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.1831v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1107.1831v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:1107.1831v1 [q-fin.CP] 10 Jul 2011\n\nAdjoints and automatic (algorithmic)\ndifferentiation in computational finance\nCristian Homescu\u2217\nRevised version: May 8, 2011\u2020\n\nTwo of the most important areas in computational finance: Greeks and, respectively, calibration, are\nbased on efficient and accurate computation of a large number of sensitivities. This paper gives an overview\nof adjoint and automatic differentiation (AD), also known as algorithmic differentiation, techniques to calculate these sensitivities. When compared to finite difference approximation, this approach can potentially\nreduce the computational cost by several orders of magnitude, with sensitivities accurate up to machine\nprecision. Examples and a literature survey are also provided.\n\n\u2217\n\u2020\n\nEmail address: cristian.homescu@gmail.com\nOriginal version: May 1, 2011\n\n1\n\n\fContents\n1 Introduction\n\n3\n\n2 General description of the approach\n2.1 Single function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2 Composite functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3 Checking the correctness of the implementation . . . . . . . . . . . . . . . . . . . . . . . .\n\n4\n4\n5\n6\n\n3 Simple example\n3.1 Forward (tangent linear) mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Adjoint (reverse) mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n7\n7\n8\n\n4 Example in PDE solver framework\n4.1 Forward (tangent linear) mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Adjoint (reverse) mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n9\n9\n10\n\n5 Example in Monte Carlo framework (evolution of SDE)\n5.1 Forward (tangent linear) mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2 Adjoint (reverse) mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n11\n11\n13\n\n6 Example in Monte Carlo framework (copula)\n6.1 Forward (tangent linear) mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6.2 Adjoint (reverse) mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n13\n14\n15\n\n7 Example in calibration/optimization framework\n\n16\n\n8 Computational finance literature on adjoint and AD\n8.1 Computation of Greeks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8.2 Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n16\n16\n19\n\n9 AD and adjoint applied within a generic framework in practitioner quant libraries\n9.1 Block architecture for Greeks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9.2 Real Time Counterparty Risk Management in Monte Carlo . . . . . . . . . . . . . . . . .\n\n19\n19\n20\n\n10 Additional resources\n\n20\n\n11 Conclusion\n\n20\n\n2\n\n\f1 Introduction\nTwo of the most important areas in computational finance: Greeks and, respectively, calibration, are based\non efficient and accurate computation of a large number of sensitivities. This paper gives an overview of\nadjoint and automatic(algorithmic) differentiation techniques to calculate those sensitivities. While only\nrecently introduced in the field of computational finance, it was successfully employed in the last 20 years\nin other areas, such as computational fluid dynamics, meteorology and atmospheric sciences, engineering\ndesign optimization, etc: [25, 27, 42, 40, 46, 47, 33, 61, 68], to mention but a few.\nThe computation of the sensitivities is done by performing differentiation at either continuous level or,\nrespectively, at discrete level. The \"continuous\" adjoint approach corresponds to the case where the adjoint\nequation is formulated at the differential equation level, and then discretized. In contrast, the \"discrete\"\nadjoint approach starts with discretized equations, and then formulates the corresponding discrete adjoint\nequations. Consequently, the continuous adjoint is also known under \"differentiate then discretize\" moniker,\nwhile the discreet adjoint corresponds to \"discretize then differentiate\" moniker.\nThe discrete adjoint approach is preferred in many occasions, for several practical reasons:\n\u2022 constructing the discrete adjoint equations is a more straightforward and systematic process\n\u2022 Automatic Differentiation software can be employed to greatly reduce the development time\nFor this reason we will concentrate in this paper on the discrete adjoint approach, and for ease of presentation we refer to it as the adjoint approach. However, since a few papers that are reviewed here are also\nbased on continuous adjoint, we will make that information explicit when we refer to those papers, and\nuse the abbreviation ContAdj for the \"continuous adjoint\" approach.\nAutomatic Differentiation (AD), also known as Algorithmic Differentiation, is a chain-rule-based technique for evaluating the derivatives with respect to the input variables of functions defined by a high-level\nlanguage computer program. AD relies on the fact that all computer programs, no matter how complicated, use a finite set of elementary (unary or binary, e.g. sin(*), sqrt(*)) operations as defined by the\nprogramming language. The value or function computed by the program is simply a composition of these\nelementary functions. The partial derivatives of the elementary functions are known, and the overall\nderivatives can be computed using the chain rule.\nAD has two basic modes of operations, the forward mode and the reverse mode. In the forward mode\nthe derivatives are propagated throughout the computation using the chain rule, while the reverse mode\ncomputes the derivatives for all intermediate variables backwards (i.e., in the reverse order) through the\ncomputation. In the literature, AD forward mode is sometimes referred to as tangent linear mode, while\nAD reverse mode is denoted as adjoint mode.\nThe adjoint method is advantageous for calculating the sensitivities of a small number of outputs with\nrespect to a large number of input parameters. The for- ward method is advantageous in the opposite\ncase, when the number of outputs (for which we need sensitivities) is larger compared to the number of\ninputs.\nWhen compared to regular methods (such as finite differencing) for computing sensitivities, AD has 2\nmain advantages: reduction in computational time and accuracy. The reduction in computational time is\nassured by a theoretical result [43]that states that the cost of the reverse mode is smaller than five times\nthe computational cost of a regular run. The computational cost of the adjoint approach is independent\nof the number of inputs for which we want to obtains the sensitivities with respect to, whereas the cost\nof thetangent linear approach increases linearly with the number of inputs. Regarding accuracy, AD\ncomputes the derivatives exactly (up to machine precision) while finite differences incur truncation errors.\n\n3\n\n\fThe size of the step h needed for finite difference varies with the current value of of input parameters,\nmaking the problem of choosing h, such that it balances accuracy and stability, a challenging one. AD on\nthe other hand, is automatic and time need not be spent in choosing step-size parameters, etc.\nAD software packages can also be employed to speed up the development time. Such tools implement\nthe semantic transformation that systematically applies the chain rule of differential calculus to source\ncode written in various programming languages. The generated code can operate in forward or reverse\nmode (tangent linear or adjoint model).\n\n2 General description of the approach\nLet us assume that we have a model dependent on a set of input parameters which produces an output Y.\nWe also denote by Z = {X1 , X2 , ..., XN } the vector of input parameters with respect to which we want to\ncompute sensitivities.\n\n2.1 Single function\nWe assume we have a single function of the model output, denoted F (Y ). The goal is to obtain sensitivities\nof F with respect to the components of Z.\nComputation of the vector of sensitivities\n\u1e1e =\n\n\u2202F\n\u2202F\n\u2202F\n=\n\u1e8a1 + * * * +\n\u1e8aN\n\u2202Z\n\u2202X1\n\u2202XN\n\n(2.1)\n\nis done using the forward (tangent linear) mode. If we include the trivial equations \u1e8a1 = \u1e8a1 , * * * , \u1e8aN =\n\u1e8aN , then we can rewrite the combined expressions in matrix notation. This will prove helpful when\nconstructing the adjoint mode, using the fact that adjoint of a matrix A is defined as its conjugate\ntranspose (or simply the transpose, if the matrix A has only real elements)\n\uf8f6 \uf8eb\n1\n\u1e8a1\n\uf8ec ... \uf8f7 \uf8ec ...\n\uf8ec\n\uf8f7 \uf8ec\n\uf8ed \u1e8aN \uf8f8 = \uf8ed 0\n\u2202F\n\u1e1e\n\u2202X1\n\uf8eb\n\nTo compute each component of the vector\n\n...\n...\n...\n...\n\n\uf8f6\n\uf8eb\n\uf8f6\n0\n\u1e8a1\n... \uf8f7\n\uf8f7\uf8ed ... \uf8f8\n1 \uf8f8\n\u1e8a1\n\u2202F\n\n(2.2)\n\n\u2202XN\n\n\u2202F\n\u2202Z\n\nwen need to evaluate\nthe expression (2.1) a number of N\no\ntimes, every time with a different input vector \u017b = \u1e8a1 , . . . , \u1e8aN .For example, to compute the derivative\n\nwith respect to Xj , the vector \u017b has the value \u017b = (0, 0, . . . , 1, 0, . . . , 0), with the only nonzero element\nin the j \u2212 th position.\nTo construct the reverse (adjoint) mode, we start with the transposed matrix equation (2.2) With the\nnotation of \u0100 for the adjoint variable corresponding to an original variable A, we have\n\uf8eb\n\uf8f6\n\uf8eb\n\uf8f6 \uf8eb\nX\u03041\n\u2202F \uf8f6\n1 ...\n0\nX\u03041\n\u2202X1\n\uf8ec\n\uf8f7\n\uf8ed ... \uf8f8 = \uf8ed ... ... ... ... \uf8f8\uf8ec ... \uf8f7\n\uf8ed X\u0304N \uf8f8\n\u2202F\nX\u0304N\n0 ...\n1 \u2202X\nN\nF\u0304\n\n4\n\n\fConsequently, we obtain the following expressions\nX\u03041\n\n= X\u03041\n\n...\n\n...\n\nX\u0304N\n\n= X\u0304N\n\n\u2202F\nF\u0304\n\u2202X1\n...\n\u2202F\n+\nF\u0304\n\u2202XN\n\n+\n\n(2.3)\n\nThus we can obtain the sensitivities in one single run of (2.3), with F\u0304 = 1 and the adjoint variables\nX\u03041 , . . . , X\u0304N initialized to zero.\n\n2.2 Composite functions\nWe can generalize this procedure if the output is obtained through evaluation of a composite function of\nP single functions (which is in fact how the computer codes are represented):\nF = F P \u25e6 F P \u22121 \u25e6 * * * \u25e6 F 1 (Z)\nWe apply the tangent linear mode to each F j , and we combine them in the recursion relationship. For\nthe adjoint (reverse mode), we construct the adjoint for each F j , and we combine them in reverse order.\nLet us describe the process using the matrix notation. If we view the tangent linear as the result of the\nmultiplication of a multiplication of a number of operator matrices\nM AT = M at1 * M at2 * * * M atP\nwhere each matrix M atj represents either a subroutine or a single statement, then the adjoint approach\ncan be viewed as a product of adjoint subproblems\nM atT = M atTP * M atTP \u22121 * * * M atT1\nLet us describe how it works through an example for P=2. The computational flow is described by the\nfollowing diagram\n\u0001\nZ \u2192 F 1 (Z) \u2192 F 2 F 1 (Z) \u2192 Y\n\u0001\nFor simplicity, we denote by A the output of F 1 (Z) and by B the output of F 2 F 1 (Z) . Y is the\nscalar that is the final output. For example, Y can be the value of the function to be calibrated (in the\ncalibration setup) or, respectively, the price of the option (in the pricing setup, e..g. using Monte Carlo\nor finite differences). With these notations, we have\nZ\u2192A\u2192B\u2192Y\nApplying tangent linear methodology (essentially differentiation line by line) we have\n\u2202A\n\u017b\n\u2202Z\n\u2202B\n\u0226\n\u2202A\n\u2202Y\n\u1e02\n\u2202B\n\n\u0226 =\n\u1e02 =\n\u1e8e\n\n=\n\n5\n\n\fPutting everything together, we get\n\u1e8e =\n\n\u2202Y \u2202B \u2202A\n\u017b\n\u2202B \u2202A \u2202Z\n\nUsing notation from AD literature, the adjoint quantities Z\u0304, \u0100, B\u0304, \u0232 denote the derivatives of Y with\nrespect to Z, A, B and, respectively, to Y . We note that this implies that \u0232 = 1. Differentiating again,\nand with a superscript T denoting a matrix or vector transpose, we obtain\nZ\u0304 =\n\n\u0012\n\n\u2202Y\n\u2202Z\n\n\u0013T\n\n=\n\n\u0012\n\n\u2202Y \u2202A\n\u2202A \u2202Z\n\n\u0013T\n\n=\n\n\u0012\n\n\u2202A\n\u2202Z\n\n\u0013T\n\n\u0100\n\nIn a similar way, we obtain\n\u0012\n\u0012\n\u0013\n\u0013\n\u0013\n\u2202Y T\n\u2202Y \u2202B T\n\u2202B T\n\u0100 =\n=\n=\nB\u0304\n\u2202A\n\u2202B \u2202A\n\u2202A\n\u0012\n\u0012\n\u0013\n\u0013\n\u0013\n\u0012\n\u2202Y T\n\u2202Y T\n\u2202Y T\n=\n*1=\n\u0232\nB\u0304 =\n\u2202B\n\u2202B\n\u2202B\n\u0012\n\nPutting everything together, we get\nZ\u0304 =\n\n\u0012\n\n\u2202A\n\u2202Z\n\n\u0013T \u0012\n\n\u2202B\n\u2202A\n\n\u0013T \u0012\n\n\u2202Y\n\u2202B\n\n\u0013T\n\n\u0232\n\nWe notice that the tangent linear approach proceeds forward (in forward mode)through the process\n\u017b \u2192 \u0226 \u2192 \u1e02 \u2192 \u1e8e\nwhile the adjoint approach proceeds backwards (in reverse mode)\n\u0232 \u2192 B\u0304 \u2192 \u0100 \u2192 Z\u0304\n\n2.3 Checking the correctness of the implementation\nThere are several checks that need to be made [61, 4, 39].\nFirst, at any level of the code, the development of the discrete adjoint model can be checked by appling\nthe following identity\n\u0002\n\u0003\n(AQ)T (AQ) = QT AT (AQ)\n\nwhere Q represents the input to original code, and A represents either a single statement or a subroutine\nWe compare the gradient computed using AD to the gradient computed by Finite Difference (with a\nstep size such that sufficient convergence is obtained). We also compare the gradient computed using AD\nin Forward mode versus the gradient computed using AD in Adjoint mode. We expect those two gradients\nto be identical up to machine precision.\nIf possible, we may use complex numbers [39, 66], to avoid roundoff errors due to computations with\nfinite difference.\n\n6\n\n\f3 Simple example\nWe want to compute the gradient of the function f (a, b, c) = (w \u2212 w0 )2 , where w is obtained using the\nfollowing sequence of statements\nu = sin (ab) + cb2 + a3 c2\n\u0001\nv = exp u2 \u2212 1 + a2\n\u0001\n\u0001\nw = ln v 2 + 1 + cos c2 \u2212 1\n\nThe input vector is denoted by by z = {a, b, c}, intermediate variables u, w, v and output f .\nWe show how the sensitivities with respect to a,b,c, namely\n\u2202f \u2202f \u2202f\n,\n,\n\u2202a \u2202b \u2202c\nare computed in both forward and adjoint mode. For forward mode we also write the expressions in matrix\nnotation, to make it easier to understand how the adjoint mode is constructed.\n\n3.1 Forward (tangent linear) mode\nWe follow the computational flow, and thus we start by getting the sensitivities with respect to the\nintermediate variables, denoted by u\u0307, v\u0307, \u1e87\nu\u0307 =\n\u2202u\n\u2202a\n\u2202u\n\u2202b\n\u2202u\n\u2202c\n\n\u2202u\n\u2202u\n\u2202u\n\u0227 +\n\u010b\n\u1e03 +\n\u2202a\n\u2202b\n\u2202c\n\n= b cos(ab) + 3a2 c2\n= a cos(ab) + 2cb\n= b2 + 2a3 c\n\nHence we obtain\n\nIn matrix notation\n\uf8eb\n\n\u0002\n\u0003\n\u0002\n\u0003\nu\u0307 = b cos(ab) + 3a2 c2 \u0227 + [a cos(ab) + 2cb] \u1e03 + b2 + 2a3 c \u010b\n\n\uf8f6 \uf8eb\n\u0227\n1\n0\n0\n\uf8ec \u1e03 \uf8f7 \uf8ec\n0\n1\n0\n\uf8ec \uf8f7=\uf8ec\n\uf8ed \u010b \uf8f8 \uf8ed\n0\n0\n1\n2 c2 a cos(ab) + 2cb b2 + 2a3 c\nb\ncos(ab)\n+\n3a\nu\u0307\n\nIn a similar way we obtain\n\u0001\nv\u0307 = 2u exp u2 \u2212 1 u\u0307 + 2a\u0227\n\u0001\n2v\n2\n\u1e87 =\nv\u0307\n\u2212\n2c\nsin\nc\n\u2212\n1\n\u010b\nv2 + 1\nf \u0307 = 2 (w \u2212 w0 ) \u1e87\n\n7\n\n\uf8f6\n\n\uf8eb \uf8f6\n\uf8f7 \u0227\n\uf8f7 \uf8ed \u1e03 \uf8f8\n\uf8f8\n\u010b\n\n\fIn matrix notation\n(v\u0307) =\n\n2a 2u exp\n\nu2\n\n\u0001 \u0001\n\u22121\n\n\u0001\n\u22122c sin c2 \u2212 1\n(\u1e87) =\n\u0010 \u0011\nf \u0307\n= (2 (w \u2212 w0 )) (\u1e87)\n\n\u0012\n\n2v\nv2 +1\n\n\u0001\n\n\u0227\nu\u0307\n\u0012\n\n\u0013\n\u010b\nv\u0307\n\n\u0013\n\nTo obtain the required sensitivity with respect to j \u2212 th component of input vectorz, we evaluate the\nabove expressions starting with \u017c which has the j \u2212 th component set to 1 and all the other components\nset to 0.\n\b\nMore specifically, the sensitivities at a given set of variables z (0) = a(0) , b(0) , c(0) are computed by\ncalling the forward mode with the initial value \u017c defined as follows:\n\u0010\n\u0011\n\u2202f \u0010 (0) \u0011\n= f \u0307 computed with \u017c = \u0227, \u1e03, \u010b = (1, 0, 0)\nz\n\u2202a\n\u0010\n\u0011\n\u2202f \u0010 (0) \u0011\n= f \u0307 computed with \u017c = \u0227, \u1e03, \u010b = (0, 1, 0)\nz\n\u2202b\n\u0010\n\u0011\n\u2202f \u0010 (0) \u0011\nz\n= f \u0307 computed with \u017c = \u0227, \u1e03, \u010b = (0, 0, 1)\n\u2202c\n\n3.2 Adjoint (reverse) mode\nTo write the Adjoint, we need to take statements in reverse order. Employing this approach to the\nstatements of the forward mode yields\n\u0001\n(w\u0304) = (2 (w \u2212 w0 )) f \u0304\n\u0013\n\u0012 \u0013\n\u0012\n\u22122c sin(c2 \u2212 1)\nc\u0304\n(w\u0304)\n=\n2v\nv\u0304\nv2 +1\n\u0012 \u0013\n\u0012\n\u0013\n\u0101\n2a\n=\n(v\u0304)\n\u016b\n2u exp(u2 \u2212 1)\n\uf8eb \uf8f6\n\uf8eb \uf8f6\n\uf8eb\n\uf8f6\n\u0101\n2\n2\n\u0101\n1 0 0 b cos(ab) + 3a c\n\uf8ec b\u0304 \uf8f7\n\uf8ed b\u0304 \uf8f8 = \uf8ed 0 1 0 a cos(ab) + 2cb \uf8f8 \uf8ec \uf8f7\n\uf8ed c\u0304 \uf8f8\nc\u0304\n0 0 1\nb2 + 2a3 c\n\u016b\nThus the adjoint (reverse) mode is constructed using the following sequence of statements\nw\u0304 = 2 (w \u2212 w0 ) f \u0304\n2\n\nc\u0304 = \u22122c sin(c \u2212 1)w\u0304\n2v\nw\u0304\nv\u0304 =\n2\nv +1\n\u0101 = 2av\u0304\n\u016b = 2u exp(u2 \u2212 1)v\u0304\n\u0101 = \u0101 + (b cos(ab) + 3a2 c2 )\u016b\nb\u0304 = b\u0304 + (a cos(ab) + 2cb)\u016b\nc\u0304 = c\u0304 + (b2 + 2a3 )c\u016b\n\n8\n\n(3.1)\n\n\fWe can compute all 3 sensitivities of function f with respect to a, b, c by a single application of the\nadjoint mode (3.1), with starting point z\u0304 = 1. More specifically, we have\n\u2202f \u0010 (0) \u0011\n= \u0101 computed with \u0101 = 1\nz\n\u2202a\n\u2202f \u0010 (0) \u0011\n= b\u0304 computed with b\u0304 = 1\nz\n\u2202b\n\u0011\n\u0010\n\u2202f (0)\n= c\u0304 computed with c\u0304 = 1\nz\n\u2202c\nThe reader is encouraged to verify that the gradient computed via the Forward mode or, respectively,\nthe Reverse mode has identical value.\n\n4 Example in PDE solver framework\nThe following PDE is considered for u(t, x), on the spatial domainA \u2264 x \u2264 B\n\uf8f1\n\u2202u\n\uf8f4\n= \u2202u\n\uf8f4\n\u2202t\n\u2202x\n\uf8f4\n\uf8f4\n\uf8f2u(0, x) = u (x)\n0\n\uf8f4u(t, A) = f (t)\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3u(t, B) = g(t)\n\n\b\nWe discretize the PDE in space and time, for a spatial grid {xj }and time grid T k .We use notation\nof superscript for time index and subscript for spatial index. For simplicity of exposition, we discretize\nusing a central difference scheme in space, a first order scheme in time, and we consider that both spatial\ngrid and, respective, temporal grid are constant, with \u2206x and \u2206t\nuk+1\n= ukj +\nj\nWe denote by c the ratio\n\n\u2206t\n2\u2206x .\n\n\u0011\n\u2206t \u0010 k\nuj+1 \u2212 2ukj + ujk\u22121\n2\u2206x\n\nWith that, and incorporating the boundary conditions, we have\n\nuk+1\n= f (T k+1 ) , fk+1\n1\n\u0011\n\u0010\nk\u22121\nk\nk\nk\nuk+1\n=\nu\n+\nc\nu\n\u2212\n2u\n+\nu\nj\nj+1\nj\nj\nj\n\nj = 2...N\n\n= g(T k+1 ) , f gk+1\nuk+1\nN\n\n\u00112\nPN \u0010 M\n, with Yj desired values to get at time T\nj=1 uj \u2212 Yj\nn o\nWe want to compute sensitivities of F with respect to the discretized initial condition u0j ,where\nWe want to minimize the difference F ,\n\nu0j , u0 (xj )\n\n4.1 Forward (tangent linear) mode\nThe tangent linear approach has the expression\n\u0011\n\u0010\nk\nk\nk\nu\u0307k+1\n+\nc\nu\u0307\n+\nu\u0307\n=\n(1\n\u2212\n2c)\nu\u0307\nj\nj+1\nj\u22121\nj\n\n9\n\nj = 2...N\n\n\fIn matrix notation\n\uf8eb k+1\nu\u03071\n\uf8ec ...\n\uf8ec\n\uf8ec u\u0307k+1\n\uf8ec j\u22121\n\uf8ec k+1\n\uf8ec u\u0307j\n\uf8ec k+1\n\uf8ec u\u0307j+1\n\uf8ec\n\uf8ed ...\nu\u0307k+1\nN\n\n\uf8f6\n\n\uf8eb\n\n\uf8f7 \uf8ec\n\uf8f7 \uf8ec\n\uf8f7 \uf8ec\n\uf8f7 \uf8ec\n\uf8f7 \uf8ec\n\uf8f7=\uf8ec\n\uf8f7 \uf8ec\n\uf8f7 \uf8ec\n\uf8f7 \uf8ed\n\uf8f8\n\n0\nc\n...\n...\n...\n...\n0\n\n...\n1 \u2212 2c\n...\n0\n...\n...\n...\n\n...\nc\n...\nc\n...\n...\n...\n\n...\n...\n...\n1 \u2212 2c\n...\n...\n...\n\n...\n...\n...\nc\n...\nc\n...\n\n...\n...\n...\n...\n...\n1 \u2212 2c\n...\n\n0\n...\n...\n...\n...\nc\n0\n\n\uf8f6\uf8eb\n\uf8f7\uf8ec\n\uf8f7\uf8ec\n\uf8f7\uf8ec\n\uf8f7\uf8ec\n\uf8f7\uf8ec\n\uf8f7\uf8ec\n\uf8f7\uf8ec\n\uf8f7\uf8ec\n\uf8f8\uf8ed\n\nu\u0307k1\n...\nu\u0307kj\u22121\nu\u0307kj\nu\u0307kj+1\n...\nu\u0307kN\n\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\n(4.1)\n\nThe last step is\n\u0001 M\n\u0001 M\n\u0001 M\nM\nM\n\u1e1e = 2 uM\n1 \u2212 Y1 u\u03071 + * * * + 2 uj \u2212 Yj u\u0307j + * * * + 2 uN \u2212 YN u\u0307N\n\nIn matrix notation\n\n\u0010 \u0011 \u0010\n\u0001\n...\n\u1e1e = 2 uM\n1 \u2212 Y1\n\n2\n\n\u0010\n\nuM\nj\n\n\u2212 Yj\n\n\u0011\n\n...\n\n2\n\nuM\nN\n\n\uf8eb\n\n\uf8ec\n\u0001 \u0011\uf8ec\n\uf8ec\n\u2212 YN\n\uf8ec\n\uf8ed\n\nu\u0307M\n1\n...\nu\u0307M\nj\n...\nu\u0307M\nN\n\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\n(4.2)\n\n4.2 Adjoint (reverse) mode\nWe go backwards. and the starting point is (4.2). The corresponding adjoint statements are\n\u0001\n\u016bk1 = 2 uM\n1 \u2212 Y1\n... ... ...\n\n\u016bM\nj\n\n=\n\n\u0001\n2 uM\nj \u2212 Yj F\u0304\n\n... ... ...\n\u016bM\nN\n\n=\n\n\u0001\n2 uM\nN \u2212 YN F\u0304\n\nThen we take the transpose of the matrix operator in (4.1). The corresponding adjoint statements are,\nfor k = M, M \u2212 1, . . . , 1, 0\n\u016bk1\n\n=\n\nc\u016bk+1\n2\n\n\u016bk2\n\n=\n\n(1 \u2212 2c)\u016bk+1\n+ c\u016bk+1\n2\n3\n\n... ... ...\n\u016bkj\n\n=\n\nk+1\nc\u016bk+1\n+ c\u016bk+1\nj\u22121 + (1 \u2212 2c)\u016bj\nj+1\n\n... ... ...\n\u016bkN \u22121\n\u016bkN\n\n=\n\nk+1\nc\u016bk+1\nN \u22122 + (1 \u2212 2c)\u016bN \u22121\n\n=\n\nc\u016bk+1\nN\n\nThe required sensitivities are given by \u016b0j , for j = 1...N\n\n10\n\n\f5 Example in Monte Carlo framework (evolution of SDE)\nWe consider the SDE for a N-dimensional X vector\ndX = a(X, t)dt + \u03c3(X, t)dW\n\u0001\nThe initial value for X, at time t=0, is denoted byX 0 = X1 (T 0 , ..., XN (T 0 ))\nFor simplicity of exposition, we consider that we discretize it using Euler-Maruyama scheme, with time\npoints T k , k = 1...M and T 0 = 0\nX(T k+1 ) \u2212 X(T k ) = a(X(T k ), T k )\u2206t + \u03c3(X(T k ), T k )\u2206W k\n\u2206T k = T k+1 \u2212 T k\np\n\u2206W k = \u03b5 * T k+1 \u2212 T k\n\nwhere the random number is chosen from N (0,1)\nWe can recast this discretization scheme into the following expression\nX(T k+1 ) = \u03a6(X(T k ), \u0398)\n\n(5.1)\n\nwhere \u03a6 may also depend on a set of model parameters \u0398 = {\u03b81 , ..., \u03b8P }\nWe also consider that we have to price an option with payoff G(X(T ))\nWe want to compute price sensitivities (\"Greeks\") with respect to X 0 and, respectively, to the model\nparameters \u0398\n\n5.1 Forward (tangent linear) mode\nWe consider first the sensitivities with respect to initial conditions\nN\n\nN\n\nj=1\n\nj=1\n\nX \u2202G(X(T ))\n\u2202G(X(T )) X \u2202G(X(T )) \u2202Xj (T )\n=\n=\n\u2206ij (T 0 , T )\n\u2202Xi (T 0 )\n\u2202Xj (T ) \u2202Xi (T 0 )\n\u2202Xj (T )\n\n(5.2)\n\n\u2202X (T k )\n\nwhere we have used notation \u2206ij (T 0 , T k ) , \u2202Xji (T 0 )\nWe rewrite (5.2) in vector matrix notation\n\u0014\n\u0014\n\u0015\n\u0015\n\u2202G(X(T )) T\n\u2202G(X(T )) T\n=\n* \u2206(T 0 , T )\n\u2202X(T 0 )\n\u2202X(T )\nFor simplicity we write the previous relationship in the following format, using the fact that T M = T\n\u0015T \u0014\n\u0015T\n\u0014\n\u2202G\n\u2202G\n[0] =\n[M ] * \u2206[M ]\n\u2202X\n\u2202X\nwhere the superscript T denotes the transpose\nDifferentiating (5.1) yields\n\u0001\n\u0001\n\u2202X T k+1\n\u2202\u03a6 X(T k ), \u0398\n=\n\u2202X (T k )\n\u2202X (T k )\n\u0001\n\u0001\n\u0001\n\u2202X T k+1\n\u2202X T k+1 \u2202X T k\n\u21d2\n=\n\u2202X (T 0 )\n\u2202X (T k ) \u2202X (T 0 )\n\n(5.3)\n=\n\n11\n\n\u2202\u03a6(X(T k ),\u0398) \u2202X (T k )\n\u2202X(T 0 )\n\u2202X (T k )\n\n\u0001\n\u2202X T k\n= D[k]\n\u2202X (T 0 )\n\n\fwhere D[k] ,\n\n\u2202\u03a6(X(T k ),\u0398)\n\u2202X (T k )\n\nWe rewrite (5.3) as\n\u2206[k + 1] = D[k] * \u2206[k]\nThen we have an iterative process\n\u0014\n\u0014\n\u0014\n\u0015T\n\u0015T\n\u0015T\n\u2202G\n\u2202G\n\u2202G\n=\n[0]\n[M ] * \u2206[M ] =\n[M ] * D[M \u2212 1] * \u2206[M \u2212 1]\n\u2202X\n\u2202X\n\u2202X\n\u0014\n\u0015T\n\u2202G\n=\n[M ] * D[M \u2212 1] * D[M \u2212 2] * \u2206[M \u2212 2] = . . .\n\u2202X\n\u0014\n\u0015T\n\u2202G\n=\n[M ] * D[M \u2212 1] * * * * D[1] * \u2206[1]\n\u2202X\n\u0014\n\u0015T\n\u2202G\n=\n[M ] * D[M \u2212 1] * * * * D[0] * \u2206[0]\n\u2202X\n\n(5.4)\n\nThe matrix \u2206[0] is the identity matrix, since\n\n\u2206[0] =\n\n\u0012\n\n\u2202Xj (T 0 )\n\u2202Xk (T 0 )\n\n\u0013\n\n\uf8eb\n\njk\n\n1\n\uf8ec ***\n=\uf8ec\n\uf8ed ***\n0\n\n***\n***\n***\n***\n\n***\n***\n1\n0\n\n\uf8f6\n0\n*** \uf8f7\n\uf8f7\n0 \uf8f8\n1\n\n(5.5)\n\nThe iterations in (5.4) employ matrix-matrix product. We will see later that the adjoint mode will\ninvolve matrix-vector product instead, which will offer important computational savings.\nNow we move to the sensitivities with respect to model parameters\nN\n\nN\n\nj=1\n\nj=1\n\n\u2202G(X(T )) X \u2202G(X(T )) \u2202Xj (T ) X \u2202G(X(T ))\n=\n=\n\u03a8jp (T )\n\u2202\u03b8p\n\u2202Xj (T )\n\u2202\u03b8p\n\u2202Xj (T )\n\n(5.6)\n\n\u2202X (T k )\n\nj\nwhere we have used notation \u03a8jp (T k ) , \u2202\u03b8\np\nDifferentiating (5.1) with respect to parameters yields\n\u0001\n\u0001\n\u0001\n\u2202X T k+1\n\u2202\u03a6 X(T k ), \u0398 \u2202X(T k ) \u2202\u03a6 X(T k ), \u0398\n=\n+\n\u2202\u0398\n\u2202X (T k )\n\u2202\u0398\n\u2202\u0398\n\nMaking the notations D[k] ,\n\n\u2202\u03a6(X(T k ),\u0398)\n\u2202X (T k )\n\nand B[k] ,\n\n\u2202\u03a6(X(T k ),\u0398)\n\u2202\u0398\n\n(5.7)\n\nfor the corresponding matrices, we\n\nrewrite (5.7) as\n\u03a8[k + 1] = D[k] * \u03a8[k] + B[k]\nThen we have an iterative process\n\u0014\n\u0014\n\u0015\n\u0015\n\u0015\n\u0014\n\u2202G(X(T )) T\n\u2202G(X(T )) T\n\u2202G(X(T )) T\n=\n* \u03a8[M ] =\n* (D[M \u2212 1] * \u03a8[M \u2212 1] + B[M \u2212 1])\n\u2202\u0398\n\u2202X(T )\n\u2202X(T )\n\u0014\n\u0015\n\u2202G(X(T )) T\n=\n* (D[M \u2212 1] * (D[M \u2212 1] * \u03a8[M \u2212 1] + B[M \u2212 1]) + B[M \u2212 1]) = * * * =\n\u2202X(T )\n\u0015\n\u0014\n\u2202G(X(T )) T\n* (B[M \u2212 1] + D[M \u2212 1] * B[N \u2212 2] + * * * + D[M \u2212 1] * * * D[1] * B[0])\n=\n\u2202X(T )\n\n12\n\n(5.8)\n\n\f5.2 Adjoint (reverse) mode\nWe construct first the adjoint for computing the sensitivities with respect to initial condition. We start\nwith the adjoint equation\nV [k] = D T [k] * V [k + 1]\n(5.9)\nwhere the superscript T denotes the transpose\nIn a recursive manner we obtain\nV [0] = D T [0] * V [1] = D T [0] * D T [1] * V [2] = * * * = D T [0] * D T [1] * * * * D T [M \u2212 1] * V [M ]\n\n(5.10)\n\nBy taking transpose of (5.10) we have\nV T [0] = V T [M ] * D[M \u2212 1] * D[M \u2212 2] * * * * D[1] * D[0]\n\u0001T\n\u2202G\n[M ] and we combine (5.11) and (5.4), which gives\nWe set V [M ] to the value \u2202X\n\n(5.11)\n\n\u2202G\n[0] = V T [0] * \u2206[0]\n\u2202X\n\nBut the matrix \u2206[0] is the identity matrix, according to 5.5\n\u2202G\nThus we obtain the sensitivities with respect to initial conditions, namely \u2202X\n[0] by applying the recursive\nT\nrelationship (5.11) to find V [0]. We note that the product in this iterative process is of the matrix-vector\ntype, not matrix-matrix as it was for tangent linear mode\nNow we move to sensitivities with respect to model parameters\nWe use again the adjoint equation (5.9). With same initial condition for V [M ] , namely V [M ] =\n\u0001T\n\u2202G\n[M\n]\n, and evolving from time T M \u2212k to time T M we have that\n\u2202X\n\u2202G(X(T ))\n* D[M \u2212 1] * D[M \u2212 2] * * * * D[M \u2212 k] * B[M \u2212 k \u2212 1] = V [M \u2212 k]\n\u2202\u0398\n\nThus we can rewrite (5.8) as\nM\n\u22121\nX\n\u2202G(X(T ))\nV T [k + 1] * B[k]\n=\n\u2202\u0398\n\n(5.12)\n\nk=0\n\nThe values of adjoint vectors V [k] were computed as part of the adjoint approach for sensitivities with\nrespect to initial conditions.\nThe values of B[k] can be precomputed. We may be able to compute them analytically (e.g., if the\nparameters are volatilities and vol surface is assumed to have a certain parametric representation, such as\ncubic spline), otherwise we can employ the adjoint procedure.\n\n6 Example in Monte Carlo framework (copula)\nWe follow a procedure similar to the one described in [20]\nLet us describe the setup. We have a state vector X of N components, an instrument with a payoff\nP (X), and the probability distribution Q(X) according to which the components of X are distributed.\nFor simplicity we consider a N-dimensional Gaussian copula to model the co-dependence between the\ncomponents of the state vector, namely a joint cumulative density function of the form\n\u03a6N [\u03a61 (\u03c61 (X1 ) , * * * , \u03c6N (XN )) ; \u03c1]\n\n13\n\n\fwhere \u03a6N [Z1 , * * * , ZN ; \u03c1] is a N-dimensional multivariate Gaussian distribution with zero mean and\na correlation matrix \u03c1, \u03a6\u22121 is the inverse of the standard normal cumulative distribution and \u03c6i (Xi ),\ni=1...N are the marginal distributions of the underlying components.\nThe option value is obtained through averaging of Monte Carlo sampling\nV =\n\n1\nNM C\n\nN\nMC\nX\nj=1\n\n\u0011\n\u0010\nP X(i)\n\nwhere the superscript (i) denotes the i\u2212th Monte Carlo path.\nThe sampling of the N jointly distributed normal random variables (Z1 , Z2 , * * * , ZN ) can be done using\nseveral approaches, such as Cholesky factorization, spectral or singular value decomposition. We select\nCholesky factorization\n\u03c1 = C * C T because it will enable us to use an existing procedure for its adjoint. Starting from a vector\n\u1e90 of independent standard normal variables, we obtain a vector Zof jointly normal random variables\ndistributed according to \u03a6N [Z1 , * * * , ZN ; \u03c1] through the product\nZ = C * \u1e90\nWe also use the following:\n\u2022 if Zi is sampled from a standard normal distribution then \u03a6 (Zi ) is in U [0,1]\n\u2022 if Xi is distributed according to marginal distribution \u03c6i , then \u03c6 (Xi ) is in U [0,1]\nThen Xi = \u03c6\u22121 (\u03a6 (Zi )) is distributed according to marginal distribution \u03c6i\nTherefore we have the following algorithm [20]\n1. Generate a vector \u039e of independent standard normal variables\n2. Correlate the components through Z = C * \u039e\n3. Set Ui = \u03a6 (Zi ) , i = 1...N\n\u22121 (U ) , i = 1...N\n4. Set Xi = \u03c6\u22121\ni\ni (\u03a6 (Zi )) = \u03c6\n\n5. Calculate the payoff estimator P (X1 , ..., XN )\nWe now show how sensitivities can be computed in this setup.\nThe correlation matrix is an input to the procedure, so we may compute correlation risk, i.e., sensitivities\nof the price with respect to entries in the correlation matrix\nThese marginal distributions may be obtained from a set of N M ARG discrete call, put, digital call\nand digital put values (which may be given as corresponding implied volatilities).We may also compute\nsensitivities with respect to those inputs, denoted by \u031fj , j = 1...N M ARG\n\n6.1 Forward (tangent linear) mode\nAssuming that the payoff function is regular enough (e.g., Lipschitz continuous) the standard pathwise\ndifferentiation corresponds to forward (tangent linear) mode. The differentiation is applied to steps 1-5 in\nthe above algorithm. We need to pay attention if any given step is dependent (implicitly or explicitly) on\nthe input parameters with respect to which we want to compute the sensitivities. Step 1 stays the same.\n\n14\n\n\fStep 2 and 3 are differentiated if we want correlation risk, otherwise they remain unchanged. Steps 4 and\n5 are differentiated regardless which of the two types of sensitivities of sensitivities we want to compute.\nTo differentiate Step 4 we start from Xi = \u03c6\u22121\ni (Ui ) \u21d2 Ui = \u03c6i (Xi ).\nDifferentiating the last equality gives the following formula, if we have the propagated derivative U\u0307i of\nthe variable Ui (i.e., we compute the correlation risk)\nU\u0307i =\n\n\u2202\u03c6i\n(Xi ) \u1e8ai \u21d2 \u1e8ai =\n\u2202x\n\nU\u0307i\n\u2202\u03c6i\n\u2202x\n\n(Xi )\n\nIf we need to compute sensitivities with respect to \u031fj , then differentiating the same equality gives\n\u1e8ai =\n\n\u031f\n \u0307j\n\u2202\u03c6i\n\u2202x\n\n(Xi )\n\nWe now present the algorithm for tangent linear mode. For ease of presentation we write explicitly the\nalgorithm only for the case of correlation sensitivities.\nWe assume that we want to compute the sensitivity with respect to entry \u03c1lk of the correlation matrix\n1. Generate a vector \u039e of independent standard normal variables\n2. Calculate \u017b = C \u0307lk * \u039e\u0307, where C \u0307lk is the sensitivity of Cholesky factor C with respect to \u03c1lk\n3. Set U\u0307i =\n4. Set \u1e8ai =\n\n\u2202\u03a6\n\u2202x\n\n(Zi ) , i = 1...N\n\nU\u0307i\n\u2202\u03c6i\n(Xi )\n\u2202x\n\n5. Calculate \u1e56 =\n\n, i = 1...N\n\nPN\n\n\u2202P\ni=1 \u2202Xi\n\n(Xi ) * \u1e8ai\n\ni\nWe note that the derivative of the marginal distribution, denoted by \u2202\u03c6\n\u2202x (Xi ), is the probability density\nfunction associated with the marginal distribution \u03c6i , while the derivative \u2202\u03a6\n\u2202x is the standard normal\nprobability density function\n\n6.2 Adjoint (reverse) mode\nThe adjoint mode consists of the adjoint counterparts for each of the steps in the forward mode, plus the\nadjoint of Cholesky factorization [63]. The computation is done in reverse order\nThe resulting algorithm consists of the 5 steps described above (in the forward mode) plus the following\nsteps corresponding to adjoint counterparts\n1. Calculate the adjoint of the payoff estimator X\u0304k =\n2. Calculate \u016ak =\n\n\u2202\u03c6i\n\u2202x\n\nX\u0304k\n\n(\u03c6\u22121\nk (Uk ))\n\n\u2202P\n\u2202Xk (Xk ),\n\nk = 1...N\n\n, k = 1...N\n\n3. Calculate Z\u0304k = \u016ak \u2202\u03a6\n\u2202x (Zk ) , k = 1...N\n4. Calculate C\u0304 = Z\u0304 * \u039eT\n\u0001\nThe matrix C\u0304 = C\u0304ij i,j=1..N obtained at the end of the procedure contains all derivatives of payoff\nestimator with respect to entries \u033aij of the correlation matrix. We can see that all these sensitivities were\ncomputed by running the adjoint mode only once, as opposed to the forward (tangent linear) mode, which\nhad to be run separately for each entry in the correlation matrix (with a total number of runs of N 2 )\n\n15\n\n\f7 Example in calibration/optimization framework\nLet us consider that we have the same SDE as in the previous chapter. We want to minimize a cost\nfunctional of the type\nX\nF =\n(M odelP rice[j] \u2212 M arketP rice[j])2\nwith variables to be optimized being given by model parameters \u0398 = {\u03b81 , ..., \u03b8P }\nThe gradient of the cost functional with respect to the model parameters would have the expression\n\uf8f6\n\uf8f6 \uf8eb\n\uf8eb\n***\n***\nP\n\u2202(M odelP rice[j]) \uf8f7\n\uf8ed \u2202F \uf8f8 = \uf8ec\n\uf8ed 2 (M odelP rice[j] \u2212 M arketP rice[j])\n\uf8f8\n\u2202\u03b8k\n\u2202\u03b8k\n***\n***\n\nrice[j])\nin a similar way to (5.12)\nThe adjoint procedure enables us to compute \u2202(M odelP\n\u2202\u03b8k\nWe note here that the above procedure assumes implicitly the existence of the gradient of the functional.\nIt is our experience [47]that the discrete adjoint can still be applied for cases such gradient doe not exist;\nin those cases the numerical adjoint code will provide us not the gradient, but rather subgradients. Consequently, one will have to employ optimization algorithms that are especially designed to use subgradients\ninstead of gradient\n\n8 Computational finance literature on adjoint and AD\nIn the last several years quite a few papers were added to the literature on adjoint/AD applied to computational finance [11, 22, 20, 21, 24, 23, 32, 30, 31, 41, 50, 48, 49, 54, 52, 55, 56, 65, 3, 9, 18, 67] . For\nselected papers we give an overview in the following sections\n\n8.1 Computation of Greeks\nA major part of the literature related to this topic is due to Giles, Capriotti and, respectively, Joshi (and\ntheir collaborators).\nThe seminal paper of [41] applied the adjoint approach to the computation of Greeks (Delta and Vega)\nfor swaptions using pathwise differentiation method in the context of LIBOR Market Model (LMM).\nThe portfolio considered had portfolio had 15 swaptions all expiring at the same time, N periods in the\nfuture, involving payments/rates over an additional 40 periods in the future. Interested in computing\nDeltas, sensitivity to initial N+40 forward rates, and Vegas, sensitivity to initial N+40 volatilities. The\ncomputational efficiency was improved by a factor of 10 when the forward method was employed instead of\nfinite differences. Then the adjoint approach reduced the cost by several orders of magnitude, compared to\nthe forward approach: for N=80 periods, by a factor of 5 for computation of Deltas only, and respectively\nby a factor of 25 for computation of both Deltas and Vegas.\nThis was extended in [58] to the pricing of Bermudan-style derivatives. For testing they have used five\n1xM receiver Bermudan swaptions (M = 4, 8, 12, 16, 20) with half-year constant tenor distances. The\nspeedup was as follows (for M=20): by a factor of 4 for only Deltas, and by factor of 10 for both Deltas\nand Vegas.\nThe pathwise approach is not applicable when the financial payoff function is not differentiable. To\naddress these limitations, a combination the adjoint pathwise approach for the stochastic path evolution\nwith likelihood ratio method (LRM) for the payoff evaluation is presented in [36, 38]. This combination\n\n16\n\n\fis termed \"Vibrato\" Monte Carlo. The Oxford English Dictionary describes \"vibrato\" as \"a rapid slight\nvariation in pitch in singing or playing some musical instruments\". The analogy to Monte Carlo methods\nis the following; whereas a path simulation in a standard Monte Carlo calculation produces a precise value\nfor the output values from the underlying stochastic process, in the vibrato Monte Carlo approach the\noutput values have a narrow probability distribution.\nApplying concepts of adjoint/AD for correlation Greeks were considered in [20]. The pricing of an\ninstrument based on N underlyings is done with Monte Carlo within a Gaussian copula framework, which\nconnects the marginal distributions of the underlying factors. The sampling of the N jointly normal\nrandom variables is efficiently implemented by means of a Cholesky factorization of the correlation matrix.\nCorrelation risk is obtained in a highly efficient way by implementing the pathwise differentiation approach\nin conjunction with AD, using the adjoint of Cholesky factorization. Numerical tests on basket default\noptions shows a speedup of 1-2 orders of magnitude (100 times faster than bumping for 20 names, and\n500 times for 40 names).\nExamples of pathwise differentiation combined with AD are shown in [18]. For a basket option priced\nin a multidimensional lognormal model, the savings are already larger than one order of magnitude for\nmedium sized baskets (N=10). For \"Best of\" Asian options in a local volatility setting, the savings are\nreported to over one order of magnitude for a relatively small number (N=12) of underlying assets.\nAdjoint algorithmic differentiation can be used to implement efficiently the calculation of counterparty\ncredit risk [22]. Numerical results show a reduction by more than two orders of magnitude in the computational cost of Credit Valuation Adjustment (CVA). The example considered is a portfolio of 5 swaps\nreferencing distinct commodities Futures with monthly expiries with a fairly typical trade horizon of 5\nyears, the CVA bears non-trivial risk to over 600 parameters: 300 Futures prices, and at the money volatilities, (say) 10 points on the zero rate curve, and 10 points on the Credit Default Spread(CDS) curve of\nthe counterparty used to calibrate the transition probabilities of the rating transition model. required\nfor the calculation of the CVA. The computational time for CVA sensitivities is less than 4 times the\ncomputational time time spent for the computation of the CVA alone, as predicted by AD theory. As a\nresult, even for this very simple application, adjoint/AD produces risk over 150 times faster than finite\ndifferences: for a CVA evaluation taking 10 seconds, adjoint approach produces the full set of sensitivities\nin less than 40 seconds, while finite differences require approximately 1 hour and 40 minutes.\nIn the framework of co-terminal swap-rate market model [51] presented an efficient algorithm to implement the adjoint method that computes sensitivities of an interest rate derivative (IRD) with respect to\ndifferent underlying rates, yielding a speedup of a factor of 15 for Deltas of a Bermudan callable swaption\nwith rates driven by a 5-factor Brownian motion. They show a total computational order of the adjoint\napproach of order O(nF ), with n the number of co-terminal swap rates, assumed to be driven by numberF\nBrownian motions, compared to the order of standard pathwise method which is O(n3 ). It was extended\nto Vegas in [55], in the context of three displaced diffusions swap-rate market models. A slight modification of the method of computing Deltas in generic market models will compute market Vegas with order\nO(nF ) per step. Numerical results for the considered tests show that computational cots for Deltas and\nVegas will be not more than 1.5 times the computational cots of only Deltas. CMS spread options in\nthe displaced-diffusion co-initial swap market model are priced and hedged in [53]. The evolution of the\nswap-rates is based on an efficient two-factor predictor-corrector(PC) scheme under an annuity measure.\nPricing options using the new method takes extremely low computational times compared with traditional\nMonte Carlo simulations. The additional time for computing model Greeks using the adjoint method can\nbe measured in milliseconds. Mostly importantly, the enormous reduction in computational times does\nnot come at a cost of poorer accuracy. In fact, the prices and Greeks produced by the new method are\nsufficiently close to those produced by a one-step two-factor PC scheme with 65,535 paths.\n\n17\n\n\fIn the framework of the displaced-diffusion LIBOR market model, [49] compared the discretization\nbias obtained when computing Greeks with pathwise adjoint method for the iterative predictor-corrector\nand Glasserman-Zhao drift approximations in the spot measure to those obtained under the log-Euler\nand predictor-corrector approximations by performing tests with interest rate caplets and cancellable\nreceiver swaps. They have found the iterative predictor-corrector method to be more accurate and slightly\nfaster than the predictor-corrector method, the Glasserman-Zhao method to be relatively fast but highly\ninconsistent, and the log-Euler method to be reasonably accurate but only at low volatilities.\nIn the framework of the cross-currency displaced-diffusion LIBOR market model, [11] employs adjoint\ntechniques to compute Greeks for two commonly traded cross-currency exotic interest rate derivatives:\ncross-currency swaps (CCS) and power reverse dual currency (PRDC) swaps. They measure the computational times relative to the basic implementation of the crosscurrency LIBOR market model, that is\nwith standard least squares regression used to compute the exercise strategy. It was reported that, using\nthe adjoint pathwise method, the computational time for all the Deltas and Vega for each step and the\nexchange Vega is only slightly larger compared to the computational time required to compute one Delta\nusing finite differences.\nAdjoint approach was applied for computation of higher order Greeks (such as Gammas) in [52] and [54],\nwhere they show that the Gamma matrix (i.e. the Hessian) can be computed in AM + B times the number\nof operations where M is the maximum number of state variables required to compute the function, and\nA,B are constants that only depend on the set of floating point operations allowed.. In the first paper\nnumerical results demonstrate that the computing all n(n+1)/2 Gammas for Bermudan cancellable swaps\nin the LMM takes roughly n/3 times as long as computing the price. In the second paper numerical results\nare shown for an equity tranche of a synthetic CDO with 125 names. To compute all the 125 finitedifference Deltas, the computational time for doing so will be 125 times of the original pricing time (if\nwe use forward difference estimates). However, it only takes up to 1.49 times of the original pricing time\nif we use algorithmic differentiation. There are 7,875 Gammas to estimate. If one uses central difference\nestimates, it will take 31,250 times of the original pricing time to achieve this. It only takes up to 195\ntimes to estimate all the Gammas with algorithmic Hessian methods.\nIn the framework of Markov-functional models, [30] demonstrates how the adjoint PDE method can\nbe used to compute Greeks. This paper belongs to the ContAdj category. It obtains an adjoint PDE,\nwhich is solved once to obtain all of the model Greeks. In the particular case of a separable LIBOR\nMarkov-functional model the price and 20 Deltas, 40 mapping Vegas and 20 skew sensitivities (80 Greeks\nin total) of a 10-year product are computed in approximately the same time it takes to compute a single\nGreek using finite difference. The instruments considered in the paper were: interest rate cap, cancellable\ninverse floater swap and callable Bermudan swaption.\nIn the framework of Heston model, the algorithmic differentiation approach was considered in [24]\nto compute the first and the second order price sensitivities.. Issues related to the applicability of the\npathwise method are discussed in this paper as most existing numerical schemes are not Lipschitz in\nmodel inputs. While AD/adjoint approach is usually considered for primarily providing a very substantial\nreduction in computational time of the Greeks, this paper also shows how its other major characteristic,\nnamely accuracy, can be extremely relevant in some cases. Computing price sensitivities is done using\nthe Lognormal (LN) scheme, the Quadratic- Exponential (QE) scheme, the Double Gamma (DG) scheme\nand the Integrated Double gamma (IDG) scheme. Numerical tests show that the sample means of price\nsensitivities obtained using the Lognormal scheme and the Quadratic-Exponential scheme can be highly\nskewed and have fat-tailed distribution while price sensitivities obtained using the Integrated Double\nGamma scheme and the Double Gamma scheme remain stable.\nThe adjoint/AD approach is also mentioned in the very recent \"opus magna\" on interest rate derivatives\n\n18\n\n\f[5, 6, 7], written by 2 well-known practitioners.\n\n8.2 Calibration\nTo the best of our knowledge, [3] was the first time that an adjoint approach was presented for calibration\nin the framework of computational finance\nIn [57] adjoint methods are employed to speed up the Monte Carlo-based calibration of financial market\nmodels. They derive the associated adjoint equation /(and thus are in ContAdj category) and propose its\napplication in combination with a multi-layer method. They calibrate two models: the Heston model is\nused to confirm the theoretical viability of the proposed approach, while for a lognormal variance model\nthe adjoint-based Monte Carlo algorithm reduces computation time from more than three hours (for finite\ndifference approximation of the gradient)to less than ten minutes\nThe adjoint approach is employed in [59, 60] to construct a volatility surface and for calibration of local\nstochastic volatility models. The fitting of market bid/ask-quotes is not a trivial task, since the quotes\ndiffer in quality as well as quantity over timeslices and strikes, and are furthermore coupled by hundreds\nof arbitrage constraints. Nevertheless, the adjoint approach enabled on the fly fitting (less than 1 second)\nfor real-life situations: a volatility matrix with 17 strikes and 8 maturities.\nAD is applied to calibration of Heston model in[44], through differentiation of vanilla pricing using characteristic functions. It takes advantage of the fact that it can be applied to complex numbers (if they are\n\"equipped\" with auto-differentiation). Applying AD to numerical integration algorithm is straightforward,\nand the resulting algorithm is fast and accurate.\nIn [69] the adjoint approach (of ContAdj type) is applied to calibrate a local volatility model\nCalibration of a local volatility model is also shown in [65]\n\n9 AD and adjoint applied within a generic framework in practitioner\nquant libraries\nAs more financial companies become familiarized with the advantages offered adjoint/AD approach when\napplied to computational finance, this approach will be more and more integrated in their quant libraries.\nVery recent presentations [19, 17, 44, 60, 10] at major practitioner quant conferences indicate that this effort\nis under way at several banks, such as Credit Suisse, Unicredit, Nomura, etc. Within this framework one\nmay imagine a situation where Greeks are computed (using AD techniques) in real time to provide hedging\nwith respect to various risk factors: interest rate, counterparty credit, correlation, model parameters etc.\nDue to complexity of quant libraries, AD tools and special techniques of memory management, checkpointing etc can prove extremely useful, potentially leveraging knowledge acquired during AD applications\nto large software packages in other areas: meteorology, computational fluid dynamics etc where such techniques were employed [43, 25, 45, 4, 39, 15, 8]\n\n9.1 Block architecture for Greeks\n[10] presents a \"Block architecture for Greeks\" , using calculators and allocators. Sensitivities are computed\nfor each block component, including root-finding and optimization routines, trees and latices.\nHere is an example included in the presentation. First some notations: every individual function\n(routine), say f : Rm \u2192 Rn y=f(x)is termed a \"calculator\" function, and a corresponding \"allocator\"\nfunction is defined as\n\u0010 \u0011\n= \u2202V\nGf : Rn \u2192 Rm Gf \u2202V\n\u2202y\n\u2202y *Jf\n19\n\n\fIf we have a sequence of functions f, g, h, then we calculate the Greeks by starting with the end point\n= 1 and working backwards by calling the allocator functions one-by-one in reverse order to the original\norder of the function calls.\nSuppose we have a pricing routine with three subroutines\n\u2202V\n\u2202V\n\n\u2022 f : Interpolates market data onto the relevant grid from input market objects\n\u2022 g : performs some calibration step on the gridded market data\n\u2022 h : core pricer which uses the calibrated parameters\nWe need to write the three risk allocator functions for these three subroutines. Then we feed the starting\npoint \u2202V\n\u2202V = 1 into the pricer's allocator to get back the vector of risks to the calibrated parameters. The\nnext step is to feed this into the calibrator's allocator to get back the vector of risks to the gridded market\ndata. Finally, we feed that risk vector into the interpolator's allocator to get back risk to the original\ninput market objects.\n\n9.2 Real Time Counterparty Risk Management in Monte Carlo\n[19] presents a framework for Real Time Counterparty Risk Management, employing AD to compute\nCredit Valuation Adjustment (CVA), with a reduction in computational time from 100 min (using bump\nand reprice) to 10 seconds, for a a portfolio of 5 commodity swaps over a 5 years horizon (over 600 risks)\n\n10 Additional resources\nAdditional resources include reference books [13, 16, 26, 43], results on matrix differentiation [37, 62, 63],\nthe online community portal for Automatic Differentiation [2], various AD tools such as FASTOPT,\nFASTBAD, ADMAT, FAD, ADOL-C, RAPSODIA, TAPENADE [1], papers on rules to construct the\nadjoint code [43, 12, 34, 35, 28, 29, 14, 3, 64]\n\n11 Conclusion\nWe have presented an overview of adjoint and automatic(algorithmic) differentiation techniques in the\nframework of computational finance, and illustrated the major advantages of this approach for computation\nof sensitivities: great reduction of computational time, improved accuracy, and a systematic process to\nconstruct the corresponding \"adjoint\" code from existing codes\n\nReferences\n[1] Automatic Differentiation tools. http://www.autodiff.org/?module=Tools.\n[2] Community portal for Automatic Differentiation. http://www.autodiff.org.\n[3] Y. Achdou and O. Pironneau. Computational methods for option pricing. SIAM, 2005.\n[4] M. Alexe, O. Roderick, J. Utke, M. Anitescu, P. Hovland, and T. Fanning. Automatic differentiation\nof codes in nuclear engineering applications. Technical report, Mathematics and Computer Science\nDivision. Argonne National Lab, 2009.\n\n20\n\n\f[5] L.B.G. Andersen and V. V. Piterbarg. Interest Rate Modeling Volume I: Foundations and Vanilla\nModels. Atlantic Financial Press, 2010.\n[6] L.B.G. Andersen and V. V. Piterbarg. Interest Rate Modeling Volume II: Term Structure Modeling.\nAtlantic Financial Press, 2010.\n[7] L.B.G. Andersen and V. V. Piterbarg. Interest Rate Modeling Volume III: Products and Risk Management. Atlantic Financial Press, 2010.\n[8] R.A. Bartlett, D.M. Gay, and E.T. Phipps. Automatic Differentiation of C++ codes for large-scale\nscientific computing. In V.N. Alexandrov, G.D. van Albada, P. M. A. Sloot, and J. Dongarra, editors,\nComputational Science \u2013 ICCS 2006, volume 3994 of Lecture Notes in Computer Science, pages 525\u2013\n532, Heidelberg, 2006. Springer.\n[9] H. Bastani and L. Guerrieri. On the application of Automatic Differentiation to the likelihood function\nfor dynamic general equilibrium models. http://www.federalreserve.gov/pubs/ifdp/2008/920/\nifdp920.pdf, 2008.\n[10] M. Baxter. Practical implementation of fast Greeks in your analytics library. In The 6th Fixed Income\nConference, Madrid, September 22-24, 2010.\n[11] C. Beveridge, M. Joshi, and W. M. Wright. Efficient pricing and Greeks in the cross-currency LIBOR\nmarket model. http://ssrn.com/paper=1662229, 2010.\n[12] C. Bischof, B. Lang, and Andre Vehreschild. Automatic differentiation for matlab programs. Proceedings in Applied Mathematics and Mechanics, 2(1):50\u201353, 2003.\n[13] C. H. Bischof, H. M. B\u00fccker, P. Hovland, and U. Naumann. Advances in Automatic Differentiation.\nSpringer, 2008.\n[14] C.H. Bischof. Automatic differentiation, tangent linear models and pseudo-adjoints. In Fran\u00e7oisXavier Le Dimet, editor, High-Performance Computing in the Geosciences, volume 462 of Mathematical and Physical Sciences, pages 59\u201380. Kluwer Academic Publishers, Boston, Mass., 1995.\n[15] C.H. Bischof, H.M. B\u00fccker, B. Lang, A. Rasch, and E. Slusanschi. Efficient and accurate derivatives\nfor a software process chain in airfoil shape optimization. Future Generation Computer Systems,\n21(8):1333\u20131344, 2005.\n[16] H.M Bucker, G. Corliss, P. Hovland, and U. Naumann. Automatic Differentiation: Applications,\nTheory, and Implementations. Springer, 2006.\n[17] L. Capriotti. Adjoint algorithmic differentiation: a new paradigm in risk management. In Quant\nCongress Europe, London, Nov 9-11, November 2010.\n[18] L. Capriotti. Fast Greeks by algorithmic differentiation. Journal of Computational Finance, 14(3):3\u2013\n35, 2011.\n[19] L. Capriotti. Making the calculation of risk through Monte Carlo methods more efficient by using\nadjoint algorithmic differentiation. In Global Derivatives Trading & Risk Management, Paris, April\n11-15, 2011.\n\n21\n\n\f[20] L. Capriotti and M.B. Giles. Fast correlation Greeks by adjoint algorithmic differentiation. RISK,\nMarch, 2010.\n[21] L. Capriotti and M.B. Giles. Algorithmic differentiation: Adjoint Greeks made easy. http://ssrn.\ncom/paper=1801522, 2011.\n[22] L. Capriotti, S. Lee, and M. Peacock. Real time counterparty credit risk management in Monte Carlo.\nhttp://ssrn.com/paper=1824864, 2011.\n[23] J.H. Chan and M. Joshi. Fast Monte-Carlo Greeks for financial products with discontinuous pay-offs.\nhttp://ssrn.com/paper=1500343, 2009.\n[24] J.H. Chan and M. Joshi. First and second order Greeks in the Heston model. http://ssrn.com/\npaper=1718102, 2010.\n[25] I. Charpentier and M. Ghemires. Efficient adjoint derivatives: application to the meteorological model\nMeso-NH. Optim. Methods Software, 13:35\u201363, 2000.\n[26] G Corliss, Christele Faure andAndreas Griewank, and Laurent Hascoet. Automatic Differentiation of\nAlgorithms. Springer, 2002.\n[27] P. Courtier and O. Talagrand. Variational assimilation of meteorological observations with the adjoint\nvorticity equation, Ii, numerical results. Q. J. R. Meteorol. Soc.,, 113:1329\u20131347, 1987.\n[28] P. Cusdin and J.-D. M\u00fcller. Deriving linear and adjoint codes for CFD using Automatic Differentiation. Technical Report QUB-SAE-03-06, QUB School of Aeronautical Engineering, 2003. Submitted\nto AIAA http://www.ea.qub.ac.uk/pcusdin/index.php.\n[29] P. Cusdin and J.-D. M\u00fcller. Generating efficient code with Automatic Differentiation. In European\nCongress on Computational Methods in Applied Sciences and Engineering ECCOMAS, 2004.\n[30] N Denson and Joshi. Fast Greeks for Markov-functional models using adjoint PDE methods. http://\nssrn.com/paper=1618026, 2010.\n[31] N Denson and M Joshi. Fast and accurate Greeks for the LIBOR market model. http://ssrn.com/\npaper=1448333, 2009.\n[32] N Denson and M Joshi. Flaming logs. Wilmott Journal, 1:259\u2013262, 2009.\n[33] F. X. Le Dimet, I. M. Navon, and D. N. Daescu. Second order information in data assimilation.\nMonthly Weather Review, 130(3):629\u2013648, 2002.\n[34] R. Giering and T. Kaminski. Recipes for adjoint code construction. ACM Transactions on Mathematical Software, 24:437\u2013474, 1998.\n[35] R. Giering and T. Kaminski. Recomputations in reverse mode AD, chapter Automatic differentiation\nof algorithms. Springer, 2002.\n[36] M.B. Giles. Monte Carlo evaluation of sensitivities in computational finance. In HERCMA, 2007.\n[37] M.B. Giles. Collected matrix derivative results for forward and reverse mode algorithmic differentiation. In Lecture Notes in Computational Science and Engineering, Volume 64, pages 35\u201344. Springer,\n2008.\n\n22\n\n\f[38] M.B. Giles. Vibrato Monte Carlo sensitivities. In Monte Carlo and Quasi Monte Carlo Methods 2008,\npages 369\u2013382. Springer, 2008.\n[39] M.B. Giles, D. Ghate, and M.C. Duta. Using automatic differentiation for adjoint CFD code development. Technical report, Oxford University Computing Laboratory\u201e 2005.\n[40] M.B. Giles, D. Ghate, and M.C. Duta. Using automatic differentiation for adjoint CFD code development. In Recent Trends in Aerospace Design and Optimization. Tata McGraw-Hill, New Delhi,\n2006.\n[41] M.B. Giles and P. Glasserman. Smoking adjoints: fast Monte Carlo Greeks. RISK, January, 2006.\n[42] M.B. Giles and N.A. Pierce. An introduction to the adjoint approach to design. Flow, Turbulence\nand Control, 65(3-4):393\u2013415, 2000.\n[43] A. Griewank and A. Walther. Evaluating derivatives : principles and techniques of algorithmic differentiation, 2nd edition. SIAM, 2008.\n[44] J Hakala. Auto-differentiation in practice. In Global Derivatives Trading & Risk Management, Paris,\nApril 11-15, 2011.\n[45] L. Hascoet and B. Dauvergne. Adjoints of large simulation codes through automatic differentiation.\nREMN Revue Europeenne de Mecanique Numerique / European Journal of Computational Mechanics,\npages 63\u201386, 2008.\n[46] C. Homescu and I. M. Navon. Numerical and theoretical considerations for sensitivity calculation of\ndiscontinuous flow. Systems & Control Letters on Optimization and Control of Distributed Systems,\n48(3):253\u2013260, 2003.\n[47] C. Homescu and I. M. Navon. Optimal control of flow with discontinuities. Journal of Computational\nPhysics, 187:660\u2013682, 2003.\n[48] M Joshi and D Pitt. Fast sensitivity computations for Monte Carlo valuation of pension funds.\nhttp://ssrn.com/paper=1520770, 2010.\n[49] M Joshi and A Wiguna. Accelerating pathwise Greeks in the LIBOR market model. http://ssrn.\ncom/paper=1768409, 2011.\n[50] M Joshi and C Yang. Efficient Greek estimation in generic market models. http://ssrn.com/\npaper=1437847, 2009.\n[51] M Joshi and C Yang. Fast Delta computations in the swap-rate market model. http://ssrn.com/\npaper=1401094, 2009.\n[52] M Joshi and C Yang. Algorithmic Hessians and the fast computation of cross-Gamma risk. http://\nssrn.com/paper=1626547, 2010.\n[53] M. Joshi and C. Yang. Fast and accurate pricing and hedging of long-dated CMS spread options.\nInternational Journal of Theoretical and Applied Finance, 13(6):839\u2013865, 2010.\n[54] M Joshi and C Yang.\npaper=1689348, 2010.\n\nFast Gamma computations for CDO tranches.\n\n23\n\nhttp://ssrn.com/\n\n\f[55] M Joshi and C Yang. Efficient Greek estimation in generic swap-rate market models. http://ssrn.\ncom/paper=1773942, 2011.\n[56] C Kaebe. Feasibility and Efficiency of Monte Carlo Based Calibration of Financial Market Models.\nPhD thesis, University of Trier, 2010.\n[57] C. Kaebe, J.H. Maruhn, and E.W. Sachs. Adjoint-based Monte Carlo calibration of financial market\nmodels. Finance and Stochastics, 13:351\u2013379, 2009.\n[58] M. Leclerc, Q. Liang, and I. Schneider. Fast Monte Carlo Bermudan Greeks. Risk, July:84\u201388, 2009.\n[59] J. Maruhn. On-the-fly bid/ask-vol fitting with applications in model calibration. In SIAM Conference\non Financial Mathematics & Engineering, San Francisco, USA, November 19-20, 2010.\n[60] J. Maruhn. Calibration of stochastic and local stochastic volatility models. In Global Derivatives\nTrading & Risk Management, Paris, April 11-15, 2011.\n[61] I. M. Navon, X. Zou, J. Derber, and J. Sela. Variational data assimilation with an adiabatic version\nof the NMC spectral model. , Monthly Weather Review, 120(7):1433\u20131446, 1992.\n[62] K.B. Petersen and M.S. Pedersen. Matrix cookbook. http://www.imm.dtu.dk/pubdb/views/edoc_\ndownload.php/3274/pdf/imm3274.pdf, 2008.\n[63] S. Smith. Differentiation of the Cholesky algorithm. Journal of Computational and Graphical Statistics, 4(2):134\u2013147, 1995.\n[64] S.P. Smith. A tutorial on simplicity and computational differentiation for statisticians. http://hep.\nphysics.ucdavis.edu/~jrsmith/backwards_differentiation/nonad3.pdf, 2000.\n[65] J. Spilda. Adjoint methods for computing sensitivities in local volatility surfaces. Master's thesis,\nUniversity of Oxford, 2010.\n[66] W. Squire and G. Trapp. Using complex variables to estimate derivatives of real functions. SIAM\nReview, 10(1):110\u2013112, 1998.\n[67] B. Stehle. Proxy scheme and Automatic Differentiation: Computing faster Greeks in Monte Carlo\nsimulations. Master's thesis, Imperial College, 2010.\n[68] O. Talagrand and P. Courtier. Variational assimilation of meteorological observations with the adjoint\nvorticity equation, I, Theory. Q. J. R. Meteorol. Soc., 113:1311\u20131328, 1987.\n[69] G. Turinici. Calibration of local volatility using the local and implied instantaneous variance. Journal\nof Computational Finance, 13(2):1\u201318, 2009.\n\n24\n\n\f"}