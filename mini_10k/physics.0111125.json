{"id": "http://arxiv.org/abs/physics/0111125v1", "guidislink": true, "updated": "2001-11-14T14:34:50Z", "updated_parsed": [2001, 11, 14, 14, 34, 50, 2, 318, 0], "published": "2001-11-14T14:34:50Z", "published_parsed": [2001, 11, 14, 14, 34, 50, 2, 318, 0], "title": "A scale invariant Bayesian method to solve linear inverse problems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0111033%2Cphysics%2F0111057%2Cphysics%2F0111055%2Cphysics%2F0111132%2Cphysics%2F0111206%2Cphysics%2F0111032%2Cphysics%2F0111094%2Cphysics%2F0111197%2Cphysics%2F0111019%2Cphysics%2F0111083%2Cphysics%2F0111006%2Cphysics%2F0111153%2Cphysics%2F0111095%2Cphysics%2F0111060%2Cphysics%2F0111114%2Cphysics%2F0111065%2Cphysics%2F0111139%2Cphysics%2F0111185%2Cphysics%2F0111141%2Cphysics%2F0111008%2Cphysics%2F0111010%2Cphysics%2F0111159%2Cphysics%2F0111093%2Cphysics%2F0111111%2Cphysics%2F0111084%2Cphysics%2F0111053%2Cphysics%2F0111188%2Cphysics%2F0111069%2Cphysics%2F0111113%2Cphysics%2F0111119%2Cphysics%2F0111004%2Cphysics%2F0111121%2Cphysics%2F0111088%2Cphysics%2F0111085%2Cphysics%2F0111178%2Cphysics%2F0111017%2Cphysics%2F0111078%2Cphysics%2F0111118%2Cphysics%2F0111131%2Cphysics%2F0111064%2Cphysics%2F0111041%2Cphysics%2F0111081%2Cphysics%2F0111156%2Cphysics%2F0111200%2Cphysics%2F0111175%2Cphysics%2F0111101%2Cphysics%2F0111003%2Cphysics%2F0111007%2Cphysics%2F0111079%2Cphysics%2F0111134%2Cphysics%2F0111047%2Cphysics%2F0111168%2Cphysics%2F0111120%2Cphysics%2F0111179%2Cphysics%2F0111058%2Cphysics%2F0111044%2Cphysics%2F0111073%2Cphysics%2F0111171%2Cphysics%2F0111070%2Cphysics%2F0111034%2Cphysics%2F0111208%2Cphysics%2F0111122%2Cphysics%2F0111031%2Cphysics%2F0111127%2Cphysics%2F0111146%2Cphysics%2F0111191%2Cphysics%2F0111138%2Cphysics%2F0111099%2Cphysics%2F0111133%2Cphysics%2F0111189%2Cphysics%2F0111145%2Cphysics%2F0111126%2Cphysics%2F0111125%2Cphysics%2F0111194%2Cphysics%2F0111045%2Cphysics%2F0111076%2Cphysics%2F0111100%2Cphysics%2F0111176%2Cphysics%2F0111214%2Cphysics%2F0111193%2Cphysics%2F0111005%2Cphysics%2F0111038%2Cphysics%2F0111037%2Cphysics%2F0111148%2Cphysics%2F0111161%2Cphysics%2F0111043%2Cphysics%2F0111172%2Cphysics%2F0111110%2Cphysics%2F0111021%2Cphysics%2F0111187%2Cphysics%2F0111112%2Cphysics%2F0111106%2Cphysics%2F0111211%2Cphysics%2F0111207%2Cphysics%2F0111072%2Cphysics%2F0111023%2Cphysics%2F0111177%2Cphysics%2F0111160%2Cphysics%2F0111135%2Cphysics%2F0111054%2Cphysics%2F0111117&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A scale invariant Bayesian method to solve linear inverse problems"}, "summary": "In this paper we propose a new Bayesian estimation method to solve linear\ninverse problems in signal and image restoration and reconstruction problems\nwhich has the property to be scale invariant. In general, Bayesian estimators\nare {\\em nonlinear} functions of the observed data. The only exception is the\nGaussian case. When dealing with linear inverse problems the linearity is\nsometimes a too strong property, while {\\em scale invariance} often remains a\ndesirable property. As everybody knows one of the main difficulties with using\nthe Bayesian approach in real applications is the assignment of the direct\n(prior) probability laws before applying the Bayes' rule. We discuss here how\nto choose prior laws to obtain scale invariant Bayesian estimators. In this\npaper we discuss and propose a familly of generalized exponential probability\ndistributions functions for the direct probabilities (the prior $p(\\xb)$ and\nthe likelihood $p(\\yb|\\xb)$), for which the posterior $p(\\xb|\\yb)$, and,\nconsequently, the main posterior estimators are scale invariant. Among many\nproperties, generalized exponential can be considered as the maximum entropy\nprobability distributions subject to the knowledge of a finite set of\nexpectation values of some knwon functions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0111033%2Cphysics%2F0111057%2Cphysics%2F0111055%2Cphysics%2F0111132%2Cphysics%2F0111206%2Cphysics%2F0111032%2Cphysics%2F0111094%2Cphysics%2F0111197%2Cphysics%2F0111019%2Cphysics%2F0111083%2Cphysics%2F0111006%2Cphysics%2F0111153%2Cphysics%2F0111095%2Cphysics%2F0111060%2Cphysics%2F0111114%2Cphysics%2F0111065%2Cphysics%2F0111139%2Cphysics%2F0111185%2Cphysics%2F0111141%2Cphysics%2F0111008%2Cphysics%2F0111010%2Cphysics%2F0111159%2Cphysics%2F0111093%2Cphysics%2F0111111%2Cphysics%2F0111084%2Cphysics%2F0111053%2Cphysics%2F0111188%2Cphysics%2F0111069%2Cphysics%2F0111113%2Cphysics%2F0111119%2Cphysics%2F0111004%2Cphysics%2F0111121%2Cphysics%2F0111088%2Cphysics%2F0111085%2Cphysics%2F0111178%2Cphysics%2F0111017%2Cphysics%2F0111078%2Cphysics%2F0111118%2Cphysics%2F0111131%2Cphysics%2F0111064%2Cphysics%2F0111041%2Cphysics%2F0111081%2Cphysics%2F0111156%2Cphysics%2F0111200%2Cphysics%2F0111175%2Cphysics%2F0111101%2Cphysics%2F0111003%2Cphysics%2F0111007%2Cphysics%2F0111079%2Cphysics%2F0111134%2Cphysics%2F0111047%2Cphysics%2F0111168%2Cphysics%2F0111120%2Cphysics%2F0111179%2Cphysics%2F0111058%2Cphysics%2F0111044%2Cphysics%2F0111073%2Cphysics%2F0111171%2Cphysics%2F0111070%2Cphysics%2F0111034%2Cphysics%2F0111208%2Cphysics%2F0111122%2Cphysics%2F0111031%2Cphysics%2F0111127%2Cphysics%2F0111146%2Cphysics%2F0111191%2Cphysics%2F0111138%2Cphysics%2F0111099%2Cphysics%2F0111133%2Cphysics%2F0111189%2Cphysics%2F0111145%2Cphysics%2F0111126%2Cphysics%2F0111125%2Cphysics%2F0111194%2Cphysics%2F0111045%2Cphysics%2F0111076%2Cphysics%2F0111100%2Cphysics%2F0111176%2Cphysics%2F0111214%2Cphysics%2F0111193%2Cphysics%2F0111005%2Cphysics%2F0111038%2Cphysics%2F0111037%2Cphysics%2F0111148%2Cphysics%2F0111161%2Cphysics%2F0111043%2Cphysics%2F0111172%2Cphysics%2F0111110%2Cphysics%2F0111021%2Cphysics%2F0111187%2Cphysics%2F0111112%2Cphysics%2F0111106%2Cphysics%2F0111211%2Cphysics%2F0111207%2Cphysics%2F0111072%2Cphysics%2F0111023%2Cphysics%2F0111177%2Cphysics%2F0111160%2Cphysics%2F0111135%2Cphysics%2F0111054%2Cphysics%2F0111117&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper we propose a new Bayesian estimation method to solve linear\ninverse problems in signal and image restoration and reconstruction problems\nwhich has the property to be scale invariant. In general, Bayesian estimators\nare {\\em nonlinear} functions of the observed data. The only exception is the\nGaussian case. When dealing with linear inverse problems the linearity is\nsometimes a too strong property, while {\\em scale invariance} often remains a\ndesirable property. As everybody knows one of the main difficulties with using\nthe Bayesian approach in real applications is the assignment of the direct\n(prior) probability laws before applying the Bayes' rule. We discuss here how\nto choose prior laws to obtain scale invariant Bayesian estimators. In this\npaper we discuss and propose a familly of generalized exponential probability\ndistributions functions for the direct probabilities (the prior $p(\\xb)$ and\nthe likelihood $p(\\yb|\\xb)$), for which the posterior $p(\\xb|\\yb)$, and,\nconsequently, the main posterior estimators are scale invariant. Among many\nproperties, generalized exponential can be considered as the maximum entropy\nprobability distributions subject to the knowledge of a finite set of\nexpectation values of some knwon functions."}, "authors": ["A. Mohammad-Djafari", "J\u00e9r\u00f4me Idier"], "author_detail": {"name": "J\u00e9r\u00f4me Idier"}, "author": "J\u00e9r\u00f4me Idier", "arxiv_comment": "Presented at MaxEnt93. Appeared in Maximum Entropy and Bayesian\n  Methods, G. Heidbreder (Ed.), Kluwer Academic Publishers, pp: 121-134,\n  (http://www.wkap.nl/prod/b/0-7923-2851-5)", "links": [{"href": "http://arxiv.org/abs/physics/0111125v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/physics/0111125v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/physics/0111125v1", "affiliation": "Laboratoire des Signaux et Syst\u00e8mes, CNRS-UPS-SUPELEC, Gif-sur-Yvette, France", "arxiv_url": "http://arxiv.org/abs/physics/0111125v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:physics/0111125v1 [physics.data-an] 14 Nov 2001\n\nA SCALE INVARIANT BAYESIAN METHOD TO SOLVE LINEAR\nINVERSE PROBLEMS\n\nAli Mohammad-Djafari and J\u00e9r\u00f4me Idier\nLaboratoire des Signaux et Syst\u00e8mes (CNRS-ESE-UPS)\n\u00c9cole Sup\u00e9rieure d'\u00c9lectricit\u00e9,\nPlateau de Moulon, 91192 Gif-sur-Yvette C\u00e9dex, France\nAbstract. In this paper we propose a new Bayesian estimation method to solve linear\ninverse problems in signal and image restoration and reconstruction problems which has\nthe property to be scale invariant. In general, Bayesian estimators are nonlinear functions\nof the observed data. The only exception is the Gaussian case. When dealing with linear\ninverse problems the linearity is sometimes a too strong property, while scale invariance\noften remains a desirable property. As everybody knows one of the main difficulties\nwith using the Bayesian approach in real applications is the assignment of the direct\n(prior) probability laws before applying the Bayes' rule. We discuss here how to choose\nprior laws to obtain scale invariant Bayesian estimators. In this paper we discuss and\npropose a familly of generalized exponential probability distributions functions for the\ndirect probabilities (the prior p(x) and the likelihood p(y|x)), for which the posterior\np(x|y), and, consequently, the main posterior estimators are scale invariant. Among\nmany properties, generalized exponential can be considered as the maximum entropy\nprobability distributions subject to the knowledge of a finite set of expectation values of\nsome knwon functions.\n\n1. Introduction\nWe address a class of linear inverse problems arising in signal and image reconstruction and restoration problems which is to solve integral equations of the form:\nZ\ngij =\nf (r \u2032 ) hij (r \u2032 ) dr\u2032 + bij , i, j = 1, * * * , M,\n(1)\nD\n\nwhere r\u2032 \u2208 IR2 , f (r \u2032 ) is the object (image reconstruction problems) or the original\nimage (image restoration problems), gij are the measured data (the projections in\nimage reconstruction or the degraded image in image restoration problems), bij\nare the measurement noise samples and hij (r \u2032 ) are known functions which depend\nonly on the measurement system. To show the generality of this relation, we give\nin the following some applications we are interested in:\n\u2212 Image restoration:\nZZ\ni = 1, * * * , N\nf (x\u2032 , y \u2032 )h(xi \u2212 x\u2032 , yj \u2212 y \u2032 ) dx\u2032 dy \u2032 + b(xi , yj ) ,\n,\ng(xi , yj ) =\nj\n= 1, * * * , M\nD\nwhere g(xi , yj ) are the observed degraded image pixels and h(x, y) is the point\nspread function (PSF) of the measurement system.\n\n\f\u2212 X-ray computed tomography (CT):\nZZ\ni = 1, * * * , N\nf (x, y)\u03b4(ri \u2212x cos \u03c6i \u2212y sin \u03c6i ) dx dy+b(ri , \u03c6j ) ,\n,\ng(ri , \u03c6j ) =\nj\n= 1, * * * , M\nD\nwhere g(ri , \u03c6j ) are the projections along the axis ri = x cos \u03c6i \u2212 y sin \u03c6i ,\nhaving the angle \u03c6j , and which can be considered as the samples of the Radon\ntransform (RT) of the object function f (x, y).\n\u2212 Fourier Synthesis in radio astronomy, in SAR imaging and in diffracted wave\ntomographic imaging systems:\nZZ\nf (x, y) exp [\u2212j(uj x + vj y)] dx dy + b(uj , vj ), j = 1, * * * , M,\ng(uj , vj ) =\nD\n\nwhere uj = (uj , vj ) is a radial direction and g(uj , vj ) are the samples of the\ncomplex valued visibility function of the sky in radio astronomy or the Fourier\ntransform of the measured signal in SAR imaging.\nOther examples can be found in [6, 7, 5, 8, 9].\nIn all these applications we have to solve the following ill-posed problem: how\nto estimate the function f (x, y) from some finite set of measured data which may\nalso be noisy, because there is no experimental measurement device, even the most\nelaborate, which could be entirely free from uncertainty, the simplest example\nbeing the finite precision of the measurements.\nThe numerical solution of these equations needs a discretization procedure\nwhich can be done by a quadrature method. The linear system of equations\nresulting from the discretization of an ill-posed problem is, in general, very illconditioned if not singular. So the problem is to find a unique and stable solution\nfor this linear system. The general methods which permit us to find a unique and\nstable solution to an ill-posed problem by introducing an a priori information on\nthe solution are called regularization . The a priori information can be either in\na deterministic form (positivity) or in a stochastic form (some constraints on the\nprobability density functions).\nWhen discretized, these problems can be described by the following:\n\"Estimate a vector of the parameters x \u2208 IRn (pixel intensities in an image for example) given a vector of measurements y \u2208 IRm (representing,\nfor example, either a degraded image pixel values in restoration problems or the projections values in reconstruction problems) and a linear\ntransformation A relating them by:\ny = Ax + b,\n\n(2)\n\nwhere b represents the discretization errors and the measurement noise\nwhich is supposed to be zero-mean and additive.\"\nIn this paper we propose to use the Bayesian approach to find a regularized solution to this problem. Noting that the Bayesian theory only gives us a framework\nfor the formulation of the inverse problem, not a solution of it. The main difficulty\n\n\fis, in general, before the application of the Bayes' formula, i.e.; how to formulate\nappropriately the problem and how to assign the direct probabilities. Keeping\nthis fact in mind, we propose the following organization to this paper: In section\n2. we give a brief description of the Bayesian approach with detail calculations of\nthe solution in the special case of Gaussian laws. In section 3. we discuss about\nthe scale invariance property and propose a familly of prior probability density\nfunctions (pdf ) which insure this property for the solution. Finally, in section 4.,\nwe present some special cases and give detailed calculations for the solution.\n2.\n\nGeneral Bayesian approach\n\nA general Bayesian approach involves the following steps:\n\u2212 Assign a prior probability law p(x) to the unknown parameter to translate\nour incomplete a priori information (prior beliefs) about these parameters;\n\u2212 Assign a direct probability law to the measured data p(y|x) to translate the\nlack of total precision and the inevitable existence of the measurement noise;\n\u2212 Use the Bayes' rule to calculate the posterior law p(x|y) of the unknown\nparameters;\nb to these parameters.\n\u2212 Define a decision rule to give values x\nTo illustrate the whole procedure, let us to consider an example; the Gaussian case.\nIf we suppose that what we know about the unknown input x is its mean E {x} =\nx0 and its covariance matrix E {(x \u2212 x0 )(x \u2212 x0 )t } = Rx = \u03c3x2 P ,\band what we\nknow about the measurement noise b is also its covariance matrix E bbt = Rb =\n\u03c3b2 I, then we can use the maximum entropy principle to assign:\n\u0015\n\u0014\n1\n(3)\np(x) \u221d exp \u2212 (x \u2212 x0 )t Rx \u22121 (x \u2212 x0 ) ,\n2\nand\n\n\u0015\n\u0014\n1\np(y|x) \u221d exp \u2212 (y \u2212 Ax)t Rb \u22121 (y \u2212 Ax) .\n2\n\n(4)\n\np(x|y) \u221d p(y|x) p(x),\n\n(5)\n\nNow we can use the Bayes' rule to find:\n\nand use, for example, the maximum a posteriori (MAP) estimation rule to give a\nsolution to the problem, i.e.;\nb = arg max {p(x|y)} ,\nx\nx\n\n(6)\n\nOther estimators are possible. In fact, all we want to know is resumed in the\nposterior law. In general, one can construct a bayesian estimator by defining a\ncost (or utility) function C(b\nx, x) and by minimizing its mean value\n\u001aZZ\n\u001b\n\b\nb = arg min EX|Y {C(z, x)} = arg min\nC(z, x)p(x|y) dx .\nx\nz\nz\n\nThe two classical estimators:\n\n\f\u2212 Posterior mean (PM):\n\nb = EX|Y {x} =\nx\n\nZ\n\nx p(x|y) dx,\nt\n\nis obtained when defining C(b\nx, x) = (b\nx \u2212 x) (b\nx \u2212 x), and\nb = arg max {p(x|y)},\n\u2212 Maximum a posteriori (MAP): x\nx\nis obtained when defining C(b\nx, x) = 1 \u2212 \u03b4(b\nx \u2212 x).\n\nNow, let us go a little further inside the calculations. Replacing (3), and (4)\nin (5), we calculate the posterior law:\n\u0014\n\u0015\n1\np(x|y) \u221d exp \u2212 2 J(x) , with J(x) = (y\u2212Ax)t (y\u2212Ax)+\u03bb(x\u2212x0 )t P \u22121 (y\u2212x0 ),\n2\u03c3b\nwhere \u03bb = \u03c3b2 /\u03c3x2 . The posterior is then also a Gaussian. We can know use any\ndecision rule to obtain a solution. For example the maximum a posteriori (MAP)\nsolution is obtained by:\nb = arg max {p(x|y)} = arg min {J(x)} .\nx\nx\nx\n\n(7)\n\nb = EX|Y {x} = arg max {p(x|y)}\nx\nx\n\n(8)\n\nJ(x) = ||y \u2212 Ax||2 + \u03bb||x \u2212 x0 ||2P\n\n(9)\n\nNote that in this special Gaussian case both estimators, i.e.; the posterior mean\n(PM) and the MAP estimators are the same:\n\nand the minimization of the criterion J(x) which can also be written in the form:\n\ncan be considered as a regularization procedure to the inverse problem (2). Indeed,\nthe Bayesian approach will give us here a new interpretation of the regularization\nparameter in terms of the signal to noise ratio, i.e.; \u03bb = \u03c3b2 /\u03c3x2 .\nb is then a linear function of the\nJ(x) is a quadratic function of x. The solution x\ndata y. This is due to the fact that the problem is linear and all the probability\nlaws are Gaussian. Excepted this case, in general, the Bayesian estimators are\nnot linear functions of the observations y. However, we may not need that the\nsolution be a linear function of the data y, but the scale invariance is the minimum\nproperty which is often needed.\n3.\n\nScale invariant Bayesian estimators\n\nWhat we are proposing in this paper is to study in what conditions we can obtain\nestimators who are scale invariant. Note that linearity is the combination of\n\u001a\nb1,\ny 1 7\u2192 x\nb1 + x\nb2,\nadditivity:\n=\u21d2 y 1 + y 2 7\u2192 x\nb2\ny 2 7\u2192 x\nand\nb 1 =\u21d2 \u2200k > 0, ky 1 7\u2192 kb\nscale invariance:\ny 1 7\u2192 x\nx1 .\n\n\fIn a linear inverse problem what is often necessary is that the solution be scale\ninvariant. As we have seen in the last section when all the probability laws are\nGaussian then the Bayesian estimators are linear functions of the data, so that\nthe methods based on this assumption have not to take care about the scale of the\nmeasured data. The Gaussian assumption is very restrictive. On the other hand,\nmore general priors yield the Bayesian estimators which are nonlinear functions of\ndata, so the result of the inversion method depend on the absolute values of the\nmeasured data. In other words, two users of the method using two different scale\nfactors would not get the same results, even rescaled:\n\nb1\ny \u2212\u2192 k1 \u2212\u2192 Estimation \u2212\u2192 x\nb2\ny \u2212\u2192 k2 \u2212\u2192 Estimation \u2212\u2192 x\n\nb 2 6= x\nb1\nx\nk2\nk1\n\nA general nonlinear (scale variant) estimation method\nWhat we want to specify in this paper is a family of probability laws for which these\nestimators are scale invariant. So the user of the inversion method can process the\ndata without worrying about rescaling them to an arbitrary level and two users of\nthe method at two different scales will obtain the proportional results:\n\nb1\ny \u2212\u2192 k1 \u2212\u2192 Estimation \u2212\u2192 x\nb2\ny \u2212\u2192 k2 \u2212\u2192 Estimation \u2212\u2192 x\n\nb1\nb2 = x\nx\nk2\nk1\n\nA scale invariant estimation method\n\nTo do this let us note\n\u2212 \u03b8 all the unknown parameters defining our measuring system (noise variance\n\u03c3 2 and the prior law parameters for example),\n\u2212 p1 (x1 |y 1 ; \u03b81 ) and pk (xk |y k ; \u03b8k ) the two expressions of the posterior law for\nscale 1 and for scale k with\nxk = kx1 ,\n\ny k = ky 1 .\n\nThen, what we need is the following:\n\u2203\u03b8k = f (\u03b81 , k) | \u2200k > 0, \u2200x1 , y 1 ,\n\npk (xk |y k ; \u03b8k ) =\n\n1\np1 (x1 |y 1 ; \u03b81 ),\nkn\n\n(10)\n\nwhich means that the functional form of the posterior law remains unchanged\nwhen the measurement's scale is changed. Only we have to modify the parameters\n\u03b8 k = f (\u03b81 , k) which is only a function of \u03b8 1 and the scale factor k.\n\n\fHowever, not all estimators based on this posterior will be scale invariant. The\ncost function must also have some property to obtain a scale invariant estimator.\nSo, the main result of this paper can be resumed in the following theorem:\nTheorem: If \u2203\u03b8 k = f (\u03b81 , k) | \u2200k > 0, \u2200x1 , y 1 ,\npk (xk |y k ; \u03b8k ) =\n\n1\np1 (x1 |y 1 ; \u03b81 ),\nkn\n\nthen any bayesian estimator with a cost function C(b\nx, x) satisfying:\nC(b\nxk , xk ) = ak + bk C(b\nx, x),\nis a scale invariant estimator, i.e.;\nb k (y k ; \u03b8k ) = kb\nx\nx1 (y 1 ; \u03b8 1 ).\n\nProof: In fact, it is easy to see the following:\n\u001aZZ\n\u001b\nb k (y k ; \u03b8k ) = arg min\nx\nC(z k , xk )pk (xk |y k ; \u03b8k ) dxk\nz k \u001aZZ\n\u001b\n1\n= k arg min\n[bk C(z 1 , x1 ) + ak ] n p1 (x1 |y 1 ; \u03b81 )k n dx1\nz1 \u001a Z\nk\n\u001b\n= k arg min bk C(z 1 , x1 )p1 (x1 |y 1 ; \u03b8 1 ) dx1 + ak\nz 1 \u001aZZ\n\u001b\n= k arg min\nC(z 1 , x1 )p1 (x1 |y 1 ; \u03b81 ) dx1\nz1\nb 1 (y 1 ; \u03b8 1 )\n= kx\n\nb (y; \u03b8) is a\nNote the great significance of this result, even if the estimateur x\nnonlinear function of the observations y it stays scale invariant.\nNow, the task is to search for a large familly of probability laws p(x) and p(y|x)\nin a manner that the posterior law p(x|y) remains scale invariant. We propose to\ndo this search in the generalized exponential familly for two reasons:\n\u2212 First the generalized exponential probability density functions form a very\nrich one, and\n\u2212 Second, they can be considered as the maximum entropy prior laws subject\nto a finite number of constraints (linear or nonlinear).\nNoting also that if p(x) and p(y|x) are scale invariant then the posterior p(x|y)\nis also scale invariant and that there is a symmetry for p(x) and p(y|x), so that\nit is only necessary to find the scale invariance conditions for one of them. In the\nfollowing, without loss of generality, we consider the case where p(y|x) is Gaussian:\n\u0002\n\u0003\np(y|x; \u03c3 2 ) \u221d exp \u2212\u03c72 (x, y; \u03c3 2 ) ,\n\nwith \u03c72 (x, y; \u03c3 2 ) =\n\n1\n[y \u2212 Hx]t [y \u2212 Hx],\n2\u03c3 2\n(11)\n\n\fand find the conditions for p(x) to be scale invariant. We choose the generalized\nexponential pdf 's for p(x), i.e.;\n#\n\" r\nX\n\u03bbi \u03c6i (x) ,\n(12)\np(x; \u03bb) \u221d exp \u2212\ni=1\n\nand find the conditions on the functions \u03c6i (x) for which p(x) is scale invariant.\nNote that these laws can be considered as the maximum entropy prior laws if\nour prior knowledge is:\n\u2212 What we know about x is:\nE {\u03c6i (x)} = di ,\n\ni = 1, * * * , r,\n\n\u2212 and what we know about the noise b is:\n\u001a\nE\b\n{b} = 0,\nE bbt = Rb = \u03c3 2 I,\n\nwhere Rb is the covariance matrix of b.\nNow, using the equations (11) and (12) and noting by \u03b8 = (\u03c3 2 , \u03bb1 , * * * , \u03bbr ), by\n\u03bb = (\u03bb1 , * * * , \u03bbr ), and by \u03c6(x) = (\u03c61 (x), * * * , \u03c6r (x)), we have\n\u0002\n\u0003\np(x|y; \u03b8) \u221d exp \u2212\u03c72 (x, y; \u03c3 2 ) \u2212 \u03bbt \u03c6(x) ,\n(13)\nand the scale invariance condition becomes:\n\u2200k > 0, \u2200x1 , y 1 ,\n\n\u03c72k (xk , y k ; \u03c3k2 ) + \u03bbtk \u03c6(xk ) = \u03c721 (x1 , y 1 ; \u03c312 ) + \u03bbt1 \u03c6(x1 ) + cte.\n\nBut with the Gaussian choice for the noise pdf we have\n\u2200k > 0, \u2200x1 , y 1 , \u03c72k (xk , y k ; \u03c3k2 ) =\n\n1\n1\n||y k \u2212Hxk ||2 = 2 2 k 2 ||y 1 \u2212Hx1 ||2 = \u03c721 (x1 , y 1 ; \u03c312 ),\n2\u03c3k2\n2k \u03c31\n\nand so the condition becomes\n\u2200k > 0, \u2200x,\n\n\u03bbtk \u03c6(xk ) = \u03bbt1 \u03c6(x1 ) + cte,\n\n(14)\n\nor equivalently,\npk (xk ; \u03bbk ) =\n\n1\np1 (x1 ; \u03bb1 )\nkn\n\nwith\n\n\u03bbk = f (\u03bb1 , k).\n\nThus, in the case of centered Gaussian pdf for the noise, to have a scale invariant\nposterior law it is sufficient to have a scale invariant prior law.\nNow, assuming interchangeable (independent) pixels, i.e.;\n#\n\"\nN\nr\nY\nX\np(xj ; \u03bb),\n(15)\n\u03bbi \u03c6i (x) =\np(x; \u03bb) = exp \u03bb0 +\ni=1\n\nj=1\n\n\for equivalently,\n\u03c6i (x) =\n\nN\nX\n\n\u03c6i (xj )\n\n(16)\n\nj=1\n\nwe have to find the conditions on the scalar functions \u03c6i (x) of scalar variables x\nwho satisfy the equation (14) or equivalently\n\u2200k > 0, \u2200x,\n\nr\nX\n\n\u03bbi (k) \u03c6i (kx) =\n\ni=1\n\nr\nX\n\n\u03bbi (1) \u03c6i (x) + cte\n\n(17)\n\ni=1\n\nWe have shown (see appendix) that, the functions \u03c6i (x) which satisfy these conditions are all either the powers of x or the powers of ln x or a multiplication of\nthem. The general expressions for these functions are:\n!\nM\nN0\nNX\nM\nm \u22121\nX\nX\nX\nn\nc0n (ln x)n , with M \u2264 r and\nNm = r\nx\u03b1m +\ncmn (ln x)\n\u03c6(x) =\nm=1\n\nn=0\n\nn=0\n\nm=0\n\n(18)\nwhere M and Nm are integer numbers, and cmn , c0n and \u03b1m are real numbers.\nFor a geometrical interpretation and more details see appendix. The following\nexamples show some special and interesting cases.\nOne parameter laws: Consider the case of r = 1. In this case we have\np(x; \u03bb) \u221d exp [\u2212\u03bb\u03c6(x)] .\n\n(19)\n\nApplying the general rule with\n\u001a\nM = 0, N0 = 1,\n\u2212\u2192 c00 + c01 ln x\nr = 1 \u2212\u2192\nM = 1, N0 = 0, N1 = 1, \u2212\u2192 c00 + c10 x\u03b11\nwe find that the only functions who satisfy these conditions are:\n\u001a\n\u001b \u001a\n\u001b\n\u03c6(x) = x\u03b1 , ln x\n\n(20)\n\nwhere \u03b1 is a real number. There isc two interesting special cases:\n\u2212 \u03c6(x) = x\u03b1 , resulting to: p(x) \u221d exp [\u2212\u03bbx\u03b1 ] , \u03b1 > 0, \u03bb > 0, which is a generalized Gaussian pdf , and\n\u2212 \u03c6(x) = ln x, resulting to: p(x) \u221d exp [\u2212\u03bb ln x] , which is a special case of the\nBeta pdf .\nNote that the famous entropic prior law:\np(x) \u221d exp [\u2212\u03bbx ln x] of Gull and\nSkilling [11, 4] does not verify the scale invariance property. But, if we add one\nmore parameter\np(x) \u221d exp [\u2212\u03bbx ln x + \u03bcx] ,\nthen, it will satisfy this condition as we can see in the next section.\n\n\fTwo parameters laws: This is the case where r = 2 and we have:\np(x; \u03bb) \u221d exp [\u2212\u03bb\u03c61 (x) \u2212 \u03bc\u03c62 (x)] ,\nand applying the general rule:\n\uf8f1\n\uf8f4\n\uf8f4 M = 2, N0 = 0, N1 = 1, N2 = 1,\n\uf8f2\nM = 1, N0 = 0, N1 = 2,\nr = 2 \u2212\u2192\nM\n= 1, N0 = 1, N1 = 1,\n\uf8f4\n\uf8f4\n\uf8f3\nM = 0, N0 = 2,\n\n(21)\n\n\u2212\u2192 c00 + c10 x\u03b11 + c20 x\u03b12\n\u2212\u2192 c00 + c10 x\u03b11 + c11 x\u03b11 ln x\n\u2212\u2192 c00 + c10 x\u03b11 + c01 ln x\n\u2212\u2192 c00 + c01 ln x + c02 ln2 x\n\nwe see that in this case the only functions (\u03c61 , \u03c62 ) which satisfy these conditions\nare:\n\u001a\n\u001b\n(\u03c61 (x), \u03c62 (x)) =\n\n\u001b\n\u001a\n(x\u03b11 , x\u03b12 ), (x\u03b11 , x\u03b11 ln x), (x\u03b11 , ln x), (ln x, ln2 x)\n\n(22)\nwhere \u03b11 and \u03b12 are two real numbers. Special cases are obtained when we choose\n\u03c62 (x) = x, the only possible functions for \u03c61 (x) are then:\n{x\u03b1 , ln x, x ln x} .\n\n(23)\n\nand we have the following interesting cases:\n\nh\n\u0001 i\n\u0002\n\u0003\n\u03bc 2\n,\np(x) \u221d exp \u2212\u03bbx2 \u2212 \u03bcx \u221d exp \u2212\u03bb x + 2\u03bb\n\u0001\n\u2212\u03bc\n1\n2\nwhich is a Gaussian pdf N m = \u03bb , \u03c3 = 2\u03bb .\n\u2212 \u03c61 (x) = ln x, resulting to: p(x) \u221d exp [\u2212\u03bb ln x \u2212 \u03bcx] = x\u2212\u03bb exp [\u2212\u03bcx] , which\nis the Gamma pdf , and finally,\n\u2212 \u03c61 (x) = x ln x, resulting to: p(x) \u221d exp [\u2212\u03bbx ln x \u2212 \u03bcx] . which is known as\nthe entropic pdf .\n\u2212 \u03c61 (x) = x2 , resulting to:\n\nThree parameters laws: This is the case where r = 3. Once more applying the\ngeneral rule we find:\n\uf8f1\nM = 3, N0 = 0, N1 = 1, N2 = 1, N3 = 1, \u2192 c00 + c10 x\u03b11 + c20 x\u03b12 + c30 x\u03b13\n\uf8f4\n\uf8f4\n\uf8f4 M = 2, N = 0, N = 1, N = 2,\n\uf8f4\n\u2192 c00 + c10 x\u03b11 + c20 x\u03b12 + c21 x\u03b12 ln x\n0\n1\n2\n\uf8f4\n\uf8f4\n\uf8f4 M = 2, N = 1, N = 1, N = 1,\n\uf8f4\n\u2192 c00 + c01 ln x + c10 x\u03b11 + c20 x\u03b12\n0\n1\n2\n\uf8f2\nM = 1, N0 = 0, N1 = 3,\n\u2192 c00 + c10 x\u03b11 + c11 x\u03b11 ln x + c12 x\u03b11 ln2 x\nr=3\u2192\n\uf8f4\n\uf8f4\nM = 1, N0 = 1, N1 = 2,\n\u2192 c00 + c01 ln x + c10 x\u03b11 + c11 x\u03b11 ln x\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nM = 1, N0 = 2, N1 = 1,\n\u2192 c00 + c01 ln x + c02 ln2 x + c10 x\u03b11\n\uf8f4\n\uf8f4\n\uf8f3\nM = 0, N0 = 3,\n\u2192 c00 + c01 ln x + c02 ln2 x + c03 ln3 x\nwhich means:\n\u001b \u001a\n\u001a\n(\u03c61 (x), \u03c62 (x), \u03c63 (x)) =\n(x\u03b11 , x\u03b12 , x\u03b13 ), (x\u03b11 , x\u03b12 , ln x), (x\u03b11 , x\u03b11 ln x, x\u03b11 ln2 x),\n\n(x\u03b11 , x\u03b11 ln x, ln x), (x\u03b11 , x\u03b12 , x\u03b12 ln x), (x\u03b11 , ln x, ln2 x),\n\n\u001b\n(ln x, ln x, ln x)\n2\n\n(24)\n\nwhere \u03b11 , \u03b12 and \u03b13 are three real numbers.\n\n3\n\n\f4.\n\nProposed method\n\nThe general procedure of the inversion method we propose can be resumed as\nfollows:\n\u2212 Choose a set of functions \u03c6i (x) between the possibles ones described in the last\nsection and assign the prior p(x). In many imaging applications we proposed\nand used successfully the following two parameters one:\np(x; \u03bb) \u221d exp [\u2212\u03bb1 H(x) \u2212 \u03bb2 S(x)] ,\n\nwith H(x) =\n\nN\nX\n\n\u03c61 (xj ), and S(x) =\n\nj=1\n\nN\nX\n\n\u03c62 (xj )\n\nj=1\n\nwhere \u03c61 (x) and \u03c62 (x) choosed between the possible ones in (22) or \b(23).\n\u2212 When what we know about the noise b is only its covariance matrix E bbt =\nRb = \u03c3b2 I, then using the maximum entropy principle we have:\n\u0015\n\u0014\n1\np(y|x) \u221d exp \u2212 Q(x) , with Q(x) = (y \u2212 Ax)t Rb \u22121 (y \u2212 Ax).\n2\nWe may note that p(y|x) is also a scale invariant probability law.\n\u2212 Using the Bayes' rule and MAP estimator the solution is determined by\nb = arg max {p(x|y)} = arg min {J(x)} ,\nx\nx\nx\n\nwith J(x) = Q(x)+\u03bb1 H(x)+\u03bb2 S(x).\n\nNote here also that, for the cases where one of the functions \u03c61 (x) or \u03c62 (x)\nis a logarithmic function of x, we have to constraint its range to the positive\nreal axis, and we have to solve the following optimization problem\nb = arg max {p(x|y)} = arg min {J(x)} .\nx\nx>0\nx>0\n\nThis optimization is achieved by a modified conjugate gradients method.\n\u2212 The choice of the functions \u03c6i (x) and the determination of the parameters\n(\u03bb1 , \u03bb2 ) in the first step is still an open problem.\nIn imaging applications we propose to do this choice from our prior knowledge\non the nature of interested quantity (physics of the application). For example,\nif the object x is a real quantity equally distributed on the positive and the\nnegative reals then a Gaussian prior, i.e.; (\u03c61 (x) = x, \u03c62 (x) = x2 ) is convenient. But, if the object x is a positive quantity or if we know that it represents\nsmall extent, bright and sharp objects on a nearly black background (images in\nradio astronomy, for example), then we may choose (\u03c61 (x) = x, \u03c62 (x) = ln x),\nor (\u03c61 (x) = x, \u03c62 (x) = x ln x) which are the priors with longer tails than the\nGaussian or truncated Gaussian one.\nWhen the choice of the functions (\u03c61 (x), \u03c62 (x)) is done, we still have to determine the hyperparameters (\u03bb1 , \u03bb2 ). For this two main approaches have been\nproposed. The first is based on the generalized maximum likelihood (GML)\nwhich tries to estimate simultaneously the parameters x and the hyperparameters \u03b8 = (\u03bb1 , \u03bb2 ) by\nb = arg max {p(x, y; \u03b8)} = arg max {p(y|x) p(x; \u03b8)} ,\n(b\nx, \u03b8)\n(x,\u03b8 )\n(x , \u03b8 )\n\n(25)\n\n\fand the second is based on the marginalization (MML), in which the hyperparameters \u03b8 are estimated first by\n\u001aZZ\n\u001a\n\u001b\n\u001b\nZ\nb\np(x, y; \u03b8) dx = arg max\np(y|x) p(x; \u03b8) dx ,\n\u03b8 = arg max p(y; \u03b8) =\n\u03b8\n\u03b8\n(26)\nand then used for the estimation of x:\nn\no\nn\no\nb = arg max p(y|x) p(x|\u03b8)\nb .\nb = arg max p(x|y; \u03b8)\nx\n(27)\nx\nx\n\nWhat is important here is that both methods preserve the scale invariant\nproperty. For practical applications we have recently proposed and used a\nmethod based on the generalized maximum likelihood [8, 9] which has been\nsuccessfully used in many signal and image reconstruction and restoration\nproblems as we mentionned in the introduction [10].\n5.\n\nConclusions\n\nExcepted the Gaussian case where all the Bayesian estimators are linear functions\nof the observed data, in general, the Bayesian estimators are nonlinear functions\nof the data. When dealing with linear inverse problems linearity is sometimes a\ntoo strong property, while scale invariance often remains a desirable property. In\nthis paper we discussed and proposed a familly of generalized exponential probability distributions for the direct probabilities (the prior p(x) and the likelihood\np(y|x)), for which the posterior p(x|y), and, consequently, the main posterior estimators are scale invariant. Among many properties, generalized exponential can\nbe considered as the maximum entropy probability distributions subject to the\nknowledge of a finite set of expectation values of some knwon functions.\n1. Appendix: General case\nWe want to find the solutions of the following equation:\n\u2200k > 0, \u2200x,\n\nr\nX\n\n\u03bbi (k)\u03c6i (kx) =\n\nr\nX\n\n\u03bbi (1)\u03c6i (x) + \u03b2(k)\n\n(A.1)\n\ni=1\n\ni=1\n\nMaking the following changes of variables and notations\n1/k = k\u0303, kx = x\u0303, \u03bbi (k) = \u03bb\u0303i (k\u0303), and \u03b2i (k) = \u03b2\u0303i (k\u0303),\nequation (A.1) becomes\nr\nX\n\n\u03bb\u0303i (k\u0303)\u03c6i (x\u0303) =\n\ni=1\n\nr\nX\n\n\u03bb\u0303i (1)\u03c6i (k\u0303x\u0303) + \u03b2\u0303(k\u0303)\n\ni=1\n\nFor convenience sake, we will drop out the tilde  \u0303, and note \u03bbi (1) = \u03bbi , so that we\ncan write\nr\nr\nX\nX\n\u03bbi \u03c6i (kx) + \u03b2(k)\n\u03bbi (k)\u03c6i (x) =\ni=1\n\ni=1\n\n\fNoting\nS(x) =\n\nr\nX\n\n\u03bbi \u03c6i (x),\n\nand so S(kx) =\n\n\u03bbi \u03c6i (kx)\n\ni=1\n\ni=1\n\nwe have\n\nr\nX\n\nr\nX\n\n\u03bbi (k)\u03c6i (x) = S(kx) + \u03b2(k)\n\n(A.2)\n\ni=1\n\nDeriving r \u2212 1 times this equation with respect to k we obtain\nr\nX\n\n\u03bb\u2032i (k)\u03c6i (x)\n\n= x S \u2032 (kx) + \u03b2 \u2032 (k)\n\n\u03bb\u2032\u2032i (k)\u03c6i (x)\n\n= x2 S \u2032\u2032 (kx) + \u03b2 \u2032\u2032 (k)\n\ni=1\n\nr\nX\ni=1\n\n..\n.\nr\nX\n\n(A.3)\n\n..\n.\n(r\u22121)\n\n\u03bbi\n\n(k)\u03c6i (x)\n\n= xr\u22121 S (r\u22121) (kx) + \u03b2 (r\u22121) (k)\n\ni=1\n\nCombinig equations (A.2) and (A.3) in matrix form we have\n\uf8f6\n\uf8eb \u03bb (k)\nS(kx) + \u03b2(k)\n***\n\u03bbr (k) \uf8f6 \uf8eb \u03c61 (x) \uf8f6 \uf8eb\n1\n\u2032\n\u2032\n\u2032\n\u2032\nxS (kx) + \u03b2 (k)\n***\n\u03bbr (k) \uf8f7 \uf8ec \u03c62 (x) \uf8f7 \uf8ec\n\uf8f7\n\uf8ec \u03bb1 (k)\n\uf8f7\n\uf8f7 \uf8ec\n\uf8f7\uf8ec\n\uf8ec\nx2 S \u2032\u2032 (kx) + \u03b2 \u2032\u2032 (k)\n***\n\u03bb\u2032\u2032r (k) \uf8f7 \uf8ec \u03c63 (x) \uf8f7 = \uf8ec\n\uf8f7\n\uf8ec \u03bb\u2032\u20321 (k)\n\uf8f7\n\uf8f7\uf8ec . \uf8f7 \uf8ec\n\uf8ec\n.\n.\n.\n\uf8f8\n\uf8f8\uf8ed . \uf8f8 \uf8ed\n\uf8ed\n.\n..\n.\n.\n.\n***\n.\n(r\u22121)\n(r\u22121)\nxr\u22121 S (r\u22121) (kx) + \u03b2 (r\u22121) (k)\n\u03c6r (x)\n\u03bb1\n(k) * * * \u03bbr\n(k)\n(A.4)\nIf this matrix equation can be inverted, this means that any function \u03c6i (x) is\na linear combination of S(kx) + \u03b2(k) and its (r \u2212 1) derivatives with respect to k:\n\u03c6i (x) =\n\nr\nX\ni=0\n\nh\ni\n\u03b7i (k) x(i\u22121) S (i\u22121) (kx) + \u03b2 (i\u22121) (k) ,\n\n(A.5)\n\nand if this is not the case, this means that there exists an interval for k, for which\nsome of the functions \u03bbi (k) are linear combinations of the others [2]. In this case\nlet us show that we will go back to the situation of the problem of lower order r.\nLet us to assume that the last column of the matrix is a linear combination of the\nothers, i.e.;\nr\u22121\nX\n\u03b3i \u03bbi (k).\n\u03bbr (k) =\ni=1\n\nPutting this in the equation (A.1) will give\n#\n\" r\u22121\n#\n\"r\u22121\nr\u22121\nr\u22121\nX\nX\nX\nX\n\u03b3i \u03bbi (1) \u03c6r (x)\n\u03bbi (1)\u03c6i (x)+\u03b2(k)+\n\u03b3i \u03bbi (k) \u03c6r (kx) =\n\u03bbi (k)\u03c6i (kx)+\ni=1\n\ni=1\n\ni=1\n\ni=1\n\n\fand noting \u03c8i (x) = \u03c6i (x) + \u03b3i \u03c6r (x) and \u03c8i (kx) = \u03c6i (kx) + \u03b3i \u03c6r (kx) we obtain\nr\u22121\nX\n\n\u03bbi (k)\u03c8i (kx) =\n\nr\nX\n\n\u03bbi (1)\u03c8i (x) + \u03b2(k)\n\ni=1\n\ni=1\n\nwhich is an equation in the same form of (A.1), but of lower order.\nDeriving now both parts of the equation (A.5) with respect to k and noting\nkx = u we obtain\nr\nX\n\nai ui S i (u) = a\n\n(A.6)\n\ni=0\n\nThis is the general expression of a rth order Euler\u2013Cauchy differential equation\n[1, 2] which is classically solved through the change of variable u = ex , and one\ncan find the general expression of its solution in the following form:\n\nS(x) =\n\nM\nX\n\nm=1\n\nNX\nm \u22121\n\nn\n\ncmn (ln x)\n\nn=0\n\n!\n\nx\u03b1m +\n\nN0\nX\n\nc0n (ln x)n\n\nwith M = 0, * * * r, and\n\nn=0\n\nM\nX\n\nm=0\n\n(A.7)\nwhere M and Nm are integer numbers, and cmn , c0n and \u03b1m are real numbers. In\nfact the most general solution also incorporate terms of the form\n\"\nX\n\n#\n\n(ln x)n (\u03b1n cos(ln x) + \u03b2n sin(ln x)) xd\n\nn\n\nderived from complex \u03b1m and cmn . But we will not consider these terms because\nthe resulting pdf 's have oscillatory behavior around zero.\nOne can give a geometric interpretation of the solutions given in (A.7). For\nany given order r make a (r + 1) \u00d7 (r + 1) table in the form\nlnr x\n..\n.\nln2 x\nln x\n1\n\n\u00d7\n1\n\nx\u03b11\n\nx\u03b12\n\n* * * x\u03b1r\n\nand let r mass points fall down into the columns. To each filled box is assigned a\nfunction \u03c6i (x) by multiplying the corresponding powers of x and ln x on the same\nline and the same column. To illustrate this, we give in the following the three\n\nNm = r\n\n\ffirst cases:\nCase r = 1:\n\nCase r = 2:\n\n2\n\nln x b\n1\n\u00d7\n1\n\na\nb\n\na\nx\u03b11\n\n\u03c6(x)\nx\u03b11\nln x\n\nln x\nln x\n1\n\na\nb\nc\nd\n\nd\nbd\nc\n\u00d7 abc\n1 x\u03b11\n\n\u03c61 (x)\nx\u03b11\nx\u03b11\nx\u03b11\nln x\n\nCase r = 3:\n\na\n\nln3 x\nln2 x\nln x\n1\n\nx\u03b12\n\n\u03c62 (x)\nx\u03b12\nln x\nx\u03b11 ln x\nln2 x\n\na\nb\nc\nd\ne\nf\ng\n\ng\nfg\nbdf g\n\u00d7\n1\n\n\u03c61 (x)\nx\u03b11\nx\u03b11\nx\u03b11\nx\u03b11\nx\u03b11\nx\u03b11\nln x\n\nc\ndc\nabcdef\nx\u03b11\n\n\u03c62 (x)\nx\u03b12\nx\u03b12\n\u03b11\nx ln x\nx\u03b11 ln x\nx\u03b12\nln x\nln2 x\n\ne\nabe\na\nx\u03b12 x\u03b13\n\n\u03c63 (x)\nx\u03b13\nln x\nx\u03b11 ln2 x\nln x\nx\u03b12 ln x\nln2 x\nln3 x\n\n\fReferences\n1. Angot A., \"Compl\u00e9ments de math\u00e9matiques,\" Masson ed., Sixi\u00e8me \u00c9dition,\nParis, 1982.\n2. Bass J., \"Cours de math\u00e9matiques,\" Masson ed., Tome II, Quatri\u00e8me \u00c9dition,\nParis, 1968.\n3. Demoment G., \"Image Reconstruction and Restoration: Overview of Common\nEstimation Structure and Problems,\" IEEE Trans. on Acoustics, Speech, and\nSignal Processing, Vol. 37, pp:2024-2036, (1989).\n4. Gull S. F. and Skilling J., \"Maximum entropy method in image processing,\"\nIEE Proc., 131-F, pp. 646-659, 1984.\n5. Mohammad-Djafari A. and Idier J., \"Maximum entropy prior laws of images\nand estimation of their parameters,\" in T.W. Grandy (ed.), Maximum-entropy\nand Bayesian methods, Kluwer Academic Publishers, Netherlands, 1990.\n6. Mohammad-Djafari A. and Demoment G., \"Maximum entropy Fourier synthesis with application to diffraction tomography,\" Applied Optics, Vol.26,\nNo. 10, pp:1745-1754, (1987).\n7. Mohammad-Djafari A. and Demoment G., \"Maximum entropy reconstruction\nin X ray and diffraction tomography,\" IEEE Trans. on Medical Imaging, Vol.\n7, No. 4 pp:345-354, (1988).\n8. Mohammad-Djafari A., \"Bayesian Approach with Maximum Entropy Priors\nto Imaging Inverse Problems, Part I: Fundations,\" submitted to IEEE Trans.\non Image Processing, (August, 1993).\n9. Mohammad-Djafari A., \"Bayesian Approach with Maximum Entropy Priors\nto Imaging Inverse Problems, Part II: Applications,\" submitted to IEEE Trans.\non Image Processing, (August, 1993).\n10. Nguyen M.K. and Mohammad-Djafari A., \"Bayesian Maximum Entropy\nImage Reconstruction from the Microwave Scattered Field Data,\" in A.\nMohammad-Djafari and G. Demoment(ed.), Maximum Entropy and Bayesian\nMethods, Kluwer Academic Publishers, the Netherlands, 1993.\n11. Skilling J., \"Maximum-Entropy and Bayesian Methods,\" J. Skilling ed., Dordrecht: Kluwer Academic Publisher, 1988.\n\n\f"}