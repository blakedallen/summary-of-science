{"id": "http://arxiv.org/abs/0908.2918v1", "guidislink": true, "updated": "2009-08-20T12:16:50Z", "updated_parsed": [2009, 8, 20, 12, 16, 50, 3, 232, 0], "published": "2009-08-20T12:16:50Z", "published_parsed": [2009, 8, 20, 12, 16, 50, 3, 232, 0], "title": "Functional linear regression that's interpretable", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0908.2178%2C0908.2531%2C0908.2169%2C0908.0906%2C0908.3895%2C0908.1996%2C0908.0120%2C0908.0070%2C0908.0297%2C0908.0224%2C0908.1773%2C0908.4323%2C0908.2062%2C0908.1826%2C0908.1812%2C0908.1601%2C0908.3128%2C0908.0416%2C0908.1799%2C0908.2975%2C0908.1198%2C0908.0624%2C0908.0467%2C0908.3402%2C0908.4537%2C0908.3970%2C0908.4216%2C0908.4232%2C0908.0840%2C0908.2916%2C0908.3127%2C0908.4375%2C0908.0048%2C0908.1676%2C0908.2918%2C0908.4435%2C0908.4513%2C0908.1104%2C0908.0526%2C0908.3976%2C0908.2142%2C0908.2991%2C0908.0494%2C0908.3237%2C0908.0810%2C0908.0305%2C0908.1616%2C0908.1181%2C0908.1830%2C0908.1915%2C0908.0447%2C0908.1981%2C0908.1822%2C0908.2364%2C0908.1039%2C0908.0740%2C0908.0967%2C0908.4487%2C0908.3099%2C0908.3622%2C0908.1793%2C0908.2416%2C0908.2381%2C0908.2755%2C0908.1264%2C0908.4576%2C0908.0012%2C0908.2929%2C0908.2655%2C0908.0727%2C0908.0582%2C0908.3176%2C0908.3926%2C0908.4248%2C0908.3074%2C0908.4133%2C0908.4556%2C0908.4317%2C0908.2192%2C0908.3236%2C0908.4046%2C0908.3898%2C0908.3825%2C0908.0456%2C0908.3063%2C0908.0648%2C0908.3847%2C0908.4026%2C0908.3942%2C0908.0870%2C0908.1146%2C0908.4138%2C0908.4387%2C0908.2094%2C0908.3760%2C0908.1387%2C0908.0863%2C0908.2285%2C0908.2931%2C0908.2818%2C0908.1904&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Functional linear regression that's interpretable"}, "summary": "Regression models to relate a scalar $Y$ to a functional predictor $X(t)$ are\nbecoming increasingly common. Work in this area has concentrated on estimating\na coefficient function, $\\beta(t)$, with $Y$ related to $X(t)$ through\n$\\int\\beta(t)X(t) dt$. Regions where $\\beta(t)\\ne0$ correspond to places where\nthere is a relationship between $X(t)$ and $Y$. Alternatively, points where\n$\\beta(t)=0$ indicate no relationship. Hence, for interpretation purposes, it\nis desirable for a regression procedure to be capable of producing estimates of\n$\\beta(t)$ that are exactly zero over regions with no apparent relationship and\nhave simple structures over the remaining regions. Unfortunately, most fitting\nprocedures result in an estimate for $\\beta(t)$ that is rarely exactly zero and\nhas unnatural wiggles making the curve hard to interpret. In this article we\nintroduce a new approach which uses variable selection ideas, applied to\nvarious derivatives of $\\beta(t)$, to produce estimates that are both\ninterpretable, flexible and accurate. We call our method \"Functional Linear\nRegression That's Interpretable\" (FLiRTI) and demonstrate it on simulated and\nreal-world data sets. In addition, non-asymptotic theoretical bounds on the\nestimation error are presented. The bounds provide strong theoretical\nmotivation for our approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0908.2178%2C0908.2531%2C0908.2169%2C0908.0906%2C0908.3895%2C0908.1996%2C0908.0120%2C0908.0070%2C0908.0297%2C0908.0224%2C0908.1773%2C0908.4323%2C0908.2062%2C0908.1826%2C0908.1812%2C0908.1601%2C0908.3128%2C0908.0416%2C0908.1799%2C0908.2975%2C0908.1198%2C0908.0624%2C0908.0467%2C0908.3402%2C0908.4537%2C0908.3970%2C0908.4216%2C0908.4232%2C0908.0840%2C0908.2916%2C0908.3127%2C0908.4375%2C0908.0048%2C0908.1676%2C0908.2918%2C0908.4435%2C0908.4513%2C0908.1104%2C0908.0526%2C0908.3976%2C0908.2142%2C0908.2991%2C0908.0494%2C0908.3237%2C0908.0810%2C0908.0305%2C0908.1616%2C0908.1181%2C0908.1830%2C0908.1915%2C0908.0447%2C0908.1981%2C0908.1822%2C0908.2364%2C0908.1039%2C0908.0740%2C0908.0967%2C0908.4487%2C0908.3099%2C0908.3622%2C0908.1793%2C0908.2416%2C0908.2381%2C0908.2755%2C0908.1264%2C0908.4576%2C0908.0012%2C0908.2929%2C0908.2655%2C0908.0727%2C0908.0582%2C0908.3176%2C0908.3926%2C0908.4248%2C0908.3074%2C0908.4133%2C0908.4556%2C0908.4317%2C0908.2192%2C0908.3236%2C0908.4046%2C0908.3898%2C0908.3825%2C0908.0456%2C0908.3063%2C0908.0648%2C0908.3847%2C0908.4026%2C0908.3942%2C0908.0870%2C0908.1146%2C0908.4138%2C0908.4387%2C0908.2094%2C0908.3760%2C0908.1387%2C0908.0863%2C0908.2285%2C0908.2931%2C0908.2818%2C0908.1904&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Regression models to relate a scalar $Y$ to a functional predictor $X(t)$ are\nbecoming increasingly common. Work in this area has concentrated on estimating\na coefficient function, $\\beta(t)$, with $Y$ related to $X(t)$ through\n$\\int\\beta(t)X(t) dt$. Regions where $\\beta(t)\\ne0$ correspond to places where\nthere is a relationship between $X(t)$ and $Y$. Alternatively, points where\n$\\beta(t)=0$ indicate no relationship. Hence, for interpretation purposes, it\nis desirable for a regression procedure to be capable of producing estimates of\n$\\beta(t)$ that are exactly zero over regions with no apparent relationship and\nhave simple structures over the remaining regions. Unfortunately, most fitting\nprocedures result in an estimate for $\\beta(t)$ that is rarely exactly zero and\nhas unnatural wiggles making the curve hard to interpret. In this article we\nintroduce a new approach which uses variable selection ideas, applied to\nvarious derivatives of $\\beta(t)$, to produce estimates that are both\ninterpretable, flexible and accurate. We call our method \"Functional Linear\nRegression That's Interpretable\" (FLiRTI) and demonstrate it on simulated and\nreal-world data sets. In addition, non-asymptotic theoretical bounds on the\nestimation error are presented. The bounds provide strong theoretical\nmotivation for our approach."}, "authors": ["Gareth M. James", "Jing Wang", "Ji Zhu"], "author_detail": {"name": "Ji Zhu"}, "author": "Ji Zhu", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/08-AOS641", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0908.2918v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0908.2918v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/08-AOS641 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62J99 (Primary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0908.2918v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0908.2918v1", "journal_reference": "Annals of Statistics 2009, Vol. 37, No. 5A, 2083-2108", "doi": "10.1214/08-AOS641", "fulltext": "The Annals of Statistics\n2009, Vol. 37, No. 5A, 2083\u20132108\nDOI: 10.1214/08-AOS641\nc Institute of Mathematical Statistics, 2009\n\narXiv:0908.2918v1 [math.ST] 20 Aug 2009\n\nFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE1\nBy Gareth M. James, Jing Wang and Ji Zhu\nUniversity of Southern California, University of Michigan\nand University of Michigan\nRegression models to relate a scalar Y to a functional predictor\nX(t) are becoming increasingly common. Work in this area has concentrated on estimating\na coefficient function, \u03b2(t), with Y related\nR\nto X(t) through \u03b2(t)X(t) dt. Regions where \u03b2(t) 6= 0 correspond\nto places where there is a relationship between X(t) and Y . Alternatively, points where \u03b2(t) = 0 indicate no relationship. Hence, for\ninterpretation purposes, it is desirable for a regression procedure to\nbe capable of producing estimates of \u03b2(t) that are exactly zero over\nregions with no apparent relationship and have simple structures over\nthe remaining regions. Unfortunately, most fitting procedures result\nin an estimate for \u03b2(t) that is rarely exactly zero and has unnatural\nwiggles making the curve hard to interpret. In this article we introduce a new approach which uses variable selection ideas, applied to\nvarious derivatives of \u03b2(t), to produce estimates that are both interpretable, flexible and accurate. We call our method \"Functional\nLinear Regression That's Interpretable\" (FLiRTI) and demonstrate\nit on simulated and real-world data sets. In addition, non-asymptotic\ntheoretical bounds on the estimation error are presented. The bounds\nprovide strong theoretical motivation for our approach.\n\n1. Introduction. In recent years functional data analysis (FDA) has become an increasingly important analytical tool as more data has arisen where\nthe primary unit of observation can be viewed as a curve or in general a function. One of the most useful tools in FDA is that of functional regression.\nThis setting can correspond to either functional predictors or functional\nresponses. See Ramsay and Silverman (2002) and Muller and Stadtmuller\n(2005) for numerous specific applications. One commonly studied problem\nReceived April 2008; revised July 2008.\nSupported by NSF Grants DMS-07-05312, DMS-05-05432 and DMS-07-05532.\nAMS 2000 subject classifications. 62J99.\nKey words and phrases. Interpretable regression, functional linear regression, Dantzig\nselector, lasso.\n1\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2009, Vol. 37, No. 5A, 2083\u20132108. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nG. M. JAMES, J. WANG AND J. ZHU\n\ninvolves data containing functional responses. A sampling of papers examining this situation includes Fahrmeir and Tutz (1994), Liang and Zeger\n(1986), Faraway (1997), Hoover et al. (1998), Wu et al. (1998), Fan and Zhang\n(2000) and Lin and Ying (2001). However, in this paper, we are primarily\ninterested in the alternative situation, where we obtain a set of observations {Xi (t), Yi } for i = 1, . . . , n, where Xi (t) is a functional predictor and\nYi a real valued response. Ramsay and Silverman (2005) discuss this scenario and several papers have also been written on the topic, both for\ncontinuous and categorical responses, and for linear and nonlinear models [Hastie and Mallows (1993), James and Hastie (2001), Ferraty and Vieu\n(2002), James (2002), Ferraty and Vieu (2003), Muller and Stadtmuller (2005),\nJames and Silverman (2005)].\nSince our primary interest here is interpretation, we will be examining the\nstandard functional linear regression (FLR) model, which relates functional\npredictors to a scalar response via\n(1)\n\nYi = \u03b20 +\n\nZ\n\nXi (t)\u03b2(t) dt + \u03b5i ,\n\ni = 1, . . . , n,\n\nwhere \u03b2(t) is the \"coefficient function.\" We will assume that Xi (t) is scaled\nsuch that 0 \u2264 t \u2264 1. Clearly, for any finite n, it would be possible to perfectly\ninterpolate the responses if no restrictions were placed on \u03b2(t). Such restrictions generally take one of two possible forms. The first method, which we\ncall the \"basis approach,\" involves representing \u03b2(t) using a p-dimensional\nbasis function, \u03b2(t) = B(t)T \u03b7, where p is hopefully large enough to capture\nthe patterns in \u03b2(t) but small enough to regularize the fit.RWith this method\n(1) can be reexpressed as Yi = \u03b20 + XTi \u03b7 + \u03b5i , where Xi = Xi (t)B(t) dt, and\n\u03b7 can be estimated using ordinary least squares. The second method, which\nwe call the \"penalization approach,\" involves a penalized least squares estimation procedure\nto shrink variability in \u03b2(t). A standard penalty is of\nR\nthe form \u03b2 (d) (t)2 dt with dP\n= 2 being a common\nchoice. In this\none\nR\nR case\nn\n2\n(d)\nwould find \u03b2(t) to minimize i=1 (Yi \u2212 \u03b20 \u2212 Xi (t)\u03b2(t) dt) + \u03bb \u03b2 (t)2 dt\nfor some \u03bb > 0.\nAs with standard linear regression, \u03b2(t) determines the effect of Xi (t) on\nYi . For example, changes in Xi (t) have no effect on Yi over regions, where\n\u03b2(t) = 0. Alternatively, changes in Xi (t) have a greater effect on Yi over\nregions, where |\u03b2(t)| is large. Hence, in terms of interpretation, coefficient\ncurves with certain structures are more appealing than others. For example,\nif \u03b2(t) is exactly zero over large regions then Xi (t) only has an effect on Yi\nover the remaining time points. Additionally, if \u03b2(t) is constant for any given\nnon-zero region then the effect of Xi (t) on Yi remains constant within that\nregion. Finally, if \u03b2(t) is exactly linear for any given region then the change\nin the effect of Xi (t) is constant over that region. Clearly the interpretation\nof the predictor-response relationship is more difficult as the shape of \u03b2(t)\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n(a)\n\n3\n\n(b)\n\nFig. 1. (a) True beta curve (grey) generated from two quadratic curves and a section\nwith \u03b2(t) = 0. The FLiRTI estimate from constraining the zeroth and third derivative is\nshown in black. (b) Same plot for the region 0.3 \u2264 t \u2264 0.7. The dashed line is the best\nB-spline fit.\n\nbecomes more complicated. Unfortunately, the basis and penalization approaches both generate \u03b2(t) curves that exhibit wiggles and are not exactly\nlinear or constant over any region. In addition, \u03b2(t) will be exactly equal to\nzero at no more than a few locations even if there is no relationship between\nX(t) and Y for large regions of t.\nIn this paper we develop a new method, which we call \"Functional Linear\nRegression That's Interpretable\" (FLiRTI), that produces accurate but also\nhighly interpretable estimates for the coefficient function \u03b2(t). Additionally,\nit is computationally efficient, extremely flexible in terms of the form of\nthe estimate and has highly desirable theoretical properties. The key to our\n(a)\n\n(b)\n\nFig. 2. Plots of true beta curve (grey) and corresponding FLiRTI estimates (black). For\neach plot we constrained (a) zeroth and second derivative, (b) zeroth and third derivative.\n\n\f4\n\nG. M. JAMES, J. WANG AND J. ZHU\n\nprocedure is to reformulate the problem as a form of variable selection. In\nparticular we divide the time period up into a fine grid of points. We then use\nvariable selection methods to determine whether the dth derivative of \u03b2(t)\nis zero or not at each of the grid points. The implicit assumption is that the\ndth derivative will be zero at most grid points, that is, it will be sparse. By\nchoosing appropriate derivatives one can produce a large range of highly interpretable \u03b2(t) curves. Consider, for example, Figures 1\u20133, which illustrate\na range of FLiRTI fits applied to simulated data sets. In Figure 1 the true\n\u03b2(t) used to generate the data consisted of a quadratic curve and a section\nwith \u03b2(t) = 0. Figure 1(a) plots the FLiRTI estimate, produced by assuming sparsity in the zeroth and third derivatives. The sparsity in the zeroth\nderivative generates the zero section while the sparsity in the third derivative ensures a smooth fit. Figure 1(b) illustrates the same plot concentrating\non the region between 0.3 and 0.7. Notice that the corresponding B-spline\nestimate, represented by the dashed line, provides a poor approximation for\nthe region, where \u03b2(t) = 0. It is important to note that we did not specify\nwhich regions would have zero derivatives, the FLiRTI procedure is capable\nof automatically selecting the appropriate shape. In Figure 2 \u03b2(t) was chosen\nas a piecewise linear curve with the middle section set to zero. Figure 2(a)\nshows the corresponding FLiRTI estimate generated by assuming sparsity\nin the zeroth and second derivatives and is almost a perfect fit. Alternatively, one can produce a smoother, but slightly less easily interpretable fit,\nby assuming sparsity in higher-order derivatives. Figure 2(b), which concentrates on the region between t = 0.2 to t = 0.8, plots the FLiRTI fit assuming\nsparsity in the zeroth and third derivative. Notice that the sparsity in the\nthird derivative induces a smoother estimate with little sacrifice in accuracy.\nFinally, Figure 3 illustrates a FLiRTI fit applied to data generated using a\nsimple cubic \u03b2(t) curve, a situation where one might not expect FLiRTI to\nprovide any advantage over a standard approach such as using a B-spline\nbasis. However, the figure, along with the simulation results in Section 5,\nshows that even in this situation the FLiRTI method gives highly accurate\nestimates. These three examples illustrate the flexibility of FLiRTI in that\nit can produce estimates ranging from highly interpretable simple linear fits,\nthrough smooth fits with zero regions, to more complicated nonlinear structures with equal ease. The key idea here is that, given a strong signal in\nthe data, FLiRTI is flexible enough to estimate \u03b2(t) accurately. However, in\nsituations where the signal is weaker, FLiRTI will automatically shrink the\nestimated \u03b2(t) towards a more interpretable structure.\nThe paper is laid out as follows. In Section 2 we develop the FLiRTI model\nand also detail two fitting procedures, one making use of the lasso [Tibshirani\n(1996)] and the other utilizing the Dantzig selector [Candes and Tao (2007)].\nThe theoretical developments for our method are presented in Section 3\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n(a)\n\n5\n\n(b)\n\nFig. 3. (a) True beta curve (grey) generated from a cubic curve. The FLiRTI estimate\nfrom constraining the zeroth and fourth derivative is represented by the solid black line and\nthe B-spline estimate is the dashed line. (b) Estimation errors using the B-spline method\n(dashed) and FLiRTI (solid).\n\nwhere we outline both nonasymptotic bounds on the error as well as asymptotic properties of our estimate as n grows. Then, in Section 4, we extend\nthe FLiRTI method in two directions. First we show how to control multiple derivatives simultaneously, which allows us to, for example, produce a\n\u03b2(t) curve that is exactly zero in certain sections and exactly linear in other\nsections. Second, we develop a version of FLiRTI that can be applied to\ngeneralized linear models (GLM). A detailed simulation study is presented\nin Section 5. Finally, we apply FLiRTI to real world data in Section 6 and\nend with a discussion in Section 7.\n2. FLiRTI methodology. In this section we first develop the FLiRTI\nmodel and then demonstrate how we use the lasso and Dantzig selector\nmethods to fit it and hence estimate \u03b2(t).\n2.1. The FLiRTI model. Our approach borrows ideas from the basis\nand penalization methods but is rather different from either. We start in\na similar vein to the basis approach by selecting a p-dimensional basis\nB(t) = [b1 (t), b2 (t), . . . , bp (t)]T . However, instead of assuming B(t) provides\na perfect fit for \u03b2(t), we allow for some error using the model\n(2)\n\n\u03b2(t) = B(t)T \u03b7 + e(t),\n\nwhere e(t) represents the deviations of the true \u03b2(t) from our model. In\naddition, unlike the basis approach where p is chosen small to provide some\nform of regularization, we typically choose p \u226b n so |e(t)| can generally be\nassumed to be small. In Section 3 we show that the error in the estimate\n\n\f6\n\nG. M. JAMES, J. WANG AND J. ZHU\n\np\n\nfor \u03b2(t), that is, |\u03b2\u0302(t) \u2212 \u03b2(t)|, can potentially be of order log(p)/n. Hence,\nlow error rates can be achieved even for values of p much larger than n.\nOur theoretical results apply to any high dimensional basis, such as splines,\nFourier or wavelets. For the empirical results presented in this paper we\nopted to use a simple grid basis, where bk (t) equals 1 if t \u2208 Rk = {t : k\u22121\np <\nk\nt \u2264 p } and 0 otherwise.\nCombining (1) and (2) we arrive at\n(3)\nR\n\nYi = \u03b20 + XTi \u03b7 + \u03b5\u2217i ,\nR\n\nwhere Xi = Xi (t)B(t) dt and \u03b5\u2217i = \u03b5i + Xi (t)e(t) dt. Estimating \u03b7 presents\na difficulty because p > n. One could potentially estimate \u03b7 using a variable\nselection procedure except that for an arbitrary basis, B(t), there is no reason to suppose that \u03b7 will be sparse. In fact for many bases \u03b7 will contain no\nzero elements. Instead we model \u03b2(t) assuming that one or more of its derivatives are sparse, that is, \u03b2 (d) (t) = 0 over large regions of t for one or more\nvalues of d = 0, 1, 2, . . . . This model has the advantage of both constraining\n\u03b7 enough to allow us to fit (3) as well as producing a highly interpretable\nestimate for \u03b2(t). For example, \u03b2 (0) (t) = 0 guarantees X(t) has no effect on\nY at t, \u03b2 (1) (t) = 0 implies that \u03b2(t) is constant at t, \u03b2 (2) (t) = 0 means that\n\u03b2(t) is linear at t, etc.\nLet A = [Dd B(t1 ), Dd B(t2 ), . . . , Dd B(tp )]T , where t1 , t2 , . . . , tp represent\na grid of p evenly spaced points and D d is the dth finite difference operator, that is, DB(tj ) = p[B(tj ) \u2212 B(tj\u22121 )], D2 B(tj ) = p2 [B(tj ) \u2212 2B(tj\u22121 ) +\nB(tj\u22122 )], etc. Then, if\n(4)\n\n\u03b3 = A\u03b7,\n\n\u03b3j provides an approximation to \u03b2 (d) (tj ) and hence, enforcing sparsity in \u03b3\nconstrains \u03b2 (d) (tj ) to be zero at most time points. For example, one may\nbelieve that \u03b2 (2) (t) = 0 over many regions of t, that is, \u03b2(t) is exactly linear\nover large regions of t. In this situation we would let\n(5)\n\nA = [D 2 B(t1 ), D2 B(t2 ), . . . , D2 B(tp )]T ,\n\nwhich implies \u03b3j = p2 [B(tj )T \u03b7 \u2212 2B(tj\u22121 )T \u03b7 + B(tj\u22122 )T \u03b7]. Hence, provided\np is large, so t is sampled on a fine grid, and e(t) is smooth, \u03b3j \u2248 \u03b2 (2) (tj ).\nIn this case enforcing sparsity in the \u03b3j 's will produce an estimate for \u03b2(t)\nthat is linear except at the time points corresponding to nonzero values of\n\u03b3j .\nIf A is constructed using a single derivative, as in (5), then we can always\nchoose a grid of p different time points, t1 , t2 , . . . , tp such that A is a square\np by p invertible matrix. In this case \u03b7 = A\u22121 \u03b3 so we may combine (3) and\n(4) to produce the FLiRTI model\n(6)\n\nY = V \u03b3 + \u03b5\u2217 ,\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n7\n\nwhere V = [1|XA\u22121 ], 1 is a vector of ones and \u03b20 has been incorporated\ninto \u03b3. We discuss the situation with multiple derivatives, where A may no\nlonger be invertible, in Section 4.\n2.2. Fitting the model. Since \u03b3 is assumed sparse, one could potentially\nuse a variety of variable selection methods to fit (6). There has recently\nbeen a great deal of development of new model selection methods that work\nwith large values of p. A few examples include the lasso [Tibshirani (1996),\nChen, Donoho and Saunders (1998)], SCAD [Fan and Li (2001)], the Elastic\nNet [Zou and Hastie (2005)], the Dantzig selector [Candes and Tao (2007)]\nand VISA [Radchenko and James (2008)]. We opted to explore both the\nlasso and Dantzig selector for several reasons. First, both methods have\ndemonstrated strong empirical results on models with large values of p. Second, the LARS algorithm [Efron et al. (2004)] can be used to efficiently\ncompute the coefficient path for the lasso. Similarly the DASSO algorithm\n[James, Radchenko and Lv (2009)], which is a generalization of LARS, will\nefficiently compute the Dantzig selector coefficient path. Finally, we demonstrate in Section 3 that identical non-asymptotic bounds can be placed on\nthe errors in the estimates of \u03b2(t) that result from either approach.\nConsider the linear regression model Y = X\u03b2 +\u03b5. Then the lasso estimate,\nb , is defined by\n\u03b2\nL\nb = arg min 1 kY \u2212 X\u03b2k2 + \u03bbk\u03b2k ,\n\u03b2\n1\nL\n2\n\u03b2 2\n\n(7)\n\nwhere k * k1 and k * k2 respectively denote the L1 and L2 norms and \u03bb \u2265 0\nb , is\nis a tuning parameter. Alternatively, the Dantzig selector estimate, \u03b2\nDS\ngiven by\n(8)\n\nb\n\u03b2\nDS = arg min k\u03b2k1\n\u03b2\n\nsubject to |XTj (Y \u2212X\u03b2)| \u2264 \u03bb,\n\nj = 1, . . . , p,\n\nwhere Xj is the jth column of X and \u03bb \u2265 0 is a tuning parameter.\nUsing either approach the LARS or DASSO algorithms can be used to\nefficiently compute all solutions for various values of the tuning parameter,\n\u03bb. Hence, using a validation/cross-validation approach to select \u03bb can be\neasily implemented. To generate the final FLiRTI estimate we first produce\nb by fitting the FLiRTI model, (6), using either the lasso, (7) or the Dantzig\n\u03b3\nselector, (8). Note that both methods generally assume a standardized design\nmatrix with columns of norm one. Hence, we first standardize V , apply the\nlasso or Dantzig selector and then divide the resulting coefficient estimates\nb . After the coefficients, \u03b3\nb,\nby the original column norms of V to produce \u03b3\nhave been obtained we produce the FLiRTI estimate for \u03b2(t) using\n(9)\n\nb = B(t)T \u03b7\nb = B(t)T A\u22121 \u03b3\nb (\u22121) ,\n\u03b2(t)\n\nb (\u22121) represents \u03b3\nb after removing the estimate for \u03b20 .\nwhere \u03b3\n\n\f8\n\nG. M. JAMES, J. WANG AND J. ZHU\n\n3. Theoretical results. In this section we show that not only does the\nFLiRTI approach empirically produce good estimates for \u03b2(t), but that for\nany p by p invertible A we can in fact prove tight, nonasymptotic bounds\non the error in our estimate. In addition, we derive asymptotic rates of\nconvergence. Note that for notational convenience the results in this section\nassume \u03b20 = 0 and drop the intercept term from the model. However, the\ntheory all extends in a straightforward manner to the situation with \u03b20\nunknown.\nb \u03bb correspond to the lasso\n3.1. A nonasymptotic bound on the error. Let \u03b3\nsolution using tuning parameter \u03bb. Let D\u03bb be a diagonal matrix with jth\ndiagonal equal to 1, \u22121 or 0 depending on whether the jth component of \u03b3b\u03bb\nis positive, negative or zero, respectively. Consider the following condition\non the design matrix, V .\n\n(10)\n\nu = (D\u03bb \u1e7c T \u1e7c D\u03bb )\u22121 1 \u2265 0 and\n\nk\u1e7c T \u1e7c D\u03bb uk\u221e \u2264 1,\n\nwhere \u1e7c corresponds to V after standardizing its columns, 1 is a vector of ones and the inequality for vectors is understood componentwise.\nJames, Radchenko and Lv (2009) prove that when (10) holds the Dantzig\nselector's nonasymptotic bounds [Candes and Tao (2007)] also apply for the\nlasso. Our Theorem 1 makes use of (10) to provide a nonasymptotic bound\non the L2 error in the FLiRTI estimate, using either the Dantzig selector\nor the lasso. Note that the values \u03b4, \u03b8 and Cn,p (t) are all known constants\nwhich we have defined in the proof of this result provided in Appendix A.\nTheorem 1. For a given p-dimensional basis Bp (t), let \u03c9p = supt |ep (t)|\nand \u03b3 p = A\u03b7 p , where A is a p by p matrix. Suppose that \u03b3 p has at most\nV + \u03b8V\nSp non-zero components and \u03b42S\nSp ,2Sp < 1. Further, suppose that we\np\nestimate \u03b2(t) using the FLiRTI estimate given by (9) using any value of \u03bb\nsuch that (10) holds and\nmax |\u1e7c T \u03b5\u2217 | \u2264 \u03bb.\n\n(11)\nThen, for every 0 \u2264 t \u2264 1,\n(12)\n\nq\nb \u2212 \u03b2(t)| \u2264 \u221a1 Cn,p (t)\u03bb Sp + \u03c9p .\n|\u03b2(t)\nn\n\nThe constants \u03b4 and \u03b8 are both measures of the orthogonality of V . The\nV +\ncloser they are to zero the closer V is to orthogonal. The condition \u03b42S\np\nV\n\u03b8Sp ,2Sp < 1, which is utilized in the paper of Candes and Tao (2007), ensures\nthat \u03b2(t) is identifiable. It should be noted that (10) is only required when\nusing the lasso to compute FLiRTI. The above results hold for the Dantzig\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n9\n\nselector even if (10) is violated. While this is a slight theoretical advantage\nfor the Dantzig selector, our simulation results suggest that both methods\nperform well in practice. Theorem 1 suggests that our optimal choice for \u03bb\nwould be the lowest value such that (11) holds. Theorem 2 shows how to\nchoose \u03bb such that (11) holds with high probability.\nTheorem\n2. Suppose that \u03b5i \u223c N (0, \u03c312 ) and that there exits\npan M < \u221e\nR\nsuch \u221a\nthat |Xi (t)| dt \u2264 M for all i. Then for any \u03c6 \u2265 0, if \u03bbp= \u03c31 2(1 + \u03c6) log p+\nM \u03c9p n then (11) will hold with probability at least 1\u2212(p\u03c6 4\u03c0(1 + \u03c6) log p)\u22121 ,\nand hence\nq\nb \u2212 \u03b2(t)| \u2264 \u221a1 Cn,p (t)\u03c31 2Sp (1 + \u03c6) log p\n|\u03b2(t)\nn\n(13)\nq\n+ \u03c9p {1 + Cn,p (t)M Sp }.\nIn addition, if we assume\n\u03b5\u2217i \u223c N (0, \u03c322 ) then (11) will hold with the same\np\nprobability for \u03bb = \u03c32 2(1 + \u03c6) log p in which case\n\n(14)\n\nq\nb \u2212 \u03b2(t)| \u2264 \u221a1 Cn,p (t)\u03c32 2Sp (1 + \u03c6) log p + \u03c9p .\n|\u03b2(t)\nn\n\nNote that (13) and (14) are non-asymptotic results that hold, with high\nprobability, for any n or p. One can show that, under suitable conditions,\nCn,p (t) converges to a constant as n and\np p grow. In this case the first terms\nof (13) and (14) are proportional to log p/n while the second terms will\ngenerally shrink as \u03c9p declines with p. For example, using the piecewise\nconstant basis it is easy to show that \u03c9p converges to zero at a rate of 1/p\nprovided \u03b2 \u2032 (t) is bounded. Alternatively using a piecewise polynomial basis\nof order d then \u03c9p converges to zero at a rate of 1/pd+1 provided \u03b2 (d+1) (t)\nis bounded.\n3.2. Asymptotic rates of convergence. The bounds presented in Theob\nrem 1 can be used to derive asymptotic rates of convergence for \u03b2(t)\nas n\nand p grow. The exact convergence rates are dependent on the choice of\nBp (t) and A so we first state A-1 through A-6, which give general conditions for convergence. We show in Theorems 3\u20136 that these conditions are\nsufficient to guarantee convergence for any choice of Bp (t) and A, and then\nCorollary 1 provides specific examples where the conditions can be shown\nV\nVn,p\nto hold. Let \u03b1n,p (t) = (1 \u2212 \u03b42Sn,p + \u03b8S,2S\n)Cn,p .\nA-1 There exists S < \u221e such that Sp \u2264 S for all p.\nA-2 There exists m > 0 such that pm \u03c9p is bounded, that is, \u03c9p \u2264 H/pm\nfor some H < \u221e.\n\n\f10\n\nG. M. JAMES, J. WANG AND J. ZHU\n\nA-3 For a given t, there exists bt such that p\u2212bt \u03b1n,p (t) is bounded for all\nn and p.\nA-4 There exists c such that p\u2212c supt \u03b1n,p (t) is bounded for all n and p.\nV \u2217\nVn,p\u2217\nA-5 There exists a p\u2217 such that \u03b42Sn,p + \u03b8S,2S\nis bounded away from one\nfor large enough n.\nV\nVn,pn\nA-6 \u03b42Sn,pn + \u03b8S,2S\nis bounded away from one for large enough n, where\nn \u2192 \u221e, pn \u2192 \u221e and pn /n \u2192 0.\nA-1 states that the number of changes in the derivative of \u03b2(t) is bounded.\nA-2 assumes that the bias in our estimate for \u03b2(t) converges to zero at the\nrate of pm , for some m > 0. A-3 requires that \u03b1n,pn (t) grows no faster than\npbt . A-4 is simply a stronger form of A-3. A-5 and A-6 both ensure that\nthe design matrix is close enough to orthogonal for (13) to hold and hence\nimposes a form of identifiability on \u03b2(t). For the following p\ntwo theorems we\nassume\n\u221a that the conditions in Theorem 1 hold, \u03bb is set to \u03c31 2(1 + \u03c6) log p +\nM \u03c9p n and \u03b5i \u223c N (0, \u03c312 ).\nTheorem 3. Suppose A-1 through A-5 all hold and we fix p = p\u2217 . Then,\nwith arbitrarily high probability, as n \u2192 \u221e,\n|\u03b2bn (t) \u2212 \u03b2(t)| \u2264 O(n\u22121/2 ) + En (t)\n\nand\n\nsup |\u03b2bn (t) \u2212 \u03b2(t)| \u2264 O(n\u22121/2 ) + sup En (t),\nt\n\nwhere En (t) =\n\nt\n\nH\n\u2217\np\u2217m {1 + Cn,p (t)M\n\n\u221a\n\nS}.\n\nMore specifically, the probability referred to in Theorem 3 converges to one\nas \u03c6 \u2192 \u221e. Theorem 3 states that, with our weakest set of assumptions, fixing\np as n \u2192 \u221e will cause the FLiRTI estimate to be asymptotically within En (t)\nof the true \u03b2(t). En (t) represents the bias in the approximation caused by\nrepresenting \u03b2(t) using a p dimensional basis.\nTheorem 4. Suppose we replace A-5 with A-6. Then, provided bt and\nc are less than m, if we let p grow at the rate of n1/(2m) ,\n\u0012 \u221a\n\u0013\nlog n\nb\n|\u03b2n (t) \u2212 \u03b2(t)| = O 1/2\u2212b /(2m)\nt\nn\n\nand\n\nb \u2212 \u03b2(t)| = O\nsup |\u03b2(t)\nt\n\n\u0012\n\n\u221a\n\nlog n\n\nn1/2\u2212c/(2m)\n\n\u0013\n\n.\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n11\n\nTheorem 4 shows that, assuming A-6 holds, \u03b2bn (t) will converge to \u03b2(t) at\nthe given convergence rate. With additional assumptions, stronger results\nare possible. In the Appendix we present Theorems 5 and 6, which provide\nfaster rates of convergence under the additional assumption that \u03b5\u2217i has a\nmean zero Gaussian distribution.\nTheorems 3\u20136 make use of assumptions A-1 to A-6. Whether these assumptions hold in practice depends on the choice of basis function and A\nmatrix. Corollary 1 below provides one specific example where conditions\nA-1 to A-4 can be shown to hold.\nCorollary 1. Suppose we divide the time interval [0, 1] into p equal\nregions and use the piecewise constant basis. Let A be the second difference\nmatrix given by (23). Suppose that Xi (t) is bounded above zero for all i\nand t. Then, provided \u03b2 \u2032 (t) is bounded and \u03b2 \u2032\u2032 (t) 6= 0 at a finite number of\npoints, A-1, A-2 and A-3 all hold with m = 1, b0 = 0 and bt = 0.5, 0 < t < 1.\nIn addition, for t bounded away from one, A-4 will also hold with c = 0.5.\nHence, if A-5 holds and \u03b5\u2217i \u223c N (0, \u03c322 ),\n|\u03b2bn (t) \u2212 \u03b2(t)| \u2264 O(n\u22121/2 ) +\n\nH\np\u2217\n\nand\n\nsup |\u03b2bn (t) \u2212 \u03b2(t)| \u2264 O(n\u22121/2 ) +\nt\n\nAlternatively, if A-6 holds and \u03b5\u2217i \u223c N (0, \u03c322 ),\n\uf8f1 \u0012\u221a\n\u0013\n\uf8f4\nlog n\n\uf8f4\n\uf8f4\n,\n\uf8f2O\nn1/2\n|\u03b2bn (t) \u2212 \u03b2(t)| =\n\u0012\u221a\n\u0013\n\uf8f4\nlog n\n\uf8f4\n\uf8f4\n,\n\uf8f3O\nn1/3\n\nH\n.\np\u2217\n\nt = 0,\n0 < t < 1,\n\nand\n\nsup\n0<t<1\u2212a\n\n|\u03b2bn (t) \u2212 \u03b2(t)| = O\n\n\u0012\u221a\n\nlog n\nn1/3\n\n\u0013\n\nfor any a > 0. Similar, though slightly weaker, results hold when \u03b5\u2217i does not\nhave a Gaussian distribution.\nNote that the choice of t = 0 for the faster rate of convergence is simply made\nfor notational convenience. By appropriately choosing A we can achieve this\nrate for any fixed value of t or indeed for any finite set of time points. In\naddition, the choice of the piecewise constant basis was made for simplicity.\nSimilar results can be derived for higher-order polynomial bases in which\ncase A-2 will hold with a higher m and hence faster rates of convergence\nwill be possible.\n\n\f12\n\nG. M. JAMES, J. WANG AND J. ZHU\n\n4. Extensions. In this section we discuss two useful extensions of the\nbasic FLiRTI methodology. First, in Section 4.1, we show how to control\nmultiple derivatives simultaneously to allow more flexibility in the types of\npossible shapes one can produce. Second, in Section 4.2, we extend FLiRTI\nto GLM models.\n4.1. Controlling multiple derivatives. So far we have concentrated on\ncontrolling a single derivative of \u03b2(t). However, one of the most powerful\naspects of the FLiRTI approach is that we can combine constraints for multiple derivatives together to produce curves with many different properties.\nFor example, one may believe that both \u03b2 (0) (t) = 0 and \u03b2 (2) (t) = 0 over\nmany regions of t, that is, \u03b2(t) is exactly zero over certain regions and \u03b2(t)\nis exactly linear over other regions of t. In this situation, we would let\nA = [D0 B(t1 ), D0 B(t2 ), . . . , D0 B(tp ), D2 B(t1 ), D2 B(t2 ), . . . , D2 B(tp )]T .\n(15)\nIn general, such a matrix will have more rows than columns so will not be\ninvertible. Let A(1) represent the first p rows of A and A(2) the remainder.\nSimilarly, let \u03b3 (1) represent the first p elements of \u03b3 and \u03b3 (2) the remaining elements. Then, assuming A is arranged so that A(1) is invertible, the\nconstraint \u03b3 = A\u03b7 implies\n(16)\n\n\u03b7 = A\u22121\n(1) \u03b3 (1)\n\nand \u03b3 (2) = A(2) A\u22121\n(1) \u03b3 (1) .\n\nHence, (3) can be expressed as\n(17)\n\nT\n\nT\n\u2217\nYi = \u03b20 + (A\u22121\n(1) Xi ) \u03b3 (1) + \u03b5i ,\n\ni = 1, . . . , n.\n\nWe then use this model to estimate \u03b3 subject to the constraint given by\n(16). We achieve this by implementing the Dantzig selector or lasso in a\nsimilar fashion to that in Section 2 except that we replace the old design\nmatrix with V(1) = [1|XA\u22121\n(1) ] and (16) is enforced in addition to the usual\nconstraints.\nFinally, when constraining multiple derivatives one may well not wish to\nplace equal weight on each derivative. For example, for the A given by (15),\nwe may wish to place a greater emphasis on sparsity in the second derivative\nthan in the zeroth, or vice versa. Hence, instead of simply minimizing the\nL1 norm of \u03b3 we minimize k\u03a9\u03b3k1 , where \u03a9 is a diagonal weighting matrix.\nIn theory a different weight could be chosen for each \u03b3j but in practice this\nwould not be feasible. Instead, for an A such as (15), we place a weight of\none on the second derivatives and select a single weight, \u03c9, chosen via crossvalidation, for the zeroth derivatives. This approach provides flexibility while\nstill being computationally feasible and has worked well on all the problems\nwe have examined. The FLiRTI fits in Figures 1\u20133 were produced using this\nmultiple derivative methodology.\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n13\n\n4.2. FLiRTI for functional generalized linear models. The FLiRTI model\ncan easily be adapted to GLM data where the response is no longer assumed\nto be Gaussian. James and Radchenko (2009) demonstrate that the Dantzig\nselector can be naturally extended to the GLM domain by optimizing\n(18)\n\nmin k\u03b2k1\n\u03b2\n\nsubject to\n\n|XTj (Y \u2212 \u03bc)| \u2264 \u03bb,\n\nj = 1, . . . , p,\n\nwhere \u03bc = g\u22121 (X\u03b2) and g is the canonical link function. Optimizing (18)\nno longer involves linear constraints but it can be solved using an iterative\nweighted linear programming approach. In particular, let Ui be\nthe condiP\ntional variance of Yi given the current estimate \u03b2\u0302 and let Zi = pj=1 Xij \u03b2\u0302 +\nb i )/Ui . Then one can apply the\u221astandard Dantzig selector, using Zi\u2217 =\n(Yi\u221a\u2212 \u03bc\n\u2217 =X\nZi Ui as the response and Xij\nij Ui as the predictor, to produce a new\nestimate for \u03b2\u0302. James and Radchenko (2009) show that iteratively applying\nthe Dantzig selector in this fashion until convergence generally does a good\njob of solving (18). We apply the same approach, except that we iteratively\napply the modified version of the Dantzig selector, outlined in Section 4.1, to\nthe transformed response and predictor variables, using V(1) = [1|XA\u22121\n(1) ] as\nthe design matrix. James and Radchenko (2009) also suggest an algorithm\nfor approximating the GLM Dantzig selector coefficient path for different\nvalues of \u03bb. With minor modifications, this algorithm can also be used to\nconstruct the GLM FLiRTI coefficient path.\n4.3. Model selection. To fit FLiRTI one must select values for three different tuning parameters, \u03bb, \u03c9 and the derivative to assume sparsity in, d.\nThe choice of \u03bb and d in the FLiRTI setting is analogous to the choice of\nthe tuning parameters in a standard smoothing situation. In the smoothing\nsituation one observes n pairs of (xi , yi ) and chooses g(t) to minimize\n(19)\n\nX\ni\n\n(yi \u2212 g(xi ))2 + \u03bb\n\nZ\n\n{g(d) (t)}2 dt.\n\nIn this case the second term, which controls the smoothness of the curve,\ninvolves two tuning parameters, namely \u03bb and the derivative, d. The choice\nof d in the smoothing situation is completely analogous to the choice of\nthe derivative in FLiRTI. As with FLiRTI, different choices will produce\ndifferent shapes. A significant majority of the time d is set to 2 resulting in\na cubic spline. If the data is used to select d then the most common approach\nis to choose the values of \u03bb and d that produce the lowest cross-validated\nresidual sum of squares.\nWe adopt the later approach with FLiRTI. In particular we implement\nFLiRTI using two derivatives, the zeroth and a second derivative, d with d\ntypically chosen from the values d = 1, 2, 3, 4. We then compute the crossvalidated residual sum of squares for d = 1, 2, 3, 4 and a grid of values for\n\n\f14\n\nG. M. JAMES, J. WANG AND J. ZHU\n\n\u03bb and \u03c9. The final tuning parameters are those corresponding to the lowest cross-validated value. Even though this approach involves three tuning\nparameters, it is still computationally feasible because there are only a few\nvalues of d to test out and, in practice, the results are relatively insensitive\nto the exact value of \u03c9 so only a few grids points need to be considered. Additional computational savings are produced if one sets \u03c9 to zero. This has\nthe effect of reducing the number of tuning parameters to two by restricting\nFLiRTI to assume sparsity in only one derivative. We explore this option in\nthe simulation section below and show that this restriction can often still\nproduce good results.\n5. Simulation study. In this section, we use a comprehensive simulation\nstudy to demonstrate four versions of the FLiRTI method, and compare the\nresults with the basis approach using B-spline bases. The first two versions of\nFLiRTI, \"FLiRTIL \" and \"FLiRTID ,\" respectively use the lasso and Dantzig\nmethods assuming sparsity in the zeroth and one other derivative. The second two versions, \"FLiRTI1L \" and \"FLiRTI1D ,\" do not assume sparsity in\nthe zeroth derivative but are otherwise identical to the first two implementations. We consider three cases. The details of the simulation models are\nas follows.\n\u2022 Case I: \u03b2(t) = 0 (no signal).\n\u2022 Case II: \u03b2(t) is piecewise quadratic with a \"flat\" region (see Figure 1).\nSpecifically,\n\u03b2(t) =\n\n\uf8f1\n\uf8f2 (t \u2212 0.5)2 \u2212 0.025,\n\n0,\n\uf8f3\n\u2212(t \u2212 0.5)2 + 0.025,\n\nif 0 \u2264 t < 0.342,\nif 0.342 \u2264 t \u2264 0.658,\nif 0.658 < t \u2264 1.\n\n\u2022 Case III: \u03b2(t) is a cubic curve (see Figure 3), that is,\n\u03b2(t) = t3 \u2212 1.6t2 + 0.76t + 1,\n\n0 \u2264 t \u2264 1.\n\nThis is a model where one might not expect FLiRTI to have any advantage\nover the B-spline method.\nIn each case, we consider three different types of X(t).\n\u2022 Polynomials: X(t) = a0 + a1 t + a2 t2 + a3 t3 , 0 \u2264 t \u2264 1.\n\u2022 Fourier: X(t) = a0 +a1 sin(2\u03c0t)+a2 cos(2\u03c0t)+a3 sin(4\u03c0t)+a4 cos(4\u03c0t), 0 \u2264\nt \u2264 1.\n\u2022 B-splines: X(t) is a linear combination of cubic B-splines, with knots at\n1/7, . . . , 6/7.\nThe coefficients in X(t) are generated from the standard normal distribution. The error term \u03b5 in (1) follows a normal distribution N (0, \u03c3 2 ), where\n\u03c3 2 is set equal to 1 for the first case and appropriate values for other cases\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n15\n\nTable 1\nThe columns are for different methods. The rows are for different X(t). The numbers\noutside the parentheses are the average MSEs over 100 repetitions, and the numbers\ninside the parentheses are the corresponding standard errors\nB-spline\n\nFLiRTIL\n\nFLiRTID\n\n2.10 (0.14)\n1.90 (0.18)\n2.40 (0.32)\n\n0.99 (0.14)\n0.53 (0.09)\n0.82 (0.14)\n\n0.38 (0.09)\n0.47 (0.11)\n0.45 (0.11)\n\n1.5 (0.16)\n1.40 (0.15)\n1.40 (0.22)\n\n0.52 (0.11)\n0.50 (0.10)\n0.57 (0.14)\n\nCase II (\u00d710\u22125 )\nPolynomial 1.20 (0.12)\nFourier\n3.90 (0.32)\nB-spline\n0.52 (0.03)\n\n0.85 (0.09)\n3.40 (0.27)\n0.44 (0.03)\n\n0.72 (0.09)\n3.30 (0.29)\n0.37 (0.03)\n\n0.92(0.09)\n3.50 (0.30)\n0.43 (0.03)\n\n0.92 (0.08)\n3.60 (0.28)\n0.46 (0.03)\n\nCase III (\u00d710\u22122 )\nPolynomial 0.96 (0.11)\nFourier\n0.79 (0.11)\nB-spline\n0.080 (0.007)\n\n0.60 (0.07)\n0.43 (0.05)\n0.066 (0.007)\n\n0.74 (0.08)\n0.62 (0.06)\n0.074 (0.008)\n\n0.57(0.08)\n0.44 (0.05)\n0.063(0.005)\n\n0.66 (0.08)\n0.46 (0.06)\n0.070 (0.008)\n\n\u22122\n\nCase I (\u00d710\nPolynomial\nFourier\nB-spline\n\nFLiRTI1L\n\nFLiRTI1D\n\n)\n\nsuch that each of the signal to noise ratios, Var(f (X))/ Var(\u03b5), is equal to 4.\nWe generate n = 200 training observations from each of the above models,\nalong with 10,000 test observations.\nAs discussed in Section 4.3, fitting FLiRTI requires choosing three tuning\nparameters, \u03bb, \u03c9 and d, the second derivative to penalize. For the B-spline\nmethod, the tuning parameters include the order of the B-spline and the\nnumber of knots (the location of the knots is evenly spaced between 0 and\n1). To ensure a fair comparison between the two methods, for each training\ndata set, we generate a separate validation data set also containing 200\nobservations. The validation set is then used to select tuning parameters\nthat minimize the validation error. Using the selected tuning parameters,\nwe calculate the mean squared error (MSE) on the test set. We repeat this\n100 times and compute the average MSEs and their corresponding standard\nerrors. The results are summarized in Table 1.\nAs we can see, in terms of prediction accuracy, the FLiRTI methods perform consistently better than the B-spline method. Since the FLiRTI1 methods do not search for \"flat\" regions their results deteriorated somewhat for\nCases I and II over standard FLiRTI, correspondingly the results improve\nslightly in Case III. However, in all cases all four versions of FLiRTI outperform the B-spline method. This is particularly interesting for Case III. Both\nFLiRTI and the B-spline method can potentially model Case III exactly but\nonly if the correct value for d is chosen in FLiRTI and the correct order\nin the B-spline. Since these tuning parameters are chosen automatically using a separate validation set neither method has an obvious advantage yet\n\n\f16\n\nG. M. JAMES, J. WANG AND J. ZHU\nTable 2\nThe table contains the percentage of truly identified zero region. The rows are for\ndifferent X(t). The numbers outside the parentheses are the averages over 100\nrepetitions, and the numbers inside the parentheses are the corresponding\nstandard errors\nFLiRTIL\n\nFLiRTID\n\nCase I\nPolynomial\nFourier\nB-spline\n\n61% (6%)\n91% (2%)\n54% (6%)\n\n65% (6%)\n71% (6%)\n74% (6%)\n\nCase II\nPolynomial\nFourier\nB-spline\n\n70% (6%)\n72% (5%)\n58% (5%)\n\n70% (6%)\n72% (5%)\n59% (5%)\n\nFLiRTI still outperforms the B-spline approach. The lasso and the Dantzig\nselector implementations of FLiRTI perform similarly, with the Dantzig selector having an edge in the first case, while lasso has a slight advantage in\nthe third case. Finally, for Cases I and II, we also computed the fraction of\nthe zero regions that FLiRTI correctly identified. The results are presented\nin Table 2.\n6. Canadian weather data. In this section we demonstrate the FLiRTI\napproach on a classic functional linear regression data set. The data consisted of one year of daily temperature measurements from each of 35 Canadian weather stations. Figure 4(a) illustrates the curves for 9 randomly selected stations. We also observed the total annual rainfall, on the log scale,\nat each weather station. The aim was to use the temperature curves to predict annual rainfall at each location. In particular, we were interested in\nidentifying the times of the year that have an effect on rainfall. Previous\nresearch suggested that temperatures in the summer months may have little\nor no relationship to rainfall whereas temperatures at other times do have an\neffect. Figure 4(b) provides an estimate for \u03b2(t) achieved using the B-spline\nbasis approach outlined in the previous section. In this case we restricted\nthe values at the start and the end of the year to be equal and chose the\ndimension, q = 4, using cross-validation. The curve suggests a positive relationship between temperature and rainfall in the fall months and a negative\nrelationship in the spring. There also appears to be little relationship during\nthe summer months. However, because of the restricted functional form of\nthe curve, there are only two points, where \u03b2(t) = 0.\nThe corresponding estimate from the FLiRTI approach, after dividing\nthe yearly data into a grid of 100 equally spaced points and restricting the\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n17\n\nFig. 4. (a) Smoothed daily temperature curves for 9 of 35 Canadian weather stations. (b)\nEstimated beta curve using a natural cubic spline. (c) Estimated beta curve using FLiRTI\napproach (black) with cubic spline estimate (grey).\n\nzeroth and third derivatives, is presented in Figure 4(c) (black line) with\nthe spline estimate in grey. The choice of \u03bb and \u03c9 were made using tenfold\ncross-validation. The FLiRTI estimate also indicates a negative relationship\nin the spring and a positive relationship in the late fall but no relationship\nin the summer and winter months. In comparing the B-spline and FLiRTI\nfits, both are potentially reasonable. The B-spline fit suggests a possible\ncos/sin relationship, which seems sensible given that the climate pattern is\nusually seasonal. Alternatively, the FLiRTI fit produces a simple and easily\ninterpretable result. In this example, the FLiRTI estimate seemed to be\nslightly more accurate with 10 fold cross-validated sum of squared errors of\n4.77 vs 5.70 for the B-spline approach.\nIn addition to estimates for \u03b2(t), one can also easily generate confidence\nintervals and tests of significance. We illustrate these ideas in Figure 5.\nPointwise confidence intervals on \u03b2(t) can be produced by bootstrapping\nthe pairs of observations {Yi , Xi (t)}, reestimating \u03b2(t) and then taking the\nappropriate empirical quantiles from the estimated curves at each time point.\nFigures 5(a) and (b) illustrate the estimates from restricting the first and\nthird derivatives, respectively, along with the corresponding 95% confidence\nintervals. In both cases the confidence intervals confirm the statistical significance of the positive relationship in the fall months. The significance of\nthe negative relationship in the spring months is less clear since the upper\n\n\f18\n\n(a)\n\nG. M. JAMES, J. WANG AND J. ZHU\n\n(b)\n\n(c)\n\nFig. 5. (a) Estimated beta curve from constraining the zeroth and first derivatives. (b)\nEstimated beta curve from constraining the zeroth and third derivatives. The dashed lines\nrepresent 95% confidence intervals. (c) R2 from permuting the response variable 500 times.\nThe grey line represents the observed R2 from the true data.\n\nbound is at zero. However, this is somewhat misleading because approximately 96% of the bootstrap curves did include a dip in the spring but,\nbecause the dips occurred at slightly different times, their effect canceled\nout to some extent. Some form of curve registration may be appropriate\nbut we will not explore that here. Note that the bootstrap estimates also\nconsistently estimate zero relationship during the summer months providing\nfurther evidence that there is little effect from temperature in this period.\nFinally, Figure 5(c) illustrates a permutation test we developed for testing\nstatistical significance of the relationship between temperature and rainfall.\nThe grey line indicates the value of R2 (0.73) for the FLiRTI method applied\nto the weather data. We then permuted the response variable 500 times and\nfor each permutation computed the new R2 . All 500 permuted R2 's were\nwell below 0.73, providing very strong evidence of a true relationship.\n7. Discussion. The approach presented in this paper takes a departure\nfrom the standard regression paradigm, where one generally attempts to\nminimize an L2 quantity, such as the sum of squared errors, subject to an\nadditional penalty term. Instead we attempt to find the sparsest solution,\nin terms of various derivatives of \u03b2(t), subject to the solution providing\na reasonable fit to the data. By directly searching for sparse solutions we\nare able to produce estimates that have far simpler structure than that from\ntraditional methods while still maintaining the flexibility to generate smooth\ncoefficient curves when required/desired. The exact shape of the curve is\ngoverned by the choice of derivatives to constrain, which is analogous to the\nchoice of the derivative to penalize in a traditional smoothing spline. The\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n19\n\nfinal choice of derivatives can be made either on subjective grounds, such as\nthe tradeoff between interpretability and smoothness, or using an objective\ncriteria, such as the derivative producing the lowest cross validated error.\nThe theoretical bounds\nderived in Section 3, which show the error rate can\n\u221a\ngrow as slowly as log p, as well as the empirical results, suggest that one\ncan choose an extremely flexible basis, in terms of a large value for p, without\nsacrificing prediction accuracy.\nThere has been some previous work along these lines. For example,\nTibshirani et al. (2005) uses an L1 lasso-type penalty on both the zeroth\nand first derivatives of a set of coefficients to produce an estimate which is\nboth exactly zero over some regions and exactly constant over other regions.\nValdes\u2013Sosa et al. (2005) also uses a combination of both L1 and L2 penalties on fMRI data. Probably the work closest to ours is a recent approach by\nLu and Zhang (2008), called the \"functional smooth lasso\" (FSL), that was\ncompleted at the same time as FLiRTI. The FSL uses a lasso-type approach\nby placing an L1 penalty on the zeroth derivative and an L2 penalty on the\nsecond derivative. This is a nice approach and, as with FLiRTI, produces regions where \u03b2(t) is exactly zero. However, our approach can be differentiated\nfrom these other methods in that we consider derivatives of different order\nso FLiRTI can generate piecewise constant, linear and quadratic sections.\nIn addition FLiRTI, possesses interesting theoretical properties in terms of\nthe nonasymptotic bounds.\nAPPENDIX A: PROOF OF THEOREM 1\nWe first present definitions of \u03b4, \u03b8 and Cn,p (t). The definitions of \u03b4 and \u03b8\nwere first introduced in Candes and Tao (2005).\nDefinition 1. Let X be an n by p matrix and let XT , T \u2282 {1, . . . , p}\nbe the n by |T | submatrix obtained by standardizing the columns of X and\nextracting those corresponding to the indices in T . Then we define \u03b4SX as\nthe smallest quantity such that (1 \u2212 \u03b4SX )kck22 \u2264 kXT ck22 \u2264 (1 + \u03b4SX )kck22 for\nall subsets T with |T | \u2264 S and all vectors c of length |T |.\nDefinition 2. Let T and T \u2032 be two disjoint sets with T, T \u2032 \u2282 {1, . . . , p},\nX\n|T | \u2264 S and |T \u2032 | \u2264 S \u2032 . Then, provided S + S \u2032 \u2264 p, we define \u03b8S,S\n\u2032 as the\n\u2032\n\u2032\nX\nT\n\u2032\nsmallest quantity such that |(XT c) XT c | \u2264 \u03b8S,S \u2032 kck2 kc k2 for all T and T \u2032\nand all corresponding vectors c and c\u2032 .\nFinally, let Cn,p (t) =\n\n4\u03b1n,p (t)\n,\nV \u2212\u03b8 V\n1\u2212\u03b42S\nS,2S\n\nv\nu p\nuX\n\u03b1n,p (t) = t\n\nj=1\n\n1/n\n\nPn\n\nwhere\n2\n(Bp (t)T A\u22121\nj )\n\nR\n\ni=1 (\n\n2\nXi (s)Bp (s)T A\u22121\nj ds)\n\n.\n\n\f20\n\nG. M. JAMES, J. WANG AND J. ZHU\n\nNext, we state a lemma which is a direct consequence of Theorem 2 in James,\nRadchenko and Lv (2009) and Theorem 1.1 in Candes and Tao (2007). The\nlemma is utilized in the proof of Theorem 1.\nLemma 1. Let Y = X\u0303 \u03b3\u0303 + \u03b5, where X\u0303 has norm one columns. Suppose\nX + \u03b8X\nb\u0303\nthat \u03b3\u0303 is an S-sparse vector with \u03b42S\nS,2S < 1. Let \u03b3 be the corresponding\n\nb\u0303 \u2212 \u03b3\u0303k \u2264\nsolution from the Dantzig selector or the lasso. Then k\u03b3\n\nprovided that (10) and max |X\u0303 T \u03b5| \u2264 \u03bb both hold.\n\n\u221a\n4\u03bb S\nX \u2212\u03b8 X\n1\u2212\u03b42S\nS,2S\n\nLemma 1 extends Theorem 1.1 in Candes and Tao (2007), which deals\nonly with the Dantzig selector, to the lasso. Now we provide the proof of\nTheorem 1. First note that the functional linear regression model given by\n(6) can be reexpressed as,\n(20)\n\nY = V \u03b3 + \u03b5\u2217 = \u1e7c \u03b3\u0303 + \u03b5\u2217 ,\n\nwhere \u03b3\u0303 = Dv \u03b3 and Dv is a diagonal matrix consisting of the\u221acolumn norms\n4\u03bb S\nb\u0303 \u2212 \u03b3\u0303k \u2264\nb \u2212 Dv \u03b3k = k\u03b3\nof V . Hence, by Lemma 1, kDv \u03b3\nprovided\n1\u2212\u03b4V \u2212\u03b8 V\n2S\n\nS,2S\n\n(11) holds.\nb = Bp (t)T A\u22121 \u03b3\nb\u0303 while \u03b2(t) = Bp (t)T \u03b7 +ep (t) =\nb = Bp (t)T A\u22121 Dv\u22121 \u03b3,\nBut \u03b2(t)\nBp (t)T A\u22121 Dv\u22121 \u03b3\u0303 + ep (t). Then\nb \u2212 \u03b2(t)| \u2264 |\u03b2(t)\nb \u2212 B (t)T \u03b7| + |e (t)|\n|\u03b2(t)\np\np\n\nb\u0303 \u2212 \u03b3\u0303)k + |ep (t)|\n= kBp (t)T A\u22121 Dv\u22121 (\u03b3\nb\u0303 \u2212 \u03b3\u0303k + \u03c9p\n\u2264 kBp (t)T A\u22121 Dv\u22121 k * k\u03b3\n\n1\nb\u0303 \u2212 \u03b3\u0303k + \u03c9p\n= \u221a \u03b1n,p (t)k\u03b3\nn\np\n\n1 4\u03b1n,p (t)\u03bb Sp\n\u2264\u221a\n+ \u03c9p .\nV \u2212 \u03b8V\nn 1 \u2212 \u03b42S\nSp ,2Sp\np\nAPPENDIX B: PROOF OF THEOREM 2\np\n\u221a\nSubstituting\n\u03bb = \u03c31 2(1 + \u03c6) log p + M \u03c9 n into (12) gives (13). Let\nR\n\u03b5\u2032i = Xi (t)ep (t) dt. Then to show that (11) holds with the appropriate probability note that\n|\u1e7cjT \u03b5\u2217 | = |\u1e7cjT \u03b5 + \u1e7cjT \u03b5\u2032 | \u2264 |\u1e7cjT \u03b5| + |\u1e7cjT \u03b5\u2032 |\n\u221a\n= \u03c31 |Zj | + |\u1e7cjT \u03b5\u2032 | \u2264 \u03c31 |Zj | + M \u03c9 n,\n\n\f21\n\nFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\nwhere Zj \u223c N (0, 1). This result follows from the fact that \u1e7cj is norm one\nand, since \u03b5i \u223c N (0, \u03c31 ), it will be the case that \u1e7cjT \u03b5 \u223c N (0, \u03c31 ). Hence\nP\n\n\u0012\n\n\u0013\n\nmax |\u1e7cjT \u03b5\u2217 | > \u03bb\nj\n\n=P\n\n\u0012\n\nmax |\u1e7cjT \u03b5\u2217 | > \u03c31\nj\n\n\u0012\n\n\u2264 P max |Zj | >\nj\n\nq\n\nq\n\n\u221a\n2(1 + \u03c6) log p + M \u03c9 n\n\n2(1 + \u03c6) log p\n\n\u0013\n\n\u0013\n\nq\n1\n\u2264 p \u221a exp{\u2212(1 + \u03c6) log p}/ 2(1 + \u03c6) log p\n2\u03c0\nq\n\n= (p\u03c6 4(1 + \u03c6)\u03c0 log p)\u22121 .\nThe penultimate line follows from the fact that P (supj |Zj | > u) \u2264 up \u221a12\u03c0 \u00d7\nexp(\u2212u2 /2).\np\nIn the case, where \u03b5\u2217i \u223c N (0, \u03c322 ) then substituting \u03bb = \u03c32 (1 + \u03c6) log p\ninto (12) gives (14). In this case \u1e7cjT \u03b5\u2217 = \u03c32 Zj , where Zj \u223c N (0, 1). Hence\nP\n\n\u0012\n\nmax |\u1e7cjT \u03b5\u2217 | > \u03c32\nj\n\nq\n\n\u0013\n\n\u0012\n\n(1 + \u03c6) log p = P max |Zj | >\nj\n\nand the result follows in the same manner as above.\n\nq\n\n\u0013\n\n2(1 + \u03c6) log p\n\nAPPENDIX C: THEOREMS 5 AND 6 ASSUMING GAUSSIAN \u03b5\u2217\np\n\nThe following theorems hold with \u03bb = \u03c32 2(1 + \u03c6) log p.\nTheorem 5. Suppose A-1 through A-5 all hold and \u03b5\u2217i \u223c N (0, \u03c322 ). Then,\nwith arbitrarily high probability,\nH\n|\u03b2bn (t) \u2212 \u03b2(t)| \u2264 O(n\u22121/2 ) + \u2217 m\np\nand\nH\nsup |\u03b2bn (t) \u2212 \u03b2(t)| \u2264 O(n\u22121/2 ) + \u2217 m .\np\nt\n\nTheorem 5 demonstrates that, with the additional assumption that \u03b5\u2217i \u223c\nN (0, \u03c322 ), asymptotically the approximation error is now bounded by H/p\u2217m ,\nwhich is strictly less than En (t). Finally, Theorem 6 allows p to grow with\nn, which removes the bias term, and hence \u03b2bn (t) becomes a consistent estimator.\nTheorem 6. Suppose we assume A-6 as well as \u03b5\u2217i \u223c N (0, \u03c322 ). Then if\nwe let p grow at the rate of n1/(2m+2bt ) , the rate of convergence improves to\n\u221a\n\u0013\n\u0012\nlog n\nb\n|\u03b2n (t) \u2212 \u03b2(t)| = O 1/2(m/(m+b ))\nt\nn\n\n\f22\n\nG. M. JAMES, J. WANG AND J. ZHU\n\nor if we let p grow at the rate of n1/(2m+2c) , the supremum converges at a\nrate of\n\u221a\n\u0013\n\u0012\nlog n\nb \u2212 \u03b2(t)| = O\n.\nsup |\u03b2(t)\nn1/2(m/(m+c))\nt\nAPPENDIX D: PROOFS OF THEOREMS 3\u20136\nProof of Theorem 3. By Theorem 2, for p = p\u2217 ,\nq\nq\nb \u2212 \u03b2(t)| \u2264 \u221a1 C \u2217 (t)\u03c3 2S \u2217 (1 + \u03c6) log p\u2217 + \u03c9 \u2217 {1 + C \u2217 (t)M S \u2217 }\n|\u03b2(t)\np\np\np\nn,p\nn,p\n1\nn\n\nwith arbitrarily high probability provided \u03c6 is large enough. But, by A-1,\nSp\u2217 is bounded, and, by A-3 and A-5, Cn,p\u2217 (t) is bounded for large n. Hence,\nsince p\u2217 , \u03c31 and \u03c6 are fixed, the first term on the right-hand side is O(n\u22121/2 ).\nm\nFinally, by A-2, \u03c9p\u2217 \u2264 H/p\u2217 and, by A-1, Sp\u2217 \u2264 S so the second term of\nb \u2212 \u03b2(t)| can be proved\nthe equation is at most En (t). The result for supt |\u03b2(t)\nin an identical fashion by replacing A-3 by A-4. \u0003\n\nProof of Theorem 4. By A-1 there exists S < \u221e such that Sp < S\nfor all p. Hence, by (13), setting \u03c6 = 0, with probability converging to one\nas p \u2192 \u221e,\np\n4\u03b1n,pn (t)\nb \u2212 \u03b2(t)| \u2264 \u221a1\n2S log pn\n\u03c3\n|\u03b2(t)\n1\nV \u2212 \u03b8V\nn 1 \u2212 \u03b42S\nS,2S\n\u001a\n\n+ \u03c9pn 1 +\n\n\u221a\n4\u03b1n,pn (t)\nM S\nV\nV\n1 \u2212 \u03b42S \u2212 \u03b8S,2S\n\n\u001b\n\n\u221a\nt\n\u221a\npbnt log pn 4p\u2212b\nn \u03b1n,pn (t)\n\u221a\n\u03c3\n2S\n=\n1\nV \u2212 \u03b8V\nn\n1 \u2212 \u03b42S\nS,2S\n\u001a\n\nt\n\u221a\n\u03c9 p n pm\n4p\u2212b\nn\nn \u03b1n,pn (t)\n\u2212bt\nM\n+ m\u2212b\n+\np\nS\nn\nV \u2212 \u03b8V\n1 \u2212 \u03b42S\npn t\nS,2S\n\u221a\nlog n\n= 1/2\u2212b /(2m) K,\nt\nn\n\nwhere K is\n\n(21)\n\n\u0012\n\npn\nn1/2m\n+\n\n\u0013bt s\n\n\u0012\n\nt\n\u221a\nlog pn 4p\u2212b\nn \u03b1n,pn (t)\n\u03c3\n2S\n1\nV \u2212 \u03b8V\nlog n 1 \u2212 \u03b42S\nS,2S\n\npn\nn1/2m\n\n\u0013bt \u2212m\n\n\u001a\n\n\u001b\n\n\u001b\n\nt\n\u221a\n4p\u2212b\n\u03c9 pm\nn \u03b1n,pn (t)\nt\n\u221apn n p\u2212b\nM S .\nn +\nV\nV\nlog n\n1 \u2212 \u03b42S \u2212 \u03b8S,2S\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n23\n\nBut if we let pn = O(n1/2m ) then (21) is bounded because pn /n1/2m and\nlog pn / log n are bounded by construction of pn , \u03c9pn pm\nn is bounded by A-2,\nV \u2212 \u03b8V\n\u22121 is bounded by A-6.\nt\n\u03b1\np\u2212b\n(t)\nis\nbounded\nby\nA-3\nand\n(1\n\u2212\n\u03b4\n)\nn,pn\nn\n2S\nS,2S\n\u221a\nlog n\nb\nHence |\u03b2n (t) \u2212 \u03b2(t)| = O( n1/2\u2212bt /(2m) ). With the addition of A-4 exactly the\n\u221a\nlog n\nb \u2212 \u03b2(t)| = O(\n). \u0003\nsame argument can be used to prove supt |\u03b2(t)\nn1/2\u2212c/(2m)\nProof of Theorem 5. By Theorem 2, if we assume \u03b5\u2217i \u223c N (0, \u03c322 ),\nthen, for p = p\u2217 ,\nq\nb \u2212 \u03b2(t)| \u2264 \u221a1 Cn,p\u2217 (t)\u03c32 2Sp\u2217 (1 + \u03c6) log p\u2217 + \u03c9p\u2217\n|\u03b2(t)\nn\nwith arbitrarily high probability provided \u03c6 is large enough. Then we can\nshow that the first term is O(n\u22121/2 ) in exactly the same fashion as for the\nm\nproof of Theorem 3. Also, by A-2, the second term is bounded by H/p\u2217 .\n\u0003\nProof of Theorem 6. If \u03b5\u2217i \u223c N (0, \u03c322 ) then, by (14), setting \u03c6 = 0,\nwith probability converging to one as p \u2192 \u221e,\n\u221a\nlog n\nb\n|\u03b2(t) \u2212 \u03b2(t)| \u2264 1/2(m/(m+b )) K,\nt\nn\nwhere\n\u0012\n\u0013bt s\nt\n\u221a\npn\nlog pn 4p\u2212b\nn \u03b1n,pn (t)\nK=\n2S\n\u03c3\n2\nV\nV\nlog n 1 \u2212 \u03b42S \u2212 \u03b8S,2S\nn(1/(2(m+bt )))\n(22)\n\u0013\u2212m m\n\u0012\np \u03c9\npn\n\u221an p .\n+\n(1/(2(m+b\n)))\nt\nlog n\nn\nHence if pn = O(n1/2(m+b) ) then (22) is bounded,\nusing the same argu\u221a\nlog\nn\nments as with (21), so |\u03b2bn (t) \u2212 \u03b2(t)| = O( n1/2(m/(m+bt )) ). We can prove that\n\u221a\nlog n\nb\nsupt |\u03b2n (t) \u2212 \u03b2(t)| = O( n1/2(m/(m+c)) ) in the same way. \u0003\nAPPENDIX E: PROOF OF COROLLARY 1\n\nHere we assume that A is produced using the\nmatrix,\n\uf8ee\n1/p2\n0\n0 0 ...\n\uf8ef \u22121/p 1/p\n0 0 ...\n\uf8ef\n\uf8ef 1\n\u22122\n1 0 ...\n\uf8ef\nA = p2 \uf8ef 0\n(23)\n1\n\u22122\n1 ...\n\uf8ef\n\uf8ef .\n.\n.\n.. . .\n..\n..\n\uf8f0 ..\n.\n.\n0\n0\n0 0 ...\n\nstandard second difference\n0\n0\n0\n0\n..\n.\n\n0\n0\n0\n0\n..\n.\n\n\uf8f9\n\n0\n0\uf8fa\n\uf8fa\n0\uf8fa\n\uf8fa\n.\n0\uf8fa\n\uf8fa\n\uf8fa\n.. \uf8fb\n.\n\n1 \u22122 1\n\n\f24\n\nG. M. JAMES, J. WANG AND J. ZHU\n\nThroughout this proof let \u03b7k = \u03b2(k/p). First we show A-1 holds. Suppose\nthat \u03b2 \u2032\u2032 (t) = 0 for all t in Rk\u22122 , Rk\u22121 and Rk then there exist b0 and b1 such\nthat \u03b2(t) = b0 + b1 t over this region. Hence, for k \u2265 2,\n\u03b3k = p2 (\u03b7k\u22122 \u2212 2\u03b7k\u22121 + \u03b7k )\n\n= p2 (\u03b2((k \u2212 2)/p) \u2212 2\u03b2((k \u2212 1)/p) + \u03b2(k/p))\n\u0012\n\nk\u22121\nk\nk\u22122\n\u2212 2b0 \u2212 2b1\n+ b0 + b1\np\np\np\n\n= p2 b0 + b1\n\n\u0013\n\n= 0.\n\nBut note that if \u03b2 \u2032\u2032 (t) 6= 0 at no more than S values of t then there are at\nmost 3S triples such that \u03b2 \u2032\u2032 (t) 6= 0 for some t in Rk\u22122 , Rk\u22121 and Rk . Hence\nthere can be no more than 3S + 2 \u03b3k 's that are not equal to zero (where the\ntwo comes from \u03b31 and \u03b32 ).\nNext we show A-2 holds. For any t \u2208 Rk , B(t)T \u03b7 = \u03b7k . But since |\u03b2 \u2032 (t)| <\nG for some G < \u221e and Rk is of length 1/p it must be the case that\nsupt\u2208Rk \u03b2(t) \u2212 inf t\u2208Rk \u03b2(t) \u2264 G/p. Let \u03b7k be any value between supt\u2208Rk \u03b2(t)\nand inf t\u2208Rk \u03b2(t) for k = 1, . . . , p. Then\nb | = max sup |\u03b2(t) \u2212 \u03b7k |\n\u03c9p = sup |\u03b2(t) \u2212 B(t)T \u03b7\nk\n\nt\n\n\u001a\n\nt\u2208Rk\n\n\u001b\n\n\u2264 max sup \u03b2(t) \u2212 inf \u03b2(t) \u2264 G/p\nk\n\nt\u2208Rk\n\nt\u2208Rk\n\nso A-2 holds with m = 1.\nNow, we show A-3 holds. For t \u2208 Rk let Lnj (t) =\n\n1\nn\n\nPn\n\n1\ni=1 ( p\n\nPp\n\nA\u22121\n\nlj\n2\nl=1 Xil A\u22121 ) ,\nkj\n\n\u22121 and X is the average of X (t) in\nwhere A\u22121\ni\nlk is the l, kth element of A\nqPil\nR\np\n\u22121\nRl , that is, p Rl Xi (s) ds. Then \u03b1n,p (t) =\nj=1 Lnj (t) . Since Xi (t) is\n\nbounded above zero Lnj (t) \u2265 W\u03b52 ( p1\n\nverified that\n\n\uf8ee\n\n(24)\n\np2\n\uf8ef p2\n\uf8ef 2\n\uf8efp\n1\n\uf8ef\nA\u22121 = 2 \uf8ef p2\np \uf8ef\n\uf8ef .\n\uf8f0 ..\np2\n\nand, hence,\n\nA\u22121\nlj\nA\u22121\nkj\n\n=\n\nA\u22121\nlj\n2\nl=1 A\u22121 )\nkj\n\nPp\n\n0\np\n2p\n3p\n..\n.\np(p \u2212 1)\n\uf8f1\n\uf8f4\n\uf8f4 \u221e,\n\uf8f2\n\n0,\n\nl\u2212j +1\n\uf8f4\n\uf8f4\n,\n\uf8f3\nk\u2212j +1\n\n0\n0\n1\n2\n..\n.\n\nfor some W\u03b5 > 0. It is easily\n0\n0\n0\n1\n..\n.\n\np\u22122 p\u22123\n\n...\n...\n...\n...\n..\n.\n...\n\nk < j, l \u2265 j,\nk \u2265 j, l < j,\nk \u2265 j, l \u2265 j\n\n\uf8f9\n\n0\n0\uf8fa\n\uf8fa\n0\uf8fa\n\uf8fa\n0\uf8fa\n\uf8fa\n\uf8fa\n\n0\uf8fb\n1\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n25\n\nexcept for j = 1 in which case the ratio equals one for all l and k. For t = 0\nthen t \u2208 R1 (i.e., k = 1) and, hence, Ln1 (0) \u2265 W\u03b52 while Lnj (0) = \u221e for all\nj > 1. Therefore, \u03b1n,p (0) \u2264 W\u03b5\u22121 for all p. Hence, A-3 holds with b0 = 0.\nAlternatively, for 0 < t < 1, then k = \u230apt\u230b and Lnj (t) = \u221e for j > k. Hence\nfor j \u2264 k,\nLnj (t)\n\n1\n\u2265 W\u03b52 2\np\n1\n= W\u03b52 2\np\n\u2265 W\u03b52\n\nTherefore\n\nk\u2212j +1\n\nl=j\n\n\u0012\n\n!2\n\n(p \u2212 j + 1)(p \u2212 j + 2)\n2(k \u2212 j + 1)\n\u0012\n\n1 4 (1 \u2212 t)4\n1\np\n2\np\n4\nk\u2212j +1\n\n\u03b1n,p (t) \u2264\n(25)\n\np\nX\nl\u2212j +1\n\n=\n\n2\n(1 \u2212 t)2 W\u03b5 p\n2\n(1 \u2212 t)2 W\u03b5 p\n\n\u2264 p1/2\n\n\u00132\n\n\u00132\n\nsince j \u2264 pt.\n\nv\nu\nk\nuX\nu\nt (k \u2212 j + 1)2\nj=1\n\ns\n\nk\n(k + 1)(2k + 1)\n6\n\n2\nt3/2\n(1 \u2212 t)2 W\u03b5\n\nsince k = \u230apt\u230b. Hence A-3 holds with bt = 1/2 for 0 < t < 1.\nFinally, note that (25) holds for any t < 1 and is increasing in t so\nsup\n0<t<1\u2212a\n\n\u03b1n,p (t) \u2264 p1/2\n\n2\naW\u03b5\n\nfor any a > 0 and hence A-4 holds with c = 1/2.\nAcknowledgments. We would like to thank the Associate Editor and\nreferees for many helpful suggestions that improved the paper.\nREFERENCES\nCandes, E. and Tao, T. (2005). Decoding by linear programming. IEEE Trans. Inform.\nTheory 51 4203\u20134215. MR2243152\nCandes, E. and Tao, T. (2007). The Dantzig selector: Statistical estimation when p is\nmuch larger than n (with discussion). Ann. Statist. 35 2313\u20132351. MR2382644\nChen, S., Donoho, D. and Saunders, M. (1998). Atomic decomposition by basis pursuit.\nSIAM J. Sci. Comput. 20 33\u201361. MR1639094\nEfron, B., Hastie, T., Johnston, I. and Tibshirani, R. (2004). Least angle regression\n(with discussion). Ann. Statist. 32 407\u2013451. MR2060166\n\n\f26\n\nG. M. JAMES, J. WANG AND J. ZHU\n\nFahrmeir, L. and Tutz, G. (1994). Multivariate Statistical Modeling Based on Generalized Linear Models. Springer, New York. MR1284203\nFan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its\noracle properties. J. Amer. Statist. Assoc. 96 1348\u20131360. MR1946581\nFan, J. and Zhang, J. (2000). Two-step estimation of functional linear models with\napplications to longitudinal data. J. R. Stat. Soc. Ser. B Stat. Methodol. 62 303\u2013322.\nMR1749541\nFaraway, J. (1997). Regression analysis for a functional response. Technometrics 39 254\u2013\n261. MR1462586\nFerraty, F. and Vieu, P. (2002). The functional nonparametric model and application\nto spectrometric data. Comput. Statist. 17 545\u2013564. MR1952697\nFerraty, F. and Vieu, P. (2003). Curves discrimination: a nonparametric functional\napproach. Comput. Statist. Data Anal. 44 161\u2013173. MR2020144\nHastie, T. and Mallows, C. (1993). Comment on \"a statistical view of some chemometrics regression tools.\" Technometrics 35 140\u2013143.\nHoover, D. R., Rice, J. A., Wu, C. O. and Yang, L. P. (1998). Nonparametric smoothing estimates of time-varying coefficient models with longitudinal data. Biometrika 85\n809\u2013822. MR1666699\nJames, G. M. (2002). Generalized linear models with functional predictors. J. R. Stat.\nSoc. Ser. B Stat. Methodol. 64 411\u2013432. MR1924298\nJames, G. M. and Hastie, T. J. (2001). Functional linear discriminant analysis for irregularly sampled curves. J. R. Stat. Soc. Ser. B Stat. Methodol. 63 533\u2013550. MR1858401\nJames, G. M. and Radchenko, P. (2009). A generalized dantzig selector with shrinkage\ntuning. Biometrika. To appear.\nJames, G. M., Radchenko, P. and Lv, J. (2009). DASSO: Connections between the\ndantzig selector and lasso. J. R. Stat. Soc. Ser. B Stat. Methodol. 71 127\u2013142.\nJames, G. M. and Silverman, B. W. (2005). Functional adaptive model estimation. J.\nAmer. Statist. Assoc. 100 565\u2013576. MR2160560\nLiang, K. Y. and Zeger, S. L. (1986). Longitudinal data analysis using generalized\nlinear models. Biometrika 73 13\u201322. MR0836430\nLin, D. Y. and Ying, Z. (2001). Semiparametric and nonparametric regression analysis\nof longitudinal data. J. Amer. Statist. Assoc. 96 103\u2013113. MR1952726\nLu, Y. and Zhang, C. (2008). Spatially adaptive functional linear regression with functional smooth lasso. To appear.\nMuller, H. G. and Stadtmuller, U. (2005). Generalized functional linear models. Ann.\nStatist. 33 774\u2013805. MR2163159\nRadchenko, P. and James, G. M. (2008). Variable inclusion and shrinkage algorithms.\nJ. Amer. Statist. Assoc. 103 1304\u20131315.\nRamsay, J. O. and Silverman, B. W. (2002). Applied Functional Data Analysis.\nSpringer, New York. MR1910407\nRamsay, J. O. and Silverman, B. W. (2005). Functional Data Analysis, 2nd ed.\nSpringer, New York. MR2168993\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. Roy. Statist.\nSoc. Ser. B 58 267\u2013288. MR1379242\nTibshirani, R., Saunders, M., Rosset, S. and Zhu, J. (2005). Sparsity and smoothness\nvia the fused lasso. J. R. Stat. Soc. Ser. B Stat. Methodol. 67 91\u2013108. MR2136641\nValdes-Sosa, P. A., Sanchez-Bornot, J, M., Lage-Castellanos, A., VegaHernandez, M., Bosch-Bayard, J., Melie-Garcia, L. and Canales-Rodriguez,\nE. (2005). Estimating brain functional connectivity with sparse multivariate autoregression. Philos. Trans. R. Soc. Ser. B 360 969\u2013981.\n\n\fFUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE\n\n27\n\nWu, C. O., Chiang, C. T. and Hoover, D. R. (1998). Asymptotic confidence regions\nfor kernel smoothing of a varying-coefficient model with longitudinal data. J. Amer.\nStatist. Assoc. 93 1388\u20131402. MR1666635\nZou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net.\nJ. R. Stat. Soc. Ser. B Stat. Methodol. 67 301\u2013320. MR2137327\nG. M. James\nMarshall School of Business\nUniversity of Southern California\nE-mail: gareth@usc.edu\nJ. Zhu\nDepartment of Statistics\nUniversity of Michigan\nE-mail: jizhu@umich.edu\n\nJ. Wang\nDepartment of Statistics\nUniversity of Michigan\nE-mail: jingjw@umich.edu\n\n\f"}