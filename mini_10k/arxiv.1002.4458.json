{"id": "http://arxiv.org/abs/1002.4458v4", "guidislink": true, "updated": "2013-02-05T04:50:26Z", "updated_parsed": [2013, 2, 5, 4, 50, 26, 1, 36, 0], "published": "2010-02-24T02:54:37Z", "published_parsed": [2010, 2, 24, 2, 54, 37, 2, 55, 0], "title": "Approximate Sparsity Pattern Recovery: Information-Theoretic Lower\n  Bounds", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.3212%2C1002.2182%2C1002.1671%2C1002.1688%2C1002.3645%2C1002.4972%2C1002.0945%2C1002.0279%2C1002.0534%2C1002.3942%2C1002.1393%2C1002.2547%2C1002.3242%2C1002.3333%2C1002.2754%2C1002.3839%2C1002.2803%2C1002.2283%2C1002.1780%2C1002.4139%2C1002.1253%2C1002.2970%2C1002.3416%2C1002.2996%2C1002.2081%2C1002.0542%2C1002.3132%2C1002.1562%2C1002.3248%2C1002.4959%2C1002.4851%2C1002.4905%2C1002.4549%2C1002.2000%2C1002.4458%2C1002.1036%2C1002.0554%2C1002.4196%2C1002.1861%2C1002.0146%2C1002.4842%2C1002.4258%2C1002.0196%2C1002.1312%2C1002.0260%2C1002.4207%2C1002.1207%2C1002.4115%2C1002.3459%2C1002.2456%2C1002.3576%2C1002.3425%2C1002.2109%2C1002.5034%2C1002.4268%2C1002.3440%2C1002.0315%2C1002.1001%2C1002.0970%2C1002.4906%2C1002.0199%2C1002.3683%2C1002.0377%2C1002.4505%2C1002.4739%2C1002.4710%2C1002.1969%2C1002.4452%2C1002.2006%2C1002.4902%2C1002.2115%2C1002.2378%2C1002.4744%2C1002.4284%2C1002.2175%2C1002.2241%2C1002.4422%2C1002.1825%2C1002.2380%2C1002.4585%2C1002.0928%2C1002.0490%2C1002.1810%2C1002.3025%2C1002.0298%2C1002.0442%2C1002.3520%2C1002.2164%2C1002.4372%2C1002.4349%2C1002.3529%2C1002.3856%2C1002.4518%2C1002.3515%2C1002.0514%2C1002.0584%2C1002.1747%2C1002.0499%2C1002.5037%2C1002.3921%2C1002.1121&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Approximate Sparsity Pattern Recovery: Information-Theoretic Lower\n  Bounds"}, "summary": "Recovery of the sparsity pattern (or support) of an unknown sparse vector\nfrom a small number of noisy linear measurements is an important problem in\ncompressed sensing. In this paper, the high-dimensional setting is considered.\nIt is shown that if the measurement rate and per-sample signal-to-noise ratio\n(SNR) are finite constants independent of the length of the vector, then the\noptimal sparsity pattern estimate will have a constant fraction of errors.\nLower bounds on the measurement rate needed to attain a desired fraction of\nerrors are given in terms of the SNR and various key parameters of the unknown\nvector. The tightness of the bounds in a scaling sense, as a function of the\nSNR and the fraction of errors, is established by comparison with existing\nachievable bounds. Near optimality is shown for a wide variety of practically\nmotivated signal models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.3212%2C1002.2182%2C1002.1671%2C1002.1688%2C1002.3645%2C1002.4972%2C1002.0945%2C1002.0279%2C1002.0534%2C1002.3942%2C1002.1393%2C1002.2547%2C1002.3242%2C1002.3333%2C1002.2754%2C1002.3839%2C1002.2803%2C1002.2283%2C1002.1780%2C1002.4139%2C1002.1253%2C1002.2970%2C1002.3416%2C1002.2996%2C1002.2081%2C1002.0542%2C1002.3132%2C1002.1562%2C1002.3248%2C1002.4959%2C1002.4851%2C1002.4905%2C1002.4549%2C1002.2000%2C1002.4458%2C1002.1036%2C1002.0554%2C1002.4196%2C1002.1861%2C1002.0146%2C1002.4842%2C1002.4258%2C1002.0196%2C1002.1312%2C1002.0260%2C1002.4207%2C1002.1207%2C1002.4115%2C1002.3459%2C1002.2456%2C1002.3576%2C1002.3425%2C1002.2109%2C1002.5034%2C1002.4268%2C1002.3440%2C1002.0315%2C1002.1001%2C1002.0970%2C1002.4906%2C1002.0199%2C1002.3683%2C1002.0377%2C1002.4505%2C1002.4739%2C1002.4710%2C1002.1969%2C1002.4452%2C1002.2006%2C1002.4902%2C1002.2115%2C1002.2378%2C1002.4744%2C1002.4284%2C1002.2175%2C1002.2241%2C1002.4422%2C1002.1825%2C1002.2380%2C1002.4585%2C1002.0928%2C1002.0490%2C1002.1810%2C1002.3025%2C1002.0298%2C1002.0442%2C1002.3520%2C1002.2164%2C1002.4372%2C1002.4349%2C1002.3529%2C1002.3856%2C1002.4518%2C1002.3515%2C1002.0514%2C1002.0584%2C1002.1747%2C1002.0499%2C1002.5037%2C1002.3921%2C1002.1121&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Recovery of the sparsity pattern (or support) of an unknown sparse vector\nfrom a small number of noisy linear measurements is an important problem in\ncompressed sensing. In this paper, the high-dimensional setting is considered.\nIt is shown that if the measurement rate and per-sample signal-to-noise ratio\n(SNR) are finite constants independent of the length of the vector, then the\noptimal sparsity pattern estimate will have a constant fraction of errors.\nLower bounds on the measurement rate needed to attain a desired fraction of\nerrors are given in terms of the SNR and various key parameters of the unknown\nvector. The tightness of the bounds in a scaling sense, as a function of the\nSNR and the fraction of errors, is established by comparison with existing\nachievable bounds. Near optimality is shown for a wide variety of practically\nmotivated signal models."}, "authors": ["Galen Reeves", "Michael Gastpar"], "author_detail": {"name": "Michael Gastpar"}, "author": "Michael Gastpar", "links": [{"href": "http://arxiv.org/abs/1002.4458v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1002.4458v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1002.4458v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1002.4458v4", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "APPROXIMATE SPARSITY PATTERN RECOVERY: INFORMATION-THEORETIC LOWER BOUNDS\n\n1\n\nApproximate Sparsity Pattern Recovery:\nInformation-Theoretic Lower Bounds\n\narXiv:1002.4458v4 [cs.IT] 5 Feb 2013\n\nGalen Reeves, Member, IEEE and Michael Gastpar Member, IEEE\n\nAbstract-Recovery of the sparsity pattern (or support) of an\nunknown sparse vector from a small number of noisy linear\nmeasurements is an important problem in compressed sensing. In\nthis paper, the high-dimensional setting is considered. It is shown\nthat if the measurement rate and per-sample signal-to-noise ratio\n(SNR) are finite constants independent of the length of the vector,\nthen the optimal sparsity pattern estimate will have a constant\nfraction of errors. Lower bounds on the measurement rate needed\nto attain a desired fraction of errors are given in terms of the\nSNR and various key parameters of the unknown vector. The\ntightness of the bounds in a scaling sense, as a function of the\nSNR and the fraction of errors, is established by comparison\nwith existing achievable bounds. Near optimality is shown for a\nwide variety of practically motivated signal models.\n\nsparsity rate (i.e. the fraction of nonzero entries) and the persample signal-to-noise ratio (SNR) are finite constants, independent of the vector length n. Our results are informationtheoretic lower bounds on the sampling rate \u03c1 = m/n needed\nto attain a desired detection error rate D. These bounds are\nfundamental in the sense that they hold for any possible\nrecovery algorithm. Our results are given explicitly in terms\nof the sparsity rate, the SNR, and various key properties of the\nunknown vector. Complementary upper bounds corresponding\nto a variety of recovery algorithms are given in the companion\npaper [17].\n\nIndex Terms-compressed sensing, information-theoretic\nbounds, random matrix theory, sparsity, support recovery.\n\nA. Overview of Main Contributions\n\nI. I NTRODUCTION\nUPPOSE that a vector x of length n is known to have\na small number k of nonzero entries, but the values and\nlocations of the nonzero entries are unknown and must be\nestimated from a set of m noisy linear projections (or samples)\ngiven by the vector\n\nS\n\ny = Ax + w\n\n(1)\n\nwhere A is a known m \u00d7 n measurement matrix and w is\nadditive white Gaussian noise. The problem of sparsity pattern\nrecovery is to determine which entries in x are nonzero. This\nproblem, which is known variously throughout the literature\nas support recovery or model selection, has applications in\ncompressed sensing [1]\u2013[3], sparse approximation [4], signal\ndenoising [5], subset selection in regression [6], and structure\nestimation in graphical models [7].\nA great deal of previous work [7]\u2013[16], has focused on\nnecessary and sufficient conditions for exact recovery of the\nsparsity pattern. By contrast, this paper studies the tradeoff\nbetween the number of samples m and the number of detection\nerrors. We focus on the high-dimensional setting where the\nMaterial in this paper was presented in part at the IEEE International\nSymposium on Information Theory, Toronto, Canada, July 2008 and the\n46th Annual Conference on Information Sciences and Systems, Princeton,\nNJ, March 2012.\nThis work was supported in part by ARO MURI No. W911NF-06-1-0076.\nG. Reeves was with the Department of Electrical Engineering and Computer\nSciences, University of California, Berkeley, CA 94720 USA. He is now with\nthe Department of Statistics, Stanford University, Stanford, CA 94305-4065\nUSA. (e-mail: greeves@stanford.edu)\nM. Gastpar is with the School of Computer and Communication Sciences,\nEcole Polytechnique F\u00e9d\u00e9rale (EPFL), 1015 Lausanne, Switzerland, and with\nthe Department of Electrical Engineering and Computer Sciences, University\nof California, Berkeley, CA 94720 USA (e-mail: michael.gastpar@epfl.ch).\n\nWe study the high-dimensional setting where the measurement matrix A is generated randomly and independently of\nthe vector x and the measurements are corrupted by additive\nwhite Gaussian noise. Three contributions of the paper are the\nfollowing:\n1) Fundamental limits: We derive lower bounds on the\nsampling rate needed for optimal recovery algorithms.\nWhile previous work has focused on exact recovery [7]\u2013\n[12] or the scaling behavior for approximate recovery\n[13], our work gives an explicit bound on the tradeoff\nbetween the sampling rate and the fraction of detection\nerrors. In conjunction with the upper bounds in [17], this\nbound provides a sharp characterization between what\ncan and cannot be recovered in the presence of noise.\nThis characterization is rigorous and thus validates recent\npredictions made using the powerful but heuristic replica\nmethod from statistical physics [18]\u2013[23].\n2) Insights into the difficulty of recovery: Using tools from\ninformation theory, we find a sharp separation into two\nproblem regimes \u2013 one in which the problem is fundamentally noise-limited, and one in which the problem\nis limited by the behavior of the sparse components\nthemselves.\n3) Effects of prior information: The upper bounds in [17]\ncorrespond to settings where the approximate number\nof nonzero entries is known. By contrast, the lower\nbounds in this paper apply to settings where the recovery\nalgorithm knows the exact number and distribution of the\nnonzero entries. Interestingly, the resulting bounds show\nthat in many cases, this additional knowledge does not\nsignificantly improve the ability to estimate the sparsity\npattern.\nBeyond these results, our framework also permits us to\nprove some further insights. For instance, we provide a tight\n\n\f2\n\nAPPROXIMATE SPARSITY PATTERN RECOVERY: INFORMATION-THEORETIC LOWER BOUNDS\n\ncharacterization of both the low-distortion and high-SNR\nbehaviors of the sampling rate for a variety of signal classes.\nB. Relation to Previous Work\nA great deal of previous work has focused directly on the\nfundamental limits of exact sparsity pattern recovery [7]\u2013[12],\n[24]. An initial necessary bound based on Fano's inequality\nwas provided by Gastpar [24] who considered Gaussian signals and deterministic measurement matrices. Necessary and\nsufficient scalings of (n, k, m) were given by Wainwright [10]\nwho considered deterministic vectors, characterized by the\nsize of their smallest nonzero elements, and i.i.d. Gaussian\nmeasurement matrices. Wainwright's necessary bound was\nstrengthened in our earlier work [15], for the special case\nwhere k scales proportionally with n, and for general scalings\nby Fletcher et al. [11] and Wang et al. [12].\nBased on the work outlined above, it is now well understood\nthat m = \u0398(k log n) samples are both necessary and sufficient\nfor exact recovery when the SNR is finite and there exists a\nfixed lower bound on the magnitude of the smallest nonzero\nelements [10]\u2013[12]. In contrast to the scaling required for\nbounded MSE, this scaling says that the ratio m/k must\ngrow without bound as the vector length n becomes large.\nAs a consequence, exact recovery is impossible in the setting\nconsidered in this paper, where the sparsity rate, sampling rate,\nand SNR are finite constants, independent of the vector length\nn.\nFrom an information-theoretic perspective, a number of\nworks have studied the rate-distortion behavior of sparse\nsources [25]\u2013[30]. Most closely related to this paper, however, is work that has addressed approximate sparsity pattern recovery directly. For the special case where the values\nof the nonzero entries are identical and known (throughout\nthe system), Aeron et al. [14, Theorem V-2] showed that\nm = C * k log(n/k) samples are necessary and sufficient for\nan ML decoder where the constant C is bounded explicitly\nin terms of the SNR and the desired detection error rate. In\nthe general setting where the nonzero values are unknown,\nAkcakaya and Tarokh [13] showed that m = C * k log(n/k)\nsamples are necessary and sufficient for a joint typicality recovery algorithm where the constant C is finite, but otherwise\nunspecified. (It can also be shown that this same result is\nimplied directly by the previous work of Cand\u00e8s et al. [31].)\nAn important difference between these previous results and the\nresults in this paper is that we give an explicit and relatively\ntight characterization of the constant C for a broad class of\nsignal models.\nC. Notation\nWhen possible, we use the following conventions: a random\nvariable X is denoted using uppercase and its realization x\nis denoted using lowercase; a random vector V is denoted\nusing boldface uppercase and its realization v is denoted using\nboldface lowercase; and a random matrix M is denoted using\nboldface uppercase and its realization M is denoted using\nuppercase. We use [n] to denote the set {1, 2, * * * , n}. For\n\na subset S \u2282 [n] and vector x, we use xS to denote the |S|dimensional vector of the entries in x indexed by S. Also,\nfor any m \u00d7 n matrix A, we use AS to denote the m \u00d7 |G|\nmatrix corresponding to the columns of A indexed by S.\nAll logarithms are taken with respect to the natural base.\nUnspecified constants are denoted by C and are assumed to\nbe positive and finite.\nII. P ROBLEM F ORMULATION\nThroughout this paper, the unknown signal is modeled as a\nn-dimensional random vector X. We consider a noisy linear\nobservation model given by\n1\nY = AX + \u221a W\nsnr\n\n(2)\n\nwhere A is a random m \u00d7 n matrix, snr \u2208 (0, \u221e) is a fixed\nscalar, and W \u223c N (0, Im\u00d7m ) is additive white Gaussian\nnoise. The vector X, matrix A, and noise W are assumed\nto be mutually independent. Note that if E[kAXk2 ] = m,\nthen snr corresponds to the per-sample signal-to-noise ratio of\nthe problem.\nThe problem studied in this paper is recovery of the sparsity\npattern S \u2217 of X which is given by\nS \u2217 = {i \u2208 [n] : Xi 6= 0}.\n\n(3)\n\nWe assume throughout that a recovery algorithm is given the\nvector Y, the matrix A, and the distribution on the vector X.\nThe algorithm then returns an estimate \u015c of the true sparsity\npattern S \u2217 .\nA. Distortion Measure\nTo assess the quality of an estimate \u015c it is important to note\nthat there are two types of errors. A missed detection occurs\nwhen an element in S \u2217 is omitted from the estimate \u015c. The\nmissed detection rate is given by\nn\n\nMDR(S \u2217 , \u015c) =\n\n1 X\n1(i \u2208 S \u2217 , i \u2208\n/ \u015c).\n|S \u2217 | i=1\n\n(4)\n\nConversely, a false alarm occurs when an element not present\nin S \u2217 is included in \u015c. The false alarm rate is given by\nFAR(S \u2217 , \u015c) =\n\nn\n1 X\n\n|\u015c|\n\ni=1\n\n1(i \u2208\n/ S \u2217 , i \u2208 \u015c).\n\n(5)\n\nIn general, various tradeoffs between the two errors types\ncan be considered. In this paper, however, we focus exclusively\non the distortion measure\n\u0001\nd(S \u2217 , \u015c) = max MDR(S \u2217 , \u015c), FAR(S \u2217 , \u015c) .\n(6)\n\nThis distortion measure is a metric on subsets of [n].\nFor any distortion D \u2208 [0, 1] we define the error probability\nh\ni\n\u03b5\u2217n (D) = min Pr d(S \u2217 , \u015c) > D\n(7)\np(\u015d|y,A)\n\nwhere the minimization is over all conditional probability mass\nfunctions p(\u015d|y, A) and probability is taken with respect to the\ndistribution on the vector X, the matrix A, and the noise W.\n\n\fREEVES AND GASTPAR\n\n3\n\nB. Signal and Measurement Models\n\nC. The Sampling Rate-Distortion Function\n\nTo characterize the problem of sparsity pattern recovery, we analyze a sequence of recovery problems\n{X(n), A(n), W(n)}n\u22651 indexed by the vector length n.\n\nUnder Assumptions SS1-SS2 and M1-M3, the asymptotic\nrecovery problem is characterized by the sampling rate \u03c1,\nlimiting distribution pX , and snr.\n\nStochastic Signal Assumptions: We consider the following\nassumptions on a sequence of random vectors X(n) \u2208 Rn .\nSS1 Linear Sparsity: The sparsity pattern S \u2217 is distributed\nuniformly over all subsets of [n] of size k(n) where k(n)\nis a known sequence that obeys\n\nDefinition 1. A distortion D is achievable for a fixed tuple\n(\u03c1, pX , snr) if there exists a sequence of measurement matrices\nsatisfying Assumptions M1-M3 such that\n\nk(n)\n=\u03ba\n(8)\nn\nfor some sparsity rate \u03ba \u2208 (0, 1/2).\nSS2 IID Nonzero Entries: The nonzero entries {Xi : i \u2208 S \u2217 }\nare i.i.d. pU where pU is a probability distribution on the\nreal line with no mass at 0, i.e. pU ({0}) = 0\nAssumption SS1 says that all but a fraction \u03ba of the entries\nare equal to zero, and Assumption SS2 characterizes the\nbehavior of the nonzero entries. Note that under these assumptions the number of nonzero value k(n) is a deterministic (nonrandom) property of the distribution on X, and thus knowledge\nof the distribution on X implies that the exact number of\nnonzero entries is known.\nThroughout the paper, we also use pX to denote the probability distribution on the real line given by\n\nfor a sequence of vectors satisfying Assumptions SS1-SS2.\n\nlim\n\nn\u2192\u221e\n\npX = (1 \u2212 \u03ba)\u03b40 + \u03ba pU\nwhere \u03b40 denotes a point-mass at x = 0. Note that there is\na one-to-one correspondence between the pair (\u03ba, pU ) and\nthe distribution pX , and that pX characterizes the marginal\ndistribution of each entry in X.\nMeasurement Assumptions: We consider a subset of the following assumptions on the sequence of measurement matrices\nA(n) \u2208 Rm(n)\u00d7n .\nM1 Non-Adaptive Measurements: The distribution on A(n)\nis independent of the vector x(n) and the noise W(n).\nM2 Finite Sampling Rate: The number of rows m(n) obeys\nm(n)\n=\u03c1\n(9)\nn\nfor some sampling rate \u03c1 \u2208 (0, \u221e).\nM3 Row Normalization: The distribution on A(n) is normalized such that each of the m(n) rows has unit magnitude\non average, i.e.\n\u0003\n\u0002\n(10)\nE kA(n)k2F = m(n)\nlim\n\nn\u2192\u221e\n\nwhere k * kF denotes the Frobenius norm.\nM4 IID Entries: The entries of A(n) are i.i.d. with mean zero\nand variance 1/n.\nAssumptions M1-M3 are used throughout the paper. A\nsampling rate \u03c1 < 1 corresponds to the compressed sensing\nsetting where the number of equations m is less than the\nnumber of unknown signal values n. A sampling rate \u03c1 = 1\ncorresponds to the number of linearly independent measurements that are needed to recover an arbitrary vector in the\nabsence of any measurement noise. Assumption M4 is used\nto provide stronger bounds Section IV.\n\nlim \u03b5\u2217 (D)\nn\u2192\u221e n\n\n=0\n\n(11)\n\nDefinition 2. For a fixed tuple (D, pX , snr), the sampling ratedistortion function \u03c1\u2217 (D, pX , snr) is given by\n\u03c1\u2217 (D, pX , snr) = inf{\u03c1 \u2265 0 : D is achievable}.\n\n(12)\n\nTo lighten the notation, we will denote the sampling ratedistortion function using \u03c1\u2217 where the dependence on the tuple\n(D, pX , snr) is implicit.\nRemark 1. In [17], upper bounds on the achievable distortion\nare derived under a related but slightly different set of signal\nassumptions (e.g. the unknown vector is nonrandom and the\nrecovery algorithm is only given the approximate fraction of\nnonzero entries). In [32], it is shown that the lower bounds derived under the assumptions of this paper imply corresponding\nlower bounds for the setting studied in [17].\nIII. B OUNDS\n\nFOR\n\nA RBITRARY M EASUREMENT M ATRICES\n\nThis section gives lower bounds on the sampling rate distortion function. These bounds apply generally to any sequence\nof measurement matrices obeying Assumptions M1-M3.\nBefore we present our bounds, two more definitions are\nneeded. First, we use the notation\nVX = Var(X)\n\n(13)\n\nto denote the variance of the distribution pX .\nAlso, we define\n(\n\u0001\n\u03baD\n, D < 1\u2212\u03ba\nH(\u03ba) \u2212 \u03baH(D) \u2212 (1\u2212\u03ba)H 1\u2212\u03ba\nR(D; \u03ba) =\n0,\nD \u2265 1\u2212\u03ba\n(14)\nwhere H(p) = \u2212p log p \u2212 (1 \u2212 p) log(1 \u2212 p) is binary entropy.\nIt is straightforward to show that R(D; \u03ba) corresponds to the\ninformation rate (given in nats per dimension) required to\nencode a sparsity pattern to within distortion D.\nA. Initial bound via Fano's inequality\nWe begin with a lower bound on the achievable distortion.\nThis bound is general in the sense that it depends only on the\nvariance VX of the distribution pX , and it serves as a building\nblock for our stronger bounds. The proof is based on Fano's\ninequality and is given in Appendix A.\nTheorem 1. Under Assumptions SS1-SS2 and M1-M3, a\ndistortion D is not achievable for the tuple (\u03c1, pX , snr) if\n\u0001\nmin(1, \u03c1) log 1 + max(1, \u03c1)VX snr < 2R(D; \u03ba). (15)\n\n\f4\n\nAPPROXIMATE SPARSITY PATTERN RECOVERY: INFORMATION-THEORETIC LOWER BOUNDS\n\nRemark 2. Results similar to Theorem 1 have been derived\npreviously in the special case of exact recovery [12], [24],\n[27], as well as for approximate recovery in the special case\nof binary signals [14].\nUsing Theorem 1 and the concavity of the logarithm, we\nobtain a simplified lower bound on the sampling rate-distortion\nfunction:\n\u03c1\u2217 \u2265\n\n2R(D; \u03ba)\n.\nlog(1 + VX snr)\n\n(16)\n\nTheorem 1 shows that a nonzero sampling rate \u03c1 is necessary in the presence of noise for any distribution pX with\nfinite variance. One critical weakness, however, is that it does\nnot reflect the true difficulty of sparsity pattern recovery when\nthe desired distortion D is small. For example, if D = 0,\nthen the corresponding lower bound on sampling rate is finite\neven though it has been shown that an infinite sampling rate is\nneeded in the presence of noise [15]. Among other things, this\ndiscrepancy leaves open the possibility that the total number\nof recovery errors could grow sublinearly with the length n\nsuch that the fraction of errors is asymptotically zero.\nB. Improved lower bound via a genie argument\nOur next result allows us to lower bound the distortion\ncorresponding to a distribution pX in terms of a different but\nrelated distribution pZ . This result is useful since it allows us\nto isolate the key aspects of the recovery problem that make\nrecovery difficult. The proof is based on a genie argument and\nis given in Appendix B.\nLemma 1. Let pX and pZ be probability measures with the\nfollowing properties:\n0 < \u03baZ \u2264 \u03baX\npX (A)\npZ (A)\n\u2264\n1 \u2212 \u03baZ\n1 \u2212 \u03baX\n\n \u0303 ) satisfying the assumptions\nthere exists a tuple (D\u0303, \u03c1\u0303, pZ , snr\nof Lemma 1 such that\n \u0303 ) < 2R(D\u0303; \u03baZ ).\nmin(1, \u03c1\u0303) log(1 + max(1, \u03c1\u0303)VZ snr\n\n(22)\n\nTo understand the implications of Theorem 2 it is useful to\nconsider the following simplification. First, observe that we\ncan parameterize the pair (D, D\u0303) in terms of the ratio D\u2032 =\nD/D\u0303. Next, let pZ be the distribution that minimizes E[Z 2 ]\nsubject to the constraints (17) and (18) with \u03baZ = \u03baX D\u2032 /(1\u2212\n\u03baX D\u2032 ). As a simple exercise, it can then be verified that\n \u0303 = P (D\u2032 ; pX ) snr\nVZ snr\n\n(23)\n\nwhere\nP (D\u2032 ; pX ) =\n\nZ\n\n\u221e\n\n0\n\n\u0001\nmax Pr[X 2 > u] \u2212 (1\u2212D\u2032 )\u03ba, 0 du.\n\n(24)\n\n\u2032\n\nThe function P (D ; pX ) corresponds to the average power\nof the smallest fraction D\u2032 of nonzero entries and has been\nstudied previously in the analysis of maximum likelihood\nestimation (see [17]). It is monotonically increasing in D\u2032 with\nP (0; pX ) = 0 and P (D\u2032 ; pX ) > 0 for any D\u2032 > 0.\nFinally, using the same simplification that led to (16) and\nmaximizing over the ratio D\u2032 gives the following result.\nCorollary 1. Under Assumptions SS1-SS3 and M1-M3, the\nsampling rate-distortion function \u03c1\u2217 obeys\n\u0001\n\u03baD\u2032\nD\n2(1 \u2212 \u03ba + \u03baD\u2032 )R D\n\u2032 ; 1\u2212\u03ba+\u03baD \u2032\n\u2217\n.\n(25)\n\u03c1 \u2265 \u2032max\nlog(1 + P (D\u2032 ; pX ) snr)\nD \u2208[D,1]\nThe next section shows that the right-hand side of (25) tends\nto infinity as D \u2192 0. Therefore, one important contribution\nof Theorem 2 is that it is not possible to have a vanishing\nfraction of errors if both the sampling rate and SNR are finite.\n\n(17)\nC. Low-Distorion Behavior\nfor all A \u2286 R\\{0}\n\n(18)\n\nwhere \u03baX = 1 \u2212 pX ({0}) and \u03baZ = 1 \u2212 pZ ({0}). For a given\ntuple (D, \u03c1, snr) define\n\u0010 1 \u2212 \u03ba \u0011\u0010 \u03ba \u0011\nZ\nX\nD\u0303 =\nD\n(19)\n1 \u2212 \u03baX\n\u03baZ\n\u00101\u2212\u03ba \u0011\nZ\n\u03c1\n(20)\n\u03c1\u0303 =\n1 \u2212 \u03baX\n\u00101\u2212\u03ba \u0011\nZ\n \u0303 =\nsnr.\n(21)\nsnr\n1 \u2212 \u03baX\n\nUnder Assumptions SS1-SS2 and M1-M3, the following statement holds: If the distortion D\u0303 is not achievable for the tuple\n \u0303 ), then the distortion D is not achievable for the\n(\u03c1\u0303, pZ , snr\ntuple (\u03c1, pX , snr).\nCombining Theorem 1 and Lemma 1 gives the first main\nresult of this paper. This result overcomes the weakness of\nTheorem 1 and characterizes the difficulty of recovery when\nthe desired distortion D is small.\nTheorem 2. Under Assumptions SS1-SS2 and M1-M3, a\ndistortion D is not achievable for the tuple (\u03c1, pX , snr) if\n\nWe now investigate the low-distortion behavior of Theorem 2. The following result follows directly from Corollary 1.\nThe proof is given in Appendix E.\nCorollary 2. Fix any \u03b1 > 1. Under assumptions SS1-SS2 and\nM1-M3, the sampling rate-distortion function \u03c1\u2217 obeys\nlim inf \u03c1\u2217 *\nD\u21920\n\nP (\u03b1D; pX )\n1\n\u2265\n.\n2(\u03b1 \u2212 1)\u03baD log(1/D)\nsnr\n\n(26)\n\nBy the continuity of P (D; pX ) over the interval D \u2208 [0, 1),\none implication of Corollary 2 is that for any distribution pX ,\nthere exists a positive constant C such that the sampling ratedistortion function obeys\n\u03c1\u2217 \u2265 C *\n\nD log(1/D)\n.\nP (D; pX ) * snr\n\n(27)\n\nTo characterize limiting behavior of the function P (D; pX)\nwe consider two different signal classes:\n\u2022 Bounded: We use PBounded (\u03ba, B) to denote the class of\nall distributions pX with sparsity rate \u03ba, second moment\nequal to one, and\nPr[|X| < B|X 6= 0] = 0\n\n\fREEVES AND GASTPAR\n\n\u2022\n\n5\n\nfor some lower bound B > 0. Due to the second \u221a\nmoment\nconstraint, the lower bound B cannot exceed 1/ \u03ba.\nPolynomial Decay: We use PPoly. (\u03ba, L, \u03c4 ) to denote the\nclass of all distributions pX with sparsity rate \u03ba, second\nmoment equal to one, and\nPr[|X| \u2264 x|X 6= 0]\n=\u03c4\nx\u21920\nxL\nlim\n\nfor some polynomial decay rate L > 0 and limiting\nconstant \u03c4 \u2208 (0, \u221e).\n\nThe bounded class corresponds to the setting where the\nnonzero entries in x have a fixed lower bound B on their\nmagnitudes, independent of the vector length n. By contrast,\nthe polynomial decay class corresponds to the setting where\nthe magnitude of the \u2308\u03b2 k\u2309'th smallest nonzero entry is proportional to \u03b2 1/L for small \u03b2. Note that in the case of polynomial\ndecay, a vanishing fraction of the nonzero entries are tending\nto zero as the vector length n becomes large.\nCombining Corollary 2 with analysis of P (D; pX ) given\nin [17] leads to the following result. The proof is given in\nAppendix E.\nCorollary 3. Under assumptions SS1-SS2 and M1-M3 the\nsampling rate-distortion function obeys the following asymptotic lower bounds:\n(a) If pX \u2208 PBounded (\u03ba, B), then\nD\u21920\n\n\u03c1\n2\n\u2265 2\n.\nlog(1/D)\nB * snr\n\n(28)\n\n\u0010\n\u0011\u22122/L 2\n\u03c4\n\u03c1\u2217\n=\n*\n. (29)\nlim inf 2/L\nD\u21920 D\n1 + L/2\nsnr\nlog(1/D)\nSimply put, Corollary 3 shows that the sampling ratedistortion function obeys\n(30)\n\nif pX is bounded and\n\u03c1\u2217 \u2265 C * D2/L log(1/D)\n\nwhere h(X|X 6= 0) denotes the differential entropy of the\nnonzero part of pX . The nonzero entropy power allows us to\nassess the relative uncertainty about the nonzero entries.\nOur next result gives a lower bound on the achievable\ndistortion in terms of the variance VX and the nonzero\nentropy power NX . The proof relies heavily on the entropy\npower inequality and the spectral convergence of i.i.d. random\nmatrices and is given in Appendix C.\nTheorem 3. Under Assumptions SS1-SS3 and M1-M4, a\ndistortion D is not achievable for the tuple (\u03c1, pX , snr) if\nV(\u03c1, VX snr) \u2212 \u03baVLB (\u03c1/\u03ba, NX snr) < 2R(D; \u03ba)\n\n(31)\n\nif pX has polynomial decay L. In [17], it is shown that,\nup to constants, these scalings are also achievable. Together,\nthese upper and lower bounds characterize precisely how\nthe sampling rate-distortion function increases as the desired\ndistortion becomes small.\n\n\u0001\n\u0001\nV(r, \u03b3) = r log 1 + \u03b3 \u2212 F (r, \u03b3) + log 1 + r \u03b3 \u2212 F (r, \u03b3)\n\u2212 F (r, \u03b3)/\u03b3\n(34)\n1\nF (r, \u03b3) =\n4\n\nFOR\n\nIID M EASUREMENT M ATRICES\n\nThis section gives stronger lower bounds for measurement\nmatrices whose entries are i.i.d. (Assumption M4). Unlike the\nbounds given in the previous section, these bounds capture the\nfact that the values of nonzero entries of X are unknown. Section IV-A gives an improved lower bound for settings where\nthe nonzero entries are continuous. Section IV-B considers the\nhigh-SNR behavior of the bound. Section IV-C extends the\nbound arbitrary distributions.\n\n\u00132\n\u0012q\nq \u221a\n\u221a\n2\n2\n\u03b3 ( r + 1) + 1 \u2212 \u03b3 ( r \u2212 1) + 1\n\n\uf8f1\n\u0010\n\u0010 1 \u00111/r\u22121 1 \u0011\n\uf8f4\n\uf8f4\nr\nlog\n1\n+\n\u03b3\n, if r < 1\n\uf8f4\n\uf8f4\n1\u2212r\ne\n\uf8f4\n\uf8f2\n\u0010\n\u0011\n1\nVLB (r, \u03b3) = log 1 + \u03b3 ,\nif r = 1 .\n\uf8f4\ne\n\uf8f4\n\uf8f4\n\u0010\n\u0010 r \u0011r\u22121 1 \u0011\n\uf8f4\n\uf8f4\n\uf8f3 log 1 + \u03b3 r\n,\nif r > 1\nr\u22121\ne\n(35)\nRemark 3. In the special case where the nonzero part of\nthe distribution pX is Gaussian, the function VLB (r, \u03b3) in the\nsecond term on the left-hand side of (33) can be replaced\nwith the function V(r, \u03b3), thus providing a slightly stronger\ncondition.\nCombining Theorem 3 with bounds on the functions V(r, \u03b3)\nand VLB (r, \u03b3) given in Appendix D, gives a simplified lower\nbound on the sampling rate-distortion function:\n\u03c1\u2217 >\n\nIV. B OUNDS\n\n(33)\n\nwhere\n\nand\n\n(b) If pX \u2208 PPoly. (\u03ba, L, \u03c4 ), then\n\n\u03c1\u2217 \u2265 C * log(1/D)\n\nWe define the nonzero entropy power of a random variable\nX \u223c pX to be\n\uf8f1\n\uf8f2 \u03ba exp (2h(X|X 6= 0))\n, if h(X|X 6= 0) exists\nNX =\n2\u03c0e\n\uf8f3\n0,\notherwise\n(32)\n\nwith\n\n\u2217\n\nlim inf\n\nA. Improved lower bound via the entropy power inequality\n\nmin(\u03c1\u2217 , \u03ba) log(1 + (NX /e) snr) + 2R(D, \u03ba)\n.\nlog(1 + VX snr)\n\n(36)\n\nNote that this bound is similar to (16), except that there is an\nadditional term on the right-hand side.\nB. High-SNR behavior\nThe key improvement of Theorem 3 is that the lower bound\non the distortion remains bounded away from zero for all SNR.\nTo illustrate this point, we first consider the infinite SNR limit\nof the lower bound on the distortion. Since the achievable\n\n\f6\n\nAPPROXIMATE SPARSITY PATTERN RECOVERY: INFORMATION-THEORETIC LOWER BOUNDS\n\ndistortion is non-increasing in the SNR, this limit gives a valid\nlower bound for any SNR.\nCorollary 4. Under Assumptions SS1-SS3 and M1-M4, a\ndistortion D is not achievable for the tuple (\u03c1, pX , snr) if\n\u03c1 < \u03ba and\n\u0012\n\u0013\n\u0010 1 \u0011\n\u0010 \u03ba \u0011\nVX\n\u03c1 log\n+ (1 \u2212 \u03c1) log\n\u2212 (\u03ba \u2212 \u03c1) log\nNX\n1\u2212\u03c1\n\u03ba\u2212\u03c1\n< 2R(D; \u03ba).\n(37)\n\nC. Extension to arbitrary distributions\nCombining Theorem 3 with Lemma 1 gives the following\nresult which is the strongest bound in this paper.\nTheorem 4. Under Assumptions SS1-SS3 and M1-M4, a\ndistortion D is not achievable for the tuple (\u03c1, pX , snr) if\n \u0303 ) satisfying the assumptions\nthere exists a tuple (D\u0303, \u03c1\u0303, pZ , snr\nof Lemma 1 such that\n \u0303 ) \u2212 \u03baZ VLB (\u03c1/\u03baZ , NZ snr\n \u0303 ) < 2R(D\u0303; \u03baZ ).\nV(\u03c1, VZ snr\n\n(43)\n\nThe proof of Corollary 4 follows directly from the infinite\nSNR limit of Theorem 3 and is given in Appendix F. In [32],\nit is shown that the same result can be obtained by direct\nanalysis of the noiseless setting.\nUsing the fact that the left-hand side of (37) is increasing in\n\u03c1 gives a simple lower bound on the sampling rate-distortion\nfunction.\n\nTheorem 4 has the same low-distortion behavior as Theorem 2. Furthermore it allows us to extend the high-SNR\nimprovements of Theorem 3 to arbitrary distributions.\nFor example, consider the following result.\n\nCorollary 5. Consider Assumptions SS1-SS3 and M1-M4. If\nthe ratio NX /VX is large relative to the desired distortion D,\ni.e. if\n\nwhere Xc is continuous with finite differential entropy. Under\nAssumptions SS1-SS2 and M1-M4, a distortion D is not\nachievable for the tuple (\u03c1, pX , snr) in the noiseless setting\n \u0303 ) given by\nif \u03c1 < \u03c9c and (43) holds for the tuple (D\u0303, \u03c1\u0303, pZ , snr\n\u03ba\nD\n(45)\nD\u0303 =\n\u03c9c\n\u0011\n\u0010\n1\n\u03c1\n(46)\n\u03c1\u0303 =\n1 \u2212 \u03ba + \u03c9c\n\u0010\n\u0011\n\u0010 1\u2212\u03ba \u0011\n\u03c9c\n(47)\n\u03b40 +\npXc\npZ =\n1 \u2212 \u03ba + \u03c9c\n1 \u2212 \u03ba + \u03c9c\n\u0011\n\u0010\n1\n \u0303 =\nsnr.\n(48)\nsnr\n1 \u2212 \u03ba + \u03c9c\n\n\u03ba log\n\n\u0010V \u0011\nX\n\nNX\n\n+ (1 \u2212 \u03ba) log\n\n\u0010\n\n1 \u0011\n< 2R(D; \u03ba),\n1\u2212\u03ba\n\n(38)\n\nthen sampling rate-distortion function obeys \u03c1\u2217 \u2265 \u03ba for all\nSNR.\nThe next result gives a precise characterization of the highSNR behavior of Theorem 3. The proof is given in Appendix F\nCorollary 6. Under the assumptions of Corollary 5, the\nsampling rate-distortion function obeys\nlim inf (\u03c1\u2217 \u2212 \u03ba) log(snr) \u2265\nsnr\u2192\u221e\n\u0010 1 \u0011\n\u0010V \u0011\nX\n.\n\u2212 (1 \u2212 \u03ba) log\n2R(D; \u03ba) \u2212 \u03ba log\nNX\n1\u2212\u03ba\n\n(39)\n\nCorollary 6 shows that under the conditions of Corollary 5,\nthe lower bound on the sampling rate distortion function \u03c1\u2217\nobeys\n\u03c1\u2217 \u2265 \u03ba +\n\nC\nlog(1 + snr)\n\n(40)\n\nfor some positive constant C.\nFor comparison, it is shown in [17] that the sampling ratedistortion function obeys the asymptotic upper bound:\nlim sup(\u03c1\u2217 \u2212 \u03ba) log(snr) \u2264 2H(\u03ba),\n\n(41)\n\nsnr\u2192\u221e\n\nC\u0303\n\u03c1 \u2264\u03ba+\nlog(1 + snr)\n\npX = (1 \u2212 \u03ba) \u03b40 + \u03c9c pXc + (\u03ba \u2212 \u03c9c ) pX\u0303\n\n(42)\n\nfor some finite constant C\u0303. Together, these lower and upper\nbounds characterize precisely how the sampling rate distortion\nfunction \u03c1\u2217 converges to the sparsity rate \u03ba as the SNR\nincreases.\n\n(44)\n\nStarting with Corollary 7 and following the same steps that\nlet to Corollary 6 gives the following high-SNR characterization.\nCorollary 8. Consider the assumptions of Corollary 7 and let\n\u0010\u03ba\n\u0011\n\u03c9c\n\u2206 = 2(1 \u2212 \u03ba + \u03c9c )R\nD;\n\u03c9c\n1\u2212\u03ba + \u03c9c\n\u0010V \u0011\n\u00101 \u2212 \u03ba + \u03c9 \u0011\nZ\nc\n\u2212 \u03c9c log\n(49)\n+ (1 \u2212 \u03ba) log\nNZ\n1\u2212\u03ba\n\nwhere pZ is given by (47). If \u2206 > 0, then the sampling ratedistortion function obeys\nlim inf (\u03c1\u2217 \u2212 \u03c9c ) log(snr) \u2265 \u2206.\n\nsnr\u2192\u221e\n\n(50)\n\nCorollary 8 shows that if the nonzero entropy power NZ is\nlarge relative to the desired distortion D, then the sampling\nrate-distortion function obeys\n\u03c1\u2217 \u2265 \u03c9 c +\n\nand hence\n\u2217\n\nCorollary 7. Suppose that pX can be expressed as\n\nC\nlog(1 + snr)\n\n(51)\n\nfor some positive constant C. This result shows that the highSNR behavior is dominated by the weight of the continuous\npart of the distribution on the nonzero entries.\nV. E XAMPLES AND I LLUSTRATIONS\nThis section provides specific examples and illustrations of\nthe bounds developed in this paper.\n\n\fREEVES AND GASTPAR\n\n7\n\nSNR = 10 (dB)\n\n0\n\nSNR = 50 (dB)\n\n0\n\n10\n\n10\n\nUpper Bnd.\nReplica Bnd.\nTheorem 4\nTheorem 3\nTheorem 2\nTheorem 1\n\nUpper Bnd.\nTheorem 4\nTheorem 3\nTheorem 2\nTheorem 1\n\u22121\n\nPSfrag replacements\n\nDistortion \u03b1\nSampling Rate \u03c1\n\nDistortion \u03b1\nSampling Rate \u03c1\n\nUpper Bound [17]\n\nUpper Bound [17]\n\nLower Bound\n(Proposition ??)\nopositions ?? & ??)\n\n\u22121\n\n10\n\n\u22124\n\n\u22123\n\n10\n\n10\nSampling Rate\n\nDistortion\n\nSfrag replacements\n\nDistortion\n\n10\n\n\u22122\n\n10\n\nLower Bound\n(Proposition ??)\n\u22122\n10\n(Propositions ?? & ??)\n\n\u22123\n\n10\n\n\u22124\n\n\u22123\n\n10\n\n10\nSampling Rate\n\nDistortion D = 0.1\n\nDistortion D = 0.01\n\nSfrag replacements\n\nPSfrag replacements\n\nDistortion \u03b1\nSampling Rate \u03c1\n\nDistortion \u03b1\nSampling Rate \u03c1\n\nUpper Bnd.\nReplica Bnd.\nTheorem 4\nTheorem 3\nTheorem 2\nTheorem 1\n\n\u22123\n\n10\n\nDistortion \u03b1\nSampling Rate \u03c1\n\n\u22123\n\n10\n\nUpper Bound [17]\nLower Bound\n(Proposition ??)\n(Propositions ?? & ??)\nDistortion \u03b1\nSampling Rate \u03c1\n\n\u22124\n\n10\n\nSampling Rate\n\nLower Bound\n(Proposition ??)\nopositions ?? & ??)\n\nSampling Rate\n\nUpper Bound [17]\n\nUpper Bnd.\nReplica Bnd.\nTheorem 4\nTheorem 3\nTheorem 2\nTheorem 1\n\n\u22124\n\n10\n\nUpper Bound [17]\n\nUpper Bound [17]\n\nLower Bound\n(Proposition ??)\nopositions ?? & ??)\n\nLower Bound\n(Proposition ??)\n60\n70\n80\n90\n100\n(Propositions ?? & ??)\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\nSNR (dB)\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n60\nSNR (dB)\n\n70\n\n80\n\n90\n\n100\n\nFig. 1. Comparison of the lower bounds in Theorems 1\u20134 for a Bernoulli-Gaussian distribution with sparsity rate \u03ba = 10\u22124 . The top row plots the lower\nbounds on the distortion D as a function of the sampling rate \u03c1 for fixed SNR. The bottom row plots the lower bounds on the sampling rate \u03c1 as a function\nthe SNR for fixed distortions. Also shown is an upper bound derived in [17] and a heuristic bound derived using the standard but nonrigorous replica method\nfrom statistical physics (see [17] for more details). We remark that the \"kinks\" in the upper bound are likely an artifact of the bounded technique used in [17].\n\nA. Comparison of Lower Bounds\nWe begin with a comparison of the lower bounds in Theorems 1\u20134. To illustrate these bounds, we consider the special\ncase of the Bernoulli-Gaussian distribution given by\n(\n0, with probability 1 \u2212 \u03ba\nX=\n(52)\nW, with probability \u03ba\nwhere W is a Gaussian random variable with mean zero and\nvariance 1/\u03ba. This distributionp\nhas polynomial decay rate L =\n1 and limiting constant \u03c4 = 2\u03ba/\u03c0. Moreover, its nonzero\nentropy power NX is equal to the variance VX .\nThe bounds in Theorems 1\u20134 corresponding to the\nBernoulli-Gaussian are shown in Figure 1. In all cases, the\ninitial lower bound given in Theorem 1 is highly sub-optimal\nand does not reflect the true difficultly of the recovery problem.\nBy contrast, the strongest bound in this paper, Theorem 4, is\nin close agreement with the behavior of the upper bound from\n[17].\nThe relative merits of Theorems 2 and 3 depend on the\nproblem regime. When the sampling rate is large relative\n\nto the SNR and the distortion, the difficulty of recovery is\ndominated by the magnitude of the smallest nonzero entries\nand Theorem 2 provides a stronger bound. Conversely, when\nthe sampling rate is small relative to the SNR and distortion,\nthe difficulty of recovery is dominated by the entropy of the\nnonzero entries and Theorem 3 provides a stronger bound.\nThe bounds on the achievable distortion plotted in top left\npanel of Figure 1 show that Theorem 4 can be strictly greater\nthan the maximum of the Theorems 2 and 3.\nB. Lower Bounds for Signal Classes\nThroughout this paper, we have assumed that the underlying\ndistribution pX is known. More realistically though, it may be\nthe case that the distribution pX is known to belong to class\nP of sparse distributions, but is otherwise unknown. In these\ncases, a distortion D is said to be achievable for a class P if\nand only if there exists a fixed estimator \u015c(Y, A) such that\nsup Pr[d(S \u2217 , \u015c(Y, A)) > D] \u2192 0\n\npX \u2208P\n\nas n \u2192 \u221e.\n\n(53)\n\n\f8\n\nAPPROXIMATE SPARSITY PATTERN RECOVERY: INFORMATION-THEORETIC LOWER BOUNDS\n\nSNR = 10 (dB)\n\n0\n\n10\n\n\u22121\n\n\u22121\n\n10\n\n10\n\n\u22122\n\n\u22122\n\n10\nDistortion\n\n10\nDistortion\n\nSNR = 50 (dB)\n\n0\n\n10\n\n\u22123\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n\u22124\n\n10\n\n10\nUpper Bnd.\nSliced\u2212Gaussian Lower Bnd.\nPoint\u2212Mass Lower Bnd.\n\n\u22125\n\n10\n\n\u22124\n\n\u22125\n\n\u22123\n\n10\n\n10\n\n\u22122\n\n10\nSampling Rate\n\n\u22124\n\n10\n\n\u22123\n\n10\n\n10\nSampling Rate\n\nDistortion D = 0.1\n\n\u22122\n\nUpper Bnd.\nSliced\u2212Gaussian Lower Bnd.\nPoint\u2212Mass Lower Bnd.\n\nDistortion D = 0.01\n\n\u22122\n\n10\n\n10\n\nUpper Bnd.\nSliced\u2212Gaussian Lower Bnd.\nPoint\u2212Mass Lower Bnd.\n\nSampling Rate\n\nSampling Rate\n\nUpper Bnd.\nSliced\u2212Gaussian Lower Bnd.\nPoint\u2212Mass Lower Bnd.\n\n\u22123\n\n10\n\n\u22124\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n10\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n60\nSNR (dB)\n\n70\n\n80\n\n90\n\n100\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n60\nSNR (dB)\n\n70\n\n80\n\n90\n\n100\n\nFig. 2. Comparison of the lower bound in Theorem 4 for the sliced-Gaussion\nand point-mass distributions. In both cases, the distributions have second\np\nmoment equal to one, sparsity rate \u03ba = 10\u22124 , and lower bound B = 0.2/\u03ba (i.e. the nonzero entries of X are lower bounded in squared magnitude by\n20% of their average power). The top row plots the lower bounds on the distortion D as a function of the sampling rate \u03c1 for fixed SNR. The bottom row\nplots the lower bounds on the sampling rate \u03c1 as a function the SNR for fixed distortions. Also shown is a minimax upper bound derived in [17] which\napplies universally over the class of bounded signals PBounded (\u03ba, B).\n\nOne class of distributions considered widely throughout the\nliterature is the bounded signal class PBounded (\u03ba, B), i.e. the\nclass of all distributions with sparsity rate \u03ba, second moment\nequal to one, and Pr[|X| < B|X 6= 0] = 0 for some lower\nbound B > 0. In [17], upper bounds on the sampling ratedistoriton function of this class are derived for several different\nrecovery algorithms. In this section, we provide corresponding\ninformation-theoretic lower bounds.\nTo proceed, we use the simple fact that a distortion is not\nachievable for a class of distributions if it is not achievable\nfor each distribution pX in that class. In the following, we\nevaluate Theorem 4 for two carefully chosen distributions in\nthe class PBounded (\u03ba, B).\n\u2022\n\nPoint-Mass Lower Bound: For the first bound, we\nconsider the distribution pX given by\n\uf8f1\n\uf8f4\nif x = 0\n\uf8f21 \u2212 \u03ba\nPr[X = x] = \u03ba (1 \u2212 \u01eb) if x = B\n\uf8f4\n2\n\uf8f3\n\u03ba\u01eb\nif x = 1/\u03ba\u2212(1\u2212\u01eb)B\n\u01eb\n\n\u2022\n\nfor some some \u01eb \u2208 (0, 1). If \u01eb is small, then this\ndistribution places almost all of its nonzero mass at the\nlower bound B, and so P (D; pX ) \u2248 DB 2 for small\ndistortions. Since the distribution is discrete, the nonzero\nentropy power NX is equal to zero.\nSliced-Gaussian Lower Bound: For the second bound,\nwe consider the distribution pX given by\n(\n0,\nwith probability 1 \u2212 \u03ba\nX=\n.\nsgn(W )B + W, with probability \u03ba\nwhere W is a Gaussian random variable with mean zero\n2\nand variance \u03c3W\nscaled so that E[X 2 ] = 1. For this\ndistribution, the function P (D; pX ) is larger than for the\npoint-mass distribution. However, the nonzero entropy\n2\npower is NX = \u03ba \u03c3W\n.\n\nThe bounds in Theorem 4 corresponding to the point-mass\nand sliced-Gaussian distributions are plotted in Figure 2 along\nwith the universal upper bound from [17]. We emphasize that\nthe maximum of the two lower bounds is also a valid lower\n\n\fREEVES AND GASTPAR\n\n9\n\nbound for the bounded signal class.\nThe relative strengths of two lower bounds depend on\nthe problem regime. When the SNR is large relative to the\nsampling rate, the sliced-Gaussion distribution provides a\nstronger bound. Conversely, when the SNR is small relative\nto the sampling rate, the point-mass distribution provides a\nstronger bound.\n\nLow-Distortion Behavior: Let the SNR be fixed. As the\ndesired distortion becomes small, the difficulty of recovery is\ndominated by the relative magnitudes of the smallest nonzero\nentries. If the nonzero entries are bounded away from zero,\nthen the sampling-rate distortion function obeys\n\u03c1\u2217 \u2265 C * log(1/D).\n\nVI. D ISCUSSION\n\nIf the nonzero entries are drawn from a distribution with decay\nrate L, then the sampling rate-distortion function obeys\n\nIn this section, we review the main contributions of the\npaper and discuss various implications of our analysis.\n\n\u03c1\u2217 \u2265 C * D2/L * log(1/D).\nThis behavior can be seen in the bottom rows of Figures 1\nand 2.\n\nA. Overview of results\nThe information-theoretic lower bounds derived in this\npaper, in conjunction with achievable bounds in [17], characterize the fundamental limit of what cannot be recovered in\npresence of noise. The results in this paper can be summarized\nas follows:\n\u2022 Theorem 1 gives an initial lower bound based on Fano's\ninequality. This result, which is closely related to existing\nbounds in the literature, serves as a building block for the\nmain results.\n\u2022 Theorem 2 gives a significantly improved lower bound\nbased on the genie result given in Lemma 1. In conjunction with the upper bounds in [17], Theorem 2 gives a\ntight characterization of the low-distortion behavior of the\nsampling rate-distortion function.\n\u2022 Theorem 3 gives a different lower bound based on the\nentropy power inequality and the asymptotic spectral\nconvergence of i.i.d. random matrices. In conjunction\nwith the upper bounds in [17], Theorem 3 gives a\ntight characterization of the high-SNR behavior of the\nsampling rate-distortion function for settings where the\nnonzero entries are continuously distributed.\n\u2022 Theorem 4 combines Theorem 3 with the genie result\nin Lemma 1 to give the strongest lower bound in the\npaper. This bound combines the low-distortion improvements of Theorem 2 and the high-SNR improvements of\nTheorem 3.\n\nC. Role of Model Assumptions\nThis paper focuses on the setting where a constant fraction\nof the entries are nonzero (Assumption SS1). In principle,\nmany of the tools developed in the paper could also be\nused to address settings where the number of nonzero entries\ngrows sub-linearly with the vector length, and hence there is\na vanishing fraction of nonzero entries.\nOur use of row normalization (Assumption M3) differs from\nmany related works which use column normalization. The\nreason for our scaling is that, from a sampling perspective,\none way to decrease the effect of noise is to take additional\nsamples (all at a fixed per-measurement SNR). If the column\nnorms of the measurement matrix are constrained, then this is\nnot possible since the per-measurement SNR will necessarily\ndecrease as the number of measurements increases. Since it\nis assumed throughout that the sampling rate \u03c1 is a fixed\nconstant, all results in this paper can be compared to existing\nworks under an appropriate rescaling of the SNR.\nThe proofs of Theorems 3 and 4 rely heavily on the\nassumption that the measurement matrices have i.i.d. entries\n(Assumption M4). In [17], it is shown that certain rate-sharing\nmatrices (which are not i.i.d.) can achieve distortions that are\nlower than the bounds given in Theorems 3 and 4. Therefore,\na further contribution of this paper is that i.i.d. matrices are\nstrictly suboptimal in some problem settings.\nA PPENDIX A\nP ROOF OF T HEOREM 1\n\nB. Fundamental Behavior of Sparsity Pattern Recovery\nOur bounds show that the tradeoffs between the sampling\nrate \u03c1, the distortion D, and the SNR can be characterized in\nterms of certain key properties of the underlying distribution\npX . The following limiting behaviors are considered.\nHigh-SNR Behavior: Let the desired distortion D be fixed.\nAs the SNR becomes large, the difficulty of recovery is\ndominated by the entropy of the nonzero entries. If the nonzero\npart of pX has a continuous component with weight \u03c9c and\na relatively large differential entropy, then the sampling ratedistortion function obeys\n\u03c1\u2217 \u2265 \u03c9 c +\n\nC\n.\nlog(snr)\n\nThis behavior can be seen in the top row of Figure 1.\n\nThe cornerstone of this proof is Fano's inequality which\ngives a lower bound on the error probability for any possible\nrecovery algorithm in terms of the mutual information between\nS \u2217 and the pair (Y, A). We assume that the tuple (D, pX , snr)\nis known throughout the system.\nLemma 2 (Fano's Inequality). Let S \u2217 be distributed uniformly\nover all subsets of [n] of size k < n/2. If S \u2217 \u2192 (Y, A) \u2192 \u015c\nforms a Markov chain then\nPr[d(S \u2217 , \u015c) > D] \u2265 1 \u2212\n\nfor all 0 \u2264 D \u2264 1.\n\nI(S \u2217 ; Y, A) + log(2)\n\u0010P\n\u0011\n\u0001\n\u2308Dk\u2309 k\u0001 n\u2212k\u0001\nlog nk \u2212 log\nl=0\nl\nl\n(54)\n\n\f10\n\nAPPROXIMATE SPARSITY PATTERN RECOVERY: INFORMATION-THEORETIC LOWER BOUNDS\n\nProof: We follow the proof of Fano's inequality given in\n[33] with some modifications to handle our error criterion. To\nbegin, we define the random variable\n(\n1, if d(S \u2217 , \u015c) > D\nE=\n,\n0, if d(S \u2217 , \u015c) \u2264 D\n\nwhere (62) follows from the independence of Assumption M3\nand (63) follows from the data processing inequality and the\nfact that S \u2217 \u2192 X \u2192 Y forms a Markov chain.\nNext, we can write\nI(X; Y|A = A) = I(X; AX + snr\u22121/2 W)\n\nand note that Pr[E = 1] = Pr[d(S \u2217 , \u015c) > D].\nUsing the chain rule for entropy, H(E, S \u2217 |Y, A, \u015c) can be\nwritten two ways as\nH(E, S \u2217 |Y, A, \u015c) = H(S \u2217 |Y, A, \u015c) + H(E|S \u2217 , Y, A, \u015c)\n= H(E|Y, A, \u015c) + H(S \u2217 |E, Y, A, \u015c).\n\nBy the Markov property, H(S \u2217 |Y, A, \u015c) = H(S \u2217 |Y, A).\nSince entropy is nonnegative, H(E|S \u2217 , Y, A, \u015c) \u2265 0. Also,\nsince conditioning cannot increase entropy, H(E|Y, A, \u015c) \u2264\nH(E) \u2264 log(2) and H(S \u2217 |E, Y, A, \u015c) \u2264 H(S \u2217 |E, \u015c).\nPutting everything together we obtain\nH(S \u2217 |Y, A) \u2212 log 2 \u2264 H(S \u2217 |E, \u015c)\n\n\u2264 max I(AZ; AZ + snr\u22121/2 W)\nZ\n\nSince the uniform distribution maximizes the entropy of S \u2217 ,\n\u0012 \u0013\nn\nH(S \u2217 |E = 1, \u015c) \u2264 log\n.\n(57)\nk\n\nAlso, since the distortion measure d(*, *) corresponds to the\nmaximum of the two detection error rates, we may assume\nwithout any loss of generality that \u015c has cardinality k.\nTherefore, a simple counting argument gives\n\u0012 \u230aDk\u230b\nX \u0012k \u0013\u0012n \u2212 k \u0013\u0013\n\u2217\nH(S |E = 0, \u015c) \u2264 log\n.\n(58)\nl\nl\nl=0\n\nPlugging (57) and (58) back into (56) and solving for the error\nprobability Pr[E = 1] completes the proof.\nThe next step in the proof is to verify that the right-hand\nside of (54) is bounded away from zero for all sequences of\nproblems obeying the assumptions of Theorem 1. For each\nproblem of size n, let k = \u2308\u03ba n\u2309 where the dependence on n is\nimplicit. Using Stirling's approximation [33, Lemma 17.5.1],\nit is straightforward to verify that\n\u0014\n\u0012 \u0013\n\u0012 \u2308Dk\u2309\nX \u0012k \u0013\u0012n \u2212 k \u0013\u0013\u0015\nn\n1\nlog\n\u2212 log\n= R(D; \u03ba)\nlim\nn\u2192\u221e n\nk\nl\nl\nl=0\n(59)\n\nwhere R(D; \u03ba) is given in (14).\nCombining (54) and (59) it follows that a distortion D is\nnot achievable if\n1\n(60)\nlim sup I(S \u2217 ; Y, A) < R(D; \u03ba).\nn\u2192\u221e n\nThe remainder of the proof is dedicated to upper bounding\nthe left-hand side of (60). Starting with the chain rule for\nmutual information, we have\nI(S \u2217 ; Y, A) = I(S \u2217 ; Y|A) + I(S \u2217 ; A)\n\n(61)\n\n\u2217\n\n(62)\n(63)\n\n(64)\n\n\u0003\n\u0002\nE[ZZT ] = E (X \u2212 E[X])(X \u2212 E[X])T = VX In\u00d7n . (65)\n\nIt is well known (see e.g. [33]) that the maximum of (64) is\nattained when the entries of Z are i.i.d. N (0, VX ), and thus\nwe obtain\nI(X; Y|A = A) \u2264\n\n+ Pr[E = 0]H(S \u2217 |E = 0, \u015c) (56)\n\n\u0001\n\nwhere the maximum is over all n-dimensional random vectors\nZ obeying the power constraint\n\n(55)\n\u2217\n\n= Pr[E = 1]H(S |E = 1, \u015c)\n\n= I(S ; Y|A)\n\u2264 I(X; Y|A)\n\n= I X \u2212 E[X]; A(X \u2212 E[X]) + snr\u22121/2 W\n\n1\nlog det(Im\u00d7m + snr VX AAT ).\n2\n\n(66)\n\nBy the concavity of the log determinant, Hadamard's inequality, and Jensen's inequality we can bound the expectation\nof (66) with respect to a random matrix A obeying the\nnormalization of Assumption M3 as follows:\ni\nh1\nE log det(Im\u00d7m + snr VX AAT )\n2\n\u0002\n\u0001\n1\n\u2264 log det Im\u00d7m + snr VX E AAT ]\n2\nm\n=\nlog(1 + snr VX ).\n2\n\n(67)\n\nAlternatively, starting with Sylvester's determinant theorem,\nwe can write\nh1\ni\nE log det(Im\u00d7m + snr VX AAT )\n2\nh1\ni\n= E log det(In\u00d7n + snr VX AT A)\n2\n\u0002\n\u0001\n1\n\u2264 log det In\u00d7n + snr VX E AT A]\n2\n\u0011\n\u0010\nn\nm\n= log 1 + snr VX .\n(68)\n2\nn\nCombining (66), (67), and (68) gives\nI(X; Y|A)\n\u0012\n\u0011\u0013\n\u0010\nn\nm\nm\nlog(1 + snr VX ), log 1 + snr VX ,\n\u2264 min\n2\n2\nn\nand hence\n1\nI(S \u2217 ; Y, A)\nn\n\u0001\nmin(1, \u03c1)\nlog 1 + max(1, \u03c1)VX snr ,\n<\n2\n\nlim sup\nn\u2192\u221e\n\n(69)\n\nfor any sequence of matrices obeying Assumptions M1-M3.\nCombining (60) and (69) completes the proof of Theorem 1.\n\n\fREEVES AND GASTPAR\n\n11\n\nestimation of S \u2217 . To see why, observe that\n\nA PPENDIX B\nP ROOF OF L EMMA 1\nThis proof is based on a genie argument. Suppose that a\ngenie provides the recovery algorithm with the pair (G, XG )\nwhere G is a subset of the sparsity pattern S \u2217 and XG is a |G|dimensional vector corresponding to the entries of X indexed\nby G. Given this extra information, the recovery algorithm\nmust then determine which of the remaining unknown entries\n{Xi : i \u2208\n/ G} are nonzero. Clearly, any lower bound on the\nachievable distortion D in the genie-aided setting is also a\nlower bound on the achievable distortion in the original setting.\nIn the following sections, we first describe how the genie\nselects the index set G. We then show that the resulting recovery problem is equivalent to the original recovery problem\nwith altered parameters.\n\nI(S \u2217 ; Y, A, G, XG )\n= I(S \u2217 ; Y \u2212 AG XG , A, G, XG )\n\u2217\n\n= I(S ; \u1ef8, A, G, XG )\n\u2217\n\n\u2217\n\n= I(S ; \u1ef8, A, G) + I(S ; XG |\u1ef8, A, G)\n\u2217\n\n= I(S ; \u1ef8, A, G)\n\nS \u2217 \u2192 (\u1ef8, A, G) \u2192 \u015c\n\nPr[Xi \u2264 t| i is not reported] = Pr[Z \u2264 t]\nwhere Z \u223c pZ . By the constraints (17) and (18) it can be\nverified that the function q(x) exists and that q(0) = 0. In\nwords, the genie \"prunes\" the entries of X in a way such that\nthe unreported entries are marginally distributed according to\nthe distribution pZ .\nWe now make several observations. First, since q(0) = 0,\nonly nonzero entries are reported and so G \u2286 S \u2217 . Second,\nsince the indices are selected independently, the remaining\nnonzero entries {Xi : i \u2208 S \u2217 \\G} are i.i.d. according to the\nnonzero part of pZ . Finally, conditioned on the cardinality |G|,\nthe set S \u2217 \\G is distributed uniformly over all subsets of [n]\\G\nof size |S \u2217 | \u2212 |G|.\nAs a consequence of the above observations, the sequence\nof vectors corresponding to X[n]\\G satisfies Assumptions SS1SS2 with distribution pZ . Moreover, if we let \u1ef8 denote\nthe measurements corresponding to the vector X[n]\\G and\nmeasurement matrix A[n]\\G , i.e.\n1\n\u1ef8 = A[n]\\G X[n]\\G + \u221a W,\nsnr\n\n(70)\n\nthen it is straightforward to show that an appropriately normalized version of the measurement model given by (70) obeys\nAssumptions M1-M3 with sampling rate \u03c1\u0303 and signal-to-noise\n \u0303.\nratio snr\n\nB. Lower Bound on Genie-Aided Recovery\nWe now derive a necessary condition for recovery in the\ngenie-aided setting. We begin with the following key fact:\nif the set G is chosen according to the selection strategy\noutlined above, the tuple (\u1ef8, A, G) is a sufficient statistic for\n\n(72)\n(73)\n(74)\n\nwhere: (72) follows from the definition of \u1ef8 ; (73) follows\nfrom the chain rule for mutual information; and (73) follows\nfrom the fact that S \u2217 and XG are conditionally independent\ngiven the pair (\u1ef8, A, G).\nLet \u015c denote the optimal estimate of the sparsity pattern in\nthe genie-aided setting (i.e. the sparsity pattern estimate that\nminimizes the error probability). By the arguments above, we\nknow that\n\nA. Genie Selection Strategy\nThe set G is constructed as follows: each index i =\n1, 2, * * * , n is reported, independently of the other indices, with\nprobability q(Xi ) where the function 0 \u2264 q(x) \u2264 1 is chosen\nsuch that for all t \u2208 R,\n\n(71)\n\n(75)\n\nforms a Markov chain. Also, by the optimality of \u015c and the\nfact that distortion measure d(*, *) corresponds to the maximum\nof the two detection error rates, it can also be shown that\n\u015c contains the set G and has the same cardinality as S \u2217 .\nTherefore, the sparsity pattern distortion can be expressed as\n\u0010 |S \u2217 | \u2212 |G| \u0011\nd(S \u2217 \\G, \u015c\\G).\n(76)\nd(S \u2217 , \u015c) =\n|S \u2217 |\nNote that\n\nlim\n\nn\u2192\u221e\n\n\u0010 |S \u2217 | \u2212 |G| \u0011\n|S \u2217 |\n\n=\n\n\u0010 1 \u2212 \u03ba \u0011\u0010 \u03ba\n\u0011\nX\nZ\n\u03baX\n1 \u2212 \u03baZ\n\n(77)\n\nalmost surely under Assumptions SS1-SS2.\nWe now arrive at the crux of the argument. Suppose that\n \u0303 ).\nthe distortion D\u0303 is not achievable for the tuple (\u03c1\u0303, pZ , snr\nBy (75) and the fact that the observation model given in (70)\n \u0303 ), it follows that the error\ncorresponds to the tuple (\u03c1\u0303, pZ , snr\nprobability\nPr[d(S \u2217 \\G, \u015c\\G) \u2265 D\u0303]\ncorresponding to the genie-aided setting is bounded away from\nzero for all n. By (76) and (77), it then follows that the\ndistortion D is not achievable for the tuple (\u03c1, pX , snr). This\nconcludes the proof of Lemma 1.\nA PPENDIX C\nP ROOF OF T HEOREM 3\nOne weakness of the proof of Theorem 1 is that the\ndata processing inequality used to upper bound the mutual\ninformation I(S \u2217 ; Y|A) in (63) is not tight. In this proof,\nwe derive a stronger upper bound that takes into account the\nfact that the values of the nonzero elements are unknown. We\nassume throughout the proof that the nonzero entropy power\nNX is strictly positive.\nUsing the chain rule for mutual information, I(S \u2217 , X; Y|A)\ncan be written two ways as\nI(S \u2217 , X; Y|A) = I(S \u2217 ; Y|A) + I(X; Y|S \u2217 , A)\n= I(X; Y, A) + I(S \u2217 ; Y|X, A).\n\n\f12\n\nAPPROXIMATE SPARSITY PATTERN RECOVERY: INFORMATION-THEORETIC LOWER BOUNDS\n\nSince S \u2192 X \u2192 (Y, A) forms a Markov chain, the mutual\ninformation I(S \u2217 ; Y|X, A) is equal to zero and\nI(S \u2217 ; Y|A) = I(X; Y|A) \u2212 I(X; Y|S \u2217 , A).\n\n(78)\n\nConceptually, the term I(X; Y|S \u2217 , A) quantifies the amount\nof I(X; Y|A) that is \"used up\" describing the values of the\nnonzero elements, and hence cannot contribute to estimation\nof the sparsity pattern.\nFollowing the proof of Theorem 1, the first term on the\nright-hand side of (78) can be upper bounded as\n\u0001i\n1 h\nI(X; Y|A) \u2264 E log det Im\u00d7m + snr VX AAT\n(79)\n2\nwhere the expectation is taken with respect to the random\nmatrix A.\nTo deal with the second term on the right-hand side of (78)\nwe first consider the case m \u2264 k. If we let\n\u00102\n\u0011\n1\nN (Z) =\nexp\nh(Z)\n(80)\n2\u03c0e\nm\ndenote the entropy power of an m-dimensional random vector\nZ, then it follows straightforwardly that\nI(X; Y|S \u2217 = S, A = A)\n\u221a\n= I(XS ; snrAS XS + W)\n\u221a\n\u221a\n= h( snrAS XS + W) \u2212 h( snrAS XS + W|XS )\n\u0001 m\n\u221a\nm\nlog 2\u03c0e N ( snrAS XS + W) \u2212\nlog(2\u03c0e)\n=\n2\n2\n\u0001\n\u221a\nm\n(81)\nlog N ( snrAS XS + W) .\n=\n2\nUsing a generalization of the entropy power inequality [34],\nwe can write\n\u221a\n\u221a\nN ( snrAS XS + W) \u2265 N ( snrAS XS ) + N (W)\n(82)\n\u0010N \u0011\nX\ndet(AS ATS )1/m + 1,\n\u2265 snr\n\u03ba\n(83)\nwhere NX = \u03baN (Xi |i \u2208 S \u2217 ) denotes the nonzero entropy\npower of pX . Note that the assumption m \u2264 k is critical here\nsince the determinant AS ATS is equal to zero for all m < k.\nPlugging (83) back into (81) leads to\nI(X; Y|S \u2217 , A)\n\u0001i\nm h\n\u2265 E log 1 + snr NX \u03ba\u22121 det(AS \u2217 ATS \u2217 )1/m\n(84)\n2\nwhere the expectation is with respect to the random matrix\nAS\u2217 .\nNext we consider the case m > k. If the matrix AS is full\nrank and we let A\u2020S denote its Moore-Penrose pseudoinverse,\nwe can write\nI(X; Y|S \u2217 = S, A = A)\n\u221a\n= I(XS ; snr XS + A\u2020S W)\n\u221a\n\u221a\n= h( snr XS + A\u2020S W) \u2212 h( snrXS + A\u2020S W|XS )\n\u0001 1\n\u221a\nk\n= log N ( snr XS + A\u2020S W) + log det(ATS AS )\n2\n2\n\u0001\nk\n\u22121\nT\n(85)\n\u2265 log 1 + snr NX \u03ba det(AS AS )1/k\n2\n\nwhere (85) follows again from the entropy power inequality.\nThus, we obtain\nI(X; Y|S \u2217 , A)\n\u0001i\nk h\n(86)\n\u2265 E log 1 + snr NX \u03ba\u22121 det(ATS \u2217 AS \u2217 )1/k ,\n2\nwhere the expectation is with respect to the random matrix\nAS\u2217 .\nFinally, to characterize the asymptotic behavior of the\nbounds in (79), (84), and (86), we use use the fact that the\nspectral distributions of the matrices A and AS converge to\na non-random limit known as the Marcenko\u2013Pastur Law (see\nAppendix D).\nCombining Lemma 3 in Appendix D with the upper bound\n(79) leads immediately to\n1\n1\nlim sup I(X; Y|A) \u2264 V(\u03c1, VX snr).\n2\nn\u2192\u221e n\n\n(87)\n\nSimilarly, combining Lemma 4 in Appendix D with the lower\nbounds (84) and (86) leads to\n1\n1\nI(X; Y|S \u2217 , A) \u2265 \u03baVLB (\u03c1/\u03ba, NX snr)\n(88)\nn\u2192\u221e n\n2\nwhere VLB (r, \u03b3) is given by (35). Plugging these limits back\ninto (78) and (60) completes the proof of Theorem 3.\nlim inf\n\nA PPENDIX D\nA SYMPTOTIC S PECTRAL C ONVERGENCE\nThis appendix states two useful results from random matrix\ntheory and gives bounds on the functions V(r, \u03b3) and VLB (r, \u03b3)\nintroduced Theorem 3.\nLemma 3. [35] Let A denote an m \u00d7 n random matrix\nwhose entries are i.i.d. with mean zero and variance 1/n. If\nm/n \u2192 r as n \u2192 \u221e, then\n\u0001\n1\n(89)\nlim\nlog det Im\u00d7m + \u03b3AAT = V(r, \u03b3)\nn\u2192\u221e n\nalmost surely where V(r, \u03b3) is given by (34).\nLemma 4. [36] Let A denote an m \u00d7 n random matrix\nwhose entries are i.i.d. with mean zero and variance 1/n. If\nm/n \u2192 r as n \u2192 \u221e, then\n\u00011/m \u0010 1 \u00111/r\u22121 1\nlim det(AAT )\n=\n, if r < 1 (90)\nn\u2192\u221e\n1\u2212r\ne\n\u00011/n\n1\nif r = 1 (91)\nlim det(AT A)\n= ,\nn\u2192\u221e\ne\n\u0011\n\u0010\nr\u22121 1\n\u00011/n\nr\nlim det(AT A)\n,\nif r > 1 (92)\n=\nn\u2192\u221e\nr\u22121\ne\nalmost surely.\nUnder the assumptions of Lemma 4, it thus follows that\n\u0001\nlim r log 1 + \u03b3 det(AAT )1/m = VLB (r, \u03b3), r \u2264 1\nn\u2192\u221e\n\n(93)\n\nand\n\u0001\nlim log 1 + r\u03b3 det(AAT )1/n = VLB (r, \u03b3),\n\nn\u2192\u221e\n\nr > 1.\n(94)\n\n\fREEVES AND GASTPAR\n\n13\n\nThe functions V(r, \u03b3) and VLB (r, \u03b3) obey the following\nseries of inequalities:\n\u0001\nr log(1 + r\u03b3) \u2265 min(1, r) log 1 + max(1, r)\u03b3\n(95)\n\u2265 V(r, \u03b3)\n(96)\n\u2265 VLB (r, \u03b3)\n\n(98)\n\nwhere (95) and (96) follow from the concavity of the logarithm\nand Jensen's inequality and (97) follows from (89), (93), (94),\nand Hadamard's inequality.\nThe next result shows that functions V(r, \u03b3) and VLB (r, \u03b3)\nbehave similarly when \u03b3 is large.\nLemma 5. For any r < 1,\nlim max V(s, \u03b3) \u2212 VLB (s, \u03b3) = 0.\n\n(99)\n\n\u03b3\u2192\u221e 0\u2264s\u2264r\n\nProof: With a bit of algebra, it can be verified that\n\n2\n\u03b3(1\u2212r)\n\nlim\n\nD\u21920\n\n\u03ba\u03b1D\n)\n(1 \u2212 \u03ba + \u03b1\u03baD)R( \u03b11 , 1\u2212\u03ba+\u03b1\u03baD\n\nD log(1/D)\n\n= (\u03b1 \u2212 1)\u03ba.\n\n(105)\n\nPlugging (105) back into (102) completes the proof.\n\n(97)\n\n\u0001\n\u2265 min(1, r) log 1 + max(1, r)\u03b3/e ,\n\n\u03b3r \u2212 F (r, \u03b3)\n\u0010r\n1h\n= \u03b3(1 \u2212 r)\n1+\n2\n1\nr\n+\n\u2264\n1 \u2212 r 2\u03b3(1 \u2212 r)\n\nit thus follows that\n\n\u0010\n\n1+r\n1\u2212r\n\n+\n\n1\n2\u03b3(1\u2212r)\n\n\u0011\n\n\u0011\ni\n\u22121 \u22121\n\n(100)\n\n\u221a\nwhere (100) follows from the bound 1 + 2x \u2264 1 + x. Plugging this inequality back into the definition of V(r, \u03b3) gives\nan upper bound VUB (r, \u03b3). At this point it is straightforward\nto verify that\nlim max VUB (s, \u03b3) \u2212 VLB (s, \u03b3)\n\nB. Proof of Corollary 3\nWe begin with distributions that are bounded away from\nzero. By the definition of P (D; \u03ba), it follows straightforwardly\nthat\nP (\u03b1D; pX ) \u2265 \u03b1\u03baDB 2\n\n(106)\n\nfor all distributions pX in the bounded class PBounded (\u03ba, B).\nCombining (106) with Corollary 2 gives\n\u0010\u03b1 \u2212 1\u0011 2\n\u03c1\u2217\nlim inf\n\u2265\n.\n(107)\nD\u21920 log(1/D)\n\u03b1\nB 2 * snr\nSince \u03b1 > 1 is arbitrary, the leading term (\u03b1 \u2212 1)/\u03b1 can be\nmade arbitrarily close to one.\nNext we consider distributions with polynomial decay. In\n[17, Eq. (215)], it is shown that\n\n\u22122/L\nP (\u03b1D; pX )\n1+2/L \u03ba \u03c4\n=\n\u03b1\n*\n(108)\nD\u21920 D 1+2/L\n1 + 2L\nfor all distributions pX in the polynomial decay class\nPPoly. (\u03ba, L, \u03c4 ). Combining (108) with Corollary 2 gives\n\u0010 \u03b1 \u2212 1 \u0011 2(1 + 2/L)\n\u03c1\u2217\n\u2265\n. (109)\nlim inf 2/L\nD\u21920 D\nlog(1/D)\n\u03b11+2/L \u03c4 \u22122/L * snr\n\nlim\n\nSince \u03b1 > 1 is arbitrary, the leading term on the right-hand\nside of (109) can be optimized by choosing \u03b1 = 1+L/2. This\ncompletes the proof.\n\n\u03b3\u2192\u221e 0\u2264s\u2264r\n\n= lim VUB (r, \u03b3) \u2212 VLB (r, \u03b3) = 0,\n\u03b3\u2192\u221e\n\nwhich completes the proof.\n\nP ROOFS\n\nOF\n\nA PPENDIX E\nL OW-D ISTORTION B EHAVIOR\n\nP ROOFS\n\nA. Proof of Corollary 4\n\nA. Proof of Corollary 2\nFor this proof we begin with the bound in Corollary 1\nevaluated with D\u2032 = min(1, \u03b1D). For all D < 1/\u03b1, this gives\n\u03c1\u2217 \u2265\n\u2265\n\n\u03ba\u03b1D\n2(1 \u2212 \u03ba + \u03b1\u03baD)R( \u03b11 , 1\u2212\u03ba+\u03b1\u03baD\n)\nlog(1 + P (\u03b1D; pX ) snr)\n\u03ba\u03b1D\n2(1 \u2212 \u03ba + \u03b1\u03baD)R( \u03b11 , 1\u2212\u03ba+\u03b1\u03baD\n)\n\nP (\u03b1D; pX ) snr\n\n(101)\n\nlim\n\nH(c p)\n= c,\np log(1/p)\n\nFor this result, we compute the infinite SNR limit of the\nleft-hand side of (33). Since, the achievable distortion is nonincreasing in the SNR, this limit gives a valid lower bound for\nany SNR. Using Lemma 5 in Appendix D, we have\nlim V(\u03c1, VX snr) \u2212 \u03baVLB (\u03c1/\u03ba, NX snr)\n\nsnr\u2192\u221e\n\n,\n\n(102)\n\nwhere (102) follows from the bound log(1 + x) \u2264 x.\nNext, we consider the numerator in (102). Observe that\n\u0011\n\u00101\n\u03ba\u03b1D\nR ,\n\u03b1 1 \u2212 \u03ba + \u03b1\u03baD\n\u0010\n\u0011 \u0010\n\u0011 \u00101\u0011\n\u03ba\u03b1D\n\u03ba\u03b1D\n=H\n\u2212\nH\n1 \u2212 \u03ba + \u03b1\u03baD\n1 \u2212 \u03ba + \u03b1\u03baD\n\u03b1\n\u0011\n\u0010\n\u0011\n\u0010\n\u03baD\n1\u2212\u03ba\nH\n.\n(103)\n\u2212\n1 \u2212 \u03ba + \u03b1\u03baD\n1\u2212\u03ba\nUsing the fact that, for any constant c > 0,\np\u21920\n\nA PPENDIX F\nH IGH -SNR B EHAVIOR\n\nOF\n\n(104)\n\n= lim VLB (\u03c1, VX snr) \u2212 \u03baVLB (\u03c1/\u03ba, NX snr)\nsnr\u2192\u221e\n\u0010V \u0011\n\u0001\nX\n1\n+ (1 \u2212 \u03c1) log 1\u2212\u03c1\n\u2212 (\u03ba \u2212 \u03c1) log\n= \u03c1 log\nNX\n\n\u03ba\n\u03ba\u2212\u03c1\n\n\u0001\n\nwhere we have used the fact that \u03c1 and \u03c1/\u03ba are both less than\none.\nB. Proof of Corollary 6\nSimilar to the proof of Corollary 4, we study the high SNR\nbehavior of the left-hand side of (33). To begin, let (D, pX )\nbe a fixed pair satisfying (38). For each \u03b3 \u2265 0, let \u03c1\u03b3 denote\nthe unique solution to the fixed point equation:\nV(\u03c1\u03b3 , VX \u03b3) \u2212 \u03baVLB (\u03c1\u03b3 /\u03ba, NX \u03b3) = 2R(D; \u03ba).\n\n(110)\n\n\f14\n\nAPPROXIMATE SPARSITY PATTERN RECOVERY: INFORMATION-THEORETIC LOWER BOUNDS\n\nClearly, \u03c1\u03b3 gives a lower bound on the sampling rate distortion\nfunction \u03c1\u2217 evaluated with snr = \u03b3.\nWe are interested in the behavior of \u03c1\u03b3 as \u03b3 becomes\nlarge. By Corollary 4 it follows that \u03c1\u03b3 \u2265 \u03ba for all \u03b3. By\ninspection of the left-hand side of (110), it also follows that\nlim sup\u03b3\u2192\u221e \u03c1\u03b3 \u2264 \u03ba, since otherwise the left-hand side would\nincrease without bound as \u03b3 \u2192 \u221e. Therefore, we conclude\nthat\n\u03c1\u03b3 = \u03ba + o(\u03b3),\n\n(111)\n\nwhere, for a function f (x), the notation f (x) = o(x) means\nthat limx\u2192\u221e f (x) = 0.\nNow, starting with the first term on the left-hand side of\n(110), we can write\nV(\u03c1\u03b3 , VX \u03b3)\n= VLB (\u03c1\u03b3 , VX \u03b3) + o(\u03b3)\n= \u03c1\u03b3 log(\u03b3) +\n= \u03c1\u03b3 log(\u03b3) +\n\n(112)\n\n1/\u03c1\u03b3 \u22121 1\n1\n\u03c1\u03b3 log VX 1\u2212\u03c1\ne\n\u03b3\n\u00011/\u03ba\u22121 1 \u0001\n1\n\u03ba log VX 1\u2212\u03ba\ne +\n\n\u0001\n\n\u0001\n\n+ o(\u03b3)\n\n(113)\n\no(\u03b3)\n\n(114)\n\nwhere: (112) follows from Lemma 5 in Appendix D; (113)\nfollows from the definition of VLB (r, \u03b3) and the fact that \u03c1\u03b3\nis eventually less than one; and (114) follows from (111).\nSimilarly, starting with the second term on the left-hand side\nof (110), we can write\n\u03baVLB (\n\n\u03c1\u03b3\n\u03ba , NX\n\n\u03b3)\n\u03c1\u03b3 \u0001\u03c1\u03b3 /\u03ba\u22121 1 \u0001\n\u03c1\u03b3\n\u03ba \u03c1\u03b3 \u2212\u03ba\ne\n\u0001\nNX 1e + o(\u03b3)\n\n= \u03ba log(\u03b3) + \u03ba log NX\n= \u03ba log(\u03b3) + \u03ba log\n\n+ o(\u03b3) (115)\n(116)\n\nwhere (115) follows from the definition of VLB (r, \u03b3) and the\nfact that \u03c1\u03b3 /\u03ba > 1, and (116) follows from (111).\nPlugging (114) and (116) back into (110) gives\n(\u03c1\u03b3 \u2212 \u03ba) log(\u03b3) + o(\u03b3)\n\u0010V \u0011\n\u0010 1 \u0011\nX\n= 2R(D; \u03ba) \u2212 \u03ba log\n. (117)\n\u2212 (1 \u2212 \u03ba) log\nNX\n1\u2212\u03ba\n\nSince \u03c1\u03b3 is a lower bound on the sampling rate-distortion\nfunction, the proof is complete.\nACKNOWLEDGMENT\nWe would like to thank Martin Wainwright for helpful\ndiscussions and pointers in early versions of this work and\nthe anonymous reviewers for their helpful comments and\nsuggestions.\nR EFERENCES\n[1] D. L. Donoho, \"Compressed sensing,\" IEEE Trans. Inf. Theory, vol. 52,\nno. 4, pp. 1289\u20131306, Apr. 2006.\n[2] E. J. Cand\u00e8s, J. Romberg, and T. Tao, \"Robust uncertainty principles:\nexact signal reconstruction from highly incomplete frequency information,\" IEEE Trans. Inf. Theory, vol. 52, no. 2, pp. 489\u2013509, Feb. 2006.\n[3] E. J. Cand\u00e8s and T. Tao, \"Near optimal signal recovery from random\nprojections: Universal encoding strategies?\" IEEE Trans. Inf. Theory,\nvol. 52, no. 12, pp. 5406\u20135425, Dec. 2006.\n[4] R. A. DeVore and G. G. Lorentz, Constructive Approximation. New\nYork, NY: Springer Verlag, 1993.\n[5] S. S. Chen, D. L. Donoho, and M. A. Saunders, \"Atomic decomposition\nby basis pursuit,\" SIAM J. of Sci. Comp., vol. 20, no. 1, pp. 33\u201361, 1999.\n\n[6] A. J. Miller, Subset selection in regression. New York, NY: ChapmanHall, 1990.\n[7] N. Meinshausen and P. B\u00fchlmann, \"High-dimensional graphs and variable selection with the lasso,\" Annals of Stat., vol. 34, pp. 1436\u20131462,\n2006.\n[8] P. Zhao and B. Yu, \"On model selection consistency of lasso,\" J. of\nMachine Learning Research, vol. 51, no. 10, pp. 2541\u20132563, Nov. 2006.\n[9] M. J. Wainwright, \"Sharp thresholds for noisy and high-dimensional recovery of sparsity using l1 -constrained quadratic programming (lasso),\"\nIEEE Trans. Inf. Theory, vol. 55, no. 5, pp. 2183\u20132202, May 2009.\n[10] --, \"Information-theoretic limitations on sparsity recovery in the highdimensional and noisy setting.\" IEEE Trans. Inf. Theory, vol. 55, pp.\n5728\u20135741, Dec. 2009.\n[11] A. K. Fletcher, S. Rangan, and V. K. Goyal, \"Necessary and sufficient\nconditions for sparsity pattern recovery,\" IEEE Trans. Inf. Theory,\nvol. 55, no. 12, pp. 5758\u20135772, Dec. 2009.\n[12] W. Wang, M. J. Wainwright, and K. Ramchandran, \"Informationtheoretic limits on sparse signal recovery: Dense versus sparse measurement matrices,\" IEEE Trans. Inf. Theory, vol. 56, no. 6, pp. 2967\u20132979,\nJun. 2010.\n[13] M. Akcakaya and V. Tarokh, \"Shannon theoretic limits on noisy compressive sampling,\" IEEE Trans. Inf. Theory, vol. 56, no. 1, pp. 492\u2013504,\nJan. 2010.\n[14] S. Aeron, V. Saligrama, and M. Zhao, \"Information theoretic bounds\nfor compressed sensing,\" IEEE Trans. Inf. Theory, vol. 56, no. 10, pp.\n5111\u20135130, Oct. 2010.\n[15] G. Reeves, \"Sparse signal sampling using noisy linear projections,\"\nDepartment of EECS, UC Berkeley, Tech. Rep. UCB/EECS-2008-3, Jan.\n2008.\n[16] G. Reeves and M. Gastpar, \"Sampling bounds for sparse support\nrecovery in the presence of noise,\" in Proc. IEEE Int. Symp. on Inf.\nTheory, Toronto, Canada, Jul. 2008.\n[17] --, \"The sampling rate-distortion tradeoff for sparsity pattern recovery\nin compressed sensing,\" IEEE Transactions on Information Theory,\nvol. 58, no. 5, pp. 3065\u20133092, May 2012.\n[18] T. Tanaka, \"A statistical-mechanics approach to large-system analysis of\ncdma multiuser detectors,\" IEEE Trans. Inf. Theory, vol. 48, no. 11, pp.\n2888\u20132910, Nov. 2002.\n[19] R. R. Muller, \"Channel capacity and minimum probability of error in\nlarge dual antenna array systems with binary modulation,\" IEEE Trans.\nInf. Theory, vol. 51, no. 11, pp. 2821\u20132822, Nov. 2003.\n[20] D. Guo and S. Verd\u00fa, \"Randomly spread CDMA: Asymptotics via\nstatistical physics,\" IEEE Trans. Inf. Theory, vol. 51, no. 6, pp. 1983\u2013\n2010, Jun. 2005.\n[21] Y. Kabashima, T. Wadayama, and T. Tanaka, \"A typical reconstruction\nlimit of compressed sensing based on lp norm minimization,\" J. Stat.\nMech., 2009.\n[22] S. Rangan, A. K. Fletcher, and V. K. Goyal, \"Asymptotic analysis of\nmap estimation via the replica method and applications to compressed\nsensing,\" in Proc. Neural Information Processing Systems Conf., vol. 22,\nVancouver, CA, Dec. 2009, pp. 1545\u20131553.\n[23] D. Guo, D. Baron, and S. Shamai, \"A single-letter characterization of\noptimal noisy compressed sensing,\" in Proc. Allerton Conf. on Comm.,\nControl, and Computing, Monticello, IL, Sep. 2009.\n[24] M. Gastpar and Y. Bresler, \"On the necessary density for spectrum-blind\nnonuniform sampling subject to quantization,\" in Proc. IEEE Int. Conf.\nAcoustics, Speech and Signal Processing, Istanbul, Turkey, Jun. 2000,\npp. 248\u2013351.\n[25] C. Weidmann, \"Oligoquantization in low-rate lossy source coding,\"\nPh.D. dissertation, EPFL, Lausanne, Switzerland, Jul. 2000.\n[26] A. K. Fletcher, S. Rangan, V. K. Goyal, and K. Ramchandran, \"Denoising by sparse approximation: Error bounds based on rate-distortion\ntheory,\" J. on Applied Signal Processing., vol. 2006, pp. 1\u201319, Mar.\n2006.\n[27] S. Sarvotham, D. Baron, and R. G. Baraniuk, \"Measurements vs. bits:\nCompressed sensing meets information theory,\" in Proc. Allerton Conf.\non Comm., Control, and Computing, Monticello, IL, Sep. 2006.\n[28] A. K. Fletcher, S. Rangan, and V. K. Goyal, \"Rate-distortion bounds for\nsparse approximation,\" in Proc. IEEE Statist. Sig. Process. Workshop,\nMadison, WI, Aug. 2007, pp. 254\u2013258.\n[29] --, \"Compressive sampling and lossy compression,\" IEEE Signal\nProcessing Magazine, vol. 25, no. 2, pp. 48\u201356, Mar. 2008.\n[30] C. Weidmann and M. Vetterli, \"Rate distortion behavior of sparse\nsources,\" Dec. 2008, submitted to IEEE Trans. Inf. Thoery.\n[31] E. J. Cand\u00e8s, J. Romberg, and T. Tao, \"Stable signal recovery from\nincomplete and inaccurate measurements,\" Comm. on Pure and Applied\nMath., vol. 59, pp. 1207\u20131223, Feb. 2006.\n\n\fREEVES AND GASTPAR\n\n[32] G. Reeves, \"Sparsity pattern recovery in compressed sensing,\" Ph.D.\ndissertation, University of California, Berkeley, 2011.\n[33] T. M. Cover and J. A. Thomas, Elements of Information Theory. New\nYork: Wiley, 1991.\n[34] R. Zamir and M. Feder, \"A generalization of the entropy power inequality with applications,\" IEEE Transactions on Information Theory,\nvol. 39, no. 5, pp. 1723\u20131728, September 1993.\n[35] S. Verd\u00fa and S. Shamai, \"Spectral efficiency of CDMA with random\nspreading,\" IEEE Trans. Inf. Theory, vol. 45, no. 2, pp. 622\u2013640, Mar.\n1999.\n[36] J. Salo, D. Seethaler, and A. Skupch, \"On the asymptotic geometric\nmean of mimo channel eigenvalues,\" in Proc. IEEE Int. Symp. on Inform.\nTheory, Seattle, WA, Jul. 2006.\n\nGalen Reeves received the B.S. degree in electrical and computer engineering\nfrom Cornell University in 2005 and the M.S. and Ph.D. degrees in electrical\nengineering and computer sciences from the University of California at\nBerkeley in 2007 and 2011 respectively. He is currently a postdoctoral scholar\nat Stanford university. His his research interests include compressed sensing,\nstatistical signal processing, information theory, and machine learning.\n\nMichael Gastpar received the Dipl. El.-Ing. degree from ETH Z\u00fcrich, in\n1997, the M.S. degree from the University of Illinois at Urbana-Champaign,\nUrbana, IL, in 1999, and the Doctorat \u00e8s Science degree from Ecole Polytechnique F\u00e9d\u00e9rale (EPFL), Lausanne, Switzerland, in 2002, all in electrical\nengineering. He was also a student in engineering and philosophy at the\nUniversities of Edinburgh and Lausanne.\nHe is a Professor in the School of Computer and Communication Sciences,\nEcole Polytechnique F\u00e9d\u00e9rale (EPFL), Lausanne, Switzerland. He was an\nAssistant (2003-2008) and tenured Associate Professor (2008-2011) with the\nDepartment of Electrical Engineering and Computer Sciences, University of\nCalifornia, Berkeley, where he still holds a faculty position. He also holds a\nfaculty position at Delft University of Technology, The Netherlands, and was a\nResearcher with the Mathematics of Communications Department, Bell Labs,\nLucent Technologies, Murray Hill, NJ. His research interests are in network\ninformation theory and related coding and signal processing techniques, with\napplications to sensor networks and neuroscience.\nDr. Gastpar won the 2002 EPFL Best Thesis Award, an NSF CAREER\nAward in 2004, an Okawa Foundation Research Grant in 2008, and an ERC\nStarting Grant in 2010. He is an Information Theory Society Distinguished\nLecturer (20092011). He was an Associate Editor for Shannon Theory for\nthe IEEE TRANSACTIONS ON INFORMATION THEORY (2008\u20132011),\nand he has served as Technical Program Committee Co-Chair for the 2010\nInternational Symposium on Information Theory, Austin, TX.\n\n15\n\n\f"}