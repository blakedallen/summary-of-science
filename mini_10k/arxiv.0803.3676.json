{"id": "http://arxiv.org/abs/0803.3676v1", "guidislink": true, "updated": "2008-03-26T08:22:33Z", "updated_parsed": [2008, 3, 26, 8, 22, 33, 2, 86, 0], "published": "2008-03-26T08:22:33Z", "published_parsed": [2008, 3, 26, 8, 22, 33, 2, 86, 0], "title": "Variable selection for the multicategory SVM via adaptive sup-norm\n  regularization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0803.3834%2C0803.2732%2C0803.3042%2C0803.4018%2C0803.3986%2C0803.3130%2C0803.3102%2C0803.1040%2C0803.3239%2C0803.4276%2C0803.1848%2C0803.0582%2C0803.3332%2C0803.1947%2C0803.2364%2C0803.1606%2C0803.3021%2C0803.1108%2C0803.3630%2C0803.2747%2C0803.3164%2C0803.1177%2C0803.1436%2C0803.0295%2C0803.3455%2C0803.2443%2C0803.3270%2C0803.3899%2C0803.4403%2C0803.2111%2C0803.1770%2C0803.0685%2C0803.3656%2C0803.4231%2C0803.1623%2C0803.0436%2C0803.3676%2C0803.1461%2C0803.1154%2C0803.1290%2C0803.3230%2C0803.1213%2C0803.3477%2C0803.4072%2C0803.2060%2C0803.2300%2C0803.3261%2C0803.2083%2C0803.1981%2C0803.0161%2C0803.4055%2C0803.0605%2C0803.2704%2C0803.1448%2C0803.0731%2C0803.0260%2C0803.1398%2C0803.4359%2C0803.3510%2C0803.4022%2C0803.2036%2C0803.3335%2C0803.3364%2C0803.0521%2C0803.2095%2C0803.2322%2C0803.3832%2C0803.2635%2C0803.3503%2C0803.1543%2C0803.0920%2C0803.2165%2C0803.3009%2C0803.4506%2C0803.3980%2C0803.2355%2C0803.2519%2C0803.0391%2C0803.3038%2C0803.2881%2C0803.2956%2C0803.2906%2C0803.2430%2C0803.1267%2C0803.1181%2C0803.1594%2C0803.1106%2C0803.3720%2C0803.2447%2C0803.0538%2C0803.3965%2C0803.2979%2C0803.1767%2C0803.0698%2C0803.0644%2C0803.1493%2C0803.1488%2C0803.1018%2C0803.2212%2C0803.3584%2C0803.4061&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Variable selection for the multicategory SVM via adaptive sup-norm\n  regularization"}, "summary": "The Support Vector Machine (SVM) is a popular classification paradigm in\nmachine learning and has achieved great success in real applications. However,\nthe standard SVM can not select variables automatically and therefore its\nsolution typically utilizes all the input variables without discrimination.\nThis makes it difficult to identify important predictor variables, which is\noften one of the primary goals in data analysis. In this paper, we propose two\nnovel types of regularization in the context of the multicategory SVM (MSVM)\nfor simultaneous classification and variable selection. The MSVM generally\nrequires estimation of multiple discriminating functions and applies the argmax\nrule for prediction. For each individual variable, we propose to characterize\nits importance by the supnorm of its coefficient vector associated with\ndifferent functions, and then minimize the MSVM hinge loss function subject to\na penalty on the sum of supnorms. To further improve the supnorm penalty, we\npropose the adaptive regularization, which allows different weights imposed on\ndifferent variables according to their relative importance. Both types of\nregularization automate variable selection in the process of building\nclassifiers, and lead to sparse multi-classifiers with enhanced\ninterpretability and improved accuracy, especially for high dimensional low\nsample size data. One big advantage of the supnorm penalty is its easy\nimplementation via standard linear programming. Several simulated examples and\none real gene data analysis demonstrate the outstanding performance of the\nadaptive supnorm penalty in various data settings.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0803.3834%2C0803.2732%2C0803.3042%2C0803.4018%2C0803.3986%2C0803.3130%2C0803.3102%2C0803.1040%2C0803.3239%2C0803.4276%2C0803.1848%2C0803.0582%2C0803.3332%2C0803.1947%2C0803.2364%2C0803.1606%2C0803.3021%2C0803.1108%2C0803.3630%2C0803.2747%2C0803.3164%2C0803.1177%2C0803.1436%2C0803.0295%2C0803.3455%2C0803.2443%2C0803.3270%2C0803.3899%2C0803.4403%2C0803.2111%2C0803.1770%2C0803.0685%2C0803.3656%2C0803.4231%2C0803.1623%2C0803.0436%2C0803.3676%2C0803.1461%2C0803.1154%2C0803.1290%2C0803.3230%2C0803.1213%2C0803.3477%2C0803.4072%2C0803.2060%2C0803.2300%2C0803.3261%2C0803.2083%2C0803.1981%2C0803.0161%2C0803.4055%2C0803.0605%2C0803.2704%2C0803.1448%2C0803.0731%2C0803.0260%2C0803.1398%2C0803.4359%2C0803.3510%2C0803.4022%2C0803.2036%2C0803.3335%2C0803.3364%2C0803.0521%2C0803.2095%2C0803.2322%2C0803.3832%2C0803.2635%2C0803.3503%2C0803.1543%2C0803.0920%2C0803.2165%2C0803.3009%2C0803.4506%2C0803.3980%2C0803.2355%2C0803.2519%2C0803.0391%2C0803.3038%2C0803.2881%2C0803.2956%2C0803.2906%2C0803.2430%2C0803.1267%2C0803.1181%2C0803.1594%2C0803.1106%2C0803.3720%2C0803.2447%2C0803.0538%2C0803.3965%2C0803.2979%2C0803.1767%2C0803.0698%2C0803.0644%2C0803.1493%2C0803.1488%2C0803.1018%2C0803.2212%2C0803.3584%2C0803.4061&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Support Vector Machine (SVM) is a popular classification paradigm in\nmachine learning and has achieved great success in real applications. However,\nthe standard SVM can not select variables automatically and therefore its\nsolution typically utilizes all the input variables without discrimination.\nThis makes it difficult to identify important predictor variables, which is\noften one of the primary goals in data analysis. In this paper, we propose two\nnovel types of regularization in the context of the multicategory SVM (MSVM)\nfor simultaneous classification and variable selection. The MSVM generally\nrequires estimation of multiple discriminating functions and applies the argmax\nrule for prediction. For each individual variable, we propose to characterize\nits importance by the supnorm of its coefficient vector associated with\ndifferent functions, and then minimize the MSVM hinge loss function subject to\na penalty on the sum of supnorms. To further improve the supnorm penalty, we\npropose the adaptive regularization, which allows different weights imposed on\ndifferent variables according to their relative importance. Both types of\nregularization automate variable selection in the process of building\nclassifiers, and lead to sparse multi-classifiers with enhanced\ninterpretability and improved accuracy, especially for high dimensional low\nsample size data. One big advantage of the supnorm penalty is its easy\nimplementation via standard linear programming. Several simulated examples and\none real gene data analysis demonstrate the outstanding performance of the\nadaptive supnorm penalty in various data settings."}, "authors": ["Hao Helen Zhang", "Yufeng Liu", "Yichao Wu", "Ji Zhu"], "author_detail": {"name": "Ji Zhu"}, "author": "Ji Zhu", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/08-EJS122", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0803.3676v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0803.3676v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/08-EJS122 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62H30 (Primary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0803.3676v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0803.3676v1", "journal_reference": "Electronic Journal of Statistics 2008, Vol. 2, 149-167", "doi": "10.1214/08-EJS122", "fulltext": "arXiv:0803.3676v1 [stat.ME] 26 Mar 2008\n\nElectronic Journal of Statistics\nVol. 2 (2008) 149\u2013167\nISSN: 1935-7524\nDOI: 10.1214/08-EJS122\n\nVariable selection for the multicategory\nSVM via adaptive sup-norm\nregularization\nHao Helen Zhang\nDepartment of Statistics\nNorth Carolina State University\nRaleigh, NC 27695\ne-mail: hzhang2@stat.ncsu.edu\n\nYufeng Liu\u2217\nDepartment of Statistics and Operations Research\nCarolina Center for Genome Sciences\nUniversity of North Carolina\nChapel Hill, NC 27599\ne-mail: yfliu@email.unc.edu\n\nYichao Wu\nDepartment of Operations Research and Financial Engineering\nPrinceton University\nPrinceton, NJ 08544\ne-mail: yichaowu@princeton.edu\n\nJi Zhu\nDepartment of Statistics\nUniversity of Michigan\nAnn Arbor, MI 48109\ne-mail: jizhu@umich.edu\nAbstract: The Support Vector Machine (SVM) is a popular classification\nparadigm in machine learning and has achieved great success in real applications. However, the standard SVM can not select variables automatically\nand therefore its solution typically utilizes all the input variables without\ndiscrimination. This makes it difficult to identify important predictor variables, which is often one of the primary goals in data analysis. In this paper,\nwe propose two novel types of regularization in the context of the multicategory SVM (MSVM) for simultaneous classification and variable selection. The MSVM generally requires estimation of multiple discriminating\nfunctions and applies the argmax rule for prediction. For each individual\n\u2217 Corresponding author. The authors thank the editor Professor Larry Wasserman, the\nassociate editor, and two reviewers for their constructive comments and suggestions. Liu's\nresearch was supported in part by the National Science Foundation DMS-0606577 and DMS0747575. Wu's research was supported by the National Institute of Health NIH R01-GM07261.\nZhang's research was supported in part by the National Science Foundation DMS-0645293 and\nthe National Institute of Health NIH/NCI R01 CA-085848. Zhu's research was supported in\npart by the National Science Foundation DMS-0505432 and DMS-0705532.\n\n149\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n150\n\nvariable, we propose to characterize its importance by the supnorm of its\ncoefficient vector associated with different functions, and then minimize the\nMSVM hinge loss function subject to a penalty on the sum of supnorms. To\nfurther improve the supnorm penalty, we propose the adaptive regularization, which allows different weights imposed on different variables according\nto their relative importance. Both types of regularization automate variable selection in the process of building classifiers, and lead to sparse multiclassifiers with enhanced interpretability and improved accuracy, especially\nfor high dimensional low sample size data. One big advantage of the supnorm penalty is its easy implementation via standard linear programming.\nSeveral simulated examples and one real gene data analysis demonstrate\nthe outstanding performance of the adaptive supnorm penalty in various\ndata settings.\nAMS 2000 subject classifications: Primary 62H30.\nKeywords and phrases: Classification, L1 -norm penalty, multicategory,\nsup-norm, SVM.\nReceived September 2007.\n\nContents\n1\n2\n3\n4\n5\n\nIntroduction . . . . . . . . . . . .\nMethodology . . . . . . . . . . .\nComputational Algorithms . . .\nAdaptive Penalty . . . . . . . . .\nSimulation . . . . . . . . . . . . .\n5.1 Five-Class Example . . . .\n5.2 Four-Class Linear Example\n5.3 Nonlinear Example . . . . .\n6 Real Example . . . . . . . . . . .\n7 Discussion . . . . . . . . . . . . .\nAppendix . . . . . . . . . . . . . . .\nLiterature Cited . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n150\n151\n155\n156\n157\n157\n158\n160\n162\n164\n165\n165\n\n1. Introduction\nIn supervised learning problems, we are given a training set of n examples from\nK \u2265 2 different populations. For each example in the training set, we observe\nits covariate xi \u2208 Rd and the corresponding label yi indicating its membership.\nOur ultimate goal is to learn a classification rule which can accurately predict\nthe class label of a future example based on its covariate. Among many classification methods, the Support Vector Machine (SVM) has gained much popularity in both machine learning and statistics. The seminal work by Vapnik\n(1995, 1998) has laid the foundation for the general statistical learning theory and the SVM, which furthermore inspired various extensions on the SVM.\nFor other references on the binary SVM, see Christianini and Shawe-Taylor\n(2000), Sch\u00f6lkopf and Smola (2002), and references therein. Recently a few attempts have been made to generalize the SVM to multiclass problems, such\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n151\n\nas Vapnik (1998), Weston and Watkins (1999), Crammer and Singer (2001),\nLee et al. (2004), Liu and Shen (2006), and Wu and Liu (2007a).\nWhile the SVM outperforms many other methods in terms of classification\naccuracy in numerous real problems, the implicit nature of its solution makes\nit less attractive in providing insight into the predictive ability of individual\nvariables. Often times, selecting relevant variables is the primary goal of data\nmining. For the binary SVM, Bradley and Mangasarian (1998) demonstrated\nthe utility of the L1 penalty, which can effectively select variables by shrinking\nsmall or redundant coefficients to zero. Zhu et al. (2003) provides an efficient algorithm to compute the entire solution path for the L1 -norm SVM. Other forms\nof penalty have also been studied in the context of binary SVMs, such as the L0\npenalty (Weston et al., 2003), the SCAD penalty (Zhang et al., 2006), the Lq\npenalty (Liu et al., 2007), the combination of L0 and L1 penalty (Liu and Wu,\n2007), the combination of L1 and L2 penalty (Wang et al., 2006), the F\u221e norm\n(Zou and Yuan, 2006), and others (Zhao et al., 2006; Zou, 2006).\nFor multiclass problems, variable selection becomes more complex than the\nbinary case, since the MSVM requires estimation of multiple discriminating\nfunctions, among which each function has its own subset of important predictors.\nOne natural idea is to extend the L1 SVM to the L1 MSVM, as done in the recent\nwork of Lee et al. (2006) and Wang and Shen (2007b). However, the L1 penalty\ndoes not distinguish the source of coefficients. It treats all the coefficients equally,\nno matter whether they correspond to the same variable or different variables,\nor they are more likely to be relevant or irrelevant. In this paper, we propose\na new regularized MSVM for more effective variable selection. In contrast to\nthe L1 MSVM, which imposes a penalty on the sum of absolute values of all\ncoefficients, we penalize the sup-norm of the coefficients associated with each\nvariable. The proposed method is shown to be able to achieve a higher degree\nof model parsimony than the L1 MSVM without compromising classification\naccuracy.\nThis paper is organized as follows. Section 2 formulates the sup-norm regularization for the MSVM. Section 3 proposes an efficient algorithm to implement\nthe MSVM. Section 4 discusses an adaptive approach to improve performance\nof the sup-norm MSVM by allowing different penalties for different covariates\naccording to their relative importance. Numerical results on simulated and gene\nexpression data are given in Sections 5 and 6, followed by a summary.\n2. Methodology\nIn K-category classification problems, we code y as {1, . . . , K} and define f =\n(f1 , . . . , fK ) as a decision function vector. Each fk , a mapping from the input\ndomain Rd to R, represents the strength of the evidence that an example with\ninput x belongs to the class k; k = 1, . . . , K. A classifier induced by f ,\n\u03c6(x) = arg max fk (x),\nk=1,...,K\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n152\n\nassigns an example with x to the class with the largest fk (x). We assume the\nn training pairs {(xi , yi ), i = 1, . . . , n} are independently and identically distributed according to an unknown probability distribution P (x, y). Given a\nclassifier f , its performance is measured by the generalization error, GE(f ) =\nP (Y 6= arg maxk fk (X)) = E(X,Y ) [I(Y 6= arg maxk fk (X))].\nLet pk (x) = Pr(Y = k|X = x) be the conditional probability of class k given\nX = x. The Bayes rule which minimizes the GE is then given by\n\u03c6B (x) = arg min\n\nk=1,...,K\n\n[1 \u2212 pk (x)] = arg max pk (x).\nk=1,...,K\n\n(2.1)\n\nPq\nFor nonlinear problems, we assume fk (x) = bk + j=1 wkj hj (x) using a set\nof basis functions {hj (x)}. This linear representation of a nonlinear classifier\nthrough basis functions will greatly facilitate the formulation of the proposed\nmethod. Alternatively nonlinear classifiers can also be achieved by applying the\nkernel trick (Boser et al., 1992). However, the kernel classifier is often given as\na black box function, where the contribution of each individual covariate to the\ndecision rule is too implicit to be characterized. Therefore we will use the basis\nexpansion to construct nonlinear classifiers in the paper.\nThe standard multicategory SVM (MSVM; Lee et al., 2004) solves\nn\n\nmin\nf\n\nK\n\nK\n\nk=1\n\nk=1\n\nd\n\nXX\n1 XX\n2\nwkj\n,\nI(yi 6= k)[fk (xi ) + 1]+ + \u03bb\nn i=1\nj=1\n\n(2.2)\n\nP\nunder the sum-to-zero constraint K\nk=1 fk = 0. The sum-to-zero constraint used\nhere is to follow Lee et al. (2004) in their framework for the MSVM. It is imposed\nto eliminate redundancy in fk 's and to assure identifiability of the solution. This\nconstraint is also a necessary condition for the Fisher consistency of the MSVM\nproposed by Lee et al. (2004). To achieve variable selection, Wang and Shen\n(2007b) proposed to impose the L1 penalty on the coefficients and the corresponding L1 MSVM then solves\nn\n\nmin\nb,w\n\nK\n\nK\n\nk=1\n\nk=1\n\nd\n\nXX\n1 XX\n|wkj |\nI(yi 6= k)[bk + wkT xi + 1]+ + \u03bb\nn i=1\nj=1\n\n(2.3)\n\nunder the sum-to-zero\nconstraint. For linear classification rules, we start with\nPd\nfk (x) = bk + j=1 wkj xj , k = 1, . . . , K. The sum-to-zero constraint then becomes\nK\nK\nX\nX\nbk = 0,\nwkj = 0, j = 1, . . . , d.\n(2.4)\nk=1\n\nk=1\n\nThe L1 MSVM treats all wkj 's equally without distinction. As opposed to\nthis, we take into account the fact that some of the coefficients are associated\nwith the same covariate, therefore it is more natural to treat them as a group\nrather than separately.\nDefine the weight matrix W of size K \u00d7 d such that its (k, j) entry is wkj .\nThe structure of W is shown as follows:\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\nClass 1\nClass k\nClass K\n\nx1\n\n***\n\nxj\n\n***\n\nxd\n\nw11\n***\nwk1\n***\nwK1\n\n***\n***\n***\n***\n***\n\nw1j\n***\nwkj\n***\nwKj\n\n***\n***\n***\n***\n***\n\nw1d\n***\nwkd\n***\nwKd\n\n153\n\n.\n\nThroughout the paper, we use wk = (wk1 , . . . , wkd )T to represent the kth\nrow vector of W , and w(j) = (w1j , . . . , wKj )T for the jth column vector of\nW . According to Crammer and Singer (2001), the value bk + wkT x defines the\nsimilarity score of the class k, and the predicted label is the index of the row\nattaining the highest similarity score with x. We define the sup-norm for the\ncoefficient vector w(j) as\nkw(j) k\u221e =\n\nmax |wkj |.\n\nk=1,*** ,K\n\n(2.5)\n\nIn this way, the importance of each covariate xj is directly controlled by its\nlargest absolute coefficient. We propose the sup-norm regularization for MSVM:\nmin\nb,w\n\nn K\nd\nX\n1 XX\nI(yi 6= k)[bk + wkT xi + 1]+ + \u03bb\nkw(j) k\u221e ,\nn i=1\nj=1\nk=1\n\nsubject to\n\nT\n\n1 b = 0,\n\n1T w(j) = 0,\n\nfor j = 1, . . . , d,\n\n(2.6)\n\nwhere b = (b1 , . . . , bK )T .\nThe sup-norm MSVM encourages more sparse solutions than the L1 MSVM,\nand identifies important variables more precisely. In the following, we describe\nthe main motivation of the sup-norm MSVM, which makes it more attractive for\nvariable selection than the L1 MSVM. Firstly, with a sup-norm penalty, a noise\nvariable is removed if and only if all corresponding K estimated coefficients are\n0. On the other hand, if a variable is important with a positive sup-norm, the\nsup-norm penalty, unlike the L1 penalty, does not put any additional penalties\non the other K \u2212 1 coefficients. This is desirable since a variable will be kept in\nthe model as long as the sup-norm of the K coefficients is positive. No further\nshrinkage is needed for the remaining coefficients in terms of variable selection.\nFor illustration, we plot the region 0 \u2264 t1 + t2 \u2264 C in Figure 1, where t1 =\nmax(w11 , w21 , w31 , w41 ) and t2 = max(w12 , w22 , w32 , w42 ). Clearly, the sup-norm\npenalty shrinks sum of two maximums corresponding to two variables. This helps\nto lead to more parsimonious models. In short, in contrast to the L1 penalty,\nthe sup-norm utilizes the group information of the decision function vector and\nconsequently the sup-norm MSVM can deliver better variable selection.\nFor three-class problems, we show that the L1 MSVM and the new proposed\nsup-norm MSVM give identical solutions after adjusting the tuning parameters,\nwhich is due to the sum-to-zero constraints on w(j) 's. This equivalence, however,\ndoes not hold for the adaptive procedures introduced in Section 4.\n\n\f154\n\nC\n\n12\n\n22\n\n32\n\n42\n\nt =max{|w |, |w |, |w |, |w |}\n\nH.H. Zhang et al./Variable selection for multicategory SVM\n\n2\n\nt1+t2=C\n\n0\n\nC\nt =max{|w |, |w |, |w |, |w |}\n1\n\n11\n\n21\n\n31\n\n41\n\nFig 1. Illustrative plot of the shrinkage property of the sup-norm.\n\nProposition 2.1. When K = 3, the L1 MSVM (2.3) and the sup-norm MSVM\n(2.6) are equivalent.\nWhen K > 3, our empirical experience shows that the sup-norm MSVM\ngenerally performs well in terms of classification accuracy.\nHere we would like to point out two fundamental differences between the supnorm penalty and the F\u221e penalty used for group variable selection (Zhao et al.,\n2006; Zou and Yuan, 2006) considering their similar expressions. The purpose\nof group selection is to select several prediction variables altogether if these\npredictors work as a group. Therefore, each F\u221e term in Zou and Yuan (2006)\nis based on the regression coefficients of several variables which belong to one\ngroup, whereas each supnorm penalty in (2.6) is associated with only one prediction variable. Secondly, in the implementation of the F\u221e , one has to decide\nin advance the number of groups and which variables belong to a certain group,\nwhereas in the supnorm SVM each variable is naturally associated with its own\ngroup and the number of groups is same as the number of covariates.\nAs a remark, we point out that Argyriou et al. (2007, 2006) proposed a similar\npenalty for the purpose of multi-task feature learning. Specifically, they used a\nmixture of L1 and L2 penalties. They first applied the L2 penalty for each\nfeature across different tasks and then used the L1 penalty for feature selection.\nIn contrast, our penalty is a combination of the L1 and supnorm penalties for\nmulticategory classification.\nThe tuning parameter \u03bb in (2.6) balances the tradeoff between the data fit\nand the model parsimony. A proper choice of \u03bb is important to assure good performance of the resulting classifier. If the chosen \u03bb is too small, the procedure\ntends to overfit the training data and gives a less sparse solution; on the other\nhand, if \u03bb is too large, the solution can become very sparse but possibly with\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n155\n\na low prediction power. The choice of the tuning parameter is typically done\nby minimizing either an estimate of generalization error or other related performance measures. In simulations, we generate an extra independent tuning set to\nchoose the best \u03bb. For real data analysis, we use leave-one-out cross validation\nof the misclassification rate to select \u03bb.\n3. Computational Algorithms\nIn this section we show that the optimization problem (2.6) can be converted to\na linear programming (LP) problem, and can therefore be solved using standard\nLP techniques in polynomial time. This great computational advantage is very\nimportant in real applications, especially for large data sets.\nLet A be an n \u00d7 K matching matrix with its entry aik = I(yi 6= k) for\ni = 1, . . . , n and k = 1, . . . , K. First we introduce slack variables \u03beik such that\n\u0002\n\u0003\n(3.1)\n\u03beik = bk + wkT xi + 1 + for i = 1, . . . , n; k = 1, . . . , K.\nThe optimization problem (2.6) can be expressed as\nmin\n\nb,w,\u03be\n\nn K\nd\nX\n1 XX\nkw(j) k\u221e ,\naik \u03beik + \u03bb\nn i=1\nj=1\nk=1\n\nsubject to\n\nT\n\n1 b = 0, 1T w(j) = 0,\n\u03beik \u2265 bk +\n\nwkT xi\n\nj = 1, . . . , d,\n\n+ 1, \u03beik \u2265 0,\n\ni = 1, . . . , n; k = 1, . . . , K. (3.2)\n\nTo further simplify (3.2), we introduce a second set of slack variables\n\u03b7j = kw(j) k\u221e =\n\nmax |wkj |,\n\nk=1,...,K\n\nwhich add some new constraints to the problem:\n|wkj | \u2264 \u03b7j ,\n\nfor\n\nk = 1, . . . , K; j = 1, . . . , d.\n\n+\n\u2212\n+\n\u2212\nFinally write wkj = wkj\n\u2212 wkj\n, where wkj\nand wkj\ndenote the positive and\n+\nnegative parts of wkj , respectively. Similarly, wj and wj\u2212 respectively consist of\nthe positive and negative parts of components in wj . Denote \u03b7 = (\u03b71 , . . . , \u03b7d )T ;\nthen (3.2) becomes\nn K\nd\nX\n1 XX\n\u03b7j ,\naik \u03beik + \u03bb\nn i=1\nj=1\n\nmin\n\nb,w,\u03be,\u03b7\n\nk=1\n\nsubject to\n\n+\n\u2212\n1 b = 0, 1T [w(j)\n\u2212 w(j)\n] = 0,\nT\n\n\u03beik \u2265 bk +\n+\nw(j)\n\n+\n\n[wk+\n\n\u2212\nw(j)\n\n\u2212\n\n\u2264 \u03b7,\n\nwk\u2212 ]T xi + 1, \u03beik \u2265 0,\n+\n\u2212\nw(j)\n\u2265 0, w(j)\n\u2265 0,\n\nj = 1, . . . , d,\ni = 1, . . . , n; k = 1, . . . , K,\nj = 1, . . . , d.\n\n(3.3)\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n156\n\n4. Adaptive Penalty\nIn (2.3) and (2.6), the same weights are used for different variables in the penalty\nterms, which may be too restrictive, since a smaller penalty may be more desired\nfor those variables which are so important that we want to retain them in the\nmodel. In this section, we suggest that different variables should be penalized\ndifferently according to their relative importance. Ideally, large penalties should\nbe imposed on redundant variables in order to eliminate them from models more\neasily; and small penalties should be used on important variables in order to\nretain them in the final classifier. Motivated by this, we consider the following\nadaptive L1 MSVM:\nn\n\nmin\nb,w\n\nK\n\nK\n\nk=1\n\nsubject to\n\nd\n\nXX\n1 XX\n\u03c4kj |wkj |,\nI(yi 6= k)[bk + wkT xi + 1]+ + \u03bb\nn i=1\nj=1\nT\n\n1 b = 0,\n\nk=1\n\nT\n\n1 w(j) = 0,\n\nfor j = 1, . . . , d,\n\n(4.1)\n\nwhere \u03c4kj > 0 represents the weight for coefficient wkj .\nAdaptive shrinkage for each variable has been proposed and studied in various contexts of regression problems, including the adaptive LASSO for linear\nregression (Zou, 2006), proportional hazard models (Zhang and Lu, 2007), and\nquantile regression (Wang et al., 2007; Wu and Liu, 2007b). In particular, Zou\n(2006) has established the oracle property of the adaptive LASSO and justified\nthe use of different amounts of shrinkage for different variables. Due to the special form of the sup-norm SVM, we consider the following two ways to employ\nthe adaptive penalties:\n[I]\nmin\nb,w\n\nn K\nd\nX\n1 XX\n\u03c4j kw(j) k\u221e ,\nI(yi 6= k)[bk + wkT xi + 1]+ + \u03bb\nn i=1\nj=1\nk=1\n\nsubject to\n\n1T b = 0,\n\n1T w(j) = 0,\n\nfor j = 1, . . . , d,\n\n(4.2)\n\n[II]\nmin\nb,w\n\nn K\nd\nX\n1 XX\nk(\u03c4 w)(j) k\u221e ,\nI(yi 6= k)[bk + wkT xi + 1]+ + \u03bb\nn i=1\nj=1\nk=1\n\nsubject to\n\nT\n\n1 b = 0,\n\n1T w(j) = 0,\n\nfor j = 1, . . . , d,\n\n(4.3)\n\nwhere the vector (\u03c4 w)(j) = (\u03c41j w1j , . . . , \u03c4Kj wKj )T for j = 1, ..., d.\nIn (4.1), (4.2), and (4.3), the weights can be regarded as leverage factors,\nwhich are adaptively chosen such that large penalties are imposed on coefficients of unimportant covariates and small penalties on coefficients of important\nones. Let w\u0303 be the solution to standard MSVM (2.2) with the L2 penalty. Our\nempirical experience suggests that\n\u03c4kj =\n\n1\n|w\u0303kj |\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n157\n\nis a good choice for (4.1) and (4.3), and\n\u03c4j =\n\n1\nkw\u0303(j) k\u221e\n\nis a good choice for (4.2). If w\u0303kj = 0, which implies the infinite penalty on wkj ,\nwe set the corresponding coefficient solution \u0175kj to be zero.\nIn terms of computational issues, all three problems (4.1), (4.2), and (4.3)\ncan be solved as LP problems. Their entire solution paths may be obtained by\nsome modifications of the algorithms in Wang and Shen (2007b).\n5. Simulation\nIn this section, we demonstrate the performance of six MSVM methods: the\nstandard L2 MSVM, L1 MSVM, sup-norm MSVM, adaptive L1 MSVM, and\nthe two adaptive sup-norm MSVMs. Three simulation models are considered:\n(1) a linear example with five classes; (2) a linear example with four classes; (3) a\nnonlinear example with three classes. In each simulation setting, n observations\nare simulated as the training data, and another n observations are generated\nfor tuning the regularization parameter \u03bb for each procedure. Therefore the\ntotal sample size is 2n for obtaining the final classifiers. To test the accuracy\nof the classification rules, we also independently generate n\u2032 observations as a\ntest set. The tuning parameter \u03bb is selected via a grid search over the grid:\nlog2 (\u03bb) = \u221214, \u221213, . . . , 15. When a tie occurs, we choose the larger value of \u03bb.\nAs we suggest in Section 4, we use the L2 MSVM solution to derive the weights\nin the adaptive MSVMs. The L2 MSVM solution is the final tuned solution using\nthe separate tuning set. Once the weights are chosen, we tune the parameter \u03bb\nin the adaptive procedure via the tuning set.\nWe conduct 100 simulations for each classification method under all settings.\nEach fitted classifier is then evaluated in terms of its classification accuracy and\nvariable selection performance. For each method, we report its average testing\nerror, the number of correct and incorrect zero coefficients among Kd coefficients, the model size as the number of important ones among the d variables,\nand the number of times that the true model is correctly identified. The numbers given in the parentheses in the tables are the standard errors of the testing\nerrors. We also summarize the frequency of each variable being selected over 100\nruns. All simulations are done using the optimization software CPLEX with the\nAMPL interface (Fourer et al., 2003). More information about CPLEX can be\nfound on the ILOG website http://www.ilog.com/products/optimization/.\n5.1. Five-Class Example\nConsider a five-class example, with the input vector x in a 10-dimensional space.\nThe first two components of the input vector are generated from a mixture\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n158\n\nGaussian in the following way: for each class k, generate (x1 , x2 ) independently\nfrom N (\u03bck , \u03c312 I2 ), with\n\u03bci = 2 (cos([2k \u2212 1]\u03c0/5), sin([2k \u2212 1]\u03c0/5)) ,\n\nk = 1, 2, 3, 4, 5,\n\nand the remaining eight components are i.i.d. generated from N (0,\u221a\u03c322 ). We\ngenerate the same number of observations in each class. Here \u03c31 = 2, \u03c32 =\n1, n = 250, and n\u2032 = 50, 000.\nTable 1\nClassification and variable selection results for the five-class example. TE, CZ, IZ, MS, and\nCM refer to the testing error, the number of correct zeros, the number of incorrect zeros, the\nmodel size, and the number of times that the true model is correctly identified, respectively.\nMethod\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\nBayes\n\nTE\n0.454 (0.034)\n0.558 (0.022)\n0.553 (0.020)\n0.453 (0.020)\n0.455 (0.024)\n0.457 (0.046)\n0.387 (-)\n\nCZ\n0.00\n24.88\n30.23\n33.90\n39.92\n39.40\n41\n\nIZ\n0.00\n2.81\n2.84\n0.01\n0.01\n0.09\n0\n\nMS\n10.00\n6.60\n5.14\n3.39\n2.08\n2.17\n2\n\nCM\n0\n21\n40\n68\n98\n97\n100\n\nTable 1 shows that, in terms of classification accuracy, the L2 MSVM, the\nsupnorm MSVM, and the two adaptive supnorm MSVMs are among the best\nand their testing errors are close to each other. In terms of other measurements\nsuch as the number of correct/incorrect zeros, the model size, and the number of\ntimes that the true model is correctly identified, the supnorm MSVM procedures\nwork much better than other MSVM methods.\nTable 2 shows the frequency of each variable being selected by each procedure\nin 100 runs. The type I sup-norm MSVM performs the best among all. Overall the adaptive MSVMs show significant improvement over the non-adaptive\nclassifiers in terms of both classification accuracy and variable selection.\nTable 2\nVariable selection frequency results for the five-class example.\nMethod\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\n\nx1\n100\n100\n100\n100\n100\n100\n\nx2\n100\n100\n100\n100\n100\n100\n\nx3\n100\n59\n44\n15\n1\n2\n\nSelection Frequency\nx4\nx5\nx6\nx7\n100\n100\n100\n100\n55\n60\n58\n56\n40\n43\n37\n39\n17\n20\n17\n14\n1\n0\n2\n1\n2\n2\n2\n2\n\nx8\n100\n61\n41\n20\n1\n2\n\nx9\n100\n57\n35\n17\n1\n3\n\nx10\n100\n54\n35\n19\n1\n2\n\n5.2. Four-Class Linear Example\nIn the simulation example in Section 5.1, the informative variables are important\nfor all classes. In this section, we consider an example where the informative vari-\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n159\n\nables are important for some classes but not important for other classes. Specifically, we generate four i.i.d important variables x1 , x2 , x3 , x4 from Unif[\u22121, 1]\nas well as six independent i.i.d noise variables x5 , . . . , x10 from N (0, 82 ). Define\nthe functions\nf1\nf2\nf3\nf4\n\n= \u22125x1 + 5x4 ,\n\n= 5x1 + 5x2 ,\n= \u22125x2 + 5x3 ,\n= \u22125x3 \u2212 5x4 ,\n\nand set pk (x) = P (Y = k|X = x) \u221d exp(fk (x)), k = 1, 2, 3, 4. In this example,\nwe set n = 200 and n\u2032 = 40, 000. Note that x1 is not important for distinguishing\nclass 3 and class 4. Similarly, x2 is noninformative for class 1 and class 4, x3 is\nnoninformative for class 1 and class 2, and x4 is noninformative for class 2 and\nclass 3.\nTable 3\nClassification and variable selection results for the four-class linear example.\nMethod\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\nBayes\n\nTE\n0.336 (0.063)\n0.340 (0.069)\n0.320 (0.079)\n0.332 (0.070)\n0.327 (0.076)\n0.326 (0.071)\n0.1366 (-)\n\nCZ\n0.0000\n2.5100\n18.2300\n0.8500\n9.3300\n9.9000\n32\n\nIZ\n0.0000\n0.1600\n0.2600\n0.1400\n0.1400\n0.1400\n0\n\nMS\n10.00\n9.99\n7.21\n9.98\n7.83\n7.69\n4\n\nCM\n0\n0\n21\n0\n15\n9\n100\n\nTable 3 summarizes the performance of various procedures, and Table 4 shows\nthe frequency of each variable being selected by each procedure in 100 runs. Due\nto the increased difficulty of this problem, the performances of all methods are\nnot as good as that of the five-class example. From these results, we can see that\nthe adaptive procedures work better than the non-adaptive procedures both in\nterms of both classification accuracy and variable selection. Furthermore, the\nadaptive L1 MSVM performs the best overall. This is due to the difference between the L1 and the supnorm penalties. Our proposed supnorm penalty treats\nall coefficients of one variable corresponding to different classes as a group and\nremoves the variable if it is non-informative across all class labels. By design of\nthis example, important variables have zero coefficients for certain classes. As a\nresult, our supnorm penalty does not deliver the best performance. Nevertheless,\nthe adaptive supnorm procedures still perform reasonably.\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n160\n\nTable 4\nVariable selection frequency results for the four-class example.\nMethod\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\n\nx1\n100\n100\n100\n100\n100\n100\n\nx2\n100\n100\n100\n100\n100\n100\n\nx3\n100\n100\n100\n100\n100\n100\n\nSelection Frequency\nx4\nx5\nx6\nx7\n100\n100\n100\n100\n100\n100\n99\n100\n100\n55\n53\n59\n100\n100\n99\n100\n100\n67\n64\n71\n100\n65\n66\n65\n\n6\n\n2 2\n\nx2\n\n3\n2 3\n33\n\n2 2\n\n2\n\n2 22\n2 2 2 2\n2\n\nx8\n100\n100\n56\n100\n60\n58\n\nx9\n100\n100\n49\n100\n58\n56\n\nx10\n100\n100\n49\n99\n63\n59\n\n22\n\n2\n\n2\n1 1\n2\n1\n1\n1 1\n2\n4\n2\n23 3\n1\n1\n2\n1\n2\n1 1\n11 11\n3\n2 2\n2\n223 2\n1\n1 2\n1\n11\n3\n111\n2 2\n3\n2\n1\n1\n3\n3\n11\n3 12\n3 3\n1\n23\n2\n11\n3\n11 1\n33\n3 3 3\n1 1\n1\n3\n3 3\n3\n3\n3\n1\n0 3\n1\n3\n1\n23 1\n1\n3\n1 1 1\n1\n33\n1\n1\n1\n1\n3\n1\n1\n3\n1\n3\n2\n3\n1\n1\n\u22122 3\n2\n1\n1\n1\n3\n3333 3\n1\n1\n1\n3\n33\n3 3\n1\n11 1\n2\n2 2\n32\n1\n2\n1 1\n3\n\u22124\n2 2\n3\n3\n22 2\n1\n2\n2 22\n2\n11 1\n2\n3 3 3 2 2 2\n1\n2\n2\n2\n2\n2\n2 2\n2\n2\n2\n2\n2\n\u22126\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\nx1\n\nFig 2. The Bayes boundary for the nonlinear three-class example.\n\n5.3. Nonlinear Example\nIn this nonlinear 3-class example, we first generate x1 \u223c Unif[\u22123, 3] and x2 \u223c\nUnif[\u22126, 6]. Define the functions\nf1\nf2\nf3\n\n= \u22122x1 + 0.2x21 \u2212 0.1x22 + 0.2,\n\n= \u22120.4x21 + 0.2x22 \u2212 0.4,\n= 2x1 + 0.2x21 \u2212 0.1x22 + 0.2,\n\nand set pk (x) = P (Y = k|X = x) \u221d exp(fk (x)), k = 1, 2, 3. The Bayes boundary\nis plotted in Figure 2. We also generate three noise variables xi \u223c N (0, \u03c3 2 ),\ni = 3, 4, 5. In this example, we set \u03c3 = 2 and n\u2032 = 40, 000.\nTo achieve nonlinear classification, we fit the nonlinear MSVM by including\nthe five main effects, their square terms, and their cross products as the basis\nfunctions. The results with n = 200 are summarized in Tables 5 and 6. Clearly,\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n161\n\nTable 5\nClassification and variable selection results using second order polynomial basis functions\nfor the nonlinear example in Section 5.3 with n = 200.\nMethod\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\nBayes\n\nTE\n0.167 (0.013)\n0.151 (0.012)\n0.140 (0.010)\n0.150 (0.012)\n0.140 (0.010)\n0.140 (0.011)\n0.120 (-)\n\nCZ\n0.00\n21.42\n43.13\n22.70\n40.84\n41.50\n52\n\nIZ\n0.00\n0.03\n0.00\n0.01\n0.00\n0.00\n0\n\nMS\n20.00\n14.91\n6.92\n14.43\n7.21\n6.21\n3\n\nCM\n0\n0\n31\n0\n31\n36\n100\n\nTable 6\nVariable selection frequency results for the nonlinear example using second order polynomial\nbasis functions with n = 200.\nMethod\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\n\nx1\n100\n100\n100\n100\n100\n100\nx1 x2\n100\n80\n31\n79\n31\n25\n\nx21\n100\n100\n100\n100\n100\n100\nx1 x3\n100\n55\n20\n62\n22\n15\n\nx22\n100\n100\n100\n100\n100\n100\nx1 x4\n100\n57\n18\n58\n17\n14\n\nSelection Frequency\nx2\nx3\nx4\nx5\n100\n100\n100\n100\n69\n44\n50\n43\n33\n21\n21\n20\n67\n37\n42\n34\n31\n21\n21\n26\n22\n18\n12\n19\nx1 x5 x2 x3 x2 x4 x2 x5\n100\n100\n100\n100\n65\n86\n88\n90\n20\n28\n26\n31\n55\n87\n89\n91\n28\n30\n29\n30\n19\n30\n23\n22\n\nx23\n100\n80\n24\n84\n21\n18\nx3 x4\n100\n69\n20\n62\n24\n16\n\nx24\n100\n84\n18\n80\n25\n16\nx3 x5\n100\n72\n17\n68\n16\n17\n\nx25\n100\n89\n22\n75\n24\n18\nx4 x5\n100\n70\n22\n73\n25\n17\n\nthe adaptive L1 SVM and the two adaptive sup-norm SVMs deliver more accurate and sparse classifiers than the other methods. In this example, there\nare correlations among covariates and consequently the variable selection task\nbecomes more challenging. This difficulty is reflected in the variable selection\nfrequency reported in Table 6. Despite the difficulty, the adaptive procedures\nare able to remove noise variables reasonably well.\nTo examine the performance of various methods using a richer set of basis\nfunctions, we also fit nonlinear MSVMs via polynomial basis of degree 3 with\n55 basis functions. Results of classification and variable selection with n = 200\nand 400 are reported in Tables 7 and 8 respectively. Compared with the case of\nthe second order polynomial basis, classification testing errors using the third\norder polynomial basis are much larger for the L2 , L1 , and supnorm MSVMs,\nbut similar for the adaptive procedures. Due to the large basis set, none of the\nmethods can identify the correct model. However, the adaptive procedures can\neliminate more noise variables than the non-adaptive procedures. This further\ndemonstrates the effectiveness of adaptive weighting. The results of variable\nselection frequency (not reported due to lack of space) show a similar pattern\nas that of the second order polynomial. When n increases from 200 and 400,\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n162\n\nTable 7\nClassification and variable selection results using third order polynomial basis functions for\nthe nonlinear example in Section 5.3 with n = 200.\nMethod\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\nBayes\n\nTE\n0.213 (0.018)\n0.170 (0.015)\n0.138 (0.015)\n0.171 (0.015)\n0.141 (0.016)\n0.142 (0.015)\n0.120 (-)\n\nCZ\n0.00\n59.22\n120.71\n60.08\n114.29\n106.78\n157\n\nIZ\n0.00\n0.57\n0.17\n0.61\n0.17\n0.22\n0\n\nMS\n55.00\n40.44\n19.28\n40.06\n20.22\n19.75\n3\n\nCM\n0\n0\n0\n0\n0\n0\n100\n\nTable 8\nClassification and variable selection results using third order polynomial basis functions for\nthe nonlinear example in Section 5.3 with n = 400.\nMethod\nL2\nL1\nAdapt-L1\nSupnorm\nAdapt-supI\nAdapt-supII\nBayes\n\nTE\n0.162 (0.008)\n0.143 (0.008)\n0.124 (0.004)\n0.144 (0.010)\n0.125 (0.005)\n0.125 (0.004)\n0.120 (-)\n\nCZ\n0.00\n60.13\n139.71\n60.51\n139.41\n132.96\n157\n\nIZ\n0.00\n0.34\n0.00\n0.32\n0.00\n0.00\n0\n\nMS\n55.00\n40.50\n11.01\n40.24\n10.37\n10.96\n3\n\nCM\n0\n0\n0\n0\n0\n0\n100\n\nwe can see that classification accuracy for all methods increases as expected.\nInterestingly, compared to the case of n = 200, the performance of variable\nselection with n = 400 for non-adaptive procedures stays relatively the same,\nwhile improves dramatically for the adaptive procedures.\n6. Real Example\nDNA microarray technology has made it possible to monitor mRNA expressions\nof thousands of genes simultaneously. In this section, we apply our six different\nMSVMs on the children cancer data set in Khan et al. (2001). Khan et al. (2001)\nclassified the small round blue cell tumors (SRBCTs) of childhood into 4 classes;\nnamely neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL), and the Ewing family of tumors (EWS) using cDNA gene expression profiles. After filtering, 2308 gene profiles out of 6567 genes are given in the\ndata set, available at http://research.nhgri.nih.gov/microarray/Supplement/.\nThe data set includes a training set of size 63 and a test set of size 20. The distributions of the four distinct tumor categories in the training and test sets are\ngiven in Table 9. Note that Burkitt lymphoma (BL) is a subset of NHL.\nTo analyze the data, we first standardize the data sets by applying a simple\nlinear transformation based on the training data. Specifically, we standardize\nthe expression x\u0303gi of the g-th gene of subject i to obtain xgi by the following\nformula:\nPn\nx\u0303gi \u2212 n1 j=1 x\u0303gj\nxgi =\n.\nsd(x\u0303g1 , * * * , x\u0303gn )\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n163\n\nTable 9\nClass distribution of the microarray example.\nData set\nTraining\nTest\n\nNB\n12\n6\n\nRMS\n20\n5\n\nBL\n8\n3\n\nEWS\n23\n6\n\nTotal\n63\n20\n\nTable 10\nClassification results of the microarray data using 200 genes.\nPenalty\nL2\nL1\nAdp-L1\nSupnorm\nAdp-supI\nAdp-supII\n\nTesting Error\n0\n1/20\n0\n1/20\n1/20\n1/20\n\nSelected genes\nTop 100\nBottom 100\n100\n100\n62\n1\n53\n1\n53\n0\n50\n0\n47\n0\n\nThen we rank all genes using their marginal relevance in class separation by\nadopting a simple criterion used in Dudoit et al. (2002). Specifically, the relevance measure for gene g is defined to be the ratio of between classes sum of\nsquares to within class sum of squares as follows:\nPn PK\n(k)\n2\nk=1 I(yi = k)(x\u0304*g \u2212 x\u0304*g )\nR(g) = Pni=1 PK\n,\n(6.1)\n(k) 2\ni=1\nk=1 I(yi = k)(xig \u2212 x\u0304*g )\n(k)\n\nwhere n is the size of the training set, x\u0304*g denotes the average expression\nlevel of gene g for class k observations, and x\u0304*g is the overall mean expression\nlevel of gene g in the training set. To examine the performance of variable\nselection of all different methods, we select the top 100 and bottom 100 genes\nas covariates according the relevance measure R. Our main goal here is to get\na set of \"important\" genes and also a set of \"unimportant\" genes, and to see\nwhether our methods can effectively remove the \"unimportant\" genes.\nAll six MSVMs with different penalties are applied to the training set. We\nuse leave-one-out cross validation on the standardized training data with 200\ngenes for the purpose of tuning parameter selection and then apply the resulting\nclassifiers on the testing data. The results are tabulated in Table 10. All methods\nhave either 0 or 1 misclassification on the testing set. In terms of gene selection,\nthree sup-norm MSVMs are able to eliminate all bottom 100 genes and they use\naround 50 genes out of the top 100 genes to achieve comparable classification\nperformance to other methods.\nIn Figure 3, we plot heat maps of both training and testing sets on the left\nand right panels respectively. In these heat maps, rows represent 50 genes selected by the Type I sup-norm MSVM and columns represent patients. The gene\nexpression values are reflected by colors on the plot, with red representing the\nhighest expression level and blue the lowest expression level. For visualization,\nwe group columns within each class together and use hierarchical clustering with\ncorrelation distance on the training set to order the genes so that genes close\nto each other have similar expressions. From the left panel on Figure 3, we can\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\nTraining data\n\n164\n\nTest data\nHomo sapiens incomplete cDNA for a mutated allele of a myosin class I, myh\u22121c\nHomo sapiens incomplete cDNA for a mutated allele of a myosin class I, myh\u22121c\ntransmembrane protein\napelin; peptide ligand for APJ receptor\nrecoverin\nglycine cleavage system protein H (aminomethyl carrier)\nHomo sapiens mRNA full length insert cDNA clone EUROIMAGE 45620\nthioredoxin reductase 1\ncadherin 2, N\u2212cadherin (neuronal)\nmicrotubule\u2212associated protein 1B\npostmeiotic segregation increased 2\u2212like 12\nglucose\u22126\u2212phosphate dehydrogenase\nprotein tyrosine phosphatase, non\u2212receptor type 12\ntranscriptional intermediary factor 1\ngrowth associated protein 43\nESTs\ndihydropyrimidinase\u2212like 2\nkinesin family member 3C\nfibroblast growth factor receptor 4\npresenilin 2 (Alzheimer disease 4)\nsarcoglycan, alpha (50kD dystrophin\u2212associated glycoprotein)\nnuclear receptor coactivator 1\nESTs\nglycine amidinotransferase (L\u2212arginine:glycine amidinotransferase)\nmesoderm specific transcript (mouse) homolog\nlymphocyte\u2212specific protein 1\nHuman DNA for insulin\u2212like growth factor II (IGF\u22122); exon 7 and additional ORF\nneurofibromin 2 (bilateral acoustic neuroma)\nplasminogen activator, tissue\ninterleukin 4 receptor\nWiskott\u2212Aldrich syndrome (ecezema\u2212thrombocytopenia)\nproteasome (prosome, macropain) subunit, beta type, 8 (large multifunctional protease 7)\nmajor histocompatibility complex, class II, DM alpha\npim\u22122 oncogene\nESTs\nproteasome (prosome, macropain) subunit, beta type, 10\nprotein kinase, cAMP\u2212dependent, regulatory, type II, beta\npostmeiotic segregation increased 2\u2212like 3\nRho\u2212associated, coiled\u2212coil containing protein kinase 1\nEST\ntranslocation protein 1\nFc fragment of IgG, receptor, transporter, alpha\nfollicular lymphoma variant translocation 1\nantigen identified by monoclonal antibodies 12E7, F21 and O13\ncaveolin 1, caveolae protein, 22kD\nATPase, Na+/K+ transporting, alpha 1 polypeptide\ncyclin D1 (PRAD1: parathyroid adenomatosis 1)\nprotein tyrosine phosphatase, non\u2212receptor type 13 (APO\u22121/CD95 (Fas)\u2212associated phosphatase)\nv\u2212myc avian myelocytomatosis viral oncogene homolog\n\nEWS\n\nBL\n\nNB\n\nRMS\n\nEWS\n\nBL\n\nNB\n\nRMS\n\nFig 3. Heat maps of the microarray data. The left and right panels represent the training and\ntesting sets respectively.\n\nobserve four block structures associated with four classes. This implies that the\n50 genes selected are highly informative in predicting the tumor types. For the\ntesting set shown on the right panel, we can still see the four blocks although\nthe structure and pattern are not as clean as the training set. It is interesting\nto note that several genes in the testing set have higher expression levels, i.e.,\nmore red, than the training set. In summary, we conclude that the proposed\nsup-norm MSVMs are indeed effective in performing simultaneous classification\nand variable selection.\n7. Discussion\nAs pointed out in Lafferty and Wasserman (2006), sparse learning is an important but challenging issue for high dimensional data. In this paper, we propose\na new regularization method which applies the sup-norm penalty to the MSVM\nto achieve variable selection. Through the new penalty, the natural group effect\nof the coefficients associated with the same variable is embedded in the regularization framework. As a result, the sup-norm MSVMs can perform better\nvariable selection and deliver more parsimonious classifiers than the L1 MSVMs.\nMoreover, our results show that the adaptive procedures work very well and improve the corresponding nonadaptive procedures. The adaptive L1 procedure\ncan in some settings be as good as and sometimes better than the adaptive\nsupnorm procedures. As a future research direction, we will further investigate\nthe theoretical properties of proposed methods.\nIn some problems, it is possible to form groups among covariates. As argued\nin Yuan and Lin (2006) and Zou and Yuan (2006), it is advisable to use such\ngroup information in the model building process to improve accuracy of the\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n165\n\nprediction. The notion of \"group lasso\" has also been studied in the context of\nlearning a kernel (Micchelli and Pontil, 2007). If such kind of group information\nis available for multicategory classification, there will be two kinds of group\ninformation available for model building, one type of group formed by the same\ncovariate corresponding to different classes as considered in the paper and the\nother kind formed among covariates. A future research direction is to combine\nboth group information to construct a new multicategory classification method.\nWe believe that such potential classifiers can outperform those without using\nthe additional information.\nThis paper focuses on the variable selection issue for supervised learning.\nIn practice, semi-supervised learning is often encountered, and many methods\nhave been developed including Zhu et al. (2003) and Wang and Shen (2007a).\nAnother future topic is to generalize the sup-norm penalty to the context of\nsemi-supervised learning.\nAppendix\nProof of Proposition 2.1: Without loss of generality, assume that {w1j , w2j , w3j }\nare all nonzero. Because of the sum-to-zero constraint w1j + w2j + w3j = 0,\nthere must be one component out of {w1j , w2j , w3j } has a different sign from\nthe other two. Suppose the sign of w1j differs from the other two and then\n|w1j | = |w2j | + |w3j | by the sum-to-zero constraint. Consequently, we have\nP3\n|w1j | = max{|w1j |, |w2j |, |w3j |}. Therefore, k=1 |wkj | = 2kw(j) k\u221e . The equivalence of problem (2.2) with the tuning parameter \u03bb and problem (2.6) with the\ntuning parameter 2\u03bb can be then established. This completes the proof.\nLiterature Cited\nArgyriou, A., Evgeniou, T. and M., P. (2006). Multi-task feature learning.\nNeural Information Processing Systems, 19.\nArgyriou, A., Evgeniou, T. and M., P. (2007). Convex multi-task feature\nlearning. Machine Learning. To appear.\nBoser, B. E., Guyon, I. M. and Vapnik, V. (1992). A training algorithm for\noptimal margin classifiers. In Fifth Annual ACM Workshop on Computational\nLearning Theory. ACM Press, Pittsburgh, PA, 144\u2013152.\nBradley, P. S. and Mangasarian, O. L. (1998). Feature selection via concave minimization and support vector machines. In Proc. 15th International\nConf. on Machine Learning. Morgan Kaufmann, San Francisco, CA, 82\u201390.\nChristianini, N. and Shawe-Taylor, J. (2000). An introduction to support\nvector machines and other kernel-based learning methods. Cambridge University Press.\nCrammer, K. and Singer, Y. (2001). On the algorithmic implementation\nof multiclass kernel-based vector machines. Journal of Machine Learning\nResearch, 2 265\u2013292.\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n166\n\nDudoit, S., Fridlyand, J. and Speed, T. (2002). Comparison of discrimination methods for the classification of tumors using gene expression data.\nJournal of American Statistical Association, 97 77\u201387. MR1963389\nFourer, R., Gay, D. and Kernighan, B. (2003). AMPL: A Modeling Language for Mathematical Programming. Duxbury Press.\nKhan, J., Wei, J. S., Ringn\u00e9r, M., , Saal, L. H., Ladanyi, M., Westermann, F., Berthold, F., Schwab, M., Antonescu, C. R., Peterson,\nC. and Meltzer, P. S. (2001). Classification and diagnostic prediction of\ncancers using gene expression profiling and artificial neural networks. Nature\nMedicine, 7 673\u2013679.\nLafferty, J. and Wasserman, L. (2006). Challenges in statistical machine\nlearning. Statistica Sinica, 16 307\u2013323. MR2267237\nLee, Y., Kim, Y., Lee, S. and Koo, J.-Y. (2006). Structured multicategory\nsupport vector machine with anova decomposition. Biometrika, 93 555\u2013571.\nMR2261442\nLee, Y., Lin, Y. and Wahba, G. (2004). Multicategory support vector machines, theory, and application to the classification of microarray data and\nsatellite radiance data. Journal of American Statistical Association, 99 67\u2013\n81. MR2054287\nLiu, Y. and Shen, X. (2006). Multicategory \u03c8-learning. Journal of the American Statistical Association, 101 500\u2013509. MR2256170\nLiu, Y. and Wu, Y. (2007). Variable selection via a combination of the l0 and\nl1 penalties. Journal of Computation and Graphical Statistics, 16 782\u2013798.\nLiu, Y., Zhang, H. H., Park, C. and Ahn, J. (2007). Support vector machines\nwith adaptive lq penalties. Computational Statistics and Data Analysis, 51\n6380\u20136394.\nMicchelli, C. and Pontil, M. (2007). Feature space perspectives for learning\nthe kernel. Machine Learning, 66 297\u2013319.\nSch\u00f6lkopf, B. and Smola, A. J. (2002). Learning with Kernels. MIT Press.\nMR1949972\nVapnik, V. (1995). The Nature of Statistical Learning Theory. Springer-Verlag,\nNew York. MR1367965\nVapnik, V. (1998). Statistical learning theory. Wiley. MR1641250\nWang, H., Li, G. and Jiang, G. (2007). Robust regression shrinkage and consistent variable selection via the lad-lasso. Journal of Business and Economics\nStatistics, 25 347\u2013355.\nWang, J. and Shen, X. (2007a). Large margin semi-supervised learning. Journal of Machine Learning Research, 8 1867\u20131891.\nWang, L. and Shen, X. (2007b). On l1 -norm multi-class support vector machines: methodology and theory. Journal of the American Statistical Association, 102 583\u2013594.\nWang, L., Zhu, J. and Zou, H. (2006). The doubly regularized support vector\nmachine. Statistica Sinica, 16 589\u2013615. MR2267251\nWeston, J., Elisseeff, A., Sch\u00f6lkopf, B. and Tipping, M. (2003). Use\nof the zero-norm with linear models and kernel methods. Journal of Machine\nLearning Research, 3 1439\u20131461. MR2020766\n\n\fH.H. Zhang et al./Variable selection for multicategory SVM\n\n167\n\nWeston, J. and Watkins, C. (1999). Multiclass support vector machines. In\nProceedings of ESANN99 (M. Verleysen, ed.). D. Facto Press.\nWu, Y. and Liu, Y. (2007a). Robust truncated-hinge-loss support vector machines. Journal of the American Statistical Association, 102 974\u2013983.\nWu, Y. and Liu, Y. (2007b). Variable selection in quantile regression. Statistica\nSinica. To appear.\nYuan, M. and Lin, Y. (2006). Model selection and estimation in regression\nwith grouped variables. Journal of the Royal Statistical Society, Series B, 68\n49\u201367. MR2212574\nZhang, H. H., Ahn, J., Lin, X. and Park, C. (2006). Gene selection using\nsupport vector machines with nonconvex penalty. Bioinformatics, 22 88\u201395.\nZhang, H. H. and Lu, W. (2007). Adaptive-lasso for cox's proportional hazard\nmodel. Biometrika, 94 691\u2013703.\nZhao, P., Rocha, G. and Yu, B. (2006). Grouped and hierarchical model\nselection through composite absolute penalties. Technical Report 703, Department of Statistics University of California at Berkeley.\nZhu, J., Hastie, T., Rosset, S. and Tibshirani, R. (2003). 1-norm support\nvector machines. Neural Information Processing Systems, 16.\nZou, H. (2006). The adaptive lasso and its oracle properties. Journal of the\nAmerican Statistical Association, 101 1418\u20131429. MR2279469\nZou, H. and Yuan, M. (2006). The f\u221e -norm support vector machine. Statistica Sinica. To appear.\n\n\f"}