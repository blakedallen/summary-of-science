{"id": "http://arxiv.org/abs/1004.0512v1", "guidislink": true, "updated": "2010-04-04T15:20:27Z", "updated_parsed": [2010, 4, 4, 15, 20, 27, 6, 94, 0], "published": "2010-04-04T15:20:27Z", "published_parsed": [2010, 4, 4, 15, 20, 27, 6, 94, 0], "title": "Analysis, Interpretation, and Recognition of Facial Action Units and\n  Expressions Using Neuro-Fuzzy Modeling", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.3951%2C1004.0550%2C1004.5195%2C1004.2755%2C1004.3482%2C1004.3059%2C1004.3162%2C1004.1847%2C1004.4993%2C1004.3089%2C1004.4224%2C1004.1072%2C1004.1020%2C1004.5495%2C1004.2805%2C1004.1536%2C1004.2609%2C1004.2450%2C1004.4426%2C1004.3578%2C1004.2165%2C1004.1060%2C1004.1597%2C1004.5256%2C1004.3233%2C1004.4081%2C1004.4444%2C1004.4158%2C1004.5440%2C1004.4472%2C1004.3198%2C1004.4788%2C1004.5310%2C1004.2736%2C1004.4843%2C1004.0507%2C1004.4681%2C1004.2199%2C1004.0732%2C1004.2945%2C1004.2670%2C1004.4562%2C1004.3835%2C1004.0809%2C1004.0612%2C1004.2499%2C1004.3264%2C1004.4973%2C1004.5209%2C1004.0512%2C1004.5219%2C1004.1955%2C1004.3056%2C1004.1136%2C1004.2485%2C1004.2912%2C1004.4749%2C1004.0052%2C1004.0838%2C1004.3929%2C1004.0105%2C1004.0599%2C1004.0263%2C1004.4274%2C1004.0186%2C1004.0284%2C1004.0481%2C1004.3791%2C1004.2297%2C1004.3418%2C1004.5239%2C1004.3475%2C1004.0104%2C1004.3261%2C1004.0401%2C1004.1349%2C1004.2305%2C1004.3737%2C1004.0994%2C1004.1802%2C1004.0959%2C1004.4094%2C1004.1215%2C1004.1480%2C1004.1902%2C1004.2688%2C1004.3765%2C1004.4773%2C1004.5303%2C1004.2664%2C1004.0513%2C1004.3780%2C1004.5498%2C1004.2911%2C1004.3620%2C1004.4719%2C1004.2976%2C1004.3865%2C1004.0921%2C1004.3881%2C1004.1766&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Analysis, Interpretation, and Recognition of Facial Action Units and\n  Expressions Using Neuro-Fuzzy Modeling"}, "summary": "In this paper an accurate real-time sequence-based system for representation,\nrecognition, interpretation, and analysis of the facial action units (AUs) and\nexpressions is presented. Our system has the following characteristics: 1)\nemploying adaptive-network-based fuzzy inference systems (ANFIS) and temporal\ninformation, we developed a classification scheme based on neuro-fuzzy modeling\nof the AU intensity, which is robust to intensity variations, 2) using both\ngeometric and appearance-based features, and applying efficient dimension\nreduction techniques, our system is robust to illumination changes and it can\nrepresent the subtle changes as well as temporal information involved in\nformation of the facial expressions, and 3) by continuous values of intensity\nand employing top-down hierarchical rule-based classifiers, we can develop\naccurate human-interpretable AU-to-expression converters. Extensive experiments\non Cohn-Kanade database show the superiority of the proposed method, in\ncomparison with support vector machines, hidden Markov models, and neural\nnetwork classifiers. Keywords: biased discriminant analysis (BDA), classifier\ndesign and evaluation, facial action units (AUs), hybrid learning, neuro-fuzzy\nmodeling.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.3951%2C1004.0550%2C1004.5195%2C1004.2755%2C1004.3482%2C1004.3059%2C1004.3162%2C1004.1847%2C1004.4993%2C1004.3089%2C1004.4224%2C1004.1072%2C1004.1020%2C1004.5495%2C1004.2805%2C1004.1536%2C1004.2609%2C1004.2450%2C1004.4426%2C1004.3578%2C1004.2165%2C1004.1060%2C1004.1597%2C1004.5256%2C1004.3233%2C1004.4081%2C1004.4444%2C1004.4158%2C1004.5440%2C1004.4472%2C1004.3198%2C1004.4788%2C1004.5310%2C1004.2736%2C1004.4843%2C1004.0507%2C1004.4681%2C1004.2199%2C1004.0732%2C1004.2945%2C1004.2670%2C1004.4562%2C1004.3835%2C1004.0809%2C1004.0612%2C1004.2499%2C1004.3264%2C1004.4973%2C1004.5209%2C1004.0512%2C1004.5219%2C1004.1955%2C1004.3056%2C1004.1136%2C1004.2485%2C1004.2912%2C1004.4749%2C1004.0052%2C1004.0838%2C1004.3929%2C1004.0105%2C1004.0599%2C1004.0263%2C1004.4274%2C1004.0186%2C1004.0284%2C1004.0481%2C1004.3791%2C1004.2297%2C1004.3418%2C1004.5239%2C1004.3475%2C1004.0104%2C1004.3261%2C1004.0401%2C1004.1349%2C1004.2305%2C1004.3737%2C1004.0994%2C1004.1802%2C1004.0959%2C1004.4094%2C1004.1215%2C1004.1480%2C1004.1902%2C1004.2688%2C1004.3765%2C1004.4773%2C1004.5303%2C1004.2664%2C1004.0513%2C1004.3780%2C1004.5498%2C1004.2911%2C1004.3620%2C1004.4719%2C1004.2976%2C1004.3865%2C1004.0921%2C1004.3881%2C1004.1766&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper an accurate real-time sequence-based system for representation,\nrecognition, interpretation, and analysis of the facial action units (AUs) and\nexpressions is presented. Our system has the following characteristics: 1)\nemploying adaptive-network-based fuzzy inference systems (ANFIS) and temporal\ninformation, we developed a classification scheme based on neuro-fuzzy modeling\nof the AU intensity, which is robust to intensity variations, 2) using both\ngeometric and appearance-based features, and applying efficient dimension\nreduction techniques, our system is robust to illumination changes and it can\nrepresent the subtle changes as well as temporal information involved in\nformation of the facial expressions, and 3) by continuous values of intensity\nand employing top-down hierarchical rule-based classifiers, we can develop\naccurate human-interpretable AU-to-expression converters. Extensive experiments\non Cohn-Kanade database show the superiority of the proposed method, in\ncomparison with support vector machines, hidden Markov models, and neural\nnetwork classifiers. Keywords: biased discriminant analysis (BDA), classifier\ndesign and evaluation, facial action units (AUs), hybrid learning, neuro-fuzzy\nmodeling."}, "authors": ["Mahmoud Khademi", "Mohammad Hadi Kiapour", "Mohammad T. Manzuri-Shalmani", "Ali A. Kiaei"], "author_detail": {"name": "Ali A. Kiaei"}, "author": "Ali A. Kiaei", "links": [{"href": "http://arxiv.org/abs/1004.0512v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1004.0512v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1004.0512v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1004.0512v1", "arxiv_comment": null, "journal_reference": "LNAI vol. 5998, pp. 161--172, Springer, Heidelberg (Proc. of 4th\n  IAPR Workshop on Artificial Neural Networks in Pattern Recognition), 2010.", "doi": null, "fulltext": "Analysis, Interpretation, and Recognition of Facial\nAction Units and Expressions Using Neuro-Fuzzy\nModeling\nMahmoud Khademi1, Mohammad Hadi Kiapour2, Mohammad T. Manzuri-Shalmani1,\nand Ali A. Kiaei1\n1\n\nDSP Lab, Sharif University of Technology, Tehran, Iran\nInstitute for Studies in Fundamental Sciences (IPM), Tehran, Iran\n{khademi@ce.,kiapour@ee.,manzuri@,kiaei@ce.}sharif.edu\n2\n\nAbstract. In this paper an accurate real-time sequence-based system for representation, recognition, interpretation, and analysis of the facial action units\n(AUs) and expressions is presented. Our system has the following characteristics: 1) employing adaptive-network-based fuzzy inference systems (ANFIS)\nand temporal information, we developed a classification scheme based on neuro-fuzzy modeling of the AU intensity, which is robust to intensity variations,\n2) using both geometric and appearance-based features, and applying efficient\ndimension reduction techniques, our system is robust to illumination changes\nand it can represent the subtle changes as well as temporal information involved\nin formation of the facial expressions, and 3) by continuous values of intensity\nand employing top-down hierarchical rule-based classifiers, we can develop accurate human-interpretable AU-to-expression converters. Extensive experiments on Cohn-Kanade database show the superiority of the proposed method,\nin comparison with support vector machines, hidden Markov models, and neural network classifiers.\nKeywords: biased discriminant analysis (BDA), classifier design and evaluation, facial action units (AUs), hybrid learning, neuro-fuzzy modeling.\n\n1 Introduction\nHuman face-to-face communication is a standard of perfection for developing a natural, robust, effective and flexible multi modal/media human-computer interface due to\nmultimodality and multiplicity of its communication channels. In this type of communication, the facial expressions constitute the main modality [1]. In this regard,\nautomatic facial expression analysis can use the facial signals as a new modality and\nit causes the interaction between human and computer more robust and flexible.\nMoreover, automatic facial expression analysis can be used in other areas such as lie\ndetection, neurology, intelligent environments and clinical psychology.\nFacial expression analysis includes both measurement of facial motion (e.g. mouth\nstretch or outer brow raiser) and recognition of expression (e.g. surprise or anger).\nReal-time fully automatic facial expression analysis is a challenging complex topic in\ncomputer vision due to pose variations, illumination variations, different age, gender,\nF. Schwenker and N. El Gayar (Eds.): ANNPR 2010, LNAI 5998, pp. 161\u2013172, 2010.\n\u00a9 Springer-Verlag Berlin Heidelberg 2010\n\n\f162\n\nM. Khademi et al.\n\nethnicity, facial hair, occlusion, head motions, and lower intensity of expressions.\nTwo survey papers summarized the work of facial expression analysis before year\n1999 [2, 3]. Regardless of the face detection stage, a typical automatic facial expression analysis system consists of facial expression data extraction and facial expression\nclassification stages. Facial feature processing may happen either holistically, where\nthe face is processed as a whole, or locally. Holistic feature extraction methods are\ngood at determining prevalent facial expressions, whereas local methods are able to\ndetect subtle changes in small areas.\nThere are mainly two approaches for facial data extraction: geometric-based methods and appearance-based methods. The geometric facial features present the shape\nand locations of facial components. With appearance-based methods, image filters,\ne.g. Gabor wavelets, are applied to either the whole face or specific regions in a face\nimage to extract a feature vector [4].\nThe sequence-based recognition method uses the temporal information of the sequences (typically from natural face towards the frame with maximum intensity) to\nrecognize the expressions. To use the temporal information, the techniques such as\nhidden Markov models (HMMs) [5], recurrent neural networks [6] and rule-based\nclassifier [7] were applied.\nThe facial action coding system (FACS) is a system developed by Ekman and Friesen [8] to detect subtle changes in facial features. The FACS is composed of 44 facial\naction units (AUs). 30 AUs of them are related to movement of a specific set of facial\nmuscles: 12 for upper face (e.g. AU 1 inner brow raiser, AU 2 outer brow raiser, AU\n4 brow lowerer, AU 5 upper lid raiser, AU 6 cheek raiser, AU 7 lid tightener) and 18\nfor lower face (e.g. AU 9 nose wrinkle, AU 10 upper lip raiser, AU 12 lip corner\npuller, AU 15 lip corner depressor, AU 17 chin raiser, AU 20 lip stretcher, AU 23 lip\ntightener, AU 24 lip pressor, AU 25 lips part, AU 26 jaw drop, AU 27 mouth stretch).\nFacial action units can occur in combinations and vary in intensity. Although the\nnumber of single action units is relatively small, more than 7000 different AU combinations have been observed. To capture such subtlety of human emotion paralinguistic communication, automated recognition of fine-grained changes in facial expression is required (for more details see [8, 9]).\nThe main goal of this paper is developing an accurate real-time sequence-based\nsystem for representation, recognition, interpretation, and analysis of the facial action\nunits (AUs) and expressions. We summarize the advantages of our system as follows:\n1) The facial action unit intensity is intrinsically fuzzy. We developed a classification\nscheme based on neuro-fuzzy modeling of the AU intensity, which is robust to intensity variations. Applying this accurate method, we can recognize lower intensity\nand combinations of AUs.\n2) Recent work suggests that spontaneous and deliberate facial expressions may be\ndiscriminated in term of timing parameters. Employing temporal information instead of using only the last frame, we can represent these parameters properly.\n3) By using both geometric and appearance features, we can increase the recognition\nrate and also make the system robust against illumination changes.\n4) By employing top-down hierarchical rule-based classifiers such as J48, we can automatically extract human interpretable classification rules to interpret each expression.\n5) Due to the relatively low computational cost, the proposed system is suitable for\nreal-time applications.\n\n\fAnalysis, Interpretation, and Recognition of Facial Action Units and Expressions\n\n163\n\nThe rest of the paper has been organized as follows: In section 2, we describe the\napproach which is used for facial data extraction and representation using both geometric and appearance features. Then, we discuss the proposed scheme for recognition of\nfacial action units and expressions in section 3 and section 4 respectively. Section 5\nreports our experimental results, and section 6 presents conclusions and a discussion.\n\n2 Facial Data Extraction and Representation\n2.1 Biased Learning\nBiased learning is a learning problem in which there are an unknown number of\nclasses but we are only interested in one class. This class is called \"positive\" class.\nOther samples are considered as \"negative\" samples. In fact, these samples can come\nfrom an uncertain number of classes. Suppose x |i 1, ... , N and y |i 1, ... , N\nare the set of positive and negative d-dimensional samples (feature vectors) respectively. Consider the problem of finding d r transformation matrix w (r d), such\nthat separates projected positive samples from projected negatives in the new subspace. The dimension reduction methods like fisher discriminant analysis (FDA) and\nmultiple discriminant analysis have addressed this problem simply as a two-class\nclassification problem with symmetric treatment on positive and negative examples.\nFor example in FDA, the goal is to find a subspace in which the ratio of between-class\nscatter over within-class scatter matrices is maximized. However, it is part of the\nobjective function that negative samples shall cluster in the discriminative subspace.\nThis is an unnecessary and potentially damaging requirement because very likely the\nnegative samples belong to multiple classes. In fact, any constraint put on negative\nsamples other than stay away from the positives is unnecessary and misleading. With\nasymmetric treatment toward the positive samples, Zhou and Huang [10] proposed\nthe following objective function:\nw\n\narg max\n\ntrace w S w\ntrace w S w\n\n(1)\n\nwhere S and S are within-class scatter matrices of negative and positive samples\nwith respect to positive centroid, respectively. The goal is to find w that clusters only\npositive samples while keeping negatives away. The problem of finding optimal w,\nbecomes finding the generalized eigenvectors \u03b1's associated with the largest eigenvalues \u03bb's in the below generalized eigenanalysis problem:\nS \u03b1\n\n\u03bbS \u03b1\n\n(2)\n\nOur goal is developing a facial action unit recognition system that can detect whether\nthe AUs occur or not. The input of the system is a sequence of frames from natural\nface towards one of the facial expressions with maximum intensity. Suppose we have\nextracted a feature matrix or a feature vector from each frame. In order to embed facial\nfeatures in a low-dimensionality space and deal with curse of dimensionality dilemma,\nwe should use a dimension reduction method. For recognition of each AU, we are\nfacing an asymmetric two-class classification problem. For example when the goal is\ndetecting whether AU 27 (mouth stretch) occur or not, the positive class includes all of\n\n\f164\n\nM. Khademi et al.\n\nsequences in the train set that represent stretching of the mouth; other sequences are\nconsidered as negative samples. These samples can come from an uncertain number of\nclasses. They can represent any single AU or AU combinations except AU 27. In fact,\nour problem is a biased learning problem.\n2.2 Appearance-Based Facial Feature Extraction Using Gabor Wavelets\nIn order to extract the appearance-based facial features from each frame, we use a set\nof Gabor wavelets. They allow detecting line endings and edge borders of each frame\nover multiple scales and with different orientations. Gabor wavelets remove also most\nof the variability in images that occur due to lighting changes [4]. Each frame is convolved with p wavelets to form the Gabor representation of the t frames (Fig. 1).\n\nAU 17+AU 23+AU 24\n\nAU 1+AU 2+AU 5\n\nFig. 1. Examples of the image sequences and their representation using Gabor wavelets\n\nHowever, for applying the Zhou and Huang's method, which is called biased discriminant analysis (BDA), to the facial action unit recognition problem we should\nfirst transform the feature matrices of the sequence into a one-dimensional vector that\nignores the underlying data structure (temporal and local information) and leads to the\ncurse of dimensionality dilemma and the small sample size problem. Thus, we use\ntwo-dimension version of BDA algorithm by simply replacing the image vector with\nimage matrix in computing the corresponding variance matrices to reduce the dimensionality of each feature matrix in two directions [11]. Then, we apply BDA algorithm\nto the vectorized representation of the reduced feature matrices. Also, In order to deal\nwith singularity in the matrices we use 2D and 1D principle component analysis\n(PCA) algorithms [12], before applying 2DBDA and BDA respectively.\n2.3 Geometric-Based Facial Feature Extraction Using Optical Flow\nIn order to extract geometric features we use a facial feature extraction method presented in [13]. The points of a 113-point grid, which is called Wincanide-3, are placed\non the first frame manually. Automatic registering of the grid with the face has been\naddressed in many literatures (e.g. see [14]). For upper face and lower face action units\na particular set of points are selected. The pyramidal optical flow tracker [15] is employed to track the points of the model in the successive frames towards the last frame\n(see Fig. 2). The loss of the tracked points is handled through a model deformation\nprocedure (for detail see [13]). For each frame, the displacements of the points in two\ndirections with respect to the first frame are calculated and placed in the columns of a\nmatrix. Then, we apply 2DBDA algorithm [11] to the matrix in two directions. The\nvectorized representation of the reduced feature matrix is used as geometric feature\nvector.\n\n\fAnalysis, Interpretation, and Recognition of Facial Action Units and Expressions\n\n165\n\nFig. 2. Geometric-based facial feature extraction using grid tracking\n\n3 Facial Action Unit Recognition Using Neuro-Fuzzy Modeling\n3.1 Takagi-Sugeno Fuzzy Inference System and Training Data Set\nThe flow diagram of the proposed system is shown in Fig. 3. In order to recognize\neach single AU we construct a fuzzy rule-based system. The reduced feature vector is\nused as the input of the system.\n\nFig. 3. Block diagram of the proposed system (n is number of the facial action units)\n\n\f166\n\nM. Khademi et al.\n\nEach system is composed of n Takagi-Sugeno type fuzzy if-then rules of the below\nformat:\nR : if x is A and ... and x is A then y\n\np\n\np x\n\np x\n\ni\n\n1, ... , n\n\nHere, y is variable of the consequence whose value is the AU intensity and we should\ninfer it. x , ... , x are variables of the premise, i.e. features, that appear also in the part\nof the consequence. A , ... , A i 1, ... , n are fuzzy sets representing a fuzzy subspace in which the rule R can be applied for reasoning (we use Gaussian membership\nfunction with two parameters), and p , p ... , p i 1, ... , n are consequence parameters. The fuzzy implication is based on a fuzzy partition of the input space. In each\nfuzzy subspace, a linear input-output relation is formed.\nWhen we are given x\nx ,x\nx ,...,x\nx , the fuzzy inference system\nproduces output of the system as follows:\nFor each implication R , y is calculated as:\ny\n\np\n\np x\n\np x\n\nThe weight of each proposition y\nw\n\nA x\n\n...\n\ni\n\n1, ... , n\n\n(3)\n\ni\n\n1, ... , n\n\n(4)\n\ny is calculated as:\nA x\n\nThen, the final output y inferred from n rules is given as the average of all y i\n\u2211 y w / \u2211 w .\n1, ... , n with the weights w , i.e. y\nFor modeling each system, i.e. each single AU, we should extract the rules using\ntraining data. Depending on the AU that we want to model it, the sequences in the\ntraining set are divided into two subsets: positive set and negative set. The positive set\nincludes all of sequences that the AU occurs on them; other sequences are placed in\nthe negative set. Then, by applying the method discussed in section 2, we extract the\nfeature vector for each sequence. Assuming the last frames of the sequences in the\ntraining set have maximum intensity, the target value of feature vectors which are in\nthe positive set and negative set is labeled 1 and 0 respectively. We also use some\nsequences several times with different intensities, i.e. by using intermediate frames as\nthe last frame and removing the frames which come after it. For these sequences, if\nthe original sequence was negative the target value is again 0. Otherwise, the target\nvalue of corresponding feature vector for produced sequence is calculated by:\nt\n\nsumdistances\nsumdistances\n\n(5)\n\nwhere sumdistances is the sum of the Euclidian distances between point of the\nWincandide-3 grid in the last frame of the produced sequence and their positions in\nthe first frame (a subset of points for upper face and lower face action units are used).\nSimilarly, sumdistances is the sum of the Euclidean distances between points of the\nmodel in the last frame of the original sequence and their positions in the first frame;\ne.g. if we remove all frames which come after the first frame then t 0, and if we\nremove none of the frames then t 1. We model each single AU two times, using\ngeometric and appearance features separately. In test phase, the outputs are added and\nthe result is passed through a threshold. When several outputs were on, it signals that\na combination of AUs has been occurred.\n\n\fAnalysis, Interpretation, and Recognition of Facial Action Units and Expressions\n\n167\n\nModeling of each system, i.e. each single AU, is composed of two parts: structure\nidentification and parameter identification. The structure identification relate to partition of the input space, i.e. number of rules. In parameter identification process the\npremise parameters and consequence parameters are determined.\n3.2 Structure Identification Algorithm\nWe use some of the training samples as the validation set. This set is used for avoiding the problem of overfitting data in the modeling process. Suppose x , x , ... , x are\nthe inputs of the system. Moreover, suppose d is the number of divided fuzzy subspace for x . The initial value of d i 1, ... , k is 1, because at first the range of each\nvariable is undivided. Also, let V, i.e. the value of mean squares of errors of the model\non validation set be a big number. The algorithm of modeling is as follows:\n1) The range of x is divided into one more fuzzy subspace (e.g. \"big\" and \"small\"\nif d\n1 or \"big\", \"medium\" and \"small\" if d\n2) and the range of the other variables x , x , ... , x are not more divided. This model is called model 1; e.g. in the\nfirst iteration, model 1 consisting of two rules of the below format:\nR : if x is big then y\n\np\n\nR : if x is samll then y\n\np\n\np x\np x\n\nSimilarly the model in which the range of x is divided in to one more subspace and\nthe rage of other variables x , x , ... , x are not more divided, is called mode 2. In this\nway we have k models.\n2) For each model the optimum premise parameters (mean and variance of the\nmembership functions) and consequence parameters are found by the parameter identification algorithm described in the next subsection. This algorithm applies hybrid\nlearning, to determine the premise and consequence parameters.\n3) For each model, the mean squares of errors (MSE) using training data is calculated:\nMSE\n\n\u2211\n\ny\nP\n\nt\n\n(6)\n\nHere, P is number of the training data, y j 1, ... , P is the final output inferred from\nrules of the model for j'th feature vector in the training set. t j 1, ... , P is target\nvalue for j'th input vector in training set, which is a number between 0 to 1. Then, the\nmodel with least mean squares of errors is selected. This model called stable state\nmodel. Let T be the MSE of the stable state model using validation set.\n4) If T V stop otherwise, let d\nd\n1. where s is index of the stable state\nmodel; let V T and go to step 1.\nAfter each iteration of the modeling algorithm, the range of a variable is divided to\none more fuzzy subspace. In each fuzzy subspace, a linear input-output relation in\nconsequence part of the corresponding rule is used to approximate the intensity of\nAU. Consequently, a highly non-linear system can be approximated efficiently by this\nmethod. Applying this accurate approach, we can recognize the lower intensity and\ncombinations of AUs.\n\n\f168\n\nM. Khademi et al.\n\n3.3 Parameter Identification Using Hybrid Learning\nThe goal of this section is determining the optimum premise parameters (mean and\nvariance of the membership functions), and consequent parameters of the model, assuming fixed structure. We use an adaptive-network-based fuzzy inference system\n(ANFIS) to determine the parameters (for more details see [16]). This architecture\nrepresents the fuzzy inference described in subsection 4.1. Given the values of premise\nparameters, the overall output can be express as a linear combinations of consequence\nparameters. In forward pass of the hybrid learning algorithm, functional signals go\nforward till layer 4 of the ANFIS and the consequence parameters are identified by the\nleast squares estimate. In the backward pass, the error rates propagate backward and\nthe premise parameters are updated by gradient descent procedure.\nAlternatively, we could apply the gradient descent procedure to identify all parameters. But this method is generally slow and likely to become trapped in local minima. By using the hybrid algorithm, we can decrease the dimension of search space\nand cut down the convergence time.\n\n4 Facial Expression Recognition and Interpretation\nAlthough we can use a SVMs for classification of six basic facial expressions (by\nfeature vectors directly or AU intensity values), employing rule-based classifiers such\nas J48 [17], we can automatically extract human interpretable classification rules to\ninterpret each expression. Thus, novel accurate AU-to-expression converters by continues values of the AU intensities can be created. These converters would be useful\nin animation, cognitive science, and behavioral science areas.\n\n5 Experimental Results\nTo evaluate the performance of the proposed system and other methods like support\nvector machines (SVMs), hidden Markov models (HMMs), and neural network (NN)\nclassifiers, we test them on Cohn-Kanade database [18]. The database includes 490\nfrontal view image sequences from over 97 subjects. The final frame of each image\nsequence has been coded using Facial Action Coding System which describes subject's expression in terms of action units. For action units that vary in intensity, a 5point ordinal scale has been used to measure the degree of muscle contraction. In\norder to test the algorithm in lower intensity situation, we used each sequence five\ntimes with different intensities, i.e. by using intermediate frames as the last frame. Of\ntheses, 1500 sequences were used as the training set. Also, for upper face and lower\nface AUs, 240 and 280 sequences were used as the test set respectively. None of the\ntest subjects appeared in training data set. Some of the sequences contained limited\nhead motion.\nImage sequences from neutral to the frame with maximum intensity, were cropped\ninto 57 102 and 52 157 pixel arrays for lower face and upper face action units\nrespectively. To extract appearance features we applied 16 Gabor kernels to each\nframe. We used the same dimension reduction method in the proposed and SVMs\nmethods. Depending on the single AU that we want to model it, the geometric and\n\n\fAnalysis, Interpretation, and Recognition of Facial Action Units and Expressions\n\n169\n\nappearance feature vectors were of dimension 4 to 8 after applying the dimension\nreduction techniques. In training phase we allowed the target value of feature vector\nfor multiple systems (single AUs) to set 1, when the input consists of AU combinations. The value of the threshold is set to 1 (see Fig. 3).\nTable 1 and Table 2 show the upper face and lower face action unit recognition\nresults respectively. In the proposed method, an average recognition rate of 88.8 and\n95.4 percent were achieved for upper face and lower face action units respectively.\nAlso, an average false alarm rate of 7.1 and 2.9 percent were achieved for upper face\nand lower face action units respectively. In SVMs method, we first concatenated the\nreduced geometric and appearance feature vectors for each single AU. Then, we\nclassify them using a two-class SMVs classifier with Gaussian kernel. Due to use of\ncrisp value for targets, this method suffers from intensity variations. In HMMs\nmethod, the best performance was obtained by three Gaussians and five states. The\nGabor coefficients were reduced to 100 dimensions per image sequence using PCA\nand 2DPCA (like [19]). The geometric features were reduced to 8 dimension using\nPCA. Then, we concatenated the geometric and appearance feature vectors. For each\nsingle AU and also each AU combination, a hidden Markov model was trained, i.e. in\nthis method we consider each AU as a class. We used the same dimension reduction\nmethod in the NN and HMMs methods.\nTable 1. Upper face action unit recognition results (R=recognition rate, F=false alarm)\nAUs\n\nSequences\n\n1\n2\n4\n5\n6\n7\n1+2\n1+2+4\n1+2+5\n1+4\n1+6\n4+5\n6+7\nTotal\nR\nF\n\n20\n10\n20\n20\n20\n10\n40\n20\n10\n10\n10\n20\n30\n240\n\nAUs\n\nSequences\n\nProposed method\nRecognized AUs\nTrue\nMissing or extra\n17\n2(1+2+4), 1(1+2)\n7\n1(1+2+4), 2(1+2)\n19\n1(1+2+4)\n20\n0\n19\n0\n9\n0\n37\n2(2), 1(1+2+4)\n18\n1(1), 1(2)\n8\n2(1+2)\n7\n3(1+2+4)\n8\n1(1+6+7)\n17\n2(4), 1(5)\n27\n2(1+6+7), 1(7)\n213\n24\n88.8%\n7.1%\n\nHMMs\nAUs\nFalse\n0\n0\n0\n0\n1(7)\n1(6)\n0\n0\n0\n0\n1(7)\n0\n0\n3\n\nSequences\n\n1\n2\n4\n5\n6\n7\n1+2\n1+2+4\n1+2+5\n1+4\n1+6\n4+5\n6+7\nTotal\nR\nF\n\n20\n10\n20\n20\n20\n10\n40\n20\n10\n10\n10\n20\n30\n240\n\nAUs\n\nSequences\n\nSVMs\n\n1\n2\n4\n5\n6\n7\n1+2\n1+2+4\n1+2+5\n1+4\n1+6\n4+5\n6+7\nTotal\nR\nF\n\n20\n10\n20\n20\n20\n10\n40\n20\n10\n10\n10\n20\n30\n240\n\nRecognized AUs\nTrue\nMissing or Extra\n15\n2(1+2+4), 1(1+2)\n6\n2(1+2+4)\n18\n1(1+2+4)\n20\n0\n19\n1(1+6)\n7\n0\n35\n1(2), 2(1+2+4)\n15\n2(1), 2(2)\n6\n2(1+5)\n4\n3(1+2+4)\n6\n3(1+6+7)\n15\n2(1+2+5)\n24\n2(1+6+7), 2(7)\n190\n28\n79.2%\n17.1%\n\nTrue\n15\n6\n18\n20\n18\n7\n38\n16\n7\n5\n6\n14\n25\n195\n\nRecognized AUs\nMissing or extra\n2(1+2+4), 2(1+2)\n2(1+2+4), 1(1+2)\n1(1+2+4)\n0\n1(1+6)\n2(6+7)\n1(1+2+4)\n2(2), 2(1+2)\n3(1+2)\n3(1+2+4)\n2(1+6+7)\n3(4), 1(5)\n2(1+6+7), 3(7)\n33\n81.3%\n12.9%\n\nFalse\n1(2)\n1(1)\n1(2)\n0\n1(7)\n1(6)\n1(4)\n0\n0\n2(5)\n2(7)\n2(2)\n0\n12\n\nNN\nFalse\n2(2)\n2(1)\n1(2)\n0\n0\n3(6)\n2(4)\n1(5)\n2(4)\n3(5)\n1(7)\n3(2)\n2(1)\n22\n\n1\n2\n4\n5\n6\n7\n1+2\n1+2+4\n1+2+5\n1+4\n1+6\n4+5\n6+7\nTotal\nR\nF\n\n20\n10\n20\n20\n20\n10\n40\n20\n10\n10\n10\n20\n30\n240\n\nTrue\n14\n5\n18\n18\n18\n6\n36\n16\n7\n5\n6\n16\n24\n189\n\nRecognized AUs\nMissing or Extra\n3(1+2+4)\n4(1+2+4)\n1(1+2+4)\n1(4+5)\n2(1+6)\n2(6+7)\n2(2), 2(1+2+4)\n2(1), 2(2)\n2(1+2)\n4(1+2+4)\n2(1+6+7)\n3(4)\n3(1+6+7), 3(7)\n38\n78.8%\n15.4%\n\nFalse\n3(2)\n1(1)\n1(2)\n1(5)\n0\n2(6)\n0\n0\n1(4)\n1(5)\n2(7)\n1(1)\n0\n13\n\n\f170\n\nM. Khademi et al.\n\nAlthough this method can deal with AU dynamics properly, it needs the probability\ndensity function for each state. Moreover, the number of AU combinations is too big\nand the density estimation methods may lead to poor result especially when the\nnumber of training samples is low. Finally, in NN methods we trained a neural\nnetwork with an output unit for each single AU and by allowing multiple output units\nto fire when the input sequence consists of AU combinations (like [20]).\nTable 2. Lower facial action unit recognition results (R=recognition rate, F=false alarm)\nAUs\n\n9\n10\n12\n15\n17\n20\n25\n26\n27\n9+17\n9+17+23+24\n9+25\n10+17\n10+15+17\n10+25\n12+25\n12+26\n15+17\n17+23+24\n20+25\nTotal\nR\nF\n\nProposed method\nSequences\nRecognized AUs\nTrue\nMissing or\nFalse\nextra\n8\n8\n0\n0\n12\n12\n0\n0\n12\n12\n0\n0\n8\n8\n0\n0\n16\n16\n0\n0\n12\n12\n0\n0\n48\n48\n0\n0\n24\n18\n4(25+26)\n2(25)\n24\n24\n0\n0\n24\n24\n0\n0\n4\n3\n1(19+17+24)\n0\n4\n4\n0\n0\n8\n5\n2(17), 1(10)\n0\n4\n3\n1(15+17)\n0\n8\n8\n0\n0\n16\n16\n0\n0\n8\n6\n2(12+25)\n0\n16\n16\n0\n0\n8\n8\n0\n0\n16\n16\n0\n0\n280\n267\n11\n2\n95.4%\n2.9%\n\nHMMs\nAUs\n\nSequences\nTrue\n\n9\n10\n12\n15\n17\n20\n25\n26\n27\n9+17\n9+17+23+24\n9+25\n10+17\n10+15+17\n10+25\n12+25\n12+26\n15+17\n17+23+24\n20+25\nTotal\nR\nF\n\n8\n12\n12\n8\n16\n12\n48\n24\n24\n24\n4\n4\n8\n4\n8\n16\n8\n16\n8\n16\n280\n\nAUs\n\nSequences\n\nSVMs\nAUs\n\n9\n10\n12\n15\n17\n20\n25\n26\n27\n9+17\n9+17+23+24\n9+25\n10+17\n10+15+17\n10+25\n12+25\n12+26\n15+17\n17+23+24\n20+25\nTotal\nR\nF\n\nSequences\n\n8\n12\n12\n8\n16\n12\n48\n24\n24\n24\n4\n4\n8\n4\n8\n16\n8\n16\n8\n16\n280\n\nRecognized AUs\nTrue\nMissing or\nextra\n8\n0\n8\n2(10+7)\n12\n0\n6\n2(15+17)\n14\n2(10+17)\n12\n0\n43\n2(25+26)\n18\n3(25+26)\n24\n0\n22\n2(9)\n1\n3(9+17+24)\n4\n0\n2\n4(10+12)\n2\n2(15+17)\n7\n1(25)\n16\n0\n3\n3(12+25)\n16\n0\n6\n2(17+24)\n11\n3(20+26)\n235\n31\n83.9%\n12.5%\n\n8\n11\n12\n6\n16\n12\n45\n19\n24\n22\n2\n4\n3\n2\n7\n16\n5\n16\n6\n12\n248\n\nRecognized AUs\nMissing or\nextra\n0\n1(10+17)\n0\n1(15+17)\n0\n0\n2(25+26)\n3(25+26)\n0\n2(9)\n2(19+17+24)\n0\n3(10+12)\n2(15+17)\n1(25)\n0\n2(12+25)\n0\n1(17+23)\n2(20+26)\n22\n88.6%\n8.9%\n\nFalse\n0\n0\n0\n1(17)\n0\n0\n1(26)\n2(25)\n0\n0\n0\n0\n2(12)\n0\n0\n0\n1(25)\n0\n1(10)\n2(26)\n10\n\nNN\nFalse\n0\n2(17)\n0\n0\n0\n0\n3(26)\n3(25)\n0\n0\n0\n0\n2(12)\n0\n0\n0\n2(25)\n0\n0\n2(26)\n14\n\nRecognized AUs\nMissing or\nextra\n7\n1(9+17)\n8\n2(10+7)\n11\n1(12+25)\n6\n2(15+17)\n13\n2(10+17)\n12\n0\n42\n3(25+26)\n19\n2(25+26)\n23\n1(27+25)\n22\n2(9)\n1\n3(9+17+24)\n4\n0\n4\n2(10+12)\n2\n2(15+17)\n7\n1(25)\n16\n0\n3\n3(12+25)\n16\n0\n6\n2(17+24)\n12\n3(20+26)\n234\n32\n83.6%\n12.9%\n\nTrue\n9\n10\n12\n15\n17\n20\n25\n26\n27\n9+17\n9+17+23+24\n9+25\n10+17\n10+15+17\n10+25\n12+25\n12+26\n15+17\n17+23+24\n20+25\nTotal\nR\nF\n\n8\n12\n12\n8\n16\n12\n48\n24\n24\n24\n4\n4\n8\n4\n8\n16\n8\n16\n8\n16\n280\n\nFalse\n0\n2(17)\n0\n0\n1(10)\n0\n3(26)\n3(25)\n0\n0\n0\n0\n2(12)\n0\n0\n0\n2(25)\n0\n0\n1(26)\n14\n\n\fAnalysis, Interpretation, and Recognition of Facial Action Units and Expressions\n\n171\n\nThe best performance was obtained by one hidden layer. Although this method can\ndeal with intensity variations by using continues values for target of feature vectors, it\nsuffer from trapping in local minima. Also unlike the proposed method, in NN classifier there is no any systematic approach to determine the structure of the network,\ni.e. number of hidden layer and hidden units. Table 3 shows the facial expression\nrecognition results using J48 [17] classifier. As discussed in section 4, by applying\neach rule-based classifier we can develop an AU-to-expression converter.\nTable 3. Facial expression recognition results using J48 [17] classifier\nConfusion matrix for J48 classifier (total number of samples=2916, correctly\nclassified samples=2710 (92.935%), incorrectly classified samples=206 (7.065%) :\n\nClassified\nas \u2192\nSurprise\nGloomy\nFear\nHappy\nAngry\nDisgust\n\nSurprise\n\nGloomy\n\nFear\n\n579\n6\n23\n0\n29\n6\n\n7\n467\n0\n0\n11\n0\n\n7\n0\n402\n0\n0\n0\n\nHappy\n0\n0\n0\n618\n0\n0\n\nAngry\n\nDisgust\n\n7\n13\n49\n0\n404\n30\n\n0\n0\n0\n0\n18\n24\n\nThe resulted tree for converting the AU intensities to\nexpressions using J48. Each path from root to leaf\nrepresents a rule (S=surprise, G=gloomy, F=fear,\nH=happy, A=angry, D=disgust, the value of each AU\nis between 0 and 1):\n\nDetailed accuracy by class for J48 classifier:\n\nTrue positive\nrate\n0.965\n0.961\n0.848\n1.000\n0.874\n0.870\n\nFalse\npositive rate\n0.028\n0.007\n0.003\n0.000\n0.040\n0.007\n\nPrecision\n\nROC area\n\nClass\n\n0.900\n0.963\n0.983\n1.000\n0.803\n0.930\n\n0.992\n0.996\n0.987\n1.000\n0.973\n0.987\n\nSurprise\nGloomy\nFear\nHappy\nAngry\nDisgust\n\n6 Discussion and Conclusions\nWe proposed an efficient system for representation, recognition, interpretation, and\nanalysis of the facial action units (AUs) and expressions. As an accurate tool, this\nsystem can be applied to many areas such as recognition of spontaneous and deliberate facial expressions, multi modal/media human computer interaction and lie detection efforts. In our neuro-fuzzy classification scheme each fuzzy rule applies a linear\napproximation to estimate the AU intensity in a specific fuzzy subspace. In addition\ncombining geometric and appearance features increases the recognition rate.\nAlthough the computational cost of the proposed method can be high in the training phase, when the fuzzy inference systems were created, it needs only some matrix\nproducts to reduce the dimensionality of the geometric and appearance features in the\ntest phase. Employing a 3 3 Gabor kernel and a grid with low number of vertices,\nwe can construct the Gabor representation of the input image sequence and also track\nthe grid in less than two seconds with moderate computing power. As a result, the\nproposed system is suitable for real-time applications. Future research direction is to\nconsider variations on face pose in the tracking algorithm.\nAcknowledgment. The authors would like to thank the Robotic Institute of Carnegie\nMellon University for allowing us to use their database.\n\n\f172\n\nM. Khademi et al.\n\nReferences\n1. Mehrabian, A.: Communication without words. Psychology Today 2(4), 53\u201356 (1968)\n2. Patnic, M., Rothkrantz, J.: Automatic analysis of facial expressions: the state of art. IEEE\nTransactions on PAMI 22(12) (2000)\n3. Fasel, B., Luettin, J.: Automatic facial expression analysis: a survey. Pattern Recognition 36(1), 259\u2013275 (2003)\n4. Lyons, M., Akamatsu, S., Kamachi, M., Gyoba, J.: Coding facial expressions with Gabor wavelets. In: 3rd IEEE Int. Conf. on Automatic Face and Gesture Recognition, pp. 200\u2013205 (1998)\n5. Cohen, I., Sebe, N., Cozman, F., Cirelo, M., Huang, T.: Coding, analysis, interpretation,\nand recognition of facial expressions. Journal of Computer Vision and Image Understanding Special Issue on Face Recognition (2003)\n6. Rosenblum, M., Yacoob, Y., Davis, L.: Human expression recognition from motion using\na radial basis function network architecture. IEEE Transactions on Neural Network 7(5),\n1121\u20131138 (1996)\n7. Cohn, J., Kanade, T., Moriyama, T., Ambadar, Z., Xiao, J., Gao, J., Imamura, H.: A comparative study of alternative faces coding algorithms, Technical Report CMU-RI-TR-0206, Robotics Institute, Carnegie Mellon University, Pittsburgh (2001)\n8. Ekman, P., Friesen, W.: The facial action coding system: A technique for the measurment\nof facial movement. Consulting Psychologist Press, San Francisco (1978)\n9. Tian, Y., Kanade, T., Cohn, F.: Recognizing action units for facial expression analysis.\nIEEE Transactions on PAMI 23(2) (2001)\n10. Sean, Z., Huang, T.: Small sample learning during multimedia retrieval using bias map. In:\nIEEE Int. Conf. on Computer Vision and Pattern Recognition, Hawaii (2001)\n11. Lu, Y., Yu, J., Sebe, N., Tian, Q.: Two-dimensional adaptive discriminant analysis. In: IEEE\nInternational Conf. on Acoustics, Speech and Signal Processing, vol. 1, pp. 985\u2013988 (2007)\n12. Yang, D., Frangi, A., Yang, J.: Two-dimensional PCA: A new approach to appearance-based\nface representation and recognition. IEEE Transactions on PAMI 26(1), 131\u2013137 (2004)\n13. Kotsia, I., Pitas, I.: Facial expression recognition in image sequences using geometric deformation features and support vector machines. IEEE Transactions on Image\nProcessing 16(1) (2007)\n14. Wiskott, L., Fellous, K.N., Malsburg, C.: Face recognition by elastic bunch graph matching. IEEE Transactions on PAMI 19(7), 775\u2013779 (1997)\n15. Bouguet, J.: Pyramidal implementation of the Lucas Kanade feature tracker description of\nthe algorithm, Technical Report, Intel Corporation, Microprocessor Research Labs (1999)\n16. Jang, R.: ANFIS: Adaptive-network-based fuzzy inference systems. IEEE Transactions on\nSystems, Man and Cybernetics 23(3), 665\u2013685 (1993)\n17. Quinlan, R.: C4.5: Programs for machine learning. Morgan Kaufmann Publishers, San Mateo (1993)\n18. Kanade, T., Tian, Y.: Comprehensive database for facial expression analysis. In: IEEE In.\nConf. on Face and Gesture Recognition, pp. 46\u201353 (2000)\n19. Bartlett, M., Braathen, B., Littlewort-Ford, G., Hershey, J., Fasel, I., Marks, T., Smith, E.,\nSejnowski, T.: Movellan. J.:Automatic analysis of spontaneous facial behavior: A final\nproject report, Technical Report INC-MPLab-TR-2001.08, UCSD (2001)\n20. Tian, Y., Kanade, T., Cohn., J.: Evaluation of gabor-wavelet-based facial action unit recognition in image sequences of increasing complexity. In: IEEE Int. Conf. on Automatic\nFace and Gesture Recognition (2002)\n21. Takagi, T., Sugeno, M.: Fuzzy identification of systems and its applications to modeling\nand control. IEEE Transactions on systems, Man and Cybernetics 15(1) (1985)\n\n\f"}