{"id": "http://arxiv.org/abs/1107.4153v1", "guidislink": true, "updated": "2011-07-21T04:15:25Z", "updated_parsed": [2011, 7, 21, 4, 15, 25, 3, 202, 0], "published": "2011-07-21T04:15:25Z", "published_parsed": [2011, 7, 21, 4, 15, 25, 3, 202, 0], "title": "Performance and Convergence of Multi-user Online Learning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.5985%2C1107.0235%2C1107.1264%2C1107.5126%2C1107.5743%2C1107.2685%2C1107.3802%2C1107.3624%2C1107.0435%2C1107.2600%2C1107.1974%2C1107.0371%2C1107.4428%2C1107.3750%2C1107.4354%2C1107.1010%2C1107.2598%2C1107.1162%2C1107.1933%2C1107.5704%2C1107.3871%2C1107.2875%2C1107.0468%2C1107.3265%2C1107.3040%2C1107.5657%2C1107.2068%2C1107.4545%2C1107.1790%2C1107.1614%2C1107.6004%2C1107.3579%2C1107.5583%2C1107.1914%2C1107.0562%2C1107.5330%2C1107.1984%2C1107.0509%2C1107.2665%2C1107.1207%2C1107.1083%2C1107.0481%2C1107.2442%2C1107.2678%2C1107.5196%2C1107.3563%2C1107.5707%2C1107.1823%2C1107.0655%2C1107.4075%2C1107.5849%2C1107.0591%2C1107.5103%2C1107.4641%2C1107.0750%2C1107.3532%2C1107.5361%2C1107.5639%2C1107.2468%2C1107.4541%2C1107.1185%2C1107.5695%2C1107.5128%2C1107.5037%2C1107.1486%2C1107.0173%2C1107.5344%2C1107.1584%2C1107.5055%2C1107.5049%2C1107.2336%2C1107.0764%2C1107.1968%2C1107.0988%2C1107.5901%2C1107.3870%2C1107.3673%2C1107.4939%2C1107.2458%2C1107.4153%2C1107.1278%2C1107.4269%2C1107.5294%2C1107.5673%2C1107.2705%2C1107.5961%2C1107.0514%2C1107.5808%2C1107.2235%2C1107.1312%2C1107.2582%2C1107.0829%2C1107.1084%2C1107.2805%2C1107.2359%2C1107.4676%2C1107.5683%2C1107.4434%2C1107.0568%2C1107.4664%2C1107.4241&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Performance and Convergence of Multi-user Online Learning"}, "summary": "We study the problem of allocating multiple users to a set of wireless\nchannels in a decentralized manner when the channel quali- ties are\ntime-varying and unknown to the users, and accessing the same channel by\nmultiple users leads to reduced quality due to interference. In such a setting\nthe users not only need to learn the inherent channel quality and at the same\ntime the best allocations of users to channels so as to maximize the social\nwelfare. Assuming that the users adopt a certain online learning algorithm, we\ninvestigate under what conditions the socially optimal allocation is\nachievable. In particular we examine the effect of different levels of\nknowledge the users may have and the amount of communications and cooperation.\nThe general conclusion is that when the cooperation of users decreases and the\nuncertainty about channel payoffs increases it becomes harder to achieve the\nsocially opti- mal allocation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.5985%2C1107.0235%2C1107.1264%2C1107.5126%2C1107.5743%2C1107.2685%2C1107.3802%2C1107.3624%2C1107.0435%2C1107.2600%2C1107.1974%2C1107.0371%2C1107.4428%2C1107.3750%2C1107.4354%2C1107.1010%2C1107.2598%2C1107.1162%2C1107.1933%2C1107.5704%2C1107.3871%2C1107.2875%2C1107.0468%2C1107.3265%2C1107.3040%2C1107.5657%2C1107.2068%2C1107.4545%2C1107.1790%2C1107.1614%2C1107.6004%2C1107.3579%2C1107.5583%2C1107.1914%2C1107.0562%2C1107.5330%2C1107.1984%2C1107.0509%2C1107.2665%2C1107.1207%2C1107.1083%2C1107.0481%2C1107.2442%2C1107.2678%2C1107.5196%2C1107.3563%2C1107.5707%2C1107.1823%2C1107.0655%2C1107.4075%2C1107.5849%2C1107.0591%2C1107.5103%2C1107.4641%2C1107.0750%2C1107.3532%2C1107.5361%2C1107.5639%2C1107.2468%2C1107.4541%2C1107.1185%2C1107.5695%2C1107.5128%2C1107.5037%2C1107.1486%2C1107.0173%2C1107.5344%2C1107.1584%2C1107.5055%2C1107.5049%2C1107.2336%2C1107.0764%2C1107.1968%2C1107.0988%2C1107.5901%2C1107.3870%2C1107.3673%2C1107.4939%2C1107.2458%2C1107.4153%2C1107.1278%2C1107.4269%2C1107.5294%2C1107.5673%2C1107.2705%2C1107.5961%2C1107.0514%2C1107.5808%2C1107.2235%2C1107.1312%2C1107.2582%2C1107.0829%2C1107.1084%2C1107.2805%2C1107.2359%2C1107.4676%2C1107.5683%2C1107.4434%2C1107.0568%2C1107.4664%2C1107.4241&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study the problem of allocating multiple users to a set of wireless\nchannels in a decentralized manner when the channel quali- ties are\ntime-varying and unknown to the users, and accessing the same channel by\nmultiple users leads to reduced quality due to interference. In such a setting\nthe users not only need to learn the inherent channel quality and at the same\ntime the best allocations of users to channels so as to maximize the social\nwelfare. Assuming that the users adopt a certain online learning algorithm, we\ninvestigate under what conditions the socially optimal allocation is\nachievable. In particular we examine the effect of different levels of\nknowledge the users may have and the amount of communications and cooperation.\nThe general conclusion is that when the cooperation of users decreases and the\nuncertainty about channel payoffs increases it becomes harder to achieve the\nsocially opti- mal allocation."}, "authors": ["Cem Tekin", "Mingyan Liu"], "author_detail": {"name": "Mingyan Liu"}, "author": "Mingyan Liu", "links": [{"href": "http://arxiv.org/abs/1107.4153v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.4153v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.MA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.MA", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.4153v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1107.4153v1", "arxiv_comment": null, "journal_reference": "in Proceedings of GAMENETS 2011", "doi": null, "fulltext": "Performance and Convergence of Multi-user\nOnline Learning\n\narXiv:1107.4153v1 [cs.MA] 21 Jul 2011\n\nCem Tekin and Mingyan Liu\nDepartment of Electrical Engineering and Computer Science\nUniversity of Michigan, Ann Arbor, Michigan, 48109-2122\n{cmtkn, mingyan}@umich.edu\n\nAbstract. We study the problem of allocating multiple users to a set\nof wireless channels in a decentralized manner when the channel qualities are time-varying and unknown to the users, and accessing the same\nchannel by multiple users leads to reduced quality due to interference.\nIn such a setting the users not only need to learn the inherent channel\nquality and at the same time the best allocations of users to channels\nso as to maximize the social welfare. Assuming that the users adopt a\ncertain online learning algorithm, we investigate under what conditions\nthe socially optimal allocation is achievable. In particular we examine\nthe effect of different levels of knowledge the users may have and the\namount of communications and cooperation. The general conclusion is\nthat when the cooperation of users decreases and the uncertainty about\nchannel payoffs increases it becomes harder to achieve the socially optimal allocation.\n\nKeywords: multi-user learning, multi-armed bandits, spectrum sharing, congestion games\n\n1\n\nIntroduction\n\nIn this paper we study the dynamic spectrum access and spectrum sharing problem in a learning context. Specifically, we consider a set of N common channels\nshared by a set of M users. A channel has time varying rate r(t), and its statistics are not completely known by the users. Thus each user needs to employ\nsome type of learning to figure out which channels are of better quality, e.g., in\nterms of their average achievable rates. At the same time, simultaneous use of\nthe same channel by multiple users will result in reduced rate due to interference\nor collision. The precise form of this performance degradation may or may not\nbe known to the user. Thus the users also need to use learning to avoid excess\ninterference or congestion. Furthermore, each user may have private information\nthat is not shared, e.g., users may perceive channel quality differently due to\ndifference in location as well as individual modulation/coding schemes.\nWithout a central agent, and in the presence of information decentralization\ndescribed above, we are interested in the following questions: (1) for a given\n\n\fcommon learning algorithm, does the multiuser learning process converge, and\n(2) if it does, what is the quality of the equilibrium point with respect to a\nglobally optimal spectrum allocation scheme, one that could be computed for a\nglobal objective function with full knowledge of channel statistics as well as the\nusers' private information.\nA few recent studies have addressed these questions in some special cases.\nFor instance, in [3] it was shown that learning using a sample-mean based index\npolicy leads to a socially optimal (sum of individual utilities) allocation when\nchannels evolve as iid processes and colliding players get zero reward provided\nthat this optimal allocation is such that each user occupies one of the M best\nchannels (in terms of average rates). This precludes the possibility that not all\nusers may have the same set of M best channels, and that in some cases the best\noption is for multiple users to share a common channel, e.g., when N < M .\nIn this study we investigate under what conditions the socially optimal allocation is achievable by considering different levels of communication (or cooperation) allowed among users, and different levels of uncertainty on the channel\nstatistics. The general conclusion, as intuition would suggest, is that when the\ncooperation of users increases and the channel uncertainty decreases it becomes\neasier to achieve the socially optimal welfare. Specifically, we assume that the\nrate (or reward) user i gets from channel j at time t is of the form rj (t)gj (nj (t))\nwhere rj (t) is the rate of channel j at time t, nj (t) is the number of users using\nchannel j at time t, and gj is the user independent interference function (IF) for\nchannel j. This model is richer than the previously used models [3,14,16] since\nrj (t) can represent environmental effects such as fading or primary user activity,\nwhile gj captures interactions between users. We consider the following three\ncases.\nIn the first case (C1), each channel evolves as an iid random process in time,\nthe users do not know the channel statistics, nor the form of the interference, nor\nthe total number of users present in the system, and no direct communication\nis allowed among users. A user can measure the overall rate it gets from using\na channel but cannot tell how much of it is due to the dynamically changing\nchannel quality (i.e., what it would get if it were the only user) vs. interference\nfrom other users. In this case, we show that if all users follow the Exp3 algorithm\n[7] then the channel allocation converges to a set of pure Nash equilibria (PNE)\nof a congestion game defined by the IFs and mean channel rates. In this case a\nsocially optimal allocation cannot be ensured, as the set of PNE are of different\nquality, and in some cases the socially optimal allocation may not be a PNE.\nIn the second case (C2), each channel again evolves as an iid random process\nin time, whose statistics are unknown to the user. However, the users now know\nthe total number of users in the system, as well as the fact that the quantitative\nimpact of interference is common to all users (i.e., user independent), though\nthe actual form of the interference function is unknown. In other words the rate\nof channel j at time t is perceived by user i as hj (t, nj (t)) so user i cannot\ndistinguish between components rj (t) and gj (nj (t)). Furthermore, users are now\nallowed minimal amount of communication when they happen to be in the same\n\n\fchannel, specifically to find out the total number of simultaneous users of that\nchannel. In this case we present a sample-mean based randomized learning policy\nthat achieves socially optimal allocation as time goes to infinity, with a sub-linear\nregret over the time horizon with respect to the socially optimal allocation.\nIn the third case (C3), as in case (C2) the users know the total number\nof users in the system, as well as the fact that the IF is user independent and\ndecreasing without knowing the actual form of the IF. However, the channels are\nassumed to have constant, albeit unknown, rates. We show that even without\nany communication among users, there is a randomized learning algorithm that\nachieves the socially optimal allocation in finite time.\nIt's worth pointing out that in the settings outlined above, the users are\nnon-strategic, i.e., each user simply follow a pre-set learning rule rather than\nplaying a game. In this context it is reasonable to introduce minimal amount of\ncommunication among users and assume they may cooperate. It is possible that\neven in this case the users may not know their IF but only the total rate they\nget for lack of better detecting capabilities (e.g., they may only be able to detect\nthe total received SNR as a result of channel rate and user interference).\nOnline learning by a single user was studied by [1,4,6,15], in which samplemean based index policies were shown to achieve logarithmic regret with respect\nto the best single-action policy without a priori knowledge of the statistics, and\nare order-optimal, when the rewards are given by an iid process. In [5,21,22]\nMarkovian rewards are considered, with [22] focusing on restless reward processes, where a process continues to evolve according to a Markov chain regardless of the users' actions. In all these studies learning algorithms were developed\nto achieve logarithmic regret. Multi-user learning with iid reward processes have\nbeen studied in a dynamic spectrum context by [3,11,16], with a combinatorial\nstructure adopted in [11], and with collision and random access models in [3,16].\nIn [13], convergence of multi-user learning with Exp3 algorithm to pure Nash\nequilibrium is investigated under the collision and fair sharing models. In the collision model, when there is more than one user on a channel all get zero reward,\nwhereas in the random access model one of them, selected randomly, gets all the\nreward while others get zero reward. In the fair sharing model, a user's utility is\ninversely proportional to the number of users who are on the same channel with\nthe user. Note that these models do not capture more sophisticated communication schemes where the rate a user gets is a function of the received SNR of\nPt\nthe form gj (n) = fj ( N0 +(n\u22121)P\n) = where Pt is the nominal transmit power of\nt\nall users and N0 the noise. Moreover, in the above studies the socially optimal\nallocation is a rather simple one: it is the orthogonal allocation of users to the\nfirst M channels with the highest mean rewards. By contrast, we model a more\ngeneral interference relationship among users, in which an allocation with users\nsharing the same channel may be the socially optimal one. The socially optimal\nallocations is not trivial in this case and additional mechanisms may be needed\nfor the learning algorithms to converge.\nAll of the above mentioned work assumes some level of communication between the users either at the beginning or during the learning. If we assume\n\n\fno communication between the users, achieving the socially optimal allocation\nseems very challenging in general. Then one may ask if it is possible to achieve\nsome kind of equilibrium allocation. Kleinberg et. al. [14] showed that it is possible for the case when the channel rates are constant and the users do not\nknow the IFs. They show that when the users use aggregate monotonic selection\ndynamics, a variant of Hedge algorithm [10], the allocation converges to weakly\nstable equilibria which is a subset of Nash equilibria (NE) of the congestion game\ndefined by the IFs. They show that for almost all congestion games weakly stable\nequilibria is the same as PNE.\nOther than the work described above [2] considers spatial congestion games, a\ngeneralization of congestion games and gives conditions under which there exists\na PNE and best-response play converges to PNE. A mechanism design approach\nfor socially optimal power allocation when users are strategic is considered in\n[12].\nThe organization of the remainder of this paper is as follows. In Sect. 2 we\npresent the notations and definitions that will be used throughout the paper.\nIn Sects. 3, 4, 5 we analyze the cases stated in (C1), (C2), (C3) and derive the\nresults respectively. Conclusion and future research is given in Sect. 6.\n\n2\n\nPreliminaries\n\nDenote the set of users by M = {1, 2, . . . , M }, and the set of channels N =\n{1, 2, . . . , N }. Time is slotted and indexed by t = 1, 2, . . . and a user can select\na single channel at each time step t. Without loss of generality let rj (t) \u2208 [0, 1]\nbe the rate of channel j at time t such that {rj (t)}t=1,2,... is generated by an\niid process with support [0, 1] and mean \u03bcj \u2208 [0, 1]. Let gj : N \u2192 [0, 1] be the\ninterference function (IF) on channel j where gj (n) represents the interference\nwhen there are n users on channel j. We express the rate of channel j seen by\na user as hj (t) = rj (t)gj (nj (t)) when a user does not know the total number of\nusers nj (t) using channel j at time t as in cases (C1) and (C3). When a user knows\nnj (t), we express the rate of channel j at time t as hj,nj (t) (t) = rj (t)gj (nj (t)) as\nin case (C2). Let Si = N be the set of feasible actions of user i and \u03c3i \u2208 Si be the\naction, i.e., channel selected by user i. Let S = S1 \u00d7 S2 \u00d7 . . . \u00d7 SN = N M be the\nset of feasible action profiles and \u03c3 = {\u03c31 , \u03c32 , . . . , \u03c3M } \u2208 S be the action profile\nof the users. Throughout the discussion we assume that the action of player i at\ntime t, i.e., \u03c3i\u03c0i (t) is determined by the policy \u03c0i . When \u03c0i is deterministic, \u03c0i (t)\nis in general a function from all past observations and decisions of user i to the set\nof actions Si . When \u03c0i is randomized, \u03c0i (t) generates a probability distribution\nover the set of actions Si according to all past observations and decisions of user\ni from which the action at time t is sampled. Since the dependence of actions to\nthe policy is trivial we use \u03c3i (t) to denote the action of user i at time t, dropping\nthe superscript \u03c0i .\nLet Kj (\u03c3) be the set of users on channel j when the action profile is \u03c3. Let\nPN\nPM\nA\u2217 = arg max\u03c3\u2208S i=1 \u03bc\u03c3i g\u03c3i (K\u03c3i (\u03c3)) = arg max\u03c3\u2208S j=1 \u03bcj Kj (\u03c3)gj (Kj (\u03c3))\nbe the set of socially optimal allocations and denote by \u03c3 \u2217 any action profile\n\n\f\u2217\n\u2217\n\u2217\nthat\nPM is in the set A\u2217 . Let v \u2217 denote the socially optimal welfare, i.e., v =\n\u2217\n\u2217\n\u2217\ni=1 \u03bc\u03c3i g\u03c3i (K\u03c3i (\u03c3 )) and vj denote the payoff a user gets from channel j\nunder the socially optimal allocation, i.e., vj\u2217 = \u03bcj gj (Kj (\u03c3 \u2217 )) if Kj (\u03c3 \u2217 ) 6= 0.\nNote that any permutation of actions in \u03c3 \u2217 is also a socially optimal allocation\nsince IFs are user-independent.\nFor any policy \u03c0, the regret at time n is\n#\n\" n M\nXX\n\u2217\nr\u03c3i (t) (t)g\u03c3i (t) (K\u03c3i (t) (\u03c3(t))) ,\nR(n) = nv \u2212 E\nt=1 i=1\n\nwhere expectation is taken with respect to the random nature of the rates and\nthe randomization of the policy. Note that for a deterministic policy expectation\nis only taken with respect to the random nature of the rates. For any randomized\npolicy \u03c0i , let pi (t) = (pi1 (t), pi2 (t), . . . , piN (t)) be the mixed strategy of user i at\ntime t, i.e., a probability distribution on {1, 2, . . . , N }. For a profile of policies\n\u03c0 = [\u03c01 , \u03c02 , . . . , \u03c0M ] for the users let p(t) = (p1 (t)T , p2 (t)T , . . . pM (t)T )T be\nthe profile of mixed strategies at time t, where pi (t)T is the transpose of pi (t).\nThen \u03c3i (t) is the action sampled from the probability distribution pi (t). The\ndependence of p to \u03c0 is trivial and not shown in the notation.\n\n3\n\nAllocations Achievable with Exp3 Algorithm (Case 1)\n\nWe start by defining a congestion game. A congestion game [17,18] is given by\nthe tuple (M, N , (\u03a3i )i\u2208M , (hj )j\u2208N ), where M denotes a set of players (users),\nN a set of resources (channels), \u03a3i \u2282 2N the strategy space of player i, and\nhj : N \u2192 R a payoff function associated with resource j, which is a function of\nthe number of players using that resource. It is well known that a congestion\ngame has a potential function and the local maxima of the potential function\ncorresponds to PNE, and every sequence of asynchronous improvement steps is\nfinite and converges to PNE.\nIn this section we relate the strategy update rule of Exp3 [7] under assumptions (C1) to a congestion game. Exp3 as given in Fig. 1 is a randomized algorithm consisting of an exploration parameter \u03b3 and weights wij that depend\nexponentially on the past observations where i denotes the user and j denotes\nthe channel. Each user runs Exp3 independently but we explicitly note the user\ndependence because a user's action affects other users' updates.\nAt any time step before the channel rate and user actions are drawn from\nthe corresponding distributions, let Rj denote the random variable corresponding to the reward of the jth channel. Let Gij = gj (1 + Kj\u2032 (i)) be the random\nvariable representing the payoff user i gets from channel j where Kj\u2032 (i) is the\nrandom variable representing the number of users on channel j other than user\ni. Let Uij = Rj Gij and \u016bij = Ej [E\u2212i [Uij ]] be the expected payoff to user i by\nusing channel j where E\u2212i represents the expectation taken with respect to the\nrandomization of players other than i, Ej represents the expectation taken with\nrespect to the randomization of the rate of channel j. Since the channel rate is\nindependent of users' actions \u016bij = \u03bcj \u1e21ij where \u1e21ij = E\u2212i [Gij ].\n\n\fExp3 (for user i)\n1: Initialize: \u03b3 \u2208 (0, 1), wij (t) = 1, \u2200j \u2208 N , t = 1\n2: while t > 0 do\n3:\n\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n12:\n13:\n14:\n\nwij (t)\n\u03b3\npij (t) = (1 \u2212 \u03b3) PN\n+\nN\nl=1 wil (t)\n\nSample \u03c3i (t) from the distribution on pi (t) = [pi1 (t), pi2 (t), . . . , piN (t)]\nPlay channel \u03c3i (t) and receive reward h\u03c3i (t) (t)\nfor j = 1, 2, . . . , N do\nif j = \u03c3i (t) then\n\u0011\n\u0010\n\u03b3h\u03c3i (t) (t)\npij (t)N\n\nSet wij (t + 1) = wij (t) exp\nelse\nSet wij (t + 1) = wij (t)\nend if\nend for\nt= t+1\nend while\n\nFig. 1. pseudocode of Exp3\n\nLemma 1. Under (C1) when all players use Exp3, the derivative of the continuoustime limit of Exp3 is the replicator equation given by\nN\n\n\u03beij =\n\nX\n1\n(\u03bcj pij )\npil (\u1e21ij \u2212 \u1e21il ) .\nN\nl=1\n\nProof. Note that\n\n(1 \u2212 \u03b3)wij (t) =\n\nN\nX\nl=1\n\n\u0010\n\u03b3\u0011\nwil (t) pij (t) \u2212\n.\nN\n\n(1)\n\nWe consider the effect of user i's action \u03c3i (t) on his probability update\non\n\u0011 channel\n\u0010\n\u03b3Uij (t)\n\u03b3,t\nj. We have two cases: \u03c3i (t) = j and \u03c3i (t) 6= j. Let Ai,j = exp pij (t)N .\nConsider the case \u03c3i (t) = j.\n\npij (t + 1) = PN\n\n(1 \u2212 \u03b3)wij (t)A\u03b3,t\ni,j\n\n\u03b3,t\nl=1 wil (t) + wij (t) Ai,j \u2212 1\n\n\u0001+\n\n\u03b3\n.\nN\n\n(2)\n\n\fSubstituting (1) into (2)\nPN\n\n\u0001 \u03b3,t\n\u03b3\nwil (t) pij (t) \u2212 N\nAi,j\n\u03b3\n\u0010\npij (t + 1) = P\n\u03b3\n\u0001\u0011 + N\npij (t)\u2212 N\nN\n\u03b3,t\nAi,j \u2212 1\nl=1 wil (t) 1 +\n1\u2212\u03b3\n\u0001\n\u03b3\nA\u03b3,t\npij (t) \u2212 N\n\u03b3\ni,j\n.\n=\n\u03b3\n\u0001+\npij (t)\u2212 N\n\u03b3,t\nN\nAi,j \u2212 1\n1 + 1\u2212\u03b3\nl=1\n\nThe continuous time process is obtained by taking the limit \u03b3 \u2192 0, i.e., the\nrate of change in pij with respect to \u03b3 as \u03b3 \u2192 0. Then, dropping the discrete\ntime script t,\np \u0307ij = lim\n\n\u03b3\u21920\n\ndpij\nd\u03b3\n\u0010\n\n\u0001 Uij \u03b3,t \u0011 \u0010\n\u0001\u0011\np \u2212\u03b3\n\u03b3\n1 + ij1\u2212\u03b3N A\u03b3,t\n+ pij \u2212 N\ni,j \u2212 1\npij N Ai,j\n= lim\n\u0010\n\u0001\u00112\n\u03b3\u21920\np \u2212\u03b3\n1 + ij1\u2212\u03b3N A\u03b3,t\n\u2212\n1\ni,j\n\u0010\n1\n1\n\u0001\n\u0001\u0011\np\n\u2212\np\n\u2212\n\u03b3,t\n\u03b3\nij\n\u03b3 \u03b3,t\nij\nN\nN\npij \u2212 N\nA\n+\nA\nA\u03b3,t\n2\ni,j\ni,j\ni,j\n(1\u2212\u03b3)\n1\u2212\u03b3\nN\n1\n+\n+\n\u0010\n\u03b3\n\u0001\u00112\npij \u2212 N\nN\n\u03b3,t\n1 + 1\u2212\u03b3 Ai,j \u2212 1\n\u22121 \u03b3,t\nN Ai,j\n\nUij (1 \u2212 pij )\n.\nN\nConsider the case \u03c3i (t) = k 6= j. Then,\n=\n\n(3)\n\n(1 \u2212 \u03b3)wij (t)\n\u03b3\n\u0010\n\u0011+\n\u03b3,t\nN\nl=1 wil (t) + wik (t) Ai,k \u2212 1\n\npij (t + 1) = P\nN\n=\n\nThus\n\u22121\nN\n\n\u03b3\npij (t) \u2212 N\n\u03b3\n\u0010\n\u0011+ .\n\u03b3\npik (t)\u2212 N\n\u03b3,t\nN\nAi,k \u2212 1\n1 + 1\u2212\u03b3\n\n\u0010\n1+\n\n\u03b3\npik \u2212 N\n1\u2212\u03b3\n\n\u0010\n\u0011\u0011\nA\u03b3,t\ni,k \u2212 1\n\u0010\n\u0011\u00112\nA\u03b3,t\n\u2212\n1\ni,k\n\np \u0307ij = lim \u0010\n\u03b3\n\u03b3\u21920\np \u2212N\n1 + ik\n1\u2212\u03b3\n\u0001 \u0010 pik \u2212 1 \u03b3,t pik \u2212 1 \u0010 \u03b3 \u03b3,t \u0011\u0011\n\u03b3\nN\nN\npij \u2212 N\n(1\u2212\u03b3)2 Ai,k + 1\u2212\u03b3\nN Ai,k\n1\n+\n+\n\u0010\n\u0011\u0011\n\u0010\n2\n\u03b3\nN\np \u2212N\nA\u03b3,t\n1 + ik\ni,k \u2212 1\n1\u2212\u03b3\n\npik Uik\n.\n(4)\nN\nThen from (3) and (4), the expected change in pij with respect to the probability distribution pi of user i over the channels is\nX\n1\npil (Uij \u2212 Uil ).\np\u0304ij = Ei [\u1e57ij ] = pij\nN\n=\u2212\n\nl\u2208N \u2212{j}\n\n\fTaking the expectation with respect to the randomization of channel rates and\nother users' actions we have\n\u03beij = Ej [E\u2212i [p\u0304ij ]]\nX\n1\n= pij\npil (Ej [E\u2212i [Uij ]] \u2212 Ej [E\u2212i [Uil ]])\nN\nl\u2208N \u2212{j}\nN\n\nX\n1\n= (\u03bcj pij )\npil (\u1e21ij \u2212 \u1e21il ) .\nN\nl=1\n\n\u2293\n\u2294\nLemma 1 shows that the dynamics of a user's probability distribution over the\nactions is given by a replicator equation which is commonly studied in evolutionary game theory [19,20]. With this lemma we can establish the following\ntheorem.\nTheorem 1. For all but a measure zero subset of [0, 1]2N from which the \u03bcj 's\nand gj 's are selected, when \u03b3 in Exp3 is arbitrarily small, the action profile\nconverges to the set of PNE of the congestion game (M, N , (Si )i\u2208M , (\u03bcj gj )j\u2208N ).\nProof. Because the replicator equation in Lemma 1 is identical to the replicator\nequation in [14], the proof of converge to PNE follows from [14]. Here, we briefly\nexplain the steps in the proof. Defining the expected potential function to be\nthe expected value of the potential function \u03c6 where expectation is taken with\nrespect to the user's randomization one can show that the solutions of the replicator equation converges to the set of fixed points. Then the stability analysis\nusing the Jacobian matrix yields that every stable fixed point corresponds to a\nNash equilibrium. Then one can prove that for any stable fixed point the eigenvalues of the Jacobian must be zero. This implies that every stable fixed point\ncorresponds to a weakly stable Nash equilibrium strategy in the game theoretic\nsense. Then using tools from algebraic geometry one can show that almost every\nweakly stable Nash equilibrium is a pure Nash equilibrium of the congestion\ngame.\nWe also need to investigate the error introduced by treating the discrete time\nupdate rule as a continuous time process. However, by taking \u03b3 infinitesimal\nwe can approximate the discrete time process by the continuous time process.\nFor a discussion when \u03b3 is not infinitesimal one can define approximately stable\nequilibria [14].\n\u2293\n\u2294\nThe main difference between Exp3 and Hedge [14] is that in Exp3 users\ndo not need to observe the payoffs from the channels that they do not select,\nwhereas Hedge assumes complete observation. In addition to that, we considered\nthe dynamic channel rates which is not considered in [14].\n\n\f4\n\nAn Algorithm for Socially Optimal Allocation with\nSub-linear Regret (Case 2)\n\nIn this section we propose an algorithm whose regret with respect to the so2M \u22121+2\u03b3\ncially optimal allocation is O(n 2M ) for \u03b3 > 0 arbitrarily small. Clearly\nthis regret is sublinear and approaches linear as the number of users M increases. This means that the time average of the sum of the utilities of the players converges to the socially optimal welfare. Let K = {k = (k1 , k2 , . . . , kN ) :\nkj \u2265 0, \u2200j \u2208 N , k1 + k2 + . . . + kN = M } denote an allocation of M users\nto N channels. Note that this allocation gives only the number of users on\neach channel. It does not say anything about which user uses which channel. We assume that the socially\nP optimal allocation is unique up to permutations so k \u2217 = arg maxk\u2208K N\nj=1 \u03bcj kj gj (kj ) is unique. We also assume the\nfollowing stability condition of the socially optimal allocation. Let vj (kj ) =\nPN\n\u03bcj gj (kj ). Then the stability condition says that arg maxk\u2208K j=1 kj v\u0302j (kj ) = k \u2217\nif |v\u0302j (k) \u2212 vj (k)| \u2264 \u01eb, \u2200k \u2208 {1, 2, . . . , M }, \u2200j \u2208 N , for some \u01eb > 0, where\ni\nv\u0302j : N \u2192 R is an arbitrary function. Let Tj,k\n(t) be the number of times user\ni used channel j and observed k users on it up to time t. We refer to the\ntuple (j, k) as an arm. Let nij,k (t) be the time of the tth observation of user\ni from arm (j, k). Let uij,k (t) be the sample mean of the rewards from arm\n(j, k) seen by user i at the end of the tth play of arm (j, k) by user i, i.e.,\nuij,k (t) = (hj,k (nij,k (1)) + . . . + hj,k (nij,k (t)))/t. Then the socially optimal alloPN\ncation estimated by user i at time t is k i\u2217 (t) = arg maxk\u2208K j=1 kj uij,k (t). The\npseudocode of the Randomized Learning Algorithm (RLA) is given in Fig. 2. At\n\u03b3\n1\ntime t RLA explores with probability 1/(t 2M \u2212 M ) by randomly choosing one of\n\u03b3\n1\nthe channels and exploits with probability 1 \u2212 1/(t 2M \u2212 M ) by choosing a channel\nwhich is occupied by a user in the estimated socially optimal allocation.\nThe following will be useful in the proof of the main theorem of this section.\nLemma 2. Let Xi , i = 1, 2, . . . be a sequence of independent Bernoulli\nPkrandom\nvariables such that Xi has mean qi with 0 \u2264 qi \u2264 1. Let X\u0304k = k1 i=1 Xi ,\nPk\nq\u0304k = k1 i=1 qi . Then for any constant \u01eb \u2265 0 and any integer n \u2265 0,\n\u0001\n2\nP X\u0304n \u2212 q\u0304n \u2264 \u2212\u01eb \u2264 e\u22122n\u01eb .\n\nProof. The result follows from symmetry and [9].\n\n(5)\n\u2293\n\u2294\n\nLemma 3. For p > 0, p 6= 1\nn\n\nn1\u2212p \u2212 1\n(n + 1)1\u2212p \u2212 1 X 1\n<1+\n<\np\n1\u2212p\nt\n1\u2212p\nt=1\nProof. See [8].\n\n(6)\n\u2293\n\u2294\n\n\fRLA (for user i)\ni\n1: Initialize: 0 < \u03b3 << 1, uij,k (1) = 0, Tj,k\n(1) = 0, \u2200j \u2208 N , k \u2208 M, t = 1,\nsample \u03c3i (1) uniformly from N .\n2: while t > 0 do\n3:\nplay channel \u03c3i (t), observe l(t) the total number of players using channel\n\u03c3i (t) and reward h\u03c3i (t),l(t) (t).\n4:\nSet T\u03c3i i (t),l(t) (t + 1) = T\u03c3i i (t),l(t) (t) + 1.\ni\ni\n5:\nSet Tj,l\n(t + 1) = Tj,l\n(t) for (j, l) 6= (\u03c3i (t), l(t)).\n6:\n7:\n8:\n9:\n10:\n\nSet ui\u03c3i (t),l(t) (t + 1) =\nuij,l (t\ni\u2217\n\nT\u03c3i\n\ni (t),l(t)\n\n(t)ui\u03c3\n\nuij,l (t)\n\nT\u03c3i\n\ni (t),l(t)\n\ni (t),l(t)\n\n(t)+h\u03c3i (t),l(t) (t)\n\n(t+1)\n\n.\n\nfor (j, l) 6= (\u03c3i (t), l(t)).\nPN\nSet k (t + 1) = arg maxk\u2208K j=1 kj uij,kj (t + 1).\nSet \u03b8\u2217i (t+1) to be the set of channels used by at least one user in k \u2217i (t+1).\n\nSet\n\n+ 1) =\n\nDraw it randomly from Bernoulli distribution with P (it = 1) =\n\n1\nt(1/2M )\u2212\u03b3/M\nif it = 0 then\nif \u03c3i (t) \u2208 \u03b8\u2217 (t\n\n11:\n12:\n+ 1) and l(t) = kji\u2217 (t + 1) then\n13:\n\u03c3i (t + 1) = \u03c3i (t)\n14:\nelse\n15:\n\u03c3i (t+1) is selected uniformly at random from the channels in \u03b8\u2217 (t+1).\n16:\nend if\n17:\nelse\n18:\nDraw \u03c3i (t + 1) uniformly at random from N .\n19:\nend if\n20:\nt= t+1\n21: end while\nFig. 2. pseudocode of RLA\n\nTheorem 2. When all players use RLA the regret with respect to the socially\n2M \u22121+2\u03b3\noptimal allocation is O(n 2M ) where \u03b3 can be arbitrarily small.\nProof. Let H(t) be the event that at time t there exists at least one user that\ncomputed the socially optimal allocation incorrectly. Let \u03c9 be a sample path.\nThen\nn\nX\n\nI(\u03c9 \u2208 H(t)) \u2264\n\nt=1\n\nI(k \u2217i (t) 6= k \u2217 )\n\nt=1 i=1\n\n(n,M,N,M)\n\n\u2264\n\nn X\nM\nX\n\nX\n\n(t,i,j,l)=(1,1,1,1)\n\ni\n(t)) \u2212 vj (l)| \u2265 \u01eb)\nI(|uij,l (Tj,l\n\n\f(n,M,N,M)\n\nX\n\n=\n\n(t,i,j,l)=(1,1,1,1)\n(n,M,N,M)\n\nX\n\n+\n\n(t,i,j,l)=(1,1,1,1)\n\nLet \u01ebij,k (t) =\n\nq\n\n\u0012\n\u0013\na ln t\ni\ni\nI |uij,l (Tj,l\n(t)) \u2212 vj (l)| \u2265 \u01eb, Tj,l\n(t) \u2265 2\n\u01eb\n\u0012\n\u0013\na ln t\ni\ni\ni\nI |uj,l (Tj,l (t)) \u2212 vj (l)| \u2265 \u01eb, Tj,l (t) < 2\n\u01eb\n\na ln t\ni (t) .\nTj,k\n\ni\nThen Tj,k\n(t) \u2265\n\na ln t\n\u01eb2\n\n\u21d2\u01eb\u2265\n\nq\n\na ln t\ni (t)\nTj,k\n\n(7)\n\n= \u01ebij,k (t). There-\n\nfore,\n\u0012\n\u0013\n\u0001\na ln t\ni\ni\ni\nI |uij,l (Tj,l\n(t)) \u2212 vj (l)| \u2265 \u01eb, Tj,l\n(t) \u2265 2\n\u2264 I |uij,l (Tj,l\n(t)) \u2212 vj (l)| \u2265 \u01ebij,l (t)\n\u01eb\n\u0013\n\u0012\n\u0013\n\u0012\na ln t\na\nln t\ni\ni\ni\n\u2264 I Tj,l\n(t) < 2\nI |uij,l (Tj,l\n(t)) \u2212 vj (l)| \u2265 \u01eb, Tj,l\n(t) < 2\n\u01eb\n\u01eb\nThen, continuing from (7),\nn\nX\n\nI(\u03c9 \u2208 H(t))\n\nt=1\n\n(n,M,N,M)\n\n\u2264\n\nX\n\n(t,i,j,l)=(1,1,1,1)\n\n\u0012\n\u0012\n\u0013\u0013\n\u0001\na ln t\ni\ni\ni\ni\nI |uj,l (Tj,l (t)) \u2212 vj (l)| \u2265 \u01ebj,l (t) + I Tj,l (t) < 2\n(8)\n\u01eb\n\nTaking the expectation over (8),\n\" n\n#\nX\nE\nI(\u03c9 \u2208 H(t))\nt=1\n\n(n,M,N,M)\n\n\u2264\n\nX\n\n(t,i,j,l)=(1,1,1,1)\n(n,M,N,M)\n\n+\n\nX\n\n(t,i,j,l)=(1,1,1,1)\n\n\u0001\ni\nP |uij,l (Tj,l\n(t)) \u2212 vj (l)| \u2265 \u01ebij,l (t)\nP\n\n\u0012\n\ni\nTj,l\n(t)\n\na ln t\n< 2\n\u01eb\n\n\u0013\n\n.\n\n(9)\n\nWe have\n\u0001\ni\nP |uij,l (Tj,l\n(t)) \u2212 vj (l)| \u2265 \u01ebij,l (t)\n\u0001\n\u0001\ni\ni\n= P uij,l (Tj,l\n(t)) \u2212 vj (l) \u2265 \u01ebij,l (t) + P uij,l (Tj,l\n(t)) \u2212 vj (l) \u2264 \u2212\u01ebij,l (t)\n!\n!\ni\ni\ni\ni\nSj,l\n(Tj,l\n(t))\nSj,l\n(Tj,l\n(t))\ni\ni\n\u2212 vj (l) \u2265 \u01ebj,l (t) + P\n\u2212 vj (l) \u2264 \u2212\u01ebj,l (t)\n=P\ni (t)\ni (t)\nTj,l\nTj,l\n!\n!\ni\ni\n2(Tj,l\n(t))2 (\u01ebij,l (t))2\n2Tj,l\n(t)a ln t\n2\n\u2264 2 exp \u2212\n(10)\n= 2 exp \u2212\n= 2a ,\ni\ni\nt\nTj,l (t)\nTj,l (t)\nwhere (10) follows from the Chernoff-Hoeffding inequality.\n\n\f\u0011\n\u0010\ni\ni\n(t) be the number of time\nNow we will bound P Tj,l\n(t) < a \u01ebln2 t . Let T Rj,l\nsteps in which player i played channel j and observed l users on channel j in the\ntime steps where all players randomized up to time t. Then\ni\n{\u03c9 : Tj,l\n(t) <\n\na ln t\na ln t\ni\n} \u2282 {\u03c9 : T Rj,l\n(t) < 2 },\n\u01eb2\n\u01eb\n(11)\n\nThus\n\u0013\n\u0012\n\u0013\n\u0012\na ln t\na ln t\ni\ni\n\u2264 P T Rj,l (t) < 2\n.\nP Tj,l (t) < 2\n\u01eb\n\u01eb\n\n(12)\n\ni\ni\nNow we define new Bernoulli random variables Xj,l\n(s) as follows: Xj,l\n(s) = 1\nif all players randomize at time s and player i selects channel j and observes l\ni\ni\nplayers on it according to the random draw. Xj,l\n(s) = 0 else. Then T Rj,l\n(t) =\nM\n+N\n\u2212l\u22122\nM\n\u22121\nPt\n( l\u22121 )( N \u22122 )\ni\ni\nand \u03c1s =\n\u22121\ns=1 Xj,l (s). P (Xj,l (s) = 1) = \u03c1s pl where pl =\n(MN+N\n\u22121 )\nP\nt\n1\n1\n. Let st = s=1 s(1/2)\u2212\u03b3\nThen\ns(1/2)\u2212\u03b3\n\u0012\n\u0013\na ln t\ni\nP T Rj,l\n(t) < 2\n\u01eb\n!\ni\nT Rj,l (t) pk st\na ln t pk st\n\u2212\n\u2212\n<\n=P\nt\nt\nt\u01eb2\nt\n!\ni\nT Rj,l\n(t) pk st\na ln t pk (t + 1)(1/2)+\u03b3 \u2212 1\n\u2264P\n,\n(13)\n\u2212\n\u2212\n<\nt\nt\nt\u01eb2\nt((1/2) + \u03b3)\n\nwhere (13) follows from Lemma 3. Let \u03c4 (M, N, \u01eb, \u03b3, \u03b3 \u2032 , a) be the time that for\nall k \u2208 {1, 2, . . . , M }.\n\u2032\npk (t + 1)(1/2)+\u03b3 \u2212 1 a ln t\n\u2265 t(1/2)+\u03b3 ,\n\u2212\n2\nt((1/2) + \u03b3)\nt\u01eb\n\n(14)\n\nwhere 0 < \u03b3 \u2032 < \u03b3. Then for all t \u2265 \u03c4 (M, N, \u01eb, \u03b3, \u03b3 \u2032 , a) (14) will hold since RHS\nincreases faster than LHS. Thus we have for t \u2265 \u03c4 (M, N, \u01eb, \u03b3, \u03b3 \u2032 , a)\n!\ni\nT Rj,l\n(t) pk st\na ln t pk (t + 1)(1/2)+\u03b3 \u2212 1\n\u2212\n\u2212\n<\nP\nt\nt\nt\u01eb2\nt((1/2) + \u03b3)\n!\ni\nT Rj,l\n(t) pk st\n\u2032\n\u2264P\n\u2212\n< t\u2212(1/2)+\u03b3\nt\nt\n2\u03b3 \u2032 \u22121\n\n\u2264 e\u22122tt\n\n2\u03b3 \u2032\n\n= e\u22122t\n\n\u2264 e\u22122 ln t =\n\n1\n.\nt2\n\n(15)\n\nLet a = 1. Then continuing from (9) by substituting (10) and (15) we have\n\" n\n#\n!\nn\nX\nX\n1\n2\n\u2032\nE\nI(\u03c9 \u2208 H(t)) \u2264 M N \u03c4 (M, N, \u01eb, \u03b3, \u03b3 , 1) + 3\n.\n(16)\nt2\nt=1\nt=1\n\n\fThus we proved that the expected number of time steps in which there exists\nat least one user that computed the socially optimal allocation incorrectly is\nfinite. Note that because RLA explores with probability t1/2M1\u2212\u03b3/M , the expected\nnumber of time steps in which all the players are not randomizing up to time n\nis\n\u0013M ! X\n\u0012\nn\nn\nX\n2M \u22121+2\u03b3\nM\n1\n\u2264\n= O(n 2M ). (17)\n1 \u2212 1 \u2212 (1/2M)\u2212\u03b3/M\n1/2M\u2212\u03b3/M\nt\nt\nt=1\nt=1\nNote that players can choose \u03b3 arbitrarily small, increasing the finite regret due\nto \u03c4 (M, N, \u01eb, \u03b3, \u03b3 \u2032 , 1). Thus if we are interested in the asymptotic performance\nthen \u03b3 > 0 can be arbitrarily small.\nNow we do the worst case analysis. We classify the time steps into two. Good\ntime steps in which all the players know the socially optimal allocation correctly\nand none of the players randomize excluding the randomizations done for settling\ndown to the socially optimal allocation. Bad time steps in which there exists a\nplayer that does not know the socially optimal allocation correctly or there is\na player that randomizes excluding the randomizations done for settling down\nto the socially optimal allocation. The number of Bad time steps in which there\nexists a player that does not know the socially optimal allocation correctly is\nfinite while the number of time steps in which there is a player that randomizes\nexcluding the randomizations done for settling down to the socially optimal\n2M \u22121+2\u03b3\nallocation is O(n 2M ). The worst case is when each bad step is followed by\na good step. Then from this good step the\u0012 expected number\nto settle\n\u0013 \u0012of times \u0013\n1\n/ M +z1\u2217\u22121\nwhere\n\u2217 \u22121\n(Mz+z\n( z\u2217 \u22121 )\n\u2217 \u22121 )\nz \u2217 is the number of channels which has at least one user in the socially optimal\nallocation. Assuming in the worst case the sum of the utilities of the players is\n0 when they are not playing the socially optimal allocation we have\n\ndown to the socially optimal allocation is\n\n1\u2212\nR(n) \u2264\n\n1\n\n\u2217\n\n\u22121\n(Mz+z\n\u2217 \u22121 )\n\n1\n\n\u2217\n\n\u22121\n(Mz+z\n\u2217 \u22121 )\n\n= O(n\n\n2M \u22121+2\u03b3\n2M\n\n2\n\nM N\n\n1\u2212\n\nn\nX\n1\n\u03c4 (M, N, \u01eb, \u03b3, \u03b3 , 1) + 3\nt2\nt=1\n\u2032\n\n!\n\n+ O(n\n\n2M \u22121+2\u03b3\n2M\n\n!\n\n)\n\n)\n\u2293\n\u2294\n\nNote that we mentioned earlier, under a classical multi-armed bandit problem approach as cited before [3,4,5,15,16,21,22], a logarithmic regret O(log n) is\nachievable. The fundamental difference between these studies and the problem\nin the present paper is the following: Assume that at time t user i selects channel\nj. This means that i selects to observe an arm from the set {(j, k) : k \u2208 M} but\nthe arm assigned to i is selected from this set depending on the choices of other\nplayers.\nAlso note that in RLA a user computes the socially optimal allocation according to its estimates at each time step. This could pose significant computational\n\n\feffort since integer programming is NP-hard in general. However, by exploiting\nthe stability condition on the socially optimal allocation a user may reduce the\nnumber of computations; this is a subject of future research.\n\n5\n\nAn Algorithm for Socially Optimal Allocation (Case 3)\n\nIn this section we assume that gj (n) is decreasing in n for all j \u2208 N . For\nsimplicity we assume that the socially optimal allocation is unique up to the\npermutations of \u03c3 \u2217 . When this uniqueness assumption does not hold we need a\nmore complicated algorithm to achieve the socially optimal allocation. All users\nuse the Random Selection (RS) algorithm defined in Fig. 3. RS consists of two\nphases. Phase 1 is the learning phase where the user randomizes to learn the\ninterference functions. Let Bj (t) be the set of distinct payoffs observed from\nchannel j up to time t. Then the payoffs in set Bj (t) can be ordered in a decreasing way with the associated indices {1, 2, . . . , |Bj (t)|}. Let O(Bj (t)) denote\nthis ordering. Since the IFs are decreasing, at the time |Bj (t)| = M , the user has\nlearned gj . At the time | \u222aN\nj=1 Bj (t)| = M N , the user has learned all IFs. Then,\nthe user computes A\u2217 and phase 2 of RS starts where the user randomizes to\nconverge to the socially optimal allocation.\n\nRandom Selection (RS)\n1: Initialize: t = 1, b = 0, Bj (1) = \u2205, \u2200j \u2208 N , sample \u03c3i (1) from the uniform\ndistribution on N\n2: Phase 1\n3: while b < M N do\n4:\nif h\u03c3i (t) (t) \u2208\n/ B\u03c3i (t) (t) then\n5:\nB\u03c3i (t+1) (t + 1) \u2190 O(B\u03c3i (t) (t) \u222a h\u03c3i (t) (t))\n6:\nb=b+1\n7:\nend if\n8:\nSample \u03c3i (t + 1) from the uniform distribution on N\n9:\nt= t+1\n10: end while\n11: find the socially optimal allocation \u03c3 \u2217\n12: Phase 2\n13: while b \u2265 M N do\n14:\nif h\u03c3i (t) (t) < v\u03c3\u2217i (t) then\n15:\nSample \u03c3i (t + 1) from the uniform distribution on N\n16:\nelse\n17:\n\u03c3i (t + 1) = \u03c3i (t)\n18:\nend if\n19:\nt= t+1\n20: end while\nFig. 3. pseudocode of RS\n\n\fTheorem 3. Under the assumptions of (C3) if all players use RS algorithm to\nchoose their actions, then the expected time to converge to the socially optimal\nallocation is finite.\nProof. Let TOP T denote the time the socially optimal allocation is achieved, TL\nbe the time when all users learn all the IFs, TF be the time it takes to reach the\nsocially optimal allocation after all users learn all the IFs. Then TOP T = TL +TF\nand E[TOP T ] = E[TL ] + E[TF ]. We will bound E[TL ] and E[TF ]. Let Ti be the\nfirst time that i users have learned the IFs. Let \u03c4i = Ti \u2212 Ti\u22121 , i = 1, 2, . . . , M\nand T0 = 0. Then TL = \u03c41 + . . . + \u03c4M . Define a Markov chain over all N M possible configurations of M users over N channels based on the randomization of\nthe algorithm. This Markov chain has a time dependent stochastic matrix which\nchanges at times T1 , T2 , . . . , TM . Let PT0 , PT1 , . . . , PTM denote the stochastic matrices after the times T0 , T1 , . . . , TM respectively. This Markov chain is irreducible\nat all times up to TM and is reducible with absorbing states corresponding to\nthe socially optimal allocations after TM . Let T\u03021 , T\u03022 , . . . T\u0302M be the times that\nall configurations are visited when the Markov chain has stochastic matrices\nPT0 , PT1 , . . . , PTM \u22121 respectively. Then because of irreducibility and finite states\nE[T\u0302i ] < z1 , i = 1, . . . , M for some constant z1 > 0 . Since \u03c4i \u2264 T\u0302i , i = 1, . . . , M\na.s. we have E[TL ] < M z1 . For the Markov chain with stochastic matrix PTM\nall the configurations that do not correspond to the socially optimal allocation\nare transient states. Since starting from any transient state the mean time to\nabsorption is finite E[TF ] < z2 , for some constant z2 > 0.\n\u2293\n\u2294\n\n6\n\nConclusion\n\nIn this paper we studied the decentralized multiuser resource allocation problem\nwith various levels of communication and cooperation between the users. Under\nthree different scenarios we proposed three algorithms with reasonable performance. Our future reserach will include characterization of achievable performance regions for these scenarios. For example, in case 2 we are interested in\nfinding an optimal algorithm and a lower bound on the performance.\n\nReferences\n1. Agrawal, R.: Sample Mean Based Index Policies with O(log(n)) Regret for the\nMulti-armed Bandit Problem. Advances in Applied Probability 27(4), 1054\u20131078\n(December 1995)\n2. Ahmad, S., Tekin, C., Liu, M., Southwell, R., Huang, J.: Spectrum Sharing as\nSpatial Congestion Games. http://arxiv.org/abs/1011.5384 (2010)\n3. Anandkumar, A., Michael, N., Tang, A.: Opportunistic Spectrum Access with Multiple Players: Learning under Competition. In: Proc. of IEEE INFOCOM (March\n2010)\n4. Anantharam, V., Varaiya, P., Walrand, J.: Asymptotically Efficient Allocation\nRules for the Multiarmed Bandit Problem with Multiple Plays-Part I: IID Rewards. IEEE Trans. Automat. Contr. pp. 968\u2013975 (November 1987)\n\n\f5. Anantharam, V., Varaiya, P., Walrand, J.: Asymptotically Efficient Allocation\nRules for the Multiarmed Bandit Problem with Multiple Plays-Part II: Markovian Rewards. IEEE Trans. Automat. Contr. pp. 977\u2013982 (November 1987)\n6. Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite-time Analysis of the Multiarmed\nBandit Problem. Machine Learning 47, 235\u2013256 (2002)\n7. Auer, P., Cesa-Bianchi, N., Freund, Y., Schapire, R.: The Nonstochastic Multiarmed Bandit Problem. SIAM Journal on Computing 32, 48\u201377 (2002)\n8. Chlebus, E.: An Approximate Formula for a Partial Sum of the Divergent p-series.\nApplied Mathematics Letters 22, 732\u2013737 (2009)\n9. D.W. Turner, D.M. Young, J.S.: A Kolmogorov Inequality for the Sum of Independent Bernoulli Random Variables with Unequal Means. Statistics and Probability\nLetters 23, 243\u2013245 (1995)\n10. Freund, Y., Schapire, R.: Adaptive Game Playing Using Multiplicative Weights.\nGames and Economic Behaviour 29, 79\u2013103 (1999)\n11. Gai, Y., Krishnamachari, B., Jain, R.: Learning Multiuser Channel Allocations in\nCognitive Radio Networks: a Combinatorial Multi-armed Bandit Formulation. In:\nIEEE Symp. on Dynamic Spectrum Access Networks (DySPAN) (April 2010)\n12. Kakhbod, A., Teneketzis, D.: Power Allocation and Spectrum Sharing in Cognitive\nRadio Networks With Strategic Users. In: 49th IEEE Conference on Decision and\nControl (CDC) (December 2010)\n13. Kasbekar, G., Proutiere, A.: Opportunustic Medium Access in Multi-channel Wireless Systems: A Learning Approach. In: Proceedings of the 48th Annual Allerton\nConference on Communication, Control, and Computation (September 2010)\n14. Kleinberg, R., Piliouras, G., Tardos, E.: Multiplicative Updates Outperform\nGeneric No-Regret Learning in Congestion Games. In: Annual ACM Symposium\non Theory of Computing (STOC) (2009)\n15. Lai, T., Robbins, H.: Asymptotically Efficient Adaptive Allocation Rules. Advances\nin Applied Mathematics 6, 4\u201322 (1985)\n16. Liu, K., Zhao, Q.: Distributed Learning in Multi-Armed Bandit with Multiple\nPlayers. IEEE Transactions on Signal Processing 58(11), 5667\u20135681 (November\n2010)\n17. Monderer, D., Shapley, L.S.: Potential Games. Games and Economic Behavior\n14(1), 124\u2013143 (1996)\n18. Rosenthal, R.: A Class of Games Possessing Pure-strategy Nash Equilibria. International Journal of Game Theory 2, 65\u201367 (1973)\n19. Sandholm, W.H.: Population Games and Evolutionary Dynamics. Manuscript\n(2008)\n20. Smith, J.M.: Evolution and the Theory of Games. Cambridge University Press\n(1982)\n21. Tekin, C., Liu, M.: Online Algorithms for the Multi-armed Bandit Problem with\nMarkovian Rewards. In: Proceedings of the 48th Annual Allerton Conference on\nCommunication, Control, and Computation (September 2010)\n22. Tekin, C., Liu, M.: Online Learning in Opportunistic Spectrum Access: A Restless\nBandit Approach. In: 30th IEEE International Conference on Computer Communications (INFOCOM) (April 2011)\n\n\f"}