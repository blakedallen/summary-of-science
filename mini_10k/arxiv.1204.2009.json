{"id": "http://arxiv.org/abs/1204.2009v4", "guidislink": true, "updated": "2014-06-17T12:03:42Z", "updated_parsed": [2014, 6, 17, 12, 3, 42, 1, 168, 0], "published": "2012-04-09T23:31:39Z", "published_parsed": [2012, 4, 9, 23, 31, 39, 0, 100, 0], "title": "Effects of the LLL reduction on the success probability of the Babai\n  point and on the complexity of sphere decoding", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.2086%2C1204.0522%2C1204.1033%2C1204.3393%2C1204.4178%2C1204.1036%2C1204.4232%2C1204.1750%2C1204.2411%2C1204.6605%2C1204.0821%2C1204.3558%2C1204.3026%2C1204.4877%2C1204.6326%2C1204.0342%2C1204.4979%2C1204.4957%2C1204.4073%2C1204.0506%2C1204.4218%2C1204.1584%2C1204.6234%2C1204.0170%2C1204.5483%2C1204.1235%2C1204.0004%2C1204.2429%2C1204.1907%2C1204.1049%2C1204.2009%2C1204.4541%2C1204.6054%2C1204.3738%2C1204.1392%2C1204.2622%2C1204.6111%2C1204.4511%2C1204.4743%2C1204.2684%2C1204.0862%2C1204.0161%2C1204.0258%2C1204.6571%2C1204.0770%2C1204.6371%2C1204.0729%2C1204.5452%2C1204.5808%2C1204.0370%2C1204.2419%2C1204.0168%2C1204.1589%2C1204.3651%2C1204.5082%2C1204.1302%2C1204.6449%2C1204.5458%2C1204.2502%2C1204.1534%2C1204.6497%2C1204.2151%2C1204.4464%2C1204.2526%2C1204.2471%2C1204.3724%2C1204.0832%2C1204.3032%2C1204.4273%2C1204.2877%2C1204.4798%2C1204.0391%2C1204.0255%2C1204.6627%2C1204.5438%2C1204.5919%2C1204.4027%2C1204.3180%2C1204.1404%2C1204.4771%2C1204.4375%2C1204.5990%2C1204.5800%2C1204.3044%2C1204.6016%2C1204.6603%2C1204.1735%2C1204.5413%2C1204.2868%2C1204.6212%2C1204.5726%2C1204.0050%2C1204.5195%2C1204.3573%2C1204.6062%2C1204.4943%2C1204.4414%2C1204.5408%2C1204.5142%2C1204.2511%2C1204.1962&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Effects of the LLL reduction on the success probability of the Babai\n  point and on the complexity of sphere decoding"}, "summary": "The common method to estimate an unknown integer parameter vector in a linear\nmodel is to solve an integer least squares (ILS) problem. A typical approach to\nsolving an ILS problem is sphere decoding. To make a sphere decoder faster, the\nwell-known LLL reduction is often used as preprocessing. The Babai point\nproduced by the Babai nearest plan algorithm is a suboptimal solution of the\nILS problem. First we prove that the success probability of the Babai point as\na lower bound on the success probability of the ILS estimator is sharper than\nthe lower bound given by Hassibi and Boyd [1]. Then we show rigorously that\napplying the LLL reduction algorithm will increase the success probability of\nthe Babai point. Finally we show rigorously that applying the LLL reduction\nalgorithm will also reduce the computational complexity of sphere decoders,\nwhich is measured approximately by the number of nodes in the search tree in\nthe literature", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.2086%2C1204.0522%2C1204.1033%2C1204.3393%2C1204.4178%2C1204.1036%2C1204.4232%2C1204.1750%2C1204.2411%2C1204.6605%2C1204.0821%2C1204.3558%2C1204.3026%2C1204.4877%2C1204.6326%2C1204.0342%2C1204.4979%2C1204.4957%2C1204.4073%2C1204.0506%2C1204.4218%2C1204.1584%2C1204.6234%2C1204.0170%2C1204.5483%2C1204.1235%2C1204.0004%2C1204.2429%2C1204.1907%2C1204.1049%2C1204.2009%2C1204.4541%2C1204.6054%2C1204.3738%2C1204.1392%2C1204.2622%2C1204.6111%2C1204.4511%2C1204.4743%2C1204.2684%2C1204.0862%2C1204.0161%2C1204.0258%2C1204.6571%2C1204.0770%2C1204.6371%2C1204.0729%2C1204.5452%2C1204.5808%2C1204.0370%2C1204.2419%2C1204.0168%2C1204.1589%2C1204.3651%2C1204.5082%2C1204.1302%2C1204.6449%2C1204.5458%2C1204.2502%2C1204.1534%2C1204.6497%2C1204.2151%2C1204.4464%2C1204.2526%2C1204.2471%2C1204.3724%2C1204.0832%2C1204.3032%2C1204.4273%2C1204.2877%2C1204.4798%2C1204.0391%2C1204.0255%2C1204.6627%2C1204.5438%2C1204.5919%2C1204.4027%2C1204.3180%2C1204.1404%2C1204.4771%2C1204.4375%2C1204.5990%2C1204.5800%2C1204.3044%2C1204.6016%2C1204.6603%2C1204.1735%2C1204.5413%2C1204.2868%2C1204.6212%2C1204.5726%2C1204.0050%2C1204.5195%2C1204.3573%2C1204.6062%2C1204.4943%2C1204.4414%2C1204.5408%2C1204.5142%2C1204.2511%2C1204.1962&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The common method to estimate an unknown integer parameter vector in a linear\nmodel is to solve an integer least squares (ILS) problem. A typical approach to\nsolving an ILS problem is sphere decoding. To make a sphere decoder faster, the\nwell-known LLL reduction is often used as preprocessing. The Babai point\nproduced by the Babai nearest plan algorithm is a suboptimal solution of the\nILS problem. First we prove that the success probability of the Babai point as\na lower bound on the success probability of the ILS estimator is sharper than\nthe lower bound given by Hassibi and Boyd [1]. Then we show rigorously that\napplying the LLL reduction algorithm will increase the success probability of\nthe Babai point. Finally we show rigorously that applying the LLL reduction\nalgorithm will also reduce the computational complexity of sphere decoders,\nwhich is measured approximately by the number of nodes in the search tree in\nthe literature"}, "authors": ["Xiao-Wen Chang", "Jinming Wen", "Xiaohu Xie"], "author_detail": {"name": "Xiaohu Xie"}, "author": "Xiaohu Xie", "arxiv_comment": "IEEE Transactions on Information Theory, 2013", "links": [{"href": "http://arxiv.org/abs/1204.2009v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.2009v4", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.2009v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.2009v4", "journal_reference": null, "doi": null, "fulltext": "1\n\nEffects of the LLL reduction on the success\nprobability of the Babai point and on the\ncomplexity of sphere decoding\n\narXiv:1204.2009v4 [cs.IT] 17 Jun 2014\n\nXiao-Wen Chang, Jinming Wen, and Xiaohu Xie\n\nAbstract-A common method to estimate an unknown\ninteger parameter vector in a linear model is to solve an\ninteger least squares (ILS) problem. A typical approach to\nsolving an ILS problem is sphere decoding. To make a sphere\ndecoder faster, the well-known LLL reduction is often used\nas preprocessing. The Babai point produced by the Babai\nnearest plane algorithm is a suboptimal solution of the ILS\nproblem. First we prove that the success probability of the\nBabai point as a lower bound on the success probability of\nthe ILS estimator is sharper than the lower bound given by\nHassibi and Boyd [1]. Then we show rigorously that applying the LLL reduction algorithm will increase the success\nprobability of the Babai point and give some theoretical and\nnumerical test results. We give examples to show that unlike\nLLL's column permutation strategy, two often used column\npermutation strategies SQRD and V-BLAST may decrease\nthe success probability of the Babai point. Finally we show\nrigorously that applying the LLL reduction algorithm will\nalso reduce the computational complexity of sphere decoders,\nwhich is measured approximately by the number of nodes\nin the search tree in the literature.\nIndex Terms-Integer least squares (ILS) problem, sphere\ndecoding, LLL reduction, success probability, Babai point,\ncomplexity.\n\nC\n\nI. I NTRODUCTION\nONSIDER the following linear model:\ny = Ax\u0302 + v,\n\n(1)\n\nwhere y \u2208 Rm is an observation vector, A \u2208 Rm\u00d7n\nis a deterministic model matrix with full column rank,\nx\u0302 \u2208 Zn is an unknown integer parameter vector, and\nv \u2208 Rm is a noise vector following the Gaussian distribution N (0, \u03c3 2 I) with \u03c3 being known. A common method\nto estimate x\u0302 in (1) is to solve the following integer least\nsquares (ILS) problem:\nmin ky \u2212 Axk22 ,\n\nx\u2208Z n\n\n(2)\n\nX.-W. Chang is with The School of Computer Science, McGill University, Montreal, QC H3A 2A7, Canada (e-mail: chang@cs.mcgill.ca).\nJinming Wen is with The Department of Mathematics and Statistics, McGill University, Montreal, QC H3A 0B9, Canada (e-mail: jinming.wen@mail.mcgill.ca).\nXiaohu Xie is with The School of Computer Science,\nMcGill University, Montreal, QC H3A 2A7, Canada (e-mail:\nxiaohu.xie@mail.mcgill.ca).\nCopyright (c) 2012 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes\nmust be obtained from the IEEE by sending a request to pubspermissions@ieee.org.\n\nwhose solution xILS is the maximum-likelihood estimator\nof x\u0302. The ILS problem is also referred to as the closest\npoint problem in the literature as it is equivalent to find a\npoint in the lattice {Ax : x \u2208 Zn } which is closest to y.\nA typical approach to solving (2) is the discrete search\napproach, referred to as sphere decoding in communications, such as the Schnorr-Euchner algorithm [2] or its\nvariants, see e.g. [3], [4]. To make the search faster, a lattice reduction is performed to transform the given problem\nto an equivalent problem. A widely used reduction is the\nLLL reduction proposed by Lenstra, Lenstra and Lov\u00e1sz\nin [5].\nIt has been shown that the ILS problem is NP-hard\n[6], [7]. Solving (2) may become time-prohibitive when\nA is ill conditioned, the noise is large, or the dimension\nof the problem is large [8]. So for some applications, an\napproximate solution, which can be produced quickly, is\ncomputed instead. One often used approximate solution\nis the Babai point, produced by Babai's nearest plane\nalgorithm [9]. This approximate solution is also the first\ninteger point found by the Schnorr-Euchner algorithm. In\ncommunications, a method for finding this approximate\nsolution is referred to as a successive interference cancelation decoder.\nIn order to verify whether an estimator is good enough\nfor a practical use, one needs to find the probability of\nthe estimator being equal to the true integer parameter\nvector, which is referred to as success probability [1].\nThe probability of wrong estimation is referred to as error\nprobability, see, e.g., [10].\nIf the Babai point is used as an estimator of the integer\nparameter vector x\u0302 in (1), certainly it is important to find\nits success probability, which can easily be computed.\nEven if one intends to compute the ILS estimator, it is\nstill important to find the success probability of the Babai\npoint. It is very difficult to compute the success probability\nof the ILS estimator, so lower and upper bounds have\nbeen considered to approximate it, see, e.g., [1], [11]. In\n[12] it was shown that the success probability of the ILS\nestimator is the largest among all \"admissible\" estimators,\nincluding the Babai point, which is referred to as a\nbootstrapping estimator in [12]. The success probability\nof the Babai point is often used as an approximation to\nthe success probability of the ILS estimator. In general, the\n\n\f2\n\nhigher the success probability of the Babai point, the lower\nthe complexity of finding the ILS estimator by the discrete\nsearch approach. In practice, if the success probability of\nthe Babai point is high, say close to 1, then one does not\nneed to spend extra computational time to find the ILS\nestimator.\nNumerical experiments have shown that after the LLL\nreduction, the success probability of the Babai point increases [13]. But whether the LLL reduction can always\nimprove the success probability of the Babai point is still\nunknown. In this paper, we will prove that the success\nprobability of the Babai point will become higher after\nthe LLL reduction algorithm is used. It is well-known that\nthe LLL reduction can make sphere decoders faster. But to\nour knowledge there is still no rigorous justification. We\nwill show that the LLL reduction can always decrease the\ncomputational complexity of sphere decoders, an approximation to the number of nodes in the search tree given\nin the literature.\nThe rest of the paper is organized as follows. In section\nII, we introduce the LLL reduction to reduce the ILS\nprobelm (2). In section III, we introduce the Babai point\nand a formula to compute the success probability of the\nBabai point, and we show that the success probability\nof the Babai point is a sharper lower bound on the\nsuccess probability of ILS estimator compared with the\nlower bound given in [1]. In section IV, we rigorously\nprove that the LLL reduction algorithm improves the\nsuccess probability of the Babai point. In section V, we\nrigorously show that the LLL reduction algorithm reduces\nthe computational complexity of sphere decoders. Finally\nwe summarize this paper in section VI.\nIn this paper, ek denotes the k-th column of the identity\nmatrix I. For x \u2208 Rn , we use \u230ax\u2309 to denote its nearest\ninteger vector, i.e., each entry of x is rounded to its nearest\ninteger (if there is a tie, the one with smaller magnitude\nis chosen). For a vector x, xi:j denotes the subvector\nof x formed by entries i, i + 1, . . . , j. For a matrix A,\nAi:j,i:j denotes the submatrix of A formed by rows and\ncolumns i, i + 1, . . . , j. The success probabilities of the\nBabai point and the ILS estimator are denoted by PB and\nPILS , respectively.\nII. LLL R EDUCTION AND TRANSFORMATION\nILS P ROBLEM\n\nOF THE\n\nAssume that A in the linear model (1) has the QR\nfactorization\n\u0014 \u0015\nR\nA = [Q1 , Q2 ]\n,\n0\nwhere [Q1 , Q2 ] \u2208 R\nn\n\nm\u2212n\n\nm\u00d7m\n\nis orthonormal and R \u2208 R\n\nn\u00d7n\n\nis upper triangular. Without loss of generality, we assume\nthe diagonal entries of R are positive throughout the paper.\nDefine \u1ef9 = QT1 y. From (1), we have \u1ef9 = Rx\u0302+QT1 v. Because v \u223c N (0, \u03c3 2 I), it follows that \u1ef9 \u223c N (Rx\u0302, \u03c3 2 I).\n\nWith the QR factorization of A, the ILS problem (2)\ncan be transformed to\nmin k\u1ef9 \u2212 Rxk22 .\n\nx\u2208Z n\n\n(3)\n\nOne can then apply a sphere decoder such as the SchnorrEuchner search algorithm [2] to find the solution of (3).\nThe efficiency of the search process depends on R. For\nefficiency, one typically uses the LLL reduction instead of\nthe QR factorization. After the QR factorization of A, the\nLLL reduction [5] reduces the matrix R in (3) to R\u0304:\nT\n\nQ\u0304 RZ = R\u0304,\n\n(4)\n\nwhere Q\u0304 \u2208 Rn\u00d7n is orthonormal, Z \u2208 Zn\u00d7n is a\nunimodular matrix (i.e., det(Z) = \u00b11), and R\u0304 \u2208 Rn\u00d7n is\nupper triangular with positive diagonal entries and satisfies\nthe following conditions:\n1\nr\u0304ii , i = 1, 2, . . . , k \u2212 1\n2\n2\n2\n2\n\u03b4r\u0304k\u22121,k\u22121\n\u2264 r\u0304k\u22121,k\n+ r\u0304kk\n, k = 2, 3, . . . , n,\n\n|r\u0304ik | \u2264\n\n(5)\n(6)\n\nwhere \u03b4 is a constant satisfying 1/4 < \u03b4 \u2264 1. The matrix\nR is said to be \u03b4-LLL reduced or simply LLL reduced.\nEquations (5) and (6) are referred to as the size-reduced\ncondition and the Lov\u00e1sz condition, respectively.\nThe original LLL algorithm given in [5] can be described in the matrix language. Two types of basic unimodular matrices are implicitly used to update R so\nthat it satisfies the two conditions. One is the integer\nGauss transformations (IGT) matrices and the other is\npermutation matrices, see below.\nTo meet the first condition in (5), we can apply an IGT,\nwhich has the following form:\nZ ik = I \u2212 \u03b6ei eTk .\nApplying Z ik (i < k) to R from the right gives\nR\u0304 = RZ ik = R \u2212 \u03b6Rei eTk .\nThus R\u0304 is the same as R, except that r\u0304jk = rjk \u2212 \u03b6rji\nfor j = 1, . . . , i. By setting \u03b6 = \u230arik /rii \u2309, we ensure\n|r\u0304ik | \u2264 r\u0304ii /2.\nTo meet the second condition in (6) permutations\nare needed in the reduction process. Suppose that\n2\n2\n2\n\u03b4 rk\u22121,k\u22121\n> rk\u22121,k\n+ rk,k\nfor some k. Then we interchange columns k \u2212 1 and k of R. After the permutation\nthe upper triangular structure of R is no longer maintained.\nBut we can bring R back to an upper triangular matrix by\nusing the Gram-Schmidt orthogonalization technique (see\n[5]) or by a Givens rotation:\nR\u0304 = GTk\u22121,k RP k\u22121,k ,\n\n(7)\n\nwhere Gk\u22121,k is an orthonormal matrix and P k\u22121,k is a\npermutation matrix, and\n2\n2\n2\nr\u0304k\u22121,k\u22121\n= rk\u22121,k\n+ rk,k\n,\n2\n2\n2\nr\u0304k\u22121,k\n+ r\u0304k,k\n= rk\u22121,k\u22121\n.\n\n(8)\n\n\f3\n\n2\nNote that the above operation guarantees \u03b4 r\u0304k\u22121,k\u22121\n<\n2\n2\nr\u0304k\u22121,k + r\u0304k,k since \u03b4 \u2264 1. The LLL reduction algorithm\nis described in Algorithm 1, where the final reduced upper\ntriangular matrix is still denoted by R.\n\nAlgorithm 1 LLL reduction\n1:\n2:\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n12:\n13:\n14:\n15:\n16:\n17:\n\n\u0014 \u0015\nR\ncompute the QR factorization: A = Q\n;\n0\nset Z = I n , k = 2;\nwhile k \u2264 n do\napply IGT Z k\u22121,k to reduce rk\u22121,k :\nR = RZ k\u22121,k ;\nupdate Z: Z = ZZ k\u22121,k ;\n2\n2\n2\nif \u03b4 rk\u22121,k\u22121\n> rk\u22121,k\n+ rkk\nthen\npermute and triangularize R:\nR = GTk\u22121,k RP k\u22121,k ;\nupdate Z: Z = ZP k\u22121,k ;\nk = k \u2212 1, when k > 2;\nelse\nfor i = k \u2212 2, . . . , 1 do\napply IGT Z ik to reduce rik : R = RZ ik ;\nupdate Z: Z = ZZ i,k ;\nend for\nk = k + 1;\nend if\nend while\n\nPB = Pr(xB = x\u0302) = P\n\u00d7\n\nn\u22121\nY\ni=1\n\n\u0001\nB\n(xB\ni = x\u0302i ) = Pr(xn = x\u0302n )\n\nB\nB\nPr(xB\ni = x\u0302i |xi+1 = x\u0302i+1 , * * * , xn = x\u0302n ).\n\n(12)\n\nSince \u1ef9 \u223c N (Rx\u0302, \u03c3 2 I), we have\n\n\u1ef9n \u223c N (rnn x\u0302n , \u03c3 2 ),\nn\nX\nrij x\u0302j , \u03c3 2 ),\n\u1ef9i \u223c N (rii x\u0302i +\nj=i+1\n\ni = n \u2212 1, . . . , 1.\n\nThus, from (10) we have\n2\ncn \u223c N (x\u0302n , \u03c3 2 /rnn\n),\nB\nand if xB\ni+1 = x\u0302i+1 , * * * , xn = x\u0302n ,\n\n(9)\n\nz\u2208Z\n\nn\n\\\n\ni=1\n\nAfter the LLL reduction (4), the ILS problem (3) is then\ntransformed to:\nminn k\u0233 \u2212 R\u0304zk22 ,\n\nTheorem 1: Suppose \u1ef9 \u223c N (Rx\u0302, \u03c3 2 I) in the ILS\nproblem (3). Let PB denotes the success probability of\nthe Babai point xB given in (10), i.e., PB = Pr(xB = x\u0302).\nThen\nr Z \u03b6/(2\u03c3)\nn\nY\n2\n1\n\u03c6(rii ), \u03c6(\u03b6) =\nPB =\nexp(\u2212 t2 )dt.\n\u03c0\n2\n0\ni=1\n(11)\nProof. By the chain rule of conditional probabilities:\n\nT\n\nwhere \u0233 = Q\u0304 \u1ef9 and z = Z \u22121 x.\nThe LLL reduction is a powerful preprocessing tool\nthat allows to reduce the complexity of search process\nfor finding the ILS solution, see, e.g., [1], [3].\n\n2\nci \u223c N (x\u0302i , \u03c3 2 /rii\n).\n\nThen it follows that\nPr(xB\nn = x\u0302n ) = Pr(|cn \u2212 x\u0302n | \u2264 1/2)\nZ 0.5\n1\nt2\n= \u221a\nexp(\u2212\n\u03c3 2 )dt\n\u03c3\n)\n2( rnn\n2\u03c0 rnn\n\u22120.5\nZ rnn /(2\u03c3)\n2\n1\n= \u221a\nexp(\u2212 t2 )dt = \u03c6(rnn ).\n2\n2\u03c0 0\nSimilarly, we can obtain\n\nIII. S UCCESS P ROBABILITY\n\nOF THE\nA LOWER BOUND\n\nBABAI\n\nPOINT AND\n\nThe Babai (integer) point xB \u2208 Zn found by the Babai\nnearest plane algorithm [9] is defined as follows:\ncn = \u1ef9n /rnn , xB\nn = \u230acn \u2309,\nn\nX\nrij xB\nci = (\u1ef9i \u2212\nj )/rii ,\nj=i+1\n\nxB\ni = \u230aci \u2309,\n\n(10)\n\nfor i = n \u2212 1, . . . , 1. Note that the entries of xB are\ndetermined from the last to the first. The Babai point xB\nis actually the first integer point found by the SchnorrEuchner search algorithm [2] for solving (3).\nIn the following we give a formula for the success\nprobability of the Babai point. The formula is equivalent\nto the one given by Teunissen in [14], which considers\na variant form of the ILS problem (2). But our proof is\neasier to follow than that given in [14].\n\nB\nB\nPr(xB\ni = x\u0302i |xi+1 = x\u0302i+1 , * * * , xn = x\u0302n ) = \u03c6(rii ).\n\nThen from (12) we can conclude that (11) holds.\n\n\u0003\n\nSince PB in (11) depends on R, sometimes we also\nwrite PB as PB (R).\nThe success probability PILS of the ILS estimator depends on its Voronoi cell [1] and it is difficult to compute\nit because the shape of Voronoi cell is complicated. In [1]\na lower bound F (d2min /(4\u03c3 2 ), n) is proposed to approximate it, where dmin is the length of the shortest lattice\nvector, i.e., dmin = min06=x\u2208Zn kRxk2 , and F is the\ncumulative distribution function of chi-square distribution.\nHowever, no polynomial-time algorithm has been found\nto compute dmin . To overcome this problem, [1] proposed\n2\na more practical lower bound F (rmin\n/(4\u03c3 2 ), n), where\nrmin \u2261 mini rii . Note that PB is also a lower bound on\nPILS (see [12]). The following result shows that PB is\n2\nsharper than F (rmin\n/(4\u03c3 2 ), n).\n\n\f4\n\n\u0011\n\u0010 2\nrmin\n\u2264 PB .\nTheorem 2: F 4\u03c3\n2 ,n\nProof.\nLet\nu\n\u223c\nN\n(0,\nI\n).\nThus u1 , u2 , . . . , un are i.i.d.\nn\nPn\n2\nand\nu\nfollows\nthe\nchi-squared\ndistribution with\ni=1 i\nPn\n2\ndegree n. Let events E = { i=1 u2i \u2264 rmin\n/(4\u03c3 2 )}\n2\n2\n2\nand Ei = {ui \u2264T rii /(4\u03c3 )} for i = 1, 2, . . . , n. Since\nrmin \u2264 rii , E \u2286 ni=1 Ei . Thus,\nF\n\n\u0010 r2\n\nmin\n,n\n4\u03c3 2\n\n\u0011\n\n= Pr(E) \u2264 Pr(\n\n=\n=\n\nn\nY\n\ni=1\nn\nY\n\n1\n\u221a\n2\u03c0\n\nn\n\\\n\nEi )\ni=1\nrii /(2\u03c3)\n\n=\n\nn\nY\n\nPr(Ei )\n\ni=1\n\n1 \u0001\nexp \u2212 t2 dt\n2\n\u2212rii /(2\u03c3)\n\nZ\n\n\u03c6(rii ) = PB .\n\n2\n2\n2\nLemma 1: Suppose that \u03b4 rk\u22121,k\u22121\n> rk\u22121,k\n+ rkk\nfor\nsome k for the R matrix in the ILS problem (3). After the\npermutation of columns k \u2212 1 and k and triangularization,\nR becomes R\u0304, i.e., R\u0304 = GTk\u22121,k RP k\u22121,k (see (7)). With\n\u0233 = GTk\u22121,k \u1ef9 and z = P \u22121\nk\u22121,k x, (3) can be transformed\n\u22121\nto (9). Denote \u1e91 \u2261 P k\u22121,k\nx\u0302. Then the Babai point z B has\na success probability greater than or equal to the Babai\npoint xB , i.e.,\n\n\u0003\n\nPr(xB = x\u0302) \u2264 Pr(z B = \u1e91),\n\nwhere the equality holds if and only if rk\u22121,k = 0.\nProof. By Theorem 1, what we need to show is the\nfollowing inequality:\nn\nY\n\ni=1\n\nIn the following, we give an example to show that\n2\nF (rmin\n/(4\u03c3 2 ), n) can be \u0014much smaller\n\u0015 than PB .\n0.001 0\nExample 1: Let R =\nand \u03c3 = 0.5. By\n0\n10\n2\nsimple calculations, we obtain F (rmin /(4\u03c3 2 ), n)/PB =\n1/1596. Although this is a contrived example, where the\nsignal-to-noise ratio is small, it shows that PB can be much\n2\nsharper than F (rmin\n/(4\u03c3 2 ), n) as a lower bound on PILS .\nIV. E NHANCEMENT\n\nOF\n\nPB\n\nBY THE\n\nLLL\n\nREDUCTION\n\nIn this section we rigorously prove that column permutations and size reductions in the LLL reduction process\ngiven in Algorithm 1 enhance (not strictly) the success\nprobability P B of the Babai point. We give simulations to\nshow that unlike LLL's column permutation strategy, two\noften used column permutation strategies SQRD [15] and\nV-BLAST [16] may decrease the success probability of\nthe Babai point. We will also discuss how the parameter \u03b4\naffects the enhancement and give some upper bounds on\nP B after the LLL reduction.\nA. Effects of the LLL reduction on PB\nSuppose that we have the QRZ factorization (4), where\nQ\u0304 is orthonormal, Z is unimodular and R\u0304 is upper\ntriangular with positive diagonal entries (we do not assume\nthat R\u0304 is LLL reduced unless we state otherwise). Then\nT\nwith \u0233 = Q\u0304 \u1ef9 and z = Z \u22121 x the ILS problem (3)\ncan be transformed to (9). For (9) we can also define its\ncorresponding Babai point z B . This Babai point can be\nused as an estimator of \u1e91 \u2261 Z \u22121 x\u0302, or equivalently Zz B\ncan be used an estimator of x\u0302. In (3) \u1ef9 \u223c N (Rx\u0302, \u03c3 2 I).\nIt is easy to verify that in (9) \u0233 \u223c N (R\u0304\u1e91, \u03c3 2 I). In the\nfollowing we look at how the success probability of the\nBabai point changes after some specific transformation is\nused to R.\nThe following result shows that if the Lov\u00e1sz condition\n(6) is not satisfied, after a column permutation and triangularization, the success probability of the Babai point\nincreases.\n\n(13)\n\ni=1\n\n\u03c6(rii ) \u2264\n\nn\nY\n\n\u03c6(r\u0304ii ).\n\n(14)\n\ni=1\n\nSince r\u0304ii = rii for i 6= k \u2212 1, k, we only need to show\n\u03c6(rk\u22121,k\u22121 )\u03c6(rkk ) \u2264 \u03c6(r\u0304k\u22121,k\u22121 )\u03c6(r\u0304kk ),\nwhich is equivalent to\nZ r2\u03c3\nZ rk\u22121,k\u22121\nkk\n2\u03c3\n1 2\n1\nexp(\u2212 t )dt\nexp(\u2212 t2 )dt\n2\n2\n0\n0\nZ r\u03042\u03c3\nZ r\u0304k\u22121,k\u22121\nkk\n2\u03c3\n1\n1\nexp(\u2212 t2 )dt\nexp(\u2212 t2 )dt.\n\u2264\n2\n2\n0\n0\n\n(15)\n\nSince Gk\u22121,k is orthonormal and P k\u22121,k is a permutation matrix, the absolute value of the determinant of the\nsubmatrix Rk\u22121:k,k\u22121:k is unchanged, i.e., we have\nrk\u22121,k\u22121 rkk = r\u0304k\u22121,k\u22121 r\u0304kk .\n\n(16)\n\nLet\nrk\u22121,k\u22121 rkk\nr\u0304k\u22121,k\u22121 r\u0304kk\n=\n,\n(17)\n2\u03c3\n2\u03c3\n2\u03c3\n2\u03c3\nZ a/\u03b6\nZ \u03b6\n1\n1\nexp(\u2212 t2 )dt.\nexp(\u2212 t2 )dt + ln\nf (\u03b6) = ln\n2\n2\n0\n0\n(18)\n\na=\n\nNote that f (\u03b6) = f (a/\u03b6) = f (max{\u03b6, a/\u03b6}). Then (15)\nis equivalent to\n\u0011\n\u0010 max{r\u0304\n\u0011\n\u0010 max{r\nk\u22121,k\u22121 , r\u0304kk }\nk\u22121,k\u22121 , rkk }\n\u2264f\n.\nf\n2\u03c3\n2\u03c3\n(19)\nObviously, if rk\u22121,k = 0, then the equality in (19) holds\nsince in this case\nmax{rk\u22121,k\u22121 , rkk }\nmax{r\u0304k\u22121,k\u22121 , r\u0304kk }\n=\n.\n2\u03c3\n2\u03c3\nSo we only need to show if rk\u22121,k 6= 0, then the strict\ninequality in (19) holds. In the following, we assume\nrk\u22121,k 6= 0.\n2\n2\n2\nFrom \u03b4rk\u22121,k\u22121\n> rk\u22121,k\n+ rkk\nand (8) we can\nconclude that\nrkk , r\u0304k\u22121,k\u22121 , r\u0304kk < rk\u22121,k\u22121 .\n\n\f5\n\nThen, with (17) it follows that\nrk\u22121,k\u22121\nmax{rk\u22121,k\u22121 , rkk }\n=\n2\u03c3\n2\u03c3\nmax{r\u0304k\u22121,k\u22121 , r\u0304kk } \u221a\n\u2265 a.\n>\n2\u03c3\nThus, to show the strict \u221a\ninequality in (19) holds, it suffices\nto show that when \u03b6 > a, f (\u03b6) is a strict monotonically\ndecreasing function or equivalently f \u2032 (\u03b6) < 0.\nFrom (18),\n\u2032\n\nexp(\u2212 12 \u03b6 2 )\n\n2\n\n(a/\u03b6)\na\n)\n\u03b6 2 exp(\u2212\n2\nR a/\u03b6\nexp(\u2212 21 t2 )dt\n0\n\n\u2212\nexp(\u2212 12 t2 )dt\n\u0012\n\u0010 a \u0011\u0013\n1\n=\ng(\u03b6) \u2212 g\n,\n\u03b6\n\u03b6\n\nf (\u03b6) = R \u03b6\n0\n\n\u221a\na, \u03b6 > a/\u03b6.\n\u221a\nThus, in order to show f (\u03b6) < 0 for \u03b6 > a, we need\nonly to show that g(\u03b6) is a strict monotonically decreasing\nfunction or equivalently g \u2032 (\u03b6) < 0 when \u03b6 > 0.\nSimple calculations give\nwhere g(\u03b6) =\n\n\u03b6 exp(\u2212 12 \u03b6 2 )\nR\u03b6\n.\nexp(\u2212 21 t2 )dt\n0\n\u2032\n\nNote that \u03b6 >\n\nexp(\u2212 21 \u03b6 2 )\ng \u2032 (\u03b6) = R \u03b6\n( 0 exp(\u2212 12 t2 )dt)2\n\"\nZ\n\u03b6\n\n2\n\n\u00d7 (1 \u2212 \u03b6 )\n\n0\n\n#\n1 2\n1 2\nexp(\u2212 t )dt \u2212 \u03b6 exp(\u2212 \u03b6 ) .\n2\n2\n\nIf 1 \u2212 \u03b6 2 \u2264 0 and \u03b6 > 0, then obviously g \u2032 (\u03b6) < 0. If\n1 \u2212 \u03b6 2 > 0 and \u03b6 > 0, since exp(\u2212 21 t2 ) \u2264 1,\nZ \u03b6\n1\n1\n(1 \u2212 \u03b6 2 )\nexp(\u2212 t2 )dt \u2264 \u03b6(1 \u2212 \u03b6 2 ) < \u03b6 exp(\u2212 \u03b6 2 ),\n2\n2\n0\n\nwhere the second inequality can easily be verified. Thus\nagain g \u2032 (\u03b6) < 0 when \u03b6 > 0, completing the proof. \u0003\nNow we make \u221a\nsome remarks. The above proof shows\n\u221a\nthat f (\u03b6) for \u03b6 \u2265 a reaches its maximum when \u03b6 = a.\nThus if r\u0304k\u22121,k\u22121 = r\u0304kk , or equivalently,\n2\n2\nrk\u22121,k\n+ rkk\n= rk\u22121,k\u22121 rkk ,\n\nPB will increase most. For a more general result, see\nLemma 4 and the remark after it.\nIn Lemma 1 there is no requirement that rk\u22121,k should\nbe size-reduced. The question we would like to ask here is\ndo size reductions in the LLL reduction algorithm affect\nPB ? From (11) we observe that PB only depends on the\ndiagonal entries of R. Thus size reductions alone will not\nchange PB . However, if a size reduction can bring changes\nto the diagonal entries of R after a permutation, then it\nwill likely affect PB . Therefore, all the size reductions on\nthe off-diagonal entries above the superdiagonal have no\neffect on PB . But the size reductions on the superdiagonal\nentries may affect PB . There are a few different situations,\nwhich we will discuss below.\nSuppose that the Lov\u00e1sz condition (6) holds for a\nspecific k. If (6) does not hold any more after the size\n\nreduction on rk\u22121,k , then columns k \u2212 1 and k of R are\npermuted by the LLL reduction algorithm and according to\nLemma 1 PB strictly increases or keeps unchanged if and\nonly if the size reduction makes rk\u22121,k zero (this occurs\nif rk\u22121,k is a multiple of rk\u22121,k\u22121 before the reduction).\nIf (6) still holds after the size reduction on rk\u22121,k , then\nthis size reduction does not affect PB .\nSuppose that the Lov\u00e1sz condition (6) does not hold\nfor a specific k. Then by Lemma 1 PB increases after a\npermutation and triangularization. If the size reduction on\nrk\u22121,k is performed before the permutation, we show in\nthe next lemma that PB increases further.\nLemma 2: Suppose that in the ILS problem (3) R\n2\n2\n2\nand |rk\u22121,k | >\n+ rkk\nsatisfies \u03b4 rk\u22121,k\u22121\n> rk\u22121,k\nrk\u22121,k\u22121 /2 for some k. Let R\u0304, \u0233, z and \u1e91 be defined\nas in Lemma 1. Suppose a size reduction on rk\u22121,k\nis performed first and then after the permutation of\ncolumns k \u2212 1 and k and triangularization, R becomes\nT\nT\nR\u0302, i.e., R\u0302 = \u011ck\u22121,k RZ k\u22121,k P k\u22121,k . Let \u0177 = \u011ck\u22121,k \u1ef9\n\u22121\nand w = P \u22121\nk\u22121,k Z k\u22121,k x, then (3) is transformed to\n\u22121\nminw\u2208Zn k\u0177 \u2212 R\u0302wk2 . Denote \u0175 = P \u22121\nk\u22121,k Z k\u22121,k x\u0302.\nThen the Babai point w B corresponding to the new\ntransformed ILS problem has a success probability greater\nthan or equal to the Babai point z B , i.e.,\nPr(z B = \u1e91) \u2264 Pr(w B = \u0175),\n\n(20)\n\nwhere the equality holds if and only if\n2\n2\n|rk\u22121,k\u22121 rk\u22121,k | = rk\u22121,k\n+ rkk\n.\n\n(21)\n\nProof. Obviously (20) is equivalent to\n\u03c6(r\u0304k\u22121,k\u22121 )\u03c6(r\u0304kk ) \u2264 \u03c6(r\u0302k\u22121,k\u22121 )\u03c6(r\u0302kk ),\nwhich, by the proof of Lemma 1, is also equivalent to\n\u0011\n\u0010 max{r\u0302\n\u0011\n\u0010 max{r\u0304\nk\u22121,k\u22121 , r\u0302kk\nk\u22121,k\u22121 , r\u0304kk }\n\u2264f\n} ,\nf\n2\u03c3\n2\u03c3\nwhere f is defined in (18). Since f (\u03b6) has been\u221ashowed\nto be strict monotonically decreasing when \u03b6 > a, what\nwe need to show is that\nmax{r\u0304k\u22121,k\u22121 , r\u0304kk } \u2265 max{r\u0302k\u22121,k\u22121 , r\u0302kk },\n\n(22)\n\nwhere the equality holds if and only if (21) holds.\nSince |rk\u22121,k | > rk\u22121,k\u22121 /2,\nq\nq\n2\n2 >\n2\n2 ,\n+ rkk\n/4 + rkk\nr\u0304k\u22121,k\u22121 = rk\u22121,k\nrk\u22121,k\u22121\nrk\u22121,k\u22121 rkk\nrk\u22121,k\u22121 rkk\nr\u0304kk = q\n< q\n.\n2\n2\n2\n2\nrk\u22121,k + rkk\nrk\u22121,k\u22121 /4 + rkk\nq\n2\n2 \u2265 \u221a rk\u22121,k\u22121 rkk\nBut rk\u22121,k\u22121\n/4 + rkk\n, thus\nr2\n/4+r 2\nk\u22121,k\u22121\n\nkk\n\nmax{r\u0304k\u22121,k\u22121 , r\u0304kk } = r\u0304k\u22121,k\u22121 .\n\nSuppose that after the size reduction, rk\u22121,k becomes\nr\u0303k\u22121,k . Note that\nq\nq\n2\n2 <\n2\n2 = r\u0304\n+ rkk\nrk\u22121,k\n+ rkk\nr\u0302k\u22121,k\u22121 = r\u0303k\u22121,k\nk\u22121,k\u22121 .\n\n\f6\n\nThus, it follows from (22) what we need to prove is that\nr\u0302kk \u2264 r\u0304k\u22121,k\u22121 or equivalently\nq\n2\n2 ,\n+ rkk\n(23)\nr\u0302kk \u2264 rk\u22121,k\n\nand the equality holds if and only if (21) holds.\nBy the conditions given in the lemma,\n|rk\u22121,k | < rk\u22121,k\u22121 < 2|rk\u22121,k |.\nThus\n\nr\u0303k\u22121,k = rk\u22121,k \u2212 \u230ark\u22121,k /rk\u22121,k\u22121 \u2309rk\u22121,k\u22121\n= rk\u22121,k \u2212 sign(rk\u22121,k )rk\u22121,k\u22121 .\nNow we consider two cases rk\u22121,k > 0 and rk\u22121,k < 0\nseparately. If rk\u22121,k > 0, then\nrk\u22121,k\u22121 rkk\nrk\u22121,k\u22121 rkk\nr\u0302kk =\n= q\nr\u0302k\u22121,k\u22121\nr\u03032\n+ r2\nk\u22121,k\n\nkk\n\nrk\u22121,k\u22121 rkk\n= p\n.\n2\n(rk\u22121,k \u2212 rk\u22121,k\u22121 )2 + rkk\n\nThus, to show (23) it suffices to show that\nq\nrk\u22121,k\u22121 rkk\n2\n2 .\np\n\u2264\nrk\u22121,k\n+ rkk\n2\n(rk\u22121,k \u2212 rk\u22121,k\u22121 )2 + rkk\n\nSimple algebraic manipulations shows that the above inequality is equivalent to\n2\n2 2\n(rk\u22121,k\u22121 rk\u22121,k \u2212 rk\u22121,k\n\u2212 rkk\n) \u2265 0,\n\nwhich certainly holds. And obviously, the equality in (23)\nholds if and only if\n2\n2\nrk\u22121,k\u22121 rk\u22121,k = rk\u22121,k\n+ rkk\n.\n\nIf rk\u22121,k < 0, we can similarly prove that (23) holds and\nthe equality holds if and only if\n2\n2\n\u2212rk\u22121,k\u22121 rk\u22121,k = rk\u22121,k\n+ rkk\n,\n\ncompleting the proof.\n\n\u0003\n\nHere we make a remark about the equality (21). From\nthe proof of Lemma 2 we see that if (21) holds, then\nthe equality in (23) holds, thus r\u0302kk = r\u0304k\u22121,k\u22121 . But\nthe absolute value of the determinant of the submatrix\nRk\u22121:k,k\u22121:k is unchanged by the size reduction, we must\nhave r\u0302k\u22121,k\u22121 = r\u0304kk . Thus if (21) holds, the effect of\nthe size reduction on rk\u22121,k is to make r\u0304k\u22121,k\u22121 and\nr\u0304kk permuted; therefore the success probability PB is not\nchanged by the size reduction.\n\u0014 Here\n\u0015 we give an example.\n5 4\nExample 2: Let R =\n. Then it is easy to\n\u221a0 \u0015 2\n\u221a \u0015\n\u0014\u221a\n\u0014 \u221a\n5 \u2212\u221a 5\n2 5 2\u221a 5\nand R\u0302 =\n.\nverify that R\u0304 =\n0\n5\n0\n2 5\nFrom the diagonal entries of R\u0304 and R\u0302 we can conclude\nthat the success probabilities of the two Babai points\ncorresponding to R\u0304 and R\u0302 are equal.\nFrom Lemmas 1 and 2 we immediately obtain the\nfollowing results.\n\nTheorem 3: Suppose that the ILS problem (3) is transformed to the ILS problem (9), where R\u0304 is obtained by\nAlgorithm 1. Then\nPr(xB = x\u0302) \u2264 Pr(z B = \u1e91),\nwhere the equality holds if and only if no column permutation occurs during the LLL reduction process or whenever\ntwo consecutive columns, say k \u2212 1 and k, are permuted,\nrk\u22121,k is a multiple of rk\u22121,k\u22121 (before the size reduction\non rk\u22121,k is performed). Any size reductions on the superdiagonal entries of R which are immediately followed by\na column permutation during the LLL reduction process\nwill enhance the success probability of the Babai point.\nAll other size reductions have no effect on the success\nprobability of the Babai point.\nNow we make some remarks. Note that the LLL reduction is not unique. Two different LLL reduction algorithms\nmay produce different R's. In Algorithm 1, when the\nLov\u00e1sz condition for two consecutive columns is not\nsatisfied, then a column permutation takes places to ensure\nthe Lov\u00e1sz condition to be satisfied. If an algorithm which\ncomputes the LLL reduction does not do permutations\nas Algorithm 1 does, e.g., the algorithm permutes two\ncolumns which are not consecutive or permutes two consecutive columns but the corresponding Lov\u00e1sz condition\nis not satisfied after the permutation, then we cannot\nguarantee this specific LLL reduction will increase PB .\nIt is interesting to note that [17] showed that all the\nsize reductions on the off-diagonal entries above the\nsuperdiagonal of R have no effect on the residual norm\nof the Babai point. Here we see that those size reductions\nare not useful from another perspective.\nIf we do not do size reductions in Algorithm 1, the\nalgorithm will do only column permutations. We refer to\nthis column permutation strategy as LLL-permute. The\ncolumn permutation strategies SQRD [15] and V-BLAST\n[16] are often used for solving box-constrained ILS problems (see [18] and [19]). In the following, we give simple\nnumerical test results to see how the four methods (SQRD,\nV-BLAST, LLL-permute with \u03b4 = 1 and LLL with \u03b4 = 1)\naffect PB .\nWe performed our M ATLAB simulations for the following two cases.\n\u2022 Case 1. A = randn(n, n), where randn(n, n) is a\nM ATLAB built-in function to generate a random n\u00d7n\nmatrix, whose entries follow the normal distribution\nN (0, 1).\nT\n\u2022 Case 2. A = U DV , U , V are random orthogonal\nmatrices obtained by the QR factorization of random\nmatrices generated by randn(n, n) and D is a n \u00d7 n\ndiagonal matrix with dii = 103(n/2\u2212i)/(n\u22121) .\nIn the tests for each case for a fixed n we gave 200 runs\nto generate 200 different A's. For n = 20, Figures 1 and\n2 display the average success probabilities of the Babai\npoints corresponding to various reduction or permutation\n\n\f7\n\nstrategies over 200 runs versus \u03c3 = 0.05 : 0.05 : 0.4, for\nCases 1 and 2, respectively. In both figures, \"QR\" means\nthe QR factorization is used, giving Pr(xB = x\u0302).\n\nTABLE I\nN UMBER OF RUNS OUT OF 200 IN WHICH PB\nCase 1\nMethods\nSQRD\n\n1\n\n0.9\n\nV-BLAST\n\nAverage PB\n\n0.8\n\nCase 2\n\n\u274d \u03c3\n\u274d\nn \u274d\u274d\n\n0.1\n\n0.2\n\n0.3\n\n0.1\n\n0.2\n\n0.3\n\n10\n20\n30\n40\n10\n20\n30\n40\n\n9\n12\n16\n15\n0\n0\n0\n0\n\n10\n11\n14\n9\n0\n0\n0\n0\n\n6\n7\n11\n5\n0\n0\n0\n0\n\n13\n6\n0\n0\n2\n0\n0\n0\n\n8\n2\n1\n0\n6\n0\n0\n0\n\n5\n1\n1\n0\n7\n0\n0\n0\n\n0.7\n\n0.6\nQR\nSQRD\nV\u2212BLAST\nLLL\u2212permute\nLLL\n\n0.5\n\n0.4\n\n0.05\n\nFig. 1.\n\n0.1\n\n0.15\n\n0.2\n\n\u03c3\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\nAverage success probability versus \u03c3 for Case 1, n = 20\n\nB. Effects of \u03b4 on the enhancement of PB\nSuppose that R1 and R2 are obtained by applying\nAlgorithm 1 to A with \u03b4 = \u03b41 and \u03b4 = \u03b42 , respectively\nand \u03b41 < \u03b42 . A natural question is what is the relation\nbetween PB (R1 ) and PB (R2 )? In the following we try to\naddress this question. First we give a result for n = 2.\nTheorem 4: Suppose that R1 and R2 are obtained by\napplying Algorithm 1 to A \u2208 Rm\u00d7n with \u03b4 = \u03b41 and\n\u03b4 = \u03b42 , respectively and \u03b41 < \u03b42 . If n = 2, then\nPB (R1 ) \u2264 PB (R2 ).\n\n0.9\nQR\nSQRD\nV\u2212BLAST\nLLL\u2212permute\nLLL\n\nAverage PB\n\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n\u03c3\n\n0.25\n\n0.3\n\n0.35\n\n(24)\n\nProof. Note that only two columns are involved in the\nreduction process and the value of \u03b4 only determines when\nthe process should terminate. In the reduction process, the\nupper triangular matrix R either first becomes \u03b41 -LLL\nreduced and then becomes \u03b42 -LLL reduced after some\nmore permutations or becomes \u03b41 -LLL reduced and \u03b42 LLL reduced at the same time. Therefore, by Lemma 1\nthe conclusion holds. \u0003\n\n1\n\nFig. 2.\n\nDECREASES\n\n0.4\n\nAverage success probability versus \u03c3 for Case 2, n = 20\n\nFrom Figures 1 and 2, we can see that on average\nthe LLL reduction improves PB much more significantly\nthan the other three, V-BLAST performs better than LLLpermute and SQRD, and LLL-permute and SQRD have\nsimilar performance. We observed the same phenomenon\nwhen we changed the dimensions of A.\nFigures 1 and 2 indicate that on average SQRD and VBLAST increase PB . However, unlike LLL-permute, both\nSQRD and V-BLAST may decrease PB sometimes. Table I\ngives the number of runs out of 200 in which SQRD and\nV-BLAST decrease PB for various \u03c3 and n. From the\ntable we can see that for both Cases 1 and 2, the chance\nthat SQRD decreases PB is much larger than V-BLAST\nand when \u03c3 increases, the chance that SQRD decreases\nPB tends to decrease. For Case 2, when n increases, the\nchance that SQRD decreases PB tends to decrease, but\nthis phenomenon is not seen for Case 1.\n\nHowever, the inequality (24) in Theorem 4 may not hold\nwhen n \u2265 3. In fact, for any given n \u2265 3, we can give an\nexample to illustrate this.\nExample 3: Let \u03b41 and \u03b42 satisfy 1/4 < \u03b41 < \u03b42 \u2264 1\n2\nand \u03b42 < \u03b4p\n1 + 1/4. Let \u03b7 and \u03b8 satisfy \u03b41 < \u03b7 < \u03b42 and\n1\n0 < \u03b8 < 2 \u03b41 (\u03b7 \u2212 \u03b41 ). Let\n\uf8f9\n\uf8ee\n1 0 1/2\n\u221a\n\u03b7\n\u03b8 \uf8fb.\n(25)\nR = \uf8f00\n0 0\n\u03b41\n\nNote that R is size reduced already.\nSuppose that we apply Algorithm 1 with \u03b4 = \u03b41 to\nR, leading to R1 . The first two columns of R do not\npermute as the Lov\u00e1sz condition holds. However, the\nLov\u00e1sz condition does not hold for the last two columns\nand a permutation is needed. Then by Lemma 1 we must\nhave PB (R1 ) > PB (R).\nApplying Algorithm 1 with \u03b4 = \u03b42 to R, we obtain\n\uf8ee\u221a\n\uf8f9\n\u03b7 0 \u03b8\nR2 = \uf8f0 0 1 1/2\uf8fb ,\n0 0 \u03b41\nwhose diagonal entries are the same as those of R with\na different order. Then we have PB (R2 ) = PB (R).\nTherefore, PB (R1 ) > PB (R2 ).\n\n\f8\n\nTABLE II\nS UCCESS PROBABILITY Pr(xB\nCase 1\nCase 2\n\n\u03c3 = 0.1\n0.839\n1.85 \u00d7 10\u22122\n\n\u03c3 = 0.2\n0.661\n1.95 \u00d7 10\u22124\n\n= x\u0302)\n\u03c3 = 0.3\n0.477\n5.56 \u00d7 10\u22126\n\nFrom Table II, Figures 3 and 4, we can see that the LLL\nreduction has a significant effect on improving PB . Figures\n3 and 4 show that as \u03b4 increases, on average PB increases\ntoo, in particular for large \u03c3. But we want to point out\nthat we also noticed that sometimes a larger \u03b4 resulted in\na smaller PB in the tests. Table III gives the exact number\nof runs out of those 200 runs in which PB decreases when\n\u03b4 increases from t to t + 0.1 for t = 0.3 : 0.1 : 0.9. From\nTable III we can see that most of the time PB does not\ndecrease when \u03b4 increases. We would like to point out that\nin our numerical tests we tried various dimension size n\nfor the two test cases and observed the same phenomena.\nTABLE III\nN UMBER OF RUNS IN WHICH PB DECREASES WHEN \u03b4\nCase 1\n\nINCREASES\n\nCase 2\n\n\u274d \u03c3\n\u274d\n\u03b4 \u274d\u274d\n\n0.1\n\n0.2\n\n0.3\n\n0.1\n\n0.2\n\n0.3\n\n0.3-0.4\n0.4-0.5\n0.5-0.6\n0.6-0.7\n0.7-0.8\n0.8-0.9\n0.9-1.0\n\n8\n10\n13\n19\n2\n3\n1\n\n9\n9\n14\n18\n10\n11\n13\n\n10\n8\n13\n16\n12\n9\n8\n\n9\n10\n12\n17\n12\n15\n16\n\n10\n11\n11\n18\n13\n18\n19\n\n11\n11\n11\n20\n14\n19\n22\n\n0.95\n\nAverage PB\n\nAlthough the above example shows that larger \u03b4 may\nnot guarantee to produce higher PB when n \u2265 3, we can\nexpect that the chance that PB (R1 ) \u2264 PB (R2 ) is much\nhigher than the chance that PB (R1 ) > PB (R2 ). Here\nwe give an explanation. If R1 is not \u03b42 -LLL reduced,\napplying Algorithm 1 with \u03b4 = \u03b42 to R1 produces R\u03041\nwith PB (R\u03041 ) \u2265 PB (R1 ). Although R\u03041 may not be equal\nto R2 , we can expect that the difference between these\ntwo \u03b42 -LLL reduced matrices is small. Thus it is likely\nthat PB (R2 ) \u2248 PB (R\u03041 ) \u2265 PB (R1 ).\nHere we give numerical results to show how \u03b4 affects\nPB (i.e., Pr(z B = \u1e91)). We used the matrices defined\nin Cases 1 and 2 of Section IV-A. As before, in the\ntests for each case we gave 200 runs to generate 200\ndifferent A's for a fixed n. For n = 20, Figures 3 and\n4 display the average Pr(z B = \u1e91) over 200 runs versus\n\u03b4 = 0.3 : 0.1 : 1.0 for Cases 1 and 2, respectively. The three\ncurves in both figures correspond to \u03c3 = 0.1, 0.2, 0.3. For\ncomparisons, we give the corresponding Pr(xB = x\u0302) in\nthe following table.\n\n1\n\n0.9\n\n0.85\n\u03c3=0.1\n\u03c3=0.2\n\u03c3=0.3\n\n0.8\n\n0.75\n\n0.7\n\n0.65\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n\u03b4\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nAverage PB after the LLL reduction for Case 1, n = 20\n\nFig. 3.\n\n1\n0.9\n0.8\n\nAverage PB\n\n3\u00d73\nR\n\u0003 \u2208 Rn\u00d7n given in (25), we define A as A =\n\u0002 RWith\n0\n, it is easy to show that we still have\n0 I n\u22123 \u2208 R\nPB (R1 ) > PB (R2 ), where R1 and R2 were obtained\nby applying Algorithm 1 to A with \u03b4 = \u03b41 and \u03b4 = \u03b42 ,\nrespectively.\n\n\u03c3=0.1\n\u03c3=0.2\n\u03c3=0.3\n\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0.2\n\nFig. 4.\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n\u03b4\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nAverage PB after the LLL reduction for Case 2, n = 20\n\nC. Some upper bounds on PB after the LLL reduction\nWe have shown that the LLL reduction by Algorithm 1\ncan enhance the success probability of the Babai point. A\nnatural question is how much is the enhancement? If the\nLLL reduction has been computed by Algorithm 1, then\nwe can easily obtain the ratio Pr(z B = \u1e91)/ Pr(xB = x\u0302)\nby using the formula given in (11). If we only know the Rfactor of the QR factorization of A, usually it is impossible\nto know the ratio exactly. However, we will derive some\nbounds on Pr(z B = \u1e91), which involve only the R-factor\nof the QR factorization of A. From these bounds one can\nimmediately obtain bounds on the ratio.\nBefore giving an upper bound on Pr(z B = \u1e91), we give\nthe following result, see, e.g., [20, Thm 6].\n\n\f9\n\nLemma 3: Let R be the R-factor of the QR factorization of A and let R(p) be the upper triangular matrix\nafter the p-th column permutation and triangularization\nin the LLL reduction process by Algorithm 1, then for\ni = 1, 2, . . . , n\nmin{rii , ri+1,i+1 , . . . , rnn }\n(p)\n\n\u2264 rii \u2264 max{r11 , r22 , . . . , rii }.\n\n(26)\n\nWhen the LLL reduction process finishes, the diagonal\nentries of the upper triangular matrix certainly satisfy (26).\nThen using the second inequality in (26) we obtain the\nfollowing result from (11).\nTheorem 5: Suppose that the ILS problem (3) is transformed to the ILS problem (9) after the LLL reduction by\nAlgorithm 1. The success probability of the Babai point\nfor the ILS problem (9) satisfies:\nPr(z B = \u1e91) \u2264 \u03a0ni=1 \u03c6(\u03b3i ),\n\n(27)\n\nwhere \u03b3i = max{r11 , r22 , * * * , rii }.\nIn the following we give another upper bound on the\nsuccess probability of the Babai point, which is invariant\nto the unimodular transformation to R. The result was\nessentially obtained in [21], but our proof is much simpler.\nn\u00d7n\n\nLemma 4: Let R \u2208 R\nbe an upper triangular matrix\nwith positive diagonal entries, then\nn\nY\n\ni=1\n\n\u03c6(rii ) \u2264 \u03c6n\n\nn\n\u0010\u0010 Y\n\ni=1\n\nrii\n\n\u00111/n \u0011\n\n,\n\n(28)\n\nwhere the equality holds if and only if all the diagonal\nentries of R are equal.\nProof. Let h(\u03be) = ln(\u03c6(exp(\u03be))\nand vi Q= ln rii for\nPn\nn\ni = 1, . . . , n. Define v = n1 i=1 vi = n1 ln( i=1 rii ). To\nprove (28), it suffices to show that\nn\n\n1X\nh(vi ) \u2264 h(v).\nn i=1\n\n(29)\n\nIt is easy to verify that\n\nMk \u2264 mk+1 ,\n\nk = 1, . . . , l,\n\n(31)\n\nwhere\nMk = max{rik\u22121 +1,ik\u22121 +1 , rik\u22121 +2,ik\u22121 +2 , . . . , rik ,ik }\nmk+1 = min{rik +1,ik +1 , rik +2,ik +2 , . . . , rik+1 ,ik+1 },\nwith i0 = 0 and il+1 = n, then\nPr(z B = \u1e91) \u2264\nwhere\n\u03bdk =\n\n\u0012\n\nik\nY\n\nrjj\n\nj=ik\u22121 +1\n\nl+1\nY\n\nk=1\n\n\u03c6ik \u2212ik\u22121 (\u03bdk ) \u2264 \u03c6n (\u03bd),\n\n\u00131/(ik \u2212ik\u22121 )\n\n,\n\n\u03bd=\n\n\u0012Y\nn\n\nj=1\n\nrjj\n\n(32)\n\n\u00131/n\n\n.\n\nProof. Partition R as follows:\nR = [R1 , R2 , * * * , Rl+1 ],\nwhere the diagonal entries of R which are in block\nRk \u2208 Rn\u00d7(ik \u2212ik\u22121 ) are rik\u22121 +1,ik\u22121 +1 , rik\u22121 +2,ik\u22121 +2 ,\n. . ., rik ,ik for k = 1, . . . , l + 1. The condition (31) is to\nensure that in the LLL reduction process by Algorithm\n1 there are no column permutations between Rk s. Now\nwe prove this claim. Suppose that Algorithm 1 has just\nfinished the operations on R2 and is going to work on\nR3 . At this moment, [R1 , R2 ] is LLL reduced. In the LLL\nreduction of [R1 , R2 ], no column permutation between\nthe last column of R1 and and the first column of R2\noccurred. In fact, by (26) in Lemma 3 and the inequality\nM1 \u2264 m2 from (31), after a permutation, say the pth permutation, in the LLL reduction of [R1 , R2 ] by\nAlgorithm 1,\n(p)\n\n\u0010 1\n\u0011\n1\nexp(\u03be)g \u2032\nexp(\u03be) ,\nh\u2032\u2032 (\u03be) =\n2\u03c3\n2\u03c3\n\nwhere g(*) was defined in the proof of Lemma 1. According to the proof of Lemma 1, g \u2032 (\u03b6) < 0 for \u03b6 > 0.\nThus h\u2032\u2032 (\u03be) < 0, i.e., h(\u03be) is a strictly concave function.\nTherefore, (29) must hold and the equality holds if and\nonly if all vi are equal, or equivalently all rii are equal.\n\u0003\nSuppose that the ILS problem (3) is transformed to the\nILS problem (9) after the LLL\nQnreduction by Algorithm 1.\nThen det(R\u0304) = det(R) = i=1 rii . Thus by Lemma 4\nwe have\nn\nn\n\u00111/n \u0011\n\u0010\u0010 Y\nY\nrii\n(30)\n\u03c6(r\u0304ii ) \u2264 \u03c6n\nPr(z B = \u1e91) =\ni=1\n\nThe upper bound is reachable if and only if all the diagonal\nentries of R\u0304 are equal to det1/n (R). If the gap between\nthe largest diagonal entry and the smallest diagonal entry\nof R\u0304 is large, the upper bound in (30) will not be tight.\nIn the following, we give an improved upper bound.\nTheorem 6: Under the same assumption as in Theorem\n5, if there exist indices i1 , i2 , . . . , il such that\n\ni=1\n\nri1 ,i1 \u2264 max{r11 , . . . , ri1 ,i1 }\n\n(p)\n\n\u2264 min{ri1 +1,i1 +1 , * * * , ri2 ,i2 } \u2264 ri1 +1,r1 +1 .\nThus for any \u03b4 satisfying 1/4 < \u03b4 \u2264 1, the Lov\u00e1sz\ncondition (6) is satisfied for columns i1 and i1 + 1 and\nno permutation between these two columns would occur.\nNow the algorithm goes to work on the first column of R3 .\nAgain we can similarly show that no column permutation\nbetween the last column of R2 and and the first column\nof R3 will occur, so the algorithm will not go back to\nR2 . The algorithm continues and whenever the current\nblock is LLL reduced it goes to next block and will not\ncome back to the previous block. Then by applying the\nresult given in (30) for each block Rk we obtain the\nfirst inequality in (32). The second inequality in (32) is\nobtained immediately by applying Lemma 4. \u0003\n\n\f10\n\nIf indices ik for k = 1, . . . , l defined in Theorem 6 do\nnot exist, we assume l = 0, then the first inequality in (32)\nstill holds as its right hand side is just \u03c6n (\u03bd).\nWe now show how to find these indices if they exist. It\nis easy to verify that (31) is equivalent to\nmax{M1 , . . . , Mk } \u2264 min{mk+1 , . . . , ml+1 }\n\n(33)\n\nfor k = 1, . . . , l. Define two vectors u, v \u2208 Rn\u22121\nas follows: u1 = r11 , ui = max{r11 , . . . , rii } =\nmax{ui\u22121 , rii } for i = 2, . . . , n \u2212 1; vn\u22121 = rnn ,\nvi = min{ri+1,i+1 , . . . , rnn } = min{ri+1,i+1 , vi+1 }.\nThen (33) is equivalent to\nuik \u2264 vik ,\n\nk = 1, . . . , l.\n\nThus we can compare the entries of u and v from the first\nto the last to obtain all indices ik . It is easy to observe\nthat that the total cost is O(n).\nLet \u03b21 , \u03b22 and \u03b23 denote the three upper bounds on\nPr(z B = \u1e91) given in (27) and (32), respectively, i.e.,\n\u03b21 = \u03a0ni=1 \u03c6(\u03b3i ), \u03b22 =\n\nl+1\nY\n\n\u03c6ik \u2212ik\u22121 (\u03bdk ), \u03b23 = \u03c6n (\u03bd).\n\nk=1\n\nIn the following, we first give some special examples to\ncompare \u03b21 , \u03b22 and \u03b23 . \u0014\n\u0015\n1/\u03b7 \u00d7\nExample 4: Let R =\n, where 0 < \u03b7 < 1\n0\n\u03b72\nand \u00d7 is any real number. Then\n\u221a\n\u03b21 = \u03c62 (1/\u03b7), \u03b22 = \u03b23 = \u03c62 ( \u03b7).\nBy the definition of \u03c6(\u03b6) given in (11), \u03c6(1/\u03b7) \u2192 1 and\n\u221a\n\u03c6( \u03b7) \u2192 0 when \u03b7 \u2192 0. Thus, when \u03b7 is very small, \u03b22\nand \u03b23 are much sharper than \u03b21 .\nExample 5: Let\n\uf8ee\n\uf8f9\n\u03b7/3 \u00d7\n\u00d7\n\u00d7\n\uf8ef 0\n\u03b7\n\u00d7\n\u00d7\uf8fa\n\uf8fa , 0 < \u03b7 < 1,\nR=\uf8ef\n3\n\uf8f0 0\n0 1/\u03b7\n\u00d7\uf8fb\n0\n0\n0\n\u03b7/2\n\nwhere \u00d7 is any real number. Then\n\n\u03b21 = \u03c6(\u03b7/3)\u03c6(\u03b7)\u03c62 (1/\u03b7 3 ),\np\np\n\u0001\n\u03b22 = \u03c6(\u03b7/3)\u03c63 3 1/(2\u03b7) , \u03b23 = \u03c64 ( 4 1/6).\n\nFrom the definition of \u03c6(\u03b6), we see that when \u03b7 \u2192 0,\n\u03b21 \u2192 0, \u03b22 \u2192 0, \u03b21 /\u03b22 \u2192 0, \u03b22 /\u03b23 \u2192 0.\nTherefore, when \u03b7 is very small, \u03b21 is much sharper than\n\u03b22 , which is also much sharper than \u03b23 .\nNow we use more general examples to compare the\nthree upper bounds and also compare them with Pr(z B =\n\u1e91). In additional to Cases 1 and 2 given in Section IV-A,\nwe also tested the following case:\nCase 3. A = QR, where Q is a random orthogonal\nmatrix obtained by the QR factorization of a random\nmatrix generated by randn(n, n) and R is an n \u00d7 n upper\n\n2\ntriangular matrix with rii\nfollowing the \u03c72 distribution\nwith freedom degree i and with rij (j > i) following the\nnormal distribution N (0, 1).\nCase 3 is motivated by Case 1. In Case 1, the entries of\nthe R-factor of the QR factorization of A have the same\ndistributions as the entries of R in Case 3, except that the\n2\nfreedom degree for rii\nis n \u2212 i + 1, see [22, p99].\nIn the numerical experiments, for a given n and for each\ncase, we gave 200 runs to generate 200 different A's.\nAll the six tables given below display the average values\nof Pr(xB = x\u0302) (corresponding to QR), Pr(z B = \u1e91)\n(corresponding to LLL with \u03b4 = 1), \u03b21 , \u03b22 and \u03b23 . For\neach case, we give two tables. In the first table, n is fixed\nand \u03c3 varies, and in the second table, n varies and \u03c3 is\nfixed. In Tables V and IX \u03c3 was fixed to be 0.4, while in\nTable VII \u03c3 was fixed to be 0.1. We used different values\nof \u03c3 for these three tables so that Pr(z B = \u1e91) is neither\nclose to 0 nor close to 1, otherwise the bounds would not\nbe much interesting.\nFor Case 1, from Tables IV and V we observe that the\nupper bounds \u03b22 and \u03b23 are sharper than the upper bound\n\u03b21 , especially when n is small, and the former are good\napproximations to Pr(z B = \u1e91).\nFor Case 2, from Table VI we observe that the upper\nbound \u03b21 is extremely loose when \u03c3 is large, and \u03b22 and\n\u03b23 are much sharper for all those \u03c3. From Table VII we\nsee that when n becomes larger, the upper bounds \u03b22 and\n\u03b23 become worse, although they are still sharper than \u03b21 .\nTables VI-VII show that \u03b22 is equal to \u03b23 . Actually it is\nindeed true.\nFor Case 3, from Tables VIII and IX we observe that\nthe success probability of the Babai point improves after\nthe LLL reduction, but not as much as Cases 1 and 2. We\nalso observe that \u03b22 is sharper than \u03b21 , both are much\nsharper than \u03b23 , and \u03b22 is a reasonable approximation to\nPr(z B = \u1e91).\nBased on the numerical experiments and Theorem 6\nwe suggest taking min{\u03b21 , \u03b22 } as an upper bound on\nPr(z B = \u1e91) in practice.\nAlthough the upper bound min{\u03b21 , \u03b22 } is a good approximation to Pr(z B = \u1e91) in the above numerical tests,\nwe want to point out that this upper bound can be very\nloose. Here is a contrived example: Suppose all the offdiagonal entries of R in Example 5 are zero. Then\n\nPr(xB = x\u0302) = Pr(z B = \u1e91) = \u03c6(\u03b7/3)\u03c6(\u03b7)\u03c6(1/\u03b7 3 )\u03c6(\u03b7/2).\nThus, when \u03b7 \u2192 0, Pr(z B = \u1e91)/ min{\u03b21 , \u03b22 } \u2192 0.\nV. R EDUCTION\n\nOF THE SEARCH COMPLEXITY BY THE\nLLL REDUCTION\n\nIn this section, we rigorously show that applying the\nLLL reduction algorithm given in Algorithm 1 can reduce\nthe computational complexity of sphere decoders, which\nis measured approximately by the number of nodes in the\nsearch tree.\n\n\f11\n\nThe complexity results of sphere decoders given in the\nliterature are often about the complexity of enumerating\nall integer points in the search region:\nk\u1ef9 \u2212 Rxk2 \u2264 \u03b2,\n\n(34)\n\nwhere \u03b2 is a constant called the search radius. A typical\nmeasure of the complexity is the number of nodes enumerated by sphere decoders, which we denotes by \u03b6.\nFor i = n, n \u2212 1, . . . , 1, define Ei as follows\nEi = |{xi:n \u2208 Zn\u2212i+1 : k\u1ef9i:n \u2212 Ri:n,i:n xi:n k2 \u2264 \u03b2}|,\n(35)\nwhere | * | denotes the number of elements in the set. As\ngiven in [23], Ei can be estimated as follows:\nEi \u2248\n\nVn\u2212i+1 \u03b2 n\u2212i+1\nVn\u2212i+1 \u03b2 n\u2212i+1\n=\n,\n| det(Ri:n,i:n )|\n|rii ri+1,i+1 * * * rnn |\n\n(36)\n\nwhere Vn\u2212i+1 denotes the volume of an (n \u2212 i + 1)dimensional unit Euclidean ball. This estimation would\nbecome the expected value to Ei if \u1ef9 i:n is uniformly\ndistributed over a Voroni cell of the lattice generated by\nRi:n,i:n . Then we have (see, e.g., [24, Sec 3.2] and [25]).\nn\nX\nVn\u2212i+1 \u03b2 n\u2212i+1\n.\nEi \u2248 \u03b6\u0302(R) \u2261\n\u03b6=\nr r\n* * * rnn\ni=1 ii i+1,i+1\ni=1\nn\nX\n\n(37)\n\nIn practice, when a sphere decoder such as the SchnorrEuchner algorithm is used in the search process, after an\ninteger point is found, \u03b2 will be updated to shrink the\nsearch region. But \u03b6 or \u03b6\u0302 here does not take this into\naccount for the sake of simplicity.\nThe following result shows that if the Lov\u00e1sz condition (6) is not satisfied, after a column permutation and\ntriangularization, the complexity \u03b6\u0302(R) decreases.\n2\n2\n2\nLemma 5: Suppose that \u03b4rk\u22121,k\u22121\n> rk\u22121,k\n+ rkk\nfor\nsome k for the R matrix in the ILS problem (3). After the\npermutation of columns k \u2212 1 and k and triangularization,\nR becomes R\u0304, i.e., R\u0304 = GTk\u22121,k RP k\u22121,k (see (7)). Then\nthe complexity \u03b6\u0302(R) of the search process decreases after\nthe transformation, i.e.,\n\u03b6\u0302(R) > \u03b6\u0302(R\u0304).\n\nTABLE V\nAVERAGE PB\nn\n5\n10\n15\n20\n25\n30\n35\n40\n\nQR\n0.37181\n0.33269\n0.30324\n0.32896\n0.31439\n0.32649\n0.34107\n0.32538\n\nLLL\n0.52120\n0.73310\n0.87116\n0.94211\n0.95364\n0.96961\n0.97361\n0.97579\n\n\u03c3\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n\nQR\n0.27379\n0.01864\n0.00161\n0.00019\n0.00003\n0.00001\n0.00000\n0.00000\n\n\u03c3\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n\nQR\n0.93242\n0.84706\n0.75362\n0.66027\n0.56905\n0.48130\n0.39864\n0.32279\n\nLLL\n1.00000\n1.00000\n0.99999\n0.99966\n0.99815\n0.99289\n0.97589\n0.93432\n\n\u03b21\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n0.99999\n0.99997\n\nC ASE 1, n = 20\n\u03b22\n1.00000\n1.00000\n1.00000\n0.99984\n0.99891\n0.99645\n0.98849\n0.96319\n\n\u03b23\n1.00000\n1.00000\n1.00000\n0.99984\n0.99891\n0.99645\n0.98849\n0.96319\n\nAND BOUNDS FOR\n\nLLL\n1.00000\n0.99490\n0.82023\n0.38963\n0.10896\n0.02248\n0.00411\n0.00074\n\n\u03b22\n0.55777\n0.75146\n0.89076\n0.97004\n0.98993\n0.99752\n0.99939\n0.99980\n\n\u03b23\n0.56437\n0.75146\n0.89076\n0.97004\n0.98993\n0.99752\n0.99939\n0.99980\n\n\u03b21\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n\nC ASE 2, n = 20\n\u03b22\n1.00000\n0.99939\n0.89650\n0.46930\n0.13462\n0.02738\n0.00489\n0.00086\n\n\u03b23\n1.00000\n0.99939\n0.89650\n0.46930\n0.13462\n0.02738\n0.00489\n0.00086\n\nTABLE VII\nAVERAGE PB\nn\n5\n10\n15\n20\n25\n30\n35\n40\n\nQR\n0.06157\n0.05522\n0.03069\n0.01865\n0.01149\n0.00562\n0.00324\n0.00175\n\nAND BOUNDS FOR\n\nLLL\n0.75079\n0.98875\n0.99670\n0.99486\n0.97374\n0.88945\n0.76654\n0.68623\n\n\u03b21\n0.99984\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n1.00000\n\nC ASE 2, \u03c3 = 0.1\n\u03b22\n0.83688\n0.99344\n0.99860\n0.99939\n0.99963\n0.99973\n0.99978\n0.99981\n\n\u03b23\n0.83688\n0.99344\n0.99860\n0.99939\n0.99963\n0.99973\n0.99978\n0.99981\n\nTABLE VIII\nAVERAGE PB\n\u03c3\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n\nQR\n0.91780\n0.85132\n0.77339\n0.68615\n0.59499\n0.50466\n0.41858\n0.33919\n\nAND BOUNDS FOR\n\nLLL\n0.92401\n0.86372\n0.79087\n0.70836\n0.62040\n0.53153\n0.44528\n0.36432\n\n(38)\n\nTABLE IV\nAND BOUNDS FOR\n\n\u03b21\n0.92083\n0.99634\n0.99967\n0.99999\n1.00000\n1.00000\n1.00000\n1.00000\n\nC ASE 1, \u03c3 = 0.4\n\nTABLE VI\nAVERAGE PB\n\n\u03b21\n0.92450\n0.87017\n0.80902\n0.74366\n0.67610\n0.60831\n0.54164\n0.47679\n\nC ASE 3, n = 20\n\u03b22\n0.92471\n0.86856\n0.79945\n0.72379\n0.64530\n0.56704\n0.49161\n0.42031\n\n\u03b23\n1.00000\n1.00000\n1.00000\n1.00000\n0.99986\n0.99837\n0.99038\n0.96432\n\nTABLE IX\nAVERAGE PB\n\nAVERAGE PB\n\nAND BOUNDS FOR\n\n5\n10\n15\n20\n25\n30\n35\n40\n\n0.35057\n0.35801\n0.32379\n0.34612\n0.35252\n0.32538\n0.33183\n0.32196\n\nAND BOUNDS FOR\n\n0.37086\n0.38542\n0.35068\n0.37149\n0.37865\n0.35542\n0.35421\n0.34759\n\n0.47342\n0.49866\n0.47865\n0.49066\n0.48907\n0.46208\n0.46524\n0.45264\n\nC ASE 3, \u03c3 = 0.4\n0.38878\n0.42252\n0.40583\n0.44551\n0.44248\n0.43224\n0.42288\n0.41220\n\nProof. Since r\u0304ii = rii for i 6= k \u2212 1, k,\n\n0.53300\n0.75949\n0.90613\n0.96841\n0.99232\n0.99708\n0.99933\n0.99975\n\n\f12\n\nr\u0304k\u22121,k\u22121 r\u0304kk = rk\u22121,k\u22121 rkk , and r\u0304kk > rkk , we have\n\u03b6\u0302(R) \u2212 \u03b6\u0302(R\u0304)\nn\nn\nX\nX\nVn\u2212i+1 \u03b2 n\u2212i+1\nVn\u2212i+1 \u03b2 n\u2212i+1\n=\n\u2212\nr r\n* * * rnn i=1 r\u0304ii r\u0304i+1,i+1 * * * r\u0304nn\ni=1 ii i+1,i+1\nVn\u2212k+1 \u03b2 n\u2212k+1\nVn\u2212k+1 \u03b2 n\u2212k+1\n\u2212\nrkk rk+1,k+1 * * * rnn\nr\u0304kk rk+1,k+1 * * * rnn\n\u0012\n\u0013\n1\n1\nVn\u2212k+1 \u03b2 n\u2212k+1\n=\n\u2212\n> 0,\nrkk\nr\u0304kk rk+1,k+1 * * * rnn\n\n=\n\ncompleting the proof.\n\n\u0003\n\nSuppose the Lov\u00e1sz condition (6) does not hold for a\nspecific k and furthermore |rk\u22121,k | > rk\u22121,k\u22121 /2. The\nnext lemma, which is analogous to Lemma 2, shows\nthat the size reduction on rk\u22121,k performed before the\npermutation can decrease the complexity \u03b6\u0302(R) further.\nLemma 6: Suppose that in the ILS problem (3) R satis2\n2\n2\nfies \u03b4rk\u22121,k\u22121\n> rk\u22121,k\n+ rkk\nand |rk\u22121,k | > rk\u22121,k\u22121 /2\nfor some k. Let R\u0304 be defined as in Lemma 5. Suppose a\nsize reduction on rk\u22121,k is performed first and then after\nthe permutation of columns k \u2212 1 and k and triangularizaT\ntion, R becomes R\u0302, i.e., R\u0302 = \u011ck\u22121,k RZ k\u22121,k P k\u22121,k .\nThen\n\u03b6\u0302(R\u0304) > \u03b6\u0302(R\u0302).\n(39)\nProof. By the same argument given in the proof of\nLemma 5, we have\n\u0013\n\u0012\n1\nVn\u2212k+1 \u03b2 n\u2212k+1\n1\n\u2212\n.\n\u03b6\u0302(R\u0304) \u2212 \u03b6\u0302(R\u0302) =\nr\u0304kk\nr\u0302kk rk+1,k+1 * * * rnn\nTo show (39) we need only to prove r\u0304kk < r\u0302kk . Since\nr\u0304k\u22121,k\u22121 r\u0304kk = r\u0302k\u22121,k\u22121 r\u0302kk and r\u0302k\u22121,k\u22121 < r\u0304k\u22121,k\u22121\n(see the proof of Lemma 2), we have r\u0304kk < r\u0302kk , completing the proof. \u0003\nFrom Lemmas 5 and 6 we immediately obtain the\nfollowing result.\nTheorem 7: Suppose that the ILS problem (3) is transformed to the ILS problem (9), where R\u0304 is obtained by\nAlgorithm 1. Then\n\u03b6\u0302(R) \u2265 \u03b6\u0302(R\u0304),\nwhere the equality holds if and only if no column permutation occurs during the LLL reduction process. Any\nsize reductions on the superdiagonal entries of R which\nis immediately followed by a column permutation during\nthe LLL reduction process will reduce the complexity \u03b6\u0302.\nAll other size reductions have no effect on \u03b6\u0302.\nThe result on the effect of the size reductions is\nconsistent with a result given in [26], which shows that\nall the size reductions on the off-diagonal entries above\nthe superdiagonal of R and the size reductions on the\nsuperdiagonal entries of R which are not followed by\ncolumn permutations have no effect on the search speed\n\nof the Schnorr-Euchner algorithm for finding the ILS\nsolution.\nLike Theorem 4 in Section IV-B we can show that when\nn = 2 larger \u03b4 will decrease the complexity \u03b6\u0302 more, but\nwhen n \u2265 3, it may not be true, although our simulation\nresults indicated that usually it is true.\nIn Section IV-C we gave some upper bounds on the\nsuccess probability of the Babai point after the LLL\nreduction. Here we can use (26) to give a lower bound on\nthe complexity \u03b6\u0302 after the LLL reduction. To save space,\nwe will not give any details.\nVI. S UMMARY\n\nAND FUTURE WORK\n\nWe have shown that the success probability PB of the\nBabai point will increase and the complexity \u03b6\u0302 of sphere\ndecoders will decrease if the LLL reduction algorithm\ngiven in Algorithm 1 is applied for lattice reduction. We\nhave also discussed how the parameter \u03b4 in the LLL\nreduction affects PB and \u03b6\u0302. Some upper bounds on PB\nafter the LLL reduction have been presented. In addition,\nwe have shown that PB is a better lower bound on the\nsuccess probability of ILS estimator than the lower bound\ngiven in [1].\nThe implementation of LLL reduction is not unique.\nThe KZ reduction [27] is also an LLL reduction. But\nthe KZ conditions are stronger than the LLL conditions.\nWhether some implementations of the KZ reduction can\nalways increase PB and decrease \u03b6\u0302 and whether the\nimprovement is more significant compared with the regular\nLLL reduction algorithm given in Algorithm 1 will be\nstudied in the future.\nIn this paper, we assumed the model matrix A is\ndeterministic. If A is a random matrix following some\ndistribution, what is the formula of PB ? what is the\nexpected value of the search complexity? and how does\nthe LLL reduction affect them? These questions are for\nfuture studies.\nACKNOWLEDGMENT\nWe are grateful to Robert Fischer and the referees for\ntheir valuable and thoughtful suggestions. We would also\nlike to thank Damien Stehl\u00e9 for helpful discussions and\nfor providing a reference.\n.\nR EFERENCES\n[1] A. Hassibi and S. Boyd, \"Integer parameter estimation in linear\nmodels with applications to GPS,\" IEEE Transactions on Singal\nProcessing, vol. 46, no. 11, pp. 2938\u20132952, 1998.\n[2] C. Schnorr and M. Euchner, \"Lattice basis reduction: improved\npractical algorithms and solving subset sum problems,\" Mathematical Programming, vol. 66, pp. 181\u2013191, 1994.\n[3] E. Agrell, T. Eriksson, A. Vardy, and K. Zeger, \"Closest point\nsearch in lattices,\" IEEE Transactions on Information Theory,\nvol. 48, no. 8, pp. 2201\u20132214, 2002.\n\n\f13\n\n[4] M. O. Damen, H. E. Gamal, and G. Caire, \"On maximum likelihood detection and the search for the closest lattice point,\" IEEE\nTransactions on Information Theory, vol. 49, no. 10, pp. 2389\u2013\n2402, 2003.\n[5] A. Lenstra, H. Lenstra, and L. Lov\u00e1sz, \"Factoring polynomials with\nrational coefficients,\" Mathematische Annalen, vol. 261, no. 4, pp.\n515\u2013534, 1982.\n[6] P. van Emde Boas, \"Another NP-complete partition problem and\nthe complexity of computing short vectors in a lattice.\" Technical\nreport 81-04,Mathematics Department, University of Amsterdam,\nTech. Rep., 1981.\n[7] D. Micciancio, \"The hardness of the closest vector problem with\npreprocessing,\" IEEE Transactions on Information Theory, vol. 47,\nno. 3, pp. 1212\u20131215, 2001.\n[8] J. Jald\u00e9n and B. Ottersten, \"On the complexity of sphere decoding\nin digital communications,\" IEEE Transactions on Signal Processing, vol. 53, no. 4, pp. 1474\u20131484, 2005.\n[9] L. Babai, \"On Lovasz lattice reduction and the nearest lattice point\nproblem,\" Combinatorica, vol. 6, no. 1, pp. 1\u201313, 1986.\n[10] J. Jald\u00e9n, L. Barbero, B. Ottersten, and J. Thompson, \"The error\nprobability of the fixed-complexity sphere decoder,\" IEEE Transactions on Singal Processing, vol. 57, no. 7, pp. 2711\u20132720, 2009.\n[11] P. Xu, \"Voronoi cells, probabilistic bounds, and hypothesis testing\nin mixed integer linear models,\" IEEE Transactions on Information\nTheory, vol. 52, no. 7, pp. 3122\u20133138, 2006.\n[12] P. J. G. Teunissen, \"An optimality property of integer least-squares\nestimator,\" Journal of Geodesy, vol. 73, no. 11, pp. 587\u2013593, 1999.\n[13] Y. H. Gan and W. H. Mow, \"Novel joint sorting and reduction\ntechnique for delay-constrained LLL-aided MIMO detection,\" IEEE\nSignal Processing Letter, vol. 15, pp. 194\u2013197, 2008.\n[14] P. J. G. Teunissen, \"Success probability of integer GPS ambiguity\nrounding and bootstrapping,\" Journal of Geodesy, vol. 72, no. 10,\npp. 606\u2013612, 1998.\n[15] D. Wubben, R. Bohnke, J. Rinas, V. Kuhn, and K. Kammeyer,\n\"Efficient algorithm for decoding layered space-time codes,\" IEEE\nElectronics Letters, vol. 37, no. 22, pp. 1348\u20131350, 2001.\n[16] G. J. Foscini, G. D. Golden, R. A. Valenzuela, and P. W. Wolniansky, \"Simplified processing for high spectral efficiency wireless\ncommunication employing multi-element arrays,\" IEEE Journal on\nSelected Areas in Communications, vol. 17, no. 11, pp. 1841\u20131852,\n1999.\n[17] C. Ling and N. Howgrave-Graham, \"Effective LLL reduction for\nlattice decoding,\" in IEEE International Symposium on Information\nTheory, 2007. IEEE, 2007, pp. 196\u2013200.\n[18] M. O. Damen, H. E. Gamal, and G. Caire, \"On maximumlikelihood detection and the search for the closest lattice point,\"\nIEEE Transactions on Information Theory, vol. 49, no. 10, pp.\n2389\u20132402, 2003.\n[19] X.-W. Chang and Q. Han, \"Solving box-constrained integer least\nsquares problems,\" IEEE Transactions on Wireless Communications, vol. 7, no. 1, pp. 277\u2013287, 2008.\n[20] P. Q. Nguyen and D. Stehl\u00e9, \"An LLL algorithm with quadratic\ncomplexity,\" SIAM J. of Computing, vol. 39, no. 3, pp. 874\u2013903,\n2009.\n[21] P. J. G. Teunissen, \"An invariant upperbound for the GNSS bootstrappend ambiguity success-rate,\" Journal of Global Positioning\nSystems, vol. 2, no. 1, pp. 13\u201317, 2003.\n[22] R. I. Muirhead, Aspects of Multivariate Statistical Theory. New\nYork: Wiley, 1982.\n[23] J. M. W. P. M. Gruber, Ed., Handbook of convex geometry. NorthHolland, Amsterdam, 1993.\n[24] W. Abediseid, \"Efficient lattice decoders for the linear gaussian\nvector channel: Performance & complexity analysis,\" Ph.D. dissertation, Department of Electrical and Computer Engineering,\nUniversity of Waterloo, 2011.\n[25] D. Seethaler, J. Jald\u00e9n, C. Studer, and H. B\u00f6lcskei, \"On the\ncomplexity distribution of sphere decoding,\" IEEE Transactions on\nInformation Theory, vol. 57, no. 9, pp. 5754\u20135768, 2011.\n[26] X. Xie, X.-W. Chang, and M. Al Borno, \"Partial LLL reduction,\"\nin Proceedings of IEEE GLOBECOM 2011, 5 pages, 2011.\n[27] A. Korkine and G. Zolotareff, \"Sur les formes quadratiques,\"\nMathematische Annalen, vol. 6, pp. 366\u2013389, 1873.\n\nXiao-Wen Chang is an Associate Professor in the School of Computer\nScience at McGill University. He obtained his B.Sc. and M.Sc. in\nComputational Mathematics from Nanjing University (1986,1989) and\nhis Ph.D. in Computer Science from McGill University (1997). His\nresearch interests are in the area of scientific computing, with particular\nemphasis on numerical linear algebra and its applications. Currently he\nis mainly interested in parameter estimation methods, including integer\nleast squares, and as well as their applications in communications, signal\nprocessing and satellite-based positioning and wireless localization. He\nhas published about fifty papers in refereed journals.\n\nJinming Wen received his Bachelor degree in Information and Computing Science from Jilin Institute of Chemical Technology, Jilin, China, in\n2008 and his M.Sc. degree in Pure Mathematics from the Mathematics\nInstitute of Jilin University, Jilin, China, in 2010. He is currently pursuing\na Ph.D. in The Department of Mathematics and Statistics, McGill\nUniversity, Montreal. His research interests are in the area of integer\nleast squares problems and their applications in communications and\nsignal processing.\n\nXiaohu Xie received his Bachelor degree in Computer Science and\nTechnology from Wuhan University of Technology, Wuhan, China, in\n2007 and his M.Sc. degree in Computer Science and Technology from\nWuhan University of Technology, Wuhan, China, in 2009. He is currently\npursuing a Ph.D. in The School of Computer Science, McGill University,\nMontreal. Currently his research focuses on the theories and algorithms\nfor integer least squares problems.\n\n\f"}