{"id": "http://arxiv.org/abs/math/0701540v1", "guidislink": true, "updated": "2007-01-19T14:37:54Z", "updated_parsed": [2007, 1, 19, 14, 37, 54, 4, 19, 0], "published": "2007-01-19T14:37:54Z", "published_parsed": [2007, 1, 19, 14, 37, 54, 4, 19, 0], "title": "The penalized profile sampler", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0701566%2Cmath%2F0701310%2Cmath%2F0701022%2Cmath%2F0701679%2Cmath%2F0701188%2Cmath%2F0701082%2Cmath%2F0701149%2Cmath%2F0701912%2Cmath%2F0701610%2Cmath%2F0701176%2Cmath%2F0701887%2Cmath%2F0701240%2Cmath%2F0701756%2Cmath%2F0701404%2Cmath%2F0701350%2Cmath%2F0701545%2Cmath%2F0701599%2Cmath%2F0701676%2Cmath%2F0701619%2Cmath%2F0701161%2Cmath%2F0701380%2Cmath%2F0701695%2Cmath%2F0701324%2Cmath%2F0701896%2Cmath%2F0701612%2Cmath%2F0701636%2Cmath%2F0701214%2Cmath%2F0701810%2Cmath%2F0701258%2Cmath%2F0701096%2Cmath%2F0701099%2Cmath%2F0701637%2Cmath%2F0701215%2Cmath%2F0701346%2Cmath%2F0701506%2Cmath%2F0701376%2Cmath%2F0701421%2Cmath%2F0701705%2Cmath%2F0701552%2Cmath%2F0701778%2Cmath%2F0701455%2Cmath%2F0701328%2Cmath%2F0701798%2Cmath%2F0701319%2Cmath%2F0701524%2Cmath%2F0701472%2Cmath%2F0701739%2Cmath%2F0701652%2Cmath%2F0701760%2Cmath%2F0701878%2Cmath%2F0701281%2Cmath%2F0701851%2Cmath%2F0701562%2Cmath%2F0701867%2Cmath%2F0701334%2Cmath%2F0701287%2Cmath%2F0701053%2Cmath%2F0701712%2Cmath%2F0701540%2Cmath%2F0701475%2Cmath%2F0701389%2Cmath%2F0701808%2Cmath%2F0701371%2Cmath%2F0701512%2Cmath%2F0701683%2Cmath%2F0701294%2Cmath%2F0701081%2Cmath%2F0701280%2Cmath%2F0701891%2Cmath%2F0701532%2Cmath%2F0701592%2Cmath%2F0701237%2Cmath%2F0701782%2Cmath%2F0701665%2Cmath%2F0701870%2Cmath%2F0701487%2Cmath%2F0701267%2Cmath%2F0701306%2Cmath%2F0701295%2Cmath%2F0701662%2Cmath%2F0701898%2Cmath%2F0701419%2Cmath%2F0701031%2Cmath%2F0701381%2Cmath%2F0701620%2Cmath%2F0701627%2Cmath%2F0701069%2Cmath%2F0701063%2Cmath%2F0701187%2Cmath%2F0701724%2Cmath%2F0701935%2Cmath%2F0701253%2Cmath%2F0701633%2Cmath%2F0701502%2Cmath%2F0701702%2Cmath%2F0701314%2Cmath%2F0701303%2Cmath%2F0701120%2Cmath%2F0701631%2Cmath%2F0701684%2Cmath%2F0701204&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The penalized profile sampler"}, "summary": "The penalized profile sampler for semiparametric inference is an extension of\nthe profile sampler method (Lee, Kosorok and Fine, 2005) obtained by profiling\na penalized log-likelihood. The idea is to base inference on the posterior\ndistribution obtained by multiplying a profiled penalized log-likelihood by a\nprior for the parametric component, where the profiling and penalization are\napplied to the nuisance parameter. Because the prior is not applied to the full\nlikelihood, the method is not strictly Bayesian. A benefit of this\napproximately Bayesian method is that it circumvents the need to put a prior on\nthe possibly infinite-dimensional nuisance components of the model. We\ninvestigate the first and second order frequentist performance of the penalized\nprofile sampler, and demonstrate that the accuracy of the procedure can be\nadjusted by the size of the assigned smoothing parameter. The theoretical\nvalidity of the procedure is illustrated for two examples: a partly linear\nmodel with normal error for current status data and a semiparametric logistic\nregression model. As far as we are aware, there are no other methods of\ninference in this context known to have second order frequentist validity.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0701566%2Cmath%2F0701310%2Cmath%2F0701022%2Cmath%2F0701679%2Cmath%2F0701188%2Cmath%2F0701082%2Cmath%2F0701149%2Cmath%2F0701912%2Cmath%2F0701610%2Cmath%2F0701176%2Cmath%2F0701887%2Cmath%2F0701240%2Cmath%2F0701756%2Cmath%2F0701404%2Cmath%2F0701350%2Cmath%2F0701545%2Cmath%2F0701599%2Cmath%2F0701676%2Cmath%2F0701619%2Cmath%2F0701161%2Cmath%2F0701380%2Cmath%2F0701695%2Cmath%2F0701324%2Cmath%2F0701896%2Cmath%2F0701612%2Cmath%2F0701636%2Cmath%2F0701214%2Cmath%2F0701810%2Cmath%2F0701258%2Cmath%2F0701096%2Cmath%2F0701099%2Cmath%2F0701637%2Cmath%2F0701215%2Cmath%2F0701346%2Cmath%2F0701506%2Cmath%2F0701376%2Cmath%2F0701421%2Cmath%2F0701705%2Cmath%2F0701552%2Cmath%2F0701778%2Cmath%2F0701455%2Cmath%2F0701328%2Cmath%2F0701798%2Cmath%2F0701319%2Cmath%2F0701524%2Cmath%2F0701472%2Cmath%2F0701739%2Cmath%2F0701652%2Cmath%2F0701760%2Cmath%2F0701878%2Cmath%2F0701281%2Cmath%2F0701851%2Cmath%2F0701562%2Cmath%2F0701867%2Cmath%2F0701334%2Cmath%2F0701287%2Cmath%2F0701053%2Cmath%2F0701712%2Cmath%2F0701540%2Cmath%2F0701475%2Cmath%2F0701389%2Cmath%2F0701808%2Cmath%2F0701371%2Cmath%2F0701512%2Cmath%2F0701683%2Cmath%2F0701294%2Cmath%2F0701081%2Cmath%2F0701280%2Cmath%2F0701891%2Cmath%2F0701532%2Cmath%2F0701592%2Cmath%2F0701237%2Cmath%2F0701782%2Cmath%2F0701665%2Cmath%2F0701870%2Cmath%2F0701487%2Cmath%2F0701267%2Cmath%2F0701306%2Cmath%2F0701295%2Cmath%2F0701662%2Cmath%2F0701898%2Cmath%2F0701419%2Cmath%2F0701031%2Cmath%2F0701381%2Cmath%2F0701620%2Cmath%2F0701627%2Cmath%2F0701069%2Cmath%2F0701063%2Cmath%2F0701187%2Cmath%2F0701724%2Cmath%2F0701935%2Cmath%2F0701253%2Cmath%2F0701633%2Cmath%2F0701502%2Cmath%2F0701702%2Cmath%2F0701314%2Cmath%2F0701303%2Cmath%2F0701120%2Cmath%2F0701631%2Cmath%2F0701684%2Cmath%2F0701204&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The penalized profile sampler for semiparametric inference is an extension of\nthe profile sampler method (Lee, Kosorok and Fine, 2005) obtained by profiling\na penalized log-likelihood. The idea is to base inference on the posterior\ndistribution obtained by multiplying a profiled penalized log-likelihood by a\nprior for the parametric component, where the profiling and penalization are\napplied to the nuisance parameter. Because the prior is not applied to the full\nlikelihood, the method is not strictly Bayesian. A benefit of this\napproximately Bayesian method is that it circumvents the need to put a prior on\nthe possibly infinite-dimensional nuisance components of the model. We\ninvestigate the first and second order frequentist performance of the penalized\nprofile sampler, and demonstrate that the accuracy of the procedure can be\nadjusted by the size of the assigned smoothing parameter. The theoretical\nvalidity of the procedure is illustrated for two examples: a partly linear\nmodel with normal error for current status data and a semiparametric logistic\nregression model. As far as we are aware, there are no other methods of\ninference in this context known to have second order frequentist validity."}, "authors": ["Guang Cheng", "Michael R. Kosorok"], "author_detail": {"name": "Michael R. Kosorok"}, "author": "Michael R. Kosorok", "arxiv_comment": "26 pages", "links": [{"href": "http://arxiv.org/abs/math/0701540v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0701540v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "Primary 62G20, 62F25; secondary 62F15, 62F12", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0701540v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0701540v1", "journal_reference": null, "doi": null, "fulltext": "Submitted to the Annals of Statistics\n\narXiv:math/0701540v1 [math.ST] 19 Jan 2007\n\nTHE PENALIZED PROFILE SAMPLER\nBy Guang Cheng\u2217 and Michael R. Kosorok\u2217\nDuke University and University of North Carolina at Chapel Hill\nThe penalized profile sampler for semiparametric inference is an\nextension of the profile sampler method [8] obtained by profiling a\npenalized log-likelihood. The idea is to base inference on the posterior distribution obtained by multiplying a profiled penalized loglikelihood by a prior for the parametric component, where the profiling and penalization are applied to the nuisance parameter. Because the prior is not applied to the full likelihood, the method is not\nstrictly Bayesian. A benefit of this approximately Bayesian method\nis that it circumvents the need to put a prior on the possibly infinitedimensional nuisance components of the model. We investigate the\nfirst and second order frequentist performance of the penalized profile sampler, and demonstrate that the accuracy of the procedure can\nbe adjusted by the size of the assigned smoothing parameter. The\ntheoretical validity of the procedure is illustrated for two examples:\na partly linear model with normal error for current status data and\na semiparametric logistic regression model. As far as we are aware,\nthere are no other methods of inference in this context known to have\nsecond order frequentist validity.\nSupported in part by CA075142\nAMS 2000 subject classifications: Primary 62G20, 62F25; secondary 62F15, 62F12\n\n\u2217\n\nKeywords and phrases: Convergence Rate, Empirical Process, Markov Chain Monte\nCarlo, Partly Linear Model, Penalized Likelihood, Posterior Distribution, Profile Likelihood, Semiparametric Inference, Semiparametric Logistic Regression, Smoothing Parameter\n\n1\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f2\n\nG. CHENG AND M. R. KOSOROK\n\n1. Introduction. Semiparametric models are statistical models indexed\nby both a finite dimensional parameter of interest \u03b8 and an infinite dimensional nuisance parameter \u03b7. The profile likelihood is typically defined as\npln (\u03b8) = sup likn (\u03b8, \u03b7),\n\u03b7\u2208H\n\nwhere likn (\u03b8, \u03b7) is the likelihood of the semiparametric model given n observations and H is the parameter space for \u03b7. We also define\n\u03b7\u0302\u03b8 = argmax\u03b7\u2208H likn (\u03b8, \u03b7).\nThe convergence rate of the nuisance parameter \u03b7 is the order of d(\u03b7\u0302\u03b8\u0303n , \u03b70 ),\nwhere d(*, *) is some metric on \u03b7, \u03b8\u0303n is any sequence satisfying \u03b8\u0303n = \u03b80 +oP (1),\nand \u03b70 is the true value of \u03b7. Typically,\n(1)\n\nd(\u03b7\u0302\u03b8\u0303n , \u03b70 ) = OP (k\u03b8\u0303n \u2212 \u03b80 k + n\u2212r ),\n\nwhere k * k is the Euclidean norm and r > 1/4. Of course, a smaller value of\nr leads to a slower convergence rate of the nuisance parameter. For instance,\nthe nuisance parameter in the Cox proportional hazards model with right\ncensored data, the cumulative hazard function, has the parametric rate, i.e.,\nr = 1/2. If current status data is applied to the Cox model instead, then the\nconvergence rate will be slower, with r = 1/3, due to the loss of information\nprovided by this kind of data.\nThe profile sampler is the procedure of sampling from the posterior of the\nprofile likelihood in order to estimate and draw inference on the parametric\ncomponent \u03b8 in a semiparametric model, where the profiling is done over the\npossibly infinite-dimensional nuisance parameter \u03b7. [8] show that the profile\nsampler gives a first order correct approximation to the maximum likelihood\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\fTHE PENALIZED PROFILE SAMPLER\n\n3\n\nestimator \u03b8\u0302n and consistent estimation of the efficient Fisher information for\n\u221a\n\u03b8 even when the nuisance parameter is not estimable at the n rate. Another\nBayesian procedure employed to do semiparametric estimation is considered\nin [16] who study the marginal semiparametric posterior distribution for a\nparameter of interest. In particular, [16] show that marginal semiparametric\nposterior distributions are asymptotically normal and centered at the corresponding maximum likelihood estimates or posterior means, with covariance\nmatrix equal to the inverse of the Fisher information. Unfortunately, this\nfully Bayesian method requires specification of a prior on \u03b7, which is quite\nchallenging since for some models there is no direct extension of the concept\nof a Lebesgue dominating measure for the infinite-dimensional parameter set\ninvolved [7]. The advantages of the profile sampler for estimating \u03b8 compared\nto other methods is discussed extensively in [2], [3] and [8].\nIn many semiparametric models involving a smooth nuisance parameter,\nit is often convenient and beneficial to perform estimation using penalization. One motivation for this is that, in the absence of any restrictions on\nthe form of the function \u03b7, maximum likelihood estimation for some semiparametric models leads to over-fitting. Seminal applications of penalized\nmaximum likelihood estimation include estimation of a probability density\nfunction in [17] and nonparametric linear regression in [18]. Note that penalized likelihood is a special case of penalized quasi-likelihood studied in [12].\nUnder certain reasonable regularity conditions, penalized semiparametric\nlog-likelihood estimation can yield fully efficient estimates for \u03b8 (see, for example, [12]). As far as we are aware, the only general procedure for inference\nfor \u03b8 in this context known to be theoretically valid is a weighted bootstrap\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f4\n\nG. CHENG AND M. R. KOSOROK\n\nwith bounded random weights (see [10]). It is even unclear whether the\nusual nonparametric bootstrap will work in this context when the nuisance\nparameter has a convergence rate r < 1/2.\nIn contrast, [2] and [3] have shown that the profile sampler procedure without penalization can essentially yield second order frequentist valid inference\nfor \u03b8 in semiparametric models, where the estimation accuracy is dependent\non the convergence rate of the nuisance parameter. In other words, a faster\nconvergence rate of the nuisance parameters can yield more precise frequentist inference for \u03b8. These second order results are verified in [2] and [3]\nfor several examples, including the Cox model for both right censored and\ncurrent status data, the proportional odds model, case-control studies with\nmissing covariates, and the partly linear normal model. The convergence\nrates for these models range from the parametric to the cubic. The work in\n[3] has shown clearly that the accuracy of the inference for \u03b8 based on the\nprofile sampler method is intrinsically determined by the semiparametric\nmodel specifications through its entropy number.\nThe purpose of this paper is to ask the somewhat natural question: does\nsampling from a profiled penalized log-likelihood (which process we refer\nhereafter to as the penalized profile sampler) yield first and even second\norder accurate frequentist inference? The conclusion of this paper is that\nthe answer is yes and, moreover, the accuracy of the inference depends in a\nfairly simple way on the size of the smoothing parameter.\nThe unknown parameters in the semiparametric models we study in this\npaper includes \u03b8, which we assume belongs to some compact set \u0398 \u2282 Rd ,\nand \u03b7, which we assume to be a function in the Sobolev class of functions\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\fTHE PENALIZED PROFILE SAMPLER\n\n5\n\nsupported on some compact set on the real line, whose k-th derivative exists\nand is absolutely continuous with J(\u03b7) < \u221e, where\nJ 2 (\u03b7) =\n\nZ\n\n(\u03b7 (k) (z))2 dz.\n\nZ\n\nHere k is a fixed, positive integer and \u03b7 (j) is the j-th derivative of \u03b7 with\nrespect to z. Obviously J 2 (\u03b7) is some measurement of complexity of \u03b7. We\ndenote Hk as the Sobolev function class with degree k. The penalized loglikelihood in this context is:\nlog lik\u03bbn (\u03b8, \u03b7) = log lik(\u03b8, \u03b7) \u2212 \u03bb2n J 2 (\u03b7),\n\n(2)\n\nwhere log lik(\u03b8, \u03b7) \u2261 Pn l\u03b8,\u03b7 (X), l\u03b8,\u03b7 (X) is the log-likelihood of the single observation X, and \u03bbn is a smoothing parameter, possibly dependent on data.\nIn practice, \u03bbn can be obtained by cross-validation [22] or by inspecting the\nvarious curves for different values of \u03bbn . The penalized maximum likelihood\nestimators \u03b8\u0302n and \u03b7\u0302n depend on the choice of the smoothing parameter \u03bbn .\nConsequently we use the notation \u03b8\u0302\u03bbn and \u03b7\u0302\u03bbn for the remainder of this paper to denote the estimators obtained from maximizing (2). In particular, a\nlarger smoothing parameter usually leads to a less rough penalized estimator\nof \u03b70 .\nFor the purpose of establishing first order accuracy of inference for \u03b8\nbased on the penalized profile sampler, we assume that the bounds for the\nsmoothing parameter are in the form below:\n(3)\n\nk/(2k+1)\n\u03bbn = oP (n\u22121/4 ) and \u03bb\u22121\n).\nn = OP (n\n\nThe condition (3) is assumed to hold throughout this paper. One way to\nensure (3) in practice is simply to set \u03bbn = n\u2212k/(2k+1) . Or we can just choose\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f6\n\nG. CHENG AND M. R. KOSOROK\n\n\u03bbn = n\u22121/3 which is independent of k. It turns out that the upper bound\n\u221a\nguarantees that \u03b8\u0302\u03bbn is n-consistent, while the lower bound controls the\npenalized nuisance parameter estimator convergence rate. Another approach\nto controlling estimators is to use sieve estimates with assumptions on the\nderivatives (see [5]). We will not pursue this further here.\nThe log-profile penalized likelihood is defined as follows:\n(4)\n\nlog pl\u03bbn (\u03b8) = log lik(\u03b8, \u03b7\u0302\u03b8,\u03bbn ) \u2212 \u03bb2n J 2 (\u03b7\u0302\u03b8,\u03bbn ),\n\nwhere \u03b7\u0302\u03b8,\u03bbn is argmax\u03b7\u2208Hk log lik\u03bbn (\u03b8, \u03b7) for fixed \u03b8 and \u03bbn . The penalized\nprofile sampler is just the procedure of sampling from the posterior distribution of pl\u03bbn (\u03b8) by assigning a prior on \u03b8. By analyzing the corresponding\nMCMC chain from the frequentist's point of view, our paper obtains the\nfollowing conclusions:\n1 Distribution Approximation: The posterior distribution with respect to\npl\u03bbn (\u03b8) can be approximated by the normal distribution with mean the\nmaximum penalized likelihood estimator of \u03b8 and variance the inverse\nof the efficient information matrix, with error OP (n1/2 \u03bb2n );\n2 Moment Approximation: The maximum penalized likelihood estimator\nof \u03b8 can be approximated by the mean of the MCMC chain with error\nOP (\u03bb2n ). The efficient information matrix can be approximated by the\ninverse of the variance of the MCMC chain with error OP (n1/2 \u03bb2n );\n3 Confidence Interval Approximation: An exact frequentist confidence\ninterval of Wald's type for \u03b8 can be estimated by the credible set\nobtained from the MCMC chain with error OP (\u03bb2n ).\nObviously, given any smoothing parameter satisfying the upper bound\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\fTHE PENALIZED PROFILE SAMPLER\n\n7\n\nin (3), the penalized profile sampler can yield first order frequentist valid\ninference for \u03b8, similar as to what was shown for the profile sampler in\n[8]. Moreover, the above conclusions are actually second order frequentist\nvalid results, whose approximation accuracy is directly controlled by the\nsmoothing parameter. Note that the corresponding results for the usual\n(non-penalized) profile sampler with nuisance parameter convergence rate r\nin [3] are obtained by replacing in the above OP (n1/2 \u03bb2n ) with OP (n\u22121/2 \u2228\nn\u2212r+1/2 ) and OP (\u03bb2n ) with OP (n\u22121 \u2228 n\u2212r ), for all respective occur where r\nis as defined in (1).\nOur results are the first higher order frequentist inference results for penalized semiparametric estimation. The layout of the article is as follows.\nThe next section, section 2, introduces the two main examples we will be\nusing for illustration: partly linear regression for current status data and\nsemiparametric logistic regression. Some background is given in section 3,\nincluding the concept of a least favorable submodel as well as some notations and the main model assumptions. In section 4, some preliminary results\nare developed, including three rather different theorems concerning the convergence rates of the penalized nuisance parameters and the order of the\nestimated penalty term under different conditions. The corresponding rates\nfor the two featured examples are also calculated in this section. The main\nresults and implications are discussed in section 5, and all remaining model\nassumptions are verified for the examples in section 6. A brief discussion of\nfuture work is given in section 7. We postpone all technical tools and proofs\nto the last section, section 8.\n2. Examples.\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f8\n\nG. CHENG AND M. R. KOSOROK\n\n2.1. Partly Linear Normal Model with Current Status Data. In this example, we study the partly linear regression model with normal residue error.\nThe continuous outcome Y , conditional on the covariates (U, V ) \u2208 Rd \u00d7 R,\nis modeled as\nY = \u03b8 T U + f (V ) + \u01eb,\n\n(5)\n\nwhere f is an unknown smooth function, and \u01eb \u223c N (0, \u03c3 2 ) with finite variance \u03c3 2 . For simplicity, we assume for the rest of the paper that \u03c3 = 1. The\ntheory we propose also works when \u03c3 is unknown, but the added complexity\nwould detract from the main issues. We also assume that only the current\nstatus of response Y is observed at a random censoring time C \u2208 R. In\nother words, we observe X = (C, \u2206, U, V ), where indicator \u2206 = 1{Y \u2264 C}.\nCurrent status data may occur due to study design or measurement limitations. Examples of such data arise in several fields, including demography,\nepidemiology and econometrics. For simplicity of exposition, \u03b8 is assumed\nto be one dimensional.\nUnder the model (5) and given that the joint distribution for (C, U, V )\ndoes not involve parameters (\u03b8, f ), the log-likelihood for a single observation\nat X = x \u2261 (c, \u03b4, u, v) is\nloglik\u03b8,f (x) = \u03b4 log {\u03a6 (c \u2212 \u03b8u \u2212 f (v))}\n(6)\n\n+(1 \u2212 \u03b4) log {1 \u2212 \u03a6 (c \u2212 \u03b8u \u2212 f (v))} ,\n\nwhere \u03a6 is the standard normal distribution. The parameter of interest, \u03b8,\nis assumed to belong to some compact set in R1 . The nuisance parameter\nis the function f , which belongs to the Sobolev function class of degree k.\nWe further make the following assumptions on this model. We assume that\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f9\n\nTHE PENALIZED PROFILE SAMPLER\n\n(Y, C) is independent given (U, V ). The covariates (U, V ) are assumed to\nbelong to some compact set, and the support for random censoring time C\nis an interval [lc , uc ], where \u2212\u221e < lc < uc < \u221e. In addition, EV ar(U |V )\nis strictly positive and Ef (V ) = 0. The first order asymptotic behaviors of\nthe penalized log-likelihood estimates of a slightly more general version of\nthis model have been extensively studied in [9].\n2.2. Semiparametric Logistic Regression. Let X1 = (Y1 , W1 , Z1 ), X2 =\n(Y2 , W2 , Z2 ), . . . be independent copies of X = (Y, W, Z), where Y is a dichotomous variable with conditional expectation E(Y |W, Z) = F (\u03b8 T W +\n\u03b7(Z)). F (u) is the logistic distribution defined as eu /(eu + 1). Obviously the\nlikelihood for a single observation is of the following form:\n(7) p\u03b8,\u03b7 (x) = F (\u03b8 T w + \u03b7(z))y (1 \u2212 F (\u03b8 T w + \u03b7(z)))1\u2212y f (W,Z)(w, z).\nThis example is a special case of quasi-likelihood in partly linear models\nwhen the conditional variance of response Y is taken to have some quadratic\nform of the conditional mean of Y . In the absence of any restrictions on the\nform of the function \u03b7, the maximum likelihood of this simple model often\nleads to over-fitting. Hence [4] propose maximizing instead the penalized\nlikelihood of the form log lik(\u03b8, \u03b7) \u2212 \u03bb2n J 2 (\u03b7); and [12] studied the asymptotic properties of the maximum penalized likelihood estimators for \u03b8 and\n\u03b7. For simplicity, we will restrict ourselves to the case where \u0398 \u2282 R1 and\n(W, Z) have bounded support, say [0, 1]2 . To ensure the identifiability of the\nparameters, we assume that EV ar(W |Z) is positive and that the support\nof Z contains at least k distinct points in [0, 1].\nRemark 1.\nimsart-aos ver.\n\nAnother interesting potential example we may apply the\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f10\n\nG. CHENG AND M. R. KOSOROK\n\npenalized profile sampler method to is the classic proportional hazards model\nwith current status data by penalizing the cumulative hazard function with its\nSobolev norm. There are two motivations for us to penalize the cumulative\nhazard function in the Cox model. One is that the estimated step functions\nfrom the unpenalized estimation cannot be used easily for other estimation\nor inference purposes. Another issue with the unpenalized approach is that\nwithout making stronger continuity assumptions, we cannot achieve uniform\nconsistency even on a compact set [9]. The asymptotic properties of the\ncorresponding penalized M-estimators have been studied in [11].\n3. Preliminaries. In this section, we present some necessary preliminary material concerning least favorable submodels, general notational conventions for the paper, and an enumeration of the main assumptions.\n3.1. Least favorable submodels. In this subsection, we briefly review the\nconcept of a least favorable submodel. A submodel t 7\u2192 pt,\u03b7t is defined to be\nleast favorable at (\u03b8, \u03b7) if l\u0303\u03b8,\u03b7 = \u2202/\u2202t log pt,\u03b7t , given t = \u03b8, where l\u0303\u03b8,\u03b7 is the\nefficient score function for \u03b8. The efficient score function for \u03b8 can be viewed\nas the projection of the score function for \u03b8 onto the tangent space of \u03b7.\nThe inverse of its variance is exactly the efficient information matrix I \u0303\u03b8,\u03b7 .\nWe abbreviate hereafter l\u0303\u03b80 ,\u03b70 and I \u0303\u03b80 ,\u03b70 with l\u03030 and I \u03030 , respectively. The\n\"direction\" along which \u03b7t approaches \u03b7 in the least favorable submodel is\ncalled the least favorable direction. An insightful review about least favorable\nsubmodels and efficient score functions can be found in Chapter 3 of [6]. By\nthe above construction of the least favorable submodel, log pl\u03bbn (\u03b8) can be\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f11\n\nTHE PENALIZED PROFILE SAMPLER\n\nrewritten in the following form:\n(8)\n\nlog pl\u03bbn (\u03b8) = l(\u03b8, \u03b8, \u03b7\u0302\u03b8,\u03bbn ) \u2212 \u03bb2n J 2 (\u03b7\u03b8 (\u03b8, \u03b7\u0302\u03b8,\u03bbn )),\n\nwhere l(t, \u03b8, \u03b7)(x) = log lik(t, \u03b7t (\u03b8, \u03b7))(x), t 7\u2192 \u03b7t (\u03b8, \u03b7) is a general map from\nthe neighborhood of \u03b8 into the parameter set for \u03b7, with \u03b7\u03b8 (\u03b8, \u03b7) = \u03b7. The\nconcrete forms of (8) will depend on the situation.\n3.2. Notation. We present in this subsection some notation that will be\nused throughout the paper. The derivatives of the function l(t, \u03b8, \u03b7) are with\nrespect to its first argument, t. For the derivatives relative to the other two\narguments \u03b8 and \u03b7, we use the following shortened notation: l\u03b8 (t, \u03b8, \u03b7) indicates the first derivative of l(t, \u03b8, \u03b7) with respect to \u03b8. Similarly, lt,\u03b8 (t, \u03b8, \u03b7)\ndenotes the derivative of l\u0307(t, \u03b8, \u03b7) with respect to \u03b8. Also, lt,t (\u03b8) and lt,\u03b8 (\u03b7)\nindicate the maps \u03b8 7\u2192 l\u0308(t, \u03b8, \u03b7) and \u03b7 7\u2192 lt,\u03b8 (t, \u03b8, \u03b7), respectively. For brevity,\n(3)\n\nwe denote l\u03070 = l\u0307(\u03b80 , \u03b80 , \u03b70 ), l\u03080 = l\u0308(\u03b80 , \u03b80 , \u03b70 ) and l0 = l(3) (\u03b80 , \u03b80 , \u03b70 ), where\n\u03b80 , \u03b70 are the true values of \u03b8 and \u03b7. Of course, we can write l\u0303(X) as l\u03070 (X).\nk * k and k * k2 indicate the Euclidean norm and L2 norm, respectively. The\n> and < mean greater than, or smaller than, up to a universal\nnotations \u223c\n\u223c\n\u221a\nconstant. The symbols Pn and Gn \u2261 n(Pn \u2212 P ) are used for the empirical\n\ndistribution and the empirical processes of the observations, respectively.\n3.3. Main Assumptions. We now make the following three classes of assumptions: Rate assumptions (R1) for the penalized nuisance parameter and\nthe estimated penalty term; Smoothness assumptions (S1-S2) and Empirical\nprocesses assumptions (E1) for l(t, \u03b8, \u03b7) and its related derivatives.\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f12\n\nG. CHENG AND M. R. KOSOROK\n\nR1 : Assume:\n(9)\n\nd(\u03b7\u0302\u03b8\u0303n ,\u03bbn , \u03b70 ) = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k)\nand\n\n(10)\n\n\u03bbn J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ) = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k).\n\nS1 : The maps\n(t, \u03b8, \u03b7) 7\u2192\n\n(11)\n\n\u2202 l+m\nl(t, \u03b8, \u03b7)\n\u2202tl \u2202\u03b8 m\n\nhave integrable envelope functions in L1 (P ) in some neighborhood of\n(\u03b80 , \u03b80 , \u03b70 ), for (l, m) = (0, 0), (1, 0), (2, 0), (3, 0), (1, 1), (1, 2), (2, 1).\nS2 : Assume:\n(12)\n\nP l\u0308(\u03b80 , \u03b80 , \u03b7) \u2212 P l\u0308(\u03b80 , \u03b80 , \u03b70 ) = O(d(\u03b7, \u03b70 )),\n\n(13)\n\nP lt,\u03b8 (\u03b80 , \u03b80 , \u03b7) \u2212 P lt,\u03b8 (\u03b80 , \u03b80 , \u03b70 ) = O(d(\u03b7, \u03b70 )),\n\n(14)\n\nP l\u0307(\u03b80 , \u03b80 , \u03b7) = O(d2 (\u03b7, \u03b70 )),\n\nfor all \u03b7 in some neighborhood of \u03b70 .\nE1 : For all random sequences \u03b8\u0303n = \u03b8\u0302n + oP (1) and \u03b8\u0304n = \u03b80 + oP (1), we\nhave\n(15)\n\n1\n\nGn (l\u0307(\u03b80 , \u03b80 , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) \u2212 l\u03070 ) = OP (n 4k+2 (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k)),\n\n(16)\n\nGn (l\u0308(\u03b80 , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )) = OP (1),\n\n(17)\n\nGn (lt,\u03b8 (\u03b80 , \u03b8\u0304n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )) = OP (1),\n\n(18) (Pn \u2212 P )l(3) (\u03b8\u0304n , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) = oP (1).\nAssumption R1 implicitly assumes that we have a metric or topology defined on the set of possible values of the nuisance parameter \u03b7. The form of\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f13\n\nTHE PENALIZED PROFILE SAMPLER\n\nd(\u03b7, \u03b70 ) may vary for different situations and does not need to be specified in\nthis subsection beyond the given conditions. (9) implies that \u03b7\u0302\u03b8\u0303n ,\u03bbn is consistent for \u03b70 as \u03b8\u0303n \u2192 \u03b80 in probability. Additionally, from (10) we know that\nthe smoothing parameter \u03bbn plays a role in determining the complexity degree of the estimated nuisance parameter. (10) implies that J(\u03b7\u0302\u03bbn ) = OP (1)\nif the \u03b8\u0302\u03bbn is asymptotically normal, which has been shown in (37). Note that\nJ(\u03b7\u0302\u03b8\u0303n ,0 ) \u2265 J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ), where \u03b7\u0302\u03b8,0 = \u03b7\u0302\u03b8 \u2261 argmax\u03b7\u2208H log lik(\u03b8, \u03b7) for a fixed\n\u03b8, based on the inequality that log lik\u03bbn (\u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,0 ) \u2264 log lik\u03bbn (\u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ).\nClearly, the assumptions S1 and S2 are separately the smoothness conditions for the Euclidean parameters (t, \u03b8) and the infinite dimensional nuisance parameter \u03b7. The boundedness of the Fr\u00e9chet derivatives of the maps\n\u03b7 7\u2192 l\u0308(\u03b80 , \u03b80 , \u03b7) and \u03b7 7\u2192 lt,\u03b8 (\u03b80 , \u03b80 , \u03b7) ensures the validity of conditions\n(12) and (13). Based on the discussions in section 2 of [3], under the given\nregularity conditions, it suffices to show (14) if the map \u03b7 7\u2192 l\u0307(\u03b80 , \u03b80 , \u03b7) is\nFr\u00e9chet differentiable and the map \u03b7 7\u2192 lik(\u03b80 , \u03b7) is second order Fr\u00e9chet\ndifferentiable.\nCondition (15) is concerned with the asymptotic equicontinuity of the\nempirical process measure of l\u0307(\u03b80 , \u03b80 , \u03b7) with \u03b7 ranging around the neighborhood of \u03b70 . It suffices to show (16) and (17) if Gn (l\u0308(\u03b80 , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) \u2212 l\u03080 ) =\noP (1) and Gn (lt,\u03b8 (\u03b80 , \u03b8\u0304n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) \u2212 lt,\u03b8 (\u03b80 , \u03b80 , \u03b70 )) = oP (1), provided l\u03080 and\nlt,\u03b8 (\u03b80 , \u03b80 , \u03b70 ) are square integrable. Thus we will be able to use technical\ntools T2 and T6 given in the appendix to show (15)\u2013(17). For the verification of (18), we need to make use of a Glivenko-Cantelli theorem for classes\nof functions that change with n which is a modification of theorem 2.4.3 in\n[21] and is explained in the appendix.\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f14\n\nG. CHENG AND M. R. KOSOROK\n\nIn principle, assumptions S1, S2 and E1 on the functions of the least\nfavorable submodel directly imply the following empirical no-bias conditions:\n(19)\n\nPn l\u0307(\u03b80 , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) = Pn l\u03030 + OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k)2 ,\n\n(20)\n\nPn l\u0308(\u03b80 , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) = P l\u03080 + OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k).\n\nThe derivations of (19) and (20) are simply based on the regular Taylor\nexpansions around the true values. The detailed arguments can be found in\nthe proof of lemmas 1 and 2 in [3]. The two empirical no-bias conditions ensure that the penalized profile likelihood behaves like a penalized likelihood\nin the parametric model asymptotically and therefore yields a second order\nasymptotic expansion of the penalized profile log-likelihood.\n4. The Penalized Convergence Rate. In the previous section, we\nhave imposed two assumptions about the convergence rates of the estimated nuisance parameter and the order of the estimated penalty term, i.e.\n(9) and (10). To compute the convergence rates, we present three different\ntheorems below which require different sets of conditions. These theorems\ncan be viewed as extension of general results on M-estimators to penalized\nM-estimators, and are therefore of independent interest. We first state the\nclassical definitions for the covering number (entropy number) and bracketing number (bracketing entropy number) for a class of functions.\nDefinition: Let A be a subset of a (pseudo-) metric space (L, d) of realvalued functions. The \u03b4-covering number N (\u03b4, A, d) of A is the smallest N\nfor which there exist functions a1 , . . . , aN in L, such that for each a \u2208 A,\nd(a, aj ) \u2264 \u03b4 for some j \u2208 {1, . . . , N }. The \u03b4-bracketing number NB (\u03b4, A, d)\nU N\nis the smallest N for which there exist pairs of functions {[aL\nj , aj ]}j=1 \u2282 L,\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f15\n\nTHE PENALIZED PROFILE SAMPLER\n\nU\nwith d(aL\nj , aj ) \u2264 \u03b4, j = 1, . . . , N , such that for each a \u2208 A there is a\nU\nj \u2208 {1, . . . , N } such that aL\nj \u2264 a \u2264 aj . The \u03b4-entropy number (\u03b4-bracketing\n\nentropy number) is defined as H(\u03b4, A, d) = log N (\u03b4, A, d) (HB (\u03b4, A, d) =\nlog NB (\u03b4, A, d)).\nBefore we present the first theorem, define\nK=\n\n\u001a\n\nl\u03b8,\u03b7 (X) \u2212 l0 (X)\n: k\u03b8 \u2212 \u03b80 k \u2264 C1 , k\u03b7 \u2212 \u03b70 k\u221e \u2264 C1 , J(\u03b7) < \u221e ,\n1 + J(\u03b7)\n\u001b\n\nfor a known constant C1 < \u221e:\nTheorem 1.\n\nAssume conditions (21), (22), (23) and (24) below hold\n\nfor every \u03b8 \u2208 \u0398n and \u03b7 \u2208 Vn :\n(21)\n\n< \u01eb\u22121/k ,\nHB (\u01eb, K, L2 (P )) \u223c\n\n(22)\n\np\u03b8,\u03b7 /p\u03b8,\u03b70 is bounded away from zero and infinity,\n\n(23)\n\n< k\u03b8 \u2212 \u03b8 k + d (\u03b7, \u03b7 ),\nkl\u03b8,\u03b7 \u2212 l0 k2 \u223c\n0\n0\n\u03b8\n\n(24)\n\n< \u2212 d2 (\u03b7, \u03b7 ) + k\u03b8 \u2212 \u03b8 k2 .\nP (l\u03b8,\u03b7 \u2212 l\u03b8,\u03b70 ) \u223c\n0\n0\n\u03b8\n\nThen we have\nd\u03b8\u0303n (\u03b7\u0302\u03b8\u0303n ,\u03bbn , \u03b70 ) = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k),\n\u03bbn J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ) = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k),\nfor (\u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) satisfying P (\u03b8\u0303n \u2208 \u0398n , \u03b7\u0302\u03b8\u0303n ,\u03bbn \u2208 Vn ) \u2192 1.\nCondition (21) determines the order of the increments of the empirical\nprocesses indexed by l\u03b8,\u03b7 . A detailed discussion about how to compute the\nincrements of the empirical processes can be found in chapter 5 of [19].\nCondition (22) is equivalent to the condition that p\u03b8,\u03b7 is bounded away\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f16\n\nG. CHENG AND M. R. KOSOROK\n\nfrom zero uniformly in x for (\u03b8, \u03b7) ranging over \u0398n \u00d7 Vn . Given that the\ndistance function d\u03b8 (\u03b7, \u03b70 ) in (23) is just kp\u03b8,\u03b7 \u2212 p0 k2 , (23) trivially holds\nprovided that condition (22) holds. For the verification of (24), we can do an\nanalysis as follows. The natural Taylor expansions of the criterion function\n(\u03b8, \u03b7) 7\u2192 P l\u03b8,\u03b7 around the maximum point (\u03b80 , \u03b70 ) implies that P (l\u03b8,\u03b70 \u2212\nR \u221a\n> \u2212 k\u03b8 \u2212 \u03b8 k2 , and (46) implies that P (l\nl\u03b80 ,\u03b70 ) \u223c\n0\n\u03b8,\u03b7 \u2212 l0 ) \u2264 \u2212 ( p\u03b8,\u03b7 \u2212\n\u221a 2\np0 ) d\u03bc \u2264 \u2212kp\u03b8,\u03b7 \u2212 p0 k22 given condition (22).\nWe now apply theorem 1 to derive the related convergence rates in the\npartly linear model in corollary 1. However, we need to strengthen our previous assumptions to require the existence of a known M < \u221e such that\n\u03b7 \u2208 HkM , where HkM = Hk \u2229 {k\u03b7k\u221e \u2264 M } and that the density for the joint\ndistribution (U, V, C) is strictly positive and finite. The additional assumptions here guarantee condition (22). The following theorem 2 and theorem 3\ncan also be employed to derive the convergence rate of the non-penalized\nestimated nuisance parameter by setting \u03bbn to zero. However, we would\nneed to assume that f \u2208 {g : kgk\u221e + J(g) \u2264 M\u0303 } for some known M\u0303 when\napplying these theorems. Thus we can argue that the the penalized method\nenables a relaxation of the assumptions needed for the nuisance parameter.\nCorollary 1.\n\nUnder the above set-up for the partly linear normal\n\nmodel with current status data, we have, for \u03b8\u0303n = \u03b80 + oP (1),\n(25)\n\nkf\u02c6\u03b8\u0303n ,\u03bbn \u2212 f0 k2 = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k),\n\n(26)\n\n\u03bbn J(f\u02c6\u03b8\u0303n ,\u03bbn ) = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k).\n\nMoreover, if we also assume that f \u2208 {g : kgk\u221e + J(g) \u2264 M\u0303 } for some\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f17\n\nTHE PENALIZED PROFILE SAMPLER\n\nknown M\u0303 , then\n(27)\n\nkf\u02c6\u03b8\u0303n \u2212 f0 k2 = OP (n\u2212k/(2k+1) + k\u03b8\u0303n \u2212 \u03b80 k),\n\nprovided condition (3) holds.\nRemark 2.\n\nCorollary 1 implies that the convergence rate of the es-\n\ntimated nuisance parameter is slower than that of the the regular nuisance\nparameter by comparing (25) and (27). This result is not surprising since\nthe slower rate is the trade off for the smoother nuisance parameter estimator. However the advantage of the penalized profile sampler is that we\ncan control the convergence rate by assigning the smoothing parameter with\ndifferent rates. Corollary 1 also indicates that kf\u02c6\u03bbn \u2212 f0 k2 = OP (\u03bbn ) and\nkf\u02c6n \u2212 f0 k2 = OP (n\u2212k/(k+2) ). Note that the convergence rate of the maximum\npenalized likelihood estimator, OP (\u03bbn ), is deemed as the optimal rate in [22].\nSimilar remarks also hold for corollary 2 below.\nThe boundedness condition (22) appears hard to achieve in some examples. Hence we propose theorem 2 below to relax this condition by choosing\nthe criterion function m\u03b8,\u03b7 = log[(p\u03b8,\u03b7 + p\u03b8,\u03b70 )/2p\u03b8,\u03b70 ]. Obviously, m\u03b8,\u03b7 is\ntrivially bounded away from zero. It is also bounded above for (\u03b8, \u03b7) around\nthe their true values if p\u03b8,\u03b70 (x) is bounded away from zero uniformly in x and\np\u03b8,\u03b7 is bounded above. The first condition is satisfied if the map \u03b8 7\u2192 p\u03b8,\u03b70 (x)\nis continuous around \u03b80 and p0 (x) is uniformly bounded away from zero. The\nsecond condition is trivially satisfied in the semiparametric logistic regression model by the given form of the density. The boundedness of m\u03b8,\u03b7 thus\npermits the application of lemma 1 below which is used to verify condition\n(29) in the following theorem:\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f18\n\nG. CHENG AND M. R. KOSOROK\n\nTheorem 2.\n\nAssume for any given \u03b8 \u2208 \u0398n , \u03b7\u0302\u03b8 satisfies Pn m\u03b8,\u03b7\u0302\u03b8 \u2265\n\nPn m\u03b8,\u03b70 for given measurable functions x 7\u2192 m\u03b8,\u03b7 (x). Assume conditions\n(28) and (29) below hold for every \u03b8 \u2208 \u0398n , every \u03b7 \u2208 Vn and every \u01eb > 0:\n(28)\n\n< \u2212 d2 (\u03b7, \u03b7 ) + k\u03b8 \u2212 \u03b8 k2 ,\nP (m\u03b8,\u03b7 \u2212 m\u03b8,\u03b70 ) \u223c\n0\n0\n\u03b8\n\n(29)\n\nE\u2217\n\nsup\n\u03b8\u2208\u0398n ,\u03b7\u2208Vn ,k\u03b8\u2212\u03b80 k<\u01eb,d\u03b8 (\u03b7,\u03b70 )<\u01eb\n\n< \u03c6 (\u01eb).\n|Gn (m\u03b8,\u03b7 \u2212 m\u03b8,\u03b70 )| \u223c\nn\n\nSuppose that (29) is valid for functions \u03c6n such that \u03b4 7\u2192 \u03c6n (\u03b4)/\u03b4\u03b1 is decreasing for some \u03b1 < 2 and sets \u0398n \u00d7Vn such that P (\u03b8\u0303 \u2208 \u0398n , \u03b7\u0302\u03b8\u0303 \u2208 Vn ) \u2192 1.\nThen d\u03b8\u0303 (\u03b7\u0302\u03b8\u0303 , \u03b70 ) \u2264 OP\u2217 (\u03b4n + k\u03b8\u0303 \u2212 \u03b80 k) for any sequence of positive numbers\n\u221a\n\u03b4n such that \u03c6n (\u03b4n ) \u2264 n\u03b4n2 for every n.\nLemma 1 below is presented to verify the modulus condition for the continuity of the empirical process in (29). Let S\u03b4 = {x 7\u2192 m\u03b8,\u03b7 (x) \u2212 m\u03b8,\u03b70 (x) :\nd\u03b8 (\u03b7, \u03b70 ) < \u03b4, k\u03b8 \u2212 \u03b80 k < \u03b4} and write\n(30)\n\nK(\u03b4, S\u03b4 , L2 (P )) =\nLemma 1.\n\nZ\n\n0\n\n\u03b4\n\nq\n\n1 + HB (\u01eb, S\u03b4 , L2 (P ))d\u01eb :\n\nSuppose the functions (x, \u03b8, \u03b7) 7\u2192 m\u03b8,\u03b7 (x) are uniformly\n\nbounded for (\u03b8, \u03b7) ranging over a neighborhood of (\u03b80 , \u03b70 ) and that\n< d2 (\u03b7, \u03b7 ) + k\u03b8 \u2212 \u03b8 k2 .\nP (m\u03b8,\u03b7 \u2212 m\u03b80 ,\u03b70 )2 \u223c\n0\n0\n\u03b8\n\nThen condition (29) is satisfied for any functions \u03c6n such that\nK(\u03b4, S\u03b4 , L2 (P ))\n\u221a\n\u03c6n (\u03b4) \u2265 K(\u03b4, S\u03b4 , L2 (P )) 1 +\n\u03b42 n\n\u0012\n\n\u0013\n\nConsequently, in the conclusion of the above theorem we may use K(\u03b4, S\u03b4 , L2 (P ))\nrather than \u03c6n (\u03b4).\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f19\n\nTHE PENALIZED PROFILE SAMPLER\n\nRemark 3.\n\nTheorem 2 and lemma 1 are theorem 3.2 and lemma 3.3\n\nin [14], respectively. We can apply theorem 2 to the penalized semiparametric\nlogistic regression model by including \u03bb in \u03b8, i.e. m\u03b8,\u03bb,\u03b7 = m\u03b8,\u03b7 \u2212 12 \u03bb2 (J 2 (\u03b7)\u2212\nJ 2 (\u03b70 )). This is accomplished in the following corollary. Note that we assume\nthat the uniform norm and Sobolev norm of \u03b7 are bounded above with known\nupper bounds when deriving (33) of the corollary, but this assumption is not\nneeded for (31) and (32).\nCorollary 2.\n\nUnder the above set-up for the semiparametric logistic\np\n\nregression model, we have for \u03bbn satisfying condition (3) and any \u03b8\u0303n \u2192 \u03b80\nthat\n(31)\n\nk\u03b7\u0302\u03b8\u0303n ,\u03bbn \u2212 \u03b70 k2 = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k),\n\n(32)\n\n\u03bbn J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ) = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k).\n\nIf we also assume that \u03b7 \u2208 {g : kgk\u221e + J(g) \u2264 M\u0303 } for some known M\u0303 , then\n(33)\n\nk\u03b7\u0302\u03b8\u0303n \u2212 \u03b70 k2 = OP (n\u2212k/(2k+1) + k\u03b8\u0303n \u2212 \u03b80 k).\nRemark 4.\n\nCorollary 1 and 2 imply that J(\u03b7\u0302\u03bbn ) = OP (1) and J(f\u02c6\u03bbn ) =\n\nOP (1), respectively. Thus the maximum likelihood estimators of the nuisance\nparameters in the two examples of this paper are consistent in the uniform\nnorm, i.e. k\u03b7\u0302\u03bbn \u2212 \u03b70 k\u221e = oP (1) and kf\u02c6\u03bbn \u2212 f0 k\u221e = oP (1), since the sequences \u03b7\u0302n and f\u02c6n consist of smooth functions defined on a compact set with\nasymptotically bounded first-order derivatives.\nThe preceding two theorems imply that the convergence rate of the penalized estimated nuisance parameter is affected by the assigned smoothness\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f20\n\nG. CHENG AND M. R. KOSOROK\n\nparameter. However, the next theorem shows that, under different conditions, the above phenomena may not hold. Let\n\u03bbn\n=\nl\u03b8,\u03b7,h\n\n\u2202\n|t=0 log lik\u03bbn (\u03b8, \u03b7t ) = A\u03b8,\u03b7 h \u2212 2\u03bb2n\n\u2202t\n\nZ\n\nh(k) \u03b7 (k) dz,\n\nV (\u03b8, \u03b7)h = P A\u03b8,\u03b7 h,\nVn (\u03b8, \u03b7)h = Pn A\u03b8,\u03b7 h,\nwhere \u03b7t = \u03b7 + th for h \u2208 Hk and A\u03b8,\u03b7 is the appropriate score operator for the model. Note that \u03b7t \u2208 Hk for sufficiently small t. Obviously Pn l\u03b8\u0303\u03bbn,\u03b7\u0302\nn\n\n\u03b8\u0303n ,\u03bbn ,h\n\n= 0 and V (\u03b80 , \u03b70 )h = 0. We assume that the maps\n\nh 7\u2192 V (\u03b8, \u03b7)h and h 7\u2192 Vn (\u03b8, \u03b7)h are uniformly bounded such that Vn\nand V can be viewed as maps from the parameters set \u0398 \u00d7 Hk into l\u221e (Hk ).\nFurther we require the following regularity conditions: For some C2 > 0,\n(34)\n\n{A\u03b8,\u03b7 h : k\u03b8 \u2212 \u03b80 k < C2 , d\u03b8 (\u03b7, \u03b70 ) < C2 , h \u2208 Hk } is P -Donsker,\n\n(35)\n\nsup P (A\u03b8,\u03b7 h \u2212 A\u03b80 ,\u03b70 h)2 \u2192 0, as \u03b8 \u2192 \u03b80 and \u03b7 \u2192 \u03b70 .\n\nh\u2208Hk\n\nTheorem 3.\n\nSuppose that V (*, *) : \u0398 \u00d7 Hk 7\u2192 l\u221e (Hk ) is Fr\u00e9chet\n\ndifferentiable at (\u03b80 , \u03b70 ) with derivative V\u0307 (*, *) : Rd \u00d7 linHk 7\u2192 l\u221e (Hk ) such\nthat the map V\u0307 (0, *) : linHk 7\u2192 l\u221e (Hk ) is invertible with an inverse that is\ncontinuous on its range. Furthermore, we assume that (34) and (35) hold.\nThen\n(36)\n\nd\u03b8\u0303n (\u03b7\u0302\u03b8\u0303n ,\u03bbn , \u03b70 ) = OP (n\u22121/2 + k\u03b8\u0303n \u2212 \u03b80 k + \u03bb2n J 2 (\u03b7\u0302\u03b8\u0303n ,\u03bbn )),\n\nfor \u03b8\u0303n \u2192 \u03b80 and \u03b7\u0302\u03b8\u0303n ,\u03bbn \u2192 \u03b70 in probability.\nRemark 5.\n\nThe preceding theorem is a variation of theorems used\n\nin [13] and [20], among others, to prove the asymptotic normality of the\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f21\n\nTHE PENALIZED PROFILE SAMPLER\n\nmaximum likelihood estimator (\u03b8\u0302n , \u03b7\u0302n ). If we can show that \u03bbn J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ) =\nOP (\u03bbn +k\u03b8\u0303n \u2212\u03b80 k) by some other means, then (36) implies that d\u03b8\u0303n (\u03b7\u0302\u03b8\u0303n ,\u03bbn , \u03b70 ) =\nOP (n\u22121/2 + k\u03b8\u0303n \u2212 \u03b80 k). This indicates that the smoothing effect of the penalized method does not occur, which may be due to some very smooth nonpenalized estimated nuisance parameter. The high degree of the smoothness\nof the non-penalized estimated nuisance parameter can be deduced from its\nfast convergence rate which equals the parametric rate in this instance.\n5. Main Results and Implications. In this section we first present\nsecond order asymptotic expansion of the log-profile penalized likelihood\nwhich prepare us for deriving the main results about the higher order structure of the penalized profile sampler. The assumptions in section 3 and\ncondition (3) are assumed throughout.\nTheorem 4.\n(37)\n(38)\n\nGiven \u03b8\u0303n = \u03b8\u0302\u03bbn + oP (1), we have\n\nn\n1 X\n\u221a\nI \u0303\u22121 l\u03030 (Xi ) + OP (n1/2 \u03bb2n ),\nn i=1 0\nn\nlog pl\u03bbn (\u03b8\u0303n ) = log pl\u03bbn (\u03b8\u0302\u03bbn ) \u2212 (\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn )T I \u03030 (\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn )\n2\n\n\u221a\n\nn(\u03b8\u0302\u03bbn \u2212 \u03b80 ) =\n\n+ OP (g\u03bbn (k\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn k)),\nwhere g\u03bbn (w) = nw3 + nw2 \u03bbn + nw\u03bb2n + n1/2 \u03bb2n , provided the efficient information I \u03030 is positive definite.\nRemark 6.\n\nThe results in theorem 4 are useful in there own right\n\nfor inference about \u03b8. (37) is a second higher order frequentist result in\npenalized semiparametric estimation regarding the asymptotic linearity of\nthe maximum penalized likelihood estimator of \u03b8.\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f22\n\nG. CHENG AND M. R. KOSOROK\n\nWe now state the main results on the penalized posterior profile distribution. A preliminary result, theorem 5 with corollary 3 below, shows that\nthe penalized posterior profile distribution is asymptotically close enough to\nthe distribution of a normal random variable with mean \u03b8\u0302\u03bbn and variance\n(nI \u03030 )\u22121 with second order accuracy, which is controlled by the smoothing parameter. Similar conclusions also hold for the penalized posterior moments.\nAnother main result, theorem 6, shows that the penalized posterior profile log-likelihood can be used to achieve second order accurate frequentist\ninference for \u03b8.\nLet P\u0303\u03b8|\u03bbnX\u0303 be the penalized posterior profile distribution of \u03b8 with respect\nto the prior \u03c1(\u03b8). Define\n\u2206\u03bbn (\u03b8) \u2261 n\u22121 {log pl\u03bbn (\u03b8) \u2212 log pl\u03bbn (\u03b8\u0302\u03bbn )}\n= n\u22121 (ln (\u03b8, \u03b7\u0302\u03b8,\u03bbn ) \u2212 ln (\u03b8\u0302\u03bbn , \u03b7\u0302\u03bbn )) \u2212 n\u22121 \u03bb2n (J 2 (\u03b7\u0302\u03b8,\u03bbn ) \u2212 J 2 (\u03b7\u0302\u03bbn )).\nTheorem 5.\n(39)\n\nAssume that\n\n\u2206\u03bbn (\u03b8\u0303n ) = oP (1) implies \u03b8\u0303n = \u03b80 + oP (1),\nn\n\no\n\nfor every random \u03b8\u0303n \u2208 \u0398. If \u03c1(\u03b80 ) > 0 and \u03c1(*) has continuous and finite\nfirst order derivative in some neighborhood of \u03b80 , then we have, for any\n\u2212\u221e < \u03be < \u221e,\n(40)\n\n\u221a 1/2\nsup P\u0303\u03b8|\u03bbnX\u0303 ( nI \u03030 (\u03b8 \u2212 \u03b8\u0302\u03bbn ) \u2264 \u03be) \u2212 \u03a6d (\u03be) = OP (n1/2 \u03bb2n ),\n\n\u03be\u2208Rd\n\nwhere \u03a6d (*) is the distribution of the d-dimensional standard normal random\nvariable.\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\fTHE PENALIZED PROFILE SAMPLER\n\nCorollary 3.\n\n23\n\nUnder the assumptions of theorem 5, we have that if\n\n\u03b8 has finite second absolute moment, then\n(41)\n\n\u03b8\u0302\u03bbn\n\n\u03bbn\n(\u03b8) + OP (\u03bb2n ),\n= E\u03b8|\nX\u0303\n\n\u03bbn\n(\u03b8))\u22121 + OP (n1/2 \u03bb2n ),\nI \u03030 = n\u22121 (V ar\u03b8|\nX\u0303\n\n(42)\n\n\u03bbn\n\u03bbn\n(\u03b8) are the penalized posterior profile mean and\n(\u03b8) and V ar\u03b8|\nwhere E\u03b8|\nX\u0303\nX\u0303\n\npenalized posterior profile covariance matrix, respectively.\nWe now present another second order asymptotic frequentist property of\nthe penalized profile sampler in terms of quantiles. The \u03b1-th quantile of\nthe penalized posterior profile distribution, \u03c4n\u03b1 , is defined as \u03c4n\u03b1 = inf {\u03be :\nP\u0303\u03b8|\u03bbnX\u0303 (\u03b8 \u2264 \u03be) \u2265 \u03b1}. Without loss of generality, P\u0303\u03b8|\u03bbnX\u0303 (\u03b8 \u2264 \u03c4n\u03b1 ) = \u03b1. We can\n\u221a\n\u221a\nalso define \u03ban\u03b1 \u2261 n(\u03c4n\u03b1 \u2212 \u03b8\u0302\u03bbn ), i.e., P\u0303\u03b8|\u03bbnX\u0303 ( n(\u03b8 \u2212 \u03b8\u0302\u03bbn ) \u2264 \u03ban\u03b1 ) = \u03b1.\nTheorem 6.\n\nUnder the assumptions of theorem 5 and assuming that\n\nl\u03030 (X) has finite third moment with a nondegenerate distribution, then there\n\u221a\nexists a \u03ba\u0302n\u03b1 based on the data such that P ( n(\u03b8\u0302\u03bbn \u2212 \u03b80 ) \u2264 \u03ba\u0302n\u03b1 ) = \u03b1 and\n\u03ba\u0302n\u03b1 \u2212 \u03ban\u03b1 = OP (n1/2 \u03bb2n ) for each choice of \u03ban\u03b1 .\nRemark 7.\n\nTheorem 6 ensures that there exists a unique \u03b1-th quan-\n\ntile for \u03b8 up to OP (\u03bb2n ) in the frequentist set-up for each fixed \u03c4n\u03b1 . Note that\n\u03c4n\u03b1 is not unique if the dimension of \u03b8 is larger than one.\nRemark 8.\n\nTheorem 5, corollary 3 and theorem 6 above show that\n\nthe penalized profile sampler generates second order asymptotic frequentist\nvalid results in terms of distributions, moments and quantiles. Moreover,\nthe second order accuracy of this procedure is controlled by the smoothing\nparameter.\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f24\n\nG. CHENG AND M. R. KOSOROK\n\nRemark 9.\n\nAnother interpretation for the role of \u03bbn in the penalized\n\nprofile sampler is that we can view \u03bbn as the prior on J(\u03b7), or on \u03b7 to some\nextent. To see this, we can write lik\u03bbn (\u03b8, \u03b7) in the following form:\n\uf8ee\n\nlik\u03bbn (\u03b8, \u03b7) = likn (\u03b8, \u03b7) \u00d7 exp \uf8f0\u2212\n\nJ 2 (\u03b7)\n2( 2\u03bb12 )\nn\n\n\uf8f9\n\uf8fb\n\nThis idea can be traced back to [22]. In other words, the prior on J(\u03b7) is a\nnormal distribution with mean zero and variance (2\u03bb2n )\u22121 . Hence it is natural\nto expect \u03bbn has some effect on the convergence rate of \u03b7. Other possible\npriors on the functional parameter include Dirichlet and Gaussian processes\nwhich are more commonly used in nonparametric Bayesian methodology.\n6. Examples (Continued). We now illustrate verification of the assumptions in section 3.3 with the two example that were introduced in section 2. Thus this section is a continuation of the earlier examples.\n6.1. Partly Linear Normal Model with Current Status Data. We will concentrate on the estimation of the regression coefficient \u03b8, considering the\ninfinite dimensional parameter f \u2208 HkM as a nuisance parameter. The score\nfunction of \u03b8, l\u0307\u03b8,f , is given as follows:\nl\u0307\u03b8,f (x) = uQ(x; \u03b8, f ),\nwhere\nQ(X; \u03b8, f ) = (1 \u2212 \u2206)\n\n\u03c6(q\u03b8,f (X))\n\u03c6(q\u03b8,f (X))\n\u2212\u2206\n,\n1 \u2212 \u03a6(q\u03b8,f (X))\n\u03a6(q\u03b8,f (X))\n\nq\u03b8,f (x) = c \u2212 \u03b8u \u2212 f (v), and \u03c6 is the density of a standard normal random\nvariable. The least favorable direction at the true parameter value is:\nh0 (v) =\nimsart-aos ver.\n\nE0 (U Q2 (X; \u03b8, f )|V = v)\n,\nE0 (Q2 (X; \u03b8, f )|V = v)\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f25\n\nTHE PENALIZED PROFILE SAMPLER\n\nwhere E0 is the expectation relative to the true parameters. The derivation\nof l\u0307\u03b8,f and h0 (*) is given in [3]. Thus, the least favorable submodel can be\nconstructed as follows:\n(43)\n\nl(t, \u03b8, f ) = log lik(t, ft (\u03b8, f )),\n\nwhere ft (\u03b8, f ) = f + (\u03b8 \u2212 t)h0 . By differentiating (43) with respect to\nt or \u03b8, we can obtain the maps assessed in assumption S1, (t, \u03b8, f ) 7\u2192\n(\u2202 l+m /\u2202tl \u2202\u03b8 m )l(t, \u03b8, f ). The concrete forms of these maps are given in [3]\nwhich considers a more rigid model with a known upper bound on the L2\nnorm of the kth derivative. The rate assumptions (9) and (10) have been\nverified previously in corollary 1. The remaining assumptions are verified in\nthe following two lemmas:\nLemma 2.\n\nUnder the above set-up for the partly linear normal model\n\nwith current status data, assumptions S1, S2 and E1 are satisfied.\nLemma 3.\n\nUnder the above set-up for the partly linear normal model\n\nwith current status data, condition (39) is satisfied.\n6.2. Semiparametric Logistic Regression. In the semiparametric logistic\nregression model, we can obtain the score function for \u03b8 and \u03b7 by similar\nanalysis performed in the first example, i.e. l\u0307\u03b8,\u03b7 (x) = (y \u2212 F (\u03b8w + \u03b7(z)))w\nand A\u03b8,\u03b7 h\u03b8,\u03b7 (x) = (y \u2212 F (\u03b8w + \u03b7(z)))h\u03b8,\u03b7 (z) for J(h) < \u221e. And the least\nfavorable direction at the true parameter is given in [14]:\nh0 (z) =\n\nP0 [W \u1e1e (\u03b80 W + \u03b70 (Z))|Z = z]\n,\nP0 [\u1e1e (\u03b80 W + \u03b70 (Z))|Z = z]\n\nwhere \u1e1e (u) = F (u)(1 \u2212 F (u)). The above assumptions plus the requirement\nthat J(h0 ) < \u221e ensures the identifiability of the parameters. Thus the least\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f26\n\nG. CHENG AND M. R. KOSOROK\n\nfavorable submodel can be written as:\nl(t, \u03b8, \u03b7) = log lik(t, \u03b7t (\u03b8, \u03b7)),\nwhere \u03b7t (\u03b8, \u03b7) = \u03b7 + (\u03b8 \u2212 t)h0 . By differentiating l(t, \u03b8, \u03b7) with respect to t\nor \u03b8, we obtain,\nl\u0307(t, \u03b8, \u03b7) = (y \u2212 F (tw + \u03b7(z) + (\u03b8 \u2212 t)h0 (z)))(w \u2212 h0 (z)),\nl\u0308(t, \u03b8, \u03b7) = \u2212\u1e1e (tw + \u03b7(z) + (\u03b8 \u2212 t)h0 (z))(w \u2212 h0 (z))2 ,\nlt,\u03b8 (t, \u03b8, \u03b7) = \u2212\u1e1e (tw + \u03b7(z) + (\u03b8 \u2212 t)h0 (z))(w \u2212 h0 (z))h0 (z),\nl(3) (t, \u03b8, \u03b7) = \u2212F\u0308 (tw + \u03b7(z) + (\u03b8 \u2212 t)h0 (z))(w \u2212 h0 (z))3 ,\nlt,t,\u03b8 (t, \u03b8, \u03b7) = \u2212F\u0308 (tw + \u03b7(z) + (\u03b8 \u2212 t)h0 (z))(w \u2212 h0 (z))2 h0 (z),\nlt,\u03b8,\u03b8 (t, \u03b8, \u03b7) = \u2212F\u0308 (tw + \u03b7(z) + (\u03b8 \u2212 t)h0 (z))(w \u2212 h0 (z))h20 (z),\nwhere F\u0308 (*) is the second derivative of the function F (*). The rate assumptions have been shown in corollary 2. The remaining assumptions are verified\nin the following two lemmas:\nLemma 4.\n\nUnder the above set-up for the semiparametric logistic re-\n\ngression model, assumptions S1, S2 and E1 are satisfied.\nLemma 5.\n\nUnder the above set-up for the semiparametric logistic re-\n\ngression model, condition (39) is satisfied.\n7. Future Work. Our paper evaluates the penalized profile sampler\nmethod from the frequentist view and discusses the effect of the smoothing\nparameter on estimation accuracy. One potential problem of interest is how\nto select a proper smoothing parameter in applications. A formal study\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f27\n\nTHE PENALIZED PROFILE SAMPLER\n\nabout the higher order comparisons between the profile sampler procedure\nand fully Bayesian procedure [16], which assign priors to both the finite\ndimensional parameter and the infinite dimensional nuisance parameter, is\nalso interesting. We expect that the involvement of a suitable prior on the\ninfinite dimensional parameter would at least not decrease the estimation\naccuracy of the parameter of interest.\nAnother worthwhile avenue of research is to develop analogs of the profile\nsampler and penalized profile sampler to likelihood estimation under model\nmisspecification and to general M-estimation. Some first order results for\nthis setting in the case where the nuisance parameter may not be root-n\nconsistent have been developed for a weighted bootstrap procedure in [10].\n8. Appendix. We first present some technical tools about the entropy\ncalculations and increments of empirical processes which will be employed\nin the proofs that follow.\nT1. For each 0 < C < \u221e and \u03b4 > 0 we have\n(44)\n(45)\n\n< ( C )1/k ,\nHB (\u03b4, {\u03b7 : k\u03b7k\u221e \u2264 C, J(\u03b7) \u2264 C}, k * k\u221e ) \u223c\n\u03b4\nC\n<\nH(\u03b4, {\u03b7 : k\u03b7k\u221e \u2264 C, J(\u03b7) \u2264 C}, k * k\u221e ) \u223c ( )1/k .\n\u03b4\n\nT2. Let F be a class of measurable functions such that P f 2 < \u03b42 and\nkf k\u221e \u2264 M for every f in F. Then\n<\nEP\u2217 kGn kF \u223c\n\nK(\u03b4, F, L2 (P ))\n\u221a\nM ,\nK(\u03b4, F, L2 (P )) 1 +\n\u03b42 n\n\u0013\n\n\u0012\n\nwhere K(\u03b4, F, k * k) =\n\nR\u03b4p\n0\n\n1 + HB (\u01eb, F, k * k)d\u01eb.\n\nT3. Let F = {ft : t \u2208 T } be a class of functions satisfying |fs (x)\u2212ft (x)| \u2264\nd(s, t)F (x) for every s and t and some fixed function F . Then, for any norm\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f28\n\nG. CHENG AND M. R. KOSOROK\n\nk * k,\nN[] (2\u01ebkF k, F, k * k) \u2264 N (\u01eb, T, d).\nT4.\np\u03b8\n\u2265\n\u2212 P\u03b80 log\np \u03b80\n\n(46)\n\nZ\n\n\u221a\n\np\u03b8 \u2212\n\n\u221a\n\np \u03b80\n\n\u00012\n\nd\u03bc.\n\nT5. Let F be a class of measurable functions f : D \u00d7 W 7\u2192 R on a\nproduct of a finite set and an arbitrary measurable space (W, W). Let P be\na probability measure on D \u00d7 W and let PW be its marginal on W. For every\nd \u2208 D, let Fd be the set of functions w 7\u2192 f (d, w) as f ranges over F. If\nevery class Fd is P -Donsker with supf \u2208F |P f (d, W )| < \u221e for every d, then\nF is P -Donsker.\nT6. Let F be a uniformly bounded class of measurable functions such\nthat for some measurable f0 , supf \u2208F kf \u2212 f0 k\u221e < \u221e. Moreover, assume\nthat HB (\u01eb, F, L2 (P )) \u2264 K\u01eb\u2212\u03b1 for some K < \u221e and \u03b1 \u2208 (0, 2) and for all\n\u01eb > 0. Then\nsup\nf \u2208F\n\n\"\n\nkf \u2212\n\n|(Pn \u2212 P )(f \u2212 f0 )|\n\n1\u2212\u03b1/2\nf0 k2\n\n\u2228\n\nn(\u03b1\u22122)/[2(2+\u03b1)]\n\n#\n\n= OP (n\u22121/2 ).\n\nT7. For a probability measure P , let F1 be a class of measurable functions\nf1 : X 7\u2192 R, and let F2 denote a class of nondecreasing functions f2 : R 7\u2192\n[0, 1] that are measurable for every probability measure. Then,\nHB (\u01eb, F2 (F1 ), L2 (P )) \u2264 2HB (\u01eb/3, F1 , L2 (P )) + sup HB (\u01eb/3, F2 , L2 (Q)).\nQ\n\nT8. Let F and G be classes of measurable functions. Then for any probability measure Q and any 1 \u2264 r \u2264 \u221e,\n(47) HB (2\u01eb, F + G, Lr (Q)) \u2264 HB (\u01eb, F, Lr (Q)) + HB (\u01eb, G, Lr (Q)),\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f29\n\nTHE PENALIZED PROFILE SAMPLER\n\nand, provided F and G are bounded by 1,\n(48) HB (2\u01eb, F \u00d7 G, Lr (Q)) \u2264 HB (\u01eb, F, Lr (Q)) + HB (\u01eb, G, Lr (Q)).\nRemark 10.\n\nThe proof of T1 is found in [1]. T1 implies that the\n\nSobolev class of functions with known bounded Sobolev norm is P -Donsker.\nT2 and T3 are separately lemma 3.4.2 and theorem 2.7.11 in [21]. (46) in\nT4 relates the Kullback-Leibler divergence and Hellinger distance. Its proof\n\u221a\ndepends on the inequality that log x \u2264 2( x\u22121) for every x > 0. T5 is lemma\n9.2 in [15]. T6 is a result presented on page 79 of [19] and is a special case\nof lemma 5.13 on the same page, the proof of which can be found in pages\n79\u201380. T7 and T8 are separately lemma 15.2 and 9.24 in [6].\nProof of theorem 1: The definition of \u03b7\u0302\u03b8\u0303n ,\u03bbn implies that\n\u03bb2n J 2 (\u03b7\u0302\u03b8\u0303n ,\u03bbn )\n\n\u2264\n\n\u03bb2n J 2 (\u03b70 )\n\u0012\n\n\u0012\n\n+ (Pn \u2212 P ) l\u03b8\u0303n ,\u03b7\u0302\n\n+ P l\u03b8\u0303n ,\u03b7\u0302\n\n\u03b8\u0303n ,\u03bbn\n\n\u2212 l\u03b8\u0303n ,\u03b70\n\n\u2264 \u03bb2n J 2 (\u03b70 ) + I + II.\n\n\u03b8\u0303n ,\u03bbn\n\n\u2212 l\u03b8\u0303n ,\u03b70\n\n\u0013\n\n\u0013\n\nNote that by T6 and assumption (21), we have\nI \u2264 (1 + J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ))OP (n\u22121/2 ) \u00d7\n+(1 + J(\u03b70 ))OP (n\u22121/2 ) \u00d7\nBy assumption (24), we have\n\n\uf8f1\n\uf8f4\n\uf8f2 l\u03b8\u0303n ,\u03b7\u0302\n\n\u03b8\u0303n ,\u03bbn\n\n\u2212 l0\n\n\uf8f4\n\uf8f3 1 + J(\u03b7\u0302\u03b8\u0303n ,\u03bbn )\n\uf8f1\n1\n\uf8f2 l\n\u2212 l0 1\u2212 2k\n\u03b8\u0303n ,\u03b70\n\n\uf8f3 1 + J(\u03b70 )\n\n2\n\n1\n1\u2212 2k\n\n2\n\n\u2228n\n\n\u2228n\n\n2k\u22121\n\u2212 2(2k+1)\n\n2k\u22121\n\u2212 2(2k+1)\n\n\uf8fc\n\uf8fd\n\n.\n\n\uf8fc\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8fe\n\n\uf8fe\n\n< \u2212 d2 (\u03b7\u0302\nII \u223c\n, \u03b7 ) + k\u03b8\u0303n \u2212 \u03b80 k2 .\n\u03b8\u0303n \u03b8\u0303n ,\u03bbn 0\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f30\n\nG. CHENG AND M. R. KOSOROK\n\nCombining with the above, we can deduce that\nd\u02c62n + \u03bb2n J\u02c6n2\n\n<\n\n\u223c\n+\n\n(49)\n\n+\n\n\uf8fc\n\uf8f1\n! 1\n\uf8f2 d\u02c6 + k\u03b8\u0303 \u2212 \u03b8 k 1\u2212 2k\n2k\u22121 \uf8fd\n\u2212\nn\nn\n0\n\u2228 n 2(2k+1)\n(1 + J\u02c6n )OP (n\u22121/2 ) \u00d7\n\uf8fe\n\uf8f3\n1 + J\u02c6n\n\uf8fc\n\uf8f1\n! 1\n\uf8f2 k\u03b8\u0303 \u2212 \u03b8 k 1\u2212 2k\n2k\u22121 \uf8fd\n\n(1 + J0 )OP (n\u22121/2 ) \u00d7\n\n\u03bb2n J02 + k\u03b8\u0303n \u2212 \u03b80 k2 ,\n\nn\n\n\uf8f3\n\n0\n\n\u2228n\n\n1 + J0\n\n\u2212 2(2k+1)\n\n\uf8fe\n\nwhere d\u02c6n = d\u03b8\u0303n (\u03b7\u0302\u03b8\u0303n ,\u03bbn , \u03b70 ), J(\u03b70 ) = J0 and J\u02c6n = J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ). The above inequality follows from assumption (23). Combining all of the above inequalities, we can deduce that\n1\n1\u2212 2k\n\n(50) u2n = OP (1) + OP (1)un\n\n,\n1\n1\u2212 2k\n\n(51) vn = vn\u22121 OP (k\u03b8\u0303n \u2212 \u03b80 k2 ) + un\n\n1\n\n1\n\n1\u2212 2k\nOP (\u03bbn ) + OP (n\u2212 2 \u03bb\u22121\n),\nn k\u03b8\u0303n \u2212 \u03b80 k\n\nwhere un = (d\u02c6n + k\u03b8\u0303n \u2212 \u03b80 k)/(\u03bbn + \u03bbn J\u02c6n ) and vn = \u03bbn J\u02c6n + \u03bbn . The equation\n(50) implies that un = OP (1). Inserting un = OP (1) into (51), we can know\nthat vn = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k), which implies un has the desired order. This\ncompletes the whole proof. \u0003\nProof of corollary 1: Conditions (22)\u2013(24) can be verified easily in this\nexample based on the arguments in theorem 1 because l\u0308\u03b8,f has finite second\nmoment, and p\u03b8,f is bounded away from zero and infinity uniformly for\n(\u03b8, f ) ranging over the whole parameter space. Note that d\u03b8 (f, f0 ) = kp\u03b8,f \u2212\n> kq\np0 k2 \u223c\n\u03b8,f \u2212 q\u03b80 ,f0 k2 by Taylor expansion. Then by the assumption that\n\nEV ar(U |V ) is positive definite, we know that kq\u03b8\u0303n ,f\u02c6\n\n\u03b8\u0303n ,\u03bbn\n\n\u2212q\u03b80 ,f0 k2 = OP (\u03bbn +\n\nk\u03b8\u0303n \u2212 \u03b80 k) implies kf\u02c6\u03b8\u0303n ,\u03bbn \u2212 f0 k2 = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k). Thus we only need to\nshow that the \u01eb-bracketing entropy number of the function class O defined\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f31\n\nTHE PENALIZED PROFILE SAMPLER\n\nbelow is of order \u01eb\u22121/k to complete the proof of (25)\u2013(26):\nO\u2261\n\n\u001a\n\nl\u03b8,f (X)\n: k\u03b8 \u2212 \u03b80 k \u2264 C1 , kf \u2212 f0 k\u221e \u2264 C1 , J(f ) < \u221e ,\n1 + J(f )\n\u001b\n\nfor some constant C1 . Note that l\u03b8,f (X)/(1 + J(f )) can be rewritten as:\n(52)\n\n\u2206A\u22121 log \u03a6 (q\u03b8,f\n \u0304 A) + (1 \u2212 \u2206)A\u22121 log (1 \u2212 \u03a6 (q\u03b8,f\n \u0304 A)) ,\n\nwhere A = 1 + J(f ) and q\u0304\u03b8,f \u2208 O1 , where\nO1 \u2261\n\n\u001a\n\nq\u03b8,f (X)\n: k\u03b8 \u2212 \u03b80 k \u2264 C1 , kf \u2212 f0 k\u221e \u2264 C1 , J(f ) < \u221e ,\n1 + J(f )\n\u001b\n\n< \u01eb\u22121/k by T1.\nand where we know HB (\u01eb, O1 , L2 (P )) \u223c\n\nWe next calculate the \u01eb-bracketing entropy number with L2 norm for the\nclass of functions R1 \u2261 {ka (t) : t 7\u2192 a\u22121 log \u03a6(at) for a \u2265 1 and t \u2208 R}. By\nsome analysis we know that ka (t) is strictly decreasing in a for t \u2208 R, and\n< |a \u2212 b| because |\u2202/\u2202a(k (t))| is bounded uniformly\nsupt\u2208R |ka (t) \u2212 kb (t)| \u223c\na\n< A\u22121\nover t \u2208 R. In addition, we know that supa,b\u2265A0 ,t\u2208R |ka (t) \u2212 kb (t)| \u223c\n0\n\nbecause the function u 7\u2192 u log \u03a6(u\u22121 t) has bounded derivative for 0 < u \u2264 1\nuniformly over t \u2208 R. The above two inequalities imply that the \u01eb-bracketing\nnumber with uniform norm is of order O(\u01eb\u22122 ) for a \u2208 [1, \u01eb\u22121 ] and is 1 for\na > \u01eb\u22121 . Thus we know HB (\u01eb, R1 , L2 ) = O(log \u01eb\u22122 ). By applying a similar\nanalysis to R2 \u2261 {ka (t) : t 7\u2192 a\u22121 log(1 \u2212 \u03a6(at)) for a \u2265 1 and t \u2208 R}, we\nobtain that HB (\u01eb, R2 , L2 ) = O(log \u01eb\u22122 ). Combining this with T7 and T8, we\n< \u01eb\u22121/k . This completes the proof of (25)\u2013(26).\ndeduce that HB (\u01eb, O, L2 ) \u223c\n\nFor the proof of (27), we apply arguments similar to those used in the\nproof of theorem 1 but after setting \u03bbn , J0 and J\u02c6n to zero in (49). Then\nwe obtain the following equality: d\u02c62n = OP (n\u22122k/(2k+1) ) + k\u03b8\u0303n \u2212 \u03b80 k2 +\nOP (n\u22121/2 )k\u03b8\u0303n \u2212 \u03b80 k1\u22121/2k + OP (n\u22121/2 )(k\u03b8\u0303n \u2212 \u03b80 k + d\u02c6n )1\u22121/2k . By treating\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f32\n\nG. CHENG AND M. R. KOSOROK\n\nk\u03b8\u0303n \u2212 \u03b80 k \u2264 n\u2212k/(2k+1) and k\u03b8\u0303n \u2212 \u03b80 k > n\u2212k/(2k+1) differently in the above\nequality, we obtain (27).\u0003\nProof of corollary 2: Lemma 7.1 in [14] establishes that\n(53)\n\np\u03b8\u0303n ,\u03b7\u0302\n\n\u03b8\u0303n ,\u03bbn\n\n\u2212 p\u03b80 ,\u03b70\n\n2\n\n+ \u03bbn J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ) = OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k)\n\nafter choosing\nm\u03b8,\u03bb,\u03b7 = log\n\np\u03b8,\u03b7 + p\u03b8,\u03b70\n1\n\u2212 \u03bb2 (J 2 (\u03b7) \u2212 J 2 (\u03b70 ))\n2p\u03b8,\u03b70\n2\n\nin theorem 2. Note that the map \u03b8 7\u2192 p\u03b8,\u03b70 /f W,Z (w, z) is uniformly bounded\naway from zero at \u03b8 = \u03b80 and continuous around a neighborhood of \u03b80 . Hence\nm\u03b8,\u03bb,\u03b7 is well defined. Moreover, Pn m\u03b8,\u03bb,\u03b7\u0302\u03b8,\u03bb \u2265 Pn m\u03b8,\u03bb,\u03b70 by the inequality\nthat ((p\u03b8,\u03b7 + p\u03b8,\u03b70 )/2p\u03b8,\u03b70 )2 \u2265 (p\u03b8,\u03b7 /p\u03b8,\u03b70 ). (53) now directly implies (32). For\nthe proof of (31), we need to consider the conclusion of lemma 7.4 (i), which\nstates that\n(54)\n\n> (k\u03b8 \u2212 \u03b8 k \u2227 1 + k|\u03b7 \u2212 \u03b7 | \u2227 1k ) \u2227 1.\nkp\u03b8,\u03b7 \u2212 p\u03b80 ,\u03b70 k2 \u223c\n0\n0\n2\n\nThus we have proved (31). For (33), we just replace the m\u03b8,\u03bb,\u03b7 with m\u03b8,0,\u03b7\nin the proof of lemma 7.1 in [14]. Thus we can show that d\u03b8 (\u03b7, \u03b70 ) = kp\u03b8,\u03b7 \u2212\np\u03b80 ,\u03b70 k2 . By combining lemma 1 and (54), we know that k\u03b7\u0302\u03b8\u0303n \u2212 \u03b70 k2 =\n\u221a\nOP (\u03b4n + k\u03b8\u0303n \u2212 \u03b80 k), for \u03b4n satisfying K(\u03b4n , S\u03b4n , L2 (P )) \u2264 n\u03b4n2 . Note that\nK(\u03b4, S\u03b4 , L2 (P )) is as defined in (30). By similar analysis as used in the proof\nof lemma 7.1 in [14] and the strengthened assumption on \u03b7, we then find\n1\u22121/2k\n\n< \u03b4\nthat K(\u03b4n , S\u03b4n , L2 (P )) \u223c\nn\n\n, which leads to the desired convergence\n\nrate given in (33). \u0003\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f33\n\nTHE PENALIZED PROFILE SAMPLER\n\nProof of theorem 3. Note that\nP l\u03b8\u0303\u03bbn,\u03b7\u0302\nn\n\n\u2212 P l\u03b8\u03bb0n,\u03b70 ,h\n\n\u03b8\u0303n ,\u03bbn ,h\n\nZ\n\n(k)\nn ,\u03bbn\n\nPn l\u03b8\u0303\u03bbn,\u03b7\u0302\nn \u03b8\u0303n ,\u03bbn ,h\n\n\u2212\n\n2\u03bb2n\n\n= \u2212(Vn \u2212 V )(\u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )h + 2\u03bb2n\n\nZ\n\nh(k) \u03b70 dz\n\n= V (\u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )h \u2212\n\n= \u2212(Vn \u2212 V )(\u03b80 , \u03b70 )h +\n= OP (n\u22121/2 ) + 2\u03bb2n\n\nZ\n\no\u2217P (n\u22121/2 )\n\n+\n\nh(k) (\u03b7\u0302\u03b8\u0303\n(k)\n\n2\u03bb2n\n\n(k)\n\n(k)\n\n\u2212 \u03b70 )dz\n\nZ\n\n(k)\n\nh(k) \u03b70 dz\n\nh(k) \u03b70 dz.\n\nThe last two equalities in the above follow from assumptions (34) and (35).\nThe Fr\u00e9chet differentiability of V (*, *) at (\u03b80 , \u03b70 ) establishes that\nP l\u03b8\u0303\u03bbn,\u03b7\u0302\nn\n\n\u03b8\u0303n ,\u03bbn ,h\n\n\u2212 P l\u03b8\u03bb0n,\u03b70 ,h\n= V\u0307 (\u03b8\u0303n \u2212 \u03b80 , \u03b7\u0302\u03b8\u0303n ,\u03bbn \u2212 \u03b70 ) + o\u2217P (k\u03b8\u0303n \u2212 \u03b80 k + d\u03b8\u0303n (\u03b7\u0302\u03b8\u0303n ,\u03bbn , \u03b70 ))\n\u22122\u03bb2n\n\nZ\n\n(k)\nn ,\u03bbn\n\nh(k) (\u03b7\u0302\u03b8\u0303\n\n(k)\n\n\u2212 \u03b70 )dz.\n\nCombining the above two sets of equations, we have, by the linearity of\nV\u0307 (*, *), established that\nV\u0307 (0, \u03b7\u0302\u03b8\u0303n ,\u03bbn ) = OP (n\u22121/2 ) + OP (k\u03b8\u0303n \u2212 \u03b80 k) + 2\u03bb2n\n\nZ\n\nZ\n\n(k)\ndz.\nn ,\u03bbn\n\nh(k) \u03b7\u0302\u03b8\u0303\n\nNow by the invertibility of V\u0307 (0, *), we can deduce that d\u03b8\u0303n (\u03b7\u0302\u03b8\u0303n ,\u03bbn , \u03b70 ) =\nOP (n\u22121/2 + k\u03b8\u0303n \u2212 \u03b80 k + \u03bb2n J 2 (\u03b7\u0302\u03b8\u0303n ,\u03bbn )). \u0003\nProof of theorem 4. We first show (37), and then we need to state one\nlemma before proceeding to the proof of (38). For the proof of (37), note\nthat\n0 = Pn l\u0307(\u03b8\u0302\u03bbn , \u03b8\u0302\u03bbn , \u03b7\u0302\u03bbn ) + 2\u03bb2n\n\nZ\n\nZ\n\n(k)\n\n(k)\n\n\u03b7\u0302\u03bbn (z)h0 (z)dz.\n\nCombining the third order Taylor expansion of \u03b8\u0302\u03bbn 7\u2192 Pn l\u0307(\u03b8\u0302\u03bbn , \u03b8, \u03b7) around\n\u03b80 , where \u03b8 = \u03b8\u0302\u03bbn and \u03b7 = \u03b7\u0302\u03bbn , with conditions (19) and (20), the first term\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f34\n\nG. CHENG AND M. R. KOSOROK\n\nin the right-hand-side of the above displayed equality equals Pn l\u03030 \u2212 I \u03030 (\u03b8\u0302\u03bbn \u2212\n\u03b80 ) + OP (\u03bbn + k\u03b8\u0302\u03bbn \u2212 \u03b80 k)2 . By the inequality 2\u03bb2n\n\nR\n\nZ\n\n(k)\n\n(k)\n\n\u03b7\u0302\u03bbn (z)h0 (z)dz \u2264\n\n\u03bb2n (J 2 (\u03b7\u0302\u03bbn ) + J 2 (h0 )) and assumption (10), the second term in the righthand-side of the above equality is equal to OP (\u03bbn + k\u03b8\u0302\u03bbn \u2212 \u03b80 k)2 . Combining\neverything, we obtain the following:\nn\n\u221a\n1 X\n(55) \u221a\nI \u03030\u22121 l\u03030 (Xi ) = n(\u03b8\u0302\u03bbn \u2212 \u03b80 ) + OP (n1/2 (\u03bbn + k\u03b8\u0302\u03bbn \u2212 \u03b80 k)2 ).\nn i=1\n\n\u221a\n\u221a\nThe right-hand-side of (55) is of the order OP ( n\u03bb2n + nwn (1 + wn + \u03bbn )),\nwhere wn represents k\u03b8\u0302\u03bbn \u2212\u03b80 k. However, its left-hand-side is trivially OP (1).\n\u221a\nConsidering the fact that n\u03bb2n = oP (1), we can deduce that \u03b8\u0302\u03bbn \u2212 \u03b80 =\nOP (n\u22121/2 ). Inserting this into the previous display completes the proof of\n(37).\nWe next prove (38). Note that \u03b8\u0302\u03bbn \u2212 \u03b80 = OP (n\u22121/2 ). Hence the order of\nthe remainder terms in (19) and (20) becomes OP (\u03bbn + k\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn k)2 and\nOP (\u03bbn + k\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn k), respectively. Expression (61) in lemma 6 below implies\nthat\n(56)\n\nlogpl\u03bbn (\u03b8\u0302\u03bbn ) = log pl\u03bbn (\u03b80 ) + n(\u03b8\u0302\u03bbn \u2212 \u03b80 )T Pn l\u03030\n\u2212\n\nn\n(\u03b8\u0302\u03bb \u2212 \u03b80 )T I \u03030 (\u03b8\u0302\u03bbn \u2212 \u03b80 ) + OP (n1/2 \u03bb2n ).\n2 n\n\nThe difference between (56) and (61) generates\n\u0010\n\n\u0011\n\nlog pl\u03bbn (\u03b8\u0303n ) = log pl\u03bbn (\u03b8\u0302\u03bbn ) + n(\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn )T Pn l\u03030 \u2212 I \u03030 (\u03b8\u0302\u03bbn \u2212 \u03b80 )\n\u2212\n\nn\n(\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn )T I \u03030 (\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn ) + OP (g\u03bbn (k\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn k)).\n2\n\n(38) is now immediately obtained after considering (37). \u0003\nProof of theorem 5. Suppose that F\u03bbn (*) is the penalized posterior profile\n\u221a\ndistribution of n\u033an with respect to the prior \u03c1(\u03b8), where the vector \u033an\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f35\n\nTHE PENALIZED PROFILE SAMPLER\n\n1/2\nis defined as I \u03030 (\u03b8 \u2212 \u03b8\u0302n ). The parameter set for \u033an is \u039en . F\u03bbn (*) can be\n\nexpressed as:\n\n(57) F\u03bbn (\u03be) =\n\n\u22121\n\n \u0303 2\n\u033an \u2208(\u2212\u221e,n\u22121/2 \u03be]\u2229\u039en \u03c1(\u03b8\u0302\u03bbn + I0 \u033an )\n\nR\n\nR\n\n\u033an \u2208\u039en\n\n\u03c1(\u03b8\u0302\u03bbn\n\n1\n\n\u2212\npl\u03bbn (\u03b8\u0302\u03bbn +I \u03030 2 \u033an )\nd\u033an\npl\u03bbn (\u03b8\u0302\u03bbn )\n1\n\n\u2212\n\u22121\npl (\u03b8\u0302 +I \u0303 2 \u033a )\n+ I \u03030 2 \u033an ) \u03bbn \u03bbn 0 n d\u033an\npl\u03bb (\u03b8\u0302\u03bb )\nn\n\n.\n\nn\n\nNote that d\u033an in the above is the short notation for d\u033an1 \u00d7 . . . \u00d7 d\u033and . To\nprove theorem 5, we first partition the parameter set \u039en as {\u039en \u2229 {k\u033an k2 >\nrn }} \u222a {\u039en \u2229 {k\u033an k2 \u2264 rn }}. By choosing the proper order of rn , we find\nthe posterior mass in the first partition region is of arbitrarily small order,\nas verified in lemma 5.1 immediately below, and the mass inside the second\npartition region can be approximated by a stochastic polynomial in powers of\nn\u22121/2 with error of order dependent on the smoothing parameter, as verified\nin lemma 5.2 below. This basic technique applies to both the denominator\nand the numerator, yielding the quotient series, which gives the desired\nresult.\nlemma 5.1. Choose rn = o(n\u22121/3 ) and\n\n\u221a\n\nnrn \u2192 \u221e. Under the conditions\n\nof theorem 5, we have\n1\n\n(58)\n\nZ\n\nk\u033an k>rn\n\n\u03c1(\u03b8\u0302\u03bbn\n\n\u2212\npl\u03bb (\u03b8\u0302\u03bb + I \u03030 2 \u033an )\n\u22121\n+ I \u03030 2 \u033an ) n n\nd\u033an = OP (n\u2212M ),\npl\u03bbn (\u03b8\u0302\u03bbn )\n\nfor any positive number M .\nProof: Fix r > 0. We then have\nZ\n\nk\u033an k>r\n\n\u03c1(\u03b8\u0302\u03bbn +\n\n1\n \u0303\u2212 2 \u033an )\n+\nI\n(\n\u03b8\u0302\npl\n\u2212 12\n\u03bb\n\u03bb\n0\nd\u033an\nI \u03030 \u033an ) n n\npl\u03bbn (\u03b8\u0302\u03bbn )\n\n\u221a Z\n1\n1\n\u2264 I{\u2206r\u03bbn < \u2212n\u2212 2 } exp(\u2212 n) \u03c1(\u03b8)d\u03b8 + I{\u2206r\u03bbn \u2265 \u2212n\u2212 2 },\n\u0398\n\n\u22121/2\n). Then by lemma 3.2 in [2],\nwhere \u2206r\u03bbn = supk\u033an k>r \u2206\u03bbn (\u03b8\u0302\u03bbn + \u033an I \u03030\n1\n\nI{\u2206r\u03bbn \u2265 \u2212n\u2212 2 } = OP (n\u2212M ) for any fixed r > 0. This implies that there\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f36\n\nG. CHENG AND M. R. KOSOROK\n\nexists a positive decreasing sequence rn = o(n\u22121/3 ) with\n\n\u221a\n\nnrn \u2192 \u221e such\n\nthat (58) holds. \u0003\nlemma 5.2. Choose rn = o(n\u22121/3 ) and\n\n\u221a\n\nnrn \u2192 \u221e. Under the conditions\n\nof theorem 5, we have\n1\n\nZ\n\n\u2212\npl\u03bbn (\u03b8\u0302\u03bbn + I \u03030 2 \u033an )\n\npl\u03bbn (\u03b8\u0302)\n\nk\u033an k\u2264rn\n\nn\n\u2212\n+ I \u03030 2 \u033an ) \u2212 exp \u2212 \u033aTn \u033an \u03c1(\u03b8\u0302\u03bbn )\n2\n\n\u03c1(\u03b8\u0302\u03bbn\n\n\u0013\n\n\u0012\n\n1\n\n\u00d7d\u033an = OP (\u03bb2n ).\n\n(59)\n\nProof: The posterior mass over the region k\u033an k2 \u2264 rn is bounded by\n\u22121\n\npl\u03bbn (\u03b8\u0302\u03bbn + I \u03030 2 \u033an )\n\nZ\n\npl\u03bbn (\u03b8\u0302\u03bbn )\n\nk\u033an k2 \u2264rn\n\nn\n\u03c1(\u03b8\u0302\u03bbn ) \u2212 exp \u2212 \u033aTn \u033an \u03c1(\u03b8\u0302\u03bbn ) d\u033an (\u2217)\n2\n\u0012\n\n1\n\n1\n\n+\n\n\u2212\npl\u03bbn (\u03b8\u0302\u03bbn + I \u03030 2 \u033an )\n\nZ\n\nk\u033an k2 \u2264rn\n\npl\u03bbn (\u03b8\u0302\u03bbn )\n\n\u0013\n\n\u03c1(\u03b8\u0302\u03bbn +\n\n\u22121\nI \u03030 2 \u033an )\n\n\u2212\n\n\u2212\npl\u03bbn (\u03b8\u0302\u03bbn + I \u03030 2 \u033an )\n\npl\u03bbn (\u03b8\u0302\u03bbn )\n\n\u03c1(\u03b8\u0302\u03bbn ) d\u033an . (\u2217\u2217)\n\nBy (38), we obtain\n(\u2217) =\n\nZ\n\nk\u033an k2 \u2264rn\n\n\"\n\nn\u033aT \u033an\n\u03c1(\u03b8\u0302\u03bbn ) exp \u2212 n\n2\n\n!\n\n#\n\n|exp(OP (g\u03bbn (k\u033an k))) \u2212 1| d\u033an .\n\nObviously the order of (\u2217) depends on that of | exp(OP (g\u03bbn (k\u033an k))) \u2212 1|\nfor \u03bbn satisfying (3) and k\u033an k \u2264 rn . In order to analyze its order, we park/(2k+1) )} with the set\ntition the set {\u03bbn = oP (n\u22121/4 ) and \u03bb\u22121\nn = OP (n\nk/(2k+1) )} \u2229\n{\u03bbn = OP (n\u22121/3 )}, i.e. Un = {\u03bbn = oP (n\u22121/4 ) and \u03bb\u22121\nn = OP (n\nk/(2k+1) )} \u2229\n{\u03bbn = OP (n\u22121/3 )} and Ln = {\u03bbn = oP (n\u22121/4 ) and \u03bb\u22121\nn = OP (n\n\n{\u03bbn = OP (n\u22121/3 )}C . For the set Un , we have | exp(OP (g\u03bbn (k\u033an k))) \u2212 1| =\ng\u03bbn (k\u033an k)\u00d7OP (1). For the set Ln , we have OP (g\u03bbn (k\u033an k)) = OP (nk\u033an k\u03bb2n +\n\u221a\nn1/2 \u03bb2n ). We can take rn = n\u22121\u2212\u03b4 \u03bb\u22122\nnrn \u2192 \u221e\nn for some \u03b4 > 0 such that\nand rn = o(n\u22121/3 ). Then | exp(OP (g\u03bbn (k\u033an k))) \u2212 1| = (nk\u033an k\u03bb2n + n1/2 \u03bb2n ) \u00d7\nOP (1). Combining with the above, we know that (\u2217) = OP (\u03bb2n ). By similar\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f37\n\nTHE PENALIZED PROFILE SAMPLER\n\nanalysis, we can also show that (\u2217\u2217) has the same order. This completes the\nproof of lemma 5.2. \u0003\nWe next start the formal proof of theorem 5. By considering both lemma 5.1\nand lemma 5.2, we know the denominator of (57) equals\nZ\n\n{k\u033an k2 \u2264rn }\u2229\u039en\n\n\u0014\n\nn\nexp \u2212 \u033aTn \u033an \u03c1(\u03b8\u0302\u03bbn ) d\u033an + OP (\u03bb2n ).\n2\n\u0013\n\n\u0012\n\n\u0015\n\nThe first term in the above display equals\nn\u22121/2 \u03c1(\u03b8\u0302\u03bbn )\n\nZ\n\nwhere un =\nR\u221e\nx\n\ne\u2212y\n\n2 /2\n\nT\n\n\u221a\n{kun k2 \u2264 nrn }\u2229 n\u039en\n\u221a\n\ne\u2212un un /2 dun = n\u22121/2 \u03c1(\u03b8\u0302\u03bbn )\n+ O(\u03bb2n ),\n\nZ\n\nRd\n\nT\n\ne\u2212un un /2 dun\n\n\u221a\nn\u033an . The above equality follows from the inequality that\n\ndy \u2264 x\u22121 e\u2212x\n\n2 /2\n\nfor any x > 0. Consolidating the above analyses,\n1\n\nwe deduce that the denominator of (57) equals n\u2212 2 \u03c1(\u03b8\u0302\u03bbn )(2\u03c0)d/2 + OP (\u03bb2n ).\nThe same analysis also applies to the numerator, thus completing the whole\nproof. \u0003\nProof of corollary 3: We only show (41) in what follows. (42) can be veri\u03bbn\nfied similarly. Showing (41) is equivalent to establishing \u1ebc\u03b8|x\n(\u033an ) = OP (\u03bb2n ).\n\u03bbn\nNote that \u1ebc\u03b8|x\n(\u033an ) can be written as:\n\n\u03bbn\n\u1ebc\u03b8|x\n(\u033an )\n\n=\n\nR\n\n1\n\n\u033an \u2208\u039en\n\nR\n\n\u033an \u03c1(\u03b8\u0302\u03bbn\n\n\u033an \u2208\u039en\n\n\u033an )\n\npl\u03bbn (\u03b8\u0302\u03bbn )\n\n1\n\n\u03c1(\u03b8\u0302\u03bbn\n\n\u22121\n2\n\n\u2212\npl (\u03b8\u0302 +I \u0303\n+ I \u03030 2 \u033an ) \u03bbn \u03bbn 0\n\n\u2212\n+ I \u03030 2 \u033an )\n\n\u22121\npl\u03bbn (\u03b8\u0302\u03bbn +I \u03030 2 \u033an )\n\npl\u03bbn (\u03b8\u0302\u03bbn )\n\nd\u033an\n.\n\nd\u033an\n\nBy analysis similar to that applied in the proof of theorem 5, we know\nthe denominator in the above display is n\u22121/2 (2\u03c0)d/2 \u03c1(\u03b8\u0302\u03bbn ) + OP (\u03bb2n ) and\nthe numerator is a random vector of order OP (n\u22121/2 \u03bb2n ). This yields the\nconclusion.\u0003\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f38\n\nG. CHENG AND M. R. KOSOROK\n\u22121/2\nProof of theorem 6. Note that (40) implies \u03ban\u03b1 = I \u03030\nz\u03b1 + OP (n1/2 \u03bb2n ),\n\nfor any \u03be < \u03b1 < 1 \u2212 \u03be, where \u03be \u2208 (0, 12 ). Note also that the \u03b1-th quantile\nof a d dimensional standard normal distribution, z\u03b1 , is not unique if d > 1.\nThe classical Edgeworth expansion implies that P (n\u22121/2\n\n \u0303\u22121/2 l\u03030 (Xi )\ni=1 I0\n\nPn\n\n\u2264\n\nz\u03b1 + an (\u03b1)) = \u03b1, where an (\u03b1) = O(n\u22121/2 ), for \u03be < \u03b1 < 1 \u2212 \u03be. Note that\nan (\u03b1) is uniquely determined for each fixed z\u03b1 since l\u03030 (Xi ) has at least one\n\u221a\n\u22121/2\nabsolutely continuous component. Let \u03ba\u0302n\u03b1 = I \u03030\nz\u03b1 + ( n(\u03b8\u0302\u03bbn \u2212 \u03b80 ) \u2212\n\u221a\nP\n\u22121/2\nn\u22121/2 ni=1 I \u03030\u22121 l\u03030 (Xi )) + I \u03030\nan (\u03b1). Then P ( n(\u03b8\u0302\u03bbn \u2212 \u03b80 ) \u2264 \u03ba\u0302n\u03b1 ) = \u03b1.\n\nCombining with (37), we obtain \u03ba\u0302n\u03b1 = \u03ban\u03b1 + OP (n1/2 \u03bb2n ). The uniqueness\n\nof \u03ba\u0302n\u03b1 up to order OP (n1/2 \u03bb2n ) follows from that of an (\u03b1) for each chosen\nz\u03b1 .\u0003\nProof of lemma 2. Assumptions S1 and S2 are verified in lemma 5 of [3].\nFor the verifications of the assumption E1, we first show the asymptotic\nequicontinuity condition (15). Without loss of generality, we assume that \u03bbn\nis bounded below by a multiple of n\u2212k/(2k+1) and bounded above by n\u22121/4\nin view of (3). Thus\n\uf8eb\n\nP\uf8ed\n\nl\u0307(\u03b80 , \u03b80 , f\u02c6\u03b8\u0303n ,\u03bbn ) \u2212 l\u03070\nn\n\n1\n4k+2\n\n(\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k)\n\n\uf8f62\n\uf8f8\n\nkf\u02c6\u03b8\u0303n ,\u03bbn \u2212 f0 k22\n\n<\n\n\u223c\n\nn\n\n1\n2k+1\n\n(\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k)2\n\n\u0010\n\n1\n\n\u0011\n\n= OP n\u2212 2k+1 ,\n\nwhere (25) implies the equality in the above expression.\nBy (26), we know that J(f\u02c6\u03b8\u0303n ,\u03bbn ) = OP (1 + k\u03b8\u0303n \u2212 \u03b80 k/\u03bbn ) and kf\u02c6\u03b8\u0303n ,\u03bbn k\u221e\nis bounded by some constant, since f \u2208 HkM . We then define the set Qn as\nfollows:\n(\n\nk\u03b8 \u2212 \u03b80 k\n: J(f ) \u2264 Cn (1 +\n), kf k\u221e \u2264 M, k\u03b8 \u2212 \u03b80 k \u2264 \u03b4\n1\n\u03bbn\nn 4k+2 (\u03bbn + k\u03b8 \u2212 \u03b80 k)\nl\u0307(\u03b80 , \u03b80 , f ) \u2212 l\u03070\n\nn\n\n1\n\n)\n\no\n\n\u2229 g \u2208 L2 (P ) : P g 2 \u2264 Cn n\u2212 2k+1 ,\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f39\n\nTHE PENALIZED PROFILE SAMPLER\n\nfor some \u03b4 > 0. Obviously the function n\u22121/(4k+2) (l\u0307(\u03b80 , \u03b80 , f\u02c6\u03b8\u0303n ,\u03bbn )\u2212 l\u03070 )/(\u03bbn +\nk\u03b8\u0303n \u2212 \u03b80 k)) \u2208 Qn on a set of probability arbitrarily close to one, as Cn \u2192 \u221e.\nIf we can show limn\u2192\u221e E \u2217 kGn kQn < \u221e by T2, then assumption (15) is\nverified. Note that l\u0307(\u03b80 , \u03b80 , f ) depends on f in a Lipschitz manner. Consequently we can bound HB (\u01eb, Qn , L2 (P )) by the product of some constant\nand H(\u01eb, Rn , L2 (P )) in view of T3. Rn is defined as\n< \u03bb\u22121 n\u22121/(4k+2) , kH (f )k\n< \u22121 \u22121/(4k+2) },\n{Hn (f ) : J(Hn (f )) \u223c\nn\n\u221e \u223c \u03bbn n\nn\n\nwhere Hn (f ) = f /(n1/(4k+2) (\u03bbn + k\u03b8 \u2212 \u03b80 k)). By [1], we know that\n\u22121\n\n< (\u03bb\u22121 n (4k+2) )/\u01eb)1/k .\nH(\u01eb, Rn , L2 (P )) \u223c\nn\n\nNote that \u03b4n = n\u22121/(4k+2) and Mn = n(2k\u22121)/(4k+2) in T2. Thus by calcu\u22121/2k \u22121/(4k+2)\nn\n.\n\n< \u03bb\nlation we know that K(\u03b4n , Qn , L2 (P )) \u223c\nn\n\nThen by T2 we\n\ncan show that limn\u2192\u221e E \u2217 kGn kQn < \u221e.\n\nWe next show (18). It suffices to verify that the sequence of classes of\nfunctions Vn is P -Glivenko-Cantelli, where Vn \u2261 {l(3) (\u03b8\u0304n , \u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn )(x)}, for\nevery random sequence \u03b8\u0304n \u2192 \u03b80 and \u03b8\u0303n \u2192 \u03b80 in probability. A GlivenkoCantelli theorem for classes of functions that change with n is needed. By\nrevising theorem 2.4.3 in [21] with minor notational changes, we obtain\nthe following suitable extension of the uniform entropy Glivenko-Cantelli\ntheorem: Let Fn be suitably measurable classes of functions with uniformly\nintegrable functions and H(\u01eb, Fn , L1 (Pn )) = o\u2217P (n) for any \u01eb > 0. Then\nkPn \u2212 P kFn \u2192 0 in probability for every \u01eb > 0. We then apply this revised\ntheorem to the set Fn of functions l(3) (t, \u03b8, f ) with t and \u03b8 ranging over\na neighborhood of \u03b80 and \u03bbn J(f ) bounded by a constant. By the form of\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f40\n\nG. CHENG AND M. R. KOSOROK\n\nl(3) (t, \u03b8, f ), the entropy number for Vn is equal to that of\nF\u0303n \u2261 {\u03c6(qt,ft (\u03b8,f ) (x))R(qt,ft (\u03b8,f ) (x)) : (t, \u03b8) \u2208 V\u03b80 , \u03bbn J(f ) \u2264 C, kf k\u221e \u2264 M }.\nBy arguments similar to those used in lemma 7.2 of [14], we know that\n< (1 + \u03bb\u22121 /\u01eb)1/k = o (n). Moreover, the F\u0303 are unisupQ H(\u01eb, F\u0303n , L1 (Q)) \u223c\nP\nn\nn\n\nformly bounded since f \u2208 HkM . Considering the fact that the probability\n\nthat Vn is contained in F\u0303n tends to 1, we have completed the proof of (18).\nFor the proof of (16), we only need to show that Gn (l\u0308(\u03b80 , \u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn )\u2212 l\u03080 ) =\noP (1) since l\u03080 (x) is uniformly bounded in x. Note that we only need to show\n(16) holds for \u03b8\u0303n = \u03b8\u0302n + o(n\u22121/3 ) based on the arguments in lemma 5.2. We\nnext show that Gn (l\u0308(\u03b80 , \u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn ) \u2212 l\u03080 ) = oP (1 + n1/3 k\u03b8\u0303n \u2212 \u03b80 k) = oP (1).\nBy the rate assumptions R1, we have\n\uf8eb\n\nP\uf8ed\n\nl\u0308(\u03b80 , \u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn ) \u2212 l\u03080\n1+\n\nn1/3 k\u03b8\u0303\n\nn\n\n\u2212 \u03b80 k\n\n\uf8f62\n\uf8f8\n\n<\n\n\u223c\n\nk\u03b8\u0303n \u2212 \u03b80 k2 + kf\u02c6\u03b8\u0303n ,\u03bbn \u2212 f0 k22\n(1 +\n\nn1/3 k\u03b8\u0303\n\nn\n\n\u2212 \u03b80\n\nk)2\n\n= OP (n\u22121/2 ).\n\nWe next define Q\u0304n as follows:\n(\n\nl\u0308(\u03b80 , \u03b8, f ) \u2212 l\u03080\nk\u03b8 \u2212 \u03b80 k\n), kf k\u221e \u2264 M, k\u03b8 \u2212 \u03b80 k < \u03b4\n: J(f ) \u2264 Cn (1 +\n1/3\n\u03bbn\n1 + n k\u03b8 \u2212 \u03b80 k\n1\n\nn\n\n)\n\no\n\n\u2229 g \u2208 L2 (P ) : P g 2 \u2264 Cn n\u2212 2 .\n\nObviously the function (l\u0308(\u03b80 , \u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn ) \u2212 l\u03080 )/(1 + n1/3 k\u03b8\u0303n \u2212 \u03b80 k) \u2208 Q\u0304n on\na set of probability arbitrarily close to one, as Cn \u2192 \u221e. If we can show\nlimn\u2192\u221e E \u2217 kGn kQ\u0304n \u2192 0 by T2, then the proof of (16) is completed. Accordingly, note that l\u0308(\u03b80 , \u03b8, f ) depends on (\u03b8, f ) in a Lipschitz manner. Consequently we can bound HB (\u01eb, Q\u0304n , L2 (P )) by the product of some constant\nand (H(\u01eb, R\u0304n , L2 (P )) + log(1/\u01eb)) in view of T3. R\u0304n is defined as\n1/3\n< 1 + (n1/3 \u03bb )\u22121 , kH (f )k\n<\n{Hn (f ) : J(Hn (f )) \u223c\n\u03bbn )\u22121 },\nn\nn\n\u221e \u223c 1 + (n\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f41\n\nTHE PENALIZED PROFILE SAMPLER\n\nwhere Hn (f ) = f /(1 + n1/3 k\u03b8 \u2212 \u03b80 k). By [1], we know that\n< ((1 + n\u22121/3 \u03bb\u22121 )/\u01eb)1/k .\nH(\u01eb, R\u0304n , L2 (P )) \u223c\nn\n\nThen by analysis similar to that used in the proof of (15), we can show that\nlimn\u2192\u221e E \u2217 kGn kQ\u0304n \u2192 0 in view of T2. This completes the proof of (16).\nFor the proof of (17), it suffices to show that Gn (lt,\u03b8 (\u03b80 , \u03b8\u0304n , f\u02c6\u03b8\u0303n ,\u03bbn ) \u2212\nlt,\u03b8 (\u03b80 , \u03b80 , f0 )) = oP (1) for \u03b8\u0303n = \u03b8\u0302n + o(n\u22121/3 ) and for \u03b8\u0304n between \u03b8\u0303n and\n\u03b80 , in view of lemma 5.2. Then we can show that Gn (lt,\u03b8 (\u03b80 , \u03b8\u0304n , f\u02c6\u03b8\u0303n ,\u03bbn ) \u2212\nlt,\u03b8 (\u03b80 , \u03b80 , f0 )) = oP (1 + n1/3 k\u03b8\u0303n \u2212 \u03b80 k) = oP (1) by similar analysis as used\nin the proof of (16).\u0003\nProof of lemma 3. By the assumption that \u2206\u03bbn (\u03b8\u0303n ) = oP (1), we have\n\u2206\u03bbn (\u03b8\u0303n ) \u2212 \u2206\u03bbn (\u03b80 ) \u2265 oP (1). Thus the following inequality holds:\nn\u22121\n\nn\nX\ni=1\n\n\uf8ee\n\n\uf8f9\n\nlik(\u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn , Xi )\n2 \u02c6\n\uf8fb \u2212 n\u22121 \u03bb2 [J 2 (f\u02c6\nlog \uf8f0\nn\n\u03b8\u0303n ,\u03bbn ) \u2212 J (f\u03b80 ,\u03bbn )] \u2265 oP (1)\n\u02c6\n,\nX\n)\nlik(\u03b80 , f\u03b8 ,\u03bb\ni\n0\n\nn\n\nBy considering assumption (10), the above inequality simplifies to\nn\u22121\n\nn\nX\ni=1\n\n\uf8ee\n\n\uf8f9\n\nH(\u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn ; Xi )\n\uf8fb \u2265 oP (1),\nlog \uf8f0\nH(\u03b80 , f\u02c6\u03b8 ,\u03bb ; Xi )\n0\n\nn\n\nwhere H(\u03b8, f ; X) = \u2206\u03a6(C \u2212 \u03b8U \u2212 f (V )) + (1 \u2212 \u2206)(1 \u2212 \u03a6(C \u2212 \u03b8U \u2212 f (V ))).\nBy arguments similar to those used in lemma 2.2 and by T5, we know\nH(\u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn ; Xi ) belongs to some P -Donsker class. Combining the above\nconclusion and the inequality \u03b1 log x \u2264 log(1 + \u03b1{x \u2212 1}) for some \u03b1 \u2208 (0, 1)\nand any x > 0, we can show that\n(60)\n\n\uf8ee\n\n\uf8eb\n\n\uf8f6\uf8f9\n\nH(\u03b8\u0303n , f\u02c6\u03b8\u0303n ,\u03bbn ; Xi )\nP log \uf8f01 + \u03b1 \uf8ed\n\u2212 1\uf8f8\uf8fb \u2265 oP (1).\nH(\u03b80 , f\u02c6\u03b8 ,\u03bb ; Xi )\n0\n\nn\n\nThe remainder of the proof follows the proof of lemma 6 in [3].\u0003\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f42\n\nG. CHENG AND M. R. KOSOROK\n\nProof of lemma 4. The maps (11) are uniformly bounded since F (*), \u1e1e (*)\nand F\u0308 (*) are all uniformly bounded in (\u2212\u221e, +\u221e). This completes the verifications of S1. Note that (W, Z) are in [0, 1]2 and h0 (*) is intrinsically bounded\nover [0, 1]. Hence we can show that the Fr\u00e9chet derivatives of \u03b7 7\u2192 l\u0308(\u03b80 , \u03b80 , \u03b7)\nand \u03b7 7\u2192 lt,\u03b8 (\u03b80 , \u03b80 , \u03b7) for any \u03b7 \u2208 Hk are bounded operators, from which we\ncan deduce that |l\u0308(\u03b80 , \u03b80 , \u03b7)(X) \u2212 l \u03080 (X)| is bounded by the product of some\nintegrable function and |\u03b7 \u2212 \u03b70 |(Z). This ensures (12) and (13). For (14),\nP l\u0307(\u03b80 , \u03b80 , \u03b7) can be written as P (F (\u03b80 w + \u03b70 ) \u2212 F (\u03b80 w + \u03b7(z)))(w \u2212 h0 (z))\nsince P l\u03070 = 0. Note that P (w \u2212 h0 (z))\u1e1e (\u03b80 w + \u03b70 (z))(\u03b7 \u2212 \u03b70 )(z) = 0. This\nimplies that P l\u0307(\u03b80 , \u03b80 , \u03b7) = P (F (\u03b80 w + \u03b70 ) \u2212 F (\u03b80 w + \u03b7(z)) + \u1e1e (\u03b80 w +\n\u03b70 (z))(\u03b7 \u2212 \u03b70 )(z))(w \u2212 h0 (z)). However, by the common Taylor expansion,\nwe have |F (\u03b80 w + \u03b7) \u2212 F (\u03b80 w + \u03b70 ) \u2212 \u1e1e (\u03b80 w + \u03b70 )(\u03b7 \u2212 \u03b70 )| \u2264 kF\u0308 k\u221e |\u03b7 \u2212 \u03b70 |2 .\nThis proves (14).\nWe next verify assumption E1. For the asymptotic equicontinuity condition (15), we first apply analysis similar to that used in the proof of lemma 2\nto obtain\n\uf8eb\n\nP\uf8ed\n\nl\u0307(\u03b80 , \u03b80 , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) \u2212 l\u03070\nn\n\n1\n4k+2\n\n(\u03bbn + k\u03b8\u0303n \u2212 \u03b80 k)\n\n\uf8f62\n\uf8f8\n\n1\n\n\u0010\n\n\u0011\n\n\u2212\n<\n\u223c OP n 2k+1 .\n\nBy lemma 7.1 in [14], we know that J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ) = OP (1 + k\u03b8\u0303n \u2212 \u03b80 k/\u03bbn ) and\nk\u03b7\u0302\u03b8\u0303n ,\u03bbn k\u221e is bounded in probability by a multiple of J(\u03b7\u0302\u03b8\u0303n ,\u03bbn ) + 1. Now we\nconstruct the set Q\u0303n as follows:\n(\n\nn\n\nl\u0307(\u03b80 , \u03b80 , \u03b7) \u2212 l\u03070\n\n1\n4k+2\n\n(\u03bbn + k\u03b8 \u2212 \u03b80 k)\n\n: J(\u03b7) \u2264 Cn (1 +\n\nn\n\nk\u03b8 \u2212 \u03b80 k\n), k\u03b7k\u221e \u2264 Cn (1 + J(\u03b7)),\n\u03bbn\n1\n\no\n\nk\u03b8 \u2212 \u03b80 k < \u03b4} \u2229 g \u2208 L2 (P ) : P g 2 \u2264 Cn n\u2212 2k+1 .\nClearly, the probability that the function n\u22121/(4k+2) (l\u0307(\u03b80 , \u03b80 , \u03b7\u0302\u03b8\u0303n ,\u03bbn )\u2212l\u03070 )/(\u03bbn +\nk\u03b8\u0303n \u2212\u03b80 k)) \u2208 Q\u0303n approaches 1 as Cn \u2192 \u221e. We next show that limn\u2192\u221e E \u2217 kGn kQ\u0303n <\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f43\n\nTHE PENALIZED PROFILE SAMPLER\n\n\u221e by T2. Note that l\u0307(\u03b80 , \u03b80 , \u03b7) depends on \u03b7 in a Lipschitz manner. Consequently, we can bound HB (\u01eb, Q\u0303n , L2 (P )) by the product of some constant\nand H(\u01eb, Rn , L2 (P )) in view of T3, where Rn is as defined in the proof of\nlemma 2. By similar calculations as those performed in lemma 2, we can ob\u22121/2k \u22121/(4k+2)\nn\n.\n\n< \u03bb\ntain K(\u03b4n , Q\u0303n , L2 (P )) \u223c\nn\n\nThus limn\u2192\u221e E \u2217 kGn kQ\u0303n < \u221e,\n\nand (15) follows.\nNext we define V\u0304n \u2261 {l(3) (\u03b8\u0304n , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )(x)}. Similar arguments as those\nused in the proof of lemma 2 can be directly applied to the verification of\n(18) in this second model. By the form of l(3) (t, \u03b8, \u03b7), the entropy number\nfor V\u0304n is bounded above by that of F \u0304n \u2261 {F\u0308 (tw + \u03b7(z) + (\u03b8 \u2212 t)h0 (z)) :\n(t, \u03b8) \u2208 V\u03b80 , \u03bbn J(\u03b7) \u2264 Cn , k\u03b7k\u221e \u2264 Cn (1 + J(\u03b7))}. Similarly, we know\n< ((1 + \u03bb\u22121 )/\u01eb)1/k = o (n).\nsupQ H(\u01eb, V\u0304n , L1 (Q)) \u2264 supQ H(\u01eb, F\u0304n , L1 (Q)) \u223c\nP\nn\n\nMoreover, the F\u0304n are uniformly bounded. This completes the proof for (18).\nThe proof of (16) and (17) follows arguments quite similar to those used in\nthe proof of lemma 2. In other words, we can show that Gn (l\u0308(\u03b80 , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )\u2212\nl\u03080 ) = oP (1+n1/3 k\u03b8\u0303n \u2212\u03b80 k) = oP (1) and Gn (lt,\u03b8 (\u03b80 , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )\u2212lt,\u03b8 (\u03b80 , \u03b80 , \u03b70 )) =\noP (1 + n1/3 k\u03b8\u0303n \u2212 \u03b80 k). This concludes the proof.\u0003\nProof of lemma 5: The proof of lemma 5 is analogous to that of lemma 3.\u0003\nLemma 6.\n\nAssuming the assumptions in theorem 4, we have\n\n(61) logpl\u03bbn (\u03b8\u0303n ) = log pl\u03bbn (\u03b80 ) + n(\u03b8\u0303n \u2212 \u03b80 )T Pn l\u03030\n\u2212\n\nn\n(\u03b8\u0303n \u2212 \u03b80 )T I \u03030 (\u03b8\u0303n \u2212 \u03b80 ) + OP (g\u03bbn (k\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn k)),\n2\n\nfor any \u03b8\u0303n = \u03b80 + oP (1).\nProof. n\u22121 (log pl\u03bbn (\u03b8\u0303n ) \u2212 log pl\u03bbn (\u03b80 )) is bounded above and below by\nPn (l(\u03b8\u0303n , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) \u2212 l(\u03b80 , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )) \u2212\nimsart-aos ver.\n\n2006/01/04 file:\n\n1 2 2\n\u03bb (J (\u03b7\u0302\u03b8\u0303n ,\u03bbn ) \u2212 J 2 (\u03b7\u03b80 (\u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )))\nn n\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f44\n\nG. CHENG AND M. R. KOSOROK\n\nand\nPn (l(\u03b8\u0303n , \u03b80 , \u03b7\u0302\u03b80 ,\u03bbn ) \u2212 l(\u03b80 , \u03b80 , \u03b7\u0302\u03b80 ,\u03bbn )) \u2212\n\n1 2 2\n\u03bb (J (\u03b7\u03b8\u0303n (\u03b80 , \u03b7\u0302\u03b80 ,\u03bbn )) \u2212 J 2 (\u03b7\u0302\u03b80 ,\u03bbn )),\nn n\n\nrespectively. By the third order Taylor expansion of \u03b8\u0303n 7\u2192 Pn l(\u03b8\u0303n , \u03b8, \u03b7)\naround \u03b80 , for \u03b8 = \u03b8\u0303n and \u03b7 = \u03b7\u0302\u03b8\u0303n ,\u03bbn , and the above empirical no-bias\nconditions (19) and (20), we can find that the order of the difference between Pn (l(\u03b8\u0303n , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ) \u2212 l(\u03b80 , \u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn )) and (\u03b8\u0303n \u2212 \u03b80 )T Pn l\u03030 \u2212 (\u03b8\u0303n \u2212\n\u03b80 )T (I \u03030 /2)(\u03b8\u0303n \u2212\u03b80 ) is OP (n\u22121 g\u03bbn (k\u03b8\u0303n \u2212\u03b8\u0302\u03bbn k)). By the inequality J 2 (\u03b7t (\u03b8, \u03b7)) \u2264\n2J 2 (\u03b7)+2(\u03b8 \u2212t)2 J 2 (h0 ), we know that \u03bb2n (J 2 (\u03b7\u0302\u03b8\u0303n ,\u03bbn )\u2212J 2 (\u03b7\u03b80 (\u03b8\u0303n , \u03b7\u0302\u03b8\u0303n ,\u03bbn ))) =\nOP (k\u03b8\u0303n \u2212 \u03b8\u0302\u03bbn k + \u03bbn )2 provided assumptions (3) and (10) hold. Similar analysis also applies to the lower bound. This proves (61).\u0003\nAcknowledgments. The authors thank Dr. Joseph Kadane for several\ninsightful discussions.\nREFERENCES\n[1] Birman, M.S. and Solomjak, M.J. (1967). Piece-wise polynomial approximations\nof functions of the classes Wp\u03b1 . Mat. Sbornik 3 295\u2013317.\n[2] Cheng, G. and Kosorok, M.R. (2006). Higher order semiparametric frequentist\ninference with the profile sampler. Annals of Statistics, Tentatively Accepted.\n[3] Cheng, G. and Kosorok, M.R. (2006). General Frequentist Properties of the Posterior Profile Distribution. Submitted.\n[4] Good, I.J. and Gaskins, R.A. (1971). Non-parametric roughness penalties for probability densities. Biometrika 58 255\u2013277.\n[5] Huang, J. (1999). Efficient estimation of the partly linear Cox model. Annals of\nStatistics 27 1536\u20131563.\n[6] Kosorok, M. R. (To appear). Introduction to Empirical Processes and Semiparametric Inference. Springer, New York.\n[7] Kuo, H. H. (1975). Gaussian Measure on Banach Spaces. Lecture Notes in Mathematics 463 Berlin: Springer.\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f45\n\nTHE PENALIZED PROFILE SAMPLER\n\n[8] Lee, B. L., Kosorok, M. R. and Fine, J. P. (2005). The profile sampler. Journal\nof the American Statistical Association 100 960\u2013969.\n[9] Ma, S. and Kosorok, M.R. (2005). Penalized Log-likelihood Estimation for Partly\nLinear Transformation Models with Current Status Data. Annals of Statistics 33 22562290.\n[10] Ma, S. and Kosorok, M.R. (2005). Robust semiparametric M-estimation and the\nweighted bootstrap. Journal of Multivariate Analysis 96 190-217.\n[11] Ma, S. and Kosorok, M.R. (2006). Adaptive penalized M-estimation with current\nstatus data. Annals of the Institute of Statistical Mathematics 58 511-526.\n[12] Mammen, E. and van de Geer, S. (1997). Penalized quasi-likelihood estimation in\npartial linear models. Annals of Statistics 25 1014-1035.\n[13] Murphy, S. A. (1995). Asymptotic Theory for the Frailty Model. Annals of Statistics\n23 182\u2013198.\n[14] Murphy, S. A. and Van der Vaart, A. W. (1999). Observed information in semiparametric models. Bernoulli 5 381\u2013412.\n[15] Murphy, S. A. and Van der Vaart, A. W. (2001). Semiparametric mixtures in\ncase-control studies. Journal of Multivariate Analysis 79 1\u201332.\n[16] Shen, X. (2002). Asymptotic normality in semiparametric and nonparametric posterior distributions. Journal of the American Statistical Association 97 222\u2013235.\n[17] Silverman, B.W. (1982). On the estimation of a probability density function by the\nmaximum penalized likelihood method. Annals of Statistics 10 795\u2013810.\n[18] Silverman, B.W. (1985). Some aspects of the spline smoothing approach to nonparametric regression curve fitting (with discussion). Journal of the Royal Statistical\nSociety. Series B 47 1\u201352.\n[19] van de Geer, S. (2000). Empirical Processes in M-estimation Cambridge University\nPress, Cambridge.\n[20] van der Vaart, A. W. (1994). Maximum Likelihood Estimation with Partially Censored Observations.. Annals of Statistics 22 1896\u20131916.\n[21] van der Vaart, A. W., and Wellner, J. A. (1996). Weak Convergence and Empirical Processes: With Applications to Statistics. Springer, New York.\n[22] Wahba, G. (1998). Spline Models for Observational Data. SIAM, Philadelphia.\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f46\n\nG. CHENG AND M. R. KOSOROK\n\nGuang Cheng\n\nMichael R. Kosorok\n\nInstitute of Statistics and\n\nDepartment of Biostatistics\n\nDecision Sciences\n\nSchool of Public Health\n\nDuke University\n\nUniversity of North Carolina\n\n214 Old Chemistry Building\n\nat Chapel Hill\n\nDurham, NC 27708\n\n3101 McGavran-Greenberg Hall\n\nUSA\n\nChapel Hill, NC 27599\n\nEmail: chengg@stat.duke.edu\n\nUSA\nEmail: kosorok@unc.edu\n\nimsart-aos ver.\n\n2006/01/04 file:\n\npenalized.tex date:\n\nOctober 24, 2018\n\n\f"}