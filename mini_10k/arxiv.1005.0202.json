{"id": "http://arxiv.org/abs/1005.0202v1", "guidislink": true, "updated": "2010-05-03T06:46:56Z", "updated_parsed": [2010, 5, 3, 6, 46, 56, 0, 123, 0], "published": "2010-05-03T06:46:56Z", "published_parsed": [2010, 5, 3, 6, 46, 56, 0, 123, 0], "title": "Dictionary Optimization for Block-Sparse Representations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1005.4787%2C1005.3751%2C1005.4731%2C1005.1742%2C1005.1537%2C1005.2248%2C1005.3195%2C1005.5339%2C1005.5534%2C1005.2158%2C1005.0497%2C1005.4575%2C1005.1484%2C1005.4313%2C1005.2064%2C1005.4429%2C1005.0087%2C1005.4046%2C1005.4521%2C1005.0659%2C1005.5678%2C1005.3560%2C1005.4832%2C1005.0804%2C1005.2327%2C1005.2029%2C1005.2773%2C1005.2718%2C1005.2528%2C1005.3694%2C1005.0515%2C1005.2799%2C1005.0756%2C1005.0746%2C1005.4766%2C1005.4853%2C1005.0352%2C1005.1757%2C1005.4797%2C1005.0122%2C1005.3932%2C1005.2245%2C1005.0163%2C1005.3557%2C1005.0295%2C1005.3744%2C1005.0955%2C1005.2242%2C1005.5289%2C1005.4613%2C1005.3121%2C1005.1913%2C1005.3207%2C1005.1348%2C1005.0211%2C1005.5253%2C1005.0054%2C1005.1798%2C1005.1127%2C1005.4697%2C1005.5500%2C1005.2490%2C1005.3344%2C1005.0366%2C1005.5669%2C1005.3978%2C1005.4516%2C1005.1204%2C1005.5607%2C1005.3629%2C1005.4928%2C1005.3135%2C1005.2926%2C1005.4133%2C1005.4942%2C1005.5225%2C1005.3729%2C1005.3174%2C1005.2911%2C1005.4191%2C1005.4965%2C1005.4612%2C1005.0202%2C1005.0966%2C1005.4304%2C1005.3869%2C1005.4659%2C1005.5306%2C1005.0608%2C1005.4465%2C1005.5059%2C1005.2509%2C1005.4738%2C1005.1154%2C1005.4260%2C1005.3804%2C1005.4301%2C1005.1233%2C1005.4262%2C1005.0386%2C1005.5735&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Dictionary Optimization for Block-Sparse Representations"}, "summary": "Recent work has demonstrated that using a carefully designed dictionary\ninstead of a predefined one, can improve the sparsity in jointly representing a\nclass of signals. This has motivated the derivation of learning methods for\ndesigning a dictionary which leads to the sparsest representation for a given\nset of signals. In some applications, the signals of interest can have further\nstructure, so that they can be well approximated by a union of a small number\nof subspaces (e.g., face recognition and motion segmentation). This implies the\nexistence of a dictionary which enables block-sparse representations of the\ninput signals once its atoms are properly sorted into blocks. In this paper, we\npropose an algorithm for learning a block-sparsifying dictionary of a given set\nof signals. We do not require prior knowledge on the association of signals\ninto groups (subspaces). Instead, we develop a method that automatically\ndetects the underlying block structure. This is achieved by iteratively\nalternating between updating the block structure of the dictionary and updating\nthe dictionary atoms to better fit the data. Our experiments show that for\nblock-sparse data the proposed algorithm significantly improves the dictionary\nrecovery ability and lowers the representation error compared to dictionary\nlearning methods that do not employ block structure.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1005.4787%2C1005.3751%2C1005.4731%2C1005.1742%2C1005.1537%2C1005.2248%2C1005.3195%2C1005.5339%2C1005.5534%2C1005.2158%2C1005.0497%2C1005.4575%2C1005.1484%2C1005.4313%2C1005.2064%2C1005.4429%2C1005.0087%2C1005.4046%2C1005.4521%2C1005.0659%2C1005.5678%2C1005.3560%2C1005.4832%2C1005.0804%2C1005.2327%2C1005.2029%2C1005.2773%2C1005.2718%2C1005.2528%2C1005.3694%2C1005.0515%2C1005.2799%2C1005.0756%2C1005.0746%2C1005.4766%2C1005.4853%2C1005.0352%2C1005.1757%2C1005.4797%2C1005.0122%2C1005.3932%2C1005.2245%2C1005.0163%2C1005.3557%2C1005.0295%2C1005.3744%2C1005.0955%2C1005.2242%2C1005.5289%2C1005.4613%2C1005.3121%2C1005.1913%2C1005.3207%2C1005.1348%2C1005.0211%2C1005.5253%2C1005.0054%2C1005.1798%2C1005.1127%2C1005.4697%2C1005.5500%2C1005.2490%2C1005.3344%2C1005.0366%2C1005.5669%2C1005.3978%2C1005.4516%2C1005.1204%2C1005.5607%2C1005.3629%2C1005.4928%2C1005.3135%2C1005.2926%2C1005.4133%2C1005.4942%2C1005.5225%2C1005.3729%2C1005.3174%2C1005.2911%2C1005.4191%2C1005.4965%2C1005.4612%2C1005.0202%2C1005.0966%2C1005.4304%2C1005.3869%2C1005.4659%2C1005.5306%2C1005.0608%2C1005.4465%2C1005.5059%2C1005.2509%2C1005.4738%2C1005.1154%2C1005.4260%2C1005.3804%2C1005.4301%2C1005.1233%2C1005.4262%2C1005.0386%2C1005.5735&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Recent work has demonstrated that using a carefully designed dictionary\ninstead of a predefined one, can improve the sparsity in jointly representing a\nclass of signals. This has motivated the derivation of learning methods for\ndesigning a dictionary which leads to the sparsest representation for a given\nset of signals. In some applications, the signals of interest can have further\nstructure, so that they can be well approximated by a union of a small number\nof subspaces (e.g., face recognition and motion segmentation). This implies the\nexistence of a dictionary which enables block-sparse representations of the\ninput signals once its atoms are properly sorted into blocks. In this paper, we\npropose an algorithm for learning a block-sparsifying dictionary of a given set\nof signals. We do not require prior knowledge on the association of signals\ninto groups (subspaces). Instead, we develop a method that automatically\ndetects the underlying block structure. This is achieved by iteratively\nalternating between updating the block structure of the dictionary and updating\nthe dictionary atoms to better fit the data. Our experiments show that for\nblock-sparse data the proposed algorithm significantly improves the dictionary\nrecovery ability and lowers the representation error compared to dictionary\nlearning methods that do not employ block structure."}, "authors": ["Kevin Rosenblum", "Lihi Zelnik-Manor", "Yonina C. Eldar"], "author_detail": {"name": "Yonina C. Eldar"}, "author": "Yonina C. Eldar", "arxiv_comment": "submitted to IEEE Transactions on Signal Processing", "links": [{"href": "http://arxiv.org/abs/1005.0202v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1005.0202v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1005.0202v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1005.0202v1", "journal_reference": null, "doi": null, "fulltext": "1\n\nDictionary Optimization for Block-Sparse\nRepresentations\n\narXiv:1005.0202v1 [cs.IT] 3 May 2010\n\nKevin Rosenblum, Lihi Zelnik-Manor, Yonina C. Eldar\n\nAbstract-Recent work has demonstrated that using a carefully\ndesigned dictionary instead of a predefined one, can improve\nthe sparsity in jointly representing a class of signals. This has\nmotivated the derivation of learning methods for designing a\ndictionary which leads to the sparsest representation for a given\nset of signals. In some applications, the signals of interest can\nhave further structure, so that they can be well approximated by\na union of a small number of subspaces (e.g., face recognition and\nmotion segmentation). This implies the existence of a dictionary\nwhich enables block-sparse representations of the input signals\nonce its atoms are properly sorted into blocks. In this paper, we\npropose an algorithm for learning a block-sparsifying dictionary\nof a given set of signals. We do not require prior knowledge on the\nassociation of signals into groups (subspaces). Instead, we develop\na method that automatically detects the underlying block structure. This is achieved by iteratively alternating between updating\nthe block structure of the dictionary and updating the dictionary\natoms to better fit the data. Our experiments show that for blocksparse data the proposed algorithm significantly improves the\ndictionary recovery ability and lowers the representation error\ncompared to dictionary learning methods that do not employ\nblock structure.\n\nI. I NTRODUCTION\nThe framework of sparse coding aims at recovering an\nunknown vector \u03b8 \u2208 RK from an under-determined system of\nlinear equations x = D\u03b8, where D \u2208 RN \u00d7K is a dictionary,\nand x \u2208 RN is an observation vector with N < K. Since the\nsystem is under-determined, \u03b8 can not be recovered without\nadditional information. The framework of compressed sensing\n[1], [2] exploits sparsity of \u03b8 in order to enable recovery.\nSpecifically, when \u03b8 is known to be sparse so that it contains\nfew nonzero coefficients, and when D is chosen properly,\nthen \u03b8 can be recovered uniquely from x = D\u03b8. Recovery is\npossible irrespectively of the locations of the nonzero entries\nof \u03b8. This result has given rise to a multitude of different\nrecovery algorithms. Most prominent among them are Basis\nPursuit (BP) [3], [1] and Orthogonal Matching Pursuit (OMP)\n[4], [5].\nRecent work [6], [7], [8], [9], [10] has demonstrated that\nadapting the dictionary D to fit a given set of signal examples leads to improved signal reconstruction. At the price\nof being slow, these learning algorithms attempt to find a\ndictionary that leads to optimal sparse representations for\na certain class of signals. These methods show impressive\nresults for representations with arbitrary sparsity structures.\nIn some applications, however, the representations have a\nunique sparsity structure that can be exploited. Our interest\nThe authors are with the Technion - Israel Institute of Technology,\nHaifa, Israel. Email: kevin@tx.technion.ac.il, lihi@ee.technion.ac.il, yonina@ee.technion.ac.il.\n\nis in the case of signals that are known to be drawn from\na union of a small number of subspaces [11], [12]. This\noccurs naturally, for example, in face recognition [13], [14],\nmotion segmentation [15], multiband signals [16], [17], [18],\nmeasurements of gene expression levels [19], and more. For\nsuch signals, sorting the dictionary atoms according to the\nunderlying subspaces leads to sparse representations which\nexhibit a block-sparse structure, i.e., the nonzero coefficients\noccur in clusters of varying sizes. Several methods, such as\nBlock BP (BBP) [11], [20], [21] and Block OMP (BOMP) [22],\n[23] have been proposed to take advantage of this structure in\nrecovering the block-sparse representation \u03b8. These methods\ntypically assume that the dictionary is predetermined and the\nblock structure is known.\nIn this paper we propose a method for designing a blocksparsifying dictionary for a given set of signals. In other\nwords, we wish to find a dictionary that provides blocksparse representations best suited to the signals in a given\nset. To take advantage of the block structure via block-sparse\napproximation methods, it is necessary to know the block\nstructure of the dictionary. We do not assume that it is known\na-priori. Instead, we infer the block structure from the data\nwhile adapting the dictionary.\nWe start by formulating this task as an optimization problem. We then present an algorithm for minimizing the proposed objective, which iteratively alternates between updating\nthe block structure and updating the dictionary. The block\nstructure is inferred by the agglomerative clustering of dictionary atoms that induce similar sparsity patterns. In other\nwords, after finding the sparse representations of the training\nsignals, the atoms are progressively merged according to the\nsimilarity of the sets of signals they represent. A variety of\nsegmentation methods through subspace modeling have been\nproposed recently [24], [25], [26]. These techniques learn an\nunderlying collection of subspaces based on the assumption\nthat each of the samples lies close to one of them. However,\nunlike our method, they do not treat the more general case\nwhere the signals are drawn from a union of several subspaces.\nThe dictionary blocks are then sequentially updated to\nminimize the representation error at each step. The proposed\nalgorithm is an intuitive extension of the K-SVD algorithm [6],\nwhich yields sparsifying dictionaries by sequentially updating\nthe dictionary atoms, to the case of block structures. In other\nwords, when the blocks are of size 1 our cost function and\nthe algorithm we propose reduce to K-SVD. Our experiments\nshow that updating the dictionary block by block is preferred\nover updating the atoms in the dictionary one by one, as in\nK-SVD.\nWe show empirically that both parts of the algorithm\n\n\f2\n\nare indispensable to obtain high performance. While fixing\na random block structure and applying only the dictionary\nupdate part leads to improved signal reconstruction compared\nto K-SVD, combining the two parts leads to even better results.\nFurthermore, our experiments show that K-SVD often fails to\nrecover the underlying block structure. This is in contrast to\nour algorithm which succeeds in detecting most of the blocks.\nWe begin by reviewing previous work on dictionary design\nin Section II. In Section III-A we present an objective for\ndesigning block-sparsifying dictionaries. We show that this\nobjective is a direct extension of the one used by K-SVD. We\nthen propose an algorithm for minimizing the proposed cost\nfunction (Section III-B). In Section III-C we give a detailed\ndescription of the algorithm for finding a block structure\nand in Section III-D we describe the dictionary update part.\nWe evaluate the performance of the proposed algorithms and\ncompare them to previous work in Section IV.\nThroughout the paper, we denote vectors by lowercase\nletters, e.g., x, and matrices by uppercase letters, e.g., A. The\njth column of a matrix A is written as Aj , and the ith row\nas Ai . The sub-matrix containing the entries of A in the rows\nwith indices r and the columns with indicesqc is denoted Arc .\nP\n2\nThe Frobenius norm is defined by kAkF \u2261\nj kAj k2 . The\nith element of a vector x is denoted x[i]. kxkp is its lp -norm\nand kxk0 counts the number of non-zero entries in x.\nII. P RIOR\n\nWORK ON DICTIONARY DESIGN\n\nThe goal in dictionary learning is to find a dictionary D and\na representation matrix \u0398 that best match a given set of vectors\nXi that are the columns of X. In addition, we would like each\nvector \u0398i of \u0398 to be sparse. In this section we briefly review\ntwo popular sparsifying dictionary design algorithms, K-SVD\n[6], [27] and MOD (Method of Optimal Directions) [7]. We\nwill generalize these methods to block-sparsifying dictionary\ndesign in Section III.\nTo learn an optimal dictionary, both MOD and K-SVD\nattempt to optimize the same cost function for a given sparsity\nmeasure k:\nmin\nD,\u0398\n\ns.t.\n\nkX \u2212 D\u0398kF\nk\u0398i k0 \u2264 k, i = 1, . . . , L\n\n(1)\n\napproaches are strongly dependent on the initial dictionary\nD(0) . The convention is to initialize D(0) as a collection of\nK data signals from the same class as the training signals X.\nThe first step of the nth iteration in both algorithms optimizes \u0398 given a fixed dictionary D(n\u22121) , so that (1) becomes:\n\u0398(n) = arg min\n\u0398\n\ns.t.\n\nkX \u2212 D(n\u22121) \u0398kF\nk\u0398i k0 \u2264 k, i = 1, . . . , L.\n\n(2)\n\nThis problem can be solved approximately using sparse coding\nmethods such as BP or OMP for each column of \u0398, since the\nproblem is separable in these columns. Next, \u0398(n) is kept fixed\nand the representation error is minimized over D:\nD(n) = arg min kX \u2212 D\u0398(n) kF .\nD\n\n(3)\n\nThe difference between MOD and K-SVD lies in the choice of\noptimization method for D(n) . While K-SVD converges faster\nthan MOD, both methods yield similar results.\nThe MOD algorithm treats the problem in (3) directly. This\nproblem has a closed form solution given by the pseudoinverse:\nD(n) = X\u0398\u2032(n) (\u0398(n) \u0398\u2032(n) )\u22121 .\n(4)\nHere we assume for simplicity that \u0398(n) \u0398\u2032(n) is invertible.\nThe K-SVD method solves (3) differently. The columns in\nD(n\u22121) are updated sequentially, along with the corresponding\nnon-zero coefficients in \u0398(n) . This parallel update leads to\na significant speedup while preserving the sparsity pattern\nof \u0398(n) . For j = 1, . . . , K, the update is as follows. Let\n\u03c9j \u2261 {i \u2208 1, . . . , L|\u0398ji 6= 0} be the set of indices corresponding to columns in \u0398(n) that use the atomP\nDj , i.e., their ith\nrow is non-zero. Denote by R\u03c9j = X\u03c9j \u2212 i6=j (Di \u0398i\u03c9j ) the\nrepresentation error of the signals X\u03c9j excluding the contribution of the jth atom. The representation error of the signals\nwith indices \u03c9j can then be written as kR\u03c9j \u2212 Dj \u0398j\u03c9j kF . The\ngoal of the update step is to minimize this representation error,\nwhich is accomplished by choosing\nDj = U1 ,\n\n\u0398j\u03c9j = \u220611 V1\u2032 .\n\nHere U \u2206V \u2032 is the Singular Value Decomposition (SVD) of\nR\u03c9j . Note, that the columns of D remain normalized after the\nupdate. The K-SVD algorithm obtains the dictionary update\nby K separate SVD computations, which explains its name.\n\nN \u00d7L\n\nwhere X \u2208 R\nis a matrix containing L given input\nsignals, D \u2208 RN \u00d7K is the dictionary and \u0398 \u2208 RK\u00d7L is a\nsparse representation of the signals. Note that the solution of\n(1) is never unique due to the invariance of D to permutation\nand scaling of columns. This is partially resolved by requiring\nnormalized columns in D. We will therefore assume throughout the paper that the columns of D are normalized to have\nl2 -norm equal 1.\nProblem (1) is non-convex and NP-hard in general. Both\nMOD and K-SVD attempt to approximate (1) using a relaxation technique which iteratively fixes all the parameters but\none, and optimizes the objective over the remaining variable.\nIn this approach the objective decreases (or is left unchanged)\nat each step, so that convergence to a local minimum is\nguaranteed. Since this might not be the global optimum both\n\nIII. B LOCK -S PARSIFYING D ICTIONARY\n\nOPTIMIZATION\n\nWe now formulate the problem of block-sparsifying dictionary design. We then propose an algorithm which can be\nseen as a natural extension of K-SVD for the case of signals\nwith block sparse representations. Our method involves an\nadditional clustering step in order to determine the block\nstructure.\nA. Problem definition\nN\nFor a given set of L signals X = {Xi }L\ni=1 \u2208 R , we wish\nN \u00d7K\nto find a dictionary D \u2208 R\nwhose atoms are sorted in\nblocks, and which provides the most accurate representation\nvectors whose non-zero values are concentrated in a fixed\n\n\f3\n\ncollection of K signals leads to similar results, but slightly\nslower convergence). Then, at each iteration n we perform\nthe following two steps:\n1) Recover the block structure by solving (5) for d and \u0398\nwhile keeping D(n\u22121) fixed:\n[d(n) , \u0398(n) ] = min\nd,\u0398\n\ns.t.\nFig. 1. Two equivalent examples of dictionaries D and block structures d\nwith 5 blocks, together with 2-block-sparse representations \u03b8. Both examples\nrepresent the same signal, since the atoms in D and the entries of d and \u03b8\nare permuted in the same manner.\n\nnumber of blocks. In previous works dealing with the blocksparse model, it is typically assumed that the block structure\nin D is known a-priori, and even more specifically, that the\natoms in D are sorted according to blocks [11], [20]. Instead,\nin this paper we address the more general case where the block\nstructure is unknown and the blocks can be of varying sizes.\nThe only assumption we make on the block structure is that\nthe maximal block size, denoted by s, is known.\nMore specifically, suppose we have a dictionary whose\natoms are sorted in blocks that enable block-sparse representations of the input signals. Assume that each block is given an\nindex number. Let d \u2208 RK be the vector of block assignments\nfor the atoms of D, i.e., d[i] is the block index of the atom\nDi . We say that a vector \u03b8 \u2208 RK is k-block-sparse over d\nif its non-zero values are concentrated in k blocks only. This\nis denoted by k\u03b8k0,d = k, where k\u03b8k0,d is the l0 -norm over\nd and counts the number of non-zero blocks as defined by d.\nFig. 1 presents examples of two different block structures and\ntwo corresponding block-sparse vectors and dictionaries.\nOur goal is to find a dictionary D and a block structure d,\nwith maximal block size s, that lead to optimal k-block sparse\nrepresentations \u0398 = {\u0398i }L\ni=1 for the signals in X:\nmin\n\nD,d,\u0398\n\ns.t.\n\nkX \u2212 D\u0398kF\nk\u0398i k0,d \u2264 k, i = 1, . . . , L\n|dj | \u2264 s, j \u2208 d\n\n[D(n) , \u0398(n) ] = min\nD,\u0398\n\ns.t.\n\nB. Algorithm Preview\nIn this section, we propose a framework for solving (5).\nSince this optimization problem is non-convex, we adopt the\ncoordinate relaxation technique. We initialize the dictionary\nD(0) as the outcome of the K-SVD algorithm (using a random\n\nk\u0398i k0,d \u2264 k, i = 1, . . . , L\n|dj | \u2264 s, j \u2208 d.\n\nkX \u2212 D\u0398kF\n\n(7)\n\nk\u0398i k0,d(n) \u2264 k, i = 1, . . . , L.\n\nIn Section III-D we propose an algorithm, referred to as\nBlock K-SVD (BK-SVD), for solving (8). This technique\ncan be viewed as a generalization of K-SVD since the\nblocks in D(n) are sequentially updated together with\nthe corresponding non-zero blocks in \u0398(n) .\nIn the following sections we describe in detail the steps\nof this algorithm. The overall framework is summarized in\nAlgorithm 1.\nAlgorithm 1 Block-Sparse Dictionary Design\nInput: A set of signals X, block sparsity k and maximal block\nsize s.\nTask: Find a dictionary D, block structure d and the corresponding sparse representation \u0398 by optimizing:\nmin\n\nwhere dj = {i \u2208 1, . . . , K|d[i] = j} is the set of indices\nbelonging to block j (i.e., the list of atoms in block j).\nThe case when there is no underlying block structure or\nwhen the block structure is ignored, is equivalent to setting s =\n1 and d = [1, . . . , K]. Substituting this into (5), reduces it to\n(1). In this setting, the objective and the algorithm we propose\ncoincide with K-SVD. In Section IV we demonstrate through\nsimulations that when an underlying block structure exists,\noptimizing (5) via the proposed framework improves recovery\nresults and lowers the representation errors with respect to (1).\n\n(6)\n\nAn exact solution would require a combinatorial search\nover all feasible d and \u0398. Instead, we propose a tractable\napproximation to (6) in Section III-C, referred to as\nSparse Agglomerative Clustering (SAC). Agglomerative\nclustering builds blocks by progressively merging the\nclosest atoms according to some distance metric [28],\n[29]. SAC uses the l0 -norm for this purpose.\n2) Fit the dictionary D(n) to the data by solving (5) for D\nand \u0398 while keeping d(n) fixed:\n\nD,d,\u0398\n\n(5)\n\nkX \u2212 D(n\u22121) \u0398kF\n\ns.t.\n\nkX \u2212 D\u0398kF\nk\u0398i k0,d \u2264 k, i = 1, . . . , L\n|dj | \u2264 s, j \u2208 d.\n\nInitialization: Set the initial dictionary D(0) as the outcome of\nK-SVD.\nRepeat from n = 1 until convergence:\n1) Fix D(n\u22121) , and update d(n) and \u0398(n) by applying\nSparse Agglomerative Clustering.\n2) Fix d(n) , and update D(n) and \u0398(n) by applying BKSVD.\n3) n = n + 1.\n\nC. Block Structure Recovery: Sparse Agglomerative Clustering\nIn this section we propose a method for recovering the block\nstructure d given a fixed dictionary D, as outlined in Fig. 2(a).\nThe suggested method is based on the coordinate relaxation\n\n\f4\n\ntechnique to solve (6) efficiently. We start by initializing d and\n\u0398. Since we have no prior knowledge on d it is initialized as\nK blocks of size 1, i.e. d = [1, . . . , K]. To initialize \u0398 we\nkeep d fixed and solve (6) over \u0398 using OMP with k \u00d7 s\ninstead of k non-zero entries, since the signals are known to\nbe combinations of k blocks of size s. Based on the obtained\n\u0398, we first update d as described below and then again \u0398\nusing BOMP [22]. The BOMP algorithm sequentially selects\nthe dictionary blocks that best match the input signals Xi , and\ncan be seen as a generalization of the OMP algorithm to the\ncase of blocks.\nTo update d we wish to solve (6) while keeping \u0398 fixed.\nAlthough the objective does not depend on d, the constraints\ndo. Therefore, the problem becomes finding a block structure\nwith maximal block size s that meets the constraint on the\nblock-sparsity of \u0398. To this end, we seek to minimize the\nblock-sparsity of \u0398 over d:\nmin\nd\n\nL\nX\n\nk\u0398i k0,d s.t. |dj | \u2264 s, j \u2208 d.\n\n(8)\n\ni=1\n\nBefore we describe how (8) is optimized we first wish to\nprovide some insight. When a signal Xi is well represented\nby the unknown block dj , then the corresponding rows in \u0398i\nare likely to be non-zero. Therefore, rows of \u0398 that exhibit\na similar pattern of non-zeros are likely to correspond to\ncolumns of the same dictionary block. Therefore, grouping\ndictionary columns into blocks is equivalent to grouping rows\nof \u0398 according to their sparsity pattern. To detect rows with\nsimilar sparsity patterns we next rewrite the objective of (8)\nas a function of the pattern on non-zeros.\nLet \u03c9j (\u0398, d) denote the list of columns in \u0398 that have\nnon-zero values in rows corresponding to block dj , i.e.,\nd\n\u03c9j (\u0398, d) = {i \u2208 1, . . . , L| k\u0398i j k2 > 0}. Problem (8) can\nnow be rewritten as:\nX\n|\u03c9j (\u0398, d)| s.t. |dj | \u2264 s, j \u2208 d\n(9)\nmin\nd\n\nj\u2208d\n\nwhere |\u03c9j | denotes the size of the list \u03c9j . We propose using a\nsub-optimal tractable agglomerative clustering algorithm [29]\nto minimize this objective. At each step we merge the pair\nof blocks that have the most similar pattern of non-zeros in\n\u0398, leading to the steepest descent in the objective. We allow\nmerging blocks as long as the maximum block size s is not\nexceeded.\nMore specifically, at each step we find the pair of blocks\n(j1\u2217 , j2\u2217 ) such that:\n[j1\u2217 , j2\u2217 ] = arg max |\u03c9j1 \u2229 \u03c9j2 | s.t. |dj1 | + |dj2 | \u2264 s.\nj1 6=j2\n\nj1\u2217\n\nWe then merge\nand j2\u2217 by setting \u2200i \u2208 dj2 : d[i] \u2190 j1 ,\n\u03c9j1 \u2190 {\u03c9j1 \u222a \u03c9j2 }, and \u03c9j2 \u2190 \u00f8. This is repeated until\nno blocks can be merged without breaking the constraint\non the block size. We do not limit the intersection size\nfor merging blocks from below, since merging is always\nbeneficial. Merging blocks that have nothing in common may\nnot reduce the objective of (8); however, this can still lower\nthe representation error at the next BK-SVD iteration. Indeed,\nwhile the number of blocks k stays fixed, the number of atoms\n\nthat can be used to reduce the error increases.\nFig. 2(b) presents an example that illustrates the notation\nand the steps of the algorithm. In this example the maximal\nblock size is s = 2. At initialization the block structure is set\nto\nPLd = [1, 2, 3, 4], which implies that the objective of (8) is\ni=1 k\u0398i k0,d = 2 + 1 + 2 + 2 = 7. At the first iteration, \u03c91\nand \u03c93 have the largest intersection. Consequently, blocks 1\nand 3 are merged. At the second iteration, \u03c92 and \u03c94 have the\nlargest intersection, so that blocks 2 and 4 are merged. This\nresults in the block structure d = [1, 2, 1, 2] where no blocks\ncan be merged without surpassing P\nthe maximal block size.\nL\nThe objective of (8) is reduced to i=1 k\u0398i k0,d = 4, since\nall 4 columns in \u0398 are 1-block-sparse. Note that since every\ncolumn contains non-zero values, this is the global minimum\nand therefore the algorithm succeeded in solving (8).\nWhile more time-efficient clustering methods exist, we have\nselected agglomerative clustering because it provides a simple\nand intuitive solution to our problem. Partitional clustering\nmethods, such as K-Means, require initialization and are\ntherefore not suited for highly sparse data and the l0 -norm\nmetric. Moreover, since oversized blocks are unwanted, it is\npreferable to limit the block size rather than the number of\nblocks. It is important to note that due to the iterative nature\nof our dictionary design algorithm, clustering errors can be\ncorrected in the following iteration, after the dictionary has\nbeen refined.\nD. Block K-SVD Algorithm\nWe now propose the BK-SVD algorithm for recovering the\ndictionary D and the representations \u0398 by optimizing (8) given\na block structure d and input signals X.\nUsing the coordinate relaxation technique, we solve this\nproblem by minimizing the objective based on alternating \u0398\nand D. At each iteration m, we first fix D(m\u22121) and use\nBOMP to solve (8) which reduces to\n\u0398(m) = arg min\n\u0398\n\ns.t.\n\nkX \u2212 D(m\u22121) \u0398kF\nk\u0398i k0,d \u2264 k, i = 1, . . . , L. (10)\n\nNext, to obtain D(m) we fix \u0398(m) , d and X, and solve:\nD(m) = arg min kX \u2212 D\u0398(m) kF .\nD\n\n(11)\n\nInspired by the K-SVD algorithm, the blocks in D(m\u22121) are\nupdated sequentially, along with the corresponding non-zero\ncoefficients in \u0398(m) . For every block\nP j \u2208 d, the update is as\nfollows. Denote by R\u03c9j = X\u03c9j \u2212 i6=j Ddi \u0398d\u03c9ij the representation error of the signals X\u03c9j excluding the contribution of\nthe jth block. Here \u03c9j and dj are defined as in the previous\nsubsection. The representation error of the signals with indices\nd\n\u03c9j can then be written as kR\u03c9j \u2212 Ddj \u0398\u03c9jj kF . Finally, the\nd\nrepresentation error is minimized by setting Ddj \u0398\u03c9jj equal to\nthe matrix of rank |dj | that best approximates R\u03c9j . This can\nobtained by the following updates:\nDdj = [U1 , . . . , U|dj | ]\n|d |\n\n\u0398d\u03c9jj = [\u220611 V1 , . . . , \u2206|djj | V|dj | ]\u2032\n\n\f5\n\n(a)\n\nwhere the |dj | highest rank components of R\u03c9j are computed\nusing the SVD R\u03c9j = U \u2206V \u2032 . The updated Ddj is now an\northonormal basis that optimally represents the signals with\nindices \u03c9j . Note that the representation error is also minimized\nd\nwhen multiplying Ddj on the right by W and \u0398\u03c9jj on the\n\u22121\n|dj |\u00d7|dj |\nleft by W , where W \u2208 R\nis an invertible matrix.\nHowever, if we require the dictionary blocks to be orthonormal\nsubspaces, the solution is unique up to the permutation of\nthe atoms. It is also important to note that if |dj | > |\u03c9j |,\nthen |dj | \u2212 |\u03c9j | superfluous atoms in block j can be discarded\nwithout any loss of performance.\nThis dictionary update minimizes the representation error\nwhile preserving the sparsity pattern of \u0398(m) , as in the K-SVD\ndictionary update step. However, the update step in the BKSVD algorithm converges faster thanks to the simultaneous\noptimization of the atoms belonging to the same block. Our\nsimulations show that it leads to smaller representation errors\nas well. Moreover, the dictionary update step in BK-SVD\nrequires about s times less SVD computations, which makes\nthe proposed algorithm significantly faster than K-SVD.\nWe next present a simple example illustrating the advantage\nof the BK-SVD dictionary update step, compared to the KSVD update. Let D1 and D2 be the atoms of the same\nblock, of size 2. A possible scenario is that D2 = U1\nand \u03982\u03c9j = \u2212\u2206(1, 1)V1\u2032 . In K-SVD, the first update of\nD is D1 \u2190 U1 and \u03981\u03c9j \u2190 \u2206(1, 1)V1\u2032 . In this case the\nsecond update would leave D2 and \u03982\u03c9j unchanged. As a\nconsequence, only the highest rank component of R\u03c9j is\nremoved. Conversely, in the proposed BK-SVD algorithm, the\natoms D1 and D2 are updated simultaneously, resulting in the\ntwo highest rank components of R\u03c9j being removed.\nIV. E XPERIMENTS\nIn this section, we evaluate the contribution of the proposed\nblock-sparsifying dictionary design framework empirically.\nWe also examine the performance of the SAC and the BKSVD algorithms separately.\nFor each simulation, we repeat the following procedure 50\ntimes: We randomly generate a dictionary D\u2217 of dimension\n30 \u00d7 60 with normally distributed entries and normalize its\ncolumns. The block structure is chosen to be of the form:\nd\u2217 = [1, 1, 1, 2, 2, 2, . . . , 20, 20, 20]\n\n(b)\nFig. 2. (a) A flow chart describing the SAC algorithm. (b) A detailed example\nof the decision making process in the SAC algorithm.\n\ni.e. D\u2217 consists of 20 subspaces of size s = 3. We generate\nL = 5000 test signals X of dimension N = 30, that\nhave 2-block sparse representations \u0398\u2217 with respect to D\u2217\n(i.e. k = 2). The generating blocks are chosen randomly\nand independently and the coefficients are i.i.d. uniformly\ndistributed. White Gaussian noise with varying SNR was\nadded to X.\nWe perform three experiments:\n1) Given D\u2217 and X, we examine the ability of SAC to\nrecover d\u2217 .\n2) Given d\u2217 and X, we examine the ability of BK-SVD to\nrecover D\u2217 .\n3) We examine the ability of BK-SVD combined with SAC\nto recover D\u2217 and d\u2217 given only X.\n\n\f6\n\n(a)\n0.8\nSAC\noracle\n\n0.6\n\n10\n\n20\n\n30\n\n0\n\n40\n\n2\n\n3\n\n4\n\n5\n\n6\n\nk\n\n(b)\n\n(e)\n\n0.4\n\n0.3\n\n0.3\n\n0\n\n20\n\n0.2\n\n40\n\nSNR\n\n50\n\n0\n\n10\n\n20\n\n30\n\n0\n\n40\n\n1\n\n2\n\n3\n\n4\n\nSNR\n\nk\n\n(c)\n\n(f)\n\n5\n\n6\n\n12\n\n10\n\n80\n\n\u221210\n\n0\n\n10\n\nSNR\n\n20\n\n30\n\n40\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nk\n\nFig. 3. Simulation results of the SAC algorithm. The graphs show e, p and\nb as a function of the SNR of the data signals for k = 2 (a, b, c), and as a\nfunction of k in a noiseless setting (d, e, f).\n\nWe use two measures to evaluate the success of the simulations based on their outputs D, d and \u0398:\nkX\u2212D\u0398kF\n.\n\u2022 The normalized representation error e =\nkXkF\n\u2022 The percentage p of successfully recovered blocks. For\nevery block in D, we match the closest block in D\u2217 without repetition, where the (normalized) distance between\ntwo blocks S1 and S2 (of sizes s1 and s2 ) is measured\nby:\ns\u0012\n\u0013\nkS1\u2032 S2 k2F\n1\u2212\nDist(S1 , S2 ) \u2261\nmax(s1 , s2 )\nassuming that both blocks are orthonormalized. If the\ndistance between the block in D and its matched block\nin D\u2217 is smaller than 0.01, we consider the recovery of\nthis block as successful.\nA. Evaluating SAC\nTo evaluate the performance of the SAC algorithm, we\nassume that D\u2217 is known, and use SAC to reconstruct d\u2217\nand then BOMP to approximate \u0398\u2217 . The SAC algorithm is\nevaluated as a function of the SNR of the signals X for k = 2,\nand as a function of k in a noiseless setting. In addition to\ne and p, Fig. 3 also shows the objective of (8), which we\ndenote by b. We compare our results with those of an \"oracle\"\nalgorithm, which is given as input the true block structure d\u2217 .\nIt then uses BOMP to find \u0398. The oracle's results provide a\nlower bound on the reconstruction error of our algorithm (we\ncannot expect our algorithm to outperform the oracle). It can\nbe seen that for SNR higher than \u22125[dB], the percentage p of\nsuccessfully recovered blocks quickly increases to 100% (Fig.\n3.(b)), the representation error e drops to zero (Fig. 3.(a)) and\nthe block-sparsity b drops to the lowest possible value k = 2\n(Fig. 3.(c)). Fig. 3.(e) shows that the block structure d\u2217 is\nperfectly recovered for k < 6. However, for k = 6, SAC fails\nin reconstructing the block structure d\u2217 , even though the block\nsparsity b reaches the lowest possible value (Fig. 3.(f)). This is\na consequence of the inability of OMP to recover the sparsest\n\n4\n\n60\n6\n\n40\n4\n\n4\n\n0\n\n3\n\n8\n\n5\n\n3\n\n4\n\n(f)\n100\n\n6\n\nb\n\n4\n\n3\n\nk\n\n8\n\n10\n\n2\n\n(d)\n\np\n\n5\n\n1\n\n12\n\n10\n\n15\n\n0.1\n\n50 100 150 200 250\n\n14\n\np\n\n\u221210\n\n6\n\nb\n\n0.2\n\nIterations\n\n(b)\n\n2\n\n0.3\n\np\n\np\n\n0.2\n\np\n\ne\n\n0.4\n\n100\n\n50\n\n0\n\n0.4\n\n0.5\n\ne\n1\n\nSNR\n100\n\n0.6\n\n0.5\n\n0.2\n0\n\n0.5\n\n0.4\n\n0.2\n\u221210\n\n(e)\n\n0.7\nBK\u2212SVD\n(B)K\u2212SVD\n\ne\n\n0.4\n\n(c)\n\n0.7\n\n0.6\n\ne\n\ne\n\n0.6\n\n0\n\n(a)\n\n(d)\n\n0.8\n\n2\n\n2\n\n0\n\n0\n\n0\n\n20\n\n40\n\nSNR\n\n20\n\n50 100 150 200 250\n\nIterations\n\n0\n\n1\n\n2\n\nk\n\nFig. 4. Simulation results of the BK-SVD and (B)K-SVD algorithms. The\ngraphs show the reconstruction error e and the recovery percentage p as a\nfunction of the SNR of the data signals for k = 2 and after 250 iterations (a,\nb), as a function of the number of iterations for k = 2 in a noiseless setting\n(c, d), and as a function of k in a noiseless setting after 250 iterations (e, f).\n\napproximation of the signals X with k \u00d7 s = 12 nonzero\nentries. In terms of e and b, our algorithm performs nearly as\ngood as the oracle.\nB. Evaluating BK-SVD\nTo evaluate the performance of the BK-SVD algorithm we\nassume that the block structure d\u2217 is known. We initialize\nthe dictionary D(0) by generating 20 blocks of size 3 where\neach block is a randomly generated linear combination of\n2 randomly selected blocks of D\u2217 . We then evaluate the\ncontribution of the proposed BK-SVD algorithm. Recall that\ndictionary design consists of iterations between two steps,\nupdating \u0398 using block-sparse approximation and updating the\nblocks in D and their corresponding non-zero representation\ncoefficients. To evaluate the contribution of the latter step,\nwe compare its performance with that of applying the same\nscheme, but using the K-SVD dictionary update step. We refer\nto this algorithm as (B)K-SVD. The algorithms are evaluated\nas a function of the SNR of the signals X for k = 2 after\n250 iterations, as a function of the number of iterations for\nk = 2 in a noiseless setting, and as a function of k in a\nnoiseless setting after 250 iterations. It is clear from Fig. 4\nthat the simultaneous update of the atoms in the blocks of\nD is imperative and does not only serve as a speedup of the\nalgorithm.\nC. Evaluating the overall framework\nTo evaluate the performance of the overall block-sparsifying\ndictionary design method, we combine SAC and BK-SVD. At\neach iteration we only run BK-SVD once instead of waiting\nfor it to converge, improving the ability of the SAC algorithm\nto avoid traps. Our results are compared with those of K-SVD\n(with a fixed number of 8 coefficients) and with those of BKSVD (with a fixed block structure) as a function of the SNR,\nas a function of the number of iterations. The algorithms are\n\n\f7\n\n(a)\n\n(c)\n\n0.7\n\n(e)\n\n0.5\nK\u2212SVD\nBK\u2212SVD+SAC\nBK\u2212SVD\noracle\n\n0.6\n0.5\n\n0.4\n\n0.3\n\ne\n\ne\n0.3\n\ne\n\n0.3\n\n0.4\n\n0.2\n\n0.2\n\n0.2\n\n0.1\n\n0.1\n\n0.1\n0\n\nD. Choosing the maximal block size\n\n0.4\n\n0\n\n20\n\n0\n\n40\n\nSNR\n\n0\n\n50 100 150 200 250\n\n(b)\n\n(d)\n\n80\n\n60\n\n60\n\n40\n\n40\n\n40\n\n20\n\n20\n\n20\n\n0\n\n0\n\n40\n\n4\n\n3\n\n4\n\np\n\n80\n\n60\n\np\n\n80\n\np\n\n100\n\nSNR\n\n3\n\n(f)\n\n100\n\n20\n\n2\n\nk\n\n100\n\n0\n\n1\n\nIterations\n\n50 100 150 200 250\n\nIterations\n\n0\n\n1\n\n2\n\nk\n\nFig. 5. Simulation results of our overall algorithm (BK-SVD+SAC), the BKSVD algorithm and the K-SVD algorithm. The graphs show the reconstruction\nerror e and the recovery percentage p as a function of the SNR of the data\nsignals for k = 2 after 250 iterations (a, b), as a function of the number of\niterations for k = 2 in a noiseless setting (c, d), and as a function of k in a\nnoiseless setting after 250 iterations (e, f).\n\nevaluated as a function of the SNR of the signals X for k = 2\nafter 250 iterations, as a function of the number of iterations\nfor k = 2 in a noiseless setting, and as a function of k in a\nnoiseless setting after 250 iterations (Fig. 5).\nOur experiments show that for SNR > 10[dB], the proposed\nblock-sparsifying dictionary design algorithm yields lower\nreconstruction errors (see Fig. 5.(a)) and a higher percentage\nof correctly reconstructed blocks (see Fig. 5.(b)), compared\nto K-SVD. Moreover, even in a noiseless setting, the K-SVD\nalgorithm fails to recover the sparsifying dictionary, while our\nalgorithm succeeds in recovering 93% of the dictionary blocks,\nas shown in Fig. 5.(d).\nFor SNR \u2264 10[dB] we observe that K-SVD reaches lower\nreconstruction error compared to our block-sparsifying dictionary design algorithm. This is since when the SNR is low the\nblock structure is no longer present in the data and the use of\nblock-sparse approximation algorithms is unjustified. To verify\nthis is indeed the cause for the failure of our algorithm, we\nfurther compare our results with those of an oracle algorithm,\nwhich is given as input the true dictionary D\u2217 and block\nstructure d\u2217 . It then uses BOMP to find \u0398. Fig. 5 shows that\nfor all noise levels, our algorithm performs nearly as good\nas the oracle. Furthermore, for SNR \u2264 10[dB] we observe\nthat K-SVD outperforms the oracle, implying that the use of\nblock-sparsifying dictionaries is unjustified. For k <= 3, in a\nnoiseless setting, the performance of our algorithm lies close\nto that of the oracle, and outperforms the K-SVD algorithm.\nHowever, we note that this is not the case for k >= 4.\nFinally, we wish to evaluate the contribution of the SAC\nalgorithm to the overall framework. One could possibly fix an\ninitial block structure and then iteratively update the dictionary\nusing BK-SVD, in hope that this will recover the block structure. Fig. 5 shows that the representation error e is much lower\nwhen including SAC in the overall framework. Moreover, BKSVD consistently fails in recovering the dictionary blocks.\n\nWe now consider the problem of setting the maximal block\nsize in the dictionary, when all we are given is that the sizes of\nthe blocks are in the range [sl sh ]. This also includes the case\nof varying block sizes. Choosing the maximal block size s to\nbe equal to sl will not allow to successfully reconstruct blocks\ncontaining more than sl atoms. On the other hand, setting\ns = sh will cause the initial sparse representation matrix \u0398,\nobtained by the OMP algorithm, to contain too many nonzero coefficients. This is experienced as noise by the SAC\nalgorithm, and may prevent it from functioning properly. It is\ntherefore favorable to use OMP with k \u00d7 sl non-zero entries\nonly, and setting the maximal block size s to be sh .\nIn Fig. 6(a), we evaluate the ability of our block sparsifying\ndictionary design algorithm to recover the optimal dictionary,\nwhich contains 12 blocks of size 3, and 12 blocks of size 2. As\nexpected, better results are obtained when choosing sl = 2. In\nFig. 6(b), the underlying block subspaces are all of dimension\n2, but sh is erroneously set to be 3. We see that when sl = 2,\nwe succeed in recovering a considerable part of the blocks,\neven though blocks of size 3 are allowed. In both simulations,\nK-SVD uses k \u00d7 sh non-zero entries, which explains why it\nis not significantly outperformed by our algorithm in terms of\nrepresentation error. Moreover, the percentage of reconstructed\nblocks by our algorithm is relatively low compared to the\nprevious simulations, due to the small block sizes.\nV. C ONCLUSIONS\nIn this paper, we proposed a framework for the design of\na block-sparsifying dictionary given a set of signals and a\nmaximal block size. The algorithm consists of two steps: a\nblock structure update step (SAC) and a dictionary update step\n(BK-SVD). When the maximal block size is chosen to be 1,\nthe algorithm reduces to K-SVD.\nWe have shown via experiments that the block structure\nupdate step (SAC) provides a significant contribution to the\ndictionary recovery results. We have further shown that for\ns > 1 the BK-SVD dictionary update step is superior to the\nK-SVD dictionary update. Moreover, the representation error\nobtained by our dictionary design method lies very close to the\nlower bound (the oracle) for all noise levels. This suggests that\nour algorithm has reached its goal in providing dictionaries\nthat lead to accurate sparse representations for a given set of\nsignals.\nTo further improve the proposed approach one could try and\nmake the dictionary design algorithm less susceptible to local\nminimum traps. Another refinement could be replacing blocks\nin the dictionary that contribute little to the sparse representations (i.e. \"unpopular blocks\") with the least represented signal\nelements. This is expected to only improve reconstruction\nresults. Finally, we may replace the time-efficient BOMP\nalgorithm, with other block-sparse approximation methods. We\nleave these issues for future research.\nVI. ACKNOWLEDGEMENTS\nThe research of Lihi Zelnik-Manor is supported by Marie\nCurie IRG-208529.\n\n\f8\n\n0.5\nK\u2212SVD\nBK\u2212SVD+SAC (sl=2, sh=3)\n\n0.4\n\ne\n\nBK\u2212SVD+SAC (sl=3, sh=3)\n0.3\n0.2\n0.1\n\n5\n\n10\n\n15\n\n20\n\n25\n\n20\n\n25\n\nIterations\n40\n\np\n\n30\n20\n10\n0\n\n5\n\n10\n\n15\n\nIterations\n\n(a)\n\n0.5\nK\u2212SVD\nBK\u2212SVD+SAC (sl=2, sh=3)\n\ne\n\n0.4\n\nBK\u2212SVD+SAC (sl=3, sh=3)\n\n0.3\n\n0.2\n\n0.1\n\n5\n\n10\n\n15\n\n20\n\n25\n\n20\n\n25\n\nIterations\n25\n20\n\np\n\n15\n10\n5\n0\n\n5\n\n10\n\n15\n\nIterations\n\n(b)\nFig. 6. Simulation results of our overall algorithm (BK-SVD+SAC) and\nthe K-SVD algorithm, with maximal block size sh = 3. The graphs show\nthe reconstruction error e and the recovery percentage p as a function of the\nnumber of iterations. (a) contains 12 blocks of size 2 and 12 block of size\n3. (b) contains 30 blocks of size 2.\n\nR EFERENCES\n[1] E. Candes, J. Romberg, and T. Tao, \"Robust uncertainty principles: Exact\nsignal reconstruction from highly incomplete frequency information,\"\nIEEE Trans. Inform. Theory, vol. 52, pp. 489\u2013509, Feb. 2006.\n[2] D. Donoho, \"Compressed sensing,\" IEEE Trans. Inform. Theory, vol.\n52, no. 4, pp. 1289\u20131306, Apr. 2006.\n[3] S. S. Chen, D. L. Donoho, and M. A. Saunders, \"Atomic decomposition\nby basis pursuit,\" SIAM J. Sci. Comput., vol. 20, no. 1, pp. 33\u201361, 1999.\n[4] J. Tropp, \"Greed is good: Algorithmic results for sparse approximation,\"\nIEEE Trans. Inform. Theory, vol. 50, no. 10, pp. 2231\u20132242, Oct. 2004.\n[5] S. G. Mallat and Z. Zhang, \"Matching pursuits and time-frequency\ndictionaries,\" IEEE Trans. Sig. Proc., vol. 41, no. 12, pp. 33973415,\nDec. 1993.\n[6] M. Aharon, M. Elad, and A. M. Bruckstein, \"The K-SVD: An algorithm\nfor designing of overcomplete dictionaries for sparse representations,\"\nIEEE Trans. SP, vol. 54, no. 11, 2006.\n[7] K. Engan, S. O. Aase, and J. H. Hakon-Husoy, \"Method of optimal\ndirections for frame design,\" IEEE Int. Conf. Acoust., Speech, Signal\nProcess, vol. 5, pp. 2443\u20132446, 1999.\n\n[8] B.A. Olshausen and D.J. Field, \"Natural image statistics and efficient\ncoding,\" Network: Comput. Neural Syst., vol. 2, no. 7, pp. 333\u2013339,\n1996.\n[9] S. Lesage, R. Gribonval, F. Bimbot, and L. Benaroya, \"Learning unions\nof orthonormal bases with thresholded singular value decomposition,\"\nIEEE Conf. on Acoustics, Speech and Signal Processing, 2005.\n[10] J. M. Duarte-Carvajalino and G. Sapiro, \"Learning to sense sparse\nsignals: Simultaneous sensing matrix and sparsifying dictionary optimization,\" IMA Preprint Series, May 2008.\n[11] Y. C. Eldar and M. Mishali, \"Robust recovery of signals from a\nstructured union of subspaces,\" IEEE Trans. Inform. Theory, vol. 55,\nno. 11, pp. 5302\u20135316, Nov 2009.\n[12] K. Gedalyahu and Y. C. Eldar, \"Time delay estimation from low rate\nsamples: A union of subspaces approach,\" arXiv.org 0905.2429; to\nappear in IEEE Trans. Signal Process., 2010.\n[13] R. Basri and D. Jacobs, \"Lambertian refelectances and linear subspaces,\"\nIEEE Transactions On Pattern Analysis And Machine Intelligence, vol.\n25, no. 2, pp. 383\u2013390, Feb 2003.\n[14] A. Y. Yang, J. Wright, Y. Ma, and S. Sastry, \"Feature selection in face\nrecognition: A sparse representation perspective,\" UC Berkeley Tech\nReport, Aug 2007.\n[15] R. Vidal and Y. Ma, \"A unified algebraic approach to 2-D and 3-D\nmotion segmentation and estimation,\" Journal of Mathematical Imaging\nand Vision, vol. 25, no. 3, pp. 403\u2013421, Oct. 2006.\n[16] M. Mishali and Y. C. Eldar, \"Blind multiband signal reconstruction:\nCompressed sensing for analog signals,\" IEEE Trans. Sig. Proc., vol.\n57, no. 3, pp. 993\u20131009, Mar. 2009.\n[17] M. Mishali and Y. C. Eldar, \"From theory to practice: Sub-nyquist\nsampling of sparse wideband analog signals,\" IEEE Journal of Selected\nTopics in Signal Processing, vol. 4, no. 2, pp. 375 \u2013 391, Apr. 2010.\n[18] H. J. Landau, \"Necessary density conditions for sampling and interpolation of certain entire functions,\" Acta Math., vol. 117, no. 1, pp. 37\u201352,\n1967.\n[19] F. Parvaresh, H. Vikalo, S. Misra, and B. Hassibi, \"Recovering\nsparse signals using sparse measurement matrices in compressed dna\nmicroarrays,\" IEEE Journal of Selected Topics in Signal Processing,\nJun. 2008.\n[20] M. Stojnic, F. Parvaresh, and B. Hassibi, \"On the reconstruction of\nblock-sparse signals with an optimal number of measurements,\" IEEE\nTrans. Sig. Proc., vol. 57, no. 8, pp. 3075\u20133085, 2009 Aug.\n[21] H. Rauhut and Y.C. Eldar, \"Average case analysis of multichannel sparse\nrecovery using convex relaxation,\" preprint, 2009.\n[22] Y. C. Eldar, P. Kuppinger, and H. B\u00f6lcskei, \"Block-sparse signals:\nUncertainty relations and efficient recovery,\" IEEE Trans. Sig. Proc.,\nApr. 2010.\n[23] Y. C. Eldar and H. B\u00f6lcskei, \"Block-sparsity: Coherence and efficient\nrecovery,\" IEEE International Conference on Acoustics, Speech, and\nSignal Processing, vol. 0, pp. 2885\u20132888, 2009.\n[24] R. Vidal, Y. Ma, and S. Sastry, \"Generalized principal component\nanalysis (GPCA),\" IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 27, no. 11, 2005.\n[25] R. V. E. Elhamifar, \"Sparse subspace clustering,\" IEEE Conference on\nComputer Vision and Pattern Recognition, 2009.\n[26] Y. Ma, H. Derksen, W. Hong, and J. Wright, \"Segmentation of\nmultivariate mixed data via lossy coding and compression,\" IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 29, no.\n9, pp. 15461562, Sep 2007.\n[27] M. Aharon, M. Elad, and A. M. Bruckstein, \"On the uniqueness of\novercomplete dictionaries and a practical way to retrieve them,\" Journal\nof Linear Algebra and Applications, vol. 416, pp. 4867, July 2006.\n[28] H. Duda and P. Hart, \"Stork, Pattern Classification,\" 2001.\n[29] S. C. Johnson, \"Hierarchical clustering schemes,\" Psychometrika, vol.\n32, pp. 241\u2013254, Sep 1967.\n\n\f"}