{"id": "http://arxiv.org/abs/cs/0511105v1", "guidislink": true, "updated": "2005-11-30T14:15:17Z", "updated_parsed": [2005, 11, 30, 14, 15, 17, 2, 334, 0], "published": "2005-11-30T14:15:17Z", "published_parsed": [2005, 11, 30, 14, 15, 17, 2, 334, 0], "title": "The Signed Distance Function: A New Tool for Binary Classification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0511003%2Ccs%2F0511065%2Ccs%2F0511030%2Ccs%2F0511045%2Ccs%2F0511007%2Ccs%2F0511086%2Ccs%2F0511100%2Ccs%2F0511051%2Ccs%2F0511040%2Ccs%2F0511066%2Ccs%2F0511058%2Ccs%2F0511048%2Ccs%2F0511090%2Ccs%2F0511064%2Ccs%2F0511102%2Ccs%2F0511059%2Ccs%2F0511052%2Ccs%2F0511105%2Ccs%2F0511063%2Ccs%2F0511020%2Ccs%2F0511053%2Ccs%2F0511011%2Ccs%2F0511035%2Ccs%2F0511026%2Ccs%2F0511093%2Ccs%2F0511037%2Ccs%2F0511019%2Ccs%2F0511041%2Ccs%2F0511103%2Ccs%2F0511082%2Ccs%2F0511106%2Ccs%2F0511076%2Ccs%2F0511027%2Ccs%2F0511002%2Ccs%2F0511074%2Ccs%2F0511085%2Ccs%2F0511099%2Ccs%2F0511008%2Ccs%2F0511056%2Ccs%2F0511024%2Ccs%2F0511060%2Ccs%2F0511039%2Ccs%2F0511022%2Ccs%2F0511080%2Ccs%2F0511013%2Ccs%2F0511067%2Ccs%2F0511062%2Ccs%2F0511073%2Ccs%2F0511025%2Ccs%2F0511012%2Ccs%2F0511014%2Ccs%2F0511091%2Ccs%2F0511089%2Ccs%2F0511078%2Ccs%2F0511042%2Ccs%2F0511055%2Ccs%2F0511097%2Ccs%2F0511032%2Ccs%2F0511077%2Ccs%2F0511088%2Ccs%2F0511016%2Ccs%2F0608088%2Ccs%2F0608004%2Ccs%2F0608110%2Ccs%2F0608049%2Ccs%2F0608118%2Ccs%2F0608065%2Ccs%2F0608075%2Ccs%2F0608102%2Ccs%2F0608035%2Ccs%2F0608059%2Ccs%2F0608076%2Ccs%2F0608067%2Ccs%2F0608078%2Ccs%2F0608124%2Ccs%2F0608012%2Ccs%2F0608097%2Ccs%2F0608036%2Ccs%2F0608071%2Ccs%2F0608043%2Ccs%2F0608109%2Ccs%2F0608058%2Ccs%2F0608034%2Ccs%2F0608042%2Ccs%2F0608090%2Ccs%2F0608113%2Ccs%2F0608121%2Ccs%2F0608083%2Ccs%2F0608047%2Ccs%2F0608079%2Ccs%2F0608087%2Ccs%2F0608011%2Ccs%2F0608041%2Ccs%2F0608040%2Ccs%2F0608100%2Ccs%2F0608031%2Ccs%2F0608091%2Ccs%2F0608103%2Ccs%2F0608032%2Ccs%2F0608115%2Ccs%2F0608114&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Signed Distance Function: A New Tool for Binary Classification"}, "summary": "From a geometric perspective most nonlinear binary classification algorithms,\nincluding state of the art versions of Support Vector Machine (SVM) and Radial\nBasis Function Network (RBFN) classifiers, and are based on the idea of\nreconstructing indicator functions. We propose instead to use reconstruction of\nthe signed distance function (SDF) as a basis for binary classification. We\ndiscuss properties of the signed distance function that can be exploited in\nclassification algorithms. We develop simple versions of such classifiers and\ntest them on several linear and nonlinear problems. On linear tests accuracy of\nthe new algorithm exceeds that of standard SVM methods, with an average of 50%\nfewer misclassifications. Performance of the new methods also matches or\nexceeds that of standard methods on several nonlinear problems including\nclassification of benchmark diagnostic micro-array data sets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0511003%2Ccs%2F0511065%2Ccs%2F0511030%2Ccs%2F0511045%2Ccs%2F0511007%2Ccs%2F0511086%2Ccs%2F0511100%2Ccs%2F0511051%2Ccs%2F0511040%2Ccs%2F0511066%2Ccs%2F0511058%2Ccs%2F0511048%2Ccs%2F0511090%2Ccs%2F0511064%2Ccs%2F0511102%2Ccs%2F0511059%2Ccs%2F0511052%2Ccs%2F0511105%2Ccs%2F0511063%2Ccs%2F0511020%2Ccs%2F0511053%2Ccs%2F0511011%2Ccs%2F0511035%2Ccs%2F0511026%2Ccs%2F0511093%2Ccs%2F0511037%2Ccs%2F0511019%2Ccs%2F0511041%2Ccs%2F0511103%2Ccs%2F0511082%2Ccs%2F0511106%2Ccs%2F0511076%2Ccs%2F0511027%2Ccs%2F0511002%2Ccs%2F0511074%2Ccs%2F0511085%2Ccs%2F0511099%2Ccs%2F0511008%2Ccs%2F0511056%2Ccs%2F0511024%2Ccs%2F0511060%2Ccs%2F0511039%2Ccs%2F0511022%2Ccs%2F0511080%2Ccs%2F0511013%2Ccs%2F0511067%2Ccs%2F0511062%2Ccs%2F0511073%2Ccs%2F0511025%2Ccs%2F0511012%2Ccs%2F0511014%2Ccs%2F0511091%2Ccs%2F0511089%2Ccs%2F0511078%2Ccs%2F0511042%2Ccs%2F0511055%2Ccs%2F0511097%2Ccs%2F0511032%2Ccs%2F0511077%2Ccs%2F0511088%2Ccs%2F0511016%2Ccs%2F0608088%2Ccs%2F0608004%2Ccs%2F0608110%2Ccs%2F0608049%2Ccs%2F0608118%2Ccs%2F0608065%2Ccs%2F0608075%2Ccs%2F0608102%2Ccs%2F0608035%2Ccs%2F0608059%2Ccs%2F0608076%2Ccs%2F0608067%2Ccs%2F0608078%2Ccs%2F0608124%2Ccs%2F0608012%2Ccs%2F0608097%2Ccs%2F0608036%2Ccs%2F0608071%2Ccs%2F0608043%2Ccs%2F0608109%2Ccs%2F0608058%2Ccs%2F0608034%2Ccs%2F0608042%2Ccs%2F0608090%2Ccs%2F0608113%2Ccs%2F0608121%2Ccs%2F0608083%2Ccs%2F0608047%2Ccs%2F0608079%2Ccs%2F0608087%2Ccs%2F0608011%2Ccs%2F0608041%2Ccs%2F0608040%2Ccs%2F0608100%2Ccs%2F0608031%2Ccs%2F0608091%2Ccs%2F0608103%2Ccs%2F0608032%2Ccs%2F0608115%2Ccs%2F0608114&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "From a geometric perspective most nonlinear binary classification algorithms,\nincluding state of the art versions of Support Vector Machine (SVM) and Radial\nBasis Function Network (RBFN) classifiers, and are based on the idea of\nreconstructing indicator functions. We propose instead to use reconstruction of\nthe signed distance function (SDF) as a basis for binary classification. We\ndiscuss properties of the signed distance function that can be exploited in\nclassification algorithms. We develop simple versions of such classifiers and\ntest them on several linear and nonlinear problems. On linear tests accuracy of\nthe new algorithm exceeds that of standard SVM methods, with an average of 50%\nfewer misclassifications. Performance of the new methods also matches or\nexceeds that of standard methods on several nonlinear problems including\nclassification of benchmark diagnostic micro-array data sets."}, "authors": ["Erik M. Boczko", "Todd R. Young"], "author_detail": {"name": "Todd R. Young"}, "author": "Todd R. Young", "arxiv_comment": "13 pages, 5 figures", "links": [{"href": "http://arxiv.org/abs/cs/0511105v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0511105v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0511105v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0511105v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:cs/0511105v1 [cs.LG] 30 Nov 2005\n\nThe Signed Distance Function:\nA New Tool for Binary Classification\nErik M. Boczko\nDepartment of Biomedical Informatics\nVanderbilt University\nNashville, TN 37232\n615-936-6668\nerik.m.boczko@vanderbilt.edu\n\nTodd Young\nDepartment of Mathematics\nOhio University\nAthens, OH 45701\n740-593-1285\nyoung@math.ohiou.edu\n\nAugust 31, 2018\n\nAbstract\nFrom a geometric perspective most nonlinear binary classification algorithms, including state\nof the art versions of Support Vector Machine (SVM) and Radial Basis Function Network\n(RBFN) classifiers, and are based on the idea of reconstructing indicator functions. We propose instead to use reconstruction of the signed distance function (SDF) as a basis for binary\nclassification. We discuss properties of the signed distance function that can be exploited in\nclassification algorithms. We develop simple versions of such classifiers and test them on several\nlinear and nonlinear problems. On linear tests accuracy of the new algorithm exceeds that of\nstandard SVM methods, with an average of 50% fewer misclassifications. Performance of the\nnew methods also matches or exceeds that of standard methods on several nonlinear problems\nincluding classification of benchmark diagnostic micro-array data sets.\n\n-----------------\nMachine Learning, Microarray Data\n\n-----------------\n\n1\n\nIntroduction\n\nBinary classification is a basic problem in machine learning with applications in many fields. Not\nonly does binary classification have many potential direct applications, it is also the basis for\nmany multi-category classification methods. Of particular interest are the applications in biology\nand medicine. The availability of micro-array and proteomic data sets that contain thousands\nor even tens of thousands of measurements have particularly made it important to develop good\nclassification algorithms, since reliable use of these data could presumably revolutionize diagnostic\nmedicine. Several binary classification algorithms have been developed and studied intensely over\nthe past few years, most notable among these are the support vector machine (SVM) methods\nusing radial basis functions and other functions as kernels. SVM methods have been shown to\nperform reasonably well in classifying micro-array data, demonstrating that the extraction of useful\ninformation from these large data sets is feasible.\n1\n\n\fWe will begin in the next section with a geometric, rather than statistical, statement of the\nbinary classification problem and our discussion will be restricted to the context of this geometric\nviewpoint. Nonlinear SVM methods, despite their geometric, maximal margin origin, have been\ndeveloped based on the idea of reconstructing \"indicator functions\", as discussed by Poggio and\nSmale [14]. RBFN methods are also currently employed in this way. The indicator function is an\nobject that encodes only the most primitive geometric information. We propose that a potentially\nbetter tool for classification is the \"signed distance function\" (SDF), an intrinsically geometric\nobject that has been employed in several areas of applied mathematics. The geometric properties\nof the SDF make it advantageous for use in classification and we give examples of how these\nproperties can be exploited.\nIn this paper we also present preliminary test results for rudimentary implementations based on\nthe idea of reconstructing the SDF from training data. We demonstrate that these non-optimized\nSDF-based algorithms outperform standard SVM (LIBSVM) methods on average by 50% (half as\nmany misclassifications) on linearly separable problems. We also present a comparison of SDF\nclassification with SVM method on the challenging geometric 4 by 4 checkboard problem in which\nthe SDF-based method performs better. Finally, on 2 benchmark cancer-diagnosis micro-array\ndata sets a nonlinear SDF algorithm performs just as well or better than highly-developed SVM\nmethods. While these results are obviously not conclusive, they do demonstrate that the SDF\nparadigm is promising and worth further investigation.\n\n2\n\nA Geometric Formulation and a Geometric Tool\n\nSuppose a set X \u2282 Rn is partitioned by A \u2282 X and its complement Ac . The set A might contain\nthe set of test values that are associated with the presence of a disease, while Ac contains those\nvalues that are not. For applications we may suppose that A is a reasonably nice set, e.g. has\na smooth boundary. The problem of binary classification is then to determine the set A given a\nc\nfinite sample of data, i.e. for a set of points {xi }m\ni=1 we know whether xi \u2208 A or xi \u2208 A , for each i.\nThe purpose of solving this problem is obviously predictive power; given a point x \u2208 X that is not\namong the given data {xi }m\ni=1 , we wish to determine if x \u2208 A. In this paper we only consider this\ngeometric formulation of the problem. While this formulation is obviously somewhat restrictive,\nit allows for geometric analysis and leads to a new class of methods that may be useful in some\napplications.\nOne way to approach this problem mathematically that is common among nonlinear classifiers\nis to consider the indicator function of A, \u03b9A : X \u2192 {\u22121, 1}, defined by\n(\n1\nif x \u2208 A,\n(1)\n\u03b9A (x) =\n\u22121 if x \u2208 Ac .\nThe known information is represented by {(xi , \u03b9A (xi )}m\ni=1 and problem of binary classification is\nequivalent to reconstructing \u03b9A (x) from the data.\nCurrent methods of binary classification such as the SVM methods and RBFN methods work by\nattempting to approximate the indicator function \u03b9A by regression over the known data, {(xi , \u03b9A (xi )}.\nThis was not the conceptual origin of the SVM, i.e. finding a separating plane of maximal margin, but is the basis for nonlinear \"kernel trick\" algorithms and efficient linear implementation as\npointed out in [14]. In practice, the SVM constructed functions are smooth and \u00b11 are not the\n2\n\n\fonly values in the range, thus x is interpreted as in A if the constructed function is positive at x\nand as being in Ac if it is negative.\nRather than using the indicator function \u03b9A , we propose using the signed distance function\n(SDF) of A, denoted bA (x) which is the distance to the boundary, \u2202A, of A if x \u2208 A or minus the\ndistance to \u2202A if x \u2208 Ac , i.e.\n(\nd(x, \u2202A)\nif x \u2208 A,\n(2)\nbA (x) =\n\u2212d(x, \u2202A) if x \u2208 Ac ,\nwhere d is a metric (distance function).\nKnowledge of bA is obviously sufficient to fully determine the set A, it carries more information\nthan \u03b9A , and it has some smoothness. These and other properties can be used in classification\nand thus the SDF has advantages over the indicator function as a basis for binary classification\nalgorithms. In fact we argue that SDF based classification, because it is more geometrical, is\nconceptually a more faithful generalization of the original SVM concept than existing nonlinear\n(kernel trick) SVM implementations.\nBinary classification based on the SDF can work in much the same way as indicator function\nbased classification; one attempts to approximate the function bA using only the given data. If the\nvalue of bA is positive at a test point x, then x is predicted to be in A and if the value is negative,\nx is predicted to be in Ac . The approximation of bA can proceed similarly to that of \u03b9A , i.e. by\nvarious forms of regression (including SVM and RBFN regression). A practical difference is that\n\u03b9A is given explicitly at the data points, whereas bA at the data points must be derived from the\ndata. We investigate simple methods for doing this and show that they give reliable results. We\nalso show that properties of bA can be used to refine those estimates. The complexity of this task\nis no worse than that needed to perform the regression itself, hence no computational performance\nis sacrificed.\n\n3\n\nSigned Distance Functions\n\nIn some places bA is called the oriented boundary distance function or the oriented distance function.\nProofs of the following facts about bA can be found in [6].\nFact 1 The function bA is Lipschitz continuous, with Lipschitz constant 1.\nIn other words, |bA (x) \u2212 bA (y)| < |x \u2212 y|, holds for all x, y \u2208 X. This implies that bA (x) is\ndifferentiable almost everywhere, i.e, DbA (x) exists except on a set of zero measure. It also implies\n1,p\nthat bA (x) belongs to the Sobolev space Wloc\nfor any 1 \u2264 p \u2264 \u221e.\nFact 2 If bA is differentiable at a point x, then there exists a unique P x \u2208 \u2202A such that bA (x) =\n|x \u2212 P x| and\nPx \u2212 x\n.\n\u2207bA (x) =\nbA (x)\nIn the case it is unique, P x \u2208 \u2202A is called the projection of x onto \u2202A. In particular, |\u2207bA (x)| = 1\nand DbA (x) points from x toward P x.\n\n3\n\n\fFact 3 Let A be a subset of Rn with nonempty boundary \u2202A. Then bA is a convex function if and\nonly if A is a convex set. If A is convex, then bA is differentiable everywhere in Ac .\nFact 4 If \u2202A is of smoothness class C k , i.e. it is k times continuously differentiable, k \u2265 1, then\nfor each y \u2208 \u2202A there is a neighborhood V (y) of y on which bA is a C k function.\nIf \u2202A is C 1 , then at any point y \u2208 \u2202A we can define the unit normal vector n(y).\nFact 5 Suppose \u2202A is of smoothness class C 1 and n(y) is Lipschitz continuous. At any point\ny \u2208 \u2202A, let V (y) be as in Fact 4. Then for any x \u2208 V (y), x = P x + bA (x)n(P x).\nIn particular, P x \u2212 x and DbA (x) are normal to \u2202A at P x.\nFact 6 Let Hy denote the mean curvature of \u2202A at a point y \u2208 \u2202A. Then wherever Hy exists it\nsatisfies\nHy = \u2206bA (x),\nwhere \u2206 is the usual Laplace operator.\nThe function bA has been used in various branches of applied math such as free boundary problems\n[5, 7, 16, 18] and grid generation for finite-element methods [17]. It is intimately related to flow\nby mean curvature [6] and occurs in the solution of certain Hamilton-Jacobi partial differential\nequations [8, p. 163]. The geometric nature of the SDF connects it to well developed areas of\ngeometry and analysis that can be expected to provide tools for both refinement and analysis of\nSDF based classification methods.\n\n4\n\nSDF Classifiers\n\n4.1\n\nPreliminary algorithms\n\nIn the SDF paradigm the input training data are marked as to class, but they do not come marked\nwith the values bA (xi ), and hence these need to be approximated. A naive SDF algorithm then\nconsists of two simple steps, with an optional third refinement step.\nm\n\u2022 Approximate bA at the training data {xi }m\ni=1 . Denote these approximations by {bi }i=1\n\n\u2022 Approximate bA by a function BD (x) on the entire domain through regression on D :=\n{(xi , bi )}m\ni=1 .\n\u2022 Use the constructed function BD and properties of bA to improve the estimates {bi } and\niterate.\nWe now detail preliminary algorithms for these three steps and point to those areas that we consider\nimportant for further investigation.\n\n4\n\n\f4.2\n\nEstimating bA at the data\n\nLet d denote a metric on X \u2282 Rn . Usually we will let d be the Euclidean metric, but we will also\nconsider weighted distances. A reasonable and simple first approximation of bA at {xi }m\ni=1 is given\nby\nbi = P \u2032 (xi ) \u2261 \u03b9A (xi ) * min{d(xi , xj ) : \u03b9A (xj ) 6= \u03b9A (xi )},\nj6=i\n\ni.e. the signed projection onto the (finite) data of opposite type. It is clear that bi has the correct\nsign and is a bound on bA (xi ), i.e.\n|bA (xi )| \u2264 |bi |.\n(3)\nIt is easy to show by counter-example that obtaining more precise, yet rigorous, bounds on\nbA (xi ) would require some assumptions on the shape of A. However, we can make some heuristic\nimprovements in {bi }. For instance, consider\nb\u0303i = bi \u2212 bA (P \u2032 xi ).\nThen we have\n|bA (xi )| \u2264 |b\u0303i | \u2264 |bi |.\n\nNow suppose that xi \u2208 A and yi \u2208 Ac where yi is the closest data point to xi in Ac and xi is the\nclosest data point in A to yi . If \u2202A is situated half way between xi and yi and is normal to yi \u2212 xi ,\nthen bA (x) = d(x, y)/2. Let ci denote the approximated signed distance of yi to the boundary, then\nwe have:\n1\n1\nbA (xi ) = bi = bi \u2212 ci .\n2\n2\n\u2032\nThen for any xi and yi = P xi we define\nb\u2032i = bi \u2212 .5ci\n\n(4)\n\nwhere bi is the first approximation of bA (xi ) and ci is the first approximation of bA (yi ). It can be\ndemonstrated that, on average, b\u2032i is a better approximation of bA (xi ) than bi .\n\n4.3\n\nLinear classifier\n\nWe will say that a binary classification problem in the context of our geometric formulation is\nlinearly separable if \u2202A is a hyperplane in Rn . In this case bA (x) will be a linear function whose\nzero set is \u2202A. To use the SDF for a linearly separable problem one would seek a linear function\nas BD (x), i.e.\ny = l(x) = w * x + c = w1 x1 + w2 x2 + . . . + wn xn + c\n\nthat fits the data {xi , bi )}m\ni=1 . The most obvious choice for this approximation is to use the linear\nleast squares approximation if m > n or projection (pseudoinverse) if m < n, for which there are\nhighly developed algorithms. In the linear tests below, we have used linear least squares regression.\nFrom \u00a72, we have |\u2207bA (x)| = 1 wherever it exists so we could let\n|Dl(x)| = |w| = 1\nbe a constraint in the linear least squares approximation.\n5\n\n\f4.4\n\nNonlinear classifier\n\nIf the problem is not linearly separable, then bA (x) will be a nonlinear function. To approximate\nit we should use some form of nonlinear regression on {(xi , bi )}m\ni=1 . There are many options for\nnonlinear regression that could be used, including SVM and RBFN regression algorithms. One\nsimple, yet appealing, choice for the nonlinear regression is the least squares regression discussed\nin [14]. We implemented this approach in the nonlinear tests reported below and so we recall it.\nLet K : X \u00d7 X \u2192 R be a kernel that is symmetric (K(x, y) = K(y, x)) and positive-definite,\ni.e.,\nk\nX\nci cj K(xi , xj ) \u2265 0,\ni,j=1\n\nfor any k, any x1 , . . . , xk and any c1 , . . . , ck . In the tests below we use the Gaussian\n2 /2\u03c3 2\n\nK(x, y) = e\u2212d(x,y)\n\n,\n\nwhich is symmetric and positive definite. Let K be the square, positive-definite matrix with elements Ki,j = K(xi , xj ). Let I be the identity matrix and b be the vector with coordinates bi . Let\n\u03b3 > 0 and let c be the solution of the linear system of equations:\n(K + m\u03b3I)c = b.\n\n(5)\n\nThis problem is well-posed since (K + m\u03b3I) is strictly positive-definite and the condition number\nwill be good provided that m\u03b3 is not too small. The number \u03b3 can be viewed as a smoothing\nparameter. Given the solution vector c, the approximation, BD (x), of bA (x) is given by\nBD (x) =\n\nm\nX\n\nci K(xi , x).\n\ni=1\n\nThe choices of \u03c3 and \u03b3 in this approximation are discussed in [14] and elsewhere. In tests\ndiscussed below, results were insensitive to \u03b3 within a fairly large range. Good choices for \u03c3, which\nwe found by cross-validation, were on the order of the mean inter-data distance.\nWe emphasize that other regression methods could be used in connection with SDF-based and\nshould be investigated.\n\n4.5\n\nIteration\n\nThere are some possibilities for refining the initial data approximation {bi }. One idea for nonlinear\nproblems would is to let\nb\u2032i = BD (xi )\nwhere BD is the regression obtained from {(xi , bi )}. Then one could take b\u2032i as a refinement of bi\nand then run the regression again. (For linear regression, this would repeat the original results.)\nWe note that iterations of this type are related to the matrix eigenvalue problem for which there\nare well developed numerical techniques and which are amenable to stability analysis.\nAnother possibility is to use Facts 2 and 5 above to refine bi . From these facts it is reasonable to\nproject yi \u2212 xi onto Df (xi ). This approach is particularly appealing for problems that are known\na priori to be linearly separable. We have successfully implemented this iterative approach in the\nlinear tests below, using the projection onto w = Dl to refine {bi }m\ni=1 .\n6\n\n\f5\n5.1\n\nTest Results\nLinearly separable problems\n\nWe applied both non-iterative and iterative forms of the linear regression signed distance classifier\nto three types of distributions: uniform, normal and skewed. In all of these tests the linear SDF\nclassifier decisively outperforms the linear classifiers in the LIBSVM package as well as the linear\nLagrangian SVM [13] and the Proximal SVM [9].\nA linearly separable problem can be transformed by a linear change of coordinate to the problem\nwhere \u2202A = {xn = 0}. Thus we use this problem for our tests. We performed the tests for n = 2.\nData in the half space xn > 0 we labeled as in A and data with xn < 0 we labeled as in Ac .\nIn the uniform distribution tests we let the domain be the square [\u22121, 1] \u00d7 [\u22121, 1] \u2282 R2 . For the\nnormal distribution we used the standard normal distribution at the origin in R2 . For the skewed\n\u221a\ndistribution, we randomly choose points in the square [0, 1]2 \u2282 R2 using the density \u03c1(xi ) = 1/ xi ,\nthen scaled them affinely to [\u22121, 1]2 .\nIn the tests, we considered training sets of size from m = 10 to m = 10, 000. For each m in the\nrange we classified 50 distributions of m points. In each test we used a test set consisting of 4000\npoints selected randomly according to the distribution type being tested.\nIn Figures 1 we show comparisons of the SDF linear classifiers with the linear classifier from the\nLIBSVM along with the Lagrangian SVM [13] and Proximal SVM [9]. In this plot csvm and usvm\nare routines from the LIBSVM package, psvm is the Proximal SVM and lsvm is the Lagrangian\nSVM algorithm. In these tests the iterated SDF method was iterated 5 times. The iterated SDF\nmethod gave a 10% to 15% decrease in the error over the non-iterated SDF method. The Lagrangian\nmethod was also iterated 5 times. The LIBSVM package methods automatically iterate. In our\ntrials, the number of iterations for the LIBSVM methods increased with m and varied from 10 to\n5000 iterations.\nIt can be seen that the SDF classifier has noticably smaller errors than either SVM method\nover a large range of number of training points m. Averaged over all 550 tests, the SDF-based\nclassification produced 52% fewer misclassifications than the best SVM (LIBSVM c-SVM) method.\n(Average \u2248 98% correct vs. \u2248 96% correct).\n\n5.2\n\nThe 4 \u00d7 4 Checkerboard Problem\n\nThere are several benchmark nonlinear problems, but perhaps the prototype is the 4 by 4 checkerboard. This geometric problem is interesting because it is known to be difficult. In this test a\nsquare is partitioned into 16 equal sub squares with alternate squares belong to two distinct types,\nblack or white (Figure 2). Following [13], we used 1,000 randomly selected points in each training\nset and 40,000 grid points as the test set.\nIn Figure 3 we show the results of applying an SDF-based classification scheme and the standard\nLIBSVM package to 100 independent training set. We used the nonlinear least squares regression\nwith parameters \u03c3 and \u03b3 found by cross-validation on the training sets. Parameters for the SVM\nclassification were chosen by precisely the same process.\nNote that the SDF-based method produces better results than the LIBSVM package. The mean\ncorrect % and standard deviation for the SDF method were 96.3% and .46%. For the SVM method\nthe mean correct % was 94.5% with a standard deviation of .36%.\n\n7\n\n\f0\n\n10\n\ncsvm\nusvm\npsvm\nlsvm\nsdf\n\n\u22121\n\nrelative error\n\n10\n\n\u22122\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n1\n\n10\n\n2\n\n3\n\n10\n\n4\n\n10\n\n10\n\n0\n\n10\n\ncsvm\nusvm\npsvm\nlsvm\nsdf\n\n\u22121\n\nrelative error\n\n10\n\n\u22122\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n1\n\n10\n\n2\n\n3\n\n10\n\n4\n\n10\n\n10\n\n0\n\n10\n\ncsvm\npsvm\nlsvm\nsdf\n\n\u22121\n\nrelative error\n\n10\n\n\u22122\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n1\n\n10\n\n2\n\n3\n\n10\n\n10\n\n4\n\n10\n\nFigure 1: A comparison of classifiers on linearly separable data. In (a) the data are uniformly\ndistributed, in (b) the data are normally distributed, and in (c) the data are from a skewed distribution. The x-axis is log(m), where m is the number of data (training) points. Each point in the\ngraph represents an average over 50 independent tests.\n\n8\n\n\f0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\nFigure 2: The 4 by 4 checkerboard problem classified by a nonlinear SDF-based classifier.\nThe standard deviation for our SDF method is slightly higher than that for the LIBSVM\npackage. This is perhaps an artifact of our naive implementation of a SDF-based method.\nWe note that [13] reported 97.1% accuracy on this problem using a Lagrangian SVM with\n100,000 iterations.\n\nSVM\n\nSDF\n93.5\n\n94\n\n94.5\n\n95 95.5 96\n% Correct\n\n96.5\n\n97\n\n97.5\n\nFigure 3: Comparison of performance by the LIBSVM package and a SDF-based method on the\nfour by four checkerboard problem.\n\n5.3\n\nMicro-array data sets\n\nWe compare the nonlinear SDF classifier with existing studies of SVM performance on two standard\nmicro-array data sets involving cancer diagnosis. The first set, called Prostate\u2013tumor, consists of\n102 micro-array samples with 10,510 measurements each. Each patient represented by a sample was\ndiagnosed independently for the presence of a prostate tumor. Of the 102 samples, 52 were from\npatients with a prostate tumor [20]. The second data set DLBCL consists of 77 samples with 5470\nvariables each. Samples were taken from patients diagnosed for a lymphoma. Of the 77 patients,\n58 had Diffuse large B-cell lymphomas and 19 had follicular lymphomas [19].\nWith the two data sets, we tested the SDF nonlinear classifier with least squares regression\non the data sets using Leave One Out Cross Validation (LOOCV). Accuracy of the LOOCV tests\n\n9\n\n\fare shown in Table 1, with comparison of reported LOOCV performance from [22] using a variety\nof SVM methods and the k-nearest neighbor (KNN) method. The percentages reported are the\npercentages of correctly identified samples. It is important to point out that [22] showed that their\nerror estimates did not change depending on the cross validation technique, and hence LOOCV is\na robust estimate of performance when applied to these data.\nData set\nProstate\u2013tumor\nDLBCL\n\nKNN\n85%\n87%\n\nSVM\n92.2%\n97.4%\n\nSDF\n94.1%\n97.4%\n\nTable 1: Results from applying SDF-based classification to the benchmark DLBCL and Prostate\u2013\ntumor micro-array data sets compared with the performance of SVM methods and the k-nearest\nneighbor method reported in [22]. All results are for Leave-one-out cross validation.\nAs seen in Table 1, the performance of the SDF classifier matches that of the SVM method on\nthe DLBCL data and exceeds it on the Prostate\u2013tumor data set.\nIn preliminary tests we found that performance on several LOOCV subsets once again was\nunaffected by \u03b3 over a broad range 10\u22124 \u201310\u221212 . Based on this we simply set \u03b3 = 10\u22127 for the rest\nof the tests. Values for \u03c3 were determined within each loop of the LOOCV process based on mean\ninterdata distances for the subset. For the DLBCL data values of \u03c3 were approximately 2.5 \u00d7 104\nand \u03c3 was generally about 3.4 \u00d7 103 for subsets of the Prostate\u2013tumor set. For both of these data\nsets the optimal results are actually robust with respect to changes in the values of \u03c3.\nIn these tests we used a weighted distance\nda (x, y) = |a * (x \u2212 y)|,\nwhere | * | is the usual Euclidean norm and a is a vector of weights. Distance functions of this type\nwere shown to be effective in high dimensional binary classification problems in [2]. Specifically, we\ntook a to be the absolute values of the correlation coefficients relating each variable to the indicator\non the data set. This was recalculated in the LOOCV process for each subset, independent of the\nexcluded sample.\nWe note that in an independent set of experiments reported in [3], the nonlinear SDF-based\nclassifier was compared to KNN, RBFN and SVM classifiers on five other cancer data sets. The\nfollowing microarray data sets are involved: The Breast Cancer data set [24] consists of 49 tumor\nsamples with 7129 human genes each. There are two different response variables in the data set:\none describes the status of the estrogen receptor (ER), and the other one describes the status of\nthe lymph nodal (LN), which is an indicator of the metastatic spread of the tumor. Of the 49\nsamples, 25 are ER+ and 24 are ER-, 25 are LN+ and 24 are LN-. The Colon Cancer data set [1]\nconsists of 40 tumor and 22 normal colon tissues with 2000 genes each. The Leukaemia data set [10]\nconsists of 72 samples with 7129 genes each. Each patient represented by a sample has either acute\nlymphoblastic leukemia (ALL) or acute myeloid leukemia (AML). Of the 72 samples, 47 are ALL\nand 25 are AML.\nWe tested the four classifiers in 100 independent trials on each of the data sets. In each trial,\nthe data were divided randomly into a training set and a test set in a ratio of 2:1. We used Gaussian\nkernel functions for RBFN, SVM and SDF classifiers. For simplicity, we did not use any heuristic\nfor the distance metrics of KNN, using the Euclidean distance. We claim that the classifiers are\n10\n\n\fcomparable in this setting since they are under exactly the same condition: (i) They share the same\ntraining set and test set in each trial, (ii) SVM and SDF share the same \u03b3 = 10\u22127 , (iii) SVM uses\nthe weighted kernel matrix returned by SDF in each trial, (iv) SVM and RBFN use the same \u03c3,\nwhich is computed in each trial as the root mean square distance (RMSD) of the training data.\nData Set\nBreast cancer, ER\nBreast cancer, LN\nColon cancer\nLeukaemia\n\nKNN\n.0912\n.2400\n.2200\n.0146\n\nRBFN\n.0912\n.2425\n.2143\n.0321\n\nSVM\n.0869\n.2106\n.1700\n.0167\n\nSDF\n.0869\n.2100\n.1662\n.0167\n\nTable 2: Comparison of misclassification ratios averaged over 100 trials on randomly divided data.\nTable 2 shows the test error rates averaged over the 100 independent trials for each classifier.\nKNN with k = 1, 9, 3, 5 neighbors achieves the best (in the averaging sense) generalization performance for the breast cancer data (ER), breast cancer data (LN), colon cancer data, and the\nleukemia data, respectively. Note that in actual use, k would have to be determined in some unbiased way from the training data only. We note again that the naive SDF method matches or beats\nthe SVM method on all data.\n\n6\n\nDiscussion\n\nIn order to make the SDF paradigm competitive with indicator function based classification, the\nmain need seems to be for more accurate, yet efficient, ways of obtaining an approximation of\nbA (xi ). In the scheme we used in these tests, we simply search the entire data set for the closest\npoint of the opposite type. In the worst case this takes m2 operations, which is easily within the\nrealm of practical computations. Increasing the accuracy of the approximation is a more difficult\nissue and should involve deeper geometric information from the data set.\nIn addition to better determination of {bi }, use of other methods of nonlinear regression, including SVM and RBFN regression, with SDF-based classification should be explored. Another\narea for future exploration is development of iterative methods for the nonlinear classifier. We have\ndescribed two possible procedures for this iteration and implemented one in a linear setting.\nSmale and coworkers have been developing methods for rigorous estimates for the least squares\nregression algorithm outlined in \u00a72.5. They produce these estimates in the framework of Reproducing Kernel Hilbert Spaces which have been shown to be isomorphic to certain Sobolev spaces [21],\n1 , to which signed distance function are known to belong [6]. The estimates\nincluding the space Hloc\ncould be used in our context if the accuracy of the initial estimates {bi }m\ni=1 are known. For the\nnaive method of determining these values, an upper bound is given by (3) and we hope to obtain\nbetter bounds under assumptions on A. Other geometric methods for approximating {bi }m\ni=1 should\nlend themselves to rigorous analysis depending on the methods. Perhaps for the case of iterative\nmethods, the iteration process could be linked to known results in PDE and related functional\nanalysis. Such a link would make an extremely rich arena of knowledge available for the purposes\nof estimates.\nThe above estimates of the approximated SDF should not only result in a overall reliability\nmeasure of the method, but should provide for any given test point an estimate of the distance\n11\n\n\fof that test point from the decision surface. Combining this with statistical knowledge of the\nunderlying application could provide a very natural \"level of confidence\" measure for any given\ntest data. Such estimates would be especially useful in the context of biomedical applications.\nThere are concrete mathematical reasons why the SDF is a better basis than the indicator\nfunction for use in classification. The SDF is fundamentally geometric and this connects it solidly to\ngeometric and analytical tools and methods. In preliminary tests, we have shown that a naive, nonoptimized implementation of SDF-based classification is non-trivially more accurate than standard\nmethods on geometric problems. In preliminary tests on nonlinear, high dimensional and noisy\ndata, we have demonstrated that a non-optimized implementation of SDF is at least as accurate as\ncurrent, standard SVM methods. These observations and results indicate that the SDF paradigm\nhas the potential to be the basis for more accurate binary classification algorithms in many contexts.\n\nReferences\n[1] U. Alon, N. Barkai, D. Notterman, K. Gish, S. Ybarra, D. Mack and A. Levine, Broad patterns\nof gene expression revealed by clustering analysis of tumor and normal colon tissues probed\nby oligonucleotide arrays, Proc. Natl. Acad. Sci 96 (1999), 6745-6750.\n[2] E. Boczko, T. Young, A. DiLullo, Binary classification based on potentials, Proceedings of The\n2005 International Conference on Mathematics and Engineering Techniques in Medicine and\nBiological Sciences (METMBS'05: Las Vegas), 2005.\n[3] E. Boczko, T. Young, D. Wu and M.H. Xie, A comparison of signed distance function methods\nwith SVM method on microarray data, preprint 2005.\n[4] J.P. Brody, B.A. Williams, B.J. Wold and S.R. Quake, Significance and statistical errors in\nthe analysis of DNA microarray data, Proc. Nat. Acad. Sci. USA, 99 (2002), 12975-12978.\n[5] J. Cagnol and J.-P. Zol\u00e9sio, Intrinsic geometric model for the vibration of a constrained shell,\nin Differential geometric methods in the control of partial differential equations (Boulder, CO,\n1999), 23\u201339, Contemp. Math., 268, Amer. Math. Soc., Providence, RI, 2000.\n[6] M.C. Delfour and J.-P. Zol\u00e9sio, Shape Analysis via Oriented Distance Functions, J. Functional\nAnalysis 123 (1994), 129-201.\n[7] L.C. Evans and J. Spruck, Motion of level sets by mean curvature II, Trans. Amer. Math. Soc.\n330 (1992), no. 1, 321\u2013332.\n[8] L.C. Evans, Partial Differential Equations, GSM 19, Amer. Math. Soc., Providence, R.I., 1998.\n[9] G. Fung and O.L. Mangasarian, Proximal Support Vector Machine Classifiers, Preprint, 2001.\n[10] T. Golub, D. Slonim, P. Tomayo, C. Huard, M. Gaasenbeck, J. Mesirov, H. Coller, M. Loh,\nJ. Downing, M. Caligiuri, C. Bloomfield, and E. Lander, Molecular classification of cancer:\nClass discovery and class prediction by gene expression monitoring, Science 286 (1999), 531537\n[11] A. Krzy\u017cak, Nonlinear function learning using optimal radial basis function networks, Nonlinear Anal., 47 (2000), 293-302.\n12\n\n\f[12] L.P. Li, C. Weinberg, T. Darden, L. Pedersen, Gene selection for sample classification based\none gene expression data: study of sensitivity to choice of parameters of the GA/KNN method,\nBioinformatics, 17 (2001), 1131-1142.\n[13] O.L. Mangasarian and D.R. Musicant, Lagrangian Support Vector Machines, J. Mach. Learn.\nRes., 1 (2001), no. 3, 161\u2013177.\n[14] T. Poggio and S. Smale, The mathematics of learning: dealing with data, Notices Amer. Math.\nSoc., 50 (2003), no. 5, 537\u2013544.\n[15] J.H. Moore, J.S. Parker, N.J. Olsen, Symbolic discriminant analysis of microarray data in\nautoimmune disease, Genet. Epidemiol., 23 (2002), 57-69.\n[16] S. Osher and R. Fedkiw, Level set methods and dynamic implicit surfaces, Applied Mathematical Sciences, 153, Springer-Verlag, New York, 2003.\n[17] P.O. Persson and G. Strang, A simple mesh generator in Matlab, SIAM Rev., 46 (2004), no.\n2, 329\u2013345.\n[18] J.A. Sethian, Level set methods. Evolving interfaces in geometry, fluid mechanics, computer\nvision, and materials science. Cambridge Monographs on Applied and Computational Mathematics, 3, Cambridge University Press, Cambridge, 1996.\n[19] M.A. Shipp, K.N. Ross, P. Tamayo, A.P. Weng, J.L. Kutok, R.C. Aguiar, M. Gaasenbeek,\nM. Angelo, M. Reich, G.S. Pinkus, T.S. Ray TS, M.A. Koval, K.W. Last, A. Norton, T.A. Lister, J. Mesirov, D.S. Neuberg, E.S. Lander, J.C. Aster, T.R. Golub, Diffuse large B-cell lymphoma outcome prediction by gene-expression profiling and supervised machine learning, Nat.\nMed. 8 (2002), 68-74.\n[20] D. Singh, P.G. Febbo, K. Ross, D.G. Jackson, J. Manola, C. Ladd, P. Tamayo, A.A. Renshaw,\nA.V. D'Amico, J.P. Richie, E.S. Lander, M. Loda, P.W. Kantoff, T.R. Golub, W.R. Sellers,\nGene expression correlates of clinical prostate cancer behavior, Cancer Cell 1 (2002), 203-9.\n[21] S. Smale and D.X. Zhou, Learning theory estimates via integral operators and their approximations, preprint.\n[22] A. Statnikov, C.F. Aliferis, I. Tsamardinos, D. Hardin, S. Levy, A Comprehensive Evaluation\nof Multicategory Classification Methods for Microarray Gene Expression Cancer Diagnosis,\nBioinformatics, to appear.\n[23] A. Statnikov, C.F. Aliferis, I. Tsamardinos. Methods for Multi-Category Cancer Diagnosis\nfrom Gene Expression Data: A Comprehensive Evaluation to Inform Decision Support System\nDevelopment, in Proceedings of the 11th World Congress on Medical Informatics (MEDINFO),\nSeptember 7-11, (2004), San Francisco, California, USA\n[24] M. West, C. Blanchette, H. Dressman, E. Huang, S. Ishida, R. Spang, H. Zuzang, J. A. Olson\nJr, J. R. Marks, and J. R. Nevins, Predicting the Clinical Status of Human Breast Cancer by\nUsing Gene Expression Profiles. (2001) Proc. Natl. Acad. Sci 98:11462-11467.\n\n13\n\n\f"}