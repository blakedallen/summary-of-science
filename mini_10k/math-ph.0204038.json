{"id": "http://arxiv.org/abs/math-ph/0204038v1", "guidislink": true, "updated": "2002-04-17T15:36:28Z", "updated_parsed": [2002, 4, 17, 15, 36, 28, 2, 107, 0], "published": "2002-04-17T15:36:28Z", "published_parsed": [2002, 4, 17, 15, 36, 28, 2, 107, 0], "title": "Conditional Expectations and Renormalization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math-ph%2F0007010%2Cmath-ph%2F0007035%2Cmath-ph%2F0007039%2Cmath-ph%2F0007013%2Cmath-ph%2F0007033%2Cmath-ph%2F0007031%2Cmath-ph%2F0007027%2Cmath-ph%2F0007001%2Cmath-ph%2F0007016%2Cmath-ph%2F0007004%2Cmath-ph%2F0007017%2Cmath-ph%2F0007023%2Cmath-ph%2F0007025%2Cmath-ph%2F0007009%2Cmath-ph%2F0007007%2Cmath-ph%2F0007034%2Cmath-ph%2F0007020%2Cmath-ph%2F0007028%2Cmath-ph%2F0007030%2Cmath-ph%2F0007038%2Cmath-ph%2F0007026%2Cmath-ph%2F0007024%2Cmath-ph%2F0007011%2Cmath-ph%2F0007021%2Cmath-ph%2F0007029%2Cmath-ph%2F0007006%2Cmath-ph%2F0007005%2Cmath-ph%2F0007032%2Cmath-ph%2F0007022%2Cmath-ph%2F0007042%2Cmath-ph%2F0007014%2Cmath-ph%2F0007015%2Cmath-ph%2F0007003%2Cmath-ph%2F0007002%2Cmath-ph%2F0007040%2Cmath-ph%2F0007008%2Cmath-ph%2F0007036%2Cmath-ph%2F0007018%2Cmath-ph%2F0007019%2Cmath-ph%2F0007041%2Cmath-ph%2F0007037%2Cmath-ph%2F0007012%2Cmath-ph%2F0204032%2Cmath-ph%2F0204004%2Cmath-ph%2F0204055%2Cmath-ph%2F0204056%2Cmath-ph%2F0204039%2Cmath-ph%2F0204007%2Cmath-ph%2F0204048%2Cmath-ph%2F0204045%2Cmath-ph%2F0204003%2Cmath-ph%2F0204006%2Cmath-ph%2F0204037%2Cmath-ph%2F0204052%2Cmath-ph%2F0204034%2Cmath-ph%2F0204025%2Cmath-ph%2F0204024%2Cmath-ph%2F0204043%2Cmath-ph%2F0204040%2Cmath-ph%2F0204049%2Cmath-ph%2F0204038%2Cmath-ph%2F0204008%2Cmath-ph%2F0204002%2Cmath-ph%2F0204054%2Cmath-ph%2F0204022%2Cmath-ph%2F0204005%2Cmath-ph%2F0204047%2Cmath-ph%2F0204027%2Cmath-ph%2F0204021%2Cmath-ph%2F0204029%2Cmath-ph%2F0204016%2Cmath-ph%2F0204041%2Cmath-ph%2F0204046%2Cmath-ph%2F0204053%2Cmath-ph%2F0204015%2Cmath-ph%2F0204031%2Cmath-ph%2F0204030%2Cmath-ph%2F0204044%2Cmath-ph%2F0204026%2Cmath-ph%2F0204023%2Cmath-ph%2F0204035%2Cmath-ph%2F0204050%2Cmath-ph%2F0204051%2Cmath-ph%2F0204009%2Cmath-ph%2F0204018%2Cmath-ph%2F0204010%2Cmath-ph%2F0204036%2Cmath-ph%2F0204013%2Cmath-ph%2F0204028%2Cmath-ph%2F0204019%2Cmath-ph%2F0204011%2Cmath-ph%2F0204014%2Cmath-ph%2F0204012%2Cmath-ph%2F0204033%2Cmath-ph%2F0204020%2Cmath-ph%2F0204042%2Cmath-ph%2F0204001%2Cmath-ph%2F0605072%2Cmath-ph%2F0605013%2Cmath-ph%2F0605022%2Cmath-ph%2F0605080&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Conditional Expectations and Renormalization"}, "summary": "In optimal prediction methods one estimates the future behavior of\nunderresolved systems by solving reduced systems of equations for expectations\nconditioned by partial data; renormalization group methods reduce the number of\nvariables in complex systems through integration of unwanted scales. We\nestablish the relation between these methods for systems in thermal\nequilibrium, and use this relation to find renormalization parameter flows and\nthe coefficients in reduced systems by expanding conditional expectations in\nseries and evaluating the coefficients by Monte-Carlo. We illustrate the\nconstruction by finding parameter flows for simple spin systems and then using\nthe renormalized (=reduced) systems to calculate the critical temperature and\nthe magnetization.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math-ph%2F0007010%2Cmath-ph%2F0007035%2Cmath-ph%2F0007039%2Cmath-ph%2F0007013%2Cmath-ph%2F0007033%2Cmath-ph%2F0007031%2Cmath-ph%2F0007027%2Cmath-ph%2F0007001%2Cmath-ph%2F0007016%2Cmath-ph%2F0007004%2Cmath-ph%2F0007017%2Cmath-ph%2F0007023%2Cmath-ph%2F0007025%2Cmath-ph%2F0007009%2Cmath-ph%2F0007007%2Cmath-ph%2F0007034%2Cmath-ph%2F0007020%2Cmath-ph%2F0007028%2Cmath-ph%2F0007030%2Cmath-ph%2F0007038%2Cmath-ph%2F0007026%2Cmath-ph%2F0007024%2Cmath-ph%2F0007011%2Cmath-ph%2F0007021%2Cmath-ph%2F0007029%2Cmath-ph%2F0007006%2Cmath-ph%2F0007005%2Cmath-ph%2F0007032%2Cmath-ph%2F0007022%2Cmath-ph%2F0007042%2Cmath-ph%2F0007014%2Cmath-ph%2F0007015%2Cmath-ph%2F0007003%2Cmath-ph%2F0007002%2Cmath-ph%2F0007040%2Cmath-ph%2F0007008%2Cmath-ph%2F0007036%2Cmath-ph%2F0007018%2Cmath-ph%2F0007019%2Cmath-ph%2F0007041%2Cmath-ph%2F0007037%2Cmath-ph%2F0007012%2Cmath-ph%2F0204032%2Cmath-ph%2F0204004%2Cmath-ph%2F0204055%2Cmath-ph%2F0204056%2Cmath-ph%2F0204039%2Cmath-ph%2F0204007%2Cmath-ph%2F0204048%2Cmath-ph%2F0204045%2Cmath-ph%2F0204003%2Cmath-ph%2F0204006%2Cmath-ph%2F0204037%2Cmath-ph%2F0204052%2Cmath-ph%2F0204034%2Cmath-ph%2F0204025%2Cmath-ph%2F0204024%2Cmath-ph%2F0204043%2Cmath-ph%2F0204040%2Cmath-ph%2F0204049%2Cmath-ph%2F0204038%2Cmath-ph%2F0204008%2Cmath-ph%2F0204002%2Cmath-ph%2F0204054%2Cmath-ph%2F0204022%2Cmath-ph%2F0204005%2Cmath-ph%2F0204047%2Cmath-ph%2F0204027%2Cmath-ph%2F0204021%2Cmath-ph%2F0204029%2Cmath-ph%2F0204016%2Cmath-ph%2F0204041%2Cmath-ph%2F0204046%2Cmath-ph%2F0204053%2Cmath-ph%2F0204015%2Cmath-ph%2F0204031%2Cmath-ph%2F0204030%2Cmath-ph%2F0204044%2Cmath-ph%2F0204026%2Cmath-ph%2F0204023%2Cmath-ph%2F0204035%2Cmath-ph%2F0204050%2Cmath-ph%2F0204051%2Cmath-ph%2F0204009%2Cmath-ph%2F0204018%2Cmath-ph%2F0204010%2Cmath-ph%2F0204036%2Cmath-ph%2F0204013%2Cmath-ph%2F0204028%2Cmath-ph%2F0204019%2Cmath-ph%2F0204011%2Cmath-ph%2F0204014%2Cmath-ph%2F0204012%2Cmath-ph%2F0204033%2Cmath-ph%2F0204020%2Cmath-ph%2F0204042%2Cmath-ph%2F0204001%2Cmath-ph%2F0605072%2Cmath-ph%2F0605013%2Cmath-ph%2F0605022%2Cmath-ph%2F0605080&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In optimal prediction methods one estimates the future behavior of\nunderresolved systems by solving reduced systems of equations for expectations\nconditioned by partial data; renormalization group methods reduce the number of\nvariables in complex systems through integration of unwanted scales. We\nestablish the relation between these methods for systems in thermal\nequilibrium, and use this relation to find renormalization parameter flows and\nthe coefficients in reduced systems by expanding conditional expectations in\nseries and evaluating the coefficients by Monte-Carlo. We illustrate the\nconstruction by finding parameter flows for simple spin systems and then using\nthe renormalized (=reduced) systems to calculate the critical temperature and\nthe magnetization."}, "authors": ["Alexandre J. Chorin"], "author_detail": {"name": "Alexandre J. Chorin"}, "author": "Alexandre J. Chorin", "arxiv_comment": "18 pages, includes 5 figures", "links": [{"href": "http://arxiv.org/abs/math-ph/0204038v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math-ph/0204038v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.MP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "82B28, 65C05, 60K10, 65C40, 76F55", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math-ph/0204038v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math-ph/0204038v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:math-ph/0204038v1 17 Apr 2002\n\nConditional Expectations and Renormalization\nAlexandre J. Chorin\nDepartment of Mathematics\nUniversity of California\nBerkeley, CA 94720\nAbstract\nIn optimal prediction methods one estimates the future behavior of\nunderresolved systems by solving reduced systems of equations for expectations conditioned by partial data; renormalization group methods\nreduce the number of variables in complex systems through integration\nof unwanted scales. We establish the relation between these methods for\nsystems in thermal equilibrium, and use this relation to find renormalization parameter flows and the coefficients in reduced systems by expanding conditional expectations in series and evaluating the coefficients by\nMonte-Carlo. We illustrate the construction by finding parameter flows\nfor simple spin systems and then using the renormalized (=reduced) systems to calculate the critical temperature and the magnetization.\n\nKey words: Conditional expectations, optimal prediction, renormalization,\nparameter flow, critical exponents, spins, averaging.\n\n1\n\n\f1\n\nIntroduction\n\nIn the optimal prediction (OP) methods presented in earlier work by the author\nand others [7],[5],[8],[4], an estimate of the future solution of an underresolved\nproblem, or of a problem where the initial data are only partially known, was\nobtained by solving a reduced system of equations for the conditional expectation of the solution given the partial data. This system, closely related to a\ngeneralized Langevin equation of the Mori-Zwanzig type [9],[15], is derived in\ndetail in [6]. Short-time estimates can be obtained by keeping only the first term\non the right hand side of this system, obtaining a relation between the rate of\nchange of a reduced set of variables and conditional expectations of the full rate\nof change; a simplified derivation of this relation is given below. Hald's theorem [5] asserts that if one starts with a Hamiltonian system, then the reduced\nsystem obtained in this way is also Hamiltonian, with a Hamiltonian equal to a\nconditional free energy of the original Hamiltonian system.\nRenormalization group (RNG) transformations [3],[12],[13] reduce the dimensionality of a system of equations by integrating out unwanted scales. That\nthere is a qualitative resemblance between OP and RNG methods is quite clear,\nand has been pointed out in particular in the related work of Goldenfeld et\nal. [10],[11]. In the present paper we focus on the special case of Hamiltonian\nsystems in thermal equilibrium, and show that in this case the RNG transformations of the Hamiltonian can be obtained by integrating conditional expectations of the derivatives of the Hamiltonian; loosely speaking, RNG transformations are integrals of OP reductions. This remark, based on Hald's theorem,\nmakes possible the efficient evaluation of the coefficients of the new Hamiltonians\nin RNG transformations (the \"RNG parameter flow\") by simple Monte-Carlo\nmethods, for example by Swendsen's small-cell Monte-Carlo RNG [3],[14]. The\ncoefficients in the new Hamiltonian define the reduced system of equations used\nto estimate the future in OP. To illustrate the construction, we apply it to spin\nsystems and obtain explicitly the parameter flows in addition to critical points,\ncritical exponents, and order parameters. We exhibit in detail a particular implementation that is a little awkward if viewed as an instance of a RNG but is\nparticularly convenient for OP.\nA little thought shows that what is offered in the present paper is a numerical\nshort-cut. Suppose x = (x1 , x2 , . . .) is a set of n random variables (n may be\ninfinite), and let m < n; partition x so that x = (x\u0302, x\u0303), x\u0302 = (x1 , x2 , . . . , xm ), x\u0303 =\n(xm+1 , xm+2 , . . .). Let p = p(x) be the joint probability density of all the\nvariables, and consider the problem of finding a function \u0124 = \u0124(x\u0302) such that\nZ\nexp(\u2212\u0124(x\u0302)) = p(x)dx\u0303,\n(1)\n2\n\n\fwhere dx\u0303 = dxm+1 dxm+2 * * *. There is no question that \u0124 is well defined but the\nobvious ways of finding it can be costly. We are offering effective ways to do so.\nThere are other situations where one wants to integrate out unwanted variables\ninside nonlinear functions, and our short-cut may serve there as well; in subsequent papers we shall apply it to problems in irreversible statistical mechanics\nand, equivalently, to problems involving the full long-time OP equations.\n\n2\n\nConditional expectations and optimal prediction\n\nConsider a set x of random variables (x1 , x2 , . . . , xn ) with a joint probability\nR\ndensity of the form Z \u22121 e\u2212H(x) , Z = e\u2212H(x) dx, dx = dx1 dx2 . . . dxn . Consider\nthe space L2 of function u(x), v(x), . . ., with the inner product hu, vi = E[uv] =\nR\nu(x)v(x)Z \u22121 exp(\u2212H)dx, where E[*] denotes an expected value.\nPartition the variables into two groups as above, x = (x\u0302, x\u0303), x\u0302 = (x1 , . . . , xm ),\nm < n. Given a function f (x), its conditional expectation given x\u0302 is\nZ\nf (x)e\u2212H(x) dx\u0303\nE[f (x)|x\u0302] = Z\n;\n(2)\ne\u2212H(x) dx\u0303\nit is the average of f keeping x\u0302 fixed. The conditional expectation is a function\nof x\u0302 only, and it is the best approximation of f in the mean square sense by a\nfunction of x\u0302:\ni\ni\nh\nh\n2\n2\n(3)\nE (f (x) \u2212 E[f (x)|x\u0302]) \u2264 E (f (x) \u2212 h(x\u0302))\n\nfor any function h = h(x\u0302). E[f |x\u0302] is the orthogonal projection of f onto the\nsubspace L\u03022 of L that contains functions of x\u0302 only. E[f (x)|x\u0302] can be approximated by expansion in a basis of L\u03022 ; keeping only a suitable finite number l of\nbasis functions \u03c61 (x\u0302), \u03c62 (x\u0302), . . . , \u03c6l (x\u0302), and minimizing the distance between f\nand the span of the \u03c6i (x\u0302), one finds\nE[f |x\u0302] =\n\nl\nX\n\nci \u03c6i (x\u0302),\n\ni=1\n\nwhere the ci satisfy the equation\n\u03a6c = r,\n\n3\n\n(4)\n\n\fwhere \u03a6 is the matrix with elements \u03a6ij = h\u03c6i , \u03c6j i, c = (c1 , . . . , cl ), and r =\n(hf, \u03c61 i, hf, \u03c62 i . . . , hf, \u03c6l i). Usually the inner products can be calculated by\nMetropolis sampling.\nSuppose you want to find a function \u0124 = \u0124(x\u0302) such that\nZ\ne\u2212\u0124(x\u0302) = e\u2212H(x\u0302,x\u0303) dx\u0303,\n(5)\ni.e., write the marginal probability density of the variables x\u0302 in exponential\nform. Suppose one can write\nH(x) =\n\nl\nX\n\n\u03b1i \u03c6i (x),\n\ni=1\n\nand and let i \u2264 m, where m is the number of components of the vector x\u0302 =\nThen\nZ\n\u2202\nH(x)e\u2212H(x) dx\u0303\nh\ni\n\u2202x\n\u2202\ni\nZ\nE\nH(x)|x\u0302 =\n\u2202xi\n(6)\ne\u2212H(x) dx\u0303\n\u2202 \u2212 log R e\u2212H(x) dx\u0303\u0001 .\n= \u2202x\ni\nAn analogous relation between the derivative of a logarithm of a partially integrated density and a conditional expectation arises also in the context of\nexpectation-maximization is statistics [1].\nIf one can find a basis for L\u03022 consisting of functions of the form \u2202 \u03c6j (x\u0302),\n\u2202x1\nj = 1, . . . , and provided the set of variables\nso that for all\nh is homogenous\ni P\n\u2202 H|x\u0302 = l c \u2202 \u03c6 (x\u0302) are\ni \u2264 m the coefficients cj in the expansions \u2202x\nj=1 j \u2202xi j\ni\nindependent of i, then the expansion\nX\n\u0124(x\u0302) =\ncj \u03c6j (x\u0302).\n(7)\nfollows immediately. This is our key observation.\nThis construction is just Hald's theorem for OP [5]: Suppose one has a\nsystem of differential equations (written as ordinary differential equations for\nsimplicity) of the form\nd\n\u03c6(t) = R (\u03c6(t)) , \u03c6(0) = x\ndt\n\n(8)\n\nwhere \u03c6, R and x are n-vectors with components \u03c6i , Ri , xi , i = 1, . . . , n and t\nis the time. Suppose we partition as above \u03c6 = (\u03c6\u0302, \u03c6\u0303), R = (R\u0302, R\u0303), where \u03c6\u0302\ncontains the first m components of \u03c6, etc. Suppose the system (8) is Hamiltonian, i.e., m, n are even, Ri = \u2202 H for i even, Ri = \u2212 \u2202 H for i odd;\n\u2202xi\u22121\n\u2202xi+1\n4\n\n\fH = H(x) is the Hamiltonian and Z \u22121 e\u2212H is then an invariant probability\ndensity for the system.\nSuppose we can afford to solve only m < n of the equations in (8) or have\nonly m data components x\u0302. We want to solve equations for \u03c6\u0302:\nd\u03c6\u0302\n= R\u0302(\u03c6), \u03c6\u0302(0) = x\u0302,\ndt\nwhere i \u2264 m, but the argument of R\u0302 is the whole vector \u03c6. It is natural to\napproximate R\u0302i (\u03c6) by the closest function of \u03c6\u0302 for each i \u2264 m, i.e., solve\nd\u03c6\u0302\n= E[R\u0302(\u03c6) | \u03c6\u0302].\ndt\n\n(9)\n\nThe approximation (9) is valid only for a short time, as one can see from the\nd\u03c6\u0302\nin [5],[6], see also below. Hald's theorem\ndt\nasserts that the system (9) is Hamiltonian, with Hamiltonian \u0124 = \u0124(x\u0302) =\nR\n\u2212 log e\u2212H(x) dx\u0303, a relation identical to equation (6). The existence of \u0124 shows\nthat the approximation (9) cannot be valid for long times: the predictive power\nof partial initial data decays at t \u2192 \u221e for a nonlinear system, and the best\nestimate of \u03c6\u0302(t) should decay to unconditional mean of \u03c6 (which is usually zero).\nThe existence of a reduced Hamiltonian shows that this decay can happen only\nto a limited extent and thus the approximation can in general be valid only\nfor short times. Equations (9) constitute the short time, or \"first-order\", OP\napproximation.\nSuppose however that instead of picking specific values for the initial data x\u0302\none samples them from the invariant density Z \u22121 e\u2212\u0124(x\u0302) . The distribution of the\nx\u0302's is then invariant, and equal to their marginal distribution in the full system\n(8) when the data are sampled from the invariant distribution Z \u22121 e\u2212H(x) , as\nR\n\u0001 R\none can also see from the identities exp(\u2212\u0124) = exp log e\u2212H dx\u0303 = e\u2212H dx\u0303.\nThe system (9) then generates the marginal probability density of part of the\nvariables of a system at equilibrium. Thus OP at equilibrium is a way of reducing\nthe number of variables without affecting the statistics of the variables that\nremain. One can make short-time predictions about the future from the reduced\nsystem with coefficients computed at equilibrium because it is self-consistent\nto assume for short times that unresolved degrees of freedom are in thermal\nequilibrium, as is explained in the OP papers cited above.\nfull equation for the evolution of\n\n3\n\nRenormalization\n\nFor simplicity, we work here with real-space renormalization applied to variables\nassociated with specific sites in a plane, x(1) = (xI1 , xI2 , . . .), where Ik = (ik , jk ),\n5\n\n\fik , jk are integers, all the Ik are inside a square D of side N with N large, and\nthe variables are listed in some convenient order. The Hamiltonian H = H (1)\nis a function of x(1) , H (1) = H (1) (x(1) ). The need for the superscript (1) will\nR\n(1)\n(1)\nappear shortly. We assume that the partition function Z = e\u2212H (x ) dx(1)\n(1)\n(1)\nis well defined, where dx(1) = dxI1 dxI2 . . ..\nSuppose we group the variables xI1 , xI2 , . . . into groups of l variables (for\nexample, we could divide D into squares each containing 4 variables). The\nvariables can be referred to as \"spins\"in conformity with common usage in\n(2)\n(2)\nphysics. Associate with each group a new variable xJ1 , xJ2 , . . . , where J1 , J2 , . . .\n(2)\n\nis some ordering of the new variables and xJk is a function (not necessar(2)\n\n(2)\nily invertible) of the xI in\n(1)\n(1)\n(1)\ng(xIm+1 , xIm+2 , . . . , xIm+l ) for\n(2)\n(2)\n(xJ1 , xJ2 . . .). We can write\n\nZ\n\n=\n=\n(2)\n\nR\n\nR\n\ne\u2212H\n\n(2)\n\ndx\n\n(2)\n\n(1)\n\nthe group labeled by Jk , for example xJk =\nthe appropriate m. The vector x(2) is x(2) =\n\n(x(1) )\n\nR\n\ndx(1)\n(2)\n\n\u03b4 x\n\n(1)\n\n\u2212 g(x\n\n\u0001 \u2212H (1) (x(1) ) (1) .\n) e\ndx\n\nwhere dx(2) = dxJ1 dxJ2 * * * , and the \u03b4 function is a product of delta functions,\none per group. If one defines H (2) (x(2) ) by the equation\nZ \u0010\n\u0011\n(1)\n\u2212H (2) (x(2) )\ne\n= \u03b4 x(2) \u2212 g(x(1) ) e\u2212H(x ) dx(1) ,\n(10)\nR\n(2)\n(2)\nthen Z = e\u2212H (x ) dx(2) .\nThe mapping x(1) \u2192 x(2) , followed by a change of numbering of the remaining variables so that J1 , J2 . . . (the indices of the new variables x(2) ) enumerate\nthe new variables by going through all integer pairs in a reduced domain of side\n\u221a\nN/ l, is a real-space renormalization group transformation; it produces a new\nset of variables which has less spatial detail than the previous set and such that\n\u221a\ndistances between the remaining spins have been scaled down by l. If the calculation is set up so that the mapping x(1) \u2192 x(2) , H (1) \u2192 H (2) can be repeated,\nfor example, if the range of the variables x(1) is invariant and the Hamiltonians\nH (1) , H (2) can be represented in the same finite-dimensional basis, then one can\nproduce in this way a sequence of Hamiltonians H (1) , H (2) , H (3) , . . .; the fixed\npoints of the transformation H (n) \u2192 H (n+1) for a spin system of infinite spatial\nextent include the critical points of the system, see any discussion of the RNG,\nfor example [12],[13].\n(1)\n(2)\nConsider the special case where xJ is one of the xI in its group\u2013i.e., replace\na block of spins by one of the spins in the block. More general and widely used\nassignments of block variables will not be needed in the present paper and will\n6\n\n\fbe discussed elsewhere. We can identify the spins that remain with x\u0302 of the\npreceding section and the spins that disappear with x\u0303. Equation (10) becomes\na special case of equation (5), and can be solved for H (2) by taking conditional\nexpectations of the derivatives of H (1) .\nNote that the usual RNG representation of a renormalized Hamiltonian by\nmeans of additional couplings [12] is interpreted here as an expansion of a conditional expectation in a convergent series. The new interpretation may be useful\nboth in understanding what is happening on the computer and in deriving error\nestimates. The relation between the RNG and conditional expectations shows\nthat the latter can be calculated recursively, as we show in the example below.\nWe have written the RNG transformation above in notation suitable for spins\nwith a continuous range. The case of discrete (e.g., Ising) spins is automatically\nincluded, even though it may seem odd to differentiate functions with a discrete\ndomain and range. Indeed, add to the Hamiltonian H a term of the form\n\uf8f6\n\uf8eb\n1 X \uf8edY\n\u03c8(xi \u2212 x0j )\uf8f8\n\u03b5 i\nj\n\nwhere \u03b5 is small, the sum is over all spins, the product is over a finite number of\nvalues x0j , and \u03c8 \u2265 0 has a minimum at 0 and is positive elsewhere. For small \u03b5\nsuch a term will constrain the xi to take on the values x0j , but since at the origin the derivative of \u03c8 is zero the calculation of the conditional expectations is\nunaffected by this term and the limit \u03b5 \u2192 0 can be taken without actually doing\nanything on the computer. All one has to do is make sure that in the MonteCarlo sampling only the values x0j are sampled. Indeed, results below will be\ngiven for Ising spins which take on the values +1 and \u22121, with a \"bare\" (unP\nrenormalized) Hamiltonian H (1) = \u03b2 xI xJ , with summation over locations\nI, J that are neighbors on the lattice; \u03b2 = 1/T , where T is the temperature.\n\n4\n\nA decimation RNG/OP scheme for a spin system\n\nWe consider in detail a RNG/recursive OP scheme where the number of variables\nis halved at each step. The spins are located on a square lattice with nodes\nIk = (ik , jk ), ik , jk integers, and at each step of the recursion those for which\nik + jk is odd are eliminated while those for which ik + jk is even are kept.\nThe spins with ik + jk even constitute x\u0302 and the others x\u0303; the choice of which\nare even and which are odd is a matter of convention only (see Figure 1). The\nvariables are labeled by Ik : xI1 , xI2 , . . ..\n7\n\n\fFigure 1: The decimation pattern\nGiven a location I = (i, j), we group the other variables according to their\ndistance from I: group 1 contains only xI , the variable at I. Group 2 (relative\nto I) contains those variables whose distance from I is 1, group 3 contains those\n\u221a\nvariables whose distance to I is 2, etc. We form the \"collective\" variables\n1 X\nxJ\nXk,I =\nnk\ngroup k\n\nwhere nk is the number of variables in the group (1 for group 1, 4 for group 2,\netc.). From these variables one can form a variety of translation-invariant polyP\nP\nP\nnomials in x of various degrees: I xI Xk,I = I X1,I Xk,I , I (Xk,I )2 (Xk+1,I )2 ,\nP\n4\nI (Xk,I ) , . . .. In practice the domain over which one sums must be finite, and\nit is natural to impose periodic boundary conditions at its edges to preserve the\ntranslation invariance. We wrote out explicitly only polynomials of even degrees\nbecause the Hamiltonians we consider are invariant under the transformation\nx \u2192 \u2212x. The translation-invariant polynomials built up from the Xk,I can be\nlabeled \u03c61 (x), \u03c62 (x), . . . in some order.\nExpand the n-th renormalized Hamiltonian in a series and keep the first l\nterms:\nl\nX\n(n)\nH (n) =\n\u03b1k \u03c6k (x).\n(11)\nk=1\n\nThe derivative of this series at the spin xI is\nl\n\nX (n)\n\u2202\nH (n) =\n\u03b1k \u03c6\u2032k (x),\n\u2202xI\n1\n\n\u03c6\u2032k =\n\nThe functions \u03c6\u2032k are easily evaluated, for example:\n!\u2032\nX\nxJ Xk,J = 2Xk,I\nJ\n\n8\n\n\u2202\n\u03c6k .\n\u2202xI\n\n(12)\n\n\fX\n\n(Xk,J )4\n\nJ\n\n!\u2032\n\n\uf8eb\n\nX\n\n= 4\uf8ed\n\ngroup k\n\n\uf8f6\n\nx3J \uf8f8 /n2k ,\n\netc., where \"group k\" refers to distances from I, the variable with respect to\nwhich we are differentiating (see Figure 2).\nPick a variable xI in x\u0302 (i.e., I = (i, j), i + j even in our conventions). Some\nof the functions \u03c6\u2032k in (12) will be functions of x\u0302 only and some will be functions\nof both x\u0302 and x\u0303 or of x\u0303 only. The task at hand is to project the latter on the\nformer and then rearrange the series so as to shrink the scale of the physical\ndomain. To explain the construction we consider a very special case.\nSuppose one can write\n(n)\n\n(n)\n\n(n)\n\nH (n) (x) = \u03b12 \u03c62 + \u03b13 \u03c63 + \u03b14 \u03c64 ,\n\n(13)\n\nwhere \u03c6k (x) =\nxI Xk,I for k = 2, 3, 4, . . .. Note that \u03c6\u2032k = \u2202x\u2202 I \u03c6k is a function only of x\u0302 when k = 3, 4 (and when k = 6, as we shall need to know\nshortly) but not when k = 2 or 5 (see Figures 1, 2). We now calculate the\nconditional expectations of the derivatives of H (n) given x\u0302 by projecting them\non the space of functions of x\u0302. First we project \u03c6\u20322 on the span of \u03c6\u20323 , \u03c6\u20324 , \u03c6\u20326\n(note that \u03c6\u20326 is not in the original expansion (13)). Form the matrix \u03a6 with\nrows h\u03c6\u2032k , \u03c6\u20323 i, h\u03c6\u2032k , \u03c6\u20324 i, h\u03c6\u2032k , \u03c6\u20326 i for k = 3, 4, 6, the primes once again denoting differentiation with respect to xI . Form the vector r with component\n(h\u03c6\u20322 , \u03c6\u20323 i, h\u03c6\u20322 , \u03c6\u20324 i, h\u03c6\u20322 , \u03c6\u20326 i). Let c = (c1 , c2 , c3 ) be the solution of \u03a6c = r (see\nequation (4)). The coefficients c are the coefficients of the orthogonal projection\nof \u03c6\u20322 onto the span of \u03c6\u20323 , \u03c6\u20324 , \u03c6\u20326 which is contained in L\u03022 . After projection, the\ncoefficients of \u03c63 , \u03c64 in (13) become\nP\n\n(n)\n\n(n)\n\n(n)\n\n(n)\n\n\u03b1new\n= \u03b13 + \u03b12 c1 ,\n3\n= \u03b14 + \u03b12 c2 ,\n\u03b1new\n4\n(n)\n\nand \u03c66 acquires the coefficient \u03b12 c3 .\nIf one relabels the remaining spins so that they occupy the lattice previously\noccupied by all the spins, group 3 becomes group 2, group 4 becomes group 3,\nand group 6 becomes group 4 (see Figure 2). The new Hamiltonian H (n+1) now\nhas the representation\n(n+1)\n\nH (n+1) = \u03b12\nwith\n\n(n+1)\n\n\u03c62 + \u03b13\n\n(n+1)\n\n\u03c63 + \u03b14\n\n(n)\n\n(n)\n\n(n)\n\n(n)\n\n(n+1)\n\n= \u03b13 + \u03b12 c1 ,\n\n(n+1)\n\n= \u03b14 + \u03b12 c2 ,\n\n(n+1)\n\n=\n\n\u03b12\n\u03b13\n\u03b14\n\n(n)\n\n\u03b12 c3 .\n9\n\n\u03c64 ,\n\n\f7\n\n7\n\n6\n\n5\n\n4\n\n5\n\n6\n\n5\n\n3\n\n2\n\n3\n\n5\n\n4\n\n2\n\nI\n\n2\n\n4\n\n5\n\n3\n\n2\n\n3\n\n5\n\n6\n\n5\n\n4\n\n5\n\n6\n\n7\n\n7\n\nFigure 2: The collective variables\nMore generally, if H (n) is expressed as a truncated series, partition the terms in\n\u2202\nthe series for \u2202x\nH (n) into functions of x\u0302 and functions of both x\u0302 and x\u0303. Add\nI\nto the terms which are functions of x\u0302 additional terms which are functions of x\u0302\nand are chosen so that after relabelling they acquire the form of terms already\nin the series (just as above, terms that depend on X3,J , for example, become\nterms that depend on X2,J after relabelling). Project the functions of x on the\nspan of the expanded set of functions of x\u0302, collect terms and relabel. This is a\nrenormalization step, and it can be repeated.\nNote that it if one wants to reduce the number of variables by a given factor,\none can in principle use an analogous RNG/conditional expectation construction\nand get there in one iteration; the recursive construction is easier to do and the\nintermediate Hamiltonians, whose coefficients constitute the parameter flow in\nthe renormalization, contain useful information.\nThe discussion so far may suggest that one sample the Hamiltonians recursively, i.e., start with H (1) , find H (2) , use Monte-Carlo to sample the density\n(2)\nZ \u22121 e\u2212H and find H (3) etc. The disadvantages of this approach are: (i) The\n(n)\nsampling of the densities Z \u22121 e\u2212H\ncan be much more expensive for n > 1\nthan for n = 1 because each proposed Monte-Carlo move may require that\n\u2202\nthe full series for \u2202x\nH (n) be summed twice; and (ii) each evaluation of a new\nI\nHamiltonian is only approximate because the series are truncated, and, more\nimportant, the Monte-Carlo evaluation of the coefficients may have limited accuracy. These errors accumulate from step to step and may produce false fixed\npoints and other artifacts.\nThe remedy lies in Swendsen's observation [3],[14] that the successive Hamiltonians can be sampled without being known explicitly. Sample the original\nHamiltonian, remove the unwanted spins and relabel the remaining spins so as\nto cover the original lattice, as in the relabelling step in the renormalization;\n10\n\n\f(2)\n\nthe probability density of the remaining spins is Z \u22121 e\u2212H ; repeating n times\n(n+1)\nyields samples of Z \u22121 e\u2212H\n. The price one pays is that to get an m by m\n(n)\nsample of Z \u22121 e\u2212H one has to start by sampling a 2q m by 2q m array of nonrenormalized spins, where q is either (n + 1)/2 or n/2 depending on the parity\nof n and on programming choices; the trade-off is in general very worthwhile.\nWhat has been added to Swendsen's calculation is an effective evaluation of the\ncoefficients of the expansion of H (n) from the samples.\nThe programming here requires some care. With the decimation scheme as\nin Figure 1, after one removes the unwanted spins in x(n) the remaining spins,\n\u221a\nthe variables x(n+1) , live on a lattice with a mesh size 2 larger than before;\nafter relabelling they find themselves on a lattice with the same mesh size as\nbefore but arranged at a \u03c0/4 angle with respect to the previous lattice. To\nextract a square array from at this set of spins one has to make the size of the\nbox that includes all the spins half the size of the previous box. At the next\nrenormalization one obtains x(n+2) which can be extracted from x(n) by taking\none spin in four and the resulting box size is the same as the size of the box that\ncontains x(n+1) . One may worry a little about boundary conditions for x(n+1) :\nthe periodicity of x(n) is not the same as the periodicity one has to assume for\nx(n+1) because of the rotation; the resulting error is too small to be detected in\nour calculations.\n\n5\n\nSome numerical results\n\nWe now present some numerical results obtained with the RNG/conditional expectation scheme. The problem we apply the construction to is Ising spins; more\ninteresting applications will be presented elsewhere. The point being made is\nthat the construction can be effectively implemented. The results are presented\nfor Ising spins.\n(n)\nIn table I we list the coefficients \u03b1k in the expansion of H (n) for n = 1, . . . 7\nand T = 2.27. The functions \u03c6k are as follows:\nX\n\u03c6k =\nxJ Xk,J for k = 1, 2, 3, 4, 5, 6\n\u03c66+k =\n\nX\n\n(Xk+1,J )4 , for k = 1, 2, 3\nX\n2\n2\n\u03c610 =\nX2,J\nX3,J\n.\n\nNote that as a result of the numbering of the \u03c6's the last coefficient is not\nnecessarily the smallest coefficient. This table represents the parameter flow\nand if the functions \u03c6k are written in terms of the variables xJ the table defines\nthe new system of equations for the reduced set of variables. Remember that\n11\n\n\fin the projection on L\u03022 additional functions are used so that after relabelling\nthe series has the same terms , but maybe with different coefficients, as before\nthe renormalization. In H (1) , \u03b12 is the sole non-zero coefficient, and its value\nis determined by T and the definition of X2,J , in particular the presence of the\ncoefficient n2 (see above).\nIt is instructive to use the parameter flow to identify the critical temperature\nTc . For T < Tc the renormalization couples ever more distant spins while\nfor T > Tc the spins become increasingly decoupled. One can measure the\nincreasing or decreasing coupling by considering the quadratic terms in the\nP\nHamiltonian (the terms of the form\nxJ Xk,J ) and calculating the \"second\n(n)\nmoments\" M2 of their coefficients \u03b1k :\n(n)\n\nM2\n\n=\n\nl\nX\n\n(n)\n\nd2k \u03b1k\n\nk=2\n\nwhere dk is the distance from J of the spins in the group k (see the definition of\nP\n(n)\nXk,J ), \u03b1k is the coefficient of\nxJ Xk,J in the expansion of H (n) , and l is the\nnumber of quadratic terms in this expansion. In Figure 3 we show the evolution\n(n)\nof M2 with n for various values of T (with l = 5 and 7 functions over-all in\nthe expansion, including non-quadratic functions).\n5\n\nT=1.8\nT=2.0\nT=2.15\nT=2.20\nT=2.30\nT=2.50\n\n4.5\n\n4\n\n3.5\n\n3\n\n2.5\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\nFigure 3: Second moments of the coefficients of the renormalized Hamiltonian\nfor various values of T for successive iterations\n(n)\n\nIn Figure 4 we show the evolution of M2 near Tc = 2.269 . . . with l = 6 and\n10 terms in the expansion. The non-uniform behavior of M2 is not a surprise (it\n12\n\n\fis related to the non-uniform convergence of critical exponents already observed\nby Swendsen). Each step in the renormalization used 105 Monte-Carlo steps\nper spin. From these graphs one would conclude that Tc \u223c 2.26, an error of\n.5%. The accuracy depends on the number of terms in the expansion and on the\nchoice of terms; with only 6 terms (4 quadratic and 2 quartic), the error in the\nlocation of Tc increases to about 3%. The point is not that this is a good way to\nfind Tc but that it is a check on the accuracy of the parameter flow. From the\nTable one can see that the system first approaches the neighborhood of a fixed\npoint and then diverges from it, as one should expect in a discrete sequence of\ntransformations.\n2.6\n\n2.4\n\n2.2\n\n2\n\n1.8\n\n1.6\n\n1.4\n\nT=2.24\nT=2.25\nT=2.26\nT=2.27\nT=2.28\nT=2.29\n\n1.2\n\n1\n\n0.8\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\nFigure 4: Second moments of the coefficients of the renormalized Hamiltonian\nnear Tc\n\n13\n\n\fParameter\niteration\n\u03b11\n\u03b12\n\u03b13\n\u03b14\n\u03b15\n\u03b16\n\u03b17\n\u03b18\n\u03b19\n\u03b110\n\nTable 1\nflow for the Ising model T = 2.26, 10 basis functions\n1\n2\n3\n4\n5\n6\n7\n0\n.26\n.35\n.44\n.48\n.52\n.54\n.893\n.47\n.47\n.35\n.30\n.25\n.21\n0\n.32\n.20\n.23\n.21\n.20\n.18\n0\n.04\n.08\n.11\n.12\n.13\n.13\n0\n.07\n.11\n.13\n.13\n.12\n.12\n0\n\u2212.01\n.01\n.01\n.02\n.03\n.02\n0\n\u2212.08 \u2212.07 \u2212.10 \u2212.09 \u2212.09\n\u2212.08\n0\n.04\n.02\n.02\n.01\n.00\n\u2212.10\n0\n\u2212.00 \u2212.01 \u2212.00 \u2212.00\n.00\n.00\n0\n\u2212.12 \u2212.17 \u2212.18 \u2212.18 \u2212.17\n\u2212.16\n\n0.9\n\n0.8\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n2.1\n\n20 x 20\n40 x 40\n60 x 60\n20 x 20, renormalized\nOnsager\n2.15\n\n2.2\n\n2.25\n\n2.3\n\n2.35\n\n2.4\n\n2.45\n\nFigure 5: Bare and renormalized magnetization near Tc\nWe now use the renormalized system to calculate the magnetization m =\nP\nE[ xI /n2 ]. To get the correct non-zero m for T < Tc on a small lattice the\nsymmetry must be broken, and we do this by imposing on all the arrays the\nboundary condition xboundary = 1 rather than the periodic boundary conditions\nused elsewhere in this paper. In Figure 5 we display m computer with the bare\n(unrenormalized) Hamiltonian H (1) on 3 lattices: 20 by 20, 40 by 40, 60 by 60,\nas well as the results obtained on a 20 by 20 lattice by sampling the density\ndefined by the renormalized Hamiltonian H (5) which corresponds in principle\nto an 80 by 80 bare calculation. We also display the exact Onsager values of m.\n14\n\n\fThe calculations focus on values of T in the neighborhood of Tc where the size\nof the lattice matters; one cannot expect the results to agree perfectly with the\nOnsager results on a finite lattice with periodic boundary conditions for any n;\nall one can expect is to have the values of the small renormalized calculation be\nconsistent with results of a larger bare calculation. We observe that they do, up\nto the shift in Tc already pointed out and due to the choice of basis functions.\nThe determination of the critical exponents for a spin model is independent\nof the determination of the coefficients in the expansion of H (n) , and is mentioned here only because it does provide a sanity check on the constructions,\nin particular on the adequacy of the basis functions. For comparable earlier\ncalculations, see in particular Swendsen's chapter in [3]. As is well known, if A\n(n)\n(n+1)\n/\u2202\u03b1j at T = Tc , those of its eigenvalues\nis the matrix of derivatives \u2202\u03b1i\nthat are larger than 1 are the critical exponents of the spin system [12]. The\nmatrix A can be found from the chain rule [2],[3]\n\u0011i X \u2202\u03b1(n+1) \u2202E[\u03c6 (x(n+1) )]\nh \u0010\nk\n(n+1)\ni\n=\nx\nE\n\u03c6\nk\n(n)\n(n)\n(n+1)\n\u2202\u03b1j\n\u2202\u03b1j\n\u2202\u03b1i\ni\n\u2202\n\nand the sum is over all the coefficients that enter the expansion. The derivatives\nof the expectations are given by correlations as follows:\n\u2202E[\u03c6k (x(n+1) )]\n(n)\n\n\u2202\u03b1j\n\n\u2202E[\u03c6k (x(n+1) )]\n(n+1)\n\n\u2202\u03b1i\n\nh\ni\n= E \u03c6k (x(n+1) )\u03c6j (x(n) ) \u2212 E[\u03c6k (x(n+1) )]E[\u03c6j (x(n) )],\n\nh\ni\n= E \u03c6k (x(n+1) )\u03c6i (x(n+1) ) \u2212 E[\u03c6k (x(n+1) )]E[\u03c6i (x(n+1) )],\n\nsee [3]. In most of the literature on real-space renormalization for Ising spins the\nvariables x(n+1) are obtained from x(n) by \"majority rule\", i.e., by assigning to\nthe group that defines x(n+1) the value +1 if most of the members of the group\nare +1, the value \u22121 if most of the members of the group are \u22121, with ties\nresolved at random. For the decimation scheme described above our \"pick one\"\nrule (x(n+1) is one of the members of the group) is identical to the majority rule.\nThere is an apparent difficulty in the decimation because at each recursion the\nnumber of terms in the summation that defines the basis functions is reduced\nby half while the square root of an integer is not in general an integer, so that\none has to perform Swendsen sampling on rectangles so designed that the ratio\nof the areas of two successive rectangles is 1/2. This has not turned out to be\nharmful, and the value of \u03bd, the correlation exponent, was found to be 1 (the\nexact value) \u00b1.01 with 106 Monte-Carlo moves per spin, the error depending\nmainly on the number of Monte-Carlo moves which has to be very large, in line\n15\n\n\fwith previous experience [14]. We also checked that in a renormalization scheme\nwhere a 2 \u00d7 2 block of spins is replaced at each iteration by a single spin, the\n\"majority rule\" and our \"pick one\" rule for x(n+1) yield similar results. One\nneeds fewer terms in the expansion of the Hamiltonian to get accurate values of\nthe exponents than to get an accurate parameter flow, but a larger number of\nMonte-Carlo moves.\n\n6\n\nConclusions\n\nWe have presented a simple relation between conditional expectations for systems at equilibrium on one hand and the RNG on the other, which makes it\npossible to find efficiently the coefficients in a reduced systems of equations for\na subset of variables whose distribution as given by reduced system equals their\nmarginal distribution in the original system. The numerical results above emphasized the neighborhood of the critical point in the simple example because\nthis is where the variables are strongly coupled without separation of scales and\na reduction in system size requires non-trivial tools. The next steps will be the\napplication of these ideas to time-dependent problems and to finite-difference\napproximations of underresolved partial differential equations, along the lines\nsuggested in [10]; this work will be presented elsewhere.\nAcknowledgments. I would like to thank Prof. G.I. Barenblatt, Prof. N.\nGoldenfeld, Prof. O. Hald, Prof. R. Kupferman, Mr. K. Lin, and Mr. P. Stinis\nfor very helpful discussions and comments. This work was supported in part by\nthe Office of Science, Office of Advanced Scientific Computing Research, Mathematical, Information, and Computational Sciences Division, Applied Mathematical Sciences Subprogram, of the U.S. Department of Energy under Contract\nNo. DE-AC03-76SF00098 and in part by the National Science Foundation under\ngrant number DMS89-19074.\n\n16\n\n\fReferences\n[1] P. Bickel and K. Doksum, Mathematical Statistics: Basic Ideas and Selected Topics, Prentice-Hall, New York, 2000, p. 133 and ff.\n[2] J. Binney, N. Dowrick, A. Fisher, and M. Newman, The Theory of Critical\nPhenomena, The Clarendon Press, Oxford, 1992.\n[3] T. Burkhardt and J. van Leeuwen, Real-Space Renormalization, Springer,\nBerlin, 1982.\n[4] A. Chorin, Stochastic Methods in Applied Mathematics and Physics, Lecture notes, UC Berkeley Math. Dept., 2002.\n[5] A. Chorin, O. Hald and R. Kupferman, Optimal prediction and the MoriZwanzig representation of irreversible processes. Proc. Nat. Acad. Sc. USA,\n97, (2000), pp. 2968\u20132973.\n[6] A. Chorin, O. Hald and R. Kupferman, Optimal prediction with memory,\nPhysica D, 2002.\n[7] A. Chorin, A. Kast and R. Kupferman, Optimal prediction of underresolved\ndynamics, Proc. Nat. Acad. Sc. USA, 95 (1998), pp. 4094\u20134098.\n[8] A. Chorin, R. Kupferman and D. Levy, Optimal prediction for Hamiltonian\npartial differential equations, J. Comput. Phys., 162, (2000), pp. 267\u2013297.\n[9] D. Evans and G. Morriss, Statistical Mechanics of Nonequilibrium Liquids,\nAcademic, London, 1990.\n[10] N. Goldenfeld, A. McKane and Q. Hou, Block spins for partial differential\nequations, J. Stat. Phys., 93, (1998), pp. 699\u2013714.\n[11] Q. Hou, N. Goldenfeld and A. McKane, Renormalization group and perfect\noperators for stochastic differential equations, Phys. Rev. E, 63 (2001), pp.\n036125:1\u201322.\n[12] L. Kadanoff, Statistical Physics: Statics, Dynamics, and Renormalization,\nWorld Scientific, Singapore, 2000.\n[13] S.S. Ma, Modern Theory of Critical Phenomena, Benjamin, Reading, Mass,\n1976.\n[14] R. Swendsen, Monte-Carlo renormalization group, Phys. Rev. Lett. 42\n(1979), pp. 859\u2013861.\n\n17\n\n\f[15] R. Zwanzig, Nonlinear generalized Langevin equations, J. Stat. Phys., 9,\n(1973), pp. 215\u2013220.\n\n18\n\n\f"}