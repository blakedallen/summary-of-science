{"id": "http://arxiv.org/abs/1004.4641v2", "guidislink": true, "updated": "2010-07-19T19:06:11Z", "updated_parsed": [2010, 7, 19, 19, 6, 11, 0, 200, 0], "published": "2010-04-26T20:12:37Z", "published_parsed": [2010, 4, 26, 20, 12, 37, 0, 116, 0], "title": "Chunky and Equal-Spaced Polynomial Multiplication", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.5245%2C1004.3750%2C1004.1459%2C1004.1209%2C1004.5452%2C1004.1817%2C1004.4466%2C1004.3324%2C1004.1036%2C1004.0411%2C1004.3474%2C1004.1966%2C1004.2962%2C1004.3640%2C1004.0044%2C1004.4612%2C1004.0443%2C1004.0903%2C1004.2517%2C1004.5538%2C1004.2806%2C1004.0237%2C1004.5118%2C1004.2830%2C1004.3384%2C1004.3837%2C1004.2440%2C1004.4358%2C1004.1529%2C1004.0836%2C1004.2765%2C1004.3487%2C1004.3380%2C1004.1996%2C1004.0657%2C1004.0523%2C1004.0213%2C1004.3518%2C1004.3956%2C1004.5349%2C1004.2614%2C1004.0588%2C1004.0665%2C1004.1328%2C1004.0692%2C1004.4641%2C1004.2813%2C1004.5420%2C1004.1850%2C1004.0048%2C1004.2767%2C1004.5475%2C1004.0340%2C1004.1450%2C1004.4554%2C1004.2790%2C1004.2058%2C1004.2753%2C1004.5068%2C1004.5314%2C1004.3828%2C1004.4126%2C1004.4782%2C1004.2694%2C1004.3369%2C1004.2021%2C1004.3313%2C1004.0584%2C1004.1723%2C1004.0125%2C1004.0400%2C1004.3926%2C1004.2896%2C1004.2298%2C1004.0602%2C1004.2169%2C1004.4722%2C1004.3364%2C1004.4760%2C1004.4093%2C1004.2259%2C1004.3996%2C1004.5427%2C1004.2216%2C1004.2286%2C1004.2064%2C1004.5109%2C1004.4587%2C1004.0504%2C1004.3604%2C1004.3024%2C1004.2320%2C1004.4960%2C1004.0675%2C1004.3139%2C1004.5378%2C1004.5515%2C1004.3076%2C1004.5197%2C1004.5576%2C1004.4169&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Chunky and Equal-Spaced Polynomial Multiplication"}, "summary": "Finding the product of two polynomials is an essential and basic problem in\ncomputer algebra. While most previous results have focused on the worst-case\ncomplexity, we instead employ the technique of adaptive analysis to give an\nimprovement in many \"easy\" cases. We present two adaptive measures and methods\nfor polynomial multiplication, and also show how to effectively combine them to\ngain both advantages. One useful feature of these algorithms is that they\nessentially provide a gradient between existing \"sparse\" and \"dense\" methods.\nWe prove that these approaches provide significant improvements in many cases\nbut in the worst case are still comparable to the fastest existing algorithms.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.5245%2C1004.3750%2C1004.1459%2C1004.1209%2C1004.5452%2C1004.1817%2C1004.4466%2C1004.3324%2C1004.1036%2C1004.0411%2C1004.3474%2C1004.1966%2C1004.2962%2C1004.3640%2C1004.0044%2C1004.4612%2C1004.0443%2C1004.0903%2C1004.2517%2C1004.5538%2C1004.2806%2C1004.0237%2C1004.5118%2C1004.2830%2C1004.3384%2C1004.3837%2C1004.2440%2C1004.4358%2C1004.1529%2C1004.0836%2C1004.2765%2C1004.3487%2C1004.3380%2C1004.1996%2C1004.0657%2C1004.0523%2C1004.0213%2C1004.3518%2C1004.3956%2C1004.5349%2C1004.2614%2C1004.0588%2C1004.0665%2C1004.1328%2C1004.0692%2C1004.4641%2C1004.2813%2C1004.5420%2C1004.1850%2C1004.0048%2C1004.2767%2C1004.5475%2C1004.0340%2C1004.1450%2C1004.4554%2C1004.2790%2C1004.2058%2C1004.2753%2C1004.5068%2C1004.5314%2C1004.3828%2C1004.4126%2C1004.4782%2C1004.2694%2C1004.3369%2C1004.2021%2C1004.3313%2C1004.0584%2C1004.1723%2C1004.0125%2C1004.0400%2C1004.3926%2C1004.2896%2C1004.2298%2C1004.0602%2C1004.2169%2C1004.4722%2C1004.3364%2C1004.4760%2C1004.4093%2C1004.2259%2C1004.3996%2C1004.5427%2C1004.2216%2C1004.2286%2C1004.2064%2C1004.5109%2C1004.4587%2C1004.0504%2C1004.3604%2C1004.3024%2C1004.2320%2C1004.4960%2C1004.0675%2C1004.3139%2C1004.5378%2C1004.5515%2C1004.3076%2C1004.5197%2C1004.5576%2C1004.4169&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Finding the product of two polynomials is an essential and basic problem in\ncomputer algebra. While most previous results have focused on the worst-case\ncomplexity, we instead employ the technique of adaptive analysis to give an\nimprovement in many \"easy\" cases. We present two adaptive measures and methods\nfor polynomial multiplication, and also show how to effectively combine them to\ngain both advantages. One useful feature of these algorithms is that they\nessentially provide a gradient between existing \"sparse\" and \"dense\" methods.\nWe prove that these approaches provide significant improvements in many cases\nbut in the worst case are still comparable to the fastest existing algorithms."}, "authors": ["Daniel S. Roche"], "author_detail": {"name": "Daniel S. Roche"}, "author": "Daniel S. Roche", "arxiv_comment": "23 Pages, pdflatex, accepted to Journal of Symbolic Computation (JSC)", "links": [{"href": "http://arxiv.org/abs/1004.4641v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1004.4641v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.1.2; G.4; F.2.1", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1004.4641v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1004.4641v2", "journal_reference": null, "doi": null, "fulltext": "arXiv:1004.4641v2 [cs.SC] 19 Jul 2010\n\nChunky and Equal-Spaced\nPolynomial Multiplication\nDaniel S. Roche\nSymbolic Computation Group\nCheriton School of Computer Science\nUniversity of Waterloo\nWaterloo, Ontario, Canada\ndroche@cs.uwaterloo.ca\nhttp://www.cs.uwaterloo.ca/~droche/\n\nOctober 28, 2018\nAbstract\nFinding the product of two polynomials is an essential and basic problem in computer algebra. While most previous results have focused on\nthe worst-case complexity, we instead employ the technique of adaptive\nanalysis to give an improvement in many \"easy\" cases. We present two\nadaptive measures and methods for polynomial multiplication, and also\nshow how to effectively combine them to gain both advantages. One useful feature of these algorithms is that they essentially provide a gradient\nbetween existing \"sparse\" and \"dense\" methods. We prove that these approaches provide significant improvements in many cases but in the worst\ncase are still comparable to the fastest existing algorithms.\n\n1\n\nIntroduction\n\nComputing the product of two polynomials is one of the most important problems in symbolic computation, and the operation is part of the basic functionality of any computer algebra system. We introduce new multiplication algorithms\nwhich use the technique of adaptive analysis to gain improvements compared to\nexisting approaches both in theory and in practice.\n\n1.1\n\nBackground\n\nFor what follows, R is an arbitrary ring (commutative, with identity), such that\nring elements have unit storage and basic ring operations have unit cost. In\ncomplexity estimates, we also count operations on word-sized integers, which\nare assumed only to be large enough (in absolute value) to store the size of the\ninput.\n\n1\n\n\fThere are essentially two representations for univariate polynomials over R,\nand existing algorithms for multiplication require one of these representations.\nLet f \u2208 R[x] with degree less than n written as\nf = c0 + c1 x + c2 x2 + * * * + cn\u22121 xn\u22121 ,\n\n(1.1)\n\nfor c0 , . . . , cn\u22121 \u2208 R. The dense representation of f is simply an array [c0 , c1 , . . . , cn\u22121 ]\nof length n.\nNext, suppose that at most t of the coefficients are nonzero, so that we can\nwrite\nf = a1 xe1 + a2 xe2 + * * * + at xet ,\n(1.2)\nfor a1 , . . . , at \u2208 R and 0 \u2264 e1 < * * * < et . Hence ai = cei for 1 \u2264 i \u2264 t, and\nin particular et = deg f . The sparse representation of f is a list of coefficientexponent tuples (a1 , e1 ), . . . , (at , et ). The exponents in this case could be multiprecision P\nintegers, and so the total size of the sparse representation is proportional to i (1 + log2 ei ). This is bounded below by \u03a9(t log t + log n) and above\nby O(t log n).\nAlgorithmic advances in dense polynomial multiplication have generally followed results for long integer multiplication. The O(n2 ) school method was first\nimproved by Karatsuba and Ofman [1963] to O(n1.59 ) with a two-way divideand-conquer scheme, later generalized to k-way by Toom [1963] and Cook [1966].\nSch\u00f6nhage and Strassen [1971] developed the first pseudo-linear time algorithm\nfor integer multiplication with cost O(n log n loglog n); this is also the cost of\nthe fastest known algorithm for polynomial multiplication [Cantor and Kaltofen,\n1991].\nIn practice, all of these algorithms will be used in certain ranges, and so we\nemploy the usual notation of a multiplication time function M(n), the cost of\nmultiplying two dense polynomials with degrees both less than n. Also define\n\u03b4(n) = M(n)/n. If f, g \u2208 R[x] with different degrees deg f < n, deg g < m, and\nn > m, by splitting f into dn/me size-m blocks we can compute the product\nn\nM(m)), or O(n * \u03b4(m)).\nf * g with cost O( m\nFor the multiplication of two sparse polynomials as in (1.2), the school\nmethod uses O(t2 ) ring operations, which cannot be improved in the worst\ncase. However, since the degrees could be very large, the cost of exponent\narithmetic becomes significant. The school method uses O(t3 log n) word operations and O(t2 ) space. Yan [1998] reduces the number of word operations\nto O(t2 log t log n) with the \"geobuckets\" data structure. Finally, recent work\nby Monagan and Pearce [2007], following Johnson [1974], gets this same time\ncomplexity but reduces the space requirement to O(t+r), where r is the number\nof nonzero terms in the product.\nThe algorithms we present are for univariate polynomials. They can also be\nused for multivariate polynomial multiplication by using Kronecker substitution:\nGiven two n-variate polynomials f, g \u2208 R[x1 , . . . , xn ] with max degrees less than\ni\u22121\nd, substitute xi = y (2d)\nfor 1 \u2264 i \u2264 n, multiply the univariate polynomials\nover R[y], then convert back. Many other representations exist for multivariate\n\n2\n\n\fpolynomials [see Fateman, 2002], but we will not compare with them or consider\nthem further.\n\n1.2\n\nOverview of Approach\n\nThe performance of an adaptive algorithm depends not only on the size of the\ninput but also on some inherent difficulty measure. Such algorithms match\nstandard approaches in their worst-case performance, but perform far better\non many instances. This idea was first applied to sorting algorithms and has\nproved useful both in theory and in practice [see Petersson and Moffat, 1995].\nSuch techniques have also proven useful in symbolic computation, for example\nthe early termination strategy of Kaltofen and Lee [2003].\nHybrid algorithms combine multiple different approaches to the same problem to effectively handle more cases [e.g. Duran et al., 2003]. Our algorithms\nare also hybrid in the sense that they provide a smooth gradient between existing sparse and dense multiplication algorithms. The adaptive nature of the\nalgorithms means that in fact they will be faster than existing algorithms in\nmany cases, while never being (asymptotically) slower.\nThe algorithms we present will always proceed in three stages. First, the\npolynomials are read in and converted to a different representation which effectively captures the relevant measure of difficulty. Second, we multiply the two\npolynomials in the alternate representation. Finally, the product is converted\nback to the original representation.\nThe computational cost of the second step (where the multiplication is actually performed) depends on the difficulty of the particular instance. Therefore\nthis step should be the dominating cost of the entire algorithm, and in particular the cost of the conversion steps must be linear in the size of the input\npolynomials.\nIn Section 2, we give the first idea for adaptive multiplication, which is to\nwrite a polynomial as a list of dense \"chunks\". The second idea, presented in\nSection 3, is to write a polynomial with \"equal spacing\" between coefficients\nas a dense polynomial composed with a power of the indeterminate. Section 4\nshows how to combine these two ideas to make one algorithm which effectively\ncaptures both difficulty measures. Finally, a few conclusions and ideas for future\ndirections are discussed in Section 5.\nPreliminary progress on some of these results was presented at the Milestones\nin Computer Algebra (MICA) conference held in Tobago in May 2008 [Roche,\n2008].\n\n2\n\nChunky Polynomials\n\nThe basic idea of chunky multiplication is a straightforward combination of the\nstandard sparse and dense representations, providing a natural gradient between\nthe two approaches for multiplication. We note that a similar idea was noticed\n\n3\n\n\f(independently) around the same time by Fateman [2008, page 11], although\nthe treatment here is much more extensive.\nFor f \u2208 R[x] of degree n, the chunky representation of f is a sparse polynomial with dense polynomial \"chunks\" as coefficients:\nf = f1 xe1 + f2 xe2 + * * * + ft xet ,\n\n(2.1)\n\nwith fi \u2208 R[x] and ei \u2208 N for each 1 \u2264 i \u2264 t. We require only that ei+1 >\nei + deg fi for 1 \u2264 i \u2264 t \u2212 1, and each fi has nonzero constant coefficient.\nRecall the notation introduced above of \u03b4(n) = M(n)/n. A unique feature\nof our approach is that we will actually use this function to tune the algorithm.\nThat is, we assume a subroutine is given to evaluate \u03b4(n) for any chosen value\nn.\nIf n is a word-sized integer, then the computation of \u03b4(n) must use a constant\nnumber of word operations. If n is more than word-sized, then we are asking\nabout the cost of multiplying two dense polynomials that cannot fit in memory,\nso the subroutine should return \u221e in such cases. Practically speaking, the \u03b4(n)\nevaluation will usually be an approximation of the actual value, but for what\nfollows we assume the computed value is always exactly correct.\nFurthermore, we require \u03b4(n) to be an increasing function which grows more\nslowly than linearly, meaning that for any a, b, d \u2208 N with a < b,\n\u03b4(a + d) \u2212 \u03b4(a) \u2265 \u03b4(b + d) \u2212 \u03b4(b).\n\n(2.2)\n\nThese conditions are clearly satisfied for all the dense multiplication algorithms\nand corresponding M(n) functions discussed above, including the piecewise function used in practice.\nThe conversion of a sparse or dense polynomial to the chunky representation\nproceeds in two stages: first, we compute an \"optimal chunk size\" k, and then we\nuse this computed value as a parameter in the actual conversion algorithm. The\nproduct of the two polynomials is then computed in the chunky representation,\nand finally the result is converted back to the original representation. The steps\nare presented in reverse order in the hope that the goals at each stage are more\nclear.\n\n2.1\n\nMultiplication in the chunky representation\n\nMultiplying polynomials in the chunky representation uses sparse multiplication\non the outer loop, treating each dense polynomial chunk as a coefficient, and\ndense multiplication to find each product of two chunks.\nFor f, g \u2208 R[x] to be multiplied, write f as in (2.1) and g as\ng = g1 xd1 + g2 xd2 + * * * + gs xds ,\n\n(2.3)\n\nwith s \u2208 N and similar conditions on each gi \u2208 R[x] and di \u2208 N as in (2.1).\nWithout loss of generality, assume also that t \u2265 s, that is, f has more chunks\nthan g. To multiply f and g, we need to compute each product fi gj for 1 \u2264 i \u2264 t\n4\n\n\fand 1 \u2264 j \u2264 s and put the resulting chunks into sorted order. It is likely that\nsome of the chunk products will overlap, and hence some coefficients will also\nneed to be summed.\nBy using heaps of pointers as in Monagan and Pearce [2007], the chunks of\nthe result are computed in order, eliminating unnecessary additions and using\nlittle extra space. A min-heap of size s is filled with pairs (i, j), for i, j \u2208 N, and\nordered by the corresponding sum of exponents ei + dj . Each time we compute\na new chunk product fi * gj , we check the new exponent against the degree of\nthe previous chunk, in order to determine whether to make a new chunk in the\nproduct or add to the previous one. The details of this approach are given in\nAlgorithm 1.\nAlgorithm 1: Chunky Multiplication\nInput: f, g as in (2.1) and (2.3)\nOutput: The product f * g = h in the chunky representation\n1\n2\n3\n\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\u03b1 \u2190 f1 * g1 using dense multiplication\nb \u2190 e1 + d1\nH \u2190 min-heap with pairs (1, j) for j = 2, 3, . . . , s, ordered by exponent\nsums\nif i \u2265 2 then insert (2, 1) into H\nwhile H is not empty do\n(i, j) \u2190 pair from top of H\n\u03b2 \u2190 fi * gj using dense multiplication\nif b + deg \u03b1 < ei + dj then\nwrite \u03b1xb as next term of h\n\u03b1 \u2190 \u03b2; b \u2190 ei + dj\nelse \u03b1 \u2190 \u03b1 + \u03b2xei +dj \u2212b stored as a dense polynomial\nif i < t then insert (i + 1, j) into H\nwrite \u03b1xb as final term of h\n\nAfter using this algorithm to multiply f and g, we can easily convert the\nresult back to the dense or sparse representation in linear time. In fact, if the\noutput is dense, we can preallocate space for the result and store the computed\nproduct directly in the dense array, requiring only some extra space for the heap\nH and a single intermediate product hnew .\nTheorem 2.1. Algorithm 1 correctly computes the product of f and g using\n\u0012\n\u0013\nX\nX\nO\n(deg fi ) * \u03b4(deg gj ) +\n(deg gj ) * \u03b4(deg fi )\ndeg fi \u2265deg gj\n1\u2264i\u2264t, 1\u2264j\u2264s\n\ndeg fi <deg gj\n1\u2264i\u2264t, 1\u2264j\u2264s\n\nring operations and O(ts * log s * log(deg f g)) word operations.\n\n5\n\n\fProof. Correctness is clear from the definitions. The bound on ring operations\ncomes from Step 7 using the fact that \u03b4(n) = M(n)/n. The cost of additions on\nStep 11 is linear and hence also within the stated bound.\nThe cost of word operations is incurred in removing from and inserting to\nthe heap on Steps 6 and 12. Because these steps are executed no more than\ntf tg times, the size of the heap is never more than tg , and each exponent sum\nis bounded by the degree of the product, the stated bound is correct.\nNotice that the cost of word operations is always less than the cost would be\nif we had multiplied f and g in the standard sparse representation. We therefore\nfocus only on minimizing the number of ring operations in the conversion steps\nthat follow.\n\n2.2\n\nConversion given optimal chunk size\n\nThe general chunky conversion problem is, given f, g \u2208 R[x], both either in the\nsparse or dense representation, to determine chunky representations for f and\ng which minimize the cost of Algorithm 1. Here we consider a simpler problem,\nnamely determining an optimal chunky representation for f given that g has\nonly one chunk of size k.\nThe following corollary comes directly from Theorem 2.1 and will guide our\nconversion algorithm on this step.\nCorollary 2.2. Given f \u2208 R[x] as in (2.1), the number of ring operations\nrequired to multiply f by a single dense polynomial with degree less than k is\n\u0012\n\u0013\nX\nX\nO \u03b4(k)\ndeg fi + k\n\u03b4(deg fi )\ndeg fi \u2265k\n\ndeg fi <k\n\nFor any high-degree chunk (i.e. deg fi \u2265 k), we see that there is no benefit\nto making the chunk any larger, as the cost is proportional to the sum of the\ndegrees of these chunks. In order to minimize the cost of multiplication, then,\nwe should not have any chunks with degree greater than k (except possibly in\nthe\nP case that every coefficient of the chunk is nonzero), and we should minimize\n\u03b4(deg fi ) for all chunks with size less than k.\nThese observations form the basis of our approach in Algorithm 2 below.\nFor an input polynomial f \u2208 R[x], each \"gap\" of consecutive zero coefficients\nin f is examined, in order. We determine the optimal chunky conversion if the\npolynomial were truncated at that gap. This is accomplished by finding the\nprevious gap of highest degree that should be included in the optimal chunky\nrepresentation. We already have the conversion for the polynomial up to that\ngap (from a previous step), so we simply add on the last chunk and we are done.\nAt the end, after all gaps have been examined, we have the optimal conversion\nfor the entire polynomial.\nLet ai , bi \u2208 Z for 0 \u2264 i \u2264 m be the sizes of each consecutive \"gap\" of zero\ncoefficients and \"block\" of nonzero coefficients, in order. Each ai and bi will\n\n6\n\n\fbe\nP nonzero except possibly for a0 (if f has a nonzero constant coefficient), and\n0\u2264i\u2264m (ai + bi ) = deg f + 1. For example, the polynomial\nf = 5x10 + 3x11 + 9x13 + 20x19 + 4x20 + 8x21\nhas a0 = 10, b0 = 2, a1 = 1, b1 = 1, a2 = 5, and b2 = 3. Also define\nP di to be the\ndegree of the polynomial up to (not including) gap i, i.e. di = 0\u2264j<i (aj + bj ).\nFor the gap at index `, for 1 \u2264 ` \u2264 m, we store the optimal chunky conversion\nof f mod xd` by a linked list of indices of all gaps in f that should also be gaps\nbetween chunks in the optimal chunky representation. In c` we also store 1/k\ntimes the cost, in ring operations, of multiplying f mod xd` (in this optimal\nrepresentation) by a single chunk of size k.\nWhen examining the gap at index `, in order to determine the previous\ngap of highest degree to be included in the optimal chunky representation if\nthe polynomial were truncated at gap j, we need to find the index i < ` that\nminimizes ci + \u03b4(d` \u2212 di ) (indices i where d` \u2212 di > k need not be considered, as\ndiscussed above). From (2.2), we know that, if 1 \u2264 i < j < ` and ci +\u03b4(d` \u2212di ) <\ncj + \u03b4(d` \u2212 dj ), then this same inequality continues to hold as ` increases. That\nis, as soon as an earlier gap results in a smaller cost than a later one, that earlier\ngap will continue to beat the later one.\nThus we can essentially precompute the values of mini<` (ci + \u03b4(d` \u2212 di )) by\nmaintaining a stack of index-index pairs. A pair (i, j) of indices indicates that\nci + \u03b4(d` \u2212 di ) is minimal as long as ` \u2264 j. The second pair of indices indicates\nthe minimal value from gap j to the gap of the second index of the second pair,\nand so forth up to the bottom of the stack and the last gap.\nThe details of this rather complicated algorithm are given in Algorithm 2.\nFor an informal justification of correctness, consider a single iteration through\nthe main for loop. At this point, we have computed all optimal costs c1 , c2 , . . . , c`\u22121 ,\nand the lists of gaps to achieve those costs L1 , L2 , . . . , L`\u22121 . We also have computed the stack S, indicating which of the gaps up to index ` \u2212 2 is optimal and\nwhen.\nThe while loop on Step 3 removes all gaps from the stack which are no\nlonger relevant, either because their cost is now beaten by a previous gap (when\nj < `), or because the size of the resulting chunk would be greater than k and\ntherefore unnecessary to consider.\nIf the condition of Step 5 is true, then there is no index at which gap (` \u2212 1)\nshould be used, so we discard it.\nOtherwise, the gap at index ` \u2212 1 is good at least some of the time, so we\nproceed to the task of determining the largest gap index v at which gap (` \u2212 1)\nmight still be useful. First, in Steps 10\u201312, we repeatedly check whether gap\n(` \u2212 1) always beats the gap at the top of the stack S, and if so remove it. After\nthis process, either no gaps remain on the stack, or we have a range r \u2264 v \u2264 j\nin which binary search can be performed to determine v.\nFrom the definitions, dm+1 = deg f +1, and so the list of gaps Lm+1 returned\non the final step gives the optimal list of gaps to include in f mod xdeg f +1 , which\nis of course just f itself.\n7\n\n\fAlgorithm 2: Chunky Conversion Algorithm\nInput: k \u2208 N, f \u2208 R[x], and integers ai , bi , di for i = 0, 1, 2, . . . , m as\nabove\nOutput: A list L of the indices of gaps to include in the optimal chunky\nrepresentation of f when multiplying by a single chunk of size k\n1 L1 \u2190 0;\nc1 \u2190 \u03b4(b0 ); S \u2190 (0, m + 1)\n2 for ` = 2, 3, . . . , m + 1 do\n3\nwhile top pair (i, j) from S satisfies j < ` or d` \u2212 di > k do\n4\nRemove (i, j) from S\n5\n\n6\n7\n8\n9\n10\n\n11\n12\n13\n14\n15\n16\n17\n\n18\n19\n20\n\nif top pair (i, j) from S satisfies ci + \u03b4(d` \u2212 di ) \u2264 c`\u22121 + \u03b4(d` \u2212 d`\u22121 )\nthen\nL` \u2190 L`\u22121\nelse\nL` \u2190 (` \u2212 1), L`\u22121\nr\u2190`\nwhile top pair (i, j) from S satisfies\nci + \u03b4(dj \u2212 di ) > c`\u22121 + \u03b4(dj \u2212 d`\u22121 ) do\nr\u2190j\nRemove (i, j) from S\nif S is empty then\nS \u2190 (` \u2212 1, m + 1)\nelse\n(i, j) \u2190 top pair from S\nv \u2190 least index with r \u2264 v < j s.t.\nc`\u22121 + \u03b4(dv \u2212 d`\u22121 ) > ci + \u03b4(dv \u2212 di )\nS \u2190 (` \u2212 1, v), S\nc` \u2190 ci + \u03b4(d` \u2212 di )\n\n(where (i, j) is top pair from S)\n\nreturn Lm+1\n\nTheorem 2.3. Algorithm 2 returns the optimal chunky representation for multiplying f by a dense size-k chunk. The running time of the algorithm is linear\nin the size of the input representation of f .\nProof. Correctness follows from the discussions above.\nFor the complexity analysis, first note that the maximal size of S, as well\nas the number of saved values ai , bi , di , si , Li , is m, the number of gaps in f .\nClearly m is less than the number of nonzero terms in f , so this is bounded above\nby the sparse or dense representation size. If the lists Li are implemented as\nsingly-linked lists, sharing nodes, then the total extra storage for the algorithm\nis O(m).\nThe total number of iterations of the two while loops corresponds to the\nnumber of gaps that are removed from the stack S at any step. Since at most\n\n8\n\n\fone gap is pushed onto S at each step, the total number of removals, and hence\nthe total cost of these while loops over all iterations, is O(m).\nNow consider the cost of Step 17 at each iteration. If the input is given\nin the sparse representation, we just perform a binary search on the interval\nfrom r to j, for a total cost of O(m log m) over all iterations. Because m is at\nmost the number of nonzero terms in f , m log m is bounded above by the sparse\nrepresentation size, so the theorem is satisfied for sparse input.\nWhen the input is given in the dense representation, we also use a binary\nsearch for Step 17, but we start with a one-sided binary search, or \"galloping\"\nsearch, from either r or j, depending on which v is closer to. The cost of this\nsearch is at a single iteration is O(log min{v \u2212r, i2 \u2212v}). Notice that the interval\n(r, j) in the stack is then effectively split at the index v, so intuitively whenever\nmore work is required through one iteration of this step, the size of intervals is\nreduced, so future iterations should have lower cost.\nMore precisely,\nPu a loose upper bound in the worst case of the total cost over all\niterations is O( i=1 2i * (u \u2212 i + 1)), where u = dlog2 me. This is less than 2u+2 ,\nwhich is O(m), giving linear cost in the size of the dense representation.\n\n2.3\n\nDetermining the optimal chunk size\n\nAll that remains is to compute the optimal chunk size k that will be used in\nthe conversion algorithm from the previous section. This is accomplished by\nfinding the value of k that minimizes the cost of multiplying two polynomials\nf, g \u2208 R[x], under the restriction that every chunk of f and of g has size k.\nIf f is written in the chunky representation as in (2.1), there are many\npossible choices for the number of chunks t, depending on how large the chunks\nare. So define t(k) to be the least number of chunks if each chunk has size at\nmost k, i.e. deg fi < k for 1 \u2264 i \u2264 t(k). Similarly define s(k) for g \u2208 R[x]\nwritten as in (2.3).\nTherefore, from the cost of multiplication in Theorem 2.1, in this part we\nwant to compute the value of k that minimizes\nt(k) * s(k) * k * \u03b4(k).\n\n(2.4)\n\nSay deg f = n. After O(n) preprocessing work (making pointers to the beginning and end of each \"gap\"), t(k) could be computed using O(n/k) word\noperations, for any value k. This leads to one possible approach to computing\nthe value of k that minimizes (2.4) above: simply compute (2.4) for each possible k = 1, 2, . . . , max{deg f, deg g}. This na\u0131\u0308ve approach is too costly for our\npurposes, but underlies the basic idea of our algorithm.\nRather than explicitly computing each t(k) and s(k), we essentially maintain\nchunky representations of f and g with all chunks having size less than k,\nstarting with k = 1. As k increases, we count the number of chunks in each\nrepresentation, which gives a tight approximation to the actual values of t(k)\nand f (k), while achieving linear complexity in the size of either the sparse or\ndense representation.\n\n9\n\n\fTo facilitate the \"update\" step, a minimum priority queue Q (whose specific\nimplementation depends on the input polynomial representation) is maintained\ncontaining all gaps in the current chunky representations of f and g. For each\ngap, the key value (on which the priority queue is ordered) is the size of the\nchunk that would result from merging the two chunks adjacent to the gap into\na single chunk.\nSo for example, if we write f in the chunky representation as\nf = (4 + 0x + 5x2 ) * x12 + (7 + 6x + 0x2 + 0x3 + 8x4 ) * x50 ,\nthen the single gap in f will have key value 3 + 35 + 5 = 43, More precisely, if\nf is written as in (2.1), then the ith gap has key value\ndeg fi+1 + ei+1 \u2212 ei . + 1\n\n(2.5)\n\nEach gap in the priority queue also contains pointers to the two (or fewer)\nneighboring gaps in the current chunky representation. Removing a gap from\nthe queue corresponds to merging the two chunks adjacent to that gap, so we\nwill need to update (by increasing) the key values of any neighboring gaps\naccordingly.\nAt each iteration through the main loop in the algorithm, the smallest key\nvalue in the priority queue is examined, and k is increased to this value. Then\ngaps with key value k are repeatedly removed from the queue until no more\nremain. This means that each remaining gap, if removed, would result in a chunk\nof size strictly greater than k. Finally, we compute \u03b4(k) and an approximation\nof (2.4).\nSince the purpose here is only to compute an optimal chunk size k, and\nnot actually to compute chunky representations of f and g, we do not have to\nmaintain chunky representations of the polynomials as the algorithm proceeds,\nbut merely counters for the number of chunks in each one. Algorithm 3 gives\nthe details of this computation.\nAll that remains is the specification of the data structures used to implement\nthe priority queues Qf and Qg . If the input polynomials are in the sparse\nrepresentation, we simply use standard binary heaps, which give logarithmic\ncost for each removal and update. Because the exponents in this case are multiprecision integers, we might imagine encountering chunk sizes that are larger\nthan the largest word-sized integer. But as discussed previously, such a chunk\nsize would be meaningless since a dense polynomial with that size cannot be\nrepresented in memory. So our priority queues may discard any gaps whose\nkey value is larger than word-sized. This guarantees all keys in the queues are\nword-size integers, which is necessary for the complexity analysis later.\nIf the input polynomials are dense, we need a structure which can perform\nremovals and updates in constant time, using O(deg f + deg g) time and space.\nFor Qf , we use an array with length deg f of (possibly empty) linked lists, where\nthe list at index i in the array contains all elements in the queue with key i.\n(An array of this length is sufficient because each key value in Qf is at least 2\n\n10\n\n\fAlgorithm 3: Optimal Chunk Size Computation\nInput: f, g \u2208 R[x]\nOutput: k \u2208 N that minimizes t(k) * s(k) * k * \u03b4(k)\n1 Qf , Qg \u2190 minimum priority queues initialized with all gaps in f and g,\nrespectively\n2 k, kmin \u2190 1;\ncmin \u2190 tf tg\n3 while Qf and Qg are not both empty do\n4\nk \u2190 smallest key value from Qf or Qg\n5\nwhile Qf has an element with key value \u2264 k do\n6\nRemove a k-valued gap from Qf and update neighbors\n7\n8\n9\n10\n11\n12\n\nwhile Qg has an element with key value \u2264 k do\nRemove a k-valued gap from Qg and update neighbors\nccurrent \u2190 (|Qf | + 1) * (|Qg | + 1) * k * \u03b4(k)\nif ccurrent < cmin then\nkmin \u2190 k; cmin \u2190 ccurrent\nreturn kmin\n\nand at most 1 + deg f .) We use the same data structure for Qg , and this clearly\ngives constant time for each remove and update operation.\nTo find the smallest key value in either queue at each iteration through\nStep 4, we simply start at the beginning of the array and search forward in each\nposition until a non-empty list is found. Because each queue element update\nonly results in the key values increasing, we can start the search at each iteration\nat the point where the previous search ended. Hence the total cost of Step 4 for\nall iterations is O(deg f + deg g).\nThe following lemma proves that our approximations of t(k) and s(k) are\nreasonably tight, and will be crucial in proving the correctness of the algorithm.\nLemma 2.4. At any iteration through Step 10 in Algorithm 3, |Qf | < 2t(k)\nand |Qg | < 2s(k).\nProof. First consider f . There are two chunky representations with each chunk\nof degree less than k to consider: the optimal having t(k) chunks and the one\nimplicitly computed by Algorithm 3 with |Qf | + 1 chunks. Call these f \u0304 and f\u02c6,\nrespectively.\nWe claim that any single chunk of the optimal f \u0304 contains at most three\nconstant terms of chunks in the implicitly-computed f\u02c6. If this were not so, then\ntwo chunks in f\u02c6 could be combined to result in a single chunk with degree less\nthan k. But this is impossible, since all such pairs of chunks would already have\nbeen merged after the completion of Step 5.\nTherefore every chunk in f \u0304 contains at most two constant terms of distinct\nchunks in f\u02c6. Since each constant term of a chunk is required to be nonzero, the\nnumber of chunks in f\u02c6 is at most twice the number in f \u0304. Hence |Qf |+1 \u2264 2t(k).\nAn identical argument for g gives the stated result.\n11\n\n\fNow we are ready for the main result of this subsection.\nTheorem 2.5. Algorithm 3 computes a chunk size k such that t(k)*s(k)*k *\u03b4(k)\nis at most 4 times the minimum value. The worst-case cost of the algorithm is\nlinear in the size of the input representations.\nProof. If k is the value returned from the algorithm and k \u2217 is the value which\nactually minimizes (2.4), the worst that can happen is that the algorithm\ncomputes the actual value of cf (k) cg (k) k \u03b4(k), but overestimates the value of\ncf (k \u2217 ) cg (k \u2217 ) k \u2217 \u03b4(k \u2217 ). This overestimation can only occur in cf (k \u2217 ) and cg (k \u2217 ),\nand each of those by only a factor of 2 from Lemma 2.4. So the first statement\nof the theorem holds.\nWrite c for the total number of nonzero terms in f and g. The initial sizes\nof the queues Qf and Qg is O(c). Since gaps are only removed from the queues\n(after they are initialized), the total cost of all queue operations is bounded\nabove by O(c), which in turn is bounded above by the sparse and dense sizes of\nthe input polynomials.\nIf the input is sparse and we use a binary heap, the cost of each queue\noperation is O(log c), for a total cost of O(c log c), which is a lower bound on\nthe size of the sparse representations. If the input is in the dense representation,\nthen each queue operation has constant cost. Since c \u2208 O(deg f + deg g), the\ntotal cost linear in the size of the dense representation.\n\n2.4\n\nChunky Multiplication Overview\n\nNow we are ready to examine the whole process of chunky polynomial conversion\nand multiplication. First we need the following easy corollary of Theorem 2.3.\nCorollary 2.6. Let f \u2208 R[x], k \u2208 N, and f\u02c6 be any chunky representation of\nf where all chunks have degree at least k, and f \u0304 be the representation returned\nby Algorithm 2 on input k. The cost of multiplying f \u0304 by a single chunk of size\n` < k is then less than the cost of multiplying f\u02c6 by the same chunk.\nProof. Consider the result of Algorithm 2 on input `. We know from Theorem 2.3 that this gives the optimal chunky representation for multiplication of\nf with a size-` chunk. But the only difference in the algorithm on input ` and\ninput k is that more pairs are removed at each iteration on Step 3 on input `.\nThis means that every gap included in the representation f \u0304 is also included\nin the optimal representation. We also know that all chunks in f \u0304 have degree less\nthan k, so that f\u02c6 must have fewer gaps that are in the optimal representation\nthan f \u0304. It follows that multiplication of a size-` chunk by f \u0304 is more efficient\nthan multiplication by f\u02c6.\nTo review, the entire process to multiply f, g \u2208 R[x] using the chunky representation is as follows:\n1. Compute k from Algorithm 3.\n\n12\n\n\f2. Compute chunky representations of f and g using Algorithm 2 with input\nk.\n3. Multiply the two chunky representations using Algorithm 1.\n4. Convert the chunky result back to the original representation.\nBecause each step is optimal (or within a constant bound of the optimal),\nwe expect this approach to yield the most efficient chunky multiplication of f\nand g. In any case, we know it will be at least as efficient as the standard sparse\nor dense algorithm.\nTheorem 2.7. Computing the product of f, g \u2208 R[x] never uses more ring\noperations than either the standard sparse or dense polynomial multiplication\nalgorithms.\nProof. In Algorithm 3, the values of t(k) * s(k) * k * \u03b4(k) for k = 1 and k =\nmin{deg f, deg g} correspond to the costs of the standard sparse and dense algorithms, respectively. Furthermore, it is easy to see that these values are never\noverestimated, meaning that the k returned from the algorithm which minimizes\nthis formula gives a cost which is not greater than the cost of either standard\nalgorithm.\nNow call f\u02c6 and \u011d the implicit representations from Algorithm 3, and f \u0304 and\n\u1e21 the representations returned from Algorithm 2 on input k. We know that the\nmultiplication of f\u02c6 by \u011d is more efficient than either standard algorithm from\nabove. Since every chunk in \u011d has size k, multiplying f \u0304 by \u011d will have an even\nlower cost, from Theorem 2.3. Finally, since every chunk in f \u0304 has size at most\nk, Corollary 2.6 tells us that the cost is further reduced by multiplying f \u0304 by \u1e21.\nThe proof is complete from the fact that conversion back to either original\nrepresentation takes linear time in the size of the output.\n\n3\n\nEqual-Spaced Polynomials\n\nNext we consider an adaptive representation which is in some sense orthogonal to the chunky representation. This representation will be useful when the\ncoefficients of the polynomial are not grouped together into dense chunks, but\nrather when they are spaced evenly apart.\nLet f \u2208 R[x] with degree n, and suppose the exponents of f are all divisible\nby some integer k. Then we can write f = a0 + a1 xk + a2 x2k + * * * . So by letting\nfD = a0 + a1 x + a2 x2 + * * * , we have f = fD \u25e6 xk (where the symbol \u25e6 indicates\nfunctional composition).\nOne motivating example suggested by Michael Monagan is that of homogeneous polynomials. Recall that a multivariate polynomial h \u2208 R[x1 , . . . , xn ] is\nhomogeneous of degree d if every nonzero term of h has total degree d. It is\nwell-known that the number of variables in a homogeneous polynomial can be\neffectively reduced by one by writing yi = xi /xn for 1 \u2264 i < n and h = xnd * \u0125,\n\n13\n\n\ffor \u0125 \u2208 R[y1 , . . . , yn\u22121 ] an (n \u2212 1)-variate polynomial with max-degree d. This\nleads to efficient schemes for homogeneous polynomial arithmetic.\nBut this is only possible if (1) the user realizes this structure in their polynomials, and (2) every polynomial used is homogeneous. Otherwise, a more generic\napproach will be used, such as the Kronecker substitution mentioned in the in2\nn\u22121\ntroduction. Choosing some integer ` > d, we evaluate h(y, y ` , y ` , . . . , y ` ),\nand then perform univariate arithmetic over R[y]. But if h is homogeneous,\na special structure arises: every exponent of y is of the form d + i(` \u2212 1) for\nn\u22121\nsome integer i \u2265 0. Therefore we can write h(y, . . . , y ` ) = (h\u0304 \u25e6 y `\u22121 ) * y d , for\nsome h\u0304 \u2208 R[y] with much smaller degree. The algorithms presented in this section will automatically recognize this structure and perform the corresponding\noptimization to arithmetic.\nThe key idea is equal-spaced representation, which corresponds to writing a\npolynomial f \u2208 R[x] as\nf = (fD \u25e6 xk ) * xd + fS ,\n(3.1)\nwith k, d \u2208 N, fD \u2208 R[x] dense with degree less than n/k \u2212 d, and fS \u2208 R[x]\nsparse with degree less than n. The polynomial fS is a \"noise\" polynomial which\ncontains the comparatively few terms in f whose exponents are not of the form\nik + d for some i \u2265 0.\nUnfortunately, converting a sparse polynomial to the best equal-spaced representation seems to be difficult. To see why this is the case, consider the\nmuch simpler problem of verifying that a sparse polynomial f can be written\nas (fD \u25e6 xk ) * xd . For each exponent ei of a nonzero term in f , this means confirming\nP that ei \u2261 d mod k. But the cost of computing each ei mod k is roughly\nO( (log ei )\u03b4(log k)), which is a factor of \u03b4(log k) greater than the size of the\ninput. Since k could be as large as the exponents, we see that even verifying a\nproposed k and d takes too much time for the conversion step. Surely computing\nsuch a k and d would be even more costly!\nTherefore, for this subsection, we will always assume that the input polynomials are given in the dense representation. In Section 4, we will see how by\ncombining with the chunky representation, we effectively handle equal-spaced\nsparse polynomials without ever having to convert a sparse polynomial directly\nto the equal-spaced representation.\n\n3.1\n\nMultiplication in the equal-spaced representation\n\nLet g \u2208 R[x] with degree less than m and write g = (gD \u25e6 x` ) * xe + gS as in\n(3.1). To compute f * g, simply sum up the four pairwise products of terms. All\nthese except for the product (fD \u25e6 xk ) * (gD \u25e6 x` ) are performed using standard\nsparse multiplication methods.\nNotice that if k = `, then (fD \u25e6 xk ) * (gD \u25e6 x` ) is simply (fD * gD ) \u25e6 xk , and\nhence is efficiently computed using dense multiplication. However, if k and `\nare relatively prime, then almost any term in the product can be nonzero.\nThis indicates that the gcd of k and ` is very significant. Write r and s for\nthe greatest common divisor and least common multiple of k and `, respectively.\n\n14\n\n\fTo multiply (fD \u25e6 xk ) by (gD \u25e6 x` ), we perform a transformation similar to the\nprocess of finding common denominators in the addition of fractions. First split\nfD \u25e6 xk into s/k (or `/r) polynomials, each with degree less than n/s and right\ncomposition factor xs , as follows:\nfD \u25e6 xk = (f0 \u25e6 xs ) + (f1 \u25e6 xs ) * xk + (f2 \u25e6 xs ) * x2k * * * + (fs/k\u22121 \u25e6 xs ) * xs\u2212k\nSimilarly split gD \u25e6x` into s/` polynomials g0 , g1 , . . . , gs/`\u22121 with degrees less\nthan m/s and right composition factor xs . Then compute all pairwise products\nfi * gj , and combine them appropriately to compute the total sum (which will\nbe equal-spaced with right composition factor xr ).\nAlgorithm 4 gives the details of this method.\nAlgorithm 4: Equal Spaced Multiplication\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\nInput: f = (fD \u25e6 xk ) * xd + fS , g = (gD \u25e6 x` ) * xe + gS ,\nwith fD = a0 + a1 x + a2 x2 + * * * , gD = b0 + b1 x + b2 x2 + * * *\nOutput: The product f * g\nr \u2190 gcd(k, `), s \u2190 lcm(k, `)\nfor i = 0, 1, . . . , s/k \u2212 1 do\nfi \u2190 ai + as+i x + a2s+i x2 + * * *\nfor i = 0, 1, . . . , s/` \u2212 1 do\ngi \u2190 bi + bs+i x + b2s+i x2 + * * *\nhD \u2190 0\nfor i = 0, 1, . . . , s/k \u2212 1 do\nfor j = 0, 1, . . . , s/` \u2212 1 do\nCompute fi * gj by dense multiplication\nhD \u2190 hD + ((fi * gj ) \u25e6 xs ) * xik+j`\nCompute (fD \u25e6 xk ) * gS , (gD \u25e6 x` ) * fS , and fS * gS by sparse multiplication\nreturn hD * xe+d + (fD \u25e6 xk ) * gS * xd + (gD \u25e6 x` ) * fS * xe + fS * gS\n\nAs with chunky multiplication, this final product is easily converted to the\nstandard dense representation in linear time. The following theorem gives the\ncomplexity analysis for equal-spaced multiplication.\nTheorem 3.1. Let f, g be as above such that n > m, and write tf , tg for the\nnumber of nonzero terms in fS and gS , respectively. Then Algorithm 4 correctly\ncomputes the product f * g using\nO ((n/r) * \u03b4(m/s) + ntg /k + mtf /` + tf tg )\nring operations.\nProof. Correctness follows from the preceding discussion.\nThe polynomials fD and gD have at most n/k and m/` nonzero terms,\nrespectively. So the cost of computing the three products in Step 11 by using\n15\n\n\fstandard sparse multiplication is O(ntg /k+mtf /`+tf tg ) ring operations, giving\nthe last three terms in the complexity measure.\nThe initialization in Steps 2\u20135 and the additions in Steps 10 and 12 all have\ncost bounded by O(n/r), and hence do not dominate the complexity.\nAll that remains is the cost of computing each product fi * gj by dense multiplication on Step 9. From the discussion above, deg fi < n/s and deg gj < m/s,\nfor each i and j. Since n > m, (n/s) > (m/s), and therefore this product can\nbe computed using O((n/s)\u03b4(m/s)) ring operations. The number of iterations\nthrough Step 9 is exactly (s/k)(s/`). But s/` = k/r, so the number of iterations\nis just s/r. Hence the total cost for this step is O((n/r)\u03b4(m/s)), which gives\nthe first term in the complexity measure.\nIt is worth noting that no additions of ring elements are actually performed\nthrough each iteration of Step 10. The proof is as follows. If any additions were\nperformed, we would have\ni1 k + j1 ` \u2261 i2 k + j2 ` mod s\nfor distinct pairs (i1 , j1 ) and (i2 , j2 ). Without loss of generality, assume i1 6= i2 ,\nand write\n(i1 k + j1 `) \u2212 (i2 k + j2 `) = qs\nfor some q \u2208 Z. Rearranging gives\n(i1 \u2212 i2 )k = (j2 \u2212 j1 )` + qs.\nBecause `|s, the left hand side is a multiple of both k and `, and therefore by\ndefinition must be a multiple of s, their lcm. Since 0 \u2264 i1 , i2 < s/k, |i1 \u2212 i2 | <\ns/k, and therefore |(i1 \u2212 i2 )k| < s. The only multiple of s with this property is\nof course 0, and since k 6= 0 this means that i1 = i2 , a contradiction.\nThe following theorem compares the cost of equal-spaced multiplication to\nstandard dense multiplication, and will be used to guide the approach to conversion below.\nTheorem 3.2. Let f, g, m, n, tf , tg be as before. Algorithm 4 does not use\nasymptotically more ring operations than standard dense multiplication to compute the product of f and g as long as tf \u2208 O(\u03b4(n)) and tg \u2208 O(\u03b4(m)).\nProof. Assuming again that n > m, the cost of standard dense multiplication\nis O(n\u03b4(m)) ring operations, which is the same as O(n\u03b4(m) + m\u03b4(n)).\nUsing the previous theorem, the number of ring operations used by Algorithm 4 is\nO ((n/r)\u03b4(m/s) + n\u03b4(m)/k + m\u03b4(n)/` + \u03b4(n)\u03b4(m)) .\nBecause all of k, `, r, s are at least 1, and since \u03b4(n) < n, every term in this\ncomplexity measure is bounded by n\u03b4(m)+m\u03b4(n). The stated result follows.\n\n16\n\n\f3.2\n\nConverting to equal-spaced\n\nThe only question when converting a polynomial f to the equal-spaced representation is how large we should allow tS (the number of nonzero terms in of\nfS ) to be. From Theorem 3.2 above, clearly we need tS \u2208 \u03b4(deg f ), but we can\nsee from the proof of the theorem that having this bound be tight will often\ngive performance that is equal to the standard dense method (not worse, but\nnot better either).\nLet t be the number of nonzero terms in f . Since the goal of any adaptive\nmethod is to in fact be faster than the standard algorithms, we use the lower\nbound of \u03b4(n) \u2208 \u03a9(log n) and t \u2264 deg f + 1 and require that tS < log2 t.\nAs usual, let f \u2208 R[x] with degree less than n and write\nf = a1 xe1 + a2 xe2 + * * * + at xet ,\nwith each ai \u2208 R \\ {0}. The reader will recall that this corresponds to the sparse\nrepresentation of f , but keep in mind that we are assuming f is given in the\ndense representation; f is written this way only for notational convenience.\nThe conversion problem is then to find the largest possible value of k such\nthat all but at most log2 t of the exponents ej can be written as ki + d, for any\nnonnegative integer i and a fixed integer d. Our approach to computing k and\nd will be simply to check each possible value of k, in decreasing order. To make\nthis efficient, we need a bound on the size of k.\nLemma 3.3. Let n \u2208 N and e1 , . . . , et be distinct integers in the range [0, n].\nIf at least t \u2212 log2 t of the integers ei are congruent to the same value modulo k,\nfor some k \u2208 N, then\nn\n.\nk\u2264\nt \u2212 2 log2 t \u2212 1\nProof. Without loss of generality, order the ei 's so that 0 \u2264 e1 < e2 < * * * <\net \u2264 n. Now consider the telescoping sum (e2 \u2212 e1 ) + (e3 \u2212 e2 ) + * * * + (et \u2212 et\u22121 ).\nEvery term in the sum is at least 1, and the total is et \u2212 e1 , which is at most n.\nLet S \u2286 {e1 , . . . , et } be the set of at most log2 t integers not congruent to the\nothers modulo k. Then for any ei , ej \u2208\n/ S, ei \u2261 ej mod k. Therefore k|(ej \u2212 ei ).\nIf j > i, this means that ej \u2212 ei \u2265 k.\nReturning to the telescoping sum above, each ej \u2208 S is in at most two of\nthe sum terms ei \u2212 ei\u22121 . So all but at most 2 log2 t of the terms are at least k.\nSince there are exactly t \u2212 1 terms, and the total sum is at most n, we conclude\nthat (t \u2212 2 log2 t \u2212 1) * k \u2264 n. The stated result follows.\nWe now employ this lemma to develop an algorithm to determine the best\nvalues of k and d, given a dense polynomial f . Starting from the largest possible\nvalue from the bound, for each candidate value k, we compute each ei mod k,\nand find the majority element - that is, a common modular image of more\nthan half of the exponents.\nTo compute the majority element, we use a now well-known approach first\ncredited to Boyer and Moore [1981] and Fischer and Salzberg [1982]. Intuitively,\n17\n\n\fpairs of different elements are repeatedly removed until only one element remains. If there is a majority element, this remaining element is it; only one\nextra pass through the elements is required to check whether this is the case.\nIn practice, this is accomplished without actually modifying the list.\nAlgorithm 5: Equal Spaced Conversion\nInput: Exponents e1 , e2 , . . . , et \u2208 N and n \u2208 N such that\n0 \u2264 e1 < e2 < * * * < et = n\nOutput: k, d \u2208 N and S \u2286 {e1 , . . . , et } such that ei \u2261 d mod k for all\nexponents ei not in S, and |S| \u2264 log2 t.\n1 if t < 32 then k \u2190 n\n2 else k \u2190 bn/(t \u2212 1 \u2212 2 log2 t)c\n3 while k \u2265 2 do\n4\nd \u2190 e1 mod k; j \u2190 1\n5\nfor i = 2, 3, . . . , t do\n6\nif ei \u2261 d mod k then j \u2190 j + 1\n7\nelse if j > 0 then j \u2190 j \u2212 1\n8\nelse d \u2190 ei mod k; j \u2190 1\n9\n10\n11\n12\n\nS \u2190 {ei : ei 6\u2261 d mod k}\nif |S| \u2264 log2 t then return k, d, S\nk \u2190k\u22121\nreturn 1, 0, \u2205\n\nGiven k, d, S from the algorithm, in one more pass through the input polynomial, fD and fS are constructed such that f = (fD \u25e6 xk ) * xd + fS . After\nperforming separate conversions for two polynomials f, g \u2208 R[x], they are multiplied using Algorithm 4.\nThe following theorem proves correctness when t > 4. If t \u2264 4, we can always\ntrivially set k = et \u2212 e1 and d = e1 mod k to satisfy the stated conditions.\nTheorem 3.4. Given integers e1 , . . . , et and n, with t > 4, Algorithm 5 computes the largest integer k such that at least t \u2212 log2 t of the integers ei are\ncongruent modulo k, and uses O(n) word operations.\nProof. In a single iteration through the while loop, we compute the majority\nelement of the set {ei mod k : i = 1, 2, . . . , t}, if there is one. Because t > 4,\nlog2 t < t/2. Therefore any element which occurs at least t \u2212 log2 t times in a\nt-element set is a majority element, which proves that any k returned by the\nalgorithm is such that at least t \u2212 log2 t of the integers ei are congruent modulo\nk.\nFrom Lemma 3.3, we know that the initial value of k on Step 1 or 2 is greater\nthan the optimal k value. Since we start at this value and decrement to 1, the\nlargest k satisfying the stated conditions is returned.\nFor the complexity analysis, first consider the cost of a single iteration\nthrough the main while loop. Since each integer ei is word-sized, computing\neach ei mod k has constant cost, and this happens O(t) times in each iteration.\n18\n\n\fIf t < 32, each of the O(n) iterations has constant cost, for total cost O(n).\nOtherwise, we start with k = bn/(t \u2212 1 \u2212 2 log2 t)c and decrement. Because\nt \u2265 32, t/2 > 1+2 log2 t. Therefore (t\u22121\u22122 log2 t) > t/2, so the initial value of k\nis less than 2n/t. This gives an upper bound on the number of iterations through\nthe while loop, and so the total cost is O(n) word operations, as required.\nAlgorithm 5 can be implemented using only O(t) space for the storage of\nthe exponents e1 , . . . , et , which is linear in the size of the output, plus the space\nrequired for the returned set S.\n\n4\n\nChunks with Equal Spacing\n\nThe next question is whether the ideas of chunky and equal-spaced polynomial\nmultiplication can be effectively combined into a single algorithm. As before, we\nseek an adaptive combination of previous algorithms, so that the combination\nis never asymptotically worse than either original idea.\nAn obvious approach would be to first perform chunky polynomial conversion, and then equal-spaced conversion on each of the dense chunks. Unfortunately, this would be asymptotically less efficient than equal-spaced multiplication alone in a family of instances, and therefore is not acceptable as a proper\nadaptive algorithm.\nThe algorithm presented here does in fact perform chunky conversion first,\nbut instead of performing equal-spaced conversion on each dense chunk independently, Algorithm 5 is run simultaneously on all chunks in order to determine,\nfor each polynomial, a single spacing parameter k that will be used for every\nchunk.\nLet f = f1 xe1 + f2 xe2 + * * * + ft xet in the optimal chunky representation for\nmultiplication by another polynomial g. We first compute the smallest bound\non the spacing parameter k for any of the chunks fi , using Lemma 3.3. Starting\nwith this value, we execute the while loop of Algorithm 5 for each polynomial\nfi , stopping at the largest value of k such that the total size of all sets S on\nStep 9 for all chunks fi is at most log2 tf , where tf is the total number of nonzero\nterms in f .\nThe polynomial f can then be rewritten (recycling the variables fi and ei )\nas\nf = (f1 \u25e6 xk ) * xe1 + (f2 \u25e6 xk ) * xe2 + * * * + (ft \u25e6 xk ) * xet + fS ,\nwhere fS is in the sparse representation and has O(log tf ) nonzero terms.\nLet k \u2217 be the value returned from Algorithm 5 on input of the entire polynomial f . Using k \u2217 instead of k, f could still be written as above with fS\nhaving at most log2 tf terms. Therefore the value of k computed in this way is\nalways greater than or equal to k \u2217 if the initial bounds are correct. This will\nbe the case except when every chunk fi has few nonzero terms (and therefore\nt is close to tf ). However, this reduces to the problem of converting a sparse\npolynomial to the equal-spaced representation, which seems to be intractable,\n\n19\n\n\fas discussed above. So our cost analysis will be predicated on the assumption\nthat the computed value of k is never smaller than k \u2217 .\nWe perform the same equal-spaced conversion for g, and then use Algorithm 1 to compute the product f * g, with the difference that each product\nfi * gj is computed by Algorithm 4 rather than standard dense multiplication.\nAs with equal-spaced multiplication, the products involving fS or gS are performed using standard sparse multiplication.\nTheorem 4.1. The algorithm described above to multiply polynomials with\nequal-spaced chunks never uses more ring operations than either chunky or equalspaced multiplication, provided that the computed \"spacing parameters\" k and `\nare not smaller than the values returned from Algorithm 5.\nProof. Let n, m be the degrees of f, g respectively and write tf , tg for the number\nof nonzero terms in f, g respectively. The sparse multiplications involving fS\nand gS use a total of tg log tf + tf log tg + (log tf )(log tg ) ring operations. Both\nthe chunky or equal-spaced multiplication algorithms always require O(tg \u03b4(tf )+\ntf \u03b4(tg )) ring operations in the best case, and since \u03b4(n) \u2208 \u03a9(log n), the cost of\nthese sparse multiplications is never more than the cost of the standard chunky\nor equal-spaced method.\nThe remaining computation is that to compute each product fi * gj using\nequal-spaced multiplication. Write k and ` for the powers of x in the right\ncomposition factors of f and g respectively. Theorem 3.1 tells us that the cost\nof computing each of these products by equal-spaced multiplication is never\nmore than computing them by standard dense multiplication, since k and ` are\nboth at least 1. Therefore the combined approach is never more costly than just\nperforming chunky multiplication.\nTo compare with the cost of equal-spaced multiplication, assume that k and\n` are the actual values returned by Algorithm 5 on input f and g. This is the\nworst case, since we have assumed that k and ` are never smaller than the values\nfrom Algorithm 5.\nNow consider the cost of multiplication by a single equal-spaced chunk of\ng. This is the same as assuming g consists of only one equal-spaced chunk.\nWrite di = deg fi for each equal-spaced chunk of f , and r, s for the gcd and\nlcm of k and `, respectively. If m > n, then of course m is larger P\nthan each\ndi , so multiplication using the combined method will use O((m/r) \u03b4(di /s))\nring operations, compared to O((m/r)\u03b4(n/s)) for the standard equal-spaced\nalgorithm, by Theorem 3.1.\nNow recall the cost equation (2.4) used for Algorithm 3:\ncf (b) * cg (b) * b * \u03b4(b),\nwhere b is the size of all dense chunks in f and g. By definition, cf (n) = 1,\nand cg (n) \u2264 m/n, so we know that cf (n) cg (n) n \u03b4(n) \u2264 m \u03b4(n). Because the\nchunk\nPt sizes di were originally chosen by Algorithm 3, we must therefore have\nm i=1 \u03b4(di ) \u2264 m\u03b4(n). The restriction\nP that the \u03b4 function grows more slowly\nthan linear then implies that (m/r) \u03b4(di /s) \u2208 O((m/r)\u03b4(n/s)), and so the\nstandard equal-spaced algorithm is never more efficient in this case.\n20\n\n\fWhen m \u2264 n, the number of ring operations to compute the product using\nthe combined method, again by Theorem 3.1, is\n\uf8eb\n\uf8f6\nX\nX\nO \uf8ed\u03b4(m/s)\n(di /r) + (m/r)\n\u03b4(di /s)\uf8f8 ,\n(4.1)\ndi \u2265m\n\ndi <m\n\ncompared with O((n/r)\u03b4(m/s))\nfor the standard equal-spaced algorithm. BePt\ncause we always have i=1 di \u2264 n, the first term of (4.1) is O((n/r)\u03b4(m/s)).\nPt\nUsing again the inequality m i=1 \u03b4(di ) \u2264 m\u03b4(n), along with the fact that\nm\u03b4(n) \u2208 O(n\u03b4(m)) because m \u2264 n, we see that the second term of (4.1) is also\nO((n/r)\u03b4(m/s)). Therefore the cost of the combined method is never more than\nthe cost of equal-spaced multiplication alone.\n\n5\n\nConclusions and Future Work\n\nTwo methods for adaptive polynomial multiplication have been given where we\ncan compute optimal representations (under some set of restrictions) in linear\ntime in the size of the input. Combining these two ideas into one algorithm\ninherently captures both measures of difficulty, and will in fact have significantly\nbetter performance than either the chunky or equal-spaced algorithm in many\ncases.\nHowever, converting a sparse polynomial to the equal-spaced representation\nin linear time is still out of reach, and this problem is the source of the restriction\nof Theorem 4.1. Some justification for the impossibility of such a conversion\nalgorithm was given, due to the fact that the exponents could be long integers.\nHowever, we still do not have an algorithm for sparse polynomial to equal-spaced\nconversion under the (probably reasonable) restriction that all exponents be\nword-sized integers. A linear-time algorithm for this problem would be useful\nand would make our adaptive approach more complete, though slightly more\nrestricted in scope.\nSome early results from a trial implementation indicate that the algorithms\nwe present are quite good at computing efficient adaptive representations, even\nin the presence of \"noise\" in the input polynomials, and although the conversion\ndoes sometimes have a measurable cost, it is almost always significantly less\nthan the cost of the actual multiplication. Some of these results were reported\nin [Roche, 2008], giving evidence that our theoretical results hold in practice,\nbut more work on an efficient implementation is still needed.\nYet another area for further development is multivariate polynomials. We\nhave mentioned the usefulness of Kronecker substitution, but developing an\nadaptive algorithm to choose the optimal variable ordering would give significant\nimprovements.\nFinally, even though we have proven that our algorithms produce optimal\nadaptive representations, it is always under some restriction of the way that\nchoice is made (for example, requiring to choose an \"optimal chunk size\" k\nfirst, and then compute optimal conversions given k). These results would be\n21\n\n\fsignificantly strengthened by proving lower bounds over all available adaptive\nrepresentations of a certain type, but such results have thus far been elusive.\n\nAcknowledgements\nThe original ideas for this work were hatched in a graduate seminar taught\nby Alex L\u00f3pez-Ortiz and J\u00e9r\u00e9my Barbay at the University of Waterloo. Many\nthanks are due to the author's Ph.D. supervisors, Mark Giesbrecht and Arne\nStorjohann, for their intellectual and financial support. Thanks also to Richard\nFateman and Michael Monagan for useful and stimulating discussions on this\nwork.\n\nReferences\nR. Boyer and J. Moore. A fast majority vote algorithm. Technical Report\n1981-32, Institute for Computing Science, University of Texas, Austin, 1981.\nDavid G. Cantor and Erich Kaltofen. On fast multiplication of polynomials over\narbitrary algebras. Acta Inform., 28(7):693\u2013701, 1991. ISSN 0001-5903.\nStephen A. Cook. On the Mininum Computation Time of Functions. PhD\nthesis, Harvard University, 1966.\nAhmet Duran, B. David Saunders, and Zhendong Wan. Hybrid algorithms for\nrank of sparse matrices. In Proc. SIAM Conf. on Appl. Linear Algebra, 2003.\nRichard Fateman. Draft: Comparing the speed of programs for sparse polynomial multiplication. Online, http://www.cs.berkeley.edu/~fateman/\npapers/fastmult.pdf, July 2002.\nRichard Fateman. Draft 11: What's it worth to write a short program for polynomial multiplication? Online, http://www.cs.berkeley.edu/~fateman/\npapers/shortprog.pdf, November 2008.\nM. J. Fischer and S. L. Salzberg. Finding a majority among n votes: Solution\nto problem 81-5. J. Algorithms, 3(4):376\u2013379, 1982.\nStephen C. Johnson. Sparse polynomial arithmetic. SIGSAM Bull., 8(3):63\u201371,\n1974. ISSN 0163-5824. doi: http://doi.acm.org/10.1145/1086837.1086847.\nErich Kaltofen and Wen-shin Lee. Early termination in sparse interpolation algorithms. J. Symbolic Comput., 36(3-4):365\u2013400, 2003. ISSN 07477171. International Symposium on Symbolic and Algebraic Computation\n(ISSAC'2002) (Lille).\nA. Karatsuba and Yu. Ofman. Multiplication of multidigit numbers on automata. Dokl. Akad. Nauk SSSR, 7:595\u2013596, 1963.\n\n22\n\n\fMichael B. Monagan and Roman Pearce. Polynomial division using dynamic arrays, heaps, and packed exponent vectors. Lecture Notes in Computer Science,\n4770:295\u2013315, 2007. Computer Algebra in Scientific Computing (CASC'07).\nOla Petersson and Alistair Moffat. A framework for adaptive sorting. Discrete\nAppl. Math., 59(2):153\u2013179, 1995. ISSN 0166-218X.\nDaniel S. Roche. Adaptive polynomial multiplication. In Proc. Milestones in\nComputer Algebra (MICA '08), pages 65\u201372, 2008.\nA. Sch\u00f6nhage and V. Strassen. Schnelle Multiplikation grosser Zahlen. Computing (Arch. Elektron. Rechnen), 7:281\u2013292, 1971.\nA. L. Toom. The complexity of a scheme of functional elements realizing the\nmultiplication of integers. Dokl. Adad. Nauk. SSSR, 150(3):496\u2013498, 1963.\nThomas Yan. The geobucket data structure for polynomials. J. Symbolic Comput., 25(3):285\u2013293, 1998. ISSN 0747-7171.\n\n23\n\n\f"}