{"id": "http://arxiv.org/abs/cs/0510078v1", "guidislink": true, "updated": "2005-10-25T16:48:36Z", "updated_parsed": [2005, 10, 25, 16, 48, 36, 1, 298, 0], "published": "2005-10-25T16:48:36Z", "published_parsed": [2005, 10, 25, 16, 48, 36, 1, 298, 0], "title": "Vector Gaussian Multiple Description with Individual and Central\n  Receivers", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0510063%2Ccs%2F0510013%2Ccs%2F0510054%2Ccs%2F0510046%2Ccs%2F0510057%2Ccs%2F0510083%2Ccs%2F0510095%2Ccs%2F0510006%2Ccs%2F0510004%2Ccs%2F0510051%2Ccs%2F0510024%2Ccs%2F0510071%2Ccs%2F0510040%2Ccs%2F0510068%2Ccs%2F0510035%2Ccs%2F0510001%2Ccs%2F0510003%2Ccs%2F0510048%2Ccs%2F0510084%2Ccs%2F0510075%2Ccs%2F0510012%2Ccs%2F0510078%2Ccs%2F0510092%2Ccs%2F0510029%2Ccs%2F0510030%2Ccs%2F0510047%2Ccs%2F0510011%2Ccs%2F0510090%2Ccs%2F0510009%2Ccs%2F0510086%2Ccs%2F0310054%2Ccs%2F0310028%2Ccs%2F0310008%2Ccs%2F0310039%2Ccs%2F0310012%2Ccs%2F0310055%2Ccs%2F0310023%2Ccs%2F0310032%2Ccs%2F0310026%2Ccs%2F0310007%2Ccs%2F0310019%2Ccs%2F0310001%2Ccs%2F0310037%2Ccs%2F0310017%2Ccs%2F0310048%2Ccs%2F0310058%2Ccs%2F0310022%2Ccs%2F0310043%2Ccs%2F0310049%2Ccs%2F0310047%2Ccs%2F0310059%2Ccs%2F0310027%2Ccs%2F0310009%2Ccs%2F0310034%2Ccs%2F0310042%2Ccs%2F0310046%2Ccs%2F0310016%2Ccs%2F0310003%2Ccs%2F0310053%2Ccs%2F0310020%2Ccs%2F0310040%2Ccs%2F0310052%2Ccs%2F0310065%2Ccs%2F0310064%2Ccs%2F0310060%2Ccs%2F0310004%2Ccs%2F0310015%2Ccs%2F0310011%2Ccs%2F0310005%2Ccs%2F0310030%2Ccs%2F0310010%2Ccs%2F0310029%2Ccs%2F0310021%2Ccs%2F0310014%2Ccs%2F0310038%2Ccs%2F0310035%2Ccs%2F0310033%2Ccs%2F0310057%2Ccs%2F0310013%2Ccs%2F0310018%2Ccs%2F0310045%2Ccs%2F0310041%2Ccs%2F0310024%2Ccs%2F0310002%2Ccs%2F0310056%2Ccs%2F0310063%2Ccs%2F0310036%2Ccs%2F0310006%2Ccs%2F0310025%2Ccs%2F0310044%2Ccs%2F0310050%2Ccs%2F0310061%2Ccs%2F0310062%2Ccs%2F0310031%2Ccs%2F0212010%2Ccs%2F0212036%2Ccs%2F0212043%2Ccs%2F0212040%2Ccs%2F0212025%2Ccs%2F0212046%2Ccs%2F0212056&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Vector Gaussian Multiple Description with Individual and Central\n  Receivers"}, "summary": "L multiple descriptions of a vector Gaussian source for individual and\ncentral receivers are investigated. The sum rate of the descriptions with\ncovariance distortion measure constraints, in a positive semidefinite ordering,\nis exactly characterized. For two descriptions, the entire rate region is\ncharacterized. Jointly Gaussian descriptions are optimal in achieving the\nlimiting rates. The key component of the solution is a novel\ninformation-theoretic inequality that is used to lower bound the achievable\nmultiple description rates.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0510063%2Ccs%2F0510013%2Ccs%2F0510054%2Ccs%2F0510046%2Ccs%2F0510057%2Ccs%2F0510083%2Ccs%2F0510095%2Ccs%2F0510006%2Ccs%2F0510004%2Ccs%2F0510051%2Ccs%2F0510024%2Ccs%2F0510071%2Ccs%2F0510040%2Ccs%2F0510068%2Ccs%2F0510035%2Ccs%2F0510001%2Ccs%2F0510003%2Ccs%2F0510048%2Ccs%2F0510084%2Ccs%2F0510075%2Ccs%2F0510012%2Ccs%2F0510078%2Ccs%2F0510092%2Ccs%2F0510029%2Ccs%2F0510030%2Ccs%2F0510047%2Ccs%2F0510011%2Ccs%2F0510090%2Ccs%2F0510009%2Ccs%2F0510086%2Ccs%2F0310054%2Ccs%2F0310028%2Ccs%2F0310008%2Ccs%2F0310039%2Ccs%2F0310012%2Ccs%2F0310055%2Ccs%2F0310023%2Ccs%2F0310032%2Ccs%2F0310026%2Ccs%2F0310007%2Ccs%2F0310019%2Ccs%2F0310001%2Ccs%2F0310037%2Ccs%2F0310017%2Ccs%2F0310048%2Ccs%2F0310058%2Ccs%2F0310022%2Ccs%2F0310043%2Ccs%2F0310049%2Ccs%2F0310047%2Ccs%2F0310059%2Ccs%2F0310027%2Ccs%2F0310009%2Ccs%2F0310034%2Ccs%2F0310042%2Ccs%2F0310046%2Ccs%2F0310016%2Ccs%2F0310003%2Ccs%2F0310053%2Ccs%2F0310020%2Ccs%2F0310040%2Ccs%2F0310052%2Ccs%2F0310065%2Ccs%2F0310064%2Ccs%2F0310060%2Ccs%2F0310004%2Ccs%2F0310015%2Ccs%2F0310011%2Ccs%2F0310005%2Ccs%2F0310030%2Ccs%2F0310010%2Ccs%2F0310029%2Ccs%2F0310021%2Ccs%2F0310014%2Ccs%2F0310038%2Ccs%2F0310035%2Ccs%2F0310033%2Ccs%2F0310057%2Ccs%2F0310013%2Ccs%2F0310018%2Ccs%2F0310045%2Ccs%2F0310041%2Ccs%2F0310024%2Ccs%2F0310002%2Ccs%2F0310056%2Ccs%2F0310063%2Ccs%2F0310036%2Ccs%2F0310006%2Ccs%2F0310025%2Ccs%2F0310044%2Ccs%2F0310050%2Ccs%2F0310061%2Ccs%2F0310062%2Ccs%2F0310031%2Ccs%2F0212010%2Ccs%2F0212036%2Ccs%2F0212043%2Ccs%2F0212040%2Ccs%2F0212025%2Ccs%2F0212046%2Ccs%2F0212056&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "L multiple descriptions of a vector Gaussian source for individual and\ncentral receivers are investigated. The sum rate of the descriptions with\ncovariance distortion measure constraints, in a positive semidefinite ordering,\nis exactly characterized. For two descriptions, the entire rate region is\ncharacterized. Jointly Gaussian descriptions are optimal in achieving the\nlimiting rates. The key component of the solution is a novel\ninformation-theoretic inequality that is used to lower bound the achievable\nmultiple description rates."}, "authors": ["H. Wang", "P. Viswanath"], "author_detail": {"name": "P. Viswanath"}, "author": "P. Viswanath", "links": [{"href": "http://arxiv.org/abs/cs/0510078v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0510078v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0510078v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0510078v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Submitted to the IEEE Transactions on Information Theory, Oct. 2005\n\nVector Gaussian Multiple Description with\nIndividual and Central Receivers1\nHua Wang and Pramod Viswanath\n\n2\n\narXiv:cs/0510078v1 [cs.IT] 25 Oct 2005\n\nAbstract\nL multiple descriptions of a vector Gaussian source for individual and central receivers are investigated. The sum rate of the descriptions with covariance distortion\nmeasure constraints, in a positive semidefinite ordering, is exactly characterized.\nFor two descriptions, the entire rate region is characterized. Jointly Gaussian descriptions are optimal in achieving the limiting rates. The key component of the\nsolution is a novel information-theoretic inequality that is used to lower bound the\nachievable multiple description rates.\n\n1\n\nIntroduction\n\nIn the multiple description problem, an information source is encoded into L packets\nand these packets are sent through parallel communication channels. There are several\nreceivers, each of which can receive a subset of the packets and needs to reconstruct\nthe information source based on the received packets. In the most general case, there\nare 2L \u2212 1 receivers and the packets received in each receiver correspond to one of 2L \u2212\n1 subsets of {1, . . . , L}. A long standing open problem in the literature [1\u201310] is\nto characterize the information-theoretic rate region subject to the specified distortion\nconstraints. Practical multiple description codes have been discussed in [11\u201318] and\nrecent work [19, 20] has considered the multiple description problem in the context of\nthe distributed source coding scenario. Optimal descriptions of even the Gaussian source\nwith quadratic distortion measures have not been fully characterized. In the special\ncase of two descriptions of a scalar Gaussian source with quadratic distortion measures,\nhowever, the entire rate region has been characterized in [1].\nOur focus is on L descriptions of a memoryless vector Gaussian source forwhere L\nindividual and a single common receiver (cf. Figure 1). Each receiver needs to reconstruct\nthe original source such that the empirical covariance matrix of the difference is less than,\nin the sense of a positive semidefinite ordering, a \"distortion\" matrix. In this setting, the\nsymmetric rate multiple description problem of a scalar Gaussian source with symmetric\ndistortion constraints has been characterized in [7, 8, 10], but a complete understanding\nof all other rate-distortion settings is open.\n1\n\nThis research was sponsored in part by NSF CCR-0325924 and a Vodafone US Foundation Fellowship.\n2\n\nThe authors are with the Department of Electrical and Computer Engineering and the Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Urbana IL 61801; e-mail:\n{huawang,pramodv}@uiuc.edu\n\n1\n\n\fencoder 1\n\nchannel 1\n\ndecoder 1\n\nencoder 2\n\nchannel 2\n\ndecoder 2\n\nx\n\nx\u03021\n\nx\u03022\n\ncentral\ndecoder\nencoder L\n\nchannel L\n\ndecoder L\n\nx\u03020\n\nx\u0302L\n\nFigure 1: MD problem with only individual reconstructions and central reconstruction\n\nOur main result is an exact characterization of the sum rate for any specified L + 1\ndistortion matrix constraints. With L = 2, we characterize the entire rate region. Our\ncontribution is two fold:\n\u2022 First, we derive a novel information-theoretic inequality that provides a lower bound\nto the sum of the description rates. The key step is to avoid using the entropy\npower inequality, which was a central part of the proof of two descriptions of the\nscalar Gaussian source in [1]: the vector entropy power inequality is tight only\nwith a certain covariance alignment condition, which arbitrary distortion matrix\nrequirements do not necessarily allow.\n\u2022 Second, we show that jointly Gaussian descriptions actually achieve the lower bound\nnot by resorting to a direct calculation and comparison, which appears to be difficult\nfor L > 2, but instead by arguing the equivalence of certain optimization problems.\nConsider another two description problem of a pair of jointly Gaussian memoryless\nsources as depicted in Figure 2. There are two encoders that describe this source to\nthree receivers: receiver i gets the description of encoder i, with i = 1, 2 and the third\nreceiver receives both the descriptions. Suppose receiver i is interested in reconstructing\nthe ith marginal of the jointly Gaussian source, with i = 1, 2. The third receiver is\ninterested in reconstructing the entire vector source. This description problem is closely\nrelated to the vector Gaussian description problem that is the main focus of this paper.\nWe exploit this connection and characterize the rate region where the reconstructions\nhave a constraint on the covariance of error at each of the receivers (in the sense of a\npositive semidefinite order).\nWe have organized the results in this paper as follows. In Section 2 we give a formal\ndescription of the problem and summarize our main result. The derivation of a lower\nbound is in Section 3. In Section 4 we provide an upper bound and provide conditions for\nthe achievable sum rate to meet the lower bound. We see in Section 5 that the conditions\n2\n\n\fencoder 1\n\nchannel 1\n\ndecoder 1\n\nx\u03021\n\ncentral\ndecoder\n\n(x1 , x2 )\n\nencoder 2\n\nchannel 2\n\ndecoder 2\n\n(x\u0302\u20321 , x\u0302\u20322 )\n\nx\u03022\n\nFigure 2: Multiple Descriptions with separate distortion constraints.\n\nare indeed satisfied in the special case of a scalar Gaussian source. The solution in the\ncase of the more complicated vector Gaussian source is in Section 6. The solution to the\nmultiple description problem depicted in Figure 2 is the topic of Section 7.1. Finally, while\nthe characterization of the rate region of general multiple descriptions of the Gaussian\nsource (with each receiver having access to some subset of the descriptions) is still open,\nwe can use the insights derived via our sum rate characterization to solve this problem\nfor a nontrivial set of covariance distortion constraints; this is done in Section 7.2.\nA note about the notation in this paper: we use lower case letters for scalars, lower\ncase and bold face for vectors, upper case and bold face for matrices. The superscript t\ndenotes matrix transpose. We use I and 0 to denote the identity matrix and the all zero\nmatrix respectively, and diag{p1 , . . . , pn } to denote a diagonal matrix with the diagonal\nentries equal to p1 , . . . , pn . The partial order \u227b (<) denotes positive definite (semidefinite)\nordering: A \u227b B (A < B) means that A \u2212 B is a positive definite (semidefinite) matrix.\nWe write N (\u03bc, Q) to denote a Gaussian random vector with mean \u03bc and covariance Q.\nAll logarithms in this paper are to the natural base.\n\n2\n2.1\n\nProblem Setting and Main Results\nProblem Setting\n\nThe information source {x[m]} is an i.i.d. random process with the marginal distribution\nN (0, Kx), i.e., a collection of i.i.d. Gaussian random vectors. Denoting the dimension\nof {x[m]} by N, we suppose that Kx is an N \u00d7 N positive definite matrix. There are\nL encoding functions at the source, encoder l encodes a source sequence, of length n,\n(n)\n(n)\n(n)\nxn = (x[1], . . . , x[n])t to a source code Cl = fl (xn ), for l = 1 . . . L. This code Cl\n(n)\nis sent through lth communication channel at the rate Rl = n1 log |Cl |. There are L\nindividual receivers and one central receiver.\n3\n\n\fFor l = 1, . . . L, the lth individual receiver\u0010 uses its \u0011information (the output of the lth\n(n)\n(n)\nfl (xn ) of the source sequence xn . The\nchannel) to generate an estimate x\u0302nl = gl\ncentral receiver uses the output of all the L channels to generate an estimate x\u0302n0 of the\nsource sequence xn . Since we are interested in covariance constraints, the decoder maps\ncan be restricted to be the minimal mean square error (MMSE) estimate of the source\nsequence based on the received codewords. So,\nh\ni\n(n)\nx\u0302nl = E xn |fl (xn ) , l = 1, . . . , L\nh\ni\n(1)\n(n)\nn\nn (n)\nn\nn\nx\u03020 = E x |f1 (x ), . . . , fL (x ) .\nSuppose the reconstructed sequences satisfy the covariance constraints\nn\ni\n1X h\nt\nE (x[m] \u2212 x\u0302l [m]) (x[m] \u2212 x\u0302l [m]) 4 Dl ,\nn m=1\nn\ni\n1X h\nE (x[m] \u2212 x\u03020 [m])t (x[m] \u2212 x\u03020 [m]) 4 D0 ,\nn m=1\n\nl = 1, . . . , L,\n(2)\n\nthen we say that multiple descriptions with distortion constraints (D1 , . . . , DL , D0 ) are\nachievable at the rate tuple (R1 , . . . , RL ).\nThe closure of the set of all achievable rate tuples is called the rate region and is denoted\nby R\u2217 (Kx , D1 , . . . , DL , D0 ). Throughout this paper, we suppose that 0 \u227a D0 \u227a Dl \u227a\nKx , \u2200l = 1, . . . , L.3\n\n2.2\n\nSum Rate\n\nOur main result is the precise characterization of the sum rate of multiple descriptions\nfor individual and central receivers.\nTheorem 1. For distortion constraints (D1 , . . . , DL , D0 ), the sum rate is\n\uf8f6\n\uf8eb\nsup\n\nKz \u227b0\n\n\uf8ec |K ||K + K |(L\u22121) |D + K | \uf8f7\n1\n\uf8ec x x\nz\n0\nz \uf8f7\nlog \uf8ec\n\uf8f7.\nL\nQ\n2\n\uf8f8\n\uf8ed\n|D0 | |Dl + Kz |\n\n(3)\n\nl=1\n\n3\n\nThat D0 4 Dl , is without loss of generality is seen by applying the data processing inequality for\nmmse estimation errors; having more access to information can only reduce the covariance of the error in\na positive semidefinite sense. Similarly, Kx 4 D0 is also not interesting; here we simplify this condition\nand take D0 \u227a Kx .\n\n4\n\n\fThis sum rate is achieved by a jointly Gaussian random multiple description scheme:\nlet w1 , * * * , wL be zero mean jointly Gaussian random vectors independent of x, with\nthe positive definite covariance matricex (w1 , * * * , wL ) denoted by Kw . Defining\nul = x + wl ,\n\nl = 1, . . . , L,\n\nwe consider Kw such that\nh\ni\ndef\nCov[x|ul ] =E (x \u2212 E[x|ul ])t (x \u2212 E[x|ul ]) 4 Dl , l = 1, . . . , L,\nh\ni\ndef\nCov[x|u1 , . . . , uL ] =E (x \u2212 E[x|u1 , . . . , uL ])t (x \u2212 E[x|u1 , . . . , uL ]) 4 D0 .\n\n(4)\n\nTo construct the code book for the lth description, draw enRl unl vectors randomly according to the marginal of ul . The encoders observe the source sequence xn , look for\ncodewords (un1 , . . . , unL ) that are jointly typical with xn and send the index of the resulting unl through the lth channel, respectively. The lth individual receiver uses this\nindex and generates a reproduction sequence E[xn |unl ] for l = 1 . . . L, the central receiver\nuses all the L indices to generate a reproduction sequence E[xn |un1 , . . . , unL ]. For every\nKw satisfying (4), the rate tuple (R1 , . . . , RL ) satisfying\nQ\n|Kx + Kwl |\nX\nX\n1\nl\u2208S\n, \u2200S \u2286 {1, . . . , L} (5)\nRl \u2265\nh(ul ) \u2212 h(ul , l \u2208 S|x) = log\n2\n|K\nwS |\nl\u2208S\nl\u2208S\nis achievable by using this coding scheme, where KwS is the covariance matrix for all\nwl , l \u2208 S, and Kwl = E[wlt wl ]. In particular, the achievable sum rate is\nL\nQ\n\n|Kx + Kwl |\n1\nl=1\nlog\n.\n2\n|Kw |\n\n(6)\n\nWe denote this ensemble of descriptions, throughout this paper, as the jointly Gaussian\ndescription scheme and the time sharing between them as the jointly Gaussian description\nstrategy. We show that jointly Gaussian description schemes are optimal in achieving\nthe sum rate (3).\n\n2.3\n\nRate Region for Two Description Problem\n\nFor two descriptions, we can characterize the entire rate region.\nTheorem 2. Given distortion constraints (D1 , D2 , D0 ), the rate region for the two\n\n5\n\n\fR2\n\nRate Region\n\nB1\n1\n2\n\nx|\nlog |K\n|D2 |\n\n0\n\nB2\n1\n2\n\nx|\nlog |K\n|D1 |\n\nRsum\n\nR1\n\nFigure 3: Rate region for two description problem\n\ndescription problem for an i.i.d. N (0, Kx) vector Gaussian source is\n\uf8f1\n(R1 , R2 ) :\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n|Kx |\n1\n\uf8f2\n, l = 1, 2\nRl \u2265 log\nR\u2217 (Kx , D1 , D2 , D0 ) =\n2\n|Dl |\n\uf8f4\n\uf8f4\n1\n|Kx ||Kx + Kz ||D0 + Kz |\n\uf8f4\n\uf8f4\n\uf8f3 R1 + R2 \u2265 sup log\n|D0 ||D1 + Kz ||D2 + Kz |\nKz \u227b0 2\n\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\n.\n\n(7)\n\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\n\nWe show that if the distortion constraints (D1 , D2 , D0 ) satisfy D0 +Kx \u2212D1 \u2212D2 \u227b 0\n\u22121\n\u22121\n\u22121\nand D\u22121\n0 + Kx \u2212 D1 \u2212 D2 \u227b 0, we can get the optimizing Kz by solving a matrix\nRiccati equation. An illustration of the rate region is shown in Figure 3. In this case, if\n\u22121 \u22121\nwe let Kwl = [D\u22121\nfor l = 0, 1, 2, then the optimizing Kz is\nl \u2212 Kx ]\nKz = Kx (Kx \u2212 A\u2217 )\u22121 Kx \u2212 Kx ,\nwhere\nh\ni1\n1\n\u2212 21\n\u2212 12 2\nA = (Kw1 \u2212Kw0 ) (Kw1 \u2212 Kw0 ) (Kw2 \u2212 Kw0 )(Kw1 \u2212 Kw0 )\n(Kw1 \u2212Kw0 ) 2 \u2212Kw0 .\n\u2217\n\n1\n2\n\nLetting Rsum denote the optimal sum rate, the two corner points in Figure 3 are\n\u0012\n\u0013\n1\n|Kx |\n1\n|Kx |\nB1 =\n, and\nlog\n, Rsum \u2212 log\n2\n|D1 |\n2\n|D1 |\n\u0013\n\u0012\n|Kx | 1\n|Kx |\n1\n.\n, log\nB2 = Rsum \u2212 log\n2\n|D2 | 2\n|D2 |\n6\n\n\f3\n\nLower Bound\n\nBy fairly procedural steps, we have the following lower bound to the sum rate of the\nmultiple descriptions:\nn\n\nL\nX\n\nRl \u2265\n\nl=1\n\n=\n\nL\nX\n\nl=1\nL\nX\n\nH(Cl ) =\n\nL\nX\n\nH(Cl ) \u2212 H(C1 , . . . , CL |xn )\n\nl=1\n\nH(Cl ) \u2212 H(C1 , * * * , CL ) + H(C1, . . . , CL ) \u2212 H(C1 , . . . , CL |xn )\n\n(8)\n\nl=1\n\n=I(C1 ; C2 ; . . . ; CL ) + I(C1 , . . . , CL ; xn ),\nwhere we have defined\ndef\n\nI(C1 ; C2 ; . . . ; CL ) =\n\nL\nX\n\nH(Cl ) \u2212 H(C1, . . . , CL ) =\n\nl=1\n\nL\nX\n\nI(Cl ; C1 . . . Cl\u22121 ),\n\nl=2\n\nand called it the symmetric mutual information between C1 , . . . , CL . Note that\nI(C1 ; C2 ; . . . ; CL ) \u2265 0 and is also well defined even when C1 , . . . , CL are continuous\nrandom variables. Our main result is the following information theoretic inequality which\ngives a lower bound to the sum of symmetric mutual information between (C1 , C2 , . . . , CL )\nand mutual information between C1 , C2 , . . . , CL and xn for given covariance constraints.\nLemma 1. Let xn = (x[1], . . . , x[n]), where x[m]'s are i.i.d. N (0, Kx ) Gaussian random vectors for m = 1, . . . , n. Let C1 , . . . , CL be random variables jointly distributed\nwith xn . Let x\u0302n0 = E[xn |C1, . . . , CL ] and x\u0302nl = E[xn |Cl ] for l = 1, . . . , L. Given positive\ndefinite matrices D1 , . . . , DL , D0 , if\nn\n1X\nE[(x[m] \u2212 x\u0302l [m])t (x[m] \u2212 x\u0302l [m])] 4 Dl ,\nn m=1\n\nl = 1, . . . , L,\n(9)\n\nn\n1X\nE[(x[m] \u2212 x\u03020 [m])t (x[m] \u2212 x\u03020 [m])] 4 D0 ,\nn m=1\n\nthen\n|Kx ||Kx + Kz |(L\u22121) |D0 + Kz |\nn\nlog\n. (10)\nL\nQ\nKz \u227b0 2\n|D0 | |Dl + Kz |\n\nI(C1 ; C2 ; . . . ; CL )+I(C1 , . . . , CL ; xn ) \u2265 sup\n\nl=1\n\nFurthermore, there exists a jointly Gaussian distribution of (C1 , . . . , CL , xn ) such that the\ninequality in (10) is tight.\n\n7\n\n\fThis is a fundamental information-theoretic inequality which involves only the joint\ndistribution4 between C1 , C2 , . . . , CL and xn and bounds on mean square error estimation\nof xn from C1 , C2 , . . . , CL ; we delegate the proof of this result to Appendix B. We can\nnow use Lemma 1 to derive a lower bound to the sum rate\nL\nX\nl=1\n\n1\n|Kx ||Kx + Kz |(L\u22121) |D0 + Kz |\n.\nlog\nL\nQ\nKz \u227b0 2\n|D0 | |Dl + Kz |\n\nRl \u2265 sup\n\n(11)\n\nl=1\n\nBy letting L = 1 in the lemma above, we can derive a simple lower bound to the rate\nof the individual descriptions as well:\n\u0001\n1\n1\nH(Cl ) \u2212 H(Cl |xn )\nH(Cl ) =\nn\nn\n1\n= I(xn ; Cl )\nn\n|Kx |\n1\n, l = 1, . . . , L.\n\u2265 log\n2\n|Dl |\n\nRl \u2265\n\n(12)\n\nThis bound is actually the point-to-point rate-distortion function for individual receivers,\nsince each individual receiver only faces a point-to-point compression problem.\nNote that for any positive definite Kz ,\n1\n|Kx ||Kx + Kz |(L\u22121) |D0 + Kz |\nlog\nL\nQ\n2\n|D0 | |Dl + Kz |\nl=1\n\nis a lower bound to the sum rate of the multiple descriptions. Two special choices of Kz\nare of particular interest:\n\u2022 Letting Kz = \u01ebI and 0 \u01eb \u2192 0+ , we have the following lower bound:\nL\nX\n\nRl \u2265\n\nl=1\n\n1\n|Kx |L\nlog\n.\n2\n|D1 | . . . |DL |\n\n(13)\n\nThis bound is actually the summation of the bounds on the individual rates.\n\u2022 Letting some eigenvalues of Kz goes to infinity, we have the following lower bound:\nL\nX\n\nRl \u2265\n\nl=1\n\n4\n\n|Kx |\n1\nlog\n.\n2\n|D0 |\n\n(14)\n\nThis inequality holds even when C1 , C2 , . . . , CL are not simply functions of xn and can also be\ncontinuous random variables.\n\n8\n\n\fThis bound is the point-to-point rate-distortion function when we only have the\ncentral distortion constraint.\nWe will see later that for some distortion constraints (D1 , . . . , DL , D0 ), (13) and (14)\ncan be tight.\n\n4\n\nUpper Bound\n\nIn the previous section we gave a lower bound to the sum rate. Now we give a upper\nbound to the sum rate by using the jointly Gaussian description scheme described in\nSection 2.2.\n\n4.1\n\nJointly Gaussian Multiple Description Scheme\n\nFirst we give a sketch of the achievable rate region by using jointly Gaussian description\nscheme. Given the source sequence xn , as long as we can find a combination of codewords\n(un1 , . . . , unL ) that are jointly typical with xn , all the receivers can generate reproduction\nsequences that satisfy their given distortion constraints. An intuitive way to understand\n(5) is the following: since (un1 , . . . , unL ) are jointly typical with xn , then for any S \u2286\n{1, . . . , L}, we have that unl , l \u2208 S are jointly typical with xn . Now the probability that\na randomly generated combination of codewords unl , l \u2208 S are jointly typical with xn is\nroughly\nenh(ul ,l\u2208S|x)\nQ nh(u ) ,\nl\ne\nl\u2208S\nQ nR\nand the number of possible combination of codewords unl , l \u2208 S are\ne l . Thus, as\nl\u2208S\n\nlong as\n\nX\nl\u2208S\n\nRl \u2265\n\nX\n\nh(ul ) \u2212 h(ul , l \u2208 S|x),\n\n(15)\n\nl\u2208S\n\nwe can find a combination of codewords unl , l \u2208 S that are jointly typical with xn .\nRigorously speaking, we need to show that as long as (15) is satisfied, then for any\ngiven source sequence xn we can find a combination of codewords (un1 , . . . , unL ) such\nthat unl , l \u2208 S are jointly typical with xn for all S \u2286 {1, . . . , L}. The second moment\nmethod [21] is commonly used to address this aspect, and a proof can be found in [7].\nEvaluating (15) based on the jointly Gaussian distribution of x and u1 , . . . , uL , we\nget that all the rate tuples (R1 , . . . , RL ) satisfying\nQ\n|Kx + Kwl |\nX\nX\n1\nl\u2208S\n, \u2200S \u2286 {1, . . . , L} (16)\nRl \u2265\nh(ul ) \u2212 h(ul , l \u2208 S|x) = log\n2\n|K\nwS |\nl\u2208S\nl\u2208S\n9\n\n\fare achievable by the jointly Gaussian description scheme. In particular, we have that\nthe achievable sum rate is\nL\nX\nl=1\n\nL\nQ\n\n|Kx + Kwl |\n1\nl=1\nh(ul ) \u2212 h(u1 , . . . , uL |x) = log\n.\n2\n|Kw |\n\n(17)\n\nThe resulting distortions (D\u22171 , . . . , D\u2217L , D\u22170 ) by using jointly Gaussian description scheme\ncan be calculated as\n\u22121 \u22121\nD\u2217l =Cov[x|ul ] = [K\u22121\nx + Kwl ] ,\n\nD\u22170\n\n4.2\n\n=Cov[x|u1 , . . . , uL ] =\n\n[K\u22121\nx\n\nl = 1, . . . , L,\n\nt \u22121\n+ (I, . . . , I)K\u22121\nw (I, . . . , I) ] .\n\n(18)\n\nCombinatorial Property of the Achievable Region\n\nThe achievable region given in (15) has useful combinatorial properties; in particular\nit belongs to the class of contra-polymatroids [22]. Certain rate regions of the multiple\naccess channel [23] and distributed source coding problems [24] are also known to have\nthis specific combinatorial property. To see this, let\nX\ndef\n\u03c6(S) =\nh(ul ) \u2212 h(ul , l \u2208 S|x), S \u2286 {1, . . . , L}.\nl\u2208S\n\nWe can readily verify that\n\u03c6(S \u222a {t}) \u2265 \u03c6(S), \u2200t \u2208 {1, . . . , L},\n\u03c6(S \u222a T ) + \u03c6(S \u2229 T ) \u2265 \u03c6(S) + \u03c6(T ).\n\n(19)\n\nBy definition, we conclude that the achievable rate region of a jointly Gaussian multiple\ndescription scheme is a contra-polymatroid. The key advantage of this combinatorial\npropety is that we can exactly characterize the vertices of the achievable rate region\n(15). Letting \u03c0 to be a permutation on {1, . . . , L}, define\n(\u03c0) def\n\nbi = \u03c6({\u03c01 , \u03c02 , . . . , \u03c0i }) \u2212 \u03c6({\u03c01 , \u03c02 , . . . , \u03c0i\u22121 }), i = 1, . . . , L,\n\u0010\n\u0011\n(\u03c0)\n(\u03c0)\nand b(\u03c0) = b1 , . . . , bL . Then the L! points {b(\u03c0) , \u03c0 a permutation} are the vertices\nof the contra-polymatroid (15).\n\n4.3\n\nComparison of Upper Bound and the Lower Bound\n\nOur goal is to show that the jointly Gaussian description scheme achieves the lower\nbound to the sum rate. In general it does not seem facile to do a direct calculation and\n10\n\n\fcomparison. We forgo this strategy and, instead, provide an alternative characterization\nof the achievable sum rate which is much easier to compare with the lower bound.\nSimilar to the derivation of the lower bound (in Appendix B), we consider an N (0, Kz )\nGaussian random vector z, independent of x and all wl 's. Defining y = x + z, we have\nthe following achievable sum rate:\nL\nX\n\nRl =\n\nl=1\n\n=\n=\n(a)\n\n\u2265\n\nL\nX\n\nl=1\nL\nX\n\nl=1\nL\nX\n\nl=1\nL\nX\n\nh(ul ) \u2212 h(u1 , . . . , uL |x)\nh(ul ) \u2212 h(u1 , . . . , uL ) + h(u1 , . . . , uL ) \u2212 h(u1 , . . . , uL |x)\nh(ul ) \u2212 h(u1 , * * * , uL ) + I(u1 , . . . , uL ; x)\nh(ul ) \u2212 h(u1 , * * * , uL ) + I(u1 , . . . , uL ; x) \u2212\n\n=\n\nl=1\n\nh(ul |y) \u2212 h(u1 , . . . , uL |y)\n\nl=1\n\nl=1\n\nL\nX\n\nL\nX\n\n!\n\n\u0001\nh(y) \u2212 h(y|ul ) \u2212 h(y) + h(y|u1 , . . . , uL ) + h(x) \u2212 h(x|u1 , . . . , uL )\n\n= h(x) + (L \u2212 1)h(y) \u2212\n\nL\nX\n\nh(y|ul ) + h(y|u1 , . . . , uL ) \u2212 h(x|u1 , . . . , uL )\n\nl=1\n(L\u22121)\n\n=\n\nCov[x|u1 , . . . , uL ] + Kz\nKx Kx + Kz\n1\nlog\n,\nL\nQ\n2\nCov[x|u1 , . . . , uL ]\nCov[x|ul ] + Kz\n\n(20)\n\nl=1\n\nwhere the last step is from a procedural Gaussian MMSE calculation.\nNote that if we have\nL\nX\n\nh(ul |y) \u2212 h(u1 , . . . , uL |y) = 0,\n\n(21)\n\nl=1\n\nthen (a) in (20) is actually an equality. Thus, if our choice of Kw and Kz satisfy the\nfollowing two conditions:\n\u2022 (21) is true.\n\u2022 distortion constraints are met with equality, i.e.,\nCov[x|ul ] = Dl , l = 1, . . . , L,\nCov[x|u1 , . . . , uL ] = D0 ,\n11\n\n(22)\n\n\fthen the upper bound matches the lower bound and we have characterized the sum rate.\nIn the following we examine under what circumstances the above two conditions are true.\nFirst, we give a necessary and sufficient condition for (21) to be true, delegating the\nproof to Appendix C.\nProposition 1. There exists some choice of positive definite Kz such that (21) is true\nif and only if Kw , the covariance matrix of (w1 , * * * , wL ), takes the following form\n\uf8f6\n\uf8eb\nKw1 \u2212A \u2212A\n...\n\u2212A\n\uf8ec \u2212A Kw2 \u2212A\n...\n\u2212A \uf8f7\n\uf8f7\n\uf8ec\n\uf8f7\n(23)\nKw = \uf8ec\n\uf8ec. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\uf8f7 ,\n\uf8ed \u2212A . . . \u2212A KwL\u22121 \u2212A \uf8f8\n\u2212A . . . \u2212A \u2212A KwL\nwhere 0 \u227a A \u227a Kx .\n\nNext, we look at the conditions for (22) to be true. From (18), we have\n\u22121\nD\u22121\n= Cov[x|ul ]\u22121 = K\u22121\nx + Kwl ,\nl\n\nD\u22121\n0\n\n= Cov[x|u1 , . . . , uL ]\n\n\u22121\n\nK\u22121\nx\n\n=\n\nl = 1, . . . , L\n\n(24)\n\nt\n+ (I, . . . , I)K\u22121\nw (I, . . . , I) .\n\nt\n(I, I, . . . , I)K\u22121\nw (I, I, . . . , I) , is calculated in the following lemma; the proof is\navailable in Appendix D.\n\nLemma 2. Let\n\n\uf8f6\n\uf8eb\nKw1 \u2212A \u2212A\n...\n\u2212A\n\uf8ec \u2212A Kw2 \u2212A\n...\n\u2212A \uf8f7\n\uf8f7\n\uf8ec\n\uf8f7.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nKw = \uf8ec\n\uf8f7\n\uf8ec\n\uf8ed \u2212A . . . \u2212A KwL\u22121 \u2212A \uf8f8\n\u2212A . . . \u2212A \u2212A KwL .\n\nIf Kw \u227b 0 and A \u0017 0, then\n\n\uf8ee\n\nt\n\uf8f0\n(I, I, . . . , I)K\u22121\nw (I, I, . . . , I) =\n\nL\nX\n\n(Kwl + A)\u22121\n\nl=1\n\n!\u22121\n\n\uf8f9\u22121\n\n\u2212 A\uf8fb\n\n.\n\nUsing this lemma, from (24) we arrive at\nL\n\u0002 \u22121\n\u0003\u22121\n\u0002 \u22121\n\u0003\u22121 X\n\u22121\n\u22121 \u22121\n(Dl \u2212 K\u22121\n+A\n.\n(D0 \u2212 Kx ) + A\n=\nx )\n\n(25)\n\nl=1\n\nDefining\n\u22121 \u22121\nKw0 = (D\u22121\n0 \u2212 Kx ) ,\n\n12\n\n(26)\n\n\f(24) is equivalent to\n\u22121\n\n[Kw0 + A]\n\n=\n\nL\nX\n\n[Kwl + A]\u22121 .\n\n(27)\n\nl=1\n\nThus, if there exists a positive definite solution A to (27), and the corresponding Kw is\npositive definite, then the distortion constraints are met with equality, i.e., (22) holds.\nIt turns out that as long as A is a solution to (27), the resulting Kw is always positive\ndefinite; we state this formally below, delegating the proof to Appendix E.\nLemma 3. If for some Kw0 \u227b 0 and A \u227b 0 (27) is true, then the covariance matrix Kw\ndefined in (23) is positive definite.\nWe summarize the state of affairs in the following theorem.\nTheorem 3. Given distortion constraints (D1 , . . . , DL , D0 ), let\n\u22121 \u22121\nKwl = (D\u22121\nl \u2212 Kx ) ,\n\nl = 0, 1, . . . , L.\n\n(28)\n\nIf there exists an solution A\u2217 to (27) and 0 \u227a A\u2217 \u227a Kx , then the jointly Gaussian\ndescription scheme with Kw defined in (23) with A = A\u2217 achieves the optimal sum rate,\nand the optimal Kz for lower bound (11) is Kz = Kx (Kx \u2212 A\u2217 )\u22121 Kx \u2212 Kx .\nThus we show that if the given distortion constraints (D1 , . . . , DL , D0 ) satisfy the\ncondition for Theorem 3, then the jointly Gaussian description scheme achieves the optimal sum rate and we can calculate the optimal Kw by solving a matrix equation.\nHowever, for arbitrarily given distortion constraints, (27) may not have a solution A\u2217\nsuch that 0 \u227a A\u2217 \u227a Kx . In this case, we can show that there exists a jointly Gaussian\ndescription scheme that achieves the sum rate lower bound, and resulting in distortions\n(D\u22171 , . . . , D\u2217L , D\u22170 ) such that D\u2217l 4 Dl for l = 0, 1, . . . , L. In the following we first study\nthe relatively simpler case of scalar Gaussian source, and then move to discuss the vector\nGaussian source.\n\n5\n\nScalar Gaussian Source\n\nHere we suppose that the information source is an i.i.d. sequence of N (0, \u03c3x2 ) scalar\nGaussian random variables. Let individual distortion constraints be (d1 , . . . , dL ) and\nthe central distortion constraints be d0 , where 0 < d0 < dl < \u03c3x2 for l = 1, . . . , L. We\nconsider the jointly Gaussian description scheme with the following covariance matrix\nfor w1 , . . . , wl .\n\uf8f6\n\uf8eb 2\n\u03c31 \u2212a \u2212a . . . \u2212a\n\uf8ec\u2212a \u03c322 \u2212a . . . \u2212a\uf8f7\n\uf8f7\n\uf8ec\n\uf8f7.\n(29)\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nKw = \uf8ec\n\uf8f7\n\uf8ec\n2\n\uf8ed\u2212a . . . \u2212a \u03c3L\u22121 \u2212a\uf8f8\n\u2212a . . . \u2212a \u2212a \u03c3L2\n13\n\n\fConsider the condition for Theorem 3 to hold: to meet the individual distortion constraint with equality, we need\n\u22122 \u22121\n\u03c3l2 = (d\u22121\n=\nl \u2212 \u03c3x )\n\ndl \u03c3x2\n,\n\u03c3x2 \u2212 dl\n\nLet\ndef\n\n\u22122 \u22121\n\u03c302 = (d\u22121\n=\n0 \u2212 \u03c3x )\n\nwe need\n\nl = 1, . . . , L.\n\n(30)\n\nd0 \u03c3x2\n,\n\u03c3x2 \u2212 d0\n\n(31)\n\nL\n\u0002 2\n\u0003\u22121\n\u0002 2\n\u0003\u22121 X\n\u03c3l + a\n\u03c30 + a\n=\n\n(32)\n\nl=1\n\nto have a solution a\u2217 \u2208 (0, \u03c3x2 ), to meet the central distortion constraint with equality.\nTowards this, define\nL\nX\n1\n1\ndef\nf (a) = 2\n\u2212\n,\n(33)\n2\n\u03c30 + a l=1 \u03c3l + a\n\nand we have\n\nL\n\nL\n\nX 1\nL\u22121 X 1\n1\n1\n\u2212\n+\n,\n=\nf (0) = 2 \u2212\n\u03c30\n\u03c3l2\nd0\n\u03c3x2\ndl\nl=1\n\n1\nf (\u03c3x2 ) = 2\n\u2212\n\u03c30 + \u03c3x2\n\nL\nX\nl=1\n\n1\n1\n= 4\n2\n2\n\u03c3l + \u03c3x\n\u03c3x\n\nl=1\nL\nX\n\n!\n\n(34)\n\ndl \u2212 d0 \u2212 (L \u2212 1)\u03c3x2 .\n\nl=1\n\nUsing induction, we can show that\nL\nX\n1\nL\u22121\n\u2212\ndl\n\u03c3x2\nl=1\n\n!\u22121\n\n\u2265\n\nL\nX\n\ndl \u2212 (L \u2212 1)\u03c3x2 .\n\n(35)\n\nl=1\n\nThus we have\nf (0) \u2264 0 \u21d2 f (\u03c3x2 ) \u2264 0,\nf (\u03c3x2 ) \u2265 0 \u21d2 f (0) \u2265 0.\nThen given distortions (d1 , . . . , dL , d0 ), f (0) and f (\u03c3x2 ) falls into the following three\ncases.\nCase 1: f (0) > 0 and f (\u03c3x2 ) < 0.\nIn this case, since f (a) is a continuous function, there exists an a\u2217 \u2208 (0, \u03c3x2 ) such that\nf (a\u2217 ) = 0. In this case the condition for Theorem 3 holds and from Theorem 3 we know\nthat jointly Gaussian description scheme with covariance matrix for w1 , . . . , wl being\n(29) with a = a\u2217 achieves the optimal sum rate.\n14\n\n\fCase 2: f (0) \u2264 0. Alternatively,\n\n1\nd0\n\n+\n\nL\u22121\n\u03c3x2\n\n\u2212\n\nL\nP\n\nl=1\n\n1\ndl\n\n\u2264 0.\n\nIn this case, the condition for Theorem 3 does not hold. But the jointly Gaussian\ndescription scheme can still achieve the sum rate. To see this, choosing a = 0 in Kw we\ncan meet individual distortions with equality and get a central distortion d\u20320 . From (24)\nwe have\n1\n1\n= 2 + (1 1 . . . 1)Kw\u22121 (1 1 . . . 1)t\n\u2032\nd0\n\u03c3x\nL\nL\nX\nX\n1\n1\n1\nL\u22121\n= 2+\n\u2212\n=\n2\n\u03c3x\n\u03c3\ndl\n\u03c3x2\nl=1 l\nl=1\n\u2265\n\n(36)\n\n1\n.\nd0\n\nHence we have achieved distortion (d1 , . . . , dL , d\u20320 ) where d\u20320 \u2264 d0 , and from (17) the\nachievable sum rate is\nL\nX\n1\n\u03c3x2L\nRl \u2265 log\n,\n(37)\n2\nd\n1 d2 * * * dL\nl=1\nwhich equals the sum of our bounds on individual rates.\nCase 3: f (\u03c3x2 ) \u2265 0, Alternatively,\n\nL\nP\n\nl=1\n\ndl \u2212 d0 \u2212 (L \u2212 1)\u03c3x2 \u2265 0.\n\nIn this case, the conditions for Theorem 3 do not hold as well. But the jointly Gaussian\ndescription strategy still achieves the sum rate. To see this, note that we can find a d\u2032L\nsuch that 0 < d\u2032L \u2264 dL and\nL\u22121\nX\n\ndl + d\u2032L \u2212 d0 \u2212 (L \u2212 1)\u03c3x2 = 0,\n\n(38)\n\nl=1\n\n\u22122 \u22121\n\u22122 \u22121\nand we choose a = \u03c3x2 , \u03c3l2 = (d\u22121\nfor l = 1, * * * , L \u2212 1, and \u03c3L2 = (d\u2032 \u22121\nL \u2212 \u03c3x )\nl \u2212 \u03c3x )\n\u22121\n2\n\u22122 \u22121\nin Kw . Defining \u03c30 = (d0 \u2212 \u03c3x ) , (38) is equivalent to the following equation:\n\n\u0002\n\n\u03c302\n\n+\n\n\u0003\u22121\n\u03c3x2\n\n=\n\nL\nX\n\u0002\nl=1\n\n\u03c3l2 + \u03c3x2\n\n\u0003\u22121\n\n.\n\n(39)\n\nFrom Lemma 3, our choice of Kw is positive definite. Thus the resulting distortions are\n(d1 , . . . , dL\u22121 , d\u2032L , d0 ), where 0 < d\u2032L \u2264 dL .\n\n15\n\n\fUsing the determinant equation\n\u03c312 \u2212\u03c3x2 \u2212\u03c3x2 \u2212\u03c3x2 . . . \u2212\u03c3x2\n\u2212\u03c3x2 \u03c322 \u2212\u03c3x2 \u2212\u03c3x2 . . . \u2212\u03c3x2\nL\nL\n\u0010\nX\n\u03c3x2 \u0011 Y 2\n\u2212\u03c3x2 \u2212\u03c3x2 \u03c332 \u2212\u03c3x2 . . . \u2212\u03c3x2\n= 1\u2212\n(\u03c3l + \u03c3x2 )\n2\n2\n..................................\n+\n\u03c3\n\u03c3\nx l=1\nl=1 l\n2\n\u2212\u03c3x2 . . . \u2212\u03c3x2 \u2212\u03c3x2 \u03c3L\u22121\n\u2212\u03c3x2\n\u2212\u03c3x2 . . . \u2212\u03c3x2 \u2212\u03c3x2 \u2212\u03c3x2 \u03c3L2\n\n(40)\n\nand (39), we have an achievable sum rate\nL\nX\nl=1\n\nRl =\n\n1\n\u03c32\nlog x .\n2\nd0\n\n(41)\n\nWe conclude that in this case the point-to-point rate-distortion bound for the central\nreceiver is achievable.\nIn summary, we have shown that the jointly Gaussian description scheme achieves the\nlower bound on the sum rate. Further, the sum rate can be calculated either trivially\n(by choosing a\u2217 = 0 in case II or a\u2217 = 1 in case III) or by solving a polynomial equation\nin a single variable (case I).\n\n6\n\nVector Gaussian Source\n\nThe essence of our proof of the optimality of jointly Gaussian description scheme for\nscalar Gaussian sources is the use of the intermediate value theorem for scalar continuous\nfunctions. However, there is no natural extension of this theorem for vector valued\nfunctions. To avoid this problem, we first explicitly solve the two description problem and\ncharacterize the optimality of jointly Gaussian description scheme. Next, we show that\nthe jointly Gaussian description scheme is optimal for L \u2265 2 by showing an equivalence\nof certain optimization problems. In the last part of this section, we show that the jointly\nGaussian description strategy can achieve the optimal rate region for the two description\nproblem.\n\n6.1\n\nExplicit Solutions for Some Cases of Two Description Problem\n\nWith only two descriptions, we can explicitly solve (27), thus generalizing the corresponding solution for the scalar Gaussian source, derived in [1].\n\n16\n\n\fSuppose the distortion constraints are denoted by (D1 , D2 , D0 ) and let\n\u0013\n\u0012\nKw1 \u2212A\u2217\n.\nKw =\n\u2212A\u2217 Kw2\nWe now solve (24), which is equivalent to (27), for Kw1 , Kw2 and A\u2217 . From (24) we get\n\u22121 \u22121\nKwl = (D\u22121\nl \u2212 Kx ) ,\n\nl = 1, 2,\n\n(42)\n\nand\n\u22121\n\u22121\nt\nD\u22121\n0 = Kx + (I I)Kw (I I) .\n\n(43)\n\nExpanding out K\u22121\nw using Lemma 6 in Appendix A, we get\n\u22121\n\u22121\n\u22121 \u2217\n\u2217 \u22121 \u2217 \u22121\n\u2217 \u22121\nD\u22121\n0 \u2212 Kx = Kw1 + (I + Kw1 A )(Kw2 \u2212 A Kw1 A ) (I + A Kw1 ).\n\n(44)\n\nTaking inverse on both sides, we have\n\u22121 \u22121\n(D\u22121\n= Kw1 \u2212 (Kw1 + A\u2217 )(Kw1 + Kw2 + 2A\u2217 )\u22121 (Kw1 + A\u2217 ).\n0 \u2212 Kx )\n\n(45)\n\nDefining Kw0 as\ndef\n\n\u22121 \u22121\nKw0 = [D\u22121\n0 \u2212 Kx ] ,\n\n(46)\n\nKw1 \u2212 Kw0 = (Kw1 + A\u2217 )(Kw1 + Kw2 + 2A\u2217 )\u22121 (Kw1 + A\u2217 ).\n\n(47)\n\n(45) is equivalent to\n\nDefining\ndef\n\nX = Kw1 + A\u2217 ,\n(47) is equivalent to\nKw1 \u2212 Kw0 = X(2X + Kw2 \u2212 Kw1 )\u22121 X,\n\n(48)\n\nwhich is further equivalent to\nX(Kw1 \u2212 Kw0 )\u22121 X = 2X + Kw2 \u2212 Kw1 .\n\n(49)\n\nThis is a version of the so-called algebraic Riccati equation; the corresponding Hamiltonian\nis readily seen to be positive semidefinite and we can even write down the following\nexplicit solution:\nX =Kw1 \u2212 Kw0\nh\ni1\n1\n1\n1\n1 2\n+ (Kw1 \u2212 Kw0 ) 2 (Kw1 \u2212 Kw0 )\u2212 2 (Kw2 \u2212 Kw0 )(Kw1 \u2212 Kw0 )\u2212 2 (Kw1 \u2212 Kw0 ) 2 .\n\n(50)\n\n17\n\n\fThus\nh\ni1\n1\n1\n1 2\n1\nA\u2217 = (Kw1 \u2212Kw0 ) 2 (Kw1 \u2212 Kw0 )\u2212 2 (Kw2 \u2212 Kw0 )(Kw1 \u2212 Kw0 )\u2212 2 (Kw1 \u2212Kw0 ) 2 \u2212Kw0 .\n(51)\nNow, if 0 \u227a A\u2217 \u227a Kx then we can appeal to Theorem 3 and arrive at the explicit\njointly Gaussian description scheme parameterized by Kw that achieves the sum rate.\nAnalogous to the scalar case (cf. [1]), we have the following sufficient condition for when\nthis is true; the proof is available in Appendix F.\nProposition 2. If the distortion constraints (D1 , D2 , D0 ) satisfy\n\nand\n\nD0 + K x \u2212 D1 \u2212 D2 \u227b 0\n\u22121\n\u22121\n\u22121\nD\u22121\n0 + Kx \u2212 D1 \u2212 D2 \u227b 0,\n\n(52)\n\nthen 0 \u227a A\u2217 \u227a Kx .\nWe now complete the proof by considering the cases that are not covered by the conditions in Proposition 2.\n\u2022 When\n\u22121\n\u22121\n\u22121\nD\u22121\n0 + Kx \u2212 D1 \u2212 D2 4 0,\n\nwe can choose A\u2217 = 0 to achieve the sum of point-to-point individual rate-distortion\nfunctions. Thus in this case, the sum rate is equal to this natural lower bound.\n\u2022 When\nD0 + Kx \u2212 D1 \u2212 D2 4 0,\nwe can choose A\u2217 = Kx to achieve the point-to-point rate distortion-function for\ncentral receiver, also a natural lower bound.\n\u22121\n\u22121\n\u22121\n\u2022 When neither D0 + Kx \u2212 D1 \u2212 D2 nor D\u22121\nis positive or\n0 + K x \u2212 D1 \u2212 D2\nnegative semidefinite (this case cannot happen in the scalar case), we cannot use\nTheorem 3, and the trivial choice of A\u2217 = 0 or A\u2217 = Kx does not meet the lower\nbound. In the next subsection we will address this case and prove that the jointly\nGaussian description scheme indeed achieves the lower bound on the sum rate for\nL \u2265 2.\n\nIf we let the source to be scalar Gaussian, our result reduces to Ozarow's solution of\nthe two description problem for a scalar Gaussian source [1]: this is because the last case\ndescribed above does not happen in the scalar case.\n\n18\n\n\f6.2\n\nSolutions for L \u2265 2\n\nWhile we exactly characterized the optimal jointly Gaussian description scheme and\nused this characterization in arguing that it achieves the fundamental lower bound to\nthe sum rate, such exact calculations do not appear to be as immediate when L > 2. So,\nwe eschew this somewhat brute-force approach and resort to a more subtle proof that\ninvolves exploring the structure of the solution to an optimization problem. First, note\nthat by a linear transformation at the encoders and the decoders, we have the following\nresult on rate region for multiple description with individual and central receivers.\nProposition 3.\n\u22121\n\n\u22121\n\n\u22121\n\n\u22121\n\n\u22121\n\n\u22121\n\nR\u2217 (Kx , D1 , . . . , , DL , D0 ) = R\u2217 (I, Kx 2 D1 Kx 2 , . . . , , Kx 2 DL Kx 2 , Kx 2 D0 Kx 2 ).\n(53)\nThus, throughout this subsection we will suppose, for notation simplicity, that Kx = I.\nGiven distortion constraints (D1 , . . . DL , D0 ), let\n\u22121\nKwl = (D\u22121\nl \u2212 I) ,\n\nl = 0, 1, . . . , L,\n\n(54)\n\nand define\ndef\n\nf (A) =\n\n[Kw0 + A]\u22121 \u2212\n\ndef\n\nF (A) = log |Kw0 + A| \u2212\n\nL\nP\n\nl=1\nL\nP\n\n[Kwl + A]\u22121 ,\n\n(55)\n\nlog |Kwl + A|.\n\n(56)\n\nl=1\n\nNote that\n\ndF (A)\n= f (A).\ndA\nConsider the following optimization problem:\nmax\n\nF (A).\n\n04A4I\n\n(57)\n\n(58)\n\nNow, since F (A) is a continuous map and 0 4 A 4 I is a compact set, there exists an optimal solution A\u2217 to (58) where A\u2217 satisfies the Karush-Kuhn-Tucker (KKT) conditions:\nthere exist \u039b1 < 0 and \u039b2 < 0 such that\nf (A\u2217 ) + \u039b1 \u2212 \u039b2 = 0\n\u039b1 A\u2217 = 0\n\u039b2 (A\u2217 \u2212 I) = 0.\nNow A\u2217 falls into the following four cases.\n19\n\n(59)\n(60)\n(61)\n\n\fCase 1: 0 \u227a A\u2217 \u227a I. Alternatively, 0 and 1 are not eigenvalues of A\u2217 . In this case,\n\u039b1 = 0 and \u039b2 = 0; thus the KKT conditions in (59) reduce to\nf (A\u2217 ) = 0.\nEquivalently,\n\u2217 \u22121\n\n[Kw0 + A ]\n\n=\n\nL\nX\n\n[Kwl + A\u2217 ]\u22121 .\n\n(62)\n\nl=1\n\nFrom Theorem 3, the jointly Gaussian description scheme with covariance matrix for\nw1 , . . . , wL being\n\uf8f6\n\uf8eb\nKw1 \u2212A\u2217 \u2212A\u2217\n...\n\u2212A\u2217\n\uf8ec\u2212A\u2217 Kw2 \u2212A\u2217\n...\n\u2212A\u2217 \uf8f7\n\uf8f7\n\uf8ec\n\uf8f7\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n(63)\nKw = \uf8ec\n\uf8f7\n\uf8ec\n\u2217\n\u2217\n\u2217\n\uf8ed\u2212A\n. . . \u2212A KwL\u22121 \u2212A \uf8f8\n\u2212A\u2217 . . . \u2212A\u2217 \u2212A\u2217 KwL\n\nachieves the lower bound to the sum rate. Thus in this case, we have characterized the\noptimality of the jointly Gaussian description scheme parameterized by (63) in terms of\nachieving the sum rate.\nCase 2: 0 4 A\u2217 \u227a I. Alternatively, some eigenvalues of A\u2217 are 0, but no eigenvalues\nof A\u2217 are 1. Thus \u039b2 = 0 and the KKT conditions in (59) reduce to\n\u2217 \u22121\n\n(Kw0 + A )\n\n\u2212\n\nL\nX\n\n(Kwl + A\u2217 )\u22121 + \u039b1 = 0,\n\n(64)\n\nl=1\n\nfor some \u039b1 < 0 satisfying \u039b1 A\u2217 = 0. The key idea now is to see that the distortion\nconstraint on the central receiver is too loose and we can in fact achieve a lesser distortion\n(in the sense of positive semidefinite ordering) for the same sum rate. We first identify\nthis lower distortion: defining\n\u0001\u22121\nK\u2217w0 = K\u22121\n,\nw0 + \u039b1\nconsider the smaller distortion matrix on the central receiver\n\u0001\u22121\n\u0001\u22121\n\u22121\nD\u22170 = K\u2217w0 \u22121 + I\n= I + K\u22121\n= (D\u22121\n\u227a D0 .\nw0 + \u039b1\n0 + \u039b1 )\n\nThis new distortion matrix on the central receiver satisfies two key properties, that we\nstate as a lemma (whose proof is available in Appendix G).\nLemma 4.\n(Kw0 + A\u2217 )\u22121 + \u039b1 = (K\u2217w0 + A\u2217 )\u22121 ,\n|D0 + Kz |\n|D\u22170 + Kz |\n=\n.\n|D0 |\n|D\u22170 |\n20\n\n(65)\n(66)\n\n\fComparing (64) with (65), we have\n\u0002\n\nK\u2217w0 + A\u2217\n\n\u0003\u22121\n\n=\n\nL\nX\n\n[Kwl + A\u2217 ]\u22121 .\n\n(67)\n\nl=1\n\nNow, the corresponding Kz = (I \u2212 A\u2217 )\u22121 \u2212 I is singular. If it hadnt been, then by\nTheorem 3 we could have concluded that jointly Gaussian description scheme achieves\nthe lower bound to the sum rate. We now address this technical difficulty.\nOur first observation is that there exists \u03b4 > 0 such that for all \u01eb \u2208 (0, \u03b4) we have\n0 \u227a A + \u01ebI \u227a I, and 0 \u227a K\u2217w0 \u2212 \u01ebI, 0 \u227a Kwl \u2212 \u01ebI, and we can rewrite (67) as\n\u0002\n\n(K\u2217w0\n\n\u2217\n\n\u2212 \u01ebI) + (A + \u01ebI)\n\n\u0003\u22121\n\nL\nX\n\u0002\n\u0003\u22121\n(Kwl \u2212 \u01ebI) + (A\u2217 + \u01ebI) .\n=\n\n(68)\n\nl=1\n\nThus if the distortion constraints were (D1 (\u01eb), . . . , DL (\u01eb), D0 (\u01eb)) with\n\u0002\n\u0003\u22121\nDl (\u01eb) = (Kwl \u2212 \u01ebI)\u22121 + I\n, l = 1, . . . , L,\n\u0002 \u2217\n\u0003\u22121\n\u22121\nD0 (\u01eb) = (Kw0 \u2212 \u01ebI) + I\n,\n\nthen A\u2217 + \u01ebI is a solution to (68). This situation corresponds to that discussed in Case I;\nwe can conclude that sum rate for this modified distortion multiple description problem\nis\n1\n|I + Kz (\u01eb)|(L\u22121) |D0 (\u01eb) + Kz (\u01eb)|\n,\n(69)\nlog\nL\nQ\n2\n|D0 (\u01eb)| |Dl (\u01eb) + Kz (\u01eb)|\nl=1\n\n\u2217\n\n\u22121\n\nwhere Kz (\u01eb) = [I \u2212 (A + \u01ebI)] \u2212 I. We would like to let \u01eb approach zero and consider\nthe limiting multiple description problem. In particular, we show that\nDl (\u01eb) \u2192 Dl ,\nD0 (\u01eb) \u2192 D\u22170 ,\n\nl = 1, . . . , L,\n\n(70)\n(71)\n\nas \u01eb \u2192 0 in Appendix H. Further, we show that\nKz (\u01eb) \u2192 (I \u2212 A\u2217 )\u22121 \u2212 I,\n\n(72)\n\nas \u01eb \u2192 0 in Appendix I. Thus we can conclude that the sum rate approaches, using (66),\n1\n|I + Kz |(L\u22121) |D0 + Kz |\n,\nlog\nL\nQ\n2\n|D0 | |Dl + Kz |\n\n(73)\n\nl=1\n\n\u2217 \u22121\n\nas \u01eb \u2192 0; here Kz = (I \u2212 A ) \u2212 I. We observe that this sum rate is achievable using\nthe jointly Gaussian multiple scheme. Further, this sum rate is identical to the lower\n21\n\n\fbound to sum rate for the original distortions (D1 , . . . , DL , D0 ). Thus we conclude the\noptimality of the jointly Gaussian description scheme in this case as well.\nCase 3: 0 \u227a A\u2217 4 I. Alternatively, some eigenvalues of A\u2217 are 1, but no eigenvalues\nof A\u2217 are 0. In this case, the \u039b1 = 0 and the KKT conditions in (59) reduce to\n\u2217 \u22121\n\n(Kw0 + A )\n\n\u2212\n\nL\nX\n\n(Kwl + A\u2217 )\u22121 \u2212 \u039b2 = 0,\n\n(74)\n\nl=1\n\nfor some \u039b2 < 0 satisfying \u039b2 (A\u2217 \u2212 I) = 0. Defining\n\u0002\n\u0003\u22121\nK\u2217wl = (Kwl + I)\u22121 + \u039b2\n\u2212 I,\n\nwe have, as in (65), that\n\n(Kwl + A\u2217 )\u22121 + \u039b2 = (K\u2217wl + A\u2217 )\u22121 .\n\n(75)\n\nThe observation\n(Kwl + A\u2217 )\u22121 + \u039b2 = [(Kwl + I) + (A\u2217 \u2212 I)]\u22121 + \u039b2 ,\ncombined with the proof of (65) suffices to justify (75). Now, from (75),\n\u2217 \u22121\n\n(Kw0 + A )\n\n\u2212\n\nL\u22121\nX\n\n(Kwl + A\u2217 )\u22121 \u2212 (K\u2217wL + A\u2217 )\u22121 = 0.\n\n(76)\n\nl=1\n\nAs in the previous case, the key step is to identify smaller distortion matrices at each of\nthe individual receivers (ordered in the positive semidefinite sense) that is achievable at\nthe same sum rate:\n\u0003\u22121\n\u0002\nD\u2217l = K\u2217wl \u22121 + I\n, l = 1, . . . , L.\n\nTo see that this is indeed a smaller distortion matrix, observe that since Kw is positive\ndefinite, it follows that K\u2217wl \u227b 0 and\n\u0003\u22121\n\u0002\nD\u2217l = K\u2217wl \u22121 + I\n\u0014\u0010\n\u0015\u22121\n\u0011\u22121\n\u0001\u22121\n\u22121\n=\n(Kwl + I) + \u039b2\n\u2212I\n+I\n\u0002\n\u0003\n(77)\n= I \u2212 (Kwl + I)\u22121 \u2212 \u039b2\n= [I + Kwl ]\u22121 \u2212 \u039b2\n= Dl \u2212 \u039b2 , l = 1, . . . , L.\n\nSince \u039b2 < 0, it follows that 0 \u227a D\u2217l 4 Dl , l = 1, . . . , L. Define\n\u0002\n\u0003\u22121\nDl (\u01eb) = (Kwl + \u01ebI)\u22121 + I\n, l = 0, 1, . . . , L \u2212 1,\n\u0002 \u2217\n\u0003\n\u22121\nDL (\u01eb) = (KwL + \u01ebI)\u22121 + I\n,\n22\n\n(78)\n\n\fthen there exists \u03b4 > 0 such that for all \u01eb \u2208 (0, \u03b4) we have 0 \u227a A\u2217 \u2212 \u01ebI \u227a I, and\n0 \u227a Dl (\u01eb) \u227a I. We can rewrite (76) as\n[(Kw0 + \u01ebI) + (A\u2217 \u2212 \u01ebI)]\u22121 =\n\nL\u22121\nX\nl=1\n\n\u0002\n\u0003\u22121\n[(Kwl + \u01ebI) + (A\u2217 \u2212 \u01ebI)]\u22121 + (K\u2217wL + \u01ebI) + (A\u2217 \u2212 \u01ebI)\n.\n\n(79)\nThus if the distortion constraints were (D1 (\u01eb), . . . , DL (\u01eb), D0 (\u01eb)), then A\u2217 \u2212 \u01ebI is a\nsolution to (79). This situation corresponds to that discussed in Case I; we conclude that\nthe sum rate for this modified distortion multiple description problem is\n1\n|I + Kz (\u01eb)|(L\u22121) |D0 (\u01eb) + Kz (\u01eb)|\n,\nlog\nL\nQ\n2\n|D0 (\u01eb)| |Dl (\u01eb) + Kz (\u01eb)|\n\n(80)\n\nl=1\n\nwhere Kz (\u01eb) = [I \u2212 (A\u2217 \u2212 \u01ebI)]\u22121 \u2212 I. We would like to let \u01eb approach zero and consider\nthe limiting multiple description problem. Similar to equations (70) and (71), we have\nDl (\u01eb) \u2192 Dl ,\nD0 (\u01eb) \u2192 D\u22170 .\n\nl = 1, . . . L,\n\n(81)\n\nFurther, we show that\n|I + Kz (\u01eb)|(L\u22121) |D0 (\u01eb) + Kz (\u01eb)|\n=1\nL\n\u01eb\u21920\nQ\n| |Dl (\u01eb) + Kz (\u01eb)|\nlim\n\n(82)\n\nl=1\n\nin Appendix J. We can now conclude that the sum rate approaches\n1\n1\nlog\n2\n|D0 |\n\n(83)\n\nas \u01eb approaches 0. In other words, the point-to-point rate-distortion function for central\nreceiver with distortion D0 can be achieved by using the jointly Gaussian description\nscheme, and the resulting distortion is (D1 , . . . , D\u2217L , D0 ) where 0 \u227a D\u2217L 4 DL . In\nconclusion, the jointly Gaussian description scheme is also optimal in this case.\nCase 4: 0 4 A\u2217 4 I. i.e., both 0 and 1 are eigenvalues of A\u2217 . In this case, the KKT\nconditions are: there exist \u039b1 < 0 and \u039b2 < 0 such that equations (59), (60) and (61)\nhold. We can combine equations (65) and (75) to get\n(K\u2217w0 + A\u2217 )\u22121 =\n\nL\u22121\nX\n\n(Kwl + A\u2217 )\u22121 + (K\u2217wL + A\u2217 )\u22121 ,\n\nl=1\n\nwhere\n\n\u0001\u22121\nK\u2217w0 = K\u22121\n+\n\u039b\n,\n1\nw0\n\u0002\n\u0003\u22121\nK\u2217wL = (KwL + I)\u22121 + \u039b2\n\u2212 I.\n23\n\n(84)\n\n\fAs in cases 2 and 3, we want to show the optimality of the jointly Gaussian multiple\ndescription scheme through a limiting procedure. We do this by first perturbing A\u2217 so\nthat it has no eigenvalue equal to 0 or 1 as follows.\nWithout loss of generality, suppose that A\u2217 has p eigenvalues equal to 0 and q eigenvalues equal 1, where p > 0 and q > 0, and there exists N \u00d7 N orthogonal matrix Q such\nthat\nQA\u2217 Qt = diag{0, . . . , 0, 1, . . . , 1, ap+q+1 , . . . , aN },\n| {z } | {z }\np\n\nq\n\nwith 0 < ap+q+1 < 1, . . . , 0 < aN < 1. We need to perturb the eigenvalues of A\u2217 away\nfrom both 0 and 1. Towards this, we define two N \u00d7 N diagonal matrices:\nE1 = diag(1, . . . , 1, 0, . . . , 0, 0, . . . , 0),\n{z\n}\n| {z } |\np\n\nN \u2212p\n\nE2 = diag(0, . . . , 0, 1, . . . , 1, 0, . . . , 0),\n| {z } | {z }\np\n\nAlso define\n\nq\n\nA\u2217 (\u01eb1 , \u01eb2 ) = A\u2217 + Qt (\u01eb1 E1 \u2212 \u01eb2 E2 )Q,\nKz (\u01eb1 , \u01eb2 ) = (I \u2212 A\u2217 (\u01eb1 , \u01eb2 ))\u22121 \u2212 I,\nKwl (\u01eb1 , \u01eb2 ) = Kwl \u2212 Qt (\u01eb1 E1 \u2212 \u01eb2 E2 )Q, l = 1, . . . , L \u2212 1,\nKwL (\u01eb1 , \u01eb2 ) = K\u2217wL \u2212 Qt (\u01eb1 E1 \u2212 \u01eb2 E2 )Q,\nKw0 (\u01eb1 , \u01eb2 ) = K\u2217w0 \u2212 Qt (\u01eb1 E1 \u2212 \u01eb2 E2 )Q.\nFurther, defining\nDl (\u01eb1 , \u01eb2 ) = (I + Kwl (\u01eb1 , \u01eb2 ))\u22121 ,\n\nl = 1, . . . , L,\n\n(85)\n\nthere exists \u03b4 > 0 such that for all \u01eb1 \u2208 (0, \u03b4) and \u01eb2 \u2208 (0, \u03b4) we have 0 \u227a A\u2217 (\u01eb1 , \u01eb2 ) \u227a I,\nand 0 \u227a Dl (\u01eb1 , \u01eb2 ) \u227a I. Now, we can rewrite (84) as\nL h\ni\u22121\nh\ni\u22121 X\nKwl (\u01eb1 , \u01eb2 ) + A\u2217 (\u01eb1 , \u01eb2 ) .\nKw0 (\u01eb1 , \u01eb2 ) + A\u2217 (\u01eb1 , \u01eb2 )\n=\n\n(86)\n\nl=1\n\nThus if the distortion constraints were (D1 (\u01eb1 , \u01eb2 ), . . . , DL (\u01eb1 , \u01eb2 ), D0 (\u01eb1 , \u01eb2 )), then\nA\u2217 (\u01eb1 , \u01eb2 ) is a solution to (86). This situation corresponds to that discussed in Case I;\nwe conclude that the sum rate for this modified distortion multiple description problem\nis\n|I + Kz (\u01eb1 , \u01eb2 )|(L\u22121) |D0 (\u01eb1 , \u01eb2 ) + Kz (\u01eb1 , \u01eb2 )|\n1\n,\n(87)\nlog\nL\nQ\n2\n|D0 (\u01eb1 , \u01eb2 )| |Dl (\u01eb1 , \u01eb2 ) + Kz (\u01eb1 , \u01eb2 )|\nl=1\n\n24\n\n\fwhere Kz (\u01eb1 , \u01eb2 ) = [I \u2212 A\u2217 (\u01eb1 , \u01eb2 )]\u22121 \u2212I. We would like to let \u01eb1 and \u01eb2 approach zero and\nconsider the limiting multiple description problem. Similar to equations (70) and (71),\nwhen \u01eb1 and \u01eb2 approach 0, we get\nDl (\u01eb1 , \u01eb2 ) \u2192 Dl , l = 1, . . . , L \u2212 1,\nDL (\u01eb1 , \u01eb2 ) \u2192 D\u2217L ,\nD0 (\u01eb1 , \u01eb2 ) \u2192 D\u22170 ,\n\n(88)\n\n\u22121 \u22121\nwhere D\u2217L = DL \u2212 \u039b2 as in case 3 and D\u22170 = [D\u22121\nas in case 2. Further, we\n0 + \u039b1 ]\nshow that\n1\n|I + Kz (\u01eb1 , \u01eb2 )|(L\u22121) |D0 (\u01eb1 , \u01eb2 ) + Kz (\u01eb1 , \u01eb2 )|\n1\n1\nlim lim log\nlog\n(89)\n=\nL\n\u01eb2 \u21920 \u01eb1 \u21920 2\nQ\n2\n|D0 |\n|D0 (\u01eb1 , \u01eb2 )| |Dl (\u01eb1 , \u01eb2 ) + Kz (\u01eb1 , \u01eb2 )|\nl=1\n\nin Appendix K. We conclude that the sum rate approaches\n1\n1\nlog\n2\n|D0 |\n\n(90)\n\nas \u01eb1 and \u01eb2 approach 0. Thus the point-to-point rate-distortion function for central\nreceiver with distortion D0 can be achieved by using the jointly Gaussian description\nscheme, and the resulting distortions are (D1 , . . . , D\u2217L , D\u22170 ) where 0 \u227a D\u2217L 4 DL and\n0 \u227a D\u22170 4 D0 . In other words, the jointly Gaussian multiple description scheme is also\noptimal in this case.\nTo summarize, we see that the jointly Gaussian description scheme achieves the limiting\nsum rate. The limiting sum rate is the solution to an optimization problem. For some\nspecific distortion constraints, the sum rate can be characterized as the solution to a\nmatrix polynomial equation (Case I).\n\n6.3\n\nRate Region for Two Descriptions\n\nApplying the result in Section 6.2 to the case of L = 2, i.e., the two description problem,\nwe can see that jointly Gaussian description scheme achieves the optimal sum rate. This\nresolves the case left out in Section 6.1. It also turns out that in the two description\nproblem, we can show that jointly Gaussian description strategy achieves the entire rate\nregion. This is the main result of this subsection.\nFrom Section 3 we have a outer bound to the rate region for the two description problem\n\uf8f1\n(R1 , R2 ) :\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n1\n|Kx |\n\uf8f2\nRl \u2265 log\n, l = 1, 2\nRout (Kx , D1 , D2 , D0 ) =\n2\n|Dl |\n\uf8f4\n\uf8f4\n|Kx ||Kx + Kz ||D0 + Kz |\n1\n\uf8f4\n\uf8f4\n\uf8f3 R1 + R2 \u2265 sup log\n|D0 ||D1 + Kz ||D2 + Kz |\nKz \u227b0 2\n25\n\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\n\n. (91)\n\n\fFollowing the discussion in Section 6.2, we show in the following that the jointly Gaussian description strategy (jointly Gaussian multiple description schemes and the time\nsharing between them) achieves the outer bound to the rate region.\nLet\n\u22121 \u22121\nKwl = (D\u22121\nl \u2212 Kx ) ,\n\nl = 0, 1, 2\n\nand\nF (A) = log |Kw0 + A| \u2212 log |Kw1 + A| \u2212 log |Kw2 + A|.\nNow consider the optimization problem:\nF (A).\n\nmax\n\n04A4Kx\n\n(92)\n\nAs in Section 6.2, the optimal solution A\u2217 falls into four cases.\nCase 1: 0 \u227a A\u2217 \u227a Kx . In this case, we know from Section 4 that the rate pair\n(R1 , R2 ) satisfying\n\uf8f1\n\uf8fc\n(R1 , R2 ) :\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n|Kx + Kwl |\n1\n\uf8f2\n\uf8fd\n, l = 1, 2\nRl \u2265 log\n(93)\n2\n|Kwl |\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n1\n|Kx + Kw1 ||Kx + Kw2 | \uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3 R1 + R2 \u2265 log\n\uf8fe\n2\n|Kw |\n\nis achievable using the jointly Gaussian multiple description scheme with the covariance\nmatrix of w1 , w2 being\n\u0013\n\u0012\nKw1 \u2212A\u2217\n.\nKw =\n\u2212A\u2217 Kw2\nDenoting the resulting distortions as (D1 , D2 , D0 ), we readily calculate\n|Kx + Kwl |\n1\n1\n|Kx |\n1\n\u22121\nlog\n= log |Kx ||K\u22121\nlog\nwl + Kx | =\n2\n|Kwl |\n2\n2\n|Dl |\n\nfor l = 1, 2. From the discussion in Section 6.2, we know that the lower bound to sum\nrate is achieved using this jointly Gaussian description scheme. Thus, in this case, the\njointly Gaussian description scheme achieves the rate region. As an aside, we note in this\ncase that, A\u2217 satisfies\n\u0002 \u2217\n\u0003\u22121\nKw0 + A\u2217\n= [Kw1 + A\u2217 ]\u22121 + [Kw2 + A\u2217 ] ,\nand, from the discussion in Section 6.1, that a sufficient condition for this case to happen\nis (52).\n\nCase 2: 0 4 A\u2217 \u227a Kx . This case is similar to case 1: the jointly Gaussian description\nscheme with covariance matrix for w1 , w2 being\n\u0013\n\u0012\nKw1 \u2212A\u2217\n.\nKw =\n\u2212A\u2217 Kw2\n26\n\n\fachieves the lower bound on the rate region. We note that in this case the resulting\ndistortions are (D1 , D2 , D\u22170 ), with D\u22170 4 D0 . Further, we know from the discussion in\n6.1, that a sufficient condition for this case to happen is\n\u22121\n\u22121\n\u22121\nD\u22121\n0 + Kx \u2212 D1 \u2212 D2 4 0.\n\nCase 3: 0 \u227a A\u2217 4 Kx . In this case, we know from the discussion in Section 6.2 that\nfor another two description problem with distortions (D1 , D\u22172 , D0 ) such that D\u22172 4 D2 ,\nthe jointly Gaussian description scheme with covariance matrix for w1 , w2 being\n\u0013\n\u0012\nKw1 \u2212A\u2217\nKw =\n\u2212A\u2217 K\u2217w2\n\u0010\n\u0011\nx|\nachieves the lower bound to sum rate 21 log |K\nto the original distortions (D1 , D2 , D0 ).\n|D0 |\nWe can see, from the contra-polymatroid structure of the achievable region of jointly\nGaussian description scheme, that the corner point\n\u0012\n\u0013\n1\n|Kx | 1\n|Kx | 1\n|Kx |\nB1 =\nlog\n, log\n\u2212 log\n2\n|D1 | 2\n|D0 | 2\n|D1 |\nin Figure 3 is achievable by this jointly Gaussian description scheme.\nNow observe that the discussion in case 3 of Section 6.2 is symmetric with respect to\nthe individual receivers. Thus, by exchanging the role of receiver 1 and receiver 2, we\ncan achieve the other corner point\n\u0012\n\u0013\n1\n|Kx | 1\n|Kx | 1\n|Kx |\nB2 =\nlog\n\u2212 log\n, log\n2\n|D0 | 2\n|D2 | 2\n|D2 |\nin Figure 3 by another appropriate jointly Gaussian description scheme. Finally, time\nsharing between these two jointly Gaussian multiple description schemes allows us to\nachieve the lower bound on the rate region. As an aside, we note, as a consequence of\nthe discussion in Section 6.1, that a sufficient condition for this case to happen is\nD0 + Kx \u2212 D1 \u2212 D2 4 0.\nCase 4: 0 4 A\u2217 4 Kx . In this case, we know, from the discussion in Section 6.2, that\nfor another two description problem with distortions (D1 , D\u22172 , D\u22170 ) such that D\u22172 4 D2\nand D\u22170 4 D0 , the jointly Gaussian description scheme with covariance matrix for w1 , w2\nbeing\n\u0013\n\u0012\nKw1 \u2212A\u2217\nKw =\n\u2212A\u2217 K\u2217w2\n\u0010\n\u0011\n|Kx |\n1\nachieves the lower bound to sum rate 2 log |D0 | to the original distortions (D1 , D2 , D0 ).\nUsing an argument entirely analogous to that applied that the jointly Gaussian description strategy achieves the rate region.\n27\n\n\fTo summarize: the jointly Gaussian description strategy achieves the rate region for\nthe two description problem. For a class of distortion constraints, the corner points of\nthe rate region can be characterized by solving a matrix polynomial equation, as already\nseen in Section 6.1.\n\n7\n\nDiscussions\n\nAlthough multiple description for individual and central receivers is a special case of the\nmost general multiple description problem, the solution to this problem sheds substantial\ninsight to the issue-at-large. In this section, we discuss two instances of other multiple\ndescription problems that can be resolved using the insights developed so far. In particular, we discuss the problem of two descriptions with separate distortion constraints and\nthe general multiple description problem for some special sets of distortion constraints.\n\n7.1\n\nTwo Description with Separate Distortion Constraints\n\nThe problem of two descriptions with separate distortion constraints is ilustrated in\nFigure 2. Suppose the vector Gaussian source x[m] = (x1 [m], x2 [m]), the dimension of\nx1 [m] is N1 and the dimension of x2 [m] is N2 . This implies that the dimension of x[m] is\nN = N1 +N2 . Let Kx = E[x[m]t x[m]], Kx1 = E[x1 [m]t x1 [m]], and Kx2 = E[x2 [m]t x2 [m]].\nThere are two encoders at the source providing two descriptions of x[m]. There are three\nreceivers: the individual receivers 1 and 2 are only interested in generating reproduction\nof x1 [m] with mean square distortion constraint D1 (an N1 \u00d7 N1 positive definite matrix)\nfrom description 1 and x2 [m] with mean square distortion constraint D2 (an N2 \u00d7 N2\npositive definite matrix) from description 2, respectively. The central receiver uses both\ndescriptions to generate a reproduction of x[m] with the error covariance meeting a\ndistortion constraint D0 (an N \u00d7 N positive definite matrix) from both descriptions.\nThis situation is closely related to the two description problem and we can harness our\nresults thus far to completely characterize the rate region of the problem at hand.\nTheorem 4. The rate region of two description with separate distortion constraints is\n[\nR(D1 , D2 , D0 ) =\nR\u2217 (D\u20321 , D\u20322 , D0 ),\n(94)\n\u03a5(D\u20321 , D\u20322 )\n\nwhere \u03a5(D\u20321 , D\u20322 ) is defined as\ndef\n\n\u03a5(D\u20321 , D\u20322 ) =\n\n\b\n\n(D\u20321 , D\u20322 ) : (D\u20321 ){1,...,N1 } 4 D1 , (D\u20322 ){N1 +1,...,N } 4 D2\n\n28\n\n.\n\n(95)\n\n\fProof. It is clear that any rate pair (R1 , R2 ) \u2208 R\u2217 (D\u20321 , D\u20322 , D0 ) for some (D\u20321 , D\u20322 ) \u2208\n\u03a5(D\u20321 , D\u20322 ) is in the rate region for the two description with separate distortion constraints, and so\nR\u2217 (D\u20321 , D\u20322 , D0 ) \u2286 R(D1 , D2 , D0 ).\nOn the other hand, although receiver 1 (2) is only interested in reconstructing x1 (x2 ),\nthey can actually reconstruct the entire source x based on their received descriptions.\nHence, any coding scheme for the two description with separate distortion constraints\nwill result in some achievable distortions (D\u20321 , D\u20322 , D\u20320 ) with (D\u20321 , D\u20322 ) \u2208 \u03a5(D\u20321 , D\u20322 )\nand D\u20320 4 D0 . Thus any rate pair (R1 , R2 ) \u2208 R(D1 , D2 , D0 ) achieved by this coding\nscheme is in the rate region R\u2217 (D\u20321 , D\u20322 , D0 ) for the two description problem. Thus\n[\nR(D1 , D2 , D0 ) \u2286\nR\u2217 (D\u20321 , D\u20322 , D0 ).\n\u03a5(D\u20321 , D\u20322 )\n\nFrom equivalence of the two regions in (94), the proof is now complete.\n\n7.2\n\nGeneral Gaussian Multiple Description Problem for Special\nChoices of Distortion Constraints\n\nConsider the general Gaussian multiple description problem with source covariance Kx\nand 2L \u2212 1 distortion constraints DS for each S \u2286 {1, . . . , L}.\nFollowing arguments similar to that used in arriving at the lower bound (11) for sum\nrate, we have an outer bound on the rate region:\n\uf8fc\n\uf8f1\n\uf8fd\n\uf8f2 (R1 , . . . , RL ) :\nP\n(|S|\u22121) |D +K |\n|Kx ||Kx +KQ\nz|\nz\n1\nS\nRout (Kx , D1 , . . . , DL , D0 ) =\n, \u2200S \u2286 {1, . . . , L} \uf8fe .\nRl \u2265 2 log\n\uf8f3\n|Dl +Kz |\n|DS |\nl\u2208S\n\nl\u2208S\n\n(96)\nFollowing arguments similar to those used in arriving at the upper bound (20) for the\nsum rate, we can use a jointly Gaussian description scheme with covariance matrix of\nwl 's (Kw ) taking the form (23), any tuple (R1 , . . . , RL ) satisfying\n\uf8f1\n\uf8fc\n(R1 , . . . , RL ) :\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n(|S|\u22121)\n\uf8f2\n\uf8fd\nK\nK\n+K\nCov[x|u\n,\nl\u2208S]+K\nx\nx\nz\nz\nl\nP\n(97)\nRl \u2265 12 log\n, \u2200S \u2286 {1, . . . , L} \uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nQ\n\uf8f3 l\u2208S\n\uf8fe\nCov[x|ul , l\u2208S]\nCov[x|ul ]+Kz\nl\u2208S\n\nis achievable. Thus if we can find a Kw of the form in (23) such that all of the 2L \u2212 1\ndistortion constraints are met with equality, i.e.,\n\u22121\nt \u22121\nDS = Cov[x|ul , l \u2208 S] = [K\u22121\nx + (I, . . . , I)KwS (I, . . . , I) ] ,\n\n29\n\n\u2200S \u2286 {1, . . . , L},\n(98)\n\n\fwhere KwS is the covariance matrix for all Kwl , l \u2208 S, then the achievable region matches\nthe outer bound and we would have characterized the rate region of the multiple description problem.\nFrom the above discussion, we see that for some choice of distortion constraints of\nthe multiple description problem, we can indeed do this: First choose L + 1 distortions\n(D1 , D2 , . . . , DL , D0 ) such that they satisfy the condition for Theorem 3 for the\nmultiple description problem with individual and central receivers. Next we can solve\nfor the Kw which is the covariance matrix of (w1 , . . . , wL ) for the sum-rate-achieving\njointly Gaussian description scheme. For any other S \u2286 {1, . . . , L}, this scheme results\n\u22121\nt \u22121\nin distortion DS = [K\u22121\nx + (I, . . . , I)KwS (I, . . . , I) ] . Finally we choose these DS 's as\nthe other distortion constraints. Now we have a general multiple description problem with\n2L \u2212 1 distortion constraints DS for each S \u2286 {1, . . . , L}, and hence we can find a Kw\nof form (23) such that all of the 2L \u2212 1 distortion constraints are met with equality. Thus\n(96) is actually the rate region and it can be achieved by a jointly Gaussian description\nscheme.\n\nAppendix\nA\n\nUseful Matrix Lemmas\n\nIn this appendix we provide some useful results in matrix analysis that are extensively\nused in this paper.\nLemma 5 (Matrix Inversion Lemma). [25, Theorem 2.5] Let A be an m \u00d7 m nonsingular matrix and B be an n \u00d7 n nonsingular matrix and let C and D be m \u00d7 n and\nn \u00d7 m matrices, respectively. If the matrix A + CBD is nonsingular, then\n(A + CBD)\u22121 = A\u22121 \u2212 A\u22121 C(B\u22121 + DA\u22121 C)\u22121 DA\u22121\nLemma 6. [25, Theorem 2.3] Suppose that the partitioned matrix\n\u0013\n\u0012\nA B\nM=\nC D\nis invertible and that the inverse is conformally partitioned as\n\u0013\n\u0012\nX Y\n\u22121\n.\nM =\nU V\n\n30\n\n\fIf A is a nonsingular principal sub-matrix of M, then\nX =A\u22121 + A\u22121 B(D \u2212 CA\u22121 B)\u22121 CA\u22121,\nY = \u2212 A\u22121 B(D \u2212 CA\u22121 B)\u22121 ,\nU = \u2212 (D \u2212 CA\u22121 B)\u22121 CA\u22121 ,\nV =(D \u2212 CA\u22121 B)\u22121 .\n\n(99)\n\nLemma 7. [25, Theorem 6.13] Let E \u2208 Mn be a positive definite matrix and let F be an\nn \u00d7 m matrix. Then for any m \u00d7 m positive definite matrix G,\n\u0013\n\u0012\nE F\n\u227b 0 \u21d0\u21d2 G \u227b Ft E\u22121F.\n(100)\nFt G\nLemma 8. [25, Theorem 6.8 and 6.9] Let A and B be positive definite matrices such\nthat A \u227b B (A < B). Then,\n|A| \u227b |B| (|A| < |B|),\nA\u22121 \u227a B\u22121 (A\u22121 4 B\u22121 ),\nA1/2 \u227b B1/2\n\nB\n\n(101)\n\n(A1/2 < B1/2 ).\n\nProof of Lemma 1\n\nDefine an i.i.d. random process {z[m]}, m = 1, . . . , n of N (0, Kz ) Gaussian random\nvectors, where z[m], m = 1, . . . , n are independent of xn and Cl , l = 1, . . . , L. Form a\nrandom process yn = (y[1], . . . , y[n])t by\ny[m] = x[m] + z[m],\n\n31\n\nm = 1, . . . , n.\n\n\fIt follows that {y[m]} is an i.i.d. random process of N (0, Ky ) Gaussian random vectors,\nwhere Ky = Kx + Kz . Then\nI(C1 ; C2 ; . . . ; CL ) + I(C1 , . . . , CL ; xn )\n=\n\u2265\n\nL\nX\n\nl=1\nL\nX\n\nH(Cl ) \u2212 H(C1 , * * * , CL ) + I(C1 , . . . , CL ; xn )\nH(Cl ) \u2212 H(C1 , * * * , CL ) + I(C1 , . . . , CL ; xn )\n\nl=1\n\n\u2212\n\nL\n\u0010X\n\nn\n\nn\n\nH(Cl |y ) \u2212 H(C1, . . . , CL |y )\n\nl=1\n\n=\n\nL\nX\n\n\u0011\n\n(h(yn ) \u2212 h(yn |Cl )) \u2212 h(yn ) + h(yn |C1 , . . . , CL ) + h(xn ) \u2212 h(xn |C1 , . . . , CL )\n\nl=1\n\nn\n\nn\n\n=h(x ) + (L \u2212 1)h(y ) \u2212\n\nL\nX\n\nh(yn |Cl ) + h(yn |C1 , . . . , CL ) \u2212 h(xn |C1 , . . . , CL ).\n\nl=1\n\n(102)\n\nSince xn and yn are Gaussian vectors, for the first two terms in (102), we have\n1\nh(xn ) = log(2\u03c0e)N n |Kx |n ,\n2\n1\n1\nh(yn ) = log(2\u03c0e)N n |Ky |n = log(2\u03c0e)N n |Kx + Kz |n .\n2\n2\n\n32\n\n(103)\n\n\fWe also have the following bound on h(yn |Cl ) for l = 1, . . . , L:\nh(yn |Cl ) \u2264\n\u2264\n\nn\nX\n\nh(y[m]|Cl )\n\nm=1\nn\nX\n\n1\nlog(2\u03c0e)N Cov[y[m]|Cl ]\n2\nm=1\n\nn\n1\nn\n1X\nNn\n\u2264 log(2\u03c0e) + log\nCov[y[m]|Cl ]\n2\n2\nn m=1\n\nn\nn\n1X\n1\nNn\nCov[(x[m] + z[m])|Cl ]\n= log(2\u03c0e) + log\n2\n2\nn m=1\n\n(104)\n\nn\n1\nn\n1X\nNn\n= log(2\u03c0e) + log\nCov[x[m]|Cl ] + Kz\n2\n2\nn m=1\n\nn\n1\nlog(2\u03c0e)N n + log |Dl + Kz |\n2\n2\n1\nNn\n= log(2\u03c0e) |Dl + Kz |n .\n2\n\n\u2264\n\nNext we bound the last two terms of (102) as follows.\nh(yn |C1 , . . . , CL ) \u2212 h(xn |C1, . . . , CL )\n= h(yn |C1 , . . . , CL ) \u2212 h(xn |zn , C1 , . . . , CL )\n= h(yn |C1 , . . . , CL ) \u2212 h(yn |zn , C1 , . . . , CL )\n= I(yn ; zn |C1 , . . . , CL ).\n\n(105)\n\nLetting\ndef\n\nKc [m] = Cov[x[m] \u2212 x\u03020 [m]],\n\n33\n\n(106)\n\n\fwe have\nI(yn ; zn |C1 , . . . , CL ) = h(zn |C1 , . . . , CL ) \u2212 h(zn |yn , C1, . . . , CL )\n= h(zn ) \u2212 h(zn |yn \u2212 x\u0302n0 , C1 , . . . , CL )\n\u2265 h(zn ) \u2212 h(zn |yn \u2212 x\u0302n0 )\nn\nX\n\u0001\n=\nh(z[m]) \u2212 h(z[m]|z[1], . . . , z[m \u2212 1], yn \u2212 x\u0302n0 )\n\u2265\n=\n(a)\n\n\u2265\n\n(b)\n\n\u2265\n\nm=1\nn\nX\n\nm=1\nn\nX\n\nh(z[m]) \u2212 h(z[m]|y[m] \u2212 x\u03020 [m])\n\n\u0001\n\n(107)\n\nI(z[m]; x[m] \u2212 x\u03020 [m] + z[m])\n\nm=1\nn\nX\n\n1\n|Kc [m] + Kz [m]|\nlog\n2\n|Kc [m]|\nm=1\n|D0 + Kz |\nn\nlog\n,\n2\n|D0 |\n\nwhere (a) is from (106) and [26, Lemma II.2]. The justfication for (b) is from the convexity\nof log |A+B|\nin A and (9). From (105) and (107) we have\n|B|\nh(yn |C1 , . . . , CL ) \u2212 h(xn |C1 , . . . , CL ) \u2265\n\nn\n|D0 + Kz |\nlog\n.\n2\n|D0 |\n\n(108)\n\nCombining (102), (103) and (108), we have\nI(C1 ; C2 ; . . . ; CL ) + I(C1 , . . . , CL ; xn ) \u2265\n\nn\n|Kx ||Kx + Kz |(L\u22121) |D0 + Kz |\n. (109)\nlog\nL\nQ\n2\n|D0 | |Dl + Kz |\nl=1\n\nTaking the supremum over all positive definite Kz , we can sharpen the lower bound in\n(109):\nL\nX\nl=1\n\nn\n|Kx ||Kx + Kz |(L\u22121) |D0 + Kz |\nI(C1 ; C2 ; . . . ; CL ) + I(C1 , . . . , CL ; x ) \u2265 sup log\n.\nL\nQ\nKz \u227b0 2\n|D0 | |Dl + Kz |\nn\n\nl=1\n\n(110)\n\n34\n\n\fC\n\nProof of Proposition 1\n\nConditioned on y, the collection of random variables (u1 , . . . , uL ) are jointly Gaussian\nand thus we have\nL\nQ\n|Cov[ul |y]|\nL\nX\n1\nl=1\nh(ul |y) \u2212 h(u1 , . . . , uL |y) = log\n.\n(111)\n2\nCov[u\n,\n.\n.\n.\n,\nu\n|y]\n1\nL\nl=1\nFrom MMSE of ul from y we have\n\nCov[ul |y] = Kx + Kwl \u2212 Kx (Kx + Kz )\u22121 Kx ,\nand\n\nl = 1, . . . , L\n\n\u0001\nCov(u1 , . . . , uL |y) = J \u2297 Kx + Kw \u2212 J \u2297 Kx (Kx + Kz )\u22121 Kx ,\n\n(112)\n(113)\n\nwhere J is an L \u00d7 L matrix of all ones and \u2297 is the Kronecker Product.\n\nBy Fischer inequality (the block matrix version of Hadamard inequality, see [25, TheL\nQ\n|Cov[ul |y]| = Cov[u1 , . . . , uL |y] if and only if the offorem 6.10]) we know that\nl=1\n\ndiagonal block matrices of Cov[u1 , . . . , uL |y] are all zero matrices. Thus we have\nL\nX\n\nh(ul |y) \u2212 h(u1 , . . . , uL |y) = 0\n\nl=1\n\nif and only if\n\nKx \u2212 A = Kx (Kx + Kz )\u22121 Kx ,\n\n(114)\n\nor equivalently, if and only if\nKz = Kx (Kx \u2212 A)\u22121 Kx \u2212 Kx .\n\n(115)\n\nTo get a valid Kz \u227b 0, we need the additional condition 0 \u227a A \u227a Kx .\n\nD\n\nProof of Lemma 2\n\nFirst we assume A \u227b 0, and hence\n\u0002 \u22121\nt \u0003\u22121\nA + (I I . . . I) K\u22121\nw (I I . . . I)\n\u0002\n\u0003\u22121\n=A \u2212 A (I I . . . I) Kw + (I I . . . I)t A (I I . . . I)\n(I I . . . I)t A\nh\ni\u22121\n=A \u2212 A (I I . . . I) diag{Kw1 + A, Kw2 + A, . . . KwL + A}\n(I I . . . I)t A\nL\nX\n[Kwl + A]\u22121 A.\n=A \u2212 A\nl=1\n\n35\n\n(116)\n\n\fThus,\nt\n(I I . . . I)K\u22121\nw (I I . . . I)\n\"\n#\u22121\nL\nX\n= A\u2212A\n(Kwl + A)\u22121 A\n\u2212 A\u22121\nl=1\n\n\uf8ee\n\n=A\u22121 \u2212 A\u22121 A \uf8f0\u2212\n\uf8ee\n\n=\uf8f0\n\nL\nX\n\nL\nX\nl=1\n\n(Kwl + A)\u22121\n\nl=1\n\n(Kwl + A)\u22121\n\n!\u22121\n\n\uf8f9\u22121\n\n\u2212 A\uf8fb\n\n!\u22121\n\n\uf8f9\u22121\n\n+ AA\u22121 A\uf8fb\n\nAA\u22121 \u2212 A\u22121\n\n(117)\n\n.\n\nWhen A is singular, we can choose \u03b4 > 0 such that A + \u01ebI \u227b 0 for \u01eb \u2208 (0, \u03b4), and thus\nwe can apply the previous argument and let \u01eb \u2192 0+ in the end.\n\nE\n\nProof of Lemma 3\n\nWe use induction. First consider the matrix\n\u0013\n\u0012\nKw1 \u2212A\n.\n\u22062 =\n\u2212A Kw2\nWe have\n\u22062 \u227b 0 \u21d0\u21d2 Kw2 \u227b AK\u22121\nw1 A\n\u21d0\u21d2 Kw2 + A \u227b AK\u22121\nw1 A + A\n\u22121\n\u22121\n\u21d0\u21d2 (Kw2 + A) \u227a (AK\u22121\nw1 A + A)\n\u21d0\u21d2 (Kw2 + A)\u22121 \u227a A\u22121 \u2212 (Kw1 + A)\u22121\n\u21d0\u21d2 (Kw1 + A)\u22121 + (Kw2 + A)\u22121 \u227a A\u22121\n\u21d0=\n\nL\nX\n\n(118)\n\u22121\n\n(Kwl + A)\n\n\u227aA\n\nl=1\n\n(a)\n\n\u21d0\u21d2 (Kw0 + A)\u22121 \u227a A\u22121\n\u21d0\u21d2 Kw0 + A \u227b A\n\u21d0\u21d2 Kw0 \u227b 0,\nwhere (a) is from (27).\n\n36\n\n\u22121\n\n\fNext we define\n\n\uf8eb\n\n\uf8f6\nKw1 \u2212A \u2212A . . .\n\u2212A\n\uf8ec \u2212A Kw2 \u2212A . . .\n\u2212A \uf8f7\n\uf8ec\n\uf8f7\n\uf8f7\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\u2206k = \uf8ec\n\uf8ec\n\uf8f7\n\uf8ed \u2212A . . . \u2212A Kwk\u22121 \u2212A \uf8f8\n\u2212A . . . \u2212A \u2212A Kwk\n\nand suppose \u2206k \u227b 0 for k = 3, . . . , l \u2212 1. Then\n\nt\n\u2206l \u227b 0 \u21d0\u21d2 Kwl \u227b A(I, I, . . . , I)\u2206\u22121\nl\u22121 (I, I, . . . , I) A\n\uf8ee\n\uf8f9\u22121\n!\u22121\nl\u22121\nX\n\u21d0\u21d2 Kwl \u227b A \uf8f0\n\u2212 A\uf8fb A\n(KWk + A)\u22121\nk=1\n\n\uf8ee\n\n\u21d0\u21d2 Kwl + A \u227b A \uf8f0\n\nl\u22121\nX\n\n(Kwk + A)\u22121\n\nk=1\n\n\uf8ee\n\n\uf8ee\n\n\u21d0\u21d2 (Kwl + A)\u22121 \u227a \uf8f0A \uf8f0\n\u21d0\u21d2 (Kwl + A)\n\n\u22121\n\n\u227aA\n\n\u22121\n\nl\u22121\nX\n\n(Kwk + A)\u22121\n\nl\u22121\nX\n\nl\u22121\nX\n\n\uf8f9\u22121\n\n\u2212 A\uf8fb\n\nk=1\n\n\uf8ee\n\n\u2212\uf8f0\n\n\u21d0\u21d2 (Kwl + A)\u22121 \u227a A\u22121 \u2212\n\n!\u22121\n\n!\u22121\n\n(Kwk + A)\n\nk=1\n\n\u22121\n\nA+A\n\uf8f9\u22121\n\n\u2212 A\uf8fb\n\n!\u22121\n\n\u21d0=\n\n(Kwk + A)\u22121\n\n(Kwk + A)\u22121 \u227a A\u22121\n\nk=1\n\n(b)\n\n\u21d0\u21d2 (Kw0 + A)\u22121 \u227a A\u22121\n\u21d0\u21d2 Kw0 + A \u227b A\n\u21d0\u21d2 Kw0 \u227b 0,\nwhere (b) is from (27).\n\nF\n\nProof of Proposition 2\n\nFirst we prove that\n\u22121\n\u22121\n\u22121\n\u2217\nD\u22121\n0 + Kx \u2212 D1 \u2212 D2 \u227b 0 \u21d2 A \u227b 0.\n\n37\n\nA + A\uf8fb\n\uf8f9\u22121\n\n\u2212 A + A\uf8fb\n\nk=1\n\nL\nX\n\n\uf8f9\u22121\n(119)\n\n\fProof. We have\nh\ni1\n1\n1\n1 2\n1\nA\u2217 = (Kw1 \u2212Kw0 ) 2 (Kw1 \u2212 Kw0 )\u2212 2 (Kw2 \u2212 Kw0 )(Kw1 \u2212 Kw0 )\u2212 2 (Kw1 \u2212Kw0 ) 2 \u2212Kw0 .\n(120)\nThus\nA\u2217 \u227b 0\nh\ni1\n1\n1\n1\n1 2\n\u21d0\u21d2 (Kw1 \u2212 Kw0 ) 2 (Kw1 \u2212 Kw0 )\u2212 2 (Kw2 \u2212 Kw0 )(Kw1 \u2212 Kw0 )\u2212 2 (Kw1 \u2212 Kw0 ) 2 \u227b Kw0\nh\ni1\n1\n1\n\u2212 21\n\u2212 12 2\n\u227b (Kw1 \u2212 Kw0 )\u2212 2 Kw0 (Kw1 \u2212 Kw0 )\u2212 2\n\u21d0\u21d2 (Kw1 \u2212 Kw0 ) (Kw2 \u2212 Kw0 )(Kw1 \u2212 Kw0 )\n1\n\n1\n\n\u21d0= (Kw1 \u2212 Kw0 )\u2212 2 (Kw2 \u2212 Kw0 )(Kw1 \u2212 Kw0 )\u2212 2\n1\n\n1\n\n\u227b (Kw1 \u2212 Kw0 )\u2212 2 Kw0 (Kw1 \u2212 Kw0 )\u22121 Kw0 (Kw1 \u2212 Kw0 )\u2212 2\n\u21d0\u21d2 Kw2 \u2212 Kw0 \u227b Kw0 (Kw1 \u2212 Kw0 )\u22121 Kw0\n\u0001\n\u21d0\u21d2 Kw2 \u2212 Kw0 \u227b Kw0 \u2212I + (Kw1 \u2212 Kw0 )\u22121 Kw1\n\u21d0\u21d2 Kw2 \u2212 Kw0 \u227b \u2212Kw0 + Kw0 (Kw1 \u2212 Kw0 )\u22121 Kw1\n\u21d0\u21d2 Kw2 \u227b Kw0 (Kw1 \u2212 Kw0 )\u22121 Kw1\n\u22121\n\u22121 \u22121 \u22121\n\u21d0\u21d2 Kw2 \u227b Kw0 K\u22121\nw0 (Kw0 \u2212 Kw1 ) Kw1 Kw1\n\u22121 \u22121\n\u21d0\u21d2 Kw2 \u227b (K\u22121\nw0 \u2212 Kw1 )\n\u22121\n\u22121\n\u21d0\u21d2 K\u22121\nw1 + Kw2 \u227a Kw0\n\u22121\n\u22121\n\u22121\n\u21d0\u21d2 D\u22121\n0 + Kx \u2212 D1 \u2212 D2 \u227b 0.\n\n(121)\n\nThe proof of\nD0 + K x \u2212 D1 \u2212 D2 \u227b 0 \u21d2 A \u2217 \u227a K x\nis similar and hence is omitted.\n\nG\n\u0002\n\nProof of Lemma 4\n(Kw0 + A\u2217 )\u22121 + \u039b1\n\n\u0003\u22121\n\n\u0002\n\u0003\u22121\n= (Kw0 + A\u2217 )\u22121 (I + (Kw0 + A\u2217 )\u039b1 )\n(a)\n\n= (I + Kw0 \u039b1 )\u22121 (Kw0 + A\u2217 )\n= (I + Kw0 \u039b1 )\u22121 (Kw0 + A\u2217 \u2212 (I + Kw0 \u039b1 )A\u2217 ) + A\u2217\n(b)\n\n= (I + Kw0 \u039b1 )\u22121 Kw0 + A\u2217\n\u0001\u22121\n= K\u22121\n(I\n+\nK\n\u039b\n)\n+ A\u2217\nw\n1\nw0\n0\n\u0001\n\u22121\n= K\u22121\n+ A\u2217 ,\nw0 + \u039b1\n38\n\n(122)\n\n\fwhere (a) and (b) are from \u039b1 A\u2217 = 0.\n|D\u22170 + Kz |\n= |I + D\u22170 \u22121 Kz |\n\u2217\n|D0 |\n= |I + (D\u22121\n0 + \u039b1 )Kz |\n\u22121\n= |I + D0 Kz + \u039b1 Kz |\n\n\u0001\n\u2217 \u22121\n= |I + D\u22121\n\u2212I |\n0 Kz + \u039b1 (I \u2212 A )\n(c)\n\n(123)\n\u0001\n\n\u2217\n\u2217 \u22121\n= |I + D\u22121\n\u2212I |\n0 Kz + \u039b1 (I \u2212 A ) (I \u2212 A )\n\u22121\n= |I + D0 Kz |\n|D0 + Kz |\n=\n,\n|D0 |\n\nwhere (c) is from \u039b1 A\u2217 = 0.\n\nH\n\nProof of Equations (70) and (71)\n\nWe first prove the following lemma.\nLemma 9. Let D be an N \u00d7 N matrix such that 0 \u227a D \u227a I. Let K = (D\u22121 \u2212 I)\u22121 .\nChoose \u01eb > 0 such that K \u2212 \u01ebI \u227b 0. Define\n\u0003\u22121\ndef \u0002\nD(\u01eb) = (K \u2212 \u01ebI)\u22121 + I\n.\n\nThen, there exist constants b1 \u2265 b2 > 0, such that\n\nD \u2212 b1 \u01ebI + o(\u01eb) \u227a D(\u01eb) \u227a D \u2212 b2 \u01ebI + o(\u01eb)\nProof. There exists an N \u00d7 N orthogonal matrix Q such that\nQKQt = diag{k1 , . . . , kN },\nwhere ki > 0 are eigenvalues of K. We have\nQDQt = Q(K\u22121 + I)\u22121 Qt\n\u001b\n\u001a\nkN\nk1\n,\n, ...,\n= diag\n1 + k1\n1 + kN\n\n39\n\n\fand\nh\ni\u22121\nQD(\u01eb)Qt = Q (K \u2212 \u01ebI)\u22121 + I Qt\nh\ni\u22121\n= (diag{k1 , . . . , kN } \u2212 \u01ebI)\u22121 + I\n\u001a\n\u001b\nk1 \u2212 \u01eb\nkN \u2212 \u01eb\n= diag\n, ...,\n1 + k1 \u2212 \u01eb\n1 + kN \u2212 \u01eb\n\u001a\n\u001b\nk1\n\u01eb\nkN\n\u01eb\n= diag\n\u2212\n+ o(\u01eb), . . . ,\n\u2212\n+ o(\u01eb) .\n1 + k1 (1 + k1 )2\n1 + kN\n(1 + kN )2\nWe now have\nQDQt \u2212 b1 \u01ebI + o(\u01eb) \u227a QD(\u01eb)Qt \u227a QDQt \u2212 b2 \u01ebI + o(\u01eb),\nwhere b1 \u2265 b2 > 0 are some constants. Hence\nD \u2212 b1 \u01ebI + o(\u01eb) \u227a D(\u01eb) \u227a D \u2212 b2 \u01ebI + o(\u01eb).\n\nEquations (70) and (71) are a direct consequence of this lemma.\n\nI\n\nProof of Equation (72)\n\nWe first prove the following lemma.\nLemma 10. Let A be an N \u00d7 N matrix such that 0 4 A \u227a I. Let Kz = (I \u2212 A)\u22121 \u2212 I.\nChoose \u01eb > 0 such that A + \u01ebI \u227a I. Define\ndef\n\nKz (\u01eb) = [I \u2212 (A + \u01ebI)]\u22121 \u2212 I.\nThen, there exist constants c1 \u2265 c2 > 0 such that\nKz \u2212 c1 \u01ebI + o(\u01eb) \u227a Kz (\u01eb) \u227a Kz \u2212 c2 \u01ebI + o(\u01eb).\nProof. There exists an N \u00d7 N orthogonal matrix Q such that\nQAQt = diag{a1 , . . . , aN }\nwhere ai > 0 are the eigenvalues of A. We have\nQKz Qt = Q((I \u2212 A)\u22121 \u2212 I)Qt\n\u001b\n\u001a\naN\na1\n,\n, ...,\n= diag\n1 \u2212 a1\n1 \u2212 aN\n40\n\n\fand\nQKz (\u01eb)Qt = Q((I \u2212 (A + \u01ebI))\u22121 \u2212 I)Qt\n\u001b\n\u001a\naN + \u01eb\na1 + \u01eb\n, ...,\n= diag\n1 \u2212 a1 \u2212 \u01eb\n1 \u2212 aN \u2212 \u01eb\n\u001a\n\u001b\na1\n(2a1 \u2212 1)\u01eb\naN\n(2aN \u2212 1)\u01eb\n= diag\n\u2212\n+ o(\u01eb), . . . ,\n\u2212\n+ o(\u01eb) .\n1 \u2212 a1\n(1 \u2212 a1 )2\n1 \u2212 aN\n(1 \u2212 aN )2\nWe now have\nQKz Qt \u2212 c1 \u01ebI + o(\u01eb) \u227a QKz (\u01eb)Qt \u227a QKz Qt \u2212 c2 \u01ebI + o(\u01eb),\nwhere c1 \u2265 c2 > 0 are some constants. Hence\nKz \u2212 c1 \u01ebI + o(\u01eb) \u227a Kz (\u01eb) \u227a Kz \u2212 c2 \u01ebI + o(\u01eb).\n\nEquation (72) is a direct result of this lemma.\n\nJ\n\nProof of equation (82)\n\nWe first prove the following lemma.\nLemma 11. Let A be an N \u00d7 N matrix such that 0 \u227a A 4 I. Choose \u01eb > 0 such that\nA \u2212 \u01ebI \u227b 0. Define\ndef\nKz (\u01eb) = [I \u2212 (A \u2212 \u01ebI)]\u22121 \u2212 I.\nThen, for any E and F such that 0 \u227a E 4 I and 0 \u227a F 4 I, we have\n|E + Kz (\u01eb)|\n= 1.\n\u01eb\u21920 |F + Kz (\u01eb)|\nlim\n\nProof. There exists an N \u00d7 N orthogonal matrix Q such that\nQAQt = diag{a1 , . . . , aN },\nwhere 0 < ai \u2264 1 are eigenvalues of A. Without loss of generality, we suppose a1 =\n1, . . . , ap = 1, ap+1 < 1, . . . , aN < 1.\nWe have\nQKz (\u01eb)Qt = Q((I \u2212 (A \u2212 \u01ebI))\u22121 \u2212 I)Qt\n\u001b\n\u001a\n1\u2212\u01eb\nap+1 \u2212 \u01eb\naN \u2212 \u01eb\n1\u2212\u01eb\n,\n, ...,\n,\n,\n= diag\n\u01eb\n\u01eb\n1 \u2212 ap+1 + \u01eb 1 \u2212 aN + \u01eb\n41\n\n\fand since\n\n|E + Kz (\u01eb)|\n|Kz (\u01eb)|\n|I + Kz (\u01eb)|\n\u2265\n\u2265\n,\n|Kz (\u01eb)|\n|F + Kz (\u01eb)|\n|I + Kz (\u01eb)|\n\nwe have\n\n|E + Kz (\u01eb)|\n= 1.\n\u01eb\u21920 |F + Kz (\u01eb)|\nlim\n\nEquation (82) is a direct consequence of this lemma.\n\nK\n\nProof of Equation (89)\n\nWe would like to have a property similar to (66), as \u01eb1 approaches zero, and a property\nsimilar to (82), as \u01eb2 approaches zero. To see this is the case, we need the following\nlemma.\nLemma 12.\n\u039b1 Kz (\u01eb1 = 0, \u01eb2 ) = 0\nProof. Since\nQ\u039b1 Qt QA\u2217 Qt = 0\nand\nQA\u2217 Qt = diag(0, . . . , 0, 1, . . . , 1, ap+q+1, . . . , as )\n| {z } | {z }\np\n\n\u2217\n\nq\n\nt\n\nQA Q \u2212 \u01eb2 E2 = diag(0, . . . , 0, 1 \u2212 \u01eb2 , . . . , 1 \u2212 \u01eb2 , ap+q+1 , . . . , as ),\n| {z } |\n{z\n}\np\n\nq\n\nwe have that\n\nQA\u2217 Qt (QA\u2217 Qt \u2212 \u01eb2 E2 ) = 0.\nThus\n\u0001\nQ\u039b1 Kz (\u01eb1 = 0, \u01eb2 )Qt = Q\u039b1 Qt Q (I \u2212 A\u2217 + Qt \u01eb2 E2 Q)\u22121 \u2212 I Qt\n\u0001\n= Q\u039b1 Qt (I \u2212 QA\u2217 Qt + \u01eb2 E2 )\u22121 \u2212 I\n\n= Q\u039b1 Qt (I \u2212 QA\u2217 Qt + \u01eb2 E2 ) (I \u2212 QA\u2217 Qt + \u01eb2 E2 )\u22121 \u2212 I\n= 0.\n\n42\n\n\u0001\n\n\fUsing this lemma, we can show a property similar to (66) as \u01eb1 approaches zero. First\nnote that similar to case 2, we have\n\u22121\n\u22121\nD\u22121\n0 + \u039b1 \u2212 e2 \u01eb2 I + o(\u01eb2 ) \u227a D0 (\u01eb1 , \u01eb2 ) \u227a D0 + \u039b1 + e1 \u01eb1 I + o(\u01eb1 )\n\nwhere e1 > 0 and e2 > 0 are constants. Hence we have\n|D0 (\u01eb1 = 0, \u01eb2 ) + Kz (\u01eb1 = 0, \u01eb2 )|\n= |I + D\u22121\n0 (\u01eb1 = 0, \u01eb2 )Kz (\u01eb1 = 0, \u01eb2 )|\n|D0 (\u01eb1 = 0, \u01eb2 )|\n\u2265 |I + (D\u22121\n0 + \u039b1 \u2212 e2 \u01eb2 I)Kz (\u01eb1 = 0, \u01eb2 )|\n\u22121\n= |I + D0 Kz (\u01eb1 = 0, \u01eb2 ) \u2212 e2 \u01eb2 Kz (\u01eb1 = 0, \u01eb2 )|\n|D0 + Kz (\u01eb1 = 0, \u01eb2 ) \u2212 e2 \u01eb2 D0 Kz (\u01eb1 = 0, \u01eb2 )|\n=\n.\n|D0 |\nSimilarly, we have\n|D0 (\u01eb1 = 0, \u01eb2 ) + Kz (\u01eb1 = 0, \u01eb2 )|\n|D0 + Kz (\u01eb1 = 0, \u01eb2 )|\n\u2264\n.\n|D0 (\u01eb1 = 0, \u01eb2 )|\n|D0 |\nThus\n1\n|I + Kz (\u01eb1 , \u01eb2 )|(L\u22121) |D0 (\u01eb1 , \u01eb2 ) + Kz (\u01eb1 , \u01eb2 )|\nlog\nL\n\u01eb2 \u21920 \u01eb1 \u21920 2\nQ\n|D0 (\u01eb1 , \u01eb2 )| |Dl (\u01eb1 , \u01eb2 ) + Kz (\u01eb1 , \u01eb2 )|\nlim lim\n\nl=1\n\n1\n|I + Kz (\u01eb1 = 0, \u01eb2 )|(L\u22121) |D0 + Kz (\u01eb1 = 0, \u01eb2 )|\nlog\nL\n\u01eb2 \u21920 2\nQ\n|D0 | |Dl (\u01eb1 = 0, \u01eb2 ) + Kz (\u01eb1 = 0, \u01eb2 )|\n\n= lim\n\n(124)\n\nl=1\n\n1\n1\n= log\n,\n2\n|D0 |\n\nwhere the last step is similar to (82).\n\nReferences\n[1] L. Ozarow, \"On a source-coding problem with two channels and three receivers,\"\nBell Syst. Tech. J., vol. 59, no. 10, pp. 1909\u20131921, Dec. 1980.\n[2] A. E. Gamal and T. M. Cover, \"Achievable rates for multiple descriptions,\" IEEE\nTrans. Inform. Theory, vol. 28, no. 6, pp. 851\u2013857, Nov. 1982.\n[3] R. Ahlswede, \"The rate-distortion region for multiple descriptions without excess\nrate,\" IEEE Trans. Inform. Theory, vol. 31, no. 6, pp. 721\u2013726, Nov. 1985.\n43\n\n\f[4] Z. Zhang and T. Berger, \"New results in binary multiple descriptions,\" IEEE Trans.\nInform. Theory, vol. 33, no. 4, pp. 502\u2013521, July 1987.\n[5] R. Zamir, \"Gaussian codes and shannon bounds for multiple descriptions,\" IEEE\nTrans. Inform. Theory, vol. 45, no. 7, pp. 2629\u20132635, Nov. 1999.\n[6] F. W. Fu and R. W. Yeung, \"On the rate-distortion region for multiple descriptions,\"\nIEEE Trans. Inform. Theory, vol. 48, no. 7, pp. 2012\u20132021, July 2002.\n[7] R. Venkataramani, G. Kramer, and V. K. Goyal, \"Multiple description coding with\nmany channels,\" IEEE Trans. Inform. Theory, vol. 49, no. 9, pp. 2106\u20132114, Sept.\n2003.\n[8] S. S. Pradhan, R. Puri, and K. Ramchandran, \"n-channel symmetric multiple\ndescroption-part I: (n,k) source-channel erasure codes,\" IEEE Trans. Inform. Theory, vol. 50, no. 1, pp. 47\u201361, Jan. 2004.\n[9] H. Feng and M. Effros, \"On the rate loss of multiple description source codes,\" IEEE\nTrans. Inform. Theory, vol. 51, no. 2, pp. 671\u2013683, Feb. 2005.\n[10] R. Puri, S. S. Pradhan, and K. Ramchandran, \"n-channel symmetric multiple\ndescroption-part II: an achievable rate-distortion region,\" IEEE Trans. Inform. Theory, vol. 51, no. 4, pp. 1377\u20131392, Apr. 2005.\n[11] V. A. Vaishampayan, \"Design of multiple description scalar quantizers,\" IEEE\nTrans. Inform. Theory, vol. 39, no. 3, pp. 821\u2013834, May 1993.\n[12] V. A. Vaishampayan and J. Domaszewicz, \"Design of entropy-constrained multipledescription scalar quantizers,\" IEEE Trans. Inform. Theory, vol. 40, no. 1, pp.\n245\u2013250, Jan. 1994.\n[13] V. A. Vaishampayan and J. C. Batllo, \"Asymptotic analysis of multiple-description\nquantizers,\" IEEE Trans. Inform. Theory, vol. 44, no. 1, pp. 278\u2013284, Jan. 1998.\n[14] V. A. Vaishampayan, N. Sloane, and S. Servetto, \"Multiple description vector quantizers with lattice codebooks: design and analysis,\" IEEE Trans. Inform. Theory,\nvol. 47, no. 4, pp. 1718\u20131734, July 2001.\n[15] V. K. Goyal, \"Multiple description coding: compression meets the network,\" IEEE\nSignal Processing Mag., vol. 18, pp. 74\u201393, Sept. 2001.\n[16] S. N. Diggavi, N. J. A. Sloane, and V. A. Vaishampayan, \"Asymmetric multiple\ndescription lattice vector quantizers,\" IEEE Trans. Inform. Theory, vol. 48, no. 1,\npp. 174\u2013191, Jan. 2002.\n\n44\n\n\f[17] V. K. Goyal, J. A. Kelner, and J. Kovacevic, \"Multiple description vector quantization with a coarse lattice,\" IEEE Trans. Inform. Theory, vol. 48, no. 3, pp. 781\u2013788,\nMar. 2002.\n[18] C. Tian and S. S. Hemami, \"Universal multiple description scalar quantizer: analysis\nand design,\" IEEE Trans. Inform. Theory, vol. 50, no. 9, pp. 2737\u20132751, Sept. 2004.\n[19] P. Ishwar, R. Puri, S. S. Pradhan, and K. Ramchandran, \"On compression for robust\nestimation in sensor networks,\" in Proc. IEEE Int. Symp. Inform. Theory (ISIT),\nYokohama, Japan, June-July 2003.\n[20] J. Chen and T. Berger, \"Robust distributed source coding,\" submitted to IEEE\nTrans. Inform. Theory, 2005.\n[21] N. Alon and J. H. Spencer, The probabilistic Method, 2nd edition. New York: Wiley,\n2000.\n[22] D. J. A. Welsh, Matroid Theory, Academic Press, London, 1976.\n[23] D. Tse and S. Hanly, \"Multi-access fading channels: part I: polymatroid structure,optimal resource allocation and throughput capacities,\" IEEE Trans. Inform.\nTheory, vol. 44, no. 7, pp. 2796\u20132815, Nov. 1998.\n[24] P. Viswanath, \"Sum rate of a class of gaussian multiterminal source coding problems,\" in Advances in Network Information Theory, P. Gupta, G. Kramer and A.\nWijngaarden editors, Rutgers, NJ, 2004, pp. 43\u201360.\n[25] F. Zhang, Matrix Theory: Basic Results and Techniques. Springer, 1999.\n[26] S. Diggavi and T. M. Cover, \"Worst additive noise under covariance constraints,\"\nIEEE Trans. Inform. Theory, vol. 47, no. 7, pp. 3072\u20133081, Nov. 2001.\n\n45\n\n\f"}