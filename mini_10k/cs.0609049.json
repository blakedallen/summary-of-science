{"id": "http://arxiv.org/abs/cs/0609049v2", "guidislink": true, "updated": "2007-05-08T07:34:29Z", "updated_parsed": [2007, 5, 8, 7, 34, 29, 1, 128, 0], "published": "2006-09-11T09:35:57Z", "published_parsed": [2006, 9, 11, 9, 35, 57, 0, 254, 0], "title": "Scanning and Sequential Decision Making for Multi-Dimensional Data -\n  Part I: the Noiseless Case", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0609130%2Ccs%2F0609021%2Ccs%2F0609056%2Ccs%2F0609166%2Ccs%2F0609088%2Ccs%2F0609103%2Ccs%2F0609162%2Ccs%2F0609126%2Ccs%2F0609070%2Ccs%2F0609167%2Ccs%2F0609031%2Ccs%2F0609054%2Ccs%2F0609105%2Ccs%2F0609165%2Ccs%2F0609002%2Ccs%2F0609049%2Ccs%2F0609164%2Ccs%2F0609067%2Ccs%2F0609080%2Ccs%2F0609076%2Ccs%2F0609158%2Ccs%2F0609039%2Ccs%2F0609009%2Ccs%2F0609136%2Ccs%2F0609008%2Ccs%2F0609069%2Ccs%2F0609041%2Ccs%2F0609149%2Ccs%2F0609122%2Ccs%2F0609097%2Ccs%2F0609139%2Ccs%2F0609007%2Ccs%2F0609037%2Ccs%2F0609124%2Ccs%2F0609053%2Ccs%2F0609116%2Ccs%2F0609141%2Ccs%2F0609029%2Ccs%2F0609001%2Ccs%2F0609140%2Ccs%2F0609028%2Ccs%2F0609163%2Ccs%2F0609159%2Ccs%2F0609134%2Ccs%2F0609035%2Ccs%2F0609017%2Ccs%2F0609022%2Ccs%2F0609061%2Ccs%2F0609016%2Ccs%2F0609077%2Ccs%2F0609047%2Ccs%2F0609094%2Ccs%2F0609074%2Ccs%2F0609108%2Ccs%2F0609086%2Ccs%2F0609102%2Ccs%2F0609084%2Ccs%2F0609052%2Ccs%2F0609095%2Ccs%2F0609125%2Ccs%2F0609065%2Ccs%2F0609082%2Ccs%2F0609051%2Ccs%2F0609112%2Ccs%2F0609046%2Ccs%2F0609010%2Ccs%2F0609093%2Ccs%2F0609045%2Ccs%2F0609120%2Ccs%2F0609104%2Ccs%2F0609115%2Ccs%2F0609150%2Ccs%2F0609129%2Ccs%2F0609018%2Ccs%2F0609092%2Ccs%2F0609033%2Ccs%2F0609146%2Ccs%2F0609087%2Ccs%2F0609106%2Ccs%2F0609160%2Ccs%2F0609138%2Ccs%2F0307008%2Ccs%2F0307041%2Ccs%2F0307035%2Ccs%2F0307048%2Ccs%2F0307003%2Ccs%2F0307034%2Ccs%2F0307021%2Ccs%2F0307010%2Ccs%2F0307073%2Ccs%2F0307072%2Ccs%2F0307062%2Ccs%2F0307004%2Ccs%2F0307054%2Ccs%2F0307018%2Ccs%2F0307006%2Ccs%2F0307064%2Ccs%2F0307066%2Ccs%2F0307031%2Ccs%2F0307001%2Ccs%2F0307027&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Scanning and Sequential Decision Making for Multi-Dimensional Data -\n  Part I: the Noiseless Case"}, "summary": "We investigate the problem of scanning and prediction (\"scandiction\", for\nshort) of multidimensional data arrays. This problem arises in several aspects\nof image and video processing, such as predictive coding, for example, where an\nimage is compressed by coding the error sequence resulting from scandicting it.\nThus, it is natural to ask what is the optimal method to scan and predict a\ngiven image, what is the resulting minimum prediction loss, and whether there\nexist specific scandiction schemes which are universal in some sense.\n  Specifically, we investigate the following problems: First, modeling the data\narray as a random field, we wish to examine whether there exists a scandiction\nscheme which is independent of the field's distribution, yet asymptotically\nachieves the same performance as if this distribution was known. This question\nis answered in the affirmative for the set of all spatially stationary random\nfields and under mild conditions on the loss function. We then discuss the\nscenario where a non-optimal scanning order is used, yet accompanied by an\noptimal predictor, and derive bounds on the excess loss compared to optimal\nscanning and prediction.\n  This paper is the first part of a two-part paper on sequential decision\nmaking for multi-dimensional data. It deals with clean, noiseless data arrays.\nThe second part deals with noisy data arrays, namely, with the case where the\ndecision maker observes only a noisy version of the data, yet it is judged with\nrespect to the original, clean data.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0609130%2Ccs%2F0609021%2Ccs%2F0609056%2Ccs%2F0609166%2Ccs%2F0609088%2Ccs%2F0609103%2Ccs%2F0609162%2Ccs%2F0609126%2Ccs%2F0609070%2Ccs%2F0609167%2Ccs%2F0609031%2Ccs%2F0609054%2Ccs%2F0609105%2Ccs%2F0609165%2Ccs%2F0609002%2Ccs%2F0609049%2Ccs%2F0609164%2Ccs%2F0609067%2Ccs%2F0609080%2Ccs%2F0609076%2Ccs%2F0609158%2Ccs%2F0609039%2Ccs%2F0609009%2Ccs%2F0609136%2Ccs%2F0609008%2Ccs%2F0609069%2Ccs%2F0609041%2Ccs%2F0609149%2Ccs%2F0609122%2Ccs%2F0609097%2Ccs%2F0609139%2Ccs%2F0609007%2Ccs%2F0609037%2Ccs%2F0609124%2Ccs%2F0609053%2Ccs%2F0609116%2Ccs%2F0609141%2Ccs%2F0609029%2Ccs%2F0609001%2Ccs%2F0609140%2Ccs%2F0609028%2Ccs%2F0609163%2Ccs%2F0609159%2Ccs%2F0609134%2Ccs%2F0609035%2Ccs%2F0609017%2Ccs%2F0609022%2Ccs%2F0609061%2Ccs%2F0609016%2Ccs%2F0609077%2Ccs%2F0609047%2Ccs%2F0609094%2Ccs%2F0609074%2Ccs%2F0609108%2Ccs%2F0609086%2Ccs%2F0609102%2Ccs%2F0609084%2Ccs%2F0609052%2Ccs%2F0609095%2Ccs%2F0609125%2Ccs%2F0609065%2Ccs%2F0609082%2Ccs%2F0609051%2Ccs%2F0609112%2Ccs%2F0609046%2Ccs%2F0609010%2Ccs%2F0609093%2Ccs%2F0609045%2Ccs%2F0609120%2Ccs%2F0609104%2Ccs%2F0609115%2Ccs%2F0609150%2Ccs%2F0609129%2Ccs%2F0609018%2Ccs%2F0609092%2Ccs%2F0609033%2Ccs%2F0609146%2Ccs%2F0609087%2Ccs%2F0609106%2Ccs%2F0609160%2Ccs%2F0609138%2Ccs%2F0307008%2Ccs%2F0307041%2Ccs%2F0307035%2Ccs%2F0307048%2Ccs%2F0307003%2Ccs%2F0307034%2Ccs%2F0307021%2Ccs%2F0307010%2Ccs%2F0307073%2Ccs%2F0307072%2Ccs%2F0307062%2Ccs%2F0307004%2Ccs%2F0307054%2Ccs%2F0307018%2Ccs%2F0307006%2Ccs%2F0307064%2Ccs%2F0307066%2Ccs%2F0307031%2Ccs%2F0307001%2Ccs%2F0307027&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We investigate the problem of scanning and prediction (\"scandiction\", for\nshort) of multidimensional data arrays. This problem arises in several aspects\nof image and video processing, such as predictive coding, for example, where an\nimage is compressed by coding the error sequence resulting from scandicting it.\nThus, it is natural to ask what is the optimal method to scan and predict a\ngiven image, what is the resulting minimum prediction loss, and whether there\nexist specific scandiction schemes which are universal in some sense.\n  Specifically, we investigate the following problems: First, modeling the data\narray as a random field, we wish to examine whether there exists a scandiction\nscheme which is independent of the field's distribution, yet asymptotically\nachieves the same performance as if this distribution was known. This question\nis answered in the affirmative for the set of all spatially stationary random\nfields and under mild conditions on the loss function. We then discuss the\nscenario where a non-optimal scanning order is used, yet accompanied by an\noptimal predictor, and derive bounds on the excess loss compared to optimal\nscanning and prediction.\n  This paper is the first part of a two-part paper on sequential decision\nmaking for multi-dimensional data. It deals with clean, noiseless data arrays.\nThe second part deals with noisy data arrays, namely, with the case where the\ndecision maker observes only a noisy version of the data, yet it is judged with\nrespect to the original, clean data."}, "authors": ["Asaf Cohen", "Neri Merhav", "Tsachy Weissman"], "author_detail": {"name": "Tsachy Weissman"}, "author": "Tsachy Weissman", "arxiv_comment": "46 pages, 2 figures. Revised version: title changed, section 1\n  revised, section 3.1 added, a few minor/technical corrections made", "links": [{"href": "http://arxiv.org/abs/cs/0609049v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0609049v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "H.1.1; I.2.6", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0609049v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0609049v2", "journal_reference": null, "doi": null, "fulltext": "Scanning and Sequential Decision Making for\n\narXiv:cs/0609049v2 [cs.IT] 8 May 2007\n\nMulti-Dimensional Data - Part I: the Noiseless Case\u2217\nAsaf Cohen\u2020, Neri Merhav\u2020 and Tsachy Weissman\u2021\nDecember 27, 2017\n\nAbstract\nWe investigate the problem of scanning and prediction (\"scandiction\", for short) of multidimensional data arrays. This problem arises in several aspects of image and video processing,\nsuch as predictive coding, for example, where an image is compressed by coding the error\nsequence resulting from scandicting it. Thus, it is natural to ask what is the optimal method\nto scan and predict a given image, what is the resulting minimum prediction loss, and whether\nthere exist specific scandiction schemes which are universal in some sense.\nSpecifically, we investigate the following problems: First, modeling the data array as a\nrandom field, we wish to examine whether there exists a scandiction scheme which is independent of the field's distribution, yet asymptotically achieves the same performance as if this\ndistribution was known. This question is answered in the affirmative for the set of all spatially\nstationary random fields and under mild conditions on the loss function. We then discuss the\nscenario where a non-optimal scanning order is used, yet accompanied by an optimal predictor,\nand derive bounds on the excess loss compared to optimal scanning and prediction.\nThis paper is the first part of a two-part paper on sequential decision making for multidimensional data. It deals with clean, noiseless data arrays. The second part deals with noisy\n\u2217\n\nThe material in this paper was presented in part at the IEEE International Symposium on Information Theory,\n\nSeattle, Washington, United States, July 2006, and the Electricity 2006 convention, Eilat, Israel, November 2006.\n\u2020\nAsaf Cohen and Neri Merhav are with the Department of the Electrical Engineering, Technion - I.I.T., Haifa\n32000, Israel. E-mails: {soofsoof@tx,merhav@ee}.technion.ac.il.\n\u2021\nTsachy Weissman is with the Department of Electrical Engineering, Stanford University, Stanford, CA 94305,\nUSA. E-mail: tsachy@stanford.edu.\n\n1\n\n\fdata arrays, namely, with the case where the decision maker observes only a noisy version of\nthe data, yet it is judged with respect to the original, clean data.\nIndex Terms-Universal scanning, Scandiction, Sequential decision making, Multi-dimensional\ndata, Random Field, Individual image.\n\n1\n\nIntroduction\n\nConsider the problem of sequentially scanning and predicting a multidimensional data array,\nwhile minimizing a given loss function. Particularly, at each time instant t, 1 \u2264 t \u2264 |B|, where\n|B| is the number of sites (\"pixels\") in the data array, the scandictor chooses a site to be\nvisited, denoted by \u03a8t , and gives a prediction, Ft , for the value at that site. Both \u03a8t and Ft\nmay depend of the previously observed values - the values at sites \u03a81 to \u03a8t\u22121 . It then observes\nthe true value, x\u03a8t , suffers a loss l(x\u03a8t , Ft ), and so on. The goal is to minimize the cumulative\nloss after scandicting the entire data array.\nThe problem of sequentially predicting the next outcome of a one-dimensional sequence (or\nany data array with a fixed, predefined, order), xt , based on the previously observed outcomes,\nx1 , x2 , . . . , xt\u22121 , is well studied. The problem of prediction in multidimensional data arrays (or\nwhen reordering of the data is allowed), however, has received far less attention. Apart from\nthe on-line strategies for the sequential prediction of the data, the fundamental problem of\nscanning it should be considered. We refer to the former problem as the prediction problem,\nwhere no reordering of the data is allowed, and to the latter as the scandiction problem.\nThe scandiction problem mainly arises in image compression, where various methods of\npredictive coding are used (e.g., [1]). In this case, the encoder may be given the freedom\nto choose the actual path over which it traverses the image, and thus it is natural to ask\nwhich path is optimal in the sense of minimal cumulative prediction loss (which may result in\nmaximal compression). The scanning problem also arises in other areas of image processing,\nsuch as one-dimensional wavelet [2] or median [3] processing of images, where one seeks a\nspace-filling curve which facilitates the one-dimensional signal processing of multidimensional\ndata, digital halftoning [4], where a space filling curve is sought in order to minimize the\neffect of false contours, and pattern recognition [5], where it is shown that under certain\nconditions, the Bayes risk as well as the optimal decision rule are unchanged if instead of\nthe original multidimensional classification problem one transforms the data using a measurepreserving space-filling curve and solves a simpler one-dimensional problem. More applications\n\n2\n\n\fcan be found in multidimensional data query [6], [7] and indexing [8], where multidimensional\ndata is stored on a one-dimensional storage device, hence a locality-preserving space-filling\ncurve is sought in order to minimize the number of continuous read operations required to\naccess a multidimensional object, and rendering of three-dimensional graphics [9], [10], where\na rendering sequence which minimizes the number of cache misses is required.\nThe above applications have already been considered in the literature, and the benefits\nof not-trivial scanning orders have been proved (see [11], or [12] and [13] which we discuss\nlater). Yet, the scandiction problem may have applications that go beyond image scanning\nalone. For example, consider a robot processing various types of products in a warehouse.\nThe robot identifies a product using a bar-code or an RFID, and processes it accordingly.\nIf the robot could predict the next product to be processed, and prepare for that operation\nwhile commuting to the product (e.g., prepare an appropriate writing-head and a message to\nbe written), then the total processing time could be smaller compared to preparing for the\noperation only after identifying the product. Since different sites in the warehouse my be\ncorrelated in terms of the various products stored in them, it is natural to ask what is the\noptimal path to scan the entire warehouse in order to achieve minimum prediction error and\nthus minimal processing time.\nIn [14], a specific scanning method was suggested by Lempel and Ziv for the lossless compression of multidimensional data. It was shown that the application of the incremental parsing\nalgorithm of [15] on the one dimensional sequence resulting from the Peano-Hilbert scan yields\na universal compression algorithm with respect to all finite-state scanning and encoding machines. These results where later extended in [16] to the probabilistic setting, where it was\nshown that this algorithm is also universal for any stationary Markov random field [17]. Using\nthe universal quantization algorithm of [18], the existence of a universal rate-distortion encoder\nwas also established. Additional results regarding lossy compression of random fields (via pattern matching) were given in [19] and [20]. For example, in [20], Kontoyiannis considered a\nlossy encoder which encodes the random field by searching for a D-closest match in a given\ndatabase, and then describing the position in the database.\nWhile the algorithm suggested in [14] is asymptotically optimal, it may not be the optimal\ncompression algorithm for real life images of sizes such as 256 \u00d7 256 or 512 \u00d7 512. In [12],\nMemon et. al. considered image compression with a codebook of block scans. Therein, the\nauthors sought a scan which minimizes the zero order entropy of the difference image, namely,\nthat of the sequence of differences between each pixel and its preceding pixel along the scan.\n\n3\n\n\fSince this problem is computationally expensive, the authors aimed for a suboptimal scan\nwhich minimizes the sum of absolute differences. This scan can be seen as a minimum spanning\ntree of a graph whose vertices are the pixels in the image and whose edges weights represent\nthe differences (in gray levels) between each pixel and its adjacent neighbors. Although the\noptimal spanning tree can be computed in linear time, encoding it may yield a total bit rate\nwhich is higher than that achieved with an ordinary raster scan. Thus, the authors suggested\nto use a codebook of scans, and encode each block in the image using the best scan in the\ncodebook, in the sense of minimizing the total loss.\nLossless compression of images was also discussed by Dafner et. al. in [13]. In this work, a\ncontext-based scan which minimizes the number of edge crossing in the image was presented.\nSimilarly to [12], a graph was defined and the optimal scan was represented through a minimal\nspanning tree. Due to the bit rate required to encode the scan itself the results fall short\nbehind [14] for two-dimensional data, yet they are favorable when compared to applying the\nalgorithm in [14] to each frame in a three-dimensional data (assuming the context-based scans\nfor each frame in the algorithm of [13] are similar).\nNote that although the criterion chosen by Memon et. al. in [12], or by Dafner et. al. in [13],\nwhich is to minimize the sum of cumulative (first order) prediction errors or edge crossings,\nis similar to the criterion defined in this work, there are two important differences. First, the\nweights of the edges of the graph should be computed before the computation of the optimal\n(or suboptimal) scan begins, namely, the algorithm is not sequential in the sense of scanning\nand prediction in one pass. Second, the weights of the edges can only represent prediction\nerrors of first order predictors (i.e., context of length one), since the prediction error for longer\ncontext depends on the scan itself - which has not been computed yet. In the context of lossless\nimage coding it is also important to mention the work of Memon et. al. in [21], where common\nscanning techniques (such as raster scan, Peano-Hilbert and random scan) were compared in\nterms of minimal cumulative conditional entropy given a finite context (note that for unlimited\ncontext the cumulative conditional entropy does not depend on the scanning order, as will be\nelaborated on later). The image model was assumed to be an isotropic Gaussian random\nfiled. Surprisingly, the results of [21] show that context-based compression techniques based\non limited context may not gain by using Hilbert scan over raster scan. Note that under a\ndifferent criterion, cumulative squared prediction error, the raster scan is indeed optimal for\nGaussian fields, as it was shown later in [22], which we discuss next.\nThe results of [14] and [16] considered a specific, data independent scan of the data set.\n\n4\n\n\fFurthermore, even in the works of Memon et. al. [12] or Dafner et. al. [13], where data dependent\nscanning was considered, only limited prediction methods (mainly, first order predictors) were\ndiscussed, and the criterion used was minimal total bit rate of the encoded image. However,\nfor a general predictor, loss function and random field (or individual image), it is not clear\nwhat is the optimal scan. This more general scenario was discussed in [22], where Merhav\nand Weissman formally defined the notion of a scandictor, a scheme for both scanning and\nprediction, as well as that of scandictability, the best expected performance on a data array.\nThe main result in [22] is the fact that if a stochastic field can be represented autoregressively\n(under a specific scan \u03a8) with a maximum-entropy innovation process, then it is optimally\nscandicted in the way it was created (i.e., by the specific scan \u03a8 and its corresponding optimal\npredictor).\nWhile defining the yardstick for analyzing the scandiction problem, the work in [22] leaves\nmany open challenges. As the topic of prediction is rich and includes elegant solutions to\nvarious problems, seeking analogous results in the scandiction scenario offers plentiful research\nobjectives.\nIn Section 3, we consider the case where one strives to compete with a finite set F of\nscandictors. Specifically, assume that there exists a probability measure Q which governs the\ndata array. Of course, given the probability measure Q and the scandictor set, one can compute\nthe optimal scandictor in the set (in some sense which will be defined later). However, we are\ninterested in a universal scandictor, which scans the data independently of Q, and yet achieves\nessentially the same performance as the best scandictor in F (see [23] for a complete survey of\nuniversal prediction). The reasoning behind the actual choice of the scandictor set F is similar\nto that common in the filtering and prediction literature (e.g., [24] and [25]). On the one hand,\nit should be large enough to cover a wide variety of random fields, in the sense that for each\nrandom field in the set, at least one scandictor is sufficiently close to the optimal scandictor\ncorresponding to that random field. On the other hand, it should be small enough to compete\nwith, at an acceptable cost of redundancy.\nAt first sight, in order to compete successfully with a finite set of scandictors, i.e., construct\na universal scandictor, one may try to use known algorithms for learning with expert advice,\ne.g., the exponential weighting algorithm suggested in [26] or the work which followed it. In this\nalgorithm, each expert is assigned a weight according to its past performance. By decreasing\nthe weight of poorly performing experts, hence preferring the ones proved to perform well\nthus far, one is able to compete with the best expert, having neither any a priori knowledge\n\n5\n\n\fon the input sequence nor which expert will perform the best. However, in the scandiction\nproblem, as each of the experts may use a different scanning strategy, at a given point in time\neach scanner might be at a different site, with different sites as its past. Thus, it is not at all\nguaranteed that one can alternate from one expert to the other. The problem is even more\ninvolved when the data is an individual image, as no statistical properties of the data can be\nused to facilitate the design or analysis of an algorithm. In fact, the first result in Section\n3 is a negative one, stating that indeed in the individual image scenario (or under expected\nminimum loss in the stochastic scenario), it is not possible to successfully compete with any two\nscandictors on any individual image. This negative result shows that the scandiction problem\nis fundamentally different and more challenging than the previously studied problems, such as\nprediction and compression, where competition with an arbitrary finite set of schemes in the\nindividual sequence setting is well known to be an attainable goal. However, in Theorem 4 of\nSection 3, we show that for the class of spatially stationary random fields, and subject to mild\nconditions on the prediction loss function, one can compete with any finite set of scandictors\n(under minimum expected loss). Furthermore, in Theorem 8, our main result in this section,\nwe introduce a universal scandictor for any spatially stationary random field. Section 3 also\nincludes almost surely analogues of the above theorems for mixing random fields and basic\nresults on cases where universal scandiction of individual images is possible.\nIn Section 4, we derive upper bounds on the excess loss incurred when non-optimal scanners\nare used, with optimal prediction schemes. Namely, we consider the scenario where one cannot\nuse a universal scandictor (or the optimal scan for a given random field), and instead uses an\narbitrary scanning order, accompanied by the optimal predictor for that scan. In a sense, the\nresults of Section 4 can be used to assess the sensitivity of the scandiction performance to the\nscanning order. Furthermore, in Section 4 we also discuss the scenario where the Peano-Hilbert\nscanning order is used, accompanied by an optimal predictor, and derive a bound on the excess\nloss compared to optimal finite state scandiction, which is valid for any individual image and\nany bounded loss function. Section 5 includes a few concluding remarks and open problems.\nIn [27], the second part of this two-part paper, we consider sequential decision making\nfor noisy data arrays. Namely, the decision maker observes a noisy version of the data, yet,\nit is judged with respect to the clean data. As the clean data is not available, two distinct\ncases are interesting to consider. The first, scanning and filtering, is when Y\u03a8t is available\nin the estimation of X\u03a8t , i.e., Ft depends on Y\u03a81 to Y\u03a8t , where {Y } is the noisy data. The\nsecond, noisy scandiction, is when the noisy observation at the current site is not available\n\n6\n\n\fto the decision maker. In both cases, the decision maker cannot evaluate its performance\nprecisely, as l(x\u03a8t , Ft ) cannot be computed. Yet, many of the results for noisy scandiction\nare extendable from the noiseless case, similarly as results for noisy prediction were extended\nfrom results for noiseless prediction [28]. The scanning and filtering problem, however, poses\nnew challenges and requires the use of new tools and techniques. Thus, in [27], we formally\ndefine the best achievable performance in these cases, derive bounds on the excess loss when\nnon-optimal scanners are used and present universal algorithms. A special emphasis is given\non the cases of binary random fields corrupted by a binary memoryless channel and real-valued\nfields corrupted by Gaussian noise.\n\n2\n\nProblem Formulation\n\nThe following notation will be used throughout this paper.1 Let A denote the alphabet, which\nd\n\nis either discrete or the real line. Let \u03a9 = AZ denote the space of all possible data arrays in\nZd . Although the results in this paper are applicable to any d \u2265 1, for simplicity, we assume\nfrom now on that d = 2. The extension to d \u2265 2 is straightforward. A probability measure Q\n\non \u03a9 is stationary if it is invariant under translations \u03c4i , where for each x \u2208 \u03a9 and i, j \u2208 Z2 ,\n\n\u03c4i (x)j = xj+i (namely, stationarity means shift invariance). Denote by M(\u03a9) and MS (\u03a9)\nthe spaces of all probability measures and stationary probability measures on \u03a9, respectively.\nElements of M(\u03a9), random fields, will be denoted by upper case letters while elements of \u03a9,\nindividual data arrays, will be denoted by the corresponding lower case.\nLet V denote the set of all finite subsets of Z2 . For V \u2208 V, denote by XV the restrictions\n\nof the data array X to V . For i \u2208 Z2 , Xi is the random variable corresponding to X at site\n\ni. Let R\u0003 be the set of all rectangles of the form V = Z2 \u2229 ([m1 , m2 ] \u00d7 [n1 , n2 ]). As a special\ncase, denote by Vn the square {0, . . . , n \u2212 1} \u00d7 {0, . . . , n \u2212 1}. For V \u2282 Z2 , let the interior\n\ndiameter of V be\n\u25b3\n\nR(V ) = sup{r : \u2203c s.t. B(c, r) \u2286 V },\n\n(1)\n\nwhere B(c, r) is a closed ball (under the l1 -norm) of radius r centered at c. Throughout, log(*)\nwill denote the natural logarithm, and entropies will be measured in nats.\nDefinition 1 ([22]). A scandictor for a finite set of sites B \u2208 V is the following pair (\u03a8, F ):\n|B|\n\n\u2022 {\u03a8t }t=1 is a sequence of measurable mappings, \u03a8t : At\u22121 7\u2192 B determining the site to\n1\n\nFor easy reference, we try to follow the notation of [22] whenever possible.\n\n7\n\n\fFigure 1: A graphical representation of the scandiction process. A scandictor (\u03a8, F ) first chooses\nan initial site \u03a81 . It then gives its prediction for the value at that site, F1 . After observing the\ntrue value at \u03a81 , it suffers a loss l(x\u03a81 , F1 ), chooses the next site to be visited, \u03a82 (x\u03a81 ), gives its\nprediction for the value at that site, F2 (x\u03a81 ), and so on.\nbe visited at time t, with the property that\nn\n\n\u0011o\n\u0010\n= B,\n\u03a81 , \u03a82 (x\u03a81 ), \u03a83 (x\u03a81 , x\u03a82 ) . . . , \u03a8|B| x\u03a81 , . . . , x\u03a8|B|\u22121\n\n\u2200x \u2208 AB .\n\n(2)\n\n|B|\n\n\u2022 {Ft }t=1 is a sequence of measurable predictors, where for each t, Ft : At\u22121 7\u2192 D determines the prediction for the site visited at time t based on the observations at previously\nvisited sites, and D is the prediction alphabet.\n|B|\n\n|B|\n\nWe allow randomized scandictors, namely, scandictors such that {\u03a8t }t=1 or {Ft }t=1 can be\nchosen randomly from some set of possible functions. At this point, it is important to note\nthat scandictors for infinite data arrays are not considered in this paper. Definition 1, and the\nresults to follow, consider only scandictors for finite sets of sites, ones which can be viewed\nmerely as a reordering of the sites in a finite set B. We will consider, though, the limit as the\nsize of the array tends to infinity. A scandictor, such that there exists a finite set of sites B,\nfor which there is no deterministic finite point in time by which all sites in B are scanned, is\nnot included in the scope of Definition 1. Figure 1 includes a graphical representation of the\nscandiction process.\nDenote by L(\u03a8,F ) (xVn ) the cumulative loss of (\u03a8, F ) over xVn , that is\nL(\u03a8,F ) (xVn ) =\n\n|Vn |\nX\nt=1\n\n\u0001\nl x\u03a8t , Ft (x\u03a81 , . . . , x\u03a8t\u22121 ) ,\n\n8\n\n(3)\n\n\fwhere l : A \u00d7 D \u2192 [0, \u221e) is a given loss function. Throughout this paper, we assume that\nl(*, *) is non-negative and bounded by lmax < \u221e. The scandictability of a source Q \u2208 M(\u03a9)\non B \u2208 V is defined by\nU (l, QB ) =\n\ninf\n\n(\u03a8,F )\u2208S(B)\n\nEQB\n\n1\nL\n(XB ),\n|B| (\u03a8,F )\n\n(4)\n\nwhere QB is the marginal probability measure of X restricted to B and S(B) is the set of all\npossible scandictors for B. The scandictability of Q \u2208 M(\u03a9) is defined by\nU (l, Q) = lim U (l, QVn ).\nn\u2192\u221e\n\n(5)\n\nBy [22, Theorem 1], the limit in (5) exists for any Q \u2208 MS (\u03a9) and, in fact, for any sequence\n{Bn } of elements of R\u0003 for which R(Bn ) \u2192 \u221e we have\nU (l, Q) = lim U (l, QBn ) = inf U (l, QB ).\nn\u2192\u221e\n\n2.1\n\nB\u2208R\u0003\n\n(6)\n\nFinite-Set Scandictability\n\nIt will be constructive to refer to the finite set scandictability as well. Let F = {Fn } be a\nsequence of finite sets of scandictors, where for each n, |Fn | = \u03bb < \u221e, and the scandictors\nin Fn are defined for the finite set of sites Vn . A possible scenario is one in which one has\na set of \"scandiction rules\", each of which defines a unique scanner for each n, but all these\nscanners comply with the same rule. In this case, F = {Fn } can also be viewed as a finite set\nF which includes sequences of scandictors. For example, |Fn | = 2 for all n, where for each\nn, Fn includes one scandictor which scans the data row-wise and one which scans the data\ncolumn-wise. We may also consider cases in which |Fn | increases with n (but remains finite\nfor every finite n). For Q \u2208 MS (\u03a9) and F = {Fn }, we thus define the finite set scandictability\nof Q as the limit\n\u25b3\n\nUF (l, Q) = lim\n\nmin\n\nn\u2192\u221e (\u03a8,F )\u2208Fn\n\nEQVn\n\n1\nL\n(XVn ),\n|Vn | (\u03a8,F )\n\n(7)\n\nif it exists. Observe that the sub-additivity property of the scandictability as defined in [22],\nwhich was fundamental for the existence of the limit in (5), does not carry over verbatim to\nfinite set scandictability. This is for the following reason. Suppose (\u03a8, F ) \u2208 S is the optimal\n\nscandictor for XV and (\u03a8\u2032 , F \u2032 ) \u2208 S is optimal for XU (assume V \u2229 U = \u2205). When scanning\n\nXV \u222aU , one may not be able to apply (\u03a8, F ) for XV and then (\u03a8\u2032 , F \u2032 ) for XU , as this scandictor\n\nmight not be in S. Hence, we seek a universal scheme which competes successfully (in a sense\nsoon to be defined) with a sequence of finite sets of scandictors {Fn }, even when the limit in\n(7) does not exist.\n\n9\n\n\f3\n\nUniversal Scandiction\n\nThe problem of universal prediction is well studied, with various solutions to both the stochastic\nsetting as well as the individual. In this section, we study the problem of universal scandiction.\nNotwithstanding strongly related to its prediction analogue, we first show that this problem\nis fundamentally different in several aspects, mainly due to the enormous degree of freedom\nin choosing the scanning order. Particularly, we first give a negative result, stating that while\nin the prediction problem it is possible to compete with any finite number of predictors and\non every individual sequence, in the scandiction problem one cannot even compete with any\ntwo scandictors on a given individual data array. Nevertheless, we show that in the setting\nof stationary random fields, and under the minimum expected loss criterion, it is possible to\ncompete with any finite set of scandictors. We then show that the set of finite-state scandictors\nis capable of achieving the scandictability of any spatially stationary source. In Theorem 8,\nour main result in this section, we give a universal algorithm which achieves the scandictability\nof any spatially stationary source.\n\n3.1\n\nA Negative Result on Scandiction\n\nAssume both the alphabet A and the prediction space D are [0, 1]. Let l be any non-degenerated\nloss function, in the sense that prediction of a bernoulli sequence under it results in a positive\nexpected loss. As an example, squared or absolute error can be kept in mind, though the\nresult below applies to many other loss functions. The following theorem asserts that in the\nindividual image scenario, it is not possible to compete successfully with any two arbitrary\nscandictors (it is possible, though, to compete with some scandictor sets, as proved in Section\n3.6).\nTheorem 2. Let A = D = [0, 1] and assume l is a non-degenerated loss function. There exist\ntwo scandictors (\u03a8, F )1 and (\u03a8, F )2 for Vn , such that for any scandictor (\u03a8, F ) for Vn there\nexists xVn for which\nL(\u03a8,F )(xVn ) \u2212 min{L(\u03a8,F )1 (xVn ), L(\u03a8,F )2 (xVn )} = \u0398(|Vn |).\n\n(8)\n\nIn words, there exist two scandictors such that for any third scandictor, there exists an\nindividual image for which the redundancy when competing with the two scandictors does not\nvanish. Theorem 2 marks a fundamental difference between the case where reordering of the\ndata is allowed, e.g., scanning of multidimensional data or even reordering of one-dimensional\n\n10\n\n\fdata, and the case where there is one natural order for the data. For example, using the\nexponential weighting algorithm discussed earlier, it is easy to show that in the prediction\nproblem (i.e., with no scanning), it is possible to compete with any finite set of predictors\nunder the above alphabets and loss functions. Thus, although the scandiction problem is\nstrongly related to its prediction analogue, the numerous scanning possibilities result in a\nsubstantially richer and more challenging problem.\nTheorem 2 is a direct application of the lemma below.\nLemma 3. Let A = D = [0, 1] and assume l is a non-degenerated loss function. There exist a\nrandom field XVn and two scandictors (\u03a8, F )1 and (\u03a8, F )2 for Vn , such that for any scandictor\n(\u03a8, F ) for Vn ,\nEL(\u03a8,F ) (XVn ) \u2212 E min{L(\u03a8,F )1 (XVn ), L(\u03a8,F )2 (XVn )} = \u0398(|Vn |).\n\n(9)\n\nLemma 3 gives another perspective on the difference between the scandiction and prediction\nscenarios. The lemma asserts that when ordering of the data is allowed, one cannot achieve\na vanishing redundancy with respect to the expected value of the minimum among a set of\nscandictors. This should be compared to the prediction scenario (no reordering), where one\ncan compete successfully not only with respect to the minimum of the expected losses of all\nthe predictors, but also with respect to the expected value of the minimum (for example, see\n[29, Corollary 1]). The main result of this section, however, is that for any stationary random\nfield and under mild conditions on the loss function, one can compete successfully with any\nfinite set of scandictors when the performance criterion is the minimum expected loss.\nProof. (Lemma 3) Let YVn be a random field such that Y (1, 1) is distributed uniformly on\n[0, 1], and YVn \\ Y (1, 1) = Y (1, 2), . . . , Y (1, n), Y (2, 1), Y (2, 2), . . . , Y (n, n) are simply the first\nn2 \u2212 1 bits in the binary representation of Y (1, 1) (ordered row-wise). Note that YVn \\ Y (1, 1)\n\nare i.i.d. unbiased bits, yet conditioned on Y (1, 1), they are deterministic and known. Assume\nnow that XVn is a random cyclic shift of YVn , in the same row-wise order YVn was created.\nFor concreteness, we assume the squared error loss function. In this case, it is easy to\nidentify the constant of the \u0398(*) expression in (8). However, the computations below are\neasily generalized to other non-degenerated loss functions. We first show that the expected\ncumulative squared error of any scandictor on XVn is at least (n2 + 1)/8, as the expected\nnumber of steps until the real valued site is located is (n2 + 1)/2, with a loss of 1/4 until that\ntime. More specifically, let J be the random number of cyclic shifts, that is, J is uniformly\ndistributed on {0, 1, . . . , n2 \u2212 1}. For fixed j, let G be the random index such that \u03a8G is the\n\n11\n\n\freal-valued X (i.e., G is the time the real valued random variable is located by the scanner\n\u03a8). Let \u03c6s denote the Bayes envelope associated with the squared error loss, i.e.,\n\u03c6s (p) = min [(1 \u2212 p)q 2 + p(q \u2212 1)2 ].\n\n(10)\n\nq\u2208[0,1]\n\nFor any scandictor (\u03a8, F ), we have,\n\uf8f1\n\uf8fc\nn2 \u0010\n\uf8f2X\n\u00112 \uf8fd\n\u03a8\nX\u03a8i \u2212 Fi (X\u03a81i\u22121 ) j\nEL(\u03a8,F ) (XVn ) = EJ E\n\uf8fe\n\uf8f3\ni=1\n\uf8f1\nn2 \u0010\nG \u0010\n\uf8f2X\n\u00112\n\u0011\nX\n\u03a8\n\u03a8i\u22121 2\n= EJ E\nX\u03a8i \u2212 Fi (X\u03a81i\u22121 )\nX\u03a8i \u2212 Fi (X\u03a81 ) +\n\uf8f3\ni=1\ni=G+1\n)\n( G\n\u00112\nX\u0010\n\u03a8\nX\u03a8i \u2212 Fi (X\u03a81i\u22121 ) j\n\u2265 EJ E\n\nj\n\n\uf8fc\n\uf8fd\n\uf8fe\n\ni=1\n\n\u2265 EJ E\n= EJ E\n=\n=\n\n( G\nX\ni=1\n\n(\n\n\u0011\n\u0010\n\u03a8\n\u03c6s P (X\u03a8i |X\u03a81i\u22121 ) j\n\nG\u03c6s (P (X\u03a81 = 1)) j\n\n1\nE{G}\n4\nn2 + 1\n.\n8\n\n)\n\n)\n\n(11)\n\nOn the other hand, consider the expected minimum of the losses of the following two scandictors: (\u03a8, F )1 which scandicts XVn row-wise from X(1, 1) to X(n, n), and (\u03a8, F )2 which\nscandicts XVn row-wise from X(n, n) to X(1, 1). Using the same method as in (11), it is\npossible to show that this expected loss is smaller than n2 /16 + o(n2 ), as the expected number\nof steps until the first locates the real-valued site is (n2 + 1)2 /(4n2 ), after which zero loss\nis incurred. This is since once the real-valued site is located, the rest of the values can be\ncalculated by the predictor by cyclic shifting the binary representation of the real-valued pixel.\nThis completes the proof.\nProof. (Theorem 2) By Lemma 3, there exists a stochastic setting under which the expected\nminimum of the losses of two scandictors is smaller than the expected loss of any single\nscandictor. Thus, for any scandictor there exists an individual image on which it cannot\ncompete successfully with the two scandictors.\n\n12\n\n\f3.2\n\nUniversal Scandiction With Respect to Arbitrary Finite\n\nSets\nAs mentioned in Section 1, straightforward implementation of the exponential weighting algorithm is not feasible, since one may not be able to alternate from one expert to the other at\nwish. However, the exponential weighting algorithm was found useful in several lossy source\ncoding works such as Linder and Lugosi [30], Weissman and Merhav [31], Gyorgy et. al. [32]\nand the derivation of sequential strategies for loss functions with memory [33], all of which\nconfronted a similar problem. A common method used in these works, is the alternation of\nexperts only once every block of input symbols, necessary to bear the price of this change (e.g.,\ntransmitting the description of the chosen quantizer [30]-[32]). Thus, although the difficulties\nin these examples differ from those we confront here, the solution suggested therein, which is\nto persist on using the same expert for a significantly long block of data before alternating it,\nwas found useful in our universal scanning problem.\nParticularly, we divide the data array into smaller blocks and alternate scandictors only\neach time a new block of data is to be scanned. Unlike the case of sequential prediction dealt\nwith in [33], here the scandictors must be restarted each time a new block is scanned, as it is\nnot at all guaranteed that all the scandictors scan the data in the same (or any) block-wise\norder (i.e., it is not guaranteed that a scandictor for Vn divides the array to sub-blocks of size\nm \u00d7 m and scans each of them separately). Hence, in order to prove that it is possible to\ncompete with the best scandictor at each stage n, we go through two phases. In the first,\nwe prove that an exponential weighting algorithm may be used to compete with the best\nscandictor among those operating in a block-wise order. This part of the proof will refer to\nany given data array (deterministic scenario). In the second phase, we use the stationarity of\nthe random field to prove that a block-wise scandictor may perform essentially as well as one\nscanning the data array as a whole. The following theorem stands at the basis of our results,\nestablishing the existence of a universal scandictor which competes successfully with any finite\nset of scandictors.\nTheorem 4. Let X be a stationary random field with a probability measure Q. Let F = {Fn }\nbe an arbitrary sequence of scandictor sets, where Fn is a set of scandictors for Vn and |Fn | =\n\u03bb < \u221e for all n. Then, there exists a sequence of scandictors {(\u03a8\u0302, F\u0302 )n }, where (\u03a8\u0302, F\u0302 )n is a\nscandictor for Vn , independent of Q, for which\nlim inf EQVn E\nn\u2192\u221e\n\n1\n1\n(XVn ) \u2264 lim inf min EQVn\nL\nL\n(XVn )\nn\u2192\u221e (\u03a8,F )\u2208Fn\n|Vn | (\u03a8\u0302,F\u0302 )n\n|Vn | (\u03a8,F )\n\n13\n\n(12)\n\n\ffor any Q \u2208 MS (\u03a9), where the inner expectation in the l.h.s. of (12) is due to the possible\nrandomization in (\u03a8\u0302, F\u0302 )n .\nBefore we prove Theorem 4, let us discuss an \"individual image\" type of result, which will\nlater be the basis of the proof. Let xVn denote an individual n \u00d7 n data array. For m < n,\n\u25b3 \u0006n\u0007\n\u2212 1. We divide xVn into K 2 blocks of size m \u00d7 m and 2K + 1 blocks of possibly\ndefine K = m\nsmaller size. Denote by xi , 0 \u2264 i \u2264 (K + 1)2 \u2212 1 the i'th block under some fixed scanning\n\norder of the blocks. Since we will later see that this scanning order is irrelevant in this case,\nassume from now on that it is a (continuous) raster scan from the upper left corner. That is,\nthe first line of blocks is scanned left to right, the second line is scanned right to left, and so\non. We will refer to this scan simply as \"raster scan\".\nAs mentioned, the suggested algorithm scans the data in xVn block-wise, that is, it does not\napply any of the scandictors in Fn , only scandictors from Fm . Omitting m for convenience,\ndenote by Lj,i the cumulative loss of (\u03a8, F )j \u2208 Fm after scanning i blocks, where (\u03a8, F )j is\nrestarted after each block, namely, it scans each block separately and independently of the\nP\nl\nother blocks. Note that Lj,i = i\u22121\nl=0 Lj (x ) and that for i = 0, Lj,i = 0 for all j. Since we\n\nassumed the scandictors are capable of scanning only square blocks, for the 2K + 1 possibly\nsmaller (and not square) blocks the loss may be lmax throughout. For \u03b7 > 0, and any i and j,\ndefine\n\u0011\n\u0010\ne\u2212\u03b7Lj,i\nPi j|{Lj,i }\u03bbj=1 = P\u03bb\n,\n\u2212\u03b7Lj,i\ne\nj=1\n\n(13)\n\nwhere \u03bb = |Fm |. We offer the following algorithm for a block-wise scan of the data array\n\nx. For each 0 \u2264 i \u2264 (K + 1)2 \u2212 1, after scanning i blocks of data, the algorithm computes\n\u0010\n\u0011\nPi j|{Lj,i }\u03bbj=1 for each j. It then randomly selects a scandictor according to this distribution,\n\nindependently of its previous selections, and uses this scandictor as its output for the (i + 1)-st\n\nblock. Namely, the universal scandictor (\u03a8\u0302, F\u0302 )n , promised by Theorem 4, is the one which\ndivides the data to blocks, performs a raster scan of the data block-wise, and uses the above\nalgorithm to decide which scandictor out of Fm to use for each block.\nIt is clear that both the block size and the number of blocks should tend to infinity with\nn in order to achieve meaningful results. Thus, we require the following: a. m = m(n)\ntends to infinity, but strictly slower than n, i.e., m(n) = o(n). b. m(n) is an integer-valued\nmonotonically increasing function, such that for each K \u2208 Z there exists n such that m(n) = K.\nThe results are summarized in the following two propositions, the first of which asserts that\nfor m(n) = o(n), vanishing redundancy is indeed possible, while the second asserts that under\nslightly stronger requirements on m(n), this is also true in the a.s. sense (with respect to the\n\n14\n\n\frandom selection of the scandictors in the algorithm).\nProposition 5. Let Lalg (xVn ) be the cumulative loss of the proposed algorithm on xVn , and\ndenote by L\u0304alg (xVn ) its expected value, where the expectation is with respect to the randomized\nscandictor selection of the algorithm. Let Lmin denote the cumulative loss of the best scandictor\nin Fm , operating block-wise on xVn . Assume |Fm | = \u03bb. Then, for any xVn ,\np\nlmax\nL\u0304alg (xVn ) \u2212 Lmin (xVn ) \u2264 m(n)(n + m(n)) log \u03bb \u221a .\n(14)\n2\n\u0001\nProposition 6. Assume m(n) = o n1/3 . Then, for any image xVn , the difference between\n\nthe normalized cumulative loss of the proposed algorithm and that of the best scandictor in Fm ,\noperating block-wise, converges to 0 with probability 1 with respect to the randomized scandictor\nselection of the algorithm.\nThe proofs of Propositions 5 and 6 are rather technical and are based on the very same\nmethods used in [34] and [33]. See Appendices A.1 and A.2 for the details.\nOn the more technical side, note that the suggested algorithm has \"finite horizon,\" that\nis, one has to know the size of the image in order to divide it to blocks, and only then can\nthe exponential weighting algorithm be used. It is possible to extend the algorithm to infinite\nhorizon. The essence of this generalization is in dividing the infinite image into blocks of\nexponentially growing size2 , and to apply the finite horizon algorithm for each block. We may\nnow proceed to the proof of Theorem 4.\nProof of Theorem 4. Since the result of Proposition 5 applies to any individual data array, it\ncertainly applies after taking the expectation with respect to Q. Therefore,\nEQVn\n\np\n1\nm(n)\n1\nL\u0304\n\u2212\nE\nL\n\u2264\nl\n2 log \u03bb.\nQ\nmin\nmax\nalg\nVn\nn2\nn2\nn\n\n(15)\n\nHowever, remember that we are not interested in competing with EQVn n12 Lmin , as this is the\nperformance of the best block-wise scandictor. We wish to compete with the best scandictor\n2\n\nFor example, take four blocks of size l \u00d7 l, then three of size 2l \u00d7 2l, and so on.\n\n15\n\n\foperating on the entire data array XVn , that is, on the whole image of size n \u00d7 n. We have\n1\nEQVn 2 Lmin\nn\n\n=\n\n1\nEQVn 2\nn\n\n(K+1)2\n\nmin\n\n(\u03a8,F )j \u2208Fm(n)\n\nX\n\nLj (X i )\n\ni=1\n\n(K+1)2\n\n\u2264\n\nmin\n\n(\u03a8,F )j \u2208Fm(n)\n\n(a)\n\n\u2264\n\nmin\n\n(\u03a8,F )j \u2208Fm(n)\n\n1 X\nLj (X i )\nn2\ni=1\n\u0014\n1\n* K 2 EQVn Lj (X 0 )\nn2\n\nEQVn\n\n+2Km(n)(n \u2212 Km(n))lmax + (n \u2212 Km(n))2 lmax\n\u2264\n\nmin\n\n(\u03a8,F )j \u2208Fm(n)\n\nEQVn\n\n1\nm(n)\nlmax ,\nLj (X 0 ) + 2\n2\nm(n)\nn\n\n\u0015\n(16)\n\nwhere (a) follows from the stationarity of Q, the assumption that each (\u03a8, F )j operates in the\nsame manner on each m(n) \u00d7 m(n) block, no matter what its coordinates are, and the fact\nthat each (\u03a8, F )j may incur maximal loss on non-square rectangles. From (15) and (16), we\nhave\nEQVn\n\np\n1\nm(n)\nm(n)\n1\n0\n\u2217 (m(n)) (X ) + 2\nL\u0304\n\u2264\nE\nL\nl\n+\nl\n2 log \u03bb\nQ\nmax\nmax\nalg\nj\nVn\nn2\nm(n)2\nn\nn\n\u0012\n\u0013\n1\nm(n) p\n0\n= EQVn\nlog\n\u03bb\nL\n\u2217\n(X\n)\n+\nO\nm(n)2 j (m(n))\nn\n\n(17)\n\nwhere (\u03a8, F )j \u2217 (m(n)) is the scandictor achieving the minimum in (16). Finally, by our assumptions on {m(n)}, we have\n\u001b\n\u001a\n1\n\u2264\ninf EQVk 2 L\u0304alg\nk\u2265n\nk\n\n\u001b\n\u001a\np\n1\nm(k)\n0\ninf EQVk\nL\u2217\n(X ) +\nlmax (2 + 2 log \u03bb)\nk\u2265n\nm(k)2 j (m(k))\nk\n\u001a\n\u001b\np\n1\nm(n)\n0\n\u2264 inf EQVk\n2 log \u03bb)\nL\n\u2217 (m(k)) (X )\n+\nl\n(2\n+\nmax\nj\nk\u2265n\nm(k)2\nn\n\u001b\n\u001a\np\nm(n)\n1\nlmax (2 + 2 log \u03bb).\n(18)\n\u2264 inf EQVk 2 Lj \u2217 (k) (XVk ) +\nk\u2265n\nk\nn\n\nTaking the limit as n \u2192 \u221e and using the fact that m(k)/k \u2192 0 together with the arbitrariness\nof k, gives:\nlim inf EQVn\nn\u2192\u221e\n\n1\n1\nL\u0304alg \u2264 lim inf EQVn 2 Lj \u2217 (n) (XVn ),\n2\nn\u2192\u221e\nn\nn\n\n(19)\n\nwhich completes the proof of (12).\nIt is evident from (14) and (18) that although the results of Theorem 4 and Proposition\n5 are formulated for fixed \u03bb < \u221e (the cardinality of the scandictor set), these results hold\nfor the more general case of \u03bb = \u03bb(n), as long as the redundancy vanishes, i.e., as long as\n\u221a\nlog \u03bb \u2192 0 when n \u2192 \u221e. The requirement that\nm(n) = o(n) and \u03bb(n) is such that m(n)\nn\n\n16\n\n\f\u0010 n2 \u0011\n\u03bb(n) = o e m(n)2 still allows very large scandictor sets, especially when m(n) grows slowly\n\nwith n. Furthermore, it is evident from equation (17) that whenever the redundancy vanishes,\nthe statement of Theorem 4 is valid with lim sup as well ,i.e.,\nlim sup EQVn E\nn\u2192\u221e\n\n3.3\n\n1\n1\nL(\u03a8\u0302,F\u0302 )n (XVn ) \u2264 lim sup min EQVn\nL\n(XVn ).\n|Vn |\n|Vn | (\u03a8,F )\nn\u2192\u221e (\u03a8,F )\u2208Fn\n\n(20)\n\nFinite-State Scandiction\n\nConsider now the set of finite-state scandictors, very similar to the set of finite-state encoders\ndescribed in [14]. At time t = 1, a finite-state scandictor starts at an arbitrary initial site \u03a81 ,\nwith an arbitrary initial state s0 \u2208 S and gives F (s0 ) as its prediction for x\u03a81 . Only then it\nobserves x\u03a81 . After observing x\u03a8i , it computes its next state, si , according to si = g(si\u22121 , x\u03a8i )\nand advances to the next site, x\u03a8i+1 , according to \u03a8i+1 = \u03a8i + d(si ), where g : S \u00d7 A 7\u2192 S\n\nis the next state function and d : S 7\u2192 B is the displacement function, B \u2282 Z2 denoting\na fixed finite set of possible relative displacements. It then gives its prediction F (si ) to the\nvalue x\u03a8i+1 . Similarly to [14], we assume the alphabet A includes an additional \"End of File\"\n\n(EoF) symbol to mark the image edges. The following lemma and the theorem which follows\nestablish the fact that the set of finite-state scandictors is indeed rich enough to achieve the\nscandictability of any stationary source, yet not too rich to compete with.\nLemma 7. Let FS = {(\u03a8, F )j } be the set of all finite-state scandictors with at most S states.\nThen, for any Q \u2208 MS (\u03a9),\nlim UFS (l, Q) = U (l, Q).\n\nS\u2192\u221e\n\n(21)\n\nThat is, the scandictability of any spatially stationary source is asymptotically achieved with\nfinite-state scandictors.\nProof. Take B = Vm and let (\u03a8\u0304, F\u0304 )m be the achiever of the infimum in (4). That is,\nEQVm\n\n1\n1\nL(\u03a8\u0304,F\u0304 )m (XVm ) \u2264\ninf\nEQVm 2 L(\u03a8,F ) (XVm ).\n2\nm\nm\n(\u03a8,F )\u2208S(Vm )\n\n(22)\n\nSince Vm is a rectangle of size m \u00d7 m, the scandictor (\u03a8\u0304, F\u0304 )m is certainly implementable\nwith a finite-state machine having S(m) < \u221e states. In other words, since Vm is finite, any\n\nscanning rule \u03a8t : At\u22121 7\u2192 B and any prediction rule Ft : At\u22121 7\u2192 A can be implemented with\n2\n\na finite-state machine having at most S\u0303(m) = Am \u00d7 m2 states, where in a straightforward\n2\n\nimplementation Am states are required to account for all possible inputs and m2 states are\nrequired to implement a counter.\n\n17\n\n\fNow, for an n \u00d7 n image (assuming now that m divides n, as dealing with the general case\ncan be done in the exact same way as (16)), we take (\u03a8\u0304\u2032 , F\u0304 \u2032 )n to be the scandictor which scans\nthe image in the block-by-block raster scan described earlier, applying (\u03a8\u0304, F\u0304 )m to each m \u00d7 m\nblock. Namely, \u03a8\u0304\u2032 scans all the blocks in the first m lines left-to-right until it reaches an EoF\nsymbol, then moves down m lines, scans all the blocks right-to-left until an EoF is reached,\nand so on. The predictor F \u0304\u2032 simply implements F\u0304 for each block separately, i.e., it resets to its\ninitial values at the beginning of each block. It is clear that the scanner \u03a8\u0304\u2032 is implementable\nwith a finite-state machine having S(m) = S\u0303(m) + 2 < \u221e states and thus (\u03a8\u0304\u2032 , F \u0304\u2032 ) \u2208 FS(m) .\nFrom the stationarity of Q, we have\ninf\n\n(\u03a8,F )\u2208S(Vn )\n\nEQVn\n\n1\nL\n(XVn ) \u2264\nn2 (\u03a8,F )\n\nmin\n\n(\u03a8,F )\u2208FS(m)\n\nEQVn\n\n1\nL\n(XVn )\nn2 (\u03a8,F )\n\n1\nL \u2032  \u0304\u2032 (XVn )\nn2 (\u03a8\u0304 ,F )n\n1\n= EQVm 2 L(\u03a8\u0304,F\u0304 )m (XVm )\nm\n1\n\u2264\ninf\nEQVm 2 L(\u03a8,F )(XVm ).\nm\n(\u03a8,F )\u2208S(Vm )\n\n\u2264 EQVn\n\n(23)\n\nTaking the limits lim supn\u2192\u221e and lim inf n\u2192\u221e , by (6), we have\nU (l, Q) \u2264 lim sup\n\nmin\n\nn\u2192\u221e (\u03a8,F )\u2208FS(m)\n\n\u2264\n\ninf\n\n(\u03a8,F )\u2208S(Vm )\n\nEQVm\n\nEQVn\n\n1\nL\n(XVn )\nn2 (\u03a8,F )\n\n1\nL\n(XVm )\nm2 (\u03a8,F )\n\n(24)\n\nand\nU (l, Q) \u2264 lim inf\n\nmin\n\nn\u2192\u221e (\u03a8,F )\u2208FS(m)\n\n\u2264\n\ninf\n\n(\u03a8,F )\u2208S(Vm )\n\nEQVm\n\nEQVn\n\n1\nL\n(XVn )\nn2 (\u03a8,F )\n\n1\nL\n(XVm ).\nm2 (\u03a8,F )\n\n(25)\n\nThe proof is completed (including the existence of the limit in the l.h.s. of (21)) by taking m to infinity, applying (6), and remembering that UFS (l, Q) is monotone in S, thus the\nconvergence of the sub-sequence {UFS(m) (l, Q)}\u221e\nm=1 implies the convergence of the sequence\n\n{UFS (l, Q)}\u221e\nS=1 ).\n\nIn words, Lemma 7 asserts that for any m, finite-state machines attain the m \u00d7 m Bayesian\nscandictability for any stationary random field. Note that the reason such results are accomplishable with FSMs is their ability to scan the entire data, block by block, with a machine\nhaving no more than S(m) states, regardless of the size of the complete data array. The\nnumber of the states depends only on the block size.\n\n18\n\n\f3.4\n\nA Universal Scandictor for Any Stationary Random Field\n\nWe now show that a universal scandictor which competes successfully with all finite-state\nmachines of the form given in the proof of Lemma 7, does exist and can, in fact, be implemented\nusing the exponential weighting algorithm. In order to show that we assume that the alphabet\nA is finite and the prediction space D is either finite or bounded (such as the |D| \u2212 1 simplex\nof probability measures on D). In the latter case we further assume that l(x, F ) is Lipschitz in\nits second argument for all x, i.e, there exists a constant c such that for all x, F and \u01eb we have\n|l(x, F ) \u2212 l(x, F + \u01eb)| \u2264 c|\u01eb|. The following theorem establishes, under the above assumptions,\nthe existence of a universal scandictor for all stationary random fields.\nTheorem 8. Let X be a stationary random field over a finite alphabet A and a probability\nmeasure Q. Let the prediction space D be either finite or bounded (with l(x, F ) then being\nLipschitz in its second argument). Then, there exists a sequence of scandictors {(\u03a8, F )n },\nindependent of Q, for which\nlim EQVn E\n\nn\u2192\u221e\n\n1\nL\n(XVn ) = U (l, Q)\n|Vn | (\u03a8,F )n\n\n(26)\n\nfor any Q \u2208 MS (\u03a9), where the inner expectation in the l.h.s. of (26) is due to the possible\nrandomization in (\u03a8, F )n .\nProof. Assume first that the range D of the predictors {Ft } is finite. Consider the exponential\nweighting algorithm described in the proof of Theorem 4, where at each m(n)\u00d7 m(n) block the\nalgorithm computes the cumulative loss of every possible scandictor for an m(n) \u00d7 m(n) block,\nthen chooses the best scandictor (according to the exponential weighting regime described\ntherein) as its output for the next block. By (17), we have\nEQVn\n\n1\n1\nL\u0304alg \u2264\nmin\nEQV\nL(\u03a8,F ) (X 0 ) + O\nm(n) m(n)2\nn2\n(\u03a8,F )\u2208S(Vm(n) )\n\n\u0012\n\n\u0013\nm(n) p\nlog \u03bb ,\nn\n\n(27)\n\nwhere S(Vm(n) ) is the set of all possible scandictors on m(n) \u00d7 m(n) and \u03bb is the size of that\n\u0011\n\u0010\n\u221a\nlog\n\u03bb\nexpression indeed\nset. Since \u03bb = \u03bb (m(n)), all that is left to check is that the O m(n)\nn\n\ndecays to zero as n tends to infinity.\n\nIndeed, the number of possible scanners for a field B over an alphabet A is\nn\no\n|B|\n{\u03a8t }t=1 , \u03a8t : At\u22121 7\u2192 B\n\n19\n\n=\n\n|B|\nY\n\n(|B| \u2212 k)|A|\n\nk\n\nk=0\n\n\u2264 (|B|!)|A|\n\n|B|\n\n,\n\n(28)\n\n\fwhile the number of possible predictors is\nn\n\n|B|\n\n{Ft }t=1 , Ft : At\u22121 7\u2192 D\n\no\n\n=\n\n|B|\nY\n\nk=1\n\n|D||A|\n\n\u2264 |D||B||A|\n\nk\n\n|B|\u22121\n\n.\n\n(29)\n\nThus, using the Stirling approximation, log k! \u2248 k log k, in the sense that limk\u2192\u221e\n\nlog k!\nk log k\n\n= 1,\n\nwe have\nr h\ni\nm(n)\nm(n)2\nm(n)2 \u22121\nlog (m(n)2 !)|A|\n|D|m(n)2 |A|\nn\nq\nm(n)\n\u2248\n2|A|m(n)2 m(n)2 log m(n) + m(n)2 |A|m(n)2 \u22121 log |D|\nn\nq\nm(n)2\n\u2248\n|A|m(n)2 log m(n),\n(30)\nn\n\u221a\n\u221a\nwhich decays to zero as n \u2192 \u221e for any m(n) = o( log n). Namely, for m(n) = o( log n),\nm(n) p\nlog \u03bb \u2264\nn\n\nequation (27) results in\nlim inf EQVn\n\n1\n1\nL\u0304alg \u2264 lim inf\nmin\nEQVm(n)\nL\n(X 0 ),\nn\u2192\u221e (\u03a8,F )\u2208S(Vm(n) )\nn2\nm(n)2 (\u03a8,F )\n\n(31)\n\nlim sup EQVn\n\n1\n1\nL\u0304alg \u2264 lim sup\nmin\nEQVm(n)\nL\n(X 0 ).\n2\nn\nm(n)2 (\u03a8,F )\nn\u2192\u221e (\u03a8,F )\u2208S(Vm(n) )\n\n(32)\n\nn\u2192\u221e\n\nand\nn\u2192\u221e\n\n1\n0\nSince m(n) \u2192 \u221e as n \u2192 \u221e, by [22] the limit limn\u2192\u221e min(\u03a8,F )\u2208S(Vm(n) ) EQVm(n) m(n)\n2 L(\u03a8,F ) (X )\n\nexists and equals the scandictability of the source, U (l, Q). However, by definition, U (l, Q) is\nthe best achievable scandiction performance for the source Q, hence,\nlim inf EQVn\nn\u2192\u221e\n\n1\nL\u0304alg \u2265 U (l, Q),\nn2\n\n(33)\n\nwhich results in\nlim EQVn\n\nn\u2192\u221e\n\n1\nL\u0304alg = U (l, Q).\nn2\n\n(34)\n\nFor the case of infinite (but bounded) range D, similarly to [25], we use the fact that the\nloss function l is Lipschitz and take an \u01eb-approximation of D. We thus have\nEQVn\n\n1\n1\nL\u0304alg \u2264\nmin\nEQV\nL\n(X 0 )\n2\nm(n)\nn\nm(n)2 (\u03a8,F )\n(\u03a8,F )\u2208S(Vm(n) )\n2\n\n+ cm(n) \u01eb (m(n)) + O\n\n\u0012\n\nm(n) p\nlog \u03bb\nn\n\n1\n1\n4\nfor some constant c. Choosing \u01eb (m(n)) = m(n)\n4 results in |D| = 2 m(n) , hence\n\u221a\nstill decays to zero for any m(n) = O( log n) and (34) is still valid.\n\n20\n\n\u0013\n\n(35)\n\nm(n) \u221a\nlog \u03bb\nn\n\n\fNote that the proof of Theorem 8 does not use the well established theory of universal\nprediction. Instead, the exponential weighting algorithm is used for all possible scans (within\na block) as well as all possible predictors. This is since important parts of the work on prediction in the probabilistic scenario include some assumption on the stationarity of the measure\ngoverning the process, such as stationarity or asymptotically mean stationarity [35].3 In the\nscandiction scenario, however, the properties of the output sequence are not easy to determine,\nand it is possible, in general, that the output sequence is not stationary or ergodic even if the\ninput data array is. Thus, although under certain assumptions, one can use a single universal\npredictor, applied to any scan in a certain set of scans, this is not the case in general.\n\n3.5\n\nUniversal Scandiction for Mixing Random Fields\n\nThe proof of Theorem 4 established the universality of (\u03a8\u0302, F\u0302 )n under the expected cumulative loss criterion. In order to establish its universality in the Q-a.s. sense, we examine the\nconditions on the measure Q such that the following equality holds.\n2\n\nK\n1 X\nLj (xi ) = EQVm Lj (X 0 ) Q \u2212 a.s.\nlim\nn\u2192\u221e K 2\n\n(36)\n\ni=1\n\nTo this end, we briefly review the conditions for the individual ergodic theorem for general\ndynamical systems given in [37], specialized for Z2 . Let {An } be a sequence of subsets of Z2 .\nFor each n, the set An is the set of sites over which the arithmetical average is taken. Let A\u25b3B\ndenote the symmetric difference between the sets A and B, A \u222a B r A \u2229 B, and remember that\n\u03c4i (x)j = xj+i .\nCondition 1 ([37, E1\u2032 ]). For all i \u2208 Z2 ,\nlim\n\nn\u2192\u221e\n\n|An \u25b3\u03c4i (An )|\n= 0.\n|An |\n\n(37)\n\nCondition 2 ([37, E3\u2032\u2032 ]). There exists a constant C1 < \u221e such that for all n,\n|k : k = i \u2212 j,\n\ni, j \u2208 An | \u2264 C1 |An |.\n\n(38)\n\nCondition 3 ([37, E4]). There exists a sequence of measurable sets {Mn } such that,\nlim inf\nn\u2192\u221e\n\n|k : k = i + j,\n\ni \u2208 An , j \u2208 Mn |\n= C2 < \u221e.\n|Mn |\n\n(39)\n\nBy [37, Theorem 6.1\u2032 ], if the sequence {An } satisfies conditions 1-3, then, for any stationary\nrandom field X with E|X0 | < \u221e, we have,\n1 X\nXi = E{X0 |I} Q \u2212 a.s.,\nlim\nn\u2192\u221e |An |\ni\u2208An\n\n3\n\nAn important exception is the Kalman filter [36, Section 7.7].\n\n21\n\n(40)\n\n\fwhere Q is the measure governing X and I is the \u03c3-algebra of invariant sets of \u03a9, that is,\nA \u2208 I iff \u03c4i (A) = A for all i \u2208 Z2 .\n\n(41)\n\nIf Q is ergodic, namely, for each A \u2208 I, Q(A) \u2208 {0, 1}, then E{X0 |I} is deterministic and\nequals EX0 .\nClearly, since Lj (xi ) depends on a set of m2 sites, with the average in taken over the sets\nAn = {i : i = m * j, j \u2208 VK }, (36) may not hold, even if Q is ergodic, as, for example, Condition\n\n1 is not satisfied.4 These two obstacles can be removed by defining an alternative random field,\nX\u0303, over the set of sites m * Z2 = {j : j = m * i, i \u2208 Z2 }, where X\u0303i equals Lj (X k ) and X k is the\ncorresponding m \u00d7 m block of X. Note that since the loss function l(*) is bounded and m is\nfinite, E|X\u03030 | < \u221e. It is not hard to see that conditions 1-3 are now satisfied (with the new\n\nspace being m * Z2 ). However, for E{X\u03030 |Im } to be deterministic, where Im is the \u03c3-algebra\nof m-invariant sets,\nA \u2208 Im iff \u03c4j (A) = A\n\nfor all j = i * m, i \u2208 Z2 ,\n\n(42)\n\nit is required that Im is the trivial \u03c3-algebra. In other words, block ergodicity of Q is required.\nWe now show that if the measure Q is strongly mixing, then it is block-ergodic for any\nfinite block size. For A, B \u2208 Z2 , define\n\u03b1Q (A, B) = sup{|Q(U \u2229 V ) \u2212 Q(U )Q(V )|, U \u2208 \u03c3(XA ), V \u2208 \u03c3(XB )},\n\n(43)\n\nwhere \u03c3(XB ) is the smallest sigma algebra generated by XB . Let \u03b1Q\na,b (k) denote the strong\nmixing coefficient [38, Sec. 1.7] of the random field Q\nQ\n\u03b1Q\na,b (k) = sup{\u03b1 (A, B), |A| \u2264 a, |B| \u2264 b, d(A, B) \u2265 k},\n\n(44)\n\nwhere d is a metric on Z2 and d(A, B) is the distance between the closest points, i.e., d(A, B) =\nmini\u2208A,j\u2208B d(i, j). Assume now that Q is strongly mixing in the sense that for all a, b \u2208\n\nN \u222a {\u221e}, \u03b1Q\na,b (k) \u2192 0 as k \u2192 \u221e. It is easy to see that Q(A) \u2208 {0, 1} for all A \u2208 Im . Indeed,\nlim\n\nd(i,0)\u2192\u221e\n\n|Q(\u03c4i*m (A) \u2229 A) \u2212 Q(\u03c4i*m (A))Q(A)| = 0,\n\n(45)\n\nhowever, since A is m-invariant, \u03c4i*m (A) = A and thus Q(A) = Q(A)2 . Hence Q is m-block\nergodic for each m (i.e., totally ergodic).\nThe following theorem asserts that under the assumption that the random field Q is strongly\nmixing, the results of Theorem 4 apply in the a.s. sense as well.\n4\n\nIn fact, Tempelman's work [37] also includes slightly weaker conditions, but neither are satisfied in the current\n\nsetting.\n\n22\n\n\fTheorem 9. Let X be a stationary strongly mixing random field with a probability measure\nQ. Let F = {Fn } be a sequence of finite sets of scandictors and assume that UF (l, Q) exists.\nThen, if the universal algorithm suggested in the proof of Theorem 4 uses a fixed block size m,\nwe have\nlim inf\nn\u2192\u221e\n\n1\nLalg (XVn ) \u2264 UF (l, Q) + \u03b4(m)\n|Vn |\n\nQ \u2212 a.s.\n\n(46)\n\nfor any such Q and some \u03b4(m) such that \u03b4(m) \u2192 0 as m \u2192 \u221e.\nProof. For each xVn , we have,\n2\n\n1\nLmin (xVn ) =\n|Vn |\n\u2264\n\n(K+1)\nX\n1\nLj (xi )\nmin\n|Vn | (\u03a8,F )j \u2208Fm\ni=1\n\uf8eb\n\uf8f6\nK2\nX\n1 \uf8ed\nmin\nLj (xi ) + 2Km(n \u2212 Km)lmax + (n \u2212 Km)2 lmax \uf8f8\n|Vn | (\u03a8,F )j \u2208Fm\ni=1\n\n2\n\n\u2264\n\nK\nm\n1 X\n1\nLj (xi ) + 2 lmax .\nmin\n2\n|Vm | (\u03a8,F )j \u2208Fm K\nn\n\n(47)\n\ni=1\n\nBy Proposition 5,\n\nThus,\n\n1\nm(n + m) p\n1\nlmax\nLmin (xVn ) +\nL\u0304alg (xVn ) \u2264\nlog \u03bb \u221a .\n|Vn |\n|Vn |\n|Vn |\n2\n\nlim inf\nn\u2192\u221e\n\n1\n1\nL\u0304alg (xVn ) \u2264 lim inf\nLmin (xVn )\nn\u2192\u221e |Vn |\n|Vn |\n\u2264\n\n(48)\n\n2\n\nK\n1\n1 X\nLj (xi )\nlim inf min\n|Vm | n\u2192\u221e (\u03a8,F )j \u2208Fm K 2\ni=1\n2\n\n\u2264\n\nK\n1\n1 X\nLj (xi ).\nmin lim inf\n|Vm | (\u03a8,F )j \u2208Fm n\u2192\u221e K 2\n\n(49)\n\ni=1\n\nSince K \u2192 \u221e as n \u2192 \u221e, by the block ergodicity of Q and the fact that for finite m and each\n(\u03a8, F )j \u2208 Fm , Lj (X) is a bounded function, it follows that\n2\n\nK\n1 X\nLj (xi ) = EQVm Lj (X 0 ) Q \u2212 a.s.\nlim inf 2\nn\u2192\u221e K\n\n(50)\n\ni=1\n\nFinally, since UF (l, Q) exists, there exists \u03b4(m) such that \u03b4(m) \u2192 0 as m \u2192 \u221e and we have\nlim inf\nn\u2192\u221e\n\n1\nL\u0304alg (xVn ) \u2264 UF (l, Q) + \u03b4(m)\n|Vn |\n\nQ \u2212 a.s.\n\n(51)\n\nThe fact that L\u0304alg (xVn ) converges to Lalg (xVn ) a.s. is clear from the proof of Proposition 6.\nVery similar to Theorem 9, we also have the following corollary.\n\n23\n\n\fCorollary 10. Let X be a stationary strongly mixing random field over a finite alphabet A and\na probability measure Q. Let the prediction space D be either finite or bounded (with l(x, F )\nthen being Lipschitz in its second argument). Then, there exists a sequence of scandictors\n{(\u03a8, F )n }, independent of Q, for which\nlim inf\nn\u2192\u221e\n\n1\nLalg (XVn ) \u2264 U (l, Q) + \u03b4(m)\n|Vn |\n\nQ \u2212 a.s.\n\n(52)\n\nfor any such Q and some \u03b4(m) such that \u03b4(m) \u2192 0 as m \u2192 \u221e. Thus, when m \u2192 \u221e, the\nperformance of {(\u03a8, F )n } equals the scandictability of the source, Q \u2212 a.s.\n\n3.6\n\nUniversal Scandiction for Individual Images\n\nThe proofs of Theorems 4 and 9 relied on the stationarity, or the stationarity and mixing\nproperty, of the random field X (respectively). In the proof of Theorem 4, we used the fact\nthat the cumulative loss of any scandictor (\u03a8, F ) on a given block of data has the same\nexpected value as that on any other block. In the proof of Theorem 9, on the other hand,\nthe fact that the Cesaro mean of the losses on finite blocks converges to a single value, the\nexpected cumulative loss, was used.\nWhen x is an individual image, however, the cumulative loss of the suggested algorithm may\nbe higher than that of the best scandictor in the scandictors set since restarting a scandictor\nat the beginning of each block may result in arbitrarily larger loss compared to the cumulative\nloss when the scandictor scans the entire data. Compared to the prediction problem, in the\nscandiction scenario, if the scanner is arbitrary, then different starting conditions may yield\ndifferent scans (i.e., a different reordering of the data) and thus arbitrarily different cumulative\nloss, even if the predictor attached to it is very simple, e.g., a Markov predictor. It is expected,\nhowever, that when the scandictors have some structure, it will be possible to compete with\nfinite sets of scandictors in the individual image scenario.\nIn this subsection, we suggest a basic scenario under which universal scandiction of individual images is possible. Further research in this area is required, though, in order to identify\nlarger sets of scandictors under which universality is achievable. As mentioned earlier, since\nthe exponential weighting algorithm used in the proofs of Theorems 4 and 9 applied only\nblock-wise scandictors, i.e., scandictors which scan every block of the data separately from all\nother blocks, stationarity or stationarity and ergodicity of the data were required in order to\nprove its convergence. Here, since the data is an individual image, we impose restrictions on\nthe families of scandictors in order to achieve meaningful results (this reasoning is analogous\n\n24\n\n\fto that described in [23, Section I-B] for the prediction problem). The first restriction is that\nthe scanners with which we compete are such that the actual path taken by each scanner when\nit is applied in a block-wise order has some kind of an overlap (in a sense which will be defined\nlater) with the path taken when it is applied to the whole image. The second restriction is\nthat the predictors are Markovian of finite order (i.e., the prediction depends only on the last k\nsymbols seen, for some finite k). Note that the first restriction does not restrict us to compete\nonly with scandictors which operate in a block-wise order, only requires that the excess loss\ninduced when the scandictors operate in a block-wise order, compared to operating on the\nentire image, is not too large, if, in addition, the predictor is Markovian.\nThe following definition, and the results which follow, make the above requirements precise.\nFor two scanners \u03a8 and \u03a8\u2032 for the data array xB , define NB,K (xB , \u03a8, \u03a8\u2032 ) as the number of\nsites in B such that their immediate past (context of unit length) under \u03a8 is contained in the\ncontext of length K under \u03a8\u2032 , namely,\nNB,K (xB , \u03a8, \u03a8\u2032 ) =\n\n\b\n\n1 \u2264 i \u2264 |B| : \u22031\u2264j\u2264|B|,k\u2264K\n\n(\u03a8i , \u03a8i\u22121 ) = (\u03a8\u2032j , \u03a8\u2032j\u2212k )\n\n.\n\n(53)\n\nNote that in the above definition, a \"context\" of size w for a site in B refers to the set of w sites\nwhich precede it in the discussed scan, and not their actual values. When {\u03a8n } is a sequence\nof scanners, where \u03a8n is a scanner for Vn , it will be interesting to consider the number of sites\nin B \u2282 Vn2 , where B is an n1 \u00d7 n1 rectangle, n1 \u2264 n2 , such that their immediate past under\n\u03a8n2 (applied to Vn2 ) is contained in the context of length K under \u03a8n1 (applied to B), that is\nNB,K (xB , \u03a8n2 , \u03a8n1 ) =\n\nn\n1 \u2264 i \u2264 |B| : \u22031\u2264j\u2264|Vn2 |,k\u2264K\n\no\n(\u03a8n2 ,i , \u03a8n2 ,i\u22121 ) = (\u03a8n1 ,j , \u03a8n1 ,j\u2212k ) ,\n\n(54)\n\nwhere \u03a8n* ,i is the i'th site the scanner \u03a8n* visits. The following proposition is proved in\nAppendix A.3.\nProposition 11. Consider two scanners \u03a8 and \u03a8\u2032 for B such that for any individual image\nxB we have\n\nNB,K (xB , \u03a8, \u03a8\u2032 )\n= 1 \u2212 o(|B|).\n|B|\n\n(55)\n\nL(\u03a8\u2032 ,F Kw,opt ) (xB ) \u2264 L(\u03a8,F w,opt ) (xB ) + o(|B|)(K + 1)w\u22121 lmax ,\n\n(56)\n\nThen, for any xB ,\n\nwhere for each scandictor (\u03a8, F w,opt ), F w,opt denotes the optimal w-order Markov predictor for\nthe scan \u03a8.\n\n25\n\n\fNote that in order to satisfy the condition in (55) for any array xB , it is likely (but not\na compulsory) that both \u03a8 and \u03a8\u2032 are data-independent scans. However, they need not be\nidentical. If, for example, \u03a8 is a raster scan from left to right, and \u03a8\u2032 applies the same left to\nright scan, but with a different ordering of the rows, then the condition is satisfied for any xB .\nThe result of Proposition 11 yields the following corollary, which gives sufficient conditions\non the scandictors sets under which a universal scandictor for any individual image exists. The\nproof can be found in Appendix A.4.\nCorollary 12. Let {Fn }, |Fn | = \u03bb < \u221e, be a sequence of scandictor sets, where\n\nFn = {(\u03a81n , F 1 ), (\u03a82n , F 2 ), . . . , (\u03a8\u03bbn , F \u03bb )} is a set of scandictors for Vn . Assume that the predictors are Markov of finite order w, the prediction space D is finite, and that there exists\nm(n) = o(n) (yet m(n) \u2192 \u221e as n \u2192 \u221e) such that for all 1 \u2264 i \u2264 \u03bb, n, and xVn we have\n\u0010\n\u0011\nNBm(n) ,K xBm(n) , \u03a8in , \u03a8im(n)\n\u0001\n= 1 \u2212 o m(n)2 ,\n(57)\n2\nm(n)\nk2\nj\nn\nsub-blocks of size m(n)\u00d7 m(n) of Vn . Then, there exists\nwhere Bm(n) is any one of the m(n)\na sequence of scandictors {(\u03a8\u0302, F\u0302 )n } such that for any image x\nlim inf E\nn\u2192\u221e\n\n1\n1\nL(\u03a8\u0302,F\u0302 )n (xVn ) \u2264 lim inf min\nL(\u03a8,F ) (xVn )\nn\u2192\u221e\n|Vn |\n(\u03a8,F )\u2208Fn |Vn |\n\n(58)\n\nwhere the expectation in the l.h.s. of (58) is due to the possible randomization in (\u03a8\u0302, F\u0302 )n .\nAlthough the condition in (57) is limiting, and may not be met by many data-dependent\nscans, Corollary 12 still answers on the affirmative the following basic question: do there exist\nscandictor sets for which one can find a universal scandictor in the individual image scenario?\nFor example, by Corollary 12, if the scandictor set includes all raster-type scans (e.g., left-toright, right-to-left, up-down, down-up, diagonal, etc.), accompanied with Markov predictors of\nfinite order, then there exists a universal scandictor whose asymptotic normalized cumulative\nloss is less or equal than that of the best scandictor in the set, for any individual image x. The\ncondition in (57) is also satisfied for some well-known \"self-similar\" space filling curves, such\nas the Sierpinski or Lebesgue curves [39].\n\n4\n\nBounds on the Excess Scandiction Loss for Non-\n\nOptimal Scanners\nWhile the results of Section 3 establish the existence of a universal scandictor for all stationary\nrandom fields and bounded loss function (under the terms of Theorem 8), it is interesting to\n\n26\n\n\finvestigate, from both practical and theoretical reasons, what is the excess scandiction loss\nwhen non-optimal scanners are used. I.e., in this section we answer the following question:\nSuppose that, for practical reasons for example, one uses a non-optimal scanner, accompanied\nwith the optimal predictor for that scan. How large is the excess loss incurred by this scheme\nwith respect to optimal scandiction?\nFor the sake of simplicity, we consider the scenario of predicting the next outcome of a\nbinary source, with D = [0, 1] as the prediction space. Hence, l : {0, 1} \u00d7 [0, 1] \u2192 R is the loss\nfunction. Furthermore, we assume deterministic scanner (though data-dependent, of course).\nThe generalization to randomized scanners is cumbersome but straightforward.\nLet \u03c6l denote the Bayes envelope associated with l, i.e.,\n\u03c6l (p) = min [(1 \u2212 p)l(0, q) + pl(1, q)].\n\n(59)\n\n\u01ebl = min max |\u03b1hb (p) + \u03b2 \u2212 \u03c6l (p)|,\n\n(60)\n\nq\u2208[0,1]\n\nWe further define\n\u03b1,\u03b2 0\u2264p\u22641\n\nwhere hb (p) is the binary entropy function. Thus \u01ebl is the error in approximating \u03c6l (p) by the\nbest affine function of hb (p). For example, when l is the Hamming loss function, denoted by\nlH , we have \u01eblH = 0.08 and when l is the squared error, denoted by ls , \u01ebls = 0.0137. For the\nlog loss, however, the expected instantaneous loss equals the conditional entropy, hence the\nexpected cumulative loss coincides with the entropy, which is invariant to the scan, and we\nhave \u01ebl = 0. To wit, the scan is inconsequential under log loss.\nAlthough the definitions of \u03c6l (p) and \u01ebl refer to the binary scenario, the results below\n(Theorem 13 and Propositions 14 and 15) hold for larger alphabets, with \u01ebl defined as in (60),\nwith the maximum ranging over the simplex of all distributions on the alphabet, and h(p)\n(replacing hb (p)) and \u03c6l (p) denoting the entropy and Bayes envelope of the distribution p,\nrespectively.\n1\nL(\u03a8,F opt ) (XB ) denote the\nLet \u03a8 be any (possibly data dependent) scan, and let EQB |B|\n\nexpected normalized cumulative loss in scandicting XB with the scan \u03a8 and the optimal\npredictor for that scan, under the loss function l. Remembering that U (l, QB ) denotes the\n1\nL(\u03a8,F opt) (XB ),\nscandictability of XB w.r.t the loss function l, namely, U (l, QB ) = inf \u03a8 EQB |B|\n\nour main result in this section is the following.\nTheorem 13. Let XB be an arbitrarily distributed binary field. Then, for any scan \u03a8,\nEQB\n\n1\nopt (XB ) \u2212 U (l, QB ) \u2264 2\u01ebl .\nL\n|B| (\u03a8,F )\n\n27\n\n(61)\n\n\fThat is, the excess loss incurred by applying any scanner \u03a8, accompanied with the optimal\npredictor for that scan, with respect to optimal scandiction is not larger than 2\u01ebl .\nTo prove Theorem 13, we first introduce a prediction result (i.e., with no data reordering)\non the error in estimating the cumulative loss of a predictor under a loss function l with the\nbest affine function of the entropy. We then generalize this result to the multi-dimensional\ncase.\nn\nProposition 14. Let X n be an arbitrarily distributed binary n-tuple and let ELopt\nl (X ) denote\n\nthe expected cumulative loss in predicting X n with the optimal distribution-dependent scheme\nfor the loss function l. Then,\n1\n1\nn\n\u03b1l H(X n ) + \u03b2l \u2212 ELopt\nl (X ) \u2264 \u01ebl ,\nn\nn\n\n(62)\n\nwhere \u03b1l and \u03b2l are the achievers of the minimum in (60).\nProof. Let \u03b1l and \u03b2l be the achievers of the minimum in (60). We have,\n1\n1\nn\n\u03b1l H(X n ) + \u03b2l \u2212 ELopt\nl (X )\nn\nn\nn\n1 XX\nP (xt\u22121 )\n=\nn\nt\nt=1 x\n\nh\n\n(a)\n\n=\n\ni\n\u2212\u03b1l P (xt |xt\u22121 ) log P (xt |xt\u22121 ) + P (xt |xt\u22121 )\u03b2l \u2212 P (xt |xt\u22121 )l(xt , Ftopt (xt\u22121 ))\n\nn\n\u0002\n\u0003\n1 XX\nP (xt\u22121 ) \u03b1l hb (P (*|xt\u22121 )) + \u03b2l \u2212 \u03c6l (P (*|xt\u22121 ))\nn\nt\u22121\n\n\u2264\n\n1\nn\n\n\u2264\n\n1\nn\n\nt=1 x\nn\nXX\n\nt=1 xt\u22121\nn X\nX\nt=1 xt\u22121\n\nP (xt\u22121 ) \u03b1l hb (P (*|xt\u22121 )) + \u03b2l \u2212 \u03c6l (P (*|xt\u22121 ))\nP (xt\u22121 ) max |\u03b1l hb (p) + \u03b2l \u2212 \u03c6l (p)|\np\n\n= max |\u03b1l hb (p) + \u03b2l \u2212 \u03c6l (p)|\np\n\n= \u01ebl ,\n\n(63)\n\nwhere (a) is by the definition of \u03c6l (*) and the optimality of Ftopt with respect to l.\nThe following proposition is the generalization of Proposition 14 to the multi-dimensional\ncase.\nProposition 15. Let XB be an arbitrarily distributed binary random field. Then, for any scan\n\u03a8,\n\u03b1l\n\n1\n1\nopt (XB ) \u2264 \u01ebl ,\nH(XB ) + \u03b2l \u2212 EQB\nL\n|B|\n|B| (\u03a8,F )\n\n28\n\n(64)\n\n\fwhere \u03b1l and \u03b2l are the achievers of the minimum in (60).\nFor data-independent scans, the proof follows the proof of Proposition 14 verbatim by applying it to the reordered |B|-tuple X\u03a81 , . . . , X\u03a8|B| and remembering that H(XB ) = H(X\u03a81 , . . . , X\u03a8|B| ).\nFor data-dependent scans, the proof is similar, but requires more caution.\nProof of Proposition 15. Let \u03b1l and \u03b2l be the achievers of the minimum in (60). For a given\ndata array xB , \u03a81 , \u03a82 (x\u03a81 ), . . . , \u03a8|B| (x\u03a8|B|\u22121 ) are fixed, and merely reflect a reordering of xB\nas a |B|-tuple. Thus,\n\u03b1l\n\n1\n1\nopt (XB )\nH(XB ) + \u03b2l \u2212 EQB\nL\n|B|\n|B| (\u03a8,F )\n\uf8f6\n\uf8eb\n|B|\nX\n1 X\uf8ed\nl(x\u03a8t , Ftopt (x\u03a8t\u22121 ))\uf8f8 + \u03b2l\n\u2212\u03b1l P (xB ) log P (xB ) \u2212 P (xB )\n=\n|B| x\nt=1\nB\n\uf8eb\n\uf8f6\n|B|\n|B|\nX\nX\n1 X\uf8ed\n=\n\u2212\u03b1l P (xB )\nlog P (x\u03a8t |x\u03a8t\u22121 ) \u2212 P (xB )\nl(x\u03a8t , Ftopt (x\u03a8t\u22121 ))\uf8f8 + \u03b2l\n|B| x\nt=1\n\nB\n\nt=1\n\n|B|\n\u0010\n\u0011\n1 XX\n=\nP (xB ) \u2212 \u03b1l log P (x\u03a8t |x\u03a8t\u22121 ) + \u03b2l \u2212 l(x\u03a8t , Ftopt (x\u03a8t\u22121 )) .\n|B|\nx\nt=1\n\n(65)\n\nB\n\nFix t = t0 in the sum over t. Consider all data arrays xB such that for a specific scanner \u03a8\nwe have\nn\n\no\n\u03a81 , \u03a82 (x\u03a81 ), . . . , \u03a8t0 \u22121 (x\u03a81 , . . . , x\u03a8t0 \u22122 ) = I(\u03a8),\n\n(66)\n\nwhere I(\u03a8) \u2282 B is a fixed set of sites, and (x\u03a81 , . . . , x\u03a8t0 \u22121 ) = a, for some a \u2208 {0, 1}t0 \u22121 . In\nthis case, \u03a8t (x\u03a8t0 \u22121 ) is also fixed, and since the term in the parentheses of (65) depends only\n\non I, a and x\u03a8t0 , we have\nX\nxn\n\n=\n\n\u0010\n\u0011\n\u03a8t0 \u22121\nP (xn ) \u2212 \u03b1l log P (x\u03a8t0 |x\u03a8t0 \u22121 ) + \u03b2l \u2212 l(x\u03a8t0 , Ftopt\n(x\n))\n0\n\nX\nI,a\n\nP (xI = a)\n\nX\n\nx\u03a8t \u2208{0,1}\n0\n\n\u0010\n\u0011\nP (x\u03a8t0 |xI = a) \u2212 \u03b1l log P (x\u03a8t0 |xI = a) + \u03b2l \u2212 l(x\u03a8t0 , Ftopt\n.\n(a))\n0\n(67)\n\n29\n\n\fConsequently,\n\u03b1l\n\n1\n1\nopt (XB )\nH(XB ) + \u03b2l \u2212 EQB\nL\n|B|\n|B| (\u03a8,F )\n|B|\n\n1 XX\n\u2264\nP (xI = a)\n|B| t=1\nI,a\nX\n\nx\u03a8t \u2208{0,1}\n\n\u0010\n\u0011\nP (x\u03a8t |xI = a) \u2212 \u03b1l log P (x\u03a8t |xI = a) + \u03b2l \u2212 l(x\u03a8t , Ftopt (a))\n\n|B|\n\n1 XX\n=\nP (xI = a) |\u03b1l hb (P (*|xI = a)) + \u03b2l \u2212 \u03c6l (P (*|xI = a))|\n|B| t=1\nI,a\n|B|\n\n1 XX\nP (xI = a) max |\u03b1l hb (p) + \u03b2l \u2212 \u03c6l (p)|\n\u2264\np\n|B| t=1\nI,a\n= max |\u03b1l hb (p) + \u03b2l \u2212 \u03c6l (p)|\np\n\n= \u01ebl .\n\n(68)\n\nIt is now easy to see why Theorem 13 holds.\nProof of Theorem 13. The proof is a direct application of Proposition 15, as for any scan \u03a8,\nEQB\n\n1\nopt (XB ) \u2212 U (l, QB )\nL\n|B| (\u03a8,F )\n\u03b1l\n1\n\u03b1l\n\u2264\nH(XB ) + \u03b2l \u2212 EQB\nL(\u03a8,F opt) (XB ) +\nH(XB ) + \u03b2l \u2212 U (l, QB )\n|B|\n|B|\n|B|\n\u2264 2\u01ebl .\n(69)\n\nAt this point, a few remarks are in order. For the bound in Theorem 13 to be tight, the\nfollowing conditions should be met. First, equality is required in (64) for both the scan \u03a8\nand the optimal scan (which achieves U (l, QB )). It is not hard to see that for a given scan \u03a8,\nequality in (64) is achieved if and only if P (*|x\u03a8t\u22121 ) = p for all x\u03a8t\u22121 , where p is a maximizer\nof (60). However, for (61) to be tight, it is also required that\n\u03b1l\n1\n\u03b1l\nH(XB ) + \u03b2l \u2212 EQB\nL(\u03a8,F opt ) (XB ) = \u2212\nH(XB ) \u2212 \u03b2l + U (l, QB ),\n|B|\n|B|\n|B|\n\n(70)\n\nso the triangle inequality is held with equality. Namely, it is required that under the scan\n\u03a8, for example, P (*|x\u03a8t\u22121 ) = p for all x\u03a8t\u22121 , where p is such that \u03b1l hb (p) + \u03b2l \u2212 \u03c6l (p) = \u01ebl ,\n\u2032\n\n\u2032\n\nyet under the optimal scan, say \u03a8\u2032 , P (*|x\u03a8t\u22121 ) = p\u2032 for all x\u03a8t\u22121 , where p\u2032 is such that\n\n30\n\n\f\u03b1l hb (p\u2032 ) + \u03b2l \u2212 \u03c6l (p\u2032 ) = \u2212\u01ebl . Clearly this is not always the case, and thus, generally, the bound\nin Theorem 13 is not tight. Indeed, although under a different setting (individual images),\nin subsection 4.1 we derive a tighter upper bound on the excess loss for the specific case of\nHamming loss. Using this bound, it is easy to see that the 0.16 bound given here (as \u01ebl = 0.08\nfor Hamming loss) is only a worst case, and typically much tighter bounds on the excess loss\napply, depending on the image compressibility. For example, consider a 1st order symmetric\nMarkov chain with transition probability 1/4. Scanning this source in the trivial (sequential)\norder results in an error rate of 1/4. By [22], this is indeed the optimal scanning order for this\nsource, as it can be represented as an autoregressive process whose innovation process has a\nmaximum entropy distribution with respect to the Hamming distance. The \"odds-then-evens\"\nscan5 , however, which was proved useful for this source but with larger transition probabilities\n(larger than 1/2, [22]), results in an error rate of 5/16, which is 1/16 away from the optimum.\nIt is not hard to show that different transition probabilities result in lower excess loss.\n\n4.1\n\nIndividual Images and the Peano-Hilbert Scan\n\nIn this subsection, we seek analogous results for the individual image scenario. Namely, the\ndata array xB has no stochastic model. A scandictor (\u03a8, F ), in this case, wishes to minimize\nthe cumulative loss over xVn , that is, L(\u03a8,F ) (xVn ) as defined in (3).\nIn this setting, although one can easily define an empirical probability measure, the invariance of the entropy H(X n ) to the reordering of the components, which stood at the heart\nof Theorem 13, does not hold for any reordering (scan) and any finite n. Thus, we limit the\npossible set of scanners to that of the finite state machines discussed earlier. Moreover, in\nthe sequel, we do not bound the difference in the scandiction losses of any two scandictors\nfrom that set, only that between the Peano-Hilbert scan (which is asymptotically optimal for\ncompression of individual images [14]) and any other finite state scanner (both accompanied\nwith an optimal Markov predictor), or between two scans (finite state or not) for which the\nFS compressibility of the resulting sequence is the same.\n|B|\n\nWe start with several definitions. Let \u03a8B be a scanner for the data array xB . Let x1\n\nbe the sequence resulting from scanning xB with \u03a8B . Fix k < |B| and for any s \u2208 {0, 1}k+1\n5\n\nAn \"odds-then-evens\" scanner for a one-dimensional vector xn1 , first scans all the sites with an odd index, in an\n\nascending order, then all the sites with an even index.\n\n31\n\n\fdefine the empirical distribution of order k + 1 as\n(s) =\nP\u0302\u03a8k+1\nB\n\n\b\n1\nk < i \u2264 |B| : xii\u2212k = s\n|B| \u2212 k\n\n.\n\n(71)\n\n(s),\nThe distributions of lower orders, and the conditional distribution are derived from P\u0302\u03a8k+1\nB\ni.e., for s\u2032 \u2208 {0, 1}k and x \u2208 {0, 1} we define\n([s\u2032 , 1])\n([s\u2032 , 0]) + P\u0302\u03a8k+1\n(s\u2032 ) = P\u0302\u03a8k+1\nP\u0302\u03a8k+1\nB\nB\nB\nand\n(x|s\u2032 ) =\nP\u0302\u03a8k+1\nB\n\n([s\u2032 , x])\nP\u0302\u03a8k+1\nB\n(s\u2032 )\nP\u0302\u03a8k+1\nB\n\n(72)\n\n,\n\n(73)\n\nk+1\n(X|X k ) be the\nwhere 0/0 is defined as 1/2 and [*, *] denotes string concatenation.6 Let \u0124\u03a8\nB\n\nempirical conditional entropy of order k, i.e.,\nk+1\n(X|X k ) = \u2212\n\u0124\u03a8\nB\n\nX\n\n(s)\nP\u0302\u03a8k+1\nB\n\ns\u2208{0,1}k\n\nX\n\n(x|s).\n(x|s) log P\u0302\u03a8k+1\nP\u0302\u03a8k+1\nB\nB\n\n(74)\n\nx\u2208{0,1}\n\nFinally, denote by F k,opt the optimal k-th order Markov predictor, in the sense that it minimizes\n|B|\n\n(*|*) and x1 . The following proposition is the individual\nthe expected loss with respect to P\u0302\u03a8k+1\nB\nimage analogue of Proposition 15.\nProposition 16. Let xB be any data array. Let\n\n1\n|B| L(\u03a8B ,F k,opt ) (xB )\n\ndenote the normalized\n\ncumulative loss of the scandictor (\u03a8B , F k,opt ), where \u03a8B is any (data dependent) scan and\nF k,opt is the optimal k-th order Markov predictor with respect to \u03a8B and l. Then,\nk+1\n(X|X k ) + \u03b2l \u2212\n\u03b1l \u0124\u03a8\nB\n\nklmax\n1\nL(\u03a8B ,F k,opt ) (xB ) \u2264 \u01ebl +\n,\n|B|\n|B|\n\n(75)\n\nwhere \u03b1l and \u03b2l are the achievers of the minimum in (60).\n|B|\n\nSince xB is an individual image, x1 = \u03a8B (xB ) is fixed. In that sense, the proof resembles\nthat of Proposition 14 and we write xt for the value of x at the t-th site \u03a8B visits. On the\n(*) and avoid the\nother hand, since the order of the predictor, k, is fixed, we can use P\u0302\u03a8k+1\nB\nsummation over the time index t. The complete details can be found in Appendix A.5.\nThe bound in Proposition 16 differs from the one in Proposition 15 for two reasons. First,\nk+1\n(X|X k )\nit is only asymptotic due to the O(k/|B|) term. Second, the empirical entropy \u0124\u03a8\nB\n\nis not invariant to the scanning order. This is a profound difference between the random and\n6\n\nNote that defining P\u0302\u03a8k+1\n(x|s\u2032 ), s\u2032 \u2208 {0, 1}k as\nB\n\nk+1\nP\u0302\u03a8\n([s\u2032 ,x])\nB\n\nk (s\u2032 )\nP\u0302\u03a8\nB\n\nP\u0302\u03a8k+1\n([s\u2032 , 1]).\nB\n\n32\n\nis not consistent since generally P\u0302\u03a8k B (s\u2032 ) 6= P\u0302\u03a8k+1\n([s\u2032 , 0])+\nB\n\n\fthe individual settings, and, in fact, is at the heart of [14]. In the random setting, the chain\nrule for entropies implies invariance of the entropy rate to the scanning order. This fact does\nnot hold for a k-th order empirical distribution of an individual image, hence the usage of the\nPeano-Hilbert scanning order.7 Consequently, we cannot directly compare between any two\nscans. Nevertheless, Proposition 16 has the following two interesting applications, given by\nProposition 17 and Corollary 18.\nFor \u03a8 = {\u03a8n }, where \u03a8n is a scan for Vn , and an infinite individual image x define\nLk\u03a8 (x) = lim sup\nn\u2192\u221e\n\n1\nL\nk,opt ) (xVn )\n|Vn | (\u03a8n ,F\n\n(76)\n\nand\nL\u03a8 (x) = lim Lk\u03a8 (x).\nk\u2192\u221e\n\n(77)\n\nProposition 17 relates the asymptotic cumulative loss of any sequence of finite state scans\n\u03a8 to that resulting from the Peano-Hilbert sequence of scans, establishing the Peano-Hilbert\nsequence as an advantageous scanning order for any loss function.\nProposition 17. Let x be any individual image. Let P H denote the Peano-Hilbert sequence of\nscans. Then, for any sequence of finite state scans \u03a8 and any loss function l : {0, 1} \u00d7 [0, 1] \u2192\nR,\nLP H (x) \u2264 L\u03a8 (x) + 2\u01ebl .\n\n(78)\n\nBefore we prove Proposition 17, define the asymptotic k-th order empirical conditional\nentropy under {\u03a8n } as\n\nk+1\nk+1\n(X|X k )\n(x) = lim sup \u0124\u03a8\n\u0124\u03a8\nn\n\n(79)\n\nk+1\n(x).\n\u0124\u03a8 (x) = lim \u0124\u03a8\n\n(80)\n\nn\u2192\u221e\n\nand further define\nk\u2192\u221e\n\nThe existence of \u0124\u03a8 (x) is established later in the proof of Proposition 17, where it is also\nk (X k ). By [15, Theorem 3], the latter\nshown that this limit equals limk\u2192\u221e lim supn\u2192\u221e k1 \u0124\u03a8\nn\n\nlimit is no other than the asymptotic finite state compressibility of x under the sequence of\nscans \u03a8, namely,\n1 k\nlim lim sup \u0124\u03a8\n(X k ) = \u03c1(\u03a8(x))\nn\nk\u2192\u221e n\u2192\u221e k\n= lim lim sup \u03c1E(s) (\u03a8n (xVn )),\ns\u2192\u221e n\u2192\u221e\n\n(81)\n\nwhere \u03c1E(s) (xn1 ) is the minimum compression ratio for xn1 over the class of all finite state\nencoders with at most s states [15, eq. (1)-(4)]. We may now introduce the following corollary.\n7\n\nYet, the Peano-Hilbert is by no means the only optimal scan. We Elaborate on this issue later in this section.\n\n33\n\n\fCorollary 18. Let \u03a81 and \u03a82 be any two sequences of scans such that \u0124\u03a81 (x) = \u0124\u03a82 (x) (in\nparticular, if both \u03a81 and \u03a82 are finite state sequences of scans they result in the same finite\nstate compressibility). Then,\n|L\u03a81 (x) \u2212 L\u03a82 (x)| \u2264 2\u01ebl .\n\n(82)\n\nfor any loss function l : {0, 1} \u00d7 [0, 1] \u2192 R.\nFor a given sequence of scans \u03a8, the set of scanning sequences \u03a8\u2032 satisfying \u0124\u03a8 (x) = \u0124\u03a8\u2032 (x)\nis larger than one might initially think. For example, a close look at the definition of finite\nstate compressibility given in [15] shows that the finite state encoders defined therein allow\nlimited scanning schemes, as an encoder might read a large data set before its output for that\ndata set is given. Thus, a legitimate finite state encoder in the sense of [15] may reorder\nthe data in a block (of bounded length, as the number of states is bounded) before actually\nencoding it. Consequently, for any individual sequence x one can define several permutations\nhaving the same finite state compressibility. In the multidimensional scenario this sums up to\nsaying that for each scanning sequence \u03a8 there exist several different scanning sequences \u03a8\u2032\nfor which H\u03a8 (x) = \u0124\u03a8\u2032 (x).\nProof of Proposition 17. For each n, \u03a8n is a scanner for Vn . Thus, by Proposition 16, we have\nk+1\n(X|X k ) + \u03b2l \u2212\n\u03b1l \u0124\u03a8\nn\n\nklmax\n1\nL\n,\nk,opt ) (xVn ) \u2264 \u01ebl +\n|Vn | (\u03a8n ,F\n|Vn |\n\n(83)\n\nTaking the limsup as n \u2192 \u221e yields\nk+1\n(X|X k ) + \u03b2l \u2212 Lk\u03a8 (x) \u2264 \u01ebl .\n\u03b1l lim sup \u0124\u03a8\nn\nn\u2192\u221e\n\n(84)\n\nFor a stationary source, it is well known (e.g., [40, Theorem 4.2.1]) that limk\u2192\u221e H(Xk |X1k\u22121 )\nexists and in fact\n1\nH(X1k ).\nk\u2192\u221e k\n\nlim H(Xk |X1k\u22121 ) = lim\n\nk\u2192\u221e\n\n(85)\n\nTo this end, we show that the same holds for empirical entropies. We start by showing that\nk+1\n(X|X k ) is a decreasing sequence in k. Since conditioning reduces the entropy,\nlim supn\u2192\u221e \u0124\u03a8\nn\nk+1\nk+1\n(*).\n(X|X k\u22121 ), where both are calculated using P\u0302\u03a8k+1\n(X|X k ) \u2264 \u0124\u03a8\nit is clear that \u0124\u03a8\nn\nn\nn\n\nk+1\nk (X|X k\u22121 ), as\n(X|X k\u22121 ) is replaced by \u0124\u03a8\nHowever, the above may not be true when \u0124\u03a8\nn\nn\n\nthe later is calculated using P\u0302\u03a8k n (*). Nevertheless, using a simple counting argument, it is not\ntoo hard to show that for every k, 0 < j \u2264 k and s \u2208 {0, 1}i , where 0 < i \u2264 j, we have\n(s) \u2212\nP\u0302\u03a8k+1\nn\n\nk+1\u2212j\nk+1\u2212j\n\u2264 P\u0302\u03a8j n (s) \u2264 P\u0302\u03a8k+1\n.\n(s) +\nn\n|Vn | \u2212 k\n|Vn | \u2212 k\n\n34\n\n(86)\n\n\fThus, by the continuity of the entropy function, we have\nk+1\nk+1\n(X|X k\u22121 )\n(X|X k ) \u2264 lim sup \u0124\u03a8\nlim sup \u0124\u03a8\nn\nn\nn\u2192\u221e\n\nn\u2192\u221e\n\nk\n= lim sup \u0124\u03a8\n(X|X k\u22121 ),\nn\n\n(87)\n\nn\u2192\u221e\n\nk (X|X k\u22121 ) is decreasing in k. Since it is a non negative sequence, \u0124 (x)\nhence lim supn\u2192\u221e \u0124\u03a8\n\u03a8\nn\n\nas defined in (80) exists and we have\n\u03b1l \u0124\u03a8 (x) + \u03b2l \u2212 L\u03a8 (x) \u2264 \u01ebl .\n\n(88)\n\nWe now show that indeed \u0124\u03a8 (x) equals \u03c1(\u03a8(x)) for every sequence of finite state scans \u03a8,\nhence when \u03a8 is a sequence of finite state scans the results of [14] can be applied. The method\nis similar to that in [40, Theorem 4.2.1]), with an adequate handling of empirical entropies.\nBy (86),\nk\n1X k\n1 k\nk\nlim sup \u0124\u03a8n (X ) = lim sup\n\u0124\u03a8n (Xi |X1i\u22121 )\nn\u2192\u221e k\nn\u2192\u221e k\n\n= lim sup\nn\u2192\u221e\n\n1\nk\n\ni=1\nk\nX\ni=1\n\ni\n(Xi |X1i\u22121 ).\n\u0124\u03a8\nn\n\n(89)\n\ni (X |X i\u22121 ) converges to \u0124 (x) as i \u2192 \u221e, thus its Cesaro\nBut the sequence lim supn\u2192\u221e \u0124\u03a8\ni\n\u03a8\n1\nn\n\nmean converges to the same limit and we have\n1 k\n(X k )\nlim lim sup \u0124\u03a8\nn\nk\u2192\u221e n\u2192\u221e k\n= \u03c1(\u03a8(x)).\n\n\u0124\u03a8 (x) =\n\n(90)\n\nConsider now the Peano-Hilbert sequence of finite state scans, denoted by P H. Let \u03c1(x)\ndenote the (finite state) compressibility of x as defined in [14, eq. (4)]. For any other sequence\nof finite state scans \u03a8\u0303 we have\n\u0124P H (x) \u2264 \u03c1(x)\n\u2264 \u0124\u03a8\u0303 (x),\n\n(91)\n\nwhere the first inequality is by [14, eq. (9) and (16)] and the second is straightforward from\nthe definition of \u03c1(x). Finally,\nLP H (x)\n\n(a)\n\n\u2264\n\n\u01ebl + \u03b2l + \u03b1l \u0124P H (x)\n\n\u2264\n\n\u01ebl + \u03b2l + \u03b1l \u0124\u03a8\u0303 (x)\n\n\u2264\n\n2\u01ebl + L\u03a8\u0303 (x),\n\n(b)\n\n35\n\n(92)\n\n\fwhere (a) and (b) result from the application of (88) to the sequences P H and \u03a8\u0303 respectively.\n\nThe proof of Corollary 18 is straightforward, using (88) for both \u03a81 and \u03a82 and the triangle\ninequality.\n\n4.1.1\n\nHamming Loss\n\nThe bound in Proposition 17 is valid for any loss function l : {0, 1} \u00d7 [0, 1] \u2192 R. When l is\nthe Hamming loss, the resulting bound is\nLHamming\n(x) \u2264 LHamming\n(x) + 0.16,\nPH\n\u03a8\n\n(93)\n\nfor any other finite state sequence of scans, namely, a uniform bound, regardless of the compressibility of x. However, using known bounds on the predictability of a sequence (under\nHamming loss) in terms of its compressibility can yield a tighter bound.\nIn [41], Feder, Merhav and Gutman proved that for any next-state function g \u2208 Gs , where\n\nGs is the set of all possible next state functions with s states, and for any sequence xn1\n1\n\u03c1(g, xn1 ),\n2\nn\n\u03bc(g, xn1 ) \u2265 h\u22121\nb (\u03c1(g, x1 )) ,\n\u03bc(g, xn1 ) \u2264\n\n(94)\n\nwhere \u03bc(g, *) (\u03c1(g, *)) is the best possible prediction (compression) performance when the next\nstate function is g. Consequently, for any two finite-state scans \u03a81n and \u03a82n for xVn ,\nmin \u03bc(g,\u03a81n (xVn )) \u2212 min \u03bc(g, \u03a82n (xVn ))\n\ng\u2208Gs\n\ng\u2208Gs\n\n\u0001\n1\n\u2264 min \u03c1(g, \u03a81n (xVn )) \u2212 min h\u22121\n\u03c1(g, \u03a82n (xVn )\nb\ng\u2208Gs 2\ng\u2208Gs\n\u0012\n\u0013\n1\n2\n= min \u03c1(g, \u03a81n (xVn )) \u2212 h\u22121\n\u03c1(g,\n\u03a8\n(x\nmin\n))\n.\nn Vn\nb\ng\u2208Gs\n2 g\u2208Gs\n\n(95)\n\nTaking \u03a81n to be the Peano-Hilbert scan, the results of [14] imply that\nmin \u03c1(g, \u03a8P H (xVn )) \u2264 min \u03c1(g, \u03a8n (xVn )) + \u01ebn,s\n\ng\u2208Gs\n\n(96)\n\ng\u2208Gs\n\nfor any finite-state scan \u03a8n , where \u01ebn,s satisfies lims\u2192\u221e lim supn\u2192\u221e \u01ebn,s = 0. Hence,\nmin \u03bc(g, \u03a8P H (xVn )) \u2212 min \u03bc(g, \u03a8(xVn ))\n\ng\u2208Gs\n\ng\u2208Gs\n\n1\n\u2264 min \u03c1(g, \u03a8P H (xVn )) \u2212 h\u22121\nb\n2 g\u2208Gs\n\n\u0012\n\nmin \u03c1(g, \u03a8P H (xVn )) \u2212 \u01ebn,s . (97)\n\ng\u2208Gs\n\nTaking the limits lim supn\u2192\u221e and then s \u2192 \u221e implies the following proposition.\n\n36\n\n\u0013\n\n\fProposition 19. Let x be any individual image. Let P H denote the Peano-Hilbert sequence\nof scans. Then, under the Hamming loss function, for any sequence of finite state scans \u03a8 we\nhave\n1\nLP H (x) \u2264 L\u03a8 (x) + \u03c1(x) \u2212 h\u22121\nb (\u03c1(x)),\n2\n\n(98)\n\nwhere \u03c1(x) is the compressibility of the individual image x.\nIn other words, the specific scandictor composed of the Peano-Hilbert scan followed by the\noptimal predictor, adheres to the same asymptotic bounds (on predictability in terms of the\ncompressibility) as the best finite-state scandictor. Figure 2 plots the function 21 \u03c1 \u2212 h\u22121\nb (\u03c1).\nThe maximum possible loss is 0.16, similar to the bound given in Proposition 17, yet this value\nis achieved only when the image's FS compressibility is around 0.75 bits/symbol. For images\nwhich are highly compressible, for example, when \u03c1 < 0.1 the resulting excess loss is smaller\nthan 0.04.\nUpper bound on the redundancy in using the Peano\u2212Hilbert scan.\n0.18\n\n0.5\u03c1 \u2212 h\u22121(\u03c1): the maximum possible redundancy\n\n0.16\n\n0.14\n\n0.12\n\n0.1\n\n0.08\n\n0.06\n\n0.04\n\n0.02\n\n0\n\n\u22120.02\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n0.5\n0.6\n0.7\n\u03c1: the compressibility of the image\n\n0.8\n\n0.9\n\n1\n\nFigure 2: A plot of 21 \u03c1 \u2212 h\u22121 (\u03c1). The maximum redundancy is not higher than 0.16 in worst case,\nbut will be much lower for more compressible arrays.\n\n5\n\nConclusion\n\nIn this paper, we formally defined finite set scandictability, and showed that there exists a\nuniversal algorithm which successfully competes with any finite set of scandictors when the\nrandom field is stationary. Moreover, the existence of a universal algorithm which achieves the\n\n37\n\n\fscandictability of any spatially stationary random field was established. We then considered\nthe scenario where non-optimal scanners are used, and derived a bound on the excess loss in\nthat case, compared to optimal scandiction.\nIt is clear that the scandiction problem is even more intricate than its prediction analogue.\nFor instance, very basic results in the prediction scenario do not apply to the scandiction case\nin a straightforward way, and, in fact, are still open problems. To name a few, consider the\ncase of universal scandiction of individual images, briefly discussed in Section 3.6. Although\nthe question whether there exists a universal scandictor which competes successfully with any\nfinite set of scandictors on any individual image was answered negatively in Section 3.1, it is\ninteresting to discover interesting sets of scandictors for which universal scandiction is possible.\nThe sequential prediction literature also includes an elegant result [41] on the asymptotic\nequivalence between finite state and Markov predictors. We conjecture that this equivalence\ndoes not hold in the multi-dimensional scenario for any individual image. Finally, the very\nbasic problem of determining the optimal scandictor for a given random field X with a known\nprobability measure Q, is still unsolved in the general case.\nIt is also interesting to consider the problems of scanning and prediction, as well as filtering, in a noisy environment. These problems are intimately related to various problems in\ncommunications and image processing, such as filtering and denoising of images and video. As\nmentioned in Section 1, these problems are the subject of [27].\n\nA\n\nAppendixes\n\nA.1\n\nProof of Proposition 5\n\nFor the sake of simplicity, we suppress the dependence of m(n) in n. Define Wi =\nWe have\nlog\n\nW(K+1)2\nW0\n\n= log\n\n\u03bb\nX\n\n\u2212\u03b7Lj,(K+1)2\n\ne\n\nj=1\nj\n\n\u2212\u03b7Lj,i .\nj=1 e\n\n\u2212 log \u03bb\n\n\u2212\u03b7Lj,(K+1)2\n\n\u2265 log max e\n\nP\u03bb\n\n\u2212 log \u03bb\n\n= \u2212\u03b7 min Lj,(K+1)2 \u2212 log \u03bb\nj\n\n= \u2212\u03b7Lmin \u2212 log \u03bb.\n\n38\n\n(99)\n\n\fMoreover,\nWi+1\nlog\nWi\n\n= log\n\nP\u03bb\n\n= log\n\n\u03bb\nX\n\n\u2212\u03b7(Lj,i +Lj (xi ))\nj=1 e\nP\u03bb\n\u2212\u03b7Lj \u2032 ,i\nj \u2032 =1 e\n\nj=1\n\n\u2264 \u2212\u03b7\n\n\u03bb\nX\nj=1\n\n\u0011\n\u0010\ni\nPi j|{Lj,i }\u03bbj=1 e\u2212\u03b7Lj (x )\n\u0011\n\u0010\n2\nm4 lmax\n\u03b72\nPi j|{Lj,i }\u03bbj=1 Lj (xi ) +\n,\n8\n\n(100)\n\nwhere the last inequality follows from the extension to Hoeffding's inequality given in [33] and\nthe fact that \u2212\u03b7Lj (xi ) is in the range [\u2212\u03b7m2 lmax , 0]. Thus,\nW(K+1)2\nlog\nW0\n\n=\n\n(K+1)2 \u22121\n\nX\n\nlog\n\ni=0\n\n\u2264 \u2212\u03b7\n\nWi+1\nWi\n\n(K+1)2 \u22121 \u03bb\nX X\ni=0\n\n= \u2212\u03b7 L\u0304alg +\n\n\u0011\n\u0010\n2\n\u03b7 2 (K + 1)2\nm4 lmax\nPi j|{Lj,i }\u03bbj=1 Lj (xi ) +\n8\n\nj=1\n4\n2\nm lmax\n\u03b7 2 (K\n\n8\n\n+ 1)2\n\n.\n\n(101)\n\nFinally, from (99) and (101), we have\n2\nlog \u03bb m4 lmax\n\u03b7(K + 1)2\n+\n\u03b7\n8\n2\n2\nlog \u03bb m lmax \u03b7(n + m)2\n+\n.\n\u03b7\n8\n\nL\u0304alg \u2212 Lmin \u2264\n\u2264\n\n(102)\n\nThe bound in (14) easily follows after optimizing the right hand side of (102) with respect to\n\u03b7.\n\nA.2\n\nProof of Proposition 6\n\nLet \u03b4(n) be some sequence satisfying \u03b4(n) \u2192 0 as n \u2192 \u221e. Define the sets\n\u001a\n\u001b\nLalg (xVn ) \u2212 Lmin (xVn )\n2\nAn = \u03c9 :\n> \u03b4(n ) ,\nn2\nwhere (\u03a9, P ) is the probability space. We wish to show that\n\u0012\n\u0013\nP lim sup An = 0,\nn\u2192\u221e\n\n(103)\n\n(104)\n\nthat is, P (An i.o.) = 0. Let (\u03a8, F )k be the scandictor chosen by the algorithm for the k + 1\nblock, xk . Define\no\nn\nZk = L(\u03a8,F )k (xk ) \u2212 E L(\u03a8,F )k (xk )|{Lj,k }\u03bbj=1 ,\n\n39\n\n(105)\n\n\f\u0010\n\u0011\nwhere the expectation is with respect to Pk j|{Lj,k }\u03bbj=1 . Namely, the actual randomization\n\nin Zk is in the choice of (\u03a8, F )k . Thus, {Zk } are clearly independent, and adhere to the\n\nfollowing Chernoff-like bound [33, eq. 33]\n\uf8eb\n\uf8f6\n\u001b\n\u001a\n(K+1)2\nX\n2(K + 1)2 \u01eb2\n2 \uf8f8\n\uf8ed\nP\nZk \u2265 (K + 1) \u01eb \u2264 exp \u2212\n(m2 lmax )2\n\n(106)\n\nk=1\n\nfor any \u01eb > 0. Note that\n\n(K+1)2\n\nX\nk=1\n\nZk = Lalg (xVn ) \u2212 L\u0304alg (xVn ),\n\n(107)\n\nthus, together with eq. (14), we have\nP\n\n\u0012\n\np\nlmax\nLalg (xVn ) \u2212 Lmin (xVn ) \u2265 (K + 1) \u01eb + m(n + m) log \u03bb \u221a\n2\n2\n\n\u0013\n\n\u001a\n\u001b\n2(K + 1)2 \u01eb2\n\u2264 exp \u2212\n. (108)\n(m2 lmax )2\n\nSet\n\u03b4(n) =\n\n\u221a\n\u221a\n(K + 1)2 \u01eb + m(n + m) log \u03bb lmax\n2\nn2\n\n.\n\n(109)\n\nClearly \u03b4(n) \u2192 0 as n \u2192 \u221e for any m(n) = o(n) satisfying m(n) \u2192 \u221e. For the summability\n\u0001\nof the r.h.s. of (108) we further require that m(n) = o n1/3 . The proposition then follows\n\ndirectly by applying the Borel-Cantelli lemma.\n\nA.3\n\nProof of Proposition 11\n\nWe show, by induction on w, that the number of sites in B for which the context of size w (in\nterms of sites in B) under the scan \u03a8 is not contained in the context of size Kw under the\nscan \u03a8\u2032 is at most o(|B|)(K + 1)w\u22121 . This proves the proposition, as the cumulative loss of\n(\u03a8\u2032 , F Kw,opt) is no larger than o(|B|)(K + 1)w\u22121 lmax on these sites, and is at least as small as\nthat of (\u03a8, F w,opt ) on all the rest |B| \u2212 o(|B|)(K + 1)w\u22121 sites.\n\nFor w = 1 this is indeed so, by our assumption on \u03a8 and \u03a8\u2032 - i.e., (55). We say that a site\n\nin B satisfies the context-condition with length w \u2212 1 if its context of size i \u2212 1, 1 < i \u2264 w,\n\nunder the scan \u03a8 is contained in its context of size K(i \u2212 1) under the scan \u03a8\u2032 . Assume\nthat the number of sites in B which do not satisfy the context-condition with length w \u2212 1\nis at most o(|B|)(K + 1)w\u22122 . We wish to lower bound the number of sites in B for which\n\nthe context-condition with length w is satisfied. A sufficient condition is that the contextcondition with length w \u2212 1 is satisfied for both the site itself and its immediate past under\n\u03a8. If the context-condition with length w \u2212 1 is satisfied for a site, its immediate past under\n\n40\n\n\f\u03a8 is contained in its past of length K under \u03a8\u2032 . Thus, if the context-condition of length w \u2212 1\n\nis satisfied for a given site, and for all K preceding sites under \u03a8\u2032 , then it is also satisfied for\nlength w. In other words, each site in B which does not satisfy the context-condition with\nlength w \u2212 1 results in at most K + 1 sites (itself and K more sites) which do not satisfy the\ncontext-condition with length w. Hence, if our inductive assumption is satisfied for w \u2212 1,\nthe number of sites in B which do not satisfy the context-condition with length w is at most\no(|B|)(K + 1)w\u22122 (K + 1), which completes the proof.\n\nA.4\n\nProof of Proposition 12\n\nThe proof is a direct application of Propositions 5 and 11. For each n, define the scandictors\nset\nF\u0303n =\n\nn\n\n(\u03a81n , F Kw,1 ), (\u03a81n , F Kw,2 ), . . . , (\u03a81n , F Kw,|D|\n\n|A|Kw\n\n(\u03a82n , F Kw,1 ), (\u03a82n , F Kw,2 ), . . . , (\u03a82n , F Kw,|D|\n\n), . . .\n\n|A|Kw\n\n), . . .\n\n...\n(\u03a8\u03bbn , F Kw,1 ), (\u03a8\u03bbn , F Kw,2 ), . . . , (\u03a8\u03bbn , F Kw,|D|\n|D||A|\n\nwhere {F Kw,i }i=1\n\nKw\n\n|A|Kw\n\no\n) ,\n\n(110)\n\nis the set of all Markov predictors of order Kw.8 Applying the results\n\nof Proposition 5 to {F\u0303n }, we have, for any image x and all n,\nEL(\u03a8\u0302,F\u0302 )n (xVn ) \u2212\n\nmin\n(\u03a8,F )\u2208F\u0303m(n)\n\nL(\u03a8,F ) (xVn ) \u2264 m(n) (n + m(n))\n\nq\n\nlmax\nlog \u03bb|D||A|Kw \u221a ,\n2\n\n(111)\n\nwhere min(\u03a8,F )\u2208Fm(n) L(\u03a8,F ) (xVn ) is the cumulative loss of the best scandictor in Fm(n) operating block-wise on xVn . However, by Proposition 11, for any 1 \u2264 i \u2264 \u03bb, x and n,\nmin\n1\u2264j\u2264|D||A|\n\nKw\n\nEL(\u03a8i\n\nm(n)\n\n,F Kw,j ) (xVn )\n\n2\n\n\u2264 EL(\u03a8in ,F i ) (xVn ) + o m(n)\n\n\u0001\n\nw\u22121\n\n(K + 1)\n\nlmax\n\n\u0016\n\n\u00172\nn\n.\nm(n)\n(112)\n\nNote that\nmin\n(\u03a8,F )\u2208F\u0303m(n)\n\nL(\u03a8,F ) (xVn ) =\n\nmin\n\nmin\n\n1\u2264i\u2264\u03bb 1\u2264j\u2264|D||A|Kw\n\nEL(\u03a8i\n\n(\n\nm(n)\n\n,F Kw,j ) (xVn )\n\n\u00172 )\nn\n\u2264 min EL(\u03a8in ,F i ) (xVn ) + o m(n) (K + 1)\nlmax\n1\u2264i\u2264\u03bb\nm(n)\n\u00172\n\u0016\n\u0001\nn\n2\nw\u22121\n.\n=\nmin EL(\u03a8,F ) (xVn ) + o m(n) (K + 1)\nlmax\nm(n)\n(\u03a8,F )\u2208Fn\n(113)\n8\n\n2\n\n\u0001\n\nw\u22121\n\n\u0016\n\nAlternatively, one can use one universal predictor which competes successfully with all the Markov predictors of\n\nthat order.\n\n41\n\n\fThus, together with (111), we have\nEL(\u03a8\u0302,F\u0302 )n (xVn ) \u2212\n\nmin\n\n(\u03a8,F )\u2208Fn\n\n\u2264 m(n) (n + m(n))\n\nEL(\u03a8,F ) (xVn )\n\nq\n\nlmax\nlog \u03bb|D||A|Kw \u221a\n2\n\n2\n\n+ o m(n)\n\n\u0001\n\nw\u22121\n\n(K + 1)\n\nlmax\n\n\u0016\n\nn\nm(n)\n\n\u00172\n\n, (114)\n\nwhich completes the proof since |D|, |A|, K and w are finite.\n\nA.5\n\nProof of Proposition 16\n\nSimilar to the proof of Proposition 14, we have,\n1\nL\nk,opt ) (xB )\n|B| (\u03a8B ,F\n\uf8eb\n\uf8f6\n|B|\nk\nX\nX\n1 \uf8ed\nk+1\n\uf8f8\n= \u03b1l \u0124\u03a8\nl(xt , F k,opt(xt\u22121\n(X|X k ) + \u03b2l \u2212\nl(xt , F k,opt (xt\u22121\n1 )) +\nt\u2212k ))\nB\n|B| t=1\n\nk+1\n(X|X k ) + \u03b2l \u2212\n\u03b1l \u0124\u03a8\nB\n\nt=k+1\n\nk+1\n(X|X k ) + \u03b2l \u2212\n= \u03b1l \u0124\u03a8\nB\n\n\u0012\n\u0013\n|B|\nk\nX\nk\n1\n1 X\nk,opt t\u22121\nl(xt , F\n(x1 )) \u2212 1 \u2212\nl(xt , F k,opt (xt\u22121\nt\u2212k ))\n|B| t=1\n|B| |B| \u2212 k\nt=k+1\n\nk+1\n\u2264 \u03b1l \u0124\u03a8\n(X|X k ) + \u03b2l \u2212\nB\n\n|B|\n\nX\n1\nklmax\n.\nl(xt , F k,opt (xt\u22121\n)) +\nt\u2212k\n|B| \u2212 k\n|B|\n\n(115)\n\nt=k+1\n\n(s) ans sum over\nSince the order of the predictor is fixed, we can use the definition of P\u0302\u03a8k+1\nB\ns \u2208 {0, 1}k+1 instead of t. Thus,\nk+1\n(X|X k ) + \u03b2l \u2212\n\u03b1l \u0124\u03a8\nB\n\n1\nL\nk,opt ) (xB )\n|B| (\u03a8B ,F\n\nk+1\n\u2264 \u03b1l \u0124\u03a8\n(X|X k ) + \u03b2l \u2212\nB\n\n=\n\nX\n\n(s\u2032 )\nP\u0302\u03a8k+1\nB\n\ns\u2032 \u2208{0,1}k\n\n+\n=\n\nX\n\nX\n\n(s)l(sk+1 , F k,opt(sk1 )) +\nP\u0302\u03a8k+1\nB\n\ns\u2208{0,1}k+1\n\nx\u2208{0,1}\n\nklmax\n|B|\n\nX\n\n\u0011\n\u0010\n\u2032\nk,opt \u2032\n\u2032\nk+1\n(x|s\n)\n+\n\u03b2\n\u2212\nl(x,\nF\n(s\n))\n(x|s\n)\n\u2212\u03b1\nlog\nP\u0302\nP\u0302\u03a8k+1\nl\nl\n\u03a8B\nB\n\n\u0011\n\u0010\nklmax\nk+1\nk+1\n\u2032\n\u2032\n\u2032\n(*|s\n))\n(*|s\n))\n+\n\u03b2\n\u2212\n\u03c6\n(\nP\u0302\n(s\n)\n\u03b1\nh\n(\nP\u0302\nP\u0302\u03a8k+1\n+\nl\nl\nl\nb\n\u03a8B\n\u03a8B\nB\n|B|\nk\n\ns\u2032 \u2208{0,1}\n\n\u2264\n\nX\n\ns\u2032 \u2208{0,1}k\n\n= \u01ebl +\n\nklmax\n|B|\n\n(s\u2032 ) max |\u03b1l hb (p) + \u03b2l \u2212 \u03c6l (p)| +\nP\u0302\u03a8k+1\nB\np\n\nklmax\n.\n|B|\n\nklmax\n|B|\n(116)\n\n42\n\n\fReferences\n[1] M.J. Weinberger, G. Seroussi, and G. Sapiro, \"LOCO-I: A low complexity, context-based,\nlossless image compression algorithm,\" Proc. IEEE Data Compression Conf., pp. 140\u2013149,\n1996.\n[2] C.-H. Lamarque and F. Robert, \"Image analysis using space-filling curves and 1D wavelet\nbases,\" Pattern Recognition, vol. 29, no. 8, pp. 1309\u20131322, 1996.\n[3] A. Krzyzak, E. Rafajlowicz, and E. Skubalska-Rafajlowicz, \"Clipped median and space\nfilling curves in image filtering,\" Nonlinear Analysis, vol. 47, pp. 303\u2013314, 2001.\n[4] L. Velho and J. M. Gomes, \"Digital halftoning with space filling curves,\" Computer\nGraphics, vol. 25, no. 4, pp. 81\u201390, July 1991.\n[5] E. Skubalska-Rafajlowicz, \"Pattern recognition algorithms based on space-filling curves\nand orthogonal expansions,\" IEEE Trans. Inform. Theory, vol. 47, no. 5, pp. 1915\u20131927,\nJuly 2001.\n[6] T. Asano, D. Ranjan, T. Roos, E. Welzl, and P. Widmayer, \"Space-filling curves and\ntheir use in the design of geometric data structures,\" Theoretical Computer Science, vol.\n181, pp. 3\u201315, 1997.\n[7] K.-L. Chung, Y.-H. Tsai, and F.-C. Hu, \"Space-filling approach for fast window query on\ncompressed images,\" IEEE Trans. img. processing, vol. 9, no. 12, pp. 2109\u20132116, 2000.\n[8] B. Moon, H. V. Jagadish, C. Faloutsos, and J. H. Saltz, \"Analysis of the clustering properties of the Hilbert space-filling curve,\" IEEE Trans. Knowledge and Data Engineering,\nvol. 13, no. 1, pp. 124\u2013141, January/February 2001.\n[9] A. Bogomjakov and C. Gotsman, \"Universal rendering sequences for transparent vertex\ncaching of progressive meshes,\" Computer Graphics Forum, vol. 21, no. 2, pp. 137\u2013148,\n2002.\n[10] R. Niedermeier, K. Reinhardt, and P. Sanders,\n\n\"Towards optimal locality in mesh-\n\nindexings,\" Discrete Applied Mathematics, vol. 117, pp. 211\u2013237, 2002.\n[11] H. Tang, S.-I. Kamata, K. Tsuneyoshi, and M.-A. Kobayashi, \"Lossless image compression\nvia multi-scanning and adaptive linear prediction,\" IEEE Asia-Pacific Conference on\nCircuits and Systems, pp. 81\u201384, December 2004.\n\n43\n\n\f[12] N. D. Memon, K. Sayood, and S. S. Magliveras, \"Lossless image compression with a\ncodebook of block scans,\" IEEE Journal on Selected Areas In Communications, vol. 13,\nno. 1, pp. 24\u201330, January 1995.\n[13] R. Dafner, D. Cohen-Or, and Y. Matias, \"Context-based space filling curves,\" EUROGRAPHICS, vol. 19, no. 3, 2000.\n[14] A. Lempel and J. Ziv, \"Compression of two-dimensional data,\" IEEE Trans. Inform.\nTheory, vol. IT-32, no. 1, pp. 2\u20138, January 1986.\n[15] J. Ziv and A. Lempel, \"Compression of individual sequences via variable-rate coding,\"\nIEEE Trans. Inform. Theory, vol. IT-24, pp. 530\u2013536, September 1978.\n[16] T. Weissman and S. Mannor, \"On universal compression of multi-dimensional data arrays\nusing self-similar curves,\" in Proc. 38th Annu. Allerton Conf. Communication, Control,\nand Computing, October 2000, vol. I, pp. 470\u2013479.\n[17] Z. Ye and T. Berger, Information measures for discrete random fields, Science Press,\nBeijing, 1998.\n[18] J. Ziv, \"On universal quantization,\" IEEE Trans. Inform. Theory, vol. IT-31, pp. 344\u2013\n347, May 1985.\n[19] A. Dembo and I. Kontoyiannis, \"Source coding, large deviations, and approximate pattern\nmatching,\" IEEE Trans. Inform. Theory, vol. 48, no. 6, pp. 1590\u20131615, June 2002.\n[20] I. Kontoyiannis, \"Pattern matching and lossy data compression on random fields,\" IEEE\nTrans. Inform. Theory, vol. 49, pp. 1047\u20131051, April 2003.\n[21] N. Memon, D. L. Neuhoff, and S. Shende, \"An analysis of some common scanning techniques for lossless image coding,\" IEEE Trans. on Image Processing, vol. 9, no. 11, pp.\n1837\u20131848, November 2000.\n[22] N. Merhav and T. Weissman, \"Scanning and prediction in multidimensional data arrays,\"\nIEEE Trans. Inform. Theory, vol. 49, no. 1, pp. 65\u201382, January 2003.\n[23] N. Merhav and M. Feder, \"Universal prediction,\" IEEE Trans. Inform. Theory, vol. 44,\nno. 6, pp. 2124\u20132147, October 1998.\n[24] L. Gyorfi, G. Lugosi, and G. Morvai, \"A simple randomized algorithm for sequential\nprediction of ergodic time series,\" IEEE Trans. Inform. Theory, vol. 45, no. 7, pp. 2642\u2013\n2650, November 1999.\n\n44\n\n\f[25] T. Weissman and N. Merhav, \"Universal prediction of random binary sequences in a noisy\nenvironment,\" Ann. App. Prob., vol. 14, no. 1, pp. 54\u201389, February 2004.\n[26] V. G. Vovk, \"Aggregating strategies,\" Proc. 3rd Annu. Workshop Computational Learning\nTheory, San Mateo, CA, pp. 372\u2013383, 1990.\n[27] A. Cohen, T. Weissman, and N. Merhav, \"Scanning and sequential decision making for\nmulti-dimensional data - part II: the noisy case,\" 2007, in preparation.\n[28] T. Weissman and N. Merhav, \"Universal prediction of individual binary sequences in the\npresence of noise,\" IEEE Trans. Inform. Theory, vol. 47, pp. 2151\u20132173, September 2001.\n[29] T. Weissman, E. Ordentlich, M. Weinberger, A. Somekh-Baruch, and N. Merhav, \"Universal filtering via prediction,\" IEEE Trans. Inform. Theory, vol. 53, no. 4, pp. 1253\u20131264,\nApril 2007.\n[30] T. Linder and G. Lugosi, \"A zero-delay sequential scheme for lossy coding of individual\nsequences,\" IEEE Trans. Inform. Theory, vol. 47, no. 6, pp. 2533\u20132538, September 2001.\n[31] T. Weissman and N. Merhav, \"On limited-delay lossy coding and filtering of individual\nsequences,\" IEEE Trans. Inform. Theory, vol. 48, no. 3, pp. 721\u2013733, March 2002.\n[32] A. Gyorgy, T. Linder, and G. Lugosi, \"Efficient adaptive algorithms and minimax bounds\nfor zero-delay lossy source coding,\" IEEE Trans. Signal Processing, vol. 52, no. 8, pp.\n2337\u20132347, August 2004.\n[33] N. Merhav, E. Ordentlich, G. Seroussi, and M. J. Weinberger, \"On sequential strategies\nfor loss functions with memory,\" IEEE Trans. Inform. Theory, vol. 48, pp. 1947\u20131958,\nJuly 2002.\n[34] D. Haussler, J. Kivinen, and M. K. Warmuth, \"Sequential prediction of individual sequences under general loss functions,\" IEEE Trans. on Information Theory, vol. 44, no.\n5, pp. 1906\u20131925, 1998.\n[35] R. M. Gray and J. C. Kieffer, \"Asymptotically mean stationary measures,\" Ann. Prob.,\nvol. 8, no. 5, pp. 962\u2013973, 1980.\n[36] B. Porat, Digital Processing of Random Signals, Prentice-Hall, New Jersey, 1994.\n[37] A. A. Tempelman, \"Ergodic theorems for general dynamical systems,\" Trans. Moscow\nMath. Soc., pp. 94\u2013132, 1972.\n[38] X. Guyon, Ramdom Fields on a Network, Springer-Verlag, 1995.\n\n45\n\n\f[39] H. Sagan, Space-Filling Curves, Springer-Verlag, New York, 1994.\n[40] T. M. Cover and J. A. Thomas, Elements of Information Theory, Wiley, New York, 1991.\n[41] M. Feder, N. Merhav, and M. Gutman, \"Universal prediction of individual sequences,\"\nIEEE Trans. Inform. Theory, vol. 38, pp. 1258\u20131270, July 1992.\n\n46\n\n\f"}