{"id": "http://arxiv.org/abs/0906.4754v4", "guidislink": true, "updated": "2009-09-23T07:58:17Z", "updated_parsed": [2009, 9, 23, 7, 58, 17, 2, 266, 0], "published": "2009-06-25T17:38:32Z", "published_parsed": [2009, 6, 25, 17, 38, 32, 3, 176, 0], "title": "Bayesian separation of spectral sources under non-negativity and full\n  additivity constraints", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.2890%2C0906.2961%2C0906.0075%2C0906.3593%2C0906.1263%2C0906.3088%2C0906.4465%2C0906.3825%2C0906.2618%2C0906.0010%2C0906.0208%2C0906.3327%2C0906.2259%2C0906.0142%2C0906.5439%2C0906.3434%2C0906.0114%2C0906.4482%2C0906.1932%2C0906.4551%2C0906.3266%2C0906.1928%2C0906.1604%2C0906.3554%2C0906.1918%2C0906.0442%2C0906.4060%2C0906.3305%2C0906.0078%2C0906.2294%2C0906.0186%2C0906.4416%2C0906.0615%2C0906.4214%2C0906.5396%2C0906.0717%2C0906.0610%2C0906.4385%2C0906.2865%2C0906.1337%2C0906.0978%2C0906.4089%2C0906.2278%2C0906.2504%2C0906.2445%2C0906.2001%2C0906.2457%2C0906.4754%2C0906.3442%2C0906.3064%2C0906.3373%2C0906.2937%2C0906.5567%2C0906.1286%2C0906.4457%2C0906.2922%2C0906.4600%2C0906.1431%2C0906.3020%2C0906.4239%2C0906.2996%2C0906.4409%2C0906.3100%2C0906.3805%2C0906.3025%2C0906.5270%2C0906.3425%2C0906.1181%2C0906.1383%2C0906.1289%2C0906.2487%2C0906.2738%2C0906.5186%2C0906.2932%2C0906.0644%2C0906.4445%2C0906.3937%2C0906.2711%2C0906.4807%2C0906.5077%2C0906.3054%2C0906.5139%2C0906.3259%2C0906.1074%2C0906.1435%2C0906.0001%2C0906.4733%2C0906.3599%2C0906.2798%2C0906.1211%2C0906.5247%2C0906.2373%2C0906.3097%2C0906.5572%2C0906.1026%2C0906.0792%2C0906.4274%2C0906.4699%2C0906.4185%2C0906.2077%2C0906.0690&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Bayesian separation of spectral sources under non-negativity and full\n  additivity constraints"}, "summary": "This paper addresses the problem of separating spectral sources which are\nlinearly mixed with unknown proportions. The main difficulty of the problem is\nto ensure the full additivity (sum-to-one) of the mixing coefficients and\nnon-negativity of sources and mixing coefficients. A Bayesian estimation\napproach based on Gamma priors was recently proposed to handle the\nnon-negativity constraints in a linear mixture model. However, incorporating\nthe full additivity constraint requires further developments. This paper\nstudies a new hierarchical Bayesian model appropriate to the non-negativity and\nsum-to-one constraints associated to the regressors and regression coefficients\nof linear mixtures. The estimation of the unknown parameters of this model is\nperformed using samples generated using an appropriate Gibbs sampler. The\nperformance of the proposed algorithm is evaluated through simulation results\nconducted on synthetic mixture models. The proposed approach is also applied to\nthe processing of multicomponent chemical mixtures resulting from Raman\nspectroscopy.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.2890%2C0906.2961%2C0906.0075%2C0906.3593%2C0906.1263%2C0906.3088%2C0906.4465%2C0906.3825%2C0906.2618%2C0906.0010%2C0906.0208%2C0906.3327%2C0906.2259%2C0906.0142%2C0906.5439%2C0906.3434%2C0906.0114%2C0906.4482%2C0906.1932%2C0906.4551%2C0906.3266%2C0906.1928%2C0906.1604%2C0906.3554%2C0906.1918%2C0906.0442%2C0906.4060%2C0906.3305%2C0906.0078%2C0906.2294%2C0906.0186%2C0906.4416%2C0906.0615%2C0906.4214%2C0906.5396%2C0906.0717%2C0906.0610%2C0906.4385%2C0906.2865%2C0906.1337%2C0906.0978%2C0906.4089%2C0906.2278%2C0906.2504%2C0906.2445%2C0906.2001%2C0906.2457%2C0906.4754%2C0906.3442%2C0906.3064%2C0906.3373%2C0906.2937%2C0906.5567%2C0906.1286%2C0906.4457%2C0906.2922%2C0906.4600%2C0906.1431%2C0906.3020%2C0906.4239%2C0906.2996%2C0906.4409%2C0906.3100%2C0906.3805%2C0906.3025%2C0906.5270%2C0906.3425%2C0906.1181%2C0906.1383%2C0906.1289%2C0906.2487%2C0906.2738%2C0906.5186%2C0906.2932%2C0906.0644%2C0906.4445%2C0906.3937%2C0906.2711%2C0906.4807%2C0906.5077%2C0906.3054%2C0906.5139%2C0906.3259%2C0906.1074%2C0906.1435%2C0906.0001%2C0906.4733%2C0906.3599%2C0906.2798%2C0906.1211%2C0906.5247%2C0906.2373%2C0906.3097%2C0906.5572%2C0906.1026%2C0906.0792%2C0906.4274%2C0906.4699%2C0906.4185%2C0906.2077%2C0906.0690&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper addresses the problem of separating spectral sources which are\nlinearly mixed with unknown proportions. The main difficulty of the problem is\nto ensure the full additivity (sum-to-one) of the mixing coefficients and\nnon-negativity of sources and mixing coefficients. A Bayesian estimation\napproach based on Gamma priors was recently proposed to handle the\nnon-negativity constraints in a linear mixture model. However, incorporating\nthe full additivity constraint requires further developments. This paper\nstudies a new hierarchical Bayesian model appropriate to the non-negativity and\nsum-to-one constraints associated to the regressors and regression coefficients\nof linear mixtures. The estimation of the unknown parameters of this model is\nperformed using samples generated using an appropriate Gibbs sampler. The\nperformance of the proposed algorithm is evaluated through simulation results\nconducted on synthetic mixture models. The proposed approach is also applied to\nthe processing of multicomponent chemical mixtures resulting from Raman\nspectroscopy."}, "authors": ["Nicolas Dobigeon", "Said Moussaoui", "Jean-Yves Tourneret", "Cedric Carteret"], "author_detail": {"name": "Cedric Carteret"}, "author": "Cedric Carteret", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.sigpro.2009.05.005", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0906.4754v4", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0906.4754v4", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "v4: minor grammatical changes; Signal Processing, 2009", "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0906.4754v4", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0906.4754v4", "journal_reference": "Signal Processing, vol. 89, no. 12, pp. 2657-2669, Dec. 2009", "doi": "10.1016/j.sigpro.2009.05.005", "fulltext": "1\n\nBayesian separation of spectral sources under\nnon-negativity and full additivity constraints\nNicolas Dobigeon1,2 , Sa\u0131\u0308d Moussaoui3 ,\nJean-Yves Tourneret1 and C\u00e9dric Carteret4\n1\n\nUniversity of Toulouse, IRIT/INP-ENSEEIHT, 2 rue Camichel, 31071 Toulouse, France.\n2\n\nUniversity of Michigan, Department of EECS, Ann Arbor, MI 48109-2122, USA\n\narXiv:0906.4754v4 [stat.ME] 23 Sep 2009\n\n3\n4\n\nIRCCyN/ECN, 1 rue de la No\u00eb, BP 92101, 44321 Nantes cedex 3, France\n\nUniversity of Nancy, LCPME, 405 rue de Vandoeuvre, 54600 Villers-l\u00e8s-Nancy, France\n{Nicolas.Dobigeon,Jean-Yves.Tourneret}@enseeiht.fr,\nSaid.Moussaoui@irccyn.ec-nantes.fr, Cedric.Carteret@lcpme.cnrs-nancy.fr\n\nAbstract\nThis paper addresses the problem of separating spectral sources which are linearly mixed with\nunknown proportions. The main difficulty of the problem is to ensure the full additivity (sum-toone) of the mixing coefficients and non-negativity of sources and mixing coefficients. A Bayesian\nestimation approach based on Gamma priors was recently proposed to handle the non-negativity\nconstraints in a linear mixture model. However, incorporating the full additivity constraint requires\nfurther developments. This paper studies a new hierarchical Bayesian model appropriate to the\nnon-negativity and sum-to-one constraints associated to the sources and the mixing coefficients\nof linear mixtures. The estimation of the unknown parameters of this model is performed using\nsamples obtained with an appropriate Gibbs algorithm. The performance of the proposed algorithm\nis evaluated through simulation results conducted on synthetic mixture data. The proposed approach\nis also applied to the processing of multicomponent chemical mixtures resulting from Raman\nspectroscopy.\n\nIndex Terms\nSpectral source separation, non-negativity constraint, full additivity constraint, Bayesian inference, Markov chain Monte Carlo methods.\n\nOctober 23, 2018\n\nDRAFT\n\n\f2\n\nI. I NTRODUCTION\nBlind source separation (BSS) is a signal processing problem arising in many applications\nwhere one is interested by extracting signals that are observed as mixtures [1]. Pioneering\nworks dealing with this problem have focused on the mutual statistical independence of\nthe sources, which led to the well known independent component analysis (ICA) [2]\u2013[5].\nHowever, when the sources and the mixing coefficients have to satisfy specific constraints\nthe resulting constrained source separation problem becomes more complicated. Therefore\nappropriate separation algorithms have to be developed to handle these constraints. When\nthe sources are actually independent, ICA provides estimates of the sources and mixing\ncoefficients which implicitly satisfy these constraints. However, these algorithms, that try\nto maximize the independence between the estimated sources, have not been designed for\ncorrelated sources.\nNon-negativity is a physical constraint which has retained a growing attention during\nthe last decade. For instance, Plumbley and his co-authors have addressed the case of nonnegative independent sources and proposed the non-negative independent component analysis\nalgorithm [6]. The case of both non-negative sources and non-negative mixing coefficients\nhas been handled by using non-negative matrix factorization algorithms (NMF) [7] and a\nBayesian positive source separation algorithm [8]. By adding a source sparsity constraint, a\nmethod ensuring the sparseness of the sources (referred to as non-negative sparse coding) has\nbeen presented in [9]. A Bayesian approach allowing one to perform the separation of sparse\nsources has also been proposed in [10] using a T-student distribution. Cauchy Hyperbolic\npriors have been introduced in [11] without considering the non-negativity constraint.\nThis paper addresses a source separation problem in the case of linear instantaneous\nmixtures where the source signals are non-negative and the mixing coefficients satisfy nonnegativity and full additivity constraints. These constraints have been observed in many\napplications. These applications include analytical chemistry for the analysis of kinetic reactions monitored by spectroscopy [12] or image processing for the analysis of hyperspectral\nimages [13]. A Bayesian framework appropriate to constrained source separation problem\nis first proposed. Prior distributions encoding non-negativity and full additivity constraints\nare assigned to the source signals and mixing coefficients. However, the standard Bayesian\nestimators resulting from these priors have no simple closed form expression. As a consequence, Markov chain Monte Carlo (MCMC) methods are proposed to generate samples\n\nOctober 23, 2018\n\nDRAFT\n\n\f3\n\naccording to the full posterior distribution of the unknown parameters. Estimators of the\nmixing coefficients and the source signals are then constructed from these generated samples.\nThe paper is organized as follows. Section III defines a hierarchical Bayesian model (HBM)\nfor the addressed constrained source separation problem. In particular, prior distributions are\nintroduced such that they are concentrated on a simplex and they satisfy the positivity and\nfull additivity constraints. Section IV describes a Gibbs sampling strategy that allows one\nto overcome the computational complexity inherent to this HBM. Simulations conducted on\nsynthetic mixture data are presented in Section V. As a consequence, the performance of the\nproposed Bayesian estimation algorithm can be appreciated for constrained source separation\nproblems. The interest of the proposed Bayesian approach is also illustrated by the analysis\nof real experimental data reported in Section VI. Conclusions and perspectives are reported\nin Section VII.\nII. P ROBLEM STATEMENT\nThe linear mixing model studied in this paper assumes that the observed signal is a\nweighted sum of M unknown sources. In the case of spectral mixture data this model can\nbe expressed by:\nyi,j =\n\nM\nX\n\nci,m sm,j + ei,j ,\n\n(1)\n\nm=1\n\nwhere yi,j is the observed spectrum at time/spatial index i (i = 1, . . . , N ) in the j th spectral\nband (j = 1, . . . , L), N is the number of observed spectra, M is the number of mixture\ncomponents and L is the number of spectral bands. The coefficient ci,m is the contribution\nof the mth component in the ith mixture and ei,j is an additive noise modeling measurement\nerrors and model uncertainties. The linear mixing model can be represented by the following\nmatrix formulation:\nY = CS + E,\n\n(2)\n\nwhere the matrices Y = [yi,j ]i,j \u2208 RN \u00d7L , C = [ci,m ]i,m \u2208 RN \u00d7M , S = [sm,j ]m,j \u2208 RM \u00d7L and\nE = [ei,j ]i,j \u2208 RN \u00d7L contain respectively the observed spectra, the mixing coefficients, the\nspectral sources and the additive noise components. The noise sequences ei = [ei,1 , . . . , ei,L ]T\n(i = 1, . . . , N ) are assumed to be independent and identically distributed (i.i.d.) according\n2\nto zero-mean Gaussian distributions with covariance matrices \u03c3e,i\nIL , where IL is the L \u00d7 L\n\nidentity matrix. Note that this last assumption implies that the noise variances are the same\nin all the spectral bands. This reasonable assumption has been considered in many recent\nOctober 23, 2018\n\nDRAFT\n\n\f4\n\nworks including [8] and [11]. It could be relaxed at the price of increasing the computational\ncomplexity of the proposed algorithm [14].\nIn the framework of spectral data analysis, it is obvious from physical considerations\nthat both the mixing coefficients and the source signals satisfy the following non-negativity\nconstraints:\nsm,j > 0 and ci,m > 0,\n\n\u2200(i, m, j).\n\n(3)\n\nMoreover, in many applications, the mixing coefficients have also to satisfy the full additivity\nconstraint1 :\n\nM\nX\n\nci,m = 1 \u2200i.\n\n(4)\n\nm=1\n\nThese applications include spectroscopy for the analysis of kinetic reactions [15] and hyperspectral imagery where the mixing coefficients correspond to abundance fractions [16].\nThe separation problem addressed in this paper consists of jointly estimating the abundances and the spectral sources under the non-negativity and the full additivity constraints.\nThere are several methods allowing one to address the estimation problem under non-negativity\nconstraint. These methods include NMF methods [17] and its variants [1]. From a Bayesian\npoint of view an original model was proposed in [8] where Gamma priors are used to encode\nthe positivity of both the sources and the mixing coefficients. This paper goes a step further\nby including the additivity of the mixing coefficients in the Bayesian model. Note that this\nconstraint allows one to resolve the scale indeterminacy inherent to the linear mixing model\neven if non-negativity constraint is imposed. Indeed, this full additivity constraint enforces\nP\nthe `1 norm of each concentration vector ci to be equal to kci k1 = M\nm=1 |ci,m | = 1.\nIII. H IERARCHICAL BAYESIAN M ODEL\nThe unknown parameter vector for the source separation problem described previously\nis \u0398 = (S, C, \u03c3 2e ) where S and C are the source and concentration matrices and \u03c3 2e =\n(\u03c3e,1 , . . . , \u03c3e,N )T contains the noise variances. Following the Bayesian estimation theory, the\ninference of the unknown parameters from the available data Y is based on the posterior distribution f (\u0398|Y), which is related to the observation likelihood f (Y|\u0398) and the parameter\npriors f (\u0398) via the Bayes' theorem:\nf (\u0398|Y) \u221d f (Y|\u0398) f (\u0398) ,\n1\n\nThis condition is also referred to as sum-to-one constraint in the literature.\n\nOctober 23, 2018\n\nDRAFT\n\n\f5\n\nwhere \u221d means \"proportional to\". The observation likelihood and the parameters priors are\ndetailed in the sequel.\nA. Observation likelihood\nThe statistical assumptions on the noise vector ei and the linear mixing model described\nin (1) allow one to write:\n\u0001\n2\n2\nyi | S, ci , \u03c3e,i\n\u223c N ST ci , \u03c3e,i\nIL ,\n\n(5)\n\nwhere yi = [yi,1 , . . . , yi,L ]T , ci = [ci,1 , . . . , ci,M ]T and N (*, *) denotes the Gaussian distribution. By assuming the mutual independence between the vectors e1 , . . . , eN , the likelihood\nof Y is:\n\u0001\n2\n\nf Y|C, S, \u03c3 e \u221d QN\n\n1\n\nL\ni=1 \u03c3e,i\n\nwhere kxk2 = xT x\n\n\u0001 21\n\nN\nX\nyi \u2212 ST ci\nexp \u2212\n2\n2\u03c3e,i\ni=1\n\n2\n2\n\n!\n,\n\n(6)\n\nstands for the standard `2 norm.\n\nB. Parameter Priors\n1) Concentrations: In order to ensure the non-negativity and additivity constraints, the\nconcentrations are assigned a Dirichlet prior distribution. This distribution is frequently used\nin statistical inference for positive variables summing to one. The Dirichlet probability density\nfunction (pdf) is defined by:\n\u0011\n\u0010P\nM\n\u03b4\n\u0393\nm=1 m\nD(ci |\u03b41 , . . . , \u03b4M ) = QM\nm=1 \u0393(\u03b4m )\n\nM\nY\n\n!\nm \u22121\nc\u03b4i,m\n1{ci,m >0; PM ci,m =1} (ci ),\nm=1\n\n(7)\n\nm=1\n\nwhere \u03b41 , . . . , \u03b4M 0 are the Dirichlet distribution parameters, \u0393 (*) is the Gamma function and\n1A (.) denotes the indicator function defined on the set A:\n\uf8f1\n\uf8f2 1 (x) = 1, if x \u2208 A;\nA\n\uf8f3 1A (x) = 0, otherwise.\n\n(8)\n\nAccording to this prior, the expected value of the mth spectral source abundance is E[ci,m ] =\nM\nP\n\u03b4m . We assume here that the abundances are a priori equiprobable (reflecting the\n\u03b4m /\nm=1\n\nabsence of knowledge regarding these parameters) which corresponds to identical parameters\n{\u03b4m = 1, \u2200m = 1, . . . , M }. An interesting reparametrization can be introduced here to handle\n\nOctober 23, 2018\n\nDRAFT\n\n\f6\n\nthe full additivity constraint. This reparametrization consists of splitting the concentration\nvectors into two parts2 :\n\u0002\n\u0003T\nci = aTi , ci,M ,\nwhere aTi = [ci,1 , . . . , ci,M \u22121 ] and ci,M = 1\u2212\n\nPM \u22121\nm=1\n\n(9)\n\nci,m . It induces a new unknown parameter\n\nvector \u0398 = {A, S, \u03c3 2e } (the same notation is used for this new parameter vector to avoid\ndefining new variables). The proposed prior for ai , i = 1, . . . , N is a uniform distribution on\nthe following simplex:\n(\nS=\n\nai ; ai,m \u2265 0, \u2200m = 1, . . . , M \u2212 1,\n\nM\n\u22121\nX\n\n)\nai,m \u2264 1 .\n\n(10)\n\nm=1\n\nBy assuming a priori mutual independence between the vectors ai , the prior distribution for\nthe matrix A = [a1 , . . . , aN ]T reduces to:\nf (A) \u221d\n\nN\nY\n\n1S (ai ).\n\n(11)\n\ni=1\n\n2) Source signals: To take into account the non-negativity constraint, the two parameter\nGamma distribution seems to be a good candidate thanks to its flexibility, i.e. the pdf has\nmany different shapes depending on the values of its parameters (see [8] for motivations).\nThis distribution encodes positivity and covers a wide range of distribution shapes3 . The\nassumption of independent source samples leads to a prior distribution for each spectral\nsource expressed as:\nf sm \u03b1m , \u03b2m\n\n\u0001\n\n\u03b1m\n\u03b2m\n=\n\u0393 (\u03b1m )\n\n\u0014\n\n\u0015L Y\nL\n\n\u0002 \u03b1m \u22121\n\u0003\nsm,j exp (\u2212\u03b2m sm,j ) 1R+ (sm,j ) .\n\n(12)\n\nj=1\n\nNote that this distribution generalizes the exponential prior presented in [19], [20] and\nhas the advantage of providing a wider variety of distributions (see also paragraph VE for additional details regarding the exponential prior). Finally, by assuming the mutual\nindependence between the spectral sources, we obtain the following prior distribution for S:\n\u0001\n\nf S \u03b1, \u03b2 =\n\nM\nY\n\n\u0001\nf sm \u03b1m , \u03b2m ,\n\n(13)\n\nm=1\n\nwhere \u03b1 = [\u03b11 , . . . , \u03b1M ]T and \u03b2 = [\u03b21 , . . . , \u03b2M ]T are the source hyperparameter vectors.\n2\n\nFrom a practical point of view, it is interesting to note that the component of ai to be discarded will be randomly chosen\n\nat each iteration of the Algorithm introduced in Section IV.\n3\n\nA more general model would consist of using a mixture of Gamma distributions as in [18]. However, the Gamma\n\ndistribution which leads to a simple Bayesian model has been preferred here for simplicity.\n\nOctober 23, 2018\n\nDRAFT\n\n\f7\n\n3) Noise variances: Conjugate priors which are here inverse Gamma (IG) distributions\n2\nare chosen for each noise variance \u03c3e,i\n[21, App. A]:\n\u0013\n\u0012\n\u03c1e \u03c8e\n2\n,\n,\n\u03c3e,i \u03c1e , \u03c8e \u223c IG\n2 2\n\n(14)\n\nwhere IG (a, b) denotes the IG distribution with parameters a and b. Note that choosing conjugate distributions as priors makes the Bayesian analysis easier [22, Chap. 2]. By assuming\n2\nthe independence between the noise variances \u03c3e,i\n, i = 1, . . . , N , the prior distribution of \u03c3 2e\n\nis:\nf\n\n\u03c3 2e\n\n\u0001\n\n\u03c1e , \u03c8e =\n\nN\nY\n\n\u0001\n2\nf \u03c3e,i\n\u03c1e , \u03c8e .\n\n(15)\n\ni=1\n\nThe hyperparameter \u03c1e will be fixed to \u03c1e = 2 whereas \u03c8e is an adjustable hyperparameter\nas in [23].\nC. Hyperparameter priors\nThe hyperparameter vector associated with the prior distributions previously introduced is\n\u03a6 = {\u03b1, \u03b2, \u03c8e }. Obviously, the BSS performances depend on the values of these hyperparameters. In this paper, we propose to estimate them within a fully Bayesian framework by\nassigning them non-informative prior distributions. This naturally introduces a second level\nof hierarchy within the Bayes' paradigm, resulting in a so-called hierarchical Bayesian model\n[24, p. 299].\n1) Source hyperparameters: Conjugate exponential densities with parameters \u03bb\u03b1m have\nbeen chosen as prior distributions for the hyperparameters \u03b1m [21, App. A]:\n\u03b1m \u03bb\u03b1m \u223c E (\u03bb\u03b1m ) .\n\n(16)\n\nConjugate Gamma distributions with parameters (\u03b1\u03b2m , \u03b2\u03b2m ) have been elected as prior distributions for the hyperparameters \u03b2m [21, App. A]:\n\u03b2m \u03b1\u03b2m , \u03b2\u03b2m \u223c G (\u03b1\u03b2m , \u03b2\u03b2m ) .\n\n(17)\n\nThe fixed hyperparameters {\u03b1\u03b2m , \u03b2\u03b2m , \u03bb\u03b1m }m have been chosen to obtain flat priors, i.e. with\nlarge variances: \u03b1\u03b2m = 2, \u03b2\u03b2m = 10\u22122 and \u03bb\u03b1m = 10\u22122 .\n\nOctober 23, 2018\n\nDRAFT\n\n\f8\n\n2) Noise variance hyperparameters: The prior for \u03c8e is a non-informative Jeffreys' prior\nwhich reflects the lack of knowledge regarding this hyperparameter:\nf (\u03c8e ) \u221d\n\n1\n1R+ (\u03c8e ).\n\u03c8e\n\n(18)\n\nAssuming the independence between the hyperparameters, the prior distribution of the hyperparameter vector \u03a6 = {\u03b1, \u03b2, \u03c8e } can be written as:\nf (\u03a6) \u221d\n\nM\nY\n\n[\u03bb\u03b1m exp (\u2212\u03bb\u03b1m \u03b1m ) 1R+ (\u03b1m )]\n\nm=1\nM h\ni 1\nY\n\u03b1 \u22121\n\u00d7\n\u03b2m\u03b2m exp (\u2212\u03b2\u03b2m \u03b2m ) 1R+ (\u03b2m )\n1R+ (\u03c8e ).\n\u03c8e\nm=1\n\n(19)\n\nD. Posterior distribution of \u0398\nThe posterior distribution of the unknown parameter vector \u0398 = {A, S, \u03c3 2e } can be\ncomputed from the following hierarchical structure:\nZ\nf (\u0398|Y) \u221d f (Y|\u0398)f (\u0398|\u03a6)f (\u03a6)d\u03a6,\n\n(20)\n\n\u0001\nwhere f Y \u0398 and f (\u03a6) have been defined in (6) and (19). Moreover, by assuming the\nindependence between A, S and \u03c3 2e , the following result can be obtained:\n\u0001\n\u0001\n\u0001\nf \u0398 \u03a6 = f (A) f S \u03c3 2s f \u03c3 2e \u03c1e , \u03c8e ,\n\n(21)\n\n\u0001\n\u0001\nwhere f (A), f S \u03c3 2s and f \u03c3 2e \u03c1e , \u03c8e have been defined previously. This hierarchical\nstructure, depicted in the directed acyclic graph (DAG) of Fig. 1, allows one to integrate out\nthe hyperparameters \u03c8e and \u03b2 from the joint distribution f (\u0398, \u03a6|Y), yielding:\n\"\n!#\n2\nN\nT\nY\n\u0001\ny\n\u2212\nS\nc\n1\n(a\n)\ni\ni\nS\ni\nf A, S, \u03c3 2e , \u03b1 Y \u221d\nexp \u2212\n2\nL+2\n2\u03c3e,i\n\u03c3e,i\ni=1\n\uf8ee\n\uf8f9\nM\nY\uf8ef\n\u0393 (L\u03b1m + \u03b1\u03b2m + 1)\n\uf8fa\n\u00d7\n\uf8f0 \u0010P\n\u0011L\u03b1j +\u03b1\u03b2m +1 \uf8fb\nL\nm=1\nj=1 sm,j + \u03b2\u03b2m\n\uf8f9\n\uf8ee\n!\u03b1m \u22121\nM\nL\nY\nY\nsm,j\n\uf8f0\n1RL+ (sm )\uf8fb .\n\u00d7\n\u0393\n(\u03b1\n)\nm\nm=1\nj=1\n\n(22)\n\nThe posterior distribution in (22) is clearly too complex to derive the classical Bayesian\nestimators of the unknown parameters, such as the minimum mean square error (MMSE)\nestimator or the maximum a posteriori (MAP) estimator. To overcome the difficulty, it is\nquite common to make use of MCMC methods to generate samples asymptotically distributed\nOctober 23, 2018\n\nDRAFT\n\n\f9\n\nFig. 1.\n\nDAG for the parameter priors and hyperpriors (the fixed parameters appear in dashed boxes).\n\naccording to the exact posterior of interest [24]. The simulated samples are then used to\napproximate integrals by empirical averages for the MMSE estimator and to estimate the\nmaximum of the posterior distribution for the MAP estimator. The next section proposes a\nGibbs sampling strategy for the BSS of the spectral mixtures under the positivity and full\nadditivity constraints.\nIV. G IBBS SAMPLER\nThe Gibbs sampler is an iterative sampling strategy that consists of generating samples\n(denoted e*(t) ) distributed according to the conditional distribution of each parameter. This\n\u0010\n\b 2 (t) (t) \u0011\n(t) e (t)\ne\ne\ne\n,\u03b1\nsection describes a Gibbs sampling strategy generating samples A , S , \u03c3\nasymptotically distributed according to (22). The main steps of the algorithm (denoted as\nAlgorithm 1) are detailed from subsection IV-A to subsection IV-C.\n\nA LGORITHM 1. Gibbs sampling algorithm for blind spectral source separation\n\u2022\n\nInitialization:\n(0)\n1) sample the hyperparameter \u03c8ee from the pdf in (18),\n\b 2 (0)\n2) for i = 1, . . . , N , sample the noise variance \u03c3\nee,i\nfrom the pdf in (14),\n(0)\n\n3) for m = 1, . . . , M , sample the hyperparameter \u03b1\nem from the pdf in (16),\n(0)\n4) for m = 1, . . . , M , sample the hyperparameter \u03b2em from the pdf in (17),\nOctober 23, 2018\n\nDRAFT\n\n\f10\n\n5) for m = 1, . . . , M , sample the source spectrum e\ns (t)\nm from the pdf in (12).\n6) Set t \u2190 1,\n\u2022\n\nIterations: for t = 1, 2, . . . , do\ne(t)\n1) for i = 1, . . . , N , sample the concentration vector a\ni from the pdf in (25),\n(t)\n2) sample the hyperparameter \u03c8ee from the pdf in (26),\n\b 2 (t)\n3) for i = 1, . . . , N , sample the noise variance \u03c3\nee,i\nfrom the pdf in (27),\n(t)\n\n4) for m = 1, . . . , M , sample the hyperparameter \u03b1\nem from the pdf in (28),\n(t)\n5) for m = 1, . . . , M , sample the hyperparameter \u03b2em from the pdf in (29),\n6) for m = 1, . . . , M , sample the source spectrum e\ns (t)\nm from the pdf in (30).\n7) Set t \u2190 t + 1.\n\nA. Generation according to f (A|S, \u03c3 2e , Y)\nStraightforward computations yield for each observation:\n\"\n#\n\u0001\n(ai \u2212 \u03bci )T \u039b\u22121\ni (ai \u2212 \u03bci )\n2\nf ai S, \u03c3e,i , yi \u221d exp \u2212\n1T (ai ),\n2\nwhere:\n\n\uf8f1\n\u0014\n\u0015\n\u0001\n\u0001 \u22121\n1\n\uf8f4\nT\nT T\nT\nT\n\uf8f4\nS\u2013M,* \u2212 sM u\n,\n\uf8f4\n\uf8f2 \u039bi = \u03c3 2 S\u2013M,* \u2212 sM u\ne,i\n\u0015\n\u0014\n\uf8f4\n\u0001\n1\n\uf8f4\nT\nT T\n\uf8f4\n(yi \u2212 sM ) ,\n\uf8f3 \u03bci = \u039bi 2 S\u2013M,* \u2212 sM u\n\u03c3e,i\n\n(23)\n\n(24)\n\nwith u = [1, . . . , 1]T \u2208 RM \u22121 and where S\u2013M,* denotes the matrix S from which the M th\n2\nrow has been removed. As a consequence, ai S, \u03c3e,i\n, yi is distributed according to a truncated\n\nGaussian distribution on the simplex S:\n2\nai S, \u03c3e,i\n, yi \u223c NS (\u03bci , \u039bi ) .\n\n(25)\n\n2\nWhen the number M of spectral sources is relatively small, the generation of ai S, \u03c3e,i\n, yi\n\ncan be achieved using a standard Metropolis Hastings (MH) step. By choosing the Gaussian\ndistribution N (\u03bci , \u039bi ) as proposal distribution for this MH step, the acceptance ratio of the\nMH algorithm reduces to 1 if the candidate is inside the simplex S and 0 otherwise. For\nhigher dimension problems, the acceptance ratio of the MH algorithm can be small, leading\nto poor mixing properties. In such cases, an alternative strategy based on a Gibbs sampler\ncan be used (see [25] and [26]).\n\nOctober 23, 2018\n\nDRAFT\n\n\f11\n\nB. Generation according to f (\u03c3 2e |A, S, Y )\nTo sample according to f (\u03c3 2e |A, S, Y ), it is very convenient to generate samples from\nf (\u03c3 2e , \u03c8e |A, S, Y ) by using the two following steps:\n1) Generation according to f (\u03c8e |\u03c3 2e , A, S, Y ): The conditional distribution is expressed\nas the following IG distribution:\nN\n\nN \u03c1e 1 X 1\n,\n2\n2 2 i=1 \u03c3e,i\n\n\u03c8e \u03c3 2e , \u03c1e \u223c IG\n\n!\n.\n\n(26)\n\n\u0001\n2) Generation according to f (\u03c3 2e |\u03c8e , A, S, Y ): After a careful examination of f \u03c3 2e , A, \u03c8e S, Y ,\nit can be deduced that the conditional distribution of the noise variance in each observation\nspectrum is the following IG distribution:\n2\n|\u03c8e , ai , S, yi \u223c IG\n\u03c3e,i\n\n\u03c1e + L \u03c8e + yi \u2212 ScTi\n,\n2\n2\n\n2\n\n!\n.\n\n(27)\n\nC. Generation according to f (S |A, \u03c3 2e , Y )\nThis generation can be achieved thanks to the three following steps, as in [8].\n1) Generation according to f (\u03b1 |\u03b2, S, A, \u03c3 2e , Y ): From the joint distribution f (A, S, \u03c3 2e , \u03b1, \u03b2|Y),\nwe can express the posterior distribution of \u03b1m (m = 1, . . . , M ) as:\n\u0015\nL \u0014\n\u03b1m\n\u0001 Y\n\u03b2m\n\u03b1m\nf \u03b1m sm , \u03b2m \u221d\nsm,j e\u2212\u03bb\u03b1m \u03b1m 1R+ (\u03b1m ).\n\u0393 (\u03b1m )\nj=1\n\n(28)\n\nThis posterior is not easy to simulate as it does not belong to a known distribution family.\n(t)\n\nTherefore, an MH step is required to generate samples \u03b1\nem distributed according to (28).\nThe reader is invited to consult [8] for more details regarding the choice of the instrumental\ndistribution in order to obtain a high acceptance rate for the MH algorithm.\n2) Generation according to f (\u03b2 |\u03b1, S, A, \u03c3 2e , Y ): Similarly, the posterior distribution\nof the hyperparameter vector \u03b2 can be determined by looking at the joint distribution\nf (A, S, \u03c3 2e , \u03b1, \u03b2|Y). In this case, the posterior distribution of the individual hyperparameter\n\u03b2m (m = 1, . . . , M ) is the following Gamma distribution:\n\u03b2m \u03b1 m , s m \u223c G\n\n1 + L\u03b1m + \u03b1\u03b1m ,\n\nL\nX\n\n!\nsm,j + \u03b2\u03b1m\n\n.\n\n(29)\n\nj=1\n\nOctober 23, 2018\n\nDRAFT\n\n\f12\n\n3) Generation according to f (S |\u03b1, \u03b2, A, \u03c3 2e , Y ): Finally, the posterior distribution of\nthe source observed in the j th spectral band is:\n\"\n\nf sm,j\n\n#\n2\n(s\n\u2212\n\u03bc\n)\nm,j\nm,j\nm \u22121\n1R+ (sm,j ) exp \u2212\n\u03b1m , \u03b2m , A, \u03c3 2e , Y \u221d s\u03b1m,j\n\u2212 \u03b2m sm,j ,\n2\n2\u03b4m\n\u0001\n\nwith\n\n\uf8f1\nhP c2 i\u22121\n\uf8f4\nN\ni,m\n2\n\uf8f2 \u03b4m\n,\n=\n2\ni=1 \u03c3e,i\n(\u2212m)\n\uf8f4 \u03bc = 1 PN ci,m \u000fi,j ,\n\uf8f3\n2\nm,j\n2\ni=1\n\u03b4\n\u03c3\nm\n\n(\u2212m)\n\nwhere \u000fi,j\n\n= yi,j \u2212\n\nP\n\nk6=m ci,k sk,j .\n\n(30)\n\n(31)\n\ne,i\n\nThe generation of samples distributed according to (30) is\n\nachieved by using an MH algorithm whose proposal is a positive truncated normal distribution\n[8]. The generation according to the positive truncated Gaussian distribution can be achieved\nthanks to an accept-reject scheme with multiple proposal distributions (see [25], [27], [28]\nfor details).\nV. E XPERIMENTAL RESULTS WITH SYNTHETIC DATA\nThis section presents some experiments performed on synthetic data to illustrate the performance of the proposed Bayesian spectral unmixing algorithm.\nA. Mixture synthesis\nThe spectral sources have been simulated to get signals similar to absorption spectroscopy\ndata. Each spectrum is obtained as a superposition of Gaussian and Lorentzian functionals\nwith randomly chosen parameters (location, amplitude and width) [8]. Figure 2 (left) shows\nan example of M = 3 source signals of L = 1000 spectral bands. For this application, a\n\"spectral\" band corresponds to a given value of the wavelength \u03bb (expressed in nanometers).\nThe mixing coefficients have been chosen to obtain evolution profiles similar to component\nabundance variation in a kinetic reaction, as depicted in Figure 2 (top, right). The abundance\nfraction profiles have been simulated for N = 10 observation times, which provides N = 10\nobservation spectra. An i.i.d. Gaussian sequence has been added to each observation with\nappropriate standard deviation to have a signal to noise ratio (SNR) equal to 20dB. One\ntypical realization of the observed spectra is shown in Figure 2 (bottom, right).\n\nOctober 23, 2018\n\nDRAFT\n\n\f13\n\nFig. 2.\n\nLeft: example of M = 3 simulated spectral sources where the x-axis corresponds to the wavelength expressed in\n\nnm and the y-axis corresponds to the absorbance of the spectra. Right, top: abundance evolution profiles. Right, bottom:\none typical realization of the observed spectra.\n\nB. Separation with non-negativity and full additivity constraints\nFigure 3 summarizes the result of a Monte Carlo simulation with 100 runs where the\nmixing matrix has been kept unchanged, while new sources and noise sequences have been\ngenerated at each run. Figure 3-a shows a comparison between the true concentrations (cross)\nand their MMSE estimates (circles) obtained for a Markov chain of NMC = 1000 iterations\nincluding Nb-i = 200 burn-in iterations. These estimates have been computed according to\nthe MMSE principle (i = 1, . . . , M ):\nNr\n1 X\nb-i +t)\ne(N\n\u00e2 i =\na\n,\ni\nNr t=1\n\n(32)\n\nwhere Nr = NMC \u2212 Nb-i is the number of iterations used for the estimation. The estimated\nabundances are clearly in good agreement with the actual abundances and the estimates satisfy\n\nOctober 23, 2018\n\nDRAFT\n\n\f14\n\nFig. 3.\n\nTop: Simulated (dotted) and estimated (continuous line) source spectra. Bottom: Simulated values (cross) and\n\nMMSE estimates (circles) of the abundances. Error bars indicate the estimated 95% confidence intervals from the simulated\nMarkov chain.\n\nthe positivity and full additivity constraints. By comparing figures 2 (left) and 3 (top), it can\nbe observed that the source signals have also been correctly estimated.\nIt is interesting to note that the proposed algorithm generates samples distributed according\n\u0001\nto the posterior distribution of the unknown parameters f A, S, \u03c3 2e , \u03b1 Y . These samples\ncan be used to obtain the posterior distributions of the concentrations or the source spectra.\nAs an example, typical posterior distributions for two mixing coefficients are depicted in\nfigure 4. These posteriors are in good agreement with the theoretical posterior distributions\nin (25), i.e. truncated Gaussian distributions.\nC. Monitoring sampler convergence\nAn important issue when using MCMC methods is convergence monitoring. The Gibbs\n\u0010\n\u0011\ne (t) , S\ne (t) , \u03c3\ne 2(t) , \u03b1\ne (t) asymptotsampler detailed in section IV generates random samples A\nOctober 23, 2018\n\nDRAFT\n\n\f15\n\nFig. 4. Left (resp. right): posterior distribution of the concentration of the 2nd (resp. 3rd ) spectral component in the mixture\nobserved at index time i = 2 (resp. i = 9). The actual values appear as black bars.\n\nically distributed according to the posterior distribution in (22). The quantities of interest,\ni.e. the concentration coefficients and the source spectra, are then approximated by empirical\naverages according to (32). However, two essential parameters have to be tuned: the length\nNMC of the constructed Markov chain and the length Nb\u2212i of the burn-in period, i.e. the\nnumber of simulated samples to be discarded before computing the averages. This section\nreports some works conducted to ensure the convergence of the proposed algorithm and the\naccuracy of the estimation for the unknown parameters.\nFirst, the burn-in period Nb-i = 200 has been determined thanks to the popular potential\nscale reduction factor (PSRF). The PSRF was introduced by Gelman and Rubin [29] and has\nbeen widely used in the signal processing literature (see for instance [30]\u2013[32]). It consists\nof running several parallel Markov chains and computing the following criterion:\n\u0013\u0014\n\u0015\n\u0012\n1\nB(\u03ba)\n1\n1+\n,\n\u03c1\u0302 = 1 \u2212\nNr\n(Nr \u2212 1) W (\u03ba)\n\n(33)\n\nwhere W and B are the within and between-sequence variances of the parameter \u03ba, respectively. Different choices for \u03ba can be used for our source separation problem. Here,\n2\nwe consider the parameters \u03c3e,i\n(i = 1, . . . , N ) as recommended in [33]. Table I shows the\n\nPSRF obtained for the N = 10 observation times computed from M = 10 Markov chains.\n\u221a\nAll these values of \u03c1\u0302 confirm the good convergence of the sampler since a recommendation\n\u221a\nfor convergence assessment is \u03c1\u0302 < 1.2 [34, p. 332].\nThe Markov chain convergence can also be monitored by a graphical supervision of the\ngenerated samples of the noise variances. As an illustration, the outputs of 10 Markov chains\n2\nfor one of the parameter \u03c3e,i\nare depicted in figure 5. All the generated samples converge to\n\na similar value after a short burn-in period (200 iterations, in this example).\nOctober 23, 2018\n\nDRAFT\n\n\f16\n\nTABLE I\n2\nP OTENTIAL SCALE REDUCTION FACTORS OF \u03c3e,i\n\nFig. 5.\n\nCOMPUTED FROM\n\n\u221a\n\nM = 10 M ARKOV CHAINS .\n\nObs. index\n\n\u221a\n\u03c1\u0302\n\nObs. index\n\n1\n\n1.0048\n\n2\n\n1.0013\n\n3\n\n1.0027\n\n4\n\n0.9995\n\n5\n\n1.0097\n\n6\n\n1.0078\n\n7\n\n1.0001\n\n8\n\n0.9994\n\n9\n\n1.0080\n\n10\n\n1.0288\n\n\u03c1\u0302\n\n2\nOutputs of M = 10 Markov chains for the parameter \u03c3e,5\n.\n\nOnce the number of burn-in iterations has been fixed, the number of iterations necessary\nto obtain accurate estimates of the unknown parameters via (32) has to be adjusted. This\npaper proposes to evaluate Nr with appropriate graphical evaluations (see [35, p. 28] for\nmotivations). Figure 6 shows the reconstruction error associated to the different spectra\ndefined as:\n\nN\n\ne2r (p)\n\n\u0010\n\u0011T\n1 X\n=\nyi \u2212 \u0109i (p) \u015c(p)\nN L i=1\n\n2\n\n,\n\n(34)\n\nwhere \u0109i (p) and \u015c(p) are the MMSE estimates of the abundance vector ci and the source\nmatrix S computed after Nb-i = 200 burn-in iterations and Nr = p iterations. The number\nof iterations Nr required to compute the empirical averages following the MMSE estimator\nOctober 23, 2018\n\nDRAFT\n\n\f17\n\n(32) can be fixed to ensure the reconstruction error is below a predefined threshold. Figure 6\nshows that a number of iterations Nr = 500 is sufficient to ensure a good estimation of the\nquantities of interest C and S.\n\nFig. 6. Evolution of the reconstruction error with respect to the iteration number (with a burn-in of Nb-i = 200 iterations).\n\nD. Comparison with other BSS algorithms\nThe proposed Bayesian approach has been compared with other standard BSS methods.\nSynthetic mixtures have been processed by the non-negative ICA (NN-ICA) algorithm proposed by Plumbley and Oja [36], the iterative NMF method described in [7] and the Bayesian\nPositive Source Separation (BPSS) algorithm introduced in [8].\nAll these methods do not include the full additivity constraint. To evaluate the relevance\nof this additional constraint, ad hoc re-scaled versions of these methods have also been\nconsidered. Simulations have been conducted by applying the 4 algorithms using 100 Monte\nCarlo runs, each run being associated to a randomly generated source. Table II shows the\nnormalized mean square errors (NMSEs) for the estimated sources and abundance matrices\nas defined in [37]:\n\nNMSE (S) =\n\nNMSE (C) =\n\nM\nX\nksm \u2212 \u015dm k2\n,\n2\nks\nk\nm\nm=1\nN\nX\nkci \u2212 \u0109i k2\ni=1\n\nkci k2\n\n(35)\n\n.\n\nIn addition, the estimation performances have been compared in terms of dissimilarity.\nDenoted diss (*, *), it measures how the estimated source spectrum differs from the reference\nOctober 23, 2018\n\nDRAFT\n\n\f18\n\none [38] and is defined by:\nq\ndiss (sm , \u015dm ) = 1 \u2212 corr (sm , \u015dm )2 ,\n\n(36)\n\nwhere corr(sm , \u015dm ) is the correlation coefficient between sm and its estimate \u015dm . Consequently the average dissimilarity over the M sources is reported in Table II.\nTABLE II\nE STIMATION PERFORMANCE FOR DIFFERENT BSS\n\nALGORITHMS\n\n(100 M ONTE C ARLO RUNS ).\n\nNMSE (S)\n\nNMSE (C)\n\nAv. Diss (S)\n\nTime (min)\n\nProposed approach\n\n0.0071\n\n0.0024\n\n13.9 %\n\n44\n\nBPSS\n\n0.0121\n\n0.0025\n\n13.4 %\n\n45\n\nre-scaled BPSS\n\n0.0126\n\n0.0023\n\n13.4 %\n\n45\n\nNN-ICA\n\n0.0613\n\n0.0345\n\n20.0 %\n\n3\n\nre-scaled NN-ICA\n\n0.0602\n\n0.0384\n\n19.4 %\n\n3\n\nNMF\n\n0.2109\n\n1.9149\n\n22.6 %\n\n1\n\nre-scaled NMF\n\n0.0575\n\n0.0496\n\n24.5 %\n\n1\n\nThese results demonstrate that an ad hoc re-scaling of the results obtained by NMF\ntechniques is not always an efficient means to improve the estimation performance. Indeed,\nthe ad hoc re-scaled version of NMF provides lower MSEs than the corresponding standard\nalgorithms. On the other hand, this constraint does not significantly improve the NN-ICA or\nthe BPSS algorithms. As far as the Bayesian algorithms are concerned, they clearly provide\nbetter estimation performance than the non-Bayesian approaches. However, the proposed fully\nconstrained algorithm clearly outperforms the two BPSS algorithms, especially regarding the\nsource estimation.\nThe computation times required by each of these algorithms are also reported in Table II\nfor a MATLAB implementation on a 2.2GHz Intel Core 2. This shows that the complexities of\nthe proposed method and BPSS algorithms are quite similar and higher than the complexities\nof the NN-ICA and MNF algorithms. This seems to be the price to pay to obtain significantly\nbetter estimation performances.\nOctober 23, 2018\n\nDRAFT\n\n\f19\n\nE. Modified Bayesian models with other source priors\nAs it has been mentioned previously, several distributions can be chosen as priors for the\nsource spectra, provided these distributions have positive supports. The previous HBM studied\nin section III-B.2 is based on Gamma distributions as source priors. However, simpler models\ncan be obtained for instance by choosing exponential priors with different scale parameters\n2\n\u03c3s,m\n:\n\nf\n\n2\nsm \u03c3s,m\n\n\u0001\n\n\u0013\n\u0012\nL\nY\n1\nsm,j\n1R+ (sm,j ),\n\u221d\nexp \u2212 2\n2\n\u03c3\n2\u03c3\ns,m\ns,m\nj=1\n\n(37)\n\n2\n:\nor positive truncated Gaussian distribution with different hidden variances \u03c3s,m\n\u0012\n\u0013\nL\n\u0001 Y\ns2m,j\n1\n2\nf sm \u03c3s,m \u221d\nexp \u2212 2\n1R+ (sm,j ).\n2\n\u03c3\n2\u03c3\ns,m\ns,m\nj=1\n\n(38)\n\n2\nThe resulting Bayesian algorithms are simpler since only one hyperparameter \u03c3s,m\nhas to be\n\nadjusted for each source.\nFor both choices, conjugate IG distributions IG\n\n\u03c1s \u03c8s\n,\n2 2\n\n\u0001\n\nare chosen as prior distributions\n\n2\nfor the hyperparameters \u03c3s,m\n, m = 1, . . . , M . After integrating out the hyperparameter vector\n\n\u03a6 = {\u03c8e , \u03c3 2s }, the posterior distribution in (22) can be expressed as:\n\"\nN\nM\nY\n\u0001 Y\nyi \u2212 ST ci\n1\n2\nT (sm , \u03c1s , \u03c8s ) 1RL+ (sm )\nf A, S, \u03c3 e Y \u221d\nexp \u2212\n2\nL+2\n2\u03c3e,i\n\u03c3e,i\nm=1\ni=1\n\n2\n\n!\n\n#\n1S (ai ) .\n(39)\n\nThe scalar T (sm , \u03c1s , \u03c8s ) depends on the prior distribution used for the source spectra:\n\uf8f1\ns\n\uf8f2 [\u03c8s + ksm k ]\u2212 L+\u03c1\n2\n, for exponential priors,\n1\nT (sm , \u03c1s , \u03c8s ) =\n(40)\nL+\u03c1s\n\u0002\n\u0003\n\uf8f3 \u03c8 + ks k2 \u2212 2 , for truncated Gaussian priors.\ns\n\nm 2\n\nIn the Gibbs sampling strategy presented in section IV, the generation according to f (S |A, \u03c3 2e , Y )\nin subsection IV-C is finally achieved using the following two steps:\n\u2022\n\ngeneration according to f (\u03c3 2s |S, A, \u03c3 2e , Y ):\n\u0010\n\u0011\nb\n2\n\u03c3s,m sm \u223c IG L + \u03c1s , \u03c8s + ksm k`b ,\n\n(41)\n\nwhere b = 1 for the exponential prior and b = 2 otherwise,\n\u2022\n\ngeneration according to f (S |\u03c3 2s , A, \u03c3 2e , Y )\n\u0001\n2\nIL ,\nsm \u03c3 2s , A, \u03c3 2e , Y \u223c N + \u03bbm , \u03b4m\n\n(42)\n\n2\nwhere \u03bbm and \u03b4m\n, similar to (31), are derived following the model in [8].\n\nOctober 23, 2018\n\nDRAFT\n\n\f20\n\nTable III reports the NMSEs (computed from 100 Monte Carlo runs following (35)) for the\nsources and concentration matrices estimated by the different Bayesian algorithms. The results\nare significantly better when employing the Gamma distribution, which clearly indicates that\nthe Gamma prior seems to be the best choice to model the distribution of the sources when\nanalyzing spectroscopy data.\nTABLE III\nNMSE FOR DIFFERENT SOURCE PRIORS (100 M ONTE C ARLO RUNS ).\n\nGamma\n\nTruncated Gaussian\n\nExponential\n\nNMSE (S)\n\n0.0071\n\n0.0269\n\n0.0110\n\nNMSE (C)\n\n0.0024\n\n0.0089\n\n0.0029\n\nVI. S EPARATION OF CHEMICAL MIXTURES MONITORED BY R AMAN SPECTROSCOPY\nCalcium carbonate is a chemical material used commercially for a large variety of applications such as filler for plastics or paper. Depending on operating conditions, calcium\ncarbonate crystallizes as calcite, aragonite or vaterite. Calcite is the most thermodynamically\nstable of the three, followed by aragonite or vaterite. Globally, the formation of calcium\ncarbonate by mixing two solutions containing respectively calcium and carbonate ions takes\nplace in two well distinguished steps. The first step is the precipitation one. This step is\nvery fast and provides a mixture of calcium carbonate polymorphs4 . The second step (a slow\nprocess) represents the phase transformation from the unstable polymorphs to the stable\none (calcite). The physical properties of the crystallized product depend largely on the\npolymorphic composition, so it is necessary to quantify these polymorphs when they are\nmixed. Several techniques based on infrared spectroscopy (IR), X ray-diffraction (XRD) or\nRaman spectroscopy (RS) can be used to determine the composition of CaCO3 polymorph\nmixtures. However, contrary to XRD and IR, RS is a faster method since it does not require a\nsample preparation and is a promising tool for an online polymorphic composition monitoring.\nIn our case, the crystallization process of calcium carbonate is carried out in 5mol/L NaCl\n4\n\nThe ability of a chemical substance to crystallize with several types of structures, depending on a physical parameter,\n\nsuch as temperature, is known as polymorphism. Each particular form is said a polymorph.\n\nOctober 23, 2018\n\nDRAFT\n\n\f21\n\nsolutions, which correspond to a real industrial situation. Under the industrial conditions, the\ncalcite is the desired product.\nThe main purpose of this experiment is to show how the proposed constrained BSS method\ncan be used for processing Raman spectroscopy data to study the relation between polymorphs\nand temperature and to explore favorable conditions for calcite formation in saline solutions.\nA. Mixture preparation and data acquisition\nCalcium chloride and sodium carbonate separately dissolved in sodium chloride solutions\nof the same concentration (5mol/L) were rapidly mixed to precipitate calcium carbonate.\nA 100mL solution containing 0.625M of Na2 CO3 and 5M of NaCl was added to a 2.5L\nsolution containing 0.025M of CaCl2 and 5M of NaCl (the precipitation is carried out under\nstoichiometric conditions). A preliminary investigation detailed in [39] suggested that the\ntemperature and the aging time are the most important factors that can affect the polymorphic\ncomposition. Therefore the experiments were operated in a temperature range between 20C\nand 70C and retaining several aging times of the precipitated mixture. A sample was collected\n2 minutes after the beginning of the experiment to determine the polymorphic composition\nat the end of the precipitation step. Then, samples were collected at regular time intervals to\nfollow the polymorph transformation.\nRaman Spectra were collected on a Jobin-Yvon T64000 spectrometer equipped with an\noptical microscope, a threefold monochromator, and a nitrogen-cooled CCD camera. The\nexcitation was induced by a laser beam of argon Spectra Physic Laser Stability 2017 at a\nwavelength of 514.5nm. The beam was focused using a long-frontal x50 objective (numerical\naperture 0.5) on an area of about 3\u03bcm2 . The laser power on the sample was approximately\n20mW and the acquisition time was 1 minute. The spectral resolution was 3cm\u22121 , with a\nwavenumber precision better than 1cm\u22121 . The Raman spectra were collected at five points,\nwhich were randomly distributed throughout the mixture. The average of all spectra was considered as the Raman spectrum of the corresponding mixture for the considered temperature\nvalue and aging time. Raman spectra were collected 2 minutes after the beginning of the\nexperiment for various temperatures ranging between 20C and 70C in order to determine\nthe influence of temperature on the polymorph precipitation. Moreover for each temperature,\nRaman spectra were collected at regular time intervals for monitoring phase transformation.\nFinally, a total of N = 37 Raman spectra of L = 477 wavelengths have been obtained.\n\nOctober 23, 2018\n\nDRAFT\n\n\f22\n\nB. Data preprocessing\nThe Raman spectra of the polymorph mixture are firstly processed using a background\nremoval approach proposed in [40]. In this method, the baseline is represented by a polynomial whose parameters are estimated by minimizing a truncated quadratic cost function. This\nmethod requires the specification of the polynomial order and the threshold of the quadratic\ncost function truncation. This method was applied for each spectrum separately with a fifth\norder polynomial and a threshold chosen by trial and error. Figure 7 shows the Raman spectra\nat the beginning of the phase transformation step, after background removal.\n\nFig. 7.\n\nMixture spectra at the beginning of the phase transformation.\n\nC. Polymorph mixture separation under non-negativity and full additivity contraints\nThe number of sources to be recovered is fixed to M = 3 according to the prior knowledge\non the mixture composition. The iteration number is fixed to 1000 iterations where the first\n200 samples are discarded since they correspond to the burn-in period of the Gibbs sampler.\nFigure 8 illustrates the estimated spectra using the proposed approach incorporating the nonnegativity and the full additivitty constraints.\nFrom a spectroscopic point of view and according to the positions of the vibrational peaks,\nthe identification of the three components is very easy: the first source corresponds to Calcite,\nthe second spectrum to Aragonite and the third one to Vaterite. A measure of the dissimilarity\nbetween the estimated spectra and the measured pure spectra of the three components gives\n4.56% for Calcite, 0.65% for Aragonite and 4.76% for Vaterite. These results show that the\n\nOctober 23, 2018\n\nDRAFT\n\n\f23\n\nproposed method can be applied successfully without imposing any prior information on the\nshape of the pure spectra.\n\nFig. 8.\n\nEstimated sources.\n\nThe evolution of the polymorph proportions versus temperature is shown in Fig. 9. Pure\nVaterite is observed at 20C and a quite pure Aragonite is obtained at 60C. However, between\n20C and 60C ternary mixtures are observed. The content of Calcite is maximal at 40C. Let us\nnow consider the phase transformation evolution at this temperature value. The concentration\nprofile versus precipitation time at 40C is reported in figure 10. At the beginning of the phase\ntransformation (2 minutes), the ternary mixture is composed of 50% Vaterite, 35% Aragonite\nand 15% Calcite. After 2 hours, the Vaterite is transformed to Aragonite and Calcite. After\n7 hours, Vaterite and Aragonite are almost totally transformed to calcite. So, aging time\npromotes the formation of Calcite which is in agreement with some results reported in the\nliterature [41], [42].\nD. Polymorph Mixture analysis using other BSS algorithms\nThe dataset resulting from this experiment is also used to compare the performances of\nstandard BSS methods taking into account the non-negativity constraint and their re-scaled\nversions ensuring the full additivity constraint. Table IV summarizes the performances of the\nconsidered separation algorithms in terms of normalized mean square errors, dissimilarity\nmeasures and computation times. It can be noticed that the proposed approach provides source\nestimates with better accuracy than the other methods. In addition to the good estimation\nquality, the second advantage of the proposed method is its ability to scale the sources during\nOctober 23, 2018\n\nDRAFT\n\n\f24\n\nFig. 9.\n\nFig. 10.\n\nThree component abundances at the beginning of the phase transformation for different temperature values.\n\nEvolution of the three component abundances for T = 40C.\n\nthe estimation algorithm. Thus it does not require any post-processing of the estimation\nresults. However, as previously highlighted, the price to pay for having such results is the\ncomputational times required by the proposed MCMC-based estimation method.\nVII. C ONCLUSION\nThis paper studied Bayesian algorithms for separating linear mixtures of spectral sources\nunder non-negativity and full additivity constraints. These two constraints are required in\nsome applications such as hyperspectral imaging and spectroscopy to get meaningful solutions. A hierarchical Bayesian model was defined based on priors ensuring the fulfillment\nof the constraints. Estimation of the sources as well as the mixing coefficients was then\nperformed by using samples distributed according to the joint posterior distribution of the\nOctober 23, 2018\n\nDRAFT\n\n\f25\n\nTABLE IV\nE STIMATION PERFORMANCE FOR DIFFERENT BSS ALGORITHMS ON REAL SPECTROSCOPIC DATA .\n\nNMSE(S)\n\nDiss(S)\n\nTime (sec)\n\nProposed approach\n\n0.0072\n\n3.34\n\n146\n\nBPSS\n\n0.0118\n\n4.87\n\n205\n\nre-scaled-BPSS\n\n0.0124\n\n4.87\n\n205\n\nNNICA\n\n0.1007\n\n11.82\n\n29\n\nre-scaled NNICA\n\n0.3996\n\n11.82\n\n29\n\nNMF\n\n0.0093\n\n4.25\n\n26\n\nre-scaled NMF\n\n0.0109\n\n4.25\n\n26\n\nunknown model parameters. A Gibbs sampler strategy was proposed to generate samples\ndistributed according to the posterior of interest. The generated samples were then used to\nestimate the unknown parameters. The performance of the algorithm was first illustrated by\nmeans of simulations conducted on synthetic signals. The application to the separation of\nchemical mixtures resulting from Raman spectroscopy was finally investigated. The proposed\nBayesian algorithm provided very promising results for this application. Particularly, when the\ncomputational times is not a study constraint, the proposed method clearly outperforms other\nstandard NMF techniques, which can give approximative solutions faster. Perspectives include\nthe development of a similar methodology for unmixing hyperspectral images. Some results\nwere already obtained for the unmixing of known sources. However, the joint estimation of\nthe mixing coefficients (abundances) and the sources (endmembers) is a still an open and\nchallenging problem.\nACKNOWLEDGMENTS\nThis work was supported by the French CNRS/GDR-ISIS. The authors would like to thank\nthe anonymous referees and the associate editor for their constructive comments improving\nthe quality of the paper. They also would like to thank Prof. Petar M. Djuri\u0107 from Stony\nBrook University for helping them to fix the English grammar.\nOctober 23, 2018\n\nDRAFT\n\n\f26\n\nR EFERENCES\n[1] A. Cichocki and S.-I. Amari, Adaptive blind signal and image processing \u2013 Learning algorithms and applications.\nChichester, England: John Wiley & Sons, 2002.\n[2] P. Comon, C. Jutten, and J. H\u00e9rault, \"Blind separation of sources, Part II: Problems statement,\" Signal Processing,\nvol. 24, no. 1, pp. 11\u201320, July 1991.\n[3] P. Comon, \"Independent component analysis \u2013 a new concept?\" Signal Processing, vol. 36, no. 3, pp. 287\u2013314, April\n1994.\n[4] J.-F. Cardoso, \"Blind signal separation: statistical principles,\" Proc. IEEE, vol. 9, no. 10, pp. 2009\u20132025, Oct. 1998.\n[5] A. Hyv\u00e4rinen, J. Karhunen, and E. Oja, Independent Component Analysis.\n\nNew York: John Wiley, 2001.\n\n[6] M. D. Plumbley, \"Algorithms for non-negative independent component analysis,\" IEEE Transactions on Neural\nNetworks, vol. 14, no. 3, pp. 534\u2013543, May 2003.\n[7] P. Sajda, S. Du, T. R. Brown, R. Stoyanova, D. C. Shungu, X. Mao, and L. C. Parra, \"Nonnegative matrix factorization\nfor rapid recovery of constituent spectra in magnetic resonance chemical shift imaging of the brain,\" IEEE Trans.\nMedical Imaging, vol. 23, no. 12, pp. 1453\u20131465, 2004.\n[8] S. Moussaoui, D. Brie, A. Mohammad-Djafari, and C. Carteret, \"Separation of non-negative mixture of non-negative\nsources using a Bayesian approach and MCMC sampling,\" IEEE Trans. Signal Processing, vol. 54, no. 11, pp. 4133\u2013\n4145, Nov. 2006.\n[9] P. O. Hoyer, \"Non-negative matrix factorization with sparseness constraints,\" Journal of Machine Learning Research,\nvol. 5, pp. 1457\u20131469, Dec. 2004.\n[10] C. F\u00e9votte and S. J. Godsill, \"A Bayesian approach for blind separation of sparse sources,\" IEEE Trans. Audio, Speech,\nLanguage Processing, vol. 14, no. 6, pp. 2174\u20132188, Nov. 2006.\n[11] H. Snoussi and J. Idier, \"Bayesian blind separation of generalized hyperbolic processes in noisy and underdeterminate\nmixtures,\" IEEE Trans. Signal Processing, vol. 54, no. 9, pp. 3257 \u2013 3269, Sept. 2006.\n[12] E. R. Malinowski, Factor Analysis in Chemistry, 3rd ed.\n\nNew York: John Wiley & Sons, 2002.\n\n[13] C.-I Chang, Hyperspectral Data Exploitation: theory and applications.\n\nHoboken, NJ: John Wiley & Sons, 2007.\n\n[14] N. Dobigeon, J.-Y. Tourneret, and A. O. Hero, \"Bayesian linear unmixing of hyperspectral images corrupted by colored\nGaussian noise with unknown covariance matrix,\" in Proc. IEEE Int. Conf. Acoust., Speech, and Signal (ICASSP),\nLas Vegas, Nevada, USA, March 2008.\n[15] A. de Juan, E. Casassas, and R. Tauler, \"Soft-modelling of analytical data,\" in Wiley encyclopedia of analytical\nchemistry: instrumentation and applications, ser. Soft-modelling of analytical data. Wiley Interscience, 2000, vol. 11,\npp. 9800\u20139837.\n[16] N. Dobigeon, J.-Y. Tourneret, and C.-I Chang, \"Semi-supervised linear spectral unmixing using a hierarchical Bayesian\nmodel for hyperspectral imagery,\" IEEE Trans. Signal Processing, vol. 56, no. 7, pp. 2684\u20132695, July 2008.\n[17] D. D. Lee and H. S. Seung, \"Learning the parts of objects by non-negative matrix factorization,\" Nature, vol. 401,\npp. 788\u2013791, 1999.\n[18] I.-T. Hsiao, A. Rangarajan, and G. Gindi, \"Bayesian image reconstruction for transmission tomography using mixture\nmodel priors and deterministic annealing algorithms,\" Journal of Electronic Imaging, vol. 12, no. 1, pp. 7\u201316, 2003.\n[19] J. Miskin and D. MacKay, \"Ensemble learning for blind source separation,\" in S. Roberts and R. Everson, editors,\nIndependent Component Analysis: Principles and Practice.\n\nCambridge University Press, 2001, pp. 209\u2013233.\n\n[20] N. Dobigeon, S. Moussaoui, and J.-Y. Tourneret, \"Blind unmixing of linear mixtures using a hierarchical Bayesian\nmodel. Application to spectroscopic signal analysis,\" in Proc. IEEE/SP Workshop Stat. Signal Processing, Madison,\nUSA, Aug. 2007, pp. 79\u201383.\n\nOctober 23, 2018\n\nDRAFT\n\n\f27\n\n[21] C. P. Robert, The Bayesian Choice: from Decision-Theoretic Motivations to Computational Implementation, 2nd ed.,\nser. Springer Texts in Statistics.\n\nNew York, NY, USA: Springer-Verlag, 2007.\n\n[22] C. M. Bishop, Pattern Recognition and Machine Learning.\n\nNew York: Springer, 2006.\n\n[23] E. Punskaya, C. Andrieu, A. Doucet, and W. Fitzgerald, \"Bayesian curve fitting using MCMC with applications to\nsignal segmentation,\" IEEE Trans. Signal Processing, vol. 50, no. 3, pp. 747\u2013758, March 2002.\n[24] C. P. Robert and G. Casella, Monte Carlo Statistical Methods.\n\nNew York: Springer-Verlag, 1999.\n\n[25] C. P. Robert, \"Simulation of truncated normal variables,\" Statistics and Computing, vol. 5, no. 2, pp. 121\u2013125, June\n1995.\n[26] N. Dobigeon and J.-Y. Tourneret, \"Efficient sampling according to a multivariate Gaussian distribution\ntruncated\n\non\n\na\n\nsimplex,\"\n\nIRIT/ENSEEIHT/T\u00e9SA,\n\nTech.\n\nRep.,\n\nMarch\n\n2007.\n\n[Online].\n\nAvailable:\n\nhttp://dobigeon.perso.enseeiht.fr/publis.html\n[27] J. Geweke, \"Efficient simulation from the multivariate normal and Student-T distributions subject to linear constraints,\"\nin Computing Science and Statistics, Proc. of the 23th Symposium on the Interface, E. M. Keramidas, Ed.\n\nFairfax,\n\nVA: Interface Foundation of North America, Inc., 1991, pp. 571\u2013578.\n[28] V. Mazet, D. Brie, and J. Idier, \"Simulation of postive normal variables using several proposal distributions,\" in Proc.\nIEEE Workshop on Statistical Signal Processing (SSP), Bordeaux, France, July 2005, pp. 37\u201342.\n[29] A. Gelman and D. B. Rubin, \"Inference from iterative simulation using multiple sequences,\" Statistical Science, vol. 7,\nno. 4, pp. 457\u2013511, 1992.\n[30] P. M. Djuri\u0107 and J.-H. Chun, \"An MCMC sampling approach to estimation of nonstationary hidden Markov models,\"\nIEEE Trans. Signal Processing, vol. 50, no. 5, pp. 1113\u20131123, 2002.\n[31] N. Dobigeon, J.-Y. Tourneret, and J. D. Scargle, \"Joint segmentation of multivariate astronomical time series: Bayesian\nsampling with a hierarchical model,\" IEEE Trans. Signal Processing, vol. 55, no. 2, pp. 414\u2013423, Feb. 2007.\n[32] N. Dobigeon, J.-Y. Tourneret, and M. Davy, \"Joint segmentation of piecewise constant autoregressive processes by\nusing a hierarchical model and a Bayesian sampling approach,\" IEEE Trans. Signal Processing, vol. 55, no. 4, pp.\n1251\u20131263, April 2007.\n[33] S. Godsill and P. Rayner, \"Statistical reconstruction and analysis of autoregressive signals in impulsive noise using\nthe Gibbs sampler,\" IEEE Trans. Speech, Audio Proc., vol. 6, no. 4, pp. 352\u2013372, 1998.\n[34] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, Bayesian Data Analysis.\n\nLondon: Chapman & Hall, 1995.\n\n[35] C. P. Robert and S. Richardson, \"Markov Chain Monte Carlo methods,\" in Discretization and MCMC Convergence\nAssessment, C. P. Robert, Ed.\n\nNew York: Springer Verlag, 1998, pp. 1\u201325.\n\n[36] M. D. Plumbley and E. Oja, \"A nonnegative PCA algorithm for independent component analysis,\" IEEE Trans. Neural\nNetworks, vol. 15, no. 1, pp. 66\u201376, Jan. 2004.\n[37] J. K. Tugnait, \"Identification and deconvolution of multichannel linear non-Gaussian processes using higher order\nstatistics and inverse filter criteria,\" IEEE Trans. Signal Processing, vol. 45, no. 3, pp. 658\u2013672, March 1997.\n[38] S. Moussaoui, C. Carteret, D. Brie, and A. Mohammad-Djafari, \"Bayesian analysis of spectral mixture data using\nMarkov chain Monte Carlo methods,\" Chemometrics and Intelligent Laboratory Systems, vol. 81, no. 2, pp. 137\u2013148,\nApril 2006.\n[39] C. Carteret, A. Dandeu, S. Moussaoui, H. Muhr, B. Humbert, and E. Plasari, \"Polymorphism studied by lattice phonon\nraman spectroscopy and statistical mixture analysis method. Application to calcium carbonate polymorphs during batch\ncrystallization,\" Crystal Growth & Design, vol. 9, no. 2, pp. 807\u2013812, 2009.\n[40] V. Mazet, C. Carteret, D. Brie, J. Idier, and B. Humbert, \"Background removal from spectra by designing and\nminimising a non-quadratic cost function,\" Chemometrics and Intelligent Laboratory Systems, vol. 76, no. 2, pp.\n121\u2013133, April 2005.\n\nOctober 23, 2018\n\nDRAFT\n\n\f28\n\n[41] M. J. Kitamura, \"Controlling factor of polymorphism in crystallization process,\" J. Cryst. Growth, vol. 237, pp.\n2205\u20132214, 2002.\n[42] A. Dandeu, B. Humbert, C. Carteret, H. Muhr, E. Plasari, and J.-M. Bossoutrot, \"Raman spectroscopy - a powerful\ntool for the quantitative determination of the composition of polymorph mixtures: Application to CaCO3 polymorph\nmixtures,\" Chem. Eng. Tech., vol. 29, no. 2, pp. 221\u2013225, 2006.\n\nOctober 23, 2018\n\nDRAFT\n\n\f"}