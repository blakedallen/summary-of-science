{"id": "http://arxiv.org/abs/cs/0406050v1", "guidislink": true, "updated": "2004-06-26T11:52:17Z", "updated_parsed": [2004, 6, 26, 11, 52, 17, 5, 178, 0], "published": "2004-06-26T11:52:17Z", "published_parsed": [2004, 6, 26, 11, 52, 17, 5, 178, 0], "title": "Finite-Length Scaling for Iteratively Decoded LDPC Ensembles", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0309040%2Ccs%2F0309006%2Ccs%2F0309003%2Ccs%2F0309030%2Ccs%2F0309010%2Ccs%2F0309052%2Ccs%2F0309047%2Ccs%2F0309046%2Ccs%2F0309014%2Ccs%2F0309011%2Ccs%2F0309015%2Ccs%2F0309004%2Ccs%2F0309050%2Ccs%2F0309026%2Ccs%2F0309042%2Ccs%2F0309008%2Ccs%2F0309044%2Ccs%2F0309022%2Ccs%2F0309007%2Ccs%2F0309051%2Ccs%2F0309053%2Ccs%2F0309024%2Ccs%2F0309025%2Ccs%2F0309039%2Ccs%2F0309009%2Ccs%2F0309037%2Ccs%2F0309045%2Ccs%2F0309033%2Ccs%2F0309036%2Ccs%2F0309032%2Ccs%2F0309041%2Ccs%2F0309028%2Ccs%2F0309016%2Ccs%2F0309021%2Ccs%2F0309020%2Ccs%2F0309017%2Ccs%2F0309001%2Ccs%2F0309043%2Ccs%2F0309054%2Ccs%2F0309048%2Ccs%2F0309038%2Ccs%2F0309055%2Ccs%2F0309023%2Ccs%2F0309035%2Ccs%2F0406051%2Ccs%2F0406043%2Ccs%2F0406014%2Ccs%2F0406055%2Ccs%2F0406047%2Ccs%2F0406002%2Ccs%2F0406057%2Ccs%2F0406040%2Ccs%2F0406015%2Ccs%2F0406019%2Ccs%2F0406061%2Ccs%2F0406010%2Ccs%2F0406052%2Ccs%2F0406046%2Ccs%2F0406005%2Ccs%2F0406017%2Ccs%2F0406009%2Ccs%2F0406053%2Ccs%2F0406059%2Ccs%2F0406038%2Ccs%2F0406021%2Ccs%2F0406026%2Ccs%2F0406056%2Ccs%2F0406030%2Ccs%2F0406036%2Ccs%2F0406029%2Ccs%2F0406003%2Ccs%2F0406011%2Ccs%2F0406044%2Ccs%2F0406008%2Ccs%2F0406039%2Ccs%2F0406016%2Ccs%2F0406050%2Ccs%2F0406037%2Ccs%2F0406022%2Ccs%2F0406023%2Ccs%2F0406024%2Ccs%2F0406041%2Ccs%2F0406020%2Ccs%2F0406031%2Ccs%2F0406006%2Ccs%2F0406013%2Ccs%2F0406048%2Ccs%2F0406025%2Ccs%2F0406042%2Ccs%2F0406027%2Ccs%2F0406028%2Ccs%2F0406034%2Ccs%2F0406054%2Ccs%2F0406049%2Ccs%2F0406004%2Ccs%2F0406035%2Ccs%2F0406001%2Ccs%2F0406058%2Ccs%2F0406045%2Ccs%2F0406032%2Ccs%2F0406012&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Finite-Length Scaling for Iteratively Decoded LDPC Ensembles"}, "summary": "In this paper we investigate the behavior of iteratively decoded low-density\nparity-check codes over the binary erasure channel in the so-called ``waterfall\nregion.\" We show that the performance curves in this region follow a very basic\nscaling law. We conjecture that essentially the same scaling behavior applies\nin a much more general setting and we provide some empirical evidence to\nsupport this conjecture. The scaling law, together with the error floor\nexpressions developed previously, can be used for fast finite-length\noptimization.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0309040%2Ccs%2F0309006%2Ccs%2F0309003%2Ccs%2F0309030%2Ccs%2F0309010%2Ccs%2F0309052%2Ccs%2F0309047%2Ccs%2F0309046%2Ccs%2F0309014%2Ccs%2F0309011%2Ccs%2F0309015%2Ccs%2F0309004%2Ccs%2F0309050%2Ccs%2F0309026%2Ccs%2F0309042%2Ccs%2F0309008%2Ccs%2F0309044%2Ccs%2F0309022%2Ccs%2F0309007%2Ccs%2F0309051%2Ccs%2F0309053%2Ccs%2F0309024%2Ccs%2F0309025%2Ccs%2F0309039%2Ccs%2F0309009%2Ccs%2F0309037%2Ccs%2F0309045%2Ccs%2F0309033%2Ccs%2F0309036%2Ccs%2F0309032%2Ccs%2F0309041%2Ccs%2F0309028%2Ccs%2F0309016%2Ccs%2F0309021%2Ccs%2F0309020%2Ccs%2F0309017%2Ccs%2F0309001%2Ccs%2F0309043%2Ccs%2F0309054%2Ccs%2F0309048%2Ccs%2F0309038%2Ccs%2F0309055%2Ccs%2F0309023%2Ccs%2F0309035%2Ccs%2F0406051%2Ccs%2F0406043%2Ccs%2F0406014%2Ccs%2F0406055%2Ccs%2F0406047%2Ccs%2F0406002%2Ccs%2F0406057%2Ccs%2F0406040%2Ccs%2F0406015%2Ccs%2F0406019%2Ccs%2F0406061%2Ccs%2F0406010%2Ccs%2F0406052%2Ccs%2F0406046%2Ccs%2F0406005%2Ccs%2F0406017%2Ccs%2F0406009%2Ccs%2F0406053%2Ccs%2F0406059%2Ccs%2F0406038%2Ccs%2F0406021%2Ccs%2F0406026%2Ccs%2F0406056%2Ccs%2F0406030%2Ccs%2F0406036%2Ccs%2F0406029%2Ccs%2F0406003%2Ccs%2F0406011%2Ccs%2F0406044%2Ccs%2F0406008%2Ccs%2F0406039%2Ccs%2F0406016%2Ccs%2F0406050%2Ccs%2F0406037%2Ccs%2F0406022%2Ccs%2F0406023%2Ccs%2F0406024%2Ccs%2F0406041%2Ccs%2F0406020%2Ccs%2F0406031%2Ccs%2F0406006%2Ccs%2F0406013%2Ccs%2F0406048%2Ccs%2F0406025%2Ccs%2F0406042%2Ccs%2F0406027%2Ccs%2F0406028%2Ccs%2F0406034%2Ccs%2F0406054%2Ccs%2F0406049%2Ccs%2F0406004%2Ccs%2F0406035%2Ccs%2F0406001%2Ccs%2F0406058%2Ccs%2F0406045%2Ccs%2F0406032%2Ccs%2F0406012&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper we investigate the behavior of iteratively decoded low-density\nparity-check codes over the binary erasure channel in the so-called ``waterfall\nregion.\" We show that the performance curves in this region follow a very basic\nscaling law. We conjecture that essentially the same scaling behavior applies\nin a much more general setting and we provide some empirical evidence to\nsupport this conjecture. The scaling law, together with the error floor\nexpressions developed previously, can be used for fast finite-length\noptimization."}, "authors": ["Abdelaziz Amraoui", "Andrea Montanari", "Tom Richardson", "Ruediger Urbanke"], "author_detail": {"name": "Ruediger Urbanke"}, "author": "Ruediger Urbanke", "arxiv_comment": "45 pages, 14 figures", "links": [{"href": "http://arxiv.org/abs/cs/0406050v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0406050v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.dis-nn", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DM", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0406050v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0406050v1", "journal_reference": null, "doi": null, "fulltext": "FINITE-LENGTH SCALING FOR ITERATIVELY DECODED LDPC\nENSEMBLES\n\narXiv:cs/0406050v1 [cs.IT] 26 Jun 2004\n\nABDELAZIZ AMRAOUI\u2217, ANDREA MONTANARI\u2020, TOM RICHARDSON\u2021, AND R\u00dcDIGER\nURBANKE\u00a7\nAbstract. In this paper we investigate the behavior of iteratively decoded low-density paritycheck codes over the binary erasure channel in the so-called \"waterfall region.\" We show that the\nperformance curves in this region follow a very basic scaling law. We conjecture that essentially the\nsame scaling behavior applies in a much more general setting and we provide some empirical evidence to support this conjecture. The scaling law, together with the error floor expressions developed\npreviously, can be used for fast finite-length optimization.\nKey words. low-density parity-check codes, iterative decoding, density evolution, binary erasure\nchannel, finite-length analysis, error probability curve.\n\n1. Introduction. It is probably fair to say that the asymptotic behavior (as\nthe blocklength tends to infinity) of iterative coding systems is reasonably well\nunderstood to date. Much less is known about the finite-length behavior though.\nAs usual, the situation is clearest for the binary erasure channel (BEC(\u01eb)).\nIn this case, the finite-length analysis of the average performance of an ensemble\nboils down to a combinatorial problem. In [6] recursions where given to solve\nthis combinatorial problem for some simple regular ensembles. These recursions\nwere generalized in [21, 25] to deal with irregular ensembles, expurgation and to\ncompute block as well as bit erasure probabilities. Therefore, in principle, by\nsolving the corresponding recursions it is possible to determine the average finitelength performance for any desired ensemble. In practice though this approach\nruns into computational limitations. Roughly, the complexity of the recursions\ngrows by a factor n (the blocklength) for each degree of freedom of the ensemble.\nFor reasonable lengths therefore only very simple ensembles can currently be\nanalyzed in this way.\nGiven the computational complexity of an exact finite-length analysis, it is\nof great interest to find good approximations. Let us consider ensembles whose\nthreshold is not determined by the stability condition, see [15]. In this case, the\nfinite-length performance curve can be divided into two regions, [20]. The waterfall region and the error floor region. In the waterfall region the performance is\ndetermined by 'large' (linear sized) failures and it improves quickly for decreasing\nerasure probabilities. In the error floor region on the other hand the performance\nis determined by 'small' (sublinear sized) weaknesses in the graph. Fortunately,\nthis second region is relatively easy to handle as was demonstrated in [20].\n\u2217 EPFL\n\n(Lausanne), CH-1015, email:abdelaziz.amraoui@epfl.ch\n(UMR 8549, Unit\u00e9 Mixte de Recherche du CNRS et de l' ENS), 24, rue Lhomond,\n75231 Paris CEDEX 05, France, email:montanar@lpt.ens.fr\n\u2021\nFlarion Technologies, Bedminster, NJ, USA-07921, email:richardson@flarion.com\n\u00a7 EPFL (Lausanne), CH-1015, email:rudiger.urbanke@epfl.ch\n\u2020 LPTENS\n\n1\n\n\f2\n\nFinite-Length Scaling\n\nIn this paper we address the issue of modeling the behavior of large error\nevents. Our approach is motivated by a general conjecture stemming from statistical physics [8, 18]: If a system, parametrized by lets say \u01eb, goes through a\nphase transition at a critical parameter, call it \u01eb\u2217 (in our case the threshold), then\nit has repeatedly been observed that around this critical parameter there is a very\nspecific scaling law. To be more concrete: We are interested in the probability of\nblock error as a function of the block length n and the channel parameter \u01eb, call\nit PB (n, \u01eb). We know that as n tends to infinity there is a phase transition at \u01eb\u2217 ,\nthe iterative decoding threshold. Asymptotically, PB (n, \u01eb) tends to zero for \u01eb < \u01eb\u2217\nand to one for \u01eb > \u01eb\u2217 . The scaling law refines this basic observation: One expects\nthat there exists a non-negative constant \u03bd and some non-negative function f (z)\nso that\nlim\n\ns.t.\n\nn\u2192\u221e\nn1/\u03bd (\u01eb\u2217 \u2212\u01eb)=z\n\nPB (n, \u01eb) = f (z).\n\n(1.1)\n\n1\n\nIn other words, if one plots PB (n, \u01eb) as a function of z = n \u03bd (\u01eb\u2217 \u2212 \u01eb) then, for\nincreasing n these finite-length curves are expected to converge to some function\nf (z). The function f (z) decreases smoothly from 1 to 0 as its argument changes\nfrom \u2212\u221e to +\u221e. This means that all finite-length curves are, to first order, scaled\nversions of some mother curve f (z). It might be helpful to think of the threshold\n\u01eb\u2217 as the zero order term in a Taylor series. Then the above scaling, if correct,\nrepresents the first order term. In fact, one can even refine the analysis to include\nhigher order terms and write\nPB (n, \u01eb) = f (z) + n\u2212\u03c9 g(z) + o(n\u2212\u03c9 ),\nwhere \u03c9 is some positive real number and g(z) is the second order correction term.\nSuch scaling laws are expected to apply in a wide array of situations in communications. The following is probably the simplest case in which such a scaling\nlaw can be proven rigorously. Let H (n, r) denote Shannon's random parity-check\nensemble of codes of length n and rate r. Consider transmission over the BEC(\u01eb)\nusing a random element of H (n, r) with maximum likelihood (ML) decoding. Let\nH denote a random parity-check matrix, let E denote the set of erased positions\nand let HE denote the submatrix of H consisting of the columns of H indexed\nby E . The ML block decoder will succeed if and only if HE has rank E := |E |.\nBy definition, HE is itself a random binary matrix of dimension E \u00d7 nr\u0304, where\nr\u0304 := 1 \u2212 r. Some thought shows that\nP {rank (HE ) = E} =\n\n(\n\n0,\n\u220fE\u22121\ni=0\n\n1 \u2212 2i\u2212nr\u0304\n\n\u0001\n\nE > nr\u0304,\n, 0 \u2264 E \u2264 nr\u0304.\n\n\f3\n\nFinite-Length Scaling\n\nA quick calculation reveals that\nEH (n,r) [PB (H, \u01eb)]\n!\n\u0012 \u0013\nn\nnr\u0304 \u0012 \u0013\nE\u22121\n\u0001\nn E n\u2212E\nn E n\u2212E\ni\u2212nr\u0304\n\u01eb \u01ed\n+ \u2211\n1\u2212 \u220f 1\u22122\n\u01eb \u01ed\n= \u2211\nE\nE\nE=nr\u0304+1\ni=0\nE=0\n\u0012\u221a \u2217\n\u0013\nn(\u01eb \u2212 \u01eb)\n\u221a\n=Q\n(1 + O(1/n)),\n\u01eb\u2217 \u01eb \u0304\u2217\nwhere in the last line we used the fact that r\u0304 = \u01eb\u2217 and we defined the Q-function\nas usual by\n1\nQ(z) := \u221a\n2\u03c0\n\nZ \u221e\nz\n\n2 /2\n\ne\u2212x\n\ndx .\n\nIn words, since the conditional probability of block erasure falls off steeply away\nfrom the threshold, the scaling law is dominated by the probability that the channel behaves atypically and that the number of erasures exceeds n\u01eb\u2217 = nr\u0304.\nIn this paper we prove a scaling law for iteratively decoded standard ensembles LDPC(n, \u03bb, \u03c1) and Poisson ensembles LDPC(n, \u03bb, r) when transmission\ntakes place over the BEC(\u01eb). In the sequel we give a leisurely overview regarding\nthe main results. The precise statements can be found in Section 3. Some of the\nbackground material is summarized in Section 2.\nAssume first that lmin \u2265 3, i.e., that the minimum left degree is at least three.\nLet G be a random element of the ensemble. Then, as stated more precisely in\nSection 3,\n\u0012\u221a \u2217\n\u0013\nn(\u01eb \u2212 \u01eb)\nE[PB (G, \u01eb)] = Q\n+ o(1),\n(1.2)\n\u03b1\nwhere \u03b1 is a quantity which depends on the ensemble and which is computable\nby a procedure similar to density evolution. This scaling law has a form almost\nidentical to (1.2) with \u03b12 representing a variance. Therefore we dub the procedure\nwhich leads to the computation of \u03b1, covariance evolution. We conjecture that in\nfact the following refined scaling law is valid,\n\u0013\n\u0012\u221a \u2217\nn(\u01eb\u2217 \u2212\u01eb)2\n1\n1\nn(\u01eb \u2212 \u01eb)\n\u2212\n+ \u03b2n\u2212 6 \u221a\ne 2\u03b12 + O(n\u22121/3)\nE[PB (G, \u01eb)] = Q\n\u03b1\n2\u03c0\u03b12\n!\n\u221a \u2217\n\u2212 32\nn(\u01eb \u2212 \u03b2n \u2212 \u01eb)\n+ O(n\u22121/3),\n(1.3)\n=Q\n\u03b1\n2\n\nwhere the term \u03b2n\u2212 3 represents a shift of the threshold for finite lengths. Again,\nthis constant \u03b2 depends on the ensemble and we will show how it can be computed.\nFigure 1 shows this scaling applied to the LDPC(n, x2 , x5 ) ensemble which\nwill serve as our running example. Note that the above scaling law models the behavior of large error events. A better comparison with equation (1.3) is therefore\n\n\f4\n\nFinite-Length Scaling\n\nobtained by considering expurgated ensembles, see [20]. For lmin \u2265 3 the scaling\n(1.3) holds true asymptotically regardless of the expurgation scheme. This follows\nsince, as shown in [25], the contribution to the block error probability stemming\n\u0001\nfrom sublinear-sized weaknesses in the graph decreases like1 \u0398 n1\u2212\u2308lmin /2\u2309 .\nThis is the probability of having a stopping set formed by a single variable node\nand \u230almin /2\u230b check nodes (such a constellation is allowed unless double edges\nare forbidden).\nPB\n10-1\n10-2\n10-3\n10-4\n10-5\n10-6\n10-7\n10-8\n0.35\n\n0.36\n\n0.37\n\n0.38\n\n0.39\n\n0.4\n\n0.41\n\n0.42\n\n0.43\n\n0.44\n\n\u01eb\n\nFig. 1: Scaling of ELDPC(n,x2 ,x5 ) [PB (G, \u01eb)] for transmission over BEC(\u01eb) and belief propagation decoding. The threshold for this combination is \u01eb\u2217 \u2248 0.42944, see Table 4.2.\nThe blocklengths/expurgation parameters are n/s = 1024/24, 2048/43, 4096/82 and\n8192/147, respectively. (More precisely, we assume that the ensembles have been expurgated so that graphs in this ensemble do not contain stopping sets of size s or smaller.)\nThe solid curves represent the exact ensemble averages. The dashed curves are computedpaccording to the refined scaling law stated in Conjecture 3.1 with scaling parameters\n\u03b1 = 0.2498692 + \u01eb\u2217 (1 \u2212 \u01eb\u2217 ) and \u03b2 = 0.616045, see Table 4.2.\n\nThe situation is somewhat more complicated once \u03bb\u2032 (0) > 0. In this case the\nblock erasure probability consists of two parts: the part which stems from linearsized error events and which scales like (1.3) and a contribution which stems from\nsub-linear sized weaknesses in the graph. The contribution from the latter part\ndepends crucially on the expurgation scheme employed and does not necessarily\nvanish as n \u2192 \u221e.\nIn the above discussion we focused on the block erasure probability. The\nequivalent scaling law for the bit erasure probability is a straightforward adapta1\nIn the sequel we follow the standard convention to write O(*) to denote an upper bound but we\nwrite \u0398(*) to denote the exact behavior (up to constants).\n\n\f5\n\nFinite-Length Scaling\n\ntion: If the decoder fails at the critical2 point then, asymptotically, it incurs a fixed\nbit erasure probability, call it \u03bd \u2217 (the fractional size of the residual graph). Therefore, if we multiply the above expressions by \u03bd \u2217 we get the corresponding scaling\nlaw for the bit erasure probability.3 Figure 2 shows the resulting approximation\nof ELDPC(n,x2 ,x5 ) [Pb (G, \u01eb)].\nPb\n10-1\n10-2\n10-3\n10-4\n10-5\n10-6\n10-7\n10-8\n0.35\n\n0.36\n\n0.37\n\n0.38\n\n0.39\n\n0.4\n\n0.41\n\n0.42\n\n0.43\n\n0.44\n\n\u01eb\n\nFig. 2: Scaling of ELDPC(n,x2 ,x5 ) [Pb (G, \u01eb)] for transmission over BEC(\u01eb) and belief propagation decoding. The threshold for this combination is \u01eb\u2217 \u2248 0.42944, see Table 4.2. The\nblocklengths/expurgation parameters are n/s = 1024/24, 2048/43 and 4096/82, respectively. The solid curves represent the exact ensemble averages. The dashed curves are\ncomputed according\nto the refined scaling law stated in Conjecture 3.1 with scaling pap\nrameters \u03b1 = 0.2498692 + \u01eb\u2217 (1 \u2212 \u01eb\u2217 ) and \u03b2 = 0.616045, see Table 4.2.\n\nThe basic form of the scaling law applies to regular as well as irregular ensembles.4 The computation of the scaling parameters though becomes significantly more involved in the irregular case and therefore we limit ourselves in this\npaper to providing the detailed calculations only for regular ensembles. Fig. 3\ndemonstrates the scaling law for the block erasure probability applied to the irregular ensemble LDPC(n, \u03bb = 16 x + 56 x3 , \u03c1 = x5 ). In this case the scaling parameters\nwere simply fitted to the data.\nThe performance of ensembles whose threshold is determined by the stability condition scales in a fundamentally different way. The simplest such representatives are cycle codes. We will discuss cycle codes in some detail since\n2 See\n\nSection 2 for a discussion of this notion.\napproximation can be improved away from the threshold by multiplying the above expression with the typical size of the failure for that particular \u01eb.\n4\nThis is true as long as the threshold is not determined by the stability condition and is determined\nby a single critical point, see Sections 2 and 3.\n3 The\n\n\f6\n\nFinite-Length Scaling\n\nPB\n10-1\n\n10-2\n\n10-3\n\n10-4\n\n10-5\n\n10-6\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\n\u01eb\n\nFig. 3: Scaling of ELDPC(n,\u03bb= 1 x+ 5 x3 ,\u03c1=x5 ) [PB (G, \u01eb)] for transmission over BEC(\u01eb) and\n6\n6\nbelief propagation decoding. The threshold for this combination is \u01eb\u2217 \u2248 0.48281. The\nblocklengths/expurgation parameters are n/s = 350/14, 700/23 and 1225/35. The solid\ncurves represent the simulated ensemble averages. The dashed curves are computed\naccording\nto the refined scaling law stated in Conjecture 3.1 with scaling parameters\np\n\u03b1 = 0.2762 + \u01eb\u2217 (1 \u2212 \u01eb\u2217 ) and \u03b2 = 0.642274. These parameters were fitted to the data.\n\nwe conjecture that the same scaling applies to all ensembles for which the stability condition determines the threshold. Fig. 4 shows block erasure curves for\nthe LDPC(n, x, r = 12 ) cycle Poisson ensemble with expurgation parameter s = 1\nfor n = 2i , i = 8, 10, 12, 14. Also shown is the limiting block erasure probability\ncurves and our approximation for the block error probability around the threshold. Clearly, these curves differ in their nature significantly from the curves discussed before. As investigated in more detail in Section 3, the block erasure probability does not show a threshold effect: instead it converges to a smooth limiting\ncurve. Around the threshold we have the following scaling law,\nn\no\nELDPC(n,x,r) [PB (G, \u01eb)] = 1 \u2212 Aan\u22121/6 f (b n1/3 (\u01eb \u2212 \u01eb\u2217)) 1 + O(n\u22121/3) ,\n(1.4)\n\nwhere a = r\u0304\u22121/6 , b = r\u0304\u22122/3 and A is a constant which depends on the expurgation\nscheme used. The form of the mother curve f (x) is given in Lemma 3.2.\n1.1. Scaling for General Channels. In many ways this paper only represents the very first step in what seems to be a promising research direction. The\nmost important extension is undoubtedly the one to general binary-input outputsymmetric channels. Although there is currently little hope of attacking this problem rigorously, empirically such a scaling seems to be true for general channels as\n\n\f7\n\nFinite-Length Scaling\n\nPB\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\n0.0\n\n0.1\n\n0.2\n\n0.3\n\n\u01eb\n\nFig. 4: Scaling of ELDPC(n,\u03bb=x,r= 1 ) [PB (G, \u01eb)] for transmission over the BEC(\u01eb) and belief\n2\n\npropagation decoding. The (bit) threshold for this combination is \u01eb\u2217 = 14 . The solid curves\nare the exact ensemble averages for blocklengths equal to n = 256, 1024, 4096 and 16384.\nThe bold curve is the limiting (in n) block erasure curve. The dashed curves are the finitelength approximations computed according to equation (1.4).\n\nwell. In principle any (function of the) channel parameter can be used for stating\nthe scaling law, however we make this choice slightly less arbitrary by the following convention. Consider a family of binary-input output-symmetric memoryless\nchannels parametrized by lets say \u03c3. Let C(\u03c3) denote the capacity for the parameter \u03c3. The role of \u01eb\u2217 \u2212 \u01eb in the case of the BEC(\u01eb) is then played by C(\u03c3) \u2212C(\u03c3 \u2217 ),\ni.e., we use the scaling law\n!\n\u221a\n2\nn(C(\u03c3) \u2212 C(\u03c3 \u2217 ) \u2212 \u03b2n\u2212 3 )\nPB = Q\n.\n(1.5)\n\u03b1\nNote that for the BEC(\u01eb), C(\u01eb) = 1 \u2212 \u01eb, so that this choice is consistent with our\nprevious convention. The parameters \u03b1 and \u03b2 reported in the captions of Figs. 5\nto 7 are defined according to the above formula.\nFig. 5 shows performance curves for the LDPC(n, \u03bb = x2 , \u03c1 = x5 ) ensemble\ntransmitted over the binary-input additive white Gaussian noise (BAWGN) channel and a quantized version of belief propagation. Fig. 6 shows the corresponding\ncurves for the same ensemble when transmission takes place over the binary symmetric channel (BSC) and belief propagation decoding is used. Finally, Fig. 7\nshows the performance curve for the Gallager algorithm A. Although these cases\nare quite distinct one can see that the empirically fitted scaling laws are in excellent agreement with the exact curves.\n\n\f8\n\nFinite-Length Scaling\n\nPB\n10-1\n\n10-2\n\n10-3\n\n10-4\n\n10-5\n1.0\n\n1.2\n\n1.4\n\n1.6\n\n1.8\n\n2.0\n\n2.2\n\n2.4\n\n2.6\n\n(Eb /N0 )dB\n\nFig. 5: Scaling of ELDPC(n,x2 ,x5 ) [PB (G, \u03c3)] for transmission over BAWGNC(\u03c3) and a quantized version of belief propagation decoding implemented in hardware. The threshold for\nthis combination is (Eb /N0 )\u2217dB \u2248 1.19658. The blocklengths n are n = 1000, 2000, 4000,\n8000, 16000 and 32000, respectively. The solid curves represent the simulated ensemble averages. The dashed curves are computed according to the refined scaling law (1.3)\nwith scaling parameters \u03b1 = 0.8694 and \u03b2 = 5.884. These parameters were fitted to the\nempirical data.\n\n1.2. Applications of Scaling to Finite-Length Optimization. An important application of the scaling laws which is left for future work is finite-length\noptimization. Combined with analytic expressions of the contribution to the error\nprobability stemming from small (sublinear sized) weaknesses of the graph, the\nscaling laws can be used as an approximation to the performance for finite-length\nensembles. Note also that from the limited examples exhibited in this paper it\nappears that the scaling parameters depend only weakly on the degree distribution. This suggest that a good optimization strategy for finite-length ensembles is\nto optimize the infinity threshold under the condition that the contribution of the\nerror floor leads to acceptable overall performance.\n1.3. Connected Work and Outline. In [13] an approach to analyze the\nfinite-length behavior of turbo-codes was introduced. This method, which the\nauthor call the \"Exit band chart\", is used to describe the probabilistic convergence of the iterative decoding algorithm and provides an approximation of the\nBER in the waterfall region. Somewhat related is also the work by Zemor and\nCohen who study in [24] the \"threshold\" behavior of general classes of codes.\nA preliminary numerical investigation of the scaling (1.2) was presented in [17].\nPartial accounts of the present work appeared in [2, 3].\nIn Section 2 we introduce the necessary notation and review some of the\n\n\f9\n\nFinite-Length Scaling\n\nPB\n10-1\n\n10-2\n\n10-3\n\n10-4\n\n10-5\n0.03\n\n0.05\n\n0.07\n\n0.09\n\n\u01eb\n\nFig. 6: Scaling of ELDPC(n,x2 ,x5 ) [PB (G, \u01eb)] for transmission over BSC(\u01eb) and belief\npropagation decoding. The threshold for this combination is \u01eb\u2217 \u2248 0.084. The blocklengths/expurgation parameters are n/s = 1024/19, 2048/39, 4096/79 and 8192/79, respectively. The solid curves represent the ensemble averages obtained via simulation. The\ndashed curves are computed according to the refined scaling law stated in equation (1.3)\nwith scaling parameters \u03b1 = 1.156 and \u03b2 = 0.1.\n\nbackground material, in particular the density evolution analysis as introduced by\nLuby et. al. in [15]. In Section 3 we state and prove the general form of the\nscaling laws. In Section 4 we then discuss for regular ensembles how the scaling\nparameters can be computed. In section 5 we discuss in detail the refined scaling law and how the shift parameter can be computed. Some of the background\nmaterial and some detailed calculations have been relegated to Appendices.\n2. Review. In this section we recall some basic facts on the density evolution\nanalysis of low-density parity-check (LDPC) codes under iterative decoding. We\nalso fix some of the notation to be used throughout the paper.\n2.1. Ensembles and Channel Models. In this paper we consider both standard as well as Poisson low-density parity-check ensembles. Standard ensembles\nare denoted in the usual way as LDPC(n, \u03bb, \u03c1), where n is the block length and \u03bb\nand \u03c1 denote the degree distributions from an edge perspective, see [15]. For the\nPoisson ensemble the right degree distribution is Poisson. More precisely, given\nthe left degree distribution \u03bb and the rate r, the right degree distribution tends to\nx\u22121\nR\n\n\u03c1(x) = e r\u0304 \u03bb as n \u2192 \u221e. We will denote such an ensemble by LDPC(n, \u03bb, r). To\nsample from the Poisson ensemble pick a bipartite graph with n variable nodes\nand the proper variable node degree distribution. Connect each edge emanating\nfrom a variable node to one of the nr\u0304 check nodes, where the choice is taken\n\n\f10\n\nFinite-Length Scaling\n\nPB\n10-1\n\n10-2\n\n10-3\n\n10-4\n\n10-5\n0.01\n\n0.02\n\n0.03\n\n0.04\n\n\u01eb\n\nFig. 7: Scaling of ELDPC(n,x2 ,x5 ) [PB (G, \u01eb)] for transmission over BSC(\u01eb) and Gallager\nAlgorithm A decoding. The threshold for this combination is \u01eb\u2217 \u2248 0.03946. The blocklengths/expurgation parameters are n/s = 512/50, 1024/70, 2048/100 and 4096/200,\nrespectively. The solid curves represent the ensemble averages obtained via simulation.\nThe dashed curves are computed according to the refined scaling law stated in equation\n(1.3) with scaling parameters \u03b1 = 1.11 and \u03b2 = 0.0.\n\naccording to a uniform probability distribution.\nFrom time to time it is more convenient to describe the degree distributions\nfrom a node perspective. Our notation for the left and right node degree distributions are \u039b and P respectively and we have the following important relationships.\n\u03bb(1) = \u03c1(1) = 1; \u039b(1) = n, P(1) = nr\u0304.\nIt will sometimes be necessary to consider expurgated ensembles. Although\nthere are many expurgation mechanisms possible, we will limit our discussion\nto the following simple scheme. Consider e.g. the case of expurgated Poisson\nensembles. Define ELDPC(n, \u03bb, r, s) as the subset of all elements in LDPC(n, \u03bb, r)\nwhose minimum stopping set size is at least s + 1. As always, endow this set with\nthe uniform probability distribution. E.g., ELDPC(n, \u03bb, r, 2) denotes the Poisson\nensemble which contains no stopping sets of size one or two. The same notational\nconvention is used for expurgated standard ensembles.\nWe will consider two channel models. The more familiar one is the binary\nerasure channel with parameter \u01eb, denoted by BEC(\u01eb), where each bit is erased\nindependently with probability \u01eb. Sometimes though it is more convenient to\nconsider the model BEC(n, n\u01eb), the channel model in which exactly n\u01eb out of all n\nbits are\n\u0001 erased and where the set of these n\u01eb erased bits is chosen uniformly from\nall nn\u01eb such choices.\n\n\fFinite-Length Scaling\n\n11\n\nWe consider scaling laws for both bit as well as block erasure probabilities\nand we will always consider ensemble averages. E.g., in its full notational glory,\nEELDPC(n,\u03bb(x)=x,r= 1 ,s=1) [PB (G, n\u01eb)]\n2\n\nwill denote the expected block erasure probability for cycle Poisson ensembles\nof rate one-half containing no double edges when transmitted over the channel\nBEC(n, n\u01eb). Because of the obvious notational burden we will often replace this\nwith shorthands and we might write e.g.,\n1\nPB (n, \u03bb(x) = x, r = , s = 1, n\u01eb).\n2\nWe might even omit some of the parameters if they are clear from the context.\n2.2. Decoding. There are essentially two alternative ways of defining the\ndecoding algorithm for the BEC(\u01eb). Although they are equivalent in performance\nthey are quite different from the point of view of analysis. First, we can think of\nthe standard message passing decoder in which messages are passed in parallel\nfrom left to right and then back from right to left until the codeword has been\ndecoded or no further progress is achieved, [12]. Alternatively one can think of\nthe decoder as a process which tries to determine one bit at a time in a greedy\nfashion. This is the point of view introduced by Luby et al. in [14, 15] and we\nwill adopt it in this paper. More precisely, the decoder proceeds as follows. Given\nthe received message, the decoder passes all known values on to the check node\nside. These values are accumulated at the check nodes and this partial metric\nis stored. Further, all known nodes and edges over which messages have been\npassed are deleted. In this way one arrives at a residual graph which has a certain\ndegree distribution. The decoder proceeds now in an iterative fashion. If the\nresidual graph contains no degree-one check nodes the decoding process stops.\nOtherwise, the decoder randomly choses one such degree-one check node and\npasses its partial metric to the connected variable node. This variable node is now\nknown. Its value is communicated to all connected check nodes, where the value\nis accumulated to the partial metric. The involved variable node, check node and\nall involved edges are deleted. In this way a new residual graph results and a new\niteration starts.\n2.3. Density Evolution. The advantage of the second description lies in the\nfact that the decoding process is seen as a stochastic process with small increments\n\u2013 at each iteration the change of the degree distribution is a random variable and\nthis change is small. By standard arguments one can show that in the large blocklength limit the behavior of individual instances follows with high probability the\nexpected such behavior and this expected behavior can be expressed as the solution of a differential equation. This is the idea introduced in [15].\nFirst recall that by definition of the ensemble the degree distribution of the\nresidual graph constitutes a sufficient statistics, i.e., given this degree distribution\nall residual graphs which are compatible with this degree distribution (and are\n\n\f12\n\nFinite-Length Scaling\n\ncompatible with the general description of the ensemble, like, e.g., the degree of\nexpurgation) are equally likely. Therefore, in order to analyze the behavior of the\ndecoder it suffices to analyze the evolution of this degree distribution. Let us now\nrecall the solution of the infinite length analysis given in [15] since it forms the\nstarting point for our investigation. Let xl denote the fraction of erasure messages\nentering the variable nodes at a given point in time (here the l stands for right-toleft message). In terms of this parametrization, the evolution of the system (i.e.,\nthe evolution of the degree distribution of the residual graph) is given by\nLi (xl ) = \u01eb\u039bi xil , i \u2265 2,\n\nR0 (xl ) = P(1) \u2212 \u2211 R j (xl ) ,\nj\u22651\n\n\u2032\n\nR1 (xl ) = \u039b (1)\u01eb\u03bb (xl ) [xl \u2212 1 + \u03c1 (1 \u2212 \u01eb\u03bb (xl ))] ,\n\u0012 \u0013\nj\n(\u01eb\u03bb (xl ))i (1 \u2212 \u01eb\u03bb (xl )) j\u2212i , i \u2265 2.\nRi (xl ) = \u2211 Pj\ni\nj\u22652\n\n(2.1)\n(2.2)\n\nHereby, Li (xl ) (Ri (xl )) denotes the expected number of variable (check) nodes\nof degree i at state xl . In the sequel we will refer to these equations as density\nevolution equations. Rather than considering the evolution of the whole degree\ndistribution it suffices often to look at some smaller set of parameters. As we\nhave discussed, the most important parameter in the decoding process is the number of degree-one check nodes, denote it by s (xl ) := R1 (xl ). Further important\nparameters are the size of the residual graph, v (xl ) := \u2211i Li (xl ) and the number of check nodes of degree at least two, t (xl ) := \u2211i\u22652 Ri (xl ). Let \u03bd (xl ), \u03c3 (xl )\nand \u03c4 (xl ) denote the respective fractions, v (xl ) = \u039b(1)\u03bd (xl ), s (xl ) = \u039b(1)\u03c3 (xl ),\nt (xl ) = \u039b(1)\u03c4 (xl ).\nE XAMPLE 1. [Density Evolution of LDPC(n, x2 , x5 )-Ensemble] Fig. 8 depicts the evolution of \u03c3 (dashed line) and \u03c4 (solid line) as a function of \u03bd for\nthe ensemble LDPC(n, x2 , x5 ) for the choice \u01eb = \u01eb\u2217 \u2248 0.4294. Note that for this\nchoice of \u01eb the expected number of check nodes of degree one reaches zero at\nsome critical time of the decoding process.\n\u0003\nThe density evolution equations completely specify the asymptotic behavior\nof the decoder. Recall that the decoder stops if the number of degree-one check\nnodes has reached zero. If this point is reached before the size of the residual\ngraph has reached zero a decoding error occurs. Therefore, if we plot \u03c3(x) as\na function of x for a given channel parameter \u01eb we know that the decoder will\nsucceed with high probability if and only if \u03c3(x) > 0 for all x \u2208 (0, 1]. From\nequation (2.1) we see that \u03c3(x) > 0 for x \u2208 (0, 1] is equivalent to\n\u03c1(1 \u2212 \u01eb\u03bb(x)) > 1 \u2212 x,\n\n\u2200x \u2208 (0, 1].\n\nWe can therefore define the threshold \u01eb\u2217 (\u03bb, \u03c1) as\n\u01eb\u2217 (\u03bb, \u03c1) := sup{\u01eb : \u01eb \u2208 [0, 1], \u03c1(1 \u2212 \u01eb\u03bb(x)) > 1 \u2212 x, \u2200x \u2208 (0, 1]}.\n\n(2.3)\n\n\f13\n\nFinite-Length Scaling\n\n\u03c3/\u03c4\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n0.0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\nv\n\nFig. 8: The evolution of \u03c3 and \u03c4 as a function of \u03bd for the LDPC(n, x2 , x5 ) ensemble and\n\u01eb = \u01eb\u2217 \u2248 0.4294. At \u03bd = \u03bd \u2217 \u2248 0.203, \u03c3(\u03bd) has a minimum and touches the \u03bd-axis.\n\nWe say that x\u2217 is a critical point if \u03c3(x) reaches a minimum at x = x\u2217 and if this\nminimum is zero, i.e., if\n\u03c1(1 \u2212 \u01eb\u2217\u03bb(x\u2217 )) = 1 \u2212 x\u2217.\nTo simplify our matters, we will only discuss ensembles that have a single critical point. The extension to several critical points poses no problems in\nprinciple but is technically more cumbersome. All regular ensembles have this\nproperty. We say that a degree distribution is unconditionally stable if x\u2217 > 0, i.e.,\nif the threshold is not determined by the stability condition. It is easy to check\nthat this is the case for all regular ensembles with lmin \u2265 3. Otherwise, i.e., if\nx\u2217 = 0 we say that the ensemble is marginally stable. The typical example are\ncycle code ensembles. As we will see, the nature of this scaling is drastically\ndifferent for the two cases. Finally, we will assume that the degree distributions\n\u03bb(x) and \u03c1(x) (or just \u03bb(x) for Poisson ensembles) are polynomials. In this case\nthe density evolution equations have only a finite number of minima and maxima.\nThis is a purely technical condition to avoid some pathological cases which are of\nno practical interest.\n3. Main Results and Discussion. The following statements apply both to\nstandard ensembles and Poisson ensembles. Generically we will denote such an\nx\u22121\nR\n\u03bb\n\nensemble by LDPC(n, \u03bb, \u03c1). In the Poisson case we can think of \u03c1(x) = e r\u0304\n\n.\n\n3.1. Unconditionally Stable Ensembles. The basic scaling law as given in\n(1.2) is stated more precisely in the following.\n\n\f14\n\nFinite-Length Scaling\n\nL EMMA 3.1. [Scaling of Unconditionally Stable Ensembles] Consider transmission over the BEC(\u01eb) using random elements from an ensemble LDPC(n, \u03bb, \u03c1)\nwhich has a single critical point and is unconditionally stable. Let \u01eb\u2217 = \u01eb\u2217 (\u03bb, \u03c1)\ndenote the threshold and let \u03bd \u2217 denote the fractional size of the residual\ngraph\n\u221a\nat the critical point corresponding to the threshold. Fix z to be z := n(\u01eb\u2217 \u2212 \u01eb).\nLet Pb (n, \u03bb, \u03c1, \u01eb) denote the expected bit erasure probability and let PB,\u03b3 (n, \u03bb, \u03c1, \u01eb)\ndenote the expected block erasure probability due to errors of size at least \u03b3\u03bd \u2217 ,\nwhere \u03b3 \u2208 (0, 1). Then as n tends to infinity,\n\u0010z\u0011\n\n(1 + on(1)),\n\u03b1\u0010 \u0011\nz\n(1 + on(1)),\nPb (n, \u03bb, \u03c1, \u01eb) = \u03bd \u2217 Q\n\u03b1\n\nPB,\u03b3 (n, \u03bb, \u03c1, \u01eb) = Q\n\nwhere \u03b1 = \u03b1(\u03bb, \u03c1) is a constant which depends on the ensemble.\nProof. First note that if \u03bb\u2032 (0) = 0, i.e., if there are no degree-two variable\nnodes, then the block erasure probability is dominated over the whole range of \u01eb\nby large error events (when n tends to infinity). This means that PB,\u03b3 is equal to\nthe ordinary block error probability.\nThis is no longer true once \u03bb\u2032 (0) > 0. If 0 < \u03bb\u2032 (0)\u03c1\u2032 (1) < 1 then the ensemble\ncan be expurgated in order to eliminate small (sublinear weaknesses in the graph)\nand the above scaling law will then account for all errors. If one the other hand no\nsuch expurgation is done or if \u03bb\u2032 (0)\u03c1\u2032 (1) > 1, then besides the contribution to PB\nstemming from large error events also the contribution stemming from sublinearsized weaknesses in the graph will be non-negligible. The above scaling law only\napplies to the first contribution. The bit erasure probability is not affected by these\nconsiderations since the contribution of sublinear-sized stopping sets in the graph\nvanishes as n-tends to infinity. Fortunately, the effect of sublinear-sized stopping\nsets is relatively easy to assess by union bounding techniques. The total erasure\nprobability can be represented as the sum of these two contributions. For a more\ndetailed discussion we refer the reader to [7, 19, 25].\nOur approach will be to consider first a situation slightly simplified with\nrespect to the one encountered in iterative decoding. This will be done in Section\n4 (see Proposition 4.1) and Appendix A. The basic tools needed for the proof of\nthis lemma will be introduced in such a simplified context. It turns out that the\nmain conclusions hold true when the simplifying assumptions are removed. This\nwill be shown in Appendix B.\nWe conjecture that in fact the following refined scaling law is valid.\nC ONJECTURE 3.1. [Refined Scaling of Unconditionally Stable Ensembles]\nConsider transmission over the BEC(\u01eb) using random elements from an ensemble\nLDPC(n, \u03bb, \u03c1) which has a single critical point and is unconditionally stable. Let\n\u01eb\u2217 = \u01eb\u2217 (\u03bb, \u03c1) denote the threshold and let \u03bd \u2217 denote the fractional size of the\nresidual graph at the threshold. Let Pb (n, \u03bb, \u03c1, \u01eb) denote the expected bit erasure\nprobability and let PB,\u03b3 (n, \u03bb, \u03c1, \u01eb) denote the expected block erasure probability\n\u221a\ndue to errors of size at least \u03b3\u03bd \u2217 , where \u03b3 \u2208 (0, 1). Fix z to be z := n(\u01eb\u2217 \u2212\n\n\fFinite-Length Scaling\n\n15\n\n2\n\n\u03b2n\u2212 3 \u2212 \u01eb). Then as n tends to infinity,\n\u0010 z \u0011\u0010\n\u0011\nPB,\u03b3 (n, \u03bb, \u03c1, \u01eb) = Q\n1 + O(n\u22121/3 ,\n\u03b1\u0010 \u0011 \u0010\n\u0011\nz\n\u2217\n1 + O(n\u22121/3 ,\nPb (n, \u03bb, \u03c1, \u01eb) = \u03bd Q\n\u03b1\nwhere \u03b1 = \u03b1(\u03bb, \u03c1) and \u03b2 = \u03b2(\u03bb, \u03c1) are constants which depend on the ensemble.\nThis conjecture can be proven in the simplified context mentioned above\n(and defined in Section 4). This is done in Sec. 5. At the end of the same section,\nwe provide some heuristic argument suggesting that the simplifying assumptions\nare in fact irrelevant.\nIn the remainder of this section we provide an informal (albeit essentially\ncorrect) justification of the above scaling forms. The question of how to compute\nthe scaling parameters will be deferred to Sections 4 (for the variance \u03b12 ) and 5\n(for the shift \u03b2).\nConsider the behavior of the individual trajectories of the decoding process\nfor particular choices of the graph and the channel realization. We will see that\nthese trajectories closely follow the expected value (given \u221a\nby the density evolution\nequations) and that their standard deviation is of order n. Consider now the\ndecoding process and assume that the channel parameter \u01eb is close to \u01eb\u2217 . If \u01eb = \u01eb\u2217\nthen at the critical point the expected number of degree-one check nodes is zero.\nAssume now that we vary \u01eb slightly. From the density evolution equation (2.1) we\nsee that the expected change in the fraction of degree-one check nodes (\u03c3 = s/n)\nat the critical point is\n\u2202\u03c3\n\u2202\u01eb\n\nx=x\u2217 ;\u01eb=\u01eb\u2217\n\n=\u2212\n\n\u039b\u2032 (1) \u2217 \u2217 2 \u2032\n\u01eb \u03bb(x ) \u03c1 (1 \u2212 \u01eb\u2217\u03bb(x\u2217 )).\n\u039b(1)\n\n(3.1)\n\nIf we vary \u01eb so that \u2206\u01eb is of order \u0398(1), then we conclude from (3.1) that the\nexpected number of degree-one check nodes\u221a\nat the critical point is of order \u0398(n).\nSince the standard deviation is of order \u0398( n), then with high probability the\ndecoding process will either succeed (if (\u01eb \u2212 \u01eb\u2217 ) < 0) or die (if (\u01eb \u2212 \u01eb\u2217 ) > 0). The\ninteresting\n\u221a scaling happens if we choose our variation of \u01eb in such a way that\n\u2206\u01eb = z/ n, where z is a constant. In this case the expected gap at the critical\npoint scales in the same way as the standard deviation and one would expect that\nthe probability of error stays constant. Varying now the constant z will give rise\nto the scaling function f (z), cf. equation (1.1).\nWe will further see that the distribution of states at any time before hitting the\ns = 0 plane is Gaussian and that the evolution of its covariance matrix is governed\nby a set of differential equations in the same way as the mean. We will therefore\ncall these equations the covariance evolution equations. As an example, consider\nthe ensemble LDPC(n, x2 , x5 ) and transmission over the channel BEC(n, n\u01eb). In\nthis case the residual graph at the start of the decoding process has exactly n\u01eb\nvariable nodes and since at each step of the decoding process exactly one variable\nnode is pealed off, the size of the residual graph after the l-th decoding step is\n\n\f16\n\nFinite-Length Scaling\n\nexactly n\u01eb \u2212 l (assuming the decoder has not stopped prematurely). As we will\ndiscuss in more detail in Section 4, it suffices in this case to keep track of the\ntuple (s,t) (i.e., we do not need to keep track of the whole degree distribution\nof the residual graph). Fig. 9 shows the evolution of (s,t) as a function of the\nsize of the residual graph for the choice \u01eb = \u01eb\u2217 . The solid line corresponds to\nthe density evolution equation (albeit now in three-dimensional form). The dot\nindicates the critical point. The ellipsoids represent the covariance matrix. More\nprecisely, they represent contours of constant probability. Note\n\u221a that this picture is\nslightly misleading. The ellipsoids really live on a scale of n whereas the rest of\nthe graph is scaled by n, i.e., for increasing length the ellipsoids will concentrate\nmore and more around the expected value. Those trajectories that hit the s = 0\n\n0.1\n0.15\ns\n\n0\n0\n\n0.2\n0.4\n\n0.2\n0.15\nv\n\n0.6\n0.1\n0.05\n\nt\n\n0.8\n0\n\nFig. 9: A pictorial representation of density and covariance evolution for the\nLDPC(n, x2 , x5 ). Notice that the ellipsoids corresponding to (s,t) covariances should be\n\u221a\nregarded as living on a smaller (by a factor n) scale than the typical trajectory.\n\nplane die. This corresponds to the part of the ellipsoids that vanish.\nOne can quantify the probability for the process to hit the s = 0 plane as\nfollows. Stop density and covariance evolution when the number of variables\nreaches the critical value v\u2217 . At this point the probability distribution of the state\nis well approximated by a Gaussian with a given mean and covariance for s \u2265 0\n(while it is obviously 0 for s < 0). Estimate the survival probability (i.e. the\nprobability of not hitting the s = 0 plane at any time) by summing the Gaussian\ndistribution over s \u2265 0. Obviously this integral can be expressed in terms of a\nQ-function.\nWe will see that the above description leads indeed to the scaling behavior\nas stated in Lemma 3.1. Where does the shift in Conjecture 3.1 come from? It\nis easy to understand that we were a bit optimistic (i.e., we underestimated the\nerror probability) in the above calculation: We correctly excluded from the sum\n\n\fFinite-Length Scaling\n\n17\n\nthe part of the Gaussian distribution lying in the s < 0 half-space \u2013 trajectories\ncontributing to this part must have hit the s = 0 plane at some point in the past.\nOn the other hand, we cannot be certain that trajectories such that s > 0 when v\ncrosses v\u2217 didn't hit the s = 0 plane at some time in the past and bounced back (or\nwill not hit it at some later point). We refer to Section 5 for an in-depth discussion\non how to estimate this effect.\nLet us finally recall that the performance over the BEC(\u01eb) channel can be\neasily derived from the results obtained using the model BEC(n, n\u01eb). One can\nderive the erasure probability for the first case by summing the conditional erasure\nprobability, where the conditioning is on the number of erasures. Notice that the\nnumber of erasures \u221a\nfor the BEC(\u01eb) is asymptotically Gaussian with mean n\u01eb and\nstandard deviation n\u01eb\u01eb. Since this standard deviation is of the same order as\nthe gap to the threshold such a convolution gives a non trivial contribution, unlike\nin the Shannon ensemble example, cf. Section 1. It is easy to verify that this\nconvolution amounts to computing the parameter \u03b12 , cf. Lemma 3.1 as the sum\nof two contributions: one due to the channel fluctuations and the other due to\ncovariance evolution. More precisely we have\n\u03b12BEC(\u01eb) = \u03b12BEC(n,n\u01eb) + \u01eb\u2217 \u01eb\u2217 ,\n\n(3.2)\n\nwhere we took \u01eb = \u01eb\u2217 since we are interested in the region \u01eb = \u01eb\u2217 + O(n\u22121/2 )\nand we can neglect O(n\u22121/2 ) corrections. Hereafter we shall mostly focus on the\nBEC(n, n\u01eb) channel. The reader is invited to use the formula (3.2) for translating\nthe results whenever necessary.\n3.2. Marginally Stable Ensembles. As already mentioned, marginally stable ensembles are expected to follow a different scaling from the one described\nin Lemma 3.1. We will limit our discussion to the simplest case, namely the case\nof cycle code ensembles. We conjecture though that the form of the scaling law\nis quite general and applies to all marginally stable ensembles. The cycle Poisson\nensemble is slightly easier to handle analytically than the standard ensemble. We\nwill therefore formulate our results mainly for this case.\nL EMMA 3.2. [Scaling of Block Probability for Cycle Poisson Ensembles]\nConsider transmission over BEC(n, n\u01eb) using elements from ELDPC(n, \u03bb(x) =\nx, r, s). Then\n\u0010\n\u0011\nPB (n, \u03bb(x) = x, r, s, n\u01eb) = 1 \u2212 A(s)an\u22121/6 f (b n1/3 (\u01eb \u2212 \u01eb\u2217)) 1 + O(n\u22121/3) ,\n\b\nwhere a = r\u0304\u22121/6 , b = r\u0304\u22122/3 , A(s) = exp \u2211ss\u2032 =1 2s1 \u2032 , and\n\u221a\n2\u03c032/3 \u2212 4x3\ne 3 p(32/3 x; 3/2, \u22121) .\nf (x) =\n2\nHereby, p(u; \u03b1, \u03b2) is a so called stable density with representation\nZ\nn\no\n\u03c0\n1\ne\u2212itu exp \u2212|t|\u03b1 e\u2212i 2 K(\u03b1)\u03b2 sign(t) dt,\np(u; \u03b1, \u03b2) =\n2\u03c0\n\n\f18\n\nFinite-Length Scaling\n\nand K(\u03b1) = 1\u2212 | 1 \u2212 \u03b1 |.\nProof. In principle one could arrive at the above result by proceeding in the\nsame fashion as for unconditionally stable ensembles, i.e., one could employ the\ntools of density evolution and covariance evolution.\nWe will however use an entirely different approach. Note that there is a\none-to-one correspondence between elements of ELDPC(n, \u03bb(x) = x, r, s = 2) and\nrandom graphs on nr\u0304 nodes with exactly n edges, see [20]. If s = 2, then double\nedges and cycles of length four are excluded from the Tanner graph. Therefore,\neach variable node connects two distinct check nodes and no two variable nodes\nconnect the same pair. If we therefore identify each variable node (and the two\nedges that emanate from it) with one edge in an ordinary graph we get our desired\ncorrespondence. Further, the decoder will be successful if and only if this random\ngraph is a forest, i.e., a collection of trees. Let F(l, k) denote the number of\nforests on l labeled nodes and k components. Such a forest has l \u2212 k edges and\ntherefore it corresponds to a constellation on v = l \u2212 k variable nodes. Since these\nvariable nodes can be ordered arbitrarily it follows that there are v!F(nr\u0304, nr\u0304 \u2212 v)\nconstellations on v variable nodes which do not contain stopping sets.\nIt remains to find the total number of constellations on v variable nodes which\nare compatible with the expurgation scheme. The desired result will then follow\nby diving these two quantities. Assume s = 0. Then the total number of constellations on v variable nodes is equal to (nr\u0304)2v , since for each edge we can choose\none of the nr\u0304 check nodes. Let ns (G) denote the number of cycles of length 2s in\na fixed portion of the bipartite graph G of size v. It is easy\u0001 to verify (and is a well\n1 2v s\nstudied problem in random graphs) that E[ns (G)] = 2s\nnr\u0304 (1 + O(1/v)). Further\nit is known that for each fixed s the random variables (n1 , * * * ns ) are asymptotically\n(as n and v tend to infinity with a fixed ratio) independent and follow a Poisson\ndistribution, [4]. Finally, for the Poisson ensemble we have \u01eb\u2217 = 2r\u0304 so that around\n2v\nthe critical value v = \u01eb\u2217 n = nr\u0304\n2 and nr\u0304 = 1. It follows that around the threshold the\ntotal number of constellations which are compatible with the expurgation scheme\nbehaves like\n1\n\nT (v \u223c n\u01eb\u2217 ) = (nr\u0304)2v e\u2212 \u2211s\u2032 =1 2s\u2032 (1 + O(1/v)) = (nr\u0304)2v /A(s)(1 + O(1/v)).\ns\n\nFrom this the block error probability around the threshold follows immediately\nonce F(l, k) is known, namely, we have\nPB (n, \u03bb(x) = x, r, s, n\u01eb \u223c n\u01eb\u2217 ) = 1 \u2212 A(s)\n\n(n\u01eb)!F(nr\u0304, nr\u0304 \u2212 n\u01eb)\n(1 + O(1/n)) .\n(nr\u0304)2n\u01eb\n\nOne of the most celebrated formulas in enumerative combinatorics states that\nthere are l l\u22122 labeled trees on l nodes, [23]. Unfortunately there does not seem\nto exist an equally elementary expression for the number of labeled forests. The\nsituation is aggravated by the fact that we are interested in the region where the\naverage number of edges per node is around one. Exactly around this region the\ngraph goes through a phase transition and so the behavior of F(l, k) is nontrivial\neven in the limit of large sizes. Fortunately, the asymptotic behavior has been\n\n\fFinite-Length Scaling\n\n19\n\ndetermined by Britkov [5] and the result has been made accessible (to the English speaking audience) in the book by Kolchin [11]. Our result now follows by\nemploying the asymptotic approximation stated in Theorem 1.4.4 in [11].5\nNote, that for the cycle case the maximum likelihood and the iterative decoder perform identical in terms of block erasure probability. This is true since in\nthis case the condition of no stopping sets is equal to the condition that there are\nno cycles which in turns implies that there is no codeword. Note, however, that\nthis is no longer true once we look at the resulting bit erasure probability.\nWe also note that if we want to get the scaling law for the channel BEC(\u01eb)\nwe need to convolve the above curves with the Binomial with mean n\u01eb. However,\non the scale \u01eb\u2217 \u2212 \u01eb = O(n\u22121/3 ), the effect of the channel fluctuations vanishes in\nthe large blocklength limit. The leading correction to the scaling law (3.3) coming\nfrom the channel consists in the substitution\nf (x) \u2192 f (x) +\n\n\u01eb\u2217 (1 \u2212 \u01eb\u2217) \u2032\u2032\nf (x) n\u22121/3 + O(n\u22121/2) .\n(1 \u2212 r)4/3\n\n(3.3)\n\nThe following lemma characterizes the corresponding limiting block erasure\nprobability curve.\nL EMMA 3.3. [Asymptotic Block Erasure Probability Curve] Consider transmission over BEC(n, n\u01eb) or BEC(\u01eb) using random elements from ELDPC(n, \u03bb(x) =\nx, r, s). Then\n(\n\u0001 \u2032)\nr\n\u01eb s\ns\n\u01eb\n\u2217\n.\nlim PB (n, \u03bb(x) = x, r, s, n\u01eb) = 1 \u2212 1 \u2212 \u2217 exp \u2211 \u01eb \u2032\nn\u2192\u221e\n\u01eb\n2s\ns\u2032 =1\nThe corresponding asymptotic bit erasure probability curve under iterative decoding can be obtained through a standard density evolution analysis and it is given\nin parametric form by\n\u0012\n\u0013\nx\nx\u039b(1 \u2212 \u03c1(1 \u2212 x))\n,\n,\n\u03bb(1 \u2212 \u03c1(1 \u2212 x)) \u03bb(1 \u2212 \u03c1(1 \u2212 x))\nwhere x \u2208 (x\u2217 , 1] and x\u2217 is the solution to the equation \u01eb\u2217 \u03bb(1 \u2212 \u03c1(1 \u2212 x)) = x.\nFigure 10 shows the resulting bit and block erasure curves for ELDPC(n, \u03bb(x) =\nx, r = 12 , s = 1).\nCycle codes can not be expurgated up to some linear fraction of the block\nlength since the number of stopping sets of size s1 , * * * sk are jointly Poisson and\nhave mean equal to (2/r\u0304)si /(2si ), respectively. Below the threshold \u01eb\u2217 = r\u0304/2, the\nbit erasure probability scales as 1/n. Expurgation changes uniquely the coefficient\nof this scaling. A simple calculation yields\n\u0012 \u0013\n2\u01eb\n1\n(1 + O(1/n)) ,\n(3.4)\nPb (n, \u03bb(x) = x, r, s, n\u01eb) = Ls\n2n\nr\u0304\n5 The\n\nreader is warned that there is a slight typo in Theorem 1.4.4 as stated in [11].\n\n\f20\n\nFinite-Length Scaling\n\nPb\n\nPB\n0.9\n\n10-1\n0.8\n0.7\n\n10-2\n\n0.6\n10-3\n\n0.5\n0.4\n\n10-4\n0.3\n0.2\n\n10-5\n\n0.1\n-6\n\n10\n\n0.0\n\n0.1\n\n0.2\n\n0.3\n\n0.0\n\n\u01eb\n\n0.4\n\n0.0\n\n0.1\n\n0.2\n\n0.3\n\n\u01eb\n\nFig. 10: The bit and block erasure probability for ELDPC(n, \u03bb(x) = x, r = 12 , s = 1) for\nn = 2i , i = 8, 10, 12, 14. As can be seen from the picture, the block erasure curves actually\nconverge to a limiting (non-zero) curve over the whole range of \u01eb, whereas the bit erasure\ncurves decrease to zero below the threshold for increasing block lengths. Also shown are\nthe result of using the scaling laws for the block erasure probability as stated in Lemma\n3.2.\n\nwhere we defined the function\n\u221e\n\nLs (x) :=\n\n\u2032\n\n\u2032\n\ns\nxs\nxs\n= \u2212 log(1 \u2212 x) \u2212 \u2211 \u2032 .\n\u2211\n\u2032\ns\ns\ns\u2032 =s+1\ns\u2032=1\n\nAs shown in Fig. 11, this formula provides a good approximation to the bit error\nprobability away from the critical region. Notice in fact that the coefficient of the\n1/n term in Eq. (3.4) diverges as \u01eb \u2192 \u01eb\u2217 .\nPB\n10-1\n\n10-2\n\n10-3\n\n10-4\n\n10-5\n\n10-6\n0.0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n\u01eb\n\nFig. 11: Comparison of the exact bit erasure curves (solid line) with the analytic expression given in (3.4) (dashed lines) for n = 2i , i = 8, 10, 12, 14 and \u01eb < \u01eb\u2217 .\n\n4. Computation of the Variance Parameter. In the previous section we\nsaw that the basic scaling law, cf. Lemma 3.1, only depends on the variance \u03b12 .\nIn this section we will work out in detail the calculation of this parameter. In\n\n\fFinite-Length Scaling\n\n21\n\nSection 5 we will present the method to be used for computing \u03b2 which is needed\nfor the conjectured refined form of the scaling law.\nAlthough conceptually it is straightforward to write down the equations for\nthe general irregular case, the actual computations are quite cumbersome. We will\ntherefore proceed as follows. In Section 4.1 we discuss the covariance evolution\nequations in an abstract setting. These are applied to particular regular LDPC\nensembles in Section 4.2.\n4.1. General Covariance Evolution. We regard iterative decoding as a Markov\nprocess in a finite dimensional space. The examples in the next two subsections\nwill make clear how this framework can be adapted to particular code ensembles.\nConsider a family of Markov chains Xn,0 , Xn,1 , . . . , Xn,t , . . . parametrized by\nn \u2208 N and taking values in Zd+1 . For iterative decoding applications, n will represent the blocklength. We drop the subscript n hereafter. Let the transition probability be\nP(Xt+1 = x\u2032 |Xt = x) = W (x\u2032 \u2212 x|x) ,\n\n(4.1)\n\nand the initial condition be a single non-random state X0 = x0 \u2208 Zd+1 . In iterative\ndecoding the initial condition is actually a distribution over states. This case is\neasy to treat by first conditioning on the initial state, and then convolving with the\ninitial distribution. We will denote the d + 1 coordinates of the state x as\n(x(0) , x(1) , . . . , x(d) ) = x \u2208 Zd+1 .\n\n(4.2)\n\nWe denote the corresponding random variable by (X (0) , X (1) , . . . , X (d) ).\nIn the following we shall always be interested in times t < \u03ba0 n for a positive\nconstant \u03ba0 (we reserve the symbols \u03ba1 , \u03ba2 , . . . for numerical constants which we\nassume not to depend upon n). We shall moreover assume the following regularity\nproperties of the Markov chain:\n1. The chain makes finite jumps. In other words, there exists a \u03ba1 > 0 such\n(i)\n(i)\nthat |Xt+1 \u2212 Xt | < \u03ba1 almost surely.\n2. The transition probabilities have a smooth n \u2192 \u221e limit. In practice there\nb : Zd+1 \u00d7 Rd+1 \u2192 R+ and a positive constant \u03ba2 such\nexist functions W\nthat\nb (\u2206|x/n)| < \u03ba2 /n .\n|W (\u2206|x) \u2212 W\n\n(4.3)\n\nb (\u2206|x/n) = 1. We shall moreover assume W\nb (\u2206|z)\nClearly, we have \u2211\u2206 W\nto be C2 (Rd+1 ) with respect to its second argument and to have bounded\nfirst and second derivatives.\n3. The process has a finite range on the n scale. In practice, there exists\n(i)\n\u03ba3 > 0 such that |Xt | < \u03ba3 n almost surely.\nUnder these hypothesis the distribution of Xt is well described by a Gaussian whose mean and variance can be obtained by solving some ordinary differential equations. In order to state this fact in a more precise fashion, we\n\n\f22\n\nFinite-Length Scaling\n\nneed some additional notation. We denote by X t \u2261 E[Xt ] the average of Xt and\n(i j)\n(i)\n( j)\n(i) ( j)\n(i)\n( j)\nDt \u2261 E[Xt ; Xt ] \u2261 E[Xt Xt ] \u2212 E[Xt ]E[Xt ] its covariance. We need furthermore the first two moments of the transition rates W (\u2206|x):\nf (i) (x) \u2261 \u2211 \u2206i W (\u2206|x) ,\n\n(4.4)\n\n\u2206\n\nf (i j) (x) \u2261 \u2211 \u2206i \u2206 j W (\u2206|x) \u2212 f (i) (x) f ( j) (x) ,\n\n(4.5)\n\n\u2206\n\nwith i, j \u2208 {0, . . . , d}. We shall call f\u02c6(i) (z), f\u02c6(i j) (z) the analogous quantities for\nb (\u2206|z).\nthe limiting rates W\nFinally, let z(\u03c4 ) \u2208 Rd+1 and \u03b4 (i j) (\u03c4 ) \u2208 R, for \u03c4 \u2208 R+ and i, j \u2208 {0, . . . , d},\ndenote the solution of\ndz(i)\n(\u03c4 ) = f\u02c6(i) (z(\u03c4 )) ,\nd\u03c4\n\n(4.6)\n\uf8ee\n\nd\nd\u03b4 (i j)\n\u2202 f\u02c6( j)\n(\u03c4 ) = f\u02c6(i j) (z(\u03c4 )) + \u2211 \uf8f0\u03b4 (ik) (\u03c4 ) (k)\nd\u03c4\n\u2202z\nk=0\n\n+\nz(\u03c4 )\n\n\u2202 f\u02c6(i)\n\u2202z(k)\n\nz(\u03c4 )\n\n\uf8f9\n\n\u03b4 (k j) (\u03c4 )\uf8fb .(4.7)\n\nwith initial conditions z(0) = x0 /n and \u03b4 (i j) (0) = 0.\nP ROPOSITION 4.1. Under the conditions stated above the following results\nhold (here we use the symbols \u03a90 , \u03a91 , . . . , for constants (independent of n) which\nwe prove to exist):\nI. Xt concentrates on the n scale. In formulae, there exist \u03a90 > 0, such that\n(i)\n\n(i)\n\nP{|Xt \u2212 X t | \u2265 \u03c1} \u2264 2 e\n\n2\n\n\u03c1\n\u2212 2\u03a9\n0t .\n\n(4.8)\n\nII. The average and covariance of Xt are accurately tracked by z(\u03c4 ) and\n\u03b4 (i j) (\u03c4 ). More precisely, there exist constants \u03a91 , \u03a92 > 0, such that\n\u03a91\n1 (i) (i)\nX t \u2212 z (t/n) \u2264\n,\n(4.9)\nn\nn\n1 (i j)\n\u03a92\nDt \u2212 \u03b4 (i j) (t/n) \u2264 \u221a .\n(4.10)\nn\nn\n\u221a\nIII. The variable (Xt \u2212 X t )/ n converges weakly to a (d + 1)-dimensional\nGaussian with variance \u03b4 (i j) (t/n). More precisely, define the logarithmic moment generating function\n\u0015\n\u0014\n1\n(4.11)\n\u039bt (\u03bb) \u2261 log E exp \u221a \u03bb * (Xt \u2212 X t ) ,\nn\nfor \u03bb \u2208 Rd+1 . Then there exist a function \u03bb 7\u2192 \u03a94 (\u03bb) \u2208 R+ , such that\n\u039bt (\u03bb) \u2212\n\n\u03a94 (\u03bb)\n1\n\u03b4 (i j) (t/n)\u03bbi \u03bb j \u2264 \u221a .\n\u2211\n2 ij\nn\n\n(4.12)\n\n\fFinite-Length Scaling\n\n23\n\nThe proof is quite straightforward and will be outlined in App. A. Here we\nlimit ourselves to a few comments.\nNotice that the statements collected in the above proposition are not all independent. Equation (4.10), may for instance be regarded as a consequence of\nEq. (4.12). The various results are presented in order of increasing sharpness.\nAlso, not all of the assumptions in the points 1-3 are needed to proof each of the\nstatements in the proposition. For instance, the concentration result is an easy\nconsequence of the Hoeffding-Azuma inequality and requires the hypotheses 1\n(uniformly bounded jumps), 3 (scaling of time with n) plus some Lipschitz property of the drift coefficients f (i) (x), cf. Eq. (4.4). This point is further discussed\nin App. A. The limitation to a deterministic initial condition is easily removed. In\niterative decoding applications\n\u221a the initial condition is a Gaussian distribution with\nstandard deviation of order n. Convolution with such a distribution amounts to\nintegrating equation (4.7) and taking as initial condition the initial covariance. Finally, the situation investigated here can be regarded as a discrete analogous of\nthe Friedlin-Wentzell theory of random perturbations of dynamical systems [9].\nIn the following section we shall apply the above analysis to two LDPC\nensembles: the standard regular ensemble LDPC(n, xl\u22121 , xr\u22121 ), and the regular\nPoisson ensemble LDPC(n, xl\u22121 , r). The general strategy is the following: (i) Determine a sufficient statistics for the decoding process. For a general LDPC(n, \u03bb, \u03c1)\nensemble, a sufficient statistics is provided by the degree distributions at variable\nand check nodes in the residual graph. As we will see, a more compact representation is available for the two special cases mentioned above. (ii) Write the\ntransition probability for iterative decoding and compute the drift and diffusion\ncoefficients, cf. Eqs. (4.4), (4.5). (iii) Determine the initial condition, namely the\naverage state, and its variance before the decoding process has been started. (iv)\nIntegrate the density evolution and covariance evolution equation, cf. Eq. (4.6)\nand (4.7) up to the critical point. The parameter \u03b1 in Lemma 3.1 is finally given\n(up to a rescaling) by the standard deviation of the number of degree one check\nnodes s at the critical point. More precisely:\n\u0012 \u0013\u22121\np\n\u2202\u03c3\n\u03b1 = \u03b4\u03c3\u03c3\n,\n\u2202\u01eb\n\n(4.13)\n\nboth factors being evaluated at the critical point.\n\n4.2. Regular Ensembles. We will now show the explicit computations that\nneed to be done in order to accomplish the program outlined in the previous section for the case of regular standard and Poisson ensembles.\nThere are some significant simplifications that arise in this case. Note that\nthe triple (v, s,t) constitutes a sufficient statistics, i.e., it suffices to keep track of\nthe number of variable nodes (all of which have degree l since by assumption the\ngraph is regular), the number of degree-one check nodes and the number of check\nnodes of degree two or higher. This can be seen as follows. We claim that all\nconstellations of \"type\" (v, s,t) have uniform probability. To see this let G\u03031 and G\u03032\nbe two residual graphs of type (v, s,t). Assume that G\u03031 is the result of applying\n\n\f24\n\nFinite-Length Scaling\n\nthe iterative decoder to the graph G1 with a particular channel realization and a\nparticular sequence of choices of the iterative decoder. It is then easy to see that\nthere exists a graph G2 which differs from G1 only on the residual part (where it\ncoincides with G\u03032 ) but agrees with it otherwise. By definition of the ensemble,\nG1 and G2 have equal probability and if the iterative decoder is applied to G2 with\nthe same channel realization and sequence of random choices we get G\u03032 . This\nshows that G\u03031 and G\u03032 (and therefore any residual graph which is compatible with\nthe degree distribution) have equal probability. It follows that, given (v, s,t), the\ndistribution of G is determined so that (v, s,t) indeed constitutes a state.\nLet us now determine the degree distribution of a \"typical\" element G of\ntype (v, s,t), since this knowledge will be required in the sequel. For the standard\nensemble define the generator polynomial p(z) := (1 + z)r \u2212 rz \u2212 1 which counts\nthe number of connections into a check node of degree two or higher. For the\nPoisson ensemble the equivalent function is p(z) := ez \u2212 z \u2212 1. Define a(z) :=\n\u2032 (z)\n. The total number of constellations on t check nodes of degree at least two\nz pp(z)\nwith vl \u2212 s edges is easily seen to be coef {p(x)t , xvl\u2212s }. Let ti , i \u2265 2, denote the\nnumber of check nodes of degree i. Then the total number of constellations which\nare compatible with the desired type can be written as\n\n\u2211\n\nt2 ,t3 ,***:\u2211i\u22652 ti =t;\u2211i\u22652 iti =vl\u2212s\n\n\u0012\n\nt\n\nt2 ,t3 , * * *\n\n\u0013\n\n\u220f\ni\u22652\n\nptii\n\n!\n\n.\n\nSince all constellations have equal probability a \"typical\" constellation will have\nthe type which \"dominates\" the above sum. Some calculus reveals that this dominating type has the form\n\u03c4i =\n\npi zi\n\u03c4,\np(z)\n\ni \u2265 2,\n\n(4.14)\n\nwhere \u03c4i , i \u2265 2, denotes the fraction of check nodes of degree i and where a(z) =\n\u03bdl\u2212\u03c3\n\u03c4 .\nWe will see shortly that for the Poisson case it suffices to consider ensembles of rate zero since the scaling parameters for the general case can be easily\nconnected to this case. Therefore in the next theorem we can assume without loss\nof generality that the rate is zero for Poisson ensembles.\nL EMMA 4.1. [Drift, Variance and Partial Derivatives for Regular Ensembles] Consider regular standard ensembles LDPC(n, xl\u22121 , xr\u22121 ) or regular Poisson ensembles LDPC(n, xl\u22121 , r = 0). Define\n(\n(1 + z)r \u2212 1 \u2212 rz, standard ensemble,\np(z) = z\ne \u2212 1 \u2212 z,\nPoisson ensemble,\n\u2032\n\n(z)\n. Let xl denote the right-to-left erasure probability and let\nand let a(z) := z pp(z)\n\n\f25\n\nFinite-Length Scaling\n\nxr := \u01eb\u03bb(xl ). Then along the density evolution path parametrized by xl we have\n2\u03c42\nf\u02c6(\u03c4 ) = \u2212(l \u2212 1)\n,\n\u03bdl\n!\n\u02c6(\u03c4 )\n\u02c6f (\u03c4 \u03c4 ) = \u2212 f\u02c6(\u03c4 ) 1 + f\n,\nl\u22121\n\n\u03c3\nf\u02c6(\u03c3) = \u22121 \u2212 (l \u2212 1) \u2212 f\u02c6(\u03c4 ) ,\n\u03bdl !\n(\u03c3 ) + 1\n\u02c6\nf\nf\u02c6(\u03c3\u03c4 ) = f\u02c6(\u03c4 ) 1 \u2212\n,\nl\u22121\n\n2(l \u2212 1) p2 z(2 \u2212 a(z))\n\u2202 f\u02c6(\u03c3)\nl \u2212 1 \u03c3 \u2202 f\u02c6(\u03c4 )\n\u2202 f\u02c6(\u03c4 )\n=\n,\n=\n\u2212\n\u2032\n\u2202\u03c3\n\u03bdl\na (z)p(z)\n\u2202\u03bd\n\u03bdl \u03bd\n\u2202\u03bd\n(\n\u03c3)\n(\n\u03c4)\n(\n\u03c3)\n\u02c6\n\u02c6\n\u02c6\n\u2202f\n\u2202f\n\u2202f\nl \u2212 1 \u2202 f\u02c6(\u03c4 )\n=\u2212\n,\n=\u2212\n\u2212\n,\n\u2202\u03c4\n\u2202\u03c4\u0010\n\u2202\u03c3\n\u03bdl\n\u2202\u03c3\n\u00112\n\u0010\n\u0010\u03c3\n\u0011 \u03c3\nf\u02c6(\u03c4 )\n\u03c3\u0011\n,\n\u2212 (l \u2212 1)\n\u22121\n\u2212 f\u02c6(\u03c4 ) 1 + 2\nf\u02c6(\u03c3\u03c3) = \u2212\nl\u22121\n\u03bdl\n\u03bdl\n\u03bdl\n\u0012\n\u0013\n\u2202 f\u02c6(\u03c4 )\n2(l \u2212 1)\n\u03c42 p2 zl(2 \u2212 a(z))\n=\u2212\n\u2212 +\n\u2202\u03bd\n\u03bdl\n\u03bd\na\u2032 (z)p(z)\n\u0012\n\u0013\n2(l \u2212 1) \u03c42 p2 za(z)(2 \u2212 a(z))\n\u2202 f\u02c6(\u03c4 )\n,\n=\u2212\n\u2212\n\u2202\u03c4\n\u03bdl\n\u03c4\na\u2032 (z)p(z)\nwhere for the standard regular ensemble z =\n\u01eb\u03bbR (xl )\n.\n\u03bb\n\n\u01eb\u03bb(xl )\n1\u2212\u01eb\u03bb(xl )\n\nwhereas for the Poisson\n\nregular ensemble z =\nProof. Let \u03c3 denote the fraction of degree-one check nodes, \u03c4i , i \u2265 2, the\nfraction of degree-i check nodes and \u03bd denote the fraction of residual variable\nnodes. Since the total edge count on the left and right must match up we have\n\u03c3\nof being\n\u03c3 + \u2211i=2 i\u03c4i = \u03bdl. A random edge therefore has probability q1 := \u03bdl\ni\u03c4i\nconnected to a degree-one check node and probability qi := \u03bdl of being connected\nto a degree-i check node, i \u2265 2. For large n, the joint probability distribution of\nall l edges emanating from a variable node converges to the product distribution.\nIt follows that (in this large blocklength limit) the probability distribution (for a\nrandomly chosen variable node) of having u1 connections into degree-one check\nnodes and u2 connections into degree-two check nodes is given by\n\u0012\n\u0013\nl\nw\u0303(u1 , u2 ) :=\nqu1 qu2 (1 \u2212 q1 \u2212 q2 )l\u2212u1 \u2212u2 .\nu1 , u2 , l \u2212 u1 \u2212 u2 1 2\nIn the iterative decoding process variables are not picked at random though. A\nvariable node is picked with a probability which is proportional to u1 . Therefore,\nthe induced probability distribution under iterative decoding is\nw(u1 , u2 ) =\n\nu1\nw\u0303(u1 , u2 )u1\n= w\u0303(u1 , u2 )\n,\nlq1\n\u2211u\u20321 ,u\u20322 w\u0303(u\u20321 , u\u20322 )u\u20321\n\nNote that the generating function of w(u, v) has the compact description\nW (x, y) :=\n\n\u2211 w(u1 , u2 )xu1 yu2 = x(xq1 + yq2 + (1 \u2212 q1 \u2212 q2))l\u22121 .\n\nu1 ,u2\n\n(4.15)\n\n\f26\n\nFinite-Length Scaling\n\nIn terms of W (x, y) we have\nf\u02c6(\u03c4 ) = \u2212\n\n\u2211 w(u1, u2 )u2 = \u2212\n\nu1 ,u2\n\n\u2202W (x, y)\n\u2202y\n\nx=y=1\n\n2\u03c42\n,\n= \u2212(l \u2212 1)q2 = \u2212(l \u2212 1)\n\u03bdl\n\n\u2202W (x, y)\n\u2212 f\u02c6(\u03c4 )\n\u2202x\nu1 ,u2\nx=y=1\n\u03c3\n= \u22121 \u2212 (l \u2212 1)q1 \u2212 f\u02c6(\u03c4 ) = \u22121 \u2212 (l \u2212 1) \u2212 f\u02c6(\u03c4 ) ,\n\u03bdl\n\u0010\n\u00112\n\u0010\n\u00112\n\u22022W (x, y)\n\u02c6(\u03c4 ) \u2212 f\u02c6(\u03c4 )\nf\u02c6(\u03c4 \u03c4 ) = \u2211 w(u1 , u2 )u22 \u2212 f\u02c6(\u03c4 ) =\n\u2212\nf\n\u2202y2\nu1 ,u2\nx=y=1\n!\n\u0010\n\u00112\n\u02c6(\u03c4 )\nf\n2\n(\n\u03c4\n)\n(\n\u03c4\n)\n(\n\u03c4\n)\n,\n1+\n= (l \u2212 1)(l \u2212 2)q2 \u2212 f\u02c6 \u2212 f\u02c6\n= \u2212 f\u02c6\nl\u22121\nf\u02c6(\u03c3) = \u2212\n\n\u2211 w(u1, u2 )(u1 \u2212 u2) = \u2212\n\n\u22022W (x, y)\nf\u02c6(\u03c3\u03c4 ) = \u2211 w(u1 , u2 )(u1 \u2212 u2 )u2 \u2212 f\u02c6(\u03c3) f\u02c6(\u03c4 ) =\n\u2202xy\nu,v\n\u0010\n\n= \u2212 f\u02c6(\u03c4 ) (1 + (l \u2212 2)q1) \u2212 f\u02c6(\u03c4 \u03c4 ) \u2212 f\u02c6(\u03c4 )\nf\u02c6(\u03c3\u03c3) =\n\n\u2211 w(u1 , u2)(u1 \u2212 u2)2 \u2212\n\nu1 ,u2\n\n=\n\n\u22022W (x, y)\n\u2202x2\n\nx=y=1\n\n\u0010\n\nf\u02c6(\u03c3)\n\n\u00112\n\n\u00112\n\nx=y=1\n\n\u0010\n\u00112\n\u2212 f\u02c6(\u03c4 \u03c4 ) \u2212 f\u02c6(\u03c4 ) \u2212 f\u02c6(\u03c3) f\u02c6(\u03c4 )\n\n\u2212 f\u02c6(\u03c3) f\u02c6(\u03c4 ) = f\u02c6(\u03c4 )\n\nf\u02c6(\u03c3) + 1\n1\u2212\nl\u22121\n\n!\n\n\u0010\n\u00112\n\u0010\n\u00112\n\u2212 f\u02c6(\u03c3\u03c4 ) \u2212 f\u02c6(\u03c3) f\u02c6(\u03c4 ) \u2212 f\u02c6(\u03c4 \u03c4 ) \u2212 f\u02c6(\u03c4 ) \u2212 f\u02c6(\u03c3) \u2212 f\u02c6(\u03c4 ) \u2212 f\u02c6(\u03c3)\n\n\u0010\n\u00112\n\u0010\n\u00112\n= (l \u2212 1)q1 (2 + (l \u2212 2)q1) \u2212 f\u02c6(\u03c3\u03c4 ) \u2212 f\u02c6(\u03c3) f\u02c6(\u03c4 ) \u2212 f\u02c6(\u03c4 \u03c4 ) \u2212 f\u02c6(\u03c4 ) \u2212 f\u02c6(\u03c3) \u2212 f\u02c6(\u03c4 ) \u2212 f\u02c6(\u03c3)\n\u0010\n\u00112\n\u0010\n\u0010\u03c3\n\u0011 \u03c3\nf\u02c6(\u03c4 )\n\u03c3\u0011\n.\n=\u2212\n\u2212 (l \u2212 1)\n\u22121\n\u2212 f\u02c6(\u03c4 ) 1 + 2\nl\u22121\n\u03bdl\n\u03bdl\n\u03bdl\nNext we need to determine the partial derivatives. From equation (4.14) for\ni = 2 we have\n\uf8eb\n\uf8f6\nz2\n(\n\u03c4)\n\u2202p\n\u02c6\n2 p(z) \u2202z\n2(l \u2212 1) \u2202\u03c42\n2(l \u2212 1) \uf8ed \u03c42\n\u2202f\n\uf8f8\n=\u2212\n=\u2212\n+\u03c4\n\u2202\u03c4\n\u03bdl\n\u2202\u03c4\n\u03bdl\n\u03c4\n\u2202z \u2202\u03c4\n\u0012\n\u0013\n2(l \u2212 1) \u03c42 p2 za(z)(2 \u2212 a(z))\n=\u2212\n.\n\u2212\n\u03bdl\n\u03c4\na\u2032 (z)p(z)\nThe remaining derivatives follow in the same way and we skip the details. Now\nnote that along the typical decoding trajectory all quantities required to compute\nthe above expressions are given by the density evolution equations (2.1) and (2.2).\nIt remains to establish the link between z and xl . We start with standard\n\n\f27\n\nFinite-Length Scaling\n\nensembles. From the density evolution equation (2.2)\n\u0010\n\u00112\n\u0001 \u0010 xr \u00112\n\u0001\nr\nxr\nr 2\nr\u22122\np\n2 1\u2212xr\n2\n1\u2212xr\n2 xr\u0001(1 \u2212 xr )\n\u03c4=\n\u03c42 =\n\u0011i \u03c4 = \u0010 x \u0011 \u03c4 .\n\u0010\nr i\n\u0001\nr\n\u2212i\nr\n\u2211i\u22652 i xr (1 \u2212 xr )\nxr\np 1\u2212xr r\n\u2211i\u22652 i 1\u2212x\nr\n\nxr\n= 1\u2212\u01eb\u03bb\u01eb\u03bb(x(xl ) ) .\nComparing this to equation (4.14) it follows that z = 1\u2212x\nr\nl\nRecall that in the Poisson case we can assume that r = 0, so that \u03c1(x) =\n\nR(x) = e\n\nx\u22121\nR\n\u03bb\n\n. Again from (2.2)\n\n\u03c42 =\n\nx2\nRr\n\n2(\n\n\u03bb)2\n\nR(1 \u2212 xr ) =\n\nfrom which it follows that for the Poisson case\n\np2\np\n\n\u0010\n\n\u0010\n\nRxr\n\n\u03bb\n\nRxr\n\n\u03bb\n\n\u00112\n\n\u0011 \u03c4,\n\n\u01eb\u03bb(xl )\n.\nz= R\n\u03bb\nFigure 12 depicts f\u02c6(\u03c3\u03c3) , f\u02c6(\u03c3\u03c4 ) and f\u02c6(\u03c4 \u03c4 ) as a function of \u03bd along the critical\ntrajectory (i.e., for the choice \u01eb = \u01eb\u2217 \u2248 0.4294) for the LDPC(n, x2 , x5 ) ensemble.\n\n1.8\n1.4\n1.0\n0.6\n0.2\n-0.2\n-0.6\n-1.0\n0.0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\nv\n\nFig. 12: The evolution of f\u02c6(\u03c3\u03c3) (dashed line), f\u02c6(\u03c3\u03c4 ) (dotted line) and f\u02c6(\u03c4 \u03c4 ) (solid line)\nalong the critical trajectory for the LDPC(n, x2 , x5 ) ensemble.\n\nThe last piece of information required to apply the strategy outlined in the\nprevious subsection, consists in determining the initial condition for the density\n\n\f28\n\nFinite-Length Scaling\n\nand covariance evolution. This is provided by the following lemmas, whose proof\nare fairly routine and therefore left to the reader.\nL EMMA 4.2. [Initial Condition for Standard Regular Ensembles] Consider\ntransmission over the channel BEC(n, n\u01eb) using a random element of LDPC(n, xl\u22121 , xr\u22121 ).\nConsider the residual graph (after reception of the transmitted word) and let Pinit (s,t)\ndenote the distribution of check nodes of degree one and of degree at least two,\nrespectively. Then\nPinit (s,t) = PGauss (s,t) (1 + O(1/n)),\nwhere PGauss (s,t) is a (discrete) Gaussian density with mean\n\nand covariance\n\n1\nE[s] = l\u01eb(1 \u2212 \u01eb)r\u22121 ,\nn\n\u0001\nl\n1\n1 \u2212 (1 \u2212 \u01eb)r \u2212 r\u01eb(1 \u2212 \u01eb)r\u22121 ,\nE[t] =\nn\nr\n\n1\nE[s; s] = l\u01eb\u01edr\u22121 (1 \u2212 \u01edr\u22122(1 + \u01eb((r \u2212 1)\u01eb \u2212 1)r)) ,\nn\n1\nE[s;t] = \u2212l\u01eb\u01edr\u22121(1 \u2212 \u01edr\u22122(1 + \u01eb((r \u2212 1)2\u01eb \u2212 1))),\nn\nl\u01edr\u22121\n1\nE[t;t] =\n(1 + (r \u2212 1)\u01eb \u2212 \u01edr\u22122(1 + \u01eb(2r \u2212 3 + (r \u2212 3)(r \u2212 1)\u01eb + (r \u2212 1)3\u01eb2 ))) .\nn\nr\nL EMMA 4.3. [Initial Condition for Regular Poisson Ensemble] A statement\nanalogous to Lemma 4.2 holds in the case of Poisson ensembles. For r = 0 the\ndistribution of s and t is again a (discrete) Gaussian with mean\n1\nE[s] = l\u01eb e\u2212l\u01eb ,\nn\n1\nE[t] = 1 \u2212 e\u2212l\u01eb \u2212 l\u01eb e\u2212l\u01eb ,\nn\nand covariance\n1\nE[s; s] = l\u01eb e\u2212l\u01eb \u2212 l\u01eb(1 \u2212 l\u01eb + l2\u01eb2 ) e\u22122l\u01eb ,\nn\n1\nE[s;t] = \u2212l\u01eb e\u2212l\u01eb + l\u01eb(1 + l2\u01eb2 ) e\u22122l\u01eb ,\nn\n1\nE[t;t] = (1 + l\u01eb) e\u2212l\u01eb \u2212 (1 + 2l\u01eb + l2\u01eb2 + l3 \u01eb3 ) e\u22122l\u01eb .\nn\nNote that, as one would expect, the random variables (s,t) are in general correlated.\nWe can now solve equations (4.6) and (4.7). This allows us to track the\nevolution of the probability distribution of s and t as v decreases from n\u01eb to 0,\nassuming that the s = 0 plane was not hit earlier.\n\n\f29\n\nFinite-Length Scaling\n\nE XAMPLE 2. [(3, 6)-Ensemble] Figure 13 shows the evolution of \u03b4 (ss) , \u03b4 (st) ,\nfor the LDPC(n, x2 , x5 ) ensemble for the choice \u01eb = \u01eb\u2217 \u2248 0.42944. Notice\nthat the variances of s and t can actually shrink as the decoding process evolves.\nThis is an effect of the term in square brackets in equation (4.7). In particular the\nvariance shrinks to 0 at \u03bd = 0 if \u01eb is low enough (whenever decoding is successful\nwith high probability). Finally, the parameter \u03b1 is given by equation (4.13), where\n\u03b4 (tt)\n\n0.06\n0.05\n0.04\n0.03\n0.02\n0.01\n0.0\n-0.01\n-0.02\n-0.03\n-0.04\n-0.05\n0.2\n\n0.22\n\n0.24\n\n0.26\n\n0.28\n\n0.3\n\n0.32\n\n0.34\n\n0.36\n\n0.38\n\n0.4\n\n0.42 v\n\nFig. 13: The evolution of \u03b4 (ss) (dashed line), \u03b4 (st) (dotted line) and \u03b4 (tt) (solid line) for the\nLDPC(n, x2 , x5 ) ensemble and the choice \u01eb = \u01eb\u2217 \u2248 0.42944.\n\nthe first factor can be computed as in equation (3.1).\nIn Table (4.2) we report the values of \u01eb\u2217 , \u03b1, and \u03b2 for a few regular standard ensembles. Further explanations concerning the parameter \u03b2 are provided in\nSection 5.\nThe computation of the scaling parameters \u03b1 = \u03b1(l, r) and \u03b2 = \u03b2(l, r) for\nthe Poisson case are made easier by the following pleasing relationship.\nL EMMA 4.4. [Scaling of Erasure Probability for Poisson Ensembles] Consider transmission over BEC(n, n\u01eb) using elements from the regular Poisson ensemble LDPC(n, xl\u22121 , r). For l fixed and (n, r, \u01eb) and (n\u2032 , r\u2032 , \u01eb\u2032 ) such that n\u01eb = n\u2032 \u01eb\u2032\nand (1 \u2212 r)n = (1 \u2212 r\u2032 )n\u2032 ,\nELDPC(n,xl\u22121 ,r) [PB (G, n\u01eb)] = ELDPC(n\u2032 ,xl\u22121 ,r\u2032 ) [PB (G, n\u2032 \u01eb\u2032 )] ,\nnELDPC(n,xl\u22121 ,r) [Pb (G, n\u01eb)] = n\u2032 ELDPC(n\u2032 ,xl\u22121 ,r\u2032 ) [Pb (G, n\u2032 \u01eb\u2032 )] .\nProof. We start with the statement regarding the block erasure probability. Compare transmission over BEC(n, n\u01eb) using elements from LDPC(n, xl\u22121 , r)\nto transmission over BEC(n\u2032 , n\u2032 \u01eb\u2032 ) using elements from LDPC(n\u2032 , xl\u22121 , r\u2032 ). The\ncondition n\u01eb = n\u2032 \u01eb\u2032 implies that the number of erased bits is the same in both\n\n\f30\n\nFinite-Length Scaling\n\nl\n3\n3\n3\n4\n4\n5\n6\n6\n\nr\n4\n5\n6\n5\n6\n6\n7\n12\n\n\u01eb\u2217\n0.6473\n0.5176\n0.4294\n0.6001\n0.5061\n0.5510\n0.5079\n0.3075\n\n\u03b1\n0.260115\n0.263814\n0.249869\n0.241125\n0.246776\n0.228362\n0.280781\n0.170218\n\n\u03b2/\u03a9\n0.593632\n0.616196\n0.616949\n0.571617\n0.574356\n0.559688\n0.547797\n0.506326\n\nTable 1: Thresholds and scaling parameters for some regular standard ensembles. The\nshift parameter is given as \u03b2/\u03a9 where \u03a9 is the universal constant stated in equation\n(5.16) whose numerical value is very close to 1.\n\ncases. Decoding fails if these erased bits contain a stopping set. The condition\n(1 \u2212 r)n = (1 \u2212 r\u2032 )n\u2032 implies that the two ensembles have the same number of\ncheck nodes. Together with the fact that l is the same in both cases (and therefore\nthe involved number of edges is the same) this shows that the erasure probability\nis the same.\nThe proof regarding the bit erasure probability is almost identical. Both\ndecoders get stuck in identical constellations. The factor n takes into account\nwhat fraction of the overall codeword this constellation is.\nIf we combine the above relationship with the general form of the scaling\nlaw, cf. equations (1.2) and (1.3) as well as Lemma 3.1, we get the following\nscaling relations.\nL EMMA 4.5. [Scaling of Scaling Parameters] Consider transmission over\nBEC(n, n\u01eb) using elements of the Poisson ensemble LDPC(n, xl\u22121 , r) with threshold \u01eb\u2217 (l, r). Assume that the scaling (1.3) holds and let \u03b1(l, r) and \u03b2(l, r) denote\nthe corresponding variance and shift parameters. Then\n1 \u2212 r\u2032\n,\n1\u2212r\n\u0013\n\u0012\n1 \u2212 r\u2032 1/2\n,\n\u03b1(l, r\u2032 ) = \u03b1(l, r)\n1\u2212r\n\u0013\n\u0012\n1 \u2212 r\u2032 1/3\n\u2032\n.\n\u03b2(l, r ) = \u03b2(l, r)\n1\u2212r\n\n\u01eb\u2217 (l, r\u2032 ) = \u01eb\u2217 (l, r)\n\n(4.16)\n(4.17)\n(4.18)\n\nProof. The proof is elementary and we leave it to the reader. We note that in\norder to prove (4.16) and (4.17) only the simplified form of the scaling law (1.2)\nis required as hypothesis and that this scaling law is proved in Lemma 3.1.\nFrom the above observations it follows that we have to determine the parameters \u01eb\u2217 (l, r), \u03b1(l, r) and \u03b2(l, r) only for one rate r. This is the reason why so far\nwe have only considered Poisson ensembles of zero rate. Our results will depend\n\n\f31\n\nFinite-Length Scaling\n\nl\n3\n4\n5\n6\n7\n8\n9\n10\n\n\u01eb\u2217\n0.818469\n0.772280\n0.701780\n0.637081\n0.581775\n0.534997\n0.495255\n0.461197\n\n\u03b1\n0.497867\n0.409321\n0.375892\n0.354574\n0.337788\n0.323501\n0.310948\n0.299739\n\n\u03b2/\u03a9\n0.964528\n0.827849\n0.760593\n0.713490\n0.676647\n0.646335\n0.620646\n0.598429\n\nTable 2:\nThresholds and scaling parameters for some Poisson ensembles\nLDPC(n, xl\u22121 , r). Note that these parameters assume that r = 0. Parameters for a\ngeneric rate can be obtained from these parameters through equations. (4.16)-(4.18). The\nshift parameter is given as \u03b2/\u03a9 where \u03a9 is the universal constant stated in (5.16) whose\nnumerical value is very close to 1.\n\nonly on l. Relations (4.16)-(4.18) can be used to reintroduce the dependence\nupon r.\n5. Computation of the Shift Parameter. In this section we explain in greater\ndetail the arguments for Conjecture 3.1, and the procedure for computing the shift\nparameter \u03b2. As in the previous section, we shall first discuss this issue in an\nabstract setting, cf. Section 5.1. The general procedure will then be applied to\nregular standard and Poisson ensembles in Section 5.2.\n5.1. The General Approach. Let us reconsider the setting of Section 4.1,\ni.e., a family of Markov chains Xn,0 , Xn,1 , . . . , Xn,t , . . . taking values in Zd+1 and\nparametrized by the (large) integer n. As before we will drop in the sequel the\nsubscript n to mitigate the notational burden. Throughout this section we shall\nassume the hypotheses of Proposition 4.1 to be fulfilled. Unlike in Section 4.1,\nwe are interested in paths X0t \u2261 {X0 , X1 , . . . , Xt } which are confined to the 'half\nspace':\nH+ \u2261 {x = (x(0) , . . . , x(d) ) \u2208 Zd+1 : x(0) > 0} .\n\n(5.1)\n\nWe would like to estimate the 'survival' probability\nPt \u2261 P(X0t \u2286 H+ ) .\n\n(5.2)\n\nNotice that Pt depends implicitly on the initial condition X0 = x0 \u2208 H+ . The\n(0)\ncoordinate Xt should be thought as (an abstraction of) the number s of degreeone check nodes in the analysis of iterative decoding, cf. Section 3. The survival\nprobability Pt is therefore the probability of not having encountered a stopping\nset after t steps of the decoding process. We are interested in a time window\nof length O(n). Without loss of generality we may fix \u03c4max > 0 and consider\nt \u2208 {0, . . . ,tmax } with tmax = \u230an\u03c4max \u230b.\n\n\f32\n\nFinite-Length Scaling\n\nWe shall denote by z(\u03c4 ) the 'critical trajectory', i.e. a solution of the density\nevolution equations (4.6), such that z(0) (\u03c4 \u2217 ) = 0, and z(0) (\u03c4 ) > 0 for any \u03c4 \u2208\n[0, \u03c4max ], \u03c4 6= \u03c4 \u2217 . We call z0 = z(0) the corresponding initial condition. In order\nto make contact with the application to iterative decoding, we shall make the\nfollowing assumptions.\n\u221a\nA. As n \u2192 \u221e, we have x0 = n z0 + n z1 + O(1), with z1 \u2208 Rd+1 independent\nof n. This corresponds to the erasure probability \u01eb being in the critical\nwindow \u01eb\u2217 \u2212 \u01eb = O(n\u22121/2 ).\nB. Let zu (\u03c4 ), u \u2208 Rd+1 , be a 'perturbed' critical trajectory obtained by solving the density evolution equations (4.6) with initial condition zu (\u03c4 \u2217 ) =\nz(\u03c4 \u2217 ) + u. As for the critical trajectory, we consider this solution in the\ninterval [0, \u03c4max ] and take u such that |u| < \u03b5 with \u03b5 small enough. We\nassume that there exist a positive u-independent constant \u03ba1 , and a function u 7\u2192 a(u) such that\n(0)\n\n(0)\n\nzu (\u03c4 ) \u2212 zu (\u03c4 \u2217 ) \u2265 a(u)(\u03c4 \u2212 \u03c4 \u2217 ) + \u03ba1(\u03c4 \u2212 \u03c4 \u2217 )2\nfor any \u03c4 \u2208 [0, \u03c4max ].\nC. We finally assume that a(u) can be chosen in such a way that |a(u)| <\n\u03ba2 |u| for some positive constant \u03ba2 .\nNotice that the assumptions B and C above can be easily checked on the 'conb (\u2206|z) introduced in Sec. 4.1. The situation considered\ntinuum' transition rates W\nhere mimics the one found in iterative decoding of unconditionally stable ensembles.\nConsider the survival probability Ptmax at the 'latest' time. As we\u221ahave seen\nin Section 4.1, most of the trajectories X0tmax are concentrated within n around\n(0)\n\nnz(t/n). Therefore the absolute minimum of Xt in the interval {0, . . . ,tmax }\nwill be realized for a t 'close' to n\u03c4 \u2217 . If this absolute minimum is positive, the\ncorresponding trajectory contributes to Ptmax , otherwise it does not.\nIn order to formalize this argument, fix t \u2217 = \u230an\u03c4 \u2217 \u230b. Then\nPtmax =\n\n\u2211\n\nx\u2208H+\n\nP(X0tmax \u2286 H+ |Xt \u2217 = x) P(Xt \u2217 = x) .\n\n(5.3)\n\nThanks to Proposition 4.1 we can accurately estimate the factor P(Xt \u2217 = x). The\n(0)\nterm P(X0tmax \u2286 H+ |Xt \u2217 = x) is the probability that the global minimum of Xt ,\nt \u2208 {0 . . .tmax }, is positive conditioned on Xt \u2217 = x. Let us denote by tg a 'time'\nfor which the global minimum is realized. More precisely, tg \u2208 {0 . . .tmax } is\n(0)\n\n(0)\n\na random variable such that Xtg \u2264 Xt for all t \u2208 {0 . . .tmax }. Call zX (\u03c4 ) the\nperturbed critical trajectory defined above with perturbation vector \u221a\nu = Xt \u2217 /n \u2212\nz(\u03c4 \u2217 ). In other words, we perturbe the critical trajectory by an O(1/ n) amount\nin order to match it to the particular (finite n) realization of the Markov process we\nare dealing with within the critical region. Concentration arguments, analogous\nto the ones used to prove the point I of Proposition 4.1, imply that, for a given t:\nn\no\np\n2\nP |Xt \u2212 nzX (t/n)| \u2265 \u03b4 |t \u2212 t \u2217 | \u2264 \u03a91 e\u2212\u03a92 \u03b4 ,\n\n\f33\n\nFinite-Length Scaling\n\nX (0)\n\ntg\n\nt\u2217\n\nO(n1/3 )\n\nO(n2/3 )\n\nt\n\nFig. 14: A pictorial view of decoding trajectories near the critical point. The type of\ntrajectory depicted here is responsible for the shift appearing in the refined scaling form\n(1.3).\n\nfor some positive constants \u03a91 and \u03a92 (as before we use this symbols to denote\ngeneric constants which are proven to exist independent of n). In fact a stronger\ncondition holds true: by Doob's maximal inequality [16, p. 227], for T fixed\n\u001a\n\u001b\n\u221a\n2\nP max |Xt \u2212 nzX (t/n)| \u2265 \u03b4 T \u2264 \u03a91 e\u2212\u03a92 \u03b4 ,\n(5.4)\n|t\u2212t \u2217 |\u2264T\n\nfor some (possibly different) constants \u03a91 and \u03a92 . Using this fact we can prove\nan useful result:\nL EMMA 5.1. Assume the same hypotheses as in Lemma 4.1 plus A, B and\n(0)\nC above. Let tg be a time at which the absolute minimum of Xt is realized, for\nt \u2208 {0 . . .tmax }. Then there exist positive constants \u03a91 , \u03a92 and \u03b40 , and a function\nn0 (\u03b4) such that, for any \u03b4 > \u03b40 and n > n0 (\u03b4)\no\nn\n(0)\n(0)\nP |tg \u2212 t \u2217| \u2264 \u03b4 2/3 n2/3 , Xtg \u2265 Xt \u2217 \u2212 \u03b4 4/3 n1/3 \u2265 1 \u2212 \u03a91 exp[\u2212\u03a92 \u03b4 2 ] . (5.5)\nThe proof is deferred to Appendix C. The content of this lemma is illustrated in\nFig. 14.\nThe above result implies that corrections to the simplified scaling of Lemma\n3.1 can be estimated through a two step procedure. In a nutshell: (i) Compute the\n(0)\n(0)\nprobability for Xt \u2217 to be of order n1/3 ; (ii) Evaluate the probability for Xtg to be\n(0)\n\npositive, conditioned on a given Xt \u2217 of order n1/3 .\n5.1.1. Distribution of Xt \u2217 . The simplified scaling form, cf. Lemma 3.1,\nwas obtained by approximating the first factor in equation (5.3) by 1. The leading\n(0)\ncorrection to this approximation comes from trajectories such that Xt \u2217 = O(n1/3 ).\n\n\f34\n\nFinite-Length Scaling\n(0)\n\nBecause of Proposition 4.1, the probability distribution\n\u221a of Xt \u2217 (second factor) is\nwell approximated by a Gaussian with center at O( n) and variance of order n.\n(0)\nThe probability of having Xt \u2217 = O(n1/3 ) is therefore of order n1/3 *n\u22121/2 = n\u22121/6.\nThis explains why the correction term in the refined scaling form (1.3) is of order\nn\u22121/6 .\nThis argument can be made more precise by rewriting equation (5.3) as\n(0)\n\nPtmax = P(Xt \u2217 > 0) \u2212\n\n\u2211\n\n(0)\n\nP(Xtg < 0|Xt \u2217 = x) P(Xt \u2217 = x) .\n\n(5.6)\n\nx\u2208H+\n\nThe first term corresponds to the simplified scaling form. We shall hereafter focus\n(0)\n(0)\non the second one, Pcorr \u2261 P(Xt \u2217 > 0) \u2212 Ptmax . Notice that P(Xtg < 0|Xt \u2217 = x)\nvaries much more rapidly (on a scale of order n1/3 ) in x(0) than in the other coordinates (on a scale of order n). It is therefore useful to introduce the notation\n~x = (x(1) . . . x(d) ) (and analogously ~X and~z) which distinguish explicitly the last d\ncoordinates of x. Since P(Xt \u2217 = x) varies on a scale n1/2 , we can safely approximate it by setting the coordinate x(0) to 0:\n(\n)\n\u0010\n\u0011\n(0)\n(0)\nPcorr = \u2211 \u2211 P Xtg < 0|Xt \u2217 = (x ,~x)\nP (Xt \u2217 = (0,~x)) (1 + O(n\u22121/6)) .\n~x\n\nx(0) >0\n\nThe term in curly brackets depends on~x only through the transition coefficients in\na neighborhood of ~x and varies therefore on a scale of order n. This point will be\ndiscussed in detail in the next section. \u221aOn the contrary P (Xt \u2217 = (0,~x)) is peaked\naround n~z(t \u2217 /n) with a width of order n. Therefore\n\u0011\n\u0010\n\u0011 \u0010\n(0)\n(0)\nPcorr = \u2211 P Xtg < 0|Xt \u2217 = (x(0) , n~z(\u03c4 \u2217 )) P Xt \u2217 = 0 (1 + O(n\u22121/6)) ,\nx(0) >0\n\n(5.7)\n\nwhere we recall that ~z(\u03c4 \u2217 ) denotes the last d coordinates of the critical point.\nThe second factor can be evaluated easily using density and covariance evolution.\nLet us consider the application to iterative decoding (here X (0) \u2261 s). Note that\nat the critical point and within the critical window X (0) is Gaussian with mean\n\u2202\u03c3\n\u2217\n\u2202\u01eb (\u01eb \u2212 \u01eb )n and variance \u03b4\u03c3\u03c3 n. We therefore have\nP\n\n\u0010\n\n(0)\nXt \u2217\n\n\u001b\n\u001a\nn(\u01eb\u2217 \u2212 \u01eb)2\n(1 + O(n\u22121/2)) .\nexp \u2212\n= 0 = \u2202\u03c3 \u221a\n2\n2\u03b12\n2\u03c0n\u03b1\n\u2202\u01eb\n\u0011\n\n1\n\nThis formula can indeed be guessed without any computation at all. The proba(0)\nbility of Xt \u2217 = 0 must be in fact proportional to the derivative of the probability\n(0)\n\nof having Xt \u2217 \u2264 0, which is given by equation (1.2) within the critical window.\n5.1.2. Distribution of the Global Minimum. We are left with the task of\nestimating the first factor in equation (5.7), and more generally the probability\n\n\f35\n\nFinite-Length Scaling\n(0)\n\ndistribution of Xtg conditioned on Xt \u2217 . Lemma 5.1 is, once again, quite helpful.\nThe difference |tg \u2212 t \u2217 | is small on the scale n on which the transition rates are\nstate-dependent. This suggests that the leading correction to the simplified scaling\ndepends on the transition rates only through their behavior at the critical point\nz(\u03c4 \u2217 ). On the other hand, |tg \u2212 t \u2217 | is large on the scale O(1) of a single step.\nWe can therefore hope to compute the leading correction within a 'continuum'\napproach.\nMore precisely, define the rescaled trajectory u(*) \u2208 Rd+1 by taking\n(0)\n\nu(0) (n\u22122/3 (t \u2212 t \u2217 )) \u2261 n\u22121/3 Xt\n(i)\n\nu (n\n\n\u22122/3\n\n\u2217\n\n(t \u2212 t )) \u2261 n\n\n\u22122/3\n\n,\n\n(i)\n(i)\n(Xt \u2212 Xt \u2217 ) i\n\n(5.8)\n= 1, * * * , d,\n\n(5.9)\n\nfor integers t such that |t \u2212t \u2217 | \u2264 \u03b8MAX n2/3 , and interpolating linearly among these\npoints. A textbook result in the theory of stochastic processes [22] implies the\nfollowing lemma.\nL EMMA 5.2. Let X be distributed as above under the condition Xt \u2217 = (n1/3\u03b6, n~z(\u03c4 \u2217 )).\nThe process u(*) defined in equations (5.8) and (5.9) converges as n \u2192 \u221e to a diffusion process with generator:\n!\nd\nd\n1 (00) \u22022\n(i) \u2202\n\u2217 (i) \u2202\nf\n\u2212\n+\n,\n(5.10)\nf\u2217\nLd = \u2212 \u2211 \u03c9i u\n\u2217\n\u2211\n\u2202u(0) i=1\n\u2202u(i) 2\n\u2202(u(0) )2\ni=1\nconditioned on u(0) (0) = \u03b6, and ~u(0) = ~0. In the above formula we used the\nnotation\n(i)\nf\u2217 = f\u02c6(i) (z(\u03c4 \u2217 )),\n\n(i j)\n\nf\u2217\n\n\u2202 f\u02c6(0)\n= f\u02c6(i j) (z(\u03c4 \u2217 )), \u03c9i\u2217 =\n\u2202zi\n\n.\nz(\u03c4 \u2217 )\n\nIn order not to burden the presentation, the proof of this statement is postponed\nto App. D. Notice that the only role of \u03b8MAX in the above lemma is to assure that\nu(\u03b8) stays within a finite neighborhood of u(0) with high probability. We want\nto use the process u(\u03b8) in order to compute the second factor in equation (5.7)\nand therefore the distribution of the absolute minimum of u(\u03b8). Let us call \u03b8g the\nlocation of the minimum. Lemma 5.1 implies that |\u03b8g | < \u03b4 4/3 with probability at\nleast 1 \u2212 \u03a91 exp(\u2212\u03a92 \u03b4 2 ). We can therefore safely let \u03b8MAX \u2192 \u221e and consider the\ndiffusion process defined above for \u03b8 \u2208 (\u2212\u221e, +\u221e).\nNotice that only the first derivative with respect to the coordinates u(1) , . . . , u(d)\nappears in equation (5.10). The process ~u(\u03b8) is therefore deterministic: u(i) (\u03b8) =\n(i)\nf\u2217 \u03b8 for i = 1, . . . , d. We can substitute this behavior in equation (5.10) and deduce that u(0) (\u03b8) is a time-dependent diffusion process with generator\n!\nd\n\u2202\n\u2202\n1\n\u2217 (i)\nL0 (\u03b8) = \u2212 \u2211 \u03c9i f\u2217 \u03b8 (0) + f\u2217(00) (0) 2 .\n(5.11)\n2\n\u2202u\n\u2202(u )\ni=1\n\n\f36\n\nFinite-Length Scaling\n\nIt is convenient to rescale u(0) and \u03b8 in order to reduce the above generator to a\nstandard form:\n!1/3\n!2/3\nd\n\n\u2211 \u03c9i\u2217 f\u2217\n\n(00) \u22121/3\n\n)\n\n\u03b8 = ( f\u2217\n\n(i)\n\n\u03b8,\n\ni=1\n\n(00) \u22122/3\n\nw = ( f\u2217\n\n)\n\nd\n\n\u2211 \u03c9i\u2217 f\u2217\n\n(i)\n\nu(0)\n(5.12)\n.\n\ni=1\n\nThe generator for w(\u03b8) has now the form (we keep the same name with an abuse\nof notation)\n\nL0 (\u03b8\u0304) = \u2212\u03b8\u0304\n\n\u2202\n1\u2202\n+\n.\n\u2202w 2 \u2202w2\n\n(5.13)\n2\n\nA little thought shows that this is equivalent to saying that w(\u03b8) = w(0) + \u03b8 /2 +\nB(\u03b8) with B(\u03b8) a two-sided standard Brownian motion with B(0) = 0. The problem of computing the distribution of the global minimum of such a process has\nbeen solved in [10]. Adapting the results of this paper we find\n\u0001\n(5.14)\nP w(\u03b8g ) \u2212 w(0) < \u2212z = 1 \u2212 K(z)2 ,\nwhere\n\nK(z) =\n\n1\n2\n\nZ\n\nAi(iy)Bi(21/3 z + iy) \u2212 Ai(21/3 z + iy)Bi(iy)\ndy .\nAi(iy)\n\n(5.15)\n\nwith Ai(*) and Bi(*) the Airy functions defined in [1].\nPutting everything together we get our final result\n\n\u2211\n\nx(0) >0\n\nP\n\n\u0010\n\n(0)\nXtg\n\n< 0|Xt \u2217 = (x\n\n(0)\n\n\u0011\n(00)\n, n~z(t /n)) = n1/3 \u03a9 ( f\u2217 )2/3\n\u2217\n\nd\n\n\u2211\n\ni=1\n\n(i)\n\u03c9i\u2217 f\u2217\n\n!\u22121/3\n\n(1 + o(1)) ,\n\nwith\n\u03a9\u2261\n\nZ \u221e\n0\n\n[1 \u2212 K(z)2 ] dz .\n\n(5.16)\n\nA numerical computation yields \u03a9 = 1.00(1).\n5.2. Application to Regular Standard and Poisson Ensembles. There is\none important difficulty in applying the general scheme explained above to iterative decoding: the Markov process is not defined for s < 0. Recall that s corre(0)\nsponds, in this context, to the 'critical' variable Xt . On the other hand, both the\n(i)\n(i\nj)\n\u02c6\n\u02c6\ndrift and diffusion coefficients f (*) and f (*) can be continued analytically\nthrough the s = 0 plane. Since the final result (5.16) depends on the transition\nrates only through these quantities, we are quite confident that it remains correct\nalso for iterative decoding applications.\nC ONJECTURE 5.1. [Shift Parameter for Regular Standard Ensembles] Consider the regular standard ensemble LDPC(n, xl\u22121 , xr\u22121 ) or the regular Poisson\n\n\f37\n\nFinite-Length Scaling\n\nensemble LDPC(n, xl\u22121 , r). Then\n\u03b2/\u03a9 = \u2212( f\n\n(\u03c3\u03c3) 2/3\n\n)\n\n\"\n\n\u2202 f (\u03c3 ) \u2202 f (\u03c3 ) (\u03c4 )\n\u2212\n+\nf\n\u2202\u03bd\n\u2202\u03c4\n\n#\u22121/3 \u0012\n\n\u2202\u03c3\n\u2202\u01eb\n\n\u0013\u22121\n\n.\n\n(5.17)\n\nFor the regular standard ensemble LDPC(n, xl\u22121 , xr\u22121 ) define\n\u0001\n\u0001\nr\n2 r2 xi x\u0304r\u2212i\n\u2211ri=2 i xi x\u0304r\u2212i i\nh(x) = (l \u2212 1) r r\u0001 i r\u2212i\ng(x) = r r\u0001 i r\u2212i ,\n\u2211i=2 i x x\u0304\n\u2211i=2 i x x\u0304 i\n\nThen\n\n\u03b2/\u03a9 =\n\n\u0012\n\n\u2202\u03c3\n\u2202\u01eb\n\n\u0013\u22121 \u0012\n\nl\u22122\nl\u22121\n\n\u00132/3 \u0012\n\nh\u2032 (z)g(z) \u2212 lh\u2032(z)\n\u03c4 g\u2032 (z)\n\n\u0013\u22121/3\n\n,\n\nwhere z = \u01ebxl\u22121 and all parameters are taken at the critical point.\nThe generic equation (5.17) follows directly from equation (5.16), applied\nto the iterative decoding setting. For regular standard ensembles these expressions can be made somewhat more explicit. First we note that at the critical point\nf (\u03c3) = \u22121 \u2212 f (\u03c4 ) since with probability approaching one (as n tends to infinity)\nthe variable node which is pealed off has (only) one check node of degree one attached to it.6 Since f (\u03c3) = 0 at the critical point it follows that f (\u03c4 ) = \u22121. Using\n\u22122\nagain the relationship f (\u03c3) = \u22121 \u2212 f (\u03c4 ) some calculations show that f (\u03c3\u03c3) = ll\u22121\nand that\n\n\u2202 f (\u03c3)\n\u2202\u03bd\n\nand\n\n\u2202 f (\u03c3)\n\u2202\u03c4\n\ncan be expressed as indicated.\nREFERENCES\n\n[1] M. A BRAMOWITZ AND I. A. S TEGUN, Handbook of mathematical functions, Nat. Bur. Stand.\n55, Washington, 1964.\n[2] A. A MRAOUI , A. M ONTANARI , T. R ICHARDSON , AND R. U RBANKE, Finite-length scaling for iteratively decoded ldpc ensembles, in Proc. 41th Annual Allerton Conference on\nCommunication, Control and Computing, Monticello, IL, 2003.\n[3]\n, Finite-length scaling for iteratively decoded ldpc ensembles. To be presented at the\nIEEE International Symposium on Information Theory, Chicago, Illinois, June-July 2004,\n2004.\n[4] B. B OLLOB \u00c1S , Random Graphs, Cambridge studies in advanced mathematics, 2001.\n[5] V. E. B RITIKOV, The asymptotic number of forests from unrooted trees, Math. Notes, 43\n(1988), pp. 387\u2013394.\n[6] C. D I , D. P ROIETTI , T. R ICHARDSON , E. T ELATAR , AND R. U RBANKE, Finite length analysis of low-density parity-check codes on the binary erasure channel, IEEE Trans. Inform.\nTheory, 48 (2002), pp. 1570\u20131579.\n[7] C. D I , T. R ICHARDSON , AND R. U RBANKE, Weight distribution of iterative coding systems:\nHow deviant can you be?, in International Symposium on Information Theory, Washington, D.C., June 2001, IEEE, p. 50.\n[8] M. E. F ISHER, in Critical Phenomena, International School of Physics Enrico Fermi, Course\nLI, edited by M. S. Green, (Academic, New York, 1971).\n6 This\n\nis true since at this point the number of degree-one check nodes is sublinear.\n\n\f38\n\nFinite-Length Scaling\n\n[9] M. I. F RIEDLIN AND A. D. W ENTZELL, Random Perturbations of Dynamical Systems,\nSpringer-Verlag, New York, 1984.\n[10] P. G ROENEBOOM, Brownian motion with a parabolic drift and airy functions, Probab. Th. Rel.\nFields, 81 (1989), pp. 79\u2013109.\n[11] V. F. K OLCHIN, Random Graphs, Cambridge University Press, 1999.\n[12] F. K SCHISCHANG , B. F REY, AND H.-A. L OELIGER, Factor graphs and the sum-product\nalgorithm, IEEE Transactions on Information Theory, 47 (2001), pp. 491\u2013519.\n[13] J. L EE AND R. B LAHUT, Bit error rate estimate of finite length turbo codes, in Proceedings of\nICC'03, Anchorage Alaska, USA, May 11\u201315 2003, p. 2728.\n[14] M. L UBY, M. M ITZENMACHER , A. S HOKROLLAHI , AND D. S PIELMAN, Efficient erasure\ncorrecting codes, IEEE Trans. Inform. Theory, 47 (2001), pp. 569\u2013584.\n[15] M. L UBY, M. M ITZENMACHER , A. S HOKROLLAHI , D. S PIELMAN , AND V. S TEMANN,\nPractical loss-resilient codes, in Proceedings of the 29th annual ACM Symposium on\nTheory of Computing, 1997, pp. 150\u2013159.\n[16] C. M C D IARMID, Concentration, in Probabilistic Methods for Algorithmic Discrete Mathematics, M. Habid, C. McDiarmid, R. Ramirez-Alfonsin, and B. Reed, eds., no. 16 in\nAlgorithms and Combinatorics, Springer, Berlin, 1998, pp. 195\u2013248.\n[17] A. M ONTANARI , Finite-size scaling of good codes, in Proc. 39th Annual Allerton Conference\non Communication, Control and Computing, Monticello, IL, 2001.\n[18] V. P RIVMAN, Finite Size Scaling and Numerical Simulation of Statistical Systems, (World\nScientific, Singapour, 1990).\n[19] T. R ICHARDSON , A. S HOKROLLAHI , AND R. U RBANKE, Error-floor analysis of various lowdensity parity-check ensembles for the binary erasure channel. To be presented at ISIT'02\nin Lausanne, 2002.\n, Finite-length analysis of various low-density parity-check ensembles for the binary\n[20]\nerasure channel, in IEEE International Symposium on Information Theory, Lausanne,\nSwitzerland, June 30\u2013July 5 2002, p. 1.\n[21] T. R ICHARDSON AND R. U RBANKE, Finite-length density evolution and the distribution of\nthe number of iterations for the binary erasure channel. in preparation, 2003.\n[22] S. R. S. VARADHAN, Lecure Notes on Stochastic Processes, 2000.\nAvailable at\nhttp://www.math.nyu.edu/faculty/varadhan/.\n[23] H. S. W ILF , Generatingfunctionology, Academic Press, 2 ed., 1994.\n[24] G. Z EMOR AND G. C OHEN, The threshold probability of a code, IEEE Trans. Inform. Theory,\n41 (1995), pp. 469\u2013477.\n[25] J. Z HANG AND A. O RLITSKY, Finite-length analysis of LDPC codes with large left degrees,\nin IEEE International Symposium on Information Theory, Lausanne, Switzerland, June\n30\u2013July 5 2002, p. 3.\n\nAPPENDIX\nA. Covariance Evolution for a General Markov Process. In this Section\nwe reconsider the abstract setting of Section 4.7 and outline a proof of Proposition\n4.1 under the assumptions 1-3.\nProof. We start with statement I, whose proof is fairly standard. Define a\nDoob's Martingale Xb0 , . . . , Xbt ,\n(i)\n\nNote that Xbt = Xt\n\n(i)\nXbs = E[Xt |X0 , . . . Xs ] .\n(i)\n\n(i)\n\nand Xb0 = E[Xt ] = X t so that\n\n(i)\n(i)\nP{|Xt \u2212 X t | \u2265 \u03c1} = P{|Xbt \u2212 Xb0 | \u2265 \u03c1}.\n\n\f39\n\nFinite-Length Scaling\n\nTherefore, by the Hoeffding-Azuma inequality we will have proven (4.8) if we\ncan show that Xb0 , . . . , Xbt has bounded differences, more specifically, if we can\nshow that\np\nbs\u22121| \u2264 \u03a90 , 1 \u2264 s \u2264 t.\n|Xbs \u2212 X\nTo accomplish this task note that\n\n(i)\n\n(i)\n\n|Xbs \u2212 Xbs\u22121| \u2264 sup |E[Xt |X0 . . . Xs\u22121 , Xs = y] \u2212 E[Xt |X0 . . . Xs\u22121 , Xs = z]| , (A.1)\ny,z\n\nwhere the sup is taken over all the y and z such that the trajectories {X0 , . . . Xs\u22121 , Xs =\ny} and {X0 , . . . Xs\u22121 , Xs = z} have non-vanishing probability. Consider therefore\ntwo realizations of the Markov chain which coincide up to time s \u2212 1 but are independent afterwards. Denote them by X0 , X1 , . . . and Y0 ,Y1 , . . . , respectively, where\nby our assumption X\u03c4 = Y\u03c4 for 0 \u2264 \u03c4 \u2264 s \u2212 1, but the processes evolve indepen(i)\n(i)\n(i)\n(i)\ndenly for \u03c4 \u2265 s. Since by assumption |Xs \u2212 Xs\u22121 | \u2264 \u03ba1 and |Ys \u2212Ys\u22121 | \u2264 \u03ba1 al(i)\n\n(i)\n\nmost surely it follows that |Xs \u2212Ys | \u2264 2\u03ba1 almost surely. Define \u03b4X\u03c4 = X\u03c4 \u2212Y\u03c4\nand \u03b4X \u03c4 = X \u03c4 \u2212 Y \u03c4 . Then we have for s \u2264 \u03c4 < t\n(i)\n\n(i)\n\n\u03b4X \u03c4 +1 \u2264 \u03b4X \u03c4 + E[| f (i) (X\u03c4 ) \u2212 f (i) (Y\u03c4 )|] \u2264\nA (i) B\n(i)\n\u2264 \u03b4X \u03c4 + \u03b4X \u03c4 + .\nn\nn\nHere we approximated f (i) (X\u03c4 ) \u2212 f (i) (Y\u03c4 ) by f\u02c6(i) (X\u03c4 /n) \u2212 f\u02c6(i) (Y\u03c4 /n) and then\nused the fact that f\u02c6(i) (z) has bounded derivative. By Gronwall's Lemma we now\n\u221a\n(i)\n(i)\n(i)\n(i)\nget |X t \u2212Y t | < \u03a90 for some suitable constant \u03a90 . Since X t = E[Xt |X0 . . . Xs\u22121 , Xs =\ny] for some particular choice of y (and some fixed \"past\" X0 . . . Xs\u22121 ) and the\n\u221a\n(i)\nequivalent statement is true for Y t it follows from (A.1) that |Xbs \u2212 Xbs\u22121| \u2264 \u03a90 .\nNotice that equation (4.8) implies\nE|Xt \u2212 X t | p \u2264 \u03b1 p (\u03a90t) p/2 ,\n\n(A.2)\n\nfor some7 positive constants \u03b1 p . Before passing to the following parts of the\nb (\u2206|z)\nProposition, let us notice that not all the assumptions on the transition rates W\nwere used here. It is in fact sufficient to assume that the drifts f\u02c6(i) (z) are Lipschitz\ncontinuous.\nLet us now consider the point II. A simple computation shows that\n(i)\n\n(i)\n\n( j)\n\n(i)\n\nE Xt+1 = E Xt + E f (i) (Xt ) ,\n(i)\n\n( j)\n\nE[Xt+1 ; Xt+1 ] = E[Xt ; Xt ] + E f (i j) (Xt ) +\n\n(A.3)\n(A.4)\n\n(i)\n( j)\n+E[Xt ; f ( j) (Xt )] + E[ f (i) (Xt ); Xt ] + E[ f (i) (Xt ); f ( j) (Xt )] .\n7 One\n\nhas in fact \u03b1 p = p\n\np\n\n\u03c0/2 E|Z| p\u22121 with Z a standard Gaussian variable.\n\n\f40\n\nFinite-Length Scaling\n\nConsider the first of these equations and notice that, approximating f (i) (Xt ) by\nf\u02c6(i) (Xt /n) one obtains\nA\n(i)\n(i)\n|X t+1 \u2212 X t \u2212 f\u02c6(i) (X t /n)| \u2264 + |E[ f\u02c6(i) (Xt /n) \u2212 f\u02c6(i) (X t /n)]| .\nn\n\n(A.5)\n\nSince the second derivative of f\u02c6(i) (z) is bounded, we have the estimate\n1\n\u2202 f\u02c6(i)\n|E[ f\u02c6(i) (Xt /n) \u2212 f\u02c6(i)(X t /n)]| \u2264 \u2211\nn j \u2202z j\n\n( j)\n\nE[Xt\nX t /n\n\n( j)\n\n\u2212 Xt ] +\n\nB\nE|Xt \u2212 X t |2 \u2264\nn2\n\nC\n\u2264 .\nn\nSumming equation (A.5) over t, and applying Gronwall's Lemma we get\nA\u2032\n1 (i) (i)\nX t \u2212 z (t/n) \u2264 .\nn\nn\n\n(A.6)\n\nNotice that if we limit ourself to assume Lipschitz continuous drift coefficients\n(i)\nf\u02c6(i)\u221a\n(z), the same derivation yields a slightly weaker result: |X t /n \u2212 z(i) (t/n)| \u2264\nA\u2032 / n.\nEquation (4.10) is proved from (A.4) much in the same way, the crucial input\nbeing an estimate on E|Xt \u2212 X t |3 , once again obtained from equation (4.8). Here\nwe limit ourselves to sketch how the various terms emerges. We start by rewriting\nequation (A.4) in the form\n\uf8f9\n\uf8ee\nd\n(i)\n( j)\n\u02c6\n\u02c6\n\u2202\nf\n1\n\u2202\nf\n(l j)\n(i j)\n(i j)\n(il)\n+\n\u2206t \uf8fb +\n\u2206t+1 = \u2206t + f\u02c6(i j) (X t /n) + \u2211 \uf8f0\u2206t\nn l=1\n\u2202zl\n\u2202zl\nX t /n\n\nX t /n\n\n(0)\n(1)\n(1)\n(2)\n(2)\n(3)\n+Ri j + Ri j + R ji + Ri j + R ji + Ri j ,\n\nWith the remainders listed below\n(0)\n\nRi j = E[ f (i j) (Xt ) \u2212 f\u02c6(i j) (Xt /n)] + E[ f\u02c6(i j)(Xt /n) \u2212 f\u02c6(i j)(X t /n)] ,\n(1)\n\n(i)\n\nRi j = E[Xt ; f ( j) (Xt ) \u2212 f\u02c6( j) (Xt /n)] ,\n\n1 d \u2202 f\u02c6( j)\n(2)\n(i)\nRi j = E[Xt ; f\u02c6( j) (Xt /n) \u2212 f\u02c6( j) (X t /n) \u2212 \u2211\nn l=1 \u2202zl\n(3)\nRi j\n\n(i)\n\n= E[ f (Xt ); f\n\n( j)\n\n(l)\n\nX t /n\n\n(l)\n\n(Xt \u2212 X t )] ,\n\n(Xt )] .\n\nEach of this terms can be bounded separately as in the derivation of Eq. (A.6).\n(1)\nConsider for instance Ri j :\n(1)\n\n(i)\n\n(i)\n\n|Ri j | \u2264 E[Xt ; Xt ]1/2 E[ f ( j) (Xt ) \u2212 f\u02c6( j) (Xt /n); f ( j) (Xt ) \u2212 f\u02c6( j) (Xt /n)]1/2 \u2264\n\u2264 An1/2\n\nC\nB\n\u2264\u221a ,\nn\nn\n\n\f41\n\nFinite-Length Scaling\n\nwhere we used the estimate (A.2).\nLet us finally consider part III of the proposition, as stated in equation (4.12).\nIt is easy to derive the following recursion for the generating function:\n\u221a\ne (\u03bb/ n|X t ) \u2212 \u221a1 \u03bb * (Xt \u2212 X t ) +\n\u039bt+1 (\u03bb) = \u039bt (\u03bb) + log W\nn\n\uf8fc\n\uf8f1\n\u03bb\n\uf8f2 E[W\ne (\u03bb/\u221an|Xt ) e \u221an *Xt ] \uf8fd\n.\n+ log\n\u03bb *X\n\u221a\n\u221a\n\uf8fe\n\uf8f3 e\nE[W (\u03bb/ n|X t ) e n t ]\n\n(A.7)\n\nHere we defined the jump generating function\n\ne (\u03bb|x) \u2261 \u2211 e\u03bb*\u2206 W (\u2206|x) .\nW\n\u2206\n\nThe proof of equation (4.12) is completed by estimating the various terms in equation (A.7) as follows\n\u221a\ne (\u03bb/ n|X t ) \u2212 \u221a\u03bb * (X t+1 \u2212 X t ) \u2212 1 \u2211 f\u02c6(i j) (X t /n) \u03bbi \u03bb j \u2264 \u03a9a (\u03bb) ,\nlog W\nn\n2n i, j\nn3/2\n\n\u03bb\ne (\u03bb/\u221an|X t )) e \u221an *Xt ]\ne (\u03bb/\u221an|Xt ) \u2212 W\nE[(W\n\u2212\n\u03bb\ne (\u03bb/\u221an|X t ) e \u221an *Xt ]\nE[W\n\uf8ee\nd\n( j)\n1\n\u2202 f\u02c6(i)\n(l j)\n(il) \u2202 f\u02c6\n\u2212 2 \u2211\uf8f0\n\u2206t + \u2206t\nn l=1 \u2202zl\n\u2202zl\n\nX t /n\n\nX t /n\n\n\uf8f9\n\n\uf8fb \u2264 \u03a9b (\u03bb) .\nn3/2\n\nWe leave to the reader the pleasure of proving these two last (straightforward)\ninequalities.\nB. Unconditionally Stable Ensembles: Proof of the Scaling Law. In this\nAppendix we prove Lemma 3.1. The idea is to regard iterative decoding as a\nMarkov process in the space of states8 x = (vG , sG ,tG ) \u2208 Z3 . The transition rates\nand the initial condition for such a process are computed in Section 4.2. As in\nSec. 4.1, we denote by z = x/n = (\u03bdG , \u03c3G , \u03c4G ) the normalized state and by z(\u03c4 )\nthe critical trajectory. This is the solution of the density evolution equations (4.6),\nsuch that z(\u03c4end ) = (0, 0, 0), corresponding to complete decoding, \u03c3G (\u03c4 \u2217 ) = 0 for\nsome \u03c4 \u2217 \u2208 (0, \u03c4end ), and \u03c3G (\u03c4 ) > 0 for any \u03c4 \u2208 (0, \u03c4end ), \u03c4 6= \u03c4 \u2217 .\nIt would be tempting to use the general covariance evolution approach provided by Proposition 4.1. However a simple remark prevents us from following\nthis route in the most straightforward fashion. Proposition 4.1 was proved unb (\u2206|z) in the n \u2192 \u221e limit become\nder the assumptions that the transition rates W\n8 For the sake of definiteness, we refer here to the case of regular ensembles: the extension to\ngeneral unconditionally stable ensembles being trivial. Also, we use the subscript G for the state\ncoordinates in order to distinguish them from the time parameters t and \u03c4 .\n\n\f42\n\nFinite-Length Scaling\n\nC2 (Rd+1 ) functions of z. On the other hand, the decoding process is well defined\nonly if sG > 0, and we are interested in trajectories passing close to the s = 0 plane.\nIn more concrete\nterms, Proposition 4.1 cannot be true when z(\u03c4 ) is at a distance\n\u221a\nof order 1/ n from the sG = 0 plane. The least that will happen is that a part of\nthe Gaussian density is 'cut away'.\nAs a way to overcome this problem, we introduce a new Markov process on\nthe same states x = (vG , sG ,tG ) which is well defined for sG \u2264 0. We extend the\ntransition rates computed in the proof of Lemma 4.1 to sG \u2264 0 by setting \u03c3G = 0\nthere. More precisely we have:\n\u2206vG = \u22121 ,\n\n\u2206sG = \u2212u1 + u2 ,\n\n\u2206tG = \u2212u2 ,\n\n(B.1)\n\nwith u1 and u2 distributed according w(u1 , u2 ), see equation (4.15), where we put\nq1 = 0 and q2 = 2\u03c42 /\u03bdl and \u03c42 is determined as in (4.14). Notice that the only\nnon-zero entries of the distribution w(u1 , u2 ) in the sG \u2264 0 space are therefore\n\u0012\n\u0013\nl \u2212 1 u2\nw(1, u2 ) =\nq2 (1 \u2212 q2)l\u22121\u2212u2 .\nu2\nSuch transition rates do not necessarily correspond to any graph process in the\nsG < 0 plane. However, upon conditioning on sG > 0 the 'extended' process coincides with the original one. Therefore the probability of not leaving the sG > 0\nhalf-space (the 'survival' probability) can be calculated on the extended process.\nFinally, let us notice that the precise form of this extension is immaterial as long\nas some requirements are met. Call W (\u2206|x) the transition rates of the extended\nMarkov process. We require that:\n\u2022 The chain makes finite jumps.\nb (\u2206|z).\n\u2022 The rates are well approximated by their continuum counterpart W\nb (\u2206|x/n)| \u2264 \u03ba/n.\nAs in Sec. 4.1 this means that |W (\u2206|x) \u2212 W\n\u2022 The continuum transition rates are C2 with bounded derivatives in the\nregion {\u03bdG > \u03b5, \u03c3G > \u03b5, \u03c4G > \u03b5} for any \u03b5 > 0.\n\u2022 There exist a \u03b4 > 0 such that the continuum drift coefficients are Lipschitz continuous uniformly in the region Crit(\u03b4) \u2261 {z s.t. |z \u2212 z(\u03c4 \u2217 )| <\n\u03b4}. This means that | f\u02c6i (z) \u2212 f\u02c6i (z\u2032 )| \u2264 \u03ba\u2032 |z \u2212 z\u2032 | for some positive \u03ba\u2032 and\nany pair of points z, z\u2032 \u2208 Crit(\u03b4).\nThese requirements are easily checked on the extension defined above.\nRecall from Lemma 3.1 that we are only interested in decoding errors of\nsize at least \u03b3\u03bdG\u2217 , where \u03bdG\u2217 := \u03bdG (\u03c4 \u2217 ) is the critical point (measured in terms of\nthe fractional size of the graph) and \u03b3 is any number in (0, 1). In particular \u03b3 is\nnon-negative but can be chosen arbitrarily small. For ensembles with \u03bb\u2032 (0) = 0\na simple union bound shows that the decoder will be successful with high probability once the residual graph is sufficiently small but if \u03bb\u2032 (0) > 0 then small\ndeficiencies in the graph can contribute non-negligibly to the error probability.\nTherefore, by choosing \u03b3 \u2208 (0, 1), we \"separate out\" the contributions to the block\nerror probability which stem from large error events.\nCall Pend the probability of not hitting the sG = 0 until vG = \u230an\u03b3\u03bd \u2217 \u230b. Fix\n\u03c4max so that \u03bd(\u03c4max ) = \u03b3\u03bd \u2217 . Define Pt to be the survival probability up to time\n\n\fFinite-Length Scaling\n\n43\n\nt. It will be useful to denote by Pt (x\u2032 ,t \u2032 ) the probability of surviving up to time t\nconditioned on having survived up to time t \u2032 and that the state at time t \u2032 is x\u2032 .\nIn order to apply Proposition 4.1 as far as we can, we decompose the time\n\u2217 } and {t \u2217 + 1, . . . ,t\nup to tmax into two intervals: {0, . . . ,t\u2212\nmax }. The survival\n\u2212\nprobability can be written as\n\u2217\n\u2217\nPtmax = \u2211 Ptmax (x,t\u2212\n) P(x,t\u2212\n|x0 , 0) .\n\n(B.2)\n\nx\n\nHere P(x\u2032 ,t \u2032 |x,t) denotes the probability of arriving in state x\u2032 at time t \u2032 without\nhitting the sG = 0 plane, conditined on being in state x at time t. The sum over x\nruns over the sG > 0 half-space.\n\u2217\nNext we chose t\u2212\n= \u230an(\u03c4 \u2217 \u2212 \u03b5)\u230b for some (small) positive number \u03b5. With\n\u2217 |x , 0) in the above equation can be estimated using\nthis choice the factor P(x,t\u2212\n0\nthe covariance evolution approach and Proposition 4.1. The reason is that the\ntrajectories contributing to this factor stay at a distance of order n from the sG = 0\napart from some exponentially rare cases. We leave to the reader the task of\nadapting the proof of Proposition 4.1.III to this situation.\nThe first factor in equation (B.2) can not be estimated through covariance\nevolution. Fortunately a less refined calculation is sufficient in this case. In fact\n\u2217\nthe Lipschitz continuity of the drift coefficients ensures that, at any time t > t\u2212\n,\nthe state is within \u03b4 of the density evolution prediction with probability at least\n\u2217 )]. This fact was stressed in the proof of Proposition 4.1,\n1 \u2212 exp[\u2212\u03b4 2 /2\u03a9(t \u2212 t\u2212\ncf. Appendix A. For any state x, consider the solution z(\u03c4 ; x) of the density\n\u2217 /n; x) = x/n. Let P\n\u2217)=\nbtmax (x,t\u2212\nevolution equations (4.6) with initial condition z(t\u2212\n\u2217\n\u2217\nb\n0 if z(\u03c4 ; x) intersects the \u03c3G = 0 plane in the interval [t\u2212 /n, \u03c4max ] and Ptmax (x,t\u2212 ) =\n\u2217\n1 otherwise. The above concentration result implies that Pbtmax (x,t\u2212\n) is a good\n\u2217\napproximation for Ptmax (x,t\u2212 ).\nLet us prove the last statement in the cases in which z(\u03c4 ; x) does not intersect\n\u2217 ) = 1). If x is distributed according\nthe \u03c3G = 0 plane (and therefore Pbtmax (x,t\u2212\n\u221a\n\u2217\nto P(x,t\u2212 |x0 , 0), the trajectory z(\u03c4 ; x) will stay at a distance of order 1/ n from\nthe critical\n\u221a one. In particular, its minimum distance from the \u03c3G = 0 plane will\nbe \u03b3/ n with \u03b3 of order 1. This minimum will be achieved for \u03c4 close to \u03c4\u2217\n\u2217 <\nwith high probability. We therefore restrict ourselves to an interval of times t\u2212\n\u2217\nt < t\u2212 + nT \u03b5 for some fixed number T > 1, and neglect the cases in which the \u03c3G\n\u2217)\nplane is touched outside this interval. The error implied in substituting Pbtmax (x,t\u2212\n\u2217\nwith Ptmax (x,t\u2212 ) is upper bounded by the probability that the maximum distance\n\u2217 < t < t \u2217 + nT \u03b5\nbetween the actual decoding trajectory and z(\u03c4\u221a\n; x) in the interval t\u2212\n\u2212\n\u2217\n(\u03c4 \u2212 \u03b5 < \u03c4 < \u03c4\u2217 \u221a\n+ (T \u2212 1)\u03b5) is larger than \u03b3 n. Using the above concentration\n\u2217 < nT \u03b5, we get\nresult with \u03b4 = \u03b3 n and t \u2212 t\u2212\n\u001b\n\u001a\n\u03b32\n\u2217\n\u2217\nb\n.\n(B.3)\n|Ptmax (x,t\u2212 ) \u2212 Ptmax (x,t\u2212 )| \u2264 exp \u2212\n2\u03a9T \u03b5\n\u2217 |x , 0), both \u03b3 and T are, with\nAs mentioned above, under the distribution P(x,t\u2212\n0\nhigh probability O(1) (both with respect to n \u2192 \u221e and \u03b5 \u2192 0). Therefore the right\nhand side of equation (B.3) can be made arbitrarily small by taking \u03b5 \u2192 0.\n\n\f44\n\nFinite-Length Scaling\n\n\u2217 ) for P\n\u2217\nThe last step consists in substituting Pbtmax (x,t\u2212\ntmax (x,t\u2212 ) and the Gaus\u2217\nsian density from covariance evolution for P(x,t\u2212 |x0 , 0) in equation (B.2) and letting n \u2192 \u221e with n1/2 (\u01eb \u2212 \u01eb\u2217 ) fixed. This yields Lemma 3.1 up to corrections of\nwhich vanish when \u03b5 \u2192 0.\n\nC. Proof of Lemma 5.1. In this Appendix we present a proof of Lemma\n5.1, making use of Doob's maximal inequality (5.4). We shall prove that each\nof the two events considered in Eq. (5.5) occurs with probability greater than\n1 \u2212 \u03a91 exp[\u2212\u03a92 \u03b4 2 ]. This implies the thesis by a simple union bound, plus a\nrescaling of the constants \u03a91 , \u03a92 .\n(0)\n(0)\nLet us begin by considering the second event, namely Xtg \u2265 Xt \u2217 \u2212 \u03b4 4/3 n1/3 .\nFor sake of simplicity we redefine tg to be the position of the global minimum of\n(0)\nXt in the domain t > t \u2217 . The minimum with an unrestricted t can be treated by\nputting together the cases t > t \u2217 and t < t\u2217 . It is also useful to define\nYt\u2212t\u2217 :=\n\n1 (0)\n(0)\n(X \u2212 Xt \u2217 ) .\n\u03ba1 t\n\nEquation (5.4) implies\n\u0015\n\u001a\n\u0014\n\u001b\n\u221a\n1 2 \u03ba2 \u03b4\n2\nP min Yt \u2212 t + \u221a t \u2264 \u2212\u03b4 T \u2264 \u03a91 e\u2212\u03a92 \u03b4 ,\n0\u2264t\u2264T\nn\nn\n\n(C.1)\n\nwhere we rescaled the constants \u03ba2 and \u03a92 .\nLet {tl : l \u2208 Z} be a non-decreasing sequence of real numbers with tl \u2192 \u221e as\nl \u2192 \u221e and tl = 0 as l \u2192 \u2212\u221e. A union bound yields\n\u001b\n\u001a\n4/3 1/3\n\u2264\nP min Yt \u2264 \u2212\u03b4 n\nt\u22650\n\n+\u221e\n\n\u2211\n\nP\n\nl=\u2212\u221e\n\n\u001a\n\nmin Yt \u2264 \u2212\u03b4\n\ntl \u2264t<tl+1\n\n4/3 1/3\n\nn\n\n\u001b\n\n\u2264\n\n\u0014\n\u001b\n\u0015\n1 2 \u03ba2 \u03b4\n1 2 \u03ba2 \u03b4\n4/3 1/3\nmin Yt \u2212 t + \u221a t \u2264 \u2212\u03b4 n \u2212 tl + \u221a tl+1 \u2264\n\u2264 \u2211 P\ntl \u2264t<tl+1\nn\nn\nn\nn\nl=\u2212\u221e\n\u001a\n\u0013\u001b\n\u0012\n+\u221e\n1\n1\n\u03ba2 \u03b4\n\u2264 \u03a91 \u2211 exp \u2212\u03a92\n,\n\u03b4 4/3 n1/3 + tl2 \u2212 \u221a tl+1\ntl+1\nn\nn\nl=\u2212\u221e\n+\u221e\n\n\u001a\n\nwhere we used Eq. (C.1) in the last inequality. At thin point we choose tl =\n2l (n\u03b4)2/3 . Plugging into the above expression we get\n\uf8f1\n!2 \uf8fc\n\u001b\n\u001a\n\uf8fd\n\uf8f2 \u03a9 \u03b42\n+\u221e\n1/3\n\u03ba\n\u03b4\n2\n2\n.\nP min Yt \u2264 \u2212\u03b4 4/3n1/3 \u2264 \u03a91 \u2211 exp \u2212 l+1 1 + 22l \u2212 1/6 2l+1\n\uf8fe\n\uf8f3 2\nt\u22650\nn\nl=\u2212\u221e\nIf n > n0 (\u03b4) := (2\u03ba2 )6 \u03b4 2 we get\n\n\u001b\n\u001a\n4/3 1/3\n\u2264 \u03a91\nP min Yt \u2264 \u2212\u03b4 n\nt\u22650\n\n\u001a\n\u00112 \u001b\n\u03a92 \u03b4 2 \u0010\n2l\nl\n.\n\u2211 exp \u2212 2l+1 1 + 2 \u2212 2\nl=\u2212\u221e\n+\u221e\n\n\f45\n\nFinite-Length Scaling\n\nIt is an elementary exercise to show that the right hand side is smaller than\n\u03a91\u2032 exp{\u2212\u03a92\u2032 \u03b4 2 } for some (eventually different) positive parameters \u03a91\u2032 and \u03a92\u2032\nand any \u03b4 > \u03b40 .\nThe second part of the proof consists in proving an analogous upper bound\nfor the probability of having |tg \u2212 t \u2217 | > \u03b4 2/3 n2/3 . In fact the proof proceeds as for\nthe first event. One splits the semi-infinite interval t > t \u2217 in intervals [tl ,tl+1 [ with\ntl = 2l (n\u03b4)2/3 and (this time) l \u2265 0, and then apply Doob's maximal inequality to\neach interval. We leave to the reader the pleasure of filling the details.\nD. Convergence to diffusion process. In this Appendix we prove Lemma\n5.2 as a straightforward application of the following statement which can be found\nin [22].\nT HEOREM D.1. Let {Xt } be a Markov process with values in Rd and transition probability \u03c0h (x, dy), with 0 < h \u2264 1 and initial condition X0 = x0 . Let Ph\nbe the measure induced on the space of continuous trajectories \u03a9 = C([0, \u221e), Rd )\nby the mapping X(th) = Xt for integer t and interpolating linearly in between.\nAssume that the limit\n1\nh\u2192\u221e h\nlim\n\nZ\n\nRd\n\n[\u03c6(y) \u2212 \u03c6(x)] \u03c0h (x, dy) = (L \u03c6)(x) ,\n\n(D.1)\n\nexists uniformly in a compact K \u2286 Rd for functions \u03c6 \u2208 C\u221e (K). Assume that the\nlimit has the form\n(L \u03c6)(x) =\n\nd\n\u2202\u03c6\n\u22022 \u03c6\n1\nai j (x)\n+ \u2211 bi (x)\n,\n\u2211\n2 ij\n\u2202xi \u2202x j i=1\n\u2202xi\n\n(D.2)\n\nwith continuous and uniformly bounded coefficients a \u2261 {ai j (x)} (a being a positive definite matrix) and b \u2261 {bi (x)}. Assume finally that the solution of the\nmartingale problem for A is unique yielding a Markov family of measures Px on\n\u03a9. Then {Ph, x} converges to {Px } as h \u2192 0.\nThe proof of Lemma 5.2 proceed then sa follows. Set h = n\u22122/3 and define\nthe a Markov chain in the variables u0 ,~u, see Eq. (5.8), (5.8) using the transition\nrates W (\u2206|x) and the initial condition u0 (0) = \u03b6, ~u(0) = 0. One has then just to\ncompute the generator\n(L \u03c6)(u0 ,~u) = lim n2/3\nn\u2192\u221e\n\n~ \u2212 f (u0 ,~u)] *\n\u2211 [\u03c6(u0 + n\u22121/3\u22060 ,~u + n\u22122/3\u2206)\n\n~\n\u22060 ,\u2206\n\n~ \u22122/3v0 , n\u22121~Xt\u2217 + n\u22121/3~u) ,(D.3)\nb (\u22060 , \u2206|n\nW\n\nb (\u2206|x/n) which implies a negligible\nwhere made the subsitution W (\u2206|x) \u2192 W\nO(1/n) error. The formula (5.10) is easily obtained by Taylor expansion the above\nequation.\n\n\f"}