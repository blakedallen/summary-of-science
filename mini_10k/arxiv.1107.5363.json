{"id": "http://arxiv.org/abs/1107.5363v1", "guidislink": true, "updated": "2011-07-27T01:30:10Z", "updated_parsed": [2011, 7, 27, 1, 30, 10, 2, 208, 0], "published": "2011-07-27T01:30:10Z", "published_parsed": [2011, 7, 27, 1, 30, 10, 2, 208, 0], "title": "Convergence of the Iterative Rational Krylov Algorithm", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.4485%2C1107.1802%2C1107.1645%2C1107.3366%2C1107.3395%2C1107.1407%2C1107.1703%2C1107.1213%2C1107.4866%2C1107.0303%2C1107.5363%2C1107.0549%2C1107.3729%2C1107.0484%2C1107.3141%2C1107.3444%2C1107.0111%2C1107.0998%2C1107.5068%2C1107.3523%2C1107.0335%2C1107.4313%2C1107.2607%2C1107.0268%2C1107.2094%2C1107.3495%2C1107.3947%2C1107.4994%2C1107.2926%2C1107.4627%2C1107.3576%2C1107.3253%2C1107.1275%2C1107.5580%2C1107.3400%2C1107.5604%2C1107.4062%2C1107.4207%2C1107.0613%2C1107.2181%2C1107.0760%2C1107.0420%2C1107.2448%2C1107.0475%2C1107.4028%2C1107.3194%2C1107.2077%2C1107.2397%2C1107.3212%2C1107.5798%2C1107.1975%2C1107.1881%2C1107.4266%2C1107.2119%2C1107.2601%2C1107.4816%2C1107.1374%2C1107.4199%2C1107.0018%2C1107.2712%2C1107.5909%2C1107.4001%2C1107.0826%2C1107.0412%2C1107.3823%2C1107.1638%2C1107.2317%2C1107.1590%2C1107.3998%2C1107.3407%2C1107.5780%2C1107.1095%2C1107.3378%2C1107.3722%2C1107.5590%2C1107.0372%2C1107.4747%2C1107.4308%2C1107.2837%2C1107.2790%2C1107.4222%2C1107.5668%2C1107.5834%2C1107.4165%2C1107.2557%2C1107.1899%2C1107.3754%2C1107.4740%2C1107.5095%2C1107.2086%2C1107.4659%2C1107.1163%2C1107.3825%2C1107.1702%2C1107.5111%2C1107.4407%2C1107.5088%2C1107.1835%2C1107.5970%2C1107.1839%2C1107.0462&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Convergence of the Iterative Rational Krylov Algorithm"}, "summary": "The Iterative Rational Krylov Algorithm (IRKA) of [8] is an interpolatory\nmodel reduction approach to the optimal $\\mathcal{H}_2$ approximation problem.\nEven though the method has been illustrated to show rapid convergence in\nvarious examples, a proof of convergence has not been provided yet. In this\nnote, we show that in the case of state-space symmetric systems, IRKA is a\nlocally convergent fixed point iteration to a local minimum of the underlying\n$\\mathcal{H}_2$ approximation problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.4485%2C1107.1802%2C1107.1645%2C1107.3366%2C1107.3395%2C1107.1407%2C1107.1703%2C1107.1213%2C1107.4866%2C1107.0303%2C1107.5363%2C1107.0549%2C1107.3729%2C1107.0484%2C1107.3141%2C1107.3444%2C1107.0111%2C1107.0998%2C1107.5068%2C1107.3523%2C1107.0335%2C1107.4313%2C1107.2607%2C1107.0268%2C1107.2094%2C1107.3495%2C1107.3947%2C1107.4994%2C1107.2926%2C1107.4627%2C1107.3576%2C1107.3253%2C1107.1275%2C1107.5580%2C1107.3400%2C1107.5604%2C1107.4062%2C1107.4207%2C1107.0613%2C1107.2181%2C1107.0760%2C1107.0420%2C1107.2448%2C1107.0475%2C1107.4028%2C1107.3194%2C1107.2077%2C1107.2397%2C1107.3212%2C1107.5798%2C1107.1975%2C1107.1881%2C1107.4266%2C1107.2119%2C1107.2601%2C1107.4816%2C1107.1374%2C1107.4199%2C1107.0018%2C1107.2712%2C1107.5909%2C1107.4001%2C1107.0826%2C1107.0412%2C1107.3823%2C1107.1638%2C1107.2317%2C1107.1590%2C1107.3998%2C1107.3407%2C1107.5780%2C1107.1095%2C1107.3378%2C1107.3722%2C1107.5590%2C1107.0372%2C1107.4747%2C1107.4308%2C1107.2837%2C1107.2790%2C1107.4222%2C1107.5668%2C1107.5834%2C1107.4165%2C1107.2557%2C1107.1899%2C1107.3754%2C1107.4740%2C1107.5095%2C1107.2086%2C1107.4659%2C1107.1163%2C1107.3825%2C1107.1702%2C1107.5111%2C1107.4407%2C1107.5088%2C1107.1835%2C1107.5970%2C1107.1839%2C1107.0462&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Iterative Rational Krylov Algorithm (IRKA) of [8] is an interpolatory\nmodel reduction approach to the optimal $\\mathcal{H}_2$ approximation problem.\nEven though the method has been illustrated to show rapid convergence in\nvarious examples, a proof of convergence has not been provided yet. In this\nnote, we show that in the case of state-space symmetric systems, IRKA is a\nlocally convergent fixed point iteration to a local minimum of the underlying\n$\\mathcal{H}_2$ approximation problem."}, "authors": ["Garret Flagg", "Christopher Beattie", "Serkan Gugercin"], "author_detail": {"name": "Serkan Gugercin"}, "author": "Serkan Gugercin", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.sysconle.2012.03.005", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1107.5363v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.5363v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.5363v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1107.5363v1", "arxiv_comment": null, "journal_reference": "Systems and Control Letters, Volume: 61, Issue: 6, pp. 688-691,\n  2012", "doi": "10.1016/j.sysconle.2012.03.005", "fulltext": "arXiv:1107.5363v1 [math.NA] 27 Jul 2011\n\nConvergence of the Iterative Rational Krylov Algorithm\nGarret Flagg, Christopher Beattie, Serkan Gugercin\nDepartment of Mathematics, Virginia Tech.\nBlacksburg, VA, 24061-0123\ne-mail:\n\n{flagg,beattie,gugercin}@math.vt.edu\n\nJune 4, 2018\nAbstract\nIterative Rational Krylov Algorithm (IRKA) of [8] is an interpolatory model reduction approach to optimal H2 approximation problem. Even though the method has\nbeen illustrated to show rapid convergence in various examples, a proof of convergence\nhas not been provided yet. In this note, we show that in the case of state-space symmetric systems, IRKA is a locally convergent fixed point iteration to a local minimum\nof the underlying H2 approximation problem.\n\n1\n\nIntroduction\n\nConsider a single-input-single-output (SISO) linear dynamical system in state-space form:\n\u1e8b(t) = A x(t) + b u(t),\n\ny(t) = cT x(t),\n\n(1)\n\nwhere A \u2208 Rn\u00d7n , and b, c \u2208 Rn . In (1), x(t) \u2208 Rn , u(t) \u2208 R, y(t) \u2208 R, are, respectively, the\nstates, input, and output of the dynamical system. The transfer function of the underlying\nsystem is H(s) = cT (sI \u2212 A)\u22121 b. H(s) will be used to denote both the system and its\ntransfer function.\nDynamical systems of the form (1) with large state-space dimension n appear in many\napplications; see, e.g., [1] and [10]. Simulations in such large-scale settings make enormous\ndemands on computational resources. The goal of model reduction is to construct a surrogate\nsystem\n\u1e8br (t) = Ar xr (t) + br u(t),\n1\n\nyr (t) = cTr xr (t),\n\n(2)\n\n\fof much smaller dimension r \u226a n, with Ar \u2208 Rr\u00d7r and br , cr \u2208 Rr such that yr (t) approximates y(t) well in a certain norm. Similar to H(s), the transfer function Hr (s) of the\nreduced-model (2) is given by Hr (s) = cTr (sIr \u2212Ar )\u22121 br . We consider reduced-order models,\nHr (s), that are obtained via projection. That is, we choose full rank matrices Vr , Wr \u2208 Rn\u00d7r\nsuch that WrT Vr is invertible and define the reduced-order state-space realization with (2)\nand\nAr = (WrT Vr )\u22121 WrT AVr , br = (WrT Vr )\u22121 WrT b, cr = VrT c.\n\n(3)\n\nWithin this \"projection framework,\" selection of Wr and Vr completely determines the\nreduced system \u2013 indeed, it is sufficient to specify only the ranges of Wr and Vr in order to\ndetermine Hr (s). Of particular utility for us is a result by Grimme [6], that gives conditions\non Wr and Vr so that the associated reduced-order system, Hr (s), is a rational Hermite\ninterpolant to the original system, H(s).\nTheorem 1.1 (Grimme [6]). Given H(s) = cT (sI \u2212A)\u22121 b, and r distinct points s1 , . . . , sr \u2208\nC, let\n\uf8ee\n\uf8f9\ncT (s1 I \u2212 A)\u22121\n\uf8ef\n\uf8fa\n..\nVr = [(s1 I \u2212 A)\u22121 b . . . (sr I \u2212 A)\u22121 b]\nWrT = \uf8f0\n(4)\n\uf8fb.\n.\nT\n\u22121\nc (sr I \u2212 A)\nDefine the reduced-order model Hr (s) = cTr (sIr \u2212 Ar )\u22121 br as in (3) Then Hr is a rational\nHermite interpolant to H at s1 , . . . , sr :\nH(si ) = Hr (si )\n\nand\n\nH \u2032 (si ) = Hr\u2032 (si )\n\nfor\n\ni = 1, . . . , r.\n\n(5)\n\nRational interpolation within this \"projection framework\" was first proposed by Skelton et\nal. [18],[20],[21]. Later in [6], Grimme established the connection with the rational Krylov\nmethod of Ruhe [14].\nSignificantly, Theorem 1.1 gives an explicit method for computing a reduced-order system\nthat is a Hermite interpolant of the orginal system for nearly any set of distinct points,\n{s1 , . . . , sr }, yet it is not apparent how one should choose these interpolation points in order\nto assure a high-fidelity reduced-order model in the end. Indeed, the lack of such a strategy\nhad been a major drawback for interpolatory model reduction until recently, when an effective\nstrategy for selecting interpolation points was proposed in [8] yielding reduced-order models\nthat solve\nkH \u2212 Hr kH2 = min\nH \u2212 \u0124r\n.\n(6)\ndim(\u0124r )=r\n\nH2\n\nwhere the H2 system norm is defined in the usual way:\n\u00131/2\n\u0012 Z \u221e\n1\n2\n| H(\uf6be\u03c9) | d\u03c9\n.\nkHkH2 =\n2\u03c0 \u2212\u221e\n\n(7)\n\nThe optimization problem (6) has been studied extensively, see, for example, [13, 19, 8, 15,\n5, 17, 7, 2, 3, 22] and references therein. (6) is a nonconvex optimization problem and finding\n2\n\n\fglobal minimizers will be infeasible, typically. Hence, the usual interpretation of (6) involves\nfinding local minimizers and a common approach to accomplish this is to construct reducedorder models satisfying first-order necessary optimality conditions. This may be posed either\nin terms of solutions to Lyapunov equations (e.g., [19, 15, 22]) or in terms of interpolation\n(e.g., [19, 8, 17, 3]):\nTheorem 1.2. ([13, 8]) Given H(s), let Hr (s) be a solution to (6) with simple poles\n\u03bb\u03021 , . . . , \u03bb\u0302r . Then\nH(\u2212\u03bb\u0302i ) = Hr (\u2212\u03bb\u0302i )\n\nand\n\nH \u2032(\u2212\u03bb\u0302i ) = Hr\u2032 (\u2212\u03bb\u0302i )\n\nfor\n\ni = 1, . . . , r.\n\n(8)\n\nThat is, any H2 -optimal reduced order model of order r with simple poles will be a Hermite\ninterpolant to H(s) at the reflected image of the reduced poles through the origin.\nAlthough this result might appear to reduce the problem of H2 -optimal model approximation to a straightforward application of Theorem 1.1 to calculate a Hermite interpolant on\nthe set of reflected poles, {\u2212\u03bb\u03021 , . . . , \u2212\u03bb\u0302r }, these pole locations will not be known a priori.\nNonetheless, these pole locations can be determined efficiently with the Iterative Rational\nKrylov Algorithm (IRKA) of Gugercin et al. [8]. Starting from an arbitrary initial selection\nof interpolation points, IRKA iteratively corrects the interpolation points until (8) is satisfied.\nA brief sketch of IRKA is given below.\nAlgorithm IRKA. Iterative Rational Krylov Algorithm [8]\n\nGiven a full-order H(s), a reduction order r, and convergence tolerance tol.\n1. Make an initial selection of r distinct interpolation points, {si }r1 , that is\nclosed under complex conjugation.\n2. Construct Vr and Wr as in (4).\n3. while (relative change in {si } > tol)\na.) Ar = (WrT Vr )\u22121 WrT AVr\nb.) Solve r \u00d7 r eigenvalue problem Ar u = \u03bbu and assign si \u2190 \u2212\u03bbi (Ar ) for\ni = 1, . . . , r.\nc.) Update Vr and Wr with new si 's using (4).\n4. Ar = (WrT Vr )\u22121 WrT AVr ,\n\nbr = (WrT Vr )\u22121 WrT b, and cr = VrT c.\n\nIRKA has been remarkably successful in producing high fidelity reduced-order approximations\nand has been successfully applied to finding H2 -optimal reduced models for systems of high\norder, n > 160, 000, see [9]. For details on IRKA, see [8].\n\n3\n\n\fNotwithstanding typically observed rapid convergence of the IRKA iteration to interpolation\npoints that generally yield high quality reduced models, no convergence theory for IRKA has\nyet been established. Evidently from the description above, IRKA may be viewed as a fixed\npoint iteration with fixed points coinciding with the stationary points of the H2 minimization\nproblem. Saddle points and local maxima of the H2 minimization problem are known to\nbe repellent [11]. However, despite effective performance in practice, it has not yet been\nestablished that local minima are attractive fixed points.\nIn this paper, we give a proof of this for the special case of state-space-symmetric systems\nand establish the convergence of IRKA for this class of systems.\n\n2\n\nState-Space-Symmetric Systems\n\nDefinition 1. H(s)=cT (sI \u2212 A)\u22121b is state-space-symmetric (SSS) if A=AT and c = b.\nSSS systems appear in many important applications such as in the analysis of RC circuits\n\nand in inverse problems involving 3D Maxwell's equations [4].\nA closely related class of systems is the class of zero-interlacing-pole (ZIP) systems.\nn\u22121\nQ\n(s \u2212 zi )\ni=1\nis a strictly proper ZIP system provided that\nDefinition 2. A system H(s) = K Q\nn\n(s \u2212 \u03bbj )\nj=1\n\n0 > \u03bb1 > z1 > \u03bb2 > z2 > \u03bb3 > * * * > zn\u22121 > \u03bbn .\n\nThe following relation serves to characterize ZIP systems.\nProposition 2.1. [16] H(s) is a strictly proper ZIP system if and only if H(s) can be written\nn\nX\nbi\nas H(s) =\nwith \u03bbi < 0, bi > 0, and \u03bbi 6= \u03bbj for all i 6= j.\ns \u2212 \u03bbi\ni=1\nThe next result clarifies the relationship between SSS and ZIP systems.\nLemma 2.1. [12] Let H(s) be SSS. Then H(s) is minimal if and only if the poles of H(s)\nare distinct. Moreover, every SSS system has a SSS minimal realization with distinct poles,\nand is therefore a strictly proper ZIP system.\nIt can easily be verified from the implementation of IRKA given above, that for SSS systems,\nthe relationship Vr =Wr is maintained throughout the iteration, and the final reduced-order\nmodel at Step 4 of IRKA can be obtained by\nAr = QTr AQr br = cr = QTr b,\n\n(9)\n\nwhere Qr is an orthonormal basis for Vr ; the reduced system resulting from IRKA is also SSS.\n4\n\n\f3\n\nThe Main Result\n\nTheorem 3.1. Let IRKA be applied to a minimal SSS system H(s). Then every fixed point\nof IRKA which is a local minimizer is locally attractive. In other words, IRKA is a locally\nconvergent fixed point iteration to a local minimizer of the H2 optimization problem.\nTo proceed with the proof of Theorem 3.1, we need four intermediate lemmas. The first\nlemma provides insight into the structure of the zeros of the error system resulting from\nreducing a SSS system.\nLemma 3.1. Let H(s) be a SSS system of order n. If Hr (s) is a ZIP system that interpolates\nH(s) at 2r points s1 , s2 , . . . , s2r , not necessarily distinct, in (0, \u221e), then all the remaining\nzeros of the error system lie in (\u2212\u221e, 0).\nProof. By Lemma 2.1, we may assume that H(s) is a strictly proper ZIP systems. Since\nH(s) is a strictly proper ZIP system, its poles are simple and all its residues are positive. Let\n\u03bbi < 0, \u03c6i > 0, for i = 1, . . . , n be the poles and residues of H(s), respectively. Now let\nR(s) =\n\n2r\nY\ni=1\n\n(s \u2212 si ), P (s) =\n\nn\u2212r\u22121\nY\ni=1\n\nn\nr\nY\nY\n(s + zi ), Q(s) =\n(s \u2212 \u03bbi ), Q\u0303(s) =\n(s \u2212 \u03bb\u0303i ),\ni=1\n\ni=1\n\nwhere \u03bb\u0303i , si , and zi are, respectively, the poles of Hr (s), the interpolation points, and the\nP (s)R(s)\n.\nremaining zeros of the error system. Then for some constant K, H(s) \u2212 Hr (s) = K Q(s)\nQ\u0303(s)\nFirst suppose that {\u03bbi }ni=1 \u2229 {\u03bb\u0303k }rk=1 = \u2205. Then for each \u03bbj , j = 1, . . . , n,\nRes(H(s) \u2212 Hr (s); \u03bbj ) = K Q\nn\n\nP (\u03bbj )R(\u03bbj )\n\n= \u03c6i > 0.\n\n(10)\n\n(\u03bbj \u2212 \u03bbi )Q\u0303(\u03bbj )\n\ni=1\n\u03bbi 6=\u03bbj\n\nThus, sgn(KP (\u03bbj )) = (\u22121)j\u22121 sgn(Q\u0303(\u03bbj )) where sgn(\u03b1) denotes the sign of \u03b1. Now if\n(\u22121)j\u22121 sgn(Q\u0303(\u03bbj )) = (\u22121)j (sgn(Q\u0303(\u03bbj+1)), then \u2212 sgn(Q\u0303(\u03bbj )) = sgn(Q\u0303(\u03bbj+1 )), so Q\u0303(s) must\nchange sign on the interval [\u03bbj+1 , \u03bbj ]. Since Q\u0303(s) is a polynomial of degree r, and r < n, Q\u0303(s)\ncan switch signs at most r times, else Q\u0303(s) \u2261 0. But this means there are at least n \u2212 r \u2212 1\nintervals [\u03bbjk +1 , \u03bbjk ], for k = 1, . . . , n \u2212 r \u2212 1, for which sgn(Q\u0303(\u03bbjk )) = sgn(Q\u0303(\u03bbjk +1 )), and\ntherefore sgn(KP (\u03bbjk )) = \u2212 sgn(KP (\u03bbjk +1 )). So KP (s) must change sign over at least\nn \u2212 r \u2212 1 intervals, and therefore has at least n \u2212 r \u2212 1 zeros on [\u03bbn , \u03bb1 ]. Again, since the\nerror is not identically zero when r < n, and the degree of KP (s) is n \u2212 r \u2212 1, this implies\nthat all the zeros of KP (s) lie in (\u2212\u221e, 0).\nSuppose with some p \u2264 r, \u03bbij = \u03bb\u0303kj for j = 1, . . . , p. Observe from partial fraction expansions\nof H(s) and Hr (s) that the error can be written as a rational function of degree n + r \u2212 p \u2212 1\n5\n\n\fover degree n + r \u2212 p with distinct poles. n of these poles belong to H(s) and the remaining\nr \u2212 p come from the poles of Hr (s) that are distinct from the poles of H(s). Now let\nr\u2212p\nn\u2212r\u2212p\u22121\nn\n2r\nY\nY\nY\nY\n(s \u2212 \u03bb\u0303kl ),\n(s + zi ), Q(s) =\n(s \u2212 \u03bbi ), Q\u0303(s) =\nR(s) =\n(s \u2212 si ), P (s) =\ni=1\n\ni=1\n\ni=1\n\nl=1\n\nP (s)R(s)\nr\u2212p\n. Observe that there are\nwhere {\u03bb\u0303kl }l=1\n= {\u03bb\u0303k }rk=1 \\{\u03bbi }ni=1 . Hence, H(s)\u2212Hr (s) = K Q(s)\nQ\u0303(s)\n\nat most 2p subintervals of the form [\u03bbi\u2217 , \u03bbi\u2217 +1 ] or [\u03bbi\u2217 \u22121 , \u03bbi\u2217 ], where \u03bbi\u2217 \u2208 {\u03bbi }ni=1 \u2229 {\u03bb\u0303k }rk=1 .\nIt follows that there are at least n \u2212 2p \u2212 1 subintervals of the form [\u03bbi , \u03bbi+1 ], where \u03bbi , \u03bbi+1 6\u2208\n{\u03bbi }ni=1 \u2229 {\u03bb\u0303k }rk=1 . On each such subinterval for which this is the case, we have\nRes(H(s) \u2212 Hr (s); \u03bbi ) = K Q\nn\n\nP (\u03bbi )R(\u03bbi )\n\n= \u03c6i > 0.\n\n(11)\n\n(\u03bbi \u2212 \u03bbj )Q\u0303(\u03bbi )\n\nj=1\n\u03bbj 6=\u03bbi\n\nSo sgn(KP (\u03bbi )) = (\u22121)i\u22121 sgn(Q\u0303(\u03bbi )). By the same argument as above where the poles of\nH(s) and Hr (s) are distinct, either Q\u0303(s) or P (s) has a zero on the interval [\u03bbi , \u03bbi+1 ]. Since\nQ\u0303(s) has at most r\u2212p zeros, this means that there are at least n\u22122p\u22121\u2212(r\u2212p) = n\u2212p\u2212r\u22121\nsubintervals between poles of H(s) where P (s) has zeros. Hence, the lemma is proved.\nLemma 3.2. Let H(s) = bT (sI \u2212 A)\u22121 b be SSS, and Hr (s) = bTr (sIr \u2212 Ar )\u22121 br be any\nreduced order model of H(s) constructed by a compression of H(s), i.e., Ar =QTr Ar Qr ,\nbr = QTr b. Then for any s \u2265 0, H(s) \u2212 Hr (s) \u2265 0.\nProof. Pick any s \u2265 0. Then (sIn \u2212 A) is symmetric, positive definite and has a Cholesky\ndecomposition, (sIn \u2212 A) = LLT . Define Zr = LT Qr . Then\nh\n\u0001\u22121 T i\n\u22121\nT\nT\nH(s) \u2212 Hr (s) = b (sIn \u2212 A) \u2212 Qr Qr (sIn \u2212 A)Qr\nQr b\nh\ni\n\u0001\n\u22121\n= (L\u22121 b)T I \u2212 Zr ZrT Zr\nZrT (L\u22121 b).\n\nNote the last bracketed expression is an orthogonal projector onto Ran(Zr )\u22a5 , hence is positive\nsemidefinite and the conclusion follows.\nOur convergence analysis of IRKA will use its formulation as a fixed-point iteration. The\nanalysis will build on the framework of [11]. Let\nH(s) =\n\nn\nX\ni=1\n\n\u03c6i\ns \u2212 \u03bbi\n\nand Hr (s) =\n\nr\nX\nj=1\n\n\u03c6\u0303j\ns \u2212 \u03bb\u0303j\n\n(12)\n\nbe the partial fraction decompositions of H(s), and Hr (s), respectively. Given a set of r\ninterpolation points {si }ri=1 , identify the set with a vector s = [s1 , . . . , sr ]T . Construct an\n6\n\n\finterpolatory reduced order model Hr (s) from s as in Theorem 1.1 and identify {\u03bb\u0303i }ri=1 with\na vector \u03bb\u0303 = [\u03bb\u03031 , . . . , \u03bb\u0303r ]T . Then define the function \u03bb : Cr \u2192 Cr by \u03bb(s) = \u2212\u03bb\u0303. Aside\nfrom ordering issues, this function is well defined, and the IRKA iteration converges when\n\u03bb(s) = s. Thus convergence of IRKA is equivalent to convergence of a fixed point iteration\non the function \u03bb(s). Similar to s and \u03bb\u0303, let \u03c6\u0303 = [\u03c6\u03031 , . . . , \u03c6\u0303r ]T . Having identified Hr (s) with\nits poles and residues, the optimal H2 model reduction problem may be formulated in terms\nof minimizing the cost function J (\u03c6\u0303, \u03bb(s)) = kH \u2212 Hr k2H2 , where\nJ (\u03c6\u0303, \u03bb(s)) =\n\nn\nX\n\n\u03c6i (H(\u03bbi ) \u2212 Hr (\u03bbi )) +\n\ni=1\n\nr\nX\n\n\u03c6\u0303j (H(\u03bb\u0303j ) \u2212 Hr (\u03bb\u0303j ))\n\n(13)\n\nj=1\n\nSee [8] for a derivation of (13). Define the matrices S11 , S12 , S22 \u2208 Rr\u00d7r as\n[S11 ]i,j = \u2212(\u03bb\u0303i + \u03bb\u0303j )\u22121 , [S12 ]i,j = \u2212(\u03bb\u0303i + \u03bb\u0303j )\u22122 and [S22 ]i,j = \u22122(\u03bb\u0303i + \u03bb\u0303j )\u22123\nfor i, j = 1, . . . , r. Also, define R, E \u2208 Rr\u00d7r :\nR = diag({\u03c6\u03031 , . . . , \u03c6\u0303r }),\n\nand E = diag({H \u2032\u2032 (\u2212\u03bb\u03031 ) \u2212 Hr\u2032\u2032 (\u2212\u03bb\u03031 ), . . . , H \u2032\u2032(\u2212\u03bb\u0303r ) \u2212 Hr\u2032\u2032 (\u2212\u03bb\u0303r )}.\n\nLemma 3.3. Let H(s) be SSS and let Hr (s) be an IRKA interpolant. Then E is positive\ndefinite at any fixed point of \u03bb(s).\nProof. By Lemma 3.2, H(s) \u2212 Hr (s) \u2265 0 for all s \u2208 [0, \u221e). Thus the points H(\u2212\u03bb\u0303i ) \u2212\nHr (\u2212\u03bb\u0303i ) are local minima of H(s) \u2212 Hr (s) on [0, \u221e) for i = 1, . . . , r. It then follows that\nH \u2032\u2032 (\u2212\u03bb\u0303i ) \u2212 Hr\u2032\u2032 (\u2212\u03bb\u0303i ) \u2265 0. But by Lemma 3.1, H(s) \u2212 Hr (s) has exactly 2r zeros in C+ , so\nH \u2032\u2032 (\u2212\u03bb\u0303i ) \u2212 Hr\u2032\u2032 (\u2212\u03bb\u0303i ) > 0 for i = 1, . . . , r.\n\u0015\n\u0014\nS11 S12\nis positive definite.\nLemma 3.4. The matrix S\u0303 =\nS12 S22\nProof. We will show that for any non-zero vector z = [z1 , z2 , . . . , z2r ]T \u2208 R2r\nZ \u221e\u0014X\nr\nr\n\u0010X\n\u0011\u00152\nT\n\u03bb\u0303i t\n\u03bb\u0303i t\nz Sz =\nzi e \u2212 t\nzr+i e\ndt > 0.\n0\n\ni=1\n\ni=1\n\nDefine zr = [z1 , z2 , . . . , zr ]T \u2208 Rr and z2r = [zr+1 , zr+2 , . . . , z2r ]T \u2208 Rr . Then\nT\nz T Sz = zrT S11 zr + 2zrT S12 z2r + z2r\nS22 z2r\n\n(14)\n\nLet \u039b = diag(\u03bb\u03031 , . . . , \u03bb\u0303r ) and u be a vector of r ones. Note that S11 solves the Lyapunov\nequation \u039bS11 + S11 \u039b + uuT = 0. Thus,\nZ \u221e\nZ \u221e\u0010X\nr\n\u00112\nT\nT \u039bt\nT \u039bt\nzr S11 zr =\nzr e uu e zr dt =\nzi e\u03bb\u0303i t dt\n(15)\n0\n\n0\n\n7\n\ni=1\n\n\fSimilarly, S12 solves \u039bS12 + S12 \u039b \u2212 S11 = 0. An application of integration by parts gives:\nZ\n\n0\n\n\u221e\n\nZ\nr\nr\n\u0010X\n\u0011\u0010 X\n\u0011\n\u03bb\u0303i t\n\u03bb\u0303i t\nt\ndt =\nzi e\nzr+i e\ni=1\n\n\u221e\n\nt(zrT (e\u039bt uuT e\u039bt )z2r ) dt\n\n0\n\ni=1\n\nh\n\n=\n\nzrT\n\n=\n\n\u2212zrT S12 z2r\n\n\u039bt\n\n\u039bt\n\n\u2212 t(e S11 e )\n\ni\u221e\n0\n\nz2r +\n\nzrT\n\n\u0010Z\n\n\u221e\n0\n\n\u0011\ne\u039bt S11 e\u039bt dt z2r\n(16)\n\nFinally, note that S22 solves \u039bS22 + S22 \u039b \u2212 2S12 = 0. Repeated applications of integration\nby parts then yields the equality:\nT\nz2r\nS22 z2r\n\n=\n\nZ\n\n\u221e\n2\n\nt\n\n0\n\nr\n\u0010X\n\n\u03bb\u0303i t\n\nzr+1 e\n\ni=1\n\n\u00112\n\ndt\n\n(17)\n\nCombining equations (14), (15), (16), and (17) gives the desired results since\nT\n\nz Sz =\n\n\u0014 r\nR\u221e P\n0\n\n\u03bb\u0303i t\n\nzi e\n\ni=1\n\n\u0010P\n\u0011\u00152\nr\n\u03bb\u0303i t\n\u2212t\nzr+i e\ndt.\ni=1\n\n\u22121\nThen it follows that the Schur complement S22 \u2212 S12 S11\nS12 of S\u0303 is also positive definite.\nWith the setup above, we may now commence with the proof of Theorem 3.1.\n\nProof of Theorem 3.1: It suffices to show that for any fixed point which is a local minimizer\nof J (\u03c6\u0303, \u03bb(s)), the eigenvalues of the Jacobian of \u03bb(s) are bounded in magnitude by 1. As\nshown in [11], the Jacobian of \u03bb(s) can be written as \u2212Sc\u22121 K where\n\u22121\nSc = S22 \u2212 S12 S11\nS12\n\nand\n\nK = ER\u22121 .\n\nFirst off, note that from Lemma 3.3, and the fact that H(s) is a ZIP system by Lemma 2.1,\nK is positive definite. Evaluating the pencil K \u2212 \u03bbSc at \u03bb = 1 gives\n\u22121\n\u03a6 = \u2212S22 + ER\u22121 + S12 S11\nS12 ,\n\nThis pencil is regular since Sc is positive definite by Lemma 3.4, and therefore det(K \u2212 \u03bbSc )\nis zero if and only if det(Sc\u22121 K \u2212 \u03bbI) = 0.\nLet \u22072 J denote the Hessian of the cost function J (\u03c6\u0303, \u03bb(s)). As shown in [11], \u22072 J can\nbe written as\n\u0015\n\u0015\n\u0014\n\u0015\n\u0014\n\u0014\nS11\nS12\nI 0\nI 0\n2\n.\n, where M =\nM\n\u2207J =\nS12 S22 \u2212 ER\u22121\n0 R\n0 R\n\n8\n\n\fNote that \u2212\u03a6 is the Schur complement of M . Hence, if the fixed point is a local minimimum,\nthen \u2212\u03a6 must be positive definite and so for \u03bb = 1 the pencil is negative definite. Since\nboth K and Sc are positive definite, there exists a nonsingular transformation Z by which\nthe quadratic form y T (K \u2212 \u03bbSc )y is transformed into z T (\u039b \u2212 \u03bbI)z, where \u039b is a diagonal\nmatrix formed from the solutions of\ndet(K \u2212 \u03bbSc ) = 0.\n\n(18)\n\nThus, the solutions of (18) correspond to the eigenvalues of Sc\u22121 K. \u039b \u2212 I must be negative\ndefinite since \u03a6 is, and therefore the eigenvalues of the Sc\u22121 K must be real-valued and less\nthan one. Furthermore, note that P = Sc\u22121 K solves the Lyapunov equation\nP Sc\u22121 + Sc\u22121 P T = 2Sc\u22121 KSc\u22121 ,\nso by the standard inertia result, all the eigenvalues of Sc\u22121 K are positive, and the desired\nresult follows. \u0003\n\nReferences\n[1] A.C. Antoulas. Approximation of Large-Scale Dynamical Systems (Advances in Design\nand Control). Society for Industrial and Applied Mathematics, Philadelphia, PA, USA,\n2005.\n[2] C.A. Beattie and S. Gugercin. Krylov-based minimization for optimal H2 model reduction. 46th IEEE Conference on Decision and Control, pages 4385\u20134390, Dec. 2007.\n[3] C.A. Beattie and S. Gugercin. A trust region method for optimal H2 model reduction.\n48th IEEE Conference on Decision and Control, Dec. 2009.\n[4] V. Druskin, L. Knizhnerman, and M. Zaslavsky. Solution of large scale evolutionary\nproblems using rational Krylov subspaces with optimized shifts. SIAM Journal on\nScientific Computing, 31(5):3760\u20133780, 2009.\n[5] P. Fulcheri and M. Olivi. Matrix rational H2 approximation: a gradient algorithm based\non Schur analysis. SIAM Journal on Control and Optimization, 36(6):2103\u20132127, 1998.\n[6] E.J. Grimme. Krylov projection methods for model reduction. PhD thesis, University of\nIllinois, 1997.\n[7] S. Gugercin. An iterative rational Krylov algorithm (IRKA) for optimal H2 model\nreduction. In Householder Symposium XVI, Seven Springs Mountain Resort, PA, USA,\nMay 2005.\n[8] S. Gugercin, A.C. Antoulas, and C. Beattie. H2 model reduction for large-scale linear\ndynamical systems. SIAM Journal on Matrix Analysis and Applications, 30(2):609\u2013638,\n2008.\n9\n\n\f[9] A.R. Kellems, D. Roos, N. Xiao, and S.J. Cox. Low-dimensional, morphologically\naccurate models of subthreshold membrane potential. Journal of Computational Neuroscience, 27(2):161\u2013176, 2009.\n[10] J.G. Korvink and E.B. Rudnyi. Oberwolfach benchmark collection. In P. Benner,\nV. Mehrmann, and D. C. Sorensen, editors, Dimension Reduction of Large-Scale Systems, volume 45 of Lecture Notes in Computational Science and Engineering, pages\n311\u2013315. Springer-Verlag, Berlin/Heidelberg, Germany, 2005.\n[11] W. Krajewski, A. Lepschy, M. Redivo-Zaglia, and U. Viaro. A program for solving the\nL2 reduced-order model problem with fixed denominator degree. Numerical Algorithms,\n9(2):355\u2013377, 1995.\n[12] W.Q. Liu, V. Sreeram, and K.L. Teo. Model Reduction for State-space Symmetric\nSystems. Systems & Control Letters, 34:209\u2013215, 1998.\n[13] L. Meier III and D. Luenberger. Approximation of linear constant systems. Automatic\nControl, IEEE Transactions on, 12(5):585\u2013588, 1967.\n[14] A. Ruhe. Rational Krylov: A practical algorithm for large sparse nonsymmetric matrix\npencils. SIAM Journal on Scientific Computing, 19(5):1535\u20131551, 1998.\n[15] J.T. Spanos, M.H. Milman, and D.L. Mingori. A new algorithm for L2 optimal model\nreduction. Automatica, 28(5):897\u2013909, 1992.\n[16] B. Srinivasan and P. Myszkorowski. Model reduction of systems with zeros interlacing\nthe poles. Systems & Control Letters, 30(1):19\u201324, 1997.\n[17] P. Van Dooren, K.A. Gallivan, and P.A. Absil. H2 -optimal model reduction of MIMO\nsystems. Applied Mathematics Letters, 21(12):1267\u20131273, 2008.\n[18] C.D. Villemagne and R.E. Skelton. Model reduction using a projection formulation.\nIntl. J. Contr, 46:2141\u20132169, 1987.\n[19] D.A. Wilson. Optimum solution of model-reduction problem. Proc. IEE, 117(6):1161\u2013\n1165, 1970.\n[20] A. Yousuff and R.E. Skelton. Covariance Equivalent Realizations with Application to\nModel Reduction of Large Scale Systems. Control and Dynamic Systems, 22, 1984.\n[21] A. Yousuff, D.A. Wagie, and R.E. Skelton. Linear system approximation via covariance\nequivalent realizations. Journal of mathematical analysis and applications, 106(1):91\u2013\n115, 1985.\n[22] D. Zigic, L.T. Watson, and C.A. Beattie. Contragredient transformations applied to the\noptimal projection equations. Linear algebra and its applications, 188:665\u2013676, 1993.\n10\n\n\f"}