{"id": "http://arxiv.org/abs/1202.2350v1", "guidislink": true, "updated": "2012-02-10T09:16:43Z", "updated_parsed": [2012, 2, 10, 9, 16, 43, 4, 41, 0], "published": "2012-02-10T09:16:43Z", "published_parsed": [2012, 2, 10, 9, 16, 43, 4, 41, 0], "title": "Streaming an image through the eye: The retina seen as a dithered\n  scalable image coder", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.4673%2C1202.2703%2C1202.2477%2C1202.3682%2C1202.5899%2C1202.3114%2C1202.3817%2C1202.4905%2C1202.1794%2C1202.2298%2C1202.3060%2C1202.5838%2C1202.6478%2C1202.4313%2C1202.6583%2C1202.3601%2C1202.0694%2C1202.1593%2C1202.6220%2C1202.6606%2C1202.1418%2C1202.5364%2C1202.6120%2C1202.2184%2C1202.5400%2C1202.5352%2C1202.0344%2C1202.5969%2C1202.6624%2C1202.3973%2C1202.5024%2C1202.1682%2C1202.0723%2C1202.2370%2C1202.2531%2C1202.6064%2C1202.4935%2C1202.5429%2C1202.5373%2C1202.0002%2C1202.5295%2C1202.5748%2C1202.4504%2C1202.1093%2C1202.1444%2C1202.0605%2C1202.5256%2C1202.3132%2C1202.6179%2C1202.5536%2C1202.0887%2C1202.0992%2C1202.0153%2C1202.5702%2C1202.2368%2C1202.6295%2C1202.3379%2C1202.1948%2C1202.2433%2C1202.1292%2C1202.2350%2C1202.2932%2C1202.5578%2C1202.1831%2C1202.2157%2C1202.5680%2C1202.3366%2C1202.5099%2C1202.5661%2C1202.5436%2C1202.2171%2C1202.2429%2C1202.0094%2C1202.6264%2C1202.0937%2C1202.3960%2C1202.2160%2C1202.6201%2C1202.5415%2C1202.3307%2C1202.3652%2C1202.5572%2C1202.5461%2C1202.2535%2C1202.2000%2C1202.0667%2C1202.3851%2C1202.3426%2C1202.0296%2C1202.0015%2C1202.1959%2C1202.1380%2C1202.0442%2C1202.4349%2C1202.6239%2C1202.4434%2C1202.6568%2C1202.6585%2C1202.6681%2C1202.4230%2C1202.4496&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Streaming an image through the eye: The retina seen as a dithered\n  scalable image coder"}, "summary": "We propose the design of an original scalable image coder/decoder that is\ninspired from the mammalians retina. Our coder accounts for the time-dependent\nand also nondeterministic behavior of the actual retina. The present work\nbrings two main contributions: As a first step, (i) we design a deterministic\nimage coder mimicking most of the retinal processing stages and then (ii) we\nintroduce a retinal noise in the coding process, that we model here as a dither\nsignal, to gain interesting perceptual features. Regarding our first\ncontribution, our main source of inspiration will be the biologically plausible\nmodel of the retina called Virtual Retina. The main novelty of this coder is to\nshow that the time-dependent behavior of the retina cells could ensure, in an\nimplicit way, scalability and bit allocation. Regarding our second\ncontribution, we reconsider the inner layers of the retina. We emit a possible\ninterpretation for the non-determinism observed by neurophysiologists in their\noutput. For this sake, we model the retinal noise that occurs in these layers\nby a dither signal. The dithering process that we propose adds several\ninteresting features to our image coder. The dither noise whitens the\nreconstruction error and decorrelates it from the input stimuli. Furthermore,\nintegrating the dither noise in our coder allows a faster recognition of the\nfine details of the image during the decoding process. Our present paper goal\nis twofold. First, we aim at mimicking as closely as possible the retina for\nthe design of a novel image coder while keeping encouraging performances.\nSecond, we bring a new insight concerning the non-deterministic behavior of the\nretina.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.4673%2C1202.2703%2C1202.2477%2C1202.3682%2C1202.5899%2C1202.3114%2C1202.3817%2C1202.4905%2C1202.1794%2C1202.2298%2C1202.3060%2C1202.5838%2C1202.6478%2C1202.4313%2C1202.6583%2C1202.3601%2C1202.0694%2C1202.1593%2C1202.6220%2C1202.6606%2C1202.1418%2C1202.5364%2C1202.6120%2C1202.2184%2C1202.5400%2C1202.5352%2C1202.0344%2C1202.5969%2C1202.6624%2C1202.3973%2C1202.5024%2C1202.1682%2C1202.0723%2C1202.2370%2C1202.2531%2C1202.6064%2C1202.4935%2C1202.5429%2C1202.5373%2C1202.0002%2C1202.5295%2C1202.5748%2C1202.4504%2C1202.1093%2C1202.1444%2C1202.0605%2C1202.5256%2C1202.3132%2C1202.6179%2C1202.5536%2C1202.0887%2C1202.0992%2C1202.0153%2C1202.5702%2C1202.2368%2C1202.6295%2C1202.3379%2C1202.1948%2C1202.2433%2C1202.1292%2C1202.2350%2C1202.2932%2C1202.5578%2C1202.1831%2C1202.2157%2C1202.5680%2C1202.3366%2C1202.5099%2C1202.5661%2C1202.5436%2C1202.2171%2C1202.2429%2C1202.0094%2C1202.6264%2C1202.0937%2C1202.3960%2C1202.2160%2C1202.6201%2C1202.5415%2C1202.3307%2C1202.3652%2C1202.5572%2C1202.5461%2C1202.2535%2C1202.2000%2C1202.0667%2C1202.3851%2C1202.3426%2C1202.0296%2C1202.0015%2C1202.1959%2C1202.1380%2C1202.0442%2C1202.4349%2C1202.6239%2C1202.4434%2C1202.6568%2C1202.6585%2C1202.6681%2C1202.4230%2C1202.4496&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose the design of an original scalable image coder/decoder that is\ninspired from the mammalians retina. Our coder accounts for the time-dependent\nand also nondeterministic behavior of the actual retina. The present work\nbrings two main contributions: As a first step, (i) we design a deterministic\nimage coder mimicking most of the retinal processing stages and then (ii) we\nintroduce a retinal noise in the coding process, that we model here as a dither\nsignal, to gain interesting perceptual features. Regarding our first\ncontribution, our main source of inspiration will be the biologically plausible\nmodel of the retina called Virtual Retina. The main novelty of this coder is to\nshow that the time-dependent behavior of the retina cells could ensure, in an\nimplicit way, scalability and bit allocation. Regarding our second\ncontribution, we reconsider the inner layers of the retina. We emit a possible\ninterpretation for the non-determinism observed by neurophysiologists in their\noutput. For this sake, we model the retinal noise that occurs in these layers\nby a dither signal. The dithering process that we propose adds several\ninteresting features to our image coder. The dither noise whitens the\nreconstruction error and decorrelates it from the input stimuli. Furthermore,\nintegrating the dither noise in our coder allows a faster recognition of the\nfine details of the image during the decoding process. Our present paper goal\nis twofold. First, we aim at mimicking as closely as possible the retina for\nthe design of a novel image coder while keeping encouraging performances.\nSecond, we bring a new insight concerning the non-deterministic behavior of the\nretina."}, "authors": ["Khaled Masmoudi", "Marc Antonini", "Pierre Kornprobst"], "author_detail": {"name": "Pierre Kornprobst"}, "author": "Pierre Kornprobst", "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:1104.1550", "links": [{"href": "http://arxiv.org/abs/1202.2350v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.2350v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.2350v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.2350v1", "journal_reference": null, "doi": null, "fulltext": "Streaming an image through the eye:\nThe retina seen as a dithered scalable image coder\nKhaled Masmoudia,1,\u2217, Marc Antoninia,1 , Pierre Kornprobstb,2\na 2000\n\narXiv:1202.2350v1 [cs.CV] 10 Feb 2012\n\nb 2004\n\nroute des lucioles 06903 Sophia Antipolis, France\nroute des lucioles 06902 Sophia Antipolis, France\n\nAbstract\nWe propose the design of an original scalable image coder/decoder that is inspired from\nthe mammalians retina. Our coder accounts for the time-dependent and also nondeterministic behavior of the actual retina. The present work brings two main contributions: As a first step, (i) we design a deterministic image coder mimicking most\nof the retinal processing stages and then (ii) we introduce a retinal noise in the coding\nprocess, that we model here as a dither signal, to gain interesting perceptual features.\nRegarding our first contribution, our main source of inspiration will be the biologically\nplausible model of the retina called Virtual Retina. The coder that we propose has two\nstages. The first stage is an image transform which is performed by the outer layers of\nthe retina. Here we model it by filtering the image with a bank of difference of Gaussians with time-delays. The second stage is a time-dependent analog-to-digital conversion\nwhich is performed by the inner layers of the retina. The main novelty of this coder is\nto show that the time-dependent behavior of the retina cells could ensure, in an implicit\nway, scalability and bit allocation. Regarding our second contribution, we reconsider the\ninner layers of the retina. We emit a possible interpretation for the non-determinism\nobserved by neurophysiologists in their output. For this sake, we model the retinal noise\nthat occurs in these layers by a dither signal. The dithering process that we propose\nadds several interesting features to our image coder. The dither noise whitens the reconstruction error and decorrelates it from the input stimuli. Furthermore, integrating the\ndither noise in our coder allows a faster recognition of the fine details of the image during\nthe decoding process. Our present paper goal is twofold. First, we aim at mimicking as\nclosely as possible the retina for the design of a novel image coder while keeping encouraging performances. Second, we bring a new insight concerning the non-deterministic\nbehavior of the retina.\n\n1. Introduction\nResearch in still image compression yielded several coding algorithms as the JPEG\nstandards [2, 5]. Though, these algorithms follow for their most a characteristic design\n\u2217 Corresponding\n\nauthor\nlaboratory\n2 INRIA\u2013NeuroMathComp project team\nPreprint submitted to arXiv\n1 UNS\u2013CNRS\u2013I3S\n\nOctober 29, 2018\n\n\fschema. Namely, three main stages are considered. First, a transform is applied to the\nimage. Second, the transformed data is quantized. Finally, an entropic coder compresses\nthe bitstream to be transmitted. Interestingly, we retrieve a similar behavior in the\nmammalians retina [7]. Indeed, accumulated neurophysiologic evidence showed that the\nretina applies a transform to the image. Then, the retina binarizes the signal obtained\nto generate a set of uniformly shaped electrical impulses: the spikes [23]. Finally, several\nworks tend to confirm that the retina generates a compact code to be transmitted to the\nvisual cortex [22]. Thus, we are convinced that an interdisciplinary approach combining\nthe signal processing techniques and the knowledge acquired by neurophysiologists would\nlead to novel coding algorithms beyond the standards.\nIn order to design a novel bio-inspired image coder, we need to capture the main\nproperties of the retina processing. Though the action of perception seems effortless,\nneurophysiological experiments proved that the mechanisms involved in the retina are\nhighly complex and demanding. Recent studies such as [11] confirmed that the retina is\ndoing non-trivial operations to the input signal before transmission to the visual cortex.\nBesides, the retina appears to be a non-deterministic system. The retinal code is characterized by random fluctuations. Indeed, recordings made at the output of the retina\nshow that a single stimulus leads to different codes across trials. So that we have to deal\nwith two issues. The first one is the complexity of the retinal processing and the need\nto decipher the code generated. The second issue is the non-determinism of the retinal\nresponse and its possible perceptual role. Therefore, our present paper will be structured\naround two parts.\nIn a first step, our goal is to reproduce the main stages of the retina processing for\nthe design of a retina-inspired coder with a deterministic behavior. Our main source of\ninspiration will be the bio-plausible Virtual Retina model [36] whose goal was to find the\nbest compromise between the biological reality and the possibility to make large-scale\nsimulations. Based on this model, we propose a coding scheme following the architecture\nand functionalities of the retina. Unlike most of the efforts mimicking the retina behavior,\nhere we focus explicitly on the coding application. So that, we do some adaptations to\nthe model in order to be able to conceive a decoding scheme and retrieve the original\nstimulus. We keep our design as close as possible to biological reality taking into account\nthe retina processing complexity, while keeping an interesting rate/distortion trade-off.\nThe coder/decoder that we design offers several interesting features such as scalability.\nIn a second step, we tackle the issue of reproducing the trial-to-trial variability in\nthe retina and its possible role. Here, we make the hypothesis that the non-determinism\nobserved is a dithering process. So that, we identify the deep retina layers behavior\nto a non-subtractive dithered A/D converter. We elaborate a multiscale model for the\ndistribution of the dither noise that we integrate in our coder. The hypothesis that we\nemit is seducing because of the perceptual impact it induces. Still, our model obeys the\nbiological plausibility constraint. Interestingly, the dither noise provides our coder with\nperceptual properties such as the enhancement of the image contours and singularities\nas well as the reconstruction error whitening.\nThis paper is organized into two parts. The first part consists of the Sections 2 to 5 and\npresents the design of our bio-inspired scalable image coder/decoder with a deterministic\nbehavior. This part is organized as follows. In Section 2 we revisit the retina model\n2\n\n\fcalled Virtual Retina [36]. In Section 3, we show how this retina model can be used\nas the basis of a novel bio-inspired image coder. In Section 4, we present the decoding\npathway. In Section 5, we show the main results that demonstrate the properties of\nour model. The second part consists of the Sections 6 to 7. In Section 6, we show how\nwe integrated the dithering process in our coder/decoder. Then, in Section 7, we detail\nthe perceptual impact of it. We show, on two test images with different properties, the\nability of our dithered scalable coder to accelerate the recognition of the image details\nand singularities during the decoding process. Finally, in Section 8, we summarize our\nmain conclusions.\n2. Virtual Retina: A bio-plausible retina model\nOne first motivation for our work is to investigate the retina functional architecture\nand use it as a design basis to devise new codecs. So, it is essential to understand what are\nthe main functional principles of the retina processing. The literature in computational\nneuroscience dealing with the retina proposes different models (see, e.g., [35] for a review).\nThese models are very numerous, ranking from detailed models of a specific physiological\nphenomenon, to large-scale models of the whole retina.\nIn this article, we focused on the category of large-scale models as we are interested in\na model that gathers the main features of the mammalians retina. Within this category,\nwe considered the retina model called Virtual Retina [36]. This model is one of the\nmost complete ones, in the literature, as it encompasses the major features of the actual\nmammalians retina. This model is mostly state-of-the-art and the authors confirmed its\nrelevance by reproducing accurately actual retina cell recordings for several experiments.\nThe architecture of the Virtual Retina model follows the structure of the mammalians\nretina as schematized in Figure 1(a). The model has several interconnected layers and\nthree main processing steps can be distinguished:\n\u2022 The outer layers: The first processing step is described by non-separable spatiotemporal filters, behaving as time-dependent edge detectors. This is a classical step\nimplemented in several retina models.\n\u2022 The inner layers: A non-linear contrast gain control is performed. This step models\nmainly bipolar cells by control circuits with time-varying conductances.\n\u2022 The ganglionic layer: Leaky integrate and fire neurons are implemented to model\nthe ganglionic layer processing that finally converts the stimulus into spikes.\nGiven this model as a basis, our goal is to adapt it to conceive the new codec presented\nin the next sections.\n3. The coding pathway\nThe coding pathway is schematized in Figure 1(b). It follows the same architecture\nas Virtual Retina. However, since we have to define also a decoding pathway, we need to\nthink about the invertibility of each processing stage. For this reason some adaptations\nare required and described in this section. The coder design, presented in this section,\nis an enhancement of our previous effort in [17].\n3\n\n\fFigure 1: (a) Schematic view of the Virtual Retina model proposed by [36]. (b) and (c): Overview\nof our bio-inspired codec. Given an image, the static DoG-based multi-scale transform generates the\nsubbands {Fk }. DoG filters are sorted from the lowest frequency-band filter DoG0 to the highest one\nDoGN \u22121 . Each subband Fk is delayed using a time-delay circuit Dtk , with tk < tk+1 . The time-delayed\nmulti-scale output is then made available to the subsequent coder stages. The final output of the coder\nis a set of spike series, and the coding feature adopted will be the spike count nkij (tobs ) recorded for\neach neuron indexed by (kij) at a given time tobs .\n\n4\n\n\f3.1. The image transform: The outer layers of the retina\nIn Virtual Retina, the outer layers were modelled by a non-separable spatio-temporal\nfiltering. This processing produces responses corresponding to spatial or temporal variations of the signal because it models time-dependent interactions between two low-pass\nfilters: this is termed center-surround differences. This stage has the property that\nit responds first to low spatial frequencies and later to higher frequencies. This timedependent frequency integration was shown for Virtual Retina [37] and it was confirmed\nexperimentally (see, e.g., [26]). This property is interesting as a large amount of the total\nsignal energy is contained in the low frequencies subbands, whereas high frequencies bring\nfurther details. This idea already motivated bit allocation algorithms to concentrate the\nresources for a good recovery on lower frequencies.\nHowever, it appears that inverting this non-separable spatio-temporal filtering is a\ncomplex problem [37, 38]. To overcome this difficulty, we propose to model differently\nthis stage while keeping its essential features. To do so, we decomposed this process into\ntwo steps: The first one considers only center-surround differences in the spatial domain\n(through differences of Gaussians) which is justified by the fact that our coder here\ngets static images as input. The second step reproduces the time-dependent frequency\nintegration by the introduction of time-delays.\nCenter-surround differences in the spatial domain: The DoG model\nNeurophysiologic experiments have shown that, as for classical image coders, the\nretina encodes the stimulus representation in a transform domain. The retinal stimulus\ntransform is performed in the cells of the outer layers, mainly in the outer plexiform layer\n(OPL). Quantitative studies such as [10, 24] have proven that the OPL cells processing\ncan be approximated by a linear filtering. In particular, the authors in [10] proposed\nthe largely adopted DoG filter which is a weighted difference of spatial Gaussians that is\ndefined as follows:\nDoG(x, y) = wc G\u03c3c (x, y) \u2212 ws G\u03c3s (x, y),\n(1)\nwhere wc and ws are the respective weights of the center and surround components of\nthe receptive fields, and \u03c3c and \u03c3s are the standard deviations of the Gaussian kernels\nG\u03c3c and G\u03c3s .\nIn terms of implementation, as in [28], the DoG cells can be arranged in a dyadic grid\nto sweep all the stimulus spectrum as schematized in Figure 2(a). Each layer k in the\ngrid, is tiled with DoGk cells having a scale k and generating a transform subband Fk ,\nwhere \u03c3sk+1 = 21 \u03c3sk and \u03c3ck+1 = 21 \u03c3ck . So, in order to measure the degree of activation\nopl\nI \u0304kij\nof a given DoGk cell at the location (i, j) with a scale k, we compute the convolution\nof the original image f by the DoGk filter:\nopl\nI \u0304kij\n=\n\n\u221e\nX\n\nDoGk (i \u2212 x, j \u2212 y) f (x, y).\n\n(2)\n\nx,y=\u2212\u221e\n\nThis transform generates a set of ( 34 N 2 \u2212 1) coefficients for an N 2 -sized image, as it\nworks in the same fashion as a Laplacian pyramid [4]. An example of such a bio-inspired\nmulti-scale decomposition is shown in Figure 2(b). Note here that we added to this bank\nof filters a Gaussian low-pass scaling function that represents the state of the OPL filters\nopl\nand enables the recovery of a\nat the time origin. This yields a low-pass coefficient I \u0304000\nlow-pass residue at the reconstruction level [8, 19].\n5\n\n\f(a)\n\n(b)\n\n(c)\n\nFigure 2: (a) Input image cameraman. (b) Example of a dyadic grid of DoG's used for the image\nanalysis (from [28]). (c) Example on image (a) of DoG coefficients generated by the retina model (the\nsubbands are shown in the logarithmic scale)\n\nIntegrating time dynamics through time-delay circuits\nOf course, the model described in (2) has no dynamical properties. In the actual\nretina, the surround G\u03c3s in (1) appears progressively across time driving the filter passband from low frequencies to higher ones. Our goal is to reproduce this phenomenon\nthat we called time-dependent frequency integration. To do so, we added in the coding\npathway of each subband Fk a time-delay circuit Dtk . The value of tk is specific to Fk\nand is an increasing function of k. The tk delay causes the subband Fk to be transmitted to the subsequent stages of the coder starting from the time tk . The time-delayed\nopl\nactivation coefficient Ikij\n(t) computed at the location (i, j) for the scale k at time t is\nnow defined as follows:\nopl\nopl\nIkij\n(t) = I \u0304kij\n1{t>tk } (t),\n(3)\nwhere 1{t>tk } is the indicator function such that, 1{t>tk } (t) = 0 if t < tk and 1 otherwise.\nWhile in our previous work [17] tk is increasing linearly as a function of k, we changed\nthe law governing tk to an exponential one with a time constant denoted by \u03c4 opl . This\nchange is intended to bring more biological plausibility to our new coder as the time\nbehavior of the outer layers cells is exponential [10, 36]. Indeed, in the actual retina,\nthe passband of the DoG cells runs through the low frequencies at a fast pace, then\ndecelerates in an exponential fashion. So, the time-dependent frequency integration is\nnot a linear phenomenon. The evolution of time delays tk with respect to the scale k, in\nthe present work, is detailed in Figure 3.\n3.2. The A/D converter: The inner and ganglionic layers of the retina\nThe retinal A/D converter is defined based on the processing occurring in the inner\nand ganglionic layers, namely a contrast gain control, a non-linear rectification and a\ndiscretization based on leaky integrate and fire neurons (LIF) [15]. A different treatment\nwill be performed for each delayed subband, and this produces a natural bit allocation\nmechanism. Indeed, as each subband Fk is presented at a different time tk , it will be\nsubject to a transform according to the state of our dynamic A/D converter at tk .\n\n6\n\n\fFigure 3: Time delays Dtk introduced in the coding process. The time-dependent frequency integration\nis reproduced by delaying the coding process start of the subband Fk by tk . The series tk is represented\nas a function of the scale k. The progression law is exponential with a time constant \u03c4 opl = 65 ms.\n\n3.2.1. Contrast gain control\nThe retina adjusts its operational range to match the input stimuli magnitude range.\nThis is done by an operation called contrast gain control mainly performed in the bipolar\ncells. Indeed, real bipolar cells conductance is time varying, resulting in a phenomenon\ntermed as shunting inhibition. This shunting avoids the system saturation by reducing\nhigh magnitudes.\nopl\nopl\nIn Virtual Retina, given the scalar magnitude I \u0304kij\nof the input step current Ikij\n(t), the\ncontrast gain control is a non-linear operation on the potential of the bipolar cells. This\nopl\npotential varies according to both the time and the magnitude value I \u0304kij\n; and will be\nopl\nopl\nb\n \u0304\ndenoted by Vkij (t, Ikij ). This phenomenon is modelled, for a constant value of I \u0304kij\n, by\nthe following differential equation:\n\uf8f1\nopl\nb\n\uf8f4\n\uf8f2 b dVkij (t, I \u0304kij )\nopl\nopl\nb\nc\n+ g b (t)Vkij\n(t, I \u0304kij\n) = Ikij\n(t), for t > 0,\n(4)\ndt\n\uf8f4\nt\n\uf8f3 g b (t) = E b \u2217 Q(V b (t, I \u0304opl )),\n\u03c4\nkij\nkij\n\u0010\n\u00112\n\u2212t\n1\nb\nb\n(t) and E\u03c4 b = b exp \u03c4 b , for t > 0. Figure 4(a) shows\nwhere Q(Vkij\n) = g0b + \u03bbb Vkij\n\u03c4\nthe time behavior of V b (t, I \u0304opl ) for different magnitude values I \u0304opl of I opl (t).\nkij\n\nkij\n\nkij\n\nkij\n\n3.2.2. Non-linear rectification\nopl\nb\nIn the next processing step, the potential Vkij\n(t, I \u0304kij\n) is subject to a non-linear recg\ntification yielding the so-called ganglionic current I (t, I \u0304opl ). Virtual Retina models it,\nkij\n\nkij\n\nopl\nfor a constant scalar value I \u0304kij\n, by:\n\u0010\n\u0011\ng\nopl\nopl\nb\nIkij\n(t, I \u0304kij\n) = N Twg ,\u03c4 g (t) \u2217 Vkij\n(t, I \u0304kij\n) ,\n\nfor t > 0,\n\n(5)\n\nwhere wg and \u03c4 g are constant scalar parameters, Twg ,\u03c4 g is the linear transient filter\ndefined by Twg ,\u03c4 g = \u03b40 (t) \u2212 wg E\u03c4 g (t), and N is defined by:\n\uf8f1\nig0\n\uf8f2\n, if v < v0g\ng\nN (v) =\ni0 \u2212 \u03bbg (v \u2212 v0g )\n\uf8f3 g\ni0 + \u03bbg (v \u2212 v0g ), if v > v0g ,\n7\n\n\f(a)\n\n(b)\n\n(c)\n\n(d)\n\nb (t) as a function of time for different values of I \u0304opl ; 4(b): I g\nFigure 4: 4(a): Vkij\nkij as a function of time\nopl\ng\nfor different values of I \u0304opl ; 4(c): The functions ftgk that map I \u0304kij\ninto Ikij\nfor different values of tk ;\n4(d): The functions f n that map I \u0304r into nkij for different values of tobs\ntobs\n\nkij\n\nwhere ig0 , v0g , and \u03bbg are constant scalar parameters. Figure 4(b) shows the time behavior\ng\nopl\nopl\nof Ikij\n(t, I \u0304kij\n) for different values of I \u0304kij\n.\nopl\nAs the currents I \u0304kij are delayed with times {tk }, our goal is to catch the instantaneous\nbehavior of the inner layers at these times {tk }. This amounts to infer the transforms\nopl\nopl\nr\nItgk (I \u0304kij\n) that maps a given scalar magnitude I \u0304kij\ninto a rectified current I \u0304kij\nas the\nmodelled inner layers would generate it at tk . To do so, we start from the time-varying\ng\nopl\ncurves of Ikij\n(t, I \u0304kij\n) in Figure 4(b) and we do a transversal cut at each time tk . We\nshow in Figure 4(c) the resulting maps ftg such that I g (tk , I \u0304opl ) = ftg (I \u0304opl ).\nk\n\nkij\n\nkij\n\nk\n\nkij\n\nopl\nAs for Ikij\n(t) (see Equation (3)), we introduce the time dimension using the indicator\nr\nfunction 1{t>tk } (t). The final output of this stage is the set of step functions Ikij\n(t)\ndefined by:\nopl\nr\nr\nr\nIkij\n(t) = I \u0304kij\n1{t>tk } (t), with I \u0304kij\n= ftgk (I \u0304kij\n).\n\n(6)\n\nThis non-linear rectification is analogous to a widely-used telecommunication technique: the companding [6]. Companders are used to make the quantization steps unequal\n8\n\n\fafter a linear gain control stage. Though, unlike A \u2212 law or \u03bc \u2212 law companders that\namplify low magnitudes, the inner layers emphasize high magnitudes in the signal. Indeed, the bio-inspired compander, defined here, emphasizes high energy signals rather\nthan high probability ones. This tendency is accentuated as the gain control gets higher\nacross time. Besides, the inner layers stage have a time dependent behavior, whereas a\nusual gain controller/compander is static, and this makes our A/D converter go beyond\nthe standards.\n3.2.3. Leaky integrate-and-fire quantization:\nThe ganglionic layer is the deepest one tiling the retina: it transforms a continuous\nr\nsignal Ikij\n(t) into discrete sets of spike trains. As in Virtual Retina, this stage is modelled\nby leaky integrate and fire neurons (LIF) which is a classical model. One LIF neuron is\nassociated to every position in each subband Fk . The time-behavior of a LIF neuron is\ngoverned by the fluctuation of its voltage Vkij (t). Whenever Vkij (t) reaches a predefined\n\u03b4 threshold, a spike is emitted and the voltage goes back to a resting potential VR0 .\n(l)\n(l+1)\nBetween two spike emission times, tkij and tkij , the potential evolves according to the\nfollowing differential equation:\ncl\n\ndVkij (t)\nr\n+ g l Vkij (t) = Ikij\n(t),\ndt\n\n(l)\n\n(l+1)\n\nfor t \u2208 [tkij , tkij ],\n\n(7)\n\nwhere g l is a constant conductance, and cl is a constant capacitance. In the literature,\nneurons activity is commonly characterized by the count of spikes emitted during an\nobservation time bin [0, tobs ], which we denote by nkij (tobs ) [34]. Obviously, as nkij (tobs )\nr\nencodes for the value of Ikij\n(t), there is a loss of information as nkij (tobs ) is an integer.\nThe LIF is thus performing a quantization. If we observe the instantaneous behavior of\nthe ganglionic layer at different times tobs , we get a quasi-uniform scalar quantizer that\nrefines in time. We can do this by a similar process to the one described in the previous\nparagraph. We show in Figure 4(d) the resulting maps ftnobs such that:\nr\nnkij (tobs ) = ftnobs (I \u0304kij\n).\n\n(8)\n\nBased on the set of spike counts {nkij (tobs )}, measured at the output of our coder,\nwe describe in the next section the decoding pathway to recover the initial image f (x, y).\n4. The decoding pathway\nThe decoding pathway is schematized in Figure 1(c). It consists in inverting, step by\nstep, each coding stage described in Section 3. At a given time tobs , the coding data is\nthe set of ( 34 N 2 \u2212 1) spike counts nkij (tobs ), this section describes how we can recover\nan estimation f \u0303tobs of the N 2 -sized input image f (x, y). Naturally, the recovered image\nf \u0303tobs (x, y) depends on the time tobs which ensures time-scalability: the quality of the\nreconstruction improves as tobs increases. The ganglionic and inner layers are inverted\nusing look-up tables constructed off-line and the image is finally recovered by a direct\nreverse transform of the outer layers processing.\n\n9\n\n\fRecovering the input of the ganglionic layer:\nr\nr\nFirst, given a spike count nkij (tobs ), we recover I \u0303kij\n(tobs ), the estimation of Ikij\n(tobs ).\nr\n \u0304\nTo do so, we compute off-line the look-up table ntobs (Ikij ) that maps the set of current\nr\nmagnitude values I \u0304kij\ninto spike counts at a given observation time tobs (see Figure 4(d)).\nThe reverse mapping is done by a simple interpolation in the reverse-look up table denoted LU TtLIF\n. Here we draw the reader's attention to the fact that, as the input of the\nobs\nganglionic layer is delayed, each coefficient of the subband Fk is decoded according to\nthe reverse map LU TtLIF\n. Obviously, the recovered coefficients do not match exactly\nobs \u2212tk\nthe original ones due to the quantization performed in the LIF's.\nRecovering the input of the inner layers:\nopl\nr\nSecond, given a rectified current value I \u0303kij\n(tobs ), we recover I \u0303kij\n(tobs ), the estimation\nopl\nof Ikij\n(tobs ). In the same way as for the preceding stage, we infer the reverse \"inner\n. The current intenlayers mapping\" through the pre-computed look up table LU TtCG\nobs\nopl\n \u0303\nsities Ikij (tobs ), corresponding to the retinal transform coefficients, are passed to the\nsubsequent retinal transform decoder.\n\nRecovering the input stimulus:\nopl\n(tobs )}, we recover f \u0303tobs (x, y), the\nFinally, given the set of ( 43 N 2 \u2212 1) coefficients {I \u0303kij\nestimation of the original image stimulus f (x, y). Though the dot product of every pair\nof DoG filters is approximately equal to 0, the set of filters considered is not strictly\northonormal. We proved in [18] that there exists a dual set of vectors enabling an exact\nreconstruction. Hence, the reconstruction estimate f \u0303 of the original input f can be\nobtained as follows:\nX opl\n] k (i \u2212 x, j \u2212 y),\nf \u0303tobs (x, y) =\nI \u0303kij (tobs ) DoG\n(9)\n{kij}\n\nwhere {kij} is the set of possible scales and locations in the considered dyadic grid and\n] k are the duals of the DoGk filters obtained as detailed in [18]. Equation (9) defines\nDoG\na progressive reconstruction depending on tobs . This provides our code with an important\nfeature: the scalability. Despite the fact that the input of our coder is a static image, we\nwill be referring to this feature as time-scalability. Indeed, in our case different levels of\nrate and quality levels are achievable thanks to the observation time tobs .\n5. Results: Case of the bio-inspired and noiseless scalable image coder\nWe show examples of image reconstruction using our bio-inspired coder at different\ntimes3 . Then, we study these results in terms of quality and bit-cost.\nQuality is assessed by classical image quality criteria (PSNR and mean SSIM [31]). The\n3 In all experiments, the model parameters are set to biologically realistic values: g b = 8 10\u2212 10 S,\n0\n\u03c4 b = 12 10\u22123 s, \u03bbb = 9 10\u22127 , cb = 1.5 10\u221210 F , v0g = 4 10\u22123 V , ig0 = 15 10\u2212 12 A, wg = 8 10\u2212 1, \u03c4 g =\n16 10\u22123 s; \u03bbg = 12 10\u22129 S, \u03b4 = 2 10\u22123 V, g L = 2 10\u22129 S, VR0 = 0 V , t0 = 10 10\u22123 s, tK\u22121 = 38 10\u22123 s,\n\u03c4 opl = 65 10\u22123 s.\n\n10\n\n\fcost is measured by the Shannon entropy H(tobs ) upon the population of {nkij (tobs )}.\nThe entropy computed in bits per pixel (bpp), for an N 2 -sized image, is defined by:\nH(tobs ) =\n\nK\u22121\no\u0011\n1 X 2k \u0010n\n2\nk\n2\n,\nH\nn\n(t\n),\n(i,\nj)\n\u2208\nJ0,\n2\n\u2212\n1K\nsk ij obs\nN2\n\n(10)\n\nk=0\n\nwhere K is the number of analyzing subbands. Figure 5 shows two examples of progressive reconstruction obtained with our new coder. Bit-rate/Quality are computed for\neach image in terms of the triplet (bit-rate in bpp/ PSNR quality in dB/ mean SSIM\nquality). Progressive reconstruction of cameraman in the left column yields: From top\nto bottom (0.006 bpp/ 16.02 dB/ 0.48), (0.077 bpp/ 18.34 dB/ 0.55), (0.23 bpp/ 21.20\ndB/ 0.65), and (1.39 bpp/ 26.30 dB/ 0.84). Progressive reconstruction of baboon in the\nright column yields: From top to bottom (0.037 bpp/ 16.98 dB/ 0.18), (0.32 bpp/ 19.07\ndB/ 0.35), (0.63 bpp/ 20.33 dB/ 0.49), and (2.24 bpp/ 27.37 dB/ 0.92).\nThe new concept of time scalability is an interesting feature as it introduces time dynamics in the design of the coder. Figure 6 illustrates this concept. This is a consequence\nof the mimicking of the actual retina. We also notice that, as expected, low frequencies\nare transmitted first to get a first approximation of the image, then details are added\nprogressively to draw its contours. The bit-cost of the coded image is slightly high. This\ncan be explained by the fact that Shannon entropy is not the most relevant metric in our\ncase as no context is taken into consideration, especially the temporal context. Indeed,\none can easily predict the number of spikes at a given time t knowing nkij (t \u2212 dt). Note\nalso that no compression techniques, such that bit-plane coding, are yet employed. Our\npaper aims mainly at setting the basis of new bio-inspired coding designs.\nFor the reasons cited above, the performance of our coding scheme in terms of bit-cost\nhave still to be improved to be competitive with the well established JPEG and JPEG\n2000 standards. Thus we show no comparison in this paper. Though primary results are\nencouraging, noting that optimizing the bit-allocation mechanism and exploiting coding\ntechniques as bit-plane coding [27] would improve considerably the bit-cost. Besides,\nthe image as reconstructed with our bio-inspired coder shows no ringing and no block\neffect as in JPEG. Finally our codec enables scalability in an original fashion through\nthe introduction of time dynamics within the coding mechanism.\nNote also that differentiation in the processing of subbands, introduced through timedelays in the retinal transform, ensures an implicit bit-allocation mechanism. In particular the non-linearity in the inner layers stage amplifies singularities and contours, and\nthese provide crucial information for the analysis of the image.\n6. Introducing the noise in the coder: The non-subtractive dither hypothesis\nIn the preceding Sections 3 and 4, we presented the design of an image coder based\non a bio-plausible model of the retina. We especially emphasized the deep retina layers\nanalogy with A/D converters. Despite the fact that our coder takes into account several\nfeatures of the actual retina as its time-dependent behavior, still it follows a deterministic\nlaw. Though, the actual neural code of the retina is clearly non-deterministic [9, 25].\nThus, in this section, we tackle the issue of the coding non-determinism in the retina.\nWe make the proposal that the processing stages prior to the ganglionic layer yield a\n11\n\n\fBaboon reconstruction\n\n50 ms\n\n40 ms\n\n30 ms\n\n20 ms\n\nCameraman reconstruction\n\nFigure 5: Progressive image reconstruction of cameraman and baboon using our new bio-inspired coder.\nThe coded/decoded image is shown at: 20 ms, 30 ms, 40 ms, and 50 ms.\n\n12\n\n\f(a) Bit-rate as a function of time\n\n(b) Quality as a function of time\n\nFigure 6: Illustration of the concept of time scalability. the test image is cameraman. 6(a) shows the bitrate variation of the encoded image as a function of the observation time tobs . The bit-rate is measured\nby means of the entropy in bits per pixel (bpp). 6(b) shows the reconstruction quality variation as a\nfunction of the observation time tobs . The quality is measured by means of the mean structural similarity\nindex (mean SSIM). The only parameter that is tuned by the user is tobs . Both quality and cost increase\nin accordance with tobs . We talk about time-scalability.\n\nspecial type of noise: the dither noise. We then experience the perceptual impact of such\na noise in our coder and give an original and plausible interpretation of its role in the\nstimuli coding process.\n6.1. The non-deterministic nature of the neural code of the retina\nOne major issue encountered by neuroscientists is the non-determinism of the retinal\nneural code. Indeed, given a single visual stimulus, spikes timings in the retina output are\nnot exactly reproducible across trials. Yet, no clear evidence is established about the phenomena at the origin of this trial-to-trial variability. Several hypotheses were discussed\nin the literature and yielded two different points of view. The first hypothesis is that the\nprecise timings of individual spikes convey a large amount of information [21, 20]. This\nhypothesis suggests that the stimulus coding process in the retina is deterministic and reports detailed information about the stimulus with a high temporal fidelity. In this case,\neach single spike timing makes sense. The second hypothesis is that only a few statistical\nquantities measured over the spike-based output convey the relevant information about\nthe stimulus to the visual cortex [3]. For instance, since [1] it was widely assumed that\nthe variable spike patterns corresponding to a single stimulus are random instantiations\nof a desired firing rate. In this case, the precise timing of each single spike may not be\nmeaningful and thus spikes may carry some amount of noise. The spike based-output\nshould then be averaged to reveal meaningful signals [9].\nThe role of spikes timings variability in the neural code of the retina is still an open\nissue and no clear evidence establishes whether this variability conveys precise information or random noise [25]. Here, we make the proposal that the non-determinism in\nthe retinal processing prior to the ganglionic layer yields a dither noise [14, 16]. This\n13\n\n\fnoise, while corrupting the input of the ganglionic layer by a completely random signal,\nbrings interesting features to the spike-based output of the retina. We will notice that\nthe dithering process helps us to recognize the fine details of the stimulus earlier than\nwith the noiseless coder. For this to be possible, the distribution of the noise that we\nintroduce obeys specific constraints defined in [33]. Obviously, the dither noise hypothesis is one possible assumption among several others and we do not claim its biological\nexactness. Still, our present effort aims at bridging the differences between the different\npoints of view reported above by exploring the hypothesis of a \"retinal useful noise\".\n6.2. Multiscale non-subtractive dithering\nWe introduce in this section a multiscale dithering process that will be integrated\nin our bio-inspired image coder. Indeed, the coder that we designed has a multiscale\narchitecture. So that the dither noise to be introduced must take into consideration the\ndifferent scales of the retina model cells used for the image analysis.\nWe will assume that the processing stages of the retina that precede the ganglionic\nlayer introduce a noise. As this noise is prior to the quantization done in the ganglionic\nlayer, it is referred to as a dither noise. As, furthermore, this dither noise takes into\nconsideration the multiscale architecture of the retina model, we will be talking about\na multiscale dithering. The present work extends our previous efforts in [14, 16] to the\nmultiscale case.\nA few techniques referred to as multiscale dithering have been described in the literature. For example, in [30] the authors considered a hierarchical wavelet transform. The\nsibling subbands, ie. lying in the same level, are decorrelated by applying a series of rotations. The transform applied on the subbands is loosely referred to as dithering because\nit introduces a change on the wavelet coefficients prior to quantization. The resulting\nimage is meant to reduce entropy while keeping the same perceptual quality. Another\nexample is given in [12]. The authors used an image hierarchical quadtree representation\nand employ an error diffusion algorithm to get a binary halftone image. The distribution\nof binary pixels over the image space gives the impression of a multi-gray level image\nwhile using only two quantization levels. Although interesting, these state-of-the-art\nalgorithms have one major drawback regarding the goals of our present work. Indeed,\nthe techniques described rely on a totally deterministic algorithm. No random behavior\nis introduced during the coding process. Whereas in our case, we need to consider a\ncoding process that may lead to different codes across trials for a single image. Besides\nthe two algorithms are iterative and time consuming and this is contradicts the speed of\nprocessing in the retina.\nr\nIn order to define the dither noise that corrupts the current Ikij\n(cf. Equation (6)) at\nthe input of the ganglionic layer, we reconsider the ganglion cell as a noisy leaky integrate\nand fire neuron (nLIF), that behaves according to the following equation:\n\ncl\n\ndVkij (t)\nr\n+ g l Vkij (t) = Ikij\n(t) + \u03b7kij ,\ndt\n\n(l)\n\n(l+1)\n\nfor t \u2208 [tkij , tkij ],\n\n(11)\n\nThe choice of the noise \u03b7kij distribution model to apply must obey two constraints:\nthe biological plausibility and the mathematical constraints that provide our coder with\ninteresting perceptual properties.\n14\n\n\fFirst, let us consider the biological plausibility constraint. Our aim is to mimic as\nclosely as possible the actual retina behavior while modelling the multiscale dithering\n\u03b7kij . So that, one must consider the nature of the dependency (if any) between the scale\nand the noise strength according to neurophysiologists observations. In this context,\nthe authors in [13] stated that: \"The main difference between small and large cells is\nthat the larger ones have lower peak sensitivity\". This means that the large retina cells\nhave a low reactivity to stimulus variations and thus are poorly affected by noise. On\nthe contrary, small cells are extremely sensitive to stimulus variations and thus could\nbe highly affected by noise. Our aim is to reproduce this phenomenon of noise strength\nr\nvariability as a function of retina cells scale. So that, we will corrupt the currents (Ikij\n) at\nthe input of the retina ganglionic layer with noise coefficients \u03b7kij , such that the dynamic\nrange of this noise distribution depends on the cells scale k. The larger the subband Fk\ncells are, the lower the noise dynamic range is. Thus, we will have to generate K noise\nsubbands, with an increasing dynamic range, to corrupt K subbands of rectified currents\nr\nIkij\n.\nSecond, let us consider the mathematical constraint. Indeed we must consider the\nstatistical properties that have to be verified by the added noise to provide our coder with\ninteresting perceptual features. To this end, we refer to the results established in [33],\nand recall the following fundamental theorem of dither noise distribution for the case of\na uniform scalar quantizer:\nTheorem 1. The choice of zero-mean dither probability distribution function (pdf ) which\nrenders the first and second moments of the total error independent of the input, such\nthat the first moment is zero and the second is minimized, is unique and is a triangular\npdf of 2 LSB peak-to-peak amplitude.\nThus, we suppose that (i) \u03b7kij has a triangular probability distribution function with no\nloss of biological plausibility, and (ii) that the dynamic range of \u03b7kij is twice wider than\nthe quantization step of the considered ganglion cell. Having these two conditions we\nverify the theorem. In this way, we identify the retinal noise \u03b7kij to a dither signal. As\nwe do not subtract the dither signal in the decoding process, our coder is said to be a\nnon-subtractive dithered system (NSD) [33, 32].\nAccording to the discussion above we will consider that the noise \u03b7kij dynamic range\n(i) is an increasing function of the scale k of the considered DoG retina cell, and (ii)\nis twice the width of the quantization step of the sussequent ganglion cell. Here we\nremind the reader that the ganglion cells are modelled, in our coder, by LIF neurons\nthat are dynamic quantizers. Indeed the ganglionic layer evolves from a coarse to a fine\nquantizer. The quantization step of a LIF neuron will be denoted Qlif . Obviously, Qlif\nis a decreasing function of the observation time tobs as shown in Figure 7. Furthermore,\naccording to our original retina transform (cf. Section 3.1), the coding process of each\nsubband Fk is delayed in time by tk . So that, the ganglion cells will have different\nlevels of progression at a given time tobs depending on the subband scale k. We set the\ndithering parameters for an optimal observation time t\u2217obs . So that, each subband will be\ncorrupted by a noise subband having a triangular pdf which dynamic range \u2206k depends\non the scale k such that:\n(\nQlif (t\u2217obs \u2212 tk ) = Q\u2217k\n(12)\n\u2206k = 2 Q\u2217k\n15\n\n\fFigure 7: Estimation of the LIF neuron quantization step Qlif as a function of the observation time\ntobs . The abscissa shows the observation times tobs between 0 ms and 100 ms. The ordinate axis shows\nthe mean quantization step Qlif estimated at a given tobs in Amperes.\n\nAn example of a multiscale dither noise thus defined is given in Figure 8. The test\nimage is cameraman and the optimal observation time chosen is t\u2217obs = 52 ms. The recr\ntified currents Ikij\nin each subband of scale k are subject to a dither noise \u03b7kij that\nhas a triangular distribution with a dynamic range \u2206k . We can notice that large cells\nin the low frequency subbands are poorly corrupted with noise while tight cells in the\nhigh frequency subbands are highly corrupted with noise. This is due to the fact that\n\u2206k+1 > \u2206k , \u22000 6 k < K \u2212 2. Interestingly, we remark that the time delays introduced\nin our model of the retinal transform allow us to implicitly satisfy the constraint of noise\ndynamic range \u2206k being an increasing function of the cells scale k.\n\n(a) Original rectified coefficients\n\n(b) Dithered rectified coefficients\n\nFigure 8: Example of dither noise introduced at the input of the ganglionic layer. The test image is\nr ).\ncameraman. 8(a) shows the noiseless rectified coefficients (Ikij\n8(b) shows the rectified coefficients\nr\n(Ikij + \u03b7kij ) with the dither noise \u03b7kij added. The noise parameters are set for the optimal observation\ntime is t\u2217obs = 52ms. The dither noise has a triangular distribution with a dynamic range \u2206k that\ndepends on the subband Fk considered. The larger the subband cells are, the lower the noise dynamic\nrange is. The high frequencies are more corrupted with noise than the low frequencies.\n\n16\n\n\fr\nAdding such a dither noise to the input of the ganglionic layer Ikij\ninduces interesting\nfeatures. As specified in the theorem above, one important feature is the decorrelation between the reconstruction error at the output of the de-quantizer and the original signal at\nthe input of the corresponding quantizer. The results of the theorem were demonstrated\nfor uniform scalar quantizers. Whereas in our coder the ganglionic layer is not strictly\na scalar quantizer but rather an approximation of it and, furthermore, the bio-inspired\nA/D converter that we designed is not uniform due to the preceding gain control and\nnon-linear rectification stages. So that, we must verify the relevance of our approach. As\nthe dithering process occurs in the DoG transform domain, we measure the error/input\ncorrelation in the transform domain. The error that we will denote by \u000fkij is defined, in\nopl\nthis case, as the difference between the output of the OPL layer Ikij\nand the estimation\nof it after decoding I \u0303opl , such that:\nkij\n\nopl\nopl\n\u000fkij = Ikij\n\u2212 I \u0303kij\n\n(13)\n\nopl\nWe can experimentally verify that, in fact, \u000fkij and the input stimuli Ikij\nare decorrelated.\nThis feature is clearly demonstrated when computing the cross correlation between \u000fkij\nopl\nand Ikij\nas shown in Figures 9(a) and 9(b) for the test image cameraman and the highest\nfrequency subband FK\u22121 . Comparable observations are made on the other subbands.\nopl\nFigure 9(a) shows the cross-correlation between \u000fkij and Ikij\nmeasured for the noiseless\ncase. The correlation is high especially when the spatial lag is small. 9(b) shows the same\ncross-correlation measures for the dithered case. We observe a very high decrease in the\ncorrelation even for the small spatial lags cases. Then, we can conclude that the signals\nopl\n\u000fkij and Ikij\nare clearly decorrelated.\nAnother perceptually important feature that is induced by the dithering process is\nthe error whitening. We verify also this feature in our case. As shown in Figures 9(c),\nthe spectrum of the error obtained when using our coder with no addition of noise is\nnon-uniform. This denotes strong geometric correlations in the error image which yields\nannoying artefacts. On the contrary, we notice in Figure 9(d) that the error spectrum is\nequally dispatched in the Fourier domain if we add a dither noise. Thus our new dithered\nscalable image coder gained interesting features through the integration of a dithering\nprocess.\n\nThe whitening and de-correlation features yield a greater reconstruction error in terms\nof mean squared error [33]. Though, the error whitening and decorrelation features acquired in the transform domain are perceptually important. Indeed, a strong correlation\nbetween the coding error and the original signal implies annoying artefacts. Besides the\nerror whitening is important because all frequencies are affected by the same noise. The\nperceptual impact of dithering on the final image reconstruction f \u0303tobs is shown in the\nnext section.\n7. Results: Case of the bio-inspired and dithered scalable image coder\nWe show in this section the perceptual impact of the dithering on the reconstructed\nimages using our decoder. Our experiments demonstrate the ability of the dither noise\n17\n\n\f(a) Noiseless case\n\n(b) Dithered case\n\n(c) Noiseless case\n\n(d) Dithered case\n\nFigure 9: Error whitening and decorrelation in the DoG transform domain induced by the dither noise\naddition. The results are shown for the highest frequency subband BK\u22121 , but comparable observations\nare made on the other subbands. The dither noise is introduced at the input of the ganglionic layer. The\ndither noise parameters are set for the optimal observation time is t\u2217obs = 52ms. 9(a) (respectively 9(b))\nopl\nshows the cross-correlation between \u000fkij and Ikij\nmeasured for the noiseless (resp. dithered) case. We\nobserve a very high decrease in the correlation induced by the noise. The error is decorrelated from the\ninput. 9(c) (respectively 9(d)) shows the amplitude spectrum of \u000fkij computed for the noiseless (resp.\ndithered) case. We observe a wide spreading of the error spectrum in Fourier domain induced by the\nnoise. The error is whitened.\n\n18\n\n\fto accelerate the recognition of the image details and singularities during the decoding\nprocess.\nA first example is given in Figure 10. The left column shows the evolution of the\nreconstruction f \u0303tobs with increasing times tobs , in the case of noiseless coding. The right\ncolumn shows the evolution of the reconstruction f \u0303tobs with increasing times tobs , in\nthe case of addition of a dither noise to the input of the ganglionic layer. The central\ncolumn shows a filtered version of cameraman. Cameraman is sharpened to enhance the\nimage details. The comparison between the noiseless case reconstruction (on the left)\nand the dithered reconstruction (on the right) demonstrates perceptual importance of\nnoise in the image coding process in the retina. With the addition of noise, details of\ncameraman are well rendered \"before date\". For example, the hand of cameraman and\nthe tower in the background appear since tobs = 44ms for the dithered case while still\ninvisible in the noiseless case at the same observation time. We can also notice that\nthe horizontal stripes in the background, the grass details, the pant folds, and the hand\nare well rendered since tobs = 48ms. On the contrary these details are still invisible or\nhighly blurry in the noiseless case at the same observation time. Finally, at the optimal\nobservation tobs = t\u2217obs = 52ms all the fine details of the image, including the coat and\nthe background details, are clearly distinguished in the dithered case while still blurry\nor invisible in the noiseless case.\nA second example is given in Figure 11 for the baboon test image. This image is\nrich of details and singularities and thus particularly challenging. Though, our dithered\ncoder still renders the image details \"before date in this case\" (with another adequate\nparametrization for the dither noise). As for the preceding example, the left image\nshows the reconstruction f \u0303tobs in the case of noiseless coding. The right image shows the\nreconstruction f \u0303tobs in the case of addition of a dither noise to the input of the ganglionic\nlayer. The central image is a sharpened version of baboon. The observation time shown in\nthis figure is also the optimal observation tobs = t\u2217obs = 44 ms. The comparison between\nthe noiseless case reconstruction (on the left) and the dithered reconstruction (on the\nright) confirms the observations made in the first example. The dither noise helps the\nrecognition of fine details \"before date\". While in the noiseless case face and bear details\nof baboon are still blurry, these details are well rendered in the dithered reconstruction\ncase.\nOn one hand, the integration of a dither noise in the coding process yield a greater\nreconstruction error in terms of mean squared error [33]. Besides, as the dither noise is\na disordered signal, it also increases the entropy of the image code. On the other hand,\nthe error whitening and de-correlation features acquired by our system are perceptually\nimportant. This is a crucial point because our current results may prove that the retina\nconveys a code that is optimized for the tasks to be performed by the visual cortex\nas categorization. While the rate/distortion trade-off remains an important goal for a\ncoding scheme it may not be the central performance criterion for the retina.\n8. Conclusion\nThe work that we presented brings two main contributions. As a first step, we\nproposed a bio-inspired codec for static images with a deterministic behavior. The image\ncoder is based on two stages. The first stage is the image transform as performed by the\nouter layers of the retina. In order to integrate time dynamics, we added to this transform\n19\n\n\fSharpened cameraman\n\nDithered reconstruction\n\n52 ms\n\n48 ms\n\n44 ms\n\n40 ms\n\nDitherless reconstruction\n\nFigure 10: Perceptual impact of the multiscale dithering on the reconstruction of cameraman. The\nobservation times tobs are shown on the left. From top to bottom, tobs take successively the values\nof: 40 ms, 44 ms, 48 ms, and 52 ms. The observation time shown in this figure is also the optimal\nobservation tobs = t\u2217obs = 44 ms.\n\n20\n\n\fSharpened baboon\n\nDithered reconstruction\n\n44 ms\n\nDitherless reconstruction\n\nFigure 11: Perceptual impact of the multiscale dithering on the reconstruction of baboon. The observation time shown in this figure is also the optimal observation tobs = t\u2217obs = 44 ms.\n\ntime delays that are subband specific so that, each subband is processed differently. The\nsecond stage is a succession of two dynamic processing steps mimicking the deep retina\nlayers behavior. These latter perform an A/D conversion and generate a spike-based,\ninvertible, retinal code for the input image in an original fashion.\nIn a second step, we investigated the issue of non-determinism in the retina neural\ncode. We proposed to model the retinal noise by a multiscale dither signal with specific\nstatistical properties. The dithering process that we proposed whitens the reconstruction\nerror and decorrelates it from the input stimuli. Besides, from a perceptual point of view,\nour coder allows an earlier recognition of the image details and singularities during the\ndecoding process.\nIn conclusion, our coding scheme offers interesting features such as (i) time-scalability,\nas the choice of the observation time of our codec enables different reconstruction qualities, and (ii) bit-allocation, as each subband of the image transform is separately mapped\naccording to the corresponding state of the inner layers. In addition, when integrating\na dithering process our coder gained interesting perceptual features. These features, if\nthe dithering hypothesis is confirmed, help the visual cortex recognize the fine details of\nthe image. This latter point is interesting because it may prove that the retina conveys\na code that is optimized for the tasks to be performed by the visual cortex. Interestingly, our dithering hypothesis found an echo recently in the computational neurosciences\ncommunity [29]. We are convinced that further neurophysiologic investigations may also\nconfirm the relevance of dithering in the retinal processing.\nIn terms of rate/distortion, the results accomplished by our coding scheme are encouraging. Though the rate/distortion performance is not the primary goal of this work,\nour coder could still be improved to be competitive with the well established JPEG and\nJPEG 2000 standards. Optimizing techniques as bit-plane coding are to be investigated.\nThis work is at the crossroads of diverse hot topics in the fields of neurosciences,\nbrain-machine interfaces, and signal processing and tries to bridge the gap between these\ndifferent domains towards the conception of new biologically inspired coders.\n21\n\n\fReferences\n[1] Adrian, E., 1926. The impulses produced by sensory nerve endings. Journal of Physiology 61 (1),\n49\u201372.\n[2] Antonini, M., Barlaud, M., Mathieu, P., Daubechies, I., 1992. Image coding using wavelet transform.\nIEEE Transactions on Image Processing.\n[3] Brown, E., Kass, R., Mitra, P., 2004. Multiple neural spike train data analysis: state-of-the-art and\nfuture challenges. Nature Neuroscience 7 (5), 456\u2013461.\n[4] Burt, P., Adelson, E., 1983. The Laplacian pyramid as a compact image code. IEEE Transactions\non Communications 31 (4), 532\u2013540.\n[5] Christopoulos, C., Skodras, A., Ebrahimi, T., 2000. The JPEG2000 still image coding system: An\noverview. IEEE Transactions on Consumer Electronics 16 (4), 1103\u20131127.\n[6] Clark, A., et al, 1928. Electrical picture-transmitting system. US Patent assigned to AT& T.\n[7] Connor, C., Brincat, S., Pasupathy, A., 2007. Transformation of shape information in the ventral\npathway. Current Opinion in Neurobiology 17 (2), 140\u2013147.\n[8] Crowley, J., Stern, R., 2009. Fast computation of the difference of low-pass transform. IEEE Transactions on Pattern Analysis and Machine Intelligence (2), 212\u2013222.\n[9] de Ruyter van Steveninck, R., Lewen, G., Strong, S., Koberle, R., Bialek, W., 1997. Reproducibility\nand variability in neural spike trains. Science 275 (5307), 1805.\n[10] Field, D., 1994. What is the goal of sensory coding? Neural Computation 6 (4), 559\u2013601.\n[11] Gollisch, T., Meister, M., 2010. Eye smarter than scientists believed: Neural computations in circuits\nof the retina. Neuron 65 (2), 150\u2013164.\n[12] Katsavounidis, I., Jay Kuo, C., 1997. A multiscale error diffusion technique for digital halftoning.\nIEEE Transactions on Image Processing 6 (3), 483\u2013490.\n[13] Kier, C., Buchsbaum, G., Sterling, P., 1995. How retinal microcircuits scale for ganglion cells of\ndifferent size. The Journal of Neuroscience 15 (11), 7673\u20137683.\n[14] Masmoudi, K., Antonini, M., Kornprobst, P., 2010. Another look at the retina as an image dithered\nscalar quantizer. In: International Workshop on Image Analysis for Multimedia Interactive Services\n(WIAMIS 2010). IEEE, pp. 1\u20134.\n[15] Masmoudi, K., Antonini, M., Kornprobst, P., 2010. Another look at the retina as an image scalar\nquantizer. In: Proceedings of the International Symposium on Circuits and Systems (ISCAS 2010).\nIEEE, pp. 3076\u20133079.\n[16] Masmoudi, K., Antonini, M., Kornprobst, P., 2010. Encoding and decoding stimuli using a biologically realistic model: The non-determinism in spike timings seen as a dither signal. In: Proceedings\nof Research in Encoding And Decoding of Neural Ensembles (AREADNE 2010). p. 75.\n[17] Masmoudi, K., Antonini, M., Kornprobst, P., 2011. A bio-inspired image coder with temporal\nscalability. In: Advanced Concepts for Intelligent Vision Systems. Springer Berlin/Heidelberg, pp.\n447\u2013458.\n[18] Masmoudi, K., Antonini, M., Kornprobst, P., 2012. Frames for exact inversion of the rank order\ncoder. to appear in IEEE Transactions on Neural Networks 23.\n[19] Masmoudi, K., Antonini, M., Kornprobst, P., Perrinet, L., 2010. A novel bio-inspired static image\ncompression scheme for noisy data transmission over low-bandwidth channels. In: Proceedings of\nthe International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2010). IEEE,\npp. 3506\u20133509.\n[20] Panzeri, S., Petersen, R., Schultz, S., Lebedev, M., Diamond, M., 2001. The role of spike timing in\nthe coding of stimulus location in rat somatosensory cortex. Neuron 29 (3), 769\u2013777.\n[21] Perkell, D., Bullock, T., 1968. Neural coding. Neurosciences Research Program Bulletin 6, 221\u2013343.\n[22] Perrinet, L., Samuelides, M., Thorpe, S., 2004. Coding static natural images using spiking event\ntimes: do neurons cooperate? IEEE Transactions on Neural Networks 15 (5), 1164\u20131175.\n[23] Rieke, F., Warland, D., de Ruyter van Steveninck, R., Bialek, W., 1997. Spikes: Exploring the\nNeural Code. The MIT Press, Cambridge, MA, USA.\n[24] Rodieck, R., 1965. Quantitative analysis of the cat retinal ganglion cells response to visual stimuli.\nVision Research 5 (11), 583\u2013601.\n[25] Shadlen, M., Newsome, W., 1998. Noise, neural codes and cortical organization. Findings and\nCurrent Opinion Cognitive Neuroscience 4, 569\u201379.\n[26] Sterling, P., Cohen, E., Smith, R., Tsukamoto, Y., 1992. Retinal circuits for daylight: why ballplayers don't wear shades. Analysis and Modeling of Neural Systems, 143\u2013162.\n[27] Taubman, D., 2000. High performance scalable image compression with ebcot. IEEE transactions\non Image Processing 9 (7), 1158\u20131170.\n\n22\n\n\f[28] Van Rullen, R., Thorpe, S., 2001. Rate coding versus temporal order coding: What the retinal\nganglion cells tell the visual cortex. Neural Computation 13, 1255\u20131283.\n[29] Vidne, M., Ahmadian, Y., Shlens, J., Pillow, J., Kulkarni, J., Litke, A., Chichilnisky, E., Simoncelli,\nE., Paninski, L., 2012. Modeling the impact of common noise inputs on the network activity of\nretinal ganglion cells. to appear in Journal of Computational Neuroscience.\n[30] Wang, C.-N., Liu, C.-M., Chiang, T., 2004. Perceptual dithering for octave subband image coding.\nJournal of Visual Communication and Image Representation 15 (2), 163 \u2013 184.\n[31] Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E., 2004. Image quality assessment: From error\nvisibility to structural similarity. IEEE Transactions on Image Processing 13 (4), 600\u2013612.\n[32] Wannamaker, R., 2004. Subtractive and nonsubtractive dithering: A mathematical comparison. J.\nAudio Eng. Soc 52 (12), 1211\u20131227.\n[33] Wannamaker, R., Lipshitz, S., Vanderkooy, J., Wright, J., 2000. A theory of nonsubtractive dither.\nIEEE Transactions on Signal Processing 48 (2), 499\u2013516.\n[34] W.Gerstner, W.Kistler, 2002. Spiking Neuron Models : Single Neurons, Populations, Plasticity.\nCambridge University Press.\n[35] Wohrer, A., 2008. Model and large-scale simulator of a biological retina, with contrast gain control.\nPh.D. thesis, Graduate School of Information and Communication Sciences, University of NiceSophia Antipolis.\n[36] Wohrer, A., Kornprobst, P., 2009. Virtual retina : A biological retina model and simulator, with\ncontrast gain control. Journal of Computational Neuroscience 26 (2), 219\u2013249.\n[37] Wohrer, A., Kornprobst, P., Antonini, M., 2009. Retinal filtering and image reconstruction. Research Report RR-6960, INRIA.\n[38] Zhang, Y., Ghodrati, A., Brooks, D., 2005. An analytical comparison of three spatio-temporal\nregularization methods for dynamic linear inverse problems in a common statistical framework.\nInverse Problems 21, 357.\n\n23\n\n\f"}