{"id": "http://arxiv.org/abs/physics/0508147v1", "guidislink": true, "updated": "2005-08-22T16:59:47Z", "updated_parsed": [2005, 8, 22, 16, 59, 47, 0, 234, 0], "published": "2005-08-22T16:59:47Z", "published_parsed": [2005, 8, 22, 16, 59, 47, 0, 234, 0], "title": "Systematic procedural and sensitivity analysis of the pattern\n  informatics method for forecasting large (M > 5) earthquake events in\n  southern California", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0508180%2Cphysics%2F0508189%2Cphysics%2F0508177%2Cphysics%2F0508204%2Cphysics%2F0508032%2Cphysics%2F0508108%2Cphysics%2F0508211%2Cphysics%2F0508209%2Cphysics%2F0508195%2Cphysics%2F0508079%2Cphysics%2F0508081%2Cphysics%2F0508115%2Cphysics%2F0508022%2Cphysics%2F0508151%2Cphysics%2F0508088%2Cphysics%2F0508109%2Cphysics%2F0508183%2Cphysics%2F0508021%2Cphysics%2F0508104%2Cphysics%2F0508124%2Cphysics%2F0508164%2Cphysics%2F0508006%2Cphysics%2F0508193%2Cphysics%2F0508201%2Cphysics%2F0508191%2Cphysics%2F0508224%2Cphysics%2F0508140%2Cphysics%2F0508122%2Cphysics%2F0508072%2Cphysics%2F0508171%2Cphysics%2F0508215%2Cphysics%2F0508228%2Cphysics%2F0508120%2Cphysics%2F0508227%2Cphysics%2F0508167%2Cphysics%2F0508184%2Cphysics%2F0508049%2Cphysics%2F0508080%2Cphysics%2F0508056%2Cphysics%2F0508157%2Cphysics%2F0508176%2Cphysics%2F0508020%2Cphysics%2F0508233%2Cphysics%2F0508094%2Cphysics%2F0508192%2Cphysics%2F0508145%2Cphysics%2F0508042%2Cphysics%2F0508197%2Cphysics%2F0508001%2Cphysics%2F0508078%2Cphysics%2F0508161%2Cphysics%2F0508150%2Cphysics%2F0508043%2Cphysics%2F0508092%2Cphysics%2F0508018%2Cphysics%2F0508200%2Cphysics%2F0508181%2Cphysics%2F0508046%2Cphysics%2F0508059%2Cphysics%2F0508132%2Cphysics%2F0508147%2Cphysics%2F0508206%2Cphysics%2F0508102%2Cphysics%2F0508100%2Cphysics%2F0508128%2Cphysics%2F0508103%2Cphysics%2F0508218%2Cphysics%2F0508125%2Cphysics%2F0508085%2Cphysics%2F0508162%2Cphysics%2F0508070%2Cphysics%2F0508037%2Cphysics%2F0508234%2Cphysics%2F0508114%2Cphysics%2F0508017%2Cphysics%2F0508074%2Cphysics%2F0508110%2Cphysics%2F0508127%2Cphysics%2F0508084%2Cphysics%2F0508101%2Cphysics%2F0508159%2Cphysics%2F0508068%2Cphysics%2F0508117%2Cphysics%2F0508054%2Cphysics%2F0508217%2Cphysics%2F0508137%2Cphysics%2F0508089%2Cphysics%2F0508138%2Cphysics%2F0508012%2Cphysics%2F0508143%2Cphysics%2F0508214%2Cphysics%2F0508039%2Cphysics%2F0508008%2Cphysics%2F0508163%2Cphysics%2F0508003%2Cphysics%2F0508027%2Cphysics%2F0508213%2Cphysics%2F0508119%2Cphysics%2F0508019%2Cphysics%2F0508121%2Cphysics%2F0508141&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Systematic procedural and sensitivity analysis of the pattern\n  informatics method for forecasting large (M > 5) earthquake events in\n  southern California"}, "summary": "Recent studies in the literature have introduced a new approach to earthquake\nforecasting based on representing the space-time patterns of localized\nseismicity by a time-dependent system state vector in a real-valued Hilbert\nspace and deducing information about future space-time fluctuations from the\nphase angle of the state vector. While the success rate of this Pattern\nInformatics (PI) method has been encouraging, the method is still in its\ninfancy. Procedural analysis, statistical testing, parameter sensitivity\ninvestigation and optimization all still need to be performed. In this paper,\nwe attempt to optimize the PI approach by developing quantitative values for\n\"predictive goodness\" and analyzing possible variations in the proposed\nprocedure. In addition, we attempt to quantify the systematic dependence on the\nquality of the input catalog of historic data and develop methods for combining\ncatalogs from regions of different seismic rates.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=physics%2F0508180%2Cphysics%2F0508189%2Cphysics%2F0508177%2Cphysics%2F0508204%2Cphysics%2F0508032%2Cphysics%2F0508108%2Cphysics%2F0508211%2Cphysics%2F0508209%2Cphysics%2F0508195%2Cphysics%2F0508079%2Cphysics%2F0508081%2Cphysics%2F0508115%2Cphysics%2F0508022%2Cphysics%2F0508151%2Cphysics%2F0508088%2Cphysics%2F0508109%2Cphysics%2F0508183%2Cphysics%2F0508021%2Cphysics%2F0508104%2Cphysics%2F0508124%2Cphysics%2F0508164%2Cphysics%2F0508006%2Cphysics%2F0508193%2Cphysics%2F0508201%2Cphysics%2F0508191%2Cphysics%2F0508224%2Cphysics%2F0508140%2Cphysics%2F0508122%2Cphysics%2F0508072%2Cphysics%2F0508171%2Cphysics%2F0508215%2Cphysics%2F0508228%2Cphysics%2F0508120%2Cphysics%2F0508227%2Cphysics%2F0508167%2Cphysics%2F0508184%2Cphysics%2F0508049%2Cphysics%2F0508080%2Cphysics%2F0508056%2Cphysics%2F0508157%2Cphysics%2F0508176%2Cphysics%2F0508020%2Cphysics%2F0508233%2Cphysics%2F0508094%2Cphysics%2F0508192%2Cphysics%2F0508145%2Cphysics%2F0508042%2Cphysics%2F0508197%2Cphysics%2F0508001%2Cphysics%2F0508078%2Cphysics%2F0508161%2Cphysics%2F0508150%2Cphysics%2F0508043%2Cphysics%2F0508092%2Cphysics%2F0508018%2Cphysics%2F0508200%2Cphysics%2F0508181%2Cphysics%2F0508046%2Cphysics%2F0508059%2Cphysics%2F0508132%2Cphysics%2F0508147%2Cphysics%2F0508206%2Cphysics%2F0508102%2Cphysics%2F0508100%2Cphysics%2F0508128%2Cphysics%2F0508103%2Cphysics%2F0508218%2Cphysics%2F0508125%2Cphysics%2F0508085%2Cphysics%2F0508162%2Cphysics%2F0508070%2Cphysics%2F0508037%2Cphysics%2F0508234%2Cphysics%2F0508114%2Cphysics%2F0508017%2Cphysics%2F0508074%2Cphysics%2F0508110%2Cphysics%2F0508127%2Cphysics%2F0508084%2Cphysics%2F0508101%2Cphysics%2F0508159%2Cphysics%2F0508068%2Cphysics%2F0508117%2Cphysics%2F0508054%2Cphysics%2F0508217%2Cphysics%2F0508137%2Cphysics%2F0508089%2Cphysics%2F0508138%2Cphysics%2F0508012%2Cphysics%2F0508143%2Cphysics%2F0508214%2Cphysics%2F0508039%2Cphysics%2F0508008%2Cphysics%2F0508163%2Cphysics%2F0508003%2Cphysics%2F0508027%2Cphysics%2F0508213%2Cphysics%2F0508119%2Cphysics%2F0508019%2Cphysics%2F0508121%2Cphysics%2F0508141&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Recent studies in the literature have introduced a new approach to earthquake\nforecasting based on representing the space-time patterns of localized\nseismicity by a time-dependent system state vector in a real-valued Hilbert\nspace and deducing information about future space-time fluctuations from the\nphase angle of the state vector. While the success rate of this Pattern\nInformatics (PI) method has been encouraging, the method is still in its\ninfancy. Procedural analysis, statistical testing, parameter sensitivity\ninvestigation and optimization all still need to be performed. In this paper,\nwe attempt to optimize the PI approach by developing quantitative values for\n\"predictive goodness\" and analyzing possible variations in the proposed\nprocedure. In addition, we attempt to quantify the systematic dependence on the\nquality of the input catalog of historic data and develop methods for combining\ncatalogs from regions of different seismic rates."}, "authors": ["James R. Holliday", "John B. Rundle", "Kristy F. Tiampo", "Bill Klein", "Andrea Donnellan"], "author_detail": {"name": "Andrea Donnellan"}, "author": "Andrea Donnellan", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s00024-006-0131-1", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/physics/0508147v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/physics/0508147v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "39 pages, 4 tables, 9 figures. Submitted to Pure and Applied\n  Geophysics on 30 November 2004", "arxiv_primary_category": {"term": "physics.geo-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "physics.geo-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.data-an", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/physics/0508147v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/physics/0508147v1", "journal_reference": null, "doi": "10.1007/s00024-006-0131-1", "fulltext": "Submitted to Pure and Applied Geophysics\n\nSystematic Procedural and Sensitivity Analysis of the\nPattern Informatics Method for Forecasting Large\n(M > 5) Earthquake Events in Southern California\nJ. R. Holliday1,2, J. B. Rundle1,2, K. F. Tiampo3, W. Klein4, and A. Donnellan5\n\n1\n\nCenter for Computational Science and Engineering, University of California, One Shields Avenue,\n\nDavis, CA 95616-8677, USA.\n\n2\n\nDepartment of Physics, University of California, One Shields Avenue, Davis, CA 95616-8677, USA.\n\n3\n\nDepartment of Earth Sciences, University of Western Ontario, Biology and Geological Sciences Bldg.,\n\nLondon, Ontario, CANADA N6A 5B7.\n\n4\n\nDepartment of Physics, Boston University, 590 Commonwealth Avenue, Boston, MA 02215, USA.\n\n5\n\nEarth and Space Sciences Division, Jet Propulsion Laboratory, Mail Stop 183-335, 4800 Oak Grove\n\nDrive, Pasadena, CA 91109-8099, USA.\n\nE-mail: holliday@cse.ucdavis.edu, jbrundle@ucdavis.edu, ktiampo@uwo.ca, klein@buphyc.bu.edu,\ndonnellan@jpl.nasa.gov\n\n\fCorresponding author:\n\nJ.R. Holliday\nCenter for Computational Science and Engineering\nUniversity of California\nOne Shields Avenue\nDavis, CA 95616-8677\nUSA\n\nE-mail: holliday@cse.ucdavis.edu\nTel: +1-530-752-6419\nFax: +1-530-754-4885\n\nAbbreviated title: Analysis of Pattern Informatics Model\n\nKeywords: Pattern Informatics, earthquake forecasting\n\n\fAbstract\nRecent studies in the literature have introduced a new approach to earthquake\nforecasting based on representing the space-time patterns of localized seismicity by a\ntime-dependent system state vector in a real-valued Hilbert space and deducing\ninformation about future space-time fluctuations from the phase angle of the state\nvector. While the success rate of this Pattern Informatics (PI) method has been\nencouraging, the method is still in its infancy. Procedural analysis, statistical testing,\nparameter sensitivity investigation and optimization all still need to be performed. In\nthis paper, we attempt to optimize the PI approach by developing quantitative values\nfor \"predictive goodness\" and analyzing possible variations in the proposed\nprocedure. In addition, we attempt to quantify the systematic dependence on the\nquality of the input catalog of historic data and develop methods for combining\ncatalogs from regions of different seismic rates.\n\n1. Introduction\nLarge magnitude earthquakes are devastating events which can have great social,\nscientific, and economic impact. The 26 December 2003 magnitude 6.7 Iran\nearthquake killed nearly 30,000 persons. The 16 January 1995 Japan magnitude 6.9\nearthquake produced an estimated $200 billion loss. Similar scenarios are possible at\nany time in San Francisco, Seattle, and other U.S. urban centers along the Pacific\nplate boundary, especially in Southern California. The gravity of potential loss of life\nand property is so great that reliable earthquake forecasting should be at the forefront\nof research goals.\n\n\fWhile millions of dollars and thousands of work years have been spent on\nobservational programs searching for reliable precursory phenomena, to date few\nsuccesses have been reported and no precursors to large earthquake events have been\ndetected that provide reliable forecasts. Indeed, many wonder if earthquake\nforecasting is even possible (see, for example, the online debate hosted at\nhttp://www.nature.com/nature/debates/earthquake).\n\nA new approach to earthquake forecasting, the pattern informatics (PI) approach, has\nbeen proposed by Rundle et al. (2000a, 2000b, 2002, 2003) and Tiampo et al. (2002a,\n2002b, 2002c). This approach is based on the strong space-time correlations that are\nresponsible for the cooperative behavior of driven threshold systems and arise both\nfrom threshold dynamics as well as from the mean field (long range) nature of the\ninteractions.\n\nUsing both simulations and observed earthquake data, they have shown that the\nspace-time patterns of threshold events (earthquakes) can be represented by a timedependent system state vector in a Hilbert space. The length of the state vector\nrepresents the average temporal frequency of events throughout the region and is\nclosely related to the rate at which stress is dissipated. It can be deduced that the\ninformation about space-time fluctuations in the system state is represented solely by\nthe phase angle of the state vector. Changes in the norm of the state vector represent\nonly random fluctuations and can for the most part be removed by requiring the\nsystem state vector to have a constant norm. A more detailed summary of the method\nis given in section 4.\n\n\f2. Background\nEarthquake fault systems are now believed to be a complex example of a highly\nnonlinear system (Bak and Tang, 1989; Rundle and Klein, 1995). Interactions among\na spatial network of fault segments are mediated by means of a potential that allows\nstresses to be redistributed to other segments following slip on any particular segment.\nFor faults embedded in a linear elastic host, this potential is a stress Green's function\nwhose exact form can be calculated from the equations of linear elasticity, once the\ncurrent geometry of the fault system is specified. A persistent driving force, arising\nfrom plate tectonic motions, increases stress on the fault segments. Once the stresses\nreach a threshold characterizing the limit of stability of the fault, a sudden slip event\nresults. The slipping segment can also trigger slip at other locations on the fault\nsurface whose stress levels are near the failure threshold as the event begins. In this\nmanner, earthquakes occur that result from the interactions and nonlinear nature of the\nstress thresholds.\n\nThe Karhunen-Loeve method (Fukunaga, 1970; Holmes et al., 1996), a linear\ndecomposition technique in which a dynamical system is decomposed into a complete\nset of orthonormal subspaces, has been applied to a number of other complex\nnonlinear systems over the last fifty years, including the ocean-atmosphere interface,\nturbulence, meteorology, biometrics, statistics, and even solid earth geophysics\n(Hotelling, 1993; Fukunaga, 1970; Aubrey and Emery, 1983; Preisendorfer, 1988;\nSavage, 1988; Penland, 1989; Vautard and Ghil, 1989; Garcia and Penland, 1991;\nPenland and Magorian, 1993; Penland and Sardeshmukh, 1995; Holmes et al., 1996;\nMoghaddam et al., 1998). The notable success of this method in analyzing the oceanatmosphere interface and such features as the El Ni\u00f1o Southern Oscillation (ENSO), a\nnonlinear system whose underlying physics is governed by the Navier-Stokes\n\n\fequation, suggested its application to the analysis of the earthquake fault system\n(North, 1984; Preisendorfer, 1988; Penland and Magorian, 1993; Penland and\nSardeshmukh, 1995). Building on these methods for analyzing nonlinear threshold\nsystems, space-time seismicity patterns can be identified in both observed phenomena\nand numerical simulations using realistic earthquake models for southern California\n(Bufe and Varnes, 1993; Bowman et al., 1998; Gross and Rundle, 1998; Brehm and\nBraile, 1999; Jaume and Sykes, 1999; Tiampo et al., 1999, 2000; Rundle et al., 2000b.\n\nThe PI method is an adaptation of the Karhunen-Loeve expansion technique to the\nanalysis of observed seismicity data from southern California in order to identify basis\npatterns for all possible space-time seismicity configurations. These basis states\nrepresent a complete, orthonormal set of eigenvectors and associated eigenvalues,\nobtained from the diagonalization of the correlation operators computed for the\nregional historic seismicity data, and, as such, can be used to reconstitute the data for\nvarious subset time periods of the entire data set.\n\n3. Data\nThe primary data set employed in this analysis is the entire historic seismic catalog\nfrom 1 January 1932 through 31 December 1999, obtained from the Southern\nCalifornia Earthquake Data Center (SCEDC) online searchable database1, with all\nnon-local and blast events specifically removed. The relevant data consists of\nlocation, in East longitude and North latitude, and the date the event occurred.\nSeismic events between -122o and -115o longitude and between 32o and 37o latitude\n(any depth and quality) and with magnitude greater than or equal to Mmin = 3.0 were\n1\n\nhttp://www.data.scec.org/catalog_search/index.html\n\n\fselected.\n\nWhile the SCEDC catalog is among the best available, both in completeness and\nhistoric depth, there are a number of known deficiencies2 that undoubtedly affect the\nquality of our constructed forecast hot-spot maps. The most notable of these issues is\nthat the four-year span of data from 1977-1980 is currently not available to web\nsearching. Fortunately, data for these missing years is available from the older\nSouthern California Seismic Network (SCSN) archives3 and was hand inserted for\nthis analysis. Unless otherwise indicated, all analysis was performed using SCEDC\ndata with the additional SCSN data.\n\nA second source of data employed in this analysis was acquired from the Northern\nCalifornia Earthquake Data Center (NCEDC) online searchable database4, with all\nnon-local and blast events again specifically removed. When incorporating this\ncatalog, seismic events between -122o and -115o longitude and between 35o and 37o\nlatitude (any depth and quality) and with magnitude greater than or equal to Mmin =\n3.0 were selected. The necessity for utilizing an additional catalog in some of our\nanalysis arises from various earthquake events in the vicinity of 35o North latitude\nmissing from the SCEDC catalog but present in the NCEDC collection.\n\n4. Basic Method\nHere we summarize the current PI method as described by Rundle et al. (2003) and\nTiampo et al. (2002c). The PI approach is a six step process that creates a time2\n\nhttp://www.data.scec.org/catalog_search/known_issues.html\nhttp://www.data.scec.org/ftp/catalogs/SCSN/\n4\nhttp://quake.geo.berkeley.edu/ncedc/catalog-search.html\n3\n\n\fdependent system state vector in a real valued Hilbert space and uses the phase angle\nto predict future states (Rundle et al., 2003). The method is based on the idea that the\nfuture time evolution of seismicity can be described by pure phase dynamics (Mori\nand Kuramoto, 1998; Rundle et al., 2000a, 2000b). Hence, a real-valued seismic\nphase function S xi ,t b , t is constructed and allowed to rotate in its Hilbert space.\nSince seismicity in active regions is a noisy function (Kanamori, 1981), only temporal\naverages of seismic activity are utilized in the method. The geographic area of interest\nis partitioned into N square bins centered on a point xi and with an edge length dx\ndetermined by the nature of the physical system. For our analysis we chose dx = 0.1o\n~ 11km, corresponding to the linear size of a magnitude M ~ 6 earthquake. Within\neach box, a time series\n\nobs\n\nxi ,t is defined by counting how many earthquakes with\n\nmagnitude greater than Mmin occurred during the time period t to t + dt. Next, the\nactivity rate function S xi ,t b ,T is defined as the average rate of occurrence of\nearthquakes in box i over the period tb to T:\n\nS ( xi ,t b ,T ) =\n\n( xi ,t )\nT \u2212 tb\n\n.\n\n(1)\n\nIf tb is held to be a fixed time, S xi ,t b ,T can be interpreted as the ith component of\na general, time-dependent vector evolving in an N-dimensional space (Tiampo et al.,\n2002c). Furthermore, it can be shown that this N-dimensional correlation space is\ndefined by the eigenvectors of an NxN correlation matrix (Rundle et al., 2000a,\n2000b). The activity rate function is then normalized by subtracting the spatial mean\nover all boxes and scaling to give a unit-norm:\n\nS\u02c6 ( xi ,t b ,T ) =\n\nS ( xi ,t b ,T ) \u2212\n\n1\nN\n\n1\nS (x j ,t b ,T ) \u2212\nN\n\nS (x j ,t b ,T )\nS ( xk ,t b ,T )\n\n2\n\n.\n\n(2)\n\n\fThe requirement that the rate functions have a constant norm helps remove random\nfluctuations from the system. Following the assumption of pure phase dynamics\n(Rundle et al., 2000a, 2000b), the important changes in seismicity will be given by the\nchange in the normalized activity rate function for the time period t1 to t2:\nS xi ,t b , t 1 ,t 2\n\nS xi ,t b , t 2\n\nS xi ,t b ,t 1 .\n\n(3)\n\nThis is simply a pure rotation of the N-dimensional unit vector S xi ,t b ,T through\ntime. In order to remove the last free parameter in the system, the choice of base year,\nand to further reduce random noise components, changes in the normalized activity\nrate function are averaged over all possible base-time periods:\nt1\ntb t 0\n\nS xi , t 0 ,t 1 , t 2\n\nS xi ,t b , t 1 ,t 2\nt1 t0\n\n.\n\n(4)\n\nFinally, the probability of change of activity in a given box is deduced from the\nsquare of its base averaged, mean normalized change in activity rate:\nP xi ,t 0 ,t 1 ,t 2\n\nS xi ,t b , t 1 ,t 2\n\n2\n\n.\n\n(5)\n\nIn phase dynamical systems, probabilities are related to the square of the associated\nvector phase function (Mori and Kuramoto, 1998; Rundle et al., 2000b). This\nprobability function is often given relative to the background by subtracting off its\nspatial mean:\nP' ( xi ,t 0 ,t1 ,t 2 ) = P ( xi ,t 0 ,t1 ,t 2 ) \u2212\n\n1\nN\n\nP (x j ,t 0 ,t1 ,t 2 ) ,\n\nwhere P' indicates the probability of change in activity and is measured relative to the\nbackground.\n\nSchematically, this whole process can be represented by\n\nN\n\nS\n\nS\n\nS\n\nS P,\n\nwhere the hat symbol is understood to mean \"calculate normalization in space\", the\n\n(6)\n\n\fcapital Delta means \"calculate the change in rate\", and the underscore symbol means\n\"average over base times\". Note that this method implicitly assumes earthquake fault\nsystems are in an unstable equilibrium state and can be treated linearly about their\nequilibrium points.\n\n4.1. Variations in Order\nTo determine the optimal application of the PI method, we identified and analyzed all\nphysically meaningful variations of the described procedure. While we have outlined\nabove a six step process, there are considerably fewer than 6! = 720 variations that\nneed to be investigated. A forecast analysis must always begin with binning the\navailable data and end with a calculation of probability change. Also, base-time\naveraging and calculation of changes in the activity rate functions can only be\nperformed after creating the activity rate vectors. With these constraints imposed,\nthere are only eight possible variations in the order to which each step is performed.\nTable 1 lists these eight variations with the original method denoted Method I.\n\nOn the basis of theoretical arguments and assumptions of linearity within the system,\nwe expect that Methods I through VI should perform qualitatively similar to each\nother. This is due largely to the fact that the operations being permuted are all linear\nand commute with each other. Qualitatively it is unclear which variation should yield\nthe best correlation with actual future events other than to expect Methods II and III\nmight perform better than Method I due to the movement of when the change in\nactivity rate is calculated to after the normalization and base-time averaging steps.\nThis essentially places all of the activity rate vectors on equal footing and legitimizes\nthe vector rotation. We also expect that Methods VII and VIII will yield both\n\n\fqualitatively and quantitatively inferior forecast hot-spot maps. This is due to the\ndirect normalization of the binned data. Such a step destroys correlations between\ndifferent spatial locations by independently scaling the relative historic intensity rates.\nEach of these expectations are verified in the results section below.\n\n4.2. Variations in Binning\nIn addition to the original binning method, we also analyzed time-centered,\ncumulative, and detrended binning. For time-centered binning, we took each time\nseries and removed the temporal mean:\nt2\nobs\n\nxi ,t\n\nobs\n\nxi ,t\n\nt t0\n\nobs\n\nxi ,t\n\nt2 t0\n\n.\n\n(7)\n\nFor cumulative binning we allowed each time series to build on its past events:\nobs\n\nt\n\nxi ,t\n\nT t0\n\nobs\n\nxi ,T .\n\n(8)\n\nFor detrended binning, we took each cumulative time series, fit it to a first order\npolynomial, and subtracted the fitted line:\nobs\n\nxi ,t\n\nt\nT t0\n\nobs\n\nxi ,T\n\nA Bt ,\n\nwhere A and B are the parameters of the regression fit. Figure 1 shows the effect of\neach binning procedure on a synthetic data sample. We will denote the four different\nbinning methods with the labels A, B, C, and D, respectively, with A denoting the\nunmodified method. Methods B and D are significant in that they remove the mean\nfor each time series from the data. Thus, anomalous activity away from background\nseismicity is expected to be emphasized. Method C is reminiscent of an unbiased\nestimator in the cumulative distribution Kolmogorov-Smirnov Test (Press et al.,\n2002) and could in theory allow more accurate comparisons among the different time\n\n(9)\n\n\fseries.\n\nWe also investigated magnitude- and energy-weighted binning where the value at\neach time step is proportional to either the total magnitude Mtot of all the events in the\ntime period or to the total energy (~10Mtot ) of all the events. These weighting factors,\nhowever, had the effect of selecting out time periods surrounding only the largest\nevents and were thus unsuitable for the analysis. We did not investigate Boolean\nbinning where each time step is given an initial value of either 1 if one or more events\noccur in that time period or 0 otherwise due to the realization that this effect can be\nachieved by sufficiently reducing the time step dt. Also, we desired the method to\nscale appropriately as dt is increased.\n\n4.3. Variations in Projection\nIn addition to calculating the change in the activity rate function through the vector\nrotation during the time period t1 to t2, we also investigated the effect of linear\nprojection of change into future times:\nS xi ,t b ,t 1 , t 2\n\nS xi ,t b ,t 2\n\nS xi ,t b ,t 1 , t 2 .\n\nThe motivation behind this investigation was that for regions with a near constant rate\nof seismicity (or with frequencies higher than an inverse time step),\n\nS\u02c6 ( xi ,t b ,t1 ,t 2 ) \u2248 0 . By linear projection, we mean that the future seismic activity for\nthis type of situation would be approximately equal to the present seismic activity\nwith a small correction added. For notational purposes, we will denote the unmodified\napproach of calculating the change in the activity rate function with the label 1 after\nthe method specification. We will denote the linear projection approach with the label\n\n2.\n\n(10)\n\n\f4.4. Variations in dt\nWhile the spatial width of the boxes, dx, is determined by the nature of the physical\nsystem, the temporal binning width dt is arbitrary. Larger values of dt result in\ngreater bin statistics and faster execution time of the algorithm while lower values\nmay potentially yield greater sensitivity to high frequency periodicity.\n\nTo investigate the effect, we performed the analysis with representative values for dt\nranging from one day to one year. If the catalog is uniform in its completeness and not\nmissing bands of data at quasi- periodic intervals, we would expect to find a smooth\ntransition through the varying choices of dt with perhaps some optimal selection. On\nthe other hand, large fluctuations in the forecast as dt is slowly modified may indicate\nunderlying chaotic phenomena and would bring into question the assumptions and\ntreatment of linearity within the system.\n\n5. Statistical Tests\nTo test the hypothesis that the probability measure Pi can forecast future (t > t2) large\n(M > 5) events, we performed a set of maximum likelihood tests [Bevington and\nRobinson, 1992; Gross and Rundle, 1998; Kagan and Jackson, 2000; Tiampo et al.,\n2002b; Schorlemmer et al., 2003]. The likelihood L is a probability measure that can\nbe used to assess the quality of one forecast measure over another. Typically, one\ncomputes L = log(L) for the proposed forecast measure L and compares that to the\nlikelihood measure L0 = log(L0) for a representative null hypothesis. The ratio of\nthese two values then yields information about which measure is more accurate in\n\n\fforecasting future events. In the likelihood ratio test, a probability density function\n(PDF) is required. Two different PDFs were used in this analysis: a global, Gaussian\nmodel and a local, Poissonian model. These distributions differ significantly in that\nthe Gaussian model assumes purely random, normal statistics while the Poissonian\nmodel assumes independent statistics over small time intervals with no temporal\nclustering [Walpole and Myers, 1993].\n\n5.1. Global Gaussian Model\nIn their original analysis, Tiampo et al. (2002b) calculated likelihood values by\ndefining Pi = P[xi] to be the union of a set of N Gaussian density functions pG(|x-xi|)\n(Bevington and Robinson, 1992) centered at each location xi. Each individual\nGaussian density has a standard deviation equal to the box width dx and a peak value\nequal to the calculated probability of change in activity Pi divided by the standard\ndeviation squared. P[x(ej)] is therefore a probability measure that a future large event\nej occurs at location x(ej):\n\nPi\n\nP x ej\n\ni\n\n2\n\nx ej\n\nxi\n2\n\ne\n\n2\n\n.\n\n(11)\n\nIf there are J future events, the normalized likelihood L that all J events are forecast\nis:\n\nL\n\nP e xj\nj\ni\n\nP xi\n\n.\n\n(12)\n\nFurthermore, the log-likelihood value L for a given calculation can be calculated and\nused in ratio comparison tests:\n\nlog L\n\nlog\nj\n\nP e xj\ni\n\nP xi\n\n.\n\n(13)\n\n\fBefore performing the statistical analysis, the change in activity values Pi were first\ntruncated by scaling all the probabilities equally up-wards and performing a\n\nP 1 . This was used to eliminate the\n\nhistogram cut to enforce the restriction\n\nexponential tail on the high end of the PDF and ensure that events that occurred\nP 1 of occurring (which, in\n\nduring the forecasting time period had a probability\nfact, they did).\n\n5.2. Local Poissonian Model\nThe second model used is based on work performed by the Regional Earthquake\nLikelihood Models (RELM) group (Schorlemmer et al., 2003). For each bin i an\nexpectation value\n\ni\n\nis calculated by scaling the local probability Pi by the number of\n\nearthquakes that occurred over all space during the forecast time period:\nn Pi ,\n\ni\n\n(14)\n\nwhere n is the number of post-t2 events. Note that for any future time interval (t2, t3), n\ncould in principle be estimated by using the Gutenberg-Richter relation. For each bin\nan observation value wi is also calculated such that wi contains the number of post-t2\nearthquakes that actually occurred in bin i. For the RELM model, it is assumed that\nearthquakes are independent of each other. Thus, the probability of observing wi\nevents in bin i with expectation\n\ni\n\nis the Poissonian probability\nwi\n\npi wi ,\n\ni\ni\n\nwi !\n\ne\n\ni\n\n.\n\nThe log-likelihood for observing w earthquakes at a given expectation\nthe logarithm of the probability pi wi ,\n\ni\n\n, thus\n\n(15)\n\nis defined as\n\n\flog L w ,\n\nlog p w,\n\nw log\n\nlog w! .\n\nSince the joint probability is the product of the individual bin probabilities, the loglikelihood value for a given calculation is the sum of log L w,\n\nover all bins i.\n\nWhen using this PDF function, we preprocess the change in activity values Pi by\nperforming the same histogram cut as with the Gaussian model.\n\n6. Results\nResults for the procedural analysis with variations in binning and calculation of\nactivity rate are presented in tables 2 and 3. All values of L are given relative to L0\ndefined to be the value supplied by our original, unaltered Method I-A1. Since these\nare ratio tests, greater values indicate better predictive ability.\n\nAs statistical evaluations of earthquake forecasts are still under development, it is\ninstructive to weigh the quantitative (\"predictive goodness\" values) against the\nqualitative (pictorial representation of the forecast hot-spot maps). Thus,\nrepresentative maps for each procedural variation are given in figures 2 and 3.\n\nOnly Methods II and III, using normal binning and change of activity calculation,\nperformed better than the original method under the two statistical tests. Naively, this\nresult is expected as both methods wait until after normalization and base year\naveraging to calculate the change in activity rate, thus giving the calculations in each\nbox equal statistical weight. For all other investigated variations, no method\nperformed better on both likelihood tests and qualitative analysis.\n\n(16)\n\n\fWhile a few of the binning and change of activity variations fared well on one or the\nother likelihood tests (for example, III-B1), most performed poorly qualitatively.\nProbability calculations gave predictions of activity that spread well into areas with no\nrecorded activity. These results can be understood by considering their mathematical\noperations. By linearly projecting the change in activity rate, heavy weight is placed\non the most recent seismic history. For the procedure to identify anomalous changes\nin the seismicity, however, the entire history must be considered equally. Also, the\ncumulative and detrended variations in the binning method create time series that are\nsignificantly altered from those apparent in nature.\n\nWhile only Methods II-A1 and III-A1 performed better than the original PI procedure\non both statistical tests, it should be stressed that at this time none of the methods can\nbe claimed to be superior. There is still a subjective element over which forecast hotspot map to prefer. Based on theoretical and mathematical considerations, Method\n\nIII-A1 is the authors' preferred choice. This method creates a unique state vector at\nevery time step and allows the purest interpretation of a vector rotation.\n\nTable 4 shows the results of varying the time step in the analysis (note that Method\n\nIII-A1 was used). Likelihood values for this investigation were referenced against a\nchoice of dt = 1 day. Note that the accuracy of the calculated forecast decreases with\nincreasing time step, slowly decreasing up to around dt = 1 week and then rapidly\ndecreasing. While larger choices of dt decrease time of computation for the PI\nalgorithm, they do so at the cost of accuracy. Evaluating the data from Table 4, along\nwith the corresponding forecast hot-spot maps, the authors believe dt = 7 days to be a\nsuitable compromise. This choice of time step is low enough to probe the seismic\nperiodicity at all scales with reasonable accuracy while being large enough to\n\n\fsignificantly speed up the computation.\n\n7. Catalog Sensitivity\nTo gauge the sensitivity of the PI method on the quality of the input catalog, we\ndecimated the available data by systematically increasing both the starting date of\ncatalog information (and thus affecting t0) and the minimum magnitude threshold.\nFigures 4 and 5 show the effect on the relative likelihood values of varying either\nparameter individually. Both probability density functions\u2013Poissonian and Gaussian\u2013\nwere used to calculate log likelihood indexes.\n\nIn Figure 4 we see the surprising result that the forecast is relatively stable as t0 is\nincreased, up to around 1965. This would indicate that accurate forecast hot-spot\nmaps can be created using only approximately 40 years of historic data. When the\nnormalized activity rate functions are averaged over all possible base-time periods,\nmore recent data gets weighted heavier than more historic data. The threshold for\nwhen historic data no longer influences the forecast appears to be approximately 40\nyears before the onset of the forecast, i.e., t2. With less than 40 years of historic data,\nhowever, the likelihood values drop sharply.\n\nThe Poissonian analysis in Figure 5 seems to indicate that higher accuracy in the\nforecast can be obtained by raising the minimum magnitude cut-off threshold of the\nanalysis from Mmin = 3.0 to ~3.7. This may have the effect of removing low\nmagnitude events that are uncorrelated with future large magnitude events and\nthereby eliminate background noise from the analysis. Care must be taken, however,\nas the likelihood values drop quickly as the magnitude threshold is raised too high. It\n\n\fis interesting to note the sudden drop in likelihood values as the magnitude threshold\nreaches 4.5 (and again near 4.8, 5.5, and 6.3). While statistics may be playing a role in\nthe latter three drops, the discontinuity at Mmin = 4.5 appears to identify an unknown\ndeficiency in the catalog.\n\nFigures 6 and 7 show the effect on the relative likelihood values of varying both\nparameters simultaneously. For these two-dimensional plots, warmer colors indicate\nbetter correlation between the forecast and actual events. All of the features\nmentioned above are again evident as well as the surprising observation that\nincreasing Mmin allows accurate forecasts with less historic data (as indicated by the\npositive slope of the high-likelihood-edge surrounding Mmin = 3.6 and t0 = 1967).\n\n8. Application Of The Method\nTo test the our optimization on the PI method, we recreated the forecast seismic hotspot map originally presented by Rundle et al. (2002) for the time period 1 January\n2000 to 31 December 2009 using Method III-A1. The result is shown in Figure 8.\nThe original forecast was made using only data from the SCEDC catalog, which does\nnot contain earthquakes from the San Simeon region (location of the M=6.5, 2003\nevent; label #7 in Figure 8). Our revised forecast was made using data from both the\nNCEDC catalog (for latitude above 35o) and the SCEDC catalog (for latitude below\n35o).\n\nSince the cut-off date for the forecast of 31 December 1999, eight large earthquake\nevents with M>5 have occurred in central or southern California. The first seven\nevents all occurred either on areas of forecasted anomalous activity or within the\n\n\fmargin of error of +/- 11km. While this hot-spot map was made after each of these\nevents occurred, it was done so using only data prior to 31 December 1999 and could\nhave in principle predicted these events. Scorecards using the original method and the\ncurrent optimized method can be found at the JPL QuakeSim website5.\n\n9. Combining Catalogs\nThe issue of how to combine historic catalogs in order to create forecast hot-spot\nmaps for large regions is a difficult one. Problems arise from the fact that different\nareas will normally have widely different seismic rates, and these differences get\nsmoothed out when we normalize our state vectors.\n\nOne way to try and account for these differences is to apply a weighting factor to the\ndifferent catalogs as they are merged into an aggregate catalog. This method,\nhowever, tends to emphasize near threshold-level anomalous activity in the catalog\nwith the highest weighted activity rate. In Figure 9 we created a forecast hot-spot map\nby combining data from the NCEDC and SCEDC catalogs with two different\nweighting ratios. With equal weighting between the two catalogs (Figure 9A), event\n#3 (Anza) occurs near a threshold-level anomalous region. Event #7 (San Simeon),\nhowever, is missed completely. As the relative weighting for the northern catalog is\nincreased to account for its lower total seismic rate (Figure 9B), anomalous activity\nbegins to appear under event #7, but disappears from event #3.\n\nAnother way to try and account for the differences is to apply a weighting factor to\neach individual time series based on its own statistics. This method, unfortunately,\n5\n\nhttp://www-aig.jpl.nasa.gov/public/dus/quakesim/scorecard.html\n\n\falso has failings. By weighing each time series individually, correlations between\nlocal events are destroyed. In practice, this approach has effects similar to the earlier\nproposed modifications VII and VIII to the PI procedure and simply results in more\napparent noise in the forecast and less correlation with actual future events.\n\nCurrently, the best approach (at least for this time period and these catalogs) appears\nto be to treat all catalogs and regions separately, combining only at the end of the\nanalysis and normalizing over all spatial bins to allow for correlations across the\ncatalog seams.\n\n10. Conclusion\nWe have analyzed the current PI procedure and developed a more optimized approach\nfor creating accurate forecast hot-spot maps. First, historic seismic data is binned by\ncounting the number of earthquakes per unit time, of any size greater than or equal to\nMmin, within a geographic box centered at xi at some time t. The geographic region\ndefined by dx is taken large enough so that seismic activity can be considered an\nincoherent superposition of phase functions. Second, an activity rate function is\ndefined as the average rate of occurrence of earthquakes in box i over the period tb to\nT. Third, the activity rate function is averaged over all possible base-time periods.\nForth, the base-year averaged activity rate function is normalized by subtracting the\nspatial mean over all boxes and scaling to give a unit-norm. Fifth, changes in the\nbase-year averaged, mean-normalized activity rate function are calculated by allowing\nthe vector to rotate over time. Finally, the probability of change of activity in a given\nbox\u2013calculated relative to the background\u2013is deduced from the square of its base-year\naveraged, mean-normalized change in activity rate.\n\n\fWe also showed that the choice of dt is relatively unimportant to the calculation if it is\ntaken low enough, that only approximately 40 years of complete historic data is\nnecessary for accurate forecasts, and that the assumptions of linearity and nearequilibrium appear valid for Southern California seismic fault systems. Applying our\nnew procedure, we recalculated and updated the southern California forecast hot-spot\nmap presented by Rundle et al. (2002) and showed that the 22 December 2003 San\nSimeon event could have been foreseen. Finally, we identified pitfalls associated with\ncombining seismic catalogs from different regions in an attempt to create a composite\nforecast hot-spot map.\n\nThere is movement in the forecast verification community to part with likelihood\ncalculations, which lightly reward successes and heavily penalize failures, and\nembrace ROC verification diagrams (Joliffee and Stephenson, 2003). Additional\nanalyses that utilize these verification techniques are currently underway.\n\nAcknowledgments\nThe authors are grateful to the anonymous reviewers for their helpful criticisms and\nsuggestions. This work has been supported by a grant from US Department of\nEnergy, Office of Basic Energy Sciences to the University of California, Davis DEFG03-95ER14499 (JRH and JBR), by a NASA Earth Science Fellowship NN6046Q98H (JRH), and through additional funding from the National Aeronautics and\nSpace Administration under grants through the Jet Propulsion Laboratory to the\nUniversity of California, Davis.\n\n\fReferences\nAubrey, D. G., and K. O. Emery (1983), Eigenanalysis of recent united states sea\nlevels, Continental Shelf Res., 2, 21\u201333.\n\nBak, P., and C. Tang (1989), Earthquakes as self-organized critical phenomena, J.\nGeophys. Res., 94, 15,635\u201315,637.\n\nBevington, P. R., and D. K. Robinson (1992), Data Reduction and Error Analysis for\nthe Physical Sciences, McGraw-Hill.\n\nBowman, D. D., G. Ouillon, C. G. Sammis, A. Sornette, and D. Sornette (1998), An\nobservational test of the critical earthquake concept, J. Geophys. Res., 103, 24,359\u2013\n24,372.\n\nBrehm, D. J., and L. W. Braile (1999), Intermediate-term earthquake prediction using\nthe modified time-to-failure method in southern California, BSSA, 89, 275\u2013293.\n\nBufe, C. G., and D. J. Varnes (1993), Predictive modeling of the seismic cycle of the\ngreater San Francisco bay region, J. Geophys. Res., 98, 98719883.\n\nFukunaga, K. (1970), Introduction to Statistical Pattern Recognition, Academic\nPress, New York.\n\nGarcia, A., and C. Penland (1991), Fluctuating hydrodynamics and principal\noscillation pattern analysis, J. Stat. Phys., 64, 1121\u20131132.\n\n\fGross, S., and J. B. Rundle (1998), A systematic test of time-to-failure analysis,\nGeophys. J. Int., 133, 57\u201364.\n\nHolmes, P., J. L. Lumley, and G. Berkooz (1996), Turbulence, Coherent Structures,\nDynamical Systems and Symmetry, Cambridge University Press, Cambridge, U.K.\n\nHotelling, H. (1993), Analysis of a complex of statistical variables into principal\ncomponents, J. Educ. Psych., 24, 417\u2013520.\n\nJaum\u00e9, S. C., and L. R. Sykes (1999), Evolving towards a critical point: A review of\naccelerating seismic moment/energy release prior to large and great earthquakes, Pure\nAppl. Geophys., 155, 279\u2013306.\n\nJoliffee, I. T. and Stephenson, D. B. (2003), Forecast Verification, John Wiley.\n\nKagan, Y. Y., and D. D. Jackson (2000), Probabilistic forecasting of earthquakes,\nGeophys. J. Int., 143, 438\u2013453.\n\nKanamori, H. (1981), The nature of seismicity patterns before large earthquakes, in\nEarthquake Prediction: An International Review, Geophys. Monogr. Ser., pp. 1\u201319,\nAGU, Washington, D. C.\n\nMoghaddam, B., W. Wahid, and A. Pentland (1998), Beyond eigenfaces: Probabilistic\nmatching for face recognition, in Third IEEE Intl. Conf. on Automatic Face and\nGesture Recognition, pp. 1\u20136.\n\n\fMori, H., and Y. Kuramoto (1998), Dissipative Structures and Chaos, SpringerVerlag, Berlin.\n\nNorth, G. R. (1984), Empirical orthogonal functions and normal modes, J. Atm. Sci.,\n41(5), 879887.\n\nPenland, C. (1989), Random forcing and forecasting using principal oscillation\npattern analysis, Monthly Weather Rev., 117, 21652185.\n\nPenland, C., and T. Magorian (1993), Prediction of Ni\u00f1o 3 sea surface temperatures\nusing linear inverse modeling, J. Climate, 6, 1067\u20131076.\n\nPenland, C., and P. D. Sardeshmukh (1995), The optimal growth of tropical sea\nsurface temperature anomalies, J. Climate, 8, 1999\u20132024.\n\nPreisendorfer, R. W. (1988), Principle Component Analysis in Meteorology and\nOceanography, Elsevier, Amsterdam.\n\nPress, W. H., S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery (2002),\nNumerical Recipes in C, Cambridge University Press, Cambridge, MA.\n\nRundle, J. B., and W. Klein (1995), New ideas about the physics of earthquakes, Rev.\nGeophys. Space Phys. Suppl. (July), 283, 283\u2013286.\n\nRundle, J. B., W. Klein, S. J. Gross, and K. F. Tiampo (2000a), Dynamics of\nseismicity patterns in systems of earthquake faults, in Geocomplexity and the Physics\n\n\fof Earthquakes, Geophys. Monogr. Ser., vol. 120, edited by J. B. Rundle, D. L.\nTurcotte, and W. Klein, pp. 127\u2013146, AGU, Washington, D. C.\n\nRundle, J. B., W. Klein, K. F. Tiampo, and S. J. Gross (2000b), Linear pattern\ndynamics in nonlinear threshold systems, Phys. Rev. E., 61, 2418\u20132432.\n\nRundle, J. B., K. F. Tiampo, W. Klein, and J. S. S. Martins (2002), Self-organization\nin leaky threshold systems: The influence of near-mean field dynamics and its\nimplications for earthquakes, neurobiology, and forecasting, Proc. Natl. Acad. Sci.\nU. S. A., 99, 2514\u20132521: Suppl. 1.\n\nRundle, J. B., D. L. Turcotte, R. Shcherbakov, W. Klein, and C. Sammis (2003),\nStatistical physics approach to understanding the multiscale dynamics of earthquake\nfault systems, Rev. Geophys., 41(4), 1019, doi:10.1029/2003RG000135.\n\nSavage, J. C. (1988), Principal component analysis of geodetically measured\ndeformation in long valley caldera, eastern California, 19831987, J. Geophys. Res.,\n93, 13,297\u201313,305.\n\nSchorlemmer, D., D. D. Jackson, and M. Gerstenberger (2003), Earthquake likelihood\nmodel testing, http://moho.ess.ucla.edu/~kagan/sjg.pdf.\n\nTiampo, K. F., J. B. Rundle, W. Klein, and S. J. Gross (1999), Systematic evolution\nof nonlocal space-time earthquake patterns in southern California, EOS Trans. AGU,\n80, 1013.\n\n\fTiampo, K. F., J. B. Rundle, S. McGinnis, S. J. Gross, and W. Klein (2000),\nObservation of systematic variations in non-local seismicity patterns from southern\nCalifornia, in Geocomplexity and the Physics of Earthquakes, Geophys. Monogr. Ser.,\nvol. 120, edited by J. B. Rundle, D. L. Turcotte, and W. Klein, pp. 211\u2013218, AGU,\nWashington, D. C.\n\nTiampo, K. F., J. B. Rundle, S. McGinnis, S. J. Gross, and W. Klein (2002a),\nEigenpatterns in southern California seismicity, J. Geophys. Res., 107(B12), 2354,\ndoi:10.1029/2001JB000562.\n\nTiampo, K. F., J. B. Rundle, S. McGinnis, S. J. Gross, and W. Klein (2002b), Mean\nfield threshold systems and earthquakes: An application to earthquake fault systems,\nEurophys. Lett., 60(3), 481\u2013487.\n\nTiampo, K. F., J. B. Rundle, S. McGinnis, and W. Klein (2002c), Pattern dynamics\nand forecast methods in seismically active regions, Pure App. Geophys, 159, 2429\u2013\n2467.\n\nVautard, R., and M. Ghil (1989), Singular spectrum analysis in nonlinear dynamics,\nwith applications to paleodynamic time series, Physica D, 35, 395\u2013424.\n\nWalpole, R. E. and Myers R. H. (1993), Probability and Statistics for Engineers and\nScientists, Prentice Hall.\n\n\fTable 1: Possible variations in the procedure ordering. The analysis must always\nbegin with data binning and end with probability calculation. Recall N is binned data,\nS is the activity rate, P is a probability calculation, the \u02c6 symbol represents\nnormalization in space, the \u2206 symbol represents calculation of change in rate, and the\nunderscore symbol represents averaging over base times.\nProcedure\n\nMethod\n\n\u2206\n\n\u2206\n\nP\n\n\u2206\n\nP\n\n\u2206\n\nP\n\n\u2206\n\n\u2206\n\nP\n\n\u2206S\n\n\u2206S\n\n\u2206\n\nP\n\nS\n\n\u2206S\n\n\u2206\n\nP\n\n\u2206\n\n\u2206\n\nP\n\n\u2206\n\nP\n\nI N\n\nS\n\nII N\n\nS\n\nIII N\n\nS\n\nS\n\nIV N\n\nS\n\n\u2206S\n\nV N\n\nS\n\nVI N\n\nS\n\nVII N\nVIII N\n\n\fTable 2: Relative likelihood values LG\u2212 L0 using a global Gaussian model over the\ntime period t = 1984\n\n1994 for the various variations in order, binning, and\n\ncalculation of change in activity rate. Recall that A \u2013 D denote normal, time-centered,\ncumulative, and detrended binning, respectively, while 1 and 2 denote normal and\nprojected calculations of change in activity rate. For our null hypothesis, L0, we took\nthe value from Method I-A1. Larger (more positive) values are better correlated with\nactual events.\nMethod\n\nA1\n\nB1\n\nC1\n\nD1\n\nA2\n\nB2\n\nC2\n\nD2\n\nI\n\n0.00\n\n-13.06 -11.27 -18.80 -36.47 -32.23 -19.43 -24.62\n\nII\n\n3.33\n\n-8.65\n\n-21.91 -17.96 -36.14 -30.92 -14.17 -23.27\n\nIII\n\n2.70\n\n-1.04\n\n-32.58 -19.89 -15.28 -15.28 -14.74 -21.99\n\nIV\n\n-2.89\n\n-2.08\n\n-16.10 -13.87 -31.20 -16.43 -15.94 -12.57\n\nV\n\n-7.99\n\n-4.75\n\n-14.35 -19.70 -34.48 -12.94 -14.67 -21.51\n\nVI\n\n-2.76\n\n-2.92\n\n-17.63 -19.92 -33.23 -10.88 -14.54 -21.05\n\nVII\n\n-20.32 -17.41 -14.87 -32.44 -48.93 -10.90 -16.03 -33.38\n\nVIII\n\n-16.65 -21.57 -37.77 -32.02 -47.32 -10.99 -15.05 -33.42\n\n\fTable 3: Relative likelihood values LP\u2212 L0 using a local Poissonian model over the\ntime period t = 1984\n\n1994 for the various variations in order, binning, and\n\ncalculation of change in activity rate. Recall that A \u2013 D denote normal, time-centered,\ncumulative, and detrended binning, respectively, while 1 and 2 denote normal and\nprojected calculations of change in activity rate. For our null hypothesis, L0, we took\nthe value from Method I-A1. Larger (more positive) values are better correlated with\nactual events.\nMethod\n\nA1\n\nB1\n\nI\n\n-0.00\n\n1.29\n\nII\n\n4.93\n\nIII\n\nC1\n\nD1\n\nA2\n\nC2\n\nD2\n\n-38.14 -30.87 -57.74 -44.65\n\n-5.77\n\n-74.67\n\n5.58\n\n-60.65 -28.60 -18.05 -29.54\n\n-2.09\n\n-48.88\n\n2.94\n\n14.74\n\n-59.22 -26.22\n\n-2.01\n\n-35.93\n\nIV\n\n7.75\n\n6.77\n\n-7.27\n\n-12.30 -32.11 -14.98\n\n-3.15\n\n-11.46\n\nV\n\n0.43\n\n-0.52\n\n-7.38\n\n-43.10 -45.47\n\n-5.94\n\n-2.12\n\n-45.89\n\nVI\n\n0.84\n\n0.63\n\n-9.89\n\n-40.51 -21.67\n\n-3.99\n\n-2.04\n\n-55.60\n\n5.04\n\nB2\n\n5.04\n\nVII\n\n-59.34 -51.33 -61.89 -85.76 -81.90 -47.86 -44.11 -81.12\n\nVIII\n\n-45.73 -57.16 -76.22 -87.33 -83.09 -48.66 -44.12 -81.55\n\n\fTable 4: Relative likelihood values using Method III-A1 with varying time steps (in\ndays) over the time period t = 1984\n\n1994. For our null hypothesis, we took the\n\nvalue at dt = 1 day. Larger (more positive) values are better correlated with actual\nevents.\ndt\n\n=\n\n1\n\n3\n\n5\n\n7\n\n15\n\n30\n\n60\n\n90\n\n180\n\n365\n\nLG\u2212 L0\n\n=\n\n0.00\n\n-0.07\n\n-0.16\n\n-0.13\n\n-0.63\n\n-1.33\n\n-2.69\n\n-17.00\n\n-34.06\n\n-20.17\n\nL P\u2212 L 0\n\n=\n\n0.00\n\n-0.55\n\n-0.70\n\n-1.43\n\n-4.52\n\n-7.31\n\n-9.66\n\n-24.43\n\n-85.22\n\n-33.66\n\n\fFigure 1: The topmost plot represents random earthquake events over an arbitrary\ntime scale. The four lower plots show the results of the different binning methods: A)\nnormal, B) time-centered, C) cumulative, and D) detrended.\n\n\fFigure 2: Representative forecast hot-spot maps created using each of the order\nvariations with normal binning and calculation of change in activity rate for the time\nperiod t = 1984 to 1994. Note the increase in apparent noise for Methods VII and\nVIII.\n\nFigure 3: Representative forecast hot-spot maps created using each of the variations\nin binning and calculation of change in activity rate for Method I over the time period\nt = 1984 to 1994.\n\n\fFigure 4: Relative likelihood values for two different probability density functions,\nGaussian (solid) and Poissonian (dashed), as a function of t0. Larger (more positive)\nvalues are better correlated with actual events. The plateau in the data before t0 =\n1965 indicates that only ~40 years of historic data is necessary for the analysis.\n\n\fFigure 5: Relative likelihood values for two different probability density functions,\nGaussian (solid) and Poissonian (dashed), as a function of the minimum magnitude\ncut-off threshold. Larger (more positive) values are better correlated with actual\nevents. Using the Poissonian PDF, more probable forecasts appear possible by\nincreasing the magnitude threshold slightly.\n\n\fFigure 6: Relative likelihood index calculated using a Gaussian density function as a\nfunction of both t0 and minimum magnitude cut-off threshold. Warmer colors are\nbetter correlated with actual events.\n\n\fFigure 7: Relative likelihood index calculated using a Poissonian density function as\na function of both t0 and minimum magnitude cut-off threshold. Warmer colors are\nbetter correlated with actual events.\n\n\fFigure 8: Seismic hot-spot map for large earthquake events with M >5 for the\nforecast time period 1 January 2000 to 31 December 2009. Since the cut-off date for\nthe forecast, eight large earthquake events with M >5 have occurred in central or\nsouthern California. Seven of the eight events occurred either on areas of forecasted\nanomalous activity or within the margin of error of \u00b111km. Data from the SCEDC\ncatalog was used below 35 o North latitude, and from the NCEDC catalog above 35 o\nNorth latitude.\n\n\fFigure 9: Equal weight for both catalogs (A) vs. higher weighting for northern\ncatalog (B). With the equally weight map, event #3 occurs near a threshold-level\nanomalous region while event #7 does not. The opposite is true with the unequally\nweight map.\n\n\f"}