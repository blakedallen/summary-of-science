{"id": "http://arxiv.org/abs/0712.0881v1", "guidislink": true, "updated": "2007-12-06T07:22:35Z", "updated_parsed": [2007, 12, 6, 7, 22, 35, 3, 340, 0], "published": "2007-12-06T07:22:35Z", "published_parsed": [2007, 12, 6, 7, 22, 35, 3, 340, 0], "title": "On the \"degrees of freedom\" of the lasso", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0712.0529%2C0712.0614%2C0712.4197%2C0712.0532%2C0712.0407%2C0712.3138%2C0712.2569%2C0712.3510%2C0712.2595%2C0712.0587%2C0712.1176%2C0712.3805%2C0712.3110%2C0712.3587%2C0712.2820%2C0712.1259%2C0712.0194%2C0712.0261%2C0712.3493%2C0712.1382%2C0712.0949%2C0712.1081%2C0712.0175%2C0712.0048%2C0712.1766%2C0712.0864%2C0712.0564%2C0712.3917%2C0712.4390%2C0712.0962%2C0712.1104%2C0712.1993%2C0712.1730%2C0712.4107%2C0712.1007%2C0712.3937%2C0712.3328%2C0712.3350%2C0712.2370%2C0712.0856%2C0712.0880%2C0712.2379%2C0712.1551%2C0712.0920%2C0712.1514%2C0712.2826%2C0712.4002%2C0712.2902%2C0712.0815%2C0712.2917%2C0712.0524%2C0712.3581%2C0712.4029%2C0712.1293%2C0712.3853%2C0712.2845%2C0712.0402%2C0712.3989%2C0712.0562%2C0712.0638%2C0712.1211%2C0712.2059%2C0712.4099%2C0712.0258%2C0712.0995%2C0712.1812%2C0712.1590%2C0712.2586%2C0712.2356%2C0712.0053%2C0712.3778%2C0712.0688%2C0712.2890%2C0712.1236%2C0712.0772%2C0712.1962%2C0712.0395%2C0712.2257%2C0712.1503%2C0712.4348%2C0712.2248%2C0712.3688%2C0712.0881%2C0712.2904%2C0712.0648%2C0712.0534%2C0712.0481%2C0712.2036%2C0712.3491%2C0712.1608%2C0712.3528%2C0712.0679%2C0712.2333%2C0712.3103%2C0712.2346%2C0712.1602%2C0712.2047%2C0712.1020%2C0712.4345%2C0712.1219%2C0712.1683&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On the \"degrees of freedom\" of the lasso"}, "summary": "We study the effective degrees of freedom of the lasso in the framework of\nStein's unbiased risk estimation (SURE). We show that the number of nonzero\ncoefficients is an unbiased estimate for the degrees of freedom of the lasso--a\nconclusion that requires no special assumption on the predictors. In addition,\nthe unbiased estimator is shown to be asymptotically consistent. With these\nresults on hand, various model selection criteria--$C_p$, AIC and BIC--are\navailable, which, along with the LARS algorithm, provide a principled and\nefficient approach to obtaining the optimal lasso fit with the computational\neffort of a single ordinary least-squares fit.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0712.0529%2C0712.0614%2C0712.4197%2C0712.0532%2C0712.0407%2C0712.3138%2C0712.2569%2C0712.3510%2C0712.2595%2C0712.0587%2C0712.1176%2C0712.3805%2C0712.3110%2C0712.3587%2C0712.2820%2C0712.1259%2C0712.0194%2C0712.0261%2C0712.3493%2C0712.1382%2C0712.0949%2C0712.1081%2C0712.0175%2C0712.0048%2C0712.1766%2C0712.0864%2C0712.0564%2C0712.3917%2C0712.4390%2C0712.0962%2C0712.1104%2C0712.1993%2C0712.1730%2C0712.4107%2C0712.1007%2C0712.3937%2C0712.3328%2C0712.3350%2C0712.2370%2C0712.0856%2C0712.0880%2C0712.2379%2C0712.1551%2C0712.0920%2C0712.1514%2C0712.2826%2C0712.4002%2C0712.2902%2C0712.0815%2C0712.2917%2C0712.0524%2C0712.3581%2C0712.4029%2C0712.1293%2C0712.3853%2C0712.2845%2C0712.0402%2C0712.3989%2C0712.0562%2C0712.0638%2C0712.1211%2C0712.2059%2C0712.4099%2C0712.0258%2C0712.0995%2C0712.1812%2C0712.1590%2C0712.2586%2C0712.2356%2C0712.0053%2C0712.3778%2C0712.0688%2C0712.2890%2C0712.1236%2C0712.0772%2C0712.1962%2C0712.0395%2C0712.2257%2C0712.1503%2C0712.4348%2C0712.2248%2C0712.3688%2C0712.0881%2C0712.2904%2C0712.0648%2C0712.0534%2C0712.0481%2C0712.2036%2C0712.3491%2C0712.1608%2C0712.3528%2C0712.0679%2C0712.2333%2C0712.3103%2C0712.2346%2C0712.1602%2C0712.2047%2C0712.1020%2C0712.4345%2C0712.1219%2C0712.1683&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study the effective degrees of freedom of the lasso in the framework of\nStein's unbiased risk estimation (SURE). We show that the number of nonzero\ncoefficients is an unbiased estimate for the degrees of freedom of the lasso--a\nconclusion that requires no special assumption on the predictors. In addition,\nthe unbiased estimator is shown to be asymptotically consistent. With these\nresults on hand, various model selection criteria--$C_p$, AIC and BIC--are\navailable, which, along with the LARS algorithm, provide a principled and\nefficient approach to obtaining the optimal lasso fit with the computational\neffort of a single ordinary least-squares fit."}, "authors": ["Hui Zou", "Trevor Hastie", "Robert Tibshirani"], "author_detail": {"name": "Robert Tibshirani"}, "author": "Robert Tibshirani", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/009053607000000127", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0712.0881v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0712.0881v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/009053607000000127 the\n  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62J05, 62J07, 90C46 (Primary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0712.0881v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0712.0881v1", "journal_reference": "Annals of Statistics 2007, Vol. 35, No. 5, 2173-2192", "doi": "10.1214/009053607000000127", "fulltext": "The Annals of Statistics\n2007, Vol. 35, No. 5, 2173\u20132192\nDOI: 10.1214/009053607000000127\nc Institute of Mathematical Statistics, 2007\n\narXiv:0712.0881v1 [math.ST] 6 Dec 2007\n\nON THE \"DEGREES OF FREEDOM\" OF THE LASSO\nBy Hui Zou, Trevor Hastie and Robert Tibshirani\nUniversity of Minnesota, Stanford University and Stanford University\nWe study the effective degrees of freedom of the lasso in the\nframework of Stein's unbiased risk estimation (SURE). We show that\nthe number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso-a conclusion that requires no special\nassumption on the predictors. In addition, the unbiased estimator is\nshown to be asymptotically consistent. With these results on hand,\nvarious model selection criteria-Cp , AIC and BIC-are available,\nwhich, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.\n\n1. Introduction. The lasso is a popular model building technique that simultaneously produces accurate and parsimonious models (Tibshirani [22]).\nSuppose y = (y1 , . . . , yn )T is the response vector and xj = (x1j , . . . , xnj )T ,\nj = 1, . . . , p, are the linearly independent predictors. Let X = [x1 , . . . , xp ] be\nthe predictor matrix. Assume the data are standardized. The lasso estimates\nfor the coefficients of a linear model are obtained by\n(1.1)\n\n\u03b2\u0302 = arg min y \u2212\n\u03b2\n\np\nX\n\nj=1\n\n2\n\nxj \u03b2j\n\n+\u03bb\n\np\nX\n\nj=1\n\n|\u03b2j |,\n\nwhere \u03bb is called the lasso regularization parameter. What we show in this\npaper is that the number of nonzero components of \u03b2\u0302 is an exact unbiased\nestimate of the degrees of freedom of the lasso, and this result can be used\nto construct adaptive model selection criteria for efficiently selecting the\noptimal lasso fit.\nDegrees of freedom is a familiar phrase for many statisticians. In linear\nregression the degrees of freedom is the number of estimated predictors.\nDegrees of freedom is often used to quantify the model complexity of a\nReceived December 2004; revised November 2006.\nAMS 2000 subject classifications. 62J05, 62J07, 90C46.\nKey words and phrases. Degrees of freedom, LARS algorithm, lasso, model selection,\nSURE, unbiased estimate.\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2007, Vol. 35, No. 5, 2173\u20132192. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nstatistical modeling procedure (Hastie and Tibshirani [10]). However, generally speaking, there is no exact correspondence between the degrees of\nfreedom and the number of parameters in the model (Ye [24]). For example, suppose we first find xj \u2217 such that | cor(xj \u2217 , y)| is the largest among\nall xj , j = 1, 2, . . . , p. We then use xj \u2217 to fit a simple linear regression model\nto predict y. There is one parameter in the fitted model, but the degrees\nof freedom is greater than one, because we have to take into account the\nstochastic search of xj \u2217 .\nStein's unbiased risk estimation (SURE) theory (Stein [21]) gives a rigorous definition of the degrees of freedom for any fitting procedure. Given\na model fitting method \u03b4, let \u03bc\u0302 = \u03b4(y) represent its fit. We assume that\ngiven the x's, y is generated according to y \u223c (\u03bc, \u03c3 2 I), where \u03bc is the true\nmean vector and \u03c3 2 is the common variance. It is shown (Efron [4]) that the\ndegrees of freedom of \u03b4 is\n(1.2)\n\ndf (\u03bc\u0302) =\n\nn\nX\n\ncov(\u03bc\u0302i , yi )/\u03c3 2 .\n\ni=1\n\nFor example, if \u03b4 is a linear smoother, that is, \u03bc\u0302 = Sy for some matrix\nS independent of y, then we have cov(\u03bc\u0302, y) = \u03c3 2 S, df (\u03bc\u0302) = tr(S). SURE\ntheory also reveals the statistical importance of the degrees of freedom.\nWith df defined in (1.2), we can employ the covariance penalty method to\nconstruct a Cp -type statistic as\n(1.3)\n\nCp (\u03bc\u0302) =\n\nky \u2212 \u03bc\u0302k2 2df (\u03bc\u0302) 2\n+\n\u03c3 .\nn\nn\n\nEfron [4] showed that Cp is an unbiased estimator of the true prediction\nerror, and in some settings it offers substantially better accuracy than crossvalidation and related nonparametric methods. Thus degrees of freedom\nplays an important role in model assessment and selection. Donoho and\nJohnstone [3] used the SURE theory to derive the degrees of freedom of\nsoft thresholding and showed that it leads to an adaptive wavelet shrinkage\nprocedure called SureShrink. Ye [24] and Shen and Ye [20] showed that\nthe degrees of freedom can capture the inherent uncertainty in modeling\nand frequentist model selection. Shen and Ye [20] and Shen, Huang and\nYe [19] further proved that the degrees of freedom provides an adaptive\nmodel selection criterion that performs better than the fixed-penalty model\nselection criteria.\nThe lasso is a regularization method which does automatic variable selection. As shown in Figure 1 (the left panel), the lasso continuously shrinks\nthe coefficients toward zero as \u03bb increases; and some coefficients are shrunk\nto exactly zero if \u03bb is sufficiently large. Continuous shrinkage also often improves the prediction accuracy due to the bias\u2013variance trade-off. Detailed\n\n\fDEGREES OF FREEDOM OF THE LASSO\n\n3\n\nFig. 1. Diabetes data with ten predictors. The left panel shows the lasso coefficient estimates \u03b2\u0302j , j = 1, 2, . . . , 10, for the diabetes study. The lasso coefficient estimates are piece\u2013\nwise linear functions of \u03bb (Osborne, Presnell and Turlach [15] and Efron, Hastie, Johnstone and Tibshirani [5]), hence they are piece-wise nonlinear as functions of log(1 + \u03bb).\nThe right panel shows the curve of the proposed unbiased estimate for the degrees of freedom\nof the lasso.\n\ndiscussions on variable selection via penalization are given in Fan and Li [6],\nFan and Peng [8] and Fan and Li [7]. In recent years the lasso has attracted\na lot of attention in both the statistics and machine learning communities. It\nis of great interest to know the degrees of freedom of the lasso for any given\nregularization parameter \u03bb for selecting the optimal lasso model. However,\nit is difficult to derive the analytical expression of the degrees of freedom of\nmany nonlinear modeling procedures, including the lasso. To overcome the\nanalytical difficulty, Ye [24] and Shen and Ye [20] proposed using a dataperturbation technique to numerically compute an (approximately) unbiased\nestimate for df (\u03bc\u0302) when the analytical form of \u03bc\u0302 is unavailable. The bootstrap (Efron [4]) can also be used to obtain an (approximately) unbiased\nestimator of the degrees of freedom. This kind of approach, however, can be\ncomputationally expensive. It is an interesting problem of both theoretical\nand practical importance to derive rigorous analytical results on the degrees\nof freedom of the lasso.\nIn this work we study the degrees of freedom of the lasso in the framework\nof SURE. We show that for any given \u03bb the number of nonzero predictors in\nthe model is an unbiased estimate for the degrees of freedom. This is a finite-\n\n\f4\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nFig. 2. The diabetes data: Cp and BIC curves with ten (top) and 64 (bottom) predictors.\nIn the top panel Cp and BIC select the same model with seven nonzero coefficients. In the\nbottom panel, Cp selects a model with 15 nonzero coefficients and BIC selects a model with\n11 nonzero coefficients.\n\nsample exact result and the result holds as long as the predictor matrix is\na full rank matrix. The importance of the exact finite-sample unbiasedness\nis emphasized in Efron [4], Shen and Ye [20] and Shen and Huang [18]. We\nshow that the unbiased estimator is also consistent. As an illustration, the\n\n\fDEGREES OF FREEDOM OF THE LASSO\n\n5\n\nright panel in Figure 1 displays the unbiased estimate for the degrees of\nfreedom as a function of \u03bb for the diabetes data (with ten predictors).\nThe unbiased estimate of the degrees of freedom can be used to construct\nCp and BIC type model selection criteria. The Cp (or BIC) curve is easily\nobtained once the lasso solution paths are computed by the LARS algorithm\n(Efron, Hastie, Johnstone and Tibshirani [5]). Therefore, with the computational effort of a single OLS fit, we are able to find the optimal lasso fit\nusing our theoretical results. Note that Cp is a finite-sample result and relies on its unbiasedness for prediction error as a basis for model selection\n(Shen and Ye [20], Efron [4]). For this purpose, an unbiased estimate of the\ndegrees of freedom is sufficient. We illustrate the use of Cp and BIC on the\ndiabetes data in Figure 2, where the selected models are indicated by the\nbroken vertical lines.\nThe rest of the paper is organized as follows. We present the main results\nin Section 2. We construct model selection criteria-Cp or BIC-using the\ndegrees of freedom. In Section 3 we discuss the conjecture raised in [5].\nSection 4 contains some technical proofs. Discussion is in Section 5.\n2. Main results. We first define some notation. Let \u03bc\u0302\u03bb be the lasso fit\nusing the representation (1.1). \u03bc\u0302i is the ith component of \u03bc\u0302. For convenience,\nwe let df (\u03bb) stand for df (\u03bc\u0302\u03bb ), the degrees of freedom of the lasso. Suppose\nM is a matrix with p columns. Let S be a subset of the indices {1, 2, . . . , p}.\nDenote by MS the submatrix MS = [* * * Mj * * *]j\u2208S , where Mj is the jth\ncolumn of M. Similarly, define \u03b2S = (* * * \u03b2j * * *)j\u2208S for any vector \u03b2 of length\np. Let Sgn(*) be the sign function: Sgn(x) = 1 if x > 0; Sgn(x) = 0 if x = 0;\nSgn(x) = \u22121 if x = \u22121. Let B = {j : Sgn(\u03b2)j 6= 0} be the active set of \u03b2,\nwhere Sgn(\u03b2) is the sign vector of \u03b2 given by Sgn(\u03b2)j = Sgn(\u03b2j ). We denote\nthe active set of \u03b2\u0302(\u03bb) as B(\u03bb) and the corresponding sign vector Sgn(\u03b2\u0302(\u03bb))\nas Sgn(\u03bb). We do not distinguish between the index of a predictor and the\npredictor itself.\n2.1. The unbiased estimator of df (\u03bb). Before delving into the technical\ndetails, let us review some characteristics of the lasso solution (Efron et al.\n[5]). For a given response vector y, there is a finite sequence of \u03bb's,\n(2.1)\n\n\u03bb0 > \u03bb1 > \u03bb2 > * * * > \u03bbK = 0,\n\nsuch that:\n\u2022 For all \u03bb > \u03bb0 , \u03b2\u0302(\u03bb) = 0.\n\u2022 In the interior of the interval (\u03bbm+1 , \u03bbm ), the active set B(\u03bb) and the sign\nvector Sgn(\u03bb)B(\u03bb) are constant with respect to \u03bb. Thus we write them as\nBm and Sgnm for convenience.\n\n\f6\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nThe active set changes at each \u03bbm . When \u03bb decreases from \u03bb = \u03bbm \u2212 0, some\npredictors with zero coefficients at \u03bbm are about to have nonzero coefficients;\nthus they join the active set Bm . However, as \u03bb approaches \u03bbm+1 +0 there are\npossibly some predictors in Bm whose coefficients reach zero. Hence we call\n{\u03bbm } the transition points. Any \u03bb \u2208 [0, \u221e) \\ {\u03bbm } is called a nontransition\npoint.\nTheorem 1. \u2200\u03bb the lasso fit \u03bc\u0302\u03bb (y) is a uniformly Lipschitz function on\ny. The degrees of freedom of \u03bc\u0302\u03bb (y) equal the expectation of the effective set\nB\u03bb , that is,\ndf (\u03bb) = E|B\u03bb |.\n\n(2.2)\n\nThe identity (2.2) holds as long as X is full rank, that is, rank(X) = p.\nc(\u03bb) = |B | is an unbiased estimate for df (\u03bb). Thus\nTheorem 1 shows that df\n\u03bb\nc(\u03bb) suffices to provide an exact unbiased estimate to the true prediction\ndf\nrisk of the lasso. The importance of the exact finite-sample unbiasedness\nis emphasized in Efron [4], Shen and Ye [20] and Shen and Huang [18].\nOur result is also computationally friendly. Given any data set, the entire\nsolution paths of the lasso are computed by the LARS algorithm (Efron et\nc(\u03bb) = |B | is easily obtained without\nal. [5]); then the unbiased estimator df\n\u03bb\nany extra effort.\nTo prove Theorem 1 we shall proceed by proving a series of lemmas whose\nproofs are relegated to Section 4 for the sake of presentation.\n\nLemma 1. Suppose \u03bb \u2208 (\u03bbm+1 , \u03bbm ). \u03b2\u0302(\u03bb) are the lasso coefficient estimates. Then we have\n\u0012\n\u0013\n\u03bb\nT\n\u22121\nT\n\u03b2\u0302(\u03bb)Bm = (XBm XBm )\nXBm y \u2212 Sgnm .\n(2.3)\n2\nLemma 2. Consider the transition points \u03bbm and \u03bbm+1 , \u03bbm+1 \u2265 0. Bm\nis the active set in (\u03bbm+1 , \u03bbm ). Suppose iadd is an index added into Bm at\n\u03bbm and its index in Bm is i\u2217 , that is, iadd = (Bm )i\u2217 . Denote by (a)k the kth\nelement of the vector a. We can express the transition point \u03bbm as\n(2.4)\n\n\u03bbm =\n\n2((XTBm XBm )\u22121 XTBm y)i\u2217\n.\n((XTBm XBm )\u22121 Sgnm )i\u2217\n\nMoreover, if jdrop is a dropped (if there is any) index at \u03bbm+1 and jdrop =\n(Bm )j \u2217 , then \u03bbm+1 can be written as\n(2.5)\n\n\u03bbm+1 =\n\n2((XTBm XBm )\u22121 XTBm y)j \u2217\n.\n((XTBm XBm )\u22121 Sgnm )j \u2217\n\n\fDEGREES OF FREEDOM OF THE LASSO\n\n7\n\nLemma 3. \u2200\u03bb > 0, \u2203 a null set N\u03bb which is a finite collection of hyperplanes in Rn . Let G\u03bb = Rn \\ N\u03bb . Then \u2200y \u2208 G\u03bb , \u03bb is not any of the transition\npoints, that is, \u03bb \u2208\n/ {\u03bb(y)m }.\nLemma 4.\n\n\u2200\u03bb, \u03b2\u0302\u03bb (y) is a continuous function of y.\n\nLemma 5. Fix any \u03bb > 0 and consider y \u2208 G\u03bb as defined in Lemma 3.\nThe active set B(\u03bb) and the sign vector Sgn(\u03bb) are locally constant with\nrespect to y.\nLemma 6. Let G0 = Rn . Fix an arbitrary \u03bb \u2265 0. On the set G\u03bb with full\nmeasure as defined in Lemma 3, the lasso fit \u03bc\u0302\u03bb (y) is uniformly Lipschitz.\nPrecisely,\n(2.6)\n\nk\u03bc\u0302\u03bb (y + \u2206y) \u2212 \u03bc\u0302\u03bb (y)k \u2264 k\u2206yk\n\nfor sufficiently small \u2206y.\n\nMoreover, we have the divergence formula\n(2.7)\n\n\u2207 * \u03bc\u0302\u03bb (y) = |B\u03bb |.\n\nProof of Theorem 1. Theorem 1 is obviously true for \u03bb = 0. We\nonly need to consider \u03bb > 0. By Lemma 6 \u03bc\u0302\u03bb (y) is uniformly Lipschitz\non G\u03bb . Moreover, \u03bc\u0302\u03bb (y) is a continuous function of y, and thus \u03bc\u0302\u03bb (y) is\nuniformly Lipschitz on Rn . Hence \u03bc\u0302\u03bb (y) is almost differentiable; see Meyer\nand Woodroofe [14] and Efron et al. [5]. Then (2.2) is obtained by invoking\nStein's lemma (Stein [21]) and the divergence formula (2.7). \u0003\nc(\u03bb). In this section we show\n2.2. Consistency of the unbiased estimator df\nc\nthat the obtained unbiased estimator df (\u03bb) is also consistent. We adopt the\nsimilar setup in Knight and Fu [12] for the asymptotic analysis. Assume the\nfollowing two conditions:\n\n1. yi = xi \u03b2 \u2217 + \u03b5i , where \u03b51 , . . . , \u03b5n are i.i.d. normal random variables with\nmean 0 and variance \u03c3 2 , and \u03b2 \u2217 denotes the fixed unknown regression\ncoefficients.\n2. n1 XT X \u2192 C, where C is a positive definite matrix.\nWe consider minimizing an objective function Z\u03bb (\u03b2) defined as\n(2.8)\n\nZ\u03bb (\u03b2) = (\u03b2 \u2212 \u03b2 \u2217 )T C(\u03b2 \u2212 \u03b2 \u2217 ) + \u03bb\n\np\nX\n\nj=1\n\n|\u03b2j |.\n\nOptimizing (2.8) is a lasso type problem: minimizing a quadratic objective\nfunction with an l1 penalty. There are also a finite sequence of transition\npoints {\u03bb\u2217m } associated with optimizing (2.8).\n\n\f8\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\u2217\n\nTheorem 2. If \u03bbnn \u2192 \u03bb\u2217 > 0, where \u03bb\u2217 is a nontransition point such\nc(\u03bb\u2217 ) \u2212 df (\u03bb\u2217 ) \u2192 0 in probability.\nthat \u03bb\u2217 6= \u03bb\u2217m for all m, then df\nn\nn\n\nProof of Theorem 2. Consider \u03b2\u0302 \u2217 = arg min\u03b2 Z\u03bb\u2217 (\u03b2) and let \u03b2\u0302 (n) be\n(n)\nthe lasso solution given in (1.1) with \u03bb = \u03bb\u2217n . Denote B (n) = {j : \u03b2\u0302j 6= 0, 1 \u2264\nj \u2264 p} and B \u2217 = {j : \u03b2\u0302j\u2217 6= 0, 1 \u2264 j \u2264 p}. We want to show P (B (n) = B \u2217 ) \u2192 1.\nFirst, let us consider any j \u2208 B \u2217 . By Theorem 1 in Knight and Fu [12] we\nknow that \u03b2\u0302 (n) \u2192p \u03b2\u0302 \u2217 . Then the continuous mapping theorem implies that\n(n)\nSgn(\u03b2\u0302j ) \u2192p Sgn(\u03b2\u0302j\u2217 ) 6= 0, since Sgn(x) is continuous at all x but zero. Thus\nP (B (n) \u2287 B \u2217 ) \u2192 1. Second, consider any j \u2032 \u2208\n/ B \u2217 . Then \u03b2\u0302j\u2217\u2032 = 0. Since \u03b2\u0302 \u2217 is the\n\u2217\nminimizer of Z\u03bb\u2217 (\u03b2) and \u03bb is not a transition point, by the Karush\u2013Kuhn\u2013\nTucker (KKT) optimality condition (Efron et al. [5], Osborne, Presnell and\nTurlach [15]), we must have\n\u03bb\u2217 > 2|Cj \u2032 (\u03b2 \u2217 \u2212 \u03b2\u0302 \u2217 )|,\n\n(2.9)\n\nwhere Cj \u2032 is the j \u2032 th row vector of C. Let r \u2217 = \u03bb\u2217 \u2212 2|Cj \u2032 (\u03b2 \u2217 \u2212 \u03b2\u0302 \u2217 )| > 0.\nNow let us consider rn = \u03bb\u2217n \u2212 2|xTj\u2032 (y \u2212 X\u03b2\u0302n\u2217 )|. Note that\nxTj\u2032 (y \u2212 X\u03b2\u0302n\u2217 ) = xTj\u2032 X(\u03b2 \u2217 \u2212 \u03b2\u0302n\u2217 ) + xTj\u2032 \u03b5.\n\n(2.10)\nThus\n\n\u2217\nrn\nn\n\n=\n\n\u03bb\u2217n\nn\n\n\u2212 2| n1 xTj\u2032 X(\u03b2 \u2217 \u2212 \u03b2\u0302n\u2217 ) + xTj\u2032 \u03b5/n|. Because \u03b2\u0302 (n) \u2192p \u03b2\u0302 \u2217 and\n\nxTj\u2032 \u03b5/n \u2192p 0, we conclude\n\n\u2217\nrn\nn\n\n(n)\n\n\u2192p r \u2217 > 0. By the KKT optimality condition,\n\nrn\u2217 > 0 implies \u03b2\u0302j \u2032 = 0. Thus P (B \u2217 \u2287 B n ) \u2192 1. Therefore P (B (n) = B \u2217 ) \u2192 1.\n\nc(\u03bb\u2217 ) \u2192 |B \u2217 |. Then invoking the dominated converImmediately we see df\np\nn\ngence theorem we have\n\n(2.11)\n\nc(\u03bb\u2217 )] \u2192 |B \u2217 |.\ndf (\u03bb\u2217n ) = E[df\nn\n\nc(\u03bb\u2217 ) \u2212 df (\u03bb\u2217 ) \u2192p 0. \u0003\nSo df\nn\nn\n\n2.3. Numerical experiments. In this section we check the validity of our\narguments by a simulation study. Here is the outline of the simulation. We\ntake the 64 predictors in the diabetes data set, which include the quadratic\nterms and interactions of the original ten predictors. The positive cone condition is violated on the 64 predictors (Efron et al. [5]). The response vector\n2 .\ny is used to fit an OLS model. We compute the OLS estimates \u03b2\u0302ols and \u03c3\u0302ols\nThen we consider a synthetic model,\n(2.12)\n\ny\u2217 = X\u03b2 + N (0, 1)\u03c3,\n\nwhere \u03b2 = \u03b2\u0302ols and \u03c3 = \u03c3\u0302ols .\n\n\fDEGREES OF FREEDOM OF THE LASSO\n\n9\n\nGiven the synthetic model, the degrees of freedom of the lasso can be\nnumerically evaluated by Monte Carlo methods. For b = 1, 2, . . . , B, we independently simulate y\u2217 (b) from (2.12). For a given \u03bb,P\nby the definition of\n\u2217\ndf , we need to evaluate cov i = cov(\u03bc\u0302i , yi ). Then df = ni=1 covi /\u03c3 2 . Since\nE[yi\u2217 ] = (X\u03b2)i and note that cov i = E[(\u03bc\u0302i \u2212 ai )(yi\u2217 \u2212 (X\u03b2)i )] for any fixed\nknown constant ai . Then we compute\n(2.13)\nPn\n\ncd\nov i =\n\nPB\n\n\u2217\nb=1 (\u03bc\u0302i (b) \u2212 ai )(yi (b) \u2212 (X\u03b2)i )\n\nB\n\n/\u03c3 2 .\n\nov i\nTypically ai = 0 is used in Monte Carlo calculation.\nand df = i=1 cd\nIn this work we use ai = (X\u03b2)i , for it gives a Monte Carlo estimate for\ndf with smaller variance\nthan that given by ai = 0. On the other hand,\nPB c\nwe evaluate E|B\u03bb | by b=1 df (\u03bb)b /B. We are interested in E|B\u03bb | \u2212 df (\u03bb).\nStandard errors are calculated based on the B replications. Figure 3 shows\nvery convincing pictures to support the identity (2.2).\n2.4. Adaptive model selection criteria. The exact value of df (\u03bb) depends\non the underlying model according to Theorem 1. It remains unknown to\nus unless we know the underlying model. Our theory provides a convenient\nunbiased and consistent estimate of the unknown df (\u03bb). In the spirit of\nSURE theory, the good unbiased estimate for df (\u03bb) suffices to provide an\nunbiased estimate for the prediction error of \u03bc\u0302\u03bb as\nky \u2212 \u03bc\u0302k2 2 c\n+ df (\u03bc\u0302)\u03c3 2 .\nn\nn\nConsider the Cp curve as a function of the regularization parameter \u03bb. We\nfind the optimal \u03bb that minimizes Cp . As shown in Shen and Ye [20], this\nmodel selection approach leads to an adaptively optimal model which essentially achieves the optimal prediction risk as if the ideal tuning parameter\nwere given in advance.\nBy the connection between Mallows' Cp (Mallows [13]) and AIC (Akaike [1]),\nwe use the (generalized ) Cp formula (2.14) to equivalently define AIC for\nthe lasso,\n(2.14)\n\nCp (\u03bc\u0302) =\n\nky \u2212 \u03bc\u0302k2 2 c\n+ df (\u03bc\u0302).\nn\u03c3 2\nn\nThe model selection results are identical by Cp and AIC. Following the usual\ndefinition of BIC [16], we propose BIC for the lasso as\n(2.15)\n\nAIC(\u03bc\u0302) =\n\nky \u2212 \u03bc\u0302k2 log(n) c\n+\ndf (\u03bc\u0302).\nn\u03c3 2\nn\nAIC and BIC possess different asymptotic optimality. It is well known that\nAIC tends to select the model with the optimal prediction performance,\n(2.16)\n\nBIC(\u03bc\u0302) =\n\n\f10\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nFig. 3. The synthetic model with the 64 predictors in the diabetes data. In the top panel\nwe compare E|B\u03bb | with the true degrees of freedom df (\u03bb) based on B = 20000 Monte Carlo\nsimulations. The solid line is the 45\u25e6 line (the perfect match). The bottom panel shows\nthe estimation bias and its point-wise 95% confidence intervals are indicated by the thin\ndashed lines. Note that the zero horizontal line is well inside the confidence intervals.\n\nwhile BIC tends to identify the true sparse model if the true model is in the\ncandidate list; see Shao [17], Yang [23] and references therein. We suggest\n\n\fDEGREES OF FREEDOM OF THE LASSO\n\n11\n\nusing BIC as the model selection criterion when the sparsity of the model\nis our primary concern.\nUsing either AIC or BIC to find the optimal lasso model, we are facing\nan optimization problem,\n(2.17)\n\n\u03bb(optimal) = arg min\n\u03bb\n\nky \u2212 \u03bc\u0302\u03bb k2 wn c\ndf (\u03bb),\n+\nn\u03c3 2\nn\n\nwhere wn = 2 for AIC and wn = log(n) for BIC. Since the LARS algorithm\nefficiently solves the lasso solution for all \u03bb, finding \u03bb(optimal) is attainable\nin principle. In fact, we show that \u03bb(optimal) is one of the transition points,\nwhich further facilitates the searching procedure.\nTheorem 3.\n\nTo find \u03bb(optimal), we only need to solve\nm\u2217 = arg min\n\n(2.18)\n\nm\n\nthen \u03bb(optimal) = \u03bbm\u2217 .\n\nky \u2212 \u03bc\u0302\u03bbm k2 wn c\n+\ndf (\u03bbm );\nn\u03c3 2\nn\n\nProof. Let us consider \u03bb \u2208 (\u03bbm+1 , \u03bbm ). By (2.3) we have\nky \u2212 \u03bc\u0302\u03bb k2 = yT (I \u2212 HBm )y +\n\n(2.19)\n\n\u03bb2\nSgnTm (XTBm XBm )\u22121 Sgnm ,\n4\n\nwhere HBm = XBm (XTBm XBm )\u22121 XTBm . Thus we can conclude that ky \u2212 \u03bc\u0302\u03bb k2\nis strictly increasing in the interval (\u03bbm+1 , \u03bbm ). Moreover, the lasso estimates\nare continuous on \u03bb, hence ky \u2212 \u03bc\u0302\u03bbm k2 > ky \u2212 \u03bc\u0302\u03bb k2 > ky \u2212 \u03bc\u0302\u03bbm+1 k2 . On the\nc(\u03bb) = |Bm | \u2200\u03bb \u2208 (\u03bbm+1 , \u03bbm ) and |Bm | \u2265 |B(\u03bbm+1 )|.\nother hand, note that df\nTherefore the optimal choice of \u03bb in [\u03bbm+1 , \u03bbm ) is \u03bbm+1 , which means\n\u03bb(optimal) \u2208 {\u03bbm }. \u0003\nAccording to Theorem 3, the optimal lasso model is immediately selected\nonce we compute the entire lasso solution paths by the LARS algorithm. We\ncan finish the whole fitting and tuning process with the computational cost\nof a single least squares fit.\n3. Efron's conjecture. Efron et al. [5] first considered deriving the analytical form of the degrees of freedom of the lasso. They proposed a stagewise algorithm called LARS to compute the entire lasso solution paths. They\nalso presented the following conjecture on the degrees of freedom of the lasso:\nConjecture 1. Starting at step 0, let mlast\nbe the index of the last\nk\nLARS-lasso sequence containing exactly k nonzero predictors. Then\ndf (\u03bc\u0302mlast ) = k.\nk\n\n\f12\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nNote that Efron et al. [5] viewed the lasso as a forward stage-wise modeling\nalgorithm and used the number of steps as the tuning parameter in the\nlasso: the lasso is regularized by early stopping. In the previous sections\nwe regarded the lasso as a continuous penalization method with \u03bb as its\nregularization parameter. There is a subtle but important difference between\nthe two views. The \u03bb value associated with mlast\nis a random quantity. In\nk\nthe forward stage-wise modeling view of the lasso, the conjecture cannot be\nused for the degrees of freedom of the lasso at a general step k for a prefixed\nk. This is simply because the number of LARS-lasso steps can exceed the\nnumber of all predictors (Efron et al. [5]). In contrast, the unbiasedness\nc(\u03bb) holds for all \u03bb.\nproperty of df\nIn this section we provide some justifications for the conjecture:\n\u2022 We give a much more simplified proof than that in Efron et al. [5] to show\nthat the conjecture is true under the positive cone condition.\n\u2022 Our analysis also indicates that without the positive cone condition the\nconjecture can be wrong, although k is a good approximation of df (\u03bc\u0302mlast ).\nk\n\u2022 We show that the conjecture works appropriately from the model selection\nperspective. If we use the conjecture to construct AIC (or BIC) to select\nthe lasso fit, then the selected model is identical to that selected by AIC\n(or BIC) using the exact degrees of freedom results in Section 2.4.\n\nFirst, we need to show that with probability one we can well define the\nlast LARS-lasso sequence containing exactly k nonzero predictors. Since\nthe conjecture becomes a simple fact for the two trivial cases k = 0 and\nk = p, we only need to consider k = 1, . . . , p \u2212 1. Let \u039bk = {m : |B\u03bbm | = k}, k \u2208\n{1, 2, . . . , (p \u2212 1)}. Then mlast\nk = sup(\u039bk ). However, it may happen that for\nsome k there is no such m with |B\u03bbm | = k. For example, if y is an equiangular\nvector of all {Xj }, then the lasso estimates become the OLS estimates after\njust one step. So \u039bk = \u2205 for k = 2, . . . , p \u2212 1. The next lemma shows that\nthe \"one at a time\" condition (Efron et al. [5]) holds almost everywhere;\ntherefore mlast\nis well defined almost surely.\nk\nLemma 7. Let Wm (y) denote the set of predictors that are to be included\nin the active set at \u03bbm and let Vm (y) be the set of predictors that are deleted\nf0 which is a collection of finite\nfrom the active set at \u03bbm+1 . Then \u2203 a set N\nn\nn\nf\nmany hyperplanes in R . \u2200y \u2208 R \\ N0 ,\n(3.1)\n\n|Wm (y)| \u2264 1\n\nand\n\n|Vm (y)| \u2264 1\n\n\u2200m = 0, 1, . . . , K(y).\n\nf0 is said to be a locally stable point for \u039bk , if \u2200y\u2032 such that\ny \u2208 Rn \\ N\n\u2212 yk \u2264 \u03b5(y) for a small enough \u03b5(y), the effective set B(\u03bbmlast )(y\u2032 ) =\nk\nB(\u03bbmlast )(y). Let LS(k) be the set of all locally stable points.\nk\nThe next lemma helps us evaluate df (\u03bc\u0302mlast ).\n\nky\u2032\n\nk\n\n\fDEGREES OF FREEDOM OF THE LASSO\n\n13\n\nLemma 8. Let \u03bc\u0302m (y) be the lasso fit at the transition point \u03bbm , \u03bbm > 0.\nThen for any i \u2208 Wm , we can write \u03bc\u0302(m) as\n\u03bc\u0302m (y) =\n(3.2)\n\n(3.3)\n\n\u001a\n\nHB(\u03bbm )\n\nXTB(\u03bbm ) (XTB(\u03bbm ) XB(\u03bbm ) ) Sgn(\u03bbm )xTi (I \u2212 HB(\u03bbm ) ) \u001b\ny\n\u2212\nSgni \u2212xTi XTB(\u03bbm ) (XTB(\u03bbm ) XB(\u03bbm ) ) Sgn(\u03bbm )\n=: Sm (y)y,\n\nwhere HB(\u03bbm ) is the projection matrix on the subspace of XB(\u03bbm ) . Moreover\ntr(Sm (y)) = |B(\u03bbm )|.\n\n(3.4)\n\nNote that |B(\u03bbmlast )| = k. Therefore, if y \u2208 LS(k), then\nk\n\n(3.5)\n\n\u2207 * \u03bc\u0302mlast (y) = tr(Smlast (y)) = k.\nk\n\nk\n\nIf the positive cone condition holds then the lasso solution paths are\nmonotone (Efron et al. [5]), hence Lemma 7 implies that LS(k) is a set of\nfull measure. Then by Lemma 8 we know that df (mlast\nk ) = k. However, it\nshould be pointed out that k \u2212 df (mlast\n)\ncan\nbe\nnonzero\nfor some k when\nk\nthe positive cone condition is violated. Here we present an explicit example\nto show this point. We consider the synthetic model in Section 2.3. Note\nthat the positive cone condition is violated on the 64 predictors [5]. As\ndone in Section 2.3, the exact value of df (mlast\nk ) can be computed by Monte\nCarlo and then we evaluate the bias k \u2212 df (mlast\nk ). In the synthetic model\n(2.12) the signal/noise ratio\n\nVar(X\u03b2\u0302ols )\n2\n\u03c3\u0302ols\n\nis about 1.25. We repeated the same\n\nols\nsimulation procedure with (\u03b2 = \u03b2\u0302ols , \u03c3 = \u03c3\u030210\n) in the synthetic model and the\ncorresponding signal/noise ratio became 125. As shown clearly in Figure 4,\nthe bias k \u2212 df (mlast\nk ) is not zero for some k. However, even if the bias\nexists, its maximum magnitude is less than one, regardless of the size of the\nsignal/noise ratio, which suggests that k is a good estimate of df (mlast\nk ).\nLet us pretend the conjecture is true in all situations and then define the\nmodel selection criteria as\n\n(3.6)\n\nky \u2212 \u03bc\u0302mlast k2\nk\n\nn\u03c3 2\n\n+\n\nwn\nk.\nn\n\nwn = 2 for AIC and wn = log(n) for BIC. Treat k as the tuning parameter\nof the lasso. We need to find k(optimal) such that\n(3.7)\n\nk(optimal) = arg min\nk\n\nky \u2212 \u03bc\u0302mlast k2\nk\n\nn\u03c3 2\n\n+\n\nwn\nk.\nn\n\n\f14\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nb (mlast\nFig. 4. B = 20000 replications were used to assess the bias of df\nk ) = k. The 95%\npoint-wise confidence intervals are indicated by the thin dashed lines. This simulation suggests that when the positive cone condition is violated, df (mlast\nk ) 6= k for some k. However,\nthe bias is small (the maximum absolute bias is about 0.8), regardless of the size of the\nsignal/noise ratio.\n\nSuppose \u03bb\u2217 = \u03bb(optimal) and k\u2217 = k(optimal). Theorem 3 implies that the\nmodels selected by (2.17) and (3.7) coincide, that is, \u03bc\u0302\u03bb\u2217 = \u03bc\u0302mlast\n. This obk\u2217\nservation suggests that although the conjecture is not always true, it actually\nworks appropriately for the purpose of model selection.\n4. Proofs of the lemmas. First, let us introduce the following matrix\n\u03bc\u0302\nrepresentation of the divergence. Let \u2202\u2202y\nbe a n \u00d7 n matrix whose elements\nare\n\u0013\n\u0012\n\u2202 \u03bc\u0302\n\u2202 \u03bc\u0302i\n(4.1)\n,\ni, j = 1, 2, . . . , n.\n=\n\u2202y i,j \u2202yj\nThen we can write\n\n\u0012\n\n\u0013\n\n\u2202 \u03bc\u0302\n\u2207 * \u03bc\u0302 = tr\n(4.2)\n.\n\u2202y\nThe above trace expression will be used repeatedly.\nProof of Lemma 1.\n(4.3)\n\nLet\n\nl(\u03b2, y) = y \u2212\n\np\nX\n\nj=1\n\n2\n\nxj \u03b2j\n\n+\u03bb\n\np\nX\n\nj=1\n\n|\u03b2j |.\n\n\f15\n\nDEGREES OF FREEDOM OF THE LASSO\n\nGiven y, \u03b2\u0302(\u03bb) is the minimizer of l(\u03b2, y). For those j \u2208 Bm we must have\n\u2202l(\u03b2,y)\n\u2202\u03b2j = 0, that is,\n(4.4)\n\n\u2212 2xTj\n\ny\u2212\n\np\nX\n\nj=1\n\nxj \u03b2\u0302(\u03bb)j\n\n!\n\nfor j \u2208 Bm .\n\n+ \u03bb Sgn(\u03b2\u0302(\u03bb)j ) = 0,\n\nSince \u03b2\u0302(\u03bb)i = 0 for all i \u2208\n/ Bm , then\nthe equations in (4.4) become\n\nPp\n\nj=1 xj \u03b2\u0302(\u03bb)j\n\n=\n\nP\n\nj\u2208B\u03bb\n\nxj \u03b2\u0302(\u03bb)j . Thus\n\n\u2212 2XTBm (y \u2212 XBm \u03b2\u0302(\u03bb)Bm ) + \u03bb Sgnm = 0,\n\n(4.5)\n\nwhich gives (2.3). \u0003\nProof of Lemma 2. We adopt the matrix notation used in SPLUS:\nM[i, *] means the ith row of M. iadd joins Bm at \u03bbm ; then \u03b2\u0302(\u03bbm )iadd = 0.\nConsider \u03b2\u0302(\u03bb) for \u03bb \u2208 (\u03bbm+1 , \u03bbm ). Lemma 1 gives\n\u0012\n\n\u03b2\u0302(\u03bb)Bm = (XTBm XBm )\u22121 XTBm y \u2212\n\n(4.6)\n\n\u0013\n\n\u03bb\nSgnm .\n2\n\nBy the continuity of \u03b2\u0302(\u03bb)iadd , taking the limit of the i\u2217 th element of (4.6)\nas \u03bb \u2192 \u03bbm \u2212 0, we have\n(4.7)\n\n2{(XTBm XBm )\u22121 [i\u2217 , *]XTBm }y = \u03bbm {(XTBm XBm )\u22121 [i\u2217 , *] Sgnm }.\n\nThe second {*} is a nonzero scalar, otherwise \u03b2\u0302(\u03bb)iadd = 0 for all \u03bb \u2208 (\u03bbm+1 , \u03bbm ),\nwhich contradicts the assumption that iadd becomes a member of the active\nset Bm . Thus we have\n(4.8)\n\n\u001a\n\n\u001b\n\n(XT XBm )\u22121 [i\u2217 , *]\n\u03bbm = 2 T Bm \u22121 \u2217\nXTBm y =: v(Bm , i\u2217 )XTBm y,\n(XBm XBm ) [i , *] Sgnm\n\nwhere v(Bm , i\u2217 ) = {2((XTBm XBm )\u22121 [i\u2217 , *])/((XTBm XBm )\u22121 [i\u2217 , *] Sgnm )}. Rearranging (4.8), we get (2.4).\nSimilarly, if jdrop is a dropped index at \u03bbm+1 , we take the limit of the\n\u2217\nj th element of (4.6) as \u03bb \u2192 \u03bbm+1 + 0 to conclude that\n\u001a\n\n\u001b\n\n(XTBm XBm )\u22121 [j \u2217 , *]\nXTBm y =: v(Bm , j \u2217 )XTBm y,\n(4.9) \u03bbm+1 = 2 T\n(XBm XBm )\u22121 [j \u2217 , *] Sgnm\n\nwhere v(Bm , j \u2217 ) = {2((XTBm XBm )\u22121 [j \u2217 , *])/((XTBm XBm )\u22121 [j \u2217 , *] Sgnm )}. Rearranging (4.9), we get (2.5). \u0003\nProof of Lemma 3. Suppose for some y and m, \u03bb = \u03bb(y)m . \u03bb > 0\nmeans m is not the last lasso step. By Lemma 2 we have\n(4.10)\n\n\u03bb = \u03bbm = {v(Bm , i\u2217 )XTBm }y =: \u03b1(Bm , i\u2217 )y.\n\n\f16\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nObviously \u03b1(Bm , i\u2217 ) = v(Bm , i\u2217 )XTBm is a nonzero vector. Now let \u03b1\u03bb be the\ntotality of \u03b1(Bm , i\u2217 ) by considering all the possible combinations of Bm , i\u2217\nand the sign vector Sgnm . \u03b1\u03bb depends only on X and is a finite set, since at\nmost p predictors are available. Thus \u2200\u03b1 \u2208 \u03b1\u03bb , \u03b1y = \u03bb defines a hyperplane\nin Rn . We define\nN\u03bb = {y : \u03b1y = \u03bb for some \u03b1 \u2208 \u03b1\u03bb } and G\u03bb = Rn \\ N\u03bb .\n\nThen on G\u03bb (4.10) is impossible. \u0003\n\nProof of Lemma 4. For writing convenience we omit the subscript\n\u03bb. Let \u03b2\u0302(y)ols = (XT X)\u22121 XT y be the OLS estimates. Note that we always\nhave the inequality\n|\u03b2\u0302(y)|1 \u2264 |\u03b2\u0302(y)ols |1 .\n\n(4.11)\n\nFix an arbitrary y0 and consider a sequence of {yn } (n = 1, 2, . . .) such\nthat yn \u2192 y0 . Since yn \u2192 y0 , we can find a Y such that kyn k \u2264 Y for all\nn = 0, 1, 2, . . . . Consequently k\u03b2\u0302(yn )ols k \u2264 B for some upper bound B (B\nis determined by X and Y ). By Cauchy's inequality and (4.11), we have\n\u221a\n|\u03b2\u0302(yn )|1 \u2264 pB for all n = 0, 1, 2, . . . . Thus to show \u03b2\u0302(yn ) \u2192 \u03b2\u0302(y0 ), it is\nequivalent to show that for every converging subsequence of {\u03b2\u0302(yn )}, say\n{\u03b2\u0302(ynk )}, the subsequence converges to \u03b2\u0302(y). Now suppose \u03b2\u0302(ynk ) converges\nto \u03b2\u0302\u221e as nk \u2192 \u221e. We show \u03b2\u0302\u221e = \u03b2\u0302(y0 ). The lasso criterion l(\u03b2, y) is written\nin (4.3). Let \u2206l(\u03b2, y, y\u2032 ) = l(\u03b2, y)\u2212l(\u03b2, y\u2032 ). By the definition of \u03b2\u0302nk , we must\nhave\nl(\u03b2\u0302(y0 ), ynk ) \u2265 l(\u03b2\u0302(ynk ), ynk ).\n\n(4.12)\nThen (4.12) gives\n\nl(\u03b2\u0302(y0 ), y0 ) = l(\u03b2\u0302(y0 ), ynk ) + \u2206l(\u03b2\u0302(y0 ), y0 , ynk )\n(4.13)\n\n\u2265 l(\u03b2\u0302(ynk ), ynk ) + \u2206l(\u03b2\u0302(y0 ), y0 , ynk )\n= l(\u03b2\u0302(ynk ), y0 ) + \u2206l(\u03b2\u0302(ynk ), ynk , y0 )\n+ \u2206l(\u03b2\u0302(y0 ), y0 , ynk ).\n\nWe observe\n(4.14)\n\n\u2206l(\u03b2\u0302(ynk ), ynk , y0 ) + \u2206l(\u03b2\u0302(y0 ), y0 , ynk )\n= 2(y0 \u2212 ynk )XT (\u03b2\u0302(ynk ) \u2212 \u03b2\u0302(y0 )).\n\nLet nk \u2192 \u221e; the right-hand side of (4.14) goes to zero. Moreover, l(\u03b2\u0302(ynk ), y0 ) \u2192\nl(\u03b2\u0302\u221e , y0 ). Therefore (4.13) reduces to\nl(\u03b2\u0302(y0 ), y0 ) \u2265 l(\u03b2\u0302\u221e , y0 ).\n\n\fDEGREES OF FREEDOM OF THE LASSO\n\n17\n\nHowever, \u03b2\u0302(y0 ) is the unique minimizer of l(\u03b2, y0 ), and thus \u03b2\u0302\u221e = \u03b2\u0302(y0 ).\n\u0003\nProof of Lemma 5. Fix an arbitrary y0 \u2208 G\u03bb . Denote by Ball(y, r)\nthe n-dimensional ball with center y and radius r. Note that G\u03bb is an open\nset, so we can choose a small enough \u03b5 such that Ball(y0 , \u03b5) \u2282 G\u03bb . Fix \u03b5.\nSuppose yn \u2192 y as n \u2192 \u221e. Then without loss of generality we can assume\nyn \u2208 Ball(y0 , \u03b5) for all n. So \u03bb is not a transition point for any yn .\nBy definition \u03b2\u0302(y0 )j 6= 0 for all j \u2208 B(y0 ). Then Lemma 4 says that\n\u2203 an N1 , and as long as n > N1 , we have \u03b2\u0302(yn )j 6= 0 and Sgn(\u03b2\u0302(yn )) =\nSgn(\u03b2\u0302(yn )), for all j \u2208 B(y0 ). Thus B(y0 ) \u2286 B(yn ) \u2200n > N1 .\nOn the other hand, we have the equiangular conditions (Efron et al. [5])\n(4.15)\n\n\u03bb = 2|xTj (y0 \u2212 X\u03b2\u0302(y0 ))|\n\n\u2200j \u2208 B(y0 ),\n\n(4.16)\n\n\u03bb > 2|xTj (y0 \u2212 X\u03b2\u0302(y0 ))|\n\n\u2200j \u2208\n/ B(y0 ).\n\n/ B(y0 )\nUsing Lemma 4 again, we conclude that \u2203 an N > N1 such that \u2200j \u2208\nthe strict inequalities (4.16) hold for yn provided n > N . Thus B c (y0 ) \u2286\nB c (yn ) \u2200n > N . Therefore we have B(yn ) = B(y0 ) \u2200n > N . Then the local\nconstancy of the sign vector follows the continuity of \u03b2\u0302(y). \u0003\nProof of Lemma 6. If \u03bb = 0, then the lasso fit is just the OLS fit.\nThe conclusions are easy to verify. So we focus on \u03bb > 0. Fix an y. Choose\na small enough \u03b5 such that Ball(y, \u03b5) \u2282 G\u03bb .\nSince \u03bb is not any transition point, using (2.3) we observe\n(4.17)\n\n\u03bc\u0302\u03bb (y) = X\u03b2\u0302(y) = H\u03bb (y)y \u2212 \u03bb\u03c9\u03bb (y),\n\nwhere H\u03bb (y) = XB\u03bb (XTB\u03bb XB\u03bb )\u22121 XTB\u03bb is the projection matrix on the space\nXB\u03bb and \u03c9\u03bb (y) = 12 XB\u03bb (XTB\u03bb XB\u03bb )\u22121 SgnB\u03bb . Consider k\u2206yk < \u03b5. Similarly,\nwe get\n(4.18)\n\n\u03bc\u0302\u03bb (y + \u2206y) = H\u03bb (y + \u2206y)(y + \u2206y) \u2212 \u03bb\u03c9 \u03bb (y + \u2206y).\n\nLemma 5 says that we can further let \u03b5 be sufficiently small such that\nboth the effective set B\u03bb and the sign vector Sgn\u03bb stay constant in Ball(y, \u03b5).\nNow fix \u03b5. Hence if k\u2206yk < \u03b5, then\n(4.19)\n\nH\u03bb (y + \u2206y) = H\u03bb (y)\n\nand \u03c9 \u03bb (y + \u2206y) = \u03c9 \u03bb (y).\n\nThen (4.17) and (4.18) give\n(4.20)\n\n\u03bc\u0302\u03bb (y + \u2206y) \u2212 \u03bc\u0302\u03bb (y) = H\u03bb (y)\u2206y.\n\nBut since kH\u03bb (y)\u2206yk \u2264 k\u2206yk, (2.6) is proved.\n\n\f18\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nBy the local constancy of H(y) and \u03c9(y), we have\n\u2202 \u03bc\u0302\u03bb (y)\n= H\u03bb (y).\n\u2202y\n\n(4.21)\n\nThen the trace formula (4.2) implies that\n\u2207 * \u03bc\u0302\u03bb (y) = tr(H\u03bb (y)) = |B\u03bb |.\n\n(4.22)\n\n\u0003\n\nProof of Lemma 7. Suppose at step m, |Wm (y)| \u2265 2. Let iadd and jadd\n\u2217\nbe two of the predictors in Wm (y), and let i\u2217add and jadd\nbe their indices in\nthe current active set A. Note the current active set A is Bm in Lemma 2.\nHence we have\n\u03bbm = v[A, i\u2217 ]XTA y\n\n(4.23)\n\nand\n\n\u03bbm = v[A, j \u2217 ]XTA y.\n\nTherefore\n\u2217\n0 = {[v(A, i\u2217add ) \u2212 v(A, jadd\n)]XTA }y =: \u03b1add y.\n\n(4.24)\n\n\u2217 )]XT is not a zero vector. Otherwise,\nWe claim \u03b1add = [v(A, i\u2217add ) \u2212 v(A, jadd\nA\n\u2217 )=\nsince {Xj } are linearly independent, \u03b1add = 0 forces v(A, i\u2217add ) \u2212 v(A, jadd\n0. Then we have\n(XTA XA )\u22121 [j \u2217 , *]\n(XTA XA )\u22121 [i\u2217 , *]\n(4.25)\n=\n,\n(XTA XA )\u22121 [i\u2217 , *] SgnA (XTA XA )\u22121 [i\u2217 , *] SgnA\n\nwhich contradicts the fact (XTA XA )\u22121 is a full rank matrix.\nSimilarly, if idrop and jdrop are dropped predictors, then\n\n\u2217\n0 = {[v(A, i\u2217drop ) \u2212 v(A, jdrop\n)]XTA }y =: \u03b1drop y,\n\n(4.26)\n\n\u2217\nand \u03b1drop = [v(A, i\u2217drop ) \u2212 v(A, jdrop\n)]XTA is a nonzero vector.\nLet M0 be the totality of \u03b1add and \u03b1drop by considering all the possible\ncombinations of A, (iadd , jadd ), (idrop , jdrop ) and SgnA . Clearly M0 is a finite\nset and depends only on X. Let\n\nf0 = {y : \u03b1y = 0 for some \u03b1 \u2208 M0 }.\nN\n\n(4.27)\n\nf0 the conclusion holds. \u0003\nThen on Rn \\ N\n\nProof of Lemma 8. Note that \u03b2\u0302(\u03bb) is continuous on \u03bb. Using (4.4) in\nLemma 1 and taking the limit of \u03bb \u2192 \u03bbm , we have\n(4.28) \u2212 2xTj y \u2212\nHowever,\n\n(4.29)\n\nPp\n\np\nX\n\nxj \u03b2\u0302(\u03bbm )j\n\nj=1\n\nj=1 xj \u03b2\u0302(\u03bbm )j\n\n=\n\nP\n\n!\n\n+ \u03bbm Sgn(\u03b2\u0302(\u03bbm )j ) = 0,\n\nj\u2208B(\u03bbm ) xj \u03b2\u0302(\u03bbm )j .\n\n\u0012\n\nfor j \u2208 B(\u03bbm ).\n\nThus we have\n\n\u03b2\u0302(\u03bbm ) = (XTB(\u03bbm ) XB(\u03bbm ) )\u22121 XTB(\u03bbm ) y \u2212\n\n\u0013\n\n\u03bbm\nSgn(\u03bbm ) .\n2\n\n\f19\n\nDEGREES OF FREEDOM OF THE LASSO\n\nHence\n\n(4.30)\n\n\u0012\n\n\u03bc\u0302m (y) = XB(\u03bbm ) (XTB(\u03bbm ) XB(\u03bbm ) )\u22121 XTB(\u03bbm ) y \u2212\n\n\u0013\n\n\u03bbm\nSgn(\u03bbm )\n2\n\n= HB(\u03bbm ) y \u2212 XB(\u03bbm ) (XTB(\u03bbm ) XB(\u03bbm ) )\u22121 Sgn(\u03bbm )\n\n\u03bbm\n.\n2\n\nSince i \u2208 Wm , we must have the equiangular condition\n\n\u03bbm\n.\n2\nSubstituting (4.30) into (4.31), we solve \u03bbm /2 and obtain\nSgni xTi (y \u2212 \u03bc\u0302(m)) =\n\n(4.31)\n\n(4.32)\n\nxTi (I \u2212 HB(\u03bbm ) )y\n\u03bbm\n=\n.\n2\nSgni \u2212xTi XTB(\u03bbm ) (XTB(\u03bbm ) XB(\u03bbm ) ) Sgn(\u03bbm )\n\nThen putting (4.32) back to (4.30) yields (3.2).\nUsing the identity tr(AB) = tr(BA), we observe\ntr(Sm (y) \u2212 HB(\u03bbm ) ) = tr\n\n\u0013\nT\nT\nB(\u03bbm ) XB(\u03bbm ) ) Sgn(\u03bbm )xi (I \u2212 HB(\u03bbm ) )XB(\u03bbm )\nSgni \u2212xTi XTB(\u03bbm ) (XTB(\u03bbm ) XB(\u03bbm ) ) Sgn(\u03bbm )\n\n\u0012 (XT\n\n= tr(0) = 0.\nSo tr(Sm (y)) = tr(HB(\u03bbm ) ) = |B(\u03bbm )|. \u0003\n5. Discussion. In this article we have proven that the number of nonzero\ncoefficients is an unbiased estimate of the degrees of freedom of the lasso.\nThe unbiased estimator is also consistent. We think it is a neat yet surprising result. Even in other sparse modeling methods, there is no such clean\nrelationship between the number of nonzero coefficients and the degrees of\nfreedom. For example, the number of nonzero coefficients is not an unbiased\nestimate of the degrees of freedom of the elastic net (Zou [26]). Another\npossible counterexample is the SCAD (Fan and Li [6]) whose solution is\neven more complex than the lasso. Note that with orthogonal predictors,\nthe SCAD estimates can be obtained by the SCAD shrinkage formula (Fan\nand Li [6]). Then it is not hard to check that with orthogonal predictors the\nnumber of nonzero coefficients in the SCAD estimates cannot be an unbiased\nestimate of its degrees of freedom.\nThe techniques developed in this article can be applied to derive the\ndegrees of freedom of other nonlinear estimating procedures, especially when\nthe estimates have piece-wise linear solution paths. Gunter and Zhu [9] used\nour arguments to derive an unbiased estimate of the degrees of freedom of\nsupport vector regression. Zhao, Rocha and Yu [25] derived an unbiased\nestimate of the degrees of freedom of the regularized estimates using the\nCAP penalties.\n\n\f20\n\nH. ZOU, T. HASTIE AND R. TIBSHIRANI\n\nB\u00fchlmann and Yu [2] defined the degrees of freedom of L2 boosting as\nthe trace of the product of a series of linear smoothers. Their approach\ntakes advantage of the closed-form expression for the L2 fit at each boosting\nstage. It is now well known that \u03b5-L2 boosting is (almost) identical to the\nlasso (Hastie, Tibshirani and Friedman [11], Efron et al. [5]). Their work\nprovides another look at the degrees of freedom of the lasso. However, it\nis not clear whether their definition agrees with the SURE definition. This\ncould be another interesting topic for future research.\nAcknowledgments. Hui Zou sincerely thanks Brad Efron, Yuhong Yang\nand Xiaotong Shen for their encouragement and suggestions. We sincerely\nthank the Co-Editor Jianqing Fan, an Associate Editor and two referees for\nhelpful comments which greatly improved the manuscript.\nREFERENCES\n[1] Akaike, H. (1973). Information theory and an extension of the maximum likelihood\nprinciple. In Second International Symposium on Information Theory (B. N.\nPetrov and F. Cs\u00e1ki, eds.) 267\u2013281. Acad\u00e9miai Kiad\u00f3, Budapest. MR0483125\n[2] B\u00fchlmann, P. and Yu, B. (2005). Boosting, model selection, lasso and nonnegative\ngarrote. Technical report, ETH Z\u00fcrich.\n[3] Donoho, D. and Johnstone, I. (1995). Adapting to unknown smoothness via\nwavelet shrinkage. J. Amer. Statist. Assoc. 90 1200\u20131224. MR1379464\n[4] Efron, B. (2004). The estimation of prediction error: Covariance penalties and crossvalidation (with discussion). J. Amer. Statist. Assoc. 99 619\u2013642. MR2090899\n[5] Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angle\nregression (with discussion). Ann. Statist. 32 407\u2013499. MR2060166\n[6] Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and\nits oracle properties. J. Amer. Statist. Assoc. 96 1348\u20131360. MR1946581\n[7] Fan, J. and Li, R. (2006). Statistical challenges with high dimensionality: Feature\nselection in knowledge discovery. In Proc. International Congress of Mathematicians 3 595\u2013622. European Math. Soc., Z\u00fcrich. MR2275698\n[8] Fan, J. and Peng, H. (2004). Nonconcave penalized likelihood with a diverging\nnumber of parameters. Ann. Statist. 32 928\u2013961. MR2065194\n[9] Gunter, L. and Zhu, J. (2007). Efficient computation and model selection for the\nsupport vector regression. Neural Computation 19 1633\u20131655.\n[10] Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models. Chapman and\nHall, London. MR1082147\n[11] Hastie, T., Tibshirani, R. and Friedman, J. (2001). The Elements of Statistical Learning; Data Mining, Inference and Prediction. Springer, New York.\nMR1851606\n[12] Knight, K. and Fu, W. (2000). Asymptotics for lasso-type estimators. Ann. Statist.\n28 1356\u20131378. MR1805787\n[13] Mallows, C. (1973). Some comments on CP . Technometrics 15 661\u2013675.\n[14] Meyer, M. and Woodroofe, M. (2000). On the degrees of freedom in shaperestricted regression. Ann. Statist. 28 1083\u20131104. MR1810920\n[15] Osborne, M., Presnell, B. and Turlach, B. (2000). A new approach to variable selection in least squares problems. IMA J. Numer. Anal. 20 389\u2013403.\nMR1773265\n\n\fDEGREES OF FREEDOM OF THE LASSO\n\n21\n\n[16] Schwarz, G. (1978). Estimating the dimension of a model. Ann. Statist. 6 461\u2013464.\nMR0468014\n[17] Shao, J. (1997). An asymptotic theory for linear model selection (with discussion).\nStatist. Sinica 7 221\u2013264. MR1466682\n[18] Shen, X. and Huang, H.-C. (2006). Optimal model assessment, selection and combination. J. Amer. Statist. Assoc. 101 554\u2013568. MR2281243\n[19] Shen, X., Huang, H.-C. and Ye, J. (2004). Adaptive model selection and assessment\nfor exponential family distributions. Technometrics 46 306\u2013317. MR2082500\n[20] Shen, X. and Ye, J. (2002). Adaptive model selection. J. Amer. Statist. Assoc. 97\n210\u2013221. MR1947281\n[21] Stein, C. (1981). Estimation of the mean of a multivariate normal distribution. Ann.\nStatist. 9 1135\u20131151. MR0630098\n[22] Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. Roy.\nStatist. Soc. Ser. B 58 267\u2013288. MR1379242\n[23] Yang, Y. (2005). Can the strengths of AIC and BIC be shared?-A conflict between model identification and regression estimation. Biometrika 92 937\u2013950.\nMR2234196\n[24] Ye, J. (1998). On measuring and correcting the effects of data mining and model\nselection. J. Amer. Statist. Assoc. 93 120\u2013131. MR1614596\n[25] Zhao, P., Rocha, G. and Yu, B. (2006). Grouped and hierarchical model selection\nthrough composite absolute penalties. Technical report, Dept. Statistics, Univ.\nCalifornia, Berkeley.\n[26] Zou, H. (2005). Some perspectives of sparse statistical modeling. Ph.D. dissertation,\nDept. Statistics, Stanford Univ.\nH. Zou\nSchool of Statistics\nUniversity of Minnesota\nMinneapolis, Minnesota 55455\nUSA\nE-mail: hzou@stat.umn.edu\n\nT. Hastie\nR. Tibshirani\nDepartment of Statistics\nStanford University\nStanford, California 94305\nUSA\nE-mail: hastie@stanford.edu\ntibs@stanford.edu\n\n\f"}