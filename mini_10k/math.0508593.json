{"id": "http://arxiv.org/abs/math/0508593v1", "guidislink": true, "updated": "2005-08-30T05:48:17Z", "updated_parsed": [2005, 8, 30, 5, 48, 17, 1, 242, 0], "published": "2005-08-30T05:48:17Z", "published_parsed": [2005, 8, 30, 5, 48, 17, 1, 242, 0], "title": "A Bayesian \u03c7^2 test for goodness-of-fit", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0508035%2Cmath%2F0508326%2Cmath%2F0508355%2Cmath%2F0508192%2Cmath%2F0508061%2Cmath%2F0508505%2Cmath%2F0508635%2Cmath%2F0508377%2Cmath%2F0508133%2Cmath%2F0508422%2Cmath%2F0508440%2Cmath%2F0508590%2Cmath%2F0508420%2Cmath%2F0508534%2Cmath%2F0508348%2Cmath%2F0508515%2Cmath%2F0508406%2Cmath%2F0508513%2Cmath%2F0508612%2Cmath%2F0508015%2Cmath%2F0508410%2Cmath%2F0508400%2Cmath%2F0508508%2Cmath%2F0508082%2Cmath%2F0508547%2Cmath%2F0508405%2Cmath%2F0508479%2Cmath%2F0508466%2Cmath%2F0508485%2Cmath%2F0508610%2Cmath%2F0508016%2Cmath%2F0508037%2Cmath%2F0508224%2Cmath%2F0508593%2Cmath%2F0508333%2Cmath%2F0508564%2Cmath%2F0508478%2Cmath%2F0508094%2Cmath%2F0508160%2Cmath%2F0508554%2Cmath%2F0508117%2Cmath%2F0508152%2Cmath%2F0508496%2Cmath%2F0508345%2Cmath%2F0508379%2Cmath%2F0508048%2Cmath%2F0508260%2Cmath%2F0508331%2Cmath%2F0508424%2Cmath%2F0508256%2Cmath%2F0508214%2Cmath%2F0508431%2Cmath%2F0508356%2Cmath%2F0508603%2Cmath%2F0508321%2Cmath%2F0508207%2Cmath%2F0508051%2Cmath%2F0508365%2Cmath%2F0508294%2Cmath%2F0508041%2Cmath%2F0508552%2Cmath%2F0508291%2Cmath%2F0508463%2Cmath%2F0508404%2Cmath%2F0508583%2Cmath%2F0508188%2Cmath%2F0508481%2Cmath%2F0508304%2Cmath%2F0508209%2Cmath%2F0508498%2Cmath%2F0508368%2Cmath%2F0508484%2Cmath%2F0508351%2Cmath%2F0508077%2Cmath%2F0508084%2Cmath%2F0508512%2Cmath%2F0508157%2Cmath%2F0508625%2Cmath%2F0508072%2Cmath%2F0508536%2Cmath%2F0508452%2Cmath%2F0508218%2Cmath%2F0508106%2Cmath%2F0508402%2Cmath%2F0508460%2Cmath%2F0508525%2Cmath%2F0508122%2Cmath%2F0508537%2Cmath%2F0508111%2Cmath%2F0508339%2Cmath%2F0508607%2Cmath%2F0508272%2Cmath%2F0508020%2Cmath%2F0508523%2Cmath%2F0508388%2Cmath%2F0508518%2Cmath%2F0508340%2Cmath%2F0508047%2Cmath%2F0508592%2Cmath%2F0508363%2Cmath%2F0508306&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Bayesian \u03c7^2 test for goodness-of-fit"}, "summary": "This article describes an extension of classical \\chi^2 goodness-of-fit tests\nto Bayesian model assessment. The extension, which essentially involves\nevaluating Pearson's goodness-of-fit statistic at a parameter value drawn from\nits posterior distribution, has the important property that it is\nasymptotically distributed as a \\chi^2 random variable on K-1 degrees of\nfreedom, independently of the dimension of the underlying parameter vector. By\nexamining the posterior distribution of this statistic, global goodness-of-fit\ndiagnostics are obtained. Advantages of these diagnostics include ease of\ninterpretation, computational convenience and favorable power properties. The\nproposed diagnostics can be used to assess the adequacy of a broad class of\nBayesian models, essentially requiring only a finite-dimensional parameter\nvector and conditionally independent observations.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0508035%2Cmath%2F0508326%2Cmath%2F0508355%2Cmath%2F0508192%2Cmath%2F0508061%2Cmath%2F0508505%2Cmath%2F0508635%2Cmath%2F0508377%2Cmath%2F0508133%2Cmath%2F0508422%2Cmath%2F0508440%2Cmath%2F0508590%2Cmath%2F0508420%2Cmath%2F0508534%2Cmath%2F0508348%2Cmath%2F0508515%2Cmath%2F0508406%2Cmath%2F0508513%2Cmath%2F0508612%2Cmath%2F0508015%2Cmath%2F0508410%2Cmath%2F0508400%2Cmath%2F0508508%2Cmath%2F0508082%2Cmath%2F0508547%2Cmath%2F0508405%2Cmath%2F0508479%2Cmath%2F0508466%2Cmath%2F0508485%2Cmath%2F0508610%2Cmath%2F0508016%2Cmath%2F0508037%2Cmath%2F0508224%2Cmath%2F0508593%2Cmath%2F0508333%2Cmath%2F0508564%2Cmath%2F0508478%2Cmath%2F0508094%2Cmath%2F0508160%2Cmath%2F0508554%2Cmath%2F0508117%2Cmath%2F0508152%2Cmath%2F0508496%2Cmath%2F0508345%2Cmath%2F0508379%2Cmath%2F0508048%2Cmath%2F0508260%2Cmath%2F0508331%2Cmath%2F0508424%2Cmath%2F0508256%2Cmath%2F0508214%2Cmath%2F0508431%2Cmath%2F0508356%2Cmath%2F0508603%2Cmath%2F0508321%2Cmath%2F0508207%2Cmath%2F0508051%2Cmath%2F0508365%2Cmath%2F0508294%2Cmath%2F0508041%2Cmath%2F0508552%2Cmath%2F0508291%2Cmath%2F0508463%2Cmath%2F0508404%2Cmath%2F0508583%2Cmath%2F0508188%2Cmath%2F0508481%2Cmath%2F0508304%2Cmath%2F0508209%2Cmath%2F0508498%2Cmath%2F0508368%2Cmath%2F0508484%2Cmath%2F0508351%2Cmath%2F0508077%2Cmath%2F0508084%2Cmath%2F0508512%2Cmath%2F0508157%2Cmath%2F0508625%2Cmath%2F0508072%2Cmath%2F0508536%2Cmath%2F0508452%2Cmath%2F0508218%2Cmath%2F0508106%2Cmath%2F0508402%2Cmath%2F0508460%2Cmath%2F0508525%2Cmath%2F0508122%2Cmath%2F0508537%2Cmath%2F0508111%2Cmath%2F0508339%2Cmath%2F0508607%2Cmath%2F0508272%2Cmath%2F0508020%2Cmath%2F0508523%2Cmath%2F0508388%2Cmath%2F0508518%2Cmath%2F0508340%2Cmath%2F0508047%2Cmath%2F0508592%2Cmath%2F0508363%2Cmath%2F0508306&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This article describes an extension of classical \\chi^2 goodness-of-fit tests\nto Bayesian model assessment. The extension, which essentially involves\nevaluating Pearson's goodness-of-fit statistic at a parameter value drawn from\nits posterior distribution, has the important property that it is\nasymptotically distributed as a \\chi^2 random variable on K-1 degrees of\nfreedom, independently of the dimension of the underlying parameter vector. By\nexamining the posterior distribution of this statistic, global goodness-of-fit\ndiagnostics are obtained. Advantages of these diagnostics include ease of\ninterpretation, computational convenience and favorable power properties. The\nproposed diagnostics can be used to assess the adequacy of a broad class of\nBayesian models, essentially requiring only a finite-dimensional parameter\nvector and conditionally independent observations."}, "authors": ["Valen E. Johnson"], "author_detail": {"name": "Valen E. Johnson"}, "author": "Valen E. Johnson", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/009053604000000616", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/math/0508593v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0508593v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published at http://dx.doi.org/10.1214/009053604000000616 in the\n  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62C10 (Primary) 62E20. (Secondary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0508593v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0508593v1", "journal_reference": "Annals of Statistics 2004, Vol. 32, No. 6, 2361-2384", "doi": "10.1214/009053604000000616", "fulltext": "arXiv:math/0508593v1 [math.ST] 30 Aug 2005\n\nThe Annals of Statistics\n2004, Vol. 32, No. 6, 2361\u20132384\nDOI: 10.1214/009053604000000616\nc Institute of Mathematical Statistics, 2004\n\nA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\nBy Valen E. Johnson\nUniversity of Michigan\nThis article describes an extension of classical \u03c72 goodness-of-fit\ntests to Bayesian model assessment. The extension, which essentially\ninvolves evaluating Pearson's goodness-of-fit statistic at a parameter\nvalue drawn from its posterior distribution, has the important property that it is asymptotically distributed as a \u03c72 random variable on\nK \u2212 1 degrees of freedom, independently of the dimension of the underlying parameter vector. By examining the posterior distribution\nof this statistic, global goodness-of-fit diagnostics are obtained. Advantages of these diagnostics include ease of interpretation, computational convenience and favorable power properties. The proposed\ndiagnostics can be used to assess the adequacy of a broad class of\nBayesian models, essentially requiring only a finite-dimensional parameter vector and conditionally independent observations.\n\n1. Introduction. Model assessment presents a challenge to Bayesian statisticians, one that has become an increasingly serious problem as computational advances have made it possible to entertain models of a complexity\nnot considered even a decade ago. Because diagnostic methods have not\nkept pace with these computational advances, practitioners are often faced\nwith the prospect of interpreting results from a model that has not been\nadequately validated.\nNumerous solutions to this problem have been considered. The most orthodox of these depend on the specification of alternative models and the use\nof Bayes factors for model selection. This approach is reasonable when both\na relatively broad class of models can be specified as alternatives, and when\nimplied Bayes factors can be readily computed. Unfortunately, it often happens in practice that neither requirement is satisfied, making this approach\nimpractical for routine application. Complicating the situation still further is\nReceived May 2003; revised February 2004.\nAMS 2000 subject classifications. Primary 62C10; secondary 62E20.\nKey words and phrases. Bayesian model assessment, Pearson's chi-squared statistic,\nposterior-predictive diagnostics, p-value, Bayes factor, intrinsic Bayes factor, discrepancy\nfunctions.\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2004, Vol. 32, No. 6, 2361\u20132384. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nV. E. JOHNSON\n\nthe fact that Bayes factors are not defined when improper priors are used in\nmodel specification, although this difficulty may be partially circumvented\nthrough the use of intrinsic Bayes factors or related devices [e.g., Berger and\nPericchi (1996) and O'Hagan (1995)].\nA second strategy for assessing model adequacy centers on the use of\nposterior-predictive model checks. This approach was initially proposed by\nGuttman (1967) and Rubin (1984), and was extended to more general discrepancy functions by Gelman, Meng and Stern (1996). [Gelfand (1996)\nhas advocated related techniques based on cross-validatory predictive densities.] The primary advantage of posterior-predictive model assessment is\nits relative ease of implementation. In many models, the output from numerical algorithms used to generate samples from the posterior distribution\ncan be used to generate observations from the predictive model, which in\nturn can be used to compute p-values for the discrepancy function of interest. Posterior-predictive model assessment also facilitates case-diagnostics,\nwhich, in many circumstances, are more telling in examining model fit than\nare global goodness-of-fit statistics. However, such approaches also have an\nimportant disadvantage. As Bayarri and Berger (2000) and Robins, van\nder Vaart and Ventura (2000) and others have noted, they do not produce p-values that have (even asymptotically) a uniform distribution. Because output from predictive posterior model checks is not calibrated, using\np-values based on them for model assessment is problematic.\nBayarri and Berger (2000) and Robins, van der Vaart and Ventura (2000)\npropose alternative distributions under which p-values, and thus model diagnostics, can be calculated. These include partial posterior-predictive p-values\nand conditional predictive p-values [Bayarri and Berger (2000)], and modifications to posterior predictive and \"plug-in\" p-values [Robins, van der Vaart\nand Ventura (2000)]. The attractive feature of each of these variations on\nmore standard definitions of p-values is that these statistics are distributed\neither as U (0, 1) random variables, or approach U (0, 1) random variables\nas sample sizes become large. Their drawback is that they can seldom be\ndefined and calculated in realistically complex models.\nThe goal of this article is to present a goodness-of-fit diagnostic that\nbridges the gap between diagnostics that are easy to compute but whose\nnull distributions are unknown, and diagnostics whose null distributions are\nknown but that cannot generally be computed. The proposed diagnostic is\nclosely related to the classical \u03c72 goodness-of-fit statistic, whose properties\nare now briefly reviewed.\nIn the case of a point null hypothesis, the standard \u03c72 statistic may be\ndefined as\nR0 =\n\nK\nX\n(mk \u2212 npk )2\n\nk=1\n\nnpk\n\n,\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n3\n\nwhere mk represents the number of observations observed within the kth partitioning element, pk the probability assigned by the null model to this interval, K the number of partitions or intervals specified over the sample space\nand n the sample size. For independent and identically distributed data satisfying certain regularity requirements, Pearson (1900) demonstrated that\nthe asymptotic distribution of R0 was \u03c72 with K \u2212 1 degrees of freedom.\nThe situation for composite hypotheses is more complicated. Assuming\nthat bins are determined a priori, Cram\u00e9r [(1946), pages 426\u2013434] demonstrated that the distribution of\nRg =\n\nK\nX\n(mk \u2212 npgk )2\n\nk=1\n\nnpgk\n\nis that of a \u03c72 random variable with K \u2212 s \u2212 1 degrees of freedom, where\ns denotes the dimension of the underlying parameter vector \u03b8 and {pgk } denote estimates of the bin probabilities based on either maximum likelihood\nestimation for the grouped data or on the minimum \u03c72 method. Maximum\nlikelihood estimation for the grouped data implies maximization of the function\nY\n\npk (\u03b8)mk\n\nk\n\nwith respect to \u03b8, while minimum \u03c72 estimation involves the determination\nof a value of \u03b8 that minimizes a function related to Rg .\nThe statistic Rg is the form of the \u03c72 test most often used in statistics,\nwhere it is routinely used to test independence in contingency tables [see,\ne.g., Fienberg (1980)]. In that context, grouped maximum likelihood estimation is natural. Although the Bayesian \u03c72 statistic proposed below can\nbe extended for testing independence in contingency tables, this is not its\nintended purpose. Instead, it is intended primarily for use as a goodnessof-fit test. In this regard, the aspect of model fit assessed is similar to that\nexamined using the classical \u03c72 goodness-of-fit test; namely, the proportion\nof counts observed in predefined parcels of the sample space is compared to\nthe proportion of counts that are expected in these parcels under a specified\nprobability model.\nChernoff and Lehmann (1954) considered the distribution of the \u03c72 statistic in the more typical situation in which values of the bin probabilities\nare based on maximum likelihood estimates obtained using the raw (ungrouped) data. Denote these values by p\u0302k . In this case, the distribution of\nthe goodness-of-fit statistic is generally not one of a \u03c72 distribution, but\ninstead produces a value R\u0302 that has a distribution that falls stochastically\nbetween R0 and Rg . For models containing many parameters, the gap between the degrees of freedom associated with these two statistics is large,\n\n\f4\n\nV. E. JOHNSON\n\nand, as a result, the \u03c72 goodness-of-fit test based on the maximum likelihood\nestimate is usually not useful for assessing model fit in high-dimensional settings.\nThe goodness-of-fit statistic proposed here represents a modification of\nthe \u03c72 statistics considered above. The modification, denoted by RB (\u03b8\u0303) (or\nmore simply, by RB when no confusion arises), is obtained by fixing the\nvalues of pk and instead considering the bin counts mk as random quantities. Allocation of observations to bins is made according to the value of\neach observation's conditional distribution function, conditionally on a single parameter value \u03b8\u0303 sampled either from the posterior distribution or the\nasymptotic distribution of the maximum likelihood estimator. [The statistic obtained in this way has some resemblance to the \u03c72 statistics considered by, e.g., Moore and Spruill (1975), although emphasis there focuses on\nrandomized cells rather than on posterior sampling of parameter vectors.]\nThe distinguishing feature of RB (\u03b8\u0303) is that, for many statistical models, its\nasymptotic distribution is \u03c72 on K \u2212 1 degrees of freedom, independently of\nthe dimension of the parameter vector \u03b8.\nBecause it is the sampling distribution of RB that has a \u03c72 distribution,\none might argue that this procedure does not really represent a Bayesian\ngoodness-of-fit diagnostic. However, sampling parameter values from a distribution for the purpose of inference occurs more naturally within the\nBayesian paradigm, and for this reason it is likely that the proposed diagnostic will find more application there. In addition, the formal test statistics proposed below are based on the posterior distribution of RB . For this\nreason, values of \u03b8\u0303 used in the definition of RB are assumed to represent\nsamples from the posterior distribution on the parameter vector, rather than\nsamples generated from the asymptotic normal distribution of the maximum\nlikelihood estimator. However, either interpretation is valid.\nThe remainder of the paper is organized as follows. In the next section,\nthe Bayesian \u03c72 statistic RB is defined and its asymptotic properties are\ndescribed. Corollaries extending these properties from i.i.d. observations to\nconditionally independent observations and to fixed-bin applications are presented, and strategies for combining information contained in dependent\nsamples of RB values generated from the same posterior distribution are\ndescribed. Following this, several examples that illustrate the application\nof this statistic and summaries from its posterior are presented. Discussion\nand concluding remarks appear in Section 4. Proofs of the theorem and\ncorollaries of Section 2 appear in the Appendix.\n2. A Bayesian \u03c72 statistic. To begin, let y1 , . . . , yn (= y) denote scalarvalued, continuous, identically distributed, conditionally independent observations drawn from probability density function f (y|\u03b8) defined with respect\nto Lebesgue measure and indexed by an s-dimensional parameter vector\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n5\n\n\u03b8 \u2208 \u0398 \u2282 Rs . Denote by F (*|\u03b8) and F \u22121 (*|\u03b8) the (nondegenerate) cumulative\ndistribution and inverse distribution functions corresponding to f (*|\u03b8). To\nconstruct a sampled value \u03b8\u0303 from the posterior, augment the observed sample y with an i.i.d. sample v1 , . . . , vs from a U (0, 1) distribution. Let p(\u03b8|y)\ndenote the posterior density of \u03b8 based on y, and let p(\u03b8 j |\u03b8 1 , . . . , \u03b8 j\u22121 , y) denote the marginal conditional posterior density of \u03b8 j given (\u03b8 1 , . . . , \u03b8j\u22121 , y).\nDefine \u03b8\u0303 implicitly by\n(1)\n\nv1 =\n\nZ\n\n\u03b8\u03031\n\n\u2212\u221e\n\np(\u03b81 |y) d\u03b8t1 , . . . , vs =\n\nZ\n\n\u03b8\u0303s\n\n\u2212\u221e\n\np(\u03b8s |\u03b8\u03031 , . . . , \u03b8\u0303s\u22121 , y) d\u03b8s .\n\nThus, \u03b8\u0303 denotes a value of \u03b8 sampled from the posterior distribution based\non y. Let \u03b8 0 denote the true but unknown value of \u03b8. The maximum likelihood estimate of \u03b8 is denoted by \u03b8\u0302.\nTo construct the Bayesian goodness-of-fit statistic proposed here, choose\nquantiles 0 \u2261 a0 < a1 < * * * < aK\u22121 < aK \u2261 1, with pk = ak \u2212ak\u22121 , k = 1, . . . , K.\nDefine zj (\u03b8\u0303) to be a vector of length K whose kth element is 0 unless\n(2)\n\nF (yj |\u03b8\u0303) \u2208 (ak\u22121 , ak ],\n\nin which case it is 1. Finally, define\nm(\u03b8\u0303) =\n\nn\nX\n\nzj (\u03b8\u0303).\n\nj=1\n\nIt follows that the kth component of m(\u03b8\u0303), mk (\u03b8\u0303), represents the number\nof observations that fell into the kth bin, where bins are determined by the\nquantiles of the inverse distribution function evaluated at \u03b8\u0303. Finally, define\n(3)\n\nRB (\u03b8\u0303) =\n\n\u0015\nK \u0014\nX\n(mk (\u03b8\u0303) \u2212 npk ) 2\n\nk=1\n\n\u221a\n\nnpk\n\n.\n\nThe asymptotic distribution of RB is provided in the following theorem.\nTheorem 1. Assuming that the regularity conditions specified in the\nAppendix apply, RB converges to a \u03c72 distribution with K \u2212 1 degrees of\nfreedom as n \u2192 \u221e.\nThe simplicity of Theorem 1 is somewhat remarkable given the complexity\nof the corresponding distribution of R\u0302. As mentioned above, the asymptotic\ndistribution of R\u0302 does not, in general, follow a \u03c72 distribution. Instead, it has\nthe distribution of the sum of a \u03c72 random variable with K \u2212 s \u2212 1 degrees\nof freedom and the weighted sum of s additional squared normal deviates\nwith weights ranging from 0 to 1. In contrast, the asymptotic distribution\n\n\f6\n\nV. E. JOHNSON\n\nof RB follows a \u03c72K\u22121 distribution, independently of the dimension of the\nparameter vector \u03b8.\nHeuristically, the idea underlying Theorem 1 is that the degrees of freedom\nlost by substituting the grouped MLE for \u03b8 in Pearson's \u03c72 statistic are\nexactly recovered by replacing the MLE with a sampled value from the\nposterior in RB . That is, the s degrees of freedom lost by maximizing over the\ngrouped likelihood function to obtain Rg are exactly recovered by sampling\nfrom the s-dimensional posterior on \u03b8.\nAs a corollary, Theorem 1 can be extended to the more general case in\nwhich the functional form of the density f (y|\u03b8) varies from observation to\nobservation. Specifically, if the density of the jth observation is denoted by\nfj (y|\u03b8), with distribution and inverse distribution functions Fj and Fj\u22121 ,\nrespectively, then the following corollary also applies.\nCorollary 1. Assume the conditions referenced in Theorem 1 are extended so as to provide also for the asymptotic normality of both the posterior\ndistribution on \u03b8 and the maximum likelihood estimator when the likelihood\nfunction is proportional to\nn\nY\n\nj=1\n\nfj (yj |\u03b8).\n\nAssume also that the functions fj (*|\u03b8) satisfy the same conditions implied\nin Theorem 1 for f (*|\u03b8). Define the kth component of zj (\u03b8) to be 1 or 0\ndepending on whether or not\n(4)\n\nFj (yj |\u03b8\u0303) \u2208 (ak\u22121 , ak ],\n\nwith a fixed. Then the asymptotic distribution of RB based on this revised\ndefinition of zj (\u03b8) is \u03c72 with K \u2212 1 degrees of freedom.\nOutlines of the proof of Theorem 1 and the corollary appear in the Appendix.\nFrom a practical perspective, the corollary is important because it extends\nthe definition of RB to essentially all models in which observations are continuous and conditionally independent given the value of a finite-dimensional\nparameter vector.\nThe results cited above for continuous-valued random variables can be\nextended to discrete random variables in one of two ways. The most direct\nextension is to simply proceed as in the continuous case, using a randomization procedure to allocate counts to bins when the mass assigned to an\nobservation spans the boundaries defining the bins. The second is to define\nfixed bins in the standard way based on the possible outcomes of the random\nvariable, and to then evaluate the bin probabilities at sampled values of \u03b8\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n7\n\nfrom the posterior distribution. That is, if f (y|\u03b8) denotes the probability\nmass function of a discrete random variable y and\n(5)\n\npk (\u03b8\u0303) =\n\nn\nX\n1X\nfj (y|\u03b8\u0303),\nn j=1 y\u2208bin k\n\nthen the \u03c72 statistic RB may be redefined as\n(6)\n\n\u0015\nK \u0014\nX\n(mk \u2212 npk (\u03b8\u0303)) 2\np\nR (\u03b8\u0303) =\n.\nB\n\nk=1\n\nnpk (\u03b8\u0303)\n\nIn this case, the asymptotic distribution of RB (\u03b8\u0303) is similar to that described\nabove in the continuous case and is detailed in the following corollary.\nCorollary 2. If the regularity conditions specified in Theorem 1 apply\nto the discrete probability mass function f (y|\u03b8), then, using predefined bins\nand the definition of the bin probabilities given in (5), the distribution of\nRB (\u03b8\u0303) as defined in (6) converges to a \u03c72 distribution with K \u2212 1 degrees\nof freedom as n \u2192 \u221e.\nThe asymptotic \u03c72 distribution of RB (\u03b8\u0303) described in the theorem and\ncorollaries above is achieved when a large sample of independent observations is drawn from a sampling density, and a value of \u03b8\u0303 is drawn from the\nposterior induced by this observation. However, when two values of \u03b8\u0303 are\ndrawn from the same posterior distribution (i.e., based on the same observation), the values of RB that result are correlated. This correlation complicates the interpretation of test statistics defined with respect to posterior\ndistribution on RB values.\nCombining information across a posterior sample of RB values might be\naccomplished in a variety of ways, including modifications of the methodologies proposed in Verdinelli and Wasserman (1998) or Robert and Rousseau\n(2002). Another possibility is to simply report the proportion of RB values\ndrawn from the posterior distribution that exceeds a specified critical value\nfrom their nominal \u03c72K\u22121 distribution. For a given data vector and probability model, such a procedure might lead to a statement that, say, 90%\nof RB values generated from the posterior distribution exceeded the 95th\npercentile of the reference \u03c72 distribution.\nThough decidedly non-Bayesian, such a report is convenient from several\nperspectives. By reporting the proportion of RB values that exceeds the\ncritical value of the test, the unpalatable aspect of basing a goodness-of-fit\ntest on a randomly selected value of RB is avoided. It is also straightforward\nto compare the proportion of RB values that exceeds the critical value of the\ntest to the size of the test; if the RB values did represent independent draws\n\n\f8\n\nV. E. JOHNSON\n\nfrom their nominal \u03c72 distribution, the proportion of values falling in the\ncritical region of the test would exactly equal the size of the test. Any excess\nin this proportion must therefore be attributed either to dependence between\nthe sampled values of RB from the given posterior or lack of fit. Finally, and\nperhaps most importantly, this strategy requires almost no computational\neffort. In most practical Bayesian models, values of RB can be computed\nalmost as an afterthought within the MCMC schemes used to sample from\nthe posterior distribution of the parameter vector.\nIn the event that formal significance tests must be performed to assess\nmodel adequacy, they can be based on a comparison of the observed value of\na summary statistic based on the posterior distribution of RB values to an\napproximation of the sampling distribution of the summary statistic induced\nby repeated sampling of the data vector. The summary statistic considered\nhere is defined as the posterior probability that a value of RB drawn from\nthe posterior distribution (based on a single value of y) exceeds the value\nof a \u03c72K\u22121 random variable. This probability, denoted by A, is related to a\ncommonly used quantity in signal detection theory and represents the area\nunder the ROC curve [e.g., Hanley and McNeil (1982)] for comparing the\njoint posterior distribution of RB values to a \u03c72K\u22121 random variable. The\nexpected value of A, if taken with respect to the joint sampling distribution\nof y and the posterior distribution of \u03b8 given y, would be 0.5. Large deviations in the expected value of A from 0.5, when the expectation is taken\nwith respect to the posterior distribution of \u03b8 for a fixed value of y, indicate\nmodel lack of fit.\nUnfortunately, approximating the sampling distribution of A is a numerically burdensome endeavor, and calculating it obviates many of the advantages that are gained by using a test statistic with a known reference\ndistribution. To a large extent, the computations required to approximate\nA's sampling distribution are as complicated as, or even more complicated\nthan, similar techniques used to approximate the sampling distribution of\ndiscrepancy functions used in posterior-predictive model checks [e.g., Sinharay and Stern (2003)]. However, knowing the nominal value of A makes\nthis computation unnecessary when the observed value of A falls within several hundredths of 0.5 or is smaller than 0.5. Procedures for approximating\nthe sampling distribution of A for the purpose of determining the significance of departures of the observed value of A from 0.5 are described in\nthe examples using methodology delineated by Dey, Gelfand, Swartz and\nVlachos (1998).\nAs an aside, it is interesting to compare the test statistic RB and its\nreference distribution to the \u03c72 discrepancy function and its reference distribution as proposed in Gelman, Meng and Stern (1996). The reference\ndistribution of RB (\u03b8\u0303) is obtained by sampling y from its \"true\" distribution\nF (*|\u03b8 0 ), and then sampling a single value of \u03b8\u0303 from the posterior distribution\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n9\n\nof \u03b8 given y. The resulting distribution is asymptotically \u03c72K\u22121 ; this result is\nunrelated to posterior-predictive distributions or samples drawn from them.\nIn contrast, the reference distribution of the \u03c72 discrepancy function proposed by Gelman, Meng and Stern (1996) is obtained as the distribution of\nthe statistic\n(7)\n\nn\nX\n(y pp \u2212 E(y pp |\u03b8))2\ni\n\ni=1\n\ni\n\nVar(yipp |\u03b8)\n\ninduced by repeatedly drawing values ypp = (y1pp , . . . , ynpp ) from the posteriorpredictive density based on the observed data vector y. As Gelman, Meng\nand Stern point out, this statistic does not have a \u03c72 distribution.\nThe power characteristics of the Bayesian \u03c72 statistics defined above, like\ntheir classical counterparts, depend on the selection of the bin probabilities\npk . Clearly, consistency of derived tests against general alternatives requires\nthat K \u2192 \u221e as n \u2192 \u221e. On the other hand, as many authors have noted\n[see, e.g., Koehler and Gan (1990) for a review of this topic], using too many\ncells can result in a significant loss of power.\nA general criterion for choosing cell probabilities was proposed by Mann\nand Wald (1942), who suggested the use of 3.8(n \u2212 1)0.4 equiprobable cells.\nSubsequent authors [e.g., Williams (1950), Watson (1957), Hamdan (1963),\nDahiya and Gurland (1973), Gvanceladze and Chibisov (1979), Best and\nRayner (1981), Quine and Robinson (1985) and Koehler and Gan (1990)]\nfound that the Mann\u2013Wald criterion often results in too many bins and\nloss of power. Based on numerical simulations of seven classes of alternative\nprobability models, Koehler and Gan (1990) noted that near-optimal power\nagainst a Gaussian null model was obtained when the Mann\u2013Wald criterion was divided by 4. Such a rule also finds approximate agreement with\nsimulation results reported by Kallenberg, Oosterhoff and Schriever (1985)\n(although they also recommend the use of nonequiprobable cells against\ncertain types of alternative hypotheses). This rule of thumb, which may be\napproximately reformulated as taking n0.4 equiprobable cells, was found to\nyield nearly optimal results in the examples described below.\n3. Examples.\n3.1. Goodness-of-fit tests under a normal model with unknown mean and\nvariance. In this example, the distribution of RB under a normal model\nis investigated and compared with the distributions of R\u0302 and Rg . Posterior\nsamples of RB generated from a single data vector are used in ROC-type\nanalyses to generate a summary model diagnostic. The power of this test\nstatistic is investigated and compared to the power of the test statistic Rg\nwhen data are generated under nonnormal alternatives.\n\n\f10\n\nV. E. JOHNSON\n\nLet y = (y1 , . . . , y50 ) denote a random sample from a standard normal distribution. For purposes of illustration, assume that the mean \u03bc and variance\n\u03c3 2 of the data are unknown and that the joint prior assumed for (\u03bc, \u03c3) is\nproportional to 1/\u03c3. Let (\u03bc\u0303, \u03c3\u0303) denote a sampled value from the posterior\ndistribution based on y.\nFor a given data vector y and posterior sample (\u03bc\u0303, \u03c3\u0303), bin counts mk (\u03bc\u0303, \u03c3\u0303)\nare determined by counting the number of observations yi that fall into the\ninterval (\u03c3\u0303\u03a6\u22121 (ak\u22121 ) + \u03bc\u0303, \u03c3\u0303\u03a6\u22121 (ak ) + \u03bc\u0303), where \u03a6\u22121 (*) denotes the standard\nnormal quantile function. Based on these counts, RB (\u03bc\u0303, \u03c3\u0303) is calculated according to (3).\nFigure 1 depicts a quantile-quantile plot of RB values calculated for 10,000\nindependent samples of y. Each value of RB depicted in this plot corresponds\nto a single draw of (\u03bc, \u03c3) from the posterior distribution based on a single\nobservation vector y. In accordance with the rule-of-thumb discussed in Section 2, five equiprobable bins were used in the definition of RB . As expected,\nthe distribution of RB closely mimics that of a \u03c724 random variable.\nThe normal deviates used in the construction of Figure 1 were also used\nto compute the classical \u03c72 statistic based on the maximum likelihood estimates of \u03bc and \u03c3 (i.e., using the ungrouped data). The quantile-quantile\nplot of 10,000 R\u0302 values obtained from these data is displayed in Figure 2. In\nthis plot, the R\u0302 values have been plotted against the expected order statistics from a \u03c722 random variable. Five equal probability bins based on the\nstandard normal distribution were also used to define these R\u0302 values. As\nmight be expected, the plotted \u03c72 values display some deviation from their\napproximate \u03c722 distribution.\nGrouped maximum likelihood estimates were also used to calculate Rg\nvalues using these normal samples. The corresponding quantile-quantile plot\nfor the 10,000 Rg values is displayed in Figure 3; as expected, these values\ndemonstrate substantially better agreement with a \u03c722 random variable than\ndo the values depicted in Figure 2.\nReturning to the investigation of the properties of RB , Figure 1 demonstrates excellent agreement between this statistic and its asymptotic distribution. To illustrate its power in detecting departures from the normal\nmodel, suppose now that the experiment above is repeated with independent\nStudent t variates substituted for the normal deviates. That is, the actual\nobservation vectors used in the simulation represent Student t variates, but\nthe statistical model used to calculate values of RB is still based on the\nassumption that the data are normally distributed. The degrees of freedom\nof the t variates used in this experiment range from 1 to 10, and for each\nvalue within this range, 10,000 independent samples of size 50 were drawn.\nTo study the power of the statistic RB in detecting departures from normality in this experiment, formal significance tests were performed using the\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n11\n\nFig. 1. Quantile-quantile plot of RB values for i.i.d. normal data. Values of RB displayed\nin this plot were determined from independent samples of 50 standard normal deviates, and\nare plotted against the expected order statistics from a \u03c724 distribution. Posterior samples\nof the mean and variance were estimated using reference priors and observations were\nbinned into five bins of equal probability [i.e., a = (0, 0.2, 0.4, 0.6, 0.8, 1)].\n\nstatistic A described in Section 2. This statistic may be defined formally as\n(8)\n\nA = Pr\u03b8\u0303|y (RB (\u03b8\u0303) > X),\n\nX \u223c \u03c72K\u22121 ,\n\nand, in repeated sampling of both y and \u03b8 given y, has a nominal value of\n0.5. Numerically, the value of A, for a fixed data vector y, can be approximated in a straightforward way using Monte Carlo integration.\nFormal model assessment using the statistic A can be based on approximating the sampling distribution of A using \"posterior-predictive-posterior\"\n\n\f12\n\nV. E. JOHNSON\n\nFig. 2. Quantile-quantile plot of R\u0302 values for i.i.d. normal data. Values of R\u0302 displayed in\nthis plot were each determined from a separate sample of 50 standard normal deviates, and\nare plotted against the expected order statistics from a \u03c722 distribution. For comparison,\nthe top curve depicts values of expected order statistics from a \u03c724 distribution.\n\nmodel checks [e.g., Dey, Gelfand, Swartz and Vlachos (1998)]. That is, sampled values \u03b8\u0303 from the posterior can be used to generate posterior-predictive\nobservations ypp according to f (*|\u03b8\u0303). In large samples, values of \u03b8\u0303 will be\nclose to \u03b8 0 , and so the distribution of ypp will be close to the distribution\nof y. Posterior-predictive-posterior values of App can be generated for each\nvalue of ypp by averaging RB , computed from ypp , over the posterior distribution of \u03b8 induced by ypp . Values of App obtained from this procedure\napproximate the sampling variability of the summary test statistic A that\ncan be attributed to computing the probability in (8) using the posterior\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n13\n\nFig. 3. Quantile-quantile plot of Rg values for i.i.d. normal data. Values of Rg displayed\nin this plot were each determined from a separate sample of 50 standard normal deviates,\nand are plotted against the expected order statistics from their asymptotic \u03c722 distribution.\n\ndistribution of \u03b8 for a given value of y, without averaging over the distribution of y. The value of A obtained for the original data vector can then be\ncompared to the empirical distribution of the values of App obtained from\nthe posterior distribution on the posterior-predictive data.\nIn principle, exactly this procedure can be implemented to calculate the\nprobability that the test statistic A, based on a random sample of t variates,\nfalls into the critical region of a test based on the empirical distribution of\nsampled values App . In this case, however, it is not necessary to generate\nvalues of App for each sample of t variates. Under the normal model, values of RB are invariant to shifts in location and scale of the data, so the\n\n\f14\n\nV. E. JOHNSON\n\nsampling distribution of A, for any future draw of 50 i.i.d. normal deviates,\ncan be approximated by the empirical distribution of A values obtained under the normal sampling scheme used at the beginning of this example. It\nfollows that critical regions for significance tests based on A are exact under this model, save for the Monte Carlo error encountered in the empirical\napproximation of their distribution.\nFigure 4 displays the proportion of times in 10,000 draws of t samples\nthat the value of the test statistic A was larger than the 0.95 quantile of the\nsampled values of App . For comparison, the observed power of the test based\non the grouped-maximum-likelihood \u03c72 statistic Rg at the 5% level is also\nshown, as is the observed power obtained using a randomized test based on\nonly a single value of RB . To facilitate comparison with the distribution of\nRB , five equiprobable bins from a standard normal distribution were used\nin the definition of Rg .\n\nFig. 4. Power of test statistics A, RB and Rg in detecting departures from normality\nwhen data are distributed according to t distributions. The uppermost curve depicts the\npower of the test statistic A against t alternatives with degrees of freedom displayed on the\nhorizontal axis. The curve in the middle depicts the corresponding power of a single value\nof RB when compared to a \u03c724 distribution. The curve at the bottom of the plot represents\nthe power of Rg against the t alternatives. All values of the power refer to the power of\nthe test statistics in rejecting the null hypothesis of normality in significance tests of size\n0.05 and samples of size 50.\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n15\n\nFrom Figure 4, it is clear that the test statistic A offers substantially better\npower than Rg against this class of alternative models. Part of this advantage\nstems from the symmetry and unimodality of the alternative hypotheses,\nwhich Rg is ill-equipped to accommodate, and part from the fact that the\nbins used in the definition of Rg were fixed according to the null hypothesis.\nSubstantially better power could be obtained by using the test statistic R\u0302\nwith bins based on the particular y vector observed, but such tests do not\nachieve their nominal levels of significance. Perhaps surprisingly, the power\nof a randomized test based on a single value of RB is comparable to the\npower of A based on the complete posterior distribution of RB values.\n3.2. Lip cancer data. Spiegelhalter, Best, Carlin and van der Linde (2002)\ndescribe a re-analysis of lip cancer incidence data originally considered by\nClayton and Kaldor (1987). Their purpose in examining these data was to\nillustrate the use of the deviance information criterion (DIC) to select from\namong five potential models for the number of lip cancer cases, yi , observed\nin 56 Scottish districts as a function of available age and sex adjusted expected rates Ei . These data and models are reconsidered here for the related\npurpose of assessing which of the models provides an adequate probabilistic\ndescription of the data.\nFollowing the Spiegelhalter et al. analysis of these data, begin by assuming that yi is Poisson with mean \u03bci = exp(\u03b8i )Ei . Five models for \u03b8i are\nconsidered:\n1. \u03b8i = \u03b10 , \u03b10 a constant.\n2. \u03b8i = \u03b10 + \u03b3i , \u03b3i exchangeable random effects.\n3. \u03b8i = \u03b10 + \u03b4i , \u03b4i spatial random effects with a conditional autoregressive\nprior [e.g., Besag (1974)].\n4. \u03b8i = \u03b10 + \u03b4i + \u03b3i , \u03b4i and \u03b3i as above.\n5. \u03b8i = \u03b1i , \u03b1i uniform on (\u2212\u221e, \u221e).\nFive thousand, thinned posterior samples of \u03bc = {\u03bci } were generated for\neach of these models using WinBUGS code [Spiegelhalter, Thomas and Best\n(2000)] kindly provided by Dr. Best. For each sampled value of \u03bci , the Poisson counts yi were assigned to one of five equiprobable bins defined according\nto the Poisson distribution function evaluated at yi for the given value of \u03bci .\nIn those cases for which the probability mass function assigned to yi spanned\nmore than one bin, allocation to a single bin was performed randomly according to the proportion of mass assigned to the bins. Averaging over all\nposterior samples of \u03bc for a given model yielded the values of A depicted in\nTable 1. Because 56 data points were available, five bins were again used in\nthe definition of the individual values of RB . The proportion of RB values\nexceeding the 95th quantile from a \u03c724 distribution was computed using the\n\n\f16\n\nV. E. JOHNSON\nTable 1\nValues of the goodness-of-fit statistic A and the proportion of\ncritical RB values for models of lip cancer incidence data\nModel\n1\n2\n3\n4\n5\n\nA\n\nProportion of RB > 9.49\n\nDIC\n\n0.999\n0.517\n0.538\n0.537\n0.677\n\n1.000\n0.055\n0.076\n0.075\n0.198\n\n382.7\n104.0\n89.9\n89.7\n111.7\n\nThe second column provides the value of the summary statistic A\nachieved for each model. The third column lists the proportion of posterior samples of RB that exceeded the 95th quantile of a \u03c724 distribution\nfor each model. DIC values obtained under the \"mean\" parameterization are listed for comparison.\n\nposterior sample \u03bc. No posterior-predictive or posterior-predictive-posterior\ncomputations were performed to obtain these values.\nIn Table 1, both the large value of A and the large proportion of RB values\nexceeding the 95th quantile of the \u03c724 distribution provide a clear indication\nof lack of fit for the first model. Lack of fit in this instance can be attributed\nto the failure of the model to adjust for district effects; the posterior mean of\nthe number of counts assigned to the five bins was (16.0, 4.9, 5.2, 7.1, 22.8).\nThe values of A and proportions of extreme values of RB reported in rows\n2\u20134 do not suggest lack of fit of the aspect of these models being tested by\nthe \u03c72 test.\nThe most interesting row in Table 1 is the last, which corresponds to\nfitting a separate Poisson model for each observation. The value of A for this\nmodel is 0.68, and nearly 20% of RB values generated from its posterior-\nnearly four times the number expected-exceeded the 5% critical value from\nthe \u03c724 distribution.\nAt first glance, one might suspect that these suspicious values arise from\noverfitting. However, the last model generates the most dispersed posterior\ndistribution of any of the models considered, since only one observation\nfigures into the marginal posterior of each \u03bci . Instead, the difficulty with\nthis model arises from the prior assumptions made on \u03bc. The assumption of\na uniform prior on \u03b8i implies a prior for the mean of each Poisson observation\nproportional to 1/\u03bci ; this prior shrinks the estimate of every \u03bci toward 0.\nThis results in an overabundance of counts in the higher bins and larger than\nexpected values of RB . The posterior mean of the bin counts for this model\nwas (8.4, 9.8, 10.9, 12.1, 14.8). Refitting the fifth model with noninformative\n\u221a\npriors proportional to 1/ \u03bci yielded a value of A = 0.501 and only 4.7% of\nRB values exceeding 9.49.\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n17\n\nIt is also interesting to compare the values in the second and third columns\nof this table with those provided for the DIC. All statistics suggest inadequacy of the first model, though for different reasons. For the first model,\nthe high values of A and RB suggest that the data do not follow Poisson\ndistributions with a common scaling of adjusted expected rates. The value\nof the DIC statistic suggests either that the model does not fit the data or\nthat it is not as precise in predicting the data as the other models considered. An advantage of the \u03c72 statistics in this case is that their values are\ninterpretable without fitting alternative models.\nThe comparatively large value of the DIC statistic for the second model\ncan be attributed to greater dispersion in its posterior as compared to posterior dispersion of the third and fourth models, even though the exchangeable\nmodel appears to adequately represent variation in the observed data. The\ncomparatively large value of DIC reported for the fifth model reflects some\ncombination of lack of fit and a posterior that is more dispersed than others\nconsidered.\n4. Extensions. In addition to providing a convenient mechanism for assessing model adequacy, values of RB generated from a posterior distribution\nmay prove useful both as a convergence diagnostic for MCMC algorithms\nand for detecting errors written in computer code to implement these algorithms.\nMonitoring values of RB generated within an MCMC algorithm provides\na rudimentary convergence diagnostic for slow-mixing chains. In fact, exceedances of RB over a prespecified quantile from its null distribution can be\nincorporated formally into the convergence diagnostics proposed in Raftery\nand Lewis (1992). To the extent that such exceedances are adequately described by a two-state Markov chain, the use of RB in this context eliminates\nthe requirement to assess convergence on a parameter-by-parameter basis,\nas is normally done in Raftery and Lewis' diagnostic scheme. It also provides\na natural mechanism for determining whether burn-in has occurred.\nA less obvious but perhaps equally important use of the RB statistic involves code verification. Many practitioners currently fit models using customized code written for their specific application, a practice that frequently\nresults in coding errors that are difficult to detect. This problem can be\nlargely overcome by simply monitoring the distribution of RB , which, in my\nexperience, tends to deviate substantially from its null distribution when a\nmodel has been misspecified or miscoded.\n5. Discussion. Goodness-of-fit tests based on the statistic RB provide a\nsimple way of assessing the adequacy of model fit in many Bayesian models.\nEssentially, the only requirement for their use is that observations be conditionally independent. From a computational perspective, such statistics can\n\n\f18\n\nV. E. JOHNSON\n\nbe calculated in a straightforward way using output from existing MCMC\nalgorithms.\nApproximating the sampling distribution of A, though conceptually straightforward, does introduce an additional computational burden, but is necessary only when the achieved value of A is \"significantly\" larger than 0.5.\nSignificance of A in this context has a natural interpretation in terms of the\nposterior probability that a sampled value of RB exceeds a random variable\ndrawn from its nominal \u03c72 distribution. In this regard, values of A that are\nclose to 0.5 may indicate adequate model fit for the purposes of a given\nanalysis even when the sampling distribution of App would permit rejection\nof the model in a significance test.\nAside from applications in Bayesian model assessment, the \u03c72 statistic\nproposed here can be extended, albeit somewhat awkwardly, to models estimated using maximum likelihood. In that setting, parameter values can\nbe sampled from their asymptotic normal distribution and used as if they\nwere sampled from a posterior distribution. Although not entirely palatable\nfrom a classical perspective, such a procedure does provide a mechanism\nfor conducting a (suboptimal) goodness-of-fit test for complicated models in\nwhich alternative tests may be difficult to perform.\nAPPENDIX\nOutlines of proofs of theorems and corollaries. The proofs of Theorem\n1 and Corollary 1 are based largely on the proof given in Chernoff and\nLehmann (1954) in establishing the asymptotic distribution of R\u0302.\nAssume that conditions specified in Cram\u00e9r [(1946), pages 426 and 427]\nand Chen (1985) apply. Cram\u00e9r specifies conditions that are sufficient for establishing the distribution of the \u03c72 goodness-of-fit statistic when evaluated\nat the parameter vector maximizing the likelihood estimate based on the\ngrouped data, whereas Chen's conditions are sufficient for establishing the\nasymptotic normality of the posterior distribution. Essentially, these conditions require that the likelihood be a smooth function of the parameter\nvector \u03b8 in an open interval containing \u03b8 0 (the true value of \u03b8), that the\nposterior distribution concentrate around a point in this interval, that the\ninformation contained in the observations increase with sample size and that\nthe prior assign nonnegligible mass to the interval containing \u03b8. In addition,\nassume that all third-order partial derivatives of f (y|\u03b8) [or, in the case of\nthe corollary, fj (y|\u03b8)] with respect to the components of \u03b8 exist and are\nbounded in an open interval containing \u03b8 0 . Finally, note that all expectations and statements regarding probabilistic orders of magnitude described\nbelow are computed under the sampling distribution of y given \u03b8 0 .\nThe following lemmas are needed.\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n19\n\nLemma A.1. Under the conditions stated above, if \u03b8\u0302 refers to the maximum likelihood estimate of \u03b8, \u03b8\u0303 refers to a value of \u03b8 sampled from the\nposterior distribution and mk (*) refers to the number of counts assigned to\nthe kth bin at a specified value of \u03b8, then\n1\n1\n\u221a (mk (\u03b8\u0303) \u2212 mk (\u03b8\u0302)) = \u221a (m\u2217k (\u03b8\u0303) \u2212 m\u2217k (\u03b8\u0302)) + op (1)\n(9)\nn\nn\ns\n1 X\n\u2202m\u2217k (\u03b8\u0302)\n=\u221a\n(\u03b8\u0303i \u2212 \u03b8\u0302i ) + op (1),\nn i=1 \u2202\u03b8i\n\n(10)\nwhere\n\nm\u2217k (\u03b8) = nE[Ind(y \u2208 [F \u22121 (ak\u22121 |\u03b8), F \u22121 (ak |\u03b8)])].\nProof. Expanding m\u2217k (\u03b8\u0303) in a Taylor series expansion about m\u2217k (\u03b8\u0302)\nyields\n(11)\nDefine\nThen\n\nm\u2217k (\u03b8\u0303) \u2212 m\u2217k (\u03b8\u0302) =\n\ns\nX\n\u2202m\u2217 (\u03b8\u0302)\nk\n\ni=1\n\n\u2202\u03b8i\n\n(\u03b8\u0303i \u2212 \u03b8\u0302i ) + Op\n\n\u0012 \u0013\n\n1\n.\nn\n\n\u2206zk,j = zk,j (\u03b8\u0303) \u2212 zk,j (\u03b8\u0302).\n|\u2206zk,j | \u2264 Ind(yj \u2208 [min(F \u22121 (ak\u22121 |\u03b8\u0303), F \u22121 (ak\u22121 |\u03b8\u0302)),\n\nmax(F \u22121 (ak\u22121 |\u03b8\u0303), F \u22121 (ak\u22121 |\u03b8\u0302))])\n\n+ Ind(yj \u2208 [min(F \u22121 (ak |\u03b8\u0303), F \u22121 (ak |\u03b8\u0302)),\n\nmax(F \u22121 (ak |\u03b8\u0303), F \u22121 (ak |\u03b8\u0302))]).\n\n\u221a\n\u221a\nBecause (\u03b8\u0302 \u2212 \u03b8\u0303) is Op (1/ n ), n\u2206zk,j = Op (1). It follows that\n\u221a X\nmk (\u03b8\u0303) \u2212 mk (\u03b8\u0302) m\u2217k (\u03b8\u0303) \u2212 m\u2217k (\u03b8\u0302)\n\u221a\n\u221a\nn\n\u2206zk,j /n =\n=\n+ op (1).\nn\nn\nj\nSubstituting this expression into (11) yields (10). \u0003\nCorollary A.2. The previous lemma also applies if \u03b8 0 is substituted\nfor \u03b8\u0303, that is,\n1\n1\n\u221a (mk (\u03b8 0 ) \u2212 mk (\u03b8\u0302)) = \u221a (m\u2217k (\u03b8 0 ) \u2212 m\u2217k (\u03b8\u0302)) + op (1)\nn\nn\ns\n1 X\n\u2202m\u2217k (\u03b8\u0302)\n=\u221a\n(\u03b80,i \u2212 \u03b8\u0302i ) + op (1).\nn i=1 \u2202\u03b8i\n\n\f20\n\nV. E. JOHNSON\n\nLemma A.3.\n(12) p\u0302k = F [F\n\n\u22121\n\nDefine\n(ak |\u03b8 0 )|\u03b8\u0302] \u2212 F [F\n\n\u22121\n\n(ak\u22121 |\u03b8 0 )|\u03b8\u0302] =\n\nZ\n\nF \u22121 (ak |\u03b80 )\n\nF \u22121 (ak\u22121 |\u03b80 )\n\nf (y|\u03b8\u0302) dy.\n\nThen, under the conditions stated above,\n\u0012 \u0013\n\n1\n1\np\u0302k \u2212 pk = (m\u2217k (\u03b8 0 ) \u2212 m\u2217k (\u03b8\u0302)) + Op\n.\nn\nn\n\n(13)\n\nProof. For notational simplicity, define\nG(\u03b3, \u03b4; c) = F [F \u22121 (c|\u03b3)|\u03b4]\nand\nHi (\u03b3; c) =\n\n\u2202G(\u03b3, \u03b4; c)\n\u2202\u03b4i\n\n.\n\u03b4=\u03b3\n\nThen, noting that m\u2217k (\u03b8 0 ) = npk = n(G(\u03b8 0 , \u03b8 0 , ak ) \u2212 G(\u03b8 0 , \u03b8 0 , ak\u22121 )),\n(p\u0302k \u2212 pk ) \u2212\n\n1 \u2217\n(m (\u03b8 0 ) \u2212 m\u2217k (\u03b8\u0302))\nn k\n\n= [G(\u03b8 0 , \u03b8\u0302; ak ) \u2212 G(\u03b8 0 , \u03b8\u0302; ak\u22121 )] + [G(\u03b8\u0302, \u03b80 ; ak ) \u2212 G(\u03b8\u0302, \u03b8 0 ; ak\u22121 )] \u2212 2pk\n=\n\n\"\nX\ni\n\n+\n\nHi (\u03b8 0 ; ak )(\u03b8\u0302i \u2212 \u03b8 0,i ) \u2212\n\n\"\nX\ni\n\n=\n\nX\ni\n\n\u2212\n=\n\n= Op\n\ni\n\nHi (\u03b8\u0302; ak )(\u03b8 0,i \u2212 \u03b8\u0302i ) \u2212\n\n#\n\nHi (\u03b8 0 ; ak\u22121 )(\u03b8\u0302i \u2212 \u03b8 0,i )\n\nX\ni\n\n#\n\nHi (\u03b8\u0302; ak\u22121 )(\u03b8 0,i \u2212 \u03b8\u0302i ) + Op\n\n\u0012 \u0013\n\n1\nn\n\n[Hi (\u03b8 0 ; ak ) \u2212 Hi (\u03b8\u0302; ak )](\u03b8\u0302i \u2212 \u03b8 0,i )\n\nX\ni\n\n[Hi (\u03b8 0 ; ak\u22121 ) \u2212 Hi (\u03b8\u0302; ak\u22121 )](\u03b8\u0302i \u2212 \u03b8 0,i ) + Op\n\nX X\u0014 \u2202Hi (\u03b8 0 ; ak )\nh\n\nX\n\ni\n\n\u2202\u03b8 0,h\n\n\u2212\n\n\u0012 \u0013\n\n1\nn\n\n\u0015\n\n\u0012 \u0013\n\n\u2202Hi (\u03b8 0 ; ak\u22121 )\n1\n(\u03b8\u0302h \u2212 \u03b8 0,h )(\u03b8\u0302i \u2212 \u03b8 0,i ) + Op\n\u2202\u03b8 0,h\nn\n\n\u0012 \u0013\n\n1\n.\nn\n\nCorollary A.4.\n\u0012\n\u0013\n\u221a\n1\n1\nn(p\u0302k \u2212 pk ) = \u221a (mk (\u03b8 0 ) \u2212 mk (\u03b8\u0302)) + Op \u221a .\nn\nn\n\n\u0003\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\nProof of Theorem 1.\nlows:\n(14)\n\n21\n\nDecompose the terms appearing in (3) as fol-\n\nmk (\u03b8\u0303) \u2212 npk mk (\u03b8\u0303) \u2212 mk (\u03b8\u0302) mk (\u03b8 0 ) \u2212 mk (\u03b8\u0302) mk (\u03b8 0 ) \u2212 npk\n=\n\u2212\n+\n.\n\u221a\n\u221a\n\u221a\n\u221a\nnpk\nnpk\nnpk\nnpk\n\nFrom Lemma A.1 and Corollary A.2, the first two terms on the right-hand\nside of (14) are asymptotically equivalent to\n(15)\n\nP\n\n\u2217\ni \u2202mk (\u03b8\u0302)/\u2202\u03b8i (\u03b8\u0303i\n\n\u221a\nnpk\n\n\u2212 \u03b8\u0302i )\n\nand\n\nP\n\n\u2217\ni \u2202mk (\u03b8\u0302)/\u2202\u03b8i (\u03b80,i\n\n\u221a\n\nnpk\n\n\u2212 \u03b8\u0302i )\n\n.\n\nAlso, (\u03b8\u0303 \u2212 \u03b8\u0302) is asymptotically normal with mean 0 and covariance matrix\nequal to the negative inverse of the information matrix [Chen (1985)]. So,\ntoo, is (\u03b8\u0302 \u2212 \u03b8 0 ), and the two quantities are asymptotically independent [e.g.,\nOlver (1974) and Cox and Hinkley (1974)].\nFollowing Chernoff and Lehmann (1954), define \u01eb to be a K \u00d7 1 vector\nwith components\n\u01ebk =\n\nmk (\u03b8 0 ) \u2212 npk\n,\n\u221a\nnpk\n\nand let \u03bd\u0302 be the vector with components\n\u221a\n\u221a\n\u03bd\u0302k = n(p\u0302k \u2212 pk )/ pk .\nIt follows from their results that\n(16)\n\n\u03bd\u0302 = D(J\u0303 + J\u2217 )\u22121 (D\u2032 \u01eb +\n\n\u221a\n\nnA\u2217 ) + op (1),\n\nwhere J\u2217 is the matrix whose (i, j)th component is\nE\n\n\u0014\n\n\u0015\n\n\u2202 log g(y|z, \u03b8) \u2202 log g(y|z, \u03b8)\n,\n\u2202\u03b8i\n\u2202\u03b8j\n\ng(y|z, \u03b8) is the conditional distribution of y given z and \u03b8, J\u0303 \u2261 D\u2032 D is the\nmatrix with elements\nK\nX\n1 \u2202pk \u2202pk\n\nk=1\n\npk \u2202\u03b8a \u2202\u03b8b\n\n,\n\nand A\u2217 is the vector whose ath component is\nn\n\u2202 log g(y|zj , \u03b8)\n1X\n.\nn j=1\n\u2202\u03b8a\n\nFrom the second corollary, the right-hand side of (16) also describes the\n\u221a\nlarge sample distribution of (mk (\u03b8 0 ) \u2212 mk (\u03b8\u0302))/ npk .\n\n\f22\n\nV. E. JOHNSON\n\n\u221a\nTaking \u03b7 = nA\u2217 and invoking the central limit theorem, Chernoff and\nLehmann note that the asymptotic distribution of (\u01eb, \u03b7) is\n\u0014\n\n\u0012\n\n\u0013\u0015\n\nI \u2212 qq\u2032 0\n,\n0\nJ\u2217\n\u221a\nwhere q is the vector with components pk . Letting \u03b5 denote a variable\nhaving the same distribution as \u01eb, and \u03c4 denote a variable having the same\ndistribution as \u03b7, with all four variables distributed independently, it follows\nthat RB has the asymptotic distribution of\nN 0,\n\n(17)\n\n(T\u03b5 + S\u03c4 \u2212 T\u01eb \u2212 S\u03b7 + \u01eb)\u2032 (T\u03b5 + S\u03c4 \u2212 T\u01eb \u2212 S\u03b7 + \u01eb),\nwhere S = D(J\u0303 + J\u2217 )\u22121 and T = SD\u2032 . Noting that D\u2032 q = 0, the asymptotic\ndistribution of (T\u03b5 + S\u03c4 \u2212 T\u01eb \u2212 S\u03b7 + \u01eb)\u2032 is N (0, I \u2212 qq\u2032 ). The result follows.\n\u0003\nProof of Corollary 1. Because the proof of this corollary is similar\nto the proof of Theorem 1, only an outline is presented here.\nTo begin, note that Lemma A.1 and Corollary A.2 extend to this setting\nif m\u2217k (\u03b8) is redefined as\nm\u2217k (\u03b8) =\n\nn\nX\n\nj=1\n\nE[Ind(yj \u2208 [Fj\u22121 (ak\u22121 |\u03b8), Fj\u22121 (ak |\u03b8)])].\n\nNext, Lemma A.3 applies if (12) is modified so that\n(18)\n\np\u0302j,k = Fj [Fj\u22121 (ak |\u03b8 0 )|\u03b8\u0302] \u2212 Fj [Fj\u22121 (ak\u22121 |\u03b8 0 )|\u03b8\u0302]\n=\n\nZ\n\nFj\u22121 (ak |\u03b80 )\n\nFj\u22121 (ak\u22121 |\u03b80 )\n\nfj (y|\u03b8\u0302) dy,\n\nwhere pj,k and related estimates refer to the probability that the jth observation falls into the kth bin. Then\n(19)\n\n\u2217\n\u2217\np\u0302j,k \u2212 pj,k = (zj,k\n(\u03b8 0 ) \u2212 zj,k\n(\u03b8\u0302)) + Op\n\n\u0012 \u0013\n\n1\n,\nn\n\nwhere\n\u2217\nzj,k\n(\u03b8) = E[Ind(yj \u2208 [Fj\u22121 (ak\u22121 |\u03b8), Fj\u22121 (ak |\u03b8)])].\n\nCorollary A.4 generalizes to\nn\nn\n1\n1 X\n1 X\n\u221a\n(p\u0302j,k \u2212 pj,k ) = \u221a\n(zk,j (\u03b8) \u2212 zk,j (\u03b8\u0302)) + Op \u221a .\nn j=1\nn j=1\nn\n\n\u0012\n\n\u0013\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n23\n\nExtending Chernoff and Lehmann's (1954) result to the case of nonidentically distributed random variables requires the following modifications of\nthe definitions of variables used in the i.i.d. case. Let\n\u0012\n\u0013\nzj,1 \u2212 pj,1\nzj,K \u2212 pj,K \u2032\n\u01ebj =\n,..., \u221a\n,\n\u01eb = (\u01eb\u20321 , . . . , \u01eb\u2032n )\u2032 ,\n\u221a\nnpj,1\nnpj,K\nJ\u0303 =\n\nn X\nK\nX\n1 \u2202p\u03b1,r \u2202p\u03b1,r\n\n\u03b1=1 r=1\n\n\uf8eb\n\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\nD=\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\np\u03b1,r \u2202\u03b8i\n\n\u2202\u03b8j\n\n,\n\n\u2202p1,1\n1 \u2202p1,1\n... \u221a\n\u2202\u03b81\np1,1 \u2202\u03b8s\n..\n..\n.\n.\n1 \u2202p1,K\n1 \u2202p1,K\n... \u221a\n\u221a\np1,K \u2202\u03b81\np1,K \u2202\u03b8s\n1 \u2202p2,1\n1 \u2202p2,1\n... \u221a\n\u221a\np2,1 \u2202\u03b81\np2,1 \u2202\u03b8s\n..\n..\n.\n.\n\u221a1\np1,1\n\n\u221a\n\n1 \u2202pn,K\n1 \u2202pn,K\n... \u221a\npn,K \u2202\u03b81\npn,K \u2202\u03b8s\n!\n\n\uf8f6\n\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7,\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\nP = Ik | . . . |Ik ,\n|\n\nJ\u2217 = E\nA\u2217i =\n\n{z\n\nn times\n\n\"\n\n}\n\nn\nX\n\u2202 log g\u03b1 (y|z, \u03b8)\n\n\u03b1=1\n\n\u2202\u03b8i\n\nn\n\u2202 log gj (y|z, \u03b8)\n1X\n,\nn j=1\n\u2202\u03b8i\n\n!\n\nn\nX\n\u2202 log g\u03b2 (y|z, \u03b8)\n\n*\n\n\u03b2=1\n\n\u2202\u03b8j\n\n!#\n\n,\n\np\u0302j,r \u2212 pj,r\n\u03bd\u0302j,r = \u221a\n.\nnpj,r\n\nThen\n\u03bd\u0302 = D(J\u0303 + J\u2217 )\u22121 (D\u2032 \u01eb +\n\n\u221a\n\nnA\u2217 ) + op (1).\n\nThe covariance matrix of \u01eb may be written\n\uf8eb\n\n\uf8f6\n\nq1 q1 \u2032 0 . . . 0\n1\n1\uf8ec .\n.. ..\n.. \uf8f7 ,\nIn\u00d7K \u2212 \uf8ed ..\n. .\n. \uf8f8\nn\nn\n0 . . . 0 qn qn \u2032\n\u221a\nwhere qi is the vector whose jth component is pi,j\n\u221a. Denote the rightmost\nmatrix in this equation by Q. Similarly, define \u03b7 = nA\u2217 . Then the asymptotic distribution of \u03b7 has mean 0 and covariance matrix equal to J\u2217 /n, and\nis independent of \u01eb.\n\n\f24\n\nV. E. JOHNSON\n\n\u221a\nLetting r\u0302 denote the vector with components (zk,j (\u03b8) \u2212 zk,j (\u03b8\u0302))/( npj,k ),\nit follows from the generalization of Corollary A.4 that the distribution of\nPr\u0302 is asymptotically the same as that of P\u03bd\u0302. Letting r\u0303 denote the vector\n\u221a\nwith components (zk,j (\u03b8\u0303) \u2212 zk,j (\u03b8\u0302))/( npj,k ), then Pr\u0303 and Pr\u0302 are, for large\nn, independent and identically distributed. Noting that\nRB = (\u01eb \u2212 r\u0302 + r\u0303)\u2032 P\u2032 P(\u01eb \u2212 r\u0302 + r\u0303)\n\nand that D\u2032 Q = 0, some algebra and application of the central limit theorem\nyields the desired result. \u0003\nProof of Corollary 2.\n(20)\n\nExpanding the components of RB (\u03b8\u0303) yields\n\nmk \u2212 npk (\u03b8\u0303) mk \u2212 npk (\u03b8 0 ) n(pk (\u03b8\u0302) \u2212 pk (\u03b8 0 )) n(pk (\u03b8\u0303) \u2212 pk (\u03b8\u0302))\n\u221a\n\u221a\n\u221a\n\u221a\n=\n\u2212\n\u2212\n.\nn\nn\nn\nn\n\nAsymptotically, Taylor series expansions show that the second term on the\nright-hand side of this equation has the distribution of T\u01eb + S\u03b7 described in\nthe proof of Theorem 1, while the third term has the distribution of T\u03b5+ S\u03c4 .\nThe result follows using methodology in the proof of Theorem 1. \u0003\nAcknowledgments. The author is grateful to the Editor, an Associate\nEditor and two referees for helpful comments and suggestions.\nREFERENCES\nBayarri, M. J. and Berger, J. O. (2000). P values for composite null models (with\ndiscussion). J. Amer. Statist. Assoc. 95 1127\u20131142, 1157\u20131170. MR1804239\nBerger, J. O. and Pericchi, L. R. (1996). The intrinsic Bayes factor for model selection\nand prediction. J. Amer. Statist. Assoc. 91 109\u2013122. MR1394065\nBesag, J. (1974). Spatial interaction and the statistical analysis of lattice systems (with\ndiscussion). J. Roy. Statist. Soc. Ser. B 36 192\u2013236. MR373208\nBest, D. J. and Rayner, J. C. W. (1981). Are two classes enough for the \u03c72 goodness\nof fit test. Statist. Neerlandica 35 157\u2013163.\nChen, C. F. (1985). On asymptotic normality of limiting density functions with Bayesian\nimplications. J. Roy. Statist. Soc. Ser. B 47 540\u2013546. MR844485\nChernoff, H. and Lehmann, E. L. (1954). The use of maximum likelihood estimates in\n\u03c72 tests for goodness of fit. Ann. Math. Statist. 25 579\u2013586. MR65109\nClayton, D. G. and Kaldor, J. (1987). Empirical Bayes estimates of age-standardized\nrelative risks for use in disease mapping. Biometrics 43 671\u2013681.\nCox, D. R. and Hinkley, D. V. (1974). Theoretical Statistics. Chapman and Hall, London. MR370837\nCram\u00e9r, H. (1946). Mathematical Methods of Statistics. Princeton Univ. Press. MR16588\nDahiya, R. C. and Gurland, J. (1973). How many classes in the Pearson chi-square\ntest? J. Amer. Statist. Assoc. 68 707\u2013712. MR365835\nde la Horra, J. and Rodr\u00edguez-Bernal, M. T. (1997). Asymptotic behavior of the posterior predictive P -value. Comm. Statist. Theory Methods 26 2689\u20132699. MR1605584\nDey, D. K., Gelfand, A. E., Swartz, T. B. and Vlachos, P. K. (1998). A simulationintensive approach for checking hierarchical models. Test 7 325\u2013346.\n\n\fA BAYESIAN \u03c72 TEST FOR GOODNESS-OF-FIT\n\n25\n\nFienberg, S. E. (1980). The Analysis of Cross-Classified Categorical Data, 2nd ed. MIT\nPress.\nGelfand, A. E. (1996). Model determination using sampling-based methods. In Markov\nChain Monte Carlo in Practice (W. R. Gilks, S. Richardson and D. J. Spiegelhalter,\neds.) 145\u2013162. Chapman and Hall, London. MR1397969\nGelman, A. and Meng, X.-L. (1996). Model checking and model improvement. In Markov\nChain Monte Carlo in Practice (W. R. Gilks, S. Richardson and D. J. Spiegelhalter,\neds.) 189\u2013202. Chapman and Hall, London. MR1397966\nGelman, A., Meng, X.-L. and Stern, H. (1996). Posterior predictive assessment of\nmodel fitness via realized discrepancies (with discussion). Statist. Sinica 6 733\u2013807.\nMR1422404\nGuttman, I. (1967). The use of the concept of a future observation in goodness-of-fit\nproblems. J. Roy. Statist. Soc. Ser. B 29 83\u2013100. MR216699\nGvanceladze, L. G. and Chibisov, D. M. (1979). On tests of fit based on grouped\ndata. In Contributions to Statistics: Jaroslav H\u00e1jek Memorial Volume (J. Jureckov\u00e1,\ned.) 79\u201389. Academia, Prague. MR561261\nHamdan, M. (1963). The number and width of classes in the chi-square test. J. Amer.\nStatist. Assoc. 58 678\u2013689. MR156398\nHanley, J. A. and McNeil, B. J. (1982). The meaning and use of the area under a\nreceiver operating characteristic (ROC) curve. Radiology 143 29\u201336.\nKallenberg, W. C. M., Oosterhoff, J. and Schriever, B. F. (1985). The number\nof classes in chi-squared goodness-of-fit tests. J. Amer. Statist. Assoc. 80 959\u2013968.\nMR819601\nKoehler, K. J. and Gan, F. F. (1990). Chi-squared goodness-of-fit tests: Cell selection\nand power. Comm. Statist. Simulation Comput. 19 1265\u20131278.\nMann, H. B. and Wald, A. (1942). On the choice of the number of class intervals in the\napplication of the chi-square test. Ann. Math. Statist. 13 306\u2013317. MR7224\nMoore, D. S. and Spruill, M. C. (1975). Unified large-sample theory of general chisquared statistics for tests of fit. Ann. Statist. 3 599\u2013616. MR375569\nO'Hagan, A. (1995). Fractional Bayes factors for model comparison (with discussion).\nJ. Roy. Statist. Soc. Ser. B 57 99\u2013138. MR1325379\nOlver, F. W. J. (1974). Asymptotics and Special Functions. Academic Press, New York.\nMR435697\nPearson, K. (1900). On the criterion that a given system of deviations from the probable\nin the case of a correlated system of variables is such that it can be reasonably supposed\nto have arisen from random sampling. Philosophical Magazine 50 157\u2013175.\nQuine, M. P. and Robinson, J. (1985). Efficiencies of chi-square and likelihood ratio\ngoodness-of-fit tests. Ann. Statist. 13 727\u2013742. MR790568\nRaftery, A. E. and Lewis, S. (1992). How many iterations in the Gibbs sampler? In\nBayesian Statistics 4 (J. M. Bernardo, J. Berger, A. P. Dawid and A. F. M. Smith,\neds.) 763\u2013773. Oxford Univ. Press.\nRobert, C. P. and Rousseau, J. (2002). A mixture approach to Bayesian goodness of\nfit. Preprint.\nRobins, J. M., van der Vaart, A. and Ventura, V. (2000). Asymptotic distribution\nof P values in composite null models (with discussion). J. Amer. Statist. Assoc. 95\n1143\u20131167, 1171\u20131172. MR1804240\nRubin, D. B. (1984). Bayesianly justifiable and relevant frequency calculations for the\napplied statistician. Ann. Statist. 12 1151\u20131172. MR760681\nSinharay, S. and Stern, H. S. (2003). Posterior predictive model checking in hierarchical\nmodels. J. Statist. Plann. Inference 111 209\u2013221. MR1955882\n\n\f26\n\nV. E. JOHNSON\n\nSpiegelhalter, D., Best, N., Carlin, B. and van der Linde, A. (2002). Bayesian\nmeasures of model complexity and fit. J. R. Stat. Soc. Ser. B Stat. Methodol. 64 583\u2013\n639. MR1979380\nSpiegelhalter, D., Thomas, A. and Best, N. (2000). WinBUGS Version 1.3 Users Manual. Medical Research Council Biostatistics Unit, Cambridge. Available at www.mrcbsu.cam.ac.uk/bugs.\nVerdinelli, I. and Wasserman, L. (1998). Bayesian goodness-of-fit testing using infinitedimensional exponential families. Ann. Statist. 26 1215\u20131241. MR1647645\nWatson, G. S. (1957). The \u03c72 goodness-of-fit test for normal distributions. Biometrika\n44 336\u2013348. MR90951\nWilliams, C. (1950). On the choice of the number and width of classes for the chi-square\ntest of goodness-of-fit. J. Amer. Statist. Assoc. 45 77\u201386.\nDepartment of Biostatistics\nand Applied Mathematics\nUniversity of Texas\nM. D. Anderson Cancer Center\n1515 Holcombe Blvd., Unit 447\nHouston, Texas 77030-4009\nUSA\ne-mail: vejohnson@mdanderson.org\n\n\f"}