{"id": "http://arxiv.org/abs/1110.2704v1", "guidislink": true, "updated": "2011-10-12T17:06:12Z", "updated_parsed": [2011, 10, 12, 17, 6, 12, 2, 285, 0], "published": "2011-10-12T17:06:12Z", "published_parsed": [2011, 10, 12, 17, 6, 12, 2, 285, 0], "title": "An Efficient Fuzzy Clustering-Based Approach for Intrusion Detection", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.6153%2C1110.5118%2C1110.5575%2C1110.0335%2C1110.3449%2C1110.4163%2C1110.0623%2C1110.4469%2C1110.2944%2C1110.0224%2C1110.0619%2C1110.1851%2C1110.6152%2C1110.1411%2C1110.5045%2C1110.6348%2C1110.4286%2C1110.4404%2C1110.5199%2C1110.3967%2C1110.4069%2C1110.6848%2C1110.3941%2C1110.3759%2C1110.0247%2C1110.5921%2C1110.5677%2C1110.5358%2C1110.1040%2C1110.0926%2C1110.1989%2C1110.3371%2C1110.3252%2C1110.1423%2C1110.0608%2C1110.3549%2C1110.4618%2C1110.3576%2C1110.6847%2C1110.1246%2C1110.4949%2C1110.6222%2C1110.3854%2C1110.2919%2C1110.6446%2C1110.0172%2C1110.5513%2C1110.5892%2C1110.4158%2C1110.6421%2C1110.0111%2C1110.3583%2C1110.0510%2C1110.3137%2C1110.3426%2C1110.4687%2C1110.6330%2C1110.3920%2C1110.0577%2C1110.0595%2C1110.6187%2C1110.2251%2C1110.2423%2C1110.2264%2C1110.1919%2C1110.1157%2C1110.0844%2C1110.3814%2C1110.1507%2C1110.5308%2C1110.4269%2C1110.4526%2C1110.2515%2C1110.4602%2C1110.6386%2C1110.0053%2C1110.0227%2C1110.6406%2C1110.4538%2C1110.0450%2C1110.0715%2C1110.2704%2C1110.2798%2C1110.1235%2C1110.1295%2C1110.6188%2C1110.5125%2C1110.2356%2C1110.1321%2C1110.2776%2C1110.3599%2C1110.6821%2C1110.4496%2C1110.5701%2C1110.2760%2C1110.2837%2C1110.3215%2C1110.5319%2C1110.6695%2C1110.2524%2C1110.4272&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "An Efficient Fuzzy Clustering-Based Approach for Intrusion Detection"}, "summary": "The need to increase accuracy in detecting sophisticated cyber attacks poses\na great challenge not only to the research community but also to corporations.\nSo far, many approaches have been proposed to cope with this threat. Among\nthem, data mining has brought on remarkable contributions to the intrusion\ndetection problem. However, the generalization ability of data mining-based\nmethods remains limited, and hence detecting sophisticated attacks remains a\ntough task. In this thread, we present a novel method based on both clustering\nand classification for developing an efficient intrusion detection system\n(IDS). The key idea is to take useful information exploited from fuzzy\nclustering into account for the process of building an IDS. To this aim, we\nfirst present cornerstones to construct additional cluster features for a\ntraining set. Then, we come up with an algorithm to generate an IDS based on\nsuch cluster features and the original input features. Finally, we\nexperimentally prove that our method outperforms several well-known methods.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.6153%2C1110.5118%2C1110.5575%2C1110.0335%2C1110.3449%2C1110.4163%2C1110.0623%2C1110.4469%2C1110.2944%2C1110.0224%2C1110.0619%2C1110.1851%2C1110.6152%2C1110.1411%2C1110.5045%2C1110.6348%2C1110.4286%2C1110.4404%2C1110.5199%2C1110.3967%2C1110.4069%2C1110.6848%2C1110.3941%2C1110.3759%2C1110.0247%2C1110.5921%2C1110.5677%2C1110.5358%2C1110.1040%2C1110.0926%2C1110.1989%2C1110.3371%2C1110.3252%2C1110.1423%2C1110.0608%2C1110.3549%2C1110.4618%2C1110.3576%2C1110.6847%2C1110.1246%2C1110.4949%2C1110.6222%2C1110.3854%2C1110.2919%2C1110.6446%2C1110.0172%2C1110.5513%2C1110.5892%2C1110.4158%2C1110.6421%2C1110.0111%2C1110.3583%2C1110.0510%2C1110.3137%2C1110.3426%2C1110.4687%2C1110.6330%2C1110.3920%2C1110.0577%2C1110.0595%2C1110.6187%2C1110.2251%2C1110.2423%2C1110.2264%2C1110.1919%2C1110.1157%2C1110.0844%2C1110.3814%2C1110.1507%2C1110.5308%2C1110.4269%2C1110.4526%2C1110.2515%2C1110.4602%2C1110.6386%2C1110.0053%2C1110.0227%2C1110.6406%2C1110.4538%2C1110.0450%2C1110.0715%2C1110.2704%2C1110.2798%2C1110.1235%2C1110.1295%2C1110.6188%2C1110.5125%2C1110.2356%2C1110.1321%2C1110.2776%2C1110.3599%2C1110.6821%2C1110.4496%2C1110.5701%2C1110.2760%2C1110.2837%2C1110.3215%2C1110.5319%2C1110.6695%2C1110.2524%2C1110.4272&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The need to increase accuracy in detecting sophisticated cyber attacks poses\na great challenge not only to the research community but also to corporations.\nSo far, many approaches have been proposed to cope with this threat. Among\nthem, data mining has brought on remarkable contributions to the intrusion\ndetection problem. However, the generalization ability of data mining-based\nmethods remains limited, and hence detecting sophisticated attacks remains a\ntough task. In this thread, we present a novel method based on both clustering\nand classification for developing an efficient intrusion detection system\n(IDS). The key idea is to take useful information exploited from fuzzy\nclustering into account for the process of building an IDS. To this aim, we\nfirst present cornerstones to construct additional cluster features for a\ntraining set. Then, we come up with an algorithm to generate an IDS based on\nsuch cluster features and the original input features. Finally, we\nexperimentally prove that our method outperforms several well-known methods."}, "authors": ["Huu Hoa Nguyen", "Nouria Harbi", "J\u00e9r\u00f4me Darmont"], "author_detail": {"name": "J\u00e9r\u00f4me Darmont"}, "author": "J\u00e9r\u00f4me Darmont", "arxiv_comment": "15th East-European Conference on Advances and Databases and\n  Information Systems (ADBIS 11), Vienna : Austria (2011)", "links": [{"href": "http://arxiv.org/abs/1110.2704v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1110.2704v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1110.2704v1", "affiliation": "ERIC", "arxiv_url": "http://arxiv.org/abs/1110.2704v1", "journal_reference": null, "doi": null, "fulltext": "An Efficient Fuzzy Clustering-Based Approach\nfor Intrusion Detection\nHuu Hoa Nguyen, Nouria Harbi and J\u00e9r\u00f4me Darmont\nUniversit\u00e9 de Lyon (ERIC Lyon 2) - France\nnhhoa@eric.univ-lyon2.fr, {nouria.harbi, jerome.darmont}@univ-lyon2.fr\n\nAbstract. The need to increase accuracy in detecting sophisticated cyber\nattacks poses a great challenge not only to the research community but also to\ncorporations. So far, many approaches have been proposed to cope with this\nthreat. Among them, data mining has brought on remarkable contributions to\nthe intrusion detection problem. However, the generalization ability of data\nmining-based methods remains limited, and hence detecting sophisticated\nattacks remains a tough task. In this thread, we present a novel method based on\nboth clustering and classification for developing an efficient intrusion detection\nsystem (IDS). The key idea is to take useful information exploited from fuzzy\nclustering into account for the process of building an IDS. To this aim, we first\npresent cornerstones to construct additional cluster features for a training set.\nThen, we come up with an algorithm to generate an IDS based on such cluster\nfeatures and the original input features. Finally, we experimentally prove that\nour method outperforms several well-known methods.\nKeywords: classification, fuzzy clustering, intrusion detection, cyber attack.\n\n1 Introduction\nIn recent years, with the dramatically increasing use of network-based services and\nthe vast spectrum of information technology security breaches, more and more\norganizational information systems are subject to attack by intruders. Among many\napproaches proposed in the literature to deal with this threat, data mining brings on a\nnoticeable success to the development of high performance intrusion detection\nsystems (IDSs). The preeminence of such an approach lies in its good generalization\nabilities to correctly classify (or detect) both known and unknown attacks. However,\nas an inherent essence, the effectiveness of data mining-based IDSs depends heavily\nupon the quality of IDS datasets. In practice, IDS datasets are often extracted from\nraw traces in a chaotic system environment, and hence could hold implicit\ndeficiencies, e.g., the existence of noise in class labels due to mistakes in\nmeasurement, and the lack of base features. Moreover, due to the sophisticated\ncharacteristics of attacks and the diversification of normal events, different data\nregions could behave differently, i.e., true class labels could seriously be interlaced.\nSuch factors pose a great difficulty for inducers to identify appropriate decision\nboundaries from the input space of IDS datasets. In other words, when the input space\n\n\fis not robust enough to discriminate class labels, making further treatments from\nalternative knowledge sources as new supplemental features is highly desirable. To\nthis aim, one common approach is to transform the input space into a higher\ndimensional space from which data are more separable. New additional features can\nbe found by either manual ways based on prior knowledge or automatic analysis\nmethods (e.g., principle component analysis). However, in a high dimensional input\nspace, finding new relevant features is a tough task that often requires human\nanalyses, but derived features are sometimes not as good as expected. As a result, in\npractice, one often applies standard dimensional-transformation methods (e.g.,\npolynomial, radial basic function) to application domains where class discrimination\nis ambiguous and additional features are hard to be identified. Yet, such methods are\ngreatly affected by input parameters and data distribution, thus not always outputting\na high performance classifier. In this vision, it is desirable to find additional features\nin a less complex way so that general-purpose algorithms such as Decision Trees\n(DT) or Support Vector Machines (SVM) can learn the data more efficiently.\nSuch a context motivates us to propose a novel approach that treats fuzzy cluster\ninformation as additional features. These features are selectively incorporated into the\ninput space for building an efficient IDS. we experimentally show that our solution\napproach is considerably superior to several well-known methods.\nThe remainder of this paper is organized as follows. Section 2 presents the\nproblem formulation of our approach, whereas section 3 describes our solution for\ngenerating an IDS. Section 4 shows the experimental results we achieved. Section 5\nfinally gives a conclusion of the method we propose.\n\n2 Problem formulation\nClustering aims to organize data into groups (clusters) according to their similarities\nmeasured by some concepts. Unlike crisp clustering that crisply assigns each data\npoint to a separate cluster, fuzzy clustering allows each data point to belong to various\nclusters with different membership degrees (or weights). Fuzzy clusters are expressed\nby their centers (or centroids) that are simultaneously found in the partitioning\nprocess of a fuzzy clustering algorithm. The number of clusters (k) is often inputted as\na parameter to a fuzzy clustering algorithm. The n\uf0b4k membership matrix W={wij \uf0ce\n[0,1]} of n data points is found in the fuzzy clustering process. For example, Figure 1\ndescribes the instance space of a training set partitioned into four fuzzy clusters,\nwhere membership weights that data point x1 belongs to clusters '1', '2', '3', and '4' are\n0.3, 0.14, 0.16, and 0.4, respectively.\nLet us first denote S={X,Y} the original training set of n data points X={x1,...,xn},\nwhere each point xi is an m-dimensional vector (xi1,...,xim) and assigned to a label\nyi\uf0ceY belonging one of the c classes \uf057={\uf0771, ...,\uf077c}. Let B={bi| bi=max(wij), j=1...k}\nhold the maximum membership weight of each point xi, and Z={zi| zi=argmaxj(wij),\nj=1...k } contains the cluster (symbolic) number assigned to each point xi.\nFor conciseness in describing the approach, we term two column matrices Z and B\nas two \"basic cluster features\". In addition, we name the jth column of the membership\nmatrix (W) as Pj, and term the columns P1, ..., Pk as \"extended cluster features\". We\n\n\falso term the training set added with cluster features {X, Z, B, P1, ..., Pk, Y} as a\n\"manipulated training set\". These notations and terminologies are depicted in Figure1.\n\nx1\n0.4\n\n0.14\n0.16\n\nCentroid 2\n\nCentroid 4\nCentroid 3\n\nClass labels\n\nn training data points\n\nCentroid 1\n0.3\n\nx2\n\nX\nx1\nx2\n...\nxn\n\nZ\n'4'\n'3'\n...\n...\n\nB\n0.4\n0.45\n...\n...\n\nBasic cluster features\n\nP1\n0.3\n0.12\n....\n....\n\nP2\n0.14\n0.35\n....\n....\n\nP3\n0.16\n0.45\n...\n...\n\nP4\n0.4\n0.18\n...\n...\n\nY\ny1\ny2\n...\nyn\n\nExtended cluster features (W)\n\nFig. 1. A manipulated training set, resulting from adding cluster features into the input space.\n\nThe problem formulation follows: \"Given a training set S={X,Y} and an inducer I,\nthe goal is to find a high performance classifier induced by I over the m initial\nfeatures of S and the supplemental cluster features {Z, B, P1, P2, ..., Pk} resulting\nfrom a parameterized-by-k fuzzy clustering based on X\".\nUndoubtedly, fuzzy clustering has a great potential in expressing the latently\nnatural relationships between data points. Here, a question is whether information\nabout fuzzy clusters benefits certain inducing types. Basically, there exist some types\nof inducers to which fuzzy cluster features are helpful. For example, in the SVM\ncontext, the decision boundary often falls into a low density region, but the true\nboundary might not pass through this region, thus resulting in a poor classifier.\nHowever, when supplemented with relevant cluster features, data points in high\ndimensional spaces can become more uniform and discriminatory, hence avoiding an\nimproper separation across this region. In fact, the crucial factor to the success of\nSVM lies in a kernel trick that maps the initial input space to a much higher\ndimensional feature space, where the transformed data are expected to be more\nseparable from a linear hyper-plane function. In order words, while other inducers\nsomewhat find dimensionality a curse, blessing of dimensionality can enable SVM to\nbe more effective. Under such a sense, incorporating relevant cluster features into the\ninput space discernibly benefits SVM inducers.\nAnother consideration relates to the univariate Decision Tree (DT) setting. Due to\nits greedy characteristic, the DT inducer examines only one ahead partitioning step for\ngrowing child trees, rather than considering deeper partitioning steps that can achieve\na better tree. This characteristic can lead to an improper tree-growing termination\n(e.g., the XOR problem), and thus generate a poor classifier. In this vision, cluster\nfeatures help the DT inducer to determine splits more properly for tree growing.\n\n3 Fuzzy Cluster Feature-based Classification\n3.1 Cluster Feature Generation and Selection\nBasically, cluster features can be generated by any fuzzy clustering algorithm.\nHowever, for concreteness, we express cluster features with the fuzzy c-means\nclustering [6], which typically solves the minimization problem to the objective\n\n\ffunction of Formula 1. In a common form, the objective function (Formula 1) reaches\nto a minimum over W (membership matrix) and V (centroids), by Formulas 2 and 3.\nk\n\nf obj ( X , W , V ) \uf03d\n\nn\n\n\uf0e5\uf0e5w\n\n\uf061\nij\n\nk\n\nd ( x i , v j ) , subject to the constrain\n2\n\nj \uf03d1 i \uf03d1\n\n\uf0e6 n\n\uf0f6\n\uf061\nv j \uf03d \uf0e7 \uf0e5 ( w ij ) x i \uf0f7\n\uf0e8 i \uf03d1\n\uf0f8\n\nij\n\n\uf03d1\n\n\uf0e6 n\n\uf061 \uf0f6\n\uf0e7 \uf0e5 ( w ij ) \uf0f7\n\uf0e8 i \uf03d1\n\uf0f8\n\n\uf0f6 \uf061 \uf02d1\n\uf0f7\n\uf0f7\n\uf0f8\n\n(1)\n\nj \uf03d1\n\n1\n\n\uf0e6\n1\nw ij \uf03d \uf0e7\n\uf0e7 d ( x , v )2\ni\nj\n\uf0e8\n\n\uf0e5w\n\n(2)\n1\n\n\uf0e6\n1\n\uf0e5 \uf0e7\uf0e7 d ( x , v ) 2\nq \uf03d1 \uf0e8\ni\nq\nk\n\n\uf0f6 \uf061 \uf02d1\n\uf0f7\n\uf0f7\n\uf0f8\n\n(3)\n\nwhere \uf061 is a fuzzy constant and d(xi,vj) is the distance from xi (\uf0ceX ) to vj (\uf0ceV)\nFuzzy c-means clustering tries to find the best fit for a fixed value of k, the number\nof clusters. However, as an essential problem of clustering, determining an\nappropriate parameter k is a tough task. The most common way to find the reasonable\nnumber of clusters is to run the clustering with various values of k \uf0ce {2,..., kmax} and\nthen use a validity measure (e.g., partition coefficient ) to evaluate cluster fitness.\nIn our approach, however, we need data to be grouped in a way that reveals helpful\ninformation for inducers, not for clustering itself, even though the number of clusters\nmight be wrong. In other words, using validity measures to determine the best number\nof clusters is not reliable enough to derive good cluster features for classifiers. In such\na vision, instead of endeavoring to find the best k with validity measures, we use the\nover-production method to generate several candidate classifiers for different values\nof k and then evaluate their performance to determine the best one. Evaluating the\nperformance of candidate classifiers can be based either on a validation set or Cross\nValidation (CV) method [9]. Thus, a proper value of k is simultaneously found in the\nprocess of finding a maximum performance classifier from candidate classifiers.\nIn addition, the use of cluster features should be examined individually for a\nconcrete inducing type. Intuitively, two basic cluster features (Z, B) are benefic\nenough for DT inducer, instead of including k extended cluster features (P1,...,Pk). By\ncontrast, in the SVM context, it is applicable to employ either only the basic cluster\nfeatures (Z, B) or all the cluster features (Z, B, P1,...,Pk) for building a classifier.\nAnother solution that can be applied for any inducing type is to employ feature\nselection techniques (e.g., filter, wrapper) to pick out high merit features from both m\ninitial input features and all (k+2) cluster features. The objective is to apply feature\nselection techniques on (m+k+2) features to bring about a smaller but more qualitative\nfeature subset than those only on m initial features. Here, note is that feature selection\nis simultaneously carried out in the process of building candidate classifiers. In a\nnutshell, formally, there are three possibilities to incorporate cluster features into the\ninitial features (A1, ..., Am), i.e., (A1, ..., Am, Z, B), (A1, ..., Am, Z, B, P1, ..., Pk), or\nFeature Selection(A1, ..., Am, Z, B, P1, ..., Pk).\n3.2 Algorithm for generating a fuzzy cluster feature-based classifier\nOur algorithm for generating a classifier from both initial and cluster features, called\nCFC, is depicted from Figure 2. Related notations are indicated in Table 1.\n\n\fTable 1. Notations used in Figure 2.\n\nNotation\n\nDescription\n\nCk\nCk*\nVk\nVk*\nWk\n\nA candidate classifier resulting from a clustering with k fuzzy clusters.\nThe best classifier among |K| candidate classifiers.\nA k \uf0b4 m matrix of k centroids obtained from clustering X into k clusters.\nA k* \uf0b4 m matrix of k* centroids, corresponding to Ck*.\nAn n \uf0b4 k membership matrix of n data points xi \uf0ce X, corresponding to Vk.\nA column matrix containing the maximum membership weight of each xi \uf0ce X.\nA column matrix representing the cluster (symbolic) number of each xi \uf0ce X.\nA horizontal concatenation operator between two matrices.\n\nB\n\nk\n\nZ\n\uf051\n\nk\n\nTraining phase\nInput: S={X, Y}: The original training set\nI: a base inducer\nK: a predefined integer set representing possible number of clusters\n\uf078: a feature selection technique that returns a specific feature subset\nT: a type to employ features for building classifiers\nOutput: C k * , V k *\n1: X \uf0a2 \uf0ac N orm alize( X )\n2: For each k \uf0ce K do\nk\nk\n{W , V } \uf0ac FuzzyC lustering ( X \uf0a2, k )\n3:\n\n//Normalize continuous features\n\n4:\n\nB \uf0ac {bi | bi \uf03d m ax ( w ij ), i \uf03d 1 ...n , j \uf03d 1 ...k }\n\n5:\n6:\n7:\n\nZ\n\nCase\nT = 1: D \uf0ac ( X\n\n8:\n9:\n\nT = 2: D \uf0ac ( X\nT = 3:\n\nk\n\nk\n\n\uf0ac { z i | z i \uf03d arg m ax j ( w ij ), i \uf03d 1 ...n , j \uf03d 1 ...k }\n\uf051\n\nZ\n\nk\n\n\uf051\n\nB )\n\n\uf051\n\nZ\n\nk\n\n\uf051\n\nB\n\nZ\n\nk\n\nF \uf0ac \uf078 (X\nD \uf0ac (X\n\n10:\n11:\n\n\uf051\n\n\uf051\n\nZ\n\nk\n\n\uf051\n\n//D is a manipulated training set\n//Initial features & basic cluster features\n\nk\n\n\uf051\n\nB\n\nk\n\n\uf051\n\nB\n\nk\n\nk\n\n\uf051\n\n//Initial features & all cluster features\n\nk\n\nW )\n\uf051\n\nk\n\nW ,Y )\nk\n\nW )\n\n[F]\n\n//Apply a feature selection\n//Project data by the derived subset\n\nEnd Case\nCk \uf0ac I ( D ,Y )\n\n//Build a classifier, using the manipulated training set D & inducer I\n\n12: Performance( C k ) \uf0ac {Average performance of q-fold CV based on (D,Y) and I }\n13: End For\n14: C k * \uf0ac arg m ax C P erfo rm an ce ( C k ), k \uf0ce K\n//Determine one best classifier\nk\n\n15: Return C k * , V k *\nOperation phase\n16: For an unlabeled testing instance x:\n17: x \uf0a2 \uf0ac N ormalize( x )\n\n//Normalize continuous features\n\n18: Compute membership weights ( w j | j \uf03d 1...k *) that x \uf0a2 belongs to v j \uf0ce V k * (Formula 3)\n19: b \uf0ac max( w j | j \uf03d 1...k *)\n20: z \uf0ac arg max j ( w j | j \uf03d 1...k *)\n21: Label x, by taking cluster features { z , b , w j } into account, using C k *\nFig. 2. Algorithm CFC.\n\n\fThe key idea is that, for each clustering with different number of clusters (k\uf0ceK),\nthe algorithm builds and valuates a candidate classifier from the training set\nmanipulated with a given feature selection type, by q-Fold Cross Validation [9]. The\nresulting classifier is the one exhibiting maximum performance.\nIn the training phase, the algorithm first normalizes continuous features (e.g., by a\nvariance-based spread measure) to avoid the dispersion in different ranges (Line 1).\nHere, it is noticed that the normalized data (X\uf0a2) is merely for clustering purpose,\nwhereas classifiers are built by using the original data (X). In addition, instead of\nexecuting clustering with parameter k ranging from 2 to a given kmax value, the\nalgorithm uses a predefined set K={k} to mainly focus on important values of k,\nwhich can be recognized by experiment or prior knowledge (Line 2). As mentioned in\nSection 3.1, there are three cases to incorporate cluster features into the initial\nfeatures. Hence, for general purpose, the algorithm introduces an input parameter T\nfor specifying the way to employ features for building classifiers (Lines 6-10).\nSubsequently, the algorithm builds and evaluates one candidate classifier for each\nclustering (Lines 11, 12). Here, note is that evaluating candidate classifiers is based\non the averaged performance of q-fold stratified cross validation from the\nmanipulated training set. Finally, the algorithm determines one best classifier from |K|\ncandidate classifiers, together with a corresponding centroid set (Lines 14, 15).\nIn the operation phase, for an unlabeled testing instance x, the algorithm first\nnormalizes x in the same way as those applied to the training set. Then, cluster\nfeatures of x are calculated based on the centroid set V k * (Lines 18-20). Finally, the\ncorresponding features are input to classifier C k * for final prediction (Line 21).\n\n4 Experiments\n4.1 Dataset\nOur experiments are conducted on the intrusion detection dataset KDD99 [3]. This\ndataset was derived from the DARPA dataset, a format of TCPdump files captured\nfrom the simulation of normal and attack activities in the network environment of an\nair-force base, created by MIT's Lincoln Laboratory. The KDD99 dataset comprises\n494,021 training instances and 311,029 testing instances. Due to data volume, the\nresearch community mostly uses small subsets of the dataset for evaluating IDS\nmethods. Each instance in the dataset represents a network connection, i.e., a\nsequence of network packets starting and ending at some well defined times, between\nwhich data flows to and from a source IP address to a target IP address under some\nwell defined protocol. Such a connection instance is described by a 41-dimensional\nfeature vector and labeled with respect to five classes: Normal, Probe, DoS (denial of\nservice), R2L (remote to local), and U2R (user to root).\nTo facilitate experiments without losing generality, we only use a smaller set of the\nKDD99 dataset for the purpose of evaluating and comparing our method to others. In\nparticular, the training and testing sets used in our experiments are made up of 33,016\ninstances and 169,687 instances that are selectively extracted from the KDD99\n\n\ftraining and testing sets, respectively. The principle for forming such reduced sets is\nto get all instances in each small group (attack type), but only a limited amount of\ninstances in each large group, from both the KDD99 training and testing sets. More\nexplicitly, for forming the reduced training, we randomly select five percent of each\nlarge group Neptune, smurf, and normal, while gathering all instances in the\nremaining groups from the KDD99 training set. For sampling the reduced testing set,\nwe randomly select 50 percent of each large group Neptune, smurf, and normal,\nwhereas collecting all instances in the remaining groups from the KDD99 testing set.\nClass distribution of these two reduced sets is shown in Table 2.\nTable 2. Class distribution of the reduced training and testing sets used in experiments.\n\nClass\n\nTraining set\n\nTesting set\n\nDoS\nProbe\nR2L\n\n22,867\n4,107\n1,126\n\n118,807\n4,166\n16,347\n\nClass\nU2R\nNormal\nTotal\n\nTraining set\n\nTesting set\n\n52\n4,864\n33,016\n\n70\n30,297\n169,687\n\n4.2 Experiment Setup\nIn our experiments, the predefined set K is set to {2, 3, ..., 50}. The convergence\ncriterion (termination tolerance) of fuzzy c-means clustering is set to 10-6, whereas the\nfuzzy degree (exponent \uf061 in Formulas 1-3) is set to 3. On the other hand, continuous\nfutures are normalized by max_min value ranges [6]. To handle different feature types\nas well as express different merit contributions of features in the Euclidian space, we\ncalculate distances between data points by the metric proposed in Formula 4.\nm\n\nd ( xi , v j ) \uf03d\n2\n\n\uf0e5G\n\nq\n\n\uf0b4 d q ( x iq , v jq )\n\n2\n\n(4)\n\nq\n\nwhere Gq is information gain of feature q [5], and\n\uf0ec1, if \uf028 x iq , v jq \uf0ce {sym bolic} \uf029 \uf0d9 ( x iq \uf0b9 v jq ) or \uf028 x iq , v jq \uf0ce {unknow n} \uf029\n\uf0ef\n\uf0ef | x iq \uf02d v jq |, if x iq , v jq \uf0ce {continuous}\n\uf0ef\nd q ( x iq , v jq ) \uf03d \uf0ed | x \uf02d v |\niq\njq\n\uf0ef\n, if x iq , v jq \uf0ce {ordinals}; t \uf03d {ordinals}\nt\n\uf02d\n1\n\uf0ef\n\uf0ef 0, otherw ise,\n\uf0ee\n\nThe base inducers (I) tested in our method are the C4.5 decision tree [5] and the\nSVM [2] with polynomial and radial basic function kernels. The feature selection\ntechnique (\uf078) used in this experiment is Correlation-based Feature Subset Evaluation\n(CfsSubsetEval) with genetic search [7]. CfsSubsetEval evaluates the merit of a\nfeature subset by considering the individual predictive ability of each feature along\nwith the degree of redundancy between them. Those subsets that are highly correlated\nwith the class while having low intercorrelation are preferred.\nCandidate classifiers are evaluated by an attack type-based stratified cross\nvalidation (q=10 folds). The maximum performance classifier is determined based on\noverall accuracy (i.e., the ratio of the number of correctly classified instances to the\ntotal number of instances in the training set).\n\n\f4.3 Experiment Results\nThe experimental comparison of our method to other well-known methods is featured\nin Table 3. All the compared classifiers are built from the same training set and tested\non the same testing set as described in Section 4.1. Moreover, Figure 4 depicts True\nPositive Rates (TPRs) and False Positive Rates (FPRs) of classifiers with respect to\neach class label, whereas Figure 3 portrays average TPRs and FPRs of classifiers.\nTPR of a class \uf077c is the ratio of \"the number of correctly classified instances in the\nclass \uf077c\" to \"the total number of instances in the class \uf077c\". FPR of a class \uf077c is the\nratio of \"the number of instances that do not belong to the class \uf077c but are classified\nas \uf077c\" to \"the total number of instances that do not belong to the class \uf077c\".\nTo have a wider comparative view, we run our algorithm (CFC) with different\nsettings of two parameters (i.e., I: base inducer; T: the way to employ cluster features\nfor building classifiers). The results of such runs are listed in Rows 10-18 of Table 3.\nAs shown in Figures 3 and 4, our method, in general, considerably outperforms the\nothers with respect to TPRs in all five classes and on average. Particularly, CFC\nclassifiers are significantly better than all the others in detecting hard classes (i.e.,\nR2L and U2R). On the other hand, FPRs of CFC classifiers are generally lower than\nthose of the others. Our method also considerably improves the classification ability\nof base inducers (SVM and DT) in both viewpoints, i.e., applying or not applying\nfeature selection. More concretely, by using the same feature selection technique, the\nSVM classifier built from the manipulated training set (i.e., CFC(I=SVM,T=3)) is\nconsiderably superior to the SVM classifier built from the original training set (i.e.,\nSVM_FS). Similarly, the performance of CFC(I=DT,T=3) is considerably better than\nthat DT_FS. This tells that applying a feature selection technique on the manipulated\ntraining set produces a higher qualitative feature subset (including base features and\ncluster features) than that on the original training set.\nRegarding the SVM context, although we further test PSVM (Polynomial SVM)\nwith exponent degrees ranging from 2 to 6, its performance remains worse than\nCFC(PSVM(degree=2),T={1,2,3}). On average, CFC(PSVM (degree=2),T={1,2,3})\ngives a 91.96% TPR (with a 2.2% FPR), whereas PSVM(degree={2,...,6}) produces\nan 86.84% TPR (with a 3.44% FPR). We also test RSVM (Radial Basic Function\nSVM) with widths Gamma ranging from 0.1 to 1.0, but its performance still\nunderperforms CFC(RSVM(Gamma=0.1),T={1,2,3}). More precisely, on average,\nRSVM(Gamma={0.1,0.2,...,1}) produces an 86.72% TPR (with a 3.62% FPR),\nwhereas CFC(RSVM(Gamma=0.1),T={1,2,3}) gives a 91.15% TPR (with a 2.3%\nFPR). This tells that cluster features benefit SVM in high dimensionality.\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\n0\n\n87.05\n\n3.00\n\n86.27\n\n5.82\n\n86.69\n\n3.20\n\n86.18\n\n4.68\n\n86.49\n\n3.25\n\n87.03\n\n3.27\n\n86.94\n\n5.03\n\n87.02\n\n3.32\n\n87.09\n\n4.61\n\n90.89\n\n90.18\n\n91.49\n\n91.70\n\n91.24\n\n92.92\n\n90.37\n\n90.96\n\n92.12\n\nTPR\n2.35\n\n2.90\n\n2.16\n\n2.18\n\n2.35\n\n2.08\n\n2.45\n\n2.38\n\nFig. 3. Average True Positive and False Positive Rates (%) of classifiers\n\n2.07\n\nFPR\n\n\fTable 3. True Possitive and False Possitive rates (%) of classifiers.\nClassifier\n1. Boosting\n\nDoS Probe\nR2L\nU2R Normal Average\nTP\n95.36 82.48\n5.51 35.71\n99.22\n87.05\nFP\n0.44\n0.46\n0.03\n0.01\n15.01\n3.00\n2. Bagging\nTP\n94.72 80.03\n3.46 42.86\n98.74\n86.27\nFP\n4.64\n0.41\n0.30\n0.02\n14.18\n5.82\n3. NBTree\nTP\n94.53 83.32\n9.54 51.43\n98.08\n86.69\nFP\n0.84\n0.62\n0.60\n0.24\n14.22\n3.20\n4. DT\nTP\n94.72 78.68\n2.84 51.43\n98.77\n86.18\nFP\n2.85\n0.57\n0.03\n0.10\n14.96\n4.68\n5. DT_FS\nTP\n94.26 85.60\n7.11 38.57\n99.06\n86.49\nFP\n0.82\n0.93\n0.26\n0.05\n14.70\n3.25\n6. PSVM\nTP\n95.14 84.09\n9.43 38.57\n97.61\n87.03\nFP\n0.91\n0.52\n0.24\n0.01\n14.56\n3.27\n7. PSVM_FS\nTP\n95.44 73.84\n8.86 44.29\n97.65\n86.94\nFP\n3.57\n0.22\n0.29\n0.01\n14.00\n5.03\n8. RSVM\nTP\n95.11 83.99\n9.51 38.57\n97.62\n87.02\nFP\n0.98\n0.53\n0.23\n0.01\n14.55\n3.32\n9. RSVM_FS\nTP\n94.98 81.73\n9.92 40.00\n98.64\n87.09\nFP\n3.04\n0.55\n0.17\n0.01\n13.75\n4.61\n10. CFC(I=DT, T=1)\nTP\n97.69 88.65 25.93 58.57\n99.62\n90.89\nFP\n0.76\n0.53\n0.02\n0.03\n10.12\n2.35\n11. CFC(I=DT, T=2)\nTP\n97.46 88.24 21.47 60.00\n99.03\n90.18\nFP\n1.45\n0.75\n0.03\n0.04\n10.46\n2.90\n12. CFC(I=DT, T=3)\nTP\n98.30 90.13 28.01 62.86\n99.27\n91.49\nFP\n0.70\n0.65\n0.03\n0.04\n9.24\n2.16\n13. CFC(I=PSVM, T=1)\nTP\n98.42 92.49 28.19 68.57\n99.57\n91.70\nFP\n0.81\n0.68\n0.03\n0.03\n8.93\n2.18\n14. CFC(I=PSVM, T=2)\nTP\n98.12 92.20 26.27 75.71\n99.20\n91.24\nFP\n0.87\n0.48\n0.02\n0.04\n9.70\n2.35\n15. CFC(I=PSVM, T=3)\nTP\n98.83 94.89 37.62 74.29\n99.36\n92.92\nFP\n1.08\n0.71\n0.03\n0.03\n7.31\n2.08\n16. CFC(I=RSVM, T=1)\nTP\n97.42 95.06 21.61 68.57\n99.23\n90.37\nFP\n0.76\n0.61\n0.04\n0.02\n10.65\n2.45\n17. CFC(I=RSVM, T=2)\nTP\n98.15 91.72 22.51 72.86\n99.62\n90.96\nFP\n0.83\n0.58\n0.03\n0.03\n9.96\n2.38\n18. CFC(I=RSVM, T=3)\nTP\n98.22 94.36 33.81 68.57\n99.45\n92.12\nFP\n0.79\n0.70\n0.02\n0.03\n8.39\n2.07\n- DT refers to the C4.5 decision tree inducer [5] with established input parameters:\npruning method = pessimistic pruning, confidence=0.2, and Min(#instances per leaf)=6.\n- Boosting uses the AdaBoost [8] with parameters: base inducer=DT, # classifiers=10.\n- Bagging uses the Bagging [4] with parameters: base inducer=DT, # classifiers=10.\n- PSVM refers to SVM inducer with Polynomial Kernel (exponent degree = 2).\n- RSVM refers to SVM inducer with Radial Basic Function Kernel (width gamma = 0.1).\n- Classifiers 1-9 are trained on the original training set (without cluster features), where\nclassifiers 5, 7, and 9 employ the feature selection technique (\uf078) as described in Section\n5.2, whereas classifiers 1-4, 6, and 8 do not apply the feature selection technique (\uf078).\n- Classifiers 10-18 are built from the CFC algorithm whose base inducers have the same\nparameter settings as stand-alone classifiers 4, 6, and 8.\n- The column Average is the average weighted by the number of instances on each class.\n\n\f100\n90\n80\n70\n60\n\n50\n\nDOS\n\n40\n\nPROBE\n\n30\n\nR2L\n\n20\n\nU2R\nNormal\n\n10\n0\n\nFig. 4. True Positive Rates (%) of classifiers on each class.\n\n5 Conclusion and Future Work\nWe propose in this paper a novel method in applying data mining to the intrusion\ndetection problem. The incorporation of cluster features resulting from a fuzzy\nclustering into the training process is proven to be efficient for enhancing the strength\nof a base classifier. The tactic to achieve a high performance classifier from a training\nset supplemented with cluster features is addressed. We experimentally show that, as\na whole, our method clearly outperforms all the tested methods. Although the\nexperiments are conducted on the KDD99 IDS dataset, the approach we propose can\nbe generally used to improve classification in other application domains. However, to\nbe more objective in evaluating any data mining solution, our future work will be to\ntest the proposed method on other real datasets. In particular, our current effort is\nfulfilling a honeypot system for gathering both real intrusion and normal traffic\nactivities. Such a real dataset will then be used to evaluate the method we proposed.\n\nReferences\n1. Amiria, F., Yousefia, M.R., Lucasa, C., Shakeryb, A., Yazdanib, N.: Mutual Information\nBased Feature Selection for Intrusion Detection Systems. JNCA, V.34, pp.1184-1199 (2011)\n2. Platt, J.: Fast Training of Support Vector Machines using Sequential Minimal Optimization.\nAdvances in Kernel Methods - Support Vector Learning, pp. 185-208, MIT Press (1999)\n3. UCI KDD Archive, http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n4. Breiman, L.: Bagging Predictors. Machine Learning, Vol. 24(2), pp. 123\u2013140 (1996)\n5. Quinlan J.R.: C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo (1993)\n6. Hoppner, F.: Fuzzy Cluster Analysis. John Wiley & Sons, pp. 37-43 (2000)\n7. Hall, M.A.: Correlation-based Feature Subset Selection for Machine Learning. Hamilton,\nNew Zealand (1998)\n8. Freund, Y., Schapire R.E.: Experiments with a New Boosting Algorithm. In: Thirteenth\nInternational Conference on Machine Learning, San Francisco, pp. 148\u2013156 (1996)\n9. Andrew, Y.N.: Preventing Overfitting of Cross-Validation Data. ICML, pp. 245-253 (1997)\n10.Gupta K.K., Nath, B., Ramamohanarao, K.: Layered Approach Using Conditional Random\nFields for Intrusion Detection. IEEE Trans. Dependable Sec. Comput, 7(1), pp. 35-49 (2010)\n\n\f"}