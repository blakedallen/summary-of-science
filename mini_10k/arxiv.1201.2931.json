{"id": "http://arxiv.org/abs/1201.2931v3", "guidislink": true, "updated": "2013-11-08T15:51:24Z", "updated_parsed": [2013, 11, 8, 15, 51, 24, 4, 312, 0], "published": "2012-01-13T20:44:52Z", "published_parsed": [2012, 1, 13, 20, 44, 52, 4, 13, 0], "title": "The HIM glocal metric and kernel for network comparison and\n  classification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.3611%2C1201.5078%2C1201.0963%2C1201.0250%2C1201.0257%2C1201.3561%2C1201.1744%2C1201.3525%2C1201.2314%2C1201.1898%2C1201.0498%2C1201.3192%2C1201.3001%2C1201.1297%2C1201.2817%2C1201.3756%2C1201.0312%2C1201.2931%2C1201.0589%2C1201.1755%2C1201.6525%2C1201.0199%2C1201.3202%2C1201.0157%2C1201.4506%2C1201.5227%2C1201.4425%2C1201.6212%2C1201.4575%2C1201.2427%2C1201.1684%2C1201.6266%2C1201.0878%2C1201.1151%2C1201.5480%2C1201.5814%2C1201.5517%2C1201.3715%2C1201.4919%2C1201.2144%2C1201.1795%2C1201.3853%2C1201.2457%2C1201.1732%2C1201.0591%2C1201.5074%2C1201.6023%2C1201.6127%2C1201.5396%2C1201.1303%2C1201.0010%2C1201.3189%2C1201.6110%2C1201.5348%2C1201.4161%2C1201.5779%2C1201.6232%2C1201.5391%2C1201.0676%2C1201.3775%2C1201.5694%2C1201.2897%2C1201.0080%2C1201.0478%2C1201.6260%2C1201.6186%2C1201.4407%2C1201.3956%2C1201.5475%2C1201.0659%2C1201.2695%2C1201.2049%2C1201.2075%2C1201.0554%2C1201.5359%2C1201.0532%2C1201.0758%2C1201.5065%2C1201.4915%2C1201.0884%2C1201.2297%2C1201.2685%2C1201.2989%2C1201.1943%2C1201.0918%2C1201.0610%2C1201.0117%2C1201.2901%2C1201.2438%2C1201.2233%2C1201.0170%2C1201.5856%2C1201.3206%2C1201.5211%2C1201.3588%2C1201.3929%2C1201.6312%2C1201.5636%2C1201.5082%2C1201.5928%2C1201.2927&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The HIM glocal metric and kernel for network comparison and\n  classification"}, "summary": "Due to the ever rising importance of the network paradigm across several\nareas of science, comparing and classifying graphs represent essential steps in\nthe networks analysis of complex systems. Both tasks have been recently tackled\nvia quite different strategies, even tailored ad-hoc for the investigated\nproblem. Here we deal with both operations by introducing the\nHamming-Ipsen-Mikhailov (HIM) distance, a novel metric to quantitatively\nmeasure the difference between two graphs sharing the same vertices. The new\nmeasure combines the local Hamming distance and the global spectral\nIpsen-Mikhailov distance so to overcome the drawbacks affecting the two\ncomponents separately. Building then the HIM kernel function derived from the\nHIM distance it is possible to move from network comparison to network\nclassification via the Support Vector Machine (SVM) algorithm. Applications of\nHIM distance and HIM kernel in computational biology and social networks\nscience demonstrate the effectiveness of the proposed functions as a general\npurpose solution.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.3611%2C1201.5078%2C1201.0963%2C1201.0250%2C1201.0257%2C1201.3561%2C1201.1744%2C1201.3525%2C1201.2314%2C1201.1898%2C1201.0498%2C1201.3192%2C1201.3001%2C1201.1297%2C1201.2817%2C1201.3756%2C1201.0312%2C1201.2931%2C1201.0589%2C1201.1755%2C1201.6525%2C1201.0199%2C1201.3202%2C1201.0157%2C1201.4506%2C1201.5227%2C1201.4425%2C1201.6212%2C1201.4575%2C1201.2427%2C1201.1684%2C1201.6266%2C1201.0878%2C1201.1151%2C1201.5480%2C1201.5814%2C1201.5517%2C1201.3715%2C1201.4919%2C1201.2144%2C1201.1795%2C1201.3853%2C1201.2457%2C1201.1732%2C1201.0591%2C1201.5074%2C1201.6023%2C1201.6127%2C1201.5396%2C1201.1303%2C1201.0010%2C1201.3189%2C1201.6110%2C1201.5348%2C1201.4161%2C1201.5779%2C1201.6232%2C1201.5391%2C1201.0676%2C1201.3775%2C1201.5694%2C1201.2897%2C1201.0080%2C1201.0478%2C1201.6260%2C1201.6186%2C1201.4407%2C1201.3956%2C1201.5475%2C1201.0659%2C1201.2695%2C1201.2049%2C1201.2075%2C1201.0554%2C1201.5359%2C1201.0532%2C1201.0758%2C1201.5065%2C1201.4915%2C1201.0884%2C1201.2297%2C1201.2685%2C1201.2989%2C1201.1943%2C1201.0918%2C1201.0610%2C1201.0117%2C1201.2901%2C1201.2438%2C1201.2233%2C1201.0170%2C1201.5856%2C1201.3206%2C1201.5211%2C1201.3588%2C1201.3929%2C1201.6312%2C1201.5636%2C1201.5082%2C1201.5928%2C1201.2927&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Due to the ever rising importance of the network paradigm across several\nareas of science, comparing and classifying graphs represent essential steps in\nthe networks analysis of complex systems. Both tasks have been recently tackled\nvia quite different strategies, even tailored ad-hoc for the investigated\nproblem. Here we deal with both operations by introducing the\nHamming-Ipsen-Mikhailov (HIM) distance, a novel metric to quantitatively\nmeasure the difference between two graphs sharing the same vertices. The new\nmeasure combines the local Hamming distance and the global spectral\nIpsen-Mikhailov distance so to overcome the drawbacks affecting the two\ncomponents separately. Building then the HIM kernel function derived from the\nHIM distance it is possible to move from network comparison to network\nclassification via the Support Vector Machine (SVM) algorithm. Applications of\nHIM distance and HIM kernel in computational biology and social networks\nscience demonstrate the effectiveness of the proposed functions as a general\npurpose solution."}, "authors": ["Giuseppe Jurman", "Roberto Visintainer", "Michele Filosi", "Samantha Riccadonna", "Cesare Furlanello"], "author_detail": {"name": "Cesare Furlanello"}, "author": "Cesare Furlanello", "arxiv_comment": "Frontiers of Network Analysis: Methods, Models, and Applications -\n  NIPS 2013 Workshop", "links": [{"href": "http://arxiv.org/abs/1201.2931v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1201.2931v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.SI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.soc-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1201.2931v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1201.2931v3", "journal_reference": null, "doi": null, "fulltext": "arXiv:1201.2931v3 [math.CO] 8 Nov 2013\n\nThe HIM glocal metric and kernel for\nnetwork comparison and classification\n\n3\n\nG. Jurman\u2217 1 , R. Visintainer1 , M. Filosi1,2 , S. Riccadonna3 , C. Furlanello1\n1\nFondazione Bruno Kessler, Trento, Italy\n2\nCIBIO, University of Trento, Italy\nResearch and Innovation Centre, Fondazione Edmund Mach, San Michele all'Adige, Italy\n{jurman,visintainer,filosi,furlan}@fbk.eu\nsamantha.riccadonna@fmach.it\n\nAbstract\nDue to the ever rising importance of the network paradigm across several areas of science, comparing\nand classifying graphs represent essential steps in the networks analysis of complex systems. Both\ntasks have been recently tackled via quite different strategies, even tailored ad-hoc for the investigated problem. Here we deal with both operations by introducing the Hamming-Ipsen-Mikhailov\n(HIM) distance, a novel metric to quantitatively measure the difference between two graphs sharing\nthe same vertices. The new measure combines the local Hamming distance and the global spectral\nIpsen-Mikhailov distance so to overcome the drawbacks affecting the two components separately.\nBuilding then the HIM kernel function derived from the HIM distance it is possible to move from\nnetwork comparison to network classification via the Support Vector Machine (SVM) algorithm.\nApplications of HIM distance and HIM kernel in computational biology and social networks science\ndemonstrate the effectiveness of the proposed functions as a general purpose solution.\n\n1\n\nIntroduction\n\nThe arising prevalence of the network paradigm [1] as the elective model for complex systems analysis in different\nworkfields has strongly contributed in stimulating graph theoretical techniques in the recent scientific literature. Methods based on graph properties have spread through the static and dynamic analysis of different economical, chemical\nand biological system, computer networking, social networks and neuroscience. As a relevant example, it is worthwhile\nmentioning the rapid diffusion, in computational biology, of the differential network analysis [2, 3, 4, 5, 6, 7, 8, 9, 10].\nIn particular, two key tasks constitute the backbone of most of the aforementioned analysis techinques, namely network\ncomparison and network classification, and they both rely on the basic idea of measuring the similarity between two\ngraphs.\nNetwork comparison consists in the quantification of the difference between two homogeneous objects in some network space, while the aim of network classification is to predictively discriminate graphs belonging to different classes,\nfor instance by means of machine learning algorithms. Network comparison has its roots in the quantitative description\nof main properties of a graph (e.g., degree distribution), which can be encoded into a feature vector [11], thus providing\na convenient representation for classification tasks (see for instance [12] for a very recent approach). As a major alternative strategy, one can adopt a direct comparison method stemming from the graph isomorphism problem, by defining\na suitable similarity measure on the topology of the underlying (possibly directed and/or weighted) graphs. This line\nof study dates back to the 70s with the theory of graph distances, regarding both inter- and intra-graphs metrics [13].\nSince then, a wide range of similarity measures has been defined, based on very different graph indicators. To mention\nsome of the most important metrics, we list the family of edit distances, evaluating the minimum cost of transformation of one graph into another by means of the usual edit operations (insertion and deletion of links), the family of\ncommon network subgraphs, looking for shared structures between the graphs and the family of spectral measures,\nrelying on functions of the eigenvalues of one of the graph connectivity matrices. Similarly, graph classification can\n\u2217\n\nCorresponding author\n\n1\n\n\fbe tackled by a number of different techniques, for instance nearest neighbours on Euclidean distance of the features'\nvectors of the graphs [14, 15, 16], or Support Vector Machine with the graph Laplacian as a regularization term [17],\nor via different subgraph-based lerning algorithms [18]. However, in general the most efficient techniques use a kernel\nmachine, where the kernel itself corresponds to a scalar product (and hence a distance) in a suitable Hilbert space\n[19, 20, 21, 22, 23, 24, 25, 26, 27]. For more recent advances, we cite the Weisfeiler-Lehman graph kernel [28], and its\nuse in neuroimaging classification for discriminating mild cognitive impairment from Alzheimer's disease [29]. This\nlast citation stands as an example of the increasing interest for these techniques recently appearing in neurosciences\n[30, 31].\nIn the present work we propose a novel solution to both the comparison and the classification tasks by introducing\nthe novel HIM metric for comparing graphs (even directed and weighted) and a graph kernel induced by the HIM\nmeasure. The HIM distance is defined as the one-parameter family of product metrics linearly combining \u2013 by a\nnon-negative real factor \u03be \u2013 the normalized Hamming distance H [32,\n\u221a 33, 34, 35] and the normalized Ipsen-Mikhailov\ndistance IM [36]; the product metric is normalized by the factor 1 + \u03be to set its upper bound to 1. In absence of\na gold standard driving the search for the optimal weight ratio, we decided for an equal contribution from the two\ncomponents \u03be = 1 as the most natural choice. The Hamming distance is the simplest member of the family of edit\ndistances, evaluating the occurrence of matching links in the compared networks: by definition, it is a local measure\nof dissimilarity between graphs, because it only focusses on the links as independent entities, disregarding the overall\nstructure. On the other hand, the spectral distances are global measures, evaluating the differences between the whole\nnetwork structures: however, they cannot discriminate between isospectral non-identical graphs: for a recent spectral\napproach, see [37]. In the comparative review [38], the properties of the existing graph spectral distances were studied,\nand the Ipsen-Mikhailov metric emerged as the more reliable and stable. The combination of the two components\nwithin a single metric allows overcoming their drawbacks and obtaining a measure which is simultaneously global\nand local. Moreover, the imposed normalization limits the values of the HIM distance between zero (reached only\nby comparing identical networks) and one (attained when comparing a clique and the empty graph), regardless of the\nnumber of vertices. Finally, the HIM distance can also be applied to multilayer networks [39, 40], since a rigorous\ndefinition of their Laplacian has just been proposed [41, 42]. By a Gaussian-like map [43], the HIM distance generates\nthe HIM kernel. Plugging the HIM kernel [44] into a Support Vector Machine gives us a classification algorithm based\non the HIM distance, to be used as is or together with other graph kernels in a Multi-Kernel Learning framework\nto increase the classification performance and to enhance the interpretability of the results [45]. Note that, although\npositive definiteness does not hold globally for the HIM kernel, this property can be guaranteed on the given training\ndata, thus leading to positive definite matrices suitable for the convergence of the SVM optimizer.\nTo conclude with, we present some applications of the HIM distance and the HIM kernel to some real datasets belonging to different areas of science. These examples support the positive impact of the HIM suite as general analysis\ntool whenever it is required to extract information from the quantitative evaluation of the difference among diverse\ninstances of a complex system.\nWe also provide for analysis the R [46] package nettools including functions to compute the HIM distance. The package is provided as a working beta version and it is accessible on GitHub at https://github.com/filosi/\nnettools.git. To reduce computing time, the software can be used on multicore workstations and on high performance computing (HPC) clusters.\n\n2\n2.1\n\nThe HIM family of distances\nNotations\n\nLet N1 and N2 be two simple networks on N nodes, described by the corresponding adjacency matrices A(1) and A(2) ,\n(1) (2)\nwith aij , aij \u2208 F, where F = F2 = {0, 1} for unweighted graphs and F = [0, 1] for weighted networks. Let then\n\u0012 1 0 *** 0 \u0013\n1 *** 0 , let 1\nIN be the N \u00d7 N identity matrix IN = 0 ***\nN be the N \u00d7 N unitary matrix with all entries equal to\n0 0 *** 1\n\none and let 0N be the N \u00d7 N null matrix with all entries equal to zero. Denote then by EN the empty network with\nN nodes and no links (with adjacency matrix 0N ) and by FN the clique (undirected full network) with N nodes and\nall possible N (N \u2212 1) links, whose adjacency matrix is 1N \u2212 IN . For an undirected network, its adjacency matrix is\nsymmetric. For a directed network N \u2191 , following the convention in [47], a link i \u2192 j is represented by setting aji = 1\nin the corresponding adjacency matrix AN \u2191 , which thus is, in general, not symmetric.\n\u2191\nFor instance, the matrix AN \u2191 = 1N \u2212 IN represents the full directed network FN\n, with all possible N 2 \u2212 N directed\nlinks i \u2192 j.\n\n2\n\n\fIM\n\n0\n\uf8ec1\n\uf8ec\n\uf8ec0\n\uf8ed1\n\n1\n1\n0\n0\n\n0\n\n1\n2\n\n\uf8eb\n2\n\n5\n\n2\n\n3\n\n4\n\n0\n0\n0\n1\n0\n\n1\n2\n\n0\n\n0\n1\n0\n1\n\n0\uf8f7\n1\uf8f8\n0\n\n\uf8f6\n\n1\n\uf8f7\n2\uf8f7\n\nHIM\u03be\n\n1\n\nHIM\n\nH\n\u221e\n\n0 1\n\n(a)\n\n\u03be\n\n(b)\n\n(c)\n\nFigure 1: A five nodes network as a oscillatory system (a) and the corresponding adjacency matrix (b), with\np two differ1\nent edge weights 1 and 21 , represented by different springs. In panel (c), the product metric HIM\u03be = \u221a1+\u03be\nH2 + \u03beIM2\nas a function of \u03be \u2208 [0, \u221e).\n\n2.2\n\nThe Hamming distance\n\nThe Hamming distance is one of the most common dissimilarity measures in coding and string theory, recently used\nalso for (biological) network comparison [32, 33, 35, 34]. Since the Hamming measure basically evaluates the presence/absence of matching links on the two networks being compared, it has a simple expression in terms of the neworks'\nadjacency matrices. This is not the case for many other members of the edit distance family, whose computation is\nknown to be a NP-hard task. The definition of the normalized Hamming distance H is in fact the following:\nH(N1 , N2 ) =\n\nHamming(N1 , N2 )\n1\nHamming(N1 , N2 )\n=\n=\nHamming(EN , FN )\nN (N \u2212 1)\nN (N \u2212 1)\n\nX\n\n(1)\n\n(2)\n\n|Aij \u2212 Aij | ,\n\n(1)\n\n1\u2264i6=j\u2264N\n\nwhere the normalization factor N (N \u2212 1) bound the range of the function H in the interval [0, 1]. The lower bound 0\nis attained only for identical networks A(1) =\u0012A(2) , while\n\u0013 the upper bound 1 is reached whenever the two networks are\ncomplementary A(1) + A(2) = 1N \u2212 IN =\n\n0 1 *** 1\n1 0 *** 1\n***\n1 1 *** 0\n\n. When N1 and N2 are unweighted networks, H(N1 , N2 ) is\n\njust the fraction of different matching links over the total number N (N \u2212 1) of possible links between the two graphs.\n2.3\n\nThe Ipsen-Mikhailov distance\n\nOriginally introduced in [36] as a tool for network reconstruction from its Laplacian spectrum, the definition of the\nIpsen-Mikhailov IM metric follows the dynamical interpretation of an N nodes network as an N molecules system\nconnected by identical elastic strings as in Fig. 1(a-b), where the pattern of connections is defined by the adjacency\nmatrix A of the corresponding network. The dynamical system is described by the set of N differential equations\n\u1e8di +\n\nN\nX\n\nAij (xi \u2212 xj ) = 0 for i = 0, * * * , N \u2212 1 .\n\n(2)\n\nj=1\n\nWe recall that the Laplacian matrix L of an undirected network is defined as the difference between the degree D and\nthe adjacency A matrices L = D \u2212 A, where D is the diagonal matrix with vertex degrees as entries. L is positive\nsemidefinite and singular [48, 49, 50, 51], so its eigenvalues are 0 = \u03bb0 \u2264 \u03bb1 \u2264 * * * \u2264 \u03bbN \u22121 . The vibrational\nfrequencies \u03c9i for the network model in Eq. 2 are given by the square root of the eigenvalues of the Laplacian matrix\nof the network: \u03bbi = \u03c9i2 , with \u03bb0 = \u03c90 = 0. In [48], the Laplacian spectrum is called the vibrational spectrum.\nEstimates (actual and asymptotic) of the eigenvalues distribution are available for complex networks [52], while the\nrelations between the spectral properties and the structure and the dynamics of a network are discussed in [53, 54, 55].\nThe spectral density for a graph as the sum of Lorentz distributions is defined as\n\u03c1(\u03c9, \u03b3) = K\n\nN\n\u22121\nX\ni=1\n\n\u03b3\n,\n(\u03c9 \u2212 \u03c9i )2 + \u03b3 2\n\n3\n\n\fA\n\nE\n\n0.8\n\nII\n\nIII\n\nP(A,E)\n\n0.6\nIM\n\nP(A,F)\n\n0.4\n\n59\n\n2\n\nF\n\n0.\n\n0.5\n\nB\n\nI\n\n0.2\n\nIV\n0.6\n\n0.2\n\nP(A,B)\n\n0.4\n\n0.6\n\n0.8\n\nH\n\nFigure 2: Representation of the HIM distance in the Ipsen-Mikhailov (IM axis) and Hamming (H axis) distance space\nbetween networks A versus B, E and F, where E is the empty network and F is the clique.\nZ\nwhere \u03b3 is the common width and K is the normalization constant defined by the condition\n\n\u221e\n\n\u03c1(\u03c9, \u03b3)d\u03c9 = 1, and\n0\n\nthus\n\n1\n\nK=\n\u03b3\n\nN\n\u22121 Z \u221e\nX\ni=1\n\n0\n\n.\n\nd\u03c9\n(\u03c9 \u2212 \u03c9i )2 + \u03b3 2\n\nThe scale parameter \u03b3 specifies the half-width at half-maximum, which is equal to half the interquartile range. An\nexample of Lorentz distribution for two networks is shown In Fig. 5. Then the spectral distance \u000f\u03b3 between two graphs\nN1 and N2 on N nodes with densities \u03c1N1 (\u03c9, \u03b3) and \u03c1N2 (\u03c9, \u03b3) can then be defined as\nsZ\n\u221e\n2\n\u000f\u03b3 (N1 , N2 ) =\n[\u03c1N1 (\u03c9, \u03b3) \u2212 \u03c1N2 (\u03c9, \u03b3)] d\u03c9 .\n0\n\nThe highest value of \u000f\u03b3 is reached, for each N , when evaluating the distance between EN and FN . Denoting by \u03b3 the\nunique solution of\n\u000f\u03b3 (EN , FN ) = 1 ,\n(3)\nthe normalized Ipsen-Mikahilov distance between two undirected (possibly weighted) networks can be defined as\nsZ\n\u221e\n2\nIM(N1 , N2 ) = \u000f\u03b3 (N1 , N2 ) =\n[\u03c1N1 (\u03c9, \u03b3) \u2212 \u03c1N2 (\u03c9, \u03b3)] d\u03c9 ,\n(4)\n0\n\nso that IM is bounded between 0 and 1, with upper bound attained only for {N1 , N2 } = {EN , FN }. A detailed\ndescription of the uniqueness of the solution of Eq. 3 is described in Appendix A. Isospectral networks (and thus\nalso isomorphic networks) cannot be distinguished by this class of measures, so this is a distance between classes of\nisospectral graphs. Although the number of isospectral networks is negligible for large number of nodes [56], their\nfraction is relevant for smaller networks. The case of directed networks is discussed in a later paragraph.\n2.4\n\nThe HIM distance\n\nConsider now two copies of the space N (N ) of all simple undirected networks on N nodes, and endow the first copy\nwith the Hamming metric H and the second copy with the Ipsen-Mikhailov distance IM. Then the two obtained pairs\nN (N ), H) and (N\nN (N ), IM) are metric spaces. Define now on\n(N\nHIM function\n\u221a their Cartesian product the one-parameter\n1\n, for \u03be \u2208 [0, +\u221e).\nas the L2 (Euclidean) product metric [57] combining H and \u03be* IM, normalized by the factor \u221a1+\u03be\nVia the natural correspondence of the same network in the two spaces, the HIM function becomes a distance on N (N ):\nq\np\n1\n1\nHIM\u03be (N1 , N2 ) = \u221a\n||(H(N1 , N2 ), \u03be * IM(N1 , N2 ))||2 = \u221a\nH2 (N1 , N2 ) + \u03be * IM2 (N1 , N2 ) , (5)\n1+\u03be\n1+\u03be\n4\n\n\fx2\n\nx1\n\n0\n1\n1\n\n0\n0\n0\n\nx3\n\n!\n1\n1\n0\n\nx1O\n\nx1I\n\nx2O\n\nx2I\n\nx3O\n\nx3I\n\nD\u2191\n\n\uf8eb\n0\n\uf8ec0\n\uf8ec\n\uf8ec0\n\uf8ec0\n\uf8ec\n\uf8ed1\n1\n\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n1\n1\n0\n\n0\n0\n1\n0\n0\n0\n\n1\n0\n1\n0\n0\n0\n\n\uf8f6\n1\n0\uf8f7\n\uf8f7\n0\uf8f7\n0\uf8f7\n\uf8f7\n0\uf8f8\n0\n\nD\u0302\u2191\n\nFigure 3: A directed network D\u2191 on three nodes and the equivalent undirected network D\u0302\u2191 on six nodes, together with\ntheir adjacency matrices.\n\nwhere in what follows we will omit the subscript \u03be when it is equal to one. Obviously, HIM0 = H and\nlim HIM\u03be = IM (see Fig. 1(c)); apart from values of \u03be close to the bounds {0, +\u221e} where the prevalence of one\n\u03be\u2192+\u221e\n\nof the factors becomes dominant, the qualitative impact of \u03be is minimal in practice when using HIM\u03be as a distance. In\nwhat follows, when no a priori hypothesis supports unbalancing the metric towards one of the two components, \u03be = 1\nwill be assumed. However, the impact of \u03be is definitely more relevant when HIM\u03be is used to generate a kernel function\nto be used for classification purposes, as we will show in a later section. The metric HIM\u03be (N1 , N2 ) is bounded in the\ninterval [0, 1], with lower bound attained for every couple of identical networks, and upper bound attained only on the\npair (EN , FN ). Moreover, all distances HIM\u03be will be nonzero for non-identical isomorphic/isospectral graphs.\nConsider now the [0, 1] \u00d7 [0, 1] Hamming/Ipsen-Mikhailov (H/IM)\n\u221a space, where a point P has coordinates\n(H(N1 , N2 ), IM(N1 , N2 )), and the distance of P from the origin is 2 * HIM(N1 , N2 ). If we (roughly) split the\nHamming/Ipsen-Mikhailov space into four main zones I,II,III,IV as in Fig. 2, two networks whose distances correspond to a point in zone I are quite close both in terms of matching links and of structure, while those falling in the\nzone III are very different with respect to both measures. Networks corresponding to a point in zone II have many common links, but their structure is rather different (for instance, they have a different number of connected components),\nwhile a point in zone IV indicates two networks with few common links, but with similar structure (e.g., isospectral\nnon-identical graphs). In Fig. 2 we show some examples of points in the Hamming/Ipsen-Mikhailov space.\n2.5\n\nThe directed network case\n\nIn this situation, the connectivity matrices are not symmetric, thus the Laplacian spectrum lies in C. Hence, computing\nthe Ipsen-Mikhailov distance would require extending the Lorentzian distribution to the complex plane. A simpler\nsolution can be obtained by transforming the directed network D\u2191 into an undirected (bipartite) one D\u0302\u2191 , as in [47].\nFor each node xi in D\u2191 , the graph D\u0302\u2191 has two nodes xIi and xO\ni (where I and O stand for In and Out respectively)\nI\n\u2191\n\u2191\nand for each directed link xi \u2212\u2192 xj in D\u2191 there is a link xO\n\u2212\nx\ni\nj in D\u0302 . If the adjacency matrix for D is AD \u2191 , the\n\u0010\n\u0011\nT\n0 AD\u2191\nO\nO\nI\nI\ncorresponding matrix for D\u0302\u2191 is AD\u0302\u2191 = A\n, with respect to the node ordering xO\n1 , x2 , . . . xn , x1 , . . . , xn . An\n0\nD\u2191\n\nexample of the above transformation is shown in Fig. 3. Thus it is possible to define HIM(N1\u2191 , N2\u2191 ) as HIM(N\u03021\u2191 , N\u03022\u2191 )\nafter substituing the normalizing factors \u03b7 and \u03b3 with the corresponding \u03b7 \u2191 and \u03b3 \u2191 derived by imposing the conditions\nHamming(\u00caN , F\u0302N )/\u03b7 \u2191 = 1 and \u000f\u03b3 \u2191 (\u00caN , F\u0302N ) = 1, so that HIM(\u00caN , F\u0302N ) = 1 by using Eq. (5). It is immediate to\ncompute \u03b7\u0304 \u2191 = 2N (N \u2212 1), while \u03b3\u0304 \u2191 can be numerically computed as for \u03b3\u0304: details are given in Appendix B.\n\n3\n\nThe HIM kernel\n\nFollowing [43], a kernel can be naturally derived from a distance by means of a Gaussian (Radial Basis Function) map\n(see also [58]). Thus, given two graphs x and y on the same n nodes and a positive real number \u03b3, the HIM kernel can\nbe defined as\n2\nK(x, y) = e\u2212\u03b3*HIM\u03be (x,y) .\nWhenever a novel kernel is introduced, one has to check whether it is positively defined.\n5\n\n\f2\n3\n\n\uf8eb0 1 0 0 1 0 0 1\uf8f6\nAI 1 =\n\n1\n\uf8ec 00\n\uf8ed1\n0\n0\n1\n\n0\n0\n0\n0\n0\n1\n1\n\n0\n0\n0\n0\n1\n1\n0\n\n0\n0\n0\n1\n1\n0\n0\n\n0\n0\n1\n0\n0\n0\n0\n\n0\n1\n1\n0\n0\n0\n0\n\n1\n1\n0\n0\n0\n0\n1\n\n1\n0\n0\uf8f7\n0\uf8f8\n0\n1\n0\n\n2\n1\n\nAI2 =\n\n0\n\n4\n\n7\n\n5\n\n3\n\n\uf8eb0 1 0 0 0 1 1 0\uf8f6\n1\n\uf8ec 00\n\uf8ed0\n1\n1\n0\n\n0\n0\n0\n1\n1\n0\n0\n\n0\n0\n0\n0\n0\n0\n0\n\n0\n0\n0\n0\n0\n1\n1\n\n1\n0\n0\n0\n0\n0\n0\n\n1\n0\n0\n0\n0\n0\n0\n\n0\n0\n1\n0\n0\n0\n1\n\n0\n0\n1\uf8f7\n0\uf8f8\n0\n1\n0\n\n1\n\n0\n\n4\n\n7\n\n5\n\n6\n\n6\n\nFigure 4: Adjacency matrix and graphical representation of I1 and I2 .\nA function \u03a8 : X \u00d7 X \u2192 R is a kernel of condionally negative type if\n1. \u03a8(x, x) = 0 \u2200x \u2208 X;\n2. \u03a8(x, y) = \u03a8(y, x) \u2200x, y \u2208 X;\nn\nn\nX\nX\n3.\nci cj \u03a8(xi , xj ) \u2264 0 \u2200n \u2208 N, \u2200x1 , . . . , xn \u2208 X, \u2200c1 , . . . , cn \u2208 R such as\nci = 0.\ni,j=1\n\ni=1\n\nA variant of Schoenberg's theorem [59] (proved in [60, 61]) states that\nTheorem 3.1 For a function \u03a8 : X \u00d7 X \u2192 R, the following are equivalent:\n1. \u03a8 is of conditionally negative type;\n2. K(x, y) = e\u2212\u03b3\u03a8(x,y) is a positive semidefinite kernel for all \u03b3 \u2208 R+\n0.\nThe above theorem describes the correspondence between negative-type distances and positive definite kernel, which\nis also equivalent to `22 embeddability [62]. Hence, K(x, y) is a positive semidefinite kernel if and only if HIM2\u03be (x, y)\nis a symmetric function of conditionally negative type. Although the square of many distances are condionally negative\ntype functions, HIM2\u03be (x, y) cannot be proven to be of conditionally negative type (actually, it is probably not of negative\ntype, as it is the case for many edit distances [43, 63, 64, 65, 66], the HIM kernel K is not positively defined in general\nfor all \u03b3 \u2208 R+\n0 . Nevertheless, this problem can be overcomed by using Prop. 1.3.4 in [67] (see also [58, 65]):\nTheorem 3.2 Suppose the data x1 , . . . , xl and the kernel k(*, *) are such that the matrix\nKij = k(xi , xj )\nis positive. Then it is possible to construct a map \u03a6 into a feature space F such that\nk(xi , xj ) = h\u03a6(xi ), \u03a6(xj )i .\nConversely, for a map \u03a6 into some feature space F , the matrix Kii j = h\u03a6(xi ), \u03a6(xj )i is positive.\nNote that Th. 3.2 does not even require x1 , . . . , xl to belong to a vector space. This theorem implies that, even though\nthe kernel is not positive definite, it is still possible to use it in Support Vector Machines or other algorithms requiring\nk to correspond to a dot product in some space if the kernel matrix K is positive for the given training data. This\ncondition can be obtained by choosing a suitable value of \u03b3: in the experiments shown hereafter, the HIM kernel is\nalways positively defined on the given training data, leading to positive definite matrices, and thus posing no difficulties\nfor the SVM optimizer, as in [68].\n\n4\n4.1\n\nApplications\nA minimal example\n\nConsider the two networks I1 , I2 \u2208 N (8) with corresponding adjacency matrices AI1 , AI2 shown in Fig. 4. The\nHamming distance between I1 and I2 is\n\uf8eb0 0 0 0 1 1 1 1\uf8f6\n00001111\nX\nX\n1\n1\n\uf8ec 00 00 00 00 01 11 11 01 \uf8f7 28\nI1\nI2\n|Aij \u2212 Aij | =\n= 0.5 .\nH(I1 , I2 ) =\n\uf8ed1 1 0 1 0 0 0 0\uf8f8 =\nN (N \u2212 1)\n56\n56\n11110000\n1\u2264i6=j\u2264N\n\n1\u2264i6=j\u22648\n\n6\n\n11110000\n11010000\n\n\f0.30\n\n0.30\n\n0.6\n\n4\n\n6\n\n8\n\nIpsen\u2212Mikhailov\n\nHIM(I1, I2)\n\n0.37\n\n0.0\n0\n\n\u03c1I1 (\u03c9, \u03b3)\n\n0.2\n\n0.5\n\n0.00\n\n0.05\n0.00\n\n2\n\n0.3\n\n2\n\n4\n\n6\n\n8\n\n\u03c1I2 (\u03c9, \u03b3)\n\n0.0\n\n0.1\n\n0.2\n\n0.3 0.4\nHamming\n\n0.14\n\n0\n\n0.4\n\n0.1\n\n0.05\n\n0.10\n\n0.10\n\n0.15\n\n0.15\n\n0.20\n\n0.20\n\n0.25\n\n0.25\n\n0.5\n\n0.5\n\n0.6\n\nHIM(I1 , I2 )\n\nFigure 5: Lorentzian distribution of the Laplacian spectra for I1 (left) and I2 (center) with vertical lines indicating\neigenvalues, and HIM(I1 , I2 ) in the Hamming/Ipsen-Mikhailov space (right).\n\nFrom the spectral point of view, the corresponding Laplacian matrices and eigenvalues are\n\uf8eb 3 \u22121 0 0 \u22121 0 0 \u22121 \uf8f6\nI1\n\nL\n\n\u22121 3\n0\n0\n0\n0\n0 \u22121\n\u22121 \u22121\n\uf8eb 3 \u22121\n\u22121 3\n\uf8ec 00 00\n\uf8ec 0 \u22121\n\uf8ed\n\u22121 \u22121\n\u22121 0\n0 0\n\n\uf8ec 0\n\uf8ec 0\n= \uf8ec \u22121\n\uf8ed 0\n\nLI2 =\n\n0 0 0 0 \u22121 \u22121\n2 0 0 \u22121 \u22121 0 \uf8f7\n0 2 \u22121 \u22121 0 0 \uf8f7\n0 \u22121 2 0 0 0 \uf8f7\n\u22121 \u22121 0 2 0 0 \uf8f8\n\u22121 0 0 0 3 \u22121\n0 0 0 0 \u22121 3\n0 0 0 \u22121 \u22121 0 \uf8f6\n0 0 \u22121 \u22121 0 0\n0 0 0 0 0 0 \uf8f7\n0 2 0 0 \u22121 \u22121 \uf8f7\n0 0 1 0 0 0 \uf8f8\n0 0 0 2 0 0\n0 \u22121 0 0 3 \u22121\n0 \u22121 0 0 \u22121 2\n\nspec(LI1 ) = [0, 0.657077, 1, 2.529317, 3, 4, 4, 4.813607]\n\nspec(LI2 ) = [0, 0, 0.340321, 1.145088, 3, 3, 3.854912, 4.659679] .\n\nFrom the above spectra, we can compute the corresponding Lorentz distributions \u03c1I{1,2} (\u03c9, \u03b3), where \u03b3 = 0.4450034:\ntheir plots are shown in Fig. 5.\nThe resulting Ipsen-Mikhailov distance is\nsZ\nIM(I1 , I2 ) =\n\n\u221e\n\n2\n\n[\u03c1I1 (\u03c9, \u03b3) \u2212 \u03c1I2 (\u03c9, \u03b3)] d\u03c9 = 0.1004144 ,\n\n0\n\nso that the HIM distance results\n\u221a\np\n2\nHIM(I1 , I2 ) =\n||(H(I1 , I2 ), IM(I1 , I2 ))||2 \u2248 0.707168 0.52 + 0.10041442 \u2248 0.3606127 .\n2\nThe situation can be graphically represented as in Fig. 5: the two networks are quite different in terms of matching\nlinks, but their structures are not so diverse.\n4.2\n\nSmall networks\nN (N \u22121)\n\nFixed the number of nodes N , there are exactly 2 2\ndifferent simple undirected unweighted networks, which\ncan be grouped into isomorphism classes. As anticipated before, isomorphic graphs cannot be distiguished by spectral\nmetrics, while their mutual Hamming distances are non zero, since their links are in different positions. As an example,\nfor N = 3 there are 8 networks grouped in 4 isomorphism classes, for N = 4 there are 11 isomorphism classes\nincluding a total of 64 graphs and for N = 5 34 classes with 1024 networks (for N = 6, 7, the number of classes is\nrespectively 156 e 1044).\nTo give an overview of a broader situation, we compute a number of mutual distances between networks with a given\nnumber of nodes (all possible couples for N = 3, 4, 5 and a subset of them for N = 15) and we display the results in\nFig. 6. To select a good range of variability for the networks with 15 nodes, we select the empty graph, the full graph\n(with 105 nodes) and 10 different graphs with i edges each, for 1 \u2264 i \u2264 104.\nAs shown by the plots, all possible situations can occur, apart from points in the northwest corner of zone II which\nare the rarest. For instance, the point P (1, 0) in Fig. 6(b) corresponds to 6 different pairs (O1 , O2 ) of networks with 4\n7\n\n\f(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 6: Mutual distances between (a) all 28 couples of networks with 3 nodes, (b) all 2016 couples of networks with\n4 nodes, (c) all 523776 couples of networks with 5 nodes and (d) the 542361 mutual distances between a set of 1042\nnetworks with 15 nodes.\n\nnodes with maximal Hamming distance and minimal spectral distance: as an example, we show one of these pairs in\nFig. 7.\n4.3\n\nComparison with Matthews Correlation Coefficient\n\nWhen assessing performances in a link prediction task (for instance, in the series of DREAM challenges [69, 70, 71]),\nthe standard strategy following the machine learning approach, is to rely on functions of the confusion matrix, i.e., the\ntable collecting the number of correct and wrong predictions with respect to the ground truth. Classical measures of\nthis kind are the pairs Sensitivity/Specificity and Precision/Recall, and the derived Area Under the Curve.\nA reliable alternative is the Matthews Correlation Coefficient (MCC for short) [72], summarizing into a single value the\nconfusion matrix of a binary classification task. This is a measure of common use in the machine learning community\n[73], recently accepted as an effective metric also for network comparison [74, 75]. Also known as the \u03c6-coefficient,\nfor a 2 \u00d7 2 contingency table MCC corresponds to the square root of the average \u03c72 statistic\np\nMCC = \u03c72 /N ,\nwhere N is the total number of observations. In the binary case of two classes positive P and negative N, for the\nFN\nconfusion matrix ( TP\nFP TN ), where T and F stand for true and false respectively, the Matthews Correlation Coefficient\nhas the following shape:\nTP * TN \u2212 FP * FN\nMCC = p\n.\n(TP + FP) (TP + FN) (TN + FP) (TN + FN)\nMCC lives in the range [\u22121, 1], where 1 is perfect classification, \u22121 is reached in the complete misclassification case\nwhile 0 corresponds to coin tossing classification, and it is invariant for scalar multiplication of the whole confusion\nmatrix.\nHere we want to provide a quick comparison of MCC and HIM distances in a few cases. First of all, some considerations on the extreme cases:\n\u2022 HIM(G, H) = 1 only for {G, H} = {EN , FN }, which has MCC = 0.\n\u2022 HIM(G, H) = 0 only for G = H; in this case, MCC(G, H) = 0 for G = H \u2208 {EN , FN }, and\nMCC(G, H) = 1 in all other cases.\n\u2022 MCC(G, H) = 1 only for G = H 6\u2208 {EN , FN }, and thus HIM=0.\n\u2022 The two values MCC = 0 or MCC = \u22121 can correspond to a landscape of quite different pairs of networks,\nfor which the HIM distance can assume very diverse values.\nTo investigate the last case in the above list, we randomly generated 250,000 pairs of networks of different size, and we\ncompared the MCC with the H, IM and HIM distances: the corresponding scatterplots are shown in Fig. 8. Since MCC\n.\nis a similarity measure, for a direct comparison we displayed it as the [0, 1]-normalized dissimilarity measure 1\u2212MCC\n2\nAs predictable, since the confusion matrix is unaware of the network structure but it takes into account only matching\nand mismatching links, the MCC is well correlated with the Hamming distance (Pearson Coefficient PC=0.92) and\npoorly correlated with the Ipsen-Mikhailov distance (PC=0.01), resulting in an good global correlation with the HIM\n8\n\n\fFigure 7: The six pairs of networks on four nodes with Hamming distance one and Ipsen-Mikhailov distance zero.\n\ndistance (PC=0.79). Nonetheless, the plots in Fig. 8 show that the relevant variability of one measure for a given value\nof the other one supports the claim of a strong independency between MCC and HIM. Finally, as an example giving a\nquantitative basis to the last claim of the above list, for all the pairs with MCC = 0 we obtain values of HIM ranging\nin [0.11, 1], with median 0.37 and mean 0.39, while when MCC = \u22121 the range of the HIM values is [0.71, 0.86], with\nmean and median equal to 0.74.\n4.4\n\nDynamical networks\n\nIn what follows we show the evolution of the Hamming, the Ipsen-Mikhailov and the HIM distance during the evolution\nof the following dynamical processes P(i) moving through consecutive steps:\n\u2022 Random Addition PRA (i + 1) is obtained from PRA (i) by randomly adding a not already present link.\n\u2022 Random Removal PRR (i + 1) is obtained from PRR (i) by randomly removing an existing link.\n\u2022 Sequential Addition PSA (i + 1) is obtained from PSA (i) by adding a new link in the same row and in the next\navailable column of the last added link, if possible, or in the following row, starting from the first available\ncolumn. The whole process starts from the first available row with the smallest index. As an example,\nif PSA (0) = E5 , then the\n\u0012 1process\n\u0013evolves inserting ones in the adjacency matrix following the sequence\n23 4\n56 7\n1 \u2192 2 \u2192 3 \u2192 * * * 10 in\n.\n8 9\n10\n\n\u2022 Sequential Removal PSR : as in PSA , but removing one link at each step.\n\u2022 Highest Degree Addition PHDA (i + 1) is obtained from PHDA (i) by adding a previously not existing link\nconnecting the node with the highest degree.\n\u2022 Highest Degree Removal PHDR (i + 1) is obtained from PHDR (i) by removing an existing link connecting the\nnode with the highest degree.\nAs a first example, consider the processes PRA and PSA with the empty graph as starting network PRA (0) = PSA (0) =\nEN . They both end at the N-nodes clique after Nmax = N (N2\u22121) steps: PRA (Nmax ) = PSA (Nmax ) = FN .\nThe corresponding inverse processes PRR and PSR evolve in the opposite direction: PRR (0) = PSR (0) = FN and\nPRR (Nmax ) = PSR (Nmax ) = EN . In Fig. 9 we show the curves of d(P\u25e6 (i), P\u25e6 (0)) for d=H, IM and HIM in the cases\nN =10, 25 and 100 nodes.\nFor the representation of the curves, we use two different spaces: the already introduced Hamming/Ipsen-Mikhailov\nspace, with the metric H on the x axis and the metric IM on the y axis, and the Fraction-of-nodes/HIM space,\nwith the ratio between the number of newly added or removed links over the total number Nmax of links on the\nx\n\u0010 axis and the HIM distance on\n\u0011 the y axis; in this representation, the i-th step P\u25e6 (i) of a process has coordinates\n2i\n,\nHIM\n(P\n(i),\nP\n(0))\n. In all cases, since one edge is removed or added at each step, in both spaces the\n\u25e6\n\u25e6\nN (N \u22121)\nevolution of the processes proceeds from left to right in both graphs, and the trend of the curves representing the distances of the same process in the Hamming/Ipsen-Mikhailov space and in the Fraction-of-nodes/HIM space are similar,\nvarying only for a scaling factor.\nFor the random processes PRA and PRR we show the means of the distances computed on 100 runs; no standard deviation\nor confidence intervals are plotted, because they are negligible at the scale of the plot. For instance, in the case N =25,\nthe order of magnitude of the standard deviation for HIM at each step is 10\u22123 , and the span of the 95% boostrap\nconfidence intervals is in the range of 10\u22124 . As a first observation, all curves are monotonically increasing and the\n9\n\n\f(a)\n\n(b)\n\n(c)\n\nFigure 8: Scatterplot of 1\u2212MCC\nversus Hamming (a), Ipsen-Mikhailov (b) and HIM (c) distances when comparing\n2\n250,000 random pairs of networks of different size 3-100.\nbigger the graph, the larger the distances, but in the empty-to-clique case, where, in the second half of the process,\nPSA induces distances which are smaller than PRA and which are smaller for larger graphs. The most interesting\nobservation is the different shape of the curves between the empty-to-clique process and the clique-to-empty: for the\nsame Hamming distance (or fraction of links), the corresponding Ipsen-Mikhailov (or HIM, respectively) distance\nis larger when the nodes are added rather than removed, because adding links quickly generates degree correlation.\nFurthermore, in the empty-to-clique case, not much difference occurs between the random and the sequential process,\nwhile this difference is much wider (with the random one inducing larger distances) for the clique-to-empty case.\nAn analogous experiment was carried out within the family of Poissonian graphs, with Erd\u00f6s-R\u00e9nyi model [76, 77]\nG(N, p). In particular, for N =10, 25 and 100, let SN be a sparse network G(N, p = 0.05), with 2, 11 and 230 edges\nrespectively and let DN be a dense network G(N, p = 0.9), with 39, 275\nand 4462 edges respectively. Consider the\n\u221a\nN\u221a\n2 N (N \u22121)\nmax\nfollowing four processes, of which we represent the initial b 2 c = b 2 *\nc + 1 steps in Fig. 10:\n2\n\u221a\n\n2 N (N \u22121)\nc, with PRA (0) = SN , for N =10, 25, 100.\n2\n2\n\u221a\n2 N (N \u22121)\nPRR (i), for i = 0, . . . , b 2\nc, with PRR (0) = DN , for N =10, 25, 100.\n2\n\u221a\nPHDA (i), for i = 0, . . . , b 22 N (N2\u22121) c, with PHDA (0) = SN , for N =10, 25, 100.\n\u221a\nPHDR (i), for i = 0, . . . , b 22 N (N2\u22121) c, with PHDR (0) = DN , for N =10, 25, 100.\n\n\u2022 PRA (i), for i = 0, . . . , b\n\u2022\n\u2022\n\u2022\n\n1.0\n\n1.0\n\n0.8\n\n0.8\n\n0.8\n\n0.6\n\n0.4\n\n10, RA\n25, RA\n100, RA\n\n0.6\n\n0.4\n\n10, SA\n25, SA\n100, SA\n\n10, RA\n25, RA\n100, RA\n\n0.2\n\n10, SA\n25, SA\n100, SA\n\n0.2\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\nHamming distance\n\n(a)\n\n0.8\n\n1.0\n\n0.6\n\n0.4\n\n0.2\n\n0.0\n\n0.0\n\nHIM distance\n\n1.0\n\n0.8\n\nIpsen\u2212Mikhailov distance\n\n1.0\n\nHIM distance\n\nIpsen\u2212Mikhailov distance\n\nIn this case, too, results on the random processes are averaged over 100 runs, with negligible confidence intervals.\nTo better highlight the differences of the resulting distances in the various processes, in Fig. 10 (c) and (d) we show\nthe ratio of some pairs of HIM distances as a function of the removed/added links. In particular, in subfigure (c), for\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n% of total links\n\n0.0\n\n0.2\n\n0.4\n\n10, SR\n25, SR\n100, SR\n0.6\n\nHamming distance\n\n(b)\n\n0.4\n\n(c)\n\n0.8\n\n10, SR\n25, SR\n100, SR\n\n10, RR\n25, RR\n100, RR\n\n0.2\n\n10, RR\n25, RR\n100, RR\n\n0.0\n\n0.0\n\n0.6\n\n0.0\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n% of total links\n\n(d)\n\nFigure 9: Distances between P\u25e6 (i) and P\u25e6 (0) for the processes evolving from the empty network to the clique (a and\nb) or vice versa (c and d), in the Hamming/Ipsen-Mikhailov space (a and c) or plot of the HIM distance as a function\nof the ratio of added/removed links (b and d), for N = 10 (black), 25 (blue), 100 (red) nodes. Solid lines denote the\naverage of distances for 100 runs of random evolution, while dashed lines denote the sequential processes PSA and PSR .\nIn all cases, the process evolves from the left-bottom corner to the right-top corner.\n10\n\n0.8\n\n1.0\n\n\f0.8\n\n3.0\n\n0.8\n\n0.2\n\n10, RA\n25, RA\n100, RA\n\n0.4\n\n10, RR\n25, RR\n100, RR\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n1.5\n\n1.0\n\n10, HDR\n25, HDR\n100, HDR\n\n10\n\n0.0\n\n0.0\n\n25\n\n100\n\n0.5\n\n0.0\n\n0.2\n\n0.4\n\nHamming distance\n\nHamming distance\n\n(a)\n\n(b)\n\n0.6\n\n0.8\n\n0.5\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\nRatio of changing links\n\n(c)\n\n1.0\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\nRatio of changing links\n\n(d)\n\nFigure 10: Plot of H, IM and HIM distances between P\u25e6 (i) and P\u25e6 (0) for \u25e6 = RA, HDA (a) and \u25e6 = RR, HDR (b), for\nN = 10 (black), 25 (blue), 100 (red) nodes. Solid lines denote the average of distances for 100 runs of PRA and PRR ,\nwhile dashed lines identify PHDA and PHDR . In all cases, the process evolves from the left-bottom corner to the right-top\nHIM(PRA (i),PRA (0))\ni\nRA (i),P RA (0))\ncorner. In panel (c), plot of HIM(P\nas a function of Nmax\nand, in panel (d), plot of HIM(P\nHIM(PRR (i),PRR (0)) as a\nHDA (i),P HDA (0))\ni\nfunction of Nmax\n.\neach step i, we show the quotient of HIM distances for PRA over PHDA , in the three cases N =10, 25 and 100. The\nthree curves show that HIM distances for PRA are larger than the HIM distances for PHDA for N =25 and 100, and their\ndifference is higher in the first steps of the process i < 0.3Nmax , while they tend to get closer as far as the processes\nevolve. In the other cases PRR and PHDR (not shown here), the differences are smaller and they converge faster to one,\nbut in this case the process PHDR accounts for the smaller values of HIM distances. In the plot (c) of Fig. 10, we\ni\nRA (i),P RA (0))\nshow the curves for HIM(P\nHIM(PRR (i),PRR (0)) as a function of Nmax . All the three curves are monotonically decreasing and\nconverging to one after the first stages of the processes, yielding that, for all values of N , adding links produces higher\nvalues of HIM distance. In the case of the evolution targeting higher degree nodes first (not shown here), the trend is\nthe same, only scaled down to smaller ratios.\nThe final examples consider processes having scale free networks as starting graphs. For N =10, 25 and 100, let SS N\nbe a scale free sparse network generated following the Albert-Barabasi model [78], with power law exponent 2.3 and\nwith 9, 24 and 99 edges rispectively, and SDN a dense network with the same exponent 2.3 but with 35,\n300 and 4150\n\u221a\n2 N (N \u22121)\nedges respectively. The same four processes of the previous case were tested for the initial b N\u221amax\nc\n=\nb\nc+1\n2 *\n2\n2\nsteps:\n\u221a\n\n2 N (N \u22121)\nc, with PRA (0) = SSN , for N =10, 25, 100.\n2\n2\n\u221a\nPRR (i), for i = 0, . . . , b 22 N (N2\u22121) c, with PRR (0) = SDN , for N =10, 25, 100.\n\u221a\nPHDA (i), for i = 0, . . . , b 22 N (N2\u22121) c, with PHDA (0) = SSN , for N =10, 25, 100.\n\u221a\nPHDR (i), for i = 0, . . . , b 22 N (N2\u22121) c, with PHDR (0) = SDN , for N =10, 25, 100.\n\n\u2022 PRA (i), for i = 0, . . . , b\n\u2022\n\u2022\n\u2022\n\nThe corresponding curves are plotted in Fig. 11 (a) and (b). We recall here that scalefree networks are not invariant for\npercolation, i.e., they do not remain scalefree when links are randomly removed or added. However, the evolution of\nthe processes is not very different from the Erd\u00f6s-R\u00e9nyi case, especially for the processes removing links as shown in\nFig. 11, panel (b). Some differences emerge for the processes adding links, and a few peculiarities that are also present\nin the Poissonian case here become more evident. In particular, for all N and for both PRA and PHDA the derivative of\nthe curves are larger than those in panel (b), and it is not true anymore that the larger the number of nodes, the larger\nthe distances. For instance, in the case N = 100, both the processes quickly modify the network structure, resulting\nin a fast increment of the Ipsen-Mikhailov distance for i < 0.2Nmax , while later the curves grow at a much smaller\nrate. To better study this behaviour, a larger starting network SB200 was generated following the scale free model in\n[79], with 200 nodes and 1000 edges, power law exponent 2.001 and degree distribution as in the histogram of Fig. 11,\npanel (c). The following processes were started from SB200 , and they were carried on until they reach either the empty\nnetwork or the clique:\n\u2022\n\u2022\n\u2022\n\u2022\n\n100\n\n2.0\n\n0.2\n\n10, HDA\n25, HDA\n100, HDA\n\n0.0\n\nRatio of HIM distances\n\n0.4\n\n25\n\n0.6\n\nRatio of HIM distances\n\nIpsen\u2212Mikhailov distance\n\nIpsen\u2212Mikhailov distance\n\n0.6\n\n10\n\n2.5\n\n1.5\n\nPRA (i), with PRA (0) = SB 200 and PRA (18900) = F200 .\nPHDA (i), with PHDA (0) = SB 200 and PHDA (18900) = F200 .\nPRR (i), with PRR (0) = SB 200 and PRR (1000) = E200 .\nPHDR (i), with PHDR (0) = SB 200 and PHDR (1000) = E200 .\n11\n\n0.8\n\n1.0\n\n\f0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.8\n\n0.2\n\n10, RA\n25, RA\n100, RA\n\n0.4\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n10\n\n0.4\n\n0.2\n\n10, RR\n25, RR\n100, RR\n\n5\n\n10, HDR\n25, HDR\n100, HDR\n\n0\n\n0.0\n\n0.0\n\n15\n\n0.2\n\n10, HDA\n25, HDA\n100, HDA\n\n0.0\n\nRA\nHDA\nRR\nHDR\n\n0.6\n\nHIM distance\n\n0.4\n\n20\nNumber of Nodes\n\nIpsen\u2212Mikhailov distance\n\nIpsen\u2212Mikhailov distance\n\n25\n\n0.0\n\n0.2\n\n0.4\n\nHamming distance\n\nHamming distance\n\n(a)\n\n(b)\n\n0.6\n\n0.8\n\n0.0\n\n20\n\n40\n\n60\n80\nNode Degree\n\n(c)\n\n100\n\n120\n\n0\n\n20\n\n40\n\n60\n\n% of process progress\n\n(d)\n\nFigure 11: Plot of Hamming versus Ipsen-Mikhailov distance for P\u25e6 (i) and P\u25e6 (0) = SB200 for \u25e6 = RA, HDA (a) and\n\u25e6 = RR, HDR (b), for N = 10 (black), 25 (blue), 100 (red) nodes. Solid lines denote the average of distances for 100\nruns of PRA and PRR , while dashed lines identify PHDA and PHDR . In all cases, the process evolves from the left-bottom\ncorner to the right-top corner. (c) Histogram of the node degrees of the network SB 200 . (d) Hamming distances versus\npercentage of processes steps for PRA , PHDA , PRR and PHDR .\nThe curves corresponding to HIM distances from SB200 in the four aforementioned processes are plotted in Fig. 11,\npanel (d), versus the percentage of progress of the process, i.e., 100 * Ni\u25e6 , with 0 \u2264 i \u2264 N\u25e6 and NRA = NHDA = 18900,\nNRR = NHDR = 1000. The HIM distance for the processes PRR and PHDR are monotonically and similarly increasing\nwhen evolving from SB200 to E200 , slower at the beginning and much faster in the last steps of the process. The\ntwo other processes instead show the same effect previously noted: HIM(PRA (i), SB200 ) and HIM(PHDAA (i), SB200 )\nchange rapidly in the initial 10% of the processes, yielding a fast increase in the Ipsen-Mikhailov distance, due to the\nquick modification in the network structure. After this initial period, the growth of both curves proceed with a smaller\nderivatives until they reach their maximum at the end of the process.\n4.5\n\nGraph families\n\nIn this section we investigate the distribution of the distances from the empty network of a set of graphs randomly\nextracted from five families. In particular, for each N = 10, 20, 50, 100 and 1000 we extracted 1000 networks on N\nnodes from each of the following class of graphs:\n\u2022 BA Barabasi-Albert model [78], with power of preferential attachment extracted from the uniform distribution\nbetween 0.1 and 10.\n\u2022 ER Erd\u00f6s-R\u00e9nyi model [76, 77], with link probability extracted from the uniform distribution between 0.1\nand 0.9.\n\u2022 WS Watts-Strogatz model [80], with neighborhood within which the vertices of the lattice will be connected\nuniformly sampled in {1, . . . , 10} and rewiring probability extracted from the uniform distribution between\n0.1 and 0.9.\n\u2022 PL Scale-free random graphs from vertex fitness scores [79], with number of edges uniformly sampled between 1 and N (N2\u22121) and power law exponent of the degree distribution extracted from the uniform distribution\nbetween 2.005 and 3.\n\u2022 KR Random regular graphs, with all possible values of node degree.\nIn Tab. 2 we list mean \u03bc and standard deviation \u03c3 of HIM(\u25e6, EN ) for all combinations of node size and network\ntype: note that we do not report the corresponding median, because its distance from the mean \u03bc is always smaller\nthan 0.02 nor the bootstrap confidence intervals, whose range is always smaller than 0.02 from either side of the\nmean. In Fig. 12 we also show the corresponding boxplots, while in Fig. 13(a) we display the scatterplot in the\nHamming/Ipsen-Mikhailov space of all the aforementioned distances. In the Hamming/Ipsen-Mikhailov space all the\nBA nets are confined in the narrow rectangle [0, 0.2] \u00d7 [0.6, 9.75], while all other classes of graphs span a much wider\narea. In particular, the points corresponding to distances of the PL nets occupy densely all the upper left triangle of\nthe H/IM plane, and the same happens, with H > 0.1, also for the ER networks, while WS and KR points lie in\nthe upper rectangle [0, 1] \u00d7 [0.6, 1]. Thus, different PL networks show very different stucture, while the BA nets are\nvery homogeneous. Notably, no point occurs in the lower right corner of the H/IM space. Moreover, in average, the\nstandard deviation decreases inversely with the network size, showing larger homogeneity in bigger networks. Finally,\nto better highlight the difference among the diverse families, we randomly extracted 100 networks with 100 nodes\nfor the four families BA, ER, WS and PL and we computed the mutual distances between all possible pairs of these\n12\n\n80\n\n100\n\n\fBA\n\n1000\n\n1000\n\n100\n\n100\n\n50\n\n50\n\n20\n\n20\n\n10\n0.4\n\n0.6\n\n0.8\n\n1000\n\n1000\n\n100\n\n100\n\n50\n\n50\n\n20\n\nER\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n20\n\n10\n\nKR\n\n10\n0.2\n\nWS\n\nPL\n\n10\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1000\n\n1000\n\n100\n\n100\n\n50\n\n50\n\n20\n\n20\n\n10\n\nAll\n\n10\n0.2\n\n0.4\n\n0.6\n\n0.8\n\nFigure 12: Boxplots of the HIM(\u25e6, EN ) for all combinations of node size and network type.\n400 graphs. A few statistics of these HIM distances are reported in Tab. 1, while the planar multidimensional scaling\nplot [81] is displayed in Fig. 13(b). Apart from the PL networks, the three families BA, ER and WS can be mutually\nwell separated as shown in the multidimensional plot; moreover, the graphs in the BA and in the WS families are\nmutually quite similar, as supported by the small interclass mean HIM distance. On the other side, the PL networks\nhave essentially the same distance from all other groups, so they cannot be easily distiguished.\n4.6\n\nThe D. melanogaster development dataset\n\nIn [82], the authors used the Keller algorithm to infer the gene regulatory networks of Drosophila melanogaster from\na time series of gene expression data measured during its full life cycle, originally published in [83]. They followed\nthe dynamics of 588 development genes along 66 time points spanning through four different stages (Embryonic \u2013\ntime points 1-30, Larval \u2013 t.p. 31-40, Pupal \u2013 t.p. 41-58, Adult \u2013 t.p. 59-66), constructing a time series of inferred\nnetworks Ni , publicly available at http://cogito-b.ml.cmu.edu/keller/downloads.html. Hereafter\nwe evaluate the structural differences between Ni and the initial network N1 , as measured by the HIM distance: the\nresulting plot is displayed in Fig. 14. The largest variations, both between consecutive terms and with respect to the\n13\n\n\f0.3\n1.0\n\nIpsen\u2212Mikhailov distance\n\nBA\nER\nWS\nPL\n\n0.2\n\n0.8\n\n0.6\n\n10\n20\n50\n100\n1000\n\n0.4\n\n0.1\n\nBA\nER\nWS\nPL\nKR\n\n0.0\n\n0.2\n\n\u22120.1\n\n0.0\n\u22120.2\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\u22120.2\n\nHamming distance\n\n0.0\n\n(a)\n\n0.2\n\n0.4\n\n(b)\n\nFigure 13: (a) Scatterplot in the Hamming/Ipsen-Mikhailov space of the HIM(\u25e6, EN ) for all combinations of node size\nand network type; (b) Multidimensional Scaling of the mutual HIM distances of 400 networks with 100 nodes in the\nBA, ER, WS and PL families.\n\ninitial network N1 , occur in the embrional stage (E): in particular, the HIM distance grows until time points 23, then\nthe following networks start getting closer again to N1 , showing that the interactions of the selected 588 genes in the\nadult stage are more similar to the corresponding net of interaction in the embrional stage, rather than in the other two\nstages. Moreover, while Hamming distance ranges between 0 and 0.0223, the Ipsen-Mikhailov distance has 0.0851 as\nits maximum, indicating an higher variability of the networks in terms of structure rather than matching links. Finally,\nusing a Support Vector Machine with HIM kernel built in the kernlab package in R, a 5-fold Cross Validation with\n\u03b3 = 103 and C = 1 reached accuracy 0.97 in discriminating Embryonic and Adult networks from Larval and Pupal,\nwhile, in the same setup, we reach perfect separation between Embryonic and Adult stages for all values of \u03b3 larger\nthan 1000.\n\nTable 1: Mean \u03bc and standard deviation (\u03c3) of the HIM distances between the 100-nodes graphs in the four families\nBA, ER, WS and PL; each family includes 100 networks.\n\nBA\nER\nWS\nPL\n\nBA\n\nER\n\nWS\n\nPL\n\n0.05 (0.06)\n\n0.69 (0.10)\n0.50 (0.13)\n\n0.47 (0.13)\n0.56 (0.15)\n0.29 (0.12)\n\n0.65 (0.17)\n0.51 (0.14)\n0.56 (0.18)\n0.50 (0.17)\n\nTable 2: Mean \u03bc and standard deviation \u03c3 of HIM distances HIM(\u25e6, EN ) from the empty network for all combinations\nof network type T and network size N, and cumulatively across node sizes and graph classes.\n\u2192N\n\u2193T\nBA\nER\nWS\nPL\nKR\nAll\n\n10\n\u03bc\n0.53\n0.69\n0.91\n0.72\n0.60\n0.69\n\n20\n\u03c3\n0.01\n0.18\n0.14\n0.19\n0.11\n0.19\n\n\u03bc\n0.51\n0.73\n0.76\n0.72\n0.54\n0.65\n\n50\n\u03c3\n0.01\n0.12\n0.15\n0.19\n0.10\n0.17\n\n\u03bc\n0.50\n0.77\n0.64\n0.75\n0.50\n0.63\n\n\u03c3\n0.02\n0.09\n0.08\n0.15\n0.05\n0.15\n\n14\n\n100\n\u03bc\n\u03c3\n0.49 0.02\n0.77 0.08\n0.62 0.06\n0.74 0.14\n0.49 0.00\n0.62 0.14\n\n1000\n\u03bc\n\u03c3\n0.50 0.02\n0.76 0.08\n0.62 0.05\n0.72 0.11\n0.48 0.00\n0.62 0.13\n\nAll\n\u03bc\n\u03c3\n0.51 0.02\n0.74 0.12\n0.71 0.16\n0.73 0.16\n0.52 0.08\n0.64 0.16\n\n\f0.06\n23\n\n0.07\n\n32\n\n43\n\n0.08\n\n0.05\n\n0.06\n\nHIM\n\n0.04\n\nIM\n\nIM\n\n0.06\n\n0.04\n\n66\n\n0.03\n\n0.05\n\n0.02\n0.02\n\n0.01\n\n0.04\n\n0.00\n\nE\n\n66\n\n2\n0.01\n\n0.02\n\n0.03\n\n0.016\n\n0.018\n\n0.020\n\nH\n\nH\n\n(a)\n\n(b)\n\n0.022\n\n0.024\n\n10\n\n20\n\nL\n30\n\nP\n40\n\n50\n\nA\n60\n\nTime Points\n\n(c)\n\nFigure 14: (a) Evolution of distances of the D. melanogaster development gene network time series in the\nHamming/Ipsen-Mikhailov space with zoom (b) on the final timepoints and (c) evolution of same HIM distances\nalong 66 time points in the 4 stages Embryonic (E), Larval (L), Pupal (P) and Adult (A).\n\n4.7\n\nThe HCC dataset\n\nPublicly available at the Gene Expression Omnibus (GEO) http://www.ncbi.nlm.nih.gov/geo, at the Accession Number GSE6857, the HepatoCellular Carcinoma (HCC) dataset [84, 85] collects 482 tissue samples from 241\npatients affected by HCC, a well-studied pathology [86, 87] where the impact of microRNA (miRNA) is notably relevant [88, 89]. For each patient, a sample from cancerous hepatic tissue and a sample from surrounding non-cancerous\nhepatic tissue are available, hybridized on the Ohio State University CCC MicroRNA Microarray Version 2.0 platform\ncollecting the signals of 11,520 probes of 250 non-redundant human and 200 mouse miRNA. After a preprocessing\nphase including imputation of missing values [90] and discarding probes corresponding to non-human (mouse and\ncontrols) miRNA, we consider the dataset HCC of 240+240 paired samples described by 210 human miRNA, with\nthe cohort consisting of 210 male and 30 female patients. We thus parted the whole dataset HCC into four subsets\ncombining the sex and disease status phenotypes, collecting respectively the cancer tissue for the male patients (MT),\nthe cancer tissue for the female patients (FT) and the corresponding two datasets including the non cancer tissues\n(MnT, FnT). Then we first generated the four co-expression networks on the 210 miRNA as vertices, inferred via absolute Pearson's correlation and corresponding to the combinations of the two binary phenotypes, and we computed\nall mutual HIM distances. In particular, to show the possible effects due to the different sample size, we computed 30\ninstances of the MT and MnT networks, inferred using only 30 matching samples and then averaging all the mutual\nHIM distances. One instance of MT and MnT is displayed as an hairball in Fruchterman-Reingold layout [91] together\nwith the nets FT and FnT. The corresponding two-dimensional scaling plot [81] in the right panel of the Figure 15.\nThe four networks are widely separated, with orthogonal separations for the two phenotypes, but the values of the HIM\ndistances between the network support the known different development of HCC in male and female: for instance, the\nFT network is closer to the MnT net (HIM=0.08), rather than to the MT and FnT (HIM=0.13 and 0.16, respectively).\nNote that the largest distance (HIM=0.23) is detected between the two non-tumoral networks MnT, FnT. An expanded\nversion of the example is shown in [92, 93], where more networks are generated from the same dataset using different\ninference algorithms and a stability analysis is performed.\n4.8\n\nThe Gulf Dataset\n\nPart of the Kansas Event Data System, available at http://vlado.fmf.uni-lj.si/pub/networks/data/\nKEDS/, the Gulf Dataset collects, on a monthly bases, political events between pairs of countries focusing on the Gulf\nregion and the Arabian peninsula for the period 15 April 1979 to 31 March 1999, for a total of 240 months. Political\nevents belong to 66 classes (including for instance \"pessimist comment\", \"meet\", \"formal protest\", \"military engagement\", etc.) and involve 202 countries. This dataset formally translates into a time series of 240 unweighted and\nundirected graphs with 202 nodes, for which we computed all the mutual 240*239\nHIM distances. These distances are\n2\nthen used to project the 240 networks on a plane through a multidimensional scaling [81]: the resulting plot is displayed\nin Fig. 16. The months corresponding to the First Gulf War months (July 1990 - April 1991) are close together and\nconfined in the lower left corner of the plane, showing both a mutual high degree of homogeneity and, at the same time,\na relevant difference to the graphs of all other months. This shows that, at the onset of the conflict, the diplomatic relations worldwide changed consistently and their structure remained very similar throughout the whole event. Note that\n15\n\n\f0.06\n0.04\n\nMT\n\nMnT\n\n0.02\n\ng\n\n0.084\n0.163\n\n\u22120.04\n\na\n\nFnT\n\n0.23\n\nFnT\n\nFT\n\u22120.10\n\nFT\n\nMnT\n\n0.128\n\n0.00\n\na\n\n\u22120.02\n\nMT\n\n0.13\n0.144\n\n\u22120.05\n\n0.00\n\n0.05\n\n0.10\n\ng\n\nFigure 15: (left) Fruchterman-Reingold layout of the networks MT, MnT, FT, FnT where the male networks are inferred\nby one random extraction of 30 samples out of the whole cohort of 210 patients. Node size is proportional to node\ndegree. (right) Multidimensional scaling plot, with mutual HIM distances, of the networks FT, FnT, MT, MnT.\n\nthe blue point (closer to the war-like period) corresponds to February 1998, the time of Iraq disarmament crisis: Iraqi\nPresident Saddam Hussein negotiates a deal with U.N. Secretary General Kofi Annan, allowing weapons inspectors to\nreturn to Baghdad, preventing military action by the United States and Britain.\n4.9\n\nThe International Trade Network data\n\nAs an application of the HIM distance on directed and weighted networks, we show four examples based on the\nInternational Trade Network (ITN) data, version 4.1, by Gledisch [94] available at http://privatewww.essex.\nac.uk/ksg/exptradegdp.html, collecting estimates of trade flows between independent states (1948-2000)\nand GDP per capita of independent states (1950-2000). As noted by [95], due to differences in reporting procedures\nbetween countries, incongruences occur between exports from i to j and imports from i to j: to avoid this issues, in\nour analysis we only use the figures reported as export in the dataset.\nIn what follows, we extract four sets of countries, and we study the evolution of their trade subnetworks during the\naforementioned period. In each example, chosen the set of N countries C1 , . . . CN , we construct, for every year, the\nweighted directed network having C1 , . . . CN as nodes. A link between country Ci and country Cj represents the\nexport from Ci to Cj , and its weight wij corresponds to the volume of the export flow. Then we compute all mutual\nHIM distances among these networks, first rescaling link weights in the unit interval. Finally, using these N (N2\u22121)\nHIM distances we construct a planar classical Multidimensional Scaling plot, transforming the networks in a set of\npoints such that the distances between the points are approximately equal to the mutual HIM dissimilarities, using the\nmethods in [96, 97, 98, 81] as implemented in R. The aim here is to connect the structural changes in yearly trade\nnetworks with time periods and events having a role in explaing such changes. Note that in [95], the authors show\nthat bilateral trade fulfills fluctuation-response theorem [99], stating that the average relative change in import (export)\nbetween two countries is a sum of relative changes in their GDPs. This result yields that directed connections, i.e.,\nbilateral trade volumes, are only characterized by the product of the trading countries GDPs.\nAs a first example we present the BRICS countries case. Introduced in 2001, the acronym BRICS collects the five\nnations Brazil, Russia, India, China and South Africa (Fig. 17(a)) which, although developing or newly industrialized\ncountries, are distinguished by their large and fast-growing economies and by their significant influence on regional\nand global affairs. To such aim, in Fig. 17 we show the bidimensional scaling of their trade networks for the years\n1950\u20132000, with the HIM matrix as the distance constraint. As shown by the plot, three groups of years can be clearly\ndivided, thus yielding that the corresponding networks are similar within each group, but diverse across different\n16\n\n\fgroups: the early years recovering after WWII (until about 1963), the seventies and eighties, where the economies of\nthe involved countries started to develop, and the nineties, where their growth begun to accelerate.\nA very similar situation occurs in the regional trade network among the South American countries (Fig. 18), where\nthe global behaviour is essentially controlled by the two local giants Brazil and Argentina, and for which the larger\ndifferences between the nets can be appreciated between the economic growth of the 90s and the suffering economies\nin the late 70s / early 80s due to the struggling political situations.\nNot much different is the case of the larger trade subnetworks of the top 20 world economies ranked by Gross Domestic\nProduct 2012 (PPP) (Top20 for short) as listed by the World Bank http://data.worldbank.org and shown in\nFig. 19, with the notable difference that the networks for the 60s are more homogeneous to those of the 70s and\n80s, supporting a faster recovery of these economies after WWII than the BRICS or the South American countries.\nAgain, the 90s are remarkably separated by the previous periods, as a consequence of the fact that economic growth\nfor high-income countries such as the United States, Japan, Singapore, Hong Kong, Taiwan, South Korea and Western\nEurope was steady and coupled with \"an unprecedented extension and intensification of globalization in terms of the\ninternational integration of capital and product markets\" [100], thus causing a structural evolution of the trade networks\nfor these countries, whose economies account for approximately 85% of the gross world product (GWP), 80 percent\nof world trade (including EU intra-trade), and two-thirds of the world population.\nWe conclude with a more local example: between 1975 and 1990, the civil war heavily damaged Lebanon's economic\ninfrastructure, reducing the role of the country as the major West Asian banking hub. The following period of relative\npeace stimulated economic recovery also through an increasing flow of manufactured and farm exports. In this last\nexample we consider the trading network W between Lebanon and its three major economic partners, Saudi Arabia,\nKuwait and United Arab Emirates. In Fig. 20 we show 4 examples of the trade networks with the Lebanon export\nfigures. In the bidimensional scaling plot of Fig. 21 the trajectory emerges of the evolution of the W graphs across the\ndifferent decades 50s, 60s, 70s, 80 and 90s, even more clearly than in the previous cases. Here the rightmost points\nin the plot, corresponding to the years 1977\u20131990, in the middle of the civil war in Lebanon, where a contraction of\nthe trading flow was recorded. Finally, in the plot of Fig. 22, we show the relation between the volume of export\nflow of Lebanon and the curve representing the HIM distance of Wi from W1950 for i \u2208 {1951, . . . , 2000}. Pearson\ncorrelation between the two curves is 0.71, and their shape shows that the trade network is following the trend of the\nother curve with a temporal shift of about a decade.\n\n0.02\n\n0.00\n\n\u22120.02\n\nOther periods\n\u22120.04\n\nFirst Gulf War\nIraq Disarmament Crisis\n\n\u22120.05\n\n0.00\n\n0.05\n\n0.10\n\nFigure 16: Planar HIM distance based multidimensional scaling plot of the monthly Gulf Dataset. Red dots corresponds\nto the First Gulf War months (July 1990 - April 1991), while grey points correspond to months outside that temporal\nwindow and the blue point corresponds to February 1998, the month of the Iraq disarmament crisis.\n17\n\n\f0.10\n\n98\n00\n\n5960\n58 61\n\n62\n\n64 6590 89\n66\n\nRUS\n\nIND\n\nCHN\n\n86\n88\n\n73\n67\n76\n68\n82 77\n7475\n80\n81\n\nSAF\n\u22120.2\n\n\u22120.1\n\n0.0\n\n(a)\n\n87\n85\n\n72\n71\n69\n\n95\n\n91\n\n63\n\n70\nBRA\n\n94\n96\n\n93\n92\n\n\u22120.05\n\n0.00\n\n0.05\n\n54\n53\n55\n52\n56\n5051\n57\n\n78\n84\n79\n83\n0.1\n\n0.2\n\n(b)\n\n0.03\n\nFigure 17: (a) Maps and flags of the BRICS countries. (b) Multidimensional Scaling of the HIM distances among the\nintertrade networks of the BRICS countries in the periods 1950\u20131963 (black), 1964\u20131990 (blue), 1991\u20132000 (red).\n\n9692\n95\n9897 99\n00 93 94\n\n0.02\n\n73\n\n0.01\n\n71\n70\n\n0.00\n\n49 6261\n\n\u22120.03\n\nCOL\n\nVEN\n\nGUY\n\nSUR\n\nECU\n\nBRA\n\nBOL\n\nPAR\n\nCHI\n\nARG\n\nURU\n\n80\n79\n838275\n78\n81\n\n64\n\n85\n76 74\n\n60\n52 5756\n51\n58 59 54\n55\n\n\u22120.02\n\n\u22120.01\n\n50\n\n89\n87\n84 88\n91 86\n\n68\n66\n63 67\n\n65\n\n77\n\n53\n\u22120.10\n\nPAN\n\n72\n\n90\n\n69\n\n48\n\n\u22120.05\n\n0.00\n\n0.05\n\nPER\n\nFigure 18: Maps (left) and flags (bottom) of the Latin America countries. (right) Multidimensional Scaling of the\nHIM distances among the intertrade networks of the countries in the Latin America in the periods 1948\u20131959 (black),\n1960\u20131969 (green), 1970\u20131990 (blue), 1991\u20132000 (red).\n\n18\n\n97 99\n\n0.10\n\n\f0.015\n\n97\n\n0.010\n\n00\n99\n54\n\n0.000\n\n0.005\n\n52 50\n55 57\n58\n53\n56\n51\n\n8485\n\n\u22120.005\n\n86\n\n82\n81\n89 75\n63\n87\n8874\n65 6476\n92\n91\n77\n78\n90\n80\n73\n79\n\n66\n67\n\n68\n\n\u22120.010\n\n95\n94\n93\n\n62\n\n69\n71\n\n\u22120.015\n\u22120.020\n\n61\n60 83\n\n59\n\n49\n\n96\n98\n\n72\n70\n\n48\n\u22120.06\n\n\u22120.04\n\n\u22120.02\n\n1 USA\n\n2 CHN\n\n3 IND\n\n4 JPN\n\n5 RUS\n\n6 GER\n\n7 FRA\n\n8 BRA\n\n9 UK\n\n10 MEX\n\n11 ITA\n\n12 KOR\n\n13 SPA\n\n14 CAN\n\n15 TUR\n\n16 IDN\n\n17 AUS\n\n18 SAU\n\n19 POL\n\n20 NED\n\n0.00\n\n0.02\n\nFigure 19: (left) Maps and flags (bottom) of the Top20 countries: in green, the top-10 economies, in orange the\ncountries ranking 11\u201320. (right) Multidimensional Scaling of the HIM distances among the intertrade networks of the\nTop20 countries in the periods 1948\u20131959 (black), 1960\u20131992 (blue), 1993\u20132000 (red).\n4.10\n\nThe MEG Biomag 2010 competition 1 dataset\n\nThe challenge dataset for the 2010 Biomag competition was derived from [101] and consisted in monitoring 4 subjects\nby a MEG in a set of trials where a fixation cross was presented to the subject and after that at regular intervals a\ncue indicated which direction, either left or right, they had to covertly attend to during the next 2500ms. After this\nperiod, a target in the indicated direction appeared. Brain activity was recorded from 500ms before cue offset to\n2500ms after cue offset through 274 sensors at 300Hz; a total of 128 trials per condition were collected, 256 total trials\nper subject. MEG data were first preprocessed as explained in [101]: the raw signals of each trial are independently\ndecomposed with a multitaper frequency transformation in the 5-40 Hz interval with 2 Hz bin width. The results of the\nfrequency transforms are used to construct a coherence network for each trial, which is successively rescaled such that\nits eigenvalues are between +1 and -1. After rescaling, on a separated instance of the dataset, the eigenvalues of the\nnetwork are subjected to a network deconvolution procedure as explained in [102]. Finally, an Elastic Net [103] linear\nregression using the Lasso [104] in two phases with the mixed `1`2 algorithm [105, 106], resulting in a final dataset of\n\n1951\n\n1974\n\n1985\n\n1996\n\nFigure 20: The trade network between Lebanon, Saudi Arabia, Kuwait and United Arab Emirates in 1951, 1974, 1985\nand 1996; red links indicate Lebanon export flow, with the corresponding volume figure. Edge width is proportional to\nexport flow volume.\n19\n\n0.04\n\n0.06\n\n\f0.15\n\n81\n\n0.05\n\n0.10\n\n80\n\nKUW\n\nUAE\n\nLEB\n\n53\n55\n54\n57\n51\n58\n52\n56\n59\n60\n\n62\n\n85\n\n86\n\n77\n\n75\n89 88\n\n61\n87\n\n\u22120.10\n\n\u22120.05\n\n0.00\n\nSAU\n\n84\n\n74\n72\n79\n73 78\n76\n\n66 70\n64 67 69\n68 71\n6365\n\n83\n\n82\n\n90\n\n\u22120.15\n\n95\n97 9193949900\n96\n98\n92\n\u22120.2\n\n\u22120.1\n\n0.0\n\n0.1\n\n0.2\n\nFigure 21: (left) Maps and flags of the countries in the Lebanon trade net . (top right) Multidimensional Scaling of\nthe HIM distances among the intertrade networks of the Lebanon trade net countries in the periods 1950\u20131961 (black),\n1962\u20131971 (green), 1971\u20131981 (gray), 1982\u20131990 (blue), 1991\u20132000 (red).\n\nTable 3: Average Classification Accuracies for Deconvolved and non Deconvolved 11 Hz Networks, standard errors\nin brackets. As a baseline, authors in [109] reach 0.67 accuracy using the Elastic Net with summary statistics of\nspatio-temporal activations and 0.73 using 2-D DCT basis.\nL-SVM\nHIM0 -SVM\nHIM+\u221e -SVM\nHIM-SVM\nRF\nEN\n\nNon-Deconvolved\n0.65 (0.02)\n0.67 (0.03)\n0.56 (0.04)\n0.61 (0.04)\n0.71 (0.01)\n0.71 (0.03)\n\nDeconvolved\n0.72 (0.02)\n0.74 (0.03)\n0.48 (0.05)\n0.63 (0.04)\n0.70 (0.02)\n0.74 (0.03)\n\n252 covariance networks on 274 nodes, equally distributed between label \"right\" and \"left\". A suite of Support Vector\nMachines from mlpy http://mlpy.fbk.eu [107] with different HIM\u03be kernels were tested, together with the linear\nkernel L-SVM, Random Forest RF [108] and Elastic-Net EN as a baseline, on a set of 100 MonteCarlo resampling of\nstratified training (84+84 networks) and test (42+42) sets of both the deconvolved and the original dataset, yielding the\nperformance shonw in Tab. 3.\n20\n\n\f1200\n(0.45)\n\nExport\nMillion USD\n\nHIM(Wi, W1950)\n\n800\n(0.3)\n400\n(0.15)\n0\n1950\n\n1960\n\n1970\n\n1980\n\n1990\n\n2000\n\nFigure 22: Lebanon export flow in the years 1950\u20132000 (Million of USD, solid multicolor line left y axis) and curve\nof HIM(Wi , W1950 ) (dashed orange line).\n\nAs a general consideration, the deconvolution procedure helps improving classification. The better accuracy is reached\nby the kernel with only the Hamming component (HIM0 ), which performs better of baseline methods, while the IpsenMikhailov component (HIM+\u221e ) performs very poorly, on both versions of the datasets. All intermediate values of\n\u03be (including \u03be = 1 reported in the table) gives decreasing performance for increasing values of \u03be, implying that the\ntopological features of the graph are not useful for classification in this task, maybe because of the symmetric nature\nof the task.\nThe obtained results of this off-the-shelf method are comparable with the range of performances obtained by far more\ncomplex and properly targeted approaches [110, 111, 109], representing a promising starting point for an effective use\nof the HIM kernel, for instance coupled with other graph kernels or some feature selection techniques. An extended\nversion of this example can be found at [112].\n\n5\n\nConclusions\n\nWe introduced HIM\u03be , a novel family of distances between graphs with same nodes, even directed and weighted,\naimed at combining the local and global aspects of the comparison between networks, i.e., the difference between\nmatching vertices and the difference between the spectral structure. After unveiling definitions and properties, we\nprovided a range of applications in several fields, from functional genomics to economics, to show the usefulness\nof the proposed solution. In particular, we underlined the effectiveness of the HIM metrics when used as a kernel\nfunctions for classification purposes, e.g., in Support Vector Machines, applied to heterogenous data in diverse areas.\nA final comment on the computational feasibility: the costly part when computing the HIM distance is the extraction\nof the spectrum from the Laplacian matrices of the two compared graph. This task is both CPU intensive and requiring\na fair amount of RAM, but allows for a wide parallelization: nonetheless, huge graphs should be dealt with HPC\nfacilities. As an example, the size of the largest graphs we compared (using a Python implementation making use of\nthe NumPy library) is about 40,000 nodes: on a workstation with 48 Intel Xeon CPU E5649 at 2.53GHz and powered\nby 48Gb RAM we were able to run 4 parallel processes which took about 36 hours to compute the mutual distances\nbetween a set of 45 networks, for a total of 990 comparisons.\n21\n\n\fReferences\n[1] A.-L. Barab\u00e1si. The network takeover. Nature Physics, 8:14\u201316, 2012.\n[2] R. Sharan and T. Ideker. Modeling cellular machinery through biological network comparison. Nature Biotechnology, 24(4):427\u2013433, 2006.\n[3] T. Ideker and N.J. Krogan. Differential network biology. Molecular Systems Biology, 8:565, 2012.\n[4] B.-J. Yoon, X. Qian, and S.M.E. Sahraeian. Comparative Analysis of Biological Networks. IEEE Signal Processing Magazine, 29(1):22\u201334, 2012.\n[5] P. Csermely, T. Korcsm\u00e1ros, H.J.M. Kiss, G. London, and R. Nussinov. Structure and dynamics of biological\nnetworks: a novel paradigm of drug discovery. A comprehensive review. Pharmacology and Therapeutics,\n138:333\u2013408, 2013.\n[6] H.-Y. Chuang, E. Lee, Y.-T. Liu, D. Lee, and T. Ideker. Network-based classification of breast cancer metastasis.\nMolecular Systems Biology, 3:140, 2007.\n[7] B. Yang, J. Zhang, Y. Yin, and Y. Zhang. Network-Based Inference Framework for Identifying Cancer Genes\nfrom Gene Expression Data. BioMed Research International, 2013:Article ID 401649, 2013.\n[8] G. Pavlopoulos, M. Secrier, C. Moschopoulos, T. Soldatos, S. Kossida, J. Aerts, R. Schneider, and P. Bagos.\nUsing graph theory to analyze biological networks. BioData Mining, 4(1):10, 2011.\n[9] A. Barla, G. Jurman, R. Visintainer, M. Squillario, M. Filosi, S. Riccadonna, and C. Furlanello. A Machine\nLearning Pipeline for Discriminant Pathways Identification. In E. Biganzoli, A. Vellido, F. Ambrogi, and\nR. Tagliaferri, editors, Computational Intelligence Methods for Bioinformatics and Biostatistics, volume 7548\nof Lecture Notes in Computer Science, pages 36\u201348. Springer, 2012.\n[10] A. Barla, G. Jurman, R. Visintainer, M. Squillario, M. Filosi, S. Riccadonna, and C. Furlanello. A Machine\nLearning Pipeline for Discriminant Pathways Identification. In N.K. Kasabov, editor, Springer Handbook of\nBio-/Neuroinformatics, chapter 53, page 1200. Springer, Berlin, 2013.\n[11] Y. Xiao, H. Dong, W. Wu, M. Xiong, W. Wang, and B. Shi. Structure-based Graph Distance Measures of High\nDegree of Precision. Pattern Recognition, 41(12):3547\u20133561, 2008.\n[12] M. Dehmer and A. Mowshowitz. The Discrimination Power of Structural SuperIndices. Plos ONE, 8(7):e70551,\n2013.\n[13] R.C. Entringer, D.E. Jackson, and D.A. Snyder. Distance in graphs. Czechoslovak Mathematical Journal,\n26(2):283\u2013296, 1976.\n[14] L. Zhu, W.K. Ng, and S. Han. Classifying graphs using theoretical metrics: A study of feasibility. In J. Xu,\nG. Yu, S. Zhou, and R. Unland, editors, Database Systems for Adanced Applications, volume 6637 of Lecture\nNotes in Computer Science, pages 53\u201364. Springer, 2011.\n[15] S. Aliakbary, S. Motallebi, J. Habibi, and A. Movaghar. Learning an Integrated Distance Metric for Comparing\nStructure of Complex Networks. arXiv:1307.3626v1 [cs.SI], 2013.\n[16] Z. Chen. Discovery of Informative and Predictive Patterns in Dynamic Networks of Complex Systems. PhD\nthesis, North Carolina State University, 2012.\n[17] L. Chen, J. Xuan, R. Riggins, R. Clarke, and Y. Wang. Identifying cancer biomarkers by network-constrained\nsupport vector machines. BMC Systems Biology, 5(1):161, 2011.\n[18] D.R. Thorat and S.S. Sonawane. A survey of graph classification approaches. In Proceedings of International\nConference on Computer Science and Information Technology ICSIT 2013, pages 169\u2013178. IRNet Explore,\n2013.\n[19] P. Mah\u00e9, N. Ueda, T. Akutsu, J.-L. Perret, and J.-P. Vert. Extensions of marginalized graph kernels. In Proceedings of the 21 International Conference on Machine learning ICML '04, page 70. ACM, 2004.\n[20] T. G\u00e4rtner, Q.V. Le, and A.J. Smola. A Short Tour of Kernel Methods for Graphs. Technical report, Fraunhofer\nIAIS and National ICT Australia, 2006.\n[21] T. G\u00e4rtner, T. Horvath, Q.V. Le, A.J. Smola, and S. Wrobel. Kernel methods for graphs. In D.J. Cook and L.B.\nHolder, editors, Mining Graph Data, page 500. Wiley, 2007.\n[22] K.M. Borgwardt. Graph kernels. PhD thesis, Ludwig Maximilians Universitaet Muenchen, 2007.\n[23] N.S. Ketkar, L.B. Holder, and D.J. Cook. Empirical comparison of graph classification algorithms. In IEEE\nSymposium on Computational Intelligence and Data Mining CIDM '09, pages 259\u2013266. IEEE, 2009.\n22\n\n\f[24] S.V.N. Vishwanathan, N.N. Schraudolph, R. Kondor, and K.M. Borgwardt. Graph Kernels. Journal of Machine\nLearning Research, 11:1201\u20131242, 2010.\n[25] K. Tsuda and H. Saigo. Graph Classification. In C.C. Aggarwal and H. Wang, editors, Managing and Mining\nGraph Data, volume 40 of Advances in Database Systems, pages 337\u2013363. Springer, 2010.\n[26] J.-P. Vert and Y. Yamanishi. Supervised graph inference. In L.K. Saul, Y. Weiss, and L. Bottou, editors, Advances\nin Neural Information Processing Systems 17 NIPS 2004, pages 1433\u20131440. MIT Press, 2005.\n[27] J.-P. Vert and M. Kanehisa. Graph-driven features extraction from microarray data using diffusion kernels and\nkernel CCA. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing\nSystems 15 NIPS 2002, pages 1425\u20131432. MIT Press, 2003.\n[28] N. Shervashidze, P. Schweitzer, E.J. van Leeuwen, M. Kurt, and K.M. Borgwardt. Weisfeiler-lehman graph\nkernels. Journal of Machine Learning Research, 12:2539\u20132561, 2011.\n[29] B. Jie, D. Zhang, W. Gao, Q. Wang, C.-Y. Wee, and D. Shen. Integration of Network Topological and Connectivity Properties for Neuroimaging Classification. IEEE Transactions on Biomedical Engineering, in press.\n[30] J. Richiardi, S. Achard, H. Bunke, and D. Van De Ville. Machine Learning with Brain Graphs: Predictive\nModeling Approaches for Functional Imaging in Systems Neuroscience. IEEE Signal Processing Magazine,\n30(3):58\u201370, 2013.\n[31] L. Su, L. Wang, H. Shen, G. Feng, and D. Hu. Discriminative analysis of non-linear brain connectivity in\nschizophrenia: an fMRI Study. Frontiers in Human Neurosciences, 7:702, 2013.\n[32] E.R. Dougherty. Validation of gene regulatory networks: scientific and inferential. Briefings in Bioinformatics,\n12(3):245\u2013252, 2010.\n[33] K. Tun, P. Dhar, M. Palumbo, and A. Giuliani. Metabolic pathways variability and sequence/networks comparisons. BMC Bioinformatics, 7(1):24, 2006.\n[34] K. Iwayama, Y. Hirata, K. Takahashi, K. Watanabe, K. Aihara, and H. Suzuki. Characterizing global evolutions\nof complex systems via intermediate network representations. Nature Scientific Report, 2:srep00423, 2012.\n[35] M. Morris, M.S. Handcock, and D.R. Hunter. Specification of Exponential-Family Random Graph Models:\nTerms and Computational Aspects. Journal of Statistical Software, 24(4):1\u201324, 2008.\n[36] M. Ipsen and A.S. Mikhailov. Evolutionary reconstruction of networks. Physics Review E, 66(4):046109, 2002.\n[37] K. Rajendran and I.G. Kevrekidis. Analysis of data in the form of graphs. arXiv:1306.3524v1 [physics.data-an],\n2013.\n[38] G. Jurman, R. Visintainer, and C. Furlanello. An introduction to spectral distances in networks. Frontiers in\nArtificial Intelligence and Applications, 226:227\u2013234, 2011.\n[39] M. Kivel\u00e4, A. Arenas, M. Barthelemy, J.P. Gleeson, Y. Moreno, and M.A. Porter. Multilayer Networks.\narXiv:1309.7233v1 [physics.soc-ph], 2013.\n[40] M. De Domenico, A. Sol\u00e9-Ribalta, E. Cozzo, M. Kivel\u00e4, Y. Moreno, M.A. Porter, S. G\u00f3mez, and A. Arenas.\nMathematical Formulation of Multi-Layer Networks. arXiv:1307.4977v1 [physics.soc-ph], 2013.\n[41] A. Sol\u00e9-Ribalta, M. De Domenico, N.E. Kouvaris, A. D\u0131\u0301az-Guilera, S. G\u00f3mez, and A. Arenas. Spectral properties of the Laplacian of multiplex networks. arXiv:1307.2090v1 [physics.soc-ph], 2013.\n[42] R.J. S\u00e1nchez-Garc\u0131\u0301a, E. Cozzo, and Y. Moreno. Dimensionality reduction and spectral properties of multiplex\nnetworks. arXiv:1311.1759v1 [physics.soc-ph], 2013.\n[43] C. Cortes, P. Haffner, and M. Mohri. Positive Definite Rational Kernels. In Proceedings of The 16th Annual\nConference on Computational Learning Theory COLT 2003, pages 41\u201356. Springer, 2003.\n[44] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004.\n[45] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. `p -Norm Multiple Kernel Learning. Journal of Machine\nLearning Research, 12:953\u2013997, 2011.\n[46] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2013.\n[47] Y.-Y. Liu, J.-J. Slotine, and A.-L. Barab\u00e1si. Controllability of complex networks. Nature, 473(7346):167\u2013173,\n2011.\n[48] F. Chung. Spectral Graph Theory. American Mathematical Society, 1997.\n[49] D.A. Spielman. Spectral Graph Theory: The Laplacian (Lecture 2). Lecture notes, 2009.\n23\n\n\f[50] R. T\u00f6njes and B. Blasius. Perturbation Analysis of Complete Synchronization in Networks of Phase Oscillators.\narXiv:0908.3365, 2009.\n[51] F.M. Atay, T. B\u0131y\u0131ko\u011flu, and J. Jost. Network synchronization: Spectral versus statistical properties. Physica D\nNonlinear Phenomena, 224:35\u201341, 2006.\n[52] G.J. Rodgers, K. Austin, B. Kahng, and D. Kim. Eigenvalue spectra of complex networks. Journal of Physics\nA: Mathematical and General, 38(43):9431, 2005.\n[53] J. Jost and M.P. Joy. Evolving Networks with distance preferences. Phys. Rev. E, 66:036126, 2002.\n[54] J. Jost. Dynamical Networks. In J. Feng, J. Jost, and M. Qian, editors, Networks: From Biology to Theory, pages\n35\u201364. Springer-Verlag, 2007.\n[55] J.A. Almendral and A. D\u0131\u0301az-Guilera. Dynamical and spectral properties of complex networks. New J. Phys.,\n9:187, 2007.\n[56] W.H. Haemers and E. Spence. Enumeration of cospectral graphs. Eur. J. Comb., 25(2):199\u2013211, 2004.\n[57] E. Deza. Encyclopedia of Distances. Springer Verlag, 2009.\n[58] M. Bolla. Spectral Clustering and Biclustering: Learning Large Graphs and Contingency Tables. Wiley, 2013.\n[59] I.J. Schoenberg. Metric spaces and positive defined functions. Transactions of the American Mathematical\nSociety, 44(3):522\u2013536, 1938.\n[60] P. Ressel. A short proof of Schoenberg's theorem. Proceedings of the American Mathematical Society, 57(1):66\u2013\n68, 1976.\n[61] B. Bekka, P. de la Harpe, and A. Valette. Kazhdan's property (T), volume 11 of New Mathematical Monographs.\nCambridge University Press, 2008.\n[62] C. Berg, J.P.R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups: Theory of Positive Definite and\nRelated Functions, volume 100 of Graduate Texts in Mathematics. Springer, 1984.\n[63] A. Martins. Generative kernels. Technical report, Instituto Superior Tcnico, UTL Lisbon, 2006.\n[64] M. Neuhaus and H. Bunke. Bridging the Gap between Graph Edit Distance and Kernel Machines, volume 68\nof Series in Machine Perception and Artificial Intelligence. World Scientific, 2007.\n[65] H. Li and T. Jiang. A Class of Edit Kernels for SVMs to Predict Translation Initiation Sites in Eukaryotic\nmRNAs. Journal of Computational Biology, 12(6):702\u2013718, 2005.\n[66] M. Cuturi. Positive Definite Kernels in Machine Learning. arXiv:0911.5367v2 [stat.ML], 2009.\n[67] B. Sch\u00f6lkopf. Support Vector Learning. Oldenbourg, Muenchen, 1997.\n[68] S. Sonnenburg, G. R\u00e4tsch, and B. Sch\u00f6lkopf. Large Scale Genomic Sequence SVM Classifiers. In Proceedings\nof the 22nd International Conference on Machine Learning ICML '05, pages 848\u2013855. ACM, 2005.\n[69] G. Stolovitzky, D. Monroe, and A. Califano. Dialogue on Reverse-Engineering Assessment and Methods: The\nDREAM of High-Throughput Pathway Inference. Annals of the New York Academy of Sciences, 1115:11\u201322,\n2007.\n[70] D. Marbach, R.J. Prill, T. Schaffter, C. Mattiussi, D. Floreano, and G. Stolovitzky. Revealing strengths and\nweaknesses of methods for gene network inference. PNAS, 107(14):6286\u20136291, 2010.\n[71] R.J. Prill, D. Marbach, J. Saez-Rodriguez, P.K. Sorger, L.G. Alexopoulos, X. Xue, N.D. Clarke, G. AltanBonnet, and G. Stolovitzky. Towards a Rigorous Assessment of Systems Biology Models: The DREAM3\nChallenges. PLoS ONE, 5(2):e9202, 02 2010.\n[72] B.W. Matthews. Comparison of the predicted and observed secondary structure of T4 phage lysozyme. Biochimica et Biophysica Acta - Protein Structure, 405(2):442\u2013451, 1975.\n[73] P. Baldi, S. Brunak, Y. Chauvin, C.A.F. Andersen, and H. Nielsen. Assessing the accuracy of prediction algorithms for classification: an overview. Bioinformatics, 16(5):412\u2013424, 2000.\n[74] J. Supper, C. Spieth, and A. Zell. Reconstructing Linear Gene Regulatory Networks. In E. Marchiori, J.H.\nMoore, and J.C. Rajapakse, editors, Proceedings of the 5th European Conference on Evolutionary Computation,\nMachine Learning and Data Mining in Bioinformatics, EvoBIO2007, LNCS 4447, pages 270\u2013279. SpringerVerlag, 2007.\n[75] D. Stokic, R. Hanel, and S. Thurner. A fast and efficient gene-network reconstruction method from multiple\nover-expression experiments. BMC Bioinformatics, 10(1):253, 2009.\n[76] P. Erd\u00f6s and A. R\u00e9nyi. On Random Graphs. I. Publicationes Mathematicae, 6:290\u2013297, 1959.\n24\n\n\f[77] P. Erd\u00f6s and A. R\u00e9nyi. On the evolution of random graphs. Publications of the Mathematical Institute of the\nHungarian Academy of Sciences, 5:17\u201361, 1960.\n[78] A.-L. Barabasi and R. Albert. Emergence of scaling in random networks. Science, 286:509\u2013512, 1999.\n[79] K.-I. Goh, B. Kahng, and D. Kim. Universal behaviour of load distribution in scale-free networks. Physical\nReview Letters, 87(27):278701, 2001.\n[80] D.J. Watts and S.H. Strogatz. Collective dynamics of small world networks. Nature, 393:440\u2013442, 1998.\n[81] T.F. Cox and M.A.A. Cox. Multidimensional Scaling. Chapman and Hall, 2001.\n[82] M. Kolar, L. Song, A. Ahmed, and E.P. Xing. Estimating time-varying networks. Ann. Appl. Stat., 4(1):94\u2013123,\n2010.\n[83] M. Arbeitman, E. Furlong, F. Imam, E. Johnson, B. Null, B. Baker, M. Krasnow, M. Scott, R. Davis, and\nK. White. Gene expression during the life cycle of Drosophila melanogaster. Science, 297:2270\u20132275, 2002.\n[84] A. Budhu, H.-L. Jia, M. Forgues, C.-G. Liu, D. Goldstein, A. Lam, K. A. Zanetti, Q.-H. Ye, L.-X. Qin, C. M.\nCroce, Z.-Y. Tang, and X. W. Wang. Identification of Metastasis-Related MicroRNAs in Hepatocellular Carcinoma. Hepatology, 47(3):897\u2013907, 2008.\n[85] J. Ji, J. Shi, A. Budhu, Z. Yu, M. Forgues, S. Roessler, S. Ambs, Y. Chen, P.S. Meltzer, C.M. Croce, L.-X. Qin,\nK. Man, C.-M. Lo, J. Lee, I.O.L. Ng, J. Fan, Z.-Y. Tang, H.-C. Sun, and X.W. Wang. MicroRNA Expression,\nSurvival, and Response to Interferon in Liver Cancer. New England Journal of Medicine, 361:1437\u20131447, 2009.\n[86] P. T.-Y. Law and N. Wong. Emerging roles of microRNA in the intracellular signaling networks of hepatocellular\ncarcinoma. Journal of Gastroenterology and Hepatology, 26(3):437\u2013449, 2011.\n[87] Z. Gu, C. Zhang, and J. Wang. Gene regulation is governed by a core network in hepatocellular carcinoma.\nBMC Systems Biology, 6(1):32, 2012.\n[88] S. Volinia, M. Galasso, S. Costinean, L. Tagliavini, G. Gamberoni, A. Drusco, J. Marchesini, N. Mascellani,\nM.E. Sana, R. Abu Jarour, C. Desponts, M. Teitell, R. Baffa, R. Aqeilan, M.V. Iorio, C. Taccioli, R. Garzon,\nG. Di Leva, M. Fabbri, M. Catozzi, M. Previati, S. Ambs, T. Palumbo, M. Garofalo, A. Veronese, A. Bottoni,\nP. Gasparini, C.C. Harris, R. Visone, Y. Pekarsky, A. de la Chapelle, M. Bloomston, M. Dillhoff, L.Z. Rassenti,\nT.J. Kipps, K. Huebner, F. Pichiorri, D. Lenze, S. Cairo, M.-A. Buendia, P. Pineau, A. Dejean, N. Zanesi,\nS. Rossi, G.A. Calin, C.-G. Liu, J. Palatini, M. Negrini, A. Vecchione, A. Rosenberg, and C.M. Croce. Reprogramming of miRNA networks in cancer and leukemia. Genome Research, 20(5):589\u2013599, 2010.\n[89] S. Bandyopadhyay, R. Mitra, U. Maulik, and M.Q. Zhang. Development of the human cancer microRNA\nnetwork. Silence, 1:6, 2010.\n[90] O.G. Troyanskaya, M. Cantor, G. Sherlock, P.O. Brown, T. Hastie, R. Tibshirani, D. Botstein, and R.B. Altman.\nMissing value estimation methods for DNA microarrays. Bioinformatics, 17(6):520\u2013525, 2001.\n[91] T.M.J. Fruchterman and E.M. Reingold. Graph Drawing by Force-directed Placement. Software - Practice and\nExperience, 21(11):1129\u20131164, 1991.\n[92] G. Jurman, M. Filosi, R. Visintainer, S. Riccadonna, and C. Furlanello. Stability Indicators in Network Reconstruction. arXiv:1209.1654v1 [q-bio.MN], 2012.\n[93] M. Filosi, R. Visintainer, S. Riccadonna, G. Jurman, and C. Furlanello. Stability Indicators in Network Reconstruction. submitted, 2013.\n[94] K.S. Gleditsch. Expanded Trade and GDP Data. Journal of Conflict Resolution, 46:712\u2013724, 2002.\n[95] A. Fronczak and P. Fronczak. Statistical mechanics of the international trade network. Physical Review E,\n85:056113, 2012.\n[96] J.C. Gower. Some distance properties of latent root and vector methods used in multivariate analysis. Biometrika,\n53:325\u2013328, 1966.\n[97] K.V. Mardia. Some properties of classical Multidimensional Scaling. Communications on Statistics Theory and\nMethods, A7:1233\u20131241, 1978.\n[98] F. Cailliez. The analytical solution of the additive constant problem. Psychometrika, 48:343\u2013349, 1983.\n[99] A. Fronczak, P. Fronczak, and J.A. Ho\u0142yst. Fluctuation-dissipation relations in complex networks. Physical\nReview E, 73:016108, 2006.\n[100] N. Crafts. The World Economy In The 1990s: A Long Run Perspective. In P.W. Rhode and G. Toniolo, editors,\nThe Global Economy in the 1990s: A Long-Run Perspective. Cambridgre Academic Press, 2006.\n25\n\n\f[101] van Gerven M. and Jensen O. Attention modulations of posterior alpha as a control signal for two-dimensional\nbrain-computer interfaces. Journal of Neuroscience Methods, 179(1):78\u201384, 2009.\n[102] S. Feizi, D. Marbach, M. M\u00e9dard, and M. Kellis. Network deconvolution as a general method to distinguish\ndirect dependencies in networks. Nature Biotechnology, 31:726\u2013733, 2013.\n[103] H. Zou and T. Hastie. Regularization and Variable Selection Via the Elastic Net. Journal of the Royal Statistical\nSociety: Series B, 67(2):301\u2013320, 2005.\n[104] R. Tibshirani. Regression Shrinkage and Selection Via the Lasso. Journal of the Royal Statistical Society. Series\nB, 58(1):267\u2013288, 1996.\n[105] C. De Mol, E. De Vito, and L. Rosasco. Elastic-Net regularization in learning theory. Journal of Complexity,\n25(2):201\u2013230, 2009.\n[106] S. Mosci, L. Rosasco, M. Santoro, A. Verri, and S. Villa. Solving structured sparsity regularization with proximal\nmethods. In Machine Learning and Knowledge Discovery in Databases, pages 418\u2013433. Springer, 2010.\n[107] D. Albanese, R. Visintainer, S. Merler, S. Riccadonna, G. Jurman, and C. Furlanello. mlpy: Machine Learning\nPython. arXiv:1202.6548 [cs.MS], 2012.\n[108] L. Breiman. Random Forests. Machine Learning, 45(1):5\u201332, 2001.\n[109] S.M. Kia, E. Olivetti, and P. Avesani. Discrete Cosine Transform for MEG Signal Decoding. In Proceedings of\nthe International Workshop on Pattern Recognition in Neuroimaging PRNI, pages 132\u2013135. IEEE, 2013.\n[110] A. Bahramisharif, M. Van Gerven, T. Heskes, and O. Jensen. Covert attention allows for continuous control of\nbrain\u2013computer interfaces. European Journal of Neuroscience, 31(8):1501\u20131508, 2010.\n[111] M. Signoretto, E. Olivetti, L. De Lathauwer, and J.A K Suykens. Classification of Multichannel Signals With\nCumulant-Based Kernels. IEEE Transactions on Signal Processing, 60(5):2304\u20132314, 2012.\n[112] T. Furlanello, M. Cristoforetti, C. Furlanello, and G. Jurman. Sparse Predictive Structure of Deconvolved Functional Brain Networks. arXiv:1310.6547 [q-bio.NC], 2013.\n\nA\n\nUniqueness of \u03b3\n\nFix the number N of nodes, and consider the two extremal networks EN and FN , whose Laplacian spectrum is respectively\nspec(L(EN )) = (0, * * * , 0) and spec(L(FN )) = (0, N, * * * , N ) ,\n| {z }\n| {z }\nN\n\nso that \u03c9i = 0 for the empty network and \u03c9i =\n\n\u221a\n\nN \u22121\n\nN for the fully connected network, for i = 1, . . . , N \u2212 1.\n\nThe Lorentz distribution for the empty network is thus\n\u03c1EN (\u03c9, \u03b3) = K\n\nN\n\u22121\nX\ni=1\n\n\u03b3\n\u03b3 2 + (\u03c9 \u2212 \u03c9i )2\n\nK\u03b3(N \u2212 1)\n=\n,\n\u03b3 2 + \u03c92\nwhere K can be computed as\n1\n\u03b3(N \u2212 1)\nd\u03c9\n\u03b3 2 + \u03c92\n0\n1\n=\nh\n\u0010 \u0011i+\u221e\n(N \u2212 1) arctan \u03c9\u03b3\n\nK=Z\n\n+\u221e\n\n0\n\n1\n\n= \u03c0\n(N \u2212 1)\n2\n2\n=\n,\n(N \u2212 1)\u03c0\n26\n\n\fso that\nK\u03b3(N \u2212 1)\n\u03b3 2 + \u03c92\n2\u03b3\n=\n.\n\u03c0(\u03b3 2 + \u03c9 2 )\n\n\u03c1EN (\u03c9, \u03b3) =\n\nFor the fully connected network we have\n\u03c1FN (\u03c9, \u03b3) = K\n\nN\n\u22121\nX\ni=1\n\n=K\n\nN\n\u22121\nX\ni=1\n\n=\n\n\u03b3\n\u03b3 2 + (\u03c9 \u2212 \u03c9i )2\n\n\u03b32\n\n\u03b32\n\n\u03b3\n\u221a\n+ (\u03c9 \u2212 N )2\n\n\u03b3K(N \u2212 1)\n\u221a ,\n+ \u03c9 2 + N \u2212 2\u03c9 N\n\nwhere K is\n1\n\nK=\n\n+\u221e\n\nZ\n\u03b3(N \u2212 1)\n\n\u03b32\n\n0\n\n+\n\n\u03c92\n\nd\u03c9\n\u221a\n+ N \u2212 2\u03c9 N\n\n1\n\n=\n\u03b3(N \u22121)\n\u03b3\n\nh\n\u0010 \u221a \u0011i+\u221e\narctan \u03c9\u2212\u03b3 N\n0\n\n1\n\n=\n(N \u2212 1)\n\n\u0010\n\n\u03c0\n2\n\n+ arctan\n\n\u0010 \u221a \u0011\u0011 ,\nN\n\u03b3\n\nso that\n\u03b3K(N \u2212 1)\n\u221a\n+ \u03c9 2 + N \u2212 2\u03c9 N\n\u03b3(N \u2212 1)\n1\n\u0010\n\u0010 \u221a \u0011\u0011 *\n\u221a\n=\n2\n\u03c0\nN\n\u03b3 + \u03c9 2 + N \u2212 2\u03c9 N\n(N \u2212 1) 2 + arctan \u03b3\n\u03b3\n\u0010 \u221a \u0011\u0011 \u0010\n=\u0010\n\u221a \u0011.\n\u03c0\nN\n\u03b3 2 + \u03c9 2 + N \u2212 2\u03c9 N\n2 + arctan\n\u03b3\n\n\u03c1FN (\u03c9, \u03b3) =\n\n\u03b32\n\nThus, we expand Eq. 3 as follows:\n1 = \u000f\u03b3 (EN , FN )\nsZ\n\u221e\n2\n=\n(\u03c1EN (\u03c9, \u03b3) \u2212 \u03c1FN (\u03c9, \u03b3)) d\u03c9\n0\n\nv\nu\nuZ\nu\n=t\n\n\u221e\n\nsZ\n\n\u221e\n\n0\n\n=\n\n\uf8f62\n2\u03b3\n\u03b3\n\uf8ed\n\u0010 \u221a \u0011\u0011 \u0010\n\u2212\u0010\n\u221a \u0011 \uf8f8 d\u03c9\n\u03c0\nN\n\u03c0(\u03b3 2 + \u03c9 2 )\n2 + \u03c9 2 + N \u2212 2\u03c9 N\n+\narctan\n\u03b3\n2\n\u03b3\n\uf8eb\n\nZ\n\n2\n\n\u221e\n2\n\nZ\n\nB d\u03c9 \u2212 2\n\nA d\u03c9 +\n0\n\n0\n\n\u221e\n\nABd\u03c9 ,\n0\n\nwhere\nA=\n\n2\u03b3\n\u03c0(\u03b3 2 + \u03c9 2 )\n\nB=\u0010\n\n\u03c0\n2\n\n\u03b3\n\u0010 \u221a \u0011\u0011 \u0010\n\u221a \u0011.\nN\n+ arctan \u03b3\n\u03b3 2 + \u03c9 2 + N \u2212 2\u03c9 N\n27\n\n(6)\n\n\fThe three terms in Eq. 6 can be expanded as follows:\nZ\n\n+\u221e\n\nZ\n\n2\n\n+\u221e\n\n\u0012\n\nA d\u03c9 =\n0\n\n0\n\n=\n\n4\u03b3 2\n\u03c02\n\nZ\n0\n\n2\u03b3\n\u03c0(\u03b3 2 + \u03c9 2 )\n\n+\u221e\n\n(\u03b3 2\n\n\u00132\nd\u03c9\n\nd\u03c9\n+ \u03c9 2 )2\n\n\u0014\n\u0012 \u0013\u0015+\u221e\n4\u03b3 1\n\u03b3\u03c9\n\u03c9\n+\narctan\n\u03c0 2 2\u03b3 3 \u03b3 2 + \u03c9 2\n\u03b3 0\n2 h\u03c0i\n=\n\u03b3\u03c0 2 2\n1\n=\n;\n\u03c0\u03b3\n2\n\n(7)\n\n=\n\nZ\n\n+\u221e\n\nB 2 d\u03c9 =\n\n0\n\nZ\n\n+\u221e\n\n\uf8f62\n\n\uf8eb\n\uf8ed\u0010\n\n0\n\n\u03c0\n2\n\n\u03b3\n\u0010 \u221a \u0011\u0011 \u0010\n\u221a \u0011 \uf8f8 d\u03c9\nN\n+ arctan \u03b3\n\u03b3 2 + \u03c9 2 + N \u2212 2\u03c9 N\n\n+\u221e\n\n\u03b32\n\u0010 \u221a \u0011\u00112 \u0010\n\u221a \u00112 d\u03c9\nN\n\u03c0\n0\n\u03b3 2 + \u03c9 2 + N \u2212 2\u03c9 N\n2 + arctan\n\u03b3\nZ +\u221e\nd\u03c9\n\u03b32\n=\u0010\n\u0010 \u221a \u0011\u00112\n\u0010\n\u221a \u00112\nN\n\u03c0\n0\n2 + \u03c9 2 + N \u2212 2\u03c9 N\n+\narctan\n\u03b3\n2\n\u03b3\n\"\n\u221a\n\u221a !#+\u221e\n\u03b32\n\u03b3(\u03c9 \u2212 N )\n\u03c9\u2212 N\n\u221a\n=\n+ arctan\n\u0010 \u221a \u0011\u00112\n\u0010\n\u03b3\n\u03b3 2 + (\u03c9 \u2212 N )2\n2\u03b3 3 \u03c02 + arctan \u03b3N\n0\n\u221a\n\u221a !!\n1\n\u03c0\n\u03b3 N\nN\n=\n+ 2\n+ arctan\n;\n\u0010\n\u0010 \u221a \u0011\u00112\n2\n\u03b3\n+\nN\n\u03b3\n2\u03b3 \u03c02 + arctan \u03b3N\nZ\n\n=\n\nZ\n\u22122\n0\n\n+\u221e\n\n\u0010\n\nZ\nABd\u03c9 = \u22122\n\n(8)\n\n+\u221e\n\n\u03b3\n2\u03b3\n\u0010\n\u0010 \u221a \u0011\u0011 \u0010\n\u221a \u0011 d\u03c9\n2 + \u03c92 ) \u03c0\nN\n\u03c0(\u03b3\n2 + \u03c9 2 + N \u2212 2\u03c9 N\n0\n+\narctan\n\u03b3\n2\n\u03b3\nZ +\u221e\n\u22122 * \u03b3 * 2\u03b3\nd\u03c9\n\u0010 \u221a \u0011\u0011\n\u0010\n= \u0010\n\u221a \u0011\n\u03c0\nN\n2\n2\n2\n0\n\u03c0 2 + arctan \u03b3\n(\u03b3 + \u03c9 ) \u03b3 + \u03c9 2 + N \u2212 2\u03c9 N\n\u0014\n\u22124\u03b3\n\u03b3\n\u03b3 2 + \u03c92\n\u0010 \u221a \u0011\u0011\n\u221a +\n\u221a log\n=\u0010\n\u03c0\nN\n2 + N)\nN\n\u03b3 2 + \u03c9 2 + N \u2212 2\u03c9 N\n+\narctan\n\u03c0(4\u03b3\n2\n\u03b3\n\u221a !\n\u0012 \u0013#+\u221e\n\u03c9\u2212 N\n\u03c9\narctan\n+ arctan\n\u03b3\n\u03b3\n\u00140\n\u22124\u03b3\n\u03c0 \u03c0\n\u03b3\n\u03b32\n\u0010 \u221a \u0011\u0011\n=\u0010\n+ \u2212 \u221a log 2\n+\n\u03c0\nN\n2\n\u03b3 +N\n2 + N) 2\nN\n+\narctan\n\u03c0(4\u03b3\n2\n\u03b3\n\u221a !#\nN\narctan\n.\n\u03b3\n28\n\n(9)\n\n\f0.2\n\n1.5\n1.0\n\n5\n10\n100000\n\n0.1\nf(\u03b3, N)\n\nf(\u03b3, N)\n\n0.5\n\n5\n10\n100000\n\n0.0\n\n0.0\n\n\u22120.5\n\u22120.1\n\u22121.0\n\u22121.5\n\n\u22120.2\n0\n\n50\n\n100\n\n150\n\n\u03b3\n\n200\n\n0.35\n\n0.40\n\n(a)\n\n\u03b3\n\n0.45\n\n0.50\n\n(b)\n\nFigure 23: (a) Behaviour of f (\u03b3, N ) for N =5, 10 and 10000, in the interval \u03b3 \u2208 (0, 200] and (b) zoomed in the interval\n\u03b3 \u2208 [0.35, 0.5] with the solutions of Eq. 3.\nPlugging Eqs. 7,8,9 into Eq. 6, we obtain:\n1 = \u000f\u03b3 (EN , FN )\n1\n=\n+\n\u03c0\u03b3\n\n\u0010\n\n\u03c0\n2\n\n\u221a\n\u03c0\n\u03b3 N\n+ 2\n+ arctan\n2\n\u03b3 +N\n\n1\n2\u03b3\n\n\u0010\n\n\u03c0\n2\n\n+ arctan\n\n+ arctan\n\n\u22124\u03b3\n\u0010 \u221a \u0011\u0011\nN\n\u03b3\n\n\u0010 \u221a \u0011\u00112\nN\n\u03b3\n\n\u221a\n\nN\n\u03b3\n\n!!\n\u2212\n\u221a\n\n\"\n\n\u03b32\n\u03b3\n+ arctan\n\u03c0 \u2212 \u221a log 2\n\u03b3 +N\nN\n\u03c0(4\u03b3 2 + N )\n\nN\n\u03b3\n\n!#\n.\n\nConsider now the function f (N, \u03b3) = \u000f\u03b3 (EN , FN )\u22121: for a fixed value of N , it is a monotonically decreasing function\nof \u03b3, so the equation Eq. 3 has an unique solution \u03b3. In Fig. 23 (a) and (b) we display the situation for N = 5,10 and\n100000.\n\nB\n\nUniqueness of \u03b3 \u2191\n\n\u2191\n\u2191\nare now\nand F\u0302N\nThe spectra of the laplacian matrices of the two extremal graphs \u00caN\n\u2191\nspec(L(\u00caN\n)) = (0, * * * , 0) and\n| {z }\n2N\n\n\u2191\nspec(L(F\u0302N\n)) = (0, N \u2212 2, * * * , N \u2212 2, N, * * * , N , 2N \u2212 2) .\n|\n{z\n} | {z }\nN \u22121\n\nIt follows that\nK\u00ca \u2191 =\nN\n\n2\n(2N \u2212 1)\u03c0\n\nand\nKF\u0302 \u2191 =\nN\n\nN \u22121\n\n1\n(2N \u2212\n\n1) \u03c02\n\n\u0010\n\n\u221a\n\n+ (N \u2212 1) arctan\n\nN \u22122\n\u03b3\n\n\u221a\n\n+ arctan\n\nN\n\u03b3\n\n\u0011\n\n\u221a\n\n+ arctan\n\n2N \u22122\n\u03b3\n\n.\n\nThus the equation\n\u000f\u03b3 (\u00ca \u2191 , F\u0302 \u2191 ) = 1\n(whose solution is the normalizing factor \u03b3 \u2191 ) reads as follows:\nv\nu\n\u0010\n\u0011\n\uf8ee\n\uf8f92\nuZ\nN \u22121\nN \u22121\n1\n\u221a\n\u221a\n\u221a\nu +\u221e\n\u03b3 \u03b3 2 +(\u03c9\u2212\n+\n+\n2\n2\n2\n2\n2\n2\u03b3\nN \u22122)\n\u03b3 +(\u03c9\u2212 2N \u22122)\n\u03b3 +(\u03c9\u2212 N )\n\uf8f0\n\uf8fb d\u03c9 .\n\u0010\n1=u\n\u2212\n\u221a \u0011\n\u221a\n\u221a\nt\nN \u22122\n\u22122\n\u03c0\n\u03b3 2 + \u03c92\n0\n(2N \u2212 1) 2 + (N \u2212 1) arctan \u03b3 + arctan \u03b3N + arctan 2N\n\u03b3\n29\n\n(10)\n\n\f0.50\n\n\u03b3(N)\n\n0.45\n\nN\n5\n10\n50\n100\n500\n1000\n10000\n\n\u03b3\n0.4272836\n0.4517012\n0.4752742\n0.4777976\n0.4787492\n0.4785596\n0.4779060\n\n\u03b3\u2191\n0.3866861\n0.4300291\n0.4704579\n0.4753463\n0.4782538\n0.4783119\n0.4778813\n\nUndirected\nDirected\n\n0.40\n\n0.35\n\n0.30\n0\n\n50\n\n100\n\n150\n\n200\n\nN\n\nFigure 24: Comparison of \u03b3 and \u03b3 \u2191 for different number of nodes N .\nIntroduce now a few shorthands: define, for T, U \u2208 R, the following integral\n\u001a\nZ +\u221e\nd\u03c9\nM (T )\nif T = U ,\n\u221a\n\u221a\n=\n2 + (\u03c9 \u2212\n2 )(\u03b3 2 + (\u03c9 \u2212\n2)\nL(T,\nU\n)\nif\nT 6= U .\n(\u03b3\nT\n)\nU\n)\n0\nThen,\nM (T ) =\n\n1\n2\n\n\u0010\n\n\u221a\n\n\u03b3 2 arctan\n\nT\n\u03b3\n\n\u221a\n\n+ T arctan\n\nT\n\u03b3\n\n\u0001\n\n2\n\n2\n\n\u0001\n\n\u2212 log \u03b3 + U + log \u03b3 + T\n\u221a\n\u221a +\n+ T + 3 U ) T \u2212 (4 \u03b3 2 + 3 T + U ) U\nTo shorten notations, define furthermore\n(4 \u03b3 2\n\nZ=\n\n+\n\n\u03b35 + T \u03b33\n\nand\nL(T, U ) =\n\n\u221a \u0011\n+\u03b3 T\n\n2\u03b3\n\u03c0\n\nW = \u03b3(N \u2212 1)KF\u0302 \u2191\n\nN\n\n\u03c0\n,\n4\u03b3 3\n\n\u0010\u221a \u0011\n\n\u0010\u221a \u0011\n+ arctan \u03b3U\n\u221a \u221a\n.\n4 \u03b33 + T \u03b3 \u2212 2 T U \u03b3 + U \u03b3\n\n\u03c0 + arctan\n\nW0 =\n\nT\n\u03b3\n\nW\n.\nN \u22121\n\nWith the aforementioned positions, Eq. 10 becomes\n1 = Z 2 M (0) + W 2 M (N \u2212 2) + W 2 M (N ) + W 02 M (2N \u2212 2)\n\u2212 2ZW L(0, N \u2212 2) \u2212 2ZW L(0, N ) \u2212 2ZW 0 L(0, 2N \u2212 2)\n2\n\n0\n\n(11)\n\n0\n\n+ 2W L(N \u2212 2, N ) + 2W W L(N \u2212 2, 2N \u2212 2) + 2W W L(N, 2N \u2212 2) .\nAs in the undirected case, for each N Eq. 11 has an unique solution \u03b3 \u2191 , whose value is quite close to \u03b3, as shown in\nFig. 24.\n\n30\n\n\f"}