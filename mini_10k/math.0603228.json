{"id": "http://arxiv.org/abs/math/0603228v1", "guidislink": true, "updated": "2006-03-09T22:59:21Z", "updated_parsed": [2006, 3, 9, 22, 59, 21, 3, 68, 0], "published": "2006-03-09T22:59:21Z", "published_parsed": [2006, 3, 9, 22, 59, 21, 3, 68, 0], "title": "Nonparametric Bayesian Classification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0603416%2Cmath%2F0603246%2Cmath%2F0603399%2Cmath%2F0603452%2Cmath%2F0603414%2Cmath%2F0603438%2Cmath%2F0603318%2Cmath%2F0603269%2Cmath%2F0603012%2Cmath%2F0603478%2Cmath%2F0603513%2Cmath%2F0603725%2Cmath%2F0603606%2Cmath%2F0603740%2Cmath%2F0603181%2Cmath%2F0603644%2Cmath%2F0603537%2Cmath%2F0603515%2Cmath%2F0603520%2Cmath%2F0603332%2Cmath%2F0603663%2Cmath%2F0603453%2Cmath%2F0603667%2Cmath%2F0603449%2Cmath%2F0603627%2Cmath%2F0603686%2Cmath%2F0603685%2Cmath%2F0603247%2Cmath%2F0603194%2Cmath%2F0603700%2Cmath%2F0603464%2Cmath%2F0603171%2Cmath%2F0603447%2Cmath%2F0603033%2Cmath%2F0603201%2Cmath%2F0603516%2Cmath%2F0603556%2Cmath%2F0603568%2Cmath%2F0603370%2Cmath%2F0603124%2Cmath%2F0603328%2Cmath%2F0603137%2Cmath%2F0603372%2Cmath%2F0603366%2Cmath%2F0603028%2Cmath%2F0603458%2Cmath%2F0603217%2Cmath%2F0603143%2Cmath%2F0603563%2Cmath%2F0603075%2Cmath%2F0603044%2Cmath%2F0603457%2Cmath%2F0603041%2Cmath%2F0603589%2Cmath%2F0603120%2Cmath%2F0603394%2Cmath%2F0603741%2Cmath%2F0603666%2Cmath%2F0603206%2Cmath%2F0603301%2Cmath%2F0603022%2Cmath%2F0603356%2Cmath%2F0603213%2Cmath%2F0603305%2Cmath%2F0603351%2Cmath%2F0603532%2Cmath%2F0603538%2Cmath%2F0603726%2Cmath%2F0603057%2Cmath%2F0603593%2Cmath%2F0603360%2Cmath%2F0603303%2Cmath%2F0603504%2Cmath%2F0603319%2Cmath%2F0603014%2Cmath%2F0603185%2Cmath%2F0603329%2Cmath%2F0603712%2Cmath%2F0603406%2Cmath%2F0603357%2Cmath%2F0603733%2Cmath%2F0603294%2Cmath%2F0603382%2Cmath%2F0603321%2Cmath%2F0603487%2Cmath%2F0603215%2Cmath%2F0603153%2Cmath%2F0603699%2Cmath%2F0603708%2Cmath%2F0603619%2Cmath%2F0603285%2Cmath%2F0603238%2Cmath%2F0603228%2Cmath%2F0603088%2Cmath%2F0603633%2Cmath%2F0603735%2Cmath%2F0603189%2Cmath%2F0603169%2Cmath%2F0603320%2Cmath%2F0603560%2Cmath%2F0603067&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Nonparametric Bayesian Classification"}, "summary": "A Bayesian approach to the classification problem is proposed in which random\npartitions play a central role. It is argued that the partitioning approach has\nthe capacity to take advantage of a variety of large-scale spatial structures,\nif they are present in the unknown regression function $f_0$. An idealized\none-dimensional problem is considered in detail. The proposed nonparametric\nprior uses random split points to partition the unit interval into a random\nnumber of pieces. This prior is found to provide a consistent estimate of the\nregression function in the $\\L^p$ topology, for any $1 \\leq p < \\infty$, and\nfor arbitrary measurable $f_0:[0,1] \\to [0,1]$. A Markov chain Monte Carlo\n(MCMC) implementation is outlined and analyzed. Simulation experiments are\nconducted to show that the proposed estimate compares favorably with a variety\nof conventional estimators. A striking resemblance between the posterior mean\nestimate and the bagged CART estimate is noted and discussed. For higher\ndimensions, a generalized prior is introduced which employs a random Voronoi\npartition of the covariate-space. The resulting estimate displays promise on a\ntwo-dimensional problem, and extends with a minimum of additional computational\neffort to arbitrary metric spaces.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0603416%2Cmath%2F0603246%2Cmath%2F0603399%2Cmath%2F0603452%2Cmath%2F0603414%2Cmath%2F0603438%2Cmath%2F0603318%2Cmath%2F0603269%2Cmath%2F0603012%2Cmath%2F0603478%2Cmath%2F0603513%2Cmath%2F0603725%2Cmath%2F0603606%2Cmath%2F0603740%2Cmath%2F0603181%2Cmath%2F0603644%2Cmath%2F0603537%2Cmath%2F0603515%2Cmath%2F0603520%2Cmath%2F0603332%2Cmath%2F0603663%2Cmath%2F0603453%2Cmath%2F0603667%2Cmath%2F0603449%2Cmath%2F0603627%2Cmath%2F0603686%2Cmath%2F0603685%2Cmath%2F0603247%2Cmath%2F0603194%2Cmath%2F0603700%2Cmath%2F0603464%2Cmath%2F0603171%2Cmath%2F0603447%2Cmath%2F0603033%2Cmath%2F0603201%2Cmath%2F0603516%2Cmath%2F0603556%2Cmath%2F0603568%2Cmath%2F0603370%2Cmath%2F0603124%2Cmath%2F0603328%2Cmath%2F0603137%2Cmath%2F0603372%2Cmath%2F0603366%2Cmath%2F0603028%2Cmath%2F0603458%2Cmath%2F0603217%2Cmath%2F0603143%2Cmath%2F0603563%2Cmath%2F0603075%2Cmath%2F0603044%2Cmath%2F0603457%2Cmath%2F0603041%2Cmath%2F0603589%2Cmath%2F0603120%2Cmath%2F0603394%2Cmath%2F0603741%2Cmath%2F0603666%2Cmath%2F0603206%2Cmath%2F0603301%2Cmath%2F0603022%2Cmath%2F0603356%2Cmath%2F0603213%2Cmath%2F0603305%2Cmath%2F0603351%2Cmath%2F0603532%2Cmath%2F0603538%2Cmath%2F0603726%2Cmath%2F0603057%2Cmath%2F0603593%2Cmath%2F0603360%2Cmath%2F0603303%2Cmath%2F0603504%2Cmath%2F0603319%2Cmath%2F0603014%2Cmath%2F0603185%2Cmath%2F0603329%2Cmath%2F0603712%2Cmath%2F0603406%2Cmath%2F0603357%2Cmath%2F0603733%2Cmath%2F0603294%2Cmath%2F0603382%2Cmath%2F0603321%2Cmath%2F0603487%2Cmath%2F0603215%2Cmath%2F0603153%2Cmath%2F0603699%2Cmath%2F0603708%2Cmath%2F0603619%2Cmath%2F0603285%2Cmath%2F0603238%2Cmath%2F0603228%2Cmath%2F0603088%2Cmath%2F0603633%2Cmath%2F0603735%2Cmath%2F0603189%2Cmath%2F0603169%2Cmath%2F0603320%2Cmath%2F0603560%2Cmath%2F0603067&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Bayesian approach to the classification problem is proposed in which random\npartitions play a central role. It is argued that the partitioning approach has\nthe capacity to take advantage of a variety of large-scale spatial structures,\nif they are present in the unknown regression function $f_0$. An idealized\none-dimensional problem is considered in detail. The proposed nonparametric\nprior uses random split points to partition the unit interval into a random\nnumber of pieces. This prior is found to provide a consistent estimate of the\nregression function in the $\\L^p$ topology, for any $1 \\leq p < \\infty$, and\nfor arbitrary measurable $f_0:[0,1] \\to [0,1]$. A Markov chain Monte Carlo\n(MCMC) implementation is outlined and analyzed. Simulation experiments are\nconducted to show that the proposed estimate compares favorably with a variety\nof conventional estimators. A striking resemblance between the posterior mean\nestimate and the bagged CART estimate is noted and discussed. For higher\ndimensions, a generalized prior is introduced which employs a random Voronoi\npartition of the covariate-space. The resulting estimate displays promise on a\ntwo-dimensional problem, and extends with a minimum of additional computational\neffort to arbitrary metric spaces."}, "authors": ["Marc A. Coram"], "author_detail": {"name": "Marc A. Coram"}, "author": "Marc A. Coram", "arxiv_comment": "PhD Thesis, Stanford University, Aug 2002", "links": [{"href": "http://arxiv.org/abs/math/0603228v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0603228v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0603228v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0603228v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:math/0603228v1 [math.ST] 9 Mar 2006\n\nNONPARAMETRIC BAYESIAN CLASSIFICATION\n\na dissertation\nsubmitted to the department of statistics\nand the committee on graduate studies\nof stanford university\nin partial fulfillment of the requirements\nfor the degree of\ndoctor of philosophy\n\nMarc A. Coram\nAugust, 2002\n\n\fc Copyright by Marc A. Coram 2002\nAll Rights Reserved\n\nii\n\n\fI certify that I have read this dissertation and that, in\nmy opinion, it is fully adequate in scope and quality as a\ndissertation for the degree of Doctor of Philosophy.\n\nPersi Diaconis\n(Principal Adviser)\n\nI certify that I have read this dissertation and that, in\nmy opinion, it is fully adequate in scope and quality as a\ndissertation for the degree of Doctor of Philosophy.\n\nJerome H. Friedman\n\nI certify that I have read this dissertation and that, in\nmy opinion, it is fully adequate in scope and quality as a\ndissertation for the degree of Doctor of Philosophy.\n\nBradley Efron\n\nApproved for the University Committee on Graduate\nStudies:\n\niii\n\n\fAbstract\nA Bayesian approach to the classification problem is proposed in which random\npartitions play a central role. It is argued that the partitioning approach has\nthe capacity to take advantage of a variety of large-scale spatial structures,\nif they are present in the unknown regression function f0 . An idealized onedimensional problem is considered in detail. The proposed nonparametric prior\nuses random split points to partition the unit interval into a random number\nof pieces. This prior is found to provide a consistent estimate of the regression\nfunction in the Lp topology, for any 1 \u2264 p < \u221e, and for arbitrary measurable\n\nf0 : [0, 1] \u2192 [0, 1]. A Markov chain Monte Carlo (MCMC) implementation is\n\noutlined and analyzed. Simulation experiments are conducted to show that\nthe proposed estimate compares favorably with a variety of conventional estimators. A striking resemblance between the posterior mean estimate and the\nbagged CART estimate is noted and discussed. For higher dimensions, a generalized prior is introduced which employs a random Voronoi partition of the\ncovariate-space. The resulting estimate displays promise on a two-dimensional\nproblem, and extends with a minimum of additional computational effort to\narbitrary metric spaces.\n\niv\n\n\fAcknowledgements\nI thank my parents, who laid the foundations of my character and have been an\nunfaltering source of love and encouragement. I also thank my advisor, Persi\nDiaconis, whose friendship, support, and insight have been vital to successfully\nenduring the Ph.D. process. I also wish to thank the other members of my\nexamining committee: Jerome Friedman, Bradley Efron, Susan Holmes, David\nSiegmund, and Hans Andersen, for their professionalism and patience. Finally,\nI thank Hua Tang for putting up with my quirks all these years.\n\nv\n\n\fContents\nAbstract\n\niv\n\nAcknowledgements\n\nv\n\n1 Introduction\n\n1\n\n1.1 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n3\n\n1.2 A Nonparametric Prior on Regression Functions . . . . . . . . .\n\n4\n\n1.3 Sample Results . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n5\n\n1.4 The Partitioning Approach . . . . . . . . . . . . . . . . . . . . .\n\n6\n\n1.5 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n7\n\n2 Literature\n\n8\n\n2.1 Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . .\n\n8\n\n2.2 Related Bayesian Work . . . . . . . . . . . . . . . . . . . . . . . 12\n2.2.1\n\nA Dyadic Prior for Binary Regression . . . . . . . . . . . 12\n\n2.2.2\n\nBayesian CART . . . . . . . . . . . . . . . . . . . . . . . 13\n\n2.2.3\n\nPoisson Rate estimates using Random Partitions . . . . . 16\n\n2.2.4\n\nBayesian \"Image\" Analysis . . . . . . . . . . . . . . . . . 17\n\n2.2.5\n\nPolya Trees . . . . . . . . . . . . . . . . . . . . . . . . . 18\n\n2.2.6\n\nThe Contributions of this Thesis\n\n. . . . . . . . . . . . . 19\n\n2.3 Other Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\nvi\n\n\f3 Computing the Posterior\n\n23\n\n3.1 Specification of the Prior in One Dimension . . . . . . . . . . . 24\n3.2 Representing the Posterior . . . . . . . . . . . . . . . . . . . . . 26\n3.3 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.4 An MCMC Algorithm . . . . . . . . . . . . . . . . . . . . . . . 31\n3.5 Posterior Mean Calculation . . . . . . . . . . . . . . . . . . . . 35\n3.6 Metropolis-Hastings Markov Chains on General Spaces . . . . . 36\n3.7 A Simple Markov Chain . . . . . . . . . . . . . . . . . . . . . . 39\n3.8 A Local-Move Markov Chain . . . . . . . . . . . . . . . . . . . . 40\n3.9 Markov Chain Convergence Theory . . . . . . . . . . . . . . . . 48\n3.10 Convergence Results . . . . . . . . . . . . . . . . . . . . . . . . 50\n4 Examples\n\n53\n\n4.1 Comparison with CART and Bagged CART . . . . . . . . . . . 54\n4.1.1\n\nCART Review . . . . . . . . . . . . . . . . . . . . . . . . 54\n\n4.1.2\n\nBagging Review and Discussion . . . . . . . . . . . . . . 56\n\n4.1.3\n\nComparative Simulation Experiment . . . . . . . . . . . 59\n\n4.1.4\n\nObservations . . . . . . . . . . . . . . . . . . . . . . . . 62\n\n4.1.5\n\nSome Explanations . . . . . . . . . . . . . . . . . . . . . 63\n\n4.1.6\n\nSituations in which the Estimates Differ . . . . . . . . . 65\n\n4.1.7\n\nPosterior Mean Behavior . . . . . . . . . . . . . . . . . . 68\n\n4.1.8\n\nExperimental Runs 3-10 and a Summary . . . . . . . . . 69\n\n4.2 Comparison with Other Popular Methods . . . . . . . . . . . . . 73\n4.2.1\n\nSmoothers . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n\n4.2.2\n\nLARS/Lasso/Boosting . . . . . . . . . . . . . . . . . . . 74\n\n4.2.3\n\nWavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n\n4.3 Comparison with Dyadic Prior . . . . . . . . . . . . . . . . . . . 77\n4.4 Dependence on the Parameter of the Geometric Prior . . . . . . 79\n4.5 The Predictive Probability Surface . . . . . . . . . . . . . . . . 83\n4.6 Behavior on a Small Data Set . . . . . . . . . . . . . . . . . . . 83\n\nvii\n\n\f4.7 Behavior on a Large Data Set . . . . . . . . . . . . . . . . . . . 85\n4.8 The Effect of Sample Size . . . . . . . . . . . . . . . . . . . . . 88\n4.9 The Effect of Sample Size: a Harder Example . . . . . . . . . . 88\n5 Consistency\n\n92\n\n5.1 Notation and the Basic Theorem . . . . . . . . . . . . . . . . . 92\n5.2 Specification of the Prior . . . . . . . . . . . . . . . . . . . . . . 98\n5.3 A Consistency Proof . . . . . . . . . . . . . . . . . . . . . . . . 98\n6 Discussion of Consistency Results\n\n106\n\n6.1 Consideration of the Diaconis and Freedman Results . . . . . . 107\n6.2 An Experiment to Check a Worrisome Case . . . . . . . . . . . 107\n6.3 Theory versus Practice . . . . . . . . . . . . . . . . . . . . . . . 108\n6.4 Heuristics about Poisson and Geometric Priors . . . . . . . . . . 111\n6.5 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n7 Extensions\n\n114\n\n8 Afterword\n\n123\n\nBibliography\n\n129\n\nviii\n\n\fList of Tables\n3.1 MCMC Mixture Probabilities . . . . . . . . . . . . . . . . . . . 33\n4.1 Numerical Summary . . . . . . . . . . . . . . . . . . . . . . . . 70\n\nix\n\n\fList of Figures\n1.1.a An Example: f0 (x) . . . . . . . . . . . . . . . . . . . . . . . . .\n\n4\n\n1.3.a A Sample Result: the Posterior Mean . . . . . . . . . . . . . . .\n\n5\n\n4.1.a Comparative Simulation Experiment: Run 1 . . . . . . . . . . . 60\n4.1.b Comparative Simulation Experiment: Run 2 . . . . . . . . . . . 61\n4.1.c The Bagged Posterior Mean Estimate . . . . . . . . . . . . . . . 64\n4.1.d Data with a Gap . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.1.e Comparative Simulation Experiment: Runs 3-10 . . . . . . . . . 71\n4.1.f Comparative Scatterplot . . . . . . . . . . . . . . . . . . . . . . 72\n4.2.a Three Smoothers . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.2.b A Lasso Estimate . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n4.2.c Wavelet Estimates . . . . . . . . . . . . . . . . . . . . . . . . . 76\n4.3.a Dyadic Posterior . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n4.4.a The Posterior Mean for a variety of Geometric Priors . . . . . . 80\n4.4.b The Posterior on Model Size . . . . . . . . . . . . . . . . . . . . 81\n4.5.a A Marginal Likelihood Surface . . . . . . . . . . . . . . . . . . . 82\n4.6.a Small Data Set Experiment . . . . . . . . . . . . . . . . . . . . 84\n4.7.a Large Data Set Experiment . . . . . . . . . . . . . . . . . . . . 86\n4.8.a The Effect of Sample Size . . . . . . . . . . . . . . . . . . . . . 89\n4.9.a The Effect of Sample Size: a Harder Example . . . . . . . . . . 90\n5.3.a The sets C and R . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\nx\n\n\f6.2.a Null Case with Poisson(5) Prior . . . . . . . . . . . . . . . . . . 109\n6.2.b Null Case with Geometric( 21 ) prior . . . . . . . . . . . . . . . . 110\n6.5.a Poisson Posterior Example . . . . . . . . . . . . . . . . . . . . . 112\n7.0.a A Two Dimensional Data Set and Target Function . . . . . . . 116\n7.0.b Modal Samples: Voronoi Posterior . . . . . . . . . . . . . . . . . 118\n7.0.c A Bivariate Posterior Mean Example . . . . . . . . . . . . . . . 119\n7.0.d Weighted Voronoi Posterior . . . . . . . . . . . . . . . . . . . . 120\n7.0.e Bagged CART in 2d . . . . . . . . . . . . . . . . . . . . . . . . 122\n\nxi\n\n\fChapter 1\nIntroduction\nThe binary classification problem is perhaps the simplest regression problem,\nbut it continues to pose fresh challenges. In the binary classification problem,\nwe are given a list of n pairs Zi = (Xi , Yi ) each pair drawn independently\nfrom an unknown probability measure F . The X's play the role of covariate or\n\"predictor\" and lie in some abstract space X , while the Y 's are interpreted as a\n\nclass label and are either 0 or 1. Our goal is to estimate certain functionals of F .\nSpecifically, in the binary regression problem, we are interested in estimating\nthe regression function f : X 7\u2192 [0, 1]. The value of f at a given point x \u2208 X\n\nis the conditional probability that Y = 1 given that X = x. In this way we\nmodel the joint distribution F by saying that to draw an (X, Y ) pair from F ,\nfirst draw a covariate X = x from the marginal distribution of X denoted by\n\u03bc. Then \"flip\" an f (x) coin to determine the value of Y .\nIn the classification problem, we are concerned with being able to predict\nfuture Y values. The standard formalization of this task is that we wish to\nchoose the \"decision rule\" that will minimize the expected loss incurred; this\nreduces to the problem of estimating the set {x \u2208 X : f (x) > c}, for some c\nthat depends upon the loss (for simplicity, ignore the possibility that c depends\n\non x). There are a great many ways to proceed on each of these problems, as\ndemonstrated by the vast literature on these subjects. Some references are\n1\n\n\fCHAPTER 1. INTRODUCTION\n\n2\n\ngiven in section 2.3.\nIn this thesis, I propose a nonparametric Bayesian approach to the binary\nclassification and regression problems. Specifically, to derive an estimator, I\nregard F itself as random. For simplicity, I regard the marginal distribution\nof X, as known. In this case, putting a prior distribution on F amounts to\nputting a prior on f . More generally, one can also put a prior on functions m\nand suppose that \u03bc(dx) = m(x)\u03bc0 (dx). Some sort of mild restriction, like this\none that all \u03bc share some dominating measure \u03bc0 , is useful to avoid technical\nproblems in defining the conditional distribution of F given the data.\nLet \u03c0 denote a prior distribution on F , or, more precisely, on (f, m) pairs.\nExtend \u03c0 to a joint distribution on F = (f, m) and the infinite data sequence\n(Z1 , Z2 , . . . ) which, conditionally on F , is drawn independently and identically\ndistributed (iid ) from F . Formally, the posterior is the measure \u03c0n (dF ) :=\n\u03c0 (dF |Z1 = z1 , . . . , Zn = zn ). In practice Markov chain Monte Carlo procedures\ncan be used to generate a sample from the posterior.\n\nThe posterior mean of f is an important summary of the posterior: its\nvalue minimizes the posterior risk under an L2 loss. Let fb denote the posterior\nR\nmean: fb(x) = f (x)\u03c0n (df ). Another important summary of the posterior is\nthe classification rule which minimizes posterior misclassification loss. If asked\nto predict the most likely value of the Y 's corresponding to Xn+1 , . . . , Xn+n\u2032\n\nall at once, the decision that would minimize the posterior-expected 0-1-loss\nis simply \u03b4i = 1 b\n. Interestingly, though, if asked sequentially instead\nf (Xi ) > 21\nof all at once, it is necessary to update fb with each new data point before\n\ndeciding.\n\nTaking this Bayesian approach assures us that the resulting estimators will\n\nhave a clear subjective interpretation. In addition, if the prior \u03c0 is carefully\nchosen, the resulting estimators, chosen indirectly through this Bayesian framework, may have frequentist advantages over the estimators that might otherwise be proposed. For example, interesting kinds of shrinkage and averaging\n\n\f3\n\nCHAPTER 1. INTRODUCTION\n\noccur automatically within this framework. Subsequent chapters assess the\nfrequentist performance of these Bayesian estimates by simulation experiments\n(chapter 4) and theoretically (chapter 5).\n\n1.1\n\nAn Example\n\nTo get started, let us consider the specific case in which X = [0, 1] and the\n\nsampling distribution of the Xi , \u03bc, is known to be the U(0, 1) distribution.\n\nFurther, let us consider a specific f = f0 which is complicated enough that\nits estimation should not be too easy for any of the standard methods; it is\npiecewise continuous with two constant regions and a smooth transition region.\nAs shown in Figure 1.1.a, f0 (x) is chosen as:\n\nf0 (x) =\n\n\uf8f1\n\uf8f4\n0\u2264x<\n\uf8f4\n\uf8f4\n\uf8f2\n\n1\n6\n\n0.6\n\n1\n\u2264 x \u2264 12\n6\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f31 < x \u2264 1\n2\n\n0.4\n\u03c6\u03c3 (x\u2212 12 )\n\u03c6\u03c3 (x\u2212 12 )+\u03c6\u03c3 (x\u22121)\n\nwhere \u03c6\u03c3 is the density of a normal with mean 0 and standard deviation \u03c3 =\n0.25.\nThe two histograms in Figure 1.1.a summarize a simulated data set of 1024\ndata points that was drawn from this model. The green histogram is a histogram of the heads. The red histogram is a histogram of the tails; it is drawn\nupside down to facilitate comparison with the green histogram. To more easily\ninterpret this display, notice that if we take the sum of the corresponding green\nand red bins at each point, we recover a histogram of the marginal distribution, which is uniform. Furthermore, the ratio of the height of a green bin to\nthe corresponding red bin represents the empirical odds of a head in that bin.\nNear x = 0.5, for example, notice the sharp transition from nearly equal green\nand red bins (on the left) to much longer green than red bins (on the right).\nThe performance of this posterior mean estimator is compared with a variety\n\n\f4\n\nCHAPTER 1. INTRODUCTION\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 1.1.a: An Example: f0 (x)\nof more conventional methods in chapter 4. This example is used to illustrate\nthe present approach in the rest of this introduction.\n\n1.2\n\nA Nonparametric Prior on Regression Functions\n\nTo specify a prior \u03c0 over functions f : [0, 1] 7\u2192 [0, 1], I explain how to choose\n\nf , at random from it. This completely specifies a prior on the probability\ndistribution F , since I consider the marginal distribution \u03bc to be known. The\n\nprior on f will concentrate on locally-constant step functions. To choose a step\nfunction at random, first, choose K, the number of locally constant intervals,\nwhere:\nP (K = k) = (1 \u2212 \u03b1)\u03b1k\u22121\n\nfor k = 1 . . . \u221e\n\nThat is, K is Geometric with parameter 1 \u2212 \u03b1. Ultimately, the choice of\n\n\u03b1 must be specified by the user. For the examples in this thesis, I have used\n\u03b1=\n\n1\n2\n\nunless otherwise noted. This choice seems to perform well. For further\n\ndiscussion of how to choose a prior on K which results in provably consistent\n\n\f5\n\nCHAPTER 1. INTRODUCTION\n\n1\n\ntruth\nposterior mean\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 1.3.a:\nestimators, see chapter 6.\nNow, conditional on K = k, choose V1 , V2 , . . . , Vk\u22121 iid from U(0, 1). Let\nV(i) (for i = 1 . . . k \u2212 1) be the i\u2032 th ordered value of the V 's. This produces k\n\nintervals:\n\nI1 = {0 \u2264 x < V(1) }, I2 = {V(1) \u2264 x < V(2) }, . . . , Ik = {V(k\u22121) \u2264 x \u2264 1}\nIf k = 1, simply take I1 = [0, 1]. Finally, conditional on K = k, choose k values\nSi (for i = 1 . . . k) iid from U(0, 1). This generates the random function f :\nf (x) =\n\nk\nX\ni=1\n\n1.3\n\nS i 1x \u2208 I i\n\nSample Results\n\nConditioning this prior on the data described in the earlier example section\nresults in a posterior distribution on regression functions f . Applying the\ntechniques in chapter 3 to sample from the posterior results in a long list of\n\n\fCHAPTER 1. INTRODUCTION\n\n6\n\nsampled functions. Although the prior specified a Geometric( 12 ) prior distribution on the number of locally-constant pieces K in each function, when drawn\nfrom the posterior, functions typically have at least 5 pieces. Taking the average of these functions (at every x) gives an estimate of the posterior mean;\nthis is illustrated in Figure 1.3.a. Further discussion of results like this can be\nfound in chapter 4.\n\n1.4\n\nThe Partitioning Approach\n\nStep functions, such as the functions f that the prior \u03c0 concentrates on, have\nthe advantage of great mathematical simplicity; but, if specified in sufficient\ndetail, they can approximate general functions. For a multi-dimensional regression function, it is natural to generalize this idea by considering some partition\nof the covariate space X into a number of pieces and constructing a function\n\nthat takes a different value on each piece. A procedure that uses a wide variety\n\nof geometric shapes to partition the space might be able to find a partition\nwith the right structure to approximate the unknown regression function. Ideally, the partition would be no more complex than necessary to achieve a good\napproximation. If the unknown regression function has certain global features\nthat the chosen partition can be adapted to, this idea becomes very powerful.\nInstead of merely \"borrowing strength\" locally, like ordinary smoothing estimators do, a partition-based estimator borrows strength across the whole range\nof a partition element. As a simple example, if the true regression function\ndoes not depend on one of the covariates, the partition elements do not need\nto break up space along this dimension at all; this results in larger partition\nelements and more efficient estimation of the success probability on each of the\npieces. Similarly, if the regression function is almost flat in some large chunk\nof space, then this whole region can become a single element. If the level-sets\nof the regression function have smooth boundaries, perhaps the partition elements can be chosen to follow these contours. Finally, since the best partition\n\n\fCHAPTER 1. INTRODUCTION\n\n7\n\nfor the unknown function is unknown, it makes sense to average together the\napproximations found by a variety of partitions that make the data likely. This\nis exactly what the posterior mean estimate will do automatically. Chapter 7\nshows one way in which this idea can be applied using Voronoi partitions. For\nthis partition, a prior distributes seed-points in the covariate space; each seed\ncorresponds to a partition element, the one that consists of all points closer to\nthat seed than any other seed. By placing the seeds appropriately, the partition\ncan be fine or coarse as needed. The boundary between partition elements is\nitself a hyperplane whose position and orientation can be controlled through\nthe placement of the seeds.\nOther authors have recently considered similar priors with good success.\nTheir work is discussed in section 2.2. To the best of my knowledge, this thesis\npresents the first theoretical examination of consistency issues for priors of this\nsort (c.f. chapter 5, with discussion in chapter 6). Additionally, I present a\ndetailed assessment of the empirical performance of my methods on certain\nnovel simulation experiments. The comparison with bagged CART regression\ntrees in chapter 4 is especially interesting.\n\n1.5\n\nOutline\n\nChapter 2 gives a literature review. Chapter 3 shows how to (approximately)\ncompute samples from the posterior and the posterior mean. Chapter 4 trys\nout the method on examples and carefully compares its performance with that\nof a variety of existing methods. Chapter 5 gives sufficient conditions on the\nprior under which it provides universally consistent estimates of f . Chapter 7\ndescribes a different prior which extends these ideas to general metric spaces\nby employing random Voronoi partitions. Certain modifications are explained\nthat make the proposal more practical and its performance on an example is\nshown. Finally, the afterword gives a philosophical argument that advocates\nthe use of Kolmogorov-complexity in future statistical thinking.\n\n\fChapter 2\nLiterature\nThis chapter reviews and discusses the literature on three subjects. The first\nsection reviews some theoretical results concerning the frequentist performance\nof Bayesian procedures. The second section gives a survey of some of the work\ndone by authors on related Bayesian efforts. The final section briefly surveys\nsome salient examples of alternative approaches to the classification problem.\n\n2.1\n\nTheoretical Results\n\nThe frequentist performance of Bayesian methods is of fundamental interest\nin statistics. Given a large sample from a smooth, finite-dimensional statistical model, the situation is quite well understood. The Bernstein-von Mises\ntheorem [49, 33] shows that the Bayes estimate and the maximum likelihood\nestimate will be close. Furthermore, the posterior distribution of the parameter vector around the posterior mean is close to the distribution of the maximum likelihood estimate around the truth: both are asymptotically normal\nwith mean 0 and the same covariance matrix. Unfortunately, though, in more\ngeneral circumstances, such as those needed for this work, the situation can\nbe much more complex. In particular, the basic model is based on an infinite\n\n8\n\n\fCHAPTER 2. LITERATURE\n\n9\n\nhierarchy of finite dimensional models. Moreover, even for a given finite dimensional submodel, the dependency of the likelihood function on the parameter\nis not smooth; the functions are allowed to take jumps. Consequently, a more\ngeneral theory is needed.\nThis section reviews some of the literature on this subject with a focus on\nresults that address the question of consistency: i.e. as the number of data\npoints tends to infinity, will the Bayesian estimate converge to the true value\n(in some suitable sense) almost surely (resp. in probability)? The literature\ncontains a number of useful and quite flexible positive results, but also a variety of interesting negative examples showing that the regularity conditions\nunder which the theorems hold are not to be taken lightly. A good introduction to these issues is by Diaconis and Freedman [23]. Throughout this\nsection, the reader may envision a family P\u03b8 (dx), a prior \u03c0(d\u03b8), and posterior\n\u03c0(\u03b8|x1 , . . . , xn ), where the xi are drawn iid from P\u03b80 (dx). Consistency means\nthat the posterior concentrates at \u03b80 for large samples.\nDoob [29] established a fundamental result under minimal regularity assumptions using a martingale convergence argument. Roughly speaking, the\nresult states that if consistent estimators exist at all, then a Bayes procedure\nwill provide an almost surely consistent estimate of the true parameter \u03b8 under\nsampling from the \u03b8 distribution for any \u03b8 in some set B which has prior probability of 1. Notice, though, that this does not specify if consistency will obtain\nat any particular point of interest \u03b80 , unless \u03b80 happens to be a point-mass of\nthe prior, or unless it possible to determine B by some more detailed line of\nargumentation.\nFreedman [32] considered the case in which the observations are discrete.\nIf the set of possible observations is finite, the posterior is consistent exactly\nfor parameter values in the topological support of the prior. The countably\ninfinite case is more complex. He constructs a class of examples showing that it\nis possible to construct a prior which assigns positive mass to every (weak star)\nneighborhood of the true parameter value, but for which the posterior converges\n\n\fCHAPTER 2. LITERATURE\n\n10\n\nto a point mass at some other (chosen) parameter value. Furthermore, he\nfinds a prior which assigns positive prior mass to every (weak star) open set of\nparameters, but for which the posterior is consistent only at a set of parameters\nof the first category. The reader should note that this prior did not assign\nmass to all entropy-neighborhoods. This sort of subtle distinction can make\nall of the difference and explains the necessity of some such assumption in the\nfollowing consistency theorems. He introduces the \"tail-free\" priors for the the\ncountably-infinite case and demonstrates that these are always consistent.\nLorraine Schwartz [61] explored the question of consistency in a very general setting. She extended Doob's result to a broad class of loss functions [61,\nlemma 4.2]. She also found sufficient conditions for the posterior to be consistent under iid sampling. These conditions, she says, are \"of an essentially\nweaker nature\" than the conditions established for the consistency of maximum likelihood estimators. Nevertheless, she constructs an example where the\nmaximum likelihood estimate is consistent and the estimates based on certain\npriors are not. The example ([61, example 3]) involves a simple parametric\nfamily of densities which satisfies Wald's conditions, thereby guaranteeing that\nthe maximum likelihood estimate will be consistent, but for which the posterior\ncan be inconsistent. The consistency of the posterior in this case, is found to\ndepend critically on the amount of mass that the prior ascribes to small neighborhoods of the true parameter value; if this mass shrinks too quickly, the\nprior \"ignores\" the data. One clever aspect of her construction is the way the\ndensities are parametrized. Parameter values close to the target value \u03b80 correspond to densities that are close to the \u03b80 -density in an L1 sense, but which are\n\nfarther and farther away in Kullback-Leibler discrepancy. In fact, there is only\none point in parameter space (the true parameter) that has Kullback-Leibler\ndiscrepancy from the truth smaller than \u01eb, for \u01eb sufficiently small.\nSchwartz then shows that the posterior will be consistent under iid sampling\nunder two basic conditions. First, the prior should have positive mass on\nKullback-Leibler neighborhoods of the true parameter (defined in section 5.1\n\n\fCHAPTER 2. LITERATURE\n\n11\n\nof this thesis), and second, the model class should not be too rich; specifically,\nshe requires that uniformly consistent tests of the hypothesis that \u03b8 = \u03b80\nagainst the alternative that \u03b8 lies outside a given (open) neighborhood of \u03b80\nexist.\nIt is not always obvious how to verify the later property directly. Modern authors have employed entropy-type bounds to guarantee their existence.\nGhoshal, Ghosh, and van der Vaart [40] state a theorem ([40, theorem 7.3])\nwhich proves that the posterior converges at a certain rate if certain uniform\ntests exist (and the prior mass is suitably distributed) and go on to find a\nvariety of entropy-type conditions that suffice to be able to construct the necessary tests. Shen and Wasserman [62] show related results, requiring slightly\ndifferent conditions on how mass needs to be allocated\u2013they do not a make a\nconnection with testing. Barron, Schervish, and Wasserman [2] find sufficient\nconditions for the posterior to be consistent; their results are reviewed and\nthen used in chapter 5.\nIt should be noted that these various conditions for consistency are not\nnecessary, but merely sufficient. Nevertheless, it is important to treat this\nsubject with care because of the variety of examples for which consistency\nfails.\nBarron, Schervish, and Wasserman also give an interesting example where\nconsistency fails. In this example, they show that the prior puts too much mass\non a very rich class of models that will be able to match any spurious structure\nthat the data might have by chance, overwhelming the true parameter. Furthermore, lest the reader get the wrong idea, inconsistency does not only occur\nin artificial examples. A series of \"natural\" yet still inconsistent estimators for\nthe symmetric location problem are discussed by Diaconis and Freedman [23].\nIn addition, the binary regression example explained in the next section has a\nnatural motivation based on conditional exchangability.\n\n\f12\n\nCHAPTER 2. LITERATURE\n\n2.2\n\nRelated Bayesian Work\n\nThe following subsections contain a review of work by other authors that is\nclosely related to this thesis. It is followed my a brief synopsis of the contributions that this thesis makes to the literature.\n\n2.2.1\n\nA Dyadic Prior for Binary Regression\n\nThe most relevant examples for the work of this thesis are the nonparametric\nbinary regression examples of Diaconis and Freedman [24, 25]. They use a\ndifferent prior; call it \u03c0DF , a hierarchical, dyadic prior on f . To describe \u03c0DF ,\nlet Ak be the set of intervals which result from partitioning the unit interval\ninto 2k equal pieces. Let Fk be the subset of functions which are constant on all\n\nintervals a \u2208 Ak . Finally, fix a prior distribution \u03ba on the non-negative integers.\nAssume, for simplicity, that \u03ba(k) > 0 for all k. To draw f from \u03c0DF , draw K\n\nfrom \u03ba and then, conditional on hierarchy-level K = k, draw f uniformly at\nrandom from Fk . In effect then, at level k one draws 2k independent U(0, 1)\n\nrandom variables to describe the success probability on each of the 2k pieces.\nThey show that for any \u03ba and any f0 (except possibly for f0 \u2261\n\n1\n),\n2\n\nthe\n\n1\n\nposterior estimates are consistent (in the sense that any L neighborhood of f0\n\nhas posterior probability tending to 1 a.s.). Remarkably, however, for f0 \u2261 12 ,\nthe posterior can be an inconsistent estimate if the tail of \u03ba is sufficiently heavy.\n1\n\nSpecifically, let \u03bbk = \u2212 log(\u03ba(K \u2265 k))/k. Then if lim sup \u03bbk > \u03bbcrit = 2\u2212 4 \u2248\n\n0.841, the posterior is inconsistent at f0 \u2261 12 . On the other hand, if lim sup \u03bbk <\n\n\u03bbcrit , the posterior is consistent for any f0 . To put this in perspective, for \u03ba(k) =\n(1\u2212\u03b2)\u03b2 k (a shifted Geometric(1\u2212\u03b2) prior), lim sup \u03bbk = \u2212 log(\u03b2). The critical\nvalue for \u03b2 is exp(\u2212\u03bbcrit ) \u2248 0.431; for larger \u03b2 (longer tails) inconsistency will\n\noccur (but only for f0 \u2261 21 ).\n\nThis result is substantially stronger than the result I have obtained for my\n\nprior \u03c0. In particular, applying the same (general) method of proof that I\nemployed to prove consistency for \u03c0 to \u03c0DF yields only the result that \u03c0DF is\n\n\fCHAPTER 2. LITERATURE\n\n13\n\nconsistent if the tails of \u03ba drop off at least as fast as those of a Poisson. (Recall,\nthat at level k, \u03c0 only divides [0, 1] into k intervals, but \u03c0DF divides it into 2k .)\nTheir method of proof is direct: using Bernstein's inequality, Poissonization,\nand special features of the prior. My method of proof is indirect; it uses general\nresults that employ entropy-type bounds.\nThere are striking similarities between \u03c0 and \u03c0DF . In fact, \u03c0 is equivalent\nto a suitably randomized \u03c0DF . To achieve this, it is not enough to simply\nrandomize the dyadic split points. Instead, recall that \u03c0DF has an alternative\ninterpretation in terms of binary sequences. At hierarchy-level k, \u03c0DF is uniform over Fk . This corresponds to independently assigning uniform success\n\nprobabilities to each binary sequence of length k. Here is an alternative way\nto draw f from \u03c0. Draw g from \u03c0DF and interpret g as function on binary\n\nsequences of length k (k depends on g). Let Vi (i = 1, . . . , k) be iid U(0, 1)\nrandom variables. To any point u \u2208 [0, 1] associate the binary random variables\n\n\u03b7i (u) = 1(u \u2264 Vi ) (i = 1, . . . , k). Define f via f (u) = g((\u03b71(u), . . . , \u03b7k (u))).\nNote that only a small fraction of possible binary sequences are realized in this\nmanner (at level k (which ranges from 0 to \u221e under \u03c0DF ), k + 1 sequences out\nof the full set of 2k possible sequences are achieved).\n\n2.2.2\n\nBayesian CART\n\nTwo other closely related priors can be described as Bayesian versions of the\nCART algorithm. This was pursued by Chipman, George, and McCulloch,\nwhose prior closely parallels the choices made in the original CART algorithm [6, 7, 8, 9]. Here is a description of their prior when the covariate\nspace is Rp . Their prior starts with a root node (which represents the whole\nspace); this node is then recursively partitioned in a random way. For each\n\nnode, randomly choose whether to split it or not, then choose a coordinate\nto split on, then choosing a split point (i.e. the cutoff value) randomly from\namong the midpoints between the ordered values of this coordinate; finally each\n\n\fCHAPTER 2. LITERATURE\n\n14\n\nleaf node is given an independent regression value. The details of how these\ndecisions are made differ in their particulars from the ones that I described in\nthe introduction. In early work, these authors observed that using MCMC to\nsample from the posterior of this prior provides a rudimentary (global) search\nprocedure, which has certain (apparent) advantages over the greedy search procedure commonly implemented in CART-type algorithms. In later work, they\nexamined and computed the (approximate) posterior mean (working primarily on the least-squares white-noise regression problem) and found that it had\ngood performance. They also considered extended priors that modeled the\nregression values as additively (not independently) generated [9].\nDenison, Mallick, and Smith, independently considered another version of\nBayesian CART [18, 17, 19]. For one-dimensional problems they propose using\nrandom splines (the prior I use is essentially a special case of this prior). They\nconsider some of the regression examples that are standard in the wavelet literature and show that their spline methods perform equally well. Additionally,\nthey propose a Bayesian version of Friedman's MARS which puts a prior on\nfunctions that are constructed by adding together random spline-type ridge\nfunctions. Denison, Adams, Holmes, and Hand discuss the usefulness of random partitions in this paper [15].\nVery recently, Denison, Holmes, Mallick, and Smith have written a book [16]\nwhich surveys some related Bayesian regression schemes, including a Bayesian\nmethod for (multiple class) classification using Voronoi partitions that is very\nclosely related (albeit independent of) the work that I present in chapter 7. The\nbook also discusses Bayesian wavelet methods, and an interesting Bayesian\nnearest-neighbor prior. As a default prior, they recommend assuming that\nevery model in a \"single dimension\" is equally likely, and each dimension is\nequally probable, a priori. This \"flat prior,\" they claim, should serve perfectly well because of the, \"natural tendency\" for the marginalized likelihood\nto penalize complex models:\n\n\fCHAPTER 2. LITERATURE\n\n15\n\nOn the face of it, we might be concerned that the flexible modeling strategy we advocate might be prone to overfitting the data by\nadding too many basis functions. Indeed, many papers found in the\nliterature advocate explicit priors on the model space that penalize\nthe dimension of the model. However, throughout this book we argue that such a measure is unnecessary. The Bayesian framework\ncontains a natural penalty against over [sic] complex models, sometimes called Occam's razor, which essentially states that a simpler\ntheory is to be favoured over a more complex one, all other things\nbeing equal.\nThere is no consideration given to the possibility that this might give rise to\ninconsistent estimates (e.g. as in the Diaconis and Freedman non-parametric\nregression example explained earlier); indeed there are few theoretical considerations at all in the book. Their explanation of why the Markov chain\ntechniques that they develop should actually give meaningful samples from the\nposterior appeals to Green's reversible jump [41]. The explanation given is\nvague and ultimately they decide to avoid the issue and appeal to the fact\nthat their chains are discrete. The chains in chapter 3 of this thesis involve a\ncontinuous state space and do not simply avoid this issue by discretizing the\ncontinuous modeling space as these authors seem to do.\nOverall, the book emphasizes main ideas, algorithms, and results. It seems\nthat for every existing regression technique, they want to demonstrate that\nthey can make a \"Bayesian\" version of it too. The book does not emphasize\nsubjectivism, but rather adopts an \"Mopen \" perspective to Bayesian modeling:\n\"we never believe that the true model lies in the set of possible models.\" The\nbook does do a good job of supplying default priors for a wide variety of possible\nparametric models. Similarly, Denison's thesis [20] emphasizes the wide variety\nof problems to which Bayesian partitioning methods of this sort can be applied.\n\n\fCHAPTER 2. LITERATURE\n\n2.2.3\n\n16\n\nPoisson Rate estimates using Random Partitions\n\nGreen [41], and Scargle [58] develop priors on piecewise constant functions\non the real line and Rd using Voronoi cells. Their priors are quite similar to\nthe ones developed in this thesis, but are intended to address the problem of\nestimating the rate function of a Poisson process. In principle, one could apply\ntheir techniques to the problem of binary regression by generating an estimate\nof the rate function of the \"heads\" process and the \"tails\" process separately\nand then combining the results. I do not think that this has been tried and it\nseems substantially less \"natural.\"\nGreen applies his method to a coal mining dataset and a synthetic twodimensional example. For these example, Green assumes that an individual\ncell's rate-parameter is drawn independently from a \u0393(\u03b1, \u03b2) prior. For the onedimensional case he advocates a prior which \"probabilistically\" spaces out the\nchange-point locations; specifically, if there are j change-points, the ordered\nlocations of the change-points are distributed like the even order statistics of\n2j + 1 independent uniform values. He argues that this is good because it\nprevents small change-point intervals from entering into the posterior. For\nthe two-dimensional example, the generating points of the Voronoi partition\nare drawn independently and uniformly. Green's methods are given, in part,\nas examples of his \"reversible jump\" MCMC technique. This technique has\nbecome an accepted part of MCMC practice, but is not accepted by all experts\nin MCMC theory because it does not lay down in a straightforward \"theoremproof\" manner the necessary conditions and consequent conclusions. For this\nreason, detailed verifications for the chains used in this thesis are given in\nchapter 3.\nScargle's work is applied to astronomical data; he concentrates on the problem of finding the mode of the posterior, rather than the posterior mean. Fortunately, he and coworkers have developed a way of computing this mode in\nthe one-dimensional case exactly and efficiently using a dynamic programming\n\n\fCHAPTER 2. LITERATURE\n\n17\n\napproach [59]. Instead of giving each cell an independent value, Scargle gives\neach cell a (logical) \"color\" and then associates each unique color with an independent rate-parameter. This allows him to use a fine partition and then group\n\"chunks\" back together into more complicated shapes. The way he forms this\npartition is also different; in particular his \"prior\" is data dependent, but not\nquite in the way of the \"prior\" that I consider in chapter 7. Rather, the data\nis used once and for all to generate the fine Voronoi partition of space that\nresults from using all of the data points as generators. These cells are then\n\"clumped\" (i.e. given a logical color) and the clumps are given an independent\nrate parameter.\n\n2.2.4\n\nBayesian \"Image\" Analysis\n\nM\u00f8ller and Skare [53] apply their work to reservoir modeling and connect their\nwork to efforts in Bayesian image analysis (including Markov random fields).\nThey use a random Voronoi partition of the data and assign each partition\nelement a random color (in a way that depends only the colors of neighboring cells). They supply several further references to work in Bayesian image\nanalysis which use Voronoi cells. From their perspective, to calculate their\nposterior they are simulating from a special \"marked point\" process. The generators of the Voronoi cells are regarded as point set that has been drawn from\na homogeneous Poisson process of rate \u03b2 on the unit cube. In the simplest\ncase, the marks or \"colors\" of these points are just integers from 1 up to M\nthat have been drawn independently. More generally, according to their prior,\nthe conditional distribution of the coloring of cells given is an Ising or Potts\nmodel. The graphical structure of this model is determined by consideration\nof which Voronoi cells are neighbors, and the \u03b8 parameter is chosen to reflect\ntheir prior belief that neighboring cells tend to be of the same color. They\nconsider two problems. The first is a simulation experiment in which a \"true\"\n\n\fCHAPTER 2. LITERATURE\n\n18\n\nbinary image is degraded with Gaussian noise. The second is a three dimensional reservoir problem based on real data. It is supposed that a certain three\ndimensional cube (the reservoir) consists of 4 different types of rock. The rock\ntypes are observed along seven vertical lines, representing the observations of\nrock that were made as seven wells were dug into the reservoir. In both problems, the true object to be recovered is itself a certain \"coloring\" of space (i.e.\nrather than a continuous regression function). For the MCMC computation of\ntheir posterior they apply the birth-death type Metropolis-Hastings algorithm\nfor point processes, as studied by Geyer and M\u00f8ller [38] and claim that their\ntarget distribution satisfies a local stability condition (see Geyer [37], Kendal\nand M\u00f8ller [45], and M\u00f8ller [52]) so that the MCMC is actually geometrically\nergodic.\n\n2.2.5\n\nPolya Trees\n\nFinally, Polya trees [48] and especially randomized Polya trees [55] deserve to\nbe mentioned. The basic Polya tree puts a prior on distribution functions on\nthe unit interval. The unit interval is divided recursively in a dyadic binary\nway and mass is allocated to each piece of the partition in a stagewise manner\nby first determining how much of the mass that is available will be on the left\nversus the right half and then continuing with such determinations layer by\nlayer. Each of these assignments is ultimately determined by independent Beta\nrandom variable, whose parameters depend upon its location in the \"tree.\" If\na suitable choice of these parameters is made the result prior on distribution\nfunctions concentrates on distributions that are absolutely continuous with\nrespect to Lebesgue measure. The essential advantage of Polya trees is that the\nposterior of Polya tree prior is easily and analytically computable, being itself\nanother Polya tree. For randomized Polya trees, the partitioning scheme is\nindependently \"jittered\" at random in a particular way [55]. A Hybrid MCMC\ncan be employed to sample from the randomized Polya tree posterior which\n\n\fCHAPTER 2. LITERATURE\n\n19\n\nuses a Gibbs step to take advantage of the ease with which the (internal) Polya\ntree posterior can be computed. Both methods can be extended (essentially\nby taking \"direct products\") to put a prior on distributions on the unit cube.\n\n2.2.6\n\nThe Contributions of this Thesis\n\nReviewing the depth and breath of the literature reviewed above may leave\nthe reader in doubt about the contributions of this thesis. After all the onedimensional prior that I consider is essentially a special case of the univariate\nspline model and the idea of using Voronoi partitions is certainly not new,\nalthough effective Bayesian methods using them only started springing up fairly\nrecently.\nStill there is room for careful analysis. This thesis establishes that the\nposterior is consistent under suitable conditions on the prior and for any measurable regression function (see chapter 5 for details): an issue which none of\nthe \"Bayesian CART\" or \"Voronoi Partition\" authors address at all. This thesis also gives an explicit Markov chain Monte Carlo algorithm (see section 3.4).\nBroadly speaking it is a fairly standard birth-death Markov chain as considered by Geyer and Moller [38], but the technicalities of the analysis seem to\nbe somewhat different. This thesis proceeds to show in detail that it satisfies\ndetailed balance by direct self-contained argumentation; further, the chain is\nshown to have an ergodicity property (see section 3.10). These considerations\nare often glossed over in modern writing.\nOn the more practical side, chapter 4 scrutinizes the behavior of the posterior mean estimate under a variety of carefully designed simulation experiments. These experiments both serve to analyze the posterior mean and to\ngive insight into the relationship between Bayesian methods and their classical counterparts. See for example the discussion of CART and bagging in\nsubsection 4.1.3.\n\n\fCHAPTER 2. LITERATURE\n\n2.3\n\n20\n\nOther Approaches\n\nThe literature on classification and regression methods is huge; the interested\nreader is urged to consult good modern books on the subject like The Elements\nof Statistical Learning, by Hastie, Tibshirani, and Friedman [43]. The following\nparagraphs outline some of the methods that have had the most impact upon\nthe author.\nIn the statistics literature, classical approaches to the classification and binary regression problem include logistic regression, Fisher's discriminant analysis, and projection pursuit methods. Logistic regression specifies that the\nsuccess probability regression function is such that its log-odds follows a linear\nmodel with a user specified basis (e.g. by using polynomial or spline functions\nof the covariate-data) and estimates the parameters by maximum likelihood.\nModel selection is commonly performed using classical methods to select a subset of the covariate variables. Fisher's discriminant analysis finds a hyperplane\nwhich \"optimally\" separates the two classes using a within versus between\nvariance criterion. Projection pursuit seeks an interesting linear (or sometimes\nnonlinear) projection of the covariate-data onto a lower dimensional subspace\n(e.g. R). Various criteria have been proposed to define \"interesting,\" some of\nwhich are suitable for the classification problem. Each of these methods has\nundergone a variety of generalizations and tweaks to address a wider range of\nproblems over the years.\nThe first general method to solve the classification problem automatically\nwas the k-nearest neighbor approach [12]. k-Nearest neighbor estimates are\nknown to be universally consistent if k = k(n) \u2191 \u221e slowly enough [21]. Their\n\nconvergence, however, especially in high dimensional problems, can be slow in\npractice [36].\nLocal regression methods are a clever extension of this approach. To predict\n\nat a given point, instead of averaging the values given at the neighbors, they\nfit a low-order linear model to a locally-weighted version of the data set [10].\n\n\fCHAPTER 2. LITERATURE\n\n21\n\nTrees [4] and neural nets [56] differ in that they search through a globallyparametrized class of functions. In all of these methods, cross-validation is\noften employed to estimate frequentist \"out-of-sample\" performance and select a regularization parameter which governs the trade-off between bias and\nvariance [43].\nWavelet methods are in some ways a compromise between the local and the\nglobal approaches mentioned above. They fit an explicit global linear model\nto the data, but the basis elements in this model are carefully constructed to\nmaintain \"localization\" (in space and frequency domains). They boast powerful asymptotic compression and approximation properties, computationally\nefficient transforms, and can employ special thresholding methods which \"optimally\" choose which coefficients in the model are kept [26]. However, their\npractical use seems to remain concentrated on the case of regularly-spaced\nregression data. Some recent papers address this shortcoming [14].\nSupport vector machines (SVMs) [65] employ a \"kernel-trick\" to reduce\nconsideration of a certain globally-parametrized model class to consideration\nof an equivalent linear model class in an abstract Hilbert space. The estimated\ndecision rule corresponds to the solution of a convex optimization problem.\nThis objective function still involves an unknown regularization parameter. In\npractice, this parameter is often chosen by cross-validation, but, in principle, it\ncan be chosen through consideration of the structural risk minimization (SRM)\nparadigm. The advantage of using the SRM paradigm is that one obtains\nprovably valid confidence statements about the error rate that will obtain on\nfuture data. Moreover, these confidence bounds improve at an exponential rate\nin the number of data points. With realistic sample sizes, however, the bounds\nare often too crude to be of practical use. There are hidden connections between\nSVMs and (1) Bayesian methods employing Gaussian-process priors on the\nregression function (including the generalized spline methods of Wahba [66, 67])\n(2) projection pursuit regression [11].\n\n\fCHAPTER 2. LITERATURE\n\n22\n\nBagging [3] and boosting [34] are meta-algorithms that \"boost\" the performance other classification algorithms (especially trees) by taking carefully\nchosen weighted averages of the results of the boosted (respectively, bagged) algorithm. There are close connections between boosting and the Lasso penalty,\nwhich itself is closely related to the least angle regression method (LARS) [30].\n\n\fChapter 3\nComputing the Posterior\nThis chapter describes a Markov chain Monte Carlo (MCMC) algorithm that\ncan be used to (approximately) draw samples from the posterior of the onedimensional random step function prior \u03c0. This is essential, because for a\ncomplex prior like \u03c0, analytical evaluation of properties of the prior is intractable. All computation about the posterior, therefore, is made through\n(approximately) generating a large sample from it. Before describing these\nalgorithms, it is natural to review the prior and then derive a more refined\nmathematical expression for the posterior. This exercise has the side effect of\nsuggesting a more efficient sampling scheme. An informal sketch of the MCMC\nalgorithm is then given in section 3.4. Additionally, section 3.5 explains an efficient way to use these samples to calculate the posterior mean. The interested\nreader is invited to download an implementation of these algorithms and others\nfrom the author's web page.\nTo define the algorithm more mathematically, a brief review of Markov\nchains and the Metropolis-Hastings algorithm for general state spaces is given.\nSection 3.7 gives a simple example of such a chain. Section 3.8 gives a mathematical treatment of the more complicated MCMC algorithm that was only\nsketched previously. It also verifies that the Markov chain satisfies detailed balance with respect to the posterior. This is done mainly to provide a thorough\n23\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n24\n\nand relatively self-contained theoretical analysis of the Markov chain.\nMany of these calculations are essentially of the same type as those considered by Green [41]. Indeed, the birth-death move that I employ is essentially\nequivalent to the ones that he calls a \"reversible jump.\" The verification of\ndetailed balance is simple using his results. One need only observe that the\ntransformations involved in these jumps essentially only permute coordinates;\nconsequently the absolute value of the determinant of the Jacobian of this\ntransformation is identically 1. Section 3.9 reviews two theoretical results that\ngive sufficient conditions for a Markov chain to produce the intended ergodic\nsequence. Section 3.10 shows that these results are applicable so that, for\nexample, the posterior mean that is computed will indeed approximate the\nintended curve.\n\n3.1\n\nSpecification of the Prior in One Dimension\n\nA review of notation and the prior is in order. Section 1.2 gives an informal\ndescription of the prior \u03c0 and section 5.2 gives a more formal specification of\nthe prior \u03c0 and its parametrized version \u03c0 \u2032 . The parameter space \u0398 = \u222a\u221e\nk=1 \u0398k\n\nwhere \u0398k parameterizes the class of functions f : [0, 1] \u2192 [0, 1] that have k\n\nlocally-constant regions. Any such function is (essentially) determined by two\nvectors s and v. Vector s lists the k values that the function takes on each\nregion (as enumerated from left to right). Vector v lists (in no particular order)\nthe k \u2212 1 locations at which the function jumps. This explains the definition:\n\u0398k := {(k, v, s) : v \u2208 [0, 1]k\u22121 , s \u2208 [0, 1]k }\n\n(3.1)\n\n\u03981 is a special case because there are no splits in a function that is everywhere\nconstant. Define \u03981 = {(1, \u226c, s) : s \u2208 [0, 1]1 }, where the symbol \u226c represents\n\nan empty list (which is not considered equivalent to an empty set). This is\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n25\n\nconsistent with the former definition of \u0398k if one allows the notation: [0, 1]0 \u2261\n\n{ \u226c }.\n\nTo specify the prior \u03c0, first select a probability distribution \u03ba on N. \u03ba\n\nrepresents the a priori distribution of the number of regions in the unknown\nd\n\nfunction. In the introduction, the choice \u03ba = Geometric( 21 ) was suggested.\nAssume that \u03ba(k) > 0 for all k \u2208 N, where (technically) \u03ba(k) is shorthand\n\nfor \u03ba({k}). To pick a value \u03b8 \u2208 \u0398 from the (parametrized) prior \u03c0 \u2032 , first\n\ndraw K \u223c \u03ba. Then, if K = k, draw S = s uniformly from [0, 1]k and draw\n\nV = v uniformly from [0, 1]k\u22121. Form \u03b8 = (k, s, v) \u2208 \u0398k . This completes the\n\ndescription of \u03c0 \u2032 .\n\nFor a given point \u03b8 \u2208 \u0398k it is convenient associate a number of objects.\n\nLet k(\u03b8), v(\u03b8), and s(\u03b8) stand for the k, v, and s parts of \u03b8 respectively. Let\nv(i) denote the i'th ordered value of v. Additionally, for \u03b8 = (k, v, s) \u2208 \u0398k\n\nassociate the function f\u03b8 whose splits points and values are determined by v\nand s. Specifically, for k > 1, let I1 = [0, v(1) ), I2 = [v(2) , v(3) ), . . . , Ik\u22121 =\n[v(k\u22122) , v(k\u22121) ), Ik = [v(k\u22121) , 1]. If k = 1, just let I1 = [0, 1]. Take f\u03b8 (x) =\nPk\ni=1 si 1x \u2208 Ii .\nI have chosen to work with uniform distributions on the splits and function\nvalues. Both of these choices could be varied, e.g. by using one-dimensional\nDirichlet distributions for the split locations and using Beta distributions for\nthe function values where, perhaps, the parameters of the Beta distribution\nare allowed to depend on spatial position. Diaconis and Freedman [25] adopt\nthis level of generality (for the success probability prior), but I have not found\nit useful. What would be useful (but is avoided for simplicity of presentation)\nis to extend from binary classification to the multi-class case. This can be\ndone by generalizing the Beta(1, 1) prior into a discrete Dirichlet prior on the\nclass probabilities. Allowing dependencies among the parameters would more\nsubstantially complicate the analysis.\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n3.2\n\n26\n\nRepresenting the Posterior\n\nThe posterior is just the result of conditioning the prior on the data. The\ndata is the list D := (x1 , y1 ), . . . , (xn , yn ) where for i from 1 to n, xi \u2208 [0, 1]\n\nrepresents where the i'th point occurred and yi \u2208 {0, 1} represents whether the\n\nBernoulli(f (xi )) \"coin\" came up heads or tails. Denote the posterior distribution on \u03b8 given the data by \u03c0\u25e6\u2032 :\n\u03c0\u25e6\u2032 (d\u03b8) := \u03c0 \u2032 (d\u03b8 | D)\n\n(3.2)\n\nProvided that the denominator is non-zero and finite (which it will be) the\nposterior \u03c0\u25e6\u2032 has a density \u03c6(\u03b8) = \u03c6(\u03b8; D) with respect to the prior \u03c0 \u2032 , so that\n\n\u03c0\u25e6\u2032 (d\u03b8) = \u03c6(\u03b8)\u03c0 \u2032 (d\u03b8), where:\n\nL(\u03b8)\nL(\u03b8\u2032 )\u03c0 \u2032 (d\u03b8\u2032 )\n\u0398\n\n\u03c6(\u03b8) = R\n\n(3.3)\n\nThe likelihood function L(\u03b8) = L(\u03b8; D) is defined by:\nL(\u03b8) :=\n\nn\nY\ni=1\n\nf\u03b8 (xi )yi (1 \u2212 f\u03b8 (xi ))1\u2212yi\n\n(3.4)\n\nPk\n\n\u2208 Ij , where the intervals Ij\nimplicitly depend \u03b8, but only through v. To evaluate f\u03b8 (xi ), then, is simply a\nRecall that for \u03b8 = (k, v, s), f\u03b8 (x) =\n\nj=1 sj 1x\n\nmatter of determining for which value j from 1 to k, xi \u2208 Ij and then retrieving\n\nthat sj . Call the value of j for which x \u2208 Ij , J(x; \u03b8). Then f\u03b8 (xi ) = s(\u03b8)J(xi ;\u03b8) .\n\nConsequently, L is simply a certain product of terms of the form s(\u03b8)j or\n1 \u2212 s(\u03b8)j . To collect these together, define:\nNj1 = Nj1 (\u03b8; D) = (# of data points in Ij labeled 1)\n\nNj0 = Nj0 (\u03b8; D) = (# of data points in Ij labeled 0)\n\n(3.5)\n(3.6)\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n27\n\nSo that for \u03b8 \u2208 \u0398k , (suppressing the dependence on \u03b8 and D from the right\n\nhand side) L(\u03b8) can be expanded as:\nL(\u03b8) =\n\nk\nY\nj=1\n\nN1\n\n0\n\nsj j (1 \u2212 sj )Nj\n\n(3.7)\n\nNotice, then, that for fixed model number k and change-point locations v,\nL(\u03b8) only depends on the data through the (conditionally) sufficient statistics\nNj1 and Nj0 for j = 1, . . . , k. Moreover, for \u03b8 \u2208 \u0398k , L is simply the product of\nk different binomial likelihood functions. This is intuitively obvious: If I have\n\nalready decided exactly where the change-points are, the only remaining parameters are the success probabilities s. Furthermore, according to the model,\nif I get a data-point (x, y), the x necessarily lands in some interval Ij and, then,\nthe y is just the result of flipping an (independent) sj coin.\nAlso notice that under the prior the Sj 's are independent U(0, 1) random\nvariables. From the above discussion, it is apparent that if we condition on\nK = k and V = v, the data simply tell us how many times each of the k\n\"coins\" with success probabilities s1 through sk came up \"heads\" and \"tails\"\nas they were (collectively) flipped n times. Consequently, under the posterior,\nconditioned on K = k, V = v, and on the values of Nj1 and Nj0 , each Sj is an\nindependent Beta(Nj1 + 1, Nj0 + 1) random variable. We recover this fact by\ndirect calculation shortly.\nThese observations can be used to motivate an MCMC scheme that is substantially more efficient than the naive one that randomly changes k, v, and\ns in the standard Metropolis-Hastings fashion. Namely, we will only have to\nuse Markov Chain Monte Carlo steps in order to sample (k, v) pairs from their\nmarginal under the posterior. If desired, we can then create a complete sample,\nincluding a realization of s by sampling from the k independent Beta random\nvariables whose parameters were explained above. To compute the posterior\nmean, s need not be simulated at all. The mean of the Beta distribution can be\n\n\f28\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\ncomputed analytically. This avoids substantial Monte Carlo error. For details,\nsee sections 3.4 and 3.5.\nContinuing with the calculations, write \u03b8 = (k, v, s), and let C = {k} \u00d7 A \u00d7\n\nB = {\u03b8 \u2208 \u0398k : v \u2208 A, s \u2208 B} for A, B measurable subsets of [0, 1]k\u22121 , [0, 1]k\nrespectively. Compute:\n\n\u03c0\u25e6\u2032 (C)\n\n\u2032\n\n= \u03c0 (C|D) =\n\nZ\n\n\u03c6(\u03b8)\u03c0 \u2032 (d\u03b8)\n\n(3.8)\n\nL(\u03b8)\u03c0 \u2032 (d\u03b8)\nL(\u03b8)\u03c0 \u2032 (d\u03b8)\n\u0398\n\n(3.9)\n\nRC\n\n= RC\nZ\n\n\u2032\n\nL(\u03b8)\u03c0 (d\u03b8) = \u03ba(k)\n\nC\n\n= \u03ba(k)\n\nZ\n\nZ\n\nv\u2208A\n\nv\u2208A\n\nZ\n\nZ\n\nL ((k, v, s)) dsdv\n\n(3.10)\n\ns\u2208B\nk\nY\n\nN 1 (v)\n\nsj j\n\ns\u2208B j=1\n\n0\n\n(1 \u2212 sj )Nj (v) dsdv\n\n(3.11)\n\nIf, in particular, B is the rectangle [a1 , b1 ] \u00d7 * * * \u00d7 [ak , bk ] we get:\nZ\n\nL(\u03b8)\u03c0 \u2032 (d\u03b8) = \u03ba(k)\n\nC\n\nZ\n\nv\u2208A\n\n\" k Z\nY\nj=1\n\nbj\n\nu\naj\n\nNj1 (v)\n\n(1 \u2212 u)\n\nNj0 (v)\n\n#\n\ndu dv\n\n(3.12)\n\nThe inner integral is a Beta integral. Consider the special case in which B =\n[0, 1]k (i.e. C = {k} \u00d7 A \u00d7 [0, 1]k ). Then,\nZ\n\nC\n\n\u2032\n\nL(\u03b8)\u03c0 (d\u03b8) = \u03ba(k)\n\nZ\n\n\u03c1k (v)dv\nv\u2208A\n\n(3.13)\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n29\n\nWhere \u03c1k (v) and also \u03c1k are defined for k \u2265 2 by:\nk\nY\n\nNj1 (v)! Nj0 (v)!\n\u03c1k (v) :=\n(Nj1 (v) + Nj0 (v) + 1)!\nj=1\nZ\n\u03c1k :=\n\u03c1k (v)dv\n\n(3.14)\n(3.15)\n\nv\u2208[0,1]k\u22121\n\nFor k = 1, v =\u226c and I1 = [0, 1] so that N11 and N10 are the total number\nof heads and tails respectively. In this case define \u03c11 = \u03c11 (\u226c) = N11 !N10 !/(N11 +\nN10 + 1)!.\nThe posterior probability of the k'th model is readily computed:\n\u03c0\u25e6\u2032 (\u0398k ) = \u03ba(k)\u03c1k /c\nWhere the normalizing constant c is the sum:\n\n(3.16)\n\nP\u221e\n\nj=1 \u03ba(j)\u03c1j .\n\nNow is a good time to notice that for any k and v \u2208 [0, 1]k\u22121, and any data\n\nset D, \u03c1k (v) and \u03c1k are both positive and \u2264 1. Consequently, the same holds\n\nfor c.\n\nLet \u03bbj denote Lebesgue measure on [0, 1]j . In the special case that j = 0,\nlet \u03bb0 denote counting measure on the set {\u226c}. Then the posterior density of\nthe change-points v with respect to \u03bbk\u22121 , given that model k holds is \u03c1k (v)/\u03c1k .\n\nInformally:\n\u03c0\u25e6\u2032 (V \u2208 dv | K = k) = \u03c1k (v)/\u03c1k \u03bbj (dv)\n\n(3.17)\n\nFinally, the posterior probability that S is in rectangle B = [a1 , b1 ] \u00d7 * * * \u00d7\n\n[ak , bk ], given that model k holds and that the change-points are given by v is\n\n\f30\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\nindeed the same as k independent Beta's:\nk\n\n\u03c0\u25e6\u2032 (S\n\n1 Y\n\u2208 B | K = k, V = v) =\n\u03c1k (v) j=1\n=\n\nk\nY\nj=1\n\nZ\n\nbj\n\naj\n\n1\n\n0\n\nuNj (v) (1 \u2212 u)Nj (v) du\n\n\u0001\nP Beta(Nj1 (v) + 1, Nj0 (v) + 1) \u2208 [aj , bj ] du\n\n(3.18)\n\n(3.19)\n\nConsequently, if sbj denotes the expected value of Sj under the posterior,\n\ngiven that model k holds and that the change-points are given by v, then sbj is\njust the mean of the Beta(Nj1 (v) + 1, Nj0 (v) + 1) distribution:\n\n3.3\n\nSetup\n\nNj1 (v) + 1\nsbj = 1\nNj (v) + Nj0 (v) + 2\n\n(3.20)\n\nThis section sets up some basic definitions and ideas that underlie the algorithm\ndescribed in the next section. The first definition, gives a new meaning to\nthe symbol X which will be used throughout the remainder of this chapter.\n\nElsewhere, X still stands for the covariate space. This should not introduce\nany confusion.\n\nWrite Xk for the parameter space formed from (k, v) pairs with v \u2208 [0, 1]k\u22121\n\nand build up the full parameter space X by taking the countable union:\nX := \u222a\u221e\nk=1 Xk\n\nXk := {(k, v) : v \u2208 [0, 1]k\u22121}\n\n(3.21)\n(3.22)\n\nAgain, X1 is a special case. Define x0 := (1, \u226c) and let X1 = {x0 }, so that X1\n\nis a singleton.\n\nFor convenience, let k(x) stand for the k-part of x and let x stand for the\nv-part of x. Bear in mind the nuisance that for x \u2208 Xk , x \u2208 [0, 1]k\u22121 . Extend\n\n\f31\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\nthis definition to sets; i.e. for a subset C \u2282 Xk , define C as {x : x \u2208 C}.\n\nFor future reference, endow X with the \u03c3-algebra B generated by sets of the\n\nform k \u00d7 B where k \u2265 2 and B is some (Borel) measurable subset of [0, 1]k\u22121\n\nand by the singleton set X1 . Let \u03c4 denote the extension of Lebesgue measure\n\nto X . That is, for k \u2265 2 and B a (Borel) measurable subset of [0, 1]k\u22121 define\n\nthe \u03c4 measure of a set C = k \u00d7 B \u2282 Xk , as the k \u2212 1-dimensional Lebesgue\nmeasure of C = B. To account for k = 1, let \u03c4 (X1 ) = 1.\n\nFinally, combining the results in Equation 3.16 and Equation 3.17, the distribution under \u03c0\u25e6\u2032 of the random point X = (K, V ) in X can be computed. In\nthe present notation, X has the density \u03c6(x) with respect to \u03c4 :\n\u03c6(x) := \u03ba(k)\u03c1k (v)/c\n\n3.4\n\n(3.23)\n\nAn MCMC Algorithm\n\nThis section contains a description of a (randomized) computational algorithm\nto produce a random sequence \u03b81 , . . . , \u03b8M drawn from the posterior distribution\n\u03c0\u25e6\u2032 . A more formal version of this algorithm will be developed in section 3.8.\nFinally in section 3.10, it will be shown that the generated sequence has an\nergodicity property. The main consequence of this is that if g is some measurR\nable function with |g(\u03b8)|\u03c0\u25e6\u2032 (d\u03b8) < \u221e, one can use the average value of g on\nthe sampled values to approximate the integral, in the sense that:\nZ\nM\n1 X\ng(\u03b8j ) \u2192 g(\u03b8)\u03c0\u25e6\u2032 (d\u03b8)\nM j=1\n\n(3.24)\n\nThe algorithm exploits the observation made in section 3.2 that under the\nposterior, conditioned on K = k, V = v, and on the values of Nj1 and Nj0 ,\neach Sj is an independent Beta(Nj1 + 1, Nj0 + 1) random variable. Since Beta\nrandom variables are easy to simulate and work with analytically, it suffices\nto simulate (k, v) pairs from the (marginalized) posterior, instead of the full\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n32\n\n\u03b8 = (k, v, s).\nEssentially, the MCMC algorithms that are developed in this chapter are a\nminor variation of the usual birth-death MCMC approach that is often used to\nsimulate point processes. This approach is described by Geyer and M\u00f8ller [38].\nThey claim geometric ergodicity for their Markov chain, under suitable restrictions on the sampling density. One technical distinction from these approaches\nis that when simulating a point process, the fundamental object is a subset of\npoints {v1 , . . . , vk\u22121}; the theoretical treatment given in this chapter considers\n\u0001\ninstead mathematical objects of the form k, (v1 , . . . , vk\u22121 ) .\nAs far as the computations are concerned, though, there is no distinction\n\nbetween these formally different objects. Both could be represented on the\ncomputer operationally as a simple list of numbers called x with each number\nin the list specifying the location of a particular change-point. The algorithm\nmerely assumes that it can call a function \u03c6(x) that evaluates to the positive\nreal number defined by Equation 3.23.\nIn particular, one can use k(x) to find out that there are k \u2212 1 elements in\n\nthis list. Furthermore the computer has no problem removing elements from\nthe list all the way down to the empty list, or (ideally) adding elements one-byone indefinitely. To agree with the notation of the previous section, if k(x) = k,\nthen for 1 \u2264 j \u2264 k \u2212 1, write xj for the j'th element of the list. Write x0 for\n\nthe empty list.\n\nLet M be the number of Monte Carlo samples that are desired. The algorithm simulates the workings of a Markov chain and generates the realizations:\nx1 , x2 , . . . , xM . It is assumed that for j from 1 to 5, pj is a positive number,\nand that these numbers sum to one. These represent the mixture probabilities\nwith with 5 component Markov chains are combined. For my computations,\nthrough a mixture of intuition and trial-and-error, I chose p as in Table 3.1.\nThese values are by no means optimal. (The irregular numbers quoted here\nresult from standardizing simpler ones so that they add to 1.)\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n33\n\np1\np2\np3\np4\np5\n0.1429 0.1429 0.2381 0.0476 0.4762\nTable 3.1: MCMC Mixture Probabilities\nMain MCMC Algorithm\nSet x = x0 .\nFor i ranging from 1 to M, repeat the following steps:\n1. Pick J at random from 1 to 5 with the probabilities p1 through p5\nrespectively.\n2. Pick a \"proposal\" point y by following the subroutine specified in Action J (defined below).\n3. Calculate \u03b1 = min(1, \u03c6(y)/\u03c6(x)).\n4. With probability \u03b1, set x = y.\n5. Set xi = x.\n\nAction 1: Add or Delete a Random Coordinate\nSet y = x.\nFlip a fair coin.\nIf heads:\nGenerate U uniformly at random on [0, 1].\nAdd U to the end of y.\nReturn y.\nIf tails:\nSet y = x.\nIf y is empty:\nReturn y.\nOtherwise:\nPermute y randomly.\nDelete the last entry from y.\nReturn y.\nEnd if\nEnd if\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n34\n\nAction 2: Randomize a Coordinate\nSet y = x.\nIf y is empty:\nReturn y.\nOtherwise:\nPermute y randomly.\nGenerate U uniformly at random on [0, 1].\nReplace the last coordinate of y with U.\nReturn y.\nEnd if\n\nAction 3: Shift a Coordinate\nSet y = x.\nIf y is empty:\nReturn y.\nOtherwise:\nPermute y randomly.\nSet U1 = the last element of y.\nSet U2 = U1+a random normal with mean 0 and standard deviation 0.1.\nIf U2 is in [0, 1]:\nReplace the last coordinate of y with U2.\nEnd if\nReturn y.\nEnd if\n\nAction 4: Randomize All Coordinates\nSet y = x.\nIf y is empty:\nReturn y.\nOtherwise:\nFor each coordinate of y do:\nGenerate U uniformly at random on [0, 1].\nReplace the current coordinate of y with U.\nEnd for\nReturn y.\nEnd if\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n35\n\nAction 5: Shift All of the Coordinates Very Slightly\nSet y = x.\nIf y is empty:\nReturn y.\nOtherwise:\nFor each coordinate of y do:\nSet U1 = the current coordinate of y.\nSet U2 = U1+a random normal with mean 0 and standard deviation 0.01.\nIf U2 is in [0, 1]:\nReplace the current coordinate of y with U2.\nOtherwise:\nContinue to the next coordinate.\nEnd if\nEnd for\nReturn y.\nEnd if\nIf desired, the resulting x1 , . . . , xM can be randomly augmented into a\nsequence \u03b81 , . . . , \u03b8M . To do so, simply generate all the necessary Beta random variables in order to sample S from its conditional distribution (c.f.\nEquation 3.18).\n\n3.5\n\nPosterior Mean Calculation\n\nThere are many potential uses of for the sample of \u03b8 values that can be approximately drawn from the posterior using the algorithm in the previous section.\nFor example, for each sampled \u03b8, a plot of the corresponding f\u03b8 can be made,\nand inspecting some of these can give some idea about how confident to be\nabout the shape of the unknown regression function.\nThis section, though, concentrates on estimating the mean of these functions. Call the resulting function the posterior mean, fb. It represents the\n\nposterior's best estimate (in an L2 sense) of the unknown regression function.\nMore formally, define the value of fe at a point u \u2208 [0, 1] as the expected value\n\n\f36\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\nof f\u03b8 (u) under the posterior on \u03b8:\n\nfb(u) =\n=\n\nZ\n\nZ\u0398\nX\n\nf\u03b8 (x)\u03c0\u25e6\u2032 (d\u03b8)\n\n(3.25)\n\nsbJ(u;x) \u03c6(x)\u03c4 (dx)\n\n(3.26)\n\nWhere b\nsj was defined by Equation 3.20, \u03c6(x) was defined by Equation 3.23,\nand \u03c4 was defined shortly before \u03c6.\n\nThe algorithm from section 3.4 can be used to approximately generate a\n\nsample x1 , . . . , xM from \u03c6(x)\u03c4 (dx) and then estimate this integral by:\n\n3.6\n\nM\n1 X\nsbJ(u;x)\nM i=1\n\n(3.27)\n\nMetropolis-Hastings Markov Chains on General Spaces\n\nWhere does the algorithm described in section 3.4 come from? This section addresses the MCMC approach and begins a description of the larger framework\nwithin which algorithms like this one can be derived and evaluated.\nGenerally, MCMC techniques suggest how to formulate algorithms (more\nspecifically Markov chains) that may be useful in order to sample a stationary\nergodic sequence that converges to a given stationary distribution. This subject\nis very broad and active. For a review of the main ideas, see Tierney [64] or\nLiu [50]. MCMC techniques have opened up to numerical investigation a wide\nvariety of Bayesian procedures, especially with the advent of the Gibbs sampler,\nthe Metropolis-Hastings algorithm [51, 44], and its extension to the problem\nof \"model determination\" through \"reversible jump\" MCMC (Green [41]).\nIn very general terms (following [1]), the Markov chain setup is as follows.\nLet \u03c0 be a probability distribution on a measurable space (X , B). Let P be a\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n37\n\ntransition probability function on this space, that is, P is a function on X \u00d7 B\n\nsuch that, for each x \u2208 X , P (x, *) is a probability measure on (X , B) and,\nfor each C \u2208 B, P (*, C) is a measurable function on (X , B). The Markov\n\nchain X0 , X1 , X2 , . . . is generated as follows. We fix a starting point X0 = x0 ,\ngenerate an observation X1 from P (X0 , *), generate an observation X2 from\n\nP (X1, *), and so on.\n\nP will be constructed to obey detailed balance with respect to \u03c0. Namely:\nZ\n\nZ\n\nP (x, B)\u03c0(dx) =\n\nA\n\nB\n\nP (x, A)\u03c0(dx) for every A, B \u2208 B\n\n(3.28)\n\nThis condition is very convenient because although it will be easy to construct\nchains that satisfy it, it is also powerful. In particular, (by choosing B = X )\n\nit implies that \u03c0 is an invariant measure for the Markov chain, that is,\n\u03c0(A) =\n\nZ\n\nX\n\nP (x, A)\u03c0(dx) for every A \u2208 B\n\n(3.29)\n\nThe goal is to choose P so that \u03c0 is the unique invariant measure; and, moreover\nthat the Markov chain will produce an ergodic sequence of observations from\n\u03c0. For this goal, detailed balance is a useful (although not necessary) \"first\nstep.\"\nSuppose Pe is some transition probability function which (presumably) does\nnot satisfy reversibility with respect to \u03c0. Let \u03bc\ne(dx, dy) := \u03c0(dx)Pe(x, dy).\nSuppose that \u03bc\ne is absolutely continuous with respect to some symmetric \u03c3-\n\nfinite measure \u03bc. Specifically, suppose that \u03bc is a measure on the measurable\n\nspace (X \u00d7X , \u03c3(B \u00d7B)) that satisfies \u03bc(A\u00d7B) = \u03bc(B \u00d7A) for all A, B \u2208 B. In\n\nsimple cases, one can choose \u03bc to be a product measure; for example \u03c0(dx)\u03c0(dy)\nor \u03bb(dx)\u03bb(dy) where perhaps \u03c0 has a density with respect to \u03bb. If no such\nmeasure is readily available, one may take \u03bc(dx, dy) = 21 \u03bc\ne(dx, dy) + 21 \u03bc\ne(dy, dx);\n\ne(A \u00d7 B) + 21 \u03bc\ne(B \u00d7 A) for A, B \u2208 B. Let pe be a version\ni.e. \u03bc(A \u00d7 B) = 21 \u03bc\n\nof the Radon-Nikod\u00fdm derivative of \u03bc\ne with respect to \u03bc so that \u03bc\ne(dx, dy) =\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n38\n\npe(x, y)\u03bc(dx, dy). If pe(x, y) happens to be symmetric; i.e. pe(x, y) = pe(y, x) for\nall x, y \u2208 X , then Pe already satisfies detailed balance with respect to \u03c0. To\nverify this, compute that for every every A, B \u2208 B:\nZ\n\nA\n\nPe(x, B)\u03c0(dx) =\n=\n\nZ\n\nx\u2208A\n\nZ\n\nZ\n\n(x,y)\u2208A\u00d7B\n\n=\n\nZ\n\n(x,y)\u2208A\u00d7B\n\n=\n\nZ\n\n(x,y)\u2208B\u00d7A\n\n=\n\nZ\n\nx\u2208B\n\n=\n\nZ\n\nB\n\n\u03c0(dx)P (x, dy)\n\n(3.30)\n\npe(x, y)\u03bc(dx, dy)\n\n(3.31)\n\ny\u2208B\n\nZ\n\npe(y, x)\u03bc(dx, dy)\n\n(3.33)\n\n\u03c0(dx)P (x, dy)\n\n(3.34)\n\npe(x, y)\u03bc(dx, dy)\n\n(3.32)\n\ny\u2208A\n\nPe(x, A)\u03c0(dx)\n\n(3.35)\n\nWhen pe is not symmetric, there is no trouble in constructing the closely\n\nrelated symmetric function p(x, y) = min(e\np(x, y), pe(y, x)). Does this suggest\nhow to construct a probability transition function P based on Pe that satisfies\n\ndetailed balance with respect to \u03c0? Yes, fortunately it does. Define:\n\uf8f1\n\uf8f2min(1, pe(y, x)/e\np(x, y)) if pe(x, y) > 0\n\u03b1(x, y) =\n\uf8f31\nif pe(x, y) = 0\n\n(3.36)\n\nThen (check) p(x, y) = \u03b1(x, y)e\np(x, y) for all x, y \u2208 X . This suggests defining\nQ(x, dy) = \u03b1(x, y)Pe(x, dy), so that \u03c0(dx)Q(x, dy) = \u03b1(x, y)e\np(x, y)\u03bc(dx, dy) =\np(x, y)\u03bc(dx, dy). The only problem is that Q(x, *) is not a probability (in\n\ngeneral), but a sub-probability. To account for the \"forgotten\" mass, set h(x) =\n\n1 \u2212 Q(x, X ) for all x \u2208 X . And define P (x, dy) = Q(x, dy) + h(x)\u03b4x (dy). Here,\n\n\f39\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\n\u03b4x (*) stands for the Dirac measure at x. In expanded form, this defines:\nP (x, dy) = \u03b1(x, y)Pe(x, dy) +\n\n\u0014Z\n\nX\n\n\u0015\n\n(1 \u2212 \u03b1(x, z))Pe(x, dz) \u03b4x (dy)\n\n(3.37)\n\nIn conclusion, one can now easily verify that this defines a probability transition function P which satisfies detailed balance with respect to \u03c0 and which\nis a simple modification of Pe. Indeed, P is simply a modification of Pe that\nsometimes \"holds\" instead of taking the transition that Pe proposes. Suppose\n\nthat Y = y is a particular value drawn from Pe(x, *). That is, suppose that Pe\n\nhas \"proposed\" the transition from x to y. Then P \"accepts\" this transition\nwith probability \u03b1(x, y), but holds with probability 1 \u2212 \u03b1(x, y). Furthermore,\n\nbecause \u03b1(x, y) only depends on the ratio pe(y, x)/e\np(x, y) it is sufficient to be\n\nable to compute pe(x, y) up to an unknown constant factor.\n\nClearly, then, P is a computationally simple modification of Pe; the only\n\ncaveat is that \u03b1(x, y) may often be very small or even 0 so that in the extreme\n\ndegenerate case in which \u03b1 \u2261 0, P is the Markov chain that always holds.\nIndeed, this Markov chain is reversible with respect to any distribution, but\n\nit certainly does not serve the larger goal of producing an ergodic sequence of\nrealizations from \u03c0. Similar problems can occur if Pe is not transitive or is oth-\n\nerwise unsuitable. For these reasons the ergodicity conditions from section 3.9\nare needed.\n\n3.7\n\nA Simple Markov Chain\n\nA simple example is in order to make these ideas more concrete. This chain\nwill not be as efficient (in practice) as the local-move Markov chain developed\nin the next section.\nIn words, this will be the chain that stays fixed (holds) at its current value\nx \u2208 X until a new value yX drawn from the prior is accepted; y will always\n\nbe accepted if y makes the data more likely (i.e. higher predictive probability\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n40\n\nunder our model); otherwise it is accepted with a probability reflecting the ratio\nof the predictive probabilities. In this way, the chain readily walks \"uphill,\"\nbut, with just the right probability (because of the detailed balance condition\nthat will be shown) it also walks \"downhill.\"\nRecall the notation from section 3.3 that defined the measurable space\n(X , B) and denote the posterior distribution on this space by \u03c0\u25e6\u2032 . For con-\n\nvenience, recall that for any x \u2208 X , x = (k, v) and write \u03ba(x) = \u03ba(k(x))\n\nand \u03c1(x) = \u03c1(v(x)) so that the prior \u03c0 on points y \u2208 X has density \u03ba(y)\nwith respect to \u03c4 . For any x \u2208 X and any B \u2208 B, let Pe be defined by\nR\nPe(x, B) = y\u2208B \u03ba(y)\u03c4 (dy). That is, Pe is the probability transition function\nthat (without reference to x) samples a new point y \u2208 X from the prior. Now\n\nexpand \u03c0\u25e6\u2032 (dx)P (x, dy) = \u03c6(x)\u03ba(y)\u03c4 (dx)\u03c4 (dy) = (\u03c1(x)/c)\u03ba(x)\u03ba(y)\u03c4 (dx)\u03c4 (dy).\nConveniently then, this distribution has a density with respect to the product measure \u03ba(x)\u03c4 (dx)\u03ba(y)\u03c4 (dy). To agree with the notation in the previous\nsection, let \u03bc denote this product measure and let pe(x, y) = \u03c1(x)/c. Recall\n\nthat each of these terms is always positive. Because of this and a convenient\ncancellation, the expression for \u03b1 becomes\n\n\u03b1(x, y) = min(1, \u03c1(y)/\u03c1(x))\nFinally, as before, define Q(x, dy) = \u03b1(x, y)Pe(x, dy), h(x) = 1 \u2212 Q(x, X ), and\nset P (x, dy) = Q(x, dy) + h(x)\u03b4x (dy).\n\n3.8\n\nA Local-Move Markov Chain\n\nThis section gives a formal definition of the Markov chain type algorithm that\nwas explained in section 3.4. It then shows that this chain satisfies detailed\nbalance with respect to \u03c0\u25e6\u2032 . To introduce this more useful, but more complicated Markov chain on (X , B), some notation is needed. When j \u2265 2, and\n\nv \u2208 [0, 1]j , write v\u2212 for the vector (v1 , . . . , vj\u22121) \u2208 [0, 1]j\u22121 which leaves off the\n\n\f41\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\nlast coordinate of v. Consider some point x = (k, v) \u2208 X . Let g\u2193 (x) stand for\n\nthe point in X that is \"one level down\" from x with the last coordinate of v\n\nhaving been removed. That is, when k \u2265 3 and v = (v1 , . . . , vk\u22121 ) \u2208 [0, 1]k\u22121,\n\nlet g\u2193 (x) = (k \u2212 1, v\u2212 ). For x = (2, (v1 )) \u2208 X2 , let g\u2193 (x) = (1, \u226c). For\nx = (1, \u226c) \u2208 X1 , there is no further down to go, and so let g\u2193 (x) = x. Let\n\u03bbx\u2193 (*) = \u03b4g\u2193 (x) (*) denote the Dirac measure at g\u2193 (x).\n\nSimilarly, let g\u2191 (x, u) stand for the point in X that is \"one level up\" from\n\nx, where the last coordinate is filled in with u. That is, for k \u2265 2 and v =\n\n(v1 , . . . , vk\u22121 ), g\u2191 (x, u) = (k+1, (v1, . . . , vk\u22121 , u)). For x = (1, \u226c), let g\u2191 (x, u) =\n(2, (u)). Let \u03bbx\u2191 (*) denote the distribution of g\u2191 (x, U) where U is uniformly\ndistributed on [0, 1].\nLet \u03bbj (*) denote Lebesgue measure on Rj and for B \u2208 B where B \u2282 Xk , let\n\nB = {v \u2208 [0, 1]k\u22121 : (k, v) \u2208 B}.\n\nTo define the transition probability function P on (X , B) satisfying detailed\n\nbalance with respect to \u03c0\u25e6\u2032 , I first define various transition probability functions\nPej (x, dy); then, for a generic function \u03b1j (x, y), define:\nQj (x, dy) = \u03b1j (x, dy)Pej (x, dy)\n\n(3.38)\n\nNext the \u03b1j 's are chosen so that for every j, Qj satisfies the appropriate detailed\nbalance formula:\nZ\n\n\u03c0(dx)\nx\u2208A\n\nZ\n\ny\u2208B\n\nQj (x, dy) =\n\nZ\n\nx\u2208B\n\n\u03c0(dx)\n\nZ\n\nQj (x, dy)\n\n(3.39)\n\ny\u2208A\n\nIt is easily verified then that Pj = Qj (x, dy) + [1 \u2212 Qj (x, X )]\u03b4x (dy), is a\n\ntransition probability which satisfies detailed balance. Finally, set:P (x, dy) =\nP\nj pj Pj (x, dy).\nGenerally the proposals I consider are \"symmetric\" and so \u03b1j (x, y) will work\n\nout to be min(1, \u03c6(y)/\u03c6(x)) in each case. An exception is in Equation 3.86,\nbut when the proposal density is simple, (as it is in the case of interest) it also\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n42\n\nreduces to the previous case.\nTo begin, set Pe1 (x, dy) = 21 \u03bbx\u2191 (dy) + 21 \u03bbx\u2193 (dy). This transition probability\nfunction represents the chain that adds or deletes coordinates from the vector\nv randomly.\n\nTo choose \u03b11 , first compute the left and right hand sides of the detailed\nbalance equation for Q1 . Let A, B \u2208 B with A \u2282 Xj and B \u2282 Xk . There are\n\nthree cases of interest for j and k: (1) k = j + 1, j \u2265 2, (2) j = 1, k = 2, (3)\nj = 1, k = 1. These account for all the possibilities because if j < k, simply\n\nreplace the roles of j and k; if k > j + 1 or j = k 6= 1 both sides will evaluate\n\nto 0. If \u03b11 can be chosen to set the left and right sides equal for every such\ncase, detailed balance is proven because general A, B \u2208 B can be decomposed\n\ninto these component subsets.\n\nSuppose that k = j + 1, and calculate:\nZ\n\nx\u2208A\n\nZ\n\nQ1 (x, dy)\n\u03c0(dx)\ny\u2208B\nZ\nZ\n1\n1\n=\n\u03c6(x)\u03b11 (x, y)\u03c4 (dx)[ \u03bbx\u2191 (dy) + \u03bbx\u2193 (dy)]\n2\n2\nZx\u2208A Zy\u2208B\n1\n=\n\u03c6(x)\u03b11 (x, y)\u03c4 (dx)\u03bbx\u2191 (dy)\n2\nZx\u2208A Zy\u2208B\n1\n=\n\u03ba(j)\u03c1j (v)\u03b11 ((j, v), g\u2191((j, v), u))\u03bbj\u22121(dv)du\nv\u2208A u\u2208C(v;B) 2\nZ\n1\n=\n\u03ba(j)\u03c1j (w\u2212 )\u03b11 ((j, w\u2212 ), g\u2191((j, w\u2212 ), u))\u03bbj (dw)\nw\u2208D 2\nZ\n1\n\u03c6(g\u2193 (x))\u03b11 (g\u2193 (x), x)\u03c4 (dx)\n=\nx\u2208D 2\n\n(3.40)\n(3.41)\n(3.42)\n(3.43)\n(3.44)\n(3.45)\n\nWhere:\nC(v; B) = {u \u2208 [0, 1] : g\u2191 ((j, v), u) \u2208 B}\n\n(3.46)\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\nD = {w \u2208 [0, 1]j : w\u2212 \u2208 A, wj \u2208 C(w\u2212 ; B)}\n= {w \u2208 B : w\u2212 \u2208 A}\nD = {x = (k, w) \u2208 Ck : w \u2208 D}\n= {x \u2208 B : g\u2193 (x) \u2208 A}\n\n43\n\n(3.47)\n(3.48)\n\n(3.49)\n(3.50)\n\nSimilarly, compute:\nZ\n\nx\u2208B\n\nZ\n\n\u03c0(dx)\nQ1 (x, dy)\ny\u2208A\nZ\nZ\n1\n1\n=\n\u03c6(x)\u03b11 (x, y)\u03c4 (dx)[ \u03bbx\u2191 (dy) + \u03bbx\u2193 (dy)]\n2\n2\nZx\u2208B Zy\u2208A\n1\n=\n\u03c6(x)\u03b11 (x, y)\u03c4 (dx)\u03bbx\u2193 (dy)\nx\u2208B y\u2208A 2\nZ\n1\n=\n\u03c6(x)\u03b11 (x, g\u2193 (x))\u03c4 (dx)1g (x) \u2208 A\n\u2193\nx\u2208B 2\nZ\n1\n\u03c6(x)\u03b11 (x, g\u2193 (x))\u03c4 (dx)\n=\nx\u2208D 2\n\n(3.51)\n(3.52)\n(3.53)\n(3.54)\n(3.55)\n\nSuppose that j = 1, k = 2, so that A = {x0 } or A = where x0 = (1, \u226c) and\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n44\n\nany x \u2208 B is of the form (2, (v1 )). Then:\nZ\n\nx\u2208A\n\nZ\n\n\u03c0(dx)\nQ1 (x, dy)\ny\u2208B\nZ\nZ\n1\n1\n=\n\u03c6(x)\u03b11 (x, y)\u03c4 (dx)[ \u03bbx\u2191 (dy) + \u03bbx\u2193 (dy)]\n2\n2\nZx\u2208A Zy\u2208B\n1\n\u03c6(x)\u03b11 (x, y)\u03c4 (dx)\u03bbx\u2191 (dy)\n=\n2\nZx\u2208A y\u2208B\n1\n=\n1x0 \u2208 A \u03c6(x0 )\u03b11 (x0 , y)\u03bbx\u2191 0 (dy)\n2\nZy\u2208B\n1\n=\n1x0 \u2208 A \u03c6(x0 )\u03b11 (x0 , (2, v1 ))dv1\nv1 \u2208B 2\nZ\n1\n\u03c6(x0 )\u03b11 (x0 , x)\u03c4 (dx)\n=\nx\u2208D 2\n\n(3.56)\n(3.57)\n(3.58)\n(3.59)\n(3.60)\n(3.61)\n\nSimilarly, compute:\nZ\n\nx\u2208B\n\nZ\n\n\u03c0(dx)\nQ1 (x, dy)\ny\u2208A\nZ\nZ\n1\n1\n=\n\u03c6(x)\u03b11 (x, y)\u03c4 (dx)[ \u03bbx\u2191 (dy) + \u03bbx\u2193 (dy)]\n2\n2\nZx\u2208B Zy\u2208A\n1\n=\n\u03c6(x)\u03b11 (x, y)\u03c4 (dx)\u03bbx\u2193 (dy)\n2\nZx\u2208B y\u2208A\n1\n=\n\u03c6(x)\u03b11 (x, g\u2193 (x))\u03c4 (dx)1g (x) \u2208 A\n\u2193\nx\u2208B 2\nZ\n1\n\u03c6(x)\u03b11 (x, x0 )\u03c4 (dx)\n=\nx\u2208D 2\n\n(3.62)\n(3.63)\n(3.64)\n(3.65)\n(3.66)\n\nThe only remaining case to compute is where j = k = 1 and the only case\nof interest here is the one in which A = B = {x0 }. Even this is trivial, because\n\nthe left and right sides of this symmetric case must surely match.\n\nRecalling that \u03c6(x) > 0 for all x \u2208 X , for x, y \u2208 X detailed balance is\n\n\f45\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\nachieved upon defining:\n\u03b11 (x, y) = min(\u03c6(y)/\u03c6(x), 1)\n\n(3.67)\n\nTo define Pe2 additional notation is needed. For k \u2265 2, 1 \u2264 j \u2264 k \u2212 1 let\n\n\u03bbx*j (*) denote the distribution on (X , B) under which the j'th coordinate of the\n\nv in x = (k, v) is replaced with a uniform random variable. For k \u2265 2 and\nPk\u22121 x\n1\nx \u2208 Xk , let \u03bbx* (dy) = k\u22121\nj=1 \u03bb*j (dy), i.e. the probability distribution that\n\npicks a coordinate of the v in x randomly and then changes it to a uniformly\nrandom value. For x = x0 , let \u03bbx* (dy) = \u03b4x0 (dy).\nNow define Pe2 (x, dy) = \u03bbx* (dy). More generally, define:\n\nPe3 (x, dy) = \u03a8(x, y)\u03bbx* (dy) + h\u03a8 (x)\u03b4x (dy)\n\n(3.68)\n\nWhere \u03a8 is any (measurable) non-negative function on X \u00d7 X satisfying for\nR\nall x, y \u2208 X : (1) \u03a8(x, y) = \u03a8(y, x), (2) Xk \u03a8(x, y)\u03bbx* (dy) + h\u03a8 (x) = 1 for\n\nsome non-negative h\u03a8 (x). For example, if x = (k, vx ), y = (k, vy ), and \u03c6\u03c3\n\nrepresents the univariate normal density with standard deviation \u03c3 > 0, take\nR\n\u03a8(x, y) = \u03c6\u03c3 (||vx \u2212vy ||2 ) and h\u03a8 (x) = 1\u2212 Xk \u03a8(x, y)\u03bbx* (dy). Fortunately, there\nwill be no need to compute h\u03a8 ; it is enough to note that it is non-negative.\n\nTo check the detailed balance for Q3 it is sufficient to consider A, B \u2208 B\n\nwhere A, B \u2282 Xk .\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\nZ\n\nx\u2208A\n\nZ\n\n\u03c0(dx)\nQ3 (x, dy)\ny\u2208B\nZ\nZ\n=\n\u03c6(x)\u03b13 (x, y)\u03c4 (dx)[\u03a8(x, y)\u03bbx* (dy) + h\u03a8 (x)\u03b4x (dy)]\nZx\u2208A Zy\u2208B\n=\n\u03c6(x)\u03b13 (x, y)\u03a8(x, y)\u03c4 (dx)\u03bbx* (dy)\nx\u2208A y\u2208B\nZ\n+\n\u03c6(x)\u03b13 (x, x)h\u03a8 (x)1x \u2208 B \u03c4 (dx)\nx\u2208A\nZ\nZ\n=\n\u03c6(x)\u03b13 (x, y)\u03a8(x, y)\u03c4 (dx)\u03bbx* (dy)\nx\u2208A y\u2208B\nZ\n+\n\u03c6(x)\u03b13 (x, x)h\u03a8 (x)\u03c4 (dx)\n\n46\n\n(3.69)\n(3.70)\n\n(3.71)\n\n(3.72)\n\nx\u2208A\u2229B\n\nThis second term in the latter expression does not change if we replace A, B\nwith B, A and consequently needs no further consideration. To expand the first\nterm, recall that in this situation \u03bbx* just picks one of the k \u2212 1 coordinates\n\nand randomizes it. Let Sjk (w) stand for the modified version of w in which the\nj'th and k'th coordinates of w are swapped. In suggestive notation, let:\ne = X(w)\ne\nX\n= (k, w\u2212 )\n\ne k (w))\nYe = Ye (w) = X(S\nj\n\ne\nDj = {w \u2208 [0, 1]k : X(w)\n\u2208 A, Ye (w) \u2208 B}\n\ne\nDj\u2032 = {w \u2208 [0, 1]k : X(w)\n\u2208 B, Ye (w) \u2208 A}\n\nNotice that w \u2208 Dj \u21d0\u21d2 Sjk (w) \u2208 Dj\u2032 .\n\n(3.73)\n(3.74)\n(3.75)\n(3.76)\n\ne and Ye , the first term expands\nThen, suppressing the w dependence from X\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n47\n\ninto:\nk\u22121\n\n1 X\nk \u2212 1 j=1\n\nZ\n\nw\u2208Dj\n\ne 3 (X,\ne Ye )\u03a8(X,\ne Ye )\u03bbk (dw)\n\u03c6(X)\u03b1\nk\u22121\n\n1 X\n=\nk \u2212 1 j=1\nk\u22121\n\n1 X\n=\nk \u2212 1 j=1\n\nZ\n\nw\u2208Dj\u2032\n\nZ\n\nw\u2208Dj\u2032\n\ne\ne k (dw)\n\u03c6(Ye )\u03b13 (Ye , X)\u03a8(\nYe , X)\u03bb\n\ne\ne Ye )\u03bbk (dw).\n\u03c6(Ye )\u03b13 (Ye , X)\u03a8(\nX,\n\n(3.77)\n\n(3.78)\n\n(3.79)\n\nIn summary then, (using Equation 3.79)\nZ\n\n\u03c0(dx)\n\nx\u2208A\n\nZ\n\nQ3 (x, dy)\n\ny\u2208B\nk\u22121\n\n1 X\n=\nk \u2212 1 j=1\n\nZ\n\nw\u2208Dj\u2032\n\ne\ne Ye )\u03bbk (dw) + const\n\u03c6(Ye )\u03b13 (Ye , X)\u03a8(\nX,\n\n(3.80)\n(3.81)\n\nWhile, using Equation 3.77 with the roles of A and B interchanged so that\n\"Dj \"=Dj\u2032 :\nZ\n\n\u03c0(dx)\n\nx\u2208B\n\nZ\n\nQ3 (x, dy)\n\ny\u2208A\nk\u22121\n\n1 X\n=\nk \u2212 1 j=1\n\nZ\n\nw\u2208Dj\u2032\n\ne 3 (X,\ne Ye )\u03a8(X,\ne Ye )\u03bbk (dw) + const\n\u03c6(X)\u03b1\n\n(3.82)\n(3.83)\n\nAgain we achieve balance using:\n\u03b13 (x, y) = min(\u03c6(y)/\u03c6(x), 1)\n\n(3.84)\n\nDefine Pe4 (x, dy) = \u03bex (y)\u03c4 (dy). Where for any x \u2208 X , \u03bex (*) is a density with\n\nrespect to \u03c4 . Accordingly:\n\n\u03c0\u25e6\u2032 (dx)Pe4 (x, dy) = \u03c6(x)\u03bex (y)\u03c4 (dx)\u03c4 (dy) = pe(x, y)\u03c4 (dx)\u03c4 (dy)\n\n(3.85)\n\n\fCHAPTER 3. COMPUTING THE POSTERIOR\n\n48\n\nBy the arguments in section 3.6 the corresponding transition probability function P4 satisfies detailed balance with respect to \u03c0\u25e6\u2032 provided that:\n\uf8f1\n\uf8f2min(1, [\u03c6(y)\u03bey (x)]/[\u03c6(x)\u03bex (y)] if \u03c6(x)\u03bex (y) > 0\n\u03b14 (x, y) =\n\uf8f31\nif \u03c6(x)\u03bex (y) = 0\n\n(3.86)\n\nFor example, for x \u2208 Xk , choose \u03bex (y) = 1y \u2208 Xk . And for this \u03bex , the\nfamiliar choice \u03b14 (x, y) = min(1, \u03c6(y)/\u03c6(x)) works equally well.\nFinally, define Pe5 (x, dy) for x \u2208 Xk as the composition of the k \u2212 1 separate\n\ntransition probability functions Pe5j applied sequentially from j = 1 to j = k \u22121.\nEach Pe5j is just intended to move the j'th coordinate of vx by a small amount\n(or hold). In effect, then, Pe5 moves all of the coordinates of x randomly, but\n\ntechnically speaking some subset of them may hold on any given step. For\nx \u2208 Xk and 1 \u2264 j \u2264 k \u2212 1 set Pe5j (x, dy) = \u03a8(x, y)\u03bbx*j (dy) + h\u03a8 (x)\u03b4x (dy) for\nsome \u03a8 and h\u03a8 satisfying the same constraints as made when defining Pe3 . I\nomit the verification that by choosing \u03b15 = min(1, \u03c6(y)/\u03c6(x)), as usual, P5\nsatisfies detailed balance.\n\nThis concludes a consideration of each chain P1 through P5 . Each was\nshown to satisfy detailed balance with respect to \u03c0\u25e6\u2032 . Consequently, their mixture P also satisfies detailed balance with respect to \u03c0\u25e6\u2032 .\nAs an aside, notice that one may compose any of the Pj with a random\npermutation since \u03c6 itself is invariant with respect to permuted v. Doing so\nwill allow P1 to add and (by pre-composing) delete coordinates in arbitrary\nlocations.\n\n3.9\n\nMarkov Chain Convergence Theory\n\nEarly results on the ergodicity of Markov chains on general state spaces used\na condition known as the Doeblin condition. It implies that there exists an\n\n\f49\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\ninvariant probability measure to which the Markov chain converges at a geometric rate, from any starting point.1\nTheorem 1 (Doob (1953) [28]). Suppose that the Markov chain on the\nmeasure space (X , B) generated by probability transition function P satisfies\n\nthe Doeblin condition that there is a probability measure \u03bb on (X , B), an integer\nk, and an \u01eb > 0 such that:\n\nP k (x, C) \u2265 \u01eb\u03bb(C)\n\nfor all x \u2208 X and all C \u2208 B\n\nThen there exists a unique invariant probability measure \u03c0 such that for all n,\nsup |P n (x, C) \u2212 \u03c0(C)| \u2264 (1 \u2212 \u01eb)(n/k)\u22121\n\nC\u2208B\n\nfor all x \u2208 X\n\nAn easy corollary is that if P satisfies the conditions of the theorem, and\nwas already known to be reversible with respect to some specific distribution,\nthen that same distribution must be the unique stationary distribution \u03c0. The\nDoeblin condition is quite strong and rarely holds in applications.\nAthreya, Doss, and Sethuraman [1] prove an ergodicity result for general\nstate spaces whose conditions hold much more broadly and remain reasonably\neasy to check. An abbreviated version is given below\nTheorem 2 (Athreya, Doss, and Sethuraman (1996)). Suppose that the\nMarkov chain {Xn } with transition function P (x, C) has an invariant probabil-\n\nity measure \u03c0, that is Equation 3.29 holds. Suppose that there is a set A \u2208 B,\n\na probability measure \u03bb with \u03bb(A) = 1, a constant \u01eb > 0, and an integer n0 \u2265 1\n\nsuch that:\n\n\u03c0({x : Px (Xm \u2208 A for some m \u2265 1) > 0}) = 1\n1\n\n(3.87)\n\nThis section reviews some material given in a paper by Athreya, Doss, and Sethuraman [1]\n\n\f50\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\nand\nP n0 (x, *) \u2265 \u01eb\u03bb(*)\n\nfor each x \u2208 A\n\n(3.88)\n\nFurther suppose that either n0 = 1 or that\ng.c.d.{m : there is an \u01ebm > 0 such that P m (x, *) \u2265 \u01ebm \u03bb(*) for each x \u2208 A} = 1\n(3.89)\n\nThen there is a set D \u2208 B such that\n\u03c0(D) = 1 and\n\nsup |P n (x, C) \u2212 \u03c0(C)| \u2192 0 for each x \u2208 D.\n\n(3.90)\n\nC\u2208B\n\nLet f (x) be a measurable function on (X , B) such that\nThen\n\nPx\n\n3.10\n\n(\n\nn\n\n1X\nf (Xj ) \u2192\nn j=1\n\nZ\n\n\u03c0(dy)f (y)\n\n)\n\n=1\n\nR\n\n\u03c0(dy)|f (y)| < \u221e.\n\nfor [\u03c0]-almost all x\n\n(3.91)\n\nConvergence Results\n\nA short proof suffices to show that theorem 2 applies to the local-move Markov\nchain. It is assumed that mixing probability p1 > 0. The proof takes advantage\n\u0001\nof the atom x0 = 1, \u226c .\n\nLet P denote the local-move Markov chain from section 3.8. It was already\n\nverified there that P satisfies detailed balance with respect to \u03c0\u25e6\u2032 , and it follows\nthat P has invariant probability measure \u03c0\u25e6\u2032 . Let A = X1 , the singleton set\n\n{x0 }. Let \u03bb(*) be counting measure on X1 . Let n0 = 1. Set \u01eb = P (x0 , X1 ),\n\ni.e. the chance of holding at this atom. This quantity is positive (because\nP (x0 , X1 ) \u2265 p1 P1 (x0 , {x0 }) \u2265 p1 Pe1 (x0 , {x0 }) > 0).\n\nThis verifies all of the conditions of theorem 2 except condition 3.87. It suf-\n\nfices to show that for any k \u2208 N and any x \u2208 Xk , Px (Xm \u2208 A for some m \u2265 1) >\n\n\f51\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\n0. It has already been shown that for any x \u2208 X1 , Px (Xm \u2208 A for some m \u2265 1) >\n0, since X1 is the singleton set considered earlier and because this quantity is\n\ngreater than P (x0 , A), which was positive. Suppose, for induction, that for\nany k \u2264 k0 and any x \u2208 Xk , Px (Xm \u2208 A for some m \u2265 1) > 0. Consider any\nx \u2208 Xk0 +1 .\n\nP (x, Xk0 ) = p1 P1 (x, Xk0 )\n(3.92)\n\u0014\n\u0015\nZ\n1\n1\nx\nx\n= p1\n\u03b11 (x, y)\u03bb\u2191 (dy) + \u03b11 (x, y)\u03bb\u2193 (dy) + h1 (x)\u03b4x (dy)\n2\ny\u2208Xk0 2\n1\n= p1\n2\n\nZ\n\n(3.93)\ny\u2208Xk0\n\n\u03b11 (x, y)\u03bbx\u2193 (dy)\n\nZ\n1\n= p1\n\u03b11 (x, y)\u03b4g\u2193 (x) (dy)\n2\ny\u2208Xk0\n1\n= p1 \u03b11 (x, g\u2193 (x))\n2\n1\n= p1 min(\u03c6(g\u2193 (x))/\u03c6(x), 1)\n2\n\n(3.94)\n(3.95)\n(3.96)\n(3.97)\n\nNow, both \u03c6(x) and \u03c6(g\u2193 (x)) are positive and finite, so their ratio is as well,\nand so P (x, Xk0 ) > 0. This proves that for any x \u2208 Xk0 +1 ,\nPx (Xm \u2208 A for some m \u2265 1) > 0\n\n(3.98)\n\nAll the conditions are now verified.\nThe same argument goes through without modification for the simple Markov\nchain from section 3.7.\nIn summary the preceding sections have proven the following theorem:\nTheorem 3. Let (X , B) be the measurable space defined in section 3.3. Let P\n\nbe the probability transistion function on this space that is defined as a mixture\nof the component probability transistion functions P1 , through P5 , explained in\n\nsection 3.8, with mixture weights p1 , through p5 positive and summing to 1.\n\n\f52\n\nCHAPTER 3. COMPUTING THE POSTERIOR\n\n(This Markov chain is a formal version of the Algorithm given in section 3.4.)\nThen, P satisifies detailed balance with respect to the distribution \u03c0\u25e6\u2032 defined\nby Equation 3.23. Furthermore, P satisfies the conditions of Theorem 2 and\ntherefore, \u03c0\u25e6\u2032 is the unique invariant distribution of P . Indeed, there is a set\nD \u2208 B such that\n\u03c0\u25e6\u2032 (D) = 1 and\n\nsup |P n (x, C) \u2212 \u03c0\u25e6\u2032 (C)| \u2192 0 for each x \u2208 D.\n\nLet f (x) be a measurable function on (X , B) such that\n\nThen\n\nPx\n\n(\n\nn\n\n(3.99)\n\nC\u2208B\n\n1X\nf (Xj ) \u2192\nn j=1\n\nZ\n\n\u03c0\u25e6\u2032 (dy)f (y)\n\n)\n\nR\n\n\u03c0\u25e6\u2032 (dy)|f (y)| < \u221e.\n\n= 1 for [\u03c0\u25e6\u2032 ]-almost all x\n\n(3.100)\n\n\fChapter 4\nExamples\nThis chapter describes the results of a variety of simulation experiments that\nI have conducted to better understand and evaluate the performance of the\nposterior mean estimates based on the prior \u03c0. In all of these experiments,\nexcept where specifically noted, the prior \u03ba on the number of steps K is taken\nto be Geometric( 12 ). Other Geometric priors are considered in section 4.4 and\nsection 4.7. A few examples in chapter 6 consider Poisson priors. There, they\nare discussed in relation to the convergence theory proven in chapter 5. Most\nof this chapter, however, concerns evaluating how efficient the Geometric( 12 )\nposterior mean estimate is by comparing it with a wide variety of competing estimation procedures. The first section establishes the standard format for these\nexperiments. It compares the posterior mean estimate with CART and bagged\nCART estimates and interprets the results. Other methods are considered and\ncompared in section 4.2, namely: a Lasso example that is connected with bagging, three smoothers, and some wavelet-based estimates. Finally, the dyadic\nDiaconis and Freedman binary regression prior is compared in section 4.3. Section 4.5 takes a step back to analyze the interaction between the data and the\nmodel by inspecting how the predictive probability changes as a function of\nwhere splits are placed. The final sections experiment with smaller and larger\ndata set size, and also evaluate the performance of the posterior mean on a\n53\n\n\fCHAPTER 4. EXAMPLES\n\n54\n\nmore challenging regression problem.\n\n4.1\n\nComparison with CART and Bagged CART\n\nSince \u03c0 puts a prior on piecewise constant regression functions, it is natural to\nask how its performance compares with conventional estimators that employ\npiecewise constant approximations. Of particular interest is the Classification\nand Regression Tree (CART) algorithm [4], and the closely related bagged\nCART algorithm. These methods are briefly reviewed in the next two sections.\nThe impatient reader should skip ahead to subsection 4.1.3, where the methods\nare compared with the posterior mean estimate on simulated data sets. I have\nnot \"filtered\" the experimental data at all: these are the originally simulated\ndata sets in their original order. Technically, there has been a certain amount\nof filtering in the results because I did try using a variety of settings for the\nCART and bagged CART methods that I do not discuss. The choices that are\npresented are among the more standard and better performing possibilities.\nAs far as the posterior mean results, these are not filtered at all, except that\narguably I would not have a thesis if the results were not interesting.\n\n4.1.1\n\nCART Review\n\nThe CART algorithm prescribes how to select a \"tree\" that represents a good\nestimate of the unknown regression function (or classification rule). The \"tree\"\nterminology connotes the fact that the covariate space X is recursively parti-\n\ntioned with each piece assigned its own regression value: this recursive partition\ncan be naturally associated (by inclusion) with a graph-theoretic tree. For completeness, a brief description of the CART algorithm is in order. The reader\n\nshould bear in mind that CART and related algorithms have been in use for\nmany years now and so there are a variety of possible tweaks and alternatives\nthat I do not discuss. The CART algorithm also has important advantages that\n\n\fCHAPTER 4. EXAMPLES\n\n55\n\nthe following discussion will not address. Its regression estimate, for example,\nis unaffected if individual coordinates of the covariate vector are rescaled, so\nthat the estimate does not depend upon the units of measurement. It is capable of dealing with large data sets because of its efficient implementation. It is\nalso very easy to interpret (although this is a risky business since the estimates\ncan sometimes change dramatically with new data).\nSuppose for simplicity that the covariate space X is Rd (specifically, the\n\ncase X = R1 is of interest at present). In its basic form, the CART algorithm\nuses coordinate-aligned splits. That is, if a certain subset A of X is being\n\npartitioned, it will be partitioned into the two subsets {x \u2208 A : xj \u2264 c}\n\nand {x \u2208 A : xj > c} for some choice of j \u2208 {1, . . . , d} and c \u2208 R. CART\n\nproceeds to construct a partition of X in a greedy manner. That is, it begins\nby finding the binary partition of X that maximizes a certain splitting criterion\n\nand (proceeding recursively) all subsequent splits are subordinate to this one.\n\nUltimately, the splitting criterion is chosen by the user, and I have chosen to\nuse the ANOVA criterion. To explain this criterion consider that at any given\nstage of partitioning there is a certain class of possible real-valued functions\nthat are constant on each partition element. Among this class, the function\nthat minimizes the mean of squared residuals to the response data y1 , . . . , yn\n(MSE) is clearly the one whose value on any given element of the partition\nis the mean of the response values whose covariates \"hit\" this element. The\nsplit that is considered best is the split that results in the greatest possible\nreduction in this measure of residual error. Having chosen a split, the CART\nalgorithm continues, recursively, to split the resulting subsets. Implicitly, it is\nbuilding up a \"tree\" of subsets at each stage with X forming the root. The\nrecursion terminates whenever there is only a single data point in the current\n\npartition element. Call the resulting binary tree of subsets the \"full\" tree.\nCART then proceeds to \"prune\" this tree. This operation depends critically\nupon a complexity parameter cp, which must itself be chosen. Typically cp\nis chosen by cross-validation with the restriction that it not be smaller than\n\n\fCHAPTER 4. EXAMPLES\n\n56\n\nsome user specified value. For these experiments I choose cp using 10-fold\ncross-validation and the one-standard-deviation selection rule. This means\nthat if the best achievable cross-validated measure of error (searching over all\npossible values of cp) is xerr and the sample standard deviation of xerr is xstd,\nthe selected value of cp will be the largest value whose xerr does not exceed\nxerr+xstd. To prune the full tree using parameter cp, each pair of leaves is\nconsidered in turn; if the pair does not improve the splitting criterion by at\nleast the value cp, it is removed. Ultimately, every pair of leaves might be\nremoved, resulting in the tree consisting only of the root node. Finally, the\npruned tree corresponds to the estimated regression function. It is constant on\neach element of the (pruned) partition and its value on a given element is the\nmean of the response values there.\n\n4.1.2\n\nBagging Review and Discussion\n\nA comparison with the bagging procedure [3] is also relevant. Bagging [3] is\na meta-algorithm that can (hypothetically) be applied to any existing classification or regression technique in an effort to improve them. This thesis\nfocuses on bagged CART. Essentially, the bagging idea is just to take many\nbootstrap resamples of the data set, apply some existing technique to each\nresampled data set, and then take an average of all of the resulting regression\nestimates. For completeness, to form a bootstrap resample of a dataset with n\nitems zi = (xi , yi ): (1) Independently, choose n integers N1 , . . . , Nn uniformly\nat random from 1 to n. (2) Form the new data set: zN1 , zN2 , . . . , zNn .\nIt is sometimes stated [43] that bagging is approximately a non-parametric\nBayesian procedure, but I think that this is a misleading claim. Bagging, or\nmore specifically bootstrapping, approximates the behavior of a Bayesian who\nhas a (limiting) Dirichlet prior (as in Rubin's Bayesian bootstrap [57]). This is\nnot really a prior for several reasons. Besides the fact that this limiting prior\nis improper, the \"prior\" depends on the data. This is not just in a partial\n\n\fCHAPTER 4. EXAMPLES\n\n57\n\nsense (such as the prior I use in chapter 7 in which the \"prior\" depends on the\ncovariates but not the response) or even in the manner of empirical Bayes procedures which choose some parameters of the \"prior\" by looking at the data.\nIndeed, this prior specifies the law of all possible data relative to the empirical distribution. This might make sense if the data were multinomial, but for\ndata on a continuous space it is quite problematic. If taken literally it specifies that all future data will consist of elements drawn from the current data\nset. Notably, for classification, this means that unless the data set happens to\ncontain a head and a tail for each case, then the predictive distribution that\nthe bootstrap prior corresponds to excludes the possibility that the missing\nflip will ever occur. Indeed, this bootstrap prior can never directly say anything about future datapoints whose covariates are new. Furthermore, there\nis no meaningful model involved. For example, the prior \u03c0 implicitly builds in\ninformation that says that if a certain region has more heads than tails, then\nfuture points in and around this region probably will as well. The bootstrap\nprior says nothing like that explicitly; if it says that in effect it is only because\nof the fitting method that is forced upon it.\nFinally, there is nothing Bayesian about using CART, so how can bagged\nCART be a non-parametric Bayesian procedure? Why would a Bayesian who\nbelieved in a bootstrap prior use CART or neural nets or whatever when he\ncould easily compute his own (very bizarre) posterior and get conclusions directly? Arguably, he might do so in order to get new information to guide his\ndecision because he has observed that CART (say) has worked well on other\nproblems so it probably will work on this one as well. In this sense, bagging\ncould be said to model the behavior of a Bayesian who had a limiting Dirichlet\nprior (that magically was supported on the data set itself), who then computed the posterior of his \"prior,\" and who then goes to seek the opinion of\nan \"expert.\" Cleverly, he does so, not only for the dataset actually received,\nbut for a multitude of datasets of size n that are about equally likely under\nhis posterior. In this way, he finds out what CART would think in a variety\n\n\fCHAPTER 4. EXAMPLES\n\n58\n\nof situations that he subjectively considers as possibilities. So far so good.\nNow he ought to weight these opinions according to how credible they seem in\nlight of the data and his prior opinion about when CART works and when it\ndoesn't. Instead, he now effectively forgets his (Dirichlet) posterior and puts a\nflat prior on the CART regression results themselves. With his \"newly found\nprior,\" he calculates the decision that minimizes squared error loss, the mean.\nSo, in summary, bagging approximates the behavior of a forgetful Bayesian\nwho looks at the data first, then formulates his \"prior\" and posterior, then ignores them, except to ask CART what it's opinion would be in the cases that\nhe thinks are likely, then promptly forgets the data altogether as well as any\npriors or posteriors of his own that he might have held (recently), so he makes\nup a new uniform prior on all the results that he got back from CART; finally\nhe computes the mean according to his latest prior and reports his \"findings.\"\nIn any case, however dubious as a \"Bayesian\" procedure, bagging works.\nOne could say that this is because it reduces modeling bias or because it eliminates certain instabilities in CART: both of these arguments make perfect sense\nto Bayesians and frequentists alike. It is clear, however, that it's not always a\ngood idea, especially when the procedure already has low \"instability.\" In this\ncase, bagging mostly adds noise and reduces the effective size of the dataset\nsomewhat.\nI also found one example where bagging was disastrous. Following the\nbagging procedure strictly, I fed bootstrap resampled data sets into CART,\nbut the result was a mess of indecipherable noise with splits everywhere. This\nwas true even though the CART procedure gave a reasonable estimate on the\noriginal data set. Why? Because in the CART step I used cross-validation\nand this is problematic because CART is working on a bootstrapped sample.\nCertain repeated data points wind up in both the test and training sets. As a\nconsequence the CART algorithm has less data that it thinks and also thinks\nthat is not over-fitting when it uses a model with too many splits; the pruning\nprocedures became completely ineffective.\n\n\fCHAPTER 4. EXAMPLES\n\n59\n\nSome authors argue that this problem is easy to avoid by simply not bothering to cross-validate within CART, and instead using the full trees. This\nmay be true in some cases, or if one implicitly uses an effective default pruning rule (and not actually full trees), but it failed on my example. To correct\nthe cross validation, some authors \"tell\" CART which points are repeated so\nthat the whole case is either left in or left out. Instead, for my experiments,\nI simply used a hand-tuned complexity parameter lower bound. For the disastrous experiment, the lower bound was set to 0; when increased to 0.005\nthe estimates were still quite rough, but usable. For the experiments that I\npresent in the next section, I set to the lower bound to 0.01, which (admittedly) is the default for the RPART implementation of CART. Still, unless\nthis default is universally good, this leaves a tuning parameter to be set, and\nI shudder to think about the bizarre computations involved in choosing it by\ncross-validating bagged CART.\nFor the experiments involving bagged CART, I used 100 bootstrap resamples. This is a larger number of bootstrap resamples than is generally considered necessary for bagging. By informal Rao-Blackwell-type reasoning one\nwould think that this only serves to reduce Monte Carlo error.\n\n4.1.3\n\nComparative Simulation Experiment\n\nThis experiment involves 10 simulated data sets each containing 1024 data\npoints that were created in the manner explained in the introductory chapter.\nIn this and in future sections these will be referred to as experimental runs\n1-10. Briefly, one simply chooses 1024 random uniforms for the x-values, and\nthen flips 1024 independent coins with the success probability of the coin being\ngiven by the function f0 that is indicated by the blue curve in figures 4.1.a,\nand 4.1.b. These figures also include a red and a green histogram (drawn upside\ndown). These are histograms of the of the x-values for which the coin came out\nheads or tails respectively. There are 75 bins in each histogram, and to keep\n\n\f60\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nFigure 4.1.a: Simulation Experiment: Run 1\nThe CART, Bagged CART,\nand posterior mean estimates are compared with the true regression curve on a\nsimulated data set (\u03b1 = 21 , n = 1024). CART misses the left hand change-point\nentirely on this example; bagged CART seems to edge out the posterior mean\nnear 0.75 but they are quite similar over all.\nKey: True f (blue), Posterior Mean (magenta), CART (black), Bagged CART\n(cyan)\n\n1\n\n\f61\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nFigure 4.1.b: Simulation Experiment: Run 2\nThe CART, Bagged CART,\nand posterior mean estimates are compared with the true regression curve on\na second (equivalently) simulated data set (\u03b1 = 12 , n = 1024). The posterior\nmean and bagged CART estimates track each other quite closely again.\nKey: True f (blue), Posterior Mean (magenta), CART (black), Bagged CART\n(cyan)\n\n1\n\n\f62\n\nCHAPTER 4. EXAMPLES\n\ntrack of the counts, yellow lines are drawn for every 5 events. In these figures,\nthe posterior mean estimate (\u03b1 = 12 ) is in magenta, the CART estimate is in\nblack, and the bagged CART estimate is in cyan. The first two experiments are\nshown in a large format, and the latter 8 are grouped together in Figure 4.1.e.\nTo be specific, in these experiments to compute the CART and bagged\nCART estimates, I used the RPART library in S-Plus with default control settings, 10-fold cross validation, the 1-standard-deviation rule to control pruning,\nand the ANOVA method. I apply the RPART regression algorithms to binaryclassification data by assigning the values 1.0 and 0.0 to the two categories.\nThe complexity parameter was restricted to be at least 0.01 (this is the default\nbehavior).\n\n4.1.4\n\nObservations\n\nHere is a summary of notable features in the results of the first two experiments.\nSome of these features are explained by the subsequent discussion. In experimental run 1 (Figure 4.1.a), the CART estimate only has 4 steps, \"leaving\nout\" an important split on the left side. In experimental run 2 (Figure 4.1.b),\nCART has 5 splits in about the right places. In both figures, of course, CART\nretains its jagged appearance, but it does do an admirable job of finding the\nlocations where the true curve has change-points. All three estimates share\nsome features with CART, they all make a fairly abrupt change at essentially\nthe same place, somewhere near the location of the true change at 12 . They\nall have substantially more trouble with the change at\n\n1\n6\n\nand this makes sense\n\nbecause it is much easier to detect the difference between two coins with success probabilities 0.8 and 0.4 than between two coins with probabilities 0.6 and\n0.4. Surprisingly, at least in experimental run 1, CART seems to be a bit more\nsimilar to the posterior mean estimate than to its own bagged version.\nComparing the posterior mean with bagged CART in the first figure, notice\nthat bagged CART smoothes out some steps that the posterior mean leaves\n\n\f63\n\nCHAPTER 4. EXAMPLES\n\nin. Notably this happens near 0.75 where bagged CART comes closer to the\ntruth. For some reason this does not happen near 0.8 where bagged CART\nis more blocky than the posterior mean. Bagged CART's smoothing was a\ndisadvantage near 12 , though. Here the posterior mean makes a much sharper\ntransition and also has an extra \"blip\" up in the correct direction. Looking at\nthe plateaus, bagged CART has been pulled closer to\n\n1\n2\n\n(away from the truth)\n\nthan the posterior mean. This is probably due to averaging in a large number\nof CART trees that omit the left hand split.\nComparing the posterior mean with bagged CART in the second figure,\nsome of the features have remained, but not all. In experimental run 2, the\nposterior mean takes a much smoother descent on the right than it did before.\nIn this case, the posterior mean is closer to the truth over all. In the first,\nbagged CART seemed to have an edge. The posterior mean has a blip near 0.1\nthat bagged CART does not.\n\n4.1.5\n\nSome Explanations\n\nOverall, though, in both experimental runs, the bagged CART estimate and\nthe posterior mean estimate seem quite similar. Consequently, despite the\nvery different way in which the estimates are arrived at, on this example at\nleast the computations achieve a similar result. This is remarkable, especially\nconsidering that CART and bagged CART are the result of years of careful\nproblem specific work and tweaking. The posterior mean estimate represent\nan enormous amount of work too, but most of the work is of a general nature\n(e.g. MCMC techniques) and not problem specific. Moreover, this prior is\nvery naive (by design) there are many ways to modify it that would improve\nperformance on this example by problem specific tweaking. For example, one\ncould allow both linear regions and constant regions, or impose a suitable (but\nnot too restrictive) dependency among the success probabilities that would help\nsmooth the right side. It may also make sense to space out the locations of the\n\n\f64\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 4.1.c: The Bagged Posterior Mean Estimate: Bagging the posterior\nmean estimate on the data from experimental run 1 produces the red curve\nwhich does not seem to change the posterior mean estimate much except to\nadd in some irregular wiggles.\nKey: True f (blue), Bagged Posterior Mean (red), Posterior Mean (magenta),\nBagged CART (cyan) Posterior Mean on a Fifteen Bootstrap Resamples (gray)\nsplits explicitly. Rather, this prior is very flat. The choice of Geometric( 12 ) as\nthe hierarchy prior is not the result of years of experience, but is, in fact, the\nfirst thing I tried. The prior \u03c0 cannot yet be recommended in general, but that\nit even performs modestly well \"out of the box\" on this example is an excellent\ndefense of the Bayesian approach.\nWhy, is it, though, that the results are so similar? If the results were\nidentical one could hope to prove a theorem about why this was so, but since\nthey are only similar, and since cross-validated CART is not easily amenable\nto mathematical analysis (much less its bagged variant), I can only speculate\nthat they are similar because they both average together roughly the same\nfunctions. They arrive at similar functions because, after all, they use the\n\n\fCHAPTER 4. EXAMPLES\n\n65\n\nsame data set. Additionally, under ordinary circumstances there are bound to\nbe similarities between the estimates that CART gives and the estimates that\nthe posterior mean gives (even if I might argue that the posterior mean estimates are preferable). Consequently, if (ignoring decision theoretic discipline),\nI decided to bag the posterior mean estimate, it \"follows\" that the bagged posterior mean estimate should be close to the bagged CART estimate. Since I\nbelieve that the posterior mean estimate is fairly stable under subsampled data\n(stability under bootstrap resampling is perhaps more questionable, but both\nquestions suggest interesting future research), I conclude that the posterior\nmean estimate should be close to the bagged CART estimate. If the reader is\nskeptical that the posterior mean has stability under subsampling, they may\nbe interested in the examples given in section 4.8, these substantiate this claim\n(but do not address it specifically).\nThe above argument is merely heuristic, of course, so it is reasonable to ask:\nwhat does happen if the posterior mean calculation is bagged? The answer\nis shown in Figure 4.1.c. The gray curves show the results of computing the\nposterior mean on fifteen bootstrap-resampled data sets. The red curve is their\naverage: the \"bagged posterior mean.\" Looking at the gray curves, they have\nmany wobbles and spikes, so it seems that the posterior mean is more sensitive\nto the repeated observations that occur in a bootstrap sample than CART is.\nAs a result, the red bagged posterior mean curve is itself rather wobbly.\n\n4.1.6\n\nSituations in which the Estimates Differ\n\nIt is possible to construct examples where the posterior mean estimate would\ndiffer more dramatically from the CART and bagged CART estimates. One\nneed only consider circumstances in which CART will reliably perform in a\nrather special way.\nFor a first example, recall the CART works by pruning a full tree and that\nthis tree is selected in a greedy manner. The first split that CART chooses,\n\n\fCHAPTER 4. EXAMPLES\n\n66\n\nfor example, is usually a very important split, but there are certainly cases\nwhere choosing it in a greedy way is suboptimal if one considers the global\nsearch for the \"best tree.\" To some extent bagging improves CART's ability to\nfind useful trees because sometimes a resampled data set suggests a different\nsplitting order. Considerations like this are not even an issue for the posterior\nproper (because it is a theoretical construct), but they are an issue for the\nactual estimates that get produced by MCMC. Still the issues are different and\ngenerally, MCMC methods will perform a much more \"global\" search over tree\nspace than CART does. This search ability was emphasized as an advantage of\nBayesian CART by [6]. In summary, then, in a circumstance where the greedy\nsearch has problems, the posterior mean estimate may avoid those problems,\nand, consequently, give a rather different answer than bagged CART. In my\nexperiments this sort of thing showed up (to a small extent) when I increased\nthe sample size to 8192. This experiment is discussed in section 4.7. The\nCART estimates preferred a split in the middle of the right-hand slope that\nthe posterior mean avoided.\nA second example occurs if the x-axis is transformed. One-dimensional\nCART estimates are invariant with respect to this, while the posterior mean\nis not. In some senses this property is desirable; it seems \"scientific.\" It is not\nalways desirable though: suppose there is a large amount of data from 0 to 0.1\nand from 0.9 to 1 but no data in between. Roughly, CART will treat this in\nthe same fashion as if there were no gap; essentially it only looks at the ordered\nvalues. If it splits the gap at all, it will split in exactly the middle of the gap:\nbetween the rightmost of the left-hand data and the leftmost of the right-hand\ndata (ignore the fact that this is not strictly invariant under transformations).\nThis effect does not go away under bagging (although it might get smoothed\na bit as the endpoints of the gap change).\nHowever, the posterior mean estimates will be quite different. Notably, it\nwill make a smooth transition from the regression value on the left to the value\non the right. Additionally, because this gap is especially large and the prior\n\n\f67\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 4.1.d: Transformed Data with a Gap: For this example, the data from\nexperimental run 1 was transformed nonlinearly to create a gap. Notice the\nsmooth transition that the posterior mean makes along the gap.\nspecifies that split points are put down uniformly, it is quite likely that the gap\nwill be split at least once or twice. If the gap is split twice or more, then the\nmiddle intervals will include no data at all. When there is no data (or even little\ndata), the prior kicks in to specify that it thinks that the success probability\nis uniformly distributed from 0 to 1 and consequently that the mean value of\nthe success probability on this middle interval is 12 . Because cases like this get\naveraged in, the posterior mean should show some shrinkage to\nShrinking to\n\n1\n2\n\n1\n2\n\non the gap.\n\nis, of course, not always ideal, but in such a case one ought\n\nsimple to modify the prior and/or loss function. To construct an example with\na gap, I took the data from experimental run 1 and transformed it so that the\nleft half of the data now lies in [0, 0.1] and the right half lies in [0.9, 1]. The\nresult of computing the posterior mean is shown in Figure 4.1.d\nThese features make obvious intuitive sense. Furthermore, if a 95% subjective confidence interval were formed by asking at each point x \u2208 [0, 1] for the\n\n\fCHAPTER 4. EXAMPLES\n\n68\n\nsmallest interval containing 95% of the posterior mass, it would grow wider\nin the middle. In contrast, if naive confidence bands were formed around\nthe CART or bagged CART estimates (by using bootstrapping perhaps) they\nwould make rather little sense.\n\n4.1.7\n\nPosterior Mean Behavior\n\nIn the first experimental run, CART left out an \"obvious\" split, while in the\nsecond it put it in. Sometimes CART will also \"add in\" splits that it should\nnot have; this is very sensitive to the particular data set and the pruning rules\nthat are used. Bagged CART averages all of these together (in some special\nway that is hard to formulate, except algorithmically) to arrive at its smoother\ncurve.\nIn contrast, I imagine that the posterior mean is considering each of these\npossibilities and giving them an appropriate weight before averaging, in order\nto give its best estimate. Examining the posterior mean curve closely, one can\nsee that wherever CART takes a step, the posterior mean also moves more\nabruptly than normal. Both estimates are, after all, both looking for steps,\nand the locations that CART chooses are bound to be special parts of the data\nset, often containing a run of heads on one side and a run of tails on the other.\nSurely this feature would stand out to both methods.\nThe reverse is not true, however. Consider the \"bump\" in the posterior\nmean, just to the right of 12 . This results because the posterior considered\nfunctions with additional splits in this area and gave them weight. It is not,\nafter all, the result of averaging a large quantity of individually pruned trees,\nbut the result of averaging over all trees (in principle) with appropriate weights.\nCART trees, if allowed to have extra splits would have included this one as well.\nAlong this line, a modest improvement to the bagged CART procedure might\nresult if in addition to using different bootstrapped datasets, one sometimes\nused different pruning criteria as well, and then averaged the less penalized\n\n\fCHAPTER 4. EXAMPLES\n\n69\n\ntrees in together with the more penalized ones (with appropriate weights).\nAs an aside, the posterior sometimes becomes more \"sure\" about the location of a split than it \"really ought to.\" This happens because of the mismatch\nbetween the truth and the prior (or perhaps more accurately, because of the\nmismatch is between \u03c0 and my own subjective prior). The posterior is doing\nthe absolutely optimal thing if the prior \u03c0 is true (indeed, it is an admissible estimator and there is no easy way to tell if the others are or not), but according\nthe prior, functions like the one pictured in blue with a smooth transition are\nimpossibly rare. When faced with data that could have resulted from a smooth\ntransition, but might also credibly be created by a step function with two steps,\nthe posterior only considers the latter possibility. This is especially apparent\nwhen there is a run of heads and then a run of tails occurs by chance (as it is\nbound to do from time to time). The posterior will concentrate more tightly\naround this cut-point than makes sense if one considers a smooth transition\nto be a credible alternative explanation. The posterior does, of course, allow\nfor the possibility that there are multiple splits, but if the success probabilities\non each side are reasonably similar, there may not be enough data to make\nthis possibility stand out and the single split will remain the most prominent\nfeature of the posterior mean. This results in a stair-step appearance that\ndoes not go away with larger sample sizes (see section 4.8), although the steps\ntend to get smaller. Indeed, it appears that the stair-step shape grows more\nprominent for larger n. Perhaps this is because a larger data set also is more\nlikely to have at least a few very long runs.\n\n4.1.8\n\nExperimental Runs 3-10 and a Summary\n\nThe results from experimental runs 3 through 10 are shown in Figure 4.1.e. By\nand large, the same observations made before above apply to these examples.\nBagged CART and the posterior mean track each other quite closely although\neach occasionally takes a \"wobble\" that the other does not. Broadly speaking\n\n\f70\n\nCHAPTER 4. EXAMPLES\n\nboth estimators still retain visible traces of the CART-type functions that they\nare averaging together. In particular, both usually have some \"stepiness\" in\ntheir appearance; the averaging \"softens\" this, but does not eliminate it. This\nis especially visible in experimental run 10 in which both estimates also follow\nthe CART estimate quite closely. CART leaves out the left hand split on 5 of\nthe 10 experiments.\nCART\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\u03bc\nb\n\u03c3\nb\n\n0.0891\n0.0776\n0.0893\n0.0996\n0.1101\n0.1130\n0.0779\n0.0907\n0.1009\n0.0671\n0.0915\n0.0147\n\nPosterior\nMean\n0.0522\n0.0478\n0.0587\n0.0660\n0.0659\n0.0609\n0.0604\n0.0669\n0.0720\n0.0531\n0.0604\n0.0076\n\nBagged\nCART\n0.0527\n0.0568\n0.0710\n0.0737\n0.0683\n0.0799\n0.0593\n0.0617\n0.0718\n0.0587\n0.0654\n0.0088\n\nTable 4.1: Numerical Summary of L2 -norm errors on the ten experimental runs\nA numerical summary of these ten experiments can be made by computing\nthe L2 -norm of the error between the estimated curve and the truth. This sum-\n\nmary is given in table 4.1.8 and illustrated by the scatter-plot in Figure 4.1.f.\nThe black points compare CART to the posterior mean. The cyan points\ncompare bagged CART to the posterior mean. In each case, the x-axis is the\nL2 -norm error of the posterior mean, and the y-axis is that of the competitor.\nObviously, small numbers are preferred, and because the black points lie ex-\n\nclusively above the identity line on these ten experiments, the posterior mean\nis preferable here. The performance of the bagged CART and the posterior\nmean estimates is much closer, although the posterior mean's performance is\nslightly better on average.\n\n\f71\n\nCHAPTER 4. EXAMPLES\n\n1\n\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\nRun 3\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\nRun 4\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\nRun 5\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nRun 6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nRun 9\n\n1\n\n0\n0\n\n0.6\n\nRun 8\n\n1\n\n0\n0\n\n0.5\n\nRun 7\n\n1\n\n0\n0\n\n0.4\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nRun 10\n\nFigure 4.1.e: Simulation Experiment: Runs 3-10\nKey: True f (blue), Posterior Mean (magenta), CART (black), Bagged CART\n(cyan)\n\n\f72\n\nCHAPTER 4. EXAMPLES\n\n6 5\n4 9\n\n0.1\n\n8\n\n76\n4 9\n3 5\n10\n8\n2 10 7\n1\n\n0.08\n\n2\n\n0.06\n\n0.04\n\n2\n\nL \u2212norm error for competitor\n\n1 3\n\n0.02\n\n0\n0\n\n0.02\n\n0.04\n0.06\n0.08\nL2\u2212norm error for posterior mean\n\n0.1\n\nFigure 4.1.f: Comparative Scatterplot: The results of the\n\u221a ten experimental\n2\nruns are summarized by L -norm error (also known as MSE ). On the xaxis is the error committed by the posterior mean estimate. On the y-axis is\nthe error committed by the CART (black) or bagged CART (cyan) estimates\nrespectively.\n\n\f73\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.9\n\n0.8\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 4.2.a: Three Smoothers:\nKey: True f (blue), Posterior Mean (magenta), Loess (black), Smoothing\nSpline (green), Kernel Smoother (red)\n\n4.2\n\nComparison with Other Popular Methods\n\nThis section compares the posterior mean estimate with a variety popular techniques on the data from experimental run 1. Throughout this section, as usual,\nthe true curve is plotted in blue and the posterior mean with Geometric( 21 ) prior\nis plotted in magenta.\n\n4.2.1\n\nSmoothers\n\nFigure 4.2.a shows the results from running three standard smoothers. The\nresults are little surprise. The three smoother's estimates are quite similar\nover all on this example. They over-smooth the jumps, but partially make up\nfor this by giving a smoother approximation on the smooth half. They also\nall take a turn at the ends; this is consistent with the data which happens to\n\n\f74\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 4.2.b: A Lasso Estimate:\nKey: True f (blue), Posterior Mean (magenta), Lasso (Cp) (green)\nbehave somewhat unusually there.\nLoess (plotted in black) fits a locally-weighted linear regression at each point\nto make its estimate. Smoothing splines (in green) use efficient computational\ntricks to compute the regression curve that optimizes a tradeoff between small\nMSE and small integrated second derivative. Gaussian kernel smoothers (plotted in red) take a weighted average of response values near a point to predict.\nFor each method, I chose a smoothing parameter that seemed to give results\nthat were about as good as possible.\n\n4.2.2\n\nLARS/Lasso/Boosting\n\nThe green curve in figure 4.2.b, shows the result of using a Lasso penalized\nregression. This was particularly easy to do using using the Least Angle Regression (LARS) software [30]. Like any linear regression, the results depend\n\n\f75\n\nCHAPTER 4. EXAMPLES\n\non what basis is chosen. For this example, the basis is constructed by considering the function 1x \u2265 c for 1200 values of c evenly spaced from 0 to 1.\nTo encode a datapoint with covariate x \u2208 [0, 1] under this basis, evaluate the\n1200 functions at x and pack the results into a vector: this vector becomes\n\nthe covariate that the regression uses. Finally, Lasso regression resembles ordinary regression except for one critical difference: the regression parameter \u03b2\nis penalized by \u03bb||\u03b2||1, where \u03bb is a tuning parameter. For this example, the\nparameter \u03bb was chosen using a Cp criterion.\nConstructed in this way, the Lasso regression estimate should be quite similar to the estimates that would be arrived at using other important techniques\nsuch as \"Boosting stumps\" and the least angle regression method. The similarity between these different methods is discussed in [30].\nAs can be seen from the figure, the Lasso estimate is has some appealing\nfeatures. It is piecewise constant, but it also takes a fairly large number of steps\nand spaces them out usefully along the smooth transition on the right side of\nthe figure. It does a reasonable jump of \"detecting\" the two change-points. On\nthe other hand, it does not go as low as it should from\n\n1\n6\n\nto 12 , nor as high as\n\nit should to the right of 12 .\n\n4.2.3\n\nWavelets\n\nFigure 4.2.c compares some estimates that were based on wavelet techniques.\nThe red, black, and dotted curves show various wavelet reconstructions of the\nregression curve. Overall, I think the results are quite disappointing; artifacts\nfrom the particular basis used show through clearly into the estimate. Since\nthe data are not regularly spaced, some accommodation is necessary to use\nconventional software (e.g. Wavelab). Algorithms exist that apply directly to\nirregularly spaced data, but I did not successfully locate any working implementations. Instead, the dotted curve shows the wavelet reconstruction that\nresults from simply using the ordered covariate values as if they were regularly\n\n\f76\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.9\n\n0.8\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\ntruth\nposterior mean (0.5)\nWavelet Shrinkage (heursure, one, 10, linear interp, db4)\nWavelet Shrinkage (heursure, one, 10, linear interp, haar)\nWavelet Shrinkage (heursure, mln, 8, ordered values, sym8)\n\n0.2\n\n0.1\n\n0\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 4.2.c: Wavelet Estimates:\nKey: True f (blue), Posterior Mean (magenta), Wavelet 1 (green), Wavelet 2\n(red), Wavelet 3 (black)\n\n\f77\n\nCHAPTER 4. EXAMPLES\n\nspaced and then extrapolating back to the irregularly spaced reality.\nThere are a number of problems with this approach and some recent work\nhas developed more sophisticated schemes. For the red and black curves, I tried\none of the simplest [14] which recommends using direct linear interpolation to\nproduce values on a fine grid and then applying wavelet shrinkage methods\nto this gridded data. The estimates were computed by the wden function in\nMatlab. It has a large number of options, most of which (quite frankly) I\ndo not understand. These include the choice of a threshold selection rule from\namong four options, the choice to use hard or soft thresholding, an option titled\n\"multiplicative threshold scaling\" with three options, the choice of the level at\nwhich to compute the coefficients, and finally the name of the wavelet family\nto use (many options). For someone with as little experience with wavelet\nmethods as me, these options are not a feature but a drawback. Furthermore,\nexperimenting with the different options, they all seemed to make a difference.\nA reasonable, but not heroic effort was made to choose working parameter\nvalues. In any case, for the illustrated curves, the Matlab commands that were\nused are:\nXi=seq(min(X),max(X),2^12); % a fine grid of X-values\nYi=interp1(X,Y,Xi);\n\n% on which to interpolate the response\n\nYhat1 = wden(Y, 'heursure','s','mln', 8,'sym8'); % 1: green\nYhat2 = wden(Yi,'heursure','s','one',10,'db4' ); % 2: red\nYhat3 = wden(Yi,'heursure','s','one',10,'haar'); % 3: black\n\n4.3\n\nComparison with Dyadic Prior\n\nFor comparison, Figure 4.3.a shows the result of computing the posterior mean\nresulting from the Diaconis and Freedman dyadic binary regression prior [25].\nLike the prior \u03c0 studied in this thesis, this prior chooses a random partition and\nassigns independent success probabilities to each partition element. However,\n\n\f78\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 4.3.a: Dyadic Posterior: The posterior mean of the Diaconis and Freedman dyadic prior on the data from experimental run 1 is drawn in green\nthis prior uses a dyadic partition, splitting the data into 2k equal pieces for\nsome k \u2265 0. The consistency of the resulting estimates is guaranteed for any\n\nchoice of hierarchy prior, except perhaps when the true regression function is\nidentically\n\n1\n2\n\nand the data is pure noise. For this case, certain priors will be\n\nconsistent and others will be inconsistent. It was of interest, then to try one\non the boundary. For this reason the prior on hierarchy level K that was used\n1\n\nassigns: P (K = k) = (1 \u2212 \u03b2)\u03b2 k for \u03b2 = exp(\u22122\u2212 4 ) \u2248 0.431. In fact, for this\n\ndata, the results were stable over a wide range of choices of \u03b2.\n\nSince the prior is dyadic, it has no trouble at all nailing the split at 12 ;\nof course, it does not have such good luck for the split at 16 . The posterior\nstrongly favors a model with around 8 steps.\n\n\f79\n\nCHAPTER 4. EXAMPLES\n\n4.4\n\nDependence on the Parameter of the Geometric Prior\n\nIn this section, consider a departure from the Geometric( 21 ) hierarchy prior.\nFigure 4.4.a shows what the posterior mean estimate would be for the data\nfrom experimental run 1, if a Geometric(1 \u2212 \u03b1) prior is used on the number\n\nof steps K. As usual, the true response curve is indicated in blue, and the\nposterior mean estimate for \u03b1 =\n\n1\n2\n\nis drawn in magenta; the posterior means\n\nfor other values of \u03b1 are also drawn. As can be seen in the figure, as \u03b1 ranges\nfrom small values (short tail prior, dotted black curve) to large values (long tail\nprior, solid black curve) there is not so much difference in the posterior mean\nestimate except that certain small bumps and wiggles that are suppressed\nfor the small \u03b1 values become visible for the larger values. Indirectly, this\nexperiment also provides a check on the stability of the Monte Carlo estimates\nof the posterior mean; it is unlikely that there would be such close agreement\namong these independently computed estimates if the MCMC was not working\nreasonably well. To make a more detailed comparison, it would be sensible to\npool the sampled regression curves and use an importance sampling technique\nto compute combined results. I do not pursue this here.\nIt is also of interest to know how many splits the posterior is using. Figure 4.4.b shows the posterior on the number of steps K for three Geometric(\u03b1)\nhierarchy priors: \u03b1 = 0.1, 0.5, and 0.9 respectively. The difference between\nthe priors has a more pronounced effect here. Under a Geometric(1 \u2212 \u03b1) prior\nwith \u03b1 = 0.1, complex models are quite rare and consequently, the posterior\n\non step size shown in the top panel has a fairly short tail. Notice, though, that\neven for this conservative model, the likelihood has been able to overwhelm\nthe conservative prior enough to shift most of the posterior mass onto models\nwith 5 splits.\nFor the bottom panel, \u03b1 = 0.99, which corresponds to a rather slowly\ndecaying tail. Notice, though, that the tail of the posterior is not nearly this\n\n\fCHAPTER 4. EXAMPLES\n\n1\ntruth\n0.10\n0.25\n0.50\n0.75\n0.90\n0.99\n\n0.9\n\n0.8\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n80\n\nFigure 4.4.a: The Posterior Mean for a variety of Geometric Priors: Consider the data of experiment 1 and\ncompute the posterior under a Geometric(1 \u2212 \u03b1) prior.\n\n\f81\n\nCHAPTER 4. EXAMPLES\n\n\u03b1=0.10\n\n4\n\nx 10\n3\n2\n1\n0\n\n5\n\n10\n\n\u03b1=0.50 15\n\n20\n\n25\n\n5\n\n10\n\n\u03b1=0.99 15\n\n20\n\n25\n\n5\n\n10\n\n15\n\n20\n\n25\n\n15000\n10000\n5000\n0\n\n6000\n4000\n2000\n0\n\nFigure 4.4.b: The Posterior on Model Size Consider the data of experiment 1\nand employ a Geometric(1 \u2212 \u03b1) prior. A histogram of the number of sampled\nregression functions that had k steps is shown for three values of \u03b1: 0.10 (top),\n0.50 (middle), 0.99 (bottom)\n\n\fCHAPTER 4. EXAMPLES\n\n82\n\nFigure 4.5.a: A Marginal Likelihood Surface: A slice of the posterior for the 3\nchange-point case (k = 4) is shown. One change-point is fixed at 0.491 (not\nshown) and the location of the other two vary along the x and y axes of plot\nrespectively.\nlong: models with a large number of steps k can fit the data well, but also have\na large number of parameters: 2k \u2212 1. This tends to down-weight them as a\n\ngroup. When the posterior is marginalized to yield the posterior distribution\n\non the number of steps, an account is taken of \"how many\" of these more\ncomplicated models give a good fit as well as how good the fit itself is. Because\nof this trade of, models with 9 steps are the most common.\nThe middle panel, corresponds to the Geometric( 21 ) prior that has been the\nsubject of so many experiments. Notice that models with fewer than 5 steps\nhave almost no effect on the posterior mean; most of the mass is on models\nwith 5 to 9 steps: 6 is the most common choice.\n\n\f83\n\nCHAPTER 4. EXAMPLES\n\n4.5\n\nThe Predictive Probability Surface\n\nTo better understand the interaction of the data and the posterior on experimental run 1, reference to Figure 4.5.a is useful. It demonstrates just how\nspiky, multi-modal, and (in particular) non-normal the posterior's density can\nbe. It shows a slice of the posterior for the 3 change-point case (i.e. four steps).\nOne change-point is fixed at 0.491 (not shown) because this was the most likely\nlocation for a single split; this location accounts for the jump at\n\n1\n2\n\nin f0 . The\n\nother two change-points are allowed to range over the x and y axes of the plot.\nMore technically, what is plotted is a self-normalized version of the function\n\u0001\n\u03c6(x), defined by Equation 3.23 where x = 4, (0.491, x-coord, y-coord) . Essen-\n\ntially \u03c6(x) computes the likelihood that the splits occur at a given location;\nit marginalizes out the different possible choices for the success probabilities.\nThe height of the surface follows \u03c6(x), and the color follows log(\u03c6(x)), so that\nthe small-probability structure is also visible. The x and y axes are symmetric, of course, because splitting at a and b is the same as splitting at b and a.\nSimilarly the function is largest along horizontal and vertical \"bands.\" This is\nbecause when one split is in a particularly fortuitous place, it tends to improve\nthe fit over all, even if the placement of the second split is suboptimal. The\nhighest two peaks (near the opposite corners), represent splitting on the left\n(in the vicinity of 61 ) to take care of the jump that is there, and on the right (in\nthe vicinity of 0.75) to split the smooth transition region into its higher and\nlower halves. The secondary peaks near the far corner, represent splitting the\nsmooth transition in two places and ignoring the left half (recall that this was\nthe choice made by CART on this data).\n\n4.6\n\nBehavior on a Small Data Set\n\nIt is interesting to see how the posterior responds to individual data points;\nthis is most easily seen in a very small dataset. In Figure 4.6.a I consider a\n\n\f84\n\nCHAPTER 4. EXAMPLES\n\n1\n\n30\n\n20\n\n0.8\n\n10\n\n0\n\n0.6\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 4.6.a: Small Data Set Experiment: On this five element data set, the\nposterior mean curve in magenta is the weighted (c.f. inset histogram) composition of the colored curves. These curves represent the mean contribution\nfrom models with a fixed number of steps which ranges from 1 to 15\n\n\f85\n\nCHAPTER 4. EXAMPLES\n\ndata set with 5 data points, that, as usual are shown by the two histograms.\nIn this case there are two heads on the left and three tails on the right. The\nposterior mean (magenta) is seen to respond in a smooth way, except at the\ndata points where it remains continuous but takes a (small) sharp turn. The\nother colored curves represent the posterior mean when the number of steps\nthat the function is allowed to take is fixed a-priori. The flat black line, for\nexample, is the posterior mean when no splits are allowed (one step). The\nred curve, on the other hand is the posterior mean if 14 splits (15 steps) are\nrequired. Notice how the red curve, especially, drifts back towards\n\n1\n2\n\nfor x-\n\nvalues that are not close to the data points. This happens because with 14\nsplits and 5 data points, there are bound to be many empty intervals and\nthose, necessarily, fall back on the prior. The posterior mean curve for a\nGeometric( 21 ) prior on the number of steps is the weighted average of these\ncurves, where the weights (as percentages) have been tabulated by the inset\nhistogram. Since the 1-split model fits this data so especially well, it winds up\ncontributing about as much as the contant model (which cannot be ruled out\nwith so little data) to the final result. The contribution of the higher models is\nnot forgotten, though; notice how the posterior mean (magenta) drifts back to\n1\n2\n\nslightly on the left of 0.2 and the right of 0.8. Overall the posterior mean is\n\nconservative; it does not, for example, split the data in half at 0.5 and declare\nthe left hand mean to be 1 and the right hand mean to be 0.\n\n4.7\n\nBehavior on a Large Data Set\n\nThe data from experimental run 1, consisted of 1024 (x, y) pairs with the xvalues drawn uniformly from [0, 1] and the y-values drawn Bernoulli(f0 (x)).\nThis section answers the question: how does the posterior change if this data\nset is enlarged to have 8192 datapoints by generating additional data from this\nmodel? As usual the true curve is drawn in blue and the posterior mean for the\nGeometric( 21 ) prior is drawn in magenta. The data set, as is visible from the\n\n\f86\n\nCHAPTER 4. EXAMPLES\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 4.7.a: Large Data Set Experiment: The data from experimental run 1\nis extended to a large data set of size 8192\nKey: True f (blue), Posterior Mean (\u03b1 = 12 ) (magenta), Posterior Mean (\u03b1 =\n0.995) (green), CART (black), min-xerr CART (dashed black)\n\n\fCHAPTER 4. EXAMPLES\n\n87\n\nhistograms, is getting rather large. Pleasantly, the posterior mean estimate is\nalso giving an accurate estimate of the true curve. Somewhat disappointingly,\nthough, the step functions on which the prior is based have not gone away.\nAlthough they are smaller, they are clearly visible in the magenta curve.\nThe CART estimate for this data is shown in black. A couple of observations\nneed to be made. First of all, because the 1-standard-deviation pruning rule\nwas used, the CART curve only has 6 splits. Looking through the full tree,\nthe model that minimizes the cross-validated error has two additional splits\nand is drawn by the dotted black line. This model agrees quite closely with\nthe posterior mean estimate, but two of its splits were pruned away when the\n1-standard-deviation pruning rule was used because the standard deviation of\nthe xerr is not small enough. It might be selected automatically if a more\nintensive cross-validation were used. Finally, note that both CART curves\nminor artifacts (when compared to the posterior mean) that result from the\ngreedy nature of the full tree.\nOne might suppose that with this much data, it ought to be possible to fit a\nmodel with many more steps. This does not seem to be true (at least for a prior\nthat models the success probabilities independently). For example, if the full\nCART tree is manually pruned to have only one or two additional splits beyond\nthe 8 that were used by the minimum xerr model, the additional splits visibly\ndegrade the fit. To understand this, consider that the right half of the data\nshould contain around 4096 points. By chance, they will not be (quite) evenly\ndistributed over this half but for simplicity suppose that these \"4096\" points\nare divided evenly into the 6 intervals selected by the larger CART model. This\nleaves around 680 points in each partition cell. Recall that the variance of a\nBinomial(n, p) random variable is np(1 \u2212 p), so that the standard deviation of\n\nthe estimated success probability on each of these partition cells is going to be\naround 0.02. Considering that the regression estimator has to not only detect\n\na difference, but also locate a good choice of split, and optimize the accuracy of\nthe estimated success probability, it does not seem too unreasonable that the\n\n\fCHAPTER 4. EXAMPLES\n\n88\n\naverage jump in success probability between neighboring cells is around 0.1.\nAlso shown (by the green curve) is the result of computing the posterior\nmean when a Geometric(1 \u2212 \u03b1) prior is used for \u03b1 = 0.995. Interestingly,\n\nthis long tail makes little difference and the green curve barely peaks out from\nunder the magenta one.\n\n4.8\n\nThe Effect of Sample Size\n\nThe previous section developed a extended version of experimental run 1 that\ncontained 8192 data points. In figure 4.8.a, this data is analyzed in more detail.\nSmaller data sets are formed by taking the first n data points for n ranging\nfrom 64 to 8192 by powers of two. It is very pleasant to see how the posterior\nmean incrementally grows closer to the truth. At first, the steps on the left are\nalmost ignored (with so little data, any pattern they contain could have resulted\nfrom noise), but gradually they fill in. The transitions near the change-points\nin f0 become very sharp and the estimates of the smooth transition steadily\nimprove.\n\n4.9\n\nThe Effect of Sample Size: a Harder Example\n\nFor the final example, I consider a much harder regression function. It is\ndepicted in blue in Figure 4.9.a. It was formed by taking multiple copies of\nthe original f0 and shrinking them to half their size repeatedly. A data set\nwith 8192 point is simulated and the posterior mean is calculated for subsets\nof increasing size. As had been hoped, the features get filled in as the data size\nincreases incrementally. The larger features rise above the noise first and then\nthe smaller, so that for this regression function, the approximation seems to\ngrow better as n increases on the right first, but then steadily spreads to the\n\n\f89\n\nCHAPTER 4. EXAMPLES\n\n1\n\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nn = 64\n\n1\n\n0\n0\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nn = 128\n\n1\n\n0\n0\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nn = 256\n\n1\n\n0\n0\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 512\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nn = 1024\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 2048\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 4192\n\n1\n\n0.8\n\n0\n0\n\n0.3\n\n1\n\n0.8\n\n0\n0\n\n0.2\n\n1\n\n0.8\n\n0\n0\n\n0.1\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 8192\n\nFigure 4.8.a: The Effect of Sample Size: The posterior mean is computed as\nsample size n runs through a wide range\n\n\f90\n\nCHAPTER 4. EXAMPLES\n\n1\n\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nn = 64\n\n1\n\n0\n0\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nn = 128\n\n1\n\n0\n0\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nn = 256\n\n1\n\n0\n0\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 512\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nn = 1024\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 2048\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 4192\n\n1\n\n0.8\n\n0\n0\n\n0.3\n\n1\n\n0.8\n\n0\n0\n\n0.2\n\n1\n\n0.8\n\n0\n0\n\n0.1\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 8192\n\nFigure 4.9.a: The Effect of Sample Size: a Harder Example The posterior\nmean is computed as sample size n runs through a wide range on this more\nchallenging problem\n\n\fCHAPTER 4. EXAMPLES\n\n91\n\nleft. Importantly, for the larger n, the posterior mean concentrates on models\nwith many more splits than it did for the easier data since. Presumably this is\nbecause more complex models are necessary to increase the likelihood. To some\nextent this makes sense because even though this example is more complex than\nthe previous one, some aspects of the regression function are relatively easy to\ndetect (e.g. the large jump) and there are more of these features available for\nanalysis. It is interesting to compare the results with the former dataset for\nn = 4096 with the results on the right half of the current dataset for n = 8192.\nThey should be quite comparable because the regression function for this more\ncomplicated model on [ 12 , 1] is just a rescaled version of the original model, and\nin both cases there should be about 4096 datapoints available. To appearances,\nthe two results are quite similar except that the latter result does not do as well\non the small interval from 0.5 to around 0.6. Perhaps it is being confused by\nthe low success probability region immediately to the left of 0.5. Conducting\na similar comparison between the original n = 1024 example the right half\nof the n = 2048 example, the latter result seems substantially inferior. On\nthe other hand, it is not so bad when compared with the original result on\nexperimental run 5. Furthermore, the performance of the posterior mean is\nquite good considering the overall increase in the difficulty of this problem.\nEven more amazing, considering the popular state-of-the-art methods, it does\nall of this without any tuning or cross-validation.\n\n\fChapter 5\nConsistency\nThis chapter establishes conditions under which the prior for one-dimensional\nclassification that was introduced in section 1.2 is a consistent estimator of\nthe true regression function. The consistency of the posterior is proven using\na result by Barron, Schervish, and Wasserman [2]. Before reviewing their\ntheorem, I pause to introduce some notation. I also present a lemma that\nshows that the original conditions given in their theorem are equivalent to\nsome others that may be easier to check. Finally, I specify the prior \u03c0 more\nformally and complete a proof of consistency.\n\n5.1\n\nNotation and the Basic Theorem\n\nLet F be the class of all (Borel) measurable functions f : [0, 1] \u2192 {0, 1}. Write\n\u03bc for the uniform distribution on [0, 1] and \u03b7 for counting measure on the set\n\n{0, 1}. Write Z for the product space [0, 1] \u00d7 {0, 1}, and call the product\nmeasure \u03bd. To any f \u2208 F there is a corresponding density on Z with respect\nto \u03bd that we denote by fe:\nfe(x, y) = f (x)1y = 1 + (1 \u2212 f (x))1y = 0\n92\n\n(5.1)\n\n\f93\n\nCHAPTER 5. CONSISTENCY\n\nFor notational convenience, we may write either fe(x, y) for x \u2208 [0, 1] and\ny \u2208 {0, 1} or fe(z) for z = (x, y) \u2208 Z. Write F = Ff for the distribution\non Z whose density with respect to \u03bd is fe. In words, the consequence of this\nconstruction is that sampling a point Z from F is the same as choosing an\nX uniformly on [0, 1] and then (conditionally on X = x) determining Y by\n\nflipping an \"f (x)\" coin.\nWrite Fe for the class of densities formed by considering fe for every f \u2208 F .\nLet d denote the Hellinger distance on Fe:\nd(fe, fe\u2032) =\n\n\b\n\nZ\n\n1\u0001\n1\nfe(z) 2 \u2212 fe\u2032 (z) 2 \u03bd(dz)\n\n1\n2\n\n(5.2)\n\nAnd let D denote the Kullback-Leibler discrepancy on Fe (employing the\nusual convention that the integrand is interpreted as 0 whenever fe(z) = 0):\nD(fe, fe\u2032 ) =\n\nZ\n\nlog\n\nfe(z) e\nf (z)\u03bd(dz)\nfe\u2032 (z)\n\n(5.3)\n\nWe are concerned with posterior consistency and so the mass that the prior\nor posterior ascribes to certain small sets containing the true parameter fe0 is\n\nof interest. For any \u01eb > 0, we define two such \"neighborhoods:\"\nK\u01eb = {fe \u2208 Fe | D(fe0 , fe) \u2264 \u01eb}\n\n(5.4)\n\nH\u01eb = {fe \u2208 Fe | d(fe0 , fe) \u2264 \u01eb}\n\n(5.5)\n\nThe \"richness\" of the parameter space is also an important quantity; to\nmake this precise we supply the following definitions.\nDefinition 1. Consider a class of functions A that are densities with respect\nto dominating measure \u03bd. We say that the collection of functions {feU , . . . , feU }\n1\n\nis a \u03b4-upper bracketing of A if:\n\n1. for every a \u2208 A there exists i such that a \u2264 feiU a.e. [\u03bd]\n\nr\n\n\f94\n\nCHAPTER 5. CONSISTENCY\n\n2. every feiU satisfies\n\nR\n\nfeiU (z)\u03bd(dz) \u2264 1 + \u03b4\n\nFurthermore, write H(A, \u03b4) for the \u03b4-upper metric entropy of A which is the\n\nlogarithm of the size of the smallest possible \u03b4-upper bracketing of A (infinity,\n\nif no finite bracketings exist).\n\nThe following result can now be stated. It gives conditions on the prior\nthat are sufficient to ensure that the posterior will concentrate on H\u01eb for any\n\u01eb > 0.\nTheorem 4 (Barron, Schervish, and Wasserman 1999). Let \u03bd be a \u03c3finite measure on a measurable space (Z, B), where the \u03c3-field B is separable.\ne a class of probability densities with\nLet \u03c0 be a probability distribution on F,\nrespect to \u03bd. Endow Fe with the Borel \u03c3-field induced by the Hellinger metric\nd. Let fe0 be a certain chosen density with respect to \u03bd and write F0 for the\ncorresponding distribution on Z. Let Z1 , Z2 , . . . be drawn iid from F0 . In the\nnotations explained above, further assume that for every \u01eb > 0:\n1. \u03c0(K\u01eb ) > 0\ne\n2. There exists a sequence {An }\u221e\nn=1 of measurable subsets of F and positive,\nreal numbers d1 , d2 , c, and \u03b4 such that:\n\n(a) \u03c0(Acn ) \u2264 d1 exp(\u2212d2 n) for all n sufficiently large\n(b) H(An , \u03b4) \u2264 cn for all n sufficiently large\n\u0010\n\u0011\n\u221a\n(c) c < (\u01eb \u2212 \u03b4)2 \u2212 \u03b4 /2\n\n(d) \u03b4 < \u01eb2 /4\n\nThen, with probability 1 [under F0\u221e measure], Bayes theorem applies for all n;\ni.e. for any measurable B \u2282 Fe and any n:\nR Qn\n\n\u03c0(B|z1 , . . . , zn ) = RB Qi=1\nn\ne\nF\n\ni=1\n\nfe(zi )\u03c0(dfe)\nfe(zi )\u03c0(dfe)\n\n\fCHAPTER 5. CONSISTENCY\n\n95\n\nAnd for any \u01eb > 0:\nlim \u03c0(H\u01eb |z1 , . . . , zn ) = 1\n\nn\u2192\u221e\n\nThe second condition of this theorem seems rather technical. To restate\nthis theorem in simpler terms we prove an elementary lemma which shows\nthat these conditions (item I in the lemma) can be expressed in three other\nequivalent forms.\nLemma 5. The following conditions, given in the notation defined above, are\nequivalent:\nI For all \u01eb > 0, there exists {An }\u221e\nn=1 , a sequence of measurable subsets of\nFe and positive, real numbers d1 , d2 , c, and \u03b4 such that:\na \u03c0(Acn ) \u2264 d1 exp(\u2212d2 n) for all n sufficiently large\nb H(An , \u03b4) \u2264 cn for all n sufficiently large\n\u0010\n\u0011\n\u221a\nc c < (\u01eb \u2212 \u03b4)2 \u2212 \u03b4 /2\n\nd \u03b4 < \u01eb2 /4\n\ni\nII There exists a sequence of positive real numbers {\u03b4 i }\u221e\ni=1 , with \u03b4 \u2193 0, and\n\u221e\n{{Ai }\u221e } , a sequence of sequences of measurable subsets of Fe such\nn n=1 i=1\n\nthat:\n\na for all i, lim supn log(\u03c0 ((Ain )c )) /n < 0\nb lim supi lim supn H(Ain , \u03b4 i )/n = 0\nIII For all \u01eb > 0 there exists {An }\u221e\nn=1 , a sequence of measurable subsets of\nFe and a positive, real number \u03b4 \u2264 \u01eb such that:\na lim supn log(\u03c0 ((An )c )) /n < 0\nb lim supn H(An , \u03b4)/n \u2264 \u01eb\n\n\f96\n\nCHAPTER 5. CONSISTENCY\n\nIV For all \u01eb > 0 there exists {An }\u221e\nn=1 , a sequence of measurable subsets of\nFe such that:\na lim supn log(\u03c0 ((An )c )) /n < 0\nb lim supn H(An , \u01eb)/n \u2264 \u01eb\nProof. I =\u21d2 II:\nFirst, notice that the condition from I.a that \"there exist d1 > 0 and\nd2 > 0 so that \u03c0(Acn ) \u2264 d1 exp(\u2212d2 n) for n sufficiently large\" implies that\n\nlim supn log(\u03c0(Acn ))/n \u2264 \u2212d2 < 0. Conversely, if lim supn log(\u03c0(Acn ))/n = \u03b1 <\n0, then for all sufficiently large n, log(\u03c0(Acn ))/n \u2264 \u03b1/2 and \u03c0(Acn ) \u2264 exp( \u03b12 n)\n\nfor all sufficiently large n.\n\nChoose a sequence of \u01ebi > 0, \u01ebi \u2193 0 and use I to establish the existence\n\nof {Ain }, di1, di2 , ci , \u03b4 i satisfying I for \u01ebi . Since \u03b4 i < (\u01ebi )2 /4 \u2193 0, we can find a\nsubsequence on which \u03b4 i \u2193 0. Without loss of generality, assume we already\n\nhave such a subsequence. The above reasoning, then, establishes II.a for this\nsequence. Since I.a implies that lim supn H(Ain , \u03b4 i )/n \u2264 ci for all i, to estab-\n\ni\nlish II.b,\n\u0010 we\u221aneed only\n\u0011 show that lim supi c = 0. Condition I.c constrains\nci < (\u01ebi \u2212 \u03b4 i )2 \u2212 \u03b4 i /2. Viewing this as a function of \u03b4 i , notice that it is\n\nmonotonically decreasing on [0, (\u01ebi )2 /4] so that, necessarily, ci < (\u01ebi )2 /2, the\nvalue of this function at \u03b4 i = 0.\nII =\u21d2 I:\nConsider some \u01eb > 0. Note that\n\n\u0010\n\u0011\n\u221a\n(\u01eb \u2212 \u03b4 i )2 \u2212 \u03b4 i /2 \u2191 \u01eb2 /2 as i \u2192 \u221e.\n\nFind an i sufficiently large so that this expression is at least \u01eb2 /3. Then find\n\na subsequent i sufficiently large so that lim supn H(Ain , \u03b4 i )/n \u2264 \u01eb2 /5. Choose\nc = \u01eb2 /4, and \u03b4 = \u03b4 i , and I is proven.\n\nII \u21d0\u21d2 III: This is a straightforward exercise in nitpicking.\n\nIII \u21d0\u21d2 IV: Observe from the definition that H(An , \u03b4) is a non-increasing\n\nfunction of \u03b4.\n\n\f97\n\nCHAPTER 5. CONSISTENCY\n\nTheorem 6 (Corollary to Barron, Schervish, and Wasserman 1999).\nLet \u03bd be a \u03c3-finite measure on a measurable space (Z, B), where the \u03c3-field\ne a class of probability\nB is separable. Let \u03c0 be a probability distribution on F,\ndensities with respect to \u03bd. Endow Fe with the Borel \u03c3-field induced by the\nHellinger metric d. Let fe0 be a certain chosen density with respect to \u03bd and\n\nwrite F0 for the corresponding distribution on Z. Let Z1 , Z2 , . . . be drawn iid\n\nfrom F0 . In the notations explained above, further assume that for every \u01eb > 0:\n1 \u03c0(K\u01eb ) > 0\ne\nThere exists a sequence {An }\u221e\nn=1 of measurable subsets of F, so that:\n2 lim supn log(\u03c0 ((An )c )) /n < 0\n\n3 lim supn H(An , \u01eb)/n \u2264 \u01eb\nThen, with probability 1 [under F0\u221e measure], Bayes theorem applies for all n;\ni.e. for any measurable B \u2282 Fe and any n:\nR Qn\n\nAnd for any \u01eb > 0:\n\n\u03c0(B|z1 , . . . , zn ) = RB Qni=1\ne\nF\n\ni=1\n\nfe(zi )\u03c0(dfe)\nfe(zi )\u03c0(dfe)\n\nlim \u03c0(H\u01eb |z1 , . . . , zn ) = 1\n\nn\u2192\u221e\n\nIn words, this is what we have required: 1) the prior must put positive mass\non all Kullback-Leibler neighborhoods of fe0 . 2) We must be able to choose an\nincreasing sequence of subsets of the parameter space so that the n'th of these\nsets captures all but exponentially much of the prior mass 3) This sequence\n\nmust not grow in \"complexity\" too quickly. We conclude that the posterior\nwill concentrate on the subset of the parameter space which is Hellinger close\nto the true parameter.\n\n\f98\n\nCHAPTER 5. CONSISTENCY\n\n5.2\n\nSpecification of the Prior\n\nWe describe \u03c0 as a distribution on Fe by means of first describing a para-\n\nmetric prior \u03c0 \u2032 . Let \u03ba be a distribution on the positive integers. Let \u0398k =\n\u2032\n{(k, v, s) : v \u2208 [0, 1]k\u22121, s \u2208 [0, 1]k } and let \u0398 = \u222a\u221e\nk=1 \u0398k . Let \u03c0 be the\n\ndistribution on \u0398, that can be described by first picking K according to \u03ba;\n\nand then, conditional on K = k, picking a point \u03b8 \u2208 \u0398 uniformly from \u0398k .\n\nThat is, v and s are chosen independently and uniformly from the appropriate\nunit cubes. Let v(i) denote the i'th ordered value of v. Now to any \u03b8 \u2208 \u0398\n\nassociate the function f\u03b8 \u2208 F given by the following construction. For k > 1,\n\nlet I1 = [0, v(1) ), I2 = [v(2) , v(3) ), . . . , Ik\u22121 = [v(k\u22122) , v(k\u22121) ), Ik = [v(k\u22121) , 1]. If\nP\nk = 1, just let I1 = [0, 1]. Take f\u03b8 (x) = ki=1 si 1x \u2208 Ii . Finally, then, to draw\nfe from \u03c0, draw \u03b8 from \u03c0 \u2032 , construct f\u03b8 , and form fe\u03b8 as in Equation 5.1.\n\n5.3\n\nA Consistency Proof\n\nThis section proves that the prior \u03c0 just described is consistent in the sense\nthat, under repeated sampling, the posterior mass will concentrate on a Hellinger\nneighborhood H\u01eb for any \u01eb > 0. The proof uses three lemmas to establish the\nconditions of Theorem 6.\nThe first lemma shows that \u03c0 puts mass on all Kullback-Leibler neighborhoods K\u01eb .\nLemma 7. Let \u03c0 be the prior distribution on Fe, the class of densities w.r.t \u03bd,\n\nthat was described above. Assume that \u03ba, the hierarchy prior, assigns positive\nmass to every natural number. Let fe0 be an arbitrary density in Fe. Then, for\n\nany \u01eb > 0, \u03c0(K\u01eb ) > 0.\n\nProof. Let f0 be the corresponding function in F . For some 0 < \u03b41 <\n\n1\n,\n16\n\ndetermined later, let:\n\nG = {g \u2208 F : ||g \u2212 f0 ||1 \u2264 2\u03b41 and \u03b41 \u2264 g(x) \u2264 1 \u2212 \u03b41 (\u2200x)}\n\nto be\n\n\f99\n\nCHAPTER 5. CONSISTENCY\n\n1\n\n(0,1\u2212\u03b41)\n\n(1\u2212\u03b41\u2212\u03b42,1\u2212\u03b41)\n\n(1,1\u2212\u03b41)\n\nC\n\nR\n(0,\u03b41)\n\n(\u03b41+\u03b42,\u03b41)\n\n(1,\u03b41)\n\n0\n0\n\n1\n\nFigure 5.3.a: The sets C and R\nLet Ag = {x \u2208 [0, 1] : |g(x) \u2212 f0 (x)| \u2264 \u03b42 } where \u03b42 :=\n\n\u221a\n\n2\u03b41 . By\n\nc\n\nChebyshev's inequality, for any g \u2208 G, the Lebesgue measure of Ag is smaller\n\nthan \u03b42 .\n\nIn other words, as shown in Figure 5.3.a, if x \u2208 Ag , this restricts the pair\n\n(f0 (x), g(x)) to lie in the convex set C (blue) whose extreme points are (0, \u03b41 ),\n(\u03b41 +\u03b42 , \u03b41 ), (1, 1\u2212\u03b41 ), (1\u2212\u03b41 \u2212\u03b42 , 1\u2212\u03b41 ). For x \u2208\n/ Ag , we still have a restriction\n\non g(x), so that the pair (f0 (x), g(x)) lies in the rectangle R (green) with\nvertices (0, \u03b41 ), (1, \u03b41 ), (1, 1 \u2212 \u03b41 ), (0, 1 \u2212 \u03b41 ). Let D1 (p, p\u2032 ) denote the Kullback-\n\nLeibler discrepancy between the Bernoulli(p) and Bernoulli(p\u2032 ) distributions.\nD1 (p, p\u2032 ) := p log\n\np\n1\u2212p\n+ (1 \u2212 p) log\n\u2032\np\n1 \u2212 p\u2032\n\n\f100\n\nCHAPTER 5. CONSISTENCY\n\nThen for any g \u2208 G we can bound D(fe0 , e\ng ), as follows:\nD(fe0 , e\ng) =\n\n\u2264\n\nZ\n\nAg\n\nZ\n\nZ\n\nD1 (f0 (x), g(x)) +\nZ\nsup D1 (a, b) +\n\nAg c\n\nD1 (f0 (x), g(x))\n\n(5.6)\n\nsup D1 (a, b)\n\n(5.7)\n\nAg c (a,b)\u2208R\n\nAg (a,b)\u2208C\n\nand because D1 (a, b) is convex in the pair (a, b) [13], the supremum is achieved\nat the vertices so that the above is bounded by:\n\u2264\n\nmax\n\n(a,b)\u2208{vertices of C}\n\nD1 (a, b) + \u03b42\n\nmax\n\n(a,b)\u2208{vertices of R}\n\nD1 (a, b)\n\n(5.8)\n\nUsing the symmetry D1 (a, b) = D1 (1 \u2212 a, 1 \u2212 b)\n\u2264 max (D1 (0, \u03b41 ), D1 (\u03b41 + \u03b42 , \u03b41 )) + \u03b42 max (D1 (0, \u03b41 ), D1 (0, 1 \u2212 \u03b41 ))\n\u0012\n= max \u2212(1 \u2212 \u03b41 ) log(1 \u2212 \u03b41 ),\n\n\u03b41 + \u03b42\n1 \u2212 \u03b41 \u2212 \u03b42\n(\u03b41 + \u03b42 ) log(\n) + (1 \u2212 \u03b41 \u2212 \u03b42 ) log(\n)\n\u03b42\n1 \u2212 \u03b41\n\n(5.9)\n\n\u0013\n\n+ \u03b42 (\u2212 log(\u03b41 ))\n\n(5.10)\n\u2264 max (\u2212 log(1 \u2212 \u03b41 ), 2 log(2)\u03b42 ) + \u03b42 (\u2212 log(\u03b41 ))\n\n(5.11)\n\n\f101\n\nCHAPTER 5. CONSISTENCY\n\nFor small \u03b41 , the last term is the most important. To simplify the first term,\nverify that for 0 \u2264 \u03b4 \u2264 12 , \u2212 log(1 \u2212 \u03b4) \u2264 \u03b4 + \u03b4 2 \u2264 23 \u03b4\n\u0012\n\n\u0013\n3\n\u2264 max\n\u03b41 , 2 log(2)\u03b42 + \u03b42 (\u2212 log(\u03b41 ))\n2\n3\n\u2264 \u03b42 + \u03b42 (\u2212 log(\u03b41 ))\n2\np\n3\n= 2\u03b41 (\u2212 log(\u03b41 ) + )\n2\n\n(5.12)\n(5.13)\n(5.14)\n\nWhich tends to zero as \u03b41 \u2193 0. For a sufficiently small choice of \u03b41 , then:\nGe = {e\ng : g \u2208 G} \u2282 K\u01eb = {fe \u2208 Fe : D(fe0 , fe) < \u01eb}.\n\nIt remains to show that Ge has positive prior mass. To do this, we first find\n\na step function g0 \u2208 G which approximates f0 and then show that G \u2217 , a set of\nf\u2217 has positive prior mass.\nperturbations of g0 , remains in G and that G\nObserve that since f0 is Lebesgue-measurable there exist two increasing\n\n\u2212\n+\n\u2212\nsequences of step functions h+\ni and hi for which ||f0 \u2212 (hi \u2212 hi )||1 \u2192 0. This\n\nis, in fact, the basis of a common construction of the Lebesgue integral [42].\nConsequently, we can find a step function h \u2208 F for which ||f0 \u2212 h||1 \u2264 \u03b41 /2.\n\nLet g0 be the function obtained by modifying h so that it always remains\n\nin [\u03b41 , 1 \u2212 \u03b41 ], i.e. g0 (x) = max(min(h(x), 1 \u2212 \u03b41 ), \u03b41 ), so that g0 \u2208 G and\n\n||f0 \u2212 g0 ||1 \u2264 32 \u03b41 .\n\nNow, parameterize g0 in the manner of section 5.2 (changing it at a set of\n\nmeasure 0 if necessary) so that k < \u221e is the number of locally constant regions\n\nk\u22121\nin g0 and the vectors v = (vi )i=1\nand s = (si )ki=1 signify the change-points and\n\nsuccess probabilities of g0 .\nIf we then perturb each si towards the value\n\n1\n2\n\nby no more than \u03b41 /4 we\n\nobtain a new function g \u2032 which remains in G and satisfies ||g0 \u2212 g \u2032 ||1 \u2264 \u03b41 /4.\nIf, in addition, we perturb the vi 's by no more than \u03b41 /(4(k \u2212 1)) we obtain g \u2032\u2032\n\nwhich also remains in G, satisfying ||g0 \u2212 g \u2032\u2032 ||1 \u2264 \u03b41 /2 or ||f0 \u2212 g \u2032\u2032 ||1 \u2264 2\u03b41 .\n\n\f102\n\nCHAPTER 5. CONSISTENCY\n\ne \u2265 \u03c0(G\nf\u2217 ) \u2265\nDenote the class of functions thus obtained by G \u2217 . Then \u03c0(G)\n\n\u03ba{k}(\u03b41 /4)k (\u03b41 /(2(k \u2212 1)))k\u22121 > 0.\n\nRemark 1. A shorter proof can be based on the martingale sequence formed\nfrom conditional expectations of f0 .\nFor the second lemma, consider Fm , the class of functions which are con-\n\nstant except possibly for as many as m \u2212 1 change-points.\n\nDefinition 2. Using the notation from section 5.2, let Fm = {f\u03b8 : \u03b8 \u2208 \u0398m }.\nLet Fem be the associated class of densities with respect to \u03bd:\nFem = {fe(x, y) = f (x)1y = 1 + (1 \u2212 f (x))1y = 0 : f \u2208 Fm }\n\nLemma 8. The \u03b4-upper bracketing entropy of the class Fem is no more than\n\n(2m \u2212 1) log(\u2308m/\u03b4\u2309).\n\nProof. Fix a, b positive integers. Partition [0, 1] into a equal intervals I1 , . . . , Ia .\nConsider the class Hm;a,b of all functions feU that can be formed in the following\n\nway: Choose some m \u2212 1 of these intervals. Let C be the union of the chosen\n\nintervals and let C1 , . . . , Ck be the nonempty subintervals of [0, 1] formed by\n\nsubtracting C (k \u2264 m). Finally choose B1 , . . . Bk \u2208 {1, . . . , b}. Construct the\nfunction feU : [0, 1] \u00d7 {0, 1} 7\u2192 [0, 1] by:\nfeU (x, y) =\n\n\uf8f1\n\uf8f4\n\uf8f4\n1\n\uf8f4\n\uf8f2\n\nif x \u2208 C\n\nBi /b\nif x \u2208 Ci and y = 1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f31 \u2212 (B \u2212 1)/b if x \u2208 C and y = 0\ni\ni\n\nIt is easy to see that, by appropriate choices, for any fe \u2208 Fem we can find\nan feU \u2208 Hm;a,b that is greater than or equal to it globally. Furthermore, the\nintegral of feU is less than or equal to 1 + 1/b + (m \u2212 1)/a. The size of Hm;a,b\n\u0001 m\na\nis \u2264 m\u22121\nb \u2264 am\u22121 bm . By choosing a = b = \u2308m\u03b4 \u22121 \u2309, we have shown that\nexp(H(Fem , \u03b4)) \u2264 (\u2308m\u03b4 \u22121 \u2309)2m\u22121 .\n\n\f103\n\nCHAPTER 5. CONSISTENCY\n\nThe final lemma establishes a result about the tail of Poisson random\nvariables.\nLemma 9. If K \u223c Poisson(\u03bb) for some \u03bb > 0, then for any k > \u03bb:\n\u2212\u03bb \u03bb\n\nP(K \u2265 k) \u2264 e\n\n\u0012\n\nk\n\nk!\n\nk\nk\u2212\u03bb\n\n\u0013\n\nAnd, consequently, for any 0 < \u03b2 < 1 there is a k0 sufficiently large so that for\nevery k \u2265 k0 , P(K \u2265 k) \u2264 exp(\u2212\u03b2k log(k)).\nProof.\n\u2212\u03bb\n\nP(K \u2265 k) = e\n\n\u221e\nX\n\u03bbi\n\n\u2212\u03bb \u03bb\n\n=e\n\ni!\n\ni=k\nk\n\nk!\n\n\u0015\n\u221e \u0012 \u0013i\u2212k \u0014\nX\n\u03bb\nk!k i\u2212k\nk\n\ni=k\n\ni!\n\nNow, the bracketed term is no more than 1 and we are left with a geometric\nseries whose factor, \u03bb/k is less than 1 by our assumption so that:\n\u0012\n\u0013\u22121\n\u03bb\nP(K \u2265 k) \u2264 e\n1\u2212\nk!\nk\n\u0012\n\u0013\nk\nk\n\u2212\u03bb \u03bb\n=e\nk! k \u2212 \u03bb\n\u2212\u03bb \u03bb\n\nk\n\nSo that for k sufficiently large, from Stirling's formula:\nlog(P(K \u2265 k)) \u2264 \u2212 log(k!) \u2212 \u03bb + k log(\u03bb) + log\n\u2264 \u2212\u03b2k log(k)\n\nThe main result can now be established.\n\n\u0012\n\nk\nk\u2212\u03bb\n\n\u0013\n\n\f104\n\nCHAPTER 5. CONSISTENCY\n\nTheorem 10. Suppose that the prior \u03c0, described in section 5.2 is based on\nthe hierarchy prior \u03ba. Suppose that \u03ba gives positive probability to every natural\nnumber and that its tail satisfies: \u03ba({k \u2208 N : k \u2265 j}) \u2264 exp(\u2212\u03b2j log(j))\n\nfor all j sufficiently large and some \u03b2 > 0. Let f0 be an arbitrary measurable function from [0, 1] into [0, 1]. Suppose that Z1 , Z2 , . . . are drawn iid from\nthe distribution F0 , which has density fe0 with respect to \u03bd. Equivalently, suppose that the Zi 's (Zi = (Xi , Yi )) are drawn as follows: the Xi 's are drawn\n\nindependently and uniformly from [0, 1]; and, conditional on Xi = xi , Yi is\n\nan independent Bernoulli(f0 (xi )) random variable. Then, in the notation of\nsection 5.1, for any \u01eb > 0, \u03c0(H\u01eb |z1 , . . . , zn ) \u2192 1 as n \u2192 \u221e [F0 \u221e -a.s.]. Specifically, for any 1 \u2264 p < \u221e, and any \u01eb > 0, the posterior mass on the set\n{fe \u2208 Fe : ||f \u2212 f0 ||p < \u01eb} tends to 1 as n \u2192 \u221e [F0 \u221e -a.s.].\nProof. Let m(n) := \u230a\u03b1n/ log(n)\u230b. Choose the sequence {An } as An = Fem(n) .\n\nThen, by Lemma 8, H(An , \u01eb) \u2264 (2m(n) \u2212 1) log(\u2308m(n)/\u01eb\u2309). Observe that\nm(n) \u2191 \u221e as n \u2192 \u221e. For n large enough, then:\n\nH(An , \u01eb) \u2264 2m(n) log(\u03b1n/ log(n)\u01eb\u22121 + 1)\n\u2264 2\u03b1n/ log(n)[log(n/ log(n)) + log(\u03b1) \u2212 log(\u01eb) + 1]\n\u2264 3\u03b1n\n\nlog(n/ log(n))\nlog(n)\n\nChoosing \u03b1 = \u01eb/3, we have proven that lim sup H(An , \u01eb)/n \u2264 \u01eb.\nNow calculate that for all sufficiently large n,\n\n\u2212 log(\u03c0(An c )) = \u2212 log(\u03ba({k \u2208 N : k \u2265 m(n) + 1}))\n\u2265 \u03b2[m(n) + 1] log(m(n) + 1)\n\u2265 \u03b2[\u03b1n/ log(n)] log(\u03b1n/ log(n))\n= \u03b1\u03b2n\n\nlog(n) \u2212 log(log(n)) + log(\u03b1)\nlog(n)\n\n1\n\u2265 \u03b1\u03b2n\n2\n\n\f105\n\nCHAPTER 5. CONSISTENCY\n\nConsequently, lim sup log(\u03c0(An c ))/n \u2264 \u2212 21 \u03b1\u03b2 < 0.\n\nFinally, recall that Lemma 7 shows that for any \u01eb > 0, \u03c0(K\u01eb ) > 0. Apply\n\nTheorem 6 to complete the proof.\nThe statement about || * ||p follows from the equivalence of the Lp and\n\nHellinger metrics for bounded densities. This fact and other useful inequalities\n\nabout Hellinger distance d (aka Jeffrey's distance) are reviewed in [22, section\n5.8 and excercise 5.7] which states:\nd(f, g)2 \u2264 ||f \u2212 g||1 \u2264 2d(f, g)\n\n(5.15)\n\nFinally, the equivalence of the L1 -norm and the Lp norm for 1 \u2264 p < \u221e and\nfor all densities uniformly bounded by a constant, is well known.\n\nRemark 2. Applying Lemma 9 shows that the preceding theorem applies to\nany hierarchy prior \u03ba whose tail behaves like that of a Poisson(\u03bb), for any\n\u03bb > 0. The theorem does not apply to the case in which \u03ba is Geometric.\nRemark 3. The restrictions on the tail of the prior occur because condition\n3 in Theorem 6 requires that the \"sieve\" sets An do not grow in \"size\" too\nquickly as n grows. The choice A = Fem(n) made in the proof of this theorem\nis (essentially) the fastest rate of growth that this situation permits (in the\n\nabsence of better bracketing estimates). Accordingly, condition 2 of Theorem 6\nrequires that the tail of the prior drops off somewhat faster than a Geometric\n\ndistribution.\nRemark 4. For further discussion of how to interpret this result, please see the\ndiscussion in chapter 6.\n\n\fChapter 6\nDiscussion of Consistency\nResults\nIt is challenging to suggest what the practical consequences (if any) of Theorem 10 are. It proves that under the modeling set up with iid observations\nthat has been considered throughout this thesis, that the posterior of the random split prior \u03c0 is a consistent estimate of any measurable true regression\nfunction if the tail of the prior decays at least as fast as exp(\u2212\u03b2j log(j)) for\nsome \u03b2 > 0. Perhaps it would be wise not to over interpret the result. After\nall, it only supplies a sufficient condition for consistency and does not either\nestablish that Geometric priors lead to inconsistent estimators or that Poissonlike priors are a good (i.e. practical) idea. Additionally, it attempts to prove\nconsistency in a fairly strong sense: that the posterior mass concentrates on\nHellinger neighborhoods of the truth. Weaker consistency results, say that the\nposterior mean be L2 consistent, might go through under milder assumptions.\n\n106\n\n\fCHAPTER 6. DISCUSSION OF CONSISTENCY RESULTS\n\n6.1\n\n107\n\nConsideration of the Diaconis and Freedman Results\n\nJudging from the results of Diaconis and Freedman for their prior, as discussed\nin subsection 2.2.1, it might, in fact, be the case that the posterior is consistent\nfor any hierarchy prior \u03ba (with full support), so long as f0 is not the constant\nfunction 12 . That is, it might be that the only situation in which \u03c0 is inconsistent\n(even using a \"poor\" choice of \u03ba) is when there is no real pattern at all in\nthe data because every coin flip was fair. Note that if this were our only\nconcern, namely that the estimates might be inconsistent under some specific\nfinite collection of possible scenarios, this could be easily addressed (albeit in a\ndecidedly non-Bayesian way), by choosing a prior that puts point mass on the\ntroublesome cases. Consistency for these exceptions would then be guaranteed\nby a suitable application of Doob's result [29]. Interestingly, it could destroy\nconsistency for other cases. An example of such a mixture is given by Diaconis\nand Freedman [31]. Of course, a true subjective Bayesian would never change\nhis or her prior in this way. Rather, unless these cases actually are a subjective\nimpossibility, a purist would merely see this as an explication of why their true\nprior (that they are perhaps still in the process of articulating) differs from the\nformer one.\n\n6.2\n\nAn Experiment to Check a Worrisome Case\n\nIt seems reasonable to try and test for the possibility of inconsistency when the\ntrue function f0 \u2261\n\n1\n2\n\nby running a simulation experiment. To do this, generate\n\nan increasing sequence of data sets over a range of sizes that are drawn from\nthe f0 \u2261\n\n1\n2\n\ndistribution. That is, since there is no true signal at all in the data,\n\nit will be interesting to see how the posterior mean responds. For a Poisson\nprior, the posterior mean should (at least eventually) settle down, but will\nthis hold for a Geometric prior? Indeed, the results in figures 6.2.a, and 6.2.b\n\n\fCHAPTER 6. DISCUSSION OF CONSISTENCY RESULTS\n\n108\n\nindicate that both estimates correctly identify the null case, whether using a\nPoisson(5) prior or a Geometric( 21 ) one respectively. A new feature of these\nfigures is the small histogram included on each one. It is a histogram of the\nposterior on the number of steps K in the unknown function. The Poisson(5)\nprior starts out believing that there will be a good number of steps in the\ndata. This is clearly reflected in the histograms in Figure 6.2.a for n = 64 and\nn = 128. The posterior mean in these cases has more \"wiggle\" than for the\nGeometric( 12 ) prior even though both estimates see the same data. For n = 512\nboth estimates find that the posterior mean is roughly constant, but somewhat\nlower than 21 . Apparently, this is a real feature of the data and not an artifact\nof either prior. Eventually, for large n all of these minor considerations wash\nout and clear preference for very flat models has triumphed. Interestingly, from\nn = 512 or so onwards, looking at the histogram of K, the Geometric prior\nhas become convinced that the model has only one split. The Poisson prior\nis only beginning to reach this level of certainty about the truth as n reaches\n8192.\n\n6.3\n\nTheory versus Practice\n\nNevertheless, if the user insists upon using an estimation procedure that has\nbeen proven to be consistent (say, by the preceding theorem), he or she still\nhave a great deal of freedom. Roughly speaking, they can use any hierarchy\nprior they like on the first 100!!! natural numbers and append to it an arbitrary\nPoisson tail. It is hard to imagine that there would be any practical difference between the estimators resulting from this \"extended\" prior and those\nresulting from the unmodified one, at least for realistic sample sizes. There\ncertainly would not be any difference in practice, because in practice we only\napproximately compute the posterior anyway and no ordinary Markov chain\nMonte Carlo would ever be run long enough to notice the change.\n\n\f109\n\nCHAPTER 6. DISCUSSION OF CONSISTENCY RESULTS\n\n1\n\n1\n\n8000\n\n15000\n6000\n10000\n\n4000\n\n0.8\n\n0.8\n5000\n\n2000\n0\n\n5\n\n10\n\n15\n\n20\n\n0\n\n25\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\nn = 64\n\n1\n\n0.5\n\n0.6\n\n1.5\n\n10\n\n15\n\n20\n\n0\n\n25\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\nn = 128\n\n1\n\n0.9\n\n25\n\n1\n\n0.5\n5\n\n0.6\n\n0.2\n\n0.8\n\n20\n\n1\n\n0.8\n\n0\n\n0.1\n\n15\n\n4\n\n2\n\n6000\n4000\n\n10\n\nx 10\n\n1\n\n8000\n\n2000\n\n0\n0\n\n0.7\n\nn = 1024\n10000\n\n0.8\n\n0.4\n\n5\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\nn = 2048\n\n10\n\n15\n\n0.8\n\n20\n\n0.9\n\n25\n\n1\n\n4\n\nx 10\n\n1\n\n10000\n\n5\n\n2.5\n2\n\n0.8\n\n1.5\n\n5000\n\n0.8\n5\n\n10\n\n15\n\n20\n\n25\n\n0.6\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0\n0\n\n1\n0.5\n\n0\n\n0.2\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\nn = 256\n\n1\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\nn = 4192\n\n0.8\n\n0.9\n\n1\n\n4\n\nx 10\n\n1\n\n3\n\n10000\n2\n\n0.8\n\n5000\n\n0.8\n0\n\n5\n\n10\n\n15\n\n20\n\n1\n\n25\n\n0.6\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n0.6\n0.4\n0.4\n0.2\n0.2\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 512\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\nn = 8192\n\nFigure 6.2.a: Null Case with Poisson(5) Prior\n\n0.8\n\n0.9\n\n1\n\n\f110\n\nCHAPTER 6. DISCUSSION OF CONSISTENCY RESULTS\n\n4\n\n1\n\nx 10\n\n1\n\n15000\n\n4\n10000\n\n0.8\n\n3\n2\n\n0.8\n\n5000\n\n1\n0\n\n5\n\n10\n\n15\n\n20\n\n0\n\n25\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\nn = 64\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.5\n\n0.6\n\n0.7\n\nn = 1024\n\n4\n\nx 10\n\n1\n\n0.4\n\n10\n\n15\n\n0.8\n\n20\n\n0.9\n\n25\n\n1\n\n4\n\nx 10\n\n1\n\n2\n\n5\n\n4\n1.5\n3\n1\n\n0.8\n\n1\n\n0\n\n5\n\n10\n\n15\n\n20\n\n0\n\n25\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\nn = 128\n\n2\n\n0.8\n\n0.5\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.5\n\n0.6\n\n0.7\n\nn = 2048\n\n4\n\nx 10\n\n1\n\n0.4\n\n10\n\n15\n\n0.8\n\n20\n\n0.9\n\n25\n\n1\n\n4\n\nx 10\n\n1\n\n3\n\n5\n\n4\n2\n\n0.8\n\n3\n\n0.8\n\n1\n\n2\n1\n\n0\n\n5\n\n10\n\n15\n\n20\n\n0\n\n25\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\nn = 256\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.5\n\n0.6\n\n0.7\n\nn = 4192\n\n4\n\nx 10\n\n1\n\n0.4\n\n10\n\n15\n\n0.8\n\n20\n\n0.9\n\n25\n\n1\n\n4\n\nx 10\n\n1\n\n4\n\n5\n\n4\n3\n3\n2\n\n0.8\n\n0.8\n\n1\n5\n\n10\n\n15\n\n20\n\n0\n\n25\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nn = 512\n\n2\n1\n\n0\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n5\n\n0.7\n\n10\n\n0.8\n\nn = 8192\n\nFigure 6.2.b: Null Case with Geometric( 12 ) prior\n\n15\n\n20\n\n0.9\n\n25\n\n1\n\n\fCHAPTER 6. DISCUSSION OF CONSISTENCY RESULTS\n\n6.4\n\n111\n\nHeuristics about Poisson and Geometric\nPriors\n\nDespite these concerns, I still (tentatively) advocate the use of a Geometric\nprior because of the following heuristic arguments: (a) it favors simple models\n(b) its tail does not drop off \"too quickly\" so that it will hopefully not require\nenormous amounts of evidence for the data to \"overwhelm\" the prior: by ruling\nout simple models in favor of better fitting models (c) its tail does drop steadily;\nhopefully this will protect us from \"over-fitting\" (d) if we consider the mode\nof the posterior (on a log scale), the geometric prior penalizes each additional\nsplit by a constant (log(\u03b1)) so that for the mode to shift to a model with an\nadditional split we require a commensurate improvement in the log-likelihood\nof the data.\nIf using a Poisson(\u03bb) prior, I would be concerned that I might have specified a parameter \u03bb that was too small. If the true regression function were\nunexpectedly complicated, it might take a large amount of data to overwhelm\nthe prior. Interestingly, if we attempt to remedy this by taking an exponential\nmixture of Poisson's, we get back a Geometric prior.\nAnother objection of mine is that (for modestly large \u03bb) the Poisson prior\nputs less mass on models with one region than on models with two regions.\nThis makes sense if I actually expect that the regression function will be fairly\ncomplicated, but it violates my (frequentist) training to consider a complex\nmodel before \"eliminating\" the simpler one.\nAs a compromise I would propose a prior on K that was Geometric until\na certain point k0 and then decays like a Poisson. On the other hand, if, in\nfact, Geometric priors prove reliable or even conservative, it might make sense\nto consider an even heavier tailed distribution. A modest proposal is to take a\nuniform mixture of Geometric(p) priors for p ranging from 0 to 1. This results\nin a prior whose tail decays like 1/k, the mass at K = k being [k(k + 1)]\u22121 .\n\n\f112\n\nCHAPTER 6. DISCUSSION OF CONSISTENCY RESULTS\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 6.5.a: The Result of Using a Poisson Prior: Consider again the data\nfrom experimental run 1, but apply a Poisson(\u03bb) prior with \u03bb = 1, 5, or 10\nKey: True f (blue), Poisson(1) prior (cyan), Poisson(5) prior (black),\nPoisson(10) prior (green)\n\n6.5\n\nConclusions\n\nNone of these arguments is conclusive. In the absence of sound theoretical\narguments it is perhaps best to rely on experimental evidence. From chapter 4\nthere is a good bit of evidence that Geometric priors perform well. What do\nPoisson priors do when applied to these data sets? In Figure 6.5.a the posterior\nmean resulting from a Poisson prior for three values of \u03bb is plotted. The results\nare comparable to what happened as \u03b1 was varied in Figure 4.4.a. The prior\nwith the shorter tail flattens out some bumps that the prior with the longer\ntail leaves in.\nStill, for some applications especially, the practical question remains: how\nto choose \u03b1 (respectively \u03bb)? Experience in the problem domain is the only\nmethod I can readily propose. Alternatives, like using cross-validation or an\n\n\fCHAPTER 6. DISCUSSION OF CONSISTENCY RESULTS\n\nempirical Bayes approach remain attractive, but unproven.\n\n113\n\n\fChapter 7\nExtensions\nThere are numerous ways in which to extend \u03c0, but perhaps the most pressing\nis to extend \u03c0 to multi-dimensional data sets Dn = {(Xi , Bi )}ni=1 where the\n\npredictor Xi is in Rd . One route to extend \u03c0 to higher dimensional problems is\nto observe that basically, all we need to consider is a suitable way to partition\nthe space randomly. If an interesting way to choose a partition at random is\nfound, then describe a new prior by saying: draw a partition and give each\nregion an independent uniform success probability. One natural way to randomly partition Rd is to suppose that a certain number of generating points are\ndrawn from a Poisson process with constant rate function \u03bb and to associate\neach point with its Voronoi (nearest neighbor) region. Alternatively, one could\nselect a subset of the observed x-values at random and use their locations to\ndetermine a Voronoi partition. This alternative is, unfortunately, not a purely\nBayesian proposal since the partitioning depends on the data set given. On the\nother hand, it only depends on the x-values of the data set, so that it remains\na Bayesian procedure with respect to the response data (the y-values).\nTo be specific, the prior I consider (call it \u03c0 \u2217 ) can be described by the following. Let x1 , . . . , xn denote the n observed values of the covariates in X and\n\nlet \u03c1 be a metric on X . Proper choice of \u03c1 is essential to good performance\n\nin complex applications, but using Euclidean distance should suffice for simple\n114\n\n\fCHAPTER 7. EXTENSIONS\n\n115\n\nproblems. To any subset xi1 , . . . , xik of the full list associate the Voronoi partition of X . That is, say that a point x is in the xij cell if \u03c1(x, xij ) \u2264 \u03c1(x, xi\u2032j )\n\nfor j \u2032 \u2208 {1, . . . , k}. For definiteness, in the case of ties say that x is in the cell\n\noccurring first in the original ordering of the x's. Consequently, every point\nx \u2208 X is in exactly one cell. Put a prior on these partitions of the space X\n\nby putting a prior on the (finite) set of all possible (nonempty) subsets of the\nlist x1 , . . . , xn . Say that the prior probability of a subset only depends on the\nsize of the subset and that the probability of a given size k, is proportional\nto the probability that a Geometric(1 \u2212 \u03b1) random variable takes the value\nk. Finally, having chosen a subset at random, and consequently having fixed\n\na partition of X , assign to each element of the partition a success probability\nsi drawn uniformly at random from [0, 1]. The generates a multi-dimensional\nregression function f : X 7\u2192 [0, 1] at random.\n\nTo explore these ideas I simulated a two dimensional data set with 250\n\ndata points, illustrated in Figure 7.0.a. The x-values were chosen randomly\n(albeit not uniformly) from the illustrated rectangle; the y-values were drawn as\nindependent Bernoulli random variables whose success probability is indicated\nby the gray-scale in the figure. Points in the whiter regions have a higher\nchance of being an \"x,\" (i.e. y = 1) while points in the darker regions have\na higher chance of being an \"o\" (i.e. y = 0). Jointly, the x and y data was\nactually generated in my simulation in the reverse manner: a fair coin was\nflipped to determine if y will be 1 or 0 and then (conditionally) an x-value was\nchosen. Suppose y came up as 1, then with probability 1/2, x will be drawn\nuniformly from the square on the right; otherwise, with probability 1/2 it will\nbe drawn from a bivariate normal distribution with standard deviation 0.1 that\nis centered on the left-hand square. If y came up a 0, the situation for x would\nbe reversed. These two descriptions are essentially equivalent and the goal for\nthe posterior mean estimate is always the same: to estimate the conditional\nprobability of y to be 1 given x; i.e. to estimate the gray-scale image.\nSampling from the posterior of this prior is (theoretically at least) quite\n\n\f116\n\nCHAPTER 7. EXTENSIONS\n\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n\u22121\n\n\u22120.8\n\n\u22120.6\n\n\u22120.4\n\n\u22120.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 7.0.a: A Two Dimensional Data Set and Target Function: The grayscale on this figure depicts a certain regression function f0\u2217 on a rectangle. 250\ndata points are drawn by flipping an f0\u2217 (x)-coin at the points indicated. Heads\n(red) tend to result when f0\u2217 is large (white). Tails (blue) tend to result when\nf0\u2217 is small (black)\n\n\f117\n\nCHAPTER 7. EXTENSIONS\n\nsimple. As before in chapter 3, the success probabilities can be integrated out\nanalytically so that the posterior probability of a particular (nonempty) subset\nof size k is proportional to:\n\n\u03b1\n\nk\u22121\n\nk\nY\n\nNj1 ! Nj0 !\n(Nj1 + Nj0 + 1)!\nj=1\n\n(7.1)\n\nWhere Nj1 and Nj0 denote the number of y-values equal to 1 or 0, respectively, on partition element j. Conditionally, the posterior distribution of a\ncertain success probability Sj is Beta(Nj1 + 1, Nj0 + 1). Since the number of\npossible subsets is finite and all of them (except the empty subset) have positive\nposterior probability, a standard Metropolis-Hastings type MCMC allows us\nto sample from the posterior (at least in theory). In practice, though, the rate\nof mixing matters; in an effort to improve this I have conducted preliminary\nwork that employs the simulated tempering technique developed by Geyer and\nThompson [39].\nAll that remains to be specified is a transitive random-walk on (nonempty)\nsubsets of a set of n elements which has a known stationary distribution. This\nis easily done. Identify the class of all subsets of a set of size n with the class\nof binary vector of length n, with each coordinate indicating the presence or\nabsence of a given element. Exclude the 0-vector from this set. Consider the\nrandom walk that picks a number J randomly from 1 to n and then proposes\nflipping the J'th bit. The proposal is not allowed if it would create the 0-vector;\nhold in this case. This Markov chain is easily seen to sample uniformly from\nthe class of all non-empty subsets and consequently it is easy to modify it with\nthe Metropolis-Hastings ratio in order to sample from the posterior.\nI have also proposed extensions of this technique to put a prior on smooth\nfunctions; under this proposal the prior concentrates on functions which softly\npartition the space using weighted Voronoi regions, but provide for smooth\n\n\f118\n\nCHAPTER 7. EXTENSIONS\n\n1\n\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n\u22121\n\n\u22120.5\n\n0\n\n0.5\n\n1\n\n0\n\u22121\n\n\u22120.5\n\nSample 1\n1\n\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n\u22121\n\n\u22120.5\n\n0\n\n0.5\n\n1\n\n0\n\u22121\n\n\u22120.5\n\nSample 2\n1\n\n1\n0.8\n\n0.6\n\n0.6\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n\u22120.5\n\n0\n\nSample 3\n\n0.5\n\n1\n\n0\n\n0.5\n\n1\n\n0.5\n\n1\n\nSample 5\n\n0.8\n\n0\n\u22121\n\n0\n\nSample 4\n\n0.5\n\n1\n\n0\n\u22121\n\n\u22120.5\n\n0\n\nSample 6\n\nFigure 7.0.b: Modal Samples: Voronoi Posterior The pictured samples were the\nones most frequently occurring in a sample drawn from the Voronoi posterior\n\u03c0 \u2217 . Each sample is equivalent to a certain subset of the original covariate list.\nThe included points determine the partition and are drawn with a green circle.\nThe gray-scale on a given partition element represents the posterior mean of\nthe corresponding success probability parameter. Notice how parsimoniously\nthe circled points determine regions that isolate out the two clumps of data.\n\n\f119\n\nCHAPTER 7. EXTENSIONS\n\ntemperedct=13922, alpha=0.5, N=250\n1\n0.8\n0.6\n0.4\n0.2\n0\n\u22121\n\n0\n\n\u22120.5\n\n0.1\n\n0.2\n\n0\n\n0.3\n\n0.4\n\n0.5\n\n0.5\n\n0.6\n\n0.7\n\n1\n\n0.8\n\n0.9\n\n1\n\nFigure 7.0.c: A Bivariate Example. 250 red and blue markers were put down\nrandomly as shown. The posterior-mean estimate of f0 under \u03c0 \u2217 , i.e. the\nestimated conditional probability of red at each position, is shown in gray;\nnotice how the modal samples from Figure 7.0.b are incorporated into the\nposterior.\n\n\f120\n\nCHAPTER 7. EXTENSIONS\n\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n\u22121\n\n\u22120.8\n\n0.1\n\n\u22120.6\n\n0.2\n\n\u22120.4\n\n0.3\n\n\u22120.2\n\n0.4\n\n0\n\n0.5\n\n0.2\n\n0.4\n\n0.6\n\n0.6\n\n0.7\n\n0.8\n\n0.8\n\n1\n\n0.9\n\nFigure 7.0.d: Weighted Voronoi Posterior: Redefining the Voronoi cells to use\nrandom weights allows for elliptical arcs and lines to be used in the partition\nand eliminates some artifacts.\n\n\fCHAPTER 7. EXTENSIONS\n\n121\n\ntransitions between regions.\nA simple extension of this technique that removes many of the artifacts\nthat are otherwise present because of the dependence on the particular locations of the covariates can be made by using a randomly weighted Voronoi\npartitions [54]. To make such a prior, simply augment each cell with a random variable Wj which is an independent \u0393(\u03b3, \u03b3 \u22121 ) a priori. Then, redefine the Voronoi partition so that cells with higher weight tend to be bigger.\nSpecifically, consider the point x to be in cell j rather than cell k whenever:\n\u03c1(x, xj )/wj \u2264 \u03c1(x, xk )/wk . Intuitively, if an ordinary Voronoi partition can\nbe understood by supposing that \"crystals\" grow out radially from the gener-\n\nating seeds until they hit other growing crystals, then this weighted Voronoi\npartition allows for the crystals to grow at different rates. It is also simple\nto allow the crystals to start growing at different times (by simply adding a\ndifferent random offset to \u03c1 for each cell). The overall scale of the prior on W\nis irrelevant so this parameterization sets the mean to 1. For the experiment\nshown in Figure 7.0.d I used \u03b3 = 5. As can be seen, this simple modification\nallows the posterior to choose neat balls and lines with which to isolate out\nthe different contours of the data. The posterior is computed using standard\nMCMC techniques.\nAnother route to extend \u03c0 is to utilize the observed connection between \u03c0\nand \u03c0DF (for details, please refer to subsection 2.2.1). To make that connection,\nI employed binary random variables \u03b7i (u) that indicated if a test point u was\nor was not above a certain random threshold Vi . To generalize, then, we can\ntake \u03b7i (x) to indicate if x is in a certain random half-space Hi . This would be\nsimilar to a version of CART in which we split first into two halves via H1 ,\nand then split each half via H2 , and so on. We could make something close to\nordinary CART if we utilized random coordinate aligned half-spaces and if we\nemployed a suitably \"regularized\" \u03c0DF which did not assign an independent\nuniform to all possible 2k binary k-tuples at level k. More generally, one can\ninvent other ways of \"regularizing\" \u03c0DF so that numerous binary tuples are\n\n\f122\n\nCHAPTER 7. EXTENSIONS\n\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n\u22121\n\n\u22120.8\n\n\u22120.6\n\n\u22120.4\n\n\u22120.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\nFigure 7.0.e: Bagged CART in 2d: Running bagged CART on this two dimensional data yields a reasonably good estimate with an interesting horizontal\nand vertical blurring pattern\ntied together.\nFinally, for comparison, Figure 7.0.e the result of bagging ordinary twodimensional CART on this same data is shown. It has a clear advantage on\nthe vertical split down the center that happens to be coordinate aligned. It\ndoes a decent job of isolating the two clumps of data. Interestingly, there\nis a clear horizontal and vertical blurring pattern that arises from the use of\npartitions that only partially isolated the clumped data: it is isolated in one\ncoordinate, but not in the other.\n\n\fChapter 8\nAfterword\nAs statisticians, we analyze data, formulate models, estimate parameters, and\nuse these models to form predictions about future data. Broadly speaking,\nthen, our business is inference. We go to a lot of trouble to formulate good\nmodels and we have spent a great deal of effort debating about the details\nof how to make inference within a model \u2013 e.g. Bayesian versus frequentist\ninference.\nGenerally, though, the way we select our model remains an art. \"Nonparametric\" methods are a step forward here, because, generally speaking, they\nat least prescribe how to select a variety of \"smoothing-parameters\" which\nessentially determine which model among some class of models we actually\napply. The main topic of this thesis is of this sort: A Bayesian approach\nto the question of how to estimate the number and location of change-points\nor, more generally, how to choose a partition of the data into approximately\nexchangeable subsets.\nIn this afterword, I would like to step back from the details of this subject\nand address the more general problem of how we formulate a model. Sometimes, we prefer to shunt responsibility for this and appeal to the scientist\nfor help; but, fundamentally, all inference comes back to data eventually \u2013\nhow else did the scientists discover their model? Considering, then, that the\n123\n\n\fCHAPTER 8. AFTERWORD\n\n124\n\nformulation of a model is essentially \"an art,\" and that we know that the results of our analyses are not absolute, but relative to the modeling choices we\nhave made, how can we be so bold as to expect that reality will conform to\nour model-specific \"confidence intervals?\" This is not to say that statistical\npractice does not work, or that the formal properties of our analyses under\nour stated assumptions are invalid, but simply to remind the reader that the\nthread that connects the prescribed model with reality has no formal basis.\nIn practice, a good rule of thumb is to consider several models and, consciously or not, select one which is simple and which we expect will fit the data\nat least nearly as well as other more complicated models that we might prefer\nnot to have to consider. In practice, we look at the data ourselves before selecting a model and build in any particular types of regularity that we happen\nto notice that it possesses into our model \u2013 at least if we think it will affect\nour conclusions.\nBayesian analysts do not escape these problems. They may subjectively\nallow for a mixture of several different models and for a range of \"hyperparameters\" but this only ameliorates the core problem because no-one ever\naccounts for all the possible regularities that might be found.\nTo further make my point, consider the following, admittedly fanciful,\nthought experiment. An alien race comes to earth and challenges mankind to\nan intelligence test. We are given a binary time-series to analyze one-hundred\nbits at a time. It begins innocuously enough:\n01000001000000100000110111101110000000100101000011\n10101010000000001110010010000000001011001101101010\nWhat statistical models shall we consider? We could try iid Bernoulli, or\nperhaps a hidden Markov model. If ambitious, we might let the data choose the\norder of our model. Upon doing so, we find that the null model fits best, despite\nthe superficial appearance of runs, and we model the data as random coins with\nsuccess probability 0.35. The aliens ask us to give a confidence interval on the\n\n\fCHAPTER 8. AFTERWORD\n\n125\n\nfraction of 1's in the next 100 bits and to estimate the probability that the\n50'th bit of this new data will be 1. Most statisticians fall back on the classics\nand use something approximately like 0.35 \u00b1 0.1 for both answers. Others who\n\nused a richer model suggest wider intervals and the ordinary debate ensues.\n\nSo far so good, right? Or should we worry that our glance at the data and\ndefault choice of classical model may miss structure that we failed to notice?\nNaah! The aliens couldn't be that tricky.... Then the next 100 bits arrive.\n00000000000000000000000000000000000000000000000000\n00000000000000000000000000000000000000000000000000\nA dramatic failure, but no problem, the advocates of the HMM model were\nready for this sort of thing. We have, they argue, simply encountered a hidden\nstate which always produces 0. This explains the data well enough, but some\nremain skeptical. They propose that the character of the data may change\nwith every new segment of 100 bits so that the effective size of our data set is\nonly 2. The next 100 bits arrive.\n00011100011100011100011100011100011100011100011100\n01110001110001110001110001110001110001110001110001\nAnd again we are surprised. Some HMM advocates insist we just need to\nadd a number of new hidden states. Others extend the HMM model to favor\nthis sort of cycle-like behavior. The next 100 bits arrive.\n00100100001111110110101010001000100001011010001100\n00100011010011000100110001100110001010001011100000\nAnd most are satisfied that the data has returned to iid , but with the\nworld attention that this situation has generated someone notices that these\nare, in fact, the first 100 bits in the expansion for the fractional part of \u03c0. (C.f.\nhttp://www.algonet.se/~eliasb/pi/binpi.html) Oh dear \u2013 we certainly\n\n\fCHAPTER 8. AFTERWORD\n\n126\n\nhadn't planned on this \u2013 but we ignore this regularity in the data at our peril.\nIt couldn't happen by chance, could it?\nAs the test continues, we continue to be surprised by the patterns we are\nsent. By now, we have learned a lot: we know less than we think about\nthe future. Every 100 bits we have been presented with a pattern that we\nhadn't expected. Finally, we are given sequence after sequence that we can't\nexplain. Eventually the aliens conclude that, however feebly, we are, at least,\na modestly intelligent form of life; and, taking pity on us, they decide to\nreveal the patterns we hadn't discovered. The last sequences, for example,\nwere actually Shakespeare, encoded by simple alien cipher that no human had\never considered. Furthermore, the first sequence wasn't actually \"random:\"\nto generate it, all we had to do was start matlab or an equivalent (alien)\ncomputational program and type:\nx=rand(1,100)<=0.397;\nsprintf('%c',x+'0')\nPerhaps you object to my example. You prefer regression to time-series\nanalysis and are content to consider data for which no-one would object to\nthe model that the responses are independent given the predictors. Perhaps\nthe problem of extrapolating a non-stationary time-series seems far too lofty\nto you. But you haven't escaped it by wishing it away; in fact, the time-series\nproblem can be embedded in the regression problem. We need only suppose\nthat the covariates are tested one by one in some fixed designed fashion so\nthat we see the responses sequentially. If we know that the regression function\n\"smoothly\" depends on the covariates, present methods can be expected to\nwork; but, if the dependence is sufficiently complicated, each new data point\ntells us something entirely new, just like in my fictitious time-series.\nEven if the data is generated iid \u2013 the regression case considered in most\nof my theoretical work \u2013 there is plenty of room for improvement. In this\nsituation, we are, indeed, much better off \u2013 we can make rigorous probability\n\n\fCHAPTER 8. AFTERWORD\n\n127\n\nstatements about the quality of our predictions on average. Even so, as more\nand more data come in, and the general shape of the regression function becomes more tightly resolved, the \"knowledge we gain\" itself comes to us in a\ntime-series fashion. Because of this, we cannot easily make predictions about\nthe regression function at some fixed point.\nFor example, suppose the unit interval were divided into subintervals of size\n1 1 1\n, , ,\n2 4 8\n\netc... and suppose that the regression function takes a different value on\n\neach piece. In this way, we would quickly have enough data to estimate largescale features, like the value of the regression function on the larger pieces, but\nif we are asked to make a prediction on one of the very small pieces, what are we\nto do? Perhaps, if we were smart enough, we wouldn't blithely approximate the\nregression function as \"smooth,\" we would look for patterns in the regression\nvalues on the large pieces to help extrapolate to the smaller pieces. If, in fact,\nthe regression values were seen to alternate between high and low, we would\nbe silly to treat the function as \"smooth.\" Instead, let us hope that were are\nlucky enough that it is quite \"regular.\"\nWhat, then are we to do? Consider again the alien's test, and the complex\nsequence of modeling decisions that we needed to make along the way. How\ncan we summarize the thought process that we went through as surprising data\ncontinued to come in? What possible prior on models could our analysis (even\napproximately) conform to? Our only recourse, it seems, is to formalize the\nidea of regularity and make explicit the manner in which we choose a model\nto accord with the regularities apparent in the data.\nA reasonable defining property of regularity is that a distribution is regular if it can be (approximately) reproduced within a certain budget of time\nby applying some modestly short computer program to the data that we have\npreviously seen and a \"random sequence.\" Roughly speaking, then, we can\nput a prior on models and/or regularities by putting a suitable prior on computer programs. Alternatively, we can select among the computer programs\nin a manner conforming more closely to the \"method of maximum likelihood.\"\n\n\fCHAPTER 8. AFTERWORD\n\n128\n\nFormal versions of these ideas have been proposed [27],[63],[5],[47],[46],[13],[60]\nthough much work is needed to formulate methods that are ready for actual\nuse. Still, it seems to this author that the further development of some version\nof these ideas is an essential, natural, and unavoidable step in the progression\nof statistical thinking.\n\n\fBibliography\n[1] Athreya, K. B., Doss, H. and Sethuraman, J. (1996). On the convergence of the Markov chain simulation method. The Annals of Statistics\n24 69\u2013100.\n[2] Barron, A., Schervish, M. J. and Wasserman, L. (1999). The\nconsistency of posterior distributions in nonparametric problems. The\nAnnals of Statistics 27 536\u2013561.\n[3] Breiman, L. (1996). Bagging predictors. Machine Learning 24 123\u2013140.\nURL citeseer.nj.nec.com/breiman94bagging.html\n[4] Brieman, L., Friedman, J., Olshen, R. and Stone, C. (1984). Classification and Regression Trees. Wadsworth.\n[5] Chaitin, G. J. (1966). On the length of programs for computing binary\nsequences. J. Assoc. Comp. Mach. 13 547\u2013569.\n[6] Chipman, H., George, E. I. and McCulloch, R. (1998a). Bayesian\nCART model search (with discussion). J. Amer. Statist. Assoc. 93 935\u2013\n960.\n[7] Chipman, H., George, E. I. and McCulloch, R. (1998b). Making\nsense of a forest of trees. Tech. rep., Department of Statistics and Actuarial\nScience, University of Waterloo.\n\n129\n\n\fBIBLIOGRAPHY\n\n130\n\n[8] Chipman, H., George, E. I. and McCulloch, R. (2000a). Bayesian\ntreed models. Tech. rep., Department of Statistics, University of Waterloo.\n[9] Chipman, H., George, E. I. and McCulloch, R. (2000b). Hierarchical priors for Bayesian CART shrinkage. Statist. Comp. 10 17\u201324.\n[10] Cleveland, W. S. and Loader, C. (1996). Smoothing by local regression: Principles and methods (disc: P80-127). In Statistical Theory and\nComputational Aspects of Smoothing. Proceedings of the COMPSTAT '94\nSatellite Meeting.\n[11] Coram, M. (2001). Projection pursuit for classification: A different\nmotivation for svms. Available upon request.\n[12] Cover, T. and Hart, P. (1967). Nearest neighbor pattern classification.\nProc. IEEE Trans. Inform. Theory IT-11 21\u201327.\n[13] Cover, T. M. and Thomas, J. A. (1991). Elements of Information\nTheory. Wiley.\n[14] Daubechies, I., Guskov, I., Schroder, P. and Sweldens, W.\n(1999). Wavelets on irregular point sets. Phil. Trans. Royal Soc. Lond. A\n257 2397\u20132413.\n[15] Denison, D., Adams, N., Holmes, C. and Hand, D. (2002). Bayesian\npartition modelling. Comp. Statist. Data Anal. 38 475\u2013485.\n[16] Denison, D., Holmes, C., Mallick, B. and Smith, A. (2002).\nBayesian Methods for Nonlinear Classification and Regression. Wiley.\n[17] Denison, D., Mallick, B. and Smith, A. (1998a). Automatic Bayesian\ncurve fitting. J. Roy. Statist. Soc. B 60 333\u2013350.\n[18] Denison, D., Mallick, B. and Smith, A. (1998b). A Bayesian CART\nalgorithm. Biometrika 85 363\u2013377.\n\n\fBIBLIOGRAPHY\n\n131\n\n[19] Denison, D., Mallick, B. and Smith, A. (1998c). Bayesian MARS.\nStatist. Comp. 8 337\u2013346.\n[20] Denison, D. G. T. (1997). Simulation based Bayesian nonparametric\nregression methods. Ph.D. thesis, Department of Mathematics, Imperial\nCollege, London.\n[21] Devroye, L., Gyorfi, L. and Lugosi, G. (1996). A Probabilistic\nTheory of Pattern Recognition. Springer-Verlag.\n[22] Devroye, L. and Lugosi, G. (2001). Combinatorial Methods in Density\nEstimation. Springer.\n[23] Diaconis, P. and Freedman, D. (1986). On the consistency of Bayes\nestimates (c/r: P26-67). The Annals of Statistics 14 1\u201326.\n[24] Diaconis, P. and Freedman, D. A. (1993). Nonparametric binary\nregression: A Bayesian approach. The Annals of Statistics 21 2108\u20132137.\n[25] Diaconis, P. and Freedman, D. A. (1995). Nonparametric binary\nregression with random covariates. Probability and Mathematical Statistics\n15 243\u2013273.\n[26] Donoho, D. and Johnstone, I. (1994). Ideal spatial adaptation by\nwavelet shrinkage. Biometrika 81 425\u2013455.\n[27] Donoho, D. L. (2002). The Kolmogorov sampler. Tech. rep., Stanford\nUniversity.\n[28] Doob, J. (1953). Stochastic Processes. Wiley.\n[29] Doob, J. L. (1949). Application of the theory of martingales. Colloque\nInternational Ctrenat. Rech. Sci, Paris.\n[30] Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2002).\nLeast angle regression. Tech. rep., Stanford University.\n\n\fBIBLIOGRAPHY\n\n132\n\n[31] Freedman, D. and Diaconis, P. (1984). On inconsistent bayes estimates in the discrete case. The Annals of Statistics 11 1109\u20131118.\n[32] Freedman, D. A. (1963). On the asymptotic behavior of bayes' estimates in the discrete case. Annals of Mathematical Statistics 34 1386\u2013\n1403.\n[33] Freedman, D. A. (1999). On the bernstein-von mises theorem with\ninfinite-dimensional parameters. Annals of Statistics 27 1119\u20131140.\n[34] Freund, Y. and Schapire, R. (1996). Machine Learning: Proceedings\nof the Thirteenth International Conference, chap. Experiments with a new\nboosting algorithm. Morgan Kauffman.\n[35] Friedman, J. (1991). Multivariate adaptive regression splines. The Annals of Statistics 19.\n[36] Friedman, J. (1996). On bias, variance, 0/1-loss, and the curse-ofdimensionality. Tech. rep., Stanford University.\n[37] Geyer, C. (1999). Stochastic Geometry: Likelihood and Computation,\nchap. Likelihood Inference for Spatial Point Processes. Wiley, 79\u2013140.\n[38] Geyer, C. and Moller, J. (1994). Simulation procedures and likelihood\ninference for spatial point processes. Scandinavian Journal of Statistics\n21 359\u2013373.\n[39] Geyer, C. and Thompson, E. (1993). Annealing marko chain monte\ncarlo with applications to ancestral inference. Tech. rep., University of\nMinnesota, School of Statistics.\n[40] Ghosal, S., Ghosh, J. and van der Vaart, A. (2000). Convergence\nrates of posterior distributions. The Annals of Statistics 28 500\u2013531.\n\n\fBIBLIOGRAPHY\n\n133\n\n[41] Green, P. J. (1995). Reversible jump markov chain monte carlo computation and bayesian model determination. Biometrika 82 711\u2013732.\n[42] Hasser, N. B. and Sullivan, J. A. (1971). Real Analysis. Dover.\n[43] Hastie, T., Tibshirani, R. and Friedman, J. (2001). The Elements\nof Statistical Learning. Springer-Verlag.\n[44] Hastings, W. (1970). Monte carlo sampling methods using markov\nchains and their applications. Biometrika 57 97 \u2013 109.\n[45] Kendall, W. and M\u00f8ller, J. (2000). Perfect simulation using dominating processes on ordered spaces, with application to stable point processes.\nAdv. Appl. Prob. 32 844\u2013865.\n[46] Kolmogorov, A. N. (1965). Three approaches to the quantitative definition of information. Problems of Information Transmission 1 4\u20137.\n[47] Kolmogorov, A. N. (1968). Logical basis for information theory and\nprobability theory. IEEE Trans. Inform. Theory IT-14 662\u2013664.\n[48] Lavine, M. (1992). Some aspects of polya tree distributions for statistical\nmodelling. The Annals of Statistics 20 1222\u20131235.\n[49] LeCam, L. M. and Yang, G. L. (1990). Asymptotics in Statistics: Some\nBasic Concepts. Springer.\n[50] Liu, J. (2001). Monte Carlo Strategies in Scientific Computing. SpringerVerlag.\n[51] Metropolis, n., Rosenbluth, A., Rosenbluth, M., Teller, M.\nand Teller, E. (1953). Equations of state calculations by fast computing\nmachines. J. Chem. Phys. 21 1087\u20131092.\n\n\f134\n\nBIBLIOGRAPHY\n\n[52] M\u00f8ller, J. (1999). Stochastic Geometry: Likelihood and Computation,\nchap. Markov Chain Monte Carlo and Spation Point Processes. Wiley,\n141\u2013172.\n[53] M\u00f8ller, J. and Skare, \u00d8. (2001).\n\nBayesian image analysis with\n\ncoloured voronoi tessellations and a view to applications in reservoir modelling.\nURL citeseer.nj.nec.com/405679.html\n[54] Okabe, A., Boots, B. and Sugihara, K. (1992). Spatial Tesselations:\nConcepts and Applications of Voronoi Diagrams. John Wiley & Sons.\n[55] Paddock, S. (1999). Randomized Polya Trees: Bayesian Nonparametrics\nfor Multivariate Data Analysis. Ph.D. thesis, Duke University.\n[56] Ripley, B. (1996). Pattern Recognition and Neural Networks. Cambridge\nUniversity Press.\n[57] Rubin, D. B. (1981). The bayesian bootstrap. The Annals of Statistics\n9 130\u2013134.\n[58] Scargle, J. (1998). Studies in astronomical time series analysis. v.\nbayesian blocks, a new method to analyze structure in photon counting\ndata. The Annals of Statistics 504 405.\n[59] Scargle, J. et al. (2002). Private communication.\n[60] Schmidhuber, J. (2000). Algorithmic theories of everything. Tech. rep.,\nIDSIA-20-00.\n[61] Schwartz, L. (1965). On Bayes procedures. Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und Verwandte Gebiete 4 10\u201326.\n[62] Shen, X. and Wasserman, L. (2001). Rates of convergence of posterior\ndistributions. The Annals of Statistics 29.\n\n\fBIBLIOGRAPHY\n\n135\n\n[63] Solomonoff, R. J. (1964). A formal theory of inductive inference.\nInform. Contr. 7 1\u201322, 224\u2013254.\n[64] Tierney, L. (1994). Markov chains for exploring posterior distributions.\nThe Annals of Statistics 22 1701\u20131762.\n[65] Vapnik, V. (1996). The Nature of Statistical Learning Theory. SpringerVerlag.\n[66] Wahba, G. (1990). Spline Models for Observational Data. SIAM.\n[67] Wahba, G., Li, Y. and Zhang, H. (2000). Advances in Large Margin\nClassifiers, chap. GACV for Support Vector Machines. MIT Press.\n[68] Wong, W. H. and Shen, X. (1995). Probability inequalities for likelihood ratios and convergence rates of sieve Mles. The Annals of Statistics\n23 339\u2013362.\n\n\f"}