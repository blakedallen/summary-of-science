{"id": "http://arxiv.org/abs/cs/0605024v1", "guidislink": true, "updated": "2006-05-06T16:56:43Z", "updated_parsed": [2006, 5, 6, 16, 56, 43, 5, 126, 0], "published": "2006-05-06T16:56:43Z", "published_parsed": [2006, 5, 6, 16, 56, 43, 5, 126, 0], "title": "A Formal Measure of Machine Intelligence", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0605077%2Ccs%2F0605098%2Ccs%2F0605133%2Ccs%2F0605144%2Ccs%2F0605073%2Ccs%2F0605116%2Ccs%2F0605129%2Ccs%2F0605140%2Ccs%2F0605135%2Ccs%2F0605056%2Ccs%2F0605143%2Ccs%2F0605035%2Ccs%2F0605136%2Ccs%2F0605075%2Ccs%2F0605037%2Ccs%2F0605105%2Ccs%2F0605121%2Ccs%2F0605087%2Ccs%2F0605079%2Ccs%2F0605120%2Ccs%2F0605005%2Ccs%2F0605132%2Ccs%2F0605104%2Ccs%2F0605038%2Ccs%2F0605011%2Ccs%2F0605068%2Ccs%2F0605130%2Ccs%2F0605106%2Ccs%2F0605060%2Ccs%2F0605062%2Ccs%2F0605114%2Ccs%2F0605127%2Ccs%2F0605123%2Ccs%2F0605008%2Ccs%2F0605128%2Ccs%2F0605111%2Ccs%2F0605036%2Ccs%2F0605103%2Ccs%2F0605013%2Ccs%2F0605085%2Ccs%2F0605045%2Ccs%2F0605099%2Ccs%2F0605134%2Ccs%2F0605032%2Ccs%2F0605029%2Ccs%2F0605074%2Ccs%2F0605026%2Ccs%2F0605054%2Ccs%2F0605113%2Ccs%2F0605024%2Ccs%2F0605027%2Ccs%2F0605047%2Ccs%2F0605019%2Ccs%2F0605086%2Ccs%2F0605025%2Ccs%2F0605040%2Ccs%2F0605007%2Ccs%2F0605028%2Ccs%2F0605030%2Ccs%2F0605082%2Ccs%2F0605053%2Ccs%2F0605090%2Ccs%2F0605004%2Ccs%2F0605094%2Ccs%2F0605095%2Ccs%2F0605084%2Ccs%2F0605012%2Ccs%2F0605100%2Ccs%2F0605081%2Ccs%2F0605022%2Ccs%2F0605052%2Ccs%2F0605069%2Ccs%2F0605110%2Ccs%2F0605057%2Ccs%2F0605088%2Ccs%2F0605109%2Ccs%2F0605145%2Ccs%2F0605102%2Ccs%2F0605112%2Ccs%2F0605009%2Ccs%2F0605023%2Ccs%2F0605078%2Ccs%2F0605059%2Ccs%2F0605101%2Ccs%2F0605126%2Ccs%2F0605016%2Ccs%2F0605010%2Ccs%2F0605071%2Ccs%2F0605117%2Ccs%2F0605137%2Ccs%2F0605051%2Ccs%2F0605108%2Ccs%2F0605091%2Ccs%2F0605018%2Ccs%2F0605142%2Ccs%2F0605119%2Ccs%2F0605006%2Ccs%2F0605003%2Ccs%2F0605141%2Ccs%2F0605034%2Ccs%2F0605080&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Formal Measure of Machine Intelligence"}, "summary": "A fundamental problem in artificial intelligence is that nobody really knows\nwhat intelligence is. The problem is especially acute when we need to consider\nartificial systems which are significantly different to humans. In this paper\nwe approach this problem in the following way: We take a number of well known\ninformal definitions of human intelligence that have been given by experts, and\nextract their essential features. These are then mathematically formalised to\nproduce a general measure of intelligence for arbitrary machines. We believe\nthat this measure formally captures the concept of machine intelligence in the\nbroadest reasonable sense.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0605077%2Ccs%2F0605098%2Ccs%2F0605133%2Ccs%2F0605144%2Ccs%2F0605073%2Ccs%2F0605116%2Ccs%2F0605129%2Ccs%2F0605140%2Ccs%2F0605135%2Ccs%2F0605056%2Ccs%2F0605143%2Ccs%2F0605035%2Ccs%2F0605136%2Ccs%2F0605075%2Ccs%2F0605037%2Ccs%2F0605105%2Ccs%2F0605121%2Ccs%2F0605087%2Ccs%2F0605079%2Ccs%2F0605120%2Ccs%2F0605005%2Ccs%2F0605132%2Ccs%2F0605104%2Ccs%2F0605038%2Ccs%2F0605011%2Ccs%2F0605068%2Ccs%2F0605130%2Ccs%2F0605106%2Ccs%2F0605060%2Ccs%2F0605062%2Ccs%2F0605114%2Ccs%2F0605127%2Ccs%2F0605123%2Ccs%2F0605008%2Ccs%2F0605128%2Ccs%2F0605111%2Ccs%2F0605036%2Ccs%2F0605103%2Ccs%2F0605013%2Ccs%2F0605085%2Ccs%2F0605045%2Ccs%2F0605099%2Ccs%2F0605134%2Ccs%2F0605032%2Ccs%2F0605029%2Ccs%2F0605074%2Ccs%2F0605026%2Ccs%2F0605054%2Ccs%2F0605113%2Ccs%2F0605024%2Ccs%2F0605027%2Ccs%2F0605047%2Ccs%2F0605019%2Ccs%2F0605086%2Ccs%2F0605025%2Ccs%2F0605040%2Ccs%2F0605007%2Ccs%2F0605028%2Ccs%2F0605030%2Ccs%2F0605082%2Ccs%2F0605053%2Ccs%2F0605090%2Ccs%2F0605004%2Ccs%2F0605094%2Ccs%2F0605095%2Ccs%2F0605084%2Ccs%2F0605012%2Ccs%2F0605100%2Ccs%2F0605081%2Ccs%2F0605022%2Ccs%2F0605052%2Ccs%2F0605069%2Ccs%2F0605110%2Ccs%2F0605057%2Ccs%2F0605088%2Ccs%2F0605109%2Ccs%2F0605145%2Ccs%2F0605102%2Ccs%2F0605112%2Ccs%2F0605009%2Ccs%2F0605023%2Ccs%2F0605078%2Ccs%2F0605059%2Ccs%2F0605101%2Ccs%2F0605126%2Ccs%2F0605016%2Ccs%2F0605010%2Ccs%2F0605071%2Ccs%2F0605117%2Ccs%2F0605137%2Ccs%2F0605051%2Ccs%2F0605108%2Ccs%2F0605091%2Ccs%2F0605018%2Ccs%2F0605142%2Ccs%2F0605119%2Ccs%2F0605006%2Ccs%2F0605003%2Ccs%2F0605141%2Ccs%2F0605034%2Ccs%2F0605080&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A fundamental problem in artificial intelligence is that nobody really knows\nwhat intelligence is. The problem is especially acute when we need to consider\nartificial systems which are significantly different to humans. In this paper\nwe approach this problem in the following way: We take a number of well known\ninformal definitions of human intelligence that have been given by experts, and\nextract their essential features. These are then mathematically formalised to\nproduce a general measure of intelligence for arbitrary machines. We believe\nthat this measure formally captures the concept of machine intelligence in the\nbroadest reasonable sense."}, "authors": ["Shane Legg", "Marcus Hutter"], "author_detail": {"name": "Marcus Hutter"}, "author": "Marcus Hutter", "arxiv_comment": "8 two-column pages", "links": [{"href": "http://arxiv.org/abs/cs/0605024v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0605024v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0605024v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0605024v1", "journal_reference": "Proc. 15th Annual Machine Learning Conference of {B}elgium and The\n  Netherlands (Benelearn 2006) pages 73-80", "doi": null, "fulltext": "Technical Report\n\nIDSIA-10-06\n\nA Formal Measure of Machine Intelligence\nShane Legg and Marcus Hutter\n\narXiv:cs/0605024v1 [cs.AI] 6 May 2006\n\nIDSIA, Galleria 2, CH-6928 Manno-Lugano, Switzerland\n{shane,marcus}@idsia.ch\nhttp://www.idsia.ch/\n\n14 April 2006\n\nAbstract\n\nof entities which are profoundly different to each\nother in terms of their cognitive capacities, speed,\nsenses, environments in which they operate, and so\non. To measure the intelligence of such diverse systems in a meaningful way we must step back from\nthe specifics of particular systems and establish the\nunderlying fundamentals of what it is that we are\nreally trying to measure. That is, we need to establish a notion of intelligence that goes beyond the\nspecifics of particular kinds of systems.\n\nA fundamental problem in artificial intelligence is\nthat nobody really knows what intelligence is. The\nproblem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this\nproblem in the following way: We take a number\nof well known informal definitions of human intelligence that have been given by experts, and extract\ntheir essential features. These are then mathematically formalised to produce a general measure of\nintelligence for arbitrary machines. We believe that\nthis measure formally captures the concept of machine intelligence in the broadest reasonable sense.\n\nThe difficulty of doing this is readily apparent.\nConsider, for example, the memory and numerical\ncomputation tasks that appear in some intelligence\ntests and which were once regarded as defining hallmarks of human intelligence. We now know that\nthese tasks are absolutely trivial for a machine and\nthus do not test the machine's intelligence. Indeed\n1 Introduction\neven the mentally demanding task of playing chess\nhas been largely reduced to brute force search. As\nMost of us think that we recognise intelligence when\ntechnology advances, our concept of what intelliwe see it, but we are not really sure how to pregence is continues to evolve with it.\ncisely define or measure it. We informally judge\nHow then are we to develop a concept of intellithe intelligence of others by relying on our past exgence\nthat is applicable to all kinds of systems? Any\nperiences in dealing with people. Naturally, this\nproposed\ndefinition must encompass the essence of\nnaive approach is highly subjective and imprecise.\nhuman\nintelligence,\nas well as other possibilities, in\nA more principled approach would be to use one\na\nconsistent\nway.\nIt\nshould not be limited to any\nof the many standard intelligence tests that are\nparticular\nset\nof\nsenses,\nenvironments or goals, nor\navailable. Contrary to popular wisdom, these tests,\nshould\nit\nbe\nlimited\nto\nany specific kind of hardwhen correctly applied by a professional, deliver\nware,\nsuch\nas\nsilicon\nor\nbiological\nneurons. It should\nstatistically consistent results and have considerable\nbe\nbased\non\nprinciples\nwhich\nare\nsufficiently fundapower to predict the future performance of individmental\nso\nas\nto\nbe\nunlikely\nto\nalter\nover time. Furuals in many mentally demanding tasks. However,\nthermore,\nthe\nintelligence\nmeasure\nshould\nideally be\nwhile these tests work well for humans, if we wish\nformally\nexpressed,\nobjective,\nand\npractically\nrealto measure the intelligence of other things, perhaps\nisable.\nof a monkey or a new machine learning algorithm,\nthey are clearly inappropriate.\nThis paper approaches this problem in the folOne response to this problem might be to de- lowing way. In Section 2 we consider a range of defvelop specific kinds of tests for specific kinds of en- initions of human intelligence that have been put\ntities; just as intelligence tests for children differ forward by well known psychologists. From these\nto intelligence tests for adults. While this works we extract the most common and essential features\nwell when testing humans of different ages, it comes and use them to create an informal definition of\nundone when we need to measure the intelligence intelligence. Section 3 then introduces the frame1\n\n\fwork which we use to construct our formal measure\nof intelligence. This framework is formally defined\nin Section 4. In Section 5 we use our developed\nformalism to produce a formal definition of intelligence. Section 7 closes with a short summary.\nA preliminary sketch of the ideas in this paper appeared in the poster [LH05]. It can be shown that\nthe intelligence measure presented here is in fact a\nvariant of the Intelligence Order Relation that appears in the theory of AIXI, the provably optimal\nuniversal agent [Hut04]. A long journal version of\nthis paper is being written in which we give the proposed measure of machine intelligence and its relation to other such tests a much more comprehensive\ntreatment.\nNaturally, we expect such a bold initiative to be\nmet with resistance. However, we hope that the\nreader will appreciate the value of our approach:\nWith a formally precise definition put forward we\naim to better our understanding of what is a notoriously subjective and slippery concept.\n\n2\n\nthink abstractly, comprehend complex ideas,\nlearn quickly and learn from experience.\"\n\u2013 L. S. Gottfredson and 52 expert signatories\nThese definitions have certain common features;\nin some cases they are explicitly stated, while in\nothers they are more implicit. Perhaps the most\nelementary feature is that intelligence is seen as a\nproperty of an entity which is interacting with an\nexternal environment, problem or situation. Indeed\nthis much is common to practically all proposed definitions of intelligence. As we will be referring back\nto these concepts regularly, we will refer to the entity whose intelligence is in question as the agent,\nand the external environment, problem or situation\nthat it faces as the environment. An environment\ncould be a large complex world in which the agent\nexists, similar to the usual meaning, or something\nas narrow as a game of tic-tac-toe.\nThe second common feature of these definitions\nis that an agent's intelligence is related to its ability to succeed in an environment. This implies that\nthe agent has some kind of an objective. Perhaps\nwe could consider an agent intelligent, in an abstract sense, without having any objective. However without any objective what so ever, the agent's\nintelligence would have no observable consequences.\nIntelligence then, at least the concrete kind that interests us, comes into effect when an agent has an\nobjective to apply its intelligence to. Here we will\nrefer to this as its goal.\nThe emphasis on learning, adaption and experience in these definitions implies that the environment is not fully known to the agent and may contain surprises and new situations which could not\nhave been anticipated in advance. Thus intelligence\nis not the ability to deal with one fixed and known\nenvironment, but rather the ability to deal with\nsome range of possibilities which cannot be wholly\nanticipated. This means that an intelligent agent\nmay not be the best possible in any specific environment, particularly before it has had sufficient\ntime to learn. What is important is that the agent\nis able to learn and adapt so as to perform well over\na wide range of specific environments.\nAlthough there is a great deal more to this topic\nthan we have presented here, the above brief analysis gives us the necessary building blocks for our\ninformal working definition of intelligence:\n\nThe concept of intelligence\n\nAlthough definitions of human intelligence given by\nexperts in the field vary, most of their views cluster around a few common perspectives. Perhaps\nthe most common perspective, roughly stated, is to\nthink of intelligence as being the ability to successfully operate in uncertain environments by learning and adapting based on experience. The following often quoted definitions, which can be found\nin [Ste00], [Wec58], [Bin37] and [Got97], all express\nthis notion of intelligence but with different emphasis in each case:\n\u2022 \"The capacity to learn or to profit by experience.\" \u2013 W. F. Dearborn\n\u2022 \"Ability to adapt oneself adequately to relatively new situations in life.\" \u2013 R. Pinter\n\u2022 \"A person possesses intelligence insofar as he\nhas learned, or can learn, to adjust himself to\nhis environment.\" \u2013 S. S. Colvin\n\u2022 \"We shall use the term 'intelligence' to mean\nthe ability of an organism to solve new problems. . . .\" \u2013 W. V. Bingham\n\u2022 \"A global concept that involves an individual's ability to act purposefully, think rationally, and deal effectively with the environment.\" \u2013 D. Wechsler\n\nIntelligence measures an agent's ability to\nachieve goals in a wide range of environments.\n\nWe realise that some researchers who study in\u2022 \"Intelligence is a very general mental capability that, among other things, involves telligence will take issue with this definition. Given\nthe ability to reason, plan, solve problems, the diversity of views on the nature of intelligence,\n2\n\n\fobservation\na debate which is still being fought, this is unavoidable. Nevertheless, we are confident that our\nproposed informal working definition is fairly mainreward\nstream. We also believe that our definition captures\nwhat we are interested in achieving in machines: A\nagent\nenvironment\nvery general and flexible capacity to succeed when\nfaced with a wide range of problems and situations.\nEven those who subscribe to different perspectives\naction\non the nature and correct definition of intelligence\nwill surely agree that this is a central objective for Figure 1: The agent and the environment interact\nanyone wishing to extend the power and usefulness by sending action, observation and reward signals\nof machines. It is also a definition that can be suc- to each other.\ncessfully formalised.\n\n3\n\nThe\nagent-environment\nframework\n\nwhat causes different levels of reward to occur. In\na complex setting the agent might be rewarded for\nwinning a game or solving a difficult puzzle. From\na broad perspective then, the goal is flexible. If the\nagent is to succeed in its environment, that is, receive a lot of reward, it must learn about the structure of the environment and in particular what it\nneeds to do in order to get reward.\n\nIn the previous section we identified three essential\ncomponents for our model of intelligence: An agent,\nan environment, and a goal. Clearly, the agent and\nthe environment must be able to interact with each\nother; specifically, the agent needs to be able to\nsend signals to the environment and also receive\nsignals being sent from the environment. Similarly\nthe environment must be able to receive and send\nsignals to the agent. In our terminology we will\nadopt the agent's perspective on these communications and refer to the signals from the agent to the\nenvironment as actions, and the signals from the\nenvironment as perceptions.\nWhat is missing from this setup is the goal. As\ndiscussed in the previous section, our definition of\nan agent's intelligence requires there to be some\nkind of goal for the agent to try to achieve. This\nimplies that the agent somehow knows what the\ngoal is. One possibility would be for the goal to be\nknown in advance and for this knowledge to be built\ninto the agent. The problem with this however is\nthat it limits each agent to just one goal. We need\nto allow agents which are more flexible than this.\nIf the goal is not known in advance, the other\nalternative is to somehow inform the agent of what\nthe goal is. For humans this is easily done using\nlanguage. In general however, the possession of a\nsufficiently high level of language is too strong an\nassumption to make about the agent. Indeed, even\nfor something as intelligent as a dog or a cat, direct\nexplanation will obviously not work.\nFortunately there is another possibility. We can\ndefine an additional communication channel with\nthe simplest possible semantics: A signal that indicates how good the agent's current situation is. We\nwill call this signal the reward. The agent's goal is\nthen simply to maximise the amount of reward it\nreceives, so in a sense its goal is fixed. This is not\nlimiting though as we have not said anything about\n\nNot surprisingly, this is exactly the way in which\nwe condition an animal to achieve a goal: by selectively rewarding certain behaviours. In a narrow\nsense the animal's goal is fixed, perhaps to get more\ntreats to eat, but in a broader sense this may require\ndoing a trick or solving a puzzle.\nIn our framework we will include the reward signal as a part of the perception generated by the\nenvironment. The perceptions also contain a nonreward part, which we will refer to as observations.\nThis now gives us the complete system of interacting agent and environment in Figure 1. The goal, in\nthe broad flexible sense, is implicitly defined by the\nenvironment as this is what defines when rewards\nare generated. Thus in this framework, to test an\nagent in any given way, it is sufficient to fully define\nthe environment.\nIn artificial intelligence, this framework is used\nin the area of reinforcement learning [SB98]. By\nappropriately renaming things, it also describes the\ncontroller-plant framework used in control theory.\nIt is a widely used and very general structure that\ncan describe seemingly any kind of learning or control problem. The interesting point for us is that\nthis type of framework follows naturally from our\ninformal definition of intelligence. The only difficulty was how to deal with the notion of success, or\nprofit. This requires the existence of some kind of\nobjective or goal, and the most flexible and elegant\nway to bring this into our framework is by using a\nsimple reward signal.\n3\n\n\f4\n\nA formal framework for intelligence\n\nor \"success\" for an agent. Informally, we know that\nthe agent must try to maximise the amount of reward it receives, however this could mean several\ndifferent things.\n\nHaving made the basic framework explicit, we can\nnow formalise things. See [Hut04] for a more complete technical description along with many more\nexample agents and environments.\nThe agent sends information to the environment\nby sending symbols from some finite set, for example, \u00c5 := {lef t, right, f orwards, backwards}. We\nwill call this set the action space and denote it by\n\u00c5. Similarly, the environment sends signals to the\nagent with symbols from a finite set called the perception space, which we will denote P. The reward\nspace, denoted by R, will always be a finite subset\nof the rational unit interval [0, 1]\u2229Q. Every perception consists of two separate parts; an observation\nand a reward. For example, we might have P :=\n{(cold, 0.0), (warm, 1.0), (hot, 0.3), (roasting, 0.0)}.\nTo denote symbols being sent we will use the\nlower case variable names a, o and r for actions,\nobservations and rewards respectively. We will also\nindex these in the order in which they occur, thus\na1 is the agent's first action, a2 is the second action and so on. The agent and the environment\nwill take turns at sending symbols, starting with\nthe environment. This produces a history of observations, rewards and actions which we will denote\nby, o1 r1 a1 o2 r2 a2 o3 r3 a3 o4 . . .. Our restriction to finite action and perception spaces is deliberate as\nan agent should not be able to receive or generate\ninformation without bound in a single cycle in time.\nOf course, the action and perception spaces can still\nbe extremely large, if required.\nFormally, the agent is a function, denoted by \u03c0,\nwhich takes the current history as input and chooses\nthe next action as output. A convenient way of representing the agent is as a probability measure over\nactions conditioned on the current history. Thus\n\u03c0(a3 |o1 r1 a1 o2 r2 ) is the probability of action a3 in\nthe third cycle, given that the current history is\no1 r1 a1 o2 r2 . A deterministic agent is simply one\nthat always assigns a probability of 1 to some action for any given history. How the agent produces\nthe distribution over actions for any given history\nis left completely open. Of course in artificial intelligence the agent will be a machine and so \u03c0 will be\na computable function.\nThe environment, denoted \u03bc, is defined\nin a similar way.\nSpecifically, for any\nk \u2208 N the probability of ok rk , given the\ncurrent history o1 r1 a1 . . . ok\u22121 rk\u22121 ak\u22121 ,\nis\n\u03bc(ok rk |o1 r1 a1 . . . ok\u22121 rk\u22121 ak\u22121 ).\nFor the moment we will not place any further restrictions on\nthe environment.\nOur next task is to formalise the idea of \"profit\"\n\nExample. Define the reward space R := {0, 1}, an\naction space \u00c5 := {0, 1} and an observation space\nthat just contains the null string, O := {\u03b5}. Now\ndefine a simple environment,\n\u03bc(rk |o1 . . . ak\u22121 ) := 1 \u2212 |rk \u2212 ak\u22121 |.\nAs the agent always get a reward equal to its action,\nthe optimal agent for this environment is clearly\n\u03c0opt (ak |o1 . . . rk ) := ak . Consider now two other\nagents for this environment, \u03c01 (ak |o1 . . . rk ) = 21\nand\n\uf8f1\n\uf8f4\n\uf8f4 1 for ak = 0 \u2227 k \u2264 100,\n\uf8f2\n1 for ak = 1 \u2227 100 < k \u2264 5000,\n\u03c02 (ak |o1 . . . rk ) :=\n1\nfor 5000 < k,\n\uf8f4\n\uf8f4\n\uf8f3 2\n0 otherwise.\n\nFor 1 \u2264 k \u2264 100 the expected reward per cycle\nfor \u03c01 is higher than it is for \u03c02 . Thus in the short\nterm \u03c01 is the most successful. On the other hand,\nfor 100 < k \u2264 5000, \u03c02 has switched to the optimal strategy of always guessing that 1 head will be\nthrown, while \u03c01 has not. Thus in the medium term\n\u03c02 is more successful. Finally, for k > 5000, both\nagents use random actions and thus in the limit they\nare equally successful.\nWhich is the better agent? If you want to maximise short term rewards, it is agent \u03c01 . If you\nwant to maximise medium term rewards, then it is\nagent \u03c02 . And if you only care about the long run,\nboth agents are equally successful. Which agent you\nprefer depends on your temporal preferences, something which is currently outside of our formulation.\nThe standard way of formalising this in reinforcement learning is to assume that the value of rewards\ndecay geometrically into the future at a rate given\nby a discount parameter \u03b3 \u2208 (0, 1), that is,\n!\n\u221e\nX\n1\n(1)\n\u03b3 i ri\nV\u03bc\u03c0 (\u03b3) := E\n\u0393\ni=1\nwhere ri is the reward in cycle i ofPa given history,\ni\nthe normalising constant is \u0393 := \u221e\ni=1 \u03b3 , and the\nexpected value is taken over all histories of \u03c0 and\n\u03bc interacting. By increasing \u03b3 towards 1 we weight\nlong term rewards more heavily, conversely by reducing it we balance the weighting towards short\nterm rewards.\nOf course this has not actually answered the\nquestion of how to weight near term rewards versus longer term rewards. Rather it has simply expressed this weighting as a parameter. While that is\n4\n\n\fadequate for some purposes, what we would like is a\nsingle test of intelligence for machines, not a range\nof tests that vary according to some free parameter.\nThat is, we would like the temporal preferences to\nbe included in the model, not external to it.\nOne possibility might be to use harmonic discounting, \u03b3t := t12 . This has some nice properties,\nin particular the agent needs to look forward into\nthe future in a way that is proportional to its current age [Hut04]. However an even more elegant\nsolution is possible.\nIf we look at the value function in Equation 1,\nwe see that geometric discounting plays two roles.\nFirstly, it normalises the total reward received\nwhich makes the sum finite, in this case with a\nmaximum value of 1. Secondly, it weights the reward at different points in the future which in effect\ndefines a temporal preference. We can solve both\nof these problems, without needing an external parameter, by simply requiring that the total reward\nreturned by the environment cannot exceed 1. For a\nreward summable environment \u03bc we can now define\nthe value function to be simply,\n!\n\u221e\nX\n\u03c0\n(2)\nri \u2264 1.\nV\u03bc := E\n\nThus the most accurate framework would consist\nof an agent, an environment and a separate goal system that interpreted the state of the environment\nand rewarded the agent appropriately. In such a set\nup the bounded rewards restriction would be a part\nof the goal system and thus the above philosophical\nproblem does not occur. However for our current\npurposes it is seem sufficient just to fold this goal\nmechanism into the environment and add an easily implemented constraint to how the environment\nmay generate rewards.\n\n5\n\nA formal measure of intelligence\n\nWe have now formally defined the space of agents,\nhow they interact with each other, and how we measure the performance of an agent in any specific\nenvironment. Before we can put all this together\ninto a single performance measure, we firstly need\nto define what me mean by \"a wide range of environments.\"\nAs our goal is to produce a measure of intelligence\nthat is as broad and encompassing as possible, the\nspace of environments used in our definition should\nbe as large as possible. Given that our environment\nis a probability measure with a certain structure, an\nobvious possibility would be to consider the space\nof all probability measures of this form. Unfortunately, this extremely broad class of environments\ncauses problems. As the space of all probability\nmeasures is uncountably infinite, we cannot list the\nmembers of this set, nor can we always describe environments in a finite way.\nThe solution is to require the environmental measures to be computable. Not only is this necessary\nif we are to have an effective measure of intelligence,\nit is also not all that restrictive. There are an infinite number of environments in this set, with no\nupper bound on their complexity. Furthermore, it is\nonly the measure which describes the environment\nthat must be computable. For example, although a\ntypical sequence of 1's and 0's generated by flipping\na coin is not computable, the probability measure\nwhich describes this process is computable. Thus,\neven environments which behave randomly are included in our space of environments. This appears\nto be the largest reasonable space of environments.\nIndeed, no physical system has ever been shown to\nlie outside of this set. If such a physical system was\nfound, it would overturn the Church-Turing thesis\nand alter our view of the universe.\nHow can we combine the agent's performance\nover all these environments? As there are an infinite\nnumber of environments, we cannot simply take a\nuniform distribution over them. Mathematically,\n\ni=1\n\nOne way of viewing this is that the rewards returned by the environment now have the temporal\npreference factored in and thus we do not need to\nadd this. The cost is that this is an additional condition that we place on the environments. Previously we required that each reward signal was in a\nfinite subset of [0, 1]\u2229Q, now we have the additional\nconstraint that the sum is bounded.\nIt may seem that there is a philosophical problem\nhere. If an environment \u03bc is an artificial game, like\nchess, then it seems fairly natural for \u03bc to meet\nany requirements in its definition, such as having a\nbounded reward sum. However if we think of the\nenvironment \u03bc as being \"the universe\" in which the\nagent lives, then it seems unreasonable to expect\nthat it should be required to respect such a bound.\nThe flaw in this argument is that a \"universe\" does\nnot have any notion of reward for particular agents.\nStrictly speaking, reward is an interpretation of\nthe state of the environment. In humans this is built\nin, for example, the pain that is experienced when\nyou touch something hot. In which case, maybe\nit should really be a part of the agent rather than\nthe environment? If we gave the agent complete\ncontrol over rewards then our framework would become meaningless: The perfect agent could simply\ngive itself constant maximum reward. Indeed humans cannot easily do this either, at least not without taking drugs designed to interfere with their\npleasure-pain mechanism.\n5\n\n\fwe must weight some environments more highly\nthan others. If we consider the agent's perspective\non the problem, this question is the same as asking:\nGiven several different hypotheses which are consistent with the data, which hypothesis should be considered the most likely? This is a frequently occurring problem in inductive inference where we must\nemploy a philosophical principle to decide which\nhypothesis is the most likely. The most successful approach is to invoke the principle of Occam's\nrazor: Given multiple hypotheses which are consistent with the data, the simplest should be preferred.\nThis is generally considered the rational and intelligent thing to do.\nConsider for example the following type of question which commonly appears in intelligence tests.\nThere is a sequence such as 2, 4, 6, 8, and the\ntest subject needs to predict the next number. Of\ncourse the pattern is immediately clear: The numbers are increasing by 2 each time. An intelligent\nperson would easily identify this pattern and predict\nthe next digit to be 10. However, the polynomial\n2k 4 \u2212 20k 3 + 70k 2 \u2212 98k + 48 is also consistent with\nthe data, in which case the next number in the sequence would be 58. Why then do we consider the\nfirst answer to be more likely? It is because we use,\nperhaps unconsciously, the principle of Occam's razor. Furthermore, the fact that the test defines this\nas the correct answer shows that it too embodies\nthe concept of Occam's razor. Thus, although we\ndon't usually mention Occam's razor when defining\nintelligence, the ability to effectively use Occam's\nrazor is clearly a part of intelligent behaviour.\nOur formal measure of intelligence needs to reflect this. Specifically, we need to test the agents in\nsuch a way that they are, at least on average, rewarded for correctly applying Occam's razor. Formally, this means that our a priori distribution over\nenvironments should be weighted towards simpler\nenvironments. The problem now becomes: How\nshould we measure the complexity of environments?\nAs each environment is computable, it can be represented by a program, or more formally, a binary\nstring p \u2208 B\u2217 on some prefix universal Turing machine U. Thus we can use Kolmogorov complexity to measure the complexity of an environment\n\u03bc \u2208 E,\n\b\nK(\u03bc) := min\u2217 |p| : U(p) computes \u03bc .\n\ncontext of universally optimal learning agents. See\n[LV97] or [Hut04] for an overview of Kolmogorov\ncomplex and universal prior distributions.\nPutting this all together, we can now define our\nformal measure of intelligence for arbitrary systems.\nLet E be the space of all programs that compute\nenvironmental measures of summable reward with\nrespect to a prefix universal Turing machine U, let\nK be the Kolmogorov complexity function. The\nintelligence of an agent \u03c0 is defined as,\nX\n\u03a5(\u03c0) :=\n2\u2212K(\u03bc) V\u03bc\u03c0 = V\u03be\u03c0 ,\n\u03bc\u2208E\n\nP\nwhere \u03be := \u03bc\u2208E 2\u2212K(\u03bc) \u03bc due to the linearity of\nV . \u03be is the Solomonoff-Levin universal a priori distribution generalised to reactive environments.\n\n6\n\nProperties of the\ngence measure\n\nintelli-\n\nTo better understand the performance of this measure consider some example agents.\nA random agent. The agent with the lowest intelligence, at least among those that are not actively\ntrying to perform badly, would be one that makes\nuniformly random actions. We will call this \u03c0 rand .\nIn general such an agent will not be very successful\nas it will fail to exploit any regularities in the environment, no matter how simple they are. It follows\nrand\nthen that the values of V\u03bc\u03c0\nwill typically be low\ncompared to other agents, and thus \u03a5(\u03c0 rand ) will\nbe low.\nA very specialised agent. From the equation for\n\u03a5, we see that an agent could have very low intelligence but still perform extremely well at a few very\nspecific and complex tasks. Consider, for example, IBM's Deep Blue chess supercomputer, which\nwe will represent by \u03c0 dblue . When \u03bcchess describes\ndblue\nthe game of chess, V\u03bc\u03c0chess is very high. However\nchess\n\n2\u2212K(\u03bc ) is small, and for \u03bc 6= \u03bcchess the value\nfunction will be low relative to other agents as \u03c0 dblue\nonly plays chess. Therefore, the value of \u03a5(\u03c0 dblue )\nwill be very low. Intuitively, this is because Deep\nBlue is too inflexible and narrow to have general\nintelligence.\nA general but simple agent. Imagine an agent\nthat does very basic learning by building up a table of observation and action pairs and keeping\nstatistics on the rewards that follow. Each time\nan observation that has been seen before occurs,\nthe agent takes the action with highest estimated\nexpected reward in the next cycle with 90% probability, or a random action with 10% probability.\nWe will call this agent \u03c0 basic . It is immediately\nclear that many environments, both complex and\n\np\u2208B\n\nThis measure is independent of the choice of U up to\nan additive constant that is independent of \u03bc, thus,\nwe simply pick one universal Turing machine U and\nfix it. The correct way to turn this into a prior\ndistribution is by taking 2\u2212K(\u03bc) . This is known as\nthe algorithmic probability distribution and it has a\nnumber of important properties, particularly in the\n\n6\n\n\fvery simple, will have at least some structure that\nsuch an agent would take advantage of. Thus for\nbasic\nrand\nalmost all \u03bc we will have V\u03bc\u03c0\n> V\u03bc\u03c0\nand so\n\u03a5(\u03c0 basic ) > \u03a5(\u03c0 rand ). Intuitively, this is what we\nwould expect as \u03c0 basic , while very simplistic, is\nsurely more intelligent than \u03c0 rand .\nA simple agent with more history. A natural extension of \u03c0 basic is to use a longer history of actions,\nobservations and rewards in its internal table. Let\n\u03c0 2back be the agent that builds a table of statistics for the expected reward conditioned on the last\ntwo actions, rewards and observations. It is immediately clear \u03c0 2back is a generalisation of \u03c0 basic by\ndefinition and thus will adapt to any regularity that\n\u03c0 basic can adapt to. It follows then that in general\n2back\nbasic\nV\u03bc\u03c0\n> V\u03bc\u03c0\nand so \u03a5(\u03c0 2back ) > \u03a5(\u03c0 basic ), as\nwe would intuitively expect.\nIn a similar way agents of increasing complexity and adaptability can be defined which will have\nstill greater intelligence. However with more complex agents it is usually difficult to theoretically establish whether one agent has more or less intelligence than another. Nevertheless, it is hopefully\nclear from these simple examples that the more flexible and powerful an agent is, the higher its machine\nintelligence.\nA human. For extremely simple environments, a\nhuman should be able to identify their simple structure and exploit this to maximise reward. For more\ncomplex environments however it is hard to know\nhow well a human would perform without experimental results.\nSuper-human intelligence. It can be easily proven\nthat the theoretical AIXI agent [Hut04] is the maximally intelligent agent with respect to \u03a5. AIXI\nhas been proven to have many universal optimality properties, including being Pareto optimal and\nself-optimising in any environment in which this is\npossible for a general agent. Thus it is clear that\nagents with very high \u03a5 must be extremely powerful.\nIn addition to sensibly ordering many simple\nlearning agents, this formal definition has many significant and desirable properties:\nValid. The most important property of a measure\nof intelligence is that it does indeed measure \"intelligence\". As \u03a5 formalises a mainstream informal\ndefinition, we believe that it is valid measure.\nMeaningful. An agent with a high \u03a5 value must\nperform well over a very wide range of environments, in particular it must perform well in almost\nall simple environments. If such a agent existed, it\nwould clearly be very powerful and practically useful. It also sensibly orders the intelligence of simple\nlearning agents.\nRepeatable. We can test an agent using the \u03a5 repeatedly without problem. This is because it is de-\n\nfined across all well defined environments, not just\na specific test subset which an agent might adapt\nto.\nAbsolute. \u03a5 gives us a single real absolute value,\nunlike the pass-fail Turing test [Tur50]. This is important if we want to make distinctions between\nsimilar learning algorithms that are not close to human level intelligence.\nWide range. As we have seen, \u03a5 can measure performance from extremely simple agents right up to\nthe super powerful AIXI agent. Other tests cannot\nhand such an enormus range.\nGeneral. The test is clearly non-specific to the\nimplementation of the agent as the inner workings\nof the agent is left completely undefined. It is also\nvery general in terms of what senses or actuators\nthe agent might have as all information exchanged\nbetween the agent and the environment takes place\nover basic Shannon like communication channels.\nDynamic. One aspect of our test of intelligence\nis that it is, in the terminology of intelligence testing, a highly dynamic test [SG02]. Normally intelligence tests for humans only test the ability to\nsolve one-off problems. There are no dynamic aspects to the test where the test subject has to interact with something and learn and adapt their\nbehaviour accordingly. This makes it very hard to\ntest things like the individual's ability to quickly\npick up new skills and adapt to new situations. One\nway to overcome these problems is to use more sophisticated dynamic tests. In these tests there is\nan active tester who constantly interacts with the\ntest subject, much like what happens in our formal\nintelligence measure.\nUnbiased. The test is not weighted towards ability in certain specific kinds of areas or problems,\nrather it is simply weighted towards simpler environments no matter what they are.\nFundamental. The test is based on the theory\nof information, Turing computation and complexity\ntheory. These are all fundamental ideas which are\nlikely to remain very stable over time irrespective\nof changes in technology.\nFormal. Unlike many tests of intelligence, \u03a5 is\ncompletely formally, mathematically, specified.\nObjective. Unlike the Turing test which requires\na panel of judges to decide if an agent is intelligent\nor not, \u03a5 is fee of such subjectivity.\nOur definition of intelligence also has some weaknesses. One is the fact that the environmental distribution 2\u2212K(\u03bc) that we have used is invariant,\nup to a multiplicative constant, to changes in the\nreference machine U. While this affords us some\nprotection, it still means that the relative intelligence of agents can change if we change our reference machine. One approach to this problem might\nbe to limit the complexity of the reference machine,\n7\n\n\ffor example by limiting its state-symbol complexity. We expect that for highly intelligent machines\nthat can deal with a wide range of environments\nof varying complexity, the effect of changing from\none simple reference machine to another will be minor. For agents which are less complex than the\nreference machine however, such a change could be\nsignificant.\nA theoretical problem is that our distribution\nover environments is not computable. While this\nis fine for a theoretical definition of intelligence,\nit makes the measure impossible to directly implement. The solution is to use a more tractable\nmeasure of complexity such as Levin's Kt complexity [Lev73], or Schmidhuber's Speed prior [Sch02].\nBoth of these consider the complexity of an algorithm to be determined by both its description\nlength and running time. Intuitively it also makes\ngood sense, because we would not usually consider\na very short algorithm that takes an enormous\namount of time to compute, to be a particularly\nsimple one.\nThe only closely related work to ours is the CTest [HO00]. While our intelligence measure is fully\ndynamic and interactive, the C-Test is a purely\nstatic sequence prediction test similar to standard\nIQ tests for humans. The C-Test always ensures\nthat each question has an unambiguous answer in\nthe sense that there is always one consistent hypothesis with significantly lower complexity than the alternatives. Perhaps this is useful for some kinds of\ntests, but we believe that it is unrealistic and limiting. Like our intelligence test, the C-Test also has\nto deal with the problem of the incomputability of\nKolmogorov complexity. By using Levin's Kt complexity, the C-Test was able to compute a number of\ntest problems which were used to test humans. The\n\"compression test\"[Mah99] for machine intelligence\nis similarly restricted to sequence prediction. We\nconsider the linguistic complexity tests of TreisterGoren et. al. to be far too narrow. The psychometric approach of Bringsjord and Schimanski is only\nappropriate if the machine has a sufficiently humanlike intelligence.\n\n7\n\nof machine intelligence in an elegant and powerful\nway. Furthermore, more tractable measures of complexity should lead to practical tests based on this\ntheoretical model.\n\nAcknowledgments\nThis was supported by SNF grant 200020-107616.\n\nReferences\n[Bin37] W. V. Bingham. Aptitudes and aptitude testing.\nHarper & Brothers, New York, 1937.\n[Got97] L. S. Gottfredson. Mainstream science on intelligence: An editorial with 52 signatories, history,\nand bibliography. Intelligence, 24(1):13\u201323, 1997.\n[HO00] J. Hern\u00e1ndez-Orallo.\nBeyond the Turing\ntest. Journal of Logic, Language and Information,\n9(4):447\u2013466, 2000.\n[Hut04] M. Hutter. Universal Artificial Intelligence:\nSequential Decisions based on Algorithmic Probability.\nSpringer, Berlin, 2004.\n300 pages,\nhttp://www.idsia.ch/\u223c marcus/ai/uaibook.htm.\n[Joh92] W. L. Johnson. Needed: A new test of intelligence. SIGARTN: SIGART Newsletter (ACM Special\nInterest Group on Artificial Intelligence), 3, 1992.\n[Lev73] L. A. Levin. Universal sequential search problems. Problems of Information Transmission, 9:265\u2013\n266, 1973.\n[LH05] S. Legg and M. Hutter. A universal measure of\nintelligence for artificial agents. In Proc. 21st International Joint Conf. on Artificial Intelligence (IJCAI2005), number IDSIA-04-05, pages 1509\u20131510, Edinburgh, 2005.\n[LV97] M. Li and P. M. B. Vit\u00e1nyi. An introduction to\nKolmogorov complexity and its applications. Springer,\n2nd edition, 1997.\n[Mah99] M. V. Mahoney. Text compression as a test\nfor artificial intelligence. In AAAI/IAAI, 1999.\n[SB98] R. Sutton and A. Barto. Reinforcement learning: An introduction. Cambridge, MA, MIT Press,\n1998.\n[Sch02] J. Schmidhuber. The Speed Prior: a new\nsimplicity measure yielding near-optimal computable\npredictions. In Proc. 15th Annual Conference on\nComputational Learning Theory (COLT 2002), Lecture Notes in Artificial Intelligence, pages 216\u2013228,\nSydney, Australia, July 2002. Springer.\n[SG02] R. J. Sternberg and E. L. Grigorenko, editors.\nDynamic Testing: The nature and measurement of\nlearning potential. Cambridge University Press, 2002.\n[Ste00] R. J. Sternberg, editor. Handbook of Intelligence. Cambridge University Press, 2000.\n[Tur50] A. M. Turing. Computing machinery and intelligence. Mind, October 1950.\n[Wec58] D. Wechsler. The measurement and appraisal\nof adult intelligence. Williams & Wilkinds, Baltimore, 4 edition, 1958.\n\nConclusions\n\nGiven the obvious significance of formal definitions\nof intelligence for research, and calls for more direct measures of machine intelligence to replace the\nproblematic Turing test and other imitation based\ntests [Joh92], very little work has been done in this\narea. In this paper we have attempted to tackle\nthis problem head on. Although the test has a few\nweaknesses, it also has many unique strengths. In\nparticular, we believe that it expresses the essentials\n8\n\n\f"}