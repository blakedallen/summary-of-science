{"id": "http://arxiv.org/abs/0903.5005v1", "guidislink": true, "updated": "2009-03-28T21:25:35Z", "updated_parsed": [2009, 3, 28, 21, 25, 35, 5, 87, 0], "published": "2009-03-28T21:25:35Z", "published_parsed": [2009, 3, 28, 21, 25, 35, 5, 87, 0], "title": "A Probabilistic Characterization of Random Proximity Catch Digraphs and\n  the Associated Tools", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0903.0728%2C0903.3422%2C0903.1703%2C0903.1382%2C0903.1170%2C0903.3495%2C0903.0632%2C0903.4531%2C0903.5359%2C0903.1289%2C0903.1009%2C0903.4709%2C0903.3765%2C0903.0812%2C0903.1161%2C0903.4590%2C0903.0320%2C0903.0197%2C0903.0073%2C0903.3148%2C0903.4194%2C0903.1558%2C0903.3996%2C0903.4221%2C0903.5111%2C0903.5465%2C0903.2236%2C0903.3121%2C0903.5087%2C0903.3492%2C0903.5160%2C0903.4965%2C0903.3282%2C0903.3882%2C0903.0796%2C0903.5244%2C0903.1979%2C0903.4068%2C0903.4380%2C0903.3742%2C0903.2126%2C0903.3584%2C0903.3194%2C0903.0499%2C0903.3747%2C0903.1343%2C0903.5259%2C0903.4609%2C0903.1856%2C0903.0936%2C0903.5467%2C0903.4246%2C0903.0299%2C0903.2089%2C0903.1715%2C0903.1366%2C0903.4092%2C0903.5005%2C0903.5145%2C0903.3453%2C0903.2110%2C0903.3704%2C0903.2905%2C0903.1249%2C0903.0459%2C0903.3636%2C0903.5206%2C0903.0827%2C0903.1197%2C0903.1905%2C0903.4546%2C0903.4983%2C0903.2506%2C0903.2227%2C0903.4250%2C0903.3666%2C0903.5218%2C0903.1193%2C0903.4567%2C0903.4033%2C0903.0080%2C0903.0044%2C0903.5508%2C0903.0770%2C0903.2886%2C0903.1428%2C0903.5335%2C0903.1384%2C0903.1149%2C0903.3008%2C0903.4596%2C0903.0751%2C0903.2333%2C0903.1025%2C0903.0952%2C0903.2558%2C0903.1315%2C0903.0932%2C0903.5225%2C0903.2709%2C0903.4078&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Probabilistic Characterization of Random Proximity Catch Digraphs and\n  the Associated Tools"}, "summary": "Proximity catch digraphs (PCDs) are based on proximity maps which yield\nproximity regions and are special types of proximity graphs. PCDs are based on\nthe relative allocation of points from two or more classes in a region of\ninterest and have applications in various fields. In this article, we provide\nauxiliary tools for and various characterizations of PCDs based on their\nprobabilistic behavior. We consider the cases in which the vertices of the PCDs\ncome from uniform and non-uniform distributions in the region of interest. We\nalso provide some of the newly defined proximity maps as illustrative examples.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0903.0728%2C0903.3422%2C0903.1703%2C0903.1382%2C0903.1170%2C0903.3495%2C0903.0632%2C0903.4531%2C0903.5359%2C0903.1289%2C0903.1009%2C0903.4709%2C0903.3765%2C0903.0812%2C0903.1161%2C0903.4590%2C0903.0320%2C0903.0197%2C0903.0073%2C0903.3148%2C0903.4194%2C0903.1558%2C0903.3996%2C0903.4221%2C0903.5111%2C0903.5465%2C0903.2236%2C0903.3121%2C0903.5087%2C0903.3492%2C0903.5160%2C0903.4965%2C0903.3282%2C0903.3882%2C0903.0796%2C0903.5244%2C0903.1979%2C0903.4068%2C0903.4380%2C0903.3742%2C0903.2126%2C0903.3584%2C0903.3194%2C0903.0499%2C0903.3747%2C0903.1343%2C0903.5259%2C0903.4609%2C0903.1856%2C0903.0936%2C0903.5467%2C0903.4246%2C0903.0299%2C0903.2089%2C0903.1715%2C0903.1366%2C0903.4092%2C0903.5005%2C0903.5145%2C0903.3453%2C0903.2110%2C0903.3704%2C0903.2905%2C0903.1249%2C0903.0459%2C0903.3636%2C0903.5206%2C0903.0827%2C0903.1197%2C0903.1905%2C0903.4546%2C0903.4983%2C0903.2506%2C0903.2227%2C0903.4250%2C0903.3666%2C0903.5218%2C0903.1193%2C0903.4567%2C0903.4033%2C0903.0080%2C0903.0044%2C0903.5508%2C0903.0770%2C0903.2886%2C0903.1428%2C0903.5335%2C0903.1384%2C0903.1149%2C0903.3008%2C0903.4596%2C0903.0751%2C0903.2333%2C0903.1025%2C0903.0952%2C0903.2558%2C0903.1315%2C0903.0932%2C0903.5225%2C0903.2709%2C0903.4078&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Proximity catch digraphs (PCDs) are based on proximity maps which yield\nproximity regions and are special types of proximity graphs. PCDs are based on\nthe relative allocation of points from two or more classes in a region of\ninterest and have applications in various fields. In this article, we provide\nauxiliary tools for and various characterizations of PCDs based on their\nprobabilistic behavior. We consider the cases in which the vertices of the PCDs\ncome from uniform and non-uniform distributions in the region of interest. We\nalso provide some of the newly defined proximity maps as illustrative examples."}, "authors": ["Elvan Ceyhan"], "author_detail": {"name": "Elvan Ceyhan"}, "author": "Elvan Ceyhan", "links": [{"href": "http://arxiv.org/abs/0903.5005v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0903.5005v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "60D05; 60C05; 60A10; 05C80; 05C20; 60-08", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0903.5005v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0903.5005v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:0903.5005v1 [math.PR] 28 Mar 2009\n\nTechnical Report # KU-EC-09-3:\nA Probabilistic Characterization of Random Proximity Catch\nDigraphs and the Associated Tools\nElvan Ceyhan\n\n\u2217\n\nNovember 20, 2018\n\nAbstract\nProximity catch digraphs (PCDs) are based on proximity maps which yield proximity regions and are\nspecial types of proximity graphs. PCDs are based on the relative allocation of points from two or more\nclasses in a region of interest and have applications in various fields. In this article, we provide auxiliary\ntools for and various characterizations of PCDs based on their probabilistic behavior. We consider the\ncases in which the vertices of the PCDs come from uniform and non-uniform distributions in the region\nof interest. We also provide some of the newly defined proximity maps as illustrative examples.\n\nKeywords: class cover catch digraph (CCCD); central similarity PCD; Delaunay triangulation; domination\nnumber; proportional-edge PCD; proximity graph; random graph; relative arc density\n\n\u2217 Address: Department of Mathematics, Ko\u00e7 University, 34450 Sar\u0131yer, Istanbul, Turkey. e-mail: elceyhan@ku.edu.tr, tel:+90\n(212) 338-1845, fax: +90 (212) 338-1559.\n\n1\n\n\f1\n\nIntroduction\n\nThe proximity catch digraphs (PCDs) are a special type of proximity graphs which are based on proximity\nmaps and are used in disciplines where shape and structure are crucial. Examples include computer vision\n(dot patterns), image analysis, pattern recognition (prototype selection), geography and cartography, visual\nperception, biology, etc. Proximity graphs were first introduced by Toussaint (1980), who called them relative\nneighborhood graphs. The notion of relative neighborhood graph has been generalized in several directions\nand all of these graphs are now called proximity graphs. From a mathematical and algorithmic point of\nview, proximity graphs fall under the category of computational geometry.\nIn recent years, a new classification and spatial pattern analysis approach which is based on the relative\npositions of the data points from various classes has been developed. Priebe et al. (2001) introduced the\nclass cover catch digraphs (CCCDs) and gave the exact and the asymptotic distribution of the domination\nnumber of the CCCD based on two data sets Xn and Ym both of which are random samples from uniform\ndistribution on a compact interval in R. DeVinney et al. (2002), Marchette and Priebe (2003), Priebe et al.\n(2003b), Priebe et al. (2003a), and DeVinney and Priebe (2006) applied the concept in higher dimensions\nand demonstrated relatively good performance of CCCD in classification. The employed methods involve\ndata reduction (condensing) by using approximate minimum dominating sets as prototype sets, since finding\nthe exact minimum dominating set is in general an NP-hard problem - in particular, for CCCD - (see\nDeVinney (2003)). Furthermore, the exact and the asymptotic distribution of the domination number of the\nCCCDs are not analytically tractable in higher dimensions. Ceyhan (2004) extended the concept of CCCDs\nby introducing PCDs, which do not suffer from some of the shortcomings of CCCDs in higher dimensions. In\nparticular, two new types of PCDs (namely, proportional-edge and central similarity PCDs) are introduced;\ndistribution of the domination number of proportional-edge PCDs is calculated, and is applied in testing\nspatial patterns of segregation and association (Ceyhan and Priebe (2005, 2007)). The distributions of the\nrelative arc density of these PCD families are also derived and used for the same purpose (Ceyhan et al.\n(2006) and Ceyhan et al. (2007)).\nA general definition of proximity graphs is as follows: Let V be any finite or infinite set of points in Rd .\nEach (unordered) pair of points (p, q) \u2208 V \u00d7 V is associated with a neighborhood N(p, q) \u2286 Rd . Let P be a\nproperty defined on N = {N(p, q) : (p, q) \u2208 V \u00d7V }. A proximity (or neighborhood ) graph GN,P (V, E) defined\nby the property P is a graph with the set of vertices V and the set of edges E such that (p, q) \u2208 E iff N(p, q)\nsatisfies property P. Examples of most commonly used proximity graphs are the Delaunay tessellation, the\nboundary of the convex hull, the Gabriel graph, relative neighborhood graph, Euclidean minimum spanning\ntree, and sphere of influence graph of a finite data set. See, e.g., Jaromczyk and Toussaint (1992).\nThe relative allocation of the data points are used to construct a proximity digraph. A digraph is a\ndirected graph, i.e., a graph with directed edges from one vertex to another based on a binary relation.\nThen the pair (p, q) \u2208 V \u00d7 V is an ordered pair and (p, q) is an arc (directed edge) denoted pq to reflect\nits difference from an edge. For example, the nearest neighbor (di)graph in Paterson and Yao (1992) is a\nproximity digraph. The nearest neighbor digraph, denoted N N D(V ), has the vertex set V and pq an arc\niff d(p, q) = minv\u2208V \\{p} d(p, v). That is, pq is an arc of N N D(V ) iff q is a nearest neighbor of p. Note\nthat if pq is an arc in N N D(V ), then (p, q) is an edge in RN G(V ). Our PCDs are based on the property\nP that is determined by the following mapping which is defined in a more general space than Rd . Let\n(\u03a9, M) be a measurable space. The proximity map N (*) is given by N : \u03a9 \u2192 \u2118(\u03a9), where \u2118(*) is the\npower set functional, and the proximity region of x \u2208 \u03a9, denoted N (x), is the image of x \u2208 \u03a9 under N (*).\nThe points in N (x) are thought of as being \"closer\" to x \u2208 \u03a9 than are the points in \u03a9 \\ N (x). Proximity\nmaps are the building blocks of the proximity graphs of Toussaint (1980); an extensive survey is available in\nJaromczyk and Toussaint (1992).\n\b\nThe PCD D = (V, A) has the vertex set V = p1 , p2 , . . . , pn and the arc set A is defined by pi pj \u2208 A\niff pj \u2208 N (pi ) for i 6= j. Notice that D depends on the proximity map N (*), and if pj \u2208 N (pi ), then N (pi ) is\nsaid to catch pj . Hence the name proximity catch digraph. If arcs of the form pi pi (i.e., loops) were allowed,\nD would have been called a pseudodigraph according to some authors (see, e.g., Chartrand and Lesniak\n\n2\n\n\f(1996)).\nIn this article, we provide a probabilistic characterization of the proximity maps, and the associated\nregions and PCDs, and introduce auxiliary tools for the PCDs. We define the proximity maps and datarandom PCDs in Section 2, describe the auxiliary tools (such as edge and vertex regions) for the construction\nof PCDs in Section 3, provide \u03931 -regions and the related concepts for proximity maps in Section 4, discuss the\nexamples of proximity maps in Delaunay triangles in Section 6, and provide the transformations preserving\nuniformity on triangles in R2 in Section 5. We investigate the characterization of proximity regions and the\nassociated PCDs in Section 7, introduce \u0393k -regions for proximity maps in T (Y3 ) in Section 8, \u03ba-values for\nthe proximity maps in T (Y3 ) in Section 9, and provide discussion and conclusions in Section 10.\n\n2\n\nProximity Maps and Data-Random PCDs\n\n\b\n\b\nLet Xn = X1 , X2 , . . . , Xn and Ym = Y1 , Y2 , . . . , Ym be two data sets from classes X and Y of \u03a9-valued\nrandom variables whose joint pdf is fX,Y . Let d(*, *) : \u03a9 \u00d7 \u03a9 \u2192 [0, \u221e) be a distance function. The class\ncover problem for a target class,\nof neighborhoods, N (Xi ) around\n\u0001\nS say Xn\u0001, refers to findingSa collection\nXi \u2208 Xn such that (i) Xn \u2282\ni N (Xi ) = \u2205. A collection of neighborhoods\ni N (Xi ) and (ii) Ym \u2229\nsatisfying both conditions is called a class cover. A cover satisfying condition (i) is a proper cover of class\nX while a collection satisfying condition (ii) is a pure cover relative to class Y. From a practical point of\nview, for example for classification, of particular interest are the class covers satisfying both (i) and (ii)\nwith the smallest collection of neighborhoods, i.e., minimum cardinality cover. This class cover problem\nis a generalization of the set cover problem in Garfinkel and Nemhauser (1972) that emerged in statistical\npattern recognition and machine learning, where an edited or condensed set (prototype set) is selected from\nXn (see, e.g., Devroye et al. (1996)).\n\nIn particular, we construct the proximity regions using data sets from two classes. Given Ym \u2286 \u03a9, the\nproximity map NY (*) associates a proximity region NY (x) \u2286 \u03a9 with each point x \u2208 \u03a9. The region NY (x)\nis defined in terms of the distance between x and Ym . More specifically, our proximity maps will be based\non the relative position of points from class X with respect to the Delaunay tessellation of the class Y. See\nOkabe et al. (2000) and Ceyhan (2009) for more on Delaunay tessellations.\n\nIf Xn is a set of \u03a9-valued random variables then NY (Xi ) are random sets. If Xi are independent identically\ndistributed then so are the random sets NY (Xi ). We define the data-random PCD D = (V, A) - associated\nwith NY (*) - with vertex set V = Xn and arc set A by Xi Xj \u2208 A \u21d0\u21d2 Xj \u2208 NY (Xi ). Since this\nrelationship is not symmetric, a digraph is needed rather than a graph. The random digraph D depends on\nthe (joint) distribution of the Xi and on the map NY (*).\nThe PCDs are closely related to the proximity graphs of Jaromczyk and Toussaint (1992) and might be\nconsidered as a special case of covering sets of Tuza (1994) and intersection digraphs of Sen et al. (1989).\nThis data random proximity digraph is a vertex-random proximity digraph which is not of standard type.\nThe randomness of the PCDs lies in the fact that the vertices are random with joint pdf fX,Y , but arcs Xi Xj\nare deterministic functions of the random variable Xj and the set NY (Xi ).\nFor example, the CCCD of Priebe et al. (2001) can be viewed as an example of PCD with NY (x) =\nB(x, r(x)), where r(x) := miny\u2208Ym d(x, y). The CCCD is the digraph of order n with vertex set Xn and an\narc from Xi to Xj iff Xj \u2208 B(Xi , r(Xi )). That is, there is an arc from Xi to Xj iff there exists an open ball\ncentered at Xi which is \"pure\" (or contains no elements) of Ym , and simultaneously contains (or \"catches\")\npoint Xj .\n\n3\n\nAuxiliary Tools for the Construction of PCDs in Rd\n\nRecall the proximity map (associated with CCCD) in R is defined as B(x, r(x)) where r(x) = miny\u2208Ym d(x, y)\nwith d(x, y) being the Euclidean distance between x and y (Priebe et al. (2001)). Our goal is to extend this\n3\n\n\fidea to higher dimensions and investigate the associated digraph. Now let Ym = {y1 , y2 , . . . , ym } \u2282 Rd .\nFor d = 1 the proximity map associated with CCCD is defined as the open ball NS (x) := B(x, r(x)) for\nall x \u2208 R \\ Ym and for x \u2208 Ym , define NS (x) = {x}. Furthermore,\ndependence on Ym is through r(x).\n\u0001\nHence NS (x) is based on the intervals Ii\u22121 = y(i\u22121):m , yi:m for i = 1, 2, . . . , (m + 1) with y0:m = \u2212\u221e and\ny(m+1):m = \u221e where yi:m is the ith order statistic in Ym . This intervalization can be viewed as a tessellation\nsince it partitions CH (Ym ), the convex hull of Ym . For d > 1, a natural tessellation that partitions CH (Ym )\nis the Delaunay tessellation (see Okabe et al. (2000) and Ceyhan (2009)). Let Ti for i = 1, 2, . . . , J be the ith\nDelaunay cell in the Delaunay tessellation of Ym . In R, we implicitly use the cell that contains x to define\nthe proximity map.\nA natural extension of the proximity region NS (x) to multiple dimensions (i.e., to Rd with d > 1) is obtained by the same definition as above; that is, NS (x) := B(x, r(x)) where r(x) := miny\u2208Ym d(x, y). Notice\nthat a ball is a sphere in higher dimensions, hence the name spherical proximity map and the notation NS .\nThe spherical proximity map NS (x) is well-defined for all x \u2208 Rd provided that Ym 6= \u2205. Extensions to R2\nand higher dimensions with the spherical proximity map - with applications in classification - are investigated in DeVinney et al. (2002), DeVinney and Wierman (2003), Marchette and Priebe (2003), Priebe et al.\n(2003a), Priebe et al. (2003b), and DeVinney and Priebe (2006). However, finding the minimum dominating\nset of the PCD associated with NS (*) is an NP-hard problem and the distribution of the domination number\nis not analytically tractable for d > 1 (Ceyhan (2004)). This drawback has motivated us to define new\ntypes of proximity maps in higher dimensions. Note that for d = 1, such problems do not occur. Ceyhan\n(2009) states some appealing properties of the proximity map NS (x) = B(x, r(x)) in R and uses them as\nguidelines for extending proximity maps to higher dimensions and defining new proximity maps. After a\nslight modification, the spherical proximity maps gives rise to arc-slice proximity maps which is defined\nas NAS (x) := B(x, r(x)) \u2229 T (Y3 ) for x \u2208 T (Y3 ) (i.e., when the arc-slice proximity region is the spherical\nproximity region restricted to the Delaunay triangle x lies in). However, for x 6\u2208 CH (Ym ) (i.e., x is not in\nany of the Delaunay triangles based on Ym ), NAS (x) is not defined.\n\u0001\nFor x \u2208 Ii , NS (x) = Ii iff x = y(i\u22121):m + yi:m /2. We define an associated region for such points in the\ngeneral context.\n\b\nDefinition 3.1. The superset region for any proximity map N (*) in \u03a9 is defined to be RS (N ) := x \u2208 \u03a9 :\nN (x) = \u03a9 . When X is \u03a9-valued random variable, then we assume X \u2208 RS (N ) if N (X) = \u03a9 a.s. \u0003\n\b For example, \u0001 for \u03a9 = Ii ( R with i = 1, 2, . . . , (m \u2212 1), RS (NS ) := {x \u2208 Ii : NS (x) = Ii } =\ny(i\u22121):m + yi:m /2 , and for i = 0, m (i.e., \u03a9 = I0 or \u03a9 = Im , then RS (NS ) = \u2205 since NS (x) ( Ii\nfor all x \u2208 Ii for i = 0, m. More generally for \u03a9 = Ti ( Rd (i.e., ith Delaunay cell), RS (NS ) := {x \u2208 Ti :\nNS (x) = Ti }. Note that for x \u2208 Ii , \u03bb(NS (x)) \u2264 \u03bb(Ii ) and \u03bb(NS (x)) = \u03bb(Ii ) iff x \u2208 RS (NS ) where \u03bb(*) is the\nLebesgue measure on R (also called as R-Lebesgue measure). So the proximity region of a point in RS (NS )\nhas the largest R-Lebesgue measure. Note that for Ym = {y1 , y2 , . . . , ym } \u2282 R (i.e. \u03a9 = R), RS (NS ) = \u2205,\nsince NS (x) \u2286 Ii for all x \u2208 Ii so NS (x) ( R for all x \u2208 R. Note also that given Ym , RS (NS ) is not a\nrandom set, but I(X \u2208 RS (NS )) is a random variable.\n\n3.1\n\nVertex and Edge Regions\n\nIn R, the spherical proximity maps are defined as open intervals where one of the endpoints is in Ym .\nIn particular, for x \u2208 Ii = (yi:m , y(i+1):m ) for i = 1, 2, . . . , m, NS (x) = (yi:m , yi:m + 2r(x)) for all x \u2208\n(yi:m , y(i\u22121):m +yi:m )/2) where r(x) = d(x, yi:m ) and NS (x) = (y(i+1):m , y(i+1):m \u22122r(x)) for all x \u2208 (y(i\u22121):m +\nyi:m )/2, y(i+1):m ) where r(x) = d(x, y(i+1):m ). Hence there are two subinterval in Ii each touching an edge\nand the midpoint of the interval, and NS (x) depends on which of these regions x lies in.\nIn Rd with d > 1, intervals become Delaunay tessellations, and our proximity maps are based on the\nDelaunay cell Ti that contains x. The region NY (x) will also depend on the location of x in Ti with respect\nto the vertices or faces (edges in R2 ) of Ti . Hence for NY (x) to be well-defined, the vertex or face of Ti\n4\n\n\fassociated with x should be uniquely determined. This will give rise to two new concepts: vertex regions\nand face regions (edge regions in R2 ).\nLet Y3 = {y1 , y2 , y3 } be three non-collinear points in R2 and T (Y3 ) = T (y1 , y2 , y3 ) be the triangle with\nvertices Y3 . To define new proximity regions based on some sort of distance or dissimilarity relative to the\nvertices Y3 , we associate each point in T (Y3 ) to a vertex of T (Y3 ). This gives rise to the concept of vertex\nregions.\nDefinition 3.2. The connected regions that partition the triangle, T (Y3 ), (in the sense that the pairwise\nintersections of the regions have zero R2 -Lebesgue measure) such that each region has one and only one\nvertex of T (Y3 ) on its boundary are called vertex regions. \u0003\nThis definition implies that we have three vertex regions. In fact, we can describe the vertex regions\nstarting with a point M \u2208 R2 \\ Y3 as follows. Join the point M to a point on each edge by a curve such\nthat the resultant regions satisfy the above definition. We call such regions M -vertex regions and denote the\nvertex region associated with vertex y as RM (y) for y \u2208 Y3 . Vertex regions can be defined using any point\nM \u2208 R2 \\ Y3 by joining M to a point on each edge. In particular, we use a center of T (Y3 ) as the starting\npoint M for vertex regions. See the discussion of triangle centers in (Ceyhan (2009)) with relevant references.\nWe think of the points in RM (y) as being \"closer\" to y than to the other vertices. It is reasonable to require\nthat the area of the region RM (y) gets larger as d(M, y) increases. Unless stated otherwise, M -vertex regions\nwill refer to regions constructed by joining M to the edges with straight line segments. Vertex regions with\ncircumcenter, incenter, and center of mass are investigated in Ceyhan (2009). For example M -vertex regions\nPSfrag\nreplacements\no\ncan be constructed with M \u2208 T (Y3 ) by using the extensions of the line segments joining y to M for all\ny \u2208 Y3 . See Figure 1 (left) with M = MC .\nPSfrag replacements\n\ny3\n\n0.7\n\ny3\n\n0.7\n\nRCM (y1 )\n\n0.6\n\n0.6\n\nRCM (y2 )\nRCM (y3 )\n\n0.5\n\n0.4\n\nRCM (y3 )0.5\n0.4\n\nM1\n\nM2\n\nM3\n\n0.2\n\nRCM (e2 )\n\nRCM (e1 )\n\n0.3\n\n0.3\n\nMC\nRCM (y1 )\n\nM1\n\nMC\n\nM20.2\n\nRCM (y2 )\n\nRCM (e3 )\n\n0.1\n\n0.1\n\ny2\n\ny1\n0.2\n\n0.4\n\nM3\n\n0.6\n\n0.8\n\ny2\n\ny1\n0\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nFigure 1: The CM -vertex regions (left) and CM -edge regions (right) with median lines.\nWe can also view the endpoints of the interval Ii as the edges of the interval which suggests the concept\nof edge regions.\nDefinition 3.3. The connected regions that partition the triangle, T (Y3 ), in such a way that each region\nhas one and only one edge of T (Y3 ) on its boundary, are called edge regions. \u0003\nThis definition implies that we have exactly three edge regions. In fact, we can describe the edge regions\no\nstarting with M in T (Y3 ) , the interior of T (Y3 ). Join the point M to the vertices by curves such that the\nresultant regions satisfy the above definition. We call such regions M -edge regions and denote the region\nfor edge e as RM (e) for e \u2208 {e1 , e2 , e3 }. Unless stated otherwise, M -edge regions will refer to the regions\nconstructed by joining M to the vertices by straight lines. In particular, we use a center of T (Y3 ) for the\nstarting point M as the edge regions. See Figure 1 (right) with M = MC . We can also consider the points\nin RM (e) to be \"closer\" to e than to the other edges. Furthermore, it is reasonable to require that the area\n5\n\n\fof the region RM (e) gets larger as d(M, e) increases. Moreover, in higher dimensions, the corresponding\nregions are called \"face regions\".\nEdge regions for incenter, center of mass, and orthocenter are investigated in (Ceyhan (2009)).\n\n4\n\n\u03931 -Regions for Proximity Maps and the Related Concepts\n\nFor any set B \u2286 \u03a9, the \u03931 -region of B associated with\n\u0001 N (*), is defined to be the region \u03931 (B, N ) := {z \u2208\n{x},\nN\nas \u03931 (x, N ). Note that \u03931 -region is based on the\n\u03a9 : B \u2286 N (z)}. For x \u2208 \u03a9, we\ndenote\n\u0393\n1\n\b\nproximity region N (*). If Xn = X1 , X2 , . . . , Xn is a set of \u03a9-valued random variables, then \u03931 (Xi , N ),\ni = 1, 2, . . . , n are random sets. If the Xi are independent and identically distributed, then so are the random\nsets \u03931 (Xi , N ). Additionally, \u03931 (Xn , N ) is also a random set.\nIn a digraph D = (V, A), a vertex v \u2208 V dominates itself and all vertices of the form {u : (v, u) \u2208 A}. A\ndominating set SD for the digraph D is a subset of V such that each vertex v \u2208 V is dominated by a vertex in\n\u2217\nSD . A minimum dominating set SD\nis a dominating set of minimum cardinality and the domination number\n\u2217\n\u03b3(D) is defined as \u03b3(D) := |SD | (see, e.g., Lee (1998)) where | * | denotes the set cardinality functional.\niid\n\nFor X1 , X2 , . . . , Xn \u223c F the domination number of the associated data-random PCD, denoted \u03b3n (N ), is\nthe minimum number of points that dominate all points in Xn . Note that, \u03b3n (N ) = 1 iff Xn \u2229\u03931 (Xn , N ) 6= \u2205.\nHence the name \u03931 -region. Suppose \u03bc is a measure on \u03a9. Following are some general results about \u03931 (*, N ).\nProposition 4.1. For any proximity map N and set B \u2286 \u03a9, RS (N ) \u2286 \u03931 (B, N ).\nProof: For x \u2208 RS (N ), N (x) = \u03a9, so B \u2286 N (x) since B \u2286 \u03a9. Then x \u2208 \u03931 (B, N ), hence RS (N ) \u2286\n\u03931 (B, N ). \u0004\nT\nLemma 4.2. For any proximity map N and B \u2286 \u03a9, \u03931 (B, N ) = x\u2208B \u03931 (x, N ).\n\nProof: Given a proximity map N and\nT subset B \u2286 \u03a9, y \u2208 \u03931 (B, N ) iff B \u2286 N (y) iff x \u2208 N (y) for all x \u2208 B\niff y \u2208 \u03931 (x, N ) for all x \u2208 B iff y \u2208 x\u2208B \u03931 (x, N ). Hence the result follows. \u0004\n\b\nCorollary 4.3. For anyTproximity map N and a realization Xn = x1 , x2 , . . . , xn from F with support\nn\nS(F ) \u2286 \u03a9, \u03931 (Xn , N ) = i=1 \u03931 (xi , N ).\nA problem of interest is finding, if possible, a subset of B, say G \u2286 B, such that \u03931 (B, N ) =\nThis implies that only the points in G are used in determining \u03931 (B, N ).\n\nT\n\nx\u2208G \u03931 (x, N ).\n\nDefinition 4.4. An active\nT set of points SA (B) \u2286 \u03a9 for determining \u03931 (B, N ) is defined to be a subset of\nB such that \u03931 (B, N ) = x\u2208SA (B) \u03931 (x, N ). \u0003\n\nThis definition allows B to be an active set, which always holds by Lemma 4.2. If B is a set of finitely\nmany points, so is the associated active set. Among the active sets, we seek an active set of minimum\ncardinality.\n\nDefinition 4.5. Let B be a set of finitely many points. An active subset of B \u2282 \u03a9 is called a minimal active\nsubset, denoted S\u03bc (B), if there is no other active subset SA of B such that SA (B) ( S\u03bc (B). The minimum\ncardinality among the active subsets of B is called the \u03b7-value and denoted as \u03b7(B, N ). An active subset of\ncardinality \u03b7(B, N ) is called a minimum active subset denoted as SM (B); that is, \u03b7(B, N ) := |SM (B)|. \u0003\nNote that Definitions 4.4 and 4.5 can be extended for any subset B \u2286 \u03a9, in a similar fashion. Moreover,\na minimal active set of minimum cardinality is a minimum active set. We will suppress the dependence on B\nfor SA (B), S\u03bc (B), and SM (B) if there is no ambiguity. In particular, if B = Xn is a set of \u03a9-valued random\nvariables, then SA and SM are random sets and \u03b7n (N ) is a random quantity.\n6\n\n\fFor example, in R with Y2 = {0, 1}, and Xn a random\n\u0012 sample (i.e.,\u0013set of iid random variables) of size\nXn:n 1 + X1:n\nn > 1 from F whose support is in (0, 1), \u03931 (Xn , NS ) =\n, where Xi:n is the ith largest value\n,\n2\n2\nin Xn . \bSo the extrema (minimum and maximum) of the set Xn are sufficient to determine the \u03931 -region; i.e.,\nSM = X1:n , Xn:n . Then \u03b7n (NS ) = 1 + I(n > 1) a.s. for Xn being a random sample from a continuous\ndistribution with support in (0, 1).\n\nIn the multidimensional case there is no natural extension of ordering that yields natural extrema such\nas minimum or maximum. Some extensions of ordering are proposed under the title of \"statistical depth\"\n(see for example Liu et al. (1999)) which is not pursued here. To get the minimum active sets associated\nwith our proximity maps, we will resort to some other type of extrema, such as, the closest points to edges\nor vertices in T (Y3 ).\nFor any proximity map N and Xn , \u03b7n (N ) \u2264 n follows trivially, since\n(\n)\n\\\n\u03931 (Z) .\n\u03b7n (N ) := min |A| : \u03931 (Xn , N ) =\nA\u2286Xn\n\nZ\u2208A\n\nLemma 4.6. Given a sequence of \u03a9-valued random variables X1 , X2 , . . . from distribution F , let X (n) :=\nX (n \u2212 1) \u222a {Xn } for n = 0, 1, 2, . . . with X (0) := \u2205. Then \u03931 (X (n), N ) is non-increasing in n in the sense\nthat \u03931 (X (n + 1), N ) \u2286 \u03931 (X (n), N ).\n\b\nProof: Given a particular\ntype of proximity map N and a data set X (n) = X1 , X2 , . . . , Xn , by Lemma\nTn\n4.2, \u03931 (X (n), N ) = i=1 \u03931 (Xi , N ) and by definition, X (n + 1) = X (n) \u222a {Xn+1 }. So,\n\u03931 (X (n + 1), N ) =\n\nn+1\n\\\n\n\u03931 (Xi , N ) =\n\ni=1\n\n\"\n\nn\n\\\n\n#\n\n\u03931 (Xi , N )\n\ni=1\n\n\\\n\n\u03931 (Xn+1 , N ) = \u03931 (X (n), N ) \u2229 \u03931 (Xn+1 , N ) \u2286 \u03931 (X (n), N ) .\n\nThus we have shown that \u03931 (X (n), N ) is non-increasing in n; i.e., \u03931 (X (n + 1), N ) \u2286 \u03931 (X (n), N ). \u0004\n\b\nRemark 4.7. By monotone sequential continuity from above (Billingsley (1995)), the sequence \u03931 (X (n), N )\nhas a limit\nG1\n\n:=\n\n\u221e\n\\\n\n\u03931 (X (j), N ) = lim\n\nm\u2192\u221e\n\nj=1\n\n=\n\nlim\n\nm\u2192\u221e\n\nm\n\\\n\ni=1\n\n\u03931 (Xi , N ) =\n\nm\n\\\n\n\u03931 (X (j), N ) = lim \u03931 (X (m), N )\nm\u2192\u221e\n\nj=1\n\u221e\n\\\n\n(1)\n\n\u03931 (Xi , N ) . \u0003\n\ni=1\n\nTheorem 4.8. Given a sequence of random variables X1 , X2 , . . . which are identically distributed as F on\n\u03a9, let X (n) := X (n \u2212 1) \u222a {Xn } with X (0) := \u2205. Then \u03931 (X (n), N ) \u2193 RS (N ), as n \u2192 \u221e a.s. in the sense\nthat \u03931 (X (n + 1), N ) \u2286 \u03931 (X (n), N ) and \u03bc(\u03931 (X (n), N ) \\ RS (N )) \u2193 0 a.s.\nProof: By Lemma 4.6, and by monotone sequential continuity from above, {\u03931 (X (n), N )}\u221e\nn=1 has a limit,\nnamely, G1 in Equation (1). We claim that G1 = RS (N ) a.s.\nSuppose RS (N ) ( \u03a9, since if RS (N ) = \u03a9 then N (x) = \u03a9 for all x \u2208 \u03a9, so \u03931 (X (n), N ) = \u03a9 for\nall x hence the result would follow trivially. Since RS (N ) \u2286 \u03931 (X (n), N ) for all n, RS (N ) \u2286 G1 . From\n\u0001\na.s.\nProposition 5.6. in Karr (1992), Yn \u2212\u2212\u2192 Y iff \u2200 \u03b5 > 0, limn\u2192\u221e P supk\u2265n |Yk \u2212 Y | > \u03b5 = 0. Let \u03b5 > 0.\nThen\n!\n!\n!\n!\nn\nk\n\\\n\\\n\u03931 (Xi , N ) \\ RS (N ) \u2264 \u03b5 \u2192 1\n\u03931 (Xi , N ) \\ RS (N ) \u2264 \u03b5 = P \u03bc\nP sup \u03bc\nk\u2265n\n\ni=1\n\ni=1\n\n7\n\n\u221e\nn=1\n\n\fas n \u2192 \u221e, because if \u03931 (X (n), N ) \\ RS (N ) had positive measure, then for each y \u2208 \u03931 (X (n), N ) \\ RS (N ),\n\u03a9 \\ N (y) will contain data points from X (k) with positive probability for sufficiently large k \u2265 n. So y can\nnot be in \u03931 (X (n), N ), which is a contradiction. Hence the desired result follows. \u0004\nNote however that \u03931 (Xn , N ) is neither strictly decreasing nor non-increasing provided that RS (N ) 6= \u03a9\nfor all Xn , because we might have \u03931 (Xn , N ) ( \u03931 (Xm , N ) for some m > n. Nevertheless, the following two\nresults hold.\nProposition 4.9. Suppose \u03a9 \\ RS (N ) has positive measure. For positive integers m > n, let Xn } and Xm\nbe two samples from F on \u03a9. Then \u03bc(\u03931 (Xm , N )) \u2264ST \u03bc(\u03931 (Xn , N )).\nProof: Recall that for X \u223c F and Y \u223c G, X \u2264ST Y if F (x) \u2265 G(x) for all x with strict inequality holding\nfor at least one x.\nLet m > n and Xn and Xm be two samples from F . Then \u03bc(\u03931 (Xm , N )) is more often smaller than\n\u03bc(\u03931 (Xn , N )). Hence P [\u03bc(\u03931 (Xm , N )) \u2264 \u03bc(\u03931 (Xn , N ))] \u2265 1/2 which only shows stochastic precedence\n(Boland et al. (2004)).\nNow, let t \u2208 (\u03bc(RS (N )), \u03bc(\u03a9)), then \u03bc(\u03931 (Xm , N )) \u2264 t happens more often than \u03bc(\u03931 (Xn , N )) \u2264 t,\nhence P (\u03bc(\u03931 (Xm , N )) \u2264 t) \u2265 P (\u03bc(\u03931 (Xn , N )) \u2264 t); that is, Fm (t) \u2265 Fn (t), where Fi (*) is the distribution\nfunction for \u03bc(\u03931 (Xi , N )) for i = m, n. For t < \u03bc(RS (N )) or t > \u03bc(\u03a9), Fi (t) = 0 for i = m, n. Letting\nNn := |Xn \\ RS (N )| and Nm := |Xm \\ RS (N )|, then P (Nn 6= Nm ) > 0 since \u03bc(\u03a9 \\ RS (N )) > 0. In fact,\nP (Nm > Nn ) \u2265 1/2. But if Fm (t) = Fn (t) for all t were the case, then P (Nn = Nm ) = 1 would hold, which\nis a contradiction. \u0004\nTheorem 4.10. Let {Xn }\u221e\nn=1 be a sequence of samples of size n from distribution F with support on \u03a9.\np\np\nThen \u03931 (Xn , N ) \u2212\u2192 RS (N ) in the sense that \u03bc(\u03931 (Xn , N ) \\ RS (N )) \u2212\u2192 0 as n \u2192 \u221e.\nProof: Given a sequence {Xn }\u221e\nn=1 as in the theorem. By Proposition 4.1, RS (N ) \u2286 \u03931 (Xn , N ) for each n. If\n\u03931 (Xn , N )\\RS (N ) has zero measure as n \u2192 \u221e then result follows trivially. Otherwise, if \u03931 (Xn , N )\\RS (N )\nhad positive measure in the limit, for each y \u2208 limn\u2192\u221e \u03931 (Xn , N ) \\ RS (N ), \u03a9 \\ N (y) would have positive\nmeasure with positive probability, then Xn \u2229 [\u03a9 \\ N (y)] 6= \u2205 with positive probability for sufficiently large n,\nthen y \u2208\n/ \u03931 (Xn , N ), which is a contradiction. \u0004\nT\nSince \u03931 (Xn , N ) = ni=1 \u03931 (Xi , N ) for a given realization of the data set Xn , first we describe the region\n\u03931 (X, N ) for X \u2208 Xn , and then describe the region \u03931 (Xn , N ).\nTheorem 4.11. If the superset region for any type of proximity map N has positive measure (i.e., \u03bc(RS (N )) >\n0), then P (\u03b3n (N ) = 1) \u2192 1 as n \u2192 \u221e.\nProof: Notice that if there is at least one data point in RS (N ) then \u03b3n (N ) = 1, because any point\nx \u2208 RS (N ) will have N (x) = \u03a9, so P (there is at least 1 point in R\n\u0012S (N )) \u2264 P (\u03b3n (N )\u0013n= 1). Now,\n\u03bc(\u03a9) \u2212 \u03bc(RS (N ))\n, which goes\nP (there is at least 1 point in RS (N )) = 1 \u2212 P (Xn \u2229 RS = \u2205) = 1 \u2212\n\u03bc(\u03a9)\nto 1 as n \u2192 \u221e. Hence P (\u03b3n (N ) = 1) \u2192 1 as n \u2192 \u221e. \u0004\nThe relative arc density of a digraph D = (V, A) of order |V| = n, denoted as \u03c1(D), is defined as\n|A|\n\u03c1(D) =\nwhere | * | denotes the cardinality of sets (Janson et al. (2000)). Thus \u03c1(D) represents the\nn(n \u2212 1)\nratio of the number of arcs in the digraph D to the number of arcs in the complete symmetric digraph of\norder n, which is n(n \u2212 1).\nTheorem 4.12. If support of the joint distribution of Xn is subset of the superset region for any type of\nproximity map, then relative arc density \u03c1(Xn ) = 1 a.s.\nProof: Suppose the support of Xn is a subset of the superset region, then the corresponding digraph is\ncomplete with n(n \u2212 1) arcs. Hence the relative arc density is 1 with probability 1. \u0004\n8\n\n\f5\n\nTransformations Preserving Uniformity on Triangles in R2\n\nThe proximity regions and hence the corresponding PCDs are based on the Delaunay tessellation of Ym ,\nwhich partitions CH (Ym ). So, suppose the set Xn is a set of iid uniform random variables on the convex\nhull of Ym ; i.e., a random sample from U(CH (Ym )). In particular, conditional on |Xn \u2229 Ti | > 0 being fixed,\nXn \u2229 Ti will also be a set of iid uniform random variables on Ti for i \u2208 {1, 2, . . . , J}, where Ti is the ith\nDelaunay cell and J is the total number of Delaunay cells. Reducing the cell (triangle in R2 ) Ti as much\nas possible while preserving uniformity and the probabilities related to PCDs will simplify the notation and\ncalculations. For simplicity we consider R2 only.\nLet Y3 = {y1 , y2 , y3 } \u2282 R2 be three non-collinear points and T (Y3 ) be the triangle (including the interior)\niid\n\nwith vertices y1 , y2 , y3 . Let Xi \u223c U(T (Y3 )), the uniform distribution on T (Y3 ), for i = 1, 2, . . . , n. The\nprobability density function (pdf) of U(T (Y3 )) is\nf (u) =\n\n1\nI(u \u2208 T (Y3 )),\nA(T (Y3 ))\n\nwhere A(*) is the area functional.\nThe triangle T (Y3 ) can be carried into the first quadrant by a composition of transformations (scaling,\ntranslation, rotation, and reflection) in such a way that the largest edge has unit length and lies on the\nx-axis, and the x-coordinate of the vertex nonadjacent to largest edge is less than\u00011/2. We call the resultant\ntriangle the basic triangle and denote it as Tb where Tb = (0, 0), (1, 0), (c1 , c2 ) with 0 < c1 \u2264 1/2, and\nc2 > 0 and (1 \u2212 c1 )2 + c22 \u2264 1. The transformation from any triangle to Tb is denoted by \u03c6b . See Ceyhan\n(2009) for a detailed description of \u03c6b . Notice that T (Y3 ) is transformed into Tb , then T (Y3 ) is similar to\niid\nTb and \u03c6b (T (Y3 )) = Tb . Thus the random variables Xi \u223c U(T (Y3 )) transformed along with T (Y3 ) in the\niid\n\ndescribed fashion by \u03c6b satisfy \u03c6b (Xi ) \u223c U(Tb ). So, without loss of generality, we can assume T (Y3 ) to be\nTb for uniform data. The functional form of Tb is\n\b\nTb = (x, y) \u2208 R2 : y \u2265 0; y \u2264 (c2 x)/c1 ; y \u2264 c2 (1 \u2212 x)/(1 \u2212 c1 ) .\n\nThere are other transformations that preserve uniformity of the random variable, but not similarity\nof the triangles. We only\u221adescribe\n\u0001\u0001 the transformation that maps Tb to the standard equilateral triangle,\nTe = T (0, 0), (1, 0), 1/2, 3/2 for exploiting the symmetry in calculations using Te .\n\u221a\n3\n1 \u2212 2 c1\ny. Then y1 is mapped to (0, 0),\ny and v(x, y) =\nLet \u03c6e : (x, y) \u2192 (u, v), where u(x, y) = x + \u221a\n2\nc\n3\n2\n\u221a \u0001\ny2 is mapped to (1, 0), and y3 is mapped to 1/2, 3/2 . See also Figure 2.\n\u0001\n(1 \u2212 2 c1 )\n\u221a\nv and\nNote that the inverse transformation is \u03c6\u22121\ne (u, v) = x(u, v), y(u, v) where x(u, v) = u \u2212\n3\n2 c2\ny(u, v) = \u221a u. Then the Jacobian is given by\n3\nJ(x, y) =\n\n\u2202x\n\u2202u\n\n\u2202x\n\u2202v\n\n\u2202y\n\u2202u\n\n\u2202y\n\u2202v\n\n=\n\n1\n\n2 c\u221a\n1 \u22121\n3\n\n0\n\n2\u221ac2\n3\n\n2 c2\n= \u221a .\n3\n\n\u0001\n4\nSo fU,V (u, v) = fX,Y (\u03c6e\u22121 (u, v)) |J| = \u221a I (u, v) \u2208 Te . Hence uniformity is preserved.\n3\nRemark 5.1. The probabilities for uniform data in T (Y3 ) involve the ratio of the event region to A(T (Y3 )).\nIf such ratios is not preserved under \u03c6e , then the probability content for N depends on the geometry of\nT (Y3 ). In particular, the probability content for uniform data for NS and NAS depend on the geometry of\nthe triangle, hence is not geometry invariant (Ceyhan (2009)). For example, P (X \u2208 NS (Y )) and P (X \u2208\nNAS (Y, M )) depends on (c1 , c2 ), hence one has to do the computations for all of (uncountably many) of\nthese triangles. Hence we do not investigate NS and NAS further in this article. \u0003\n9\n\n\f\u03c6e(y3)\ny3 = (c1, c2)\ny\n\nv\n\n\u03c6e(x, y)\n\n(x, y)\n\ny1 = (0, 0)\n\ny2 = (1, 0)\n\n\u03c6e(y1) = (0, 0)\n\nx\n\n\u03c6e(y2) = (1, 0)\nu\n\nFigure 2: The description of \u03c6e (x, y) for (x, y) \u2208 Tb (left) and the equilateral triangle \u03c6e (Tb ) = Te (right).\n\n6\n\nSample Proximity Maps in Rd\n\nLet Ym = {y1 , y2 , . . . , ym } be m points in general position in Rd and Ti be the ith Delaunay cell for i =\n1, 2, . . . , J, where J is the number of Delaunay cells. Let Xn be a random sample from a distribution F in\nRd with support S(F ) \u2286 CH (Ym ).\n\nIn particular, for illustrative purposes, we focus on R2 , where a Delaunay tessellation is a triangulation,\nprovided that no more than three points in Ym are cocircular. Furthermore, for simplicity, let Y3 = {y1 , y2 , y3 }\nbe three non-collinear points in R2 and T (Y3 ) = T (y1 , y2 , y3 ) be the triangle with vertices Y3 . Let Xn be\na random sample from F with support S(F ) \u2286 T (Y3 ). In this section, we will describe two families of\ntriangular proximity regions.\n\n6.1\n\nProportional-Edge Proximity Maps\n\nThe first type of triangular proximity map we introduce is the proportional-edge proximity map. For this\nproximity map, the asymptotic distribution of domination number and the relative density of the corresponding PCD will have mathematical tractability.\nFor r \u2208 [1, \u221e], define NPr E (*, M ) := N (*, M ; r, Y3 ) to be the proportional-edge proximity map with M vertex regions as follows (see also Figure 3 with M = MC and r = 2). For x \u2208 T (Y3 ) \\ Y3 , let v(x) \u2208 Y3 be\nthe vertex whose region contains x; i.e., x \u2208 RM (v(x)). If x falls on the boundary of two M -vertex regions,\nwe assign v(x) arbitrarily. Let e(x) be the edge of T (Y3 ) opposite v(x). Let l(v(x), x) be the line parallel\nto e(x) through x. Let d(v(x), l(v(x), x)) be the Euclidean (perpendicular) distance from v(x) to l(v(x), x).\nFor r \u2208 [1, \u221e), let lr (v(x), x) be the line parallel to e(x) such that\nd(v(x), lr (v(x), x)) = r d(v(x), l(v(x), x)) and d(l(v(x), x), lr (v(x), x)) < d(v(x), lr (v(x), x)).\nLet Tr (x) be the triangle similar to and with the same orientation as T (Y3 ) having v(x) as a vertex and\nlr (v(x), x) as the opposite edge. Then the proportional-edge proximity region NPr E (x, M ) is defined to be\nTr (x) \u2229 T (Y3 ). Notice that l(v(x), x) divides the edges of Tr (x) (other than lr (v(x), x)) proportionally with\nthe factor r. Hence the name proportional edge proximity region.\nNotice that r \u2265 1 implies x \u2208 NPr E (x, M ). Furthermore, limr\u2192\u221e NPr E (x, M ) = T (Y3 ) for all x \u2208\nT (Y3 ) \\ Y3 , so we define NP\u221eE (x, M ) = T (Y3 ) for all such x. For x \u2208 Y3 , we define NPr E (x, M ) = {x} for all\nr \u2208 [1, \u221e].\niid\n\nNotice that Xi \u223c F , with the additional assumption that the non-degenerate two-dimensional probability\ndensity function f exists with support S(F ) \u2286 T (Y3 ), implies that the special case in the construction of\n10\n\n\fNPr E - X falls on the boundary of two vertex regions - occurs with probability zero. Note that for such\nan F , NPr E (X) is a triangle a.s.\ny3\n\n(v\nl2\n(x\n),\nx)\nx\ne(\n)\n\n(x\n\n),\n\nx)\n\nx)\nv(\n\n(v\n\nx)\n\n,l\n\n(x\n\n(v\n\n),\n\n(x\n\nl(\n\n),\n\nx)\n\n)\n\nMC\n\n,x\n\n))\n\nl (v\n\n2d\nv(\n\nx)\n\n,x\n\n))\n\nd(\n\n=\n\nv(\n\nx\n\nx)\n\n,l\n\n2(\n\ny2\n\nd(\n\nv(\n\ny1 = v(x)\n\nFigure 3: Construction of proportional edge proximity region, NP2 E (x) (shaded region).\nThe functional form of NPr E (x, M )\u221afor x = (x0 , y0 ) \u2208 Tb\u221ais given in Ceyhan (2009). Of particular interest\nis NPr E (x, M ) with any M and r \u2208 { 2, 3/2, 2}. For r = 2, l(v(x), x) divides T\u221a2 (x) into two regions of\n\u221a\n\nequal area, hence NP E2 (x, M ) is also referred to as double-area proximity region. For r = 2, l(v(x), x) divides\nthe edges of T2 (x) -other than lr (v(x), x) - into two segments of equal length, hence NP2 E (x, M ) is also\nreferred to as double-edge proximity region. For r < 3/2, RS (NPr E , MC ) = \u2205, and for r > 3/2, RS (NPr E , MC )\nhas positive area; for r = 3/2, RS (NPr E , MC ) = {MC }. Therefore, r = 3/2 is the threshold for the superset\nregion to be nonempty. Furthermore, r = 3/2 will be the value at which the asymptotic distribution of the\ndomination number of the PCD based on NPr E (*, MC ) is nondegenerate (Ceyhan and Priebe (2005)).\nLet RS\u22a5 (NPr E , M ) be the superset region for NPr E based\u0001 on M -vertex regions with orthogonal projections.\nThen the superset region with the incenter RS\u22a5 NP2 E , MI is as in Figure 4 (right). Let Mi be the midpoint\nof edge ei for i = 1, 2, 3. Then T (M1 , M2 , M3 ) \u2286 RS\u22a5 (NP2 E , MI ) for all T (Y3 ) with equality holding when\nT (Y3 ) is an equilateral triangle.\n\u0001\n2\nFor NP2 E (*, MC ) constructed using\nthe\nmedian\nlines\nR\nN\n,\nM\n= T (M1 , M2 , M3 ), and for NP2 E (*, MC )\nS\nC\nP\nE\nPSfrag replacements\n\u0001\n\u22a5\n2\nconstructed by the orthogonal projections, RS NP E , MC \u2287 T (M1 , M2 , M3 ) with equality holding when\n0.7\n\ny3\n\n0.7\n\nMCC\n\n0.6\n\nPSfrag replacements\n\ny3\n\n0.6\n\n0.5\n\n0.5\n\n0.4\n\n0.4\n\n00IC\n11\n11\n00\nP1\n00\n11\n\nT2\n\nM2\n\n11\n00\nT3\n11\n00\nP IC\n\nM2 2\n\nM1\n\n0.3\n\n0.3\n\n0.2\n\n0.2\n\nQ1\n\nM1\n\nMI\nMCC\n\n0.1\n\n0.1\n\ny1\n0\n\n0.2\n\n0.4\n\nM30.6\nx\n\ny2\n0.8\n\nIC\n\ny1\n\n1\n\n0.2\n\nP3\n00\n11\n\n00\n0.411\nT1\n\nM3 0.6\nx\n\ny2\n0.8\n\n1\n\n\u0001\nFigure 4: Superset region RS NP2 E , MCC in an acute triangle (left), superset region, RS\u22a5 (NP2 E , MI ) (right)\n11\n\n\fPSfrag replacements\n\n0.7\n\ny3\n\nx 0.6\n0.5\n\n0.4\n\n0.3\n\nMI\n0.2\n\nMCC\n\n0.1\n\ny2\n\ny1\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\nFigure 5: The triangle T r=\n\n\u221a\n2\n\n1\n\n.\n\nT (Y3 ) is an equilateral triangle.\nIn T (Y3 ), drawing the lines \u03bei (r, x) such that\u221ad(yi , ei ) = r d(\u03bei (r, x), yi ) for j \u2208 {1, 2, 3} yields a triangle,\nT r , for r < 3/2 . See Figure 5 for T r with r = 2.\nThe functional form of T r in Tb is\nff\n\uf6be\nc2 (1 \u2212 r x)\nc2 (r (x \u2212 1) + 1)\nc2 (r \u2212 1)\n; y\u2264\n; y\u2264\n(x, y) \u2208 Tb : y \u2265\nr\nr (1 \u2212 c1 )\nr c1\n\u00ab \u201e\n\u00ab \u201e\n\u00ab!\n\u201e\n2 \u2212 r + c1 (r \u2212 1) c2 (r \u2212 1)\nc1 (2 \u2212 r) + r \u2212 1 c2 (r \u2212 2)\n(r \u2212 1) (1 + c1 ) c2 (r \u2212 1)\n.\n=T\n,\n,\n,\n,\n,\nr\nr\nr\nr\nr\nr\n\nTr=\n\n(2)\n\nThere is a crucial difference between T r and T (M1 , M2 , M3 ): T (M1 , M2 , M3 ) \u2286 RS (NPr E , M ) for all M\nand r \u2265 2, but (T r )o and RS (NPr E , M ) are disjoint for all M and r.\n\nSo if M \u2208 (T r )o , then RS (NPr E , M ) = \u2205; if M \u2208 \u2202(T r ), then RS (NPr E , M ) = {M }; and if M 6\u2208 T r ,\nthen RS (NPr E , M ) has positive area. The triangle T r defined above plays a crucial role in the analysis of\nthe distribution of the domination number of the PCD based on proportional-edge proximity maps. The\nsuperset region RS (NPr E , M ) will be important for both the domination number and the relative density of\nthe corresponding PCDs.\nThe functional forms of the superset region, RS (NPr E , M ), and T (M1 , M2 , M3 ) in Tb are provided in\nCeyhan (2009).\no\n\nNPr E (*, M ) is geometry invariant if M -vertex regions are constructed with M \u2208 T (Y3 ) by using the\nextensions of the line segments joining y to M for all y \u2208 Y3 . But when the vertex regions are constructed\nby orthogonal projections, NPr E (*, M ) is not geometry invariant (Ceyhan (2009)), hence such vertex regions\nare not considered here.\n6.1.1\n\nExtension of NPr E to Higher Dimensions\n\nThe extension to Rd for d > 2 is straightforward. The extension with M = MC is given her, but the\nextension for general M is similar. Let Yd+1 = {y1 , y2 , . . . , yd+1 } be d + 1 points that do not lie on the same\nd \u2212 1-dimensional hyperplane. Denote the simplex formed by these d + 1 points as S(Yd+1 ). A simplex is\nthe simplest polytope in Rd having d + 1 vertices, d (d + 1)/2 edges and d + 1 faces of dimension (d \u2212 1). For\nr \u2208 [1, \u221e], define the proximity map as follows. Given a point x in S(Yd+1 ), let v := argminy\u2208Yd+1 V(Qy (x))\nwhere Qy (x) is the polytope with vertices being the d (d + 1)/2 midpoints of the edges, the vertex v and\nx and V(*) is the d-dimensional volume functional. That is, the vertex region for vertex v is the polytope\nwith vertices given by v and the midpoints of the edges. Let v(x) be the vertex in whose region x falls. If\n12\n\n\fx falls on the boundary of two vertex regions, v(x) is assigned arbitrarily. Let \u03c6(x) be the face opposite\nto vertex v(x), and \u03a5(v(x), x) be the hyperplane parallel to \u03c6(x) which contains x. Let d(v(x), \u03a5(v(x), x))\nbe the (perpendicular) Euclidean distance from v(x) to \u03a5(v(x), x). For r \u2208 [1, \u221e), let \u03a5r (v(x), x) be the\nhyperplane parallel to \u03c6(x) such that\nd(v(x), \u03a5r (v(x), x)) = r d(v(x), \u03a5(v(x), x)) and d(\u03a5(v(x), x), \u03a5r (v(x), x)) < d(v(x), \u03a5r (v(x), x)).\nLet Sr (x) be the polytope similar to and with the same orientation as S having v(x) as a vertex and\n\u03a5r (v(x), x) as the opposite face. Then the proximity region NPr E (x, MC ) := Sr (x) \u2229 S(Yd+1 ). Notice that\nr \u2265 1 implies x \u2208 NPr E (x, MC ).\n6.1.2\n\n\u03931 -Regions for Proportional-Edge Proximity Maps\n\nFor NPr E (*, M ), the \u03931 -region is constructed as follows; see also Figure 6. Let \u03bei (r, x) be the line parallel to\nei such that \u03bei (r, x) \u2229 T (Y3 ) 6= \u2205 and r d(yi , \u03bei (r, x)) = d(yi , l(yi , x)) for i \u2208 {1, 2, 3}. Then\n\u03931 (x, NPr E , M ) =\n\n3\n[\n\u0002\n\u0003\n\u03931 (x, NPr E , M ) \u2229 RM (yi )\n\ni=1\n\nwhere\n\u03931 (x, NPr E , M ) \u2229 RM (yi ) = {z \u2208 RM (yi ) : d(yi , l(yi , z)) \u2265 d(yi , \u03bei (r, x)} for i \u2208 {1, 2, 3}.\nNotice that r \u2265 1 implies x \u2208 \u03931 (x, NPr E , M ). Furthermore, limr\u2192\u221e \u03931 (x, NPr E , M ) = T (Y3 ) for all\nr\nx \u2208 T (Y3 ) \\ Y3 and so we define \u03931 (x, NPr=\u221e\nE , M ) = T (Y3 ) for all such x. For x \u2208 Y3 , \u03931 (x, NP E , M ) = {x}\nfor all r \u2208 [1, \u221e].\ny3\n\n(2\n1,\n\n\u03be1\n\nx)\n\n)=\n\nrd\n\n(y\n\n1,\n\n,x\n\n))\n\n\u03be3(2, x)\nl (y\n\n\u03be2 (2\n\n(y\n\n1,\n\n, x)\n\nx)\n\nMC\n\nx)\n\n)\n\nd(\n\ny1\n\n,l\n\nx\n\n2,\n\ny2\n\n1(\n\n,\u03be\nd(\n\ny1\n\ny1\n\n\u03be1\n\n(2\n\n,x\n\n)\n\n\u0001\nFigure 6: Construction of the \u03931 -region, \u03931 x, NPr=2\nE , MC (shaded region).\n\nThe functional form of \u03931 (x = (x0 , y0 ), NPr E , M ) in the basic triangle Tb is given by\n\u03931 (x = (x0 , y0 ), NPr E , M ) =\n\n3\n[\n\ni=1\n\n[\u03931 (x = (x0 , y0 ), NPr E , M ) \u2229 RM (yi )]\n\n13\n\n\fPSfrag replacements\n\nPSfrag replacements\ny3\n\n0.7\n\nB3\nB1\n\n0.6\n\n0.7\n\ny3\n\n0.6\n\nB2\n0.5\n\n0.5\n\nx\nB3\n\n0.4\n\n \u0301\n`\n2\n\u03931 x, NP\nE , MCC \u2229 RCC (y3 )\nMCC\n\nx\n\n \u0301\n`\n\u22a5\n2\n\u03931 x, NP\nE , MI \u2229 RIC (y3 )\n\n0.4\n\nB1\nB2\n\n0.3\n\n0.3\n\n0.2\n\n \u0301\n`\n2\n\u03931 x, NP\nE , MCC \u2229 RCC (y3 ) 0.2\nMCC\n\n0.1\n\nMI\n\n0.1\n\ny2\n\ny1\ny1\n\n0\n\n0.2\n\n0.4\n\n0.6\n\ny2\n\n0.8\n\n0\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n\u0001\n\u22a5\n(y3 ) (right).\nFigure 7: \u03931 x, NP2 E , MCC with x \u2208 RCC (y3 ) (left), x \u2208 RIC\nwhere\n\n\u001b\nc2 (r x \u2212 x0 )\ny0\n\u2212\n,\n\u03931 (x =\n\u2229 RM (y1 ) = (x, y) \u2208 RM (y1 ) : y \u2265\nr\n(1 \u2212 c1 ) r\n\u001b\n\u001a\nc2 (r (x \u2212 1) + 1 \u2212 x0\ny0\nr\n,\n\u2212\n\u03931 (x = (x0 , y0 ), NP E , M ) \u2229 RM (y2 ) = (x, y) \u2208 RM (y1 ) : y \u2265\nr\nc1 r\n\u001a\n\u001b\ny0 \u2212 c2 (1 \u2212 r)\n\u03931 (x = (x0 , y0 ), NPr E , M ) \u2229 RM (y3 ) = (x, y) \u2208 RM (y1 ) : y \u2264\n.\nr\n\u001a\n\n(x0 , y0 ), NPr E , M )\n\nNotice that \u03931 (x, NPr E , MC ) is a convex hexagon for all r \u2265 2 and x \u2208 T (Y3 ) \\ Y3 , (since for such an x,\n\u03931 (x, NPr E , MC ) is bounded by \u03bei (r, x) and ei for all i \u2208 {1, 2, 3}, see also Figure 6) else it is either a convex\nhexagon or a non-convex polygon depending on the location of x and the value of r.\nFurthermore, in Rd with d > 2 let \u03b6i (x) be the hyperplane such that \u03b6i (x)\u2229S (Ym ) 6= \u2205 and r d(yi , \u03b6i (x)) =\nd(yi , \u03b7(yi , x)) for i = 1, 2, . . . , (d + 1). Then \u03931 (x, NPr E ) \u2229 RCM (yi ) = {z \u2208 RCM (yi ) : d(yi , \u03b7(yi , z)) \u2265\nSd+1\nd(yi , \u03b6i (x)}, for i = 1, 2, 3. Hence \u03931 (x, NPr E ) = i=1 (\u03931 (x, NPr E ) \u2229 RCM (yi )). Notice that r \u2265 1 implies\nx \u2208 NPr E (x) and x \u2208 \u03931 (x, NPr E ).\n\nSo far, we have described the \u03931 -region for a point in x \u2208 T (Y3 ). For a set Xn of size n in T (Y3 ), the\nregion \u03931 (Xn , NPr E , M ) can be exactly described by the edge extrema.\nDefinition 6.1. The (closest) edge extrema of a set B in T (Y3 ) are the points closest to the edges of T (Y3 ),\ndenoted xei for i \u2208 {1, 2, 3}; that is, xei \u2208 arginf x\u2208B d(x, ei ).\nNote that if B = Xn is a random sample of size n from F then the edge extrema, denoted Xei (n), are\nrandom variables.\nProposition 6.2. Let B be any set of n distinct points in T (Y3 ) and xei \u2208 arginf x\u2208B d(x, ei ). For\nT\nproportional-edge proximity maps with M -vertex regions, \u03931 (B, NPr E , M ) = 3i=1 \u03931 (xei , NPr E , M ).\n\nProof: Given B = {x1 , x2 , . . . , xn } in T (Y3 ). Note that\n\"n\n#\n\\\n\u03931 (B, NPr E , M ) \u2229 RM (yi ) =\n\u03931 (xi , NPr E , M ) \u2229 RM (yi ),\ni=1\n\n\u2032\n\nNPr E (x, M )\n\nand if d(yi , l(yi , x)) \u2264 d(yi , l(yi , x )) then\ndefinition xei \u2208 argmaxx\u2208B d(yi , \u03bei (r, x)), so\n\n\u2286 NPr E (x\u2032 , M ) for all x, x\u2032 \u2208 RM (yi ). Further, by\n\n\u03931 (B, NPr E , M ) \u2229 RM (yi ) = \u03931 (xei , NPr E , M ) \u2229 RM (yi ) for i \u2208 {1, 2, 3}.\n14\n\n\f\u0003\n\u03931 (xei , NPr E , M ) \u2229 RM (yi ) , and\n\uf8ee\n\uf8f9\n3\n\\\n\u0001\n\u03931 (xei , NPr E , M ) \u2229 RM (yi ) = \uf8f0\n\u03931 xej , NPr E , M \uf8fb \u2229 RM (yi ) for i \u2208 {1, 2, 3}.\n\nFurthermore, \u03931 (B, NPr E , M ) =\n\nS3 \u0002\ni=1\n\nj=1\n\nCombining these two results, we obtain \u03931 (B, NPr E , M ) =\n\nT3\n\nj=1\n\n\u0001\n\u03931 xej , NPr E , M . \u0004\n\nFrom the above proposition, we see that the \u03931 -region for B as in the proposition can also be written as\nthe union of three regions of the form\n\u03931 (B, NPr E , M ) \u2229 RM (yi ) = {z \u2208 RM (yi ) : d(yi , l(yi , z)) \u2265 d(yi , \u03bei (r, xei ))} for i \u2208 {1, 2, 3}.\nCorollary 6.3. Let Xn be a random sample from a continuous distribution F on T (Y3 ). For proportionaledge proximity maps with M -vertex regions, \u03b7n (NPr E ) \u2264 3 with equality holding with positive probability for\nn \u2265 3.\nProof: From Proposition 6.2, \u03b7n (NPr E ) \u2264 3. Furthermore, Xe (n) is unique for each edge e a.s. since F is\ncontinuous, and there are three distinct edge extrema with positive probability. Hence P (\u03b7n (NPr E ) = 3) > 0\nfor n \u2265 3. \u0004\nT\nThen \u03931 (Xn , NPr E , M ) = 3i=1 \u03931 (Xei , NPr E ), where ei is the edge opposite vertex yi , for i = 1, 2, 3. So\n\u03931 (Xn , NPr E , M ) \u2229 RM (yi ) = {z \u2208 RM (yi ) : d(yi , l(yi , z)) \u2265 d(yi , \u03bei (r, xei ))}, for i = 1, 2, 3.\n\nNote that P (\u03b7n (NPr E ) = 3) \u2192 1 as n \u2192 \u221e for Xn a random sample from U(T (Y3 )), since edge extrema\nare distinct with probability 1 as n \u2192 \u221e as shown in the following theorem.\nTheorem 6.4. Let Xn be a random sample from U(T (Y3 )) and let Ec,3 (n) be the event that (closest) edge\nextrema are distinct. Then P (Ec,3 (n)) \u2192 1 as n \u2192 \u221e.\nProof: Using the uniformity preserving transformation \u03c6e without loss of generality, one can assume Xn is\na random sample from U(Te ). Observe also that the edge extrema in Tb are mapped into the edge extrema\nin Te . Note that the probability of edge extrema all being equal to each other is P (Xe1 (n) = Xe2 (n) =\nXe3 (n)) = I(n = 1). Let Ec,2 (n) be the event that there are only two distinct (closest) edge extrema. Then\nfor n > 1,\nP (Ec,2 (n)) = P (Xe1 (n) = Xe2 (n)) + P (Xe1 (n) = Xe3 (n)) + P (Xe2 (n) = Xe3 (n))\nsince the intersection of events Xe1 (n) = Xe2 (n)), Xe1 (n) = Xe3 (n), and Xe2 (n) = Xe3 (n) is equivalent to\nthe event Xe1 (n) = Xe2 (n) = Xe3 (n). Notice also that P (Ec,2 (n = 2)) = 1. So, for n > 2, there are two or\nthree distinct edge extrema with probability 1; i.e., P (Ec,3 (n)) + P (Ec,2 (n)) = 1 for n > 2.\nWe will show that P (Ec,2 (n)) \u2192 0 as n \u2192 \u221e, which will imply the desired result.\n\nFirst consider P (Xe1 (n) = Xe2 (n)). The event Xe1 (n) = Xe2 (n) = Xe = (X, Y ) is equivalent to the\nevent that Xn \u2282 {U \u2208 Te : d(y1 , l(y1 , U )) \u2264 d(y1 , l(y1 , Xe )), d(y2 , l(y2 , U )) \u2264 d(y2 , l(y2 , Xe ))}. For example,\nif given Xe1 (n) = Xe2 (n) = (x, y) the remaining n \u2212 1 points will lie in the shaded region in Figure 8 (left).\nFor other pairs of edge extrema, see Figures 8 (right) and 9.\n\u221a \u0001\n\u221a \u0001n\u22121\nThe pdf of such Xe = (X, Y ) is f (x, y) = n 4/ 3 y 2 / 3\n. Let \u03b5 > 0, by Markov's inequality,\n\u221a\n\u0001\n\u0002\u221a\n\u0003\nP\n3/2 \u2212 Y > \u03b5 \u2264 E\n3/2 \u2212 Y /\u03b5. But,\nh\u221a\ni\nE\n3/2 \u2212 Y\n\n=\n\nZ\n\n1/2\n\n0\n\n+\n\nZ\n\n1\n\nZ\n\n1/2\n\n\u221a\n3x\n\n0\n\nZ\n\n0\n\n\u221a\n\n\u0011 \u0010 \u221a \u0011n\u22121 \u0010 \u221a \u0011\n\u0010\u221a\n3/2 \u2212 y n y 2 / 3\n4/ 3 dy dx\n\n3 (1\u2212x)\n\n\u0011n\n\u0010\u221a\n3/4\n= 4\n\n\u0010\u221a\n\u0011 \u0010 \u221a \u0011n\u22121 \u0010 \u221a \u0011\n3/2 \u2212 y n y 2 / 3\n4/ 3 dy dx\n\n1\n,\nn (4 n2 \u2212 1)\n15\n\n\fwhich converges to 0 as n \u2192 \u221e. So if\u221aXe1 (n) = Xe\u00012 (n) were the case, then geometric locus of this point\ngoes to y3 . That is, for each \u03b5 > 0, P\n3/2 \u2212 Y > \u03b5 \u2192 0 as n \u2192 \u221e. Hence P (Xe1 (n) = Xe2 (n) 6= y3 ) \u2192 0\nas n \u2192 \u221e. Furthermore, P (Xe1 (n) = Xe2 (n) = y3 ) \u2264 P (Xe2 (n) \u2208 e2 ) = 0 for all n \u2265 1. So P (Xe1 (n) =\nXe2 (n)) \u2192 0 as n \u2192 \u221e.\nLikewise, by symmetry, it follows that limn\u2192\u221e P (Xe1 (n) = Xe3 (n)) = limn\u2192\u221e P (Xe2 (n) = Xe3 (n)) = 0.\nHence P (Ec,2 (n)) \u2192 0 as n \u2192 \u221e. Thus P (Ec,3 (n)) \u2192 1 as n \u2192 \u221e. \u0004\nThe above theorem implies that the asymptotic distribution of \u03b7n (NPr E ) is degenerate with P (\u03b7n (NPr E ) =\niid\n\n3) \u2192 1 as n \u2192 \u221e. But for finite n, \u03b7n (NPr E ) for Xi \u223c U(T (Y3 )) has the following non-degenerate\ndistribution.\n(\n2\nwp \u03c02 (n)\n\u03b7n (NPr E ) =\n3\nwp \u03c03 (n) = 1 \u2212 \u03c02 (n),\nwhere \u03c02 (n) \u2208 (0, 1) is the probability of edge extrema for any two distinct edges being concurrent.\ny3 = (c1, c2)\n\ny3 = (c1, c2)\n\n(x, y)\n\n(x, y)\n\ny1 = (0, 0)\n\ny2 = (1, 0)\n\ny1 = (0, 0)\n\ny2 = (1, 0)\n\nFigure 8: The figure for Xe1 (n) = Xe2 (n) = (x, y) (left) and Xe2 (n) = Xe3 (n) = (x, y) (right).\nRemark 6.5. If Xn is a random sample from F such that S(F ) \u2229 {x \u2208 T (Y3 ) : d(x, ei ) \u2264 \u03b51 } has positive\nmeasure and S(F ) \u2229 B(yi , \u03b52 ) = \u2205 for some \u03b51 , \u03b52 > 0, then P (Ec,3 (n)) \u2192 1 as n \u2192 \u221e follows trivially.\nHowever, the case that F has positive density around the vertices Y3 requires more work to prove as shown\nbelow. \u0003\nTheorem 6.6. Let Xn be a random sample from F such that B(yi , \u03b5) \u2286 S(F ) for some \u03b5 > 0 and for each\ni = 1, 2, 3, then P (Ec,3 (n)) \u2192 1 as n \u2192 \u221e.\ny3\n\nPSfrag replacements\n(x, y)\n\ny1\n\ny2\n\nFigure 9: The figure for Xe1 (n) = Xe3 (n) = (x, y).\n16\n\n\fProof: Using the transformation \u03c6e : (x, y) \u2192 (u, v), above, we can without loss of generality assume Xn is\na random sample from F with support S(F ) \u2286 Te . After \u03c6e is applied, suppose F becomes Fe , then \u03c6e (Xn )\nbecomes a random sample from Fe such that B(\u03c6e (yi ), \u03b5e ) \u2286 S(Fe ) for some \u03b5e > 0 and for each i = 1, 2, 3.\nFirst consider P (Xe1 = Xe3 ). Given Xe1 = Xe3 = (x, y) the remaining n \u2212 1 points will lie in the shaded\nregion in Figure 8 (right). Xe1 = Xe3 = (X, Y ) is equivalent to the event that\nXn \u2282 SR (X, Y ) := {(U, W ) \u2208 Te : l(y1 , (U, W )) \u2264 l(y1 , (X, Y )), l(y3 , (U, W )) \u2264 l(y3 , (X, Y ))}.\nThe pdf of such (X, Y ) is f (x, y) = n G(x, y)n\u22121 f (x, y) where G(u, v) := PF (X \u2208 SR (u, v)). Note\nthat P (Xe2 = Xe3 = y1 ) = 0 for all n, and Xe2 = Xe3 6= y1 is equivalent to d((X,\n\u0002\u221aY ), y1 ) >\u0003 0.\nLet \u03b5 > 0, by Markov's inequality, P (d((X, Y ), y1 ) > \u03b5) \u2264 E [d((X, Y ), y1 )] /\u03b5 = E\nX 2 + Y 2 /\u03b5.\n\u221a\n2\n2\nSwitching to the polar coordinates as X = R cos \u03b8 and Y = R sin \u03b8, we get X + Y = R. But,\nZ \u03b5 Z \u03c0/3\nE [R] =\nn r G(r, \u03b8)n\u22121 f (r, \u03b8)r dr d\u03b8. Integrand is critical at r = 0, since for r > 0 it converges\n0\n\n0\n\nto zero as n \u2192 \u221e. So we use the Taylor series expansion around r = 0 as\n\n\u0001\n\u2202f (0, \u03b8)\nr + O r2 ,\n\u2202r\n\u0001\n\u0001\n\u2202G(0, \u03b8)\n\u2202G(0, \u03b8)\nG(r, \u03b8) = G(0, \u03b8) +\nr + O r2 = 1 +\nr + O r2 .\n\u2202r\n\u2202r\nf (r, \u03b8) = f (0, \u03b8) +\n\n\u2202G(0, \u03b8)\n< 0, since area of SR (u, v) decreases as r increases for fixed \u03b8. So let r = w/n, then\n\u2202r\n\u0012\n\u0013\n\u0012\n\u0013\nZ n \u03b5 Z \u03c0/3\n\u0001 n\u22121\n\u0001 w\n\u2202G(0, \u03b8) w\n\u2202f (0, \u03b8) w\nw\n1+\nf (0, \u03b8) +\ndw d\u03b8\n+ O n\u22122\n+ O n\u22122\nE [R] \u223c\nn\nn\n\u2202r\nn\n\u2202r\nn\nn2\n0\n0\n\u0013\n\u0012\nZ \u221e Z \u03c0/3\n\u0001\n\u2202G(0, \u03b8)\n1\n2\nw f (0, \u03b8)w dw d\u03b8 = O n\u22122 .\nw exp\n=\nn2 0\n\u2202r\n0\n\nNote that\n\nHence P (Xe2 = Xe3 6= y1 ) \u2192 0 as n \u2192 \u221e. Then P (Xe2 = Xe3 ) \u2192 0 as n \u2192 \u221e.\n\nLikewise, it follows that limn\u2192\u221e P (Xe1 = Xe2 ) = limn\u2192\u221e P (Xe1 = Xe3 ) = 0. Hence P (Ec,2 (n)) \u2192 0 as\nn \u2192 \u221e. Thus P (Ec,3 (n)) \u2192 1 as n \u2192 \u221e. \u0004\n\nNotice that Theorem 6.4 follows as a corollary from Theorem 6.6. For r \u2265 3/2 and M \u2208 R2 \\ Y3 ,\n\u03931 (Xn , NPr E , M ) 6= \u2205 a.s., since RS (NPr E , M ) 6= \u2205 and RS (NPr E , M ) \u2286 \u03931 (Xn , NPr E , M ).\n\nNow, for n > 1, let Xei (n) = xei = (ui , wi ) be given for i \u2208 {1, 2, 3}, be the edge extrema in a given\nrealization of Xn . Then the functional form of \u03931 -region in Tb is given by\n\u03931 (Xn , NPr E , M ) =\n\n3\n[\n\u0002\n\u0003\n\u03931 (Xn , NPr E , M ) \u2229 RM (yi )\n\ni=1\n\nwhere\n\u001a\n\u001b\nw1\nc2 (r x \u2212 u1 )\n\u2229 RM (y1 ) = (x, y) \u2208 RM (y1 ) : y \u2265\n,\n\u2212\nr\n(1 \u2212 c1 ) r\n\u001b\n\u001a\nc2 (r (x \u2212 1) + 1 \u2212 u2\nw2\nr\n,\n\u2212\n\u03931 (Xn , NP E , M ) \u2229 RM (y2 ) = (x, y) \u2208 RM (y1 ) : y \u2265\nr\nc1 r\n\u001a\n\u001b\nw3 \u2212 c2 (1 \u2212 r)\n\u03931 (Xn , NPr E , M ) \u2229 RM (y3 ) = (x, y) \u2208 RM (y1 ) : y \u2264\n.\nr\n\n\u03931 (Xn , NPr E , M )\n\n\u0001\nSee Figure 10 for \u03931 Xn , NP2 E , M with n \u2265 3 where M -vertex regions for M = MCC and M = MI with\northogonal projections are used. Note that only the edge extrema are shown in Figure 10 (right).\n17\n\n\fPSfrag replacements\n\nPSfrag replacements\n\n0.7\n\nQ3\n\ny3\n\nQ1\n\n0.6\n\ny3\n\n0.6\n\nQ2\n\nxe1\n\nxe1\n\n0.5\n\n0.5\n\nxe2\n\nxe2\n\nQ3\n\n0.7\n\nMCC\n\n0.4\n\n0.4\n\n \u0301\n`\n\u22a5\n2\n\u03931 x, NP\nE \u2229 RIC (y3 )\n\nQ1\nQ2\n\n0.3\n`\n \u0301\n2\n(y )\n\u03931 Xn , NP\nE, M \u2229 R\n`\n \u0301 CC 3\n2\n,\nM\n\u2229\nR\n(y\n)\n\u03931 Xn , NP\nCC 3 0.2\nE\n\n0.3\n\n0.2\n\nMI\n\nMCC\n0.1\n\n0.1\n\n0\n\nxe3\n\ny1\nxe3\n\ny1\n0.2\n\ny2\n\n0.4\n\n0.6\n\n0.8\n\n0\n\n0.2\n\n0.4\n\ny2\n0.6\n\n0.8\n\n1\n\n1\n\n\u0001\nM = Mreplacements\nFigure 10: \u03931 Xn , NP2 E , M for PSfrag\nCC (left) and M = MI (right) with three distinct edge extrema.\n0.7\n\nPSfrag replacements\n\n0.7\n\ny3\n\n0.6\n\ny3\n\n0.6\n\n0.5\n\n0.5\n\nx\n\nx\n \u0301\n`\n2\n\u03931 x, NP\nE \u2229 RCC (y3 )\n\n0.4\n\nMCC\n\n0.3\n\n \u0301\n`\n\u22a5\n2\n\u03931 x, NP\nE \u2229 RIC (y3 )\n\n0.4\n\n0.3\n\n0.2\n\nMCC\n\nMI\n\n \u0301\n`\n2\n\u03931 x, NP\nE \u2229 RCC (y3 ) 0.2\n\n0.1\n\n0.1\n\ny2\n\ny1\n0.2\n\n0.4\n\n0.6\n\n0.8\n\ny2\n\ny1\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n\u0011\n\u0010\n\u221a\n\u22a5\n(y3 ).\nFigure 11: \u03931 x, NP E2 , M with x \u2208 RCC (y3 ) (left), x \u2208 RIC\nNote that, for Xn a random sample from U(T (Y3 )), P (\u03b7n (NPr E ) = 3) \u2192 1 as n \u2192 \u221e, since the edge\nextrema are distinct with probability 1 as n \u2192 \u221e. However, for r < 3/2, the region \u03931 (x, NPr E , M )\u2229RM (yi )\nmight be empty for some i \u2208 {1, 2, 3}. Furthermore, if M \u2208 (T r )o (see Equation (2) for T r ) with r <\n3/2, then \u03931 (Xn , NPr E , M ) will be empty with probability 1 as n \u2192 \u221e. In such a case, there is no \u03931 r\nregion\nT to construct. r But the definition of the \u03b7-value still works inr the sense that \u03931 (Xn , NP E , M ) =\n\u2205 = x\u2208SM \u03931 (x, NP E , M ) (see Definition 4.4 for SM ) and \u03931 (x, NP E , M ) 6= \u2205 for all x \u2208 Xn since x \u2208\n\u03931 (x, NPr E , M ). To determine whether the \u03931 -region is empty or not, it suffices to check the intersection of\nthe \u03931 -regions of the edge extrema. If M \u2208\n/ (T r )o , the \u03931 -region is guaranteed to be nonempty.\nd\n\nd\n\nNote that \u03b7n (NPr1E ) = \u03b7n (NPr2E ) for all (r1 , r2 ) \u2208 [1, \u221e)2 , where = stands for \"equality in distribution\".\n\u0010\n\u0011\n\u221a\nSee Figure 12 for \u03931 Xn , NP E2 , M where M -vertex regions for M = MCC and M = MI with orthogonal\nprojections are used.\nRemark 6.7.\n\u2022 For r1 < r2 , \u03931 (x, NPr1E , M ) \u2286 \u03931 (x, NPr2E , M ) for all x \u2208 T (Y3 ) with equality holding only when\nx \u2208 Y3 .\n\u2022 For r1 < r2 , \u03931 (Xn , NPr1E , M ) \u2286 \u03931 (Xn , NPr2E , M ) with equality holding only when Xn \u2286 Y3 or\n\u03931 (Xn , NPr E , M ) = \u2205 for r = r1 , r2 .\n18\n\n\fPSfrag replacements\n\nPSfrag replacements\n\n0.7\n\nQ3\n\ny3\n\nQ1\n\n0.6\n\n0.7\n\ny3\n\n0.6\n\nQ2\n0.5\n\nQ3\n\n0.5\n\nxe3\n\nxe3\n\nxe2\n\nMCC\n\n0.4\n\nxe2\n\n0.4\n\nQ1\nQ2\n\n0.3\n\n0.3\n\n0.2\n\nMCC\nxe1\n\n0.1\n\ny2\n0.2\n\n0.4\n\nxe1\n\n0.1\n\ny1\n\n`\n \u0301\n2\n\u03931 Xn , NP\nE , M \u2229 RCC (y3 )\n\nMI\n\n`\n \u0301\n2\n\u03931 Xn , NP\nE , M \u2229 RCC (y3 ) 0.2\n\ny2\n\ny1\n\n0.6\n\n0.8\n1\n`\n \u0301\n2\n\u22a5\n\u03931 Xn , NP\nE , M \u2229 RIC (y3 )\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n\u0011\n\u0010\n\u221a\nFigure 12: \u03931 Xn , NP E2 , M for M = MCC (left) and M = MI (right) with three distinct edge extrema.\n\u2022 Suppose X, Y are iid from a continuous distribution F whose support is S(F ) \u2286 T (Y3 ). Then for\nr1 < r2 , A (\u03931 (X, NPr1E , M )) \u2264ST A (\u03931 (Y, NPr2E , M )).\n\u2022 Suppose Xn and Xn\u2032 are two random samples from a continuous distribution F whose support is\nS(F ) \u2286 T (Y3 ). Then for r1 < r2 , A(\u03931 (Xn , NPr1E , M )) \u2264ST A(\u03931 (Xn\u2032 , NPr2E , M )). \u0003\n\nRemark 6.8. In Rd with d > 2, recall S (Ym ), the simplex based on d + 1 points that do not lie on the same\nhyperplane. Furthermore, let \u033ai (r, x) be the hyperplane such that \u033ai (x) \u2229 S (Ym ) 6= \u2205 and r d(yi , \u033ai (r, x)) =\nd(yi , \u03a5(yi , x)) for i \u2208 {1, 2, . . . , d + 1. Then\n\u03931 (x, NPr E , MC ) \u2229 RCM (yi ) = {z \u2208 RCM (yi ) : d(yi , \u03a5(yi , z)) \u2265 d(yi , \u033ai (r, x)} for i \u2208 {1, 2, 3}.\nS\nr\nr\nHence \u03931 (x, NPr E , MC ) = d+1\ni=1 (\u03931 (x, NP E , MC )\u2229RCM (yi )). Furthermore, it is easy to see that \u03931 (Xn , NP E , MC ) =\nTd+1\nr\ni=1 \u03931 (X\u03c6i (n), NP E , MC ), where X\u03c6i (n) is one of the closest points in Xn to face \u03c6i . \u0003\n\n6.2\n\nCentral Similarity Proximity Maps\n\nThe other type of triangular proximity map we introduce is the central similarity proximity map. Furthermore, the relative density of the corresponding PCD will have mathematical tractability. Alas, the\ndistribution of the domination number of the associated PCD is still an open problem.\n\u03c4\nFor \u03c4 \u2208 [0, 1], define NCS\n(*, M ) := N (*, M ; \u03c4, Y3 ) to be the central similarity proximity map with M -edge\nregions as follows; see also Figure 13 with M = MC . For x \u2208 T (Y3 ) \\ Y3 , let e(x) be the edge in whose\nregion x falls; i.e., x \u2208 RM (e(x)). If x falls on the boundary of two edge regions, we assign e(x) arbitrarily.\n\u03c4\nFor \u03c4 \u2208 (0, 1], the parametrized central similarity proximity region NCS\n(x, M ) is defined to be the triangle\nT\u03c4 (x) with the following properties:\n\n(i) T\u03c4 (x) has edges e\u03c4i (x) parallel to ei for each i \u2208 {1, 2, 3}, and for x \u2208 RM (e(x)), d(x, e\u03c4 (x)) =\n\u03c4 d(x, e(x)) and d(e\u03c4 (x), e(x)) \u2264 d(x, e(x)) where d(x, e(x)) is the Euclidean (perpendicular) distance\nfrom x to e(x);\n(ii) T\u03c4 (x) has the same orientation as and is similar to T (Y3 );\n(iii) x is the same type of center of T\u03c4 (x) as M is of T (Y3 ).\nNote that (i) implies the parametrization of the proximity region, (ii) explains \"similarity\", and (iii) explains\n\u03c4 =0\n\"central\" in the name, central similarity proximity map. For \u03c4 = 0, we define NCS\n(x, M ) = {x} for all\n\u03c4\nx \u2208 T (Y3 ). For x \u2208 \u2202(T (Y3 )), we define NCS (x, M ) = {x} for all \u03c4 \u2208 [0, 1].\n19\n\n\f\u03c4\n\u03c4\nNotice that by definition x \u2208 NCS\n(x, M ) for all x \u2208 T (Y3 ). Furthermore, \u03c4 \u2264 1 implies that NCS\n(x, M ) \u2286\no\no\nT (Y3 ) for all x \u2208 T (Y3 ) and M \u2208 T (Y3 ) . For all x \u2208 T (Y3 ) \u2229 RM (e(x)), the edges e\u03c4 (x) and e(x) are\ncoincident iff \u03c4 = 1.\niid\n\nNotice that Xi \u223c F , with the additional assumption that the non-degenerate two-dimensional probability\ndensity function f exists with support S(F ) \u2286 T (Y3 ), implies that the special case in the construction of\n\u03c4\nNCS\n(*) - X falls on the boundary of two edge regions - occurs with probability zero. Note that for such\n\u03c4\nan F , NCS\n(X, M ) is a triangle a.s.\ny3\n\ne2\n\ne1\nMC\ne\u03c41 (x)\n\ne\u03c42 (x)\nx\ne\u03c43 (x)\nd(x, e\u03c43 (x)) = \u03c4 d(x, e(x))\ny1\n\nd(x, e(x))\ny2\n\ne3 = e(x)\n\u03c4 =1/2\n\nFigure 13: Construction of central similarity proximity region, NCS\n\n(x, MC ) (shaded region).\no\n\nCentral similarity proximity maps are defined with M -edge regions for M \u2208 T (Y3 ) . In general, for\n\u03c4\ncentral similarity proximity regions with M -edge regions, the similarity ratio of NCS\n(x, M ) to T (Y3 ) is\n\u03c4 =1/2\n\u03c4\n\u03c4\nd(x, e (x))/d(M, e(x)). See Figure 13 for NCS (x, MC ) with e = e3 . The functional form of NCS\n(x, MC )\nis provided in Ceyhan (2009).\n6.2.1\n\n\u03c4\nExtension of NCS\nto Higher Dimensions\n\n\u03c4\nThe extension of NCS\nto Rd for d > 2 is straightforward. the extension for M = MC is described, the\nextension for general M is similar. Let Yd+1 = {y1 , y2 , . . . , yd+1 } be d + 1 points that do not lie on the\nsame (d \u2212 1)-dimensional hyperplane. Denote the simplex formed by these d + 1 points as S(Yd+1 ). For\n\u03c4 \u2208 (0, 1], define the central similarity proximity map as follows. Let \u03c6i be the face opposite vertex yi for\ni \u2208 {1, 2, . . . , (d + 1)}, and \"face regions\" RCM (\u03c61 ), RCM (\u03c62 ), . . . , RCM (\u03c6d+1 ) partition S(Yd+1 ) into d + 1\nregions, namely the d + 1 polytopes with vertices being the center of mass together with d vertices chosen\nfrom d + 1 vertices. For x \u2208 S(Yd+1 ) \\ Yd+1 , let \u03c6(x) be the face in whose region x falls; x \u2208 R(\u03c6(x)). If x\nfalls on the boundary of two face regions, \u03c6(x) is assigned arbitrarily. For \u03c4 \u2208 (0, 1], the central similarity\n\u03c4\nproximity region NCS\n(x, MC ) = S\u03c4 (x) is defined to be the simplex S\u03c4 (x) with the following properties:\n\n(i) S\u03c4 (x) has faces \u03c6\u03c4i (x) parallel to \u03c6i (x) for i \u2208 {1, 2, . . . , (d+1)}, and for x \u2208 RCM (\u03c6(x)), \u03c4 d(x, \u03c6(x)) =\nd(\u03c6\u03c4 (x), x) where d(x, \u03c6(x)) is the Euclidean (perpendicular) distance from x to \u03c6(x);\n(ii) S\u03c4 (x) has the same orientation as and similar to S(Yd+1 );\n\u03c4\n(iii) x is the center of mass of S\u03c4 (x), as MC is of S(Yd+1 ). Note that \u03c4 > 1 implies that x \u2208 NCS\n(x).\n\n20\n\n\f6.2.2\n\n\u03931 -Regions for Central Similarity Proximity Maps\n\n\u03c4\nFor NCS\n, the \u03931 -region is constructed as follows. Let e\u03c4i (x) be the edge of T\u03c4 (x) parallel to edge ei for\ni \u2208 {1, 2, 3}. Now, suppose u \u2208 RM (e3 ) and let \u03b6i (\u03c4, x) for i \u2208 {1, 2, . . . , 7} be the lines such that\n\nv \u2208 \u03b61 (\u03c4, u) \u2229 RM (e3 ) =\u21d2 u \u2208 e\u03c41 (v),\nv \u2208 \u03b62 (\u03c4, u) \u2229 RM (e3 ) =\u21d2 u \u2208 e\u03c42 (v),\nv \u2208 \u03b63 (\u03c4, u) \u2229 RM (e1 ) =\u21d2 u \u2208\nv \u2208 \u03b64 (\u03c4, u) \u2229 RM (e1 ) =\u21d2 u \u2208\n\nv \u2208 \u03b65 (\u03c4, u) \u2229 RM (e2 ) =\u21d2 u \u2208 e\u03c43 (v),\nv \u2208 \u03b66 (\u03c4, u) \u2229 RM (e2 ) =\u21d2 u \u2208 e\u03c42 (v),\n\ne\u03c42 (v),\ne\u03c43 (v),\n\nv \u2208 \u03b67 (\u03c4, u) \u2229 RM (e3 ) =\u21d2 u \u2208\n\n(3)\n\ne\u03c43 (v).\n\n\u03c4\n\u03c4\nThen \u03931 (x, NCS\n, M ) is the region bounded by these lines. See also Figure 14. \u03931 (x, NCS\n, M ) for x \u2208 RM (ei )\nfor i \u2208 {1, 2} can be described similarly.\n\n\u03c4\n\u03c4\nNotice that \u03c4 > 0 implies that x \u2208 \u03931 (x, NCS\n, M ). Furthermore, \u03931 (x, NCS\n, M ) = {x} iff (i) \u03c4 = 0 or\n(ii) x \u2208 \u2202(T (Y3 )).\n\n\u03c4\nThe \u03931 -region\n\u0001 \u03931 (x, NCS , M ) is a convex k-gon with 3 \u2264 k \u2264 7 vertices. In particular, for \u03c4 = 1,\n\u03c4 =1\n\u03931 x, NCS , M is a convex hexagon. See Figure 16 (left).\n\ny3\n\ny3\n\ne2\n\ne2\n\ne1\n\ne1\n\n\u03b65(\u03c4, x)\n\u03b66(\u03c4, x)\n\u03b61(\u03c4, x)\n\n\u03b67(\u03c4, x)\n\u03b62(\u03c4, x)\n\u03b61(\u03c4, x)\ny1\n\ny3\n\ny2\n\ne3 = e(x)\n\ne2\n\ny1\n\n\u03b66(\u03c4, x)\n\u03b61(\u03c4, x)\n\n\u03b62(\u03c4, x)\ne3 = e(x)\n\ny2\ny1\n\ny2\n\ne3 = e(x)\n\ne1\n\u03b65(\u03c4, x)\n\n\u03b65(\u03c4, x) \u03b67(\u03c4, x) \u03b64(\u03c4, x)\n\u03b63(\u03c4, x)\n\u03b66(\u03c4, x)\n\ny1\n\ny3\n\ne2\n\ne1\n\n\u03b61(\u03c4, x)\n\n\u03b67(\u03c4, x)\n\u03b62(\u03c4, x)\n\n\u03b64(\u03c4, x)\n\u03b63(\u03c4, x)\n\u03b62(\u03c4, x)\n\ne3 = e(x)\n\ny2\n\n\u0011\n\u0010\n\u03c4 =1/2\nFigure 14: Examples of the four types of the \u03931 -region, \u03931 x, NCS , MC with four distinct x \u2208 RCM (e3 )\n(shaded regions).\n\u03c4\nThen the functional form of \u03b6i (x) in Equation (3) that define the boundary of \u03931 (x = (x0 , y0 ), NCS\n) with\n\n21\n\n\fM -central proximity regions in the basic triangle Tb for x \u2208 RM (e3 ) is\nm2 (c2 (x \u2212 x0 ) + y0 c1 )\nm2 (c2 (x0 \u2212 x) + y0 (1 \u2212 c1 ))\n, \u03b62 (x) =\n,\nm2 c1 (1 \u2212 \u03c4 ) + c2 \u03c4 m1\nm2 (1 \u2212 c1 ) (1 \u2212 \u03c4 ) + c2 \u03c4 (1 \u2212 m1 )\nm2 c2 (x \u2212 1) + y0 (m2 (1 \u2212 c1 ) \u2212 c2 (1 \u2212 m2 ))\n,\n\u03b63 (x) =\nc1 (1 \u2212 c1 ) (1 \u2212 \u03c4 )\nh\n\u03b64 (x) = c1 (m2 (1 \u2212 c1 ) + c2 (1 \u2212 m1 )) y0 + c2 (m2 (1 \u2212 c1 ) + c2 (1 \u2212 m1 )) x0 + c2 (\u03c4 (m2 c1 \u2212 m1 c2 )\ni.h\n+ c2 (1 \u2212 m1 ) + m2 (1 \u2212 c1 )) x + c2 \u03c4 (m1 c2 \u2212 m2 c1 )\nc2 (\u03c4 \u2212 c1 (1 \u2212 \u03c4 )) m1\ni\n+ c1 (1 \u2212 c1 ) (1 \u2212 \u03c4 ) m2 + c1 c2 ,\n\u03b61 (x) =\n\n\u03c4 m2 c2 x \u2212 y0 (c1 m2 \u2212 c2 m1 )\ny0\n, \u03b67 (x) =\nc2 m1 \u2212 c1 m2 (1 \u2212 \u03c4 )\n1\u2212\u03c4\nh\n\u03b66 (x) = (1 \u2212 c1 ) c1 (m2 (1 \u2212 c1 ) \u2212 c2 (1 \u2212 m1 )) y0 + c1 c2 (m2 (1 \u2212 c1 ) \u2212 c2 (1 \u2212 m1 )) x0 + (c22 c1 (1 \u2212 \u03c4 ) m1\ni.h\n+ c2 \u03c4 (1 \u2212 c1 ) + (1 \u2212 \u03c4 ) (c1 c2 \u2212 c21 c2 ) m2 \u2212 c22 \u03c4 (1 \u2212 c1 ) \u2212 c22 c1 (1 \u2212 \u03c4 )) x + c22 \u03c4 (m1 \u2212 c1 )\n(1 \u2212 c1 )\ni\n(c2 (c2 (1 \u2212 \u03c4 ) \u2212 \u03c4 ) m1 + c1 (1 \u2212 c1 ) (1 \u2212 \u03c4 ) m2 + c1 c2 (1 \u2212 2 \u03c4 ))\n\n\u03b65 (x) =\n\nProposition 6.9. Let Xn be a random sample from U(T (Y3 )). For central similarity proximity maps with\no\n\u03c4\nM -edge regions (by definition M \u2208 T (Y3 ) ) and \u03c4 > 0, \u03b7n (NCS\n) \u2264 3 with equality holding with positive\nprobability for n \u2265 3.\no\n\nProof: Let M \u2208 T (Y3 ) and \u03c4 > 0. Then given Xn , for i \u2208 {1, 2, 3}, we have\n\u03c4\n\u03931 (Xn , NCS\n, M )) \u2229 RM (ei ) =\n\nn\n\\\n\ni=1\n\n\u03c4\n, M ) \u2229 RM (ei )]\n[\u03931 (Xi , NCS\n\n\u0001\n\u03c4\n\u03c4\n, M ) \u2229 RM (ei )\n, M \u2229 \u03931 (Xek (n), NCS\n= \u03931 Xej (n), NCS\n\n\u03c4\n\u03c4\nwhere j, k 6= i, since for x \u2208 RM (ei ), if {Xek (n), Xel (n)} \u2282 NCS\n(x, M ). Hence\n(x, M ), then Xn \u2282 NCS\nfor each edge we need the edge extrema with respect to the other edges, then the minimum active set is\n\u03c4\n) \u2264 3. Furthermore, for the random sample Xn , Xe (n) is\nSM = {Xe1 (n), Xe2 (n), Xe3 (n)}, hence \u03b7n (NCS\nunique for each edge e with probability 1 and there are three distinct edge extrema with positive probability\n\u03c4\n(see Theorem 6.4). Hence P (\u03b7n (NCS\n) = 3) > 0 for n \u2265 3. \u0004\n\n\u03c4\nNote that for \u03c4 > 0 and Xn a random sample from U(T (Y3 )), P (\u03b7(Xn , NCS\n) = 3) \u2192 1 as n \u2192 \u221e, since\nthe edge extrema are distinct with probability 1 \u0001as n \u2192 \u221e. For \u03c4 = 1, the \u03931 -region can be determined by\n\u03c4 =1\n, M \u2229 RM (yi ) can be determined by the\u0001 edge extrema Xej and\nthe edge extrema, in particular \u03931 Xn , NCS\n\u03c4 =1\n, M 6= \u2205 for all n, because\nXek for i, j, k are all distinct. See Figure 15 for an example. Here \u03931 Xn , NCS\n\u0001\nd\n\u03c4 =1\n\u03c4 =1\n\u03c4\n) = \u03b7n (NPr E ) for\nby construction M \u2208 \u03931 Xn , NCS , M since NCS (M ) = T (Y3 ). Furthermore, \u03b7n (NCS\nall (r, \u03c4 ) \u2208 [1, \u221e) \u00d7 (0, 1].\n\u0001\no\n\u03c4 =1\nProposition 6.10. The \u03931 -region \u03931 B, NCS\n, M is a convex hexagon for any set B of size n in T (Y3 ) .\n\nProof: This follows from the fact that each \u03b6i (x) for i \u2208 {1,\n\u0001 2, . . . , 6} is parallel to a line joining yi to M for\n\u03c4 =1\n, M ). See Figure 16 (right) for an example. \u0004\n\u03c4 = 1 (\u03b67 (x) is not used in construction of \u03931 B, NCS\n\u03c4\nWith M = MC , the functional forms of the lines that determine the boundary of \u03931 (x = (x0 , y0 ), NCS\n)\n\n22\n\n\fy3\n\n0.7\n\n0.6\n\nPSfrag replacements\n0.5\n\n0.4\n\n0.3\n\nMC\nG(AB)\n\n0.2\n\n0.1\n\ny1\n\ny2\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nFigure 15: G(AB) region for x1 and \u03931 -region for n \u2265 1\nin Equation (3) for Tb with x \u2208 RCM (e3 ) are given by\ny0 c1 + c2 (x \u2212 x0 )\nc2 (x0 \u2212 x) + y0 (1 \u2212 c1 )\n,\n, \u03b62 (x) =\nc1 + \u03c4\n\u03c4 + 1 \u2212 c1\n\u03c4 c2 (1 \u2212 x) + c2 (x0 \u2212 x) \u2212 y0 c1\nc2 \u03c4 (1 \u2212 x) + y0\n,\n, \u03b64 (x) =\n\u03b63 (x) =\n1 + \u03c4 (1 \u2212 c1 )\n\u03c4 (1 \u2212 c1 ) \u2212 c1\nc2 x + y 0\ny0\n\u03b65 (x) =\n,\n\u03b67 (x) =\n,\n1 + \u03c4 c1\n1\u2212\u03c4\nc1 (1 \u2212 c1 ) y0 + c1 c2 x0 \u2212 c2 (2 \u03c4 (1 \u2212 c1 ) + c1 (1 \u2212 \u03c4 )) x + \u03c4 c2 (1 \u2212 2 c1 )\n\u03b66 (x) =\n.\nc1 (1 \u2212 c1 ) (1 \u2212 \u03c4 )\n\u03b61 (x) =\n\nFor n > 1 with M = MC , let Xei (n) = xei = (ui , wi ) be given for i \u2208 {1, 2, 3}. Then the functional forms\n\u03c4\nof the lines that determine the boundary of \u03931 (Xn , NCS\n, MC ) are given by\nw2 c1 + c2 (x \u2212 u2 )\nc2 \u03c4 (1 \u2212 x) + y0\n\u03b61 (x) =\n,\n\u03b63 (x) =\n,\nc1 + \u03c4\n1 + \u03c4 (1 \u2212 c1 )\nc2 (u1 \u2212 x) + w1 (1 \u2212 c1 )\nc2 x + w3\n\u03b62 (x) =\n,\n\u03b65 (x) =\n,\n\u03c4 + 1 \u2212 c1\n1 + \u03c4 c1\nw3\n\u03c4 c2 (1 \u2212 x) + c2 (u3 \u2212 x) \u2212 w3 c1\n, \u03b67 (x) =\n,\n\u03b64 (x) =\n\u03c4 (1 \u2212 c1 ) \u2212 c1\n1\u2212\u03c4\nc1 (1 \u2212 c1 ) w1 + c1 c2 u1 \u2212 c2 (2 \u03c4 (1 \u2212 c1 ) + c1 (1 \u2212 \u03c4 )) x + \u03c4 c2 (1 \u2212 2 c1 )\n\u03b66 (x) =\n.\nc1 (1 \u2212 c1 ) (1 \u2212 \u03c4 )\n\u0001\n\u0001\n\u03c4 =1\n\u03c4 =1\n, MC .\n, MC and \u03931 Xn , NCS\nSee Figure 16 for \u03931 x, NCS\n\nRemark 6.11.\n\n\u03c41\n\u03c42\n\u2022 For \u03c41 < \u03c42 , \u03931 (x, NCS\n, M ) \u2286 \u03931 (x, NCS\n, M ) for all x \u2208 T (Y3 ) with equality holding only when\nx \u2208 \u2202(T (Y3 )).\n\u03c41\n\u03c42\n\u2022 Let Xn be a set of n points in T (Y3 ). Then for \u03c41 < \u03c42 , \u03931 (Xn , NCS\n, M ) \u2286 \u03931 (Xn , NCS\n, M ) with\nequality holding only when Xn \u2282 \u2202(T (Y3 )).\n\n\u2022 Suppose X, Y are iid from a continuous distribution F whose support is S(F ) \u2286 T (Y3 ). Then for\n\u03c41\n\u03c42\n\u03c41 < \u03c42 , A (\u03931 (X, NCS\n, M )) \u2264ST A (\u03931 (Y, NCS\n, M )).\n23\n\n\fPSfrag replacements\n\nPSfrag replacements\ny3\n\n0.7\n\ny3\n\n0.7\n\n0.6\n0.6\n\n0.5\n0.5\n\nx\n\n0.4\n\nx\n0.4\n\nxe2\n\nxe1\n\n0.3\n0.3\n\nMC\n\nMC\n\n0.2\n\n0.2\n\n0.1\n\ny12\n\nxe3\n\n0.1\n\n0.2\n\n0.4\n\n0.6\n\ny12\n\n0.8\n\ny1\n\n0\n\n0.2\n\n0.4\n\n0.6\n\ny2\n\n0.8\n\n1\n\n\u03c4\n\u03c4\nFigure 16: The \u03931 -region \u03931 (x, NCS\n, MC ) with x \u2208 RM (e2 ) (left) and \u03931 (Xn , NCS\n, MC ) with n > 1 (right).\n\n\u2022 Suppose Xn and Xn\u2032 are two random samples from a continuous distribution F whose support is\n\u03c41\n\u03c42\nS(F ) \u2286 T (Y3 ). Then for \u03c41 < \u03c42 , A(\u03931 (Xn , NCS\n, M )) \u2264ST A(\u03931 (Xn\u2032 , NCS\n, M )). \u0003\n\n7\n\nInvestigation of the Proximity Regions and the Associated PCDs\nUsing the Auxiliary Tools\n\n7.1\n\nCharacterization of Proximity Maps Using \u03b7-Values\n\nBy definition, it is trivial to show that that the minimum number of points to describe the \u03931 -region,\n\u03c4\n\u03b7n (N ) \u2264 n for any proximity map. We have improved the upper bound for NPr E and NCS\n: \u03b7(Xn , NPr E ) \u2264 3\n\u03c4\nand \u03b7(Xn , NCS\n) \u2264 3. However, finding such an improvement does not hold for \u03b7n (NAS ); that is, finding a\nk < n such that \u03b7n (NAS ) \u2264 k for all Xn is still an open problem.\nBelow we state a condition for N (*, M ) defined with M -vertex regions to have \u03b7n (N ) \u2264 3 for Xn with\nsupport in T (Y3 ).\n\nTheorem 7.1. Suppose N (*, M ) is a proximity region defined with M -vertex regions and B is a set of n\ndistinct points in T (Y3 ). Then \u03b7(B, N ) \u2264 3 if\n(i) for each yi \u2208 Y3 there exists a point x(yi ) \u2208 B (i.e., related to yi ) such that \u03931 (B, N ) \u2229 RM (yi ) =\n\u03931 (x(yi ), N ) \u2229 RM (yi ),\nor\n(ii) there exist points x(yj ), x(yk ) \u2208 B such that \u03931 (B, N )\u2229RM (yi ) = \u03931 (x(yj ), N )\u2229\u03931 (x(yk ), N )\u2229RM (yi )\nfor j, k 6= i with i \u2208 {1, 2, 3} and (j, k) \u2208 {(1, 2), (1, 3), (2, 3)}.\nProof: Let B = {x1 , x2 , . . . , xn } \u2282 T (Y3 ).\n(i) Suppose there exists a point x(yi ) \u2208 B such that \u03931 (B, N ) \u2229 RM (yi ) = \u03931 (x(yi ), N ) \u2229 RM (yi ) for each\ni \u2208 {1, 2, 3}. Then\n\u03931 (B, N ) \u2229 RM (yi ) = \u03931 (x(yi ), N ) \u2229 RM (yi ) =\nn\n\\\n\n[\u03931 (xi , N ) \u2229 RM (yi )] =\n\ni=1\n\n3\n\\\n\nj=1\n\n\uf8ee\n\n[\u03931 (x(yj ), N ) \u2229 RM (yi )] = \uf8f0\n\n3\n\\\n\nj=1\n\n24\n\n\uf8f9\n\n\u03931 (x(yj ), N )\uf8fb \u2229 RM (yi )\n\n\fand\n\u03931 (B, N ) =\n\n3\n[\n\ni=1\n\n[\u03931 (B, N ) \u2229 RM (yi )] =\n\n3\n[\n\ni=1\n\nThen, we get\n\u03931 (B, N ) =\n\n3\n\\\n\n\uf8eb\uf8ee\n\uf8ed\uf8f0\n\n3\n\\\n\nj=1\n\n\uf8f9\n\n\uf8f6\n\n\u03931 (x(yj ), N )\uf8fb \u2229 RM (yi )\uf8f8 .\n\n\u03931 (x(yi ), N ) .\n\ni=1\n\nHence, the minimum active set SM \u2286 {x(y1 ), x(y2 ), x(y3 )}, which implies \u03b7(B, N ) \u2264 3. The \u03b7-value\n\u03b7(B, N ) < 3 will hold if x(yi ) are not all distinct.\n(ii) Suppose there exist points x(yj ) and x(yk ) such that \u03931 (B, N )\u2229RM (yi ) = \u03931 (x(yj ), N )\u2229\u03931 (x(yk ), N )\u2229\nRM (yi ) for j, k 6= i. Then\n\u03931 (B, N ) \u2229 RM (yi ) = \u03931 (x(yj ), N ) \u2229 \u03931 (x(yk ), N ) \u2229 RM (yi ) =\nn\n\\\n\n[\u03931 (xi , N ) \u2229 RM (yi )] =\n\ni=1\n\nand\n\u03931 (B, N ) =\n\n3\n[\n\ni=1\n\n3\n\\\n\nq=1\n\n[\u03931 (x(yq ), N ) \u2229 RM (yi )] =\n\n[\u03931 (B, N ) \u2229 RM (yi )] =\n\n3\n[\n\ni=1\n\n\"\n\n3\n\\\n\nq=1\n\n\"\n\n3\n\\\n\nq=1\n\n#\n\n\u03931 (x(yq ), N ) \u2229 RM (yi )\n\n#\n\n!\n\n\u03931 (x(yq ), N ) \u2229 RM (yi ) .\n\nT3\nThen, we get \u03931 (B, N ) = i=1 \u03931 (x(yi ), N ) . Hence, the minimum active set SM \u2286 {x(y1 ), x(y2 ), x(y3 )}\nwhich implies \u03b7(B, N ) \u2264 3. \u0004\nNotice that NPr E satisfies condition (i) Theorem 7.1.\nBelow we state some conditions for N (*, M ) defined with M -edge regions to have \u03b7-value less than equal\nto 3.\nTheorem 7.2. Suppose N (*, M ) is a proximity region defined with M -edge regions and B is set of n distinct\npoints in T (Y3 ). Then \u03b7(B, N ) \u2264 3 if\n(i) for each ei \u2208 {e1 , e2 , e3 }, there exists a point x(ei ) \u2208 B such that \u03931 (B, N ) \u2229 RM (ei ) = \u03931 (x(ei ), N ) \u2229\nRM (ei ),\nor\n(ii) there exist points x(ej ), x(ek ) \u2208 B such that \u03931 (B, N )\u2229RM (ei ) = \u03931 (x(ej ), N )\u2229\u03931 (x(ek ), N )\u2229RM (ei )\nfor j, k 6= i with i \u2208 {1, 2, 3} and (j, k) \u2208 {(1, 2), (1, 3), (2, 3)}.\nProof: Let B = {x1 , x2 , . . . , xn } \u2282 T (Y3 ).\n(i) Suppose there exists a point x(ei ) \u2208 B such that \u03931 (B, N ) \u2229 RM (ei ) = \u03931 (x(ei ), N ) \u2229 RM (ei ) for each\ni \u2208 {1, 2, 3}. Then\n\u03931 (B, N ) \u2229 RM (ei ) = \u03931 (x(ei ), N ) \u2229 RM (ei ) =\nn\n\\\n\n[\u03931 (xi , N ) \u2229 RM (ei )] =\n\ni=1\n\nand\n\u03931 (B, N ) =\n\n3\n[\n\n3\n\\\n\nj=1\n\n\uf8ee\n\n[\u03931 (x(ej ), N ) \u2229 RM (ei )] = \uf8f0\n\n3\n\\\n\nj=1\n\n[\u03931 (B, N ) \u2229 RM (ei )] =\n\ni=1\n\n25\n\n3\n[\n\ni=1\n\n\uf8eb\uf8ee\n\uf8ed\uf8f0\n\n3\n\\\n\nj=1\n\n\uf8f9\n\n\u03931 (x(ej ), N )\uf8fb \u2229 RM (ei )\n\n\uf8f9\n\n\uf8f6\n\n\u03931 (x(ej ), N )\uf8fb \u2229 RM (ei )\uf8f8 .\n\n\fThen, we get\n\u03931 (Xn , N ) =\n\n3\n\\\n\n\u03931 (x(ej ), N ) .\n\nj=1\n\nHence, the minimum active set SM \u2286 {x(e1 ), x(e2 ), x(e3 )} which implies \u03b7n (N ) \u2264 3.\n(ii) Suppose there exist points x(ej ) and x(ek ) such that \u03931 (Xn , N )\u2229RM (ei ) = \u03931 (x(ej ), N )\u2229\u03931 (x(ek ), N )\u2229\nRM (ei ) for j, k 6= i. Then\n\u03931 (B, N ) \u2229 RM (ei ) = \u03931 (x(ej ), N ) \u2229 \u03931 (x(ek ), N ) \u2229 RM (ei ) =\nn\n\\\n\ni=1\n\n[\u03931 (xi , N ) \u2229 RM (ei )] =\n\nand\n\u03931 (B, N ) =\n\n3\n[\n\n3\n\\\n\n[\u03931 (x(eq ), N ) \u2229 RM (ei )] =\n\nq=1\n\n[\u03931 (B, N ) \u2229 RM (ei )] =\n\ni=1\n\n3\n[\n\ni=1\n\n\"\n\n3\n\\\n\nq=1\n\n\"\n\n3\n\\\n\nq=1\n\n#\n\n\u03931 (x(eq ), N ) \u2229 RM (ei )\n\n#\n\n!\n\n\u03931 (x(eq ), N ) \u2229 RM (ei ) .\n\nT\nThen, we get \u03931 (B, N ) = 3i=1 \u03931 (x(ei ), N ) . Hence, the minimum active set SM \u2286 {x(e1 ), x(e2 ), x(e3 )}\nwhich implies \u03b7(B, N ) \u2264 3. \u0004\n\u03c4\nNotice that NCS\nsatisfies condition (ii) in Theorem 7.2.\n\n7.2\n\nThe Behavior of \u03931 (Xn , N) for the Proximity Maps in T (Y3 )\n\nIn Section 4, we have investigated the behavior of \u03931 (Xn , N ) for general proximity maps in \u03a9. The assertions\nmade about \u03931 -regions will be stronger for the proximity regions we have defined, i.e., for NAS , NPr E , and\n\u03c4\nNCS\n, compared to the general assertions in Section 4. One property enjoyed by these proximity maps is that\nthe region N (x) gets larger as x moves along a line from \u2202(T (Y3 )) to RS (N ) in a region with positive R2 \u03c4\nLebesgue measure. So the modifications of the assertions in Section 4 also hold for {NS , NAS , NPr E , NCS\n}.\nIn particular, we have a stronger result than the one in Proposition 4.1 in the sense that, RS (N ) is a proper\nsubset of \u03931 (Xn , N ) as shown below.\n\u03c4\nProposition\n7.3. For each type of proximity map N \u2208 {NS , NAS , NPr E , NCS\n} and any random sample Xn =\n\b\nX1 , X2 , . . . , Xn from a continuous distribution F on T (Y3 ), if RS (N ) 6= \u2205, then RS (N ) ( \u03931 (Xn , N )\na.s. for each n < \u221e.\n\nProof: We have shown that RS (N ) \u2286 \u03931 (Xn , N ) (see Proposition 4.1). Moreover, RS (N ) 6= \u2205 for NAS ,\n\u03c4 =1\nNPr E with r > 3/2, and NCS\n. For these proximity regions, RS (N ) = \u03931 (Xn , N ) with probability 0 for each\nfinite n since\n(i) for N \u2208 {NS , NAS }, \u03931 (Xn , N ) = RS (N ) iff Xy (n) = y for each y \u2208 Y3 which happens with probability\n0,\no\nn\nr>3/2\n\u03c4 =1\n, \u03931 (Xn , N, M ) = RS (N, M ) iff Xei (n) \u2208 ei for each i \u2208 {1, 2, 3} which\n, NP E\n(ii) for N \u2208 NCS\nhappens with probability 0.\n/ (T r )o (see Equation (2) for T r ), say\nFurthermore, for NPr E with r < 3/2, RS (NPr E , M ) 6= \u2205 iff M \u2208\nM = (mx , my ) is such that d(\u03b6(mx , y2 ), y2 ) < d(y2 , e2 )/r. Then \u03931 (Xn , NPr E , M ) = RS (NPr E , M ) iff\nXe2 (n) \u2208 e2 which happens with probability 0. Similarly the same result also holds for edges e1 and e3 . \u0004\n\nNote that, if RS (N ) = \u2205 and Xn is a random sample from a continuous distribution on T (Y3 ), then\n\u03931 (Xn , N ) = \u2205 a.s. as n \u2192 \u221e. In particular, this holds for NPr E (*, M ) with r < 3/2 and M \u2208 (T r )o . Lemma\n26\n\n\f4.6 holds as stated. In Lemma 4.6, we have shown that \u03931 (X (n), N ) is non-increasing. Furthermore, for the\n\u03c4\nproximity regions {NS , NAS , NPr E , NCS\n}, we can state that \u03931 (X (n + 1), N ) ( \u03931 (X (n), N ) with positive\nprobability, since the new point in X (n + 1) has positive probability to fall closer to the subset of T (Y3 )\nthat defines RS (N ) (e.g., \u2202(T (Y3 ))). The general results in Theorems 4.8 and 4.10 and Proposition 4.9 hold\n\u03c4\nfor the proximity maps {NS , NAS , NPr E , NCS\n} also. In particular we have the following corollaries to these\nresults.\niid\n\nCorollary to Theorem 4.8: Given a sequence of random variables X1 , X2 , . . . \u223c U(T (Y3 )), let X (n) :=\n\u03c4\nX (n\u22121)\u222a{Xn } for n = 0, 1, 2, . . . with X (0) := \u2205. Then for each N \u2208 {NS , NAS , NPr E , NCS\n}, \u03931 (X (n), N ) \u2193\nRS (N ) as n \u2192 \u221e a.s., in the sense that \u03931 (X (n + 1), N ) \u2286 \u03931 (X (n), N ) and A(\u03931 (X (n), N ) \\ RS (N )) \u2193 0\na.s.\nCorollary to Proposition 4.9: For positive integers m > n, let Xm and Xn be two random samples\n\u03c4\nfrom U(T (Y3 )). Then A (\u03931 (Xm , N, M )) \u2264ST A (\u03931 (Xn , N, M )) for proximity maps N \u2208 {NPr E , NCS\n}, for\nr \u2208 [1, \u221e) and \u03c4 \u2208 (0, 1].\n\nFor N \u2208 {NS , NAS }, the stochastic ordering of A (\u03931 (Xn , N, M )) is still an open problem, although\nwe conjecture that A (\u03931 (Xm , N, M )) \u2264ST A (\u03931 (Xn , N, M )) for Xm and Xn two random samples from\nU(T (Y3 )) with m > n.\nCorollary to Theorem 4.10: Let {Xn }\u221e\nn=1 be a sequence of data sets which are iid U(T (Y3 )). Then\na.s.\n\u03c4\n\u03931 (Xn , N ) \u2212\u2212\u2192 RS (N ) as n \u2192 \u221e for N \u2208 {NS , NAS , NPr E , NCS\n}.\n\nExpected Measure of \u03931 -Regions\n\n7.3\n\nLet \u03bb(*) be the Rd -Lebesgue measure on Rd with d \u2265 1. In R, \u03bb(*) is the length | * |, in R2 , \u03bb(*) is the area\nA(*), and in Rd , \u03bb(*) is the d-dimensional volume V (*). In R with Y2 = {0, 1}, let Xn be a random sample\nfrom U(0, 1), and NS (x) = B(x, r(x)) where r(x) = min(x, 1 \u2212 x). Then,\n\u03931 (Xn , NS ) = [Xn:n /2, (1 + X1:n )/2] =\u21d2 \u03bb(\u03931 (Xn , NS )) = |\u03931 (Xn , NS ) | = (1 + X1:n \u2212 Xn:n ) /2.\nHence the expected length of the \u03931 -region is\n\u0015\n\u0014\n1 + E [X1:n ] \u2212 E [Xn:n ]\n1 + X1:n \u2212 Xn:n\n=\nE [\u03bb(\u03931 (Xn , NS )] = E\n2\n2\n=\n\n1+\n\n1\nn+1\n\n2\n\n\u2212\n\nn\nn+1\n\n=\n\n1\n\u2192 0 as n \u2192 \u221e.\nn+1\n\n2\n\nIn R , with three non-collinear points Y3 = {y1 , y2 , y3 }, let Xn be a random sample from U(T (Y3 )). \u0001Then\n\u03c4 =1\n,M ) > 0\nA(\u03931 (Xn , NPr E , M )) > 0 a.s. for all n < \u221e, r > 3/2, M \u2208 R2 \\ Y3 . Furthermore, A(\u03931 Xn , NCS\no\na.s. for all n < \u221e and M \u2208 (T (Y3 )) ; and for N \u2208 {NS , NAS } A(\u03931 (Xn , N, M )) > 0 a.s. for all n < \u221e and\nM \u2208 R2 \\ Y3 .\n\nThe \u03931 -region, \u03931 (Xn , N ), is closely related to the distribution of the domination number of the PCD\nassociated with N . Hence we study the asymptotic behavior of the expected area E [A(\u03931 (Xn , N, M )], as\n\u03c4\nn \u2192 \u221e, for N \u2208 {NAS , NPr E , NCS\n}.\nIn R, NS and NAS are equivalent functions, with the extension that NAS is defined as NS for X points\noutside (y1:m , ym:m ). In higher dimensions, determining the areas of \u03931 (Xn , N, M ) for for N \u2208 {NS , NAS }\nand for general Xn and hence finding its expected areas are both open problems.\n7.3.1\n\n\u03c4\nThe Limit of Expected Area of \u03931 (Xn , N, M ) for NPr E and NCS\n\n\u03c4\nRecall that for N \u2208 {NPr E , NCS\n}, \u03931 (Xn , N, M ) is determined by the (closest) edge extrema Xe (n) \u2208\nargminX\u2208Xn d(X, e), for e \u2208 {e1 , e2 , e3 }. So, to find the expected area of \u03931 (Xn , N, M ), we need to find\nthe expected locus of Xe (n); i.e., the expected distance of d(Xe (n) from e. For example, for Xn a random\n\n27\n\n\fsample from a continuous distribution F , argminX\u2208Xn d(X, e) is unique a.s., and if d(Xe (n), e) = u, then\nXe (n) falls on a line parallel to e whose distance from e is u a.s.\nLemma 7.4. Let Di (n) := d(Xei (n), ei ) for i \u2208 {1, 2, 3} and Xn be a random sample from U(T (Y3 )). Then\nE [Di (n)] \u2192 0 (i.e., the expected locus of Xei (n) is on ei ) for each i \u2208 {1, 2, 3}, as n \u2192 \u221e.\niid\n\nProof: Given Zi = (Xi , Yi ) \u223c U(T (Y3 )). Then for e = e3 , D3 (n) = Y1:n (the minimum y-coordinate of\ny (2 c2 \u2212 y)\nZi \u2208 Xn ). First observe that P (Yi \u2264 y) =\n, hence\nc22\nFY (y) =\nSo the pdf of Yi is fY (y) = 2\n\ny (2 c2 \u2212 y)\nI(0 \u2264 y < c2 ) + I(y \u2265 c2 ).\nc22\n\nc2 \u2212 y\nI(0 \u2264 y \u2264 c2 ). Then the pdf of Y1:n is\nc22\n\n\u0013n\u22121\n\u0012\ny (2 c2 \u2212 y)\nc2 \u22122 I(0 \u2264 y \u2264 c2 ).\nf1:n (y) = 2 n(c2 \u2212 y) 1 \u2212\nc22\nTherefore,\nE [Y1:n ] =\n\nZ\n\n0\n\nc2\n\n\u0012\n\u0013n\u22121\ny (2 c2 \u2212 y)\nc2\n2 y n(c2 \u2212 y) 1 \u2212\n\u2192 0, as n \u2192 \u221e.\nc2 \u22122 dy =\n2\nc2\n2n+1\n\nHence E [Y1:n ] = E [D3 (n)] \u2192 0. Similarly, E [Di (n)] \u2192 0 for i \u2208 {1, 2}, as n \u2192 \u221e. \u0004\no\n\n\u03c4\nTheorem 7.5. Let Xn be a random sample from U(T (Y3 )) and M \u2208 T (Y3 ) . For N \u2208 {NPr E , NCS\n},\nE [A(\u03931 (Xn , N, M ))] \u2192 A(RS (N, M )) as n \u2192 \u221e.\n\nT\n\u03c4\nProof: Recall that for N \u2208 {NPr E , NCS\n}, \u03931 (Xn , N, M ) = 3i=1 \u03931 (Xei (n), N, M ). Moreover, \u03931 (Xn , N, M ) =\nRS (N, M ) iff Xei (n) \u2208 ei for i \u2208 {1, 2, 3}. In Lemma 7.4, we have shown that expected locus of Xe (n) converges to edge e as n \u2192 \u221e. Hence the expected locus of \u2202(\u03931 (Xn , N, M )) \u2229 RM (ei ) converges to the\n\u2202(RS (N, M )) \u2229 RM (ei ) for each i \u2208 {1, 2, 3}. Hence\nE [A(\u03931 (Xn , N, M )] \u2192 A(RS (N, M )) as n \u2192 \u221e. \u0004\nRemark 7.6. In particular,\n\u0002\n\u0001\u0001\u0003\n\u0001\ni- E A \u03931 Xn , NP2 E , MC\n\u2192 1/4 as n \u2192 \u221e, since RS NP2 E , MC = T (M1 , M2 , M3 ).\n\nii- E [A (\u03931 (Xn , NPr E , M ))] \u2192 0 as n \u2192 \u221e if M \u2208 T r , since RS (NPr E , M ) = \u2205 for M \u2208 T r .\n\u0011\n\u0010\n\u0011\u0011i\nh \u0010 \u0010\n3/2\n3/2\n\u2192 0 since RS NP E , MC = {MC }.\niii- Furthermore, E A \u03931 Xn , NP E , MC\no\n\n\u03c4\n\u03c4\niv- For any M \u2208 T (Y3 ) , E [A (\u03931 (Xn , NCS\n, M )))] \u2192 0 as n \u2192 \u221e, since RS (NCS\n, M ) = {M }.\n\nv- We also have E [A (\u03931 (Xn , NPr E , MC ))] \u2192 0 for r \u2208 [1, 3/2] as n \u2192 \u221e.\nvi- Furthermore, by careful geometric calculations, we get E [A (\u03931 (Xn , NPr E , MC ))] \u2192\nfor r \u2208 (3/2, 2].\n\u221a \u0002\n\u0003\nvii- E [A (\u03931 (Xn , NPr E , MC ))] \u2192 3/ 4 (1 \u2212 3/r2 ) for r \u2208 (2, \u221e], as n \u2192 \u221e.\nWe also derive the rate of convergence of E [A(\u03931 (Xn , NPr E , MC ))] for r = 3/2.\n\n28\n\n\u221a\n\n3 [1 \u2212 3/(2 r)]\n\n2\n\n\fTheorem 7.7. Let Xn be a random sample from U(T (Y3 )). For r \u0001= 3/2, the expected area of the the\n\u03931 -region, E [A (\u03931 (Xn , NPr E , MC ))], converges to zero, at rate O n\u22122 .\n\nProof: For r = 3/2 and M = MC , and sufficiently large n, \u03931 (Xei , NPr E ) \u2229 RCM (yi ) is a triangle for\ni = 1, 2, 3 w.p. 1. See Figure 17. With the realization of the edge extrema denoted as xei = (xi , yi ) close\nenough to ei , for i = 1, 2, 3,\n\u0011\n\u0010\n3/2\n\u03931 xe1 , NP E \u2229 RCM (y1 ) is the triangle with vertices\n!\n\u221a\n\u221a \u0010\n\u0011\n\u221a\n1\n3\n3\n,\n\u22129 + 2 3 y2 + 6 x2\ny2 + x2 \u2212 , \u2212\n3\n2\n18\n\n\u221a \u0012\n\u0013!\n\u221a\n1\n9\n3\n\u2212 + 2 3 y2 + 6 x2\n,\n,\n2 9\n2\n\n\u0011\n\u0010\n3/2\n\u03931 xe2 , NP E \u2229 RCM (y2 ) is the triangle with vertices\n!\n\u221a \u0010\n\u0011\n\u221a\n1\n3\n,\n,\n3 + 4 3 y3 \u2212 12 x3\n2 18\n\n!\n\u221a\n\u221a \u0010\n\u0011\n\u221a\n1\n3\n3\n,\n\u2212\ny3 + x3 , \u2212\n\u22123 + 2 3 y3 \u2212 6 x3\n2\n3\n18\n\n\u0011\n\u0010\n3/2\nand \u03931 xe3 , NP E \u2229 RCM (y3 ) is the triangle with vertices\n!\n\u221a \u0010\n\u0011 \u221a3 2\n\u221a\n3\n\u2212\n\u2212 3 + 4 y1 ,\n+ y1 ,\n6\n6\n3\n\n\u221a !\n3\n1\n,\n,\n2 6\n\ny3 = 1/2,\n\n\u221a !\n3\n1\n,\n,\n2 6\n\n\u221a !\n1\n3\n,\n,\n2 6\n\n!\n\u221a \u0010\n\u0011 \u221a3 2\n3 \u221a\n3 + 4 y1 ,\n+ y1 .\n6\n6\n3\n\n\u221a \u0001\n3/2\n\ne 1(\nx)\n\nxe2 = (x2, y2)\nMC\nxe3 = (x3, y3)\nxe1 = (x1, y1)\ny1 = (0, 0)\n\ny2 = (1, 0)\n3/2\n\nFigure 17: The shaded regions are the triangular \u03931 (Xei ) \u2229 RCM (yi ) regions for i = 1, 2, 3.\nThen for sufficiently large n,\n\u221a \u0010\n\u00112 \u221a3 \u0010\n\u00112 4 \u221a3\n\u221a\n\u221a\n3\n3 x2 \u2212 3 + 3 y2 +\n\u22123 x3 + 3 y3 +\ny12\n=\nA \u03931\n27\n27\n9\n\u221a \u0010\n\u0011\n\u221a\n\u221a\n\u221a\n3\n3 x22 \u2212 6 x2 + 2 3 y2 x2 \u2212 2 3 y2 + y22 + 3 + y32 \u2212 2 3 y3 x3 + 3 x23 + 4 y12 .\n=\n9\n\u0010\n\n\u0010\n\n3/2\nXn , NP E , M\n\n\u0011\u0011\n\n29\n\n\fy3 = 1/2,\n\n\u221a \u0001\n3/2\n\ne 1(\nx)\n\nxe2 = (x2, y2)\nxe3 = (x3, y3)\n\nMC\n\nxe1 = (x1, y1)\n\ny1 = (0, 0)\n\ny2 = (1, 0)\n\nFigure 18: The figure for the joint pdf of Xei . The shaded region is T (\u03b6).\nTo find the expected area, we need the joint density of the Xei . The edge extrema are all distinct with\nprobability 1 as n \u2192 \u221e (see Theorem 6.4). Let T (\u03b6) be the triangle formed by the lines at xei parallel to ei\nfor i = 1, 2, 3 where \u03b6 = (x1 , y1 , x2 , y2 , x3 , y3 ). See Figure 18.\nThen the asymptotically accurate joint pdf of Xei (Ceyhan and Priebe (2007)) is\nf3 (\u03b6)\n\n=\n=\n\n\u0013n\u22123\n1\nA(T (\u03b6))\nn(n \u2212 1)(n \u2212 2)\nA(T (Y3 ))\nA(T (Y3 ))3\n\u0012\n\u00112 \u0013n\u22123 . \u0010\u221a\n\u0010 \u221a\n\u0011n\n\u221a\n\u221a\n\u221a\n3/36 \u22122 3 y1 + 3 y3 \u2212 3 x3 + 3 y2 + 3 x2\n3/4 .\nn(n \u2212 1)(n \u2212 2)\n\u0012\n\n\b\nwith the support DS = \u03b6 \u2208 R6 : (xi , yi )'s are distinct .\nThen for sufficiently large n,\n\nZ\n\u0011\u0011\n\u0011\u0011i\n\u0010 \u0010\nh \u0010 \u0010\n3/2\n3/2\nf3 (\u03b6)d\u03b6\n\u223c\nA \u03931 Xn , NP E\nE A \u03931 Xn , NP E , M\n=\n\nZ\n\n\u0012\n\u0013n\u22123\n\u0011\u0011\n\u0010 \u0010\nA(T (\u03b6))\n1\n3/2\nn(n \u2212 1)(n \u2212 2)\nA \u03931 Xn , NP E\nd\u03b6.\nA(T (Y3 ))\nA(T (Y3 ))3\n\ni = 1, 2, 3, since G(\u03b6)\nLet G(\u03b6) = A(T (\u03b6))/A(T (Y3 )). Notice that the integrand is critical when xei \u2208 ei for \u221a\nwhen\u221axei \u2208 ei for each i = 1, 2, 3. So we make the change of variables y1 = z1 , y2 = 3 (1 \u2212 x2 ) \u2212 z2 , and\ny3 = 3 x3 \u2212 z3 , then G(\u03b6) and A(\u03931 (Xn , N )) becomes\n\u0010\n\u00112 .\n\u221a\n\u221a\n\u0001\nG(\u03b6) = G(z1 , z2 , z3 ) = 2 z1 + z3 \u2212 3 + z2\n3 and A(\u03931 (Xn , N )) = 3 z22 + z32 + 4 z12 /9,\n\nrespectively. Hence the integrand does not depend on x1 , x2 , x3 and integrating with respect to x1 , x2 , x3\nyields a constant K. Now, the integrand is critical at (z1 , z2 , z3 ) = (0, 0, 0), since G(0, 0, 0) = 1. So\nlet Eu\u03b5 be the event that 0 \u2264 zi \u2264 \u03b5 for i = 1, 2,\nsmall enough. Then making the change\n\u0010 \u03b5 > 0 \u0011\u0011\n\u0010 3 for\n\u0001\n3/2\n= O n\u22122 and G(z1 , z2 , z3 ) becomes\nof variables zi = wi /n for i = 1, 2, 3, we get A \u03931 Xn , NP E\n\n30\n\n\f\u221a\n\u0001\n\u0001\nG(w1 , w2 , w3 ) = 1 \u2212 n1 2/ 3 (2 w1 + w2 + w3 ) + O n\u22122 , hence\nh \u0010 \u0010\n\u0011\u0011i\n3/2\nE A \u03931 Xn , NP E , M\n\u223c\nZ n\u03b5 Z n\u03b5 Z n\u03b5 \u0010 \u0010\n\u0011\u0011\n1\n3/2\nn(n \u2212 1)(n \u2212 2) 3 G(w1 , w2 , w3 )n\u22123 dw1 dw2 dw3 ,\nK\nA \u03931 Xn , NP E\nn\n0\n0\n0\nletting n \u2192 \u221e\nZ \u221eZ \u221eZ \u221e\n\u0010\n\u0011\n\u221a\n\u0001\n\u0001\n\u2248K\nO n\u22122 exp \u22122/ 3 (2 w1 + w2 + w3 ) dw1 dw2 dw3 = O n\u22122 ,\n0\n\n0\n\n0\n\n\u221a\n\u221a\nR\u221eR\u221eR\u221e\n\u0001\nsince\nh \u00100 \u00100 0 exp \u22122/\n\u0011\u0011i 3 (2 w1 + w2 + w3 ) dw1 dw2 dw3 =\u0001 3 3/16 which is a finite constant. Hence\n3/2\nE A \u03931 Xn , NP E , M\n\u2192 0 as n \u2192 \u221e at the rate O n\u22122 . \u0004\n\n8\n\n\u0393k -Regions for Proximity Maps in T (Y3 )\n\nWe can also define the regions associated with \u03b3n (N ) = k for k \u2264 n.\n\nIn\u0002 R with Y2 \u0003= {0, 1}, \u03b3n (N ) \u2264 2, hence we can only define \u03932 -regions. Recall that \u03b3n (N ) = 2 iff\n1+x1:n\nXn \u2229 xn:n\n= \u2205 iff Xn \u2282 [0, 1] \\ \u03931 (Xn , N ). So\n2 ,\n2\n\b\n\u03932 (Xn , N ) =\n(x, y) \u2208 [0, 1]2 : Xn \u2282 N (x) \u222a N (y); x, y \u2208\n/ \u03931 (Xn , N )\nn\no\n2\n2\n=\n(x, y) \u2208 [0, 1] \\ \u03931 (Xn , N ) : Xn \u2282 N (x) \u222a N (y) .\n\nNotice that \u03932 (Xn , N ) \u2286 [0, 1]2 . Let\n\nx\ne1 := argminx\u2208Xn \u2229(0,1/2) (1/2 \u2212 x) and x\ne2 := argminx\u2208Xn \u2229(1/2,1) (x \u2212 1/2),\n\nthen \u03b3n (N ) = 2 iff x\ne1 , x\ne2 \u2208\n/ \u03931 (Xn , N ). In such a case Xn \u2282 N (e\nx1 ) \u222a N (e\nx2 ) by construction.\nIn general,\n\nDefinition 8.1. The \u03932 -region for proximity map NY (*) and set B \u2282 \u03a9 is \u03932 (B, N ) = {(x, y) \u2208 [\u03a9\\\u03931 (B)]2 :\nB \u2286 NY (x) \u222a NY (y)}. In general, \u0393k -region for proximity map NY (*) and set B \u2282 \u03a9 for k = 1, 2, . . . , n is\n\u0393k (B, N ) =\n\n(\n\n(x1 , x2 , . . . , xk ) \u2208 \u03a9k : B \u2286\n\nk\n[\n\ni=1\n\nNY (xi ) and all possible m-permutations (u1 , u2 , . . . , um )\n)\n\nof (x1 , x2 , . . . , xk ) satisfy (u1 , u2 , . . . , um ) 6\u2208 \u0393m (B, N ) for each m = 1, 2, . . . , k \u2212 1 .\u0003\nNote that \u0393k -regions are defined for k \u2264 n and they might be empty. Moreover, \u0393k -regions are in \u03a9k ,\nnot in \u03a9.\nnno\nn o\nA\nLet\ndenote the Stirling partition number for a set of size n into m blocks and let m\ndenote all\nm\nStirling partitions of a set A into m blocks; that is,\n)\n(\n\u001a \u001b\n[\nA\nBi .\n:= {B1 , B2 , . . . , Bm } : Bi 6= \u2205, Bi \u2282 A; A =\nm\ni\nIn particular,\n\nn\n\nXn\n2\n\no\n\nis the unordered pair of blocks B1 and B2 such that Bi 6= \u2205 and Bi \u2282 Xn for i = 1, 2,\n\nand B1 \u222a B2 = Xn . Note that B2 = Xn \\ B1 . Then\n\n31\n\n\fS\n2\nProposition 8.2. \u03932 (Xn , N ) = {B1 ,B2 }\u2208{ Xn } [\u03931 (B1 , N ) \u00d7 \u03931 (B2 , N )] \\ \u03931 (Xn , N ) for any nonempty\n2\nn o\nStirling blocks B1 and B2 in X2n .\n\nProof: Given Xn , suppose (u, v) \u2208 \u03932 (Xn , N ), then Xn \u2282 N (u) \u222a N (v) and u, v \u2208\n/ \u03931 n\n(Xn ,oN ). Let\nB1 = Xn \u2229 N (u), and B2 = [Xn \u2229 N (v)] \\ N (u). Then B1 and B2 are two Stirling blocks in X2n . Hence\nB1 \u2282 N (u) \u21d2 u \u2208 \u03931 (B1 , N ) \\ \u03931 (Xn , N ) and B2 \u2282 N (v) \u21d2 v \u2208 \u03931 (B2 , N ) \\ \u03931 (Xn , N ), hence \u03932 (Xn , N ) \u2286\nS\n2\n{B1 ,B2 }\u2208{ X2n } [\u03931 (B1 , N ) \u00d7 \u03931 (B2 , N )] \\ (\u03931 (Xn , N )) . The other direction is trivial, hence the desired\nresult follows. \u0004\nIn R with N = NS , we can exploit the natural ordering available.\nProposition 8.3. In R with Y2 = {0, 1}, \u03932 (Xn , NS ) =\nS\n\nn\u22121\n[\u0010\nk=1\n\nxk:n xn:n \u0011\n,\n\u00d7\n2\n2\n\n\u0012\n\n\u0013\n1 + x1:n 1 + x(k+1):n\n.\n,\n2\n2\n2\n\nProof: Recall that \u03932 (Xn , NS ) = {B1 ,B2 }\u2208{ Xn } [\u03931 (B1 , NS ) \u00d7 \u03931 (B2 , NS )] \\ \u03931 (Xn , NS ) . Let {B1 , B2 } be\n2\nn o\na Stirling partition in X2n . First observe that B1 = {x1:n , . . . , xk:n } and B2 = {x(k+1):n , . . . , xn:n } forms a\nStirling partition of Xn , and\n!\n\u0015 \u0014\n\u0015 \"\n\u0014\nxk:n xn:n\nxn:n 1 + x1:n\nxk:n 1 + x1:n\n\\\n=\n,\n,\n,\n\u03931 ({x1:n , . . . , xk:n }, NS ) \\ \u03931 (Xn , NS ) =\n2\n2\n2\n2\n2\n2\nand\n\u03931 {x(k+1) , . . . , xn:n }, NS\n\n\u0001\n\n\u0015 \u0014\n\u0015\nxn:n 1 + x1:n\nxn:n 1 + x(k+1)\n\\\n=\n,\n,\n\\ \u03931 (Xn , NS ) =\n2\n2\n2\n2\n\u0014\n\n#\n1 + x1:n 1 + x(k+1)\n.\n,\n2\n2\n\nFurthermore,\n\u0001\n\u03931 ({x1:n , . . . , xk:n }, NS ) \u00d7 \u03931 {x(k+1) , . . . , xn:n }, NS \u2287\n\n[\n\n{B1 ,B2 }\u2208\u039b\n\n\u03931 (B1 , NS ) \u00d7 \u03931 (B2 , NS )\n\n(4)\n\nwhere\n\u039b=\n\n\u001a\n\n{B1 , B2 } \u2208\n\n\u001a\n\nXn\n2\n\n\u001b\n\n\u001b\n: x1:n \u2208 B1 , xn:n \u2208 B2 , max (B1 ) = xk:n , min (B2 ) = x(l) , l < k ,\n\nsince the left hand side in Equation (4) is\nfollows. \u0004\n\nhx\n\n\u0014\n\u0015\n1 + x1:n 1 + x(k\u22121)\nxn:n i\n\u00d7\n. Hence the desired result\n,\n,\n2\n2\n2\n2\n\nk:n\n\nSimilarly, for NPr E ,\n\u03932 (Xn , NPr E ) =\n\n[\n\n{B1 ,B2 }\u2208{ X2n }\n\n2\n\n[\u03931 (B1 , NPr E , M ) \u00d7 \u03931 (B2 , NPr E , M )] \\ \u03931 (Xn , NPr E , M ) .\n\nNote that \u03931 (Bi , NPr E , M ) is determined by the edge extrema in Bi , i = 1, 2. Furthermore, if (u, v) \u2208\n\u03932 (Xn , NPr E , M ), then (u, v) \u2208\n/ RM (y)2 , since either NPr E (u) \u2286 NPr E (v) or NPr E (v) \u2286 NPr E (u) should hold if\n2\n(u, v) \u2208 RM (y) .\n\u03c4\nFor NCS\n,\n\n\u03c4\n\u03932 (Xn , NCS\n, M) =\n\n[\n\n{B1 ,B2 }\u2208{ X2n }\n\n2\n\n\u03c4\n\u03c4\n\u03c4\n[\u03931 (B1 , NCS\n, M ) \u00d7 \u03931 (B2 , NCS\n, M )] \\ \u03931 (Xn , NCS\n, M) .\n\n32\n\n\f\u03c4\n\u03c4\nNote that \u03931 (Bi , NCS\n, M ) is determined by the edge extrema of Bi , i = 1, 2. But if (u, v) \u2208 \u03932 (Xn , NCS\n, M ),\nthen (u, v) \u2208 RM (e)2 can hold. In these proximity regions, not all edge extrema should fall in the same\n\u03c4\npartition set for \u0393k (Xn , NCS\n, M ) to be nonempty.\n\nFor any proximity map N ,\nP (\u03b3n (N ) = 2) = P (Xn2 \u2229 \u03932 (Xn , N ) 6= \u2205, \u03b3n (N ) 6= 1)\n\uf8f9\n\uf8ee\n\uf8f6\n\uf8eb\n[\n\\\n\uf8fa\n\uf8ef\n\uf8f7\n\uf8ec\n[\u03931 (B1 , N ) \u00d7 \u03931 (B2 , N )] \\ (\u03931 (Xn , N ))2 \uf8fb 6= \u2205, Xn \u2229 \u03931 (Xn , N ) = \u2205\uf8f8 .\n= P \uf8edXn2\n\uf8f0\n{B1 ,B2 }\u2208{ X2n }\n\nA more compact way to write this is as\n\nwhere \u0393\u22642 (Xn , N ) :=\n\nS\n\nP (\u03b3n (N ) > 2) = P Xn2 \u2229 \u0393\u22642 (Xn , N ) = \u2205\n{B1 ,B2 }\u2208{ X2n }\n\n\u03931 (B1 , N ) \u00d7 \u03931 (B2 , N ).\n\n\u0001\n\nFurthermore, for k \u2265 3, the \u0393k -regions are defined similarly as\n[\n\u0393k (Xn , N ) =\n[\u03931 (B1 , N ) \u00d7 . . . \u00d7 \u03931 (Bk , N )] \\ \u03931 (Xn , N )k .\n{B1 ,...,Bk }\u2208{ Xkn }\nHence,\n\u0001\nP (\u03b3n (N ) = k) = P Xnk \u2229 \u0393k (Xn , N ) 6= \u2205, \u03b3n (N ) > k \u2212 1\n\uf8eb\n\uf8ee\n\uf8f9\n\uf8f6\n[\n\uf8ec\n\uf8ef\n\uf8f7\nk\uf8fa\n= P \uf8edXnk \u2229 \uf8f0\n\u03931 (B1 , N ) . . . \u00d7 \u03931 (Bk , N ) \\ \u03931 (Xn , N ) \uf8fb 6= \u2205\uf8f8 .\n{B1 ,...,Bk }\u2208{ Xkn }\n\nA more compact way to write this is as\n\nwhere \u0393\u2264k (Xn , N ) :=\n\n9\n\nS\n\nP (\u03b3n (N ) > k) = P Xnk \u2229 \u0393\u2264k (Xn , N ) 6= \u2205\n\n{B1 ,...,Bk }\u2208{ Xkn }\n\n\u03931 (B1 , N ) \u00d7 . . . \u00d7 \u03931 (Bk , N ).\n\n\u0001\n\n\u03ba-Values for the Proximity Maps in T (Y3)\n\nRecall that the domination number, \u03b3n (N ) is the cardinality of a minimum dominating set of the PCD based\non N . So by definition, \u03b3n (N ) \u2264 n. We will seek an a.s. least upper bound for \u03b3n (N ) which suggests the\nfollowing concept.\nDefinition 9.1. Let Xn be a random sample from F on T (Y3 ) and let \u03b3n (N ) be the domination number\nfor the PCD based on a proximity map N . The general a.s. least upper bound for \u03b3n (N ) that works for all\nn \u2265 1 is called the \u03ba-value; i.e., \u03ban (N ) := min{k(n) : \u03b3 (Xn , N ) \u2264 k(n) a.s. for all n \u2265 1}. \u0003\nIt is more desirable to have a \u03ba-value that is independent of n. Further, if \u03ban (N ) = \u03ba exists for N and\nis independent of n, then the domination number has the following discrete probability mass function:\n\uf8f1\n1 w.p. p1 ,\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f22 w.p. p2 ,\n\u03b3 (Xn , N ) = .\n..\n..\n..\n\uf8f4\n\uf8f4\n.,\n.\n\uf8f4\n\uf8f4\n\uf8f3\n\u03ba w.p. p\u03ba .\n33\n\n\fy3\n\nPSfrag replacements\nM\n\ny1\n(x1 , y1 )\n\ny2\n(xn , yn )\n\n(x2 , y2 )\n\n\u03c4\nFigure 19: The figure for \u03ban (NCS\n) = n.\n\nIn R with Y2 = {0, 1}, for Xn a random sample from U(0, 1), \u03b3n (N ) \u2264 2 with equality holding with\npositive probability for N \u2208 {NS , NAS }. Hence \u03ban (NS ) = 2. But in Rd with d > 1, finding \u03ban (N ) for\n\u03c4\nN \u2208 {NS , NAS } is an open problem. Next, we investigate the \u03ba-values for {NAS , NPr E , NCS\n} in R2 .\nTheorem 9.2. Let Xn be a random sample from U(T (Y3 )), and M \u2208 R2 \\Y3 . For NPr E (*, M ), \u03ban (NPr E ) = 3.\nProof: For NPr E (*, M ), pick the point closest to edge ei in vertex region RM (yi ); that is, pick Ui \u2208\nargminX\u2208Xn \u2229RM (yi ) d(X, ei ) = argmaxX\u2208Xn \u2229RM (yi ) d(l(y, X), yi ) in the vertex region for which Xn \u2229RM (yi ) 6=\n\u2205 for i \u2208 {1, 2, 3} (note that as n \u2192 \u221e, Xn \u2229 RM (yi ) 6= \u2205 for all i \u2208 {1, 2, 3} a.s., and also Ui is unique a.s.\nS3\nfor each i, since X is from U(T (Y3 ))). Then Xn \u2229 RM (yi ) \u2282 NPr E (Ui , M ). Hence Xn \u2282 i=1 NPr E (Ui , M ).\nSo \u03b3n (NPr E , MC ) \u2264 3 with equality holding with positive probability. Thus \u03ban (NPr E ) = 3. \u0004\n\u03c4\nThere is no least upper bound for \u03b3n (NCS\n, M ) that works for all n > 0 as shown below.\n\n\u03c4\nTheorem 9.3. Let Xn be random sample from U(T (Y3 )). Then \u03b3n (NCS\n, M ) = n holds with positive\nprobability for all \u03c4 \u2208 [0, 1].\n\nProof: For \u03c4 = 0, the result follows trivially. For \u03c4 = 1, we will prove the theorem by showing that there\n\u03c4 =1\nis a union of n regions of positive area in T (Y3 ), so that u \u2208 NCS\n(v, M ) iff u = v, for any u, v \u2208 Xn . Let\no\nM = (m1 , m2 ) \u2208 T (Y3 ) . In RM (e3 ) locate n triangles evenly on e3 with base length 1/n and similar to\nT (Y3 ) (with similarity ratio 1/n). See also Figure 19. Then locate n points in each triangle at zi = (xi , yi )\nsuch that (xi , yi ) is the same type of center of Ti as M is of T (Y3 ). Then using the similarity ratio\n\u03c4 =1\nof NCS\n(zi , M ) to T (Y3 ), namely, yi /m2 = 1/n, we get yi = m2 /n for all i = 1, 2, . . . , n. Moreover,\n\u03c4 =1\nxi \u2212xi\u22121 = 1/n for i = 2, 3, . . . , n with x1 = m1 /n and xn = 1\u2212(1\u2212m1)/n. Then (xi , yi ) \u2208 NCS\n((xj , yj ), M )\niff i = j. Furthermore, for sufficiently small \u03b5 > 0, the same holds for the \u03b5 neighborhood of each zi = (xi , yi ).\n\u03c4 =1\nThat is, NCS\n(x, M )\u2229B(zj , \u03b5) = \u2205 for all x \u2208 B(zi , \u03b5) for any distinct pair i, j \u2208 {1, 2, . . . , n}, and probability\n\u03c4\nof Xn being composed of n points one from each B(zi , \u03b5) is positive. Then \u03b3n (NCS\n, M ) = n holds with\npositive probability. The result for \u03c4 \u2208 (0, 1) follows similarly. \u0004\n\n34\n\n\f9.1\n\nCharacterization of Proximity Maps Using \u03ba-Values\n\n\u03c4\nNotice that for the proximity maps we have considered, \u03ban (NPr E ) = 3 and \u03ban (NCS\n) = n. One property of\nproximity maps that makes \u03ban (N ) < n is that probability of having an Xn for which N (X) \u2229 Xn = {X} for\nall X \u2208 Xn is zero.\n\nBelow we state a condition for \u03ban (N (*, M )) \u2264 3 for N (*, M ) defined with M -vertex regions.\n\nTheorem 9.4. Suppose N (*, M ) is defined with M -vertex regions with M \u2208 R2 \\ Y3 and N (x, M ) gets larger\nas d(l(y, x), y) increases for x \u2208 RM (y) in the sense that N (x, M ) \u2286 N (z, M ) for all x, z \u2208 RM (y) when\nd(l(y, x), y) \u2264 d(l(y, z), y). Then \u03ban (N ) \u2264 3.\nProof: When Xn \u2229 RM (yi ) 6= \u2205, pick one of the points Ui (n) \u2208 argmaxX\u2208Xn \u2229RM (yi ) d(l(yi , X), yi ), then\nXn \u2229 RM (yi ) \u2282 N (Ui (n)) for each i \u2208 {1, 2, 3}. So \u03b3 (Xn , N, M ) \u2264 3, and hence \u03ban (N ) \u2264 3. \u0004\nNotice that NPr E satisfies the conditions of Theorem 9.1.\n\nTheorem 9.5. Suppose N (*, M ) is defined with M -edge regions and N (x, M ) gets larger as d(x, e) increases\nfor x \u2208 RM (e) in the sense that N (x, M ) \u2286 N (y, M ) for all x, y \u2208 RM (e) when d(x, e) \u2264 d(y, e). Then\n\u03ban (N ) \u2264 3.\nProof: When Xn \u2229 RM (ei ) 6= \u2205, pick one of the points Uei (n) \u2208 argmaxX\u2208Xn \u2229RM (ei ) d(X, ei ). Then\nXn \u2229 RM (ei ) \u2282 N (Ue (n)) for each i \u2208 {1, 2, 3}. So \u03b3 (Xn , N, M ) \u2264 3, and hence \u03ban (N ) = 3. \u0004\n\nIn Theorems 9.1 and 9.1, we have an upper bound for \u03ban (N ). To determine an exact value for \u03ban (N )\nwe need further restrictions. Let A1 := {x \u2208 T (Y3 ) : \u03bc(N (x, M )) = \u03bc(T (Y3 ))} and A2 := {(x, y) \u2208\nT (Y3 ) \u00d7 T (Y3 ) : \u03bc(N (x, M ) \u222a N (y, M )) = \u03bc(T (Y3 ))}. If in addition to the hypothesis of Theorem (and )\nwe have A1 and A2 have zero measure (e.g., zero area for continuous distributions with support in T (Y3 ))\nthen \u03ban (N ) = 3 would hold.\n\n10\n\nDiscussion and Conclusions\n\nIn this article, we provide a probabilistic characterization of proximity maps, associated regions, and digraphs\nand related quantities. In particular, we discuss the probabilistic behavior of proximity regions, superset\nregions, and \u03931 -regions; construct digraphs (proximity catch digraphs(PCDs)) and investigate related quantities such as domination number, \u03b7 and kappa values. We also provide auxiliary tools such as vertex and\nedge regions for the construction of proximity regions.\nAlthough \u03931 -regions and superset regions were introduced before (Ceyhan and Priebe (2005),Ceyhan et al.\n(2006), and Ceyhan and Priebe (2007)) a thorough investigation is only performed in this article. G1 -regions\nare a sort of a \"dual\" of proximity regions and are associated with domination number being equal to 1. We\nprovide a probabilistic characterization of \u03931 -regions for general proximity maps N and data points from\ndistribution F . We also extend this concept by introducing \u0393k -regions, which are associated with domination\nnumber being equal to k.\nWe introduce the quantities related to \u03931 -regions and domination number, namely, \u03b7-values and \u03bavalues. \u03b7-value is the minimum number of points in a set required to determine the \u03931 -region for that set.\nWe determine some general conditions that make \u03b7n (N ) \u2264 3 for data in the triangle T (Y3 ). \u03ba-value is the\na.s. least upper bound for the domination number of the PCDs. We also determine some general conditions\nthat make \u03ban (N ) \u2264 3 for data in the triangle T (Y3 ).\n\nWe provide two PCD families, namely proportional-edge PCDs (Ceyhan et al. (2006) and central similarity PCDs (Ceyhan et al. (2007)) as illustrative examples. We discuss the construction of proximity regions\nand \u03931 -regions for these PCDs. Furthermore, we calculate the expected area of \u03931 -regions for these PCDs\nin the limit.\n\n35\n\n\fDetermining \u03931 -regions, \u03b7 and \u03ba values for spherical and arc-slice PCDs contain many open problems\nand are subjects of ongoing research.\nWith the above characterizations, given another PCD, then we can determine how it behaves in terms of\n\u03931 -regions and related quantities; in particular we can determine a.s. least upper bounds for the domination\nnumber of the new PCD.\n\nAcknowledgments\nThis research was supported by the research agency TUBITAK via the Kariyer Project # 107T647.\n\nReferences\nBillingsley, P. (1995). Probability and Measure. Wiley-Interscience, New York, NY.\nBoland, P. J., Singh, H., and Cukic, B. (2004). The stochastic precedence ordering with applications in\nsampling and testing. Journal of Applied Probability, 41(1):73\u201382.\nCeyhan, E. (2004). An Investigation of Proximity Catch Digraphs in Delaunay Tessellations. PhD thesis,\nThe Johns Hopkins University, Baltimore, MD, 21218.\nCeyhan, E. (2009). Extension of one-dimensional proximity regions to higher dimensions. arXiv:0902.1306v1\n[math.MG]. Technical Report # KU-EC-08-2.\nCeyhan, E. and Priebe, C. E. (2005). The use of domination number of a random proximity catch digraph\nfor testing spatial patterns of segregation and association. Statistics & Probability Letters, 73:37\u201350.\nCeyhan, E. and Priebe, C. E. (2007). On the distribution of the domination number of a new family of\nparametrized random digraphs. Model Assisted Statistics and Applications, 1(4):231\u2013255.\nCeyhan, E., Priebe, C. E., and Marchette, D. J. (2007). A new family of random graphs for testing spatial\nsegregation. Canadian Journal of Statistics, 35(1):27\u201350.\nCeyhan, E., Priebe, C. E., and Wierman, J. C. (2006). Relative density of the random r-factor proximity\ncatch digraphs for testing spatial patterns of segregation and association. Computational Statistics & Data\nAnalysis, 50(8):1925\u20131964.\nChartrand, G. and Lesniak, L. (1996). Graphs & Digraphs. Chapman & Hall/CRC Press LLC, Florida.\nDeVinney, J. (2003). The Class Cover Problem and its Applications in Pattern Recognition. PhD thesis,\nThe Johns Hopkins University, Baltimore, MD, 21218.\nDeVinney, J. and Priebe, C. E. (2006). A new family of proximity graphs: Class cover catch digraphs.\nDiscrete Applied Mathematics, 154(14):1975\u20131982.\nDeVinney,\nJ.,\nPriebe,\nC.\nE.,\nMarchette,\nD.\nJ.,\nand\nSocolinsky,\nD.\n(2002).\nRandom\nwalks\nand\ncatch\ndigraphs\nin\nclassification.\nhttp://www.galaxy.gmu.edu/interface/I02/I2002Proceedings/DeVinneyJason/DeVinneyJason.paper.pdf.\nProceedings of the 34th Symposium on the Interface: Computing Science and Statistics, Vol. 34.\nDeVinney, J. and Wierman, J. C. (2003). A SLLN for a one-dimensional class cover problem. Statistics &\nProbability Letters, 59(4):425\u2013435.\nDevroye, L., Gyorfi, L., and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition. Springer\nVerlag, New York.\n36\n\n\fGarfinkel, R. S. and Nemhauser, G. L. (1972). Integer Programming. John Wiley & Sons, New York.\nJanson, S., Luczak, T., and Rucin\u0144ski, A. (2000). Random Graphs. Wiley-Interscience Series in Discrete\nMathematics and Optimization, John Wiley & Sons, Inc., New York.\nJaromczyk, J. W. and Toussaint, G. T. (1992). Relative neighborhood graphs and their relatives. Proceedings\nof IEEE, 80:1502\u20131517.\nKarr, A. F. (1992). Probability. Springer-Verlag, New York.\nLee, C. (1998). Domination in digraphs. Journal of Korean Mathematical Society, 4:843\u2013853.\nLiu, R. Y., Parelius, J. M., and Singh, K. (1999). Multivariate analysis by data depth: Descriptive statistics\ngraphics and inference (with discussion). The Annals of Statistics, 27:783\u2013858.\nMarchette, D. J. and Priebe, C. E. (2003). Characterizing the scale dimension of a high dimensional classification problem. Pattern Recognition, 36(1):45\u201360.\nOkabe, A., Boots, B., and Sugihara, K. (2000). Spatial Tessellations: Concepts and Applications of Voronoi\nDiagrams. Wiley.\nPaterson, M. S. and Yao, F. F. (1992). On nearest neighbor graphs. In Proceedings of 19th Int. Coll.\nAutomata, Languages and Programming, Springer LNCS, volume 623, pages 416\u2013426.\nPriebe, C. E., DeVinney, J. G., and Marchette, D. J. (2001). On the distribution of the domination number\nof random class cover catch digraphs. Statistics & Probability Letters, 55:239\u2013246.\nPriebe, C. E., Marchette, D. J., DeVinney, J., and Socolinsky, D. (2003a). Classification using class cover\ncatch digraphs. Journal of Classification, 20(1):3\u201323.\nPriebe, C. E., Solka, J. L., Marchette, D. J., and Clark, B. T. (2003b). Class cover catch digraphs for\nlatent class discovery in gene expression monitoring by DNA microarrays. Computational Statistics &\nData Analysis on Visualization, 43-4:621\u2013632.\nSen, M., Das, S., Roy, A., and West, D. (1989). Interval digraphs: An analogue of interval graphs. Journal\nof Graph Theory, 13:189\u2013202.\nToussaint, G. T. (1980). The relative neighborhood graph of a finite planar set. Pattern Recognition,\n12(4):261\u2013268.\nTuza, Z. (1994). Inequalities for minimal covering sets in sets in set systems of given rank. Discrete Applied\nMathematics, 51:187\u2013195.\n\n37\n\n\f"}