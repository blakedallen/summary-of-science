{"id": "http://arxiv.org/abs/1110.1580v1", "guidislink": true, "updated": "2011-10-07T16:39:34Z", "updated_parsed": [2011, 10, 7, 16, 39, 34, 4, 280, 0], "published": "2011-10-07T16:39:34Z", "published_parsed": [2011, 10, 7, 16, 39, 34, 4, 280, 0], "title": "A Polylogarithmic-Competitive Algorithm for the k-Server Problem", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.6287%2C1110.2418%2C1110.6612%2C1110.1839%2C1110.1414%2C1110.6077%2C1110.4465%2C1110.1133%2C1110.1007%2C1110.2621%2C1110.4084%2C1110.0986%2C1110.5808%2C1110.0285%2C1110.6592%2C1110.5066%2C1110.6104%2C1110.1054%2C1110.1444%2C1110.5159%2C1110.1842%2C1110.2624%2C1110.2030%2C1110.5507%2C1110.0229%2C1110.5917%2C1110.1607%2C1110.3678%2C1110.5970%2C1110.1967%2C1110.1358%2C1110.0093%2C1110.3202%2C1110.0885%2C1110.0346%2C1110.1109%2C1110.1849%2C1110.6893%2C1110.0413%2C1110.3643%2C1110.1891%2C1110.5064%2C1110.5805%2C1110.2983%2C1110.3363%2C1110.2960%2C1110.5773%2C1110.5194%2C1110.6145%2C1110.5673%2C1110.0221%2C1110.0168%2C1110.4800%2C1110.2299%2C1110.4696%2C1110.0678%2C1110.3855%2C1110.6216%2C1110.6691%2C1110.4688%2C1110.5353%2C1110.2184%2C1110.4427%2C1110.0656%2C1110.6278%2C1110.4481%2C1110.6629%2C1110.6620%2C1110.3692%2C1110.6666%2C1110.2410%2C1110.3306%2C1110.3420%2C1110.1589%2C1110.3908%2C1110.4705%2C1110.5593%2C1110.4441%2C1110.5389%2C1110.2628%2C1110.5704%2C1110.6332%2C1110.2823%2C1110.6256%2C1110.5078%2C1110.5992%2C1110.1253%2C1110.5817%2C1110.5542%2C1110.3048%2C1110.5604%2C1110.5370%2C1110.1472%2C1110.5962%2C1110.1002%2C1110.6408%2C1110.1580%2C1110.5390%2C1110.5850%2C1110.6130%2C1110.0871&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Polylogarithmic-Competitive Algorithm for the k-Server Problem"}, "summary": "We give the first polylogarithmic-competitive randomized online algorithm for\nthe $k$-server problem on an arbitrary finite metric space. In particular, our\nalgorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any\nmetric space on n points. Our algorithm improves upon the deterministic\n(2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95]\nwhenever n is sub-exponential in k.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.6287%2C1110.2418%2C1110.6612%2C1110.1839%2C1110.1414%2C1110.6077%2C1110.4465%2C1110.1133%2C1110.1007%2C1110.2621%2C1110.4084%2C1110.0986%2C1110.5808%2C1110.0285%2C1110.6592%2C1110.5066%2C1110.6104%2C1110.1054%2C1110.1444%2C1110.5159%2C1110.1842%2C1110.2624%2C1110.2030%2C1110.5507%2C1110.0229%2C1110.5917%2C1110.1607%2C1110.3678%2C1110.5970%2C1110.1967%2C1110.1358%2C1110.0093%2C1110.3202%2C1110.0885%2C1110.0346%2C1110.1109%2C1110.1849%2C1110.6893%2C1110.0413%2C1110.3643%2C1110.1891%2C1110.5064%2C1110.5805%2C1110.2983%2C1110.3363%2C1110.2960%2C1110.5773%2C1110.5194%2C1110.6145%2C1110.5673%2C1110.0221%2C1110.0168%2C1110.4800%2C1110.2299%2C1110.4696%2C1110.0678%2C1110.3855%2C1110.6216%2C1110.6691%2C1110.4688%2C1110.5353%2C1110.2184%2C1110.4427%2C1110.0656%2C1110.6278%2C1110.4481%2C1110.6629%2C1110.6620%2C1110.3692%2C1110.6666%2C1110.2410%2C1110.3306%2C1110.3420%2C1110.1589%2C1110.3908%2C1110.4705%2C1110.5593%2C1110.4441%2C1110.5389%2C1110.2628%2C1110.5704%2C1110.6332%2C1110.2823%2C1110.6256%2C1110.5078%2C1110.5992%2C1110.1253%2C1110.5817%2C1110.5542%2C1110.3048%2C1110.5604%2C1110.5370%2C1110.1472%2C1110.5962%2C1110.1002%2C1110.6408%2C1110.1580%2C1110.5390%2C1110.5850%2C1110.6130%2C1110.0871&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We give the first polylogarithmic-competitive randomized online algorithm for\nthe $k$-server problem on an arbitrary finite metric space. In particular, our\nalgorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any\nmetric space on n points. Our algorithm improves upon the deterministic\n(2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95]\nwhenever n is sub-exponential in k."}, "authors": ["Nikhil Bansal", "Niv Buchbinder", "Aleksander Madry", "Joseph", "Naor"], "author_detail": {"name": "Naor"}, "author": "Naor", "links": [{"href": "http://arxiv.org/abs/1110.1580v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1110.1580v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1110.1580v1", "affiliation": "Seffi", "arxiv_url": "http://arxiv.org/abs/1110.1580v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "A Polylogarithmic-Competitive Algorithm for the\nk-Server Problem\n\narXiv:1110.1580v1 [cs.DS] 7 Oct 2011\n\nNikhil Bansal\n\n\u2217\n\nNiv Buchbinder\n\n\u2020\n\nAleksander M\u0105dry\n\n\u2021\n\nJoseph (Seffi) Naor\n\n\u00a7\n\nAbstract\nWe give the first polylogarithmic-competitive randomized online algorithm for the k-server\nproblem on an arbitrary finite metric space. In particular, our algorithm achieves a competitive\n3\ne\nratio of O(log\nn log2 k) for any metric space on n points. Our algorithm improves upon the\ndeterministic (2k \u2212 1)-competitive algorithm of Koutsoupias and Papadimitriou [23] whenever\nn is sub-exponential in k.\n\n1\n\nIntroduction\n\nThe k-server problem is one of the most fundamental and extensively studied problems in online\ncomputation. Suppose there is an n-point metric space and k servers are located at some of the\npoints of the metric space. At each time step, an online algorithm is given a request at one of the\npoints of the metric space, and this request is served by moving a server to the requested point (if\nthere is no server there already). The cost of serving a request is defined to be the distance traveled\nby the server. Given a sequence of requests, the task is to devise an online strategy minimizing the\nsum of the costs of serving the requests.\nThe k-server problem was originally proposed by Manasse et al. [24] as a broad generalization of\nvarious online problems. The most well studied problem among them is the paging (also known as\ncaching) problem, in which there is a cache that can hold up to k pages out of a universe of n pages.\nAt each time step a page is requested; if the page is already in the cache then no cost is incurred,\notherwise it must be brought into the cache (possibly causing an eviction of some other page) at\na cost of one unit. It is easy to see that the paging problem is equivalent to the k-server problem\non a uniform metric space, and already in their seminal paper on competitive analysis, Sleator\nand Tarjan [27] gave several k-competitive algorithms for paging, and showed that no deterministic\nalgorithm can do better. This prompted Manasse et al. [24] to state a far-reaching conjecture that\na similar result holds for an arbitrary metric. More precisely, they conjectured that there is a\nk-competitive online algorithm for the k-server problem on any metric space and for any value of\nk. This conjecture is known as as the k-server conjecture.\n\u2217\n\nTechnical University of Eindhoven, Netherlands. E-mail: n.bansal@tue.nl.\nComputer Science Department, Open University of Israel. E-mail: niv.buchbinder@gmail.com. Supported by\nISF grant 954/11 and BSF grant 2010426.\n\u2021\nMicrosoft Research, Cambridge, MA USA. E-mail: madry@mit.edu. Research done while at the Computer\nScience and Artificial Intelligence Laboratory, MIT, Cambridge, MA USA, and partially supported by NSF grant\nCCF-0829878 and by ONR grant N00014-11-1-0053.\n\u00a7\nComputer Science Department, Technion, Haifa, Israel. E-mail: naor@cs.technion.ac.il. Supported by ISF grant\n954/11 and BSF grant 2010426.\n\u2020\n\n1\n\n\fAt the time that the k-server conjecture was stated, an online algorithm with competitive\nratio that depends only on k was not known. It was first obtained by Fiat et al. [19]. Improved\nbounds were obtained later on by [22, 9], though the ratio still remained exponential in k. A major\nbreakthrough was achieved by Koutsoupias and Papadimitriou [23], who showed that so-called work\nfunction algorithm is (2k \u2212 1)-competitive. This result is almost optimal, since we know that any\ndeterministic algorithm has to be at least k-competitive. We note that a tight competitive factor\nof k is only known for special metrics such as the uniform metric, line metric, and \u2013 more generally\n\u2013 trees [14, 15].\nEven though the aforementioned results are all deterministic, there is also a great deal of interest\nin randomized algorithms for the k-server problem. This is motivated primarily by the fact that\nrandomized online algorithms (i.e., algorithms working against an oblivious adversary) tend to\nhave much better performance than their deterministic counterparts. For example, for the paging\nproblem, several O(log k)-competitive algorithms are known [20, 25, 1, 2], as well as a lower bound\nof \u03a9(log k) on the competitive ratio.\nUnfortunately, our understanding of the k-server problem when randomization is allowed is\nmuch weaker than in the deterministic case. Despite much work [12, 8, 10], no better lower bound\nthan \u03a9(log k) is known on competitive factors in the randomized setting. Conversely, no better\nupper bound, other than the deterministic guarantee of 2k \u2212 1 [23] mentioned above, is known\nfor general metrics. Thus, an exponential gap still remains between the known lower and upper\nbounds.\nGiven the lack of any lower bounds better than \u03a9(log k), it is widely believed that there is an\nO(log k)-competitive randomized algorithm for the k-server problem on every metric space against\nan oblivious adversary - this belief is captured by the so-called randomized k-server conjecture.\nUnfortunately, besides the previously-mentioned O(log k)-competitive algorithm for the case of a\nuniform metric, even when we allow the competitiveness to depend on other parameters of the\nmetric, such as the number of points n, or the diameter \u2206, non-trivial guarantees are known only\nfor very few special cases. For example, the case of a well-separated metric [26], the case of a metric\ncorresponding to a binary HST with high separation [16], the case of n = k + O(1) [7], as well as\nsome other cases [17, 3, 4]. For the weighted paging problem1 , [2] gave an O(log k)-competitive\nalgorithm (see also [3]) which is based on the online primal-dual approach. However, no non-trivial\nguarantees are known even for very simple extensions of the uniform metric, e.g., two-level HSTs\nwith high separation.\nFor a more in-depth treatment of the extensive literature on both paging and the k-server\nproblem, we suggest [13].\n\n1.1\n\nOur Result\n\nWe give the first polylogarithmic competitive algorithm for the k-server problem on a general metric\nwith a finite number of points n. More precisely, our main result is the following.\nTheorem 1. There is a randomized algorithm for the k-server problem that achieves a competitive\n2\ne\nratio of O(log2 k log3 n log log n) = O(log\nk log3 n) on any metric space on n points.\n\nThe starting point of our algorithm is the recent approach proposed by Cot\u00e9 et al. [16] for\nsolving the k-server problem on hierarchically well-separated trees (HSTs). It is well known that\n1\n\nIn weighted paging, arbitrary weights are associated with fetching the pages into the cache. This problem\ncorresponds to the k-server problem on a weighted star.\n\n2\n\n\fsolving the problem on HSTs suffices, as any metric space can be embedded into a probability\ndistribution over HSTs with low distortion [18].\nMore precisely, Cot\u00e9 et al. defined a problem on uniform metrics which we call the allocation\nproblem. They showed that an online randomized algorithm for the allocation problem that provides\ncertain refined competitive guarantees can be used as a building block to recursively solve the kserver problem on an HST, provided the HST is sufficiently well-separated. Roughly speaking, in\ntheir construction, each internal node of the HST runs an instance of the allocation problem that\ndetermines how to distribute the available servers among its children nodes. Starting from the\nroot, which has k servers, the recursive calls to the allocation problem determine the number of\nservers at each leaf of the HST, giving a valid k-server solution. The guarantee of this k-server\nsolution depends on both the guarantees for the allocation problem, as well as the depth of the\nHST (i.e., the number of levels of recursion). The guarantees obtained by Cot\u00e9 et al. [16] for the\nallocation problem on a metric space with two points allowed them to obtain an algorithm for the\nk-server problem on a sufficiently well-separated binary HST having a competitive ratio that is\npolylogarithmic in k, n, and the diameter \u2206 of the underlying metric space. Unfortunately, the\nfact that the HST has to be binary as well as have a sufficiently good separation severely restricts\nthe metric spaces to which this algorithm can be applied.\nGiven the result of Cot\u00e9 et al. [16], a natural approach to establishing our result is coming\nup with a randomized algorithm having the required refined guarantees for the allocation problem\non an arbitrary number of points. However, it is unclear to us how to obtain such an algorithm.\nInstead, we pursue a more refined approach to solving the k-server problem via the allocation\nproblem. By doing so we are able to bypass the need for a \"true\" randomized algorithm for the\nallocation problem and are able to work with a (much) weaker formulation. More precisely, our\nresult consists of three main parts.\n1. We show that instead of obtaining a randomized algorithm for the allocation problem, it suffices to obtain an algorithm for a certain fractional relaxation of it. Employing this relaxation\nmakes the task of designing such a fractional allocation algorithm easier than designing the\nversion of the allocation problem that was considered earlier. Next, building upon the arguments in Cot\u00e9 et al. [16], we show that a sufficiently good online algorithm for this fractional\nallocation problem can be used as a building block to obtain a good fractional solution to\nthe k-server problem on an HST. Finally, by proving that such a fractional k-server solution\ncan be rounded in an online randomized manner, while losing only an O(1) factor in the\ncompetitive ratio, we get a reduction of the k-server problem to our version of the fractional\nallocation problem.\nAn interesting feature of this reduction is that our fractional relaxation is too weak to give\nanything better than an O(k) guarantee for the (integral) allocation problem, since there are\ninstances on which any integral solution must pay \u03a9(k) times the fractional cost. Therefore,\nit is somewhat surprising that even though our relaxation is unable to provide any reasonable\nalgorithm for the (integral) allocation problem, it suffices to give a good guarantee for the\n(integral) k-server problem.\n2. As the next step, we design an online algorithm for the fractional allocation problem with the\nrefined guarantees required in the above reduction. Our techniques here are inspired by the\nideas developed recently in the context of the caching with costs problem [3] and weighted\npaging [2]. However, while these previous algorithms were designed and described using the\n3\n\n\fonline primal-dual framework, our algorithm is combinatorial. To analyze the performance\nwe employ a novel potential function approach.\nBy plugging the algorithm for the fractional allocation problem into the above reduction, we\nget a (roughly) O(l log(kl))-competitive algorithm for the k-server problem on an HST of\ndepth l, provided that the HST is sufficiently well-separated.\n3. Finally, we note that the competitive guarantee provided by the above k-server algorithm\ndepends on the depth l of the HST we are working with and, as l can be \u03a9(log \u2206), this\nguarantee can be polylogarithmic in \u2206. Therefore, as \u2206 can be 2\u03a9(n) , this would lead to\ncompetitiveness that is even polynomial in n.2 To deal with this issue, we define a weighted\nversion of an HST in which the edge lengths on any root-to-leaf path still decrease at (at\nleast) an exponential rate, but the lengths of the edges from a given node to its children could\nbe non-uniform. We prove that any HST can be transformed to a weighted HST of depth\nl = O(log n) while incurring only an O(1) distortion in leaf-to-leaf distances. We then show\nthat our previous ideas can be applied to weighted HSTs as well. In particular, our online\nfractional allocation algorithm is actually developed for a weighted star metric (instead of a\nuniform one), and, as we show, it can be employed in our reduction to obtain a fractional kserver algorithm on a weighted HST. The fractional k-server algorithm can again be rounded\nto a randomized algorithm with only an O(1) factor loss. Since l is now O(log n) and thus\ndoes not depend on \u2206, it gives us an overall guarantee which is polylogarithmic only in n and\nk.\nIn Section 2 we describe the above ideas more formally and also give an overview of the paper.\n\n1.2\n\nPreliminaries\n\nWe provide definitions and concepts that will be needed in the paper.\nHierarchically well-separated trees. Hierarchical well-separated trees (HST-s), introduced\nby Bartal [5, 6], is a metric embedding technique in which a general metric is embedded into a\nprobability distribution defined over a set of structured trees (the HST-s). The points of the metric\nare mapped onto the leaves of the HST, while internal tree nodes represent clusters. The distances\nalong a root-leaf path form a geometric sequence, and this factor is called the stretch of the HST.\nAn HST with stretch \u03c3 is called a \u03c3-HST. The embedding guarantees that the distance between\nany pair of vertices in the metric can only increase in an HST, and the expected blowup of each\ndistance, known as the distortion, is bounded. It is well known that any metric on n points can\nbe embedded into a distribution over \u03c3-HSTs with distortion O(\u03c3 log\u03c3 n) [18]. This approach of\nembedding into HSTs is particularly useful for problems (both offline and online) which seem hard\non a general metric, but can be solved fairly easily on trees (or HSTs).\nDue to the special structure of HSTs, the task of solving problems on them can sometimes be\nreduced to the task of solving a more general (and thus harder) problem, but on a uniform metric.\nFor example, this approach was used to obtain the first polylogarithmic guarantees for the metrical\ntask systems problem (MTS) by [7] (later further refined by [21]). More precisely, Blum et al. [7]\ndefined a refined version of MTS on a uniform metric known as unfair-MTS and showed how an\n2\n\nTo see an example when this is the case, one could consider a metric space corresponding to n points on a line\nthat are spaced at geometrically increasing distances.\n\n4\n\n\falgorithm with a certain refined guarantee for it can be used recursively to obtain an algorithm for\nMTS on an HST. This approach is especially appealing in the context of the k-server problem, as\nthis problem on a uniform metric (i.e. paging) is well- understood. This motivated Cot\u00e9 et al. [16]\nto define a problem on a uniform metric, that we call the allocation problem, and show how a good\nalgorithm for it can be used to design good k-server algorithms on HSTs. This problem is defined\nas follows.\nThe allocation problem. Suppose that a metric on d points is defined by a weighted star in\nwhich the distance from the center to each point i, 1 \u2264 i \u2264 d, is wi .3 At time step t, the total\nnumber of available servers, \u03ba(t) \u2264 k, is specified, and we call the vector \u03ba = (\u03ba(1), \u03ba(2), . . .) the\nquota pattern. A request arrives at a point it and it is specified by a (k + 1)-dimensional vector\n~ht = (ht (0), ht (1), . . . , ht (k)), where ht (j) denotes the cost of serving the request using j servers.\nThe cost vectors at any time are guaranteed to satisfy the following monotonicity property: for\nany 0 \u2264 j \u2264 k \u2212 1, the costs satisfy ht (j) \u2265 ht (j + 1). That is, serving a request with more servers\ncannot increase the cost. Upon receiving a request, the algorithm may choose to move additional\nservers from other locations to the requested point and then serve it. The cost is divided into two\nparts. The movement cost incurred for moving the servers, and the hit cost determined by the cost\nvector and the number of servers at location it .\nIn this paper, we will be interested in designing algorithms for (a fractional version of) this\nproblem that provide a certain refined competitive guarantee. Namely, we say that an online\nalgorithm for the allocation problem is (\u03b8, \u03b3)-competitive if it incurs:\n\u2022 a hit cost of at most \u03b8 * (Optcost + \u2206 * g(\u03ba));\n\u2022 a movement cost of at most \u03b3 * (Optcost + \u2206 * g(\u03ba)),\nwhere Optcost is the total cost (i.e., hit costP\nplus movement cost) of an optimal solution to a given\ninstance of the allocation problem, g(\u03ba) := t |\u03ba(t) \u2212 \u03ba(t \u2212 1)| is the total variation of the server\nquota pattern, and \u2206 is the diameter of the underlying metric space.\nFrom allocation to k-server. Cot\u00e9 et al. [16] showed that a (1 + \u03b5, \u03b2)-competitive online algorithm for the allocation problem on d points \u2013 provided \u03b5 is small enough and \u03b2 = O\u03b5 (polylog(d, k))\n\u2013 can be used to obtain a polylogarithmic competitive algorithm for the k-server problem on general\nmetrics. In particular, the next theorem follows from their work and is stated explicitly in [3].\nTheorem 2. Suppose there is a (1 + \u03b5, \u03b2)-competitive algorithm for the allocation problem on a\nuniform metric on d points. Let H be an \u03c3-HST with depth l. Then, for any \u03b5 \u2264 1, there is an\nO(\u03b2\u03b3 l+1 /(\u03b3 \u2212 1))-competitive algorithm for the k-server problem on H, where\n\u0013\n\u0012 \u0013\n\u0012\n\u03b2\n3\n+O\n.\n\u03b3 = (1 + \u03b5) 1 +\n\u03c3\n\u03c3\nSetting \u03b5 = 1/l, this gives an O(\u03b2l)-competitive algorithm on \u03c3-HSTs, provided the HST separation\nparameter \u03c3 is at least \u03b2l.\n3\n\nEven though Cot\u00e9 et al. [16] considered the allocation problem on a uniform metric, we find it useful to work\nwith the more general weighted-star metric version of this problem.\n\n5\n\n\fAt a high level, the k-server algorithm in Theorem 2 is obtained as follows. Each internal\nnode p in the HST runs an instance of the allocation problem on the uniform metric formed by its\nchildren. In this instance, the cost vectors appearing at a child i are guided by the evolution of\nthe cost of the optimal solution to the instance of the k-server problem restricted to the leaves of\nthe subtree that is rooted at i. Furthermore, the quota patterns for each of the allocation problem\ninstances is determined recursively. The root of the tree has a fixed server quota of k, and the quota\ncorresponding to a non-root node i is specified by the number of servers that are allocated to i by\nthe instance of the allocation problem run at the parent of i. The distribution of the servers on the\nleaves of the tree is determined in this manner, thus leading to a solution to the k-server problem.\nThe overall guarantee in Theorem 2 follows roughly by showing that the hit cost guarantee of (1+\u03b5)\nmultiplies at each level of the recursion, while the movement cost guarantee of \u03b2 adds up.\nWeighted HSTs. Note that the guarantee in Theorem 2 depends on l, the depth of the \u03c3-HST,\nwhich in general is \u0398(log\u03c3 \u2206). To avoid the resulting dependence on the diameter \u2206 that can be as\nlarge as 2\u03a9(n) , we introduce the notion of a weighted \u03c3-HST. A weighted \u03c3-HST is a tree having the\nproperty that for any node p, which is not the root or a leaf, the distance from p to its parent is at\nleast \u03c3 times the distance from p to any of its children. Thus, unlike an HST, distances from p to\nits children can be non-uniform. The crucial property of weighted HSTs that we will show later is\nthat any \u03c3-HST T with O(n) nodes can be embedded into a weighted \u03c3-HST with depth O(log n),\nsuch that the distance between any pair of leaves of T is distorted by a factor of at most 2\u03c3/(\u03c3 \u2212 1)\n(which is O(1) if, say, \u03c3 \u2265 2). Reducing the depth from O(log \u2206) to O(log n) allows us to replace\nthe factor of log \u2206 by log n in the bound on the competitive factor we get for the k-server problem.\nFractional view of randomized algorithms. The relation between randomized algorithms\nand their corresponding fractional views is an important theme in our paper. By definition, a\nrandomized algorithm is completely specified by the probability distribution over the configurations (deterministic states) at each time step of the algorithm. However, working explicitly with\nsuch distributions is usually very cumbersome and complex, and it is often simpler to work with\na fractional view of it. In a fractional view, the algorithm only keeps track of the marginal distributions on certain quantities, and specifies how these marginals evolve with time. Note that\nthere are multiple ways to define a fractional view (depending on which marginals are tracked).\nFor example, for the k-server problem on an HST, the fractional view might simply correspond to\nspecifying the probability pi of having a server at leaf i (instead of specifying the entire distribution\non the k-tuples of possible server locations). Clearly, the fractional view is a lossy representation of\nthe actual randomized algorithm. However, in many cases (though not always), a fractional view\ncan be converted back to a randomized algorithm with only a small loss. We now describe the\nfractional views we employ for the two main problems considered in this paper.\nFractional view of the k-server problem on an HST. Let T be a \u03c3-HST. For a node j \u2208 T ,\nlet T (j) be the set of leaves in the subtree rooted at j. In the fractional view, at each time step t,\nthe probability of having a server at leaf i, denoted by pti , is specified. Upon getting a request at\nleaf i at time t, a fractional algorithm must ensureP\nthat pti = 1. Let the expected number of servers\nat time t at leaves of T (j) be denoted by kt (j) = i\u2208T (j) pti . Clearly, the movement cost incurred\nP\nat time t is j\u2208T W (j)|kt (j) \u2212 kt\u22121 (j)|, where W (j) is the distance from j to its parent in T .\n6\n\n\fIt is easy to verify that the cost incurred by any randomized algorithm is at least as large as\nthe cost incurred by its induced fractional view. Conversely, it turns out that the fractional view is\nnot too lossy for a \u03c3-HST (provided \u03c3 > 5). In particular, in Section 5.2 we show that for a \u03c3-HST\n(\u03c3 > 5), an online algorithm for the k-server problem in the fractional view above can be converted\ninto an online randomized algorithm, while losing only an O(1) factor in the competitive ratio.\nThe fractional allocation problem. For the allocation problem we consider the following fractional view. For each location i \u2208 [d], and all possible number of servers j \u2208 {0, . . . , k}, there is a\nvariable xti,j denoting the probability of having exactly j servers at location i at time t. For each\ntime t, the variables xti,j must satisfy the following constraints.\n1. For each location i, the variables xti,j specify a probability distribution, i.e.,\neach xti,j is non-negative.\n\nP\n\nj\n\nxti,j = 1 and\n\n2. The number of servers used is at most \u03ba(t), the number of available servers. That is,\nXX\nj * xti,j \u2264 \u03ba(t).\ni\n\nj\n\nAt time step t, when cost vector ht arrives at location it , and possibly\nP \u03ba(t) changes, the algorithm\ncan change its distribution from xt\u22121 to xt incurring a hit cost of j ht (j)xtit ,j . The movement\ncost incurred is defined to be\nX\ni\n\nwi\n\nk X\nX\n\nxti,j \u2032 \u2212\n\nj=1 j \u2032 <j\n\nX\n\nxt\u22121\ni,j \u2032 .\n\n(1)\n\nj \u2032 <j\n\nRemark: Note that when our configurations are integral, this quantity is exactly the cost of\nmoving the servers from configuration xt\u22121 to configuration xt . In the fractional case, each term\nin the outermost sum can be seen as equal to the earthmover distance between the probability\nt\u22121\nt\nt\nvectors (xt\u22121\ni,0 , . . . , xi,k ) and (xi,0 , . . . , xi,k ) with respect to a linear metric defined on {0, 1, . . . , k}.\nThe earthmover distance is the optimal solution to a transportation problem in which xt\u22121 is the\nsupply vector, xt is the demand vector, and the cost of sending one unit of flow between xt\u22121\ni,j and\n\u2032\n\u2032\nt\nxi,j \u2032 is wi * |j \u2212 j |, since |j \u2212 j | is the change in number of servers resulting from sending this\nunit of flow. It is not hard to see4 that in the case of a linear metric, the optimal solution to the\ntransportation problem (up to a factor of 2) is captured by (1).\nA gap instance for the fractional allocation problem. As mentioned earlier, unlike the\nfractional view of the k-server problem presented above, the fractional view of the allocation problem\nturns out to be too weak to yield a randomized algorithm for its integral counterpart. We thus\npresent an instance of the allocation problem for which the ratio between the cost of any integral\nsolution and the cost of an optimal fractional solution is \u03a9(k). However, we stress that even though\nthe fractional view fails to approximate the integral allocation problem, we are still able to use it\nto design a fractional (and, in turn, integral) solution to the k-server problem. In particular, we\n4\n\nUsing uncrossing arguments on the optimal transportation solution.\n\n7\n\n\fshow in Section 4 that Theorem 2 holds even when we substitute the randomized algorithm for the\nallocation problem with the fractional algorithm.\nLet us consider a uniform metric space over d = 2 points, and consider an instance of the\nallocation problem in which exactly \u03ba(t) = k servers are available at each time. Furthermore, at\neach odd time step 1, 3, 5, . . ., the cost vector h = (1, 1, ..., 1, 0) arrives at location 1, and at each\neven time step 2, 4, 6, . . ., the vector h\u2032 = (1, 0, 0, ..., 0) arrives at location 2.\nWe show that any integral solution to this instance of the allocation problem must incur a high\ncost, while there is an \u03a9(k) times cheaper solution in the fractional view.\nClaim 3. Any solution to the instance above incurs a cost of \u03a9(T ) over T time steps.\nProof. Observe that the hit cost can be avoided at location 1 only if it contains k servers, and it\ncan be avoided at location 2 only if it contains at least one server. Thus, any algorithm that does\nnot pay a hit cost of at least 1 during any two consecutive time steps, must move at least one\nserver between locations 1 and 2, incurring a movement cost of at least 1, concluding that the cost\nis \u03a9(T ).\nClaim 4. There is a solution in the fractional view of cost O(T /k) over T time steps.\nProof. Consider the following solution in the fractional view. At each time step t, let:\nxt1,0 =\n\n1\n,\nk\n\n1\nxt1,k = 1 \u2212 ,\nk\n\nand\n\nxt2,1 = 1.\n\nNote that this solution satisfies all the constraints in the fractional view. Since location 2 always has\na server, it never pays any hit cost. Moreover, location 1 has fewer than k servers with probability\n1/k, it thus incurs only a hit cost 1 * x1,0 = 1/k at every odd time step. Also, as the solution does\nnot change over time, the movement cost is 0.\n\n2\n\nOverview of Our Approach\n\nIn this section we give a formal description of our results, outline how they are organized, and\ndiscuss how they fit together so as to obtain our main result.\nFractional allocation algorithm. In Section 3 we consider the fractional allocation problem\non a weighted star, and prove the following theorem.\nTheorem 5. For any \u03b5 > 0, there exists a fractional (1 + \u03b5, O(log(k/\u03b5)))-competitive allocation\nalgorithm on a weighted star metric.\nFrom allocation to k-server problem. In Section 4 we show how the algorithm from Theorem\n5 can be used to obtain a fractional k-server algorithm on a sufficiently well-separated weighted\nHST. In particular, we show that:\nTheorem 6. Let T be a weighted \u03c3-HST of depth l. If, for any 0 \u2264 \u03b5 \u2264 1, there exists a\n(1 + \u03b5, log(k/\u03b5))-competitive algorithm for the fractional allocation problem on a weighted star, then\nthere is an O(l log(kl))-competitive algorithm for the fractional k-server problem on T , provided\n\u03c3 = \u03a9(l log(kl)).\n8\n\n\fPutting it all together. We now show how to use Theorems 5 and 6 to prove our k-server\nguarantee for general metrics, i.e., to prove Theorem 1.\nTo this end, we need two more results that we prove in Section 5. First,\nTheorem 7. Let T be a \u03c3-HST with \u03c3 > 5. Then any online fractional k-server algorithm on\nT can be converted into a randomized k-server algorithm on T with an O(1) factor loss in the\ncompetitive ratio.\nNote that the above result gives a rounding procedure only for HSTs (and not weighted HSTs).\nTo relate HSTs to weighted HSTs, we show the following.\nTheorem 8. Let T be a \u03c3-HST with n leaves, but possibly arbitrary depth. Then T can be transformed into a weighted \u03c3-HST Te such that: Te has depth O(log n), the leaves of Te and T are\nidentical, and any leaf to leaf distance in T is distorted by a factor of at most 2\u03c3/(\u03c3 \u2212 1) in Te.\nGiven the above results, we can present the proof of our main theorem.\n\nProof of Theorem 1. Our algorithm proceeds as follows. First, we use the standard technique\n[18] to embed the input (arbitrary) metric M into a distribution \u03bc over \u03c3-HSTs with stretch\n\u03c3 = \u0398(log n log(k log n)). This incurs a distortion of O(\u03c3 log\u03c3 n) and the resulting HSTs have\ndepth O(log\u03c3 \u2206), where \u2206 is the diameter of M .\nNext, we pick a random HST T according to the distribution \u03bc, and transform T into Te using\nTheorem 8. As Te has depth l = O(log n), it holds that \u03c3 = \u0398(l log(kl)) and hence applying\nTheorem 6 to Te gives an O(l log(kl)) = O(log n log(k log n))-competitive fractional k-server algorithm on Te. Since the leaves of T and Te are identical, and the distances only have O(1) distortion,\nthe fractional k-server solution on Te induces an O(log n log(k log n))-competitive fractional k-server\nsolution on T . By Theorem 7, this gives an O(log n log(k log n))-competitive randomized k-server\nalgorithm on T .\nWe now relate the optimum k-server cost on M to the optimum on T . Let Opt\u2217M denote the\noptimum k-server solution on M , and let cT denote the cost of this solution on T . Since the\nexpected distortion of distances in our ensemble of HSTs is small, we have:\nE\u03bc [cT ] = O(\u03c3 log\u03c3 n) * Opt\u2217M .\n\n(2)\n\nLet AlgT denote the cost of the solution produced by the online algorithm on T , and let AlgM\ndenote the cost of this solution on the metric M . As the pairwise distances in T are at least the\ndistances in M , AlgM \u2264 AlgT . Also, as AlgT is O(log n log(k log n))-competitive, it follows that:\nAlgM \u2264 AlgT = O(log n log(k log n)) * c\u2217T \u2264 O(log n log(k log n)) * cT\nwhere c\u2217T is the optimum k-server cost on T (and hence c\u2217T \u2264 cT ). Taking expectation with respect\nto \u03bc above and using (2), the expected cost of our solution E\u03bc [AlgM ] satisfies:\nE\u03bc [AlgM ] = O(log n log(k log n)) * E\u03bc [cT ] = O(\u03c3 log\u03c3 n) * O(log n log(k log n)) * Opt\u2217M ,\nwhich implies that the overall algorithm has a competitive ratio of\n\u0012 \u0012\n\u0013\u0013\n\u0012 3\n\u0013\n\u0001\nlog n\nlog n(log(k log n))2\nO \u03c3\n* O (log n log(k log n)) = O\n= O log2 k log3 n log log n .\nlog \u03c3\nlog log n\n9\n\n\f3\n\nThe Fractional Allocation Problem\n\nConsider a metric corresponding to a weighted star on d leaves (also called locations) 1, . . . , d, where\nwi is the distance from leaf i to the root. Let us fix a sequence of cost vectors h0 , h1 , . . . and a\nserver quota pattern \u03ba = (\u03ba(1), \u03ba(2), . . .), where \u03ba(t) is the number of servers available at time t,\nand \u03ba(t) \u2264 k for all times t.\nRecall that in the fractional allocation problem the state at each time t is described by nonnegative variables xti,j denoting the probability\nservers at location i. At\nP t that there are exactly\nPj P\nt\neach time t, the variables xi,j satisfy: (1) j xi,j = 1, for each i; (2) i j jxti,j \u2264 \u03ba(t).\nAs we shall see, when describing and analyzing our algorithm for the fractional allocation\nt , defined as\nproblem, it will be easier to work with variables yi,j\nt\nyi,j\n=\n\nj\u22121\nX\n\nxti,j \u2032 ,\n\nfor i \u2208 {1, . . . , d},\n\nj \u2208 {1, 2, . . . , k + 1}.\n\nj \u2032 =0\nt is the probability that at time t we have less than j servers at location i. Clearly, for every\nI.e., yi,j\ni, as long as:\nt\nyi,j\nt\nyi,j\u22121\n\n\u2208 [0, 1]\n\u2264\n\n(3)\n\nt\nyi,j\n,\n\nyi,k+1 = 1,\n\n\u2200i \u2208 {1, . . . , d},\n\nj \u2208 {2, . . . , k + 1},\n\n(4)\n\nt s. Therefore, in\nthere is always a unique setting of the variables xti,j s that corresponds to the yi,j\nt\nwhat follows we make sure that the variables yi,j s generated by our algorithm satisfy the above two\nconditions.\nThe condition that at most \u03ba(t) servers are available at each time t can be expressed in terms\nt as:\nof yi,j\nk\nd X\nX\ni=1 j=1\n\nt\nyi,j\n=\n\nk\nd X\nk\nd X\nk\nd X\nX\nX\nX\njxti,j\nxti,j \u2212\n(k \u2212 j)xti,j = k\ni=1 j=0\n\ni=1 j=0\n\n\uf8eb\n\uf8f6\nd X\nk\nX\njxti,j \uf8f8 \u2265 kd \u2212 \u03ba(t).\n= kd \u2212 \uf8ed\n\ni=1 j=0\n\n(5)\n\ni=1 j=0\n\nLet us now focus on a particular cost vector ht = (ht (0), ht (1), . . . , ht (k)) corresponding to time\nstep t. Recall that ht (j) is the hit cost incurred when serving the request using exactly j servers.\nWe can express ht as\n\u001a t\nh (j \u2212 1) \u2212 ht (j) j = 1, 2, . . . , k\nt\n\u03bbj =\nht (k)\nj =k+1\n\nThe variables \u03bbtj are non-negative as the hit costs are non-increasing in j, i.e., ht (0) \u2265 ht (1) \u2265\n. . . \u2265 ht (k). Intuitively, \u03bbtj captures the marginal cost of serving the request with strictly less than\nt }\nj servers.5 The hit cost incurred by a configuration y t = {yi,j\ni,j now has a simple formulation.\n\nWe note that we can assume that \u03bbtk+1 is always 0. Otherwise, as any valid algorithm (including the optimal one)\nalways has at most k servers at a given location, any competitive analysis established for the case \u03bbtk+1 = ht (k) = 0\ncarries over to the general case. Thus, from now on we remove \u03bbtk+1 and also yi,k+1 (that is always 1) from our\nconsiderations and notation.\n5\n\n10\n\n\fLet it denote the location on which the hit cost vector ht appears, then the hit cost\ncan be expressed as\nk\nX\n\u03bbtj * y tit ,j .\n\nPk\u22121\nj=0\n\nht (j)xtit ,j\n\nj=1\n\nSimilarly, expression (1) for the movement cost from a configuration y t\u22121 to a configuration yt\nbecomes\n\uf8eb\n\uf8f6\nd\nk\nX\nX\nt\nt\u22121 \uf8f8\nwi \uf8ed\nyi,j\n\u2212 yi,j\n.\ni=1\n\n3.1\n\nj=1\n\nDescription of the Algorithm\n\nt\u22121\nt }\nIn light of the above discussion it suffices to specify how state {yi,j\n}i,j evolves to {yi,j\ni,j at time t\nt\nupon arrival of cost vector h and server quota \u03ba(t). Our algorithm performs this evolution in two\nstages. First, it executes a fix stage in which the number of servers is decreased so as to comply\nwith a decrease of the quota \u03ba(t). Then, it proceeds with a hit stage, during which the (fractional)\nconfiguration of the servers is modified to react to the cost vector ht . We describe the dynamics\nof both stages as a continuous process governed by a set of differential equations. As it turns out,\nviewing the evolution of the server configuration this way allows us to both simplify the description\nand the analysis of the algorithm. The evolution of the fractional solution during the fix stage is\nparametrized by a time index \u03c4 that starts at 0 and grows until the number of servers is no more\nthan \u03ba(t). The hit stage is parametrized by a time index \u03b7 that starts initially at 0 and ends at 1.\nFor the sake of simplicity, let us drop the index t from our notation since it does not play any\nrole in our analysis. We denote the configuration at time t \u2212 1 by y 0 and the configuration at time\nt by y 1 . Let \u03bb denote the hit cost vector \u03bbt and let i denote the location it that \u03bbt penalizes. The\nintermediate states of the algorithm are denoted by y \u03c4 , \u03c4 \u2265 0, during the fix stage, and by y \u03b7 ,\n\u03b7 \u2208 [0, 1], during the hit stage. At each time \u03b7 \u2208 [0, 1] (respectively, \u03c4 \u2265 0), the algorithm specifies\ndy \u03b7\n\ndy \u03c4\n\n\u03b7\n\u03c4 ). Denote by \u03c4 the final value\n(respectively, d\u03c4i,j of each yi,j\nthe derivative d\u03b7i,j of each variable yi,j\ne\nt is defined as follows.\nthat \u03c4 reaches during the fix stage. Eventually, each yi,j\nt\nyi,j\n\n=\n\nt\u22121\nyi,j\n\n+\n\nZ\n\n\u03c4e\n\u03c4 =0\n\n\u03c4\ndyi,j\nd\u03c4 +\nd\u03c4\n\nZ\n\n1\n\n\u03b7\ndyi,j\n\n\u03b7=0\n\nd\u03b7\n\nd\u03b7.\n\n(6)\n\nAn important issue that needs to be addressed is proving that the differential equations specifying the derivatives at each step have a (unique) solution and thus the algorithm is well-defined.\nThis proof turns out to be non-trivial in the case of the hit stage, since the derivative during this\nstage might change in a non-continuous manner. Nevertheless, as we will show, the process is still\nwell-defined.\nAnother technical detail is that during the hit stage, in intermediate times \u03b7 \u2208 [0, 1], we will\nnot work with the hit cost vector \u03bb directly, but rather with a modified cost vector \u03bb\u03b7 that can\nvary with \u03b7. (During the first reading, the reader may assume that \u03bb\u03b7 = \u03bb and skip the part below\nabout blocks and go directly to the description of the fractional algorithm.)\nWe initialize \u03bb0 = \u03bb. To define \u03bb\u03b7 for \u03b7 > 0, we need the notion of blocks.\n\n11\n\n\fBlocks: During the hit stage, for each \u03b7 \u2208 [0, 1], we maintain a partition of the index set\n{1, . . . , k + 1} (for location \u012b) into blocks B1\u03b7 , B2\u03b7 . . . , Bl\u03b7 . The collection of blocks is denoted by\nD \u03b7 and it satisfies the following properties.\n\u03b7\n1. y\u012b,j\nis identical for all indices j within any block B \u2208 D \u03b7 . For future reference, let us denote\nby y \u03b7 (B) this common value for all j \u2208 B.\n\n2. For any block B = {j, . . . , j + s \u2212 1} of length s in D \u03b7 , it holds that for every 1 \u2264 r \u2264 s,\nj+r\u22121\nX\nj \u2032 =j\n\nj+s\u22121\nX 1\n1\n\u03bbi,j \u2032 \u2264\n\u03bb \u2032.\nr\ns i,j\n\u2032\n\n(7)\n\nj =j\n\nThat is, the average value of the \u03bb's in any prefix of a block is no more than the average of\nthe entire block.\nWe define \u03bb\u03b7 to be the cost\nvector obtained by averaging \u03bb over the blocks in D \u03b7 . That is, for\nP\neach B \u2208 D \u03b7 , we set \u03bb(B) = ( j\u2208B \u03bbi,j )/|B|, and then\n\u03bb\u03b7i,j\n\n=\n\n\u001a\n\n\u03bb(B) if i = i and j \u2208 B, for B \u2208 D \u03b7 ,\n0\notherwise.\n\nNow, in our algorithm, the initial partitioning D 0 of blocks is the trivial one, i.e., one in which\neach index j forms its own block. (Note that in this case we indeed have \u03bb0 = \u03bb.) Next, blocks are\nupdated as \u03b7 increases. For any \u03b7 \u2265 0, if two consecutive blocks Bp , Bp+1 \u2208 D \u03b7 satisfy:\ny \u03b7 (Bp ) = y \u03b7 (Bp+1 )\n\nand\n\n\u03bb(Bp ) < \u03bb(Bp+1 ),\n\n(8)\n\nthen Bp and Bp+1 are merged and D \u03b7 is modified accordingly. Note that the condition \u03bb(Bp ) \u2264\n\u03bb(Bp+1 ) guarantees that (7) is satisfied in the new block created by merging Bp and Bp+1 . As we\nshall see later (Lemma 11), a crucial property of the evolution of D \u03b7 during the hit stage is that\n\u03b7\nyi,j\ns are updated in a way that guarantees that a block never splits once it is formed.\nThe algorithm. We are now ready to state our algorithm. It is parameterized by a parameter\n\u03b5 > 0 that will be fixed later.\n\n12\n\n\fFractional Allocation Algorithm:\n\u03b5\n, \u03b1 = ln(1 + \u03b21 ) = ln(1 + 1+k\nSet \u03b2 = 1+k\n\u03b5 ).\nFix stage: Set y 0 = y t\u22121 .P\n\u03c4 < kd \u2212 \u03ba(t) (i.e., while the total volume of servers exceeds\nFor any \u03c4 \u2208 [0, \u221e), while i,j yi,j\n\u03c4 at a rate:\nthe quota) we increase each variable yi,j\n\u03c4\ndyi,j\nd\u03c4\n\n=\n\n(\n\n1\nwi\n\n0\n\n\u0010\n\u0011\n\u03c4 +\u03b2\n\u03c4 <1\nyi,j\nyi,j\n\n\u03c4 =1\nyi,j\n\nDenote by \u03c4e the termination time of the fix stage.\nHit stage: Set y 0 to be the state obtained at the end of the fix stagea . Define the following\nupdate rule for any \u03b7 \u2208 [0, 1]:\nP\n\u03b7\n\u2022 If i,j yi,j\n= kd \u2212 \u03ba(t), choose N (\u03b7) \u2265 0 such thatb :\n\u03b7\ndyi,j\n\nd\u03b7\n\nand\n\n=\n\n\uf8f1\n\uf8f4\n0\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\n1\nwi\n\n\u0011\n\u0010\n\u03b7\n= 1,\nif either N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j > 0 and yi,j\n\u0011\n\u0010\n\u03b7\n=0\nor N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j \u2264 0 and yi,j\n(9)\n\n\u0011\n\u0010\n\u0011 \u0010\n\u03b7\notherwise\nyi,j\n+ \u03b2 * N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j\n\u03b7\nX dyi,j\ni,j\n\nd\u03b7\n\n= 0.\n\nP\n\u03b7\n\u2022 Otherwise (i.e., if i,j yi,j\n> kd \u2212 \u03ba(t)), set N (\u03b7) = 0, and define the derivatives of the\nvariables as above.\nt , y t\u22121 +\nOutput: For each (i, j), return yi,j\ni,j\n\nR \u03c4e\n\n\u03c4\ndyi,j\n\u03c4 =0 d\u03c4 d\u03c4\n\n+\n\nR1\n\n\u03b7\ndyi,j\n\u03b7=0 d\u03b7 d\u03b7.\n\nP\n\u03c4e\n\u2265 kd \u2212 \u03ba(t).\nNote that upon termination of the fix stage, i,j yi,j\nb\nAs we show in Lemma 10, there is always a way of choosing N (\u03b7) such that the desired conditions are\nsatisfied.\na\n\nHigh-level intuition. Before proving correctness and analyzing the performance of the above\nalgorithm, we provide some intuition on the dynamics underlying it.\nDynamics of the fix stage: This is fairly straightforward. The algorithm simply increases all\n\u03c4 that are strictly less than 1 (which decreases the total number of servers), until\nthe variables yi,j\nthe quota \u03ba(t) on the number of servers is met. We note that it may also be the case that the\ntotal number of servers used is already strictly smaller than the quota \u03ba(t) to begin with, e.g., if\nthe server quota increases at time t. In this case, nothing is done during the fix stage. Notice that\n\u03c4 is proportional to its value, which means that the change is\nthe rate of change of a variable yi,j\ngoverned by an exponential function. This kind of update rule is in line with previous algorithms\nfor weighted paging [2, 3].\n\n13\n\n\fDynamics of the hit stage: For simplicity, let us assume that during this stage we have that\n\u03b7\n\u03b7\n0 < yi,j\n< 1, for all (i, j), and each yi,j\nis a strictly increasing function of j. That is,\n\u03b7\n\u03b7\n\u03b7\n0 < yi,1\n< yi,2\n< . . . < yi,k\n< 1,\n\n(10)\n\nfor all locations i and \u03b7 \u2208 [0, 1].\nNote that under this assumption condition (8) will never trigger. As a result, no blocks are\n\u03b7\nmerged and we have \u03bb\u03b7 = \u03bb for all \u03b7 \u2208 [0, 1]. Furthermore, as in this case each variable yi,j\nis\nstrictly between 0 and 1, its rate of change during the hit stage simplifies to:\n\u03b7\ndyi,j\n\nd\u03b7\n\n=\n\nwith\n\nN (\u03b7) =\n\n\uf8f1\n0\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\nP\n\nif\n\nP\n\ni,j\n\n\u0011\n1 \u0010 \u03b7\nyi,j + \u03b2 * (N (\u03b7) \u2212 \u03b1\u03bbi,j ) ,\nwi\n\u03b7\nyi,j\n> kd \u2212 \u03ba(t),\n\n\u03b7\n+\u03b2 )*\u03b1\u03bbi,j\n(yi,j\n\u03b7\n1\ny\n(\ni,j wi\ni,j +\u03b2 )\n\n1\ni,j wi\nP\n\n(11)\n\notherwise (i.e. if\n\nP\n\n\u03b7\ni,j yi,j\n\n(12)\n= kd \u2212 \u03ba(t)).\n\nP dy\u03b7\n(Note that the value of N (\u03b7) in the second case of (12) is determined by the fact that i,j d\u03b7i,j\nhas to be zero.)\nRecall that \u03bbi,j = 0 for i 6= i. First, if\nLet us study the dynamics given by (11) more carefully.\nP\n\u03b7\nthe number of servers used is below the quota, i.e. if i,j yi,j > kd \u2212 \u03ba(t), the algorithm responds\n\u03b7\nto the cost vector \u03bb by simply increasing the number of servers at location \u012b by decreasing each y\u012b,j\n\u03b7\n+ \u03b2) * \u03b1\u03bb\u03b7\u012b,j . To understand this better, it is instructive to consider the special\nat a rate of w1 (y\u012b,j\n\u012b\ncase when \u03bb\u03b7i,j = 1 for some particular index j and is 0 otherwise (this corresponds to the hit cost\n\nvector h\u03b7 that incurs cost 1 if there are strictly fewer than j servers at i and cost 0 otherwise). In\n\u03b7\n\u03b7\n\u03b7\n\u03b7\nand keeps other yi,j\n's unchanged (in particular yi,j+1\nand yi,j\u22121\nthis case, the algorithm reduces yi,j\n\u03b7\n\u03b7\nand yi,j\u22121\ndo not change while yi,j decreases, this has the effect of\nremain unchanged). As yi,j+1\n\u03b7\n\u03b7\n\u03b7\nincreasing the probability mass xi,j\n= yi,j+1\n\u2212 yi,j\non (i, j), and decreasing the probability mass\n\n\u03b7\n\u03b7\n\u03b7\n\u03b7\n= yi,j\n\u2212 yi,j\u22121\non (i, j \u2212 1)6 Moreover, note that the decrease in xi,j\u22121\nis exactly equal to the\nxi,j\u22121\n\u03b7\nincrease in xi,j .\nNow, let us consider the case when the number of servers used is exactly equal to the quota.\nHere, we also need to ensure that the quota is maintained. This is done by offsetting the increase\nin the number of servers at location i (as described by the dynamics in the previous paragraph),\nby decreasing the number of servers at all locations (including i). This is precisely the purpose\n\u03b7\nof the term N (\u03b7) in (11). It increases yi,j\n(and hence decreases the number of servers) at a rate\n\u03b7\n1\nproportional to wi (yi,j + \u03b2) (as in the fix stage). Note that as \u03bbi,j = 0 for i 6= \u012b, this update can\nonly decrease the number of servers at locations i 6= i. The overall number of servers at location i\ncan only increase, but of course due to the redistribution of probability mass at i, it may happen\nthat the probability mass at some (i, j) goes down.\nUnfortunately, when assumption (10) does not hold, the simple dynamics described above may\nproduce infeasible configurations. First, increasing or decreasing variables according to (11) does\n6\n\n\u03b7\n\u03b7\n\u03b7\nIn the simplified discussion here we are implicitly assuming that xi,j\u22121\n> 0 by assuming that yi,j\n> yi,j\u22121\n.\n\n14\n\n\fnot take into account that the variables need to stay in the range [0, 1], and hence this may be\nviolated. This happens if (i) a variable is equal to 0 and has a negative derivative, or (ii) when\nit is equal to 1 and has a positive derivative. To avoid this problem we need to deactivate such\nvariables (by setting their derivative to be 0) when either one of these two cases occurs. Moreover,\nthe above dynamics may also violate the monotonicity condition (4). To avoid this issue, we need\nto merge blocks and modify \u03bb\u03b7 accordingly, as was previously described.\nNow, the resulting algorithm does not produce infeasible configurations anymore. However, its\ndynamics is somewhat more involved. Before we discuss it, let us first provide a formal definition\nof an inactive coordinate or variable.\n\u03c4 < 1 is said to be active at time\nDefinition 9. During the fix stage, a coordinate (i, j) for which yi,j\n\u03b7\n\u03c4 . Otherwise it is said to be inactive. During the hit stage, coordinate (i, j) (or variable yi,j\n) is said\n\u0011\n\u0011\n\u0010\n\u0010\n\u03b7\n\u03b7\nto be inactive at time \u03b7 \u2208 [0, 1] if either N (\u03b7) \u2212 \u03b1\u03bbi,j > 0 and yi,j = 1, or N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j \u2264\n\u03b7\n0 and yi,j\n= 0. Otherwise, coordinate (i, j) is said to be active. Denote by A\u03b7 (respectively A\u03c4 ) the\nset of active coordinates at time \u03b7 (respectively \u03c4 ).\n\nNow, by definition, during the hit stage at time \u03b7 only the active variables might change. So,\nwe can compactly rewrite the evolution of the variables during the hit stage given in (9) as\n(\n\u03b7\n0 \u0010\nif (i, j) \u2208\n/ A\u03b7 ,\ndyi,j\n\u0011\n\u0011\n\u0010\n(13)\n=\n\u03b7\n\u03b7\n1\notherwise\nd\u03b7\nwi yi,j + \u03b2 * N (\u03b7) \u2212 \u03b1\u03bbi,j\n\nP\nP\nP dy\u03b7\ndy \u03b7\n\u03b7\n> kd \u2212 \u03ba(t), we\nFurthermore, as we still require that i,j d\u03b7i,j = (i,j)\u2208A\u03b7 d\u03b7i,j = 0, if i,j yi,j\nhave that N (\u03b7) can be expressed as\n\uf8f1\nP\n\u03b7\nif\n\uf8f2 0P\ni,j yi,j > kd \u2212 \u03ba(t),\n\u03b7\n\u03b7\n1\nP\nN (\u03b7) =\n(14)\n(i,j)\u2208A\u03b7 wi (yi,j +\u03b2 )*\u03b1\u03bbi,j\n\u03b7\nP\notherwise\n(i.e.\nif\n\uf8f3\n\u03b7\n1\ni,j yi,j = kd \u2212 \u03ba(t)).\n(i,j)\u2208A\u03b7 wi (yi,j +\u03b2 )\n\nIn light of the above, one can see that the simple evolution of the variables in the special case\nof (10), as described by (11), is a special case of the general evolution in which all coordinates\nare being active and \u03bb\u03b7 = \u03bb for all \u03b7 \u2208 [0, 1]. The reason why the analysis of the general process\nis more complicated is that the set of active coordinates (and the hit cost \u03bb\u03b7 ) can, in principle,\nchange very abruptly between two values of \u03b7. Moreover, as stated, equation (14) and Definition\n9, have a circular dependency. In particular, the value of N (\u03b7) depends on the set A\u03b7 , but in turn\nthe definition of A\u03b7 also depends on the value of N (\u03b7). As a result, a priori it is not even clear that\nour algorithm is well defined. That is, a unique trajectory consistent with our local evolutionary\nrules indeed exists. We proceed to proving this now.\nWell-definiteness of the algorithm. We start by addressing the above-mentioned issue of the\ncircular dependency between the value of N (\u03b7) and the set A\u03b7 . Note that it is not clear any more\nthat there always exists a non-negative normalization factor N (\u03b7) as required by our algorithm.\nAs we prove in the next lemma, however, one can use a simple continuity argument to prove the\nexistence of the desired normalization factor.\nP dy\u03b7\ndy \u03b7\nLemma 10. There exists a N (\u03b7) \u2265 0 for which i,j d\u03b7i,j = 0, where the derivatives d\u03b7i,j are as\ndefined in the algorithm. Moreover, the set A\u03b7 of active coordinates is never empty.\n15\n\n\fP\n\u03b7\nProof. Fix any \u03b7 \u2208 [0, 1]. Let us consider the function f (s) = i,j (dyi,j\n/d\u03b7)|N (\u03b7)=s , i.e., f (s) is the\nsum of all derivatives given by equation (9) for the case when N (\u03b7) is equal to s.\n\u03b7\nClearly, if s = N (\u03b7) = 0, then (dyi,j\n/d\u03b7)|N (\u03b7)=s \u2264 0 for each (i, j) and hence f (s) \u2264 0. If\nf (0) = 0 then N (\u03b7) satisfies the requirements. Note that in this case all coordinates that are\nnon-zero are active. This set is non-empty as at the beginning of the hit stage the sum over all\ncoordinates is at least kd \u2212 \u03ba(t) > 0.\nThus, suppose that f (0) < 0. Let \u03bbmax = maxi,j \u03bb\u03b7i,j be the largest entry in \u03bb\u03b7 . Then, at\n\u03b7\ns = N (\u03b7) = \u03b1\u03bbmax we have (dyi,j\n/d\u03b7)|N (\u03b7)=s \u2265 0 for each (i, j) and hence f (s) \u2265 0.\n\u03b7\nNext, we claim that each derivative (dyi,j\n/d\u03b7)|N (\u03b7)=s is a continuous function of s. To this end,\n\u03b7\n\u03b7\nnote that if 0 < yi,j < 1, then the function (dyi,j\n/d\u03b7)|N (\u03b7)=s is a linear function of s (and thus is\n\u03b7\n\u03b7\n/d\u03b7)|N (\u03b7)=s is zero for s \u2264 \u03b1\u03bb\u03b7i,j , and then increases\n= 0, the function (dyi,j\ncontinuous). For yi,j\ndy \u03b7\n\n\u03b7\nlinearly for s \u2265 \u03b1\u03bb\u03b7i,j \u2013 so, again, it is continuous. Similarly, for yi,j\n= 1, ( d\u03b7i,j )|N (\u03b7)=s is negative\n\u03b7\n\u03b7\nif s \u2264 \u03b1\u03bbi,j , and increases linearly until s = \u03b1 * \u03bbi,j , and then remains zero.\nNow, as each derivative is continuous, so is f . Thus, we know that by the intermediate value theorem the preimage f \u22121 (0) in the interval [0, \u03b1\u03bbmax ] is non-empty. Furthermore, as f is continuous\nand f (s) < 0 for s = 0, there exists a minimal s\u2217 such that f (s\u2217 ) = 0 and 0 < s\u2217 \u2264 \u03b1\u03bbmax .\nWe take N (\u03b7) = s\u2217 and claim that the corresponding set A\u03b7 is non-empty (which would prove\n\u03b7\nthe lemma). To see why it is the case, note that if there exists a coordinate 0 < yi,j\n< 1 then it\nis always active and we are done. Otherwise, let us consider A0 (A1 ) to be the set of (i, j) with\n\u03b7\n\u03b7\n= 1). As the sum over all coordinates is at least kd \u2212 \u03ba(t) > 0, the set A1 is\n= 0 (resp. with yi,j\nyi,j\nnon-empty. Suppose for s\u2217 > 0 all the coordinates in A0 , A1 are inactive, then by definition for all\n(i, j) \u2208 A1 , s\u2217 > \u03b1\u03bb\u03b7i,j (note the strict inequality), and for all coordinates in A0 , s\u2217 \u2264 \u03b1\u03bb\u03b7i,j . This is\na contradiction to the minimality of s\u2217 as we could find a 0 < s\u2032 < s\u2217 , such that f (s\u2032 ) = 0.\n\nNow, as our algorithm is defined via a set of differential equations indexed by \u03c4 and \u03b7, to\nprove that it is well-defined we need to show that there exists a unique solution to this set, and\nfurthermore this solution is feasible for the allocation problem. To this end, we prove the following\nlemma, whose proof is in Appendix A.\nLemma 11. There exists a unique solution y \u03c4 and y \u03b7 , defined on the intervals \u03c4 \u2265 0, \u03b7 \u2208 [0, 1],\nto the set of differential equations defined by the algorithm. Furthermore, the solution satisfies the\nfollowing properties:\n\u03b7\n\u03c4 \u2264 1.\n\u2022 Boundary: For each (i, j), and for all 0 \u2264 \u03c4 , and 0 \u2264 \u03b7 \u2264 1: 0 \u2264 yi,j\n, yi,j\n\u03b7\n\u03b7\n\u03c4 \u2264 y\u03c4\n\u2022 Monotonicity: For each (i, j), (j \u2264 k), yi,j\n\u2264 yi,j+1\nand yi,j\ni,j+1 .\n\n\u2022 Quota: The expected number (volume)Pof servers at the end of the fix stage and at any\n\u03b7\n\u03b7 \u2208 [0, 1] does not exceed \u03ba(t). That is, i,j yi,j\n\u2265 kd \u2212 \u03ba(t) for all \u03b7 \u2208 [0, 1].\n\n\u2022 Blocks: During the hit stage, Blocks can only merge (and they never split).\n\n\u2022 Discontinuity: The total number of times \u03b7 \u2208 [0, 1] that each location (i, j) changes its\nstatus from active to inactive, as well as the number of discontinuity points of N (\u03b7) as a\nfunction of \u03b7, is finite (in fact, polynomial in k and d).\n\n16\n\n\f3.2\n\nCost Accounting\n\nIn this section we prove some helpful properties that allow us to charge the algorithm and the\noptimal solution in a continuous fashion. This will simplify the potential function based analysis\nthat we later perform. First, we deal with the charging of the hit cost, and then with the accounting\nof the movement cost.\nCharging the hit cost. The issue we want to address here is that at a given time t the hit costs\nof the optimal solution and our algorithm depend only on the final states of both solutions. More\nprecisely, if y \u2217 is the optimal solution at time t, and y = y 1 is the final state of the algorithm at\ntime t, then the hit cost of the optimal solution (respectively, of the algorithm) at time t is equal to\n\u03bb * y \u2217 (respectively, \u03bb * y). However, as our algorithm is described in a continuous fashion, it would\nbe simpler to also have a way of accounting for the hit costs in a continuous and local fashion.\nIn particular, we would like to account for the hit cost of the optimal solution as:\nZ 1\n\u03bb\u03b7 * y \u2217 * d\u03b7,\n(15)\n\u03b7=0\n\nand for the hit cost of the algorithm as:\nZ\n\n1\n\n\u03bb\u03b7 * y \u03b7 d\u03b7.\n\n(16)\n\n\u03b7=0\n\nNote that the above expressions can be interpreted as charging locally at every time \u03b7 \u2208 [0, 1] an\ninfinitesimally small hit cost of \u03bb\u03b7 *y \u2217 d\u03b7 (respectively, \u03bb\u03b7 *y \u03b7 d\u03b7) to the optimal solution (respectively,\nto the algorithm). Now, to make this accounting valid, we need to show that the above expressions\ncan only overestimate the hit cost of our algorithm and underestimate the hit cost of the optimal\nsolution. We prove that this is indeed the case in the following lemma.\nLemma 12. The following inequalities hold:\nZ 1\n\u03bb\u03b7 * y \u2217 * d\u03b7 \u2264 \u03bb * y \u2217 ,\n\n(17)\n\n\u03b7=0\n\nZ\n\n1\n\n\u03bb\u03b7 * y \u03b7 d\u03b7 \u2265 \u03bb * y.\n\n(18)\n\n\u03b7=0\n\nProof. We first prove inequality (17). To this end, we show that for any non-decreasing vector\nv = (v1 , v2 , . . .), and any 0 \u2264 \u03b71 < \u03b72 \u2264 1, it holds that\n\u03bb\u03b71 * v \u2265 \u03bb\u03b72 * v.\n\n(19)\n\nNote that as \u03bb = \u03bb0 and y \u2217 is feasible (and thus satisfies property (4)), taking v equal to y \u2217\nimmediately gives Inequality (17).\nBy Lemma 11, we know that the only difference between \u03bb\u03b71 and \u03bb\u03b72 is that some of the\nblocks in D \u03b71 can be merged in D \u03b72 . Therefore, it suffices to show that\nP whenever two consecutive\nP\nblocksPB1 and B2 merge to form another block B, it holds that \u03bb(B1 ) i\u2208B1 vi + \u03bb(B2 ) i\u2208B2 vi \u2265\n\u03bb(B) i\u2208B vi for any hit cost vector \u03bb.\n17\n\n\fP\nP\nLet l1 = |B1 |, l2 = |B2 |, and let a1 = ( i\u2208B1 vi )/l1 , a2 = ( i\u2208B2 vi )/l2 . Then, by the definition\nof \u03bb(B), the inequality above is equivalent to showing that\n\u0013\n\u0012\n\u03bb(B1 )l1 + \u03bb(B2 )l2\n(a1 l1 + a2 l2 ).\n(20)\n\u03bb(B1 )l1 a1 + \u03bb(B2 )l2 a2 \u2265\nl1 + l2\nAs v is increasing we have a1 \u2264 a2 , and since B1 and B2 were merged, by (8) it must be that\n\u03bb(B1 ) < \u03bb(B2 ). A direct calculation shows that (20) holds under these conditions.\nNow, to prove that inequality (18) also holds, we prove that whenever the derivative of \u03bb\u03b7 * y \u03b7\nis defined, i.e., whenever neither A\u03b7 nor \u03bb\u03b7 change (which is the case except for possibly finitely\nmany points, cf. Lemma 11) we have that\nd (\u03bb\u03b7 * y \u03b7 )\n\u2264 0.\nd\u03b7\n\n(21)\n\nThat is, the state y \u03b7 evolves in a way that reduces the hit cost of the algorithm with respect to the\ncorresponding hit cost vector.\nTo see how (18) follows from (21) we first note that (21) implies that\n\u03bb\u03b7 * y \u03b7 \u2265 \u03bb1 * y 1 ,\n1 = y 1 (B) for all j \u2208 B, and thus\nfor any \u03b7 \u2208 [0, 1]. Now, we have for any block B \u2208 D 1 , yi,j\n\n\u03bb0 * y 1 =\n\nX X\n\n0 1\n\u03bbi,j\nyi,j =\n\nB\u2208D 1 j\u2208B\n\nX\n\ny 1 (B)\n\nB\u2208D 1\n\nX\n\n\u03bb0i,j =\n\nj\u2208B\n\nX\n\ny 1 (B)\u03bb1 (B)|B| = \u03bb1 * y 1 ,\n\nB\u2208D 1\n\nP\nP\n\u03b7\n)/|B| = ( j\u2208B \u03bb0i,j )/|B|.\nwhere we recall that \u03bb1 (B) = ( j\u2208B \u03bbi,j\nSo, we can conclude that\nZ 1\nZ 1\nZ 1\n\u03b7\n\u03b7\n1\n1\n\u03bb * y d\u03b7 \u2265\n\u03bb * y d\u03b7 =\n\u03bb0 * y 1 d\u03b7 = \u03bb0 * y 1 = \u03bb * y.\n\u03b7=0\n\n\u03b7=0\n\n\u03b7=0\n\nIn light of the above, it remains to prove (21). To this end, recall that when A\u03b7 and \u03bb\u03b7 are fixed,\n\u03b7\nthe evolution of yi,j\ns is described by Equation (13), thus the statement we need to prove is\nX\n\n(i,j)\u2208A\u03b7\n\n\u0011\n\u0011 \u0010\n\u03bb\u03b7i,j \u0010 \u03b7\n\u2264 0.\nyi,j + \u03b2 * N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j\nwi\n\nPlugging in the expression for N (\u03b7) given by (14) and canceling \u03b1, we need to show that\n\uf8f62\n\uf8f6 \uf8eb\n\uf8f6\n\uf8eb\n\uf8eb\n\u0011\n\u0011\n\u0011\nX 1 \u0010 \u03b7\nX (\u03bb\u03b7i,j )2 \u0010 \u03b7\nX \u03bb\u03b7i,j \u0010 \u03b7\n\uf8ed\nyi,j + \u03b2 \uf8f8 \u2264 \uf8ed\nyi,j + \u03b2 \uf8f8 * \uf8ed\ny + \u03b2 \uf8f8.\nwi\nwi\nwi i,j\n\u03b7\n\u03b7\n\u03b7\n(i,j)\u2208A\n\n(22)\n\n(i,j)\u2208A\n\n(i,j)\u2208A\n\nNow, this inequality follows from the Cauchy-Schwarz\ninequality (a * b)2 \u2264 |a|22 |b|22 , by taking\nr \u03b7 \u0010\n\u0011\n(\u03bbi,j )2\n\u03b7\na to be the vector with entries ai,j =\ny\n+\n\u03b2\n, and b to be the vector with entries\ni,j\nwi\nr \u0010\n\u0011\n\u03b7\nbi,j = w1i yi,j\n+\u03b2 .\n18\n\n\fCharging the movement cost. We turn our attention to the accounting of the movement cost\nof our algorithm. Recall that the movement cost at time t is defined as\n\uf8f6\n\uf8eb\nd\nk\nX\nX\nt\nt\u22121 \uf8f8\nwi \uf8ed\n(23)\n|yi,j\n\u2212 yi,j\n| .\ni=1\n\nj=1\n\nWe would like to approximate this expression by one which is simpler and more convenient to work\nt as in the\nwith. First, instead of keeping track of both increases and decreases of the variables yi,j\nt . That\nabove, we will account for the movement cost only through the increases of the variables yi,j\nis, our bound for the movement cost is\n\uf8f6\n\uf8eb\nd\nk\nX\nX\nt\nt\u22121\nwi \uf8ed\n(24)\nmax{yi,j\n\u2212 yi,j\n, 0}\uf8f8 .\ni=1\n\nj=1\n\nNote that we have for any coordinate (i, j) and t \u2265 1\n\nt\u22121\nt\u22121\nt\u22121\nt\nt\nt\n\u2212 yi,j\n\u2212 yi,j\n|yi,j\n| \u2264 2 * max{yi,j\n, 0} + yi,j\n,\n\u2212 yi,j\n\nand thus on any input sequence consisting of T requests, it is the case that\nT X\nd\nX\nt=1 i=1\n\n\uf8eb\n\nwi \uf8ed\n\nk\nX\nj=1\n\n\uf8f6\n\nt\nt\u22121 \uf8f8\n|yi,j\n\u2212 yi,j\n| \u2264 2*\n\nT X\nd\nX\nt=1 i=1\n\n\uf8eb\n\uf8f6\n\uf8eb\n\uf8f6\nk\nd\nk\nX\nX\nX\nt\nt\u22121\n0\nT \uf8f8\nwi \uf8ed\nmax{yi,j\n\u2212 yi,j\n, 0}\uf8f8 +\nwi \uf8ed\nyi,j\n\u2212 yi,j\n.\nj=1\n\ni=1\n\nj=1\n\nThus, accounting for the movement cost via expression (24) approximates the true movement cost\n(corresponding\nto expression (23)) up to a multiplicative factor of two and an additive factor of\nP\nat most di=1 kwi , which depends only on the starting and final configuration, and is zero if the\ntwo configurations coincide. As we will see, for the sake of our competitive analysis, such an\napproximation suffices.\nNext, similarly to the way we accounted for the hit cost described above, we wish to further\nsimplify the charging of the movement cost and perform it in a continuous and local fashion.\nNamely, it is easy to see that the following quantity\n\uf8eb\n\uf8f6\nZ 1\n\u03b7\nd\nk Z 1\n\u03c4\nX\nX\ndyi,j\ndyi,j\nwi \uf8ed\nmax{\nd\u03c4 +\n, 0}d\u03b7 \uf8f8\nd\u03b7\n\u03c4 =0 d\u03c4\n\u03b7=0\ni=1\n\nj=1\n\n\u03c4\ndyi,j\nd\u03c4 is always non-negative.)\n\u03b7\nA , we can write\n\ncan only overestimate the movement cost given by (24). (Note that\ndy \u03b7\n\nFurthermore, as the derivatives d\u03b7i,j can only be positive if (i, j) \u2208\n( \u03b7\n)\nZ 1\nZ 1\n\u0011\ndyi,j\n1 \u0010 \u03b7\nmax\n, 0 d\u03b7 \u2264\nyi,j + \u03b2 N (\u03b7) * 1(i,j)\u2208A\u03b7 d\u03b7,\nd\u03b7\n\u03b7=0\n\u03b7=0 wi\n\nwhere we used (13) and the fact that, trivially, N (\u03b7) \u2212 \u03bb\u03b7i,j \u2264 N (\u03b7). Thus, we can use the above\nin our final version of the estimate of the movement cost. The following claim summarizes the\ndiscussion.\n19\n\n\fClaim 13. For any sequence of requests of length T \u2265 1,\n\uf8eb\n\uf8f6\nT X\nd\nk\nX\nX\nt\u22121 \uf8f8\nt\nwi \uf8ed\n| \u2264\n|yi,j\n\u2212 yi,j\nt=1 i=1\n\n2*\n\nT X\nd\nX\nt=1 i=1\n\nj=1\n\n\uf8eb\nk Z\nX\nwi \uf8ed\nj=1\n\n1\n\n\u03c4 =0\n\n\u03c4\ndyi,j\nd\u03c4 +\nd\u03c4\n\nZ\n\n1\n\n\u03b7=0\n\n\uf8f6\n\u0010\n\u0011\n1\ny \u03b7 + \u03b2 N (\u03b7) * 1(i,j)\u2208A\u03b7 d\u03b7 \uf8f8 + C \u2032 ,\nwi i,j\n\nP\nwhere C \u2032 \u2264 i kwi depends only on the starting and final configuration of the algorithm, and C \u2032 = 0\nif the two configurations coincide.\nThus, at time t, the movement cost of our algorithm is given by:\n\uf8f6\n\uf8eb\nZ 1\nd\nk Z 1\n\u03c4\n\u0010\n\u0011\nX\nX\ndyi,j\n1\n\u03b7\nwi \uf8ed\nd\u03c4 +\nyi,j\n+ \u03b2 N (\u03b7) * 1(i,j)\u2208A\u03b7 d\u03b7 \uf8f8 .\n\u03c4 =0 d\u03c4\n\u03b7=0 wi\ni=1\n\n3.3\n\n(25)\n\nj=1\n\nCompetitive Analysis\n\nWe are finally ready to bound the competitiveness of our algorithm. To this end, we prove the\nfollowing theorem.\nTheorem 14. Consider an arbitrary instance of the allocation problem with cost vectors h1 , h2 , . . .,\na starting configuration y 0 and a quota pattern \u03ba = (\u03ba(1), \u03ba(2), . . .). For any 0 \u2264 \u03b5 \u2264 1, we have\nthe following bounds:\nH \u2264 (1 + \u03b5) (Opt + wmax * g(k)) + C,\nM \u2264 O(log(k/\u03b5)) * (Opt + wmax * g(\u03ba)) .\nHere, H and M denote the hit and movement costs of our fractional algorithm, and Opt denotes\nthe sum of the total hit and movement\ncosts of a fixed integral optimum solution to the allocation\nP\nproblem instance. Let g(\u03ba) := t |\u03ba(t) \u2212 \u03ba(t \u2212 1)|, and denote by wmax = maxi wi the diameter of\nour metric space. Let C be a quantity that depends only on the start and final configurations of the\nonline algorithm, and C = 0 if the two configurations coincide.\nIt is easy to see that Theorem 5 immediately follows from the above theorem.\nTo prove Theorem 14 we employ a potential function approach. Namely, we define potentials\n\u03a6h (y, t) and \u03a6m (y, t) that depend on the state y of the online algorithm and on the state of some\narbitrary fixed integral optimum solution at time t. Then we show that the following inequalities\nare satisfied at each time step t.\nMt + \u2206\u03a6m\n\u2264 (1 + \u03b5) * \u03b1 * (wmax * |\u03ba(t) \u2212 \u03ba(t \u2212 1)| + M\u2217 t + H\u2217 t ) ,\nt\n1\n\u2264 (1 + \u03b5) (wmax * |\u03ba(t) \u2212 \u03ba(t \u2212 1)| + M\u2217 t + H\u2217 t ) .\nHt + \u2206\u03a6ht + \u2206\u03a6m\nt\n\u03b1\n\n(26)\n(27)\n\nHere, Ht (respectively H\u2217 t ) and Mt (respectively M\u2217 t ) denote the hit and movement costs\nincurred by the algorithm (respectively optimum) at time t. The quantities\n\u2206\u03a6ht := \u03a6h (y t , t) \u2212 \u03a6h (y t\u22121 , t \u2212 1),\n\nm t\nm t\u22121\n\u2206\u03a6m\n, t \u2212 1)\nt := \u03a6 (y , t) \u2212 \u03a6 (y\n\n20\n\n\frespectively denote the change in the potentials \u03a6h and \u03a6m at time step t.\nAs we shall see, it will be the case that \u03a6m (y, t) \u2265 0 and \u03a6m (y 0 , 0) = 0. Moreover, both\nm\n\u03a6 (y, t) and \u03a6h (y, t) will be bounded by some universal constant C, independent of the length of\nthe request sequence. Thus, Theorem 14 will follow by summing up (26) and (27) over all times t.\n(Note that in our algorithm \u03b1 = log(1 + 1/\u03b2) = O(log(k/\u03b5)).)\nThe potential function \u03a6m is defined as follows.\n\uf8eb\n!\uf8f6\nX\nX\n1\n+\n\u03b2\n\u2217t\n\uf8f8.\n\u03a6m (y, t) := (1 + \u03b5) *\nwi \uf8ed\nyi,j\n* log\n\u03b7\nyi,j\n+\u03b2\n\nThe potential functions.\n\ni\n\nj\n\nHere, y \u2217t denotes the configuration of the optimum solution at time t. Note that if the configuration\ny \u2217t is integral and the optimum has ki\u2217 servers at location i at time t, then the contribution of\nlocation i to \u03a6m (y t , t) is\n\uf8eb\n!\uf8f6\nX\n1+\u03b2 \uf8f8\nwi \uf8ed\n.\nlog\nt +\u03b2\nyi,j\n\u2217\nj>ki\n\nSo, roughly speaking, \u03a6m (y t , t) accounts for the excess servers in the online configuration yt\nat location i compared to the optimum solution. For example, suppose that y t has ki servers at\nt = 0 for j \u2264 k , for some k > k \u2217 , and y t = 1 otherwise. Then the contribution\nlocation i, i.e., yi,j\ni\ni\ni\ni,j\nof location i to \u03a6m (y t , t) is O(wi (ki \u2212 ki\u2217 ) log k). Intuitively, the offline adversary can penalize the\nonline algorithm for \"wasting\" ki \u2212 ki\u2217 servers at i, by giving cost vectors at the other locations\n(where the optimum has more servers), and making it pay a larger hit cost. The potential \u03a6m (y t , t)\nwill be used to offset this additional hit cost in such situations.\nNext, we define the potential \u03a6h to be\n\uf8eb\n\uf8f6\nX\n1\nt \uf8f8\n\uf8ed\nwi * yi,j\n.\n\u03a6h (y, t) :=\n\u03b1\ni,j\n\nIt is easily verified that both potentials are bounded. Moreover \u03a6m (y 0 , 0) = 0, as both offline and\nonline are assumed to start with the same initial configuration.\nThe proof plan. Our goal now is to show that Inequalities (26) and (27) always hold. For ease\nof analysis, we will consider the events at time t in three steps, and show that Inequalities (26) and\n(27) hold at each of these steps. The steps are the following:\n1. The quota \u03ba(t) either increases, decreases, or stays unchanged, compared to \u03ba(t \u2212 1), and the\noptimal solution removes/adds servers accordingly.\n2. The optimal solution moves some servers and its state changes from y \u2217t\u22121 to y \u2217t .\n3. The online algorithm changes its state from y t\u22121 to yt : first, we analyze the fix stage then\nwe analyze the hit stage. Also, while analyzing the hit stage, the hit costs of both the online\nalgorithm and the optimal solution are accounted for.\n\n21\n\n\fThe server quota increases/decreases and the optimum removes/adds servers. Note\nthat the only quantities that can change in this step are the movement cost of the optimum and\nthe potential \u03a6m , thus it suffices to prove that Inequality (26) is preserved (in this case, (27) is\nidentical to (26) scaled by 1/\u03b1). Now, there are two cases to consider, depending on how \u03ba(t)\nchanges.\n\u2022 Suppose \u03ba(t) < \u03ba(t \u2212 1), then the optimum has to withdraw |\u03ba(t)\u2212 \u03ba(t \u2212 1)| servers from some\n\u2217(t\u22121)\nlocations. That is, for |\u03ba(t) \u2212 \u03ba(t \u2212 1)| locations (i, j), the corresponding variables yi,j\nare\nm\nset to 1, and as a result these locations start contributing to \u03a6 . Clearly, each such location\n(i, j) increases \u03a6m by at most\n!\n\u0013\n\u0012\n1+\u03b2\n1\n* wmax = (1 + \u03b5)\u03b1 * wmax .\n(1 + \u03b5)wi * log\n\u2264 (1 + \u03b5) ln 1 +\nt\u22121\n\u03b2\nyi,j\n+\u03b2\nAs a result, the total increase of \u03a6m is at most wmax * (1 + \u03b5)\u03b1 * |\u03ba(t) \u2212 \u03ba(t \u2212 1)|, and hence\n(26) holds even without accounting for the movement cost of the optimum.\n\u2022 Suppose \u03ba(t) \u2265 \u03ba(t\u22121), then the optimum can bring in |\u03ba(t)\u2212\u03ba(t\u22121)| servers, and as a result\n\u2217(t\u22121)\n\u03a6m can only decrease, as some terms yi,j\nmay change from 1 to 0, thus not contributing\nm\nanymore to \u03a6 . Hence, (26) also holds in this case.\nThe optimum moves its servers. Without loss of generality, it suffices to analyze the case in\nwhich the optimum moves exactly one server from location i to i\u2032 . (If multiple servers are moved,\nwe can consider these moves one by one.) Also, as before, only \u03a6m and the offline movement cost\nchange, and hence it suffices to just show that (26) holds, and in particular that\n\u2217\n\u2206\u03a6m\nt \u2264 (1 + \u03b5)\u03b1M t .\n\nSuppose that location i had j servers prior to the movementand this number is reduced to j \u2212 1\n(recall that by our convention we account only for the movement cost corresponding to withdrawal\nof servers). Then, the contribution of location i to \u03a6m increases by precisely\n!\n1+\u03b2\nwi (1 + \u03b5) * log\n\u2264 wi (1 + \u03b5) * \u03b1 = (1 + \u03b5)\u03b1M\u2217 t .\nt\u22121\nyi,j\n+\u03b2\nIn contrast, increasing the number of servers at i\u2032 can only decrease \u03a6m . Thus, we get that the\ndesired inequalities hold in this case.\nThe online algorithm is executed. The case in which the online algorithm changes its distribution y t is the most interesting, and we analyze Inequalities (26) and (27) separately. Recall that\nour online algorithm works in two steps: the fix stage and the hit stage, and hence we consider\nthese separately. Moreover, as the evolution of the algorithm is described in a continuous manner,\nwe will analyze Inequalities (26) and (27) in such a manner too.\n\n22\n\n\fThe fix stage: proof of Inequality (26).\nstage it suffices to prove that for any \u03c4 ,\n\nTo show that Inequality (26) holds during the fix\n\ndMt d\u03a6m\n+\n\u2264 0.\nd\u03c4\nd\u03c4\n\n(28)\n\ndy \u03c4\n\nBy definition of d\u03c4i,j during the fix stage and our way of accounting for the movementcost (cf.\nClaim 13) we have\ndMt\n=\nd\u03c4\n\nX\n\nwi *\n\n(i,j)\u2208A\u03c4\n\n\u0001\n1\n\u03c4\nyi,j\n+\u03b2 =\nwi\n\nX\n\n(i,j)\u2208A\u03c4\n\n\u0001\n\u03c4\nyi,j\n+\u03b2 .\n\n(Recall that A\u03c4 is the set of active coordinates (i, j), i.e. those for which yi,j < 1.) Also, it is easy\nto see that the change in the potential function \u03a6m is\nX\nd\u03a6m\n\u2217t\n= \u2212(1 + \u03b5)\nyi,j\n.\nd\u03c4\n\u03c4\n(i,j)\u2208A\n\nNext we need to prove the following claim\nClaim 15. Consider a subset A of the coordinates and two configurations y and y \u2032 with\nP\n\u2032 \u2265 kd \u2212 k, we have then that\n1, and i,j yi,j\nX\n\n(yi,j + \u03b2) \u2212 (1 + \u03b5)\n\n(i,j)\u2208A\n\nX\n\n\u2032\nyi,j\n\u2264\n\n(i,j)\u2208A\n\nX\n\nyi,j \u2212\n\n(i,j)\u2208A\n\nX\n\nP\n\n\u2032\n(i,j)\u2208A yi,j\n\n\u2265\n\n\u2032\nyi,j\n.\n\n(i,j)\u2208A\n\nBefore we prove this claim,\nlet us describe how (28) follows from it. If we set y = y \u03c4 , y\u2032 = y \u2217t ,\nP\n\u2217t = kd \u2212 \u03ba(t) \u2265 kd \u2212 k. Furthermore, since (i, j) \u2208\nand A = A\u03c4 , then, clearly, i,j yi,j\n/ A\u03c4 only if\nP\n\u03c4\n\u03c4\nyi,j = 1, and as we apply the fix stage only if i,j yi,j < kd \u2212 \u03ba(t), we need to have |A\u03c4 | > \u03ba(t),\nand thus\nX\n\u2217t\nyi,j\n\u2265 kd \u2212 \u03ba(t) \u2212 |A\u03c4 | \u2265 1.\n(i,j)\u2208A\n\nSo, both requirements of the claim are satisfied and it follows that\ndMt d\u03a6m\n+\n=\nd\u03c4\nd\u03c4\n\nX\n\n\u03c4\n(yi,j\n+ \u03b2) \u2212 (1 + \u03b5)\n\n(i,j)\u2208A\u03c4\n\nX\n\n(i,j)\u2208A\u03c4\n\n\u2217t\nyi,j\n\u2264\n\nX\n\n(i,j)\u2208A\u03c4\n\n\u03c4\nyi,j\n\u2212\n\nX\n\n\u2217t\nyi,j\n\u2264 0,\n\n(29)\n\n(i,j)\u2208A\u03c4\n\nP\nP\n\u03c4 < kd \u2212 \u03ba(t) =\n\u2217t\n\u03c4 =1\nwhere the last inequality follows, as i,j yi,j\n/ A\u03c4 only if yi,j\ni,j yi,j , and (i, j) \u2208\nP\nP\n\u2217t\n\u03c4\nimplies that (i,j)\u2208A\u03c4 yi,j \u2264 (i,j)\u2208A\u03c4 yi,j .\nP\nP\n\u2032 \u2265 kd \u2212 k and\n\u2032\nProof of Claim 15. As i,j yi,j\n(i,j)\u2208A yi,j \u2265 1, we have that\nX\n|A|\n|A|\n\u2032\n\u2264\n\u2212\nyi,j\n\u2212 max{|A| \u2212 k, 1} \u2264 0,\nk+1\nk+1\n\n(30)\n\n(i,j)\u2208A\n\nwhere the last inequality follows as\n\n|A|\nk+1\n\n< 1 when |A| \u2264 k, and\n\n23\n\n|A|\nk+1\n\n\u2264 |A| \u2212 k when |A| \u2265 k + 1.\n\n\fNow, the claim is proved by noticing that\n\uf8eb\n\uf8eb\n\uf8f6\n\uf8f6\nX\nX\nX\nX\nX\n|A|\n\u2032\n\u2032 \uf8f8\n\u2032 \uf8f8\n(yi,j + \u03b2) \u2212 (1 + \u03b5)\nyi,j\n= \u03b5\uf8ed\nyi,j \u2212\nyi,j\n\u2212\nyi,j\n+ \uf8ed\nk+1\n(i,j)\u2208A\n(i,j)\u2208A\n(i,j)\u2208A\n(i,j)\u2208A\n(i,j)\u2208A\nX\nX\n\u2032\n\u2264\nyi,j \u2212\nyi,j ,\n(i,j)\u2208A\n\n(i,j)\u2208A\n\nwhere the inequality follows by (30).\nThe fix stage, proof of Inequality (27). To prove that Inequality (27) holds during the fix\nstage as well, we note that as we are accounting for the hit cost during the hit stage, currently\nthere is no hit cost incurred, and hence we just need to show that\n1 d\u03a6m\nd\u03a6h\n+\n= 0.\nd\u03c4\n\u03b1 d\u03c4\nObserve that\n\nd\u03a6h\n1\n=\nd\u03c4\n\u03b1\n\nX\n\nwi\n\n(i,j)\u2208A\u03c4\n\n(31)\n\ndyi,j\nd\u03c4\n\nwhich is exactly 1/\u03b1 times our accounting for the movement cost in the fix stage (cf. Claim 13).\nTherefore, (31) is identical to (29) (up to a scaling by 1/\u03b1), and hence follows from the above proof.\nThe hit stage, proof of Inequality (27). Recall that both the optimal solution and the online\nalgorithm incur a hit cost in this stage. We start by proving that Inequality (27) holds during this\nstage \u2013 later, we will analyze Inequality (26). We need to show that\ndHt d\u03a6ht\n1 d\u03a6m\ndH \u2217\nt\n+\n+\n\u2264 (1 + \u03b5) t .\nd\u03b7\nd\u03b7\n\u03b1 d\u03b7\nd\u03b7\n\n(32)\n\nFirst, we note that by our way of accounting for the hit cost of the algorithm (cf. (16))\ndHt X \u03b7 \u03b7\n=\n\u03bbi,j yi,j\nd\u03b7\n\nd\u03a6ht\n1\n=\nd\u03b7\n\u03b1\n\nand\n\ni,j\n\nX \u0010\n\n(i,j)\u2208A\u03b7\n\n\u0011\n\u0011 \u0010\n\u03b7\nyi,j\n+ \u03b2 * N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j .\n\nNow, we have that\ndHt d\u03a6ht\n+\nd\u03b7\nd\u03b7\n\n=\n\nX\n\n\u03b7\n\u03bb\u03b7i,j yi,j\n+\n\ni,j\n\n=\n\nX\n\n1\n\u03b1\n\n(i,j)\u2208A\u03b7\n\n\u03b7\n\u03bb\u03b7i,j yi,j\n+\n\n(i,j)\u2208A\n/ \u03b7\n\n\u2264\n\nX\n\n(i,j)\u2208A\n/ \u03b7\n\n\u0011\n\u0011 \u0010\nX \u0010 \u03b7\nyi,j + \u03b2 * N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j\n\n\u03b7\n+\n\u03bb\u03b7i,j yi,j\n\n1\n\u03b1\n1\n\u03b1\n\nX\n\n\u03b7\n(yi,j\n+ \u03b2) * N (\u03b7) \u2212\n\n(i,j)\u2208A\u03b7\n\nX\n\nX\n\n\u03b2\u03bb\u03b7i,j\n\n(i,j)\u2208A\u03b7\n\u03b7\n+ \u03b2) * N (\u03b7) =\n(yi,j\n\n(i,j)\u2208A\u03b7\n\n24\n\nX\n\n(i,j)\u2208A\n/ \u03b7\n\n\u03b7\n+\n\u03bb\u03b7i,j yi,j\n\n1 dMt\n,\n\u03b1 d\u03b7\n\n\fwhere in the last step we used the expression for our movement cost accounting (cf. Claim 13),\n\u0011\nX \u0010 \u03b7\ndMt\n=\nyi,j + \u03b2 * N (\u03b7).\n(33)\nd\u03b7\n\u03b7\n(i,j)\u2208A\n\nThus, to establish (32), it suffices to show that\nX\n1 dMt\n1 d\u03a6m\ndH \u2217\n\u03b7\nt\n\u03bb\u03b7i,j yi,j\n+\n+\n\u2264 (1 + \u03b5) t .\n\u03b1 d\u03b7\n\u03b1 d\u03b7\nd\u03b7\n\u03b7\n\n(34)\n\n(i,j)\u2208A\n/\n\n\u03b7\nBy our update rule for yi,j\n, the derivative of \u03a6m\nt is\n\nd\u03a6m\nt\nd\u03b7\n\n= \u2212(1 + \u03b5)\n\nX\n\n(i,j)\u2208A\u03b7\n\n\u0011\n\u0010\n\u2217t\nyi,j\n* N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j .\n\nFinally, by our way of accounting for the hit cost of the optimal solution (cf. (15)) we have\ndH \u2217t X \u03b7 \u2217t\n=\n\u03bbi,j yi,j .\nd\u03b7\ni,j\n\nThus, after multiplying Inequality (34) by \u03b1 and plugging in the above equalities, we need to prove\nthat\n\u0011\n\u0011\n\u0010\nX\nX \u03b7\nX \u0010 \u03b7\nX\n\u03b7\n\u2217t\n\u2217t\n\u03b1\n\u03bbi,j yi,j\n\u2264 0.\n\u03bb\u03b7i,j yi,j\n+\nyi,j + \u03b2 *N (\u03b7)\u2212(1+\u03b5)\nyi,j\n* N (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j \u2212(1+\u03b5)\u03b1\n(i,j)\u2208A\n/ \u03b7\n\n(i,j)\u2208A\u03b7\n\ni,j\n\n(i,j)\u2208A\u03b7\n\nA\u2032\n\nLet\nbe the set of all coordinates that are either active or have\n\u03b7\nhas yi,j\n\u2208 {0, 1}, we observe that:\n\n\u03b7\nyi,j\n\n(35)\n= 0. As any inactive coordinate\n\n\u03b7\n\u2022 If (i, j) \u2208 A\u2032 \\ A\u03b7 , then it must be that yi,j\n= 0 and \u03b1\u03bb\u03b7i,j \u2265 N (\u03b7). This holds, as a coordinate\n\u03b7\nfor which yi,j\n= 0 can be inactive only if\n\n\u03b7\ndyi,j\nd\u03b7\n\n\u03b7\n= (yi,j\n+ \u03b2)(N (\u03b7) \u2212 \u03b1\u03bb) \u2264 0.\n\n\u03b7\n\u2022 If (i, j) \u2208\n/ A\u2032 it must be that yi,j\n= 1 and \u03b1\u03bb\u03b7i,j < N (\u03b7). This holds, as a coordinate for which\n\u03b7\nyi,j\n= 1 can be inactive only if\n\n\u03b7\ndyi,j\nd\u03b7\n\n\u03b7\n= (yi,j\n+ \u03b2)(N (\u03b7) \u2212 \u03b1\u03bb) > 0.\n\nThus, using the above observations, we may rewrite (35) and get that\n\uf8eb\n\n\uf8f6\n\u0011\nX \u0010 \u03b7\nX\nX\nX\n\u03b7\n\u2217t \uf8f8\n\u2217t\nN (\u03b7) * \uf8ed\nyi,j + \u03b2 \u2212 (1 + \u03b5)\nyi,j\n\u2212 (1 + \u03b5)\u03b1\n\u03bb\u03b7i,j yi,j\n+\u03b1\n\u03bb\u03b7i,j yi,j\n(i,j)\u2208A\u03b7\n\n\uf8eb\n\n(i,j)\u2208A\u03b7\n\n(i,j)\u2208A\u2032\n\n\u2264 N (\u03b7) * \uf8ed\n\n(i,j)\u2208A\u2032\n\n(i,j)\u2208A\u2032\n\n(i,j)\u2208A\u2032\n\n(i,j)\u2208A\u2032\n\n\uf8eb\n\n(i,j)\u2208A\n/ \u03b7\n\n(i,j)\u2208A\n/ \u03b7\n\n\u0011\nX \u0010 \u03b7\nX\nX \u03b7\nX\n\u03b7\n\u2217t \uf8f8\n\u2217t\nyi,j + \u03b2 \u2212 (1 + \u03b5)\nyi,j\n\u2212 (1 + \u03b5)\u03b1\n\u03bbi,j yi,j\n+\u03b1\n\u03bb\u03b7i,j yi,j\n\n\u2264 N (\u03b7) * \uf8ed\n\uf8eb\n\n\uf8f6\n\n(i,j)\u2208A\u2032\n\n\uf8f6\n\n(i,j)\u2208A\n/ \u2032\n\n\u0011\n\u0011\nX \u0010 \u03b7\nX\nX \u03b7 \u0010 \u03b7\n\u2217t \uf8f8\n\u2217t\nyi,j + \u03b2 \u2212 (1 + \u03b5)\nyi,j\n+\u03b1\n\u03bbi,j yi,j \u2212 yi,j\n(i,j)\u2208A\n/ \u2032\n\n\uf8f6\n\u0011\n\u0011\nX \u0010 \u03b7\nX\nX \u0010 \u03b7\n\u2217t\n\u2217t \uf8f8\nyi,j + \u03b2 \u2212 (1 + \u03b5)\nyi,j\n+\n.\nyi,j \u2212 yi,j\n\u2264 N (\u03b7) * \uf8ed\n25\n\n(i,j)\u2208A\n/ \u2032\n\n(i,j)\u2208A\n/ \u03b7\n\n\fThe first inequality follows as (i, j) \u2208 A\u2032 \\ A\u03b7 , then it must be that \u03b1\u03bb\u03b7i,j \u2265 N (\u03b7), and because\n\u03b7\nA\u03b7 \u2286 A\u2032 . The second inequality follows since for all (i, j) \u2208 A\u2032 \\ A\u03b7 it must be that yi,j\n= 0.\nThe third inequality follows as \u03b1\u03bb\u03b7i,j < N (\u03b7) for (i, j) \u2208\n/ A\u2032 , and by the observation that for each\n\u03b7\n\u2217t .\n(i, j) \u2208\n/ A\u2032 , yi,j\n= 1 \u2265 yi,j\nIf N (\u03b7) = 0, then the above expression equals 0, and we are done. Otherwise, if N (\u03b7) > 0, we\nget that\nX \u0010\n\n(i,j)\u2208A\u2032\n\n\u2264\n\n\u0011\n\u0011\nX\nX \u0010 \u03b7\n\u03b7\n\u2217t\n\u2217t\nyi,j\n+ \u03b2 \u2212 (1 + \u03b5)\nyi,j\n+\nyi,j \u2212 yi,j\n\nX\n\n\u03b7\nyi,j\n\u2212\n\n(i,j)\u2208A\u2032\n\n=\n\nX\n\n(i,j)\n\n\u03b7\n\u2212\nyi,j\n\nX\n\n(i,j)\u2208A\u2032\n\n\u2217t\nyi,j\n\n(i,j)\u2208A\u2032\n\nX\n\n\u2217t\nyi,j\n= 0.\n\n(i,j)\u2208A\n/ \u2032\n\n\u0011\nX \u0010 \u03b7\n\u2217t\n+\nyi,j \u2212 yi,j\n\n(36)\n\n(i,j)\u2208A\n/ \u2032\n\n(37)\n\n(i,j)\n\n\u00bfFrom Lemma 10 we know that\nInequality (36) follows from Claim 15 by the following arguments.\nP\n\u03b7\nhas to be positive. Otherwise,\nthe set A\u03b7 is non-empty. We claim that this implies that (i,j)\u2208A\u2032 yi,j\n\u03b7\nall the active coordinates would have yi,j = 0, and thus could only increase, contradicting the fact\nP dy\u03b7\nP\nP\ndy \u03b7\n\u03b7\nthat when N (\u03b7) > 0, i,j d\u03b7i,j = (i,j)\u2208A\u03b7 d\u03b7i,j = 0. Moreover, as N (\u03b7) > 0, i,j yi,j\nis equal\nP\n\u03b7\n\u2217t\n\u2032\nto\ny = kd \u2212 \u03ba(t) (and thus is integral). Now, as yi,j = 1 if (i, j) \u2208\n/ A , we have that\nP\nP\nP i,j \u2217t\n\u03b7\n\u03b7\ny\nis\nintegral\nas\nwell.\nAs\nthis\nquantity is positive\ny\nand\nthat\ny\n\u2265\n(i,j)\u2208A\u2032 i,j\n(i,j)\u2208A\u2032 i,j\n(i,j)\u2208A\u2032 i,j\nby the argument above, we get that\nX\nX \u03b7\n\u2217t\nyi,j\n\u2265\nyi,j \u2265 1.\n(38)\n(i,j)\u2208A\u2032\n\n(i,j)\u2208A\u2032\n\nThus, we can use Claim 15 with y = y\u03b7 , y \u2032 = y \u2217t and A = A\u2032 (as all the requirements of this\nclaim\nare satisfied). The\nP\nP last\u2217t equality follows, since for N (\u03b7) > 0, we get from the algorithm,\n\u03b7\ni,j yi,j = kd \u2212 \u03ba(t) =\ni,j yi,j .\nThe hit stage, proof of Inequality (26).\n\nTo show that (26) holds, we need to show that\n\ndMt\nd\u03a6m\ndH \u2217t\nt\n+\n\u2264 (1 + \u03b5)\u03b1 *\n.\nd\u03b7\nd\u03b7\nd\u03b7\n\n(39)\n\nHowever, this follows directly by noting that the above is simply Inequality (34) after removing the\nfirst (non-negative) term and scaling by \u03b1.\n\n4\n\nFractional k-server on Weighted HSTs\n\nIn this section, we show how the fractional allocation algorithm on a weighted star can be used as\na building block to obtain a fractional k-server solution on a weighted HST. In particular, we prove\nthe following.\nTheorem 6. Let T be a weighted \u03c3-HST with depth l. If, for any 0 \u2264 \u03b5 \u2264 1, there exists a\n(1 + \u03b5, log(k/\u03b5))-competitive algorithm for the fractional allocation problem on a weighted star, then\n26\n\n\fthere is an O(l log(kl))-competitive algorithm for the fractional k-server problem on T , provided\n\u03c3 = \u03a9(l log(kl)).\nTo this end, we focus on a particular weighted \u03c3-HST T and show how to construct a fractional\nk-server algorithm on it. Roughly speaking, the construction works as follows. Each internal node\np of T will run a number of instances of the allocation problem which differ with respect to their\nquota patterns, but have the same hit cost vectors. These instances are maintained as a convex\ncombination. The fractional solutions to the different instances, which we compute online using\nthe fractional allocation algorithm, determine in a recursive manner how the servers available at\neach node are distributed among its children.\nWhile this approach is similar to the approach of Cot\u00e9 et al. [16], the main difference here is\nthat we can use the (much weaker) fractional allocation problem instead of using a randomized\n(integral) algorithm for the allocation problem.\nLet us denote by r the root of our \u03c3-HST T . Recall that for a node p of T , T (p) denotes the\nsubtree rooted at p, W (p) is the length of the edge connecting p to its parent, and w(p, i) denotes\nthe length of the edge connecting p to its child pi . By the definition of a weighted \u03c3-HST, we have\nW (p) \u2265 \u03c3w(p, i) for all children i of p, unless p is either a leaf or the root.\nRecall that the input to the fractional allocation problem running at node p consists of the\nquota pattern \u03ba = (\u03ba(1), \u03ba(2), . . .) specifying the number of servers \u03ba(t) available at each time t,\nand the hit cost vectors ht that arrive at each time t at location it . The output of an algorithm for\nthe fractional allocation problem specifies a fractional solution xt that provides a distribution on\nthe number of servers at each location pi , subject to the aggregate bound of \u03ba(t) on the (expected)\nnumber of servers.\nNow, let us fix some instance \u03c1 of the k-server problem on the leaves of T . Let \u03c1 = (\u03c1(1), \u03c1(2), . . .)\nbe the request sequence, where \u03c1(t) denotes the leaf requested at time t.\nDefinition 16. For a node p, integer j and time t, let Optcost(p, j * ~1, t) be the optimum cost for\nserving the k-server instance {\u03c1(1), . . . , \u03c1(t)} \u2229 T (p), i.e. the request sequence \u03c1 restricted to the\nleaves of T (p) until time t, subject to the constraint that exactly j servers are available.\nRemark: Optcost is well defined only with respect to an initial configuration, which we will\nalways assume to be the initial starting positions of the servers at t = 0. Also, we use the notation\nOptcost(p, j * ~1, t), instead of just Optcost(p, j, t), as we will later extend the definition of Optcost\nto the case in which j can vary with time. For now, we only consider fixed j.\n\n4.1\n\nThe algorithm\n\nIn this section we define the ensemble of allocation problems that will be running at each of the\ninternal nodes of T . To do this, we have to define how the hit-cost vectors and the quota patterns\nare generated. Consider some internal node p. As mentioned earlier, each internal node p will run\nseveral instances of the allocation problem that are different with respect to their quota pattern.\nIt will also hold a convex combination over these instances. All instances will have the same hit\ncost vector that will be defined later. We denote the convex combination over allocation instances\non node p at time t by \u039btp . \u039btp is specified via the collection\n\u039btp = {(\u03bbtp,s , \u03batp,s , Hpt )}s ,\n\n\u2200t, p,\n\nX\ns\n\n27\n\n\u03bbtp,s = 1\n\n\fHere \u03bbtp,s determines the fraction at time t given to the instance with quota pattern \u03batp,s (until\ntime t), and Hpt = {h1p , h2p , . . . , htp } is the sequence of hit cost vectors that have appeared until\ntime t. As we will see shortly, the hit cost vector will be the same for all instances s \u2208 \u039btp and\ntherefore there is no subscript s to the hit cost. We will use the notation s \u2208 \u039btp to index the triples\n(\u03bbtp,s , \u03batp,s , Hpt ) in \u039btp . As we shall see later, the convex combination \u039btp will be a refinement of the\nconvex combination \u039bt\u22121\np , for every t.\nTo complete the description of the fractional k-server algorithm we need to define \u039btp for each\nnode p and time t, and show how the fractional number of servers at the leaves of T is computed.\nWe begin with defining how the hit costs htp are generated for each node p.\nHit costs: Consider any internal node p. Let p1 , . . . , pd be the children of p. For the allocation\nproblems running at p, at time t we give the hit cost vector\nhtpi (j) = Optcost(pi , j * ~1, t) \u2212 Optcost(pi , j * ~1, t \u2212 1).\nAs Cot\u00e9 et al. [16] prove, the cost vectors htpi have the desired monotonicity property, i.e., htpi (0) \u2265\nhtpi (1) \u2265 . . . \u2265 htpi (k) for each i and time t. The following crucial observation follows directly from\nthe definition of the k-server problem.\nObservation 17. Consider subtree T (p) and request \u03c1(t). If \u03c1(t) \u2208 T (pi ), then\n1. htp (i, 0) = \u221e. (This follows since any 0-server solution is infeasible for any instance with one\nor more requests, or equivalently incurs infinite cost.).\n2. htp (i\u2032 , j) = 0 for all i\u2032 6= i and for all j. (This follows simply since the request is not in the\nsub-tree of pi\u2032 for i\u2032 6= i.).\nThis completes the description of the cost vectors of node p. We next define the quota patterns\n\u03bap,s (t) for the various allocation instances running at node p.\nQuota patterns: The quota patterns are determined recursively in a top down manner over\nthe tree T (and inductively over time) by the fractional solutions of the allocation instances that\nare generated at each node. To specify how these patterns evolve, we describe below a procedure\nfor updating both the quota patterns \u03batp,s and the convex combination \u03bbtp,s , associated with the\nallocation instances maintained.\nBase case:\n1. At the root r of the tree T there is a single allocation instance running with a quota of k at\nall times. That is, \u039btr consists of a single allocation instance (with fraction 1), hit costs as\ndescribed above, and \u03ba = k * ~1.\n2. For any internal node p \u2208 T and time t = 0, \u039b0p consists of a single allocation instance (with\nfraction 1). The quota pattern \u03bap,s for this single instance s, until time t = 0, is simply the\nnumber of servers present initially at the leaves of subtree T (p). Moreover, there is no hit\ncost thus far.\n\n28\n\n\fin a top\nThe inductive step: Consider time t. We describe the procedure to obtain \u039btp from \u039bt\u22121\np\nt\ndown manner on the tree as follows. As the base case, recall that \u039br has already been determined\nfor all t. Arguing inductively top down on the tree, suppose that \u039btp has already been determined.\nThen, for the children p1 , . . . , pd of p, we determine \u039btpi as follows.\nConsider the allocation instances that are executed at node p. Let {xti,j,s }i,j,s be the fractional\nsolutions generated (by the allocation instances) at time t. The algorithm will maintain the following consistency between the quota for servers available at pi and what the allocation problems\nrunning at the parent p determine. In particular,\nX\n\n(Consistency)\n\n\u03bbts =\n\nX\n\n\u03bbts xi,j,s , xti,j ,\n\n(40)\n\ns\u2208\u039btp\n\ns\u2208\u039btpi | \u03bats (t)=j\n\nAlso, for each child pi it should maintain\nX\n\n(Convex combination)\n\n\u03bbts = 1.\n\n(41)\n\ns\u2208\u039btpi\nt\nt\nSuppose that xt\u22121\ni,j changes to xi,j due to the execution of the allocation instances s \u2208 \u039bp at\ntime t. We show how to update \u039btpi from \u039bt\u22121\npi such that it remains consistent with (40) and (41).\nThis update will be done in a natural (and cost-efficient) way.\nConsider first the cost paid by the convex combination of the allocation instances running at\nnode p. The cost is\n\nXX\ns\n\ni\n\nw(p, i)\u03bbts\n\nk X\u0010\nk X\n\u0011\n\u0011\nX\u0010\nX\nX\nX\nt\u22121\nt\n,\nxti,l,s \u2212 xt\u22121\n\u03bbts\nxi,l,s \u2212 xi,l,s \u2265\nw(p, i)\ni,l,s\nj=1 l<j\n\nj=1\n\ni\n\ns\n\n(42)\n\nl<j\n\nP\nP\nwhere the inequality follows, since for any non-negative numbers pi , i pi |ai | \u2265 | i pi ai |.\nt\nWe note that the change from xt\u22121\ni,j to xi,j can be decomposed into a collection of elementary\nmoves in which \u00b1\u03b4(i, j) units of mass are removed from xi,j and put on xi,j\u00b11 , such that the total\nfractional movement cost remains the same. Thus, we can assume without loss of generality that\nxti,j and xt\u22121\ni,j differ by an elementary move.\nt\nt\nConsider an elementary move where xti,j = xt\u22121\ni,j \u2212 \u03b4 and xi,j\u22121 = xi,j\u22121 + \u03b4 (all other types of\nelementary moves are handled analogously). To implement this, we choose an arbitrary \u03b4 measure\nof allocation problems s \u2208 \u039bt\u22121\npi with \u03bas (t \u2212 1) = j and set \u03bas (t) to j \u2212 1. For all other \u03bas , we\nset \u03bas (t) = \u03bas (t \u2212 1). After all entries are updated by applying the elementary moves, \u03bas (t) is\ndetermined.\nIt is clear that this update rule maintains both (40) and (41). This completes the procedure for\nobtaining \u039bti from \u039bt\u22121\ni .\nObtaining the fractional k-server solution: To complete the description of our algorithm we\nshould describe how to determine the fractional number of servers at each leaf q at each time t.\nThis is determined in a natural way using the following observation. Consider a leaf q and let p be\nits parent. Then,\nz(q, t) :=\n\nX\n\n\u03bbts\n\ns\u2208\u039btp\n\n29\n\nX\nj\n\nj * xtq,j,s\n\n\fis the number of servers at q at time t. Here, xtq,j,s is the probability of having j servers at q at\ntime t, when the fractional allocation algorithm is applied to the allocation instance s \u2208 \u039btp .\n\n4.2\n\nFeasibility\n\nWe first note that our fractional k-server solution is feasible since it satisfies the following.\nLemma 18. Whenever there is a k-server request \u03c1(t), then there is at least one server unit at the\nlocation \u03c1(t), i.e. z\u03c1(t),t \u2265 1. This holds provided the total cost incurred by the allocation problems\nis finite.\nProof. The lemma follows by the way the hit costs are generated. Suppose leaf q is requested at\ntime t, and q is the i-th child of its parent p. Then, by observation 17 (part 1), the hit cost entry\nht (i, 0) for every allocation instance running at p is \u221e. Thus, if the total cost of the allocation\nthat xtq,0,s = 0. Since,\nproblems\nis finite, it must P\nbe that for each s \u2208 \u039btp , the algorithmPensures P\nP t\nt\nt\ns\u2208\u039btp \u03bbs\nj xq,j,s = 1 for all s, and\ns\u2208\u039btp \u03bbs = 1, it follows that z(q, t) =\nj j * xq,j,s \u2265 1.\nRemark: Lemma 18 assumes that the total cost of the allocation problems is finite. Later on\nwe show that the cost is in fact bounded by at most a polylogarithmic factor from the optimal\nk-server cost, and hence finite.\n\n4.3\n\nPerformance analysis\n\nWe first show that the cost of the fractional k-server solution we generate (at the leaves of the tree)\nis at most the total convex combination cost of the allocation instances running on T . For a node\np (not necessarily a leaf) in T , let z(p, t) denote the total (fractional) number of servers at time t\nat the leaves of the subtree T (p). The cost of the k-server solution is\nXX\nW (p)|z(p, t) \u2212 z(p, t \u2212 1)|.\nt\n\np\n\nLemma 19. The movement cost incurred by the fractional k-server solution is no more than the\ntotal movement cost incurred by the convex combination of the allocation instances running on\ninternal nodes of T .\nP\nProof. First, we claim that z(p, t) = s\u2208\u039btp \u03bbts \u03bats (t). This follows from the consistency relation (40)\n\nt\nwe maintain, and our procedure for generating \u039btp from \u039bt\u22121\np . In particular, \u03bas (t) is the number of\nservers available for the fractional allocation instance s running\nPatPp. Since the solution produced\nby the fractional allocation algorithm on this instance satisfies i j j * xti,j,s = \u03bats (t),7 this implies\nthat\nX\nX\nXX\nXX\nX\nX X\n\u03bbts \u03bats (t) =\nj * xti,j,s =\nj\u03bbts\u2032 =\n\u03bbts\n\u03bbts\u2032 \u03bats\u2032 (t).\ns\u2208\u039btp\n\ns\u2208\u039btp\n\ni\n\nj\n\ni\n\nj\n\ns\u2032 \u2208\u039btpi |\u03bats\u2032 (t)=j\n\ni\n\ns\u2032 \u2208\u039btpi\n\n7\nNote that when we designed the fractional allocation algorithm in Section 3, we allowed it to deploy\nP P less servers\nthan the current quota. As a result, when applying this algorithm here we could sometimes have i j j * xti,j,s <\n\u03bats (t). To see that our analysis is still valid in this case, it suffices to consider a modified version of the tree T .\nIn this version, each non-leaf node p of T would have a dummy leaf ip (that will never be requested in our input\nsequence) added as its child and set the length w(p, ip ) of the corresponding edge to 0. Now, we would just make\neach instance of the fractional allocation run at each such p deposit any unused quota of servers at the leaf ip . Note\nthat as w(p, ip ) = 0, this depositing would not incur any additional movement cost and that the modified tree would\nstill be a weighted \u03c3-HST.\n\n30\n\n\fThe second equality\nfrom (40). Applying this\nand noting that z(q, t) for\nPiteratively,\nP abovet follows\nt\nt\nt\na leaf q is simply s\u2032 \u2208\u039btq \u03bbs\u2032 \u03bas\u2032 (t), it follows that z(p, t) = s\u2208\u039btp \u03bbs \u03bas (t).\nSuppose that \u03b4 = z(p, t) \u2212 z(p, t \u2212 1) > 0 server units are removed from the subtree T (p) (the\n\u2032\ncase when \u03b4 < 0 is P\nanalogous).\nP Let pt be the parent of p (note that p 6= r, since z(r, t) = k for\nt\nall t). As z(p, t) = s\u2208\u039bt \u2032 \u03bbs j j * xp,j,s, it follows that the allocation algorithm running on the\np\n\ninstances s \u2208 \u039btp\u2032 will incur a movement cost of at least\nW (p)\n\nX\n\ns\u2208\u039btp\u2032\n\n\u03bbts\n\nk X\nX\n(xtp,l,s \u2212 xt\u22121\np,l,s )\n\n\u2265 W (p) *\n\nj=1 l<j\n\nX\n\n\u03bbts\n\nX\n\n\u03bbts\n\nX\n\n\u03bbts\n\nj=1 l<j\n\ns\u2208\u039btp\u2032\n\n= W (p) *\n\ns\u2208\u039btp\u2032\n\nwhere we used the fact that k *\n\nPk\n\nt\nj=0 xp,j,s\n\nk\nX\n(k \u2212 j)(xtp,j,s \u2212 xt\u22121\np,j,s )\nj=0\n\ns\u2208\u039btp\u2032\n\n= W (p) *\n\nk X\u0010\n\u0011\nX\nxtp,l,s \u2212 xt\u22121\np,l,s\n\nk\nX\n(\u2212j)(xtp,j,s \u2212 xt\u22121\np,j,s )\nj=0\n\n= W (p) * |z(p, t) \u2212 z(p, t \u2212 1)|,\nP\n= k = k * kj=0 xt\u22121\np,j,s .\n\nGiven Lemma 18 and 19 above, it suffices to consider the total movement cost incurred by the\nallocations instances running on the tree T and compare it with the optimum k-server cost. This\nwill be our goal in the following. We begin by defining a notion of optimum k-server cost on a\nweighted \u03c3-HST T when k varies over time.\nDefinition 20. Let T (p) be the subtree rooted at p, and let \u03ba be a quota pattern. We define\nOptcost(p, \u03ba, t) as the optimum cost of serving the request sequence \u03c1 \u2229 T (p) until time step t\nsubject to the constraint that \u03ba(t\u2032 ) servers are available at each time t\u2032 , for 1 \u2264 t\u2032 \u2264 t.\nWe should be precise about the meaning of a k-server solution on T in the case \u03ba(t) can vary.\nFirst, at any time t\u2032 there should be one server unit at the requested location \u03c1(t\u2032 ). The cost of\nthe solution is the total movement cost of the servers. The servers are always located on the leaves\nof T . At time t, when the number of servers changes from \u03ba(t \u2212 1) to \u03ba(t), we will require that\n\u03ba(t) \u2212 \u03ba(t \u2212 1) servers enter (or leave, if \u03ba(t)\nP < \u03ba(t \u2212 1)) from the root of T .\nFor a vector \u03ba, let us define g(\u03ba, t) = tt\u2032 =1 |\u03ba(t\u2032 ) \u2212 \u03ba(t\u2032 \u2212 1)|. The following is a simple but\nvery useful fact about Optcost, that we will need.\nLemma 21. Let p be an internal node in T with children p1 , . . . , pd . For any k-server request\nsequence \u03c1 on the leaves of T and any quota pattern vector \u03ba, the following recurrence holds.\n!\nd\nd\nX\nX\ng(\u03bai , t) .\n(43)\nOptcost(pi , \u03bai , t) + w(p, i)\nOptcost(p, \u03ba, t) =\nmin\nP\n\u03ba1 ,...,\u03bad :\n\nd\ni=1\n\n\u03bai =\u03ba\n\ni=1\n\ni=1\n\nHere, in the base case in which p is a leaf, define Optcost(p, \u03ba, t) = \u221e if there is some time\nt\u2032 \u2264 t such that \u03c1(t\u2032 ) = p and \u03ba(t\u2032 ) = 0. Otherwise, if \u03ba(t\u2032 ) \u2265 1, whenever \u03c1(t\u2032 ) = p, define\nOptcost(p, \u03ba, t) = 0.\n31\n\n\fP\nProof. The condition di=1 \u03bai = \u03ba ensures consistency between the number of servers in T (p) and\nits subtrees T (pi ). The term Optcost(pi , \u03bai , t) measures the cost of serving the requests within\nT (pi ) and g(\u03bai , t) measures the cost of servers leaving or entering subtree T (pi ).\nNext, we need the following key lemma that relates Optcost(p, \u03ba, t) to our procedure for generating hit costs at node p.\nLemma 22. Let p be a non-root node of T . Given a quota pattern \u03ba for T (p), let Optcost(p, \u03ba, t)\nbe as defined above. Then,\nOptcost(p, \u03ba, t) \u2212\n\nt\nX\n\n\u2032\n\nhtp (\u03ba(t\u2032 )) \u2264 2 *\n\nt\u2032 =1\n\n1\n* W (p) * g(\u03ba, t),\n\u03c3\u22121\n\n(44)\n\n\u2032\n\nwhere htp (j) = Optcost(p, j *1, t\u2032 )\u2212Optcost(p, j *1, t\u2032 \u22121) denotes the incremental cost of the optimal\nk-server solution for T (p) with exactly j servers.\nThis lemma (for the case of HSTs) is implicit in the work of Cot\u00e9 et al. [16]. For completeness,\nand since we need the extension to weighted \u03c3-HSTs, we give a proof of Lemma 22 in Appendix B.\nWe are now ready to prove the following theorem.\nTheorem 23. Let T be a weighted \u03c3-HST with \u03c3 > 9, depth l, and diameter \u2206. Let \u03c1 be a k-server\nrequest sequence on T , and \u03ba be the quota pattern. Consider the total movement cost incurred by\nthe convex combination of the allocation instances running on nodes of T (based on the algorithm\ndescribed in Section 4.1). This cost is no more than\n\u03b2l (Optcost(r, \u03ba, \u221e) + \u2206 * g(\u03ba, \u221e)),\nwhere \u03b2l satisfies the recurrence \u03b2l = \u03b3\u03b2l\u22121 + O (log(k/\u03b5)), and \u03b20 = 1. Here, \u01eb is any constant\nfor which the fractional allocation algorithm is (1 + \u03b5, O(log(k/\u03b5)))-competitive, and\n\u0012\n\u0013\n\u0013\n\u0012\n3\n1\n\u03b3 = (1 + \u03b5) 1 +\nlog(k/\u03b5) .\n+O\n\u03c3\n\u03c3\nWe first show how Theorem 23 implies Theorem 6. The recurrence \u03b2l \u2264 \u03b3\u03b2l\u22121 + O(log(k/\u03b5))\nin Theorem 23, together with \u03b20 = 1, implies that\n\u0012 l+1\n\u0013\n\u03b3\n\u22121\n\u03b2l = O(log(k/\u03b5))\n.\n\u03b3 \u22121\nChoosing \u03b5 = 1/(4l), and provided \u03c3 = \u03a9(\u03b5\u22121 log(k/\u03b5)) = \u0398(l log(kl)), we get that \u03b3 \u2264 (1 +\nand hence\n\u03b2l = O(l log(k/\u03b5)) = O(l log(kl)).\n\n1\n2l ),\n\nAs g(k * ~1, \u221e) = 0, this implies an O(l log(kl)) guarantee for a weighted \u03c3-HST of depth l, provided\n\u03c3 = \u03a9(l log(kl)). We now prove Theorem 23.\nProof. (Theorem 23): We prove by induction on the depth of the tree.\nBase case:\n\nthe theorem is clearly true for l = 0 (i.e. a single point space).\n32\n\n\fInductive step: suppose the theorem is true for weighted \u03c3-HSTs of depth l \u2212 1, and let T be\na weighted \u03c3-HST of depth l, rooted at r. Let wi be the distance to the i-th child of r, and let\nw = maxi wi . Given \u03ba, consider some optimal solution for T that achieves value Optcost(r, \u03ba, \u221e).\nWe also denote the total cost Optcost(r, \u03ba, \u221e) by Optcost(r, \u03ba), and g(\u03ba, \u221e) by g(\u03ba). Let \u03ba\u2217i\nbe optimal vectors for the children pi of r corresponding to this solution. Since \u03ba\u2217i determines\nOptcost(r, \u03ba), by (43) we have\nX\nOptcost(r, \u03ba) =\n(Optcost(pi , \u03ba\u2217i ) + wi * g(\u03ba\u2217i )) .\ni\n\nOptcost(pi , \u03ba\u2217i )\n\nBy (44), for each\ni,\nP child\n\u2032\n\u2032\nt\nHitcost(i, \u03ba ) = t hi (\u03ba (t)). Thus,\nOptcost(r, \u03ba) \u2265\n\nX\u0012\ni\n\n\u2265 Hitcost(i, \u03ba\u2217i ) \u2212 2wi * g(\u03ba\u2217i )/(\u03c3 \u2212 1), where we denote\n\n\u0012\nHitcost(i, \u03ba\u2217i ) + wi * 1 \u2212\n\n2\n\u03c3\u22121\n\n\u0013\n\n\u0013\ng(\u03ba\u2217i ) .\n\nMultiplying throughout by (\u03c3 \u2212 1)/(\u03c3 \u2212 3), which is at most 1 + 3/\u03c3 (as \u03c3 \u2265 9), implies\n\u0012\n\u0013\nX\n3\n\u03c3\u22121\n\u2217\n\u2217\n* Optcost(r, \u03ba) \u2264 1 +\n* Optcost(r, \u03ba).\n(Hitcost(i, \u03bai ) + wi * g(\u03bai )) \u2264\n\u03c3\u22123\n\u03c3\n\n(45)\n\ni\n\nConsider the fractional allocation instance A running at r in our algorithm and let {xti,j }i,j be its\nsolution at time step t. (Since there is only a single quota pattern \u03ba(r) at the root, we assume that\nthere is only one instance running, and keep it notationally convenient.) By Theorem 14 and by\n(45), the hit cost incurred by A satisfies\n!\nd\nd\nk\nd X\nX\nX\nXX\n\u2217\n\u2217\nt\nt\nwi * g(\u03bai ) + w * g(\u03ba)\nHitcost(i, \u03bai ) +\nxi,j hi (j) \u2264 (1 + \u03b5)\nt\n\ni=1 j=0\n\n\u0012\n\ni=1\n\n\u2264 (1 + \u03b5) 1 +\n\n\u0013\n\n3\n\u03c3\n\ni=1\n\nOptcost(r, \u03ba) + (1 + \u03b5)w * g(\u03ba),\n\nand the movement cost satisfies\nd\nXX\nt\n\ni=1\n\nwi\n\nX\n\nt\n|yi,j\n\n\u2212\n\nt\u22121\nyi,j\n|\n\n\u2264 O(log(k/\u03b5))\n\nd\nX\n\nHitcost(i, \u03ba\u2217i )\n\n+\n\nwi * g(\u03ba\u2217i )\n\ni=1\n\ni=1\n\nj\n\nd\nX\n\n\u2264 O(log(k/\u03b5))(Optcost(r, \u03ba) + w * g(\u03ba)).\n\n(46)\n!\n\n+ w * g(\u03ba)\n\n(47)\n\nNow, recall that each of the children p1 , . . . , pd of r is running a convex combination on allocation\ninstances \u039bti , the quota pattern of which is determined by {xti,j }i,j . So, the hit costs and movement\ncosts of A (i.e. left hand sides of (46) and (47)) can be expressed alternately as follows. Since\nthe quota patterns at pi maintain the invariant (40) throughout the algorithm, the total hit cost\naccumulated by A can be expressed as\nk\nd X\nXX\nt\n\nxti,j hti (j)\n\n=\n\nk\nd X\nXX\nt\n\ni=1 j=0\n\n=\n\n\u03bbts hti (j)\n\ni=1 j=0 s\u2208\u039bti ,\u03bats (t)=j\n\nd X\nX\n\ni=1 s\u2208\u039b\u221e\ni\n\n33\n\nX\n\n\u221e\n\u03bb\u221e\ns Hitcost(i, \u03bas ).\n\n(48)\n\n\f\u221e\nHenceforth, we use \u039bi and \u03bas to denote \u039b\u221e\ni and \u03bas . By our cost preserving procedure for updating\nt\u22121\n\u03bats when xi,j changes to xti,j , the movement cost incurred by A can be expressed as\nd\nXX\nt\n\nwi\n\nX\n\nt\u22121\nt\n|yi,j\n\u2212 yi,j\n|) =\n\nwi\n\n=\n\nd\nX\n\nwi\n\ni=1\n\nXX\nt\n\ni=1\n\nj\n\ni=1\n\nd\nX\n\n\u03bbts |\u03bats (t) \u2212 \u03bats (t \u2212 1)|\n\ns\u2208\u039bti\n\nX\n\n\u03bbs g(\u03bas ).\n\n(49)\n\ns\u2208\u039bi\n\nLet us now consider the overall movement cost incurred by the convex combination of the allocation\ninstances. This is equal to the movement cost for A (running at the root) plus the sum movement\ncosts incurred within p1 , . . . , pd . By the induction hypothesis, the movement cost for each of these\nrecursive algorithms that are run on subtrees T (pi ) with quota pattern \u03bas is at most\n\u0013\n\u0012\nwi\n* g(\u03bai ) .\n\u03b2l\u22121 Optcost(pi , \u03bas ) +\n\u03c3\u22121\nThus, the total recursive cost is at most\n\u0012\n\u0013\nd X\nX\nwi\n\u03bbs \u03b2l\u22121 Optcost(pi , \u03bas ) +\n* g(\u03bas )\n\u03c3\u22121\ni=1 s\u2208\u039bi\n\n\u2264\n\nd X\nX\n\n\u03bbs \u03b2l\u22121\n\ni=1 s\u2208\u039bi\n\n\u2264 (1 + \u03b5)\u03b2l\u22121\n\n\u0012\u0012\n\n\u0012\n\n\u0013\n3wi\nHitcost(i, \u03bas ) +\n* g(\u03bas )\n\u03c3\u22121\n\n3\n1+\n\u03c3\n\n\u0013\n\n(50)\n\n\u0013 X\nd X\n3wi\n\u03bbs \u03b2l\u22121\n* g(\u03bas ).\nOptcost(r, \u03ba) + w * g(\u03ba) +\n\u03c3\u22121\n\n(51)\n\ni=1 s\u2208\u039bi\n\nHere, (50) follows as Optcost(pi , \u03bas ) \u2264 Hitcost(i, \u03bas ) + 2wi /(\u03c3 \u2212 1) by (44), and (51) follows from\n(48) and (46). Now, the total cost of movement of servers across the subtrees p1 , . . . , pd is\nd\nX\ni=1\n\nwi\n\nX\n\n\u03bbs g(\u03bas ).\n\n(52)\n\ns\u2208\u039bi\n\nAdding up the costs from (51) and (52), the total cost incurred by the algorithm is at most\n(1 + \u03b5)\u03b2l\u22121\n\n\u0012\u0012\n\n3\n1+\n\u03c3\n\n\u0013\n\n\u0012\n\u0013 X\n\u0013\nd X\n3\u03b2l\u22121\n\u03bbs 1 +\nOptcost(r, \u03ba) + w * g(\u03ba) +\nwi * g(\u03bas ).\n\u03c3\u22121\n\n(53)\n\ni=1 s\u2208\u039bi\n\nBy (49) and (47) we have\nd\nX\ni=1\n\nwi\n\nX\n\n\u03bbs g(\u03bas ) \u2264 O(log(k/\u03b5)) (Optcost(r, \u03ba) + w * g(\u03ba)) .\n\ns\u2208\u039bi\n\nPlugging (54) into (53) implies that the total cost is at most\n\u0010\n\u0011\u0010\n\u0011\n\u03b2l\u22121 * \u03b3 + O (log(k/\u03b5)) Optcost(r, \u03ba) + w * g(\u03ba) ,\n34\n\n(54)\n\n\fwhere\n\n\u0013\n\u0012\n\u0013\n\u0012\nlog(k/\u03b5)\n3\n+O\n.\n\u03b3 = (1 + \u03b5) 1 +\n\u03c3\n\u03c3\n\nThus, the claimed result follows.\n\n5\n\nWeighted HSTs and Online Rounding\n\nIn this section, we show how one can embed a \u03c3-HST into a small depth weighted \u03c3-HST with\nconstant distortion, i.e., we prove Theorem 8. Also, we present an online rounding procedure for\nthe fractional k-server problem on an (un-weighted) \u03c3-HST, that is, we establish Theorem 7.\n\n5.1\n\nEmbedding \u03c3-HSTs into Weighted \u03c3-HSTs:\n\nTheorem 8. Let T be a \u03c3-HST with n leaves, but possibly arbitrary depth. Then, T can be\ntransformed into a weighted \u03c3-HST Te such that: the leaves of Te and T are identical, Te has depth\nO(log n), and any leaf to leaf distance in T is distorted in Te by a factor of at most 2\u03c3/(\u03c3 \u2212 1).\n\nProof. For a given rooted tree T \u2032 , we say that it is balanced if: (1) there is no child p of the root\nsuch that the subtree T \u2032 (p) (rooted at p) contains more than half of the nodes of T \u2032 , and (2) each\nsubtree T \u2032 (p\u2032 ), rooted at a child p\u2032 of the root, is balanced as well. It is easy to see that if a balanced\nT \u2032 has n leaves then its depth is O(log n).\nWe now present a procedure that contracts some of the edges of T and yields a weighted \u03c3-HST\nT \u2032 such that: (a) T \u2032 is balanced, and (b) for any leaf-to-leaf path in T , at least one (out of two) of\nthe longest edges on this path has not been contracted in T \u2032 .\nThe procedure works as follows. Let r be the root of T and p1 , . . . , pd be its children. We first\nrecursively transform each of the trees T (pi ) rooted at pi . Next, we check if there is a child pi such\nthat T (pi ) contains more than half of the nodes of T . (Note that there can be at most one such\nchild.) If not, then T (with modified T (p1 ), . . . , T (pd )) is the desired T \u2032 . Otherwise, T \u2032 is the tree\nT with the edge that connects pi to r contracted.\nIt is easy to see that the tree T \u2032 obtained by the above procedure is indeed balanced and also\nthe lengths of the edges on any root-to-leaf part decrease at rate of at least \u03c3. Thus, T \u2032 is a\nweighted \u03c3-HST and (a) holds. Now, to prove (b), let us inductively assume that it holds for all\nthe transformed subtrees T (pi ). We prove it for the transformed T . Consider a leaf-to-leaf path in\nT . If the path is contained entirely in one of the subtrees T (pi ), then we are done by our inductive\nassumption. Otherwise, the path contains two edges incident to the root r. As T is a \u03c3-HST, these\ntwo edges must be the longest ones on this path. Thus, as the procedure could contract only one\nof them, (b) follows as well.\nNow, clearly, taking Te to be T \u2032 satisfies the first two desired properties of Te, as stated in the\ntheorem. To prove that the last one holds too, we note that the length of any leaf-to-leaf path can\nonly decrease in Te (as we only contract edges). However, as we retain at least one of the longest\nedges, we have that the worst case distortion it incurs is at most:\nl\nl\nX\n1\n2\u03c3\n2 X j\n\u03c3 \u22642*\n*\n\u2264\n,\n\u03c3l\n\u03c3j\n\u03c3\u22121\nj=0\n\nj=0\n\nwhere \u03c3 l is the length of the longest edge on the path. The theorem thus follows.\n35\n\n\f5.2\n\nRounding the Fractional k-server Solution Online\n\nWe now show how to obtain an online randomized (integral) k-server algorithm from a fractional\nk-server algorithm, when the underlying metric corresponds to a \u03c3-HST T . The competitiveness of\nthe obtained algorithm will only be an O(1) factor worse than the competitiveness of the fractional\nalgorithm, provided \u03c3 > 5. The rounding procedure builds on ideas in [11] which were developed\nin the context of the finely competitive paging problem, and extends those ideas from a uniform\nmetric to HSTs.\nLet 1, . . . , n denote the leaves of the \u03c3-HST T . Recall that the fractional solution to the k-server\nproblem\nat each time t the probability xti of having a server at leaf i. The variables xti\nPspecifies\n8\nt\nsatisfy i xi = k. When these probabilities change at each time step t, the movement cost paid by\nthe fractional algorithm is equal to the earthmover distance between the distribution xt\u22121 and xt .\nIn contrast, an execution of a randomized algorithm can be viewed as an evolution of a distribution\nover k-tuples of leaves. (There is no point in having more than one server at a leaf.) To make this\nmore precise, let us define a configuration to be a subset C of {1, . . . , n} of size exactly k. The state\nS t at a given time t of a randomized k-server algorithm is specified by a probability distribution\n\u03bcS t on the configurations, where \u03bcS (C) denotes the probability mass of configuration C in state S.\nNow, we say that a state S is consistent with a fractional state x if,\nP\n(consistency) for each i \u2208 [n]\n(55)\nC:i\u2208C \u03bcS (C) = xi ,\ni.e., if the marginal probabilities of the state S coincide with x.\nWe therefore see that in order to round a fractional algorithm to a randomized algorithm, we\nneed to devise a way of maintaining (in an online manner) a sequence of states S 0 , S 1 , . . . that\nare always consistent with the corresponding states x0 , x1 , . . . of the fractional algorithm, and such\nthat the cost of the maintenance is within an O(1) factor of the movement cost of the fractional\nalgorithm. More precisely, our goal is to establish the following result.\n\nTheorem 24. Let T be a \u03c3-HST with n leaves, \u03c3 > 5, and let x0 , x1 , . . . be a sequence of states of a\nfractional k-server algorithm. There is an online procedure that maintains a sequence of randomized\nk-server states S 0 , S 1 , . . . with the following properties:\n1. At any time t, the state S t is consistent with the fractional state xt .\n2. If the fractional state changes from xt\u22121 to xt at time t, incurring a movement cost of ct ,\nthen the state S t\u22121 can be modified to a state S t while incurring a cost of O(ct ).\nThe key idea in bounding the maintenance cost of our rounding in Theorem 7 is to require that\nthe states S t that we produce are not only consistent with xt , but also each configuration in the\nsupport of S t does not deviate much from the fractional state xt . To this end, we introduce the\nfollowing additional property for k-server states.\nP\nFor a node p of T and a fractional state x, let xp = i\u2208T (p) xi be the fractional amount of servers\nthat x has on the leaves of the subtree T (p) rooted at p. Also, let np (C) = C \u2229 T (p) denote the\nnumber of servers in configuration C on leaves of T (p). We say that a configuration C is balanced\nwith respect to x if np (C) \u2208 {\u230axp \u230b, \u2308xp \u2309} for every node p. Now, we say that a k-server state S\nP\nNote that, in principle, the definition of a fractional solution allows us to have i xti to be strictly smaller than\n0\nk. However, as the starting configuration x has exactly k servers, it is easy to modify our fractional solution so that\nit always has exactly k servers, while not increasing its movement cost.\n8\n\n36\n\n\fis balanced with respect to x if every configuration in its support (i.e., with non-zero probability\nmass) is balanced with respect to x. That is, for all p and C for which \u03bcS (C) > 0,\n\u230axp \u230b \u2264 np (C) \u2264 \u2308xp \u2309.\n\n(56)\n\nNow, our approach to making the states we work with balanced is facilitated by the following\ndefinition. Let x be a fractional state and let S be some k-server state consistent with x (but S\nmight be not balanced with respect to x). We define the balance gap, G(S, x) of S, (with respect\nto x) to be:\nX\nX\nW (p)\n\u03bcS (C) min (|np (C) \u2212 \u230axp \u230b|, |np (C) \u2212 \u2308xp \u2309|) .\n(57)\nG(S, x) =\np\n\nC\u2208S\n\nHere, W (p) denotes the length of the edge from p to its parent. Clearly, when S is balanced with\nrespect to x, its balance gap is zero. Intuitively, the balance gap measures the distance of the state\nS from being balanced. This intuition is made concrete by the following lemma.\nLemma 25. Let x be a fractional state and let S be a k-server state on the leaves of a \u03c3-HST T ,\nwith \u03c3 > 5, which is consistent with x (but not necessarily balanced with respect to it). Then, S\ncan be converted to another state S \u2032 which is both consistent and balanced with respect to x, while\nincurring a cost of O(G(S, x)).\nWe will prove Lemma 25 later. First we show show how Theorem 24 follows from it.\nProof of Theorem 24. Consider a fractional state x that changes to some other fractional state x\u2032 ,\nand let S be a k-server state which is both consistent and balanced with respect to x. As S 0 (the\nstate initially at time t = 0) is consistent and balanced with respect to x0 , it is easy to see that\nto establish the theorem, it suffices to show that for any x, x\u2032 , and S as above, there is a k-server\nstate S \u2032 which is consistent and balanced with respect to x\u2032 , and such that the cost of changing the\nstate S to state S \u2032 is within O(1) factor of the cost of changing the state x to x\u2032 . Furthermore, it\nis enough to restrict oneself to the case in which x\u2032 is obtained from x by applying an elementary\nmove, i.e. xi , for some leaf i, is increased by \u03b4 and xi\u2032 , for some other leaf i\u2032 , is decreased by \u03b4,\nwhere \u03b4 can be chosen to be infinitesimally small.\nIn light of this, we can focus on proving the existence of such S \u2032 . To this end, let p denote the\nleast common ancestor of i and i\u2032 . Note that in this case the fractional cost of changing x to x\u2032 is\nat least 2\u03b4w(p), where w(p) = W (p)/\u03c3 is the length of the edges from p to its children.\nConsider now the following transformation of the state S. First, we add the leaf i to a probability\nmass of \u03b4 on arbitrarily chosen configurations in S that do not contain i already. Next, we remove i\u2032\nfrom some probability mass \u03b4 of configurations containing i\u2032 . (Note that the existence of sufficient\nmass of each type of configurations follows from the consistency property of S with x.) Let Se be\nthe resulting state.\nBefore proceeding, we note that as \u03b4 can be taken to be arbitrarily small, we can restrict our\ndiscussion to the case in which i is added to a mass of \u03b4 of a particular configuration C, and i\u2032 is\nremoved from a mass of \u03b4 of a particular configuration C \u2032 .\nNow, to continue the proof, we observe that Se is consistent with x\u2032 . However, the modified\nconfigurations C and C \u2032 that Se contains are not legal anymore as they do not consist of exactly k\nleaves. Also, Se might be unbalanced with respect to x\u2032 .\n37\n\n\fWe show how we can modify Se to fix these shortcomings. To this end, we note that as C contains\ni, and it was balanced with respect to x prior to adding i, it must hold now that np (C) \u2265 \u230axp \u230b + 1.\nSimilarly, for C \u2032 , it must hold that np (C \u2032 ) \u2264 \u230axp \u230b < \u230axp \u230b + 1 = np (C). Thus, by the pigeon hole\nprinciple, there must exist a leaf j of T (p) which is contained in C, but not in C \u2032 . Let us modify\nSe by removing j from C and adding it to C \u2032 . Clearly, this makes all the configurations in Se legal\nagain, keeps Se consistent with x\u2032 , and the total movement cost corresponding to this modification\n(due to deleting i, adding i\u2032 , and swapping j) is at most 4\u03b4w(p)\u03c3/(\u03c3 \u2212 1) = O(\u03b4w(p)), for \u03c3 > 5,\nwhich is within an O(1) factor of the cost of changing x to x\u2032 .\nUnfortunately, Se might still be unbalanced with respect to x\u2032 . To bound the imbalance, let us\nfirst consider the case in which \u230axq \u230b = \u230ax\u2032q \u230b and \u2308xq \u2309 = \u2308x\u2032q \u2309 for all nodes q. This implies that\nall the configurations in Se other than the modified configurations C and C \u2032 are already balanced\nwith respect to x\u2032 as they were balanced with respect to x. Now, we note that xq 6= x\u2032q only for\nnodes q that are on the path between i and i\u2032 (but excluding p). Similarly, nq (C) and nq (C \u2032 ) could\nchange only for nodes on the path from p to i, i\u2032 , or j (but, again, excluding p). Therefore, as both\nC and C \u2032 were initially balanced with respect to x, we can conclude that the total imbalance gap\ne x\u2032 ) of Se after our modifications is at most:\ne x) = G(S,\nG(S,\n\u0012\n\u0013\n1\n1\n3 * 2\u03b4w(p) 1 + + 2 + . . . = O(\u03b4w(p)).\n\u03c3 \u03c3\ne we obtain a state S \u2032 that is consistent and balanced with respect\nThus, by applying Lemma 25 to S,\nto x\u2032 and the cost of this procedure is O(\u03b4w(p)), as desired.\nNow, it remains to deal with the case in which either \u230axq \u230b =\n6 \u230ax\u2032q \u230b, or \u2308xq \u2309 6= \u2308x\u2032q \u2309, for some\nnodes q. To this end, we note that by taking \u03b4 to be small enough (but non-zero), we can ensure\nthat for each q for which at least one of these two inequalities holds, it must be the case that\neither xq or x\u2032q is an integer. In the former case, we have that for all the configurations C \u2032\u2032 in S\nthat have non-zero mass, nq (C \u2032\u2032 ) = xq = \u230axq \u230b = \u2308xq \u2309 and thus \u230ax\u2032q \u230b \u2264 nq (C \u2032\u2032 ) \u2264 \u2308x\u2032q \u2309. In the\nlatter case, as |xq \u2212 x\u2032q | \u2264 \u03b4 and for every relevant configuration C \u2032\u2032 in S, \u230axq \u230b \u2264 nq (C \u2032\u2032 ) \u2264 \u2308xq \u2309,\nthe total probability mass of configurations C \u2032\u2032 in S, such that nq (C \u2032\u2032 ) = \u230axq \u230b < \u230ax\u2032q \u230b = x\u2032q or\nnq (C \u2032\u2032 ) = \u2308xq \u2309 > \u2308x\u2032q \u2309 = x\u2032q , can be at most \u03b4.\nAs a result, we see that the total probability mass of configurations in Se that are not balanced\nwith respect to x\u2032 is at most 3\u03b4 (the contribution of 2\u03b4 comes from the modified configurations C\nand C \u2032 ). Thus, by calculating the imbalance gap similarly to what we did before, we can show that\ne x\u2032 ) is O(\u03b4w(p)), and once again use Lemma 25 to obtain the desired S \u2032 . This concludes the\nG(S,\nproof of the theorem.\nIt remains to prove Lemma 25.\n\nProof of Lemma 25. Let us call a node p in our \u03c3-HST T imbalanced if\nX\n\u03bcS (C) min(|np (C) \u2212 \u230axp \u230b|, |np (C) \u2212 \u2308xp \u2309|) > 0.\nC\u2208S\n\nIf no node is imbalanced, then clearly G(S, x) = 0, and we are already done, so we assume that\nthis is not the case. Let p be an imbalanced nodes which is at the highest level of T (breaking ties\narbitrarily). We note that p cannot be the root r of T , as each configuration has exactly k servers,\nand xr = k.\n38\n\n\fConsider now a configuration C for which \u03bcS (C) > 0 and np (C) \u2208\n/ {\u230axp \u230b, \u2308xp \u2309} and let us\nassume\nthat np (C) < \u230axp \u230b (the other case can be treated similarly). As S is consistent with x,\nP\n\u2032\u2032 )n (C \u2032\u2032 ) = x , and so there must be some other configuration C \u2032 with \u03bc (C \u2032 ) > 0 such\n\u03bc\n(C\n\u2032\u2032\nS\np\np\nS\nC\n\u2032\nthat np (C ) \u2265 \u230axp \u230b + 1. So, in particular, we have np (C \u2032 ) \u2212 np (C) \u2265 2.\nNow, let pe denote the parent of p in T (recall that p is not the root). As we choose p to be an\nimbalanced node at the highest possible level, pe must be balanced, and hence |npe(C \u2032 ) \u2212 npe(C)| \u2264 1.\nBut, since np (C \u2032 ) \u2212 np (C) \u2265 2, it implies the existence of some other child p\u2032 , p\u2032 6= p, of pe such that\nnp\u2032 (C \u2032 ) < np\u2032 (C).\nTherefore, by the pigeon hole principle, there must exist a leaf i in the subtree T (p) rooted at\np which is contained in C \u2032 , but not in C. Similarly, C must contain a leaf i\u2032 in T (p\u2032 ) which is not\ncontained in C \u2032 . Let \u03b4 = min(\u03bcS (C), \u03bcS (C \u2032 )) (note that \u03b4 > 0). Consider a modification of S in\nwhich we take any arbitrary probability mass \u03b4 of configurations C and replace i\u2032 by i in them.\nNext, we take any arbitrary probability mass \u03b4 of configurations C \u2032 and replace i by i\u2032 in them.\nLet us summarize the properties satisfied by S after this modification. First, the state remains\nconsistent with the fractional solution x, because the marginals of the leaves i and i\u2032 have not\nchanged. Second, since neither npe(C) nor npe(C \u2032 ) have changed, pe remains balanced. Moreover, the\nonly nodes for which the imbalance could have changed are on the path from i to p and i\u2032 to p\u2032 .\nThird, replacing i\u2032 with i (in a \u03b4 measure of C) increases np (C) by 1 for these configurations, and\nreplacing i with i\u2032 , leaves the quantity np (C \u2032 ) to be of value at least \u230axp \u230b. Together, this implies\nthat the imbalance\nby at least \u03b4. Finally, as np\u2032 (C) > np\u2032 (C \u2032 ) before the modification,\nP of p decreases\n\u2032\u2032\nthe imbalance C \u2032\u2032 \u2208S \u03bcS (C ) min(|np\u2032 (C \u2032\u2032 ) \u2212 \u230axp\u2032 \u230b|, |np\u2032 (C \u2032\u2032 ) \u2212 \u2308xp\u2032 \u2309|) of p\u2032 can only decrease.\nNow, if we analyze the change in the imbalance gap of S, in the worst case, the imbalance of\nevery node from p to i (excluding p) could have increased by 2\u03b4 due to the addition of i in C or\nremoval of i from C \u2032 . Similarly, the imbalance of every node from p\u2032 to i\u2032 (excluding p\u2032 ) could have\nincreased by up to 2\u03b4. Together with the above observations, this implies that the imbalance gap\nof S decreases by at least\n\u0013\n\u0012\n\u0013\n\u0012\n\u03c3\u22125\n1\n1\n= \u03a9(W (p)\u03b4),\nW (p)\u03b4 \u2212 4\u03b4w(p) 1 + + 2 + . . . = W (p)\u03b4\n\u03c3 \u03c3\n\u03c3\u22121\nwhere the last inequality uses the fact that \u03c3 > 5.\nOn the other hand, as both i and i\u2032 lie in T (e\np), the movement cost incurred in the above\n\u03c3\nprocedure is at most 4\u03b4w(e\np)(\u03c3/(\u03c3\u22121)) = 4\u03b4W (p) (\u03c3\u22121) , which is within O(1) factor of the reduction\nin the imbalance gap. The lemma follows by applying the above steps repeatedly until the imbalance\ngap reaches zero.\n\nReferences\n[1] Dimitris Achlioptas, Marek Chrobak, and John Noga. Competitive analysis of randomized\npaging algorithms. Theoretical Computer Science, 234(1-2):203\u2013218, 2000.\n[2] Nikhil Bansal, Niv Buchbinder, and Joseph (Seffi) Naor. A primal-dual randomized algorithm for weighted paging. In FOCS'07: Proceedings of the 48th Annual IEEE Symposium on\nFoundations of Computer Science, pages 507\u2013517, 2007.\n\n39\n\n\f[3] Nikhil Bansal, Niv Buchbinder, and Joseph (Seffi) Naor. Towards the randomized k-server\nconjecture: A primal-dual approach. In SODA'10: Proceedings of the 21st Annual ACMSIAM Symposium on Discrete Algorithms, 2010.\n[4] Nikhil Bansal, Niv Buchbinder, and Joseph (Seffi) Naor. Unfair metrical task systems on\nhsts and applications. In ICALP'10: Proceedings of the 37th International Colloquium on\nAutomata, Languages and Programming, 2010.\n[5] Yair Bartal. Probabilistic approximations of metric spaces and its algorithmic applications.\nIn FOCS'96: Proceedings of the 37th Annual IEEE Symposium on Foundations of Computer\nScience, pages 184\u2013193, 1996.\n[6] Yair Bartal. On approximating arbitrary metrices by tree metrics. In STOC'98: Proceedings\nof the 30th Annual ACM Symposium on Theory of Computing, pages 161\u2013168, 1998.\n[7] Yair Bartal, Avrim Blum, Carl Burch, and Andrew Tomkins. A polylog(n)-competitive algorithm for metrical task systems. In STOC'97: Proceedings of the 29th Annual ACM Symposium\non Theory of Computing, pages 711\u2013719, 1997.\n[8] Yair Bartal, B\u00e9la Bollob\u00e1s, and Manor Mendel. A ramsy-type theorem for metric spaces and\nits applications for metrical task systems and related problems. In FOCS'01: Proceedings of\nthe 42nd Annual IEEE Symposium on Foundations of Computer Science, pages 396\u2013405, 2001.\n[9] Yair Bartal and Eddie Grove. The harmonic k-server algorithm is competitive. Journal of the\nACM, 47(1):1\u201315, 2000.\n[10] Yair Bartal, Nathan Linial, Manor Mendel, and Assaf Naor. On metric ramsey-type phenomena. In STOC'03: Proceedings of the 35th Annual ACM Symposium on Theory of Computing,\npages 463\u2013472, 2003.\n[11] Avrim Blum, Carl Burch, and Adam Kalai. Finely-competitive paging. In FOCS'99: Proceedings of the 40th Annual Symposium on Foundations of Computer Science, page 450, 1999.\n[12] Avrim Blum, Howard J. Karloff, Yuval Rabani, and Michael E. Saks. A decomposition theorem\nand bounds for randomized server problems. In FOCS'92: Proceedings of the 31st Annual IEEE\nSymposium on Foundations of Computer Science, pages 197\u2013207, 1992.\n[13] Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis. Cambridge\nUniversity Press, 1998.\n[14] M. Chrobak, H. Karloff, T. Payne, and S. Vishwanathan. New results on server problems.\nSIAM Journal on Discrete Mathematics, 4(2):172\u2013181, 1991.\n[15] M. Chrobak and L. Larmore. An optimal on-line algorithm for k-servers on trees. SIAM\nJournal on Computing, 20(1):144\u2013148, 1991.\n[16] A. Cot\u00e9, A. Meyerson, and L. Poplawski. Randomized an optimal on-line algorithm for k-server\non hierarchical binary trees. In STOC'08: Proceedings of the 40th Annual ACM Symposium\non Theory of Computing, pages 227\u2013234, 2008.\n\n40\n\n\f[17] B. Csaba and S. Lodha. A randomized on-line algorithm for the k-server problem on a line.\nRandom Structures and Algorithms, 29(1):82\u2013104, 2006.\n[18] Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree metrics. In STOC'03: Proceedings of the 35th Annual ACM Symposium\non Theory of Computing, pages 448\u2013455, 2003.\n[19] A. Fiat, Y. Rabani, and Y. Ravid. Competitive k-server algorithms. Journal of Computer and\nSystem Sciences, 48(3):410\u2013428, 1994.\n[20] Amos Fiat, Richard M. Karp, Michael Luby, Lyle A. McGeoch, Daniel Dominic Sleator, and\nNeal E. Young. Competitive paging algorithms. Journal of Algorithms, 12(4):685\u2013699, 1991.\n[21] Amos Fiat and Manor Mendel. Better algorithms for unfair metrical task systems and applications. SIAM Journal on Computing, 32(6):1403\u20131422, 2003.\n[22] Edward F. Grove. The harmonic online k-server algorithm is competitive. In STOC'91:\nProceedings of the 23rd Annual ACM Symposium on Theory of Computing, pages 260\u2013266,\n1991.\n[23] Elias Koutsoupias and Christos H. Papadimitriou. On the k-server conjecture. Journal of the\nACM, 42(5):971\u2013983, 1995.\n[24] M. Manasse, L.A. McGeoch, and D. Sleator. Competitive algorithms for server problems.\nJournal of Algorithms, 11:208\u2013230, 1990.\n[25] Lyle A. McGeoch and Daniel D. Sleator. A strongly competitive randomized paging algorithm.\nAlgorithmica, 6(6):816\u2013825, 1991.\n[26] Steven S. Seiden. A general decomposition theorem for the k-server problem. In ESA'01:\nProceedings of the 9th Annual European Symposium on Algorithms, pages 86\u201397, 2001.\n[27] Daniel D. Sleator and Robert E. Tarjan. Amortized efficiency of list update and paging rules.\nCommunications of the ACM, 28(2):202\u2013208, 1985.\n\nA\n\nProof of Lemma 11\n\nConsider first the simpler case of the fix stage. Here the variables evolve according to\n\u03c4\ndyi,j\nd\u03c4\n\n=\n\n(\n\n1\nwi\n\n0\n\n\u0010\n\u0011\n\u03c4 +\u03b2\n\u03c4 <1\nyi,j\nyi,j\n\n\u03c4 = 1.\nyi,j\n\n\u03c4 only depends on y \u03c4 and is continuous, the function y \u03c4 is well defined. As\nAs the derivative of yi,j\ni,j\ni,j\n\u03c4 = 1, and non-negative otherwise, it ensures that y \u03c4 always stays in\nthe derivative is 0 when yi,j\ni,j\nthe range [0, 1]. Finally, the monotonicity property holds here, as it holds initially when \u03c4 = 0, and\n\u03c4 /d\u03c4 \u2264 dy \u03c4 /d\u03c4 , unless y \u03c4 = 1, in which\nwhenever yi,j \u2264 yi,j \u2032 for some j < j \u2032 , we have that dyi,j\ni,j \u2032\ni,j \u2032\ncase monotonicity holds trivially.\n\n41\n\n\f\u03b7\nWe now consider the hit stage. Recall that configuration yi,j\n, for each (i, j), evolves according\nto Equation (13), which we reproduce here for convenience:\n\u03b7\ndyi,j\n\nd\u03b7\n\n=\n\n(\n\n0 \u0010\n/ A\u03b7 ,\n\u0011 if (i, j) \u2208\n\u0011 \u0010\n\u03b7\n\u03b7\n1\notherwise.\nwi yi,j + \u03b2 * N (\u03b7) \u2212 \u03b1\u03bbi,j\n\n(58)\n\nIn the above, the set A\u03b7 denotes the active coordinates at time \u03b7 (cf. Definition 9) and the\nnormalization factor N (\u03b7) can be expressed as (cf. (14))\n\uf8f1\nP\n\u03b7\nif\n\uf8f2 0P\ni,j yi,j > kd \u2212 \u03ba(t),\n\u03b7\n\u03b7\n1\nP\n(59)\nN (\u03b7) =\n(i,j)\u2208A\u03b7 wi (yi,j +\u03b2 )*\u03b1\u03bbi,j\n\u03b7\nP\n\uf8f3\ny\n=\nkd\n\u2212\n\u03ba(t)).\notherwise\n(i.e.\nif\n\u03b7\n1\ni,j\ni,j\n(i,j)\u2208A\u03b7 wi (yi,j +\u03b2 )\n\nFirst, we show that during the hit stage blocks never split. To this end, we note that when two\nblocks merge, their y-values are identical, and since we also modify \u03bb\u03b7 to be identical for these\nblocks, all the variables contained in the merged block evolve in the same way from that point on,\nas desired. (Note that we do not assume here that the trajectory y \u03b7 is well defined and unique, we\njust argue that any trajectory compatible with our definition of derivatives cannot split blocks.)\nNow, we proceed to analyzing the properties of the evolution described by Equations (58) and\n(59). As a first step, let us prove the following claim that will be helpful later.\n\u2032\n\nClaim 26. Consider a feasible configuration y \u03b7 , for \u03b7 \u2032 \u2208 [0, 1], a subset of coordinates A, and a\nhit cost vector \u03bb. Define for any \u03b7 \u2265 \u03b7 \u2032 ,\nP\n\u03b7\n(i,j)\u2208A ui,j * \u03b1\u03bbi,j\nP\nNA (\u03b7) =\n,\n\u03b7\n(i,j)\u2208A ui,j\n\u0010\n\u0011\n\u2032\n\u03b7\n+ \u03b2 . Now, if we make the configuration y \u03b7 evolve according to\nwhere u\u03b7i,j = w1i yi,j\n(\n\u03b7\n0 \u0010\nif (i, j) \u2208\n/ A,\ndyi,j\n\u0011\n=\n,\n\u03b7\n1\nd\u03b7\nwi yi,j + \u03b2 * (NA (\u03b7) \u2212 \u03b1\u03bbi,j ) otherwise\nthen we have that NA (\u03b7) does not increase, i.e., NA (\u03b7) \u2264 NA (\u03b7 \u2032 ) for any \u03b7 \u2265 \u03b7 \u2032 .\n\nProof. Let us fix a \u03b7 \u2265 \u03b7 \u2032 and denote \u03b7 + = \u03b7 + d\u03b7. We will prove that NA (\u03b7 + ) \u2264 NA (\u03b7), which, in\nturn, implies our claim. To this end, note that one can view NA (\u03b7) as a weighted average, over all\ncoordinates in A, of the value of \u03b1\u03bbi,j , where u\u03b7i,j is the weight which we attribute to coordinate\n(i, j) at time \u03b7.\n\u03b7+\n\u03b7\nNow, the key observation is that the way the yi,j s evolve implies that yi,j\n\u2265 yi,j\nif \u03b1\u03bbi,j \u2264 NA (\u03b7),\n+\n\n\u03b7\n\u03b7\n\u03b7\nand yi,j\n\u2264 yi,j\nif \u03b1\u03bbi,j \u2265 NA (\u03b7). So, as the weights u\u03b7i,j are directly proportional to yi,j\n, we can\nconclude that during our evolution, the weights of coordinates that have a value of \u03b1\u03bbi,j above the\n+\naverage value NA (\u03b7) (i.e. u\u03b7i,j \u2264 u\u03b7i,j in this case) can only decrease, and the weights of coordinates\n+\n\nthat have a value of \u03b1\u03bbi,j which is at most the average (i.e., we have u\u03b7i,j \u2265 u\u03b7i,j for such (i, j)) can\nonly increase. As a result, we can express NA (\u03b7 + ) as\nP\nP\nP\n\u03b7\n\u03b7+\n(i,j)\u2208A ui,j * \u03b1\u03bbi,j +\n(i,j)\u2208A \u03b1\u03bbi,j \u2206i,j\n(i,j)\u2208A ui,j * \u03b1\u03bbi,j\n+\nP\nP\n=\n,\n(60)\nNA (\u03b7 ) =\nP\n\u03b7\n+\n\u03b7\n(i,j)\u2208A ui,j +\n(i,j)\u2208A \u2206i,j\n(i,j)\u2208A ui,j\n42\n\n\f+\n\nwhere \u2206i,j = u\u03b7i,j \u2212 u\u03b7i,j and, by our discussion above, we have that \u2206i,j \u2265 0 if \u03b1\u03bbi,j \u2264 NA (\u03b7), and\n\u2206i,j \u2264 0 otherwise.\nNow, if the right hand side of (60) is at most NA (\u03b7), then we are done. Otherwise, we must\nhave that\nP\n\u03b7\n(i,j)\u2208A ui,j * \u03b1\u03bbi,j\nP\n> NA (\u03b7),\n(61)\n\u03b7\n(i,j)\u2208A ui,j\n\na\nas it is easy to check that if a+d*t\nb+t > c, then also b > c, as long as t and d are such that t \u2265 0 if\nd \u2264 c, and t \u2264 0 otherwise.\nBut, the left hand side of (61) is by definition equal to NA (\u03b7). So, the obtained contradiction\nimplies that indeed NA (\u03b7 + ) \u2264 NA (\u03b7), and the claim follows.\n\nNow, observe that the set A\u03b7 , as well as N (\u03b7), depend only on the state y \u03b7 and the hit cost\nvector \u03bb\u03b7 . As a result, both A\u03b7 and N (\u03b7) can, in principle, vary drastically between points of time.\nHowever, as we show in the following claim, \"locally\" they tend to behave in a regular manner.\n\u2032\n\nClaim 27. For any \u03b7 \u2032 \u2208 [0, 1) and feasible configuration y \u03b7 , there exists an \u03b7 \u2032\u2032 , \u03b7 \u2032 < \u03b7 \u2032\u2032 \u2264 1, such\nthat:\n\u2032\n\n(a) A\u03b7 = A\u03b7 for each \u03b7 \u2208 [\u03b7 \u2032 , \u03b7 \u2032\u2032 ), i.e., the set A\u03b7 of active coordinates does not change for \u03b7 < \u03b7 \u2032\u2032 ;\n\u2032\n\n(b) \u03bb\u03b7 = \u03bb\u03b7 for any \u03b7 \u2208 [\u03b7 \u2032 , \u03b7 \u2032\u2032 ), i.e., there are no block merges until time \u03b7 \u2032\u2032 ;\nP\n\u03b7\n(c) the sum i,j yi,j\nis bigger than kd \u2212 \u03ba(t) for all \u03b7 < \u03b7 \u2032\u2032 , unless it was already equal to kd \u2212 \u03ba(t)\n\u2032\nat time \u03b7 ;\n\u2032\u2032\n\n(d) the configuration y \u03b7 is well defined and feasible for \u03b7 \u2208 [\u03b7 \u2032 , \u03b7 \u2032\u2032 ] (note that this interval contains\n\u03b7 \u2032\u2032 ).\nFurthermore, one can assume \u03b7 \u2032\u2032 is maximal, that is, unless \u03b7 \u2032\u2032 is already equal to 1, there exists\nno larger value of \u03b7 \u2032\u2032 for which the above holds.\n\u2032\n\nFor future reference, we call \u03b7 \u2032\u2032 , defined as in the above claim, the horizon of y\u03b7 .\n\u03b7\nProof of Claim 27. Let us define a new process for the evolution of yi,j\nin a suitably small neigh\u2032\nborhood of \u03b7 as follows:\n\n\u03b7\ndyi,j\n\nd\u03b7\n\n= =\n\n(\n\n1\nwi\n\n0\n\n\u0011\n\u0010\n\u0011 \u0010\n\u2032\n\u2032\n\u03b7\nif (i, j) \u2208 A\u03b7\nyi,j\n+ \u03b2 * \u00d1 (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j\notherwise\n\n(62)\n\nwhere\n\u00d1 (\u03b7) =\n\n\uf8f1\n\uf8f4\n\uf8f2 0\n\uf8f4\n\uf8f3\n\nP\n\n1\n\u2032\n(i,j)\u2208A\u03b7 wi\n\nP\n\n\u2032\n(i,j)\u2208A\u03b7\n\n\u2032\n\n\u03b7\n+\u03b2 )\u03b1\u03bb\u03b7i,j\n(yi,j\n1\ny \u03b7 +\u03b2 )\nwi ( i,j\n\nP\n\n\u2032\n\n\u03b7\nyi,j\n> kd \u2212 \u03ba(t),\nP\n\u03b7\u2032\notherwise (i.e. if\ni,j yi,j = kd \u2212 \u03ba(t)).\n\nif\n\ni,j\n\n(63)\n\nWhile the new process looks similar to the original one, there are some crucial differences. First, we\n\u03b7\neither exceeds 1 or becomes negative, i.e., the active set is not updated\ndo not care if a variable yi,j\n\u2032\n\u2032\nas \u03b7 progresses and remains the set A\u03b7 . Second, the cost vector is not updated and it remains \u03bb\u03b7 ,\n43\n\n\fi.e., blocks are not merged and the monotonicity requirement (4) are ignored. Third, the value of\nP\nP\n\u03b7\n\u03b7\u2032\n\u00d1 (\u03b7) is not changed once the value of i,j yi,j\nhits the quota. If i,j yi,j\n> kd \u2212 \u03ba(t) at time \u03b7 \u2032 ,\n\u00d1 (\u03b7) will be always 0. As a result, the latter sum might become less than kd \u2212 \u03ba(t) at some point.\nNow, the key observation that makes this new process useful to us is that it is identical to our\noriginal process, as long as the evolving configuration is still feasible, and no coordinate becomes\nactive and changes in the original process. More precisely, the trajectories of the two processes\ncoincide as long as in the configuration y \u03b7 evolving with respect to this new process:\n\u03b7\n(1) yi,j\nis in [0, 1] for each (i, j) \u2208 A\u03b7 . (Otherwise, a coordinate (i, j) violating this condition would\nhave already become inactive in the original process.), or\n\u2032\n\n\u2032\n\n\u2032\n\n\u03b7\n= 1 (respectively\n/ A\u03b7 with yi,j\n(2) \u00d1 (\u03b7)\u2212\u03b1\u03bb\u03b7i,j is non-negative (respectively non-positive) for (i, j) \u2208\n\u2032\n\n\u2032\n\n\u03b7\nyi,j\n= 0). (Otherwise, a coordinate (i, j) violating this condition would have already become\nactive and would have changed in the original process.), or\n\u03b7\n\u03b7\n(3) the monotonicity condition holds, i.e., yi,j\n\u2264 yi,j+1\nfor each (i, j). (Otherwise, a block merge\nwould have already happened in the original process.), or\nP\n\u03b7\n\u2265 kd \u2212 \u03ba(t). (Otherwise, N (\u03b7) would have already\n(4) the server quota is obeyed, i.e., i,j yi,j\nchanged so as to guarantee that the number of servers stays at the quota.)\n\nLet \u03b7 l for l \u2208 {1, 2, 3, 4} be the last \u03b7 \u2265 \u03b7 \u2032 for which the l-th condition is satisfied in the interval\n\u03b7\n\u03b7\n= 0 (respectively, yi,j\n= 1)\n[\u03b7 \u2032 , \u03b7]. Also, let \u03b7 0 (respectively \u03b7 1 ) be the first time \u03b7 \u2265 \u03b7 \u2032 in which yi,j\n\u2032\n\n/ A\u03b7 ), and \u00d1 (\u03b7) \u2212 \u03b1\u03bb\u03b7i,j = 0. As in the new process \u00d1 (\u03b7) \u2013 and\nfor (i, j) \u2208 A\u03b7 (respectively (i, j) \u2208\nthus all the derivatives and the trajectory of y \u03b7 \u2013 are continuous and bounded in the our interval\nof interest9 , such maximal \u03b7 l s and \u03b7 0 , \u03b7 1 always exists. Let us take \u03b7 \u2032\u2032 = min{minl \u03b7 l , \u03b7 0 , \u03b7 1 , 1}.\nNote that this implies that y \u03b7 is well defined and feasible for any \u03b7 \u2208 [\u03b7 \u2032 , \u03b7 \u2032\u2032 ] \u2013 so, condition (d)\nis satisfied. Furthermore, it is not hard to verify that all of the conditions (a)-(c) hold for such\n\u03b7 \u2032\u2032 , and that this \u03b7 \u2032\u2032 \u2264 1 is indeed maximal with respect to satisfying these conditions. So, if we\nmanage to prove that also \u03b7 \u2032\u2032 > \u03b7 \u2032 , our claim will follow.\n\u2032\n\u2032\nTo this end, observe that by applying Claim 26 with A = A\u03b7 and \u03bb = \u03bb\u03b7 , we see that \u00d1 (\u03b7)\ncan only decrease for \u03b7 \u2265 \u03b7 \u2032 . Therefore, all the derivatives (62) in this process can only decrease\ntoo. This implies, in particular, that if some variable stops increasing (i.e., its derivative becomes\nnon-positive) at some point, it will never increase again.\n\u2032\nNow, note that if (i, j) \u2208 A\u03b7 then\n\u2032\n\n\u2032\n\n\u2032\n\n\u03b7\n\u03b7\n\u2022 if yi,j\n\u2208 (0, 1), then, as the derivatives are bounded, it must be the case that yi,j\n\u2208 (0, 1) until\n\u2032\nsome time \u03b7 > \u03b7 ;\n\u2032\n\n\u2032\n\n\u03b7\n\u2022 if yi,j\n= 0, then its derivative \u00d1 (\u03b7 \u2032 ) \u2212 \u03b1\u03bb\u03b7i,j at time \u03b7 \u2032 has to be strictly positive (otherwise,\n\u03b7\n(i, j) would be inactive at time \u03b7 \u2032 ) and thus yi,j\n(respectively its derivative) has to stay\nnon-negative (respectively positive) until some time \u03b7 > \u03b7 \u2032 too;\n\u03b7\nMore precisely, \u00d1 (\u03b7) is continuous and bounded, as long as each yi,j\nis bounded away from \u2212\u03b2. However, we are\ninterested in only analyzing the process as long as all the variables stay non-negative, so for the sake of our analysis,\n\u00d1 (\u03b7) is indeed continuous and bounded.\n9\n\n44\n\n\f\u2032\n\n\u2032\n\n\u03b7\n\u2022 finally, if yi,j\n= 1, then its derivative \u00d1 (\u03b7 \u2032 ) \u2212 \u03b1\u03bb\u03b7i,j at time \u03b7 \u2032 has to be non-positive (as oth\u2032\nerwise, (i, j) \u2208\n/ A\u03b7 ). However, by our discussion above, it means that neither this derivative,\n\u03b7\nnor yi,j , will ever increase again.\n\nThus, we can infer from the above that both \u03b7 1 and \u03b7 0 are strictly larger than \u03b7 \u2032 .\n\u2032\nNext, let us focus on some (i, j) \u2208\n/ A\u03b7 . We have that\n\u2032\n\n\u2032\n\n\u03b7\n\u2022 if yi,j\n= 0, then \u00d1 (\u03b7 \u2032 ) \u2212 \u03b1\u03bb\u03b7i,j has to be non-positive (otherwise, (i, j) would be active at time\n\u2032\n\n\u03b7 \u2032 ) and as \u00d1 (\u03b7) never increases, \u00d1 (\u03b7 \u2032 ) \u2212 \u03b1\u03bb\u03b7i,j will never become positive;\n\u2032\n\n\u2032\n\n\u03b7\n= 1, then \u00d1 (\u03b7 \u2032 ) \u2212 \u03b1\u03bb\u03b7i,j at time \u03b7 \u2032 has to be strictly positive (as otherwise, (i, j) \u2208 A\u03b7 ).\n\u2022 if yi,j\nSo, this quantity has to remain positive for some time \u03b7 > \u03b7 \u2032 .\n\u2032\n\nThus, we see that both \u03b7 2 and \u03b7 1 are also strictly larger than \u03b7 \u2032 .\n\u2032\n\u2032\n\u2032\n\u03b7\u2032\n\u03b7\u2032\nObserve now that if yi,j\n= yi,j+1\nfor some (i, j), (i, j + 1) \u2208 A\u03b7 , we must have that \u03bb\u03b7i,j \u2265 \u03bb\u03b7i,j+1 .\nOtherwise, the blocks to which (i, j) and (i, j + 1) belong would have been merged at time \u03b7 \u2032 .\n\u03b7\n\u03b7\n.\nis always bounded from above by the derivative of yi,j+1\nThis means that the derivative of yi,j\nThus, such a pair of coordinates will never violate the monotonicity property. On the other hand,\n\u03b7\n\u03b7\nif yi,j\n< yi,j+1\n, then, as the derivatives are bounded, there always exists \u03b7 > \u03b7 \u2032 for which this strict\ninequality still holds (and thus monotonicity is not violated). Hence, we get that \u03b7 3 > \u03b7 \u2032 .\nP\n\u03b7\u2032\nFinally, to see that \u03b7 4 > \u03b7 \u2032 as well, we note that if i,j yi,j\n= kd \u2212 \u03ba(t), then, by design, it will\nremain equal henceforth. If, however, the latter sum is larger than kd \u2212 \u03ba(t) at time \u03b7 \u2032 , then it\nwould still remain so for some \u03b7 > \u03b7 \u2032 .\nThus, indeed we have \u03b7 \u2032\u2032 = min{minl \u03b7 l , \u03b7 0 , \u03b7 1 , 1} > \u03b7 \u2032 , concluding the proof of the claim.\nIn light of the above claim, one can consider obtaining a feasible configuration y 1 from the\nstarting (feasible) configuration y0 by simply gluing together the trajectories corresponding to the\nhorizons. More precisely, one could start with \u03b70 = 0, and for each \u03b7s , with s \u2265 0 and \u03b7s < 1, define\n\u03b7s+1 to be the horizon of y\u03b7s . Note that, as we start with the feasible configuration y 0 , Claim 27\nimplies that all y \u03b7s are well defined and feasible too.\nNow, the only reason why the above approach might not end up giving us the desired feasible\nconfiguration y 1 is that, a priori, it is not clear whether the sequence {\u03b7s }s ever reaches 1. That\nis, even though we know that \u03b70 = 0 and \u03b7s+1 > \u03b7s , it might still be possible that this sequence\nconverges without ever reaching 1, and thus there is no s with \u03b7s = 1.\nIn order to rule out this possibility, we will prove that the total number of horizons is always\nfinite. Observe that each horizon can be associated with at least one of the following events: (a)\nthe set of active coordinates changes, or (b) a block merge occurs, or (c) the number of servers hits\nthe quota. Thus, it suffices to show that the total number\nevents is bounded.\nP of such\n\u03b7\nTo this end, let\nthat in our evolution, once i,j yi,j becomes equal to kd \u2212 \u03ba(t), N (\u03b7)\nP us note\n\u03b7\nis chosen so that i,j dyi,j\n/d\u03b7 = 0. So, once we hit the quota we stay there throughout the rest of\nthe hit stage. Hence, there can be at most one event of type (c). Also, as we have already argued,\nduring our evolution blocks never split once they are formed, and thus the total number of block\nmerges (i.e., events of type (b)) can be at most k.\nIt remains to bound the number of events of type (a), i.e., the ones corresponding to variables becoming active/inactive. For notational convenience, let us say that a coordinate (i, j)\n0-inactivates (respectively, 1-inactivates) at time \u03b7s , for some s \u2265 1, if N (\u03b7s ) \u2264 \u03b1\u03bb\u03b7i,js (respectively,\n45\n\n\f\u03b7s\n\u03b7s\nN (\u03b7s ) > \u03b1\u03bb\u03b7i,js ) and yi,j\n= 0 (respectively, yi,j\n= 1), but (i, j) was active at time \u03b7s\u22121 . We prove\nthe following claim.\n\nClaim 28. N (\u03b7) can increase only at a horizon, i.e., for any s \u2265 0 with \u03b7s < 1, N (\u03b7s ) \u2265 N (\u03b7)\nfor \u03b7 \u2208 [\u03b7s , \u03b7s+1 ). Furthermore, if N (\u03b7) indeed increases at time \u03b7s+1 , then at \u03b7s+1 we have an\n\u03b7s\noccurrence of either a block merge, or the quota is hit, or a 1-inactivation of some (i, j) with yi,j\n< 1.\nProof. First, consider the case where at time \u03b7s the number of servers is still below the quota. By\n(59), it means that N (\u03b7) = 0 = N (\u03b7s ), for \u03b7 \u2208 (\u03b7s , \u03b7s+1 ), and N (\u03b7s+1 ) = 0 unless the quota is hit\nat time \u03b7s+1 . So, the claim follows in this case, and in the rest of the proof we can assume that the\nnumber of servers is already at the quota at time \u03b7s .\nFirst, we prove that N (\u03b7s ) \u2265 N (\u03b7) for \u03b7 \u2208 [\u03b7s , \u03b7s+1 ), i.e., the first part of the claim. Let us\nfix some \u03b7 \u2208 [\u03b7s , \u03b7s+1 ). Note that by the definition of the horizon, we have that A\u03b7 = A\u03b7s and\n+\n+\n\u03bb\u03b7 = \u03bb\u03b7s . So, for our purposes, it suffices to show that whenever A\u03b7 = A\u03b7 = A, and \u03bb\u03b7 = \u03bb\u03b7 = \u03bb,\nfor \u03b7 + = \u03b7 + d\u03b7, we have that N (\u03b7) \u2265 N (\u03b7 + ). This follows immediately from Claim 26.\nNow, to prove the second part of the claim, let us assume that none of the events mentioned\nin the statement of the claim occurred at time \u03b7s+1 , otherwise we are already done. So, we have,\n\u03b7\nin particular, that \u03bb\u03b7i,js = \u03bbi,js+1 = \u03bbi,j for each (i, j). This implies that if there is an (i, j) with\n\u03b7\n\u03b7s\nyi,js+1 = yi,j\n= 1 that becomes active at time \u03b7s+1 , then by Definition 9 it must be the case that\n\u03b7\n\nN (\u03b7s ) > \u03b1\u03bb\u03b7i,js = \u03b1\u03bbi,js+1 \u2265 N (\u03b7s+1 ).\nSo, in this case N (\u03b7s ) \u2265 N (\u03b7s+1 ), and thus we can restrict ourselves to the scenario in which the\n\u03b7\nonly coordinates (i, j) that become active at time \u03b7s+1 have yi,js+1 = 0. As a result, we have\nA\u03b7s+1 = (A\u03b7s \\ (A0\u2212 \u222a A1\u2212 )) \u222a A+ ,\nwhere A0\u2212 is the set of coordinates (i, j) such that (i, j) 0-inactivates at time \u03b7s+1 , A1\u2212 contains\n\u03b7s\n(i, j)-s which 1-inactivate at that time and yi,j\n= 1, and A+ is the set of (i, j)-s that become active\n\u03b7s+1\nat time \u03b7s+1 with yi,j = 0.\nNow, observe that if some (i, j) \u2208 A1\u2212 , then we need to have \u03b1\u03bbi,j \u2265 N (\u03b7s ). Otherwise, (i, j)\nwould have already been inactive at time \u03b7s . Furthermore, we actually need to have \u03b1\u03bbi,j = N (\u03b7s ),\n\u03b7\nas otherwise the derivative of yi,j\nwould be negative in the interval [\u03b7s , \u03b7s+1 ), contradicting the fact\n\u03b7\nthat yi,js+1 = 1. (Recall that we have already proved that N (\u03b7) \u2013 and thus all the derivatives \u2013 do\nnot increase in the interval [\u03b7s , \u03b7s+1 ).)\nSo, by the above, and Definition 9, we can conclude that\nfor each\n\n(i, j) \u2208 A0\u2212 ,\n\n(64)\n\n\u03b1\u03bbi,j = N (\u03b7s )\n\nfor each\n\n(i, j) \u2208\n\nA1\u2212 ,\n\n(65)\n\n\u03b1\u03bbi,j < N (\u03b7s+1 )\n\nfor each\n\n(i, j) \u2208 A+ .\n\n(66)\n\n\u03b1\u03bbi,j \u2265 N (\u03b7s+1 )\n\nOn the other hand, we can express N (\u03b7s+1 ) as the weighted average of \u03b1\u03bbi,j s over the set A\u03b7s+1\n(cf. Claim 26), i.e. we have\nP\n\u03b7s+1\n(i,j)\u2208A\u03b7s+1 ui,j * \u03b1\u03bbi,j\nP\n,\nN (\u03b7s+1 ) =\n\u03b7s+1\n(i,j)\u2208A\u03b7s+1 ui,j\n46\n\n\fwhere u\u03b7i,j =\n\n\u0010\n\n\u0011\n\n\u03b7\n1\nwi yi,j + \u03b2 .\nA\u03b7s+1 = (A\u03b7s \\\n\n(A0\u2212 \u222a A1\u2212 )) \u222a A+ , we can utilize conditions (64) and (66) to bound\nNow, as\nN (\u03b7p+1 ) from above by a corresponding weighted average of \u03b1\u03bbi,j -s over the set A\u03b7s \\ A1\u2212 . In\nparticular, we have\nP\nP\n\u03b7s+1\n\u03b7s+1\n\u03b7s+1 u\n*\n\u03b1\u03bb\ni,j\n(i,j)\u2208((A\u03b7s \\(A0\u2212 \u222aA1\u2212 ))\u222aA+ ) ui,j * \u03b1\u03bbi,j\n(i,j)\u2208A\ni,j\nP\nP\nN (\u03b7s+1 ) =\n=\n\u03b7s+1\n\u03b7s+1\n(i,j)\u2208A\u03b7s+1 ui,j\n(i,j)\u2208((A\u03b7s \\(A0\u2212 \u222aA1\u2212 ))\u222aA+ ) ui,j\nP\n\u03b7s+1\n(i,j)\u2208(A\u03b7s \\A1\u2212 ) ui,j * \u03b1\u03bbi,j\nP\n\u2264\n,\n(67)\n\u03b7s+1\n(i,j)\u2208(A\u03b7s \\A1 ) ui,j\n\u2212\n\na\na\n1 t1 \u2212c2 t2\nwhere the last inequality follows as for any a, b > 0, ab \u2264 a+c\nb+t1 \u2212t2 , whenever c1 \u2265 b , c2 \u2264 b , and\nt1 \u2265 0, b > t2 \u2265 0.\nIf the last expression in (67) is at most N (\u03b7s ), then we are already done. So, let us assume,\nfor the sake of contradiction, that it is strictly larger than N (\u03b7s ). In this case, by (66), we need to\nhave that also\nP\n\u03b7s+1\n(i,j)\u2208A\u03b7s ui,j * \u03b1\u03bbi,j\nP\n> N (\u03b7s ),\n(68)\n\u03b7s+1\n(i,j)\u2208A\u03b7s ui,j\n\nas for any a, b, c, t > 0, if\nwe have that\nP\n\na\nb\n\n> c, then also\n\u03b7\n\n(i,j)\u2208A\u03b7s\n\nP\n\na+ct\nb+t\n\nui,js+1 * \u03b1\u03bbi,j\n\u03b7\n\n(i,j)\u2208A\u03b7s\n\nui,js+1\n\n> c. However, by applying Claim 26 with A = A\u03b7s ,\nP\n\u03b7s\n(i,j)\u2208A\u03b7s ui,j * \u03b1\u03bbi,j\nP\n= N (\u03b7s ),\n\u2264\n\u03b7s\n(i,j)\u2208A\u03b7s ui,j\n\ncontradicting (68), and thus proving that indeed N (\u03b7s ) \u2265 N (\u03b7s+1 ). The proof of the claim is now\nconcluded.\n\nNow, we are ready to bound the number of events of type (a). To show that the number of\nthese events is finite, it suffices to show that the number of 0-inactivations and 1-inactivations is\nfinite. Also, observe that during the period in which the number of servers is below the quota, by\ndefinition, we have N (\u03b7) = 0, and thus variables can only decrease. As a result, coordinates can\nonly 0-inactivate in that period, and once they become inactive they stay that way. Hence, we have\nat most kd such events.\nIn light of the above, we can focus on analyzing the events after reaching the quota. Note that in\nthis case we can assume that N (\u03b7) > 0. (If N (\u03b7) = 0, then all derivatives are equal to zero, and the\ndesired bounds trivially follow.) As we have that \u03bb\u03b7i,j is always zero when i 6= i, N (\u03b7) > 0 implies\nthat coordinates (i, j) with i 6= i can only increase, and once they 1-inactivate they stay inactive.\nAs a consequence, it suffices to show that the number of 0-inactivations and 1-inactivations is finite\nfor all coordinates (i, j) with i = i. In order to do so, we prove the following claim.\nClaim 29. The total number of 1-inactivations of coordinates (i, j) is finite.\nProof. We will prove the claim first for j = k and then consider consecutive j-s in decreasing order.\nAs a result, our task is to prove for a given j, that (i, j) 1-inactivates a finite number of times,\nprovided that the number of 1-inactivations is finite for all coordinates (i, j \u2032 ) with j < j \u2032 \u2264 k.\nTo this end, we argue that whenever there are two consecutive 1-inactivations of some coordinate\n(i, j) \u2013 the first one at time \u03b7s\u2032 , and the second one at time \u03b7s\u2032\u2032 \u2013 then in the interval [\u03b7s\u2032 , \u03b7s\u2032\u2032 ] we\n47\n\n\fhave either a block merge, or the quota is hit, or a 1-activation of a coordinate (i\u2032 , j \u2032 ) that has\neither i\u2032 6= i or j \u2032 > j. As we know that the number of occurrences of each of these events is finite,\nwe get the desired proof.\nTo establish the above, let \u03b7s for s\u2032 < s < s\u2032\u2032 be the time in which (i, j) is activated between the\ntwo 1-inactivations. Observe that as N (\u03b7s ) \u2264 \u03b1\u03bbi,j and N (\u03b7s\u2032\u2032 ) > \u03b1\u03bbi,j , there is a time \u03b7s\u2217 with\ns < s\u2217 \u2264 s\u2032\u2032 in which N (\u03b7) increases above \u03b1\u03bbi,j . (Recall that by Claim 28 we know that N (\u03b7) can\nincrease only at horizons.) Without loss of generality we take s\u2217 to be the first s > s corresponding\nto such an increase.\nNow, in light of Claim 28, we know that N (\u03b7) increases at time \u03b7s\u2217 . Thus, to conclude our proof\n\u03b7 \u2217\nit suffices to show that if we have a coordinate (i\u2032 , j \u2032 ) that 1-inactivates at time \u03b7s\u2217 and yi\u2032s,j \u2032\u22121 < 1,\nthen we cannot have i\u2032 = i and j \u2032 \u2264 j.\nWe consider two cases here. The first one corresponds to s\u2217 < s\u2032\u2032 . In this case we have\n\u03b7s\u2217\nyi,j < 1, as otherwise (i, j) would be 1-inactivated already at time s\u2217 , instead of s\u2032\u2032 . However, by\n\u03b7\n\n\u03b7\n\nthe monotonicity property (4), we have that yi,js\u2217\u2032\u2032 \u2264 yi,js\u2217 for all j \u2032\u2032 \u2264 j. So, if i\u2032 = i, then (i\u2032 , j \u2032 )\ncannot 1-inactivate at time \u03b7s\u2217 if j \u2032 \u2264 j, and the claim follows.\nConsider now the remaining case of s\u2217 = s\u2032\u2032 . If we have i\u2032 = i and j \u2032 \u2264 j, then we must have\n\u03bbi,j \u2032 \u2265 \u03bbi,j . Otherwise, condition (8) for block merge would trigger at time \u03b7s\u2032\u2032 . As a result, by\n\u03b7\n(58), we know that in the interval of our interest the derivatives of yi,j\n\u2032 are bounded from above by\n\u03b7\nthe derivatives of yi,j .\n\u03b7\nis always non-positive for \u03b7 \u2208 [\u03b7s , \u03b7s\u2032\u2032 ). This is\nFurthermore, we have that the derivative of yi,j\n\u2217\nso, as by the definition of s , N (\u03b7) \u2264 \u03b1\u03bbi,j for \u03b7 \u2208 [\u03b7s , \u03b7s\u2217 ) = [\u03b7s , \u03b7s\u2032\u2032 ). As a consequence, we must\n\u03b7\n\u03b7\n\u2032\u2032\nhave both yi,j\nand yi,j\n\u2032 to be equal to 1 for all \u03b7 \u2208 [\u03b7s , \u03b7s ), as otherwise these variables would not\n\u03b7\n\n\u2217\n\nbe able to reach 1 at time \u03b7s\u2032\u2032 . This, however, contradicts the fact that yi\u2032s,j \u2032\u22121 has to be strictly\nsmaller than 1, as s\u2217 \u2212 1 = s\u2032\u2032 \u2212 1 \u2265 s. Thus, we cannot have i\u2032 = i and j \u2032 \u2264 j and our claim\nfollows.\nFinally, it remains to bound the number of 0-activations of coordinates (i, j). We do this by\nsimply noting that if there are two consecutive 0-activations of some coordinate (i, j), then N (\u03b7)\nhas to increase at least once between these two events. But, by Claim 27, it means that one of the\nevents, (whose total number is already bounded), would also occur in this period. Therefore, the\nnumber of 0-activations is also finite and we can conclude the proof of Lemma 11.\n\nB\n\nProof of Lemma 22\n\nHere we prove Lemma 22. As mentioned earlier, the proof is implicit in the work of [16], and we\nmake it explicit here for completeness. We begin with some notation, and state another result that\nwe need.\nLet M be an arbitrary metric space. Let C[0] denote the configuration specifying the initial\nlocation of the k-servers. We assume that the servers are labeled, so for every k\u2032 \u2264 k, the first\nk\u2032 entries of C[0] specify the location of the first k \u2032 servers. Let \u03c1 be some fixed k-server request\nsequence. Let Opt(k\u2032 , X) denote the optimum cost of serving \u03c1 with k\u2032 servers on M , starting in\nC[0] and ending in configuration X (for notational ease, we are suppressing the dependence on\n\u03c1, M, C[0] here). Let Opt(k\u2032 ) = minX Opt(k\u2032 , X), denote the minimum cost of server \u03c1 starting in\nC[0].\n48\n\n\fLemma 30 ([16], Corollary 2). Let \u03c1 be some fixed request sequence and C[0] be some fixed initial\nconfiguration. For any k1 , k2 \u2208 [k], given any state X on k1 locations, there exists another state Y\nsuch that\n1. |X \u2229 Y | = min(|X|, |Y |), i.e. Y overlaps with X as much as possible, and\n2. Opt(k2 , Y ) \u2264 Opt(k2 ) + Opt(k1 , X) \u2212 Opt(k1 ). That is, the excess cost incurred for an\noptimum k2 -server solution to end in Y , is no more than the excess cost incurred for the\noptimum k1 -server to end in X.\nThis lemma and its proof can be found in [16] (Corollary 2).\nLet T be a weighted \u03c3-HST. Again, for notational convenience, let us drop \u03c1, C[0], and the\nunderlying metric T from the notation (these remain the same, and dropping them will not cause\nany confusion). Given a quota pattern \u03ba, recall the definition of Optcost(\u03ba, t) as the optimum\ncost of serving \u03c1 until time t with quota pattern \u03ba. We also use Optcost(\u03ba) = Optcost(\u03ba, \u221e) to\ndenote the optimum cost of serving the entire sequenceP\u03c1. As previously, let us define ht (\u03ba) =\nOptcost(\u03ba(t) * ~1, t) \u2212 Optcost(\u03ba(t) * ~1, t \u2212 1) and g(\u03ba) = t\u22651 |\u03ba(t) \u2212 \u03ba(t \u2212 1)|. Let D denote the\ndiameter of T .\nWe will prove the following, which is the same as Lemma 22. (In that notation, note that\n\u2206 \u2264 W (p)/(\u03c3 \u2212 1) for T (p).)\nTheorem 31.\n\nX\n\nht (\u03ba) \u2212 \u2206 * g(\u03ba) \u2264 Optcost(\u03ba) \u2264\n\nt\n\nX\n\nht (\u03ba) + \u2206 * g(\u03ba).\n\nt\n\nProof. We do an induction on the value of g(\u03ba). In the base case, when g(\u03ba) = 0, the vector \u03ba is\nconstant throughout, say \u03ba = k * ~1. In this case, the claimed result holds trivially as the sum over\nht telescopes and we obtain\nX\nX\nht (\u03ba) =\n(Optcost(k * ~1, t) \u2212 Optcost(k * ~1, t \u2212 1)) = Optcost(\u03ba).\nt\n\nt\n\nSo, let \u03ba be such that g(\u03ba) > 0. Let \u03c4 be the earliest time when \u03ba(\u03c4 ) 6= \u03ba(\u03c4 + 1). Define a new\nquota pattern \u03ba\u2032 as\n\u001a\n\u03ba(\u03c4 + 1) if t \u2264 \u03c4\n\u2032\n\u03ba (t) =\n\u03ba(t)\nif t > \u03c4 .\n\nNote that both \u03ba and \u03ba\u2032 are constant for t \u2264 \u03c4 . Also, g(\u03ba\u2032 ) = g(\u03ba) \u2212 |\u03ba(\u03c4 + 1) \u2212 \u03ba(\u03c4 )| < g(\u03ba), and\nhence we can inductively assume that the\nresult holds for \u03ba\u2032 .\nP claimed\nt\nWe first show that Optcost(\u03ba) \u2265\nt h (\u03ba) \u2212 D * g(\u03ba). Fix some solution that attains cost\nOptcost(\u03ba) and let C[t] denote its configuration at time t. Applying Lemma 30 with X = C(\u03c4 )\nand k1 = \u03ba(\u03c4 ) and k2 = \u03ba\u2032 (\u03c4 ), there is some configuration Y satisfying\nOptcost(k2 * ~1, \u03c4, Y ) \u2264 Optcost(k2 * ~1, \u03c4 ) + Optcost(k1 * ~1, \u03c4, X) \u2212 Optcost(k1 * ~1, \u03c4 )\n|X \u2229 Y | = min(k1 , k2 ).\n\n(69)\n(70)\n\nWe construct a solution S \u2032 corresponding to \u03ba\u2032 as follows: Until time \u03c4 , S \u2032 follows the solution\nOptcost(k2 * ~1, \u03c4, Y ). Then, after serving the request at t = \u03c4 , it switches to state C[\u03c4 ] = X, and\nhenceforth for t > \u03c4 sets its the configurations C \u2032 [t] = C[t]. Now\ncost(S \u2032 ) = Optcost(k2 * ~1, \u03c4, Y ) + c(Y, X) + Q,\n49\n\n\fwhere c(Y, X) is the cost of moving from state Y to X, and Q is the contribution of the solution\nOptcost(\u03ba) starting from time \u03c4 and state X (recall that C[\u03c4 ] = X).\nIt is easily checked that the solution S \u2032 constructed above is feasible for quota pattern \u03ba\u2032 . As\nthe optimum solution for \u03ba\u2032 can only be better, cost(S \u2032 ) \u2265 Optcost(\u03ba\u2032 ) and since Optcost(\u03ba\u2032 ) \u2265\nP\nt \u2032\n\u2032\nt h (\u03ba ) \u2212 D * g(\u03ba ) by the inductive hypothesis, it follows that\nX\nht (\u03ba\u2032 ) \u2212 D * g(\u03ba\u2032 ) \u2264 cost(S \u2032 ) = Optcost(k2 * ~1, \u03c4, Y ) + c(Y, X) + Q.\n(71)\nt\n\nBy (70), |Y \u2229 X| = min(k1 , k2 ) and hence c(Y, X) \u2264 D|k2 \u2212 k1 | = D|\u03ba(\u03c4 + 1) \u2212 \u03ba(\u03c4 )|. Thus\nc(Y, X) + D * g(\u03ba\u2032 ) \u2264 D * g(\u03ba), and hence (71) implies that\nX\nQ\u2265\nht (\u03ba\u2032 ) \u2212 D * g(\u03ba) \u2212 Optcost(k2 * ~1, \u03c4, Y ).\n(72)\nt\n\nOn the other hand, as X = C[\u03c4 ] we have\nOptcost(\u03ba) = Optcost(\u03ba, \u03c4, X) + Q = Optcost(k1 * ~1, \u03c4, X) + Q.\nThus by (72),\nOptcost(\u03ba) \u2265 Optcost(k1 * ~1, \u03c4, X) \u2212 Optcost(k2 * ~1, \u03c4, Y ) +\n\u2265 Optcost(k1 * ~1, \u03c4 ) \u2212 Optcost(k2 * ~1, \u03c4 ) +\n=\n\nX\n\nX\n\nX\n\nht (\u03ba\u2032 ) \u2212 D * g(\u03ba)\n\nt\n\nt\n\nh (\u03ba\u2032 ) \u2212 D * g(\u03ba)\n\n(73)\n\nt\n\nht (\u03ba) \u2212 D * g(\u03ba),\n\n(74)\n\nt\n\nimplying the desired lower bound. Here (73) follows from (69), and (74) follows from (73) since\n\u03c4\nX\n\nht (\u03ba) = Optcost(k1 * ~1, \u03c4 ),\n\nt=1\n\n\u03c4\nX\n\nht (\u03ba\u2032 ) = Optcost(k2 * ~1, \u03c4 ),\n\nt=1\n\nP\nP\nP\nP\nand t ht (\u03ba) \u2212 t ht (\u03ba\u2032 ) = \u03c4t=1 ht (\u03ba) \u2212 \u03c4t=1 ht (\u03ba\u2032 ), since \u03ba, and \u03ba\u2032 are the same for any t > \u03c4 .\nWe now show show the upper bound on Optcost(\u03ba). The proof is similar to the one above. Let\n\u2032\n\u03ba be defined as previously. Let {C \u2032 [t]}t denote the configurations for some fixed solution that has\nvalue Optcost(\u03ba\u2032 ). Applying Lemma 30 with X = C \u2032 [\u03c4 ],k1 = \u03ba\u2032 (\u03c4 ) and k2 = \u03ba(\u03c4 ), we obtain a\nconfiguration Y satisfying\nOptcost(k2 * ~1, \u03c4, Y ) \u2264 Optcost(k2 * ~1, \u03c4 ) + Optcost(k1 * ~1, \u03c4, X) \u2212 Optcost(k1 * ~1, \u03c4 )\n|X \u2229 Y | = min(k1 , k2 ).\n\n(75)\n(76)\n\nConsider the following solution S corresponding to \u03ba: Until time \u03c4 , S mimics the solution Optcost(k2 *\n~1, \u03c4, Y ). Then, after serving the request at t = \u03c4 , it switches to state C \u2032 [\u03c4 ] = X, and henceforth\nfor t > \u03c4 sets its the configurations C[t] = C \u2032 [t]. Now\ncost(S) = Optcost(k2 * ~1, \u03c4, Y ) + c(Y, X) + Q,\n50\n\n\fwhere Q is the cost of solution Optcost(\u03ba\u2032 ) incurred from time \u03c4 starting at state Y . Again S is\nfeasible for quota \u03ba, and hence Optcost(\u03ba) \u2264 cost(S). By definition of X,\nOptcost(\u03ba\u2032 ) = Optcost(k1 * ~1, \u03c4, X) + Q.\nThus,\nOptcost(\u03ba) \u2264 Optcost(k2 * ~1, \u03c4, Y ) + c(Y, X) + Optcost(\u03ba\u2032 ) \u2212 Optcost(k1 * ~1, \u03c4, X).\nP\nAs Optcost(\u03ba\u2032 ) \u2264 t ht (\u03ba\u2032 )+D *g(\u03ba\u2032 ) by the inductive hypothesis and c(Y, X)+D *g(\u03ba\u2032 ) \u2264 D *g(\u03ba),\nwe obtain\nX\nOptcost(\u03ba) \u2264 Optcost(k2 * ~1, \u03c4, Y ) +\nht (\u03ba\u2032 ) + D * g(\u03ba) \u2212 Optcost(k1 * ~1, \u03c4, X)\n(77)\nt\n\n\u2264 Optcost(k2 * ~1, \u03c4 ) \u2212 Optcost(k1 * ~1, \u03c4 ) +\n=\n\nX\n\nX\n\nht (\u03ba\u2032 ) \u2212 D * g(\u03ba)\n\nht (\u03ba) \u2212 D * g(\u03ba),\n\nt\n\nimplying the desired inequality. Here (78) follows from (75), and (79) follows by noting that\nX\nt\n\nht (\u03ba\u2032 ) \u2212\n\nX\nt\n\n(78)\n\nt\n\nht (\u03ba) =\n\n\u03c4\nX\nt=1\n\nht (\u03ba\u2032 ) \u2212\n\n\u03c4\nX\n\nht (\u03ba) = Optcost(k1 * ~1, \u03c4 ) \u2212 Optcost(k2 * ~1, \u03c4 ).\n\nt=1\n\n51\n\n(79)\n\n\f"}