{"id": "http://arxiv.org/abs/cs/0209018v2", "guidislink": true, "updated": "2002-09-25T16:11:20Z", "updated_parsed": [2002, 9, 25, 16, 11, 20, 2, 268, 0], "published": "2002-09-16T16:26:02Z", "published_parsed": [2002, 9, 16, 16, 26, 2, 0, 259, 0], "title": "Probabilistic Reversible Automata and Quantum Automata", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0412013%2Ccs%2F0412101%2Ccs%2F0412111%2Ccs%2F0412033%2Ccs%2F0412061%2Ccs%2F0412015%2Ccs%2F0412010%2Ccs%2F0412071%2Ccs%2F0412064%2Ccs%2F0412028%2Ccs%2F0412037%2Ccs%2F0412068%2Ccs%2F0412120%2Ccs%2F0412066%2Ccs%2F0412030%2Ccs%2F0412076%2Ccs%2F0412060%2Ccs%2F0412110%2Ccs%2F0412034%2Ccs%2F0412024%2Ccs%2F0412118%2Ccs%2F0412052%2Ccs%2F0412095%2Ccs%2F0412074%2Ccs%2F0412006%2Ccs%2F0412098%2Ccs%2F0412063%2Ccs%2F0412021%2Ccs%2F0412031%2Ccs%2F0412099%2Ccs%2F0412100%2Ccs%2F0412036%2Ccs%2F0412080%2Ccs%2F0412096%2Ccs%2F0412077%2Ccs%2F0412109%2Ccs%2F0412043%2Ccs%2F0412048%2Ccs%2F0412117%2Ccs%2F0412081%2Ccs%2F0412007%2Ccs%2F0412059%2Ccs%2F0412026%2Ccs%2F0412023%2Ccs%2F0412050%2Ccs%2F0412027%2Ccs%2F0412017%2Ccs%2F0412116%2Ccs%2F0412106%2Ccs%2F0412003%2Ccs%2F0412083%2Ccs%2F0412041%2Ccs%2F0412002%2Ccs%2F0412104%2Ccs%2F0412025%2Ccs%2F0412056%2Ccs%2F0412121%2Ccs%2F0412012%2Ccs%2F0412042%2Ccs%2F0412091%2Ccs%2F0209003%2Ccs%2F0209016%2Ccs%2F0209032%2Ccs%2F0209004%2Ccs%2F0209033%2Ccs%2F0209021%2Ccs%2F0209028%2Ccs%2F0209019%2Ccs%2F0209011%2Ccs%2F0209023%2Ccs%2F0209002%2Ccs%2F0209025%2Ccs%2F0209018%2Ccs%2F0209020%2Ccs%2F0209013%2Ccs%2F0209015%2Ccs%2F0209034%2Ccs%2F0209029%2Ccs%2F0209010%2Ccs%2F0209006%2Ccs%2F0209026%2Ccs%2F0209009%2Ccs%2F0209008%2Ccs%2F0209024%2Ccs%2F0209005%2Ccs%2F0209012%2Ccs%2F0209014%2Ccs%2F0209017%2Ccs%2F0209030%2Ccs%2F0209031%2Ccs%2F0209001%2Ccs%2F0209027%2Ccs%2F0612055%2Ccs%2F0612117%2Ccs%2F0612013%2Ccs%2F0612135%2Ccs%2F0612107%2Ccs%2F0612101%2Ccs%2F0612014%2Ccs%2F0612111%2Ccs%2F0612103&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Probabilistic Reversible Automata and Quantum Automata"}, "summary": "To study relationship between quantum finite automata and probabilistic\nfinite automata, we introduce a notion of probabilistic reversible automata\n(PRA, or doubly stochastic automata). We find that there is a strong\nrelationship between different possible models of PRA and corresponding models\nof quantum finite automata. We also propose a classification of reversible\nfinite 1-way automata.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0412013%2Ccs%2F0412101%2Ccs%2F0412111%2Ccs%2F0412033%2Ccs%2F0412061%2Ccs%2F0412015%2Ccs%2F0412010%2Ccs%2F0412071%2Ccs%2F0412064%2Ccs%2F0412028%2Ccs%2F0412037%2Ccs%2F0412068%2Ccs%2F0412120%2Ccs%2F0412066%2Ccs%2F0412030%2Ccs%2F0412076%2Ccs%2F0412060%2Ccs%2F0412110%2Ccs%2F0412034%2Ccs%2F0412024%2Ccs%2F0412118%2Ccs%2F0412052%2Ccs%2F0412095%2Ccs%2F0412074%2Ccs%2F0412006%2Ccs%2F0412098%2Ccs%2F0412063%2Ccs%2F0412021%2Ccs%2F0412031%2Ccs%2F0412099%2Ccs%2F0412100%2Ccs%2F0412036%2Ccs%2F0412080%2Ccs%2F0412096%2Ccs%2F0412077%2Ccs%2F0412109%2Ccs%2F0412043%2Ccs%2F0412048%2Ccs%2F0412117%2Ccs%2F0412081%2Ccs%2F0412007%2Ccs%2F0412059%2Ccs%2F0412026%2Ccs%2F0412023%2Ccs%2F0412050%2Ccs%2F0412027%2Ccs%2F0412017%2Ccs%2F0412116%2Ccs%2F0412106%2Ccs%2F0412003%2Ccs%2F0412083%2Ccs%2F0412041%2Ccs%2F0412002%2Ccs%2F0412104%2Ccs%2F0412025%2Ccs%2F0412056%2Ccs%2F0412121%2Ccs%2F0412012%2Ccs%2F0412042%2Ccs%2F0412091%2Ccs%2F0209003%2Ccs%2F0209016%2Ccs%2F0209032%2Ccs%2F0209004%2Ccs%2F0209033%2Ccs%2F0209021%2Ccs%2F0209028%2Ccs%2F0209019%2Ccs%2F0209011%2Ccs%2F0209023%2Ccs%2F0209002%2Ccs%2F0209025%2Ccs%2F0209018%2Ccs%2F0209020%2Ccs%2F0209013%2Ccs%2F0209015%2Ccs%2F0209034%2Ccs%2F0209029%2Ccs%2F0209010%2Ccs%2F0209006%2Ccs%2F0209026%2Ccs%2F0209009%2Ccs%2F0209008%2Ccs%2F0209024%2Ccs%2F0209005%2Ccs%2F0209012%2Ccs%2F0209014%2Ccs%2F0209017%2Ccs%2F0209030%2Ccs%2F0209031%2Ccs%2F0209001%2Ccs%2F0209027%2Ccs%2F0612055%2Ccs%2F0612117%2Ccs%2F0612013%2Ccs%2F0612135%2Ccs%2F0612107%2Ccs%2F0612101%2Ccs%2F0612014%2Ccs%2F0612111%2Ccs%2F0612103&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "To study relationship between quantum finite automata and probabilistic\nfinite automata, we introduce a notion of probabilistic reversible automata\n(PRA, or doubly stochastic automata). We find that there is a strong\nrelationship between different possible models of PRA and corresponding models\nof quantum finite automata. We also propose a classification of reversible\nfinite 1-way automata."}, "authors": ["Marats Golovkins", "Maksim Kravtsev"], "author_detail": {"name": "Maksim Kravtsev"}, "author": "Maksim Kravtsev", "arxiv_comment": "COCOON 2002, extended version of the paper, 22 pages", "links": [{"href": "http://arxiv.org/abs/cs/0209018v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0209018v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.FL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "F.1.1; F.4.3", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0209018v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0209018v2", "journal_reference": "Lecture Notes in Computer Science, 2002, Vol. 2387, pp. 574-583", "doi": null, "fulltext": "Probabilistic Reversible Automata and Quantum\nAutomata\n\narXiv:cs/0209018v2 [cs.CC] 25 Sep 2002\n\nMarats Golovkins\n\n\u22c6\n\nand Maksim Kravtsev\n\n\u22c6\u22c6\n\nInstitute of Mathematics and Computer Science, University of Latvia\nRai\u0146a bulv. 29, Riga, Latvia\nmarats@latnet.lv, maksims@batsoft.lv\n\nAbstract. To study relationship between quantum finite automata and\nprobabilistic finite automata, we introduce a notion of probabilistic reversible automata (PRA, or doubly stochastic automata). We find that\nthere is a strong relationship between different possible models of PRA\nand corresponding models of quantum finite automata. We also propose\na classification of reversible finite 1-way automata.\n\n1\n\nIntroduction\n\nHere we introduce common notions used throughout the paper as well as summarize its contents.\nWe analyze two models of probabilistic reversible automata in this paper,\nnamely, 1-way PRA and 1.5-way PRA.\nIn this section, we define notions applicable to both models in a quasi-formal\nway, including a general definition for probabilistic reversibility. These notions\nare defined formally in further sections.\nIf not specified otherwise, we denote by \u03a3 an input alphabet of an automaton.\nEvery input word is enclosed into end-marker symbols # and $. Therefore\nwe introduce a working alphabet as \u0393 = \u03a3 \u222a {#, $}.\nBy Q we normally understand the set of states of an automaton.\nBy L we understand complement of a language L.\nGiven an input word \u03c9, by |\u03c9| we understand the number of symbols in \u03c9 and\nwith [\u03c9]i we denote i-th symbol of \u03c9, counting from the beginning (excluding\nend-markers).\nDefinition 1.1. A configuration of a finite automaton is c = h\u03bdi qj \u03bdk i, where\nthe automaton is in a state qj \u2208 Q, \u03bdi \u03bdk \u2208 #\u03a3 \u2217 $ is a finite word on the input\ntape and input tape head is above the first symbol of the word \u03bdk .\n\u22c6\n\n\u22c6\u22c6\n\nResearch partially supported by the Latvian Council of Science, grant No. 01.0354\nand grant for Ph.D. students; University of Latvia, K. Morbergs grant; European\nCommission, contract IST-1999-11234\nResearch partially supported by the Latvian Council of Science, grant No. 01.0354\nand European Commission, contract IST-1999-11234\n\n\fBy C we denote the set of all configurations of an automaton. This set is\ncountably infinite.\nAfter its every step, a probabilistic automaton is in some probability distribution p0 c0 + p1 c1 + . . . + pn cn , where p0 + p1 + . . . + pn = 1. Such probability\ndistribution is called a superposition of configurations. Given an input word \u03c9,\nthe number of configurations in every accessible superposition does not exceed\n|Q| in case of 1-way automata, and |\u03c9||Q| in case of 1.5-way automata.\nA linear closure of C forms a linear space, where every configuration can be\nviewed as a basis vector. This basis is called a canonical basis. Every probabilistic\nautomaton defines a linear operator over this linear space.\nLet us consider A. Nayak's model of quantum automata with mixed states.\n(Evolution is characterized by a unitary matrix and subsequent measurements\nare performed after each step, POVM measurements not being allowed, [N 99].)\nIf a result of every measurement is a single configuration, not a superposition,\nand measurements are performed after each step, we actually get a probabilistic\nautomaton. However, the following property applies to such probabilistic automata - their evolution matrices are doubly stochastic. This encourages us to\ngive the following definition for probabilistic reversible automata:\nDefinition 1.2. A probabilistic automaton is called reversible if its linear operator can be described by a doubly stochastic matrix, using canonical basis.\nTo make accessible configurations of type hqi #\u03c9$i, we assume that every word\nis written on a circular tape, and after the right end-marker $ the next symbol is\nthe left end-marker #. Such precondition is the same as used for quantum finite\nautomata. (See, for example, [KW 97].)\nAt least two definitions exist, how to interpret word acceptance, and hence,\nlanguage recognition, for reversible automata.\nDefinition 1.3. Classical acceptance. We say that an automaton accepts (rejects) a word classically, if its set of states consists of two disjoint subsets: accepting states and rejecting states, and the following conditions hold:\n\u2013 the\nthe\n\u2013 the\nthe\n\nautomaton accepts the word, if it is in accepting state after having read\nlast symbol of the word;\nautomaton rejects the word, if it is in rejecting state after having read\nlast symbol of the word.\n\nWe refer to the classical acceptance automata as C-automata further in the\npaper.\nDefinition 1.4. \"Decide and halt\" acceptance. We say that an automaton accepts (rejects) a word in a decide-and-halt manner, if its set of states consists\nof three disjoint subsets: accepting states, rejecting states and non-halting states,\nand the following conditions hold:\n\u2013 the computation is continued only if the automaton enters a non-halting\nstate.\n\u2013 if the automaton enters an accepting state, the word is accepted;\n\n\f\u2013 if the automaton enters a rejecting state, the word is rejected.\nWe refer to the decide-and-halt automata as DH-automata further in the paper.\nHaving defined word acceptance, we define language recognition in an equivalent way as in [R 63]. We consider only bounded error language recognition in\nthis paper.\nBy Px,A we denote the probability that a word x is accepted by an automaton\nA.\nDefinition 1.5. We say that a language L is recognized with bounded error by\nan automaton A with interval (p1 , p2 ) if p1 < p2 and p1 = sup{Px,A | x \u2208\n/ L},\np2 = inf{Px,A | x \u2208 L}.\nDefinition 1.6. We say that a language is recognized with a probability p if the\nlanguage is recognized with interval (1 \u2212 p, p).\nDefinition 1.7. We say that a language is recognized with probability 1 \u2212 \u03b5, if\nfor every \u03b5 > 0 there exists an automaton which recognizes the language with\ninterval (\u03b51 , 1 \u2212 \u03b52 ), where \u03b51 , \u03b52 \u2264 \u03b5.\nS\n\nDefinition 1.8. By q \u2212\u2192 q \u2032 , S \u2282 \u03a3 \u2217 , we denote that there is a positive probability to get to a state q \u2032 by reading a single word \u03be \u2208 S, starting in a state\nq.\nWe refer to several existing models of quantum finite automata:\n1. Measure-once quantum finite automata [MC 97] (QFA-MC);\n2. Measure-many quantum finite automata [KW 97] (QFA-KW);\n3. Enhanced quantum finite automata [N 99] (QFA-N).\nFollowing the notions above, QFA-MC can be characterized as C-automata\nwhereas QFA-KW and QFA-N as DH-automata.\nIn Section 2, we discuss properties of PRA C-automata (PRA-C). We prove\nthat PRA-C recognize the class of languages a\u22171 a\u22172 . . . a\u2217n with probability 1 \u2212 \u03b5.\nThis class can be recognized by QFA-KW, with worse acceptance probabilities,\nhowever [ABFK 99]. This also implies that QFA-N recognize this class of languages with probability 1 \u2212 \u03b5.\nFurther, we show general class of regular languages, not recognizable by\nPRA-C. In particular, such languages as (a,b)*a and a(a,b)* are in this class.\nThis class has strong similarities with the class of languages, not recognizable\nby QFA-KW [AKV 00].\nWe also show that the class of languages recognized by PRA-C is closed\nunder boolean operations, inverse homomorphisms and word quotient, but is\nnot closed under homomorphisms.\nIn Section 3 we prove, that PRA DH-automata do not recognize the language\n(a,b)*a.\nIn Section 4 we discuss some properties of 1.5-way PRA. We also present\nan alternative notion of probabilistic reversibility, not connected with quantum\nautomata.\nIn Section 5 we propose a classification of reversible automata (deterministic,\nprobabilistic and quantum).\n\n\f2\n\n1-way Probabilistic Reversible C-Automata\n\nDefinition 2.1. 1-way probabilistic reversible C-automaton (PRA-C)\nA = (Q, \u03a3, q0 , QF , \u03b4) is specified by a finite set of states Q, a finite input alphabet\n\u03a3, an initial state q0 \u2208 Q, a set of accepting states QF \u2286 Q, and a transition\nfunction\n\u03b4 : Q \u00d7 \u0393 \u00d7 Q \u2212\u2192 IR[0,1] ,\nwhere \u0393 = \u03a3 \u222a {#, $} is the input tape alphabet of A and #, $ are end-markers\nnot in \u03a3. Furthermore, transition function satisfies the following requirements:\nX\n\u2200(q1 , \u03c31 ) \u2208 Q \u00d7 \u0393\n\u03b4(q1 , \u03c31 , q) = 1\n(1)\nq\u2208Q\n\n\u2200(q1 , \u03c31 ) \u2208 Q \u00d7 \u0393\n\nX\n\n\u03b4(q, \u03c31 , q1 ) = 1\n\n(2)\n\nq\u2208Q\n\nFor every input symbol \u03c3 \u2208 \u0393 , the transition function may be determined\nby a |Q| \u00d7 |Q| matrix V\u03c3 , where (V\u03c3 )i,j = \u03b4(qj , \u03c3, qi ).\nLemma 2.2. All matrices V\u03c3 are doubly stochastic iff conditions (1) and (2) of\nDefinition 2.1 hold.\nProof. Trivial.\n\n\u2293\n\u2294\n\nWe define word acceptance as specified in Definition 1.3. The set of rejecting\nstates is Q \\ QF . We define language recognition as in Definition 1.5.\nA linear operator UA corresponds to the automaton A. Formal definition of\nthis operator follows:\nDefinition 2.3. Given a configuration c = h\u03bdi qj \u03c3\u03bdk i,\nX\ndef\nUA c =\n\u03b4(qj , \u03c3, q)h\u03bdi \u03c3q\u03bdk i.\nq\u2208Q\n\nGiven a superposition of configurations \u03c8 =\n\nP\n\npc c,\n\nc\u2208C\ndef\n\nUA \u03c8 =\n\nX\n\npc UA c.\n\nc\u2208C\n\nUsing canonical basis, UA is described by an infinite matrix MA .\nTo comply with Definition 1.2, we have to state the following:\nLemma 2.4. Matrix MA is doubly stochastic iff conditions (1) and (2) of Definition 2.1 hold.\nProof. Condition (1) takes place if and only if the sum of elements in every\ncolumn in MA equal to 1. Condition (2) takes place if and only if the sum of\nelements in every row in MA equal to 1.\n\u2293\n\u2294\n\n\fThis completes our formal definition of PRA-C.\nUse of end-markers does not affect computational power of PRA-C. For every PRA-C with end-markers which recognizes some language it is possible to\nconstruct a PRA-C without end-markers which recognizes the same language.\n(Number of states needed may increase, however.) See Appendix for further\ndetails.\nLemma 2.5. If a language is recognized by a PRA-C A with interval (p1 , p2 ),\nexists a PRA-C which recognizes the language with probability p, where\n(\np2\nif p1 + p2 \u2265 1\n+p2 ,\np = p11\u2212p\n1\n,\nif\np1 + p2 < 1.\n2\u2212p1 \u2212p2\nProof. Let us assume, that the automaton A has n \u2212 1 states. We consider the\ncase p1 + p2 > 1.\nInformally, having read end-marker symbol #, we simulate the automaton A\n1\n2 \u22121\nwith probability p1 +p\nand reject input with probability p1p+p\n.\n2\n1 +p2\n2\nFormally, to recognize the language with probability p1p+p\n,\nwe\nmodify the\n2\nautomaton A. We add a new state qr \u2208\n/ QF , and change the transition function\nin the following way:\ndef\n\n\u2013 \u2200\u03c3, \u03c3 6= #, \u03b4(qr , \u03c3, qr ) = 1;\ndef\n2 \u22121\n;\n\u2013 \u03b4(q0 , #, qr ) = p1p+p\n1 +p2\ndef\n\n\u2013 \u2200q, q 6= qr , \u03b4(q0 , #, q) =\n\n1\np1 +p2 \u03b4old (q0 , #, q).\n\nNow the automaton has n states. Since end-marker symbol # is read only once at\nthe beginning of an input word, we can disregard the rest of transition function\ndef 1\u2212\u03b4(q0 ,#,qj )\nvalues, associated with #: \u2200qi , qj , where qi 6= q0 , \u03b4(qi , #, qj ) =\n.\nn\u22121\nThe transition function satisfies the requirements of Definition 2.1 and the\n2\n.\nconstructed automaton recognizes the language with probability p1p+p\n2\nThe case p1 + p2 < 1 is very similar. Informally, having read end-marker\nsymbol #, we simulate the automaton A with probability 2\u2212p11\u2212p2 and accept\n1\u2212p1 \u2212p2\n.\n\u2293\n\u2294\ninput with probability 2\u2212p\n1 \u2212p2\nTheorem 2.6. If a language is recognized by a PRA-C, it is recognized by\nPRA-C with probability 1 \u2212 \u03b5.\nProof. We assume that a language L is recognized by a PRA-C automaton\nA = (Q, \u03a3, q0 , QF , \u03b4) with interval (p1 , p2 ). Let \u03b4 = 12 (p1 + p2 ).\nLet us consider a system of m copies of the automaton A, denoted as Am .\nWe say that our system has accepted (rejected) a word if more (less or equal)\nthan m\u03b4 automata in the system have accepted (rejected) the word. We define\nlanguage recognition as in Definition 1.5.\nLet us consider a word \u03c9 \u2208 L. The automaton A accepts \u03c9 with probability\np\u03c9 \u2265 p2 . As a result of reading \u03c9, \u03bc\u03c9\naccept the word,\nm automata of the system\n\u03bc\u03c9\nand the rest reject it. The system has accepted the word, if mm > \u03b4. Let us take\n\n\f\u03bc\u03c9\n\n\u03b70 , such that 0 < \u03b70 < p2 \u2212 \u03b4 \u2264 pw \u2212 \u03b4. Estimating the probability that mm > \u03b4,\nwe have\n\u001b\n\u001b\n\u001a \u03c9\n\u001b\n\u001a\n\u001a \u03c9\n\u03bcm\n\u03bc\u03c9\n\u03bcm\nm\n(3)\n> \u03b4 \u2265 P p\u03c9 \u2212 \u03b70 <\n< p\u03c9 + \u03b70 = P\n\u2212 p\u03c9 < \u03b70\nP\nm\nm\nm\nIn case of m Bernoulli trials, Chebyshev's inequality may be used to prove the\nfollowing ([GS 97], p. 312):\n\u001a \u03c9\n\u001b\n\u03bcm\np\u03c9 (1 \u2212 p\u03c9 )\n1\nP\n\u2264\n(4)\n\u2212 p\u03c9 \u2265 \u03b70 \u2264\nm\nm\u03b702\n4m\u03b702\nThe last inequality induces that\n\u001b\n\u001a \u03c9\n1\n\u03bcm\n\u2212 p\u03c9 < \u03b70 \u2265 1 \u2212\nP\nm\n4m\u03b702\nFinally, putting (3) and (5) together,\n\u001a \u03c9\n\u001b\n1\n\u03bcm\nP\n>\u03b4 \u22651\u2212\nm\n4m\u03b702\n\n(5)\n\n(6)\n\nInequality (6) is true for every \u03c9 \u2208 L.\nOn the other hand, let us consider a word \u03be \u2208\n/ L. The automaton A accepts\n\u03be with probability p\u03be \u2264 p1 . If we take the same \u03b70 , 0 < \u03b70 < \u03b4 \u2212 p1 \u2264 \u03b4 \u2212 p\u03be and\nfor every \u03be we have\n\u001b\n\u001a \u03be\n\u001b\n\u001a \u03be\n1\n\u03bcm\n\u03bcm\nP\n(7)\n>\u03b4 \u2264P\n\u2212 p\u03be \u2265 \u03b70 \u2264\nm\nm\n4m\u03b702\n1\nDue to (6) and (7), for every \u03b5 > 0, if we take n > 4\u03b5\u03b7\n2 , we get a system An\n0\nwhich recognizes the language L with interval (\u03b51 , 1 \u2212 \u03b52 ), where \u03b51 , \u03b52 < \u03b5.\nLet us show that An can be simulated by a PRA-C. The automaton A\u2032 =\n(Q\u2032 , \u03a3, q0\u2032 , Q\u2032F , \u03b4 \u2032 ) is constructed as follows:\ndef\ndef\nQ\u2032 = {hqs1 qs2 . . . qsn i | 0 \u2264 si \u2264 |Q| \u2212 1}; q0\u2032 = hq0 q0 . . . q0 i.\nA sequence hqs1 qs2 . . . qsn i is an accepting state of A\u2032 if more than n\u03b4 elements\nin the sequence are accepting states of A. We have defined the set Q\u2032F .\nn\ndef Q\n\u03b4(qai , \u03c3, qbi ).\nGiven \u03c3 \u2208 \u0393 , \u03b4 \u2032 (hqa1 qa2 . . . qan i, \u03c3, hqb1 qb2 . . . qbn i) =\ni=1\n\nIn essence, Q\u2032 is n-th Cartesian power of Q and the linear space formed by\n\u2032\nA is n-th tensor power of the linear space formed by A. If we take a symbol\n\u03c3 \u2208 \u0393 , transition is determined by |Q|n \u00d7 |Q|n matrix V\u03c3\u2032 , which is n-th matrix\nn\nN\nV\u03c3 .\ndirect power of V\u03c3 , i.e, V\u03c3\u2032 =\ni=1\n\nA\u2032 simulates the system An . Since matrix direct product of two doubly\nstochastic matrices is a doubly stochastic matrix, \u2200\u03c3 V\u03c3\u2032 are doubly stochastic matrices. Therefore our automaton A\u2032 is a PRA-C.\nWe have proved that \u2200\u03b5 > 0 the language L is recognized by some PRA-C\nwith interval (\u03b51 , 1\u2212\u03b52 ), where \u03b51 , \u03b52 < \u03b5. Therefore the language L is recognized\nwith probability 1 \u2212 \u03b5.\n\u2293\n\u2294\n\n\fLemma 2.7. If a language L1 is recognizable with probability greater than 23\nand a language L2 is recognizable with probability greater than 23 then languages\nL1 \u2229 L2 and L1 \u222a L2 are recognizable with probability greater than 12 .\nProof. Let us consider automata A = (QA , \u03a3, q0,A , QF,A , \u03b4A ) and\nB = (QB , \u03a3, q0,B , QF,B , \u03b4B ) which recognize the languages L1 , L2 with probabilities p1 , p2 > 23 , respectively. Let us assume that A, B have m and n states,\nrespectively. Without loss of generality we can assume that p1 \u2264 p2 .\nInformally, having read end-marker symbol #, with probability 21 we simulate\nthe automaton A1 and with the same probability we simulate the automaton A2 .\nFormally, we construct an automaton C = (Q, \u03a3, q0 , QF , \u03b4) with the following\nproperties.\ndef\ndef\ndef\ndef\nQ = QA \u222a QB ; q0 = q0,A ; QF = QF,A \u222a QF,B ; \u03b4 = \u03b4A \u222a \u03b4B , with an exception\nthat:\n\u2013 \u03b4(q0 , #, qi,A ) = 12 \u03b4A (q0 , #, qi,A );\n\u2013 \u03b4(q0 , #, qi,B ) = 21 \u03b4B (q0 , #, qi,B );\n0 ,#,q)\n\u2013 \u2200qi , qi 6= q0 , \u03b4(qi , #, q) = 1\u2212\u03b4(q\nm+n\u22121 .\nSince \u03b4 satisfies Definition 2.1, our construction of PRA-C is complete.\n2\nThe automaton C recognizes the language L1 \u2229 L2 with interval (p, p1 +p\n2 ),\np1 +p2\n1\n2\n1\nwhere p \u2264 1 \u2212 2 p1 . (Since p1 , p2 > 3 , 1 \u2212 2 p1 < 2 )\nThe automaton C recognizes the language L1 \u222a L2 with interval ( 2\u2212p21 \u2212p2 , p),\nwhere p \u2265 21 p1 . (Again, 2\u2212p21 \u2212p2 < 12 p1 )\nTherefore by Lemma 2.5, the languages L1 \u2229 L2 and L1 \u222a L2 are recognizable\n\u2293\n\u2294\nwith probabilities greater than 21 .\nTheorem 2.8. The class of languages recognized by PRA-C is closed under\nintersection, union and complement.\nProof. Let us consider languages L1 , L2 recognized by some PRA-C automata.\nBy Theorem 2.6, these languages is recognizable with probability 1 \u2212 \u03b5, and\ntherefore by Lemmas 2.5 and 2.7, union and intersection of these languages are\nalso recognizable. If a language L is recognizable by a PRA-C A, we can construct\nan automaton which recognizes a language L just by making accepting states of\nA to be rejecting, and vice versa.\n\u2293\n\u2294\nIt is natural to ask what are the languages recognized by PRA-C with probability exactly 1.\nTheorem 2.9. If a language is recognized by a PRA-C with probability 1, the\nlanguage is recognized by a permutation automaton.\nProof. Let us consider a language L and a PRA-C A, which recognizes L with\nprobability 1.\nIf a word is in L, the automaton A has to accept the word with probability\n1. Conversely, if a word is not in L, the word must be accepted with probability\n0. Therefore,\n\u2200q \u2208 Q \u2200\u03c9 \u2208 \u03a3 \u2217 either q\u03c9 \u2286 QF , or q\u03c9 \u2286 QF .\n\n(8)\n\n\fConsider a relation between the states of A defined as\nR = {(qi , qj ) | \u2200\u03c9 qi \u03c9 \u2286 QF \u21d4 qj \u03c9 \u2286 QF }. R is symmetric, reflexive\nand transitive, therefore Q can be partitioned into equivalence classes Q/R =\n{[q0 ], [qi1 ], . . . , [qik ]}. Suppose A is in a state q. Due to (8), \u2200\u03c9 \u2203n q\u03c9 \u2286 [qin ]. In\nfact, having read a symbol in the alphabet, A goes from one equivalence class\nto another with probability 1.\nHence it is possible to construct the following deterministic automaton D,\nwhich simulates A. The states are s0 , . . . , sk and sn \u03c3 = sm iff [qin ]\u03c3 \u2286 [qim ] and\nsn is an accepting state iff [qin ] \u2286 QF . Since all transition matrices of A are\ndoubly stochastic, all transition matrices of D are permutation matrices.\n\u2293\n\u2294\nTheorem 2.10. The class of languages recognized by PRA-C is closed under\ninverse homomorphisms.\nProof. Let us consider finite alphabets \u03a3, T , a homomorphism h : \u03a3 \u2212\u2192 T \u2217 , a\nlanguage L \u2286 T \u2217 and a PRA-C A = (Q, T, q0 , QF , \u03b4), which recognizes L with\ninterval (p1 , p2 ). We prove that exists an automaton B = (Q, \u03a3, q0 , QF , \u03b4 \u2032 ) which\nrecognizes the language h\u22121 (L).\nTransition function \u03b4 of A sets transition matrices V\u03c4 , where \u03c4 \u2208 T . To\ndetermine \u03b4 \u2032 , we define transition matrices V\u03c3 , \u03c3 \u2208 \u03a3. Let us define a transition\nmatrix V\u03c3k :\nV\u03c3k = V[h(\u03c3k )]m V[h(\u03c3k )]m\u22121 . . . V[h(\u03c3k )]1 ,\nwhere m = |h(\u03c3k )|. Multiplication of two doubly stochastic matrices is a doubly\nstochastic matrix, therefore B is a PRA-C. Automaton B recognizes h\u22121 (L)\nwith the same interval (p1 , p2 ).\n\u2293\n\u2294\nCorollary 2.11. The class of languages recognized by PRA-C is closed under\nword quotient.\nProof. This follows from closure under inverse homomorphisms and presence of\nend-markers #, $.\n\u2293\n\u2294\nEven if PRA-C without end-markers are considered, closure under word quotient remains true. See Appendix for details.\nLemma 2.12. If A is a doubly stochastic matrix and X - a vector, then\nmax(X) \u2265 max(AX) and min(X) \u2264 min(AX).\n\uf8eb \uf8f6\n\uf8f6\n\uf8eb\nx1\na11 a12 . . . a1n\n\uf8ec x2 \uf8f7\n\uf8ec a21 a22 . . . a2n \uf8f7\n\uf8f7\n\uf8f7\n\uf8ec\nProof. Let us consider X = \uf8ec\n\uf8ed . . . \uf8f8 and A = \uf8ed . . . . . . . . . . . . \uf8f8, where A is\nxn\nan1 an2 . . . ann\ndoubly stochastic. Let us suppose that xj = max(X). For any i, 1 \u2264 i \u2264 n,\nxj = ai1 xj + ai2 xj + . . . + ain xj \u2265 ai1 x1 + ai2 x2 + . . . + ain xn .\nTherefore xj is greater or equal than any component of AX. The second inequality is proved in the same way.\n\u2293\n\u2294\n\n\fTheorem 2.13. For every natural positive n, a language Ln = a\u22171 a\u22172 . . . a\u2217n is\nrecognizable by some PRA-C with alphabet {a1 , a2 , . . . , an }.\nProof. We construct a PRA-C with n + 1 states,\n0 being the initial state, cor\uf8eb q\uf8f6\n1\n\uf8ec 0 \uf8f7\n\uf8f7\nresponding to probability distribution vector \uf8ec\n\uf8ed . . . \uf8f8. The transition function is\n0\ndetermined by (n + 1) \u00d7 (n + 1) matrices\n\uf8eb1 1\n\uf8f6\n\uf8f6\n\uf8f6\n\uf8eb1\n\uf8eb\n1\n2 2 0 ... 0\n1 0 ... 0\n1\n1\nn ... n 0\n\uf8ec\n\uf8f7\n0\n.\n.\n.\n0\n\uf8ec2 2 1\n\uf8ec .. . . .. .. \uf8f7\n\uf8ec0 1 ... 1 \uf8f7\n1 \uf8f7\nn\uf8f7\n\uf8ec\n\uf8f7\n\uf8f7\n\uf8ec\n\uf8ec n\nVa1 = \uf8ec . . . . \uf8f7, Va2 = \uf8ec 0 0 n\u22121 . . . n\u22121 \uf8f7, . . . , Van = \uf8ec . . . . \uf8f7.\n\uf8ec .. .. .. . . .. \uf8f7\n\uf8ed 1 ... 1 0\uf8f8\n\uf8ed .. .. . . .. \uf8f8\nn\nn\n\uf8ed. . .\n. . \uf8f8\n0 n1 . . . n1\n0 ... 0 1\n1\n1\n0 0 n\u22121\n. . . n\u22121\nThe accepting states are q0 . . . qn\u22121 , the only rejecting state is qn . We prove,\nthat the automaton recognizes the language Ln .\nCase \u03c9 \u2208 Ln . Having read \u03c9 \u2208 a\u22171 . . . a\u2217k\u22121 a+\nk , the automaton is in probability\n\uf8eb 1 \uf8f6\nk\n\n\uf8ec...\uf8f7\n\uf8ec 1 \uf8f7\n\uf8ec \uf8f7\nk \uf8f7\ndistribution \uf8ec\n\uf8ec 0 \uf8f7. Therefore all \u03c9 \u2208 Ln are accepted with probability 1.\n\uf8ec \uf8f7\n\uf8ed...\uf8f8\n0\nCase \u03c9 \u2208\n/ Ln . Consider k such that \u03c9 = \u03c91 \u03c3\u03c92 , |\u03c91 | = k, \u03c91 \u2208 Ln and\n\u03c91 \u03c3 \u2208\n/ Ln . Since all one-letter words are in Ln , k > 0. Let at = [\u03c9]k and as = \u03c3.\nSo we have s < t, 1 \u2264 s \u2264 n \uf8eb\n\u2212 1, 2\uf8f6\u2264 t \u2264 n. Having read \u03c91 \u2208 a\u22171 . . . a\u2217t\u22121 a+\nt , the\n1\nt\n\n\uf8ec...\uf8f7\n\uf8ec 1 \uf8f7\n\uf8ec \uf8f7\nt \uf8f7\nautomaton is in distribution \uf8ec\n\uf8ec 0 \uf8f7. After that, having read as , the automaton is\n\uf8ec \uf8f7\n\uf8ed...\uf8f8\n0\n\uf8fc\n\uf8eb 1\n\uf8f6\uf8eb 1 \uf8f6 \uf8eb\n\uf8f6\n1\n1\n\uf8fd\n0 ... 0\nt\ns ... s\nt\n\uf8ec... ... ... ... ... ... \uf8f7\uf8ec...\uf8f7 \uf8ec\n\uf8f7\n...\ns\n\uf8ec 1\n\uf8f7\uf8ec 1 \uf8f7 \uf8ec\n\uf8f7\n\uf8fe\n1\n1\n\uf8ec\n\uf8ec\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8ec\n0 ... 0 \uf8f7\uf8ec t \uf8f7 \uf8ec\nt\ns ... s\n\uf8fc\n\uf8f7.\nin distribution \uf8ec\n=\nt\u2212s\n1\n1\n\uf8ec 0 ... 0\n\uf8ec\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8ec\nn\u2212s+1 . . . n\u2212s+1 \uf8f7 \uf8ec 0 \uf8f7\n\uf8ec\n\uf8f7\n\uf8ec t(n\u2212s+1) \uf8fd\n\uf8ed... ... ... ... ... ... \uf8f8\uf8ed...\uf8f8 \uf8ed\n...\nn\u2212s+1 \uf8f8\n\uf8fe\nt\u2212s\n1\n1\n0\n0 . . . 0 n\u2212s+1\n. . . n\u2212s+1\nt(n\u2212s+1)\nt\u2212s\nSo the word \u03c91 as is accepted with probability 1\u2212 t(n\u2212s+1) . By Lemma 2.12, since\n1\nt\u2212s\nt(n\u2212s+1) < t , reading the symbols succeeding \u03c91 as does not increase accepting\nprobability. Therefore, to find maximum accepting probability for words not in\nt\u2212s\nLn , we have to maximize 1 \u2212 t(n\u2212s+1)\n, where s < t, 1 \u2264 s \u2264 n \u2212 1, 2 \u2264 t \u2264 n.\nSolving this problem, we get t = k + 1, s = k for n = 2k, and we get t =\nk + 1, s = k or t = k + 2, s = k + 1 for n = 2k + 1. So the maximum accepting\n\n\f1\n(k+1)2 ,\n\n1\n,\n(k+1)(k+2)\n\u0012\n\nif n = 2k + 1. All in\n\u0013\nall, the automaton recognizes the language with interval 1 \u2212 ( n )2 1 +n+1 , 1 .\n\u230a 2 \u230b\n(Actually, by Theorem 2.6, Ln can be recognized with probability 1 \u2212 \u03b5).\n\u2293\n\u2294\nprobability is 1 \u2212\n\nif n = 2k, and it is 1 \u2212\n\nCorollary 2.14. Quantum finite automata with mixed states (model of Nayak,\n[N 99]) recognize Ln = a\u22171 a\u22172 . . . a\u2217n with probability 1 \u2212 \u03b5.\nProof. This comes from the fact, that matrices Va1 , Va2 , . . . , Van from the proof\nof Theorem 2.13 (as well as tensor powers of those matrices) all have unitary\nprototypes (see Definition 5.1).\n\u2293\n\u2294\nDefinition 2.15. We say that a regular language is of type (\u2217) if the following\nis true for the minimal deterministic automaton recognizing this language: Exist\nthree states q, q1 , q2 , exist words x, y such that\n1.\n2.\n3.\n4.\n5.\n\nq1 6= q2 ;\nqx = q1 , qy = q2 ;\nq1 x = q1 , q2 y = q2 ;\n\u2200t \u2208 (x, y)\u2217 \u2203t1 \u2208 (x, y)\u2217 q1 tt1 = q1 ;\n\u2200t \u2208 (x, y)\u2217 \u2203t2 \u2208 (x, y)\u2217 q2 tt2 = q2 .\n\nDefinition 2.16. We say that a regular language is of type (\u2217\u2032 ) if the following\nis true for the minimal deterministic automaton recognizing this language: Exist\nthree states q, q1 , q2 , exist words x, y such that\n1.\n2.\n3.\n4.\n\nq1 6= q2 ;\nqx = q1 , qy = q2 ;\nq1 x = q1 , q1 y = q1 ;\nq2 x = q2 , q2 y = q2 .\n\u271b\u2718\n\n\u271b\u2718\n\nq\n\nx\n\ny\n\ny\n\u271b\u2718\n\u271b\u2718\n\u2747\n\u25c6\n\u270e\n\u2704\n\u271bt \u276f\n\u261b t \u2773\n3\nq2 \u2773\nt1 \u2732 q1\nt\n22\nx\n\nq\n\n\u271a\u2719\n\n\u271a\u2719 \u271a\u2719\n\nFig. 1. Type (\u2217) construction\n\nx\nx, y\n\n\u271a\u2719\n\ny\n\n\u271b\u2718\n\u271b\u2718\n\u2704\u270e\n\u25c6\u2747\n\nx, y\nq1\nq2\n3\u271a\u2719 \u271a\u2719\n\u2773\n\u273e\n\u2718\n\nFig. 2. Type (\u2217\u2032 ) construction\n\n\fDefinition 2.17. We say that a regular language is of type (\u2217\u2032\u2032 ) if the following\nis true for the minimal deterministic automaton recognizing this language: Exist\ntwo states q1 , q2 , exist words x, y such that\n1. q1 6= q2 ;\n2. q1 x = q2 , q2 x = q2 ;\n3. q2 y = q1 .\n\u271b\u2718\nx \u2732\u271b\u2718\nx\nq1 \u271b y q2\n\u273e\n\u2718\n\u271a\u2719 \u271a\u2719\n\nFig. 3. Type (\u2217\u2032\u2032 ) construction\nType (\u2217\u2032\u2032 ) languages are exactly those languages that violate the partial order\ncondition of [BP 99].\nLemma 2.18. If A is a deterministic finite automaton with a set of states Q\nand alphabet \u03a3, then \u2200q \u2208 Q \u2200x \u2208 \u03a3 \u2217 \u2203k > 0 qxk = qx2k .\nProof. We paraphrase a result from the theory of finite semigroups. Consider\na state q and a word x. Since number of states is finite, \u2203m \u2265 0 \u2203s \u2265 1\n\u2200n qxm = qxm xsn . Take n0 , such that sn0 > m. Note that \u2200t \u2265 0 qxm+t =\n\u2293\n\u2294\nqxm+t xsn0 . We take t = sn0 \u2212 m, so qxsn0 = qxsn0 xsn0 . Take k = sn0 .\nLemma 2.19. A regular language is of type (\u2217) iff it is of type (\u2217\u2032 ) or type (\u2217\u2032\u2032 ).\nProof. 1) If a language is of type (\u2217\u2032 ), it is of type (\u2217). Obvious.\n2) If a language is of type (\u2217\u2032\u2032 ), it is of type (\u2217). Consider a language of type\n\u2032\u2032\n(\u2217 ) with states q1\u2032\u2032 , q2\u2032\u2032 and words x\u2032\u2032 , y \u2032\u2032 . To build construction of type (\u2217), we\ntake q = q1 = q1\u2032\u2032 , q2 = q2\u2032\u2032 , x = x\u2032\u2032 y \u2032\u2032 , y = x\u2032\u2032 . That forms transitions qx = q1 ,\nqy = q2 , q1 x = q1 , q1 y = q2 , q2 x = q1 , q2 y = q2 . We have satisfied all the rules\nof (\u2217).\n3) If a language is of type (\u2217), it is of type (\u2217\u2032 ) or (\u2217\u2032\u2032 ). Consider a language\nwhose minimal deterministic automaton has construction (\u2217). By Lemma 2.18,\n\u2203t\u2203b q1 y b = qt and qt y b = qt ;\n\u2203u\u2203c q2 xc = qu and qu xc = qu .\nIf q1 6= qt , by the 4th rule of (\u2217), \u2203z qt z = q1 . Therefore the language is of type\n(\u2217\u2032\u2032 ). If q2 6= qu , by the 5th rule of (\u2217), \u2203z qu z = q2 , and the language is of type\n(\u2217\u2032\u2032 ).\nIf q1 = qt and q2 = qu , we have qxc = q1 , qy b = q2 , q1 xc = q1 y b = q1 , q2 xc =\nq2 y b = q2 . We get the construction (\u2217\u2032 ) if we take x\u2032 = xc , y \u2032 = y b .\n\u2293\n\u2294\nWe are going to prove that every language of type (\u2217) is not recognizable by\nany PRA-C. For this purpose, we recall several definitions from the theory of\nfinite Markov chains ([KS 76], etc.)\n\n\fA Markov chain with n states can be determined by an n \u00d7 n stochastic\nmatrix A, i.e., matrix, where the sum of elements of every column in the matrix\nis 1. If Ai,j = p > 0, it means that a state qi is accessible from a state qj with\na positive probability p in one step. Generally speaking, the matrix depends on\nthe numbering of the states; if the states are renumbered, the matrix changes,\nas its rows and columns also need to be renumbered.\nDefinition 2.20. A state qj is accessible from qi (denoted qi \u2192 qj ) if there is\na positive probability to get from qi to qj (possibly in several steps).\nDefinition 2.21. States qi and qj communicate (denoted qi \u2194 qj ) if qi \u2192 qj\nand qj \u2192 qi .\nDefinition 2.22. A state q is called ergodic if \u2200i q \u2192 qi \u21d2 qi \u2192 q. Otherwise\nthe state is called transient.\nDefinition 2.23. A Markov chain without transient states is called irreducible\nif for all qi , qj qi \u2194 qj . Otherwise the chain without transient states is called\nreducible.\nDefinition 2.24. The period of an ergodic state qi \u2208 Q of a Markov chain with\na matrix A is defined as d(qi ) = gcd{n > 0 | (An )i,i > 0}.\nDefinition 2.25. An ergodic state qi is called aperiodic if d(qi ) = 1. Otherwise\nthe ergodic state is called periodic.\nDefinition 2.26. A Markov chain without transient states is called aperiodic if\nall its states are aperiodic. Otherwise the chain without transient states is called\nperiodic.\nDefinition 2.27. A probability distribution X of a Markov chain with a matrix\nA is called stationary, if AX = X.\nDefinition 2.28. A Markov chain is called doubly stochastic, if its transition\nmatrix is a doubly stochastic matrix.\nWe recall the following theorem from the theory of finite Markov chains:\nTheorem 2.29. If a Markov chain with a matrix A is irreducible and aperiodic,\nthen\na) it has a unique stationary distribution Z;\nb) lim An = (Z, . . . , Z);\nn\u2192\u221e\n\nc) \u2200X lim An X = Z.\nn\u2192\u221e\n\nCorollary 2.30. If a doubly stochastic Markov chain with an m \u00d7 m matrix A\nis irreducible and\n\uf8eb 1aperiodic,\n\uf8f6\n1\nm ... m\na) lim An = \uf8ed . . . . . . . . . \uf8f8;\nn\u2192\u221e\n1\n1\n.. m\nm .\uf8eb\n\uf8f6\n1\nm\n\nb) \u2200X lim An X = \uf8ed . . . \uf8f8.\nn\u2192\u221e\n\n1\nm\n\n\fProof. By Theorem 2.29.\n\n\u2293\n\u2294\n\nLemma 2.31. If M is a doubly stochastic Markov chain with a matrix A, then\n\u2200q q \u2192 q.\nProof. Assume existence of q0 such that q0 is not accessible from itself. Let\nQq0 = {qi | q0 \u2192 qi } = {q1 , . . . , qk }. Qq0 is not empty set. Consider those rows\nand columns of A, which are indexed by states in Qq0 . These rows and columns\nform a submatrix A\u2032 . Each column j of A\u2032 must include all non-zero elements\nof the corresponding column of A as those states are accessible from the state\nk\nP\nA\u2032i,j = 1 and\nqj , hence also from q0 and are in Qq0 . Therefore \u2200j, 1 \u2264 j \u2264 k,\ni=1\nP\nA\u2032i,j = k. On the other hand, since q0 \u2208\n/ Qq0 , a row of A\u2032 indexed by a\n\n1\u2264i,j\u2264k\n\nstate accessible in one step from q0 does not include all nonzero elements. Since\nk\nP\nP\nA\u2032i,j < k. This is a\nA\u2032i,j < 1 and\nA is doubly stochastic, \u2203i, 1 \u2264 i \u2264 k,\nj=1\n\n1\u2264i,j\u2264k\n\ncontradiction.\n\n\u2293\n\u2294\n\nCorollary 2.32. Suppose A is a doubly stochastic matrix. Then exists k > 0,\nsuch that \u2200i (Ak )i,i > 0.\nProof. Consider an m \u00d7 m doubly stochastic matrix A. By Lemma 2.31, \u2200i\nm\nQ\n\u2203ni > 0 (Ani )i,i > 0. Take n =\nns . For every i, (An )i,i > 0.\n\u2293\n\u2294\ns=1\n\nLemma 2.33. If M is a doubly stochastic Markov chain with a matrix A, then\n\u2200qa , qb Ab,a > 0 \u21d2 qb \u2192 qa .\n\nProof. Ab,a > 0 means that qb is accessible from qa in one step. We have to\nprove, that qb \u2192 qa . Assume from the contrary, that qa is not accessible from\nqb . Let Qqb = {qi | qb \u2192 qi } = {q1 , q2 , . . . , qk }. By Lemma 2.31, qb \u2208 Qqb . As\nin proof of Lemma 2.31, consider a matrix A\u2032 , which is a submatrix of A and\nwhose rows and columns are indexed by states in Qqb . Each column j has to\ninclude all nonzero elements of the corresponding column of A. Therefore \u2200j,\nk\nP\nP\nA\u2032i,j = k. On the other hand, Ab,a > 0\nA\u2032i,j = 1 and\n1 \u2264 j \u2264 k,\ni=1\n\n1\u2264i,j\u2264k\n\nand qa \u2208\n/ Qqb , therefore a row of A\u2032 indexed by qb does not include all nonzero\nk\nP\nP\nA\u2032i,j < k. This is\nA\u2032b,j < 1 and\nelements. Since A is doubly stochastic,\nj=1\n\na contradiction.\n\n1\u2264i,j\u2264k\n\n\u2293\n\u2294\n\nCorollary 2.34. If M is a doubly stochastic Markov chain and qa \u2192 qb , then\nqa \u2194 qb .\nProof. If qa \u2192 qb then exists a sequence qi1 , qi2 , . . . , qik , such that Ai1 ,a >\n0, Ai2 ,i1 > 0, . . . , Aik ,ik\u22121 > 0, Ab,ik > 0. By Lemma 2.33, we get qb \u2192 qik ,\n\u2293\n\u2294\nqik \u2192 qik\u22121 , . . ., qi2 \u2192 qi1 , qi1 \u2192 qa . Therefore qb \u2192 qa .\n\n\fBy Corollary 2.34, every doubly stochastic Markov chain does not have transient states, so it is either periodic or aperiodic, either reducible or irreducible.\nLemma 2.35. If a regular language is of type (\u2217\u2032 ), it is not recognizable by any\nPRA-C.\nProof. Assume from the contrary, that A is a PRA-C automaton which recognizes a language L \u2282 \u03a3 \u2217 of type (\u2217\u2032 ).\nSince L is of type (\u2217\u2032 ), it is recognized by a deterministic automaton D which\nhas three states q, q1 , q2 such that q1 6= q2 , qx = q1 , qy = q2 , q1 x = q1 , q1 y = q1 ,\nq2 x = q2 , q2 y = q2 , where x, y \u2208 \u03a3 \u2217 . Furthermore, exists \u03c9 \u2208 \u03a3 \u2217 such that\nq0 \u03c9 = q, where q0 is an initial state of D, and exists a word z \u2208 \u03a3 \u2217 , such that\nq1 z = qacc if and only if q2 z = qrej , where qacc is an accepting state and qrej is\na rejecting state of D. Without loss of generality we assume that q1 z = qacc and\nq2 z = qrej .\nThe transition function of the automaton A is determined by doubly stochastic matrices V\u03c31 , . . . , V\u03c3n . The words from the construction (\u2217\u2032 ) are x = \u03c3i1 . . . \u03c3ik\nand y = \u03c3j1 . . . \u03c3js . The transitions induced by words x and y are determined\nby doubly stochastic matrices X = V\u03c3ik. . . V\u03c3i1 and Y = V\u03c3js. . . V\u03c3j1 . Similarly,\nthe transitions induced by words \u03c9 and z are determined by doubly stochastic\nmatrices W and Z. By Corollary 2.32, exists K > 0, such that\n\u2200i (X K )i,i > 0 and (Y K )i,i > 0.\n\n(9)\n\nConsider a relation between the states of the automaton defined as R =\n(xK ,y K )*\n{(qi , qj ) | qi \u2212\u2192 qj }. By (9), this relation is reflexive.\n\u03be\n\nSuppose exists a word \u03be = \u03be1 \u03be2 . . . \u03bek , \u03bes \u2208 {xK , y K }, such that q \u2212\u2192 q \u2032 . This\n\u03be2\n\n\u03be1\n\n\u03bek\n\nmeans that q \u2212\u2192 qi1 , qi1 \u2212\u2192 qi2 ,. . ., qik\u22121 \u2212\u2192 q \u2032 . By Corollary 2.34, since both\nX K and Y K are doubly stochastic, \u2203\u03bek\u2032 . . . \u03be1\u2032 , \u03bes\u2032 \u2208 {(xK )\u2217 , (y K )\u2217 }, such that\n\u03be\u2032\n\n\u03be\u2032\n\n\u03be\u2032\n\n\u03be\u2032\n\n1\n2\nk\nq, therefore q \u2032 \u2212\u2192 q, where \u03be \u2032 \u2208 (xK , y K )\u2217 .\nqi1 , qi1 \u2212\u2192\nq \u2032 \u2212\u2192\nqik\u22121 ,. . ., qi2 \u2212\u2192\nSo the relation R is symmetric.\nSurely R is transitive. Therefore all states of A may be partitioned into\nequivalence classes [q0 ], [qi1 ], . . . , [qin ]. Let us renumber the states of A in such\na way, that states from one equivalence class have consecutive numbers. First\ncome the states in [q0 ], then in [qi1 ], etc.\nConsider the word xK y K . The transition induced by this word is determined\nby a doubly stochastic matrix C = Y K X K . We prove the following proposition.\nStates qa and qb are in one equivalence class if and only if qa \u2192 qb with matrix\nC. Suppose qa \u2192 qb . Then (qa , qb ) \u2208 R, and qa , qb are in one equivalence class.\nSuppose qa , qb are in one equivalence class. Then\n\n\u03be1\n\n\u03be2\n\n\u03bek\n\nqa \u2212\u2192 qi1 , qi1 \u2212\u2192 qi2 , . . . , qik\u22121 \u2212\u2192 qb , where \u03bes \u2208 {xK , y K }.\nx\n\nK\n\ny\n\nK\n\nx\n\nK\n\nK\n\nx y\n\n(10)\n\nK\n\nBy (9), qi \u2212\u2192 qi and qj \u2212\u2192 qj . Therefore, if qi \u2212\u2192 qj , then qi \u2212\u2192 qj , and\ny\n\nK\n\nK\n\nx y\n\nK\n\nagain, if qi \u2212\u2192 qj , then qi \u2212\u2192 qj . That transforms (10) to\nqa\n\n(xK y K )t\n\n\u2212\u2192\n\nqb , where t > 0.\n\n(11)\n\n\fWe have proved the proposition.\nBy the proved proposition, due to the renumbering of states, matrix C\nis a block diagonal matrix, where each block corresponds to an equivalence\nclass of the relation R. Let us identify these blocks as C0 , C1 , . . . , Cn . By (9),\na Markov chain with matrix C is aperiodic. Therefore each block Cr corresponds to an aperiodic irreducible doubly stochastic Markov chain with states\n[qir ]. By Corollary 2.30, lim C m = J, J is a block diagonal matrix, where\nm\u2192\u221e\n\n(y K )\u2217\n\nfor each (p \u00d7 p) block Cr (Cr )i,j = 1p . Relation qi \u2212\u2192 qj is a subrelation\nof R, therefore Y K is a block diagonal matrix with the same block ordering\nand sizes as C and J. (This does not eliminate possibility that some block\nof Y K is constituted of smaller blocks, however.) Therefore JY K = J, and\nlim Z(Y K X K )m W = lim Z(Y K X K )m Y K W = ZJW . So\nm\u2192\u221e\n\nm\u2192\u221e\n\n\u2200\u03b5 > 0 \u2203m\n\n\u0010\n\n\u0011\nZ(Y K X K )m W \u2212 Z(Y K X K )m Y K W Q0 < \u03b5.\n\n(12)\n\nHowever, by construction (\u2217\u2032 ), \u2200k \u2200m \u03c9(xk y k )m z \u2208 L and \u03c9y k (xk y k )m z \u2208\n/ L.\nThis requires existence of \u03b5 > 0, such that\n\u0010\n\u0011\n(13)\n\u2200m\nZ(Y K X K )m W \u2212 Z(Y K X K )m Y K W Q0 > \u03b5.\nThis is a contradiction.\n\n\u2293\n\u2294\n\nLemma 2.36. If a regular language is of type (\u2217\u2032\u2032 ), it is not recognizable by any\nPRA-C.\nProof. Proof is nearly identical to that of Lemma 2.35. Consider a PRA-C\nwhich recognizes the language L of type (\u2217\u2032\u2032 ). We prove that for words x, y\nexists constant K, such that for every \u03b5 exists m, such that for two words\n\u03be1 = \u03c9(xK (xy)K )m z and \u03be2 = \u03c9(xK (xy)K )m xK z, |p\u03be1 \u2212 p\u03be2 | < \u03b5. We can\nchoose z, such that \u03be1 \u2208 L iff \u03be2 \u2208\n/ L.\n\u2293\n\u2294\nTheorem 2.37. If a regular language is of type (\u2217), it is not recognizable by any\nPRA-C.\nProof. By Lemmas 2.19, 2.35, 2.36.\n\n\u2293\n\u2294\n\nWe proved (Lemma 2.19) that the construction of type (\u2217) is a generalization\nthe construction proposed by [BP 99]. Also it can be easily noticed, that the\ntype (\u2217) construction is a generalization of construction proposed by [AKV 00].\n(Constructions of [BP 99] and [AKV 00] characterize languages, not recognized\nby measure-many quantum finite automata of [KW 97].)\nCorollary 2.38. Languages (a,b)*a and a(a,b)* are not recognized by PRA-C.\nProof. Both languages are of type (\u2217).\n\n\u2293\n\u2294\n\n\fCorollary 2.39. Class of languages recognizable by PRA-C is not closed under\nhomomorphisms.\nProof. Consider a homomorphism a \u2192 a, b \u2192 b, c \u2192 a. Similarly as in Theorem\n2.13, the language (a,b)*cc* is recognizable by a PRA-C. (Take n = 2, Va = Va1 ,\nVb = Va1 , Vc = Va2 from Theorem 2.13, QF = {q1 }) However, by Corollary 2.38\nthe language (a,b)*aa*=(a,b)*a is not recognizable.\n\u2293\n\u2294\n\n3\n\n1-way Probabilistic Reversible DH-Automata\n\nDefinition 3.1. The definition differs from one for PRA-C (Definition 2.1) by\nthe following: languages are recognized according to Definition 1.4.\nIt is easy to see that the class of languages recognized by PRA-C is a proper\nsubclass of languages recognized by PRA-DH. For example, the language a(a,b)*\nis recognizable by PRA-DH. However, the following theorem holds:\nTheorem 3.2. Language (a,b)*a is not recognized by PRA-DH.\nProof. Assume from the contrary that such automaton exists. While reading\nany sequence of a and b, this automaton can halt only with some probability p\nstrictly less then 1, so accepting and rejecting probabilities may differ only by\n1-p, because any word belonging to the language is not dependent on any prefix.\nTherefore for each \u03b5 > 0 we can find that after reading of a prefix of certain\nlength, the total probability to halt while continue reading the word is less then \u03b5.\nIn this case we can apply similar techniques as in the proof of Lemma 2.35, such\nthat for words x, y exists constant K, such that for every \u03b5 exists s, such that for\n\u2294\ntwo words \u03be1 = \u03c9(xK (xy)K )s z and \u03be2 = \u03c9(xK (xy)K )s xK z, |p\u03be1 \u2212 p\u03be2 | < \u03b5. \u2293\n\n4\n\nAlternative Approach to Finite Reversible Automata\nand 1.5-way Probabilistic Reversible Automata\n\nLet us consider automaton A\u2032 = (Q, \u03a3, q0 , QF , \u03b4 \u2032 ) that can be obtained from a\nprobabilistic automaton A = (Q, \u03a3, q0 , QF , \u03b4) by specifying\n\u03b4 \u2032 (q, \u03c3, q \u2032 ) = \u03b4(q \u2032 , \u03c3, q) for all q \u2032 , \u03c3 and q.\nIf A\u2032 is a valid probabilistic automaton then we can call A and A\u2032 probabilistic\nreversible automata.\nDefinition 4.1. An automaton of some type is called weakly reversible if the\nreverse of its transition function corresponds to the transition function of a valid\nautomaton of the same type.\nNote: in case of deterministic automaton where \u03b4 : Q \u00d7 \u0393 \u00d7 Q \u2212\u2192 {0, 1} this\nproperty means that A' is still deterministic automaton, not nondeterministic.\nIn case of one-way automata it is easy to check that this definition is equivalent to the one in Section 2.\nWe give an example that illustrates that in case of 1.5-way automata these\ndefinitions are different.\n\n\fDefinition 4.2. 1.5-way probabilistic weakly reversible C-automaton\nA = (Q, \u03a3, q0 , QF , \u03b4) is specified by Q, \u03a3, q0 , QF defined as in 1-way PRA-C\nDefinition 2.1, and a transition function\n\u03b4 : Q \u00d7 \u0393 \u00d7 Q \u00d7 D \u2212\u2192 IR[0,1] ,\nwhere \u0393 defined as in 1-way PRA-C definition and D = {0, 1} denotes whether\nautomaton stays on the same position or moves one letter ahead on the input\ntape. Furthermore, transition function satisfies the following requirements:\n\u2200(q1 , \u03c31 ) \u2208 Q \u00d7 \u0393\n\nX\n\n\u03b4(q1 , \u03c31 , q, d) = 1\n\n(14)\n\n\u03b4(q, \u03c31 , q1 , d) = 1\n\n(15)\n\nq\u2208Q,d\u2208D\n\n\u2200(q1 , \u03c31 ) \u2208 Q \u00d7 \u0393\n\nX\n\nq\u2208Q,d\u2208D\n\nDefinition 4.3. 1.5-way probabilistic reversible C-automaton\nA = (Q, \u03a3, q0 , QF , \u03b4) is specified by Q, \u03a3, q0 , QF defined as in 1-way PRA-C\nDefinition 2.1, and a transition function\n\u03b4 : Q \u00d7 \u0393 \u00d7 Q \u00d7 D \u2212\u2192 IR[0,1] ,\nwhere \u0393 defined as in 1-way PRA-C definition and D = {0, 1} denotes whether\nautomaton stays on the same position or moves one letter ahead on the input\ntape. Furthermore, transition function satisfies the following requirements:\n\u2200(q1 , \u03c31 ) \u2208 Q \u00d7 \u0393\n\nX\n\n\u03b4(q1 , \u03c31 , q, d) = 1\n\n(16)\n\nq\u2208Q,d\u2208D\n\n\u2200(q1 , \u03c31 , \u03c32 ) \u2208 Q \u00d7 \u0393 2\n\nX\n\nq\u2208Q\n\n\u03b4(q, \u03c31 , q1 , 0) +\n\nX\n\n\u03b4(q, \u03c32 , q1 , 1) = 1 (17)\n\nq\u2208Q,\u03c3\u2208\u0393\n\nTheorem 4.4. Language (a,b)*a is recognizable by 1.5-way weakly reversible\nPRA-C.\nProof. The Q = {q0 , q1 }, QF = {q1 }, \u03b4 is defined as follows\n\u03b4(q0 , a, q0 , 0) = 12 \u03b4(q0 , a, q1 , 1) = 12 \u03b4(q1 , a, q0 , 0) = 12 \u03b4(q1 , a, q1 , 1) = 21\n\u03b4(q0 , b, q0 , 1) = 12 \u03b4(q0 , b, q1 , 0) = 12 \u03b4(q1 , b, q0 , 1) = 21 \u03b4(q1 , b, q1 , 0) = 12\n\u03b4(q0 , $, q0 , 1) = 1 \u03b4(q1 , $, q1 , 1) = 1\nIt is easy to check that such automaton moves ahead according to the transition of the following deterministic automaton\n\u03b4(q0 , a, q1 , 1) = 1 \u03b4(q1 , a, q1 , 1) = 1\n\u03b4(q0 , b, q0 , 1) = 1 \u03b4(q1 , b, q0 , 1) = 1\n\u03b4(q0 , $, q0 , 1) = 1 \u03b4(q1 , $, q1 , 1) = 1\nSo the probability of wrong answer is 0. The probability to be at the m-th\nposition of the input tape after n steps of calculation for m \u2264 n is Cnm . Therefore\nit is necessary no more than O(n \u2217 log(p)) steps to reach the end of the word of\n\u2293\n\u2294\nlength n (and so obtain correct answer) with probability 1 \u2212 1p .\n\n\f5\n\nA Classification of Reversible Automata\n\nWe propose the following classification for finite 1-way reversible automata:\nC-automata\nDH-automata\nDeterministic\nPermutation Automata\nReversible Finite Automata\nAutomata\n[HS 66,T 68] (DRA-C)\n[AF 98] (DRA-DH)\nQuantum\nMeasure-Once Quantum\nMeasure-Many Quantum\nAutomata with Finite Automata [MC 97]\nFinite Automata [KW 97]\nPure States\n(QRA-P-C)\n(QRA-P-DH)\nProbabilistic\nProbabilistic Reversible\nProbabilistic Reversible\nAutomata\nC-automata (PRA-C)\nDH-automata (PRA-DH)\nQuantum Finite not considered yet\nEnhanced Quantum\nAutomata with (QRA-M-C)\nFinite Automata [N 99]\nMixed States\n(QRA-M-DH)\nLanguage class problems are solved for DRA-C, DRA-DH, QRA-P-C, for\nthe rest types they are still open. Every type of DH-automata may simulate the\ncorresponding type of C-automata.\nGenerally, language classes recognized by C-automata are closed under\nboolean operations (though this is open for QRA-M-C), while DH-automata are\nnot (though this is open for QRA-M-DH and possibly for PRA-DH).\nDefinition 5.1. We say that a unitary matrix U is a prototype for a doubly\nstochastic matrix S, if \u2200i, j |Ui,j |2 = Si,j .\nNot every\uf8eb\ndoubly stochastic\nmatrix has a unitary prototype. Such matrix is,\n\uf8f6\n1 1\n0\n2 2\nfor example, \uf8ed 21 0 12 \uf8f8.\n0 21 12\nIn Introduction, we demonstrated some relation between PRA-C and QRAM-DH (and hence, QRA-M-C). However, due to the example above, we do not\nknow exactly, whether every PRA-C can be simulated by QRA-M-C, or whether\nevery PRA-DH can be simulated by QRA-M-DH.\nTheorem 5.2. If all matrices of a PRA-C have unitary prototypes, then the\nPRA-C may be simulated by a QRA-M-C and by a QRA-M-DH.\nProof. Trivial.\n\n\u2293\n\u2294\n\nTheorem 5.3. If all matrices of a PRA-DH have unitary prototypes, then the\nPRA-DH may be simulated by a QRA-M-DH.\nProof. Trivial.\n\n\u2293\n\u2294\n\nReferences\nABFK 99. A. Ambainis, R. Bonner, R. Freivalds, A. \u0136ikusts. Probabilities to Accept\nLanguages by Quantum Finite Automata. COCOON 1999, Lecture Notes in Computer Science, 1999, Vol. 1627, pp. 174-183.\nhttp://arxiv.org/abs/quant-ph/9904066\n\n\fAF 98. A. Ambainis, R. Freivalds. 1-Way Quantum Finite Automata: Strengths,\nWeaknesses and Generalizations. Proc. 39th FOCS, 1998, pp. 332-341.\nhttp://arxiv.org/abs/quant-ph/9802062\nAKV 00. A. Ambainis, A. \u0136ikusts, M. Valdats. On the Class of Languages Recognizable by 1-Way Quantum Finite Automata. STACS 2001, Lecture Notes in Computer Science, 2001, Vol. 2010, pp. 75-86.\nhttp://arxiv.org/abs/quant-ph/0009004\nBP 99. A. Brodsky, N. Pippenger. Characterizations of 1-Way Quantum Finite Automata.\nhttp://arxiv.org/abs/quant-ph/9903014\nGS 97. C. M. Grinstead, J. L. Snell. Introduction to Probability. American Mathematical Society, 1997.\nhttp://www.dartmouth.edu/~chance/teaching aids/articles.html\nHW 79. G. H. Hardy, E. M. Wright. An Introduction to the Theory of Numbers. Fifth\nEdition. Oxford University Press, 1979.\nHS 66. J. Hartmanis, R. E. Stearns. Algebraic Structure Theory of Sequential Machines. Prentice Hall, 1966.\nKS 76. J. G. Kemeny, J. L. Snell. Finite Markov Chains. Springer Verlag, 1976.\nKW 97. A. Kondacs, J. Watrous. On The Power of Quantum Finite State Automata.\nProc. 38th FOCS, 1997, pp. 66-75.\nMC 97. C. Moore, J. P. Crutchfield. Quantum Automata and Quantum Grammars.\nTheoretical Computer Science, 2000, Vol. 237(1-2), pp. 275-306.\nhttp://arxiv.org/abs/quant-ph/9707031\nN 99. A. Nayak. Optimal Lower Bounds for Quantum Automata and Random Access\nCodes. Proc. 40th FOCS, 1999, pp. 369-377.\nhttp://arxiv.org/abs/quant-ph/9904093\nR 63. M. O. Rabin. Probabilistic Automata. Information and Control, 1963, Vol. 6(3),\npp. 230-245.\nT 68. G. Thierrin. Permutation Automata. Mathematical Systems Theory, 1968, Vol.\n2(1), pp. 83-90.\n\nA\n\nEnd-Marker Theorems for PRA-C Automata\n\nWe denote a PRA-C with both end-markers as #,$-PRA-C. We denote a PRA-C\nwith left end-marker only as #-PRA-C.\nTheorem A.1. Let A be a #,$-PRA-C, which recognizes a language L. There\nexists a #-PRA-C which recognizes the same language.\nProof. Suppose A = (Q, \u03a3, q0 , QF , \u03b4), where |Q| = n. A recognizes L with interval (p1 , p2 ). We construct the following automaton A\u2032 = (Q\u2032 , \u03a3, q0,0 , Q\u2032F , \u03b4 \u2032 ) with\nmn states. Informally, A\u2032 equiprobably simulates m copies of the automaton A.\nQ\u2032 = {q0,0 , . . . , q0,m\u22121 , q1,0\u001a\n, . . . , q1,m\u22121 , . . . , qn\u22121,0 , . . . , qn\u22121,m\u22121 }.\n\u03b4(qi , \u03c3, qj ), if k = l\nIf \u03c3 6= #, \u03b4 \u2032 (qi,k , \u03c3, qj,l ) =\n0, if k 6= l.\n1\n\u2032\nOtherwise, \u03b4 (q0,0 , #, qj,l ) = m \u03b4(q0 , #, qj ), and if qi,k 6= q0,0 , \u03b4 \u2032 (qi,k , #, q) =\n1\u2212\u03b4 \u2032 (q0,0 ,#,q)\n.\nmn\u22121\n\nFunction \u03b4 \u2032 satisfies the requirements (1) and (2) of Definition 2.1.\n\n\fWe define Q\u2032F as follows. A state qi,k \u2208 Q\u2032F if and only if 0 \u2264 k < mp(qi ),\nP\ndef\n\u03b4(qi , $, q).\nwhere p(qi ) =\nq\u2208QF\n\nSuppose #\u03c9$ is an input word. Having read #\u03c9, A is in superposition\nn\u22121\nn\u22121\nP \u03c9\nP \u03c9\nai p(qi ).\nai qi . After A has read $, #\u03c9$ is accepted with probability p\u03c9 =\ni=0\nm\u22121\nP n\u22121\nP\n\ni=0\n\nOn the other hand, having read #\u03c9, A\u2032 is in superposition\n\nSo the input word #\u03c9 is accepted with probability p\u2032\u03c9 =\nConsider \u03c9 \u2208 L. Then\n\np\u2032\u03c9\n\n=\n\n1\nm\n\nConsider \u03be \u2208\n/ L. Then p\u2032\u03be =\n\nn\u22121\nP\n\na\u03c9\ni \u2308mp(qi )\u2309\n\n\u2265\n\ni=0\nn\u22121\n1 P \u03be\nai \u2308mp(qi )\u2309\nm\ni=0\n\nn\u22121\nP\n\n1\nm\n\n1\nm\n\nn\u22121\nP\ni=0\n\n<\n\ni=0\n\na\u03c9\ni qi,j .\n\na\u03c9\ni \u2308mp(qi )\u2309.\n\na\u03c9\ni p(qi )\n\ni=0\nn\u22121\nP\n\nj=0 i=0\n\n= p\u03c9 \u2265 p2 .\n\na\u03bei p(qi ) +\n\n1\n1\n\u2264 p1 + m\n.\np\u03be + m\n\u2032\nTherefore A recognizes L with bounded error, provided m >\n\n1\nm\n\nn\u22121\nP\ni=0\n\n1\np2 \u2212p1 .\n\na\u03bei =\n\u2293\n\u2294\n\nNow we are going to prove that PRA-C without end-markers recognize the\nsame languages as #-PRA-C automata.\nIf A is a #-PRA-C, then, having read the left end-marker #, the automaton simulates some other automata A0 , A1 , . . . , Am\u22121 with positive probabilities\np0 , . . . , pm\u22121 , respectively. A0 , A1 , . . . , Am\u22121 are automata without end-markers.\nBy pi,\u03c9 , 0 \u2264 i < m, we denote the probability that the automaton Ai accepts\nthe word \u03c9.\nWe prove the following lemma first.\nLemma A.2. Suppose A\u2032 is a #-PRA-C which recognizes a language L with\ninterval (a1 , a2 ). Then for every \u03b5, 0 < \u03b5 < 1, exists a #-PRA-C A which\nrecognizes L with interval (a1 , a2 ), such that\na) if \u03c9 \u2208 L, p0,\u03c9 + p1,\u03c9 + . . . + pn\u22121,\u03c9 >\nb) if \u03c9 \u2208\n/ L, p0,\u03c9 + p1,\u03c9 + . . . + pn\u22121,\u03c9 <\n\na2 n\n1+\u03b5\na1 n\n1\u2212\u03b5 .\n\nHere n is the number of automata without end-markers, being simulated by A,\nand pi,\u03c9 is the probability that i-th simulated automaton Ai accepts \u03c9.\nProof. Suppose a #-PRA-C A\u2032 recognizes a language L with interval (a1 , a2 ).\nHaving read the symbol #, A\u2032 simulates automata A\u20320 , . . . , A\u2032m\u22121 with probabilities p\u20320 , . . . , p\u2032m\u22121 , respectively. We choose \u03b5, 0 < \u03b5 < 1.\nBy Dirichlet's principle ([HW 79], p. 170), \u2200\u03c6 > 0 exists n \u2208 IN+ such that\n\u2200i p\u2032i n differs from some positive\ninteger by less than \u03c6.\n\u0001\n1\nLet 0 < \u03c6 < min m\n, \u03b5 . Let gi be the nearest integer of p\u2032i n. So |p\u2032i n\u2212 gi | < \u03c6\nm\u22121\nP\np\u2032\n\u03c6\n\u2032\nand gii \u2212 n1 < ng\ngi < \u03c6m < 1.\n\u2264 \u03c6\nn . Since |pi n \u2212 gi | < \u03c6, we have n \u2212\ni\ni=0\n\n+\n\nTherefore, since gi \u2208 IN ,\n\nm\u22121\nP\ni=0\n\ngi = n.\n\n\fNow we construct the #-PRA-C A, which satisfies the properties expressed\nin Lemma A.2. For every i, we make gi copies of A\u2032i . Having read #, for every\np\u2032\ni A simulates each copy of A\u2032i with probability gii . The construction of V# is\nequivalent to that used in the proof of Lemma 2.7. Therefore A is characterized\nby doubly stochastic matrices. A recognizes L with the same interval as A\u2032 , i.e.,\n(a1 , a2 ).\nUsing new notations, A simulates n automata A0 , A1 , . . . , An\u22121 with probabilities p0 , p1 , . . . , pn\u22121 , respectively. Note that \u2200i pi \u2212 n1 < \u03c6\nn . Let pi,\u03c9 be the\nprobability that Ai accepts the word \u03c9.\nConsider \u03c9 \u2208 L. We have p0 p0,\u03c9 + p1 p1,\u03c9 + . . . + pn\u22121 pn\u22121,\u03c9 \u2265 a2 . Since\n1+\u03c6\npi < 1+\u03c6\nn , n (p0,\u03c9 + p1,\u03c9 + . . . + pn\u22121,\u03c9 ) > a2 . Hence\np0,\u03c9 + p1,\u03c9 + . . . + pn\u22121,\u03c9 >\n\na2 n\na2 n\n>\n.\n1+\u03c6\n1+\u03b5\n\nConsider \u03be \u2208\n/ L. We have p0 p0,\u03be + p1 p1,\u03be + . . . + pn\u22121 pn\u22121,\u03be \u2264 a1 . Since\n1\u2212\u03c6\npi > 1\u2212\u03c6\n,\n(p\n0,\u03be + p1,\u03be + . . . + pn\u22121,\u03be ) < a1 . Hence\nn\nn\np0,\u03be + p1,\u03be + . . . + pn\u22121,\u03be <\n\na1 n\na1 n\n<\n.\n1\u2212\u03c6\n1\u2212\u03b5\n\u2293\n\u2294\n\nTheorem A.3. Let A be a #-PRA-C, which recognizes a language L. There\nexists a PRA-C without end-markers, which recognizes the same language.\nProof. Consider a #-PRA-C which recognizes a language L with interval (a1 , a2 ).\n\u2032\n1\nUsing Lemma A.2, we choose \u03b5, 0 < \u03b5 < aa22 \u2212a\n+a1 , and construct an automaton A\nwhich recognizes L with interval (a1 , a2 ), with the following properties.\nHaving read #, A\u2032 simulates A\u20320 , . . . , A\u2032m\u22121 with probabilities p\u20320 , . . . , p\u2032m\u22121 ,\nrespectively. A\u20320 , . . . , A\u2032m\u22121 are automata without end-markers. A\u2032i accepts \u03c9 with\n2m\nprobability p\u2032i,\u03c9 . If \u03c9 \u2208 L, p\u20320,\u03c9 + p\u20321,\u03c9 + . . . + p\u2032m\u22121,\u03c9 > a1+\u03b5\n. Otherwise, if \u03c9 \u2208\n/ L,\na1 m\n\u2032\n\u2032\n\u2032\np0,\u03c9 + p1,\u03c9 + . . . + pm\u22121,\u03c9 < 1\u2212\u03b5 .\nThat also implies that for every n = km, k \u2208 IN+ , we are able to construct\na #-PRA-C A which recognizes L with interval (a1 , a2 ), such that\na) if \u03c9 \u2208 L, p0,\u03c9 + p1,\u03c9 + . . . + pn\u22121,\u03c9 >\nb) if \u03c9 \u2208\n/ L, p0,\u03c9 + p1,\u03c9 + . . . + pn\u22121,\u03c9 <\n\na2 n\n1+\u03b5 ;\na1 n\n1\u2212\u03b5 .\n\nA simulates A0 , . . . , An\u22121 . Let us consider the system Fn = (A0 , . . . , An\u22121 ).\na1\na2\n1\nLet \u03b4 = 12 (a1 + a2 ). Since \u03b5 < aa22 \u2212a\n+a1 , 1+\u03b5 > \u03b4 and 1\u2212\u03b5 < \u03b4. As in the proof\nof Theorem 2.6, we define that the system accepts a word, if more than n\u03b4\nautomata in the system accept the word.\na2\na1\nLet us take \u03b70 , such that 0 < \u03b70 < 1+\u03b5\n\u2212 \u03b4 < \u03b4 \u2212 1\u2212\u03b5\n.\nn\u22121\nP\na2 n\n> n\u03b4. As a result of reading \u03c9,\npi,\u03c9 > 1+\u03b5\nConsider \u03c9 \u2208 L. We have that\ni=0\n\n\u03bc\u03c9\nn automata in the system accept the word, and the rest reject it. The system\n\n\f\u03bc\u03c9\nn\nn\n\nhas accepted the word, if\n\n> \u03b4. Since 0 < \u03b70 <\n\nhave\nP\nIf we look on\nV (X) =\n\n1\nn2\n\ning:\nP\n\nn\u22121\nP\n\n\u03bc\u03c9\nn\nn\n\n\u001a\n\n\u03bc\u03c9\nn\n>\u03b4\nn\n\n\u001b\n\n\u2265P\n\n(\n\na2\n1+\u03b5\n\n\u2212\u03b4 <\n\nn\u22121\n1X\n\u03bc\u03c9\nn\npi,\u03c9 < \u03b70\n\u2212\nn\nn i=0\n\nas a random variable X, E(X) =\n\n1\nn\n\n)\n\nn\u22121\nP\n\n1\nn\n\nn\u22121\nP\n\npi,\u03c9 \u2212 \u03b4, we\n\ni=0\n\n.\n\n(18)\n\npi,\u03c9 and variance\n\ni=0\n\npi,\u03c9 (1 \u2212 pi,\u03c9 ), therefore Chebyshev's inequality yields the follow-\n\ni=0\n\n(\n\nn\u22121\n\u03bc\u03c9\n1X\nn\npi,\u03c9 \u2265 \u03b70\n\u2212\nn\nn i=0\n\n\u001a\n\nThat is equivalent to P\naccount (18),\n\n\u03bc\u03c9\nn\nn\n\nP\n\n\u2212\n\n\u001a\n\n1\nn\n\n)\n\n\u2264\n\nn\u22121\nP\n\nn\u22121\n1 X\n1\npi,\u03c9 (1 \u2212 pi,\u03c9 ) \u2264\n.\nn2 \u03b702 i=0\n4n\u03b702\n\npi,\u03c9 < \u03b70\n\ni=0\n\n\u03bc\u03c9\nn\n>\u03b4\nn\n\n\u001b\n\n\u22651\u2212\n\nOn the other hand, consider \u03be \u2208\n/ L. So\n\n\u001b\n\n\u2265 1\u2212\n\na1\n1\u2212\u03b5\n\nP\n\n\u001a\n\n<\u03b4\u2212\n\n1\nn\n\n\u03bc\u03ben\n>\u03b4\nn\n\nn\u22121\nP\n\n\u001b\n\npi,\u03be ,\n\nSo, taking into\n\n1\n.\n4n\u03b702\n\nn\u22121\nP\n\npi,\u03be <\n\ni=0\n\n0 < \u03b70 < \u03b4 \u2212\n\n1\n.\n4n\u03b702\n\n(19)\na1 n\n1\u2212\u03b5\n\n< n\u03b4. Again, since\n\n)\n\n1\n.\n4n\u03b702\n\ni=0\n\n\u2264P\n\n(\n\nn\u22121\n1X\n\u03bc\u03ben\npi,\u03be \u2265 \u03b70\n\u2212\nn\nn i=0\n\n\u2264\n\n(20)\n\nThe constant \u03b70 does not depend on n and n may be chosen sufficiently large.\nTherefore, by (19) and (20), the system Fn recognizes L with bounded error, if\nn > 2\u03b712 .\n0\nFollowing a way identical to that used in the proof of Theorem 2.6, it is\npossible to construct a single PRA-C without end-markers, which simulates the\nsystem Fn and therefore recognizes the language L.\n\u2293\n\u2294\n\n\f"}