{"id": "http://arxiv.org/abs/1011.4071v1", "guidislink": true, "updated": "2010-11-17T21:01:46Z", "updated_parsed": [2010, 11, 17, 21, 1, 46, 2, 321, 0], "published": "2010-11-17T21:01:46Z", "published_parsed": [2010, 11, 17, 21, 1, 46, 2, 321, 0], "title": "Supervised Random Walks: Predicting and Recommending Links in Social\n  Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.1310%2C1011.0242%2C1011.2036%2C1011.1289%2C1011.2628%2C1011.3420%2C1011.2820%2C1011.0370%2C1011.4637%2C1011.2618%2C1011.1314%2C1011.4035%2C1011.6013%2C1011.2314%2C1011.0213%2C1011.4701%2C1011.5252%2C1011.4840%2C1011.0048%2C1011.3571%2C1011.4555%2C1011.1620%2C1011.3448%2C1011.2735%2C1011.3423%2C1011.4886%2C1011.4673%2C1011.6627%2C1011.3113%2C1011.2607%2C1011.2868%2C1011.1916%2C1011.2459%2C1011.3099%2C1011.2075%2C1011.1170%2C1011.5270%2C1011.3070%2C1011.5809%2C1011.1043%2C1011.4682%2C1011.2512%2C1011.6345%2C1011.6312%2C1011.4993%2C1011.2528%2C1011.5364%2C1011.2350%2C1011.5970%2C1011.0974%2C1011.2015%2C1011.2585%2C1011.3597%2C1011.4793%2C1011.0708%2C1011.1748%2C1011.2051%2C1011.4389%2C1011.3438%2C1011.1908%2C1011.6580%2C1011.0296%2C1011.1027%2C1011.4537%2C1011.3000%2C1011.0623%2C1011.2298%2C1011.0602%2C1011.3507%2C1011.2210%2C1011.0975%2C1011.3984%2C1011.0140%2C1011.4941%2C1011.4293%2C1011.2368%2C1011.2796%2C1011.4071%2C1011.5049%2C1011.1576%2C1011.3358%2C1011.6229%2C1011.2373%2C1011.0799%2C1011.6191%2C1011.1844%2C1011.3490%2C1011.0801%2C1011.5802%2C1011.5523%2C1011.3302%2C1011.6331%2C1011.1320%2C1011.6042%2C1011.0419%2C1011.1728%2C1011.2620%2C1011.1358%2C1011.3579%2C1011.4194%2C1011.5197&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Supervised Random Walks: Predicting and Recommending Links in Social\n  Networks"}, "summary": "Predicting the occurrence of links is a fundamental problem in networks. In\nthe link prediction problem we are given a snapshot of a network and would like\nto infer which interactions among existing members are likely to occur in the\nnear future or which existing interactions are we missing. Although this\nproblem has been extensively studied, the challenge of how to effectively\ncombine the information from the network structure with rich node and edge\nattribute data remains largely open.\n  We develop an algorithm based on Supervised Random Walks that naturally\ncombines the information from the network structure with node and edge level\nattributes. We achieve this by using these attributes to guide a random walk on\nthe graph. We formulate a supervised learning task where the goal is to learn a\nfunction that assigns strengths to edges in the network such that a random\nwalker is more likely to visit the nodes to which new links will be created in\nthe future. We develop an efficient training algorithm to directly learn the\nedge strength estimation function.\n  Our experiments on the Facebook social graph and large collaboration networks\nshow that our approach outperforms state-of-the-art unsupervised approaches as\nwell as approaches that are based on feature extraction.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.1310%2C1011.0242%2C1011.2036%2C1011.1289%2C1011.2628%2C1011.3420%2C1011.2820%2C1011.0370%2C1011.4637%2C1011.2618%2C1011.1314%2C1011.4035%2C1011.6013%2C1011.2314%2C1011.0213%2C1011.4701%2C1011.5252%2C1011.4840%2C1011.0048%2C1011.3571%2C1011.4555%2C1011.1620%2C1011.3448%2C1011.2735%2C1011.3423%2C1011.4886%2C1011.4673%2C1011.6627%2C1011.3113%2C1011.2607%2C1011.2868%2C1011.1916%2C1011.2459%2C1011.3099%2C1011.2075%2C1011.1170%2C1011.5270%2C1011.3070%2C1011.5809%2C1011.1043%2C1011.4682%2C1011.2512%2C1011.6345%2C1011.6312%2C1011.4993%2C1011.2528%2C1011.5364%2C1011.2350%2C1011.5970%2C1011.0974%2C1011.2015%2C1011.2585%2C1011.3597%2C1011.4793%2C1011.0708%2C1011.1748%2C1011.2051%2C1011.4389%2C1011.3438%2C1011.1908%2C1011.6580%2C1011.0296%2C1011.1027%2C1011.4537%2C1011.3000%2C1011.0623%2C1011.2298%2C1011.0602%2C1011.3507%2C1011.2210%2C1011.0975%2C1011.3984%2C1011.0140%2C1011.4941%2C1011.4293%2C1011.2368%2C1011.2796%2C1011.4071%2C1011.5049%2C1011.1576%2C1011.3358%2C1011.6229%2C1011.2373%2C1011.0799%2C1011.6191%2C1011.1844%2C1011.3490%2C1011.0801%2C1011.5802%2C1011.5523%2C1011.3302%2C1011.6331%2C1011.1320%2C1011.6042%2C1011.0419%2C1011.1728%2C1011.2620%2C1011.1358%2C1011.3579%2C1011.4194%2C1011.5197&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Predicting the occurrence of links is a fundamental problem in networks. In\nthe link prediction problem we are given a snapshot of a network and would like\nto infer which interactions among existing members are likely to occur in the\nnear future or which existing interactions are we missing. Although this\nproblem has been extensively studied, the challenge of how to effectively\ncombine the information from the network structure with rich node and edge\nattribute data remains largely open.\n  We develop an algorithm based on Supervised Random Walks that naturally\ncombines the information from the network structure with node and edge level\nattributes. We achieve this by using these attributes to guide a random walk on\nthe graph. We formulate a supervised learning task where the goal is to learn a\nfunction that assigns strengths to edges in the network such that a random\nwalker is more likely to visit the nodes to which new links will be created in\nthe future. We develop an efficient training algorithm to directly learn the\nedge strength estimation function.\n  Our experiments on the Facebook social graph and large collaboration networks\nshow that our approach outperforms state-of-the-art unsupervised approaches as\nwell as approaches that are based on feature extraction."}, "authors": ["L. Backstrom", "J. Leskovec"], "author_detail": {"name": "J. Leskovec"}, "author": "J. Leskovec", "links": [{"href": "http://arxiv.org/abs/1011.4071v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.4071v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.SI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.SI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.soc-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.4071v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.4071v1", "arxiv_comment": null, "journal_reference": "Proceedings of the Fourth ACM International Conference on Web\n  Search and Data Mining (WSDM '11), February, 2011", "doi": null, "fulltext": "Supervised Random Walks:\nPredicting and Recommending Links in Social Networks\nLars Backstrom\n\nJure Leskovec\n\nFacebook\n\nStanford University\n\narXiv:1011.4071v1 [cs.SI] 17 Nov 2010\n\nlars@facebook.com\n\nABSTRACT\nPredicting the occurrence of links is a fundamental problem in networks. In the link prediction problem we are given a snapshot of a\nnetwork and would like to infer which interactions among existing\nmembers are likely to occur in the near future or which existing\ninteractions are we missing. Although this problem has been extensively studied, the challenge of how to effectively combine the\ninformation from the network structure with rich node and edge\nattribute data remains largely open.\nWe develop an algorithm based on Supervised Random Walks\nthat naturally combines the information from the network structure\nwith node and edge level attributes. We achieve this by using these\nattributes to guide a random walk on the graph. We formulate a\nsupervised learning task where the goal is to learn a function that\nassigns strengths to edges in the network such that a random walker\nis more likely to visit the nodes to which new links will be created\nin the future. We develop an efficient training algorithm to directly\nlearn the edge strength estimation function.\nOur experiments on the Facebook social graph and large collaboration networks show that our approach outperforms state-of-theart unsupervised approaches as well as approaches that are based\non feature extraction.\nCategories and Subject Descriptors: H.2.8 [Database Management]: Database applications-Data mining\nGeneral Terms: Algorithms; Experimentation.\nKeywords: Link prediction, Social networks\n\n1.\n\nINTRODUCTION\n\nLarge real-world networks exhibit a range of interesting properties and patterns [7, 20]. One of the recurring themes in this line of\nresearch is to design models that predict and reproduce the emergence of such network structures. Research then seeks to develop\nmodels that will accurately predict the global structure of the network [7, 20, 19, 6].\nMany types of networks and especially social networks are highly\ndynamic; they grow and change quickly through the additions of\nnew edges which signify the appearance of new interactions be-\n\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee.\nWSDM'11, February 9\u201312, 2011, Hong Kong, China.\nCopyright 2011 ACM 978-1-4503-0493-1/11/02 ...$10.00.\n\njure@cs.stanford.edu\n\ntween the nodes of the network. Thus, studying the networks at\na level of individual edge creations is also interesting and in some\nrespects more difficult than global network modeling. Identifying\nthe mechanisms by which such social networks evolve at the level\nof individual edges is a fundamental question that is still not well\nunderstood, and it forms the motivation for our work here.\nWe consider the classical problem of link prediction [21] where\nwe are given a snapshot of a social network at time t, and we seek\nto accurately predict the edges that will be added to the network\nduring the interval from time t to a given future time t\u2032 . More concretely, we are given a large network, say Facebook, at time t and\nfor each user we would like to predict what new edges (friendships)\nthat user will create between t and some future time t\u2032 . The problem can be also viewed as a link recommendation problem, where\nwe aim to suggest to each user a list of people that the user is likely\nto create new connections to.\nThe processes guiding link creation are of interest from more\nthan a purely scientific point of view. The current Facebook system\nfor suggesting friends is responsible for a significant fraction of link\ncreations, and adds value for Facebook users. By making better\npredictions, we will be able to increase the usage of this feature,\nand make it more useful to Facebook members.\nChallenges. The link prediction and link recommendation problems are challenging from at least two points of view. First, real\nnetworks are extremely sparse, i.e., nodes have connections to only\na very small fraction of all nodes in the network. For example, in\nthe case of Facebook a typical user is connected to about 100 out\nof more than 500 million nodes of the network. Thus, a very good\n(but unfortunately useless) way to predict edges is to predict no new\nedges since this achieves near perfect predictive accuracy (i.e., out\nof 500 million possible predictions it makes only 100 mistakes).\nThe second challenge is more subtle; to what extent can the links\nof the social network be modeled using the features intrinsic to the\nnetwork itself? Similarly, how do characteristics of users (e.g., age,\ngender, home town) interact with the creation of new edges? Consider the Facebook social network, for example. There can be many\nreasons exogenous to the network for two users to become connected: it could be that they met at a party, and then connected on\nFacebook. However, since they met at a party they are likely to be\nabout the same age, and they also probably live in the same town.\nMoreover, this link might also be hinted at by the structure of the\nnetwork: two people are more likely to meet at the same party if\nthey are \"close\" in the network. Such a pair of people likely has\nfriends in common, and travel in similar social circles. Thus, despite the fact that they became friends due to the exogenous event\n(i.e., a party) there are clues in their social networks which suggest\na high probability of a future friendship.\nThus the question is how do network and node features interact\n\n\fin the creation of new links. From the link creation point of view:\nhow important is it to have common interests and characteristics?\nFurthermore, how important is it to be in the same social circle and\nbe \"close\" in the network in order to eventually connect. From the\ntechnical point of view it is not clear how to develop a method that,\nin a principled way, combines the features of nodes (i.e., user profile information) and edges (i.e., interaction information) with the\nnetwork structure. A common, but somewhat unsatisfactory, approach is to simply extract a set of features describing the network\nstructure (like node degree, number of common friends, shortest\npath length) around the two nodes of interest and combine it with\nthe user profile information.\nPresent work: Supervised Random Walks. To address these\nchallenges we develop a method for both link prediction and link\nrecommendation. We develop a concept of Supervised Random\nWalks that naturally and in a principled way combines the network\nstructure with the characteristics (attributes, features) of nodes and\nedges of the network into a unified link prediction algorithm.\nWe develop a method based on Supervised Random Walks that in\na supervised way learns how to bias a PageRank-like random walk\non the network [3, 2] so that it visits given nodes (i.e., positive\ntraining examples) more often than the others.\nWe achieve this by using node and edge features to learn edge\nstrengths (i.e., random walk transition probabilities) such that the\nrandom walk on a such weighted network is more likely to visit\n\"positive\" than \"negative\" nodes. In the context of link prediction,\npositive nodes are nodes to which new edges will be created in the\nfuture, and negative are all other nodes. We formulate a supervised\nlearning task where we are given a source node s and training examples about which nodes s will create links to in the future. The\ngoal is to then learn a function that assigns a strength (i.e., random\nwalk transition probability) to each edge so that when computing\nthe random walk scores in such a weighted network nodes to which\ns creates new links have higher scores to s than nodes to which s\ndoes not create links.\nFrom a technical perspective, we show that such edge strength\nfunction can be learned directly and efficiently. This means, that\nwe do not postulate what it means for edge to be \"strong\" in an adhoc way and then use this heuristic estimate. Rather, we show how\nto directly find the parameters of the edge strength function which\ngive optimal performance. This means we are able to compute the\ngradient of the parameters of the edge strength function with respect to the PageRank-like random walk scores. The formulation\nresults in an optimization problem for which we derive an efficient\nestimation procedure.\nFrom the practical point of view, we experiment with large collaboration networks and data from the Facebook network, showing that our approach outperforms state-of-the-art unsupervised approaches as well as supervised approaches based on complex network feature extraction. An additional benefit of our approach is\nthat no complex network feature extraction or domain expertise are\nnecessary as our algorithm nicely combines the node attribute and\nnetwork structure information.\nApplications and consequences. As networks evolve and grow by\naddition of new edges, the link prediction problem offers insights\ninto the factors behind creation of individual edges as well as into\nnetwork formation in general.\nMoreover, the link-prediction and the link-recommendation problems are relevant to a number of interesting current applications of\nsocial networks. First, for online social networking websites, like\nFacebook and Myspace, being able to predict future interactions\nhas direct business consequences. More broadly, large organiza-\n\ntions can directly benefit from the interactions within the informal\nsocial network among its members and link-prediction methods\ncan be used to suggest possible new collaborations and interactions within the organization. Research in security has recently\nrecognized the role of social network analysis for this domain (e.g.,\nterrorist networks). In this context link prediction can be used to\nsuggest the most likely links that may form in the future. Similarly,\nlink prediction can also be used for prediction of missing or unobserved links in networks [9] or to suggest which individuals may\nbe working together even though their interaction has yet been directly observed. Applications go well beyond social networks, as\nour techniques can be used to predict unobserved links in proteinprotein interaction networks in systems biology or give suggestions\nto bloggers about which relevant pages on the Web to link to.\nFurthermore, the framework we develop is more general than\nlink prediction, and could be used for any sort of interaction. For\ninstance, in a collaboration network, it could easily be used not to\npredict who s will link to next (write a paper with a previously\nun-collaborated-with person) but to predict who s will coauthor a\npaper with next, including all those with whom s has previously\ncoauthored.\nFurther related work. The link prediction problem in networks\ncomes in many flavors and variants. For example, the network inference problem [13, 24] can be cast as a link prediction problem\nwhere no knowledge of the network is given. Moreover, even models of complex networks, like Preferential Attachment [7], Forest\nFire model [20] and models based on random walks [19, 8], can be\nviewed as ways for predicting new links in networks.\nThe unsupervised methods for link prediction were extensively\nevaluated by Liben-Nowell and Kleinberg [21] who found that the\nAdamic-Adar measure of node similarity [1] performed best. More\nrecently approaches based on network community detection [9, 16]\nhave been tested on small networks. Link prediction in supervised\nmachine learning setting was mainly studied by the relational learning community [28, 26]. However, the challenge with these approaches is primarily scalability.\nRandom walks on graphs have been considered for computing\nnode proximities in large graphs [31, 30, 29, 27]. They have also\nbeen used for learning to rank nodes in graphs [3, 2, 23, 11].\n\n2. SUPERVISED RANDOM WALKS\nNext we describe our algorithm for link prediction and recommendation. The general setting is that we are given a graph and a\nnode s for which we would like to predict/recommend new links.\nThe idea is that s has already created some links and we would like\nto predict which links it will create next (or will be created to it,\nsince the direction of the links is often not clear). For simplicity\nthe following discussion will focus on a single node s and how to\npredict the links it will create in the future.\nNote that our setting is much more general than it appears. We\nrequire that for a node s we are given a set of \"positive\" and \"negative\" training nodes and our algorithm then learns how to distinguish them. This can be used for link prediction (positive nodes are\nthose to which links are created in the future), link recommendation (positive nodes are those which user clicks on), link anomaly\ndetection (positive nodes are those to which s has anomalous links)\nor missing link prediction (positive nodes are those to which s has\nmissing links), to name a few. Moreover, our approach can also\nbe generalized to a setting where prediction/recommendation is not\nbeing made for only a single node s but also for a group of nodes.\nGeneral considerations. A first general approach to link prediction would be to view it as a classification task. We take pairs\n\n\fof nodes to which s has created edges as positive training examples, and all other nodes as negative training examples. We then\nlearn a classifier that predicts where node s is going to create links.\nThere are several problems with such an approach. The first is the\nclass imbalance; s will create edges to a very small fraction of the\ntotal nodes in the network and learning is particularly hard in domains with high class imbalance. Second, extracting the features\nthat the learning algorithm would use is a challenging and cumbersome task. Deciding which node features (e.g., node demographics\nlike, age, gender, hometown) and edge features (e.g., interaction\nactivity) to use is already hard. However, it is even less clear how\nto extract good features that describe the network structure and patterns of connectivity between the pair of nodes under consideration.\nEven in a simple undirected graph with no node/edge attributes,\nthere are countless ways to describe the proximity of two nodes.\nFor example, we might start by counting the number of common\nneighbors between the two nodes. We might then adjust the proximity score based on the degrees of the two nodes (with the intuition\nbeing that high-degree nodes are likely to have common neighbors\nby mere happenstance). We might go further giving different length\ntwo paths different weights based on things like the centrality or degree of the intermediate nodes. The possibilities are endless, and\nextracting useful features is typically done by trial and error rather\nthan any principled approach. The problem becomes even harder\nwhen annotations are added to edges. For instance, in many networks we know the creation times of edges, and this is likely to be\na useful feature. But how do we combine the creation times of all\nthe edges to get a feature relevant to a pair of nodes?\nA second general approach to the link prediction problem is to\nthink about it as a task to rank the nodes of the network. The idea\nis to design an algorithm that will assign higher scores to nodes\nwhich s created links to than to those that s did not link to. PageRank [25] and variants like Personalized PageRank [17, 15] and\nRandom Walks with Restarts [31] are popular methods for ranking\nnodes on graphs. Thus, one simple idea would be to start a random\nwalk at node s and compute the proximity of each other node to\nnode s [30]. This can be done by setting the random jump vector\nso that the walk only jumps back to s and thus restarts the walk.\nThe stationary distribution of such random walk assigns each node\na score (i.e., a PageRank score) which gives us a ranking of how\n\"close\" to the node s are other nodes in the network. This method\ntakes advantage of the structure of the network but does not consider the impact of other properties, like age, gender, and creation\ntime.\nOverview of our approach. We combine the two above approaches\ninto a single framework that will at the same time consider rich\nnode and edge features as well as the structure of the network. As\nRandom Walks with Restarts have proven to be a powerful tool for\ncomputing node proximities on graphs we use them as a way to\nconsider the network structure. However, we then use the node and\nedge attribute data to bias the random walk so that it will more often\nvisit nodes to which s creates edges in the future.\nMore precisely, we are given a source node s. Then we are also\ngiven a set of destination nodes d1 , . . . , dk \u2208 D to which s will\ncreate edges in the near future. Now, we aim to bias the random\nwalk originating from s so that it will visit nodes di more often\nthan other nodes in the network. One way to bias the random walk\nis to assign each edge a random walk transition probability (i.e.,\nstrength). Whereas the traditional PageRank assumes that transition probabilities of all edges to be the same, we learn how to assign each edge a transition probability so that the random walk is\nmore likely to visit target nodes di than other nodes of the network.\nHowever, directly setting an arbitrary transition probability to each\n\nedge would make the task trivial, and would result in drastic overfitting. Thus, we aim to learn a model (a function) that will assign\nthe transition probability for each edge (u, v) based on features of\nnodes u and v, as well as the features of the edge (u, v). The question we address next is, how to directly and in a principled way\nestimate the parameters of such random walk biasing function?\nProblem formulation. We are given a directed graph G(V, E), a\nnode s and a set of candidates to which s could create an edge.\nWe label nodes to which s creates edges in the future as destination nodes D = {d1 , . . . , dk }, while we call other nodes to which\ns does not create edges no-link nodes L = {l1 , . . . , ln }. We label candidate nodes with a set C = {ci } = D \u222a L. We think of\nnodes in D as positive and nodes in L as negative training examples. Later we generalize to multiple instances of s, L and D. Each\nnode and each edge in G is further described with a set of features.\nWe assume that each edge (u, v) has a corresponding feature vector\n\u03c8uv that describes the nodes u and v (e.g., age, gender, hometown)\nand the interaction attributes (e.g., when the edge was created, how\nmany messages u and v exchanged, or how many photos they appeared together in).\nFor edge (u, v) in G we compute the strength auv = fw (\u03c8uv ).\nFunction fw parameterized by w takes the edge feature vector \u03c8uv\nas input and computes the corresponding edge strength auv that\nmodels the random walk transition probability. It is exactly the\nfunction fw (\u03c8) that we learn in the training phase of the algorithm.\nTo predict new edges of node s, first edge strengths of all edges\nare calculated using fw . Then a random walk with restarts is run\nfrom s. The stationary distribution p of the random walk assigns\neach node u a probability pu . Nodes are ordered by pu and top\nranked nodes are then predicted as destinations of future links of s.\nNow our task is to learn the parameters w of function fw (\u03c8uv )\nthat assigns each edge a transition probability auv . One can think\nof the weights auv as edge strengths and the random walk is more\nlikely to traverse edges of high strength and thus nodes connected\nto node s via paths of strong edges will likely be visited by the\nrandom walk and will thus rank higher.\nThe optimization problem. The training data contains information that source node s will create edges to nodes d \u2208 D and not\nto nodes l \u2208 L. So, we aim to set the parameters w of function\nfw (\u03c8uv ) so that it will assign edge weights auv in such a way that\nthe random walk will be more likely to visit nodes in D than L, i.e.,\npl < pd , for each d \u2208 D and l \u2208 L.\nThus, we define the optimization problem to find the optimal set\nof parameters w of edge strength function fw (\u03c8uv ) as follows:\nmin F (w) = ||w||2\nw\n\nsuch that\n\u2200 d\u2208D, l\u2208L : pl < pd\n\n(1)\n\nwhere p is the vector of PageRank scores. Note that PageRank\nscores pi depend on edge strengths auv and thus actually depend\non fw (\u03c8uv ) that is parameterized by w. The idea here is that we\nwant to find the parameter vector w such that the PageRank scores\nof nodes in D will be greater than the scores of nodes in L. We\nprefer the shortest w parameter vector simply for regularization.\nHowever, Eq. 1 is a \"hard\" version of the optimization problem\nas it allows no constraints to be violated. In practice it is unlikely\nthat a solution satisfying all the constraints exists. Thus similarly to\nformulations of Support Vector Machines we make the constraints\n\"soft\" by introducing a loss function h that penalizes violated con-\n\n\fstraints. The optimization problem now becomes:\nX\nh(pl \u2212 pd )\nmin F (w) = ||w||2 + \u03bb\nw\n\nInitialize PageRank scores p and partial derivatives\n(2)\n\nd\u2208D,l\u2208L\n\nSolving the optimization problem. First we need to establish the\nconnection between the parameters w of the edge strength function\nfw (\u03c8uv ) and the random walk scores p. Then we show how to obtain the derivative of the loss function and the random walk scores\np with respect to w and then perform gradient based optimization\nmethod to minimize the loss and find the optimal parameters w.\nFunction fw (\u03c8uv ) combines the attributes \u03c8uv and the parameter vector w to output a non-negative weight auv for each edge. We\nthen build the random walk stochastic transition matrix Q\u2032 :\n(\nP auv\nif (u, v) \u2208 E,\n\u2032\nw auw\nQuv =\n(3)\n0\notherwise\nTo obtain the final random walk transition probability matrix Q,\nwe also incorporate the restart probability \u03b1, i.e., with probability\n\u03b1 the random walk jumps back to seed node s and thus \"restarts\":\nQuv = (1 \u2212 \u03b1)Q\u2032uv + \u03b11(v = s).\nNote that each row of Q sums to 1 and thus each entry Quv defines\nthe conditional probability that a walk will traverse edge (u, v)\ngiven that it is currently at node u.\nThe vector p is the stationary distribution of the Random walk\nwith restarts (also known as Personalized PageRank), and is the\nsolution to the following eigenvector equation:\n(4)\n\nEquation 4 establishes the connection between the node PageRank scores pu \u2208 p, and parameters w of function fw (\u03c8uv ) via the\nrandom walk transition matrix Q. Our goal now is to minimize\nEq. 2 with respect to the parameter vector w. We approach this by\nfirst deriving the gradient of F (w) with respect to w, and then use a\ngradient based optimization method to find w that minimize F (w).\nNote that is non-trivial due to the recursive relation in Eq. 4.\nFirst, we introduce a new variable \u03b4ld = pl \u2212 pd and then we can\nwrite the derivative:\nX \u2202h(pl \u2212 pd )\n\u2202F (w)\n=\n2w +\n\u2202w\n\u2202w\nl,d\n(5)\nX \u2202h(\u03b4ld ) \u2202pl\n\u2202pd\n(\n\u2212\n)\n= 2w +\n\u2202\u03b4ld \u2202w\n\u2202w\nl,d\nFor commonly used loss functions h(*) (like, hinge-loss or squared\nld )\n. However, it is\nloss), it is simple to compute the derivative \u2202h(\u03b4\n\u2202\u03b4ld\n\nu\nnot clear how to compute \u2202p\n, the derivative of the score pu with\n\u2202w\nrespect to the vector w. Next we show how to do this.\nNote that p is thePprincipal eigenvector of matrix Q. Eq. 4 can be\nrewritten as pu = j pj Qju and taking the derivative now gives:\n\nX\n\u2202pj\n\u2202Qju\n\u2202pu\nQju\n=\n+ pj\n\u2202w\n\u2202w\n\u2202w\nj\n\n=\n\n\u2202pu\n\u2202wk\n\n:\n\n1\n|V |\n(0)\n\nwhere \u03bb is the regularization parameter that trades-off between the\ncomplexity (i.e., norm of w) for the fit of the model (i.e., how much\nthe constraints can be violated). Moreover, h(*) is a loss function\nthat assigns a non-negative penalty according to the difference of\nthe scores pl \u2212 pd . If pl \u2212 pd < 0 then h(*) = 0 as pl < pd and\nthe constraint is not violated, while for pl \u2212 pd > 0, also h(*) > 0.\n\npT = pT Q\n\nforeach u \u2208 V do\n\n(0)\npu\n\n(6)\n\nu\nare recursively entangled in the equation.\nNotice that pu and \u2202p\n\u2202w\nHowever, we can still compute the gradient iteratively [4, 3]. By\n\n\u2202pu\n=0\nforeach u \u2208 V, k = 1, . . . , |w| do \u2202w\nk\nt=1\nwhile not converged do\nforeach u \u2208 V do\nP (t\u22121)\n(t)\nQju\npu = j pj\nt=t+1\nt=1\nforeach k = 1, . . . , |w| do\nwhile not converged do\nforeach u \u2208 V do\nP\n\u2202p (t\u22121)\n(t\u22121) \u2202Qju\n\u2202pu (t)\n+ pj\n= j Qju \u2202wjk\n\u2202wk\n\u2202wk\n\nt = t+1\n\n\u2202pu (t\u22121)\n\u2202w\n\nreturn\nAlgorithm 1: Iterative power-iterator like computation of\nu\n.\nPageRank vector p and its derivative \u2202p\n\u2202w\n\nrecursively applying the chain rule to Eq. 6 we can use a powermethod like algorithm to compute the derivative. We repeatedly\nu\nbased on the estimate obtained in the\ncompute the derivative \u2202p\n\u2202w\nprevious iteration. Thus, we first compute p and then update the\nu\nestimate of the gradient \u2202p\n. We stop the algorithm when both p\n\u2202w\n\u2202p\ndo not change (i.e., \u03b5 = 10\u221212 in our experiments) between\nand \u2202w\niterations. We arrive at Algorithm 1 that iteratively computes the\neigenvector p as well as the partial derivatives of p. Convergence\nof Algorithm 1 is similar to those of power-iteration [5].\n\u2202Q\nTo solve Eq. 4 we further need to compute \u2202wju which is the\npartial derivative of entry Qju (Eq. 3). This calculation is straight\u2202Q\nforward. When (j, u) \u2208 E we find \u2202wju =\n(1 \u2212 \u03b1)\n\n\u2202fw (\u03c8ju )\n\u2202w\n\n\u2202Q\n\nP\n\nk\n\n\u0001\nP\nfw (\u03c8jk ) \u2212 fw (\u03c8ju )\nk\n\u00012\nP\nf\n(\u03c8\n)\nw\njk\nk\n\n\u2202fw (\u03c8jk ) \u0001\n\u2202w\n\nand otherwise \u2202wju = 0. The edge strength function fw (\u03c8uv )\nw\nmust be differentiable and so \u2202f\n(\u03c8jk ) can be easily computed.\n\u2202w\nThis completes the derivation and shows how to evaluate the\nderivative of F (w) (Eq. 5). Now we apply a gradient descent based\nmethod, like a quasi-Newton method, and directly minimize F (w).\nFinal remarks. First we note that our problem is not convex in\ngeneral, and thus gradient descent methods will not necessarily find\nthe global minimum. In practice we resolve this by using several\ndifferent starting points to find a good solution.\nSecond, since we are only interested in the values of p for nodes\nin C, it makes sense to evaluate the loss function at a slightly different point: h(p\u2032l \u2212 p\u2032d ) where p\u2032 is a normalized version of p such\nthat p\u2032u = P pu pv . This adds one more chain rule application to\nv\u2208C\nthe derivative calculation, but does not change the algorithm. The\neffect of this is mostly to allow larger values of \u03b1 to be used without having to change h(*) (We omit the tick marks in our notation\nfor the rest of this paper, using p to refer to the normalized score).\nSo far we only considered training and estimating the parameter\nvector w for predicting the edges of a particular node s. However,\nour aim to estimate w that make good predictions across many different nodes s \u2208 S. We easily extend the algorithm to multiple\nsource nodes s \u2208 S, that may even reside in different graphs. We\ndo this by taking the sum of losses over all source nodes s and the\n\n\fcorresponding pairs of positive Ds and negative Ls training examples. We slightly modify the Eq. 2 to obtain:\n\nPerformance on Synthetic Data Generated Deterministically\n1\n0.99\n\nmin F (w) = ||w|| + \u03bb\nw\n\nX\n\nX\n\nh(pl \u2212 pd )\n\n0.98\n\ns\u2208S d\u2208Ds ,l\u2208Ls\n\n0.97\n0.96\n0.95\n0.94\nUnweighted Pagerank\nWeights +1,-1\nLearned Weights\n\n0.93\n0.92\n0\n\n0.5\n\n1\nNoise Level\n\n1.5\n\n2\n\nFigure 1: Experiments on synthetic data. Deterministic D.\nPerformance on Synthetic Data Generated Probabilistically\n0.88\n0.86\n0.84\nAUC\n\nThe gradients of each instance s \u2208 S remain independent, and can\nthus be computed independently for all instances of s (Alg. 1). By\noptimizing parameters w over many individuals s, the algorithm is\nless likely to overfit, which improves the generalization.\nAs a final implementation note, we point out that gradient descent often makes many small steps which have small impact on\nthe eigenvector and its derivative. A 20% speedup can be achieved\nby using the solutions from the previous position (in the gradient\ndescent) as initialization for the eigenvector and derivative calculations in Alg. 1. Our implementation of Supervised Random Walks\nuses the L-BFGS algorithm [22]. Given a function and its partial derivatives, the solver iteratively improves the estimate of w,\nconverging to a local optima. The exact runtime of the method depends on how many iterations are required for convergence of both\nthe PageRank and derivative computations, as well as of the overall\nprocess (quasi-Newton iterations).\n\nAUC\n\n2\n\n0.82\n0.8\n0.78\n\n3.\n\nEXPERIMENTS ON SYNTHETIC DATA\n\nBefore experimenting with real data, we examine the soundness\nand robustness of the proposed algorithm using synthetic data. Our\ngoal here is to generate synthetic graphs, edge features and training\ndata (triples (s, D, L)) and then try to recover the original model.\nSynthetic data. We generate scale-free graphs G on 10,000 nodes\nby using the Copying model [18]: Graph starts with three nodes\nconnected in a triad. Remaining nodes arrive one by one, each\ncreating exactly three edges. When a node u arrives, it adds three\nedges (u, vi ). Existing node vi is selected uniformly at random\nwith probability 0.8, and otherwise vi is selected with probability\nproportional to its current degree. For each edge (u, v) we create\ntwo independent Gaussian features with mean 0 and variance 1. We\nset the edge strength auv = exp(\u03c8uv1 \u2212 \u03c8uv2 ), i.e., w\u2217 = [1, \u22121].\nFor each G, we randomly select one of the oldest 3 nodes of G\nas the start node, s. To generate a set of destination D and no-link\nnodes L for a given s we use the following approach.\nOn the graph with edge strengths auv we run the random walk\n(\u03b1 = 0.2) starting from s and obtain node PageRank scores p\u2217 . We\nuse these scores to generate the destinations D in one of two ways.\nFirst is deterministic and selects the top K nodes according to p\u2217\nto which s is not already connected. Second is probabilistic and\nselects K nodes, selecting each node u with probability p\u2217u .\nNow given the graph G, attributes \u03c8uv and targets D our goal is\nto recover the true edge strength parameter vector w\u2217 = [1, \u22121].\nTo make the task more interesting we also add random noise to all\n\u2032\nof the attributes, so that \u03c8uvi\n= \u03c8uvi + N (0, \u03c3 2 ), where N (0, \u03c3 2 )\nis a Gaussian random variable with mean 0 and variance \u03c3 2 .\nResults. After applying our algorithm, we are interested in two\nthings. First, how well does the model perform in terms of the\nclassification accuracy and second, whether it recovers the edge\nstrength function parameters w\u2217 = [1, \u22121]. In the deterministic\ncase of creating D and with 0 noise added, we hope that the algorithm is able achieve near perfect classification. As the noise\nincreases, we expect the performance to drop, but even then, we\nhope that the recovered values of \u0175 will be close to true w\u2217 .\n\nUnweighted Pagerank\nWeights +1,-1\nLearned Weights\n\n0.76\n0.74\n0\n\n0.5\n\n1\nNoise Level\n\n1.5\n\n2\n\nFigure 2: Experiments on synthetic data. Probabilistic D.\nIn running the experiment we generated 100 synthetic graphs.\nWe used 50 of them for training the weights w, and report results\non the other 50. We compute Area under the ROC curve (AUC)\nof each of 50 test graphs, and report the mean (AUC of 1.0 means\nperfect classification, while random guessing scores 0.5).\nFigures 1 and 2 show the results. We plot the performance of the\nmodel that ignores edge weights (red), the model with true weights\nw\u2217 (green) and a model with learned weights \u0175 (blue).\nFor the deterministically generated D (Fig. 1), the performance\nis perfect in the absence of any noise. This is good news as it\ndemonstrates that our training procedure is able to recover the correct parameters. As the noise increases, the performance slowly\ndrops. When the noise reaches \u03c3 2 \u2248 1.5, using the true parameters w\u2217 (green) actually becomes worse than simply ignoring them\n(red). Moreover, our algorithm learns the true parameters [+1, \u22121]\nalmost perfectly in the noise-free case, and decreases their magnitude as the noise level increases. This matches the intuition that,\nas more and more noise is added, the signal in the edge attributes\nbecomes weaker and weaker relatively to the signal in the graph\nstructure. Thus, with more noise, the parameter values w decrease\nas they are given less and less credence.\nIn the probabilistic case (Fig. 2), we see that our algorithm does\nbetter (statistically significant at p = 0.01) than the model with\ntrue parameters w\u2217 , regardless of the presence or absence of noise.\nEven though the data was generated using parameters w\u2217 = [+1, \u22121],\nthese values are not optimal and our model gets better AUC by finding different (smaller) values. Again, as we add noise, the overall\nperformance slowly drops, but still does much better than the baseline method of ignoring edge strengths (red), and continues to do\nbetter than the model that uses true parameter values w\u2217 (green).\nWe also note that regardless of where we initialize the parameter\n\n\f1\n\n0.1\nAstro\nHep-ph\nFacebook\n\n0.1\n\n0.01\n(Facebook)\n\nProbability of becoming friends (Hep + Astro)\n\nProbability of Friendship vs. Number of Mutual Friends\n\n0.01\n\n0.001\n\n0.001\n\n0.0001\n0\n\n2\n\n4\n6\n8\n10\n12\nNumber of mutual friends\n\n14\n\nFigure 3: Probability of a new link as a function of the number\nof mutual friends.\n\nAstro-Ph\nCond-Mat\nHep-Ph\nHep-Th\nFacebook\n\nN\n19,144\n23,608\n12,527\n10,700\n174,000\n\nE\n198,110\n94,492\n118,515\n25,997\n29M\n\nS\n1,123\n140\n340\n55\n200\n\nD\u0304\n18.0\n9.1\n29.2\n6.3\n43.6\n\nC\u0304\n775.6\n335.5\n345.3\n110.5\n1987\n\nD\u0304/C\u0304\n0.023\n0.027\n0.084\n0.057\n0.022\n\nTable 1: Dataset statistics. N, E: number of nodes and edges\nin the full network, S: number of sources, C\u0304: avg. number of\ncandidates per source, D\u0304: avg. number of destination nodes.\nwork degree) ku and let tu be the time when u created it's ku /2-th\nedge. Then we define mu to be the number of co-authorship links\nthat u created after time tu and that at the time of creation spanned\n2-hops (i.e., closed a triangle). We attempt to make predictions\nonly for \"active\" authors, where we define a node u to be active if\nku \u2265 K and mu \u2265 \u2206. In this work, we set K = 10 and \u2206 = 5.\nFor every source node s that is above this threshold, we extract the\nnetwork at time ts and try to predict the ds new edges that s creates\nin the time after ts . Table 1 gives dataset statistics.\nFor every edge (i, j) of the network around the source node u at\ntime tu we generate the following six features:\n\u2022\n\u2022\n\u2022\n\u2022\n\nNumber of papers i written before tu\nNumber of papers j written before tu\nNumber of papers i and j co-authored\nCosine similarity between the titles of papers written by i and\ntitles of j's papers\n\u2022 Time since i and j last co-authored a paper.\n\u2022 The number of common friends between j and s.\n\nFigure 4: Facebook Iceland: Hop distance between a pair of\nnodes just before they become friends. Distance x=-1 denotes\nnodes that were in separate components, while x=2 (friends of\nfriends) is order of magnitude higher than next highest point.\nvector w before starting gradient descent, it always converges to the\nsame solution. Having thus validated our algorithm on synthetic\ndata, we now move on to predicting links in real social networks.\n\n4.\n\nEXPERIMENTAL SETUP\n\nFor experiments on real data we consider four real physics coauthorship networks and a complete Facebook network of Iceland.\nGenerally we focus on predicting links to nodes that are 2-hops\nfrom the seed node s. We do this for two reasons. First, in online\nsocial networks more than half of all edges at the time of creation\nclose a triangle, i.e., a person connects to a friend of a friend [19].\nFor instance, Figure 4 shows that 92% of all edges created on Facebook Iceland close a path of length two, i.e., a triangle. Second, this\nalso makes the Supervised Random Walks run faster as graphs get\nsmaller. Given that some Facebook users have degrees in the thousands, it is not practical to incorporate them (a user may have as\nmany as a hundred million nodes at 3 hops).\nCo-authorship networks. First we consider the co-authorship networks from arXiv e-print archive [12] where we have a time-stamped\nlist of all papers with author names and titles submitted to arXiv\nduring 1992 and 2002. We consider co-authorship networks from\nfour different areas of physics: Astro-physics (Astro-Ph), Condensed Matter (Cond-Mat), High energy physics theory (Hep-th)\nand High energy physics phenomenology (Hep-ph). For each of\nthe networks we proceed as follows. For every node u we compute\nthe total number of co-authors at the end of the dataset (i.e., net-\n\nThe Facebook network. Our second set of data comes from the\nFacebook online social network. We first selected Iceland since it\nhas high Facebook penetration, but relatively few edges pointing\nto users in other countries. We generated our data based on the\nstate of the Facebook graph on November 1, 2009. The destination\nnodes D from a node s are those that s became friends with between November 1 2009 and January 13 2010. The Iceland graph\ncontains more than 174 thousand people, or 55% of the country's\npopulation. The average user had 168 friends, and during the period Nov 1 \u2013 Jan 23, an average person added 26 new friends.\nFrom these users, we randomly selected 200 as the nodes s.\nAgain, we only selected \"active\" nodes, this time with the criteria |D| > 20. As Figure 3 shows, individuals without many mutual\nfriends are exceedingly unlikely to become friends. As the Facebook graph contains users whose 2-hop neighborhood have several\nmillion nodes we can prune such graphs and speed-up the computations without loosing much on prediction performance. Since we\nknow that individuals with only a few mutual friends are unlikely to\nform friendships, and our goal is to predict the most likely friendships, we remove all individuals with less than 4 mutual friends\nwith practically no loss in performance. As demonstrated in Figure 3, if a user creates an edge, then the probability that she links\nto a node with whom she has less than 4 friends is about 0.1%.).\nWe annotated each edge of the Facebook network with seven\nfeatures. For each edge (i, j), we created:\n\u2022 Edge age: (T \u2212 t)\u2212\u03b2 , where T is the time cutoff Nov. 1, and\nt is the edge creation time. We create three features like this\nwith \u03b2 = {0.1, 0.3, 0.5}.\n\u2022 Edge initiator: Individual making the friend request is encoded as +1 or \u22121.\n\u2022 Communication and observation features. They represent the\nprobability of communication and profile observation in a\none week period.\n\n\f\u2022 The number of common friends between j and s.\n\nEffect of \u03b1 value on Hep-ph performance\n\nAll features in all datasets are re-scaled to have mean 0 and standard\ndeviation 1. We also add a constant feature with value 1.\n\n0.7\n0.68\nAUC\n\nEvaluation methodology. For each dataset, we assign half of the\nnodes s into training and half into test set. We use the training set\nto train the algorithm (i.e., estimate w). We evaluate the method on\nthe test set, considering two performance metrics: the Area under\nthe ROC curve (AUC) and the Precision at Top 20 (Prec@20), i.e.,\nhow many of top 20 nodes suggested by our algorithm actually\nreceive links from s. This measure is particularly appropriate in\nthe context of link-recommendation where we present a user with a\nset of friendship suggestions and aim that most of them are correct.\n\nLearned weights\nUnweighted\n\n0.72\n\n0.66\n0.64\n0.62\n0.6\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n\u03b1\n\n5.\n\nEXPERIMENTS ON REAL DATA\n\nFigure 5: Impact of random walk restart parameter \u03b1.\n\nNext we describe the results of on five real datasets: four coauthorship networks and the Facebook network of Iceland.\n\n5.1 General considerations\nFirst we evaluate several aspects of our algorithm: (A) the choice\nof the loss function, (B) the choice of the edge strength function\nfw (*), (C) the choice of random walk restart (jump) parameter \u03b1,\nand (D) choice of regularization parameter \u03bb. We also consider the\nextension where we learn a separate edge weight vector depending\non the type of the edge, i.e., whether an edge touches s or any of\nthe candidate nodes c \u2208 C.\n(A) Choice of the loss function. As is the case with most machine\nlearning algorithms, the choice of loss function plays an important\nrole. Ideally we would like to optimize the loss function h(*) which\ndirectly corresponds to our evaluation metric (i.e., AUC or Precision at top k). However, as such loss functions are not continuous\nand not differentiable and so it is not clear how to optimize over\nthem. Instead, we experiment with three common loss functions:\n\u2022 Squared loss with margin b:\n2\n\nh(x) = max{x + b, 0}\n\n\u2022 Huber loss with margin b and window z > b:\n\uf8f1\n\uf8f4\nif x \u2264 \u2212b,\n\uf8f20\nh(x) = (x + b)2 /(2z) if \u2212b < x \u2264 z \u2212 b,\n\uf8f4\n\uf8f3(x + b) \u2212 z/2 if x > z \u2212 b\n\n(7)\n\n\u2022 Wilcoxon-Mann-Whitney (WMW) loss with width b (Proposed to be used when one aims to maximize AUC [32]):\n1\nh(x) =\n1 + exp(\u2212x/b)\n\nEach of these loss functions is differentiable and needs to be\nevaluated for all pairs of nodes d \u2208 D and l \u2208 L (see Eq. 2). Performing this naively takes approximately O(c2 ) where c = |D\u222aL|.\nHowever, we next show that the first two loss functions have the advantage that they can be computed in O(c log c). For example, we\nrewrite the squared loss as:\nX\nX\nh(pl \u2212 pd ) =\n(pl \u2212 pd + b)2\nl,d:pl +b>pd\n\nd,l\n\n=\n\nX\nl\n\n=\n\nX\n\n(pl + b)2 \u2212 2(pl + b)pd + p2d\n\nd:pl +b>pd\n\nX\n\n|{d : pl + b > pd }|(pl + b)2\n\nl\n\n\u22122(pl + b)\n\nX\n\nd:pl +b>pd\n\npd +\n\nX\n\nd:pl +b>pd\n\np2d\n\nOnce we have the lists {pl } and {pd } sorted, we can iterate over\nthe list {pl } in reverse order. As we do this, we can incrementally\nupdate the two terms which sum over d above. The Huber loss can\nas well be quickly evaluated using a similar calculation.\nComputation of the WMW loss is more expensive, as there is no\nway to go around the summation over all pairs. Evaluating WMW\nloss thus takes time O(|D| * |L|). In our case, |D| is typically\nrelatively small, and so the computation is not a significant part\nof total runtime. However, the primary advantage of it is that it\nperforms slightly better. Indeed, in the limit as b goes to 0, it reflects\nAUC, as it measures the number of inversions in the ordering [32].\nIn our experiments we notice that while the gradient descent\nachieves significant reduction in the value of the loss for all three\nloss functions, this only translates to improved AUC and Prec@20\nfor the WMW loss. In fact, the model trained with the squared or\nthe Huber loss does not perform much better than the baseline we\nobtain through unweighted PageRank. Consequently, we use the\nWMW loss function for the remainder of this work.\n(B) Choice of edge strength function fw (\u03c8uv ). The edge strength\nfunction fw (\u03c8uv ) must be non-negative and differentiable. While\nmore complex functions are certainly possible, we experiment with\ntwo functions. In both cases, we start by taking the inner product of\nthe weight vector w and the feature vector \u03c8uv of an edge (u, v).\nThis yields a single scalar value, which may be negative. To transform this into the desired domain, we apply either an exponential\nor logistic function:\n\u2022 Exponential edge strength: auv = exp(\u03c8uv * w)\n\u2022 Logistic edge strength: auv = (1 + exp(\u2212\u03c8uv * w))\u22121\nOur experiments show that the choice of the edge strength function does not seem to make a significant impact on performance.\nThere is slight evidence from our experiments that the logistic function performs better One problem that can occur with the exponential version is underflow and overflow of double precision floating\npoint numbers. As the performance seems quite comparable, we\nrecommend the use of the logistic to avoid this potential pitfall.\n(C) Choice of \u03b1. To get a handle on the impact of random walk\nrestart parameter \u03b1, it is useful to think of the extreme cases, for unweighted graphs. When \u03b1 = 0, the PageRank of a node in an undirected graph is simply its degree. On the other hand, when \u03b1 approaches 1, the score will be exactly proportional to the \"RandomRandom\" model [19] which simply makes two random hops from\ns, as random walks of length greater than 2 become increasingly\nunlikely, and hence the normalized eigenvector scores become the\nsame as the Random-Random scores [19]. When we add the notion\nof edge strengths, these properties remain. Intuitively, \u03b1 controls\n\n\fFigure 6: Stationary random walk distribution with \u03b1 = 0.15.\nLearning Curve for Facebook Data\n0.83\n\n(D) Regularization parameter \u03bb. Empirically we find that overfitting is not an issue in our model as the number of parameters w\nis relatively small. Setting \u03bb = 1 gives best performance.\n\nExtension: Social capital. Before moving on to the experimental\nresults, we also briefly examine somewhat counterintuitive behavior of the Random Walk with Restarts. Consider a graph in Figure 6\nwith the seed node s. There are two nodes which s could form a\nnew connection to v1 and v2 . These two are symmetric except for\nthe fact that the two paths connecting s to v1 are connected themselves. Now we ask, is s more likely to link to v1 or to v2 ?\nBuilding on the theory of embeddedness and social capital [10]\none would postulate that s is more likely to link to v1 than to v2 .\nHowever, the result of an edge (u1 , u2 ) is that when \u03b1 > 0, v2\nends up with a higher PageRank score than v1 . This is somewhat\ncounterintuitive, as v1 somehow seems \"more connected\" to s than\nv2 . Can we remedy this in a natural way?\nOne solution could be that carefully setting \u03b1 resolves the issue. However, there is no value of \u03b1 > 0 which will make the\nscore of v1 higher than v2 and changing to other simple teleporting schemes (such as a random jump to a random node) does not\nhelp either. However, a simple correction that works is to add the\nnumber of friends a node w has in common with s, and use this\nas an additional feature \u03b3 on each edge (u, w). If we apply this to\nthe graph shown in Figure 6, and set the weight along each edge to\n1 + \u03b3, then the PageRank score pv1 of node v1 is 1.9 greater than\nof v2 (as opposed to 0.1 smaller as in Fig 6).\nIn practice, we find that introducing this additional feature \u03b3\nhelps on the Facebook graph. In Facebook, connection (u1 , u2 )\nincreases the probability of a link forming to v1 by about 50%. In\nthe co-authorship networks, the presence of (u1 , u2 ) actually decreases the link formation probability by 37%. Such behavior of\n\n0.825\n\n20\n\n0.82\n\nTraining AUC\nTest AUC\nTraining Loss\nTesting Loss\n\nAUC\n\nExtension: Edge types. The Supervised Random Walks framework we have presented so far captures the idea that some edges\nare stronger than others. However, it doesn't allow for different\ntypes of edges. For instance, it might be that an edge (u, v) between s's friends u and v should be treated differently than the\nedge (s, u) between s and u. Our model can easily capture this\nidea by declaring different edges to be of different types, and learning a different set of feature weights w for each edge type. We can\ntake the same approach to learning each of these weights, computing partial derivatives with respect to each one weight. The price\nfor this is potential overfitting and slower runtime.\nIn our experiments, we find that dividing the edges up into multiple types provides significant benefit. Given a seed node s we label\nthe edges according to the hop-distance from s of their endpoints,\ne.g., edges (s, u) are of type (0,1), edges (u, v) are either of type\n(1,1) (if both u and v link to s) or (1,2) (if v does not link to s).\nSince the nodes are at distance 0, 1, or 2 from s, there are 6 possible edge types: (0,1), (1,0), (1,1), (1,2), (2,1) and (2,2). While\nlearning six sets of more parameters w increases the runtime, using\nmultiple edge types gives a significant increase in performance.\n\n20.5\n\n0.815\n\n0.81\n\n19.5\nLoss\n\nfor how \"far\" the walk wanders from seed node s before it restarts\nand jumps back to s. High values of \u03b1 give very short and local\nrandom walks, while low values allow the walk to go farther away.\nWhen evaluating on real data we observe that \u03b1 plays an important role in the simple unweighted case when we ignore the edge\nstrengths, but as we give the algorithm more power to assign different strengths to edges, the role of \u03b1 diminishes, and we see no\nsignificant difference in performance for a broad range of choices\n\u03b1. Figure 5 illustrates this; in the unweighted case (i.e., ignoring\nedge strengths) \u03b1 = 0.3 performs best, while in the weighted case\na broad range from 0.3 to 0.7 seem to do about equally well.\n\n19\n\n18.5\n\n0.805\n0\n\n10\n\n20\n\n30\n\n40 50 60\nIteration\n\n70\n\n80\n\n18\n90 100\n\nFigure 7: Performance of Supervised Random Walks as a function of the number of steps of parameter estimation procedure.\nco-authorship networks can be explained by the argument that long\nrange weak ties help in access to new information [14] (i.e., s is\nmore likely to link to v2 than v1 of Fig 6). Having two independent\npaths is a stronger connection in the co-authorship graph, as this\nindicates that s has written papers with two people, on two different occasions, and both of these people have written with the target\nv, also on two different occasions. Thus, there must be at least\nfour papers between these four people when the edge (u1 , u2 ) is\nabsent, and there may be as few as two when it is present. Note this\nis exactly the opposite to the social capital argument [10], which\npostulates that individuals who are well embedded in a network or\na community have higher trust and get more support and information. This is interesting as it shows that Facebook is about social\ncontacts, norms, trying to fit in and be well embedded in a circle of\nfriends, while co-authorship networks are about access to information and establishing long-range weak ties.\n\n5.2 Experiments on real data\nNext we evaluate the predictive performance of Supervised Random Walks (SRW) on real datasets. We examine the performance\nof the parameter estimation and then compare Supervised Random\nWalks to other link-prediction methods.\nParameter estimation. Figure 7 shows the results of gradient descent on the Facebook dataset. At iteration 0, we start with unweighted random walks, by setting w = 0. Using L-BFGS we\nperform gradient descent on the WMW loss. Notice the strong correlation between AUC and WMW loss, i.e., as the value of the loss\ndecreases, AUC increases. We also note that the method basically\nconverges in only about 25 iterations.\nComparison to other methods. Next we compare the predictive\nperformance of Supervised Random Walks (SRW) to a number of\nsimple unsupervised baselines, along with two supervised machine\nlearning methods. All results are evaluated by creating two inde-\n\n\fAUC\n0.63831\n0.60570\n0.59370\n0.56522\n0.60961\n0.59302\n0.63711\n0.56213\n0.61820\n0.64754\n0.58732\n0.64644\n0.67237\n0.67426\n0.69996\n0.71238\n\nPrec@20\n3.41\n3.13\n3.11\n3.05\n3.54\n3.69\n3.95\n1.72\n3.77\n3.19\n3.27\n3.81\n2.78\n3.82\n4.24\n4.25\n\nTable 2: Hep-Ph co-authorship network. DT: decision tree, LR:\nlogistic regression, and SRW: Supervised Random Walks.\nLearning Method\nRandom Walk with Restart\nAdamic-Adar\nCommon Friends\nDegree\nDT: Node features\nDT: Network features\nDT: Node+Network\nDT: Path features\nDT: All features\nLR: Node features\nLR: Network features\nLR: Node+Network\nLR: Path features\nLR: All features\nSRW: one edge type\nSRW: multiple edge types\n\nAUC\n0.81725\n0.81586\n0.80054\n0.58535\n0.59248\n0.76979\n0.76217\n0.62836\n0.72986\n0.54134\n0.80560\n0.80280\n0.51418\n0.81681\n0.82502\n0.82799\n\nPrec@20\n6.80\n7.35\n7.35\n3.25\n2.38\n5.38\n5.86\n2.46\n5.34\n1.38\n7.56\n7.56\n0.74\n7.52\n6.87\n7.57\n\nTable 3: Results for the Facebook dataset.\npendent datasets, one for training and one for testing. Each performance value is the average over all of the graphs in the test set.\nFigure 8 shows the ROC curve for Astro-Ph dataset, comparing our method to an unweighted random walk. Note that much\nof the improvement in the curve comes in the area near the origin, corresponding to the nodes with the highest predicted values.\nThis is the area that we most care about, i.e., since we can only\ndisplay/recommend about 20 potential target nodes to a Facebook\nuser we want the top of the ranking to be particularly good (and do\nnot care about errors towards the bottom of the ranking).\nWe compare the Supervised Random Walks to unsupervised linkprediction methods: plain Random Walk with Restarts, AdamicAdar score [1], number of common friends, and node degree. For\nsupervised machine learning methods we experiments with decision trees and logistic regression and group the features used for\ntraining them into three groups:\n\u2022 Network features: unweighted random walk scores, AdamicAdar score, number of common friends, and degrees of nodes\ns and the potential target c \u2208 C\n\u2022 Node features: average of the edge features for those edges\nincident to the nodes s and c \u2208 C, as described in Section 4\n\u2022 Path features: averaged edge features over all paths between\nseed s and the potential destination c.\n\nDataset\nCo-authorship Astro-Ph\nCo-authorship Cond-Mat\nCo-authorship Hep-Ph\nCo-authorship Hep-Th\nFacebook (Iceland)\n\nAUC\nSRW\nLR\n0.70548 0.67639\n0.74173 0.71672\n0.71238 0.67426\n0.72505 0.69428\n0.82799 0.81681\n\nPrec@20\nSRW\nLR\n2.55 2.15\n2.54 2.61\n4.18 3.82\n2.59 2.61\n7.57 7.52\n\nTable 4: Results for all datasets. We compare favorably to logistic features as run on all features. Our Supervised Random\nWalks (SRW) perform significantly better than the baseline in\nall cases on ROC area. The variance is too high on the Top20\nmetric, and the two methods are statistically tied on this metric.\nROC Curve for SRW and RW w/ Restart\n1\nRandom Walk w/ Restarts\nSupervised Randow Walk\n\n0.8\n\nFraction Positives\n\nLearning Method\nRandom Walk with Restart\nAdamic-Adar\nCommon Friends\nDegree\nDT: Node features\nDT: Network features\nDT: Node+Network\nDT: Path features\nDT: All features\nLR: Node features\nLR: Network features\nLR: Node+Network\nLR: Path features\nLR: All features\nSRW: one edge type\nSRW: multiple edge types\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nFraction Negatives\n\nFigure 8: ROC curve of Astro-Ph test data.\nTables 2 and 3 compare the results of various methods on the\nHep-Ph co-authorship and Facebook networks. In general, we note\nvery performance of Supervised Random Walks (SRW): AUC is in\nthe range 0.7\u20130.8 and precision at top 20 is between 4.2\u20137.6. We\nconsider this surprisingly good performance. For example, in case\nof Facebook this means that out of 20 friendships we recommend\nnearly 40% of them realize in near future.\nOverall, Supervised Random Walks (SRW) give a significant improvement over the unweighted Random Walk with Restarts (RWR).\nSRW also gives gains over other techniques such as logistic regression which combine features. For example, in co-authorship\nnetwork (Tab. 2) we note that unsupervised RWR outperforms decision trees and slightly trails logistic regression in terms of AUC\nand Prec@20. Supervised Random Walks outperform all methods.\nIn terms of AUC we get 6% and in terms of Prec@20 near 12%\nrelative improvement. In Facebook (Tab. 3), Random Walk with\nRestarts already gives near-optimal AUC, while Supervised Random Walks still obtain 11% relative improvement in Prec@20.\nIt is important to note that, in addition to outperforming the other\nmethods, Supervised Random Walks do so without the tedious process of feature extraction. There are many network features relating\npairs of unconnected nodes (Adamic-Adar was the best out of the\ndozens examined in [21], for example). Instead, we need only select the set of node and edge attributes, and Supervised Random\nWalks take care of determining how to combine them with the network structure to make predictions.\nLast, Table 4 compares the performance of top two methods: Supervised Random Walks and logistic regression. We note that Supervised Random Walks compare favorably to logistic regression.\nAs logistic regression requires state of the art network feature extraction and Supervised Random Walks outperforms it out of the\nbox and without any ad hoc feature engineering.\nWhen we examine the weights assigned, we find that for Facebook the largest weights are those which are related to time. This\n\n\fmakes sense as if a user has just made a new friend u, she is\nlikely to have also recently met some of u's friends. In the coauthorship networks, we find that the number of co-authored papers and the cosine similarity amongst titles were the features with\nhighest weights.\nRuntime. While the exact runtime of Supervised Random Walks\nis highly dependent on the graph structure and features used, we\ngive some rough guidelines. The results here are for single runs on\na single 2.3Ghz processor on the Facebook dataset.\nWhen putting all edges in the same category, we have 8 weights\nto learn. It took 98 iterations of the quasi-Newton method to converge and minimize the loss. This required computing the PageRanks of all the nodes in all the graphs (100 of them) 123 times,\nalong with the partial derivatives of each of the 8 parameters 123\ntimes. On average, each PageRank computation took 13.2 steps\nof power-iteration before converging, while each partial derivative\ncomputation took 6.3 iterations. Each iteration for PageRank or\nits derivative takes O(|E|). Overall, the parameter estimation on\nFacebook network took 96 minutes. By contrast, increasing the\nnumber of edge types to 6 (which gives best performance) required\nlearning 48 weights, and increased the training time to 13 hours on\nthe Facebook dataset.\n\n6.\n\nCONCLUSION\n\nWe have proposed Supervised Random Walks, a new learning algorithm for link prediction and link recommendation. By utilizing\nnode and edge attribute data our method guides the random walks\ntowards the desired target nodes. Experiments on Facebook and coauthorship networks demonstrate good generalization and overall\nperformance of Supervised Random Walks. The resulting predictions show large improvements over Random Walks with Restarts\nand compare favorably to supervised machine learning techniques\nthat require tedious feature extraction and generation. In contrast,\nour approach requires no network feature generation and in a principled way combines rich node and edge features with the structure\nof the network to make reliable predictions.\nSupervised Random Walks are not limited to link prediction, and\ncan be applied to many other problems that require learning to rank\nnodes in a graph, like recommendations, anomaly detection, missing link, and expertise search and ranking.\nAcknowledgements. We thank Soumen Chakrabarti for discussion. Research was in-part supported by NSF CNS-1010921, NSF\nIIS-1016909, AFRL FA8650-10-C-7058, Albert Yu & Mary Bechmann Foundation, IBM, Lightspeed, Microsoft and Yahoo.\n\n7.\n\nREFERENCES\n\n[1] L. Adamic and E. Adar. Friends and neighbors on the web.\nSocial Networks, 25(3):211\u2013230, 2003.\n[2] A. Agarwal and S. Chakrabarti. Learning random walks to\nrank nodes in graphs. In ICML '07, pages 9\u201316, 2007.\n[3] A. Agarwal, S. Chakrabarti, and S. Aggarwal. Learning to\nrank networked entities. In KDD '06, pages 14\u201323, 2006.\n[4] A. Andrew. Iterative computation of derivatives of\neigenvalues and eigenvectors. IMA Journal of Applied\nMathematics, 24(2):209\u2013218, 1979.\n[5] A. L. Andrew. Convergence of an iterative method for\nderivatives of eigensystems. Journal of Computational\nPhysics, 26:107\u2013112, 1978.\n[6] L. Backstrom, D. P. Huttenlocher, J. M. Kleinberg, and\nX. Lan. Group formation in large social networks:\nmembership, growth, and evolution. In KDD '06, pages\n44\u201354, 2006.\n\n[7] A.-L. Barab\u00e1si and R. Albert. Emergence of scaling in\nrandom networks. Science, 286:509\u2013512, 1999.\n[8] A. Blum, H. Chan, and M. Rwebangira. A random-surfer\nweb-graph model. In ANALCO '06, 2006.\n[9] A. Clauset, C. Moore, and M. E. J. Newman. Hierarchical\nstructure and the prediction of missing links in networks.\nNature, 453(7191):98\u2013101, May 2008.\n[10] J. Coleman. Social Capital in the Creation of Human Capital.\nThe American Journal of Sociology, 94:S95\u2013S120, 1988.\n[11] M. Diligenti, M. Gori, and M. Maggini. Learning web page\nscores by error back-propagation. In IJCAI '05, 2005.\n[12] J. Gehrke, P. Ginsparg, and J. M. Kleinberg. Overview of the\n2003 kdd cup. SIGKDD Explorations, 5(2):149\u2013151, 2003.\n[13] M. Gomez-Rodriguez, J. Leskovec, and A. Krause. Inferring\nnetworks of diffusion and influence. In KDD '10, 2010.\n[14] M. S. Granovetter. The strength of weak ties. American\nJournal of Sociology, 78:1360\u20131380, 1973.\n[15] T. H. Haveliwala. Topic-sensitive pagerank. In WWW '02,\npages 517\u2013526, 2002.\n[16] K. Henderson and T. Eliassi-Rad. Applying latent dirichlet\nallocation to group discovery in large graphs. In SAC '09,\npages 1456\u20131461.\n[17] G. Jeh and J. Widom. Scaling personalized web search. In\nWWW '03, pages 271\u2013279, 2003.\n[18] R. Kumar, P. Raghavan, S. Rajagopalan, D. Sivakumar,\nA. Tomkins, and E. Upfal. Stochastic models for the web\ngraph. In FOCS '00, page 57, 2000.\n[19] J. Leskovec, L. Backstrom, R. Kumar, and A. Tomkins.\nMicroscopic evolution of social networks. In KDD '08,\npages 462\u2013470, 2008.\n[20] J. Leskovec, J. M. Kleinberg, and C. Faloutsos. Graphs over\ntime: densification laws, shrinking diameters and possible\nexplanations. In KDD '05, pages 177\u2013187, 2005.\n[21] D. Liben-Nowell and J. Kleinberg. The link prediction\nproblem for social networks. In CIKM '03, pages 556\u2013559,\n2003.\n[22] D. Liu and J. Nocedal. On the limited memory bfgs method\nfor large scale optimization. Mathematical Programming,\n45:503\u2013528, 1989. 10.1007/BF01589116.\n[23] R. Minkov and W. W. Cohen. Learning to rank typed graph\nwalks: Local and global approaches. In WebKDD/SNA-KDD\n'07, pages 1\u20138, 2007.\n[24] S. Myers and J. Leskovec. On the convexity of latent social\nnetwork inference. In NIPS '10, 2010.\n[25] L. Page, S. Brin, R. Motwani, and T. Winograd. The\npagerank citation ranking: Bringing order to the web.\nTechnical report, Stanford Dig. Lib. Tech. Proj., 1998.\n[26] A. Popescul, R. Popescul, and L. H. Ungar. Statistical\nrelational learning for link prediction, 2003.\n[27] P. Sarkar and A. W. Moore. Fast dynamic reranking in large\ngraphs. In WWW '09, pages 31\u201340, 2009.\n[28] B. Taskar, M. F. Wong, P. Abbeel, and D. Koller. Link\nprediction in relational data. In NIPS '03, 2003.\n[29] H. Tong and C. Faloutsos. Center-piece subgraphs: problem\ndefinition and fast solutions. In KDD '06, pages 404\u2013413,\n2006.\n[30] H. Tong, C. Faloutsos, and Y. Koren. Fast direction-aware\nproximity for graph mining. In KDD '07, pages 747\u2013756,\n2007.\n[31] T. Tong, C. Faloutsos, and J.-Y. Pan. Fast randomwalk with\nrestart and its applications. In ICDM '06, 2006.\n[32] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.\nOptimizing classifier performance via an approximation to\nthe wilcoxon-mann-whitney statistic. In ICML '03, pages\n848\u2013855, 2003.\n\n\f"}