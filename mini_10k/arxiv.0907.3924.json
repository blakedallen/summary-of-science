{"id": "http://arxiv.org/abs/0907.3924v1", "guidislink": true, "updated": "2009-07-22T20:15:11Z", "updated_parsed": [2009, 7, 22, 20, 15, 11, 2, 203, 0], "published": "2009-07-22T20:15:11Z", "published_parsed": [2009, 7, 22, 20, 15, 11, 2, 203, 0], "title": "Timescales of spike-train correlation for neural oscillators with common\n  drive", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0907.1623%2C0907.4547%2C0907.0958%2C0907.3441%2C0907.4075%2C0907.0578%2C0907.2961%2C0907.1610%2C0907.4901%2C0907.3521%2C0907.5416%2C0907.4247%2C0907.1457%2C0907.2176%2C0907.3022%2C0907.3113%2C0907.2686%2C0907.2431%2C0907.2415%2C0907.3600%2C0907.4246%2C0907.3040%2C0907.0848%2C0907.3175%2C0907.5335%2C0907.3370%2C0907.0698%2C0907.0937%2C0907.0239%2C0907.3181%2C0907.3101%2C0907.2564%2C0907.1883%2C0907.1483%2C0907.5432%2C0907.2720%2C0907.3456%2C0907.3657%2C0907.4007%2C0907.1451%2C0907.4167%2C0907.2820%2C0907.5182%2C0907.3882%2C0907.0312%2C0907.3760%2C0907.4010%2C0907.3815%2C0907.4803%2C0907.5597%2C0907.1754%2C0907.0649%2C0907.0722%2C0907.3614%2C0907.3924%2C0907.4221%2C0907.0065%2C0907.3391%2C0907.1621%2C0907.2323%2C0907.0734%2C0907.0335%2C0907.1908%2C0907.2156%2C0907.1758%2C0907.0378%2C0907.0972%2C0907.1951%2C0907.4078%2C0907.4480%2C0907.1113%2C0907.4407%2C0907.1241%2C0907.3892%2C0907.4249%2C0907.3985%2C0907.0830%2C0907.4988%2C0907.4499%2C0907.4187%2C0907.4158%2C0907.4064%2C0907.3188%2C0907.0008%2C0907.2641%2C0907.3926%2C0907.0501%2C0907.5328%2C0907.2242%2C0907.3260%2C0907.5021%2C0907.4304%2C0907.5511%2C0907.2950%2C0907.5344%2C0907.1853%2C0907.4853%2C0907.2616%2C0907.2363%2C0907.3217%2C0907.1435&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Timescales of spike-train correlation for neural oscillators with common\n  drive"}, "summary": "We examine the effect of the phase-resetting curve (PRC) on the transfer of\ncorrelated input signals into correlated output spikes in a class of neural\nmodels receiving noisy, super-threshold stimulation. We use linear response\ntheory to approximate the spike correlation coefficient in terms of moments of\nthe associated exit time problem, and contrast the results for Type I vs. Type\nII models and across the different timescales over which spike correlations can\nbe assessed. We find that, on long timescales, Type I oscillators transfer\ncorrelations much more efficiently than Type II oscillators. On short\ntimescales this trend reverses, with the relative efficiency switching at a\ntimescale that depends on the mean and standard deviation of input currents.\nThis switch occurs over timescales that could be exploited by downstream\ncircuits.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0907.1623%2C0907.4547%2C0907.0958%2C0907.3441%2C0907.4075%2C0907.0578%2C0907.2961%2C0907.1610%2C0907.4901%2C0907.3521%2C0907.5416%2C0907.4247%2C0907.1457%2C0907.2176%2C0907.3022%2C0907.3113%2C0907.2686%2C0907.2431%2C0907.2415%2C0907.3600%2C0907.4246%2C0907.3040%2C0907.0848%2C0907.3175%2C0907.5335%2C0907.3370%2C0907.0698%2C0907.0937%2C0907.0239%2C0907.3181%2C0907.3101%2C0907.2564%2C0907.1883%2C0907.1483%2C0907.5432%2C0907.2720%2C0907.3456%2C0907.3657%2C0907.4007%2C0907.1451%2C0907.4167%2C0907.2820%2C0907.5182%2C0907.3882%2C0907.0312%2C0907.3760%2C0907.4010%2C0907.3815%2C0907.4803%2C0907.5597%2C0907.1754%2C0907.0649%2C0907.0722%2C0907.3614%2C0907.3924%2C0907.4221%2C0907.0065%2C0907.3391%2C0907.1621%2C0907.2323%2C0907.0734%2C0907.0335%2C0907.1908%2C0907.2156%2C0907.1758%2C0907.0378%2C0907.0972%2C0907.1951%2C0907.4078%2C0907.4480%2C0907.1113%2C0907.4407%2C0907.1241%2C0907.3892%2C0907.4249%2C0907.3985%2C0907.0830%2C0907.4988%2C0907.4499%2C0907.4187%2C0907.4158%2C0907.4064%2C0907.3188%2C0907.0008%2C0907.2641%2C0907.3926%2C0907.0501%2C0907.5328%2C0907.2242%2C0907.3260%2C0907.5021%2C0907.4304%2C0907.5511%2C0907.2950%2C0907.5344%2C0907.1853%2C0907.4853%2C0907.2616%2C0907.2363%2C0907.3217%2C0907.1435&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We examine the effect of the phase-resetting curve (PRC) on the transfer of\ncorrelated input signals into correlated output spikes in a class of neural\nmodels receiving noisy, super-threshold stimulation. We use linear response\ntheory to approximate the spike correlation coefficient in terms of moments of\nthe associated exit time problem, and contrast the results for Type I vs. Type\nII models and across the different timescales over which spike correlations can\nbe assessed. We find that, on long timescales, Type I oscillators transfer\ncorrelations much more efficiently than Type II oscillators. On short\ntimescales this trend reverses, with the relative efficiency switching at a\ntimescale that depends on the mean and standard deviation of input currents.\nThis switch occurs over timescales that could be exploited by downstream\ncircuits."}, "authors": ["Andrea K. Barreiro", "Eric Shea-Brown", "Evan L. Thilo"], "author_detail": {"name": "Evan L. Thilo"}, "author": "Evan L. Thilo", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1103/PhysRevE.81.011916", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0907.3924v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0907.3924v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "q-bio.NC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "q-bio.NC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.QM", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0907.3924v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0907.3924v1", "arxiv_comment": null, "journal_reference": "Physical Review E, vol. 81: 011916 (2010)", "doi": "10.1103/PhysRevE.81.011916", "fulltext": "arXiv:0907.3924v1 [q-bio.NC] 22 Jul 2009\n\nTimescales of spike-train correlation for neural\noscillators with common drive\nAndrea K. Barreiro, Eric Shea-Brown, Evan L. Thilo\u2217\nOctober 18, 2018\n\nAbstract\nWe examine the effect of the phase-resetting curve (PRC) on the\ntransfer of correlated input signals into correlated output spikes in\na class of neural models receiving noisy, super-threshold stimulation.\nWe use linear response theory to approximate the spike correlation\ncoefficient in terms of moments of the associated exit time problem,\nand contrast the results for Type I vs. Type II models and across the\ndifferent timescales over which spike correlations can be assessed. We\nfind that, on long timescales, Type I oscillators transfer correlations\nmuch more efficiently than Type II oscillators. On short timescales this\ntrend reverses, with the relative efficiency switching at a timescale that\ndepends on the mean and standard deviation of input currents. This\nswitch occurs over timescales that could be exploited by downstream\ncircuits.\n\n1\n\nIntroduction\n\nThroughout the nervous system, neurons produce spike trains that are correlated from cell-to-cell. This correlation, or synchrony, has received major interest because of its impact on how neural populations encode information. For example, correlations can strongly limit the fidelity of a\nneural code as measured by the signal-to-noise ratio of homogeneous populations [51, 23, 8, 4]. However, the presence or stimulus-dependence of\ncorrelations can also enhance coding strategies that rely on discriminating\namong competing populations [2, 3, 39]; in general, the effects of correlation\non coding are complex and can be surprisingly strong [39, 12, 36, 33, 3,\n\u2217\n\nDepartment of Applied Mathematics, University of Washington, Box 352420, Seattle,\nWA 98195\n\n1\n\n\f44, 26, 2, 46, 47, 49, 43]. In addition, stimulus-dependent correlations can\nmodulate or directly carry information directly [15, 42, 25, 14, 21, 5, 11, 24].\nCorrelations also play a major role in how signals are transmitted from\nlayer-to-layer in the brain [41, 27, 28].\nWhat is the origin of correlated spiking? One natural mechanism is the\noverlap in the inputs to different neurons \u2013 these common inputs can drive\ncommon output spikes. This poses the question: how does the process of\ntransferring of input correlations to spike train correlations depend on the\nnonlinear dynamics of individual neurons? Such correlation transfer has\nbeen modeled in integrate-and-fire type neurons [6, 32, 14, 48, 45, 28] and,\nvery recently, in phase reductions of neural oscillators [31, 19].\nIn particular, [31, 19] contrast the correlated activity evoked in neural\noscillators with Type I (i.e., always positive) vs. Type II (i.e., positive and\nnegative) PRCs. When correlations are measured via equilibrium probability distributions of pairs of neuron phases, or via cross-correlation functions\nof these phases over time, Type II oscillators display relatively higher levels\nof correlation [31, 19]. These results for phase correlation imply a similar\nfinding for spike train correlation in the limit of very short timescales (the\nconnection arises because the spike train cross-correlation function at \u03c4 = 0\ncan be related to the probability that phases will be nearly coincident for\nthe two cells [35].)\nIn this study, we also contrast correlation transfer in Type I and Type II\noscillators (as well as in a continuum of intermediate models). The primary\nextension that we make is to study spike train correlation over a range of\ndifferent timescales. Specifically, we measure the correlation coefficient \u03c1T\nbetween the number of spikes produced by a pair of neurons in a time window\nof length T :\n\u03c1T\n\n=\n\np\n\nCov(n1 , n2 )\np\n;\nV ar(n1 ) V ar(n2 )\n\n(1)\n\nHere, n1 , n2 are the numbers of spikes output by neurons 1 and 2 respectively\nin the time window; see Fig. 1 for an illustration.\nWe first derive a tractable expression for \u03c1T in the long timescale limit\nT \u2192 \u221e (cf. [14, 48]). This can be given in terms of moments of an associated\nexit time problem. This reveals a dramatically higher level of long-timescale\ncorrelation transfer in Type I vs. Type II neural oscillator models, the\nopposite of what was found in the earlier studies over short timescales (see\nFig. 1). Next, we study \u03c1T for successively shorter T , recovering the earlier\nfindings of [19, 31], and noting the critical timescale below which Type\nII neurons become more efficient at transferring correlations. Additional\n2\n\n\fFigure 1: (a) A schematic of the setup and correlation metric used in this\nstudy. (b) The principal result of our study; that correlation transfer efficiency depends on both internal dynamics and timescale of readout, with\nrelative efficiency between Type I (light red) and Type II (dark blue) switching as readout timescale changes. The graph shows \u03c1T for a particular set\nof model parameters vs. logarithm of the time window log(T ). Insets show\nthe phase-resetting curves used for the Type I and Type II models.\nresults on how correlation transfer depends on neurons' operating range \u2013\nthat is, their spike rate and coefficient of variation (CV) of spiking \u2013 are\ndeveloped as we go along.\n\n2\n2.1\n\nModels of neural oscillators and correlation transfer\nPhase reductions\n\nNeural oscillators can be classified into two types based on their intrinsic\ndynamics. Both types have the feature that as applied inputs (or \"injected\ncurrent\") increases, the system transitions from a stable rest state to periodic\nfiring through a bifurcation, the nature of which defines the type [38]. Here,\nas is often taken to be the case, Type I neurons undergo a saddle-node on\ninvariant circle bifurcation, in which two fixed points collide and disappear,\nproducing a periodic orbit that can have arbitrarily low frequency. Type\nII neurons undergo either a subcritical or supercritical Hopf bifurcation, in\nwhich a periodic orbit emerges at a non-zero minimum frequency.\nOnce the oscillator has passed through this bifurcation, it can be described by a single equation for its phase, in which inputs are mediated\nthrough a phase-resetting curve (PRC) which indicates the degree to which\nan input advances or delays the next spike. A PRC is derived for a mathematical model by phase reduction methods and determined experimentally\n3\n\n\fby repeated perturbations of a system by input \"kicks\" [50]. Several investigators have demonstrated a connection between the type of bifurcation\nand the shape of the PRC: a Type I oscillator PRC is everywhere positive,\nso that positive injected current advances the time of the next spike [17],\nwhereas a Type II PRC has both positive and negative regions, so positive\ninputs advance or delay the next spike depending on their timing ( [18]; see\nalso [9]). Moreover, the form of Type I and Type II phase-resetting curves\nnear the bifurcation are given by (1 \u2212 cos \u03b8) and \u2212 sin(\u03b8) respectively. We\ninvestigate these two PRCs, together with a family of PRCs given by a linear\ncombination of these prototypical examples:\nZ(\u03b8) = \u2212\u03b1 sin(\u03b8) + (1 \u2212 \u03b1)(1 \u2212 cos(\u03b8)),\n\n0\u2264\u03b1\u22641 .\n\n(2)\n\nHere \u03b1 homotopes the PRC from \"purely\" Type I to Type II; along the way,\nintermediate PRCs more representative of phase reductions of biophysical\nmodels are encountered [17, 9, 22].\nThe question of how oscillators with different PRCs synchronize when\nthey are coupled has been the subject of extensive study. Here, we ask about\na different mechanisms by which such oscillators can become correlated.\nSpecifically, we consider an uncoupled pair of neurons receiving partially\ncorrelated noise. The dynamics have been reduced to a phase oscillator;\neach neuron is represented by a phase only. Each phase \u03b8i , is governed by\nthe stochastic differential equation\n\u221a\n\u221a\nd\u03b8i = \u03c9 dt + \u03c3Z(\u03b8i ) \u25e6 ( 1 \u2212 c dWti + c dWtc ),\n\u03b8i \u2208 (0, 2\u03c0)\n(3)\nwhere \u03c9, \u03c3 > 0, \u25e6 denotes the Stratonovich integral, and\nZ(\u03b8) = \u2212\u03b1 sin(\u03b8) + (1 \u2212 \u03b1)(1 \u2212 cos(\u03b8)) .\nEach \u03b8i receives independent white noise, dWti , and common white noise\ndWtc is received by both. The noises are weighted so that the total variance\nof the noise terms in (3) is \u03c3 2 . For the remainder of the paper, we treat the\nequivalent It\u00f4 integral\nd\u03b8i = (\u03c9 +\n\n2.2\n\n\u03c32\nZ(\u03b8i )Z 0 (\u03b8i )) dt + \u03c3Z(\u03b8i )dWt ,\n2\n\n\u03b8i \u2208 (0, 2\u03c0).\n\n(4)\n\nMeasuring spike train correlation\n\nWe record spike times as those times tki at which \u03b8i crosses 2\u03c0 [17, 9, 18].\nBecause Z(2\u03c0) = 0 and \u03c9 > 0 for all the models we consider, \u03b8 always\n4\n\n\fcontinues through 2\u03c0 and begins the next P\nperiod of inter-spike dynamics.\nWe consider the output spike trains yi (t) = i \u03b4(t\u2212tki ), where tki is the time\nof the kth spike of the ith neuron. The firing rate of the ith cell, hyi (t)i, is\ndenoted \u03bdi . As a quantitative measure of correlation over a given time scale\nT , we compute the following statistic:\n\u03c1T\n\n=\n\nCov(n1 , n2 )\np\np\nV ar(n1 ) V ar(n2 )\n\nwhere n1 , n2 are the numbers of spikes output by\nneurons 1 and 2 respecR t+T\ntively in a time window of length T ; i.e. ni (t) = t\nyi (s) ds.\nOne can show that this is equivalent to\nC12 (t) T \u2212|t|\nT dt\nR\nT\nT \u2212|t|\nC11 (t) T \u2212|t|\nT dt \u2212T C22 (t) T dt\nRT\n\n\u03c1T\n\n\u2212T\n\n=\n\nqR\nT\n\u2212T\n\n(5)\n\nwhere Cij (\u03c4 ) = hyi (t)yj (t + \u03c4 )i \u2212 \u03bdi \u03bdj [13]. It will be convenient for us to\nanalyze the system in the Fourier domain. By the Wiener-Khinchin theorem,\nwe can write (5) in terms of the power spectra Pij \u2261 h\u0177i\u2217 \u0177j i as\nR\u221e\n\u2212\u221e P12 (f )KT (f ) df\n(6)\n\u03c1T = qR\nR\u221e\n\u221e\nP\n(f\n)K\n(f\n)\ndf\nP\n(f\n)K\n(f\n)\ndf\n22\n11\nT\nT\n\u2212\u221e\n\u2212\u221e\nwhere the kernel KT is\nKT (f ) =\n\n3\n3.1\n\n4\nsin2\nTf2\n\n\u0012\n\nTf\n2\n\n\u0013\n.\n\nCorrelation in the long timescale limit\nLinear Response Theory for \u03c1T\n\nWe recall the following derivation from [29, 14, 48]. Assume that the fraction\nof noise variance c of the correlated input noise is small; then we treat the\nsystem with common noise as a perturbation to the system without common\nnoise. Because the common noise is small, we will assume that the response\nof the system can be treated by linear response theory; that is in the Fourier\ndomain it can be characterized by a susceptibility function A\u03c9,\u03c3 (f ) which\ngives the scaling factor between input and response at frequency f .\n\n5\n\n\fSpecifically, we make the ansatz [29] that the Fourier transform can be\nwritten to lowest order in c\n\u221a\n\u0177i (f ) = \u01770,i (f ) + cA\u03c9,\u03c3 (f )Q\u0302(f )\nwhere y0,i is the spike output of the neuron without correlated noise, Q\u0302(f )\nis the Fourier transform of the correlated noise, and A is the susceptibility\nfunction. Then the cross-spectrum of the two spike trains, P12 (f ), satisfies\nP12 (f ) = c|A\u03c9,\u03c3 (f )|2 hQ\u0302\u2217 Q\u0302i\n= c\u03c3 2 |A\u03c9,\u03c3 (f )|2\nas the base spike trains are independent of each other and the correlated\nnoise Q, and taking the variance of Q to be \u03c3 2 . Then at any finite T , \u03c1T is\nlinear in c, and (cf. [48, 4]):\n\u03c1T (\u03c9, \u03c3) = cST (\u03c9, \u03c3)\n= c qR\n\n(7)\n\u03c32\n\nR\u221e\n\n2\n\u2212\u221e |A\u03c9,\u03c3 (f )| KT (f ) df\n\n\u221e\n\u2212\u221e P11 (f )KT (f ) df\n\nR\u221e\n\n.\n\n(8)\n\n\u2212\u221e P22 (f )KT (f ) df\n\nNote that ST (\u03c9, \u03c3) multiplies the input correlation c to yield the spike train\ncorrelation; for this reason we refer to ST (\u03c9, \u03c3) as the correlation gain. We\nrecover a simple expression for (6) in the limit T \u2192 \u221e. In the numerator,\nwe converge to the value of the integrand at 0 as f \u2192 \u221e:\nc\u03c3 2 |A\u03c9,\u03c3 (0)|2\nAs A\u03c9,\u03c3 (0) is the limit of the susceptibility function as the frequency becomes\narbitrarily small, it must be equivalent to the ratio of the DC response of\nthe system (that is, the firing rate \u03bd) to the strength of a constant DC\ninput. Later we will use the symbol \u03bc to include a DC input explicitly for\nthe purposes of this computation. Therefore we define\nd\u03bd\nd\u03bc\n\n\u2261 A\u03c9,\u03c3 (0)\n\n(9)\n\nThe denominator converges to P11 (0) (assuming the unperturbed oscillators\nto be statistically identical) which for renewal processes is simply CV 2 \u03bd.\nPutting these results together, as T \u2192 \u221e, the finite-time correlations\nsatisfy\nd\u03bd 2\n\u03c3 2 ( d\u03bc\n)\n\u2261 cS(\u03c9, \u03c3)\n(10)\nlim \u03c1T = c\n2\nT \u2192\u221e\nCV \u03bd\n6\n\n\fwhere \u03bd is the mean output firing rate, CV the coefficient of variation of\nthe interspike intervals, \u03c3 the input noise amplitude. Each quantity can be\ncomputed from statistics of a single oscillator, and combined to yield the\nlong-time correlation gain S(\u03c9, \u03c3).\n\n3.2\n\nComputing moments of the exit time problem\n\nd\u03bd\nThe quantities \u03bd, CV , and d\u03bc\nare related by moments of the interspike\nintervals (ISI), and as such can be computed using the associated exit time\nproblem. Specifically,\n\n1\nT1 (0)\np\n(T2 (0) \u2212 (T1 (0))2 )\n=\nT1 (0)\n1\ndT1 (0)\n= \u2212\n2\n(T1 (0)) d\u03bc\n\n\u03bd =\nCV\nd\u03bd\nd\u03bc\n\n(11)\n(12)\n(13)\n\nwhere T1 (0) is the average time to cross \u03b8 = 2\u03c0, given a start at \u03b80 = 0 (in\nother words, given a start at the last spike), and T2 (0) is the second moment\nof this same quantity.\nT1 (x) and T2 (x) are given by solutions of the adjoint equation [20]\n1\nfi (x) = A(x)\u2202x Ti + B(x)\u2202x2 Ti\n2\n\n(14)\n\nwhere\nf1 (x) = \u22121\n\n(15)\n\nf2 (x) = \u22122T1 (x)\n\u03c32\nA(x) = \u03c9 + Z(x)Z 0 (x)\n2\n2\nB(x) = \u03c3 Z(x)2\n\n(16)\n(17)\n(18)\n\ni\nand boundary conditions are given by Ti (2\u03c0) = 0, \u2202T\n\u2202x bounded at x = 0.\nThe solution can be obtained by integrating equation (14) twice over the\nrange [0, 2\u03c0], using the integrating factor \u03a8:\n\u0012Z x\n\u0013\n0\n0 2A(x )\n\u03a8(x) = exp\ndx\n,\n\u03a8(0) = 0 .\n(19)\nB(x0 )\n\n7\n\n\fIf \u03b1 = 0, then B(x) has no interior zero and we proceed as follows:\n[\u03a8\n\n\u2202Ti 0\n] =\n\u2202x\n\u2202Ti\n=\n\u2202x\n\n2fi (x)\n\u03a8(x)\nB(x)\nZ x\n2fi (x0 )\n1\n\u03a8(x0 ) dx0 .\n\u03a8(x) 0 B(x0 )\n\n(20)\n(21)\n\nIf \u03b1 > 0 then B(x) has an interior zero at \u03c7(\u03b1) and we integrate separately\non (0, \u03c7(\u03b1)) and (\u03c7(\u03b1), 2\u03c0):\n(\nR x 2fi (x0 )\n1\n0\n0\nx < \u03c7(\u03b1)\n\u2202Ti\n\u03a8(x) 0 B(x0 ) \u03a8(x ) dx ,\nR\n(22)\n(x) =\n0\nx\n2fi (x )\n1\n0\n0\n\u2202x\n\u03a8(x) \u03c7(\u03b1) B(x0 ) \u03a8(x ) dx , x > \u03c7(\u03b1)\nFinally Ti (0) is given by a second integration:\nZ 0\n\u2202Ti\nTi (0) =\n(x) dx .\n2\u03c0 \u2202x\n\n(23)\n\nHere, the argument of the exponential function in \u03a8(x) can be any\nantiderivative of 2A(x)\nB(x) . \u03a8(0) (and \u03a8(\u03c7(\u03b1)), if \u03b1 > 0) is zero because\nthe adjoint equation (14) has an irregular singularity at x = 0 (and at\nR x 2A(x)\ndx B(x) \u2192 \u2212\u221e as x \u2192 0+\nx = \u03c7(\u03b1)); consequently 2A(x)\n\u2192\n\u221e\nand\nB(x)\n+\n(and as x \u2192 \u03c7(\u03b1) ). This accounts for the difference between (23) and, for\nexample, Eqn. (5.2.157) in [20] 1 . Note that, also because Z(0) = 0 and\n\u03c9 > 0, x = 0 is an entrance boundary, so any exit must take place at 2\u03c0\n(i.e. an exit from the interval (0, 2\u03c0) is equivalent to a spike).\nFor the class of PRCs that we consider, the antiderivative in (19) can be\nevaluated symbolically, therefore \u03a8(x) can be evaluated analytically. The\nintegrals in (21, 23) must be evaluated by numerical quadrature.\nWe next discuss the integrability of (21) and (22). First, we consider the\ncase of the theta neuron (\u03b1 = 0), for which B(x) has only two zeros; at 0 and\n2\u03c0. We can confirm the integrability by checking the following conditions:\nZ x\n2f (x0 )\nlim\n\u03a8(x0 )dx0 = 0\n(24)\nx\u21920+ 0 B(x0 )\nlim \u03a8(x) = 0\n(25)\nx\u21920+\nZ x\n2f (x0 )\nlim\n\u03a8(x0 )dx0 = \u00b1\u221e\n(26)\nx\u21922\u03c0 \u2212 0 B(x0 )\nlim \u03a8(x) = \u221e\n(27)\nx\u21922\u03c0 \u2212\n\n1\n\nThe expression we obtain is identical to the evaluation of this expression in the case\nof a reflecting boundary at 0.\n\n8\n\n\fIf these are satisfied then by l'Hospital's rule,\nZ x\n\u2202Ti\n2fi (x0 )\n1\n=\n\u03a8(x0 )dx0\n\u2202x\n\u03a8(x) 0 B(x0 )\n\n(28)\n\nis finite at the endpoints; in fact\n\u2202Ti\nx\u21920 \u2202x\n\u2202Ti\nlim\nx\u21922\u03c0 \u2202x\n\nfi (x)\nx\u21920 A(x)\nfi (x)\n= lim\nx\u21922\u03c0 A(x)\n\nlim\n\n=\n\nlim\n\n(29)\n(30)\n\nand we can use a quadrature method that can handle integrable singularities.\nFor \u03b1 > 0, B(x) has one interior zero, dividing the domain into two intervals\non which (14) has irregular singularities at each end. \u03a8 must be computed\ni\nseparately on each interval. Again we find that \u2202T\n\u2202x is finite on each interval,\npermitting computation of (22) with a standard quadrature routine.\nFinally, we compute the derivative of the firing rate with respect to a\n\u2202\u03bd\nDC input; i.e. \u2202\u03bc\nfor the system\nd\u03b8 = \u03c9 dt + Z(\u03b8)(\u03bc dt + \u03c3 \u25e6 dWt ),\n\n\u03b8 \u2208 [0, 2\u03c0)\n\n(31)\n\nwhich is equivalent to the It\u00f4 SDE\nd\u03b8 = (\u03c9 +\n\n\u03c32\nZ(\u03b8)Z 0 (\u03b8) + \u03bcZ(\u03b8)) dt + \u03c3Z(\u03b8)dWt ,\n2\n\n\u03b8 \u2208 [0, 2\u03c0) .\n\n(32)\n\nWe wish to differentiate \u03bd with respect to \u03bc and evaluate at \u03bc = 0. According\n1\nto equation (13) we must evaluate dT\nd\u03bc (0, \u03bc) where the drift term A(x, \u03bc) =\n2\n\n\u03c9 + \u03c32 Z(x)Z 0 (x) + \u03bcZ(x) is a function of both x and \u03bc. In general, T1 ,\nT2 and \u03a8 are also functions of two arguments (e.g. T1 (x, \u03bc)) and we have\n\u2202\nindicated the arguments where needed for clarity. The notation \u2202x\nwill refer\nto differentiation with respect to the first argument.\nWe first consider the case \u03b1 = 0. Rewriting (23), we have\nZ\n\n0\n\nT1 (0, \u03bc) =\n2\u03c0\n\n1\n\u03a8(x, \u03bc)\n\nwhere\n\n\u0014Z\n\nZ\n0\nx\n\n\u03a8(x, \u03bc) = exp\n\n9\n\nx\n\n\u22122\n\u03a8(x0 , \u03bc)dx0 dx\nB(x0 )\n\n2A(x0 , \u03bc) 0\ndx\nB(x0 )\n\n(33)\n\n\u0015\n.\n\n(34)\n\n\fTherefore,\n\u0012Z 0 Z x\n\u0013\n\u22122 \u03a8(x0 , \u03bc) 0\nd\ndx dx\n0\nd\u03bc\n2\u03c0 0 B(x ) \u03a8(x, \u03bc)\nZ 0Z x\n\u22122 \u03a8\u03bc (x0 , \u03bc)\u03a8(x, \u03bc) \u2212 \u03a8\u03bc (x, \u03bc)\u03a8(x0 , \u03bc) 0\ndx dx\n(35)\n=\n0\n\u03a8(x, \u03bc)2\n2\u03c0 0 B(x )\n\n\u2202T1\n(0, \u03bc) =\n\u2202\u03bc\n\nWe use the relationship\nZ\n\u03a8\u03bc (x, \u03bc) = \u03a8(x, \u03bc)\n0\n\nx\n\n2 \u2202A(y, \u03bc)\ndy\nB(y) \u2202\u03bc\n\n(36)\n\nto find that\nZ\n2 \u03a8(x0 , \u03bc) x 2 \u2202A(y, \u03bc)\ndy dx0 dx\n0 ) \u03a8(x, \u03bc)\nB(x\nB(y)\n\u2202\u03bc\n0\nx\n2\u03c0 0\nZ 0Z xZ y\n2 \u03a8(x0 , \u03bc) 2 \u2202A(y, \u03bc) 0\n=\ndx dy dx (37)\n0\n\u2202\u03bc\n2\u03c0 0\n0 B(x ) \u03a8(x, \u03bc) B(y)\n\n\u2202T1\n(0, \u03bc) =\n\u2202\u03bc\n\nZ\n\n0\n\nZ\n\nx\n\nassuming that the order of integration over x0 and y can be switched. By\nTonelli's theorem, this is valid if the integrand is single-signed. For \u03b1 = 0,\nthe integrand is always nonnegative (note that \u2202A\n\u2202\u03bc (x) = Z(x) and that B(x)\nis nonnegative).\nNext, we establish the integrability of the expression (37). Rewriting,\nwe have\nZ 0\nZ x\nZ y\n1\n2 \u2202A\n2\n\u2202T1\n1\n(0, \u03bc) =\n\u03a8(y, \u03bc) \u00d7\n\u03a8(x0 , \u03bc)dx0 dy dx\n0)\n\u2202\u03bc\n\u03a8(x,\n\u03bc)\nB(y)\n\u2202\u03bc\n\u03a8(y,\n\u03bc)\nB(x\n2\u03c0\n0\n0\nZ x\nZ 0\n2 \u2202A\n\u2202T1\n1\n\u03a8(y, \u03bc) \u00d7\n(y)dy dx\n=\n\u2202x\n2\u03c0 \u03a8(x, \u03bc) 0 B(y) \u2202\u03bc\n\u2202T1\nAs \u2202A\n\u2202\u03bc (x) = Z(x) and \u2202x are bounded, we can use the same conditions for\nintegrability unchanged from (24 - 27).\nA parallel derivation to (33 - 37) can be made when \u03b1 > 0. In this case\nwe have\n\u0012\n\u0013\nZ 0\n\u2202T1\n\u2202 \u2202T1\n(0, \u03bc) =\n(x, \u03bc) dx\n(38)\n\u2202\u03bc\n\u2202x\n2\u03c0 \u2202\u03bc\n\nwhere we have used the boundary condition T1 (2\u03c0, \u03bc) = 0. The integrand\nis given by\n(R x R y\n2 \u03a8(x0 ,\u03bc) 2 \u2202A(y,\u03bc)\n0\nx < \u03c7(\u03b1)\n\u2202 \u2202T1\n\u2202\u03bc dx dy,\n0 0 B(x0 ) \u03a8(x,\u03bc) B(y)\n(x, \u03bc) = R x R y\n(39)\n0 ,\u03bc)\n\u03a8(x\n\u2202A(y,\u03bc)\n2\n2\n0 dy, x > \u03c7(\u03b1)\n\u2202\u03bc \u2202x\ndx\n0\n\u2202\u03bc\n\u03c7(\u03b1) \u03c7(\u03b1) B(x ) \u03a8(x,\u03bc) B(y)\n10\n\n\fHere, the switch in the order of integration is justified by the fact that the\nintegrand \u2202A\n\u2202\u03bc is positive on (0, \u03c7(\u03b1)) and negative on (\u03c7(\u03b1), 2\u03c0), while the\nrange of integration in Eqn. (39) is always restricted to lie in one region or\nthe other.\n\n3.3\n\nPatterns of correlation transfer over long timescales\n\nFigure 2: Firing rate (left) and CV (right) of the theta model neuron over\na range of parameters \u03c9 and \u03c3. Spike trains are illustrated for four sets\nof (\u03c9, \u03c3) values (see white squares), notably mean-driven (high \u03c9, low \u03c3 bottom spike train) and fluctuation-driven (low \u03c9, high \u03c3 - top spike train).\nLevel sets of \u03c3\u0303 are plotted (see text).\nHaving derived and shown how to evaluate formula (10), we next use it\nto evaluate spike count correlations \u03c1. The results are formally valid in the\nlimits of timescale T \u2192 \u221e and input correlation c \u2192 0, so we also conduct\nMonte Carlo simulations to reveal behavior of \u03c1T for large but finite T and\nintermediate input correlation up to c = 0.3, and to test the applicability of\nour formula in these regimes.\nWe compute these spike correlations over a range of \u03c9 and \u03c3 values that\nexplores a full dynamical regime of the model. By this we mean that the\nvalues we use span from dominantly mean-driven firing ( e.g. \u03c9 = 2.5,\n\u03c3 = 0.4 at lower-right white square in Fig. 2), to dominantly fluctuationdriven firing (e.g. \u03c9 = 0.4, \u03c3 = 2.4 at upper-left white square), and all\n11\n\n\fFigure 3: (Left) Susceptibility for \u03b1 = 0 (top), \u03b1 = 0.5 (middle), and\n\u03b1 = 1 (bottom) over a range of parameters \u03c9 and \u03c3; lines are level sets of\n\u221a\n\u03c3\u0303 = \u03c3/ \u03c9. (Right) \u03c1T vs. c from Monte Carlo simulations for \u03b1 = 0 (top),\n\u03b1 = 0.5, and \u03b1 = 1 (bottom). Specific (\u03c9, \u03c3) values shown as in Figure 2.\n\n12\n\n\fintermediate possibilities.\nThe resulting output firing rate \u03bd, computed via Eqn. (11), ranges from\n0 to 0.9 (measured in spikes per time unit); see Fig. 2 (left panel). Note\nthat \u03bd increases strongly with \u03c9 and only weakly with \u03c3 (see Sec. 3.4). The\nCV (via Eqn. (12)) ranges from 0 to 0.55 (Fig. 2, right panel). We were\nunable to reach higher values of CV even with a much expanded range of\n\u03c3. By using a time change in the equations, CV can be seen to depend on\n\u221a\nthe input parameters \u03c9 and \u03c3 only through the relationship \u03c3\u0303 = \u03c3/ \u03c9 (see\nlater in this section) and is therefore invariant on level curves of this ratio.\nAs Fig. 2 shows, CV increases with this ratio.\nWe first fix a moderately long time window T = 32, corresponding to\n1.6\u221216 interspike intervals for the parameter range at hand, and compute \u03c1T\nfrom Monte Carlo simulations for a range of correlation strengths c \u2208 [0, 0.3];\nsee Fig. 3 (right column). We see that, for Type I models, \u03c1T is close to\nlinear in this range of c, as for the linear response theory. For Type II\nmodels, linearity holds over a decreased range of c. Additionally, note that\nthe limiting formula Eqn. (10) for \u03c1 gives a close approximation to \u03c1T =32\nfor Type I models in more fluctuation-driven regimes. The approximation\nis worse for dominantly mean-driven firing, and for Type II models, but the\ntrend that correlations are lower for Type II models is correctly predicted\nby Eqn. 10.\nNext, we discuss the trends in \u03c1 predicted by the linear response theory.\nTwo findings stand out in the left hand panels of Fig. 3. First, values of\nS(\u03c9, \u03c3) (and hence \u03c1 \u2248 S c) are much larger for Type I (\u03b1 = 0) than for\nType II (\u03b1 = 1) models. Second, S(\u03c9, \u03c3) is nearly constant as the input\nmean and standard deviation \u03c9 and \u03c3 vary over a wide range, for both the\nType I and Type II models. In sum, Type I models transfer \u2248 66% of\ntheir input correlations into spike correlations over long timescales; Type II\nmodels transfer none of these input correlations, producing long-timescale\nspike counts that are uncorrelated. Additionally, an intermediate model\n(\u03b1 = 1/2) transfers an intermediate level of correlations, and these levels do\ndepend on \u03c9 and \u03c3. We will provide a partial explanation for overall trends\nin correlation transfer with \u03b1 in Section 3.4.\nFig. 4 provides an alternative view of these results, by plotting the correlation gain S vs. the firing rate and CV that are evoked by input parameters\ndrawn from the whole range of \u03c9, \u03c3. First, note that S does not vary with\nfiring rate for the Type I or Type II models, as expected from the previous\nplots. For the intermediate (\u03b1 = 1/2) model, S does not display a clear\nfunctional relationship with firing rate, but there is such a relationship with\nCV (Fig. 4(b)). We note that all of these findings for the phase oscilla13\n\n\fFigure 4: Susceptibility vs. \u03bd (a) and susceptibility vs. CV (b) for \u03b1 = 0\n(light gray), 0.5 (medium gray), 1 (black).\ntors under study are in contrast to the behavior of linear integrate and fire\nneurons, which produce a strongly increasing, nearly functional relationship\nwith firing rate [14, 48]; we revisit this point in the discussion.\nNow, we discuss a scaling relationship for the underlying equations that\nsimplifies the parameter dependence and helps to explain the plots of S vs.\n\u03bd and S vs. CV. This symmetry (which was noted in [30] for the quadratic\nintegrate and fire model), allows us to reduce the free parameters \u03c9, \u03c3 to\n\u221a\none parameter \u03c3\u0303 \u2261 \u03c3/ \u03c9. The stochastic differential equation\n\u0013\n\u0012\n\u03c32\n0\nd\u03b8t =\n\u03c9 + Z(\u03b8)Z (\u03b8) + \u03bcZ(\u03b8) dt + \u03c3Z(\u03b8)dWt\n2\nbecomes, under a time change \u03c4 = \u03c9t,\n\u0012\n\u0013\n\u03c32\n\u03bc\n\u03c3\n0\nd\u03b8\u03c4 =\n1+\nZ(\u03b8)Z (\u03b8) + Z(\u03b8) d\u03c4 + \u221a Z(\u03b8)dW\u03c4\n2\u03c9\n\u03c9\n\u03c9\n\u0012\n\u0013\n2\n\u03c3\u0303\n\u03bc\n=\n1 + Z(\u03b8)Z 0 (\u03b8) + Z(\u03b8) d\u03c4 + \u03c3\u0303Z(\u03b8)dW\u03c4\n2\n\u03c9\n\n(40)\n(41)\n\nEach exit time must scale identically under this transformation, so that the\nexit time moments scale as\n\u03bd\nT\u03031 = \u03c9T1 \u2192 \u03bd\u0303 =\n(42)\n\u03c9\nT\u03032 = \u03c9 2 T2\n(43)\n14\n\n\fq\nand therefore the CV = T\u03032 \u2212 T\u030312 /T\u03031 is invariant under the time change.\nThus, the CV, which is computed with \u03bc = 0, depends only on \u03c3\u0303, not on \u03c9\nand \u03c3 separately. Now consider the two sets of parameters (\u03c9, \u03c3) and (1, \u03c3\u0303),\n\u221a\nsuch that \u03c3\u0303 = \u03c3/ \u03c9. According to (41), the effect of \u03bc is scaled by 1/\u03c9 so\nthat\nd\u03bd\u0303\nd\u03bc\n\n=\n\n1 d\u03bd\n.\n\u03c9 d\u03bc\n\n(44)\n\nTherefore, the susceptibility\nS(\u03c9, \u03c3) =\n=\n\n=\n\nd\u03bd 2\n\u03c3 2 ( d\u03bc\n)\n\nCV 2 \u03bd\nd\u03bd\u0303 2\n\u03c3 2 \u03c912 ( d\u03bc\n)\n \u0303 2\nCV\n\n\u03bd\u0303\n\u03c9\nd\u03bd\u0303 2\n\u03c3 2 ( d\u03bc )\n2\n\u03c9\n\n \u0303 \u03bd\u0303\nCV\n= S(1, \u03c3\u0303)\n\n(45)\n(46)\n\n(47)\n(48)\n\n\u221a\nis invariant under the change of parameters (\u03c9, \u03c3) \u2192 (1, \u03c3/ \u03c9); exactly the\nsame change of parameters under which the CV is conserved. Therefore,\nif there is a single contour for each value of CV (i.e., there are no disconnected contours), we expect that S will be a function of CV, as in Fig. 4(b).\nConversely, note that the firing rate will vary widely over any particular\n\u03c3\u0303 contour, so that many different firing rates can be expected to yield the\nsame S; unless S is constant, we expect a given firing rate to be associated\nwith a range of S values, as also seen in Fig. 4 (a).\nFinally, in Fig. 5, we demonstrate that the behavior of S seems to\nroughly \"interpolate\" between the \u03b1 = 0, 1/2, 1 cases considered here as\nit varies over it whole range of \u03b1. S typically decreases as the total noise\nvariance \u03c3 increases.\n\n3.4\n\nAnalytical arguments for Type I vs. Type II difference\nin correlation gain S\n\nWe now seek to explain the drop in S(\u03c9, \u03c3) (Eqn. (10)) as \u03b1 increases,\nranging from Type I (\u03b1 = 0) to Type II (\u03b1 = 1) PRCs. There are two\ntractable limits in which explicit calculations can be performed. First, we\nshow that S(\u03c9, \u03c3) = 0 for \"purely\" Type II models, for any values of \u03c9 and\n15\n\n\fFigure 5: S(1, \u03c3) for a continuum of models \u03b1 \u2208 [0, 1].\n\u03c3. Next, for arbitrary \u03b1, we derive an S(\u03c9, \u03c3) valid to first order in \u03c3 2 \u2013 this\nreveals a monotonic decrease in S with \u03b1 and gives a good description of\ncorrelation transfer even for \u03c3 \u2248 1. Finally, we given an intuitive argument\nto buttress these calculations.\nS = 0 for purely Type II models\nWe start with \u03b1 = 1. It is straightforward to see that d\u03bd/d\u03bc = 0 for any\n\u03c9, \u03c3. Recall that d\u03bd/d\u03bc is given by the integral of a function over the\ninterval (0, 2\u03c0), in Eqn. (38); we will show this function integrates to zero.\nRewriting Eqn. (39), using \u03c7(1) = \u03c0,\n(\nR x 2 \u2202A\n\u2202T1\n1\n\u2202 \u2202T1\n\u03a8(x,\u03bc) 0 B(y) \u2202\u03bc (y, \u03bc)\u03a8(y, \u03bc) \u00d7 \u2202x (y, \u03bc) dy, x < \u03c0\nR x 2 \u2202A\n=\n(49)\n\u2202T1\n1\n\u2202\u03bc \u2202x\n\u03a8(x,\u03bc) \u03c0 B(y) \u2202\u03bc (y, \u03bc)\u03a8(y, \u03bc) \u00d7 \u2202x (y, \u03bc) dy, x > \u03c0.\n1\n\u03a8(x, \u03bc),B(x), and \u2202T\n\u2202x (x, \u03bc) are each \u03c0-periodic (i.e. f (x) = f (x + \u03c0)).\n\u2202A\n\u2202\u03bc \u2261 Z(x) = \u2212 sin(x), however, is anti-periodic (f (x) = \u2212f (x + \u03c0)). Then\n\n\u2202 \u2202T1\n\u2202 \u2202T1\n(x, \u03bc) = \u2212\n(x + \u03c0, \u03bc)\n(50)\n\u2202\u03bc \u2202x\n\u2202\u03bc \u2202x\n\u0010\n\u0011\n\u2202T1\n\u2202\nand integrating \u2202\u03bc\n(x,\n\u03bc)\nover a full period x \u2208 (0, 2\u03c0) yields zero.\n\u2202x\nPlugging this into Eqn. (10), we see that S(\u03c9, \u03c3) \u2261 0 for models with Type\nII PRCs Z(\u03b8) = \u2212 sin(\u03b8) (\u03bd and CV are always nonzero in this paper).\n\n16\n\n\fEvaluation of S(\u03c9, \u03c3) in the limit \u03c3 \u2192 0\nWe next derive an analytical expression for S(\u03c9, \u03c3) in the limit \u03c3 \u2192 0.\n1\nWe will show that each relevant term T1 (0), T2 (0), and \u2202T\n\u2202\u03bc (0) admits an\nasymptotic expansion in the small parameter \u03c3 2 . We compute these terms\nexplicitly and combine them to get the first term of the associated expansion\nfor S(\u03c9, \u03c3).\nIf we examine the integral in (21) and the inner integral of (35) for \u03b1 = 0,\nor the integrals in (22), and (39) for \u03b1 > 0, we see that we can write each\nof them in the form\n\u0012\n\u0013\nZ y\n1\n1\n0\nf (x0 ) exp\n(F\n(x\n)\n\u2212\nF\n(y))\ndx0\n(51)\nZ(y) a\n\u03c32\nwhere F (x0 ) is strictly increasing on (a, y). a may be either 0 or \u03c7\u03b1 , depending on the circumstance. This integral admits an asymptotic expansion in\n\u03c3 2 , with successive terms essentially given by integrating by parts and retaining only the contribution from the end of the interval where F (x0 )\u2212F (y) = 0.\nIn the case of T1 (x) we have\nf (y) =\n\n1\n.\nZ(y)\n\nf (y) =\n\nT1 (y)\nZ(y)\n\nIn the case of T2 (x) we have\n\nand for d\u03bd/d\u03bc\nf (y) =\n\n\u2202T1\n(y).\n\u2202x\n\n2\u03c9\nIn each case F (x0 ) is given by the antiderivative of Z(x\n0 )2 ; because this\n0\nis a positive function on (0, \u03c7(\u03b1)) and (\u03c7(\u03b1), 2\u03c0), F (x ) must clearly be\nincreasing on these intervals. The integral\nZ b\n\u0001\nf (x0 ) exp \u03bb\u03c6(x0 ) dx0\n(52)\na\n\nadmits the following expansion for \u03bb \u2192 \u221e, if \u03c60 > 0 on [a, b] and f meets\ncertain conditions (see, for example, [7]):\n\u0014\n\u0014\n\u0015\n\u0015b\n\u221e\nX\n(\u22121)n\n1 d n f (x)\nI(\u03bb) \u223c\nexp (\u03bb\u03c6(x)) 0\n(53)\n\u03bbn+1\n\u03c6 (x) dx \u03c60 (x) a\nn=0\n\n17\n\n\fIf \u03c6(x) \u2192 \u2212\u221e as x \u2192 a, as is the case in our use of the formula, only the\nright-hand endpoint makes a contribution to the integral.\nSubstituting this contribution into the outer integral and evaluating\nthese quantities to the required order, we find\n2\u03c0\n+ O(\u03c3 4 )\n\u03c9\n\u0010\u03c0\n\u0001\u0011\n4\u03c0 2\n2\n2\n+\n\u03c3\n3\n\u2212\n6\u03b1\n+\n4\u03b1\n+ O(\u03c3 4 )\nT2 (0) =\n\u03c92\n\u03c93\ndT1\n2\u03c0\n(0) = \u2212 (1 \u2212 \u03b1) + O(\u03c3 4 )\nd\u03bc\n\u03c9\nT1 (0) =\n\n(54)\n(55)\n(56)\n\nIn passing, we note that the firing rate gain, d\u03bd/d\u03bc, is given by\nd\u03bd\nd\u03bc\n\n1 dT1\n(0)\nT1 (0)2 d\u03bc\n1\u2212\u03b1\n+ O(\u03c3 4 )\n2\u03c0\n\n= \u2212\n\n(57)\n\n=\n\n(58)\n\nPutting these results together we see that\nS(\u03c9, \u03c3) =\n\n2(1 \u2212 \u03b1)2\n+ O(\u03c3 2 )\n3 \u2212 6\u03b1 + 4\u03b12\n\n(59)\n\nIt can be readily checked that this function decreases monotonically from a\nvalue of 2/3 at \u03b1 = 0, to a value of 0 at \u03b1 = 1. While this calculation is in\nthe limit \u03c3 \u2192 0+ , it in fact remains a good approximation for moderate \u03c3, in\nfact, even for \u03c3 \u223c \u03c9. Figure 6 shows the limiting value as well as computed\nS values at small (relative to \u03c9 = 1; \u03c3 = 0.2) and moderate (\u03c3 = 1) values of\n\u03c3. The limiting value remains a good approximation throughout this range.\n\nAn argument for general PRCs\nWe close this by section by noting that, for arbitrary PRCs z(\u03b8) and small\n\u03c3,\nZ 2\u03c0\nd\u03bd/d\u03bc \u221d\nZ(\u03b8) d\u03b8 .\n0\n2\n\n2\n\nThe calculations showing this are in the appendix. While S(\u03c9, \u03c3) = \u03c3 (d\u03bd/d\u03bc)\nCV 2 \u03bd\nhas other terms that depend on the PRC, this calculation does suggest that\nS is likely to be smaller for PRCs with lower means in general, and lends\nsome intuition to what drives the decrease in S with \u03b1.\n\n18\n\n\fFigure 6: lim\u03c3\u21920+ S(1, \u03c3) for a continuum of models \u03b1 \u2208 [0, 1] (black\ndashed). This is a good approximation for small (\u03c3 = 0.2; dark gray) and\nmoderate (\u03c3 = 1; light gray) values of \u03c3.\n\n4\n\nCorrelation over shorter timescales\n\nFigure 7 shows \u03c1T computed for a sequence of finite time windows T . We\ncan characterize the non-dimensional T in terms of its length in terms of a\ntypical interspike interval (ISI) of the oscillator. The time window T = 1\nvaries from 0.05 - 0.5 ISI, roughly, from left to right; the time window T = 32\nvaries from 1.6 \u2212 16 ISI.\nWe see a striking dependence of transferred correlations on T . \u03c132 is\nlarger for Type I than for Type II oscillators for most parameter values,\nconsistent with our long time results.\nHowever, \u03c11 (\u03c9, \u03c3) is smaller for Type I than for Type II for 95 % of\nparameter pairs (\u03c9, \u03c3). This is consistent with recent results on the response\nof phase oscillators to correlated noise [31, 19]; in particular, [31] study the\ndistribution of phase difference \u2206\u03b8 \u2261 \u03b81 \u2212 \u03b82 between two oscillators driven\nby common noise and find that the probability that \u2206\u03b8 = 0 is greater for\nType II than for Type I. As noted in the Introduction, this metric can be\nshown to have a direct relationship with our \u03c1T as T \u2192 0. To summarize,\nthe \"switch\" in ST (\u03c9, \u03c3) (from higher correlation gain in Type II models\nto higher correlation gain in Type I) occurs at time scales T over which\neach cell fires several spikes; such timescales are biologically relevant, as we\ndiscuss in the next section.\n\n19\n\n\fFigure 7: \u03c1T (\u03c9, \u03c3) as measured from Monte Carlo simulations for an increasing sequence of T , for simulations with \u03b1 = 0 (top row), \u03b1 = 0.5 (middle)\nand \u03b1 = 1 (bottom). The fraction of common variance is c = 0.1; therefore,\nto recover the approximate ST (\u03c9, \u03c3) \u2248 \u03c1T (\u03c9, \u03c3)/c, multiply by 10. The \u03c9\nand \u03c3 axes of each plot are the same. As T increases, ST approaches the\nT \u2192 \u221e limit illustrated in Fig. 3.\n\n5\n\nSummary and discussion\n\nWe asked how correlated input currents are transferred into correlated spike\ntrains in a class of nonlinear phase models that are generic reductions of\nneural oscillators. Linear response methods, asymptotics, and Monte Carlo\nsimulations gave the following answers:\n1. Over long timescales, Type I oscillators transfer 66% of incoming current correlations into correlated spike counts, while Type II oscillators\ntransfer almost none of their input correlations into spike count correlations. Models with intermediate phase response curves transfer\nintermediate levels of correlations.\n2. Over long timescales, correlation transfer in Type I and Type II models\nis independent of the rate and coefficient of variation (CV) of spiking.\n20\n\n\fFor intermediate models, correlation transfer decreases with CV and\nshows no clear dependence on rate.\n3. That there is a timescale T beneath which these results reverse: Type\nII neurons become more efficient at transferring correlations than Type\nI, there is an increasing dependence of correlation transfer on spike\nrate, and the strong dependence on CV weakens.\nWe note that results (1) and (2) are highly distinct from findings for the leaky\nintegrate-and-fire neuron model, for which up to 90-100% of correlations\nare transferred over long timescales, with this level depending strongly on\nfiring rate but very weakly on CV [14, 48]. This demonstrates a strong\nrole for subthreshold nonlinearities in determining correlation transfer in\nthe oscillatory regime (as seen for the quadratic integrate-and-fire model\nin [48]).\nWhat timescales of spike count correlation actually matter in a given\napplication? This depends on the circuit that is \"downstream\" of the pair\n(or, similarly, layer) of neural oscillators that we have studied in this paper; in other words, on what system is reading out the neurons we study\nhere. Clearly, different neurons and networks are sensitive to input fluctuations over widely varying timescales. For example, some single neurons\nand circuits can respond only to events in which many of the cells that provide inputs spike nearly simultaneously. This is the coincidence detector\nmode of operation (cf. [40] and references therin), and can result from fast\nmembrane time constants (as occur in high-conductance states [16]); circuit\nmechanisms, such as feed-forward inhibition [37], can also play a role. For\nsuch systems, short-timescale correlations among upstream cells are relevant\n\u2013 small window lengths T . On the opposite extreme, networks operating as\nneural integrators will accumulate inputs over arbitrarily long timescales\n(see [10] and references therein); in this case, spike-time correlations over\nlarge windows T are reflected in circuit activity. In general there is a range\nof possible behaviors, and the timescales over which inputs are integrated\ncan differ among various components of a network, among components of an\nindividual cell [37, 40], or among different times in a cell lifetime, depending\non background input characteristics [16].\nOne domain in which different levels of correlation transfer \u2013 and different dependences of this correlation transfer on neurons' operating ranges\n\u2013 can have a strong effect is the population coding of sensory stimuli. For\nexample, if neurons are read out over long timescales, then Type I vs Type\nII populations offer a choice between relatively high and low levels of correlation across the population. Depending on heterogeneity of the population\n21\n\n\fresponse to the stimulus at hand, one or the other of these choices can\nyield dramatically greater (Fisher) information about the encoded stimulus [51, 1, 49]. The opposite choice of neuron type would be preferred for\nreadout over short timescales, where trends in correlation transfer reverse.\nBeyond averaged levels of correlation, a separate question that can affect\nencoding is whether correlations depend on the stimulus [34, 24]. A natural way that this can occur is when correlations depend on the evoked\nrate or CV of firing. We demonstrate that such dependencies are present in\nphase models over short timescales and, over long timescales, that they are\npresent in intermediate but not \"purely\" Type-I or Type-II models. Once\nagain, depending on details of stimulus encoding, these dependencies can\neither enhance or degrade encoding. Overall, the picture that emerges is\nthat correlation transfer is another factor to consider in asking which nonlinearities allow neuron models to best encode stimuli, and that there will\nbe different answers for different stimuli.\nIn closing, we note that we have studied only simple (but widely-used)\none-dimensional neural models here. However, preliminary simulations suggest that the trends for correlation transfer found here also hold in some\nstandard Type-I vs. Type-II conductance-based neuron models [38]. The\nsituation is more complex, as global features of the neural dynamics can be\ninvolved, and will be explored in future work.\n\n22\n\n\f6\n\nAppendix\n\nIn this appendix we give some more details about calculation of T1 (0), T2 (0),\n1\nand dT\nd\u03bc (0) for specific values of \u03b1.\n\n6.1\n\n\u03b1 = 1 and numerical details\n\nFor Z(x) = \u2212 sin(x) (\u03b1 = 1),\n\u03c32\nsin(x) cos(x)\n2\nB(x) = \u03c3 2 [sin(x)]2\nA(x) = \u03c9 +\n\n(60)\n(61)\n\nare periodic on [0, \u03c0] with B(x) = 0 at 0(2\u03c0) and \u03c0.\n\u03a8(x) is defined as an anti-derivative as in Eqn. (19); we can compute\nthis symbolically yielding\n\u03a8(x) = sin(x) exp(\u2212\n\n2\u03c9\ncot(x)).\n\u03c32\n\n(62)\n\nWe can check that limx\u21920+ \u03a8(x) = 0, limx\u2192\u03c0\u2212 \u03a8(x) = \u221e, and that \u03a8/B is\nnot integrable at either 0 or \u03c0.\nFor the interval \u03c0 to 2\u03c0 we use the fact that A(x) and B(x) are \u03c0-periodic;\n\u03a8 repeats on this second interval; that is\n\u0013\n\u0012\n2\u03c9\n(63)\n\u03a8(x) = | sin(x)| exp \u2212 2 cot(x)\n\u03c3\nis an expression that is valid everywhere Z(\u03b8) 6= 0.\n1\nTo compute values of \u2202T\n\u2202x on a uniform mesh, we use an adaptive quadrature routine to evaluate Eqns. (22); in particular Simpson's rule implemented via the MATLAB routine quad. We have already demonstrated\nthe integrability of Eqns. (22) in \u00a73.2. T1 (0) is then computed using Simpson's 3-point rule.\n2\nTo compute values of \u2202T\n\u2202x , we use adaptive quadrature as well, with the\n1\ncaveat that \u2202T\n\u2202x is evaluated in between mesh points by linear interpolation.\n\n6.2\n\n\u03b1=0\n\nFor the theta model, Z(x) = 1 \u2212 cos(x) or \u03b1 = 0,\n\u03c32\n(1 \u2212 cos(x)) sin(x)\n2\nB(x) = \u03c3 2 [1 \u2212 cos(x)]2\nA(x) = \u03c9 +\n\n23\n\n(64)\n(65)\n\n\fare periodic on [0, 2\u03c0] with B(x) = 0 at 0(2\u03c0). Again we can integrate\n2A(x)/B(x) symbolically, finding\n\u0012\n\u0013\n2\u03c9 (2 \u2212 cos(x)) sin(x)\n\u03a8 = |1 \u2212 cos(x)| exp \u2212 2\n(66)\n3\u03c3\n(1 \u2212 cos(x))2\nWe can verify Eqn. (24) as before.\n6.2.1\n\nA small \u03c3 argument for general PRC\n\nHere we replicate the small \u03c3 value of d\u03bd/d\u03bc, using a perturbation expansion\nvalid for an arbitrary PRC.\nWe consider the stationary densities p(\u03b8) and p\u0302(\u03b8) of two different processes,\nd\u03b8 = a(\u03b8)dt + b(\u03b8)dWt\n\n(67)\n\nd\u03b8\u0302 = \u00e2(\u03b8)dt + b(\u03b8)dWt\n\n(68)\n\nwhere\n\u03c32\nZ(\u03b8)Z 0 (\u03b8)\n2\n\u03c32\n\u00e2(\u03b8) = \u03c9 + Z(\u03b8)Z 0 (\u03b8) + \u03bcZ(\u03b8)\n2\n\na(\u03b8) = \u03c9 +\n\n(69)\n(70)\n\np(\u03b8) satisfies the stationary Fokker-Planck equation\n\u2212\n\n\u2202\n\u22022\n(ap) + 2 (bp) = 0\n\u2202\u03b8\n\u2202\u03b8\n\n(71)\n\nThis can be integrated and the constant of integration is equal to the firing\nrate:\nap \u2212\n\n\u2202\n(bp) = \u03bd\n\u2202\u03b8\n\n(72)\n\nand therefore\n\u03bd =\n\u03bd\u0302 =\n\nZ 2\u03c0\n1\na(\u03b8)p(\u03b8)d\u03b8\n2\u03c0 0\nZ 2\u03c0\n1\n\u00e2(\u03b8)p\u0302(\u03b8)d\u03b8\n2\u03c0 0\n\n24\n\n(73)\n(74)\n\n\fThe DC input response, d\u03bd/d\u03bc, can be computed by differentiating \u03bd\u0302 with\nrespect to \u03bc and evaluating at \u03bc = 0. Therefore we expand \u03bd\u0302 as a series in\n\u03bc and take the first-order term.\np\u0302 = p + \u03bcp1 + O(\u03bc2 )\n\n(75)\n\n2\n\n\u03bd\u0302 = \u03bd + \u03bc\u03bd1 + O(\u03bc )\n\n(76)\n\nWe find that\nd\u03bd\n= \u03bd1 =\nd\u03bc\n\n1\n2\u03c0\n\n\u0014Z\n\n2\u03c0\n\nZ\n\n2\u03c0\n\n\u0015\np1 (\u03b8)a(\u03b8)d\u03b8\n\np(\u03b8)Z(\u03b8)d\u03b8 +\n\n(77)\n\n0\n\n0\n\nWe consider (77) more carefully. We will see that all but one term are\nmultiplied by \u03c3 2 ; for small \u03c3, the term that remains significant is the mean\nof the phase-resetting curve.\nWe rewrite Eqn. (77) as follows:\n\u0014Z 2\u03c0 \u0012\n\u0013\n\u0015\nZ 2\u03c0\n1\n1\n1\n2\n0\n\u03bd1 =\n+ p\u0303(\u03b8) Z(\u03b8)d\u03b8 + \u03c3\nZ(\u03b8)Z (\u03b8)p1 (\u03b8)d\u03b8\n(78)\n2\u03c0 0\n2\u03c0\n2\n0\n\u0014Z 2\u03c0\n\u0015\nZ 2\u03c0\nZ 2\u03c0\n1\n1\n1\n2\n0\n=\nZ(\u03b8)d\u03b8 +\np\u0303(\u03b8)Z(\u03b8)d\u03b8 + \u03c3\nZ(\u03b8)Z (\u03b8)p1 (\u03b8)d\u03b8\n(79)\n2\u03c0 0 2\u03c0\n2\n0\n0\nusing the fact that p1 (\u03b8) must average to 0 so that p\u0302 remains a probability\ndensity. We have also written p(\u03b8), the stationary density for the process\nwith \u03bc = 0, as the sum of a uniform density and a deviation p\u0303(\u03b8).\nConsider the process\n\u0012\n\u0013\n\u03c32\n0\nd\u03b8 =\n\u03c9 + Z(\u03b8)Z (\u03b8) dt + \u03c3Z(\u03b8)dWt\n(80)\n2\nThe stationary density p and firing rate J satisfy\n\u0012\n\u0013\n\u0012\n\u0013\n\u2202 \u03c32 2\n\u03c32\n0\nZ (\u03b8)p(\u03b8)\n= J\n\u03c9 + Z(\u03b8)Z (\u03b8) p(\u03b8) \u2212\n2\n\u2202\u03b8 2\nIf \u03c3 = 0 then the stationary density is p0 =\nand J in powers of \u03c3 2 ;\n\n1\n2\u03c0\n\nand J0 =\n\np(\u03b8) = p0 + p\u0303\nJ\n\n\u03c9\n2\u03c0 .\n\n(81)\n\nWe expand p\n\n(82)\n\n= p0 + \u03c3 2 p\u03031 + O(\u03c3 4 )\n\n(83)\n\n2\n\n(84)\n\n4\n\n= J0 + \u03c3 J1 + O(\u03c3 )\n25\n\n\fAt O(1) we have\np0 =\n\nJ0\n\u03c9\n\n(85)\n\nas already stated. At O(\u03c3 2 )\np\u03031 =\n=\n\n\u0012\n1\nJ1 +\n\u03c9\n\u0012\n1\nJ1 +\n\u03c9\n\nZ 2 \u2202p0 1\n+ ZZ 0 p0\n2 \u2202\u03b8\n2\n\u0013\n1\nZZ 0 d\u03b8\n4\u03c0\n\n\u0013\n\nJ1 is determined so that p is a probability density at any order:\nZ 2\u03c0\nZ 2\u03c0\n1\nJ1 = 2\u03c0J1 = \u2212\nZZ 0 d\u03b8 = 0\n4\u03c0 0\n0\n\n(86)\n(87)\n\n(88)\n\nas ZZ 0 is the perfect derivative of a periodic function. For general order, we\nhave\n\u0012\n\u0012\n\u0013\u0013\n1\n1\n\u2202 Z2\n0\np\u0303n =\nJn \u2212 ZZ p\u0303n\u22121 +\np\u0303n\u22121\n(89)\n\u03c9\n2\n\u2202\u03b8 2\nZ 2\u03c0\n1\nJn =\nZZ 0 p\u0303n\u22121 d\u03b8\n(90)\n4\u03c0 0\nwhere we no longer expect Jn to be zero.\nLet's return to (79). The first integral is the mean of Z(\u03b8). The second\nintegral, in our expansion, first appears at fourth-order in \u03c3. To see this, we\nexamine\nZ 2\u03c0\nZ 2\u03c0\n1\np\u03031 Z(\u03b8)d\u03b8 =\nZ 2 (\u03b8)Z 0 (\u03b8)d\u03b8\n(91)\n4\u03c0\n0\n0\n= 0\n(92)\nThe third term is second-order in \u03c3. Thus the dominant term in (79) is the\nfirst one, and we have shown the desired result.\n\nReferences\n[1] L. F. Abbott and P. Dayan. The effect of correlated variability on the\naccuracy of a population code. Neural Comput, 11(1):91\u2013101, 1999.\n\n26\n\n\f[2] L.F. Abbott and P. Dayan. The effect of correlated variability on the\naccuracy of a population code. Neural Computation, 11:91\u2013101, 1999.\n[3] B.B. Averback, P.E. Latham, and A. Pouget. Neural correlations, population coding and computation. Nature Reviews Neuroscience, 7:358\u2013\n366, 2006.\n[4] Wyeth Bair, Ehud Zohary, and William T. Newsome. Correlated Firing\nin Macaque Visual Area MT: Time Scales and Relationship to Behavior.\nJ. Neurosci., 21(5):1676\u20131697, 2001.\n[5] J. Biederlack, M. Castelo-Branco, S. Neuenschwander, D. W. Wheeler,\nW. Singer, and D. Nikoli\u0107. Brightness induction: rate enhancement and\nneuronal synchronization as complementary codes. Neuron, 52(6):1073\u2013\n1083, 2006.\n[6] M. D. Binder and R. K. Powers. Relationship Between Simulated Common Synaptic Input and Discharge Synchrony in Cat Spinal Motoneurons. J Neurophysiol, 86(5):2266\u20132275, 2001.\n[7] N. Bleistein and R.A. Handelsman. Asymptotic expansions of integrals.\nDover, 1986.\n[8] K. H. Britten, M. N. Shadlen, W. T. Newsome, and J. A. Movshon. The\nanalysis of visual motion: a comparison of neuronal and psychophysical\nperformance. J Neurosci, 12(12):4745\u20134765, 1992.\n[9] E. Brown, J. Moehlis, and P. Holmes. On the phase reduction and\nresponse dynamics of neural oscillator populations. Neural Comp.,\n16:673\u2013715, 2004.\n[10] Brody C.D., Romo R., and Kepecs A. Basic mechanisms for graded\npersistent activity: discrete attractors, continuous attractors, and dynamic representations. Current Opinion in Neurobiology, 13:204\u2013211,\n2003.\n[11] M. J. Chacron and J. Bastian. Population Coding by Electrosensory\nNeurons. J Neurophys, 99(4):1825\u20131835, 2008.\n[12] Y. Chen, W. S. Geisler, and E. Seidemann. Optimal decoding of correlated neural population responses in the primate visual cortex. Nat\nNeurosci, 9(11):1412\u20131420, 2006.\n\n27\n\n\f[13] D.R. Cox and P.A.W. Lewis. The Statistical Analysis of a Series of\nEvents. Jonh Wiley, London, 1966.\n[14] J. de la Rocha, B. Doiron, E. Shea-Brown, K. Josic, and A. Reyes. Correlation between neural spike trains increases with firing rate. Nature,\n448, 2007.\n[15] R. C. deCharms and M. M. Merzenich. Primary cortical representation\nof sounds by the coordination of action potentials. Nature, 381:610\u2013613,\n1996.\n[16] Alain Destexhe, Michael Rudolph, and Denis Par\u00e9.\nThe highconductance state of neocortical neurons in vivo. Nature Reviews Neuroscience, 4:739\u2013751, 2003.\n[17] G.B. Ermentrout. Type I membranes, phase resetting curves, and synchrony. Neural Comp., 8:979\u20131001, 1996.\n[18] G.B. Ermentrout and N. Kopell. Frequency Plateaus in a Chain of\nWeakly Coupled Oscillators, I. SIAM Journal on Mathematical Analysis, 15:215, 1984.\n[19] Roberto F. Galn, G. Bard Ermentrout, and Nathaniel N. Urban.\nStochastic dynamics of uncoupled neural oscillators: Fokker-planck\nstudies with the nite element method. Phys. Rev. E, 76:056110, 2007.\n[20] C.W. Gardiner. Handbook of Stochastic Methods. Series in Synergetics.\nSpringer, 3rd edition, 2004.\n[21] C. M. Gray, P. K\u00f6ing A. K. Engel, and W. Singer. Oscillatory responses\nin cat visual cortex exhibit inter-columnar synchronization which reflects global stimulus properties. Nature, 338:334\u2013337, 1989.\n[22] D. Hansel, G. Mato, and C. Meunier. Phase dynamics for weakly coupled Hodgkin-Huxley neurons. Europhys. Lett., 25(5):367\u2013372, 1993.\n[23] K. O. Johnson. Sensory discrimination: neural processes preceding\ndiscrimination decision. J Neurophys, 43(6):1793\u20131815, 1980.\n[24] K. Josic, E. Shea-Brown, B. Doiron, and J. de la Rocha. Stimulusdependent correlations and population codes. Neural Computation,\n2009. In Press.\n\n28\n\n\f[25] A. Kohn and M. A. Smith. Stimulus dependence of neuronal correlation\nin primary visual cortex of the macaque. J Neurosci, 25(14):3661\u20133673,\n2005.\n[26] A. Kohn, M. A. Smith, and J. A. Movshon. Effect of prolonged and\nrapid adaptation on correlation in V1. Computational and Systems\nNeuroscience, Cold Spring Harbor NY (abstract), 2004.\n[27] A. Kuhn, A. Aertsen, and S. Rotter. Higher-order statistics of input\nensembles and the response of simple model neurons. Neural Comp.,\n15:67\u2013101, 2003.\n[28] A. Kuhn, A. Aertsen, and S. Rotter. Dependence of neuronal correlations on filter characteristics and marginal spike train statistics. Neural\nComp., 20:2133\u20132185, 2008.\n[29] B. Lindner, B. Doiron, and A. Longtin. Theory of oscillatory firing\ninduced by spatially correlated noise and delayed inhibitory feedback.\nPhysical Review E, 72, 2005.\n[30] B. Lindner, A. Longtin, and A. Bulsara. Analytic expressions for rate\nand CV of a type I neuron driven by gaussian write noise. Neural\nComputation, pages 1761\u20131780, 2003.\n[31] S. Marella and G.B. Ermentrout. Class-II neurons display a higher degree of stochastic synchronization than class-I neurons. Physical Review\nE, 77, 2008.\n[32] R. Moreno-Bote and N. Parga. Auto- and crosscorrelograms for the\nspike response of leaky integrate-and-fire neurons with slow synapses.\nPhys. Rev. Lett., 96:028101, 2006.\n[33] M. W. Oram, P. F\u00f6ldi\u00e1k, D. I. Perrett, and F. Sengpiel. The 'Ideal\nHomunculus': decoding neural population signals. Trends Neurosci,\n21(6):259\u2013265, 1998.\n[34] S. Panzeri, S. Schultz, A. Treves, and E. T. Rolls. Correlations and the\nencoding of information in the nervous system. Proc Royal Soc Lond\nB, 266:1001\u20131012, 1999.\n[35] B. Pfeuty, G. Mato, D. Golomb, and D. Hansel. The combined effects\nof inhibitory and electrical synapses in synchrony. Neural Computation,\n17:633\u2013670, 2005.\n29\n\n\f[36] J. Poort and P. R. Roelfsema. Noise correlations have little influence\non the coding of selective attention in area v1. Cerebal Cortex, 2008.\nAdvanced Online Publication.\n[37] Frederic Pouille and Massimo Scanziani. Enforcement of temporal fidelity in pyramidal cells by somatic feed-forward inhibition. Science,\n293:1159\u20131164, 2001.\n[38] J. Rinzel and G.B. Ermentrout. Analysis of neural excitability and\noscillations. In C. Koch and I. Segev, editors, Methods in Neuronal\nModeling, pages 251\u2013291. MIT Press, 1998.\n[39] R. Romo, A. Hernandez, A. Zainos, and E. Salinas. Correlated neuronal\ndischarges that increase coding efficiency during perceptual discrimination. Neuron, 38(4):649\u2013657, 2003.\n[40] M. Rudolph and A. Destexhe. Tuning neocortical pyramidal neurons\nbetween integrators and coincidence detectors. Journal of Computational Neuroscience, 14:239\u2013251, 2003.\n[41] E. Salinas and T.J. Sejnowski. Impact of correlated synaptic input on\noutput firing rate and variability in simple neuronal models. Journal\nof Neuroscience, 20(16):6193\u20136209, 2000.\n[42] J. M. Samonds, J. D. Allison, H. A. Brown, and A. B. Bonds. Cooperation between Area 17 Neuron Pairs Enhances Fine Discrimination of\nOrientation. J Neurosci, 23(6):2416, 2003.\n[43] E. Schneidman, M. J. Berry, R. S. II, and W. Bialek. Weak pairwise\ncorrelations imply strongly correlated network states in a neural population. Nature, 440(7087):1007, 2006.\n[44] P. Seri\u00e8s, P. E. Latham, and A. Pouget. Tuning curve sharpening for\norientation selectivity: coding efficiency and the impact of correlations.\nNat Neurosci, 7:11291135, 2004.\n[45] M.N. Shadlen and W.T. Newsome. The variable discharge of cortical\nneurons: Implications for connectivity, computation, and information\ncoding. Journal of Neuroscience, 18(10):3870\u20133896, 1998.\n[46] M. Shamir and H. Sompolinsky. Nonlinear population codes. Neural\nComput, 16(6):1105\u20131136, 2004.\n\n30\n\n\f[47] M. Shamir and H. Sompolinsky. Implications of neuronal diversity on\npopulation coding. Neural Comput, 18(8):1951\u20131986, 2006.\n[48] E. Shea-Brown, K. Josi\u0107, B. Doiron, and J. de la Rocha. Correlation\nand synchrony transfer in integrate-and-fire neurons: Basic properties\nand consequences for coding. Phys Rev Lett, 100:108102, 2008.\n[49] H. Sompolinsky, H. Yoon, K. Kang, and M. Shamir. Population coding in neuronal systems with correlated noise. Phys Rev E, 64(5 Pt\n1):051904, 2001.\n[50] A. Winfree. The Geometry of Biological Time. Springer, New York,\n2001.\n[51] E. Zohary, M. Shadlen, and W. Newsome. Correlated neuronal discharge rate and its implications for psychophysical performance. Nature, 370:140\u2013143, 1994.\n\n31\n\n\f"}