{"id": "http://arxiv.org/abs/0910.1294v1", "guidislink": true, "updated": "2009-10-07T15:42:30Z", "updated_parsed": [2009, 10, 7, 15, 42, 30, 2, 280, 0], "published": "2009-10-07T15:42:30Z", "published_parsed": [2009, 10, 7, 15, 42, 30, 2, 280, 0], "title": "Visual object categorization with new keypoint-based adaBoost features", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0910.5310%2C0910.5275%2C0910.2136%2C0910.3556%2C0910.3504%2C0910.0563%2C0910.3484%2C0910.5428%2C0910.4044%2C0910.1868%2C0910.3833%2C0910.5409%2C0910.3767%2C0910.2488%2C0910.4038%2C0910.4369%2C0910.0152%2C0910.0634%2C0910.0230%2C0910.5890%2C0910.5177%2C0910.0798%2C0910.1038%2C0910.1826%2C0910.1864%2C0910.0036%2C0910.3335%2C0910.5858%2C0910.5499%2C0910.1078%2C0910.4192%2C0910.4697%2C0910.1519%2C0910.2691%2C0910.2165%2C0910.5948%2C0910.0148%2C0910.1938%2C0910.3707%2C0910.5910%2C0910.0425%2C0910.1537%2C0910.0664%2C0910.0751%2C0910.1593%2C0910.1518%2C0910.3618%2C0910.3610%2C0910.3628%2C0910.2553%2C0910.5415%2C0910.2009%2C0910.4747%2C0910.2130%2C0910.3421%2C0910.4130%2C0910.2115%2C0910.1087%2C0910.5922%2C0910.1311%2C0910.3989%2C0910.2978%2C0910.5163%2C0910.2431%2C0910.1670%2C0910.0358%2C0910.2936%2C0910.2010%2C0910.0973%2C0910.0710%2C0910.1648%2C0910.5683%2C0910.4670%2C0910.4893%2C0910.4350%2C0910.3897%2C0910.1352%2C0910.0173%2C0910.3861%2C0910.5111%2C0910.1554%2C0910.3446%2C0910.1325%2C0910.5657%2C0910.0511%2C0910.1599%2C0910.4040%2C0910.5755%2C0910.2484%2C0910.3467%2C0910.0418%2C0910.2447%2C0910.3239%2C0910.0098%2C0910.3303%2C0910.1460%2C0910.2081%2C0910.4736%2C0910.1294%2C0910.4308%2C0910.5680&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Visual object categorization with new keypoint-based adaBoost features"}, "summary": "We present promising results for visual object categorization, obtained with\nadaBoost using new original ?keypoints-based features?. These weak-classifiers\nproduce a boolean response based on presence or absence in the tested image of\na ?keypoint? (a kind of SURF interest point) with a descriptor sufficiently\nsimilar (i.e. within a given distance) to a reference descriptor characterizing\nthe feature. A first experiment was conducted on a public image dataset\ncontaining lateral-viewed cars, yielding 95% recall with 95% precision on test\nset. Preliminary tests on a small subset of a pedestrians database also gives\npromising 97% recall with 92 % precision, which shows the generality of our new\nfamily of features. Moreover, analysis of the positions of adaBoost-selected\nkeypoints show that they correspond to a specific part of the object category\n(such as ?wheel? or ?side skirt? in the case of lateral-cars) and thus have a\n?semantic? meaning. We also made a first test on video for detecting vehicles\nfrom adaBoostselected keypoints filtered in real-time from all detected\nkeypoints.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0910.5310%2C0910.5275%2C0910.2136%2C0910.3556%2C0910.3504%2C0910.0563%2C0910.3484%2C0910.5428%2C0910.4044%2C0910.1868%2C0910.3833%2C0910.5409%2C0910.3767%2C0910.2488%2C0910.4038%2C0910.4369%2C0910.0152%2C0910.0634%2C0910.0230%2C0910.5890%2C0910.5177%2C0910.0798%2C0910.1038%2C0910.1826%2C0910.1864%2C0910.0036%2C0910.3335%2C0910.5858%2C0910.5499%2C0910.1078%2C0910.4192%2C0910.4697%2C0910.1519%2C0910.2691%2C0910.2165%2C0910.5948%2C0910.0148%2C0910.1938%2C0910.3707%2C0910.5910%2C0910.0425%2C0910.1537%2C0910.0664%2C0910.0751%2C0910.1593%2C0910.1518%2C0910.3618%2C0910.3610%2C0910.3628%2C0910.2553%2C0910.5415%2C0910.2009%2C0910.4747%2C0910.2130%2C0910.3421%2C0910.4130%2C0910.2115%2C0910.1087%2C0910.5922%2C0910.1311%2C0910.3989%2C0910.2978%2C0910.5163%2C0910.2431%2C0910.1670%2C0910.0358%2C0910.2936%2C0910.2010%2C0910.0973%2C0910.0710%2C0910.1648%2C0910.5683%2C0910.4670%2C0910.4893%2C0910.4350%2C0910.3897%2C0910.1352%2C0910.0173%2C0910.3861%2C0910.5111%2C0910.1554%2C0910.3446%2C0910.1325%2C0910.5657%2C0910.0511%2C0910.1599%2C0910.4040%2C0910.5755%2C0910.2484%2C0910.3467%2C0910.0418%2C0910.2447%2C0910.3239%2C0910.0098%2C0910.3303%2C0910.1460%2C0910.2081%2C0910.4736%2C0910.1294%2C0910.4308%2C0910.5680&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present promising results for visual object categorization, obtained with\nadaBoost using new original ?keypoints-based features?. These weak-classifiers\nproduce a boolean response based on presence or absence in the tested image of\na ?keypoint? (a kind of SURF interest point) with a descriptor sufficiently\nsimilar (i.e. within a given distance) to a reference descriptor characterizing\nthe feature. A first experiment was conducted on a public image dataset\ncontaining lateral-viewed cars, yielding 95% recall with 95% precision on test\nset. Preliminary tests on a small subset of a pedestrians database also gives\npromising 97% recall with 92 % precision, which shows the generality of our new\nfamily of features. Moreover, analysis of the positions of adaBoost-selected\nkeypoints show that they correspond to a specific part of the object category\n(such as ?wheel? or ?side skirt? in the case of lateral-cars) and thus have a\n?semantic? meaning. We also made a first test on video for detecting vehicles\nfrom adaBoostselected keypoints filtered in real-time from all detected\nkeypoints."}, "authors": ["Taoufik Bdiri", "Fabien Moutarde", "Bruno Steux"], "author_detail": {"name": "Bruno Steux"}, "author": "Bruno Steux", "links": [{"href": "http://arxiv.org/abs/0910.1294v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0910.1294v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0910.1294v1", "affiliation": "CAOR", "arxiv_url": "http://arxiv.org/abs/0910.1294v1", "arxiv_comment": null, "journal_reference": "IEEE Symposium on Intelligent Vehicles (IV'2009), XiAn : China\n  (2009)", "doi": null, "fulltext": "Visual object categorization with\nnew keypoint-based adaBoost features\nTaoufik Bdiri, Fabien Moutarde, and Bruno Steux\n\nAbstract- We present promising results for visual object\ncategorization, obtained with adaBoost using new original\n\"keypoints-based features\". These weak-classifiers produce a\nboolean response based on presence or absence in the tested\nimage of a \"keypoint\" (a kind of SURF interest point) with a\ndescriptor sufficiently similar (i.e. within a given distance) to a\nreference descriptor characterizing the feature. A first\nexperiment was conducted on a public image dataset containing\nlateral-viewed cars, yielding 95% recall with 95% precision on\ntest set. Preliminary tests on a small subset of a pedestrians\ndatabase also gives promising 97% recall with 92 % precision,\nwhich shows the generality of our new family of features.\nMoreover, analysis of the positions of adaBoost-selected\nkeypoints show that they correspond to a specific part of the\nobject category (such as \"wheel\" or \"side skirt\" in the case of\nlateral-cars) and thus have a \"semantic\" meaning. We also\nmade a first test on video for detecting vehicles from adaBoostselected keypoints filtered in real-time from all detected\nkeypoints.\n\nI. INTRODUCTION AND RELATED WORK\n\nO\n\nne of the key features for enhancing safety in intelligent\nvehicles is efficient and reliable detection of\nsurrounding moving objects such as pedestrians and\nvehicles. It is particularly interesting to be able to properly\ndetect laterally incoming cars that could lead to lateral\ncollisions.\nMany techniques have been proposed for visual object\ndetection and classification (see e.g. [3] for a review of some\nof the state-of-the-art methods for pedestrian detection,\nwhich is the most challenging). Of the various machinelearning approaches applied to this problem, only few are\nable to process videos in real-time. Among those, the\nboosting algorithm with feature selection was successfully\nextended to machine-vision by Viola & Jones [2].\nThe adaBoost algorithm was introduced in 1995 by Y.\nFreund and R. Shapire [1], and its principle is to build a\nstrong classifier, assembling weighted weak classifiers, those\nbeing obtained iteratively by using successive weighting of\nthe examples in the training set. Most published works using\nManuscript received January 15, 2009.\nFabien Moutarde and Bruno Steux are with the Robotics Laboratory\n(CAOR) - Unit\u00e9 Maths & Syst\u00e8me, Mines ParisTech, 60 Bd St Michel F75006 Paris, FRANCE (phone: 33-1-40.51.92.92; fax: 33-1-43.26.10.51;\ne-mail: Fabien.Moutarde@ mines-paristech.fr). Taoufik Bdiri was an intern\nduring spring 2008 in the same laboratory.\n\nadaBoost for visual object class detection are using the Haarlike features initially proposed by Viola & Jones for face and\npedestrian detection.\nHowever, adaBoost outcome may strongly depend on the\nfamily of features from which the weak classifiers are drawn.\nRecently, several teams [4][5] have reported interesting\nresults with boosting using other kinds of features directly\ninspired from the Histogram of Oriented Gradient (HOG)\napproach. Our lab has been successfully investigating\nboosting with pixel-comparison-based features named\n\"control-points\" (see [6] for original proposal, and [7] for\nrecent results with a new variant).\nIn the present work we investigate boosting of \"keypoint\npresence features\", where \"keypoint\" are a variant of SURF\npoints implemented in our lab (see below), and already\nsuccessfully applied to real-time person re-identification\n[11]. To our knowledge, the idea of using interest point\ndescriptors as boosting features was first proposed by Opelt\net al. in [8], but it was in a more general framework, and they\nwere considering SIFT points and descriptors [9] which are\nquite slow to compute, compared to the SURF points and\ndescriptors [10].\nThe paper is organized as follows: in section II we briefly\npresent the principle of the \"Camellia keypoints\" we are\nusing; section III explains how we use keypoints to define a\nnew original family of weak classifiers, and how is realized\nthe feature-selection in this family during each boosting step;\nsection IV presents experimental results on a publicly\navailable image dataset of laterally-viewed cars, and\npreliminary evaluation on a small pedestrians dataset;\nsection V presents our first step in building an original object\ndetection scheme that could be used with our particular\nkeypoint-based classifier; and section VI draws some\nconclusions and perspectives.\nII. CAMELLIA \"KEYPOINTS\"\nThe interest point detection and descriptor computation is\nperformed using \"key-points\" functions available in the\nCamellia\n(http://camellia.sourceforge.net)\nimage processing library. These Camellia key-points\ndetection and descriptor functions \u2013 named CamKeypoints implement a variant of SURF [10]. SURF itself is an\nextremely efficient method (thanks to the use of integral\nimages) inspired from the more classic and widely used\ninterest point detector and descriptor SIFT [9].\n\n\fIII. ADABOOST WITH \"KEYPOINT PRESENCE\" FEATURES\n\nFig. 1. SURF interest points (left) v.s. Camellia keypoints (right);\nthey are very similar except for voluntary suppression of multiple\nimbricated blobs at different scales (cf. upper left).\n\nAs for SURF interest points, the detection of Camellia\nkeypoints is a \"blob detector\" based on finding local Hessian\nmaxima, those being efficiently obtained by approximating\nsecond order derivatives with box filters computed with\nintegral image. Our keypoints are however not exactly the\nsame as SURF points (as can be seen on figure 1), in\nparticular because multiple imbricated blobs at various\nscales are voluntarily avoided. In contrary to SURF and\nSIFT, CamKeypoint scale selection is not based on\noverlapping octaves, but on a set of discrete scales from\nwhich the scale of a keypoint is derived by quadratic\ninterpolation. This speeds up the keypoints detection wrt. to\nSURF by a factor of 2 without sacrificing the quality of scale\ninformation, as was shown by some experiments.\nThe descriptor used for each Camellia keypoint is similar to\nthe SURF descriptor : an image patch corresponding to the\nkeypoint location and scale is divided in 4x4=16 subregions, on each of which are efficiently computed (by using\nintegral image approach) the following 4 quantities:\n\n\u2211 dx\n\u2211 dx\n\u2211 dy\n\u2211 dy\nwhere dx and dy are respectively the horizontal and vertical\ngradient. The total descriptor size is therefore 16 x 4 = 64. In\norder to avoid all boundary effects in which the descriptor\nabruptly changes when a keypoint physically changes, bilinear extrapolation is used to distribute each of the 4\nquantities above into 4 sub-regions. Experiments have shown\nthat this really improves the quality of the descriptor wrt.\nSURF. In addition to this, CamKeypoints support color\nimages by adding 32 elements of gradient information by\ncolor channel (U and V) to the signature, resulting in a 128\ndescriptor size for color descriptors.\nAnother main difference between Camellia Keypoints and\nSURF lies in that the Camellia implementation uses integeronly computations \u2013 even for the scale interpolation \u2013, which\nmakes it even faster than SURF, and particularly well-suited\nfor potential embedding in camera hardware. SIFT and\nSURF make extensive use of floating point computations,\nwhich makes these algorithms power hungry.\n\nOur object recognition approach uses the same general\nfeature-selecting boosting framework as pioneered by\nViola&Jones in [2]. The originality of our work is to define\nand use as weak classifiers a new original feature family,\ninstead of Haar features. This new feature type is a weak\nclassifier that answers positively on an image if and only if,\namong all the Camellia keypoints detected in the image,\nthere is at least one of them whose descriptor is similar\nenough to the \"reference keypoint descriptor\" associated\nwith the weak-classifier.\nMore formally, each \"keypoint presence\" weak-classifier is\ndefined by a keypoint SURF descriptor D in R64, and a\ndescriptor difference threshold scalar value d. This weakclassifier h(D,d,I) answers positively on an image I if and\nonly if I contains at least one keypoint whose descriptor D' is\nsuch that \uf8f4D-D'\uf8f4<d, where the \"sum of absolute difference\"\n(SAD) L1-distance is used: if two keypoints K1 and K2\nrespectively have {Desc1[i], i = 1...64} and {Desc2[i], i =\n1...64}, then, the distance between K1 and K2 is given by\nequation 1 below:\n\nDist(K1 , K 2 ) = \u2211i=1 abs(Desc1[i] \u2212Desc2[i]) (1)\n64\n\nThe rationale of boosting \"keypoint presence features\" for\nimage categorization is that it should be possible, for a given\nobject category, to determine a set of characteristic interest\npoints whose simultaneous presence would be representative\nof that particular category. This is similar in spirit, but with a\ncompletely different algorithm, to the \"part based\" approach\nproposed by [12].\nThe training method is the standard feature-selecting\nadaBoost algorithm, in which, at each boosting step, the\nSURF descriptor D is chosen among all descriptors found in\npositive example images. More formally, let the training set\nbe composed of positive images Ip1, Ip2, ...., and of\nnegative images In1, In2, ...We first apply the Camellia\nkeypoint detector on all positive images Ip1, Ip2, ..., and\nbuild the \"positive keypoints set\" Spk = {K1, K2, K3, ..., KQ}\nas the union of all Camellia keypoints detected on any\npositive examples of the training set. The adaBoost featureselection has to select, at each boosting step, a particular\n\"keypoint presence\" weak classifier defined by a 64D\ndescriptor and a scalar threshold. The descriptor will be\nchosen among those of positive keypoints collected in Spk.\nIn order to choose a threshold value, we apply keypoints\ndetection on all negative images as well, so that we can\ncompare descriptors of the positive keypoints in Spk to\ndescriptors of all keypoints found in training images. We\ndefine the \"distance\" between any given keypoint K and any\ngiven image I as the smallest descriptor difference between\nK and all keypoints KIj found in image I:\ndist(K,I)=min KIj keypoint found in image I { dist(K,KIj) }\n\n(2)\n\n\fwhere dist(K,KIj) is the SAD of descriptors as defined in\nequation (1). This allows us to build a matrix M of distances\nbetween positive keypoints and all training images, where\nMij = dist(Ki , Ij). As illustrated on figure 2, this QxN matrix\n(with Q the number of positive keypoints and N the number\nof training images) has at least one zero on each line, on the\ncolumn corresponding to the positive image in which the\nkeypoints was found.\n\nthere seems to be no clear improvement on test dataset for\nboosting steps T>150.\n\nFig.3. Some positive (2 left columns) end negative (right column)\nexamples from the training set\n\nFig. 2. Matrix of distances between keypoints found on positive\nimage example (one for each row) and all N training images\n(positives and negatives, one for each column)\n\nWe make a growing sorting of the distance matrix M, row\nby row, and then we take the middle of each two successive\ndistances in the sorted matrix to build the set {Tik ,\nk=1,...,N} of candidate threshold values for a feature testing\npresence of the corresponding positive keypoint Ki.\nAt each boosting step, we choose among all (Ki,Tik) couples\nthe one that gives the lowest weighted error on the training\nset: (i*,k*) = argminik (\n\n\u2211\n\nN\nj=1\n\nwj |h(Ki,Tik,Ij) - lj| ), and the\n\nselected weak classifier is h(Ki*,Ti*k*, . ).\n\nIV. EXPERIMENTS AND RESULTS\n\nFig.4. Typical evolution, during successive boosting steps,\nof errors on training and test\n\nFigure 5 shows the precision-recall curve, computed on the\nindependent test set, for boosted strong classifiers with\nrespectively 10 and 300 \"keypoint presence\" weak-classifiers\nassembled. The classification result is very good, with a\nrecall of ~95% for a precision of ~95%.\n\nA. Lateral cars database\nFor a first evaluation of our approach, we used the publicly\navailable (http://l2r.cs.uiuc.edu/~cogcomp/Data/Car/) lateralcar dataset collected by Agarwal et al. [12]. This database\ncontains 550 positive images and 500 negative images. For\ntraining, we use 352 positive images, and 322 negative\nimages, the rest being used as a test set for evaluation. Note\nthat the partition between training and testing subset is\nrandom. Some examples from the training set are shown on\nfigure 3.\nFigure 4 shows the typical error evolution during adaBoost\ntraining: as is usual with boosting, the training error quickly\nfalls to zero, and the error on test set continues to diminish\nafterwards. This shows that boosting by assembling features\nextracted from our new \"keypoint presence\" family does\nwork and allow to build a strong classifier able to\ndiscriminate a given object category. On this particular case,\n\nFig. 5. Precision-recall curve computed on test set, for\nstrong boosted classifier assembling 10 and 300 weak classifiers\nselected from our new \"keypoint presence\" family.\n\n\fIn order to further analyze how the obtained classifier works,\nwe looked at the evolution of strong classifier output on test\nimages as a function of the boosting step. As can be seen on\nfigure 6a, we typically obtain, on positive images, only\npositive votes by the first few weak-classifiers, and then\nsome negative votes decrease the global output, but the\nweighted vote remains largely above the 0.5 threshold for\npositive classification.\n\nFig. 7. Illustration on one positive image and one negative image\nof the positively responding adaboost-selected keypoints; some of\nthem do vote positive on some negative images, but the strong\nclassifier still correctly classifies those negatives images.\n\nFig. 6a. Evolution with increasing boosting steps of strong\nclassifier output on a given positive image\n\nB. Pedestrians database\nAs a quick check for the generality of our new family of\nfeatures, we have applied our method to a small subset of the\npublicly available pedestrians database collected by Munder\nand Gavrila [3]. For training computation time reasons, we\nused only 550 positive images and 500 negative images from\ntheir first training set, and split them as 2/3 for our training,\nand 1/3 for our testing.\n\nThe typical strong classifier output evolution on negative\nimage is roughly symmetric, as illustrated on figure 6b.\n\nFig. 8. Some examples from the pedestrians database subset.\n\nThe boosting with our new family of features indeed\nlearns normally, as can be checked on the evolution with\nboosting steps of training and testing errors shown on\nfigure 9.\nFig. 6b. Evolution with increasing boosting steps of strong\nclassifier output on a given negative image\n\nWe also checked how the boosting-selected \"keypoint\npresence\" features respond on positive and negative images.\nAs illustrated on figure 7, some of the adaboost-selected\nfeatures vote positive on negative images, but this does not\nprevent correct classification as negative by the strong\nclassifier.\nFig. 9. Evolution during successive boosting steps, of the training\nand testing errors on the subset of pedestrians database.\n\n\fThe precision-recall curve computed on the test set is also\ncorrectly evolving to the upper-right corner, attaining a good\n97% recall / 92% precision with a 100-features strong\nclassifier, as can be seen on figure 10. However, the\nclassification performance of our method on the much bigger\nfull-sized pedestrians database still remains to be evaluated.\n\nThis clearly shows that the keypoints selected correspond to\nspecific parts of the object category, such as the wheels or\nthe side skirt, which means they have a semantic\nsignification relative to the object category.\n\nFig. 12. Position of adaBoost-selected positively responding\nkeypoints cumulated on all positive example images.\n\nFig. 10. Precision-recall curves computed on test set, at boosting\nsteps 10 (lower curve), 50 (middle curve) and 100 (upper curve).\n\nFinally, we illustrate on figure 11 what are the\nadaboost-selected keypoints replying \"positive\" on some\ntypical positive examples. It can be noticed that some\nkeypoints typically seem to circle the head, others the\nshoulder, and others the \"upper inter-leg\" part\n\nAnother motivation for these new kind of adaBoost features\nis that, by nature of the features, it should be possible to\nderive the localizations in the image of objects of the\nsearched category quite straightforwardly by some kind of\nclustering, or possibly a Hough-like method, applied to the\npositions of positively-responding keypoints, thus making\ncostly window-scanning unnecessary.\nAs a first test, we computed all keypoints on a video, and\nfiltered them to keep only the positively-responding ones, as\nillustrated on figure 13, where one can see that laterally\nincoming car on upper-right part of field is rather well\ndelineated as a single group of positive keypoints.\nNote that the computation of all keypoints, as well as their\nfiltering for keeping only the positively-responding ones is\ndone in real-time on the video.\n\nFig.11. Illustration on some positive examples of the positively\nresponding adaboost-selected keypoints\n\nV. OBJECT DETECTION FROM KEYPOINTS\nThere are several motivations for our new feature type. One\nis that a classifier based on the simultaneous presence of\nseveral characteristic keypoints matches the intuition we can\nhave on how human do categorize image by spotting some\ncharacteristic parts. In order to check if our adaBoostselected keypoints make sense from this point of view, we\ndecided to check on positive images where are located the\n\"positively responding keypoints\" for a given feature of the\nstrong classifier.\nFigure 12 illustrates the positions of all keypoints, cumulated\non all positive example images, that are within the descriptor\ndistance threshold of one given adaBoost-selected keypoints.\n\nFig. 13. First detection test on a video: all keypoints on the left\nside, and only positively-reponding keypoints on the right side.\n\n\fand Machine Intelligence (PAMI), Vol. 28, No. 3, March 2006.\nLowe, D. \"Distinctive Image Features from Scale-Invariant\nKeypoints\" International Journal of Computer Vision, Vol. 60, pp.\n91-110, Springer, 2004.\n[10] Bay H., Tuytelaars T. & Gool L. V., \"SURF:Speeded Up Robust\nFeatures\", Proceedings of the 9th European Conference on Computer\nVision (ECCV'2006), Springer LNCS volume 3951, part 1, pp 404-417, 2006.\n[11] Hamdoun O., Moutarde F., Stanciulescu B. and Steux B., \"Interest\npoints harvesting in video sequences for efficient person\nidentification\", proceedings of '8th international workshop on Visual\nSurveillance (VS2008)' of \"10th European Conference on Computer\nVision (ECCV'2008)\", Marseille, France, 17 oct. 2008.\n[12] Agarwal S., Aatif Awan A., and Roth D., \"Learning to Detect Objects\nin Images via a Sparse, Part-Based Representation\", IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol.26,\nnr 11, pp. 1475-1490, 2004.\n\n[9]\n\nVI. CONCLUSIONS AND PERSPECTIVES\nWe have presented a new family of weak-classifiers,\n\"keypoint presence features\", to be used for boosting for\nobject category visual recognition. We have obtained first\nsuccessful test of boosting \"keypoint presence features\",\napplied to lateral car recognition, yielding 95% recall with\n95% precision on test set. Moreover, analysis of the\npositions of adaBoost-selected keypoints show that they\ncorrespond to a specific part of the object category (such as\n\"wheel\" or \"side skirt\") and thus have a \"semantic\" meaning.\nPreliminary test on a small subset of a pedestrians database\nalso gives promising results, showing that our new family of\nfeatures can be used for recognition of various types of\nobject categories.\nPerspectives include tests on other datasets, in particular\nfor other object categories. Also, an optimization of the\nkeypoint-threshold selection is underway, as the current\nversion makes training rather computer-intensive for large\ndatasets.\nMore importantly, we are currently developing an objectlocalization method based on the analysis of positions of\npositively-responding keypoints. Finally, we are considering\nexploiting the relative positions of keypoints, instead of only\ntheir simultaneous presence, for further improvement of the\nperformances.\n\nREFERENCES\n[1]\n\n[2]\n\n[3]\n\n[4]\n\n[5]\n\n[6]\n\n[7]\n\n[8]\n\nFreund Y., and Schapire R.E., \"A decision-theoretic generalization of\non-line learning and an application to boosting\", 1995 European\nConference on Computational Learning Theory, pages 23\u201337, 1995.\nViola P., and Jones M., \"Rapid Object Detection using a Boosted\nCascade of Simple Features\", IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR'2001), Volume 1,\npage 511, Kauai , Hawai, USA, 2001.\nMunder S. and Gavrila D. M., \"An Experimental Study on Pedestrian\nClassification\". IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol.28, nr 11, pp. 1863-1868, 2006.\nZhu Q., Yeh M.-C., Kwang-Ting Cheng K.-T., and Avidan S., \"Fast\nHuman Detection Using a Cascade of Histograms of Oriented\nGradients\", proceedings of 2006 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2006), held in\nNew York, NY, USA, 17-22 June 2006.\nPettersson N., Petersson L. and Andersson L., \"The Histogram Feature\n\u2013 A Resource-Efficient Weak Classifier\", proc. IEEE Intelligent\nVehicles Symposium (IV 2008), Eindhoven (Nederland), June 2008.\nAbramson Y., Steux B., and Ghorayeb H., \"YEF (Yet Even Faster)\nReal-Time Object Detection \", 2005 Proceedings of International\nWorkshop on Automatic Learning and Real-Time (ALART'05),\nSiegen, Germany, page 5, 2005.\nMoutarde F., Stanciulescu B., and Breheret A., \"Real-time visual\ndetection of vehicles and pedestrians with new efficient adaBoost\nfeatures\", proceedings of 'Workshop on Planning, Perception and\nNavigation for Intelligent Vehicles (PPNIV)' of \"2008 International\nConference on Intelligent RObots and Systems (IROS'2008)\", Nice,\nFrance, 26 sept 2008.\nOpelt A., Pinz A., Fussenegger M., and Auer P., \"Generic Object\nRecognition with Boosting\", IEEE Transactions on Pattern Analysis\n\n\f"}