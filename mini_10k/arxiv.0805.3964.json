{"id": "http://arxiv.org/abs/0805.3964v2", "guidislink": true, "updated": "2011-05-15T18:57:54Z", "updated_parsed": [2011, 5, 15, 18, 57, 54, 6, 135, 0], "published": "2008-05-26T14:16:06Z", "published_parsed": [2008, 5, 26, 14, 16, 6, 0, 147, 0], "title": "DimReduction - Interactive Graphic Environment for Dimensionality\n  Reduction", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0805.1678%2C0805.3864%2C0805.2286%2C0805.1187%2C0805.1422%2C0805.0283%2C0805.1343%2C0805.4269%2C0805.2414%2C0805.1768%2C0805.1046%2C0805.4800%2C0805.3378%2C0805.3052%2C0805.0495%2C0805.4060%2C0805.1279%2C0805.3077%2C0805.3433%2C0805.3663%2C0805.1745%2C0805.1077%2C0805.3995%2C0805.3135%2C0805.4427%2C0805.1191%2C0805.1844%2C0805.3033%2C0805.4752%2C0805.3767%2C0805.0600%2C0805.0378%2C0805.3350%2C0805.1076%2C0805.1755%2C0805.3219%2C0805.0306%2C0805.1522%2C0805.1603%2C0805.4232%2C0805.0501%2C0805.3158%2C0805.3443%2C0805.0008%2C0805.3448%2C0805.2775%2C0805.1970%2C0805.1793%2C0805.2258%2C0805.0863%2C0805.2603%2C0805.4491%2C0805.1395%2C0805.0390%2C0805.1937%2C0805.2255%2C0805.0164%2C0805.2421%2C0805.3964%2C0805.1124%2C0805.1363%2C0805.3369%2C0805.1451%2C0805.3025%2C0805.2207%2C0805.3585%2C0805.3711%2C0805.0645%2C0805.2471%2C0805.1597%2C0805.3258%2C0805.0639%2C0805.0823%2C0805.1099%2C0805.1171%2C0805.4058%2C0805.1684%2C0805.4690%2C0805.1746%2C0805.0067%2C0805.1729%2C0805.1651%2C0805.0716%2C0805.1737%2C0805.3582%2C0805.2374%2C0805.3825%2C0805.3639%2C0805.4147%2C0805.0317%2C0805.0023%2C0805.2277%2C0805.4228%2C0805.4201%2C0805.4159%2C0805.1095%2C0805.3360%2C0805.3700%2C0805.1246%2C0805.3182%2C0805.1318&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "DimReduction - Interactive Graphic Environment for Dimensionality\n  Reduction"}, "summary": "Feature selection is a pattern recognition approach to choose important\nvariables according to some criteria to distinguish or explain certain\nphenomena. There are many genomic and proteomic applications which rely on\nfeature selection to answer questions such as: selecting signature genes which\nare informative about some biological state, e.g. normal tissues and several\ntypes of cancer; or defining a network of prediction or inference among\nelements such as genes, proteins, external stimuli and other elements of\ninterest. In these applications, a recurrent problem is the lack of samples to\nperform an adequate estimate of the joint probabilities between element states.\nA myriad of feature selection algorithms and criterion functions are proposed,\nalthough it is difficult to point the best solution in general. The intent of\nthis work is to provide an open-source multiplataform graphical environment to\napply, test and compare many feature selection approaches suitable to be used\nin bioinformatics problems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0805.1678%2C0805.3864%2C0805.2286%2C0805.1187%2C0805.1422%2C0805.0283%2C0805.1343%2C0805.4269%2C0805.2414%2C0805.1768%2C0805.1046%2C0805.4800%2C0805.3378%2C0805.3052%2C0805.0495%2C0805.4060%2C0805.1279%2C0805.3077%2C0805.3433%2C0805.3663%2C0805.1745%2C0805.1077%2C0805.3995%2C0805.3135%2C0805.4427%2C0805.1191%2C0805.1844%2C0805.3033%2C0805.4752%2C0805.3767%2C0805.0600%2C0805.0378%2C0805.3350%2C0805.1076%2C0805.1755%2C0805.3219%2C0805.0306%2C0805.1522%2C0805.1603%2C0805.4232%2C0805.0501%2C0805.3158%2C0805.3443%2C0805.0008%2C0805.3448%2C0805.2775%2C0805.1970%2C0805.1793%2C0805.2258%2C0805.0863%2C0805.2603%2C0805.4491%2C0805.1395%2C0805.0390%2C0805.1937%2C0805.2255%2C0805.0164%2C0805.2421%2C0805.3964%2C0805.1124%2C0805.1363%2C0805.3369%2C0805.1451%2C0805.3025%2C0805.2207%2C0805.3585%2C0805.3711%2C0805.0645%2C0805.2471%2C0805.1597%2C0805.3258%2C0805.0639%2C0805.0823%2C0805.1099%2C0805.1171%2C0805.4058%2C0805.1684%2C0805.4690%2C0805.1746%2C0805.0067%2C0805.1729%2C0805.1651%2C0805.0716%2C0805.1737%2C0805.3582%2C0805.2374%2C0805.3825%2C0805.3639%2C0805.4147%2C0805.0317%2C0805.0023%2C0805.2277%2C0805.4228%2C0805.4201%2C0805.4159%2C0805.1095%2C0805.3360%2C0805.3700%2C0805.1246%2C0805.3182%2C0805.1318&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Feature selection is a pattern recognition approach to choose important\nvariables according to some criteria to distinguish or explain certain\nphenomena. There are many genomic and proteomic applications which rely on\nfeature selection to answer questions such as: selecting signature genes which\nare informative about some biological state, e.g. normal tissues and several\ntypes of cancer; or defining a network of prediction or inference among\nelements such as genes, proteins, external stimuli and other elements of\ninterest. In these applications, a recurrent problem is the lack of samples to\nperform an adequate estimate of the joint probabilities between element states.\nA myriad of feature selection algorithms and criterion functions are proposed,\nalthough it is difficult to point the best solution in general. The intent of\nthis work is to provide an open-source multiplataform graphical environment to\napply, test and compare many feature selection approaches suitable to be used\nin bioinformatics problems."}, "authors": ["Fabricio Martins Lopes", "David Correa Martins-Jr", "Roberto M. Cesar-Jr"], "author_detail": {"name": "Roberto M. Cesar-Jr"}, "author": "Roberto M. Cesar-Jr", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1186/1471-2105-9-451", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0805.3964v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0805.3964v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "13 pages, 4 figures, site http://code.google.com/p/dimreduction/", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.5.2", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0805.3964v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0805.3964v2", "journal_reference": "BMC Bioinformatics 2008, 9:451", "doi": "10.1186/1471-2105-9-451", "fulltext": "arXiv:0805.3964v2 [cs.CV] 15 May 2011\n\nDimReduction - Interactive Graphic Environment\nfor Dimensionality Reduction\n\nFabr\u0131\u0301cio Martins Lopes(1,2) , David Correa Martins-Jr(1) and Roberto\nM. Cesar-Jr(1)\n(1) Instituto\n\nde Matem\u00e1tica e Estat\u0131\u0301stica da Universidade de S\u00e3o Paulo,\nBrazil.\n(2) Universidade Tecnol\u00f3gica Federal do Paran\u00e1, Brazil.\nAbstract\n\nFeature selection is a pattern recognition approach to choose important variables according to some criteria to distinguish or explain\ncertain phenomena. There are many genomic and proteomic applications which rely on feature selection to answer questions such as:\nselecting signature genes which are informative about some biological\nstate, e.g. normal tissues and several types of cancer; or defining a network of prediction or inference among elements such as genes, proteins,\nexternal stimuli and other elements of interest. In these applications,\na recurrent problem is the lack of samples to perform an adequate estimate of the joint probabilities between element states. A myriad of\nfeature selection algorithms and criterion functions are proposed, although it is difficult to point the best solution in general. The intent of\nthis work is to provide an open-source multiplataform graphical environment to apply, test and compare many feature selection approaches\nsuitable to be used in bioinformatics problems.\n\n1\n\nIntroduction\n\nThe pattern recognition methods allow the classification of objects or patterns in a number of classes [1]. Specifically in statistical pattern recognition, given a set Y = {y1 , ..., yc } of classes and an unknown pattern\nX = {X1 , X2 , ..., Xn }, a pattern recognition system associates x to a class\nyi based on defined measures in a feature space. In many applications, especially in bioinformatics, the feature space dimension tends to be very large,\n1\n\n\fmaking difficult the classification task. In order to overcome this inconvenient situation, the study of dimensionality reduction problem in pattern\nrecognition becomes imperative.\nThe so called \"curse of dimensionality\" [2] is a phenomenon in which\nthe number of training samples required to a satisfactory classifier performance is given by an exponential function of the feature space. This is the\nmain motivation by which performing of dimensionality reduction is important in problems with large number of features and small number of training\nsamples. Many bioinformatics applications are perfectly inserted in this context. Data sets containing mRNA transcription expressions from microarray\nor SAGE, for example, possess thousands of genes (features) and only some\ndozens of samples that may be cell states or types of tissues. If time is a\nfactor involved, the samples are called dynamical states, otherwise they are\ncalled steady states.\nThere are basically two dimensionality reduction approaches: feature\nextraction and feature selection [1, 3, 4]. The feature extraction methods\ncreate new features from transformations or combinations of the original feature set. On the other hand, feature selection algorithms just search for the\noptimal feature subset according to some criterion function. The software\nproposed in this paper is initially focused on feature selection methods.\nA feature selection method is composed by two main parts: a search\nalgorithm and a criterion function. As far as the search algorithms, there\nare two main categories: the optimal and sub-optimal algorithms. The\noptimal algorithms (including exhaustive and branch-and-bound searches)\nreturn the best feature subspace, but their computational costs are very high\nto be applied in general. The sub-optimal algorithms do not guarantee that\nthe solution is optimal, but some of them present a reasonable cost-benefit\nbetween computational cost and quality of the solution. Up to now, we have\nimplemented in the software the exhaustive search (optimal), the Sequential\nForward Selection (SFS - sub-optimal) and the Sequential Forward Floating\nSelection (SFFS - sub-optimal with excellent cost-benefit) [5].\nThere is a large number of criterion functions proposed in the literature.\nThe most common functions are based on the classifier error and distances\nbetween patterns. There are also criterion functions based on information\ntheory. They are closely related to the classifier error, but instead of using\nthe error, it is based on the conditional entropy of the class probabilities\ndistributions given the observed pattern.\nDue to the curse of dimensionality phenomenon, error estimation is a\ncrucial issue. We have developed some ways to embed error estimation\nin the criterion functions based on classifier error or conditional entropy.\n2\n\n\fThe main idea is based on penalization of non-oberved or rarely observed\ninstances. A good advantage in doing this is that the right dimension of the\nfeature subset solution is also estimated (the dimension parameter is not\nrequired). After the feature selection, it is possible to apply classical error\nestimation techniques like resubstitution, leave-one-out, cross validation or\nbootstrap.\nThe software is implemented in Java, so it can be executed in many\noperational systems. It is open source and intended to be continuously\ndeveloped in a world-wide collaboration. The software is available at http:\n//code.google.com/p/dimreduction/.\nFollowing this introduction, Section 2 and 3 will describe the feature\nselection algorithms and criterion functions implemented so far. Section 4\ndiscusses the implemented software. Section 5 will shows some preliminary\nresults obtained on gene regulation networks and classification of breast\ncancer cells. This paper is finalized with some conclusions in Section 6.\n\n2\n\nImplemented feature selection algorithms\n\nThe first and simpler feature selection algorithm implemented in this work\nis the exhaustive search. This algorithm searches the whole search space,\nand as a result, the selected features are optimal. However in bioinformatics\ncontext, normally the computational cost makes this approach inadequate.\nThen, it is clear the existence of a trade-off between optimality and computational cost.\nAn alternative way is to adopt sub-optimal search methods. In this\nwork we have implemented two sub-optimal approaches with unique solution, which are known as top down and bottom up. In the first one, the\nselection subset starts empty and features are inserted by optimizing a criterion function until a stop condition is satisfied, which is often based on\nthe subset size or a threshold. In the second algorithm, the subset starts\nfull and features are removed, trying to optimize the criterion function until\na stop condition is reached. Methods that implement these approaches are\nknown as SFS (Sequential Forward Search) and SBS (Sequential Backward\nSearch), respectively. Considering the context of this work, our choice was\nto implement the SFS approach.\nHowever, these suboptimal search methods present an undesirable drawback known as nesting effect. This effect happens because the discarded\nfeatures in the top-down approach are not inserted anymore, or the inserted\nfeatures in the bottom-up approach are never discarded.\n\n3\n\n\fIn order to circumvent this problem, the Sequential Forward Floating\nSelection (SFFS) [5] was also implemented. The SFFS algorithm tries to\navoid the nesting effect allowing to insert and exclude features on subset in\na floating way, i.e. without defining the number of insertions or exclusions.\nThe SFFS may be formalized as in [5]. Let Xk = {xi : 1 \u2264 i \u2264 k, xi \u2208 X}\nbe the subset with k features of the complete set X = {xi : 1 \u2264 i \u2264 n} with\nn features available. Let E(Xk ) the criterion function value for the subset\nXk . The algorithm initializes with k = 0, therefore the subset Xk is empty.\nFirst Step (insert): using the SFS method, select the feature xk+1 of\nthe set X \u2212 Xk to form the set Xk+1 , such that xk+1 be the most relevant\nfeature of the subset Xk . The new subset is Xk+1 = Xk \u222a xk+1 .\nSecond Step (conditional exclusion): Find the least relevant feature in\nthe set Xk+1 . If xk+1 is the least relevant feature in the subset Xk+1 , then\nk \u2190 k + 1, Xk \u2190 Xk+1 and back to the first step. If xr , 1 \u2264 r \u2264 k is the\nleast relevant feature in the subset Xk+1 , then exclude xr from Xk+1 to\nform a new subset X0k = Xk+1 \u2212 xr and k \u2190 k \u2212 1. If k = 2, then Xk = X0k ,\nand return to the first step, else execute the third step.\nThird Step (continuation of conditional exclusion): Find the least relevant feature xs in the set X0k . If E(X0k \u2212 xs ) \u2264 E(Xk\u22121 ), then Xk \u2192 X0k\nand return to first step. If E(X0k \u2212 xs ) > E(Xk\u22121 ) then exclude xs from\nX0k to form a new reduced subset X0k\u22121 = X0k \u2212 xs and k \u2192 k \u2212 1. If k = 2,\nthen Xk = X0k and return to first step, else repeat the third step.\nThe SFFS algorithm starts by setting k = 0 e Xk = 0, and the SFS\nmethod is used until the subset size k = 2.Then the SBS is performed in\norder to exclude bad features. SFFS proceeds by alternating between SFS\nand SBS until a stop criteria is reached. The best result set for each cardinality is stored in a list. The best set among them is selected as algorithm\nresult, and tie occurs, the set with lower cardinality is selected.\n\n3\n\nImplemented criterion functions\n\nWe implemented criterion functions based on classifier information (mean\nconditional entropy) and classifier error (Coefficient of Determination [6]),\nintroducing some penalization on poorly or non-observed patterns.\n\n3.1\n\nMean conditional entropy\n\nThe information theory was originated by Shannon [7] and can be employed\non feature selection problems [3]. The Shannon's entropy H is a measure of\nrandomness of a variable Y given by:\n4\n\n\fX\n\nH(Y ) = \u2212\n\nP (y)logP (y),\n\n(1)\n\ny\u2208Y\n\nwhere P is the probability distribution function. By convention 0 * log0 = 0.\nThe conditional entropy is a fundamental concept related to the mutual\ninformation. It is given by the following equation:\nH(Y |X = x) = \u2212\n\nX\n\nP (y|X = x)logP (y|X = x)\n\n(2)\n\ny\u2208Y\n\nwhere X is a feature vector and P (Y |X = x) is the conditional probability\nof Y given the observation of an instance x \u2208 X. And finally, the mean\nconditional entropy of Y given all the possible instances x \u2208 X is given by:\nH(Y |X) =\n\nX\n\nP (x)H(Y |x)\n\n(3)\n\nx\u2208X\n\nLower values of H yield better feature subspaces (the lower H, the larger\nis the information gained about Y by observing X).\n\n3.2\n\nCoefficient of Determination\n\nThe Coefficient of Determinstion (CoD) [6], like the conditional entropy, is\na non-linear criterion useful for feature selection problems [8]. It is given by:\n1 \u2212 maxy\u2208Y P (y) \u2212 (1 \u2212 x\u2208X P (x) maxy\u2208Y P (y|x))\n1 \u2212 maxy\u2208Y P (y)\nP\n\nCoDY (X) =\n\n(4)\n\nwhere 1\u2212maxy\u2208Y P (y) is the error of predicting Y in the absence of other obP\nservations (let us denote it by \u03b5Y ) and 1 \u2212 x\u2208X maxy\u2208Y P (x, y) is the error\nof predicting Y based on the observation of X (let us denote it by \u03b5Y (X)).\nLarger values of CoD yield better feature subspaces (CoD = 0 means that\nthe feature subspace does not improve the priori error and CoD = 1 means\nthat the error was fully eliminated).\n\n3.3\n\nPenalization of non-observed instances\n\nA way to embed the error estimation caused by using feature vectors with\nlarge dimensions and insufficient number of samples is to involve non-observed\ninstances in the criterion value calculus [9]. A positive probability mass is\nattributed to the non-observed instances and their contribution is the same\nas observing only the Y values with no other observations.\n5\n\n\fIn the case of mean conditional entropy, the non-observed instances get\nthe entropy equal to H(Y ) and, for the CoD, they get the prior error \u03b5Y\nvalue. The probability mass for the non-observed instances is parametrized\nby \u03b1. This parameter is added to the relative frequency (number of occurrences) of all possible instances. So, the mean conditional entropy with this\ntype of penalization becomes:\nN\nX\n1\nH(Y |X) =\n\u03b1(M \u2212 N )H(Y ) +\n(fi + \u03b1)H(Y |X = xi )\n\u03b1M + s\ni=1\n\n\"\n\n#\n\n(5)\n\nwhere M is the number of possible instances of the feature vector X, N is\nthe number of observed instances (so, the number of non-observed instances\nis given by M \u2212 N ), fi is the relative frequence (number of observations) of\nthe instance xi and s is the number of samples.\nAnd CoD becomes:\n\nCoDY (X) =\n\n3.4\n\n\u03b5Y \u2212\n\nh\n\n\u03b1(M \u2212N )\u03b5Y\n\u03b1M +s\n\n+1\u2212\n\n(fi +\u03b1)\ni=1 \u03b1M +s\n\nPN\n\ni\n\nmaxy\u2208Y P (y|xi )\n\n\u03b5Y\n\n(6)\n\nPenalization of rarely observed instances\n\nIn this penalization, the non-observed instances are not taken into account.\nThis penalization consists in changing the conditional probability distribution of the instances that have just a unique observation [10]. It makes sense\nbecause if an instance x has only 1 observation, the value of Y is fully determined (H(Y |X = x) = 0 and CoDY (X) = 1), but the confidence about\nthe real distribution of P (Y |X = x) is very low. A parameter \u03b2 gives a\nconfidence value that Y = y. The main idea is to distrubute 1 \u2212 \u03b2 equally\nover all P (Y 6= y|X = x) and to attribute \u03b2 to P (Y = y|X = x). In Barrera\net al [10], the \u03b2 value is |Y1 | where |Y | is the number of classes (cardinality\nof Y ), becoming the uniform distribution (strongest penalization).\nAdapting this penalization to the Equation 3, the mean conditional entropy becomes:\nH(Y |x) =\n\nM \u2212N\nH(F (Y )) +\ns\n\nX\n\nP (x)H(Y |x),\n\nx\u2208X:P (x)> 1s\n\nwhere F (Y ) is the probability distribution given by\n(\n\nF (i) =\n\n\u03b2,\n\nif i = 1\n,\nif i = 2, 3..., c\n\n1\u2212\u03b2\nc\u22121 ,\n\n6\n\n(7)\n\n\fand N in this case is the number of instances x with P (x) > 1s (more than\none observation).\nSince \u03b5Y (x) = 1 \u2212 \u03b2 when P (Y |x) = 1t , the CoD with this penalization\nis given by:\n\nCoDY (X) =\n\n3.5\n\n\u03b5Y \u2212 (1 \u2212\n\n(M \u2212N )\n\u03b2\ns\n\n\u2212\n\nP\n\nx\u2208X:P (x)> 1s\n\nP (x)maxy\u2208Y P (y|x))\n\n\u03b5Y\n\n(8)\n\nClassifier design and generalization\n\nAfter the feature selection using H or CoD, the classifier is designed from\nthe table of conditional probabilities where each row is a possible instance\nx \u2208 X, each column is a possible class Y = y and each cell of this table\nrepresents P (Y |X = x). This table is used as a Bayesian classifier where,\nfor each given instance, the chosen label Y = y is the one with maximum\nconditional probability for the considered instance. In case of instances that\nhave two or more labels of maximum probability (including non-observed\ninstances), it is possible to generalize these instances according to some\ncriterion. A commonly used criterion is the nearest neighbors with some\ndistance metric [1]. We implemented the nearest neighbors using Euclidean\ndistance. In this implementation, the nearest neighbors are taken successively. The occurrences of each label are summed until only one of such\nlabels has the maximum number of occurrences and may be chosen as the\nclass to which the considered instance belongs. This featured can be turned\noff. In this case, the label is guessed, i.e., chosen randomly from the labels\nwith maximum number of occurrences (including non-observed instances).\n\n4\n\nSoftware description\n\nThe software is implemented in Java in order to be executable in different\nplatforms. It is open source and intended to be continuously developed\nin a world-wide collaboration. The software is available at http://code.\ngoogle.com/p/dimreduction/.\nThere are four main panels: the first panel allows the user to load the\ndata set (Figure 1-a). The second is optional for the user to define a quantization degree to the data set. The quantized data may be visualized (Figure 1-b). It is worth noting that some feature selection criteria like mean\nconditional entropy or CoD require data quantization to discrete values.\nThis fact explains the quantization step available in the software. The data\n7\n\n\f(a) Upload the biological data\n\n(b) Quantization process\n\n(c) Single execution\n\n(d) Cross-validation\n\nFigure 1: Application panels.\nquantization is based on a common rule, searching for the extreme values\n(positive and negative) and dividing equally the negative and positive space\nconsidering the number of divisions specified by the quantization degree\nparameter.\nThe next step can be the single execution or cross-validation. The first\none is dedicated to perform single tests (Figure 1-c). It is represented by\na panel where the user is able to enter input parameters such as the feature selection algorithm (see Section 2 for the algorithms implemented) and\nthe criterion function (see Section 3 for the criteria implemented). Other\nimplemented utilities, including the visualization results of the feature selection, area found in the middle of the panel. There are three forms to\nvisualize the results: graphs (Figure 4), scatterplot (Figure 2-a) and parallel\ncoordinates (Figure 2-b). The graphs show the connections among different classes, chosen in feature selection execution, as directed edges between\nselected vertices. The parallel coordinates proposed by [11] allows to visualize in adjacent axes (selected features) similar patterns of behavior in data,\nvisually indicating how separated are the classes, considering the adjacent\n8\n\n\f(a) Scatterplot\n\n(b) Parallel coordinates\n\nFigure 2: Examples of scatterplot and parallel coordinates generated by the\nsoftware.\n\nfeatures. In the software application, the features and it and its order to\nbuild he parallel coordinates chart are defined by the user.\nThe cross-validation panel (Figure 1-d) is very similar to the prior.\nCross-validation [12] consists in to divide the whole data set in two subsets: training and test, mutually exclusive, and the user can define the size\nof both sets. The training set is entered as input to the feature selection\nalgorithm. The classifier designed from the feature selection and the joint\nprobability distributions table labels the test set samples. At the end of\nthe cross-validation process, it is plotted a chart with the results of each\nexecution, and it is possible to visualize the rate of hits and its variation\nalong the executions.\n9\n\n\fAnother available option is the generalization of non-observed instances.\nWith this option selected, the instances of the selected feature set not present\nin the training samples are generalized by a nearest neighbors method [1]\nwith Euclidean distance (see Section 3.5 for more details). This method is\nalso applied to take a decision among classes with tied maximum conditional\nprobability distributions given a certain instance.\n\n5\n\nIllustrative Results\n\nThis section presents the results in two main aspects. Initially the software\nwas applied as feature selection in a biological classification problem to classify breast cancer cells in two possible classes: benign and malignant. The\nbiological data used here was obtained from [13] which has 589 instances\nand 32 features. The results shown figure 3, presents very low variations\nand high accurate classification achieving 99.96% of accuracy on average.\n\nFigure 3: Cross-validation results using 10 executions, 80% of data as training set and 20% as test set.\n\nThe second computational biology problem addressed was gene network\nrecovery. In this case we used an artificial gene network generated by the\napproach presented in [14]. The parameters used were: 10 nodes, binary\nquantization, 20 observations (timestamps), 1 average of edges per vertex\nand Random graphs of Erd\u00f6s-R\u00e9nyi as network architecture. In figure 4, it is\npresented the network recovered. This result did not present false negatives\nand just few false positives.\n\n10\n\n\fFigure 4: Identified network: dashed lines represent the false positives and\nsolid lines the positives. There are no false negatives.\n\n6\n\nConclusion\n\nThe proposed feature selection environment allows data analysis using several algorithms, criterion functions and graphic visualization tools. Since\nit is an open-source and multi-platform software, it is suitable for the user\nthat wants to analyze data and draw some conclusions about it, as well as\nfor the specialist that has as objective to compare several combinations of\napproaches and parameters for each specific data set or to include more features in the software such as a new algorithm or a new criterion function.\nThis system can evolve and include feature extraction methods as well, not\nlimited only to feature selection methods.\nThe environment can be used in many pattern recognition applications,\nalthough the main concern is with Bioinformatics tasks, especially those\ninvolving high-dimensional data (large number of genes, for example) with\nsmall number of samples. Even users not familiar with programming are\nallowed to manipulate the software in an easy way, just by clicking to select\nfile inputs, quantization, algorithms, criterion functions, error estimation\nmethods and visualization of the results. The environment is implemented\nas \"wizard style\", i.e., it has tabs delimiting each procedure.\nThis software opens a great space for future works. The next step consists in the implementation of other classical feature selection algorithms\n(e.g. GSFS and PTA [1, 15]), criterion functions (e.g. based on distances\nbetween classes [1]), error estimation methods (e.g. Leave-one-out and Bootstrap) and then the inclusion of classical methods of feature extraction (e.g.\n11\n\n\fPCA [16]).\n\nAcknowledgement\nThis work was supported by FAPESP, CNPq and CAPES.\n\nReferences\n[1] S. Theodoridis and K. Koutroumbas. Pattern Recognition. Academic\nPress, USA, 1st edition, 1999.\n[2] A. K. Jain, R. P. W. Duin, and J. Mao. Statistical pattern recognition: A review. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 22(1):4\u201337, 2000.\n[3] R. O. Duda, P. E. Hart, and D. Stork. Pattern Classification. WileyInterscience, NY, 2000.\n[4] T. E. Campos. T\u00e9cnicas de sele\u00e7\u00e3o de caracter\u0131\u0301sticas com aplica\u00e7\u00f5es\nem reconhecimento de faces. Master's thesis, IME-USP, 2001.\n[5] P. Pudil, J. Novovicova, and J. Kittler. Floating search methods in\nfeature-selection. PRL, 15(11):1119\u20131125, November 1994.\n[6] E. R. Dougherty, S. Kim, and Y. Chen. Coefficient of determination in\nnonlinear signal processing. Signal Processing, 80:2219\u20132235, 2000.\n[7] C. E. Shannon. A mathematical theory of communication. Bell System\nTechnical Journal, 27:379\u2013423, 623\u2013656, July, October 1948.\n[8] T. Hsing, L. Liu, Marcel Brun, and E. R. Dougherty. The coefficient\nof intrinsic dependence (feature selection using el cid). Pattern Recognition, 38(5):623\u2013636, 2005.\n[9] D. C. Martins-Jr, R. M. Cesar-Jr, and J. Barrera. W-operator window\ndesign by minimization of mean conditional entropy. Pattern Analysis\n& Applications, 9:139\u2013153, 2006.\n[10] J. Barrera, R. M. Cesar-Jr, D. C. Martins-Jr, R. Z. N. Vencio, E. F.\nMerino, M. M. Yamamoto, F. G. Leonardi, C. A. B. Pereira, and H. A.\ndel Portillo. Constructing probabilistic genetic networks of Plasmodium\nfalciparum from dynamical expression signals of the intraerythrocytic\ndevelopment cycle, chapter 2, pages 11\u201326. Springer, 2006.\n12\n\n\f[11] A. Inselberg. The plane with parallel coordinates. The Visual Computer, 1(2):69\u201391, 1985.\n[12] Ron Kohavi. A study of cross-validation and bootstrap for accuracy\nestimation and model selection. In IJCAI, pages 1137\u20131145, 1995.\n[13] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.\n[14] F. M. Lopes, R. M. Cesar-Jr, and L. F. Costa. Agn simulation and\nvalidation model. In Proceedings of Brazilian Symposium on Bioinformatics (in press), 2008.\n[15] P. Somol, P. Pudil, J. Novovicov, and P. Paclk. Adaptive floating search\nmethods in feature selection. Pattern Recognition Letters, 20:1157\u2013\n1163, 1999.\n[16] I. T. Jolliffe. Principal component analysis. Springer-Verlag, New York,\n1986.\n\n13\n\n\f"}