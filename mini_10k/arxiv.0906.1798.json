{"id": "http://arxiv.org/abs/0906.1798v1", "guidislink": true, "updated": "2009-06-09T18:12:40Z", "updated_parsed": [2009, 6, 9, 18, 12, 40, 1, 160, 0], "published": "2009-06-09T18:12:40Z", "published_parsed": [2009, 6, 9, 18, 12, 40, 1, 160, 0], "title": "A Generalization of the 2D-DSPM for Solving Linear System of Equations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.0346%2C0906.5166%2C0906.0855%2C0906.2136%2C0906.3543%2C0906.4288%2C0906.3532%2C0906.1798%2C0906.4221%2C0906.4141%2C0906.4065%2C0906.5050%2C0906.4264%2C0906.2926%2C0906.2458%2C0906.4930%2C0906.5525%2C0906.3214%2C0906.4736%2C0906.0128%2C0906.5212%2C0906.0723%2C0906.1325%2C0906.4142%2C0906.0695%2C0906.4434%2C0906.0225%2C0906.3349%2C0906.5201%2C0906.2096%2C0906.0349%2C0906.0954%2C0906.1827%2C0906.0639%2C0906.0083%2C0906.3578%2C0906.0511%2C0906.2113%2C0906.2480%2C0906.3449%2C0906.1406%2C0906.1919%2C0906.0061%2C0906.2979%2C0906.2491%2C0906.4321%2C0906.1038%2C0906.4912%2C0906.1006%2C0906.3547%2C0906.5471%2C0906.0981%2C0906.0427%2C0906.1119%2C0906.1045%2C0906.4959%2C0906.3206%2C0906.4832%2C0906.0852%2C0906.3845%2C0906.2776%2C0906.4865%2C0906.1482%2C0906.4955%2C0906.0963%2C0906.1315%2C0906.1663%2C0906.3124%2C0906.1489%2C0906.5475%2C0906.1085%2C0906.2221%2C0906.4674%2C0906.3958%2C0906.1880%2C0906.1763%2C0906.2548%2C0906.4372%2C0906.4571%2C0906.3145%2C0906.0925%2C0906.1519%2C0906.1637%2C0906.2921%2C0906.4652%2C0906.3938%2C0906.2592%2C0906.3241%2C0906.3388%2C0906.2835%2C0906.5433%2C0906.2103%2C0906.3564%2C0906.4183%2C0906.1477%2C0906.2412%2C0906.5259%2C0906.4182%2C0906.4804%2C0906.4013%2C0906.4260&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Generalization of the 2D-DSPM for Solving Linear System of Equations"}, "summary": "In [7], a new iterative method for solving linear system of equations was\npresented which can be considered as a modification of the Gauss-Seidel method.\nThen in [4] a different approach, say 2D-DSPM, and more effective one was\nintroduced. In this paper, we improve this method and give a generalization of\nit. Convergence properties of this kind of generalization are also discussed.\nWe finally give some numerical experiments to show the efficiency of the method\nand compare with 2D-DSPM.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.0346%2C0906.5166%2C0906.0855%2C0906.2136%2C0906.3543%2C0906.4288%2C0906.3532%2C0906.1798%2C0906.4221%2C0906.4141%2C0906.4065%2C0906.5050%2C0906.4264%2C0906.2926%2C0906.2458%2C0906.4930%2C0906.5525%2C0906.3214%2C0906.4736%2C0906.0128%2C0906.5212%2C0906.0723%2C0906.1325%2C0906.4142%2C0906.0695%2C0906.4434%2C0906.0225%2C0906.3349%2C0906.5201%2C0906.2096%2C0906.0349%2C0906.0954%2C0906.1827%2C0906.0639%2C0906.0083%2C0906.3578%2C0906.0511%2C0906.2113%2C0906.2480%2C0906.3449%2C0906.1406%2C0906.1919%2C0906.0061%2C0906.2979%2C0906.2491%2C0906.4321%2C0906.1038%2C0906.4912%2C0906.1006%2C0906.3547%2C0906.5471%2C0906.0981%2C0906.0427%2C0906.1119%2C0906.1045%2C0906.4959%2C0906.3206%2C0906.4832%2C0906.0852%2C0906.3845%2C0906.2776%2C0906.4865%2C0906.1482%2C0906.4955%2C0906.0963%2C0906.1315%2C0906.1663%2C0906.3124%2C0906.1489%2C0906.5475%2C0906.1085%2C0906.2221%2C0906.4674%2C0906.3958%2C0906.1880%2C0906.1763%2C0906.2548%2C0906.4372%2C0906.4571%2C0906.3145%2C0906.0925%2C0906.1519%2C0906.1637%2C0906.2921%2C0906.4652%2C0906.3938%2C0906.2592%2C0906.3241%2C0906.3388%2C0906.2835%2C0906.5433%2C0906.2103%2C0906.3564%2C0906.4183%2C0906.1477%2C0906.2412%2C0906.5259%2C0906.4182%2C0906.4804%2C0906.4013%2C0906.4260&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In [7], a new iterative method for solving linear system of equations was\npresented which can be considered as a modification of the Gauss-Seidel method.\nThen in [4] a different approach, say 2D-DSPM, and more effective one was\nintroduced. In this paper, we improve this method and give a generalization of\nit. Convergence properties of this kind of generalization are also discussed.\nWe finally give some numerical experiments to show the efficiency of the method\nand compare with 2D-DSPM."}, "authors": ["Davod Khojasteh Salkuyeh"], "author_detail": {"name": "Davod Khojasteh Salkuyeh"}, "author": "Davod Khojasteh Salkuyeh", "arxiv_comment": "11 pages, submitted", "links": [{"href": "http://arxiv.org/abs/0906.1798v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0906.1798v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "65F10", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0906.1798v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0906.1798v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:0906.1798v1 [math.NA] 9 Jun 2009\n\nA Generalization of the 2D-DSPM for Solving\nLinear System of Equations\nDavod Khojasteh Salkuyeh\nDepartment of Mathematics, University of Mohaghegh Ardabili,\nP. O. Box. 56199-11367, Ardabil, Iran\nE-mail: khojaste@uma.ac.ir\n\nAbstract\nIn [N. Ujevi\u0107, New iterative method for solving linear systems, Appl. Math. Comput.\n179 (2006) 725730], a new iterative method for solving linear system of equations was\npresented which can be considered as a modification of the Gauss-Seidel method.\nThen in [Y.-F. Jing and T.-Z. Huang, On a new iterative method for solving linear\nsystems and comparison results, J. Comput. Appl. Math., In press] a different\napproach, say 2D-DSPM, and more effective one was introduced. In this paper, we\nimprove this method and give a generalization of it. Convergence properties of this\nkind of generalization are also discussed. We finally give some numerical experiments\nto show the efficiency of the method and compare with 2D-DSPM.\nAMS Subject Classification : 65F10.\nKeywords: linear system, projection method, Gauss-Seidel method, Petrov-Galerkin\ncondition, convergence.\n\n1. Introduction\nConsider the linear system of equations\nAx = b,\n\n(1)\n\nwhere A \u2208 Rn\u00d7n is a symmetric positive definite (SPD) matrix and x, b \u2208 Rn . The\nGauss-Seidel method is an stationary iterative method for solving linear system of\nequation and is convergent for SPD matrices. This method is frequently used in\nscience and engineering, both for solving linear system of equations and preconditioning [2, 6]. It can be easily seen that the Gauss-Seidel method is an special case of\n1\n\n\fa projection method [1, 6]. Let K and L be two m-dimensional subspaces of Rn . Let\nalso x0 be an initial guess of the solution of (1). A projection method onto K and\northogonal to L is a process which finds an approximate solution x \u2208 Rn to (1) by\nimposing the conditions that x \u2208 x0 + K and the new residual vector be orthogonal\nto L (Petrov-Galerkin condition), i.e.\nFind x \u2208 x0 + K,\n\nsuch that\n\nb \u2212 Ax\u22a5L.\n\n(2)\n\nIt is well known that an iteration of the elementary Gauss-Seidel method can be\nviewed as a set of projection methods with L = K = {ei }, i = 1, 2, . . . , n, where ei\nis the ith column of the identity matrix. In fact, a single correction is made at each\nstep of these projection steps cycled for i = 1, . . . , n.\nIn [7], Ujevi\u0107 proposed a modification of the Gauss-Seidel method which may\nbe named as a \"one-dimensional double successive projection method\" and referred\nto as 1D-DSPM. In an iteration of 1D-DSPM, a set of double successive projection\nmethods with two pairs of one-dimensional subspaces are used. In fact, in an iteration of 1D-DSPM two pairs of subspaces (K1 , L1 ) and (K2 , L2 ) of one dimension\nare chosen while it makes double correction at each step of the process cycled for\ni = 1, 2, . . . , n. In [4], Jing and Huang proposed the \"two-dimensional double successive projection method\" and referred to as 2D-DSPM. In an iteration of 2D-DSPM,\na set of projection methods with a pairs of two-dimensional subspaces K and L is\nused and a double correction at each step of the projection steps is made.\nIn this paper, a generalization of 2D-DSPM say mD-SPM which is referred to\nas \"m-dimensional successive projection method\" is proposed and its convergence\nproperties are studied. For m = 2, the mD-SPM results in 2D-DSPM.\nThroughout this paper we use some notations as follows. By h., .i we denote the\nstandard inner product in Rn . In fact, for two vectors x and y in Rn , hx, yi = y T x.\nFor any SPD matrix M \u2208 Rn\u00d7n , the M-inner product is defined as hx, yiM = hMx, yi\n1/2\nand its corresponding norm is kxkM = hx, xiM .\nThis paper is organized as follows. In section 2, a brief description of 1D-DSPM\nand 2D-DSPM are given. In section 3, the mD-SPM is presented and its convergence\nproperties are studied. In section 4 the new algorithm and its practical implementations are given. Section 5 is devoted to some numerical experiments to show the\nefficiency of the method and comparing with 1D-DSPM and 2D-DSPM. Some con2\n\n\fcluding remarks are given in 6.\n\n2. A brief description of 1D-DSPM and 2D-DSPM\nWe review 1D-DSPM and 2D-DSPM in the literature of the projection methods.\nAs we mentioned in the previous section in each iteration of 1D-DSPM a set of double\nsuccessive projection method is used. Let xk be the current approximate solution.\nThen the double successive projection method is applied as following. The first step\nis to choose two pairs of the subspaces K1 = L1 = {v1 }, K2 = L2 = {v2 } and the\nnext approximate solution xk+1 is computed as follows\nFind x\nek+1 \u2208 xk + K1 ,\n\nFind xk+1 \u2208 x\nek+1 + K2 ,\n\nsuch that\n\nb \u2212 Ae\nxk+1 \u22a5L1 ,\n\nsuch that\n\nb \u2212 Axk+1 \u22a5L2 .\n\n(3)\n(4)\n\nThis framework results in [4, 7]\n\nxk+1 = xk + \u03b11 v1 + \u03b22 v2\nwhere\n\u03b11 = \u2212p1 /a,\n\n\u03b22 = (cp1 \u2212 ap2 )/ad,\n\n(5)\n\nin which\na = hv1 , v1 iA ,\n\nc = hv1 , v2 iA ,\n\nd = hv2 , v2 iA .\n\n(6)\n\nIn [7], it has been proven that this method is convergent to the exact solution x\u2217 of\n(1) for any initial guess.\nIn the 2D-DSPM, two two-dimensional subspaces K = L = span{v1 , v2 } are\nchosen and a projection process onto K and orthogonal to L is used instead of double successive projection method used in 1D-DSPM. In other words, two subspaces\nK = L = span{v1 , v2 } are chosen and a projection method is defined as following.\nFind xk+1 \u2208 xk + K,\n\nsuch that\n\nb \u2212 Axk+1 \u22a5L.\n\nIn [4], it has been shown that this projection process gives\n\u03b1=\n\ncp2 \u2212 dp1\n,\nad \u2212 c2\n\n\u03b2=\n3\n\ncp1 \u2212 ap2\n,\nad \u2212 c2\n\n(7)\n\n\fwhere p1 , p2 , a, b, and c were defined in Eqs. (5) and (6). It has been proven in\n[4] that the 2D-DSPM is also convergent. Theoretical analysis and numerical experiments presented in [4] show that 2D-DSPM is more effective than the 1D-DSPM.\nA main problem with 1D-DSPM and 2D-DSPM is to choose the optimal vectors v1 and v2 . In this paper, we first propose a generalization of 2D-DSPM and then\ngive a strategy to choose vectors v1 and v2 in a special case.\n\n3. m-dimensional successive projection method\nLet {v1 , v2 , . . . , vm } be a set of m independent vectors in Rn . For later use, let\nalso Vm = [v1 , v2 , . . . , vm ]. Now we define the mD-SPM as follows. In an iteration of\nthe mD-SPM we use a set of projection process onto K = span{v1 , v2 , . . . , vm } and\northogonal to L = K. In other words, two m-dimensional subspaces K and L are used\nin the projection step instead of two two-dimensional subspaces used in 2D-DSPM.\nIn this case Eq. (2) turns the form\nFind ym \u2208 Rm\n\nsuch that xk+1 = xk + Vm ym ,\n\nand VmT (b \u2212 Axk+1 ) = 0.\n\n(8)\n\nWe have\nrk+1 = b \u2212 Axk+1\n= b \u2212 A(xk + Vm ym )\n= rk \u2212 AVm ym ,\nwhere rk = b \u2212 Axk . Hence from Eq. (8) we deduce\n0 = VmT rk+1 = VmT (rk \u2212 AVm y) = VmT rk \u2212 VmT AVm ym \u21d2 VmT AVm ym = VmT rk .\nThe matrix VmT AVm is an SPD matrix, since A is SPD. Therefore\nym = (VmT AVm )\u22121 VmT rk .\n\n(9)\n\nHence, from (8) we conclude that\nxk+1 = xk + Vm (VmT AVm )\u22121 VmT rk .\n4\n\n(10)\n\n\fTheorem 1. Let A be an SPD matrix and assume that xk is an approximate solution\nof (1). Then\nkdk kA \u2265 kdk+1 kA ,\n(11)\nwhere dk = x\u2217 \u2212 xk and dk+1 = x\u2217 \u2212 xk+1 in which xk+1 is the approximate solution\ncomputed by Eq. (10).\nProof. It can be easily verified that dk+1 = dk \u2212 Vm ym and Adk = rk where ym is\ndefined by (7). Then\nhAdk+1, dk+1i = hAdk \u2212 AVm ym , dk \u2212 Vm ym i\n= hAdk , dk i \u2212 2hVm ym , rk i + hAVm ym , Vm ym i\n= hAdk , dk i \u2212 hym , VmT rk i\n\nby Adk = rk\nby (9).\n\nTherefore\nkdk k2A \u2212 kdk+1 k2A\n\n=\n=\n=\n=:\n\nhAdk , dk i \u2212 hAdk+1 , dk+1i\nhym , VmT rk i\n(VmT rk )T (VmT AVm )\u22121 VmT rk ,\nS(rk ).\n\nby (9)\n\nSince (VmT AVm )\u22121 is SPD then we have\nkdk k2A \u2212 kdk+1 k2A \u2265 0,\nand the desired result is obtained.\n\n\u0003\n\nThis theorem shows that if VmT rk = 0 then S(rk ) = 0 and we don't have any\nreduction in the square of the A-norm of error. But, if VmT rk 6= 0 then the square of\nthe A-norm of error is reduced by S(rk ) > 0).\nTheorem 2. Assume that A is an SPD matrix and L = K. Then a vector xk+1\nis the result of projection method onto K orthogonal to L with the starting vector xk\niff it minimizes the A-norm of the error over xk + K.\nProof. See [6], page 126.\n\n\u0003\n\nThis theorem shows that if K1 = L1 \u2282 K2 = L2 , then the reduction of A-norm of\nthe errors obtained by the subspaces K2 = L2 is more than or equal to that of the\n5\n\n\fsubspaces K1 = L1 . Hence by increasing the value of m, the convergence rate may\nincrease.\nIn the continue we consider the special case that the vectors vi are the column\nvectors of the identity matrix. The next theorem not only proves the convergence of\nthe method in this special case but also gives an idea to choose the optimal vectors vi .\nTheorem 3. Let {i1 , i2 , . . . , im } be the set of indices of m components of largest\nabsolute values in rk such that i1 < i2 < . . . < im . If vj = eij , j = 1, . . . , m then\nkdk2A \u2212 kdnew k2A \u2265\n\nm\nn\u03bbmax (A)\n\nkrk k22 .\n\n(12)\n\nProof. Let Em = [ei1 , ei2 , . . . , eim ]. By Theorem 1, we have\nkdk k2A \u2212 kdk+1k2A = S(rk )\nT\nT\nT\n= (Em\nrk )T (Em\nAEm )\u22121 Em\nrk .\nThen by using Theorem 1.19 in [6] we conclude\nT\nT\nS(rk ) \u2265 \u03bbmin((Em\nAEm )\u22121 )kEm\nrk k22 \u2265\n\n1\nT\nkEm\nrk k22 ,\nT AE )\n\u03bbmax (Em\nm\n\n(13)\n\nwhere for a square matrix Z, \u03bbmin(Z) and \u03bbmax (Z) stand for the smallest and largest\neigenvalues of Z. It can be easily verified that [3]\nT\nT\nm\n(Em\nrk , Em\nrk )\n\u2265 .\n(rk , rk )\nn\n\nT\n\u03bbmax (Em\nAEm ) \u2264 \u03bbmax (A),\n\nHence\nS(rk ) \u2265\n\nm\nn\u03bbmax (A)\n\nand the desired result is obtained.\n\nkrk k22 ,\n\n(14)\n\n(15)\n\n\u0003\n\nEq. (12) shows the convergence of the method. Eq. (13) together with the\nfirst relation of the equation (14) give\nS(rk ) \u2265\n\n1\nkE T rk k2 .\n\u03bbmax (A) m 2\n6\n\n\fThis equation gives an idea to choose indices ij , j = 1, . . . , m. In fact, it shows that\nif these indices are the m components of the largest absolute values in rk , then the\nT\nlower bound of S(rk ) depends on kEm\nrk k22 , and will be as large as possible.\nIn [3], another theorem for the convergence of the method obtained by vi = eij was\npresented and an algorithm based upon this theorem was constructed for computing a\nsparse approximate inverse factor of an SPD matrix and was used as a preconditioner\nfor SPD linear system.\n\n4. Algorithm and its practical implementations\nHence, according to the results obtained in the previous section we can summarized the mD-DSM in the special case that vj = eij as following.\nAlgorithm 1: mD-DSM\n1. Choose an initial guess x0 \u2208 Rn to (1) and r = b \u2212 Ax0 .\n2. Until convergence, Do\n3.\nx := x0\n4.\nFor i = 1, . . . , n, Do\n5.\nSelect the indices i1 , i2 , . . . , im of r as defined in Theorem 3\n6.\nEm := [ei1 , ei2 , . . . , eim ]\nT\nT\nAEm )ym = Em\nr for ym\n7.\nSolve (Em\n8.\nx := x + Em ym\n9.\nr := r \u2212 AEm ym\n10.\nEndDo\n11.\nx0 := x and if x0 has converged then Stop\n12. EndDo\nT\nIn practice, we see that the matrix Em\nAEm is a principal submatrix of A with\ncolumn and row indices in {i1 , i2 , . . . , im }. Hence, we do not need any computation\nT\nfor computing the matrix Em\nAEm in step 7. For solving the linear system in step\n7 of this algorithm one can use the Cholesky factorization of the coefficient matrix.\nStep 8 of the algorithm may be written as\n\n7\n\n\f\u2022 For j = 1, 2, . . . , m\n\u2022\n\nxij := xij + (ym )j\n\n\u2022 EndDo\nHence only m components of the vector x are modified. Step 9 of the algorithm can\nbe written as\nm\nX\nr=r\u2212\n(ym )j aij\nj=1\n\nwhere aij is the ij column of the matrix A.\nIt can be seen that Algorithm 2 in [4] is an special case of this algorithm. In\nfact, if m = 2 and the indices i1 and i2 are chosen as i1 = i and i2 = i \u2212 ijgap\n(i2 = i \u2212 ijgap + n if i \u2264 ijgap ), where ijgap is a positive integer parameter less than\nn, then Algorithm 2 in [4] is obtained.\nAs we see, the first advantage of our algorithm over Algorithm 2 in [4] is that\nour algorithm chooses the indices {i1 , i2 , . . . , im }, automatically. Another advantage\nis that our algorithm chooses the indices such that the reduction in the square of the\nA-norm of the error is more than that of Algorithm 2 in [4]. Numerical experiments\nin the next section also confirm also this fact. The main advantage of Algorithm 2\nin [4] over our algorithm is that only m components of the current residual are computed whereas in our algorithm the residual vector should be computed for choosing\nm indices of largest components in absolute value.\n\n5. Numerical experiments\nIn this section we give some numerical experiments to compare our method with\nAlgorithm 2 in [4]. Numerical results have been obtained by some MATLAB codes.\nWe use all of the assumptions such as initial guess, exact solution, stopping criterion,\nand the examples used in [4]. Let b = Ae, where e is an n-vector whose elements\nare all equal to unity, i.e., e = (1, 1, . . . , 1)T . We use kxk+1 \u2212 xk k\u221e < 10\u22126 as the\nstopping criterion. An initial guess equal to x0 = (x1 , . . . , xn ), where xi = 0.001 \u00d7 i,\ni = 1, 2, . . . , n is chosen. For each of the systems we give the numerical experiments\nof Algorithm 2 in [4] with ijgap = 2 and 500 and our algorithm with m = 2, 3, 4 and 5.\n8\n\n\fTable 1: Results for the Example 1\nAlgorithm 2 in [4]\n6 (ijgap = 2)\n7 (ijgap = 500)\n\n2D-DSM 3D-DSM 4D-DSM 5D-DSM\n5\n4\n3\n2\n\nTable 2: Results for the Example 2\nAlgorithm 2 in [4]\n8 (ijgap = 2)\n9 (ijgap = 500)\n\n2D-DSM 3D-DSM 4D-DSM 5D-DSM\n7\n6\n4\n4\n\nExample 1. Let A = (aij ) where\naii = 4n,\n\nai,i+1 = ai+1,i = n,\n\naij = 0.5 for i = 1, 2, . . . , n, j 6= i, i + 1.\n\nWe also assume n = 1000. Numerical experiments in terms of iteration number were\nshown in Table 1.\nNumerical experiments presented in Table 1 show that the 2D-DSM method gives\nbetter results than the Algorithm 2 in [4]. This table also shows the effect of increase\nin m on the number of iterations for convergence.\nExample 2. Let A = (aij ) be the same matrix used in the previous example except\nthe diagonal entries are changed to\naii = 3n, i = 1, 2, . . . , n.\nNumerical experiments were given in Table 2.\nThis table also shows the advantages of our method on Algorithm 2 [4].\nExample 3. Our third set of test matrices used arise from standard five point\nfinite difference scheme to discretize\n\u2212\u25b3u + a(x, y)ux + b(x, y)uy + c(x, y)u = f (x, y),\n\nin \u03a9 = [0, 1] \u00d7 [0, 1],\n\nwhere a(x, y), b(x, y), c(x, y) and d(x, y) are given real valued functions. We consider\nthree following cases:\n9\n\n\fTable 3: Results for the Example 3\nCases\nCase 1\n\nAlgorithm 2 in [4] 2D-DSM 3D-DSM 4D-DSM 5D-DSM\n391 (ijgap = 2)\n226\n153\n116\n94\n323 (ijgap = 500)\n\nCase 2\n\n312 (ijgap = 2)\n256 (ijgap = 500)\n\n192\n\n131\n\n100\n\n80\n\nCase 3\n\n302 (ijgap = 2)\n250 (ijgap = 500)\n\n218\n\n151\n\n115\n\n93\n\nCase1 : a(x, y) = 0, b(x, y) = 10(x + y), c(x, y) = 10(x \u2212 y), f (x, y) = 0,\nCase2 : a(x, y) = \u221210(x + y), b(x, y) = \u221210(x \u2212 y), c(x, y) = 1, f (x, y) = 0,\nCase3 : a(x, y) = 10exy , b(x, y) = 10e\u2212xy , c(x, y) = 0, f (x, y) = 0.\nWe assume m = 32. In this case we obtain three SPD matrices of order n = 32 \u00d7 32\n[4] and used them as the coefficient of the linear systems. Numerical results were\ngiven in Table 3.\nThis table also confirm that our method is more effective that the Algorithm 2\n[4].\n\n5. Conclusion\nIn this paper a generalization of the 2D-DSPM [4] which itself is a generalization\nof 1D-DSPM [7] is presented. 1D-DSPM and 2D-DPM need to prescribed some subspaces of Rn for the projection steps. But our method in the spacial case chooses this\nsubspaces automatically. Theoretical analysis and numerical experiments presented\nin this paper showed that our method is more effective that 2D-DSPM.\n\n6. Acknowledgments\nThe author would like to thank Yan-Fei Jing for providing the matrices of Example\n3.\n10\n\n\fReferences\n[1] R. Barrett et al., Template for the solution of linear systems: building blocks for\niterative methods, SIAM Press: Philadelphia, 1994.\n[2] M. Benzi, Preconditioning techniques for large linear systems: A survey, J. Comput. Phys., 182 (2002) 418-477.\n[3] D. Khojasteh Salkuyeh and F. Toutounian, A sparse-sparse iteration for computing a sparse incomplete factorization of an SPD matrix, submitted.\n[4] Y.-F. Jing and T.-Z. Huang, On a new iterative method for solving linear systems\nand comparison results, J. Comput. Appl. Math., doi:10.1016/j.cam.2007.07.035,\n2007.\n[5] C.D. Meyer, Matrix analysis and applied linear algebra, SIAM, 2004.\n[6] Y. Saad, Iterative Methods for Sparse linear Systems, PWS press, New York,\n1995.\n[7] N. Ujevi\u0107, A new iterative method for solving linear systems, Appl. Math. Comput.\n179 (2006) 725730.\n\n11\n\n\f"}