{"id": "http://arxiv.org/abs/1104.5284v1", "guidislink": true, "updated": "2011-04-28T03:16:42Z", "updated_parsed": [2011, 4, 28, 3, 16, 42, 3, 118, 0], "published": "2011-04-28T03:16:42Z", "published_parsed": [2011, 4, 28, 3, 16, 42, 3, 118, 0], "title": "Content-Based Spam Filtering on Video Sharing Social Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.1947%2C1104.2012%2C1104.4531%2C1104.3639%2C1104.2062%2C1104.4672%2C1104.1335%2C1104.3364%2C1104.5284%2C1104.0115%2C1104.2657%2C1104.0723%2C1104.3123%2C1104.2350%2C1104.0707%2C1104.2591%2C1104.0082%2C1104.4562%2C1104.2524%2C1104.0296%2C1104.4336%2C1104.3089%2C1104.1168%2C1104.2987%2C1104.1302%2C1104.0108%2C1104.5370%2C1104.5516%2C1104.1924%2C1104.2865%2C1104.3385%2C1104.1714%2C1104.1126%2C1104.0943%2C1104.3588%2C1104.4467%2C1104.1929%2C1104.4830%2C1104.4549%2C1104.5452%2C1104.0744%2C1104.2647%2C1104.5506%2C1104.1008%2C1104.4466%2C1104.4771%2C1104.0852%2C1104.3093%2C1104.3490%2C1104.5106%2C1104.0592%2C1104.2834%2C1104.3724%2C1104.2261%2C1104.0095%2C1104.4867%2C1104.4148%2C1104.3305%2C1104.0331%2C1104.2506%2C1104.0130%2C1104.3441%2C1104.4237%2C1104.0942%2C1104.4617%2C1104.3208%2C1104.1000%2C1104.4752%2C1104.1769%2C1104.0704%2C1104.4106%2C1104.1620%2C1104.3013%2C1104.3288%2C1104.2923%2C1104.3862%2C1104.1816%2C1104.3155%2C1104.4185%2C1104.2718%2C1104.3294%2C1104.4004%2C1104.4049%2C1104.2794%2C1104.3869%2C1104.4169%2C1104.3264%2C1104.3257%2C1104.4622%2C1104.4613%2C1104.1616%2C1104.4792%2C1104.4981%2C1104.1471%2C1104.3129%2C1104.5543%2C1104.4843%2C1104.5018%2C1104.1846%2C1104.2738%2C1104.5455&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Content-Based Spam Filtering on Video Sharing Social Networks"}, "summary": "In this work we are concerned with the detection of spam in video sharing\nsocial networks. Specifically, we investigate how much visual content-based\nanalysis can aid in detecting spam in videos. This is a very challenging task,\nbecause of the high-level semantic concepts involved; of the assorted nature of\nsocial networks, preventing the use of constrained a priori information; and,\nwhat is paramount, of the context dependent nature of spam. Content filtering\nfor social networks is an increasingly demanded task: due to their popularity,\nthe number of abuses also tends to increase, annoying the user base and\ndisrupting their services. We systematically evaluate several approaches for\nprocessing the visual information: using static and dynamic (motionaware)\nfeatures, with and without considering the context, and with or without latent\nsemantic analysis (LSA). Our experiments show that LSA is helpful, but taking\nthe context into consideration is paramount. The whole scheme shows good\nresults, showing the feasibility of the concept.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.1947%2C1104.2012%2C1104.4531%2C1104.3639%2C1104.2062%2C1104.4672%2C1104.1335%2C1104.3364%2C1104.5284%2C1104.0115%2C1104.2657%2C1104.0723%2C1104.3123%2C1104.2350%2C1104.0707%2C1104.2591%2C1104.0082%2C1104.4562%2C1104.2524%2C1104.0296%2C1104.4336%2C1104.3089%2C1104.1168%2C1104.2987%2C1104.1302%2C1104.0108%2C1104.5370%2C1104.5516%2C1104.1924%2C1104.2865%2C1104.3385%2C1104.1714%2C1104.1126%2C1104.0943%2C1104.3588%2C1104.4467%2C1104.1929%2C1104.4830%2C1104.4549%2C1104.5452%2C1104.0744%2C1104.2647%2C1104.5506%2C1104.1008%2C1104.4466%2C1104.4771%2C1104.0852%2C1104.3093%2C1104.3490%2C1104.5106%2C1104.0592%2C1104.2834%2C1104.3724%2C1104.2261%2C1104.0095%2C1104.4867%2C1104.4148%2C1104.3305%2C1104.0331%2C1104.2506%2C1104.0130%2C1104.3441%2C1104.4237%2C1104.0942%2C1104.4617%2C1104.3208%2C1104.1000%2C1104.4752%2C1104.1769%2C1104.0704%2C1104.4106%2C1104.1620%2C1104.3013%2C1104.3288%2C1104.2923%2C1104.3862%2C1104.1816%2C1104.3155%2C1104.4185%2C1104.2718%2C1104.3294%2C1104.4004%2C1104.4049%2C1104.2794%2C1104.3869%2C1104.4169%2C1104.3264%2C1104.3257%2C1104.4622%2C1104.4613%2C1104.1616%2C1104.4792%2C1104.4981%2C1104.1471%2C1104.3129%2C1104.5543%2C1104.4843%2C1104.5018%2C1104.1846%2C1104.2738%2C1104.5455&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this work we are concerned with the detection of spam in video sharing\nsocial networks. Specifically, we investigate how much visual content-based\nanalysis can aid in detecting spam in videos. This is a very challenging task,\nbecause of the high-level semantic concepts involved; of the assorted nature of\nsocial networks, preventing the use of constrained a priori information; and,\nwhat is paramount, of the context dependent nature of spam. Content filtering\nfor social networks is an increasingly demanded task: due to their popularity,\nthe number of abuses also tends to increase, annoying the user base and\ndisrupting their services. We systematically evaluate several approaches for\nprocessing the visual information: using static and dynamic (motionaware)\nfeatures, with and without considering the context, and with or without latent\nsemantic analysis (LSA). Our experiments show that LSA is helpful, but taking\nthe context into consideration is paramount. The whole scheme shows good\nresults, showing the feasibility of the concept."}, "authors": ["Antonio da Luz", "Eduardo Valle", "Arnaldo Araujo"], "author_detail": {"name": "Arnaldo Araujo"}, "author": "Arnaldo Araujo", "links": [{"href": "http://arxiv.org/abs/1104.5284v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1104.5284v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.MM", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1104.5284v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1104.5284v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "CONTENT-BASED SPAM FILTERING ON VIDEO SHARING SOCIAL NETWORKS\nAntonio da Luz1,2, Eduardo Valle3, Arnaldo Araujo1\n1\n\nNPDI Lab - DCC / UFMG, Belo Horizonte, MG, Brazil\nFederal Institute of Technology of Tocantins \u2013 IFTO, Para\u00edso, TO, Brazil\n3\nRECOD Lab - IC / UNICAMP, Campinas, SP, Brazil\nE-mail: daluz@dcc.ufmg.br, mail@eduardovalle.com, arnaldo@dcc.ufmg.br\n2\n\nABSTRACT\nIn this work we are concerned with the detection of spam in\nvideo sharing social networks. Specifically, we investigate\nhow much visual content-based analysis can aid in detecting\nspam in videos. This is a very challenging task, because of\nthe high-level semantic concepts involved; of the assorted\nnature of social networks, preventing the use of constrained\na priori information; and, what is paramount, of the contextdependent nature of spam. Content filtering for social\nnetworks is an increasingly demanded task: due to their\npopularity, the number of abuses also tends to increase,\nannoying the user base and disrupting their services. We\nsystematically evaluate several approaches for processing\nthe visual information: using static and dynamic (motionaware) features, with and without considering the context,\nand with or without latent semantic analysis (LSA). Our\nexperiments show that LSA is helpful, but taking the context\ninto consideration is paramount. The whole scheme shows\ngood results, showing the feasibility of the concept.\nIndex Terms- Semantic Video Classification, Latent\nSemantic Analysis, Bag-of-Features, SIFT, STIP\n1. INTRODUCTION\nIn this paper, we are concerned with the detection of spam\non video sharing social networks - online communities\nbuilt upon the production, sharing and watching of short\nvideo clips, which have been nourished by the\npopularization of broadband web access and the availability\nof cheap video acquisition devices. The crowds of users who\nemploy the services of websites like Dailymotion, MetaCafe\nand YouTube, not only post and watch videos, but also share\nratings, comments, \"favorite lists\" and other personal\nappreciation data.\nThe emergence of those networks has created a demand\nfor specialized tools, including mechanisms to control\nabuses and terms-of-use violations. Indeed, the success of\nsocial networks has been inevitably accompanied by the\nemergence of users with non-collaborative behavior, which\nprevents them from operating evenly. Those behaviors\n\ninclude instigating the anger of other users (trolling, in the\nweb jargon), diffusing materials of genre inappropriate for\nthe target community (e.g., diffusing advertisement or\npornography in inadequate channels), or manipulating\nillegitimately popularity ratings.\nNon-collaborative behavior pollutes the communication\nchannels with unrelated information, and prevents the virtual\ncommunities from reaching their original goals of\ndiscussion, learning and entertainment. It alienates\nlegitimate users and depreciates the social network value as\na whole [1].\nSpecifically, we are interested on detecting spam on\nthreads of video answers, a popular feature of some social\nnetworks, like YouTube, where the user can post a video in\nresponse to another. Here, we consider \"spam\" a video\nanswer whose subject is unrelated with the original video\n(sometimes advertisement, commercial or not; sometimes\nvideos posted in the hope to attract attention; sometimes\nvideos posted intentionally to anger the other users). Figure\n1 illustrates the diversity of the spam phenomenon.\n2. RELATED WORK\nThe identification of pollution in online video social\nnetworks is a new topic with very few published works.\nAfter extensive research, we could only find [2] and [3]. The\nfirst tries to identify non-cooperative users and addresses\nboth spamming and ballot stuffing (which they call\n\"promoting\"), by analyzing parameters like tags, user\nprofile, the user posting behavior and the user social\nrelations. The second classifies videos, accordingly to their\npattern of access by users, into three categories: quality (the\nnormal ones), viral (videos which experience a sudden surge\nin popularity) and junk (spurious videos, like spam).\nNeither work use the video content itself for\nclassification, instead, they rely on metadata and access logs.\nBy contrast, our scheme mainly relies on the visual content.\nFew works have been proposed to detect objectionable\ncontent in visual documents (images and videos). From\nthose, the vast majority is concerned with nudity,\npornography or graphical violence.\n\n\fFigure 1: The complexity of spam. The first frames (topmost\nframes, red outline) are from the original video, which is related to\nthe Free Hugs Campaign. The next three videos (one video per\nline) are legitimate ones. And, the last three are from spam videos.\n\nThe vast majority of pornographic detectors in images\nor videos is based on the detection of exposed skin [4] and\nseriously suffers from false positives of face close-ups, sport\nscenes or other innocent skin exposures. The available\nliterature on violence detectors for content-filtering tends to\nconcentrate on feature films and to give the soundtrack a\nvery special attention (e.g. [5]). That kind of specialization\nwarrants good performances, but makes the adaptation of\nthose techniques difficult to the chaotic nature of social\nnetworks.\nRecently, however, much attention has been devoted to\nless constrained approaches, using general-purpose features\nand classifiers. From those general approaches, one of most\nsuccessful is the visual dictionary of local features.\nThe acceptance of local features as a broad technique of\nimage description was an important watershed in the history\nof image understanding. Local features, like the popular\nSIFT descriptors [6], allow excellent discriminating power\nand great robustness to geometric and photometric\ntransformations. If they were initially available only for\nstatic images, nowadays there exist local features that take\n\ninto account the spatio-temporal nature of video, one of the\nmost popular being STIP [7].\nThe discriminating power of local descriptors is\nextremely advantageous when matching objects in scenes, or\nretrieving specific target documents. However, when\nconsidering high-level semantic categories, it quickly\nbecomes an obstacle, since the ability to generalize becomes\nthen essential. A solution to this problem is to quantize the\ndescription spaces by using codebooks of local descriptors,\nin a technique sometimes named \"visual dictionary\". The\nvisual dictionary is nothing more than a representation which\nsplits the descriptor space into multiple regions, usually by\nemploying non-supervised learning techniques, like\nclustering. Each region becomes then a \"visual word\" and is\nincluded in a \"dictionary of visual words\". The idea is that\ndifferent regions of the description space will become\nassociated to different semantic concepts, for example, parts\nof the human body, corners of furniture, vegetation, clear\nsky, clouds, features of buildings, etc. The technique has\nbeen employed successfully on several works for retrieval\nand classification of visual documents [8].\nIn addition to moderating the discriminating power of\ndescriptors, the dictionaries allow adapting to visual\ndocuments techniques formerly available only to textual\ndata. Among those borrowings, one of the most successful\nhas been the technique of bags of words (which considers\ntextual documents simply as sets of words, ignoring any\ninherent structure). The equivalent in the CBIR universe has\nbeen called bags of visual words, bags of features or bags of\nvisual features, sometimes abbreviated as BoVF. It greatly\nsimplifies document description, which becomes a histogram\nof the visual words it contains (Figure 2). The introduction\nof this technique had a huge impact on content-based\nretrieval and classification of visual documents [9].\nThe BoVF model also opens the opportunity to employ\nthe Latent Semantic Analysis (LSA) [10].\nThe LSA is nothing more than an operation of change of\nbasis in the document description in order to make more\nexplicit some latent associations between them. Using the\ninformation provided by the bags of words, we create an\noccurrence matrix (telling which word occurs in which\ndocument, and usually applying some frequency\nnormalization). LSA will then apply Singular Value\nDecomposition (SVD) to project this data in a new space of\n\"topics\". Usually the dimensionality of the topics space will\nbe reduced, by discarding the component of low singular\nvalue. The documents can then be described by their\nhistogram of topics instead of words.\nLSA was initially intended for large corpora of text, but\nusing the metaphor of the visual words has allowed\nemploying it for visual documents. It has been applied on\nimage tasks, achieving good results (e.g., [11] [12]).\n\n\f3. DETECTING SPAM USING VISUAL CONTENT\nPerhaps the most serious problem to detect spam in video\nthreads is its relative, context-dependent, nature. Accepting\nthe definition that a spam video is simply a video unrelated\nto the thread topic, the same video (e.g., a viral video with a\ncelebrity breakdown) may be spam in a thread (e.g., a thread\nabout how to cook asparagus correctly), but not in another\n(e.g., a thread about famous people behaving oddly). It is, of\ncourse, possible that some videos are intrinsically more\nprobable to be used as spam (like viral videos) than others\n(like someone cooking asparagus), but this does not solve,\nby itself, the problem.\nThe other serious difficulty is the large variety of visual\ncontent that can be found in legitimate elements. Even\nconsidering a very restricted thread (e.g., \"how to cook\nasparagus\") and observing only the legitimate answers, the\ndiversity of the videos is overwhelming. Even a human\noperator has sometimes difficulty in establishing the\nlegitimacy \u00d7 spam status of the videos by watching just the\nimages.\nFinally, we face other difficulties, dictated by the\nflexible nature of the social networks. The number of videos\nin the threads is not fixed, and is usually quite small.\nExamples of spam are uncommon and tend to appear in\nclusters. This is fortunate, in one way, but it makes training\nthe classifier quite difficult.\nTo test those hypotheses, we have built a video\nclassification system, based on the following scheme:\n1. The classes considered were spam (positive) and\nlegitimate (negative);\n2. We start by extracting the low level features. Those\nwere either STIP [7] features extracted from the videos,\neither SIFT [6] features extracted from keyframes of the\nvideo (the keyframes were extracted using a state-of-the-art\nstatic video summarization method [13]).\n3. The codebook (visual dictionary) was constructed\nby choosing at random 5000 low level features.\n4. Using the codebook, each video was given a\ndescription in terms of a bag-of-visual-features (BoVF),\nwhich is simply a histogram of the low-level features,\nquantized by their proximity to the codebook;\n5. As an optional step, the BoVF is projected in the\nlatent semantic space was LSA (preserving all topics with\nnon-zero singular values);\n6. Also as an optional step, context information can be\nincorporated by making the description relative to the\noriginal videos. This is made simply by taking the video\nfeature vector (either in the BoVF or topic space) and\nsubtracting the feature vector of its corresponding original\nvideo;\n7. To classify the videos, we have used the wellknown SVM with linear kernel.\nThe idea of step 5 is to detect cross-video appearance\ncorrelations which might be revealing of semantic patterns.\n\nIn a nutshell, while the BoVF-model takes the appearances\nat \"face value\", the Visual Topics-model of LSA looks for\nhidden patterns that may indicate that different visual words\nmay be related or that certain combinations of visual words\nmay make sense together. That might alleviate the problem\nof extreme visual diversity in legitimate and spam patterns.\nStep 6 tries to solve the problem of context, creating\nbags of \"differences\" between the answer video and the\noriginal video (those differences can be between words or\ntopics). Abstractly, the context-free video description was a\nvector in the BoVF space or the topic space. The contextaware description is now the vector difference between the\nvideo and its original one. This allows us to take the\ndifferent contexts (the different threads) while keeping the\nclassification model extremely simple (only two classes,\nspam and legitimate, for the entire dataset). As far as we\nknow, this use of \"bags of differences\" as a context-aware\ndescription model is original.\n4. EXPERIMENTS AND RESULTS\nGiven the novelty of this application, it is unsurprising that\nno standard database is available for evaluation purposes.\nTherefore, we have collected ourselves 8182 videos from 84\nthreads, chosen at random from the \"Most Responded\nVideos\" list generated by YouTube. The collection took\nplace in mid-2008. We have selected the videos in the \"most\nresponded\" list, because they form long threads, often with a\nlot of spam.\nFrom the 8182 videos, 84 were the original \"heads of\nthe thread\". From the remainder answers, manual inspection\ndetermined 3420 to be legitimate and 4678 to be spam.\nDeciding which videos are spam is sometimes hard, even for\nhumans: we have considered them as spam when their visual\ncontents did not match the subject of the thread. In case of\ndoubt, we adopted the policy of [3] and marked the video as\nlegitimate.\nFor the experiment reported here, we have subsampled\nthe original dataset, randomly selecting (besides the 84\noriginals) 1000 legitimate and 1000 spam.\nThe experimental design was a classical 5-fold crossvalidation, generating approximately 1600 videos for\ntraining and 400 for testing on each fold. The numbers\nreported are the average of the folds.\nFigure 4 is presented the results with the evaluation of\nthe two purposed approaches, with both visual features, and\na baseline result in an experiment without considers the\ncontext location of the videos (traditional BoVF).\nThe results indicate that the context information is\ncritical to identifying spam using a two-class classification\nmodel. The visual characteristics of the video allows\nfiltering some of the spam (blue circles in Figure 4): this is\ninteresting and show that, at least in our sample, videos used\nas spam tended to share some visual characteristics.\n\n\fHowever, even the worst context-aware experiment (red data\npoints) was best than the best context-blind experiment.\n100%\n90%\n\nTrue Positive Rate\n\n80%\n\nevidences, such as those provided by the soundtrack,\nmetadata, social network statistics, etc. Currently we are\ninvestigation on how to best incorporate our visual classifier\nin a system taking into account all those evidences.\n6. ACKNOWLEDGMENTS\n\n70%\n60%\n\nThe authors are thankful to the Brazilian agencies CNPq,\nCAPES, FAPEMIG and FAPESP, for the financial support.\n\n50%\n40%\n\n7. REFERENCES\n\n30%\n20%\n10%\n0%\n0%\n\n10%\n\n20%\n\n30%\n\n40%\n\n50%\n\n60%\n\n70%\n\n80%\n\n90%\n\n100%\n\nFalse Positive Rate\nVisual Words - SIFT\n\nVisual Words - STIP\n\nVisual Differences - SIFT\n\nVisual Differences - STIP\n\nTopics - SIFT\n\nTopics - STIP\n\nTopic Differecences - SIFT\n\nTopic Differences - STIP\n\nRandom\n\nFigure 4: Experimental results. The sweet spot is the upper left\ncorner. The data points represent different choices: empty \u00d7 filled\nsymbols are SIFT \u00d7 STIP; circles \u00d7 squares are BoVF \u00d7 topics;\nblue \u00d7 red are without and with context. The use of context is\ncritical (the worst experiment with context is still better than the\nbest without context). In this case, the use of LSA (to project\nBoVF on topic space) is best (red squares).\n\nThe use of LSA to pass from the BoVF space to the\ntopic space proved advantageous, especially when using\nSIFT descriptors, which, in association with context\ninformation have, in the topic space, the best performance\noverall.\nThe choice of low-level feature is somewhat\ninconclusive. For the context-blind experiments, they\npresent mainly a different in bias (with SIFT tending to have\nmore false positives, and STIP more false negatives). For the\ncontext-aware experiments, STIP was better in the BoVF\nspace, and SIFT was better in the topic space. SIFT tends to\ngenerate more local features than STIP: it is thus possible\nthat the difference observed is a consequence of SIFT being\ndenser.\n5. DISCUSSION\nRemoving content automatically is only possible when false\npositive rates are very low, because removing a legitimate\nanswer is much more problematic than accepting a spurious\none. At this stage, our technique, used in isolation, does not\nallow such low rates and thus cannot be used to forceful\nremoving content from the social network. That does not\nmean, however, that the technique is of no practical value.\nAn interesting strategy may be employed to make it feasible:\ncombining it with manual inspection of the \"suspect\" videos,\nin order to only remove those which are indeed deemed as\nillegitimate.\nOf course, our current approach explores only the visual\ninformation contained in the video, and thus is only the\nlower bound on what could be obtained adding other\n\n[1] T. Deselaers, L. Pimenidis and H. Ney. \"Bag-of-Visual-Words\nModels for Adult Image Classification and Filtering\", In:\nInternational Conference on Pattern Recognition, pp. 1-4, 2008.\n[2] F. Benevenuto, T. Rodrigues, V. Almeida, J. Almeida and M.\nGon\u00e7alves, \"Detecting Spammers and Content Promoters in Online\nVideo Social Networks\", In: International ACM SIGIR Conference\non Research and Development in Information Retrieval, pp. 620627, 2009.\n[3] R. Crane and D. Sornette, \"Robust Dynamic Classes Revealed\nby Measuring the Response Function of a Social System\", In:\nNational Academy of Sciences, 105(41):15649-15653, 2008.\n[4] W. Kelly, A. Donnellan, D. Molloy. \"Screening for\nObjectionable Images: A Review of Skin Detection Techniques\",\nIn: International Machine Vision and Image Processing\nConference, pp. 151-158, 2008.\n[5] T. Giannakopoulos, D. I. Kosmopoulos, A. Aristidou, and S.\nTheodoridis, \"Violence Content Classification Using Audio\nfeatures\", In: Hellenic Artificial Intelligence Conference SETN-06,\nLNAI 3955, pp. 502-507, 2006.\n[6] D. G. Lowe. \"Distinctive Image Features from Scale-Invariant\nKeypoints\", In: International Journal of Computer Vision, vol. 60,\nno. 2, pp. 91-110, 2004.\n[7] I. Laptev. \"On Space-Time Interest Points\", In: International\nJournal of Computer Vision, vol 64, number 2/3, p.107-123, 2005.\n[8] J. Sivic and A. Zisserman. \"Video Google: A Text Retrieval\nApproach to Object Matching in Videos\", In: IEEE International\nConference on Computer Vision, pp. 1470-1477, 2003.\n[9] J. Yang, Y.-G. Jiang, A. G. Hauptmann and C.-W. Ngo.\n\"Evaluating Bag-of-Visual-Words Representations in Scene\nClassification\", In: ACM MIR'07, pp. 197-206, 2007.\n[10] T. Landauer, P. Foltz and D. Laham. \"Introduction to Latent\nSemantic Analysis\". Discourse Processes, 25, 259-284, 1998.\n[11] J. C. Caicedo, J. G. Moreno, E. A. Ni\u00f1o, and F. A. Gonzalez.\n\"Combining visual features and text data for medical image\nretrieval using latent semantic kernels\". In ACM MIR'10. ACM,\nNew York, NY, USA, 359-366, 2010.\n[12] K. Yanai and K. Barnard. \"Region-based automatic web\nimage selection\". In ACM MIR'2010. ACM, New York, NY, USA,\n305-312, 2010.\n[13] S. Avila, A. Lopes, A. da Luz Jr., and A. de A. Araujo.\n\"VSUMM: A mechanism designed to produce static video\nsummaries and a novel evaluation method\". Pattern Recogn. Lett.\n32, 1, 56-68, 2011.\n\n\f"}