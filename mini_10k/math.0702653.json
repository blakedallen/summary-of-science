{"id": "http://arxiv.org/abs/math/0702653v1", "guidislink": true, "updated": "2007-02-22T13:22:31Z", "updated_parsed": [2007, 2, 22, 13, 22, 31, 3, 53, 0], "published": "2007-02-22T13:22:31Z", "published_parsed": [2007, 2, 22, 13, 22, 31, 3, 53, 0], "title": "From $\u03b5$-entropy to KL-entropy: Analysis of minimum information\n  complexity density estimation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0702075%2Cmath%2F0702500%2Cmath%2F0702385%2Cmath%2F0702648%2Cmath%2F0702501%2Cmath%2F0702194%2Cmath%2F0702671%2Cmath%2F0702386%2Cmath%2F0702336%2Cmath%2F0702230%2Cmath%2F0702046%2Cmath%2F0702084%2Cmath%2F0702348%2Cmath%2F0702392%2Cmath%2F0702265%2Cmath%2F0702518%2Cmath%2F0702591%2Cmath%2F0702752%2Cmath%2F0702002%2Cmath%2F0702489%2Cmath%2F0702766%2Cmath%2F0702703%2Cmath%2F0702735%2Cmath%2F0702552%2Cmath%2F0702627%2Cmath%2F0702569%2Cmath%2F0702202%2Cmath%2F0702155%2Cmath%2F0702512%2Cmath%2F0702303%2Cmath%2F0702610%2Cmath%2F0702748%2Cmath%2F0702102%2Cmath%2F0702699%2Cmath%2F0702032%2Cmath%2F0702492%2Cmath%2F0702097%2Cmath%2F0702448%2Cmath%2F0702299%2Cmath%2F0702618%2Cmath%2F0702179%2Cmath%2F0702704%2Cmath%2F0702289%2Cmath%2F0702542%2Cmath%2F0702172%2Cmath%2F0702144%2Cmath%2F0702170%2Cmath%2F0702420%2Cmath%2F0702519%2Cmath%2F0702815%2Cmath%2F0702886%2Cmath%2F0702611%2Cmath%2F0702318%2Cmath%2F0702334%2Cmath%2F0702875%2Cmath%2F0702851%2Cmath%2F0702128%2Cmath%2F0702653%2Cmath%2F0702564%2Cmath%2F0702589%2Cmath%2F0702269%2Cmath%2F0702628%2Cmath%2F0702043%2Cmath%2F0702563%2Cmath%2F0702660%2Cmath%2F0702693%2Cmath%2F0702162%2Cmath%2F0702095%2Cmath%2F0702675%2Cmath%2F0702780%2Cmath%2F0702200%2Cmath%2F0702281%2Cmath%2F0702593%2Cmath%2F0702583%2Cmath%2F0702636%2Cmath%2F0702587%2Cmath%2F0702019%2Cmath%2F0702189%2Cmath%2F0702206%2Cmath%2F0702471%2Cmath%2F0702094%2Cmath%2F0702868%2Cmath%2F0702397%2Cmath%2F0702090%2Cmath%2F0702773%2Cmath%2F0702005%2Cmath%2F0702662%2Cmath%2F0702783%2Cmath%2F0702357%2Cmath%2F0702261%2Cmath%2F0702678%2Cmath%2F0702330%2Cmath%2F0702891%2Cmath%2F0702379%2Cmath%2F0702616%2Cmath%2F0702781%2Cmath%2F0702827%2Cmath%2F0702553%2Cmath%2F0702264%2Cmath%2F0702338%2Cmath%2F0702173&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "From $\u03b5$-entropy to KL-entropy: Analysis of minimum information\n  complexity density estimation"}, "summary": "We consider an extension of $\\epsilon$-entropy to a KL-divergence based\ncomplexity measure for randomized density estimation methods. Based on this\nextension, we develop a general information-theoretical inequality that\nmeasures the statistical complexity of some deterministic and randomized\ndensity estimators. Consequences of the new inequality will be presented. In\nparticular, we show that this technique can lead to improvements of some\nclassical results concerning the convergence of minimum description length and\nBayesian posterior distributions. Moreover, we are able to derive clean\nfinite-sample convergence bounds that are not obtainable using previous\napproaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0702075%2Cmath%2F0702500%2Cmath%2F0702385%2Cmath%2F0702648%2Cmath%2F0702501%2Cmath%2F0702194%2Cmath%2F0702671%2Cmath%2F0702386%2Cmath%2F0702336%2Cmath%2F0702230%2Cmath%2F0702046%2Cmath%2F0702084%2Cmath%2F0702348%2Cmath%2F0702392%2Cmath%2F0702265%2Cmath%2F0702518%2Cmath%2F0702591%2Cmath%2F0702752%2Cmath%2F0702002%2Cmath%2F0702489%2Cmath%2F0702766%2Cmath%2F0702703%2Cmath%2F0702735%2Cmath%2F0702552%2Cmath%2F0702627%2Cmath%2F0702569%2Cmath%2F0702202%2Cmath%2F0702155%2Cmath%2F0702512%2Cmath%2F0702303%2Cmath%2F0702610%2Cmath%2F0702748%2Cmath%2F0702102%2Cmath%2F0702699%2Cmath%2F0702032%2Cmath%2F0702492%2Cmath%2F0702097%2Cmath%2F0702448%2Cmath%2F0702299%2Cmath%2F0702618%2Cmath%2F0702179%2Cmath%2F0702704%2Cmath%2F0702289%2Cmath%2F0702542%2Cmath%2F0702172%2Cmath%2F0702144%2Cmath%2F0702170%2Cmath%2F0702420%2Cmath%2F0702519%2Cmath%2F0702815%2Cmath%2F0702886%2Cmath%2F0702611%2Cmath%2F0702318%2Cmath%2F0702334%2Cmath%2F0702875%2Cmath%2F0702851%2Cmath%2F0702128%2Cmath%2F0702653%2Cmath%2F0702564%2Cmath%2F0702589%2Cmath%2F0702269%2Cmath%2F0702628%2Cmath%2F0702043%2Cmath%2F0702563%2Cmath%2F0702660%2Cmath%2F0702693%2Cmath%2F0702162%2Cmath%2F0702095%2Cmath%2F0702675%2Cmath%2F0702780%2Cmath%2F0702200%2Cmath%2F0702281%2Cmath%2F0702593%2Cmath%2F0702583%2Cmath%2F0702636%2Cmath%2F0702587%2Cmath%2F0702019%2Cmath%2F0702189%2Cmath%2F0702206%2Cmath%2F0702471%2Cmath%2F0702094%2Cmath%2F0702868%2Cmath%2F0702397%2Cmath%2F0702090%2Cmath%2F0702773%2Cmath%2F0702005%2Cmath%2F0702662%2Cmath%2F0702783%2Cmath%2F0702357%2Cmath%2F0702261%2Cmath%2F0702678%2Cmath%2F0702330%2Cmath%2F0702891%2Cmath%2F0702379%2Cmath%2F0702616%2Cmath%2F0702781%2Cmath%2F0702827%2Cmath%2F0702553%2Cmath%2F0702264%2Cmath%2F0702338%2Cmath%2F0702173&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider an extension of $\\epsilon$-entropy to a KL-divergence based\ncomplexity measure for randomized density estimation methods. Based on this\nextension, we develop a general information-theoretical inequality that\nmeasures the statistical complexity of some deterministic and randomized\ndensity estimators. Consequences of the new inequality will be presented. In\nparticular, we show that this technique can lead to improvements of some\nclassical results concerning the convergence of minimum description length and\nBayesian posterior distributions. Moreover, we are able to derive clean\nfinite-sample convergence bounds that are not obtainable using previous\napproaches."}, "authors": ["Tong Zhang"], "author_detail": {"name": "Tong Zhang"}, "author": "Tong Zhang", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/009053606000000704", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/math/0702653v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0702653v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published at http://dx.doi.org/10.1214/009053606000000704 in the\n  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62C10, 62G07 (Primary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0702653v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0702653v1", "journal_reference": "Annals of Statistics 2006, Vol. 34, No. 5, 2180-2210", "doi": "10.1214/009053606000000704", "fulltext": "arXiv:math/0702653v1 [math.ST] 22 Feb 2007\n\nThe Annals of Statistics\n2006, Vol. 34, No. 5, 2180\u20132210\nDOI: 10.1214/009053606000000704\nc Institute of Mathematical Statistics, 2006\n\nFROM \u03b5-ENTROPY TO KL-ENTROPY: ANALYSIS OF MINIMUM\nINFORMATION COMPLEXITY DENSITY ESTIMATION\nBy Tong Zhang\nYahoo Research\nWe consider an extension of \u03b5-entropy to a KL-divergence based\ncomplexity measure for randomized density estimation methods. Based\non this extension, we develop a general information-theoretical inequality that measures the statistical complexity of some deterministic and randomized density estimators. Consequences of the new inequality will be presented. In particular, we show that this technique\ncan lead to improvements of some classical results concerning the\nconvergence of minimum description length and Bayesian posterior\ndistributions. Moreover, we are able to derive clean finite-sample convergence bounds that are not obtainable using previous approaches.\n\n1. Introduction. The purpose of this paper is to study a class of complexity minimization based density estimation methods using a generalization\nof \u03b5-entropy, which has become a central technical tool in the traditional\nfinite-sample convergence analysis. Specifically, we derive a simple yet general information-theoretical inequality that can be used to measure the convergence of this very basic inequality.\nWe shall first introduce basic notation used in the paper. Consider a\nsample space X and a measure \u03bc on X (with respect to some \u03c3-field). In\nstatistical inference, nature picks a probability measure Q on X which is\nunknown. We assume that Q has a density q with respect to \u03bc. In density\nestimation, we consider a set of probability densities p(*|\u03b8) (with respect\nto \u03bc on X ) indexed by \u03b8 \u2208 \u0393. Without causing any confusion, we may also\noccasionally denote the model family {p(*|\u03b8) : \u03b8 \u2208 \u0393} by the same symbol\n\u0393. Throughout this paper, we always denote the true underlying density\nby q, and we do not assume that q belongs to the model class \u0393. Given\n\u0393, our goal is to select a density p(*|\u03b8) \u2208 \u0393 based on the observed data\nReceived November 2003; revised October 2005.\nAMS 2000 subject classifications. 62C10, 62G07.\nKey words and phrases. Bayesian posterior distribution, minimum description length,\ndensity estimation.\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2006, Vol. 34, No. 5, 2180\u20132210. This reprint differs from the original in\npagination and typographic detail.\n1\n\n\f2\n\nT. ZHANG\n\nX = {X1 , . . . , Xn } \u2208 X n , such that p(*|\u03b8) is as close to q as possible when\nmeasured by a certain distance function (which we shall specify later).\nIn the framework considered in this paper, we assume that there is a\nprior distribution d\u03c0(\u03b8) on the parameter space \u0393 that is independent of\nthe observed data. For notational simplicity, we shall call any observation\nX dependent probability density \u0175X (\u03b8) on \u0393 (measurable on X n \u00d7 \u0393) with\nrespect to d\u03c0(\u03b8) a posterior randomization measure, or simply a posterior.\nIn particular, a posterior randomization measure in our sense is not limited to a Bayesian posterior distribution, which has a very specific meaning.\nWe are interested in the density estimation performance of randomized estimators that draw \u03b8 according to posterior randomization measures \u0175X (\u03b8)\nobtained from a class of density estimation schemes. We should note that\nin this framework, our density estimator is completely characterized by the\nassociated posterior \u0175X (\u03b8).\nThe paper is organized as follows. In Section 2, we introduce a generalization of \u03b5-entropy for randomized estimation methods, which we call\nKL-entropy. Then a fundamental information-theoretical inequality, which\nforms the basis of our approach, will be obtained. Section 3 introduces the\ngeneral information complexity minimization (ICM) density estimation formulation, where we derive various finite-sample convergence bounds using\nthe fundamental information-theoretical inequality established earlier. Sections 4 and 5 apply the analysis to the case of minimum description length\n(MDL) estimators and to the convergence of Bayesian posterior distributions. In particular, we are able to simplify and improve most results in [1]\nas well as various recent analysis on the consistency and concentration of\nBayesian posterior distributions. Some concluding remarks will be presented\nin Section 6.\nThroughout this paper, we ignore the measurability issue, and assume\nthat all quantities appearing in the derivations are measurable. Similarly to\nempirical process theory [14], the analysis can also be written in the language\nof outer-expectations, so that the measurability requirement imposed in this\npaper can be relaxed.\n2. The basic information-theoretical inequality. In this section we introduce an information-theoretical complexity measure of randomized estimators represented as posterior randomization measures. As we shall see, this\nquantity directly generalizes the concept of \u03b5-entropy for deterministic estimators. We also develop a simple yet very general information-theoretical\ninequality, which bounds the convergence behavior of an arbitrary randomized estimator using the introduced complexity measure. This inequality is\nthe foundation of the approach introduced in this paper.\n\n\fMINIMUM COMPLEXITY ESTIMATION\n\n3\n\nDefinition 2.1. Consider a probability density w(*) on \u0393 with respect\nto \u03c0. The KL-divergence DKL (w d\u03c0||d\u03c0) is defined as\nDKL (w d\u03c0||d\u03c0) =\n\nZ\n\nw(\u03b8) ln w(\u03b8) d\u03c0(\u03b8).\n\n\u0393\n\nFor any posterior randomization measure \u0175X , we define its KL-entropy with\nrespect to \u03c0 as DKL (\u0175X d\u03c0||d\u03c0).\nNote that DKL (w d\u03c0||d\u03c0) may not always be finite. However, it is always\nnonnegative.\nKL-divergence is a rather standard information-theoretical concept. In\nthis section we show that it can be used to measure the complexity of a\nrandomized estimator. We can immediately see that the quantity directly\ngeneralizes the concept of \u03b5-entropy on an \u03b5-net; assuming that we have N\npoints in an \u03b5-net, we may consider a prior that puts a mass of 1/N on every\npoint. It is easy to see that any deterministic estimator in the \u03b5-net can be\nregarded as a randomized estimator that is concentrated on one of the N\npoints with posterior weight N (and weight of zero elsewhere). Clearly this\nestimator has a KL-entropy of ln N , which is essentially the \u03b5-entropy. In\nfact, it is also easy to verify that any randomized estimator on the \u03b5-net\nhas a KL-entropy bounded by its \u03b5-entropy ln N . Therefore \u03b5-entropy is the\nworst-case KL-entropy on an \u03b5-net with a uniform prior.\nThe concept of \u03b5-entropy can be regarded as a notion to measure the\ncomplexity of an explicit discretization, usually for a deterministic estimator\non a discrete \u03b5-net. The concept of KL-entropy can be regarded as a notation\nto measure the complexity of a randomized estimation method, where the\ndiscretization is done implicitly through randomization with respect to an\narbitrary prior. This difference is important for practical purposes since\nit is usually impossible (or very difficult) to perform computation on an\nexplicitly discretized \u03b5-net. Therefore estimators based on \u03b5-nets are often\nof theoretical interest only. However, it is often feasible to draw samples\nfrom a posterior randomization measure with respect to a continuous prior\nby using standard Monte Carlo techniques. Therefore randomized estimation\nmethods are potentially useful for practical problems.\nSince KL-entropy allows nonuniform priors, the concept can directly characterize local adaptivity of randomized estimators when we put more prior\nmass in certain regions of the model family. In contrast, \u03b5-entropy is a notation that tries to treat every part of the space equally, which may not give\nthe best possible results. For example, for convergence of posterior distributions, the fact that entropy conditions are not always the most appropriate\nwas pointed out in [4], pages 522\u2013523. The issue of adaptivity (and related\nnonuniform prior) cannot be directly addressed with \u03b5-entropy. In the literature, one has to employ additional techniques such as peeling (e.g., see\n\n\f4\n\nT. ZHANG\n\n[13]) for this purpose. As a comparison, the ability to use a nonuniform prior\ndirectly in our analysis is conceptually useful. Putting a large prior mass in\na certain region indicates that we want to achieve a more accurate estimate\nin that region, in exchange for slower convergence in a region with smaller\nprior mass. The prior structure reflects our belief that the true density is\nmore likely to have a certain form than some alternative forms. Therefore\nthe theoretical analysis should also imply a more accurate estimate when\nwe are lucky enough to guess the true density q correctly by putting a large\nprior mass around it. As we will see later, finite-sample convergence bounds\nderived in this paper using KL-entropy have this behavior.\nNext we prove a simple information-theoretical inequality using the KLentropy of randomized estimators, which forms the basis of our analysis. For\na real-valued function f (\u03b8) on \u0393, we denote by E\u03c0 f (\u03b8) the expectation of\nf (*) with respect to \u03c0. Similarly, for a real-valued function l(x) on X , we\ndenote by Eq l(x) the expectation of l(*) with respect to the true underlying\ndistribution q. We also use EX to denote the expectation with respect to\nthe observation X (n independent samples from q).\nThe key ingredient of our analysis using KL-entropy is a well-known convex duality, which has already been used in some recent machine learning\npapers to study sample complexity bounds. For example, see [8, 11]. For\ncompleteness, we include a simple information-theoretical proof.\nProposition 2.1. Assume that f (\u03b8) is a measurable real-valued function on \u0393, and w(\u03b8) is a density with respect to \u03c0; we have\nE\u03c0 w(\u03b8)f (\u03b8) \u2264 DKL (w d\u03c0||d\u03c0) + ln E\u03c0 exp(f (\u03b8)).\nProof. We assume that E\u03c0 exp(f (\u03b8)) < \u221e; otherwise the bound is trivial. Consider v(\u03b8) = exp(f (\u03b8))/E\u03c0 exp(f (\u03b8)). Since E\u03c0 v(\u03b8) = 1, we can regard it as a density with respect to \u03c0. Using this definition, it is easy to\nverify that the inequality in Proposition 2.1 can be rewritten equivalently\nas\nE\u03c0 w(\u03b8) ln w(\u03b8) + ln E\u03c0 exp(f (\u03b8)) \u2212 E\u03c0 w(\u03b8)f (\u03b8) = DKL (w d\u03c0||v d\u03c0) \u2265 0,\nwhich is a well-known information-theoretical inequality, and follows easily\nfrom Jensen's inequality. \u0003\nThe main technical result which forms the basis of the paper is given by\nthe following lemma, where we assume that \u0175X (\u03b8) is a posterior (represented\nas a density with respect to \u03c0 that depends on X and is measurable on\nX n \u00d7 \u0393).\n\n\fMINIMUM COMPLEXITY ESTIMATION\n\n5\n\nLemma 2.1. Consider any posterior \u0175X (\u03b8). Let \u03b1 and \u03b2 be two real\nnumbers. The following inequality holds for all measurable real-valued functions LX (\u03b8) on X n \u00d7 \u0393:\nEX exp[E\u03c0 \u0175X (\u03b8)(LX (\u03b8) \u2212 \u03b1 ln EX e\u03b2LX (\u03b8) ) \u2212 DKL (\u0175X d\u03c0||d\u03c0)]\n\u2264 E\u03c0\n\nEX eLX (\u03b8)\n,\nE\u03b1X e\u03b2LX (\u03b8)\n\nwhere EX is the expectation with respect to the observation X.\nProof. From Proposition 2.1, we obtain\nL\u0302(X) = E\u03c0 \u0175X (\u03b8)(LX (\u03b8) \u2212 \u03b1 ln EX e\u03b2LX (\u03b8) ) \u2212 DKL (\u0175X d\u03c0||d\u03c0)\n\u2264 ln E\u03c0 exp(LX (\u03b8) \u2212 \u03b1 ln EX e\u03b2LX (\u03b8) ).\n\nNow applying Fubini's theorem to interchange the order of integration, we\nhave\nEX eL\u0302(X) \u2264 EX E\u03c0 eLX (\u03b8)\u2212\u03b1 ln EX exp(\u03b2LX (\u03b8)) = E\u03c0\n\nEX eLX (\u03b8)\n.\nE\u03b1X e\u03b2LX (\u03b8)\n\n\u0003\n\nRemark 2.1. The importance of the above inequality is that the lefthand side is a quantity that involves an arbitrary posterior randomization\nmeasure \u0175X d\u03c0. The right-hand side is a numerical constant independent of\nthe estimator \u0175X . Therefore the inequality gives a bound that can be applied\nto an arbitrary randomized estimator. The remaining issue is merely how to\ninterpret the resulting bound, which we shall focus on later in this paper.\nRemark 2.2. The main technical ingredients of the proof are motivated from techniques in the recent machine learning literature. The general\nidea for analyzing randomized estimators using Fubini's theorem and decoupling was already in [17]. The specific decoupling mechanism using Proposition 2.1 appeared in [3]; see [8, 11] for related problems. A simplified form\nof Lemma 2.1 was used in [18] to analyze Bayesian posterior distributions.\nThe following bound is a straightforward consequence of Lemma 2.1. Note\nthat for density estimation, the loss l\u03b8 (x) has the form of l(p(x|\u03b8)), where\nl(*) is a scaled log-loss.\nTheorem 2.1. We use the notation of Lemma 2.1. Let X = {X1 , . . . , Xn }\nbe n-samples that are independently drawn from q. Consider a measurable\nfunction l\u03b8 (x) : \u0393 \u00d7 X \u2192 R, and real numbers \u03b1 and \u03b2, and define\nEq e\u2212l\u03b8 (x)\n1\ncn (\u03b1, \u03b2) = ln E\u03c0\nn\nE\u03b1q e\u2212\u03b2l\u03b8 (x)\n\u0012\n\n\u0013n\n\n.\n\n\f6\n\nT. ZHANG\n\nThen \u2200 t, the following event holds with probability at least 1 \u2212 exp(\u2212t):\n\u2212\u03b1E\u03c0 \u0175X (\u03b8) ln Eq e\u2212\u03b2l\u03b8 (x)\n\u2264\n\nE\u03c0 \u0175X (\u03b8)\n\nPn\n\ni=1 l\u03b8 (Xi ) + DKL (\u0175X\n\nd\u03c0||d\u03c0) + t\n\nn\nMoreover, we have the expected risk bound\n\u2212\u03b1EX E\u03c0 \u0175X (\u03b8) ln Eq e\u2212\u03b2l\u03b8 (x)\n\u2264 EX\n\nE\u03c0 \u0175X (\u03b8)\n\nPn\n\ni=1 l\u03b8 (Xi ) + DKL (\u0175X\n\nd\u03c0||d\u03c0)\n\nn\n\n+ cn (\u03b1, \u03b2).\n\n+ cn (\u03b1, \u03b2).\n\nProof. We use the notation of Lemma 2.1, with LX (\u03b8) = \u2212\nIf we define\n\nPn\n\ni=1 l\u03b8 (Xi ).\n\nL\u0302(X) = E\u03c0 \u0175X (\u03b8)(LX (\u03b8) \u2212 \u03b1 ln EX e\u03b2LX (\u03b8) ) \u2212 DKL (\u0175X d\u03c0||d\u03c0)\n= E\u03c0 \u0175X (\u03b8) \u2212\n\nn\nX\ni=1\n\n!\n\nl\u03b8 (Xi ) \u2212 n\u03b1 ln Eq e\u2212\u03b2l\u03b8 (x) \u2212 DKL (\u0175X d\u03c0||d\u03c0),\n\nthen by Lemma 2.1 we have EX eL\u0302(X) \u2264 encn (\u03b1,\u03b2) . This implies \u2200 \u03b5: e\u03b5 P (L\u0302(X) >\n\u03b5) \u2264 encn (\u03b1,\u03b2) . Now given any t, and letting \u03b5 = t + ncn (\u03b1, \u03b2), we obtain\net+ncn (\u03b1,\u03b2) P (L\u0302(X) > t + ncn (\u03b1, \u03b2)) \u2264 encn (\u03b1,\u03b2) .\n\nThat is, with probability at least 1 \u2212 e\u2212t , L\u0302(X) \u2264 \u03b5 = ncn (\u03b1, \u03b2) + t. By\nrearranging the equation, we establish the first inequality of the theorem.\nTo prove the second inequality, we still start with EX eL\u0302(X) \u2264 encn (\u03b1,\u03b2)\nfrom Lemma 2.1. From Jensen's inequality with the convex function ex ,\nwe obtain eEX L\u0302(X) \u2264 EX eL\u0302(X) \u2264 encn (\u03b1,\u03b2) . That is, EX L\u0302(X) \u2264 nc(\u03b1, \u03b2). By\nrearranging the equation, we obtain the desired bound. \u0003\nRemark 2.3. The special case of Theorem 2.1 with \u03b1 = \u03b2 = 1 is very\nuseful since in this case the term cn (\u03b1, \u03b2) vanishes. In fact, in order to obtain\nthe correct rate of convergence for nonparametric problems, it is sufficient to\nchoose \u03b1 = \u03b2 = 1. The more complicated case with general \u03b1 and \u03b2 is only\nneeded for parametric problems, where we would like to obtain a convergence\nrate of the order O(1/n). In such cases the choice of \u03b1 = \u03b2 = 1 would lead\nto a rate of O(ln n/n), which is suboptimal.\n3. Information complexity minimization. Let S be a predefined set of\ndensities on \u0393 with respect to the prior \u03c0. We consider a general information\ncomplexity minimization estimator,\n(1)\n\nS\n\u0175X\n\n\"\n\n= arg min \u2212E\u03c0 w(\u03b8)\nw\u2208S\n\nn\nX\ni=1\n\n#\n\nln p(Xi |\u03b8) + \u03bbDKL (w d\u03c0||d\u03c0) .\n\n\fMINIMUM COMPLEXITY ESTIMATION\n\n7\n\nGiven the true density q, if we define\n(2)\n\nn\nX\nq(Xi )\n\u03bb\n1\nln\n+ DKL (w d\u03c0||d\u03c0),\nR\u0302\u03bb (w) = E\u03c0 w(\u03b8)\nn\np(Xi |\u03b8) n\ni=1\n\nthen it is clear that\nS\n\u0175X\n= arg min R\u0302\u03bb (w).\nw\u2208S\n\nThe above estimation procedure finds a randomized estimator by minimizing the regularized empirical risk R\u0302\u03bb (w) among all possible densities\nwith respect to the prior \u03c0 in a predefined set S. The purpose of this section is to study the performance of this estimator using Theorem 2.1. For\nsimplicity, we shall only study the expected performance using the second\ninequality, although similar results can be obtained using the first inequality\n(which leads to exponential probability bounds).\nOne may define the true risk of w by replacing the empirical expectation\nin (2) with the true expectation with respect to q:\n\u03bb\nDKL (w d\u03c0||d\u03c0),\nn\nwhere DKL (q||p) = Eq ln(q(x)/p(x)) is the KL-divergence between q and p.\nThe information complexity minimizer in (1) can be regarded as an approximate solution to (3) using empirical expectation.\nUsing empirical process techniques, one can typically expect to bound\nR\u03bb (w) in terms of R\u0302\u03bb (w). Unfortunately, it does not work in our case\nsince DKL (q||p) is not well defined for all p. This implies that as long as\nw has nonzero concentration around a density p with DKL (q||p) = +\u221e,\nS ) = +\u221e with nonzero\nthen R\u03bb (w) = +\u221e. Therefore we may have R\u03bb (\u0175X\nprobability even when the sample size approaches infinity.\nA remedy is to use a distance function that is always well defined. In\nstatistics, one often considers the \u03c1-divergence for \u03c1 \u2208 (0, 1), which is defined\nas\n\u0014\n\u0012\n\u0013 \u0015\np(x) \u03c1\n1\nEq 1 \u2212\n(4)\n.\nD\u03c1 (q||p) =\n\u03c1(1 \u2212 \u03c1)\nq(x)\n(3)\n\nR\u03bb (w) = E\u03c0 w(\u03b8)DKL (q||p(*|\u03b8)) +\n\nThis divergence is always well defined and DKL (q||p) = lim\u03c1\u21920 D\u03c1 (q||p). In\nthe statistical literature, convergence results were often specified under the\nsquared Hellinger distance (\u03c1 = 0.5). In this paper we specify convergence\nresults with general \u03c1. We shall mention that bounds derived in this paper\nwill become trivial when \u03c1 \u2192 0. This is consistent with the above discussion\nsince R\u03bb (corresponding to \u03c1 = 0) may not converge at all. However, under\nadditional assumptions, such as the boundedness of q/p, DKL (q||p) exists\nand can be bounded using the \u03c1-divergence D\u03c1 (q||p).\n\n\f8\n\nT. ZHANG\n\nA concept related to the \u03c1-divergence in (4) is the R\u00e9nyi entropy introduced in [9]. The notion has been widely used in information theory. Up to\na scaling factor, it can be defined as\nD\u03c1Re (q||p) = \u2212\n\np(x)\n1\nln Eq\n\u03c1(1 \u2212 \u03c1)\nq(x)\n\u0012\n\n\u0013\u03c1\n\n.\n\nNote that the standard definition of R\u00e9nyi entropy in the literature is \u03c1D\u03c1Re (q||p).\nWe employ a scaled version in this paper for compatibility with our \u03c1divergence definition. Using the inequality 1 \u2212 x \u2264 \u2212 ln x \u2264 x\u22121 \u2212 1 (x \u2208\n[0, 1]), we can see that \u2200 p, q\nD\u03c1 (q||p) \u2264 D\u03c1Re (q||p) \u2264\n\nD\u03c1 (q||p)\n.\n1 \u2212 \u03c1(1 \u2212 \u03c1)D\u03c1 (q||p)\n\nThe following bounds imply that up to a constant, the \u03c1-divergence with\nany \u03c1 \u2208 (0, 1) is equivalent to the squared Hellinger distance. Therefore a\nconvergence bound in any \u03c1-divergence implies a convergence bound of the\nsame rate in the Hellinger distance.\nProposition 3.1.\n\nWe have the following inequalities \u2200 \u03c1 \u2208 [0, 1]:\n\nmax(\u03c1, 1 \u2212 \u03c1)D\u03c1 (q||p) \u2265 21 D1/2 (q||p) \u2265 min(\u03c1, 1 \u2212 \u03c1)D\u03c1 (q||p).\nProof. We prove the first half of the two inequalities. Due to the symmetry D\u03c1 (q||p) = D1\u2212\u03c1 (p||q), we only need to consider the case \u03c1 \u2264 1/2. The\nproof of the second half (with \u03c1 \u2265 1/2) is identical except that the sign in\nthe Taylor expansion step is reversed.\n1/2 \u2212q 1/2\nWe use Taylor expansion. Let x = p q1/2\n; then x \u2265 \u22121, and there exists\n\u03be > \u22121 such that\n(1 + x)2\u03c1 = 1 + 2\u03c1x + \u03c1(2\u03c1 \u2212 1)(1 + \u03be)2\u03c1\u22122 x2 \u2264 1 + 2\u03c1x.\nNow taking expectation with respect to q, we obtain\nEq\n\n\u0012 \u0013\u03c1\n\np\nq\n\n\u0012\n\n= Eq 1 +\n\np1/2 \u2212 q 1/2\nq 1/2\n\n\u00132\u03c1\n\n\u2264 1 + 2\u03c1Eq\n\np1/2 \u2212 q 1/2\n.\nq 1/2\n\nBy rearranging the equation, we obtain 2\u03c1( 14 D1/2 (q||p)) \u2264 \u03c1(1 \u2212 \u03c1)D\u03c1 (q||p).\n\u0003\n3.1. A general convergence bound. The following theorem is a consequence of Theorem 2.1. Most of our later discussion can be considered as\ninterpretation of this theorem under different conditions.\n\n\f9\n\nMINIMUM COMPLEXITY ESTIMATION\n\nS defined in (1). Let \u03b1 > 0.\nTheorem 3.1. Consider the estimator \u0175X\n\u03bb\u03b3\u22121\n\u2032\nThen \u2200 \u03c1 \u2208 (0, 1) and \u03b3 \u2265 \u03c1 such that \u03bb = \u03b3\u2212\u03c1 \u2265 0, we have\nS\nS\nEX E\u03c0 \u0175X\n(\u03b8)D\u03c1 (q||p(*|\u03b8)) \u2264 EX E\u03c0 \u0175X\n(\u03b8)D\u03c1Re (q||p(*|\u03b8))\n\n\u2264\n\n\u03b3 inf w\u2208S R\u03bb (w)\n\u03b3 \u2212\u03c1\nS\n\u2212\nEX R\u0302\u03bb\u2032 (\u0175X\n)\n\u03b1\u03c1(1 \u2212 \u03c1)\n\u03b1\u03c1(1 \u2212 \u03c1)\n+\n\nwhere c\u03c1,n (\u03b1) =\n\n1\nn\n\nc\u03c1,n (\u03b1)\n,\n\u03b1\u03c1(1 \u2212 \u03c1)\n\n(1\u2212\u03b1)n p(x|\u03b8) \u03c1\n( q(x) )\n\nln E\u03c0 Eq\n\n=\n\n1\nn\n\nRe (q||p(*|\u03b8))\n\nln E\u03c0 e\u2212\u03c1(1\u2212\u03c1)(1\u2212\u03b1)nD\u03c1\n\n.\n\nProof. Consider an arbitrary data-independent density w(\u03b8) \u2208 S with\nrespect to \u03c0. Using (4), we can obtain from Theorem 2.1 the chain of equations\nS\n\u03b1\u03c1(1 \u2212 \u03c1)EX E\u03c0 \u0175X\n(\u03b8)D\u03c1 (q||p(*|\u03b8))\n\nS\n\u2264 \u03b1\u03c1(1 \u2212 \u03c1)EX E\u03c0 \u0175X\n(\u03b8)D\u03c1Re (q||p(*|\u03b8))\n\n\u0012\n\nS\n= \u2212\u03b1EX E\u03c0 \u0175X\n(\u03b8) ln Eq exp \u2212\u03c1 ln\n\n\"\n\nS\n\u2264 EX \u03c1E\u03c0 \u0175X\n\nq(x)\np(x|\u03b8)\n\n\u0013\n\nn\nX\n1\n\n#\n\nS d\u03c0||d\u03c0)\nq(Xi )\nDKL (\u0175X\nln\n+\n+ c\u03c1,n (\u03b1)\nn p(Xi |\u03b8)\nn\ni=1\n\nS\nS\n)] + c\u03c1,n (\u03b1)\n= EX [\u03b3 R\u0302\u03bb (\u0175X\n) + (\u03c1 \u2212 \u03b3)R\u0302\u03bb\u2032 (\u0175X\nS\n)] + c\u03c1,n (\u03b1)\n\u2264 EX [\u03b3 R\u0302\u03bb (w) + (\u03c1 \u2212 \u03b3)R\u0302\u03bb\u2032 (\u0175X\nS\n= \u03b3R\u03bb (w) \u2212 (\u03b3 \u2212 \u03c1)EX R\u0302\u03bb\u2032 (\u0175X\n) + c\u03c1,n (\u03b1),\n\nwhere R\u03bb (w) is defined in (3). Note that the first inequality uses the fact\n\u2212 ln(1 \u2212 x) \u2265 x. The second inequality follows from Theorem 2.1 with the\nq(x)\nchoice l\u03b8 (x) = \u03c1 ln p(x|\u03b8)\nand \u03b2 = 1. The third inequality follows from the\nS\ndefinition of \u0175X in (1). \u0003\nRemark 3.1.\nlet \u03bb\u2032 = 0.\n\nIf \u03b3 = \u03c1 in Theorem 3.1, then we also require \u03bb\u03b3 = 1, and\n\nAlthough the bound in Theorem 3.1 looks complicated, the most important part on the right-hand side is the first term. The second term is only\nneeded to handle the situation \u03bb \u2264 1. The requirement that \u03b3 \u2265 \u03c1 is to ensure\nthat the second term is nonpositive. Therefore in order to apply the theoS ), which (as we shall\nrem, we only need to estimate a lower bound of R\u0302\u03bb\u2032 (\u0175X\n\n\f10\n\nT. ZHANG\n\nsee later) is much easier than obtaining an upper bound. The third term is\nmainly included to get the correct convergence rate of O(1/n) for parametric\nproblems, and can be ignored for nonparametric problems. The effect of this\nterm is quite similar to using localized \u03b5-entropy in the empirical process\napproach for analyzing the maximum-likelihood method; for example, see\n[13]. As a comparison, the KL-entropy in the first term corresponds to the\nglobal \u03b5-entropy.\nNote that one can easily obtain a simplified bound from Theorem 3.1 by\nchoosing specific parameters so that both the second term and the third\nterm vanish:\nS defined in (1). Assume\nCorollary 3.1. Consider the estimator \u0175X\nthat \u03bb > 1 and let \u03c1 = 1/\u03bb. We have\nS\nEX E\u03c0 \u0175X\n(\u03b8)D\u03c1Re (q||p(*|\u03b8)) \u2264\n\n1\ninf R\u03bb (w).\n1 \u2212 \u03c1 w\u2208S\n\nProof. We simply let \u03b1 = 1 and \u03b3 = \u03c1 in Theorem 3.1. \u0003\nAn important observation is that for \u03bb > 1, the convergence rate is solely\ndetermined by the quantity inf w\u2208S R\u03bb (w), which we shall refer to as the\nmodel resolvability associated with S.\n3.2. Some consequences of Theorem 3.1. In order to apply Theorem 3.1,\nS ) from below. Some of these results\nwe need to bound the quantity EX R\u0302\u03bb\u2032 (\u0175X\ncan be found in the Appendix, and by using these results, we are able to\nobtain some refined bounds from Theorem 3.1.\nS defined in (1). Assume\nCorollary 3.2. Consider the estimator \u0175X\nthat \u03bb > 1; then \u2200 \u03c1 \u2208 (0, 1/\u03bb]\nS\nEX E\u03c0 \u0175X\n(\u03b8)D\u03c1Re (q||p(*|\u03b8)) \u2264\n\n1\ninf R\u03bb (w).\n\u03c1(\u03bb \u2212 1) w\u2208S\n\nProof. We simply let \u03b1 = 1 and \u03b3 = (1 \u2212 \u03c1)/(\u03bb \u2212 1) in Theorem 3.1.\nNote that in this case, \u03bb\u2032 = 1, and hence by Lemma A.1 in the Appendix,\nS ) \u2265 0. \u0003\nwe have EX R\u0302\u03bb\u2032 (\u0175X\nNote that Lemma A.1 is only applicable for \u03bb\u2032 \u2265 1. If \u03bb\u2032 \u2264 1, then we\nneed a discretization device which generalizes the upper \u03b5-covering number\nconcept used in [2] for showing the consistency (or inconsistency) of Bayesian\nposterior distributions:\n\n\f11\n\nMINIMUM COMPLEXITY ESTIMATION\n\nDefinition 3.1. The \u03b5-upper bracketing number of \u0393, denoted by\nNub (\u0393, \u03b5), is the minimum number of nonnegative functions {fj } on X\nwith respect to \u03bc such that Eq (fj /q) \u2264 1 + \u03b5, and \u2200 \u03b8 \u2208 \u0393, \u2203 j such that\np(x|\u03b8) \u2264 fj (x) a.e. [\u03bc].\nThe discretization device which we shall use in this paper is based on the\nfollowing definition.\nDefinition 3.2.\ndius as\n\nGiven a set \u0393\u2032 \u2282 \u0393, we define its upper-bracketing rarub (\u0393\u2032 ) =\n\nZ\n\nsup p(x|\u03b8) d\u03bc(x) \u2212 1.\n\n\u03b8\u2208\u0393\u2032\n\nAn \u03b5-upper discretization of \u0393 consists\nS of a covering of \u0393 by countably many\nmeasurable subsets {\u0393j } such that j \u0393j = \u0393 and rub (\u0393j ) \u2264 \u03b5.\n\nUsing this concept, we may combine the estimate in Lemma A.2 in the\nAppendix with Theorem 3.1, and obtain the following simplified bound for\n\u03bb = 1. Similar results can also be obtained for \u03bb < 1.\nCorollary 3.3. Consider the estimator defined in (1). Let \u03bb = 1. Consider an arbitrary covering {\u0393j } of \u0393. \u2200 \u03c1 \u2208 (0, 1) and \u2200 \u03b3 \u2265 1, we have\nS\nEX E\u03c0 \u0175X\n(\u03b8)D\u03c1Re (q||p(*|\u03b8))\n\n\u2264\n\nX\n\u03b3\u2212\u03c1\n\u03b3 inf w\u2208S R\u03bb (w)\n\u03c0(\u0393j )(\u03b3\u22121)/(\u03b3\u2212\u03c1) (1 + rub (\u0393j ))n .\n+\nln\n\u03c1(1 \u2212 \u03c1)\n\u03c1(1 \u2212 \u03c1)n\nj\n\nIn particular, if {\u0393\u03b5j } is an \u03b5-upper discretization of \u0393, then\nS\nEX E\u03c0 \u0175X\n(\u03b8)D\u03c1Re (q||p(*|\u03b8))\n\n\u03b3 \u2212 \u03c1 ln\n\u03b3 inf w\u2208S R\u03bb (w)\n+\n\u2264\n\u03c1(1 \u2212 \u03c1)\n\u03c1(1 \u2212 \u03c1)\n\u0014\n\nP\n\nj\n\n\u03c0(\u0393\u03b5j )(\u03b3\u22121)/(\u03b3\u2212\u03c1)\n+ ln(1 + \u03b5) .\nn\n\u0015\n\nProof. We let \u03b1 = 1 in Theorem 3.1 and apply Lemma A.2. \u0003\nNote that the above results immediately imply the following bound using\n\u03b5-upper entropy by letting \u03b3 \u2192 1 with a finite \u03b5-upper bracketing cover of\nsize Nub (\u0393, \u03b5) as the discretization:\nS\nEX E\u03c0 \u0175X\n(\u03b8)D\u03c1Re (q||p(*|\u03b8)) \u2264\n\ninf w\u2208S R\u03bb (w) 1\nln Nub (\u0393, \u03b5)\n+ inf\n+ ln(1 + \u03b5) .\n\u03c1(1 \u2212 \u03c1)\n\u03c1 \u03b5>0\nn\n\u0014\n\n\u0015\n\nIt is clear that Corollary 3.3 is significantly more general. We are able to\ndeal with an infinite cover as long as P\nthe decay of the prior \u03c0 is fast enough\non an \u03b5-upper discretization so that j \u03c0(\u0393\u03b5j )(\u03b3\u22121)/(\u03b3\u2212\u03c1) < +\u221e.\n\n\f12\n\nT. ZHANG\n\n3.3. Weak convergence bound. The case of \u03bb = 1 is related to a number\nof important estimation methods in statistical applications. However, for an\narbitrary prior \u03c0 without any additional assumption such as the fast decay\ncondition in Corollary 3.3, it is impossible to establish any convergence rate\nresult in terms of Hellinger distance using the model resolvability quantity\nalone, as in the case of \u03bb > 1 (Corollary 3.2). See Section 4.4 for an example\ndemonstrating this claim. However, one can still obtain a weaker convergence\nresult in this case.\nS defined in (1) with \u03bb = 1.\nTheorem 3.2. Consider the estimator \u0175X\nThen \u2200 f : X \u2192 [\u22121, 1], we have\nS\nEX E\u03c0 \u0175X\n(\u03b8)Ep(*|\u03b8)f (x) \u2212\n\nn\np\n1X\nf (Xi ) \u2264 2An + 2An ,\nn i=1\n\nR\n\nwhere Ep(*|\u03b8)f (x) = f (x)p(x|\u03b8) d\u03bc(x) is the expectation with respect to p(*|\u03b8)\non X , and An = inf w\u2208S EX R\u03bb (w) + lnn2 .\nProof. The first half of the proof, leading to (5), is an application of\nTheorem 2.1. The second half is very similar to the proof of Theorem 3.1.\nq(x)\nLet g\u03b5 (x) = 1 \u2212 \u03b5f (x), and h\u03b5 (\u03b8, x) = p(x|\u03b8)g\n, where \u03b5 \u2208 (\u22121, 1) is a\n\u03b5 (x)\nparameter to be determined later. Note that g\u03b5 (x) > 0.\nWe consider an extension of \u0393 to \u0393\u2032 = \u0393 \u00d7 {\u00b11}. Let \u03c3 = \u00b11, and \u03b8 \u2032 =\n(\u03b8, \u03c3) \u2208 \u0393\u2032 . We define a prior \u03c0 \u2032 on \u0393\u2032 such that \u03c0 \u2032 ((\u03b8, \u03c3)) = 0.5\u03c0(\u03b8). For a\nS (\u03b8) on \u0393, we consider for u = \u00b11 a posterior \u0175S (\u03b8, \u03c3) on \u0393\u2032\nposterior \u0175X\nu,X\nS (\u03b8, \u03c3) = 2\u0175S (\u03b8) when \u03c3 = u, and \u0175S (\u03b8, \u03c3) = 0 otherwise.\nsuch that \u0175u,X\nX\nu,X\nLet \u03b1 = \u03b2 = 1 and l\u03b8,\u03c3 (x) = ln h\u03c3\u03b5 (\u03b8, Xi ). For all u(X) \u2208 {\u00b11}, we apply\nS\nTheorem 2.1 to the posterior \u0175u(X),X\n, and obtain\n\u2212EX E\u03c0 \u0175X (\u03b8) ln Eq e\u2212 ln hu\u03b5 (\u03b8,x)\n\u2264 EX\n\nE\u03c0 \u0175X (\u03b8)\n\nPn\n\ni=1 ln hu\u03b5 (\u03b8, Xi ) + DKL (\u0175X\n\nd\u03c0||d\u03c0) + ln 2\n\nn\n\n.\n\nNote that Eq e\u2212 ln hu\u03b5 (\u03b8,x) = Ep(*|\u03b8) g\u03b5 (x). Therefore if we let\nS\n\u2206\u03b5 (X) = E\u03c0 \u0175X\n(\u03b8)\n\nn\nX\ni=1\n\n!\n\nln g\u03b5 (Xi ) \u2212 n ln Ep(*|\u03b8)g\u03b5 (x) ,\n\nthen\n(5)\n\nS\nEX \u2206u(X)\u03b5 (X) \u2264 nEX R\u0302\u03bb (\u0175X\n) + ln 2 \u2264 n inf R\u03bb (w) + ln 2,\nw\u2208S\n\nS in (1). This\nwhere the second inequality follows from the definition of \u0175X\ninequality plays the same role as Theorem 2.1 in the proof of Theorem 3.1.\n\n\f13\n\nMINIMUM COMPLEXITY ESTIMATION\n\nConsider x \u2264 y < 1. We have the inequalities (which follow from Taylor\nexpansion)\nx \u2264 \u2212 ln(1 \u2212 x) \u2264 x +\n\nx2\n.\n2(1 \u2212 y)2\n\n2\n\n\u03b5\nThis implies ln g\u03b5 (x) \u2265 \u2212\u03b5f (x) \u2212 2(1\u2212|\u03b5|)\n2 and \u2212 ln Ep(*|\u03b8) g\u03b5 (x) \u2265 \u03b5Ep(*|\u03b8) f (x).\nTherefore\nS\n\u2206\u03b5 (X) \u2265 \u03b5E\u03c0 \u0175X\n(\u03b8)\n\n\u2212\n\nn\nX\ni=1\n\n!\n\nf (Xi ) + nEp(*|\u03b8)f (x) \u2212\n\nn\u03b52\n.\n2(1 \u2212 |\u03b5|)2\n\nSubstitute into (5); we have\nEX sup\nu\u2208{\u00b11}\n\nS\nu\u03b5E\u03c0 \u0175X\n(\u03b8)\n\n\u2212\n\nn\nX\n\n!!\n\nf (Xi ) + nEp(*|\u03b8)f (x)\n\ni=1\n\nn\u03b52\n2(1 \u2212 |\u03b5|)2\n\n\u2212\n\n\u2264 n inf EX R\u03bb (w) + ln 2.\nw\u2208S\n\nTherefore we have\nS\nEX E\u03c0 \u0175X\n(\u03b8)\n\nLet |\u03b5| =\n\n\u221a\n\n\u2212\n\nn\nX\n\n!\n\nf (Xi ) + nEp(*|\u03b8)f (x)\n\ni=1\n\n\u2264\n\nnAn\nn|\u03b5|\n.\n+\n2(1 \u2212 |\u03b5|)2\n|\u03b5|\n\n\u221a\n2An /( 2An + 1) and we obtain the desired bound. \u0003\n\nNote that for all f \u2208 [\u22121, 1] the empirical average\nto Eq f (x),\nEX\n\n1\nn\n\nPn\n\ni=1 f (Xi )\n\n!2\n\nn\nn\n1X\n1/2 1 X\nf (Xi ) \u2212 Eq f (x) \u2264 EX\nf (Xi ) \u2212 Eq f (x)\nn i=1\nn i=1\n\nconverges\n\n1\n\u2264\u221a .\nn\n\nIt follows from Theorem 3.2 that\nS\nEX |E\u03c0 \u0175X\n(\u03b8)Ep(*|\u03b8)f (x) \u2212 Eq f (x)| \u2264 2An +\n\np\n\n2An + n\u22121/2 .\n\nThis means that as long as limn An = 0, for all bounded functions f (x) \u2208\nS (\u03b8)E\n[\u22121, 1], the posterior average E\u03c0 \u0175X\np(*|\u03b8) f (x) converges to Eq f (x) in\nprobability. Since Theorem 3.2 uses the same weak topology as that in the\nusual definition of weak convergence of measures, we can interpret this reS (\u03b8)p(*|\u03b8) converges weakly to q in\nsult to mean the posterior average E\u03c0 \u0175X\nprobability. In particular, by letting f (x) be an indicator function for an\narbitrary set B \u2282 X , we obtain the consistency of the probability estimate.\nS (\u03b8)p(*|\u03b8) conThat is, the probability of B under the posterior mean E\u03c0 \u0175X\nverges to the probability of B under q (when limn An = 0).\n\n\f14\n\nT. ZHANG\n\n4. Two-part code MDL on discrete net. The minimum description length\n(MDL) method has been widely used in practice [10]. The two-part code\nMDL we consider here is the same as that of Barron and Cover [1]. In fact,\nresults in this section improve those of Barron and Cover [1]. The MDL\nmethod considered in [1] can be regarded as a special case of information\ncomplexity minimization. The model space \u0393 is countable: \u03b8 \u2208 \u0393 = {1, 2, . . .}.\nWe denote the corresponding models\np(x|\u03b8 = j) by pj (x). The prior \u03c0 has\nP\nthe form \u03c0 = {\u03c01 , \u03c02 , . . .} such that j \u03c0j = 1, where we assume that \u03c0j > 0\nfor each j. A randomized algorithm\ncan be represented as a nonnegative\nP\nweight vector w = [wj ] such that j \u03c0j wj = 1.\nMDL gives a deterministic estimator, which corresponds to the set of\nweights concentrated on any one specific point k. That is, we can select S\nin (1), where each weight w in S corresponds to an index k \u2208 \u0393 such that\nwk = 1/\u03c0k and wj = 0 when j 6= k. It is easy to check that DKL (w d\u03c0||d\u03c0) =\nln(1/\u03c0k ). The corresponding algorithm can thus be described as finding a\nprobability density pk\u0302 with k\u0302 obtained by\n(6)\n\n\" n\nX\n\n#\n\n1\n1\n+ \u03bb ln\n,\nk\u0302 = arg min\nln\npk (Xi )\n\u03c0k\nk\ni=1\n\nwhere \u03bb \u2265 1 is a regularization parameter. The first term corresponds to the\ndescription of the data, and the second term corresponds to the description\nof the model. The choice \u03bb = 1 can be interpreted as minimizing the total\ndescription length, which corresponds to the standard MDL. The choice \u03bb >\n1 corresponds to heavier penalty on the model description, which makes the\nestimation method more stable. This modified MDL method was considered\nin [1] and the authors obtained results on the asymptotic rate of convergence.\nHowever, no simple finite-sample bound was obtained. For the case of \u03bb = 1,\nonly weak consistency was shown. In the following, we shall improve these\nresults using the analysis presented in Section 3.\n4.1. Modified MDL under global entropy condition. Consider the case\n\u03bb > 1 in (6). We can obtain the following theorem from Corollary 3.2.\nTheorem 4.1. Consider the estimator k\u0302 defined in (6). Assume that\n\u03bb > 1. Then \u2200 \u03c1 \u2208 (0, 1/\u03bb]\nEX D\u03c1 (q||pk\u0302 ) \u2264 EX D\u03c1Re (q||pk\u0302 ) \u2264\n\n1\n\u03bb\n1\ninf DKL (q||pk ) + ln\n.\n\u03c1(\u03bb \u2212 1) k\nn \u03c0k\n\u0014\n\n\u0015\n\nThe term r\u03bb,n (q) = inf k [DKL (q||pk ) + \u03bbn ln \u03c01k ] is referred to as index of\nresolvability in [1]. They showed (Theorem 4) that D1/2 (q||pk\u0302 ) = Op (r\u03bb,n (q))\nwhen \u03bb > 1, which is a direct consequence of Theorem 4.1.\n\n\f15\n\nMINIMUM COMPLEXITY ESTIMATION\n\nTheorem 4.1 generalizes a result by Andrew Barron and Jonathan Li,\nwhich gave a similar inequality but only for the case of \u03bb = 2 and \u03c1 = 1/2.\nThe result can be found in [7], Theorem 5.5, page 78. In particular, consider\n\u0393 such that |\u0393| = N with uniform prior \u03c0j = 1/N ; one obtains a bound\nfor the maximum likelihood estimate over \u0393 (take \u03bb = 2 and \u03c1 = 1/2 in\nTheorem 4.1),\n(7)\n\n2\n1\nEX D1/2 (q||pk\u0302 ) \u2264 2 inf DKL (q||pk ) + ln\n.\nk\nn N\n\u0014\n\n\u0015\n\nExamples of indexes of resolvability for various function classes can be\nfound in [1], which we shall not repeat in this paper. In particular, it is\nknown that for nonparametric problems, with appropriate discretization the\nrate resulting from (7) matches the minimax rate, such as those in [16].\n4.2. Local entropy analysis. Although the bound based on the index of\nresolvability in Theorem 4.1 is quite useful for nonparametric problems, see\n[1], it does not handle the parametric case satisfactorily. To see this, we\nconsider a one-dimensional parameter family indexed by \u03b8 \u2208 [0, 1], and we\ndiscretize the family using a uniform discrete net of size N + 1, \u03b8j = j/N (j =\n0, . . . , N ). In the following, we assume that q is taken from the parametric\nfamily, and for some fixed \u03c1, both D\u03c1Re (q||pk ) and DKL (q||pk ) are of the order\n(\u03b8 \u2212 \u03b8k )2 . That is, we assume that there exist constants c1 and c2 where\n(8)\n\nc1 (\u03b8 \u2212 \u03b8k )2 \u2264 D\u03c1Re (q||pk ),\n\nDKL (q||pk ) \u2264 c2 (\u03b8 \u2212 \u03b8k )2 .\n\nWe will thus have inf k DKL (q||pk ) \u2264 c2 N \u22122 , and the bound in (7), which\nrelies on the index of resolvability, becomes EX D1/2 (q||pk\u0302 ) \u2264 O(N \u22122 ) +\n4\n1\n\u22121/2 ), we obtain a suboptimal convern ln N +1 . Now by choosing N = O(n\ngence rate EX D1/2 (q||pk\u0302 ) \u2264 O(ln n/n). Note that convergence rates established in [1] for parametric examples are also of the order O(ln n/n).\nThe main reason for this suboptimality is that the complexity measure\nO(ln N ) or O(\u2212 ln \u03c0k ) corresponds to the globally defined entropy. However,\nreaders who are familiar with the empirical process theory know that the\nrate of convergence of the maximum-likelihood estimate is determined by\nlocal entropy mentioned in [5]. For nonparametric problems, it was pointed\nout in [16] that the worst-case local entropy is of the same order as the global\nentropy. Therefore a theoretical analysis which relies on global entropy (such\nas Theorem 4.1) leads to the correct worst-case rate at least in the minimax\nsense. For parametric problems, at the O(1/n) approximation level, local\nentropy is constant but the global entropy is ln n. This leads to a ln(n)\ndifference in the resulting bound.\nAlthough it may not be immediately obvious how to define a localized\ncounterpart of the index of resolvability, we can introduce a correction term\n\n\f16\n\nT. ZHANG\n\nwhich has the same effect. As pointed out earlier, this is essentially the role\nof the c\u03c1,n (\u03b1) term in Theorem 3.1. We include a simplified version below,\nwhich can be obtained by choosing \u03b1 = 1/2 and \u03b3 = \u03c1 = 1/\u03bb.\nTheorem 4.2. Consider the estimator k\u0302 defined in (6). Assume that\n\u03bb > 1, and let \u03c1 = 1/\u03bb. Then\n2\n\u03bb\ninf DKL (q||pk ) + ln\n1\u2212\u03c1 k\nn\n\u0014\n\nEX D\u03c1Re (q||pk\u0302 ) \u2264\n\nP\n\nj\n\nRe (q||p\n\n\u03c0j e\u22120.5\u03c1(1\u2212\u03c1)nD\u03c1\n\u03c0k\n\nj)\n\n\u0015\n\n.\n\nThe bound relies on a localized version of the index of resolvability,\nP\nwith the global entropy \u2212 ln \u03c0k replaced by a localized entropy ln j \u03c0j \u00d7\nRe (q||p\n\ne\u22120.5\u03c1(1\u2212\u03c1)nD\u03c1\n\nj)\n\nln\n\n\u2212 ln \u03c0k . Since\n\nX\n\nRe (q||p\n\n\u03c0j e\u22120.5\u03c1(1\u2212\u03c1)nD\u03c1\n\nj)\n\nj\n\n\u2264 ln\n\nX\n\n\u03c0j = 0,\n\nj\n\nthe localized entropy is always smaller than the global entropy. Intuitively,\nwe can see that if pj (x) is far away from q(x), then exp(\u2212\u03c1(1 \u2212 \u03c1)(1 \u2212\n\u03b1)nD\u03c1Re (q||pj )) is exponentially small as n \u2192 \u221e. It follows that the main\nRe\n\ncontribution to the summation in j \u03c0j e\u22120.5\u03c1(1\u2212\u03c1)nD\u03c1 (q||pj ) is from terms\nsuch that D\u03c1Re (q||pj ) is small. This is equivalent to a reweighting of the prior\n\u03c0k in such a way that we only count points that are localized within a small\nD\u03c1Re ball of q.\nThis localization leads to the correct rate of convergence for parametric\nproblems. The effect is similar to using localized entropy in the empirical process analysis. We still consider the same one-dimensional problem discussed\nat the beginning of the section, with a uniform discretization consisting of\nN + 1 points. We will consider the maximum-likelihood estimate. For onedimensional parametric problems, using the assumption in (8), we have for\nall N 2 = O(n),\nP\n\nX\n\nRe (q||p\n\ne\u2212\u03c1(1\u2212\u03c1)(1\u2212\u03b1)nD\u03c1\n\nj\n\nj)\n\n\u2264\n\nX\n\ne\u2212\u03c1(1\u2212\u03c1)(1\u2212\u03b1)nc1 j\n\n2 /N 2\n\n= O(1).\n\nj\n\nSince \u03c0j = 1/(N + 1), the localized entropy\nln\n\nP\n\nj\n\nRe (q||p\n\n\u03c0j e\u2212\u03c1(1\u2212\u03c1)(1\u2212\u03b1)nD\u03c1\n\u03c0k\n\nj)\n\n= O(1)\n\nis a constant when N = O(n1/2 ). Therefore with a discretization size N =\nO(n1/2 ), Theorem 4.2 implies a convergence rate of the correct order O(1/n).\n\n\fMINIMUM COMPLEXITY ESTIMATION\n\n17\n\n4.3. The standard MDL (\u03bb = 1). The standard MDL with \u03bb = 1 in (6)\nis more complicated to analyze. It is impossible to give a bound similar to\nTheorem 4.1 that depends only on the index of resolvability. As a matter of\nfact, no bound was established in [1]. As we will show later, the method can\nconverge very slowly even if the index of resolvability is well behaved.\nHowever, it is possible to obtain bounds in this case under additional\nassumptions on the rate of decay of the prior \u03c0. The following theorem is a\nstraightforward interpretation of Corollary 3.3, where we consider the family\nitself as a 0-upper discretization, \u0393i = {pi }.\nTheorem 4.3. Consider the estimator defined in (6) with \u03bb = 1. For\nall \u03c1 \u2208 (0, 1) and \u2200 \u03b3 \u2265 1, we have\nEX D\u03c1Re (q||pk\u0302 ) \u2264\n\n\u03b3 inf k [DKL (q||pk ) + (1/n) ln(1/\u03c0k )]\n\u03c1(1 \u2212 \u03c1)\nX (\u03b3\u22121)/(\u03b3\u2212\u03c1)\n\u03b3 \u2212\u03c1\n\u03c0j\n.\nln\n+\n\u03c1(1 \u2212 \u03c1)n\nj\n\nThe above theorem depends only on the index of resolvability and the\nP (\u03b3\u22121)/(\u03b3\u2212\u03c1)\n<\ndecay of the prior \u03c0. If \u03c0 has a fast decay in the sense of j \u03c0j\n+\u221e and does not change with respect to n, then the second term on the\nright-hand side of Theorem 4.3 is O(1/n). In this case the convergence rate is\ndetermined by the index of resolvability. The prior decay condition specified\nhere is rather mild. This implies that the standard MDL is usually Hellinger\nconsistent when used with care.\n4.4. Slow convergence of the standard MDL. The purpose of this section\nis to illustrate that the index of resolvability cannot by itself determine the\nrate of convergence for the standard MDL. We consider a simple example\nrelated to the Bayesian inconsistency counterexample given in [2], with an\nadditional randomization argument. Note that due to the randomization, we\nshall allow two densities in our model class to be identical. It is clear from\nthe construction that this requirement is for convenience only, rather than\nanything essential.\nGiven a sample size n, consider an integer m such that m \u226b n. Let the\nspace X consist of 2m points {1, . . . , 2m}. Assume that the truth q is the\nuniform distribution, q(u) = 1/(2m) for u = 1, . . . , 2m.\nConsider a density class \u0393\u2032 consisting of all densities p such that either\np(u) = 0 or p(u) = 1/m. That is, a density p in \u0393\u2032 takes the value 1/m at m\nof the 2m points, and 0 elsewhere. Now let our model class \u0393 consist of the\ntrue density q with prior 1/4, as well as 2n densities pj (j = 1, . . . , 2n ) that\nare randomly and uniformly drawn from \u0393\u2032 (with replacement), where each\npj is given the same prior 3/2n+2 .\n\n\f18\n\nT. ZHANG\n\nWe shall show that for a sufficiently large integer m, with large probability\nwe will estimate one of the 2n densities from \u0393\u2032 with probability of at least\n1 \u2212 e\u22121/2 . Since the index of resolvability is ln 4/n, which is small when n\nis large, the example implies that the convergence of the standard MDL\nmethod cannot be characterized by the index of resolvability alone.\nLet X = {X1 , . . . , Xn } be a set of n-samples from q and let p\u0302 be the\nWe would\nestimator from (6) with \u03bb = 1 and \u0393 randomly generated above.\nQ\nlike to estimate P (p\u0302 = q). By construction, p\u0302 = q only when ni=1 pj (Xi ) = 0\nfor all pj \u2208 \u0393\u2032 \u2229 \u0393. Now pick m large enough such that (m \u2212 n)n /mn \u2265 0.5;\nwe have\n\u2032\n\nP (p\u0302 = q) = P \u2200 pj \u2208 \u0393 \u2229 \u0393 :\n\nn\nY\n\n= EX P\n\npj (Xi ) = 0\n\ni=1\n\n= E X P \u2200 p j \u2208 \u0393\u2032 \u2229 \u0393 :\nn\nY\n\n!\n\nn\nY\n\np1 (Xi ) = 0 X\n\ni=1\n\n\u0012\n\n= EX 1 \u2212\n\npj (Xi ) = 0 X\n\ni=1\n\n!\n\n!2n\n\n\u00132n\nm\nC2m\u2212|X|\nm\nC2m\n\nn\n\nn\nm\u2212n n 2\n\u2264 EX 1 \u2212\n\u2264 (1 \u2212 2\u2212(n+1) )2 \u2264 e\u22120.5 ,\n2m\nwhere |X| denotes the number of distinct elements in X. Therefore with a\nconstant probability we have p\u0302 6= q no matter how large n is.\nThis example shows that it is impossible to obtain any rate of convergence\nresult using the index of resolvability alone. In order to estimate convergence,\nit is thus necessary to make additional assumptions, such as the prior decay\ncondition of Theorem 4.3. The randomization used in the construction is\nnot essential. This is because there exists at least one draw (a deterministic\nconfiguration) that leads to convergence probability (the probability of correct estimation) at least as large as the expected convergence probability of\ne\u22120.5 under randomization.\nWe shall also mention that starting from this example, together with a\nconstruction scheme similar to that of the Bayesian inconsistency counterexample in [2], it is not difficult to show that the standard MDL is not Hellinger\nconsistent even when the index of resolvability approaches zero as n \u2192 \u221e.\nFor simplicity, we skip the detailed construction in this paper.\n\n\u0012\n\n\u0012\n\n\u0013 \u0013\n\n4.5. Weak convergence of the standard MDL. Although Hellinger consistency cannot be obtained for standard MDL based on the index of resolvability alone, it was shown in [1] that as n \u2192 \u221e, if the index of resolvability\n\n\f19\n\nMINIMUM COMPLEXITY ESTIMATION\n\napproaches zero, then pk\u0302 converges weakly to q in probability (in the sense\ndiscussed at the end of Section 3.3). This result is a direct consequence of\nTheorem 3.2, which we shall restate here.\nTheorem 4.4. Consider the estimator defined in (6) with \u03bb = 1. Then\n\u2200 f : X \u2192 [\u22121, 1], we have\nEX Epk\u0302 f (x) \u2212\n\nn\np\n1X\nf (Xi ) \u2264 2An + 2An ,\nn i=1\n\nwhere An = inf k [DKL (q||pk ) + n1 ln \u03c01k ] +\n\nln 2\nn .\n\nNote that in the sense discussed at the end of Section 3.3, this theorem\nessentially implies that the standard MDL estimator is weakly consistent\n(in probability) as long as the index of resolvability approaches zero when\nn \u2192 \u221e. Moreover, it establishes a rate of convergence result which depends\nonly on the index of resolvability. This theorem improves the consistency\nresult in [1], where no rate of convergence result was established and f was\nassumed to be an indicator function.\n5. Bayesian posterior distributions. Assume we observe n-samples X =\n{X1 , . . . , Xn } \u2208 X n , independently drawn from the true underlying distribution Q with density q. As mentioned earlier, we call any probability density\n\u0175X (\u03b8) with respect to \u03c0 that depends on the observation X (and measurable on X n \u00d7 \u0393) a posterior. For all \u03b3 > 0, we define a generalized Bayesian\nposterior \u03c0\u03b3 (*|X) with respect to \u03c0 as (also see [15])\n(9)\n\nQn\np\u03b3 (Xi |\u03b8)\n\u03c0\u03b3 (\u03b8|X) = R Qn i=1\u03b3\n\u0393\n\ni=1 p\n\n(Xi |\u03b8) d\u03c0(\u03b8)\n\n.\n\nWe call \u03c0\u03b3 the \u03b3-Bayesian posterior. The standard Bayesian posterior is\ndenoted as \u03c0(*|X) = \u03c01 (*|X).\nThe key starting point of our analysis is the following simple observation\nthat relates the Bayesian posterior to an instance of information complexity\nminimization which we have already analyzed in this paper.\nProposition 5.1.\n\nConsider a prior \u03c0 and \u03bb > 0. Then\n\nn\np(Xi |\u03b8)\n1X\n\u03bb\nln\nR\u0302\u03bb (\u03c01/\u03bb (*|X)) = \u2212 ln E\u03c0 exp\nn\n\u03bb i=1\nq(Xi )\n\n!\n\n= inf R\u0302\u03bb (w),\nw\n\nwhere R\u0302\u03bb (w) is defined in (2), and the inf on the right-hand side is over all\npossible densities w with respect to the prior \u03c0.\n\n\f20\n\nT. ZHANG\n\nProof. The first\nequality follows from simple algebra.\nP\nNow let f (\u03b8) = \u03bb1 ni=1 ln p(Xi |\u03b8) in Proposition 2.1; we obtain\n\n\u03bb\n\u2212 ln E\u03c0 exp(f (\u03b8)) \u2264 inf R\u0302\u03bb (w) \u2264 R\u0302\u03bb (\u03c01/\u03bb (*|X)).\nw\nn\nCombining this with the first equality, we know that equality holds in the\nabove chain of inequalities. This proves the second inequality. \u0003\nThe above proposition indicates that the generalized Bayesian posterior\ncan be regarded as a minimum information complexity estimator (1) with\nS consisting of all possible densities. Therefore results parallel to those of\nMDL can be obtained.\n5.1. Generalized Bayesian methods. Similarly to the index of resolvability complexity measure for MDL, for Bayesian-like methods the corresponding model resolvability, which controls the complexity, becomes the Bayesian\nresolvability defined as\n\n(10)\n\n\u03bb\nr\u03bb,n (q) = inf E\u03c0 w(\u03b8)DKL (q||p(*|\u03b8)) + DKL (w d\u03c0||d\u03c0)\nw\nn\n\n\u0015\n\n\u0014\n\n\u03bb\n= \u2212 ln E\u03c0 e\u2212(n/\u03bb)DKL (q||p(*|\u03b8)).\nn\nThe density that attains the infimum of (10) is given by\nn\nw(\u03b8) \u221d exp \u2212 DKL (q||p(*||\u03b8)) .\n\u03bb\n\u0014\n\n\u0015\n\nThe following proposition gives a simple and intuitive estimate of the\nBayesian index of resolvability. This bound implies that the Bayesian resolvability can be estimated using local properties of the prior \u03c0 around the\ntrue density q. The quantity is small as long as there is a positive prior mass\nin a small KL-ball around the truth q.\nProposition 5.2.\nbounded as\n\nThe Bayesian resolvability defined in (10) can be\n\n\u03bb\nr\u03bb,n (q) \u2264 inf \u03b5 \u2212 ln \u03c0({p \u2208 \u0393 : DKL (q||p) \u2264 \u03b5}) .\n\u03b5>0\nn\n\u0014\n\n\u0015\n\nProof. For all \u03b5 > 0, we simply note that E\u03c0 e\u2212(n/\u03bb)DKL (q||p(*|\u03b8)) \u2265 e\u2212(n/\u03bb)\u03b5 \u00d7\n\u03c0({p \u2208 \u0393 : DKL (q||p) \u2264 \u03b5}). Now taking the logarithm and using (10), we obtain the desired inequality. \u0003\nThe following bound is a direct consequence of Corollary 3.2.\n\n\f21\n\nMINIMUM COMPLEXITY ESTIMATION\n\nTheorem 5.1. Consider the generalized Bayesian posterior \u03c01/\u03bb (\u03b8|X)\ndefined in (9) with \u03bb > 1. Then \u2200 \u03c1 \u2208 (0, 1/\u03bb]\nEX E\u03c0 \u03c01/\u03bb (\u03b8|X)D\u03c1Re (q||p(*|\u03b8)) \u2264 \u2212\n\n\u03bb\nn\nln E\u03c0 exp \u2212 DKL (q||p(*|\u03b8)) .\n\u03c1(\u03bb \u2212 1)n\n\u03bb\n\u0012\n\n\u0013\n\nThe above theorem gives a general convergence bound on the \u03b3-Bayesian\nmethod with \u03b3 < 1, depending only on the globally defined Bayesian resolvability. Note that similarly to Theorem 4.2 for the MDL case, a bound using\na localized Bayesian resolvability can also be obtained.\nTheorem 5.1 immediately implies the concentration of a generalized Bayesian\nposterior. Define the posterior mass outside an \u03b5 D\u03c1Re -ball around q as\n\u03c01/\u03bb ({p \u2208 \u0393 : D\u03c1Re (q||p) \u2265 \u03b5}|X).\nUsing the bound in Theorem 5.1 and Proposition 5.2, we can show that\nwith large probability, the generalized Bayesian posterior outside a D\u03c1Re ball of size O(\u03b5) is exponentially small when \u03b5 \u226b \u03b5\u03c0,n . However, the average\nperformance bound in Theorem 5.1 is not refined enough to yield exponential\ntail probability directly under the prior \u03c0. In order to obtain the correct\nbehavior, we shall thus consider a prior \u03c0 \u2032 related to \u03c0 which is more heavily\nconcentrated on distributions that are far away from q. We choose \u03c0 \u2032 for\nwhich Theorem 5.1 can be used to obtain a constant probability of posterior\nconcentration. We then translate the concentration of posterior with respect\nto \u03c0 \u2032 to a concentration result with respect to \u03c0.\nCorollary 5.1. Let \u03bb > 1 and \u03c1 \u2208 (0, 1/\u03bb]. Then for all t \u2265 0 and\n\u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4,\n\u03c01/\u03bb\n\n\u0012\u001a\n\np \u2208 \u0393 : D\u03c1Re(q||p) \u2265\n\n4\u03b5\u03c0,n + 2t\n\u03c1(\u03bb \u2212 1)\u03b4\n\n\u001b\n\n\u0013\n\nX \u2264\n\n1\n,\n1 + ent/\u03bb\n\nwhere the critical prior-mass radius \u03b5\u03c0,n = inf{\u03b5 : \u03b5 \u2265 \u2212 n\u03bb ln \u03c0({p \u2208 \u0393 :\nDKL (q||p) \u2264 \u03b5})}.\nProof. Let \u03b5t = 2(2\u03b5\u03c0,n + t)/(\u03c1(\u03bb \u2212 1)\u03b4), \u03931 = {p \u2208 \u0393 : D\u03c1Re (q||p) < \u03b5t }\nand \u03932 = {p \u2208 \u0393 : D\u03c1Re(q||p) \u2265 \u03b5t }. We let a = e\u2212nt/\u03bb and define \u03c0 \u2032 (\u03b8) =\na\u03c0(\u03b8)C when \u03b8 \u2208 \u03931 and \u03c0 \u2032 (\u03b8) = \u03c0(\u03b8)C when \u03b8 \u2208 \u03932 , where the normalization constant C = (a\u03c0(\u03931 ) + \u03c0(\u03932 ))\u22121 \u2208 [1, 1/a].\nNow apply Theorem 5.1 and Proposition 5.2 with the prior \u03c0 \u2032 . We obtain\n(using the Markov inequality)\n\u2032\n\u2032\nEX \u03c01/\u03bb\n(\u03932 |X)\u03b5t \u2264 EX E\u03c0\u2032 \u03c01/\u03bb\n(\u03b8|X)D\u03c1Re (q||p)\n\nn\n\u03bb\nln E\u03c0\u2032 exp \u2212 DKL (q||p(*|\u03b8))\n\u2264\u2212\n\u03c1(\u03bb \u2212 1)n\n\u03bb\n\u0012\n\n\u0013\n\n\f22\n\nT. ZHANG\n\n\u2264\u2212\n\u2264\n\u2264\n\nn\n\u03bb\nln a + ln E\u03c0 exp \u2212 DKL (q||p(*|\u03b8))\n\u03c1(\u03bb \u2212 1)n\n\u03bb\n\u0014\n\n\u0012\n\n\u0013\u0015\n\n\u03bb\n1\nt + \u03b5\u03c0,n \u2212 ln \u03c0({p \u2208 \u0393 : DKL (q||p) \u2264 \u03b5\u03c0,n })\n\u03c1(\u03bb \u2212 1)\nn\n\u0014\n\n\u0015\n\n2\u03b5\u03c0,n + t\n.\n\u03c1(\u03bb \u2212 1)\n\nIn the above derivation, the first inequality is the Markov inequality; the\nsecond inequality is from Theorem 5.1; the third inequality follows from \u03c0 \u2032 \u2265\na\u03c0; the fourth inequality follows from Proposition 5.2; the final inequality\nuses the definition of \u03b5\u03c0,n .\nNow we can divide both sides by \u03b5t , and obtain with probability 1 \u2212 \u03b4\n\u2032 (\u0393 |X) \u2264 0.5. By construction, \u03c0 \u2032 (\u0393 |X) = \u03c0\nthat \u03c01/\u03bb\n2\n1/\u03bb (\u03932 |X)/(a(1\u2212\n1/\u03bb 2\n\u03c01/\u03bb (\u03932 |X)) + \u03c01/\u03bb (\u03932 |X)). We can solve for \u03c01/\u03bb (\u03932 |X) as \u03c01/\u03bb (\u03932 |X) =\n\u2032 (\u0393 |X)/(1 \u2212 (1 \u2212 a)\u03c0 \u2032 (\u0393 |X)) \u2264 a/(1 + a). \u0003\na\u03c01/\u03bb\n2\n1/\u03bb 2\nFrom the bound, we can see that with large probability the posterior\nprobability outside a D\u03c1Re -ball with large distance t decays exponentially in\nnt and is independent of the complexity of the prior (as long as t is larger\nthan the scale of the critical radius \u03b5\u03c0,n ). As we will see later, the same is\ntrue for the standard Bayesian posterior distributions.\n5.2. The standard Bayesian method. For the standard Bayesian posterior distribution, it is impossible to bound its convergence using only the\nBayesian resolvability. The reason is the same as in the MDL case. In fact,\nit is immediately obvious that the example for MDL can also be applied\nhere. Also see [2] for a related example.\nTherefore in order to obtain a rate of convergence (and concentration) for\nthe standard Bayesian method, additional assumptions are necessary. Similarly to Theorem 4.3, bounds using upper-bracketing radius can be easily\nobtained from Corollary 3.3.\nTheorem 5.2. Consider the Bayesian posterior \u03c0(*|X) = \u03c01 (*|X) defined in (9). Consider an arbitrary cover {\u0393j } of \u0393. Then \u2200 \u03c1 \u2208 (0, 1) and\n\u03b3 \u2265 1, we have\nEX E\u03c0 \u03c0(\u03b8|X)D\u03c1Re (q||p(*|\u03b8))\n\u2264\n\n\u03b3 ln E\u03c0 e\u2212nDKL (q||p(*|\u03b8))\n\u03c1(\u03c1 \u2212 1)n\nX\n\u03b3\u2212\u03c1\nln\n\u03c0(\u0393j )(\u03b3\u22121)/(\u03b3\u2212\u03c1) (1 + rub (\u0393j ))n .\n+\n\u03c1(1 \u2212 \u03c1)n\nj\n\n\f23\n\nMINIMUM COMPLEXITY ESTIMATION\n\nFor all \u03b5 > 0, consider an \u03b5-upper discretization {\u0393\u03b5j } of \u0393. We obtain\nfrom Theorem 5.2,\nEX E\u03c0 \u03c0(\u03b8|X)D\u03c1Re (q||p(*|\u03b8))\n\u2264\n\n\u03b3 ln E\u03c0 e\u2212nDKL (q||p(*|\u03b8))\n\u03c1(\u03c1 \u2212 1)n\nln\n\u03b3\u2212\u03c1\n+\ninf\n\u03c1(1 \u2212 \u03c1) \u03b5>0\n\u0014\n\nP\n\nj\n\n\u03c0(\u0393\u03b5j )(\u03b3\u22121)/(\u03b3\u2212\u03c1)\n+ ln(1 + \u03b5) .\nn\n\u0015\n\nIn particular, let \u03b3 \u2192 1. We have\n\nEX E\u03c0 \u03c0(\u03b8|X)D\u03c1Re (q||p(*|\u03b8))\nln E\u03c0 e\u2212nDKL (q||p(*|\u03b8)) 1\nln Nub (\u0393, \u03b5)\n+ inf\n+ ln(1 + \u03b5) ,\n\u03c1(\u03c1 \u2212 1)n\n\u03c1 \u03b5>0\nn\n\u0015\n\n\u0014\n\n\u2264\n\nwhere Nub (\u0393, \u03b5) is the \u03b5-upper-bracketing covering number of \u0393.\nSimilarly to Corollary 5.1, we obtain the following concentration result\nfor the standard Bayesian posterior distribution from Theorem 5.2.\nCorollary 5.2. Let \u03b5\u03c0,n = inf{\u03b5 : \u03b5 \u2265 \u2212 n1 ln \u03c0({p \u2208 \u0393 : DKL (q||p) \u2264 \u03b5})}\nbe the critical prior-mass radius. Let \u03c1 \u2208 (0, 1). For all s \u2208 [0, 1], let\n\u03b5upper,n (s) =\n\nX\n1\n\u03c0(\u0393j )s (1 + rub (\u0393j ))n\ninf ln\nn {\u0393j }\nj\n\nbe the critical upper-bracketing radius with coefficient s, where {\u0393j } denotes\nan arbitrary covering of \u0393. Now \u2200 \u03c1 \u2208 (0, 1) and \u03b3 \u2265 1, let\n\u03b5n = 2\u03b3\u03b5\u03c0,n + (\u03b3 \u2212 \u03c1)\u03b5upper,n ((\u03b3 \u2212 1)/(\u03b3 \u2212 \u03c1)).\nWe have for all t \u2265 0 and \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4,\n\u03c0\n\n\u0012\u001a\n\np \u2208 \u0393 : D\u03c1Re(q||p) \u2265\n\n2\u03b5n + (4\u03b3 \u2212 2)t\n\u03c1(1 \u2212 \u03c1)\u03b4\n\n\u001b\n\n\u0013\n\nX \u2264\n\n1\n.\n1 + ent\n\nProof. The proof is similar to that of Corollary 5.1. We let \u03b5t = (2\u03b5n +\n(4\u03b3 \u2212 2)t)/((\u03c1 \u2212 \u03c12 )\u03b4). Define \u03931 = {p \u2208 \u0393 : D\u03c1Re (q||p) < \u03b5t } and \u03932 = {p \u2208\n\u0393 : D\u03c1Re (q||p) \u2265 \u03b5t }. We let a = e\u2212nt and define \u03c0 \u2032 (\u03b8) = a\u03c0(\u03b8)C when \u03b8 \u2208\n\u03931 and \u03c0 \u2032 (\u03b8) = \u03c0(\u03b8)C when \u03b8 \u2208 \u03932 , where the normalization constant C =\n(a\u03c0(\u03931 ) + \u03c0(\u03932 ))\u22121 \u2208 [1, 1/a].\nUsing Proposition 5.2 and the assumption of the theorem, we obtain\n\u03b3 ln E\u03c0\u2032 e\u2212nDKL (q||p(*|\u03b8))\n\u03c1(\u03c1 \u2212 1)n\n\n\f24\n\nT. ZHANG\n\n+\n\n\u2264\n\nX\n\u03b3\u2212\u03c1\ninf ln\n\u03c0 \u2032 (\u0393j )(\u03b3\u22121)/(\u03b3\u2212\u03c1) (1 + rub (\u0393j ))n\n\u03c1(1 \u2212 \u03c1)n {\u0393j }\nj\n\n\u03b3t + (\u03b3/n) ln E\u03c0 e\u2212nDKL (q||p(*|\u03b8))\n\u03c1(1 \u2212 \u03c1)\n\n\u03b3 \u22121\n\u03b3 \u2212 \u03c1 (\u03b3 \u2212 1)t\n+ \u03b5upper,n\n+\n\u03c1(1 \u2212 \u03c1) \u03b3 \u2212 \u03c1\n\u03b3\u2212\u03c1\n\u0014\n\n\u2264\n\n\u0012\n\n\u0013\u0015\n\n(2\u03b3 \u2212 1)t + \u03b5n\n.\n\u03c1(1 \u2212 \u03c1)\n\nIn the first inequality, we have used the fact that a\u03c0(\u03b8) \u2264 \u03c0 \u2032 (\u03b8) \u2264 \u03c0(\u03b8)/a.\nSimilarly to the proof of Corollary 5.1, we can use Markov inequality to\nobtain \u03c0 \u2032 (\u03932 |X) \u2264 0.5 with probability 1 \u2212 \u03b4. This leads to the desired bound\n\u2032 (\u0393 |X)/(1 \u2212 (1 \u2212 a)\u03c0 \u2032 (\u0393 |X)). \u0003\nfor \u03c0(\u03932 |X) = a\u03c01/\u03bb\n2\n1/\u03bb 2\nIn this theorem, we can use the estimate\n\u0014\n\u0015\n1\n\u03b5upper,n (s) \u2264 inf\nln Nub (\u0393, \u03b5) + ln(1 + \u03b5) ,\n\u03b5>0 n\nwhere Nub (\u0393, \u03b5) is the upper-bracketing covering number of \u0393 at scale \u03b5. The\nresult implies that if the critical upper-bracketing radius \u03b5upper,n is at the\nsame (or smaller) order of the critical prior-mass radius \u03b5\u03c0,n , then with large\nprobability, the standard Bayesian posterior distribution will concentrate in\na D\u03c1Re -ball of size \u03b5\u03c0,n . In this case, the standard Bayesian posterior has\nthe same rate of convergence when compared with the generalized Bayesian\nposterior with \u03bb > 1. However, if \u03b5upper,n is large, then the standard Bayesian\nmethod may fail to concentrate in a small D\u03c1Re -ball around the truth q, even\nwhen the critical prior radius \u03b5\u03c0,n is small. This can be easily seen from the\nsame counterexample used to illustrate the slow convergence of the standard\nMDL.\nAlthough the standard Bayesian posterior distribution may not concentrate even when \u03b5\u03c0,n is small, Theorem 3.2 implies that the Bayesian density\nestimator E\u03c0 \u03c0(\u03b8|X)p(*|X) is close to q in the sense of weak convergence.\nThe consistency theorem given in [2] also relies on the upper covering\nnumber Nub (\u0393, \u03b5). However, no convergence rate was established. Therefore\nCorollary 5.2 in some sense can be regarded as a refinement of their analysis using their covering definition. Other kinds of covering numbers (e.g.,\nHellinger covering) can also be used in convergence analysis of nonparametric Bayesian methods. For example, some different definitions can be found\nin [4] and [12].\nThe convergence analysis in [12] employed techniques from empirical processes, which can possibly lead to suboptimal convergence rates when the covering number grows relatively fast as the scale \u03b5 \u2192 0. We shall focus on\n\n\f25\n\nMINIMUM COMPLEXITY ESTIMATION\n\n[4], which employed techniques from hypothesis testing in [6]. The resulting\nconvergence theorem from their analysis cannot be as simply stated as those\nin this paper. Moreover, some of their conditions can be relaxed. Using techniques of this paper, we can obtain the following result. The proof, which\nrequires two additional lemmas, is left to the Appendix.\nTheorem 5.3. Consider a partition of \u0393 as the union of countably many\ndisjoint measurable sets \u0393j (j = 1, . . .). Then \u2200 \u03c1 \u2208 (0, 1) and \u03b3 \u2265 1\nEX\n\nX\nj\n\n\u2264\n\n\u03c0(\u0393j |X)\n\ninf\n\np\u2208co(\u0393j )\n\n(\u03b3 \u2212 \u03c1) ln\n\nP\n\nj\n\nD\u03c1Re (q||p)\n\u2212n supp\u2208co(\u0393j ) DKL (q||p)\n\n\u03c0(\u0393j )(\u03b3\u22121)/(\u03b3\u2212\u03c1) \u2212 \u03b3 ln j \u03c0(\u0393j )e\n\u03c1(1 \u2212 \u03c1)n\nP\n\n,\nR\n\nwhere co(\u0393j ) is the convex hull of densities in \u0393j , \u03c0(\u0393j ) = \u0393j d\u03c0(\u03b8) is\nR Q\nthe prior probability of \u0393j and \u03c0(\u0393j |X) = \u0393j ni=1 p(Xi |\u03b8) d\u03c0(\u03b8)/\nR Qn\n\u0393 i=1 p(Xi |\u03b8) d\u03c0(\u03b8) is the Bayesian posterior probability of \u0393j .\n\nAn immediate consequence of the above theorem is a result on the concentration of Bayesian posterior distributions that refines some aspects of\nthe main result in [4]. It also complements the upper-bracketing radiusbased bound in Corollary 5.2. For simplicity, we only state a version for\n\u03c1-divergence so that the result is directly comparable to that of [4]. A similar bound can be stated for R\u00e9nyi entropy.\nCorollary 5.3. Let \u03b5\u03c0,n = inf{\u03b5 : \u03b5 \u2265 \u2212 n1 ln \u03c0({p \u2208 \u0393 : DKL (q||p) \u2264 \u03b5})}.\nGiven \u03c1 \u2208 (0, 1), we assume that \u2200 \u03b5 > 0, {p \u2208 \u0393 : D\u03c1 (q||p) \u2265 \u03b5} can be covered\nby the union of measurable sets \u0393\u03b5j (j = 1, . . .) such that inf{D\u03c1 (q||p) : p \u2208\nS\n\u03b5\nj co(\u0393j )} \u2265 \u03b5/2. For all s \u2208 [0, 1], let\n(\n\n!)\n\nX\n1\n\u03b5conv,n (s) = sup \u03b50 : \u03b50 < sup inf\u03b5 ln\n\u03c0(\u0393\u03b5j )s + 2\nn \u03b5\u2265\u03b50 {\u0393j }\nj\n\nbe the critical convex-cover radius. Now \u2200\u03b3 \u2265 1 let\n\u03b5n = 2\u03b3\u03b5\u03c0,n + (\u03b3 \u2212 \u03c1)\u03b5conv,n ((\u03b3 \u2212 1)/(\u03b3 \u2212 \u03c1)).\nFor all t \u2265 0 and \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4,\n\u03c0\n\n4\u03b5n + (8\u03b3 \u2212 4)t\np \u2208 \u0393 : D\u03c1 (q||p) \u2265\n\u03c1(1 \u2212 \u03c1)\u03b4\n\n\u0012\u001a\n\n\u001b\n\n\u0013\n\nX \u2264\n\n1\n.\n1 + ent\n\nProof. Let \u03b5t = 4(\u03b5n + (2\u03b3 \u2212 1)t)/(\u03c1(1 \u2212 \u03c1)\u03b4). Similarly to the proof\nof Corollary 5.1, we define \u03931 = {p \u2208 \u0393 : D\u03c1 (q||p) < \u03b5t }, \u03932 = \u0393 \u2212 \u03931 . We let\n\n\f26\n\nT. ZHANG\n\na = e\u2212nt and define \u03c0 \u2032 (\u03b8) = a\u03c0(\u03b8)C when \u03b8 \u2208 \u03931 and \u03c0 \u2032 (\u03b8) = \u03c0(\u03b8)C when\n\u03b8 \u2208 \u03932 , where the normalization constant C = (a\u03c0(\u03931 ) + \u03c0(\u03932 ))\u22121 \u2208 [1, 1/a].\nLet \u0393\u20320 = {p \u2208 \u0393 : DKL (q||p) < \u03b5\u03c0,n }. Since DKL (q||p) = D0 (q||p) and\n\u03b5t \u2265 \u03b5\u03c0,n / min(\u03c1, 1 \u2212 \u03c1), we know from Proposition 3.1 that \u0393\u20320 \u2282 \u03931 . Let\n\u0393\u2032\u22121 = \u03931 \u2212 \u0393\u20320 . By assumption, it is clear that \u03932 can be partitioned into\nthe union of disjoint measurable sets {\u0393\u2032j } (j \u2265 1) such that \u0393\u2032j \u2282 \u0393\u03b5j t and\ninf p\u2208S co(\u0393\u2032 ) D\u03c1 (q||p) \u2265 \u03b5t /2. For this partition, we have\nj\n\nj\u22651\n\nEX \u03c0 \u2032 (\u03932 |X)\u03b5t /2 \u2264 EX\n\nX\n\nj\u2265\u22121\n\n\u03c0 \u2032 (\u0393\u2032j |X)\n\nD\u03c1 (q||p).\n\ninf\n\np\u2208co(\u0393\u2032j )\n\nNote that\nln\n\nX\n\n\u03c0\n\n\u2032\n\n(\u0393\u2032j )(\u03b3\u22121)/(\u03b3\u2212\u03c1)\n\nj\u2265\u22121\n\n\"\n\n\u2264\u2212\nand\n\u2212 ln\n\nX\n\n\u03b3\u22121\nln a + n\u03b5conv,n\n\u03b3\u2212\u03c1\n\n\u2212n supp\u2208co(\u0393\u2032 ) DKL (q||p)\n\n\u03c0 \u2032 (\u0393\u2032j )e\n\n#\n\nX\n\u03b3\u22121\n\u2264\u2212\nln a + ln\n\u03c0(\u0393\u03b5j t )(\u03b3\u22121)/(\u03b3\u2212\u03c1) + 2\n\u03b3\u2212\u03c1\nj\u22651\n\nj\n\n\u2264 n sup DKL (q||p) \u2212 ln \u03c0 \u2032 (\u0393\u20320 )\np\u2208co(\u0393\u20320 )\n\nj\u2265\u22121\n\n\u2264 2n\u03b5\u03c0,n + nt.\nCombining the above estimates, and plugging them into Theorem 5.3, we\nobtain\n(\u03b3 \u2212 \u03c1)(\u2212(ln a)(\u03b3 \u2212 1)/(\u03b3 \u2212 \u03c1) + n\u03b5conv,n ) + \u03b3(2n\u03b5\u03c0,n + nt)\nEX \u03c0 \u2032 (\u03932 |X) \u2264\n\u03c1(1 \u2212 \u03c1)n\u03b5t /2\n= 0.5\u03b4.\n\nTherefore \u03c0 \u2032 (\u03932 |X) \u2264 0.5 with probability 1 \u2212 \u03b4. The desired bound for\n\u2032 (\u0393 |X)/(1\u2212(1\u2212a)\u03c0 \u2032 (\u0393 |X)).\n\u03c0(\u03932 |X) can be obtained from \u03c0(\u03932 |X) = a\u03c01/\u03bb\n2\n1/\u03bb 2\n\u0003\nIf we can cover {p \u2208 \u0393 : D\u03c1 (q||p) \u2265 \u03b5} bySN\u03b5 convex measurable sets \u0393\u03b5j\n(j = 1, . . . , N\u03b5 ) such that inf{D\u03c1 (q||p) : p \u2208 j \u0393\u03b5j } \u2265 \u03b5/2, then we may take\n\u03b3 = 1 in Corollary 5.3 with \u03b5conv,n defined as\n1\n\u03b5conv,n = sup \u03b50 : \u03b50 < ln sup N\u03b5 + 2\nn\n\u03b5\u2265\u03b50\n\u001a\n\n\u0012\n\n\u0013\u001b\n\n.\n\nClearly if n1 ln N\u03b5 = O(\u03b5\u03c0,n ) for some \u03b5 = O(\u03b5\u03c0,n ), then with large probability Bayesian posterior distributions concentrate on a D\u03c1 -ball of size O(\u03b5\u03c0,n )\naround q. Note that this result relaxes a condition of [4], where our definition of \u03b5\u03c0,n was replaced by possibly smaller balls {p \u2208 \u0393 : DKL (q||p) \u2264\n\n\fMINIMUM COMPLEXITY ESTIMATION\n\n27\n\n\u03b5, Eq ln( pq )2 \u2264 \u03b5}. Moreover, their covering definition N\u03b5 does not apply to\narbitrary convex covering sets directly (although it is not difficult to modify\ntheir proof to deal with this case), and their result does not directly handle\nnoncompact families where N\u03b5 = \u221e (which can be directly handled by our\nresult with \u03b3 > 1).\nIt is worth mentioning that for practical purposes, the balls {p \u2208 \u0393 :\nDKL (q||p)\u2264 \u03b5, Eq ln( pq )2 \u2264 \u03b5} and {p \u2208 \u0393 : DKL (q||p) \u2264 \u03b5} are usually of comparable size. Therefore relaxing this condition may not always lead to significant practical advantages. However, it is possible to construct examples such that this refinement makes a difference. For example, consider the\ndiscrete family \u0393 = {pj } (j \u2265 1) with prior \u03c0j = 1/j(j + 1). Assume that\nthe truth q(x) is the uniform distribution on [0, 1], and pj (x) = 2\u2212j when\nx \u2208 [0, j \u22122 /2] and pj (x) = (j 2 \u2212 2\u2212j\u22121 )/(j 2 \u2212 0.5) otherwise. It is clear that\nEq ln( pqj )2 \u2265 0.5 ln 4, while limj\u2192\u221e DKL (q||pj ) = 0. Therefore the result in [4]\ncannot be applied, while Corollary 5.3 implies that the posterior distribution\nis consistent in this example.\nApplications of convergence results similar to Corollary 5.2 and Corollary 5.3 can be found in [4] and [12]. It is also useful to note that Corollary 5.1\nrequires less assumptions to achieve good convergence rates, implying that\ngeneralized Bayesian methods are more stable than the standard Bayesian\nmethod. This fact has also been observed in [15].\n6. Discussion. This paper studies certain randomized (and deterministic) density estimation methods which we call information complexity minimization. We introduced a general KL-entropy based convergence analysis,\nand demonstrated that this approach can lead to simplified and improved\nconvergence results for MDL and Bayesian posterior distributions.\nAn important observation from our study is that generalized information\ncomplexity minimization methods with regularization parameter \u03bb > 1 are\nmore robust than the corresponding standard methods with \u03bb = 1. That\nis, their convergence behavior is completely determined by the local prior\ndensity around the true distribution measured by the model resolvability\ninf w\u2208S R\u03bb (w). For MDL, this quantity (index of resolvability) is well behaved\nif we put a not too small prior mass at a density that is close to the truth\nq. For the Bayesian posterior, this quantity (Bayesian resolvability) is well\nbehaved if we put a not too small prior mass in a small KL-ball around\nq. We have also demonstrated through an example that the standard MDL\n(and Bayesian posterior) does not have this desirable property. That is, even\nif we can guess the true density by putting a relatively large prior mass at\nthe true density q, we may not be able to estimate q very well as long as\nthere exists a bad (random) prior structure even at places very far from the\ntruth q.\n\n\f28\n\nT. ZHANG\n\nTherefore, although the standard Bayesian method is \"optimal\" in a certain averaging sense, its behavior is heavily dependent on the regularity of\nthe prior distribution globally. Intuitively, the standard Bayesian method can\nput too much emphasis on the difficult part of the prior distribution, which\ndegrades the estimation quality in the easier part in which we are actually\nmore interested. Therefore even if one is able to guess the true distribution\nby putting a large prior mass around its neighborhood, the Bayesian method\ncan still behave poorly if one accidentally makes bad choices elsewhere. This\nimplies that unless one completely understands the impact of the prior, it\nis much safer to use a generalized Bayesian method with \u03bb > 1.\nAPPENDIX\nS ). In order to apply Theorem 3.1, we\nA.1. Lower bounds of EX R\u0302\u03bb\u2032 (\u0175X\nS ) from below.\nshall bound the quantity EX R\u0302\u03bb\u2032 (\u0175X\n\nLemma A.1.\n\n\u2032\n\nS ) \u2265 \u2212 \u03bb ln E En ( p(x|\u03b8) )1/\u03bb \u2265 0.\nFor all \u03bb\u2032 \u2265 1, EX R\u0302\u03bb\u2032 (\u0175X\n\u03c0 q q(x)\nn\n\u2032\n\nProof. The convex duality in Proposition 2.1 with f (x) = \u2212 \u03bb1\u2032\nimplies\n\nq(Xi )\ni=1 ln p(Xi |\u03b8)\n\nPn\n\nn\n1 X\nq(Xi )\nln E\u03c0 exp \u2212 \u2032\nln\n.\nn\n\u03bb i=1 p(Xi |\u03b8)\n\n\u03bb\nS\n)\u2265\u2212\nR\u0302\u03bb\u2032 (\u0175X\n\n!\n\n\u2032\n\nNow by taking expectation and using Jensen's inequality with the convex\nfunction \u03c8(x) = \u2212 ln(x), we obtain\nS\nEX R\u0302\u03bb\u2032 (\u0175X\n)\n\nn\n\u03bb\u2032\n1 X\nq(Xi )\n\u2265 \u2212 ln EX E\u03c0 exp \u2212 \u2032\nln\nn\n\u03bb i=1 p(Xi |\u03b8)\n\n=\u2212\n\np(x|\u03b8)\n\u03bb\u2032\nln E\u03c0 Enq\nn\nq(x)\n\u0012\n\n\u00131/\u03bb\u2032\n\n!\n\n\u2265 0,\n\nwhich proves the lemma. \u0003\nLemma A.2. Consider an arbitrary cover {\u0393j } of \u0393. The following inequality is valid \u2200 \u03bb\u2032 \u2208 [0, 1]:\n1 X\n\u2032\nS\nEX R\u0302\u03bb\u2032 (\u0175X\n) \u2265 \u2212 ln\n\u03c0(\u0393j )\u03bb (1 + rub (\u0393j ))n ,\nn\nj\nwhere rub is the upper-bracketing radius in Definition 3.2.\n\n\f29\n\nMINIMUM COMPLEXITY ESTIMATION\n\nProof. The proof is similar to that of Lemma A.1, but with a slightly\ndifferent estimate. We again start with the inequality\nn\n1 X\nq(Xi )\nln E\u03c0 exp \u2212 \u2032\nln\n.\nn\n\u03bb i=1 p(Xi |\u03b8)\n\n\u03bb\nS\n)\u2265\u2212\nR\u0302\u03bb\u2032 (\u0175X\n\n!\n\n\u2032\n\nTaking expectation and using Jensen's inequality with the convex function\n\u03c8(x) = \u2212 ln(x), we obtain\nS\n\u2212EX R\u0302\u03bb\u2032 (\u0175X\n)\u2264\n\nn\n1\n\u2032\n1 X\nq(Xi )\nln EX E\u03bb\u03c0 exp \u2212 \u2032\nln\nn\n\u03bb i=1 p(Xi |\u03b8)\n\n!\n\nn\nX\n1 X\n1\nq(Xi )\n\u03c0(\u0393j ) exp \u2212 \u2032\n\u2264 ln EX\nln\nn\n\u03bb i=1 sup\u03b8\u2208\u0393j p(Xi |\u03b8)\nj\n\n\"\n\nn\nX\nX\n\u2032\nq(Xi )\n1\n\u03c0(\u0393j )\u03bb exp \u2212\nln\n\u2264 ln EX\nn\nsup\u03b8\u2208\u0393j p(Xi |\u03b8)\nj\ni=1\n\n\"\n\nn sup\nX\nY\n1\n\u2032\n\u03b8\u2208\u0393j p(Xi |\u03b8)\n= ln\n\u03c0(\u0393j )\u03bb EX\nn\nq(Xi )\nj\ni=1\n\n\"\n\"\n\n!#\u03bb\u2032\n\n!#\n\n#\n\n#\n\nX\n\u2032\n1\n\u03c0(\u0393j )\u03bb (1 + rub (\u0393j ))n .\n= ln\nn\nj\n\nThe third inequality\nfollows\nthe fact that \u2200 \u03bb\u2032 \u2208 [0, 1] and positive numP \u03bbfrom\nP\n\u2032\n\u2032\n\u03bb\nbers {aj }, ( j aj ) \u2264 j aj . \u0003\nA.2. Proof of Theorem 5.3. The proof requires two lemmas.\n\nLemma A.3. Consider a partition of \u0393 as the union of countably many\ndisjoint measurable sets \u0393j (j = 1, . . .). Let\nq(X) =\n\nn\nY\n\n1\npj (X) =\n\u03c0(\u0393j )\n\nq(Xi ),\n\ni=1\n\nn\nY\n\nZ\n\n\u0393j i=1\n\nThen we have \u2200 \u03c1 \u2208 (0, 1) and \u03b3 \u2265 1,\nEX\n\nX\nj\n\n\u2264\n\np(Xi |\u03b8) d\u03c0(\u03b8).\n\n\u03c0(\u0393j |X)D\u03c1Re (q(X \u2032 )||pj (X \u2032 ))\n(\u03b3 \u2212 \u03c1) ln\n\nP\n\nj\n\n\u03c0(\u0393j )(\u03b3\u22121)/(\u03b3\u2212\u03c1) \u2212 \u03b3 ln\n\u03c1(1 \u2212 \u03c1)\n\nwhere X \u2032 , X \u2208 X n , q(X) =\n\nQn\n\npj (X) =\n\nP\n\nj\n\n\u03c0(\u0393j )e\u2212DKL (q(X\n\n\u2032 )||p\n\ni=1 q(Xi )\n\nis the true density of X and\n\n1\n\u03c0(\u0393j )\n\nn\nY\n\nZ\n\n\u0393j i=1\n\np(Xi |\u03b8) d\u03c0(\u03b8)\n\nj (X\n\n\u2032 ))\n\n,\n\n\f30\n\nT. ZHANG\n\nis the mixture density over \u0393j under \u03c0.\nProof. We shall apply Corollary 3.3 with a slightly different interpretation. Instead of considering X as n independent samples Xi as before,\nwe simply regard it as one random variable by itself. Consider the family\n\u0393\u2032 which consists of discrete densities pj (X), with prior \u03c0j = \u03c0(\u0393j ). This\ndiscretization itself can be regarded as a 0-upper discretization of \u0393\u2032 . Also,\ngiven X, it is easy to see that the Bayesian posterior on \u0393\u2032 with respect to\n{\u03c0j } is \u03c0\u0302j = \u03c0(\u0393j |X). We can thus apply Corollary 3.3 on \u0393\u2032 , which leads\nto the stated bound [with the help of (10)]. \u0003\nIn order to apply the above lemma, we also need to simplify D\u03c1Re (q(X \u2032 )||pj (X \u2032 ))\nand DKL (q(X \u2032 )||pj (X \u2032 )).\nLemma A.4.\ninf\n\nWe have the bounds\n\np\u2208co(\u0393j )\n\nD\u03c1Re (q(X1 )||p(X1 )) \u2264\n\u2264\n\nD\u03c1Re (q(X)||pj (X))\nn\nsup D\u03c1Re (q(X1 )||p(X1 ))\n\np\u2208co(\u0393j )\n\nand\ninf\n\np\u2208co(\u0393j )\n\nDKL (q(X1 )||p(X1 )) \u2264\n\u2264\n\nDKL (q(X)||pj (X))\nn\nsup DKL (q(X1 )||p(X1 )).\np\u2208co(\u0393j )\n\nProof. Since DKL (q||p) = lim\u03c1\u21920+ D\u03c1Re (q||p), we only need to prove the\nfirst two inequalities. The proof is essentially the same as that of Lemma 4\non page 478 of [6], which dealt with the existence of tests under the Hellinger\ndistance. We include it here for completeness.\nWe shall only prove the first half of the first two inequalities (the second\nhalf has an identical proof ) and we shall prove the claim by induction. If\nn = 1, then since pj (X) \u2208 co(\u0393j ) the claim holds trivially. Now assume that\nthe claim holds for n = k. For n = k + 1, if we let\nw(\u03b8|X1 , . . . , Xk ) = R\n\n\u0393j\n\nthen\n\nexp(\u2212\u03c1(1 \u2212 \u03c1)D\u03c1Re (q(X)||pj (X)))\n\nQk\n\nQk\n\ni=1 p(Xi |\u03b8)\n\ni=1 p(Xi |\u03b8) d\u03c0(\u03b8)\n\n,\n\n\f31\n\nMINIMUM COMPLEXITY ESTIMATION\n\n= EX1 ,...,Xk\n\n\u0012R\n\n\u00d7 EXk+1\n\n\u0012R\n\n\u2264 EX1 ,...,Xk\n\nQk\n\ni=1 p(Xi |\u03b8) d\u03c0(\u03b8)\nQ\n\u03c0(\u0393j ) ki=1 q(Xi )\n\n\u0393j\n\n\u0393j\n\n\u0013\u03c1\n\nw(\u03b8|X1 , . . . , Xk )p(Xk+1 |\u03b8) d\u03c0(\u03b8) \u0013\u03c1\nq(Xk+1 )\n\n\u0012 1/\u03c0(\u0393 )\nj\n\n\u00d7 sup EXk+1\np\u2208co(\u0393j )\n\n\u0012\n\nR\n\n\u0393j\n\nQk\n\nQk\n\ni=1 p(Xi |\u03b8) d\u03c0(\u03b8)\n\ni=1 q(Xi )\n\u0013\np(Xk+1 ) \u03c1\n\n\u0013\u03c1\n\nq(Xk+1 )\n\n\u0012 1/\u03c0(\u0393 ) R Qk p(X |\u03b8) d\u03c0(\u03b8) \u0013\u03c1\ni\nj \u0393j\ni=1\n= EX1 ,...,Xk\nQk\ni=1 q(Xi )\n\n\u2212\u03c1(1\u2212\u03c1)D\u03c1Re (q(Xk+1 )||p(Xk+1 ))\n\n\u00d7 sup e\np\u2208co(\u0393j )\n\n\u2212\u03c1(1\u2212\u03c1)k inf p\u2208co(\u0393j ) D\u03c1Re (q(X1 )||p(X1 ))\n\n\u2264e\n\nRe (q(X\nk+1 )||p(Xk+1 ))\n\n* sup e\u2212\u03c1(1\u2212\u03c1)D\u03c1\np\u2208co(\u0393j )\n\n\u0012\n\n= exp \u2212\u03c1(1 \u2212 \u03c1)n\n\ninf\n\np\u2208co(\u0393j )\n\n\u0013\n\nD\u03c1Re (q(X1 )||p(X1 ))\n\n.\n\nThis proves the claim for n = k + 1. Note that in the above derivation, the\nfirst of the two inequalities\nfollows from the fact that with fixed X1 , . . . , Xk ,\nR\nthe density p(Xk+1 ) = \u0393j wi (\u03b8|X1 , . . . , Xk )p(Xk+1 |\u03b8) d\u03c0(\u03b8) \u2208 co(\u0393j ); the second of the two inequalities follows from the induction hypothesis. \u0003\nProof of Theorem 5.3.\nma A.4 into Lemma A.3. \u0003\n\nWe simply substitute the estimates of Lem-\n\nAcknowledgments. The author would like to thank Andrew Barron for\nhelpful discussions that motivated some ideas presented in this paper, and\nMatthias Seeger for useful comments on an earlier version of the paper. The\nauthor would also like to thank the anonymous referees for helpful comments\nand for pointing out related papers.\nREFERENCES\n[1] Barron, A. and Cover, T. (1991). Minimum complexity density estimation. IEEE\nTrans. Inform. Theory 37 1034\u20131054. MR1111806\n[2] Barron, A., Schervish, M. J. and Wasserman, L. (1999). The consistency of\nposterior distributions in nonparametric problems. Ann. Statist. 27 536\u2013561.\nMR1714718\n[3] Catoni, O. (2004). A PAC-Bayesian approach to adaptive classification. Available\nat www.proba.jussieu.fr/users/catoni/homepage/classif.pdf.\n\n\f32\n\nT. ZHANG\n\n[4] Ghosal, S., Ghosh, J. K. and van der Vaart, A. W. (2000). Convergence rates\nof posterior distributions. Ann. Statist. 28 500\u2013531. MR1790007\n[5] Le Cam, L. (1973). Convergence of estimates under dimensionality restrictions. Ann.\nStatist. 1 38\u201353. MR0334381\n[6] Le Cam, L. (1986). Asymptotic Methods in Statistical Decision Theory. Springer,\nNew York. MR0856411\n[7] Li, J. (1999). Estimation of mixture models. Ph.D. dissertation, Dept. Statistics,\nYale Univ.\n[8] Meir, R. and Zhang, T. (2003). Generalization error bounds for Bayesian mixture\nalgorithms. J. Mach. Learn. Res. 4 839\u2013860.\n[9] R\u00e9nyi, A. (1961). On measures of entropy and information. Proc. Fourth Berkeley Symp. Math. Statist. Probab. 1 547\u2013561. Univ. California Press, Berkeley.\nMR0132570\n[10] Rissanen, J. (1989). Stochastic Complexity in Statistical Inquiry. World Scientific,\nSingapore. MR1082556\n[11] Seeger, M. (2002). PAC-Bayesian generalization error bounds for Gaussian process\nclassification. J. Mach. Learn. Res. 3 233\u2013269. MR1971338\n[12] Shen, X. and Wasserman, L. (2001). Rates of convergence of posterior distributions.\nAnn. Statist. 29 687\u2013714. MR1865337\n[13] van de Geer, S. (2000). Empirical Processes in M -Estimation. Cambridge Univ.\nPress.\n[14] van der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical Processes. With Applications to Statistics. Springer, New York. MR1385671\n[15] Walker, S. and Hjort, N. (2001). On Bayesian consistency. J. R. Stat. Soc. Ser.\nB Stat. Methodol. 63 811\u2013821. MR1872068\n[16] Yang, Y. and Barron, A. (1999). Information-theoretic determination of minimax\nrates of convergence. Ann. Statist. 27 1564\u20131599. MR1742500\n[17] Zhang, T. (1999).\nTheoretical analysis of a class of randomized regularization methods. In Proc. Twelfth\nAnnual Conference on Computational Learning Theory 156\u2013163. ACM Press,\nNew York. MR1811611\n[18] Zhang, T. (2004). Learning bounds for a generalized family of Bayesian posterior\ndistributions. In Advances in Neural Information Processing Systems 16 (S.\nThrun, L. K. Saul and B. Sch\u00f6lkopf, eds.) 1149\u20131156. MIT Press, Cambridge,\nMA.\nYahoo Research\n135 Oakland Avenue\nTuckahoe, New York 10707\nUSA\nE-mail: tzhang@yahoo-inc.com\n\n\f"}