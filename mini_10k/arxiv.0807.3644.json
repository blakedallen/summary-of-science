{"id": "http://arxiv.org/abs/0807.3644v1", "guidislink": true, "updated": "2008-07-23T12:24:54Z", "updated_parsed": [2008, 7, 23, 12, 24, 54, 2, 205, 0], "published": "2008-07-23T12:24:54Z", "published_parsed": [2008, 7, 23, 12, 24, 54, 2, 205, 0], "title": "A Sparse-Sparse Iteration for Computing a Sparse Incomplete\n  Factorization of the Inverse of an SPD Matrix", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0807.5096%2C0807.2495%2C0807.0877%2C0807.3797%2C0807.3066%2C0807.2675%2C0807.3631%2C0807.4367%2C0807.2371%2C0807.2700%2C0807.1686%2C0807.0958%2C0807.4030%2C0807.1180%2C0807.4404%2C0807.1757%2C0807.0944%2C0807.5059%2C0807.3015%2C0807.2229%2C0807.3160%2C0807.2951%2C0807.4194%2C0807.1147%2C0807.1591%2C0807.4663%2C0807.0392%2C0807.3475%2C0807.2243%2C0807.0828%2C0807.4325%2C0807.3477%2C0807.2056%2C0807.2237%2C0807.1488%2C0807.3417%2C0807.0815%2C0807.5118%2C0807.3813%2C0807.2809%2C0807.4766%2C0807.0918%2C0807.4100%2C0807.1463%2C0807.1201%2C0807.2683%2C0807.2110%2C0807.1661%2C0807.4555%2C0807.1320%2C0807.3644%2C0807.2885%2C0807.4596%2C0807.4631%2C0807.0539%2C0807.0143%2C0807.5113%2C0807.3955%2C0807.4257%2C0807.0549%2C0807.1080%2C0807.4739%2C0807.0561%2C0807.4023%2C0807.2731%2C0807.3695%2C0807.1756%2C0807.1391%2C0807.3380%2C0807.3616%2C0807.2962%2C0807.4547%2C0807.3065%2C0807.1856%2C0807.1379%2C0807.0632%2C0807.4167%2C0807.1039%2C0807.4992%2C0807.1566%2C0807.3625%2C0807.2455%2C0807.1442%2C0807.1390%2C0807.3549%2C0807.4201%2C0807.2784%2C0807.1345%2C0807.1155%2C0807.2562%2C0807.0097%2C0807.3778%2C0807.2557%2C0807.3098%2C0807.3193%2C0807.2687%2C0807.4922%2C0807.3683%2C0807.3426%2C0807.2764%2C0807.0437&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Sparse-Sparse Iteration for Computing a Sparse Incomplete\n  Factorization of the Inverse of an SPD Matrix"}, "summary": "In this paper, a method via sparse-sparse iteration for computing a sparse\nincomplete factorization of the inverse of a symmetric positive definite matrix\nis proposed. The resulting factorized sparse approximate inverse is used as a\npreconditioner for solving symmetric positive definite linear systems of\nequations by using the preconditioned conjugate gradient algorithm. Some\nnumerical experiments on test matrices from the Harwell-Boeing collection for\ncomparing the numerical performance of the presented method with one available\nwell-known algorithm are also given.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0807.5096%2C0807.2495%2C0807.0877%2C0807.3797%2C0807.3066%2C0807.2675%2C0807.3631%2C0807.4367%2C0807.2371%2C0807.2700%2C0807.1686%2C0807.0958%2C0807.4030%2C0807.1180%2C0807.4404%2C0807.1757%2C0807.0944%2C0807.5059%2C0807.3015%2C0807.2229%2C0807.3160%2C0807.2951%2C0807.4194%2C0807.1147%2C0807.1591%2C0807.4663%2C0807.0392%2C0807.3475%2C0807.2243%2C0807.0828%2C0807.4325%2C0807.3477%2C0807.2056%2C0807.2237%2C0807.1488%2C0807.3417%2C0807.0815%2C0807.5118%2C0807.3813%2C0807.2809%2C0807.4766%2C0807.0918%2C0807.4100%2C0807.1463%2C0807.1201%2C0807.2683%2C0807.2110%2C0807.1661%2C0807.4555%2C0807.1320%2C0807.3644%2C0807.2885%2C0807.4596%2C0807.4631%2C0807.0539%2C0807.0143%2C0807.5113%2C0807.3955%2C0807.4257%2C0807.0549%2C0807.1080%2C0807.4739%2C0807.0561%2C0807.4023%2C0807.2731%2C0807.3695%2C0807.1756%2C0807.1391%2C0807.3380%2C0807.3616%2C0807.2962%2C0807.4547%2C0807.3065%2C0807.1856%2C0807.1379%2C0807.0632%2C0807.4167%2C0807.1039%2C0807.4992%2C0807.1566%2C0807.3625%2C0807.2455%2C0807.1442%2C0807.1390%2C0807.3549%2C0807.4201%2C0807.2784%2C0807.1345%2C0807.1155%2C0807.2562%2C0807.0097%2C0807.3778%2C0807.2557%2C0807.3098%2C0807.3193%2C0807.2687%2C0807.4922%2C0807.3683%2C0807.3426%2C0807.2764%2C0807.0437&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper, a method via sparse-sparse iteration for computing a sparse\nincomplete factorization of the inverse of a symmetric positive definite matrix\nis proposed. The resulting factorized sparse approximate inverse is used as a\npreconditioner for solving symmetric positive definite linear systems of\nequations by using the preconditioned conjugate gradient algorithm. Some\nnumerical experiments on test matrices from the Harwell-Boeing collection for\ncomparing the numerical performance of the presented method with one available\nwell-known algorithm are also given."}, "authors": ["Davod Khojasteh Salkuyeh", "Faezeh Toutounian"], "author_detail": {"name": "Faezeh Toutounian"}, "author": "Faezeh Toutounian", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.apnum.2008.07.002", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0807.3644v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0807.3644v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "15 pages, 1 figure", "arxiv_primary_category": {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "65F10, 65F50", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0807.3644v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0807.3644v1", "journal_reference": null, "doi": "10.1016/j.apnum.2008.07.002", "fulltext": "A Sparse-Sparse Iteration for Computing a Sparse\nIncomplete Factorization of the Inverse of an SPD Matrix\n\narXiv:0807.3644v1 [math.NA] 23 Jul 2008\n\nDavod Khojasteh Salkuyeh1 and Faezeh Toutounian2\n1\n\n2\n\nDepartment of Mathematics, University of Mohaghegh Ardabili,\nP. O. Box. 56199-11367, Ardabil, Iran\nE-mail: khojaste@uma.ac.ir\nSchool of Mathematical Sciences, Ferdowsi University of Mashhad,\nP. O. Box. 1159-91775, Mashhad, Iran\nE-mail: toutouni@math.um.ac.ir\n\nAbstract: In this paper, a method via sparse-sparse iteration for computing a\nsparse incomplete factorization of the inverse of a symmetric positive definite matrix is proposed. The resulting factorized sparse approximate inverse is used as\na preconditioner for solving symmetric positive definite linear systems of equations by using the preconditioned conjugate gradient algorithm. Some numerical\nexperiments on test matrices from the Harwell-Boeing collection for comparing\nthe numerical performance of the presented method with one available well-known\nalgorithm are also given.\nAMS Subject Classification : 65F10, 65F50.\n\nKeywords: Sparse matrices; Factorized sparse approximate inverse; Preconditioning; Krylov subspace methods; Symmetric positive definite; Preconditioned CG\nalgorithm\n\n1. Introduction\nConsider the nonsingular linear system of equations\nAx = b,\n\n(1)\n\nwhere the coefficient matrix A \u2208 Rn\u00d7n is large, sparse and x, b \u2208 Rn . It is wellknown that the rate of convergence of iterative methods such as Krylov subspace\nmethods for solving (1) is strongly influenced by the spectral properties of A.\n1\n\n\fHence, iterative methods usually involve a second matrix that transforms the coefficient matrix into one with a more favorable spectrum. The transformation\nmatrix is called a preconditioner. If M is a nonsingular matrix that approximates\nthe inverse of A (M \u2248 A\u22121 ), then the transformed linear system\nAMy = b,\n\nx = My,\n\n(2)\n\nwill have the same solution as system (1), but the convergence rate of iterative\nmethods applied to (2) may be higher. System (2) is preconditioned from the\nright, but left preconditioning is also possible, i.e., MAx = Mb. One can also\ndefine split-preconditioned systems. Let us assume that A has the LU factorization\nand\nM = MU ML , where MU \u2248 U \u22121 and ML \u2248 L\u22121 ,\n(3)\nwhere L and U are the lower and upper triangular factors of A. This type of\npreconditioning is known as factorized approximate inverses and MU and ML are\ncalled approximate inverse factors of A. Here, the transformed linear system can\nbe considered as follows\nMU AML y = MU b, x = ML y.\n\n(4)\n\nSystem (4) is called a split-preconditioned system.\nIn this paper we focus our attention on the computation of sparse approximate\ninverse factors of a matrix. There are different ways to compute sparse approximate\ninverse factors of a matrix and each of them has its own advantages and disadvantages. In [5, 7], the AINV method was proposed which is based on an algorithm\nwhich computes two sets of vectors {zi }ni=1 and {wi }ni=1 which are A-biconjugate,\ni.e., such that wiT Azj = 0 if and only if i 6= j. Although the construction phase\nfor the original AINV algorithm is sequential, its application is highly parallel,\nsince it consists of matrix-vector products. A fully parallel AINV algorithm can\nbe achieved by means of graph partitioning (see [2, 4, 8]). For symmetric positive\ndefinite (SPD) matrices, there exists a variant of the AINV method, denoted by\nSAINV (for Stabilized AINV), that is breakdown-free [3]. Another approach which\nwas proposed by Kolotilina and Yeremin is the FSAI algorithm [11, 12]. They\nassume that A is SPD and then construct factorized sparse approximate inverse\npreconditioners which are also SPD. Each factor implicitly approximates the inverse of the lower triangular Cholesky factor of A. This method can be easily\n2\n\n\fextended to the nonsymmetric case. The FSAI algorithm is inherently parallel but\nits main disadvantage is the need to prescribe the sparsity of approximate inverse\nfactors in advance.\nIn this paper, we first propose an iterative method for solving SPD linear systems of equations, and then, by exploiting this method, we develop an algorithm\nfor computing an incomplete factorization of the inverse of an SPD matrix. The\nresulting factorized sparse approximate inverse is used as an explicit preconditioner\nfor the solution of Ax = b by the preconditioned conjugate gradient (PCG) method.\nThroughout the paper kzkA stands for the A-norm of any vector z, i.e., kzkA =\n(Az, z)1/2 . We will denote the largest and smallest eigenvalues of the matrix X by\n\u03bbmax (X) and \u03bbmin (X), respectively.\nThis paper is organized as follows. In section 2, we introduce an approach for\ncomputing a sparse approximate solution of an SPD linear system of equations.\nSection 3 is devoted to computing an incomplete factorization of the inverse of an\nSPD matrix. Numerical experiments are given in section 4. Finally, we give some\nconcluding remarks in section 5.\n\n2. Sparse approximate solution of an SPD linear system of equations\nIn this section, we first present an approach, based on the projection method,\nfor solving an SPD linear system of equations. Then we develop an algorithm for\ncomputing a sparse approximate solution of an SPD linear system of equations.\nLet A = (aij ) be an SPD matrix and let us consider a projection method with\nL = K = span{ei1 , ei2 , . . . , eim }, where eij is the ij -th column of identity matrix\nand m is a small natural number. Given an initial guess x of the solution of (1)\nand the residual vector r = b \u2212 Ax, the new approximation takes the form\nxnew = x + Ey,\n\n(5)\n\nfor some y \u2208 Rm , and E = [ei1 , ei2 , . . . , eim ]. The Petrov-Galerkin condition r \u2212\nAEy \u22a5 L yields\ny = (E T AE)\u22121 E T r.\n(6)\nAs known [15], this kind of update minimizes\nkx + Ee\ny \u2212 xexact kA\n3\n\n\fover all ye \u2208 Rm , where xexact is the exact solution of Ax = b. It is obvious that the\nmatrix S = E T AE is an SPD matrix of dimension m. Defining J = {i1 , i2 , . . . , im },\nthe matrix E T AE is the principal submatrix of A consisting of the rows and columns\nwhose indices are in J . This new approach for solving an SPD linear system of\nequations can be stated as follows.\nAlgorithm 1:\n1. Choose an initial guess x and compute r = b \u2212 Ax\n2. Until convergence, Do\n3.\n\nSelect J = {i1 , i2 , . . . , im } \u2286 {1, 2, . . . , n}\nand E := [ei1 , ei2 , . . . , eim ]\n\n4.\n\nSolve (E T AE)y = E T r for y\n\n5.\n\nCompute x := x + Ey\n\n6.\n\nCompute r := r \u2212 AEy\n\n7. EndDo\nStep 6 of this algorithm can be written as\nX\nyk a:,k ,\nr := r \u2212\n\n(7)\n\nk\u2208J\n\nwhere a:,k is the k-th column of A and y = [y1 , y2 , . . . , ym ]T . The relation (7) shows\nthat, for updating r, we need m sparse SAXPY operations (a SAXPY operation\nis defined as z := x + \u03b1y, where x and y are n-vectors and \u03b1 is a scalar). The\nfollowing theorem regarding the convergence rate of the Algorithm 1 can be stated.\nTheorem 1. Let A be a symmetric positive definite matrix. Assume that, at each\nprojection step, the selected index set {i1 , i2 , . . . , im } contains the indices of the m\ncomponents with largest absolute value in the current residual vector r = b \u2212 Ax.\nThen\nP\nr2\n2\n2\nkdkA \u2212 kdnew kA \u2265 P k\u2208J k ,\n(8)\nk\u2208J akk\n4\n\n\fand\n\nP\n2\n\u03bbmin (A)\nk\u2208J rk 1/2\nkdnew kA \u2264 (1 \u2212 ( P\n)( Pn\n)) kdkA ,\n2\nk=1 rk\nk\u2208J akk\n\n(9)\n\nwhere dnew = A\u22121 b \u2212 xnew and d = A\u22121 b \u2212 x. Relation (9) shows that Algorithm 1\nconverges for any initial guess.\nProof. We start by observing that dnew = d \u2212 Ey, Ad = r, and\n(Adnew , dnew ) = (Ad, d) \u2212 (y, E T r).\nFrom (6) and using the Courant-Fisher min-max theorem [1, 15], we have\n\nand\n\n(y, E T r) = ((E T AE)\u22121 E T r, E T r)\nkE T rk22\n\u2265\n\u03bbmax (E T AE)\nkE T rk22\n\u2265 P\n,\nk\u2208J akk\nkrk22\n(Ad, d) = (r, A r) \u2264\n.\n\u03bbmin (A)\n\u22121\n\nFrom these observations the desired results immediately\nfollow.\nRelation (9) esP\nPn\n2\n2\ntablishes the convergence\nof\nthe\nmethod,\nsince\nr\n\u2264\nr\nk\u2208J k\nk=1 k and \u03bbmin (A) \u2264\nP\nT\n\u03bbmin (E AE) \u2264 k\u2208J akk (see [1]).\n\u0003\nThis theorem not only shows the convergence of the algorithm but also the rate\nof the reduction in the square of the A-norm of the error (Eq. (8)). In fact, the\nindices ij , j = 1, . . . , m are chosen in such a way that the reduction in the square\nof the A-norm of the\ndiagonally\nPerror is as large as possible. If A is a symmetric\nP\nscaled matrix then k\u2208J akk = m and in the Eqs. (8) and (9), k\u2208J akk may be\nreplaced by m.\nNow, by using Algorithm 1, we propose an algorithm to compute a sparse\napproximate solution of an SPD linear system of equations. In this algorithm\nno dropping strategy is needed and sparsity of the solution is preserved only by\nspecifying the maximum number of its nonzero entries, lf il, in advance. In each\n5\n\n\fiteration at most m (m \u226a n) entries are added to the current approximate solution.\nThis algorithm can be stated as follows.\nAlgorithm 2 : Sparse approximate solution to the SPD system Ax = b\n1. Set x := 0 and r := b\n2. While k rk > eps and nnz(x) < lf il Do\n3.\n\nSelect the indices of m components with largest absolute value in the\ncurrent residual vector r, i.e., J = {i1 , i2 , . . . , im } \u2286 {1, 2, . . . , n}\nand set E := [ei1 , ei2 , . . . , eim ]\n\n4.\n\nSolve (E T AE)y = E T r for y\n\n5.\n\nCompute x := x + Ey\n\n6.\n\nCompute r := r \u2212 AEy\n\n7. EndDo\nThe vector x computed by Algorithm 2 has at most lf il nonzero entries. In\npractical implementations of Algorithm 2 the number m is usually chosen to be\ntoo small, for example m = 1, 2 or 3. Throughout this paper we take m = 2. The\nparameter eps is used for stopping the process when the residual norm is small\nenough. As can be seen, in Algorithm 2 no dropping strategy is used and in each\nstep of the algorithm, according to Theorem 1, the A-norm of the error is reduced.\n\n3. Approximate inverse factors of a matrix via sparse-sparse iterations\nIn this section, for computing a sparse factorized approximate inverse of an SPD\nmatrix, we combine Algorithm 2 of section 2 with the AIB (Approximate Inverse\nvia Bordering) algorithm proposed by Saad in [15]. We first give a brief description\nof the AIB algorithm for symmetric matrices.\nIn the AIB algorithm, the sequence of matrices\n\u0013\n\u0012\nAk vk\n,\n(10)\nAk+1 =\nvkT \u03b1k+1\n6\n\n\fis made in which An = A. If the inverse factor Uk is available for Ak , i.e.,\nUkT Ak Uk = Dk ,\nthen the inverse factor Uk+1 for Ak+1 will be obtained by writing\n\u0013\n\u0013 \u0012\n\u0013\u0012\n\u0013\u0012\n\u0012 T\nDk\n0\nUk \u2212zk\nAk vk\nUk 0\n,\n=\n0 \u03b4k+1\n0\n1\nvkT \u03b1k+1\n\u2212zkT 1\n\n(11)\n\n(12)\n\nin which\nAk zk = vk ,\n\n(13)\n\n\u03b4k+1 = \u03b1k+1 \u2212 zkT vk .\n\n(14)\n\nRelation (14) can be exploited if the system (13) is solved exactly. Otherwise we\nshould use\n\u03b4k+1 = \u03b1k+1 \u2212 vkT zk \u2212 zkT (vk \u2212 Ak zk )\n= \u03b1k+1 \u2212 vkT zk \u2212 zkT rk\n= \u03b1k+1 \u2212 zkT (vk + rk ),\n\n(15)\n\ninstead of (14), where rk = vk \u2212 Ak zk . Starting from k = 1, this procedure suggests\nan algorithm for computing the inverse factors of A. If a sparse approximate\nsolution of (13) is computed, then an approximate factorization of A\u22121 is obtained.\nTo do this, we use Algorithm 2 of section 2. This scheme can be summarized as\nfollows.\nAlgorithm 3. AIB algorithm\n1. Set A1 = [a11 ], U1 = [1] and \u03b41 = a11\n2. For k = 1, . . . , n \u2212 1 Do: (in parallel)\n3.\n\nCompute a sparse approximate solution to Ak zk = vk ,\nby using Algorithm 2, and the residual rk = vk \u2212 Ak zk .\n\n4.\n\nCompute \u03b4k+1 = \u03b1k+1 \u2212 zkT (vk + rk ).\n\n5.\n\nForm Uk+1 and Dk+1\n\n6. EndDo.\n7. Set U := Un and D := Dn\n7\n\n\fThis algorithm returns U and D such that U T AU \u2248 D. The following theorem\nshows that \u03b4k+1 is always positive, independently of the accuracy with which the\nsystem (13) is solved.\nTheorem 2. Let A be an SPD matrix. Then, the scalar \u03b4k+1 computed in step 4\nof Algorithm 3 is positive.\nProof. Let rk be the residual obtained in step 3 of the AIB algorithm, i.e.,\nrk = vk \u2212 Ak zk .\nHence, we have\nzk = A\u22121\nk (vk \u2212 rk ).\nBy a little computation one can see that\nT \u22121\nT \u22121\n\u03b4k+1 = \u03b1k+1 \u2212 vkT A\u22121\nk vk + rk Ak rk = s + rk Ak rk ,\n\nwhere s = \u03b1k+1 \u2212 vkT A\u22121\nk vk \u2208 R is the Schur complement of Ak+1 and is a positive\nreal number (see Theorem 3.9 in [1]). So, the scalar \u03b4k+1 is positive, since A is an\nSPD matrix and rkT A\u22121\n\u0003\nk rk > 0 for rk 6= 0.\nHence the AIB algorithm is well-defined for SPD matrices.\n4. Numerical examples\nAll the numerical experiments presented in this section were computed in double\nprecision using Fortran PowerStation version 4.0 on a Pentium 4 PC, with a 3.06\nGHz CPU and 1.00GB of RAM.\nFor the first set of the numerical experiments, we used nine SPD matrices (BCSSTK* and S*RMT3M*) from the Matrix-Market website [14] and three matrices\n(EX15, MSC04515 and KUU ) from Tim Davis's collection [10]. These matrices\nwith their generic properties are given in Table 1. For each matrix, the problem\nsize n and the number of nonzero entries in the lower triangular part nnz are provided. In last two columns, the number of iterations (iters) and time required to\nsolve the linear system using the conjugate gradient method without any scaling\nare given. The time was measured with the function etime() and given in seconds.\nThe stopping criterion\nkb \u2212 Axi k2\n< 10\u22128 ,\nkbk2\n8\n\n\fTable 1: First set of test problems information.\nmatrix\nBCSSTK11\nBCSSTK13\nBCSSTK15\nBCSSTK21\nBCSSTK38\nS1RMT3M1\nS2RMT3M1\nS3RMT3M1\nS3RMT3M3\nMSC04515\nEX15\nKUU\n\nn\n1473\n2003\n3948\n3600\n8032\n5489\n5489\n5489\n5357\n4515\n6867\n7102\n\nnnz\n17857\n42943\n60882\n15100\n181746\n112505\n112505\n112505\n106526\n51111\n52769\n173651\n\ntime\n29.07\n15.32\n24.13\n15.12\n6.48\n3.84\n\niters\n\u2020\n\u2020\n9219\n7805\n\u2020\n4953\n\u2020\n\u2020\n\u2020\n4728\n1506\n550\n\nwas used and the initial guess was taken to be the zero vector. For all the examples,\nthe right hand side of each system was taken such that the exact solution is a vector\nwith random entries uniformly distributed in (0, 1). No significant differences were\nobserved for other choices of the right hand side vector. The maximum number of\niterations was 10000. In all the tables a dagger (\u2020) indicates no convergence of the\niterative method.\nWe compare the numerical results of the new preconditioner with that of the\nSAINV preconditioner. The AINV and the SAINV algorithms have been widely\ncompared with other preconditioning techniques, showing that they are the most\neffective algorithms for computing a sparse incomplete factorization of the inverse\nof a matrix [2, 3, 5, 6, 7]. For the SAINV algorithm we used the SAINV code\nof the SPARSLAB software provided by Tuma 1 with drop tolerance \u03c4 = 0.1.\nThis drop tolerance is very often the right one based on the numerical results\nreported in several papers. For Algorithm 2, we used the parameters eps = 0.01\nand lf il = 10. We also used a parameter lf il such that the number of nonzero\nentries in the incomplete U factor divided by the number of nonzero entries in\nthe upper triangular part of A, \u03c1, is approximately equal to or less than that of\n1\n\nhttp://www.cs.cas.cz/\u223ctuma/sparslab.html\n\n9\n\n\fTable 2: Setup time to compute sparse approximate inverse factors and results for\nthe split-preconditioned CG algorithm.\nAlgorithm 3\nmatrix\nBCSSTK11\nBCSSTK13\nBCSSTK15\nBCSSTK21\nBCSSTK38\nS1RMT3M1\nS2RMT3M1\nS3RMT3M1\nS3RMT3M3\nMSC04515\nEX15\nKUU\n\nSAINV Algorithm\n\nlf il\n\n\u03c1\n\nP-Its\n\nP-time\n\nIt-time\n\nT-time\n\n\u03c1\n\nP-Its\n\nP-time\n\nIt-time\n\nT-time\n\n13\n10\n29\n10\n9\n10\n6\n10\n11\n10\n15\n10\n15\n10\n15\n10\n15\n10\n13\n10\n20\n10\n8\n10\n\n0.58\n0.45\n0.71\n0.26\n0.35\n0.37\n0.97\n1.48\n0.28\n0.25\n0.41\n0.29\n0.41\n0.28\n0.36\n0.26\n0.37\n0.26\n0.65\n0.52\n1.11\n0.65\n0.19\n0.23\n\n628\n650\n343\n550\n504\n491\n246\n164\n559\n572\n244\n318\n526\n585\n1298\n1458\n984\n1087\n712\n809\n601\n511\n141\n131\n\n0.23\n0.17\n0.65\n0.10\n0.19\n0.20\n0.09\n0.13\n0.70\n0.64\n0.45\n0.32\n0.58\n0.34\n0.65\n0.33\n0.76\n0.31\n0.28\n0.22\n0.81\n0.34\n0.38\n0.42\n\n1.20\n1.19\n1.42\n1.80\n2.67\n2.73\n0.73\n0.52\n7.72\n7.77\n2.28\n2.77\n4.97\n5.11\n11.89\n12.55\n8.58\n8.95\n4.16\n4.47\n4.88\n3.53\n1.70\n1.67\n\n1.43\n1.36\n2.07\n1.90\n2.86\n2.93\n0.82\n0.65\n8.42\n8.41\n2.73\n3.09\n5.55\n5.45\n12.54\n12.88\n9.34\n9.26\n4.44\n4.69\n5.69\n3.87\n2.08\n2.09\n\n0.58\n\n2099\n\n0.11\n\n3.95\n\n4.06\n\n0.72\n\n370\n\n0.63\n\n1.55\n\n2.18\n\n0.32\n\n214\n\n0.22\n\n1.11\n\n1.33\n\n1.05\n\n167\n\n0.06\n\n0.5\n\n0.56\n\n0.29\n\n1131\n\n0.81\n\n15.67\n\n16.48\n\n0.83\n\n257\n\n0.98\n\n2.97\n\n3.95\n\n1.29\n\n539\n\n1.53\n\n7.5\n\n9.03\n\n2.81\n\n5434\n\n6.33\n\n117.80\n\n124.13\n\n2.00\n\n5047\n\n3.83\n\n84.40\n\n88.23\n\n0.66\n\n1020\n\n0.20\n\n5.93\n\n6.13\n\n1.83\n\n1325\n\n0.55\n\n12.88\n\n13.43\n\n0.18\n\n144\n\n0.25\n\n1.73\n\n1.98\n\nthe SAINV preconditioner. The results of the split-preconditioned CG algorithm\n[15] in conjunction with the SAINV preconditioner and Algorithm 3 are given in\nTable 2. This table reports the density (\u03c1), the number of split-preconditioned\nCG iterations for convergence (P-Its), the setup time for the preconditioner (Ptime), the time for the split-preconditioned iterations (It-time), and T-time which\nis equal to the sum of P-time and It-time. Numerical results presented in this\ntable show that both algorithms are robust and the new method is better than\nthe SAINV algorithm for 9 out of 12 problems, especially on the shell problems\n(S3RMT3M1 and S3RMT3M3). The results of this table also indicate that the\nparameters eps = 0.01 and lf il = 10 give good results.\nIn Table 3, the numerical results for matrices BCSSTK13 and BCSSTK27 with\ndifferent values of lf il are given. This table shows the effect of an increase in lf il\non the reduction of the number of the iterations for convergence. The results of\nthis table also indicate that the choices eps = 0.01 and lf il = 10 lead to good\nresults.\nIn [3], the numerical results of the SAINV preconditioner in conjunction with\nsome preliminary transformations operated on the coefficient matrix such as symmetric diagonal scaling, reordering with the multiple minimum degree (MMD) al10\n\n\fTable 3: Results for matrices BCSSTK13 and BCSSTK21 with different values of\nlf il\nlf il\n2\n4\n6\n8\n10\n12\n14\n16\n\nBCSSTK13\nP-time It-time\n0.03\n3.02\n0.05\n2.77\n0.06\n2.48\n0.08\n2.19\n0.11\n1.81\n0.14\n1.69\n0.17\n1.84\n0.22\n1.78\n\nP-Its\n1039\n917\n793\n685\n550\n514\n529\n502\n\nBCSSTK21\nP-time It-time\n0.06\n0.81\n0.08\n0.75\n0.09\n0.72\n0.10\n0.61\n0.11\n0.53\n0.14\n0.50\n0.17\n0.50\n0.20\n0.47\n\nP-Its\n316\n270\n246\n198\n164\n148\n142\n125\n\ngorithm [13] and diagonally compensated reduction of positive off-diagonal entries\n(DCR), were given. The authors have concluded that the SAINV preconditioner in\nconjunction with symmetric diagonal scaling and reordering with MMD (J-MMDSAINV) is often the best choice between the variants of the preconditioners used\nin [3]. In continuation, we use 14 out of 16 matrices used in [3] for the numerical\nexperiments. Most of these matrices can be extracted from the Matrix Market website. The exceptions are NASA2910 and NASA4704 which can be downloaded from\nTim Davis's collection, and the SMT matrix, which was provided by R. Kouhia of\nthe Helsinki University of Technology 2 . In Table 4, we give the numerical results\nof the new preconditioner in conjunction with symmetric diagonal scaling (J-N-M)\nand the J-MMD-SAINV preconditioner. Results of the J-MMD-SAINV preconditioner and the number of iterations of the conjugate gradient algorithm with Jacobi\npreconditioning (JCG-Its) were extracted from Table 7 and Table 1 in [3], respectively. It is necessary to mention that the parameter lf il was chosen such that the\nparameter \u03c1 of the new preconditioner is less than or approximately equal to that\nof the SAINV preconditioner. All assumptions and notations are as before.\nTable 4 shows that the new preconditioner is better than the J-MMD-SAINV\npreconditioner for 9 out of 14 matrices presented in this table.\nThe last section of numerical experiments is devoted to some large matrices\n2\n\nhttp://users.tkk.fi/\u223ckouhia/sparse.html\n\n11\n\n\fTable 4: Numerical results of the new method in conjunction with symmetric\ndiagonal scaling and the J-MMD-SAINV preconditioner.\nJ-N-M\n\nJ-MMD-SAINV\n\nmatrix\n\nn\n\nnnz\n\nlf il\n\n\u03c1\n\nP-Its\n\nP-time\n\nIt-time\n\nT-time\n\n\u03c1\n\nP-Its\n\nJCG-Its\n\nBCSSTK13\nBCSSTK14\nBCSSTK15\nBCSSTK16\nBCSSTK17\nBCSSTK18\nBCSSTK21\nBCSSTK25\nS1RMQ4M1\nS2RMQ4M1\nS3RMQ4M1\nNASA2910\nNASA4704\nSMT\n\n2003\n1806\n3948\n4884\n10974\n11948\n3600\n15439\n5489\n5489\n5489\n2910\n4704\n25710\n\n42943\n32630\n60882\n147631\n219812\n80519\n15100\n133840\n143300\n143300\n143300\n88603\n54730\n1889447\n\n17\n9\n11\n6\n16\n8\n10\n10\n8\n10\n13\n20\n20\n15\n\n0.38\n0.28\n0.32\n0.12\n0.40\n0.57\n1.46\n0.59\n0.19\n0.23\n0.24\n0.32\n0.85\n0.11\n\n275\n83\n176\n95\n653\n515\n179\n1614\n247\n403\n569\n262\n567\n734\n\n0.17\n0.06\n0.17\n0.20\n1.19\n0.78\n0.11\n1.45\n0.27\n0.33\n0.36\n0.41\n0.47\n6.28\n\n0.94\n0.23\n0.95\n0.89\n12.03\n5.73\n0.58\n26.86\n2.41\n3.98\n5.66\n1.64\n3.77\n74.83\n\n1.11\n0.29\n1.12\n1.09\n13.22\n6.51\n0.69\n28.31\n2.68\n4.31\n6.02\n2.05\n4.24\n81.11\n\n0.39\n0.27\n0.33\n0.12\n0.40\n0.58\n1.51\n0.57\n0.20\n0.25\n0.24\n0.95\n0.91\n0.11\n\n349\n73\n167\n98\n711\n261\n191\n1512\n248\n528\n1140\n341\n1176\n546\n\n1406\n409\n518\n191\n2522\n1120\n559\n\u2020\n692\n1529\n6884\n1350\n4866\n1984\n\nTable 5: Numerical results of the new method in conjunction with symmetric\ndiagonal scaling.\nNew method with symmetric diagonal scaling\n\nJCG\n\nmatrix\n\nn\n\nnnz\n\nlf il\n\n\u03c1\n\nP-Its\n\nP-time\n\nIt-time\n\nT-time\n\nIts\n\nTime\n\nGRIDGENA\n\n48962\n\n280523\n\n199984\n\n311492\n\n79.56\n\n1605669\n\n41.21\n41.32\n41.81\n56.71\n52.80\n49.12\n53.64\n49.84\n49.96\n268.79\n250.97\n238.11\n\n1796\n\n123440\n\n29.85\n29.34\n27.89\n45.33\n41.00\n36.84\n24.20\n19.48\n18.52\n200.16\n179.8\n164.23\n\n90.95\n\n80800\n\n11.36\n11.98\n13.92\n11.38\n11.80\n12.28\n29.44\n30.36\n31.44\n68.63\n71.17\n73.88\n\n3330\n\nAPACHE1\n\n609\n540\n464\n1053\n886\n750\n325\n245\n218\n1002\n850\n717\n\n48.61\n\n50000\n\n1.07\n1.52\n2.00\n1.14\n1.50\n1.79\n1.43\n1.80\n2.22\n0.45\n0.65\n0.85\n\n1720\n\nCVXBQP1\n\n10\n15\n20\n10\n15\n20\n10\n15\n20\n10\n15\n20\n\n3870\n\n368.91\n\nCF2D2\n\nextracted from Tim Davis's collection. Numerical results with different values of\nlf il and with eps = 0.01 are given in Table 5. As can be seen, the new preconditioner in conjunction with symmetric diagonal scaling furnishes good results for\nlarge matrices.\nWe end this section by giving the numerical results for the matrix APACHE2 extracted from Tim Davis's collection. This is a large matrix of dimension n = 715176\nwith nnz = 2776523 nonzero entries in the lower part. The conjugate gradient\nmethod in conjunction with symmetric diagonal scaling converges in 2482 iterations. The conjugate gradient method in conjunction with the new preconditioner\nand symmetric diagonal scaling with lf il = 5 (\u03c1 = 0.98) and lf il = 10 (\u03c1 = 1.52)\nconverges in 839 and 575 iterations, respectively. Convergence history of these\n12\n\n\f0\n\n\u22121\n\nlfil=10, \u03c1=1.52\nlfil=5, \u03c1=0.98\nwithout preconditioning\n\n\u22122\n\n\u22123\n\n\u22124\n\n\u22125\n\n\u22126\n\n\u22127\n\n\u22128\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\nFigure 1: Convergence history of the new preconditioner for the matrix APACHE2.\nmethods is displayed in Figure 1.\nNumerical results for the matrix APACHE2 and matrices in Table 5 (and previous tables) show that the new preconditioner reduces the number of iterations by\nabout a factor of three. This is true for the J-MMD-SAINV preconditioner based\nupon a conclusion reported in ( [3], page 1328).\n5. Conclusion and future work\nWe have proposed an approach for computing a sparse incomplete factorization of the inverse of an SPD matrix. The resulting factorized sparse approximate\ninverse was used as a preconditioner for solving symmetric positive definite linear\nsystems of equations by using the conjugate gradient algorithm. The new preconditioner does not need to specify the sparsity pattern of the inverse factor in\nadvance. For preserving sparsity it is enough to specify two parameters eps and\nlf il. Numerical results show that eps = 0.01 and lf il = 10 usually give good\nresults. Our numerical results also show that the proposed method in conjunction\nwith the symmetric diagonal scaling is somewhat better than the J-MMD-SAINV\npreconditioner.\nThe new preconditioner is suitable for parallel computers. It can also be implemented for normal equations with a little revision.\n13\n\n\fFuture work may focus on extending the proposed preconditioner to general matrices and studying the effect of different reordering techniques on the convergence\nrate.\n6. Acknowledgments\nThe authors are gratefully indebted to Edmond Chow for carefully reading of\nan earlier draft of this paper and giving several valuable comments.\n\nReferences\n[1] O. Axelsson, Iterative solution methods, Cambridge University Press, Cambridge, 1996.\n[2] M. Benzi, Preconditioning techniques for large linear systems: A survey, J. of\nComputational Physics, 182 (2002) 418-477.\n[3] M. Benzi, J. K. Cullum, and M. Tuma, and C. D. Meyer, Robust approximate\ninverse preconditioning for the conjugate gradient Method, SIAM J. Sci. Comput., 22 (2000) 1318-1332.\n[4] M. Benzi, J. Marin and M. Tuma A Two-Level Parallel Preconditioner Based on\nSparse Approximate Inverses, in Iterative Methods in Scientific Computation\nIV, D. R. Kincaid and A. C. Elster, eds., IMACS Series in Computational and\nApplied Mathematics, Vol. 5, IMACS, New Brunswick, NJ (1999), pp. 167-178.\n[5] M. Benzi, C. D. Meyer, and M. Tuma, A sparse approximate inverse preconditioner for the conjugate gradient method, SIAM J. Sci. Comput., 17 (1996)\n1135-1149.\n[6] M. Benzi, M. Tuma, A comparative study of sparse approximate inverse preconditioners, Applied Numerical Mathematics, 30 (1999)305-340.\n[7] M. Benzi, M. Tuma, A sparse approximate inverse preconditioner for nonsymmetric linear systems, SIAM J. Sci. Comput., 19 (1998) 968-994.\n[8] M. Benzi and M. Tuma, A parallel solver for large-scale Markov chains, Appl.\nNumer. Math., 41(2002) 305-340.\n14\n\n\f[9] B. N. Datta, Numerical Linear Algebra and Applications, Brooks Cole Publishing Company, 1995.\n[10] T. Davis, University of Florida sparse matrix collection, NA Digest, 92(1994),\nhttp://www.cise.ufl.edu/research/sparse/matrices.\n[11] L. Y. Kolotilina and A. Y. Yeremin, Factorized sparse approximate inverse\npreconditioning I. Theory, SIAM J. Matrix Anal. Appl., 14 (1993) 45-58.\n[12] L. Y. Kolotilina and A. Y. Yeremin, Factorized sparse approximate inverse\npreconditioning II: Solution of 3D FE systems on massively parallel computers,\nInt. J. High Speed Comput., 7 (1995) 191-215.\n[13] J. W. H. Liu, Modification of the minimum degree algorithm by multiple elimination, ACM Trans. Math. Software, 11(1985)141-153.\n[14] Matrix Market, http://math.nist.gov/MatrixMarket (August 2005).\n[15] Y. Saad, Iterative Methods for Sparse linear Systems, PWS press, New York,\n1995.\n\n15\n\n\f"}