{"id": "http://arxiv.org/abs/math/0610181v1", "guidislink": true, "updated": "2006-10-05T12:35:29Z", "updated_parsed": [2006, 10, 5, 12, 35, 29, 3, 278, 0], "published": "2006-10-05T12:35:29Z", "published_parsed": [2006, 10, 5, 12, 35, 29, 3, 278, 0], "title": "Parallel and interacting Markov chains Monte Carlo method", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0610765%2Cmath%2F0610155%2Cmath%2F0610512%2Cmath%2F0610895%2Cmath%2F0610595%2Cmath%2F0610911%2Cmath%2F0610457%2Cmath%2F0610679%2Cmath%2F0610084%2Cmath%2F0610825%2Cmath%2F0610768%2Cmath%2F0610077%2Cmath%2F0610209%2Cmath%2F0610659%2Cmath%2F0610771%2Cmath%2F0610353%2Cmath%2F0610781%2Cmath%2F0610470%2Cmath%2F0610179%2Cmath%2F0610585%2Cmath%2F0610472%2Cmath%2F0610060%2Cmath%2F0610135%2Cmath%2F0610124%2Cmath%2F0610181%2Cmath%2F0610379%2Cmath%2F0610614%2Cmath%2F0610025%2Cmath%2F0610708%2Cmath%2F0610883%2Cmath%2F0610432%2Cmath%2F0610340%2Cmath%2F0610879%2Cmath%2F0610786%2Cmath%2F0610491%2Cmath%2F0610634%2Cmath%2F0610752%2Cmath%2F0610681%2Cmath%2F0610073%2Cmath%2F0610889%2Cmath%2F0610145%2Cmath%2F0610529%2Cmath%2F0610210%2Cmath%2F0610910%2Cmath%2F0610399%2Cmath%2F0610148%2Cmath%2F0610418%2Cmath%2F0610156%2Cmath%2F0610687%2Cmath%2F0610486%2Cmath%2F0610286%2Cmath%2F0610152%2Cmath%2F0610969%2Cmath%2F0610501%2Cmath%2F0610232%2Cmath%2F0610494%2Cmath%2F0610841%2Cmath%2F0610952%2Cmath%2F0610318%2Cmath%2F0610117%2Cmath%2F0610854%2Cmath%2F0610483%2Cmath%2F0610321%2Cmath%2F0610371%2Cmath%2F0610235%2Cmath%2F0610845%2Cmath%2F0610621%2Cmath%2F0610559%2Cmath%2F0610892%2Cmath%2F0610217%2Cmath%2F0610697%2Cmath%2F0610692%2Cmath%2F0610819%2Cmath%2F0610972%2Cmath%2F0610448%2Cmath%2F0610823%2Cmath%2F0610716%2Cmath%2F0610886%2Cmath%2F0610123%2Cmath%2F0610382%2Cmath%2F0610104%2Cmath%2F0610064%2Cmath%2F0610760%2Cmath%2F0610813%2Cmath%2F0610284%2Cmath%2F0610088%2Cmath%2F0610449%2Cmath%2F0610799%2Cmath%2F0610601%2Cmath%2F0610323%2Cmath%2F0610377%2Cmath%2F0610265%2Cmath%2F0610857%2Cmath%2F0610988%2Cmath%2F0610099%2Cmath%2F0610641%2Cmath%2F0610569%2Cmath%2F0610343%2Cmath%2F0610129%2Cmath%2F0610433%2Cmath%2F0610479&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Parallel and interacting Markov chains Monte Carlo method"}, "summary": "In many situations it is important to be able to propose $N$ independent\nrealizations of a given distribution law. We propose a strategy for making $N$\nparallel Monte Carlo Markov Chains (MCMC) interact in order to get an\napproximation of an independent $N$-sample of a given target law. In this\nmethod each individual chain proposes candidates for all other chains. We prove\nthat the set of interacting chains is itself a MCMC method for the product of\n$N$ target measures. Compared to independent parallel chains this method is\nmore time consuming, but we show through concrete examples that it possesses\nmany advantages: it can speed up convergence toward the target law as well as\nhandle the multi-modal case.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0610765%2Cmath%2F0610155%2Cmath%2F0610512%2Cmath%2F0610895%2Cmath%2F0610595%2Cmath%2F0610911%2Cmath%2F0610457%2Cmath%2F0610679%2Cmath%2F0610084%2Cmath%2F0610825%2Cmath%2F0610768%2Cmath%2F0610077%2Cmath%2F0610209%2Cmath%2F0610659%2Cmath%2F0610771%2Cmath%2F0610353%2Cmath%2F0610781%2Cmath%2F0610470%2Cmath%2F0610179%2Cmath%2F0610585%2Cmath%2F0610472%2Cmath%2F0610060%2Cmath%2F0610135%2Cmath%2F0610124%2Cmath%2F0610181%2Cmath%2F0610379%2Cmath%2F0610614%2Cmath%2F0610025%2Cmath%2F0610708%2Cmath%2F0610883%2Cmath%2F0610432%2Cmath%2F0610340%2Cmath%2F0610879%2Cmath%2F0610786%2Cmath%2F0610491%2Cmath%2F0610634%2Cmath%2F0610752%2Cmath%2F0610681%2Cmath%2F0610073%2Cmath%2F0610889%2Cmath%2F0610145%2Cmath%2F0610529%2Cmath%2F0610210%2Cmath%2F0610910%2Cmath%2F0610399%2Cmath%2F0610148%2Cmath%2F0610418%2Cmath%2F0610156%2Cmath%2F0610687%2Cmath%2F0610486%2Cmath%2F0610286%2Cmath%2F0610152%2Cmath%2F0610969%2Cmath%2F0610501%2Cmath%2F0610232%2Cmath%2F0610494%2Cmath%2F0610841%2Cmath%2F0610952%2Cmath%2F0610318%2Cmath%2F0610117%2Cmath%2F0610854%2Cmath%2F0610483%2Cmath%2F0610321%2Cmath%2F0610371%2Cmath%2F0610235%2Cmath%2F0610845%2Cmath%2F0610621%2Cmath%2F0610559%2Cmath%2F0610892%2Cmath%2F0610217%2Cmath%2F0610697%2Cmath%2F0610692%2Cmath%2F0610819%2Cmath%2F0610972%2Cmath%2F0610448%2Cmath%2F0610823%2Cmath%2F0610716%2Cmath%2F0610886%2Cmath%2F0610123%2Cmath%2F0610382%2Cmath%2F0610104%2Cmath%2F0610064%2Cmath%2F0610760%2Cmath%2F0610813%2Cmath%2F0610284%2Cmath%2F0610088%2Cmath%2F0610449%2Cmath%2F0610799%2Cmath%2F0610601%2Cmath%2F0610323%2Cmath%2F0610377%2Cmath%2F0610265%2Cmath%2F0610857%2Cmath%2F0610988%2Cmath%2F0610099%2Cmath%2F0610641%2Cmath%2F0610569%2Cmath%2F0610343%2Cmath%2F0610129%2Cmath%2F0610433%2Cmath%2F0610479&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In many situations it is important to be able to propose $N$ independent\nrealizations of a given distribution law. We propose a strategy for making $N$\nparallel Monte Carlo Markov Chains (MCMC) interact in order to get an\napproximation of an independent $N$-sample of a given target law. In this\nmethod each individual chain proposes candidates for all other chains. We prove\nthat the set of interacting chains is itself a MCMC method for the product of\n$N$ target measures. Compared to independent parallel chains this method is\nmore time consuming, but we show through concrete examples that it possesses\nmany advantages: it can speed up convergence toward the target law as well as\nhandle the multi-modal case."}, "authors": ["Fabien Campillo", "Vivien Rossi"], "author_detail": {"name": "Vivien Rossi"}, "author": "Vivien Rossi", "links": [{"href": "http://arxiv.org/abs/math/0610181v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0610181v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0610181v1", "affiliation": "IURC", "arxiv_url": "http://arxiv.org/abs/math/0610181v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:math/0610181v1 [math.PR] 5 Oct 2006\n\nINSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE\n\nParallel and interacting\nMarkov chains Monte Carlo method\nFabien Campillo and Vivien Rossi\n\nN \u030a????\nOctober 2006\n\nSyst\u00e8mes num\u00e9riques\n\nN 0249-6399\n\napport\nde recherche\n\n\f\fParallel and intera ting\nMarkov hains Monte Carlo method\n\nFabien Campillo \u2217 and Vivien Rossi \u2020 \u2021\nSyst\u00e8mes num\u00e9riques\nProjets Aspi\nRapport de re her he n\u0006???? \u0016 O tober 2006 \u0016 27 pages\n\nIn many situations it is important to be able to propose N independent\nrealizations of a given distribution law. We propose a strategy for making N parallel Monte\nCarlo Markov Chains (MCMC) intera t in order to get an approximation of an independent\nN -sample of a given target law. In this method ea h individual hain proposes andidates for\nall other hains. We prove that the set of intera ting hains is itself a MCMC method for the\nprodu t of N target measures. Compared to independent parallel hains this method is more\ntime onsuming, but we show through on rete examples that it possesses many advantages:\nit an speed up onvergen e toward the target law as well as handle the multi-modal ase.\nKey-words: Markov hain Monte Carlo method, Metropolis-Hastings, intera ting hains,\nparti le approximation\n\nAbstra t:\n\n(R\u00e9sum\u00e9 : tsvp)\n\nINRIA/IRISA, Rennes, Fabien.Campillo\binria.fr\nIURC, University of Montpellier I \u0015 Viven.Rossi\biur .montp.inserm.fr\n\u2021 The resear h of the se ond author was done during a postdo toral stay at the INRIA/IRISA, Rennes.\n\n\u2217\n\u2020\n\nUnit\u00e9 de recherche INRIA Rennes\nIRISA, Campus universitaire de Beaulieu, 35042 RENNES Cedex (France)\nT\u00e9l\u00e9phone : 02 99 84 71 00 - International : +33 2 99 84 71 00\nT\u00e9l\u00e9copie : 02 99 84 71 71 - International : +33 2 99 84 71 71\n\n\fM\u00e9thode de Monte Carlo par ha\u00eenes de Markov\nen parall\u00e8le et en intera tion\n\nDans de nombreuses situations il est important de pouvoir disposer de N\nr\u00e9alisations ind\u00e9pendantes d'une loi donn\u00e9e. Notre but est de d\u00e9velopper une strat\u00e9gie\nd'intera tion de N m\u00e9thodes de Monte Carlo par Cha\u00eene de Markov (MCCM) dans le but\nde proposer une approximation d'un \u00e9 hantillon ind\u00e9pendant de taille N d'une loi ible\ndonn\u00e9e. L'id\u00e9e est que haque ha\u00eene propose un andidat pour elle-m\u00eame mais \u00e9galement\npour toutes les autres ha\u00eenes. On montre que l'ensemble de es N ha\u00eenes en intera tion\nest lui-m\u00eame une m\u00e9thode MCCM pour le produit de N mesures ibles. Cette appro he est\nnaturellement plus o\u00fbteuse que N ha\u00eenes ind\u00e9pendantes, on montre toutefois au travers\nd'exemples on rets qu'elle poss\u00e8de plusieurs avantages : elle peut sensiblement a \u00e9l\u00e9rer la\nonvergen e vers la loi ible, elle permet \u00e9galement d'appr\u00e9hender le as multimodal.\nMots- l\u00e9 : m\u00e9thode de Monte Carlo par ha\u00eene de Markov, Metropolis-Hastings, ha\u00eenes\nen intera tion, approximation parti ulaire\n\nR\u00e9sum\u00e9 :\n\n\fParallel and intera ting MCMC's\n\n3\n\nContents\n1\n2\n\nIntrodu tion\n\n5\n\nParallel/intera ting MH algorithm\n\n5\n\n2.1 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2 Des ription of the MH kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.3 Invarian e property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n3\n\nParallel/intera ting MwG algorithm\n\n12\n\n4\n\nNumeri al tests\n\n19\n\nCon lusion\n\n23\n\n5\n\n3.1 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Des ription of the MH kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.3 Invarian e property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.1 A multi-modal example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.2 An hidden Markov model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\nRR n\u00060123456789\n\n\f4\n\nF. Campillo & V. Rossi\n\nINRIA\n\n\fParallel and intera ting MCMC's\n\n1\n\n5\n\nIntrodu tion\n\nMarkov hain Monte Carlo (MCMC) algorithms [19, 12, 18\u2104 allows us to draw samples from\na probability distribution \u03c0(x) dx known up to a multipli ative onstant. They onsist in\nsequentially simulating a single Markov hain whose limit distribution is \u03c0(x) dx. There exist\nmany te hniques to speed up the onvergen e toward the target distribution by improving\nthe mixing properties of the hain [13\u2104. Moreover, spe ial attention should be given to the\nonvergen e diagnosis of this method [1, 6, 15\u2104.\nAn alternative is to run many Markov hains in parallel. The simplest multiple hain\nalgorithm is to make use of parallel independent hains [9\u2104. The re ommendations on erning\nthis idea seem ontradi tory in the literature ( f. the \u0010many short runs\u0011 vs \u0010one long run\u0011\ndebate des ribed in [10\u2104). We an note with [11\u2104 and [18, \u009f 6.5\u2104 that independent parallel\nhains ould be a poor idea: among these hains some may not onverge, so one long hain\nould be preferable to many short ones. Moreover, many parallel independent hains an\narti\u001c ially exhibit a more robust behavior whi h does not orrespond to a real onvergen e\nof the algorithm.\nIn pra ti e one however make use of several hains in parallel. It is then tempting to\nex hange information between these hains to improve mixing properties of the MCMC\nsamplers [4, 5, 16, 3, 7, 8\u2104. A general framework of \u0010Population Monte Carlo\u0011 has been\nproposed in this ontext [14, 17, 2\u2104. In this paper we propose an intera ting method between\nparallel hains whi h provides an independent sample from the target distribution. Contrary\nto papers previously ited, the proposal law n our work is given and does not adapt itself to\nthe previous simulations. Hen e, the problem of the hoi e of this law still remains.\nThe Metropolis-Hastings (MH) algorithm and its theoreti al properties are presented in\nse tion 2. The orresponding Metropolis within Gibbs (MwG) algorithm and its theoreti al\nproperties are presented in se tion 3. In Se tion 4, two simple numeri al examples illustrate\nhow the introdu tion of intera tions an speed up the onvergen e and handle multi-modal\nases.\n2\n\nParallel/intera ting Metropolis Hastings (MH) algorithm\n\nConsider a target density law \u03c0(x) de\u001cned on (Rn , B(Rn)) and a proposal kernel density\n\u03c0 prop (y|x). We propose a method for sampling N independent values X 1 , . . . , X N \u2208 Rn of\nthe law \u03c0(x) dx.\nNotations:\n\nLet\n\nX = X 1:N = X1:n \u2208 Rn\u00d7N ,\n\nso that Xl \u2208 RN and X i \u2208 Rn (the same for Y and Z ); x \u2208 Rn so that xl \u2208 R (the same for\ny and z ); \u03be, \u03be \u2032 \u2208 R. Here X 1:N = (X 1 , . . . , X N ) and X1:n = (X1 , . . . , Xn ). We also de\u001cne\n\nRR n\u00060123456789\n\n\f6\n\nF. Campillo & V. Rossi\n\n\u00acl = {1, . . . , n} \\ {l}. Note that the stru ture of the matrix X is:\nXi\n\u2191\n\uf8ee\n\nX11\n\n\uf8ef ..\n\uf8ef .\n\uf8ef 1\nX= \uf8ef\n\uf8ef Xl\n\uf8ef .\n\uf8f0 ..\nXn1\n\n2.1\n\n***\n\nX1i\n\n..\n.\n\n***\n\nX1N\n\n..\n.\n\n***\n\nXli\n\n***\n\nXlN\n\n***\n\nXni\n\n***\n\nXnN\n\n..\n.\n\n..\n.\n\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\n\u2192 Xl\n\n.\n\nThe algorithm\n\nWe des ribe the Markov hain {X(k) }k\u22650 over Rn\u00d7N orresponding the MH algorithm. It\nonsists in N mutually dependent realizations Xi,(k) (i = 1, . . . , N ) of the state variable and\nits limit distribution will be\ndef\n\u03a0(dX) = \u03c0(X 1 ) dX 1 * * * \u03c0(X N ) dX N .\nWe detail an iteration X(k) = X \u2192 X(k+1) = Z of the MH algorithm. The N ve tors are\nupdated sequentially:\n[X 1:N ] \u2192 [Z 1 X 2:N ] \u2192 [Z 1:2 X 3:N ] * * * [Z 1:N \u22121 X N ] \u2192 [Z 1:N ] .\n\nAt sub-iteration \u0010 i \u0011, that is [Z 1:i\u22121 X i:N ] \u2192 [Z 1:iX i+1:N ], we simulate Z i in two steps:\nProposal step: independently one from the other, ea h hain j = 1 * * * N proposes a andidate Y j \u2208 Rn a ording to the proposal kernel starting from its urrent position,\ni.e.\nprop (y|Z 1:i\u22121 , X i , X i+1:N ) dy .\nY j \u223c \u03c0i,j\nNote that the andidates Y j depend also on i. We will use a lighter notation:\nprop (y|X i ) = \u03c0 prop (y|Z 1:i\u22121 , X i , X i+1:N ) .\n\u03c0i,j\ni,j\n\n(1)\n\nINRIA\n\n\f7\n\nParallel and intera ting MCMC's\n\nWe an hose among these N andidates Y 1:N or stay at X i a ording to\nthe multinomial law:\n\uf8f1 1\nwith probability N1 \u03b1i,1 (X i, Y 1 ) ,\nY\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 ..\n.\nZi \u2190\nN\n\uf8f4\nY\nwith probability N1 \u03b1i,N (X i , Y N ) ,\n\uf8f4\n\uf8f4\n\uf8f3\ni\nwith probability \u03c1\u0303i (X i, Y )\nX\nwhere the a eptan e probabilities are\n\nSele tion step:\n\nprop\ndef \u03c0(y) \u03c0i,j (x|y)\n\u03b1i,j (x, y) =\nprop (y|x) \u2227 1 ,\n\u03c0(x) \u03c0i,j\n\nN\n1 X i,j i j\ndef\n\u03c1\u0303i (X i , Y ) = 1 \u2212\n\u03b1 (X , Y ) .\nN j=1\n\nThe \u001cnal algorithm is depi ted in Algorithm 1.\nhoose X \u2208 Rn\u00d7N\n\nfor k = 1, 2, . . . do\nfor i = 1 : N do\nfor j = 1 : N do\n\nprop (y|X i ) dy\nY j \u223c \u03c0i,j\nprop (X i |Y j )]/[\u03c0(X i ) \u03c0 prop (Y j |X i )] \u2227 1\nj\n\u03b1 \u2190 [\u03c0(Y j ) \u03c0i,j\ni,j\n\nend for\nPN\n\u03c1\u0303 \u2190 1 \u2212 N1 j=1 \u03b1j\n\uf8f1 1\nwith\nY\n\uf8f4\n\uf8f4\n\uf8f2 ..\n.\nXi \u2190\nN\nY\nwith\n\uf8f4\n\uf8f4\n\uf8f3 i\nX\nwith\n\nend for\nend for\n\nAlgorithm 1:\n2.2\n\nprobability \u03b1N /N\nprobability \u03c1\u0303\n\nParallel/intera ting MH algorithm.\n\nDes ription of the MH kernel\n\nLemma 2.1\n\nis\n\nprobability \u03b11 /N\n\nThe Markov kernel asso iated with the MH pro edure des ribed in Se tion 2.1\n\ndef\nP (X; dZ) = P 1 (X 1:N ; dZ 1 ) P 2 (Z 1 , X 2:N ; dZ 2 ) * * * P N (Z 1:N \u22121 , X N ; dZ N )\n\nRR n\u00060123456789\n\n(2)\n\n\f8\n\nF. Campillo & V. Rossi\n\nwhere\nN\ndef 1 X i,j i\nprop (z|X i) dz + \u03c1i (X i ) \u03b4 i (dz) .\n\u03b1 (X , z) \u03c0i,j\nP i (Z 1:i\u22121 , X i:N ; dz) =\nX\nN j=1\n\n(3)\n\nA eptation probability is\n\ndef\n\u03b1i,j (x, z) =\n\n\u001a\n\nri,j (x, z) \u2227 1 if (x, z) \u2208 Ri,j ,\n0\notherwise,\n\nprop\ndef \u03c0(z) \u03c0i,j (x|z)\nri,j (x, z) =\nprop (z|x) ,\n\u03c0(x) \u03c0i,j\nN Z\n1 X\ndef\nprop (z|x) dz .\n\u03b1i,j (x, z) \u03c0i,j\n\u03c1i (x) = 1 \u2212\nN j=1 R\n\n(4)\n(5)\n(6)\n\nThe set Ri,j is de\u001cned by:\ndef \b\nprop (x|z) > 0 and \u03c0(x) \u03c0 prop (z|x) > 0 .\nRi,j = (x, z) \u2208 Rn \u00d7 Rn ; \u03c0(z) \u03c0i,j\ni,j\n\nNote that the fun tions \u03b1i,j (x, z), \u03c1i (x), ri,j (x, z) and the set Ri,j depend on Z 1:i\u22121 and\nX i:N .\nThe measures\nprop (x|z) dz dx ,\n\u03bd(dx \u00d7 dz) = \u03c0(z) \u03c0i,j\n\nprop (z|x) dz dx\n\u03bd T (dx \u00d7 dz) = \u03c0(x) \u03c0i,j\n\nare mutually absolutely ontinuous over Ri,j and mutually singular on the omplementary\nset [Ri,j ]c . The set Ri,j is unique, up to the \u03bd and \u03bd T negligible sets, and symmetri , i.e.\n(x, z) \u2208 Ri,j \u21d2 (z, x) \u2208 Ri,j .\nProof This onstru tion follows the general setup proposed by Luke Tierney in [20\u2104. We\nnow derive the probability kernel asso iated with the iteration des ribed in the previous\nsubse tion 2.1. The kernel P i(Z 1:i\u22121 , X i:N ; dz) is the omposition of a proposition kernel\nand of a sele tion kernel:\nZ\nS i (Z 1:i\u22121 , X i:N , Y 1:N ; dz) Qi (Z 1:i\u22121 , X i:N ; dY 1:N )\nP i (Z 1:i\u22121 , X i:N ; dz) =\nY 1:N\n\nwhi h onsists in proposing independently N andidates Y 1:N sampled from the density\nproposition, i.e.\nN\ndef Y prop k i\nQi (Z 1:i\u22121 , X i:N ; dY 1:N ) =\n\u03c0i,k (Y |X ) dY k\nk=1\n\nINRIA\n\n\f9\n\nParallel and intera ting MCMC's\n\nthen to sele t among these andidates or to stay at X i with the MH a eptan e probability,\ni.e.\n\nHen e:\n\nN\ndef 1 X i,j i j\n\u03b1 (X , Y ) \u03b4Y j (dz) + \u03c1\u0303i (X i , Y ) \u03b4X i (dz) .\nS i (Z 1:i\u22121 , X i:N , Y 1:N ; dz) =\nN j=1\n\nP i (Z 1:i\u22121 , X i:N ; dz) =\nN\nN Z\nnY\no\n1 X\nprop (Y k |X i ) dY k\n\u03c0i,k\n=\n\u03b1i,j (X i , Y j ) \u03b4Y j (dz)\nN j=1 Y 1:N\nk=1\n\n+\n\nZ\n\nY 1:N\n\n\u03c1\u0303i (X i , Y ) \u03b4X i (dz)\n\nand\nA1 =\n\nbe ause\n\nR\n\nYj\n\n|\n\nZ\n\nY 1:N\n\nk=1\n\no\n\n= A1 + A2\n\nk6=j\n\n}\n\nThe se ond term A2 reads:\n\n\u03c1\u0303i (X i , Y ) \u03b4X i (dz)\n\n= \u03b4X i (dz)\n\n{z\n\n=1\n\nN\n1 X i,j i\nprop (z|X i) dz\n\u03b1 (X , z) \u03c0i,j\nN j=1\n\n\u03b4Y j (dz) dY j = dz .\n\nA2 =\n\nprop (Y k |X i ) dY k\n\u03c0i,k\n\nN Z\n1 X\nprop (Y j |X i )\n\u03b1i,j (X i , Y j ) \u03b4Y j (dz) \u03c0i,j\nN j=1 Y j\nZ\nN\nnY\no\nprop (Y k |X i ) dY k dY j\n\u03c0i,k\nY \u00acj\n\n=\n\nn\n\nN\nY\n\nN\nnY\n\nprop (Y k |X i ) dY k\n\u03c0i,k\n\nk=1\n\nZ\n\nY 1:N\n\no\n\nN\nN\no\nn\n1 X i,j i j o n Y prop k i\n\u03c0i,k (Y |X ) dY k\n1\u2212\n\u03b1 (X , Y )\nN j=1\nk=1\n\nN\nN Z\no\nn\nY\n1 X\nprop (Y k |X i ) dY k\n= \u03b4X i (dz) 1 \u2212\n\u03b1i,j (X i , Y j )\n\u03c0i,k\nN j=1 Y 1:N\nk=1\n\nN Z\no\nn\n1 X\nprop (Y j |X i ) dY j .\n= \u03b4X i (dz) 1 \u2212\n\u03b1i,j (X i , Y j ) \u03c0i,j\nN j=1 Y j\n\nSumming up A1 and A2 proves the Lemma.\n\nRR n\u00060123456789\n\n\u2737\n\n\f10\n2.3\n\nF. Campillo & V. Rossi\nInvarian e property\n\nLemma 2.2\n\nFor all (x, z) \u2208 Rn \u00d7 Rn a.e. we have:\nprop (z|x) = \u03b1i,j (z, x) \u03c0(z) \u03c0 prop (x|z) .\n\u03b1i,j (x, z) \u03c0(x) \u03c0i,j\ni,j\n\nProof\n\nFor (x, z) 6\u2208 Ri,j the result is obvious. For (x, z) \u2208 Ri,j we have:\nprop (z|x)\n(ri,j (x, z) \u2227 1) \u03c0(x) \u03c0i,j\no\nn\nprop (x|z) , \u03c0(x) \u03c0 prop (z|x)\n= min \u03c0(z) \u03c0i,j\ni,j\n= (ri,j (z, x) \u2227 1) \u03c0(z) \u03c0 prop (x|z) .\ni,j\n\n\u2737\nLemma 2.3 ( onditional detailed balan e)\n\non Rn \u00d7 Rn\n\nThe following equality of measures de\u001cned\n\nP i (Z 1:i\u22121 , X i:N ; dZ i ) \u03c0(X i ) dX i = P i (Z 1:i , X i+1:N ; dX i ) \u03c0(Z i ) dZ i\n\n(7)\n\nholds true for any i = 1, . . . , N , Z 1:i\u22121 \u2208 R(i\u22121)\u00d7N , and X i+1:N \u2208 R(N \u2212i)\u00d7N .\n\nLeft hand side of (7) is a measure, say \u03bd(dZ i \u00d7 dX i) on (Rn \u00d7 Rn , B(Rn \u00d7 Rn )).\nFor all A1 , A2 \u2208 B(Rn ), we want to prove that \u03bd(A1 \u00d7 A2 ) = \u03bd(A2 \u00d7 A1 ). We have:\n\nProof\n\n\u03bd(A1 \u00d7 A2 ) =\n\nand\ni\n\nP (Z\n\n1:i\u22121\n\n,X\n\ni:N\n\nZ\n\nP i (Z 1:i\u22121 , X i:N ; A1 ) 1A2 (X i ) \u03c0(X i ) dX i\n\nN Z\n1 X\nprop (Z i |X i ) dZ i\n1A1 (Z i ) \u03b1i,j (X i , Z i ) \u03c0i,j\n; A1 ) =\nN j=1\n+ \u03c1i (X i ) 1A1 (X i )\n\nso that\n\u03bd(A1 \u00d7 A2 )\n=\n\nN ZZ\n1 X\nprop (Z i |X i ) dX i dZ i\n1A1 (Z i ) 1A2 (X i ) \u03b1i,j (X i , Z i ) \u03c0(X i ) \u03c0i,j\nN j=1\nZ\n+ \u03c1i (X i ) 1A1 (X i ) 1A2 (X i ) \u03c0(X i ) dX i .\n\n(8)\n\nINRIA\n\n\f11\n\nParallel and intera ting MCMC's\n\nAnd from Lemma 2.2, we get:\n\u03bd(A1 \u00d7 A2 )\n=\n\nN ZZ\n1 X\nprop (X i |Z i ) dZ i dX i\n1A1 (Z i ) 1A2 (X i ) \u03b1i,j (Z i , X i ) \u03c0(Z i ) \u03c0i,j\nN j=1\nZ\n+ \u03c1i (X i ) 1A1 (X i ) 1A2 (X i ) \u03c0(X i ) dX i\n\nEx hanging the name of variables X i \u2194 Z i in the \u001crst term of the right hand side of the\nprevious equality, leads to the same expression as (8) where A1 and A2 were inter hanged,\nin other words \u03bd(A1 \u00d7 A2 ) = \u03bd(A2 \u00d7 A1 ).\n\u2737\nProposition 2.4 (invarian e)\n\nThe probability measure\n\n\u03a0(dX) = \u03c0(X 1 ) dX 1 * * * \u03c0(X N ) dX N\n\nis an invariant distribution of the Markov kernel P , i.e. \u03a0P = \u03a0 that is:\nZ\n\nX\n\nP (X, dZ)\n\nN\nnY\n\ni=1\n\nN\no Y\n\u03c0(Z i ) dZ i .\n\u03c0(X i ) dX i =\n\n(9)\n\ni=1\n\nProof\n\nZ\n\nX\n\nP (X, dZ)\n\nN\nnY\n\ni=1\n\n=\n\nZ\n\nX\n\n\u03c0(X i ) dX i\n\no\n\nP 1 (X 1:N ; dZ 1 ) P 2 (Z 1 , X 2:N ; dZ 2 ) * * *\nN\n\n* * * P (Z\n\n1:N \u22121\n\n, X ; dZ )\nN\n\nN\n\nN\nnY\n\ni=1\n\n=\n\nZ\n\nX\n\nX\n\nP (X, dZ)\n\nN\nnY\n\ni=1\n\n=\n\nZ\n\nX\n\ni=2\n\no\n\u03c0(X i ) dX i =\n\nP (Z 1 , X 2:N ; dX 1 ) \u03c0(Z 1 ) dZ 1 P 2 (Z 1 , X 2:N ; dZ 2 ) * * *\nN\no\nnY\n\u03c0(X i ) dX i .\n* * * P n (Z 1:N \u22121 , X N ; dZ N )\n1\n\ni=2\n\nRR n\u00060123456789\n\no\n\nP 1 (X 1:N ; dZ 1 ) \u03c0(X 1 ) dX 1 P 2 (Z 1 , X 2:N ; dZ 2 ) * * *\nN\no\nnY\n\u03c0(X i ) dX i .\n* * * P n (Z 1:N \u22121 , X N ; dZ N )\n\nUsing (7) with i = 1 gives:\nZ\n\n\u03c0(X i ) dX i\n\n\f12\n\nF. Campillo & V. Rossi\n\nIn this last expression the kernel P 1 (Z 1 , X 2:N ; dX 1) is a measure on the variable X 1 whi h\nno longer appears in the integrand. Therefore its integral with respe t to this variable is 1,\nhen e:\nZ\n\nX\n\nP (X, dZ)\n\nN\nnY\n\ni=1\n\n= \u03c0(Z ) dZ\n1\n\n1\n\no\n\u03c0(X i ) dX i =\n\nZ\n\nX 2:N\n\nP 2 (Z 1 , X 2:N ; dZ 2 ) * * *\nn\n\n* * * P (Z\n\n1:N \u22121\n\n, X ; dZ )\nN\n\nN\n\nN\nnY\n\ni=2\n\nRepeating this pro edure su essively for X 2 to X N leads to (9).\n3\n\no\n\u03c0(X i ) dX i .\n\u2737\n\nParallel/intera ting Metropolis within Gibbs\n(MwG) algorithm\n\nLet \u03c0(x) be the probability density fun tion of a target distribution de\u001cned on (Rn , B(Rn)).\nFor l = 1, . . . , n, we de\u001cne the onditional laws:\ndef\n\u03c0l (xl |x\u00acl ) = R\n\n\u03c0(x1:n )\n.\n\u03c0(x1:n ) dx\u00acl\n\n(10)\n\nWhen we know to sample from (10), we are able to use the Gibbs sampler. It is possible\nto adapt our intera ting method to parallel Gibbs sampler. But very often we do not know\nhow to sample from (10) and therefore we onsider proposal onditional densities \u03c0lprop (xl )\nde\u001cned for all l. In this ase, we use Metropolis within Gibbs algorithm (see appendix).\nWe present in the following how to make intera tions between parallel MwG algorthims.\nThe MwG algorithm is more general than Gibbs algorithm, so a parallel/intera ted Gibbs\nalgorithm an easily be dedu ed from the parallel/intera ted MwG algorithm.\n3.1\n\nThe algorithm\n\nOne iteration X \u2192 Z of the parallel/intera ting Metropolis within Gibbs method onsists\nin updating the omponents Xl su essively for l = 1, . . . , n, i.e.\n[X1:n ] \u2192 [Z1 X2:n ] \u2192 [Z1:2 X3:n ] * * * [Z1:n\u22121 Xn ] \u2192 [Z1:n ] .\n\nFor ea h l \u001cxed, the sub omponents Xli are updated sequentially for i = 1, . . . , N in two\nsteps:\n(i ) Proposal step: We sample independently N andidates Ylj \u2208 R for j = 1 : N a ording\nto:\nl,prop\n(\u03be|JZ, Xli , XKil ) d\u03be ,\nYlj \u223c \u03c0i,j\n1\u2264j\u2264n\n\nINRIA\n\n\f13\n\nParallel and intera ting MCMC's\n\nwhere\n\n\uf8ee\n\nZl1\n\n\uf8ef\n\uf8ef\n\uf8ef\ni def \uf8ef\nJZ, \u03be, XKl = \uf8efZ1:l\u22121\n\uf8ef\n\uf8ef\n\uf8f0\n\nWe also use the following lighter notation:\n\n..\n.\n\nZli\u22121\n\u03be\nXli+1\n\n..\n.\n\nXlN\n\n\uf8f9\n\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\nXl+1:n \uf8fa .\n\uf8fa\n\uf8fa\n\uf8fb\n\nl,prop\nl,prop\n(\u03be|\u03be \u2032 ) = \u03c0i,j\n(\u03be|JZ, \u03be \u2032 , XKil ) .\n\u03c0i,j\n\n(ii )\n\nThe sub omponent Xli ould be repla ed by one of the N andidates\nor stay un hanged a ording to a multinomial sampling, the resulting value is\nalled Zli, i.e.:\n\uf8f1 1\ni\n1\nYl\nwith probability N1 \u03b1i,1\n\uf8f4\nl (Xl , Yl ) ,\n\uf8f4\n\uf8f4\n\uf8f4\n.\n\uf8f2 .\n.\nZli \u2190\ni\nN\n\uf8f4\nYlN with probability N1 \u03b1i,N\n\uf8f4\nl (Xl , Yl ) ,\n\uf8f4\n\uf8f4\n\uf8f3 i\nXl with probability \u03c1\u0303il (Xli , Yl1:N )\nwhere:\nSele tion step:\nYl1:N\n\n\u2032\ni\n\u03c0 l,prop (\u03be|\u03be \u2032 )\n\u2032 def \u03c0l (\u03be |X\u00acl ) i,j\n\u22271,\n\u03b1i,j\n(\u03be,\n\u03be\n)\n=\nl\ni )\nl,prop \u2032\n\u03c0l (\u03be|X\u00acl\n(\u03be |\u03be)\n\u03c0i,j\n\n\u03c1\u0303il (Xli , Yl1:N )\n\nN\n1 X i,j i j\ndef\n\u03b1 (Xl , Yl ) .\n= 1\u2212\nN j=1 l\n\nThe resulting algorithm is depi ted in Algorithm 2.\n3.2\n\nDes ription of the MH kernel\n\nThe Markov kernel on Rn\u00d7N asso iated with the MH algorithm des ribed in\nSe tion 3.1, is\n\nLemma 3.1\n\ndef\nP (X, dZ) = P1 (X1:n ; dZ1 ) P2 (Z1 , X2:n ; dZ2 ) * * * Pn (Z1:n\u22121 , Xn ; dZn ) .\n\n(11)\n\nAt iteration l, the kernel Pl (Z1:l\u22121 , Xl:n ; dZl ) generates Zl1:N from the already updated om1:N\n1:N\nponents Z1:l\u22121\nand the remaining omponents Xl:n\n.\ni\nEa h omponent Z1:l , for i = 1 * * * N , is updated independently one from ea h other:\nN\ndef Y i\nPl (JZ, Xli , XKil ; dZli ) .\nPl (Z1:l\u22121 , Xl:n ; dZl ) =\ni=1\n\nRR n\u00060123456789\n\n(12)\n\n\f14\n\nF. Campillo & V. Rossi\n\nhoose X \u2208 Rn\u00d7N\n\nfor k = 1, 2, . . . do\nfor l = 1 : n do\nfor i = 1 : N do\nfor j = 1 : N do\n\nl,prop\nYlj \u223c \u03c0i,j\n(\u03be) d\u03be\n\n\u03b1j \u2190\n\nl,prop\ni\n(Xli |Ylj )\n\u03c0l (Ylj |X\u00acl\n) \u03c0i,j\ni )\nl,prop\n\u03c0l (Xli |X\u00acl\n(Ylj |Xli )\n\u03c0i,j\n\nend for\nP\n\u03b1j\n\u03c1\u0303 \u2190 1 \u2212 N1 N\n\uf8f1 1 j=1\nYl\nwith\n\uf8f4\n\uf8f4\n\uf8f2 ..\n.\nXli \u2190\nY N with\n\uf8f4\n\uf8f4\n\uf8f3 li\nXl with\n\nend for\nend for\nend for\n\n\u22271\n\nprobability \u03b11 /N\nprobability \u03b1N /N\nprobability \u03c1\u0303\n\nAlgorithm 2:\n\nParallel/intera ting MwG.\n\nHere Zli is generated from JZ, Xli , XKil a ording to:\nN\ndef 1 X i,j\nl,prop \u2032\nPli (JZ, \u03be, XKil ; d\u03be \u2032 ) =\n\u03b1 (\u03be, \u03be \u2032 ) \u03c0i,j\n(\u03be |\u03be) d\u03be \u2032 + \u03c1il (\u03be) \u03b4\u03be (d\u03be \u2032 )\nN j=1 l\n\n(13)\n\nA eptation probabilities are:\n(\n\nif (\u03be, \u03be \u2032 ) \u2208 Rli,j ,\notherwise,\n\n(14)\n\nl,prop\n\u2032\n\u2032\ni\ni\ndef \u03c0l (\u03be |Z1:l\u22121 , Xl+1:n ) \u03c0i,j (\u03be|\u03be )\nrli,j (\u03be, \u03be \u2032 ) =\n,\ni\ni\nl,prop \u2032\n\u03c0l (\u03be|Z1:l\u22121\n, Xl+1:n\n) \u03c0i,j\n(\u03be |\u03be)\n\n(15)\n\n\u2032\n\u03b1i,j\nl (\u03be, \u03be )\n\ndef\n=\n\nrli,j (\u03be, \u03be \u2032 ) \u2227 1\n0\n\nN Z\n1 X\ndef\nl,prop \u2032\n\u03b1i,j (\u03be, \u03be \u2032 ) \u03c0i,j\n(\u03be |\u03be) d\u03be \u2032 .\n\u03c1il (\u03be) = 1 \u2212\nN j=1 R l\n\n(16)\n\nINRIA\n\n\f15\n\nParallel and intera ting MCMC's\nFinally, Rli,j is the set of ordered pairs (\u03be, \u03be \u2032 ) \u2208 R2 su h that\nl,prop\ni\ni\n(\u03be|\u03be \u2032 ) > 0 ,\n\u03c0l (\u03be \u2032 |Z1:l\u22121\n, Xl+1:n\n) \u03c0i,j\n\u03c0 (\u03be|Z i\n, Xi\n) \u03c0 l,prop (\u03be \u2032 |\u03be) > 0 .\nl\n\n1:l\u22121\n\nl+1:n\n\ni,j\n\ni,j\ni,j\n\u2032\ni\n\u2032\nNote that the fun tions \u03b1i,j\nl (\u03be, \u03be ), \u03c1l (\u03be), rl (\u03be, \u03be ) and the set Rl depend on Z1:l\u22121 and\nXl+1:n .\n\nThis onstru tion follows the general setup proposed by Luke Tierney in [20\u2104. The\nkernel is de\u001cned by:\n\nProof\n\ndef\nPli (JZ, \u03be, XKil ; d\u03be \u2032 ) =\n\nZ\n\nRN\n\nSli (JZ, \u03be, XKil , \u03b6 1:N ; d\u03be \u2032 ) \u00d7 Qil (JZ, \u03be, XKil ; d\u03b6 1:N ) .\n|\n{z\n} |\n{z\n}\nsele tion kernel\nproposal kernel\n\nThis kernel onsists \u001crstly in proposing a population of N andidates \u03b6 1:N\nfrom:\n\n\u2208 RN\n\nN\ndef Y l,prop j\n\u03c0i,j (\u03b6 |\u03be) d\u03b6 j ,\nQil (JZ, \u03be, XKil ; d\u03b6 1:N ) =\n\nsampled\n(17)\n\nj=1\n\nthen se ondly in sele ting among these andidates or reje ting them a ording to a MH\nte hnique, i.e.\nSli (JZ, \u03be, XKil , \u03b6 1:N ;\n\nN\ndef 1 X i,j\nd\u03be ) = N \u03b1l (\u03be, \u03b6 j ) \u03b4\u03b6 j (d\u03be \u2032 ) + \u03c1\u0303il (\u03be, \u03b6 1:N ) \u03b4\u03be (d\u03be \u2032 )\nj=1\n\u2032\n\ni\n1:N def\n) = 1 \u2212 N1\nwhere \u03b1i,j\nl is given by (14) and \u03c1\u0303l (\u03be, \u03b6\n\nRR n\u00060123456789\n\nPN\n\nj=1\n\nj\n\u03b1i,j\nl (\u03be, \u03b6 ).\n\n(18)\n\n\f16\n\nF. Campillo & V. Rossi\n\nHen e:\ndef\nPli (JZ, \u03be, XKil ; d\u03be \u2032 ) =\n=\n\nZ\n\n\u03b6 1:N\n\nSli (JZ, \u03be, XKil , \u03b6 1:N ; d\u03be \u2032 ) Qil (JZ, \u03be, XKil ; d\u03b6 1:N )\n\nN Z\nN\nY\n1 X\nl,prop k\nj\n\u2032\nj\n\u03b1i,j\n(\u03be,\n\u03b6\n)\n\u03b4\n(\n(\u03b6 |\u03be) d\u03b6 k\nd\n\u03be\n)\n\u03c0i,j\n\u03b6\nN j=1 \u03b6 1:N l\nk=1\n\nn\n1\n+ 1\u2212\nN\n=\n\n1\nN\n\nN Z\nX\nj=1\n\n\u03b6j\n\nN Z\nX\nj=1\n\n\u03b6 1:N\n\nj\n\u03b1i,j\nl (\u03be, \u03b6 )\n\nN\nY\n\nl,prop k\n\u03c0i,j\n(\u03b6 |\u03be) d\u03b6 k\n\nk=1\n\no\n\n\u03b4\u03be (d\u03be \u2032 )\n\nprop (\u03b6 j |\u03be) d\u03b6 j\nj\n\u2032\n\u03b1i,j\nl (\u03be, \u03b6 ) \u03b4\u03b6 j (d\u03be ) \u03c0l\n\nN Z\no\nn\n1 X\nprop (\u03b6 j |\u03be) d\u03b6 j \u03b4 (d\u03be \u2032 )\nj\n\u03b1i,j\n+ 1\u2212\n\u03be\nl (\u03be, \u03b6 ) \u03c0l\nN j=1 \u03b6 j\n\nN\n1 X i,j\n\u03b1 (\u03be, \u03be \u2032 ) \u03c0lprop (\u03be \u2032 |\u03be) d\u03be \u2032\n=\nN j=1 l\n\nN Z\no\n1 X\nprop (\u03be \u2032\u2032 |\u03be) d\u03be \u2032\u2032 \u03b4 (d\u03be \u2032 )\n\u2032\u2032\n\u03b1i,j\n+ 1\u2212\n\u03be\nl (\u03be, \u03be ) \u03c0l\nN j=1 \u03be\u2032\u2032\n\nn\n\nwhi h orrespond to Equations (13) to (16).\n3.3\n\n\u2737\n\nInvarian e property\n\nLemma 3.2\n\nFor almost all (\u03be, \u03be \u2032 ) \u2208 R2 :\n\nl,prop \u2032\n\u2032\ni\ni\n(\u03be |\u03be)\n\u03b1i,j\nl (\u03be, \u03be ) \u03c0l (\u03be|Z1:l\u22121 , Xl+1:n ) \u03c0i,j\n\nl,prop\n\u2032\n\u2032\ni\ni\n= \u03b1i,j\n(\u03be|\u03be \u2032 )\nl (\u03be , \u03be) \u03c0l (\u03be |Z1:l\u22121 , Xl+1:n ) \u03c0i,j\n\nj\nj\ni\ni\nfor any l, i, j , (Z1:l\u22121\n, Xl+1:n\n), and (Z1:l\u22121\n, Xl+1:n\n).\n\nProof\n\nFor (\u03be, \u03be \u2032 ) 6\u2208 Rli,j , the result is obvious. For (\u03be, \u03be \u2032 ) \u2208 Rli,j a.e.:\ni\ni\n(rli,j (\u03be, \u03be \u2032 ) \u2227 1) \u03c0l (\u03be|Z1:l\u22121\n, Xl+1:n\n) \u03c0lprop (\u03be \u2032 |\u03be)\no\nn\ni\ni\ni\ni\n, Xl+1:n\n) \u03c0lprop (\u03be \u2032 |\u03be)\n= min \u03c0l (\u03be \u2032 |Z1:l\u22121\n, Xl+1:n\n) \u03c0lprop (\u03be|\u03be \u2032 ) , \u03c0l (\u03be|Z1:l\u22121\ni\ni\n= (rli,j (\u03be \u2032 , \u03be) \u2227 1) \u03c0l (\u03be \u2032 |Z1:l\u22121\n, Xl+1:n\n) \u03c0lprop (\u03be|\u03be \u2032 ) .\n\n\u2737\n\nINRIA\n\n\f17\n\nParallel and intera ting MCMC's\nLemma 3.3 ( onditional detailed balan e)\n\non R \u00d7 R\n\nThe following equality of measures de\u001cned\n\ni\ni\nPli (JZ, \u03be, XKil ; d\u03be \u2032 ) \u00d7 \u03c0l (\u03be|Z1:l\u22121\n, Xl+1:n\n) d\u03be\n\ni\ni\n= Pli (JZ, \u03be \u2032 , XKil ; d\u03be) \u00d7 \u03c0l (\u03be \u2032 |Z1:l\u22121\n, Xl+1:n\n) d\u03be \u2032\n\n(19)\n\nholds true for any l = 1 * * * n, i = 1 * * * N and Z1:l\u22121 \u2208 RN \u00d7(l\u22121) , Xl+1:n \u2208 RN \u00d7(n\u2212l) .\n\nThe left hand side of equality (19) is a measure \u03bd(d\u03be \u2032 \u00d7 d\u03be) de\u001cned on (R2 , B(R2)).\nFor all A1 , A2 \u2208 B(R), we want to prove that \u03bd(A1 \u00d7 A2 ) = \u03bd(A2 \u00d7 A1 ).\nWe have:\nZ\ni\ni\n, Xl+1:n\n) d\u03be\n\u03bd(A1 \u00d7 A2 ) = Pli (JZ, \u03be, XKil ; A1 ) 1A (\u03be) \u03c0l (\u03be|Z1:l\u22121\nand\nProof\n\n2\n\nPli (JZ, \u03be, XKil ; A1 )\n\nso that\n\u03bd(A1 \u00d7 A2 ) =\n\nUsing Lemma 3.2 we get:\n\u03bd(A1 \u00d7 A2 ) =\n\nN Z\n1 X\nprop (\u03be \u2032 |\u03be) d\u03be \u2032 + \u03c1i (\u03be) 1 (\u03be)\n\u2032\n=\n1A1 (\u03be \u2032 ) \u03b1i,j\nA1\nl\nl (\u03be, \u03be ) \u03c0l\nN j=1\nN ZZ\n1 X\n\u2032\n1A1 (\u03be \u2032 ) 1A2 (\u03be) \u03b1i,j\nl (\u03be, \u03be )\nN j=1\ni\ni\n\u03c0l (\u03be|Z1:l\u22121\n, Xl+1:n\n) \u03c0lprop (\u03be \u2032 |\u03be) d\u03be d\u03be \u2032\nZ\ni\ni\n, Xl+1:n\n) d\u03be\n+ \u03c1il (\u03be) 1A1 (\u03be) 1A2 (\u03be) \u03c0l (\u03be|Z1:l\u22121\n\n(20)\n\nN ZZ\n1 X\n\u2032\n1A1 (\u03be \u2032 ) 1A2 (\u03be) \u03b1i,j\nl (\u03be , \u03be)\nN j=1\ni\ni\n\u03c0l (\u03be \u2032 |Z1:l\u22121\n, Xl+1:n\n) \u03c0lprop (\u03be|\u03be \u2032 ) d\u03be \u2032 d\u03be\nZ\ni\ni\n, Xl+1:n\n) d\u03be\n+ \u03c1il (\u03be) 1A1 (\u03be) 1A2 (\u03be) \u03c0l (\u03be|Z1:l\u22121\n\nEx hanging the name of variables \u03be \u2194 \u03be \u2032 in the \u001crst term of the right hand side of the\nprevious equality leads to the same expression as (20) where A1 and A2 were inter hanged,\nin other words \u03bd(A1 \u00d7 A2 ) = \u03bd(A2 \u00d7 A1 ).\n\u2737\nProposition 3.4 (invarian e)\n\nThe measure\n\n\u03a0(dX) = \u03c0(X 1 ) dX 1 * * * \u03c0(X N ) dX N\n\nis invariant for the kernel P , that is \u03a0P = \u03a0 i.e.:\nZ\n\nX\n\nRR n\u00060123456789\n\nP (X, dZ)\n\nN\nnY\n\ni=1\n\nN\no Y\n\u03c0(Z i ) dZ i .\n\u03c0(X i ) dX i =\ni=1\n\n(21)\n\n\f18\n\nF. Campillo & V. Rossi\n\nProof\n\nZ\n\nX\n\nP (X, dZ)\n\nN\nnY\n\n\u03c0(X i ) dX i\n\ni=1\n\n=\n\nZ\n\nX\n\no\n\nP1 (X1:n ; dZ1 ) P2 (Z1 , X2:n ; dZ2 ) * * * Pn (Z1:n\u22121 , Xn ; dZn )\nN\nY\n\b\ni\ni\ni\n) dX2:n\n\u03c01 (X1i |X2:n\n) dX1i \u03c0\u00ac1 (X2:n\ni=1\n\n=\n\nZ\n\nX\n\nP1 (X1:n ; dZ1 )\n\nN\nnY\n\ni\n\u03c01 (X1i |X2:n\n) dX1i\n\ni=1\n\no\n\nP2 (Z1 , X2:n ; dZ2 ) * * * Pn (Z1:n\u22121 , Xn ; dZn )\n\nN\nnY\n\ni\ni\n\u03c0\u00ac1 (X2:n\n) dX2:n\n\nN\nnY\n\ni\ni\n\u03c0\u00ac1 (X2:n\n) dX2:n\n\ni=1\n\n=\n\nZ nY\nN\nX\n\ni=1\n\nN\no nY\no\ni\n\u03c01 (X1i |X2:n\n) dX1i\nP1i (JZ, X1i , XKi1 ; dZ1i )\ni=1\n\nP2 (Z1 , X2:n ; dZ2 ) * * * Pn (Z1:n\u22121 , Xn ; dZn )\n\ni=1\n\nMoreover\nP1 (X1:n ; dZ1 )\n\nN\nnY\n\ni=1\n\n=\n\nN\nnY\n\ni=1\n\n=\n\nN\nY\n\no\n\no\n\no\ni\n\u03c01 (X1i |X2:n\n) dX1i =\n\nN\no nY\no\ni\n\u03c01 (X1i |X2:n\n) dX1i\nP1i (JZ, X1i , XKi1 ; dZ1i )\ni=1\n\ni\nP1i (JZ, X1i , XKi1 ; dZ1i ) \u03c01 (X1i |X2:n\n) dX1i\n\ni=1\n\n=\n\nN\nY\n\ni\nP1i (JZ, Z1i , XKi1 ; dX1i ) \u03c01 (Z1i |X2:n\n) dZ1i\n\ni=1\n\nthis last equality follows from Equation (19). Hen e,\nZ\n\nX\n\nP (X, dZ)\n\n=\n\nN\nnY\n\ni=1\n\nZ Y\nN n\nX i=1\n\n\u03c0(X i ) dX i\n\no\n\no\ni\nP1i (JZ, Z1i , XKi1 ; dX1i ) \u03c01 (Z1i |X2:n\n) dZ1i P2 (Z1 , X2:n ; dZ2 ) * * *\n* * * Pn (Z1:n\u22121 , Xn ; dZn )\n\nN\nnY\n\ni=1\n\ni\ni\n\u03c0\u00ac1 (X2:n\n) dX2:n\n\no\n\nINRIA\n\n\f19\n\nParallel and intera ting MCMC's\n\nIn this last expression, for i = 1, . . . , N , the kernel P1i (JZ, Z1i , XKi1 ; dX1i ) is a measure for the\nvariable X1i whi h no longer appears in the integrand. Using the fa t that the integral of\nthe kernel w.r.t. X1i is 1 we get:\nZ\n\nX\n\nP (X, dZ)\n\n=\n\nN\nnY\n\n\u03c0(X i ) dX i\n\ni=1\n\nZ\n\nN n\nY\n\nZ\n\nN\nY\n\nX2:N i=1\n\no\n\no\ni\n\u03c01 (Z1i |X2:n\n) dZ1i P2 (Z1 , X2:n ; dZ2 ) * * *\n* * * Pn (Z1:n\u22121 , Xn ; dZn )\n\nN\nnY\n\ni\ni\n\u03c0\u00ac1 (X2:n\n) dX2:n\n\nN\nnY\n\ni\ni\n\u03c0(Z1i X2:n\n) dZ1i dX2:n\n\ni=1\n\n=\n\nX2:N i=1\n\nP2 (Z1 , X2:n ; dZ2 ) * * *\n* * * Pn (Z1:n\u22121 , Xn ; dZn )\n\ni=1\n\nRepeating this pro ess su essively for X2 to Xn leads to (21).\n4\n4.1\n\no\n\no\n\u2737\n\nNumeri al tests\nA multi-modal example\n\nWe apply now the parallel/intera ting Metropolis-Hastings sampler, see Se tion 2, to a ase\nwhere the target distribution is multimodal:\n\u03c0 = p1 N (C1 , I) + p2 N (C2 , I) + p3 N (C3 , I)\n\nwith p1 = 0.1, p2 = 0.3, p3 = 0.6, and C1 = (\u221210, \u221210), C2 = (5, 0), C3 = (\u22125, 5). It is a\nmixture of 3 two-dimensional Gaussian densities.\nWe des ribe the proposal kernel (1), for updating the omponent X i, ea h hain j propose\na new andidate a ording to the following distribution law:\n(\nN (X i , d1 I) , if i 6= j ,\nprop\nprop\ni\n1:i\u22121\ni\ni+1:N\n\u03c0i,j (y|X ) = \u03c0i,j (y|Z\n,X ,X\n)=\nN (X j , I) ,\nif i = j\nwhere d def\n= |X i \u2212 X j |.\nThe idea here is to explore the spa e with a Gaussian random walk (i = j ) but also to\nallow \u0010jumps\u0011 toward already explored interesting areas (i 6= j ). If X i and X j are lose one\nthe other, then \u0010the hain j will propose a andidate far from X j and X i\u0011. If X i and X j\nare far one to the other, then the \u0010 hain j will propose a andidate lose to X j \u0011.\n\nRR n\u00060123456789\n\n\f20\n\nF. Campillo & V. Rossi\n\nFigure 1: Target distribution \u03c0(x) (left) and initial positions of the hains X(0),i , for i =\n1 * * * N (right).\n\nFigure 2: Positions of the hains X(k),i , for i = 1 * * * N , at iterations k = 1000 (left) and\nk = 5000 (right).\nestimation of p\n\n1\n\n1\n0.5\n0\n\n0\n\n1000\n\n2000\n3000\nestimation of p\n\n4000\n\n5000\n\n2000\n3000\nestimation of p3\n\n4000\n\n5000\n\n2\n\n1\n0.5\n0\n\n1\n0.5\n0\n\n0\n\n1000\n\nINRIA\n\n\f21\n\nParallel and intera ting MCMC's\n\nHere N = 50, and the initial points X(0),i , for i = 1 * * * N , are sampled a ording to the\nuniform law on the square [\u221215, 10] \u00d7 [0, 10], see Figure 1 (right). Figures 2 learly demonstrate the onvergen e of the method. In Figure 3 we present the evolution of the proportion\nof parti les lo ated in the neighbor of the three di\u001berent modes: this also demonstrates the\ngood behavior of the method. Note that the initial parti les do not over the mode number\n2, so the algorithm is able to rea h the isolated mode and to balan es the parti les among\nthe modes a ording to the parameters pi.\n4.2\n\nAn hidden Markov model\n\nWe apply the parallel/intera ting Metropolis within Gibbs sampler, see Se tion 3, to a toy\nproblem where a good estimate \u03c0\u0302 of the target distribution \u03c0 is available. Consider the\nlinear Gaussian state spa e model:\nsl+1 = a sl + wl ,\ns1 \u223c N (s\u03041 , Q1 ) ,\n(22a)\nyl = b sl + vl\n(22b)\nfor l = 1 * * * n, where w1:n and v1:n are entered white Gaussian noises with varian es \u03c3w2\nand \u03c3v2 . Suppose that b is known and a = \u03b8 is unknown with a priori law N (\u03bc\u03b8 , \u03c3\u03b82 ). We\nalso suppose that w1:n , v1:n , s1 and \u03b8 are mutually independent.\nThe state variable is\ndef\nx1:n+1 = (s1:n , \u03b8)\n\nand the target onditional density is\ndef\n\u03c0(x1:n+1 ) dx1:n+1 = \u03c0(s1:n , \u03b8) ds1:n d\u03b8 = law(s1:n , \u03b8|y1:n = y1:n ) .\n\nThis target law is not Gaussian, but we an perform a Gibbs sampler. Indeed the marginal\nonditional laws are available:\ndef\n\u03c0s (sl |s\u00acl , \u03b8) dsl = law(sl |s\u00acl = s\u00acl , \u03b8 = \u03b8, y1:n = y1:n ) = N (ml , r2 ) ,\ndef\n\u03c0\u03b8 (\u03b8|s1:n ) d\u03b8 = law(\u03b8|s1:n = s1:n , y1:n = y1:n ) = N (m\u0303, r\u03032 )\nwith\nl\n\ndef\nr2 =\ndef\nr\u03032 =\n\nb2\n\u03c3v2\n\n+\n\n1\n\u03c3\u03b82\n\n+\n\n\u0001\n\n1\n\u03b82 \u22121\n,\n\u03c3w2 + \u03c3w2\nPn\n2\n\u0001\nl=2 sl\u22121 \u22121\n\u03c3w2\n\ndef\nml = r2\n,\n\ndef\nm\u0303 = r\u03032\n\nb yl\n\u03c3v2\n\u03bc\u03b8\n\u03c3\u03b82\n\n\u0001\n\u03b8 sl+1\n+ \u03b8 s\u03c3l\u22121\n,\n2\n\u03c3w2\nw\nPn\n\u0001\nl=2 sl\u22121 sl\n.\n\u03c3w2\n\n+\n\n+\n\nWe will perform three algorithms:\n(i ) N parallel/intera ting Metropolis within Gibbs samplers (Alg. 2),\n(ii ) N parallel/independent Metropolis within Gibbs samplers (Alg. 3),\n\nRR n\u00060123456789\n\n\f22\n\nF. Campillo & V. Rossi\n\n2\n1.8\n1.6\n\n1\n\nL error estimation\n\n1.4\n1.2\n1\n0.8\n0.6\n0.4\n0.2\n0\n\n0\n\n5\n\n10\n\n15\n\n20\n25\n30\nCPU time (sec.)\n\n35\n\n40\n\n45\n\n50\n\nFigure 4: Evolution of the indi ator \u03b5k , see (23), for the parallel/independent MwG sampler\n(- -), and for the parallel/intera ting MH sampler (\u0015). This evolution is depi ted as a\nfun tion of the CPU time and not as a fun tion of the iteration number k. The residual\nerror of about 0.22 for the se ond method is due to the limited size of the sample.\n2\n1.8\n1.6\n\nL1 error estimation\n\n1.4\n1.2\n1\n0.8\n0.6\n0.4\n0.2\n0\n\n0\n\n1000\n\n2000\n3000\nCPU time (sec.)\n\n4000\n\n5000\n\nFigure 5: Evolution of the indi ator \u03b5k , see (23), for the parallel/independent MwG sampler\n(- -). After 5000 se . CPU time, the onvergen e of this method is still unsatisfa tory.\nINRIA\n\n\f23\n\nParallel and intera ting MCMC's\n\n(iii ) NGibbs parallel/independent Gibbs samplers.\nOur aim is to show that making parallel samplers intera t ould speed up the onvergen e\ntoward the stationary distribution.\nBe ause of its good onvergen e property, method (iii ) is onsidered as a referen e\nmethod. Here we perform k = 10000 iterations of NGibbs = 5000 independent Gibbs samplers. We obtain a kernel density estimate \u03c0\u0302 of the target density based on the NGibbs = 5000\n\u001cnal values. Let \u03c0\u0302x be the orresponding l-th marginal density.\nFor methods (i ) and (ii ) we perform N = 50 parallel samplers. Let \u03c0int,k and \u03c0ind,k be\nthe kernel density estimates of the target density based on the \u001cnal values of methods (i )\nand (ii ) respe tively. Let \u03c0xint,k and \u03c0xind,k be the orresponding l-th marginal densities.\nThe parameter values for the simulations are a = 2, b = 2, \u03c3w2 = 9, \u03c3v2 = 25, s1 \u223c N (4, 9),\n\u03b8 \u223c N (1, 4) and n = 10.\nFor ea h algorithm (i ) and (ii ), that is for \u03c0xk = \u03c0xind,k and \u03c0xint,k , we ompute\nl\n\nl\n\nl\n\nl\n\ndef\n\u03b5kl =\n\nZ\n\n|\u03c0xkl (\u03be) \u2212 \u03c0\u0302xl (\u03be)| d\u03be ,\n\nl\n\nl\n\nl = 1***n + 1.\n\nHen e \u03b5kl is an estimation of the L1 error between the target probability distribution and\nits estimation provided by the algorithm used. To sum up the information of the n = 10\nindi ators we onsider their mean:\n\u03b5k =\n\nn+1\n1 X k\n\u03b5l .\nn+1\n\n(23)\n\nl=1\n\nThese estimations are based on a sample of size N = 50 only, so they su\u001ber from variability.\nThis is not problemati al, indeed we do not want to estimate L1 errors but to diagnose the\nonvergen e toward the stationary distribution. So we use \u03b5kl as an indi ator whi h must\nde rease and remain lose to a small value when onvergen e o urs.\nTo ompare fairly the parallel/independent MwG algorithm and the parallel/intera ted\nMwG algorithm, we represent on Figures 4 and 5 the indi ator \u03b5k for ea h algorithm not as\na fun tion of k but as a fun tion of the CPU time.\nIn Figure 4 we see that even if one iteration of algorithm (i ) needs more CPU than one\nof (ii ), still the \u001crst algorithm onverges more rapidly than the se ond one. The residual\nerror of 0.22 is due to the limited size of the sample. This error de reases to 0 as N \u2191 \u221e.\nFigure 5 shows the ine\u001e ien y of parallel/independent MwG on this simple model.\n5\n\nCon lusion\n\nThis work showed that making parallel MCMC hains intera t ould improve their onvergen e properties. We proved the basi properties of the MCMC method, we did not prove\nthat the proposed strategy speeds up the onvergen e. This di\u001e ult point is related to the\nproblem of the rate of the onvergen e of the MCMC algorithms.\n\nRR n\u00060123456789\n\n\f24\n\nF. Campillo & V. Rossi\n\nThrough a simple example we saw that the Metropolis within Gibbs strategy ould be\na poor strategy. However this method is widely used in pra ti e on more omplex non\nlinear models. In this situation our strategy improved the onvergen e properties. We also\ndemonstrated that this approa h an handle multimodal ases.\n\nINRIA\n\n\f25\n\nParallel and intera ting MCMC's\n\nAppendix: MwG algorithm\n\nOne iteration X \u2192 Z of the Metropolis within Gibbs method onsists in updating the\nomponents Xl su essively for l = 1, . . . , n, i.e.\n[X1:n ] \u2192 [Z1 X2:n ] \u2192 [Z1:2 X3:n ] * * * [Z1:n\u22121 Xn ] \u2192 [Z1:n ] .\n\nEa h omponents Xl is updated in two steps:\n(i ) Proposal step: We sample a andidate Yl a ording to:\nYl \u223c \u03c0lprop (\u03be) d\u03be\n(ii )\n\nThe omponent Xl ould be repla ed by the andidate Yl or stay\nun hanged a ording to a binomial sampling, the resulting value is alled Zl, i.e.:\n(\nYl with probability \u03b1l (Xl , Yl ) ,\nZl \u2190\nXl with probability 1 \u2212 \u03b1l (Xl , Yl )\nwhere:\nSele tion step:\n\nprop\ndef \u03c0l (\u03be \u2032 ) \u03c0l (\u03be)\n\u03b1l (\u03be, \u03be \u2032 ) =\n\u22271\n\u03c0l (\u03be) \u03c0lprop (\u03be \u2032 )\n\nThe resulting algorithm is depi ted in Algorithm 3.\nhoose X1:n \u2208 Rn\n\nfor k = 1, 2, . . . do\nfor l = 1 : n do\n\nYl \u223c \u03c0lprop (\u03be) d\u03be {proposed\nu \u223c U[0, 1]\nif u \u2264 \u03b1l (Xl , Yl ) then\nXl \u2190 Yl\n\nandidate}\n\nend if\nend for\nend for\n\nAlgorithm 3:\n\nMetropolis within Gibbs sampler. We\n\nin a random way.\n\nRR n\u00060123456789\n\nan go through the\n\nomponent indi es\n\n\f26\n\nF. Campillo & V. Rossi\n\nReferen es\n\n[1\u2104 Stephen P. Brooks and Gareth O. Roberts. Convergen e assessment te hniques for\nMarkov hain Monte Carlo. Statisti s and Computing, 8(4):319\u0015335, 1998.\n[2\u2104 Olivier Capp\u00e9, Arnaud Guillin, Jean-Mi hel Marin, and Christian P. Robert. Population\nMonte Carlo. Journal of Computational and Graphi al Statisti s, 13(4):907\u0015929, 2004.\n[3\u2104 Chang-Tai Chao. Markov Chain Monte Carlo on optimal adaptive sampling sele tion.\nEnvironmental and E ologi al Statisti s, 10(1):129 \u0015 151, 2003.\n[4\u2104 Didier Chauveau and Pierre Vandekerkhove. Algorithmes de Hastings-Metropolis en\nintera tion. Comptes Rendus de l'A ad\u00e9mie des S ien es, S\u00e9rie I, Math\u00e9matique,\n333(9):881\u0015884, 2001.\n[5\u2104 Didier Chauveau and Pierre Vandekerkhove. Improving onvergen e of Hastings\u0015\nMetropolis algorithm with an adaptive proposal. S andinavian Journal of Statisti s,\n29(1):13\u001529, Mar h 2002.\n[6\u2104 Mary Kathryn Cowles and Bradley P. Carlin. Markov hain Monte Carlo onvergen e\ndiagnosti s: a omparative review. Journal of the Ameri an Statisti al Asso iation,\n91(434):883\u0015904, 1996.\n[7\u2104 Madalina M. Drugan and Dirk Thierens. Evolutionary Markov Chain Monte Carlo.\nLe ture Notes in Computer S ien e, 2936:63\u001576, 2004.\n[8\u2104 Madalina M. Drugan and Dirk Thierens. Re ombinative EMCMC algorithms. In IEEE\nCongress on Evolutionary Computation, pages 2024\u0015 2031, 2005.\n[9\u2104 Andrew Gelman and Donald B. Rubin. Inferen e from iterative simulation using multiple sequen es (with dis ussion). Statisti al S ien e, 7:457\u0015511, 1992.\n[10\u2104 Charles J. Geyer. Markov hain monte arlo maximum likelihood. In E.M. Keramidas,\neditor, Computing S ien e and Statisti s: Pro eedings of the 23rd Symposium on the\nInterfa e, 1991.\n[11\u2104 Charles J. Geyer. Pra ti al Markov hain Monte Carlo (with dis ussion). Statisti al\nS ien e, 4:473\u0015482, 1992.\n[12\u2104 Walter R. Gilks, Sylvia Ri hardson, and David J. Spiegelhalter, editors. Markov Chain\nMonte Carlo in pra ti e. Chapman & Hall, London, 1995.\n[13\u2104 Walter R. Gilks and Gareth O. Roberts. Strategies for improving MCMC. In W.R.\nGilks, S. Ri hardson, and D.J. Spiegelhalter, editors, Markov Chain Monte Carlo in\npra ti e. Chapman & Hall, 1995.\n[14\u2104 Yukito Iba. Population Monte Carlo algorithms. Transa tions of the Japanese So iety\nfor Arti\u001c ial Intelligen e, 16(2):279\u0015286, 2001.\n\nINRIA\n\n\fParallel and intera ting MCMC's\n\n27\n\n[15\u2104 Robert E. Kass, Bradley P. Carlin, Andrew Gelman, and Radford M. Neal. Markov\nChain Monte Carlo in pra ti e: A roundtable dis ussion. The Ameri an Statisti ian,\n52:93\u0015100, 1998.\n[16\u2104 Kathryn B. Laskey and James W. Myers. Population Markov Chain Monte Carlo.\nMa hine Learning, 50(1-2):175\u0015196, 2003.\n[17\u2104 Kerrie L. Mengersen and Christian P. Robert. Population Markov Chain Monte Carlo:\nthe pinball sampler. In J.O. Berger, A.P. Dawid, and A.F.M. Smith, editors, Bayesian\nStatisti s 7. Oxford University Press, 2003.\n[18\u2104 Christian P. Robert. M\u00e9thodes de Monte Carlo par Cha\u00eenes de Markov. E onomi a,\nParis, 1996.\n[19\u2104 Luke Tierney. Markov hains for exploring posterior distributions (with dis ussion).\nThe Annals of Statisti s, 22(4):1701\u00151728, De ember 1994.\n[20\u2104 Luke Tierney. A note on Metropolis-Hastings kernels for general state spa es. The\nAnnals of Applied Probability, 8(1):1\u00159, 1998.\n\nRR n\u00060123456789\n\n\fUnit\u00e9 de recherche INRIA Lorraine, Technop\u00f4le de Nancy-Brabois, Campus scientifique,\n615 rue du Jardin Botanique, BP 101, 54600 VILLERS L\u00c8S NANCY\nUnit\u00e9 de recherche INRIA Rennes, Irisa, Campus universitaire de Beaulieu, 35042 RENNES Cedex\nUnit\u00e9 de recherche INRIA Rh\u00f4ne-Alpes, 655, avenue de l'Europe, 38330 MONTBONNOT ST MARTIN\nUnit\u00e9 de recherche INRIA Rocquencourt, Domaine de Voluceau, Rocquencourt, BP 105, 78153 LE CHESNAY Cedex\nUnit\u00e9 de recherche INRIA Sophia-Antipolis, 2004 route des Lucioles, BP 93, 06902 SOPHIA-ANTIPOLIS Cedex\n\n\u00c9diteur\nINRIA, Domaine de Voluceau, Rocquencourt, BP 105, 78153 LE CHESNAY Cedex (France)\nhttp://www.inria.fr\n\nISSN 0249-6399\n\n\f\f\f\f\f\f"}