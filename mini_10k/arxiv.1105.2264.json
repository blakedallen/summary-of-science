{"id": "http://arxiv.org/abs/1105.2264v1", "guidislink": true, "updated": "2011-05-11T17:46:15Z", "updated_parsed": [2011, 5, 11, 17, 46, 15, 2, 131, 0], "published": "2011-05-11T17:46:15Z", "published_parsed": [2011, 5, 11, 17, 46, 15, 2, 131, 0], "title": "Distributed Semantic Web Data Management in HBase and MySQL Cluster", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1105.0632%2C1105.4318%2C1105.5808%2C1105.4862%2C1105.5648%2C1105.5458%2C1105.3628%2C1105.0198%2C1105.4114%2C1105.1581%2C1105.2227%2C1105.4928%2C1105.3301%2C1105.3544%2C1105.6216%2C1105.5558%2C1105.3135%2C1105.0253%2C1105.0351%2C1105.3729%2C1105.3475%2C1105.2835%2C1105.4658%2C1105.5512%2C1105.0696%2C1105.0823%2C1105.2467%2C1105.1150%2C1105.5014%2C1105.5616%2C1105.1346%2C1105.0957%2C1105.1438%2C1105.2940%2C1105.5075%2C1105.2154%2C1105.2806%2C1105.5198%2C1105.1311%2C1105.5311%2C1105.4378%2C1105.2084%2C1105.2382%2C1105.4983%2C1105.6186%2C1105.2518%2C1105.0928%2C1105.5127%2C1105.6291%2C1105.5524%2C1105.1621%2C1105.4132%2C1105.2130%2C1105.6121%2C1105.4340%2C1105.0952%2C1105.1647%2C1105.5305%2C1105.6067%2C1105.3136%2C1105.5163%2C1105.2264%2C1105.0968%2C1105.1555%2C1105.0786%2C1105.1014%2C1105.2218%2C1105.2976%2C1105.4525%2C1105.0541%2C1105.6261%2C1105.1983%2C1105.5438%2C1105.0679%2C1105.0008%2C1105.3932%2C1105.3100%2C1105.2929%2C1105.0108%2C1105.2790%2C1105.1728%2C1105.1955%2C1105.4598%2C1105.4226%2C1105.1767%2C1105.4240%2C1105.1220%2C1105.5176%2C1105.0544%2C1105.0402%2C1105.1409%2C1105.2640%2C1105.4841%2C1105.1874%2C1105.1153%2C1105.0708%2C1105.0256%2C1105.3481%2C1105.5806%2C1105.0516%2C1105.0888&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Distributed Semantic Web Data Management in HBase and MySQL Cluster"}, "summary": "Various computing and data resources on the Web are being enhanced with\nmachine-interpretable semantic descriptions to facilitate better search,\ndiscovery and integration. This interconnected metadata constitutes the\nSemantic Web, whose volume can potentially grow the scale of the Web. Efficient\nmanagement of Semantic Web data, expressed using the W3C's Resource Description\nFramework (RDF), is crucial for supporting new data-intensive,\nsemantics-enabled applications. In this work, we study and compare two\napproaches to distributed RDF data management based on emerging cloud computing\ntechnologies and traditional relational database clustering technologies. In\nparticular, we design distributed RDF data storage and querying schemes for\nHBase and MySQL Cluster and conduct an empirical comparison of these approaches\non a cluster of commodity machines using datasets and queries from the Third\nProvenance Challenge and Lehigh University Benchmark. Our study reveals\ninteresting patterns in query evaluation, shows that our algorithms are\npromising, and suggests that cloud computing has a great potential for scalable\nSemantic Web data management.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1105.0632%2C1105.4318%2C1105.5808%2C1105.4862%2C1105.5648%2C1105.5458%2C1105.3628%2C1105.0198%2C1105.4114%2C1105.1581%2C1105.2227%2C1105.4928%2C1105.3301%2C1105.3544%2C1105.6216%2C1105.5558%2C1105.3135%2C1105.0253%2C1105.0351%2C1105.3729%2C1105.3475%2C1105.2835%2C1105.4658%2C1105.5512%2C1105.0696%2C1105.0823%2C1105.2467%2C1105.1150%2C1105.5014%2C1105.5616%2C1105.1346%2C1105.0957%2C1105.1438%2C1105.2940%2C1105.5075%2C1105.2154%2C1105.2806%2C1105.5198%2C1105.1311%2C1105.5311%2C1105.4378%2C1105.2084%2C1105.2382%2C1105.4983%2C1105.6186%2C1105.2518%2C1105.0928%2C1105.5127%2C1105.6291%2C1105.5524%2C1105.1621%2C1105.4132%2C1105.2130%2C1105.6121%2C1105.4340%2C1105.0952%2C1105.1647%2C1105.5305%2C1105.6067%2C1105.3136%2C1105.5163%2C1105.2264%2C1105.0968%2C1105.1555%2C1105.0786%2C1105.1014%2C1105.2218%2C1105.2976%2C1105.4525%2C1105.0541%2C1105.6261%2C1105.1983%2C1105.5438%2C1105.0679%2C1105.0008%2C1105.3932%2C1105.3100%2C1105.2929%2C1105.0108%2C1105.2790%2C1105.1728%2C1105.1955%2C1105.4598%2C1105.4226%2C1105.1767%2C1105.4240%2C1105.1220%2C1105.5176%2C1105.0544%2C1105.0402%2C1105.1409%2C1105.2640%2C1105.4841%2C1105.1874%2C1105.1153%2C1105.0708%2C1105.0256%2C1105.3481%2C1105.5806%2C1105.0516%2C1105.0888&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Various computing and data resources on the Web are being enhanced with\nmachine-interpretable semantic descriptions to facilitate better search,\ndiscovery and integration. This interconnected metadata constitutes the\nSemantic Web, whose volume can potentially grow the scale of the Web. Efficient\nmanagement of Semantic Web data, expressed using the W3C's Resource Description\nFramework (RDF), is crucial for supporting new data-intensive,\nsemantics-enabled applications. In this work, we study and compare two\napproaches to distributed RDF data management based on emerging cloud computing\ntechnologies and traditional relational database clustering technologies. In\nparticular, we design distributed RDF data storage and querying schemes for\nHBase and MySQL Cluster and conduct an empirical comparison of these approaches\non a cluster of commodity machines using datasets and queries from the Third\nProvenance Challenge and Lehigh University Benchmark. Our study reveals\ninteresting patterns in query evaluation, shows that our algorithms are\npromising, and suggests that cloud computing has a great potential for scalable\nSemantic Web data management."}, "authors": ["Craig Franke", "Samuel Morin", "Artem Chebotko", "John Abraham", "Pearl Brazier"], "author_detail": {"name": "Pearl Brazier"}, "author": "Pearl Brazier", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/CLOUD.2011.19", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1105.2264v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1105.2264v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "In Proc. of the 4th IEEE International Conference on Cloud Computing\n  (CLOUD'11)", "arxiv_primary_category": {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.PF", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1105.2264v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1105.2264v1", "journal_reference": null, "doi": "10.1109/CLOUD.2011.19", "fulltext": "Distributed Semantic Web Data Management in HBase and MySQL Cluster\nCraig Franke, Samuel Morin, Artem Chebotko \u2020 , John Abraham, and Pearl Brazier\nDepartment of Computer Science\nUniversity of Texas - Pan American\n1201 West University Drive, Edinburg, TX 78539-2999, USA\n\u2020\nCorresponding author. Email: artem@cs.panam.edu\n\nAbstract-Various computing and data resources on the Web\nare being enhanced with machine-interpretable semantic descriptions to facilitate better search, discovery and integration.\nThis interconnected metadata constitutes the Semantic Web,\nwhose volume can potentially grow the scale of the Web.\nEfficient management of Semantic Web data, expressed using\nthe W3C's Resource Description Framework (RDF), is crucial\nfor supporting new data-intensive, semantics-enabled applications. In this work, we study and compare two approaches to\ndistributed RDF data management based on emerging cloud\ncomputing technologies and traditional relational database\nclustering technologies. In particular, we design distributed\nRDF data storage and querying schemes for HBase and\nMySQL Cluster and conduct an empirical comparison of these\napproaches on a cluster of commodity machines using datasets\nand queries from the Third Provenance Challenge and Lehigh\nUniversity Benchmark. Our study reveals interesting patterns\nin query evaluation, shows that our algorithms are promising,\nand suggests that cloud computing has a great potential for\nscalable Semantic Web data management.\nKeywords-Semantic Web; cloud computing; distributed\ndatabase; SPARQL; SQL; RDF; query; performance; scalability; HBase; MySQL Cluster\n\nI. I NTRODUCTION\nThe World Wide Web Consortium (W3C) has recommended and standardized a number of principles, languages,\nframeworks and best practices to interconnect various metadata into a next-generation web \u2013 the Semantic Web. The\nW3C's metadata acquisition languages include Resource\nDescription Framework (RDF), RDF in attributes (RDFa),\nRDF Schema (RDFS), and Web Ontology Language (OWL).\nGovernment, academia, and industry actively embrace these\ntechnologies for capturing and sharing metadata on the\nSemantic Web. Just to name a few examples, oeGOV is\nmaking and publishing OWL ontologies for e-Government,\nU.S. census data is being published in RDF, bioinformaticians maintain the Universal Protein Resource (UniProt)\nin RDF, geoscientists publish worldwide geographical RDF\ndatabase GeoNames, the largest electronics retailer in the\nU.S., BestBuy, publishes its full catalog in RDF, the largest\nsocial networking provider in the U.S., Facebook, embeds\nmetadata in its webpages using RDFa, and the services\ncomputing community enhances existing Web services with\nsemantic annotations using vocabularies, such as Semantic\n\nMarkup for Web Services (OWL-S), Web Service Semantics\n(WSDL-S), and Semantic Web Services Ontology (SWSO).\nThe RDF data model is a directed, labeled graph that can\nalso be serialized and viewed as a set of triples. A running\nexample in this paper includes 10 triples that describe the\nauthors using the Lehigh University Benchmark (LUBM)\nvocabulary [1] as shown in Fig. 1. Each triple consists of\na subject, predicate, and object and defines a relationship\nbetween a subject and an object. In the figure, <> and \"\"\ndenote resource identifiers and literals of some data type,\nrespectively. For example, the first three triples state that a\nresource with identifier C is a Student, has name Craig and is\na member of IEEE. This sample dataset can be queried using\nSPARQL \u2013 a standard query language for RDF. SPARQL\nuses triple patterns and graph patterns that are matched over\nRDF data. For example, query Q14 from LUBM contains\none triple pattern ?X <type> <UndergraduateStudent> that\nreturns all undergraduate student identifiers as bindings of\nvariable ?X. More details on SPARQL features and semantics can be found in the W3C's SPARQL specification.\nWith the rapid growth of the Semantic Web and\nwidespread use of RDF as the primary language for metadata, efficient management of RDF data will become crucial for supporting new semantics-enabled applications in\nvarious domains. Many researchers have proposed using\nrelational databases to store and query large RDF datasets.\nSuch systems, called relational RDF databases or relational\nRDF stores [2], are now frequently in production. More\nrecently, distributed technologies that are often used in cloud\ncomputing, such as Hadoop1 and HBase2 , are being explored\nfor distributed and scalable RDF data management [3],\n[4]. To our best knowledge, this work provides the first\nperformance comparison of the two worlds using our design\nand algorithmic solutions for storing and querying RDF data\nin HBase and MySQL Cluster.\nThe main contributions of this paper are: (i) a novel\ndatabase schema design for storing RDF data in HBase,\n(ii) efficient algorithms for SPARQL triple and basic graph\npattern matching in HBase according to our schema, (iii) efficient SPARQL-to-SQL translation algorithm that results\n1 Apache\n2 Apache\n\nHadoop, http://hadoop.apache.org\nHBase, http://hbase.apache.org\n\n\f<C>\n<C>\n<C>\n<S>\n<S>\n<S>\n<A>\n<A>\n<A>\n<A>\n\n<type>\n<name>\n<memberOf>\n<type>\n<name>\n<memberOf>\n<type>\n<name>\n<memberOf>\n<memberOf>\n\nFigure 1.\n\n<Student>\n\"Craig\"\n<IEEE>\n<Student>\n\"Sam\"\n<ACM>\n<Faculty>\n\"Artem\"\n<IEEE>\n<ACM>\n\nSample RDF triples.\n\nin flat SQL queries over our schema in MySQL Cluster,\nand (iv) empirical comparison of the proposed HBase and\nMySQL Cluster approaches for efficient and scalable storing\nand querying of Semantic Web data. Our work reveals\ninteresting patterns in query evaluation, shows that our\nalgorithms are promising, and suggests that cloud computing has a great potential for scalable Semantic Web data\nmanagement.\nThe organization of this paper is as follows. Related work\nis discussed in Section II. Our design and algorithms for\ndistributed RDF data storage and querying in HBase and\nMySQL Cluster are presented in Sections III and IV, respectively. The performance study of the two approaches using\ndatasets and queries from the Third Provenance Challenge\nand Lehigh University Benchmark is reported in Section V.\nFinally, our concluding remarks are given in Section VI.\nII. R ELATED W ORK\nBesides HBase, which is an open-source implementation\nof Google's Bigtable [5], there are multiple projects under\nthe Apache umbrella that focus on distributed computing,\nincluding Hadoop, Cassandra, Hive, Pig, and CouchDB.\nHadoop implements a MapReduce software framework and\na distributed file system. Cassandra blends a fully distributed\ndesign with a column-oriented storage model and supports\nMapReduce as one of its features. Hive deals with data\nwarehousing on top of Hadoop and provides its own Hive\nQL query language. Pig is geared towards analyzing large\ndatasets through use of its high-level Pig Latin language for\nexpressing data analysis programs, which are then turned\ninto MapReduce jobs. CouchDB is a distributed, documentoriented, non-relational database that supports incremental\nMapReduce queries written in JavaScript. Along the same\nlines, other projects in academia and industry include Cheetah, Hadoop++, G-Store, and HadoopDB.\nSeveral related works on distributed RDF data management are briefly discussed in the following. Techniques for\nevaluating SPARQL basic graph patterns using MapReduce\nare presented in [3] and [6]. Efficient approaches to analytical query processing and distributed reasoning on RDF\ngraphs in MapReduce-based systems are proposed in [7]\nand [8], respectively. RDF query processing in peer-to-peer\nenvironments is studied in [9] and [10], and mediation\ntechniques for federated querying of distributed RDF sources\nare reported in [11] and [12]. Use of HBase for text indexing\n\nis described in [13]. While the SPIDER system [14] that uses\nHBase for RDF query processing and the HBase extension\nfor Jena3 are announced, no details are reported. Finally,\nour previous work [4] presents our initial findings on RDF\ndata management in HBase. This paper, when compared to\n[4], proposes new, more effective HBase database schema\ndesign, more efficient algorithms for SPARQL triple and\nbasic graph pattern matching, and an empirical comparison\nwith a distributed relational RDF database. Our experimental\ncomparison with [4] (not reported in the paper) showed\nseveral orders of magnitude speedup for some queries and\nsubstantial improvements in scalability. To our best knowledge, this paper and our previous paper [4] are the first\npublished research works on Semantic Web data management in HBase. Our comparison of RDF data management\ntechniques in HBase and MySQL Cluster is also unique.\nIII. D ISTRIBUTED RDF DATA S TORAGE AND Q UERYING\nIN HBASE\nHBase stores data in tables that can be described as sparse\nmultidimensional sorted maps and are structurally different\nfrom relations found in conventional relational databases. An\nHBase table (hereafter \"table\" for short) stores data rows that\nare sorted based on the row keys. Each row has a unique row\nkey and an arbitrary number of columns, such that columns\nin two distinct rows do not have to be the same. A full\ncolumn name (hereafter \"column\" for short) consists of a\ncolumn family and a column qualifier (e.g., family:qualifier),\nwhere column families are usually specified at the time of\ntable creation and their number does not change and column\nqualifiers are dynamically added or deleted as needed. A\ncolumn of a given row, which we denote as table cell, can\nstore a list of timestamp-value pairs, where timestamps are\nunique in the cell scope and values may contain duplicates.\nRows in a table can be distributed over different machines in\nan HBase cluster and searched using two basic operations:\n(1) table scan and (2) retrieval of row data based on a given\nrow key and, if available, columns and timestamps. Given\nthat the table scan access path is inefficient for large datasets,\nthe row key-based retrieval is the best feasible choice.\nThe sparse nature of tables makes them an attractive storage alternative for RDF data. RDF graphs are usually sparse\nas well: different resources are annotated with different\nproperties and some annotations may not be stated explicitly\ndue to inference. To support efficient retrieval of RDF data\nfrom tables in HBase, the basic querying constructs of\nSPARQL, such as triple patterns, should be considered. At\nthe very minimum, the database should support retrieval of\nRDF triples based on values of their subjects, predicates,\nobjects, and their arbitrary combination.\nWe propose to use a database schema with two tables to\nstore RDF triples as shown in Fig. 2. Table Tsp stores triple\n3 HBase\nGraph\nfor\nJena,\nhttp://cs.utdallas.edu/semanticweb/\nHBase-Extension/hbase-extension.html\n\n\fs\n<C>\n<S>\n<A>\n\np:type\n{<Student>}\n{<Student>}\n{<Faculty>}\n\no\n<Student>\n<Faculty>\n\"Craig\"\n\"Sam\"\n\"Artem\"\n<IEEE>\n<ACM>\n\nFigure 2.\n\nTsp\np:name\n{\"Craig\"}\n{\"Sam\"}\n{\"Artem\"}\n\np:type\n{<C>,<S>}\n{<A>}\n\nTop\np:name\n{<C>}\n{<S>}\n{<A>}\n\np:memberOf\n{<IEEE>}\n{<ACM>}\n{<IEEE>,<ACM>}\n\np:memberOf\n\n{<C>,<A>}\n{<S>,<A>}\n\n...\n...\n...\n...\n\n...\n...\n...\n...\n...\n...\n...\n...\n\nStorage schema and sample instance in HBase.\n\nsubjects as row keys, triple predicates as column names and\ntriple objects as cell values. Table Top stores triple objects\nas row keys, triple predicates as column names and triple\nsubjects as cell values. Fig. 2 shows a two-dimensional\ngraphical representation of these tables with our sample RDF\ntriples (see Fig. 1) stored. In the figure, s and o denote row\nkeys rather than columns; type, name, and memberOf are\ncolumn qualifiers that belong to the same column family\np; { } denote sets of cell values with timestamps omitted.\nMore precisely, the structure of the rows can be shown using\nJavaScript Object Notation (JSON):\n//the first row of Tsp\n<C>: {\np: {\ntype:\n{\nname:\n{\nmemberOf: {\n}\n}\n//the first row of Top\n<Student>: {\np: {\ntype:\n{\n}\n\n}\n\nt1 : <Student> },\nt2 : \"Craig\" },\nt3 : <IEEE> }\n\nt4 : <C>,\nt5 : <S> }\n\nIn the first row of Tsp , <C> is a row key, p is a column\nfamily, type, name, and memberOf are column qualifiers,\nt1 , t2 , and t3 are timestamps, and the rest are values. The\nstructure of the first row of Top can be interpreted in a\nsimilar way but it should be noted that, while the graphical\nrepresentation in Fig. 2 shows blank values for some table\ncells, the row contains no information about such values or\nthe respective columns. This illustrates the sparse storage\nnature of HBase tables and shows that no space is wasted.\nThe proposed schema requires that RDF data is stored\ntwice - replication that contributes to the robustness of\nthe system. Tables Tsp and Top can be used to efficiently\nretrieve triples with known subjects and objects, respectively.\nRetrieval of triples based on a predicate value requires a scan\nof one of the tables, which may not be efficient. To try to\nremedy this problem, we could have created a table, i.e., Tps\nor Tpo , with predicates as row keys and subjects or objects\nas columns. However, such a solution can only provide\nmarginal improvements, since the number of predicates in an\n\nontology is usually fixed and relatively small, which implies\nthat this new table can contains only a small number of\nlarge rows (one per distinct predicate) and retrieval of any\nindividual row is still expensive.\nFor HBase to be able to evaluate SPARQL queries, we\ndesign three functions that deal with triple patterns and basic\ngraph patterns.\nOur first function, matchTP-T, is a general-purpose function that depends on neither our storage schema nor HBase.\nmatchTP-T takes a triple pattern tp and a triple t and returns\ntrue if they match or false otherwise. Its pseudocode is\noutlined in [4].\nFunction matchTP-DB as outlined in Algorithm 1 is\nused to match a triple pattern tp in an HBase database\nDB according to our storage schema with two tables. The\noutput of this function is a bag (multi-set) B that holds all\nmatching triples in the database. The algorithm deals with\nthree disjoint cases. First, if tp's subject pattern is not a\nvariable, the function retrieves matching triples from table\nTsp , such that a row with key tp.sp is accessed. If tp.pp is\nnot a variable, only values in the column with qualifier tp.pp\nare retrieved for this row; otherwise, all columns must be\nretrieved. Triples are reconstructed from row keys, column\nqualifiers, and cell values and are placed into B. Since tp.op\nmay not be a variable or it may be a variable that occurs\ntwice in the triple pattern, matchTP-T is applied on all the\ntriples to eliminate non-matching ones. After this filtering,\ntriples in B are returned. Second, if tp's object pattern is\nnot a variable, the function retrieves matching triples from\ntable Top using a similar strategy. Finally, when both tp.sp\nand tp.op are variables, one of the tables must be scanned\nto retrieve all rows. If tp.pp is not a variable, non-matching\ncolumns are discarded; otherwise, values in all columns are\nused.\nOur last function matchBGP-DB is outlined in Algorithm 2. It matches a SPARQL basic graph pattern bgp that\nconsists of a set of triple patterns tp1 , tp2 , ..., tpn over\nan HBase database and returns a relation with a bag B of\ngraphs constituted by matching triples. The algorithm starts\nby ordering triple patterns in bgp using two criteria: (1) triple\npatterns that yield a smaller result should be evaluated first\nto decrease a number of iterations and (2) triple patterns that\nhave a shared variable with preceding triple patterns should\nbe given a preference over triple patterns with no shared\nvariables to avoid unnecessary Cartesian products. As an\nexample, consider the following query from LUBM [1] and\nits reordered version:\n//original query Q7 from LUBM\ntp1 : ?X <type> <Student> .\ntp2 : ?Y <type> <Course> .\ntp3 : <http://...Professor0> <teacherOf> ?Y .\ntp4 : ?X <takesCourse> ?Y .\n//reordered basic graph pattern\ntp3 : <http://...Professor0> <teacherOf> ?Y .\ntp2 : ?Y <type> <Course> .\ntp4 : ?X <takesCourse> ?Y .\ntp1 : ?X <type> <Student> .\n\n\fAlgorithm 1 Matching a triple pattern over a database\n1:\n2:\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n\nfunction matchTP-DB\ninput: triple pattern tp = (sp, pp, op), database DB = {Tsp , Top }\noutput: bag of triples B(sp,pp,op) = {t\u2223t is in DB \u2227 t matches tp}\nB=\u00f8\nif tp.sp is not a variable then\nif tp.pp is not a variable then\nRetrieve triples into bag B from Tsp where row key s = tp.sp using\ncolumn tp.pp\nelse\nRetrieve triples into bag B from Tsp where row key s = tp.sp using\nall columns\nend if\nRemove any triple t \u2208 B from B if matchTP-T(tp, t) = false\nreturn B\nend if\n\n10:\n11:\n12:\n13:\n14: if tp.op is not a variable then\n15:\nif tp.pp is not a variable then\n16:\nRetrieve triples into bag B from Top where row key o = tp.op using\n17:\n18:\n19:\n20:\n21:\n22:\n23:\n24:\n25:\n26:\n27:\n28:\n29:\n30:\n\ncolumn tp.pp\nelse\nRetrieve triples into bag B from Top where row key o = tp.op using\nall columns\nend if\nRemove any triple t \u2208 B from B if matchTP-T(tp, t) = false\nreturn B\nend if\nif tp.pp is not a variable then\nRetrieve triples into bag B from Tsp (or Top ) using column tp.pp\nelse\nRetrieve triples into bag B from Tsp (or Top ) using all columns\nend if\nRemove any triple t \u2208 B from B if matchTP-T(tp, t) = false\nreturn B\nend function\n\nThe order in the original query does not satisfy the desired\ncriteria: tp1 yields a large result set with all students across\nall universities in a dataset; tp2 has no shared variables\nwith tp1 and a memory-expensive Cartesian product must\nbe computed between tp1 's and tp2 's results. The reordered\nquery can save both memory and network transfer time:\nnot only is tp3 , the triple pattern with the smallest result,\nplaced at the first position, but the Cartesian product is also\neliminated.\nNext, the algorithm evaluates the first triple pattern in\nordered bgp using matchTP-DB. If the result in B is empty,\nthe algorithm returns an empty result without evaluating subsequent triple patterns. Otherwise, matchBGP-DB iterates\nover other triple patterns computing either joins on shared\nvariables or Cartesian products if no shared variables exist.\nEach join resembles the index-nested-loops join strategy\nknown in relational databases. Instead of directly evaluating\ntriple pattern tpi using matchTP-DB, shared variables are\nfirst substituted with their bindings found in B and the\nresulting triple patterns tp\u2032 in set T P are evaluated using\nmatchTP-DB. If tp\u2032 yields a non-empty result, triples\nin B \u2032 are concatenated with the corresponding triples in\nB; otherwise, previous solutions from B whose variable\nbindings were used in variable substitution to obtain tp\u2032 are\nremoved as the join condition has failed.\nOther SPARQL constructs, such as projection (SELECT),\nfiltering (FILTER), alternative graph patterns (UNION), and\n\nAlgorithm 2 Matching a basic graph pattern over a database\n1: function matchBGP-DB\n2: input: basic graph pattern bgp = {tp1 , tp2 , . . . , tpn\u22121 , tpn } and n \u2265 1,\ndatabase DB = {Tsp , Top }\n\n3: output: bag of tuples B(tp1 .sp,tp1 .pp,tp1 .op,tp2 .sp,...) = {g\u2223g is a graph\nin DB \u2227 g matches bgp}\n\n4: B = \u00f8\n5: Order triple patterns in bgp, such that triple patterns that yield a smaller result\n6:\n7:\n8:\n9:\n10:\n11:\n12:\n13:\n14:\n15:\n16:\n17:\n18:\n19:\n20:\n21:\n22:\n23:\n24:\n25:\n26:\n27:\n\nand triple patterns that have a shared variable with preceding triple patterns\nshould be evaluated first.\nLet ordered bgp = (tp1 , tp2 , ..., tpn )\nB = matchTP-DB(tp1 , DB)\nif B = \u00f8 then return B end if\nfor each tpi in (tp2 , ..., tpn ) do\nif tpi has shared variables with tpi\u22121 , ..., tp1 then\nLet T P be a set of triple patterns obtained by substituting shared\nvariables with their respective bindings from B\nfor each tp\u2032 in T P do\nB \u2032 = matchTP-DB(tp\u2032 , DB)\nif B \u2032 \u2215= \u00f8 then\nAdd triples in B \u2032 to B by concatenating each triple t\u2032 \u2208 B \u2032\nwith every tuple t \u2208 B if t's bindings were used in variable\nsubstitution to obtain tp\u2032\nelse\nRemove any tuple t from B if t's bindings were used in variable\nsubstitution to obtain tp\u2032\nif B = \u00f8 then return B end if\nend if\nend for\nelse\nB \u2032 = matchTP-DB(tpi , DB)\nCompute Cartesian product of B and B \u2032 , i.e. B = B \u00d7 B \u2032\nend if\nend for\nreturn B\nend function\n\noptional graph patterns (OPTIONAL) can be incorporated in\nthe presented algorithmic framework, but is out of this paper\nscope.\nIV. D ISTRIBUTED RDF DATA S TORAGE AND Q UERYING\nIN M Y SQL C LUSTER\nRelational RDF databases use several approaches to\ndatabase schema generation that include schema-oblivious,\nschema-aware, data-driven, and hybrid strategies [15]. These\napproaches feature various database relations, such as property, class, class-subject, class-object, and clustered property\ntables. In this work, we use a schema-oblivious approach\nthat employs a generic schema with a single table T (s, p, o),\nwhere columns s, p, and o store triple subjects, predicates,\nand objects, respectively. Fig. 3 shows table T with our\nsample RDF triples (see Fig. 1) stored.\nOur rationale for choosing this schema is threefold. First,\nit can support ontology evolution with no schema modifications. The schema proposed for HBase is also very flexible\nas only column qualifiers may dynamically change and\nsuch changes are performed on the row level. Second, most\nmentioned tables employed by relational RDF databases can\nbe viewed as a result of horizontal partitioning of table\nT . However, partitioning is already performed by MySQL\nCluster automatically. Finally, this schema allows lossless\nstorage and is easy to implement. In particular, it greatly\nsimplifies SPARQL-to-SQL translation that is required to\n\n\fs\n<C>\n<C>\n<C>\n<S>\n<S>\n<S>\n<A>\n<A>\n<A>\n<A>\n\nFigure 3.\n\nT\np\n<type>\n<name>\n<memberOf>\n<type>\n<name>\n<memberOf>\n<type>\n<name>\n<memberOf>\n<memberOf>\n\no\n<Student>\n\"Craig\"\n<IEEE>\n<Student>\n\"Sam\"\n<ACM>\n<Faculty>\n\"Artem\"\n<IEEE>\n<ACM>\n\nStorage schema and sample instance in MySQL Cluster.\n\nquery stored RDF data.\nTo execute SPARQL queries over our database schema\nin MySQL Cluster, we present a SPARQL-to-SQL query\ntranslation algorithm for basic graph patterns. The algorithm\nis based on our previous work [15] on semantics-preserving\nSPARQL-to-SQL translation, but it is optimized to generate\nflat SQL queries. Query flattening (vs. nesting) removes a\nconcern of triple pattern reordering in basic graph patterns\nsince a relational query optimizer is capable of selecting a\n\"good\" join execution order automatically.\nAlgorithm 3 Translation of SPARQL basic graph patterns\nto flat SQL queries\n1: function BGPtoFlatSQL\n2: input: basic graph pattern bgp = {tp1 , tp2 , . . . , tpn\u22121 , tpn } and n \u2265 1,\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n12:\n13:\n14:\n15:\n16:\n17:\n18:\n19:\n20:\n21:\n22:\n23:\n24:\n25:\n26:\n27:\n28:\n29:\n30:\n31:\n32:\n33:\n34:\n35:\n36:\n37:\n\ndatabase DB = {T }\noutput: flat SQL query\nAssign a unique alias ai to each triple pattern tpi \u2208 bgp\nselect = \"\"; f rom = \"\"; where = \"\"\n//Construct the SQL From clause:\nfor each tpi \u2208 bgp do\nf rom += \"T $ai , \"\nend for\n//Construct an inverted index (hash) h on variables in bgp:\nfor each tpi \u2208 bgp do\nfor each variable ?v found in tpi do\nLet p be \"s\", \"p\", or \"o\" if ?v is at the subject, predicate, or object\nposition, respectively, in tpi\nh(?v) = h(?v) \u222a {\"$ai .$p\"}\nend for\nend for\n//Construct the SQL Where clause:\nfor each tpi \u2208 bgp do\nfor each instance or literal l found in tpi do\nLet p be \"s\", \"p\", or \"o\" if l is at the subject, predicate, or object position,\nrespectively, in tpi\nwhere += \"$ai .$p = '$l' And \"\nend for\nend for\nfor each distinct variable ?v found in bgp and \u2223h(?v)\u2223 > 1 do\nLet x \u2208 h(?v)\nfor each y \u2208 h(?v) and y \u2215= x do\nwhere += \"$x = $y And \"\nend for\nend for\n//Construct the SQL Select clause:\nfor each distinct variable ?v found in bgp do\nLet x \u2208 h(?v)\nLet m is the name of variable ?v\nselect += \"$x As $m, \"\nend for\nreturn \"Select $select From $f rom Where $where\"\nend function\n\nThe BGPtoFlatSQL function is outlined in Algorithm 3.\nIt translates a SPARQL basic graph pattern bgp that consists\n\nof a set of triple patterns tp1 , tp2 , ..., tpn into an equivalent\nflat SQL query that can be executed over a MySQL Cluster database with our schema. BGPtoFlatSQL constructs\nf rom, where, and select clauses of an SQL query as\nfollows. For each triple pattern in bgp, a unique table alias\nis assigned and table T with this alias is appended to the\nf rom clause. The algorithm then computes an inverted\nindex on all variables in bgp, such that each distinct variable\nis associated with attributes in the respective tables from\nthe f rom clause. The where clause is first constructed to\nensure that any non-variables in bgp are restricted to their\nvalues (e.g., literals or identifiers). The inverted index is\nthen used to append join conditions into the where clause,\nsuch that all attributes that correspond to the same variable\nmust be equal. Finally, the select clause is generated to\ninclude attributes that correspond to every distinct variable\nin bgp, with attributes being renamed as variable names.\nThe following example illustrates the result of a translation\nperformed with BGPtoFlatSQL:\n//input SPARQL query Q7 from LUBM\ntp1 : ?X <type> <Student> .\ntp2 : ?Y <type> <Course> .\ntp3 : <http://...Professor0> <teacherOf> ?Y .\ntp4 : ?X <takesCourse> ?Y .\n//output equivalent SQL query\nSelect tp1 .s As X, tp2 .s As Y\nFrom T tp1 , T tp2 , T tp3 , T tp4\nWhere tp1 .p = '<type>' And\ntp1 .o = '<Student>' And\ntp2 .p = '<type>' And\ntp2 .o = '<Course>' And\ntp3 .s = '<http://...Professor0>' And\ntp3 .p = '<teacherOf>' And\ntp4 .p = '<takesCourse>' And\ntp1 .s = tp4 .s And tp2 .s = tp3 .o And\ntp2 .s = tp4 .o\n\nTranslation of other SPARQL constructs into SQL is out\nof this paper scope; details can be found in [15].\nV. P ERFORMANCE S TUDY\nThis section reports our empirical comparison of the proposed approaches to distributed Semantic Web data storage\nand querying in HBase and MySQL Cluster.\nA. Experimental Setup\nHardware. Our experiments used nine commodity machines with identical hardware. Each machine had a latemodel 3.0 GHz 64-bit Pentium 4 processor, 2 GB DDR2533 RAM, 80 GB 7200 rpm Serial ATA hard drive. The\nmachines were networked together via their add-on gigabit\nEthernet adapters connected to a Dell PowerConnect 2724\ngigabit Ethernet switch and were all running 64-bit Debian\nLinux 5.0.7 and Oracle JDK 6.\nHBase and MySQL Cluster. Hadoop 0.20.2, with a modified core library, and HBase 0.90 were used. Minor changes\nto the default configuration for stability included setting each\nblock of data to replicate two times and increasing the HBase\nmax heap size to 1.2 GB. MySQL Cluster 7.1.9a was used\nwith a modified configuration based on the MySQL Cluster\n\n\fQuick Start Guide with increased memory available for use\nby NDB data nodes.\nOur implementation. Our algorithms were implemented\nin Java and the experiments were conducted using Bash shell\nscripts to execute the Java class files and store the results in\nan automated and repeatable manner.\nB. Datasets and Queries\nThe experiments used datasets from the Third Provenance Challenge (PC3)4 and Lehigh University Benchmark\n(LUBM) [1]. PC3 employed the Load Workflow that was a\nvariation of a workflow used in the Pan-STARRS project.\nVia simulation, a number of scientific workflow provenance\ndocuments for multiple workflow runs was generated and\nrepresented using Tupelo's OWL vocabulary available from\nthe Open Provenance Model website5 . Each workflow execution generated approximately 700 RDF triples. Table I indicates the characteristics of each PC3 dataset. The three PC3\nSPARQL queries utilized for the experiments can be found\nin our previous work [4]. LUBM is a popular benchmark for\nRDF databases that includes the OWL university ontology,\nRDF data generator, and 14 test queries. Table II indicates\nthe characteristics of each generated LUBM dataset. The\nLUBM queries expressed in a KIF-like language can be\nfound on the LUBM website6 ; for the purpose of our\nexperiments, they were rewritten in SPARQL. Since our\nexperiments tested query performance and not reasoning\nability, each generated LUBM dataset was augmented with\nadditional triples needed to produce the sample query results\nsupplied by LUBM.\nTable I\nPC3 DATASET C HARACTERISTICS .\nDataset\nD1\nD2\nD3\nD4\nD5\nD6\n\n# of workflow runs\n1\n10\n100\n1,000\n10,000\n100,000\n\n# of RDF triples\n700\n7,000\n70,000\n700,000\n7,000,000\n70,000,000\n\nDisk space\n86 KB\n860 KB\n8.7 MB\n88 MB\n895 MB\n9 GB\n\nC. Data Ingest Performance\nDue to the space limit, we only report a few observations\non data ingest. First, out of tested statement-by-statement,\nbatch, and bulk load methods, MySQL Cluster and HBase\nshowed the best data ingest performance with bulk and\nbatch methods, respectively. Second, MySQL Cluster was\nable to bulk load datasets up to D5 and L8 and HBase\nsuccessfully batch loaded all the datasets. Finally, MySQL\nCluster initially demonstrated a significant advantage over\nHBase (3 times faster on L1), however this performance\n4 Third Provenance Challenge, http://twiki.ipaw.info/bin/view/Challenge/\nThirdProvenanceChallenge\n5 Open Provenance Model, http://openprovenance.org\n6 Lehigh University Benchmark, http://swat.cse.lehigh.edu/projects/lubm/\n\nTable II\nLUBM DATASET C HARACTERISTICS .\nDataset\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\n\n# of universities\n1\n5\n10\n30\n50\n70\n90\n110\n200\n400\n600\n\n# of RDF triples\n38,600\n563,000\n1,211,000\n3,908,000\n6,593,000\n9,308,000\n11,964,000\n14,649,000\n26,635,000\n53,301,000\n80,043,000\n\nDisk space\n4.4 MB\n68 MB\n146 MB\n477 MB\n807 MB\n1.1 GB\n1.5 GB\n1.8 GB\n3.3 GB\n6.6 GB\n9.9 GB\n\nadvantage decreased with dataset size growth (only 1.5 times\nfaster on L8); it also should be noted that HBase required\nto store twice as many triples as MySQL Cluster.\nD. Query Evaluation Performance\nHBase and MySQL Cluster query performance and scalability on PC3 and LUBM datasets are reported in Fig. 4. The\nPC3 benchmark used three queries with varying complexity:\nQ1 was the simplest query with one triple pattern, Q2 had\nthree triple patterns, and Q3 was the most complex one\nconsisting of six triple patterns. The basic graph patterns\nin all three queries returned a small result. Both HBase\nand MySQL Cluster showed very efficient and comparable\nresponse times, with the former being slightly faster. At D6,\nHBase took a slight upward turn in times that had previously\nremained nearly flat, which signifies that the graphs have a\nsmall slope (while the dataset size increased by a factor of\n10, the response times increased by a factor of only around\n2 to 4); similar behavior was also observed for some LUBM\nqueries.\nThe LUBM benchmark used 14 queries whose complexities are shown in Table III. LUBM query evaluation results\nfor HBase and MySQL Cluster revealed several interesting\npatterns, denoted as A, B, C, D, and E in Table III. Pattern\nA (Q1, Q3, Q7, and Q11) is characterized by the rapidly\nincreasing query execution time for MySQL Cluster and\nnearly constant response time for HBase as the dataset size\nincreased. Pattern B (Q2 and Q14) is characterized by rapid\nperformance degradation in both systems. While Q2 had\nsix triple patterns, Q14 had only one triple pattern that\nretrieved all undergraduate students across all universities\nin the database. Both queries yielded large results, such that\nresults for L9, L10, and L11 could not fit into main memory\non the HBase master server. In the case of Q14, which\ninvolved no joins, it is evident that the major factor in query\nperformance is data transfer time and it is hardly possible to\nachieve better performance on the given hardware. Patterns\nC (Q4, Q5, Q6, and Q8) and D (Q9, Q10, and Q13)\ninclude queries whose performance showed limited or no\ngrowth in execution times with an increase in the data size\nin both systems. Pattern C queries were approximately 2\nto 3 times faster on MySQL Cluster and pattern D queries\n\n\fLegend\n\nPC3 Q1\n\nHBase\n\n15\n\nMySQL Cluster\n\n10\n\nPC3\n\nPC3 Q2\n30\n\n10\n\n9\n\n5\n\nX-Axis:\nY-Axis:\nLUBM\nX-Axis:\nY-Axis:\n\nNumber of workflows\nin the database, log\nscale\nQuery execution\ntime, ms\n\n0\n\n0\n1\n\n100\n\n10000\n\n50\n40\n30\n20\n10\n0\n\n10000\n\nLUBM Q2\n600,000\n\n20,000\n\nLUBM Q3\n200,000\n\n567,855\n484,783\n\n0\n\n100 200 300 400 500 600\n\n100 200 300 400 500 600\n\n53\n\nLUBM Q6\n645\n\n133\n\n200\n0\n\n0\n\nLUBM Q7\n\n100 200 300 400 500 600\n\n0\n\nLUBM Q9\n6,000\n\n10,000\n\n19,089\n\n10,099\n\n4,746\n\n5,000\n\n123\n\n4,000\n\n4,222\n\n2,000\n\n0\n\n100 200 300 400 500 600\n\n100 200 300 400 500 600\n\n0\n\n100 200 300 400 500 600\n\nLUBM Q11\n\nLUBM Q12\n\n6,000\n\n800\n\n5,422\n\n1,282\n\n4,000\n\n500\n\n2,000\n\n27\n\n463\n\n0\n\n702\n\n600\n400\n200\n\n374\n\n0\n0\n\n100 200 300 400 500 600\n\n1,561\n\n0\n0\n\nLUBM Q10\n1,500\n\n470\n\n100 200 300 400 500 600\n\nLUBM Q8\n\n10,000\n\n0\n\n197\n\n200\n\n15,000\n\n0\n\n400\n\n0\n0\n\n100 200 300 400 500 600\n\n30,000\n\n1,000\n\n100 200 300 400 500 600\n\n600\n\n600\n400\n\n153\n\n0\n\n0\n\nLUBM Q5\n800\n\n0\n\n28\n\n0\n0\n\nLUBM Q4\n200\n\n20,000\n\n163,357\n\n150,000\n100,000\n50,000\n\n200,000\n\n27\n\n0\n\n10000\n\n11\n100\n\n400,000\n\n150\n100\n50\n\n100\n\n47\n\n1\n\n47,212\n\n0\n\n1\n\nPC3 Q3\n\nNumber of\nuniversities in the\ndatabase\nQuery execution\ntime, ms\n\n0\n\n10\n\n10\n\nLUBM Q1\n40,000\n\n27\n\n20\n\nLUBM Q13\n\n100 200 300 400 500 600\n\n0\n\n100 200 300 400 500 600\n\nLUBM Q14\n\n150\n\n30,000\n\n26,456\n\n123\n100\n\n20,000\n\n16,362\n50\n\n10,000\n\n16\n\n0\n\n0\n0\n\n100 200 300 400 500 600\n\nFigure 4.\n\n0\n\n100 200 300 400 500 600\n\nQuery performance and scalability.\n\nwere anywhere from 3 to 47 times faster on HBase. Pattern\nE stands out on its own with a single representative query Q12. For smaller datasets, Q12 was much faster on MySQL\nCluster, however its performance quickly decreased on larger\ndatasets, much like in pattern A. HBase, on the other hand,\ndemonstrated a gradual increase in execution time: close\nto the 100 university mark, HBase performance exceeded\nMySQL Cluster performance.\nThe comparison of the query evaluation patterns and\nquery complexity in LUBM (see Table III) does not re-\n\nveal any strong correlation between the two characteristics.\nThe query complexity is not the sole indicator of query\nperformance under HBase and MySQL Cluster: the size of\nintermediate and final results can have a significant impact.\nOverall, in our experiments, the HBase approach showed\nbetter performance and scalability than the MySQL Cluster\napproach. Neglecting Q2 and Q14 of LUBM, which are\nexpensive due to returning large results, the evaluation of\ntwo queries over the largest LUBM dataset in HBase took\nover 1s: Q8 (10s) and Q9 (1.5s). In contrast, six LUBM\n\n\fTable III\nLUBM Q UERY C OMPLEXITY AND E VALUATION PATTERNS .\nQuery complexity\n(# of triple patterns)\n1\n2\n3\n4\n5\n6\n\nLUBM queries and\ntheir evaluation patterns\nQ6(C), Q14(B)\nQ1(A), Q3(A), Q5(C),\nQ10(D), Q11(A), Q13(D)\nN/A\nQ7(A), Q12(E)\nQ4(C), Q8(C)\nQ2(B), Q9(D)\n\nqueries took over 1s in MySQL Cluster under similar\ncircumstances. Finally, Q1, Q3, Q7, and Q11 of LUBM\nscaled significantly worse in MySQL Cluster.\nE. Summary\nOur performance study revealed interesting patterns in\nquery evaluation, showed that our algorithms are efficient,\nand suggested that cloud computing has a great potential\nfor scalable Semantic Web data management. Given that the\nexperiments were performed with large datasets on commodity machines, both HBase and MySQL Cluster approaches\nshowed to be quite efficient and promising. The proposed\napproaches were up to the task of efficiently storing and\nquerying large RDF datasets. Overall, the experimental\nresults were in favor of the HBase approach: not only were\nlarger datasets able to load, but query performance and\nscalability were shown to be superior in many cases.\nVI. C ONCLUSIONS AND F UTURE W ORK\nIn this paper, we studied the problem of distributed\nSemantic Web data management using state of the art\ncloud and relational database technologies represented by\nHBase and MySQL Cluster. We designed a novel database\nschema for HBase to efficiently store RDF data and proposed\nscalable querying algorithms to evaluate SPARQL queries\nin HBase. We chose a generic RDF database schema for\nMySQL Cluster and presented a SPARQL-to-SQL translation algorithm that generates flat SQL queries for SPARQL\nbasic graph patterns. Finally, we conducted an experimental\ncomparison of the two proposed approaches on a cluster of\ncommodity machines using datasets and queries of the Third\nProvenance Challenge and Lehigh University Benchmark.\nOur study concluded that, while both approaches were up\nto the task of efficiently storing and querying large RDF\ndatasets, the HBase solution was capable of dealing with\nlarger RDF datasets and showed superior query performance\nand scalability. We believe that cloud computing has a great\npotential for scalable Semantic Web data management.\nIn the future, we will focus on architectural aspects of an\nRDF database management system in the cloud, search for\noptimizations in schema design, explore additional SPARQL\nfeatures, and research inference support in distributed environments.\n\nR EFERENCES\n[1] Y. Guo, Z. Pan, and J. Heflin, \"LUBM: A benchmark for\nOWL knowledge base systems.\" Journal of Web Semantics,\nvol. 3, no. 2-3, pp. 158\u2013182, 2005.\n[2] A. Chebotko and S. Lu, Querying the Semantic Web: An Efficient Approach Using Relational Databases. LAP Lambert\nAcademic Publishing, 2009.\n[3] M. F. Husain, L. Khan, M. Kantarcioglu, and B. M. Thuraisingham, \"Data intensive query processing for large RDF\ngraphs using cloud computing tools,\" in Proc. of CLOUD,\n2010, pp. 1 \u2013 10.\n[4] J. Abraham, P. Brazier, A. Chebotko, J. Navarro, and A. Piazza, \"Distributed storage and querying techniques for a\nSemantic Web of scientific workflow provenance,\" in Proc.\nof SCC, 2010, 178-185.\n[5] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A.\nWallach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber,\n\"Bigtable: A distributed storage system for structured data,\"\nACM Transactions on Computer Systems, vol. 26, no. 2, 2008.\n[6] J. Myung, J. Yeon, and S. Lee, \"SPARQL basic graph pattern\nprocessing with iterative MapReduce,\" in Proc. of MDAC,\n2010, pp. 6:1\u20136:6.\n[7] P. Ravindra, V. V. Deshpande, and K. Anyanwu, \"Towards\nscalable RDF graph analytics on MapReduce,\" in Proc. of\nMDAC, 2010, pp. 5:1\u20135:6.\n[8] J. Urbani, S. Kotoulas, E. Oren, and F. van Harmelen,\n\"Scalable distributed reasoning using MapReduce,\" in Proc.\nof ISWC, 2009, pp. 634\u2013649.\n[9] A. Matono, S. M. Pahlevi, and I. Kojima, \"RDFCube: A\nP2P-based three-dimensional index for structural joins on\ndistributed triple stores,\" in Proc. of DBISP2P Workshops,\n2006, pp. 323\u2013330.\n[10] M. Cai, M. R. Frank, B. Yan, and R. M. MacGregor,\n\"A subscribable peer-to-peer RDF repository for distributed\nmetadata management,\" Journal of Web Semantics, vol. 2,\nno. 2, pp. 109\u2013130, 2004.\n[11] B. Quilitz and U. Leser, \"Querying distributed RDF data\nsources with SPARQL,\" in Proc. of ESWC, 2008, pp. 524\u2013\n538.\n[12] H. Stuckenschmidt, R. Vdovjak, J. Broekstra, and G.J. Houben, \"Towards distributed processing of RDF path\nqueries,\" International Journal of Web Engineering and Technology, vol. 2, no. 2/3, pp. 207\u2013230, 2005.\n[13] N. Li, J. Rao, E. J. Shekita, and S. Tata, \"Leveraging a\nscalable row store to build a distributed text index,\" in Proc.\nof CloudDb, 2009, pp. 29\u201336.\n[14] H. Choi, J. Son, Y. Cho, M. K. Sung, and Y. D. Chung, \"SPIDER: a system for scalable, parallel/distributed evaluation of\nlarge-scale RDF data,\" in Proc. of CIKM, 2009, pp. 2087\u2013\n2088.\n[15] A. Chebotko, S. Lu, and F. Fotouhi, \"Semantics preserving\nSPARQL-to-SQL translation,\" Data & Knowledge Engineering, vol. 68, no. 10, pp. 973\u20131000, 2009.\n\n\f"}