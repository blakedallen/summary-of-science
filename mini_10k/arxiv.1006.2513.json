{"id": "http://arxiv.org/abs/1006.2513v3", "guidislink": true, "updated": "2013-08-25T17:12:28Z", "updated_parsed": [2013, 8, 25, 17, 12, 28, 6, 237, 0], "published": "2010-06-13T06:07:09Z", "published_parsed": [2010, 6, 13, 6, 7, 9, 6, 164, 0], "title": "On the Achievability of Cram\u00e9r-Rao Bound In Noisy Compressed Sensing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1006.2513%2C1006.0238%2C1006.2343%2C1006.5411%2C1006.0950%2C1006.1525%2C1006.2239%2C1006.1310%2C1006.5474%2C1006.2812%2C1006.2026%2C1006.3423%2C1006.3493%2C1006.5785%2C1006.4281%2C1006.1870%2C1006.2274%2C1006.2544%2C1006.1665%2C1006.2383%2C1006.4561%2C1006.3304%2C1006.3006%2C1006.3992%2C1006.1836%2C1006.0682%2C1006.1891%2C1006.2003%2C1006.3572%2C1006.4170%2C1006.5809%2C1006.0474%2C1006.2970%2C1006.3775%2C1006.1707%2C1006.1305%2C1006.3013%2C1006.5477%2C1006.3252%2C1006.5700%2C1006.5208%2C1006.0431%2C1006.4847%2C1006.4280%2C1006.3857%2C1006.2593%2C1006.2804%2C1006.4312%2C1006.2989%2C1006.0323%2C1006.5170%2C1006.4523%2C1006.2490%2C1006.2263%2C1006.4249%2C1006.4055%2C1006.2663%2C1006.4895%2C1006.5051%2C1006.0086%2C1006.4654%2C1006.4144%2C1006.0462%2C1006.5546%2C1006.1646%2C1006.3400%2C1006.3929%2C1006.1944%2C1006.1968%2C1006.4353%2C1006.3262%2C1006.3287%2C1006.2139%2C1006.2116%2C1006.3393%2C1006.4418%2C1006.2973%2C1006.3139%2C1006.5962%2C1006.5428%2C1006.5617%2C1006.0535%2C1006.2914%2C1006.1475%2C1006.1994%2C1006.2615%2C1006.0958%2C1006.1317%2C1006.0784%2C1006.1295%2C1006.1496%2C1006.3441%2C1006.4447%2C1006.1990%2C1006.3377%2C1006.3574%2C1006.1323%2C1006.1105%2C1006.4125%2C1006.4580%2C1006.0907&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On the Achievability of Cram\u00e9r-Rao Bound In Noisy Compressed Sensing"}, "summary": "Recently, it has been proved in Babadi et al. that in noisy compressed\nsensing, a joint typical estimator can asymptotically achieve the Cramer-Rao\nlower bound of the problem.To prove this result, this paper used a lemma,which\nis provided in Akcakaya et al,that comprises the main building block of the\nproof. This lemma is based on the assumption of Gaussianity of the measurement\nmatrix and its randomness in the domain of noise. In this correspondence, we\ngeneralize the results obtained in Babadi et al by dropping the Gaussianity\nassumption on the measurement matrix. In fact, by considering the measurement\nmatrix as a deterministic matrix in our analysis, we find a theorem similar to\nthe main theorem of Babadi et al for a family of randomly generated (but\ndeterministic in the noise domain) measurement matrices that satisfy a\ngeneralized condition known as The Concentration of Measures Inequality. By\nthis, we finally show that under our generalized assumptions, the Cramer-Rao\nbound of the estimation is achievable by using the typical estimator introduced\nin Babadi et al.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1006.2513%2C1006.0238%2C1006.2343%2C1006.5411%2C1006.0950%2C1006.1525%2C1006.2239%2C1006.1310%2C1006.5474%2C1006.2812%2C1006.2026%2C1006.3423%2C1006.3493%2C1006.5785%2C1006.4281%2C1006.1870%2C1006.2274%2C1006.2544%2C1006.1665%2C1006.2383%2C1006.4561%2C1006.3304%2C1006.3006%2C1006.3992%2C1006.1836%2C1006.0682%2C1006.1891%2C1006.2003%2C1006.3572%2C1006.4170%2C1006.5809%2C1006.0474%2C1006.2970%2C1006.3775%2C1006.1707%2C1006.1305%2C1006.3013%2C1006.5477%2C1006.3252%2C1006.5700%2C1006.5208%2C1006.0431%2C1006.4847%2C1006.4280%2C1006.3857%2C1006.2593%2C1006.2804%2C1006.4312%2C1006.2989%2C1006.0323%2C1006.5170%2C1006.4523%2C1006.2490%2C1006.2263%2C1006.4249%2C1006.4055%2C1006.2663%2C1006.4895%2C1006.5051%2C1006.0086%2C1006.4654%2C1006.4144%2C1006.0462%2C1006.5546%2C1006.1646%2C1006.3400%2C1006.3929%2C1006.1944%2C1006.1968%2C1006.4353%2C1006.3262%2C1006.3287%2C1006.2139%2C1006.2116%2C1006.3393%2C1006.4418%2C1006.2973%2C1006.3139%2C1006.5962%2C1006.5428%2C1006.5617%2C1006.0535%2C1006.2914%2C1006.1475%2C1006.1994%2C1006.2615%2C1006.0958%2C1006.1317%2C1006.0784%2C1006.1295%2C1006.1496%2C1006.3441%2C1006.4447%2C1006.1990%2C1006.3377%2C1006.3574%2C1006.1323%2C1006.1105%2C1006.4125%2C1006.4580%2C1006.0907&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Recently, it has been proved in Babadi et al. that in noisy compressed\nsensing, a joint typical estimator can asymptotically achieve the Cramer-Rao\nlower bound of the problem.To prove this result, this paper used a lemma,which\nis provided in Akcakaya et al,that comprises the main building block of the\nproof. This lemma is based on the assumption of Gaussianity of the measurement\nmatrix and its randomness in the domain of noise. In this correspondence, we\ngeneralize the results obtained in Babadi et al by dropping the Gaussianity\nassumption on the measurement matrix. In fact, by considering the measurement\nmatrix as a deterministic matrix in our analysis, we find a theorem similar to\nthe main theorem of Babadi et al for a family of randomly generated (but\ndeterministic in the noise domain) measurement matrices that satisfy a\ngeneralized condition known as The Concentration of Measures Inequality. By\nthis, we finally show that under our generalized assumptions, the Cramer-Rao\nbound of the estimation is achievable by using the typical estimator introduced\nin Babadi et al."}, "authors": ["Rad Niazadeh", "Masoud Babaie-Zadeh", "Christian Jutten"], "author_detail": {"name": "Christian Jutten"}, "author": "Christian Jutten", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TSP.2011.2171953", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1006.2513v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1006.2513v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1006.2513v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1006.2513v3", "arxiv_comment": null, "journal_reference": "IEEE Transactions on Signal Processing, Volume 60, Issue 1, Pages\n  518- 526, January 2012", "doi": "10.1109/TSP.2011.2171953", "fulltext": "1\n\nOn the Achievability of Cram\u00e9r-Rao Bound\nIn Noisy Compressed Sensing\n\narXiv:1006.2513v3 [cs.IT] 25 Aug 2013\n\nRad Niazadeh, Member, IEEE Massoud Babaie-Zadeh, Senior Member, IEEE, and\nChristian Jutten, Fellow, IEEE\n\nAbstract-Recently, it has been proved in [1] that in noisy compressed\nsensing, a joint typical estimator can asymptotically achieve the Cram\u00e9rRao lower bound of the problem. To prove this result, [1] used a lemma,\nwhich is provided in [2], that comprises the main building block of the\nproof. This lemma is based on the assumption of Gaussianity of the\nmeasurement matrix and its randomness in the domain of noise. In this\ncorrespondence, we generalize the results obtained in [1] by dropping\nthe Gaussianity assumption on the measurement matrix. In fact, by\nconsidering the measurement matrix as a deterministic matrix in our\nanalysis, we find a theorem similar to the main theorem of [1] for a\nfamily of randomly generated (but deterministic in the noise domain)\nmeasurement matrices that satisfy a generalized condition known as The\nConcentration of Measures Inequality. By this, we finally show that under\nour generalized assumptions, the Cram\u00e9r-Rao bound of the estimation\nis achievable by using the typical estimator introduced in [1].\nIndex Terms-Compressed Sensing, Joint Typicality, Typical Estimation, Chernof Bound\n\nI. I NTRODUCTION\nCompressed Sensing (CS) which is also known as Compressive\nSampling [3]\u2013[5] is a well known method for taking linear measurements from a sparse vector. Compressed sensing proposes that\none can recover a sparse signal from a few number of measurements, and so it can override the usual sampling method based on\nNyquist criteria [3]. In this correspondence, we revisit the problem\nof signal recovery in noisy compressed sensing, in which the above\nmentioned measurements are blended with noise. Indeed, suppose\nthat noisy measurements of the sparse signal are taken by a random\nmeasurement matrix in the following form:\ny = As + n ,\n\n(1)\n\nin which, s is the original M \u00d7 1 sparse signal, y is the N \u00d7 1\nvector of measurements, n \u223c N (0, \u03c3n2 IN\u00d7N ) is an N \u00d7 1 Gaussian\nnoise vector and A = [a1 a2 . . . aM ] is an N \u00d7 M measurement\nmatrix whose elements are usually generated at random. More\nprecisely, these elements are independent and identically distributed\nrandom variables drawn from some specific distributions (such as\nGaussian, Bernoulli, etc.), so that the overall measurement matrix\nwill be appropriate in the framework of recovery in compressive\nsampling [3], [4], [6], [7]. Suppose that s is sparse, i.e. ksk0 =\nK \u226a M where k.k0 denotes the l0 -norm, i.e. the number of nonzero components of s. Moreover, define \u03c4 , supp(s) as a subset\nCopyright c 2011 IEEE. Personal use of this material is permitted.\nHowever, permission to use this material for any other purposes must be\nobtained from the IEEE by sending a request to pubs-permissions@ieee.org.\nThis work has been partially funded by Iran Telecom Research Center (ITRC)\nand Iran National Science Foundation (INSF). Part of this work was done\nwhen the second author was in sabbatical at the University of Minnesota.\nR. Niazadeh is with the Electrical and Computer Engineering Department,\nCornell University, Ithaca, NY 14850 (email: rn274@cornell.edu).\nM. Babaie-Zadeh is with the Electrical Engineering Department,\nSharif University of Technology, Tehran 14588-89694, Iran (e-mail:\nmbzadeh@yahoo.com).\nC. Jutten is with the GIPSA-Lab, Department of Images and Signals,\nUMR CNRS 5216, University of Grenoble, Grenoble, France (e-mail:\nChristian.Jutten@gipsa-lab.grenoble-inp.fr), and with Institut Universitaire de\nFrance.\n\nof {1, 2 .\b. . M } that contains the indices of non-zero elements of s,\ni.e. \u03c4 = i \u2208 {1, 2, . . . M } : si 6= 0 in which si stands for the ith\nelement of s. For this model, one can also define the size parameters 1\nas in [1]:\nK\nM\n\u03b1,\n\u03b2,\n.\n(2)\nN\nK\nThe main problem of compressive sampling is to estimate the\nunknown sparse signal from its noisy measurements which are\ntaken as in (1). Many efforts have been done to find a practical\nrecovery method and some acceptable solutions have been proposed\nin the literature whose computational cost are tolerable, such as the\nalgorithms that are proposed in [9]\u2013[16]. On the other hand, there\nis another related problem which is indeed the framework of our\ncorrespondence. In this problem, we are searching for the existence\nof an efficient estimator, an estimator that can achieve the Cram\u00e9r-Rao\nlower bound [17] for the Mean Square Error (MSE) of the estimation.\nIt is important to note that in this problem, computational complexity\nof the proposed estimator has no importance (or much less importance\nwhen comparing to practical methods), while the achievablity of\nCram\u00e9r-Rao bound and the existence of such an estimator is in the\npoint of interest.\nFor our problem, two different Cram\u00e9r-Rao lower bounds for MSE\nhave been studied in [1], [18] depending on the amount of knowledge\nof the estimators about the sparsity structure of the original vector.\nThe first bound, which is known as CRB-S [18], is the Cram\u00e9r-Rao\nlower bound of a Genie Aided Estimation (GAE) problem in which\nthe estimators know the location of the non-zero taps i.e. \u03c4 , as if a\nGenie has aided them with the location of the taps [18], [19]. This\nbound can be described in closed form as [18]:\nCRB-S = \u03c3n2 Trace{(AT\u03c4 A\u03c4 )\u22121 } ,\n\n(3)\n\nin which A\u03c4 is a sub-matrix of A that contains the columns\ncorresponding to the indices in \u03c4 . Among all of the estimators that\nknow the location of the taps, it can be shown that (as we will also\nshow later in this correspondence) the efficient estimator will be the\nStructural Least Square Estimator (SLSE) which finds the solution\nof the following problem [18]:\n\u015d\u03c4 = argminky \u2212 A\u03c4 s\u03c4 k22 ,\n\n(4)\n\ns\u03c4\n\nin which s\u03c4 is the K \u00d7 1 vector of non-zero taps. The second bound,\nwhich is known as CRB-US [1], is the Cram\u00e9r-Rao lower bound\nfor the estimation problem in which the estimators have only prior\nknowledge about the cardinality of \u03c4 i.e. K, which indicates the\ndegree of sparsity. It is obvious that the Cram\u00e9r-Rao bound for this\nkind of estimation is not less than that of GAE, i.e.:\nCRB-US \u2265 CRB-S .\n\n(5)\n\n1 In the context of compressive sampling, the linear system in (1) is underdetermined, i.e. M > N . However, this assumption is not required in any\nof our presented analyses. Hence, our provided lemmas and theorems in this\ncorrespondence could be applied to the case of overdetermined noisy sparse\nrecovery, which appears in many applications in communication theory, for\nexample in sparse channel estimation [8].\n\n\f2\n\nFurthermore, in a recent work by Ben-Haim et al. [20] an expression\nfor CRB-US has been stated. In fact, they have shown that the behaviour of the CRB differs depending on whether or not the unknown\nsparse vector has maximal support (i.e., ksk0 = K or ksk0 < K).\nMore accurately, they have shown that if the measurement matrix\nsatisfies the uniqueness theorem provided by Donoho et al. [3] and\nCand\u00e9s et al. [4], and if we consider the case of maximal support, i.e.\nwhen ksk0 = K which is indeed our case in this correspondence, and\nif we consider the case of finite size sparse recovery, i.e. when M ,\nN , and K are fixed and limited, the Cram\u00e9r-Rao bound equals to that\nof GAE (when the sparsity pattern is known by the estimator), i.e.\nCRB-US equals CRB-S. However, according to our best knowledge,\nno evidence of exact achievability of CRB-US by the means of any\npractical estimator or non-practical estimators has been presented in\nthe literature for the case of fixed and limited M , N , and K. So, if\nsomeone proposes an estimator that can achieve CRB-S instead of\nCRB-US while it has only prior knowledge about the sparsity degree,\nthen it will be proven that CRB-S and CRB-US are equal to each\nother (as stated in [20]) and both of them are achievable by this\nproposed estimator.\nMany efforts have been done to design an estimator with just\nthe knowledge about the cardinality of \u03c4 that can achieve MSE as\nclose as possible to the GAE Cram\u00e9r-Rao lower bound (CRB-S).\nCand\u00e9s et al. [19] and Haupt et al. [21] proposed estimators that can\nachieve CRB-S up to a factor of log M which is far from CRB-S.\nInterestingly, recent works done by Babadi et al. [1] and Ak\u00e7akaya et\nal. [2] have shown that by using an impractical estimator known as\n\"Typical Estimator\", under certain constraints on s and A, one can\nasymptotically achieve the Cram\u00e9r-Rao bound of the GAE problem,\ni.e. CRB-S, without a priori knowing \u03c4 . By asymptotic, we mean\nwhere N , M and K tend to infinity while the size parameters in (2)\nremain constant. In other words, since the proposed typical estimator\nasymptotically achieves CRB-S, one can conclude that a) CRB-S and\nCRB-US are asymptotically equal, and b) this Cram\u00e9r-Rao bound\nis achievable (note that in general, the Cram\u00e9r-Rao bound of an\nestimation problem is not achievable, i.e. it is not generally a tight\nbound for MSE).\nThe typical estimation in [1], [2] is based on checking the Joint\nTypicality of the noisy observations vector with all possible choices\nof \u03c4 , and then decoding the one which is jointly typical with the\nobserved y. Definition of joint typicality is introduced in [1], [2]\nand we will review it later in this correspondence.2 After detecting\nthe support of s, typical estimator estimates the unknown vector s\nby using a structural least square estimation method, i.e. it finds the\nsolution of (4). In [1], the proof of the achievability of the Cram\u00e9rRao bound by using the typical estimator is based on a lemma\n(Lemma 3.3 of [2]), which bounds the probability of two error events\nin the mentioned estimation process. The first of these probabilities\nis the probability of the event that the support of s is not jointly\ntypical with y which we denote3 by (\u03c4 \u2241 y) and the second one is\nthe probability of the event that a subset J \u2282 {1, 2, . . . M } =\n6 \u03c4 with\ncardinality K is jointly typical with y which we denote by (J \u223c y).\nUsing this lemma, [1] shows that if the average power of s is limited\nand if\n1\n\u03b1<\n,\n(6)\n9 + 4 log(\u03b2 \u2212 1)\n\nN \u2192 \u221e.\nIt is important to mention that the proof of the above mentioned\nstatement in [1] depends on the assumption that the elements of the\nmeasurement matrix are drawn randomly from a Gaussian distribution, in addition to the assumption that this matrix is stochastic in\nthe noise domain. By this, we mean that this assumption will impose\nthe consideration of the elements of measurement matrix as random\nvariables in our analysis, just like the elements of noise vector.\nOn the contrary, these assumptions are unnecessary in the ordinary\nframework of compressed sensing, while we are looking to find a\nstable recovery method. In fact, it is common to use non-stochastic\nbut randomly generated measurement matrices in this context, while\nassuming that the noise vector is stochastic (because the estimator\nknows the exact measurement matrix, but it is not aware of the\nnoise vector). Additionally, among all randomly generated matrices,\nappropriate measurement matrices are those that satisfy a constraint\ncalled The Concentration of Measures Inequality4 , i.e. the following\ncondition [24]:\nP{| kAxk2 \u2212 kxk2 | \u2265 \u01ebkxk2 |} \u2264 2e\u2212Nc0 (\u01eb) , \u01eb \u2208 (0, 1)\n\n, (7)\n\nwhere the probability is taken over random space for N \u00d7M randomgenerated matrix A, \u01eb \u2208 (0, 1) is arbitrary, c0 (\u01eb) is a constant\ndepending only on \u01eb and such that for all \u01eb \u2208 (0, 1), c0 (\u01eb) > 0\nand x is an arbitrary fixed vector in RM . Because of this mentioned\ndifference in the assumptions made in [1] and ordinary assumptions\nmade in the framework of compressed sensing, one may wonder that\nthe results obtained in [1] may be also valid in the case of a larger\nfamily of measurement matrices than just the Gaussian matrices.\nIndeed, we will introduce a family of random-generated matrices\nwhich satisfies a modified version of concentration of measures\ninequality, i.e. the following condition:\nP{| kAxk2 \u2212 N kxk2 | \u2265 \u01ebN kxk2 |} \u2264 2e\u2212Nc0 (\u01eb) , \u01eb \u2208 (0, 1)\n\n,\n(8)\nin which all the variables are the same as those in (7). Perhaps, the\nmost prominent example of matrices that satisfy (8), are those with\nelements drawn independently and identically distributed according\nto N (0, 1) [24]; but, there is no force on having Gaussian entries in\nthe measurement matrix. More precisely, one can also use matrices\nwhose entries are independent realizations of \u00b11 Bernoulli random\nvariables\n(\n+1 with probability 1/2,\nAi,j =\n(9)\n\u22121 with probability 1/2.\nor related distributions such as\n\uf8f1 \u221a\n\uf8f4\n\uf8f2 + 3\nAi,j =\n0\n\uf8f4\n\uf8f3 \u221a\n\u2212 3\n\nwith probability\nwith probability\nwith probability\n\n1\n,\n6\n2\n,\n3\n1\n.\n6\n\n(10)\n\nthen the joint typical estimator achieves the Cram\u00e9r-Rao bound as\n\nand yet these matrices satisfy (8). In addition to example random\nmatrices described in (9) and (10), there are many other examples\nof random matrices that satisfy the condition in (8) and have an\nimportant role in statistical signal processing, communications5 , and\nin particular compressive sampling. In fact, there is a well known\nclass of linear projections, mostly known as data-base friendly\nrandom projections [25], that satisfies the condition in (8), and at\nthe same time can exploit the full allotment of dimensionality of\n\n2 It is worth noting that the concepts of typicality and typical estimation\nhave been first introduced in the literature of Shannon's work on information\ntheory [22], [23]. With some changes, this concept is adapted to the field of\ncompressive sampling in [1], [2].\n3 We will use the notations \"\u223c\" and \"\u2241\" for indicating a jointly typical or\na non jointly typical pair in the rest of this correspondence.\n\n4 This condition is a preliminary condition for Restricted Isometry Property\n(RIP) which is a well-known sufficient condition in the area of compressed\nsensing for robust and stable recovery of the original sparse vector via l1 minimization [6], [9].\n5 Many applications of using such non-Gaussian random projection, such\nas sparse channel estimation [8], have been reported in the literature.\n\n\f3\n\na high-dimensional point set. Random i.i.d Gaussian matrices and\nthose in (9) and (10) are considered as examples within this class.\nHence, as satisfying (8) is a general property of commonly used\nrandom projection in signal processing and compressive sampling,\nit may be interesting to generalize the results obtained in [1] for\nthis class of matrices. Then, one can conclude that the Cram\u00e9r-Rao\nbound of the estimation is also asymptotically achievable by using\nthe typical estimator introduced in [1] and [2], while we use nonGaussian matrices that satisfy the condition depicted in (8), which\nis a common and general condition for measurement matrices in\ncompressed sensing according to the literature.\nIn this correspondence, according to the above discussion, we\ninvestigate the results obtained in [1], and then we generalize the\nconditions for the problem of asymptotic achievability of Cram\u00e9rRao bound in noisy compressed sensing. More accurately, by using\nan alternative approach to this problem comparing to the one used\nin [1] and [2], i.e. by assuming that the measurement matrix, A, is\nnot stochastic in the noise domain, we will find a lemma similar to\nLemma 3.3 in [2] and prove it using a different method compared to\nthe original one (by using Chernof tail bounds for probability [26]).\nSince Lemma 3.3 of [2] has been used as the main building block to\nobtain the results of [1], one wonders if those results (achievablity of\nCRB-S and asymptotic equivalence of CRB-S and CRB-US) may be\nincorrect under our new assumption (A is just generated at random,\nbut it is deterministic when compared to noise) and hence if they\nshould be revised. In this purpose, we first re-state our proved lemma\nin the case of randomly generated (but deterministic in noise domain)\nmeasurement matrices that satisfy (8). Subsequently, we see that the\nfinal obtained form have very minor differences from Lemma 3.3\nin [2], while it is valid under the assumption that A is a deterministic\nrandomly generated matrix. Finally, we re-study the results of [1] and\nsee that although the main lemma used in [1] has been changed in\nour analysis, fortunately, all of the results in [1] remain valid. In other\nwords, in noisy compressed sensing and under our modified version\nof concentration of measures inequality condition, the Cram\u00e9r-Rao\nbound is asymptomatically achievable by using a typical estimator\ndescribed in [1], and the constraint in (6) will also be valid without\nany changes.\nThis correspondence paper is organized as follows. In the next\nsection, we will first review the definition of joint typicality and the\ntypical estimator introduced in [1]. Moreover, the main theorem of [1]\nand the Lemma 3.3 in [2] will be re-studied. Indeed, we provide a new\nform of the mentioned lemma under our new assumptions, in which\nthe measurement matrix is considered as a randomly generated matrix\nthat satisfies (8), although is deterministic in the noise domain. In\nSection III, the Cram\u00e9r-Rao lower bound on MSE for the compressed\nsensing problem in a noisy setting will be discussed and we will show\nthat the results obtained in [1] remain valid under our generalized\nassumptions. So the Cram\u00e9r-Rao bound of the GAE problem and that\nof the problem in which estimators have only prior knowledge about\nthe degree of sparsity are asymptotically equal if the measurement\nmatrix satisfies (8), although it may not be Gaussian or random in the\nnoise domain. In all of the above discussions, we will use the model\ndescribed in (1) and we will assume that the matrix A is randomly\ngenerated, but since it is known to the estimator, it should be treated\nas a deterministic matrix.\n\ncorrespond to the indices in \u03be. Let also \u03a0A\u03be , A\u03be (AT\u03be A\u03be )\u22121 AT\u03be\nand \u03a0\u22a5\nA\u03be , I \u2212 \u03a0A\u03be . \u03be and y are said to be jointly typical with\norder \u01eb, denoted by (y \u223c \u03be)\u01eb , if and only if:\n|\n\n(11)\n\nIn order to generalize the results in [1], we neglect the assumption\nthat A is a Gaussian random matrix in the noise domain. Indeed, we\nassume that A is a randomly generated matrix, but is known to the\nestimator, and hence should be considered as a deterministic matrix.\nAccordingly, we first introduce the following theorem, which is\nsimilar to Lemma 3.3 in [2] and only depends on our new assumption\non measurement matrix:\nTheorem 2.1 (Bounds on the Probabilities of Typicality):\nAssume that in (1), \u03c4 = supp(s). Additionally, assume that\n\u03be \u2282 {1, 2, . . . M } and |\u03be| = K. Considering an arbitrary small\nenough \u01eb > 0, the following expressions hold as N \u2192 \u221e:\n1\nN \u2212K 2\nexponentially\n2\nk\u03a0\u22a5\n\u03c3n | > \u01eb} \u2212\u2192 0 ,\nA\u03c4 yk \u2212\nN\nN\n1\nN \u2212K 2\n2\nP{| k\u03a0\u22a5\n\u03c3n | < \u01eb} \u2264\nA\u03be yk \u2212\nN\nN\nP\nP\n\u2032T \u2032\ni2\n\b N \u2212 K h N1\ni\u2208\u03c4 \\\u03be\nj\u2208\u03c4 \\\u03be si sj a i a j \u2212 \u01eb\nP\nP\nexp \u2212\n2\nT\n2\n\u2032\n\u2032\n\u2032\n4\ni\u2208\u03c4 \\\u03be\nj\u2208\u03c4 \\\u03be si sj a i a j + \u03c3 n\nN\n\nP{|\n\n(12)\n\n.\n(13)\n\n2\n\nin which \u03c3 \u2032 n = (1 \u2212 \u03b1)\u03c3n2 and a\u2032 i = (U\u03be ai )N\u2212K , where U\u03be is a\nunitary matrix extracted from the eigenvalue decomposition of \u03a0\u22a5\nA\u03be ,\nT\ni.e. \u03a0\u22a5\nA\u03be = U\u03be DU\u03be and D is a diagonal matrix. The (.)m operator\ndenotes a vector comprising of the first m elements of the operand.\nProof of (12) : The proof of this part is the same as the proof of\nthe first part of Lemma 3.3 in [2] with some minor modifications. For\nthe sake of readability, we will go through the steps of this proof. In\nthese steps, we will try to find the PDF (Probability Density Function)\n2\nof k\u03a0\u22a5\nA\u03c4 yk assuming that A is known and deterministic, while the\nnoise vector is random.\nDue to the fact that \u03a0A\u03c4 is the projector transform onto S =\nspan{columns of A\u03c4 } and since supp(s) = \u03c4 , we have:\n\u22a5\n\u22a5\n\u03a0\u22a5\nA\u03c4 y = \u03a0A\u03c4 (A\u03c4 s\u03c4 + n) = 0 + \u03a0A\u03c4 n .\n\n\u03a0\u22a5\nA\u03c4 is a symmetric matrix, therefore we can decompose it as\nU\u03c4 DUT\u03c4 , in which D is a diagonal matrix and U\u03c4 is a unitary\nmatrix (U\u03c4 UT\u03c4 = I). \u03a0\u22a5\nA\u03c4 is an N \u00d7 N matrix (which obviously\nhas N eigenvalues). In addition to that, [1] shows that A\u03c4 is fullrank with probability 1. This means that S = span{columns of A\u03c4 }\nis a K dimensional subspace of RN as N \u2192 \u221e. Moreover,\nfor every y \u2208 S, we have \u03a0\u22a5\nA\u03c4 y = 0 and so, the K basis\nvectors of S are K linearly independent eigenvectors of \u03a0\u22a5\nA\u03c4\ncorresponding to the eigenvalue 0. Additionally, for every y \u2208 S \u2032 =\n{orthogonal compliment of S in RN }, we have \u03a0\u22a5\nA\u03c4 y = y. In a\nsimilar way, we can show that S \u2032 is an N \u2212 K dimensional subspace\nof RN as N \u2192 \u221e and so, the N \u2212 K basis vectors of S \u2032 are the\nN \u2212 K linearly independent eigenvectors of \u03a0\u22a5\nA\u03c4 corresponding to\nthe eigenvalue 1. Consequently, the main diagonal of D consists of\nN \u2212 K 1's and K 0's. Moreover we have:\n2\n\u22a5\n2\nT\n2\nk\u03a0\u22a5\nA\u03c4 yk = k\u03a0A\u03c4 nk = kU\u03c4 DU\u03c4 nk =\n\nII. S TATEMENT AND PROOF OF THE MAIN THEOREM\nFirst, consider the noisy compressed sensing model in (1). As\nin [1], we use the following definition for joint typicality:\nDefinition 2.1 (Joint Typicality): Suppose that \u03be \u2282 {1, 2, . . . M }\nand |\u03be| = K, in which |*| denotes the cardinality of a set. Let A\u03be\ndenote the N \u00d7K sub-matrix of A including those columns of A that\n\n1\nN \u2212K 2\n2\nk\u03a0\u22a5\n\u03c3n | < \u01eb .\nA\u03be yk \u2212\nN\nN\n\nnT U\u03c4 DUT\u03c4 U\u03c4 DUT\u03c4 n =\n\n(Dn\u2032 )T Dn\u2032 = kDn\u2032 k2 ,\nin\nto\n\n\u2032\n\n(14)\n\nwhich n = UT\u03c4 n is a white Gaussian random vector (according\nthe fact that UT\u03c4 is just a deterministic rotation transform), i.e.,\n\n\f4\n\nn\u2032 \u223c N (0, \u03c3n2 I). Without loss of generality, we can assume that the\nN \u2212 K first elements of D are 1. So we can say that:\n\u2032 2\n\n\u03c61 = kDn k =\n\n|n\u20321 |2\n\n+\n\n|n\u20322 |2\n\n*** +\n\n|n\u2032N\u2212K |2\n\n.\n\nvar{\u03c61 } = 2(N \u2212 K)\u03c3n4 .\n\nThis \u03c72 random variable has a moment generating function \u03a6\u03c61 (s),\nwhich is defined by \u03a6\u03c61 (s) , E{e\u03c61 s }, and for every s satisfying\nthe condition 1 \u2212 2s\u03c3n2 > 0 can be expressed as [27]:\n\u03a6\u03c61 (s) =\n\n1\n\n(1 \u2212 2s\u03c3n2 )\n\nN \u2212K\n2\n\n.\n\n(16)\n\nWe can rewrite the probability in (12) as follows:\nN \u2212K 2\n1\n2\nk\u03a0\u22a5\n\u03c3n | > \u01eb} =\nA\u03c4 yk \u2212\nN\nN\nP{|\u03c61 \u2212 (N \u2212 K)\u03c3n2 | > N \u01eb} \u2264\n\nP{|\n\nP{\u03c61 > N \u01eb + (N \u2212 K)\u03c3n2 } + P{\u03c61 < \u2212N \u01eb + (N \u2212 K)\u03c3n2 } .\n(17)\nSo, by using Chernof bounds on the tail probability [26], i.e.:\n\u2200\u03bd > 0 : P{\u03c61 > \u03b4} \u2264 e\u2212\u03bd\u03b4 \u03a6\u03c61 (\u03bd)\n\n\u2200\u03bd < 0 : P{\u03c61 < \u03b4} \u2264 e\u2212\u03bd\u03b4 \u03a6\u03c61 (\u03bd) ,\n\n(18)\n(19)\n\nwe can bound the probabilities in (48). By applying (18) and (16),\nand also considering the constraints needed for these equations, we\nhave:\n1\n: P{\u03c61 > N \u01eb + (N \u2212 K)\u03c3n2 } \u2264\n\u22000<\u03bd<\n2\u03c3n2\n1\n2\n(20)\nN \u2212K exp{\u2212\u03bd(N \u01eb + (N \u2212 K)\u03c3n )} , f (\u03bd) .\n(1 \u2212 2\u03bd\u03c3n2 ) 2\nBy taking the derivative of f (\u03bd) and finding its minimum in order to\nobtain the tightest bound, we find that this minimum occurs at \u03bd \u2217 =\n\u2217\n1\nN\u01eb\nsatisfies the\n2 N\u01eb+(N\u2212K)\u03c3 2 . Moreover, it is easy to check that \u03bd\n2\u03c3n\nn\n\u2217\nconstraints imposed by (18) and (16), i.e. \u03bd > 0 and 1 \u2212 2\u03c3n2 \u03bd \u2217 =\n2\n(N\u2212K)\u03c3n\n> 0. Hence we have:\nN\u01eb+(N\u2212K)\u03c3 2\nn\n\nP{\u03c61 > N \u01eb + (N \u2212 K)\u03c3n2 } \u2264 f (\u03bd \u2217 ) =\nn N \u01eb + (N \u2212 K)\u03c3 2 o N \u2212K\nN\u01eb\n2\nn\nexp{\u2212 2 } =\n(N \u2212 K)\u03c3n2\n2\u03c3n\nn (N \u2212 K)\nN\u01eb\nN\u01eb o\nexp\nln(1 +\n)\u2212 2 =\n2\n2\n(N \u2212 K)\u03c3n\n2\u03c3n\nn (N \u2212 K) \u0010\n\u01eb\n\u01eb \u0011o\n\u2212 ln(1 + \u2032 2 ) + \u2032 2\n,\nexp \u2212\n2\n\u03c3n\n\u03c3n\n\ni\u2208\u03c4 \u2229\u03be\n\ni\u2208\u03c4 \\\u03be\n\ni\u2208\u03c4 \\\u03be\n\n\u03a0\u22a5\nA\u03be\n\nIn the same way, we can decompose\n= U\u03be DUT\u03be , in which D\nis similar to the one in the previous part and U\u03be is a unitary matrix.\nThen we have:\nX\n2\nT\nsi ai + n)k2 = kDn\u2032\u2032 k2 ,\nk\u03a0\u22a5\nA\u03be yk = kU\u03be DU\u03be (\ni\u2208\u03c4 \\\u03be\n\nin which, n\u2032\u2032 = UT\u03be n+ i\u2208\u03c4 \\\u03be si UT\u03be ai is a Gaussian random vector\nP\nT\nwith mean n\u0304\u2032\u2032 = E{n\u2032\u2032 } =\ni\u2208\u03c4 \\\u03be si U\u03be ai and auto-covariance\n\u2032\u2032\n\u2032\u2032\n\u2032\u2032\n\u2032\u2032 T\nmatrix E{(n \u2212 n\u0304 )(n \u2212 n\u0304 ) } = Cn\u2032\u2032 = \u03c3n2 I, which are results\nof the fact that A is deterministic. It is important to note that the\nremaining proof of this part of Lemma 3.3 in [2] (which is so similar\nto our proposed lemma) is based on the Gaussian assumption on A,\nin addition to the assumption that this matrix is random in the domain\nof noise6 ; nevertheless, our proof is free from such assumptions\nwhile we assume that the measurement matrix is deterministic. As\na result, this assumption will help us to generalize our results for\nother types of randomly generated measurement matrices that are\ncommon in the compressed sensing area, as will be shown later in\nthis correspondence.\nTo continue our proof, without loss of generality we can assume\nthat the first N \u2212 K elements of the main diagonal of D are 1 and\nso:\n\u03c62 = kDn\u2032\u2032 k2 = |n\u2032\u20321 |2 + |n\u2032\u20322 |2 * * * + |n\u2032\u2032N\u2212K |2 ,\n(23)\nP\n\nin which, n\u2032\u2032i \u223c N (mi , \u03c3n2 ) for every 1 \u2264 i \u2264 N \u2212 K. In addition\nto these, we have:\nN\u2212K\nX\ni=1\n\nm2i = k\n\nX\n\ni\u2208\u03c4 \\\u03be\n\n(UT\u03be ai )N\u2212K si k2 =\n\nX X\n\nT\n\ns i s j a\u2032 i a\u2032 j ,\n\ni\u2208\u03c4 \\\u03be j\u2208\u03c4 \\\u03be\n\n(24)\nin which, the (*)N\u2212K operator denotes a sub-vector of the first N \u2212\nK elements and a\u2032 i = (UT\u03be ai )N\u2212K . For the sake of simplicity of\nP\nP\n\u2032T \u2032\nnotations, we define \u03b3 2 = N1\ni\u2208\u03c4 \\\u03be\nj\u2208\u03c4 \\\u03be si sj a i a j . Now, the\nsum of the squares of N \u2212K independent Gaussian random variables\nn\u2032\u2032i , each having mean mi , is a non-central\n\u03c72 random variable of\nPN\u2212K\n2\norder N \u2212 K with parameters \u03c3n and i=1 m2i . So we have:\n\nE{\u03c62 } = (N \u2212K)\u03c3n2 +N \u03b3 2 , var{\u03c62 } = 2(N \u2212K)\u03c3n4 +4\u03c3n2 N 2 \u03b3 4 .\n\n(21)\n\n2\n\n\u03c3n2 . Using the inequality ln(1 + \u03c3\u01eb\u20322 ) \u2264 \u03c3\u01eb\u20322 ,\nin which \u03c3 \u2032 n = (N\u2212K)\nN\nn\nn\nwe can say that the bound in (21) decreases exponentially to 0 as\nN \u2192 \u221e. Similarly, using (19) and following the same approach as\nin the proof of (21), we can bound P{\u03c61 < \u2212N \u01eb + (N \u2212 K)\u03c3n2 }\nand so we will have:\nP{\u03c61 < \u2212N \u01eb + (N \u2212 K)\u03c3n2 } \u2264\nn (N \u2212 K) \u0010\n\u01eb\n\u01eb \u0011o\nexp \u2212\n\u2212 ln(1 \u2212 \u2032 2 ) \u2212 \u2032 2\n.\n2\n\u03c3n\n\u03c3n\n\n\u22a5\n\u03a0\u22a5\nA\u03be y = \u03a0A\u03be (A\u03c4 s\u03c4 + n) =\nX\nX\nX\nsi ai + n) .\nsi ai + n) = \u03a0\u22a5\ns i ai +\n\u03a0\u22a5\nA\u03be (\nA\u03be (\n\n(15)\n\nSince \u03c61 is the sum of squares of N \u2212 K independent Gaussian\nrandom variables with mean 0 and variance \u03c3n2 , it is a \u03c72 random\nvariable of order N \u2212 K with parameter \u03c3n2 . i.e.,\nE{\u03c61 } = (N \u2212 K)\u03c3n2\n\nProof of (13): Similar to the previous part we have:\n\nAdditionally, this \u03c72 random variable has a moment generating\nfunction \u03a6\u03c62 (s), which is defined by \u03a6\u03c62 (s) , E{e\u03c62 s }, and for\nevery s satisfying 1 \u2212 2s\u03c3n2 > 0 can be expressed as [27]:\n\u0010 s PN\u2212K m2 \u0011\n1\ni\ni=1\n\u03a6\u03c62 (s) =\n.\n(25)\nN \u2212K exp\n1 \u2212 2s\u03c3n2\n(1 \u2212 2s\u03c3n2 ) 2\nBy centralizing the probability in (13) with respect to the mean of\n\u03c62 , we can rewrite the probability in (13) as:\nN \u2212K 2\n1\n2\nk\u03a0\u22a5\n\u03c3n | < \u01eb} =\nA\u03be yk \u2212\nN\nN\n2\nP{|\u03c62 \u2212 (N \u2212 K)\u03c3n | < N \u01eb} \u2264 P{\u03c62 \u2212 (N \u2212 K)\u03c3n2 < N \u01eb} =\n\nP{|\n\nP{\u03c62 \u2212 (N \u2212 K)\u03c3n2 < N (\u03b3 2 \u2212 \u01ed)} =\n\n(22)\n\nUsing the inequality ln(1 \u2212 \u03c3\u01eb\u20322 ) \u2264 \u2212 \u03c3\u01eb\u20322 , it is seen that the bound\nn\nn\nin (22) approaches 0 exponentially as N \u2192 \u221e. Consequently the\nprobability in (12) will tend at least exponentially to 0, and so the\nproof is complete.\n\nP{\u03c62 \u2212 (N \u2212 K)\u03c3n2 \u2212 N \u03b3 2 < \u2212N \u01ed},\n\n(26)\n\n2\n\nin which \u01ed = \u03b3 \u2212 \u01eb > 0 (we assume that \u01eb is small enough so\nthat \u01eb < \u03b3 2 ). Similar to the proof of (12), we will use Chernof\n6 This approach is very common in the framework of information theory,\nwhen one tries to show the achievability of a rate in a channel [22].\n\n\f5\n\n2\n\nbounds stated in (18) and (19) to bound the probability in (26). More\naccurately, by the use of (19) we get:\n\u2200\u03bd < 0 : P{\u03c62 < (N \u2212 K)\u03c3n2 + N \u03b3 2 \u2212 N \u01ed} \u2264\n\u0010\n\u0011\nexp \u2212 \u03bd[(N \u2212 K)\u03c3n2 + N \u03b3 2 \u2212 N \u01ed] \u03a6\u03c62 (\u03bd) , g(\u03bd) .\n\n(27)\n\nBy plugging in the value of \u03a6\u03c62 (\u03bd) from (25) for every \u03bd satisfying\n1 \u2212 2\u03bd\u03c3n2 > 0, g(\u03bd) is equal to:\n1\n\ng(\u03bd) =\n\nN \u2212K\n\n\u00d7\n\n(1 \u2212 2\u03bd\u03c3n2 ) 2\n\u0011\n\u0010 \u03bd PN\u2212K m2\ni\ni=1\n\u2212 \u03bd[(N \u2212 K)\u03c3n2 + N \u03b3 2 \u2212 N \u01ed] .\nexp\n2\n1 \u2212 2\u03bd\u03c3n\n\n(28)\n\nAs shown in the Appendix (Lemma A.1), by taking the derivative of\ng(\u03bd) with respect to \u03bd, one can see that this function will reach its\nminimum value at \u03bd \u2217 , calculated as following:\np\n2\n2\u03b3 2 \u2212 2\u01ed + \u03c3 \u2032 n \u2212 (\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed\n\u2217\n\u03bd =\n,\n(29)\n4\u03c3n2 (\u03b3 2 \u2212 \u01ed + \u03c3 \u2032 2n )\n2\n\n\u03c3n2 . Moreover, this \u03bd \u2217 is negative (and hence\nin which \u03c3 \u2032 n = N\u2212K\nN\nsatisfies the constraint 1 \u2212 2\u03bd \u2217 \u03c3n2 > 0), as stated by Lemma A.1.\nBy plugging (29) and the expressions obtained in the Appendix for\n\u03b32 \u03bd\u2217\n1 \u2212 2\u03bd \u2217 \u03c3n2 , 1\u22122\u03bd1\u2217 \u03c3 2 and 1\u22122\u03bd\n\u2217 \u03c3 2 in (27), we will have the tightest\nn\nn\nChernof bound for the probability in (27) as:\nh p(\u03c3 \u2032 2 + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed \u2212 \u03c3 \u2032 2 i N \u2212K\n2\nn\nn\n\u2217\n}\n\u00d7\ng(\u03bd ) =\n2\u03b3 2\np\n2\nn\n\u0002 \u03c3 \u2032 n + 2\u03b3 2 \u2212 \u01ed \u2212 (\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed \u0003o\n. (30)\nexp \u2212 N\n2\u03c3n2\nAfter some manipulations, this bound can be re-written as:\nq\n4\u03b3 2 \u01ed\n2\n2\n\u20322\n\u20322\n\b N h \u03c3 n + 2\u03b3 \u2212 \u01ed \u2212 (\u03c3 n + 2\u03b3 ) 1 \u2212 (\u03c3 \u20322n +2\u03b3 2 )2\nexp \u2212\n2\n\u03c3n2\nq\n2 \u01ed\n2\n\u20322\n(\u03c3 \u2032 n + 2\u03b3 2 ) 1 \u2212 (\u03c3 \u203224\u03b3\n2 2 \u2212 \u03c3 n \u0001i\nN \u2212K\nn +2\u03b3 )\n\u2212\n. (31)\nln\nN\n2\u03b3 2\n\nBefore proceeding any further, we will introduce the following\nlemma:\nLemma 2.2: for any x \u2208 R and 0 \u2264 x \u2264 1 we have:\n\u221a\n\u2022\n1 \u2212 x \u2264 1 \u2212 12 x\n1 2\n\u2022 ln(1 \u2212 x) \u2264 \u2212x \u2212 x\n2\nthe proof is elementary and is left to the reader.\n2\n\u01ed\n4\u03b3 4 \u22124\u03b3 2 \u01eb\nIt is important to note that (\u03c3 \u203224\u03b3\n= 4\u03b3 4 +\u03c3\n\u20324 +2\u03b3 2 \u03c3 \u20322 , and so\n+2\u03b3 2 )2\nn\n\n2\n\nn\n\nn\n\n4\u03b3 \u01ed\n2 2\n(\u03c3 \u20322\nn +2\u03b3 )\n\n\u2264 1. Hence, by using the first part of\nwe have that 0 \u2264\nLemma (2.2) and some further manipulations, we can bound (30) as:\n\b N h\u03c3\ng(\u03bd \u2217) \u2264 exp \u2212\n2\nN \u2212K\nln\n\u2212\nN\n\n2\n(\u03c3 \u2032 n\n\n\u20322\nn\n\n2\n\n+ 2\u03b3 2 \u2212 \u01ed \u2212 (\u03c3 \u2032 n + 2\u03b3 2 ) +\n\n2\u03b3 2 \u01ed\n2\n(\u03c3 \u20322\nn +2\u03b3 )\n\n\u03c3n2\n\n+ 2\u03b3 2 ) \u2212\n\n2\u03b3 2 \u01ed\n2\n(\u03c3 \u20322\nn +2\u03b3 )\n\n2\u03b3 2\n\n\u2212\n\n2\n\u03c3 \u2032 n \u0001i\n\n.\n\n(32)\n\nAfter simplifying (32) we have:\n2\n\b Nh\n\u03c3 \u2032 \u01ed\n\u2212 2 \u20322 n\ng(\u03bd \u2217) \u2264 exp \u2212\n2\n\u03c3n (\u03c3 n + 2\u03b3 2 )\n\u0001i\n\u01ed\nN \u2212K\n=\nln 1 \u2212 \u2032 2\n\u2212\n2\nN\n(\u03c3 n + 2\u03b3 )\n\b N \u2212Kh\n\u0001i\n\u01ed\n\u01ed\nexp \u2212\n.\n\u2212 ln 1 \u2212 \u2032 2\n\u2212 \u20322\n2\n2\n2\n(\u03c3 n + 2\u03b3 )\n(\u03c3 n + 2\u03b3 )\n(33)\n\n\u03b3 \u2212\u01eb\n\u01ed\nIt is also important to note that as (\u03c3 \u20322 +2\u03b3\n2 ) = (\u03c3 \u20322 +2\u03b3 2 ) , we have\nn\nn\n\u01ed\n0 \u2264 (\u03c3 \u20322 +2\u03b3 2 ) \u2264 1. Now, by applying the second part of Lemma 2.2\nn\nwe can make an upper bound for (33) as the following:\ni2\n\b N \u2212Kh\n\u01ed\n.\n(34)\ng(\u03bd \u2217 ) \u2264 exp \u2212\n4\n(\u03c3 \u2032 2n + 2\u03b3 2 )\n\nTherefore, according to (34), (27) and (26), we have finally come to\nthe following result:\n1\nN \u2212K 2\n2\nk\u03a0\u22a5\n\u03c3n | < \u01eb} \u2264\nA\u03be yk \u2212\nN\nN\nh\ni2\n\b N \u2212K\n\u01ed\n=\nexp \u2212\n2\n2\n\u2032\n4\n(\u03c3 n + 2\u03b3 )\nP\nP\n\u2032T \u2032\ni2\n\b N \u2212 K h N1\ni\u2208\u03c4 \\\u03be\nj\u2208\u03c4 \\\u03be si sj a i a j \u2212 \u01eb\nP\nP\nexp \u2212\n2\nT\n2\n\u2032\n\u2032\n\u2032\n4\ni\u2208\u03c4 \\\u03be\nj\u2208\u03c4 \\\u03be si sj a i a j + \u03c3 n\nN\n\nP{|\n\n,\n(35)\n\nand this will complete the proof of (13).\nIt is important to note that if we see the proof of Theorem 2.1,\nthen we will conclude that this theorem holds asymptotically in\nprobability, i.e. if you test the validity of Theorem 2.1 for infinite\nnumbers of randomly generated A, then this theorem may not be\nvalid for just finite numbers of A. Moreover, as N \u2192 \u221e the size\nof this finite set will tend to zero. Accordingly, one can say that\nas N \u2192 \u221e, Theorem 2.1 may not be valid for just asymptotic zero\nnumber of randomly generated A, or simply it is asymptotically valid.\nHowever, as we will see later in Section III, we want to consider the\nachievability of Cram\u00e9r-Rao bound in asymptotic case, and so this\nasymptotic validation should be enough.\nIn addition to what has been stated in Theorem 2.1, when the\nsize of the problem tends to infinity and A satisfies the introduced\nconcentration of measures inequality depicted in (8) (for instance, its\nelements are drawn i.i.d from N (0, 1) or distributions such as the\nones introduced in (9) and (10)), one may find an equivalent bound\nusing the following lemma:\nLemma 2.3: If the elements of A are randomly and independently\ngenerated according to a distribution that satisfies (8), then we have:\nX\nX X\nT\n|si |2 ,\n(36)\nsi sj a\u2032 i a\u2032 j \u2192 (N \u2212 K)\ni\u2208\u03c4 \\\u03be\n\ni\u2208\u03c4 \\\u03be j\u2208\u03c4 \\\u03be\n\n\u2032\n\nin which \u03c4 , \u03be, s and a i are defined as in Theorem 2.1 .\nProof: Suppose that x1 and x2 are two arbitrary fixed vectors\nin RM . Then for every \u01eb \u2208 (0, 1), the following inequalities hold\nwith a probability that tends exponentially to 1 as N tends to \u221e:\n(1 \u2212 \u01eb)N kx1 k2 \u2264 kAx1 k2 \u2264 (1 + \u01eb)N kx1 k2 ,\n\n(1 \u2212 \u01eb)N kx2 k2 \u2264 kAx2 k2 \u2264 (1 + \u01eb)N kx2 k2 ,\n\n(37)\n(38)\n\n(1 \u2212 \u01eb)N kx1 \u2212 x2 k2 \u2264 kA(x1 \u2212 x2 )k2 \u2264 (1 + \u01eb)N kx1 \u2212 x2 k2 .\n(39)\nUsing (37), (38) and (39), it is straightforward to show that:\n(1 + \u01eb)N xT1 x2 \u2212 \u01ebN (kx1 k2 + kx2 k2 ) \u2264\n\n(Ax1 )T (Ax2 ) \u2264\n\n(1 \u2212 \u01eb)N xT1 x2 + \u01ebN (kx1 k2 + kx2 k2 ) .\n\n(40)\n\nBy setting x1 and x2 with 1's in their i-th and j-th elements\nrespectively and 0's in their other elements, for i 6= j we will have:\n1 T\na aj \u2264 2\u01eb ,\nN i\nand in the case of i = j, we will have:\n\u2212 2\u01eb \u2264\n\n1\u2212\u01eb \u2264\n\n1 T\na ai \u2264 1 + \u01eb .\nN i\n\n(41)\n\n(42)\n\n\f6\n\nThese events hold valid with a probability that tends exponentially to\n1 as N tends to \u221e\na fix value of i and j. By applying the union\n\u0001 for\nM (M \u22121)\nbound on all M\n=\nchoices for i and j, if \u01eb \u2192 0 then the\n2\n2\nfollowing equation holds for every i and j, with a probability that\nstill tends to 1 as N increases:\n1 T\n1 T\nai aj \u2192 0 if i 6= j ,\nai aj \u2192 1 if i = j .\n(43)\nN\nN\nNow, consider the matrix (UT\u03be A)N\u2212K . We want to show this\n(N \u2212 K) \u00d7 M matrix will also satisfy the modified version of the\nconcentration of measures inequality. In other words, we want to\nshow that for every x \u2208 RM , the following equation holds with a\nprobability that tends exponentially to 1 as N tends \u221e:\n\nSubstituting (48) in (24) will complete the proof.\nNow, using Lemma 2.3, we can rewrite the bound in (13) after\nsome manipulations as:\nN \u2212K 2\n1\n2\nk\u03a0\u22a5\n\u03c3n | < \u01eb} \u2264\nA\u03be yk \u2212\nN\nPN\n|si |2 \u2212 \u01eb\u2032 i2\n\b N \u2212Kh\nPi\u2208\u03c4 \\\u03be\nexp \u2212\n,\n4\n2 i\u2208\u03c4 \\\u03be |si |2 + \u03c3n2\n\nP{|\n\nN\n\u01eb. Interestingly, the asymptotic bound obtained\nin which \u01eb\u2032 = N\u2212K\nin (49) is very similar to the bound obtained in Lemma 3.3 of [2].\nIn fact, the bound obtained in [2] is as the following:\n\nN \u2212K 2\n1\n2\nk\u03a0\u22a5\n\u03c3n | < \u01eb} \u2264\nA\u03be yk \u2212\nN\nPN\nh\n|si |2 \u2212 \u01eb\u2032 i2\n\b N \u2212K\nP i\u2208\u03c4 \\\u03be 2\nexp \u2212\n2\n4\ni\u2208\u03c4 \\\u03be |si | + \u03c3n\n\nP{|\n\n(1\u2212\u01eb)(N \u2212K)kxk2 \u2264 k(UT\u03be A)N\u2212K xk2 \u2264 (1+\u01eb)(N \u2212K)kxk2 .\n(44)\nTo show this, we have:\nk(UT\u03be A)N\u2212K xk2 = xT AT (UT\u03be )TN\u2212K (UT\u03be )N\u2212K Ax .\n\n(45)\n\nTo simplify (45), lets see how the matrix U\u03be is constructed. First,\nchoose a set of indices in {1, 2, . . . , M } such as L \u2282 {1, 2, ...M },\nso that |L | = N , and also \u03be \u2282 L and \u03c4 \u2282 L . Then, we choose\nN columns of Anorm = \u221a1N A corresponding to the indices in L .\nFollowing (43), we can say that the columns of Anorm corresponding\nto the indices in \u03be are an approximate orthonormal basis for the\nspan of columns of A\u03be with a probability that tends exponentially\nto 1 as N \u2192 \u221e, and this approximation will become more accurate\nas \u01eb is chosen smaller. Therefore, these columns can be considered\nasymptotically as approximations for the orthonormal eigenvectors\nof the symmetric matrix \u03a0\u22a5\nA\u03be corresponding to zero eigenvalue, and\nagain these approximations will become more accurate as \u01eb is chosen\nsmaller. Similarly, the columns of Anorm corresponding to the indices\nin L \\\u03be can be considered as an approximate orthonormal basis for\nthe kernel space of A\u03be with a probability that tends exponentially\nto 1, and so they are approximations for orthonormal eigenvectors\nof \u03a0\u22a5\nA\u03be corresponding to the eigenvalue 1. Consequently, by the\ndefinition of U\u03be (i.e. its first N \u2212 K columns are orthonormal\neigenvectors of \u03a0\u22a5\nA\u03be corresponding to eigenvalue 1 and the next K\ncolumn are orthonormal eigenvectors of \u03a0\u22a5\nA\u03be corresponding to zero\neigenvalue) and the approximate orthogonal property of the selected\ncolumns of A as N \u2192 \u221e (equation (43)), and by doing some simple\nmanipulations, we have:\nN \u2212K\nIM \u00d7M ,\n(46)\nN\nand this approximation will become more accurate as \u01eb is chosen\nsmaller. By substituting the approximation stated in (46) with corresponding term in (45) we have:\n(UT\u03be )TN\u2212K (UT\u03be )N\u2212K \u2248\n\nN \u2212K\nkAxk2 ,\n(47)\nN\nand again, this approximation will be more accurate with smaller \u01eb.\nSo, following (8), one can say that for small enough \u01eb the equation\n(44) holds with a probability that tends exponentially to 1 as N\ngrows 7 . Now, using (44) and similar to what we have stated about the\ncolumns of A, we can conclude that the columns of (UT\u03be A)N\u2212K =\n[a\u2032 1 , a\u2032 2 , . . . , a\u2032 M ] satisfy the following equation as N \u2192 \u221e:\nk(UT\u03be A)N\u2212K xk2 \u2248\n\n1\n1\nT\nT\na\u2032 a\u2032 j \u2192 0 if i 6= j ,\na\u2032 a\u2032 j \u2192 1 if i = j .\n(N \u2212 K) i\n(N \u2212 K) i\n(48)\n7 Note that for small enough \u01eb we require large enough N (following what\nhas been stated in (8)), so that the concentration of measures inequality will\nbe satisfied with high probability.\n\n(49)\n\n.\n\n(50)\n\nAlthough these bounds are not identical, but they are very similar.\nIII. OVERVIEW OF C RAM \u00c9R -R AO LOWER BOUND AND THE\nJ OINTLY T YPICAL E STIMATOR\nIn this section, we will discuss the problem of estimating s from\nnoisy observations. The estimation process has two phases. In the first\nphase, the estimator will detect \u03c4 = supp(s) = {i1 , i2 , . . . iK } which\nis the location of the taps. The second phase includes estimating\ns\u03c4 = [si1 , si2 , . . . , siK ]T which is the value of the taps. In our\ndiscussion, we are going to survey the Cram\u00e9r-Rao lower bound of\nthe estimation problem. By using the idea of two-phase estimation,\nwe consider two special kinds of estimation process in this work. In\nthe first case, the estimator has a complete prior knowledge of \u03c4 , i.e.\na genie has aided us with \u03c4 . In the second case, we have no prior\nknowledge of \u03c4 except for its cardinality, K, which shows the level of\nsparsity. We will then derive that these two bounds are asymptotically\nequal to each other and are achievable by typical estimation, as shown\nin [1] although the main theorem used in [1] has been changed.\nThe model in (1) can be rewritten as:\ny = As + n = A\u03c4 s\u03c4 + n .\n\n(51)\n\nNow, if the estimator knows \u03c4 and wants to estimate s\u03c4 from y\nand \u03c4 , then the Cram\u00e9r-Rao bound of the estimation can be computed\nusing the following theorem, stated in [1], [28]:\nTheorem 3.1 (Cram\u00e9r-Rao bound of genie aided estimation):\nConsidering the model depicted in (1) and estimators of the form\nf (y, \u03c4 ) = \u015d\u03c4 , the Fisher information matrix of the GAE, which is\ndefined as:\nh \u2202\nih \u2202\niT\nJGAE = E{\nlog P(y|s\u03c4 )\nlog P(y|s\u03c4 ) } ,\n(52)\n\u2202s\u03c4\n\u2202s\u03c4\nis equal to:\n1\nJGAE = 2 AT\u03c4 A\u03c4 ,\n(53)\n\u03c3n\nand so we have the following Cram\u00e9r-Rao bound8 for the estimator\n\u015d\u03c4 = f (y, \u03c4 ):\nE{(s\u03c4 \u2212 \u015d\u03c4 )(s\u03c4 \u2212 \u015d\u03c4 )T } \u2265 J \u22121 = \u03c3n2 (AT\u03c4 A\u03c4 )\u22121\n\nE{ks\u03c4 \u2212 \u015d\u03c4 k2 } \u2265 \u03c3n2 Trace[(AT\u03c4 A\u03c4 )\u22121 ] = CRB-S .\n\n(54)\n(55)\n\nProof: The proof is given in [1] and [28].\nIn a GAE, by using a simple least square estimator for the model\nof (51) we can achieve the Cram\u00e9r-Rao bound mentioned in (55), i.e.\nthis estimator is efficient. In a more mathematical way, we have the\nfollowing theorem:\n8 The\n\nequation A \u2265 B means that A \u2212 B is non-negative definite.\n\n\f7\n\nTheorem 3.2 (Structural Least Square Estimator (SLSE)):\nConsider the following genie aided estimator\n\u015d\u03c4 = f (y, \u03c4 ) = argmin ky \u2212 A\u03c4 x\u03c4 k =\n\n(AT\u03c4 A\u03c4 )\u22121 AT\u03c4 y\n\nK\u03bc4 (s)\n\u2192 \u221e as N \u2192 \u221e ,\nlog(K)\n2\n\u2022 ksk2 grows polynomially in\n1\n,\n\u2022 \u03b1 <\n9+4 log(\u03b2\u22121)\n\u2022\n\n,\n\nthen we have:\nE{ks\u03c4 \u2212 \u015d\u03b6 k2 } \u2264 \u03b1\u03c3n2 ,\n\nthen we have:\n2\n\nE{ks\u03c4 \u2212 \u015d\u03c4 k } =\n\n\u03c3n2 Trace[(AT\u03c4 A\u03c4 )\u22121 ]\n\n(56)\n\nProof: The proof is similar to the proof of achievability of\nthe Cram\u00e9r-Rao bound by the least square estimator where noise\nis Gaussian [17] and is omitted due to the lack of space.\nWhen considering the asymptotic case in the estimation process, one\nmay use the equivalent limit of the bound in (55) using the following\nlemma:\nLemma 3.3: If the elements of A are generated independently and\nidentically distributed according to a distribution that satisfies (8),\nconsidering the model in (51), we will have:\nK 2\n\u03c3 .\n(57)\nN n\nProof: The proof of this lemma is given in [1] for the special\ncase that elements of A are i.i.d Gaussian random variables. Generalization of this proof for the family of distributions that satisfy (8)\nis elementary and is left to the reader.\nNow, we are going to investigate the relation between CRB-S and\nCRB-US (which is Cram\u00e9r-Rao bound of the estimators with just\nknowledge about the cardinality of \u03c4 ) under the assumption that the\nmeasurement matrix, A, is a randomly generated but deterministic\nmatrix that satisfies our modified concentration of measures inequality described in (8). As was mentioned before, CRB-S and CRB-US\nseem to be different bounds at the first glance. But interestingly, as\nwas shown in [1], in the asymptotic case they tend to each other.\nThe proof of this statement in [1] is based on the Lemma 3.3 in [2]\nwhich is based on the Gaussianity of the measurement matrix and its\nrandomness in the noise domain. So, one may wonder if the results\nin [1] are still correct under our new generalized assumptions, which\nfortunately is, as we will discuss later in this section. For showing\nthis, we investigate the method of estimation in [1] which is based\non a combinatorial search for finding the support of original sparse\nvector. Before proceeding any further, we will state the definition of\nthis estimator as in [1]:\nDefinition 3.1 (Joint Typicality Estimator): The Joint Typicality\nEstimator finds a set of indices, \u03b6 \u2282 {1, 2, . . . M } with cardinality\nof K which is jointly typical with y with order of \u01eb. After that, it\nwill produce the estimate \u015d\u03b6 as:\nCRB-S \u2212\u2192 \u03b1\u03c3n2 =\n\n(AT\u03b6 A\u03b6 )\u22121 AT\u03b6 y .\n\nN ,\n\n(58)\n\nIf the estimator does not find a unique solution for \u03b6, it will return\nan all-zero vector as its output.\nIn the main theorem of [1], it is shown that under certain constraints, the MSE of the jointly typical estimator is upper bounded\nby \u03b1\u03c3n2 . But the proof of this property is strongly based on Lemma\n3.3 in [2], which cannot be used under our new assumptions, as\nwas mentioned before. Instead, we use our variant of this lemma\n(Theorem 2.1 and especially its asymptotic form in (49)). According\nto the fact that this variant and the original form in [2] are not much\ndifferent from each other, we can show that the main theorem in [1]\nremains valid without any necessary changes. More accurately, we\nhave the following theorem:\nTheorem 3.4 (Revised version of Main Theorem in [1]):\nConsider the model described in (51) and suppose that A is a\nrandomly generated, but a deterministic matrix in the noise domain\nthat satisfies (8). Let \u015d\u03b6 be the output of the Jointly Typical Estimator\ndefined in Definition 3.1. Additionally, let \u03bc(s) , mini\u2208\u03c4 |si |. If\n\n(59)\n\nas N \u2192 \u221e for a fixed \u03b1 and \u03b2.\nProof: Our proof, is exactly the same as the proof in [1] with\nsome minor changes. First, similar to the mentioned proof, we try\nto upper bound the MSE of the estimation. Indeed, by repeating the\nfirst steps described by equations (17)-(22) of [1], applying the new\nform of the Lemma 3.3 in [2] which contains the bounds in (12) and\n(13) and also by using the asymptotic form of Theorem 2.1 described\nin (49), we can upper bound the MSE of joint typical estimator, i.e.\nE{ks\u03c4 \u2212 \u015d\u03b6 k2 }, by:\n!\n!\nK\nX\nK\nM \u2212K\n2\n2\n\u03b1\u03c3 \u2032 n + (K\u03c3 \u2032 n + ksk2 )\n\u00d7\nk\u2032\nk\u2032\nk\u2032 =1\n(60)\nn N \u2212 K k\u2032 \u03bc2 (s) \u2212 \u01eb\u2032 \u0001 o\n2\nexp \u2212\n.\n4\n2k\u2032 \u03bc2 (s) + \u03c3n2\nSimilar to [1], we use the inequality\n!\n\u0010\nK\nKe \u0011\n\u2032\n) ,\n\u2264\nexp\nk\nlog(\nk\u2032\nk\u2032\n\nto upper bound the k\u2032 -th term in the summation of (60) by:\n\u0010 k\u2032\ne \u0001\n(\u03b2 \u2212 1)e \u0001\nk\u2032\nexp K log k\u2032 +K log\n\u2212\nk\u2032\nK\nK\nK\nK\n\u2032\nK kK \u03bc2 (s) \u2212 \u01eb\u2032 \u00012 \u0011\n,\nC0 K\n\u2032\n2K kK \u03bc2 (s) + \u03c3n2\n\nin which C0 ,\n\nN\u2212K\n.\n4K\n\n(61)\n\n(62)\n\nAgain, similar to [1] we define:\n\nf (z) , Kz log\n\ne\u0001\n(\u03b2 \u2212 1)e \u0001\n+ Kz log\n\u2212\nz\nz\nKz\u03bc2 (s) \u2212 \u01eb\u2032 \u00012\n.\nC0 K\n2Kz\u03bc2 (s) + \u03c3n2\n\n(63)\n\nNow, by Lemmas 3.4, 3.5 and 3.6 of [2] we can easily conclude that\n4\n(s)\n1\nf (z) attains its maximum at either z = 1 or z = K\nif K\u03bc\n\u2192\u221e\nlog(K)\nas N \u2192 \u221e. So, we can upper bound (60) as:\nK\nX\n\nn\n\b\n1 o\nexp max f (1), f ( )\n=\nK\nk\u2032 =1\nn\n\b\n1 o\n.\n\u03b1\u03c3n2 + exp log(K 2 \u03c3n2 + Kksk2 ) + max f (1), f ( )\nK\n(64)\n\n\u03b1\u03c3n2 + (K\u03c3n2 + ksk2 )\n\nAdditionally, we have:\nf (1) = K(2 + log(\u03b2 \u2212 1)) \u2212 C0 K\nand\nf(\n\nK\u03bc2 (s) \u2212 \u01eb\u2032 \u00012\n,\n2K\u03bc2 (s) + \u03c3n2\n\n(65)\n\n1\n\u03bc2 (s) \u2212 \u01eb\u2032 \u00012\n) = 2 log(K) + 2 + log(\u03b2 \u2212 1) \u2212 C0 K\n. (66)\nK\n2\u03bc2 (s) + \u03c3n2\n\n1\nIt is obvious that f ( K\n) grows linearly to \u2212\u221e as N \u2192 \u221e.\n1\nAdditionally, if C0 > 2+log(\u03b2\u22121) or equivalently \u03b1 < 9+4 log(\u03b2\u22121)\nthen f (1) will also grow linearly to \u2212\u221e as N \u2192 \u221e. Hence, the\nexponent of the second term in (64) tends to \u2212\u221e as long as ksk2\ngrows polynomially with respect to N . So we have the following\ninequality when N \u2192 \u221e\n\nE{ks\u03c4 \u2212 \u015d\u03b6 k2 } \u2264 \u03b1\u03c3n2 ,\n\n(67)\n\n\f8\n\nwhich completes the proof.\nNow, by comparing the result of Theorem 3.4 with (57) and (5),\nwe come to the conclusion that under the assumption we made\nabout A (its distribution satisfies (8)), the CRB-S and CRB-US\nare asymptotically equal. Additionally, they can be asymptotically\nachieved using the Jointly Typical Estimator.\n\nIV. C ONCLUSION\nIn this correspondence paper, we examined the problem of the\nachievability of the Cram\u00e9r-Rao bound in noisy compressed sensing\nunder some new assumptions on the measurement matrix. Indeed, we\nrelax our analysis from the Gaussianity constraint on the measurement matrix and its randomness in the domain of noise. Instead,\nwe assumed that this matrix is randomly generated according to\na distribution that satisfies some sort of concentration of measures\ninequality (described in (8)), but is deterministic in the noise domain.\nMainly, we focused on the proof of Lemma 3.3 in [2] which was\nthe main building block of the interesting results obtained in [1].\nAfter reproving a new form of the above mentioned lemma using\nour new assumptions, we showed that the main theorem of [1] is\nstill valid under these assumptions. So, the Cram\u00e9r-Rao bound of\nthe GAE and the Cram\u00e9r-Rao bound for estimation with no prior\nknowledge about the original vector except for its degree of sparsity,\nare indeed asymptotically equal and the Jointly Typical Estimator\nfirst introduced in [1] can achieve this bound. Unfortunately, this\nmethod of estimation is impractical and to the best knowledge of the\nauthors, the problem of finding a practical estimator that can achieve\nthe Cram\u00e9r-Rao bound is still open.\n\nA PPENDIX\nLemma A.1: The function g(\u03bd) defined in (28) will reach its\nminimum at \u03bd \u2217 given in (29). Moreover, \u03bd \u2217 < 0 and 1 \u2212 2\u03c3n2 \u03bd \u2217 > 0.\nProof: By taking the derivative of g(\u03bd) with respect to \u03bd and\nsetting it to zero, we will have the following equation for finding the\n\u2202g\nroots of \u2202\u03bd\n:\n2\n\n2\n\n(\u2212\u01ed + \u03c3 \u2032 n + \u03b3 2 )X 2 \u2212 \u03c3 \u2032 n X \u2212 \u03b3 2 = 0 ,\nin which X , (1 \u2212 2\u03bd \u2217 \u03c3n2 ). By solving this equation with respect\nto X, we will have two solutions for X:\nX1 =\n\np\n2\n\u03c3 \u2032 n + (\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed\n,\n2(\u2212\u01ed + \u03c3 \u2032 2n + \u03b3 2 )\n\nX2 =\n\np\n2\n\u03c3 \u2032 n \u2212 (\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed\n.\n2(\u2212\u01ed + \u03c3 \u2032 2n + \u03b3 2 )\n\nand\n\nFirst, it is important to note that both of these solutions are real, as\n2\nby substituting \u03b3 2 \u2212\u01ed with \u01eb we will have that (\u03c3 \u2032 n +2\u03b3 2 )2 \u22124\u03b3 2 \u01ed =\n2\n4\n\u03c3 \u2032 n + 4\u03b3 2 \u03c3 \u2032 n + 4\u03b3 2 \u01eb > 0. Furthermore, it also shows that X1 > 0\nand X2 < 0. As we are looking for a \u03bd \u2217 that satisfies the constraint\n1 \u2212 2\u03bd \u2217 \u03c3n2 > 0, the latter solution X2 is not acceptable, and so we\nhave X = (1 \u2212 2\u03bd \u2217 \u03c3n2 ) = X1 . By taking the second derivative of\n\u2217\n\u22022\ng(\u03bd) with respect to \u03bd, it is easy to show that \u2202\u03bd\n2 g(\u03bd ) \u2265 0 and so\n\u2217\ng(\u03bd) will reach its minimum value at \u03bd . It is important to note that\nthe following expressions are also valid and can be extracted from\n\nthe expression for X:\np\n2\n\u03c3 \u2032 n + (\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed\n\u22122(\u01ed + \u03c3 \u2032 2n + \u03b3 2 )\np\n2\n(\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed \u2212 \u03c3 \u2032 n\n1\n=\n1 \u2212 2\u03bd \u2217 \u03c3n2\n2\u03b3 2\np\n2\n\u20322\n2\u03b3 \u2212 2\u01ed + \u03c3 n \u2212 (\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed\n\u03bd\u2217 =\n4\u03c3 2 (\u03b3 2 \u2212 \u01ed + \u03c3 \u2032 2n )\np n\n2\n2 \u2217\n(\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed \u2212 \u03c3 \u2032 n \u2212 2\u03b3 2\n\u03b3 \u03bd\n=\n.\n\u2217\n2\n2\n1 \u2212 2\u03bd \u03c3n\n4\u03c3n\n1 \u2212 2\u03bd \u2217 \u03c3n2 =\n\n(68)\n(69)\n(70)\n(71)\n\nBy looking at (71), it is obvious that the nominator of the\nright\nhand side of this equation is a negative term (because\np\n2\n(\u03c3 \u2032 2n + 2\u03b3 2 )2 \u2212 4\u03b3 2 \u01ed < \u03c3 \u2032 n + 2\u03b3 2 ), while the denominator, i.e.\n2\n4\u03c3n , is a positive term. So, by using the fact that 1 \u2212 2\u03c3n2 \u03bd \u2217 > 0\n(as we have proven before), one can concludes that \u03bd \u2217 < 0, which\ncompletes the proof.\nACKNOWLEDGMENT\nThe authors would like to thank Maryam Sharifzadeh, MS student of Mathematical and Computer Science Department at Sharif\nUniversity of Technology, for her helpful comments. Additionally,\nthe authors thank Prof. Vahid Tarokh for his remarkable research\naround this subject which was the main motivation for this work.\nThe authors are also grateful to the anonymous reviewers and to the\nassociate editor for their constructive comments.\nR EFERENCES\n[1] B. Babadi, N. Kalouptsidis, and V. Tarokh, \"Asymptotic achievability\nof the Cram\u00e9r\u2013Rao bound for noisy compressive sampling,\" IEEE\nTransactions on Signal Processing, vol. 57, no. 3, pp. 1233\u20131236, 2009.\n[2] M. Ak\u00e7akaya and V. Tarokh, \"Shannon theoretic limits on noisy compressive sampling,\" IEEE Transactions on Information Theory, vol. 56,\nno. 1, pp. 492\u2013504, 2010.\n[3] D. Donoho, \"Compressed sensing,\" IEEE Transactions on Information\nTheory, vol. 52, no. 4, pp. 1289\u20131306, 2006.\n[4] E. Cand\u00e8s, \"Compressive sampling,\" in Proceedings of the International\nCongress of Mathematicians, vol. 3. Citeseer, 2006, p. 14331452.\n[5] Y. Tsaig and D. Donoho, \"Extensions of compressed sensing,\" Signal\nprocessing, vol. 86, no. 3, pp. 549\u2013571, 2006.\n[6] E. Cand\u00e8s, J. Romberg, and T. Tao, \"Robust uncertainty principles: Exact\nsignal reconstruction from highly incomplete frequency information,\"\nIEEE Transactions on information theory, vol. 52, no. 2, p. 489, 2006.\n[7] R. Baraniuk, E. Cand\u00e8s, R. Nowak, and M. Vetterli, \"Compressive\nsampling,\" IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 12\u201313,\n2008.\n[8] R. Niazadeh, M. Babaie-Zadeh, and C. Jutten, \"An alternating minimization method for sparse channel estimation,\" in Ninth International\nConference on Latent Variable Analysis and Signal Seperation (LVAICA, formerly known as ICA). Springer-Verlag, 2010, pp. 319\u2013327.\n[9] E. Cand\u00e8s, J. Romberg, and T. Tao, \"Stable signal recovery from\nincomplete and inaccurate measurements,\" Communications on Pure and\nApplied Mathematics, vol. 59, no. 8, pp. 1207\u20131223, 2006.\n[10] J. Tropp and A. Gilbert, \"Signal recovery from random measurements\nvia orthogonal matching pursuit,\" IEEE Transactions on Information\nTheory, vol. 53, no. 12, p. 4655, 2007.\n[11] D. Donoho, \"For most large underdetermined systems of linear equations\nthe minimal l1-norm solution is also the sparsest solution,\" Communications on Pure and Applied Mathematics, vol. 59, no. 6, pp. 797\u2013829,\n2006.\n[12] S. Chen, D. Donoho, and M. Saunders, \"Atomic decomposition by basis\npursuit,\" SIAM review, vol. 43, no. 1, pp. 129\u2013159, 2001.\n[13] T. Blumensath and M. Davies, \"Gradient pursuits,\" IEEE Transactions\non Signal Processing, vol. 56, no. 6, pp. 2370\u20132382, 2008.\n[14] I. Gorodnitsky and B. Rao, \"Sparse signal reconstruction from limited\ndata using FOCUSS: are-weighted minimum norm algorithm,\" IEEE\nTransactions on Signal Processing, vol. 45, no. 3, pp. 600\u2013616, 1997.\n[15] D. Needell and J. Tropp, \"CoSaMP: Iterative signal recovery from\nincomplete and inaccurate samples,\" Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301\u2013321, 2009.\n\n\f9\n\n[16] H. Mohimani, M. Babaie-Zadeh, and C. Jutten, \"A Fast Approach for\nOvercomplete Sparse Decomposition Based on Smoothed l0 Norm,\"\nIEEE Transactions on Signal Processing, vol. 57, no. 1, pp. 289\u2013301,\n2009.\n[17] S. Kay, \"Fundamentals of statistical signal processing: estimation theory,\" Prentice-Hall Signal Processing Series, 1993.\n[18] C. Carbonelli, S. Vedantam, and U. Mitra, \"Sparse channel estimation\nwith zero tap detection,\" IEEE Transactions on Wireless Communications, vol. 6, no. 5, pp. 1743\u20131763, 2007.\n[19] E. Cand\u00e8s and T. Tao, \"The Dantzig selector: statistical estimation when\np is much larger than n,\" Annals of Statistics, vol. 35, no. 6, pp. 2313\u2013\n2351, 2007.\n[20] Z. Ben-Haim and Y. Eldar, \"The Cram\u00e9r-Rao bound for estimating\na sparse parameter vector,\" Signal Processing, IEEE Transactions on,\nvol. 58, no. 6, pp. 3384\u20133389, 2010.\n[21] J. Haupt and R. Nowak, \"Signal reconstruction from noisy random\nprojections,\" IEEE Transactions on Information Theory, vol. 52, no. 9,\npp. 4036\u20134048, 2006.\n[22] T. Cover and J. Thomas, Elements of information theory. Wiley, 2006.\n[23] C. Shannon, \"A mathematical theory of communication,\" ACM SIGMOBILE Mobile Computing and Communications Review, vol. 5, no. 1,\np. 55, 2001.\n[24] R. Baraniuk, M. Davenport, R. DeVore, and M. Wakin, \"A simple proof\nof the restricted isometry property for random matrices,\" Constructive\nApproximation, vol. 28, no. 3, pp. 253\u2013263, 2008.\n[25] D. Achlioptas, \"Database-friendly random projections: JohnsonLindenstrauss with binary coins,\" Journal of Computer and System\nSciences, vol. 66, no. 4, pp. 671\u2013687, 2003.\n[26] T. Hagerup and C. Rub, \"A guided tour of Chernoff bounds,\" Proc. Lett,\nvol. 33, pp. 305\u2013308, 1989.\n[27] A. Papoulis, S. Pillai, P. A, and P. SU, Probability, random variables,\nand stochastic processes. McGraw-Hill New York, 1965.\n[28] J. Haupt, W. Bajwa, G. Raz, and R. Nowak, \"Toeplitz compressed sensing matrices with applications to sparse channel estimation,\" Information\nTheory, IEEE Transactions on, vol. 56, no. 11, pp. 5862\u20135875, 2010.\n\n\f"}