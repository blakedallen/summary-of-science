{"id": "http://arxiv.org/abs/0802.1412v1", "guidislink": true, "updated": "2008-02-11T11:12:06Z", "updated_parsed": [2008, 2, 11, 11, 12, 6, 0, 42, 0], "published": "2008-02-11T11:12:06Z", "published_parsed": [2008, 2, 11, 11, 12, 6, 0, 42, 0], "title": "Extreme Learning Machine for land cover classification", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0802.3825%2C0802.0770%2C0802.1230%2C0802.0996%2C0802.0412%2C0802.1790%2C0802.2701%2C0802.4431%2C0802.3106%2C0802.4263%2C0802.0323%2C0802.0678%2C0802.2345%2C0802.2761%2C0802.4084%2C0802.2473%2C0802.0220%2C0802.3492%2C0802.2892%2C0802.2025%2C0802.0779%2C0802.3301%2C0802.1702%2C0802.0628%2C0802.3691%2C0802.0102%2C0802.3263%2C0802.2804%2C0802.4015%2C0802.4105%2C0802.2585%2C0802.3119%2C0802.2239%2C0802.3845%2C0802.2472%2C0802.4132%2C0802.0700%2C0802.4362%2C0802.2905%2C0802.2336%2C0802.0614%2C0802.0696%2C0802.1095%2C0802.3240%2C0802.3877%2C0802.2246%2C0802.0016%2C0802.0237%2C0802.4417%2C0802.3568%2C0802.2831%2C0802.2510%2C0802.3554%2C0802.2300%2C0802.2900%2C0802.4204%2C0802.1278%2C0802.4432%2C0802.0717%2C0802.0080%2C0802.2645%2C0802.3182%2C0802.3831%2C0802.0811%2C0802.2433%2C0802.4412%2C0802.1076%2C0802.3571%2C0802.1121%2C0802.2271%2C0802.0612%2C0802.0930%2C0802.1336%2C0802.1829%2C0802.4396%2C0802.3070%2C0802.2969%2C0802.0136%2C0802.1412%2C0802.1062%2C0802.3080%2C0802.0005%2C0802.3633%2C0802.2925%2C0802.0496%2C0802.2806%2C0802.4429%2C0802.2981%2C0802.2375%2C0802.2785%2C0802.1316%2C0802.4305%2C0802.3274%2C0802.3158%2C0802.3285%2C0802.2310%2C0802.3704%2C0802.3234%2C0802.0572%2C0802.3220%2C0802.1190&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Extreme Learning Machine for land cover classification"}, "summary": "This paper explores the potential of extreme learning machine based\nsupervised classification algorithm for land cover classification. In\ncomparison to a backpropagation neural network, which requires setting of\nseveral user-defined parameters and may produce local minima, extreme learning\nmachine require setting of one parameter and produce a unique solution. ETM+\nmultispectral data set (England) was used to judge the suitability of extreme\nlearning machine for remote sensing classifications. A back propagation neural\nnetwork was used to compare its performance in term of classification accuracy\nand computational cost. Results suggest that the extreme learning machine\nperform equally well to back propagation neural network in term of\nclassification accuracy with this data set. The computational cost using\nextreme learning machine is very small in comparison to back propagation neural\nnetwork.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0802.3825%2C0802.0770%2C0802.1230%2C0802.0996%2C0802.0412%2C0802.1790%2C0802.2701%2C0802.4431%2C0802.3106%2C0802.4263%2C0802.0323%2C0802.0678%2C0802.2345%2C0802.2761%2C0802.4084%2C0802.2473%2C0802.0220%2C0802.3492%2C0802.2892%2C0802.2025%2C0802.0779%2C0802.3301%2C0802.1702%2C0802.0628%2C0802.3691%2C0802.0102%2C0802.3263%2C0802.2804%2C0802.4015%2C0802.4105%2C0802.2585%2C0802.3119%2C0802.2239%2C0802.3845%2C0802.2472%2C0802.4132%2C0802.0700%2C0802.4362%2C0802.2905%2C0802.2336%2C0802.0614%2C0802.0696%2C0802.1095%2C0802.3240%2C0802.3877%2C0802.2246%2C0802.0016%2C0802.0237%2C0802.4417%2C0802.3568%2C0802.2831%2C0802.2510%2C0802.3554%2C0802.2300%2C0802.2900%2C0802.4204%2C0802.1278%2C0802.4432%2C0802.0717%2C0802.0080%2C0802.2645%2C0802.3182%2C0802.3831%2C0802.0811%2C0802.2433%2C0802.4412%2C0802.1076%2C0802.3571%2C0802.1121%2C0802.2271%2C0802.0612%2C0802.0930%2C0802.1336%2C0802.1829%2C0802.4396%2C0802.3070%2C0802.2969%2C0802.0136%2C0802.1412%2C0802.1062%2C0802.3080%2C0802.0005%2C0802.3633%2C0802.2925%2C0802.0496%2C0802.2806%2C0802.4429%2C0802.2981%2C0802.2375%2C0802.2785%2C0802.1316%2C0802.4305%2C0802.3274%2C0802.3158%2C0802.3285%2C0802.2310%2C0802.3704%2C0802.3234%2C0802.0572%2C0802.3220%2C0802.1190&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper explores the potential of extreme learning machine based\nsupervised classification algorithm for land cover classification. In\ncomparison to a backpropagation neural network, which requires setting of\nseveral user-defined parameters and may produce local minima, extreme learning\nmachine require setting of one parameter and produce a unique solution. ETM+\nmultispectral data set (England) was used to judge the suitability of extreme\nlearning machine for remote sensing classifications. A back propagation neural\nnetwork was used to compare its performance in term of classification accuracy\nand computational cost. Results suggest that the extreme learning machine\nperform equally well to back propagation neural network in term of\nclassification accuracy with this data set. The computational cost using\nextreme learning machine is very small in comparison to back propagation neural\nnetwork."}, "authors": ["Mahesh Pal"], "author_detail": {"name": "Mahesh Pal"}, "author": "Mahesh Pal", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1080/01431160902788636", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0802.1412v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0802.1412v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "6 pages, mapindia 2008 conference", "arxiv_primary_category": {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0802.1412v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0802.1412v1", "journal_reference": null, "doi": "10.1080/01431160902788636", "fulltext": "Extreme Learning Machine for land cover classification\nBy\n\nMahesh Pal\n\nThis paper is part of the Proceeding of Mapindia2008, 11th Annual international\nconference and exhibition on geospatial information, technology and application, held at\nGreater Noida, India from 6-8 Feb. 2008.\n\nWebsite of the conference is: www.mapindia.org\n\n\fExtreme Learning Machine for land cover classification\nMahesh Pal\nDepartment of Civil Engineering\nN. I. T. Kurukshetra, Harayana, 136119, INDIA\nmpce_pal@yahoo.co.uk, 01744 233356, Fax: 01744 238050\n\nAbstract: This paper explores the potential of extreme learning machine based supervised\nclassification algorithm for land cover classification. In comparison to a backpropagation\nneural network, which requires setting of several user-defined parameters and may produce\nlocal minima, extreme learning machine require setting of one parameter and produce a unique\nsolution. ETM+ multispectral data set (England) was used to judge the suitability of extreme\nlearning machine for remote sensing classifications. A back propagation neural network was\nused to compare its performance in term of classification accuracy and computational cost.\nResults suggest that the extreme learning machine perform equally well to back propagation\nneural network in term of classification accuracy with this data set. The computational cost\nusing extreme learning machine is very small in comparison to back propagation neural\nnetwork.\n1. Introduction\nWithin the last two decades, neural network classifiers, particularly the feed-forward multilayer perceptron using back-propagation algorithm, have been extensively tested for different\napplications by remote sensing community (Benediktsson et al., 1990; Hepner et al., 1990;\nHeermann and Khazenie, 1992; Civco, 1993; Schaale and Furrer, 1995; Tso and Mather,\n2001). The popularity of neural network based classifiers is due to their ability to learn and\ngeneralize well with test data. In particular, neural networks make no prior assumptions about\nthe statistics of input data. This property makes neural networks an attractive solution to many\nland cover classification of remotely sensed data whose underlying distribution is quite often\nunknown. The multilayer feed forward network is one of the most widely used neural network\narchitectures. Among the various learning algorithms, the error backpropagation algorithm is\none of the most important and widely used algorithms in remote sensing. A number of studies\nhave reported that use of back propagation neural classifier have problems in setting various\nparameters during training (Kavzoglu, 2001; Wilkinson, 1997). The choice of network\narchitecture (i.e. number of hidden layers and nodes in each layer, learning rate as well as\nmomentum), weight initialisation and number of iterations required for training are some of the\nimportant parameters the affects the learning performance of these classifiers. The other\nshortcomings of the conventional backpropagation learning algorithm are slow convergence\nrate and it can get stuck to a local minimum.\nRecently, Huang et al., (2006) proposed extreme learning machine based classification\napproach (also called as single hidden layer feed forward neural network) with randomly\nassigned input weights and bias. They suggested that this classification approach may not\nrequire adjusting the input weights like a backpropagation method and found it working well\nwith different data set in comparison to back propagation neural network. Keeping this in view\npresent study compares the performance of extreme learning machine with a back propagation\nneural network using multispectral data.\n\n\f2.0 Extreme Learning Machine\nLet the training data with K number of samples be represented by {xi , y i } , where x i \u2208 R p\nand y i \u2208 R q , a standard single hidden layer feed forward neural network (Huang and Babri,\n1997) having H hidden neurons and activation function f (x ) can be represented as:\nH\n\n( )\n\n(\n\nH\n\n)\n\n\u2211 \u03b1 i f i x j = \u2211 \u03b1 i f w i . x j + ci = e j\n\ni =1\n\ni =1\n\nwhere j=1, ......,K,\n\n(1)\n\nWhere w i and \u03b1 i are the weight vectors connecting inputs and the ith hidden neurons and\nthe ith hidden neurons and output neurons respectively, ci is the threshold of the ith hidden\nneuron and e j is the output from single hidden layer feed forward neural network (SHLFN) for\nthe data point j. The weight vector w i is randomly generated and based on a continuous\nprobability distribution (Huang et al., 2006).\nHuang et al., (2006) suggested that a standard single layer feed forward neural network with\nH hidden neurons and activation function f (x ) can approximate K training data with zero error\nmeans such that:\nK\n\n\u2211 ej \u2212 oj = 0\n\n(2)\n\nj =1\n\nAnd the equation (1) can be expressed as\n\n(\n\nH\n\n)\n\n\u2211 \u03b1 i f w i . x j + ci = o j ,\n\ni =1\n\nj = 1, ......, K\n\n(3)\n\nfor particular values of \u03b1 i , w i , and ci .\nFurther, Huang et al., (2006) proposed that equation (3) can be written in a compact form and\nrepresented by the following equation:\nA\u03b1 = Y\n(4)\nWhere A is called the hidden layer output matrix of the neural network (Huang and Babri,\n1997) and defined as:\n\u23a1 f (w 1 . x1 + c1 ) ..... f (w H . x1 + c H )\u23a4\n\u23a2\n\u23a5\n.\n.\n\u23a5\nA(w 1 ,......, w H , c1 ,......., c H , x1 ,......., x K ) = \u23a2\n(5)\n\u23a2\n\u23a5\n.\n.\n\u23a2\n\u23a5\n\u23a3 f (w 1 . x K + c1 ) ..... f (w H . x K + c H )\u23a6 K \u00d7 H\n\n\u23a1y 1T \u23a4\n\u23a1\u03b1 1T \u23a4\n\u23a2 \u23a5\n\u23a2 \u23a5\n. \u23a5\n.\n\u23a2\n(6)\n\u03b1 =\u23a2 \u23a5\nand\nY = \u23a2\u23a2 \u23a5\u23a5\n.\n.\n\u23a2 \u23a5\n\u23a2 \u23a5\n\u23a2\u23a3y TK \u23a5\u23a6 K \u00d7m\n\u23a2\u23a3\u03b1 HT \u23a5\u23a6 H \u00d7m\nThe ith column of A is the ith hidden neuron's output vector with respect to\ninputs x1 , x 2 ......, x K .\nThe SHLFN can be solved by using a gradient based solution and one need to find the\nsuitable\nvalues\nof\nw 'i , ci' and \u03b1 ' (i = 1,......, H )\nsuch\nthat\n\n(\n\n)\n\nA w 1' , ......, w 'H , c1' ,......., c H' \u03b1 ' \u2212 Y\n\n=\n\nmin A (w 1 , ......, w H , c1 ,......., c H )\u03b1 \u2212 Y\n\nw i , ci , \u03b1\n\nEquation (7) can be written in form of the following cost function\n\n(7)\n\n\f2\n\nK \u239b H\n\u239e\n(8)\nC = \u2211 \u239c\u239c \u2211 \u03b1 i f w i . x j + ci \u2212 o j \u239f\u239f\nj =1\u239d i =1\n\u23a0\nwhich can be minimised to find suitable values of w 'i , ci' and \u03b1 ' (i = 1,......, H ) .\nIn case the hidden layer output matrix of neural network (i.e. A) is unknown, a gradient based\nlearning algorithm minimise the A \u03b1 \u2212 Y by adjusting a vector W (i.e. a set\n\n(\n\n)\n\nof weights (w i , \u03b1 i ) and biases (ci ) ) iteratively by using the following relationship:\n\u2202 C (W )\nW p = W p \u22121 \u2212\u03b7\n(9)\n\u2202W\nwhere \u03b7 is the learning rate and backpropagation learning algorithms is one of the most\npopular algorithm used to compute the gradients in a feed forward neural network.\nRecently, the study carried out by Huang et al., (2003) proved that single layer feed forward\nneural network with randomly assigned input weights and hidden layer biases and with almost\nany nonzero activation function can universally approximate any continuous functions on any\ninput data sets. Huang et al., (2006) suggested an alternate way to train a SHLFN by finding a\nleast square solution \u03b1 ' of the linear system represented by equation 4:\n(10)\nA(w1 , ......, w H , c1 ,......., cH )\u03b1 ' \u2212 Y = min A(w1 , ......, w H , c1 ,......., cH )\u03b1 \u2212 Y\n\u03b1\n\nIf the number H = K, matrix A is square and invertible but in most of the cases number of\nhidden nodes are less than the number of training samples, which makes matrix A to be a non\nsquare matrix and there may not exist w i , ci , \u03b1 i (i =1,.......H ) such that A \u03b1 = Y . To overcome\nthis problem, Huang et al., (2006) proposed in using smallest norm least squares solution\nof A \u03b1 = Y , thus, the solution of equation 4 becomes:\n\n\u03b1 ' = A@ Y\n\n(11)\n\n@\n\nWhere A is called Moore-Penrose generalized inverse of matrix A (Serre, 2002). This\nsolution has the following important properties (Huang et al., 2006):\n1. The smallest training error can be reached by this solution.\n2. Smallest norm of weights and best generalization performance.\n3. The minimum norm least-square solution is a unique solution, thus involving no local\nminima like one in backpropagation learning algorithm.\nThus, the algorithm proposed by Huang et al., (2006) and called as extreme learning machine\ncan be summarized as:\nWith the training data set {xi , y i } , x i \u2208 R n , y i = \u2208 R m having K number of samples, a standard\nSHLFN algorithm with H hidden neurons and activation function g (x ) will work as:\n1. Assign random input weights w i and bias ci , i = 1,.......H .\n2. Calculate the hidden layer output matrix A.\n3. Calculate the output weights \u03b1 by using the following equation\n\u03b1 = A@ Y\nWhere A \u03b1 and Y are as defined in equations 5 and 6.\n3. Data Sets and Methodology\nThe study areas used in this study is located near the town of Littleport in eastern England\nand the image was acquired on 19 June 2000. A sub-image consisting of 307-pixel (columns)\nby 330-pixel (rows) covering the area of interest was used for subsequent analysis and\nclassification problem involved in identification of seven land cover types (i.e. wheat, potato,\nsugar beet, onion, peas, lettuce and beans). A total of 4737 pixels were selected for all seven\n\n\fclasses using stratified random sampling. The pixels collected were divided into two subsets,\none of which was used for training and the second for testing the classifiers, so as to remove\nany bias resulting from the use of the same set of pixels for both training and testing. Also,\nbecause the same test and training data sets are used for each classifier, any difference resulting\nfrom sampling variations was avoided. A total of 2700 training and 2037 test pixels were used.\n4. Results\nThe purpose of the present study is to evaluate the performance of extreme learning machine\nfor land cover classification and comparing its performance with a back propagation neural\nnetwork classifier. Unlike back propagation neural network, the design of extreme learning\nmachine requires setting of one user-defined parameter i.e. number of hidden nodes in hidden\nlayer. A number of experiments were carried out by using the training and test data set of 2700\npixels and varying the hidden nodes from 25 to 450. Results suggests that extreme learning\nmachine achieves highest classification accuracy with a total of 300 hidden nodes.\n\nTable1. Classification accuracy, user-defined parameters as well computational cost with\nboth classifiers.\nClassifier used\nUser defined parameters\nAccuracy\nComputational\n(%)\ncost (seconds)\n\nExtreme learning machine\n\nNumber of hidden nodes = 300\n\n89.0\n\nBack propagation neural\nnetwork\n\nLearning rate =0.25, Momentum =\n0.2, nodes in hidden layers =26,\nnumber of iterations = 2200,\nnumber of hidden layers =1\n\n87.75\n\n1.25\n\n336.20\n\nTable 1 provides the results obtained by using extreme learning machine as well as back\npropagation neural network using ETM+ (England) data set. Results suggest that extreme\nlearning machine perform well in comparison to the back propagation neural network. With\nthis dataset, extreme learning machine provide a classification accuracy of 89% in comparison\nto 87.87% by a back propagation neural network. Computational cost (i.e. training and test\ntime) of a classifier often represents a significant proportion of cost in remote sensing\nclassifications. For all experiments in this study, a personal computer with a Pentium IV\nprocessor and 512 MB of RAM was used. Table 1 also provide the computational cost using\nETM+ data set with extreme learning machine and back propagation neural network. The\nresults (table 1) suggest the usefulness of extreme learning machine in comparison to back\npropagation neural network in term of computational cost also.\n5.0 Conclusions\n\nThe main aim of this study was to assess the usefulness of extreme learning machine based\nclassification approach for land cover classification using multispectral data. The performance\nof extreme learning machine was compared with a back propagation neural network. The\nresults presented above suggest that extreme learning machine works equally well to back\npropagation neural network in term of classification accuracy and involves in using a smaller\ncomputational cost. Another conclusion about the use extreme learning machine classification\napproach is that unlike a back propagation neural network classifier its performance is affected\nby one user-defined parameter only which can easily be identified for a particular data set.\n\n\fReferences\nBenediktsson, J. A., Swain, P. H. & Erase, O. K. (1990). Neural network approaches versus\nstatistical methods in classification of multisource remote sensing data. IEEE Transactions on\nGeoscience and Remote Sensing, 28, 540-551.\nCivco, D. L. (1993). Artificial neural networks for land-cover classification and mapping.\nInternational Journal of Geographic Information Systems, 7, 173-183.\nHeerman, P. D. & Khazenie, N. (1992). Classification of multispectral remote sensing data\nusing a back propagation neural network. IEEE Transactions on Geoscience and Remote\nSensing, 30, 81-88.\nHepner, G. F., Logan, T., Ritter, N. & Bryant, N. (1990). Artificial neural network\nclassification using a minimal training set: comparison to conventional supervised\nclassification. Photogrammetric Engineering and Remote Sensing, 56, 469-473.\nHuang, G.-B. & Babri, H. A. (1997). General approximation theorem on feed forward\nnetworks. IEEE Proceedings of International Conference on Information, Communications\nand Signal Processing, 698-702.\nHuang, G.-B. Zhu, Q.-Y. & Siew, C.-K. (2006). Extreme learning machine: Theory and\napplications, Neurocomputing, 70, 489\u2013501\nHuang, G.-B., Chen, L. & Siew, C.-K. (2003). Universal approximation using incremental feed\nforward networks with arbitrary input weights, Technical Report ICIS/46/2003, School of\nElectrical and Electronic Engineering, Nanyang Technological University, Singapore.\nKavzoglu, T. (2001). An Investigation of the Design and Use of Feed-forward Artificial Neural\nNetworks in the Classification of Remotely Sensed Images. PhD thesis. School of Geography,\nThe University of Nottingham, Nottingham, UK.\nSerre, D. (2002). Matrices: Theory and applications. Springer-Verlag New York.\nSchaale M. & Furrer, R. (1995). Land surface classification by neural networks. International\nJournal of Remote Sensing, 16, 3003-3032.\nTso, B. K. C. & Mather, P.M. (2001). Classification Methods for remotely Sensed Data.\nLondon: Taylor and Francis Ltd.\nWilkinson, G.G. (1997). Open questions in neurocomputing for Earth observation. NeuroComputational in Remote Sensing Data Analysis. New York: Springer-Verlag, 3-13.\n\nMahesh Pal is currently working as assistant professor in the department of civil\nengineering, NIT Kurukshetra, Haryana. He has 35 Paper in\ninternational/national journals and conferences. His current interests include\nkernel based approaches for land cover classification and application of GIS in\nconstruction management.\n\n\f"}