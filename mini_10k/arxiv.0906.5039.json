{"id": "http://arxiv.org/abs/0906.5039v1", "guidislink": true, "updated": "2009-06-27T04:46:23Z", "updated_parsed": [2009, 6, 27, 4, 46, 23, 5, 178, 0], "published": "2009-06-27T04:46:23Z", "published_parsed": [2009, 6, 27, 4, 46, 23, 5, 178, 0], "title": "A new approach for digit recognition based on hand gesture analysis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.2422%2C0906.3290%2C0906.0227%2C0906.5118%2C0906.2547%2C0906.5109%2C0906.0951%2C0906.3211%2C0906.1185%2C0906.0071%2C0906.0968%2C0906.0672%2C0906.4209%2C0906.4454%2C0906.5129%2C0906.2764%2C0906.4296%2C0906.5039%2C0906.1736%2C0906.4014%2C0906.1575%2C0906.5253%2C0906.1126%2C0906.5164%2C0906.1672%2C0906.2164%2C0906.2884%2C0906.5374%2C0906.3147%2C0906.3121%2C0906.1037%2C0906.0534%2C0906.0824%2C0906.1320%2C0906.0285%2C0906.3036%2C0906.2055%2C0906.1424%2C0906.1167%2C0906.0758%2C0906.0988%2C0906.5407%2C0906.2346%2C0906.2272%2C0906.3287%2C0906.1426%2C0906.1332%2C0906.2387%2C0906.2689%2C0906.1709%2C0906.1051%2C0906.1503%2C0906.2840%2C0906.4271%2C0906.1292%2C0906.0263%2C0906.3747%2C0906.3592%2C0906.4354%2C0906.1625%2C0906.1817%2C0906.0035%2C0906.1662%2C0906.0536%2C0906.4842%2C0906.3639%2C0906.2770%2C0906.2266%2C0906.0398%2C0906.1479%2C0906.0698%2C0906.2134%2C0906.1206%2C0906.4904%2C0906.5240%2C0906.2263%2C0906.3075%2C0906.3518%2C0906.3538%2C0906.4507%2C0906.5430%2C0906.4818%2C0906.1992%2C0906.1909%2C0906.2818%2C0906.0609%2C0906.4731%2C0906.2537%2C0906.0270%2C0906.1877%2C0906.1803%2C0906.2879%2C0906.5479%2C0906.3168%2C0906.5070%2C0906.3531%2C0906.0420%2C0906.0571%2C0906.4659%2C0906.2019%2C0906.2982&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A new approach for digit recognition based on hand gesture analysis"}, "summary": "We present in this paper a new approach for hand gesture analysis that allows\ndigit recognition. The analysis is based on extracting a set of features from a\nhand image and then combining them by using an induction graph. The most\nimportant features we extract from each image are the fingers locations, their\nheights and the distance between each pair of fingers. Our approach consists of\nthree steps: (i) Hand detection and localization, (ii) fingers extraction and\n(iii) features identification and combination to digit recognition. Each input\nimage is assumed to contain only one person, thus we apply a fuzzy classifier\nto identify the skin pixels. In the finger extraction step, we attempt to\nremove all the hand components except the fingers, this process is based on the\nhand anatomy properties. The final step consists on representing histogram of\nthe detected fingers in order to extract features that will be used for digit\nrecognition. The approach is invariant to scale, rotation and translation of\nthe hand. Some experiments have been undertaken to show the effectiveness of\nthe proposed approach.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0906.2422%2C0906.3290%2C0906.0227%2C0906.5118%2C0906.2547%2C0906.5109%2C0906.0951%2C0906.3211%2C0906.1185%2C0906.0071%2C0906.0968%2C0906.0672%2C0906.4209%2C0906.4454%2C0906.5129%2C0906.2764%2C0906.4296%2C0906.5039%2C0906.1736%2C0906.4014%2C0906.1575%2C0906.5253%2C0906.1126%2C0906.5164%2C0906.1672%2C0906.2164%2C0906.2884%2C0906.5374%2C0906.3147%2C0906.3121%2C0906.1037%2C0906.0534%2C0906.0824%2C0906.1320%2C0906.0285%2C0906.3036%2C0906.2055%2C0906.1424%2C0906.1167%2C0906.0758%2C0906.0988%2C0906.5407%2C0906.2346%2C0906.2272%2C0906.3287%2C0906.1426%2C0906.1332%2C0906.2387%2C0906.2689%2C0906.1709%2C0906.1051%2C0906.1503%2C0906.2840%2C0906.4271%2C0906.1292%2C0906.0263%2C0906.3747%2C0906.3592%2C0906.4354%2C0906.1625%2C0906.1817%2C0906.0035%2C0906.1662%2C0906.0536%2C0906.4842%2C0906.3639%2C0906.2770%2C0906.2266%2C0906.0398%2C0906.1479%2C0906.0698%2C0906.2134%2C0906.1206%2C0906.4904%2C0906.5240%2C0906.2263%2C0906.3075%2C0906.3518%2C0906.3538%2C0906.4507%2C0906.5430%2C0906.4818%2C0906.1992%2C0906.1909%2C0906.2818%2C0906.0609%2C0906.4731%2C0906.2537%2C0906.0270%2C0906.1877%2C0906.1803%2C0906.2879%2C0906.5479%2C0906.3168%2C0906.5070%2C0906.3531%2C0906.0420%2C0906.0571%2C0906.4659%2C0906.2019%2C0906.2982&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present in this paper a new approach for hand gesture analysis that allows\ndigit recognition. The analysis is based on extracting a set of features from a\nhand image and then combining them by using an induction graph. The most\nimportant features we extract from each image are the fingers locations, their\nheights and the distance between each pair of fingers. Our approach consists of\nthree steps: (i) Hand detection and localization, (ii) fingers extraction and\n(iii) features identification and combination to digit recognition. Each input\nimage is assumed to contain only one person, thus we apply a fuzzy classifier\nto identify the skin pixels. In the finger extraction step, we attempt to\nremove all the hand components except the fingers, this process is based on the\nhand anatomy properties. The final step consists on representing histogram of\nthe detected fingers in order to extract features that will be used for digit\nrecognition. The approach is invariant to scale, rotation and translation of\nthe hand. Some experiments have been undertaken to show the effectiveness of\nthe proposed approach."}, "authors": ["Ahmed Ben Jmaa", "Walid Mahdi", "Yousra Ben Jemaa", "Abdelmajid Ben Hamadou"], "author_detail": {"name": "Abdelmajid Ben Hamadou"}, "author": "Abdelmajid Ben Hamadou", "arxiv_comment": "8 Pages, International Journal of Computer Science and Information\n  Security", "links": [{"href": "http://arxiv.org/abs/0906.5039v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0906.5039v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0906.5039v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0906.5039v1", "journal_reference": "IJCSIS June 2009 Issue, Vol. 2, No. 1", "doi": null, "fulltext": "(IJCSIS) International Journal of Computer Science and Information Security,\nVol. 2, No. 1, 2009\n\nA new approach for digit recognition based on hand\ngesture analysis\nAhmed BEN JMAA#1, Walid MAHDI#2, Yousra BEN JEMAA*4 and Abdelmajid BEN HMADOU#3\n# Multimedia InfoRmation systems and Advanced Computing Laboratory\nHigher Institute of Computer Science and Multimedia\nSfax, Tunisia\n1\nahmed.benjmaa@gmail.com\n2\nwalid.mahdi@isimsf.rnu.tn\n3\nabdelmajid.benhamadou@isimsf.rnu.tn\n* Signal and System Research Unit\nNational Engineering School\nTunis, Tunisia\n4\nyousra.benjemaa@planet.tn\nAbstract- We present in this paper a new approach for hand\ngesture analysis that allows digit recognition. The analysis is\nbased on extracting a set of features from a hand image and then\ncombining them by using an induction graph. The most\nimportant features we extract from each image are the fingers\nlocations, their heights and the distance between each pair of\nfingers. Our approach consists of three steps: (i) Hand detection\nand localization, (ii) fingers extraction and (iii) features\nidentification and combination to digit recognition. Each input\nimage is assumed to contain only one person, thus we apply a\nfuzzy classifier to identify the skin pixels. In the finger extraction\nstep, we attempt to remove all the hand components except the\nfingers, this process is based on the hand anatomy properties.\nThe final step consists on representing histogram of the detected\nfingers in order to extract features that will be used for digit\nrecognition. The approach is invariant to scale, rotation and\ntranslation of the hand. Some experiments have been undertaken\nto show the effectiveness of the proposed approach.\nKeywordsrecognition\n\nSign\n\nlanguage,\n\nI.\n\nkeyboard\n\napplication,\n\ndigits\n\nINTRODUCTION\n\nThe gesture is a natural way of communication for many\ndeaf with sign language. Many machines are designed to be\noperated by gestures through mechanical and electronic\ninterfaces, such as a driving car, a robot remote control...\nThe gesture recognition is very difficult and complex task\nsince the full recognition system should be able to identify the\nhand in different scales, positions, orientations, contrasts,\nfunds, Luminosity, and others.\nHowever, many methods, has been developed in hand\ndetection, classified into many categories: knowledge-based\nmethods, feature based methods, template-based methods, and\nappearance based methods [16][17][18][19][20].\nWhen used separately, these methods cannot solve all\nproblems of hand detection. Hence, it is better to operate with\nseveral successive or parallel methods.\n\nThe gesture communication with machines are enriched by\ninstrumented gloves [11], which are used to control a virtual\nactors, to describe and manipulate objects on computer screens\n[1][2] or even to recognize the sign language [3].\nUnfortunately, these gloves are expensive and fragile, and their\ncables are a hindrance. Thus many researches are rather\ninterested in gesture acquisition. The hand gesture acquisition\nwith cameras for \"Human-Machine\" interaction applications or\nsign language recognition need to localize the hand in the\npicture, then to evaluate the 2D settings such as fingers tips\npositions or articulation angles. Gesture analysis can make use\nof colored gloves or markers on the hand [4]. The 2D analysis\nmethods of gestures are limited to recognize a limited number\nof predefined layouts. Berard et al. [5] have developed a\ndrawing application that provides a real-time position of a\nfinger, by correlation on the fingertip. Cootes et al. [6] have\nproposed deformable models by using a model of distribution\npoints, which represents any form of skeleton by a set of\nfeature points and variation patterns that describe the\nmovements of these points. This model is made from a set of\ntraining sequence by a singular values decomposition of the\ndifferences between the forms of a set of training sequence and\nthe averaged form. The recognition is based on the model of\ndistribution points. This has also been proposed by Martin and\nCrowley [7].\nAs for us, we consider that the fingers localization provides\na powerful issue of evidence to predict the hand sign. Thus,\nfingers analysis could be used efficiently to hand gesture\nrecognition.\nIn this paper, we present a new hand detection technique\nbased on the color classification using fuzzy logic system,\nimage segmentation using edge detection and we try testing\ntwo methods (Ellipse method and Comparative based method)\nin order to localize the hands from others objects detected in\nthe image. The results obtained from this new technique of\nhand detection and hand localization are used in our hand\ngesture recognition systems.\n\n\f(IJCSIS) International Journal of Computer Science and Information Security,\nVol. 2, No. 1, 2009\n\nThen, we present a method that put the fingers into a\nuniform localization. Based on its properties, the fingers pixels\ndistribution are basically the main input parameters to the\ngesture recognition process. We are based on digit recognition\nwhich represents a meaningful need in the sign language\nframework, to show the effectiveness of our hypothesis.\n\n2) Fuzzy classification: The proposed arrangements are not\nsufficient to find a good classification because of the diversity\nof human skin color and for many other reasons such as noise\nand shad. To overcome the limitation of classic classification,\nwe propose to apply a fuzzy approach for pixel classification.\n\nThis paper is organized as follows, in section two we\ndescribe the hand detection system and adjustment of its\norientation. In section three we present all techniques that we\nuse to remove all hand components except the fingers. In\nsection four the digit system recognition will be described.\nFinally, we present some experiments and results to show the\neffectiveness of the proposed approach.\n\nThis is considered as a good solution since that, fuzzy set\ntheory can represent and manipulate uncertainly and ambiguity\n[12]. In this paper, we use the Takagi-sugeno fuzzy inference\nsystem. This system is composed of two inputs (the 2\ncomponents Cb and Cr) and one output (the decision skin or\n\nII.\n\nHAND DETECTION AND LOCALIZATION\n\nHand detection is a very important stage to start the step of\nsign language recognition. In fact, to identify a geste, it is\nnecessary to localize hands and their characteristics in the\nimage. Our system supposes that only one person is presented\nin the field of the camera, it is described in (figure 1).\n\n(a) Filtered image\n\n(c) Cb\n\n(b) Y\n\n(d) Cr\n\nFigure 2. Conversion of an RGB image into YCbCr space : (a) the original\nimage, (b) the Y component, (c) the Cb component and (d) Cr component\nFigure 1. Steps of hand detection\n\nA. Skin color detection\nTo improve the quality of the image, we attenuate the noise\nby applying a low-pass filter. Many techniques have been\nproposed to determine human skin color and results are usually\nsubmitted to conditions of lighting [21][22][23].\n\nnon skin color). Each input has three sub-sets: light, medium\nand dark. Our algorithm uses the concept of fuzzy logic IFTHEN rules; these rules are applied in each pixel in the image\nin order to decide whether the pixel represents a skin or nonskin region [24]. (Figure 3) represents the input image and the\noutput one after YCbCr space conversion and fuzzy\nclassification.\n\n1) Skin color space: It is known that human skin color\ncontains a majority of red color due to the blood, but this\nremains largely insufficient to develop a robust algorithm using\nthe RGB space. In fact, the values of R, G and B component are\ndepending on the lighting conditions. We propose for our\nsystem to use the YCbCr color space in order to dissociate the\nluminance from the color information. Furthermore, the Cb and\nCr are the chrominance components used in MPEG and JPEG\n[9]. In (figure 2), we illustrate how the YCbCr space can\nseparate the skin color from the background.\nIn order to classify each pixel of the image in skin pixel and\nnon skin pixel, the most suitable arrangements that we found\nfor all images are Cb in [77, 127] and Cr in [139, 210]. These\nterminals are approved experimentally and are independent of\nrace human skin color [10].\n\nFigure 3. Fuzzy classification of the image into skin regions and non skin\nregions\n\n\f(IJCSIS) International Journal of Computer Science and Information Security,\nVol. 2, No. 1, 2009\n\nB. Edge detection\nEdge detection is the most common approach for detecting\nsignificant discontinuities in gray level. Edge detection\nalgorithms locate and accentuate edges in order to identify\nobjects in images.\nMany different methods have been proposed for edge\ndetection, such as Sobel filtering, Prewit filtering, Laplacian\nfiltering, Moment based operator, Shen operator and Canny\noperator. Among the edge operators mentioned above, the most\npowerful edge-detection method is the Canny method. It differs\nfrom the other ones because it uses two different thresholds\nallowing it to detect strong and weak edges, and includes the\nweak edges in the output only if they are connected to strong\nedges [25]. In (figure 4), we show the robustness of Canny\noperator for detecting edges.\n\nFigure 4. Edge detection with Canny operators\n\nAfter edge detection, the following step allowing to decide\nwhether the region represents a hand or not.\nC. Hand localization\nIn this step we test two methods such as (Ellipse and\nComparison based method) to localize hands.\n1) Ellipse based method: Because human face and hands\nhave an elliptic shapes, an algorithm searching such shape is\nhelpful for face and hand detection. Hough transformation is a\nwell-known shape detection technique of image processing\n[26][27][28]. So, we use it in our detection algorithm.\n\nFigure 6. Hand detection using elliptic method\n\n2) Comparison based method: This method consists to\nextract the three largest skin regions and to localize their\nbarycenters. In order to distinguish the face from hands we\npropose the following rules depending on the number of skin\nregions detected in the image.\n\u2022 Case 1: Image with three skin regions:\nIn this case we assume that there are a face and two hands\nin the image. Since the face have the largest area and the\nhighest barycenter in the image, we can localize it and\neliminate it. Each skin region is approximated by a rectangular\nsurface characterized by its height and width in order to\ndetermine its area. The region having the largest area represents\na potential face. To get the highest barycenter, we need to\ncompare the three centers and extract who has the largest\ncoordinate.\n\u2022\n\nCase 2: Image with two skin regions:\n\nIn this case we determine the ratio between the two surfaces\nof skin regions already detected. If the ratio is near to 1 (less\nthan 1.5), we assume then that there is two hands in the image,\nelse there are a hand and face. We must employ the same\ncriteria (the largest area and the highest barycenter in the\nimage) already mentioned to differentiate the face from the\nhand.\n\u2022\n\nCase 3: Image with a single skin region:\n\nWhen having a unique skin region, we cannot apply any\ncomparison and therefore we assume that it's a hand. This is the\nmajor weakness of this method.\n\nFigure 5. A samples of ellipse visualization\n\nAs is known, face and hands have elliptical shapes, we\nmust distinguish between them. To differentiate between the\nhands and the face, we must count the number of white pixels\nbelonging to the perimeter of the ellipse. Then we calculate for\neach ellipse the ratio between the number of white pixels\ncounted and the number of all pixels making up the perimeter.\nThis ratio is small for the hand than for the face because the\nhand contains black regions between the fingers.\n\nFigure 7. Hand detection using comparison based method\n\n\f(IJCSIS) International Journal of Computer Science and Information Security,\nVol. 2, No. 1, 2009\n\nD. Hand adjustment into the vertical direction\nThe hand adjustment is the most important phase in the\nhand localization steps. The main goal of this phase is to put\nthe hand into the vertical direction (normalized hand direction\nin our approach). We give details of each phase in the\nfollowing.\n\nThus, in order to identify it, we compute the number of skin\npixels in each ellipse half and retain the appropriate one.\n\nThe human hands have an elliptic shape, an algorithm\nfinding such shape is helpful for hand orientation.\nThe fit-estimation method we use is based on the least\nsquares method. The input parameters to find the best ellipse\nare a set of (x,y) points (the x,y coordinates of all skin detected\npixels), and the output is the minor and major axis, and the \u03b8\norientation for the best ellipse (the one that contains the\nmajority of (x,y) input points). The former parameter is\nassumed as the hand orientation. (Figure 8) gives an illustration\nof ellipse determined to the hand shape. The \u03b8 orientation is\ngiven by the angle between the ellipse major axis and the x\ncoordinate axis.\nWe use it to adjust the hand into the vertical direction.\n\n(a)\n\n(b)\n\nFigure 9. Morphology operations correction : (a) finding ellipse without\ncorrection. (b) finding ellipse after correction\n\nIII.\n\nFigure 8. Ellipse representation for hand orientation\n\nIn order to obtain a best orientation we attempt to find an\nellipse with major axis parallel to the hand axis. But when the\nthumb is far of other fingers we obtain an ellipse having a\nmajor axis not parallel to the hand one (figure 9). Thus we use\nmorphology operations to resolve this problem to capture more\nprecisely the elliptic shape.\nThe first morphology operation is dilation with a diamond\nstructuring element to add a white pixels between all the\nfingers except the thumb (because the thumb is usually far from\nthe other fingers), then we apply an erosion operation with a\ndisk structuring element to eliminate a large part of the thumb.\nAfter these morphology operations, we put the rest of pixels\ncoordinate in the ellipse finding algorithm.\nOnce the best ellipse is correctly found, we assume that the\nfingers belong to one of the two ellipse half's. We split the\nellipse into two half's using the minor axis. The ellipse half that\ncontains the fingers is the one that contains less skin pixels than\nthe other; because of the gap that separates each pair of fingers.\n\n1. These dimensions are different from the ellipse minor and major axis\nsize, but give better estimation of hand dimensions\n\nFINGERS EXTRACTION\n\nAll the features used in the digit recognition phase are\nextracted from the active fingers attributes (localizations,\ndistributions, heights and distances). Thus, we have to remove\nfrom the image all the hand components except the fingers.\nThis step consists of two phases: palm localization and fingers\nextraction.\nThe palm localization phase is based on rules defined in the\nhand anthropometry study of comparative measurements of the\nhuman body [8]. These rules are useful to define the palm\ndimension, starting from hand dimensions:\n\u2022\n\nThe palm length is equal to 0.496 \u00b1 0.003 times\nthe hand length.\n\n\u2022\n\nThe palm width is equal to 0.44 \u00b1 0.007 times the\nhand length.\n\nTo estimate the hand dimensions, we attempt to bound all\nskin pixels in the rectangular area having the smallest\nperimeter. The dimensions of this area will be considered as\nthose of the hand1. We start by defining the convex hull of the\nskin pixels; the most appropriate rectangular area should\ncontain at least one edge of the convex hull.\n(Figure 10) provides the result of the hand bounding on a\ngreen rectangular area applied on some examples.\nTo localize the palm, we start by translating randomly a\nrectangle window having the palm dimensions.\n\n\f(IJCSIS) International Journal of Computer Science and Information Security,\nVol. 2, No. 1, 2009\n\nThen we localize it in the region having the maximum\nnumber of skin pixels. The region bounded by the red rectangle\nshown by (figure 10) is the palm region.\n\nIV.\n\nFEATURES IDENTIFICATION FOR DIGIT RECOGNITION\n\nThe remaining skin pixels are those of the hand fingers.\n(Figure 11) shows that only the fingers pixels remain on the\nimage.\nStarting from these pixels, we project each one on the xcoordinate axis. For each x coordinate value x, we count the\nnumber of skin pixels (x,.), the obtained numbers results on a\nhistogram shown by (figure 12), with normalization by the\nhand length.\n\n(a) digit 1\n\n(b) digit 6\n\n(c) digit 4\n\n(d) digit 7\n\nFigure 10. Palm localization\n\nIn the fingers extraction phase, we then attempt to remove\nfrom the image all the hand components except the fingers.\nFirst, we start by drawing a circle passing through the vertices\nof rectangle found in the previous phase, so we remove all the\nskin pixels in this circle. This process is made to remove the\npalm. The part of wist is eliminated by removing all the skin\npixels below this rectangle.\n\nFigure 11. Removing hand components except fingers\n\nFigure 12. Histogram of skin pixels projection\n\nThe digit recognition is based on the set of peaks in the\nhistogram. We have used the smoothing method to capture all\nthe significant peaks in the histogram. The number of peaks\nprovides a significant closes for the digit recognition. In fact, if\nthe histogram contains one peak, the digit is 1 without any\ndoubt. But if the number of fingers is 3, many decisions could\nbe made depending on other features. In such case, 3, 6, 7, 8\nand 9 are shown using 3 fingers, but fingers configurations are\ndifferent from a digit to another. To differentiate between such\ncases, we have defined a set of additional features which take\ninto account the distance between each pair of successive\nfingers and their lengths, which are done by the amplitudes of\nthe retained peaks. Let us consider (xi,yi), i \u2208 {1...n} the set of\npeaks where n is the number of peaks (the number of active\nfingers), xi is the x coordinate of the ith peak and yi is its\namplitude. We compute the features distxi=|xi-xi+1| and\ndistyi=|yi-yi+1|, i \u2208 {1...n-1} and we attribute the zero value to\ndistxi and distyi for all n \u2264 i \u2264 4 when the number of peaks\ncannot exceed 5. For example, if the histogram contains three\npeaks (x1,y1), (x2,y2) and (x3,y3), we compute the distances\ndistx1=|x1-x2|, disty1=|y1-y2|, distx2=|x2-x3| and disty2=|y2-y3|, the\nremaining values of distx3, disty3, distx4 and disty4 are zeroed\nsince they correspond to all the i values with n=3 \u2264 i \u2264 4. These\nfeatures are used to make our approach invariant of the\nhorizontal and vertical positions of the hand in the image.\nIn order to enhance the invariance of our model to the hand\nscale, we compute an additional feature which represents the\n\n\f(IJCSIS) International Journal of Computer Science and Information Security,\nVol. 2, No. 1, 2009\n\nratio between two successive non zero distances\n(rxi=distxi/distxi+1 and ryi=distyi/distyi+1). In fact, the distance\n(distxi, distyi) depends on the scale, but the ratio (rxi, ryi)\ncorresponding to the same hand in two different scales is\nmaintained the same. These features are then provided in a\nvector form, let:\nV=(n,distx1,disty1...distx4,disty4,rx1,ry1... rx4,ry4)\nwhere n is the number of detected peaks. A set of vectors in\nsuch format is provided as input of the decision tree based\nalgorithm which looks for the best decision rules to apply in\norder to decide which digit is shown. The algorithm is based on\na set of learning vectors to identify these rules.\nV.\n\nEXPERIMENTS AND RESULTS\n\nOur system has been evaluated using two series of tests. In\nthe first one, we have used a dataset containing 920 images;\neach one contains only one person to evaluate our detection and\nlocalization system. In the second one, a dataset containing\n1980 hand images has been used to evaluate our digit\nrecognition system. These images have different sizes, lighting\nconditions and complex backgrounds. A study of performance\nof each part of our system (skin color detection, hand detection\nand digit recognition) is detailed in this section.\nA. Skin color detection\nWe calculate the detection ratio in two levels:\n\u2022\n\nAfter classic classification using arrangements in\nCb and Cr plane.\n\n\u2022\n\nAfter fuzzy classification.\n\nThe Comparison based method has been detected 1262\nhands from 1396 detected in the previous classification step.\nBut the ellipse method has been detected 1308 hands.\nAccording to these observations, the ellipse method is the\nbetter for our application. So we will use its results in the next\nstep to recognize hand gestures.\nC. Digit recognition\nThis database has been divided randomly into a set of\ntraining images (70%) and a set of test images (the remaining\n30%). In the literature, there are several techniques of\nsupervised learning. We have evaluated three techniques of\ngraphs of decision tree [14] to know: ID3 [12], C4.5 [13] and\nIMPROVED C4.5 [15].\nWe present two series of experiments: in the first, we\nexperimented with the three techniques mentioned above on\nthe training data set and validate the quality of the learned\nmodels using random error rate techniques. Three measures\nhave been used: precision rate, recall rate for each digit and\nclassification accuracy rate.\nThe performance of obtained classifiers can be assessed by\na confusion matrix opposing assigned class (column) of the\nsamples by the classifier with their true original class (row).\n(Table 3) illustrate, the confusion matrix used for the validation\nstage. Three global indicators on the quality of a classifier from\nsuch a confusion matrix can be build:\nTABLE III.\n\n\u239b Class\n\u239c\n\u239c 1\n\u239c 2\n\u239c\n\u239c M\n\u239c 9\n\u239d\n\nResults are shown in (table 1).\nTABLE I.\n\nTHE CLASSIFICATION STEP RESULTS\n\nDetection of skin region\n\nSkin color detection ratio\n\nClassic classification\n\n91.67%\n\nFuzzy Classification\n\n96.4%\n\u2022\n\nThe standard classification has detected 1327 hands from\n1448 total hands, while the fuzzy classification has detected\n1396 hands. Consequently we opt for fuzzy classification\nbecause it is more efficient in skin regions detection rate.\nB. Hand detection\n(Table 2) shows the performances of the two methods used\nfor hand detection step such as, the Comparison based method\nand the ellipse method.\n\nCONFUSION MATRIX\n\n1\n\n2\n\nN1,1\n\nN1,2 L\n\nN 2,1\n\nN 2,2 L\n\nM\nN 9,1\n\nM\n\nO\n\nN 9,2 L\n\nGlobal error rate: is the complement of classification\naccuracy rate or success classification rate.\n\n\u03b5 global =\n\n\u2211\n\n1\u2264 i , j \u2264 9;i \u2260 j\n\nNi , j\ncard ( M )\n\nWhere card(M) is the number of samples in a test bed.\n\u2022\n\nA priori error rate: this indicator measures the\nprobability that a simple of class (Digiti) where\nI \u2208 {1...9} is classified by the system to other class.\n9\n\nTABLE II.\n\nTHE GESTURE HAND RECOGNITION RESULTS\n\nDetection of hands\n\nHand detection ratio\n\nComparison based method\n\n90,4%\n\nEllipse method\n\n93,7%\n\n9 \u239e\n\u239f\nN1,9 \u239f\nN 2,9 \u239f\n\u239f\nM \u239f\nN 9,9 \u239f\u23a0\n\nL\n\n\u03b5 apriori ( Digiti ) =\n\n\u2211\n\nj =1, j \u2260 i\n9\n\nNi, j\n\n\u2211N\nj =1\n\ni, j\n\n\f(IJCSIS) International Journal of Computer Science and Information Security,\nVol. 2, No. 1, 2009\n\nThis indicator is thus clearly the complement of the\nclassical recall rate which is defined for (Digiti) where\ni \u2208 {1...9} class by:\n\nRe call ( Digiti ) =\n\nN i ,i\n9\n\n\u2211N\nj =1\n\n\u2022\n\ni, j\n\nA posteriori error rate: this indicator measures the\nprobability that a simple assigned to class (Digiti)\nwhere i \u2208 {1...9} by the system effectively belongs to\nclass (Digiti) where i \u2208 {1...9}.\n9\n\n\u03b5 aposteriori ( Digiti ) =\n\n\u2211\n\nj =1, j \u2260 i\n9\n\nN j ,i\n\n\u2211N\nj =1\n\nFigure 14. Recall rate (ID3 classifier)\nj ,i\n\nThis indicator is thus clearly the complement of the\nclassical precision rate which is defined for (Digiti) where\ni \u2208 {1...9} class by:\n\nPr ecision( Digiti ) =\n\nN i ,i\n9\n\n\u2211N\nj =1\n\nj ,i\n\nA good decision tree obtained by a data mining algorithm\nfrom the learning dataset should not only produce classification\nperformance on data already seen but also on unseen data as\nwell. In our experiments, in order to ensure the performance\nstability of our learned models from the learning data, we thus\nalso tested the learned models on our test data.\n\nFigure 15. Precision rate (ID3 classifier)\n\nWe emphasize the remaining of our experiments on the\nrecall and precision rates obtained for each digit. Over our\nexperiments, we show that our system provides a good\nclassification rate on some digits, in particular for digits 1 and\n2. In fact, according to these digits, the number of peaks is still\nsufficient for the decision rule, indeed, the classifier ID3 (as\nwell as the others) puts the emphasis on these criteria.\nThe confusion problem, which causes less performance, is\nshown in the other digits; we observed that the recall rate of\ndigit 4 is relatively low because of many false alarm cases of\ndigit 5.\nThis confusion problem is basically caused by the thumb\nposition which is sometimes plotted with the index finger, so\nthat, only 4 peaks are detected.\nFigure 13. Classification rate\n\n(Figure 13) shows that our approach provides very\nsatisfying results over 90% in our test dataset on the\nclassification accuracy rate, and that ID3 and C4.5 provide the\nbest classification rates. As for us, we retain ID3.\n\nThe second confusion problem is observed for digits 3,6,7,8\nand 9, where exactly 3 peaks are detected (not that the thumb is\nnot considered in digits 6,7,8 and 9, and in digit 3, the thumb is\ncorrectly plotted in general).\nSeveral precision and recall rates are shown because of the\nsame problem of false alarm. The main cause of this problem is\nthe distance between the fingers used to sign each digit which\nare different from a person to another.\n\n\f(IJCSIS) International Journal of Computer Science and Information Security,\nVol. 2, No. 1, 2009\n\nIn our future work, we think of include some other\naposteriori criteria's to differentiate between digits 4 and 5\nwhen 4 peaks are detected, and an additional study of the\nfingers distances in the case of digits 3,6,7,8 and 9.\nVI.\n\nCONCLUSION\n\nWe have presented in this paper an approach of digit\nrecognition based on fingers detection and analysis. The\nanalysis requires many adjustment and removal steps to avoid\ninefficient skin pixels influence. On the other hand, the human\nhand properties are straight forward for the sign language. In\nfact, thanks to these properties many measures have been\ncorrectly estimated and used in several steps of our approach.\nThrow on experiments, we show that the approach is\neffective and the performance is over 90% in our test data set\nas classification accuracy rate. Our future orientation concerns\nthe use of other hand properties, such as finger length. We also\nthink about identifying alphabetic signs.\nREFERENCES\n[1]\n\n[2]\n\n[3]\n\n[4]\n[5]\n\n[6]\n\n[7]\n\n[8]\n[9]\n\n[10]\n\n[11]\n\n[12]\n[13]\n[14]\n[15]\n\nT. Baudel and M. Beaudouin-Lafon, ``Charade : Remote Control of\nobjects using Free-Hand Gestures'' , Communications of the ACM, Vol\n36,no 7, 1993, p . 28-35.\nY. Bellik, \"Modality Integration : Speech and Gesture\", Survey of the\nState of the Art in Human Language Technology, Section 9.4, R . A .\nCole ed in chief, 1996.\nA. Braffort, ``Reconnaissance et Compr\u00e9hension de gestes, application \u00e0\nla langue des signes'', th\u00e8se de l'universit\u00e9 de Paris XI, sp\u00e9cialit\u00e9\ninformatique, juin 1996.\nY. Iwai, Y. Yagi and M . Yachida, \"Gesture Recognition using Colored\nGloves\", Proc. of ICPR'96, p . 662-666.\nF. Berard, J . Coutaz and J. L . Crowley, ``Finger Tracking as Input\nDevice for Augmented Reality'', Proc . Intel Workshop on Automatic\nFace and Gesture-Recognition, Zurich, Switzerland, June 1995.\nT . F. Cootes, C. J . Taylor, D .H . Cooper and J . Graham, ``Active\nShape Models : their Training and Application'', Computer Vision and\nImage Understanding, vol . 61, N 1, p . 38-59, 1995.\nJ . Martin and J . L . Crowley, ``An Appearance-Based Approach to\nGesture-Recognition'', Proc . of 9 th Conf on Image Analysis and\nProcessing, Italy, 1997.\nC . Wagner, ``The pianist's hand anthropometry and biomechanics'',\nErgonomics, Vol. 31, N 1, 1988, p 97-131.\nH.Wang and S.F Chauf ``An highly efficient system for automatic face\nregion detection in MPEG video'', IEEE Trans. On Circuit Systems for\nvideo technology.Vol 7, N4, pp615-628, Augest 1997.\nD. Chai and A. Bouzerdoum, ``A Bayesian Approach to Skin Color\nClassification in YCbCr Color Space'', IEEE Region Ten Conference\n(TENCON'2000), vol. II, pp.421-424, September, 2000.\nLaura Dipietro, Angelo M. Sabatini and Paolo Dario ``Evaluation of an\nInstrumented Glove for Hand-Movement Acquisition'', Journal of\nRehabilitation Research and Development Vol. 40, N 2, March/April\n2003 p 179-190.\nQuinlan, J. R. ``Induction of Decision Trees. Machine Learning (1986)\n81--106\nQuinlan, J. R. ``C4.5: Programs for Machine Learning''.Morgan\nKaufmann (1993)\nZighed, D.A. and Rakotomalala, R. ``Graphes d'Induction Apprentissage et Data Mining``. Hermes (2000)\nRakotomalala, R. and Lallich, S. ''Handling noise with generalized\nentropy of type beta in induction graphs algorithm`` Int Conf on\nComputer Science and Informatics (1998) 25--27\n\n[16] Quentin Delamarre, Rapport technique n\u00b00198 \"Mod\u00e9lisation de la main\npour sa localisation dans une s\u00e9quence d'images\", d\u00e9cembre 1996.\n[17] Christos Davatzikos, Jerry L.Prince, \"Convexity analysis of active\ncontour problems\", 1996.\n[18] Romer Rosales, Vassilis Athitsos, Stan Sclaroff, Technical Report, N\u00b0\n2000-22, \"3D hand pose reconstruction using specialized mappings\"\n(Specialized Mappings Architectures - SMA) D\u00e9cembre 2000.\n[19] R.Gherbi, A.Braffort, Syst\u00e8me PoG, groupe CHM du LIMSI \u00e0 Evry\n1997.\n[20] Hocine Ouhaddi, P.Horain, \"Vers la mod\u00e9lisation du geste par la vision\",\n1999.\n[21] A. Albiol, L. Torres and E. Delp, An unsupervised color image\nsegmentation algorithm for face detection applications, IEEE Conf. on\nImage Processing , October 2001.\n[22] R.Chellappa, C.L Wilson and S.Sorihey, Human and machine\nrecognition of faces: a survey\", proc IEEE, Vol.83, pp 705-740, May\n1995.\n[23] R. L. Hsu, M. A. Mottaleb and A. K. Jain, Face detection in color\nimages, IEEE Trans on Pattern Analysis and Machine Intelligence,\nVol.24, No.5, pp. 696-706, May 2002.\n[24] M. Ben Hmida and Y. Ben Jemaa, Fuzzy classification, image\nsegmentation and shape analysis for Human face detection, IEEE Conf.\non Electrinics, Circuits and Systems (ICECS), pp 640-643, December\n2006.\n[25] J.Canny, A computational approach to edge detection, IEEE Trans. on\nPattern Analysis and Machine Intelligence, Vol.8, No6, pp 679-698,\n1986.\n[26] S. S. Ghidary, Y. Nakata, T. Takamori, and M. Hattori, \"Human\ndetection and localization at indoor environment by home robot\" 2000\nIEEE International Conference on Systems, Man, and Cybernetics, vol.\n2, pp. 1360-1365, 2000.\n[27] N. Guil and E.L.Zapata, Lower order circle and ellipse Hough transform,\nJ.Pattern Recognition, vol. 30, no. 10, pp. 1729-1744, 1997.\n[28] K. Sobottka and I. Pitas. \"Face Localization and Facial Feature\nExtraction Based on Shape and Color Information\". 1996 IEEE\nInternational Conference on Image Processing (ICIP'96), Lausanne,\nSwitzerland, vol.III, pp. 483-486, 16-19 September 1996.\n\n\f"}