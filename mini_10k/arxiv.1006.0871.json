{"id": "http://arxiv.org/abs/1006.0871v1", "guidislink": true, "updated": "2010-06-04T11:34:32Z", "updated_parsed": [2010, 6, 4, 11, 34, 32, 4, 155, 0], "published": "2010-06-04T11:34:32Z", "published_parsed": [2010, 6, 4, 11, 34, 32, 4, 155, 0], "title": "Capacity for Half-Duplex Line Networks with Two Sources", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1006.1772%2C1006.4750%2C1006.3832%2C1006.4068%2C1006.5212%2C1006.0689%2C1006.0871%2C1006.2761%2C1006.2448%2C1006.5847%2C1006.3845%2C1006.5537%2C1006.4181%2C1006.1483%2C1006.1544%2C1006.2159%2C1006.0678%2C1006.0828%2C1006.2045%2C1006.0894%2C1006.4562%2C1006.0101%2C1006.1998%2C1006.4023%2C1006.0062%2C1006.2840%2C1006.3666%2C1006.0854%2C1006.5823%2C1006.0501%2C1006.0954%2C1006.5415%2C1006.4699%2C1006.5677%2C1006.5907%2C1006.2192%2C1006.5441%2C1006.4166%2C1006.5928%2C1006.1391%2C1006.2170%2C1006.1491%2C1006.3179%2C1006.5574%2C1006.2476%2C1006.4323%2C1006.2209%2C1006.3302%2C1006.3111%2C1006.4155%2C1006.3391%2C1006.0608%2C1006.2701%2C1006.3350%2C1006.5640%2C1006.2815%2C1006.4988%2C1006.5959%2C1006.3517%2C1006.3075%2C1006.3753%2C1006.4886%2C1006.2509%2C1006.0783%2C1006.0469%2C1006.2342%2C1006.4082%2C1006.5435%2C1006.3066%2C1006.2478%2C1006.0709%2C1006.5491%2C1006.4837%2C1006.5406%2C1006.2837%2C1006.2811%2C1006.0220%2C1006.2055%2C1006.4748%2C1006.4413%2C1006.4385%2C1006.2156%2C1006.5164%2C1006.0708%2C1006.4157%2C1006.3941%2C1006.1000%2C1006.4915%2C1006.1322%2C1006.0577%2C1006.0975%2C1006.0696%2C1006.0371%2C1006.4599%2C1006.0013%2C1006.3889%2C1006.0324%2C1006.4045%2C1006.3515%2C1006.2318%2C1006.4465&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Capacity for Half-Duplex Line Networks with Two Sources"}, "summary": "The focus is on noise-free half-duplex line networks with two sources where\nthe first node and either the second node or the second-last node in the\ncascade act as sources. In both cases, we establish the capacity region of\nrates at which both sources can transmit independent information to a common\nsink. The achievability scheme presented for the first case is constructive\nwhile the achievability scheme for the second case is based on a random coding\nargument.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1006.1772%2C1006.4750%2C1006.3832%2C1006.4068%2C1006.5212%2C1006.0689%2C1006.0871%2C1006.2761%2C1006.2448%2C1006.5847%2C1006.3845%2C1006.5537%2C1006.4181%2C1006.1483%2C1006.1544%2C1006.2159%2C1006.0678%2C1006.0828%2C1006.2045%2C1006.0894%2C1006.4562%2C1006.0101%2C1006.1998%2C1006.4023%2C1006.0062%2C1006.2840%2C1006.3666%2C1006.0854%2C1006.5823%2C1006.0501%2C1006.0954%2C1006.5415%2C1006.4699%2C1006.5677%2C1006.5907%2C1006.2192%2C1006.5441%2C1006.4166%2C1006.5928%2C1006.1391%2C1006.2170%2C1006.1491%2C1006.3179%2C1006.5574%2C1006.2476%2C1006.4323%2C1006.2209%2C1006.3302%2C1006.3111%2C1006.4155%2C1006.3391%2C1006.0608%2C1006.2701%2C1006.3350%2C1006.5640%2C1006.2815%2C1006.4988%2C1006.5959%2C1006.3517%2C1006.3075%2C1006.3753%2C1006.4886%2C1006.2509%2C1006.0783%2C1006.0469%2C1006.2342%2C1006.4082%2C1006.5435%2C1006.3066%2C1006.2478%2C1006.0709%2C1006.5491%2C1006.4837%2C1006.5406%2C1006.2837%2C1006.2811%2C1006.0220%2C1006.2055%2C1006.4748%2C1006.4413%2C1006.4385%2C1006.2156%2C1006.5164%2C1006.0708%2C1006.4157%2C1006.3941%2C1006.1000%2C1006.4915%2C1006.1322%2C1006.0577%2C1006.0975%2C1006.0696%2C1006.0371%2C1006.4599%2C1006.0013%2C1006.3889%2C1006.0324%2C1006.4045%2C1006.3515%2C1006.2318%2C1006.4465&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The focus is on noise-free half-duplex line networks with two sources where\nthe first node and either the second node or the second-last node in the\ncascade act as sources. In both cases, we establish the capacity region of\nrates at which both sources can transmit independent information to a common\nsink. The achievability scheme presented for the first case is constructive\nwhile the achievability scheme for the second case is based on a random coding\nargument."}, "authors": ["Tobias Lutz", "Gerhard Kramer", "Christoph Hausl"], "author_detail": {"name": "Christoph Hausl"}, "author": "Christoph Hausl", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/ISIT.2010.5513750", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1006.0871v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1006.0871v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Proceedings of the IEEE International Symposium on Information\n  Theory, Austin, TX, USA, June 12 - 18, 2010", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1006.0871v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1006.0871v1", "journal_reference": null, "doi": "10.1109/ISIT.2010.5513750", "fulltext": "arXiv:1006.0871v1 [cs.IT] 4 Jun 2010\n\nCapacity for Half-Duplex Line Networks with Two\nSources\nTobias Lutz\n\nGerhard Kramer\n\nChristoph Hausl\n\nInst. for Communications Engineering\nTechnische Universit\u00e4t M\u00fcnchen\nMunich, 80290 Germany\ntobi.lutz@tum.de\n\nDepartment of Electrical Engineering\nUniversity of Southern California\nLos Angeles, CA 90089 USA\ngkramer@usc.edu\n\nInst. for Communications Engineering\nTechnische Universit\u00e4t M\u00fcnchen\nMunich, 80290 Germany\nchristoph.hausl@tum.de\n\nAbstract-The focus is on noise-free half-duplex line networks\nwith two sources where the first node and either the second\nnode or the second-last node in the cascade act as sources. In\nboth cases, we establish the capacity region of rates at which\nboth sources can transmit independent information to a common\nsink. The achievability scheme presented for the first case is\nconstructive while the achievability scheme for the second case\nis based on a random coding argument.\n\nI. I NTRODUCTION\nMost wireless networks are half-duplex constrained, i.e. the\nnetwork nodes cannot transmit and receive simultaneously.\nIn order to handle the half-duplex constraint, transmission\nprotocols deterministically split the time of each network\nnode into transmission and reception periods. This approach\nis easy to realize since nodes do not have to change rapidly\nbetween their transmission and reception modes. However, the\napproach is suboptimal from an information theoretic point\nof view. It does not take into account that the throughput\nof each half-duplex node can be increased by allowing it to\nchoose the transmission-reception patterns in dependence of\nthe information to be sent.\nThis observation goes back to [1] that introduced a binary,\ndeterministic channel model for half-duplex constrained relays\nand demonstrated, using the example of a three node line\nnetwork, that larger rates as compared to time-sharing are\npossible by modulating the operation modes of the relay\nbased on the underlying information [2]. In [3], the capacity\nof the degraded half-duplex relay channel was derived. The\nauthors also noted that the schedule of the relay has to carry\ninformation in order to achieve the capacity.\nAn extension of this result to line networks with multiple\nsources was presented in [4] and [5]. In what follows, we\nrefer to the intermediate nodes, i.e., the second to secondlast nodes in the cascade, as relays since they must relay the\nfirst node's message to the last node that is the destination\nfor all messages. Within the setup of [5] a source and a\nsubset of the relays deliver independent information to the\ndestination under the assumption that adjacent node pairs are\nconnected by noise-free (q + 1)-ary pipes. A coding scheme\nbased on timing was proposed, and based on the asymptotic\nbehavior of the coding scheme, the capacity of deterministic\nrelay cascades of arbitrary length and a single source was\n\nestablished. If the cascade includes a certain number of relays\nwith their own information, the coding scheme achieves the\ncut-set bound provided that the rates of the relay sources fall\nbelow individual thresholds.\nIn the present paper, we treat deterministic half-duplex line\nnetworks with two sources where either the first or the last\nrelay in the cascade is the second source. In both cases, we\nestablish the capacity region of rates at which both sources\ncan transmit independent information to a common sink.\nIf the first relay acts as a source, it is shown that the capacity\nregion is the cut-set region. This improves a result derived in\n[5] which says that the cut-set bound is achievable if the rate\nof the relay source falls below a certain threshold. In order\nto understand the new step in the achievability scheme, we\nbriefly describe the scheme in [5]. Therein, the source node\nencodes its information by means of transmission symbols and\nidle symbols. An idle symbol indicates a channel use without\ntransmission. The relays encode received information with the\ntransmission pattern and with the value of the transmission\nsymbols. Based on this, codes can be constructed which allow\nthe nodes to cooperate in a sense that each node controls the\ntransmission pattern applied by the next node. Hence, new\ninformation injected by the relay source is not allowed to be\nrepresented by the transmission pattern since, otherwise, the\nprevious node is not able to control the applied transmission\npattern. This is, in fact, the reason why the rate of the relay\nsources cannot exceed a certain threshold. In the new scheme,\nthe relays still use the original idea. However, the source\nregards its link to the relay source as an erasure channel where\nthe erasures are a consequence of the half-duplex constraint.\nHence, the source and the relay source do not cooperate\nanymore which enables the relay source to represent own\ninformation with transmission patterns. It turns out that this\nnew step is necessary to achieve all points in the cut-set region.\nIn the second part of this paper, we focus on the case where\nthe last relay in the cascade acts as a source. The capacity\nregion is derived by means of a random coding argument.\nII. N ETWORK M ODEL\nConsider a discrete memoryless relay cascade as depicted\nin Fig. 1. Each node is labeled by a distinct number from\nV = {0, . . . , m} with m > 0. The integers 0 and m refer to\n\n\fk\nk\u22121\nW1\n\nW0\nChannel\n\nY1\n\nX0\n\nYk\n\nX1\n\nXk\n\nMoreover, Yki\u22121 is used as short hand notation for the set\n{Yk1 , . . . , Yk,i\u22121 }.\n\nYm\n\nW1\nX0\nWk\u22121\n\n2\n1\n\nY1\n\nDec.\n\nW0\n\nMux\n\nIII. T HE F IRST R ELAY\nEnc.\n\nX1\n\nRelay Source\n\nWm\n\nFig. 1. A noiseless relay cascade with two sources. The link model is\nillustrated by means of feedback. If relay 1 is transmitting, the switch is in\nposition 1 otherwise in position 2.\n\nthe source and sink, respectively, while all remaining integers\n1 to m \u2212 1 represent half-duplex constrained relays, i. e.\nrelays which cannot transmit and receive at the same time.\nThe connectivity within the network is described by the set of\nedges E = {(k, k + 1) : 0 \u2264 k \u2264 m \u2212 1}, i.e. the ordered pair\n(k, k + 1) represents the communications link from node k to\nnode k + 1. The output of the kth node, which is the input to\nchannel (k, k + 1) is denoted as Xk and takes values on the\nalphabet Xk = Qk \u222a {N} where Qk denotes the transmission\nalphabet of node k while the idle symbol \"N\" signifies a\nchannel use in which node k is not transmitting. The input\nof the kth node, which is the output of channel (k \u2212 1, k) is\ndenoted as Yk and is given by\n\u001a\nXk\u22121 , if Xk = N\nYk =\n(1)\nXk ,\nif Xk \u2208 Qk\nYm = Xm\u22121 .\n(2)\nwhere 1 \u2264 k \u2264 m \u2212 1. Channel model (1) captures the\nhalf-duplex constraint as follows. Assume relay k is in its\ntransmission mode, i.e. Xk \u2208 Qk . Then relay k hears itself\n(Yk = Xk ) but cannot listen to relay k \u2212 1 or, equivalently,\nrelay k and relay k\u22121 are disconnected. However, if relay k is\nnot transmitting, i. e. Xk = N, it is able to listen to relay k \u2212 1\nvia a noise-free |Xk\u22121 |-ary pipe (Yk = Xk\u22121 ). Another\ninterpretation of the channel model is that the output Xk of\neach relay k controls the position of a switch which is placed\nat its input. If relay k is transmitting, the switch is in position 1\notherwise it is in position 2 (see Fig. 1). Since a pair of nodes is\neither perfectly connected or disconnected, we obtain a deterministic network with p(y1 , . . . , ym |x0 , . . . , xm\u22121 ) \u2208 {0, 1}.\nAt the beginning of a new block b of n channel uses, source\nnode 0 and relay k \u2208 {1, m\u22121} produce\na uniformly and inde\b\npendently\ndrawn\nmessage\nW\n\u2208\n1,\n.\n.\n. , 2nR0 and Wk,b \u2208\n0,b\n\b\nnRk\n1, . . . , 2\n, respectively. Based on the received sequence\nin block b, sink node m forms the estimates \u01750,b\u2212(m\u22121) and\n\u0175k,b\u2212(m\u22121\u2212k) of W0,b\u2212(m\u22121) and Wk,b\u2212(m\u22121\u2212k) . We assume\nthe following encoding functions\nx0i\nxki\n\n= f0i (W0 )\n= fki (Wk , Yki\u22121 )\n\nxli\n\n= fli (Yli\u22121 ),\n\n(3)\n(4)\n\u2200l 6= {0, k, m}.\n\n(5)\n\nThe first subscript describes the node number while the second\nsubscript i corresponds to the time instance where 1 \u2264 i \u2264 n.\n\nIS A\n\nS OURCE\n\nTheorem 1: The capacity region C of the line network of\nFig. 1, where node 0 and relay node 1 are sources, is\n\uf8fc\n\uf8f1\n\uf8fd\n[ \uf8f2 R0 \u2264 H(Y1 |X1 )\nR0 + R1 \u2264 H(Ym )\nC=\n.\n(6)\n\uf8fe\n\uf8f3\nR0 + R1 \u2264 min2\u2264i\u2264m\u22121 H(Yi |Xi )\n\nThe union is over all probability distributions of the form\nPX0 (*)PX1 (*)PX2 |X1 (*)PX3 |X2 (*) . . . PXm\u22121 |Xm\u22122 (*).\n\n(7)\n\nProof: We start with the achievability of C. At the end of\nblock b\u22121, node 0 and relay node 1 choose new messages w0,b\nand w1,b , respectively, which are sent in block b by means of\nthe sequences x0 (w0,b ) and x1 (w0,b\u22121 , w1,b ). The remaining\nrelays i, 2 \u2264 i \u2264 m\u2212 1, forward older messages. In particular,\nrelay i sends xi (w0,b\u2212i , w1,b\u2212(i\u22121) ) in block b.\nCoding:\n\u2022 At node m \u2212 1 [5]: Node m \u2212 1 represents information by taking nm\u22121 < n transmission symbols per block of length n from the alphabet Qm\u22121\nand by allocating the nm\u22121 symbols \u0001to the transn\ndifferent semission block. Thus, |Qm\u22121 |nm\u22121 nm\u22121\n\u0001\nquences xm\u22121 w0,b\u2212(m\u22121) , w1,b\u2212(m\u22122) are available\nat relay m \u2212 1. Observe that |Qm\u22121 |nm\u22121 equals the\nnumber of possible distinct sequences when the |Q\n\u0001 m\u22121 |n\nequals\nary symbols are located at fixed slots while nm\u22121\nthe number of possible transmission-listen patterns.\n\u2022 At node i, 1 \u2264 i \u2264 m\u22122 [5]: For each transmission-listen\npattern used by node i + 1, node i generates a codebook.\nFor a particular pattern, node i allocates ni transmission\nsymbols from the alphabet Qi in all possible ways to\nthe n \u2212 ni+1 listen slots of the pattern. The slots of the\npattern, in which node i + 1 transmits, are filled with idle\nsymbols \"N\". This procedure generates a certain number\nof transmission-listen patterns used by node i.\n\u2022 Due to the above codebook construction, adjacent nodes\ncan cooperate since each node i \u2265 1 knows the messages\nto be forwarded by the next node and, thus, is aware of\nthe applied codeword. The construction guarantees that\nadjacent nodes i and i + 1, i \u2265 1, do not transmit at the\nsame time.\n\u2022 At node 0: In contrast to [5], node 0 does not adapt to\nthe transmission-listen patterns used by node 1. Instead it\nuses an optimal point to point erasure channel code with\nalphabet X0 for encoding W0,b . Output symbols Y1 of\nlink (0, 1) are erased with a probability of 1 \u2212 pX1 (N),\ni.e. the erasure probability is equal to the fraction of time\nin which node 1 transmits. It should be noted that node\n0 transmits a part of the information in the timing of the\ntransmission symbols since the erasure code makes use\nof symbol \"N\".\n\n\fAchievable Rates: The capacity of a |X0 |-ary erasure channel with erasure probability 1\u2212pX1 (N) equals pX1 (N) log |X0 |\nachieved by a uniform input distribution over X0 . Due to the\nchannel model, we clearly have\nH(X0 |X1 = N )\npX1 (N) log |X0 |\n\nPSfrag replacements\n(8)\n(a)\n\nwith equality if pX0 |X1 (*|N ) is the uniform distribution over\n|X0 |. Thus, an optimal erasure channel code for the link (0, 1)\nsatisfies R0 = H(Y1 |X1 ) \u2212 \u01eb with \u01eb \u2192 0 as n \u2192 \u221e.\nFurther, we know from the results derived in [5, Sec. III.A]\nthat\n\u0013\n\u0012\nn \u2212 ni+1\n\u2192 2nH(Yi+1 |Xi+1 )\nas n \u2192 \u221e. (9)\n|Qi |ni\nni\nfor 0 \u2264 i \u2264 m \u2212 2. For i = m \u2212 1, the exponent in\n(9) becomes H(Ym ) (with nm = 0). Hence, R0 + R1 \u2264\nmin2\u2264i\u2264m\u22121 H(Yi |Xi ) and R0 + R1 \u2264 H(Ym ).\nThe converse is immediate since the bounds of (6) correspond to the cut-set upper bound [5, Sec. IV]. (7) follows from\nthe following consideration. Again, due to the channel model\nH(Yi |Xi ) = pXi (N )H(Xi\u22121 |Xi = N ),\n\n1.4\n\n(10)\n\nso that H(Yi |Xi ) is a function of pXi\u22121 Xi (*) for all 2 \u2264 i \u2264\nm \u2212 1. Hence, without restriction we may assume the Markov\nchain X1 \u2212 * * * \u2212 Xm\u22121 . Further, we can choose a uniform\npX0 |X1 (*|N ) over |X0 | since this achieves the upper bound (8).\nClearly, such a distribution also exists when X0 is independent\nof X1 , . . . , Xm\u22121 .\nRemark 1: Theorem 1 shows that the capacity region of the\nconsidered line network is equal to the cut-set region. This\nimproves a result in [5] which says that the cut-set bound\nis achieved when the rate of the relay source falls below a\ncertain threshold. The new ingredient here is that the relay\nsource is allowed to encode its own information in the timing\nof transmission symbols. In fact, node 0 accepts that a part\nof its information is erased by node 1. We point out that this\napproach, namely to treat the link to the relay source k as an\nerasure channel, is not cut-set bound achieving if k \u2265 2.\nExample 1: We apply Theorem 1 to a line network composed of three nodes where the first two nodes have their\nown information. The alphabets are X0 = X1 = {0, 1, N}.\nThis example has already appeared in [4], [5]. However, we\nare now able to characterize the complete capacity region.\nMoreover, the approach here is easier since we can restrict\nattention to independent X0 and X1 . By choosing PX0 (*) to\nbe the uniform distribution and, further, by assigning the same\nprobability masses to X1 = 0 and X1 = 1 (due to symmetry),\nwe obtain the following expression for the capacity region\n\u001b\n[ \u001a R0 \u2264 pX (N) log 3\n1\nC=\n.\nR0 + R1 \u2264 (1 \u2212 pX1 (N)) log 2 + h(pX1 (N))\n(11)\nThe union is over pX1 (N) and h(*) denotes the binary entropy\nfunction. C is depicted in Fig. 2. Note that the region bounded\nby the dashed line contains the rates which are achievable\n\n(b)\n(c)\n\nR1 (bits per use)\n\nH(Y1 |X1 ) =\n\u2264\n\n1.6\n\n1.2\n1\n0.8\n0.6\n0.4\n0.2\n0\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nR0 (bits per use)\nFig. 2. Capacity region (11) is given by the solid curve. The time-sharing\nregion is bounded by the dashed line.\n\nwhen the time of the relay is deterministically split into\ntransmission and reception periods. In order to obtain this\nregion, time-sharing between (R0 , R1 ) = (0.5 log2 3, 0) and\n(0, log2 3) bits per use has to be performed.\nIV. T HE L AST R ELAY\n\nIS A\n\nS OURCE\n\nIn the remainder, we will make use of the following notation\nw0,b\u2212[i+j;i+k]\nX[l;t]\n\ndef\n\n=\n\ndef\n\n=\n\n{w0,b\u2212(i+j) , . . . , w0,b\u2212(i+k) }\n{Xl , . . . , Xt }.\n(n)\n\nLemma 1: [6, Th. 14.2.3] Let A\u01eb denote the typical set\nfor the probability mass function p(x1 , . . . , xn ) and let\nP (X 1 = x1 , . . . , X n = xn ) =\n\nn\nY\n\np(x1l |x3l , . . . , xnl )\n\nl=1\n\np(x2l |x3l , . . . , xnl )p(x3l , . . . , xnl ).\nThen\n. \u2212n(I(X1 ;X2 |X3 ,...,Xn )\u00b16\u01eb)\n.\nP {(X 1 , . . . , X n ) \u2208 A(n)\n\u01eb } =2\nTheorem 2: The capacity region C of the line network of\nFig. 1, where node 0 and relay node m \u2212 1 are sources, is\n\uf8f1\n\uf8fc\n[ \uf8f2 R0 \u2264 min1\u2264i\u2264m\u22121 H (Yi |Xi ) \uf8fd\nRm\u22121 \u2264 H(Ym |U )\nC=\n.\n(12)\n\uf8f3\n\uf8fe\nR0 + Rm\u22121 \u2264 H(Ym )\n\nThe union is over all probability distributions of the form\n\nPX0 PX1 PX2 |X1 PX3 |X2 . . . PXm\u22122 |Xm\u22123 PU|Xm\u22122 PXm\u22121 |U .\n(13)\nRemark 2: C is equal to the cut-set region if there exists\na probability distribution for each boundary point such that\nXm\u22121 is independent of U . Otherwise, C is smaller than the\ncut-set region.\n\n\fV. P ROOF O UTLINE OF T HEOREM 2\nA. Achievability\nRandom codebook generation:\n\u2022 Split W0 into B sub-blocks W0,b , b = 1, 2, . . . , B, that\neach take on 2nR0 values. Similarly, split Wm\u22121 into B\nsub-blocks Wm\u22121,b , b = 1, 2, . . . , B, that each take on\n2nRm\u22121 values.\nnR0\nindependent\n\u2022 Node m \u2212 1 generates at random 2\n\u0001\n,\nw\nsequences of length n, u wQ\n0,b\u2212(m\u22121)\n0,b\u2212(m\u22121) \u2208\nn\n{1, . . . , 2nR0 }, according to l=1 p(ul ).\n\u2022 Codebook at node m \u2212 1: On\neach of the\n\u0001\nsequences u w0,b\u2212(m\u22121) , node m \u2212 1 superposes\nnRm\u22121\ncodewords\na random codebook \u0001with 2Q\nn\nxm\u22121 w0,b\u2212(m\u22121) , wm\u22121,b using l=1 p(xm\u22121,l |ul ).\u0001\n\u2022 Codebook at node m \u2212 2: For each u w0,b\u2212(m\u22121) ,\nindependent\nnode\nm \u2212 2\ngenerates\n2\u0001nR0\nsequences\nx\nw\naccording\nto\n0,b\u2212[m\u22122;m\u22121]\nm\u22122\nQn\np(x\n|u\n).\nm\u22122,l\nl\nl=1\n\u2022 Codebook at node i, 0 \u2264 i < m \u2212 2: For each\n\u0001\nnR0\nxi+1 w0,b\u2212[i+1;m\u22121] node i generates at\n\u0001 random 2\nindependent sequences xi w0,b\u2212[i;m\u22121] according to\nQ\nn\nl=1 p(xi,l |xi+1,l ).\nEncoding: At the beginning of each block b, node i, 0 \u2264\ni \u2264 m \u2212 2, has the estimates1 \u01750,b\u2212i\u2212l of w0,b\u2212i\u2212l , l \u2265\n0. To send the estimate\n\u01750,b\u2212i , node i selects the codeword\n\u0001\nxi \u01750,b\u2212[i;m\u22121] .\nSimilarly, at the beginning\n\b of block b, node\nm \b\n\u2212 1 has the estimates\n\u01750,b\u2212(m\u22121)\u2212l , \u0175m\u22121,b\u2212l\n,\nl\n\u2265\n0. To send the pair\nof\nw\n,\nw\n0,b\u2212l\n0,b\u2212(m\u22121)\u2212l\n\b\n\u01750,b\u2212(m\u22121) , \u0175m\u22121,b , node m \u2212 1 selects the codeword\n\u0001\nxm\u22121 \u01750,b\u2212(m\u22121) , \u0175m\u22121,b .\nEvery node i, 1 \u2264 i \u2264 m, receives the sequence y i (b) in\nblock b.\nDecoding: At the end of block b, sink node m performs the\nfollowing \u01eb-typicality check in order to determine \u01750,b\u2212(m\u22121)\nand \u0175m\u22121,b :\no\nn\n\u0001\n\u0001\nu \u01750,b\u2212(m\u22121) , xm\u22121 \u01750,b\u2212(m\u22121) , \u0175m\u22121,b , y m (b)\n\u2208\n\nA(n)\n\u01eb (U, Xm\u22121 , Ym ).\n\n(14)\n\nBy Lemma 1, it follows that the error probability of (14) is\nsmaller than\n2\u2212n(I(U,Xm\u22121 ;Ym )\u22126\u01eb) .\n(15)\nFurther, if the estimate \u01750,b\u2212(m\u22121) is known at the sink, the\nerror probability regarding the estimate \u0175m\u22121,b is smaller than\n2\u2212n(I(Xm\u22121 ;Ym |U)\u22126\u01eb) .\n\n(16)\n\nSimilarly, at the end of block b, node i, 1 \u2264 i \u2264 m \u2212 1,\nperforms the following \u01eb-typicality check in order to determine\n1 The source knows its own messages. However, for simplicity, we will also\ndenote this message with a hat. The same is done for the relay source m \u2212 1.\n\n\u01750,b\u2212i :\nn\no\n\u0001\n\u0001\nxi\u22121 \u01750,b\u2212[i\u22121;m\u22121] , xi \u01750,b\u2212[i;m\u22121] , y i (b)\n\n\u2208 A(n)\n\u01eb (Xi\u22121 , Xi , Yi ).\n\n(17)\n\nAccording to Lemma 1, the error probability of (17) regarding\nthe estimate \u01750,b\u2212(i\u22121) is smaller than\n2\u2212n(I(Xi\u22121 ;Yi |Xi )\u22126\u01eb) .\n\n(18)\n\nNow, by considering all possible error events we obtain from\n(15), (16) and (18) that\n\uf8f1\n\uf8fc\n[ \uf8f2 R0 \u2264 min1\u2264i\u2264m\u22121 H (Yi |Xi ) \uf8fd\nRm\u22121 \u2264 H(Ym |U )\nR=\n(19)\n\uf8f3\n\uf8fe\nR0 + Rm\u22121 \u2264 H(Ym )\nis an achievable region. Observe that the exponents of the\nerror probabilities can be simplified since Yi is a function of\nXi , Xi\u22121 .\n\nB. Converse\nConsider the following bounds, where Pb,0 and Pb,m\u22121 are\nthe average bit error probabilities when decoding W0 and\nWm\u22121 at the destination node m. For 1 \u2264 l \u2264 m \u2212 1, we\nhave\nnR0 (1 \u2212 h(Pb,0 ))\n\n(20)\n\n(a)\n\n\u2264 I (W0 ; Ymn )\n\u2264 I (W0 ; Wm\u22121 Yln Ymn )\nn\n\u0001\n(b) X\nI W0 ; Yli Ymi |Wm\u22121 Yli\u22121 Ymi\u22121\n=\n(c)\n\n=\n\n(d)\n\n=\n\n(e)\n\n=\n\ni=1\nn\nX\n\ni=1\nn\nX\n\ni=1\nn\nX\n\ni=1\nn\n(f ) X\n\n\u2264\n\nI W0 ; Yli |Wm\u22121 Yli\u22121 Ymi\u22121\n\n\u0001\n\nI W0 ; Yli |Wm\u22121 Yli\u22121 Ymi\u22121 Xli\nH Yli |Wm\u22121 Yli\u22121 Ymi\u22121 Xli\nH (Yli |Xli )\n\n\u0001\n\n(21)\n(22)\n(23)\n(24)\n\n\u0001\n\n(25)\n(26)\n(27)\n\ni=1\n\n(g)\n\n= nH (Yl |Xl , Q)\n\n(28)\n\n(h)\n\n\u2264 nH (Yl |Xl )\n\n(29)\n\nwhere\n\u2022 (a) follows by Fano's inequality\n\u2022 (b) follows from the chain rule for mutual information\nand from the independence of W0 and Wm\u22121\n\u2022 (c) follows by Markovity\ni\u22121\ni\n\u2022 (d) follows because Xl is a function of Yl\nfor all 1 \u2264\ni\u22121\ni\nl < m \u2212 1 and Xm\u22121 is a function of Ym\u22121 and Wm\u22121\ni\u22121\ni\u22121\n\u2022 (e) follows because W0 determines X0 , . . . , Xm\u22122\ni\u22121\ni\u22121\ni\u22121\nwhat, in turn, determines Y1 , . . . , Ym\u22122 and Ym\u22122\n\u2022 (f) conditioning does not increase entropy\n\n\f(g) follows by defining Q to be a time-sharing random\nvariable with Yl := YlQ , Xl := XlQ .\n\u2022 (h) conditioning does not increase entropy\nFurther, we have the bounds\n\n\u2022\n\nnRm\u22121 (1 \u2212 h(Pb,m\u22121 ))\n\n(30)\n\n(a)\n\n\u2264 I (Wm\u22121 ; Ymn )\n\n(31)\n\n\u2264 I(Wm\u22121 ; W0 Ymn )\nn\n\u0001\n(b) X\nI Wm\u22121 ; Ymi |W0 Ymi\u22121\n=\n(c)\n\n=\n\n(d)\n\n=\n\ni=1\nn\nX\n\ni=1\nn\nX\ni=1\n\nI\n\ni\u22121\nWm\u22121 ; Ymi |W0 Ymi\u22121 Xm\u22121\n\n(32)\n(33)\n\u0001\n\n\u0011\n\u0010\ni\u22121\ni\u22121\nI Wm\u22121 ; Ymi |W0 Y[1;m]\nX[0;m\u22121]\n\nn\n\u0011\n\u0010\n(e) X\ni\u22121\ni\u22121\nH Ymi |W0 Y[1;m]\nX[0;m\u22121]\n=\n\ni=1\nn\n(f ) X\n\n\u2264\n\nH (Ymi |Vi )\n\n= nH (Ym |U )\n\n\u2264\n\nI(W0 ; Ymn )\n\n+\n\nI(Wm\u22121 ; W0 Ymn )\n\n(b)\n\n= I(W0 Wm\u22121 ; Ymn )\nn\nX\nI(W0 Wm\u22121 ; Ymi |Ymi\u22121 )\n=\n\ni=1\nn\n(c) X\n\nX0 , . . . , Xm\u22122 \u2212 U \u2212 Xm\u22121 .\n\n(37)\n\nFinally, by the explanations in the last section of the proof of\nTheorem 1 we have the Markov chain\nX1 \u2212 * * * \u2212 Xm\u22122 \u2212 U \u2212 Xm\u22121\n\n(38)\n\n(51)\n\n(52)\n\nand the independence of X0 from X1 , . . . , Xm\u22121 , U .\nVI. D ISCUSSION\n\n(42)\n\nAn obvious extension is to allow any relay in the cascade to\nact as second source. However, the solution for this case turns\nout to be elusive. Though developing achievable rate regions\nusing superposition random coding is straightforward, proving\na converse seems to be more difficult. An intuitive explanation\nis that having the second source located at the first or the last\nlink offers greater freedom for choosing a coding strategy as\ncompared to the other links. This is related to the fact that the\nfirst source does not receive information while the sink node\ndoes not send information and, therefore, both nodes are not\naffected by the half-duplex constraint. One could also think of\nextending the erasure coding technique outlined in the proof\nof Theorem 1. In particular, if all nodes before the relay source\nuse independent erasure codes, the relay source would be able\nto send own information in the timing of transmission symbols.\nHowever, it can be shown that this approach does not achieve\nthe cut-set bound and, therefore, a converse is missing again.\n\n(43)\n\nACKNOWLEDGMENT\n\n(39)\n(40)\n(41)\n\n= nH(Ym |Q)\n\n(45)\n\n(e)\n\nwhere\n\u2022 (a) follows by Fano's inequality\n\n(50)\n\n(36)\n\n(44)\n\n\u2264 nH(Ym ).\n\n(49)\n\nwhich shows that\n\ni=1\n\n(d)\n\n(48)\n\ni\u22121\nP (u)P (x0i , . . . , xm\u22122,i |i, xi\u22121\nm\u22121 , ym\u22121 )\ni\u22121\n* P (xm\u22121,i |i, xi\u22121\nm\u22121 , ym\u22121 )\n\n= P (u)P (x0 , . . . , xm\u22122 |u)P (xm\u22121 |u)\n\nH(Ymi )\n\n\u2264\n\n= P (u)P (x0 , . . . , xm\u22121 |u)\n\n(35)\n\n(a)\n\n\u2264 I(W0 ; Ymn ) + I(Wm\u22121 ; Ymn )\n\n(47)\n\n=\n\nwhere\n\u2022 (a) follows by Fano's inequality\n\u2022 (b) follows from the chain rule for mutual information\nand from the independence of W0 and Wm\u22121\ni\u22121\ni\u22121\n\u2022 (c) follows because Ym\n= Xm\u22121\ni\u22121\ni\u22121\n\u2022 (d) follows because W0 determines Y1\n, . . . , Ym\u22121\n\u2022 (e) follows because W0 and Wm\u22121 determine Ym,i\n\u0001\ni\u22121\ni\u22121\n\u2022 (f) follows by defining Vi = Xm\u22121 , Ym\u22121 and from\nthe fact that conditioning does not increase entropy\n\u2022 (g) follows by defining Q to be a time-sharing random\nvariable with U := (VQ , Q) and Ym := YmQ .\nConcerning the sum-rate, we obtain\nnR0 (1 \u2212 h(Pb,0 )) + nRm\u22121 (1 \u2212 h(Pb,m\u22121 ))\n\nP (u, x0 , . . . , xm\u22121 )\n\n(34)\n\ni=1\n\n(g)\n\n(b) follows from the independence of W0 and Wm\u22121\n(c) follows since W0 and Wm\u22121 determine Ym,i and from\nthe fact that conditioning does not increase entropy\n\u2022 (d) follows by defining Q to be a time-sharing random\nvariable and Ym := YmQ\n\u2022 (e) conditioning does not increase entropy.\nIt remains to check (13). Observe that Xm\u22121,i is a funci\u22121\ntion of Ym\u22121\nand W2 . Since X0i , . . . , Xm\u22122,i do not depend on W2 , we have the Markov chain X0i , . . . , Xm\u22122,i \u2212\ni\u22121\ni\u22121\nXm\u22121\nYm\u22121\n\u2212 Xm\u22121,i . Hence, we have\n\u2022\n\n\u2022\n\n(46)\n\nT. Lutz and C. Hausl were supported by the European\nCommission in the framework of the FP7 (contract n. 215252).\nG. Kramer was supported by NSF Grant CCF-09-05235.\nR EFERENCES\n[1] G. Kramer. Models and Theory for Relay Channels with Receive\nConstraints. In Proc. 42nd Annual Allerton Conf. Commun., Control,\nand Computing, (Monticello, IL), Sept. 29 - Oct. 1 2004.\n[2] G. Kramer. Communication Strategies and Coding for Relaying. Wireless\nCommunications, vol. 143 of the IMA Volumes in Mathematics and its\nApplications:163\u2013175, Springer: New York, 2007.\n\n\f[3] S. Vijayakumaran, T. F. Wong, and T. M. Lok. Capacity of the Degraded\nHalf-Duplex Relay Channel. available on arXiv:0708.2270v1 [cs.IT].\n[4] T. Lutz, C. Hausl, and R. K\u00f6tter. Coding Strategies for Noise-Free\nRelay Cascades with Half-Duplex Constraint. In IEEE International\nSymposium on Information Theory (ISIT), pages 2385\u20132389, Toronto,\nOntario, Canada, July 2008.\n[5] T. Lutz, C. Hausl, and R. K\u00f6tter. Bits Through Relay Cascades with\nHalf-Duplex Constraint. submitted to the IEEE Trans. Inform. Theory,\navailable on arXiv:0906.1599v2 [cs.IT].\n[6] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley,\nInc., 1991.\n\n\f"}