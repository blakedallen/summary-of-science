{"id": "http://arxiv.org/abs/0805.4029v1", "guidislink": true, "updated": "2008-05-27T01:31:05Z", "updated_parsed": [2008, 5, 27, 1, 31, 5, 1, 148, 0], "published": "2008-05-27T01:31:05Z", "published_parsed": [2008, 5, 27, 1, 31, 5, 1, 148, 0], "title": "Event Synchronization by Lightweight Message Passing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0805.4634%2C0805.4830%2C0805.3036%2C0805.2751%2C0805.3179%2C0805.0263%2C0805.3923%2C0805.1656%2C0805.2644%2C0805.2839%2C0805.3462%2C0805.0274%2C0805.3549%2C0805.0385%2C0805.0759%2C0805.1050%2C0805.1566%2C0805.3006%2C0805.0080%2C0805.1959%2C0805.1350%2C0805.3487%2C0805.1846%2C0805.2869%2C0805.0449%2C0805.2317%2C0805.2864%2C0805.1537%2C0805.3618%2C0805.1832%2C0805.3273%2C0805.2645%2C0805.0748%2C0805.1654%2C0805.2748%2C0805.4736%2C0805.1777%2C0805.0707%2C0805.3417%2C0805.4826%2C0805.1006%2C0805.3035%2C0805.0245%2C0805.3741%2C0805.0738%2C0805.3778%2C0805.3261%2C0805.1640%2C0805.3848%2C0805.3769%2C0805.0381%2C0805.4320%2C0805.0650%2C0805.2331%2C0805.1908%2C0805.0315%2C0805.4397%2C0805.3165%2C0805.4112%2C0805.4274%2C0805.0064%2C0805.2915%2C0805.1101%2C0805.4502%2C0805.3100%2C0805.2822%2C0805.1211%2C0805.2440%2C0805.2420%2C0805.1947%2C0805.0545%2C0805.4532%2C0805.0943%2C0805.1748%2C0805.4029%2C0805.2422%2C0805.1647%2C0805.4723%2C0805.2487%2C0805.1020%2C0805.2447%2C0805.0392%2C0805.0077%2C0805.2965%2C0805.0505%2C0805.2804%2C0805.1126%2C0805.4618%2C0805.4036%2C0805.1261%2C0805.1018%2C0805.2335%2C0805.1550%2C0805.1008%2C0805.1485%2C0805.0967%2C0805.4114%2C0805.1365%2C0805.0001%2C0805.0785%2C0805.0475&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Event Synchronization by Lightweight Message Passing"}, "summary": "Concurrent ML's events and event combinators facilitate modular concurrent\nprogramming with first-class synchronization abstractions. A standard\nimplementation of these abstractions relies on fairly complex manipulations of\nfirst-class continuations in the underlying language. In this paper, we present\na lightweight implementation of these abstractions in Concurrent Haskell, a\nlanguage that already provides first-order message passing. At the heart of our\nimplementation is a new distributed synchronization protocol. In contrast with\nseveral previous translations of event abstractions in concurrent languages, we\nremain faithful to the standard semantics for events and event combinators; for\nexample, we retain the symmetry of $\\mathtt{choose}$ for expressing selective\ncommunication.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0805.4634%2C0805.4830%2C0805.3036%2C0805.2751%2C0805.3179%2C0805.0263%2C0805.3923%2C0805.1656%2C0805.2644%2C0805.2839%2C0805.3462%2C0805.0274%2C0805.3549%2C0805.0385%2C0805.0759%2C0805.1050%2C0805.1566%2C0805.3006%2C0805.0080%2C0805.1959%2C0805.1350%2C0805.3487%2C0805.1846%2C0805.2869%2C0805.0449%2C0805.2317%2C0805.2864%2C0805.1537%2C0805.3618%2C0805.1832%2C0805.3273%2C0805.2645%2C0805.0748%2C0805.1654%2C0805.2748%2C0805.4736%2C0805.1777%2C0805.0707%2C0805.3417%2C0805.4826%2C0805.1006%2C0805.3035%2C0805.0245%2C0805.3741%2C0805.0738%2C0805.3778%2C0805.3261%2C0805.1640%2C0805.3848%2C0805.3769%2C0805.0381%2C0805.4320%2C0805.0650%2C0805.2331%2C0805.1908%2C0805.0315%2C0805.4397%2C0805.3165%2C0805.4112%2C0805.4274%2C0805.0064%2C0805.2915%2C0805.1101%2C0805.4502%2C0805.3100%2C0805.2822%2C0805.1211%2C0805.2440%2C0805.2420%2C0805.1947%2C0805.0545%2C0805.4532%2C0805.0943%2C0805.1748%2C0805.4029%2C0805.2422%2C0805.1647%2C0805.4723%2C0805.2487%2C0805.1020%2C0805.2447%2C0805.0392%2C0805.0077%2C0805.2965%2C0805.0505%2C0805.2804%2C0805.1126%2C0805.4618%2C0805.4036%2C0805.1261%2C0805.1018%2C0805.2335%2C0805.1550%2C0805.1008%2C0805.1485%2C0805.0967%2C0805.4114%2C0805.1365%2C0805.0001%2C0805.0785%2C0805.0475&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Concurrent ML's events and event combinators facilitate modular concurrent\nprogramming with first-class synchronization abstractions. A standard\nimplementation of these abstractions relies on fairly complex manipulations of\nfirst-class continuations in the underlying language. In this paper, we present\na lightweight implementation of these abstractions in Concurrent Haskell, a\nlanguage that already provides first-order message passing. At the heart of our\nimplementation is a new distributed synchronization protocol. In contrast with\nseveral previous translations of event abstractions in concurrent languages, we\nremain faithful to the standard semantics for events and event combinators; for\nexample, we retain the symmetry of $\\mathtt{choose}$ for expressing selective\ncommunication."}, "authors": ["Avik Chaudhuri"], "author_detail": {"name": "Avik Chaudhuri"}, "author": "Avik Chaudhuri", "links": [{"href": "http://arxiv.org/abs/0805.4029v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0805.4029v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "D.3.3; D.1.3; F.3.3", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0805.4029v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0805.4029v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Event Synchronization by Lightweight Message Passing\n\narXiv:0805.4029v1 [cs.PL] 27 May 2008\n\nAvik Chaudhuri\nComputer Science Department\nUniversity of California, Santa Cruz\navik@cs.ucsc.edu\n\nAbstract. Concurrent ML's events and event combinators facilitate modular concurrent programming with first-class synchronization abstractions. A standard\nimplementation of these abstractions relies on fairly complex manipulations of\nfirst-class continuations in the underlying language. In this paper, we present a\nlightweight implementation of these abstractions in Concurrent Haskell, a language that already provides first-order message passing. At the heart of our implementation is a new distributed synchronization protocol. In contrast with several\nprevious translations of event abstractions in concurrent languages, we remain\nfaithful to the standard semantics for events and event combinators; for example,\nwe retain the symmetry of choose for expressing selective communication.\n\n1 First-class synchronization abstractions\nIn his doctoral thesis [12], Reppy invents the concept of first-class synchrony to facilitate modular concurrent programming in ML. He argues:\nUnfortunately there is a fundamental conflict between the desire for abstraction and the\nneed for selective communication [in concurrent programs]. . . . To resolve the conflict\n. . . requires introducing a new abstraction mechanism that preserves the synchronous\nnature of the abstraction.\n\nThus, Reppy introduces a new type constructor event to type synchronous operations\nin much the same way as \u2192 (\"arrow\") types functional values.\nThis allows us to represent synchronous operations as first-class values, instead of\nmerely as functions . . . [and design] a collection of combinators for defining new event\nvalues [from primitive ones]. . . . Selective communication is expressed as a choice among\nevent values, which means that user-defined abstractions can be used in a selective\ncommunication without breaking the abstraction.\n\nReppy implements events in an extension of ML, called Concurrent ML (CML) [13],\nand provides a formal semantics for synchronization of events [12]. While the implementation itself is fairly complex, it allows programmers to write sophisticated communication and synchronization protocols as first-class abstractions in the resulting language. Next, we provide a brief introduction to CML's events and event combinators.\nThe interested reader can find a more detailed account of first-class synchrony and its\nsignificance as a programming paradigm for concurrency in [13].\nIn particular, note that channel and event are polymorphic type constructors in\nCML, as follows:\n\n\f\u2013 The type channel \u03c4 is given to channels that carry values of type \u03c4 .\n\u2013 The type event \u03c4 is given to events that return values of type \u03c4 on synchronization.\nThe combinators receive and transmit can build primitive events for synchronous\ncommunication.\nreceive : channel \u03c4 \u2192 event \u03c4\ntransmit : channel \u03c4 \u2192 \u03c4 \u2192 event ()\n\u2013 receive c returns an event that, on synchronization, accepts a message M on channel c and returns M . Such an event must synchronize with transmit c M .\n\u2013 transmit c M returns an event that, on synchronization, sends the message M on\nchannel c and returns () (\"unit\"). Such an event must synchronize with receive c.\nThe combinator choose can non-deterministically select an event from a list of events,\nso that the selected event can be synchronized. In particular, choose can express any\nselective communication.\nThe combinator wrapabort can specify an action that is spawned if an event is not\nselected by a choose.\nchoose : [event \u03c4 ] \u2192 event \u03c4\nwrapabort : (() \u2192 ()) \u2192 event \u03c4 \u2192 event \u03c4\n\u2013 choose V returns an event that, on synchronization, synchronizes one of the events\nin list V and \"aborts\" the other events.\n\u2013 wrapabort f v returns an event that, on synchronization, synchronizes the event\nv, and on abortion, spawns a thread that runs the code f (). Here, if v itself is of\nthe form choose V and one of the events in V is selected, then v is considered\nselected, so f is not spawned.\nThe combinators guard and wrap can specify actions that are run before and after\nsynchronization, respectively.\nguard : (() \u2192 event \u03c4 ) \u2192 event \u03c4\nwrap : event \u03c4 \u2192 (\u03c4 \u2192 \u03c4 \u2032 ) \u2192 event \u03c4 \u2032\n\u2013 guard f returns an event that, on synchronization, synchronizes the event returned\nby the code f (). Here, f () is run every time a thread tries to synchronize guard f .\n\u2013 wrap v f returns an event that, on synchronization, synchronizes the event v and\napplies function f to the result.\nFinally, the function sync can synchronize an event and return the result.\nsync : event \u03c4 \u2192 \u03c4\nNote that by construction, an event can synchronize at exactly one \"commit point\",\nwhere a message is either sent or accepted on a channel. This commit point may be\nselected among several other, potential commit points. Some code may be run before\nsynchronization, as specified by guard functions throughout the event. Some more code\nmay be run after synchronization, as specified by wrap functions that surround the\ncommit point, and by wrapabort functions that do not surround the commit point.\n\n\fReppy's implementation of synchronization involves fairly complex manipulations\nof first-class continuations in phases [13]. Even the channel communication functions\naccept : channel \u03c4 \u2192 \u03c4\nsend : channel \u03c4 \u2192 \u03c4 \u2192 ()\nare derived by synchronization on the respective base events.\naccept c = sync (receive c)\nsend c M = sync (transmit c M )\nIn contrast, in this paper we show how to implement first-class event synchronization\nin a language that already provides first-order synchronous communication. Our implementation relies on a new distributed synchronization protocol, which we formalize as\nan abstract state machine and prove correct (Section 2). We concretely implement this\nprotocol by message passing in Concurrent Haskell [8], a language that is quite close to\nthe pi calculus [11]. Building on this implementation, we present an encoding of CML\nevents and event combinators in Concurrent Haskell (Sections 3 and 4).\nWe are certainly not the first to encode CML-style concurrency primitives in another\nlanguage. However, a lightweight implementation of first-class event synchronization\nby message passing, in the exact sense of Reppy [12], has not appeared before. We defer\na more detailed discussion on related work to Section 6.\nBefore we present our protocol, we introduce its main ideas through the following\n(fictional) narrative, which describes an analogous protocol for arranging marriages.\nIn the Land of Fr\u0131\u0304g, there are many young inhabitants who are eager to get\nmarried. At the same time, the rivalry among siblings is so fierce that if a\nyoung man or woman gets married, his or her siblings commit suicide. There\nare many priests, who serve as matchmakers. Eager young inhabitants flock to\nthese matchmakers to meet prospective partners of the opposite sex. When two\nsuch partners meet, they reserve the priest who matches them, and then inform\ntheir parents. Meanwhile, the priest stops matching other couples.\nParents select the first child to inform them about meeting a partner, and\nsend their approval to the concerned priest. For all other children who are too\nlate, on the other hand, they send back their refusal. If a priest receives approval\nfrom both parties, he confirms the marriage date to both sides; following this\ninformation, the couple weds. If one party refuses and the other approves, the\npriest alerts the approving side that the impending marriage must be canceled;\nfollowing this information, the young members of that family begin searching\nfor partners once again. The priest now resumes matching other couples.\nObviously, we would like to have progress in the Land of Fr\u0131\u0304g-weddings should be\npossible as long as there remain eligible couples. Can we prove this property? Yes. We\nfirst prove the following lemma: if there remain two inhabitants of opposite sex who are\nlooking for partners, a priest can eventually match them. Indeed, pick any priest. Either\nthat priest is free, or two partners who have already met have reserved that priest. In the\nlatter case, both partners inform their parents; both parents send their decisions to the\npriest; therefore, the priest is eventually free.\n\n\fNow, suppose that there remain two inhabitants A and B of opposite sex who are\nlooking for partners. Then (by the lemma above) a priest eventually matches them. Next,\nA and B inform their parents. Now, if both A and B are the first among their siblings to\ninform their parents, they eventually get married and we are done. On the other hand,\nsuppose that one of A's siblings informs A's parent first. (Clearly, that sibling could not\nhave been married when A was looking for partners, since otherwise, A would have\nbeen dead, not looking for partners.) Now, either that sibling gets married, and we are\ndone; or, that sibling does not get married, and A tries again. Similarly, either one of B's\nsiblings gets married and we are done, or B tries again. Now, if A and B both try again,\nthey can be the first among their siblings to inform their parents; so, they eventually get\nmarried, and we are done.\n\n2 A distributed event synchronization protocol\nWe now present a distributed protocol for synchronizing events, that is based on the\nprotocol for arranging marriages in the Land of Fr\u0131\u0304g. Specifically, we interpret\n\u2013 young inhabitants as potential commit points, or simply, points;\n\u2013 priests as channels;\n\u2013 parents as synchronization sites, or simply, synchronizers.\nIn other words, points, channels, and synchronizers are principals in our protocol. A\npoint is a site of pending input or output on a channel-every receive or transmit\nevent contains a point. Every application of sync contains a synchronizer.\nWe focus on synchronization of events that are built with the combinators receive,\ntransmit, and choose. The other combinators do not fundamentally affect the protocol; we consider them only in the concrete implementation in Section 3.\nA source language For brevity, we simplify the syntax of the source language.\n\u2013 Actions \u03b1, \u03b2 are of the form c or c (input or output on channel c).\n\u2013 Programs are of the form S1 | . . . | Sn (parallel composition of S1 , . . . , Sn ), where\n\u2192\neach Si is either an action \u03b1, or a synchronization of a choice of actions select(\u2212\n\u03b1 ).\nFurther, we consider only the following local reduction rule, which models selective\n\u2192\n\u2212\ncommunication:\n\u2192\nc\u2208\u2212\n\u03b1\nc\u2208 \u03b2\n\u2192\n\u2212\n\u2192\nselect(\u2212\n\u03b1 ) | select( \u03b2 ) \u2192 c | c\nbesides the usual structural rules for parallel composition. In particular, we ignore reduction of actions at this level of abstraction.\nA distributed abstract state machine for synchronization We formalize our protocol\nas a distributed abstract state machine that implements the above semantics of selective\ncommunication. Let \u03c3 range over states of the machine. These states may be of the\nform \u03c3 | \u03c3 \u2032 (parallel composition), (\u03bdp) \u03c3 (name restriction), or \u03c2 (sub-state of some\nprincipal in the protocol). Sub-states may be of the following forms.\n\n\f\u03c2p ::=\nsub-states of points\np 7\u2192 \u03b1\nunmatched\nCandidatep\nmatched\n\u03b1\nmarried\n\u03c2c ::=\nsub-states of channels\n\u2299c\nfree\nMatchc (p, q)\nbusy\n\n\u03c2s ::=\nsub-states of synchronizers\n\u0003s\nopen\n\u22a0s\nclosed\nSelects (p)\napproved\nReject(p)\nrefused\nDones (p)\nconfirmed\nRetrys\ncanceled\n\nHere p, c, and s range over points, channels, and synchronizers. A synchronizer is a partial function from points to actions; we represent this function as a parallel composition\nof bindings of the form p 7\u2192 \u03b1. Further, we require that each point belongs to exactly\none synchronizer, that is, for any s and s\u2032 , s 6= s\u2032 \u21d2 dom(s) \u2229 dom(s\u2032 ) = \u2205. The\nsemantics of the machine is described by the following local transition rules, plus the\nusual structural rules for parallel composition and name restriction (cf. the pi calculus [11], for example). In Section 3, these rules are implemented by message passing\nbetween appropriate processes run at points, channels, and synchronizers.\np 7\u2192 c | q 7\u2192 c | \u2299c \u2192 Candidatep | Candidateq | Matchc (p, q)\np \u2208 dom(s)\n(II.i)\nCandidatep | \u0003s \u2192 \u22a0s | Selects (p)\n\n(I)\n\np \u2208 dom(s)\n(II.ii)\nCandidatep | \u22a0s \u2192 \u22a0s | Reject(p)\n\nSelects (p) | Selects\u2032 (q) | Matchc (p, q) \u2192 Dones (p) | Dones\u2032 (q) | \u2299c\nSelects (p) | Reject(q) | Matchc (p, q) \u2192 Retrys | \u2299c\n\n(III.ii)\n\nReject(p) | Selects (q) | Matchc (p, q) \u2192 Retrys | \u2299c\n\n(III.iii)\n\n(III.i)\n\nReject(p) | Reject(q) | Matchc (p, q) \u2192 \u2299c\n(III.iv)\n\u2192\n\u2212\ndom(s) = p\ns(p) = \u03b1\n(IV.i)\n(IV.ii)\n\u2192\nDones (p) \u2192 \u03b1\nRetrys \u2192 (\u03bd \u2212\np ) (\u0003s | s)\nThe rules may be read as follows.\n(I) Two points p and q, bound to complementary actions on channel c, react with c if it\nis free (\u2299c ), so that p and q become candidates and the channel becomes busy.\n(II.i\u2013ii) Next, p (and likewise, q) reacts with its synchronizer s. If the synchronizer\nis open (\u0003s ), it now becomes closed (\u22a0s ), and p is declared selected by s. If the\nsynchronizer is already closed, then p is rejected.\n(III.i\u2013iv) If both p and q are selected, c confirms the selections to both parties. If only\none of them is selected, c cancels that selection. The channel now becomes free.\n(IV.i\u2013ii) If the selection of p is confirmed, the action bound to p is released. Otherwise,\nthe synchronizer \"reboots\" with fresh names for the points in its domain.\nCompilation We compile the source language to this machine. Let the symbol \u03a0 denote finite parallel composition. Suppose that the set of channels in a program \u03a0i\u22081..n Si\nis C. We compile this program to the state \u03a0c\u2208C \u2299c | \u03a0i\u22081..n Sbi , where\n\u001a\n\u03b1\nif S = \u03b1\nb\nS=\n\u2192\n\u2192), where s = \u03a0 (p 7\u2192 \u03b1 ) for fresh names \u2212\n\u2192\n(\u03bd \u2212\npj ) (\u0003s | s) if S = select(\u2212\n\u03b1\npj\nj\nj\nj\nj\n\n\fCorrectness We prove that our protocol is correct, that is, the abstract machine correctly implements selective communication, by showing that the compilation from programs to states preserves progress and safety. Let a denotation be a list of actions. The\ndenotations of programs and states are derived by the function p*q, as follows.\npS1 | . . . | Sn q = pS1 q \u228e * * * \u228e pSn q\np\u03b1q = [\u03b1]\n\u2192\n\u2212\npselect( \u03b1 )q = []\n\np\u03c3 | \u03c3 \u2032 q = p\u03c3q \u228e p\u03c3 \u2032 q\np(\u03bdp) \u03c3q = p\u03c3q\n\u001a\n[\u03b1] if \u03c2 = \u03b1\np\u03c2q =\n[] otherwise\n\nNow, if a program is compiled to some state, then the denotations of the program and\nthe state coincide. Further, we have the following theorem, proved in the appendix.\nTheorem 1 (Correctness). Let C be the set of channels in a program \u03a0i\u22081..n Si . Then\n\u03a0i\u22081..n Si \u223c \u03a0c\u2208C \u2299c | \u03a0i\u22081..n Sbi , where \u223c is the largest relation such that P \u223c \u03c3 iff\n\n(Correspondence) \u03c3 \u2192\u22c6 \u03c3 \u2032 for some \u03c3 \u2032 such that pPq = p\u03c3 \u2032 q;\n(Safety) if \u03c3 \u2192 \u03c3 \u2032 for some \u03c3 \u2032 , then P \u2192\u22c6 P \u2032 for some P \u2032 such that P \u2032 \u223c \u03c3 \u2032 ;\n(Progress) if P \u2192 , then \u03c3 \u2192+ \u03c3 \u2032 and P \u2192 P \u2032 for some \u03c3 \u2032 and P \u2032 such that P \u2032 \u223c \u03c3 \u2032 .\n\nExample Consider the program select(x, y) | select(y, z) | select(z) | select(x). The\nprogram can reduce either to x | z | z | x, or to y | y | select(z) | select(x). The\ndenotations of these reduced programs are {x, x, z, z} and {y, y}, respectively. The\noriginal program is compiled to the state\n\u2299x | \u2299y | \u2299z | (\u03bdpx\u0304 p\u0233 ) (\u0003(px\u0304 7\u2192x | p\u0233 7\u2192y) | px\u0304 7\u2192 x | p\u0233 7\u2192 y) |\n(\u03bdpy pz ) (\u0003(py 7\u2192y | pz 7\u2192z) | py 7\u2192 y | pz 7\u2192 z) |\n(\u03bdpz\u0304 ) (\u0003(pz\u0304 7\u2192z) | pz\u0304 7\u2192 z) |\n(\u03bdpx ) (\u0003(px 7\u2192x) | px 7\u2192 x)\nThis state can transition in multiple steps to either of the following states, with denotations {x, x, z, z} and {y, y}, respectively. (In these states, \u03c3junk can be garbagecollected, and is separated out for readability.)\n\u2013 x | z | z | x | \u2299x | \u2299y | \u2299z | \u03c3junk\n\u2013 y | y | (\u03bdpz\u0304 ) (\u0003(pz\u0304 7\u2192z) | pz\u0304 7\u2192 z) | (\u03bdpx ) (\u0003(px 7\u2192x) | px 7\u2192 x) | \u2299x | \u2299y | \u2299z | \u03c3junk\n\u03c3junk , (\u03bdpx\u0304 p\u0233 py pz pz\u0304 px ) (\u22a0(px\u0304 7\u2192x | p\u0233 7\u2192y) | \u22a0(py 7\u2192y | pz 7\u2192z) | \u22a0(pz\u0304 7\u2192z) | \u22a0(px 7\u2192x) )\n\n3 A concrete implementation in Concurrent Haskell\nThe abstract machine of the previous section can be concretely implemented by a system of communicating processes. Indeed, we now present a complete implementation\nof a CML-style event library in a fragment of Concurrent Haskell with first-order message passing. This fragment is rather close to the pi calculus. Thus, we ensure that our\nimplementation can be ported without difficulty to languages that support first-order\ncommunication. At the same time, we take advantage of Haskell's type system to show\nhow events and event combinators can be typed under the IO monad [7,9].\n\n\fBefore we proceed, we briefly review some of the concurrency primitives in Concurrent Haskell. Note that MVar and IO are polymorphic type constructors, as follows:\n\u2013 The type MVar \u03c4 is given to a communication cell that carries values of type \u03c4 .\n\u2013 The type IO \u03c4 is given to a computation that yields results of type \u03c4 , with possible\nside effects via communication.\nWe rely on the following semantics of MVar cells.\n\u2013 A cell can carry at most one value at a time.\n\u2013 The function New :: IO (MVar \u03c4 ) returns a fresh, empty cell.\n\u2013 The function Get :: MVar \u03c4 \u2192 IO \u03c4 is used to read from a cell; Get m blocks if\nthe cell m is empty, else gets the content of m (thereby emptying it).\n\u2013 The function Put :: MVar \u03c4 \u2192 \u03c4 \u2192 IO () is used to write to a cell; Put m M\nputs the term M in cell m if it is empty, else blocks.\nFurther, we rely on the following semantics of IO computations.\n\u2013 The function fork :: IO () \u2192 IO () is used to spawn a concurrent computation.\n\u2013 The function return :: \u03c4 \u2192 IO \u03c4 is used to inject a value into a computation.\n\u2013 Computations can be sequentially composed by \"piping\". We use Haskell's convenient do{. . . ; . . . } notation for this purpose, instead of applying the de jure function\n>>= :: IO \u03c4 \u2192 (\u03c4 \u2192 IO \u03c4 \u2032 ) \u2192 IO \u03c4 \u2032 .\nImplementing synchronization by message passing We implement the following\nfunctions for programming with first-class events in Concurrent Haskell. (Note the differences between ML and Haskell types for these functions. Since Haskell is purely\nfunctional, we must embed types for computations with possible side-effects via communication, within the IO monad. Further, since evaluation in Haskell is lazy, we can\ndiscard abstractions that only serve to \"delay\" evaluation.)\nnew :: IO (channel \u03c4 )\nreceive :: channel \u03c4 \u2192 event \u03c4\ntransmit :: channel \u03c4 \u2192 \u03c4 \u2192 event ()\nguard :: IO (event \u03c4 ) \u2192 event \u03c4\nwrap :: event \u03c4 \u2192 (\u03c4 \u2192 IO \u03c4 \u2032 ) \u2192 event \u03c4 \u2032\nchoose :: [event \u03c4 ] \u2192 event \u03c4\nwrapabort :: IO () \u2192 event \u03c4 \u2192 event \u03c4\nsync :: event \u03c4 \u2192 IO \u03c4\nFor now, we focus on events that are built without wrapabort (i.e., we focus on programs without abort actions); the full implementation appears in Section 4.\nWe begin by concretizing the abstract state machine in Section 2. Specifically, we\nrun some protocol code at points, channels, and synchronizers, which reduce by simple\nmessage passing on MVar cells. In this implementation:\n\u2013 Each point is identified with a fresh name p :: Point.\n\n\f\u2013 Each channel c is identified with a pair of fresh cells in [c] :: In and out [c] :: Out\non which it receives messages from points that are bound to actions on c.\n\u2013 Each synchronizer is identified with a fresh cell s :: Synchronizer on which it\nreceives messages from points in its domain.\nBefore we present protocol code, let us describe the sequence of messages exchanged\nin a typical session of the protocol, and mention the involved sub-states. On the way, we\ndevelop type definitions for the MVar cells on which those messages are exchanged.\n\u2013 A point p (at state p 7\u2192 c or p 7\u2192 c) begins by sending a message to c on its\nrespective input or output cell in [c] or out [c] ; the message contains a fresh cell\ncandidate [p] :: Candidate on which p expects a reply from c.\ntype In = MVar Candidate\ntype Out = MVar Candidate\n\u2013 When c (at state \u2299c ) gets a pair of messages on in [c] and out [c] , say from p and\nanother point q, it replies by sending fresh cells decision [p] :: Decision and\ndecision [q] :: Decision on candidate [p] and candidate [q] respectively (reaching\nstate Matchc (p, q)), and expects the synchronizers for p and q to reply on them.\ntype Candidate = MVar Decision\n\u2013 On receiving a message from c on candidate [p] , p (reaching state Candidatep ) tags\nthe message with its name and forwards it to its synchronizer on the cell s.\ntype Synchronizer = MVar (Point, Decision)\n\u2013 If p is the first point to send such a message on s (that is, s is at state \u0003s ), a fresh cell\ncommit [p] :: Commit is sent back on decision [p] (reaching state \u22a0s | Selects (p));\nfor each subsequent message received on s, say from p\u2032 , a blank message is sent\n\u2032\nback on decision [p ] (reaching state \u22a0s | Reject(p\u2032 )).\ntype Decision = MVar (Maybe Commit)1\n\u2013 On receiving messages from the respective synchronizers of p and q on decision [p]\nand decision [q] , c inspects the messages and responds (reaching state \u2299c ).\n\u2022 If both commit [p] and commit [q] have come in, a positive signal is sent back\non commit [P ] and commit [Q] .\n\u2022 If only commit [p] has come in, a negative signal is sent back on commit [p] ; if\nonly commit [q] has come in, a negative signal is sent back on commit [q] .\ntype Commit = MVar Bool\n\u2013 If s receives a positive signal on commit [p] (reaching state Dones (p)), it signals\non p to continue. If, instead, a negative signal is received (reaching state Retrys ),\nanother session ensues.\ntype Point = MVar ()\n1\n\nRecall that the Haskell type Maybe \u03c4 is given to a value that is either Nothing, or of the\nform Just v where v is of type \u03c4 . The function maybe :: \u03c4 \u2032 \u2192 (\u03c4 \u2192 \u03c4 \u2032 ) \u2192 Maybe \u03c4 \u2192 \u03c4 \u2032\nis the associated case analyzer. For instance, the function isJust :: Maybe \u03c4 \u2192 Bool is\ndefined as maybe False (\u03bb . True).\n\n\fProtocol code for points The protocol code run by points abstracts on a cell s for the\nassociated synchronizer, and a name p for the point itself. Depending on whether the\npoint is for input or output, the code additionally abstracts on an input cell i or output\ncell o, and an input or output action \u03b1.\n@PointI :: Synchronizer \u2192 Point \u2192 In \u2192 IO \u03c4 \u2192 IO \u03c4\n@PointI s p i \u03b1 = do {candidate \u2190 New; Put i candidate;\ndecision \u2190 Get candidate; Put s (p, decision );\nGet t; \u03b1}\n@PointO :: Synchronizer \u2192 Point \u2192 Out \u2192 IO () \u2192 IO ()\n@PointO s p o \u03b1 = do {candidate \u2190 New; Put o candidate;\ndecision \u2190 Get candidate; Put s (p, decision );\nGet t; \u03b1}\nProtocol code for channels The protocol code run by channels abstracts on an input\ncell i and an output cell o for the channel.\n@Chan :: In \u2192 Out \u2192 IO ()\n@Chan i o = do {candidate i \u2190 Get i; candidate o \u2190 Get o;\ndecision i \u2190 New; Put candidate i decision i ; xi \u2190 Get decision i ;\ndecision o \u2190 New; Put candidate o decision o ; xo \u2190 Get decision o ;\nmaybe (return ()) (\u03bbcommit i . Put commit i (isJust xo )) xi ;\nmaybe (return ()) (\u03bbcommit o . Put commit o (isJust xi )) xo }\nProtocol code for synchronizers The protocol code run by synchronizers abstracts on\na cell s for that synchronizer and some \"rebooting code\" X. (Here, we encode a loop\nwith the function fix :: (\u03c4 \u2192 \u03c4 ) \u2192 \u03c4 ; recall that fix f reduces to f (fix f ).)\n@Sync :: Synchronizer \u2192 IO () \u2192 IO ()\n@Sync s X = do {(p, decision) \u2190 Get s;\nfork (fix (\u03bbiter . do {\n(p\u2032 , decision \u2032 ) \u2190 Get s; Put decision \u2032 Nothing; iter }));\ncommit \u2190 New; Put decision (Just commit );\ndone \u2190 Get commit ; if done then (Put p ()) else X}\nWe instantiate these processes in the translation of new, receive, transmit, and sync\nbelow. But first, let us translate types for channels and events.\nTranslation of types The Haskell types for ML channel and event values are:\nchannel \u03c4 = (In, Out, MVar \u03c4 )\nevent \u03c4 = Synchronizer \u2192 IO \u03c4\nAn ML channel is a Haskell MVar tagged with a pair of input and output cells. An\nML event is a Haskell IO function that abstracts on a synchronizer cell.\n\n\fTranslation of functions We now translate functions for programming with events.\nWe begin by compiling the ML function for creating channels.\nnew :: IO (channel \u03c4 )\nnew = do {i \u2190 New; o \u2190 New;\nfork (fix (\u03bbiter . do {@Chan i o; iter }));\nm \u2190 New; return (i, o, m)}\n\u2013 The term new spawns a looping instance of @Chan with a fresh pair of input and\noutput cells, and returns that pair along with a fresh MVar cell that carries messages for the channel.\nNext, we compile the ML combinators for building base communication events. Recall\nthat a Haskell event is an IO function that abstracts on the cell of its synchronizer.\nreceive :: channel \u03c4 \u2192 event \u03c4\nreceive (i, o, m) = \u03bbs. do {p \u2190 New; @PointI s p i (Get m)}\ntransmit :: channel \u03c4 \u2192 \u03c4 \u2192 event ()\ntransmit (i, o, m) M = \u03bbs. do {p \u2190 New; @PointO s p o (Put m M )}\n\u2013 The term receive c s runs an instance of @PointI with the synchronizer s, a\nfresh name for the point, the input cell for channel c, and an action that inputs on c.\n\u2013 The term transmit c M s is symmetric; it runs an instance of @PointO with\nthe synchronizer s, a fresh name for the point, the output cell for channel c, and an\naction that outputs term M on c.\nNext, we compile the ML event combinators for specifying actions that are run before\nand after synchronization.\nguard :: IO (event \u03c4 ) \u2192 event \u03c4\nguard f = \u03bbs. do {v \u2190 f ; v s}\nwrap :: event \u03c4 \u2192 (\u03c4 \u2192 IO \u03c4 \u2032 ) \u2192 event \u03c4 \u2032\nwrap v f = \u03bbs. do {x \u2190 v s; f x}\n\u2013 The term guard f s runs the computation f and passes the synchronizer cell s to\nthe event returned by the computation.\n\u2013 The term wrap v f s passes the synchronizer cell s to the event v and pipes the\nreturned value to function f .\nNext, we compile the ML combinator for choosing among a list of events. (Here, we\nencode recursion over a list with the function fold :: (\u03c4 \u2032 \u2192 \u03c4 \u2192 \u03c4 \u2032 ) \u2192 \u03c4 \u2032 \u2192 [\u03c4 ] \u2192\n\u03c4 \u2032 ; recall that fold f x [] reduces to x and fold f x [v, V ] reduces to fold f (f x v) V .)\nchoose :: [event \u03c4 ] \u2192 event \u03c4\nchoose V = \u03bbs. do {temp \u2190 New;\nfold (\u03bb v. fork (do {x \u2190 v s; Put temp x})) () V ;\nGet temp}\n\n\f\u2013 The term choose V s spawns a thread for each event v in V , passing the synchronizer s to v; any value returned by one of these threads is collected in a fresh cell\ntemp and returned.\nFinally, we compile the ML function for event synchronization.\nsync :: event \u03c4 \u2192 IO \u03c4\nsync v = do {temp \u2190 New;\nfork (fix (\u03bbiter . do {\ns \u2190 New; fork (@Sync s iter ); x \u2190 v s; Put temp x}));\nGet temp}\n\u2013 The term sync v recursively spawns an instance of @Sync with a fresh synchronizer s and passes s to the event v; any value returned by one of these instances is\ncollected in a fresh cell temp and returned.\n\n4 Compiling abort actions\nThe implementation of the previous section does not account for wrapabort. We now\nshow how wrapabort can be handled by slightly extending our notion of event.\nRecall that abort actions (such as those specified by wrapabort) are spawned only\nat events that do not enclose the commit point. Therefore, in an implementation of\nwrapabort, it makes sense to name events with the sets of points they enclose. However, computing the set of points that an event encloses should not interfere with the dynamic semantics. In particular, for an event built with guard, we cannot run the guard\nfunctions to compute the set of points that the event encloses. Thus, we refrain from\nnaming events at compile time. Instead, we introduce events as principals in our protocol; each event is named in situ by computing the list of points it encloses at runtime.\nThis list is carried on a fresh cell name :: Name for the event.\ntype Name = MVar [Point]\nFurther, each synchronizer carries a fresh cell abort :: Abort on which it receives\nwrapabort functions from events, tagged with the list of points they enclose.\ntype Abort = MVar ([Point], IO ())\nProtocol code run by points and channels remain the same. We add a handler for\nwrapabort functions to the protocol code run by synchronizers. Accordingly, the code\nnow abstracts on an abort cell.\n@Sync :: Synchronizer \u2192 Abort \u2192 IO () \u2192 IO ()\n@Sync s abort X = do {. . . ;\nif done then do {. . . ;\nfix (\u03bbiter . do {\n(P, f ) \u2190 Get abort ; fork iter ;\nif p \u2208 P then return () else f })}\nelse . . . }\n\n\fHere, after signaling the commit point p to continue (as earlier), the synchronizer continues to accept abort code f on abort ; such code is spawned only if the list of points\nP , enclosed by the event that sends that code, does not include p.\nThe extended Haskell type for event values is as follows.\nevent \u03c4 = Synchronizer \u2192 Name \u2192 Abort \u2192 IO \u03c4\nNow, an ML event is a Haskell IO function that abstracts on a synchronizer, an abort\ncell, and a name cell that carries the list of points the event encloses.\nThe Haskell function new does not change. We highlight minor changes in the remaining translations. We begin with the functions transmit and receive. An event\nbuilt with either function is named by a singleton containing the name of that point.\ntransmit (i, o, m) M = \u03bbs name abort . do {. . . ; fork (Put name [p]); . . . }\nreceive (i, o, m) = \u03bbs name abort . do {. . . ; fork (Put name [p]); . . . }\nThe function choose becomes slightly lengthy. A fresh name \u2032 cell is passed to each\nevent in the list; the names of those events are concatenated to name the choose event.\nchoose V = \u03bbs name abort . do {. . . ;\nP \u2190 fold (\u03bbP v. do {\nname \u2032 \u2190 New;\nfork (do {x \u2190 v s name \u2032 abort ; . . . });\nP \u2032 \u2190 Get name \u2032 ; Put name \u2032 P \u2032 ;\nreturn (P \u2032 \u228e P )}) [] V ;\nfork (Put name P );\n...}\nWe now compile the ML event combinator for specifying abort actions.\nwrapabort :: IO () \u2192 event \u03c4 \u2192 event \u03c4\nwrapabort f v = \u03bbs name abort . do {\nfork (do {P \u2190 Get name; Put name P ; Put abort (P, f )});\nv s name abort }\n\u2013 The term wrapabort f v s name abort spawns a thread that reads the list of\nenclosed events P on the cell name and sends the function f along with P on the\ncell abort ; the synchronizer s is passed to the event v along with name and abort .\nThe functions guard and wrap remain similar.\nguard f = \u03bbs name abort . do {v \u2190 f ; v s name abort }\nwrap v f = \u03bbs name abort . do {x \u2190 v s name abort ; f x}\nFinally, in the function sync, a fresh abort cell is now passed to @Sync, and a fresh\nname cell is created for the event to be synchronized.\n\n\fsync v = do {. . . ;\nfork (fix (\u03bbiter . do {\n. . . ; name \u2190 New; abort \u2190 New;\nfork (@Sync s abort iter ); x \u2190 v s name abort ; . . . }));\n...}\n\n5 Implementing communication guards\nBeyond the standard primitives for communication in CML, some previous implementations of events further consider guarded communication. We discuss how our implementation can be easily extended to handle such communication. Specifically, we\nrequire the following receive combinator, that can carry a communication guard.\nreceive :: channel \u03c4 \u2192 (\u03c4 \u2192 Bool) \u2192 event \u03c4\nIntuitively, receive c cond synchronizes with transmit c M only if cond M is true.\nIn our implementation, we make some slight adjustments to the types of some MVar\ncells.\ntype In \u03c4 = MVar (Candidate, \u03c4 \u2192 Bool)\ntype Out \u03c4 = MVar (Candidate, \u03c4 )\ntype Candidate = MVar (Maybe Decision)\nNext, we adjust the protocol code run by points and channels. Input and output points\nthat are bound to actions on c respectively send their conditions and messages to c. A\npair of points is matched only if the message of one satisfies the condition of the other.\n@Chan :: In \u03c4 \u2192 Out \u03c4 \u2192 IO ()\n@Chan i o = do {(candidate i , cond ) \u2190 Get i; (candidate o , M ) \u2190 Get o;\nif (cond M ) then do {\n. . . ; Put candidate i (Just decision i ); . . . ;\n. . . ; Put candidate o (Just decision o ); . . . ;\n...}\nelse do {Put candidate i Nothing; Put candidate i Nothing}}\n@PointI :: Synchronizer \u2192 Point \u2192 In \u03c4 \u2192 (\u03c4 \u2192 Bool) \u2192 IO \u03c4 \u2192 IO \u03c4\n@PointI s p i cond \u03b1 = do {. . . ; Put i (candidate, cond );\nx \u2190 Get candidate;\nmaybe (@PointI s p i cond \u03b1)\n(\u03bbdecision . do {Put s (p, decision ); . . . }) x}\n@PointO :: Synchronizer \u2192 Point \u2192 Out \u03c4 \u2192 \u03c4 \u2192 IO () \u2192 IO ()\n@PointO s p o M \u03b1 = do {. . . ; Put o (candidate, M );\nx \u2190 Get candidate;\nmaybe (@PointO s p o M \u03b1)\n(\u03bbdecision . do {Put s (p, decision ); . . . }) x}\n\n\fFinally, we make the following trivial adjustments to the type constructor channel,\nand the functions receive and transmit.\ntype channel \u03c4 = (In \u03c4, Out \u03c4, MVar \u03c4 )\nreceive (i, o, m) cond = \u03bbs name abort . do {. . . ; @PointI s p i cond (Get m)}\ntransmit (i, o, m) M = \u03bbs name abort . do {. . . ; @PointO s p o M (Put m M )}\n\n6 Related work\nWe are not the first to implement CML-style concurrency primitives in another language. In particular, Russell presents an implementation of events in Concurrent Haskell\nin [14]. The implementation provides guarded channels, which filter communication\nbased on conditions on message values (as in Section 5). Unfortunately, the implementation requires a rather complex Haskell type for event values. In particular, a value of\ntype event \u03c4 must carry (among other things) a continuation of type IO \u03c4 \u2192 IO ().\nAn important difference between Russell's implementation and ours is that Russell's\nchoose combinator is asymmetric. In contrast, we implement a symmetric choose\ncombinator, following the standard CML semantics. While it is difficult to compare\nother aspects of our implementations, we should point out that Russell's event library is\nmore than 1300 lines of Haskell code (without comments), compared to our 150. Yet,\nguarded communication in the sense of Russell can be readily implemented in our setting, as shown in Section 5. In the end, we believe that this difference in complexity is\nlargely due to the elegance of our synchronization protocol.\nRecently, Donnelly and Fluet [5] introduce transactional events and implement\nthem over the software transactional memory (STM) module in Concurrent Haskell.\nTheir key observation is that combining all-or-nothing transactions with CML-style\nconcurrency recovers a monad. Unfortunately, implementing transactional events requires solving NP-hard problems [5]. In contrast, our direct implementation of CMLstyle concurrency remains rather lightweight.\nOther implementations of events include those of Flatt and Findler in Scheme [6]\nand of Demaine in Java [4]. While Flatt and Findler focus on kill-safety, Demaine focuses on efficiency by exploiting communication patterns that involve either single receivers or single senders. Demaine does not consider event combinators-in particular,\nit is not clear whether his implementation can accommodate abort actions.\nDistributed protocols for implementing selective communication date back to the\n1980s. The protocols of Buckley and Silberschatz [3] and Bagrodia [1] seem to be\namong the earliest in this line of work. Unfortunately, those protocols are prone to\ndeadlock. Bornat [2] proposes a protocol that is deadlock-free assuming communication between single receivers and single senders. Finally in [10], Knabe presents the\nfirst deadlock-free protocol to implement selective communication for arbitrary channel communication. Knabe's protocol appears to be the closest to ours. Channels are\nconsidered as sites of control, and messages are exchanged between communication\npoints and channels to negotiate synchronization. However, Knabe assumes a global\nordering on processes and maintains queues for matching points; we do not require either of these facilities in our protocol. Moreover, as in [4], it is not clear whether the\nprotocol can accommodate event combinators such as guard and wrapabort.\n\n\f7 Conclusion\nIn this paper, we show how to implement first-class event synchronization in Concurrent Haskell, a language with first-order message passing. We appear to be the first to\nimplement the standard semantics for events and event combinators in this setting. An\ninteresting consequence of our work is that implementing distributed selective communication is reduced to implementing distributed message-passing in Concurrent Haskell.\nAt the heart of our implementation is a new deadlock-free protocol that is run among\ncommunication points, channels, and synchronization sites. This protocol seems to be\nrobust enough to allow implementations of sophisticated synchronization primitives.\nAll the code presented in this paper is available online at\nhttp://www.soe.ucsc.edu/ \u0303avik/projects/CML\nAcknowledgments This work started off as a class project for Cormac Flanagan's course\non Concurrent Programming and Transactional Memory at UC Santa Cruz in Spring\n2007. Thanks to him, Mart\u0131\u0301n Abadi, and Andy Gordon for their enthusiasm and curiosity, which encouraged further development of this work since that course.\n\nReferences\n1. R. Bagrodia. A distributed algorithm to implement the generalized alternative command of\nCSP. In ICDCS '86: Distributed Computing Systems, pages 422\u2013427. IEEE, 1986.\n2. R. Bornat. A protocol for generalized Occam. Software Practice and Experience, 16(9):783\u2013\n799, 1986.\n3. G. N. Buckley and A. Silberschatz. An effective implementation for the generalized inputoutput construct of CSP. ACM Transactions on Programming Languages and Systems,\n5(2):223\u2013235, 1983.\n4. E. D. Demaine. Protocols for non-deterministic communication over synchronous channels.\nIn IPPS/SPDP '98: Parallel and Distributed Processing, pages 24\u201330. IEEE, 1998.\n5. K. Donnelly and M. Fluet. Transactional events. SIGPLAN Notices, 41(9):124\u2013135, 2006.\n6. M. Flatt and R. B. Findler. Kill-safe synchronization abstractions. In PLDI '04: Programming Language Design and Implementation, pages 47\u201358. ACM, 2004.\n7. A. D. Gordon. Functional programming and input/output. Cambridge University, 1994.\n8. S. L. P. Jones, A. D. Gordon, and S. Finne. Concurrent Haskell. In POPL '96: Principles of\nProgramming Languages, pages 295\u2013308. ACM, 1996.\n9. S. L. P. Jones and P. Wadler. Imperative functional programming. In POPL '93: Principles\nof programming languages, pages 71\u201384. ACM, 1993.\n10. F. Knabe. A distributed protocol for channel-based communication with choice. In PARLE\n'92: Parallel Architectures and Languages, Europe, pages 947\u2013948. Springer-Verlag, 1992.\n11. R. Milner, J. Parrow, and D. Walker. A calculus of mobile processes, parts I and II. Information and Computation, 100(1):1\u201377, 1992.\n12. J. H. Reppy. Higher-order concurrency. PhD thesis, Cornell University, 1992. Available as\nTechnical Report 92-1852.\n13. J. H. Reppy. Concurrent programming in ML. Cambridge University, 1999.\n14. G. Russell. Events in Haskell, and how to implement them. In ICFP '01: Functional Programming, pages 157\u2013168. ACM, 2001.\n\n\fA\n\nCorrectness proof for the synchronization protocol\n\nIn this appendix, we prove correctness of the synchronization protocol (Theorem 1).\nThis proof closely follows the informal proof of progress in the Land of Fr\u0131\u0304g.\nConsider any state \u03c3. We begin by defining some invariants that \u03c3 must satisfy;\n\u2192\nthe satisfaction of these invariants is written as \u22a2 \u03c3. Let \u03c3 be in the form (\u03bdP ) \u03a0 \u2212\n\u03c2.\n\u2192\n\u2212\n\u2192\n\u2212\nWe assume that the set of points in \u03c2 is P , the set of channels in \u03c2 is C, the set of\n\u2192\nsynchronizers in \u2212\n\u03c2 is S, and C \u2229 P = \u2205. Then \u22a2 \u03c3 if:\n1. For any p \u2208 P, there is a unique s \u2208 S such that p \u2208 dom(s). Further, let s(p) = \u03b1.\n\u2192\nThen at most one of the following sub-states is in \u2212\n\u03c2:\n{p 7\u2192 \u03b1, Candidatep , Selects (p), Reject(p), Dones (p), Retrys }\n\u2192\nand exactly one of the following sub-states is in \u2212\n\u03c2:\n{\u0003s , \u22a0s }\n\u2192\n2. For every c \u2208 C, exactly one of the following sub-states is in \u2212\n\u03c2:\n{\u2299c , Matchc ( , )}\n3. Let p \u2208 P and s \u2208 S such that p \u2208 dom(s). Then, if one of the following sub-states\n\u2192\nis in \u2212\n\u03c2:\n{Selects (p), Reject(p), Dones (p), Retrys }\n\u2192\n\u2192\nthen \u22a0s is in \u2212\n\u03c2 . On the other hand, if \u22a0s is in \u2212\n\u03c2 then there is at most one p such\n\u2192\nthat p \u2208 dom(s) and one of the following sub-states is in \u2212\n\u03c2:\n{Selects (p), Dones (p), Retrys }\n\u2192\n4. Let c \u2208 C and p, q \u2208 P . Then Matchc (p, q) is in \u2212\n\u03c2 iff there are (not necessarily\n\u2032\n\u2032\ndistinct) s, s \u2208 S such that s(p) = c, s (q) = c, one of the following sub-states is\n\u2192\nin \u2212\n\u03c2:\n{Candidatep , Selects (p), Reject(p)}\n\u2192\nand one of the following sub-states is in \u2212\n\u03c2:\n{Candidateq , Selects\u2032 (q), Reject(q)}\nIt is easy to see that (1\u20134) are invariants for any state compiled from a program.\nLemma 1. Let C be the set of channels in a program \u03a0i\u22081..n Si . Now, suppose that\n\u03a0c\u2208C \u2299c | \u03a0i\u22081..n Sbi \u2192\u22c6 \u03c3. Then \u22a2 \u03c3.\nNext, we prove the analog of the lemma in Section 1.\n\n\u2192\nLemma 2. Suppose that \u22a2 (\u03bdP ) \u03a0 \u2212\n\u03c2 . Let p, q \u2208 P , and let p 7\u2192 c and q 7\u2192 c be in\n\u2192\n\u2212\n\u2192\n\u2212\n\u2192\n\u2212\n\u2192\n\u2212\n\u22c6\n\u2032\n\u03c2 . Then (\u03bdP ) \u03a0 \u03c2 \u2212\u2192 (\u03bdP )\u03a0 \u03c2 such that \u2299c is in \u03c2 \u2032 .\n\n\f\u2192\nProof. If \u2299c is in \u2212\n\u03c2 , we are done. Otherwise, by (2) it follows that Matchc (p\u2032 , q \u2032 ) in\n\u2192\n\u2212\n\u2032 \u2032\nin \u03c2 for some p , q \u2208 P . Now, by (1) and (4), p\u2032 6= p and q \u2032 6= q. Further, by (4),\n\u2192\nthere are s and s\u2032 in the set of synchronizers in \u2212\n\u03c2 such that p\u2032 \u2208 dom(s), q \u2032 \u2208 dom(s\u2032 ),\n\u2192\n\u2212\n\u2032\n\u2032\nCandidatep\u2032 or Selects (p ) or Reject(p ) is in \u03c2 , and Candidateq\u2032 or Selects\u2032 (q \u2032 ) or\n\u2192\n\u2192\n\u2192\n\u03c2 . Now,\nReject(q \u2032 ) is in \u2212\n\u03c2 . Further, by (1), \u0003s or \u22a0s is in \u2212\n\u03c2 , and \u0003s\u2032 or \u22a0s\u2032 is in \u2212\n\u2192\n\u2212\n\u2032\u2032\n\u2032\n(II.i\u2013ii) can be applied to move to a state (\u03bdP )\u03a0 \u03c2 such that Selects (p ) or Reject(p\u2032 )\n\u2192\n\u2212\n\u2192\n\u2212\nis in \u03c2 \u2032\u2032 , and Selects\u2032 (q \u2032 ) or Reject(q \u2032 ) is in \u03c2 \u2032\u2032 . Finally, (III.i\u2013iv) can be applied to move\n\u2192\n\u2212\nto the required state (\u03bdP )\u03a0 \u03c2 \u2032 .\nWe are now ready to prove the main theorem.\nRestatement of Theorem 1. Let C be the set of channels in a program \u03a0i\u22081..n Si . Then\n\u03a0i\u22081..n Si \u223c \u03a0c\u2208C \u2299c | \u03a0i\u22081..n Sbi , where \u223c is the largest relation such that P \u223c \u03c3 iff\n(Correspondence) \u03c3 \u2192\u22c6 \u03c3 \u2032 for some \u03c3 \u2032 such that pPq = p\u03c3 \u2032 q;\n(Safety) if \u03c3 \u2192 \u03c3 \u2032 for some \u03c3 \u2032 , then P \u2192\u22c6 P \u2032 for some P \u2032 such that P \u2032 \u223c \u03c3 \u2032 ;\n(Progress) if P \u2192 , then \u03c3 \u2192+ \u03c3 \u2032 and P \u2192 P \u2032 for some \u03c3 \u2032 and P \u2032 such that P \u2032 \u223c \u03c3 \u2032 .\n\nProof. The proof of (Safety) is fairly easy. (Progress) follows from Lemmas 1 and 2, as\nfollows. By Lemma 1, we can consider only a subset \u2243 of \u223c such that \u2243 \u03c3 \u21d2 \u22a2 \u03c3.\n\u2192\nNow, suppose that P \u2243 (\u03bdP ) \u2212\n\u03c2 and P \u2192 .\n\u2192\n\u03c2 . Then, by Lemma 2\nWe first assume that there are some p 7\u2192 c and q 7\u2192 c in \u2212\n\u2192\n\u2212\n\u2192\n\u2212\n\u2192\n\u2212\n\u22c6\n\u2032\nand (I), (\u03bdP ) \u03c2 \u2192 (\u03bdP ) \u03c2 such that Candidatep and Candidateq are in \u03c2 \u2032 . It can be\nshown, following the reasoning of Section 1, that eventually: either c and c are released;\nor, some \u03b1 is released such that for some point p\u2032 and synchronizer s, we have s(p\u2032 ) = \u03b1\nand either p \u2208 dom(s) or q \u2208 dom(s). In the former case, P makes the corresponding\nreduction, and we are done. In the latter case, the action complementary to \u03b1 is released\nin the next step; then P makes the corresponding reduction, and we are done.\n\u2192\nOn the other hand, if there are no p 7\u2192 c and q 7\u2192 c in \u2212\n\u03c2 , then by (1) there must\n\u2032\n\u2032\nbe some p and q, and s and s , such that s(p) = c, s (q) = c, and one of the following\n\u2192\nsub-states is in \u2212\n\u03c2:\n{Candidatep , Selects (p), Reject(p), Dones (p), Retrys }\n\u2192\nand one of the following sub-states is in \u2212\n\u03c2:\n{Candidateq , Selects\u2032 (q), Reject(q), Dones\u2032 (q), Retrys\u2032 }\n\u2192\nSo either Candidatep and Candidateq are in \u2212\n\u03c2 , or p and q are in sub-states reachable\nfrom Candidatep and Candidateq ; in either case, we proceed as before.\n\n\f"}