{"id": "http://arxiv.org/abs/0801.1772v1", "guidislink": true, "updated": "2008-01-11T14:48:43Z", "updated_parsed": [2008, 1, 11, 14, 48, 43, 4, 11, 0], "published": "2008-01-11T14:48:43Z", "published_parsed": [2008, 1, 11, 14, 48, 43, 4, 11, 0], "title": "Bi-criteria Pipeline Mappings for Parallel Image Processing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0801.1119%2C0801.0652%2C0801.2652%2C0801.0152%2C0801.0585%2C0801.2463%2C0801.2834%2C0801.4360%2C0801.4874%2C0801.0950%2C0801.3116%2C0801.3694%2C0801.1337%2C0801.2357%2C0801.4508%2C0801.2984%2C0801.4575%2C0801.3916%2C0801.0928%2C0801.1410%2C0801.4905%2C0801.1882%2C0801.2141%2C0801.4598%2C0801.1684%2C0801.3484%2C0801.1772%2C0801.2410%2C0801.0704%2C0801.2215%2C0801.4272%2C0801.0065%2C0801.2286%2C0801.3831%2C0801.4565%2C0801.0772%2C0801.0016%2C0801.4201%2C0801.3619%2C0801.3659%2C0801.2227%2C0801.2003%2C0801.1028%2C0801.2688%2C0801.2785%2C0801.3872%2C0801.3419%2C0801.0479%2C0801.1045%2C0801.3765%2C0801.4812%2C0801.4241%2C0801.4580%2C0801.3727%2C0801.4520%2C0801.0165%2C0801.1273%2C0801.2295%2C0801.1877%2C0801.2478%2C0801.0223%2C0801.1795%2C0801.4760%2C0801.3002%2C0801.1332%2C0801.1695%2C0801.0736%2C0801.4055%2C0801.3588%2C0801.2565%2C0801.3426%2C0801.3225%2C0801.3090%2C0801.3574%2C0801.0679%2C0801.3041%2C0801.1931%2C0801.1338%2C0801.3135%2C0801.0262%2C0801.0171%2C0801.0402%2C0801.2461%2C0801.3543%2C0801.3628%2C0801.1447%2C0801.1027%2C0801.2944%2C0801.3532%2C0801.4456%2C0801.1617%2C0801.3366%2C0801.1009%2C0801.4395%2C0801.4806%2C0801.0381%2C0801.1011%2C0801.3432%2C0801.2991%2C0801.4095%2C0801.3712&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Bi-criteria Pipeline Mappings for Parallel Image Processing"}, "summary": "Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonistic criteria should be optimized, such as throughput and latency (or a\ncombination). Typical applications include digital image processing, where\nimages are processed in steady-state mode. In this paper, we study the mapping\nof a particular image processing application, the JPEG encoding. Mapping\npipelined JPEG encoding onto parallel platforms is useful for instance for\nencoding Motion JPEG images. As the bi-criteria mapping problem is NP-complete,\nwe concentrate on the evaluation and performance of polynomial heuristics.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0801.1119%2C0801.0652%2C0801.2652%2C0801.0152%2C0801.0585%2C0801.2463%2C0801.2834%2C0801.4360%2C0801.4874%2C0801.0950%2C0801.3116%2C0801.3694%2C0801.1337%2C0801.2357%2C0801.4508%2C0801.2984%2C0801.4575%2C0801.3916%2C0801.0928%2C0801.1410%2C0801.4905%2C0801.1882%2C0801.2141%2C0801.4598%2C0801.1684%2C0801.3484%2C0801.1772%2C0801.2410%2C0801.0704%2C0801.2215%2C0801.4272%2C0801.0065%2C0801.2286%2C0801.3831%2C0801.4565%2C0801.0772%2C0801.0016%2C0801.4201%2C0801.3619%2C0801.3659%2C0801.2227%2C0801.2003%2C0801.1028%2C0801.2688%2C0801.2785%2C0801.3872%2C0801.3419%2C0801.0479%2C0801.1045%2C0801.3765%2C0801.4812%2C0801.4241%2C0801.4580%2C0801.3727%2C0801.4520%2C0801.0165%2C0801.1273%2C0801.2295%2C0801.1877%2C0801.2478%2C0801.0223%2C0801.1795%2C0801.4760%2C0801.3002%2C0801.1332%2C0801.1695%2C0801.0736%2C0801.4055%2C0801.3588%2C0801.2565%2C0801.3426%2C0801.3225%2C0801.3090%2C0801.3574%2C0801.0679%2C0801.3041%2C0801.1931%2C0801.1338%2C0801.3135%2C0801.0262%2C0801.0171%2C0801.0402%2C0801.2461%2C0801.3543%2C0801.3628%2C0801.1447%2C0801.1027%2C0801.2944%2C0801.3532%2C0801.4456%2C0801.1617%2C0801.3366%2C0801.1009%2C0801.4395%2C0801.4806%2C0801.0381%2C0801.1011%2C0801.3432%2C0801.2991%2C0801.4095%2C0801.3712&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonistic criteria should be optimized, such as throughput and latency (or a\ncombination). Typical applications include digital image processing, where\nimages are processed in steady-state mode. In this paper, we study the mapping\nof a particular image processing application, the JPEG encoding. Mapping\npipelined JPEG encoding onto parallel platforms is useful for instance for\nencoding Motion JPEG images. As the bi-criteria mapping problem is NP-complete,\nwe concentrate on the evaluation and performance of polynomial heuristics."}, "authors": ["Anne Benoit", "Harald Kosch", "Veronika Rehn-Sonigo", "Yves Robert"], "author_detail": {"name": "Yves Robert"}, "author": "Yves Robert", "links": [{"href": "http://arxiv.org/abs/0801.1772v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0801.1772v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0801.1772v1", "affiliation": "INRIA Rh\u00f4ne-Alpes, LIP", "arxiv_url": "http://arxiv.org/abs/0801.1772v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Bi-criteria Pipeline Mappings\nfor Parallel Image Processing\nAnne Benoit - Harald Kosch - Veronika Rehn-Sonigo - Yves Robert\n\nN\u00b0 6410\nJanuary 2008\n\napport\nde recherche\n\nISRN INRIA/RR--6410--FR+ENG\n\nTh\u00e8me NUM\n\nISSN 0249-6399\n\narXiv:0801.1772v1 [cs.DC] 11 Jan 2008\n\nINSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE\n\n\f\fBi-criteria Pipeline Mappings\nfor Parallel Image Processing\nAnne Benoit , Harald Kosch , Veronika Rehn-Sonigo , Yves Robert\nTh\u00e8me NUM - Syst\u00e8mes num\u00e9riques\nProjet GRAAL\nRapport de recherche n\u00b0 6410 - January 2008 - 13 pages\n\nAbstract: Mapping workflow applications onto parallel platforms is a challenging problem,\neven for simple application patterns such as pipeline graphs. Several antagonistic criteria\nshould be optimized, such as throughput and latency (or a combination). Typical applications\ninclude digital image processing, where images are processed in steady-state mode.\nIn this paper, we study the mapping of a particular image processing application, the\nJPEG encoding. Mapping pipelined JPEG encoding onto parallel platforms is useful for\ninstance for encoding Motion JPEG images. As the bi-criteria mapping problem is NPcomplete, we concentrate on the evaluation and performance of polynomial heuristics.\nKey-words: pipeline, workflow application, multi-criteria optimization, JPEG encoding\n\nThis text is also available as a research report of the Laboratoire de l'Informatique du Parall\u00e9lisme\nhttp://www.ens-lyon.fr/LIP.\n\nUnit\u00e9 de recherche INRIA Rh\u00f4ne-Alpes\n655, avenue de l'Europe, 38334 Montbonnot Saint Ismier (France)\nT\u00e9l\u00e9phone : +33 4 76 61 52 00 - T\u00e9l\u00e9copie +33 4 76 61 52 52\n\n\fMapping bi-cit\u00e8re de pipelines pour le traitement d'image en\nparall\u00e8le\nR\u00e9sum\u00e9 : L'ordonnancement et l'allocation des workflows sur plates-formes parall\u00e8les est\nun probl\u00e8me crucial, m\u00eame pour des applications simples comme des graphes en pipeline.\nPlusieurs crit\u00e8res contradictoires doivent \u00eatre optimis\u00e9s, tels que le d\u00e9bit et la latence (ou une\ncombinaison des deux). Des applications typiques incluent le traitement d'images num\u00e9riques,\no\u00f9 les images sont trait\u00e9es en r\u00e9gime permanent.\nDans ce rapport, nous \u00e9tudions l'ordonnancement et l'allocation d'une application de\ntraitement d'image particuli\u00e8re, l'encodage JPEG. L'allocation de l'encodage JPEG pipelin\u00e9\nsur des plates-formes parall\u00e8les est par exemple utile pour l'encodage des images Motion\nJPEG. Comme le probl\u00e8me de l'allocation bi-crit\u00e8re est NP-complet, nous nous concentrons\nsur l'analyse et \u00e9valuation d'heuristiques polynomiales.\nMots-cl\u00e9s : pipeline, application workflow, optimisation multi-crit\u00e8re, encodage JPEG\n\n\fBi-criteria Pipeline Mappings\n\n1\n\n3\n\nIntroduction\n\nThis work considers the problem of mapping workflow applications onto parallel platforms.\nThis is a challenging problem, even for simple application patterns. For homogeneous architectures, several scheduling and load-balancing techniques have been developed but the\nextension to heterogeneous clusters makes the problem more difficult.\nStructured programming approaches rule out many of the problems which the low-level\nparallel application developer is usually confronted to, such as deadlocks or process starvation.\nWe therefore focus on pipeline applications, as they can easily be expressed as algorithmic\nskeletons. More precisely, in this paper, we study the mapping of a particular pipeline application: we focus on the JPEG encoder (baseline process, basic mode). This image processing\napplication transforms numerical pictures from any format into a standardized format called\nJPEG. This standard was developed almost 20 years ago to create a portable format for the\ncompression of still images and new versions are created until now (see http://www.jpeg.org/).\nJPEG (and later JPEG 2000) is used for encoding still images in Motion-JPEG (later MJ2).\nThese standards are commonly employed in IP-cams and are part of many video applications in the world of game consoles. Motion-JPEG (M-JPEG) has been adopted and further\ndeveloped to several other formats, e.g., AMV (alternatively known as MTV) which is a proprietary video file format designed to be consumed on low-resource devices. The manner of\nencoding in M-JPEG and subsequent formats leads to a flow of still image coding, hence\npipeline mapping is appropriate.\nWe consider the different steps of the encoder as a linear pipeline of stages, where each\nstage gets some input, has to perform several computations and transfers the output to the\nnext stage. The corresponding mapping problem can be stated informally as follows: which\nstage to assign to which processor? We require the mapping to be interval-based, i.e., a\nprocessor is assigned an interval of consecutive stages. Two key optimization parameters\nemerge. On the one hand, we target a high throughput, or short period, in order to be able\nto handle as many images as possible per time unit. On the other hand, we aim at a short\nresponse time, or latency, for the processing of each image. These two criteria are antagonistic:\nintuitively, we obtain a high throughput with many processors to share the work, while we\nget a small latency by mapping many stages to the same processor in order to avoid the cost\nof inter-stage communications.\nThe rest of the paper is organized as follows: Section 2 briefly describes JPEG coding\nprinciples. In Section 3 the theoretical and applicative framework is introduced, and Section 4\nis dedicated to linear programming formulation of the bi-criteria mapping. In Section 5 we\ndescribe some polynomial heuristics, which we use for our experiments of Section 6. We\ndiscuss related work in Section 7. Finally, we give some concluding remarks in Section 8.\n\n2\n\nPrinciples of JPEG encoding\n\nHere we briefly present the mode of operation of a JPEG encoder (see [13] for further details).\nThe encoder consists in seven pipeline stages, as shown in Figure 1. In the first stage, the\nimage is scaled to have a multiple of an 8x8 pixel matrix, and the standard even claims a\nmultiple of 16x16. In the next stage a color space conversion is performed: the colors of the\npicture are transformed from the RGB to the YUV-color model. The sub-sampling stage is\nan optional stage, which, depending on the sampling rate, reduces the data volume: as the\n\nRR n\u00b0 6410\n\n\f4\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\nSource\nImage Data\n\nScaling\n\nYUV\nConversion\n\nBlock\nStorage\n\nSubsampling\n\nFDCT\n\nQuantizer\n\nEntropy\nEncoder\n\nQuantization\nTable\n\nHuffman\nTable\n\nCompressed\nImage Data\n\nFigure 1: Steps of the JPEG encoding.\nhuman eye can dissolve luminosity more easily than color, the chrominance components are\nsampled more rarely than the luminance components. Admittedly, this leads to a loss of data.\nThe last preparation step consists in the creation and storage of so-called MCUs (Minimum\nCoded Units), which correspond to 8x8 pixel blocks in the picture. The next stage is the\ncore of the encoder. It performs a Fast Discrete Cosine Transformation (FDCT) (eg. [14])\non the 8x8 pixel blocks which are interpreted as a discrete signal of 64 values. After the\ntransformation, every point in the matrix is represented as a linear combination of the 64\npoints. The quantizer reduces the image information to the important parts. Depending on\nthe quantization factor and quantization matrix, irrelevant frequencies are reduced. Thereby\nquantization errors can occur, that are remarkable as quantization noise or block generation\nin the encoded image. The last stage is the entropy encoder, which performs a modified\nHuffman coding: it combines the variable length codes of Huffman coding with the coding of\nrepetitive data in run-length encoding.\n\n3\n3.1\n\nFramework\nApplicative framework\n\nOn the theoretical point of view, we consider a pipeline of n stages Sk , 1 \u2264 k \u2264 n. Tasks\nare fed into the pipeline and processed from stage to stage, until they exit the pipeline after\nthe last stage. The k-th stage Sk first receives an input from the previous stage, of size \u03b4 k\u22121 ,\nthen performs a number of wk computations, and finally outputs data of size \u03b4 k to the next\nstage. These three operations are performed sequentially. The first stage S1 receives an input\nof size \u03b4 0 from the outside world, while the last stage Sn returns the result, of size \u03b4 n , to the\noutside world, thus these particular stages behave in the same way as the others.\nOn the practical point of view, we consider the applicative pipeline of the JPEG encoder\nas presented in Figure 1 and its seven stages.\n\n3.2\n\nTarget platform\n\nWe target a platform with p processors Pu , 1 \u2264 u \u2264 p, fully interconnected as a (virtual)\nclique. There is a bidirectional link linku,v : Pu \u2192 Pv between any processor pair Pu and\nPv , of bandwidth bu,v . The speed of processor Pu is denoted as su , and it takes X/su timeunits for Pu to execute X floating point operations. We also enforce a linear cost model for\ncommunications, hence it takes X/b time-units to send (resp. receive) a message of size X\nto (resp. from) Pv . Communications contention is taken care of by enforcing the one-port\nmodel [3].\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n3.3\n\n5\n\nBi-criteria interval mapping problem\n\nWe seek to map intervals of consecutive stages onto processors [12]. Intuitively, assigning\nseveral consecutive tasks to the same processor will increase their computational load, but\nmay well dramatically decrease communication requirements. We search for a partition of\n[1..n] into m \u2264 p intervals Ij = [dj , ej ] such that dj \u2264 ej for 1 \u2264 j \u2264 m, d1 = 1, dj+1 = ej + 1\nfor 1 \u2264 j \u2264 m \u2212 1 and em = n.\nThe optimization problem is to determine the best mapping, over all possible partitions\ninto intervals, and over all processor assignments. The objective can be to minimize either\nthe period, or the latency, or a combination: given a threshold period, what is the minimum\nlatency that can be achieved? and the counterpart: given a threshold latency, what is the\nminimum period that can be achieved?\nThe decision problem associated to this bi-criteria interval mapping optimization problem\nis NP-hard, since the period minimization problem is NP-hard for interval-based mappings\n(see [2]).\n\n4\n\nLinear program formulation\n\nWe present here an integer linear program to compute the optimal interval-based bi-criteria\nmapping on Fully Heterogeneous platforms, respecting either a fixed latency or a fixed period.\nWe assume n stages and p processors, plus two fictitious extra stages S0 and Sn+1 respectively\nassigned to Pin and Pout . First we need to define a few variables:\nFor k \u2208 [0..n + 1] and u \u2208 [1..p] \u222a {in, out}, xk,u is a boolean variable equal to 1 if stage Sk is\nassigned to processor Pu ; we let x0,in = xn+1,out = 1, and xk,in = xk,out = 0 for 1 \u2264 k \u2264 n.\nFor k \u2208 [0..n], u, v \u2208 [1..p] \u222a {in, out} with u 6= v, zk,u,v is a boolean variable equal to 1 if\nstage Sk is assigned to Pu and stage Sk+1 is assigned to Pv : hence linku,v : Pu \u2192 Pv is used\nfor the communication between these two stages. If k 6= 0 then zk,in,v = 0 for all v 6= in and\nif k 6= n then zk,u,out = 0 for all u 6= out.\nFor k \u2208 [0..n] and u \u2208 [1..p] \u222a {in, out}, yk,u is a boolean variable equal to 1 if stages Sk and\nSk+1 are both assigned to Pu ; we let yk,in = yk,out = 0 for all k, and y0,u = yn,u = 0 for all u.\nFor u \u2208 [1..p], first(u) is an integer variable which denotes the first stage assigned to Pu ;\nsimilarly, last(u) denotes the last stage assigned to Pu . Thus Pu is assigned the interval\n[first(u), last(u)]. Of course 1 \u2264 first(u) \u2264 last(u) \u2264 n.\nTopt is the variable to optimize, so depending on the objective function it corresponds either\nto the period or to the latency.\nP\nWe list below the constraints that need to be enforced. For simplicity, we write u\nP\ninstead of u\u2208[1..p]\u222a{in,out} when summing over all processors. First there are constraints for\nprocessor and link usage:\nP\nEvery stage is assigned a processor: \u2200k \u2208 [0..n + 1],\nu xk,u = 1.\nEvery communication either is assigned a link or collapses because both stages are assigned\nto the same processor:\n\u2200k \u2208 [0..n],\n\nX\n\nzk,u,v +\n\nu6=v\n\nX\n\nyk,u = 1\n\nu\n\nIf stage Sk is assigned to Pu and stage Sk+1 to Pv , then linku,v : Pu \u2192 Pv is used for this\ncommunication:\n\u2200k \u2208 [0..n], \u2200u, v \u2208 [1..p] \u222a {in, out}, u 6= v,\n\nRR n\u00b0 6410\n\nxk,u + xk+1,v \u2264 1 + zk,u,v\n\n\f6\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\nIf both stages Sk and Sk+1 are assigned to Pu , then yk,u = 1:\n\u2200k \u2208 [0..n], \u2200u \u2208 [1..p] \u222a {in, out},\n\nxk,u + xk+1,u \u2264 1 + yk,u\n\nIf stage Sk is assigned to Pu , then necessarily firstu \u2264 k \u2264 lastu . We write this constraint as:\n\u2200k \u2208 [1..n], \u2200u \u2208 [1..p],\n\nfirstu \u2264 k.xk,u + n.(1 \u2212 xk,u )\n\n\u2200k \u2208 [1..n], \u2200u \u2208 [1..p],\n\nlastu \u2265 k.xk,u\n\nIf stage Sk is assigned to Pu and stage Sk+1 is assigned to Pv 6= Pu (i.e., zk,u,v = 1) then\nnecessarily lastu \u2264 k and firstv \u2265 k + 1 since we consider intervals. We write this constraint\nas:\n\u2200k \u2208 [1..n \u2212 1], \u2200u, v \u2208 [1..p], u 6= v,\nlastu \u2264 k.zk,u,v + n.(1 \u2212 zk,u,v )\n\u2200k \u2208 [1..n \u2212 1], \u2200u, v \u2208 [1..p], u 6= v,\n\nfirstv \u2265 (k + 1).zk,u,v\n\nThe latency of schedule is bounded by Tlatency :\nand t \u2208 [1..p] \u222a {in, out}.\np X\nn\nX\n\nu=1 k=1\n\n\uf8ee\uf8eb\n\uf8f0\uf8ed\n\nX \u03b4 k\u22121\n\nt6=u\n\nbt,u\n\n\uf8f6\n\nzk\u22121,t,u \uf8f8 +\n\n\uf8f9\n\n\uf8eb\n\nwk\n\uf8ec\nxk,u \uf8fb + \uf8ed\nsu\n\nX\n\nu\u2208[1..p]\u222a{in}\n\n\u03b4n\nbu,out\n\n\uf8f6\n\nzn,u,out \uf8f8 \u2264 Tlatency\n\uf8f7\n\nand t \u2208 [1..p] \u222a {in, out}.\nThere remains to express the period of each processor and to constrain it by Tperiod :\n\u2200u \u2208 [1..p],\n\uf8f1\uf8eb\n\uf8f6\n\uf8eb\n\uf8f6\uf8fc\nn \uf8f2 X\n\uf8fd\nX\nX\n\u03b4 k\u22121\nwk\n\u03b4k\n\uf8ed\nzk\u22121,t,u \uf8f8 +\nxk,u + \uf8ed\nzk,u,v \uf8f8 \u2264 Tperiod\n\uf8f3\n\uf8fe\nbt,u\nsu\nbu,v\n\nk=1\n\nt6=u\n\nv6=u\n\nFinally, the objective function is either to minimize the period Tperiod respecting the fixed\nlatency Tlatency or to minimize the latency Tlatency with a fixed period Tperiod . So in the\nfirst case we fix Tlatency and set Topt = Tperiod . In the second case Tperiod is fixed a priori\nand Topt = Tlatency . With this mechanism the objective function reduces to minimizing Topt\nin both cases.\n\n5\n\nOverview of the heuristics\n\nThe problem of bi-criteria interval mapping of workflow applications is NP-hard [2], so in\nthis section we briefly describe polynomial heuristics to solve it. See [2] for a more complete\ndescription or refer to the Web at:\nhttp://graal.ens-lyon.fr/~vsonigo/code/multicriteria/\nIn the following, we denote by n the number of stages, and by p the number of processors.\nWe distinguish two sets of heuristics. The heuristics of the first set aim to minimize the\nlatency respecting an a priori fixed period. The heuristics of the second set minimize the\ncounterpart: the latency is fixed a priori and we try to achieve a minimum period while\nrespecting the latency constraint.\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n5.1\n\n7\n\nMinimizing Latency for a Fixed Period\n\nAll the following heuristics sort processors by non-increasing speed, and start by assigning all\nthe stages to the first (fastest) processor in the list. This processor becomes used.\nH1-Sp-mono-P: Splitting mono-criterion \u2013 At each step, we select the used processor j\nwith the largest period and we try to split its stage interval, giving some stages to the next\nfastest processor j \u2032 in the list (not yet used). This can be done by splitting the interval at\nany place, and either placing the first part of the interval on j and the remainder on j \u2032 , or\nthe other way round. The solution which minimizes max(period(j), period(j \u2032 )) is chosen if it\nis better than the original solution. Splitting is performed as long as we have not reached the\nfixed period or until we cannot improve the period anymore.\nH2-Sp-bi-P: Splitting bi-criteria \u2013 This heuristic uses a binary search over the latency.\nFor this purpose at each iteration we fix an authorized increase of the optimal latency (which\nis obtained by mapping all stages on the fastest processor), and we test if we get a feasible\nsolution via splitting. The splitting mechanism itself is quite similar to H1-Sp-mono-P\n\u2206latency\n) within the authorized\nexcept that we choose the solution that minimizes maxi\u2208{j,j \u2032 } ( \u2206period(j)\nlatency increase to decide where to split. While we get a feasible solution, we reduce the\nauthorized latency increase for the next iteration of the binary search, thereby aiming at\nminimizing the mapping global latency.\nH3-3-Sp-mono-P: 3-splitting mono-criterion \u2013 At each step we select the used processor j with the largest period and we split its interval into three parts. For this purpose we try to\nmap two parts of the interval on the next pair of fastest processors in the list, j \u2032 and j \u2032\u2032 , and to\nkeep the third part on processor j. Testing all possible permutations and all possible positions\nwhere to cut, we choose the solution that minimizes max(period(j), period(j \u2032 ), period(j \u2032\u2032 )).\nH4-3-Sp-bi-P: 3-splitting bi-criteria \u2013 In this heuristic the choice of where to split is\nmore elaborated: it depends not only of the period improvement, but also of the latency\nincrease. Using the same splitting mechanism as in H3-3-Sp-mono-P, we select the solu\u2206latency\n). Here \u2206latency denotes the difference between\ntion that minimizes maxi\u2208{j,j \u2032 ,j \u2032\u2032 } ( \u2206period(i)\nthe global latency of the solution before the split and after the split. In the same manner\n\u2206period(i) defines the difference between the period before the split (achieved by processor\nj) and the new period of processor i.\n\n5.2\n\nMinimizing Period for a Fixed Latency\n\nAs in the heuristics described above, first of all we sort processors according to their speed\nand map all stages on the fastest processor.\nH5-Sp-mono-L: Splitting mono-criterion \u2013 This heuristic uses the same method as H1Sp-mono-P with a different break condition. Here splitting is performed as long as we do not\nexceed the fixed latency, still choosing the solution that minimizes max(period(j), period(j \u2032 )).\n\nRR n\u00b0 6410\n\n\f8\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\nPf ix = 310\n1\n\nLopt = 337, 575\n2\n\n3\n\n4\n\n5\n\nP6\n\n6\n\n3\n\n4\n\nP6\n\n5\n\n2\n\nP5\n\nLopt = 336, 729\n2\n\n6\n\n1\n\n3\n\n4\n\n5\n\n4\n\n1\n\n1\n\n7\n\nPopt = 307, 319\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nP3\n\nLf ix = 330\n\n7\n\n6\nP3\n\nP4\n\n6\n\n5\n\nLf ix = 340\n\n7\n\nLopt = 322, 700\n2\n\n3\nP7\n\nP3\n\nPf ix = 330\n\nPopt = 307, 319\n\n1\n\nP3\n\nPf ix = 320\n1\n\nLf ix = 370\n\n7\n\nPopt = 322, 700\n2\n\n3\n\n4\n\nP3\n\nP3\n\na)\n\nb)\n\n5\n\n6\n\n7\n\nFigure 2: LP solutions strongly depend on fixed initial parameters.\nH6-Sp-bi-L: Splitting bi-criteria \u2013 This variant of the splitting heuristic works similarly\n\u2206latency\nto H5-Sp-mono-L, but at each step it chooses the solution which minimizes maxi\u2208{j,j \u2032 } ( \u2206period(i)\n)\nwhile the fixed latency is not exceeded.\nRemark In the context of M-JPEG coding, minimizing the latency for a fixed period corresponds to a fixed coding rate, and we want to minimize the response time. The counterpart\n(minimizing the period respecting a fixed latency L) corresponds to the question: if I accept\nto wait L time units for a given image, which coding rate can I achieve? We evaluate the\nbehavior of the heuristics with respect to these questions in Section 6.2.\n\n6\n\nExperiments and simulations\n\nIn the following experiments, we study the mapping of the JPEG application onto clusters of\nworkstations.\n\n6.1\n\nInfluence of fixed parameters\n\nIn this first test series, we examine the influence of fixed parameters on the solution of the\nlinear program. As shown in Figure 2, the division into intervals is highly dependant of the\nchosen fixed value. The optimal solution to minimize the latency (without any supplemental\nconstraints) obviously consists in mapping the whole application pipeline onto the fastest\nprocessor. As expected, if the period fixed in the linear program is not smaller than the\nlatter optimal mono-criterion latency, this solution is chosen. Decreasing the value for the\nfixed period imposes to split the stages among several processors, until no more solution can\nbe found. Figure 2(a) shows the division into intervals for a fixed period. A fixed period\nof Tperiod = 330 is sufficiently high for the whole pipeline to be mapped onto the fastest\nprocessor, whereas smaller periods lead to splitting into intervals. We would like to mention,\nthat for a period fixed to 300, there exists no solution anymore. The counterpart - fixed\nlatency - can be found in Figure 2(b). Note that the first two solutions find the same period,\nbut for a different latency. The first solution has a high value for latency, which allows more\nsplits, hence larger communication costs. Comparing the last lines of Figures 2(a) and (b), we\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n9\n\n350\n\n330\nL_fixed\n325\n\n340\nOptimal Period\n\nOptimal Latency\n\nP_fixed\n\n330\n320\n310\n300\n300\n\n320\n315\n310\n305\n\n305\n\n310 315 320\nFixed Period\n\n325\n\n330\n\n(a) Fixed P.\n\n300\n320 330 340 350 360 370 380 390 400\nFixed Latency\n\n(b) Fixed L.\n\nFigure 3: Bucket behavior of LP solutions.\nstate that both solutions are the same, and we have Tperiod = Tlatency . Finally, expanding\nthe range of the fixed values, a sort of bucket behavior becomes apparent: Increasing the\nfixed parameter has in a first time no influence, the LP still finds the same solution until the\nincrease crosses an unknown bound and the LP can find a better solution. This phenomenon\nis shown in Figure 3.\n\n6.2\n\nAssessing heuristic performance\n\nThe comparison of the solution returned by the LP program, in terms of optimal latency\nrespecting a fixed period (or the converse) with the heuristics is shown in Figure 4. The\nimplementation is fed with the parameters of the JPEG encoding pipeline and computes the\nmapping on 10 randomly created platforms with 10 processors. On platforms 3 and 5, no\nvalid solution can be found for the fixed period. There are two important points to mention.\nFirst, the solutions found by H2 often are not valid, since they do not respect the fixed period,\nbut they have the best ratio latency/period. Figure 5(b) plots some more details: H2 achieves\ngood latency results, but the fixed period of P=310 is often violated. This is a consequence of\nthe fact that the fixed period value is very close to the feasible period. When the tolerance for\nthe period is bigger, this heuristic succeeds to find low-latency solutions. Second, all solutions,\nLP and heuristics, always keep the stages 4 to 7 together (see Figure 2 for an example). As\nstage 5 (DCT) is the most costly in terms of computation, the interval containing these stages\nis responsible for the period of the whole application.\nFinally, in the comparative study H1 always finds the optimal period for a fixed latency\nand we therefore recommend this heuristic for period optimization. In the case of latency\nminimization for a fixed period, then H5 is to use, as it always finds the LP solution in the\nexperiments. This is a striking result, especially given the fact that the LP integer program\nmay require a long time to compute the solution (up to 11389 seconds in our experiments),\nwhile the heuristics always complete in less than a second, and find the corresponding optimal\nsolution.\n\n6.3\n\nMPI simulations on a cluster\n\nThis last experiment performs a JPEG encoding simulation. All simulations are made on a\ncluster of homogeneous Optiplex GX 745 machines with an Intel Core 2 Duo 6300 of 1,83Ghz.\nHeterogeneity is enforced by increasing and decreasing the number of operations a processor\nhas to execute. The same holds for bandwidth capacities. We call this experiment simulation,\n\nRR n\u00b0 6410\n\n\f10\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\n400\n\n360\nLP fixed P\nH1\nH2\nH3\nH4\n\n360\n\nLP fixed L\nH5\nH6\n\n350\nheuristical solution\n\nheuristical solution\n\n380\n\n340\n320\n300\n\n340\n330\n320\n310\n300\n290\n280\n\n280\n\n270\n0\n\n1\n\n2\n\n3\n4\n5\n6\nrandom platform\n\n7\n\n8\n\n9\n\n0\n\n(a) Fixed P = 310.\n\n1\n\n2\n\n3\n4\n5\n6\nrandom platform\n\n7\n\n8\n\n9\n\n(b) Fixed L = 370.\n\nFigure 4: Behavior of the heuristics (comparing to LP solution).\n350\ntheoretical\nsimulation\n\n600\n500\n\n330\nlatency\n\nlatency\n\nLP\nH2\n\n340\n\n400\n300\n\n320\n310\n300\n\n200\n\n290\n\n100\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n280\n280\n\n290\n\n300\n\nheuristic\n\n(a) Simulative latency.\n\n310\n\n320\n\n330\n\n340\n\n350\n\nperiod\n\n(b) H2 versus LP.\n\nFigure 5: MPI simulation results.\nas we do not parallelize a real JPEG encoder, but we use a parallel pipeline application which\nhas the same parameters for communication and computation as the JPEG encoder. An\nmpich implementation of MPI is used for communications.\nIn this experiment the same random platforms with 10 processors and fixed parameters\nas in the theoretical experiments are used. We measured the latency of the simulation, even\nfor the heuristics of fixed latency, and computed the average over all random platforms.\nFigure 5(a) compares the average of the theoretical results of the heuristics to the average\nsimulative performance. The simulative behavior nicely mirrors the theoretical behavior, with\nthe exception of H2 (see Figure 5(b)). Here once again, some solutions of this heuristic are\nnot valid, as they do not respect the fixed period.\n\n7\n\nRelated work\n\nThe blockwise independent processing of the JPEG encoder allows to apply simple data parallelism for efficient parallelization. Many papers have addressed this fine-grain parallelization\nopportunity [5, 11]. In addition, parallelization of almost all stages, from color space conversion, over DCT to the Huffman encoding has been addressed [1, 7]. Recently, with respect\nto the JPEG2000 codec, efficient parallelization of wavelet coding has been introduced [8].\nAll these works target the best speed-up with respect to different architectures and possible\nvarying load situations. Optimizing the period and the latency is an important issue when\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n11\n\nencoding a pipeline of multiple images, as for instance for Motion JPEG (M-JPEG). To meet\nthese issues, one has to solve in addition to the above mentioned work a bi-criteria optimization problem, i.e., optimize the latency, as well as the period. The application of coarse grain\nparallelism seems to be a promising solution. We propose to use an interval-based mapping\nstrategy allowing multiple stages to be mapped to one processor which allows meeting the\nmost flexible the domain constraints (even for very large pictures). Several pipelined versions\nof the JPEG encoding have been considered. They rely mainly on pixel or block-wise parallelization [6, 9]. For instance, Ferretti et al. [6] uses three pipelines to carry out concurrently\nthe encoding on independent pixels extracted from the serial stream of incoming data. The\npixel and block-based approach is however useful for small pictures only. Recently, Sheel et\nal. [10] consider a pipeline architecture where each stage presents a step in the JPEG encoding. The targeted architecture consists of Xtensa LX processors which run subprograms\nof the JPEG encoder program. Each program accepts data via the queues of the processor,\nperforms the necessary computation, and finally pushes it to the output queue into the next\nstage of the pipeline. The basic assumptions are similar to our work, however no optimization\nproblem is considered and only runtime (latency) measurements are available. The schedule\nis static and set according to basic assumptions about the image processing, e.g., that the\nDCT is the most complex operation in runtime.\n\n8\n\nConclusion\n\nIn this paper, we have studied the bi-criteria (minimizing latency and period) mapping of\npipeline workflow applications, from both a theoretical and practical point of view. On the\ntheoretical side, we have presented an integer linear programming formulation for this NPhard problem. On the practical side, we have studied in depth the interval mapping of the\nJPEG encoding pipeline on a cluster of workstations. Owing to the LP solution, we were able\nto characterize a bucket behavior in the optimal solution, depending on the initial parameters.\nFurthermore, we have compared the behavior of some polynomial heuristics to the LP solution\nand we were able to recommended two heuristics with almost optimal behavior for parallel\nJPEG encoding. Finally, we evaluated the heuristics running a parallel pipeline application\nwith the same parameters as a JPEG encoder. The heuristics were designed for general\npipeline applications, and some of them were aiming at applications with a large number of\nstages (3-splitting), thus a priori not very efficient on the JPEG encoder. Still, some of these\nheuristics reach the optimal solution in our experiments, which is a striking result.\nA natural extension of this work would be to consider further image processing applications\nwith more pipeline stages or a slightly more complicated pipeline architecture. Naturally, our\nwork extends to JPEG 2000 encoding which offers among others wavelet coding and more\ncomplex multiple-component image encoding [4]. Another extension is for the MPEG coding\nfamily which uses lagged feedback: the coding of some types of frames depends on other\nframes. Differentiating the types of coding algorithms, a pipeline architecture seems again to\nbe a promising solution architecture.\n\nReferences\n[1] L. V. Agostini, I. S. Silva, and S. Bampi. Parallel color space converters for JPEG image\ncompression. Microelectronics Reliability, 44(4):697\u2013703, 2004.\n\nRR n\u00b0 6410\n\n\f12\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\n[2] A. Benoit, V. Rehn-Sonigo, and Y. Robert. Multi-criteria Scheduling of Pipeline Workflows. In HeteroPar'07, Algorithms, Models and Tools for Parallel Computing on Heterogeneous Networks (in conjunction with Cluster 2007). IEEE Computer Society Press,\n2007.\n[3] P. Bhat, C. Raghavendra, and V. Prasanna. Efficient collective communication in distributed heterogeneous systems. Journal of Parallel and Distributed Computing, 63:251\u2013\n263, 2003.\n[4] C. Christopoulos, A. Skodras, and T. Ebrahimi. The JPEG2000 still image coding\nsystem: an overview. IEEE Transactions on Consumer Electronics, 46(4):1103\u20131127,\n2000.\n[5] J. Falkemeier and G. Joubert. Parallel image compression with jpeg for multimedisa\napplications. In High Performance Computing: Technologies, Methods and Applications,\nAdvances in Parallel Computing, pages 379\u2013394. North Holland, 1995.\n[6] M. Ferretti and M. Boffadossi. A Parallel Pipelined Implementation of LOCO-I for JPEGLS. In 17th International Conference on Pattern Recognition (ICPR'04), volume 1, pages\n769\u2013772, 2004.\n[7] T. Kumaki, M. Ishizaki, T. Koide, H. J. Mattausch, Y. Kuroda, H. Noda, K. Dosaka,\nK. Arimoto, and K. Saito. Acceleration of DCT Processing with Massive-Parallel\nMemory-Embedded SIMD Matrix Processor. IEICE Transactions on Information and\nSystems - LETTER- Image Processing and Video Processing, E90-D(8):1312\u20131315, 2007.\n[8] P. Meerwald, R. Norcen, and A. Uhl. Parallel JPEG2000 Image Coding on Multiprocessors. In IPDPS'02, International Parallel and Distributed Processing Symposium. IEEE\nComputer Society Press, 2002.\n[9] M. Papadonikolakis, V. Pantazis, and A. P. Kakarountas. Efficient high-performance\nASIC implementation of JPEG-LS encoder. In Proceedings of the Conference on Design,\nAutomation and Test in Europe (DATE2007), volume IEEE Communications Society\nPress, 2007.\n[10] S. L. Shee, A. Erdos, and S. Parameswaran. Architectural Exploration of Heterogeneous\nMultiprocessor Systems for JPEG. International Journal of Parallel Programming, 35,\n2007.\n[11] K. Shen, G. Cook, L. Jamieson, and E. Delp. An overview of parallel processing approaches to image and video compression. In Image and Video Compression, volume\nProc. SPIE 2186, pages 197\u2013208, 1994.\n[12] J. Subhlok and G. Vondran. Optimal latency-throughput tradeoffs for data parallel\npipelines. In ACM Symposium on Parallel Algorithms and Architectures SPAA'96, pages\n62\u201371. ACM Press, 1996.\n[13] G. K. Wallace. The JPEG still picture compression standard. Commun. ACM, 34(4):30\u2013\n44, 1991.\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n13\n\n[14] C. Wen-Hsiung, C. Smith, and S. Fralick. A Fast Computational Algorithm for the\nDiscrete Cosine Tranfsorm. IEEE Transactions on Communications, 25(9):1004\u20131009,\n1977.\n\nRR n\u00b0 6410\n\n\fUnit\u00e9 de recherche INRIA Rh\u00f4ne-Alpes\n655, avenue de l'Europe - 38334 Montbonnot Saint-Ismier (France)\nUnit\u00e9 de recherche INRIA Futurs : Parc Club Orsay Universit\u00e9 - ZAC des Vignes\n4, rue Jacques Monod - 91893 ORSAY Cedex (France)\nUnit\u00e9 de recherche INRIA Lorraine : LORIA, Technop\u00f4le de Nancy-Brabois - Campus scientifique\n615, rue du Jardin Botanique - BP 101 - 54602 Villers-l\u00e8s-Nancy Cedex (France)\nUnit\u00e9 de recherche INRIA Rennes : IRISA, Campus universitaire de Beaulieu - 35042 Rennes Cedex (France)\nUnit\u00e9 de recherche INRIA Rocquencourt : Domaine de Voluceau - Rocquencourt - BP 105 - 78153 Le Chesnay Cedex (France)\nUnit\u00e9 de recherche INRIA Sophia Antipolis : 2004, route des Lucioles - BP 93 - 06902 Sophia Antipolis Cedex (France)\n\n\u00c9diteur\nINRIA - Domaine de Voluceau - Rocquencourt, BP 105 - 78153 Le Chesnay Cedex (France)\n\nhttp://www.inria.fr\nISSN 0249-6399\n\n\f\f\fINSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE\n\nBi-criteria Pipeline Mappings\nfor Parallel Image Processing\nAnne Benoit - Harald Kosch - Veronika Rehn-Sonigo - Yves Robert\n\nN\u00b0 ????\nJanuary 2008\n\nISSN 0249-6399\n\napport\nde recherche\n\nISRN INRIA/RR--????--FR+ENG\n\nTh\u00e8me NUM\n\n\f\fBi-criteria Pipeline Mappings\nfor Parallel Image Processing\nAnne Benoit , Harald Kosch , Veronika Rehn-Sonigo , Yves Robert\nTh\u00e8me NUM - Syst\u00e8mes num\u00e9riques\nProjet GRAAL\nRapport de recherche n\u00b0 ???? - January 2008 - 13 pages\n\nAbstract: Mapping workflow applications onto parallel platforms is a challenging problem,\neven for simple application patterns such as pipeline graphs. Several antagonistic criteria\nshould be optimized, such as throughput and latency (or a combination). Typical applications\ninclude digital image processing, where images are processed in steady-state mode.\nIn this paper, we study the mapping of a particular image processing application, the\nJPEG encoding. Mapping pipelined JPEG encoding onto parallel platforms is useful for\ninstance for encoding Motion JPEG images. As the bi-criteria mapping problem is NPcomplete, we concentrate on the evaluation and performance of polynomial heuristics.\nKey-words: pipeline, workflow application, multi-criteria optimization, JPEG encoding\n\nThis text is also available as a research report of the Laboratoire de l'Informatique du Parall\u00e9lisme\nhttp://www.ens-lyon.fr/LIP.\n\nUnit\u00e9 de recherche INRIA Rh\u00f4ne-Alpes\n655, avenue de l'Europe, 38334 Montbonnot Saint Ismier (France)\nT\u00e9l\u00e9phone : +33 4 76 61 52 00 - T\u00e9l\u00e9copie +33 4 76 61 52 52\n\n\fMapping bi-cit\u00e8re de pipelines pour le traitement d'image en\nparall\u00e8le\nR\u00e9sum\u00e9 : L'ordonnancement et l'allocation des workflows sur plates-formes parall\u00e8les est\nun probl\u00e8me crucial, m\u00eame pour des applications simples comme des graphes en pipeline.\nPlusieurs crit\u00e8res contradictoires doivent \u00eatre optimis\u00e9s, tels que le d\u00e9bit et la latence (ou une\ncombinaison des deux). Des applications typiques incluent le traitement d'images num\u00e9riques,\no\u00f9 les images sont trait\u00e9es en r\u00e9gime permanent.\nDans ce rapport, nous \u00e9tudions l'ordonnancement et l'allocation d'une application de\ntraitement d'image particuli\u00e8re, l'encodage JPEG. L'allocation de l'encodage JPEG pipelin\u00e9\nsur des plates-formes parall\u00e8les est par exemple utile pour l'encodage des images Motion\nJPEG. Comme le probl\u00e8me de l'allocation bi-crit\u00e8re est NP-complet, nous nous concentrons\nsur l'analyse et \u00e9valuation d'heuristiques polynomiales.\nMots-cl\u00e9s : pipeline, application workflow, optimisation multi-crit\u00e8re, encodage JPEG\n\n\fBi-criteria Pipeline Mappings\n\n1\n\n3\n\nIntroduction\n\nThis work considers the problem of mapping workflow applications onto parallel platforms.\nThis is a challenging problem, even for simple application patterns. For homogeneous architectures, several scheduling and load-balancing techniques have been developed but the\nextension to heterogeneous clusters makes the problem more difficult.\nStructured programming approaches rule out many of the problems which the low-level\nparallel application developer is usually confronted to, such as deadlocks or process starvation.\nWe therefore focus on pipeline applications, as they can easily be expressed as algorithmic\nskeletons. More precisely, in this paper, we study the mapping of a particular pipeline application: we focus on the JPEG encoder (baseline process, basic mode). This image processing\napplication transforms numerical pictures from any format into a standardized format called\nJPEG. This standard was developed almost 20 years ago to create a portable format for the\ncompression of still images and new versions are created until now (see http://www.jpeg.org/).\nJPEG (and later JPEG 2000) is used for encoding still images in Motion-JPEG (later MJ2).\nThese standards are commonly employed in IP-cams and are part of many video applications in the world of game consoles. Motion-JPEG (M-JPEG) has been adopted and further\ndeveloped to several other formats, e.g., AMV (alternatively known as MTV) which is a proprietary video file format designed to be consumed on low-resource devices. The manner of\nencoding in M-JPEG and subsequent formats leads to a flow of still image coding, hence\npipeline mapping is appropriate.\nWe consider the different steps of the encoder as a linear pipeline of stages, where each\nstage gets some input, has to perform several computations and transfers the output to the\nnext stage. The corresponding mapping problem can be stated informally as follows: which\nstage to assign to which processor? We require the mapping to be interval-based, i.e., a\nprocessor is assigned an interval of consecutive stages. Two key optimization parameters\nemerge. On the one hand, we target a high throughput, or short period, in order to be able\nto handle as many images as possible per time unit. On the other hand, we aim at a short\nresponse time, or latency, for the processing of each image. These two criteria are antagonistic:\nintuitively, we obtain a high throughput with many processors to share the work, while we\nget a small latency by mapping many stages to the same processor in order to avoid the cost\nof inter-stage communications.\nThe rest of the paper is organized as follows: Section 2 briefly describes JPEG coding\nprinciples. In Section 3 the theoretical and applicative framework is introduced, and Section 4\nis dedicated to linear programming formulation of the bi-criteria mapping. In Section 5 we\ndescribe some polynomial heuristics, which we use for our experiments of Section 6. We\ndiscuss related work in Section 7. Finally, we give some concluding remarks in Section 8.\n\n2\n\nPrinciples of JPEG encoding\n\nHere we briefly present the mode of operation of a JPEG encoder (see [13] for further details).\nThe encoder consists in seven pipeline stages, as shown in Figure 1. In the first stage, the\nimage is scaled to have a multiple of an 8x8 pixel matrix, and the standard even claims a\nmultiple of 16x16. In the next stage a color space conversion is performed: the colors of the\npicture are transformed from the RGB to the YUV-color model. The sub-sampling stage is\nan optional stage, which, depending on the sampling rate, reduces the data volume: as the\n\nRR n\u00b0 0123456789\n\n\f4\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\nSource\nImage Data\n\nScaling\n\nYUV\nConversion\n\nBlock\nStorage\n\nSubsampling\n\nFDCT\n\nQuantizer\n\nEntropy\nEncoder\n\nQuantization\nTable\n\nHuffman\nTable\n\nCompressed\nImage Data\n\nFigure 1: Steps of the JPEG encoding.\nhuman eye can dissolve luminosity more easily than color, the chrominance components are\nsampled more rarely than the luminance components. Admittedly, this leads to a loss of data.\nThe last preparation step consists in the creation and storage of so-called MCUs (Minimum\nCoded Units), which correspond to 8x8 pixel blocks in the picture. The next stage is the\ncore of the encoder. It performs a Fast Discrete Cosine Transformation (FDCT) (eg. [14])\non the 8x8 pixel blocks which are interpreted as a discrete signal of 64 values. After the\ntransformation, every point in the matrix is represented as a linear combination of the 64\npoints. The quantizer reduces the image information to the important parts. Depending on\nthe quantization factor and quantization matrix, irrelevant frequencies are reduced. Thereby\nquantization errors can occur, that are remarkable as quantization noise or block generation\nin the encoded image. The last stage is the entropy encoder, which performs a modified\nHuffman coding: it combines the variable length codes of Huffman coding with the coding of\nrepetitive data in run-length encoding.\n\n3\n3.1\n\nFramework\nApplicative framework\n\nOn the theoretical point of view, we consider a pipeline of n stages Sk , 1 \u2264 k \u2264 n. Tasks\nare fed into the pipeline and processed from stage to stage, until they exit the pipeline after\nthe last stage. The k-th stage Sk first receives an input from the previous stage, of size \u03b4 k\u22121 ,\nthen performs a number of wk computations, and finally outputs data of size \u03b4 k to the next\nstage. These three operations are performed sequentially. The first stage S1 receives an input\nof size \u03b4 0 from the outside world, while the last stage Sn returns the result, of size \u03b4 n , to the\noutside world, thus these particular stages behave in the same way as the others.\nOn the practical point of view, we consider the applicative pipeline of the JPEG encoder\nas presented in Figure 1 and its seven stages.\n\n3.2\n\nTarget platform\n\nWe target a platform with p processors Pu , 1 \u2264 u \u2264 p, fully interconnected as a (virtual)\nclique. There is a bidirectional link linku,v : Pu \u2192 Pv between any processor pair Pu and\nPv , of bandwidth bu,v . The speed of processor Pu is denoted as su , and it takes X/su timeunits for Pu to execute X floating point operations. We also enforce a linear cost model for\ncommunications, hence it takes X/b time-units to send (resp. receive) a message of size X\nto (resp. from) Pv . Communications contention is taken care of by enforcing the one-port\nmodel [3].\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n3.3\n\n5\n\nBi-criteria interval mapping problem\n\nWe seek to map intervals of consecutive stages onto processors [12]. Intuitively, assigning\nseveral consecutive tasks to the same processor will increase their computational load, but\nmay well dramatically decrease communication requirements. We search for a partition of\n[1..n] into m \u2264 p intervals Ij = [dj , ej ] such that dj \u2264 ej for 1 \u2264 j \u2264 m, d1 = 1, dj+1 = ej + 1\nfor 1 \u2264 j \u2264 m \u2212 1 and em = n.\nThe optimization problem is to determine the best mapping, over all possible partitions\ninto intervals, and over all processor assignments. The objective can be to minimize either\nthe period, or the latency, or a combination: given a threshold period, what is the minimum\nlatency that can be achieved? and the counterpart: given a threshold latency, what is the\nminimum period that can be achieved?\nThe decision problem associated to this bi-criteria interval mapping optimization problem\nis NP-hard, since the period minimization problem is NP-hard for interval-based mappings\n(see [2]).\n\n4\n\nLinear program formulation\n\nWe present here an integer linear program to compute the optimal interval-based bi-criteria\nmapping on Fully Heterogeneous platforms, respecting either a fixed latency or a fixed period.\nWe assume n stages and p processors, plus two fictitious extra stages S0 and Sn+1 respectively\nassigned to Pin and Pout . First we need to define a few variables:\nFor k \u2208 [0..n + 1] and u \u2208 [1..p] \u222a {in, out}, xk,u is a boolean variable equal to 1 if stage Sk is\nassigned to processor Pu ; we let x0,in = xn+1,out = 1, and xk,in = xk,out = 0 for 1 \u2264 k \u2264 n.\nFor k \u2208 [0..n], u, v \u2208 [1..p] \u222a {in, out} with u 6= v, zk,u,v is a boolean variable equal to 1 if\nstage Sk is assigned to Pu and stage Sk+1 is assigned to Pv : hence linku,v : Pu \u2192 Pv is used\nfor the communication between these two stages. If k 6= 0 then zk,in,v = 0 for all v 6= in and\nif k 6= n then zk,u,out = 0 for all u 6= out.\nFor k \u2208 [0..n] and u \u2208 [1..p] \u222a {in, out}, yk,u is a boolean variable equal to 1 if stages Sk and\nSk+1 are both assigned to Pu ; we let yk,in = yk,out = 0 for all k, and y0,u = yn,u = 0 for all u.\nFor u \u2208 [1..p], first(u) is an integer variable which denotes the first stage assigned to Pu ;\nsimilarly, last(u) denotes the last stage assigned to Pu . Thus Pu is assigned the interval\n[first(u), last(u)]. Of course 1 \u2264 first(u) \u2264 last(u) \u2264 n.\nTopt is the variable to optimize, so depending on the objective function it corresponds either\nto the period or to the latency.\nP\nWe list below the constraints that need to be enforced. For simplicity, we write u\nP\ninstead of u\u2208[1..p]\u222a{in,out} when summing over all processors. First there are constraints for\nprocessor and link usage:\nP\nEvery stage is assigned a processor: \u2200k \u2208 [0..n + 1],\nu xk,u = 1.\nEvery communication either is assigned a link or collapses because both stages are assigned\nto the same processor:\n\u2200k \u2208 [0..n],\n\nX\n\nzk,u,v +\n\nu6=v\n\nX\n\nyk,u = 1\n\nu\n\nIf stage Sk is assigned to Pu and stage Sk+1 to Pv , then linku,v : Pu \u2192 Pv is used for this\ncommunication:\n\u2200k \u2208 [0..n], \u2200u, v \u2208 [1..p] \u222a {in, out}, u 6= v,\n\nRR n\u00b0 0123456789\n\nxk,u + xk+1,v \u2264 1 + zk,u,v\n\n\f6\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\nIf both stages Sk and Sk+1 are assigned to Pu , then yk,u = 1:\n\u2200k \u2208 [0..n], \u2200u \u2208 [1..p] \u222a {in, out},\n\nxk,u + xk+1,u \u2264 1 + yk,u\n\nIf stage Sk is assigned to Pu , then necessarily firstu \u2264 k \u2264 lastu . We write this constraint as:\n\u2200k \u2208 [1..n], \u2200u \u2208 [1..p],\n\nfirstu \u2264 k.xk,u + n.(1 \u2212 xk,u )\n\n\u2200k \u2208 [1..n], \u2200u \u2208 [1..p],\n\nlastu \u2265 k.xk,u\n\nIf stage Sk is assigned to Pu and stage Sk+1 is assigned to Pv 6= Pu (i.e., zk,u,v = 1) then\nnecessarily lastu \u2264 k and firstv \u2265 k + 1 since we consider intervals. We write this constraint\nas:\n\u2200k \u2208 [1..n \u2212 1], \u2200u, v \u2208 [1..p], u 6= v,\nlastu \u2264 k.zk,u,v + n.(1 \u2212 zk,u,v )\n\u2200k \u2208 [1..n \u2212 1], \u2200u, v \u2208 [1..p], u 6= v,\n\nfirstv \u2265 (k + 1).zk,u,v\n\nThe latency of schedule is bounded by Tlatency :\nand t \u2208 [1..p] \u222a {in, out}.\np X\nn\nX\n\nu=1 k=1\n\n\uf8ee\uf8eb\n\uf8f0\uf8ed\n\nX \u03b4 k\u22121\n\nt6=u\n\nbt,u\n\n\uf8f6\n\nzk\u22121,t,u \uf8f8 +\n\n\uf8f9\n\n\uf8eb\n\nwk\n\uf8ec\nxk,u \uf8fb + \uf8ed\nsu\n\nX\n\nu\u2208[1..p]\u222a{in}\n\n\u03b4n\nbu,out\n\n\uf8f6\n\nzn,u,out \uf8f8 \u2264 Tlatency\n\uf8f7\n\nand t \u2208 [1..p] \u222a {in, out}.\nThere remains to express the period of each processor and to constrain it by Tperiod :\n\u2200u \u2208 [1..p],\n\uf8f1\uf8eb\n\uf8f6\n\uf8eb\n\uf8f6\uf8fc\nn \uf8f2 X\n\uf8fd\nX\nX\n\u03b4\nw\n\u03b4\nk\u22121\nk\nk\n\uf8ed\nzk\u22121,t,u \uf8f8 +\nxk,u + \uf8ed\nzk,u,v \uf8f8 \u2264 Tperiod\n\uf8f3\n\uf8fe\nbt,u\nsu\nbu,v\n\nk=1\n\nt6=u\n\nv6=u\n\nFinally, the objective function is either to minimize the period Tperiod respecting the fixed\nlatency Tlatency or to minimize the latency Tlatency with a fixed period Tperiod . So in the\nfirst case we fix Tlatency and set Topt = Tperiod . In the second case Tperiod is fixed a priori\nand Topt = Tlatency . With this mechanism the objective function reduces to minimizing Topt\nin both cases.\n\n5\n\nOverview of the heuristics\n\nThe problem of bi-criteria interval mapping of workflow applications is NP-hard [2], so in\nthis section we briefly describe polynomial heuristics to solve it. See [2] for a more complete\ndescription or refer to the Web at:\nhttp://graal.ens-lyon.fr/~vsonigo/code/multicriteria/\nIn the following, we denote by n the number of stages, and by p the number of processors.\nWe distinguish two sets of heuristics. The heuristics of the first set aim to minimize the\nlatency respecting an a priori fixed period. The heuristics of the second set minimize the\ncounterpart: the latency is fixed a priori and we try to achieve a minimum period while\nrespecting the latency constraint.\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n5.1\n\n7\n\nMinimizing Latency for a Fixed Period\n\nAll the following heuristics sort processors by non-increasing speed, and start by assigning all\nthe stages to the first (fastest) processor in the list. This processor becomes used.\nH1-Sp-mono-P: Splitting mono-criterion \u2013 At each step, we select the used processor j\nwith the largest period and we try to split its stage interval, giving some stages to the next\nfastest processor j \u2032 in the list (not yet used). This can be done by splitting the interval at\nany place, and either placing the first part of the interval on j and the remainder on j \u2032 , or\nthe other way round. The solution which minimizes max(period(j), period(j \u2032 )) is chosen if it\nis better than the original solution. Splitting is performed as long as we have not reached the\nfixed period or until we cannot improve the period anymore.\nH2-Sp-bi-P: Splitting bi-criteria \u2013 This heuristic uses a binary search over the latency.\nFor this purpose at each iteration we fix an authorized increase of the optimal latency (which\nis obtained by mapping all stages on the fastest processor), and we test if we get a feasible\nsolution via splitting. The splitting mechanism itself is quite similar to H1-Sp-mono-P\n\u2206latency\n) within the authorized\nexcept that we choose the solution that minimizes maxi\u2208{j,j \u2032 } ( \u2206period(j)\nlatency increase to decide where to split. While we get a feasible solution, we reduce the\nauthorized latency increase for the next iteration of the binary search, thereby aiming at\nminimizing the mapping global latency.\nH3-3-Sp-mono-P: 3-splitting mono-criterion \u2013 At each step we select the used processor j with the largest period and we split its interval into three parts. For this purpose we try to\nmap two parts of the interval on the next pair of fastest processors in the list, j \u2032 and j \u2032\u2032 , and to\nkeep the third part on processor j. Testing all possible permutations and all possible positions\nwhere to cut, we choose the solution that minimizes max(period(j), period(j \u2032 ), period(j \u2032\u2032 )).\nH4-3-Sp-bi-P: 3-splitting bi-criteria \u2013 In this heuristic the choice of where to split is\nmore elaborated: it depends not only of the period improvement, but also of the latency\nincrease. Using the same splitting mechanism as in H3-3-Sp-mono-P, we select the solu\u2206latency\n). Here \u2206latency denotes the difference between\ntion that minimizes maxi\u2208{j,j \u2032 ,j \u2032\u2032 } ( \u2206period(i)\nthe global latency of the solution before the split and after the split. In the same manner\n\u2206period(i) defines the difference between the period before the split (achieved by processor\nj) and the new period of processor i.\n\n5.2\n\nMinimizing Period for a Fixed Latency\n\nAs in the heuristics described above, first of all we sort processors according to their speed\nand map all stages on the fastest processor.\nH5-Sp-mono-L: Splitting mono-criterion \u2013 This heuristic uses the same method as H1Sp-mono-P with a different break condition. Here splitting is performed as long as we do not\nexceed the fixed latency, still choosing the solution that minimizes max(period(j), period(j \u2032 )).\n\nRR n\u00b0 0123456789\n\n\f8\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\nPf ix = 310\n1\n\nLopt = 337, 575\n2\n\n3\n\n4\n\n5\n\nP6\n\n6\n\n2\n\n3\n\n4\n\n5\n\n2\n\nP5\n\nLopt = 336, 729\n\nP6\n\n6\n\n1\n\n3\n\n4\n\n5\n\n4\n\n1\n\n1\n\n7\n\nPopt = 307, 319\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nP3\n\nLf ix = 330\n\n7\n\n6\nP3\n\nP4\n\n6\n\n5\n\nLf ix = 340\n\n7\n\nLopt = 322, 700\n2\n\n3\nP7\n\nP3\n\nPf ix = 330\n\nPopt = 307, 319\n\n1\n\n7\n\nP3\n\nPf ix = 320\n1\n\nLf ix = 370\n\nPopt = 322, 700\n2\n\n3\n\n4\n\nP3\n\nP3\n\na)\n\nb)\n\n5\n\n6\n\n7\n\nFigure 2: LP solutions strongly depend on fixed initial parameters.\nH6-Sp-bi-L: Splitting bi-criteria \u2013 This variant of the splitting heuristic works similarly\n\u2206latency\nto H5-Sp-mono-L, but at each step it chooses the solution which minimizes maxi\u2208{j,j \u2032 } ( \u2206period(i)\n)\nwhile the fixed latency is not exceeded.\nRemark In the context of M-JPEG coding, minimizing the latency for a fixed period corresponds to a fixed coding rate, and we want to minimize the response time. The counterpart\n(minimizing the period respecting a fixed latency L) corresponds to the question: if I accept\nto wait L time units for a given image, which coding rate can I achieve? We evaluate the\nbehavior of the heuristics with respect to these questions in Section 6.2.\n\n6\n\nExperiments and simulations\n\nIn the following experiments, we study the mapping of the JPEG application onto clusters of\nworkstations.\n\n6.1\n\nInfluence of fixed parameters\n\nIn this first test series, we examine the influence of fixed parameters on the solution of the\nlinear program. As shown in Figure 2, the division into intervals is highly dependant of the\nchosen fixed value. The optimal solution to minimize the latency (without any supplemental\nconstraints) obviously consists in mapping the whole application pipeline onto the fastest\nprocessor. As expected, if the period fixed in the linear program is not smaller than the\nlatter optimal mono-criterion latency, this solution is chosen. Decreasing the value for the\nfixed period imposes to split the stages among several processors, until no more solution can\nbe found. Figure 2(a) shows the division into intervals for a fixed period. A fixed period\nof Tperiod = 330 is sufficiently high for the whole pipeline to be mapped onto the fastest\nprocessor, whereas smaller periods lead to splitting into intervals. We would like to mention,\nthat for a period fixed to 300, there exists no solution anymore. The counterpart - fixed\nlatency - can be found in Figure 2(b). Note that the first two solutions find the same period,\nbut for a different latency. The first solution has a high value for latency, which allows more\nsplits, hence larger communication costs. Comparing the last lines of Figures 2(a) and (b), we\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n9\n\n350\n\n330\nL_fixed\n325\n\n340\nOptimal Period\n\nOptimal Latency\n\nP_fixed\n\n330\n320\n310\n300\n300\n\n320\n315\n310\n305\n\n305\n\n310 315 320\nFixed Period\n\n325\n\n330\n\n(a) Fixed P.\n\n300\n320 330 340 350 360 370 380 390 400\nFixed Latency\n\n(b) Fixed L.\n\nFigure 3: Bucket behavior of LP solutions.\nstate that both solutions are the same, and we have Tperiod = Tlatency . Finally, expanding\nthe range of the fixed values, a sort of bucket behavior becomes apparent: Increasing the\nfixed parameter has in a first time no influence, the LP still finds the same solution until the\nincrease crosses an unknown bound and the LP can find a better solution. This phenomenon\nis shown in Figure 3.\n\n6.2\n\nAssessing heuristic performance\n\nThe comparison of the solution returned by the LP program, in terms of optimal latency\nrespecting a fixed period (or the converse) with the heuristics is shown in Figure 4. The\nimplementation is fed with the parameters of the JPEG encoding pipeline and computes the\nmapping on 10 randomly created platforms with 10 processors. On platforms 3 and 5, no\nvalid solution can be found for the fixed period. There are two important points to mention.\nFirst, the solutions found by H2 often are not valid, since they do not respect the fixed period,\nbut they have the best ratio latency/period. Figure 5(b) plots some more details: H2 achieves\ngood latency results, but the fixed period of P=310 is often violated. This is a consequence of\nthe fact that the fixed period value is very close to the feasible period. When the tolerance for\nthe period is bigger, this heuristic succeeds to find low-latency solutions. Second, all solutions,\nLP and heuristics, always keep the stages 4 to 7 together (see Figure 2 for an example). As\nstage 5 (DCT) is the most costly in terms of computation, the interval containing these stages\nis responsible for the period of the whole application.\nFinally, in the comparative study H1 always finds the optimal period for a fixed latency\nand we therefore recommend this heuristic for period optimization. In the case of latency\nminimization for a fixed period, then H5 is to use, as it always finds the LP solution in the\nexperiments. This is a striking result, especially given the fact that the LP integer program\nmay require a long time to compute the solution (up to 11389 seconds in our experiments),\nwhile the heuristics always complete in less than a second, and find the corresponding optimal\nsolution.\n\n6.3\n\nMPI simulations on a cluster\n\nThis last experiment performs a JPEG encoding simulation. All simulations are made on a\ncluster of homogeneous Optiplex GX 745 machines with an Intel Core 2 Duo 6300 of 1,83Ghz.\nHeterogeneity is enforced by increasing and decreasing the number of operations a processor\nhas to execute. The same holds for bandwidth capacities. We call this experiment simulation,\n\nRR n\u00b0 0123456789\n\n\f10\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\n400\n\n360\nLP fixed P\nH1\nH2\nH3\nH4\n\n360\n\nLP fixed L\nH5\nH6\n\n350\nheuristical solution\n\nheuristical solution\n\n380\n\n340\n320\n300\n\n340\n330\n320\n310\n300\n290\n280\n\n280\n\n270\n0\n\n1\n\n2\n\n3\n4\n5\n6\nrandom platform\n\n7\n\n8\n\n9\n\n0\n\n(a) Fixed P = 310.\n\n1\n\n2\n\n3\n4\n5\n6\nrandom platform\n\n7\n\n8\n\n9\n\n(b) Fixed L = 370.\n\nFigure 4: Behavior of the heuristics (comparing to LP solution).\n350\ntheoretical\nsimulation\n\n600\n500\n\n330\nlatency\n\nlatency\n\nLP\nH2\n\n340\n\n400\n300\n\n320\n310\n300\n\n200\n\n290\n\n100\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n280\n280\n\n290\n\n300\n\nheuristic\n\n(a) Simulative latency.\n\n310\n\n320\n\n330\n\n340\n\n350\n\nperiod\n\n(b) H2 versus LP.\n\nFigure 5: MPI simulation results.\nas we do not parallelize a real JPEG encoder, but we use a parallel pipeline application which\nhas the same parameters for communication and computation as the JPEG encoder. An\nmpich implementation of MPI is used for communications.\nIn this experiment the same random platforms with 10 processors and fixed parameters\nas in the theoretical experiments are used. We measured the latency of the simulation, even\nfor the heuristics of fixed latency, and computed the average over all random platforms.\nFigure 5(a) compares the average of the theoretical results of the heuristics to the average\nsimulative performance. The simulative behavior nicely mirrors the theoretical behavior, with\nthe exception of H2 (see Figure 5(b)). Here once again, some solutions of this heuristic are\nnot valid, as they do not respect the fixed period.\n\n7\n\nRelated work\n\nThe blockwise independent processing of the JPEG encoder allows to apply simple data parallelism for efficient parallelization. Many papers have addressed this fine-grain parallelization\nopportunity [5, 11]. In addition, parallelization of almost all stages, from color space conversion, over DCT to the Huffman encoding has been addressed [1, 7]. Recently, with respect\nto the JPEG2000 codec, efficient parallelization of wavelet coding has been introduced [8].\nAll these works target the best speed-up with respect to different architectures and possible\nvarying load situations. Optimizing the period and the latency is an important issue when\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n11\n\nencoding a pipeline of multiple images, as for instance for Motion JPEG (M-JPEG). To meet\nthese issues, one has to solve in addition to the above mentioned work a bi-criteria optimization problem, i.e., optimize the latency, as well as the period. The application of coarse grain\nparallelism seems to be a promising solution. We propose to use an interval-based mapping\nstrategy allowing multiple stages to be mapped to one processor which allows meeting the\nmost flexible the domain constraints (even for very large pictures). Several pipelined versions\nof the JPEG encoding have been considered. They rely mainly on pixel or block-wise parallelization [6, 9]. For instance, Ferretti et al. [6] uses three pipelines to carry out concurrently\nthe encoding on independent pixels extracted from the serial stream of incoming data. The\npixel and block-based approach is however useful for small pictures only. Recently, Sheel et\nal. [10] consider a pipeline architecture where each stage presents a step in the JPEG encoding. The targeted architecture consists of Xtensa LX processors which run subprograms\nof the JPEG encoder program. Each program accepts data via the queues of the processor,\nperforms the necessary computation, and finally pushes it to the output queue into the next\nstage of the pipeline. The basic assumptions are similar to our work, however no optimization\nproblem is considered and only runtime (latency) measurements are available. The schedule\nis static and set according to basic assumptions about the image processing, e.g., that the\nDCT is the most complex operation in runtime.\n\n8\n\nConclusion\n\nIn this paper, we have studied the bi-criteria (minimizing latency and period) mapping of\npipeline workflow applications, from both a theoretical and practical point of view. On the\ntheoretical side, we have presented an integer linear programming formulation for this NPhard problem. On the practical side, we have studied in depth the interval mapping of the\nJPEG encoding pipeline on a cluster of workstations. Owing to the LP solution, we were able\nto characterize a bucket behavior in the optimal solution, depending on the initial parameters.\nFurthermore, we have compared the behavior of some polynomial heuristics to the LP solution\nand we were able to recommended two heuristics with almost optimal behavior for parallel\nJPEG encoding. Finally, we evaluated the heuristics running a parallel pipeline application\nwith the same parameters as a JPEG encoder. The heuristics were designed for general\npipeline applications, and some of them were aiming at applications with a large number of\nstages (3-splitting), thus a priori not very efficient on the JPEG encoder. Still, some of these\nheuristics reach the optimal solution in our experiments, which is a striking result.\nA natural extension of this work would be to consider further image processing applications\nwith more pipeline stages or a slightly more complicated pipeline architecture. Naturally, our\nwork extends to JPEG 2000 encoding which offers among others wavelet coding and more\ncomplex multiple-component image encoding [4]. Another extension is for the MPEG coding\nfamily which uses lagged feedback: the coding of some types of frames depends on other\nframes. Differentiating the types of coding algorithms, a pipeline architecture seems again to\nbe a promising solution architecture.\n\nReferences\n[1] L. V. Agostini, I. S. Silva, and S. Bampi. Parallel color space converters for JPEG image\ncompression. Microelectronics Reliability, 44(4):697\u2013703, 2004.\n\nRR n\u00b0 0123456789\n\n\f12\n\nA. Benoit , H. Kosch , V. Rehn-Sonigo , Y. Robert\n\n[2] A. Benoit, V. Rehn-Sonigo, and Y. Robert. Multi-criteria Scheduling of Pipeline Workflows. In HeteroPar'07, Algorithms, Models and Tools for Parallel Computing on Heterogeneous Networks (in conjunction with Cluster 2007). IEEE Computer Society Press,\n2007.\n[3] P. Bhat, C. Raghavendra, and V. Prasanna. Efficient collective communication in distributed heterogeneous systems. Journal of Parallel and Distributed Computing, 63:251\u2013\n263, 2003.\n[4] C. Christopoulos, A. Skodras, and T. Ebrahimi. The JPEG2000 still image coding\nsystem: an overview. IEEE Transactions on Consumer Electronics, 46(4):1103\u20131127,\n2000.\n[5] J. Falkemeier and G. Joubert. Parallel image compression with jpeg for multimedisa\napplications. In High Performance Computing: Technologies, Methods and Applications,\nAdvances in Parallel Computing, pages 379\u2013394. North Holland, 1995.\n[6] M. Ferretti and M. Boffadossi. A Parallel Pipelined Implementation of LOCO-I for JPEGLS. In 17th International Conference on Pattern Recognition (ICPR'04), volume 1, pages\n769\u2013772, 2004.\n[7] T. Kumaki, M. Ishizaki, T. Koide, H. J. Mattausch, Y. Kuroda, H. Noda, K. Dosaka,\nK. Arimoto, and K. Saito. Acceleration of DCT Processing with Massive-Parallel\nMemory-Embedded SIMD Matrix Processor. IEICE Transactions on Information and\nSystems - LETTER- Image Processing and Video Processing, E90-D(8):1312\u20131315, 2007.\n[8] P. Meerwald, R. Norcen, and A. Uhl. Parallel JPEG2000 Image Coding on Multiprocessors. In IPDPS'02, International Parallel and Distributed Processing Symposium. IEEE\nComputer Society Press, 2002.\n[9] M. Papadonikolakis, V. Pantazis, and A. P. Kakarountas. Efficient high-performance\nASIC implementation of JPEG-LS encoder. In Proceedings of the Conference on Design,\nAutomation and Test in Europe (DATE2007), volume IEEE Communications Society\nPress, 2007.\n[10] S. L. Shee, A. Erdos, and S. Parameswaran. Architectural Exploration of Heterogeneous\nMultiprocessor Systems for JPEG. International Journal of Parallel Programming, 35,\n2007.\n[11] K. Shen, G. Cook, L. Jamieson, and E. Delp. An overview of parallel processing approaches to image and video compression. In Image and Video Compression, volume\nProc. SPIE 2186, pages 197\u2013208, 1994.\n[12] J. Subhlok and G. Vondran. Optimal latency-throughput tradeoffs for data parallel\npipelines. In ACM Symposium on Parallel Algorithms and Architectures SPAA'96, pages\n62\u201371. ACM Press, 1996.\n[13] G. K. Wallace. The JPEG still picture compression standard. Commun. ACM, 34(4):30\u2013\n44, 1991.\n\nINRIA\n\n\fBi-criteria Pipeline Mappings\n\n13\n\n[14] C. Wen-Hsiung, C. Smith, and S. Fralick. A Fast Computational Algorithm for the\nDiscrete Cosine Tranfsorm. IEEE Transactions on Communications, 25(9):1004\u20131009,\n1977.\n\nRR n\u00b0 0123456789\n\n\fUnit\u00e9 de recherche INRIA Rh\u00f4ne-Alpes\n655, avenue de l'Europe - 38334 Montbonnot Saint-Ismier (France)\nUnit\u00e9 de recherche INRIA Futurs : Parc Club Orsay Universit\u00e9 - ZAC des Vignes\n4, rue Jacques Monod - 91893 ORSAY Cedex (France)\nUnit\u00e9 de recherche INRIA Lorraine : LORIA, Technop\u00f4le de Nancy-Brabois - Campus scientifique\n615, rue du Jardin Botanique - BP 101 - 54602 Villers-l\u00e8s-Nancy Cedex (France)\nUnit\u00e9 de recherche INRIA Rennes : IRISA, Campus universitaire de Beaulieu - 35042 Rennes Cedex (France)\nUnit\u00e9 de recherche INRIA Rocquencourt : Domaine de Voluceau - Rocquencourt - BP 105 - 78153 Le Chesnay Cedex (France)\nUnit\u00e9 de recherche INRIA Sophia Antipolis : 2004, route des Lucioles - BP 93 - 06902 Sophia Antipolis Cedex (France)\n\n\u00c9diteur\nINRIA - Domaine de Voluceau - Rocquencourt, BP 105 - 78153 Le Chesnay Cedex (France)\n\nhttp://www.inria.fr\nISSN 0249-6399\n\n\f350\nLP fixed L\nH1\nH2\nH3\nH4\nLP fixed P\nH5\nH6\n\n340\n\n330\n\nheuristical solution\n\n320\n\n310\n\n300\n\n290\n\n280\n\n270\n\n260\n0\n\n2\n\n4\n\n6\nrandom platform\n\n8\n\n10\n\n\fH1\nH2\nH3\nH4\nH5\nH6\n\n500\n\nsimulative solution\n\n450\n\n400\n\n350\n\n300\n\n0\n\n2\n\n4\n\n6\nrandom platform\n\n8\n\n10\n\n\fLP fixed L\nH1\nH2\nH3\nH4\n\nsimulative solution\n\n500\n450\n400\n350\n300\n250\n0\n\n1\n\n2\n\n3\n4\n5\n6\nrandom platform\n\n7\n\n8\n\n9\n\n\fsimulative solution\n\n500\nLP fixed P\nH5\nH6\n\n450\n400\n350\n300\n250\n0\n\n1\n\n2\n\n3\n4\n5\n6\nrandom platform\n\n7\n\n8\n\n9\n\n\f400\nLP fixed P\nH1\nH2\nH3\nH4\nH5\nH6\n\nsimulative solution\n\n380\n360\n340\n320\n300\n280\n260\n0\n\n1\n\n2\n\n3\n4\n5\n6\nrandom platform\n\n7\n\n8\n\n9\n\n\f"}