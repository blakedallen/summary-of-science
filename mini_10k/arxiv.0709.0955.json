{"id": "http://arxiv.org/abs/0709.0955v1", "guidislink": true, "updated": "2007-09-06T20:44:40Z", "updated_parsed": [2007, 9, 6, 20, 44, 40, 3, 249, 0], "published": "2007-09-06T20:44:40Z", "published_parsed": [2007, 9, 6, 20, 44, 40, 3, 249, 0], "title": "Fastest mixing Markov chain on graphs with symmetries", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0709.3961%2C0709.0780%2C0709.3295%2C0709.3934%2C0709.4230%2C0709.3573%2C0709.3575%2C0709.2992%2C0709.2238%2C0709.1386%2C0709.2031%2C0709.3664%2C0709.3076%2C0709.2440%2C0709.4654%2C0709.4106%2C0709.2605%2C0709.1588%2C0709.4537%2C0709.2508%2C0709.0585%2C0709.2875%2C0709.2014%2C0709.4226%2C0709.3380%2C0709.4164%2C0709.3468%2C0709.0420%2C0709.0955%2C0709.3066%2C0709.1622%2C0709.2924%2C0709.3490%2C0709.2113%2C0709.4655%2C0709.4499%2C0709.4397%2C0709.3750%2C0709.1280%2C0709.0934%2C0709.0645%2C0709.0389%2C0709.3471%2C0709.0028%2C0709.0661%2C0709.4329%2C0709.3932%2C0709.4564%2C0709.3949%2C0709.2953%2C0709.2429%2C0709.4540%2C0709.3690%2C0709.4224%2C0709.3099%2C0709.2375%2C0709.4153%2C0709.1287%2C0709.3185%2C0709.0490%2C0709.3495%2C0709.0391%2C0709.1335%2C0709.0268%2C0709.3486%2C0709.3115%2C0709.2357%2C0709.2037%2C0709.3529%2C0709.4463%2C0709.0895%2C0709.2328%2C0709.3212%2C0709.0920%2C0709.0224%2C0709.2841%2C0709.1824%2C0709.1298%2C0709.2294%2C0709.0155%2C0709.4432%2C0709.0099%2C0709.0478%2C0709.3727%2C0709.3402%2C0709.2682%2C0709.2597%2C0709.0988%2C0709.2363%2C0709.3011%2C0709.1567%2C0709.3330%2C0709.1458%2C0709.2589%2C0709.2888%2C0709.4370%2C0709.2464%2C0709.1468%2C0709.0737%2C0709.1462%2C0709.1455&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Fastest mixing Markov chain on graphs with symmetries"}, "summary": "We show how to exploit symmetries of a graph to efficiently compute the\nfastest mixing Markov chain on the graph (i.e., find the transition\nprobabilities on the edges to minimize the second-largest eigenvalue modulus of\nthe transition probability matrix). Exploiting symmetry can lead to significant\nreduction in both the number of variables and the size of matrices in the\ncorresponding semidefinite program, thus enable numerical solution of\nlarge-scale instances that are otherwise computationally infeasible. We obtain\nanalytic or semi-analytic results for particular classes of graphs, such as\nedge-transitive and distance-transitive graphs. We describe two general\napproaches for symmetry exploitation, based on orbit theory and\nblock-diagonalization, respectively. We also establish the connection between\nthese two approaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0709.3961%2C0709.0780%2C0709.3295%2C0709.3934%2C0709.4230%2C0709.3573%2C0709.3575%2C0709.2992%2C0709.2238%2C0709.1386%2C0709.2031%2C0709.3664%2C0709.3076%2C0709.2440%2C0709.4654%2C0709.4106%2C0709.2605%2C0709.1588%2C0709.4537%2C0709.2508%2C0709.0585%2C0709.2875%2C0709.2014%2C0709.4226%2C0709.3380%2C0709.4164%2C0709.3468%2C0709.0420%2C0709.0955%2C0709.3066%2C0709.1622%2C0709.2924%2C0709.3490%2C0709.2113%2C0709.4655%2C0709.4499%2C0709.4397%2C0709.3750%2C0709.1280%2C0709.0934%2C0709.0645%2C0709.0389%2C0709.3471%2C0709.0028%2C0709.0661%2C0709.4329%2C0709.3932%2C0709.4564%2C0709.3949%2C0709.2953%2C0709.2429%2C0709.4540%2C0709.3690%2C0709.4224%2C0709.3099%2C0709.2375%2C0709.4153%2C0709.1287%2C0709.3185%2C0709.0490%2C0709.3495%2C0709.0391%2C0709.1335%2C0709.0268%2C0709.3486%2C0709.3115%2C0709.2357%2C0709.2037%2C0709.3529%2C0709.4463%2C0709.0895%2C0709.2328%2C0709.3212%2C0709.0920%2C0709.0224%2C0709.2841%2C0709.1824%2C0709.1298%2C0709.2294%2C0709.0155%2C0709.4432%2C0709.0099%2C0709.0478%2C0709.3727%2C0709.3402%2C0709.2682%2C0709.2597%2C0709.0988%2C0709.2363%2C0709.3011%2C0709.1567%2C0709.3330%2C0709.1458%2C0709.2589%2C0709.2888%2C0709.4370%2C0709.2464%2C0709.1468%2C0709.0737%2C0709.1462%2C0709.1455&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We show how to exploit symmetries of a graph to efficiently compute the\nfastest mixing Markov chain on the graph (i.e., find the transition\nprobabilities on the edges to minimize the second-largest eigenvalue modulus of\nthe transition probability matrix). Exploiting symmetry can lead to significant\nreduction in both the number of variables and the size of matrices in the\ncorresponding semidefinite program, thus enable numerical solution of\nlarge-scale instances that are otherwise computationally infeasible. We obtain\nanalytic or semi-analytic results for particular classes of graphs, such as\nedge-transitive and distance-transitive graphs. We describe two general\napproaches for symmetry exploitation, based on orbit theory and\nblock-diagonalization, respectively. We also establish the connection between\nthese two approaches."}, "authors": ["Stephen Boyd", "Persi Diaconis", "Pablo A. Parrilo", "Lin Xiao"], "author_detail": {"name": "Lin Xiao"}, "author": "Lin Xiao", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1137/070689413", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0709.0955v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0709.0955v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "39 pages, 15 figures", "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.OC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0709.0955v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0709.0955v1", "journal_reference": "SIAM Journal on Optimization, Vol. 20, Issue 2, pp. 792-819 (2009)\n  .", "doi": "10.1137/070689413", "fulltext": "Fastest Mixing Markov Chain on Graphs with Symmetries\nStephen Boyd\u2217\n\nPersi Diaconis\u2020\n\nPablo A. Parrilo\u2021\n\nLin Xiao\u00a7\n\narXiv:0709.0955v1 [math.PR] 6 Sep 2007\n\nApril 27, 2007\n\nAbstract\nWe show how to exploit symmetries of a graph to efficiently compute the fastest mixing\nMarkov chain on the graph (i.e., find the transition probabilities on the edges to minimize the\nsecond-largest eigenvalue modulus of the transition probability matrix). Exploiting symmetry\ncan lead to significant reduction in both the number of variables and the size of matrices in the\ncorresponding semidefinite program, thus enable numerical solution of large-scale instances that\nare otherwise computationally infeasible. We obtain analytic or semi-analytic results for particular classes of graphs, such as edge-transitive and distance-transitive graphs. We describe two\ngeneral approaches for symmetry exploitation, based on orbit theory and block-diagonalization,\nrespectively. We also establish the connection between these two approaches.\n\nKey words. Markov chains, eigenvalue optimization, semidefinite programming, graph automorphism, group representation.\n\n1\n\nIntroduction\n\nIn the fastest mixing Markov chain problem, we choose the transition probabilities on the edges\nof a graph to minimize the second-largest eigenvalue modulus of the transition probability matrix.\nIn [BDX04] we formulated this problem as a convex optimization problem, in particular as a\nsemidefinite program. Thus it can be solved, up to any given precision, in polynomial time by\ninterior-point methods. In this paper, we show how to exploit symmetries of a graph to make the\ncomputation more efficient.\n\n1.1\n\nThe fastest mixing Markov chain problem\n\nWe consider an undirected graph G = (V, E) with vertex set V = {1, . . . , n} and edge set E and\nassume that G is connected. We define a discrete-time Markov chain on the vertices as follows.\nThe state at time t will be denoted X(t) \u2208 V, for t = 0, 1, . . .. Each edge in the graph is associated\nwith a transition probability with which X makes a transition between the two adjacent vertices.\nThis Markov chain can be described via its transition probability matrix P \u2208 Rn\u00d7n , where\nPij = Prob ( X(t + 1) = j | X(t) = i ),\n\u2217\n\ni, j = 1, . . . , n.\n\nDepartment of Electrical Engineering, Stanford University, Stanford, CA 94305. Email: boyd@stanford.edu.\nDepartment of Statistics and Department of Mathematics, Stanford University, Stanford, CA 94305.\n\u2021\nDepartment of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge,\nMA 02139. Email: parrilo@mit.edu.\n\u00a7\nMicrosoft Research, 1 Microsoft Way, Redmond, WA 98052. Email: lin.xiao@microsoft.com.\n\u2020\n\n1\n\n\fNote that Pii is the probability that X(t) stays at vertex i, and Pij = 0 for {i, j} \u2208\n/ E (transitions\nare allowed only between vertices that are linked by an edge). We assume that the transition\nprobabilities are symmetric, i.e., P = P T , where the superscript T denotes the transpose of a\nmatrix. Of course this transition probability matrix must also be stochastic:\nP \u2265 0,\n\nP 1 = 1,\n\nwhere the inequality P \u2265 0 means elementwise, and 1 denotes the vector of all ones.\nSince P is symmetric and stochastic, the uniform distribution (1/n)1T is stationary. In addition,\nthe eigenvalues of P are real, and no more than one in modulus. We denote them in non-increasing\norder\n1 = \u03bb1 (P ) \u2265 \u03bb2 (P ) \u2265 * * * \u2265 \u03bbn (P ) \u2265 \u22121.\nWe denote by \u03bc(P ) the second-largest eigenvalue modulus (SLEM) of P , i.e.,\n\u03bc(P ) = max |\u03bbi (P )| = max {\u03bb2 (P ), \u2212\u03bbn (P )}.\ni=2,...,n\n\nThis quantity is widely used to bound the asymptotic convergence rate of the distribution of the\nMarkov chain to its stationary distribution, in the total variation distance or chi-squared distance\n(e.g., [DS91, DSC93]). In general the smaller \u03bc(P ) is, the faster the Markov chain converges. For\nmore background on Markov chains, eigenvalues and rapid mixing, see, e.g., the text [Br\u00e999].\nIn [BDX04], we addressed the following problem: What choice of P minimizes \u03bc(P )? In other\nwords, what is the fastest mixing (symmetric) Markov chain on the graph? This can be posed as\nthe following optimization problem:\nminimize \u03bc(P )\nsubject to P \u2265 0, P 1 = 1, P = P T\nPij = 0, {i, j} \u2208\n/ E.\n\n(1)\n\nHere P is the optimization variable, and the graph is the problem data. We call this problem the\nfastest mixing Markov chain (FMMC) problem. This is a convex optimization problem, in particular, the objective function can be explicitly written in a convex form \u03bc(P ) = kP \u2212(1/n)11T k2 , where\nk * k2 denotes the spectral norm of a matrix. Moreover, this problem can be readily transformed\ninto a semidefinite program (SDP):\nminimize s\nsubject to \u2212sI \u0016 P \u2212 (1/n)11T \u0016 sI\nP \u2265 0, P 1 = 1, P = P T\nPij = 0, {i, j} \u2208\n/ E.\n\n(2)\n\nHere I denote the identity matrix, and the variables are the matrix P and the scalar s. The symbol\n\u0016 denotes matrix inequality, i.e., X \u0016 Y means Y \u2212 X is positive semidefinite.\nThere has been some follow-up work on this problem. Boyd, Diaconis, Sun, Xiao ([BDSX06])\nproved analytically that on an n-path the fastest mixing chain can be obtained by assigning the same\ntransition probability half at the n \u2212 1 edges and two loops at the two ends. Roch ([Roc05]) used\nstandard mixing-time analysis techniques (variational characterizations, conductance, canonical\npaths) to bound the fastest mixing time. Gade and Overton ([GO06]) have considered the fastest\nmixing problem for a nonreversible Markov chain. Here, the problem is non-convex and much\nremains to be done. Finally, closed form solutions of fastest mixing problems have recently been\napplid in statistics to give a generalization of the usual spectral analysis of time series for more\ngeneral discrete data. see [Sal06].\n2\n\n\f1.2\n\nExploiting problem structure\n\nThe SDP formulation (2) means that the FMMC problem can be efficiently solved using standard\nSDP solvers, at least for small or medium size problems (with number of edges up to a thousand\nor so). General background on convex optimization and SDP can be found in, e.g., [NN94, VB96,\nWSV00, BTN01, BV04]. The current SDP solvers (e.g., [Stu99, TTT99, YFK03]) mostly use\ninterior-point methods which have polynomial time worst-case complexity.\nWhen solving the SDP (2) by interior-point methods, in each iteration we need to compute\nthe first and second derivatives of the logarithmic barrier functions (or potential functions) for\nthe matrix inequalities, and assemble and solve a linear system of equations (the Newton system).\nLet n be the number of vertices and m be the number of edges in the graph (equivalently m is the\nnumber of variables in the problem). The Newton system is a set of m linear equations with m\nunknowns. Without exploiting any structure, the number of flops per iteration in a typical barrier\nmethod is on the order max{mn3 , m2 n2 , m3 }, where the first two terms come from computing and\nassembling the Newton system, and the third term amounts to solving it (see, e.g., [BV04, \u00a711.8.3]).\n(Other variants of interior-point methods have similar orders of flop count.)\nExploiting problem structure can lead to significant improvement of solution efficiency. As for\nmany other problems defined on a graph, sparsity is the most obvious structure to consider here. In\nfact, many current SDP solvers already exploit sparsity. However, as a well-known fact, exploiting\nsparsity alone in interior-point methods for SDP has limited effectiveness. The sparsity of P , and\nthe sparsity plus rank-one structure of P \u2212 (1/n)11T , can be exploited to significantly reduce the\ncomplexity of assembling the Newton system, but typically the Newton system itself is dense. The\ncomputational cost per iteration can be reduced to order O(m3 ), dominated by solving the dense\nlinear system (see analysis for similar problems in, e.g., [BYZ00, XB04, XBK07]).\nIn addition to using interior-point methods for the SDP formulation (2), we can also solve the\nFMMC problem in the form (1) by subgradient-type (first-order) methods. The subgradients of\n\u03bc(P ) can be obtained by computing the extreme eigenvalues and associated eigenvectors of the\nmatrix P . This can be done very efficiently by iterative methods, specifically the Lanczos method,\nfor large sparse symmetric matrices (e.g., [GL96, Saa92]). Compared with interior-point methods,\nsubgradient-type methods can solve much larger problems but only to a moderate accuracy (they\ndon't have polynomial-time worst-case complexity). In [BDX04], we used a simple subgradient\nmethod to solve the FMMC problem on graphs with up to a few hundred thousand edges. More\nsophisticated first-order methods for solving large-scale eigenvalue optimization problems and SDPs\nhave been reported in, e.g., [HR00, BM03, Nem04, LNM04, Nes05]. A successive partial linear\nprogramming method was developed in [Ove92].\nIn this paper, we focus on the FMMC problem on graphs with large symmetry groups, and\nshow how to exploit symmetries of the graph to make the computation more efficient. A result by\nErd\u0151s and R\u00e9nyi [ER63] states that with probability one, the symmetry group of a (suitably defined)\nrandom graph is trivial, i.e., it contains only the identity element. Nevertheless, many of the graphs\nof theoretical and practical interest, particularly in engineering applications have very interesting,\nand sometimes very large, symmetry groups. Symmetry reduction techniques have been explored\nin several different contexts, e.g., dynamical systems and bifurcation theory [GSS88], polynomial\nsystem solving [Gat00, Wor94], numerical solution of partial differential equations [FS92], and Lie\nsymmetry analysis in geometric mechanics [MR99]. In the context of optimization, a class of SDPs\nwith symmetry has been defined in [KOMK01], where the authors study the invariance properties\nof the search directions of primal-dual interior-point methods. In addition, symmetry has been\nexploited to prune the enumeration tree in branch-and-cut algorithms for integer programming\n[Mar03], and to reduce matrix size in a spectral radius optimization problem [HOY03].\n3\n\n\fClosely related to our approach in this paper, the recent work [dKPS07] considers general SDPs\nthat are invariant under the action of a permutation group, and developed a technique based\non matrix \u2217-representation to reduce problem size. This technique has been applied to simplify\ncomputations in SDP relaxations for graph coloring and maximal clique problems [DR07], and to\nstrengthen SDP bounds for some coding problems [Lau07].\nFor the FMMC problem, we show that exploiting symmetry allows significant reduction in both\nnumber of optimization variables and size of matrices. Effectively, they correspond to reducing m\nand n, respectively, in the flop counts for interior-point methods mentioned above. The problem\ncan be considerably simplified and is often solvable analytically by only exploiting symmetry. We\npresent two general approaches for symmetry exploitation, based on orbit theory [BDPX05] and\nblock-diagonalization [GP04], respectively. We also establish the connection between these two\napproaches.\n\n1.3\n\nOutline\n\nIn \u00a72, we explain the concepts of graph automorphisms and the automorphism group (symmetry\ngroup) of a graph. We show that the FMMC problem always attains its optimum in the fixed-point\nsubset of the feasible set under the automorphism group. This allows us to only consider a number\nof distinct transition probabilities that equals the number of orbits of the edges. We then give\na formulation of the FMMC problem with reduced number of variables (transition probabilities),\nwhich appears to be very convenient in subsequent sections.\nIn \u00a73, we give closed-form solutions for the FMMC problem on some special classes of graphs,\nnamely edge-transitive graphs and distance-transitive graphs. Along the way we also discuss FMMC\non graphs formed by taking Cartesian products of simple graphs.\nIn \u00a74, we first review the orbit theory for reversible Markov chains, and give sufficient conditions\non constructing an orbit chain that contain all distinct eigenvalues of the original chain. This orbit\nchain is usually no longer symmetric but always reversible. We then solve the fastest reversible\nMarkov chain problem on the orbit graph, from which we immediately obtain optimal solution to\nthe original FMMC problem.\nIn \u00a75, we review some group representation theory and show how to block diagonalize the linear\nmatrix inequalities in the FMMC problem by constructing a symmetry-adapted basis. The resulting\nblocks usually have much smaller sizes and repeated blocked can be discarded in computation.\nExtensive examples in \u00a74 and \u00a75 reveal interesting connections between these two general symmetry\nreduction methods.\nIn \u00a76, we conclude the paper by pointing out some possible future work.\n\n2\n\nSymmetry analysis\n\nIn this section we explain the basic concepts that are essential in exploiting graph symmetry, and\nderive our result on reducing the number of optimization variables in the FMMC problem.\n\n2.1\n\nGraph automorphisms and classes\n\nThe study of graphs that possess particular kinds of symmetry properties has a long history. The\nbasic object of study is the automorphism group of a graph, and different classes can be defined\ndepending on the specific form in which the group acts on the vertices and edges.\nAn automorphism of a graph G = (V, E) is a permutation \u03c3 of V such that {i, j} \u2208 E if and\nonly if {\u03c3(i), \u03c3(j)} \u2208 E. The (full) automorphism group of the graph, denoted by Aut(G), is the set\n4\n\n\f1\n\n4\n\n2\n\n3\n\nFigure 1: The graph on the left side is edge-transitive, but not vertex-transitive. The one on the\nright side is vertex-transitive, but not edge-transitive.\n\nof all such permutations, with the group operation being composition. For instance, for the graph\non the left in Figure 1, the corresponding automorphism group is generated by all permutations of\nthe vertices {1, 2, 3}. This group, isomorphic to the symmetric group S3 , has six elements, namely\nthe permutations 123 \u2192 123 (the identity), 123 \u2192 213, 123 \u2192 132, 123 \u2192 321, 123 \u2192 231, and\n123 \u2192 312. Note that vertex 4 cannot be permuted with any other vertex.\nRecall that an action of a group G on a set X is a homomorphism from G to the set of all\npermutations of the elements in X (i.e., the symmetric group of degree |X |). For an element\nx \u2208 X , the set of all images g(x), as g varies through G, is called the orbit of x. Distinct orbits\nform equivalent classes and they partition the set X . The action is transitive if for every pair of\nelements x, y \u2208 X , there is a group element g \u2208 G such that g(x) = y. In other words, the action\nis transitive if there is only one single orbit in X .\nA graph G = (V, E) is said to be vertex-transitive if Aut(G) acts transitively on V. The action\nof a permutation \u03c3 on V induces an action on E with the rule \u03c3({i, j}) = {\u03c3(i), \u03c3(j)}. A graph\nG is edge-transitive if Aut(G) acts transitively on E. Graphs can be edge-transitive without being\nvertex-transitive and vice versa; simple examples are shown in Figure 1.\nA graph is 1-arc-transitive if given any four vertices u, v, x, y with {u, v}, {x, y} \u2208 E, there\nexists an automorphism g \u2208 Aut(G) such that g(u) = x and g(v) = y. Notice that, as opposed to\nedge-transitivity, here the ordering of the vertices is important, even for undirected graphs. In fact,\na 1-arc transitive graph must be both vertex-transitive and edge-transitive, and the reverse may\nnot be true. The 1-arc-transitive graphs are called symmetric graphs in [Big74], but the modern\nuse extends this term to all graphs that are simultaneously edge- and vertex-transitive. Finally,\nlet \u03b4(u, v) denote the distance between two vertices u, v \u2208 V. A graph is called distance-transitive\nif, for any four vertices u, v, x, y with \u03b4(u, v) = \u03b4(x, y), there is an automorphism g \u2208 Aut(G) such\nthat g(u) = x and g(v) = y.\nThe containment relationship among the four classes of graphs described above is illustrated\nin Figure 2. Explicit counterexamples are known for each of the non-inclusions. It is generally\nbelieved that distance-transitive graphs have been completely classified. This work has been done\nby classifying the distance-regular graphs. It would take us too far afield to give a complete\ndiscussion. See the survey in [DSC06, Section 7].\nThe concept of graph automorphism can be naturally extended to weighted graphs, by requiring\nthat the permutation must also preserve the weights on the edges (e.g., [BDPX05]). This extension\nallows us to exploit symmetry in more general reversible Markov chains, where the transition\nprobability matrix is not necessarily symmetric.\n\n5\n\n\fDistance-transitive\n?\n\n1-arc transitive\n@\n@\nR\n@\n\nEdge-transitive\n\nVertex-transitive\n\nFigure 2: Classes of symmetric graphs, and their inclusion relationship.\n\n2.2\n\nFMMC with symmetry constraints\n\nA permutation \u03c3 \u2208 Aut(G) can be represented by a permutation matrix Q, where Qij = 1 if\ni = \u03c3(j) and Qij = 0 otherwise. The permutation \u03c3 induces an action on the transition probability\nmatrix by \u03c3(P ) = QP QT . We denote the feasible set of the FMMC problem (1) by C, i.e.,\nC = {P \u2208 Rn\u00d7n | P \u2265 0, P 1 = 1, P = P T , Pij = 0 for {i, j} \u2208\n/ E}.\nThis set is invariant under the action of graph automorphism. To see this, let h = \u03c3(i) and k = \u03c3(j).\nThen we have\nX\nX\n(\u03c3(P ))hk = (QP QT )hk =\n(QP )hl Qkl = (QP )hj =\nQhl Plj = Pij .\nl\n\nl\n\nSince \u03c3 is a graph automorphism, we have {h, k} \u2208 E if and only if {i, j} \u2208 E, so the sparsity\npattern of the probability transition matrix is preserved. It is straightforward to verify that the\nconditions P \u2265 0, P 1 = 1, and P = P T , are also preserved under this action.\nLet F denote the fixed-point subset of C under the action of Aut(G); i.e.,\nF = {P \u2208 C | \u03c3(P ) = P, \u03c3 \u2208 Aut(G)}.\nWe have the following theorem (see also [GP04, Theorem 3.3]).\nTheorem 2.1. The FMMC problem always has an optimal solution in the fixed-point subset F.\nProof. Let \u03bc\u22c6 denote the optimal value of the FMMC problem (1), i.e., \u03bc\u22c6 = inf{\u03bc(P )|P \u2208 C}.\nSince the objective function \u03bc is continuous and the feasible set C is compact, there is at least one\noptimal transition matrix P \u22c6 such that \u03bc(P \u22c6 ) = \u03bc\u22c6 . Let P denote the average over the orbit of P \u22c6\nunder Aut(G)\nX\n1\n\u03c3(P \u22c6 ).\nP =\n|Aut(G)|\n\u03c3\u2208Aut(G)\nThis matrix is feasible because each \u03c3(P \u22c6 ) is feasible and the feasible set is convex. By construction,\nit is also invariant under the actions of Aut(G). Moreover, using the convexity of \u03bc, we have\n\u03bc(P ) \u2264 \u03bc(P \u22c6 ). It follows that P \u2208 F and \u03bc(P ) = \u03bc\u22c6 .\nAs a result of Theorem 2.1, we can replace the constraint P \u2208 C by P \u2208 F in the FMMC\nproblem and get the same optimal value. In the fixed-point subset F, the transition probabilities\non the edges within an orbit must be the same. So we have the following corollaries:\n6\n\n\fCorollary 2.2. The number of distinct edge transition probabilities we need to consider in the\nFMMC problem is at most equal to the number of orbits of E under Aut(G).\nCorollary 2.3. If G is edge-transitive, then all the edge transition probabilities can be assigned the\nsame value.\nP\nNote that the holding probability at the vertices can always be eliminated using Pii = 1\u2212 j Pij .\nSo it suffices to only consider the edge transition probabilities.\n\n2.3\n\nFormulation with reduced number of variables\n\nFrom the results of the previous section, we can reduce the number of optimization variables in the\nFMMC problem from the number of edges to the number of edge orbits under the automorphism\ngroup. Here we give an explicit parametrization of the FMMC problem with the reduced number\nof variables. This parametrization is also the precise characterization of the fixed-point subset F.\nRecall that the adjacency matrix of a graph with n vertices is a n\u00d7n matrix A whose entries are\ngiven by Aij = 1 if {i, j} \u2208 E and Aij = 0 otherwise. Let \u03bdi be the valency (degree) of vertex i. The\nLaplacian matrix of the graph is given by L = Diag(\u03bd1 , \u03bd2 , . . . , \u03bdn ) \u2212 A, where Diag(\u03bd) denotes a\ndiagonal matrix with the vector \u03bd as its diagonal. Extensive account of the Laplacian matrix and\nits use in algebraic graph theory are provided in, e.g., [Mer94, Chu97, GR01].\nSuppose that there are N orbits of edges under the action of Aut(G). For each orbit, we define\nan orbit graph Gk = (V, Ek ), where Ek is the set of edges in the kth orbit. Note that the orbit graphs\nare disconnected (there are disconnected vertices) if the original graph is not edge-transitive. Let\nLk be the Laplacian matrix of Gk . Note that the diagonal entries (Lk )ii equals the valency of node i\nin Gk (which is zero if vertex i is disconnected with all other vertices in Gk ).\nBy Corollary 2.2, we can assign the same transition probability on all the edges in the k-th orbit.\nDenote this transition probability by pk and let p = (p1 , . . . , pN ). Then the transition probability\nmatrix can be written as\nN\nX\nP (p) = I \u2212\np k Lk .\n(3)\nk=1\n\nThis parametrization of the transition probability matrix automatically satisfies the constraints\nP = P T , P 1 = 1, and Pij = 0 for {i, j} \u2208 E. The entry-wise nonnegative constraint P \u2265 0 now\ntranslates into\npk \u2265 0,\nk = 1, . . . , N\nN\nX\n(Lk )ii pk \u2264 1,\ni = 1, . . . , n\nk=1\n\nwhere the first set of constraints are for the off-diagonal entries of P , and the second set of constraints are for the diagonal entries of P .\nIt can be verified that the parametrization (3), together with the above inequality constraints,\nis the precise characterization of the fixed-point subset F. Therefore we can explicitly write the\nFMMC problem restricted to the fixed-point subset as\n\u0010\n\u0011\nP\nminimize \u03bc I \u2212 N\np\nL\nk=1 k k\nsubject to pk \u2265 0, k = 1, . . . , N\nPN\ni = 1, . . . , n.\nk=1 (Lk )ii pk \u2264 1,\n7\n\n(4)\n\n\fLater in this paper, we will also need the corresponding SDP formulation\nminimize\n\ns\n\nsubject to \u2212sI \u0016 I \u2212\n\nPN\n\nk=1 pk Lk\n\n\u2212 (1/n)11T \u0016 sI\n\npk \u2265 0, k = 1, . . . , N\nPN\ni = 1, . . . , n.\nk=1 (Lk )ii pk \u2264 1,\n\n3\n\n(5)\n\nSome analytic results\n\nFor some special classes of graphs, the FMMC problem can be considerably simplified and often\nsolved by only exploiting symmetry. In this section, we give some analytic results for the FMMC\nproblem on edge-transitive graphs, Cartesian product of simple graphs, and distance-transitive\ngraphs (a subclass of edge-transitive graphs). The optimal solution is often expressed in terms of\nthe eigenvalues of the adjacency matrix or the Laplacian matrix of the graph. It is interesting to\nnotice that even for such highly structured class of graphs, neither the maximum-degree nor the\nMetropolis-Hastings heuristics discussed in [BDX04] give the optimal solution. Throughout, we use\n\u03b1\u22c6 to denote the common edge weight of the fastest mixing chain and \u03bc\u22c6 to denote the optimal\nSLEM.\n\n3.1\n\nFMMC on edge-transitive graphs\n\nTheorem 3.1. Suppose the graph G is edge-transitive, and let \u03b1 be the transition probability assigned on all the edges. Then the optimal solution of the FMMC problem is\n\u001b\n\u001a\n2\n1\n\u22c6\n,\n(6)\n\u03b1 = min\n\u03bdmax \u03bb1 (L) + \u03bbn\u22121 (L)\n\u001a\n\u001b\n\u03bbn\u22121 (L) \u03bb1 (L) \u2212 \u03bbn\u22121 (L)\n\u22c6\n,\n\u03bc = max 1 \u2212\n,\n(7)\n\u03bdmax\n\u03bb1 (L) + \u03bbn\u22121 (L)\nwhere \u03bdmax = maxi\u2208V \u03bdi is the maximum valency of the vertices in the graph, and L is the Laplacian\nmatrix defined in \u00a72.3.\nProof. By definition of an edge-transitive graph, there is a single orbit of edges under the actions\nof its automorphism group. Therefore we can assign the same transition probability \u03b1 on all the\nedges in the graph (Corollary 2.3), and the parametrization (3) becomes P = I \u2212 \u03b1L. So we have\n\u03bbi (P ) = 1 \u2212 \u03b1\u03bbn+1\u2212i (L),\n\ni = 1, . . . , n\n\nand the SLEM\n\u03bc(P ) = max{\u03bb2 (P ), \u2212\u03bbn (P )}\n\n= max{1 \u2212 \u03b1\u03bbn\u22121 (L), \u03b1\u03bb1 (L) \u2212 1}.\n\nTo minimize \u03bc(P ), we let 1 \u2212 \u03b1\u03bbn\u22121 (L) = \u03b1\u03bb1 (L) \u2212 1 and get \u03b1 = 2/(\u03bbn\u22121 (L) + \u03bbn\u22121 (L)).\nBut the nonnegativity constraint P \u2265 0 requires that the transition probability must also satisfy\n0 < \u03b1 \u2264 1/\u03bdmax . Combining these two conditions gives the optimal solution (6) and (7).\nWe give two examples of FMMC on edge-transitive graphs.\n\n8\n\n\fFigure 3: The cycle graph Cn with n = 9.\n3.1.1\n\nCycles\n\nThe first example is the cycle graph Cn ; see Figure\n\uf8ee\n2 \u22121\n0\n\uf8ef \u22121\n2 \u22121\n\uf8ef\n\uf8ef 0 \u22121\n2\n\uf8ef\nL=\uf8ef .\n.\n..\n..\n\uf8ef ..\n.\n\uf8ef\n\uf8f0 0\n0\n0\n\u22121\n0\n0\nwhich has eigenvalues\n\n2 \u2212 2 cos\n\n2k\u03c0\n,\nn\n\n3. The Laplacian matrix is\n\uf8f9\n***\n0 \u22121\n***\n0\n0 \uf8fa\n\uf8fa\n***\n0\n0 \uf8fa\n\uf8fa\n..\n.. \uf8fa\n..\n.\n.\n. \uf8fa\n\uf8fa\n***\n2 \u22121 \uf8fb\n* * * \u22121\n2\nk = 1, . . . , n.\n\nThe two extreme eigenvalues are\n\u03bb1 (L) = 2 \u2212 2 cos\n\n2\u230an/2\u230b\u03c0\n,\nn\n\n\u03bbn\u22121 (L) = 2 \u2212 2 cos\n\n2\u03c0\nn\n\nwhere \u230an/2\u230b denotes the largest integer that is no larger than n/2, which is n/2 for n even or\n(n \u2212 1)/2 for n odd. By Theorem 3.1, the optimal solution to the FMMC problem is\n\u03b1\u22c6 =\n\n\u03bc\u22c6 =\n\n1\n2 \u2212 cos\n\n2\u03c0\nn\n\n(8)\n\n\u2212 cos 2\u230an/2\u230b\u03c0\nn\n\n2\u230an/2\u230b\u03c0\ncos 2\u03c0\nn \u2212 cos\nn\n\n2\u230an/2\u230b\u03c0\n2 \u2212 cos 2\u03c0\nn \u2212 cos\nn\n\n.\n\n(9)\n\nWhen n \u2192 \u221e, the transition probability \u03b1\u22c6 \u2192 1/2 and the SLEM \u03bc\u22c6 \u2192 1 \u2212 2\u03c0 2 /n2 .\n3.1.2\n\nComplete bipartite graphs\n\nThe complete bipartite graph, denoted Km,n , has two subsets of vertices with cardinalities m and n\nrespectively. Each vertex in a subset is connected to all the vertices in the other subset, and is not\nconnected to any of the vertices in its own subset; see Figure 4. Without loss of generality, assume\nm \u2264 n. So the maximum degree is \u03bdmax = n. This graph is edge-transitive but not vertex-transitive.\nThe Laplacian matrix of this graph is\n\u0014\n\u0015\nnIm\n\u22121m\u00d7n\nL=\n\u22121n\u00d7m\nmIn\n9\n\n\fv\nu\n\nx\ny\nFigure 4: The complete bipartite graph Km,n with m = 3 and n = 4.\nwhere Im denotes the m by m identity matrix, and 1m\u00d7n denotes the m by n matrix whose\ncomponents are all ones. For n \u2265 m \u2265 2, this matrix has four distinct eigenvalues m + n, n, m and\n0, with multiplicities 1, m \u2212 1, n \u2212 1 and 1, respectively (see \u00a75.3.1). By Theorem 3.1, the optimal\ntransition probability on the edges and the corresponding SLEM are\n\u001b\n\u001a\n2\n1\n\u22c6\n,\n(10)\n\u03b1 = min\nn n + 2m\n\u001a\n\u001b\nn\u2212m\nn\n\u22c6\n,\n\u03bc = max\n.\n(11)\nn\nn + 2m\n\n3.2\n\nCartesian product of graphs\n\nMany graphs we consider can be constructed by taking Cartesian product of simpler graphs. The\nCartesian product of two graphs G1 = (V1 , E1 ) and G2 = (V2 , E2 ) is a graph with vertex set V1 \u00d7 V2 ,\nwhere two vertices (u1 , u2 ) and (v1 , v2 ) are connected by an edge if and only if u1 = v1 and\n{u2 , v2 } \u2208 E2 , or u2 = v2 and {u1 , v1 } \u2208 E1 . Let G1 \u2295 G2 denote this Cartesian product. Its\nLaplacian matrix is given by\nLG1 \u2295G2 = LG1 \u2297 I|V1| + I|V2 | \u2297 LG2\n\n(12)\n\nwhere \u2297 denotes the matrix Kronecker product ([Gra81]). The eigenvalues of LG1 \u2295G2 are given by\n\u03bbi (LG1 ) + \u03bbj (LG2 ),\n\ni = 1, . . . , |V1 |,\n\nj = 1, . . . , |V2 |\n\n(13)\n\nwhere each eigenvalue is obtained as many times as its multiplicity (e.g., [Moh97]). The adjacency\nmatrix of the Cartesian product of graphs also has similar properties, which we will use later for\ndistance-transitive graphs. Detailed background on spectral graph theory can be found in, e.g.,\n[Big74, DCS80, Chu97, GR01].\nCombining Theorem 3.1 and the above expression for eigenvalues, we can easily obtain solutions\nto the FMMC problem on graphs formed by taking Cartesian product of simple graphs.\n3.2.1\n\nTwo-dimensional meshes\n\nHere we consider the two-dimensional mesh with wraparounds at two ends of each row and column,\nsee Figure 5. It is the Cartesian product of two copies of Cn . We write it as Mn = Cn \u2295 Cn . By\nequation (13), its Laplacian matrix has eigenvalues\n4 \u2212 2 cos\n\n2j\u03c0\n2i\u03c0\n\u2212 2 cos\n,\nn\nn\n10\n\ni, j = 1, . . . , n.\n\n\fFigure 5: The two-dimensional mesh with wraparounds Mn with n = 4.\nBy Theorem 3.1, we obtain the optimal transition probability\n\u03b1\u22c6 =\nand the smallest SLEM\n\u03bc\u22c6 =\n\n1\n3 \u2212 2 cos\n\n2\u230an/2\u230b\u03c0\nn\n\n\u2212 cos 2\u03c0\nn\n\n+ cos 2\u03c0\n1 \u2212 2 cos 2\u230an/2\u230b\u03c0\nn\nn\n3 \u2212 2 cos 2\u230an/2\u230b\u03c0\n\u2212 cos 2\u03c0\nn\nn\n\nWhen n \u2192 \u221e, the transition probability \u03b1\u22c6 \u2192 1/4 and the SLEM \u03bc\u22c6 \u2192 1 \u2212 \u03c0 2 /n2 .\n3.2.2\n\nHypercubes\n\nThe d-dimensional hypercube, denoted Qd , has 2d vertices, each labeled with a binary word with\nlength d. Two vertices are connected by an edge if their words differ in exactly one component\n(see Figure 6). This graph is isomorphic to the Cartesian product of d copies of K2 , the complete\ngraph with two vertices. The Laplacian of K2 is\n\u0014\n\u0015\n1 \u22121\nLK 2 =\n,\n\u22121\n1\nwhose two eigenvalues are 0 and 2. The one-dimensional hypercube Q1 is just K2 . Higher dimensional hypercubes are defined recursively:\nQk+1 = Qk \u2295 K2 ,\n\nk = 1, 2, . . . .\n\nBy equation (12), their Laplacian matrices are\nLQk+1 = LQk \u2297 I2 + I2k \u2297 LK2 ,\n\nk = 1, 2, . . . .\n\nUsing equation\n\u0010 \u0011(13) recursively, the Laplacian of Qd has eigenvalues 2k, k = 0, 1, . . . , d, each with\nmultiplicity kd . The FMMC is achieved for:\n\u03b1\u22c6 =\n\n1\n,\nd+1\n\n\u03bc\u22c6 =\n\nd\u22121\n.\nd+1\n\nThis solution has also been worked out, for example, in [Moh97].\n\n11\n\n\fFigure 6: The hypercubes Q1 , Q2 and Q3 .\n\n3.3\n\nFMMC on distance-transitive graphs\n\nDistance-transitive graphs have been studied extensively in the literature (see, e.g., [BCN89]).\nIn particular, they are both edge- and vertex-transitive. In previous examples, the cycles and the\nhypercubes are actually distance-transitive graphs; so are the bipartite graphs when the two parties\nhave equal number of vertices.\nIn a distance-transitive graph, all vertices have the same valency, which we denote by \u03bd. The\nLaplacian matrix can be written as L = \u03bdI \u2212 A, with A being the adjacency matrix. Therefore\n\u03bbi (L) = \u03bd \u2212 \u03bbn+1\u2212i (A),\n\ni = 1, . . . , n.\n\nWe can substitute the above equation in equations (6) and (7) to obtain the optimal solution in\nterms of \u03bb2 (A) and \u03bbn (A).\nSince distance-transitive graphs usually have very large automorphism groups, the eigenvalues\nof the adjacency matrix A (and the Laplacian L) often have very high multiplicities. But to solve\nthe FMMC problem, we only need to know the distinct eigenvalues; actually, only \u03bb2 (A) and \u03bbn (A)\nwould suffice. In this regard, it is more convenient to use a much smaller matrix, the intersection\nmatrix, which has all the distinct eigenvalues of the adjacency matrix.\nLet D be the diameter of the graph. For a nonnegative integer k \u2264 D, choose any two vertices u\nand v such that their distance satisfies \u03b4(u, v) = k. Let ak , bk and ck be the number of vertices\nthat are adjacent to u and whose distance from v are k, k + 1 and k \u2212 1, respectively. That is,\nak = |{w \u2208 V | \u03b4(u, w) = 1, \u03b4(w, v) = k}|\n\nbk = |{w \u2208 V | \u03b4(u, w) = 1, \u03b4(w, v) = k + 1}|\n\nck = |{w \u2208 V | \u03b4(u, w) = 1, \u03b4(w, v) = k \u2212 1}|.\nFor distance-transitive graphs, these numbers are independent of the particular pair of vertices u\nand v chosen. Clearly, we have a0 = 0, b0 = \u03bd and c1 = 1. The intersection matrix B is the\nfollowing tridiagonal (D + 1) \u00d7 (D + 1) matrix\n\uf8ee\n\uf8f9\na0 b0\n\uf8ef c1 a1 b1\n\uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\n\uf8fa\n..\n\uf8ef\n\uf8fa.\n.\nB=\uf8ef\nc2 a2\n\uf8fa\n\uf8ef\n\uf8fa\n.. ..\n\uf8f0\n.\n. bD\u22121 \uf8fb\ncD aD\nDenote the eigenvalues of the intersection matrix, in decreasing order, as \u03b70 , \u03b71 , . . . , \u03b7D .\nThese are precisely the (D + 1) distinct eigenvalues of the adjacency matrix A (see, e.g., [Big74]).\nIn particular, we have\n\u03bb1 (A) = \u03b70 = \u03bd,\n\n\u03bb2 (A) = \u03b71 ,\n12\n\n\u03bbn (A) = \u03b7D .\n\n\fThe following corollary is a direct consequence of Theorem 3.1.\nCorollary 3.2. The optimal solution of the FMMC problem on a distance-transitive graph is\n\u001b\n\u001a\n2\n1\n,\n(14)\n\u03b1\u22c6 = min\n\u03bd 2\u03bd \u2212 (\u03b71 + \u03b7D )\n\u001a\n\u001b\n\u03b71\n\u03b71 \u2212 \u03b7D\n\u22c6\n\u03bc = max\n,\n.\n(15)\n\u03bd 2\u03bd \u2212 (\u03b71 + \u03b7D )\nNext we give solutions for the FMMC problem on several families of distance-transitive graphs.\n3.3.1\n\nComplete graphs\n\nThe case of the complete graph with n vertices, usually called Kn , is very simple. It is distancetransitive, with diameter D = 1 and valency \u03bd = n \u2212 1. The intersection matrix is\n\u0015\n\u0014\n0 n\u22121\n,\nB=\n1 n\u22122\nwith eigenvalues \u03b70 = n \u2212 1, \u03b71 = \u22121. Using equations (14) and (15), the optimal parameters are\n\u03b1\u22c6 =\n\n1\n,\nn\n\n\u03bc\u22c6 = 0.\n\nThe associated matrix P = (1/n)11T has one eigenvalue equal to 1, and the remaining n \u2212 1\neigenvalues vanish. Such Markov chains achieve perfect mixing after just one step, regardless of\nthe value of n.\n3.3.2\n\nPetersen graph\n\nThe Petersen graph, shown in Figure 7, is a well-known distance-transitive graph with 10 vertices\nand 15 edges. The diameter of the graph is D = 2, and the intersection matrix is\n\uf8f9\n\uf8ee\n0 3 0\nB=\uf8f0 1 0 2 \uf8fb\n0 1 2\nwith eigenvalues \u03b70 = 3, \u03b71 = 1 and \u03b72 = \u22122. Applying the formula (14) and (15), we obtain\n2\n\u03b1\u22c6 = ,\n7\n3.3.3\n\n3\n\u03bc\u22c6 = .\n7\n\nHamming graphs\n\nThe Hamming graphs, denoted H(d, n), have vertices labeled by elements in the Cartesian product\n{1, . . . , n}d , with two vertices being adjacent if they differ in exactly one component. By the\ndefinition, it is clear that Hamming graphs are isomorphic to the Cartesian product of d copies\nof the complete graph Kn . Hamming graphs are distance-transitive, with diameter D = d and\nvalency \u03bd = d (n \u2212 1). Their eigenvalues are given by \u03b7k = d (n \u2212 1) \u2212 kn for k = 0, . . . , d. These\n\n13\n\n\fFigure 7: The Petersen graph.\n\ncan be obtained using an equation for eigenvalues of adjacency matrices, similar to (13), with the\neigenvalues of Kn being n \u2212 1 and \u22121. Therefore the FMMC has parameters:\n\u001b\n\u001a\n2\n1\n\u22c6\n,\n\u03b1 = min\nd (n \u2212 1) n (d + 1)\n\u001a\n\u001b\nn\nd\u22121\n\u22c6\n\u03bc = max 1 \u2212\n,\n.\nd(n \u2212 1) d + 1\nWe note that hypercubes (see \u00a73.2.2) are special Hamming graphs with n = 2.\n3.3.4\n\nJohnson graphs\n\nThe Johnson graph J(n, q) (for 1 \u2264 q \u2264 n/2) is defined as follows: the vertices are the q-element\nsubsets of {1, . . . , n}, with two vertices being connected with an edge\u0001 if and only if the subsets\u0001\ndiffer exactly by one element. It is a distance-transitive graph, with nq vertices and 12 q (n \u2212 q) nq\nedges. It has valency \u03bd = q (n \u2212 q) and diameter D = q. The eigenvalues of the intersection matrix\ncan be computed analytically and they are:\n\u03b7k = q (n \u2212 q) + k (k \u2212 n \u2212 1),\n\nk = 0, . . . , q.\n\nTherefore, by Corollary 3.2, we obtain the optimal transition probability\n\u001a\n\u001b\n1\n2\n\u22c6\n\u03b1 = min\n,\nq (n \u2212 q) qn + n + q \u2212 q 2\nand the smallest SLEM\n\u001a\n\u03bc\u22c6 = max 1 \u2212\n\n4\n\nn\n2n\n, 1\u2212\nq(n \u2212 q)\nqn + n + q \u2212 q 2\n\n\u001b\n\n.\n\nFMMC on orbit graphs\n\nFor graphs with large automorphism groups, the eigenvalues of the transition probability matrix\noften have very high multiplicities. To solve the FMMC problem, it suffices to work with only\nthe distinct eigenvalues without consideration of their multiplicities. This is exactly what the\nintersection matrix does for distance-transitive graphs. In this section we develop similar tools for\nmore general graphs. More specifically, we show how to construct an orbit chain which is much\nsmaller in size than the original Markov chain, but contains all its distinct eigenvalues (with much\nfewer multiplicities). The FMMC on the original graph can be found by solving a much smaller\nproblem on the orbit chain.\n14\n\n\f4.1\n\nOrbit theory\n\nHere we review the orbit theory developed in [BDPX05]. Let P be a symmetric Markov chain on\nthe graph G = (V, E), and H be a group of automorphisms of the graph. Often, it is a subgroup of\nthe full automorphism group Aut(G). The vertex set V partitions into orbits Ov = {hv : h \u2208 H}.\nFor notational convenience, in this section we use P (v, u), for v, u \u2208 V, to denote entries of the\ntransition probability matrix. We define the orbit chain by specifying the transition probabilities\nbetween orbits\nX\nPH (Ov , Ou ) = P (v, Ou ) =\nP (v, u\u2032 ).\n(16)\nu\u2032 \u2208Ou\n\nThis transition probability is independent of which v \u2208 O(v) is chosen, so it is well defined and the\nlumped orbit chain is indeed Markov.\nThe orbit chain is in general no longer symmetric, but it is always reversible. Let \u03c0(i), i \u2208 V,\nbe the stationary distribution of the original Markov chain. Then the stationary distribution on\nthe orbit chain is obtained as\nX\n\u03c0(i).\n(17)\n\u03c0H (Ov ) =\ni\u2208Ov\n\nIt can be verified that\n\u03c0H (Ov )PH (Ov , Ou ) = \u03c0H (Ou )PH (Ou , Ov ),\n\n(18)\n\nwhich is the detailed balance condition to test reversibility.\nThe following is a summary of the orbit theory we developed in [BDPX05], which relate the\neigenvalues and eigenvectors of the orbit chain PH to the eigenvalues and eigenvectors of the original\nchain P .\n\u2022 Lifting ([BDPX05, \u00a73.1]). If \u03bb\u0304 is an eigenvalue of PH with associated eigenvector f \u0304, then\n\u03bb\u0304 is an eigenvalue of P with H-invariant eigenfunction f (v) = f \u0304(Ov ). Conversely, every\nH-invariant eigenfunction appears uniquely from this construction.\n\u2022 Projection\n([BDPX05, \u00a73.2]). Let \u03bb be an eigenvalue of P with eigenvector f . Define f \u0304(Ov ) =\nP\n\u22121\n \u0304\nh\u2208H f (h (v)). Then \u03bb appears as an eigenvalue of PH , with eigenvector f , if either of the\nfollowing two conditions holds:\n(a) H has a fixed point v \u2217 and f (v \u2217 ) 6= 0.\n\n(b) f is nonzero at a vertex v \u2217 in an Aut(G)-orbit which contains a fixed point of H.\nEquipped with this orbit theory, we would like to construct one or multiple orbit chains that\nretain all the eigenvalues of the original chain. Ideally the orbit chains are much smaller in size than\nthe original chain, with the eigenvalues having much fewer multiplicities. The following theorem\n(Theorem 3.7 in [BDPX05]) gives sufficient conditions that guarantee that the orbit chain(s) attain\nall the eigenvalues of the original chain.\nTheorem 4.1. Suppose that V = O1 \u222a . . . \u222a OK is a disjoint union of the orbits under Aut(G). Let\nHi be the subgroup of Aut(G) that has a fixed point in Oi . Then all eigenvalues of P occur among\nthe eigenvalues of {PHi }K\ni=1 . Further, every eigenvector of P occurs by lifting an eigenvector of\nsome PHi .\nObserve that if H \u2286 G \u2286 Aut(G), then the eigenvalues of PH contain all eigenvalues of PG .\nThis allows disregarding some of the Hi in Theorem 4.1. In particular, it is possible to construct a\nsingle orbit chain that contains all eigenvalues of the original chain. Therefore we have\n15\n\n\fnp\n(m\u22121)p\n\nOu\n\nnp\nOu\n\nOv\nnp\n\nOv\np\n\nmp\nx\n(a)\n\nOrbit chain under Sm \u00d7 Sn .\n\n(b)\n\nOrbit chain under Sm\u22121 \u00d7 Sn .\n(n\u22121)p\n\n(n\u22121)p\nmp\n\nOu\nmp\n\n(m\u22121)p\n\ny\n\nx\n\nOrbit chain under Sm \u00d7 Sn\u22121 .\n\n(d)\n\nOv\n(n\u22121)p\n\np\n\np\n\n(c)\n\n(m\u22121)p\n\nOu\n\nOv\n\np\np\n\ny\n\nOrbit chain under Sm\u22121 \u00d7 Sn\u22121 .\n\nFigure 8: Orbit chains of Km,n under different automorphism groups. The vertices labeled Ou and\nOv are orbits of vertices u and v (labeled in Figure 4) under corresponding actions. The vertices\nlabeled x and y are fixed points.\nCorollary 4.2. Suppose that V = O1 \u222a . . . \u222a Ok is a disjoint union of the orbits under Aut(G), and\nH is a subgroup of Aut(G). If H has a fixed point in every Oi , then all distinct eigenvalues of P\noccur among the eigenvalues of PH .\nRemarks. To find H in the above corollary, we can just compute the corresponding stabilizer,\ni.e., compute the largest subgroup of Aut(G) that fixes one point in each orbit. Note that the H\npromised by the corollary may be trivial in some cases; see the example in \u00a75.3.6.\nWe illustrate the orbit theory with the bipartite graph Km,n shown in Figure 4. It is easy to\nsee that Aut(Km,n ) is the direct product of two symmetric groups, namely Sm \u00d7 Sn , with each\nsymmetric group permuting one of the two subsets of vertices. This graph is edge-transitive. So\nwe assign the same transition probability p on all the edges.\nThe orbit chains under four different subgroups of Aut(Km,n ) are shown in Figure 8. The\ntransition probabilities between orbits are calculated using equation (16). Since the transition\nprobabilities are not symmetric, we represent the orbit chains by directed graphs, with different\ntransition probabilities labeled on opposite directions between two adjacent vertices. The full\nautomorphism group Aut(Km,n ) has two orbits of vertices; see Figure 8(a). The orbit graphs under\nthe subgroups Sm\u22121 \u00d7 Sn (Figure 8(b)) and Sm \u00d7 Sn\u22121 (Figure 8(c)) each contains a fixed point of\nthe two orbits under Aut(Km,n ). By Theorem 4.1, these two orbit chains contain all the distinct\neigenvalues of the original chain on Km,n . Alternatively, we can construct the orbit chain under the\nsubgroup Sm\u22121 \u00d7 Sn\u22121 , shown in Figure 8(d). This orbit chain contain a fixed point in both orbits\nunder Aut(Km,n ). By Corollary 4.1, all distinct eigenvalues of Km,n appear in this orbit chain. In\nparticular, this shows that there are at most four distinct eigenvalues in the original chain.\n\n16\n\n\fIf we order the vertices in Figure 8(d) as (x, y, Ou , Ov ), then the transition probability matrix\nfor this orbit chain is\n\uf8ee\n\uf8f9\n1 \u2212 np\np\n0\n(n \u2212 1)p\n\uf8ef\n\uf8fa\np\n1 \u2212 mp (m \u2212 1)p\n0\n\uf8fa\nPH = \uf8ef\n\uf8f0\n0\np\n1 \u2212 np (n \u2212 1)p \uf8fb\np\n0\n(m \u2212 1)p 1 \u2212 mp\n\nwhere H = Sm\u22121 \u00d7 Sn\u22121 . By equation (17), its stationary distribution is\n\u0013\n\u0012\n1\nm\u22121 n\u22121\n1\n,\n,\n,\n.\n\u03c0H =\nm+n m+n m+n m+n\n\n4.2\n\nFastest mixing reversible Markov chain on orbit graph\n\nSince in general the orbit chain is no longer symmetric, we cannot directly use the convex optimization formulation (1) or (2) to minimize \u03bc(PH ). Fortunately, the detailed balance condition (18)\nleads to a simple transformation that allow us to formulate the problem of finding the fastest\nreversible Markov chain as a convex program [BDX04].\nSuppose the orbit chain PH contains all distinct eigenvalues of the original chain. Let \u03c0H be the\nstationary distribution of the orbits, and let \u03a0 = Diag(\u03c0H ). The detailed balance condition (18)\ncan be written as \u03a0PH = PHT \u03a0, which implies that the matrix \u03a01/2 PH \u03a0\u22121/2 is symmetric (and\n1/2\n\u22121/2 associated with the\nof course, has the same eigenvalues\np eigenvector of \u03a0 PH \u03a0\np as PH ). The\nmaximum eigenvalue 1 is q = ( \u03c0H (O1 ), . . . , \u03c0H (Ok )). The SLEM \u03bc(PH ) equals the spectral\nnorm of \u03a01/2 PH \u03a0\u22121/2 restricted to the orthogonal complement of the subspace spanned by q. This\ncan be written as\n\u03bc(PH ) = k(I \u2212 qq T )\u03a01/2 PH \u03a0\u22121/2 (I \u2212 qq T )k2 = k\u03a01/2 PH \u03a0\u22121/2 \u2212 qq T k2 .\nIntroducing a scalar variable s to bound the above spectral norm, we can formulate the fastest\nmixing reversible Markov chain problem as an SDP\nminimize s\nsubject to \u2212sI \u0016 \u03a01/2 PH \u03a0\u22121/2 \u2212 qq T \u0016 sI\nPH \u2265 0, PH 1 = 1, \u03a0PH = PHT \u03a0\nPH (O, O\u2032 ) = 0, (O, O\u2032 ) \u2208\n/ EH .\n\n(19)\n\nThe optimization variables are the matrix PH and scalar s, and problem data is given by the orbit\ngraph and the stationary distribution \u03c0H . Note that the reversibility constraint \u03a0PH = PHT \u03a0 can\nbe dropped since it is always satisfied by the construction of the orbit chain; see equation (18).\nBy pre- and post-multiplying the matrix inequality by \u03a01/2 , we can write then another equivalent\nformulation:\nminimize s\nT \u0016 s\u03a0\nsubject to \u2212s\u03a0 \u0016 \u03a0PH \u2212 \u03c0H \u03c0H\n(20)\nPH \u2265 0, PH 1 = 1,\nPH (O, O\u2032 ) = 0, (O, O\u2032 ) \u2208\n/ EH .\nTo solve the fastest mixing reversible Markov chain problem on the orbit graph, we need the\nfollowing three steps.\n\n17\n\n\f1. Conduct symmetry analysis on the original graph: identify the automorphism graph Aut(G)\nand determine the number of orbits of edges N . By Corollary 2.2, this is the number of\ntransition probabilities we need to consider.\n2. Find a group of automorphisms H that satisfies the conditions in Corollary 4.2. Construct\nits orbit chain by computing the transition probabilities using equation (16), and compute\nthe stationary distribution using equation (17). Note that the entries of PH are multiples of\nthe transition probabilities on the original graph.\n3. Solve the fastest mixing reversible Markov chain problem (19). The optimal SLEM \u03bc(PH\u22c6 ) is\nalso the optimal SLEM for the original chain, and the optimal transition probabilities on the\noriginal chain can be obtained by simple scaling of the optimal orbit transition probabilities.\nWe have assumed a single orbit chain that contains all distinct eigenvalues of the original chain.\nSometimes it is more convenient to use multiple orbit chains. Let PHi , i = 1, . . . , K, be the\ncollection of orbit chains in Theorem 4.1. In this case we need to minimize maxi \u03bc(PHi ). This can\nbe done by simply adding the set of constraints in (19) for every matrix PHi . For example, for the\ncomplete bipartite graph Km,n , instead of using the single orbit chain in Figure 8(d), we can use\nthe two orbit chains in Figure 8(b) and Figure 8(c) together, with two sets of constraints in the\nSDP (19).\n\n4.3\n\nExamples\n\nWe demonstrate the above computational procedure on orbit graphs with two examples: the graph\nKn -Kn and the complete binary tree. Both examples will be revisited in \u00a75 using the method of\nblock diagonalization.\n4.3.1\n\nThe graph Kn -Kn\n\nThe graph Kn -Kn consists of two copies of the complete graph Kn joined by a bridge (see Figure 9(a)). We follow the three steps described in \u00a74.2.\nFirst, it is clear by inspection that the full automorphism group is C2 \u22c9 (Sn\u22121 \u00d7 Sn\u22121 ). The\nactions of Sn\u22121 \u00d7 Sn\u22121 are all possible permutations of the two set of n \u2212 1 vertices, distinct from\nthe two center vertices x and y, among themselves. The group C2 acts on the graph by switching\nthe two halves. The semi-direct product symbol \u22c9 means that the actions of Sn\u22121 \u00d7 Sn\u22121 and C2\ndo not commute.\nBy symmetry analysis in \u00a72, there are three edge orbits under the full automorphism group: the\nbridging edge between vertices x and y, the edges connecting x and y to all other vertices, and the\nedges connecting all other vertices. Thus it suffices to consider just three transition probabilities\np0 , p1 , and p2 , each labeled in Figure 9(a) on one representative of the three edge orbits.\nAs the second step, we construct the orbit chains. The orbit chain of Kn -Kn under the full\nautomorphism group is depicted in Figure 9(b). The orbit Ox includes vertices x and y, and the\norbit Oz consists of all other 2(n \u2212 1) vertices. The transition probabilities of this orbit chain are\ncalculated from equation (16) and are labeled on the directed edges in Figure 9(b). Similarly, the\norbit chain under the subgroup Sn\u22121 \u00d7 Sn\u22121 is depicted in Figure 9(c). While these two orbit chains\nare the most obvious to construct, none of them contains all eigenvalues of the original chain, nor\ndoes their combination. For the one in Figure 9(b), the full automorphism group does not have a\nfixed point either of its orbit Ox or Oz . For the one in 9(c), the automorphism group Sn\u22121 \u00d7 Sn\u22121\nhas a fixed point in Ox (either x or y), but does not have a fixed point in Oz (note here Oz is the\n\n18\n\n\fp2\n\nz\np1\np0\nx\n\ny\n\nu\n(a)\n\nv\nThe graph Kn -Kn .\n\n(n \u2212 1)p1\n\nOz\n\nOx\n\np1\n(b)\n\nOrbit chain under C2 \u22c9 (Sn\u22121 \u00d7 Sn\u22121 ).\n\n(n \u2212 1)p1\n\nOu\n\nx\n\np1\n(c)\n\n(n \u2212 1)p1\n\np0\ny\n\nOv\n\np1\n\nOrbit chain under Sn\u22121 \u00d7 Sn\u22121 .\n\nz\n(n\u22122)p2\np2\n\np1\n\np1\nOu\n\np1\np0\nx\n\n(n\u22121)p1\n\n(n\u22122)p1\n(d)\n\nOv\n\ny\n\nOrbit chain under Sn\u22122 \u00d7 Sn\u22121 .\n\nFigure 9: The graph Kn -Kn and its orbit chains under different automorphism groups. Here\nOx , Oz , Ou , Ov represent orbits of the vertices x, z, u, v (labeled in Figure 9(a)), respectively, under\nthe corresponding automorphism groups in each subgraph.\n\n19\n\n\forbit of z under the full automorphism group). To fix the problem, we consider the orbit chain\nunder the group Sn\u22122 \u00d7 Sn\u22121 , which leave the vertex x, y, and z fixed, while permuting the rest\nn \u2212 2 vertices on the left and the n \u2212 1 points on the right, respectively. The corresponding orbit\nchain is shown in Figure 9(d). By Corollary 4.2, all distinct eigenvalues of the original Markov\nchain on Kn -Kn appear as eigenvalues of this orbit chain. Thus there are at most five distinct\neigenvalues in the original chain no matter how large n is.\nTo finish the second step, we calculate the transition probabilities of the orbit chain under\nH = Sn\u22122 \u00d7 Sn\u22121 using equation (16) and label them in Figure 9(d). If we order the vertices of\nthis orbit chain as (x, y, z, Ou , Ov ), then the transition probability matrix on the orbit chain is\n\uf8ee\n\uf8f9\n1 \u2212 p0 \u2212 (n \u2212 1)p1\np0\np1\n(n \u2212 2)p1\n0\n\uf8ef\np0\n1 \u2212 p0 \u2212 (n \u2212 1)p1\n0\n0\n(n \u2212 1)p1 \uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\n\uf8fa.\nPH = \uf8ef\np1\n0\n1 \u2212 p1 \u2212 (n \u2212 2)p2 (n \u2212 2)p2\n0\n\uf8fa\n\uf8f0\n\uf8fb\np1\n0\np2\n1 \u2212 p1 \u2212 p2\n0\n0\np1\n0\n0\n1 \u2212 p1\nBy equation (17), the stationary distribution of the orbit chain is\n\u0013\n\u0012\n1\n1 n\u22122 n\u22121\n1\n,\n,\n,\n,\n.\n\u03c0H =\n2n 2n 2n\n2n\n2n\n\nAs the third step, we solve the SDP (19) with the above parametrization. It is remarkable to\nsee that we only need to solve an SDP with 4 variables (three transition probabilities p0 , p1 , p2 ,\nand the extra scalar s) and 5 \u00d7 5 matrices no matter how large the graph (the number n) is.\nWe will revisit this example in \u00a75.3.4 using the block diagonalization method, where we present\nan analytic expression for the exact optimal SLEM and corresponding transition probabilities.\n4.3.2\n\nComplete binary tree\n\nWe consider a complete binary tree with n levels of branches, denoted as Tn . The total number\nof nodes is |V| = 2n+1 \u2212 1. The matrix inequalities in the corresponding SDP have size |V| \u00d7 |V|,\nwhich is clearly exponential in n. However, the binary tree has a very large automorphism group,\nn\nof size 2(2 \u22121) . This automorphism group is best described recursively. Plainly, for n = 1, we have\nAut(T1 ) = S2 . For n > 1, it can be obtained by the recursion\nAut(Tk+1 ) = Aut(Tk ) \u2240 S2 ,\n\nk = 1, . . . , n \u2212 1,\n\nwhere \u2240 represents the wreath product of two groups (e.g., [JK81]). More specifically, let g = (g1 , g2 )\nand h = (h1 , h2 ) be elements of the product group Aut(Tk ) \u00d7 Aut(Tk ), and \u03c3 and \u03c0 be in S2 . The\nmultiplication rule of the wreath product is\n\u0001\n(g, \u03c3)(h, \u03c0) = (g1 h\u03c3\u22121 (1) , g2 h\u03c3\u22121 (2) ), \u03c3\u03c0 .\n\nThis is a semi-direct product Aut(Tk )2 \u22ca S2 (cf. the automorphism group of Kn -Kn ). From the\nabove recursion, the automorphism group of Tn is\nAut(Tn ) = S2 \u2240 S2 \u2240 * * * \u2240 S2\n\n(n times).\n\n(The wreath product is associative, but not commutative.) The representation theory of the automorphism group of the binary tree has been thoroughly studied as this group is the Sylow\n2-subgroup of a symmetric group; see [OOR04, AV05].\n20\n\n\f2p1\n2p2\n2p3\n\np1\n\np2\n\np3\n\n(a)\n\nOrbit graph and chain under S2 \u2240 S2 \u2240 S2 .\n\n(c)\n\nOrbit graph under (S2 \u00d7 S2 ) \u00d7 (S2 \u2240 S2 ).\n\n(b)\n\nOrbit graph under (S2 \u2240 S2 ) \u00d7 (S2 \u2240 S2 ).\n\n(d)\n\nOrbit graph under S2 \u00d7 (S2 \u2240 S2 ).\n\nFigure 10: Orbit graphs of the complete binary tree Tn (n = 3) under different automorphism\ngroups. The vertices surrounded by a circle are fixed points of the corresponding automorphism\ngroup.\nThe orbit graph of Tn under its full automorphism group is a path with n+1 nodes (Figure 10(a),\nleft). Since there are n orbits of edges, there are n different transition probabilities we need to\nconsider. We label them as pk , k = 1, . . . , n, from top to bottom of the tree. The corresponding\norbit chain, represented by a directed graph labeled with transition probabilities between orbits,\nis shown on the right of Figure 10(a). To simplify presentation, only the orbit graphs are shown\nin other subfigures of Figure 10. The corresponding orbit chains should be straightforward to\nconstruct.\nThe largest subgroup of Aut(Tn ) that has a fixed point in every orbit under Aut(Tn ) is\nWn =\n\nn\u22121\nY\nk=1\n\n(S2 \u2240 * * * \u2240 S2 ) (k times)\n\nQ\n\nwhere\ndenotes direct product of groups. The corresponding orbit graph is shown in Figure 10(d)\nfor n = 3. The number of vertices in this orbit graph is\n\u0012\n\u0013\nn+1\n1\n1 + 2 + * * * + n + (n + 1) =\n= (n + 1)(n + 2),\n2\n2\nwhich is much smaller than 2n+1 \u2212 1, the size of Tn .\nFrom the above analysis,\u0001we only need to solve the fastest reversible Markov chain problem on\nthe orbit graph of size n+1\nwith n variables p1 , . . . , pn . In next section, using the technique of\n2\n\u0001\nblock diagonalization, we will see that the transition probability matrix of size n+1\ncan be further\n2\ndecomposed into smaller matrices with sizes 1, 2, . . . , n + 1. Due to an eigenvalue interlacing result,\nwe only need to consider the orbit chain with 2n + 1 vertices in Figure 10(b).\n\n21\n\n\f5\n\nSymmetry reduction by block diagonalization\n\nBy definition of the fixed-point subspace F (in \u00a72.2), any transition probability matrix P \u2208 F\nis invariant under the actions of Aut(G). More specifically, for any permutation matrix Q given\nby \u03c3 \u2208 Aut(G), we have QP QT = P , equivalently QP = P Q. In this section we show that this\nproperty allows the construction of a coordinate transformation matrix that can block diagonalize\nevery P \u2208 F. The resulting blocks usually have much smaller sizes and repeated blocks can be\ndiscarded in computation.\nThe method we use in this section is based on classical group representation theory (e.g.,\n[Ser77]). It was developed for more general SDPs in [GP04], and has found applications in sumof-squares decomposition for minimizing polynomial functions [Par00, Par03, PS03] and controller\ndesign for symmetric dynamical systems [CLP03]. A closely related approach is developed in\n[dKPS07], which is based on a low-order representation of the commutant (collection of invariant\nmatrices) of the matrix algebra generated by the permutation matrices.\n\n5.1\n\nSome group representation theory\n\nLet G be a group. A representation \u03c1 of G assigns an invertible matrix \u03c1(g) to each g \u2208 G in such\na way that the matrix assigned to the product of two elements in G is the product of the matrices\nassigned to each element: \u03c1(gh) = \u03c1(g)\u03c1(h). The matrices we work with are all invertible and are\nconsidered over the real or complex numbers. We thus regard \u03c1 as a homomorphism from g to the\nlinear maps on a vector space V . The dimension of \u03c1 is the dimension of V . Two representations\nare equivalent if they are related by a fixed similarity transformation.\nIf W is a subspace of V invariant under G, then \u03c1 restricted to W gives a subrepresentation.\nOf course the zero subspace and the subspace W = V are trivial subrepresentations. If the representation \u03c1 admits no non-trivial subrepresentation, then \u03c1 is called irreducible.\nWe consider first complex representations, as the theory is considerably simpler in this case.\nFor a finite group G there are only finitely many inequivalent irreducible representations \u03b81 , . . . , \u03b8h\nof dimensions\nPh n1 ,2. . . , nh , respectively. The degrees ni divide the group order |G|, and satisfy the\ncondition i=1 ni = |G|. Every linear representation of G has a canonical decomposition as a\ndirect sum of irreducible representations\n\u03c1 = m1 \u03b8 1 \u2295 m2 \u03b8 2 \u2295 * * * \u2295 mh \u03b8 h ,\nwhere m1 , . . . , mh are the multiplicities. Accordingly, the representation space Cn has an isotypic\ndecomposition\nC n = V1 \u2295 * * * \u2295 Vh\n(21)\nwhere each isotypic components consists of mi invariant subspaces\nVi = Vi1 \u2295 * * * \u2295 Vimi ,\n\n(22)\n\neach of which has dimension ni and transforms after the manner of \u03b8i . A basis of this decomposition\ntransforming with respect to the matrices \u03b8i (g) is called symmetry-adapted and can be computed\nusing the algorithm presented in [Ser77, \u00a72.6-2.7] or [FS92, \u00a75.2]. This basis defines a change of\ncoordinates by a matrix T collecting the basis as columns. By Schur's lemma, if a matrix P satisfies\n\u03c1(g)P = P \u03c1(g),\n\n22\n\n\u2200g \u2208 G,\n\n(23)\n\n\fthen T \u22121 P T has block diagonal form with one block Pi for each isotypic component of dimension\nmi ni , which further decomposes into ni equal blocks Bi of dimension mi . That is\n\uf8f9\n\uf8ee\n\uf8f9\n\uf8ee\nP1\n0\nBi\n0\n\uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\n..\n..\n(24)\nT \u22121 P T = \uf8f0\nPi = \uf8f0\n\uf8fb.\n\uf8fb,\n.\n.\n0\n\nPh\n\n0\n\nBi\n\nFor our application of semidefinite programs, the problems are presented in terms of real matrices, and therefore we would like to use real coordinate transformations. In fact a generalization\nof the classical theory to the real case is presented in [Ser77, \u00a713.2]. If all \u03b8i (g) are real matrices\nthe irreducible representation is called absolutely irreducible. Otherwise, for each \u03b8i with complex\ncharacter its complex conjugate will also appear in the canonical decomposition. Since \u03c1 is real\nboth will have the same multiplicity and real bases of Vi + V\u0304i can be constructed. So two complex conjugate irreducible representations form one real irreducible representation of complex type.\nThere is a third case, real irreducible representations of quaternonian type, rarely seen in practical\nexamples.\nIn this paper, we assume that the representation \u03c1 is orthogonal, i.e., \u03c1(g)T \u03c1(g) = \u03c1(g)\u03c1(g)T = I\nfor all g \u2208 G. As a result, the transformation matrix T can also be chosen to be orthogonal. Thus\nT \u22121 = T T (for complex matrices, it is the conjugate transpose). For symmetric matrices the block\ncorresponding to a representation of complex type or quaternonian type simplifies to a collection\nof equal subblocks. For the special case of circulant matrices, complete diagonalization reveals all\nthe eigenvalues [Dia88, page 50].\n\n5.2\n\nBlock diagonalization of SDP constraint\n\nAs in \u00a72.2, for every \u03c3 \u2208 Aut(G) we assign a permutation matrix Q(\u03c3) by letting Qij (\u03c3) = 1 if\ni = \u03c3(j) and Qij (\u03c3) = 0 otherwise. This is an n-dimensional representation of Aut(G), which\nis often called the natural representation. As mentioned in the beginning of this section, every\nmatrix P in the fixed-point subset F has the symmetry of Aut(G); i.e., it satisfies the condition (23)\nwith \u03c1 = Q. Thus a coordinate transformation matrix T can be constructed such that P can be\nblock diagonalized into the form (24).\nNow we consider the SDP (5), which is the FMMC problem\nformulated in the fixed-point\nPN\nsubset F. In \u00a72.3, we have derived the expression P (p) = I \u2212 k=1 pk Lk , where Lk is the Laplacian\nmatrix for the kth orbit graph and pk is the common transition probability assigned on all edges in\nthe kth orbit graph. Note the matrix P (p) has the symmetry of Aut(G). Applying the coordinate\ntransformation T to the linear matrix inequalities, we obtain the following equivalent problem\nminimize\n\ns\n\nsubject to \u2212sImi \u0016 Bi (p) \u2212 Ji \u0016 sImi ,\n\ni = 1, . . . , h\n\npk \u2265 0, k = 1, . . . , N\nPN\ni = 1, . . . , n\nk=1 (Lk )ii pk \u2264 1,\n\n(25)\n\nwhere Bi (p) correspond to the small blocks Bi in (24) of the transformed matrix T T P (p)T , and Ji\nare the corresponding diagonal blocks of T T (1/n)11T T . The number of matrix inequalities h is the\nnumber of inequivalent irreducible representations, and the size of each matrix inequality mi is the\nmultiplicity of the corresponding irreducible representation. Note that we only need one out of ni\ncopies of each Bi in the decomposition (24). Since mi can be much smaller than n (the number of\nvertices in the graph), the improvement in computational complexity over the SDP formulation (5)\n23\n\n\fcan be significant (see the flop counts discussed in \u00a71.2). This is especially the case when there\nare high-dimensional irreducible representations (i.e., when ni is large; see, e.g., Kn -Kn defined\nin \u00a74.3.1).\nThe transformed SDP formulation (25) needs some further justification. Namely, all the offdiagonal blocks of the matrix T T (1/n)11T T have to be zero. This is in fact the case. Moreover,\nthe following theorem reveals an interesting connection between the block diagonalization approach\nand the orbit theory in \u00a74.\nTheorem 5.1. Let H be a subgroup of Aut(G), and T be the coordinate transformation matrix\nwhose columns are a symmetry-adapted basis for the natural representation of H. Suppose a Markov\nchain P defined on the graph has the symmetry of H. Then the matrix T T (1/n)11T T has the same\nblock diagonal form as T T P T . Moreover, there is only one nonzero block. Without loss of generality,\nlet this nonzero block be J1 and the corresponding block of T T P T be B1 . These two blocks relate to\nthe orbit chain PH by\nB1 = \u03a01/2 PH \u03a0\u22121/2\n\n(26)\n\nT\n\n(27)\n\nJ1 = qq\nwhere \u03a0 = Diag(\u03c0H ), q =\n\n\u221a\n\n\u03c0H , and \u03c0H is the stationary distribution of PH .\n\nProof. First we note that P always has a single eigenvalue 1 with associated eigenvector 1. Thus 1\nspans an invariant subspace of the natural representation, which is obviously irreducible. The corresponding irreducible representation is isomorphic to the trivial representation (which assigns the\nscalar 1 to every element in the group). Without loss of generality, let V1 be the isotypic component\nthat contains the vector 1. Thus V1 is a direct product of H-fixed vectors (each corresponds to a\ncopy of the trivial representation), and 1 is a linear combination of these vectors.\nLet m1 be the dimension of V1 , which is the number of H-fixed vectors. We can calculate m1\nby Frobenius reciprocity, or \"Burnside's Lemma\"; see, e.g., [Ser77]. To do so, we note that the\ncharacter \u03c7 of the natural representation Q(g), g \u2208 H, is the number of fixed points of g, i.e.,\n\u03c7(g) = Tr Q(g) = FP(g) = #{v \u2208 V : g(v) = v}.\n\"Burnside's Lemma\" says that\n\n1 X\nFP(g) = #orbits.\n|H|\ng\u2208H\n\nThe left-hand side is the inner product of \u03c7 with the trivial representation. It thus counts the\nnumber of H-fixed vectors in V . So m1 equals the number of orbits under H.\np\nSuppose that V = O1 \u222a . . . \u222a Om1 as a disjoint union of H-orbits. Let bi (v) = 1/ |Oi | if\nv \u2208 Oi and zero otherwise. Then b1 , . . . , bm1 are H-fixed vectors, and they form an orthonormal\nsymmetry-adapted basis for V1 (these are not unique). Let T1 = [b1 * * * bm1 ] be the first m1 columns\nof T . They are orthogonal to all other columns of T . Since 1 is a linear combination of b1 , . . . , bm1 ,\nit is also orthogonal to other columns of T . Therefore the matrix T T (1/n)11T T has all its elements\nzero except for the first m1 \u00d7 m1 diagonal block, which we denote as J1 . More specifically, J1 = qq T\nwhere\nq =\n=\n\n\u0003T\n1\n1 \u0002\n\u221a T1T 1 = \u221a bT1 1 * * * bTm1 1\nn\nn\n\"\n#T \"r\n#T\nr\n|Om1 |\n|O1 |\n|O1 |\n|Om1 |\n1\n\u221a\np\n*** p\n=\n...\n.\nn\nn\nn\n|O1 |\n|Om1 |\n24\n\n\f1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nFigure 11: A 3 \u00d7 3 grid graph.\nNote that by (17) the stationary distribution of the orbit chain PH is\n\u03c0H =\n\n\u0014\n\n|O1 |\n|Om1 |\n***\nn\nn\n\n\u0015T\n\n.\n\n\u221a\nThus we have q = \u03c0H . This proves (27).\nFinally we consider the relationship between B1 = T1T P T1 and PH . We prove (26) by showing\n\u03a0\u22121/2 B1 \u03a01/2 = \u03a0\u22121/2 T1T P T1 \u03a01/2 = PH .\nIt is straightforward to verify that\n\u22121/2\n\n\u03a0\n\nT1T\n\nT1 \u03a01/2\n\n\uf8f9\nb\u2032T\n1\n\u221a \uf8ef\n\uf8fa\n= n \uf8f0 ... \uf8fb ,\nb\u2032T\nm1\n\uf8ee\n\nb\u2032i (v)\n\n\u0003\n1 \u0002\n= \u221a b\u2032\u20321 * * * b\u2032\u2032m1 ,\nn\n\n1\n|Oi |\n=\n\uf8f3\n0\n\uf8f1\n\uf8f2\n\nb\u2032\u2032i (v) =\n\n(\n\n1\n\n0\n\nif v \u2208 Oi\nif v \u2208\n/ Oi\nif v \u2208 Oi\n\nif v \u2208\n/ Oi\n\nThe entry at the i-th row and j-th column of the matrix \u03a0\u22121/2 T1T P T1 \u03a01/2 are given by\n\u2032\u2032\nb\u2032T\ni P bj =\n\n1 X\n1 X X\nP (v, u) =\nPH (Oi , Oj ) = PH (Oi , Oj ).\n|Oi |\n|Oi |\nv\u2208Oi u\u2208Oj\n\nv\u2208Oi\n\nIn the last equation, we have used the fact that PH (Oi , Oj ) is independent of which v \u2208 Oi is\nchosen. This completes the proof.\nFrom Theorem 5.1, we know that B1 contains the eigenvalues of the orbit chain under H.\nOther blocks Bi contain additional eigenvalues (not including those of PH ) of the orbit chains\nunder various subgroups of H. (Note that the eigenvalues of the orbit chain under H are always\ncontained in the orbit chain under its subgroups). With this observation, it is possible to identify\nthe multiplicities of eigenvalues in orbit chains under various subgroups of Aut(G) by relating to\nthe decompositions (21), (22) and (24) (some preliminary results are discussed in [BDPX05]).\n5.2.1\n\nA running example\n\nAs a running example for this section, we consider a Markov chain on a 3 \u00d7 3 grid G, with a total of\n9 nodes (see Figure 11). The automorphism group Aut(G) is isomorphic to the 8-element dihedral\n25\n\n\fgroup D4 , and corresponds to flips and 90-degree rotations of the graph. The orbits of Aut(G)\nacting on the vertices and edges are\n{1, 3, 7, 9},\n\n{5},\n\n{2, 4, 6, 8}\n\nand\n{{1, 2}, {1, 4}, {2, 3}, {3, 6}, {4, 7}, {7, 8}, {6, 9}, {8, 9}},\n\n{{2, 5}, {4, 5}, {5, 6}, {5, 8}},\n\nrespectively. So G is neither vertex- nor edge-transitive.\nBy Corollary 2.2, we associate transition probabilities a and b to the two edge orbits, respectively.\nThe transition probability matrix has the form\n\uf8ee\n\uf8f9\n1\u22122a\na\n0\na\n0\n0\n0\n0\n0\n\uf8ef a\n1\u22122a\u2212b\na\n0\nb\n0\n0\n0\n0 \uf8fa\n\uf8ef\n\uf8fa\n\uf8ef 0\na\n1\u22122a\n0\n0\na\n0\n0\n0 \uf8fa\n\uf8ef\n\uf8fa\n\uf8ef a\n0\n0\n1\u22122a\u2212b\nb\n0\na\n0\n0 \uf8fa\n\uf8ef\n\uf8fa\n\uf8fa.\nP =\uf8ef\n0\nb\n0\nb\n1\u22124b\nb\n0\nb\n0\n\uf8ef\n\uf8fa\n\uf8ef 0\n0\na\n0\nb\n1\u22122a\u2212b\n0\n0\na \uf8fa\n\uf8ef\n\uf8fa\n\uf8ef 0\n0\n0\na\n0\n0\n1\u22122a\na\n0 \uf8fa\n\uf8ef\n\uf8fa\n\uf8f0 0\n0\n0\n0\nb\n0\na\n1\u22122a\u2212b\na \uf8fb\n0\n0\n0\n0\n0\na\n0\na\n1\u22122a\n\nThe matrix P satisfies Q(\u03c3)P = P Q(\u03c3)\nwe found a symmetry-adapted basis for\n\uf8ee\n0 1 0\n\uf8ef 0 0 1\n\uf8ef\n\uf8ef 0 1 0\n\uf8ef\n\uf8ef\n0 0 1\n1\uf8ef\n\uf8ef\nT = \uf8ef 2 0 0\n2\uf8ef\n\uf8ef 0 0 1\n\uf8ef\n\uf8ef 0 1 0\n\uf8ef\n\uf8f0 0 0 1\n0 1 0\n\nfor every \u03c3 \u2208 Aut(G). Using the algorithm in [FS92, \u00a75.2],\nthe representation Q, which we take as columns to form\n\uf8f9\n\u221a\n1\n0\n2\n0\n0\n0\n0 \u22121\n0\n1\n1 \uf8fa\n\uf8fa\n\u221a0\n\u22121\n0\n0\n0\n2\n0 \uf8fa\n\uf8fa\n\uf8fa\n0\n1\n0\n1\n0 \u22121 \uf8fa\n\uf8fa\n0\n0\n0\n0\n0\n0 \uf8fa.\n\uf8fa\n0\n1\n0 \u22121\n1 \uf8fa\n\u221a0\n\uf8fa\n\u22121\n0\n0\n0 \u2212 2\n0 \uf8fa\n\uf8fa\n\uf8fb\n0 \u22121\n0\n\u22121\n0\n\u22121\n\u221a\n0\n0\n0\n1\n0 \u2212 2\n\nWith this coordinate transformation matrix, we obtain\n\uf8ee\n1\u22124b\n0\n2b\n\uf8ef 0\n1\u22122a\n2a\n\uf8ef\n\uf8ef 2b\n2a\n1\u22122a\u2212b\n\uf8ef\n\uf8ef\n1\u22122a\n\uf8ef\n\uf8ef\n1\u22122a\u2212b\nTTPT = \uf8ef\n\u221a\n\uf8ef\n1\u22122a\n2a\n\uf8ef\n\u221a\n\uf8ef\n\uf8ef\n2a 1\u22122a\u2212b\n\u221a\n\uf8ef\n\uf8f0\n1\u22122a\n2a\n\u221a\n2a 1\u22122a\u2212b\n\n\uf8f9\n\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa.\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\nThe 3-dimensional block B1 contains the single eigenvalue 1, and it is related to the orbit chain in\nFigure 12 by the equation (26). The corresponding nonzero block of T T (1/n)11T T is\n\uf8ee\n\uf8f9\n1 2 2\n1\nJ1 = \uf8f0 2 4 4 \uf8fb .\n9\n2 4 4\n26\n\n\f2a\nO1\n\nO2\n\nb\nO5\n\n2a\n4b\nFigure 12: The orbit chain of the 3 \u00d7 3 grid graph.\nNext, we substitute the above expressions into the SDP (25) and solve it numerically. Since\nthere are repeated 2 \u00d7 2 blocks, the original 9 \u00d7 9 matrix is replaced by four smaller blocks, of\ndimension 3,1,1,2. The optimal solutions are\na\u22c6 \u2248 0.363,\n\nb\u22c6 \u2248 0.2111,\n\n\u03bc\u22c6 \u2248 0.6926.\n\nInterestingly, it can be shown that these optimal values are not rational, but instead algebraic\nnumbers with defining minimal polynomials:\n18157 a5 \u2212 17020 a4 + 6060 a3 \u2212 1200 a2 + 180 a \u2212 16 = 0\n\n1252833 b5 \u2212 1625651 b4 + 791936 b3 \u2212 173536 b2 + 15360 b \u2212 256 = 0\n\n54471 \u03bc5 \u2212 121430 \u03bc4 + 88474 \u03bc3 \u2212 18216 \u03bc2 \u2212 2393 \u03bc + 262 = 0.\n\n5.3\n\nExamples\n\nWe revisit some previous examples with the block diagonalization method, and draw connections\nto the method based on orbit theory in \u00a74. We also discuss some additional examples that are\ndifficult if one uses the orbit theory, but are nicely handled by block diagonalization. In many of\nthe examples, the coordinate transformation matrix T can be constructed directly by inspection.\n5.3.1\n\nComplete bipartite graphs\n\nFor the complete bipartite graph Km,n (see Figure 4), This graph is edge-transitive, so we can\nassign the same transition probability p on all the edges. The transition probability matrix has the\nform\n\u0014\n\u0015\n(1 \u2212 np)Im\np 1m\u00d7n\nP (p) =\np 1n\u00d7m\n(1 \u2212 mp)In\nWe can easily find a decomposition of the associated matrix algebra. It will have three blocks, and\nan orthogonal block-diagonalizing change of basis is given by\n\u0015\n\u0014\n\u221a\n0\nFm 0\n(1/ m)1m\u00d71\n\u221a\nT =\n0\n(1/ n)1n\u00d71 0 Fm\nwhere Fn is an n \u00d7 (n \u2212 1) matrix whose columns are an orthogonal basis of the subspace complementary to that generated by 1n\u00d71 .\nIn the new coordinates, the matrix T T P (p)T has the following diagonal blocks\n\u0015\n\u0014\n\u221a\n1 \u2212 mp p nm\n\u221a\n,\nIn\u22121 \u2297 (1 \u2212 mp),\nIm\u22121 \u2297 (1 \u2212 np).\np nm 1 \u2212 np\nThe 2 \u00d7 2 block has eigenvalues 1 and 1 \u2212 (m + n)p. The other diagonals reveal the eigenvalue\n1 \u2212 mp and 1 \u2212 np, with multiplicities n \u2212 1 and m \u2212 1, respectively. The optimal solution to the\nFMMC problem can be easily obtained as in (10) and (11).\n27\n\n\fTo draw connections to the orbit theory, we note that the above 2 \u00d7 2 block is precisely B1 in\nthe equation (26), and the corresponding PH is the orbit chain shown in Figure 8(a). In addition\nto the two eigenvalues in B1 , the extra eigenvalue in the orbit chain of Figure 8(b) is 1 \u2212 np, and\nthe extra eigenvalue in Figure 8(c) is 1 \u2212 mp. All these eigenvalues appear in the orbit chain in\nFigure 8(d). As we have seen, the block diagonalization technique reveals the multiplicities in the\noriginal chain of the eigenvalues from various orbit chains.\n5.3.2\n\nComplete k-partite graphs\n\nThe previous example generalizes nicely to the complete\nk-partite graph Kn1 ,...,nk . In this case, the\nP\nfixed-point reduced matrix will have dimensions i ni , and the structure\nP\n\uf8ee\n\uf8f9\n(1 \u2212 j6=1 nj p1j )In1\np12 1n1 \u00d7n2\n***\np1k 1n1 \u00d7nk\nP\n\uf8ef\n\uf8fa\np21 1n2 \u00d7n1\n(1 \u2212 j6=2 nj p2j )In2 * * *\np2k 1n2 \u00d7nk\n\uf8ef\n\uf8fa\nP (p) = \uf8ef\n\uf8fa\n..\n..\n..\n..\n\uf8f0\n\uf8fb\n.\n.\n.\n.\nP\npk1 1nk \u00d7n1\npk2 1nk \u00d7n2\n* * * (1 \u2212 j6=k nj pkj )Ink\n\u0001\nwhere the probabilities satisfy pij = pji . There are total k2 independent variables.\nIn a very similar fashion to the bipartite case, we can explicitly write the orthogonal coordinate\ntransformation matrix\n\uf8f9\n\uf8ee\n\u221a\n0\nFn1 . . . 0\n(1/ n1 )1n1 \u00d71 . . .\n\uf8ef\n..\n..\n..\n.. \uf8fa .\n..\n..\nT =\uf8f0\n.\n.\n.\n.\n.\n. \uf8fb\n\u221a\n0\n. . . (1/ nk )1nk \u00d71 0 . . . Fnk\nThe matrix T T P (p)T decomposes into k + 1 blocks: one of dimension k, with the remaining k\nblocks each having dimension ni \u2212 1. The decomposition is:\nP\n\uf8f9\n\uf8ee\n\u221a\n\u221a\n***\np1k n1 nk\n(1 \u2212 j6=1 nj p1j )\np12 n1 n2\nP\n\u221a\n\u221a\n\uf8fa\n\uf8ef\np21 n2 n1\n(1 \u2212 j6=2 nj p2j ) * * *\np2k n2 nk\n\uf8fa\n\uf8ef\n\uf8fa,\n\uf8ef\n..\n..\n.\n.\n.\n.\n\uf8fb\n\uf8f0\n.\n.\n.\n.\nP\n\u221a\n\u221a\npk2 nk n2\n* * * (1 \u2212 j6=k nj pkj )\npk1 nk n1\nIni \u22121 \u2297 (1 \u2212\n\nX\n\nnj pij ),\n\ni = 1, . . . , k.\n\nj6=i\n\nThese blocks can be substituted into the SDP (25) to solve the FMMC problem.\n5.3.3\n\nWheel graph\n\nThe wheel graph consists of a center vertex (the hub) and a ring of n peripheral vertices, each\nconnected to the hub; see Figure 13. It has total n+1 nodes. Its automorphism group is isomorphic\nto the dihedral group Dn with order 2n. The transition probability matrix has the structure\n\uf8f9\n\uf8ee\n1 \u2212 np\np\np\n...\np\np\n\uf8fa\n\uf8ef\np\n1 \u2212 p \u2212 2q\nq\n...\n0\nq\n\uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\np\nq\n1 \u2212 p \u2212 2q . . .\n0\n0\n\uf8fa\n\uf8ef\n(28)\nP =\uf8ef\n\uf8fa,\n.\n.\n.\n.\n.\n.\n..\n..\n..\n..\n..\n..\n\uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\n\uf8fb\n\uf8f0\np\n0\n0\n. . . 1 \u2212 p \u2212 2q\nq\np\nq\n0\n...\nq\n1 \u2212 p \u2212 2q\n28\n\n\fFigure 13: The wheel graph with n = 9 (total 10 nodes).\n\nwhere p and q are the transition probabilities between the hub and each peripheral vertex, and\nbetween adjacent peripheral vertices, respectively.\nFor this structure, the block-diagonalizing transformation is given by\nT = Diag(1, Fn ),\n\n1 2\u03c0\u0131(j\u22121)(k\u22121)\nn\n[Fn ]jk = \u221a e\nn\n\nwhere Fn is the unitary Fourier matrix of size n \u00d7 n. As a consequence, the matrix T \u22121 P T is block\ndiagonal with a 2 \u00d7 2 matrix and n \u2212 1 scalars on its diagonal, given by\n\u0014\n\u0015\n\u221a\n1 \u2212 np\nnp\n\u221a\nnp 1 \u2212 p\nand\n2\u03c0\u0131\nn\n\n1 \u2212 p + (\u03c9nk + \u03c9n\u2212k \u2212 2) * q,\n\nk = 1, . . . , n \u2212 1\n\nwhere \u03c9n = e\nis an elementary n-th root of unity. The 2 \u00d7 2 block is B1 , which contains\neigenvalues of the orbit chain under Dn (it has only two orbits).\nWith the above decomposition, we obtain the optimal solution to the FMMC problem in closed\nform\n1 \u2212 n1\n1\n.\nq\u22c6 =\np\u22c6 = ,\nn\n2 \u2212 cos 2\u03c0 \u2212 cos 2\u230an/2\u230b\u03c0\nn\n\nn\n\nThe optimal value of the SLEM is\n\u22c6\n\n\u03bc =\n\n\u0012\n\n1\n1\u2212\nn\n\n\u0013\n\n2\u230an/2\u230b\u03c0\ncos 2\u03c0\nn \u2212 cos\nn\n\n2\u230an/2\u230b\u03c0\n2 \u2212 cos 2\u03c0\nn \u2212 cos\nn\n\n.\n\nCompared with the optimal solution for the cycle graph in (8) and (9), we see an extra factor\nof 1 \u2212 1/n in both the SLEM and the transition probability between peripheral vertices. This is\nexactly the factor improved by adding the central hub over the pure n-cycle case.\nThe wheel graph is an example for which the block diagonalization technique works out nicely,\nwhile the orbit theory leads to much less reduction. Although there are only two orbits under\nthe full automorphism group, any orbit graph that has a fixed peripheral vertex will have at least\n(n + 1)/2 orbits (the corresponding symmetry is the reflection through that vertex).\n\n29\n\n\f5.3.4\n\nKn -Kn\n\nWe did careful symmetry analysis for the graph Kn -Kn in \u00a74.3.1; see Figure 9. The transition\nprobability matrix on this graph has the structure\n\uf8ee\n\uf8f9\nC\np1 1\n0\n0\n\uf8ef p1 1T 1 \u2212 p0 \u2212 (n \u2212 1)p1\np0\n0 \uf8fa\n\uf8fa\nP =\uf8ef\n\uf8f0 0\np0\n1 \u2212 p0 \u2212 (n \u2212 1)p1 p1 1T \uf8fb\n0\n0\np1 1\nC\nwhere C is a circulant matrix\n\nC = (1 \u2212 p1 \u2212 (n \u2212 3)p2 )In\u22121 + p2 1(n\u22121)\u00d7(n\u22121) .\nSince circulant matrices are diagonalized by\nmatrix\n\uf8ee\nFn\u22121\n\uf8ef 0\nT1 = \uf8ef\n\uf8f0 0\n0\n\nFourier matrices, we first use the transformation\n\uf8f9\n0 0\n0\n1 0\n0 \uf8fa\n\uf8fa\n0 1\n0 \uf8fb\n0 0 Fn\u22121\n\nwhere Fn\u22121 is the unitary Fourier matrix of dimension n \u2212 1. This corresponds to block diagonalization using the symmetry group Sn\u22121 \u00d7 Sn\u22121 , which is a subgroup of Aut(Kn -Kn ). The matrix\nT1\u22121 P T1 has diagonal blocks\n\u221a\n\uf8ee\n\uf8f9\nn \u2212 1p1\n0\n0\n1\n\u2212\np\n1\n\u221a\n\uf8ef n \u2212 1p1 1 \u2212 p0 \u2212 (n \u2212 1)p1\n\uf8fa\np0\n\uf8fa\n\u221a 0\nB1\u2032 = \uf8ef\n\uf8f0\nn \u2212 1p1 \uf8fb\n0\np0\n1 \u2212 p\u221a\n0 \u2212 (n \u2212 1)p1\nn \u2212 1p1\n0\n0\n1 \u2212 p1\nand\n\nI2n\u22124 \u2297 (1 \u2212 p1 \u2212 (n \u2212 1)p2 ).\n\n(29)\n\nFrom this we know that P has an eigenvalue 1 \u2212 p1 \u2212 (n \u2212 1)p2 with multiplicity 2n \u2212 4, and\nthe remaining four eigenvalues are the eigenvalues of the above 4 \u00d7 4 block B1\u2032 . The block B1\u2032\ncorresponds to the orbit chain under the symmetry group H = Sn\u22121 \u00d7 Sn\u22121 . More precisely,\nB1\u2032 = \u03a01/2 PH \u03a0\u22121/2 , where \u03a0 = Diag(\u03c0H ), PH and \u03c0H are the transition probability matrix and\nstationary distribution of the orbit chain shown in Figure 9(c), respectively.\nExploring the full automorphism group of Kn -Kn , we can further block diagonalize B1\u2032 . Let\n\uf8ee\n\uf8f9\n\uf8ee\n\uf8f9\n1 0\n0\n1\nIn\u22122\n1 \uf8ef 0 1\n1\n0 \uf8fa\n\uf8fa.\n\uf8f0\n\uf8fb,\nT = T1\nT2\nT2 = \u221a \uf8ef\n\uf8f0 0 1 \u22121\n0 \uf8fb\n2\nIn\u22122\n1 0\n0 \u22121\nThe 4 \u00d7 4 block B1\u2032 is decomposed into\n\u0015\n\u0014\n\u221a\nn \u2212 1p1\n\u221a1 \u2212 p1\n,\nn \u2212 1p1 1 \u2212 (n \u2212 1)p1\n\n\u0014\n\n1 \u2212 2p\n\u221a0 \u2212 (n \u2212 1)p1\nn \u2212 1p1\n\n\u0015\n\u221a\nn \u2212 1p1\n1 \u2212 p1\n\nThe first block is B1 , which has eigenvalues 1 and 1 \u2212 np1 . By Theorem 5.1, B1 is related to the\norbit chain under Aut(Kn -Kn ) (see Figure 9(b)) by the equation (26). The second 2 \u00d7 2 block has\neigenvalues\np\n1 \u2212 p0 \u2212 (1/2)np1 \u00b1 (p0 + (1/2)np1 )2 \u2212 2p0 p1 .\n30\n\n\fThese are the eigenvalues contained in the orbit chain of Figure 9(c) but not in Figure 9(b).\nIn summary, the distinct eigenvalues of the Markov chain on Kn -Kn are\np\n1, 1 \u2212 np1 , 1 \u2212 p0 \u2212 (1/2)np1 \u00b1 (p0 + (1/2)np1 )2 \u2212 2p0 p1 , 1 \u2212 p1 \u2212 (n \u2212 1)p2\n\nwhere the last one has multiplicity 2n \u2212 4, and all the rest have multiplicity 1. To solve the FMMC\nproblem, we still need to solve the SDP (25). There are three blocks of matrix inequality constraints,\nwith sizes 2, 2, 1, respectively. Note that the total size is 5, which is exactly the size of the single\nmatrix inequality in the SDP (19) when we used the orbit theory to do symmetry reduction. As\nwe mentioned before, the huge reduction for Kn -Kn is due to the fact that it has an irreducible\nrepresentation with high dimension 2n \u2212 4 and multiplicity 1 (see [BDPX05, Proposition 2.4]). In\nthe decomposition (24), this means a block of size 1 repeated 2n \u2212 4 times; see equation (29).\nSince now the problem has been reduced to something much more tractable, we can even obtain\nan analytic expression for the optimal transition probabilities. The optimal solution for the Kn -Kn\ngraph (for n \u2265 2) is given by:\n\u221a\n\u221a\n\u221a\n\u221a\nn\u2212 2\nn+ 2\u22122\n2\u2212 2\n\u22c6\n\u22c6\n\u22c6\n\u221a ,\n\u221a ,\n\u221a .\np2 =\np1 =\np0 = ( 2 \u2212 1)\nn+2\u22122 2\nn+2\u22122 2\n(n \u2212 1)(n + 2 \u2212 2 2)\nThe corresponding optimal convergence rate is\n\u221a\nn\u22124+2 2\n\u221a .\n\u03bc =\nn+2\u22122 2\n\u221a\n\u0001\nFor large n, we have \u03bc\u22c6 = 1 \u2212 6\u22124n 2 + O n12 . This is quite close to the SLEM of a suboptimal\nconstruction with transition probabilities\n\u22c6\n\n1\np0 = ,\n2\n\np1 = p2 =\n\n1\n.\n2(n \u2212 1)\n\n(30)\n\n\u0001\n1\nAs shown in [BDPX05], the corresponding SLEM is of the order \u03bc = 1 \u2212 3n\n+ O n12 ; here we\n\u221a\nhave 6 \u2212 4 \u221a2 \u2248 0.3431. The limiting value of the optimal transition probability between the two\nclusters is 2 \u2212 1 \u2248 0.4142.\n5.3.5\n\nComplete binary trees\n\nSince the automorphism groups of the complete binary trees Tn are given recursively (see \u00a74.3.2),\nit is also convenient to write the transition probability matrices in a recursive form. We start from\nthe bottom by considering the last level of branches. If we cut-off the rest of the tree, the last level\nhas three nodes and two edges with the transition probability matrix\n\uf8ee\n\uf8f9\n1 \u2212 2pn\npn\npn\n\uf8fb.\npn\n1 \u2212 pn\n0\nPn = \uf8f0\n(31)\npn\n0\n1 \u2212 pn\n\nFor the tree with n levels Tn , the transition matrix P1 can be computed from the recursion\n\uf8f9\n\uf8ee\npk\u22121 eTk\n1 \u2212 2pk\u22121\npk\u22121 eTk\n\uf8fb,\nPk \u2212 pk\u22121 ek eTk\n0\nk = n, n \u2212 1 . . . , 2\nPk\u22121 = \uf8f0 pk\u22121 ek\nT\npk\u22121 ek\n0\nPk \u2212 pk\u22121 ek ek\n31\n\n(32)\n\n\fwhere ek = [1 0 . . . 0], a unit vector in Rtk with tk = 2k+1 \u2212 1.\nThe coordinate transformations are also best written in recursive form. Let\n\u0014\n\u0015\n1\n1 1\n\u221a\n,\nTn Diag(1, F2 ),\nF2 =\n2 1 \u22121\nand define the matrices\nTk\u22121 = Diag(1, F2 \u2297 Tk ),\n\nk = n, n \u2212 1, . . . , 2.\n\nIt is clear that all the Tk are orthogonal. It is easy to verify that Tn block-diagonalizes Pn\n\u221a\n\uf8ee\n\uf8f9\n1\u221a\n\u2212 2pn\n2pn\n0\n\uf8fb.\nTnT Pn Tn = \uf8f0\n2pn 1 \u2212 pn\n0\n0\n0\n1 \u2212 pn\n\nIn fact Tk block-diagonalizes Pk , and the transformed matrices can be obtained recursively\n\u221a\n\uf8ee\n\uf8f9\nT\n2p\ne\n0\n1\n\u2212\n2p\nk\u22121\nk\u22121\nk\n\u221a\nT\n\uf8fb\nPk\u22121 Tk\u22121 = \uf8f0 2pk\u22121 ek TkT Pk Tk \u2212 pk\u22121 ek eTk\nTk\u22121\n0\nT\nT\n0\n0\nTk Pk Tk \u2212 pk\u22121 ek ek\n\nfor k = n, n \u2212 1, . . . , 2.\nThe matrix T1T P1 T1 has a very special structure. It has n + 1 distinct blocks, each with size\n1, . . . , n + 1, respectively. Order these blocks with increasing sizes as B1 , B2 , . . . , Bn+1 . The largest\nblock of size n + 1 is\n\u221a\n\uf8f9\n\uf8ee\n2p1\n1\u22122p\n\u221a 1\n\u221a\n\uf8fa\n\uf8ef\n2p1 1\u2212p\n2p2\n\uf8fa\n\uf8ef\n\u221a\n\u221a1 \u22122p2\n\uf8fa\n\uf8ef\n2p\n1\u2212p\n\u22122p\n2p\n2\n2\n3\n3\n\uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\n\uf8fa.\n\uf8ef\nBn+1 = \uf8ef\n..\n..\n..\n\uf8fa\n.\n.\n.\n\uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\n\uf8fa\n\uf8ef\n\u221a\n\u221a\n\uf8f0\n2pn\u22121 1\u2212p\u221a\n2pn \uf8fb\nn\u22121 \u22122pn\n2pn\n1\u2212pn\n\nThe matrix Bn is the submatrix of Bn+1 by removing its first row and column. The matrix Bn\u22121\nis the submatrix of Bn+1 by removing its first two rows and first two columns, and so on. The\nmatrix B1 is just the scalar 1 \u2212 pn . The matrix Bn+1 only appears once and it is related by (26)\nto the orbit chain in Figure 10(a) (for this example we use Bn+1 instead of B1 for notational\nconvenience). The eigenvalues of Bn+1 appear in Tn with multiplicity one. For k = 1, . . . , n, the\nblock Bk is repeated 2n\u2212k times. These blocks, in a recursive form, contain additional eigenvalues\nof Tn , and the numbers of their occurrences reveal the multiplicities of the eigenvalues.\nMore specifically, we note that the orbit chain under the full automorphism group has only one\nfixed point - the root vertex (see Figure 10(a)). We consider next the orbit chain that has a fixed\npoint in the first level of child vertices (the other child vertex in the same level is also fixed). This\nis the orbit graph in Figure 10(b), which has 2n + 1 vertices. The matrix Bn contains exactly the n\neigenvalues that appear in this orbit chain but not in the one of Figure 10(a). These n eigenvalues\neach has multiplicity 2n\u2212n = 1 in Tn . Then we consider the orbit chain that has a fixed point in\nthe second level of child vertices (it also must have a fixed point in the previous level). This is the\norbit graph in Figure 10(c), which has 3n vertices. The matrix Bn\u22121 contains exactly the n \u2212 1\n32\n\n\fFigure 14: Left: the simplest graph with no symmetry. Right: two copies joined head-to-tail.\n\neigenvalues that appear in this orbit chain but not in the previous one. These n \u2212 1 eigenvalues\neach has multiplicity 2n\u2212(n\u22121) = 2. In general, for k = 1, . . . , n, the size of the orbit chain that has\na fixed point in the k-th level of child vertices is\n(n + 1) + n + * * * + (n + 1 \u2212 k)\n(it must have a fixed point in all previous levels). Compared with the orbit chain of (k \u2212 1)-th\nlevel, the orbit chain of k-th level contains additional n + 1 \u2212 k eigenvalues. These are precisely the\neigenvalues of the matrix Bn+1\u2212k , and they all appear in Tn with multiplicity 2n\u2212(n+1\u2212k) = 2k\u22121 .\nBecause of the special structure of B1 , . . . , Bn+1 , we have the following eigenvalue interlacing\nresult (e.g., [HJ85, Theorem 4.3.8])\n\u03bbk+1 (Bk+1 ) \u2264 \u03bbk (Bk ) \u2264 \u03bbk (Bk+1 ) \u2264 \u03bbk\u22121 (Bk ) \u2264 * * * \u2264 \u03bb2 (Bk ) \u2264 \u03bb2 (Bk+1 ) \u2264 \u03bb1 (Bk ) \u2264 \u03bb1 (Bk+1 )\nfor k = 1, . . . , n. Thus for the FMMC problem, we only need to consider the two blocks Bn+1 and\nBn (note that \u03bb1 (Bn+1 ) = 1). In other words, we only need to consider the orbit chain with 2n + 1\nvertices in Figure 10(b). This is a further simplification over the method based on orbit theory.\nWe conjecture that the optimal transition probabilities are\n\u0012 \u0013k !\n1\n1\n\u22c6\n1\u2212 \u2212\n,\nk = 1, . . . , n.\npk =\n3\n2\nNotice that these probabilities do not depend explicitly on n, and so they coincide for any two\nbinary trees, regardless of the height. With increasing k, the limiting optimal values oscillate\naround and converge to 1/3.\n5.3.6\n\nAn example of Ron Graham\n\nWe finish this section with an example raised by Ron Graham. Consider the simplest graph with\nno symmetry (Figure 14, left). Take n copies of this six vertex graph and join them, head to tail,\nin a cycle. By construction, this 6n vertex graph certainly has Cn symmetry. Careful examination\nreveals that the automorphism group is isomorphic to the dihedral group Dn (with order 2n). The\nconstruction actually brings symmetry under reflections in addition to rotations (Figure 14, right).\nThe orbit graphs under Cn and Dn are shown in Figure 15.\n33\n\n\fp1\n\np3\n\np3\n\np3\np2\n\np1\n\np4\n\n2p3\n\np1\n\np4\n2p4\n\np4\n\nFigure 15: Left: orbit graph with Cn symmetry. Right: orbit graph with Dn symmetry.\n\nAlthough the automorphism group of this graph (with 6n vertices) is isomorphic to the ones\nof n-cycles (Figure 3) and wheels (Figure 13), finding the symmetry-adapted basis for blockdiagonalization is a bit more involved. This is due to the different types of orbits we have for\nthis graph. The details of block-diagonalizing this type of graphs is described in [FS92, \u00a73.1]. The\ndiagonal blocks of the resulting matrix all have sizes no larger than 6 \u00d7 6. Numerical experiments\nshow that for n \u2265 3, the fastest mixing chain seems to satisfy\n1\np\u22c61 = p\u22c64 = ,\n2\n\np\u22c62 + p\u22c63 =\n\n1\n.\n2\n\nIntuitively, this 6n vertex graph is the same as modifying a 5n vertex cycle by adding a triangular\nbump (with an additional vertex) for every 5 vertices. Recall that for a pure cycle, we have to use a\ntransition probability that is slightly less than 1/2 to achieve fastest mixing; see equation (8). Here\nbecause of the added bumps, it seems optimal to assign transition probability 1/2 to every edge on\nthe cycle (p\u22c61 and p\u22c64 ), except for edges being part of a bump. For the bumps, the probability 1/2\nis shared between the original edge on the cycle (p\u22c62 ) and the edge connecting to the bump points\n(p\u22c63 ). Moreover, we observe that as n increases, p\u22c63 gets smaller and p\u22c62 gets closer to 1/2. So for\nlarge n, the added bump vertices seem to be ignored, with very small probability to be reached;\nbut once it is reached, it will staying there with high probability.\n\n6\n\nConclusions\n\nWe have shown that exploiting graph symmetry can lead to significant reduction in both the\nnumber of variables and the size of matrices, in solving the FMMC problem. For special classes of\ngraphs such as edge-transitive and distance-transitive graphs, symmetry reduction leads to closed\nform solutions in terms of the eigenvalues of the Laplacian matrix or the intersection matrix. For\nmore general graphs, we gave two symmetry reduction methods, based on orbit theory and block\ndiagonalization, respectively.\nThe method based on orbit theory is very intuitive, but the construction of \"good\" orbit chains\ncan be of more art than technique. The method of block diagonalization can be mostly automated\nonce the irreducible representations of the automorphism groups are generated (for small graphs,\nthey can be generated using software for computational discrete algebra such as GAP [gro05]).\nThese two approaches have an interesting connection: orbit theory gives nice interpretation of\nthe diagonal blocks, while the block diagonalization approach offers theoretical insights about the\nconstruction of the orbit chains.\nThe symmetry reduction method developed in this paper can be very useful in many combinatorial optimization problems where the graph has rich symmetry properties, in particular, problems\nthat can be formulated as or approximated by SDP or eigenvalue optimization problems involving\nweighted Laplacian matrices (e.g., [MP93, Goe97]). In addition to the reduction of problem size,\n\n34\n\n\fother advantages of symmetry exploitation includes degeneracy removal, better conditioning and\nreliability [GP04].\nThere is still much to do in understanding how to exploit symmetry in semidefinite programming. The techniques presented in this paper requires a good understanding of orbit theory, group\nrepresentation theory and interior-point methods for SDP. It is of practical importance to develop\ngeneral purpose methods that can automatically detect symmetries (e.g., the code nauty [McK03]\nfor graph automorphisms), and then exploit them in computations. A good model here is general\npurpose (but heuristic) methods for exploiting sparsity in numerical linear algebra, where symbolic\noperations on graphs (e.g., minimum degree permutation) reduce fill-ins in numerical factorization\n(e.g., [GL81]). As a result of this work, even very large sparse optimization problems are now routinely solved by users who are not experts in sparse matrix methods. For exploiting symmetry in\nSDP, the challenges include the development of fast methods to detect large symmetry groups (for\ncomputational purposes, it often suffices to recognize parts of the symmetries), and the integration\nof algebraic methods (e.g., orbit theory and group representations) and numerical algorithms (e.g.,\ninterior-point methods).\n\nReferences\n[AV05]\n\nM. Ab\u00e9rt and B. Vir\u00e1g. Dimension and randomness in groups acting on rooted trees.\nJournal of the American Mathematical Society, 18(1):157\u2013192, 2005.\n\n[BCN89]\n\nA. E. Brouwer, A. M. Cohen, and A. Neumaier. Distance-Regular Graphs. SpringerVerlag, Berlin, 1989.\n\n[BDPX05] S. Boyd, P. Diaconis, P. A. Parrilo, and L. Xiao. Symmetry analysis of reversible\nMarkov chains. Internet Mathematics, 2(1):31\u201371, 2005.\n[BDSX06] S. Boyd, P. Diaconis, J. Sun, and L. Xiao. Fastest mixing Markov chain on a path.\nThe American Mathematical Monthly, 113(1):70\u201374, January 2006.\n[BDX04]\n\nS. Boyd, P. Diaconis, and L. Xiao. Fastest mixing Markov chain on a graph. SIAM\nReview, 46(4):667\u2013689, 2004.\n\n[Big74]\n\nN. Biggs. Algebraic Graph Theory. Cambridge University Press, 1974.\n\n[BM03]\n\nS. Burer and R. D. C. Monteiro. A nonlinear programming algorithm for solving\nsemidefinite programs via low-rank factorization. Mathematical Programming, Series\nB, 95:329\u2013357, 2003.\n\n[Br\u00e999]\n\nP. Br\u00e9maud. Markov Chains, Gibbs Fields, Monte Carlo Simulation and Queues. Texts\nin Applied Mathematics. Springer-Verlag, Berlin-Heidelberg, 1999.\n\n[BTN01]\n\nA. Ben-Tal and A. Nemirovski. Lectures on Modern Convex Optimization, Analysis,\nAlgorithms, and Engineering Applications. MPS/SIAM Series on Optimization. SIAM,\n2001.\n\n[BV04]\n\nS. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,\n2004. Available at http://www.stanford.edu/~boyd/cvxbook.html.\n\n[BYZ00]\n\nS. Benson, Y. Ye, and X. Zhang. Solving large-scale sparse semidefinite programs for\ncombinatorial optimization. SIAM Journal Optimization, 10:443\u2013461, 2000.\n35\n\n\f[Chu97]\n\nF. R. K. Chung. Spectral Graph Theory. Number 92 in CBMS Regional Conference\nSeries in Mathematics. American Mathematical Society, 1997.\n\n[CLP03]\n\nR. Cogill, S. Lall, and P. A. Parrilo. On structured semidefinite programs for the control\nof symmetric systems. In Proceedings of 41st Allerton Conference on Communication,\nControl, and Computing, pages 1536\u20131545, Monticello, IL, October 2003.\n\n[DCS80]\n\nM. Doob, D. Cvetkovic, and H. Sachs. Spectra of Graphs: Theory and Application.\nAcademic Press, New York, 1980.\n\n[Dia88]\n\nP. Diaconis. Group Representations in Probability and Statistics. IMS, Hayward, CA,\n1988.\n\n[dKPS07]\n\nE. de Klerk, D. V. Pasechnik, and A. Schrijver. Reduction of symmetric semidefinite\nprograms using the regular \u2217-representation. Mathematical Programming, Series B,\n109:613\u2013624, 2007.\n\n[DR07]\n\nI. Dukanovic and F. Rendl. Semidefinite programming relaxations for graph coloring\nand maximal clique problems. Mathematical Programming, Series B, 109:345\u2013365,\n2007.\n\n[DS91]\n\nP. Diaconis and D. Stroock. Geometric bounds for eigenvalues of Markov chains. The\nAnnals of Applied Probability, 1(1):36\u201361, 1991.\n\n[DSC93]\n\nP. Diaconis and L. Saloff-Coste. Comparison theorems for reversible Markov chains.\nAnn. Appl. Probab., 3:696\u2013730, 1993.\n\n[DSC06]\n\nP. Diaconis and L. Saloff-Coste. Separation cut-offs for birth and death chains. Submitted to Annals of Applied Probabilities, 2006.\n\n[ER63]\n\nP. Erd\u0151s and R\u00e9nyi. Asymmetric graphs. Acta Math. Acad. Sci. Hungar., 14:295\u2013315,\n1963.\n\n[FS92]\n\nA. F\u00e4ssler and E. Stiefel.\nBirkh\u00e4user, Boston, 1992.\n\n[Gat00]\n\nK. Gatermann. Computer Algebra Methods for Equivariant Dynamical Systems, volume\n1728 of Lecture Notes in Mathematics. Springer-Verlag, 2000.\n\n[GL81]\n\nA. George and J. Liu. Computer Solution of Large Sparse Positive Definite Systems.\nPrentice Hall, Englewood Cliffs, NJ, 1981.\n\n[GL96]\n\nG. H. Golub and C. F. Van Loan. Matrix Computations. The John Hopkins University\nPress, Baltimore, 3rd edition, 1996.\n\n[GO06]\n\nK. K. Gade and M. L. Overton. Optimizing the asymptotic convergence rate of the\nDiaconis-Holmes-Neal sampler. To appear in Advances in Applied Mathematics, 2006.\n\n[Goe97]\n\nM. X. Goemans. Semidefinite programming in combinatorial optimization. Mathematical Programming, 79:143\u2013161, 1997.\n\n[GP04]\n\nK. Gatermann and P. A. Parrilo. Symmetry groups, semidefinite programs, and sums\nof squares. Journal of Pure and Appl. Algebra, 192:95\u2013128, 2004.\n\nGroup Theoretical Methods and Their Applications.\n\n36\n\n\f[GR01]\n\nC. Godsil and G. Royle. Algebraic Graph Theory, volume 207 of Graduate Texts in\nMathematics. Springer, 2001.\n\n[Gra81]\n\nA. Graham. Kronecker Products and Matrix Calculus with Applications. Ellis Horwoods\nLtd., Chichester, UK, 1981.\n\n[gro05]\n\nThe GAP group. GAP - groups, algorithms, programming - a system for computational\ndiscrete algebra, version 4.4.6, 2005. http://www.gap-system.org.\n\n[GSS88]\n\nM. Golubitsky, I. Stewart, and D. G. Schaeffer. Singularities and Groups in Bifurcation\nTheory II, volume 69 of Applied Mathematical Sciences. Springer, New York, 1988.\n\n[HJ85]\n\nR. A. Horn and C. A. Johnson. Matrix Analysis. Cambridge University Press, 1985.\n\n[HOY03]\n\nB. Han, M. L. Overton, and T. P.-Y. Yu. Design of Hermite subdivision schemes aided\nby spectral radius optimization. SIAM Journal on Matrix Analysis and Applications,\n25:80\u2013104, 2003.\n\n[HR00]\n\nC. Helmberg and F. Rendl. A spectral bundle method for semidefinite programming.\nSIAM Journal on Optimization, 10(3):673\u2013696, 2000.\n\n[JK81]\n\nG. D. James and A. Kerber. The Representation Theory of the Symmetric Group.\nAddison-Wesley, Reading, Massachusetts, 1981.\n\n[KOMK01] Y. Kanno, M. Ohsaki, K. Murota, and N. Katoh. Group symmetry in interior-point\nmethods for semidefinite programming. Optimization and Engineering, 2:293\u2013320,\n2001.\n[Lau07]\n\nM. Laurent. Strengthend semidefinite programming bounds for codes. Mathematical\nProgramming, Series B, 109:239\u2013261, 2007.\n\n[LNM04]\n\nZ. Lu, A. Nemirovski, and R. D. C. Monteiro. Large-scale semidefinite programming via\nsaddle point mirror-prox algorithm. Submitted to Mathematical Programming, 2004.\n\n[Mar03]\n\nF. Margot. Exploiting orbits in symmetric ILP. Mathematical Programming, Series B,\n98:3\u201321, 2003.\n\n[McK03]\n\nB.D. McKay. nauty User's guide (Version 2.2). Australian National University, 2003.\nAvailable from http://cs.anu.edu.au/~bdm/nauty/.\n\n[Mer94]\n\nR. Merris. Laplacian matrices of graphs: a survey. Linear Algebra and Its Applications,\n197:143\u2013176, 1994.\n\n[Moh97]\n\nB. Mohar. Some applications of Laplace eigenvalues of graphs. In G. Hahn and\nG. Sabidussi, editors, Graph Symmetry: Algebraic Methods and Applications, NATO\nASI Ser. C 497, pages 225\u2013275. Kluwer, 1997.\n\n[MP93]\n\nB. Mohar and S. Poljak. Eigenvalues in combinatorial optimization. In R. A. Brualdi,\nS. Friedland, and V. Klee, editors, Combinatorial and Graph-Theoretical Problems in\nLinear Algebra, volume 50 of IMA Volumes in Mathematics and Its Applications, pages\n107\u2013151. Springer-Verlag, 1993.\n\n[MR99]\n\nJ. E. Marsden and T. Ratiu. Introduction to Mechanics and Symmetry, volume 17 of\nTexts in Applied Mathematics. Springer-Verlag, 2nd edition, 1999.\n37\n\n\f[Nem04]\n\nA. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities\nwith Lipschitz continuous monotone operators and smooth convex-concave saddle point\nproblems. SIAM Journal on Optimization, 15(1):229\u2013251, 2004.\n\n[Nes05]\n\nY. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103:127\u2013152, 2005.\n\n[NN94]\n\nY. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex\nProgramming. SIAM Studies in Applied Mathematics. SIAM, 1994.\n\n[OOR04]\n\nR. C. Orellana, M. E. Orrison, and D. N. Rockmore. Rooted trees and iterated weath\nproducts of cyclic groups. Advances in Applied Mathematics, 33(3):531\u2013547, 2004.\n\n[Ove92]\n\nM. L. Overton. Large-scale optimization of eigenvalues. SIAM Journal on Optimization,\n2:88\u2013120, 1992.\n\n[Par00]\n\nP. A. Parrilo. Structured semidefinite programs and semialgebraic geometry methods in\nrobustness and optimization. PhD thesis, California Institute of Technology, May 2000.\nAvailable at http://resolver.caltech.edu/CaltechETD:etd-05062004-055516.\n\n[Par03]\n\nP. A. Parrilo. Semidefinite programming relaxations for semialgebraic problems. Mathematical Programming, 96:293 \u2013 320, 2003.\n\n[PS03]\n\nP. A. Parrilo and B. Sturmfels. Minimizing polynomial functions. In S. Basu and\nL. Gonzalez-Vega, editors, Algorithmic and quantitative real algebraic geometry, volume 60 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science,\npages 83\u201399. AMS, 2003.\n\n[Roc05]\n\nS. Roch. Bounding fastest mixing. Electronic Communications in Probability, 10:282\u2013\n296, 2005.\n\n[Saa92]\n\nY. Saad. Numerical Methods for Large Eigenvalue Problems. Manchester University\nPress, Manchester, UK, 1992.\n\n[Sal06]\n\nJ. Saltzman. A generalization of spectral analysis for discrete data using Markov chains.\nPhD thesis, Department of Statistics, Stanford University, 2006.\n\n[Ser77]\n\nJ.-P. Serre. Linear Representations of Finite Groups. Springer-Verlag, New York, 1977.\n\n[Stu99]\n\nJ. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric\ncones. Optimization Methods and Software, 11-12:625\u2013653, 1999. Special issue on\nInterior Point Methods (CD supplement with software).\n\n[TTT99]\n\nK. C. Toh, M. J. Todd, and R. H. Tutuncu. SDPT3 - a Matlab software package for\nsemidefinite programming. Optimization Methods and Software, 11:545\u2013581, 1999.\n\n[VB96]\n\nL. Vandenberghe and S. Boyd. Semidefinite programming. SIAM Review, 38(1):49\u201395,\n1996.\n\n[Wor94]\n\nP. Worfolk. Zeros of equivariant vector fields: Algorithms for an invariant approach.\nJournal of Symbolic Computation, 17:487\u2013511, 1994.\n\n38\n\n\f[WSV00]\n\nH. Wolkowicz, R. Saigal, and L. Vandenberghe, editors. Handbook of Semidefinite\nProgramming, Theory, Algorithms, and Applications. Kluwer Academic Publishers,\n2000.\n\n[XB04]\n\nL. Xiao and S. Boyd. Fast linear iterations for distributed averaging. Systems and\nControl Letters, 53:65\u201378, 2004.\n\n[XBK07]\n\nL. Xiao, S. Boyd, and S.-J. Kim. Distributed average consensus with least-mean-square\ndeviation. Journal of Parallel and Distributed Computing, 67:33\u201346, 2007.\n\n[YFK03]\n\nM. Yamashita, K. Fujisawa, and M. Kojima. Implementation and evaluation of\nSDPA 6.0 (semidefinite programming algorithm 6.0). Optimization Methods and Software, 18:491\u2013505, 2003.\n\n39\n\n\f"}