{"id": "http://arxiv.org/abs/1105.2165v1", "guidislink": true, "updated": "2011-05-11T11:51:04Z", "updated_parsed": [2011, 5, 11, 11, 51, 4, 2, 131, 0], "published": "2011-05-11T11:51:04Z", "published_parsed": [2011, 5, 11, 11, 51, 4, 2, 131, 0], "title": "Unbiased risk estimation and scoring rules", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1105.1092%2C1105.1834%2C1105.1089%2C1105.0323%2C1105.3359%2C1105.4739%2C1105.1692%2C1105.5915%2C1105.3398%2C1105.0145%2C1105.3875%2C1105.2339%2C1105.5224%2C1105.3788%2C1105.3286%2C1105.0329%2C1105.2849%2C1105.4208%2C1105.1104%2C1105.2310%2C1105.3696%2C1105.2051%2C1105.6316%2C1105.4367%2C1105.0766%2C1105.3852%2C1105.4005%2C1105.1758%2C1105.3397%2C1105.1029%2C1105.4521%2C1105.4877%2C1105.4637%2C1105.3165%2C1105.0492%2C1105.2062%2C1105.5372%2C1105.3975%2C1105.0137%2C1105.0430%2C1105.6330%2C1105.4488%2C1105.0157%2C1105.0293%2C1105.4062%2C1105.6205%2C1105.6262%2C1105.4927%2C1105.1465%2C1105.5169%2C1105.3128%2C1105.2910%2C1105.2516%2C1105.0812%2C1105.5504%2C1105.4701%2C1105.1822%2C1105.3827%2C1105.6086%2C1105.5327%2C1105.5354%2C1105.6059%2C1105.1854%2C1105.0855%2C1105.0093%2C1105.5257%2C1105.5602%2C1105.5860%2C1105.5058%2C1105.5937%2C1105.5956%2C1105.1655%2C1105.2165%2C1105.2438%2C1105.3764%2C1105.5756%2C1105.6237%2C1105.2847%2C1105.0417%2C1105.4711%2C1105.0209%2C1105.5243%2C1105.3221%2C1105.5288%2C1105.2292%2C1105.5359%2C1105.3727%2C1105.1048%2C1105.1423%2C1105.4048%2C1105.0972%2C1105.1419%2C1105.2459%2C1105.4885%2C1105.3681%2C1105.3929%2C1105.0165%2C1105.5590%2C1105.4155%2C1105.4381%2C1105.3166&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Unbiased risk estimation and scoring rules"}, "summary": "Stein unbiased risk estimation is generalized twice, from the Gaussian shift\nmodel to nonparametric families of smooth densities, and from the quadratic\nrisk to more general divergence type distances. The development relies on a\nconnection with local proper scoring rules.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1105.1092%2C1105.1834%2C1105.1089%2C1105.0323%2C1105.3359%2C1105.4739%2C1105.1692%2C1105.5915%2C1105.3398%2C1105.0145%2C1105.3875%2C1105.2339%2C1105.5224%2C1105.3788%2C1105.3286%2C1105.0329%2C1105.2849%2C1105.4208%2C1105.1104%2C1105.2310%2C1105.3696%2C1105.2051%2C1105.6316%2C1105.4367%2C1105.0766%2C1105.3852%2C1105.4005%2C1105.1758%2C1105.3397%2C1105.1029%2C1105.4521%2C1105.4877%2C1105.4637%2C1105.3165%2C1105.0492%2C1105.2062%2C1105.5372%2C1105.3975%2C1105.0137%2C1105.0430%2C1105.6330%2C1105.4488%2C1105.0157%2C1105.0293%2C1105.4062%2C1105.6205%2C1105.6262%2C1105.4927%2C1105.1465%2C1105.5169%2C1105.3128%2C1105.2910%2C1105.2516%2C1105.0812%2C1105.5504%2C1105.4701%2C1105.1822%2C1105.3827%2C1105.6086%2C1105.5327%2C1105.5354%2C1105.6059%2C1105.1854%2C1105.0855%2C1105.0093%2C1105.5257%2C1105.5602%2C1105.5860%2C1105.5058%2C1105.5937%2C1105.5956%2C1105.1655%2C1105.2165%2C1105.2438%2C1105.3764%2C1105.5756%2C1105.6237%2C1105.2847%2C1105.0417%2C1105.4711%2C1105.0209%2C1105.5243%2C1105.3221%2C1105.5288%2C1105.2292%2C1105.5359%2C1105.3727%2C1105.1048%2C1105.1423%2C1105.4048%2C1105.0972%2C1105.1419%2C1105.2459%2C1105.4885%2C1105.3681%2C1105.3929%2C1105.0165%2C1105.5590%2C1105.4155%2C1105.4381%2C1105.3166&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Stein unbiased risk estimation is generalized twice, from the Gaussian shift\nmodel to nonparametric families of smooth densities, and from the quadratic\nrisk to more general divergence type distances. The development relies on a\nconnection with local proper scoring rules."}, "authors": ["Werner Ehm"], "author_detail": {"name": "Werner Ehm"}, "author": "Werner Ehm", "arxiv_comment": "This is the author's version of a work that was accepted for\n  publication in Comptes rendus Mathematique", "links": [{"href": "http://arxiv.org/abs/1105.2165v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1105.2165v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1105.2165v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1105.2165v1", "journal_reference": null, "doi": null, "fulltext": "Unbiased risk estimation and scoring rules\nWerner Ehm\n\narXiv:1105.2165v1 [math.ST] 11 May 2011\n\nAbstract\nStein unbiased risk estimation is generalized twice, from the Gaussian shift model\nto nonparametric families of smooth densities, and from the quadratic risk to more\ngeneral divergence type distances. The development relies on a connection with local\nproper scoring rules.\n\n1\n\nIntroduction: SURE and the Hyv\u00e4rinen score\n\nConsider the problem of estimating the parameter \u03b8 in the standard Gaussian shift family\nP\u03b8 = N (\u03b8, Id ), \u03b8 \u2208 Rd , based on an observation x \u2208 Rd . Let T be an estimator of \u03b8 of the\nform T = x + g(x). Using partial integration, Stein [8] showed that under weak conditions\nabout g, the quadratic risk R(T, \u03b8) = E\u03b8 |T \u2212\u03b8|2 of T can be estimated unbiasedly by the\nb ) = 2divg(x) + |g(x)|2 + d called SURE (Stein unbiased risk estimate), so\nexpression R(T\nb ) = R(T, \u03b8) for every \u03b8 \u2208 Rd . Here | * | and h*, *i denote the Euclidean norm\nthat E\u03b8 R(T\nand inner product on Rd , respectively, and divg is the divergence of g. If in particular\ng = \u2207 log f for some function f > 0 on Rd , the risk estimate becomes\nb ) = 2\u2206 log f (x) + |\u2207 log f (x)|2 + d,\nR(T\n\n(1)\n\nwhere as usual, \u2207 denotes the gradient and \u2206 = div\u2207 the Laplace operator on Rd . This\nspecial case occurs if T is the posterior\nmean with respect to a prior distribution \u03c0 : then\nR\nT = x + \u2207 log f (x) where f (x) = p\u03b8 (x) d\u03c0(\u03b8) is the corresponding mixture density, so\nthat g = \u2207 log f.\nThe striking similarity between SURE and the Hyv\u00e4rinen score [5],\nH(p, x) = 2\n\n\u2207p(x)\n\u2206p(x)\n\u2212\np(x)\np(x)\n\n2\n\n= 2\u2206 log p(x) + |\u2207 log p(x)|2 ,\n\n(2)\n\nhas been noted in, e.g., [6]. In eq. (2), p denotes a sufficiently smooth, strictly positive probability density on Rd . Originally, the Hyv\u00e4rinen score was introduced for score\nmatching, a minimum distance type estimation method. Its formal similarity to SURE is\nsubstantiated on reexpressing the risk of T as a distance between densities. Consider the\nHyv\u00e4rinen divergence defined for smooth, positive densitites p, q on Rd as\nZ\ndH (p, q) = |\u2207 log p(y) \u2212 \u2207 log q(y)|2 q(y) dy.\n(3)\nIf p = f is a mixture density as above and q = p\u03b8 is the density of P\u03b8 , we have\n\u2207 log f (x) \u2212 \u2207 log p\u03b8 (x) = \u2207 log f (x) + x \u2212 \u03b8 = T \u2212 \u03b8, where again T = x + \u2207 log f (x) is\nthe corresponding posterior mean. Consequently,\nZ\n2\nR(T, \u03b8) = E\u03b8 |T \u2212 \u03b8| = |\u2207 log f (x) \u2212 \u2207 log p\u03b8 (x)|2 p\u03b8 (x) dx = dH (f, p\u03b8 ),\n(4)\nthat is, the risk R(T, \u03b8) of the parameter estimate T = x + \u2207 log f (x) equals a distance\nbetween densities, dH (f, p\u03b8 ). Furthermore, the analogue of SURE in the density scenario\n1\n\n\fis the Hyv\u00e4rinen score H(f, x), essentially. In fact, Hyv\u00e4rinen's idea, reinventing Stein's,\nwas to apply partial integration to (3) which, assuming boundary terms vanish, gives\nZ\nZ\n\u0001\n2\u2206 log p(y) + |\u2207 log p(y)|2 q(y) dy + |\u2207 log q(y)|2 q(y) dy;\n(5)\ndH (p, q) =\n\nR\ncf. [1], [5]. Since |\u2207 log p\u03b8 (x)|2 p\u03b8 (x) dx = d (\u03b8 \u2208 Rd ) in the standard normal case, where\nq = p\u03b8 , it follows that\n\u0001\n(6)\nE\u03b8 (H(f, x) + d) = E\u03b8 2\u2206 log f (x) + |\u2207 log f (x)|2 + d = dH (f, p\u03b8 ).\n\nThat is, the modified Hyv\u00e4rinen score H(f, x) + d respresents an unbiased estimate of the\ndistance dH (f, p\u03b8 ) of f from the unknown \"true\" density p\u03b8 , for any density f > 0 on\nRd satisfying suitable regularity conditions.\nThe purpose of this note is to expand on this aspect of unbiased risk estimation by\ntying it to scoring rules. Local proper scoring rules are constructed as gradients of concave\nfunctionals [3], [4], and then shown to generalize SURE in that they furnish unbiased\nestimates of modified Bregman type distances. The development is related to (parts of)\nwork by Dawid and Lauritzen [1]. See also [2], [7].\n\n2\n\nLocal proper scoring rules and unbiased risk estimation\n\nWe restrict the discussion of scoring rules to the setting relevant for this note, and refer to\n[3] for general information. Let P denote the class of all probabilitiy densities with respect\nto the Lebesgue measure on Rd such that the following conditions hold for every p \u2208 P :\n(P1) p \u2208 C 2 ; (P2) p > 0 everywhere on Rd ; (P3) for every m > 0 and i, j \u2208 {1, . . . , d}\n\u0010\n\u0011\nlim |x|m p(x) + |\u2202xi p(x)| + |\u2202x2i xj p(x)| = 0;\n|x|\u2192\u221e\n\n(P4) there exists a = a(p) > 0 such that for i, j \u2208 {1, . . . , d} ,\n\u2212a\n\nlim |x|\n\n|x|\u2192\u221e\n\n\u0014\n\n\u2202xi p(x)\n| log p(x)| +\np(x)\n\n\u00152\n\n+\n\n|\u2202x2i xj p(x)|\np(x)\n\n!\n\n= 0.\n\nThe class P is quite large, being convex and comprising, e.g., all normal and logistic\ndistributions.\nA scoring rule is a mapping S : P \u00d7 Rd \u2192 R assigning a numerical score, S(p, x), to\nRthe density forecast, p, when the observation that materializes is x. We write S(p, q) =\nS(p, x) q(x) dx = Eq S(p, *) for the expected score when the density forecast is p and\nthe probability measure underlying x is q(x)dx. The scoring rule S is (strictly) proper\nrelative to P if S(q, q) \u2264 S(p, q) for all p, q \u2208 P (with equality only if p = q ). The\nscoring rule S is local (of order two, for the class P ) if there exists a real function s such\nthat\n\u0001\nS(p, x) = s x, log p(x), \u2207 log p(x), \u22072 log p(x)\n(p \u2208 P, x \u2208 Rd ),\n\n\u22072 f (x) denoting the Hessian matrix of second-order partial derivatives of a function\nf : Rd \u2192 R at x.\nThe classical example of a (strictly) proper local scoring rule is the logarithmic score,\nS(p, x) = \u2212 log p(x). Another example is the Hyv\u00e4rinen score (2). The latter can be\n2\n\n\fregarded as being local of order two, in the obvious sense, and the former as local of order\nzero. Local scoring rules of any order m \u2265 0 were recently investigated in [7], in the case\nd = 1. Hereafter, \"local\" always is understood as \"local of order two.\"\nThe following result lifts the construction of local proper scoring rules in [2] from\nthe one- to the higher-dimensional case d \u2265 1. Let K denote the class of the kernels\nk : Rd \u00d7 Rd \u2192 R satisfying the following conditions: (K1) k \u2208 C 2 ; (K2) there are\nconstants C, r \u2208 (0, \u221e) such that whenever k\u2217 stands for the function k = k(x, y) or any\nof its partial derivatives up to order two, then |k\u2217 (x, y)| \u2264 C (1 + |x| + |y|)r (x, y \u2208 Rd ).\nWith any k \u2208 K we associate a functional \u03a6 = \u03a6k : P \u2192 R defined by\nZ\nk(x, \u2207 log p(x)) p(x) dx\n(p \u2208 P).\n(7)\n\u03a6(p) =\nRd\n\nIn view of the growth and decay conditions (K2), (P4), and (P3), the integral in (7)\nexists and is finite for every p \u2208 P. Let \u2207y k denote the partial gradient referring to the\nargument y \u2208 Rd of k = k(x, y), and recall that divg(x) stands for the trace of the total\nderivative at x of a function x 7\u2192 g(x) mapping Rd into itself.\nTheorem 2.1 Let k \u2208 K, and suppose that the associated functional \u03a6 is concave on P.\nThen\n\u0014\n\u0015\n1\ndiv p(x) \u2207y k (x, \u2207 log p(x))\n(8)\nS(p, x) = k(x, \u2207 log p(x)) \u2212\np(x)\n\nis a local proper scoring rule relative to P. It is strictly proper if \u03a6 is strictly concave.\nFurthermore, if y 7\u2192 k(x, y) is concave on Rd for every x \u2208 Rd , then the functional \u03a6 is\nconcave on P.\n\nThe Proof follows similar lines as in the case d = 1, see [2, Sections 4.1, 5.1]. We only\nindicate that the tangent construction in [2, Section 4.1] yields the scoring rule (8). To\ncompute the (weak) gradient of \u03a6 at q \u2208 P, let pt = (1 \u2212 t)q + tp where p \u2208 P, t \u2208 [0, 1].\nFormal differentation ignoring all technicalities gives\n\u0015\nZ\nZ\nZ \u0014\n\u2202\n\u2202\nd\n[\u03a6(pt )] =\n[Kpt pt ] dx = [Kpt ] (p \u2212 q) dx +\nKpt pt dx,\n(9)\ndt\n\u2202t\n\u2202t\nwherein we put Kpt (x) = k(x, \u2207 log pt (x)) and omitted the argument x of the integrands.\nFor the last integral in (9) we get by the divergence theorem, assuming the boundary\nintegral vanishes,\n\u0013\u001d\n\u0014\n\u0015\n\u0012\nZ\nZ \u001c\np\u2212q\np\u2212q\ndx. (10)\npt dx = \u2212 div pt \u2207y k (* , \u2207 log pt )\n\u2207y k (* , \u2207 log pt ) , \u2207\npt\npt\nSetting t = 0 in (9) and (10) and noting that p0 = q we find that\n\u0014\n\u0015\u001b\nZ \u001a\n1\nd\n[\u03a6(pt )]\n=\nk(* , \u2207 log q) \u2212 div q \u2207y k (* , \u2207 log q) (p \u2212 q) dx .\ndt\nq\nt=0\n\n(11)\n\nThus, the gradient of \u03a6 at q is given by the expression in curly brackets in (11). The scoring rule resulting from the tangent construction, S(q, *), differs from this gradient only by\na correction term which can be shown to vanish. The negligibility of the boundary integral\nin (10), and all the technicalities (existence of integrals, exchangeability of differentiation\n3\n\n\fand integration, etc.) can be settled similarly as in [2, Section 4.1], using the assumptions\nmade about the classes P and K.\n\u0003\nAny convex combination of a scoring rule S as in Theorem 2.1 with the logarithmic\nscore yields a local proper scoring rule. In the case d = 1, scoring rules of this form\nexhaust the class of all local proper scoring rules [2], [7]. The complete characterization\nin the case d > 1 remains open.\nExamples. Let k \u2208 K be a kernel of the form k(x, y) = k(y) = \u03c8(|y|), where \u03c8 is a\nconcave C 2 -function on [0, \u221e) with \u03c8(0) = \u03c8 \u2032 (0) = 0. Then y 7\u2192 k(y) is concave on Rd ,\nand the corresponding scoring rule (8) is proper. Explicitly we have\n\u0014\n\u0015\u001c\n\u001d\n\u0001 \u03c3\n\u0001\n\u03c8 \u2032 (|\u03c3|)\n\u03c3\n\u03c8 \u2032 (|\u03c3|)\n2\n2\n\u2032\u2032\nS(p, * ) = \u03c8(|\u03c3|) \u2212\n, \u2207 log p\n|\u03c3| + \u2206 log p \u2212 \u03c8 (|\u03c3|) \u2212\n|\u03c3|\n|\u03c3|\n|\u03c3|\n|\u03c3|\n\nwhere \u03c3 = \u2207 log p. For \u03c8(t) = \u2212t2 we obtain the Hyv\u00e4rinen score (2); putting \u03c8(t) =\n\u2212 log cosh t yields another interesting example parallel to [2, Example 5.3].\nA local scoring rule S that is proper relative to P gives rise to a Bregman type\ndivergence measure dS (p, q) = S(p, q) \u2212 S(q, q) on P \u00d7 P. The following representation of\ndS is closely related to [7, Eq. (53)].\nTheorem 2.2 Suppose that S is of the form (8) for some kernel k \u2208 K such that\ny 7\u2192 k(x, y) is concave on Rd for every x \u2208 Rd . Then the divergence dS admits the\nrepresentation\ndS (p, q)\n\u001d\u001b\n\u001a\n\u001c\n\u2207q \u2207p\n\u2212\n, \u2207y k (*, \u2207 log p)\n= Eq k(* , \u2207 log p) \u2212 k(* , \u2207 log q) +\nq\np\n\n(12)\n(p, q \u2208 P).\n\nProof. Let p, q \u2208 P. By the assumptions on P and K, the divergence theorem applied to\nthe scalar function u(x) = q(x)/p(x) and the vector field v(x) = p(x) \u2207y k (x, \u2207 log p(x))\ngives\n\u0014\n\u0015\nZ\nq\nlim \u2212\ndiv p \u2207y k (*, \u2207 log p) dx\n(13)\nr\u2192\u221e\n|x|\u2264r p\n\u001d\n\u001c\nZ\n\u2207q \u2207p\n\u2212\n, \u2207y k (*, \u2207 log p) q dx.\n= lim\nr\u2192\u221e |x|\u2264r\nq\np\nThe relation (12) follows on writing\ndS (p, q) = Eq {S(p, * ) \u2212 S(q, * )}, substituting (8) and\nR\nusing (13), and observing that q \u22121 div (q \u2207y k (*, \u2207 log q)) q dx = 0.\n\u0003\n\nNote that the expression in curly brackets in (12) is nonnegative because for a concave\nfunction f on Rd one has f (y1 ) \u2212 f (y2 ) \u2265 hy1 \u2212 y2 , \u2207f (y1 )i (y1 , y2 \u2208 Rd ). For the\nHyv\u00e4rinen score, where k(x, y) = \u2212|y|2 , that expression becomes |\u2207 log p \u2212 \u2207 log q|2 , and\ndS becomes the Hyv\u00e4rinen divergence (3).\nTo clarify the connection with SURE we note that the partial integration in (13) was\nused conversely by Stein and Hyv\u00e4rinen, to pass from the risk representation (12) to an\nexpression of the form Eq {S(p, * ) \u2212 S(q, * )}. In the latter, the scoring rule S(p, * ) may\nserve as an unbiased estimate of Eq S(p, * ), while the term Eq S(q, * ) is the same for all\ncandidates p, hence can be ignored if the focus is on risk comparison. In nonparametric\ndensity estimation, e.g., risk comparison of competing estimates is applied for bandwidth\n4\n\n\fselection, using cross-validation. Briefly, if pbn = pbn (* |x1 , . . . , xn ) is an estimate of the\nunkown density q \u2208 P underlying the i. i. d. observations P\nx1 , . . . , xn that is symmetric\nn\n\u22121\nb\nin the xi , the cross-validated expression Rn (b\npn\u22121 ) = n\npn,\u2212i , xi ) is an unbii=1 S(b\nased estimate of Rn\u22121 (b\npn\u22121 , q), where Rn (b\npn , q) = Eq S(b\npn , q) denotes the modified risk\nignoring the term Eq S(q, * ) = S(q, q), which depends only on q.\nThe possibility of risk estimation is of course not confined to the local scoring rules considered here, as any proper scoring rule S, whether local or not, gives rise to a divergence\nmeasure dS . Therefore, cross-validatory estimation of the (modified) risk generally is feasible, although exact unbiasedness as with the local scoring rules may not be achievable\nR\nwhen global terms are involved. For example, unbiased estimation of the term p(x)2 dx\nentering the quadratic score [3] does not seem possible.\nThe particular interest of the scoring rules of the form (8) ensues from the fact that\nthey do not require the knowledge of the normalizing constants of the probability densities,\nwhich may be unknown or hard to obtain in complex settings [5], [7]. This advantage can\nbe combined with other desirable features such as improved robustness by working, for\ninstance, with the log cosh scoring rule mentioned above.\n\nAcknowledgement\nThe author thanks Tilmann Gneiting, Steffen Lauritzen, and Matthew Parry for comments\non earlier drafts of the paper.\n\nReferences\n[1] A P Dawid and S L Lauritzen (2005). The geometry of decision theory. In Proc. 2nd\nInt. Symp. Inf. Geom. Appl. 22-28. Univ. Tokyo\n[2] W Ehm and T Gneiting (2011). Local proper scoring rules of order two. Preprint,\narXiv:1102.5031v1\n[3] T Gneiting and A E Raftery (2007). Strictly proper scoring rules, prediction, and\nestimation. J. Amer. Statist. Assoc. 102, 359-378\n[4] A D Hendrickson and R J Buehler (1971). Proper scores for probability forecasters.\nAnn. Math. Statist. 42, 1916-1921\n[5] A Hyv\u00e4rinen (2005). Estimation of non-normalized statistical models using score\nmatching. J. Mach. Learn. Res. 6, 695-709\n[6] A Hyv\u00e4rinen (2008). Optimal approximation of signal priors. Neural Computation\n20, 3087-3110\n[7] M Parry, A P Dawid and S Lauritzen (2011). Proper local scoring rules. Preprint,\narXiv:1101.5011v1\n[8] C M Stein (1981). Estimation of the mean of a multivariate normal distribution.\nAnn. Statist. 9, 1135-1151\n\n5\n\n\fAuthor address:\nInstitute for Frontier Areas of Psychology and Mental Health\nWilhelmstr. 3a, 79098 Freiburg, Germany\ne\u2013mail: ehm@igpp.de\n\n6\n\n\f"}