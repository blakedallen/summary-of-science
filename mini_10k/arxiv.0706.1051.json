{"id": "http://arxiv.org/abs/0706.1051v1", "guidislink": true, "updated": "2007-06-07T18:13:59Z", "updated_parsed": [2007, 6, 7, 18, 13, 59, 3, 158, 0], "published": "2007-06-07T18:13:59Z", "published_parsed": [2007, 6, 7, 18, 13, 59, 3, 158, 0], "title": "Improved Neural Modeling of Real-World Systems Using Genetic Algorithm\n  Based Variable Selection", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0706.1315%2C0706.1867%2C0706.0078%2C0706.1011%2C0706.0208%2C0706.1855%2C0706.0906%2C0706.3910%2C0706.4231%2C0706.1493%2C0706.1908%2C0706.3495%2C0706.2116%2C0706.1067%2C0706.2017%2C0706.2207%2C0706.1863%2C0706.0251%2C0706.3570%2C0706.0760%2C0706.2907%2C0706.0562%2C0706.0136%2C0706.1830%2C0706.1860%2C0706.4370%2C0706.1998%2C0706.3943%2C0706.2108%2C0706.0539%2C0706.0021%2C0706.1357%2C0706.3191%2C0706.1448%2C0706.1130%2C0706.2718%2C0706.4163%2C0706.2290%2C0706.0768%2C0706.1725%2C0706.1252%2C0706.0810%2C0706.2422%2C0706.0023%2C0706.1478%2C0706.2366%2C0706.0098%2C0706.2238%2C0706.3067%2C0706.1633%2C0706.0442%2C0706.1421%2C0706.2265%2C0706.3537%2C0706.4213%2C0706.1577%2C0706.4041%2C0706.0415%2C0706.0901%2C0706.3630%2C0706.1520%2C0706.0518%2C0706.0935%2C0706.1419%2C0706.3829%2C0706.2463%2C0706.1416%2C0706.0536%2C0706.3110%2C0706.4082%2C0706.2880%2C0706.4117%2C0706.2885%2C0706.0549%2C0706.0378%2C0706.0121%2C0706.3801%2C0706.2671%2C0706.1084%2C0706.2891%2C0706.2895%2C0706.0202%2C0706.1953%2C0706.3849%2C0706.2878%2C0706.0447%2C0706.0040%2C0706.3154%2C0706.1051%2C0706.4116%2C0706.1079%2C0706.1459%2C0706.3796%2C0706.3698%2C0706.2019%2C0706.0612%2C0706.3051%2C0706.3139%2C0706.2906%2C0706.3598%2C0706.4234&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Improved Neural Modeling of Real-World Systems Using Genetic Algorithm\n  Based Variable Selection"}, "summary": "Neural network models of real-world systems, such as industrial processes,\nmade from sensor data must often rely on incomplete data. System states may not\nall be known, sensor data may be biased or noisy, and it is not often known\nwhich sensor data may be useful for predictive modelling. Genetic algorithms\nmay be used to help to address this problem by determining the near optimal\nsubset of sensor variables most appropriate to produce good models. This paper\ndescribes the use of genetic search to optimize variable selection to determine\ninputs into the neural network model. We discuss genetic algorithm\nimplementation issues including data representation types and genetic operators\nsuch as crossover and mutation. We present the use of this technique for neural\nnetwork modelling of a typical industrial application, a liquid fed ceramic\nmelter, and detail the results of the genetic search to optimize the neural\nnetwork model for this application.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0706.1315%2C0706.1867%2C0706.0078%2C0706.1011%2C0706.0208%2C0706.1855%2C0706.0906%2C0706.3910%2C0706.4231%2C0706.1493%2C0706.1908%2C0706.3495%2C0706.2116%2C0706.1067%2C0706.2017%2C0706.2207%2C0706.1863%2C0706.0251%2C0706.3570%2C0706.0760%2C0706.2907%2C0706.0562%2C0706.0136%2C0706.1830%2C0706.1860%2C0706.4370%2C0706.1998%2C0706.3943%2C0706.2108%2C0706.0539%2C0706.0021%2C0706.1357%2C0706.3191%2C0706.1448%2C0706.1130%2C0706.2718%2C0706.4163%2C0706.2290%2C0706.0768%2C0706.1725%2C0706.1252%2C0706.0810%2C0706.2422%2C0706.0023%2C0706.1478%2C0706.2366%2C0706.0098%2C0706.2238%2C0706.3067%2C0706.1633%2C0706.0442%2C0706.1421%2C0706.2265%2C0706.3537%2C0706.4213%2C0706.1577%2C0706.4041%2C0706.0415%2C0706.0901%2C0706.3630%2C0706.1520%2C0706.0518%2C0706.0935%2C0706.1419%2C0706.3829%2C0706.2463%2C0706.1416%2C0706.0536%2C0706.3110%2C0706.4082%2C0706.2880%2C0706.4117%2C0706.2885%2C0706.0549%2C0706.0378%2C0706.0121%2C0706.3801%2C0706.2671%2C0706.1084%2C0706.2891%2C0706.2895%2C0706.0202%2C0706.1953%2C0706.3849%2C0706.2878%2C0706.0447%2C0706.0040%2C0706.3154%2C0706.1051%2C0706.4116%2C0706.1079%2C0706.1459%2C0706.3796%2C0706.3698%2C0706.2019%2C0706.0612%2C0706.3051%2C0706.3139%2C0706.2906%2C0706.3598%2C0706.4234&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Neural network models of real-world systems, such as industrial processes,\nmade from sensor data must often rely on incomplete data. System states may not\nall be known, sensor data may be biased or noisy, and it is not often known\nwhich sensor data may be useful for predictive modelling. Genetic algorithms\nmay be used to help to address this problem by determining the near optimal\nsubset of sensor variables most appropriate to produce good models. This paper\ndescribes the use of genetic search to optimize variable selection to determine\ninputs into the neural network model. We discuss genetic algorithm\nimplementation issues including data representation types and genetic operators\nsuch as crossover and mutation. We present the use of this technique for neural\nnetwork modelling of a typical industrial application, a liquid fed ceramic\nmelter, and detail the results of the genetic search to optimize the neural\nnetwork model for this application."}, "authors": ["Donald A. Sofge", "David L. Elliott"], "author_detail": {"name": "David L. Elliott"}, "author": "David L. Elliott", "arxiv_comment": "4 pages", "links": [{"href": "http://arxiv.org/abs/0706.1051v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0706.1051v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0706.1051v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0706.1051v1", "journal_reference": "D. Sofge and D. Elliott, \"Improved Neural Modeling of Real-World\n  Systems Using Genetic Algorithm Based Variable Selection,\" In Int'l Conf. on\n  Neural Networks and Brain (ICNN&B'98-Beijing), 1998", "doi": null, "fulltext": "Proceedings Conference on Neural Networks & Brain (NN&B'98-Beijing), Oct. 1998\n\nImproved Neural Modeling of Real-World Systems\nUsing Genetic Algorithm Based Variable Selection\nDonald A. Sofge & David L. Elliott\nNeuroDyne, Inc.\nOne Kendall Square\nCambridge, MA 02139, USA\nsofge@ai.mit.edu, delliott@isr.umd.edu\nAbstract\nNeural network models of real-world systems, such as\nindustrial processes, made from sensor data must often\nrely on incomplete data. System states may not all be\nknown, sensor data may be biased or noisy, and it is not\noften known which sensor data may be useful for\npredictive modelling. Genetic algorithms may be used to\nhelp to address this problem by determining the near\noptimal subset of sensor variables most appropriate to\nproduce good models. This paper describes the use of\ngenetic search to optimize variable selection to\ndetermine inputs into the neural network model. We\ndiscuss genetic algorithm implementation issues\nincluding data representation types and genetic\noperators such as crossover and mutation. We present\nthe use of this technique for neural network modelling\nof a typical industrial application, a liquid fed ceramic\nmelter, and detail the results of the genetic search to\noptimize the neural network model for this application.\nIntroduction\nWhen modeling a complex system (such as a chemical\nreactor), it is not generally known a priori which system\nstates are necessary to develop a good model, or which\nstates are observable based upon available sensor\ntechnology (although it is often known that many\nsystem states are not observable). In addition, there is a\ngreater problem in identifying useful data. Complex\ndynamic systems such as the chemical reactor may be\ninstrumented with tens, hundreds or even thousands of\nsensors. The problem with so much sensor information\nis that most of it will be irrelevant. Worse still,\nunfiltered incorporation of irrelevant data will adulterate\na model, eroding its predictive capabilities.\nA key data pretreatment problem is sensor redundancy.\nIt is well known that smaller models are often better\nmodels [Sofge92]. This translates to fewer inputs and\nfewer hidden layer nodes. While it may be nice to have\nhighly redundant data from a large number of sensors, in\nreality we may only need a few key sensors in order to\nproduce a good model. The problem is in determining\n\nwhich few sensors to choose, and ignoring most of the\nremaining sensors. This is confounded by the fact that\ndue to differing sensor response characteristics and\nnoise, in the aggregate there is a considerable amount of\nnoise and bias in the data.\nIn the example given in this paper, modelling of a liquid\nfed ceramic melter (LFCM) process is undertaken in\norder to predict the surface level. The melt chamber is\ninstrumented with 20 thermocouple sensors placed at\ndifferent sites within the chamber. Each sensor may\nhave a slightly different characteristic response curve\ndue to differences in manufacturing, usage history, etc.\nEach sensor also is susceptible to some level of noise.\nWe take a time history of data from all 20 sensors and\nstore it in our database, and then use this database to\ntrain a neural network model.\nSome sensors, such as those near the surface in the\nreactor vessel, may offer fairly high-variance data\nthroughout the process, but be largely irrelevant to\naccurately predicting final product quality. We would\nlike to select a near- optimal set of sensor variables in\norder to train a neural network model with the greatest\npredictive accuracy.\nVariable Selection Using Genetic Algorithm\nA genetic algorithm (GA) is fundamentally a search\nmethod which is used to optimize a complex system\nwhich is too large to fully explore or to locate a true\noptimal solution. The GA search procedure is inspired\nby rules of natural selection in Darwinian evolution\nwhich suggest that only the fittest members of a group\nwill survive, to then recombine genetically with other fit\nmembers to yield even fitter members, thereby passing\ntheir successful characteristics on to the next generation\n[Holland75]. Less competitive members of the group are\ndiscarded or die off and are not recombined, and thus\nthe characteristics that they carry are not propagated.\nThus a population \"evolves\", with successive\ngenerations replacing older ones and more successful\nmembers replacing less successful ones. Each member\n\n\fof the population, called a \"chromosome\", is represented\nby a string of \"genes\", which are encoded characteristics\nto be optimized.\nThe genes need to be defined for a given application\nsuch that finding a better or more optimal set of genes\nmeans finding a better solution to the problem. A GA\nmay perform variable selection if each gene in a\nchromosome represents an available sensor variable.\nFitness is judged for each chromosome by determining\nhow good the models are (accuracy, robustness)\ngenerated by that combination of variables. An initial\npopulation of chromosomes is generated by choosing a\nstring length (# of genes) and randomly assigning a\nvariable to each gene. The GA search is then set in\nmotion and the chromosomes compete, reproduce, and\ndie off as they are replaced by more fit chromosomes. It\nis usually desirable to maintain a fixed-size population\nin order to make sure that the fitter chromosomes\nquickly replace the less fit ones. An occasional\nmutation is introduced to make sure that certain genes\n(variables) which may be really useful aren't quickly\neliminated (possibly because they are randomly\ncombined with really noisy variables early on) and then\nnever incorporated again. This is referred to as a\npopulation in danger due to lack of genetic variation,\nand to avoid this situation a mutation rate is\npredetermined and mutated chromosomes are\nintroduced into the population at regular intervals during\nGA search. As these parameters are application\ndependent, it is not possible to know beforehand which\nvalues will work best. The GA process is automated\nwith automatic gene sequence selection, model building\nand discarding, and evaluation of accuracy and\nrobustness of the models (scoring). Successive\ngenerations will inherit the best characteristics from the\nprevious generation, while eliminating the less valuable\ncharacteristics.\nGA Representation\nGenetic algorithms are often thought of, discussed and\nimplemented using binary strings, or bit strings. Each\nbit represents a \"gene\" expression. If the bit is turned\non, then the gene corresponding to that bit can be said to\nbe \"expressed\". While this representation works fine for\nmost purposes, it is not necessary to use binary\nrepresentations to implement genetic algorithms. In this\nproject each chromosome (representing a subset of\nselected input variables for the neural network model) is\nexpressed as a vector of integers, with each integer\nrepresenting a particular gene.\nFor certain GA\noperators, however, such as the crossover operator\n(discussed below), this form is translated into an\nequivalent binary form in order to facilitate simpler\ncomputation using expressions of binary logical\noperators. The resulting offspring is then translated\n\nback to its equivalent vector form. In general it is best to\nuse whatever representation format for storing and\nmanipulating the chromosomes is most appropriate for\nthe application domain.\nGA Operators\nGenetic algorithms require the use of special operators\nin order to simulate the evolutionary processes which\nthey emulate. The most important are the crossover and\nmutation operators. The crossover operator takes two\nparent chromosomes (in this application, each parent\nchromosome represents a group of input variables used\nto build a neural network model), and combines them to\nproduce an offspring. The most common form of\ncrossover operator in GA literature is known as uniform\ncrossover [Spears91]. In uniform crossover, if a specific\ngene is turned on in both parents, then it will be turned\non in the offspring. If a gene is turned on in only one of\nthe parents, then it may be turned on (with a\npredetermined probability, usually 0.5) in the offspring.\nUniform crossover was used in this project as well.\nThe mutation operator is applied independently but\nimmediately following the crossover operator.\nA\nmutation is a random addition or deletion of a gene in a\nchromosome, and is governed by a preset mutation rate\n(usually quite low, e.g. 0.001).\nA technique not as commonly used in GA literature, but\ndeveloped for this application, is a survival rate, which\ndetermines what percentage of the population (the fittest\nmembers) will survive to be continued into the next\ngeneration. Many early applications of GAs assumed\nthat all chromosomes of a generation would be replaced\nby their offspring. However, this was found to often\neliminate the fittest chromosomes and interfere with the\nsearch. An approach called \"elitism\" or \"superiority\"\n[deGaris93] was employed to keep the fittest\nchromosome from a previous generation. In this\nproject, a percentage (e.g., top 20%) of the\nchromosomes from the previous generation were carried\non to the next generation. These top performers were\nused to generate the offspring for the following\ngeneration.\nAnother feature employed in this work was to guarantee\nthat when a new offspring is generated it does not\nduplicate any current chromosome in the population. A\ngraveyard is used to store old chromosomes which\nrepresent models which have been built, tested, and then\ndiscarded. Each new offspring is compared with\nchromosomes in the graveyard to make sure that it\nhasn't been tested before in a previous generation. Since\nwe assume that all of the neural network models use the\nsame superset of data (same output data, input data\nincludes sensor streams for all possible input variables),\n\n\fthen the process of choosing variables for a particular\nmodel is deterministic, so there is never a need to retest\na chromosome once its corresponding model has been\nbuilt, tested and scored. This promotes better crossover\nby preventing the generation of chromosomes which are\nalready represented or have been generated and tested in\nprior generations. Chromosomes which are carried from\none generation to the next are stored along with their\nscores, but are not retested since this would serve no\npurpose.\nSimulation Results\nThe process being modelled in this effort is a liquid fed\nceramic melter (LFCM). The LFCM is instrumented\nwith 20 thermocouples distributed throughout the melt\nchamber which provide temperature feedback during the\nprocess. Data from these 20 sensors, 200 samples of\neach taken at a specific interval of time, is collected\nalong with a measurement of level in the melt chamber\n(see Figure 1). This data is used to train the neural\nnetwork models.\n\nFigure 1. Training Data for LFCM Process\nAs shown in the top part of Figure 1, the thermocouple\nreadings (20 readings overlaid onto the same plot) are\nquite noisy. In some there is no apparent correlation\nbetween the sensor readings and the level measurement\nshown in the bottom part of the figure. Also, there is\nconsiderable variability in the response of various\nsensors (which may be due to each sensor's location in\nthe chamber, or due to the response characteristics of the\nsensor itself, or both).\nIn order to examine every possible grouping of input\nvariables (not including permutations, only combinations) to find the optimal subset of input parameters for\nmodelling the level in the LFCM, it is useful to think of\na bit string of length 20. A bit turned on would indicate\n\nthat that variable was included in the solution.\nExcluding the all zeros case (where no inputs are used),\nthere are 2^20-1 or 1,048,575 unique models which can\nbe formed using these inputs. It is clearly unreasonable\nto try to build, train and test this many neural network\nmodels. Since we don't know a priori which inputs will\nused, we need a procedure for finding a near optimal\nsubset. The GA provides the solution.\nEach combination of input variables, which can be\nthought of as a 20-bit string, is a chromosome. In the\nGA procedure we build a fixed size population of\nchromosomes, which are each evaluated and scored\naccording to a fitness function. In this application the\nmodels were trained on the training data (200\nexemplars), and then tested using an independent crossvalidation dataset not used for training. The crossvalidation data consisted of 200 exemplars. The neural\nnetworks all used the same number of hidden-layer and\noutput nodes, and the same non-linear activation\nfunction.\nThe neural networks were multilayer\nperceptrons trained using the Levenberg-Marquardt\nalgorithm (which generally converges more quickly than\nstandard back-propagation). Each network was allowed\nto train to completion. The score for each model, or\nchromosome, is simply the sum-squared-error (SSE)\nobtained from applying each network to the crossvalidation dataset. The goal of the genetic search then is\nto find the model with the minimum total SSE on the\ncross-validation dataset.\nThe population was initialized using a combination of\nordered and random selection of chromosomes. First,\nall of the single variable subsets were included, and the\nsolution utilizing all of the input variables was included.\nIn between these extremes, the database was populated\nwith a roughly even (by the number of genes expressed)\ndistribution of chromosomes, though whether a\nparticular gene was expressed in a particular\nchromosome was determined by a random number\ngenerator.\nVarious runs were made using population ranges from\n30 to 100 chromosomes. The survival rate was varied\nbetween 20% and 50%, and various mutation rates were\ntried. Crossover was achieved by random selection of\nthe fittest chromosomes from the previous generation.\nAs noted in GA literature [Goldberg89, Davis91] use of\na uniqueness operator as employed in this effort allows\nhigher crossover and mutation rates, and enables more\nrapid convergence of the genetic search procedure. A\nmutation rate of 0.1 (usually the mutation rate is closer\nto 0.001) was found to work quite well in this instance.\n\n\fAfter several runs of 20 to 30 generations (1000-2000\nmodels built and tested for each run) the genetic search\nreturned the same result each time as the best solution,\ndespite use of different randomly generated populations,\ndifferent population sizes, and different GA operator\nsettings. Out of 20 input variables, numbered 1 through\n20, it found that the best model resulted from selection\nof 11 of these variables: 1-2-3-4-5-8-9-14-16-18-19.\nThe genetic search procedure excluded the 9 variables\n6-7-10-11-12-13-15-17-20. Whether this is the optimal\nsolution is an open question without exhaustively testing\nall 1,048,575 possible models and comparing their\nscores. The final solution on the cross-validation dataset\nis shown in Figure 2.\n\nBibliography\nCarbonell, J., (Ed.) Machine Learning:\nParadigms and Methods, MIT Press, 1990.\nElliott, D.L., (Ed.), Neural Systems for\nControl, Academic Press, 1997.\nDavis, L., Handbook of Genetic Algorithms,\nVan Nostrand Reinhold, 1991.\nde Garis, H., \"Genetic Programming: GenNets,\nArtificial\nNervous\nSystems,\nArtificial\nEmbryos\", Ph.D. dissertation, 1993.\nGoldberg, D.E., Genetic Algorithms in Search,\nOptimization, and Machine Learning, AddisonWesley, 1989.\nHolland, J.H., Adaptation in Natural and\nArtificial Systems, University of Michigan\nPress, 1975.\nSofge, D. and D.L. Elliott, \"An Approach to\nIntelligent Identification and Control of\nNonlinear Dynamical Systems,\" Neural\nAdaptive Control Technology, Chapter 9,\nWorld Scientific, 1996.\n\nFigure 2. Final Solution: Model Tested on\nCross-Validation Data\nConclusions\nThis paper presents a solution to the problem of trying\nto build neural network models of real-world systems,\nsuch as chemical and industrial processes, using data\nfrom numerous sensors where sensor data may be noisy,\nbiased, corrupted, or even irrelevant to the parameter(s)\nbeing modelled. The use of genetic search makes it\npossible to find a near-optimal subset of variables for\nuse in model-building under conditions where the data\nmay make this quite difficult. The technique of GA\nbased variable selection may be applied to numerous\napplication areas where models (neural network or\nother) are required, the selection of input variables is not\nalways clear, and the data may be noisy. A typical\nexample of this would be in financial forecasting.\n\nSofge, D. and D. White, (Eds.), Handbook of\nIntelligent Control: Neural, Fuzzy, and\nAdaptive Approaches, New York: Van\nNostrand Reinhold, 1992.\nSpears, W.M. and K.A. De Jong, \"On the\nVirtues of Parameterized Uniform Crossover\",\nProceedings of the 4th Int. Conf. on Genetic\nAlgorithms, San Diego, 1991.\n\n\f"}