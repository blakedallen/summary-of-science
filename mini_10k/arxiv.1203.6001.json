{"id": "http://arxiv.org/abs/1203.6001v2", "guidislink": true, "updated": "2012-09-26T16:57:49Z", "updated_parsed": [2012, 9, 26, 16, 57, 49, 2, 270, 0], "published": "2012-03-27T15:35:00Z", "published_parsed": [2012, 3, 27, 15, 35, 0, 1, 87, 0], "title": "Probabilistic Recovery Guarantees for Sparsely Corrupted Signals", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.4473%2C1203.5849%2C1203.2324%2C1203.3455%2C1203.6797%2C1203.0116%2C1203.2786%2C1203.2082%2C1203.0313%2C1203.2562%2C1203.6177%2C1203.1123%2C1203.3024%2C1203.6858%2C1203.0044%2C1203.5831%2C1203.2237%2C1203.2674%2C1203.6393%2C1203.2332%2C1203.1855%2C1203.1170%2C1203.6416%2C1203.4636%2C1203.1125%2C1203.1174%2C1203.0359%2C1203.1667%2C1203.4641%2C1203.6001%2C1203.5772%2C1203.4676%2C1203.5947%2C1203.0817%2C1203.4523%2C1203.1028%2C1203.2442%2C1203.5975%2C1203.5933%2C1203.4769%2C1203.1195%2C1203.5492%2C1203.3741%2C1203.6786%2C1203.4866%2C1203.1422%2C1203.6140%2C1203.3749%2C1203.1801%2C1203.6340%2C1203.4140%2C1203.3835%2C1203.5090%2C1203.5200%2C1203.4803%2C1203.3628%2C1203.4044%2C1203.3042%2C1203.4147%2C1203.6575%2C1203.3064%2C1203.1093%2C1203.5736%2C1203.1240%2C1203.2598%2C1203.4887%2C1203.3511%2C1203.3356%2C1203.2246%2C1203.5181%2C1203.6087%2C1203.1463%2C1203.5785%2C1203.2724%2C1203.0454%2C1203.4851%2C1203.4043%2C1203.2038%2C1203.3348%2C1203.0718%2C1203.5083%2C1203.6328%2C1203.5986%2C1203.3334%2C1203.4025%2C1203.3048%2C1203.6246%2C1203.0660%2C1203.3237%2C1203.3127%2C1203.4681%2C1203.6466%2C1203.4530%2C1203.5029%2C1203.6438%2C1203.1859%2C1203.2884%2C1203.5381%2C1203.6315%2C1203.4519%2C1203.2899&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Probabilistic Recovery Guarantees for Sparsely Corrupted Signals"}, "summary": "We consider the recovery of sparse signals subject to sparse interference, as\nintroduced in Studer et al., IEEE Trans. IT, 2012. We present novel\nprobabilistic recovery guarantees for this framework, covering varying degrees\nof knowledge of the signal and interference support, which are relevant for a\nlarge number of practical applications. Our results assume that the sparsifying\ndictionaries are characterized by coherence parameters and we require\nrandomness only in the signal and/or interference. The obtained recovery\nguarantees show that one can recover sparsely corrupted signals with\noverwhelming probability, even if the sparsity of both the signal and\ninterference scale (near) linearly with the number of measurements.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1203.4473%2C1203.5849%2C1203.2324%2C1203.3455%2C1203.6797%2C1203.0116%2C1203.2786%2C1203.2082%2C1203.0313%2C1203.2562%2C1203.6177%2C1203.1123%2C1203.3024%2C1203.6858%2C1203.0044%2C1203.5831%2C1203.2237%2C1203.2674%2C1203.6393%2C1203.2332%2C1203.1855%2C1203.1170%2C1203.6416%2C1203.4636%2C1203.1125%2C1203.1174%2C1203.0359%2C1203.1667%2C1203.4641%2C1203.6001%2C1203.5772%2C1203.4676%2C1203.5947%2C1203.0817%2C1203.4523%2C1203.1028%2C1203.2442%2C1203.5975%2C1203.5933%2C1203.4769%2C1203.1195%2C1203.5492%2C1203.3741%2C1203.6786%2C1203.4866%2C1203.1422%2C1203.6140%2C1203.3749%2C1203.1801%2C1203.6340%2C1203.4140%2C1203.3835%2C1203.5090%2C1203.5200%2C1203.4803%2C1203.3628%2C1203.4044%2C1203.3042%2C1203.4147%2C1203.6575%2C1203.3064%2C1203.1093%2C1203.5736%2C1203.1240%2C1203.2598%2C1203.4887%2C1203.3511%2C1203.3356%2C1203.2246%2C1203.5181%2C1203.6087%2C1203.1463%2C1203.5785%2C1203.2724%2C1203.0454%2C1203.4851%2C1203.4043%2C1203.2038%2C1203.3348%2C1203.0718%2C1203.5083%2C1203.6328%2C1203.5986%2C1203.3334%2C1203.4025%2C1203.3048%2C1203.6246%2C1203.0660%2C1203.3237%2C1203.3127%2C1203.4681%2C1203.6466%2C1203.4530%2C1203.5029%2C1203.6438%2C1203.1859%2C1203.2884%2C1203.5381%2C1203.6315%2C1203.4519%2C1203.2899&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider the recovery of sparse signals subject to sparse interference, as\nintroduced in Studer et al., IEEE Trans. IT, 2012. We present novel\nprobabilistic recovery guarantees for this framework, covering varying degrees\nof knowledge of the signal and interference support, which are relevant for a\nlarge number of practical applications. Our results assume that the sparsifying\ndictionaries are characterized by coherence parameters and we require\nrandomness only in the signal and/or interference. The obtained recovery\nguarantees show that one can recover sparsely corrupted signals with\noverwhelming probability, even if the sparsity of both the signal and\ninterference scale (near) linearly with the number of measurements."}, "authors": ["Graeme Pope", "Annina Bracher", "Christoph Studer"], "author_detail": {"name": "Christoph Studer"}, "author": "Christoph Studer", "arxiv_comment": "submitted", "links": [{"href": "http://arxiv.org/abs/1203.6001v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1203.6001v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1203.6001v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1203.6001v2", "journal_reference": null, "doi": null, "fulltext": "1\n\nProbabilistic Recovery Guarantees\nfor Sparsely Corrupted Signals\nGraeme Pope, Student Member, IEEE, Annina Bracher, and\n\narXiv:1203.6001v2 [cs.IT] 26 Sep 2012\n\nChristoph Studer, Member, IEEE\n\nAbstract\nWe consider the recovery of sparse signals subject to sparse interference, as introduced in Studer\net al., IEEE Trans. IT, 2012. We present novel probabilistic recovery guarantees for this framework,\ncovering varying degrees of knowledge of the signal and interference support, which are relevant for a\nlarge number of practical applications. Our results assume that the sparsifying dictionaries are characterized by coherence parameters and we require randomness only in the signal and/or interference. The\nobtained recovery guarantees show that one can recover sparsely corrupted signals with overwhelming\nprobability, even if the sparsity of both the signal and interference scale (near) linearly with the number\nof measurements.\n\nIndex Terms\nSparse signal recovery, probabilistic recovery guarantees, coherence, basis pursuit, signal restoration,\nsignal separation, compressed sensing.\n\nThe material in this paper was presented in part at the IEEE Information Theory Workshop (ITW), September, Lausanne,\nSwitzerland, 2012 [1].\nG. Pope and A. Bracher are with the Dept. of Information Technology and Electrical Engineering, ETH Zurich, 8092 Zurich,\nSwitzerland (e-mail: gpope@nari.ee.ethz.ch, brachera@student.ethz.ch).\nC. Studer is with the Dept. of Electrical and Computer Engineering, Rice University, Houston, TX 77005, USA (e-mail:\nstuder@rice.edu).\nThe work of C. Studer was supported by the Swiss National Science Foundation (SNSF) under Grant PA00P2-134155.\nNovember 21, 2018\n\nDRAFT\n\n\f2\n\nI. I NTRODUCTION\nWe consider the problem of recovering the sparse signal vector x \u2208 Cna with support set X\n\n(containing the locations of the non-zero entries of x) from m linear measurements [2]\n\n(1)\n\nz = Ax + Be.\n\nHere, A \u2208 Cm\u00d7na and B \u2208 Cm\u00d7nb are given and known dictionaries, i.e., matrices that are\n\npossibly over-complete and whose columns have unit Euclidean norm. The vector e \u2208 Cnb with\n\nsupport set E represents the sparse interference. We investigate the following models for the\nsparse signal vector x and sparse interference vector e, and their support sets X and E:\n\u2022\n\nThe interference support set E is arbitrary, i.e., E \u2286 {1, . . . , nb } can be any subset of\ncardinality ne . In particular, E may depend upon the sparse signal vector x and/or the\n\ndictionary A, and hence, may also be chosen adversarially. The support set X of x is\nchosen uniformly at random, i.e., X is chosen uniformly at random from all subsets of\n\n{1, . . . , na } with cardinality nx .\n\n\u2022\n\nThe support set E of the sparse interference vector e is chosen uniformly at random, i.e.,\nE is chosen uniformly at random from all subsets of {1, . . . , nb } with cardinality ne . The\nsupport set X is assumed to be arbitrary and of size nx .\n\n\u2022\n\nBoth X and E, the support sets of the signal and of the interference with size nx and ne ,\nrespectively, are chosen uniformly at random.\n\nIn addition, for each model on the support sets X and E we may or may not know either of the\nsupport sets prior to recovery.\n\nAs discussed in [2], recovery of the sparse signal vector x from the sparsely corrupted\nobservation z in (1) is relevant in a large number of practical applications. In particular, restoration\nof saturated or clipped signals [3]\u2013[5], signals impaired by impulse noise [6]\u2013[8], or removal of\nnarrowband interference is captured by the input-output relation (1). Furthermore, the model (1)\nenables us to investigate sparsity-based super-resolution and in-painting [9], [10], as well as\nsignal separation [11], [12]. Hence, identifying the fundamental limits on the recovery of the\nvector x from the sparsely corrupted observation z is of significant practical interest.\nRecovery guarantees for sparsely corrupted signals have been partially studied in [2], [3],\n[13]\u2013[20]. In particular, [2], [13] investigated coherence-based recovery guarantees for arbitrary\nsupport sets X and E and for varying levels of support-set knowledge; [14] analyzed the special\nDRAFT\n\nNovember 21, 2018\n\n\f3\n\ncase where both support sets are unknown, but one is chosen arbitrarily and the other at random.\nThe recovery guarantees in [15]\u2013[17] require that the measurement matrix A is chosen at random\nand that B is unitary. The guarantees in [3], [18]\u2013[20] characterize A by the restricted isometry\nproperty (RIP), which is, in general, difficult to verify in practice. The recovery guarantees [3],\n[16], [18] require B to be unitary, whereas [19], [20] only consider a single dictionary A and\npartial support-set knowledge within A. The case of support-set knowledge was also addressed\nin [21], but for a model that differs considerably from the setting here. Specifically, [21] uses\na time-evolution model that incorporates support-set knowledge obtained in previous iterations\nand the corresponding results are based on the RIP. Finally, [22] considered a model where the\ninterference is sparse in an unknown basis. The specific models and assumptions underlying\nthe results in [3], [15]\u2013[22] reduce their utility for the applications outlined above.\nA. Generality of the signal and interference model\nIn this paper, we will exclusively focus on probabilistic results where the randomness is in the\nsignal and/or the interference but not in the dictionary. Furthermore, the dictionaries A and B\nwill be characterized only by their coherence parameters and their dimensions. Such results\nenable us to operate with a given (and arbitrary) pair of sparsifying dictionaries A and B, rather\nthan hoping that the signal will be sparse in a randomly generated dictionary (as in [17]) or\nthat A satisfies the RIP. The following two application examples illustrate the generality of our\nresults.\n1) Restoration of saturated signals: In this example, a signal y = Ax is subject to saturation [2]. This impairment is captured by setting z = ga (y) in (1), where ga (*) implements\nelement-wise saturation to [\u2212a, a] with a being the saturation level. By writing z = y + e with\ne = ga (y) \u2212 y, where e is non-zero only for the entries where the saturation in z occurs, we\nsee that for moderate saturation levels a, the vector e will be sparse. The reconstruction of\n\nthe (uncorrupted) signal y from the saturated measurement z, amounts to recovering x from\nz = Ax + e, followed by computing y = Ax.\nWe assume that the signal y = Ax is drawn from a stochastic model where x has a support\nset chosen uniformly at random. Since the saturation artifacts modeled by e are dependent on y,\nwe want to guarantee recovery for arbitrary E. Furthermore, we can identify the locations where\n\nthe saturation occurs (e.g., by comparing the entries of z to the saturation level a) and hence,\n\nNovember 21, 2018\n\nDRAFT\n\n\f4\n\nwe can assume that E is known prior to recovery. The recovery guarantees developed in this\n\npaper include this particular combination of support-set knowledge and randomness as a special\ncase, whereas the recovery guarantees in [2], [14], [23] are unable to consider all aspects of this\nmodel and turn out to be more restrictive.\n2) Removal of impulse noise: Consider a signal y = Ax that is subject to impulse noise.\nSpecifically, we observe z = y + e, where e is the impulse noise vector. For a sufficiently low\nimpulse-noise rate, e will be sparse in the identity basis, i.e., B = I. As before, consider the\nsetting where y = Ax is generated from a stochastic model with unknown support set X . Since\n\nimpulse noise does not, in general, depend on the signal y, we may chose E at random. In\naddition, the locations E of the impulse noise are normally unknown.\n\nRecovery guarantees for this setting are partially covered by [2], [14], [23]. However, as for\n\nthe saturation example above, the recovery guarantees in [2], [14], [23] are unable to exploit\nall aspects of support-set knowledge and randomness. The results developed here cover this\nparticular setting as a special case and hence, lead to less restrictive recovery guarantees.\nIn fact, there is an even more general setting compared to (1), which encompasses the cases\nlisted in Table I. Specifically, a generalization would be to consider the model z = Ax + Be\nwith X = supp(x) = Xr \u222a Xa and E = supp(e) = Er \u222a Ea where the support set X is known\nand E is unknown, and, furthermore, Xa and Ea are chosen arbitrarily and Xr and Er are chosen\nuniformly at random. The analysis of this model, however, is left for future work.1\n\nB. Contributions\nIn this paper, we present probabilistic recovery guarantees that improve or refine the ones in [2],\n[14], [23] and cover novel cases for varying degrees of knowledge of the signal and interference\nsupport sets. Our results depend on the coherence parameters of the two dictionaries A and B,\ntheir dimensions, and their spectral norms. In particular, we present novel recovery guarantees\nfor the situations where the support sets X and/or E are chosen at random, and for the cases\nwhere knowledge of neither, one, or both support sets X and E is available prior to recovery.\nFor the case where one support set is random and the other arbitrary, but no knowledge of X\n\nand E is available, we present an improved (i.e., less restrictive) recovery guarantee than the\n1\n\nNote that our model corresponds to the case where two of the sets Xr , Xa , Er , and Ea are forced to be the empty set.\n\nDRAFT\n\nNovember 21, 2018\n\n\f5\n\nTABLE I: Summary of all recovery guarantees for sparsely corrupted signals.\n\nX , E known\n\nE known\n\nX known\n\nneither known\n\nX , E arbitrary\n\nX random, E arbitrary\n\nX arbitrary, E random\n\nX , E random\n\nCase 1a\n\nCase 1b\n\nCase 1b\n\nCase 1c\n\n[2, Thm. 3]\n\nTheorem 1\n\nTheorem 1\n\nTheorem 1\n\nCase 2a\n\nCase 2b\n\nCase 2c\n\nCase 2d\n\n[2, Thm. 4]\n\nTheorem 2\n\nTheorem 4\n\nTheorem 3\n\nCase 2a\n\nCase 2c\n\nCase 2b\n\nCase 2d\n\n[2, Cor. 6]\n\nTheorem 4\n\nTheorem 2\n\nTheorem 3\n\nCase 3a\n\nCase 3b\n\nCase 3b\n\nCase 3c\n\n[14, Thms. 2 and 3]\n\nTheorem 5 and [14, Thm. 6]\n\nTheorem 5 and [14, Thm. 6]\n\nTheorem 6\n\nexisting one in [14, Thm. 6]. Finally, we show that `1 -norm minimization is able to recover the\nvectors x and e with overwhelming probability, even if the number of non-zero components in\nboth scales (near) linearly with the number of measurements.\nA summary of all the cases studied in this paper is given in Table I; the theorems highlighted\nin dark gray indicate novel recovery guarantees, light gray indicates refined ones. We will only\nprove the boldface theorems; the corresponding symmetric cases are shown in italics and the\nassociated recovery guarantees can be obtained by interchanging the roles of x and e.\n\nC. Notation\nLowercase and uppercase boldface letters stand for column vectors and matrices, respectively.\nFor the matrix M, we denote its transpose, adjoint, and (Moore\u2013Penrose) pseudo-inverse by\nMT , MH , and M\u2020 , respectively. The jth column and the entry in the ith row and jth column\nof the matrix M is designated by mj and [M]i,j , respectively. The minimum and maximum\nsingular value of M are given by \u03c3min (M) and \u03c3max (M), respectively; the spectral norm is\nkMk2,2 = \u03c3max (M). The `1 -norm of the vector v is denoted by kvk1 and kvk0 stands for\n\nthe number of nonzero entries in v. Sets are designated by upper-case calligraphic letters; the\ncardinality of the set S is |S|. The support set of v, i.e., the indices of the nonzero entries, is\n\ngiven by supp(v). The matrix MS is obtained from M by retaining the columns of M with\nindices in S; the vector vS is obtained analogously from the vector v. The sign(*) function\nNovember 21, 2018\n\nDRAFT\n\n\f6\n\napplied to a vector returns a vector consisting of the phases of each entry. The N \u00d7 N restriction\n\nmatrix RS for the set S \u2286 {1, . . . , N } has [RS ]k,k = 1 if k \u2208 S and is zero otherwise. For\n\nrandom variables X and Y , we define Eq [X] = E[|X|q ]\n\n1/q\n\nto be the qth moment, which defines\n\nan `q -norm on the space of complex-valued random variables, and hence satisfies the triangle\ninequality. We define EqX [f (X, Y )] to be the qth moment with respect to X and we define\n\n1[\u03bc 6= 0] to be equal to 1 if the condition \u03bc 6= 0 holds and 0 otherwise. For two functions f and\ng we write f \u223c g to indicate that f (n)/g(n) \u2192 1 as n \u2192 \u221e, and we say that \"f scales with g.\"\n\nThroughout the paper, X = supp(x) is assumed to be of cardinality nx and E = supp(e)\n\nof cardinality ne . We define D = [ A B ] and DX ,E = [ AX BE ] to be the sub-dictionary of D\nassociated with the non-zero entries of x and e. Similarly, we define the vector sX ,E = [ xTX eTE ]T\n\nwhich consists of the non-zero components of s = [ xT eT ]T .\nD. Outline of the paper\n\nThe remainder of the paper is organized as follows. Related prior work is summarized in\nSection II. The main theorems are presented in Section III and a corresponding discussion is\ngiven in Section IV. We conclude in Section V. All proofs are relegated to the Appendices.\nII. R ELATED P RIOR W ORK\nWe next summarize relevant prior work on sparse signal recovery and sparsely corrupted\nsignals, and we put our results into perspective.\nA. Coherence-based recovery guarantees\nDuring the last decade, numerous deterministic and probabilistic guarantees for the recovery\nof sparse signals from linear (and non-adaptive) measurements have been developed [23]\u2013[31].\nThese results give sufficient conditions for when one can reconstruct the sparse signal vector x\nfrom the (interference-less) observation y = Ax by solving\n(P0)\n\nminimize kx\u0302k0\nx\u0302\n\nsubject to y = Ax\u0302,\n\nor its convex relaxation, known as basis pursuit, defined as\n(BP) minimize kx\u0302k1\nx\u0302\n\nDRAFT\n\nsubject to y = Ax\u0302.\nNovember 21, 2018\n\n\f7\n\nIn particular, in [24]\u2013[26] it is shown that if kxk0 6 nx for some nx < (1 + 1/\u03bca ) /2 with the\ncoherence parameter\n\n(2)\n\n\u03bca = max |hai , aj i| ,\ni,j,i6=j\n\nthen (P0) and (BP) are able to perfectly recover the sparse signal vector x. Such coherence-based\nrecovery guarantees are, however, subject to the \"square-root bottleneck\", which only guarantees\n\u221a\nthe recovery of x for sparsity levels on the order of nx \u223c m [23]. This behavior is an immediate\nconsequence of the Welch bound [32] and dictates that the number of measurements must grow\nat least quadratically in the sparsity level of x to guarantee recovery. In order to overcome this\nsquare-root bottleneck, one must either resort to a RIP-based analysis, e.g., [27]\u2013[30], which\ntypically requires randomness in the dictionary A, or a probabilistic analysis that only considers\nrandomness in the vector x, and where A is constant (known) and solely characterized by its\ncoherence parameter, dimension, and spectral norm [23]. In this paper, we are interested in the\nlatter type of results. Such probabilistic and coherence-based recovery guarantees that overcome\nthe square-root bottleneck have been derived for (P0) and (BP) in [23]. The corresponding\nresults, however, do not exploit the structure of the problem (1), i.e., the fact that we are dealing\nwith two dictionaries and that knowledge of X and/or E may be available prior to recovery.\nB. Recovery guarantees for sparsely corrupted signals\nGuarantees for the recovery of sparsely corrupted signals as modeled by (1) have been\ndeveloped recently in [2], [13], [14]. The reference [2] considers deterministic (and coherencebased) results for several cases2 which arise in different applications: 1) X = supp(x) and\nE = supp(e) are known prior to recovery, 2) only one of X and E is known, and 3) neither X\n\nnor E are known. For case 1), the non-zero entries of both the signal and interference vectors\ncan be recovered by [2]\n\nsX ,E = D\u2020X ,E z,\n\n(3)\n\nif the recovery guarantee in [2, Thm. 2] is satisfied. For case 2), recovery is performed by using\nmodified versions of (P0) and (BP); the associated recovery guarantees can be found in [2,\n2\n\nNote that no efficient recovery algorithm with corresponding guarantees is known for the case studied in [2], where only the\n\ncardinality of X or E is known. Thus, we do not consider this case in the remainder of the paper.\nNovember 21, 2018\n\nDRAFT\n\n\f8\n\nThm. 4 and Cor. 6]. For case 3), recovery guarantees for the standard (P0) or (BP) algorithms\nare given in [14, Thms. 2 and 3]. However, all these recovery guarantees suffer from the squareroot bottleneck, as they guarantee recovery for all signal and all interference vectors satisfying\nthe given sparsity constraints. A notable exception for case 3) was discussed in [14, Thm. 6].\nThere, e is assumed to be random, but x is assumed to be arbitrary. This model overcomes the\nsquare-root bottleneck and is able to significantly improve upon the corresponding deterministic\nrecovery guarantees in [14, Thms. 2 and 3].\nAnother strain of recovery guarantees for sparsely corrupted signals that are able to overcome\nthe square-root bottleneck have been developed in [3], [15]\u2013[20]. The references [15]\u2013[17]\nconsider the case where A is random, whereas [3], [18]\u2013[20] consider matrices A that are\ncharacterized by the RIP, which is, in general, difficult to verify for a given (deterministic) A.\nIndeed, it has been recently shown that calculating the RIP for a given matrix is NP-hard [33].\nMoreover, the recovery guarantees in [3], [15]\u2013[18] require that B is an orthogonal matrix\nand, hence, these results do not allow for arbitrary pairs of dictionaries A and B. In addition,\n[16], [18] do not study the impact of support-set knowledge on the recovery guarantees. The\nresults in [19], [20] only consider a single dictionary with partial support-set knowledge and,\nthus, are unable to exploit the fact that the signal and interference exhibit sparse representations\nin two different dictionaries. While all these assumptions are valid for applications based on\ncompressive sensing (see, e.g., [34], [35]), they are not suitable for the application scenarios\noutlined in Section I.\nTo overcome the square-root bottleneck for arbitrary pairs of dictionaries A and B, we next\npropose a generalization of the probabilistic models developed in [14], [23] for the cases 1), 2),\nand 3) outlined above. In particular, we impose a random model on the signal and/or interference\nvectors rather than on the dictionaries, and we allow for varying degrees of knowledge of the\nsupport sets X and E. An overview of the coherence-based recovery guarantees developed next\n\nis given in Table I.\n\nIII. M AIN R ESULTS\nThe recovery guarantees developed next rely upon the models M(P0) and M(BP) summarized\n\nin Model 1 and Model 2, respectively. Model 2 differs subtly from the model in [14] in that we\ndo not require the uniform phase assumption in the vector with known support, a setting which\nDRAFT\n\nNovember 21, 2018\n\n\f9\n\nModel 1 M(P0)\nn\nn\n\u2022 Let x \u2208 C a and e \u2208 C b have support set X and E, respectively, of which at least one\nis chosen uniformly at random and where the non-zero entries of both x and e are drawn\nfrom a continuous distribution.\n\u2022\n\nThe observation z is given by z = Ax + Be.\n\nModel 2 M(BP)\n\u2022\n\u2022\n\nThe conditions of M(P0) hold.\n\nIf X or E is unknown, then assume that the corresponding non-zero entries of the associated\n\nvector(s) are drawn from a continuous distribution, where the phases of the individual\ncomponents are independent and uniformly distributed on [0, 2\u03c0).\n\nwas not considered in [14]. In addition to the Models 1 and 2, our results require the coherence\nparameters3 of the dictionaries A and B, i.e., the coherence \u03bca of A in (2), the coherence \u03bcb\nof B given by\n\u03bcb = max |hbi , bj i| ,\ni,j,i6=j\n\nand the mutual coherence \u03bcm between A and B, defined as\n\u03bcm = max |hai , bj i| .\ni,j\n\nOur main results for the cases highlighted in Table I are detailed next.\n\nA. Cases 1b and 1c: X and E known\nWe start with the case where both support sets X and E are known prior to recovery. The\n\nfollowing theorem guarantees recovery of x and e from z, using (3), with high probability.\n\nTheorem 1 (Cases 1b and 1c): Let x and e be signals satisfying the conditions of M(P0),\n\nassume that both X and E are known, and choose \u03b2 > log(nx ). If X is chosen uniformly at\n3\n\nNote that we could also characterize the dictionaries A and B with the cumulative coherence [25]. For the sake of simplicity\n\nof exposition, however, we stick to the coherence parameters \u03bca , \u03bcb , and \u03bcm only.\nNovember 21, 2018\n\nDRAFT\n\n\f10\n\nrandom, E is arbitrary, and if\n\u03b4e\n\n\u2212 14\n\nr\n\np\nnx\n+ 12\u03bca \u03b2nx + (ne \u2212 1)\u03bcb\nna\np\n2nx\n+ 1[\u03bca 6= 0]\nkAk22,2 + 3\u03bcm 2\u03b2ne ,\nna\n\n> kAk2,2 kBk2,2\n\n(4)\n\nholds with4 \u03b4 = 1, then we can recover x and e using (3) with probability at least 1 \u2212 e\u2212\u03b2 .\nIf both X and E are chosen at random and if\n\np\n\u221a\n\u221a\n1\n2nx\n2ne\n\u03b4e\u2212 4 > 12 \u03b2 (\u03bca nx + \u03bcb ne ) + 1[\u03bca 6= 0]\nkAk22,2 + 1[\u03bcb 6= 0]\nkBk22,2\nna\nnb\n\u001a\n\u001b\nr\nr\np\np\nne\nnx\nH\nH\n+ min 3\u03bcm 2\u03b2nx +\nA B 2,2 , 3\u03bcm 2\u03b2ne +\nA B 2,2 ,\nnb\nna\n\n(5)\n\nholds with \u03b4 = 1 and \u03b2 > max{log(nx ), log(ne )}, then we can recover x and e using (3) with\nprobability at least 1 \u2212 e\u2212\u03b2 .\n\nProof: See Appendix B.\n\nA discussion of the recovery conditions (4) and (5) is relegated to Section IV.5\nB. Cases 2b and 2d: E known\nConsider the case where only the support set E of e is known prior to recovery. In this case,\n\nrecovery of x (and the non-zero entries of e) from z can be achieved by solving [2]6\n\uf8f1\n\uf8f2 minimize kx\u0302k0 + k\u00eaE k0\nx\u0302,\u00eaE\n(P0? , E)\n\uf8f3 subject to z = Ax\u0302 + B \u00ea ,\nE E\n\n(6)\n\nor its convex relaxation7\n(BP? , E)\n\n\uf8f1\n\uf8f2 minimize\nx\u0302,\u00eaE\n\nkx\u0302k1 + k\u00eaE k1\n\n(7)\n\n\uf8f3 subject to z = Ax\u0302 + B \u00ea .\nE E\n\n4\n\nLater we will require (4) to hold for different values of \u03b4.\n\n5\n\nIn order to slightly improve the conditions in (4) or (5), one could replace the term (ne \u2212 1)\u03bcb with the cumulative coherence\n\nas defined in [25].\n6\n\nNote that since E is known, the term k\u00eaE k0 in (P0? , E) can be omitted. We keep the term, however, for the sake of consistency\n\nwith the problem (BP? , E).\n7\n\nNote that we consider a slightly different convex optimization problem (BP? , E) to that proposed in [2], (BP, E), for the\n\ncase where E is known prior to recovery. In practice, however, both problems exhibit similar recovery performance.\nDRAFT\n\nNovember 21, 2018\n\n\f11\n\nThe following theorems guarantee the recovery of x and e from z, using (P0? , E) or (BP? , E),\nwith high probability.\n\nTheorem 2 (Case 2b): Let x and e be signals satisfying the conditions of M(P0), assume\n\nthat E is known prior to recovery and chosen arbitrarily, and assume that X is unknown and\n\ndrawn uniformly at random. Choose \u03b2 > log(nx ). If (4) holds for some \u03b4 where 0 < \u03b4 < 1 and\nif\nnx \u03bc2a + ne \u03bc2m < 1 \u2212 \u03b4,\n\n(8)\n\nthen we can recover x and e using (P0? , E) with probability at least 1 \u2212 e\u2212\u03b2 .\nif\n\nMoreover, if x and e are signals satisfying the conditions of M(BP), and, in addition to (4),\nnx \u03bc2a\n\n+\n\nne \u03bc2m\n\n(1 \u2212 \u03b4)2\n<\n,\n2(log(na ) + \u03b2)\n\n(9)\n\nholds, then we can recover x and e using (BP? , E) with probability at least 1 \u2212 3e\u2212\u03b2 .\nProof: See Appendices C and D.\n\nNote that by combining (4), (8), and possibly (9) into a single recovery condition, thereby\neffectively removing \u03b4, we can easily calculate the largest values of nx and ne for which\nsuccessful recovery with high probability is guaranteed (see Section IV-C for a corresponding\ndiscussion).\nTheorem 3 (Case 2d): Let x and e be signals satisfying the conditions of M(P0), assume\n\nthat E is known but X is unknown prior to recovery, and assume that both X and E are drawn\n\nuniformly at random. If (5) and (8) hold for some 0 < \u03b4 < 1 and \u03b2 > max{log(nx ), log(ne )},\nthen we can recover x and e using (P0? , E) with probability at least 1 \u2212 e\u2212\u03b2 .\n\nMoreover, if x and e are signals satisfying the conditions of M(BP) and if (9) holds in\n\naddition to (5) and (8), then we can recover x and e using (BP? , E) with probability at least\n1 \u2212 3e\u2212\u03b2 .\n\nProof: See Appendices C and D.\n\nA discussion of both theorems is relegated to Section IV.\nC. Case 2c: X known\nThe case where X is random and known, and E is unknown and arbitrary, differs slightly to the\n\ncase where X is random and unknown, and E is arbitrary and known (covered by Theorem 2).\nNovember 21, 2018\n\nDRAFT\n\n\f12\n\nHence, we need to consider both cases separately. The recovery problems (P0? , X ) and (BP? , X )\nrequired here are defined analogously to (P0? , E) and (BP? , E).\n\nTheorem 4 (Case 2c): Let x and e be signals satisfying the conditions of M(P0), assume\n\nthat the support set X is known and chosen uniformly at random, and assume that E is unknown\nand arbitrary. If\n\n\u03b4e\n\n\u2212 41\n\nr\n\np\nne\n+ 12\u03bcb \u03b2ne + (nx \u2212 1)\u03bca\nnb\np\n2ne\nkBk22,2 + 3\u03bcm 2\u03b2nx ,\n+ 1[\u03bcb 6= 0]\nnb\n\n> kAk2,2 kBk2,2\n\n(10)\n\nholds for some 0 < \u03b4 < 1 and \u03b2 > log(ne ), and if\n(11)\n\nnx \u03bc2m + ne \u03bc2b < 1 \u2212 \u03b4,\nthen we can recover x and e using (P0, X ) with probability at least 1 \u2212 e\u2212\u03b2 .\nif\n\nMoreover, if x and e are signals satisfying the conditions of M(BP), and, in addition to (10),\nnx \u03bc2m\n\n+\n\nne \u03bc2b\n\n(1 \u2212 \u03b4)2\n<\n,\n2(log(nb ) + \u03b2)\n\n(12)\n\nholds, then we can recover x and e using (BP, X ) with probability at least 1 \u2212 3e\u2212\u03b2 .\nProof: See Appendices C and D.\n\nA discussion of this theorem is relegated to Section IV.\n\nD. Cases 3b and 3c: No support-set knowledge\nRecovery guarantees for the case of no support-set knowledge, but where one support set is\nchosen at random and the other arbitrarily can be found in [14, Thm. 6]. The theorem shown\nnext is able to refine the result in [14, Thm. 6]. The refinements are due to the following facts: i)\nWe allow for arbitrary 0 < \u03b4 < 1, whereas \u03b4 = 1/2 in [14, Thm. 6], ii) we add a correction\nterm improving the bounds when either A or B are unitary, and iii) we do not use a global\ncoherence parameter \u03bc = max{\u03bca , \u03bcb , \u03bcm }, but rather we further exploit the individual coherence\nparameters \u03bca , \u03bcb , and \u03bcm of A and B. See Section IV-A for a corresponding discussion.\n\nTheorem 5 (Case 3b): Let x and e be signals satisfying the conditions of M(P0), assume\n\nthat X is chosen uniformly at random, and assume that E is arbitrary. If (4), (8), and (11) hold\nDRAFT\n\nNovember 21, 2018\n\n\f13\n\nfor some 0 < \u03b4 < 1 and \u03b2 > log(nx ), then\n(P0? )\n\nminimize kx\u0302k0 + k\u00eak0\nx\u0302,\u00ea\n\nsubject to z = Ax\u0302 + B\u00ea,\n\nrecovers x and e with probability at least 1 \u2212 e\u2212\u03b2 .\n\nMoreover, if x and e are signals satisfying the conditions of M(BP) and if (9) and (12) hold\n\nin addition to (4), (8), and (11), then\n(BP? )\n\nminimize kx\u0302k1 + k\u00eak1\nx\u0302,\u00ea\n\nsubject to z = Ax\u0302 + B\u00ea,\n\nrecovers x and e with probability at least 1 \u2212 3e\u2212\u03b2 .\nProof: See Appendices C and D\n\nTheorem 6 (Case 3c): Let x and e be signals satisfying the conditions of M(P0) and assume\n\nthat X and E are both unknown and chosen uniformly at random. If (5), (8), and (11) hold for\n\nsome 0 < \u03b4 < 1 and \u03b2 > max{log(nx ), log(ne )}, then (P0? ) recovers x and e with probability\nat least 1 \u2212 e\u2212\u03b2 .\n\nMoreover, if x and e are signals from M(BP) and if (9) and (12) hold in addition to (5), (8),\n\nand (11), then (BP? ) recovers x and e with probability at least 1 \u2212 3e\u2212\u03b2 .\nProof: See Appendices C and D.\n\nA discussion of both theorems is given below.\nIV. D ISCUSSION OF THE R ECOVERY G UARANTEES\nWe now discuss the theorems presented in Section III. In particular, we study the impact of\nsupport-set knowledge on the recovery guarantees and characterize the asymptotic behavior of\nthe corresponding recovery conditions, i.e., the threshold for which recovery is guaranteed with\nhigh probability.\nIn the ensuing discussion, we consider two scenarios. For the first scenario, we assume that\nA and B are unitary, i.e., na = nb = m and \u03bca = \u03bcb = 0, and maximally incoherent, i.e.,\n\u221a\n\u03bcm = 1/ m. For example, A could be the discrete Fourier transform (or Hadamard) matrix\nwith appropriately normalized columns and B the identity matrix. The corresponding plots are\nshown in Figure 1. For the second scenario, A is assumed to be unitary and B is assumed to\nbe the concatenation of two unitary matrices so that m = na = 108 , nb = 2na , \u03bca = 0, and\n\u221a\n\u03bcb = \u03bcm = 1/ m as described in [36], [37]. The corresponding plots are shown in Figure 2.\nNovember 21, 2018\n\nDRAFT\n\n\f14\n\nX , E arb [2]\n\nX , E rand\n\nX , E rand\n(P0? , E)\n(BP? , E)\n\nX rand, E arb [14]\n\nX rand, E arb\n\n105 X , E arb [2]\n103\n\nX rand, E arb\n\n101\n\nX , E rand [23]\n\nX rand, E arb\n(P0? , E) & (BP? , E)\n\nX , E arb [14]\n\nX , E rand\n101\n\n103\n\n105\n\n107\n\ninterference sparsity, ne\n\n(a) X and E known\n\nX , E rand\n\nsignal sparsity, nx\n\n107\n\n101\n\n103\n\n105\n\n107\n\ninterference sparsity, ne\n\n(b) E known\n\n101\n\n103\n\n105\n\n107\n\ninterference sparsity, ne\n\n(c) X and E unknown\n\n\u221a\nFig. 1: A and B are assumed to be unitary with m = na = nb = 108 and \u03bcm = 1/ m. In\n(a) the darker curves in the upper-right are for m = 108 and the lighter curves in the lower-left\nare for m = 104 . In (c) we show the recovery regions only for (BP? ). In each case, recovery is\nguaranteed with probability at least 1 \u2212 10\u22128 .\n\nIn each case we set \u03b2 = log(m) or \u03b2 = log(m)/3 for the `0 -norm and `1 -norm-based recovery\nproblems, respectively, so that recovery is guaranteed with probability at least 1 \u2212 1/m.\n\nIn order to plot the recovery conditions, we note that for a pair of unitary matrices and a\n\u221a\ngiven ne , the recovery conditions of the theorems are quadratic equations in nx ; this enables\nus to calculate the maximum nx guaranteeing the successful recovery of x and e in closed form.\nA. Recovery guarantees\n1) X and E known: Figure 1a shows the recovery conditions for the cases when both support\n\nsets X and E are assumed to be known. For small problem dimensions, i.e., m = 104 , the\n\nrecovery conditions where both support sets are assumed to be arbitrary turn out to be less\nrestrictive than for the case where both support sets are chosen at random. For large problem\ndimensions, i.e., m = 108 , we see, however, that the probabilistic results of Theorem 1 guarantee\nthe recovery (with high probability) for larger nx and ne than the deterministic results of [2]\nconsidering arbitrary support sets. Hence, the probabilistic recovery conditions presented here\nrequire a sufficiently large problem size in order to outperform the corresponding deterministic\nresults. We furthermore see from Figure 1a that one can guarantee the recovery of signals having\nDRAFT\n\nNovember 21, 2018\n\n\f15\n\na larger number of non-zero entries if both support sets are chosen at random compared to the\nsituation where X is random but E is arbitrary.\n\n2) Only E known: Figure 1b shows the recovery conditions from Theorems 2 and 3 for the\n\ncases where only E is known prior to recovery (the case of only X known behaves analogously).\n\nWe see that for a random X and random E successful recovery at high probability is guaranteed\n\nfor significantly larger nx and ne compared to the case where one or both support sets are assumed\n\nto be arbitrary. Hence, having more randomness in the support sets leads to less restrictive\nrecovery guarantees. We now see that the recovery conditions for (P0? , E) are slightly less\nrestrictive than those for (BP? , E).\n\n3) No support-set knowledge: Finally, Figure 1c shows the recovery conditions for (BP? ) for\n\nthe case of no support-set knowledge. We see that for random X and E, successful recovery is\n\nguaranteed for significantly larger nx and ne compared to the case where one or both support\nsets are assumed to be arbitrary. As a comparison, we also show the recovery conditions derived\nin [14, Thm. 6] and the conditions from [23], the latter of which does not take into account the\nstructure of the problem (1). We see that the recovery conditions derived in Theorems 5 and 6\nare less restrictive, i.e., they guarantee the successful recovery (with high probability) for a larger\nnumber of nonzero coefficients in both the sparse signal vector x and the sparse interference e.\n4) Non-unitary B: We now consider the setting where B is the concatenation of two unitary matrices and plot the corresponding recovery threshold for differing levels of support set\nknowledge in Figure 2. For a fixed nx and na , we see that by increasing nb and \u03bcb , we suffer a\nsignificant loss in the number of non-zero entries of e that we can recover, when compared to\nthe case where B is unitary. However, the number of non-zero entries of x that we can guarantee\nto recover is virtually unchanged-an effect which is also present in the deterministic recovery\nconditions [2].\nB. Impact of support-set knowledge\nAs detailed in [2], having knowledge of the support set of x or e implies that one can guarantee\nthe recovery of x and e having up to twice as many non-zero entries (compared to the case of\nno support-set knowledge).\nA similar behavior is also apparent in the probabilistic results presented here. Specifically, for\nunitary and maximally incoherent A and B, the recovery conditions in Figure 3 using (3), (P0),\nNovember 21, 2018\n\nDRAFT\n\n\f16\n\nsignal sparsity, nx\n\n107\n\nX , E rand\n(P0? , E) &\n(BP? , E)\n\nX , E rand\n\nX rand, E arb [14]\n\nX rand, E arb\n\n105\nX , E arb [2]\n103\n\nX , E arb [2]\n\nX rand,\nE arb\n\n101\n101\n\n103\n\n105\n\n107\n\n101\n\ninterference sparsity, ne\n\n(a) X and E known\n\n103\n\nX rand,\nE arb\n(P0? , E) &\n(BP? , E)\n105\n\n107\n\ninterference sparsity, ne\n\n(b) E known\n\nX , E arb [14]\n\nX , E rand\n\nX , E rand [23]\n101\n\n103\n\n105\n\n107\n\ninterference sparsity, ne\n\n(c) X and E unknown\n\nFig. 2: A is assumed to be unitary and B is assumed to be the concatenation of two unitary\n\u221a\nmatrices so that m = na = 108 , nb = 2na , \u03bca = 0, and \u03bcb = \u03bcm = 1/ m as described in [36],\n[37]. In (c) we show the recovery regions only for (BP? ). In each case, recovery is guaranteed\nwith probability at least 1 \u2212 10\u22128 .\n\nand (P0? , E) show a similar factor-of-two gain in the case where both X and E are chosen at\n\nrandom. For example, knowledge of X enables one to recover a pair (x, e) with approximately\n\ntwice as many non-zero entries compared to the case of not knowing X . In Figure 4, we show the\n\nrecovery conditions for the case where one dictionary is unitary, but the other is a concatenation\nof two unitary matrices, as described earlier in Section IV. We again see that the extra supportset knowledge allows us to guarantee the recovery of a signal with more non-zero entries. It is\ninteresting to note that in both of these scenarios, by adding the knowledge of one of the support\nsets, we increase the number of non-zero components we can guarantee to recover in the other\nsignal component. For example, by knowing X prior to recovery, we can guarantee to recover\na signal with more non-zero entries in e.\n\nWe note that a similar gain is apparent for X arbitrary and E random, as well as for using\n\n(BP) and (BP? , E) instead of (P0) and (P0? , E).\n\nC. Asymptotic behavior of the recovery conditions\nWe now compare the asymptotic behavior of probabilistic and deterministic recovery conditions, i.e., we study the scaling behavior of nx and ne . To this end, we are interested in the\nDRAFT\n\nNovember 21, 2018\n\n\f17\n\n108\n\nX unk., E kn.\n\n107\n\nsignal sparsity, nx\n\n106\n\nX , E unk.\n\n105\n104\n\nX , E kn.\n\n103\n102\n101\n100\n100\n\nX unk., E kn.\n101\n\n102\n\n103\n\n104\n\n105\n\n106\n\n107\n\n108\n\ninterference sparsity, ne\n\nFig. 3: Impact of support-set knowledge on the recovery conditions for (3), (P0), and (P0? , E)\nin the case where X and E are both random. A and B are unitary with m = na = nb = 106\n\u221a\n(lower-left curves) and m = na = nb = 108 (upper-right curves) and \u03bcm = 1/ m.\n\nlargest nx for which recovery of x (and e) from z can be guaranteed with high probability. In\nparticular, we consider the following models for the sparse interference vector e: i) Constant\nsparsity, i.e., ne = 103 , ii) sparsity proportional to the square root of the problem size, i.e.,\n\u221a\nne = m, and iii) sparsity proportional to the problem size, i.e., ne = m/105 .\nFigure 5 shows the largest nx for which recovery can be guaranteed using (BP? , E). Here, E\n\nis assumed to be known and arbitrary and X is unknown and chosen at random. Note that the\n\nother cases of support-set knowledge and arbitrary/random exhibit the same scaling behavior.\nWe see from Figure 5 that for a constant interference sparsity (i.e., ne = 103 ), the probabilistic\n\nand deterministic results show the same scaling behavior. For the cases where ne scales with\n\u221a\nm or m, however, the deterministic thresholds developed in [2] result in worse scaling, while\nthe behavior of the probabilistic guarantees derived in this paper remain unaffected.\nWe now investigate the scaling behavior observed in Figure 5 analytically. Again, we only\nconsider the case where X is unknown and chosen at random and E is known and chosen\n\narbitrarily; an analysis of the other cases yields similar results. Assume that A and B are\nNovember 21, 2018\n\nDRAFT\n\n\f18\n\n108\n107\n\nE kn./unk.,\nX unknown\n\nsignal sparsity, nx\n\n106\n105\n104\n103\n102\n\nX kn./unk.,\nE known\n\n101\n100\n100\n\n101\n\n102\n\n103\n\n104\n\n105\n\n106\n\n107\n\n108\n\ninterference sparsity, ne\n\nFig. 4: Impact of support-set knowledge on the recovery conditions for (3), (P0), and (P0? , E)\n\nin the case where X and E are both random. In the top left we assume A is unitary and B\nis the concatenation of two unitary matrices so that m = na = 108 , nb = 2na , \u03bca = 0, and\n\u221a\n\u03bcb = \u03bcm = 1/ m as described in [36], [37]. For the curves in the bottom right (with X\n\nknown/unknown and E known) we reverse the roles of A and B, so that now B is unitary.\n\n\u221a\nunitary and maximally incoherent, i.e., \u03bca = \u03bcb = 0, na = nb = m, and \u03bcm = 1/ m. Then,\nby Theorem 2, the recovery of x from z using (BP? , E) is guaranteed with probability at least\n1 \u2212 3/na (i.e., for \u03b2 = log(na )) if\n\n\u03b4e\u22121/4 >\n\np\np\nnx /na + 3\u03bcm 2\u03b2ne ,\n\nand\n2ne \u03bc2m (log(na ) + \u03b2) < (1 \u2212 \u03b4)2 ,\nhold. Combining these two conditions gives\n\u221a\n\u221a\n1\u221a\n1 p\ne\u2212 4 m > nx + (3 2 + 2e\u2212 4 ) ne log(m).\n\n(13)\n\nHence, if nx \u223c m and ne \u223c m/ log(m), the condition (13) can be satisfied. Consequently,\n\nrecovery of x (and of e) is guaranteed with probability at least 1 \u2212 3/m even if nx scales\nDRAFT\n\nNovember 21, 2018\n\n\f19\n\nX arb, ne = 103\n\n1013\n\nsignal sparsity, nx\n\nX arb, ne =\n1010\n\n\u221a\n\nm\n\nX rand, ne = m/105\n\n107\n\n104\n\nX arb, ne = m/105\n101\n104\n\n106\n\nX rand, ne = 103 & ne =\n108\n\n1010\n\n1012\n\n\u221a\n\nm\n\n1014\n\nsignal dimensions, na = nb = m\n\nFig. 5: Maximum signal sparsity nx that ensures recovery of x for E known and arbitrary. We\n\u221a\nassume ne = 103 , ne = m, and ne = m/105 . The probability of successful recovery is set to\nbe at least 1 \u2212 10\u221215 .\n\nlinearly in the number of (corrupted) measurements m and ne scales near-linearly (i.e., with\nm/ log(m)) in m.\nWe finally note that the recovery guarantees in [16] also allow for the sparsity of the interference vector to scale near-linearly in the number of measurements. The results in [16], however,\nrequire the matrix A to be random and B to be orthogonal, whereas the recovery guarantees\nshown here are for arbitrary pairs of dictionaries A and B (characterized by the coherence\nparameters) and for varying degrees of support-set knowledge.\n\nD. No error component\nIt is worth briefly discussing how our results behave when there is no error, that is when\nne = 0. In this case, the relevant setting is with X unknown and chosen uniformly at random.\n\nAs Theorem 2 holds for any B, it suffices to take B equal to a single column8 , since ne = 0\n8\n\nTaking B to be the zero-matrix and so removing all the terms that appear in the recovery conditions also leads to the same\n\nscaling behavior.\nNovember 21, 2018\n\nDRAFT\n\n\f20\n\nmeans we do not consider any component of B when attempting to recover the signals. And\nsince the mutual coherence \u03bcm only appears as a product with ne , it does not matter what we\nassume \u03bcm to be. Thus by taking ne = 0 and applying Theorem 2 we find that for (P0? , E),\nrecovery is guaranteed with probability at least 1 \u2212 e\u2212\u03b2 if\nr\np\nnx\n2\n\u2212 14\n+ 12\u03bca \u03b2nx .\ne (1 \u2212 nx \u03bca ) > kAk2\nna\n\n(14)\n\nFor (BP? , E), recovery is guaranteed with probability at least 1 \u2212 3e\u2212\u03b2 if\nr\n\u0010\n\u0011\np\np\nnx\n\u2212 14\n2\n1 \u2212 2nx \u03bca (log(na ) + \u03b2) > kAk2\n+ 12\u03bca \u03b2nx .\ne\n(15)\nna\n\u221a\nNow assume that \u03bca \u223c 1/ m, kAk22 = na /m, and that \u03b2 = log(na ). Then (after ignoring lower\n\norder terms), we find that (14) and (15) imply recovery with probability at least 1 \u2212 1/na and\n\n1 \u2212 3/na , respectively, provided that\n\nm > C nx log(na ),\nfor some positive constant C. This result is in accordance with [23], the RIP-based proof of\n[38] which requires m > C0 nx log(na /nx ) to guarantee recovery with high probability, and\nthe random sub-sampling model of [27], which, for a maximally incoherent sparsity basis and\nmeasurement matrix9 , requires m > C1 nx log(na ) to guarantee recovery with high probability.\nThus, our results reduce to some of the existing results in the setting where there is no error.\nV. C ONCLUSIONS\nIn this paper, we have presented novel coherence-based recovery guarantees for sparsely\ncorrupted signals in the probabilistic setting. In particular, we have studied the case where the\nsparse signal and/or sparse interference vectors are modeled as random and the dictionaries\nA and B are solely characterized by their coherence parameters. Our recovery guarantees\ncomplete all missing cases of support-set knowledge and improve and refine the results in [2],\n[14]. Furthermore, we have shown that the reconstruction of sparse signals is guaranteed with\nhigh probability, even if the number of non-zero entries in both the sparse signal and sparse\ninterference are allowed to scale (near) linearly with the number of (corrupted) measurements.\n9\n\nFor example, measuring with a randomly sub-sampled Fourier matrix and taking the Identity matrix as the sparsity basis, so\n\nthat with the differently normalized definition of coherence as in [27], \u03bca = 1.\nDRAFT\n\nNovember 21, 2018\n\n\f21\n\nThere are many avenues for follow-on work. The derivation of probabilistic recovery guarantees for the more general setting studied in [13], i.e., z = Ax + Be + n with n being additive\nnoise and x and e being approximately sparse (rather than perfectly sparse), is left for future\nwork. In addition, our framework could be generalized to the setting where we split both the\nknown and the unknown support sets into a random and arbitrary part, resulting in four parts,\nas outlined in Section I-A2. Finally, the derivation of probabilistic uncertainty relations for pairs\nof general dictionaries is an interesting open problem and would complete the deterministic\nuncertainty relations in [2], [14].\nACKNOWLEDGMENTS\nThe authors would like to thank C. Aubel, R. G. Baraniuk, H. B\u00f6lcskei, I. Koch, P. Kuppinger,\nA. Pope, and E. Riegler for inspiring discussions. We would also like to thank the anonymous\nreviewers for their valuable comments, which improved the overall quality of the paper.\nA PPENDIX A\nB OUNDS ON \u03c3min (DX ,E )\nWe now derive probabilistic bounds on \u03c3min (DX ,E ), which are key in showing when the\nrecovery from sparsely corrupted signals succeeds. We extend [14, Lemma 7] to the case where\nboth supports X and E are chosen at random and give improved results for the case where only\none support set is random. First, we require the following two results from [23].\n\nTheorem 7 (Thm. 8 of [23]): Let M \u2208 Cm\u00d7n be a matrix. Let S \u2286 {1, 2, . . . , n} be a set of\n\nsize s drawn uniformly at random. Fix q > 1, then for each p > max{2, 2 log(rank(MRS )), q/2}\nwe have\nq\n\nh\n\nE kMRS k2,2\n\ni\n\n\u221a\n6 3 p kMk1,2 +\n\nr\n\ns\nkMk2,2 ,\nn\n\nwhere kMk1,2 = supv\u2208Cn kMvk2 / kvk1 and is the maximum `2 -norm of the columns of M.\n\nLemma 8 (Eq. 6.1 of [23]): Let M \u2208 Cm\u00d7n be a matrix with coherence \u03bc and let S \u2286\n\n{1, 2, . . . , n} be a set of size s chosen uniformly at random. Then, for \u03b2 > log(s) and q = 4\u03b2\nEq\nNovember 21, 2018\n\nh\n\nMH\nS MS \u2212 I\n\ni\n2,2\n\np\n2s\n6 12\u03bc \u03b2s + 1[\u03bc 6= 0] kMk22,2 .\nn\nDRAFT\n\n\f22\n\nNote that the result in [23, Eq. 6.1] does not include the indicator function 1[\u03bc 6= 0]. It\n\nis, however, straightforward to verify that if M is orthonormal, then \u03bc = 0 and hence,\nMH\nS MS \u2212 I\n\n2,2\n\n= 0 for all sets S.\n\nWe now state the main result for \u03c3min (DX ,E ).\n\nTheorem 9: Choose \u03b2 > log(nx ), q = 4\u03b2 and assume that A and B are characterized by the\n\ncoherence parameters \u03bca , \u03bcb , and \u03bcm . If i) X is chosen uniformly at random with cardinality nx ,\n\nE is arbitrary, and (4) holds, or ii) E is chosen uniformly at random with cardinality ne , X is\narbitrary, and (10) holds, or iii) both X and E are chosen uniformly at random with cardinalities\nnx and ne respectively, and (5) holds, then\nn\nP DH\nX ,E DX ,E \u2212 I\n\no\n\n2,2\n\n> \u03b4 6 e\u2212\u03b2 ,\n\n(16)\n\nand if (4), (5) or (10) hold with \u03b4 = 1, then\nP{\u03c3min (DX ,E ) = 0} 6 e\u2212\u03b2 .\n\n(17)\n\nProof: The proof follows that of [14, Lemma 7]. We start by defining the hollow Gram\nmatrix\nH = DH\nX ,E DX ,E\n\n\uf8ee\n\uf8f9\nH\nH\nAX AX \u2212 I\nA X BE\n\uf8fb.\n\u2212I=\uf8f0\nH\nBH\nA\nB\nB\n\u2212\nI\nX\nE\nE\nE\n\nSplitting H into diagonal and off-diagonal blocks and applying the triangle inequality leads to\n\uf8ee\n\uf8f9\n\uf8ee\n\uf8f9\nH\nH\nAX AX \u2212 I\n0\n0\nAX BE\n\uf8fb + \uf8f0\n\uf8fb\nkHk2,2 6 \uf8f0\nH\n0\nBH\nB\n\u2212\nI\nB\nA\n0\nE\nX\nE\nE\n2,2\n2,2\nn\no\nH\nH\n6 max AH\nX AX \u2212 I 2,2 , BE BE \u2212 I 2,2 + BE AX 2,2\n6 AH\nX AX \u2212 I\n\n2,2\n\n+ BH\nE BE \u2212 I\n\n2,2\n\n+ BH\nE AX\n\n2,2\n\n.\n\nSince the qth moment effectively defines an `q -norm, it satisfies the triangle inequality, namely,\nEq [|X + Y |] 6 Eq [|X|] + Eq [|Y |]. Hence, it follows that\nh\ni\nh\ni\nh\nq\nEq kHk2,2 6 Eq AH\nA\n\u2212\nI\n+\nE\nBH\nX\nX\nE BE \u2212 I\n2,2\n\ni\n2,2\n\n+ Eq\n\nh\n\nBH\nE AX\n\ni\n2,2\n\n.\n\n(18)\n\nWe now separately bound each of the terms in (18) and we do this for each case where X and E\n\nis either chosen at random or arbitrarily. If X is chosen uniformly at random, then it follows\nfrom Lemma 8 that\n\nEq\nDRAFT\n\nh\n\nAH\nX AX \u2212 I\n\ni\n2,2\n\n6 12\u03bca\n\np\n2nx\n\u03b2nx + 1[\u03bca 6= 0]\nkAk2,2 ,\nna\n\n(19)\nNovember 21, 2018\n\n\f23\n\nfor any 4\u03b2 = q > 4 log(nx ). If X is allowed to be arbitrary, then for all X we have\nAH\nX AX \u2212 I\n\n2,2\n\n6 max\nk\n\nX\nj6=k\n\n(20)\n\n[AH\nX AX ]j,k 6 (nx \u2212 1)\u03bca ,\n\nwhere the first inequality follows from the Ger\u0161gorin disc theorem [39, Thm. 6.1.1] and the\nsecond inequality is a consequence of the definition of \u03bca . By reversing the role of A and B,\nh\ni\nq\nH\nwe get the analogous bounds for the right-hand side (RHS) term E BE BE \u2212 I 2,2 in (18).\n\nFor the third summand appearing in the RHS of (18), let us first consider the case where E is\n\nchosen arbitrarily and X uniformly at random. We then want to apply Theorem 7 to M = BH\nE A\n\nand RX . Since MRX has ne rows and nx non-zero columns, rank(MRX ) 6 min{nx , ne }\n\nand thus we can apply Theorem 7 with q = 2p = 4\u03b2 where q > 4 min{log(nx ), log(ne )} >\n4 log(rank(MRX )) to get\nEq\n\nh\n\nBH\nE AX\n\ni\n2,2\n\n= EqX\n\nh\n\nBH\nE AX\n\n\u221a\n6 3 p BH\nE A\n\ni\n\n(21a)\n\n2,2\n\nr\n\nnx\nBH\n+\nE A 2,2\nna\nr\np\nnx\nBH A 2,2 ,\n6 3\u03bcm 2\u03b2ne +\nna\n1,2\n\n(21b)\n\nwhere the entries of BH\nE A are bounded by the mutual coherence \u03bcm . The case where E is\nrandom and X is arbitrary follows by reversing the roles of A and B.\n\nNow consider the case where both E and X are random. We can set M = BH A so that\nh\ni\nh h\nii\nq\nq\nA\nwe may write Eq BH\n=\nE\nE\nkR\nMR\nk\nin order to apply Theorem 7 to first\nX 2,2\nE\nX 2,2\nE\nE\nX\nbound the inner expectation, and then to bound the resulting outer expectation. However, this\n\napproach results in a worse bound compared to reusing (21b), which does not depend on E and\n\nhence holds for all E. By also taking the expectation in (21a) with respect to E instead of X\nand bounding similarly, we get that\nq\n\nE\n\nh\n\nBH\nE AX\n\ni\n2,2\n\n\u001a\n\nr\np\nne\nAH B\n6 min 3\u03bcm 2\u03b2nx +\nnb\nr\np\nnx\n3\u03bcm 2\u03b2ne +\nAH B\nna\n\n,\n\u001b\n\n2,2\n\n2,2\n\n,\n\n(22)\n\nfor any \u03b2 > min{log(nx ), log(ne )}. Combining (19), (20), (21b), and (22) with the analogous\nresults for B and E leads to the conditions (4), (5), and (10).\nNovember 21, 2018\n\nDRAFT\n\n\f24\n\nDue to (19) and the analogous result for BE , if X is chosen at random, we require \u03b2 > log(nx ),\n\nif E is chosen at random we need \u03b2 > log(ne ), and if both X and E are chosen at random, both\nof these conditions need to be satisfied, namely that \u03b2 > max{log(nx ), log(ne )}.\n\nWe now show that the conditions (4), (5), and (10) are sufficient to show that (16) holds.\nChebyshev's Inequality [40, Sec. 1.3] states that for a random variable X and a function\nf : R \u2192 R+\nP{X \u2208 A} 6\n\nE[f (X)]\n.\ninf{f (x) : x \u2208 A}\n\nApplication of (23) with f (x) = xq and the random variable X = DH\nS DS \u2212 I\n\u0001q\n\u03b4e\u22121/4\nE[X q ]\n6\n= e\u2212q/4 ,\nP{X > \u03b4} 6\nq\nq\ninf{x : x > \u03b4}\n\u03b4\n\n(23)\n2,2\n\ngives\n(24)\n\nprovided that (\u03b4e\u22121/4 )q > E[X q ]. But this is guaranteed by the assumptions in (4), (5), or (10),\ndepending on the signal and interference model. Therefore, we have\nn\no\nP kHk2,2 > \u03b4 6 e\u2212\u03b2 ,\nsince q = 4\u03b2. The second part of the theorem, (17), is a result of the fact that \u03c3min (DX ,E ) = 0\nn\no\nimplies that kHk2,2 > 1 and hence, P{\u03c3min (DX ,E ) = 0} 6 P kHk2,2 > 1 .\nA PPENDIX B\nB OTH S UPPORTS K NOWN\nProof of Theorem 1: It suffices to show that DX ,E is invertible, which is equivalent to the\ncondition that \u03c3min (DX ,E ) > 0. By assumption, the conditions of Theorem 9 hold, which implies\nP{\u03c3min (DX ,E ) = 0} 6 e\u2212\u03b2 . Hence, recovery of x and e using (3) succeeds with probability at\nleast 1 \u2212 e\u2212\u03b2 .\nA PPENDIX C\n(P0) WITH L IMITED S UPPORT K NOWLEDGE\nWe now prove the recovery guarantees for (P0? ), (P0? , E), and (P0, X ) for partial (or no)\n\nsupport-set knowledge of E and X . We follow the proof of [23] and present the three cases\n1) X known, 2) E known, and 3) no support-set knowledge, all together, since the corresponding\n\nproofs are similar. Note that R(D) denotes the space spanned by the columns of D.\nDRAFT\n\nNovember 21, 2018\n\n\f25\n\nWe begin by generalizing [23, Thm. 13] to the case of pairs of dictionaries A and B where\nwe know the support set of e. The result gives us a sufficient condition for when there is a\nunique minimizer of (P0? ), (P0? , E), or (P0, X ).\n\nLemma 10 (Based on Thm. 13 of [23]): Let \u00c3 \u2208 Cm\u00d7na and B\u0303 \u2208 Cm\u00d7nb be two dictionaries\n\nand suppose that we observe the signal z = \u00c3x + B\u0303e where X = supp(x) and E = supp(e) and\n\nthe non-zero entries of x and e are drawn from a continuous distribution. Furthermore, suppose\nthat E is known. Write D\u0303 = [ \u00c3 B\u0303 ] and D\u0303X ,E = [ \u00c3X B\u0303E ]. If\n\u0010 \u0010\n\u0011\n\u0010\n\u0011\u0011\ndim R D\u0303X ,E \u2229 R D\u0303X 0 ,E\n< |X | + |E| ,\n\n(25)\n\nfor all sets X 0 6= X where |X | = |X 0 |, then, almost surely, (P0? , E) recovers the vectors x and e.\nThis result also provides a sufficient condition for (P0? ), if we set \u00c3 = D and take B\u0303 to be the\nempty matrix, or for (P0, X ), if we set \u00c3 = B and B\u0303 = A.\n\nProof: We follow the proof of [23, Thm. 13]. We begin by defining the set of all alternative\n\nrepresentations as follows:\n\nDXE ,X 0 =\n\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\n0\n\n0\n\n\u00c3x + B\u0303e = \u00c3x + B\u0303e\n\n(x, e) : supp(x) = X , supp(x0 ) = X 0\nsupp(e) = supp(e0 ) = E\n\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fd\n\n,\n\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\n\nand the set of observations that have alternative representations\nn\no\nAEX ,X 0 = z : z = \u00c3X xX + B\u0303E eE , (x, e) \u2208 DXE ,X 0 ,\nso that AEX ,X 0 is the set of observations that can be written in terms of two pairs of signals (x, e)\nand (x0 , e0 ) where X = supp(x), X 0 = supp(x0 ), and E = supp(e) = supp(e0 ).\nFor any X 0 of size |X | and X 0 6= X , we have\n\u0010\n\u0011\n\u0010\n\u0011\nAEX ,X 0 \u2286 R D\u0303X ,E \u2229 R D\u0303X 0 ,E .\n\nNow assume that (25) holds for X , X 0 , and E, then dim(AEX ,X 0 ) < |X | + |E|. Thus the smallest\n\nsubspace containing AEX ,X 0 is a strict subspace of R(D\u0303X ,E ) and hence, has zero measure with\n\nrespect to any nonatomic measure defined in the range of D\u0303X ,E . Since x and e, and hence z,\nhave non-zero entries drawn from a continuous distribution\nn\no\nP \u00c3x + B\u0303e = z \u2208 AEX ,X 0 = 0.\nNovember 21, 2018\n\nDRAFT\n\n\f26\n\nThus, with probability zero, there exists no alternative pair (x0 , e0 ) with supports X 0 and E,\n\nrespectively, otherwise z would lie in AEX ,X 0 . Therefore, if (25) holds for all X 0 , then the\n\nprobability of choosing random x and e so that z admits an alternative representation is zero,\nand hence, almost surely, given z = \u00c3x + B\u0303e, (P0? , E) returns the vectors x and e.\n\nWe can use Lemma 10 to prove the first part of Theorems 2, 3, 4, 5, and 6 by showing\n\nthat (25) holds with high probability. To show that (25) holds for all X 0 we show that for every\n\ncolumn \u00e3\u03b3 of \u00c3 not in \u00c3X (i.e., for all \u03b3 \u2208\n/ X ) that \u00e3\u03b3 \u2208\n/ R(D\u0303X ,E ), which is equivalent to\nshowing that\n\nP\u0303X ,E \u00e3\u03b3\n\n2\n\n(26)\n\n< k\u00e3\u03b3 k2 = 1,\n\nfor all \u03b3 \u2208\n/ X and where P\u0303X ,E = (D\u0303\u2020X ,E )H D\u0303H\nX ,E is the projection onto the range space of D\u0303X ,E .\nWe will now bound the probability that (26) holds for the following three situations: 1) only E\nknown, 2) only X known, and 3) both support sets unknown.\n\n1) Only E known: Consider the setting where E is known, but X is unknown; this case fits\n\nthe setting of Lemma 10 with \u00c3 = A and B\u0303 = B. Hence, the condition (26) is equivalent to\nkPX ,E a\u03b3 k2 < ka\u03b3 k2 = 1. We have\nkPX ,E a\u03b3 k2 6 (D\u2020X ,E )H\n\n2,2\n\nDH\nX ,E a\u03b3\n\n2\n\nq\n2\n2\n\u22121\nH\n6 \u03c3min (DX ,E ) kAH\nX a\u03b3 k2 + kBE a\u03b3 k2 .\nFrom the definitions of the coherence parameters10\nDH\nX ,E a\u03b3\n\n2\n\n6 \u03beE =\n\np\n\n(27)\n\n\u03bc2a nx + \u03bc2m ne .\n\nThus, in order to guarantee kPX ,E a\u03b3 k2 < 1 it suffices to have\n(28)\n\n\u03beE < \u03c3min (DX ,E ).\n\n10\n\nNote that we use bounds that hold for all X , rather than a bound that holds with high probability. The underlying reason\n\nis the fact that if A is an equiangular tight frame, the associated inequalities hold with equality and hence, we cannot do any\nbetter by using probabilistic bounds, unless we take advantage of a property of A other than the coherence \u03bca .\nDRAFT\n\nNovember 21, 2018\n\n\f27\n\n2) Only X known: For the setting where only X is known, we apply Lemma 10 with \u00c3 = B\n\nand B\u0303 = A, thus the condition of (25) becomes dim(R(DX ,E ) \u2229 R(DX ,E 0 )) < |X | + |E|, and so\nwe only want to show that kPX ,E b\u03b3 k2 < kb\u03b3 k2 for all \u03b3 \u2208\n/ E. Proceeding as before, it follows\nthat\n\n\u22121\nkPX ,E b\u03b3 k2 6 \u03c3min\n(DX ,E ) DH\nX ,E b\u03b3\n\n2\n\n\u22121\n6 \u03c3min\n(DX ,E ) \u03beX ,\n\nwhere \u03beX =\n\n(29)\n\np\n\u03bc2m nx + \u03bc2b ne . Hence, it suffices to show that\n\u03beX < \u03c3min (DX ,E ).\n\n(30)\n\n3) No support-set knowledge: Finally, we consider the setting where neither X nor E is\n\nknown, so we apply Lemma 10 with \u00c3 = [ A B ] and B\u0303 being the empty matrix, thus this\nis exactly the condition of [23, Thm. 13]. Then, we show that kPX ,E d\u03b3 k2 < kd\u03b3 k2 for any\n\ncolumn d\u03b3 of D not in DX ,E . In other words, we want both (27) and (29) to hold as d\u03b3 can be\na column of either A or B. So it suffices to show\n\u22121\nkPX ,E d\u03b3 k2 6 \u03c3min\n(DX ,E ) \u03be+ < 1,\n\n(31)\n\nwhere \u03be+ = max{\u03beX , \u03beE }.\n\nFinally, to show that the (P0) based problems succeed, we want to bound the probability that\n\n(28), (30), or (31) holds (depending on which, if any, support sets we know). In each of the\ncases, we know that (P0? ), (P0? , E), or (P0, X ) returns the correct solution if \u03be < \u03c3min (DX ,E ),\n\nwhere \u03be \u2208 (0, 1) is equal to \u03beE , \u03beX , or \u03be+ (as appropriate to the case). Hence, we can bound the\nprobability of error as follows\n\nP{error} 6 P{\u03be > \u03c3min (DX ,E )}\nn\no\n2\n6 P DH\nD\n\u2212\nI\n>\n1\n\u2212\n\u03be\n6 e\u2212\u03b2 ,\nX ,E X ,E\n2,2\nwhere we use Theorem 9 with \u03b4 = 1 \u2212 \u03be 2 . Therefore, with probability exceeding 1 \u2212 e\u2212\u03b2 , the\n\npair (x, e) is the unique minimizer.\nNovember 21, 2018\n\nDRAFT\n\n\f28\n\nA PPENDIX D\n(BP) WITH L IMITED S UPPORT K NOWLEDGE\nWe now prove the recovery results for the (BP) based algorithms. To do this, we restate the\nsufficient recovery condition of [41] and then show when we can satisfy this condition, thereby\nguaranteeing the successful recovery of x with (BP? , E), (BP, X ), or (BP? ).\n\nTheorem 11 (Thm. 5 of [41]): Suppose that the sparsest representation of a complex vector\n\nz is D\u0303S sS . If DX ,E is full rank and there exists a vector h \u2208 Cm such that\nD\u0303H\nS h = sign(sS ), and\n\n(32a)\n\n|hh, d\u0303\u03b3 i| < 1 for all columns d\u0303\u03b3 of D\u0303 not in D\u0303S ,\n\n(32b)\n\nthen s is the unique minimizer of (BP).\nWe can easily apply Theorem 11 to attain recovery conditions for (BP? , E), (BP, X ), and\n\n(BP? ). For (BP? , E), we apply Theorem 11 to the matrix D\u0303 = [ A BE ] so that the two\nproblems (BP) and (BP? , E) are the same. We want to show that sTS = [ xTX eTE ] is the sparsest\n\nrepresentation of the observation z. By rewriting (32a) and (32b) it follows that it is sufficient\nto guarantee recovery with (BP? , E) if there exists a vector h \u2208 Cm such that\n\uf8eb\uf8ee \uf8f9\uf8f6\nxX\n[ AX BE ]H h = sign\uf8ed\uf8f0 \uf8fb\uf8f8 , and\neE\n|hh, a\u03b3 i| < 1 for all columns a\u03b3 of A not in AX .\n\n(33a)\n(33b)\n\nSimilarly, to get a recovery condition for (BP? ), we merely apply Theorem 11 to the matrix\nD\u0303 = [ A B ].\nFinally, before we can prove the probabilistic recovery guarantees for the `1 -norm-based\nalgorithms of Theorems 2, 3, 4, 5, and 6, we require the following lemma.\nLemma 12 (Bernstein's Inequality, Prop. 16 of [23]): Let v \u2208 Cn and let \u03b5 \u2208 Cn be a\n\nSteinhaus sequence. Then, for u > 0 we have\n( n\n)\n\u0012 2\u0013\nX\nu\n\u03b5i vi > u kvk2 6 2 exp \u2212\nP\n.\n2\ni=1\n\n(34)\n\nA Steinhaus sequence is a (countable) collection of independent complex-valued random\nvariables, whose entries are uniformly distributed on the unit circle [23].\nDRAFT\n\nNovember 21, 2018\n\n\f29\n\nWe now prove the second part of Theorems 2, 3, 4, 5, and 6. To show that recovery with\n(BP? ), (BP? , E), or (BP, X ) succeeds, we demonstrate that the vector h, as in Theorem 11,\n\nexists with high probability. We now consider the following three settings in turn: 1) only E\n\nknown, 2) only X known, and 3) both support sets unknown. But first, let us assume that in\neach case DX ,E is full rank.\n\n1) Only E known: Consider the case where E is known but X is unknown, we show that\n\na vector h exists that satisfies (33a) and (33b) with high probability. To this end, set h =\n\u0001\u22121\nDX ,E DH\nD\nsign(sX ,E ), so that (33a) is satisfied. Then, for any column a\u03b3 of A where\nX\n,E\nX ,E\n\u03b3\u2208\n/ X,\n\n|hh, a\u03b3 i| =\n\nD\n\n=\n\nD\n\nDX ,E\n\n\u0001\u22121\nDH\nX ,E DX ,E\n\nsign(sX ,E ),\n\nsign(sX ,E ), a\u03b3\n\n\u0001\u22121\nDH\nX ,E DX ,E\n\nwith \u03b5 = sign(sX ,E ) and v\u03b3 = DH\nX ,E DX ,E\n\n\u0001\u22121\n\nDH\nX ,E a\u03b3\n\nE\n\nE\n\n=\n\nnx\nX\n\n\u03b5j vj\u03b3 ,\n\nj=1\n\nDH\nX ,E a\u03b3 . Since \u03b5 is a Steinhaus sequence (by\n\nassumption), we can apply Lemma 12 with u = kv\u03b3 k\u22121\n2 to arrive at\n!\n( n\n)\nx\nX\n1\n.\nP\n\u03b5j vj\u03b3 > 1 6 2 exp \u2212\n\u03b3 k2\n2\nkv\n2\nj=1\n\n(35)\n\nBut we have that\nkv\u03b3 k22 =\n6\n\nDH\nX ,E DX ,E\n\n\u0001\u22121\n\nDH\nX ,E DX ,E\n\n\u0001\u22121\n\nDH\nX ,E a\u03b3\n2\n2,2\n\n2\n2\n\nDH\nX ,E a\u03b3\n\n2\n2\n\n\u22124\n6 \u03c3min\n(DX ,E ) \u03beE2 ,\n\nwhere \u03beE2 = nx \u03bc2a + ne \u03bc2m . Hence, (35) results in\n( n\n)\n\u0013\n\u0012 4\nx\nX\n\u03c3min (DX ,E )\n\u03b3\n.\nP\n\u03b5j vj > 1 6 2 exp \u2212\n2\n2\u03be\nE\nj=1\nNow we want (32b) to hold for all \u03b3 \u2208\n/ X . Hence, applying the union bound to the result above\n\nleads to\n\n(\nP max\n\u03b3 \u2208X\n/\n\nNovember 21, 2018\n\nnx\nX\nj=1\n\n)\n\u03b5j vj\u03b3\n\n>1\n\n\u0012 4\n\u0013\n\u03c3min (DX ,E )\n6 2na exp \u2212\n.\n2\u03beE2\n\n(36)\n\nDRAFT\n\n\f30\n\n2) Only X known: Consider the setting where X is known, but E is unknown. This setting\n\nfollows exactly as in the setting where E is known and X is unknown by switching the roles of\nX and E. Thus, we arrive at\n(\n\nP max\n\u03b3 \u2208E\n/\n\nne\nX\n\n)\n\u03b5j vj\u03b3\n\n>1\n\nj=1\n\n\u0013\n\u0012 4\n\u03c3min (DX ,E )\n,\n6 2nb exp \u2212\n2\u03beX2\n\n(37)\n\nwhere \u03beX2 = nx \u03bc2m + ne \u03bc2b and v\u03b3 = D\u2020X ,E b\u03b3 .\n\n3) No support-set knowledge: Finally, we consider the third setting where neither X nor E\n\nare known. In particular, we want to show that in Theorem 11, we can satisfy (32a) and (32b)\nwith high probability. For any column d\u03b3 of D not in DX ,E , set v\u03b3 = D\u2020X ,E d\u03b3 . In this case, we\nhave\n\nkv\u03b3 k22 6\n\nDH\nX ,E DX ,E\n\n\u0001\u22121\n\n2\n2,2\n\nDH\nX ,E d\u03b3\n\n2\n2\n\n\u22124\n2\n6 \u03c3min\n(DX ,E ) \u03be+\n,\n2\n= max{nx \u03bc2a + ne \u03bc2m , nx \u03bc2m + ne \u03bc2b } and hence,\nwhere \u03be+\n( n +n\n)\n\u0012 4\n\u0013\nx\ne\nX\n\u03c3min (DX ,E )\n\u03b3\nP\n\u03b5j vj > 1 6 2 exp \u2212\n.\n2\n2\u03be\n+\nj=1\n\nFinally, we want (32b) to hold for all d\u03b3 . Therefore, applying the union bound to the result\nabove leads to\n(\nP\n\nmax\n\n\u03b3 \u2208X\n/ \u222aE\n\nnX\nx +ne\n\n)\n\u03b5j vj\u03b3\n\nj=1\n\n>1\n\n\u0013\n\u0012 4\n\u03c3min (DX ,E )\n.\n6 2(na + nb ) exp \u2212\n2\n2\u03be+\n\n(38)\n\nWe now want to derive an upper bound on the right hand sides of (36), (37), and (38). First\nwe calculate the probability conditioned on \u03c3min (DX ,E ) > \u03bb \u2208 (0, 1). Note that if \u03bb > 0, then\n\u03c3min (DX ,E ) > \u03bb > 0 and we satisfy the remaining assumption of Theorem 11, namely that DX ,E\n\nis full rank.\nFor convenience, in the case where E is known, let us set N = na and \u03be = \u03beE . In the case\n\nwhere X is known, set N = nb and \u03be = \u03beX and finally, in the case where neither X nor E are\nknown, set N = na + nb and \u03be = \u03be+ .\nThus, we have\n(\nP max\n\u03b3 \u2208S\n/\n\nDRAFT\n\nN\nX\nj=1\n\n)\n\u03b5j vj\u03b3\n\n> 1 \u03c3min (DX ,E ) > \u03bb\n\n\u0012\n\u0013\n\u03bb4\n6 2N exp \u2212 2 6 2e\u2212\u03b2 ,\n2\u03be\n\n(39)\n\nNovember 21, 2018\n\n\f31\n\nfor some \u03b2 6 \u03bb4 /(2\u03be 2 ) \u2212 log N .\n\nFor our particular choice of h, (33a) (in the case where X or E is known) or (32a) (in the case\n\nwhere both supports are unknown) will always be satisfied. So let E be the event that (33b) (in\nthe case where one support is unknown) or (32b) (in the case where both supports are known) is\nnot fulfilled with our choice of h and let R be the event that DX ,E is not full rank. As E \u222a R is a\n\nnecessary condition for the (BP) based algorithms not to be able to recover the vectors x and e,\nP{E \u222a R} is an upper bound on the probability of error. Then, since \u03c3min (DX ,E ) > \u03bb > 0 implies\n\b\n\b\nthat R cannot occur, and hence that P E \u222a R \u03c3min (DX ,E ) > \u03bb = P E \u03c3min (DX ,E ) > \u03bb , we\nhave that for any \u03bb > 0\n\n\b\nP{E \u222a R} = P E \u222a R \u03c3min (DX ,E ) > \u03bb P{\u03c3min (DX ,E ) > \u03bb}\n\b\n+ P E \u222a R \u03c3min (DX ,E ) 6 \u03bb P{\u03c3min (DX ,E ) 6 \u03bb}\n\b\n6 P E \u03c3min (DX ,E ) > \u03bb + P{\u03c3min (DX ,E ) 6 \u03bb} .\n\n(40)\n\nWe can bound the first summand in (40) using (39) under the assumption that \u03b2 6 \u03bb4 /(2\u03be 2 ) \u2212\n\nlog N . The second term we can bound using Theorem 9 with \u03b4 = 1 \u2212 \u03bb2 \u2208 (0, 1), which,\nprovided that \u03b2 > N 0 where N 0 is the size of the supports chosen at random, says that\nP{\u03c3min (DX ,E ) 6 \u03bb} 6 e\u2212\u03b2 . Therefore, we have\nP{E \u222a R} 6 3e\u2212\u03b2 ,\n\n(41)\n\nand hence, we can recover x and e with probability at least 1 \u2212 3e\u2212\u03b2 .\nR EFERENCES\n[1] A. Bracher, G. Pope, and C. Studer, \"Coherence-based probabilistic recovery guarantees for sparsely corrupted signals,\"\nin Proc. of IEEE Inf. Th. Workshop, Lausanne, Switzerland, Sep. 2012.\n[2] C. Studer, P. Kuppinger, G. Pope, and H. B\u00f6lcskei, \"Recovery of sparsely corrupted signals,\" IEEE Trans. Inf. Theory,\nvol. 58, no. 5, pp. 3115\u20133130, May 2012.\n[3] J. N. Laska, P. T. Boufounos, M. A. Davenport, and R. G. Baraniuk, \"Democracy in action: Quantization, saturation, and\ncompressive sensing,\" App. Comp. Harm. Anal., vol. 31, no. 3, pp. 429\u2013443, Nov. 2011.\n[4] A. Adler, V. Emiya, M. G. Jafari, M. Elad, R. Gribonval, and M. D. Plumbley, \"A constrained matching pursuit approach\nto audio declipping,\" in Proc. of IEEE Int. Conf. Acoustics, Speech, and Sig. Proc., Prague, Czech Republic, May 2011,\npp. 329\u2013332.\n[5] --, \"Audio inpainting,\" IEEE Trans. on Audio, Speech, and Language Processing, vol. 20, no. 3, pp. 922\u2013932, Mar.\n2012.\nNovember 21, 2018\n\nDRAFT\n\n\f32\n\n[6] S. V. Vaseghi and R. Frayling-Cork, \"Restoration of old gramophone recordings,\" Journal of Audio Engineering, vol. 40,\npp. 791\u2013801, Oct. 1992.\n[7] S. J. Godsill and P. J. W. Rayner, Digital audio restoration: a statistical model based approach.\n\nBerlin, Germany:\n\nSpringer-Verlag, 1998.\n[8] C. Novak, C. Studer, A. Burg, and G. Matz, \"The effect of unreliable LLR storage on the performance of MIMO-BICM,\"\nin Proc. of 44th Asilomar Conf. on Signals, Systems, and Comput., Pacific Grove, CA, USA, Nov. 2010, pp. 736\u2013740.\n[9] M. Elad and Y. Hel-Or, \"A fast super-resolution reconstruction algorithm for pure translational motion and common\nspace-invariant blur,\" IEEE Trans. Image Process, vol. 10, no. 8, pp. 1187\u20131193, Aug. 2001.\n[10] S. G. Mallat and G. Yu, \"Super-resolution with sparse mixing estimators,\" IEEE Trans. Image Process., vol. 19, no. 11,\npp. 2889\u20132900, Nov. 2010.\n[11] M. Elad, J.-L. Starck, P. Querre, and D. L. Donoho, \"Simultaneous cartoon and texture image inpainting using morphological\ncomponent analysis (MCA),\" App. Comp. Harm. Anal., vol. 19, pp. 340\u2013358, Dec. 2005.\n[12] J.-F. Cai, S. Osher, and Z. Shen, \"Split bregman methods and frame based image restoration,\" Multiscale Model. Simul,\nvol. 8, no. 2, pp. 337\u2013369, Dec. 2009.\n[13] C. Studer and R. G. Baraniuk, \"Stable restoration and separation of approximately sparse signals,\" submitted,\narXiv:1107.0420v1, July 2011.\n[14] P. Kuppinger, G. Durisi, and H. B\u00f6lcskei, \"Uncertainty relations and sparse signal recovery for pairs of general signal\nsets,\" IEEE Trans. Inf. Theory, vol. 58, no. 1, pp. 263\u2013277, Jan. 2012.\n[15] J. Wright and Y. Ma, \"Dense error correction via `1 -minimization,\" IEEE Trans. Inf. Theory, vol. 56, no. 7, pp. 3540\u20133560,\nJuly 2010.\n[16] X. Li, \"Compressed sensing and matrix completion with constant proportion of corruptions,\" arXiv:1104.1041v2, Jan.\n2012.\n[17] M. B. McCoy and J. A. Tropp, \"Sharp recovery bounds for convex deconvolution, with applications,\" arXiv:1205.1580v1,\nMay 2012.\n[18] J. N. Laska, M. A. Davenport, and R. G. Baraniuk, \"Exact signal recovery from sparsely corrupted measurements through\nthe pursuit of justice,\" Proc. of 43rd Asilomar Conf. on Signals, Systems, and Comput., pp. 1556\u20131560, Nov. 2009.\n[19] N. Vaswani and W. Lu, \"Modified-CS: Modifying compressive sensing for problems with partially known support,\" IEEE\nTrans. Sig. Proc., vol. 58, no. 9, pp. 4595\u20134607, Sep. 2010.\n[20] L. Jacques, \"A short note on compressed sensing with partially known signal support,\" Signal Processing, vol. 90, no. 12,\npp. 3308\u20133312, Dec. 2010.\n[21] N. Vaswani, \"LS-CS-residual (LS-CS): compressive sensing on least squares residual,\" IEEE Trans. Sig. Proc., vol. 58,\nno. 8, pp. 4108\u20134120, Aug. 2010.\n[22] C. Qiu and N. Vaswani, \"Recursive sparse recovery in large but correlated noise,\" in 49th Allerton Conf. on Communication,\nControl, and Computing, Monticello, IL, USA, Sep. 2011, pp. 752\u2013759.\n[23] J. A. Tropp, \"On the conditioning of random subdictionaries,\" App. Comp. Harm. Anal., vol. 25, pp. 1\u201324, July 2008.\n[24] D. L. Donoho and M. Elad, \"Optimally sparse representation in general (nonorthogonal) dictionaries via `1 minimization,\"\nProc. of Natl. Acad. Sci, vol. 100, no. 5, pp. 2197\u20132202, Dec. 2003.\n[25] J. A. Tropp, \"Greed is good: Algorithmic results for sparse approximation,\" IEEE Trans. Inf. Theory, vol. 50, no. 10, pp.\n2231\u20132242, Oct. 2004.\nDRAFT\n\nNovember 21, 2018\n\n\f33\n\n[26] S. S. Chen, D. L. Donoho, and M. A. Saunders, \"Atomic decomposition by basis pursuit,\" SIAM J. Sci. Comput., vol. 20,\nno. 1, pp. 33\u201361, 1998.\n[27] E. J. Cand\u00e8s and J. Romberg, \"Sparsity and incoherence in compressive sampling,\" Inverse Problems, vol. 23, no. 3, pp.\n969\u2013985, 2007.\n[28] E. J. Cand\u00e8s and Y. Plan, \"A probabilistic and RIPless theory of compressed sensing,\" IEEE Trans. Inf. Theory, vol. 57,\nno. 11, pp. 7235\u20137254., Nov. 2010.\n[29] E. J. Cand\u00e8s, J. Romberg, and T. Tao, \"Stable signal recovery from incomplete and inaccurate measurements,\" Comm.\nPure and Appl. Math., vol. 59, no. 8, pp. 1207\u20131223, Aug. 2006.\n[30] E. J. Cand\u00e8s, \"The restricted isometry property and its implications for compressed sensing,\" C. R. Acad. Sci. Paris, vol.\n346, no. 9-10, pp. 589\u2013592, 2008.\n[31] T. Cai, L. Wany, and G. Xu, \"Stable recovery of sparse signals and an oracle inequality,\" IEEE Trans. Inf. Theory, vol. 56,\nno. 7, pp. 3516\u20133522, July 2010.\n[32] L. R. Welch, \"Lower bounds on the maximum cross correlation of signals,\" IEEE Trans. Inf. Theory, vol. 20, no. 3, pp.\n397\u2013399, May 1974.\n[33] M. E. Pfetsch and A. M. Tillmann, \"The computational complexity of the restricted isometry property, the nullspace\nproperty, and related concepts in compressed sensing,\" arXiv:1205.2081v2, May 2012.\n[34] E. J. Cand\u00e8s and T. Tao, \"Decoding by linear programming,\" IEEE Trans. Inf. Theory, vol. 51, no. 12, pp. 4203\u20134215,\nDec. 2005.\n[35] D. L. Donoho, \"Compressed sensing,\" IEEE Trans. Inf. Theory, vol. 52, no. 4, pp. 1289\u20131306, Apr. 2006.\n[36] A. R. Calderbank, P. J. Cameron, W. M. Kantor, and J. J. Seidel, \"Z4 -kerdock codes, orthogonal spreads, and extremal\nEuclidean line-sets,\" Proc. London Math. Soc, vol. 75, no. 2, pp. 436\u2013480, Sep. 1997.\n[37] R. Gribonval and M. Nielsen, \"Sparse representations in unions of bases,\" IEEE Trans. Inf. Theory, vol. 49, no. 12, pp.\n3320\u20133325, Dec. 2003.\n[38] R. G. Baraniuk, M. A. Davenport, R. DeVore, and M. B. Wakin, \"A simple proof of the restricted isometry property for\nrandom matrices,\" Constructive Approximation, vol. 28, no. 3, pp. 253\u2013263, Dec. 2008.\n[39] R. Horn and C. Johnson, Matrix analysis.\n\nNew York, NY, USA: Cambridge Univ. Press, 1990.\n\n[40] R. Durrett, Probability: Theory and Examples.\n\nNew York, NY, USA: Cambridge Univ. Press, 2010.\n\n[41] J. A. Tropp, \"Recovery of short, complex linear combinations via `1 minimization,\" IEEE Trans. Inf. Theory, vol. 51,\nno. 4, pp. 1568\u20131570, Apr. 2005.\n\nNovember 21, 2018\n\nDRAFT\n\n\f"}