{"id": "http://arxiv.org/abs/1111.4422v3", "guidislink": true, "updated": "2018-06-15T20:35:52Z", "updated_parsed": [2018, 6, 15, 20, 35, 52, 4, 166, 0], "published": "2011-11-18T16:49:57Z", "published_parsed": [2011, 11, 18, 16, 49, 57, 4, 322, 0], "title": "On the stability and accuracy of least squares approximations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1111.0180%2C1111.2955%2C1111.5091%2C1111.2057%2C1111.6446%2C1111.1301%2C1111.5592%2C1111.6787%2C1111.0433%2C1111.3876%2C1111.1630%2C1111.6907%2C1111.0867%2C1111.2914%2C1111.2947%2C1111.2420%2C1111.1777%2C1111.0376%2C1111.0028%2C1111.1074%2C1111.2704%2C1111.3735%2C1111.6376%2C1111.5211%2C1111.2089%2C1111.1487%2C1111.1069%2C1111.1952%2C1111.0854%2C1111.3664%2C1111.2567%2C1111.4736%2C1111.6637%2C1111.1623%2C1111.5146%2C1111.1706%2C1111.0193%2C1111.4649%2C1111.5243%2C1111.5722%2C1111.0798%2C1111.5862%2C1111.2350%2C1111.3103%2C1111.7311%2C1111.2728%2C1111.5668%2C1111.4487%2C1111.0490%2C1111.6916%2C1111.4245%2C1111.1954%2C1111.1931%2C1111.6629%2C1111.1258%2C1111.2738%2C1111.1906%2C1111.3493%2C1111.4789%2C1111.6661%2C1111.6690%2C1111.2315%2C1111.1522%2C1111.5415%2C1111.4823%2C1111.4707%2C1111.5549%2C1111.1750%2C1111.0330%2C1111.3358%2C1111.0653%2C1111.1835%2C1111.6778%2C1111.4518%2C1111.6600%2C1111.4422%2C1111.7133%2C1111.4206%2C1111.6626%2C1111.0648%2C1111.6497%2C1111.1960%2C1111.0105%2C1111.3108%2C1111.2192%2C1111.5207%2C1111.6185%2C1111.7172%2C1111.3768%2C1111.0522%2C1111.5065%2C1111.5461%2C1111.1160%2C1111.3795%2C1111.5943%2C1111.6522%2C1111.6216%2C1111.3615%2C1111.0082%2C1111.3665%2C1111.5304&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On the stability and accuracy of least squares approximations"}, "summary": "We consider the problem of reconstructing an unknown function $f$ on a domain\n$X$ from samples of $f$ at $n$ randomly chosen points with respect to a given\nmeasure $\\rho_X$. Given a sequence of linear spaces $(V_m)_{m>0}$ with ${\\rm\ndim}(V_m)=m\\leq n$, we study the least squares approximations from the spaces\n$V_m$. It is well known that such approximations can be inaccurate when $m$ is\ntoo close to $n$, even when the samples are noiseless. Our main result provides\na criterion on $m$ that describes the needed amount of regularization to ensure\nthat the least squares method is stable and that its accuracy, measured in\n$L^2(X,\\rho_X)$, is comparable to the best approximation error of $f$ by\nelements from $V_m$. We illustrate this criterion for various approximation\nschemes, such as trigonometric polynomials, with $\\rho_X$ being the uniform\nmeasure, and algebraic polynomials, with $\\rho_X$ being either the uniform or\nChebyshev measure. For such examples we also prove similar stability results\nusing deterministic samples that are equispaced with respect to these measures.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1111.0180%2C1111.2955%2C1111.5091%2C1111.2057%2C1111.6446%2C1111.1301%2C1111.5592%2C1111.6787%2C1111.0433%2C1111.3876%2C1111.1630%2C1111.6907%2C1111.0867%2C1111.2914%2C1111.2947%2C1111.2420%2C1111.1777%2C1111.0376%2C1111.0028%2C1111.1074%2C1111.2704%2C1111.3735%2C1111.6376%2C1111.5211%2C1111.2089%2C1111.1487%2C1111.1069%2C1111.1952%2C1111.0854%2C1111.3664%2C1111.2567%2C1111.4736%2C1111.6637%2C1111.1623%2C1111.5146%2C1111.1706%2C1111.0193%2C1111.4649%2C1111.5243%2C1111.5722%2C1111.0798%2C1111.5862%2C1111.2350%2C1111.3103%2C1111.7311%2C1111.2728%2C1111.5668%2C1111.4487%2C1111.0490%2C1111.6916%2C1111.4245%2C1111.1954%2C1111.1931%2C1111.6629%2C1111.1258%2C1111.2738%2C1111.1906%2C1111.3493%2C1111.4789%2C1111.6661%2C1111.6690%2C1111.2315%2C1111.1522%2C1111.5415%2C1111.4823%2C1111.4707%2C1111.5549%2C1111.1750%2C1111.0330%2C1111.3358%2C1111.0653%2C1111.1835%2C1111.6778%2C1111.4518%2C1111.6600%2C1111.4422%2C1111.7133%2C1111.4206%2C1111.6626%2C1111.0648%2C1111.6497%2C1111.1960%2C1111.0105%2C1111.3108%2C1111.2192%2C1111.5207%2C1111.6185%2C1111.7172%2C1111.3768%2C1111.0522%2C1111.5065%2C1111.5461%2C1111.1160%2C1111.3795%2C1111.5943%2C1111.6522%2C1111.6216%2C1111.3615%2C1111.0082%2C1111.3665%2C1111.5304&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider the problem of reconstructing an unknown function $f$ on a domain\n$X$ from samples of $f$ at $n$ randomly chosen points with respect to a given\nmeasure $\\rho_X$. Given a sequence of linear spaces $(V_m)_{m>0}$ with ${\\rm\ndim}(V_m)=m\\leq n$, we study the least squares approximations from the spaces\n$V_m$. It is well known that such approximations can be inaccurate when $m$ is\ntoo close to $n$, even when the samples are noiseless. Our main result provides\na criterion on $m$ that describes the needed amount of regularization to ensure\nthat the least squares method is stable and that its accuracy, measured in\n$L^2(X,\\rho_X)$, is comparable to the best approximation error of $f$ by\nelements from $V_m$. We illustrate this criterion for various approximation\nschemes, such as trigonometric polynomials, with $\\rho_X$ being the uniform\nmeasure, and algebraic polynomials, with $\\rho_X$ being either the uniform or\nChebyshev measure. For such examples we also prove similar stability results\nusing deterministic samples that are equispaced with respect to these measures."}, "authors": ["Albert Cohen", "Mark A. Davenport", "Dany Leviatan"], "author_detail": {"name": "Dany Leviatan"}, "author": "Dany Leviatan", "links": [{"href": "http://arxiv.org/abs/1111.4422v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1111.4422v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1111.4422v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1111.4422v3", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "On the stability and accuracy of least squares approximations\nAlbert Cohen, Mark A. Davenport and Dany Leviatan\u2217\n\narXiv:1111.4422v3 [math.NA] 15 Jun 2018\n\nJune 19, 2018\n\nAbstract\nWe consider the problem of reconstructing an unknown function f on a domain X from samples of\nf at n randomly chosen points with respect to a given measure \u03c1X . Given a sequence of linear spaces\n(Vm )m>0 with dim(Vm ) = m \u2264 n, we study the least squares approximations from the spaces Vm . It\nis well known that such approximations can be inaccurate when m is too close to n, even when the\nsamples are noiseless. Our main result provides a criterion on m that describes the needed amount\nof regularization to ensure that the least squares method is stable and that its accuracy, measured in\nL2 (X, \u03c1X ), is comparable to the best approximation error of f by elements from Vm . We illustrate\nthis criterion for various approximation schemes, such as trigonometric polynomials, with \u03c1X being the\nuniform measure, and algebraic polynomials, with \u03c1X being either the uniform or Chebyshev measure.\nFor such examples we also prove similar stability results using deterministic samples that are equispaced\nwith respect to these measures.\n\n1\n\nIntroduction and main results\n\nLet X be a domain of Rd and \u03c1X be a probability measure on X. We consider the problem of estimating\nan unknown function f : X \u2192 R from samples (yi )i=1,...,n which are either noiseless or noisy observations of\nf at the points (xi )i=1,...,n , where the xi are i.i.d. with respect to \u03c1X . We measure the error between f and\nits estimator f \u0303 in the L2 (X, \u03c1X ) norm\n\u00111/2\n\u0010Z\n|v(x)|2 d\u03c1X (x)\n,\nkvk :=\nX\n\nand we denote by h*, *i the associated inner product.\nGiven a fixed sequence of finite dimensional spaces (Vm )m\u22651 of L2 (X, \u03c1X ) such that dim(Vm ) = m.\nWe would like to compute the best approximation of f in Vm . This is given by the L2 (X, \u03c1X ) orthogonal\nprojector onto Vm , which we denote by Pm :\nPm f := argmin kf \u2212 vk.\nv\u2208Vm\n\nWe let\nem (f ) = kf \u2212 Pm f k\ndenote the best approximation error.\n\u2217 This research has been partially supported by the ANR Defi08 \"ECHANGE\" and by the US NSF grant DMS-1004718.\nPortions of this work were completed while M.A.D. and D.L. were visitors at University Pierre et Marie Curie\n\n1\n\n\fIn general, we may not have access to either \u03c1X or any information about f aside from the observations\nat the points (xi )i=1,...,n . In this case we cannot explicitly compute Pm f . A natural approach in this setting\nis to consider the solution of the least squares problem\nw = argmin\nv\u2208Vm\n\nn\nX\n\n|yi \u2212 v(xi )|2 .\n\ni=1\n\nTypically, we are interested in the case where m \u2264 n which is the regime where this problem may admit a\nunique solution.\nIn the noiseless case yi = f (xi ), and hence w may be viewed as the application of the least squares\nprojection operator onto Vm to f , i.e., we can write\nn\nw = Pm\nf := argmin kf \u2212 vkn\nv\u2208Vm\n\nwhere\nkvkn :=\n\nn\n\u00101 X\n\nn\n\n|v(xi )|2\n\n\u00111/2\n\ni=1\n\nis the L2 norm with respect to the empirical measure and, analogously, h*, *in the associated empirical inner\nproduct.\nIt is well known that least squares approximations may be inaccurate even when the measured samples\nare noiseless. For example, if Vm is the space Pm\u22121 of algebraic polynomials of degree m \u2212 1 over the\ninterval [\u22121, 1] and if we choose m = n, this corresponds to Lagrange interpolation, which is known to be\nhighly unstable, failing to converge towards f when given values at uniformly spaced samples, even when f\nis infinitely smooth (the \"Runge phenomenon\"). Regularization by taking m substantially smaller than n\nmay therefore be needed even in a noise-free context. The goal of this paper is to provide a mathematical\nanalysis on the exact needed amount of such regularization.\nStability of the least squares problem. The solution of the least squares problem can be computed by\nsolving an m \u00d7 m system: specifically, if (L1 , . . . , Lm ) is an arbitrary basis for Vm , then we can write\nw=\n\nm\nX\n\nuj Lj ,\n\nj=1\n\nwhere u = (uj )j=1,...,m is the solution of the m \u00d7 m system\nGu = f ,\n\n(1.1)\n\nPn\nwith G := (hLj , Lk in )j,k=1,...,m and f = ( n1 i=1 yi Lk (xi ))k=1,...,m . In the noiseless case yi = f (xi ), so that\nwe can also write f := (hf, Lk in )k=1,...,m . In the event that G is singular, we simply set w = 0.\nFor the purposes of our analysis, suppose that the basis (L1 , . . . , Lm ) is orthonormal in the sense of\nL2 (X, \u03c1X ).1 In this case we have\nE(G) = (hLj , Lk i)j,k=1,...,m = I.\n1 While\n\nsuch a basis is generally not accessible when \u03c1X is unknown, we require it only for the analysis. The actual\ncomputation of the estimator can be made using any known basis of Vm , since the solution w is independent of the basis used\nin computing it.\n\n2\n\n\fOur analysis requires an understanding of how the random matrix G deviates from its expectation I in\nprobability. Towards this end, we introduce the quantity\nm\nX\n\nK(m) := sup\n\n|Lj (x)|2 .\n\nx\u2208X j=1\n\nPm\nNote that the function j=1 |Lj (x)|2 is invariant with respect to a rotation applied to (L1 , . . . , Lm ) and\ntherefore independent of the choice of the orthonormal basis: it only depends on the space Vm and on the\nmeasure \u03c1X , and hence K(m) also depends only on Vm and \u03c1X . Also note that\nK(m) \u2265\n\nm\nX\n\nkLj k2 = m.\n\nj=1\n\nWe also will use the notation\n9M9 = max\nv6=0\n\n|Mv|\n,\n|v|\n\nfor the spectral norm of a matrix.\nOur first result is a probabilistic estimate of the comparability of the norms k * k and k * kn uniformly\nover the space Vm . This is equivalent to the proximity of the matrices G and I in spectral norm, since we\nhave that for all \u03b4 \u2208 [0, 1],\n9G \u2212 I9 \u2264 \u03b4 \u21d4 kvk2n \u2212 kvk2 \u2264 \u03b4kvk2 , v \u2208 Vm .\nTheorem 1 For 0 < \u03b4 < 1, one has the estimate\nPr {9G \u2212 I9 > \u03b4} = Pr {\u2203v \u2208 Vm :\n\nkvk2n\n\n2\n\n\u2212 kvk\n\n\u001a\n\nc\u03b4 n\n> \u03b4kvk } \u2264 2m exp \u2212\nK(m)\n2\n\n\u001b\n,\n\n(1.2)\n\nwhere c\u03b4 := (1 + \u03b4) log(1 + \u03b4) \u2212 \u03b4 > 0.\nThe proof of Theorem 1 is a simple application of tail bounds for sums of random matrices obtained in\n[1]. A consequence of this result is that the norms k * k and k * kn are comparable with high probability if\nK(m) is smaller than n by a logarithmic factor: for example taking \u03b4 = 12 , we find that for any r > 0,\n\u001a\n\u001b\n\u001a\n\u001b\n1\n1\nPr 9G \u2212 I9 >\n= Pr \u2203v \u2208 Vm : kvk2n \u2212 kvk2 > kvk2 \u2264 2n\u2212r ,\n(1.3)\n2\n2\nif m is such that\nK(m) \u2264 \u03ba\n\nc1/2\nn\n3 log(3/2) \u2212 1\n, with \u03ba :=\n=\n.\nlog n\n1+r\n2 + 2r\n\n(1.4)\n\nThe above condition thus ensures that G is well conditioned with high probability. It can also be thought\nof as ensuring that the least squares problem is stable with high probability. Indeed the right side of the\nleast squares system can be written as f = My with\nM=\n\n1\n(Lj (xi ))j,i\u2208{1,...,m}\u00d7{1,...,n} ,\nn\n\nan m \u00d7 n matrix. Observing that hGv, vi = n|MT v|2 , we find that\n9M9 = 9MT 9 =\n3\n\n\u00101\nn\n\n\u00111/2\n9 G9\n.\n\n\fTherefore, if 9G \u2212 I9 \u2264 12 , then we have that for any data vector y the solution w =\n\u22121\n\nkwk = |u| \u2264 9G\n\n1\n9 * 9 M 9 *|y| \u2264 \u221a 2\nn\n\nr\n\nPm\n\nj=1\n\nuj Lj satisfies\n\n3\n|y|,\n2\n\nwhich thus gives the stability estimate\nkwk \u2264 C\n\nn\n\u00101 X\n\nn\n\n|yi |2\n\n\u00111/2\n\n, C=\n\n\u221a\n\n6.\n\ni=1\n\nn\nIn the noiseless case, this can be written as kPm\nf k \u2264 Ckf kn , i.e., the least squares projection is stable\nbetween the norms k * kn and k * k. Note that since K(m) not only depends on Vm but also on the measure\n\u03c1X , the range of m such that the condition (1.4) holds is strongly tied to the choice of the measure. This\nissue is illustrated further in our numerical experiments.\nLet us mention that similar probabilistic bounds have been previously obtained, see in particular \u00a75.2 in\n[2]. These earlier results allow us to obtain the bound (1.3), however relying on the stronger condition\n\nK(m) <\n\u223c\n\n\u0010\n\nn \u00111/2\n.\nlog(n)\n\nThe numerical results for polynomial least squares that we present in \u00a73 hint that the weaker condition\nn\n\u221e\nK(m) <\n\u223c log(n) is sharp. The quantity K(m) was also used in [3] in order to control the L (X) norm and\n2\nthe L (\u03c1X ) norm.\nAccuracy of least squares approximation. As an application, we can derive an estimate for the error\nof least squares approximation in expectation. Here, we make the assumption that a uniform bound\n|f (x)| \u2264 L,\n\n(1.5)\n\nholds for almost every x with respect to \u03c1X . For m \u2264 n, we consider the truncated least squares estimator\nf \u0303 = TL (w),\nwhere TL (t) = sign(t) max{L, |t|}. Our first result deals with the noiseless case.\nTheorem 2 In the noiseless case, for any r > 0, if m is such that the condition (1.4) holds, then\nE(kf \u2212 f \u0303k2 ) \u2264 (1 + \u03b5(n))em (f )2 + 8L2 n\u2212r ,\nwhere \u03b5(n) :=\n\n4\u03ba\nlog(n)\n\n(1.6)\n\n\u2192 0 as n \u2192 +\u221e, with \u03ba as in (1.4)\n\nAt this point a few remarks are due regarding the implications of this result in terms of the convergence\nrate of the estimate.\nConsider the following general setting of regression on a random design: we observe independent samples\nzi = (xi , yi )i=1,...,n\n\n(1.7)\n\nof a variable z = (x, y) of law \u03c1 over X \u00d7 Y and marginal law \u03c1X over X, and we want to estimate from\nthese samples the regression function defined as the conditional expectation\nf (x) := E(y|x).\n4\n\n(1.8)\n\n\fWe assume that the maximal variance\n\u03c3 2 := sup E( |y \u2212 f (x)|2 x),\n\n(1.9)\n\nx\u2208X\n\nis bounded. We thus think of the yi as noisy observations of f at xi with additive noise of variance at most\n\u03c3 2 , namely\nyi = f (xi ) + \u03b7i ,\n(1.10)\nwhere the \u03b7i are independent realizations of the variable \u03b7 := y \u2212 f (x).\nAssuming that f satisfies the uniform bound (1.5), one computes the truncated least squares estimator\nnow with yi in place of f (xi ). A typical convergence bound for this estimator, see for example Theorem 11.3\nin [6], is\n\u0010\nm log n \u0011\n.\n(1.11)\nE(kf \u2212 f \u0303k2 ) \u2264 C em (f )2 + max{L2 , \u03c3 2 }\nn\nConvergence rates may be found after balancing the two terms, but they are limited by the optimal learning\nrate n\u22121 , and this limitation persists even in the noiseless case \u03c3 2 = 0 due to the presence of L2 in the right\nside of (1.11). In contrast, Theorem 2 yields fast convergence rates, provided that the approximation error\nem has fast decay and that the value of m satisfying (1.4) can be chosen large enough.\nOne motivation for studying the noiseless case is the numerical treatment of parameter dependent PDEs\nof the general form\nF(f, x) = 0,\nwhere x is a vector of parameters in some compact set P \u2208 Rd . We can consider the solution map x 7\u2192 f (x)\neither as giving the exact solution to the PDE for the given value of the parameter vector x or as the exact\nresult of a numerical solver for this value of x. In the stochastic PDE context, x is random and obeys\na certain law which may be known or unknown. From a random draw (xi )i=1,...,n , we obtain solutions\nfi = f (xi ) which are noiseless observations of the solution map, and are interested in reconstructing this\nmap. In instances such as elliptic problems with parameters in the diffusion coefficients, the solution map can\nbe well-approximated by polynomials in x (see [4]). In this context, an initial study of the needed amount\nof regularization was given in [7], however specifically targeted towards polynomial least squares.\nFor the noisy regression problem described above, our analysis can also be adapted in order to derive the\nfollowing result.\nTheorem 3 For any r > 0, if m is such that the condition (1.4) holds, then\nE(kf \u2212 f \u0303k2 ) \u2264 (1 + 2\u03b5(n))em (f )2 + 8L2 n\u2212r + 8\u03c3 2\n\nm\n,\nn\n\n(1.12)\n\nwith \u03b5(n) as in Theorem 2 and \u03c3 is the maximal variance given by (1.9).\nIn the noiseless case, the bound in Theorem 2 suggests that m should be chosen as large as possible under\nthe constraint that (1.4) holds. In the noisy case, the value of m minimizing the bound in Theorem 3 also\ndepends on the decay of em , which is generally unknown. In such a situation, a classical way of choosing the\nvalue of m is by a model selection procedure, such as adding a complexity penalty in the least squares or\nusing an independent validation sample. Such procedures can also be of interest in the noiseless case when\nthe measure \u03c1X is unknown, since the maximal value of m such that (1.4) holds is then also unknown.\nLet us give an example of how the results in Theorems 2 and 3 lead to specific rates of convergence in\nterms of the number of samples: assume that X = [\u22121, 1] is equipped with the uniform measure \u03c1X = dx\n2\nand that Vm = Pm\u22121 is the space of algebraic polynomials of degree m \u2212 1. Then, if f belongs to C r (X) the\n5\n\n\f\u22122r\nspace of r-times differentiable functions, it is well-known that em (f )2 <\n\u223c m . On the one hand the results\nin \u00a73 show that condition (1.4) can be ensured with m \u223c (n/ log n)1/2 . Therefore, in the noiseless case, we\nobtain a bound proportional to n\u2212r for the mean squared error, up the logarithmic factor. In the noisy case,\nafter balancing the approximation and variance terms, we obtain a bound proportional to \u03c3 2r/(r+1) n\u2212r/(r+1) .\nOn the other hand, these rates can be improved with r replaced by 2r if we use the Chebyshev non-uniform\nmeasure that concentrates near the end-points, since in that case the results in \u00a73 show that condition (1.4)\ncan be ensured with m \u223c n/ log n.\nThe rest of our paper is organized as follows: we give the proofs of the above results in \u00a72 and we present\nin \u00a73 examples of applications to classical approximation schemes such as piecewise constants, trigonometric\npolynomials, or algebraic polynomials. For such examples, we study the range of m such that (1.4) holds and\nshow that this range is in accordance with stability results that can be proved for deterministic sampling.\nNumerical illustrations are given for algebraic polynomial approximation.\n\n2\n\nProofs\n\nProof of Theorem 1: The matrix G can be written as\nG = X1 + * * * + Xn ,\nwhere the Xi are i.i.d. copies of the random matrix\nX=\n\n1\n(Lj (x)Lk (x))j,k=1,...,m ,\nn\n\nwhere x is distributed according to \u03c1X . We use the following Chernoff bound from [8], originally obtained\nby [1]: if X1 , . . . , Xn are independent m \u00d7 m random self-adjoint and positive matrices satisfying\n\u03bbmax (Xi ) = 9Xi 9 \u2264 R,\nalmost surely, then with\n\u03bcmin := \u03bbmin\n\nn\n\u0010X\n\n\u0011\nE(Xi )\n\nand\n\n\u03bcmax := \u03bbmax\n\ni=1\n\none has\n\n(\nPr\n\n\u03bbmin\n\nn\n\u0010X\n\n(\n\n)\n\n\u0011\n\nXi \u2264 (1 \u2212 \u03b4)\u03bcmin\n\nPr\n\n\u03bbmax\n\nn\n\u0010X\n\n\u0011\nE(Xi ) ,\n\ni=1\n\n\u0010\n\u2264m\n\ni=1\n\nand\n\nn\n\u0010X\n\n\u0011\u03bcmin /R\ne\u2212\u03b4\n, 0 \u2264 \u03b4 < 1,\n(1 \u2212 \u03b4)1\u2212\u03b4\n\n)\n\n\u0011\n\nXi \u2265 (1 + \u03b4)\u03bcmax\n\ni=1\n\n\u2264m\n\n\u0010\n\n\u0011\u03bcmax /R\ne\u03b4\n, \u03b4\u22650\n1+\u03b4\n(1 + \u03b4)\n\nPn\n\nIn our present case, we have i=1 E(Xi ) = nE(X) = I so that \u03bcmin = \u03bcmax = 1. It is easily checked that\ne\u03b4\ne\u2212\u03b4\n\u2265 (1\u2212\u03b4)\n1\u2212\u03b4 for 0 < \u03b4 < 1, and therefore\n(1+\u03b4)1+\u03b4\nPr {9G \u2212 I9 > \u03b4} \u2264 2m\n\n\u0010\n\n\u00111/R\n\u0010 c \u0011\ne\u03b4\n\u03b4\n= 2m exp \u2212\n.\n1+\u03b4\n(1 + \u03b4)\nR\n\nWe next use the fact that a rank 1 symmetric matrix abT = (bj ak )j,k=1,...,m has its spectral norm equal to\nthe product of the Euclidean norms of the vectors a and b, and therefore\nm\n\n9X9 \u2264\n\n1X\nK(m)\n|Lj (x)|2 =\n,\nn j=1\nn\n6\n\n\falmost surely. We may therefore take R =\n\nK(m)\nn\n\n2\n\nwhich concludes the proof.\n\nProof of Theorem 2: We denote by d\u03c1nX := \u2297n d\u03c1X the probability measure of the draw. We also\ndenote by \u03a9 the set of all possible draws, that we divide into the set \u03a9+ of all draw such that\n1\n,\n2\nand the complement set \u03a9\u2212 := \u03a9 \\ \u03a9+ . According to (1.3), we have\nZ\nPr{\u03a9\u2212 } =\nd\u03c1nX \u2264 2n\u2212r ,\n9G \u2212 I9 \u2264\n\n(2.1)\n\n\u03a9\u2212\n\nunder the condition (1.4). This leads to\nZ\nZ\nn\nE(kf \u2212 f \u0303k2 ) = kf \u2212 f \u0303k2 d\u03c1nX \u2264\nkf \u2212 Pm\nf k2 d\u03c1nX + 8L2 n\u2212r ,\n\u03a9\n\n\u03a9+\n\nwhere we have used kf \u2212 f \u0303k2 \u2264 2L2 , as well as the fact that TL is a contraction that preserves f .\nIt remains to prove that the first term in the above right side is bounded by (1 + \u03b5(n))em (f )2 . With\ng := f \u2212 Pm f , we observe that\nn\nn\nn\nn\nf \u2212 Pm\nf = f \u2212 Pm f + P m\nPm f \u2212 Pm\nf = g \u2212 Pm\ng.\n\nSince g is orthogonal to Vm , we thus have\nn\nn\nkf \u2212 Pm\nf k2 = kgk2 + kPm\ngk2 = kgk2 +\n\nm\nX\n\n|aj |2 ,\n\nj=1\n\nwhere a = (aj )j=1,...,m is solution of the system\nGa = b,\nwith b := (hg, Lk in )k=1,...,m . When the draw belongs to \u03a9+ , we have kG\u22121 k2 \u2264 2 and therefore\nm\nX\n\n|aj |2 \u2264 4\n\nj=1\n\nIt follows that\nZ\nZ\nn\n2\nn\nkf \u2212 Pm f k d\u03c1X \u2264\n\u03a9+\n\n2\n\nkgk + 4\n\nm\nX\nk=1\n\nm\nX\n\n!\n2\n\n|hg, Lk in |\n\nk=1\n\n\u03a9+\n\n|hg, Lk in |2 .\n\nd\u03c1nX \u2264 kgk2 + 4\n\nm\nX\n\nE(|hg, Lk in |2 ).\n\nk=1\n\nWe estimate each of the E(|hg, Lk in |2 ) as follows:\nn\nn\n1 XX\nE(g(xi )g(xj )Lk (xi )Lk (xj ))\nn2 i=1 j=1\n\u0011\n1\u0010\n= 2 n(n \u2212 1)|E(g(x)Lk (x))|2 + nE(|g(x)Lk (x)|2 )\nn\nZ\n\u0010\n1\u0011\n1\n2\n= 1\u2212\n|hg, Lk i| +\n|g(x)|2 |Lk (x)|2 d\u03c1X\nn\nn\nX\nZ\n1\n=\n|g(x)|2 |Lk (x)|2 d\u03c1X ,\nn\n\nE(|hg, Lk in |2 ) =\n\nX\n\n7\n\n\fwhere we have used the fact that g is orthogonal to Vm and thus to Lk . Summing over k, we obtain\nm\nX\n\nE(|hg, Lk in |2 ) \u2264\n\nk=1\n\nK(m)\n\u03ba\nkgk2 \u2264\nkgk2 ,\nn\nlog(n)\n\nwhere we have used (1.4). We have thus proven that\nZ\n4\u03ba\nn\nkf \u2212 Pm\nf k2 d\u03c1nX \u2264 (1 +\n)kgk2 = (1 + \u03b5(n))em (f )2 ,\nlog(n)\n\u03a9+\n\n2\n\nwhich concludes the proof.\nProof of Theorem 3: We define the additive noise in the sample by writing\nyi = f (xi ) + \u03b7i ,\nand thus the \u03b7i are i.i.d. copies of the variable\n\u03b7 = y \u2212 f (x).\nNote that \u03b7 and x are not assumed to be independent. However we have\nE(\u03b7|x) = 0,\nwhich implies the decorrelation property\nE(\u03b7h(x)) = 0,\nfor any function h. As in the proof of Theorem 2 we split \u03a9 into \u03a9+ and \u03a9\u2212 and find that\nZ\nE(kf \u2212 f \u0303k2 ) \u2264\nkf \u2212 wk2 d\u03c1nX + 8M 2 n\u2212r ,\n\u03a9+\n\nwhere w now stands for the solution to the least squares problem with noisy data (y1 , . . . , yn ). With the\nsame definition of g = f \u2212 Pm f , we can write\nn\nf \u2212 w = g \u2212 Pm\ng \u2212 w,\ne\n\nwhere w\ne stands for the solution to the least squares problem for the noise data (\u03b71 , . . . , \u03b7n ). Therefore\nn\nn\nn\nkf \u2212 Pm\nf k2 = kgk2 + kPm\ng + wk\ne 2 \u2264 kgk2 + 2kPm\ngk2 + 2kwk\ne 2 = |gk2 + 2\n\nm\nX\nj=1\n\n|aj |2 + 2\n\nm\nX\n\n|dj |2 ,\n\nj=1\n\nwhere a = (aj )j=1,...,m is as in the proof of Theorem 2 and d = (dj )j=1,...,m is solution of the system\nGd = n,\nPn\nwith n := ( n1 i=1 \u03b7i Lk (xi ))k=1,...,m = (nk )k=1,...,m . By the same arguments as in the proof of Theorem 2,\nwe thus obtain\nm\nX\nE(kf \u2212 f \u0303k2 ) \u2264 (1 + 2\u03b5(n))em (f )2 + 8L2 n\u2212r + 8\nE(|nk |2 ).\nk=1\n\n8\n\n\fWe are left to show that\n\nPm\n\nk=1\n\n\u03c32 m\nn .\n\nE(|nk |2 ) \u2264\nE(|nk |2 ) =\n\nFor this we simply write that\n\nn\nn\n1 XX\nE(\u03b7i Lk (xi )\u03b7j Lk (xj )).\nn2 i=1 j=1\n\nFor i 6= j, we have\nE(\u03b7i Lk (xi )\u03b7j Lk (xj )) = (E(\u03b7Lk (x)))2 = 0.\nFor i = j, we have\nE(|\u03b7i Lk (xi )|2 ) = E(|\u03b7Lk (x)|2 )\nZ\n= E(|\u03b7Lk (x)|2 |x)d\u03c1X\nX\n\nZ\n\nE(|\u03b7|2 |x)|Lk (x)|2 d\u03c1X\n\n=\nX\n\n\u2264\u03c3\n\n2\n\nZ\n\n|Lk (x)|2 d\u03c1X = \u03c3 2 .\n\nX\n\nIt follows that E(|nk |2 ) \u2264\n\n3\n\n2\n\n\u03c3\nn\n\n2\n\n, which concludes the proof.\n\nExamples and numerical illustrations\n\nWe now give several examples of approximation schemes for which one can compute the quantity K(m) and\ntherefore estimate the range of m such that the condition (1.4) holds. For each of these examples, we also\nexhibit a deterministic sampling (x1 , . . . , xn ) for which the stability property\n9G \u2212 I9 \u2264\n\n1\n,\n2\n\nor equivalently\n\n1\nkvk2 , v \u2208 Vm ,\n2\nis ensured for the same range of m (actually slightly better by a logarithmic factor). For the sake of simplicity, we work in the one dimensional setting, with X a bounded interval.\nkvk2n \u2212 kvk2 \u2264\n\nPiecewise constant functions. Here X = [a, b] and Vm is the space of piecewise constant functions over\na partition of X into intervals I1 , . . . , Im . In such a case, an orthonormal basis with respect to L2 (X, \u03c1X ) is\ngiven by the characteristic functions Lk := (\u03c1X (Ik ))\u22121/2 \u03c7Ik , and therefore\nK(m) =\n\nmax (\u03c1X (Ik ))\u22121 .\n\nk=1,...,m\n\nGiven a measure \u03c1X , the partition that minimizes K(m), and therefore allows us to fulfill (1.4) for the largest\nrange of m, is one that evenly distributes the measure \u03c1X . With such partitions, K(m) reaches its minimal\nvalue\nK(m) = m,\nand (1.4) can be achieved with m \u223c logn n .\nIf we now choose n = m deterministic points x1 , . . . , xm with xk \u2208 Ik , we clearly have\nkvk2n = kvk2 , v \u2208 Vm .\n9\n\n\fTherefore the stability of the least squares problem can be ensured with m up to the value n using a deterministic sample.\nTrigonometric polynomials and uniform measure. Without loss of generality, we take X = [\u2212\u03c0, \u03c0],\nand we consider for odd m = 2p + 1 the space Vm of trigonometric polynomials of degree p, which is spanned\nby the functions Lk (x) = eikx for k = \u2212p, . . . , p. Assuming that \u03c1X is the uniform measure, this is an\northonormal basis with respect to L2 (X, \u03c1X ). In this example, we again obtain the minimal value\nK(m) = m.\nTherefore (1.4) can be achieved with m \u223c logn n .\nWe now consider the deterministic uniform sampling xi := \u2212\u03c0 + 2\u03c0i\nn for i = 1, . . . , n. With such a sampling,\none has the identity\nZ\u03c0\nZ\u03c0\nn\n1\n1X\nv(x)d\u03c1X =\nv(x)dx =\nv(xi ),\n2\u03c0\nn i=1\n\u2212\u03c0\n\n\u2212\u03c0\n\nfor all trigonometric polynomials v of degree n \u2212 1 (this is easily seen by checking the identity on every basis\nelement). When v \u2208 Vm with m = 2p + 1, we know that |v|2 is a trigonometric polynomial of degree 2p. We\nthus find that\nkvk2n = kvk2 , v \u2208 Vm ,\nprovided that 2p \u2264 n \u2212 1, or equivalently m \u2264 n. Therefore the stability of the least squares problem can\nbe ensured with m up to the value n using a deterministic sample.\nAlgebraic polynomials and uniform measure. Without loss of generality, we take X = [\u22121, 1], and we\nconsider Vm = Pm\u22121 the space of algebraic polynomials of degree m \u2212 1. When \u03c1X is the uniform measure,\nan orthonormal basis is given by defining Lk as the Legendre polynomial of degree k \u2212 1 with normalization\nkLk kL\u221e ([\u22121,1]) = |Lk (1)| =\nand thus\nK(m) =\n\nm\nX\n\n\u221a\n\n2k \u2212 1,\n\n(2k \u2212 1) = m2 .\n\nk=1\n\nTherefore (1.4) can be achieved with m \u223c\n\nq\n\nn\nlog n\n\nwhich is a lower range compared to the previous examples.\n\nWe now consider the deterministic sampling obtained by partitioning X into n intervals (I1 , . . . , In ) of equal\n\n10\n\n\flength\n\n2\nn,\n\nand picking one point xi in each Ii . For any v \u2208 Vm , we may write\nZ\n\n1\n1\n|v(x)| d\u03c1X \u2212 |v(xi )|2 =\nn\n2\n2\n\nIi\n\nZ\n\n|v(x)|2 dx \u2212\n\n1\n|v(xi )|2\nn\n\nIi\n\nZ\n1\n(|v(x)|2 \u2212 |v(xi )|2 )dx\n2\nI\nZi\n1\n\u2264\n|v(x)|2 \u2212 |v(xi )|2 dx\n2\nIi\nZ\n1\n\u2264 |Ii | |(v 2 )0 (x)|dx\n2\nI\nZ i\n2\n=\n|v 0 (x)v(x)|d\u03c1X .\nn\n=\n\nIi\n\nSumming over i, it follows that\nkvk2n\n\n\u2212 kvk\n\n2\n\n2\n\u2264\nn\n\nZ\n\n|v 0 (x)v(x)|d\u03c1X \u2264\n\n2 0\n2(m \u2212 1)2\nkv k kvk \u2264\nkvk2 ,\nn\nn\n\nX\n\nwhere we have used the Cauchy-Schwarz and Markov\ninequalities. Therefore the stability of the least squares\n\u221a\nn\nproblem can be ensured with m up to the value 2 + 1 using a deterministic sample.\nAlgebraic polynomials and Chebyshev measure. Consider again algebraic polynomials of degree\nm \u2212 1 on X = [\u22121, 1], now equipped with the measure\nd\u03c1X =\n\ndx\n.\n\u03c0 1 \u2212 x2\n\u221a\n\nThen an orthonormal basis is given by defining Lk as the Chebyshev polynomial of degree k \u2212 1, with L1 = 1\nand\n\u221a\nLk (x) = 2 cos((k \u2212 1) arccos x),\nfor k > 1, and thus\nK(m) = 2m \u2212 1.\nTherefore (1.4) can be achieved with m \u223c logn n , which expresses the fact that least squares approximations\nare stable for higher polynomial degrees when working with the Chebyshev measure rather than with the\nuniform measure.\nWe now consider the deterministic sampling obtained by partitioning X into n intervals (I1 , . . . , In ) of equal\n\n11\n\n\fChebyshev measure \u03c1X (Ii ) =\nZ\n\n1\nn,\n\nand picking one point xi in each Ii . For any v \u2208 Vm , we may write\n\n|v(x)|2 d\u03c1X \u2212\n\n1\n|v(xi )|2 =\nn\n\nZ\n\n(|v(x)|2 \u2212 |v(xi )|2 )d\u03c1X\n\nI\n\nIi\n\nZi\n\u2264\n\n|v(x)|2 \u2212 |v(xi )|2 d\u03c1X\n\nIi\n\nZ\n\n|(v 2 )0 (x)|dx\n\n\u2264 \u03c1X (Ii )\nIi\n\n1\n=\nn\n\nZ\n\n|v 0 (x)v(x)|dx.\n\nIi\n\nSumming over i, it follows that\nkvk2n \u2212 kvk2 \u2264\n\n1\nn\n\nZ\n\n|v 0 (x)v(x)|dx \u2264\n\n\u0010\n1\nkvk \u03c0\nn\n\nX\n\nZ\n\n|v 0 (x)|2\n\np\n\n\u00111/2\n1 \u2212 x2 dx\n.\n\nX\n\nUsing the change of variable x = cos t, it is easily seen that the inverse estimate\nZ\nZ\np\n1\n|v 0 (x)|2 1 \u2212 x2 dx \u2264 (m \u2212 1)2 |v(x)|2 \u221a\ndx,\n1 \u2212 x2\nX\n\nX\n\nholds for any v \u2208 Vm . Therefore\nkvk2n\n\n2\n\n\u2212 kvk\n\n1\n\u2264\nn\n\nZ\n\n|v 0 (x)v(x)|dx \u2264\n\n\u03c0(m \u2212 1)\nkvk2\nn\n\nX\n\nwhich shows that the stability of the least squares problem can be ensured with m up to the value\nusing a deterministic sample.\n\nn\n2\u03c0\n\n+1\n\nLet us observe that in several practical scenarios, the measure \u03c1X of the observations may be unknown\nto us, therefore raising the question of the behavior of K(m) for an arbitrary measure.\nIt is not too difficult to check that when the space Vm is not the trivial space of constant functions (which\nis the case as soon as m \u2265 2) the quantity K(m) may become arbitrarily large for certain measures \u03c1X . We\nleave the proof of this general fact as an exercise for the reader, and rather provide a simple illustration:\n1 \u03c7\nwhere\nconsider the space V2 of polynomials of degree 1 on [\u22121, 1] and the measure \u03c1X = 2\u03b5\n[\u2212\u03b5,\u03b5] (x)dx\n\u221a\n\u03b5 > 0 is small. Then an orthonormal basis is provided by the functions L0 (x) = 1 and L1 (x) = \u03b53 x, so\nthat K(m) \u223c \u03b5\u22122 . An interesting problem is to understand if for certain families of space (Vm ), the quantity\nK(m) can be controlled under fairly general assumptions on the measure \u03c1X . One typical such assumption\nis the the strong density assumption, which states that\n\u03c1X (E) \u223c |E|, E measurable,\n\n(3.1)\n\nwhere | * | is the Lebesgue measure. In the case of piecewise constant functions on uniform partitions, or for\nmore general spline functions on uniform grids, it is not difficult to check that this assumption implies the\nbehavior K(m) \u223c m.\n12\n\n\f30\n\n10\n\nUniform\nChebyshev\n\nUniform\nChebyshev\n200\n\n10\n20\n\nError\n\nError\n\n10\n\n10\n\n10\n\n100\n\n10\n\n0\n\n10\n\n0\n\n10\n\u221210\n\n10\n\n0\n\n50\n\n100\nm\n\n150\n\n200\n\n0\n\n(a)\n\n200\n\n400\n\nm\n\n600\n\n800\n\n1000\n\n(b)\n\nFigure 3.1: The L2 (X, \u03c1X ) error as m varies (a) for f1 and (b) for f2 .\n\nNumerical illustration. We conclude with a brief numerical illustration of our theoretical results for\nthe setting of algebraic polynomials. Specifically, we consider the smooth function f1 (x) = 1/(1 + 25x2 )\noriginally considered by Runge to illustrate the instability of polynomial interpolation at equispaced points,\nand the non-smooth function f2 (x) = |x|, both restricted to the interval [\u22121, 1].\nFor both functions, we take n i.i.d. samples x1 , . . . , xn with respect to a measure \u03c1X on X = [\u22121, 1] and\ncompute the noise-free observations yi = f (xi ). We consider either the uniform measure \u03c1X := dx\n2 or the\n.\nIn\nboth\ncases,\nwe\ncompute\nthe\nleast\nsquares\napproximating\npolynomial\nChebyshev measure \u03c1X := \u03c0\u221adx\n1\u2212x2\nof degree m using these points for a range of different values of m \u2264 n. We then numerically compute the\nerror in the L2 (X, \u03c1X ) norm, with \u03c1X the corresponding measure in which the sample have been drawn,\nusing the adaptive Simpson's quadrature rule [5] implemented in Matlab.\nFigure 3.1 shows the results of this simulation using n1 = 200 samples for estimating f1 and n2 = 1000\nsamples for estimating f2 . We observe that, in all cases, as m approaches n the solutions become highly\ninaccurate due to the inherent instability of the problem. However, we can set m to be much larger before\ninstability starts to develop when the points are drawn with respect to the Chebyshev measure, as is expected.\nNext we consider the effect of n on the best choice of m. Specifically, for any given sample of points\nwe can compute the value m(n) that corresponds to the polynomial degree for which we obtain the best\napproximation to f1 or f2 and examine how this behaves as a function of n. This is shown in Figure 3.2, that\ndisplays as a function of n the average value of m(n) over 50 realizations of the sample, for both measures\nand both functions f1 and f2 (the averaging has the effect of reducing oscillation in the curve n 7\u2192 m(n)\nmaking it more readable). We vary the sample size from n = 1 to 1000 for f2 , but only from n = 1 to 200\nfor the smooth function f1 , since in that case the L2 (X, \u03c1X ) error drops below machine precision for larger\nvalues of n with m in the regime where the least squares problem is stable and therefore the minimal value\nm(n) cannot be precisely located.\n\u221a\nWe observe that, in accordance with our theoretical results, m(n) behaves like n when the points are\ndrawn with respect to the uniform measure, while it behaves almost linear in n when the points are drawn\nwith respect to the Chebyshev measure.\n\n13\n\n\f150\n\n120\nUniform\nChebyshev\n\nUniform\nChebyshev\n100\n\n100\nm(n)\n\nm(n)\n\n80\n\n50\n\n60\n40\n20\n\n0\n0\n\n50\n\n100\nn\n\n150\n\n0\n0\n\n200\n\n(a)\n\n200\n\n400\n\nn\n\n600\n\n800\n\n1000\n\n(b)\n\n\u221a\nFigure 3.2: Optimal values m(n) as n varies (a) for f1 (comparison with 0.7n and 2.5 n) and (b) for f2\n\u221a\n(comparison with 0.1n and 0.4 n).\n\nReferences\n[1] Ahlswede, R. and A. Winter, Strong converse for identification via quantum channels, IEEE Trans.\nInformation Theory 48, 569\u2013579, 2002.\n[2] Baraud, Y., Model selection for regression on a random design, ESAIM Prob. Stat. 6, 127\u2013146, 2002.\n[3] Birg\u00e9 L. and P. Massart, Minimum contrast estimators on sieves: Exponential bounds and rates of\nconvergence, Bernoulli 4, 329\u2013375, 1998.\n[4] Cohen, A., R. DeVore and C. Schwab, Analytic regularity and polynomial approximation of parametric\nelliptic PDE's, Analysis and Applications 9, 11\u201347, 2011.\n[5] Gander, W. and W. Gautschi, Adaptive Quadrature \u2013 Revisited, BIT 40, 84\u2013101, 2000.\n[6] Gy\u00f6rfi, L., M. Kohler, A. Krzyzak, A. and H. Walk, A distribution-free theory of nonparametric regression, Springer, Berlin, 2002.\n[7] Migliorati, G., F. Nobile, E. von Schweriny and R. Tempone, Analysis of the point collocation method,\npreprint MOX, Politecnico di Milano, 2011.\n[8] Tropp, J. User friendly tail bounds for sums of random matrices, to appear in J. FoCM, 2011.\n\nThe authors would like to thank Lukas Meier for bringing a small error in the original proof of Theorem 1\nto our attention.\nAlbert Cohen\nLaboratoire Jacques-Louis Lions\nUniversit\u00e9 Pierre et Marie Curie\n4, Place Jussieu, 75005 Paris, France\ncohen@ann.jussieu.fr\n14\n\n\fMark Davenport\nSchool of Electrical and Computer Engineering\nGeorgia Institute of Technology\n777 Atlantic Drive NW\nAtlanta, GA 30332, USA\nmdav@gatech.edu\nDany Leviatan\nRaymond and Beverly Sackler School of Mathematics\nTel Aviv University\n69978, Tel Aviv, Israel\nleviatan@post.tau.ac.il\n\n15\n\n\f"}