{"id": "http://arxiv.org/abs/1111.6358v1", "guidislink": true, "updated": "2011-11-28T07:25:54Z", "updated_parsed": [2011, 11, 28, 7, 25, 54, 0, 332, 0], "published": "2011-11-28T07:25:54Z", "published_parsed": [2011, 11, 28, 7, 25, 54, 0, 332, 0], "title": "Bounds for tail probabilities of martingales using skewness and kurtosis", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1111.5325%2C1111.5143%2C1111.5426%2C1111.4990%2C1111.6087%2C1111.5677%2C1111.6832%2C1111.6879%2C1111.2786%2C1111.3839%2C1111.6358%2C1111.5319%2C1111.4794%2C1111.0196%2C1111.3984%2C1111.0149%2C1111.1226%2C1111.4766%2C1111.1100%2C1111.4669%2C1111.0668%2C1111.6235%2C1111.2177%2C1111.2352%2C1111.2729%2C1111.3205%2C1111.5940%2C1111.3310%2C1111.0369%2C1111.0944%2C1111.6316%2C1111.2343%2C1111.3406%2C1111.6477%2C1111.0562%2C1111.4779%2C1111.4966%2C1111.2147%2C1111.5901%2C1111.7005%2C1111.6876%2C1111.1591%2C1111.5534%2C1111.7138%2C1111.1386%2C1111.4113%2C1111.5831%2C1111.6781%2C1111.3813%2C1111.1145%2C1111.0986%2C1111.4224%2C1111.2982%2C1111.3004%2C1111.0857%2C1111.3236%2C1111.6474%2C1111.5925%2C1111.1288%2C1111.2461%2C1111.3367%2C1111.1093%2C1111.4546%2C1111.6984%2C1111.1338%2C1111.2346%2C1111.1554%2C1111.0134%2C1111.5565%2C1111.6844%2C1111.6012%2C1111.1266%2C1111.3468%2C1111.2860%2C1111.0831%2C1111.0909%2C1111.4119%2C1111.1437%2C1111.0936%2C1111.2998%2C1111.5374%2C1111.7100%2C1111.2758%2C1111.4358%2C1111.3784%2C1111.2622%2C1111.5654%2C1111.1874%2C1111.2523%2C1111.4401%2C1111.3942%2C1111.7285%2C1111.4379%2C1111.0305%2C1111.6799%2C1111.2212%2C1111.4743%2C1111.5084%2C1111.3375%2C1111.5868%2C1111.2848&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Bounds for tail probabilities of martingales using skewness and kurtosis"}, "summary": "Let $M_n= \\fsu X1n$ be a sum of independent random variables such that $\nX_k\\leq 1$, $\\E X_k =0$ and $\\E X_k^2=\\s_k^2$ for all $k$. Hoeffding 1963,\nTheorem 3, proved that $$\\P{M_n \\geq nt}\\leq H^n(t,p),\\quad H(t,p)=\n\\bgl(1+qt/p\\bgr)^{p +qt} \\bgl({1-t}\\bgr)^{q -qt}$$ with $$q=\\ffrac\n1{1+\\s^2},\\quad p=1-q, \\quad \\s^2 =\\ffrac {\\s_1^2+...+\\s_n^2}n,\\quad 0<t<1.$$\nBentkus 2004 improved Hoeffding's inequalities using binomial tails as upper\nbounds. Let $\\ga_k =\\E X_k^3/\\s_k^3$ and $ \\vk_k= \\E X_k^4/\\s_k^4$ stand for\nthe skewness and kurtosis of $X_k$. In this paper we prove (improved)\ncounterparts of the Hoeffding inequality replacing $\\s^2$ by certain functions\nof $\\fs \\ga 1n$ respectively $\\fs \\vk 1n$. Our bounds extend to a general\nsetting where $X_k$ are martingale differences, and they can combine the\nknowledge of skewness and/or kurtosis and/or variances of ~$X_k$. Up to factors\nbounded by $e^2/2$ the bounds are final. All our results are new since no\ninequalities incorporating skewness or kurtosis control so far are known.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1111.5325%2C1111.5143%2C1111.5426%2C1111.4990%2C1111.6087%2C1111.5677%2C1111.6832%2C1111.6879%2C1111.2786%2C1111.3839%2C1111.6358%2C1111.5319%2C1111.4794%2C1111.0196%2C1111.3984%2C1111.0149%2C1111.1226%2C1111.4766%2C1111.1100%2C1111.4669%2C1111.0668%2C1111.6235%2C1111.2177%2C1111.2352%2C1111.2729%2C1111.3205%2C1111.5940%2C1111.3310%2C1111.0369%2C1111.0944%2C1111.6316%2C1111.2343%2C1111.3406%2C1111.6477%2C1111.0562%2C1111.4779%2C1111.4966%2C1111.2147%2C1111.5901%2C1111.7005%2C1111.6876%2C1111.1591%2C1111.5534%2C1111.7138%2C1111.1386%2C1111.4113%2C1111.5831%2C1111.6781%2C1111.3813%2C1111.1145%2C1111.0986%2C1111.4224%2C1111.2982%2C1111.3004%2C1111.0857%2C1111.3236%2C1111.6474%2C1111.5925%2C1111.1288%2C1111.2461%2C1111.3367%2C1111.1093%2C1111.4546%2C1111.6984%2C1111.1338%2C1111.2346%2C1111.1554%2C1111.0134%2C1111.5565%2C1111.6844%2C1111.6012%2C1111.1266%2C1111.3468%2C1111.2860%2C1111.0831%2C1111.0909%2C1111.4119%2C1111.1437%2C1111.0936%2C1111.2998%2C1111.5374%2C1111.7100%2C1111.2758%2C1111.4358%2C1111.3784%2C1111.2622%2C1111.5654%2C1111.1874%2C1111.2523%2C1111.4401%2C1111.3942%2C1111.7285%2C1111.4379%2C1111.0305%2C1111.6799%2C1111.2212%2C1111.4743%2C1111.5084%2C1111.3375%2C1111.5868%2C1111.2848&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Let $M_n= \\fsu X1n$ be a sum of independent random variables such that $\nX_k\\leq 1$, $\\E X_k =0$ and $\\E X_k^2=\\s_k^2$ for all $k$. Hoeffding 1963,\nTheorem 3, proved that $$\\P{M_n \\geq nt}\\leq H^n(t,p),\\quad H(t,p)=\n\\bgl(1+qt/p\\bgr)^{p +qt} \\bgl({1-t}\\bgr)^{q -qt}$$ with $$q=\\ffrac\n1{1+\\s^2},\\quad p=1-q, \\quad \\s^2 =\\ffrac {\\s_1^2+...+\\s_n^2}n,\\quad 0<t<1.$$\nBentkus 2004 improved Hoeffding's inequalities using binomial tails as upper\nbounds. Let $\\ga_k =\\E X_k^3/\\s_k^3$ and $ \\vk_k= \\E X_k^4/\\s_k^4$ stand for\nthe skewness and kurtosis of $X_k$. In this paper we prove (improved)\ncounterparts of the Hoeffding inequality replacing $\\s^2$ by certain functions\nof $\\fs \\ga 1n$ respectively $\\fs \\vk 1n$. Our bounds extend to a general\nsetting where $X_k$ are martingale differences, and they can combine the\nknowledge of skewness and/or kurtosis and/or variances of ~$X_k$. Up to factors\nbounded by $e^2/2$ the bounds are final. All our results are new since no\ninequalities incorporating skewness or kurtosis control so far are known."}, "authors": ["Vidmantas Bentkus", "Tomas Ju\u0161kevi\u010dius"], "author_detail": {"name": "Tomas Ju\u0161kevi\u010dius"}, "author": "Tomas Ju\u0161kevi\u010dius", "arxiv_comment": "Lithuanian Mathematical Journal (2008)", "links": [{"href": "http://arxiv.org/abs/1111.6358v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1111.6358v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1111.6358v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1111.6358v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:1111.6358v1 [math.PR] 28 Nov 2011\n\nBOUNDS FOR TAIL PROBABILITIES OF\nMARTINGALES USING SKEWNESS AND KURTOSIS\n\nV. Bentkus1 and T. Ju\u0161kevi\u010dius1\nJanuary 2008\nAbstract. Let Mn = X1 + * * * + Xn be a sum of independent random variables such that Xk \u2264 1,\nE Xk = 0 and E Xk2 = \u03c3k2 for all k. Hoeffding 1963, Theorem 3, proved that\nP {Mn \u2265 nt} \u2264 H n (t, p),\nwith\nq=\n\n1\n,\n1 + \u03c32\n\np = 1 \u2212 q,\n\nH(t, p) = 1 + qt/p\n\n\u03c32 =\n\n\u0001p+qt\n\n2\n\u03c312 + * * * + \u03c3n\n,\nn\n\n1\u2212t\n\n\u0001q\u2212qt\n\n0 < t < 1.\n\nBentkus 2004 improved Hoeffding's inequalities using binomial tails as upper bounds. Let \u03b3k =\nE Xk3 /\u03c3k3 and \u03bak = E Xk4 /\u03c3k4 stand for the skewness and kurtosis of Xk . In this paper we prove\n(improved) counterparts of the Hoeffding inequality replacing \u03c3 2 by certain functions of \u03b31 , . . . , \u03b3n\nrespectively \u03ba1 , . . . , \u03ban . Our bounds extend to a general setting where Xk are martingale differences,\nand they can combine the knowledge of skewness and/or kurtosis and/or variances of Xk . Up\nto factors bounded by e2 /2 the bounds are final. All our results are new since no inequalities\nincorporating skewness or kurtosis control so far are known.\n\n1. Introduction and results\nIn a celebrated paper of Hoeffding 1963 several inequalities for sums of bounded random\nvariables were established. For improvements of the Hoeffding inequalities and related results\nsee, for example, Talagrand 1995, McDiarmid 1989, Godbole and Hitczenko 1998, Pinelis 1998\u2013\n2007, Laib 1999, B 2001\u20132007, van de Geer 2002, Perron 2003, BGZ 2006\u20132006, BGPZ 2006,\nBKZ 2006, 2007, BZ 2003, etc. Up to certain constant factors, these improvements are close\nto the final optimal inequalities, see B 2004, BKZ 2006. However so far no bounds taking\ninto account information related to skewness and/or kurtosis are known, not to mention certain\nresults related to symmetric random variables, see BGZ 2006, BGPZ 2006. In this paper we prove\ngeneral and optimal counterparts of Hoeffding's 1963 Theorem 3, using assumptions related to\nskewness and/or kurtosis.\n1 Vilnius Institute of Mathematics and Informatics. Supported by grant ???\n1991 Mathematics Subject Classification. 60E15.\nKey words and phrases. skewness, kurtosis, Hoeffding's inequalities, sums of independent random variables,\nmartingales, bounds for tail probabilities, Bernoulli random variables, binomial tails, probabilities of large deviations, method of bounded differences.\n\n1\n\nTypeset by AMS-TEX\n\n\f2\n\nV. BENTKUS\n\nLet us recall Hoeffding's 1963 Theorem 3. Let Mn = X1 + * * * + Xn be a sum of independent\nrandom variables such that Xk \u2264 1, E Xk = 0, and E Xk2 = \u03c3k2 for all k. Write\n\u03c32 =\n\n2\n\u03c312 + * * * + \u03c3n\nn\n\n,\n\n\u03c32\n1 + \u03c32\n\np=\n\n,\n\nq = 1 \u2212 p.\n\nHoeffding 1963, Theorem 3, established the inequality\nP {Mn \u2265 nt} \u2264 H n (t, p),\n\nH(t, p) = 1 + qt/p\n\n\u0001p+qt\n\n1\u2212t\n\n\u0001q\u2212qt\n\n(1.1)\n\nassuming that 0 < t < 1. One can rewrite H n (x, p) as\nH n (x, p) = inf exp{\u2212hnt} E exp{hTn },\nh>0\n\nwhere Tn = \u03b51 + * * * + \u03b5n is a sum of n independent copies of a Bernoulli random variable,\nsay \u03b5 = \u03b5(\u03c3 2 ), such that\nP {\u03b5 = \u2212\u03c3 2 } = q,\n\nP {\u03b5 = 1} = p,\n\nE \u03b52 = \u03c3 2 .\n\n(1.2)\n\nUsing the shorthand x = nt, we can rewrite the Hoeffding result as\nP {Mn \u2265 x} \u2264 inf e\u2212hx E ehTn ,\n\n(1.3)\n\nh>0\n\nIn B 2004 the inequality (1.3) is improved to\nP {Mn \u2265 x} \u2264 inf (x \u2212 h)\u22122 E (Tn \u2212 h)2+ .\nh<x\n\n(1.4)\n\nActually, inequalities (1.1), (1.3) and (1.4) extend to cases where Mn is a martingale or even\nsuper-martingale, see B 2004 for a proof. In the case of (1.1) and (1.3) this was noted already\nby Hoeffding 1963.\nThe right hand side of (1.4) satisfies\ninf (t \u2212 h)\u22122 E (Tn \u2212 h)2+ \u2264\n\nh<x\n\ne2\n2\n\nP {Tn \u2265 x},\n\ne = 2.718...\n\n(1.5)\n\nfor integer x \u2208 Z. For non-integer x one has to interpolate the probability log-linearly, see B 2004\nfor details. The right-hand side of (1.4) can be given explicitly as a function of x, p and n,\nsee BKZ 2006, as well as Section 2 of the present paper. To have bounds as tight as possible is\nessential for statistical applications, like those in audit, see BZ 2003.\nOur intention in this paper is to develop methods leading to counterparts of (1.1), (1.3)\nand (1.4) such that information related to the skewness and kurtosis\n\u03b3k =\n\nE (Xk \u2212 E Xk )3\n\u03c3k3\n\n,\n\n\u03bak =\n\nE (Xk \u2212 E Xk )4\n\u03c3k4\n\n(1.6)\n\nof Xk is taken into account (in this paper we define \u03b3k = \u221e and \u03bak = 1 if \u03c3k = 0). All our\nresults hold in general martingale setting.\n\n\fSKEWNESS AND CURTOSIS\n\n3\n\nAll known proofs of inequalities of type (1.3) and (1.4) start with an application of Chebyshev's inequality. For example, in the case of (1.4) we can estimate\nP {Mn \u2265 x} \u2264 inf (x \u2212 h)\u22122 E (Mn \u2212 h)2+\n\n(1.7)\n\nh<x\n\nsince the indicator function t 7\u2192 I{t \u2265 x} obviously satisfies I{t \u2265 x} \u2264 (x \u2212 h)\u22122 (t \u2212 h)2+ for\nall t \u2208 R. The further proof of (1.4) consists in showing that E (Mn \u2212 h)2+ \u2264 E (Tn \u2212 h)2+ for\nall h \u2208 R. We would like to emphasize that all our proofs are optimal in the sense that no further\nimprovements are possible in estimation of E (Mn \u2212 h)2+ . Indeed, in the special case Mn = Tn\nthe inequality E (Mn \u2212 h)2+ \u2264 E (Tn \u2212 h)2+ turns into the equality E (Tn \u2212 h)2+ = E (Tn \u2212 h)2+ .\nIn view of (1.7) it is natural to introduce and to study transforms G 7\u2192 G\u03b2 of survival\nfunctions G(x) = P {X \u2265 x} of the type\nG\u03b2 (x) = inf (x \u2212 h)\u2212\u03b2 E (X \u2212 h)\u03b2+ ,\n\n\u03b2 > 0,\n\nh<x\n\n(1.8)\n\ndefining G0 = G in the case \u03b2 = 0. See Pinelis 1988, 1989, B 2004, BKZ 2006 for related known\nresults.\nThe paper is organized as follows. In the Introduction we provide necessary definitions and\nformulations of our results, including their versions for sums of martingale differences. In Section\n2 we recall a description of the transform G 7\u2192 G2 of binomial survival functions-our bounds\nare given using G2 . Section 3 contains proofs of the results.\nHenceforth Mn = X1 + * * * + Xn stands for a martingale sequence such that the differences Xk\nare uniformly bounded (we set M0 = X0 = 0). Without loss of generality we can assume that the\nbounding constant is 1, that is, that Xk \u2264 1. Let F0 \u2282 F1 \u2282 * * * \u2282 Fn be a related sequence of\n\u03c3-algebras such that Mk are Fk -measurable. Introduce the conditional variance s2k , skewness gk\nand kurtosis ck of Xk by\n\u0001\ns2k = E Xk2 Fk\u22121 ,\n\n\u0001\ngk = E Xk3 Fk\u22121 /s3k ,\n\n\u0001\nck = E Xk4 Fk\u22121 /s4k .\n\n(1.9)\n\nNote that s2k , gk , ck are Fk\u22121 -measurable random variables.\nRemark 1.1. We prove our results using (1.4) for martingales. It is proved in B 2004 that\nall three inequalities (1.1), (1.3) and (1.4) hold with \u03c3 2 = (\u03c312 + * * * + \u03c3n2 )/n if Mn is a martingale\nwith differences Xk \u2264 1 such that the conditional variances s2k satisfy s2k \u2264 \u03c3k2 for all k.\nIt is easy to check that Bernoulli random variables \u03b5 = \u03b5(\u03c3 2 ) of type (1.2) have variance \u03c3 2\nand skewness \u03b3 related as\n\u03b3 =\u03c3\u2212\n\n1\n\u03c3\n\n,\n\n2\n\n2\n\n\u03c3 = u (\u03b3),\n\nwhere u(x) =\n\nr\n\n1+\n\nx2\n4\n\n\u2212\n\nx\n2\n\n.\n\n(1.10)\n\n\f4\n\nV. BENTKUS\n\nTheorem 1.2. Assume that the differences Xk of a martingale Mn satisfy Xk \u2264 1, and that\nthe conditional skewness gk of Xk are bounded from below by some non-random \u03b3k , that is, that\ngk \u2265 \u03b3k ,\n\nk = 1, 2, . . . , n.\n\n(1.11)\n\nThen (1.3) and (1.4) hold with Tn being a sum of n independent copies of a Bernoulli random\nvariable \u03b5 = \u03b5(\u03c3 2 ) of type (1.2) with skewness \u03b3 and variance \u03c3 2 defined by\n\u03b3=\n\n1 \u03b31 u(\u03b31 ) + * * * + \u03b3n u(\u03b3n )\np\n\u221a\nn\nu2 (\u03b31 ) + * * * + u2 (\u03b3n )\n\n,\n\nu2 (\u03b31 ) + * * * + u2 (\u03b3n )\nn\n\n\u03c32 =\n\n.\n\n(1.12)\n\nIn the special case where all \u03b3k are equal, \u03b31 = * * * = \u03b3n = \u03b3, the Bernoulli random variable has\nskewness \u03b3 and variance \u03c3 2 = u2 (\u03b3).\nIt is easy to see that Bernoulli random variables \u03b5 = \u03b5(\u03c3 2 ) of type (1.2) have variance \u03c3 2 and\nkurtosis \u03ba related as\np\n1\n(1.13)\n\u03ba = 2 \u2212 1 + \u03c3 2 , 2\u03c3 2 = \u03ba + 1 \u00b1 (\u03ba + 1)2 \u2212 4.\n\u03c3\n\nIn particular\n\u03c3 2 \u2264 v(\u03ba),\n\nwhere 2v(t) = t + 1 +\n\np\n\n(t + 1)2 \u2212 4.\n\n(1.14)\n\nTheorem 1.3. Assume that the differences Xk of a martingale Mn satisfy Xk \u2264 1, and that\nthe conditional kurtosis ck of Xk are bounded from above by some non-random \u03bak , that is, that\nck \u2264 \u03bak ,\n\nk = 1, 2, . . . , n.\n\n(1.15)\n\nThen (1.3) and (1.4) hold with Tn being a sum of n independent copies of a Bernoulli random\nvariable \u03b5 = \u03b5(\u03c3 2 ) of type (1.2) with kurtosis \u03ba and variance \u03c3 2 defined by\n\u03ba=\n\n1\n\u03c32\n\n\u2212 1 + \u03c32 ,\n\n\u03c32 =\n\nv(\u03ba1 ) + * * * + v(\u03ban )\nn\n\n,\n\n(1.16)\n\nwhere the function v is given in (1.14). In the special case where \u03ba1 = * * * = \u03ban = \u03ba, the\nBernoulli random variable has kurtosis \u03ba and variance \u03c3 2 = v(\u03ba).\nThe next Theorem 1.4 allows to combine our knowledge about variances, skewness and kurtosis. Theorems 1.2, 1.3 and (1.4), (1.4) for martingales (see Remark 1.1) are special cases of\nTheorem 1.4 setting in various combinations \u03c3k2 = \u221e, \u03b3k = \u2212\u221e, \u03bak = \u221e.\nTheorem 1.4. Assume that the differences Xk of a martingale Mn satisfy Xk \u2264 1, and that\ntheir conditional variances s2k , skewness gk and kurtosis ck satisfy\ns2k \u2264 \u03c3k2 ,\n\ngk \u2265 \u03b3k ,\n\nck \u2264 \u03bak ,\n\nk = 1, 2, . . . , n\n\n(1.17)\n\nwith some non-random s2k \u2265 0, gk \u2265 \u2212\u221e and 1 \u2264 ck \u2264 \u221e. Assume that numbers \u03b12k satisfy\n\u03b12k \u2265 min{\u03c3k2 , u2 (\u03b3k ), v(\u03bak )}.\n\n\fSKEWNESS AND CURTOSIS\n\n5\n\nThen (1.3) and (1.4) hold with Tn being a sum of n independent copies of a Bernoulli random\nvariable \u03b5 = \u03b5(\u03c3 2 ) of type (1.2) with\n\u03c32 =\n\n\u03b121 + * * * + \u03b12n\nn\n\n,\n\nwhere functions u and v are defined in (1.10) and (1.14) respectively.\nRemark 1.5. All our inequalities can be extended to the case where Mn is a supermartingale. Furthermore, their maximal versions\n\u001a hold, that is,\n\u001b in the left hand sides of these\ninequalities we can replace P {Mn \u2265 x} by P\n\nmax Mk \u2265 x .\n\n1\u2264k\u2264n\n\nRemark 1.6. One can estimate the right hand sides of our inequalities using Poisson\ndistributions. In the case of Hoeffding's functions this is done by Hoeffding 1963. In notation\nof (1.1) his bound is\nn\n\nH n (t, p) \u2264 inf e\u2212hx E eh(\u03b7\u2212\u03bb) = exp x \u2212 (x + \u03bb) ln\nh>0\n\nx+\u03bb o\n,\n\u03bb\n\n(1.18)\n\nwhere x = tn, \u03bb = n\u03c3 2 , and \u03b7 is a Poisson random variable with parameter \u03bb. It is shown in\nthe proof of Theorem 1.1 in B 2004, that if Tn is a sum of n independent copies of a Bernoulli\nrandom variable \u03b5 = \u03b5(\u03c3 2 ), then\ninf (t \u2212 h)\u22122 E (Tn \u2212 h)2+ \u2264 inf (t \u2212 h)\u22122 E (\u03b7 \u2212 \u03bb \u2212 h)2+ ,\n\nh<x\n\nh<x\n\n(1.19)\n\nwhere \u03b7 is a Poisson random variable with parameter \u03bb = n\u03c3 2 . The right hand side of (1.19) is\ngiven as an explicit function of \u03bb and x in BKZ 2006.\nRemark 1.7. A law of transformation {\u03c312 , . . . , \u03c3n2 } 7\u2192 \u03c3 2 in (1.1), (1.3) and (1.4) is a linear\nfunction. In bounds involving skewness and kurtosis corresponding transformations are nonlinear, see, for example, (1.12), where the transformation {\u03b31 , . . . , \u03b3n } 7\u2192 \u03b3 is given explicitly.\n\n2. An analytic description of transforms G2 of binomial survival functions G.\nIn this section we recall an explicit analytical description of the right hand side of (1.4)\ndef\n\nG2 (x) = inf (x \u2212 h)\u22122 E (Tn \u2212 h)2+ ,\nh<x\n\nwhere Tn is a sum of n independent copies of the Bernoulli random variable (1.2). The description\nis taken from BKZ 2006. Let G(x) = P {Tn \u2265 x} be the survival function of Tn . The probabilities p, q and the variance \u03c3 2 are defined in (1.2). Write \u03bb = pn. The sum Tn = \u03b51 + * * * + \u03b5n\nassumes the values\nds = \u2212n\u03c3 2 + s(1 + \u03c3 2 ) \u2261\n\ns\u2212\u03bb\nq\n\n,\n\ns = 0, 1, ..., n.\n\n\f6\n\nV. BENTKUS\n\nThe related probabilities satisfy\npn,s = P {Tn = ds } =\n\n\u0010 \u0011\nn\ns\n\nq n\u2212s ps .\n\nThe values G(ds ) of the survival function of the random variable Tn are given by\nG(ds ) = pn,s + * * * + pn,n .\nWrite\n\u03bdn,s =\n\nspn,s\nG(ds )\n\n.\n\nNow we can describe the transform G2 . Consider a sequence\n0 = r0 < r1 < . . . < rn\u22121 < rn = n\nof points which divide the interval [0, n] into n subintervals [rs , rs+1 ]. To define G2 take\nrs =\nand\nG2 (x) =\n\n\u03bb \u2212 p\u03bdn,s\nq\u03bdn,s + \u03bb \u2212 s\n\n,\n\ns = 0, 1, . . . , n \u2212 1,\n\n2\n\u03bb + \u03bdn,s (s \u2212 \u03bb \u2212 p) \u2212 q\u03bdn,s\n\nqx2 \u2212 2q\u03bdn,s + \u03bb + \u03bdn,s (s \u2212 \u03bb \u2212 p)\n\nG(ds ),\n\nrs \u2264 x \u2264 rs+1 .\n\n3. Proofs\nProof of Theorem 1.2. This theorem is a special case of Theorem 1.4. Indeed, choosing\n\u03c3k2 = \u221e,\n\n\u03bak = \u221e,\n\nk = 1, 2, . . . , n,\n\nwe have v(\u03bak ) = \u221e. Hence \u03b12k from the condition of Theorem 1.4 have to satisfy \u03b12k \u2265 u2 (\u03b3k ).\nWe choose \u03b12k = u2 (\u03b3k ). Then \u03c3 2 = (u2 (\u03b31 ) + * * * + u2 (\u03b3n ))/n. A small calculation shows that\n1\nwith such \u03c3 2 the skewness \u03b3 = \u03c3 \u2212\nof Bernoulli random variables \u03b5 = \u03b5(\u03c3 2 ) coincides with\n\u03c3\nthe expression given in (1.12) \u0003\nProof of Theorem 1.3. This theorem is a special case of Theorem 1.4. Indeed, choosing\n\u03c3k2 = \u221e,\n\n\u03b3k = \u2212\u221e,\n\nk = 1, 2, . . . , n,\n\nwe have u(\u03b3k ) = \u221e. Hence \u03b12k from the condition of Theorem 1.4 have to satisfy \u03b12k \u2265 v(\u03bak ).\nWe choose \u03b12k = v(\u03bak ). Then \u03c3 2 = (v(\u03ba1 ) + * * * + v(\u03ban ))/n. \u0003\nIn the proof of Theorem 1.4 we use the next two lemmas.\n\n\fSKEWNESS AND CURTOSIS\n\n7\n\nLemma 3.1. Assume that a random variable X \u2264 1 has mean E X = 0, variance s2 = E X 2 ,\nand skewness such that\n\nE X3\ns3\n\n\u2265 g. Then\n\ns2 \u2264 u2 (g),\n\ndef\n\nu(x) =\n\nr\n\n1+\n\nx2\n4\n\n\u2212\n\nx\n2\n\n.\n\n(3.1)\n\nProof. It is clear that\n(t + s2 )2 (1 \u2212 t) \u2265 0 for t \u2264 1.\n\n(3.2)\n\nReplacing in (3.2) the variable t by X and taking the expectation, we get s2 \u2212 s4 \u2265 E X 3 .\nE X3\n\nDividing by s3 and using\n\u2265 g, we derive\ns3\nthat the latter inequality implies (3.1). \u0003\n\n1\ns\n\n\u2212 s \u2265 g. Elementary considerations show\n\nLemma 3.2. Assume that a random variable X \u2264 1 has mean E X = 0, variance s2 = E X 2 ,\nand kurtosis such that\n\nE X4\ns4\n\n\u2264 c with some c \u2265 1. Then\n\ns2 \u2264 v(c),\n\ndef\n\n2v(t) = t + 1 +\n\np\n\n(t + 1)2 \u2212 4.\n\n(3.3)\n\nE X4\n\n\u2265 1. Hence, the condition c \u2265 1 is natural. The\nProof. By H\u00f6lder's inequality we have\ns4\nfunction v satisfies v(c) \u2265 1 for c \u2265 1. Therefore in cases where s2 \u2264 1, inequality (3.3) turns to\nthe trivial s2 \u2264 1 \u2264 v(c). Excluding this trivial case from the further considerations, we assume\nthat s2 > 1. Write a = 2\u03c3 2 \u2212 1. Then a \u2265 1. It is clear that\n(t + s2 )2 (1 \u2212 t)(a \u2212 t) \u2265 0 for t \u2264 1.\n\n(3.4)\n\nReplacing in (3.4) the variable t by X and taking the expectation, we get E X 4 \u2265 s2 \u2212 s4 + s6 .\nE X4\n\n\u2264 c, we derive\nDividing by s4 and using\ns4\nshow that the latter inequality implies (3.3). \u0003\n\n1\ns2\n\n\u2212 1 + s2 \u2264 c. Elementary considerations\n\nProof of Theorem 1.4. The proof starts with an application of the Chebyshev inequality\nsimilar to (1.7). This reduces the estimation of P {Mn \u2265 x} to estimation of expectations\nE exp{hMn },\n\nE (Mn \u2212 h)2+ .\n\nAs it is noted in the proof of Lemma 4.4 in B 2004, it suffices to estimate E (Mn \u2212 h)2+ since\nthe desired bound for the other expectation is implied by\nE (Mn \u2212 h)2+ \u2264 E (Tn \u2212 h)2+ .\n\n(3.5)\n\nLet us prove (3.5). By Lemma 3.1 the condition gk \u2265 \u03b3k implies s2 \u2264 u2 (\u03b3k ). While applying\nLemma 3.1 one has to replace X by Xk , etc. In a similar way, by Lemma 3.2 the condition ck \u2264 \u03bak implies s2k \u2264 v(\u03bak ). Combining the inequalities and the assumption s2k \u2264 \u03c3k2 , we\nhave\n(3.6)\ns2k \u2264 min{\u03c3k2 , u2 (\u03b3k ), v(\u03bak )}.\nThe inequality (3.6) together with the condition of the theorem yields s2k \u2264 \u03b12k . As it is shown\nin the proof of Theorem 1.1 in B 2004, the latter inequality implies (3.5). \u0003\n\n\f8\n\nV. BENTKUS\n\nReferences\n[B] Bentkus, V., On measure concentration for separately Lipschitz functions in product spaces, Israel. J. Math.\n158 (2007), 1\u201317.\n[B] Bentkus, V., On Hoeffding's inequalities, Ann. Probab. 32 (2004), no. 2, 1650\u20131673.\n[B] Bentkus, V., An inequality for tail probabilities of martingales with differences bounded from one side 16\n(2003), no. 1, 161\u2013173.\n[B] Bentkus, V., A remark on the inequalities of Bernstein, Prokhorov, Bennett, Hoeffding, and Talagrand, Lith.\nMath. J. 42 (2002), no. 3, 262\u2013269.\n[B] Bentkus, V., An inequality for tail probabilities of martingales with bounded differences, Lith. Math. J. 42\n(2002), no. 3, 255\u2013261.\n[B] Bentkus, V., An inequality for large deviation probabilities of sums of bounded i.i.d. random variables, Lith.\nMath. J. 41 (2001), no. 2, 112\u2013119.\n[BGZ] Bentkus, V., Geuze, G.D.C., and van Zuijlen, M., Optimal Hoeffding-like inequalities under a symmetry\nassumption, Statistics 40 (2006), no. 2, 159\u2013164.\n[BGZ] Bentkus, V., Geuze, G.D.C., and van Zuijlen, M., Unimodality: The linear case, Report no. 0607 of Dept.\nof Math. Radboud University Nijmegen (2006), 1\u201311.\n[BGZ] Bentkus, V., Geuze, G.D.C., and van Zuijlen, M., Unimodality: The general case, Report no. 0608 of\nDept. of Math. Radboud University Nijmegen (2006), 1\u201324.\n[BGPZ] Bentkus, V., Geuze, G.D.C., Pinenberg, M.G.F., and van Zuijlen, M., Unimodality: The symmetric case,\nReport no. 0612 of Dept. of Math. Radboud University Nijmegen (2006), 1\u201312.\n[BKZ] Bentkus, V., N. Kalosha, and van Zuijlen, M., Confidence bounds for the mean in nonparametric multisample problems, Statist. Neerlandica 61 (2007), no. 2, 209\u2013231.\n[BKZ] Bentkus, V., N. Kalosha, and van Zuijlen, M., On domination of tail probabilities of (super)martingales:\nexplicit bounds, Lith. Math. J. 46 (2006), no. 1, 1\u201343.\n[BZ] Bentkus, V., and van Zuijlen, M., On conservative confidence intervals, Lith. Math. J. 43 (2003), no. 2,\n141\u2013160.\nvan de Geer, S. A., On Hoeffding's inequalities for dependent random variables, Empirical process techniques\nfor dependent data, Birkhauser Boston, Boston, MA Contemp. Math., 234, Am. Math. Soc., Providence, RI,\n2002, pp. 161\u2013169.\nGodbole, A., and Hitczenko, P., Beyond the method of bounded differences, Microsurveys in discrete probability\n(Princeton, NJ, 1997), Dimacs Ser Discrete Math. Theoret. Comput. Sci., Amer. Math. Soc., Providence, RI\n41 (1998), 43\u201358.\nHoeffding, W., Probability inequalities for sums of bounded random variables, J. Am. Statist. Assoc. 58 (1963),\n13\u201330.\nLaib, N., Exponential-type inequalities for martingale difference sequences. Application to nonparametric regression estimation, Comm. Statist. Theory Methods 28 (1999), 1565\u20131576.\nMcDiarmid, C., On the method of bounded differences, Surveys in combinatorics, 1989 (Norwich 1989), London\nMath. Soc. Lecture Note Ser., vol. 141, 1989, pp. 148\u2013188.\nPerron, F., Extremal properties of sums of Bernoulli random variables, Statist. Probab. Lett. 62 (2003), 345\u2013354.\nPinelis, I., Toward the best constant factor for the Rademacher-Gaussian tail comparison, ESAIM Probab. Stat.\n11 (2007), 412\u2013426.\nPinelis, I., Inequalities for sums of asymmetric random variables, with applications, Probab. Theory Related\nFields 139 (2007), no. 3\u20134, 605\u2013635.\nPinelis, I., On normal domination of (super)martingales, Electron. J. Prabab 11 (2006), no. 39, 1049\u20131070.\nPinelis, I., Fractional sums and integrals of r-concave tails and applications to comparison probability inequalities,\nAdvances in stochastic inequalities (Atlanta, GA, 1997), Contemp. Math., 234, Am. Math. Soc., Providence,\nRI, 1999, pp. 149\u2013168.\nPinelis, I., Optimal tail comparison based on comparison of moments, High dimensional probability (Oberwolfach,\n1996), Progr. Probab., 43, Birkh\u00e4user, Basel,, 1998, pp. 297\u2013314.\nTalagrand, M., The missing factor in Hoeffding's inequalities, Ann. Inst. H. Poincar\u00e9 Probab. Statist. 31 (1995),\nno. 4, 689\u2013702.\nVilnius institute of mathematics and informatics, Akademijos 4, LT-08663 Vilnius\nE-mail address: bentkus@ktl.mii.lt, ?????????????\n\n\f"}