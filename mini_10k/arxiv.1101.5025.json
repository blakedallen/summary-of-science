{"id": "http://arxiv.org/abs/1101.5025v1", "guidislink": true, "updated": "2011-01-26T11:19:36Z", "updated_parsed": [2011, 1, 26, 11, 19, 36, 2, 26, 0], "published": "2011-01-26T11:19:36Z", "published_parsed": [2011, 1, 26, 11, 19, 36, 2, 26, 0], "title": "Order Statistics Based List Decoding Techniques for Linear Binary Block\n  Codes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1101.2116%2C1101.1612%2C1101.0620%2C1101.4705%2C1101.0942%2C1101.2731%2C1101.6051%2C1101.0817%2C1101.0377%2C1101.3469%2C1101.4919%2C1101.5918%2C1101.1623%2C1101.1847%2C1101.4846%2C1101.5597%2C1101.1690%2C1101.5258%2C1101.1182%2C1101.4563%2C1101.2796%2C1101.5033%2C1101.5763%2C1101.1354%2C1101.3471%2C1101.5440%2C1101.0404%2C1101.2106%2C1101.4730%2C1101.0027%2C1101.3373%2C1101.2747%2C1101.5349%2C1101.3137%2C1101.4326%2C1101.0816%2C1101.1969%2C1101.1355%2C1101.2453%2C1101.0444%2C1101.4274%2C1101.4239%2C1101.3389%2C1101.0086%2C1101.4067%2C1101.2193%2C1101.0067%2C1101.4743%2C1101.1643%2C1101.2521%2C1101.0064%2C1101.1181%2C1101.5417%2C1101.0018%2C1101.1821%2C1101.1831%2C1101.0366%2C1101.1516%2C1101.3943%2C1101.5904%2C1101.3453%2C1101.0626%2C1101.3981%2C1101.5851%2C1101.1449%2C1101.2132%2C1101.3311%2C1101.1061%2C1101.4892%2C1101.3392%2C1101.0659%2C1101.5025%2C1101.0968%2C1101.0613%2C1101.5675%2C1101.4228%2C1101.2803%2C1101.1835%2C1101.1515%2C1101.4419%2C1101.2930%2C1101.5382%2C1101.0775%2C1101.0260%2C1101.4158%2C1101.0425%2C1101.2626%2C1101.2262%2C1101.2329%2C1101.0217%2C1101.2816%2C1101.2829%2C1101.2683%2C1101.0765%2C1101.4058%2C1101.2703%2C1101.4453%2C1101.2752%2C1101.1241%2C1101.3481%2C1101.5278&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Order Statistics Based List Decoding Techniques for Linear Binary Block\n  Codes"}, "summary": "The order statistics based list decoding techniques for linear binary block\ncodes of small to medium block length are investigated. The construction of the\nlist of the test error patterns is considered. The original order statistics\ndecoding is generalized by assuming segmentation of the most reliable\nindependent positions of the received bits. The segmentation is shown to\novercome several drawbacks of the original order statistics decoding. The\ncomplexity of the order statistics based decoding is further reduced by\nassuming a partial ordering of the received bits in order to avoid the complex\nGauss elimination. The probability of the test error patterns in the decoding\nlist is derived. The bit error rate performance and the decoding complexity\ntrade-off of the proposed decoding algorithms is studied by computer\nsimulations. Numerical examples show that, in some cases, the proposed decoding\nschemes are superior to the original order statistics decoding in terms of both\nthe bit error rate performance as well as the decoding complexity.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1101.2116%2C1101.1612%2C1101.0620%2C1101.4705%2C1101.0942%2C1101.2731%2C1101.6051%2C1101.0817%2C1101.0377%2C1101.3469%2C1101.4919%2C1101.5918%2C1101.1623%2C1101.1847%2C1101.4846%2C1101.5597%2C1101.1690%2C1101.5258%2C1101.1182%2C1101.4563%2C1101.2796%2C1101.5033%2C1101.5763%2C1101.1354%2C1101.3471%2C1101.5440%2C1101.0404%2C1101.2106%2C1101.4730%2C1101.0027%2C1101.3373%2C1101.2747%2C1101.5349%2C1101.3137%2C1101.4326%2C1101.0816%2C1101.1969%2C1101.1355%2C1101.2453%2C1101.0444%2C1101.4274%2C1101.4239%2C1101.3389%2C1101.0086%2C1101.4067%2C1101.2193%2C1101.0067%2C1101.4743%2C1101.1643%2C1101.2521%2C1101.0064%2C1101.1181%2C1101.5417%2C1101.0018%2C1101.1821%2C1101.1831%2C1101.0366%2C1101.1516%2C1101.3943%2C1101.5904%2C1101.3453%2C1101.0626%2C1101.3981%2C1101.5851%2C1101.1449%2C1101.2132%2C1101.3311%2C1101.1061%2C1101.4892%2C1101.3392%2C1101.0659%2C1101.5025%2C1101.0968%2C1101.0613%2C1101.5675%2C1101.4228%2C1101.2803%2C1101.1835%2C1101.1515%2C1101.4419%2C1101.2930%2C1101.5382%2C1101.0775%2C1101.0260%2C1101.4158%2C1101.0425%2C1101.2626%2C1101.2262%2C1101.2329%2C1101.0217%2C1101.2816%2C1101.2829%2C1101.2683%2C1101.0765%2C1101.4058%2C1101.2703%2C1101.4453%2C1101.2752%2C1101.1241%2C1101.3481%2C1101.5278&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The order statistics based list decoding techniques for linear binary block\ncodes of small to medium block length are investigated. The construction of the\nlist of the test error patterns is considered. The original order statistics\ndecoding is generalized by assuming segmentation of the most reliable\nindependent positions of the received bits. The segmentation is shown to\novercome several drawbacks of the original order statistics decoding. The\ncomplexity of the order statistics based decoding is further reduced by\nassuming a partial ordering of the received bits in order to avoid the complex\nGauss elimination. The probability of the test error patterns in the decoding\nlist is derived. The bit error rate performance and the decoding complexity\ntrade-off of the proposed decoding algorithms is studied by computer\nsimulations. Numerical examples show that, in some cases, the proposed decoding\nschemes are superior to the original order statistics decoding in terms of both\nthe bit error rate performance as well as the decoding complexity."}, "authors": ["Saif E. A. Alnawayseh", "Pavel Loskot"], "author_detail": {"name": "Pavel Loskot"}, "author": "Pavel Loskot", "arxiv_comment": "17 pages, 2 tables, 6 figures, submitted to IEEE Transactions on\n  Information Theory", "links": [{"href": "http://arxiv.org/abs/1101.5025v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1101.5025v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1101.5025v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1101.5025v1", "journal_reference": null, "doi": null, "fulltext": "Order Statistics Based List Decoding\nTechniques for Linear Binary Block Codes\n\narXiv:1101.5025v1 [cs.IT] 26 Jan 2011\n\nSaif E. A. Alnawayseh, and Pavel Loskot, Member, IEEE\n\nAbstract\nThe order statistics based list decoding techniques for linear binary block codes of small to medium\nblock length are investigated. The construction of the list of the test error patterns is considered.\nThe original order statistics decoding is generalized by assuming segmentation of the most reliable\nindependent positions of the received bits. The segmentation is shown to overcome several drawbacks\nof the original order statistics decoding. The complexity of the order statistics based decoding is further\nreduced by assuming a partial ordering of the received bits in order to avoid the complex Gauss\nelimination. The probability of the test error patterns in the decoding list is derived. The bit error\nrate performance and the decoding complexity trade-off of the proposed decoding algorithms is studied\nby computer simulations. Numerical examples show that, in some cases, the proposed decoding schemes\nare superior to the original order statistics decoding in terms of both the bit error rate performance as\nwell as the decoding complexity.\n\nIndex Terms\nDecoding, fading, linear code, performance evaluation.\n\nThe authors are with College of Engineering, Swansea University, Singleton Park, Swansea SA2 8PP, United Kingdom (email:\n{491959,p.loskot}@swan.ac.uk)\nCorresponding author: Pavel Loskot, phone: +44 1792 602619, fax: +44 1792 295676\nThis work was presented in part at the IEEE Wireless Communications & Signal Processing Conference (WCSP), Nanjing,\nChina, Nov. 2009.\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n1\n\nI. I NTRODUCTION\nA major difficulty in employing forward error correction (FEC) coding is the implementation\ncomplexity especially of the decoding at the receiver and the associated decoding latency for\nlong codewords. Correspondingly, the FEC coding is often designed to trade-off the bit error rate\n(BER) with the decoding complexity and latency. Many universal decoding algorithms have been\nproposed for the decoding linear binary block codes [1]. The decoding algorithms in [3]\u2013 [11]\nare based on the testing and re-encoding of the information bits as initially considered by Dorsch\nin [2]. In particular, a list of the likely transmitted codewords is generated using the reliabilities\nof the received bits, and then, the most likely codeword is selected from this list. The list of the\nlikely transmitted codewords can be constructed from a set of the test error patterns. The test\nerror patterns can be predefined as in [3] and [4] and in this paper, predefined and optimized\nfor the channel statistics as in [5], or defined adaptively for a particular received sequence as\nsuggested in [6]. The complexity of the list decoding can be further reduced by the skipping\nand stopping rules as shown, for example, in [3] and [4].\nAmong numerous variants of the list decoding techniques, the order statistics decoding (OSD)\nis well-known [3], [4]. The structural properties of the FEC code are utilized to reduce the\nOSD complexity in [7]. The achievable coding gain of the OSD is improved by considering\nthe multiple information sets in [8]. The decoding proposed in [9] exploits randomly generated\nbiases to present the decoder with the multiple received soft-decision values. The sort and match\ndecoding of [10] divides the received sequence into two disjoint segments. The list decoding\nis then performed for each of the two segments independently, and the two lists are combined\nusing the sort and match algorithm to decide on the most likely transmitted codeword. The box\nand match decoding strategy is developed in [11]. An alternative approach to the soft-decision\ndecoding of linear binary block codes relies on the sphere decoding techniques [12], [13]. For\nexample, the input sphere decoder (ISD) discussed in this paper can be considered to be a trivial\nsphere decoding algorithm.\nIn this paper, we investigate the OSD-based decoding strategies for linear binary block codes.\nOur aim is to obtain low-complexity decoding schemes that provide sufficiently large or valuable\ncoding gains, and most importantly, that are well-suited for implementation in communication\nsystems with limited hardware resources, e.g., at nodes of the wireless sensor network. We\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n2\n\nmodify the original OSD by considering the disjoint segments of the most reliable independent\npositions (MRIPs). The segmentation of the MRIPs creates flexibility that can be exploited to\nfine tune a trade-off between the BER performance and the decoding complexity. Thus, the\noriginal OSD can be considered to be a special case of the segmentation-based OSD having\nonly one segment corresponding to the MRIPs. Since the complexity of obtaining a row echelon\nform of the generator matrix for every received codeword represents a significant part of the\noverall decoding complexity, we examine a partial-order statistics decoding (POSD) when only\nthe systematic part of the received codeword is ordered.\nThis paper is organized as follows. System model is described in Section II. Construction of the\nlist of test error patterns is investigated in Section III. The list decoding algorithms are developed\nin Section IV. The performance analysis is considered in Section V. Numerical examples to\ncompare the BER performance and the decoding complexity of the proposed decoding schemes\nare presented in Section VI. Finally, conclusions are given in Section VII.\nII. S YSTEM M ODEL\nConsider transmission of codewords of a linear binary block code C over an additive white\nGaussian noise (AWGN) channel with Rayleigh fading. The code C, denoted as (N, K, dmin), has\nblock length N, dimension K, and the minimum Hamming distance between any two codewords\ndmin. Binary codewords c \u2208 ZN\n2 where Z2 = {0, 1} are generated from a vector of information\nK\u00d7N\n, i.e., c = uG, and all binary operations\nbits u \u2208 ZK\n2 using the generator matrix G \u2208 Z2\n\nare considered over a Galois field GF(2). If the code C is systematic, the generator matrix\nK\u00d7(N \u2212K)\n\nhas the form, G = [I P], where I is the K \u00d7 K identity matrix, and P \u2208 Z2\n\nis the\n\nmatrix of parity checks. The codeword c is mapped to a binary phase shift keying (BPSK)\nsequence x \u2208 {+1, \u22121}N before transmission using a mapping, xi = M (ci ) = (\u22121)ci , for\ni = 1, 2, * * * , N. Assuming bits ui and uj , the mapping M has the property,\nM (ui \u2295 uj ) = M (ui ) M (uj )\n\n(1)\n\nwhere \u2295 denotes the modulo 2 addition. The encoded bit ci can be recovered from the symbol\nxi using the inverse mapping, ci = M\u22121 (xi ) = (1 \u2212 xi )/2. For brevity, we also use the notation,\nx = M (c) and c = M\u22121 (x), to denote the component-wise modulation mapping and demapping, respectively. The code C is assumed to have equally probable values of the encoded\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n3\n\nbits, i.e., the probability, Pr{ci = 0} = Pr{ci = 1} = 1/2, for i = 1, 2, * * * , N. Consequently,\nall the codewords are transmitted with the equal probability, i.e., Pr{c} = 2\u2212K for \u2200c \u2208 C.\nThe signal at the output of the matched filter at the receiver can be written as,\nyi = hi xi + wi\nwhere the frequency non-selective channel fading coefficients hi as well as the AWGN samples wi\nare mutually uncorrelated zero-mean circularly symmetric complex Gaussian random variables.\nThe variance of hi is unity, i.e., E[|hi |2 ] = 1 where E[*] is expectation, and | * | denotes the\nabsolute value. The samples wi have the variance, E[|wi |2 ] = (R\u03b3c )\u22121 , where R = K/N is\nthe coding rate of C, and \u03b3c is the signal-to-noise ratio (SNR) per transmitted encoded binary\n\u0002\n\u0003\nsymbol. The covariance, E hi h\u2217j = 0 for i 6= j, where (*)\u2217 denotes the complex conjugate,\n\ncorresponds to the case of a fast fading channel with ideal interleaving and deinterleaving. For\n\u0002\n\u0003\na slowly block-fading channel, the covariance, E hi h\u2217j = 1 for \u2200i, j = 1, 2, * * * , N, and the\n\nfading coefficients are uncorrelated between transmissions of adjacent codewords.\n\nIn general, denote as f (*) the probability density function (PDF) of a random variable. The\nreliability ri of the received signal yi corresponds to a ratio of the conditional PDFs of yi [14],\ni.e.,\nf (yi |xi = +1, hi )\n\u221d Re{h\u2217i yi } = ri\nf (yi |xi = \u22121, hi )\nsince the PDF f (yi |xi , hi ) is conditionally Gaussian. Thus, the reliability ri can be written as,\nri = Re{hi } Re{yi } + Im{hi } Im{yi } = |hi |2 xi + |hi |wi .\nThe bit-by-bit quantized (i.e., hard) decisions are then defined as,\n\u0109i = M\u22121 (sign(ri ))\nwhere sign(*) denotes the sign of a real number.\nMore importantly, even though the primary metric of our interest is the BER performance\nof the code C, it is mathematically more convenient to obtain and analyze the list decoding\nalgorithms assuming the probability of codeword error. Thus, we assume that the list decoding\nwith a given decoding complexity obtained for the probability of codeword error will also have\na good BER performance. The maximum likelihood (ML) decoder minimizing the probability\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n4\n\nof codeword error provides the decision \u0109ML on the most likely transmitted codeword, i.e.,\n\u0109ML =\n\nargmin ky \u2212 h \u2299 xk2\nc\u2208C: x=M(c)\n\n= argmax\nc\u2208C\n\nN\nX\n\nBPSK\n\nRe{yi h\u2217i x\u2217i } =\n\ni=1\n\nargmax\n\nr*x\n\n(2)\n\nc\u2208C: x=M(c)\n\nwhere y, h, x, and r denote the N-dimensional row vectors of the received signals yi , the\nchannel coefficients hi , the transmitted symbols xi , and the reliabilities ri within one codeword,\nrespectively, k*k is the Euclidean norm of a vector, \u2299 is the component-wise (Hadamard) product\nof vectors, and the binary operator * is used to denote the dot-product of vectors. The codewords\nc \u2208 C used in (2) to find the maximum or the minimum value of the ML metric are often referred\nto as the test codewords. In the following subsection, we investigate the soft-decision decoding\nalgorithms with low implementation complexity to replace the computationally demanding ML\ndecoding (2).\nA. List Decoding\nWe investigate the list-based decoding algorithms. For simplicity, we assume binary block\ncodes that are linear and systematic [15]. We note that whereas the extension of the list-based\ndecoding algorithms to non-systematic codes is straightforward, the list based decoding of nonlinear codes is complicated by the fact that the list of the test codewords is, in general, dependent\non the received sequence. The decoding (time) complexity O of the list decoding algorithms\ncan be measured as the list size given by the number of the test codewords that are examined\nin the decoding process. Thus, the ML decoding (2) has the complexity, OML = 2K , which is\nprohibitive for larger values of K. Among the practical list-based decoding algorithms with the\nacceptable decoding complexity, we investigate the order statistics decoding (OSD) [3] based\nlist decoding algorithms for soft-decision decoding of linear binary block codes.\nThe OSD decoding resumes by reordering the received sequence of reliabilities as,\n\u2032\n|r\u03031\u2032 | \u2265 |r\u03032\u2032 | \u2265 * * * |r\u0303N\n|\n\n(3)\n\nwhere the tilde is used to denote the ordering. This ordering of the reliabilities defines a\npermutation, \u03bb\u2032 , i.e.,\n\u2032\nr\u0303\u2032 = \u03bb\u2032 [r] = (r\u03031\u2032 , * * * , r\u0303N\n).\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n5\n\nThe permutation \u03bb\u2032 corresponds to the generator matrix G\u0303\u2032 = \u03bb\u2032 [G] having the reordered\ncolumns. In order to obtain the most reliable independent positions (MRIPs) for the first K\nbits in the codeword, additional swapping of the columns of G\u0303\u2032 may have to be used which\nh i\n\u2032\u2032\n\u2032\u2032\n\u2032\u2032\ncorresponds to the permutation \u03bb , and the generator matrix G\u0303 = \u03bb G\u0303\u2032 . The matrix G\u0303\u2032\u2032 can\nbe manipulated into a row (or a reduced row) echelon form using the Gauss (or the Gauss-\n\nJordan) elimination. To simplify the notation, let r\u0303 and G\u0303 denote the reordered sequence of the\nreliabilities r and the reordered generator matrix G\u0303 in a row (or a reduced row) echelon form,\nrespectively, after employing the permutations \u03bb\u2032 and \u03bb\u2032\u2032 , to decode the received sequence y.\nThus, for i \u2265 j, the reordered sequence r\u0303 has elements, |r\u0303i | \u2265 |r\u0303j |, for i, j = 1, * * * , K, and for\ni, j = K + 1, * * * , N.\nThe complexity of the ML decoding (2) of the received sequence y can be reduced by assuming\na list of the L test codewords, so that L \u226a 2K . Denote such a list of the test codewords of\ncardinality L generated by the matrix G\u0303 as, EL = {e0 , e2 , * * * , eL\u2212 1 }, and let e0 = 0 be the\nall-zero codeword. Then, the list decoding of y is defined as,\n\u0109 =\n\nargmax\n\nr\u0303 * x\n\n(4)\n\ne\u2208EL : x=M(\u01090 \u2295e)\n\nwhere the systematic part of the codeword \u01090 is given by the hard-decision decoded bits at\nthe MRIPs. The decoding step to obtain the decision \u01090 is referred to as the 0-th order OSD\nreprocessing in [3]. In addition, due to linearity of C, we have that (c0 \u2295 e) \u2208 C, and thus, the\ntest codewords e \u2208 EL can be also referred to as the test error patterns in the decoding (4).\nUsing the property (1), we can rewrite the decoding (4) as,\n\u0109 = argmax r\u0303 * x\u03020 * M (e) = argmax r\u03030 * M (e)\ne\u2208EL\n\n(5)\n\ne\u2208EL\n\nwhere we denoted x\u03020 = M (\u01090 ) and r\u03030 = r\u0303 \u2299 x\u03020 . The system model employing the list decoding\n(5) is illustrated in Fig. 1. More importantly, as indicated in Fig. 1, the system model can be\nrepresented as an equivalent channel with the binary vector input c and the vector soft-output\nr\u03030 .\nIII. L IST S ELECTION\nThe selection of the test error patterns e to the list EL as well as the list size L have a\ndominant effect upon the probability of incorrect codeword decision by the list decoding. Denote\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n6\n\nsuch probability of codeword error as Pe , and let cTx be the transmitted codeword. In [12], the\nprobability Pe is expanded as,\nPe = Pr{\u0109 6= cTx |\u0109ML 6= cTx } Pr{\u0109ML 6= cTx }\n+Pr{\u0109 6= cTx |\u0109ML = cTx } Pr{\u0109ML = cTx }\nwhere the decision \u0109 is obtained by the decoding (5), and the condition, \u0109ML 6= cTx , is true\nprovided that the vectors \u0109ML and cTx differ in at least one component, i.e., \u0109ML = cTx if\nand only if all the components of the vectors are equal. Since, for any list EL , the probability,\nPr{\u0109 6= cTx |\u0109ML 6= cTx } = 1, and usually, the probability, Pr{\u0109ML = cTx } is close to 1, Pe can\nbe tightly upper-bounded as,\nPe \u2264 Pr{\u0109ML 6= cTx } + Pr{\u0109 6= cTx |\u0109ML = cTx } .\n\n(6)\n\nThe first term on the right hand side of (6) is the codeword error probability of the ML decoding,\nand the second term is the conditional codeword error probability of the list decoding. The\nprobability, Pr{\u0109 6= cTx |\u0109ML = cTx }, is decreasing with the list size. In the limit of the maximum\nlist size when the list decoding becomes the ML decoding, the bound (6) becomes, Pe =\nPr{\u0109ML 6= cTx }. The bound (6) is particularly useful to analyze the performance of the list\ndecoding (5). However, in order to construct the list of the test error patterns, we consider the\nfollowing expansion of the probability Pe , i.e.,\nPe = Pr{\u0109 6= cTx |(cTx \u2295 \u01090 ) \u2208 EL } Pr{(cTx \u2295 \u01090 ) \u2208 EL }\n+Pr{\u0109 6= cTx |(cTx \u2295 \u01090 ) 6\u2208 EL } Pr{(cTx \u2295 \u01090 ) 6\u2208 EL }\n= 1 \u2212 Pr{\u0109 = cTx |(cTx \u2295 \u01090 ) \u2208 EL } Pr{(cTx \u2295 \u01090 ) \u2208 EL } .\n|\n{z\n}|\n{z\n}\nPI\n\nPII\n\nUsing (4) and (5), the probability PI that the list decoding (5) selects the transmitted codeword\nprovided that such codeword is in the list (more precisely, provided that the error pattern cTx \u2295\u01090\nis in the list) can be expressed as,\nPI = Pr{r\u0303 * M (cTx \u2295 \u01090 ) \u2265 r\u03030 * M (e) , \u2200e \u2208 EL } .\n\n(7)\n\nThe probability (7) decreases with the list size, and, in the limit of the maximum list size\nL = 2K , PI = 1 \u2212 Pe . On the other hand, the probability PII that the transmitted codeword is\nin the decoding list increases with the list size, and PII = 1, for L = 2K .\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n7\n\nSince the coding C and the communication channel are linear, then, without loss of generality,\nwe can assume that the all-zero codeword, cTx = 0, is transmitted. Consequently, given the list\ndecoding complexity L, the optimum list EL\u2217 minimizing the probability Pe is constructed as,\nEL\u2217 = argmax Pr{\u0109 = 0|\u01090 \u2208 E} Pr{\u01090 \u2208 E}\n\n(8)\n\nE: |E|=L\n\nwhere |E| is the cardinality of the test list E, and the hard-decision codeword \u01090 \u2208 C represents\nthe error pattern observed at the receiver after transmission of the codeword cTx = 0. For a given\nlist of the error patterns E in (8), and for the system model in Section II with asymptotically large\nSNR, the probability PI = Pr{\u0109 = 0|\u01090 \u2208 E} is dominated by the error events corresponding\nto the error patterns with the smallest Hamming distances. Since the error patterns are also\ncodewords of C, the smallest Hamming distance between any two error patterns in the list\nE is at least dmin . Assuming that the search in (8) is constrained to the lists E having the\nminimum Hamming distance between any two error patterns given by dmin , the probability PI is\napproximately constant for all the lists E, and we can consider the suboptimum list construction,\nEL = argmax Pr{\u01090 \u2208 E} .\n\n(9)\n\nE: |E|=L\n\nThe list construction (9) is recursive in its nature, since the list E maximizing (9) consists of the\nL most probable error patterns. However, in order to achieve a small probability of decoding\nerror Pe and approach the probability of decoding error, Pr{\u0109ML 6= cTx }, of the ML decoding,\nthe list size L must be large. We can obtain a practical list construction by assuming the L\nsufficiently probable error patterns rather than assuming the L most likely error patterns. We\nrestate Theorem 1 and Theorem 2 in [3] to obtain the likely error patterns and to define the\npractical list decoding algorithms.\nDenote as P(i1 , i2 , * * * , in ) the n-th order joint probability of bit errors at bit positions 1 \u2264\ni1 < i2 < * * * < in \u2264 N in the received codeword after the ordering \u03bb\u2032 and \u03bb\u2032\u2032 and before the\ndecoding. Since the test error pattern e is a codeword of C, the probability P(i1 , i2 , * * * , in ), for\nin \u2264 K, is equal to the probability Pr{e = \u01090 } assuming that the n bit errors occurred during\nthe transmission corresponding to the positions (after the ordering) i1 , i2 , * * * , in . We have the\nfollowing lemma.\nLemma 1: For any bit positions I1 \u2286 I \u2286 {1, 2, * * * , N},\nP(I) \u2264 P(I1 ).\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n8\n\nProof: The lemma is proved by noting that P(I) = P(I1 , I\\I1 ) = P(I\\I1 |I1 )P(I1 ) \u2264\nmin{P(I1 ), P(I\\I1 |I1 )} \u2264 P(I1 ) where I\\I1 denotes the difference of the two sets.\nUsing Lemma 1, we can show, for example, that, P(i, j) \u2264 P(i) and P(i, j) \u2264 P(j). We can\nnow restate Theorem 1 and Theorem 2 in [3] as follows.\nTheorem 1: Assume bit positions 1 \u2264 i < j < k \u2264 N, and let the corresponding reliabilities\nbe |r\u0303i | \u2265 |r\u0303j | \u2265 |r\u0303k |. Then, the bit error probabilities,\nP(i) \u2264 P(j)\nP(i, j) \u2264 P(i, k).\nProof: Without loss of generality, we assume that the symbols xi = \u22121, xj = \u22121 and\nxk = \u22121 have been transmitted. Then, before the decoding, the received bits would be decided\nerroneously if the reliabilities r\u0303i > 0, r\u0303j > 0, and r\u0303k > 0. Conditioned on the transmitted\nsymbols, let f (*) denote the conditional PDF of the ordered reliabilities r\u0303i , r\u0303j and r\u0303k .\nConsider first the inequality P(i) \u2264 P(j). Since, for r\u0303i > 0, f (r\u0303i ) < f (\u2212r\u0303i ), using f (r\u0303i , r\u0303j ) =\nf (r\u0303i |r\u0303j )f (r\u0303j ), we can show that, for r\u0303i > 0 and any r\u0303j , f (r\u0303i , r\u0303j ) < f (\u2212r\u0303i , r\u0303j ). Similarly,\nusing f (\u2212r\u0303i , r\u0303j ) = f (r\u0303j | \u2212 r\u0303i )f (\u2212r\u0303i ), we can show that, for r\u0303j > 0 and any r\u0303i , f (\u2212r\u0303i , r\u0303j ) <\nf (\u2212r\u0303i , \u2212r\u0303j ). Then, the probability of error for bits i and j, respectively, is,\nZ \u221e Z r\u0303i\nP(i) =\nf (r\u0303i , r\u0303j )dr\u0303j dr\u0303i\n0\n\n=\n\nZ\n\n\u221e\n\nZ\n\n\u221e\n\n0\n\nP(j) =\n=\n\n\u2212r\u0303i\nr\u0303i\n\nZ\n\nf (r\u0303i , r\u0303j )dr\u0303j dr\u0303i +\n\n0\n\nZ\n\n0\n\n\u221e\n0\n\nZ\n\n0\n\nf (r\u0303i , r\u0303j )dr\u0303j dr\u0303i\n\u2212r\u0303i\n\n|r\u0303i |\n\n\u2212\u221e 0\nZ \u221e Z r\u0303i\n0\n\nZ\n\nf (r\u0303i , r\u0303j )dr\u0303j dr\u0303i\nf (r\u0303i , r\u0303j )dr\u0303j dr\u0303i +\n\nZ\n\n\u221e\n0\n\nZ\n\n0\n\nf (\u2212r\u0303i , \u2212r\u0303j )dr\u0303j dr\u0303i\n\u2212r\u0303i\n\nand thus, P(i) \u2264 P(j).\nThe second inequality, P(i, j) \u2264 P(i, k), can be proved by assuming conditioning, P(i, j) =\nP(j|i)P(i), P(i, k) = P(k|i)P(i), and f (r\u0303i , r\u0303j , r\u0303k ) = f (r\u0303j , r\u0303k |r\u0303i )f (r\u0303i ), and by using inequality\nP(i) \u2264 P(j), and following the steps in the first part of the theorem proof.\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n9\n\nIV. L IST D ECODING A LGORITHMS\nUsing Theorem 1 and Theorem 2 in [3], the original OSD assumes the following list of error\npatterns,\nEL = {eG : 0 \u2264 wH [e] \u2264 I, e \u2208 ZK\n2 }\n\n(10)\n\nwhere I is the so-called reprocessing order of the OSD, and wH [e] is the Hamming weight of\nthe vector e. The list (10) uses a K-dimensional sphere of radius I defined about the origin\nPI K \u0001\nwhere l is\n0 = (0, * * * , 0) in ZK\n2 . The decoding complexity for the list (10) is L =\nl=0 l\n\nreferred to as the phase of the order I reprocessing in [3]. Assuming an AWGN channel, the\nrecommended reprocessing order is I = \u2308dmin /4\u2309 \u226a K where \u2308*\u2309 is the ceiling function. Since\nthe OSD algorithm may become too complex for larger values of I and K, a stopping criterion\nfor searching the list EL was developed in [7].\n\nWe can identify the following inefficiencies of the original OSD algorithm. First, provided\nthat no stopping nor skipping rules for searching the list of the test error patterns are used,\nonce the MRIPs are found, the ordering of bits within the MRIPs according to their reliabilities\nbecomes irrelevant. Second, whereas the BER performance of the OSD is modestly improving\nwith the reprocessing order I, the complexity of the OSD increases rapidly with I [7]. Thus,\nfor given K, the maximum value of I is limited by the allowable OSD complexity to achieve\na certain target BER performance. We can address the inefficiencies of the original OSD by\nmore carefully exploiting the properties of the probability of bit errors given by Lemma 1 and\nTheorem 1. Hence, our aim is to construct a well-defined list of the test error patterns without\nconsidering the stopping and the skipping criteria to search this list.\nRecall that the error patterns can be uniquely specified by bits in the MRIPs whereas the bits\nof the error patterns outside the MRIPs are obtained using the parity check matrix. In order to\ndesign a list of the test error patterns independently of the particular generator matrix of the\ncode as well as independently of the particular received sequence, we consider only the bit errors\nwithin the MRIPs. Thus, we can assume that, for all error patterns, the bit errors outside the\nMRIPs affect the value of the metric in (5) equally. More importantly, in order to improve the\nlist decoding complexity and the BER performance trade-off, we consider partitioning of the\nMRIPs into disjoint segments. This decoding strategy employing the segments of the MRIPs is\ninvestigated next.\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n10\n\nA. Segmentation-Based OSD\nAssuming Q disjoint segments of the MRIPs, the error pattern e corresponding to the K\nMRIPs can be expressed as a concatenation of the Q error patterns e(q) of length Kq bits,\nq = 1, * * * , Q, i.e.,\ne = (e(1) , * * * , e(Q) ) \u2208 ZK\n2\nso that\n\nPQ\n\nq=1\n\n\u0002 \u0003\n\u0002\n\u0003\nKq = K, and wH [e] = wH e(1) + * * * + wH e(Q) . As indicated by Lemma 1\n\nand Theorem 1, more likely error patterns have smaller Hamming weights and they correct\n\nthe bit positions with smaller reliabilities. In addition, the decoding complexity given by the\ntotal number of error patterns in the list should grow linearly with the number of segments Q.\nConsequently, for a small number of segments Q, it is expected that a good decoding strategy is\nto decode each segment independently, and then, the final decision is obtained by selecting the\nbest error (correcting) pattern from each of the segments decodings. In this paper, we refine this\nstrategy for Q = 2 segments as a generalization of the conventional OSD having only Q = 1\nsegment.\nAssuming that the two segments of the MRIPs are decoded independently, the list of error\npatterns can be written as,\n(1)\n\n(2)\n\nE L = E L1 \u222a E L2\n(1)\n\n(11)\n\n(2)\n\nwhere EL1 and EL2 are the lists of error patterns corresponding to the list decoding of the first\nsegment and of the second segment, respectively, and L = L1 + L2 . Obviously, fewer errors,\nand thus, fewer error patterns can be assumed in the shorter segments with larger reliabilities\nof the received bits. Similarly to the conventional OSD having one segment, for both MRIPs\nsegments, we assume all the error patterns up to the maximum Hamming weight Iq , q = 1, 2.\nThen, the lists of error patterns in (11) can be defined as,\n(1)\n\n1\n= {(e, 0)G : 0 \u2264 wH [e] \u2264 I1 , e \u2208 ZK\n2 }\n\n(2)\n\n2\n= {(0, e)G : 0 \u2264 wH [e] \u2264 I2 , e \u2208 ZK\n2 }.\n\nE L1\n\nE L2\n\n(12)\n\nThe decoding complexity of the segmentation-based OSD with the lists of error patterns defined\nin (12) is,\nL=\n\n\u0013\nI1 \u0012\nX\nK1\nl=0\n\nl\n\n+\n\n\u0013\nI2 \u0012\nX\nK2\nl=0\n\nl\n\nwhere K = K1 + K2 , and we assume I1 \u226a K1 and I2 \u226a K2 .\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n11\n\nRecall that the original OSD, denoted as OSD(I), has one segment of length K bits, and that\nthe maximum number of bit errors assumed in this segment is I. The segmentation-based OSD\nis denoted as OSD(I1 , I2 ), and it is parametrized by the segment length K1 , and K2 , and the\nmaximum number of errors I1 and I2 , respectively. The segment sizes K1 and K2 are chosen\nempirically to minimize the BER for a given decoding complexity and for a class of codes under\nconsideration. In particular, for systematic block codes of block length N < 128 and of rate\nR \u2265 1/2, we found that the recommended length of the first segment is,\nK1 \u2248 0.35K\nso that the second segment length is K2 = K \u2212K1 . The maximum number of bit errors I1 and I2\nin the two segments are selected to fine-tune the BER performance and the decoding complexity\ntrade-off. For instance, we can obtain the list decoding schemes having the BER performance as\nwell as the decoding complexity between those corresponding to the original decoding schemes\nOSD(I) and OSD(I + 1).\nFinally, we note that it is straightforward to develop the skipping criteria for efficient searching\nof the list of error patterns in the OSD-based decoding schemes. In particular, one can consider the\nHamming distances for one or more segments of the MRIPs between the received hard decisions\n(before the decoding) and the temporary decisions obtained using the test error patterns from\nthe list. If any or all of the Hamming distances are above given thresholds, the test error pattern\ncan be discarded without re-encoding and calculating the corresponding Euclidean distance. For\nthe Q = 2 segments OSD, our empirical results indicate that the thresholds for the first and the\nsecond segments should be 0.35 dmin and dmin , respectively.\nB. Partial-Order Statistics Decoding\nThe Gauss (or the Gauss-Jordan) elimination employed in the OSD-based decoding algorithms\nrepresents a significant portion of the overall implementation complexity. A new row (or a\nreduced row) echelon form of the generator matrix must be obtained after every permutation \u03bb\u2032\u2032\nuntil the MRIPs are found. Hence, we can devise a partial-order statistics decoding (POSD) that\ncompletely avoids the Gauss elimination, and thus, it further reduces the decoding complexity\nof the OSD-based decoding. The main idea of the POSD is to order only the first K received\nbits according to their reliabilities, so that the generator matrix remains in its systematic form.\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n12\n\nThe ordering of the first K received bits in the descending order can improve the coding gain\nof the segmentation-based OSD. Assuming Q = 2 segments, we use the notation POSD(I1 , I2 ).\nThe parameters K1 , K2 , I1 and I2 of POSD(I1 , I2 ) can be optimized similarly as in the case\nof OSD(I1 , I2 ) to fine-tune the BER performance versus the implementation complexity. On the\nother hand, we will show in Section V that the partial ordering (i.e., the ordering of the first K\nout of N received bits) is irrelevant for the OSD decoding having one segment of the MRIPs\nand using the list of error patterns (10). In this case, the POSD(I) decoding can be referred to\nas the input-sphere decoding ISD(I).\nC. Implementation Complexity\nWe compare the number of binary operations (BOPS) and the number of floating point operations (FLOPS) required to execute the decoding algorithms proposed in this paper. Assuming a\n(N, K, dmin) code, the complexity of the OSD and the POSD are given in Table I and Table II.\nThe implementation complexity expressions in Table I for OSD(I) are from the reference [3].\nFor example, the OSD decoding of the BCH code (128, 64, 22) requires at least 1152 FLOPS and\n528448 BOPS to find the MRIPs and to obtain the corresponding equivalent generator matrix\nin a row echelon form. All this complexity can be completely avoided by assuming the partial\nordering in the POSD decoding. The number of the test error patterns is L = 2080 for OSD(2),\nand L = 1177 for OSD(2, 2) with K1 = 21 and K2 = 43 whereas the coding gain of OSD(2)\ncan be only slightly better than the coding gain of OSD(2, 2); see, for example, Fig. 4. Hence,\nthe overall complexity of the OSD-based schemes can be substantially reduced by avoiding the\nGauss (Gauss-Jordan) elimination.\nV. P ERFORMANCE A NALYSIS\nRecall that we assume a memoryless communication channel as described in Section II. We\nderive the probability Pr{\u01090 \u2208 EL } in (9) that the error pattern \u01090 observed at the receiver\nafter transmission of the codeword cTx = 0 is an element of the chosen decoding list EL . The\nderivation relies on the following generalization of Lemma 3 in [3].\nLemma 2: For any ordering of the N received bits, consider the I bit positions I \u2286 {1, 2, * * * , N},\n\u0001\nand the II1 subsets I1 \u2286 I of I1 \u2264 I \u2264 N bit positions. Then, the total probability of the I1\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n13\n\nbit errors within the I bits can be calculated as,\n\u0012 \u0013\nX\nI\npI 1\nP(I1 ) =\nI1 0\nI1 : |I1 |=I1\n\nwhere p0 is the probability of bit error corresponding to the bit positions I.\nProof: The ordering of the chosen I bits given by the set I is irrelevant since all subsets\nI1 of I1 errors within the I bits I are considered. Consequently, the bit errors in the set I can\nbe considered to be independent having the equal probability denoted as p0 .\nUsing Lemma 2, we observe that the lists of error patterns (10) and (12) are constructed, so that\nthe ordering of bits within the segments is irrelevant. Then, the bit errors in a given segment can\nbe considered to be conditionally independent. This observation is formulated as the following\ncorollary of Lemma 2.\nCorollary 1: For the OSD(I) and the list of error patterns (10), the bit errors in the MRIPs\ncan be considered as conditionally independent. Similarly, for the POSD(I1 , I2 ) and the list\nof error patterns (12), the bit errors in the two segments can be considered as conditionally\nindependent.\nThus, the bit errors in Corollary 1 are independent conditioned on the particular segment being\nconsidered as shown next.\nLet P0 be the bit error probability of the MRIPs for the OSD(I) decoding. Similarly, let P1\nand P2 be the bit error probabilities in the first and the second segments of the OSD(I1 , I2 )\ndecoding, respectively. Denote the auxiliary variables, v1 = |r\u0303K1 |, v2 = |r\u0303K1 +1 |, and v3 = |r\u0303K+1|\nof the order statistics (3), and let u \u2261 |ri |, i = 1, 2, * * * , K. Hence, always, v1 \u2265 v2 , and, for\nsimplicity, ignoring the second permutation \u03bb\u2032\u2032 , also, v2 \u2265 v3 . The probability of bit error P0 for\nthe MRIPs is calculated as,\nP0 = Eu\n\n\u0014Z\n\n0\n\nu\n\nfv3 (v)\ndv\n1 \u2212 Fu (v)\n\n\u0015\n\nwhere Eu [*] denotes the expectation w.r.t. (with respect to) u, fv3 (v) is the PDF of the (K + 1)-th\norder statistic in (3), and Fu (v) is the cumulative distribution function (CDF) of the magnitude\n(the absolute value) of the reliability of the received bits (before ordering). Similarly, the\nprobability of bit error P1 for the first segment is calculated as,\n\u0015\n\u0014Z u\nfv2 (v)\ndv\nP1 = Eu\n0 1 \u2212 Fu (v)\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n14\n\nwhere fv2 (v) is the PDF of the (K1 + 1)-th order statistic in (3). The probability of bit error P2\nfor the second segment is calculated as,\n\u0015\n\u0014Z u Z \u221e\nfv1 (v)fv3 (v \u2032 )\n\u2032\ndvdv\nP2 = Eu\n(Fu (v) \u2212 Fu (v \u2032))(1 \u2212 Fv1 (v \u2032 ))\n0\nu\nwhere fv1 (v) and Fv1 (v \u2032 ) is the PDF and the CDF of the K1 -th order statistic in (3), respectively.\nThe values of the probabilities P0 , P1 and P2 have to be evaluated by numerical integration.\nFinally, we use Lemma 2 and substitute the probabilities P0 , P1 and P2 for p0 to calculate the\nprobability Pr{\u01090 \u2208 EL } of the error patterns in the list EL .\nVI. N UMERICAL E XAMPLES\nWe use computer simulations to compare the BER performances of the proposed soft-decision\ndecoding schemes. All the block codes considered are linear and systematic.\nThe BER of the (31, 16, 7) BCH code over an AWGN channel is shown in Fig. 2 assuming\nISD(2) and ISD(3) with K = 16 having 137 and 697 test error patterns, respectively, and\nassuming POSD(1, 3) and POSD(2, 3) with K1 = 6 and K2 = 10 having 183 and 198 test error\npatterns, respectively. We observe that POSD(1, 3) achieves the same BER as ISD(3) while using\nmuch less error patterns which represents the gain of the ordering of the received information\nbits into two segments. At the BER of 10\u22124 , POSD(1, 3) outperforms ISD(2) by 1.1 dB using\napproximately 50% more test error patterns. Thus, the POSD(1, 3) decoding provides 2.3 dB\ncoding gain with the small implementation complexity at the expense of 2 dB loss compared to\nthe ML decoding.\nFig. 3 shows the BER of the (63, 45, 14) BCH code over an AWGN channel. The number\nof test error patterns for the ISD(2), ISD(3), POSD(1, 3) and OSD(2) decodings are 1036,\n15226, 5503 and 1036, respectively. We observe from Fig. 3 that ISD(3) has the same BER as\nPOSD(1, 3) with two segments of K1 = 13 and K2 = 32 bits. However, especially for the high\nrate codes (i.e. of rates greater than 1/2), one has to also consider the complexity of the Gauss\nelimination to obtain the row echelon form of the generator matrix for the OSD. For example,\nthe Gauss elimination for the (63, 45, 14) code requires approximately 20, 400 BOPS; cf. Table I.\nThe BER of the (128, 64, 22) BCH code over an AWGN channel is shown in Fig. 4 assuming\nOSD(1) and OSD(2) with K = 64, and assuming OSD(2, 2) with K1 = 21 and K2 = 43. The\nnumber of test error patterns for the OSD(1), OSD(2) and OSD(2, 2) decodings are 64, 2081\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n15\n\nand 1179. A truncated union bound of the BER in Fig. 4 is used to indicate the ML performance\n[14, Ch. 10]. We observe that both OSD(2) and OSD(2, 2) have the same BER performance for\nthe BER values larger than 10\u22123 , and OSD(2) outperforms OSD(2, 2) by at most 0.5 dB for the\nsmall values of the SNR. Our numerical results indicate that, in general, OSD(2, 2) decoding can\nachieve approximately the same BER as OSD(2) for small to medium SNR while using about\n50% less error patterns. Thus, a slightly smaller coding gain (less than 0.5dB) of OSD(2, 2) in\ncomparison with OSD(2) at larger values of the SNR is well-compensated for by the reduced\ndecoding complexity. More importantly, OSD(2, 2) can trade-off the BER performance and the\ndecoding complexity between those provided by OSD(1) and OSD(2), especially at larger values\nof SNR.\nThe BER of the (31, 16, 7) BCH code over a fast Rayleigh fading channel is shown in Fig. 5.\nWe assume the same decoding schemes as in Fig. 2. The POSD(1, 3) decoding with 183 error\npatterns achieves the coding gain of 17 dB over an uncoded system, the coding gain of 4 dB over\nISD(2) with 137 error patterns, and it has the same BER as OSD(3) with 697 error patterns.\nThe BER of the high rate BCH code (64, 57, 4) over a fast Rayleigh channel is shown in Fig. 6.\nIn this case, the number of test error patterns for the ISD(2), ISD(3), POSD(2, 3) and OSD(2)\ndecoding is 1654, 30914, 8685 and 1654, respectively. We observe that, for small to medium\nSNR, POSD(2, 3) which does not require the Gauss elimination (corresponding to approximately\n3, 000 BOPS) outperforms OSD(2) by 1dB whereas, for large SNR values, these two decoding\nschemes achieve approximately the same BER performance.\nVII. C ONCLUSIONS\nLow-complexity soft-decision decoding techniques employing a list of the test error patterns\nfor linear binary block codes of small to medium block length were investigated. The optimum\nand suboptimum construction of the list of error patterns was developed. Some properties of\nthe joint probability of error of the received bits after ordering were derived. The original OSD\nalgorithm was generalized by assuming a segmentation of the MRIPs. The segmentation of the\nMRIPs was shown to overcome several drawbacks of the original OSD and to enable flexibility\nfor devising new decoding strategies. The decoding complexity of the OSD-based decoding\nalgorithms was reduced further by avoiding the Gauss (or the Gauss Jordan) elimination using\nthe partial ordering of the received bits in the POSD decoding. The performance analysis was\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n16\n\nconcerned with the problem of finding the probability of the test error patterns contained in\nthe decoding list. The BER performance and the decoding complexity of the proposed decoding\ntechniques were compared by extensive computer simulations. Numerical examples demonstrated\nexcellent flexibility of the proposed decoding schemes to trade-off the BER performance and\nthe decoding complexity. In some cases, both the BER performance as well as the decoding\ncomplexity of the segmentation-based OSD were found to be improved compared to the original\nOSD.\nA PPENDIX\nWe derive the probabilities P0 , P1 and P2 in Section V. Without loss of generality, we assume\nthat the all-ones codeword was transmitted, i.e., xi = \u22121 for \u2200i. Then, after ordering, the i-th\nreceived bit, i = 1, 2, * * * , N, is in error, provided that r\u0303i > 0. The probability of bit error P0\nfor the MRIPs is obtained as,\nP0 =\n\nZ\n\n0\n\n\u221eZ \u221e\n\nfu (u|u \u2265 v3 )fv3 (v3 )dv3 du\n\n0\n\nwhere the conditional PDF [16],\nfu (u|u \u2265 v3 ) =\n\n\uf8f1\n\uf8f2\n\uf8f3\n\nfu (u)\n1\u2212Fu (v3 )\n\nu \u2265 v3\n\n0\n\nu < v3\n\nand fu (u) and Fu (v3 ) are the PDF and the CDF of the reliability of the received bits, respectively,\nso that,\nP0 =\n\nZ\n\n\u221e\n\nfu (u)\n\n0\n\nZ\n\nu\n\n0\n\nfv3 (v3 )\ndv3 du.\n1 \u2212 Fu (v3 )\n\nSimilarly, the probability of bit error P1 for the first segment is calculated as,\nZ \u221eZ \u221e\nP1 =\nfu (u|u \u2265 v2 )fv2 (v2 )dv2 du\n0\n0\nZ \u221e\nZ u\nfv2 (v2 )\ndv2 du.\n=\nfu (u)\n0\n0 1 \u2212 Fu (v2 )\nThe probability of bit error P2 for the second segment is calculated as,\nZ \u221e Z \u221eZ \u221e\nP2 =\nfu (u|v1 \u2265 u \u2265 v3 )fv1 ,v3 (v1 , v3 )dv1 dv3 du\n0\n\n0\n\n0\n\nwhere the conditional PDF,\nfu (u|v1 \u2265 u \u2265 v3 ) =\n\n\uf8f1\n\uf8f2\n\uf8f3\n\nfu (u)\nFu (v1 )\u2212Fu (v3 )\n\nv1 \u2265 u \u2265 v3\n\n0\n\notherwise\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n17\n\nand the joint PDF of the order statistics v1 \u2265 v3 is,\n\uf8f1\n\uf8f2 fv1 (v1 ) f (v ) v \u2265 v\n1\n3\n1\u2212Fv1 (v3 ) v3 3\nfv1 ,v3 (v1 , v3 ) =\n\uf8f3\n0\nv1 < v3\n\nand thus,\n\nP2 =\n\nZ\n\n\u221e\n\nfu (u)\n0\n\nZ uZ\n0\n\nu\n\n\u221e\n\nfv1 (v)fv3 (v \u2032 )\ndvdv \u2032 du.\n\u2032\n\u2032\n(Fu (v) \u2212 Fu (v ))(1 \u2212 Fv1 (v ))\nR EFERENCES\n\n[1] H. Yagi, \"A study on complexity reduction of the reliability-based maximum likelihood decoding algorithm for block\ncodes,\" PhD dissertation, Waseda University, 2005.\n[2] B. Dorsch, \"A decoding algorithm for binary block codes and J-ary output channels,\" IEEE Trans. Inf. Theory, vol. 20,\npp. 391-394, 1974.\n[3] M. P. C. Fossorier and S. Lin, \"Soft-decision decoding of linear block codes based on ordered statistics,\" IEEE Trans. Inf.\nTheory, vol. 41, pp. 1379-1396. Sept. 1995.\n[4] D. Gazelle and J. Snyders, \"Reliability-based code-search algorithms for maximum-likelihood decoding of block codes,\"\nIEEE Trans. Inf. Theory, vol. 43, pp. 239-249, Jan. 1997.\n[5] A. Kabat, F. Guilloud and R. Pyndiah, \"New approach to order statistics decoding of long linear block codes,\" in Proc.\nGlobecom, pp. 1467-1471, Nov. 2007.\n[6] H. Yagi, T. Matsushima and S. Hirasawa, \"Fast algorithm for generating candidate codewords in reliability-based maximum\nlikelihood decoding,\" IEICE Trans. Fundamentals, vol. E89-A, pp. 2676-2683, Oct. 2006.\n[7] M. Fossorier and S. Lin, \"Computationally efficient soft decision decoding of linear block codes based on ordered statistics,\"\nIEEE Trans. Inf. Theory, vol. 42, pp. 738-750, May 1996.\n[8] M. Fossorier, \"Reliability-based soft-decision decoding with iterative information set reduction,\" IEEE Trans. Inf. Theory.,\nvol. 48, no. 12, pp. 3101-3106, Dec. 2002.\n[9] W. Jin and M. Fossorier, \"Reliability based soft decision decoding with multiple biases,\" IEEE Trans. Inf. Theory, vol. 53,\nno. 1, pp. 105-119, Jan. 2007.\n[10] A. Valembois and M. Fossorier, \"Sort-and-match algorithm for soft-decision decoding,\" IEEE Trans. Inf. Theory, vol. 45,\npp. 2333-2338, Nov. 1999.\n[11] A. Valembois and M. Fossorier, \"Box and match techniques applied to soft decision decoding,\" IEEE Trans. Inf. Theory,\nvol. 50, no. 5, pp. 796-810, May 2004.\n[12] M. El-Khamy, H. Vialko, B. Hassibi and R. McEliecce,\"Performance of sphere decoding of block codes,\" IEEE Trans.\nComms., vol. 57, pp. 2940\u20132950, Oct. 2009.\n[13] H. Vikalo and B. Hassibi, \"On joint detection and decoding of linear block codes on Gaussian vector channels,\" IEEE\nTrans. Signal Proc., vol. 54, pp. 3330\u20133342, Sep. 2006.\n[14] S. Benedetto and E. Biglieri, Principles of Digital Transmission With Wireless Applications, Kluwer Academic, 1999.\n[15] S. Lin and D. J. Costello, Error Control Coding: Fundamentals and Applications, Prentice-Hall, 1983.\n[16] A. Papoulis and S. U. Pillai, Probability, Random Variables, and Stochastic Processes, 4th Ed., McGraw-Hill, 2002.\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n18\n\nTABLE I\nI MPLEMENTATION C OMPLEXITY OF THE OSD AND THE POSD\nOSD(I1 ) and OSD(I1 , I2 )\noperation\n\ncomplexity\n\nr\n\n2N FLOPS\n\nr\u0303\u2032\n\nN log2 (N ) FLOPS\n\nGauss el. G\u2032\n\nN min(K, N \u2212 K)2 BOPS\n\nr\u0303\u2032\u2032\n\nK + K(N \u2212 K) BOPS\nPOSD(I1 ) \u2261 ISD(I1 )\n\noperation\n\ncomplexity\n\nr\n\n2N FLOPS\n\n\u2032\n\n0 BOPS\n\nr\u0303\n\nPOSD(I1 , I2 )\noperation\n\ncomplexity\n\nr\n\n2N FLOPS\n\n\u2032\n\nK log2 (K) FLOPS\n\nr\u0303\n\nTABLE II\nD ECODING L IST S IZES FOR THE OSD AND\nPI\n\nOSD(I)\n\nl=0\n\nPI1\n\nOSD(I1 , I2 )\n\nl=0\n\nc\n\nM (*)\n\nx\n\ny\nh\n\nFig. 1.\n\nw\n\n+\n\nl=0\n\nPI1\n\nl=0\n\nRe{*}\nh\u2217\n\n\u0001\n\nPI\n\nPOSD(I) \u2261 ISD(I)\nPOSD(I1 , I2 )\n\nK1\nl\n\nr\n\nK1\nl\n\n\u0001\n\n+\n\nTHE\n\nPOSD\n\n\u0001\nK\nl\n\nPI2\n\nl=0\n\nK\nl\n\nK2\nl\n\n\u0001\n\nK2\nl\n\n\u0001\n\n\u0001\n\nPI2\n\nl=0\n\n0-OSD\n\nr\u0303\nx0\n\nr\u03030\n\nargmax\ne\u2208EL\n\n\u0109\n\nr\u03030 *M (e)\nEL\n\nThe system model and an equivalent vector channel with the binary vector input c and the vector soft-output r\u03030 .\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n19\n\n0\n\n10\n\nBPSK\nISD(2)\nISD(3)\nPOSD(1|6,3|10)\nPOSD(2|6,3|10)\nML\n\n\u22121\n\n10\n\n\u22122\n\nBER\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n\u22125\n\n10\n\n\u22126\n\n10\n\nFig. 2.\n\n0\n\n1\n\n2\n\n3\nSNR[dB]\n\nThe BER of the (31, 16, 7) BCH code over an AWGN channel.\n\n4\n\n5\n\n6\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n20\n\n0\n\n10\n\nBPSK\nISD(2)\nISD(3)\nPOSD(1|13,3|32)\nOSD(2)\n\n\u22121\n\n10\n\n\u22122\n\nBER\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n\u22125\n\n10\n\n\u22126\n\n10\n\n0\n\n1\n\n2\n\n3\n\n4\nSNR[dB]\n\nFig. 3.\n\nThe BER of the (63, 45, 14) BCH code over an AWGN channel.\n\n5\n\n6\n\n7\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n21\n\n0\n\n10\n\nBPSK\nOSD(1)\nOSD(2)\nOSD(2|21,2|43)\nsoft union bound\n\n\u22121\n\n10\n\n\u22122\n\n10\n\n\u22123\n\nBER\n\n10\n\n\u22124\n\n10\n\n\u22125\n\n10\n\n\u22126\n\n10\n\n\u22127\n\n10\n\n0\n\n1\n\n2\n\n3\nSNR[dB]\n\nFig. 4.\n\nThe BER of the (128, 64, 22) BCH code over an AWGN channel.\n\n4\n\n5\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n22\n\n0\n\n10\n\nBPSK\nISD(2)\nISD(3)\nPOSD(1|6,3|10)\nOSD(2)\n\n\u22121\n\n10\n\n\u22122\n\nBER\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n0\n\n5\n\n10\n\n15\n(Eb/N0)[dB]\n\nFig. 5.\n\nThe BER of the (31, 16, 7) BCH code over a Rayleigh fading channel.\n\n20\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY\n\n23\n\n0\n\n10\n\nBPSK\nISD(2)\nISD(3)\nPOSD(2|20,3|37)\nOSD(2)\n\u22121\n\nBER\n\n10\n\n\u22122\n\n10\n\n\u22123\n\n10\n\n\u22124\n\n10\n\n0\n\nFig. 6.\n\n2\n\n4\n\n6\n\n8\n\n10\n(Eb/N0)[dB]\n\n12\n\nThe BER of the (64, 57, 4) BCH coded over a Rayleigh fading channel.\n\n14\n\n16\n\n18\n\n20\n\n\f"}