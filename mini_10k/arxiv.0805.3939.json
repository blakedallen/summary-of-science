{"id": "http://arxiv.org/abs/0805.3939v1", "guidislink": true, "updated": "2008-05-26T11:55:55Z", "updated_parsed": [2008, 5, 26, 11, 55, 55, 0, 147, 0], "published": "2008-05-26T11:55:55Z", "published_parsed": [2008, 5, 26, 11, 55, 55, 0, 147, 0], "title": "Decision Support with Belief Functions Theory for Seabed\n  Characterization", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0805.3153%2C0805.1821%2C0805.1282%2C0805.4192%2C0805.4276%2C0805.2314%2C0805.2296%2C0805.4541%2C0805.4440%2C0805.0363%2C0805.2547%2C0805.0379%2C0805.0797%2C0805.0425%2C0805.4420%2C0805.3243%2C0805.0760%2C0805.4202%2C0805.0122%2C0805.4737%2C0805.0472%2C0805.2559%2C0805.2535%2C0805.1496%2C0805.1301%2C0805.3051%2C0805.4599%2C0805.3877%2C0805.0230%2C0805.1837%2C0805.3402%2C0805.3613%2C0805.3519%2C0805.3941%2C0805.3555%2C0805.0298%2C0805.4198%2C0805.2812%2C0805.3715%2C0805.1083%2C0805.2939%2C0805.2037%2C0805.0646%2C0805.1039%2C0805.2801%2C0805.0342%2C0805.1726%2C0805.0918%2C0805.3107%2C0805.3226%2C0805.1108%2C0805.2047%2C0805.0723%2C0805.0102%2C0805.1540%2C0805.2675%2C0805.3018%2C0805.2002%2C0805.3292%2C0805.1364%2C0805.1638%2C0805.3793%2C0805.4156%2C0805.0176%2C0805.4359%2C0805.2150%2C0805.4769%2C0805.2809%2C0805.3656%2C0805.0097%2C0805.1992%2C0805.3688%2C0805.3065%2C0805.2260%2C0805.2524%2C0805.0063%2C0805.0851%2C0805.1144%2C0805.3780%2C0805.1658%2C0805.0460%2C0805.4068%2C0805.2324%2C0805.3980%2C0805.3221%2C0805.4026%2C0805.1520%2C0805.1958%2C0805.0838%2C0805.2733%2C0805.2950%2C0805.0410%2C0805.3874%2C0805.2737%2C0805.0986%2C0805.3500%2C0805.2307%2C0805.2228%2C0805.3939%2C0805.3059%2C0805.4405&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Decision Support with Belief Functions Theory for Seabed\n  Characterization"}, "summary": "The seabed characterization from sonar images is a very hard task because of\nthe produced data and the unknown environment, even for an human expert. In\nthis work we propose an original approach in order to combine binary\nclassifiers arising from different kinds of strategies such as one-versus-one\nor one-versus-rest, usually used in the SVM-classification. The decision\nfunctions coming from these binary classifiers are interpreted in terms of\nbelief functions in order to combine these functions with one of the numerous\noperators of the belief functions theory. Moreover, this interpretation of the\ndecision function allows us to propose a process of decisions by taking into\naccount the rejected observations too far removed from the learning data, and\nthe imprecise decisions given in unions of classes. This new approach is\nillustrated and evaluated with a SVM in order to classify the different kinds\nof sediment on image sonar.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0805.3153%2C0805.1821%2C0805.1282%2C0805.4192%2C0805.4276%2C0805.2314%2C0805.2296%2C0805.4541%2C0805.4440%2C0805.0363%2C0805.2547%2C0805.0379%2C0805.0797%2C0805.0425%2C0805.4420%2C0805.3243%2C0805.0760%2C0805.4202%2C0805.0122%2C0805.4737%2C0805.0472%2C0805.2559%2C0805.2535%2C0805.1496%2C0805.1301%2C0805.3051%2C0805.4599%2C0805.3877%2C0805.0230%2C0805.1837%2C0805.3402%2C0805.3613%2C0805.3519%2C0805.3941%2C0805.3555%2C0805.0298%2C0805.4198%2C0805.2812%2C0805.3715%2C0805.1083%2C0805.2939%2C0805.2037%2C0805.0646%2C0805.1039%2C0805.2801%2C0805.0342%2C0805.1726%2C0805.0918%2C0805.3107%2C0805.3226%2C0805.1108%2C0805.2047%2C0805.0723%2C0805.0102%2C0805.1540%2C0805.2675%2C0805.3018%2C0805.2002%2C0805.3292%2C0805.1364%2C0805.1638%2C0805.3793%2C0805.4156%2C0805.0176%2C0805.4359%2C0805.2150%2C0805.4769%2C0805.2809%2C0805.3656%2C0805.0097%2C0805.1992%2C0805.3688%2C0805.3065%2C0805.2260%2C0805.2524%2C0805.0063%2C0805.0851%2C0805.1144%2C0805.3780%2C0805.1658%2C0805.0460%2C0805.4068%2C0805.2324%2C0805.3980%2C0805.3221%2C0805.4026%2C0805.1520%2C0805.1958%2C0805.0838%2C0805.2733%2C0805.2950%2C0805.0410%2C0805.3874%2C0805.2737%2C0805.0986%2C0805.3500%2C0805.2307%2C0805.2228%2C0805.3939%2C0805.3059%2C0805.4405&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The seabed characterization from sonar images is a very hard task because of\nthe produced data and the unknown environment, even for an human expert. In\nthis work we propose an original approach in order to combine binary\nclassifiers arising from different kinds of strategies such as one-versus-one\nor one-versus-rest, usually used in the SVM-classification. The decision\nfunctions coming from these binary classifiers are interpreted in terms of\nbelief functions in order to combine these functions with one of the numerous\noperators of the belief functions theory. Moreover, this interpretation of the\ndecision function allows us to propose a process of decisions by taking into\naccount the rejected observations too far removed from the learning data, and\nthe imprecise decisions given in unions of classes. This new approach is\nillustrated and evaluated with a SVM in order to classify the different kinds\nof sediment on image sonar."}, "authors": ["Arnaud Martin", "Isabelle Quidu"], "author_detail": {"name": "Isabelle Quidu"}, "author": "Isabelle Quidu", "links": [{"href": "http://arxiv.org/abs/0805.3939v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0805.3939v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.4; I.5", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0805.3939v1", "affiliation": "E3I2", "arxiv_url": "http://arxiv.org/abs/0805.3939v1", "arxiv_comment": null, "journal_reference": "Dans Proceeding of the 11th International Conference on\n  Information Fusion - International Conference on Information Fusion, Cologne\n  : Allemagne (2008)", "doi": null, "fulltext": "arXiv:0805.3939v1 [cs.AI] 26 May 2008\n\nDecision Support with Belief Functions Theory for\nSeabed Characterization\nArnaud Martin\n\nIsabelle Quidu\n\nENSIETA, E3 I2 - EA3876\n2, rue Fran\u00e7ois Verny\n29806 Brest, Cedex 9, France\nEmail: Arnaud.Martin@ensieta.fr\n\nENSIETA, E3 I2 - EA3876\n2, rue Fran\u00e7ois Verny\n29806 Brest, Cedex 9, France\nEmail: Isabelle.Quidu@ensieta.fr\n\nAbstract- The seabed characterization from sonar images is\na very hard task because of the produced data and the unknown\nenvironment, even for an human expert. In this work we propose\nan original approach in order to combine binary classifiers\narising from different kinds of strategies such as one-versusone or one-versus-rest, usually used in the SVM-classification.\nThe decision functions coming from these binary classifiers are\ninterpreted in terms of belief functions in order to combine\nthese functions with one of the numerous operators of the belief\nfunctions theory. Moreover, this interpretation of the decision\nfunction allows us to propose a process of decisions by taking\ninto account the rejected observations too far removed from the\nlearning data, and the imprecise decisions given in unions of\nclasses. This new approach is illustrated and evaluated with a\nSVM in order to classify the different kinds of sediment on image\nsonar.\n\nKeywords: belief functions theory, decision support, SVM,\nsonar image.\nI. I NTRODUCTION\nSonar images are obtained from temporal measurements\nmade by a lateral, or frontal sonar trailed by the back of a boat.\nEach emitted signal is reflected on the bottom then received on\nthe antenna of the sonar with an adjustable delayed intensity.\nReceived data are very noisy. There are some interferences\ndue to the signal travelling on multiple paths (reflection on the\nbottom or surface), due to speckle, and due to fauna and flora.\nTherefore, sonar images are chraracterized by imprecision\nand uncertainty; thus sonar image classification is a difficult\nproblem [1]. Figure 1 shows the differences between the\ninterpretation and the certainty of two sonar experts trying\nto differentiate types of sediment (rock, cobbles, sand, ripple,\nsilt) or shadow when the information is invisible (each color\ncorresponds to a kind of sediment and the associated certainty\nof the expert is expressed in terms of sure, moderately sure\nand not sure) [2].\nThe automatic classification approaches, for sonar images,\nare based on texture analysis and a classifier such as a SVM\n[3]. The support vector machines (SVM) is based on an\noptimization approach in order to separate two classes by\nan hyperplane. For pattern recognition with several classes,\nthis optimization approach is possible (see [4]) but time consuming. Hence a preferable solution is to combine the binary\nclassifiers according to a classical strategy such as one-versusone or one-versus-rest. The combination of these classifiers\n\nFig. 1.\n\nSegmentation given by two experts.\n\nis generally formed with very simple approaches such as a\nvoting rule or a maximization of decision function coming\nfrom the classifiers. However, many combination operators can\nbe used, especially in the belief functions framework (cf. [5]).\nBelief functions theory has been already employed in order to\ncombine the binary classifier originally from SVM (see [6],\n[7]). The operators in the belief functions theory deal with the\nconflict arising from the binary classifiers. Another interest\nof this theory is that we can obtain a belief degree on the\nunions of classes and not only on exclusive classes. Indeed\nthe decisions of the binary classifiers can be difficult to take\nwhen data overlap. From the decision function, we can define\nprobabilities in order to combine them (cf. [8]). However, a\nprobability measure is an additive measure and so probabilities\ncannot easily provide a decision on unions of classes unlike\nbelief functions.\nHence, once the binary classifiers have been combined, we\npropose belief functions in order to take the decision for one\n\n\fclass only if this class is credible enough, for the union of\ntwo or more classes otherwise. Moreover, according to the\napplication it could be interesting to not take the decision on\none of the learning classes, and reject data too far from the\nlearning classes. Many classical approaches are possible in\npattern recognition for outliers rejection (see [9], [10]). We\npropose here to integrate outliers rejection in our decision\nprocess based on belief functions.\nIn addition to this new decision process, the originality of\nthe paper concerns the modelization that we propose, i.e. how\nto define the belief functions, on the basis of decision functions\ncoming from the binary classifiers.\nThis paper is organized as follows: in section II we recall\nthe principle of the support vector machines for classification.\nNext, we present the belief functions theory in section III in\norder to propose in section IV our belief approach to combine the binary classifiers and to provide a decision process\nallowing the outliers rejection and the indecision expressed as\npossible decisions on unions. This approach is evaluated for\nseabed characterization on sonar images in section V.\n\nwhere the \u039bt \u2265 0 are the Lagrange multipliers, satisfying\nl\nX\n\u039bt yt = 0.\nt=1\n\nIf the data are not linearly separable, the constraints (2) are\nrelaxed with the introduction of positive terms \u03bet . In this case\nwe search to minimize:\nl\n\nX\n1\nk w k2 +C\n\u03bet ,\n2\nt=1\n\nwith the constraints given for all t:\n\u001a\nyt (w.xt + b) \u2265 1 \u2212 \u03bet\n\u03bet \u2265 0\n\n(4)\n\n(5)\n\nwhere C is a constant given by the user in order to weight the\nerror. This problem is solved in the same way as the linear\nseparable case with Lagrange multipliers 0 \u2264 \u039bt \u2264 C.\nTo classify a new pattern x we simply need to study the\nsign of the decision function given by:\nX\nf (x) =\nyt \u039bt xt .x \u2212 b,\n(6)\nt\u2208SV\n\nII. SVM\n\nFOR CLASSIFICATION\n\nSupport vector machines were introduced by [11] based on\nthe statistical learning theory. Hence, SVM can be used for\nestimation, regression or pattern recognition like in this paper.\nA. Principle of the SVM\nThe support vector machine approach is a binary classification method. It classifies positive and negative patterns\nby searching the optimal hyperplane that separates the two\nclasses, while guaranteeing a maximum distance between the\nnearest positive and negative patterns. The hyperplane that\nmaximizes this distance called margin is determined by particular patterns called support vectors situated at the bounds of\nthe margin. These only few support vector numbers are used to\nclassify a new pattern, which makes SVM very fast. The power\nof SVM is also due to their simplicity of implementation and\nto solid theoretical bases.\nIf the patterns are linearly separable, we search the hyperplane y = w.x + b which maximizes the margin between the\ntwo classes where w.x is the dot product of w and x, and b\nthe bias. Thus w is the solution of the convex optimization\nproblem:\nMin kwk2 /2\n(1)\nsubject to:\nyt (w.xt + b) \u2212 1 \u2265 0 \u2200t = 1, . . . , l,\n\n(2)\n\nwhere xt \u2208 IRd stands for one of the l learning data, and\nyt \u2208 {\u22121, +1} the associated class. We can solve this\noptimization problem with the following Lagrangian:\nL=\n\nl\nkwk2 X\n\u039bt (yt (w.xt \u2212 b) \u2212 1) ,\n2 t=1\n\n(3)\n\nwhere SV = {t ; \u039bt > 0} for the separable case and\nSV = {t ; 0 < \u039bt < C} for the non-separable case, is the\nset of indices of the support vectors, and \u039bt are the Lagrange\nmultipliers.\nIn the nonlinear cases, the common idea of the kernel\napproaches is to map the data in a high dimension. To do\nthat we use a kernel function that must be bilinear, symmetric\nand positive and corresponds to a dot product in the new space.\nThe classification of a new pattern x is given by the sign of\nthe decision function:\nX\nf (x) =\nyt \u039bt K(x, xt ) \u2212 b\n(7)\nt\u2208SV\n\nwhere K is the kernel function. The most used kernels are the\npolynomial K(x, xt ) = (x.xt + 1)\u03b4 , \u03b4 \u2208 IN, and the radial\n2\nbasis functions K(x, xt ) = e\u2212\u03b3kx\u2212xtk , \u03b3 \u2208 IR+ . The choice\nof the kernel is not always easy and generally left to the user.\nB. Multi-class classification with SVM\n\nWe can distinguish two kinds of approaches in order to\nuse SVM for classification with n classes, n > 2. The first\none consists in fusing several binary classifiers given by the\nSVM - the obtained results by each classifier are combined to\nproduce a final result following strategies such as one-versusone or one-versus-rest. The second one consists in considering\nthe optimization problem.\n\u2022 Direct approach: in [4], the notion of margin is extended\nto the multi-class problem. However, this approach becomes very time consuming, especially in the nonlinear\ncase.\n\u2022 one-versus-rest: This approach consists in learning n decision functions fi , i = 1, ..., n given by the equations (6)\nor (7) according to the cases, allowing the discrimination\nof each class from the n \u2212 1 others. The affection of\na class wk to a new pattern x is generally given by\n\n\fthe relation: k = arg max fi (x). In the nonlinear case,\ni=1,...,n\n\n\u2022\n\nwe have to be careful of the parameters of the kernel\nfunctions that could have some different orders following\nthe learning binary classifiers. So, it could be better\nto decide on normalized functions calculated from the\ndecision functions (see [12], [13]).\none-versus-one: Instead of learning n decision functions,\nwe try here to discriminate each class from each other.\nHence we have to learn n(n \u2212 1)/2 decision functions,\nstill given by equations (6) or (7) according to the\ndifferent cases. Each decision function is considered as a\nvote in order to classify a new pattern x. The class of x\nis given by the majority voting rule.\n\nSome other methods have been proposed based on previous\nones:\n\u2022\n\n\u2022\n\n\u2022\n\nError-Correcting Output Codes (ECOC): let wi ,\ni = 1, ..., n, be the classes, Sj , j = 1, ..., s, the different\nclassifiers (s = n in the case one-versus-rest and\ns = n(n \u2212 1)/2 in the case one-versus-one), (Mij ),\nthe matrix of the codes with the classes in row and the\nclassifiers in column, stands for the contribution of each\nclassifier to the final result of the classification (based\non the error of all the classifiers). The final decision is\ngiven comparing the results of the classifiers with each\nrow of the matrix; the class of a new pattern x is the\nclass giving the least error (see [14]).\nAccording to the decision functions, [8] defined a probability (19) in order to normalize the decision functions. Hence, we can combine the binary classifiers (for\nboth one-versus-rest and one-versus-one cases) with a\nBayesian rule (see [15]) or with more simple rules (see\n[7]).\nDAGSVM (Directed Acyclic Graph SVM) proposed by\n[16]: In this approach, the learning is made as the oneversus-one with the learning of n(n\u22121)/2 binary decision\nfunctions. In order to generalize, a binary decision tree is\nconsidered where each node stands for a binary classifier\nand each leaf stands for a class. Each binary classifier\neliminates a class and the class of a new pattern is the\nclass given by the last node.\n\nand\nX\n\nmj (X) = 1,\n\n(9)\n\nX\u22082\u0398\n\nwhere mj (.) is the basic belief assignments for an expert (or a\nbinary classifier) Sj , j = 1, ..., s. Thus in the one-versus-rest\ncase s = n and in the one-versus-one case s = n(n \u2212 1)/2.\nThe equation (8) makes the assumption of a closed world\n(that means that all the classes are exhaustive) [18]. We can\ndefine the belief functions only with:\nmj (\u2205) \u2265 0,\n\n(10)\n\nand the world is open (cf. [19]). But in order to change an\nopen world to a closed world, we can add one element in the\ndiscriminating space and this element can be considered as the\ngarbage class. The difficulty, as we will see later, is the mass\nthat we have to allocate to this element.\nWe have two advantages with the belief functions theory\ncompared to the probabilities and Bayesian approaches. The\nfirst one is the possibility for one expert (i.e. a binary classifier)\nto decide that a new pattern belongs to the union of some\nclasses without needing to decide an unique class. The basic\nbelief functions are not additive that gives more freedom for\nthe modelization of some problems. The second one is the\nmodelization of some problems without any a priori by giving\nthe mass of belief on the ignorances (i.e. the unions of classes).\nThese simple conditions in equations (8) and (9), give\na large panel of definitions of the belief functions, which\nis one of the difficulties of the theory. From these basic\nbelief assignments, other belief functions can be defined such\nas credibility and plausibility. The credibility represents the\nintensity that the information given by one expert supports an\nelement of 2\u0398 , this is a minimal belief function given for all\nX \u2208 2\u0398 by:\nX\nmj (Y ).\n(11)\nbel(X) =\nY \u2286X,Y 6=\u2205\n\nThe plausibility represents the intensity with which there is no\ndoubt on one element. This function is given for all X \u2208 2\u0398\nby:\nX\npl(X) =\nmj (Y )\nY \u22082\u0398 ,Y \u2229X6=\u2205\n\nIII. B ELIEF\n\n=\n=\n\nFUNCTIONS THEORY\n\nThe belief functions theory, also called evidence theory or\nDempster-Shafer theory (see [17], [18]) is more and more\nemployed in order to take into account the uncertainties\nand imprecisions in pattern recognition. The belief functions\nframework is based on the use of functions defined on\nthe power set 2\u0398 (the set of all the subsets of \u0398), where\n\u0398 = {w1 , . . . , wn } is the set of exclusive and exhaustive\nclasses. These belief functions or basic belief assignments, mj\nare defined by the mapping of the power set 2\u0398 onto [0, 1] with\ngenerally:\nmj (\u2205) = 0,\n\n(8)\n\nbel(\u0398) \u2212 bel(X c )\n1 \u2212 mj (\u2205) \u2212 bel(X c ),\n\n(12)\n\nwhere X c is the complementary of X in \u0398.\nTo keep a maximum of information, it is preferable to\ncombine information given by the basic belief assignments\ninto a new basic belief assignment and take the decision on\none of the obtained belief functions. Many combination rules\nhave been proposed. The conjunctive rule proposed by [20]\nallows us to stay in an open world. It is defined for s experts,\nand for X \u2208 2\u0398 by:\nmConj (X) =\n\nX\n\ns\nY\n\nY1 \u2229...\u2229Ys =X j=1\n\nmj (Yj ),\n\n(13)\n\n\fwhere Yj \u2208 2\u0398 is the response of the expert j, and mj (Yj )\nthe corresponding basic belief assignment.\nInitially, [17] and [18] have proposed a conjunctive normalized rule, in order to stay in a closed world. This rule is\ndefined for s classifiers, for all X \u2208 2\u0398 , X 6= \u2205 by:\ns\nX\nY\n1\nmj (Yj )\nmDS (X) =\n1 \u2212 mConj (\u2205)\nY1 \u2229...\u2229Ys =X j=1\n(14)\nmConj (X)\n=\n,\n1 \u2212 mConj (\u2205)\nwhere Yj \u2208 2\u0398 is the response of the expert j, and mj (Yj ) the\ncorresponding basic belief assignment. mConj (\u2205) is generally\ninterpreted as a conflict measure or more exactly as the\ninconsistence of the fusion - because of the nonidempotence\nof the rule. This rule applied on basic belief assignments\nwhere the only focal elements are the classes wj (i.e. some\nprobabilities) is equivalent to a Bayesian approach. A short\nreview of all the combination rules in the belief functions\nframework and a number of new rules are given in [5].\nIf the credibility function provides a pessimistic decision,\nthe plausibility function is often too optimistic. The pignistic\nprobability [19] is generally considered as a compromise. It is\ncalculated from a basic belief assignment m for all X \u2208 2\u0398 ,\nwith X 6= \u2205 by:\nX\n|X \u2229 Y | m(Y )\n,\n(15)\nbetP(X) =\n|Y | 1 \u2212 m(\u2205)\n\u0398\nY \u22082 ,Y 6=\u2205\n\nwhere |X| is the cardinality of X.\nIn this paper, we wish to reject part of the data that we\ndo not consider in the learning classes. Hence a pessimistic\ndecision as to the maximum of the credibility function is\npreferable. Another criterion proposed by [21], consists in\nattributing the class wk for a new pattern x if:\n(\nbel(wk )(x) = max bel(wi )(x),\n1\u2264i\u2264n\n(16)\nbel(wk )(x) \u2265 bel(wkc )(x).\n\nThe addition of this second condition on the maximum of\ncredibility, allows a decision only if it is nonambiguous, i.e.\nif we believe more in the class wk than in the subset of the\nother classes (the complementary of the class).\nAnother approach proposed in [22] considers the plausibility\nfunctions and gives the possibility to decide whichever element\nof 2\u0398 and not only the singletons as previously. Thus the new\npattern x belongs to the element A of 2\u0398 if:\nA = arg max (mb (X)(x)pl(X)(x)) ,\n\n(17)\n\nX\u22082\u0398\n\nwhere mb is a basic belief assignment given by:\n\u0012\n\u0013\n1\nmb (X) = Kb \u03bbX\n,\n(18)\n|X|r\nr is a parameter in [0, 1] allowing a decision from a simple\nclass (r = 1) until the total indecision \u0398 (r = 0). \u03bbX allows\nthe integration of the lack of knowledge on one of the elements\nX in 2\u0398 . In this paper, we will chose \u03bbX = 1. The constant\nKb is the normalization factor giving by the condition of the\nequation (9).\n\nIV. B ELIEF\n\nFUNCTIONS THEORY FOR CLASSIFICATION\nWITH SUPPORT VECTOR MACHINES\n\nIn the previous sections, we have described the two main\nstrategies in order to build a multi-class classifier from binary\nclassifiers: the one-versus-rest and one-versus-one approaches.\nMost of the time the formalism to combine the binary classifier\nresults is different according to the strategy. [23] have proposed a combination approach of the binary classifier decisions\nbased on the belief functions theory given an unique formalism\nfor both one-versus-one and one-versus-rest strategies. The\nbasic belief assignments are defined from confusion matrices\nof the binary classifiers. Working directly on the classifier\ndecisions allows a loss of information contained first in the\ndecision functions. Thus it could be better to define the basic\nbelief assignments from the decision functions rather than\nfrom the confusion matrices (i.e. form the classifier decisions).\nHowever, the decision functions are not normalized, so\nwe can have problems in the combination of this function\nespecially with the one-versus-rest strategy. [8] has defined a\nprobability from these decision functions f such as:\nP (y = 1/f ) =\n\n1\n,\n1 + exp(Af + B)\n\n(19)\n\nwhere A and B are calculated in order to get\nP (y = 1/f = 0) = 0.5. Different approaches have\nbeen proposed for the estimation of these parameters (see\n[24]).\n[7] uses a one class SVM, introduced by [25]. So the\ncombination can be done only with a one-versus-rest strategy.\nThe decision functions coming from this particular classifier\nare employed to define some plausibility functions on the\nsingleton wi :\npl(wi )(x) =\n\nfi (x) + \u03c1\n,\n\u03c1\n\n(20)\n\nwhere fi (x) is the decision function giving the distance between x and the fronter of class wi and \u03c1 is a factor estimated\nin the one-SVM algorithm that depends on the kernel (cf. [25]).\nThe first originality of this paper resides in the definition\nof the basic belief assignments that we obtain directly from\nthe decision functions f given by the equations (6) or (7).\nThe basic idea consists in considering the data dispersion in\none of the semi-spaces given by the hyperplane, following an\nexponential distribution. This distribution gives a dispersion of\nthe data around the mean more or less near to the hyperplane,\nwith the opportunity to observe data very far away from\nthe hyperplane. Doing this we keep the basic idea of the\nSVM. Hence, according to the sign of the decision function\n(i.e. the semi-space defined by the hyperplane), the belief\ncan be obtained by the cumulative density function of the\nexponential distribution (see figure 2). We define the basic\n\n\fbelief assignment by:\n\uf8f1\n\u0010\n1\n1\n\uf8f4\nm\n(w\n)(x)\n=\u03b1\n\uf8f4\ni\ni\ni (1 \u2212 2 exp(\u2212 \u03bbi,p fi (x)))1l [0,+\u221e[ (fi (x))\n\uf8f4\n\uf8f4\n\u0011\n\uf8f4\n\uf8f4\n1\n\uf8f4\nf\n(x))1\nl\n(f\n(x))\nexp(\u2212\n\uf8f4\ni\ni\n]\u2212\u221e,0[\n\u03bbi,n\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\u0010\n1\nmi (wic )(x) =\u03b1i exp(\u2212 \u03bbi,p\nfi (x))1l[0,+\u221e[ (fi (x))\n\uf8f4\n\uf8f4\n\u0011\n\uf8f4\n\uf8f4\n1\n1\n\uf8f4\nexp(\u2212\nf\n(x)))1\nl\n(f\n(x))\n(1\n\u2212\n\uf8f4\ni\ni\n]\u2212\u221e,0(\n2\n\u03bbi,n\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nmi (\u0398)(x) =1 \u2212 \u03b1i\n\nwhere \u03b1i is a discounting factor of the basic belief assignment,\n\u03bbi,p and \u03bbi,n are some parameters depending on the decision\nfunctions of class wi that we define in equation (21). The ratio\n1\n2 is introduced to increase the belief to the class related to the\nsemi-space where the data are located (see figure 2). There are\nmany ways to choose or to calculate the discounting factor that\nis generally close to one. [26] proposes a method to obtain the\ndiscounting factor that optimizes the decision taking advantage\nof the pignistic probability. We propose here to calculate this\ndiscounting factor according to the good classification rate of\nbinary classifiers. The good classification rates are calculated\nwith the study of the sign of the decision function fi on the\nlearning data used to determine the model of binary classifiers.\n\nFig. 2. Illustration of the basic belief assignment based on the cumulative\ndensity function of the exponential distribution.\n\nWe propose to estimate the \u03bbi parameters from the mean\nof the decision functions on the learning data in order to be\ncoherent with the exponential distribution. Hence \u03bbi,p and \u03bbi,n\nare given by:\n\uf8f1\nl\n\uf8f4\n1X\n\uf8f4\n\uf8f4\n\u03bb\n=\nfi (x)1l[0,+\u221e[ (fi (x)),\n\uf8f4\ni,p\n\uf8f2\nl t=1\n(21)\nl\n\uf8f4\n1X\n\uf8f4\n\uf8f4\n\uf8f4\nfi (x)1l]\u2212\u221e,0[ (fi (x)).\n\uf8f3 \u03bbi,n = l\nt=1\n\nThis proposed basic belief assignment model allows a good\nmodelization of the information given by the binary classifiers\nin order to combine them by both one-versus-rest and oneversus-one strategies. Thus for a one-versus-rest strategy,\nwic represents the union of the other classes than wi , i.e.\n\u0398 r {wi }. In the one-versus-one case, the decision functions\n\nfi , i = 1, ..., n(n \u2212 1)/2 can be rewritten as fij with i < j\nand i, j = 1, ..., n, where i and j correspond to the considered\nclasses wi and wj . In this one-versus-one case, wic must be\nseen as wj and the basic belief assignment are given by:\n\uf8f1\n\u0010\n1\n\uf8f4\n(1 \u2212 exp(\u2212 \u03bbij,p\nfij (x)))1l[0,+\u221e[ (fij (x))\nm\n(w\n)(x)\n=\u03b1\n\uf8f4\nij\ni\nij\n\uf8f4\n\uf8f4\n\u0011\n\uf8f4\n\uf8f4\n1\n\uf8f4\nfij (x))1l]\u2212\u221e,0[ (fij (x))\n+ exp(\u2212 \u03bbij,n\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\u0010\n1\nfij (x)))1l[0,+\u221e[ (fij (x))\nexp(\u2212 \u03bbij,p\nm\n(w\n)(x)\n=\u03b1\nij\nj\nij\n\uf8f4\n\uf8f4\n\u0011\n\uf8f4\n\uf8f4\n1\n\uf8f4\n(1 \u2212 exp(\u2212 \u03bbij,n\nfij (x)))1l]\u2212\u221e,0[ (fij (x))\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nmij (\u0398)(x) =1 \u2212 \u03b1ij\nwith\n\n\uf8f1\nl\n\uf8f4\n1X\n\uf8f4\n\uf8f4\n\u03bb\n=\nfij (x)1l[0,+\u221e[ (fij (x)),\n\uf8f4\nij,p\n\uf8f2\nl t=1\nl\n\uf8f4\n1X\n\uf8f4\n\uf8f4\n\uf8f4\n\u03bb\n=\nfij (x)1l]\u2212\u221e,0[ (fij (x)).\n\uf8f3 ij,n\nl\n\n(22)\n\nt=1\n\nWe use here the conjunctive normalized rule (equation (14)).\nThus we can apply this rule in order to combine the n\nbasic belief assignments in the one-versus-rest case and the\nn(n \u2212 1)/2 basic belief assignments in the one-versus-one\ncase. When the data overlap a lot, more complicated rules\nsuch as proposed in [5] could be preferred.\nFor the decision step, we want to keep the possibility to\ntake the decision on a union of classes (i.e. when we can not\ndecide between two particular classes) and also to not take a\ndecision when our belief in one focal element is too weak.\nThus we propose the following decision rule in two steps:\n1) The decision rule of the maximum of the credibility with\nreject defined by the equation (16) is applied in order to\ndetermine the patterns that do not belong to the learning\nclasses.\n2) The decision rule given by the equation (17) is next\napplied to the non-rejected patterns.\nAnother possible decision process could be first the application of the decision rule given by the equation (17), and\nnext the decision rule of the maximum of the credibility with\nreject on the imprecise patterns that first belong to the unions\nof classes. On the illustrated data given in the next section,\nwe obtain similar results. We call this decision process (2-1)\nand the previous one (1-2).\nV. A PPLICATION\nA. Sonar data\nOur database contains 42 sonar images provided by the\nGESMA (Groupe d'Etudes Sous-Marines de l'Atlantique).\nThese images were obtained with a Klein 5400 lateral sonar\nwith a resolution of 20 to 30 cm in azimuth and 3 cm in range.\nThe sea-bottom depth was between 15 m and 40 m.\nSome experts have manually segmented these images giving\nthe kind of sediment (rock, cobble, sand, silt, ripple (vertical or\nat 45 degrees)), shadow or other (typically shipwrecks) parts\n\n\fon images. It is very difficult to discriminate the rock and the\nthe cobble and also the sand and silt. However, it is important\nfor the sedimentologists to discriminate the sand and the silt.\nThe type \"ripple\" can be some ripple of sand or ripple of\nsilt. Hence, with the point of view of the sedimentologists we\nconsider only the three classes of sediment: C1 =rock-cobble,\nC2 =sand and C3 =silt. And in order to evaluate our decision\nprocess, we take the ripple as the fourth class (C4 ) that is\nunlearned.\nEach image is cut off in tiles of size 32\u00d732 pixels (about\n6.5 meter by 6.5 meter). With these tiles, we keep 3500 tiles of\neach class with only one kind of sediment in the tile. Hence,\nour database is made of 4\u00d73500 tiles. We consider 2/3 of them\nfor the learning step (only for the three classes of sediment)\nand 1/3 of them for the test step (i.e. 1167 tiles for each kind\nof sediment).\nIn order to classify the tiles of size 32\u00d732 pixels, we\nfirst have to extract texture parameters from each tile. Here,\nwe choose the co-occurrence matrices approach [1]. The cooccurrence matrices are calculated by numbering the occurrences of identical gray level of two pixels. Six parameters\ngiven by Haralick are calculated: homogeneity, contrast estimation, entropy estimation, the correlation, the directivity, and\nthe uniformity. Concerning these six parameters, we calculate\ntheir mean on four directions: 0, 45, 90 and 135 degrees.\nThe problem for co-occurrence matrices is the non-invariance\nin translation. Typically, this problem can appear in a ripple\ntexture characterization. More features extraction approaches\ncan be used such as the run-lengths matrix, the wavelet\ntransform and the Gabor filters [1].\nWe use the libSVM [27], and after comparing several kernels, we have retained the radial basis function (with \u03b3 = 1/6\nwhere 6 is the dimension of the data) and we take weighting\nof the error C = 1 because of the data overlap.\nB. Results\nThe table I shows the results for the SVM classifier with\nthe strategies one-versus-one and one-versus-rest. We note that\nthere are many errors between the sand (C2 ) and silt (C3 ),\nthat are two homogeneous sediments. The ripple (C4 ), the\nunlearning class, is more heterogeneous than the sand and\nsilt, this why it is more classified as rock (C1 ). The table II\n%\nC1\nC2\nC3\nC4\n\nC1\n91.00\n7.11\n2.06\n65.13\n\none-vs-one\nC2\n8.83\n80.72\n30.42\n33.16\n\nC3\n0.17\n12.17\n67.52\n1.71\n\nC1\n84.40\n2.57\n0.86\n52.36\n\none-vs-rest\nC2\n15.08\n61.27\n22.71\n45.41\n\nprovides some similar results than the basic versions of the\nSVM (table I). Note that the strategy one-versus-rest provides\nmore errors between the sand and silt. This can be explained\nbecause the data overlap. In the rest of the paper we consider\nonly the one-versus-one strategy.\n%\nC1\nC2\nC3\nC4\n\nC1\n88.00\n4.80\n1.20\n56.38\n\none-vs-one\nC2\n11.91\n83.29\n32.05\n41.99\n\nC3\n0.09\n11.91\n66.75\n1.63\n\nC1\n91.51\n8.83\n2.06\n67.52\n\none-vs-rest\nC2\n6.18\n20.90\n5.23\n20.57\n\nC3\n2.31\n70.27\n92.71\n11.91\n\nTABLE II\nR ESULTS OF THE SVM CLASSIFIER WITH BELIEF FUNCTION THEORY FOR\nTHE BOTH STRATEGIES ONE - VERSUS - ONE AND ONE - VERSUS - REST.\n\nThe table III gives the results with possible decision on\nunions with r = 0.6. We can see that this kind of cautious\ndecision provides less hard errors (i.e. say one kind of sediment instead of another). Of course these results depend on\nthe values of r that provide a more or less cautious decision as\nwe can see on figure 3. If we add the possibility of rejection\nto these results (table IV), we can see that the most of rejected\ntiles come from the ripple (the unknown class C4 ). For a given\nclass, the rejected tiles come as a majority from the unions\n(imprecise data). Of course this rejection does not depend on\nthe r value if we begin by the rejection in our decision process\n(1-2) (presented in section III).\nFigure 3 shows the results of the classification of class\nof ripple (C4 ) according to the value of r without possible\nrejection. Of course when the value of r is weak the data of\nthe three learning classes are classified on the unions. We can\ndistinguished three kinds of work intervals on these data:\n\u2022 r \u2208 [0; 0.3]: The classifier is too undecided,\n\u2022 r \u2208 [0.4; 0.6]: the ambiguity between the classes is\ncorrectly considered,\n\u2022 r \u2208 [0.7; 1]: the decision is too hard.\n\nC3\n0.51\n36.16\n76.44\n2.21\n\nTABLE I\nR ESULTS OF THE SVM CLASSIFIER FOR THE BOTH STRATEGIES\nONE - VERSUS - ONE AND ONE - VERSUS - REST.\nFig. 3. Classification of the class of ripple (C4 ) with possible decision on\nunion according to r.\n\nshows the same results, but with the proposed approach based\non the belief function theory (presented in section III) with\nthe decision based on the pignistic probability. This approach\n\nAccording to the application, if we want to privilege the\nhard decision at the expense of the rejection, we can try to\n\n\f%\nC1\nC2\nC3\nC4\n\nC1\n76.69\n0.86\n0.35\n38.99\n\nC2\n6.86\n50.04\n17.05\n22.62\n\nC3\n0\n4.97\n53.21\n1.12\n\nC1 \u222a C2\n15.68\n11.83\n2.14\n33.16\n\nC1 \u222a C3\n0\n0\n0\n0\n\nC2 \u222a C3\n0.77\n32.30\n27.25\n4.11\n\nC1 \u222a C2 \u222a C3\n0\n0\n0\n0\n\nTABLE III\nR ESULTS WITH A BELIEF COMBINATION WITH POSSIBLE DECISION ON UNIONS .\n\n%\nC1\nC2\nC3\nC4\n\nC1\n76.69\n0.86\n0.34\n38.99\n\nC2\n6.26\n48.93\n15.42\n20.65\n\nC3\n0\n4.97\n53.22\n1.11\n\nC1 \u222a C2\n6.34\n1.63\n0.34\n9.68\n\nC1 \u222a C3\n0\n0\n0\n0\n\nC2 \u222a C3\n0.34\n19.45\n12.77\n1.63\n\nC1 \u222a C2 \u222a C3\n0\n0\n0\n1.71\n\nC4\n10.37\n24.16\n17.91\n27.93\n\nTABLE IV\nR ESULTS WITH A BELIEF COMBINATION WITH POSSIBLE DECISION ON UNIONS AND ON THE REJECTED CLASS .\n\ndecide first, possibly on the unions and next try to reject only\non the unions. In this case we can choose a higher value of\nr. For example with r = 0.8, we propose a comparison of\nthe decision processes (1-2) with (2-1) given in the table V.\nOf course the decision process (2-1) rejects less data, but it is\nonly with the rock (class C1 ) that we win, and we reject less\nripple (class unknown C4 ). Hence, it seems that the decision\nprocess (1-2) is better for this application.\nNow let's consider the tiles containing more than two\nkinds of sediment. We still learn the SVM classifier with\nthe same parameter and the one-versus-one strategy on the\nhomogeneous tiles of the three classes rock (C1 ), sand (C2 )\nand silt (C3 ) as previously. For the tests, we only take 299\ntiles with the classes: S1 =tiles with rock and sand, S2 =sand\nand silt, S3 =silt and ripple and S4 =sand and ripple.\nTable VI presents the obtained results of the SVM classifier\nwith the classical voting combination and a belief combination with pignistic decision and with credibility with reject\ndecision. For the two classes S1 and S2 , the tiles contain only\nlearning sediment (rock and sand for S1 and sand and silt for\nS2 ). For S1 and S2 the classifiers without reject classify these\ntiles more in sand. The rejection decreases the errors, but for\nS2 the rejection is essentially on the sand. The two classes S3\nand S4 contain ripple, the unknown class. Here also, we note\na confusion with the rock sediment that is an heterogeneous\ntexture like the ripple. The rejection for these two classes\nworks well, because a large part of the tiles classified in rock\nare rejected and for S3 a large part of tiles classified in sand\nare also rejected.\nTable VII shows the results with possible decision on the\nunion with r = 0.6, with and without possible rejection. The\naddition of the possible decision on the union reduces the\nerrors. The rejection is essentially on the tiles classified on\nthe unions, except for S2 (sand and silt) a lot of classifiedsand tiles are rejected, maybe because of the learning step.\nHence, for the tiles containing more than one kind of\nsediments our decision support could help the human experts.\nOf course, in this case, the evaluation is really difficult. In [2]\nwe have propose confusion matrices taking into account the\n\nproportion of each sediment in a tile.\n\nVI. C ONCLUSIONS\nWe have proposed an original approach based on the belief\nfunctions theory for the combination of binary classifiers\ncoming from the SVM with one-versus-one or one-versus-rest\nstrategies. The modelization of the basic belief assignments\nis proposed directly from the decision functions given by the\nSVM. These basic belief assignments allow to take correctly\ninto account the principle of the binary classification with\nSVM by comparison with an hyperplane in linear or nonlinear\ncases.\nThe belief functions theory provides a decision support\nwithout necessary deciding an exclusive class. The decision\nprocess that we have proposed with possible outliers rejection\nand with possible decision on the union of classes, is very\ninteresting because it works like the intuitive classification\nthat a human could perform based on the position of support\nvectors and considering the ambiguity of the classes. This decision support can really help experts for seabed characterization\nfrom sonar images. We have seen with the point of view of the\nsedimentologists that if we only consider the different kinds\nof sediments (rock, sand and silt), the ambiguity between the\nsand and the silt is well recognize and the ripple can be partly\nrejected.\n\n%\nS1\nS2\nS3\nS4\n\nC1\n27.4\n1.3\n40.1\n40.8\n\nvote\nC2 C3\n67.2 5.4\n40.5 58.2\n38.1 21.8\n58.2 1.0\n\nC1\n20.4\n0.3\n34.1\n31.8\n\npignistic\nC2 C3\n74.9 4.6\n44.8 55.9\n44.8 21.1\n67.2 1.0\n\nC1\n15.4\n0\n24.4\n22.7\n\nwith reject\nC2 C3 C4\n56.5 3.3 24.8\n14.4 47.8 37.8\n24.1 18.1 34.4\n51.2 1.0 26.1\n\nTABLE VI\nR ESULTS OF THE SVM CLASSIFIER WITH THE CLASSICAL VOTING\nCOMBINATION AND A BELIEF COMBINATION WITH PIGNISTIC DECISION\nAND WITH CREDIBILITY WITH REJECT DECISION .\n\n\f%\nC1\nC2\nC3\nC4\nC1\nC2\nC3\nC4\n\nC1\n82.86\n2.23\n0.69\n48.67\n87.75\n4.54\n1.20\n55.78\n\nC2\n6.77\n64.44\n20.48\n21.94\n4.20\n64.44\n20.48\n21.94\n\nC3\n0\n9.17\n60.92\n1.46\n0\n9.17\n60.93\n1.46\n\nC1 \u222a C2\n0\n0\n0\n0\n0.43\n0.34\n0.08\n1.20\n\nC1 \u222a C3\n0\n0\n0\n0\n0\n0\n0\n0\n\nC2 \u222a C3\n0\n0\n0\n0\n0\n0\n0\n0\n\nC1 \u222a C2 \u222a C3\n0\n0\n0\n0\n0\n0\n0\n0\n\nC4\n10.37\n24.16\n17.91\n27.93\n5.06\n21.51\n17.31\n19.62\n\nTABLE V\nR ESULTS WITH A BELIEF COMBINATION WITH POSSIBLE DECISION ON UNIONS AND ON THE REJECTED CLASS (1-2) AND WITH REJECTION ON THE\nUNION ONLY\n\n%\nS1\nS2\nS3\nS4\nS1\nS2\nS3\nS4\n\nC1\n8.03\n0\n16.05\n15.72\n8.03\n0\n16.05\n15.72\n\nC2\n49.50\n23.08\n22.41\n47.49\n48.16\n12.71\n21.40\n47.16\n\nC3\n1.67\n37.46\n15.38\n0.33\n1.67\n37.46\n15.38\n0.33\n\nC1 \u222a C2\n26.75\n3.34\n30.44\n31.44\n9.36\n0\n8.70\n7.02\n\n(2-1).\n\nC1 \u222a C3\n0\n0\n0\n0\n0\n0\n0\n0\n\nC2 \u222a C3\n14.05\n36.12\n15.72\n5.02\n8.03\n12.04\n5.02\n3.68\n\nC1 \u222a C2 \u222a C3\n0\n0\n0\n0\n0\n0\n0\n0\n\nC4\n24.75\n37.79\n33.45\n26.09\n\nTABLE VII\nR ESULTS WITH A\n\nBELIEF COMBINATION WITH POSSIBLE DECISION ON UNIONS WITH AND WITHOUT POSSIBLE REJECTION .\n\nR EFERENCES\n[1] A. Martin, \"Comparative study of information fusion methods for sonar\nimages classification,\" in The Eighth International Conference on Information Fusion, Philadelphia, USA, 25-29 July 2005.\n[2] A. Martin, \"Fusion for Evaluation of Image Classification in Uncertain\nEnvironments,\" in The Eighth International Conference on Information\nFusion, Florence, Italy, 10-13 July 2006.\n[3] H. Laanaya, A. Martin, D. Aboutajdine, and A. Khenchaf, \"Knowledge\nDiscovery on Database for Seabed Characterization,\" in Magrebian Conference on Information Technologies, Agadir, Morocco, 7-9 December\n2006.\n[4] J. Weston and C. Watkins, \"Support Vector Machines for Multi-Class Pattern Recognition Machines,\" in CSD-TR-98-04, Department of Computer\nScience, Royal Holloway, University of London, 1998.\n[5] A. Martin and C. Osswald, \"Toward a combination rule to deal with\npartial conflict and specificity in belief functions theory,\" in International\nConference on Information Fusion, Qu\u00e9bec, Canada, 2007.\n[6] A. Aregui and T. Denoeux, \"Fusion of one-class classifier in the belief\nfunction framework,\" in International Conference on Information Fusion,\nQu\u00e9bec, Canada, 2007.\n[7] B. Quost and T. Denoeux and M. Masson, \"Pairwise classifier combination\nusing belief functions,\" in Pattern Recognition Letters, vol. 28, pp. 644\u2013\n653, 2007.\n[8] J.C. Platt, \"Probabilities for SV Machines,\" in Advances in Large Margin\nClassifiers, ed. A.J. Smola, P. Barlett, B. Sch\u00f6lkopf and D. Schurmans,\nMIT Press, pp. 61\u201374, 1999.\n[9] C. Fr\u00e9licot, \"On unifing probabilistic/fuzzy and possibilistic rejectionbased classifier from possibilistic clustered noisy data,\" in Advance in\nPattern Recognition, series Lecture Notes in Computer Science, vol. 1451,\npp. 736\u2013745, 1998.\n[10] J. Liu and P. Gader, \"Neural networks with enhanced outlier rejection\nability for off-line handwritten word recognition,\" in Pattern Recognition,\nvol. 35, no. 10, pp. 2061\u20132071, 2002.\n[11] V.N. Vapnik, Statistical Learning Theory. John Wesley and Sons, 1998.\n[12] J. Milgram and M. Cherier and R. Sabourin, \"One-Against-One or\nOne-Against-All: Which One is Better for Hanwriting Recognition with\nSVMs?,\" in 10th International Workshop on Frontiers in Handwriting\nRecognition, La Baule, France, October 2006.\n[13] Y. Liu and Y.F. Zhen, \"One-Against-All Multi-Class SVM Classification\nUsing Reliability Measures,\" in IEEE International Joint Conference on\nNeural Networks, vol. 2, pp. 849\u2013854, 2005.\n\n[14] T.G. Dietterich and G. Bakiri, \"Solving Multiclass Learning Problems\nvia Error-Correcting Output Codes,\" in Journal of Artificial Intelligence\nResearch, vol. 2, pp. 263\u2013286, 1995.\n[15] A. Lauberts and D. Lindgren, \"Generalization Ability of a Support\nVector Classifier Applied to Vehicle Data in a Microphone Network,\" in\nInternational Conference on Information Fusion, Florence, Italia, 2006.\n[16] J.C. Platt and N. Cristianini and J. Shawe-Taylor, \"Large Margin\nDAGs for Multiclass Classification,\" in Advances in Neural Information\nProcessing Systems, vol. 12, pp. 547\u2013553, 2000.\n[17] A.P. Dempster, \"Upper and Lower probabilities induced by a multivalued\nmapping,\" in Annals of Mathematical Statistics, vol. 38, pp. 325\u2013339,\n1967.\n[18] G. Shafer, A mathematical theory of evidence.\nPrinceton University\nPress, 1976.\n[19] Ph. Smets, \"Constructing the pignistic probability function in a context\nof uncertainty,\" in Uncertainty in Artificial Intelligence, vol. 5, pp. 29\u201339,\n1990.\n[20] Ph. Smets, \"The Combination of Evidence in the Transferable Belief\nModel,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 12, no. 5, pp. 447\u2013458, 1990.\n[21] S. Le H\u00e9garat-Mascle and I. Bloch and D. Vidal-Madjar, \"Application\nof Dempster-Shafer Evidence Theory to Unsupervised Classification in\nMultisource Remote Sensing,\" in IEEE Transactions on Geoscience and\nRemote Sensing, vol. 35, no. 4, pp. 1018\u20131031, july 1997.\n[22] A. Appriou, \"Approche g\u00e9n\u00e9rique de la gestion de l'incertain dans les\nprocessus de fusion multisenseur,\" in Traitement du Signal, vol. 22, no 4,\npp. 307\u2013319, 2005.\n[23] H. Laanaya and A. Martin and D. Aboutajdine and A. Khenchaf,\n\"Seabed classification using belief multi-class support vector machines,\"\nin Marine Environment Characterization, Brest, France, october 2006.\n[24] H.T. Lin and C.-J. Lin and C. Weng, \"A Note on Platt's Probabilistic\nOutput for Support Vector Machines,\" in Machine Learning, 2008.\n[25] B. Sch\u00f6lkopf and J.C. Platt and J. Shawe-Taylor and A.J. Smola, \"Estimating the Support of a High-Dimensional Distribution,\" in Microsoft\nResearch, 1999.\n[26] Z. Elouedi and K. Mellouli and Ph. Smets, \"Assessing Sensor Reliability\nfor Multisensor Data Fusion Within The Transferable Belief Model,\"\nin IEEE Transactions on Systems, Man, and Cybernetics - Part B:\nCybernetics, vol. 34, no. 1, pp. 782\u2013787, 2004.\n[27] C.C.\nChang\nand\nC.J.\nLin,\n\"Software\navailable\nat\nhttp://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/,\" in LIBSVM: a library\nfor support vector machines, 2001.\n\n\f"}