{"id": "http://arxiv.org/abs/0710.4234v1", "guidislink": true, "updated": "2007-10-23T10:57:15Z", "updated_parsed": [2007, 10, 23, 10, 57, 15, 1, 296, 0], "published": "2007-10-23T10:57:15Z", "published_parsed": [2007, 10, 23, 10, 57, 15, 1, 296, 0], "title": "Stability of the Gibbs Sampler for Bayesian Hierarchical Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0710.4662%2C0710.4234%2C0710.3460%2C0710.0779%2C0710.0203%2C0710.0011%2C0710.5025%2C0710.4630%2C0710.4910%2C0710.3581%2C0710.2232%2C0710.2880%2C0710.4558%2C0710.2313%2C0710.5023%2C0710.2279%2C0710.1445%2C0710.5001%2C0710.2835%2C0710.2183%2C0710.3139%2C0710.3925%2C0710.5452%2C0710.1019%2C0710.3869%2C0710.2791%2C0710.3455%2C0710.5288%2C0710.2209%2C0710.5241%2C0710.5610%2C0710.0247%2C0710.2620%2C0710.1033%2C0710.0288%2C0710.2324%2C0710.0281%2C0710.1926%2C0710.1072%2C0710.4733%2C0710.3699%2C0710.5875%2C0710.0210%2C0710.1206%2C0710.4868%2C0710.0982%2C0710.3006%2C0710.0684%2C0710.4790%2C0710.2353%2C0710.5478%2C0710.1717%2C0710.2722%2C0710.0103%2C0710.0677%2C0710.0215%2C0710.3486%2C0710.4753%2C0710.5396%2C0710.2783%2C0710.2568%2C0710.1549%2C0710.4008%2C0710.2508%2C0710.3708%2C0710.1235%2C0710.1604%2C0710.1101%2C0710.3761%2C0710.3422%2C0710.4887%2C0710.1610%2C0710.1646%2C0710.3478%2C0710.2422%2C0710.3135%2C0710.2854%2C0710.4597%2C0710.3287%2C0710.4435%2C0710.4822%2C0710.2535%2C0710.3084%2C0710.1726%2C0710.5631%2C0710.4252%2C0710.0101%2C0710.4611%2C0710.2448%2C0710.4722%2C0710.4385%2C0710.4976%2C0710.3123%2C0710.1519%2C0710.4150%2C0710.4981%2C0710.2901%2C0710.5237%2C0710.1687%2C0710.0145%2C0710.1806&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Stability of the Gibbs Sampler for Bayesian Hierarchical Models"}, "summary": "We characterise the convergence of the Gibbs sampler which samples from the\njoint posterior distribution of parameters and missing data in hierarchical\nlinear models with arbitrary symmetric error distributions. We show that the\nconvergence can be uniform, geometric or sub-geometric depending on the\nrelative tail behaviour of the error distributions, and on the parametrisation\nchosen. Our theory is applied to characterise the convergence of the Gibbs\nsampler on latent Gaussian process models. We indicate how the theoretical\nframework we introduce will be useful in analyzing more complex models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0710.4662%2C0710.4234%2C0710.3460%2C0710.0779%2C0710.0203%2C0710.0011%2C0710.5025%2C0710.4630%2C0710.4910%2C0710.3581%2C0710.2232%2C0710.2880%2C0710.4558%2C0710.2313%2C0710.5023%2C0710.2279%2C0710.1445%2C0710.5001%2C0710.2835%2C0710.2183%2C0710.3139%2C0710.3925%2C0710.5452%2C0710.1019%2C0710.3869%2C0710.2791%2C0710.3455%2C0710.5288%2C0710.2209%2C0710.5241%2C0710.5610%2C0710.0247%2C0710.2620%2C0710.1033%2C0710.0288%2C0710.2324%2C0710.0281%2C0710.1926%2C0710.1072%2C0710.4733%2C0710.3699%2C0710.5875%2C0710.0210%2C0710.1206%2C0710.4868%2C0710.0982%2C0710.3006%2C0710.0684%2C0710.4790%2C0710.2353%2C0710.5478%2C0710.1717%2C0710.2722%2C0710.0103%2C0710.0677%2C0710.0215%2C0710.3486%2C0710.4753%2C0710.5396%2C0710.2783%2C0710.2568%2C0710.1549%2C0710.4008%2C0710.2508%2C0710.3708%2C0710.1235%2C0710.1604%2C0710.1101%2C0710.3761%2C0710.3422%2C0710.4887%2C0710.1610%2C0710.1646%2C0710.3478%2C0710.2422%2C0710.3135%2C0710.2854%2C0710.4597%2C0710.3287%2C0710.4435%2C0710.4822%2C0710.2535%2C0710.3084%2C0710.1726%2C0710.5631%2C0710.4252%2C0710.0101%2C0710.4611%2C0710.2448%2C0710.4722%2C0710.4385%2C0710.4976%2C0710.3123%2C0710.1519%2C0710.4150%2C0710.4981%2C0710.2901%2C0710.5237%2C0710.1687%2C0710.0145%2C0710.1806&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We characterise the convergence of the Gibbs sampler which samples from the\njoint posterior distribution of parameters and missing data in hierarchical\nlinear models with arbitrary symmetric error distributions. We show that the\nconvergence can be uniform, geometric or sub-geometric depending on the\nrelative tail behaviour of the error distributions, and on the parametrisation\nchosen. Our theory is applied to characterise the convergence of the Gibbs\nsampler on latent Gaussian process models. We indicate how the theoretical\nframework we introduce will be useful in analyzing more complex models."}, "authors": ["Omiros Papaspiliopoulos", "Gareth Roberts"], "author_detail": {"name": "Gareth Roberts"}, "author": "Gareth Roberts", "links": [{"href": "http://arxiv.org/abs/0710.4234v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0710.4234v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0710.4234v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0710.4234v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Submitted to the Annals of Statistics\n\nSTABILITY OF THE GIBBS SAMPLER FOR BAYESIAN\nHIERARCHICAL MODELS\n\narXiv:0710.4234v1 [stat.ME] 23 Oct 2007\n\nBy Omiros Papaspiliopoulos\u2217\n\nand Gareth Roberts\n\nUniversity of Warwick and Lancaster University\nWe characterise the convergence of the Gibbs sampler which samples from the joint posterior distribution of parameters and missing\ndata in hierarchical linear models with arbitrary symmetric error distributions. We show that the convergence can be uniform, geometric\nor sub-geometric depending on the relative tail behaviour of the error\ndistributions, and on the parametrisation chosen. Our theory is applied to characterise the convergence of the Gibbs sampler on latent\nGaussian process models. We indicate how the theoretical framework\nwe introduce will be useful in analyzing more complex models.\n\n1. Introduction. Hierarchical modelling is a widely adopted approach\nto constructing complex statistical models. The appeal of the method lies in\nthe simplicity in specifying a highly multivariate model by joining many simple and tractable models, the foundational justification based on the ideas of\npartial exchangeability, the flexibility to extend or simplify the model in the\nlight of new information, and the ease of inference using powerful Markov\nchain Monte Carlo (MCMC) methods which have been developed to this\nend during the last two decades. Thus, hierarchical models have been used\nin many areas of applied statistics such as geostatistics [8], longitudinal analysis [9], disease mapping [3], and financial econometrics [23] to name just a\nfew.\nResearch funded by EPSRC grant GR/S61577/01\nAMS 2000 subject classifications: Primary 65C05; secondary 60J27\n\n\u2217\n\nKeywords and phrases: Geometric ergodicity, capacitance, collapsed Gibbs sampler,\nstate-space models, parametrisation, Bayesian robustness\n\n1\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f2\n\nPAPASPILIOPOULOS AND ROBERTS\n\nA rather general form of a two-level hierarchical model is\nY\n(1)\n\n\u223c L(Y |X)\n\nX \u223c L(X|\u0398) ,\n\nwhere L(X) and L(Y | X) denote the distribution of X and the conditional\ndistribution of Y given X respectively. We will refer to Y as the data, X as\nthe missing data and \u0398 as the parameters. In a Bayesian context the model is\ncompleted by specifying a prior distribution for \u0398. Typically the dimension\nof X is much larger than that of \u0398 and it can increase with the size of the\ndata set. Most of the applications cited above fit into (1) by imposing the\nappropriate structure on L(Y | X) and L(X | \u0398). It is straightforward to\nconstruct models with more levels.\nBayesian inference for (1) involves the posterior distribution L(X, \u0398 |\nY = y). This is typically analytically intractable, but it can be sampled\nrelatively easily using the Gibbs sampler [29], by simulating iteratively from\nthe two conditional distributions L(X | \u0398, Y = y), and L(\u0398 | X, Y =\ny). It has been demonstrated both theoretically and empirically that the\nconvergence (to be formally defined in Section 3) of the Gibbs sampler relates\nto the structure of the hierarchical model and particularly to the dependence\nbetween the updated components, X and \u0398. Nevertheless, the exact way in\nwhich the model structure interferes with the convergence remains largely\nunresolved. Concrete theoretical results exist only for Gaussian hierarchical\nmodels, but we will see that these results do not extend to more general cases.\nAlthough interesting characterizations of the convergence rate in terms of the\ndependence between X and \u0398 exist when the Gibbs sampler is geometrically\nergodic [1], there exist no general results which establish geometric ergodicity\nfor the Gibbs sampler. The difficulty in obtaining such general results lies\nin the intrinsic dependence of the convergence of the Gibbs sampler on the\nmodel structure.\nIn this paper we show explicitly how the relative tail behaviour of L(Y |\nX) and L(X | \u0398) determines the stability of the Gibbs sampler, i.e. whether\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\fSTABILITY OF GIBBS SAMPLER\n\n3\n\nthe convergence is uniform, geometric or sub-geometric. Moreover, we show\nthat the relative tail behaviour dictates the type of parametrisation that\nshould be adopted. In order to retain tractability and formulate interpretable\nand easy to check conditions we restrict attention to the class of linear hierarchical models with general error distributions; the precise model structure\nis given in Section 2.1. Nevertheless, our main theoretical results, in particular Theorems 3.3, 3.4, 3.5 and 6.3, and the methodology for proving\nthem are expected to be useful in a much more general context than the one\nconsidered here.\nConsideration of the class of linear non-Gaussian hierarchical models is\nnot merely motivated by mathematical convenience. These models are very\nuseful in real applications, for example in longitudinal random effects modelling [9, 13], time series analysis [4, 12, 28] and spatial modelling [8]. They\nalso are a fundamental tool in the robust Bayesian analysis [7, 20, 22, 30].\nFurthermore, we will see that the stability of the Gibbs sampler for linear\nnon-Gaussian models is very different compared to the Gaussian case, the\nlocal dependence between X and \u0398 being crucial in the non-Gaussian case.\nNotice that several other models can be approximately written as linear nonGaussian models. Actually, this work has been motivated by the behaviour\nof MCMC for non-Gaussian Ornstein-Uhlnebeck stochastic volatility models\n[23].\nThe paper is organised as follows. Section 2.1 specifies the models we\nwill be concerned with and it establishes some basic notation. Section 2.2\ndiscusses Gibbs sampling under different parametrisations of the model and\nSection 2.3 motivates the theory and the methodology developed in this paper by a simple example. Section 3 is the theoretical core of this paper; the\nsection commences with a short review of stability concepts for the Gibbs\nsampler; Section 3.1 recalls the existing results for Gaussian linear models;\nSection 3.2 develops stability theory for hierarchical models and states three\nmain theorems for the stability of the Gibbs sampler; based on these theorems Section 3.3 provides the characterization of the stability of the Gibbs\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f4\n\nPAPASPILIOPOULOS AND ROBERTS\n\nsampler under different parametrisations for a broad class of linear hierarchical models; Section 3.4 considers an alternative augmentation scheme when\none of the error distributions is a scale mixture of normals and compares the\nconvergence of a three-component Gibbs sampler with that of its collapsed\ntwo-component counterpart. Section 4 extends the theory to hierarchical\nmodels which involve latent Gaussian processes. Section 5 discusses extensions and contains some practical guidelines. Section 6 contains the proofs\nof all theorems and propositions. The proofs are based on establishing geometric drift conditions and minorization conditions and using capacitance\narguments in conjunction with Cheeger's inequality.\n2. Models, parametrisations and motivation.\n2.1. Linear hierarchical models. The models we consider in this paper\nare of the following form, where Y i is mi \u00d7 1, Ci is mi \u00d7 p, Xi is p \u00d7 1, D\nis p \u00d7 1 and \u0398 is a scalar:\nYi = Ci Xi + Z1i ,\n(2)\n\ni = 1, . . . , m\n\nXi = D\u0398 + Z2i .\n\nZ1i , i = 1, . . . , m, are iid with distribution L(Z1 ), Z2i , i = 1, . . . , m, are iid\nwith distribution L(Z2 ), and L(Z1 ) and L(Z2 ) are symmetric distributions\naround 0 (a vector of 0s with the appropriate dimension). In the sequel,\nbold-face letters will correspond to vectors and matrices, capital letters to\nrandom variables and lower-case letters to their realisations. In this setting\nY = (Y1 , . . . , Ym ) and X = (X1 , . . . , Xm ). The first equation in (2) will be\ntermed the observation equation and the second the hidden equation.\nIt is often conveniently assumed that both L(Z1 ) and L(Z2 ) are Gaussian. However there are several applications where this assumption is clearly\ninappropriate, especially if we wish to make the inference about X robust\nin the presence of prior-data conflict. It is known [see e.g. 20, 22, 30, and\nreferences therein] that if the tails of L(Z1 ) are heavier than the tails of\nL(Z2 ) then inference for X is robust to outlying observations, whereas if\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\fSTABILITY OF GIBBS SAMPLER\n\n5\n\nL(Z2 ) has heavier tails than L(Z1 ) inference for X is less influenced by the\nprior in case of data-prior conflict; these robustness is absent from Gaussian\nmodels. This type of robust modelling has been undertaken in time-series\nanalysis, see for example [12].\n2.2. Gibbs sampling and parametrisations. As is common in this framework, we place an improper flat prior on \u0398, which in this context leads to a\nproper posterior. Bayesian inference for (2) involves the joint posterior distribution L(X, \u0398 | Y = y), which will abbreviate to L(X, \u0398 | Y). Although\nit is often analytically intractable, it can be sampled easily using the Gibbs\nsampler.\nThe parametrisation P0 := (X, \u0398) is termed the centred parametrisation.\nThis terminology was first used in the linear Gaussian context by [10]. Following [21] we shall use the term more generally to refer to a parametrisation where the parameters and the data are conditionally independent\ngiven the missing data. We can use the Gibbs sampler to collect samples\nfrom L(U, \u0398 | Y) where U = h(X, \u0398), for some invertible transformation\nh, and then transform the draws to obtain samples from L(X, \u0398 | Y). In\nthe rest of the paper we will use P to refer to a general parametrisation\n(U, \u0398). It is known [16] that the convergence (to be formally introduced\nin Section 3) of the Gibbs sampler improves as the dependence between\nthe updated components, U and \u0398, decreases. Hence, the development of\ngeneral re-parametrisation strategies has been actively researched, see [21]\nfor a recent account. In that work, the authors introduce the non-centred\nreparametrisation P1 := (X\u0303, \u0398), which replaces X with X\u0303 := h(X, \u0398), where\nh is a transformation which makes \u0398 and X\u0303 apriori independent. In the context of linear hierarchical models X\u0303 = (X\u03031 , . . . , X\u0303m ), where X\u0303i = h(Xi , \u0398),\nand h(x, \u03b8) := x \u2212 D\u03b8. We will see that P0 and P1 present two natural\nchoices.\nThe prolific expansion in the use of Gibbs sampling for inference in hierarchical models during the 1990s was fuelled by the apparent rapid convergence\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f6\n\nPAPASPILIOPOULOS AND ROBERTS\n\nof the algorithm in many cases. However, to date, there has been little theoretical analysis linking the stability of the Gibbs sampler to the structure of\nhierarchical models. A notable exception are the explicit convergence results\nfor Gaussian linear hierarchical models obtained in [24] and summarised in\nSection 3.1. The following example is revealing as to what might go wrong\nwhen considering non-Gaussian linear models, and motivates the methodology and theory developed in this article.\n2.3. A motivating example. Consider a simplified version of (1) where\nm = m1 = C1 = D = 1,\nY\n(3)\n\n= X + Z1\n\nX = \u0398 + Z2 .\n\nAssume that L(Z1 ) = Ca(0, 1), a standard Cauchy distribution, L(Z2 ) =\nN(0, 5), and y = 0 is observed. Figure 2.3a shows the sampled values of \u0398\nafter two independent runs of the Gibbs sampler, each of 104 iterations. The\ntop one is started from the mode, \u03980 = 0, and superficially it appears to\nbe mixing well: the autocorrelation in the series becomes negligible after 10\nlags, and most convergence diagnostic tests would assess that the chain has\nconverged. Nevertheless, the chain never exits the set (\u221240, 40), although\nthis is an event with stationary probability about 0.015. The second run,\nFigure 2.3a bottom, is started from \u03980 = 200, and the chain spends more\nthan 4,000 iterations wondering around \u03980 . The contour plot of the joint\nposterior log-density of X and \u0398 in Figure 2.3b, provides an explanation:\nthe contours look roughly spherical near the mode, but they become asymptotically concentrated around x = \u03b8 as |\u03b8| \u2192 \u221e. Thus, restricted to an area\naround the mode, X and \u0398 look roughly independent, but in the tails they\nare highly dependent. In fact, L(X \u2212 \u03b8 | Y, \u0398 = \u03b8) \u2192 N(0, 5) as |\u03b8| \u2192 \u221e,\nand we show in Section 3.3 that the Gibbs sampler which updates X and\n\u0398 converges sub-geometrically. In contrast, L(X\u0303 | Y, \u0398 = \u03b8) \u2192 L(X\u0303), as\n|\u03b8| \u2192 \u221e, and as we show in Section 3.3 the Gibbs sampler which updates\nX\u0303 and \u0398 is uniformly ergodic.\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f7\n\n\u221240\n\n20\n\n\u221220\n\n\u03b8\n\n0\n\n20\n\n40\n\nSTABILITY OF GIBBS SAMPLER\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n10000\n\n6000\n\n8000\n\n10000\n\n\u03b8\n\n\u221220\n\n300\n200\n\n\u221240\n\n100\n0\n\n\u03b8\n\n0\n\niteration\n\n0\n\n2000\n\n4000\n\n\u221240\n\niteration\n\n\u221220\n\n0\n\n20\n\n40\n\nX\n\n(a)\n\n(b)\n\nFig 1. (a): two runs of the Gibbs sampler under P0 for the model (3) started at \u03980 = 0\n(top) and \u03980 = 200 (bottom). (b): contours of the joint posterior log-density of X and \u0398.\n\n3. Convergence of the Gibbs sampler for linear hierarchical models. Given the parametrisation P = (U, \u0398), the two-component Gibbs\nsampler simulates iteratively from L(U|Y, \u0398 = \u0398n\u22121 ), and L(\u0398|Y, U =\nUn ), where \u03980 is a starting value and n \u2265 1 denotes the iteration number.\nThis algorithm generates a Markov chain {(Un , \u0398n )} with stationary distribution L(U, \u0398 | Y). The marginal chain {\u0398n } is also Markov and reversible\nwith respect to L(\u0398 | Y) (Lemma 3.1. of [16]). Moreover, it can be shown\n[26] that the convergence rate of the joint chain coincides with the convergence rate of the marginal chain, {\u0398n }. Notice that this result does not hold\nfor Gibbs samplers which update more than two components. In the sequel,\nfor any random variables W and V , and probability law \u03bc, we will use the\nshort-hand notation,\nL(V | W \u223c \u03bc) :=\n\nZ\n\nL(V | W = w)\u03bc(dw).\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f8\n\nPAPASPILIOPOULOS AND ROBERTS\n\nWe will consider the convergence of {\u0398n } through the total variation\nnorm, defined as\nkLh (\u0398n | Y, \u03980 ) \u2212 L(\u0398|Y)k = sup |Eh {g(\u0398n ) | Y, \u03980 } \u2212 E{g(\u0398)|Y}|.\n|g|\u22641\n\nLh (\u0398n | Y, \u03980 ) is the distribution of the chain after n steps started from\n\u03980 , and Eh {g(\u0398n ) | Y, \u03980 } is the expected value of a real bounded function\ng with respect to this distribution. Lh (\u0398n | Y, \u03980 ) clearly depends on the\nparametrisation U = h(X, \u0398), since,\nLh (\u03981 | Y, \u03980 ) = L{ \u0398\n\n|\n\nY, U \u223c L(U | Y, \u0398 = \u03980 )}.\n\nUnder standard regularity conditions (Theorem 13.0.1 of [19]) the total variation norm converges to 0 as n \u2192 \u221e. We say that {\u0398n } is geometrically\nergodic when there exist an r < 1 and some function M (*), such that\n(4)\n\nkLh (\u0398n | Y, \u03980 ) \u2212 L(\u0398|Y)k \u2264 M (\u03980 )r n .\n\nThe smallest r for which (4) holds, say rh , is known as the rate of convergence of {\u0398n }. However, the actual distance from stationarity will in general\ndepend on the starting point and this is represented by the term M (\u03980 ) in\n(4). When M (*) is bounded above, {\u0398n } is called uniformly ergodic. Uniform ergodicity is a valuable property, since it ensures that the convergence\nof the chain does not depend critically on the initial value chosen. Whilst\nthis does not guarantee rapid convergence, it ensures that the \"burn-in\"\nproblem cannot become arbitrarily bad from certain starting points.\nGeometric ergodicity is a qualitative stability property, and geometrically\nergodic algorithms may still converge slowly and give Monte Carlo estimates\nwith high variance (for example when rh \u2248 1). However, algorithms which\nfail to be geometrically ergodic can lead to various undesirable properties,\nincluding the break down of the central limit theorem for ergodic average estimates. In this case the simulation can be unreliable and the drawn samples\nmight poorly represent the target distribution.\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\fSTABILITY OF GIBBS SAMPLER\n\n9\n\nTo keep nomenclature simple we will identify a parametrisation P =\n(U, \u0398) with the Gibbs sampler which updates U and \u0398. Thus, we say that\na parametrisation P is geometrically (respectively uniformly) ergodic, if the\nGibbs sampler implemented using this parametrisation is geometrically (respectively uniformly) ergodic.\n3.1. Gaussian models. The Gibbs sampler for the Gaussian linear model\nis geometrically ergodic with rate given in [24]. In the simplified model (3)\nassume that L(Zi ) = N (0, \u03c3i2 ), i = 1, 2, and define \u03ba = \u03c322 /(\u03c322 + \u03c312 ). Then,\n[21] building on the results of [24] showed that, when U = h(X, \u0398) = X \u2212\u03c1\u0398,\n(5)\n\nrh := r\u03c1 =\n\n(\u03c1 \u2212 (1 \u2212 \u03ba))2\n= {corr(U, \u0398 | Y )}2\n\u03c12 \u03ba + (1 \u2212 \u03c1)2 (1 \u2212 \u03ba)\n\nwhich gives rise to the two special cases of interest, r0 = 1 \u2212 \u03ba, r1 = \u03ba. In\nthis setting, the dependence between U and \u0398 is appropriately quantified\nby the correlation coefficient, and (5) shows that the larger the correlation\nthe worse the convergence. Many refinements and generalizations of these\nresults can be found in [24], [21] and [17]. Notice that both P0 and P1 are\ngeometrically ergodic. P0 converges rapidly when the observation equation is\n\"more precise\" than the hidden equation, that is \u03c31 << \u03c32 , and it converges\nslowly when the hidden equation is relatively precise. P1 converges rapidly\nwhen the hidden equation is relatively more precise.\n3.2. General theory for linear hierarchical models. This section gives\ngeneral results which can be used to characterise the stability of the Gibbs\nsampler on linear hierarchical models of the form (2) where the Xi s are\nunivariate and D = 1. Our results are valid when m > 1 and mi > 1 (see\nRemark 1 in page 13), however in order to keep the notation simple we will\nwork with the simplified model (3), where all Y, X and \u0398 are scalars. L(Z1 )\nand L(Z2 ) are arbitrary symmetric distributions with continuous bounded\neverywhere positive densities, f1 and f2 respectively; common examples include the Gaussian, the Cauchy and the double exponential. This section\ngives the general results, while Section 3.3 applies them to characterise the\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f10\n\nPAPASPILIOPOULOS AND ROBERTS\n\nconvergence of the Gibbs sampler for (a broad class of) linear non-Gaussian\nhierarchical models. Section 4 deals with extensions where the Xi s are vectors of dependent variables, therefore covering state-space and spatial models. Nevertheless, the results even for the more structured models follow\nrelatively easily from the results of this section. All proofs are deferred to\nSection 6.\nWe begin by introducing a collection of posterior robustness concepts,\nwhich are related with the behaviour of the conditional posterior distribution L(U | Y, \u0398 = \u03b8) as |\u03b8| \u2192 \u221e. All these concepts have statistical\ninterpretations but they turn out to provide the required mathematical conditions for characterising the stability of the Gibbs sampler, as we show in\nTheorems 3.3, 3.4 and 3.5 below.\nDefinition 3.1.\n\nThe parametrisation P = (U, \u0398) is called:\n\n1. partially tight in parameter (PTIP), if for all y, there is some k > 0\nsuch that,\n(6)\n\nlim sup P(|U | > k|Y = y, \u0398 = \u03b8) < 1,\n|\u03b8|\u2192\u221e\n\n2. geometrically tight in parameter (GTIP), if there exist positive constants, a, b (independent of \u03b8) such that for all \u03b8,\nP(|U | > x|Y = y, \u0398 = \u03b8) \u2264 ae\u2212bx .\nGTIP not only implies that L(U | Y, \u0398 = \u03b8) is a tight family of distributions, but also that the tail probabilities are bounded exponentially. (We\nrecall that a family of distributions on the real line, say F\u03b8 , indexed by a\nscalar \u03b8, is called tight when limk\u2192\u221e sup\u03b8 F\u03b8 ([\u2212k, k]c ) = 0.) Clearly, GTIP\nis much stronger condition than PTIP. We consider also the following model\nrobustness concepts.\nDefinition 3.2.\n\nWe say that the linear hierarchical model (3) is\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\fSTABILITY OF GIBBS SAMPLER\n\n11\n\n1. robust in parameter (RIP), if\nlim L(X|Y = y, \u0398 = \u03b8) = L(Z1 + y),\n\n|\u03b8|\u2192\u221e\n\n2. robust in data (RID), if\nlim L(X\u0303|Y = y, \u0398 = \u03b8) = L(X\u0303),\n\n|\u03b8|\u2192\u221e\n\n3. data uniformly relevant (DUR), if there exist positive constants d, k\nsuch that for all |\u03b8| > k,\n|E{X|Y = y, \u0398 = \u03b8}| \u2264 |\u03b8| \u2212 d,\n4. parameter uniformly relevant (PUR), if there exist positive constants\nd, k such that for all |\u03b8| > k,\nsgn(\u03b8)E{X \u2212 y|Y = y, \u0398 = \u03b8} \u2265 d.\nThese definitions characterise the hierarchical model according to how\ninference for X (conditionally on \u0398 = \u03b8) is affected by a large discrepancy\nbetween the data y and the prior guess \u03b8. When the model is RIP inference\nfor X ignores \u03b8, and it is symmetric around y. Conversely, when the model\nis RID inference for X ignores the data and becomes symmetric around \u03b8.\nWhen the model is DUR (PUR) the data (the parameter) always influences\nthe conditional expectation of X. Notice that when the model is RIP P0 is\nPTIP (although not necessarily GTIP), and when it is RID P1 is PTIP. The\nexample in Section 2.3 describes a RID model. A model can be both DUR\nand PUR (for example the Gaussian linear model).\nTheorem 3.3.\n\nConsider the linear hierarchical model (3) where the er-\n\nror densities f1 and f2 are continuous, bounded and everywhere positive. If\nP0 (P1 ) is PTIP, then it is uniformly ergodic.\nTheorem 3.4.\n\nConsider the linear hierarchical model (3) where the er-\n\nror densities f1 and f2 are continuous, bounded and everywhere positive. If\nthe model is RID then P0 is not geometrically ergodic, and if the model is\nRIP then P1 is not geometrically ergodic.\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f12\n\nPAPASPILIOPOULOS AND ROBERTS\nDistribution\nCauchy\nDouble exponential\nGaussian\nExponential power distribution\n\nCode\nC\nE\nG\nL\n\nDensity g(x) up to proportionality\n\u03c3 2 /(1 + x2 )\nexp\n\b {\u2212|x|/\u03c3}\nexp \u2212(x/\u03c3)2 /2\n\b\nexp \u2212|x/\u03c3|\u03b2 , \u03b2 > 2\n\nTable 1\nDistributions for the error terms and their densities. In the paper they are coded\naccording to the letter in the middle column.\n\nThe proof Theorem 3.4 is based on the general Theorem 6.3 about Markov\nchains on the real line, which is stated and proved in Section 6.\nTheorem 3.5.\n\n1. If the model is DUR, P1 is GTIP, and L(Z2 ) has\n\nfinite moment generating function in a neighbourhood of 0, then P0 is geometrically ergodic. 2. If the model is PUR, P0 is GTIP, and L(Z1 ) has finite\nmoment generating function in a neighbourhood of 0, then P1 is geometrically ergodic.\nThe theorems are proved by establishing a geometric drift condition. The\nrequirements of GTIP for P1 (P0 ) and finite moment generating function\nfor L(Z2 ) (L(Z1 )) are in order to tilt exponentially the linear drift condition\nprovided by DUR (PUR).\n3.3. Characterising the stability of the Gibbs sampler according to the distribution tails of the error terms. In this section, building upon the general\ntheory of Section 3.2, we characterise the stability of the Gibbs sampler on\nthe linear hierarchical model (3) for different specifications of L(Z1 ), L(Z2 ).\nAlthough we consider the error distributions in Table 1, our proofs remain\nvalid for much broader families of distributions (see Remark 2 on page 13).\nNotice that the exponential power distribution contains both the Gaussian\n(\u03b2 = 2) and the double exponential (\u03b2 = 1) as special cases. Here we consider densities with tails lighter than Gaussian (\u03b2 > 2). For the use of this\ndistribution in Bayesian robustness see [5].\nWe shall specify linear models giving first L(Z1 ) and then L(Z2 ), for\ninstance the (C, E) model corresponds to (3) with Cauchy distribution for\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f13\n\nSTABILITY OF GIBBS SAMPLER\n\nL(Z2 )\n\nStability of P0\nL(Z1 )\nC\nE\nG\nC U\nU\nU\nE N G/U U\nG N\nG\nG\nL N\nG\nG\n\nL\nU\nU\nG\nG\n\nL(Z2 )\n\nStability of P1\nL(Z1 )\nC\nE\nG\nC U\nN\nN\nE U U/G G\nG U\nU\nG\nL U\nU\nG\n\nL\nN\nG\nG\nG\n\nTable 2\nStability P0 (left) and P1 (right) for the linear hierarchical model (3) for specifications of\nthe distribution of the error terms as in Table 1.\n\nZ1 , and double exponential distribution for Z2 . For each model we have two\nparametrisations, thus two algorithms, P0 and P1 . When we refer to the\nstability of an algorithm we shall write U, G, and N to refer to uniform,\ngeometric and non-geometric (i.e. sub-geometric) ergodicity, respectively.\nTheorem 3.6.\n\nThe stability P0 and P1 is given in Table 2.\n\nRemark 1. The determining factor in classifying the stability of a parametrisation is the tail behaviour of L(Z1 ) and L(Z2 ). Thus, Theorem 3.6 generalises to the case of multiple random effects and observations:\nYij\n\n= Xi + Z1ij , j = 1, . . . , mi\n\nXi = \u0398 + Z2i , i = 1, . . . , m\nwhere Z1** and Z2* are independently distributed identically to L(Z1 ) and\nL(Z2 ) respectively. This extension is immediate where obvious sufficient\nstatistics exist (the C and N cases). However, since proving formally the\nfull generalisation would be extremely tedious (although in the same lines\nas in Section 6), we do not attempt it here.\nRemark 2. The same results can be obtained when any of the distributions\nconsidered in Table 2 is replaced by another symmetric distribution with\nthe same tail behaviour, which possess a bounded continuous everywhere\npositive density.\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f14\n\nPAPASPILIOPOULOS AND ROBERTS\n\nRemark 3. Different results hold when a proper prior for \u0398 is imposed. In\nthis case the convergence improves.\nRemark 4. The results of Theorem 3.6 are independent of the actual value\nof y. This does not necessarily hold in other contexts.\nRemark 5. In the (E, E) model, the stability depends on the ratio of the\nscale parameters in L(Z1 ) and L(Z2 ). Depending on this ratio convergence\ncan be either geometric or uniform (see Section 6 for details).\nRemark 6. The following heuristic can be derived from Table 2: convergence\nof P0 is best when L(Z1 ) has lighter tails than L(Z2 ), and worst when it has\nheavier tails. The situation for P1 is the reverse. Both algorithms become\nmore stable the lighter the tails of L(Z1 ) and L(Z2 ) become.\n3.4. Convergence of the grouped Gibbs sampler. An alternative augmentation scheme and sampling algorithm can be adopted when one of the\nerror distributions, say L(Z2 ) for convenience, is Gaussian and the other,\nsay L(Z1 ), is a scale mixture of Gaussian distributions. Several symmetric distributions belong in this class, for instance the Student-t (thus the\nCauchy) and the double exponential [2]. In this case, Z1 can be represented\nas Z1 = V /Q, where V has a standard Gaussian distribution and Q is\npositive and independent of V . We can treat Q as missing data and construct a three-component Gibbs sampler which updates iteratively X, Q\nand \u0398 from their conditional distributions. (When X = (X1 , . . . , Xm ) then\nQ = (Q1 , . . . , Qm ) where Qi is independent from Qj for every i 6= j). A\nmajor computational advantage of this approach is that L(X | Y, \u0398, Q) is\nGaussian and it can be easily sampled. Notice that Q and \u0398 are independent given X, thus we can implement the Gibbs sampler using a grouped\nscheme [15] where \u0398 and Q are updated in one block. It is of interest to\nknow whether the convergence of this grouped Gibbs sampler is better than\nthe convergence of the collapsed Gibbs sampler (as defined in [15]), where Q\nhas been integrated out. The \"Three-schemes Theorem\" of [15] states that\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\fSTABILITY OF GIBBS SAMPLER\n\n15\n\nthe norm of the transition operator of the grouped Gibbs sampler is larger\nthan the one which corresponds to the collapsed Gibbs sampler. This result,\nhowever, is not enough to guarantee that the collapsed sampler will have\nbetter convergence rate.\nIn order to give a concrete answer, we consider the important special\ncase, where L(Z1 ) is the Cauchy distribution, therefore Q \u223c Ga(1/2, 1/2).\nWe have the following proposition, whose proof is based on Theorem 6.3.\nProposition 3.7.\n\nThe grouped Gibbs sampler is not geometrically er-\n\ngodic.\nThis result remains true for a number of random effects m > 1, and it\nwill hold for more general Student-t distributions. This result has important\npractical implications especially in algorithms for latent Gaussian models,\nconsidered in Section 4. It is also significant that it contrasts the result obtained by [27], who establishes geometric ergodicity for variance component\nmodels (of which the model considered here is a special case). However, the\nresult in [27] is true when the number of data Yij , mi , per random effect Xi\nis larger than some number bigger than one, whereas in Lemma 3.7 we take\nmi = 1.\n4. Latent Gaussian process models. In this section we consider a\nrather specific though useful model and demonstrate that the results of\nSection 3.2 can be extended quite readily to this context giving some clearcut conclusions and advice for practical implementation. The results below\nare certainly not the most general possible, but it is hoped that the method\nof proof will indicate how analogous models might be addressed.\nTheorem 4.1.\n\nConsider the latent Gaussian process model:\nY = X + Z1\nX = 1\u0398 + \u03a31/2 Z2\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f16\n\nPAPASPILIOPOULOS AND ROBERTS\n\nwhere Z1 = {Z11 , . . . Z1p } is a vector of independent and identically distributed standard Cauchy random variables, Z2 = {Z21 , . . . Z2p } is a vector\nof independent and identically distributed standard Gaussian random variables, and 1 is a vector of 1's. \u03a3 is assumed known and a flat is prior is\nassigned to \u0398. Then 1. P0 fails to be geometrically ergodic; 2. P1 is uniformly\nergodic.\nAs we remarked on page 13, the result holds when the Cauchy is generalised to a Student-t with any degrees of freedom. The MCMC for latent\nGaussian process models is often implemented using a different augmentation scheme. As in Section 3.4, we can augment the model with Q =\n(Q1 , . . . , Qp ), where L(Qi ) = Ga(1/2, 1/2). However, a similar argument as\nin the proof of Proposition 3.7 shows that the Gibbs sampler which updates\nX, Q and \u0398 is not geometrically ergodic.\nAs a numerical illustration we consider a linear non-Gaussian state-space\nmodel: X1 , . . . , Xp are consecutive draws from an AR(1) model, which are\nobserved with Cauchy error. We have simulated p = 100 data from this\nmodel using \u0398 = 0. The update of \u0398 given X is from a Gaussian distribution,\nhowever the update of X given \u0398 and Y is non-trivial. We update all the\nstates together using a highly efficient Langevin algorithm, see [6] for details.\nMoreover, we perform several updates of X for every update of \u0398 so that our\nresults are not critically affected by not being able to simulate directly from\nL(X | Y, \u0398). Figure 4 depicts our theoretical findings. P0 has a random\nwalk-like behaviour in the tails, whereas P1 returns rapidly to the modal\narea. On the other hand, P0 mixes better than P1 around the mode. Note\nthat the instability of P0 in the tails is not due to lack of information about\n\u0398 but due to the robustness properties of the model.\nIn this context it is definitely advisable to mix between P0 and P1 , i.e to\nuse a hybrid sampler which at every iteration with some probability updates\n(\u0398, X) and with the remaining probability it updates (\u0398, X\u0303). This hybrid\nsampler will inherit the uniform ergodicity from P1 but it will also mix well\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f17\n\n\u22120.5\n\n\u22121.0 \u22120.5\n\n0.0\n\n0.0\n\n0.5\n\n0.5\n\n1.0\n\n1.0\n\n1.5\n\nSTABILITY OF GIBBS SAMPLER\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n80\n\n100\n\n400\n300\n200\n100\n0\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\niteration\n\n500\n\niteration\n\n0\n\n20\n\n40\n\n60\n\niteration\n\n80\n\n100\n\n0\n\n20\n\n40\n\n60\niteration\n\nFig 2. Two runs of P0 (left) and P1 (right) with two different starting values: \u03980 = 0\n(top) and \u03980 = 500 (bottom).\n\naround the modal area.\n5. Discussion. We have obtained rigorous theoretical results for the\nstability of the Gibbs sampler which explores the posterior distribution arising from a broad class of linear hierarchical models. We have also proved\nresults regarding more complicated hierarchical models with latent Gaussian processes, and we have compared different sampling schemes. We have\nshown how the model structure dictates which parametrisation should be\nadopted for improving the convergence of the Gibbs sampler.\nOur results are certainly not the most general possible, though the method\nof proof we have used indicates clearly how analogous problems might be\naddressed. As an example of this, it is easy to extend the conclusions of\nTable 2 to the case where the light-tailed distributions are replaced by (say)\nuniform distributions on finite ranges. The robustness concepts of PTIP,\nGTIP, RIP and RID are already stated in a general form, while the concepts of DUR and PUR can be translated in a natural way using Lyapunov\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f18\n\nPAPASPILIOPOULOS AND ROBERTS\n\ndrift conditions. Families of models to which we are currently investigating\nextensions of our methods, include stochastic volatility models prevalent in\nfinance. This is the subject of on-going research by the authors.\nThe general heuristic is clear - the stability of the centred and non-centred\nalgorithms, P0 and P1 respectively, depends on the relative tail behaviour of\nL(Z1 ) and L(Z2 ), with the centred method being more stable when L(Z1 ) is\nrelatively light tailed, and the non-centered being more stable when L(Z2 )\nis relatively light tailed. An additional conclusion of Table 2 is that, as\nexpected, both algorithms possess comparatively more stable convergence\nproperties the lighter the tails of L(Z1 ) and L(Z2 ) become.\nThe main message of the paper for the MCMC practitioner is a positive\none: the competition between P0 and P1 works to the user's benefit. Our\nresults suggest that a combination of P0 and P1 is often desirable. When\nthe tails of the error distributions are very different we have found that\none of the algorithms might be very good for visiting the tails of the target\ndistribution whereas the other for exploring the modal area (as for example\nwe demonstrate in Figure 4). Therefore, it is advisable to use a hybrid Gibbs\nsampler which at every iteration with some probability updates (\u0398, X) and\nwith the remaining probability it updates (\u0398, X\u0303). Moreover, by linking the\nstability of the Gibbs sampler to the robustness properties of the hierarchical\nmodel we provide intuition which can be found useful for models outside the\nscope of this paper.\nAnother interesting product of this work is that linear re-parametrisations,\nwhich can substantially improve the convergence rate in (approximately)\nGaussian models, might be of little relevance when the tail behaviour of\nL(Z1 ) is very different from L(Z2 ). For example, in (C,G) model, where\nthe observation error is Cauchy and the prior for X is Gaussian, we can\nprove that the Gibbs sampler which updates U = X \u2212 \u03c1\u0398 and \u0398 is subgeometrically ergodic for all \u03c1 < 1, whereas it is uniformly ergodic for \u03c1 = 1\nas we already know from Theorem 3.6. This emphasizes the special role of\nP1 , which differs because of the prior independence it induces on X\u0303 and \u0398.\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\fSTABILITY OF GIBBS SAMPLER\n\n19\n\nThis result suggests that conditional augmentation (as in [18]) algorithms\nmight fail to be geometrically ergodic when P0 does.\nAll the results presented here are specific to the Gibbs sampler, however\nour findings are clearly relevant to contexts where certain direct simulation\nsteps have to be replaced by appropriate Metropolis-Hastings steps (as for\nexample in the simulation illustration in Section 4).\nIt is worth mentioning that once we have established geometric ergodicity\nfor an algorithm, it is important to obtain computable bounds on the rate\nof convergence. We have not attempted to do so, since it is outside the focus\nof this paper. For advances in this direction see for example [11, 27].\nOne interesting feature resulting from this paper is that the marginal\nchain {\u0398n } of the Gibbs sampler on linear non-Gaussian models often behaves asymptotically (i.e in the tails) like a random auto-regression of the\nform:\n\u0398n = \u03c1n \u0398n\u22121 + \u01ebn\nwhere \u03c1n is a random variable taking values in [0, 1], and \u01ebn is an error\nterm. For instance in the (G, G) case of Theorem 3.6 for P0 (P1 ) \u03c1n is\ndeterministically equal to r0 (r1 ) defined in Section 3.1. The cases where we\ndemonstrate that the algorithm is random-walk like correspond to taking\n\u03c1n = 1 (almost surely). Furthermore in a number of cases, \u03c1n is genuinely\nrandom. For instance, in the (E, E) case with identical rates, \u03c1n \u223c U [0, 1].\nIn the (C,C) case, we find that \u03c1n takes the value 0 or 1 with probabilities\ndetermined by the scale parameters of the Cauchy distributions involved.\nAn extension of our ideas is possible for hierarchical models with more\nlevels. For instance consider the linear structure given by\nY\n(7)\n\n= \u0398 1 + Z1\n\n\u0398i = \u0398i+1 + Zi+1 ,\n\ni = 1, . . . d \u2212 1 ,\n\nwith a flat prior on \u0398d . Since Y is the only information available, the posterior tails of \u03981 , \u03982 . . . become progressively heavier. If at any stage, Zi has\nlighter tails than Zi\u22121 , then whenever \u0398i\u22121 and \u0398i+1 strongly disagree, the\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f20\n\nPAPASPILIOPOULOS AND ROBERTS\n\nconditional distribution of \u0398i given Y, \u0398\u2212i will virtually ignore \u0398i\u22121 and\nhence the data. This will lead to potential instabilities in the chain in components \u0398i , \u0398i+1 , . . . , \u0398d . We call this phenomenon the quicksand principle,\nand this is the subject of ongoing investigation by the authors.\n6. Proofs of main results. In the sequel we will use \u03c0 to denote the\ndensity of any stationary measure, in particular \u03c0(\u03b8 | y) and \u03c0(x | y, \u03b8)\nwill be the Lebesgue densities of L(\u0398 | Y = y) and L(X | Y = y, \u0398 = \u03b8)\nrespectively. With p(*, *) we denote the transition density of a Markov chain,\nand with \u03980 and \u03981 the consecutive values of the marginal chain {\u0398n }.\nproof of Theorem 3.3. We show the result for P0 , since the corresponding result for P1 can be proved in an analogous way. In particular, we\nshow that when P0 is PTIP, the transition density of the the marginal chain\n{\u0398n }, is such that inf \u03b80 p(\u03b80 , \u03b81 ) > 0, and p is also continuous in \u03b81 . This\nguarantees uniform ergodicity by Theorem 16.0.2 of [19].\nZ\n\np(\u03b80 , \u03b81 ) =\n\u2265\n\nf2 (|x \u2212 \u03b81 |)\u03c0(x | y, \u03b80 )dx \u2265\n\nZ\n\nk\n\n\u2212k\n\nf2 (|x \u2212 \u03b81 |)\u03c0(x | y, \u03b80 )dx\n\ninf f2 (|x \u2212 \u03b81 |) P(|X| \u2264 k|Y = y, \u0398 = \u03b80 ),\n\n|x|\u2264k\n\nfor k such that (6) holds. Since f1 and f2 are everywhere positive, bounded\nand continuous, P(|X| \u2264 k|Y = y, \u0398 = \u03b80 ) is also positive and continuous\nin \u03b80 , therefore by the PTIP property it follows that inf \u03b80 P(|X| \u2264 k|Y =\ny, \u0398 = \u03b80 ) > 0. Moreover, inf |x|\u2264k f2 (|x \u2212 \u03b81 |), is positive and continuous in\n\u03b81 , thus the result follows.\n\n\u2294\n\u2293\n\nThe proof of Theorem 3.4 requires Theorem 6.3, hence it is proved on\npage 24. The proof of Theorem 3.5 requires the following lemmas.\n1. If (3) is DUR and the parametrisation (X\u0303, \u0398) is GTIP,\n\nLemma 6.1.\n\nthen for all sufficiently small \u03b1 > 0,\nn\n\nE e\u03b1X |Y, \u0398 = \u03b8\nn\n\nE e\u2212\u03b1X |Y, \u0398 = \u03b8\n\no\n\no\n\n\u2264 e\u03b1\u03b8 (1 \u2212 \u03b1d/2),\n\u2264 e\u2212\u03b1\u03b8 (1 \u2212 \u03b1d/2),\n\nfor \u03b8 > k\nfor \u03b8 < \u2212k,\n\nwhere k, d are defined in Definition 3.2.\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f21\n\nSTABILITY OF GIBBS SAMPLER\n\n2. If (3) is PUR and the parametrisation (X, \u0398) is GTIP, then for all\nsufficiently small \u03b1 > 0,\nn\n\nE e\u03b1(y\u2212X\u0303 ) |Y = y, \u0398 = \u03b8\nn\n\nE e\u2212\u03b1(y\u2212X\u0303 ) |Y = y, \u0398 = \u03b8\n\no\n\no\n\n\u2264 e\u03b1\u03b8 (1 \u2212 \u03b1d/2),\n\nfor \u03b8 > k\n\n\u2264 e\u2212\u03b1\u03b8 (1 \u2212 \u03b1d/2),\n\nfor \u03b8 < \u2212k,\n\nProof. 1. We will prove only the first inequality, for \u03b8 > k, since the\nn\n\nother is proved in a similar fashion. We define G\u03b8 (t) = E et\n\n(X\u2212\u03b8)\n\no\n\n| Y, \u0398 = \u03b8 ,\n\nwhich is finite for all sufficiently small t > 0, say 0 < t < t0 for some t0 ,\n\nand for all \u03b8, since by the GTIP assumption L(|X \u2212 \u03b8| | Y, \u0398 = \u03b8) has exponential or lighter tails. By a second order Taylor series expansion of G\u03b8 (t)\naround t = 0, we obtain for some 0 < t1 < t0 , and for \u03b8 > k,\nG\u03b8 (t) = 1 + t E{X \u2212 \u03b8 | Y, \u0398 = \u03b8} +\n\u2264 1 \u2212 td +\n\nt2 n\nE (X \u2212 \u03b8)2 et1\n2\n\n(X\u2212\u03b8)\n\n|Y, \u0398 = \u03b8\n\no\nt2 n\nE (X \u2212 \u03b8)2 et1 (X\u2212\u03b8) |Y, \u0398 = \u03b8 .\n2\n\no\n\nn\n\no\n\nNow pick \u03b1 < t1 small enough so that for all \u03b8 > k \u03b1E (X \u2212 \u03b8)2 et1 (X\u2212\u03b8) |Y, \u0398 = \u03b8 <\nd. Such \u03b1 exists due to the GTIP assumption. Then, G\u03b8 (\u03b1) \u2264 1 \u2212 \u03b1d/2, and\nthe result follows. 2. It is proved as 1, recognising that X\u0303 = X \u2212 \u03b8.\nLemma 6.2.\n\n\u2294\n\u2293\n\n1. If (3) is DUR and the parametrisation (X\u0303, \u0398) is GTIP,\n\nthen for all sufficiently small \u03b1 > 0,\nn\n\nE e\u03b1|X| |Y, \u0398 = \u03b8\n\no\n\n\u2264 e\u03b1|\u03b8| (1 \u2212 \u03b1d/2) + K,\n\nfor |\u03b8| > k,\n\nwhere k, d are defined in Definition 3.2, and 0 < K < \u221e.\n2. If (3) is PUR and the parametrisation (X, \u0398) is GTIP, then for all\nsufficiently small \u03b1 > 0,\nn\n\nE e\u03b1|y\u2212X\u0303| |Y = y, \u0398 = \u03b8\n\no\n\n\u2264 e\u03b1|\u03b8| (1 \u2212 \u03b1d/2) + K,\n\nfor |\u03b8| > k,\n\nwhere k, d are defined in Definition 3.2, and 0 < K < \u221e.\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f22\n\nPAPASPILIOPOULOS AND ROBERTS\n\nProof. 1. We prove the result for \u03b8 > 0 exploiting the first inequality\ngiven in Lemma 6.1. The case \u03b8 < 0 is proved analogously but exploiting\nthe second inequality of Lemma 6.1. Notice that\nn\n\no\n\nn\n\no\n\nE e\u03b1|X| |Y, \u0398 = \u03b8 \u2264 E e\u03b1X |Y, \u0398 = \u03b8 +\n\nZ\n\n0\n\ne\u2212\u03b1x \u03c0(x | y, \u03b8)dx,\n\n\u2212\u221e\n\nthus, due to Lemma 6.1 we only need to show that the second term of\nthe sum above can be bounded above for all \u03b8. Recall a, b from the GTIP\nDefinition 3.2. Choose \u03b1 < b. Using integration by parts, we find that the\nsecond summand is bounded above by, e\u2212b\u03b8 [a+\u03b1/(b\u2212\u03b1)], which can easily be\nbounded above for all \u03b8 > k. 2. It is proved as 1, recognising that X\u0303 = X \u2212\u0398.\n\u2294\n\u2293\nproof of Theorem 3.5 1. We prove the result establishing a geometric\ndrift condition for the marginal chain {\u0398n }, using the function V (\u03b8) = e\u03b1|\u03b8| ,\nfor appropriately chosen \u03b1 > 0. Notice first that L(\u0398 | Y, X = x) \u2261 L(\u0398 |\nX = x) is symmetric around x and has a finite moment generating function\nin a neighbourhood of the origin. Thus, working as in Lemma 6.1 and Lemma\n6.2, we can show that for all sufficiently small \u03b1 > 0, there exists K1 > 0\nand \u01eb > 0, such that,\n\u0010\n\n\u0011\n\nE{e\u03b1|\u0398| | X = x} \u2264 1 + \u03b12 \u01eb e\u03b1|x| + K1 .\nThen, for |\u03b80 | > k, and appropriate K1 > 0, K > 0,\nE{e\u03b1|\u03981 | | Y, \u03980 = \u03b80 } = E{E{e\u03b1|\u03981 | | X1 } | Y, \u03980 = \u03b80 }\n\u2264 E{(1 + \u03b12 \u01eb)e\u03b1|X1 | + K1 | Y, \u03980 = \u03b80 }\n\u2264 (1 + \u03b12 \u01eb)(1 \u2212 \u03b1d/2)e\u03b1|\u03b80 | + K\n\u2264 (1 \u2212 \u03b1\u03b4)e\u03b1|\u03b80 | + K.\nNow since standard arguments (see for example [25]) show that compact sets\nare small for this problem, the Gibbs sampler is shown to be geometrically\nergodic by Theorem 15.0.1 of [19].\n2. The second result is proved almost identically. Notice that L(\u0398 | Y =\ny, X\u0303 = x) is symmetric around y \u2212 x and possesses finite moment generating\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f23\n\nSTABILITY OF GIBBS SAMPLER\n\nfunction in a neighbourhood of 0, thus as we showed above, for all sufficiently\nsmall \u03b1 > 0, there exists a K1 > 0 such that,\n\u0010\n\n\u0011\n\nE{e\u03b1|\u0398| | Y = y, X\u0303 = x} \u2264 1 + \u03b12 \u01eb e\u03b1|y\u2212x| + K1 .\nUsing Lemma 6.2 and arguing as in 1 proves the theorem.\n\n\u2294\n\u2293\n\nBefore proving Theorems 3.4 and 3.6 we need the following general result\nabout Markov chains on the real line.\nTheorem 6.3.\n\nLet {Wn } be an ergodic and reversible with respect to\n\na density \u03c0, Markov chain on R with transition density p(x, y) which is\nrandom walk-like in the tails, in the sense that there is a continuous positive\nsymmetric density q such that\n(8)\n\nlim p(x, x + z) = q(z), z \u2208 R.\n\n|x|\u2192\u221e\n\nThen\n1. \u03c0 has heavy tails, in the sense that\n(9)\n\nlim\n\nlog\n\nx\u2192\u221e\n\nR\u221e\nx\n\nlog\n\u03c0(u)du\n= lim\nx\u2192\u221e\nx\n\nR \u2212x\n\n\u2212\u221e \u03c0(u)du\n\n\u2212x\n\n=0;\n\n2. {Wn } is not geometrically ergodic.\nproof 1. We will prove the result for x \u2192 \u221e, since the case x \u2192 \u2212\u221e, is\nproved in the same way. Fix z, \u03b4 \u2208 R+ , and let W denote a random variable\nwhich has density \u03c0. By (8), there exists k > 0 such that for x > k\np(x + z, x)\n\u2264 (1 + \u03b4) .\np(x, x + z)\nThis uses the fact that q(z) > 0. Thus by reversibility, and for x > k,\n\u03c0(x)\np(x + z, x)\n=\n\u2264 (1 + \u03b4) ,\n\u03c0(x + z)\np(x, x + z)\nso that\n(10)\n\n\u03c0(x + z) \u2265 (1 + \u03b4)\u22121 \u03c0(x) .\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f24\n\nPAPASPILIOPOULOS AND ROBERTS\n\nIntegrating (10) over x > k, gives that\n(11)\n\nP(W > k + z) \u2265 (1 + \u03b4)\u22121 P(W > k) .\n\nIterating this expression, and after some algebra, we get that\nlim\n\nn\u2192\u221e\n\nlog P(W > k + nz)\n\u2265 \u2212\u03b4,\nn\n\nwhich, since \u03b4 can be chosen arbitrarily small, proves the statement.\n2. The second follows from the following standard capacitance argument;\nsee [25] for similar arguments for MCMC algorithms and [14] for an introduction to Cheeger's inequality using capacitance. Cheeger's inequality for\nreversible Markov chains implies that geometric ergodicity must fail if we\ncan find k > 0, such that the probability\n\u0010\n\nP |W1 | \u2264 k | W0 \u223c \u03c0(\u2212k,k)c\n\n\u0011\n\nis arbitrarily small, where we use \u03c0(\u2212k,k)c to denote the density \u03c0 restricted\nand re-normalised to the set {|x| > k}. Notice that (11) implies that for\nsufficiently large k, for |x| > k, and any l > 0, there\nP(|W1 | > x + l|W0 > k) \u2265 (1 + \u03b4)\u22121 \u2265 1 \u2212 \u03b4 .\nNow choose l sufficiently large that\n\nR\u221e\nl\n\nq(u)du < \u03b4 then for all |x| > k,\n\nP (|W1 | < k) \u2264 P(|W1 | < k | W0 \u223c \u03c0(\u2212k,k)c ) + P(|W1 \u2212 W0 | > l)\nwhich converges as |x| \u2192 \u221e to a limit bounded by 3\u03b4. Since \u03b4 is arbitrary,\nthe result is proved.\n\n\u2294\n\u2293\n\nproof of Theorem 3.4 we prove the theorem for the case where the\nmodel is RID, since the proof when the model is RIP is identical. We will\nshow that under the assumptions the marginal chain {\u0398n } generated by\nthe centred Gibbs sampler is random walk-like, thus by Theorem 6.3 P0 is\nnot geometrically ergodic. By assumption, lim|\u03b8|\u2192\u221e L(X\u0303|Y, \u0398 = \u03b8) = L(X\u0303),\nwhich is symmetric around 0, and let F denote its corresponding distribution\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f25\n\nSTABILITY OF GIBBS SAMPLER\n\nfunction. Therefore P(X \u2264 \u03b8 + z | Y, \u0398 = \u03b8) \u2192 F (z), as |\u03b8| \u2192 \u221e. Notice\nthat,\np(\u03b80 , \u03b80 +z) =\n\nZ\n\nf2 (|x\u2212\u03b80 \u2212z|)dF (x | Y, \u0398 = \u03b80 ) =\n\ntherefore, since f2 is bounded, p(\u03b80 , \u03b80 + z) \u2192\n\nR\n\nZ\n\nf2 (|u\u2212z|)dF (u+\u03b80 | Y, \u0398 = \u03b80 ),\n\nf2 (|u \u2212 z|)dF (u) = q(z), as\n\n|\u03b80 | \u2192 \u221e, where q is a symmetric density around 0.\n\n\u2294\n\u2293\n\nproof of Theorem 3.6 Throughout the proof we shall use the following\nnotation: f1 and f2 denote the density of Z1 and Z2 respectively (at least\nup to proportionality), and we define\nf\u03b8 (x) = f1 (|y \u2212 x|)f2 (|x \u2212 \u03b8|),\nthus, \u03c0(x | y, \u03b8) = f\u03b8 (x)/c\u03b8 , where c\u03b8 is the normalisation constant. Any\nscale parameter involved in fi will be denoted by \u03c3i , i = 1, 2.\nFor each model, we first prove the result for P0 and subsequently for P1 .\nWe will prove the statements corresponding to the upper triangular elements\nof the P0 and P1 tables. This is without loss of generality, since we can write\n(3) as\nX\u0303 = Y \u2212 \u0398 \u2212 Z1\nX\u0303 = Z2 .\nSince the actual value of Y does not affect convergence (as can be verified\nby our proofs below), we may as well set it to be 0, and since L(Z1 ), L(Z2 )\nare symmetric around 0, the model written above under a non-centred\nparametrisation coincides with (3) under a centred parametrisation but with\nthe error distributions interchanged. We first prove the results concerning\nthe diagonal elements.\nThe (C, C) model\nWe prove the result by verifying the PTIP property. The result then follows by Theorem 3.3. Notice that in this model, c\u03b8 =\n\nR\u221e\n\n\u2212\u221e f\u03b8 (x)dx\n\n=\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f26\n\n2\n\nPAPASPILIOPOULOS AND ROBERTS\n\nR (y+\u03b8)/2\n\u2212\u221e\n\nf\u03b8 (x)dx. We show that P0 is PTIP by demonstrating that for\n\narbitrary k > 0,\nlim inf\n|\u03b8|\u2192\u221e\n\nZ\n\ny+k\n\nf\u03b8 (x)/c\u03b8 dx > 0 .\n\ny\u2212k\n\nBy symmetry, it is enough to prove this statement for large positive \u03b8 values,\nso from now on we shall assume that \u03b8 > y.\nFor x < (y + \u03b8)/2, 1 + (y \u2212 \u03b8)2 \u2264 1 + 4(x \u2212 \u03b8)2 \u2264 4(1 + (x \u2212 \u03b8)2 ), so that\nc\u03b8 \u2264 4/\u03c0(1 + (y \u2212 \u03b8)2 ). Moreover, notice that when x \u2208 (y \u2212 k, y + k), then\nthere exist a d > 0 (depending on k, y), such that for all \u03b8 > d,\n1 + (y \u2212 \u03b8)2\n1 + (y \u2212 \u03b8)2\n\u2265\n\u2265 1/2.\n1 + (x \u2212 \u03b8)2\n1 + (y + k \u2212 \u03b8)2\nTherefore, for \u03b8 > d,\nZ\n\ny+k\n\ny\u2212k\n\nf\u03b8 (x)/c\u03b8 dx \u2265\n\u2265\n\n1 + (y \u2212 \u03b8)2\ndx\n2\n2\ny\u2212k 4\u03c0(1 + (y \u2212 x) )(1 + (x \u2212 \u03b8) )\nZ\n1\n1 y+k\n> 0,\n8 y\u2212k \u03c0(1 + (y \u2212 x)2 )\n\nZ\n\ny+k\n\nwhich proves the result. The result for P1 is proved identically.\nThe (E, E) model\nWithout loss of generality we assume that f1 (x) \u221d exp{\u2212|x|}, and f2 (x) \u221d\nexp{\u2212|x|/\u03c3}, \u03c3 > 0. The stability of the Gibbs sampler depends on whether\n\u03c3 < 1, \u03c3 = 1 or \u03c3 > 1, thus we consider these cases separately. Again by\nsymmetry it is enough to consider y < \u03b8.\n1. \u03c3 = 1: here we can write\n\nf\u03b8 (x) =\n\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\n1 2x\u2212y\u2212\u03b8\n,\n4e\n1 \u2212(\u03b8\u2212y)\n,\n4e\n1 y+\u03b8\u22122x\n,\n4e\n\nx<y\ny\u2264x\u2264\u03b8\nx > \u03b8.\n\nFrom this it is easy to demonstrate that E(\u03981 |\u03980 = \u03b80 ) = (y + \u03b80 )/2.\nSince all compact sets are small for the Markov chain {\u0398n } this is\nenough to demonstrate geometric ergodicity by Theorem 15.0.1 of [19].\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f27\n\nSTABILITY OF GIBBS SAMPLER\n\n2. \u03c3 > 1: here we can write:\n\nf\u03b8 (x) =\n\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\n1 (1+\u03c3)x\u2212y\u2212\u03c3\u03b8\n,\n4e\n1 y\u2212\u03c3\u03b8+(\u03c3\u22121)x\n,\n4e\n1 y+\u03c3\u03b8\u2212(1+\u03c3)x\n,\n4e\n\nx<y\ny\u2264x\u2264\u03b8\nx > \u03b8.\n\nDirect algebra shows that\nE{X\u2212\u03b8 | Y, \u0398 = \u03b8} = p1 (\u03b8)(Y \u22121)+[p2 (\u03b8)+p3 (\u03b8)\u22121]\u03b8+p2 (\u03b8)r(\u03b8)+\n\np3 (\u03b8) p2 (\u03b8)\n\u2212\n,\n\u03c3+1 \u03c3\u22121\n\nwhere p1 (\u03b8) + p2 (\u03b8) + p3 (\u03b8) = 1, and as \u03b8 \u2192 \u221e, p2 (\u03b8) \u2192 (\u03c3 +\n1)/(2\u03c3), p1 (\u03b8) \u2192 0, r(\u03b8) \u2192 0. Therefore,\nlim E{X \u2212 \u03b8|Y, \u0398 = \u03b8} \u2264\n\n\u03b8\u2192\u221e\n\n\u22122\n,\n\u22121\n\n\u03c32\n\nand the model is DUR. Since P1 is easily seen to be GTIP, by part 1\nof Theorem 3.5, P0 is geometrically ergodic.\n3. \u03c3 < 1: Here, in an analogous way to the above, we can demonstrate\nthat P0 is RIP therefore by Theorem 3.3, P0 is uniformly ergodic.\nDue to symmetry, the results for P1 are proved in a similar fashion,\nnotice however, that P1 is uniformly ergodic when \u03c3 > 1.\nThe (G, G) model\nThis is covered in [21, 24] and reviewed in Section 3.1.\nThe (L, L) model\nWe assume that f1 (x) \u221d exp{\u2212|x/\u03c31 |\u03b2 }, f2 (x) \u221d exp{\u2212|x/\u03c32 |\u03b2 }, and we let\na = \u03b2/(\u03b2 \u2212 1). Again by symmetry we just consider the case y < \u03b8. For large\n\u03b8, L(X|Y, \u0398 = \u03b8) converges weakly and in L1 to a point mass at \u03c1\u03b8 +(1\u2212\u03c1)y\nwhere\n\u03c1=\n\n\u03c31\u2212a\n.\n\u03c32\u2212a + \u03c31\u2212a\n\nAs a result, neither P0 nor P1 are GTIP, so it is not possible to establish\ngeometric ergodicity using the DUR and PUR properties (which hold for\nthis model) in conjunction with Theorem 3.5. Instead, we have to construct\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f28\n\nPAPASPILIOPOULOS AND ROBERTS\n\ndirectly a geometric drift condition. However, this is rather easy. Notice that\nsince L(\u0398 | X = x) is symmetric around x, we can find a b > 0 such that\nE{|\u0398| | X = x} \u2264 |x| + b. Moreover, for any \u01eb > 0, there is some k > 0, such\nthat for all \u03b8| > k, E{|X \u2212 y| | Y = y, \u0398 = \u03b8} \u2264 (1 + \u01eb)\u03c1|\u03b8 \u2212 y|, thus\nE{|\u03981 \u2212 y| |\u03980 = \u03b80 } \u2264 b + \u03c1(1 + \u01eb)|\u03b80 \u2212 y|\nwhich implies geometric ergodicity for P0 since compact sets can easily be\nseen to be small. The result for P1 is proved identically.\nThe (C, G), (E, C) and (L, C) models\nWe show that the model is RIP, therefore since P0 is PTIP, by Theorem\n3.3 P0 is uniformly ergodic, and by Theorem 3.4 P1 is not geometrically\nergodic. Notice, however, that for any x, using dominated convergence we\ncan show that c\u03b8 /f2 (|x \u2212 \u03b8|) \u2192 1, as |\u03b8| \u2192 \u221e. The argument is that, for\nany u, f2 (|u \u2212 \u03b8|)/f2 (|x \u2212 \u03b8|) \u2192 1, and the ratio is bounded above (as a\nfunction of \u03b8) by a function of u which is integrable with respect to f1 , as\nlong as f1 has exponential tails or lighter, which is the case in the models\nconsidered here. However, since f\u03b8 /c\u03b8 \u2192 f1 (|y\u2212x|), and this limit is a proper\ndensity, it follows that the corresponding distribution functions converge and\nL(X | Y = y, \u0398 = \u03b8) \u2192 L(|Z1 \u2212 y|) as |\u03b8| \u2192 \u221e.\nThe (G, E) model\nCalculations show that\nlim L(X|Y, \u0398 = \u03b8) = N (y+\u03c312 /\u03c32 , \u03c312 ), and lim L(X|Y, \u0398 = \u03b8) = N (y\u2212\u03c312 /\u03c32 , \u03c312 ),\n\n\u03b8\u2192\u221e\n\n\u03b8\u2192\u2212\u221e\n\ntherefore P0 is PTIP (but not RIP) and by Theorem 3.3 uniformly ergodic.\nThe above result, however, shows that the model is PUR, and since all\nconditions of Theorem 3.5 are satisfied, P1 is geometrically ergodic.\nThe (L, E) model\nThe result is proved as above.\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f29\n\nSTABILITY OF GIBBS SAMPLER\n\nThe (L, G) model\nHere (perhaps surprisingly) P0 is not PTIP but the model is DUR and PUR,\nand both P0 and P1 are GTIP so that Theorem 3.5 can be applied.\n\u2294\n\u2293\n\nproof of Lemma 3.7 Consider the Gibbs sampler with initial value\nX0 which updates (\u0398, Q) first and then X. Direct calculation gives that\nL(Q | Y = y, X = x, \u0398 = \u03b8) = Ga(1, (y \u2212 x)2 /2), L(X | Y = y, \u0398 =\n\u03b8, Q = q) = N (\u03b8/(q + 1) + qy/(q + 1), 1/(q + 1)), therefore L(X1 \u2212 X0 | Y =\ny, Q1 = q) = N (q(y \u2212 X0 )/(q + 1), 1 + 1/(q + 1)). However, since q \u2192 0 in\nprobability, when X0 \u2192 \u221e, the algorithm is random walk-like in the tails\nand by Theorem 6.3 fails to be geometrically ergodic.\n\n\u2294\n\u2293\n\nproof of Theorem 4.1 It is easy to demonstrate that the model is RID,\nlim L(X\u0303|Y, \u0398 = \u03b8) = Np (0, \u03a3) .\n\n|\u03b8|\u2192\u221e\n\nTherefore, P1 is PTIP and by Theorem 3.3 is uniformly ergodic. Since\n\u0398|X \u223c\n\n1\u03a3\u22121 X1\n1\n,\n\u22121\n1\u03a3 1 1\u03a3\u22121 1\n\n!\n\nthis implies that for the Gibbs sampler using P0 ,\n2\nlim L(\u0398n+1 \u2212 \u03b8n |\u0398n = \u03b8n ) = N 0,\n1\u03a3\u22121 1\n|\u03b8n |\u2192\u221e\n\u0012\n\n\u0013\n\n,\n\nTherefore by Theorem 6.3, geometric ergodicity fails.\nReferences.\n[1] Yali Amit. On rates of convergence of stochastic relaxation for Gaussian and nonGaussian distributions. J. Multivariate Anal., 38(1):82\u201399, 1991.\n[2] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. J. Roy.\nStatist. Soc. Ser. B, 36:99\u2013102, 1974.\n[3] Julian Besag, Jeremy York, and Annie Molli\u00e9. Bayesian image restoration, with two\napplications in spatial statistics. Ann. Inst. Statist. Math., 43(1):1\u201359, 1991. With\ndiscussion and a reply by Besag.\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f30\n\nPAPASPILIOPOULOS AND ROBERTS\n\n[4] C. K. Carter and R. Kohn. On Gibbs sampling for state space models. Biometrika,\n81(3):541\u2013553, 1994.\n[5] S. T. Boris Choy and Stephen G. Walker. The extended exponential power distribution and Bayesian robustness. Statist. Probab. Lett., 65(3):227\u2013232, 2003.\n[6] O.F. Christensen, G.O. Roberts, and M. Sk\u00f6ld. Robust mcmc methods for spatial\nGLMM's. J. Comput. Graph. Statist., 15:1\u201317, 2006.\n[7] A. P. Dawid. Posterior expectations for large observations. Biometrika, 60:664\u2013667,\n1973.\n[8] P. J. Diggle, J. A. Tawn, and R. A. Moyeed. Model-based geostatistics. J. Roy. Statist.\nSoc. Ser. C, 47(3):299\u2013350, 1998. With discussion and a reply by the authors.\n[9] Peter Diggle, Kung-Yee Liang, and Scott L. Zeger. Analysis of Longitudinal Data.\nOxford University Press, 1994.\n[10] Alan E. Gelfand, Sujit K. Sahu, and Bradley P. Carlin. Efficient parameterisations\nfor normal linear mixed models. Biometrika, 82(3):479\u2013488, 1995.\n[11] Galin L. Jones and James P. Hobert. Honest exploration of intractable probability\ndistributions via Markov chain Monte Carlo. Statist. Sci., 16(4):312\u2013334, 2001.\n[12] Genshiro Kitagawa. Non-Gaussian state-space modeling of nonstationary time series.\nJ. Amer. Statist. Assoc., 82(400):1032\u20131063, 1987. With comments and a reply by\nthe author.\n[13] Nan M. Laird and James H. Ware. Random-effects models for longitudinal data.\nBiometrics, 38:963\u2013974, 1982.\n[14] G. Lawler and A. Sokal. Bounds on the l2 spectrum for markov chains and markov\nprocesses. Transations of the AMS, 309:557\u2013580, 1988.\n[15] Jun S. Liu. The collapsed Gibbs sampler in Bayesian computations with applications\nto a gene regulation problem. J. Amer. Statist. Assoc., 89(427):958\u2013966, 1994.\n[16] Jun S. Liu, Wing Hung Wong, and Augustine Kong. Covariance structure of the\nGibbs sampler with applications to the comparisons of estimators and augmentation\nschemes. Biometrika, 81(1):27\u201340, 1994.\n[17] Jun S. Liu and Ying Nian Wu. Parameter expansion for data augmentation. J. Amer.\nStatist. Assoc., 94(448):1264\u20131274, 1999.\n[18] Xiao-Li Meng and David van Dyk. The EM algorithm-an old folk-song sung to a\nfast new tune. J. Roy. Statist. Soc. Ser. B, 59(3):511\u2013567, 1997. With discussion and\na reply by the authors.\n[19] S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. SpringerVerlag, London, 1993.\n[20] A. O'Hagan. On outlier rejection phenomena in bayes inference. J. Roy. Statist. Soc.\nSer. B, 41:358\u2013367, 1979.\n[21] Omiros Papaspiliopoulos, Gareth O. Roberts, and Martin Sk\u00f6ld. Non-centered pa-\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\fSTABILITY OF GIBBS SAMPLER\n\n31\n\nrameterizations for hierarchical models and data augmentation. In Bayesian statistics, 7 (Tenerife, 2002), pages 307\u2013326. Oxford Univ. Press, New York, 2003. With\na discussion by Alan E. Gelfand, Ole F. Christensen and Darren J. Wilkinson, and a\nreply by the authors.\n[22] L.R. Pericchi and A.F.M. Smith. Exact and approximate posterior moments for a\nnormal location parameter. J. Roy. Statist. Soc. Ser. B, 54:793\u2013804, 1992.\n[23] G. O. Roberts, O. Papaspiliopoulos, and P. Dellaportas. Bayesian inference for NonGaussian Ornstein-Uhlenbeck Stochastic Volatility processes. J. Roy. Statist. Soc.\nSer. B, 66:369\u2013394, 2003.\n[24] G. O. Roberts and S. K. Sahu. Updating schemes, correlation structure, blocking and\nparameterization for the Gibbs sampler. J. Roy. Statist. Soc. Ser. B, 59(2):291\u2013317,\n1997.\n[25] G. O. Roberts and R. L. Tweedie. Understanding MCMC. Springer-Verlag, London,\n2005. in preparation.\n[26] Gareth O. Roberts and Jeffrey S. Rosenthal. Markov chains and de-initializing processes. Scand. J. Statist., 28(3):489\u2013504, 2001.\n[27] Jeffrey S. Rosenthal. Rates of convergence for Gibbs sampling for variance component\nmodels. Ann. Statist., 23(3):740\u2013761, 1995.\n[28] Neil Shephard. Partial non-Gaussian state space. Biometrika, 81(1):115\u2013131, 1994.\n[29] A. F. M. Smith and G. O. Roberts. Bayesian computation via the Gibbs sampler and\nrelated Markov chain Monte Carlo methods. J. Roy. Statist. Soc. Ser. B, 55(1):3\u201323,\n1993.\n[30] J. C. Wakefield, A. F. M. Smith, A. Racine-Poon, and A. E. Gelfand. Bayesian\nanalysis of linear and non-linear population models by using the Gibbs sampler. J.\nRoy. Statist. Soc. Ser. C, 43:201\u2013221, 1994.\nMathematics Institute\n\nDepartment of Mathematics & Statistics\n\nUniversity of Warwick\n\nLancaster University\n\nCoventry, CV4 7AL\n\nLancaster, LA1 4YF\n\nE-mail: o.papaspiliopoulos@warwick.ac.uk\n\nE-mail: g.o.roberts@lancaster.ac.uk\n\nimsart-aos ver. 2007/01/24 file: papaspiliopoulos_roberts_stabilty-REVISION.tex date: October 30, 2018\n\n\f30\n20\n10\n\u221210\n\n0\n\nY\n\n0\n\n20\n\n40\n\n60\ntime\n\n80\n\n100\n\n\f"}