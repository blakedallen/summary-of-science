{"id": "http://arxiv.org/abs/cs/0410002v1", "guidislink": true, "updated": "2004-10-01T16:54:45Z", "updated_parsed": [2004, 10, 1, 16, 54, 45, 4, 275, 0], "published": "2004-10-01T16:54:45Z", "published_parsed": [2004, 10, 1, 16, 54, 45, 4, 275, 0], "title": "Shannon Information and Kolmogorov Complexity", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0109068%2Ccs%2F0109050%2Ccs%2F0109053%2Ccs%2F0109031%2Ccs%2F0109005%2Ccs%2F0109088%2Ccs%2F0109021%2Ccs%2F0109016%2Ccs%2F0109093%2Ccs%2F0109036%2Ccs%2F0109007%2Ccs%2F0109013%2Ccs%2F0109058%2Ccs%2F0109104%2Ccs%2F0109045%2Ccs%2F0109061%2Ccs%2F0109071%2Ccs%2F0109030%2Ccs%2F0109112%2Ccs%2F0109052%2Ccs%2F0109002%2Ccs%2F0109116%2Ccs%2F0109109%2Ccs%2F0109091%2Ccs%2F0109028%2Ccs%2F0109010%2Ccs%2F0109051%2Ccs%2F0109008%2Ccs%2F0109025%2Ccs%2F0109056%2Ccs%2F0109079%2Ccs%2F0109054%2Ccs%2F0109040%2Ccs%2F0109090%2Ccs%2F0109103%2Ccs%2F0109066%2Ccs%2F0109075%2Ccs%2F0109024%2Ccs%2F0109034%2Ccs%2F0109004%2Ccs%2F0109015%2Ccs%2F0109069%2Ccs%2F0109065%2Ccs%2F0109113%2Ccs%2F0109009%2Ccs%2F0109077%2Ccs%2F0410032%2Ccs%2F0410004%2Ccs%2F0410054%2Ccs%2F0410023%2Ccs%2F0410007%2Ccs%2F0410072%2Ccs%2F0410045%2Ccs%2F0410035%2Ccs%2F0410025%2Ccs%2F0410049%2Ccs%2F0410030%2Ccs%2F0410047%2Ccs%2F0410006%2Ccs%2F0410024%2Ccs%2F0410016%2Ccs%2F0410022%2Ccs%2F0410067%2Ccs%2F0410008%2Ccs%2F0410001%2Ccs%2F0410034%2Ccs%2F0410070%2Ccs%2F0410051%2Ccs%2F0410043%2Ccs%2F0410063%2Ccs%2F0410019%2Ccs%2F0410027%2Ccs%2F0410040%2Ccs%2F0410069%2Ccs%2F0410057%2Ccs%2F0410018%2Ccs%2F0410055%2Ccs%2F0410046%2Ccs%2F0410041%2Ccs%2F0410002%2Ccs%2F0410071%2Ccs%2F0410012%2Ccs%2F0410050%2Ccs%2F0410013%2Ccs%2F0410052%2Ccs%2F0410037%2Ccs%2F0410059%2Ccs%2F0410058%2Ccs%2F0410068%2Ccs%2F0410074%2Ccs%2F0410015%2Ccs%2F0410061%2Ccs%2F0410039%2Ccs%2F0410056%2Ccs%2F0410062%2Ccs%2F0410031%2Ccs%2F0410014%2Ccs%2F0410060%2Ccs%2F0410020%2Ccs%2F0410005%2Ccs%2F0410053&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Shannon Information and Kolmogorov Complexity"}, "summary": "We compare the elementary theories of Shannon information and Kolmogorov\ncomplexity, the extent to which they have a common purpose, and where they are\nfundamentally different. We discuss and relate the basic notions of both\ntheories: Shannon entropy versus Kolmogorov complexity, the relation of both to\nuniversal coding, Shannon mutual information versus Kolmogorov (`algorithmic')\nmutual information, probabilistic sufficient statistic versus algorithmic\nsufficient statistic (related to lossy compression in the Shannon theory versus\nmeaningful information in the Kolmogorov theory), and rate distortion theory\nversus Kolmogorov's structure function. Part of the material has appeared in\nprint before, scattered through various publications, but this is the first\ncomprehensive systematic comparison. The last mentioned relations are new.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0109068%2Ccs%2F0109050%2Ccs%2F0109053%2Ccs%2F0109031%2Ccs%2F0109005%2Ccs%2F0109088%2Ccs%2F0109021%2Ccs%2F0109016%2Ccs%2F0109093%2Ccs%2F0109036%2Ccs%2F0109007%2Ccs%2F0109013%2Ccs%2F0109058%2Ccs%2F0109104%2Ccs%2F0109045%2Ccs%2F0109061%2Ccs%2F0109071%2Ccs%2F0109030%2Ccs%2F0109112%2Ccs%2F0109052%2Ccs%2F0109002%2Ccs%2F0109116%2Ccs%2F0109109%2Ccs%2F0109091%2Ccs%2F0109028%2Ccs%2F0109010%2Ccs%2F0109051%2Ccs%2F0109008%2Ccs%2F0109025%2Ccs%2F0109056%2Ccs%2F0109079%2Ccs%2F0109054%2Ccs%2F0109040%2Ccs%2F0109090%2Ccs%2F0109103%2Ccs%2F0109066%2Ccs%2F0109075%2Ccs%2F0109024%2Ccs%2F0109034%2Ccs%2F0109004%2Ccs%2F0109015%2Ccs%2F0109069%2Ccs%2F0109065%2Ccs%2F0109113%2Ccs%2F0109009%2Ccs%2F0109077%2Ccs%2F0410032%2Ccs%2F0410004%2Ccs%2F0410054%2Ccs%2F0410023%2Ccs%2F0410007%2Ccs%2F0410072%2Ccs%2F0410045%2Ccs%2F0410035%2Ccs%2F0410025%2Ccs%2F0410049%2Ccs%2F0410030%2Ccs%2F0410047%2Ccs%2F0410006%2Ccs%2F0410024%2Ccs%2F0410016%2Ccs%2F0410022%2Ccs%2F0410067%2Ccs%2F0410008%2Ccs%2F0410001%2Ccs%2F0410034%2Ccs%2F0410070%2Ccs%2F0410051%2Ccs%2F0410043%2Ccs%2F0410063%2Ccs%2F0410019%2Ccs%2F0410027%2Ccs%2F0410040%2Ccs%2F0410069%2Ccs%2F0410057%2Ccs%2F0410018%2Ccs%2F0410055%2Ccs%2F0410046%2Ccs%2F0410041%2Ccs%2F0410002%2Ccs%2F0410071%2Ccs%2F0410012%2Ccs%2F0410050%2Ccs%2F0410013%2Ccs%2F0410052%2Ccs%2F0410037%2Ccs%2F0410059%2Ccs%2F0410058%2Ccs%2F0410068%2Ccs%2F0410074%2Ccs%2F0410015%2Ccs%2F0410061%2Ccs%2F0410039%2Ccs%2F0410056%2Ccs%2F0410062%2Ccs%2F0410031%2Ccs%2F0410014%2Ccs%2F0410060%2Ccs%2F0410020%2Ccs%2F0410005%2Ccs%2F0410053&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We compare the elementary theories of Shannon information and Kolmogorov\ncomplexity, the extent to which they have a common purpose, and where they are\nfundamentally different. We discuss and relate the basic notions of both\ntheories: Shannon entropy versus Kolmogorov complexity, the relation of both to\nuniversal coding, Shannon mutual information versus Kolmogorov (`algorithmic')\nmutual information, probabilistic sufficient statistic versus algorithmic\nsufficient statistic (related to lossy compression in the Shannon theory versus\nmeaningful information in the Kolmogorov theory), and rate distortion theory\nversus Kolmogorov's structure function. Part of the material has appeared in\nprint before, scattered through various publications, but this is the first\ncomprehensive systematic comparison. The last mentioned relations are new."}, "authors": ["Peter Grunwald", "Paul Vitanyi"], "author_detail": {"name": "Paul Vitanyi"}, "author": "Paul Vitanyi", "arxiv_comment": "Survey, LaTeX 54 pages, 3 figures, Submitted to IEEE Trans\n  Information Theory", "links": [{"href": "http://arxiv.org/abs/cs/0410002v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0410002v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "E.4, H.1.1", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0410002v1", "affiliation": "CWI and University of Amsterdam", "arxiv_url": "http://arxiv.org/abs/cs/0410002v1", "journal_reference": "There are some errors in this paper draft; when in doubt see the\n  textbook Li, Vitanyi, An Introduction to Kolmogorov Complexity and Its\n  Applications, Springer, 1993, 1997, 2008, 2019", "doi": null, "fulltext": "Shannon Information and Kolmogorov Complexity\nPeter Gr\u00fcnwald and Paul Vit\u00e1nyi\u2217\n\narXiv:cs/0410002v1 [cs.IT] 1 Oct 2004\n\nFebruary 1, 2008\n\nAbstract\nWe compare the elementary theories of Shannon information and Kolmogorov complexity, the extent\nto which they have a common purpose, and where they are fundamentally different. We discuss and relate\nthe basic notions of both theories: Shannon entropy versus Kolmogorov complexity, the relation of both\nto universal coding, Shannon mutual information versus Kolmogorov ('algorithmic') mutual information, probabilistic sufficient statistic versus algorithmic sufficient statistic (related to lossy compression\nin the Shannon theory versus meaningful information in the Kolmogorov theory), and rate distortion\ntheory versus Kolmogorov's structure function. Part of the material has appeared in print before, scattered through various publications, but this is the first comprehensive systematic comparison. The last\nmentioned relations are new.\n\nContents\n1 Introduction\n1.1 Overview and Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3 Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2 Shannon Entropy versus Kolmogorov Complexity\n2.1 Shannon Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2 Kolmogorov Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.1 Formal Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.2 Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2.3 Kolmogorov complexity of sets, functions and probability distributions\n2.2.4 Kolmogorov Complexity and the Universal Distribution . . . . . . . .\n2.3 Expected Kolmogorov Complexity Equals Shannon Entropy . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n2\n3\n4\n6\n8\n8\n11\n12\n13\n13\n14\n15\n\n3 Mutual Information\n16\n3.1 Probabilistic Mutual Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.2 Algorithmic Mutual Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.3 Expected Algorithmic Mutual Information Equals Probabilistic Mutual Information . . . . . 20\n4 Mutual Information Non-Increase\n4.1 Probabilistic Version . . . . . . . . . .\n4.2 Algorithmic Version . . . . . . . . . .\n4.2.1 A Triangle Inequality . . . . .\n4.2.2 Deterministic Data Processing:\n4.2.3 Randomized Data Processing: .\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n21\n21\n21\n22\n22\n23\n\n\u2217 Manuscript received xxx, 2004; revised yyy 200?. This work supported in part by the EU fifth framework project QAIP,\nIST\u20131999\u201311234, the NoE QUIPROCONE IST\u20131999\u201329064, the ESF QiT Programmme, and the EU Fourth Framework BRA\nNeuroCOLT II Working Group EP 27150, the EU NoE PASCAL, and by the Netherlands Organization for Scientific Research (NWO) under Grant 612.052.004. Address: CWI, Kruislaan 413, 1098 SJ Amsterdam, The Netherlands. Email:\nPeter.Grunwald@cwi.nl, Paul.Vitanyi@cwi.nl.\n\n1\n\n\f5 Sufficient Statistic\n5.1 Probabilistic Sufficient Statistic . . . .\n5.2 Algorithmic Sufficient Statistic . . . .\n5.2.1 Meaningful Information . . . .\n5.2.2 Data and Model . . . . . . . .\n5.2.3 Typical Elements . . . . . . . .\n5.2.4 Optimal Sets . . . . . . . . . .\n5.2.5 Sufficient Statistic . . . . . . .\n5.3 Relating Probabilistic and Algorithmic\n\n. . . . . . .\n. . . . . . .\n. . . . . . .\n. . . . . . .\n. . . . . . .\n. . . . . . .\n. . . . . . .\nSufficiency\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n6 Rate Distortion and Structure Function\n6.1 Rate Distortion . . . . . . . . . . . . . . . . . . . . .\n6.2 Structure Function . . . . . . . . . . . . . . . . . . .\n6.2.1 Probability Models . . . . . . . . . . . . . . .\n6.3 Expected Structure Function Equals Distortion\u2013Rate\n6.3.1 Distortion Spheres . . . . . . . . . . . . . . .\n6.3.2 Randomness Deficiency-Revisited . . . . . .\n6.3.3 Sufficient Statistic-Revisited . . . . . . . . .\n6.3.4 Expected Structure Function . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n24\n25\n27\n27\n27\n28\n28\n29\n29\n\n. . . . . .\n. . . . . .\n. . . . . .\nFunction\n. . . . . .\n. . . . . .\n. . . . . .\n. . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n32\n32\n37\n42\n44\n44\n46\n47\n48\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n7 Conclusion\n\n49\n\nA Appendix: Universal Codes\n\n50\n\n1\n\nIntroduction\n\nShannon information theory, usually called just 'information' theory was introduced in 1948, [22], by C.E.\nShannon (1916\u20132001). Kolmogorov complexity theory, also known as 'algorithmic information' theory, was\nintroduced with different motivations (among which Shannon's probabilistic notion of information), independently by R.J. Solomonoff (born 1926), A.N. Kolmogorov (1903\u20131987) and G. Chaitin (born 1943) in\n1960/1964, [24], 1965, [10], and 1969 [3], respectively. Both theories aim at providing a means for measuring\n'information'. They use the same unit to do this: the bit. In both cases, the amount of information in an\nobject may be interpreted as the length of a description of the object. In the Shannon approach, however,\nthe method of encoding objects is based on the presupposition that the objects to be encoded are outcomes\nof a known random source-it is only the characteristics of that random source that determine the encoding,\nnot the characteristics of the objects that are its outcomes. In the Kolmogorov complexity approach we\nconsider the individual objects themselves, in isolation so-to-speak, and the encoding of an object is a short\ncomputer program (compressed version of the object) that generates it and then halts. In the Shannon\napproach we are interested in the minimum expected number of bits to transmit a message from a random\nsource of known characteristics through an error-free channel. Says Shannon [22]:\n\"The fundamental problem of communication is that of reproducing at one point either exactly or\napproximately a message selected at another point. Frequently the messages have meaning; that\nis they refer to or are correlated according to some system with certain physical or conceptual\nentities. These semantic aspects of communication are irrelevant to the engineering problem.\nThe significant aspect is that the actual message is one selected from a set of possible messages.\nThe system must be designed to operate for each possible selection, not just the one which will\nactually be chosen since this is unknown at the time of design.\"\nIn Kolmogorov complexity we are interested in the minimum number of bits from which a particular message or file can effectively be reconstructed: the minimum number of bits that suffice to store the file in\nreproducible format. This is the basic question of the ultimate compression of given individual files. A\nlittle reflection reveals that this is a great difference: for every source emitting but two messages the Shannon information (entropy) is at most 1 bit, but we can choose both messages concerned of arbitrarily high\nKolmogorov complexity. Shannon stresses in his founding article that his notion is only concerned with\ncommunication, while Kolmogorov stresses in his founding article that his notion aims at supplementing the\ngap left by Shannon theory concerning the information in individual objects. Kolmogorov [12]:\n\n2\n\n\f\"Our definition of the quantity of information has the advantage that it refers to individual objects\nand not to objects treated as members of a set of objects with a probability distribution given\non it. The probabilistic definition can be convincingly applied to the information contained, for\nexample, in a stream of congratulatory telegrams. But it would not be clear how to apply it, for\nexample, to an estimate of the quantity of information contained in a novel or in the translation\nof a novel into another language relative to the original. I think that the new definition is capable\nof introducing in similar applications of the theory at least clarity of principle.\"\nTo be sure, both notions are natural: Shannon ignores the object itself but considers only the characteristics\nof the random source of which the object is one of the possible outcomes, while Kolmogorov considers only the\nobject itself to determine the number of bits in the ultimate compressed version irrespective of the manner\nin which the object arose. In this paper, we introduce, compare and contrast the Shannon and Kolmogorov\napproaches. An early comparison between Shannon entropy and Kolmogorov complexity is [14].\nHow to read this paper: We switch back and forth between the two theories concerned according to\nthe following pattern: we first discuss a concept of Shannon's theory, discuss its properties as well as some\nquestions it leaves open. We then provide Kolmogorov's analogue of the concept and show how it answers\nthe question left open by Shannon's theory. To ease understanding of the two theories and how they relate,\nwe supplied the overview below and then Sections 1.3 and Section 2, which discuss preliminaries, fix notation\nand introduce the basic notions. The other sections are largely independent from one another. Throughout\nthe text, we assume some basic familiarity with elementary notions of probability theory and computation,\nbut we have kept the treatment elementary. This may provoke scorn in the information theorist, who\nsees an elementary treatment of basic matters in his discipline, and likewise from the computation theorist\nconcerning the treatment of aspects of the elementary theory of computation. But experience has shown\nthat what one expert views as child's play is an insurmountable mountain for his opposite number. Thus,\nwe decided to ignore background knowledge and cover both areas from first principles onwards, so that the\nopposite expert can easily access the unknown discipline, possibly helped along by the familiar analogues in\nhis own ken of knowledge.\n\n1.1\n\nOverview and Summary\n\nA summary of the basic ideas is given below. In the paper, these notions are discussed in the same order.\n1. Coding: Prefix codes, Kraft inequality (Section 1.3) Since descriptions or encodings of objects are\nfundamental to both theories, we first review some elementary facts about coding. The most important\nof these is the Kraft inequality. This inequality gives the fundamental relationship between probability\ndensity functions and prefix codes, which are the type of codes we are interested in. Prefix codes and\nthe Kraft inequality underly most of Shannon's, and a large part of Kolmogorov's theory.\n2. Shannon's Fundamental Concept: Entropy (Section 2.1) Entropy is defined by a functional that\nmaps probability distributions or, equivalently, random variables, to real numbers. This notion is\nderived from first principles as the only 'reasonable' way to measure the 'average amount of information\nconveyed when an outcome of the random variable is observed'. The notion is then related to encoding\nand communicating messages by Shannon's famous 'coding theorem'.\n3. Kolmogorov's Fundamental Concept: Kolmogorov Complexity (Section 2.2) Kolmogorov complexity is defined by a function that maps objects (to be thought of as natural numbers or sequences of\nsymbols, for example outcomes of the random variables figuring in the Shannon theory) to the natural\nnumbers. Intuitively, the Kolmogorov complexity of a sequence is the length (in bits) of the shortest\ncomputer program that prints the sequence and then halts.\n4. Relating entropy and Kolmogorov complexity (Section 2.3 and Appendix A) Although their primary aim is quite different, and they are functions defined on different spaces, there are close relations\nbetween entropy and Kolmogorov complexity. The formal relation \"entropy = expected Kolmogorov\ncomplexity\" is discussed in Section 2.3. The relation is further illustrated by explaining 'universal\ncoding' (also introduced by Kolmogorov in 1965) which combines elements from both Shannon's and\nKolmogorov's theory, and which lies at the basis of most practical data compression methods. While\n\n3\n\n\frelated to the main theme of this paper, universal coding plays no direct role in the later sections, and\ntherefore we delegated it to Appendix A.\nEntropy and Kolmogorov Complexity are the basic notions of the two theories. They serve as building blocks\nfor all other important notions in the respective theories. Arguably the most important of these notions is\nmutual information:\n5. Mutual Information-Shannon and Kolmogorov Style (Section 3) Entropy and Kolmogorov\ncomplexity are concerned with information in a single object: a random variable (Shannon) or an\nindividual sequence (Kolmogorov). Both theories provide a (distinct) notion of mutual information\nthat measures the information that one object gives about another object. In Shannon's theory, this is\nthe information that one random variable carries about another; in Kolmogorov's theory ('algorithmic\nmutual information'), it is the information one sequence gives about another. In an appropriate setting\nthe former notion can be shown to be the expectation of the latter notion.\n6. Mutual Information Non-Increase (Section 4) In the probabilistic setting the mutual information\nbetween two random variables cannot be increased by processing the outcomes. That stands to reason,\nsince the mutual information is expressed in probabilities of the random variables involved. But in the\nalgorithmic setting, where we talk about mutual information between two strings this is not evident at\nall. Nonetheless, up to some precision, the same non-increase law holds. This result was used recently\nto refine and extend the celebrated G\u00f6del's incompleteness theorem.\n7. Sufficient Statistic (Section 5) Although its roots are in the statistical literature, the notion of probabilistic \"sufficient statistic\" has a natural formalization in terms of mutual Shannon information, and\ncan thus also be considered a part of Shannon theory. The probabilistic sufficient statistic extracts the\ninformation in the data about a model class. In the algorithmic setting, a sufficient statistic extracts\nthe meaningful information from the data, leaving the remainder as accidental random \"noise\". In a\ncertain sense the probabilistic version of sufficient statistic is the expectation of the algorithmic version.\nThese ideas are generalized significantly in the next item.\n8. Rate Distortion Theory versus Structure Function (Section 6) Entropy, Kolmogorov complexity\nand mutual information are concerned with lossless description or compression: messages must be described in such a way that from the description, the original message can be completely reconstructed.\nExtending the theories to lossy description or compression leads to rate-distortion theory in the Shannon setting, and the Kolmogorov structure function in the Kolmogorov section. The basic ingredients\nof the lossless theory (entropy and Kolmogorov complexity) remain the building blocks for such extensions. The Kolmogorov structure function significantly extends the idea of \"meaningful information\"\nrelated to the algorithmic sufficient statistic, and can be used to provide a foundation for inductive inference principles such as Minimum Description Length (MDL). Once again, the Kolmogorov structure\nfunction can be related to Shannon's rate-distortion function by taking expectations in an appropriate\nmanner.\n\n1.2\n\nPreliminaries\n\nStrings: Let B be some finite or countable set. We use the notation B \u2217 to denote the set of finite strings\nor sequences over X . For example,\n{0, 1}\u2217 = {\u01eb, 0, 1, 00, 01, 10, 11, 000, . . .},\nwith \u01eb denoting the empty word '' with no letters. Let N denotes the natural numbers. We identify N and\n{0, 1}\u2217 according to the correspondence\n(0, \u01eb), (1, 0), (2, 1), (3, 00), (4, 01), . . .\n\n(1.1)\n\nThe length l(x) of x is the number of bits in the binary string x. For example, l(010) = 3 and l(\u01eb) = 0. If x\nis interpreted as an integer, we get l(x) = \u230alog(x + 1)\u230b and, for x \u2265 2,\n\u230alog x\u230b \u2264 l(x) \u2264 \u2308log x\u2309.\n\n4\n\n(1.2)\n\n\fHere, as in the sequel, \u2308x\u2309 is the smallest integer larger than or equal to x, \u230ax\u230b is the largest integer smaller\nthan or equal to x and log denotes logarithm to base two. We shall typically be concerned with encoding\nfinite-length binary strings by other finite-length binary strings. The emphasis is on binary strings only for\nconvenience; observations in any alphabet can be so encoded in a way that is 'theory neutral'.\n+\n\n+\n\nPrecision and <, = notation: It is customary in the area of Kolmogorov complexity to use \"additive\nconstant c\" or equivalently \"additive O(1) term\" to mean a constant, accounting for the length of a fixed\nbinary program, independent from every variable or parameter in the expression in which it occurs. In this\npaper we use the prefix complexity variant of Kolmogorov complexity for convenience. Since (in)equalities\nin the Kolmogorov complexity setting typically hold up to an additive constant, we use a special notation.\n+\nWe will denote by < an inequality to within an additive constant. More precisely, let f, g be functions\n+\nfrom {0, 1}\u2217 to R, the real numbers. Then by 'f (x) < g(x)' we mean that there exists a c such that for all\n+\n\n+\n\n+\n\nx \u2208 {0, 1}\u2217, f (x) < g(x) + c. We denote by = the situation when both < and > hold.\n\nProbabilistic Notions:\nLet X be a finite or countable set. A function f : X P\n\u2192 [0, 1] is a probability\nP\nmass function if\nx\u2208X f (x) = 1. We call f a sub-probability mass function if\nx\u2208X f (x) \u2264 1. Such\nsub-probability mass functions will sometimes be used for technical convenience. We can think of them as\nordinary probability mass functions by considering the surplus probability to be concentrated on an undefined\nelement u 6\u2208 X .\nIn the context of (sub-) probability mass functions, X is called the sample space. Associated with mass\nfunction f and sample space X is the random variable X and the probability distribution P such that X\ntakes value x \u2208 X with probability P (X = x) = f (x). A subset of X is called an event. We extend the\nprobability of individual outcomes to events. With thisP\nterminology, P (X = x) = f (x) is the probability\nthat the singleton event {x} occurs, and P (X \u2208 A) = x\u2208A f (x). In some cases (where the use of f (x)\nwould be confusing) we write px as an abbreviation of P (X = x). In the sequel, we often refer to probability\ndistributions in terms of their mass functions, i.e. we freely employ phrases like 'Let X be distributed\naccording to f '.\nWhenever we refer to probability mass functions without explicitly mentioning the sample space X is\nassumed to be N or, equivalently, {0, 1}\u2217.\nFor a given probability mass function f (x, y) on sample space X \u00d7 Y with random variable (X, Y ), we\ndefine the conditional probability mass function f (y | x) of outcome Y = y given outcome X = x as\nf (x, y)\n.\nf (y|x) := P\ny f (x, y)\n\nNote that X and Y are not necessarily independent.\nIn some cases (esp. Section 5.3 and Appendix A), the notion of sequential information source will be\nneeded. This may be thought of as a probability distribution over arbitrarily long binary sequences, of which\nan observer gets to see longer and longer initial segments. Formally, a sequential information source P is a\nprobability distribution on the set {0, 1}\u221e of one-way infinite sequences. It is characterized by a sequence of\nprobability mass functions (f (1) , f (2) , . . .) where f (n) is a probability mass function on {0, 1}n that denotes\nthe marginal distribution of P on the first n-bit segments. By definition, the sequence f \u2261 (f (1) , f (2) , . . .)\nrepresents a P\nsequential information source if for all n > 0, f (n) is related to f (n+1) as follows: for all\nn\nx \u2208 {0, 1} , y\u2208{0,1} f (n+1) (xy) = f (n) (x) and f (0) (x) = 1. This is also called Kolmogorov's compatibility\ncondition [20].\nSome (by no means all!) probability mass functions on {0, 1}\u2217 can be thought of as information sources.\nNamely, given a probability mass function g on {0, 1}\u2217, we can define g (n) as the conditional distribution of\n(n)\nx given that the length of x is n, with domain\n: {0, 1}n \u2192 [0, 1]\nPrestricted to x of length n. That is, g\nn\n(n)\nis defined, for x \u2208 {0, 1} , as g (x) = g(x)/ y\u2208{0,1}n g(y). Then g can be thought of as an information\nsource if and only if the sequence (g (1) , g (2) , . . .) represents an information source.\nComputable Functions: Partial functions on the natural numbers N are functions f such that f (x) can\nbe 'undefined' for some x. We abbreviate 'undefined' to '\u2191'. A central notion in the theory of computation\nis that of the partial recursive functions. Formally, a function f : N \u2192 N \u222a {\u2191} is called partial recursive or\ncomputable if there exists a Turing Machine T that implements f . This means that for all x\n\n5\n\n\f1. If f (x) \u2208 N , then T , when run with input x outputs f (x) and then halts.\n2. If f (x) =\u2191 ('f (x) is undefined'), then T with input x never halts.\nReaders not familiar with computation theory may think of a Turing Machine as a computer program written\nin a general-purpose language such as C or Java.\nA function f : N \u2192 N \u222a {\u2191} is called total if it is defined for all x (i.e. for all x, f (x) \u2208 N ). A total\nrecursive function is thus a function that is implementable on a Turing Machine that halts on all inputs.\nThese definitions are extended to several arguments as follows: we fix, once and for all, some standard\ninvertible pairing function h*, *i : N \u00d7 N \u2192 N and we say that f : N \u00d7 N \u2192 N \u222a {\u2191} is computable if\nthere exists a Turing Machine T such that for all x1 , x2 , T with input hx1 , x2 i outputs f (x1 , x2 ) and halts\nif f (x1 , x2 ) \u2208 N and otherwise T does not halt. By repeating this construction, functions with arbitrarily\nmany arguments can be considered.\nReal-valued Functions: We call a distribution f : N \u2192 R recursive or computable if there exists a Turing\nmachine that, when input hx, yi with x \u2208 {0, 1}\u2217 and y \u2208 N , outputs f (x) to precision 1/y; more precisely,\nit outputs a pair hp, qi such that |p/q \u2212 |f (x)|| < 1/y and an additional bit to indicate whether f (x) larger or\nsmaller than 0. Here h*, *i is the standard pairing function. In this paper all real-valued functions we consider\nare by definition total. Therefore, in line with the above definitions, for a real-valued function 'computable'\n(equivalently, recursive), means that there is a Turing Machine which for all x, computes f (x) to arbitrary\naccuracy; 'partial' recursive real-valued functions are not considered.\nIt is convenient to distinguish between upper and lower semi-computability. For this purpose we consider\nboth the argument of an auxiliary function \u03c6 and the value of \u03c6 as a pair of natural numbers according\nto the standard pairing function h*i. We define a function from N to the reals R by a Turing machine T\ncomputing a function \u03c6 as follows. Interpret the computation \u03c6(hx, ti) = hp, qi to mean that the quotient\np/q is the rational valued tth approximation of f (x).\nDefinition 1.1 A function f : N \u2192 R is lower semi-computable if there is a Turing machine T computing\na total function \u03c6 such that \u03c6(x, t + 1) \u2265 \u03c6(x, t) and limt\u2192\u221e \u03c6(x, t) = f (x). This means that f can\nbe computably approximated from below. A function f is upper semi-computable if \u2212f is lower semicomputable, Note that, if f is both upper- and lower semi-computable, then f is computable.\n(Sub-) Probability mass functions: Probability mass functions on {0, 1}\u2217 may be thought of as real-valued\nfunctions on N . Therefore, the definitions of 'computable' and 'recursive' carry over unchanged from the\nreal-valued function case.\n\n1.3\n\nCodes\n\nWe repeatedly consider the following scenario: a sender (say, A) wants to communicate or transmit some\ninformation to a receiver (say, B). The information to be transmitted is an element from some set X (This\nset may or may not consist of binary strings). It will be communicated by sending a binary string, called the\nmessage. When B receives the message, he can decode it again and (hopefully) reconstruct the element of X\nthat was sent. To achieve this, A and B need to agree on a code or description method before communicating.\nIntuitively, this is a binary relation between source words and associated code words. The relation is fully\ncharacterized by the decoding function. Such a decoding function D can be any function D : {0, 1}\u2217 \u2192 X .\nThe domain of D is the set of code words and the range of D is the set of source words. D(y) = x is\ninterpreted as \"y is a code word for the source word x\". The set of all code words for source word x is the\nset D\u22121 (x) = {y : D(y) = x}. Hence, E = D\u22121 can be called the encoding substitution (E is not necessarily\na function). With each code D we can associate a length function LD : X \u2192 N such that, for each source\nword x, L(x) is the length of the shortest encoding of x:\nLD (x) = min{l(y) : D(y) = x}.\n\u2217\n\nWe denote by x the shortest y such that D(y) = x; if there is more than one such y, then x\u2217 is defined to\nbe the first such y in some agreed-upon order-for example, the lexicographical order.\nIn coding theory attention is often restricted to the case where the source word set is finite, say X =\n{1, 2, . . . , N }. If there is a constant l0 such that l(y) = l0 for all code words y (which implies, L(x) = l0\nfor all source words x), then we call D a fixed-length code. It is easy to see that l0 \u2265 log N . For instance,\nin teletype transmissions the source has an alphabet of N = 32 letters, consisting of the 26 letters in the\nLatin alphabet plus 6 special characters. Hence, we need l0 = 5 binary digits per source letter. In electronic\ncomputers we often use the fixed-length ASCII code with l0 = 8.\n\n6\n\n\fPrefix code: It is immediately clear that in general we cannot uniquely recover x and y from E(xy). Let\nE be the identity mapping. Then we have E(00)E(00) = 0000 = E(0)E(000). We now introduce prefix\ncodes, which do not suffer from this defect. A binary string x is a proper prefix of a binary string y if we\ncan write y = xz for z 6= \u01eb. A set {x, y, . . .} \u2286 {0, 1}\u2217 is prefix-free if for any pair of distinct elements in the\nset neither is a proper prefix of the other. A function D : {0, 1}\u2217 \u2192 N defines a prefix-code if its domain is\nprefix-free. In order to decode a code sequence of a prefix-code, we simply start at the beginning and decode\none code word at a time. When we come to the end of a code word, we know it is the end, since no code\nword is the prefix of any other code word in a prefix-code.\nSuppose we encode each binary string x = x1 x2 . . . xn as\nx\u0304 = |11 {z\n. . . 1} 0x1 x2 . . . xn .\nn times\n\nThe resulting code is prefix because we can determine where the code word x\u0304 ends by reading it from left to\nright without backing up. Note l(x\u0304) = 2n + 1; thus, we have encoded strings in {0, 1}\u2217 in a prefix manner\nat the price of doubling their length. We can get a much more efficient code by applying the construction\nabove to the length l(x) of x rather than x itself: define x\u2032 = l(x)x, where l(x) is interpreted as a binary\nstring according to the correspondence (1.1). Then the code D\u2032 with D\u2032 (x\u2032 ) = x is a prefix code satisfying,\nfor all x \u2208 {0, 1}\u2217, l(x\u2032 ) = n + 2 log n + 1 (here we ignore the 'rounding error' in (1.2)). D\u2032 is used throughout\nthis paper as a standard code to encode natural numbers in a prefix free-manner; we call it the standard\nprefix-code for the natural numbers. We use LN (x) as notation for l(x\u2032 ). When x is interpreted as an integer\n(using the correspondence (1.1) and (1.2)), we see that, up to rounding, LN (x) = log x + 2 log log x + 1.\nPrefix codes and the Kraft inequality: Let X be the set of natural numbers and consider the straightforward non-prefix representation (1.1). There are two elements of X with a description of length 1, four\nwith a description of length 2 and so on. However, for a prefix code D for the natural numbers there are\nless binary prefix code words of each length: if x is a prefix code word then no y = xz with z 6= \u01eb is a prefix\ncode word. Asymptotically there are less prefix code words of length n than the 2n source words of length\nn. Quantification of this intuition for countable X and arbitrary prefix-codes leads to a precise constraint\non the number of code-words of given lengths. This important relation is known as the Kraft Inequality and\nis due to L.G. Kraft [13].\nTheorem 1.2 Let l1 , l2 , . . . be a finite or infinite sequence of natural numbers. There is a prefix-code with\nthis sequence as lengths of its binary code words iff\nX\n2\u2212ln \u2264 1.\nn\n\nUniquely Decodable Codes: We want to code elements of X in a way that they can be uniquely\nreconstructed from the encoding. Such codes are called 'uniquely decodable'. Every prefix-code is a uniquely\ndecodable code. For example, if E(1) = 0, E(2) = 10, E(3) = 110, E(4) = 111 then 1421 is encoded as\n0111100, which can be easily decoded from left to right in a unique way.\nOn the other hand, not every uniquely decodable code satisfies the prefix condition. Prefix-codes are\ndistinguished from other uniquely decodable codes by the property that the end of a code word is always recognizable as such. This means that decoding can be accomplished without the delay of observing subsequent\ncode words, which is why prefix-codes are also called instantaneous codes.\nThere is good reason for our emphasis on prefix-codes. Namely, it turns out that Theorem 1.2 stays valid\nif we replace \"prefix-code\" by \"uniquely decodable code.\" This important fact means that every uniquely\ndecodable code can be replaced by a prefix-code without changing the set of code-word lengths. In Shannon's\nand Kolmogorov's theories, we are only interested in code word lengths of uniquely decodable codes rather\nthan actual encodings. By the previous argument, we may restrict the set of codes we work with to prefix\ncodes, which are much easier to handle.\nProbability distributions and complete prefix codes: A uniquely decodable code is complete if the\naddition of any new code word to its code word set results in a non-uniquely decodable code. It is easy to see\nthat a code is complete iff equality holds in the associated Kraft Inequality. Let l1 , l2 , . . . be the code words\nof some complete uniquely decodable code. Let us define qx = 2\u2212lx . By definition of completeness, we have\n\n7\n\n\fP\n\nx qx = 1. Thus, the qx can be thought of as probability mass functions corresponding to some probability\ndistribution Q. We say Q is the distribution corresponding to l1 , l2 , . . .. In this way, each complete uniquely\ndecodable code is mapped to a unique probability distribution. Of course, this is nothing more than a formal\ncorrespondence: we may choose to encode outcomes of X using a code corresponding to a distribution q,\nwhereas the outcomes are actually distributed according to some p 6= q. But, as we show below, if X is\ndistributed according to p, then the code to which p corresponds is, in an average sense, the code that\nachieves optimal compression of X.\n\n2\n\nShannon Entropy versus Kolmogorov Complexity\n\n2.1\n\nShannon Entropy\n\nIt seldom happens that a detailed mathematical theory springs forth in essentially final form from a single\npublication. Such was the case with Shannon information theory, which properly started only with the\nappearance of C.E. Shannon's paper \"The mathematical theory of communication\" [22]. In this paper,\nShannon proposed a measure of information in a distribution, which he called the 'entropy'. The entropy\nH(P ) of a distribution P measures the 'the inherent uncertainty in P ', or (in fact equivalently), 'how much\ninformation is gained when an outcome of P is observed'. To make this a bit more precise, let us imagine an\nobserver who knows that X is distributed according to P . The observer then observes X = x. The entropy\nof P stands for the 'uncertainty of the observer about the outcome x before he observes it'. Now think of\nthe observer as a 'receiver' who receives the message conveying the value of X. From this dual point of view,\nthe entropy stands for\nthe average amount of information that the observer has gained after receiving a realized outcome\nx of the random variable X. (\u2217)\nBelow, we first give Shannon's mathematical definition of entropy, and we then connect it to its intuitive\nmeaning (\u2217).\nDefinition 2.1 Let X be a finite or countable set, let X be a random variable taking values in X with\ndistribution P (X = x) = px . Then the (Shannon-) entropy of random variable X is given by\nX\nH(X) =\npx log 1/px,\n(2.1)\nx\u2208X\n\nEntropy is defined here as a functional mapping random variables to real numbers. In many texts, entropy\nis, essentially equivalently, defined as aPmap from distributions of random variables to the real numbers.\nThus, by definition: H(P ) := H(X) = x\u2208X px log 1/px.\n\nMotivation: The entropy function (2.1) can be motivated in different ways. The two most important ones\nare the axiomatic approach and the coding interpretation. In this paper we concentrate on the latter, but we\nfirst briefly sketch the former. The idea of the axiomatic approach is to postulate a small set of self-evident\naxioms that any measure of information relative to a distribution should satisfy. One then shows that the\nonly measure satisfying all the postulates is the Shannon entropy. We outline this approach for finite sources\nX = {1, . . . , N }. We look for a function H that maps probability distributions on X to real numbers. For\ngiven distribution P , H(P ) should measure 'how much information is gained on average when an outcome\nis made available'. We can write H(P ) = H(p1 , . . . , pN ) where pi stands for the probability of i. Suppose\nwe require that\n1. H(p1 , . . . , pN ) is continuous in p1 , . . . , pN .\n2. If all the pi are equal, pi = 1/N , then H should be a monotonic increasing function of N . With equally\nlikely events there is more choice, or uncertainty, when there are more possible events.\n3. If a choice is broken down into two successive choices, the original H should be the weighted sum of\nthe individual values of H. Rather than formalizing this condition, we will give a specific example.\nSuppose that X = {1, 2, 3}, and p1 = 21 , p2 = 1/3, p3 = 1/6. We can think of x \u2208 X as being generated\nin a two-stage process. First, an outcome in X \u2032 = {0, 1} is generated according to a distribution P \u2032\n8\n\n\fwith p\u20320 = p\u20321 = 21 . If x\u2032 = 1, we set x = 1 and the process stops. If x\u2032 = 0, then outcome '2' is\ngenerated with probability 2/3 and outcome '3' with probability 1/3, and the process stops. The final\nresults have the same probabilities as before. In this particular case we require that\n1 1\n1\n2 1\n1\n1 1 1\nH( , , ) = H( , ) + H( , ) + H(1).\n2 3 6\n2 2\n2\n3 3\n2\nThus, the entropy of P must be equal to entropy of the first step in the generation process, plus the\nweighted sum (weighted according to the probabilities in the first step) of the entropies of the second\nstep in the generation process.\nAs a special case, if X is the n-fold product space of another space Y, X = (Y1 , . . . , Yn ) and the Yi\nare all independently distributed according to PY , then H(PX ) = nH(PY ). For example, the total\nentropy of n independent tosses of a coin with bias p is nH(p, 1 \u2212 p).\nTheorem 2.2 The only H satisfying the three above assumptions is of the form\nH =K\n\nN\nX\n\npi log 1/pi ,\n\ni=1\n\nwith K a constant.\nThus, requirements (1)\u2013(3) lead us to the definition of entropy (2.1) given above up to an (unimportant)\nscaling factor. We shall give a concrete interpretation of this factor later on. Besides the defining characteristics (1)\u2013(3), the function H has a few other properties that make it attractive as a measure of information.\nWe mention:\n4. H(p1 , . . . , pN ) is a concave function of the pi .\n5. For each N , H achieves its unique maximum for the uniform distribution pi = 1/N .\n6. H(p1 , . . . , pN ) is zero iff one of the pi has value 1. Thus, H is zero if and only if we do not gain any\ninformation at all if we are told that the outcome is i (since we already knew i would take place with\ncertainty).\nThe Coding Interpretation: Immediately after stating Theorem 2.2, Shannon [22] continues, \"this theorem, and the assumptions required for its proof, are in no way necessary for the present theory. It is\ngiven chiefly to provide a certain plausibility to some of our later definitions. The real justification of these\ndefinitions, however, will reside in their implications\". Following this injunction, we emphasize the main\npractical interpretation of entropy as the length (number of bits) needed to encode outcomes in X . This\nprovides much clearer intuitions, it lies at the root of the many practical applications of information theory,\nand, most importantly for us, it simplifies the comparison to Kolmogorov complexity.\nExample 2.3 The entropy of a random variable X with equally likely outcomes in a finite sample space X\nis given by H(X) = log |X |. By choosing a particular message x from X , we remove the entropy from X\nby the assignment X := x and produce or transmit information I = log |X | by our selection of x. We show\nbelow that I = log |X | (or, to be more precise, the integer I \u2032 = \u2308log |X |\u2309) can be interpreted as the number\nof bits needed to be transmitted from an (imagined) sender to an (imagined) receiver.\n\u2666\nWe now connect entropy to minimum average code lengths. These are defined as follows:\nDefinition 2.4 Let source words x \u2208 {0, 1}\u2217 be produced by a random variable X with probability P (X =\nx) = px for the event X = x. The characteristics of X are fixed. Now consider prefix codes D : {0, 1}\u2217 \u2192 N\nwith one code word per source word, and denote the length of the code word for x by lx . We want to\nminimize the expected number of bits we have to transmit for the given source X and choose P\na prefix code\nD that achieves this. In order to do so, we must minimize the average code-word length L\u0304D = x px lx . We\ndefine the minimal average code word length as L\u0304 = min{L\u0304D : D is a prefix-code}. A prefix-code D such\nthat L\u0304D = L\u0304 is called an optimal prefix-code with respect to prior probability P of the source words.\n\n9\n\n\fThe (minimal) average code length of an (optimal) code does not depend on the details of the set of code\nwords, but only on the set of code-word lengths. It is just the expected code-word length with respect to\nthe given distribution. Shannon discovered that the minimal average code word length is about equal to the\nentropy of the source word set. This is known as the Noiseless Coding Theorem. The adjective \"noiseless\"\nemphasizes that we ignore the possibility of errors.\nP\nTheorem 2.5 Let L\u0304 and P be as above. If H(P ) = x px log 1/px is the entropy, then\nH(P ) \u2264 L\u0304 \u2264 H(P ) + 1.\n\n(2.2)\n\nWe are typically interested in encoding a binary string of length n with entropy proportional to n (Example A.1). The essence of (2.2) is that, for all but the smallest n, the difference between entropy and minimal\nexpected code length is completely negligible.\nIt turns out that the optimum L\u0304 in (2.2) is relatively easy to achieve, with the Shannon-Fano code. Let\nthere be N symbols (also called basic messages or source words). Order these symbols according to decreasing\nPr\u22121\nprobability, say X = {1, 2, . . . , N } with probabilities p1 , p2 , . . . , pN . Let Pr = i=1 pi , for r = 1, . . . , N .\n\u2217\nThe binary code E : X \u2192 {0, 1} is obtained by coding r as a binary number E(r), obtained by truncating\nthe binary expansion of Pr at length l(E(r)) such that\nlog 1/pr \u2264 l(E(r)) < 1 + log 1/pr .\nThis code is the Shannon-Fano code. It has the property that highly probable symbols are mapped to short\ncode words and symbols with low probability are mapped to longer code words (just like in a less optimal,\nnon-prefix-free, setting is done in the Morse code). Moreover,\n2\u2212l(E(r)) \u2264 pr < 2\u2212l(E(r))+1 .\nNote that the code for symbol r differs from all codes of symbols r +1 through N in one or more bit positions,\nsince for all i with r + 1 \u2264 i \u2264 N ,\nPi \u2265 Pr + 2\u2212l(E(r)) .\nTherefore the binary expansions of Pr and Pi differ in the first l(E(r)) positions. This means that E is\none-to-one, and it has an inverse: the decoding mapping E \u22121 . Even better, since no value of E is a prefix of\nany other value of E, the set of code words is a prefix-code. This means we can recover the source message\nfrom the code message by scanning it from left to right without\nlook-ahead. If H1 is the average number\nP\nof bits used per symbol of an original message, then H1 = r pr l(E(r)). Combining this with the previous\ninequality we obtain (2.2):\nX\nX\nX\npr log 1/pr \u2264 H1 <\n(1 + log 1/pr )pr = 1 +\npr log 1/pr .\nr\n\nr\n\nr\n\nProblem and Lacuna: Shannon observes, \"Messages have meaning [ . . . however . . . ] the semantic aspects\nof communication are irrelevant to the engineering problem.\" In other words, can we answer a question\nlike \"what is the information in this book\" by viewing it as an element of a set of possible books with a\nprobability distribution on it? Or that the individual sections in this book form a random sequence with\nstochastic relations that damp out rapidly over a distance of several pages? And how to measure the quantity\nof hereditary information in biological organisms, as encoded in DNA? Again there is the possibility of seeing\na particular form of animal as one of a set of possible forms with a probability distribution on it. This seems\nto be contradicted by the fact that the calculation of all possible lifeforms in existence at any one time on\nearth would give a ridiculously low figure like 2100 .\nShannon's classical information theory assigns a quantity of information to an ensemble of possible\nmessages. All messages in the ensemble being equally probable, this quantity is the number of bits needed\nto count all possibilities. This expresses the fact that each message in the ensemble can be communicated\nusing this number of bits. However, it does not say anything about the number of bits needed to convey any\nindividual message in the ensemble. To illustrate this, consider the ensemble consisting of all binary strings\nof length 9999999999999999.\nBy Shannon's measure, we require 9999999999999999 bits on the average to encode a string in such\nan ensemble. However, the string consisting of 9999999999999999 1's can be encoded in about 55 bits by\n\n10\n\n\fexpressing 9999999999999999 in binary and adding the repeated pattern \"1.\" A requirement for this to work\nis that we have agreed on an algorithm that decodes the encoded string. We can compress the string still\nfurther when we note that 9999999999999999 equals 32 \u00d7 1111111111111111, and that 1111111111111111\nconsists of 24 1's.\nThus, we have discovered an interesting phenomenon: the description of some strings can be compressed\nconsiderably, provided they exhibit enough regularity. However, if regularity is lacking, it becomes more\ncumbersome to express large numbers. For instance, it seems easier to compress the number \"one billion,\"\nthan the number \"one billion seven hundred thirty-five million two hundred sixty-eight thousand and three\nhundred ninety-four,\" even though they are of the same order of magnitude.\nWe are interested in a measure of information that, unlike Shannon's, does not rely on (often untenable)\nprobabilistic assumptions, and that takes into account the phenomenon that 'regular' strings are compressible. Thus, we aim for a measure of information content of an individual finite object, and in the information\nconveyed about an individual finite object by another individual finite object. Here, we want the information\ncontent of an object x to be an attribute of x alone, and not to depend on, for instance, the means chosen\nto describe this information content. Surprisingly, this turns out to be possible, at least to a large extent.\nThe resulting theory of information is based on Kolmogorov complexity, a notion independently proposed\nby Solomonoff (1964), Kolmogorov (1965) and Chaitin (1969); Li and Vit\u00e1nyi (1997) describe the history of\nthe subject.\n\n2.2\n\nKolmogorov Complexity\n\nSuppose we want to describe a given object by a finite binary string. We do not care whether the object has\nmany descriptions; however, each description should describe but one object. From among all descriptions\nof an object we can take the length of the shortest description as a measure of the object's complexity. It is\nnatural to call an object \"simple\" if it has at least one short description, and to call it \"complex\" if all of its\ndescriptions are long.\nAs in Section 1.3, consider a description method D, to be used to transmit messages from a sender to a\nreceiver. If D is known to both a sender and receiver, then a message x can be transmitted from sender to\nreceiver by transmitting the description y with D(y) = x. The cost of this transmission is measured by l(y),\nthe length of y. The least cost of transmission of x is determined by the length function L(x): recall that\nL(x) is the length of the shortest y such that D(y) = x. We choose this length function as the descriptional\ncomplexity of x under specification method D.\nObviously, this descriptional complexity of x depends crucially on D. The general principle involved is\nthat the syntactic framework of the description language determines the succinctness of description.\nIn order to objectively compare descriptional complexities of objects, to be able to say \"x is more complex\nthan z,\" the descriptional complexity of x should depend on x alone. This complexity can be viewed as related\nto a universal description method that is a priori assumed by all senders and receivers. This complexity is\noptimal if no other description method assigns a lower complexity to any object.\nWe are not really interested in optimality with respect to all description methods. For specifications to\nbe useful at all it is necessary that the mapping from y to D(y) can be executed in an effective manner.\nThat is, it can at least in principle be performed by humans or machines. This notion has been formalized\nas that of \"partial recursive functions\", also known simply as \"computable functions\", which are formally\ndefined later. According to generally accepted mathematical viewpoints it coincides with the intuitive notion\nof effective computation.\nThe set of partial recursive functions contains an optimal function that minimizes description length of\nevery other such function. We denote this function by D0 . Namely, for any other recursive function D,\nfor all objects x, there is a description y of x under D0 that is shorter than any description z of x under\nD. (That is, shorter up to an additive constant that is independent of x.) Complexity with respect to D0\nminorizes the complexities with respect to all partial recursive functions.\nWe identify the length of the description of x with respect to a fixed specification function D0 with\nthe \"algorithmic (descriptional) complexity\" of x. The optimality of D0 in the sense above means that the\ncomplexity of an object x is invariant (up to an additive constant independent of x) under transition from\none optimal specification function to another. Its complexity is an objective attribute of the described object\nalone: it is an intrinsic property of that object, and it does not depend on the description formalism. This\ncomplexity can be viewed as \"absolute information content\": the amount of information that needs to be\ntransmitted between all senders and receivers when they communicate the message in absence of any other a\n\n11\n\n\fpriori knowledge that restricts the domain of the message. Thus, we have outlined the program for a general\ntheory of algorithmic complexity. The three major innovations are as follows:\n1. In restricting ourselves to formally effective descriptions, our definition covers every form of description\nthat is intuitively acceptable as being effective according to general viewpoints in mathematics and\nlogic.\n2. The restriction to effective descriptions entails that there is a universal description method that minorizes the description length or complexity with respect to any other effective description method.\nSignificantly, this implies Item 3.\n3. The description length or complexity of an object is an intrinsic attribute of the object independent of\nthe particular description method or formalizations thereof.\n2.2.1\n\nFormal Details\n\nThe Kolmogorov complexity K(x) of a finite object x will be defined as the length of the shortest effective\nbinary description of x. Broadly speaking, K(x) may be thought of as the length of the shortest computer\nprogram that prints x and then halts. This computer program may be written in C, Java, LISP or any other\nuniversal language: we shall see that, for any two universal languages, the resulting program lengths differ\nat most by a constant not depending on x.\nTo make this precise, let T1 , T2 , . . . be a standard enumeration [18] of all Turing machines, and let\n\u03c61 , \u03c62 , . . . be the enumeration of corresponding functions which are computed by the respective Turing\nmachines. That is, Ti computes \u03c6i . These functions are the partial recursive functions or computable\nfunctions, Section 1.2. For technical reasons we are interested in the so-called prefix complexity, which is\nassociated with Turing machines for which the set of programs (inputs) resulting in a halting computation\nis prefix free1 . We can realize this by equipping the Turing machine with a one-way input tape, a separate\nwork tape, and a one-way output tape. Such Turing machines are called prefix machines since the halting\nprograms for any one of them form a prefix free set.\nWe first define KTi (x), the prefix Kolmogorov complexity of x relative to a given prefix machine Ti ,\nwhere Ti is the i-th prefix machine in a standard enumeration of them. KTi (x) is defined as the length of\nthe shortest input sequence y such that Ti (y) = \u03c6i (y) = x. If no such input sequence exists, KTi (x) remains\nundefined. Of course, this preliminary definition is still highly sensitive to the particular prefix machine Ti\nthat we use. But now the 'universal prefix machine' comes to our rescue. Just as there exists universal\nordinary Turing machines, there also exist universal prefix machines. These have the remarkable property\nthat they can simulate every other prefix machine. More specifically, there exists a prefix machine U such\nthat, with as input the pair hi, yi, it outputs \u03c6i (y) and then halts. We now fix, once and for all, a prefix\nmachine U with this property and call U the reference machine. The Kolmogorov complexity K(x) of x is\ndefined as KU (x).\nLet us formalize this definition. Let h*i be a standard invertible effective one-one encoding from N \u00d7 N\nto a prefix-free subset of N . h*i may be thought of as the encoding function of a prefix code. For example,\nwe can set hx, yi = x\u2032 y \u2032 . Comparing to the definition of in Section 1.2, we note that from now on, we require\nh*i to map to a prefix-free set. We insist on prefix-freeness and effectiveness because we want a universal\nTuring machine to be able to read an image under h*i from left to right and determine where it ends.\nDefinition 2.6 Let U be our reference prefix machine satisfying for all i \u2208 N , y \u2208 {0, 1}\u2217, U (hi, yi) = \u03c6i (y).\nThe prefix Kolmogorov complexity of x is\nK(x)\n\n= min{l(z) : U (z) = x, z \u2208 {0, 1}\u2217} =\nz\n\n= min{l(hi, yi) : \u03c6i (y) = x, y \u2208 {0, 1}\u2217, i \u2208 N }.\ni,y\n\n(2.3)\n\nWe can alternatively think of z as a program that prints x and then halts, or as z = hi, yi where y is a\nprogram such that, when Ti is input program y, it prints x and then halts.\nThus, by definition K(x) = l(x\u2217 ), where x\u2217 is the lexicographically first shortest self-delimiting (prefix)\nprogram for x with respect to the reference prefix machine. Consider the mapping E \u2217 defined by E \u2217 (x) = x\u2217 .\n1 There exists a version of Kolmogorov complexity corresponding to programs that are not necessarily prefix-free, but we will\nnot go into it here.\n\n12\n\n\fThis may be viewed as the encoding function of a prefix-code (decoding function) D\u2217 with D\u2217 (x\u2217 ) = x. By\nits definition, D\u2217 is a very parsimonious code. The reason for working with prefix rather than standard\nTuring machines is that, for many of the subsequent developments, we need D\u2217 to be prefix.\nThough defined in terms of a particular machine model, the Kolmogorov complexity is machineindependent up to an additive constant and acquires an asymptotically universal and absolute character\nthrough Church's thesis, from the ability of universal machines to simulate one another and execute any\neffective process. The Kolmogorov complexity of an object can be viewed as an absolute and objective\nquantification of the amount of information in it.\n2.2.2\n\nIntuition\n\nTo develop some intuitions, it is useful to think of K(x) as the shortest program for x in some standard\nprogramming language such as LISP or Java. Consider the lexicographical enumeration of all syntactically correct LISP programs \u03bb1 , \u03bb2 , . . ., and the lexicographical enumeration of all syntactically correct\nJava programs \u03c01 , \u03c02 , . . .. We assume that both these programs are encoded in some standard prefix-free\nmanner. With proper definitions we can view the programs in both enumerations as computing partial\nrecursive functions from their inputs to their outputs. Choosing reference machines in both enumerations\nwe can define complexities KLISP (x) and KJava (x) completely analogous to K(x). All of these measures of\nthe descriptional complexities of x coincide up to a fixed additive constant. Let us show this directly for\nKLISP (x) and KJava (x). Since LISP is universal, there exists a LISP program \u03bbP implementing a Java-toLISP compiler. \u03bbP translates each Java program to an equivalent LISP program. Consequently, for all x,\nKLISP (x) \u2264 KJava (x) + 2l(P ). Similarly, there is a Java program \u03c0L that is a LISP-to-Java compiler, so that\nfor all x, KJava (x) \u2264 KLISP (x) + 2l(L). It follows that |KJava (x) \u2212 KLISP (x)| \u2264 2l(P ) + 2l(L) for all x!\nThe programming language view immediately tells us that K(x) must be small for 'simple' or 'regular'\nobjects x. For example, there exists a fixed-size program that, when input n, outputs the first n bits of \u03c0\nand then halts. Specification of n takes at most LN (n) = log n + 2 log log n + 1 bits. Thus, if x consists of\n+\n\nthe first n binary digits of \u03c0, then K(x) < log n + 2 log log n. Similarly, if 0n denotes the string consisting of\n+\n\nn 0's, then K(0n ) < log n + 2 log log n.\n+\n\nOn the other hand, for all x, there exists a program 'print x; halt'. This shows that for all K(x) < l(x).\nAs was previously noted, for any prefix code, there are no more than 2m strings x which can be described\nby m or less bits. In particular, this holds for the prefix code E \u2217 whose length function is K(x). Thus, the\nfraction of strings x of length n with K(x) \u2264 m is at most 2m\u2212n : the overwhelming majority of sequences\ncannot be compressed by more than a constant. Specifically, if x is determined by n independent tosses\nof a fair coin, then with overwhelming probability, K(x) \u2248 l(x). Thus, while for very regular strings, the\nKolmogorov complexity is small (sub-linear in the length of the string), most strings are 'random' and have\nKolmogorov complexity about equal to their own length.\n2.2.3\n\nKolmogorov complexity of sets, functions and probability distributions\n\nFinite sets: The class of finite sets consists of the set of finite subsets S \u2286 {0, 1}\u2217. The complexity of\nthe finite set S is K(S)-the length (number of bits) of the shortest binary program p from which the\nreference universal prefix machine U computes a listing of the elements of S and then halts. That is, if\nS = {x1 , . . . , xn }, then U (p) = hx1 , hx2 , . . . , hxn\u22121 , xn i . . .ii. The conditional complexity K(x | S) of x given\nS, is the length (number of bits) in the shortest binary program p from which the reference universal prefix\nmachine U , given S literally as auxiliary information, computes x.\nInteger-valued functions: The (prefix-) complexity K(f ) of a partial recursive function f is defined by\nK(f ) = mini {K(i) : Turing machine Ti computes f }. If f \u2217 is a shortest program for computing the function\nf (if there is more than one of them then f \u2217 is the first one in enumeration order), then K(f ) = l(f \u2217 ).\nRemark 2.7 In the above definition of K(f ), the objects being described are functions instead of finite\nbinary strings. To unify the approaches, we can consider a finite binary string x as corresponding to a\nfunction having value x for argument 0. Note that we can upper semi-compute (Section 1.2) x\u2217 given x, but\nwe cannot upper semi-compute f \u2217 given f (as an oracle), since we should be able to verify agreement of a\nprogram for a function and an oracle for the target function, on all infinitely many arguments.\n\u2666\n\n13\n\n\fProbability Distributions: In this text we identify probability distributions on finite and countable sets\nX with their corresponding mass functions (Section 1.2). Since any (sub-) probability mass function f is a\ntotal real-valued function, K(f ) is defined in the same way as above.\n2.2.4\n\nKolmogorov Complexity and the Universal Distribution\n\nFollowing the definitions above we now consider lower semi-computable and computable probability mass\nfunctions (Section 1.2). By the fundamentalPKraft's inequality, Theorem 1.2, we know that if l1 , l2 , . . . are\nthe code-word lengths of a prefix code, then x 2\u2212lx \u2264 1. Therefore, since K(x) is the length of a prefix-free\nprogram for x, we can interpret 2\u2212K(x) as a sub-probability mass function, and we define m(x) = 2\u2212K(x) .\nThis is the so-called universal distribution-a rigorous form of Occam's razor. The following two theorems\nare to be considered as major achievements in the theory of Kolmogorov complexity, and will be used again\nand again in the sequel. For the proofs we refer to [18].\nTheorem 2.8 Let f represent a lower semi-computable (sub-) probability distribution on the natural numbers\n(equivalently, finite binary strings). (This implies K(f ) < \u221e.) Then, 2cf m(x) > f (x) for all x, where\ncf = K(f ) + O(1). We call m a universal distribution.\nThe family of lower semi-computable sub-probability mass functions contains all distributions with computable parameters which have a name, or in which we could conceivably be interested, or which have ever\nbeen considered2 . In particular, it contains the computable distributions. We call m \"universal\" since it\nassigns at least as much probability to each object as any other lower semi-computable distribution (up to\na multiplicative factor), and is itself lower semi-computable.\nTheorem 2.9\nlog 1/m(x) = K(x) \u00b1 O(1).\n\n(2.4)\n\nThat means that m assigns high probability to simple objects and low probability to complex or random\nobjects. For example, for x = 00 . . . 0 (n 0's) we have K(x) = K(n) \u00b1 O(1) \u2264 log n + 2 log log n + O(1) since\nthe program\nprint n times a ''0''\nprints x. (The additional 2 log log n term is the penalty term for a prefix encoding.) Then, 1/(n log2 n) =\nO(m(x)). But if we flip a coin to obtain a string y of n bits, then with overwhelming probability K(y) \u2265 n \u00b1\nO(1) (because y does not contain effective regularities which allow compression), and hence m(y) = O(1/2n ).\nProblem and Lacuna: Unfortunately K(x) is not a recursive function: the Kolmogorov complexity is\nnot computable in general. This means that there exists no computer program that, when input an arbitrary\nstring, outputs the Kolmogorov complexity of that string and then halts. While Kolmogorov complexity is\nupper semi-computable (Section 1.2), it cannot be approximated in general in a practically useful sense; and\neven though there exist 'feasible', resource-bounded forms of Kolmogorov complexity (Li and Vit\u00e1nyi 1997),\nthese lack some of the elegant properties of the original, uncomputable notion.\nNow suppose we are interested in efficient storage and transmission of long sequences of data. According\nto Kolmogorov, we can compress such sequences in an essentially optimal way by storing or transmitting the\nshortest program that generates them. Unfortunately, as we have just seen, we cannot find such a program\nin general. According to Shannon, we can compress such sequences optimally in an average sense (and\ntherefore, it turns out, also with high probability) if they are distributed according to some P and we know\nP . Unfortunately, in practice, P is often unknown, it may not be computable-bringing us in the same\nconundrum as with the Kolmogorov complexity approach-or worse, it may be nonexistent. In Appendix A,\nwe consider universal coding, which can be considered a sort of middle ground between Shannon information\nand Kolmogorov complexity. In contrast to both these approaches, universal codes can be directly applied\nfor practical data compression. Some basic knowledge of universal codes will be very helpful in providing\nintuition for the next section, in which we relate Kolmogorov complexity and Shannon entropy. Nevertheless,\n2 To be sure, in statistical applications, one often works with model classes containing distributions that are neither uppernor lower semi-computable. An example is the Bernoulli model class, containing the distributions with P (X = 1) = \u03b8 for all\n\u03b8 \u2208 [0, 1]. However, every concrete parameter estimate or predictive distribution based on the Bernoulli model class that has\never been considered or in which we could be conceivably interested, is in fact computable; typically, \u03b8 is then rational-valued.\nSee also Example A.2 in Appendix A.\n\n14\n\n\funiversal codes are not directly needed in any of the statements and proofs of the next section or, in fact,\nanywhere else in the paper, which is why delegated their treatment to an appendix.\n\n2.3\n\nExpected Kolmogorov Complexity Equals Shannon Entropy\n\nSuppose the source words x are distributed as a random variable X with probability P (X = x) = f (x).\nWhile K(x) is fixed for each x and gives the shortest code word length (but only up to a fixed constant)\nand is independent of the probability distribution P , we may wonder whether K is also universal in the\nfollowing sense: If we weigh each individual\ncode word length for x with its probability f (x), does the\nP\nf\n(x)K(x)\nachieve the minimal average code word length H(X) =\nresulting\nf\n-expected\ncode\nword\nlength\nx\nP\nx f (x) log 1/f (x)? Here we sum over the entire support of f ; restricting summation to a small set, for\nexample the singleton set {x}, can give a different result. The reasoning above implies that, under some\nmild restrictions on the distributions f , the answerP\nis yes. This is expressed in the following theorem, where,\ninstead of the quotient we look at the difference of x f (x)K(x) and H(X). This allows us to express really\nsmall distinctions.\nTheorem 2.10 Let f be a computable probability mass function (Section 1.2)\nP f (x) = P (X = x) on sample\nspace X = {0, 1}\u2217 associated with a random source X and entropy H(X) = x f (x) log 1/f (x). Then,\n!\nX\n0\u2264\nf (x)K(x) \u2212 H(X) \u2264 K(f ) + O(1).\nx\n\nProof. Since K(x) is the code word length of a prefix-code for x, the first inequality of the Noiseless\nCoding Theorem 2.5 states that\nX\nH(X) \u2264\nf (x)K(x).\nx\n\nK(f )+O(1)\n\nSince f (x) \u2264 2\nm(x) (Theorem 2.8) and log m(x) = K(x) + O(1) (Theorem 2.9), we have\nlog 1/f (x) \u2265 K(x) \u2212 K(f ) \u2212 O(1). It follows that\nX\nf (x)K(x) \u2264 H(X) + K(f ) + O(1).\nx\n\nSet the constant cf to\ncf := K(f ) + O(1),\nand the theorem is proved. As an aside, the constant implied in the O(1) term depends on the lengths of\nthe programs occurring in the proof of the cited Theorems 2.8, 2.9 (Theorems 4.3.1 and 4.3.2 in [18]). These\ndepend only on the reference universal prefix machine.\n\u0003\nThe theorem shows that for simple (low complexity) distributions the expected Kolmogorov complexity\nis close to the entropy, but these two quantities may be wide apart for distributions of high complexity. This\nexplains the apparent problem arising in considering a distribution f that concentrates all probability on\nan element x of length n. Suppose\nwe choose K(x) > n. Then f (x) = 1 and hence the entropy H(f ) = 0.\nP\nOn the other hand the term x\u2208{0,1}\u2217 f (x)K(x) = K(x). Therefore, the discrepancy between the expected\nKolmogorov complexity and the entropy exceeds the length n of x. One may think this contradicts the\ntheorem, but that is not the case: The complexity of the distribution is at least that of x, since we can\nreconstruct x given f (just compute f (y) for all y of length n in lexicographical order until we meet one\nthat has probability 1). Thus, cf = K(f ) + O(1) \u2265 K(x) + O(1) \u2265 n + O(1). Thus, if we pick a probability\ndistribution with a complex support, or a trickily skewed probability distribution, than this is reflected in the\ncomplexity of that distribution, and as consequence in the closeness between the entropy and the expected\nKolmogorov complexity.\nFor example, bringing the discussion in line with the universal coding counterpart of Appendix A by\nconsidering f 's that can be interpreted as sequential information sources and denoting the conditional version\nof f restricted to strings of length n by f (n) as in Section 1.2, we find by the same proof as the theorem that\nfor all n,\nX\nf (n) (x)K(x) \u2212 H(f (n) ) \u2264 cf (n) ,\n0\u2264\nx\u2208{0,1}n\n\n15\n\n\fwhere cf (n) = K(f (n) ) + O(1) \u2264 K(f ) + K(n) + O(1) is now a constant depending on both f and n. On the\nother hand, we can eliminate the complexity of the distribution, or its recursivity for that matter, and / or\nrestrictions to a conditional version of f restricted to a finite support A (for example A = {0, 1}n), denoted\nby f A , in the following conditional formulation (this involves a peek in the future since the precise meaning\nof the \"K(* | *)\" notation is only provided in Definition 3.2):\nX\n0\u2264\nf A (x)K(x | f, A) \u2212 H(f A ) = O(1).\n(2.5)\nx\u2208A\n\nThe Shannon-Fano code for a computable distribution is itself computable. Therefore, for every computable distribution f , the universal code D\u2217 whose length function is the Kolmogorov complexity compresses\non average at least as much as the Shannon-Fano code for f . This is the intuitive reason why, no matter\nwhat computable distribution f we take, its expected Kolmogorov complexity is close to its entropy.\n\n3\n3.1\n\nMutual Information\nProbabilistic Mutual Information\n\nHow much information can a random variable X convey about a random variable Y ? Taking a purely\ncombinatorial approach, this notion is captured as follows: If X ranges over X and Y ranges over Y, then we\nlook at the set U of possible events (X = x, Y = y) consisting of joint occurrences of event X = x and event\nY = y. If U does not equal the Cartesian product X \u00d7 Y, then this means there is some dependency between\nX and Y . Considering the set Ux = {(x, u) : (x, u) \u2208 U } for x \u2208 X , it is natural to define the conditional\nentropy of Y given X = x as H(Y |X = x) = log d(Ux ). This suggests immediately that the information\ngiven by X = x about Y is\nI(X = x : Y ) = H(Y ) \u2212 H(Y |X = x).\nFor example, if U = {(1, 1), (1, 2), (2, 3)}, U \u2286 X \u00d7 Y with X = {1, 2} and Y = {1, 2, 3, 4}, then I(X = 1 :\nY ) = 1 and I(X = 2 : Y ) = 2.\nIn this formulation it is obvious that H(X|X = x) = 0, and that I(X = x : X) = H(X). This approach\namounts to the assumption of a uniform distribution of the probabilities concerned.\nWe can generalize this approach, taking into account the frequencies or probabilities of the occurrences\nof the different values X and Y can assume. Let the joint probability f (x, y) be the \"probability of the joint\noccurrence\nevent Y = y.\" The marginal probabilities f1 (x) and f2 (y) are defined by\nP of event X = x and P\nf1 (x) = y f (x, y) and f2 (y) = x f (x, y) and are \"the probability of the occurrence of the event X = x\"\nand the \"probability of the occurrence of the event Y = y\", respectively. This leads to the self-evident\nformulas for joint variables X, Y :\nX\nH(X, Y ) =\nf (x, y) log 1/f (x, y),\nx,y\n\nH(X) =\n\nX\n\nf (x) log 1/f (x),\n\nx\n\nH(Y ) =\n\nX\n\nf (y) log 1/f (y),\n\ny\n\nwhere summation over x is taken over all outcomes of the random variable X and summation over y is taken\nover all outcomes of random variable Y . One can show that\nH(X, Y ) \u2264 H(X) + H(Y ),\n\n(3.1)\n\nwith equality only in the case that X and Y are independent. In all of these equations the entropy quantity\non the left-hand side increases if we choose the probabilities on the right-hand side more equally.\nConditional entropy: We start the analysis of the information in X about Y by first considering the\nconditional entropy of Y given X as the average of the entropy for Y for each value of X weighted by the\n\n16\n\n\fprobability of getting that particular value:\nX\n\nH(Y |X) =\n\nx\n\nX\n\n=\n\nf1 (x)H(Y |X = x)\nf1 (x)\n\nx\n\nX\n\n=\n\nX\n\nf (y|x) log 1/f (y|x)\n\ny\n\nf (x, y) log 1/f (y|x).\n\nx,y\n\nHere f (y|x) is the conditional probability mass function as defined in Section 1.2.\nThe quantity on the left-hand side tells us how uncertain we are on average about the outcome of Y\nwhen we know an outcome of X. With\nX\nH(X) =\nf1 (x) log 1/f1 (x)\nx\n\n=\n\nX X\n\nf (x, y) log\n\ny\n\nx\n\n=\n\n!\n\nX\n\nf (x, y) log\n\nx,y\n\nX\n\nX\n\n1/f (x, y)\n\ny\n\n1/f (x, y),\n\ny\n\nand substituting the formula for f (y|x), we find H(Y |X) = H(X, Y ) \u2212 H(X). Rewrite this expression as\nthe Entropy Equality\nH(X, Y ) = H(X) + H(Y |X).\n(3.2)\nThis can be interpreted as, \"the uncertainty of the joint event (X, Y ) is the uncertainty of X plus the\nuncertainty of Y given X.\" Combining Equations 3.1, 3.2 gives H(Y ) \u2265 H(Y |X), which can be taken to\nimply that, on average, knowledge of X can never increase uncertainty of Y . In fact, uncertainty in Y will\nbe decreased unless X and Y are independent.\nInformation: The information in the outcome X = x about Y is defined as\nI(X = x : Y ) = H(Y ) \u2212 H(Y |X = x).\n\n(3.3)\n\nHere the quantities H(Y ) and H(Y |X = x) on the right-hand side of the equations are always equal to\nor less than the corresponding quantities under the uniform distribution we analyzed first. The values of\nthe quantities I(X = x : Y ) under the assumption of uniform distribution of Y and Y |X = x versus any\nother distribution are not related by inequality in a particular direction. The equalities H(X|X = x) = 0\nand I(X = x : X) = H(X) hold under any distribution of the variables. Since I(X = x : Y ) is a function\nof outcomes of X, while I(Y = y : X) is a function of outcomes of Y , we do not compare them directly.\nHowever, forming the expectation defined as\nX\nE(I(X = x : Y )) =\nf1 (x)I(X = x : Y ),\nx\n\nE(I(Y = y : X)) =\n\nX\n\nf2 (y)I(Y = y : X),\n\ny\n\nand combining Equations 3.2, 3.3, we see that the resulting quantities are equal. Denoting this quantity by\nI(X; Y ) and calling it the mutual information in X and Y , we see that this information is symmetric:\nI(X; Y ) = E(I(X = x : Y )) = E(I(Y = y : X)).\n\n(3.4)\n\nWriting this out we find that the mutual information I(X; Y ) is defined by:\nI(X; Y ) =\n\nXX\nx\n\nf (x, y) log\n\ny\n\n17\n\nf (x, y)\n.\nf1 (x)f2 (y)\n\n(3.5)\n\n\fAnother way to express this is as follows: a well-known criterion for the difference between a given distribution\nf (x) and a distribution g(x) it is compared with is the so-called Kullback-Leibler divergence\nX\nD(f k g) =\nf (x) log f (x)/g(x).\n(3.6)\nx\n\nIt has the important property that\nD(f k g) \u2265 0\n\n(3.7)\n\nwith equality only iff f (x) = g(x) for all x. This is called the information inequality in [4], p. 26. Thus,\nthe mutual information is the Kullback-Leibler divergence between the joint distribution and the product\nf1 (x)f2 (y) of the two marginal distributions. If this quantity is 0 then f (x, y) = f1 (x)f2 (y) for every pair\nx, y, which is the same as saying that X and Y are independent random variables.\nExample 3.1 Suppose we want to exchange the information about the outcome X = x and it is known\nalready that outcome Y = y is the case, that is, x has property y. Then we require (using the ShannonFano code) about log 1/P (X = x|Y = y) bits to communicate x. On average, over the joint distribution\nP (X = x, Y = y) we use H(X|Y ) bits, which is optimal by Shannon's noiseless coding theorem. In fact,\nexploiting the mutual information paradigm, the expected information I(Y ; X) that outcome Y = y gives\nabout outcome X = x is the same as the expected information that X = x gives about Y = y, and is never\nnegative. Yet there may certainly exist individual y such that I(Y = y : X) is negative. For example, we\nmay have X = {0, 1}, Y = {0, 1}, P (X = 1|Y = 0) = 1, P (X = 1|Y = 1) = 1/2, P (Y = 1) = \u01eb. Then\nI(Y ; X) = H(\u01eb, 1 \u2212 \u01eb) whereas I(Y = 1 : X) = H(\u01eb, 1 \u2212 \u01eb) + \u01eb \u2212 1. For small \u01eb, this quantity is smaller than\n0.\n\u2666\nProblem and Lacuna: The quantity I(Y ; X) symmetrically characterizes to what extent random variables\nX and Y are correlated. An inherent problem with probabilistic definitions is that - as we have just seen\n- although I(Y ; X) = E(I(Y = y : X)) is always positive, for some probability distributions and some\ny, I(Y = y : X) can turn out to be negative-which definitely contradicts our naive notion of information\ncontent. The algorithmic mutual information we introduce below can never be negative, and in this sense\nis closer to the intuitive notion of information content.\n\n3.2\n\nAlgorithmic Mutual Information\n\nFor individual objects the information about one another is possibly even more fundamental than for random\nsources. Kolmogorov [10]:\nActually, it is most fruitful to discuss the quantity of information \"conveyed by an object\" (x)\n\"about an object\" (y). It is not an accident that in the probabilistic approach this has led to a\ngeneralization to the case of continuous variables, for which the entropy is finite but, in a large\nnumber of cases,\nZ Z\nPxy (dx dy)\nIW (x, y) =\nPxy (dx dy) log2\nPx (dx)Py (dy)\nis finite. The real objects that we study are very (infinitely) complex, but the relationships\nbetween two separate objects diminish as the schemes used to describe them become simpler.\nWhile a map yields a considerable amount of information about a region of the earth's surface,\nthe microstructure of the paper and the ink on the paper have no relation to the microstructure\nof the area shown on the map.\"\nIn the discussions on Shannon mutual information, we first needed to introduce a conditional version of\nentropy. Analogously, to prepare for the definition of algorithmic mutual information, we need a notion of\nconditional Kolmogorov complexity.\nIntuitively, the conditional prefix Kolmogorov complexity K(x|y) of x given y can be interpreted as the\nshortest prefix program p such that, when y is given to the program p as input, the program prints x and\nthen halts. The idea of providing p with an input y is realized by putting hp, yi rather than just p on the\ninput tape of the universal prefix machine U .\n\n18\n\n\fDefinition 3.2 The conditional prefix Kolmogorov complexity of x given y (for free) is\nK(x|y) = min{l(p) : U (hp, yi) = x, p \u2208 {0, 1}\u2217}.\np\n\nWe define\nK(x) = K(x|\u01eb).\n\n(3.8)\n\nNote that we just redefined K(x) so that the unconditional Kolmogorov complexity is exactly equal to the\nconditional Kolmogorov complexity with empty input. This does not contradict our earlier definition: we\ncan choose a reference prefix machine U such that U (hp, \u01ebi) = U (p). Then (3.8) holds automatically.\nWe now have the technical apparatus to express the relation between entropy inequalities and Kolmogorov\ncomplexity inequalities. Recall that the entropy expresses the expected information to transmit an outcome\nof a known random source, while the Kolmogorov complexity of every such outcome expresses the specific\ninformation contained in that outcome. This makes us wonder to what extend the entropy-(in)equalities hold\nfor the corresponding Kolmogorov complexity situation. In the latter case the corresponding (in)equality is\na far stronger statement, implying the same (in)equality in the entropy setting. It is remarkable, therefore,\nthat similar inequalities hold for both cases, where the entropy ones hold exactly while the Kolmogorov\ncomplexity ones hold up to a logarithmic, and in some cases O(1), additive precision.\n+\n\nAdditivity: By definition, K(x, y) = K(hx, yi). Trivially, the symmetry property holds: K(x, y) =\nK(y, x). Another interesting property is the \"Additivity of Complexity\" property that, as we explain further\nbelow, is equivalent to the \"Symmetry of Algorithmic Mutual Information\" property. Recall that x\u2217 denotes\nthe first (in a standard enumeration order) shortest prefix program that generates x and then halts.\nTheorem 3.3 (Additivity of Complexity/Symmetry of Mutual Information)\nK(x, y) = K(x) + K(y | x\u2217 ) = K(y) + K(x | y \u2217 ).\n+\n\n+\n\n(3.9)\n\nThis is the Kolmogorov complexity equivalent of the entropy equality (3.2). That this latter equality holds\nis true by simply rewriting both sides of the equation according to the definitions of averages of joint and\nmarginal probabilities. In fact, potential individual differences are averaged out. But in the Kolmogorov\ncomplexity case we do nothing like that: it is truly remarkable that additivity of algorithmic information\nholds for individual objects. It was first proven by Kolmogorov and Leonid A. Levin for the plain (non-prefix)\nversion of Kolmogorov complexity, where it holds up to an additive logarithmic term, and reported in [29].\nThe prefix-version (3.9), holding up to an O(1) additive term is due to [6], can be found as Theorem 3.9.1\nin [18], and has a difficult proof.\nSymmetry: To define the algorithmic mutual information between two individual objects x and y with\nno probabilities involved, it is instructive to first recall the probabilistic notion (3.5). Rewriting (3.5) as\nXX\nf (x, y)[log 1/f (x) + log 1/f (y) \u2212 log 1/f (x, y)],\nx\n\ny\n\nand noting that log 1/f (s) is very close to the length of the prefix-free Shannon-Fano code for s, we are led\nto the following definition. The information in y about x is defined as\nI(y : x) = K(x) \u2212 K(x | y \u2217 ) = K(x) + K(y) \u2212 K(x, y),\n+\n\n(3.10)\n\nwhere the second equality is a consequence of (3.9) and states that this information is symmetrical, I(x :\n+\ny) = I(y : x), and therefore we can talk about mutual information.3\n3 The notation of the algorithmic (individual) notion I(x : y) distinguishes it from the probabilistic (average) notion I(X; Y ).\nWe deviate slightly from [18] where I(y : x) is defined as K(x) \u2212 K(x | y).\n\n19\n\n\fPrecision \u2013 O(1) vs. O(log n): The version of (3.9) with just x and y in the conditionals doesn't hold\n+\nwith =, but holds up to additive logarithmic terms that cannot be eliminated. To gain some further insight\nin this matter, first consider the following lemma:\nLemma 3.4 x\u2217 has the same information as the pair x, K(x), that is, K(x\u2217 | x, K(x)), K(x, K(x) | x\u2217 ) =\nO(1).\nProof. Given x, K(x) we can run all programs simultaneously in dovetailed fashion and select the first\nprogram of length K(x) that halts with output x as x\u2217 . (Dovetailed fashion means that in phase k of the\nprocess we run all programs i for j steps such that i + j = k, k = 1, 2, . . .)\n\u0003\nThus, x\u2217 provides more information than x. Therefore, we have to be very careful when extending Theorem 3.3. For example, the conditional version of (3.9) is:\n+\n\nK(x, y | z) = K(x | z) + K(y | x, K(x | z), z).\nNote that a naive version\n\n(3.11)\n\nK(x, y | z) = K(x | z) + K(y | x\u2217 , z)\n+\n\nis incorrect: taking z = x, y = K(x), the left-hand side equals K(x\u2217 | x) which can be as large as log n \u2212\n+\nlog log n + O(1), and the right-hand side equals K(x | x) + K(K(x) | x\u2217 , x) = 0.\nBut up to logarithmic precision we do not need to be that careful. In fact, it turns out that every linear\nentropy inequality holds for the corresponding Kolmogorov complexities within a logarithmic additive error,\n[9]:\nTheorem 3.5 All linear (in)equalities that are valid for Kolmogorov complexity are also valid for Shannon\nentropy and vice versa-provided we require the Kolmogorov complexity (in)equalities to hold up to additive\nlogarithmic precision only.\n\n3.3\n\nExpected Algorithmic Mutual Information Equals Probabilistic Mutual Information\n\nTheorem 2.10 gave the relationship between entropy and ordinary Kolmogorov complexity; it showed that\nthe entropy of distribution P is approximately equal to the expected (under P ) Kolmogorov complexity.\nTheorem 3.6 gives the analogous result for the mutual information (to facilitate comparison to Theorem 2.10,\nnote that x and y in (3.12) below may stand for strings of arbitrary length n).\nTheorem 3.6 Given a computable probability distribution f (x, y) over (x, y) we have\nXX\n\n+\n\nI(X; Y ) \u2212 K(f ) <\n\nx\n\n+\n\nf (x, y)I(x : y)\n\n(3.12)\n\ny\n\n< I(X; Y ) + 2K(f ),\nProof. Rewrite the expectation\nXX\nx\n\nDefine\n\nP\n\ny\n\nf (x, y) = f1 (x) and\n\ny\n\nP\n\nXX\nx\n\n+\n\nf (x, y)I(x : y) =\n\ny\n\nx\n\nXX\nx\n\nf (x, y)[K(x)\n\ny\n\n+ K(y) \u2212 K(x, y)].\n\nf (x, y) = f2 (y) to obtain\n+\n\nf (x, y)I(x : y) =\n\nX\n\nf1 (x)K(x) +\n\nx\n\nX\n\nf2 (y)K(y)\n\ny\n\n\u2212\n\nX\n\nf (x, y)K(x, y).\n\nx,y\n\nP\nGiven the program that computes f , we can approximate f1 (x) by q1 (x, y0 ) = y\u2264y0 f (x, y), and similarly\nfor f2 . That is, the distributions fi (i = 1, 2) are lower semicomputable. Because they sum to 1 it can be\n20\n\n\f+\n+ P\nshown they must also be computable. By Theorem 2.10, we have H(g) < x g(x)K(x) < H(g) + K(g) for\nevery computable probability mass function g.\n+\n+\n+ P\n+ P\nHence, H(fi ) < x fi (x)K(x) < H(fi ) + K(fi ) (i = 1, 2), and H(f ) < x,y f (x, y)K(x, y) < H(f ) +\nK(f ). On the other hand, the probabilistic mutual information (3.5) is expressed in the entropies by\n+\n\nI(X; Y ) = H(f1 ) + H(f2 ) \u2212 H(f ). By construction of the fi 's above, we have K(f1 ), K(f2 ) < K(f ). Since\nthe complexities are positive, substitution establishes the lemma.\n\u0003\n\nCan we get rid of the K(f ) error term? The answer is affirmative; by putting f (*) in the conditional,\nand applying (2.5), we can even get rid of the computability requirement.\nLemma 3.7 Given a joint probability distribution f (x, y) over (x, y) (not necessarily computable) we have\nXX\n+\nI(X; Y ) =\nf (x, y)I(x : y | f ),\nx\n\ny\n\nwhere the auxiliary f means that we can directly access the values f (x, y) on the auxiliary conditional information tape of the reference universal prefix machine.\nProof.\nfrom the definition of conditional algorithmic mutual information, if we show\nP The lemma follows\n+\n+\nthat x f (x)K(x | f ) = H(f ), where the O(1) term implicit in the = sign is independent of f .\nEquip the reference universal prefix machine, with an O(1) length program to compute a Shannon-Fano\ncode from the auxiliary table of probabilities. Then, given an input r, it can determine whether r is the\n+\nShannon-Fano code word for some x. Such a code word has length = log 1/f (x). If this is the case, then\n+\n\nthe machine outputs x, otherwise it halts without output. Therefore, K(x | f ) < log 1/f (x). This shows\nthe upper bound on the expected prefix complexity. The lower bound follows as usual from the Noiseless\nCoding Theorem.\n\u0003\nThus, we see that the expectation of the algorithmic mutual information I(x : y) is close to the probabilistic mutual information I(X; Y ) - which is important: if this were not the case then the algorithmic\nnotion would not be a sharpening of the probabilistic notion to individual objects, but something else.\n\n4\n4.1\n\nMutual Information Non-Increase\nProbabilistic Version\n\nIs it possible to increase the mutual information between two random variables, by processing the outcomes\nin some deterministic manner? The answer is negative: For every function T we have\nI(X; Y ) \u2265 I(X; T (Y )),\n\n(4.1)\n\nthat is, mutual information between two random variables cannot be increased by processing their outcomes\nin any deterministic way. The same holds in an appropriate sense for randomized processing of the outcomes\nof the random variables. This fact is called the data processing inequality [4], Theorem 2.8.1. The reason\nwhy it holds is that (3.5) is expressed in terms of probabilities f (a, b), f1 (a), f2 (b), rather than in terms of\nthe arguments. Processing the arguments a, b will not increase the value of the expression in the right-hand\nside. If the processing of the arguments just renames them in a one-to-one manner then the expression keeps\nthe same value. If the processing eliminates or merges arguments then it is easy to check from the formula\nthat the expression value doesn't increase.\n\n4.2\n\nAlgorithmic Version\n\nIn the algorithmic version of mutual information, the notion is expressed in terms of the individual arguments\ninstead of solely in terms of the probabilities as in the probabilistic version. Therefore, the reason for (4.1)\nto hold is not valid in the algorithmic case. Yet it turns out that the data processing inequality also holds\nbetween individual objects, by far more subtle arguments and not precisely but with a small tolerance. The\nfirst to observe this fact was Leonid A. Levin who proved his \"information non-growth,\" and \"information\nconservation inequalities\" for both finite and infinite sequences under both deterministic and randomized\ndata processing, [15, 16].\n\n21\n\n\f4.2.1\n\nA Triangle Inequality\n\nWe first discuss some useful technical lemmas. The additivity of complexity (symmetry of information) (3.9)\ncan be used to derive a \"directed triangle inequality\" from [8], that is needed later.\nTheorem 4.1 For all x, y, z,\n+\n\n+\n\nK(x | y \u2217 ) < K(x, z | y \u2217 ) < K(z | y \u2217 ) + K(x | z \u2217 ).\nProof. Using (3.9), an evident inequality introducing an auxiliary object z, and twice ( 3.9) again:\nK(x, z | y \u2217 ) = K(x, y, z) \u2212 K(y)\n+\n\n+\n\n< K(z) + K(x | z \u2217 ) + K(y | z \u2217 ) \u2212 K(y)\n\n= K(y, z) \u2212 K(y) + K(x | z \u2217 )\n+\n\n= K(x | z \u2217 ) + K(z | y \u2217 ).\n+\n\n\u0003\nRemark 4.2 This theorem has bizarre consequences. These consequences are not simple unexpected artifacts of our definitions, but, to the contrary, they show the power and the genuine contribution to our\nunderstanding represented by the deep and important mathematical relation (3.9).\nDenote k = K(y) and substitute k = z and K(k) = x to find the following counterintuitive corollary: To\ndetermine the complexity of the complexity of an object y it suffices to give both y and the complexity of\ny. This is counterintuitive since in general we cannot compute the complexity of an object from the object\nitself; if we could this would also solve the so-called \"halting problem\", [18]. This noncomputability can\nbe quantified in terms of K(K(y) | y) which can rise to almost K(K(y)) for some y. But in the seemingly\nsimilar, but subtly different, setting below it is possible.\n+\n\n+\n\nCorollary 4.3 As above, let k denote K(y). Then, K(K(k) | y, k) = K(K(k) | y \u2217 ) < K(K(k) | k \u2217 ) + K(k |\n+\ny, k) = 0.\n\u2666\nNow back to whether mutual information in one object about another one cannot be increased. In\nthe probabilistic setting this was shown to hold for random variables. But does it also hold for individual\noutcomes? In [15, 16] it was shown that the information in one individual string about another cannot be\nincreased by any deterministic algorithmic method by more than a constant. With added randomization this\nholds with overwhelming probability. Here, we follow the proof method of [8] and use the triangle inequality\nof Theorem 4.1 to recall, and to give proofs of this information non-increase.\n4.2.2\n\nDeterministic Data Processing:\n\nRecall the definition 3.10 and Theorem 3.12. We prove a strong version of the information non-increase law\nunder deterministic processing (later we need the attached corollary):\nTheorem 4.4 Given x and z, let q be a program computing z from x\u2217 . Then\n+\n\nI(z : y) < I(x : y) + K(q).\nProof. By the triangle inequality,\n+\n\nK(y | x\u2217 ) < K(y | z \u2217 ) + K(z | x\u2217 )\n= K(y | z \u2217 ) + K(q).\n+\n\n22\n\n(4.2)\n\n\fThus,\nI(x : y) = K(y) \u2212 K(y | x\u2217 )\n+\n\n> K(y) \u2212 K(y | z \u2217 ) \u2212 K(q)\n\n= I(z : y) \u2212 K(q).\n\n\u0003\nThis also implies the slightly weaker but intuitively more appealing statement that the mutual information\nbetween strings x and y cannot be increased by processing x and y separately by deterministic computations.\nCorollary 4.5 Let f, g be recursive functions. Then\n+\n\nI(f (x) : g(y)) < I(x : y) + K(f ) + K(g).\n\n(4.3)\n\nProof. It suffices to prove the case g(y) = y and apply it twice. The proof is by replacing the program\nq that computes a particular string z from a particular x\u2217 in (4.2). There, q possibly depends on x\u2217 and\nz. Replace it by a program qf that first computes x from x\u2217 , followed by computing a recursive function f ,\nthat is, qf is independent of x. Since we only require an O(1)-length program to compute x from x\u2217 we can\n+\nchoose l(qf ) = K(f ).\nBy the triangle inequality,\n+\n\nK(y | x\u2217 ) < K(y | f (x)\u2217 ) + K(f (x) | x\u2217 )\n= K(y | f (x)\u2217 ) + K(f ).\n+\n\nThus,\nI(x : y) = K(y) \u2212 K(y | x\u2217 )\n+\n\n> K(y) \u2212 K(y | f (x)\u2217 ) \u2212 K(f )\n= I(f (x) : y) \u2212 K(f ).\n\u0003\n\n4.2.3\n\nRandomized Data Processing:\n\nIt turns out that furthermore, randomized computation can increase information only with negligible probability. Recall from Section 2.2.4 that the universal probability m(x) = 2\u2212K(x) is maximal within a multiplicative constant among lower semicomputable semimeasures. So, in particular, for each computable measure\nf (x) we have f (x) \u2264 c1 m(x), where the constant factor c1 depends on f . This property also holds when we\nhave an extra parameter, like y \u2217 , in the condition.\nSuppose that z is obtained from x by some randomized computation. We assume that the probability\nf (z | x) of obtaining z from x is a semicomputable distribution over the z's. Therefore it is upperbounded\n\u2217\nby m(z | x) \u2264 c2 m(z | x\u2217 ) = 2\u2212K(z|x ) . The information increase I(z : y) \u2212 I(x : y) satisfies the theorem\nbelow.\nTheorem 4.6 There is a constant c3 such that for all x, y, z we have\nm(z | x\u2217 )2I(z:y)\u2212I(x:y) \u2264 c3 m(z | x\u2217 , y, K(y | x\u2217 )).\nRemark 4.7 For example,\nthe probability of an increase of mutual information by the amount d is O(2\u2212d ).\nP\nThe theorem implies z m(z | x\u2217 )2I(z:y)\u2212I(x:y) = O(1), the m(* | x\u2217 )-expectation of the exponential of the\nincrease is bounded by a constant.\n\u2666\nProof. We have\nI(z : y) \u2212 I(x : y) = K(y) \u2212 K(y | z \u2217 ) \u2212 (K(y) \u2212 K(y | x\u2217 ))\n= K(y | x\u2217 ) \u2212 K(y | z \u2217 ).\n23\n\n\fThe negative logarithm of the left-hand side in the theorem is therefore\nK(z | x\u2217 ) + K(y | z \u2217 ) \u2212 K(y | x\u2217 ).\nUsing Theorem 4.1, and the conditional additivity (3.11), this is\n+\n\n> K(y, z | x\u2217 ) \u2212 K(y | x\u2217 ) = K(z | x\u2217 , y, K(y | x\u2217 )).\n+\n\n\u0003\nRemark 4.8 An example of the use of algorithmic mutual information is as follows [17]. A celebrated result\nof K. G\u00f6del states that Peano Arithmetic is incomplete in the sense that it cannot be consistently extended\nto a complete theory using recursively enumerable axiom sets. (Here 'complete' means that every sentence of\nPeano Arithmetic is decidable within the theory; for further details on the terminology used in this example,\nwe refer to [18]). The essence is the non-existence of total recursive extensions of a universal partial recursive\npredicate. This is usually taken to mean that mathematics is undecidable. Non-existence of an algorithmic\nsolution need not be a problem when the requirements do not imply unique solutions. A perfect example\nis the generation of strings of high Kolmogorov complexity, say of half the length of the strings. There\nis no deterministic effective process that can produce such a string; but repeatedly flipping a fair coin we\ngenerate a desired string with overwhelming probability. Therefore, the question arises whether randomized\nmeans allow us to bypass G\u00f6del's result. The notion of mutual information between two finite strings can\nbe refined and extended to infinite sequences, so that, again, it cannot be increased by either deterministic\nor randomized processing. In [17] the existence of an infinite sequence is shown that has infinite mutual\ninformation with all total extensions of a universal partial recursive predicate. As Levin states \"it plays the\nrole of password: no substantial information about it can be guessed, no matter what methods are allowed.\"\nThis \"forbidden information\" is used to extend the G\u00f6del's incompleteness result to also hold for consistent\nextensions to a complete theory by randomized means with non-vanishing probability.\n\u2666\n\nProblem and Lacuna: Entropy, Kolmogorov complexity and mutual (algorithmic) information are concepts that do not distinguish between different kinds of information (such as 'meaningful' and 'meaningless'\ninformation). In the remainder of this paper, we show how these more intricate notions can be arrived at,\ntypically by constraining the description methods with which strings are allowed to be encoded (Section 5.2)\nand by considering lossy rather than lossless compression (Section 6). Nevertheless, the basic notions entropy,\nKolmogorov complexity and mutual information continue to play a fundamental r\u00f4le.\n\n5\n\nSufficient Statistic\n\nIn introducing the notion of sufficiency in classical statistics, Fisher [5] stated:\n\"The statistic chosen should summarize the whole of the relevant information supplied by the\nsample. This may be called the Criterion of Sufficiency . . . In the case of the normal curve\nof distribution it is evident that the second moment is a sufficient statistic for estimating the\nstandard deviation.\"\nA \"sufficient\" statistic of the data contains all information in the data about the model class. Below we first\ndiscuss the standard notion of (probabilistic) sufficient statistic as employed in the statistical literature. We\nshow that this notion has a natural interpretation in terms of Shannon mutual information, so that we may\njust as well think of a probabilistic sufficient statistic as a concept in Shannon information theory. Just as\nin the other sections of this paper, there is a corresponding notion in the Kolmogorov complexity literature:\nthe algorithmic sufficient statistic which we introduce in Section 5.2. Finally, in Section 5.3 we connect the\nstatistical/Shannon and the algorithmic notions of sufficiency.\n\n24\n\n\f5.1\n\nProbabilistic Sufficient Statistic\n\nLet {P\u03b8 } be a family of distributions, also called a model class, of a random variable X that takes values in\na finite or countable set of data X . Let \u0398 be the set of parameters \u03b8 parameterizing the family {P\u03b8 }. Any\nfunction S : X \u2192 S taking values in some set S is said to be a statistic of the data in X . A statistic S is\nsaid to be sufficient for the family {P\u03b8 } if, for every s \u2208 S, the conditional distribution\nP\u03b8 (X = * | S(x) = s)\n\n(5.1)\n\nis invariant under changes of \u03b8. This is the standard definition in the statistical literature, see for example\n[2]. Intuitively, (5.1) means that all information about \u03b8 in the observation x is present in the (coarser)\nobservation S(x), in line with Fisher's quote above.\nThe notion of 'sufficient statistic' can be equivalently expressed in terms of probability mass functions.\nLet f\u03b8 (x) = P\u03b8 (X = x) denote the probability mass of x according to P\u03b8 . We identify distributions P\u03b8 with\ntheir mass functions f\u03b8 and denote the model class {P\u03b8 } by {f\u03b8 }. Let f\u03b8 (x|s) denote the probability mass\nfunction of the conditional distribution (5.1), defined as in Section 1.2. That is,\n(\nP\nf\u03b8 (x)/ x\u2208X :S(x)=s f\u03b8 (x) if S(x) = s\nf\u03b8 (x|s) =\n0\nif S(x) 6= s.\nThe requirement of S to be sufficient is equivalent to the existence of a function g : X \u00d7 S \u2192 R such that\ng(x | s) = f\u03b8 (x | s),\n\n(5.2)\n\nfor every \u03b8 \u2208 \u0398, s \u2208 S, x \u2208 X . (Here we change the common notation 'g(x, s)' to 'g(x | s)' which is more\nexpressive for our purpose.)\nExample 5.1 Let X = {0, 1}n, let X = (X1 , . . . , Xn ). Let {P\u03b8 : \u03b8 \u2208 (0, 1)} be the set of n-fold Bernoulli\ndistributions on X with parameter \u03b8. That is,\nf\u03b8 (x) = f\u03b8 (x1 . . . xn ) = \u03b8S(x) (1 \u2212 \u03b8)n\u2212S(x)\n\nwhere S(x) is the number of 1's in x. Then S(x) is a sufficient statistic for {P\u03b8 }. Namely, fix an arbitrary\nP\u03b8 with \u03b8 \u2208 (0, 1) and an arbitrary s with\n\u0001 0 < s < n. Then all x's with s ones and n \u2212 s zeroes are equally\n\u0001\nprobable. The number of such x's is ns . Therefore, the probability P\u03b8 (X = x | S(x) = s) is equal to 1/ ns ,\nand this does not depend on the parameter \u03b8. Equivalently, for all \u03b8 \u2208 (0, 1),\n(\n\u0001\n1/ ns\nif S(x) = s\nf\u03b8 (x | s) =\n(5.3)\n0\notherwise.\nSince (5.3) satisfies (5.2) (with g(x|s) the uniform distribution on all x with exactly s ones), S(x) is a\nsufficient statistic relative to the model class {P\u03b8 }. In the Bernoulli case, g(x|s) can be obtained by starting\nfrom the uniform distribution on X (\u03b8 = 21 ), and conditioning on S(x) = s. But g is not necessarily\nuniform. For example, for the Poisson model class, where {f\u03b8 } represents the set of Poisson distributions\non n observations, the observed mean is a sufficient statistic and the corresponding g is far from uniform.\nAll information about the parameter \u03b8 in the observation x is already contained in S(x). In the Bernoulli\ncase, once we know the number S(x) of 1's in x, all further details of x (such as the order of 0s and 1s) are\nirrelevant for determination of the Bernoulli parameter \u03b8.\nTo give an example of a statistics that is not sufficient for the Bernoulli model class, consider the statistic\nT (x) which counts the number of 1s in x that are followed by a 1. On the other hand, for every statistic U ,\nthe combined statistic V (x) := (S(x), U (x)) with S(x) as before, is sufficient, since it contains all information\nin S(x). But in contrast to S(x), a statistic such as V (x) is typically not minimal, as explained further below.\n\u2666\nIt will be useful to rewrite (5.2) as\nlog 1/f\u03b8 (x | s) = log 1/g(x|s).\n\n(5.4)\n\nDefinition 5.2 A function S : X \u2192 S is a probabilistic sufficient statistic for {f\u03b8 } if there exists a function\ng : X \u00d7 S \u2192 R such that (5.4) holds for every \u03b8 \u2208 \u0398, every x \u2208 X , every s \u2208 S (Here we use the convention\nlog 1/0 = \u221e).\n25\n\n\fExpectation-version of definition: The standard definition of probabilistic sufficient statistics is ostensibly of the 'individual-sequence'-type: for S to be sufficient, (5.4) has to hold for every x, rather than\nmerely in expectation or with high probability. However, the definition turns out to be equivalent to an\nexpectation-oriented form, as shown in Proposition 5.3. We first introduce an a priori distribution over \u0398,\nthe parameter set for our model class {f\u03b8 }. We denote the probability density of this distribution by p1 .\nThis way we can define a joint distribution p(\u03b8, x) = p1 (\u03b8)f\u03b8 (x).\nProposition 5.3 The following two statements are equivalent to Definition 5.2: (1) For every \u03b8 \u2208 \u0398,\nX\nX\nf\u03b8 (x) log 1/f\u03b8 (x | S(x)) =\nf\u03b8 (x) log 1/g(x | S(x)) .\n(5.5)\nx\n\nx\n\n(2) For every prior p1 (\u03b8) on \u0398,\nX\nX\np(\u03b8, x) log 1/f\u03b8 (x | S(x)) =\np(\u03b8, x) log 1/g(x | S(x)) .\n\u03b8,x\n\n(5.6)\n\n\u03b8,x\n\nProof. Definition 5.2 \u21d2 (5.5): Suppose (5.4) holds for every \u03b8 \u2208 \u0398, every x \u2208 X , every s \u2208 S. Then it\nalso holds in expectation for every \u03b8 \u2208 \u0398:\nX\nX\nf\u03b8 (x) log 1/f\u03b8 (x|S(x)) =\nf\u03b8 (x) log 1/g(x|S(x))].\n(5.7)\nx\n\nx\n\n(5.5)\u21d2 Definition 5.2: Suppose that for every \u03b8 \u2208 \u0398, (5.7) holds. Denote\nX\nf\u03b8 (y).\nf\u03b8 (s) =\n\n(5.8)\n\ny\u2208X :S(y)=s\n\nBy adding\n\nP\n\nx\n\nf\u03b8 (x) log 1/f\u03b8 (S(x)) to both sides of the equation, (5.7) can be rewritten as\nX\nX\nf\u03b8 (x) log 1/f\u03b8 (x) =\nf\u03b8 (x) log 1/g\u03b8 (x),\nx\n\n(5.9)\n\nx\n\nwith g\u03b8 (x) = f\u03b8 (S(x)) * g(x|S(x))]. By the information inequality (3.7), the equality (5.9) can only hold if\ng\u03b8 (x) = f\u03b8 (x) for every x \u2208 X . Hence, we have established (5.4).\n(5.5) \u21d4 (5.6): follows by linearity of expectation.\n\u0003\nMutual information-version of definition: After some rearranging of terms, the characterization (5.6)\ngives rise to the intuitively appealing definition of probabilistic sufficient statistic in terms of mutual information (3.5). The resulting formulation of sufficiency is as follows [4]: S is sufficient for {f\u03b8 } iff for all priors\np1 on \u0398:\nI(\u0398; X) = I(\u0398; S(X))\n(5.10)\nfor all distributions of \u03b8.\nThus, a statistic S(x) is sufficient if the probabilistic mutual information is invariant under taking the\nstatistic (5.10).\nMinimal Probabilistic Sufficient Statistic: A sufficient statistic may contain information that is not\nrelevant: for a normal distribution the sample mean is a sufficient statistic, but the pair of functions which\ngive the mean of the even-numbered samples and the odd-numbered samples respectively, is also a sufficient\nstatistic. A statistic S(x) is a minimal sufficient statistic with respect to an indexed model class {f\u03b8 }, if it\nis a function of all other sufficient statistics: it contains no irrelevant information and maximally compresses\nthe information in the data about the model class. For the family of normal distributions the sample mean\nis a minimal sufficient statistic, but the sufficient statistic consisting of the mean of the even samples in\ncombination with the mean of the odd samples is not minimal. Note that one cannot improve on sufficiency:\nThe data processing inequality (4.1) states that I(\u0398; X) \u2265 I(\u0398; S(X)), for every function S, and that for\nrandomized functions S an appropriate related expression holds. That is, mutual information between data\nrandom variable and model random variable cannot be increased by processing the data sample in any way.\n\n26\n\n\fProblem and Lacuna: We can think of the probabilistic sufficient statistic as extracting those patterns in\nthe data that are relevant in determining the parameters of a statistical model class. But what if we do not\nwant to commit ourselves to a simple finite-dimensional parametric model class? In the most general context,\nwe may consider the model class of all computable distributions, or all computable sets of which the observed\ndata is an element. Does there exist an analogue of the sufficient statistic that automatically summarizes\nall information in the sample x that is relevant for determining the \"best\" (appropriately defined) model\nfor x within this enormous class of models? Of course, we may consider the literal data x as a statistic of\nx, but that would not be satisfactory: we would still like our generalized statistic, at least in many cases, to\nbe considerably coarser, and much more concise, than the data x itself. It turns out that, to some extent,\nthis is achieved by the algorithmic sufficient statistic of the data: it summarizes all conceivably relevant\ninformation in the data x; at the same time, many types of data x admit an algorithmic sufficient statistic\nthat is concise in the sense that it has very small Kolmogorov complexity.\n\n5.2\n5.2.1\n\nAlgorithmic Sufficient Statistic\nMeaningful Information\n\nThe information contained in an individual finite object (like a finite binary string) is measured by its\nKolmogorov complexity-the length of the shortest binary program that computes the object. Such a\nshortest program contains no redundancy: every bit is information; but is it meaningful information? If we\nflip a fair coin to obtain a finite binary string, then with overwhelming probability that string constitutes its\nown shortest program. However, also with overwhelming probability all the bits in the string are meaningless\ninformation, random noise. On the other hand, let an object x be a sequence of observations of heavenly\nbodies. Then x can be described by the binary string pd, where p is the description of the laws of gravity\nand the observational parameter setting, while d accounts for the measurement errors: we can divide the\ninformation in x into meaningful information p and accidental information d. The main task for statistical\ninference and learning theory is to distill the meaningful information present in the data. The question arises\nwhether it is possible to separate meaningful information from accidental information, and if so, how. The\nessence of the solution to this problem is revealed when we write Definition 2.6 as follows:\nK(x) = min{K(i) + l(p) : Ti (p) = x} + O(1),\np,i\n\n(5.11)\n\nwhere the minimum is taken over p \u2208 {0, 1}\u2217 and i \u2208 {1, 2, . . .}. The justification is that for the fixed\nreference universal prefix Turing machine U (hi, pi) = Ti (p) for all i and p. Since i\u2217 denotes the shortest\nself-delimiting program for i, we have |i\u2217 | = K(i). The expression (5.11) emphasizes the two-part code\nnature of Kolmogorov complexity. In a randomly truncated initial segment of a time series\nx = 10101010101010101010101010,\nwe can encode x by a small Turing machine printing a specified number of copies of the pattern \"01.\" This\nway, K(x) is viewed as the shortest length of a two-part code for x, one part describing a Turing machine T ,\nor model, for the regular aspects of x, and the second part describing the irregular aspects of x in the form\nof a program p to be interpreted by T . The regular, or \"valuable,\" information in x is constituted by the\nbits in the \"model\" while the random or \"useless\" information of x constitutes the remainder. This leaves\nopen the crucial question: How to choose T and p that together describe x? In general, many combinations\nof T and p are possible, but we want to find a T that describes the meaningful aspects of x.\n5.2.2\n\nData and Model\n\nWe consider only finite binary data strings x. Our model class consists of Turing machines T that enumerate\na finite set, say S, such that on input p \u2264 |S| we have T (p) = x with x the pth element of T 's enumeration of\nS, and T (p) is a special undefined value if p > |S|. The \"best fitting\" model for x is a Turing machine T that\nreaches the minimum description length in (5.11). There may be many such T , but, as we will see, if chosen\nproperly, such a machine T embodies the amount of useful information contained in x. Thus, we have divided\na shortest program x\u2217 for x into parts x\u2217 = T \u2217 (p) such that T \u2217 is a shortest self-delimiting program for T .\nNow suppose we consider only low complexity finite-set models, and under these constraints the shortest twopart description happens to be longer than the shortest one-part description. For example, this can happen\n\n27\n\n\fif the data is generated by a model that is too complex to be in the contemplated model class. Does the\nmodel minimizing the two-part description still capture all (or as much as possible) meaningful information?\nSuch considerations require study of the relation between the complexity limit on the contemplated model\nclasses, the shortest two-part code length, and the amount of meaningful information captured.\nIn the following we will distinguish between \"models\" that are finite sets, and the \"shortest programs\" to\ncompute those models that are finite strings. The latter will be called 'algorithmic statistics'. In a way the\ndistinction between \"model\" and \"statistic\" is artificial, but for now we prefer clarity and unambiguousness\nin the discussion. Moreover, the terminology is customary in the literature on algorithmic statistics. Note\nthat strictly speaking, neither an algorithmic statistic nor the set it defines is a statistic in the probabilistic\nsense: the latter was defined as a function on the set of possible data samples of given length. Both notions\nare unified in Section 5.3.\n5.2.3\n\nTypical Elements\n\nConsider a string x of length n and prefix complexity K(x) = k. For every finite set S \u2286 {0, 1}\u2217 containing\nx we have K(x|S) \u2264 log |S| + O(1). Indeed, consider the prefix code of x consisting of its \u2308log |S|\u2309 bit long\nindex of x in the lexicographical ordering of S. This code is called data-to-model code. We identify the\nstructure or regularity in x that are to be summarized with a set S of which x is a random or typical member:\ngiven S containing x, the element x cannot be described significantly shorter than by its maximal length\nindex in S, that is, K(x | S) \u2265 log |S| + O(1).\nDefinition 5.4 Let \u03b2 \u2265 0 be an agreed upon, fixed, constant. A finite binary string x is a typical or random\nelement of a set S of finite binary strings, if x \u2208 S and\nK(x | S) \u2265 log |S| \u2212 \u03b2.\n\n(5.12)\n\nWe will not indicate the dependence on \u03b2 explicitly, but the constants in all our inequalities (O(1)) will be\nallowed to be functions of this \u03b2.\nThis definition requires a finite S. In fact, since K(x | S) \u2264 K(x) + O(1), it limits the size of S to\nO(2k ). Note that the notion of typicality is not absolute but depends on fixing the constant implicit in the\nO-notation.\nExample 5.5 Consider the set S of binary strings of length n whose every odd position is 0. Let x be an\nelement of this set in which the subsequence of bits in even positions is an incompressible string. Then x is\na typical element of S (or by with some abuse of language we can say S is typical for x). But x is also a\ntypical element of the set {x}.\n\u2666\n5.2.4\n\nOptimal Sets\n\nLet x be a binary data string of length n. For every finite set S \u220b x, we have K(x) \u2264 K(S) + log |S| + O(1),\nsince we can describe x by giving S and the index of x in a standard enumeration of S. Clearly this can be\nimplemented by a Turing machine computing the finite set S and a program p giving the index of x in S.\nThe size of a set containing x measures intuitively the number of properties of x that are represented: The\nlargest set is {0, 1}n and represents only one property of x, namely, being of length n. It clearly \"underfits\"\nas explanation or model for x. The smallest set containing x is the singleton set {x} and represents all\nconceivable properties of x. It clearly \"overfits\" as explanation or model for x.\nThere are two natural measures of suitability of such a set as a model for x. We might prefer either\nthe simplest set, or the smallest set, as corresponding to the most likely structure 'explaining' x. Both the\nlargest set {0, 1}n (having low complexity of about K(n)) and the singleton set {x} (having high complexity\nof about K(x)), while certainly statistics for x, would indeed be considered poor explanations. We would like\nto balance simplicity of model versus size of model. Both measures relate to the optimality of a two-stage\ndescription of x using a finite set S that contains it. Elaborating on the two-part code:\nK(x) \u2264 K(x, S) \u2264 K(S) + K(x | S) + O(1)\n\u2264 K(S) + log |S| + O(1),\n\n28\n\n(5.13)\n\n\fwhere only the final substitution of K(x | S) by log |S| + O(1) uses the fact that x is an element of S. The\ncloser the right-hand side of (5.13) gets to the left-hand side, the better the description of x is in terms of\nthe set S. This implies a trade-off between meaningful model information, K(S), and meaningless \"noise\"\nlog |S|. A set S (containing x) for which (5.13) holds with equality\nK(x) = K(S) + log |S| + O(1),\n\n(5.14)\n\nis called optimal. A data string x can be typical for a set S without that set S being optimal for x. This is\nthe case precisely when x is typical for S (that is K(x|S) = log S + O(1)) while K(x, S) > K(x).\n5.2.5\n\nSufficient Statistic\n\nIntuitively, a model expresses the essence of the data if the two-part code describing the data consisting of\nthe model and the data-to-model code is as concise as the best one-part description.\nMindful of our distinction between a finite set S and a program that describes S in a required representation format, we call a shortest program for an optimal set with respect to x an algorithmic sufficient\nstatistic for x. Furthermore, among optimal sets, there is a direct trade-off between complexity and log-size,\nwhich together sum to K(x) + O(1).\nExample 5.6 It can be shown that the set S of Example 5.5 is also optimal, and so is {x}. Sets for which\nx is typical form a much wider class than optimal sets for x: the set {x, y} is still typical for x but with most\ny, it will be too complex to be optimal for x.\nFor a perhaps less artificial example, consider complexities conditional on the length n of strings. Let y\nbe a random string of length n, let Sy be the set of strings of length n which have 0's exactly where y has,\nand let x be a random element of Sy . Then x has about 25% 1's, so its complexity is much less than n. The\nset Sy has x as a typical element, but is too complex to be optimal, since its complexity (even conditional\non n) is still n.\n\u2666\nAn algorithmic sufficient statistic is a sharper individual notion than a probabilistic sufficient statistic. An\noptimal set S associated with x (the shortest program computing S is the corresponding sufficient statistic\nassociated with x) is chosen such that x is maximally random with respect to it. That is, the information\nin x is divided in a relevant structure expressed by the set S, and the remaining randomness with respect\nto that structure, expressed by x's index in S of log |S| bits. The shortest program for S is itself alone an\nalgorithmic definition of structure, without a probabilistic interpretation.\nThose optimal sets that admit the shortest possible program are called algorithmic minimal sufficient\nstatistics of x. They will play a major role in the next section on the Kolmogorov structure function.\nSummarizing:\nDefinition 5.7 (Algorithmic sufficient statistic, algorithmic minimal sufficient statistic) An algorithmic sufficient statistic of x is a shortest program for a set S containing x that is optimal, i.e. it\nsatisfies (5.14). An algorithmic sufficient statistic with optimal set S is minimal if there exists no optimal\nset S \u2032 with K(S \u2032 ) < K(S).\nExample 5.8 Let k be a number in the range 0, 1, . . . , n of complexity\n\u0001 log n + O(1) given n and let x be\na string of length n having k ones of complexity K(x | n, k) \u2265 log nk given n, k. This x can be viewed as\na typical result of tossing a coin with a bias about p = k/n. A two-part description of x is given by the\nnumber k of 1's in x first, followed by the index j \u2264 log |S| of x in the set S of strings of length n with k\n1's. This set is optimal, since K(x | n) = K(x, k | n) = K(k | n) + K(x | k, n) = K(S) + log |S|.\nNote that S encodes the number of 1s in x. The shortest program for S is an algorithmic minimal\nsufficient statistic for most x of length n with k 1's, since only a fraction of at most 2\u2212m x's of length n with\nk 1s can have K(x) < log |S| \u2212 m (Section 2.2). But of course there exist x's with k ones which have much\nmore regularity. An example is the string starting with k 1's followed by n \u2212 k 0's. For such strings, S is still\noptimal and the shortest program for S is still an algorithmic sufficient statistic, but not a minimal one. \u2666\n\n5.3\n\nRelating Probabilistic and Algorithmic Sufficiency\n\nWe want to relate 'algorithmic sufficient statistics' (defined independently of any model class {f\u03b8 }) to probabilistic sufficient statistics (defined relative to some model class {f\u03b8 } as in Section 5.1). We will show that,\n29\n\n\fessentially, algorithmic sufficient statistics are probabilistic nearly-sufficient statistics with respect to all\nmodel families {f\u03b8 }. Since the notion of algorithmic sufficiency is only defined to within additive constants,\nwe cannot expect algorithmic sufficient statistics to satisfy the requirements (5.4) or (5.5) for probabilistic\nsufficiency exactly, but only 'nearly4 '.\nNearly Sufficient Statistics: Intuitively, we may consider a probabilistic statistic S to be nearly sufficient\nif (5.4) or (5.5) holds to within some constant. For long sequences x, this constant will then be negligible\ncompared to the two terms in (5.4) or (5.5) which, for most practically interesting statistical model classes,\ntypically grow linearly in the sequence length. But now we encounter a difficulty:\nwhereas (5.4) and (5.5) are equivalent if they are required to hold exactly, they express something\nsubstantially different if they are only required to hold within a constant.\nBecause of our observation above, when relating probabilistic and algorithmic statistics we have to be very\ncareful about what happens if n is allowed to change. Thus, we need to extend probabilistic and algorithmic\nstatistics to strings of arbitrary length. This leads to the following generalized definition of a statistic:\nDefinition 5.9 A sequential statistic is a function S : {0, 1}\u2217 \u2192 2{0,1} , such that for all n, all x \u2208 {0, 1}n,\n(1) S(x) \u2286 {0, 1}n, and (2) x \u2208 S(x), and (3) for all n, the set\n\u2217\n\n{s | There exists x \u2208 {0, 1}n with S(x) = s }\nis a partition of {0, 1}n .\nAlgorithmic statistics are defined relative to individual x of some length n. Probabilistic statistics are\ndefined as functions, hence for all x of given length, but still relative to given length n. Such algorithmic\nand probabilistic statistics can be extended to each n and each x \u2208 {0, 1}n in a variety of ways; the three\nconditions in Definition 5.9 ensure that the extension is done in a reasonable way. Now let {f\u03b8 } be a model\nclass of sequential information sources (Section 1.2), i.e. a statistical model class defined for sequences of\n(n)\narbitrary length rather than just fixed n. As before, f\u03b8 denotes the marginal distribution of f\u03b8 on {0, 1}n.\nDefinition 5.10 We call sequential statistic S nearly-sufficient for {f\u03b8 } in the probabilistic-individual sense\nif there exist functions g (1) , g (2) , . . . and a constant c such that for all \u03b8, all n, every x \u2208 {0, 1}n ,\n\u0003\n(n)\nlog 1/f\u03b8 (x | S(x)) \u2212 log 1/g (n) (x|S(x)) \u2264 c.\n\n(5.15)\n\n\u0002\n\u0003\n(n)\n(n)\nf\u03b8 (x) log 1/f\u03b8 (x | S(x)) \u2212 log 1/g (n) (x|S(x)) \u2264 c\u2032 .\n\n(5.16)\n\nWe say S is nearly-sufficient for {f\u03b8 } in the probabilistic-expectation sense if there exists functions\ng (1) , g (2) , . . . and a constant c\u2032 such that for all \u03b8, all n,\nX\n\nx\u2208{0,1}n\n\nInequality (5.15) may be read as '(5.4) holds within a constant', whereas (5.16) may be read as '(5.5) holds\nwithin a constant'.\nRemark 5.11 Whereas the individual-sequence definition (5.4) and the expectation-definition (5.5) are\nequivalent if we require exact equality, they become quite different if we allow equality to within a constant\nas in Definition 5.10. To see this, let S be some sequential statistic such that for all large n, for some \u03b81 , \u03b82 ,\nfor some x \u2208 {0, 1}n,\n(n)\n(n)\nf\u03b81 (x | S(x)) \u226b f\u03b82 (x | S(x)),\n(n)\n\n(n)\n\nwhile for all x\u2032 6= x of length n, f\u03b81 (x|S(x)) \u2248 f\u03b82 (x|S(x)). If x has very small but nonzero probability\naccording to some \u03b8 \u2208 \u0398, then with very small f\u03b8 -probability, the difference between the left-hand and\nright-hand side of (5.4) is very large, and with large f\u03b8 -probability, the difference between the left-hand and\nright-hand side of (5.4) is about 0. Then S will be nearly sufficient in expectation, but not in the individual\nsense.\n\u2666\n\n4 We use 'nearly' rather than 'almost' since 'almost' suggests things like 'almost everywhere/almost surely/with probability\n1'. Instead, 'nearly' means, roughly speaking, 'to within O(1)'.\n\n30\n\n\fIn the theorem below we focus on probabilistic statistics that are 'nearly sufficient in an expected sense'. We\nconnect these to algorithmic sequential statistics, defined as follows:\nDefinition 5.12 A sequential statistic S is sufficient in the algorithmic sense if there is a constant c such\nthat for all n, all x \u2208 {0, 1}n, the program generating S(x) is an algorithmic sufficient statistic for x (relative\nto constant c), i.e.\nK(S(x)) + log |S(x)| \u2264 K(x) + c.\n(5.17)\nIn Theorem 5.13 we relate algorithmic to probabilistic sufficiency. In the theorem, S represents a sequential\nstatistic, {f\u03b8 } is a model class of sequential information sources and g (n) is the conditional probability mass\nfunction arising from the uniform distribution:\n(\n1/|{x \u2208 {0, 1}n : S(x) = s}| if S(x) = s\ng (n) (x|s) =\n0\notherwise.\nTheorem 5.13 (algorithmic sufficient statistic is probabilistic sufficient statistic) Let S be a sequential statistic that is sufficient in the algorithmic sense. Then for every \u03b8 with K(f\u03b8 ) < \u221e, there\nexists a constant c, such that for all n, inequality (5.16) holds with g (n) the uniform distribution. Thus,\nif sup\u03b8\u2208\u0398 K(f\u03b8 ) < \u221e, then S is a nearly-sufficient statistic for {f\u03b8 } in the probabilistic-expectation sense,\nwith g equal to the uniform distribution.\nProof. The definition of algorithmic sufficiency, (5.17) directly implies that there exists a constant c such\nthat for all \u03b8, all n,\nX\nX\n\u0002\n\u0003\n(n)\n(n)\nf\u03b8 (x)K(x) + c.\n(5.18)\nf\u03b8 (x) K(S(x)) + log |S(x)| \u2264\nx\u2208{0,1}n\n\nx\u2208{0,1}n\n\nNow fix any \u03b8 with K(f\u03b8 ) < \u221e. It follows (by the same reasoning as in Theorem 2.10) that for some\nc\u03b8 \u2248 K(f\u03b8 ), for all n,\nX\nX\n(n)\n(n)\nf\u03b8 (x) log 1/f\u03b8 (x) \u2264 c\u03b8 .\n(5.19)\nf\u03b8 (x)K(x) \u2212\n0\u2264\nx\u2208{0,1}n\n\nx\u2208{0,1}n\n\nEssentially, the left inequality follows by the information inequality (3.7): no code can be more efficient\nin expectation under f\u03b8 than the Shannon-Fano code with lengths log 1/f\u03b8 (x); the right inequality follows\nbecause, since K(f\u03b8 ) < \u221e, the Shannon-Fano code can be implemented by a computer program with a\nfixed-size independent of n. By (5.19), (5.18) becomes: for all n,\nX\nX\n\u0002\n\u0003 X (n)\n(n)\n(n)\nf\u03b8 (x) K(S(x)) + log |S(x)| \u2264\nf\u03b8 (x) log 1/f\u03b8 (x) \u2264\nf\u03b8 (x) log 1/f\u03b8 (x) + c\u03b8 . (5.20)\nx\u2208{0,1}n\n\nx\n\nx\u2208{0,1}n\n\n(n)\n\nFor s \u2286 {0, 1}n, we use the notation f\u03b8 (s) according to (5.8). Note that, by requirement (3) in the definition\nof sequential statistic,\nX\n(n)\nf\u03b8 (s) = 1,\ns:\u2203x\u2208{0,1}n :S(x)=s\n\n(n)\n\nwhence f\u03b8 (s) is a probability mass function on S, the set of values the statistic S can take on sequences of\nlength n. Thus, we get, once again by the information inequality (3.7),\nX\nX\n(n)\n(n)\n(n)\nf\u03b8 (x)K(S(x)) \u2265\nf\u03b8 (x) log 1/f\u03b8 (S(x)).\n(5.21)\nx\u2208{0,1}n\n\nx\u2208{0,1}n\n\nNow note that for all n,\nX\n\u0002\n\u0003\n(n)\n(n)\n(n)\nf\u03b8 (x) log 1/f\u03b8 (S(x)) + log 1/f\u03b8 (x | S(x)) =\nx\u2208{0,1}n\n\n31\n\nX\n\nx\u2208{0,1}n\n\n(n)\n\nf\u03b8 (x) log 1/f\u03b8 (x).\n\n(5.22)\n\n\f(n)\n\nConsider the two-part code which encodes x by first encoding S(x) using log 1/f\u03b8 (S(x)) bits, and then\nencoding x using log |S(x)| bits. By the information inequality, (3.7), this code must be less efficient than\nthe Shannon-Fano code with lengths log 1/f\u03b8 (x), so that if follows from (5.22) that, for all n,\nX\nX\n(n)\n(n)\n(n)\nf\u03b8 (x) log |S(x)| \u2265\nf\u03b8 (x) log 1/f\u03b8 (x | S(x)).\n(5.23)\nx\u2208{0,1}n\n\nx\u2208{0,1}n\n\nNow defining\nu\n\n=\n\nX\n\n(n)\n\nf\u03b8 (x)K(S(x))\n\nx\u2208{0,1}n\n\nv\n\n=\n\nX\n\n(n)\n\nx\u2208{0,1}n\n\nu\u2032\n\n=\n\nX\n\nf\u03b8 (x) log |S(x)|\n(n)\n\n(n)\n\n(n)\n\n(n)\n\nf\u03b8 (x) log 1/f\u03b8 (S(x))\n\nx\u2208{0,1}n\n\nv\n\n\u2032\n\n=\n\nX\n\nx\u2208{0,1}n\n\nw\n\n=\n\nX\n\nf\u03b8 (x) log 1/f\u03b8 (x | S(x))\n(n)\n\nf\u03b8 (x) log 1/f\u03b8 (x),\n\nx\u2208{0,1}n\n+\n\nwe find that (5.20), (5.22), (5.21) and (5.23) express, respectively, that u + v = w, u\u2032 + v \u2032 = w, u \u2265 u\u2032 ,\n+\nv \u2265 v \u2032 . It follows that v = v \u2032 , so that (5.23) must actually hold with equality up to a constant. That is,\n\u2032\nthere exist a c such that for all n,\nX\nX\n(n)\n(n)\n(n)\nf\u03b8 (x) log 1/f\u03b8 (x | S(x)) \u2264 c\u2032 .\n(5.24)\nf\u03b8 (x) log |S(x)| \u2212\nx\u2208{0,1}n\n\nx\u2208{0,1}n\n\nThe result now follows upon noting that (5.24) is just (5.16) with g (n) the uniform distribution.\n\n6\n\n\u0003\n\nRate Distortion and Structure Function\n\nWe continue the discussion about meaningful information of Section 5.2.1. This time we a priori restrict\nthe number of bits allowed for conveying the essence of the information. In the probabilistic situation\nthis takes the form of allowing only a \"rate\" of R bits to communicate as well as possible, on average,\nthe outcome of a random variable X, while the set X of outcomes has cardinality possibly exceeding 2R .\nClearly, not all outcomes can be communicated without information loss, the average of which is expressed\nby the \"distortion\". This leads to the so-called \"rate\u2013distortion\" theory. In the algorithmic setting the\ncorresponding idea is to consider a set of models from which to choose a single model that expresses the\n\"meaning\" of the given individual data x as well as possible. If we allow only R bits to express the model,\nwhile possibly the Kolmogorov complexity K(x) > R, we suffer information loss-a situation that arises for\nexample with \"lossy\" compression. In the latter situation, the data cannot be perfectly reconstructed from\nthe model, and the question arises in how far the model can capture the meaning present in the specific data\nx. This leads to the so-called \"structure function\" theory.\nThe limit of R bits to express a model to capture the most meaningful information in the data is an\nindividual version of the average notion of \"rate\". The remaining less meaningful information in the data\nis the individual version of the average-case notion of \"distortion\". If the R bits are sufficient to express all\nmeaning in the data then the resulting model is called a \"sufficient statistic\", in the sense introduced above.\nThe remaining information in the data is then purely accidental, random, noise. For example, a sequence\nof outcomes of n tosses of a coin with computable bias p, typically has a sufficient statistic of\n\u221a K(p) bits,\nwhile the remaining random information is typically at least about pn \u2212 K(p) bits (up to an O( n) additive\nterm).\n\n6.1\n\nRate Distortion\n\nInitially, Shannon [22] introduced rate-distortion as follows: \"Practically, we are not interested in exact\ntransmission when we have a continuous source, but only in transmission to within a given tolerance. The\n\n32\n\n\fquestion is, can we assign a definite rate to a continuous source when we require only a certain fidelity\nof recovery, measured in a suitable way.\" Later, in [23] he applied this idea to lossy data compression of\ndiscrete memoryless sources-our topic below. As before, we consider a situation in which sender A wants\nto communicate the outcome of random variable X to receiver B. Let X take values in some set X , and\nthe distribution P of X be known to both A and B. The change is that now A is only allowed to use a\nfinite number, say R bits, to communicate, so that A can only send 2R different messages. Let us denote\nby Y the encoding function used by A. This Y maps X onto some set Y. We require that |Y| \u2264 2R . If\n|X | > 2R or if X is continuous-valued, then necessarily some information is lost during the communication.\nThere is no decoding function D : Y \u2192 X such that D(Y (x)) = x for all x. Thus, A and B cannot\nensure that x can always be reconstructed. As the next best thing, they may agree on a code such that\nfor all x, the value Y (x) contains as much useful information about x as is possible-what exactly 'useful'\nmeans depends on the situation at hand; examples are provided below. An easy example would be that\nY (x) is a finite list of elements, one of which is x. We assume that the 'goodness' of Y (x) is gaged by a\ndistortion function d : X \u00d7 Y \u2192 [0, \u221e]. This distortion function may be any nonnegative function that\nis appropriate to the situation at hand. In the example above it could be the logarithm of the number of\nelements in the list Y (x). Examples of some common distortion functions are the Hamming distance and\nthe squared Euclidean distance. We can view Y as a a random variable on the space Y, a coarse version of\nthe random\nP variable X, defined as taking value Y = y if X = x with Y (x) = y. Write f (x) = P (X = x) and\ng(y) = x:Y (x)=y P (X = x). Once the distortion function d is fixed, we define the expected distortion by\nE[d(X, Y )] =\n\nX\n\nf (x)d(x, Y (x))\n\n(6.1)\n\nx\u2208X\n\n=\n\nX\n\ny\u2208Y\n\ng(y)\n\nX\n\nf (x)/g(y)d(x, y).\n\nx:Y (x)=y\n\nIf X is a continuous random variable, the sum should be replaced by an integral.\nExample 6.1 In most standard applications of rate distortion theory, the goal is to compress x in a 'lossy'\nway, such that x can be reconstructed 'as well as possible' from Y (x). In that case, Y \u2286 X and writing\nx\u0302 = Y (x), the value d(x, x\u0302) measures the similarity between x and x\u0302. For example, with X is the set of\nreal numbers and Y is the set of integers, the squared difference d(x, x\u0302) = (x \u2212 x\u0302)2 is a viable distortion\nfunction. We may interpret x\u0302 as an estimate of x, and Y as the set of values it can take. The reason we\nuse the notation Y rather than X\u0302 (as in, for example, [4]) is that further below, we mostly concentrate on\nslightly non-standard applications where Y should not be interpreted as a subset of X .\n\u2666\nWe want to determine the optimal code Y for communication between A and B under the constraint that there\nare no more than 2R messages. That is, we look for the encoding function Y that minimizes the expected\ndistortion, under the constraint that |Y| \u2264 2R . Usually, the minimum achievable expected distortion is\nnonincreasing as a function of increasing R.\nExample 6.2 Suppose X is a real-valued, normally (Gaussian) distributed random variable with mean\nE[X] = 0 and variance E[X \u2212 E[X]]2 = \u03c3 2 . Let us use the squared Euclidean distance d(x, y) = (x \u2212 y)2 as\na distortion measure. If A is allowed to use R bits, then Y can have no more than 2R elements, in contrast\nto X that is uncountably infinite. We should choose Y and the function Y such that (6.1) is minimized.\nSuppose first R = 1. Then the optimal Y turns out to be\n\uf8f1q\n\uf8f2 2 \u03c32\nif x \u2265 0\n\u03c0\nq\nY (x) =\n\uf8f3\u2212 2 \u03c3 2 if x < 0.\n\u03c0\n\nThus, the domain X is partitioned into two regions, one corresponding to x \u2265 0, and one to x < 0. By the\nsymmetry of the Gaussian distribution around 0, it should be clear that this is the best one can do. Within\neach of the two region, one picks a 'representative point' so as to minimize (6.1). This mapping allows B to\nestimate x as well as possible.\nSimilarly, if R = 2, then X should be partitioned into 4 regions, each of which are to be represented\nby a single point such that (6.1) is minimized. An extreme case is R = 0: how can B estimate X if it is\nalways given the same information? This means that Y (x) must take the same value for all x. The expected\ndistortion (6.1) is then minimized if Y (x) \u2261 0, the mean of X, giving distortion equal to \u03c3 2 .\n\u2666\n33\n\n\fIn general, there is no need for the space of estimates Y to be a subset of X . We may, for example, also\nlossily encode or 'estimate' the actual value of x by specifying a set in which x must lie (Section 6.2) or a\nprobability distribution (see below) on X .\nExample 6.3 Suppose receiver B wants to estimate the actual x by a probability distribution P on X . Thus,\nif R bits are allowed to be used, one of 2R different distributions on X can be sent to receiver. The most\naccurate that can be done is to partition X into 2R subsets A1 , . . . , A2R . Relative to any such partition, we\nintroduce a new random variable Y and abbreviate the event x \u2208 Ay to Y = y. Sender observes that Y = y\nfor some y \u2208 Y = {1, . . . , 2R } and passes this information on to receiver. The information y actually means\nthat X is now distributed according to the conditional distribution P (X = x | x \u2208 Ay ) = P (X = x | Y = y).\nIt is now natural to measure the quality of the transmitted distribution P (X = x | Y = y) by its\nconditional entropy, i.e. the expected additional number of bits that sender has to transmit before receiver\nknows the value of x with certainty. This can be achieved by taking\nd(x, y) = log 1/P (X = x | Y = y),\n\n(6.2)\n\nwhich we abbreviate to d(x, y) = log 1/f (x|y). In words, the distortion function is the Shannon-Fano code\nlength for the communicated distribution. The expected distortion then becomes equal to the conditional\nentropy H(X | Y ) as defined in Section 3.1 (rewrite according to (6.1), f (x|y) = f (x)/g(y) for P (X =\nx|Y (x) = y) and g(y) defined earlier, and the definition of conditional probability):\nX\nX\n(f (x)/g(y))d(x, y)\n(6.3)\nE[d(X, Y )] =\ng(y)\ny\u2208Y\n\n=\n\nX\n\nx:Y (x)=y\n\ng(y)\n\ny\u2208Y\n\nX\n\nf (x|y) log 1/f (x|y)\n\nx:Y (x)=y\n\n= H(X|Y ).\nHow is this related to lossless compression? Suppose for example that R = 1. Then the optimal distortion is\nachieved by partitioning X into two sets A1 , A2 in the most 'informative' possible way, so that the conditional\nentropy\nX\nH(X|Y ) =\nP (Y = y)H(X|Y = y)\ny=1,2\n\nis minimized. If Y itself is encoded\nwith the Shannon-Fano code, then H(Y ) bits\nPare needed to communicate\nP\nY . Rewriting H(X|Y ) = y\u2208Y P (Y = y)H(X|Y = y) and H(X|Y = y) = x:Y (x)=y f (x|y) log 1/f (x|y)\nwith f (x|y) = P (X = x)/P (Y = y) and rearranging, shows that for all such partitions of X into |Y| subsets\ndefined by Y : X \u2192 Y we have\nH(X|Y ) + H(Y ) = H(X).\n(6.4)\nThe minimum rate distortion is obtained by choosing the function Y that minimizes H(X|Y ). By (6.4) this\nis also the Y maximizing H(Y ). Thus, the average total number of bits we need to send our message in this\nway is still equal to H(X)-the more we save in the second part, the more we pay in the first part.\n\u2666\nRate Distortion and Mutual Information: Already in his 1948 paper, Shannon established a deep\nrelation between mutual information and minimum achievable distortion for (essentially) arbitrary distortion\nfunctions. The relation is summarized in Theorem 6.8 below. To prepare for the theorem, we need to slightly\nextend our setting by considering independent repetitions of the same scenario. This can be motivated in\nvarious ways such as (a) it often corresponds to the situation we are trying to model; (b) it allows us to\nconsider non-integer rates R, and (c) it greatly simplifies the mathematical analysis.\nDefinition 6.4 Let X , Y be two sample spaces. The distortion of y \u2208 Y with respect to x \u2208 X is defined by\na nonnegative real-valued function d(x, y) as above. We extend the definition to sequences: the distortion of\n(y1 , . . . , yn ) with respect to (x1 , . . . , xn ) is\nn\n\nd((x1 , . . . , xn ), (y1 , . . . , yn )) :=\n\n34\n\n1X\nd(xi , yi ).\nn i=1\n\n(6.5)\n\n\fLet X1 , . . . , Xn be n independent identically distributed random variables on outcome space X . Let Y be\na set of code words. We want to find a sequence of functions Y1 , . . . , Yn : X \u2192 Y so that the message\n(Y1 (x1 ), . . . , Yn (xn )) \u2208 Y n gives as much expected information about the sequence of outcomes (X1 =\nx1 , . . . , Xn = xn ) as is possible, under the constraint that the message takes at most R * n bits (so that R bits\nare allowed on average per outcome of Xi ). Instead of Y1 , . . . , Yn above write Zn : X n \u2192 Y n . The expected\ndistortion E[d(X n , Zn )] for Zn is\nE[d(X n , Zn )] =\n\nX\n\nn\n\n(x1 ,...,xn )\u2208X n\n\nP (X n = (x1 , . . . , xn )) *\n\n1X\nd(xi , Yi (xi )).\nn i=1\n\n(6.6)\n\nConsider functions Zn with range Zn \u2286 Y n satisfying |Zn | \u2264 2nR . Let for n \u2265 1 random variables a choice\nY1 , . . . , Yn minimize the expected distortion under these constraints, and let the corresponding value Dn\u2217 (R)\nof the expected distortion be defined by\nDn\u2217 (R) =\n\nmin\n\nZn :|Zn |\u22642nR\n\nE(d(X n , Zn )).\n\n(6.7)\n\n\u2217\n\u2217\nLemma 6.5 For every distortion measure, and all R, n, m \u2265 1, (n + m)Dn+m\n(R) \u2264 nDn\u2217 (R) + mDm\n(R).\n\u2217\nProof. Let Y1 , . . . , Yn achieve Dn\u2217 (R) and Y1\u2032 , . . . , Ym\u2032 achieve Dm\n(R). Then, Y1 , . . . , Yn , Y1\u2032 , . . . , Ym\u2032\n\u2217\n\u2217\n\u2217\nachieves (nDn (R) + mDm (R))/(n + m). This is an upper bound on the minimal possible value Dn+m\n(R)\nfor n + m random variables.\n\u0003\n\u2217\nIt follows that for all R, n \u2265 1 we have D2n\n(R) \u2264 Dn\u2217 (R). The inequality is typically strict; [4] gives an\nintuitive explanation of this phenomenon. For fixed R the value of D1\u2217 (R) is fixed and it is finite. Since also\nDn\u2217 (R) is necessarily positive for all n, we have established the existence of the limit\n\nD\u2217 (R) = lim inf Dn\u2217 (R).\nn\u2192\u221e\n\n(6.8)\n\nThe value of D\u2217 (R) is the minimum achievable distortion at rate (number of bits/outcome) R. Therefore,\nD\u2217 (*) It is called the distortion-rate function. In our Gaussian Example 6.2, D\u2217 (R) quickly converges to 0\nwith increasing R. It turns out that for general d, when we view D\u2217 (R) as a function of R \u2208 [0, \u221e), it is\nconvex and nonincreasing.\nExample 6.6 Let X = {0, 1}, and let P (X = 1) = p. Let Y = {0, 1} and take the Shannon-Fano distortion\nfunction d(x, y) = log 1/f (x | y) with notation as in Example 6.3. Let Y be a function that achieves the\nminimum expected Shannon-Fano distortion D1\u2217 (R). As usual we write Y for the random variable Y (x)\ninduced by X. Then, D1\u2217 (R) = E[d(X, Y )] = E[log 1/f (X|Y )] = H(X|Y ). At rate R = 1, we can set Y = X\nand the minimum achievable distortion is given by D1\u2217 (1) = H(X|X) = 0. Now consider some rate R with\n0 < R < 1, say R = 21 . Since we are now forced to use less than 2R < 2 messages in communicating, only a\nfixed message can be sent, no matter what outcome of the random variable X is realized. This means that\nno communication is possible at all and the minimum achievable distortion is D1\u2217 ( 21 ) = H(X) = H(p, 1 \u2212 p).\nBut clearly, if we consider n repetitions of the same scenario and are allowed to send a message out of \u230a2nR \u230b\ncandidates, then some useful information can be communicated after all, even if R < 1. In Example 6.9 we\nwill show that if R > H(p, 1 \u2212 p), then D\u2217 (R) = 0; if R \u2264 H(p, 1 \u2212 p), then D\u2217 (R) = H(p, 1 \u2212 p) \u2212 R. \u2666\nUp to now we studied the minimum achievable distortion D as a function of the rate R. For technical reasons,\nit is often more convenient to consider the minimum achievable rate R as a function of the distortion D.\nThis is the more celebrated version, the rate-distortion function R\u2217 (D). Because D\u2217 (R) is convex and\nnonincreasing, R\u2217 (D) : [0, \u221e) \u2192 [0, \u221e] is just the inverse of the function D\u2217 (R).\nIt turns out to be possible to relate distortion to the Shannon mutual information. This remarkable fact,\nwhich Shannon proved already in [22, 23], illustrates the fundamental nature of Shannon's concepts. Up till\nnow, we only considered deterministic encodings Y : X \u2192 Y. But it is hard to analyze the rate-distortion,\nand distortion-rate, functions in this setting. It turns out to be advantageous to follow an indirect route\nby bringing information-theoretic techniques into play. To this end, we generalize the setting to randomized\nencodings. That is, upon observing X = x with probability f (x), the sender may use a randomizing device\n(e.g. a coin) to decide which code word in y \u2208 Y he is going to send to the receiver. A randomized encoding\nY thus maps each x \u2208 X to y \u2208 Y with probability gx (y), denoted in conditional probability format as\ng(y|x). Altogether we deal with a joint distribution g(x, y) = f (x)g(y|x) on the joint sample space X \u00d7 Y.\n(In the deterministic case we have g(Y (x) | x) = 1 for the given function Y : X \u2192 Y.)\n35\n\n\fDefinition 6.7 Let X and Y be joint random variables as above, and let d(x, y) be a distortion measure.\nThe expected distortion D(X, Y ) of Y with respect to X is defined by\nX\nD(X, Y ) =\ng(x, y)d(x, y).\n(6.9)\nx\u2208X ,y\u2208Y\n\nNote that for a given problem the source probability f (x) of outcome X = x is fixed, but the randomized\nencoding Y , that is the conditional probability g(y|x) of encoding source word x by code word y, can be\nchosen to advantage. We define the auxiliary notion of information rate distortion function R(I) (D) by\nR(I) (D) =\n\ninf\n\nY :D(X,Y )\u2264D\n\nI(X; Y ).\n\n(6.10)\n\nThat is, for random variable X, among all joint random variables Y with expected distortion to X less than\nor equal to D, the information rate R(I) (D) equals the minimal mutual information with X.\nTheorem 6.8 (Shannon) For every random source X and distortion measure d:\nR\u2217 (D) = R(I) (D)\n\n(6.11)\n\nThis remarkable theorem states that the best deterministic code achieves a rate-distortion that equals the\nminimal information rate possible for a randomized code, that is, the minimal mutual information between\nthe random source and a randomized code. Note that this does not mean that R\u2217 (D) is independent of the\ndistortion measure. In fact, the source random variable X, together with the distortion measure d, determines\na random code Y for which the joint random variables X and Y reach the infimum in (6.10). The proof\nof this theorem is given in [4]. It is illuminating to see how it goes: It is shown first that, for a random\nsource X and distortion measure d, every deterministic code Y with distortion \u2264 D has rate R \u2265 R(I) (D).\nSubsequently, it is shown that there exists a deterministic code that, with distortion \u2264 D, achieves rate\nR\u2217 (D) = R(I) (D). To analyze deterministic R\u2217 (D) therefore, we can determine the best randomized code\nY for random source X under distortion constraint D, and then we know that simply R\u2217 (D) = I(X; Y ).\nExample 6.9 (Example 6.6, continued) Suppose we want to compute R\u2217 (D) for some D between 0 and\n1. If we only allow encodings Y that are deterministic functions of X, then either Y (x) \u2261 x or Y (x) \u2261\n|1 \u2212 x|. In both cases E[d(X, Y )] = H(X|Y ) = 0, so Y satisfies the constraint in (6.10). In both cases,\nI(X, Y ) = H(Y ) = H(X). With (6.11) this shows that R\u2217 (D) \u2264 H(X). However, R\u2217 (D) is actually smaller:\nby allowing randomized codes, we can define Y\u03b1 as Y\u03b1 (x) = x with probability \u03b1 and Y\u03b1 (x) = |1 \u2212 x| with\nprobability 1 \u2212 \u03b1. For 0 \u2264 \u03b1 \u2264 21 , E[d(X, Y\u03b1 )] = H(X|Y\u03b1 ) increases with \u03b1, while I(X; Y\u03b1 ) decreases\nwith \u03b1. Thus, by choosing the \u03b1\u2217 for which the constraint E[d(X, Y\u03b1 )] \u2264 D holds with equality, we find\nR\u2217 (D) = I(X; Y\u03b1\u2217 ). Let us now calculate R\u2217 (D) and D\u2217 (R) explicitly.\nSince I(X, Y ) = H(X) \u2212 H(X|Y ), we can rewrite R\u2217 (D) as\nR\u2217 (D) = H(X) \u2212\n\nsup\n\nH(X|Y ).\n\nY :D(X,Y )\u2264D\n\nIn the special case where D is itself the Shannon-Fano distortion, this can in turn be rewritten as\nR\u2217 (D) = H(X) \u2212\n\nsup\nY :H(X|Y )\u2264D\n\nH(X | Y ) = H(X) \u2212 D.\n\nSince D\u2217 (R) is the inverse of R\u2217 (D), we find D\u2217 (R) = H(X) \u2212 R, as announced in Example 6.6.\n\n\u2666\n\nProblem and Lacuna: In the Rate-Distortion setting we allow (on average) a rate of R bits to express\nthe data as well as possible in some way, and measure the average of loss by some distortion function. But\nin many cases, like lossy compression of images, one is interested in the individual cases. The average over\nall possible images may be irrelevant for the individual cases one meets. Moreover, one is not particularly\ninterested in bit-loss, but rather in preserving the essence of the image as well as possible. As another\nexample, suppose the distortion function is simply to supply the remaining bits of the data. But this can\nbe unsatisfactory: we are given an outcome of a measurement as a real number of n significant bits. Then\nthe R most significant bits carry most of the meaning of the data, while the remaining n \u2212 R bits may\nbe irrelevant. Thus, we are lead to the elusive notion of a distortion function that captures the amount of\n\n36\n\n\f\u03bb x (\u03b1)\n\n|x|\nK(x)\n\nlog |S|\n\nh\n\nx\n\n(\u03b1)\n\n\u03b2 x (\u03b1)\n\nK(x)\n\nminimal sufficient statistic\n\u03b1\n\nFigure 1: Structure functions hx (i), \u03b2x (\u03b1), \u03bbx (\u03b1), and minimal sufficient statistic.\n\"meaning\" that is not included in the R rate bits. These issues are taken up by Kolmogorov's proposal of\nthe structure function. This cluster of ideas puts the notion of Rate\u2013Distortion in an individual algorithmic\n(Kolmogorov complexity) setting, and focuses on the meaningful information in the data. In the end we\ncan recycle the new insights and connect them to Rate-Distortion notions to provide new foundations for\nstatistical inference notions as maximum likelihood (ML) [5], minimum message length (MML) [28], and\nminimum description length (MDL) [20].\n\n6.2\n\nStructure Function\n\nThere is a close relation between functions describing three, a priori seemingly unrelated, aspects of modeling\nindividual data, depicted in Figure 1. One of these was introduced by Kolmogorov at a conference in Tallinn\n1974 (no written version) and in a talk at the Moscow Mathematical Society in the same year of which the\nabstract [11] is as follows (this is the only writing by Kolmogorov about this circle of ideas):\n\"To each constructive object corresponds a function \u03a6x (k) of a natural number k-the log of\nminimal cardinality of x-containing sets that allow definitions of complexity at most k. If the\nelement x itself allows a simple definition, then the function \u03a6 drops to 1 even for small k. Lacking\nsuch definition, the element is \"random\" in a negative sense. But it is positively \"probabilistically\nrandom\" only when function \u03a6 having taken the value \u03a60 at a relatively small k = k0 , then\nchanges approximately as \u03a6(k) = \u03a60 \u2212 (k \u2212 k0 ).\"\nKolmogorov's \u03a6x is commonly called the \"structure function\" and is here denoted as hx and defined in (6.14).\nThe structure function notion entails a proposal for a non-probabilistic approach to statistics, an individual\ncombinatorial relation between the data and its model, expressed in terms of Kolmogorov complexity. It turns\nout that the structure function determines all stochastic properties of the data in the sense of determining\nthe best-fitting model at every model-complexity level, the equivalent notion to \"rate\" in the Shannon\ntheory. A consequence is this: minimizing the data-to-model code length (finding the ML estimator or\nMDL estimator), in a class of contemplated models of prescribed maximal (Kolmogorov) complexity, always\nresults in a model of best fit, irrespective of whether the source producing the data is in the model class\nconsidered. In this setting, code length minimization always separates optimal model information from the\nremaining accidental information, and not only with high probability. The function that maps the maximal\nallowed model complexity to the goodness-of-fit (expressed as minimal \"randomness deficiency\") of the best\nmodel cannot itself be monotonically approximated. However, the shortest one-part or two-part code above\ncan-implicitly optimizing this elusive goodness-of-fit.\nIn probabilistic statistics the goodness of the selection process is measured in terms of expectations over\nprobabilistic ensembles. For current applications, average relations are often irrelevant, since the part of the\n\n37\n\n\fsupport of the probability mass function that will ever be observed has about zero measure. This may be\nthe case in, for example, complex video and sound analysis. There arises the problem that for individual\ncases the selection performance may be bad although the performance is good on average, or vice versa.\nThere is also the problem of what probability means, whether it is subjective, objective, or exists at all.\nKolmogorov's proposal strives for the firmer and less contentious ground of finite combinatorics and effective\ncomputation.\nModel Selection: It is technically convenient to initially consider the simple model class of finite sets to\nobtain our results, just as in Section 5.2. It then turns out that it is relatively easy to generalize everything\nto the model class of computable probability distributions (Section 6.2.1). That class is very large indeed:\nperhaps it contains every distribution that has ever been considered in statistics and probability theory,\nas long as the parameters are computable numbers-for example rational numbers. Thus the results are\nof great generality; indeed, they are so general that further development of the theory must be aimed at\nrestrictions on this model class.\nBelow we will consider various model selection procedures. These are approaches for finding a model S\n(containing x) for arbitrary data x. The goal is to find a model that captures all meaningful information in\nthe data x . All approaches we consider are at some level based on coding x by giving its index in the set S,\ntaking log |S| bits. This codelength may be thought of as a particular distortion function, and here lies the\nfirst connection to Shannon's rate-distortion:\nExample 6.10 A model selection procedure is a function Zn mapping binary data of length n to finite sets of\nn\nstrings of length n, containing the mapped data, Zn (x) = S (x \u2208 S). The range of Zn satisfies Zn \u2286 2{0,1} ,\nThe distortion function d is defined to be d(x, Y (x)) = n1 log |S|. To define the rate\u2013distortion function we\nneed that x is the outcome of a random variable X. Here we treat the simple case that X represents n flips of\na fair coin; this is substantially generalized in Section 6.3. Since each outcome ofP\na fair coin can be described\nby one bit, we set the rate R at 0 < R < 1. Then, Dn\u2217 (R) = minZn :|Zn |\u22642nR |x|=n 2\u2212n n1 log |Zn (x)| For\nthe minimum of the right-hand side we can assume that if y \u2208 Zn (x) then Zn (y) = Zn (x) (the distinct\nZn (x)'s are disjoint). Denote the distinct Zn (x)'s by Zn,i with i = 1, . . . , k for some k \u2264 2nR . Then,\nP\nDn\u2217 (R) = minZn :|Zn |\u22642nR k1=1 |Zn,i |2\u2212n n1 log |Zn,i |. The right-hand side reaches its minimum for all Zn,i 's\nhaving the same cardinality and k = 2nR . Then, Dn\u2217 (R) = 2nR 2(1\u2212R)n 2\u2212n n1 log 2(1\u2212R)n = 1 \u2212 R. Therefore,\nD\u2217 (R) = 1 \u2212 R and therefore R\u2217 (D) = 1 \u2212 D.\nAlternatively, and more in line with the structure-function approach below, one may consider repetitions\nof a random variable X with outcomes in {0, 1}n . Then, a model selection procedure is a function Y\nmapping binary data of length n to finite sets of strings of length n, containing the mapped data, Y (x) = S\nn\n(x \u2208 S). The range of Y satisfies Y \u2286 2{0,1} , The distortion function d is defined by d(x, Y (x)) = log |S|.\nTo define the rate\u2013distortion function we need that x is the outcome of a random variable X, say a toss\nof a fair 2n -sided coin. Since each outcome of a fair coin can be described by n bits, we set the rate R\nat 0 < R < n. Then, for outcomes xP= x1 . . . xm (|xi | = n), resulting from m i.i.d. random variables\nm\n1\n1\n\u2217\nX1 , . . . , Xm , we P\nhave d(x, Zm (x)) = m\ni=1 log |Yi (xi )| = m log |Y1 (x1 ) \u00d7 * * * \u00d7 Ym (xm )|. Then, Dm (R) =\n\u2212mn\nminZm :|Zm |\u22642mR x 2\nd(x, Zm (x)). Assume that y \u2208 Zm (x) if Zm (y) = Zm (x): the distinct Zm (x)'s\nmn\nare disjoint and partition {0,\ninto disjoint subsets Zm,i , with i = 1, . . . , k for some k \u2264 2mR . Then,\nP 1}\n1\n\u2217\nlog |Zm,i |. The right-hand side reaches its minimum for all\nDm (R) = minZm :|Zm |\u22642mR i=1,...,k |Zm,i |2\u2212mn m\n1\nmR\n\u2217\nZm,i 's having the same cardinality and k = 2 , so that Dm\n(R) = 2(n\u2212R)m 2mR 2\u2212mn m\nlog 2(n\u2212R)m = n\u2212R.\n\u2217\n\u2217\nTherefore, D (R) = n \u2212 R and R (D) = n \u2212 D. In Example 6.13 we relate these numbers to the structure\nfunction approach described below.\n\u2666\nModel Fitness: A distinguishing feature of the structure function approach is that we want to formalize\nwhat it means for an element to be \"typical\" for a set that contains it. For example, if we flip a fair coin\nn times, then the sequence of n outcomes, denoted by x, will be an element of the set {0, 1}n. In fact,\nmost likely it will be a \"typical\" element in the sense\u221athat it has all properties that hold on average for an\nelement of that set. For example, x will have n2 \u00b1 O( n) frequency of 1's, it will have a run of about log n\nconsecutive 0's, and so on for many properties. Note that the sequence x = 0 . . . 01 . . . 1, consisting of one\nhalf 0's followed by one half ones, is very untypical, even though it satisfies the two properties described\nexplicitly. The question arises how to formally define \"typicality\". We do this as follows: The lack of\ntypicality of x with respect to a finite set S (the model) containing it, is the amount by which K(x|S) falls\n\n38\n\n\fshort of the length log |S| of the data-to-model code (Section 5.2). Thus, the randomness deficiency of x in\nS is defined by\n\u03b4(x|S) = log |S| \u2212 K(x|S),\n(6.12)\nfor x \u2208 S, and \u221e otherwise. Clearly, x can be typical for vastly different sets. For example, every x is\ntypical for the singleton set {x}, since log |{x}| = 0 and K(x | {x}) = O(1). Yet the many x's that have\nK(x) \u2265 n are also typical for {0, 1}n , but in another way. In the first example, the set is about as complex\nas x itself. In the second example, the set is vastly less complex than x: the set has complexity about\nK(n) \u2264 log n + 2 log log n while K(x) \u2265 n. Thus, very high complexity data may have simple sets for\nwhich they are typical. As we shall see, this is certainly not the case for all high complexity data. The\nquestion arises how typical data x of length n can be in the best case for a finite set of complexity R when\nR ranges from 0 to n. The function describing this dependency, expressed in terms of randomness deficiency\nto measure the optimal typicality, as a function of the complexity \"rate\" R (0 \u2264 R \u2264 n) of the number of\nbits we can maximally spend to describe a finite set containing x, is defined as follows:\nThe minimal randomness deficiency function is\n\u03b2x (R) = min{\u03b4(x|S) : S \u220b x, K(S) \u2264 R},\nS\n\n(6.13)\n\nwhere we set min \u2205 = \u221e. If \u03b4(x|S) is small, then x may be considered as a typical member of S. This means\nthat S is a \"best\" model for x-a most likely explanation. There are no simple special properties that single\nit out from the majority of elements in S. We therefore like to call \u03b2x (R) the best-fit estimator. This is not\njust terminology: If \u03b4(x|S) is small, then x satisfies all properties of low Kolmogorov complexity that hold\nwith high probability (under the uniform distribution) for the elements of S. To be precise [26]: Consider\nstrings of length n and let S be a subset of such strings. We view a property of elements in S as a function\nfP : S \u2192 {0, 1}. If fP (x) = 1 then x has the property represented by fP and if fP (x) = 0 then x does not\nhave the property. Then: (i) If fP is a property satisfied by all x with \u03b4(x|S) \u2264 \u03b4(n), then fP holds with\nprobability at least 1 \u2212 1/2\u03b4(n) for the elements of S.\n(ii) Let fP be any property that holds with probability at least 1 \u2212 1/2\u03b4(n) for the elements of S. Then,\nevery such fP holds simultaneously for every x \u2208 S with \u03b4(x|S) \u2264 \u03b4(n) \u2212 K(fP |S) \u2212 O(1).\nExample 6.11 Lossy Compression: The function \u03b2x (R) is relevant to lossy compression (used, for instance, to compress images) \u2013 see also Remark 6.19. Assume we need to compress x to R bits where\nR \u226a K(x). Of course this implies some loss of information present in x. One way to select redundant\ninformation to discard is as follows: Find a set S \u220b x with K(S) \u2264 R and with small \u03b4(x|S), and consider\na compressed version S \u2032 of S. To reconstruct an x\u2032 , a decompresser uncompresses S \u2032 to S and selects at\nrandom an element x\u2032 of S. Since with high probability the randomness deficiency of x\u2032 in S is small, x\u2032\nserves the purpose of the message x as well as does x itself. Let us look at an example. To transmit a picture\nof \"rain\" through a channel with limited capacity R, one can transmit the indication that this is a picture\nof the rain and the particular drops may be chosen by the receiver at random. In this interpretation, \u03b2x (R)\nindicates how \"random\" or \"typical\" x is with respect to the best model at complexity level R-and hence\nhow \"indistinguishable\" from the original x the randomly reconstructed x\u2032 can be expected to be.\n\u2666\nRemark 6.12 This randomness deficiency function quantifies the goodness of fit of the best model at\ncomplexity R for given data x. As far as we know no direct counterpart of this notion exists in Rate\u2013\nDistortion theory, or, indeed, can be expressed in classical theories like Information Theory. But the situation\nis different for the next function we define, which, in almost contradiction to the previous statement, can be\ntied to the minimum randomness deficiency function, yet, as will be seen in Example 6.13 and Section 6.3,\ndoes have a counterpart in Rate\u2013Distortion theory after all.\n\u2666\nMaximum Likelihood estimator: The Kolmogorov structure function hx of given data x is defined by\nhx (R) = min{log |S| : S \u220b x, K(S) \u2264 R},\nS\n\n(6.14)\n\nwhere S \u220b x is a contemplated model for x, and R is a nonnegative integer value bounding the complexity of\nthe contemplated S's. The structure function uses models that are finite sets and the value of the structure\nfunction is the log-cardinality of the smallest such set containing the data. Equivalently, we can use uniform\nprobability mass functions over finite supports (the former finite set models). The smallest set containing\n\n39\n\n\fthe data then becomes the uniform probability mass assigning the highest probability to the data-with\nthe value of the structure function the corresponding negative log-probability. This motivates us to call hx\nthe maximum likelihood estimator. The treatment can be extended from uniform probability mass functions\nwith finite supports, to probability models that are arbitrary computable probability mass functions, keeping\nall relevant notions and results essentially unchanged, Section 6.2.1, justifying the maximum likelihood\nidentification even more.\nClearly, the Kolmogorov structure function is non-increasing and reaches log |{x}| = 0 for the \"rate\"\nR = K(x) + c1 where c1 is the number of bits required to change x into {x}. It is also easy to see that for\nargument K(|x|) + c2 , where c2 is the number of bits required to compute the set of all strings of length |x|\nof x from |x|, the value of the structure function is at most |x|; see Figure 1\nExample 6.13 Clearly the structure function measures for individual outcome x a distortion that is related\nto the one measured by D1\u2217 (R) in Example 6.10 for the uniform average of outcomes x. Note that all strings\nx of length n satisfy hx (K(n) + O(1)) \u2264 n (since x \u2208 Sn = {0, 1}n and K(Sn ) = K(n) + O(1)). For every\nR (0 \u2264 R \u2264 n), we can describe every x = x1 x2 . . . xn as an element of the set AR = {x1 . . . xR yR+1 . . . yn :\nyi \u2208 {0, 1}, R < i \u2264 n}. Then, |AR | = 2n\u2212R and K(AR ) \u2264 R + K(n, R) + O(1) \u2264 R + O(log n). This shows\nthat hx (R) \u2264 n \u2212 R + O(log n) for every x and every R with 0 \u2264 R \u2264 n; see Figure 1.\nFor all x's and R's we can describe x in a two-part code by the set S witnessing hx (R) and x's index\nin that set. The first part describing S in K(S) = R allows us to generate S, and given S we know\nlog |S|. Then, we can parse the second part of log |S| = hx (R) bits that gives x's index in S. We also\nneed a fixed O(1) bit program to produce x from these descriptions. Since K(x) is the lower bound on\nthe length of effective descriptions of x, we have hx (R) + R \u2265 K(x) \u2212 O(1). There are 2n \u2212 2n\u2212K(n)+O(1)\nstrings x of complexity K(x) \u2265 n, [18]. For all these strings hx (R) + R \u2265 n \u2212 O(1). Hence, the expected\nvalue hx (R) equals 2\u2212n {(2n \u2212 2n\u2212K(n)+O(1) )[n \u2212 R + O(log n)] + 2n\u2212K(n)+O(1) O(n \u2212 R + O(log n))} =\nn \u2212 R + O(n \u2212 R/2\u2212K(n)) = n \u2212 R + o(n \u2212 R) (since K(n) \u2192 \u221e for n \u2192 \u221e). That is, the expectation of\nhx (R) equals (1 + o(1))D1\u2217 (R) = (1 + o(1))D\u2217 (R), the Distortion-Rate function, where the o(1) term goes to\n0 with the length n of x. In Section 6.3 we extend this idea to non-uniform distributions on X.\n\u2666\nFor every S \u220b x we have\n\nK(x) \u2264 K(S) + log |S| + O(1).\n\n(6.15)\n\nIndeed, consider the following two-part code for x: the first part is a shortest self-delimiting program p of S\nand the second part is \u2308log |S|\u2309 bit long index of x in the lexicographical ordering of S. Since S determines\nlog |S| this code is self-delimiting and we obtain (6.15) where the constant O(1) is the length of the program\nto reconstruct x from its two-part code. We thus conclude that K(x) \u2264 R + hx (R) + O(1), that is, the\nfunction hx (R) never decreases more than a fixed independent constant below the diagonal sufficiency line\nL defined by L(R) + R = K(x), which is a lower bound on hx (R) and is approached to within a constant\ndistance by the graph of hx for certain R's (for instance, for R = K(x) + c1 ). For these R's we thus have\nR + hx (R) = K(x) + O(1). In the terminology we have introduced in Section 5.2.5 and Definition 5.7, a\nmodel corresponding to such an R (witness for hx (R)) is an optimal set for x and a shortest program to\ncompute this model is a sufficient statistic. It is minimal for the least such R for which the above equality\nholds.\nMDL Estimator: The length of the minimal two-part code for x consisting of the model cost K(S) and\nthe length of the index of x in S, the complexity of S upper bounded by R, is given by the MDL (minimum\ndescription length) function:\n\u03bbx (R) = min{\u039b(S) : S \u220b x, K(S) \u2264 R},\nS\n\n(6.16)\n\nwhere \u039b(S) = log |S| + K(S) \u2265 K(x) \u2212 O(1) is the total length of two-part code of x with help of model S.\nClearly, \u03bbx (R) \u2264 hx (R)+R+O(1), but a priori it is still possible that hx (R\u2032 )+R\u2032 < hx (R)+R for R\u2032 < R. In\nthat case \u03bbx (R) \u2264 hx (R\u2032 ) + R\u2032 < hx (R) + R. However, in [26] it is shown that \u03bbx (R) = hx (R) + R + O(log n)\nfor all x of length n. Even so, this doesn't mean that a set S that witnesses \u03bbx (R) in the sense that x \u2208 S,\nK(S) \u2264 R, and K(S) + log |S| = \u03bbx (R), also witnesses hx (R). It can in fact be the case that K(S) \u2264 R \u2212 r,\nand log |S| = hx (R) + r for arbitrarily large r \u2264 n.\nApart from being convenient for the technical analysis in this work, \u03bbx (R) is the celebrated two-part\nMinimum Description Length code length [20] with the model-code length restricted to at most R. When\n\n40\n\n\fR is large enough so that \u03bbx (R) = K(x), then there is a set S that is a sufficient statistic, and the smallest\nsuch R has an associated witness set S that is a minimal sufficient statistic.\nThe most fundamental result in [26] is the equality\n\u03b2x (R) = hx (R) + R \u2212 K(x) = \u03bbx (R) \u2212 K(x)\n\n(6.17)\n\nwhich holds within logarithmic additive terms in argument and value. Additionally, every set S that witnesses\nthe value hx (R) (or \u03bbx (R)), also witnesses the value \u03b2x (R) (but not vice versa). It is easy to see that hx (R)\nand \u03bbx (R) are upper semi-computable (Definition 1.1); but we have shown [26] that \u03b2x (R) is neither upper\nnor lower semi-computable (not even within a great tolerance). A priori there is no reason to suppose that a\nset that witnesses hx (R) (or \u03bbx (R)) also witnesses \u03b2x (R), for every R. But the fact that they do, vindicates\nKolmogorov's original proposal and establishes hx 's pre-eminence over \u03b2x \u2013 the pre-eminence of hx over \u03bbx\nis discussed below.\nRemark 6.14 What we call 'maximum likelihood' in the form of hx is really 'maximum likelihood' under\na complexity constraint R on the models' as in hx (R). In statistics, it is a well-known fact that maximum\nlikelihood often fails (dramatically overfits) when the models under consideration are of unrestricted complexity (for example, with polynomial regression with Gaussian noise, or with Markov chain model learning,\nmaximum likelihood will always select a model with n parameters, where n is the size of the sample-and\nthus typically, maximum likelihood will dramatically overfit, whereas for example MDL typically performs\nwell). The equivalent, in our setting, is that allowing models of unconstrained complexity for data x, say\ncomplexity K(x), will result in the ML-estimator hx (K(x) + O(1)) = 0-the witness model being the trivial,\nmaximally overfitting, set {x}. In the MDL case, on the other hand, there may be a long constant interval\nwith the MDL estimator \u03bbx (R) = K(x) (R \u2208 [R1 , K(x)]) where the length of the two-part code doesn't\ndecrease anymore. Selecting the least complexity model witnessing this function value we obtain the, very\nsignificant, algorithmic minimal sufficient statistic, Definition 5.7. In this sense, MDL augmented with a bias\nfor the least complex explanation, which we may call the 'Occam's Razor MDL', is superior to maximum\nlikelihood and resilient to overfitting. If we don't apply bias in the direction of simple explanations, then \u2013 at\nleast in our setting \u2013 MDL may be just as prone to overfitting as is ML. For example, if x is a typical random\nelement of {0, 1}n, then \u03bbx (R) = K(x) + O(1) for the entire interval K(n) + O(1) \u2264 R \u2264 K(x) + O(1) \u2248 n.\nChoosing the model on the left side, of simplest complexity, of complexity K(n) gives us the best fit with\nthe correct model {0, 1}n . But choosing a model on the right side, of high complexity, gives us a model {x}\nof complexity K(x) + O(1) that completely overfits the data by modeling all random noise in x (which in\nfact in this example almost completely consists of random noise).\nThus, it should be emphasized that 'ML = MDL' really only holds if complexities are constrained to a\nvalue R (that remains fixed as the sample size grows-note that in the Markov chain example above, the\ncomplexity grows linearly with the sample size); it certainly does not hold in an unrestricted sense (not even\nin the algorithmic setting).\n\u2666\nRemark 6.15 In a sense, hx is more strict than \u03bbx : A set that witnesses hx (R) also witnesses \u03bbx (R) but not\nnecessarily vice versa. However, at those complexities R where \u03bbx (R) drops (a little bit of added complexity\nin the model allows a shorter description), the witness set of \u03bbx is also a witness set of hx . But if \u03bbx stays\nconstant in an interval [R1 , R2 ], then we can trade-off complexity of a witness set versus its cardinality,\nkeeping the description length constant. This is of course not possible with hx where the cardinality of the\nwitness set at complexity R is fixed at hx (R).\n\u2666\nThe main result can be taken as a foundation and justification of common statistical principles in model\nselection such as maximum likelihood or MDL. The structure functions \u03bbx , hx and \u03b2x can assume all possible\nshapes over their full domain of definition (up to additive logarithmic precision in both argument and value),\nsee [26]. (This establishes the significance of (6.17), since it shows that \u03bbx (R) \u226b K(x) is common for (x, R)\npairs-in which case the more or less easy fact that \u03b2x (R) = 0 for \u03bbx (R) = K(x) is not applicable, and it is\na priori unlikely that (6.17) holds: Why should minimizing a set containing x also minimize its randomness\ndeficiency? Surprisingly, it does!) We have exhibited a-to our knowledge first-natural example, \u03b2x , of a\nfunction that is not semi-computable but computable with an oracle for the halting problem.\nExample 6.16 \"Positive\" and \"Negative\" Individual Randomness: In [8] we showed the existence\nof strings for which essentially the singleton set consisting of the string itself is a minimal sufficient statistic.\n\n41\n\n\f|x| =|y|\nK(x)=K(y)\nh (\u03b1)\nx\nlog |S|\nh (\u03b1)\ny\n\nminimal sufficient statistic x\n\n\u03b1\n\nK(x)=K(y)\n\nminimal sufficient statistic y\n\nFigure 2: Data string x is \"positive random\" or \"stochastic\" and data string y is just \"negative random\" or\n\"non-stochastic\".\nWhile a sufficient statistic of an object yields a two-part code that is as short as the shortest one part code,\nrestricting the complexity of the allowed statistic may yield two-part codes that are considerably longer\nthan the best one-part code (so the statistic is insufficient). In fact, for every object there is a complexity\nbound below which this happens-but if that bound is small (logarithmic) we call the object \"stochastic\"\nsince it has a simple satisfactory explanation (sufficient statistic). Thus, Kolmogorov in [11] makes the\nimportant distinction of an object being random in the \"negative\" sense by having this bound high (it has\nhigh complexity and is not a typical element of a low-complexity model), and an object being random in the\n\"positive, probabilistic\" sense by both having this bound small and itself having complexity considerably\nexceeding this bound (like a string x of length n with K(x) \u2265 n, being typical for the set {0, 1}n, or\nthe uniform probability distribution over that set, while this set or probability distribution has complexity\nK(n) + O(1) = O(log n)). We depict the distinction in Figure 2. In simple terms: High Kolmogorov\ncomplexity of a data string just means that it is random in a negative sense; but a data string of high\nKolmogorov complexity is positively random if the simplest satisfactory explanation (sufficient statistic) has\nlow complexity, and it therefore is the typical outcome of a simple random process.\nIn [26] it is shown that for every length n and every complexity k \u2264 n + K(n) + O(1) (the maximal\ncomplexity of x of length n) and every R \u2208 [0, k], there are x's of length n and complexity k such that the\nminimal randomness deficiency \u03b2x (i) \u2265 n \u2212 k \u00b1 O(log n) for every i \u2264 R \u00b1 O(log n) and \u03b2x (i) \u00b1 O(log n)\nfor every i > R \u00b1 O(log n). Therefore, the set of n-length strings of every complexity k can be partitioned\nin subsets of strings that have a Kolmogorov minimal sufficient statistic of complexity \u0398(i log n) for i =\n1, . . .\u221a, k/\u0398(log n). For instance,\nthere are n-length non-stochastic strings of almost maximal complexity\n\u221a\nn \u2212 n having significant n \u00b1 O(log n) randomness deficiency with respect to {0, 1}n or, in fact, every\nother finite set of complexity less than n \u2212 O(log n)!\n\u2666\n6.2.1\n\nProbability Models\n\nThe structure function (and of course the sufficient statistic) use properties of data strings modeled by finite\nsets, which amounts to modeling data by uniform distributions. As already observed by Kolmogorov himself,\nit turns out that this is no real restriction. Everything holds also for computable probability mass functions\n(probability models), up to additive logarithmic precision. Another version of hx uses probability models\nf rather than finite set models. It is defined as h\u2032x (R) = minf {log 1/f (x) : f (x) > 0, K(f ) \u2264 R}. Since\nh\u2032x (R) and hx (R) are close by Proposition 6.17 below, Theorem 6.27 and Corollary 6.29 also apply to h\u2032x and\nthe distortion-rate function D\u2217 (R) based on a variation of the Shannon-Fano distortion measure defined by\nusing encodings Y (x) = f with f a computable probability distribution. In this context, the Shannon-Fano\ndistortion measure is defined by\nd\u2032 (x, f ) = log 1/f (x).\n(6.18)\nIt remains to show that probability models are essentially the same as finite set models. We restrict ourselves\nto the model class of computable probability distributions. Within the present section, we assume these are\n\n42\n\n\f1\nlogarithmic scale\n\n|x|\nK(x)\n\n2\n\n\u2212 log P(x)\n\n\u2212h (\u03b1)\nx\n\nh\n\nx\n\n2\n\n(\u03b1)\n\n\u2212K(x)\n\nP(x)\n\n2\nminimal sufficient statistic\n\n\u2212|x|\n\nK(x)\n\n\u03b1\n\nFigure 3: Structure function hx (i) = minf {log 1/f (x) : f (x) > 0, K(f ) \u2264 i} with f a computable probability\nmass function, with values according to the left vertical coordinate, and the maximum likelihood estimator\n2\u2212hx (i) = max{f (x) : p(x) > 0, K(f ) \u2264 i}, with values according to the right-hand side vertical coordinate.\ndefined on strings of arbitrary length; so they are represented by mass functions f : {0, 1}\u2217 \u2192 [0, 1] with\nP\nf (x) = 1 being computable according to Definition 1.1. A string x is typical for a distribution f if the\nrandomness deficiency \u03b4(x | f ) = log 1/f (x) \u2212 K(x | f ) is small. The conditional complexity K(x | f ) is\ndefined as follows. Say that a function A approximates f if |A(y, \u01eb) \u2212 f (y)| < \u01eb for every y and every positive\nrational \u01eb. Then K(x | f ) is the minimum length of a program that given every function A approximating f\nas an oracle prints x. Similarly, f is c-optimal for x if K(f ) + log 1/f (x) \u2264 K(x) + c. Thus, instead of the\ndata-to-model code length log |S| for finite set models, we consider the data-to-model code length log 1/f (x)\n(the Shannon-Fano code). The value log 1/f (x) measures also how likely x is under the hypothesis f . The\nmapping x 7\u2192 fmin where fmin minimizes log 1/f (x) over f with K(f ) \u2264 R is a maximum likelihood estimator,\nsee figure 3. Our results thus imply that that maximum likelihood estimator always returns a hypothesis\nwith minimum randomness deficiency.\nIt is easy to show that for every data string x and a contemplated finite set model for it, there is an\nalmost equivalent computable probability model. The converse is slightly harder: for every data string x\nand a contemplated computable probability model for it, there is a finite set model for x that has no worse\ncomplexity, randomness deficiency, and worst-case data-to-model code for x, up to additive logarithmic\nprecision:\nProposition 6.17 (a) For every x and every finite set S \u220b x there is a computable probability mass function\nf with log 1/f (x) = log |S|, \u03b4(x | f ) = \u03b4(x | S) + O(1) and K(f ) = K(S) + O(1).\n(b) There are constants c, C, such that for every string x, the following holds: For every computable\nprobability mass function f there is a finite set S \u220b x such that log |S| < log 1/f (x) + 1, \u03b4(x | S) \u2264 \u03b4(x |\nf ) + 2 log K(f ) + K(\u230alog 1/f (x)\u230b) + 2 log K(\u230alog 1/f (x)\u230b) + C and K(S) \u2264 K(f ) + K(\u230alog 1/f (x)\u230b) + C.\nProof. (a) Define f (y) = 1/|S| for y \u2208 S and 0 otherwise.\n(b) Let m = \u230alog 1/f (x)\u230b, that is, 2\u2212m\u22121 < f (x) \u2264 2\u2212m . Define S = {y : f (y) > 2\u2212m\u22121 }. Then,\n|S| < 2m+1 \u2264 2/f (x), which implies the claimed value for log |S|. To list S it suffices to compute all\nconsecutive values of f (y) to sufficient precision until the combined probabilities exceed 1 \u2212 2\u2212m\u22121 . That\nis, K(S) \u2264 K(f ) + K(m) + O(1). Finally, \u03b4(x | S) = log |S| \u2212 K(x|S \u2217 ) < log 1/f (x) \u2212 K(x | S \u2217 ) + 1 =\n\u03b4(x | f ) + K(x | f ) \u2212 K(x | S \u2217 ) + 1 \u2264 \u03b4(x | f ) + K(S \u2217 | f ) + O(1). The term K(S \u2217 | f ) can be upper\nbounded as K(K(S)) + K(m) + O(1) \u2264 2 log K(S) + K(m) + O(1) \u2264 2 log(K(f ) + K(m)) + K(m) + O(1) \u2264\n2 log K(f ) + 2 log K(m) + K(m) + O(1), which implies the claimed bound for \u03b4(x | S).\n\u0003\nHow large are the nonconstant additive complexity terms in Proposition 6.17 for strings x of length n?\nIn item (b), we are commonly only interested in f such that K(f ) \u2264 n + O(log n) and log 1/f (x) \u2264 n + O(1).\nIndeed, for every f there is f \u2032 such that K(f \u2032 ) \u2264 min{K(f ), n} + O(log n), \u03b4(x | f \u2032 ) \u2264 \u03b4(x | f ) + O(log n),\nlog 1/f \u2032 (x) \u2264 min{log 1/f (x), n} + 1. Such f \u2032 is defined as follows: If K(f ) > n then f \u2032 (x) = 1 and f \u2032 (y) = 0\n43\n\n\ffor every y 6= x; otherwise f \u2032 = (f + Un )/2 where Un stands for the uniform distribution on {0, 1}n . Then\nthe additive terms in item (b) are O(log n).\n\n6.3\n\nExpected Structure Function Equals Distortion\u2013Rate Function\n\nIn this section we treat the general relation between the expected value of hx (R), the expectation taken on\na distribution f (x) = P (X = x) of the random variable X having outcome x, and D\u2217 (R). This involves\nthe development of a rate-distortion theory for individual sequences and arbitrary computable distortion\nmeasures. Following [27], we outline such a theory in Sections 6.3.1- 6.3.3. Based on this theory, we present\nin Section 6.3.4 a general theorem (Theorem 6.27) relating Shannon's D\u2217 (R) to the expected value of hx (R),\nfor arbitrary random sources and computable distortion measures. This generalizes Example 6.13 above,\nwhere we analyzed the case of the distortion function\nd(x, Y (x)) = log |Y (x)|,\n\n(6.19)\n\nwhere Y (x) is an x-containing finite set, for the uniform distribution. Below we first extend this example to\narbitrary generating distributions, keeping the distortion function still fixed to (6.19. This will prepare us\nfor the general development in Sections 6.3.1\u20136.3.3\nExample 6.18 In Example 6.13 it transpired that the distortion-rate function is the expected structure\nfunction, the expectation taken over the distribution on the x's. If, instead of using the uniform distribution\non {0, 1}n we use an arbitrary distribution f (x), it is not difficult to compute the rate-distortion function\nR\u2217 (D) = H(X) \u2212 supY :d(X,Y )\u2264D H(X|Y ) where Y is a random vaiable with outcomes that are finite sets.\nSince d is a special type of Shannon-Fano distortion, with d(x, y) = P (X = x|Y = y) = log |y| if x \u2208 y,\nand 0 otherwise, we have already met D\u2217 (R) for the distortion measure (6.19) in another guise. By the\nconclusion of Example 6.9, generalized to the random variable X having outcomes in {0, 1}n, and R being a\nrate in between 0 and n, we know that\nD\u2217 (R) = H(X) \u2212 R.\n\n(6.20)\n\u2666\n\nIn the particular case analyzed above, the code word for a source word is a finite set containing the source\nword, and the distortion is the log-cardinality of the finite set. Considering the set of source words of length\nn, the distortion-rate function is the diagonal line from n to n. The structure functions of the individual\ndata x of length n, on the other hand, always start at n, decrease at a slope of at least -1 until they hit\nthe diagonal from K(x) to K(x), which they must do, and follow the diagonal henceforth. Above we proved\nthat the average of the structure function is simply the straight line, the diagonal, between n and n. This\nis the case, since the strings x with K(x) \u2265 n are the overwhelming majority. All of them have a minimal\nsufficient statistic (the point where the structure function hits the diagonal from K(x) to K(x). This point\nhas complexity at most K(n). The structure function for all these x's follows the diagonal from about n\nto n, giving overall an expectation of the structure function close to this diagonal, that is, the probabilistic\ndistortion-rate function for this code and distortion measure.\n6.3.1\n\nDistortion Spheres\n\nModeling the data can be viewed as encoding the data by a model: the data are\nand models are code words for the data. As before, the set of possible data is X\nthe set of non-negative real numbers. For every model class Y (particular set of\nappropriate recursive function d : X \u00d7 Y \u2192 R+ defining the distortion d(x, y)\nmodel y \u2208 Y.\n\nsource words to be coded,\n= {0, 1}n. Let R+ denote\ncode words) we choose an\nbetween data x \u2208 X and\n\nRemark 6.19 (Lossy Compression) The choice of distortion function is a selection of which aspects of\nthe data are relevant, or meaningful, and which aspects are irrelevant (noise). We can think of the distortionrate function as measuring how far the model at each bit-rate falls short in representing the data. Distortionrate theory underpins the practice of lossy compression. For example, lossy compression of a sound file gives\nas \"model\" the compressed file where, among others, the very high and very low inaudible frequencies have\nbeen suppressed. Thus, the rate-distortion function will penalize the deletion of the inaudible frequencies\nbut lightly because they are not relevant for the auditory experience.\n\n44\n\n\fBut in the traditional distortion-rate approach, we average twice: once because we consider a sequence\nof outcomes of m instantiations of the same random variable, and once because we take the expectation over\nthe sequences. Essentially, the results deal with typical \"random\" data of certain simple distributions. This\nassumes that the data to a certain extent satisfy the behavior of repeated outcomes of a random source.\nKolmogorov [10]:\nThe probabilistic approach is natural in the theory of information transmission over communication channels carrying \"bulk\" information consisting of a large number of unrelated or weakly\nrelated messages obeying definite probabilistic laws. In this type of problem there is a harmless and (in applied work) deep-rooted tendency to mix up probabilities and frequencies within\nsufficiently long time sequence (which is rigorously satisfied if it is assumed that \"mixing\" is\nsufficiently rapid). In practice, for example, it can be assumed that finding the \"entropy\" of a\nflow of congratulatory telegrams and the channel \"capacity\" required for timely and undistorted\ntransmission is validly represented by a probabilistic treatment even with the usual substitution\nof empirical frequencies for probabilities. If something goes wrong here, the problem lies with the\nvagueness of our ideas of the relationship between mathematical probabilities and real random\nevents in general.\nBut what real meaning is there, for example, in asking how much information is contained in\n\"War and Peace\"? Is it reasonable to include the novel in the set of \"possible novels\", or even\nto postulate some probability distribution for this set? Or, on the other hand, must we assume\nthat the individual scenes in this book form a random sequence with \"stocahstic relations\" that\ndamp out quite rapidly over a distance of several pages?\nCurrently, individual data arising in practice are submitted to analysis, for example sound or video files,\nwhere the assumption that they either consist of a large number of weakly related messages, or being an\nelement of a set of possible messages that is susceptible to analysis, is clearly wrong. It is precisely the\nglobal related aspects of the data which we want to preserve under lossy compression. The rich versatility\nof the structure functions, that is, many different distortion-rate functions for different individual data, is\nall but obliterated in the averaging that goes on in the traditional distortion-rate function. In the structure\nfunction approach one focuses entirely on the stochastic properties of one data item.\n\u2666\nBelow we follow [27], where we developed a rate-distortion theory for individual data for general computable\ndistortion measures, with as specific examples the 'Kolmogorov' distortion below, but also Hamming distortion and Euclidean distortion. This individual rate-distortion theory is summarized in Sections 6.3.2\nand 6.3.3. In Section 6.3.4, Theorem 6.27. we connect this indivual rate-distortion theory to Shannon's. We\nemphasize that the typical data items of i.i.d. distributed simple random variables, or simple ergodic stationary sources, which are the subject of Theorem 6.27, are generally unrelated to the higly globally structured\ndata we want to analyze using our new rate-distortion theory for individual data. From the prespective\nof lossy compression, the typical data have the characteristics of random noise, and there is no significant\n\"meaning\" to be preserved under the lossy compression. Rather, Theorem 6.27 serves as a 'sanity check'\nshowing that in the special, simple case of repetitive probabilistic data, the new theory behaves essentially\nlike Shannon's probabilistic rate-distortion theory.\nExample 6.20 Let us look at various model classes and distortion measures:\n(i) The set of models are the finite sets of finite binary strings. Let S \u2286 {0, 1}\u2217 and |S| < \u221e. We define\nd(x, S) = log |S| if x \u2208 S, and \u221e otherwise.\n(ii) The set of models are the computable probability density functions f mapping {0, 1}\u2217 to [0, 1]. We\ndefine d(x, S) = log 1/f (x) if f (x) > 0, and \u221e otherwise.\n(iii) The set of models are the total recursive functions f mapping {0, 1}\u2217 to N . We define d(x, f ) =\nmin{l(d) : f (d) = x}, and \u221e if no such d exists.\nAll of these model classes and accompanying distortions [26], together with the \"communication exchange\" models in [1], are loosely called Kolmogorov models and distortion, since the graphs of their structure functions (individual distortion-rate functions) are all within a strip-of width logarithmic in the binary\nlength of the data-of one another.\n\u2666\nIf Y is a model class, then we consider distortion spheres of given radius r centered on y \u2208 Y:\nBy (r) = {x : d(x, y) = r}.\n45\n\n\fThis way, every model class and distortion measure can be treated similarly to the canonical finite set case,\nwhich, however, is especially simple in that the radius not variable. That is, there is only one distortion\nsphere centered on a given finite set, namely the one with radius equal to the log-cardinality of that finite\nset. In fact, that distortion sphere equals the finite set on which it is centered.\n6.3.2\n\nRandomness Deficiency-Revisited\n\nLet Y be a model class and d a distortion measure. Since in our definition the distortion is recursive,\ngiven a model y \u2208 Y and diameter r, the elements in the distortion sphere of diameter r can be recursively\nenumerated from the distortion function. Giving the index of any element x in that enumeration we can\n+\nfind the element. Hence, K(x|y, r) < log |By (r)|. On the other hand, the vast majority of elements x in\n+\n\nthe distortion sphere have complexity K(x|y, r) > log |By (r)| since, for every constant c, there are only\n2log |By (r)|\u2212c \u2212 1 binary programs of length < log |By (r)| \u2212 c available, and there are |By (r)| elements to be\ndescribed. We can now reason as in the similar case of finite set models. With data x and r = d(x, y),\n+\n\nif K(x|y, d(x, y)) > log |By (d(x, y))|, then x belongs to every large majority of elements (has the property\nrepresented by that majority) of the distortion sphere By (d(x, y)), provided that property is simple in the\nsense of having a description of low Kolmogorov complexity.\nDefinition 6.21 The randomness deficiency of x with respect to model y under distortion d is defined as\n\u03b4(x | y) = log |By (d(x, y))| \u2212 K(x|y, d(x, y)).\nData x is typical for model y \u2208 Y (and that model \"typical\" or \"best fitting\" for x) if\n+\n\n\u03b4(x | y) = 0.\n\n(6.21)\n\nIf x is typical for a model y, then the shortest way to effectively describe x, given y, takes about as many\nbits as the descriptions of the great majority of elements in a recursive enumeration of the distortion sphere.\nSo there are no special simple properties that distinguish x from the great majority of elements in the\ndistortion sphere: they are all typical or random elements in the distortion sphere (that is, with respect to\nthe contemplated model).\nExample 6.22 Continuing Example 6.20 by applying (6.21) to different model classes:\n+\n\n(i) Finite sets: For finite set models S, clearly K(x|S) < log |S|. Together with (6.21) we have that x is\n+\ntypical for S, and S best fits x, if the randomness deficiency according to (6.12) satisfies \u03b4(x|S) = 0.\n(ii) Computable probability density functions: Instead of the data-to-model code length log |S| for finite\nset models, we consider the data-to-model code length log 1/f (x) (the Shannon-Fano code). The value\nlog 1/f (x) measures how likely x is under the hypothesis f . For probability models f , define the conditional\ncomplexity K(x | f, \u2308log 1/f (x)\u2309) as follows. Say that a function A approximates f if |A(x, \u01eb) \u2212 f (x)| < \u01eb\nfor every x and every positive rational \u01eb. Then K(x | f, \u2308log 1/f (x)\u2309) is defined as the minimum length of a\nprogram that, given \u2308log 1/f (x)\u2309 and any function A approximating f as an oracle, prints x.\n+\n\nClearly K(x|f, \u2308log 1/f (x)\u2309) < log 1/f (x). Together with (6.21), we have that x is typical for f , and f\n+\n\nbest fits x, if K(x|f, \u2308log 1/f (x)\u2309) > log |{z : log 1/f (z) \u2264 log 1/f (x)}|. The right-hand side set condition\nis the same as f (z) \u2265 f (x), and there can be only \u2264 1/f (x) such z, since otherwise the total probability\n+\n\nexceeds 1. Therefore, the requirement, and hence typicality, is implied by K(x|f, \u2308log 1/f (x)\u2309) > log 1/f (x).\nDefine the randomness deficiency by \u03b4(x | f ) = log 1/f (x) \u2212 K(x | f, \u2308log 1/f (x)\u2309). Altogether, a string x is\n+\n+\ntypical for a distribution f , or f is the best fitting model for x, if \u03b4(x | f ) = 0. if \u03b4(x | f ) = 0.\n(iii) Total Recursive Functions: In place of log |S| for finite set models we consider the data-to-model\ncode length (actually, the distortion d(x, f ) above)\nlx (f ) = min{l(d) : f (d) = x}.\n\nDefine the conditional complexity K(x | f, lx (f )) as the minimum length of a program that, given lx (f ) and\nan oracle for f , prints x.\n+\nClearly, K(x|f, lx (f )) < lx (f ). Together with (6.21), we have that x is typical for f , and f best fits x, if\n+\n\nK(x|f, lx (f )) > log{z : lz (f ) \u2264 lx (f )}. There are at most (2lx (f )+1 \u2212 1)- many z satisfying the set condition\n46\n\n\f+\n\nsince lz (f ) \u2208 {0, 1}\u2217 . Therefore, the requirement, and hence typicality, is implied by K(x|f, lx (f )) > lx (f ).\nDefine the randomness deficiency by \u03b4(x | f ) = lx (f ) \u2212 K(x | f, lx (f )). Altogether, a string x is typical for a\n+\ntotal recursive function f , and f is the best fitting recursive function model for x if \u03b4(x | f ) = 0, or written\ndifferently,\n+\nK(x|f, lx (f )) = lx (f ).\n(6.22)\nNote that since lx (f ) is given as conditional information, with lx (f ) = l(d) and f (d) = x, the quantity\nK(x|f, lx (f )) represents the number of bits in a shortest self-delimiting description of d.\n\u2666\nRemark 6.23 We required lx (f ) in the conditional in (6.22). This is the information about the radius of\nthe distortion sphere centered on the model concerned. Note that in the canonical finite set model case, as\ntreated in [11, 8, 26], every model has a fixed radius which is explicitly provided by the model itself. But\nin the more general model classes of computable probability density functions, or total recursive functions,\nmodels can have a variable radius. There are subclasses of the more general models that have fixed radiuses\n(like the finite set models).\n(i) In the computable probability density functions one can think of the probabilities with a finite support,\nfor example fn (x) = 1/2n for l(x) = n, and f (x) = 0 otherwise.\n(ii) InP\nthe total recursive function case one can similarly think of functions with finite support, for example\nn\nfn (x) = i=1 xi for x = x1 . . . xn , and fn (x) = 0 for l(x) 6= n.\nThe incorporation of the radius in the model will increase the complexity of the model, and hence of the\nminimal sufficient statistic below.\n\u2666\n6.3.3\n\nSufficient Statistic-Revisited\n\nAs with the probabilistic sufficient statistic (Section 5.1), a statistic is a function mapping the data to an\nelement (model) in the contemplated model class. With some sloppiness of terminology we often call the\nfunction value (the model) also a statistic of the data. A statistic is called sufficient if the two-part description\nof the data by way of the model and the data-to-model code is as concise as the shortest one-part description\nof x. Consider a model class Y.\nDefinition 6.24 A model y \u2208 Y is a sufficient statistic for x if\n+\n\nK(y, d(x, y)) + log |By (d(x, y))| = K(x).\n\n(6.23)\n\n+\n\nLemma 6.25 If y is a sufficient statistic for x, then K(x | y, d(x, y) = log |By (d(x, y))|, that is, x is typical\nfor y.\n+\n\n+\n\n+\n\nProof. We can rewrite K(x) < K(x, y, d(x, y)) < K(y, d(x, y)) + K(x|y, d(x, y)) < K(y, d(x, y)) +\n+\nlog |By (d(x, y))| = K(x). The first three inequalities are straightforward and the last equality is by the\nassumption of sufficiency. Altogether, the first sum equals the second sum, which implies the lemma.\n\u0003\nThus, if y is a sufficient statistic for x, then x is a typical element for y, and y is the best fitting model for\nx. Note that the converse implication, \"typicality\" implies \"sufficiency,\" is not valid. Sufficiency is a special\ntype of typicality, where the model does not add significant information to the data, since the preceding\n+\nproof shows K(x) = K(x, y, d(x, y)). Using the symmetry of information (3.9) this shows that\n+\n\n+\n\nK(y, d(x, y) | x) = K(y | x) = 0.\n\n(6.24)\n\nThis means that:\n(i) A sufficient statistic y is determined by the data in the sense that we need only an O(1)-bit program,\npossibly depending on the data itself, to compute the model from the data.\n(ii) For each model class and distortion there is a universal constant c such that for every data item x\nthere are at most c sufficient statistics.\nExample 6.26 Finite sets: For the model class of finite sets, a set S is a sufficient statistic for data x if\n+\n\nK(S) + log |S| = K(x).\n\n47\n\n\fComputable probability density functions: For the model class of computable probability density functions,\na function f is a sufficient statistic for data x if\n+\n\nK(f ) + log 1/f (x) = K(x).\nFor the model class of total recursive functions, a function f is a sufficient statistic for data x if\n+\n\nK(x) = K(f ) + lx (f ).\n\n(6.25)\n\nFollowing the above discussion, the meaningful information in x is represented by f (the model) in K(f ) bits,\nand the meaningless information in x is represented by d (the noise in the data) with f (d) = x in l(d) = lx (f )\n+\n+\nbits. Note that l(d) = K(d) = K(d|f \u2217 ), since the two-part code (f \u2217 , d) for x cannot be shorter than the\nshortest one-part code of K(x) bits, and therefore the d-part must already be maximally compressed. By\n+\n+\nLemma 6.25, lx (f ) = K(x | f \u2217 , lx (f )), x is typical for f , and hence K(x) = K(f ) + K(x | f \u2217 , lx (f )).\n\u2666\n6.3.4\n\nExpected Structure Function\n\nWe treat the relation between the expected value of hx (R), the expectation taken on a distribution f (x) =\nP (X = x) of the random variable X having outcome x, and D\u2217 (R), for arbitrary random sources provided\nthe probability mass function f (x) is recursive.\nTheorem 6.27 Let d be a recursive distortion measure. Given m repetitions of a random variable X with\noutcomes x \u2208 X (typically, X = {0, 1}n) with probability f (x), where f is a total recursive function, we have\nE\n\n1\n1\n\u2217\n(R) \u2264 E hx (mR),\nhx (mR + K(f, d, m, R) + O(log n)) \u2264 Dm\nm\nm\n\nthe expectations are taken over x = x1 . . . xm where xi is the outcome of the ith repetition of X.\nProof. As before, let X1 , . . . , Xm be m independent identically distributed random variables on outcome\nspace X . Let Y be a set of code words. We want to find a sequence of functions Y1 , . . . , Ym : X \u2192 Y so that\nthe message (Y1 (x1 ), . . . , Ym (xm )) \u2208 Y m gives as much expected information about the sequence of outcomes\n(X1 = x1 , . . . , Xm = xm ) as is possible, under the constraint that the message takes at most R * m bits (so\nthat R bits are allowed on average per outcome of Xi ). Instead of Y1 , . . . , Ym above write Y : X m \u2192 Y m .\nDenote the cardinality of the range of Y by \u03c1(Y ) = |{Y (x) : x \u2208 X m }|. Consider distortion spheres\nBy (d) = {x : d(x, y) = d},\n\n(6.26)\n\nwith x = x1 . . . xm \u2208 X m and y \u2208 Y m .\nLeft Inequality: Keeping the earlier notation, for m i.i.d. random variables X1 , .P\n. . , Xm , and extending\n1\n\u2217\nf to the m-fold Cartesian product of {0, 1}n, we obtain Dm\n(R) = m\nminY :\u03c1(Y )\u22642mR x f (x)d(x, Y (x)). By\n\u2217\ndefinition of Dm\n(R) it equals the following expression in terms of a minimal canonical covering of {0, 1}nm\nby disjoint nonempty spheres By\u2032 i (di ) (1 \u2264 i \u2264 k) obtained from the possibly overlapping distortion spheres\nByi (di ) as follows. Every element x in the overlap between two or more spheres is assigned to the sphere\nwith the smallest radius and removed from the other spheres. If there is more than one sphere of smallest\nradius, then we take the sphere of least index in the canonical\ncovering. Empty B \u2032 -spheres are removed from\nP\n\u2032\nnm\nthe B -covering. If S \u2286 {0, 1} , then f (S) denotes x\u2208S f (x). Now, we can rewrite\nk\n\n\u2217\nDm\n(R) =\n\nmin\n\ny1 ,...,yk ;d1 ,...,dk ;k\u22642mR\n\n1 X\nf (By\u2032 i (di ))di .\nm\n\n(6.27)\n\ni=1\n\nIn the structure function setting we consider some individual data x residing in one of the covering spheres.\nGiven m, n, R and a program to compute f and d, we can compute the covering spheres centers y 1 , . . . , yk ,\nand radiuses d1 , . . . , dk , and hence the B \u2032 -sphere canonical covering. In this covering we can identify every\npair (y i , di ) by its index i \u2264 2mR . Therefore, K(yi , di ) \u2264 mR + K(f, d, m, R) + O(log n) (1 \u2264 i \u2264 k). For\n1\nhx (mR + K(f, d, m, R) +\nx \u2208 By\u2032 i (di ) we have hx (mR + K(f, d, m, R) + O(log n)) \u2264 di . Therefore, E m\nmn\n\u2217\nO(log n)) \u2264 Dm (R), the expectation taken over f (x) for x \u2208 {0, 1} .\n\n48\n\n\fRight Inequality: Consider a covering of {0, 1}nm by the (possibly overlapping) distortion spheres Byi (di )\nsatisfying K(Byi (di )|mR) < mR \u2212 c, with c an appropriate constant choosen so that the remainder of the\nargument goes through. If there are more than one spheres with different (center, radius)-pairs representing\nthe same subset of {0, 1}nm, then we eliminate all of them except the one with the smallest radius. If there\nare more than one such spheres, then we only keep the one with the lexicographically least center. From this\ncovering we obtain a canonical covering by nonempty disjoint spheres By\u2032 i (di ) similar to that in the previous\nparagraph, (1 \u2264 i \u2264 k).\nFor every x \u2208 {0, 1}nm there is a unique sphere By\u2032 i (di ) \u220b x (1 \u2264 i \u2264 k). Choose the constant c above so\nthat K(By\u2032 i (di )|mR) < mR. Then, k \u2264 2mR . Moreover, by construction, if By\u2032 i (di ) is the sphere containing\nx, then hx (mR) = di . Define functions \u03b3 : {0, 1}nm \u2192 Y m , \u03b4 : {0, 1}nm \u2192 R+ defined by \u03b3(x) = y i and\n\u03b4(x) = di for x in the sphere By\u2032 i (di ). Then,\nE\n\n1\n1\nhx (mR) =\nm\nm\n\nX\n\nf (x)d(x, \u03b3(x)) =\n\nx\u2208{0,1}mn\n\n1\nm\n\nX\n\nf (By\u2032 i (di ))di .\n\n(6.28)\n\ny 1 ,...,yk ;d1 ,...,dk\n\n\u2217\nThe distortion Dm\n(R) achieves the minimum of the expression in right-hand side of (6.27). Since\n\u2032\nK(B\u03b3(x) (\u03b4(x))|mR) < mR, the cover in the right-hand side of (6.28) is a possible partition satisfying\n\u2217\nthe expression being minimized in the right-hand side of (6.27), and hence majorizes the minumum Dm\n(R).\n1\n\u2217\n\u0003\nTherefore, E m hx (mR) \u2265 Dm (R).\n\nRemark 6.28 A sphere is a subset of {0, 1}nm. The same subset may correspond to more than one\nspheres with different centers and radiuses: By0 (d0 ) = By1 (d1 ) with (y0 , d0 ) 6= (y1 , d1 ). Hence, K(By (d)) \u2264\nK(y, d)) + O(1), but possibly K(y, d)) > K(By (d)) + O(1). However, in the proof we constructed the ordered\nsequence of B \u2032 spheres such that every sphere uniquely corresponds to a (center, radius)-pair. Therefore,\n+\n\u2666\nK(By\u2032 i (di )|mR) = K(yi , di |mR).\nCorollary 6.29 It follows from the above theorem that, for a recursive distortion function d: (i) Ehx (R +\nK(f, d, R) + O(log n)) \u2264 D1\u2217 (R) \u2264 Ehx (R), for outcomes of a single repetition of random variable X = x\nwith x \u2208 {0, 1}n, the expectation taken over f (x) = P (X = x); and\n1\nhx (mR) = D\u2217 (R) for outcomes x = x1 . . . xm of i.i.d. random variables Xi = xi with\n(ii) limm\u2192\u221e E m\nn\nxi \u2208 {0, 1} for 1 \u2264 i \u2264 m, the expectation taken over f (x) = P (Xi = xi , i = 1, . . . , m) (the extension of f\nto m repetitions of X).\nThis is the sense in which the expected value of the structure function is asymptotically equal to the\nvalue of the distortion-rate function, for arbitrary computable distortion measures. In the structure function\napproach we dealt with only two model classes, finite sets and computable probability density functions, and\nthe associated quantities to be minimized, the log-cardinality and the negative log-probability, respectively.\nTranslated into the distortion-rate setting, the models are code words and the minimalizable quantities\nare distortion measures. In [26] we also investigate the model class of total recursive functions, and in [1]\nthe model class of communication protocols. The associated quantities to be minimized are then function\narguments and communicated bits, respectively. All these models are equivalent up to logarithmic precision\nin argument and value of the corresponding structure functions, and hence their expectations are asymptotic\nto the distortion-rate functions of the related code-word set and distortion measure.\n\n7\n\nConclusion\n\nWe have compared Shannon's and Kolmogorov's theories of information, highlighting the various similarities\nand differences. Some of this material can also be found in [4], the standard reference for Shannon information\ntheory, as well as [18], the standard reference for Kolmogorov complexity theory. These books predate much\nof the recent material on the Kolmogorov theory discussed in the present paper, such as [9] (Section 3.2),\n[17] (Section 4.2), [8] (Section 5.2), [26, 27] (Section 6.2). The material in Sections 5.3 and 6.3 has not been\npublished before. The present paper summarizes these recent contributions and systematically compares\nthem to the corresponding notions in Shannon's theory.\n\n49\n\n\fRelated Developments: There are two major practical theories which have their roots in both Shannon's\nand Kolmogorov's notions of information: first, universal coding, briefly introduced in Appendix A below,\nis a remarkably successful theory for practical lossless data compression. Second, Rissanen's Minimum\nDescription Length (MDL) Principle [20, 7] is a theory of inductive inference that is both practical and\nsuccessful. Note that direct practical application of Shannon's theory is hampered by the typically untenable\nassumption of a true and known distribution generating the data. Direct application of Kolmogorov's\ntheory is hampered by the noncomputability of Kolmogorov complexity and the strictly asymptotic nature\nof the results. Both universal coding (of the individual sequence type, Appendix A) and MDL seek to\novercome both problems by restricting the description methods used to those corresponding to a set of\nprobabilistic predictors (thus making encodings and their lengths computable and nonasymptotic); yet when\napplying these predictors, the assumption that any one of them generates the data is never actually made.\nInterestingly, while in its current form MDL bases inference on universal codes, in recent work Rissanen and\nco-workers have sought to found the principle on a restricted form of the algorithmic sufficient statistic and\nKolmogorov's structure function as discussed in Section 6.2 [21].\nBy looking at general types of prediction errors, of which codelengths are merely a special case, one\nachieves a generalization of the Kolmogorov theory that goes by the name of predictive complexity, pioneered\nby Vovk, Vyugin, Kalnishkan and others5 [25]. Finally, the notions of 'randomness deficiency' and 'typical\nset' that are central to the algorithmic sufficient statistic (Section 5.2) are intimately related to the celebrated\nMartin-L\u00f6f-Kolmogorov theory of randomness in individual sequences, an overview of which is given in [18].\n\nA\n\nAppendix: Universal Codes\n\nShannon's and Kolmogorov's idea are not directly applicable to most actual data compression problems.\nShannon's theory is hampered by the typically untenable assumption of a true and known distribution\ngenerating the data. Kolmogorov's theory is hampered by the noncomputability of Kolmogorov complexity\nand the strictly asymptotic nature of the results. Yet there is a middle ground that is feasible: universal\ncodes that may be viewed as both an generalized version of Shannon's, and a feasible approximation to\nKolmogorov's theory. In introducing the notion of universal coding Kolmogorov says [10]:\n\"A universal coding method that permits the transmission of any sufficiently long message [of\nlength n] in an alphabet of s letters with no more nh [h is the empirical entropy] binary digits is\nnot necessarily excessively complex; in particular, it is not essential to begin by determining the\nfrequencies pr for the entire message.\"\nBelow we repeatedly use the coding concepts introduced in Section 1.3. Suppose we are given a recursive\nenumeration of prefix codes D1 , D2 , . . .. Let L1 , L2 , . . . be the length functions associated with these codes.\nThat is, Li (x) = miny {l(y) : Di (y) = x}; if there exists no y with Di (y) = x, then Li (y) = \u221e. We may\nencode x by first encoding a natural number k using the standard prefix code for the natural numbers.\nWe then encode x itself using the code Dk . This leads to a so-called two-part code D\u0303 with lengths L\u0303. By\nconstruction, this code is prefix and its lengths satisfy\nL\u0303(x) := min LN (k) + Lk (x),\nk\u2208N\n\n(A.1)\n\nLet x be an infinite binary sequence and let x[1:n] \u2208 {0, 1}n be the initial n-bit segment of this sequence.\nSince LN (k) = O(log k), we have for all k, all n:\nL\u0303(x[1:n] ) \u2264 Lk (x[1:n] ) + O(log k).\nRecall that for each fixed Lk , the fraction of sequences of length n that can be compressed by more than m\nbits is less than 2\u2212m . Thus, typically, the codes Lk and the strings x[1:n] will be such that Lk (x[1:n] ) grows\nlinearly with n. This implies that for every x, the newly constructed L\u0303 is 'almost as good' as whatever code\nDk in the list is best for that particular x: the difference in code lengths is bounded by a constant depending\non k but not on n. In particular, for each infinite sequence x, for each fixed k,\nlim\n\nn\u2192\u221e\n5 See\n\nL\u0303(x[1:n] )\n\u2264 1.\nLk (x[1:n] )\n\nwww.vovk.net for an overview.\n\n50\n\n(A.2)\n\n\fA code satisfying (A.2) is called a universal code relative to the comparison class of codes {D1 , D2 , . . .}. It\nis 'universal' in the sense that it compresses every sequence essentially as well as the Dk that compresses\nthat particular sequence the most. In general, there exist many types of codes that are universal: the 2-part\nuniversal code defined above is just one means of achieving (A.2).\nUniversal codes and Kolmogorov: In most practically interesting cases we may assume that for all k,\nthe decoding function Dk is computable, i.e. there exists a prefix Turing machine which for all y \u2208 {0, 1}\u2217,\nwhen input y \u2032 (the prefix-free version of y), outputs Dk (y) and then halts. Since such a program has finite\nlength, we must have for all k,\nl(E \u2217 (x[1:n] )) = K(x[1:n] ) \u2264+ Lk (x[1:n] )\nwhere E \u2217 is the encoding function defined in Section 2.2, with l(E \u2217 (x)) = K(x). Comparing with (A.2)\nshows that the code D\u2217 with encoding function E \u2217 is a universal code relative to D1 , D2 , . . .. Thus, we see\nthat the Kolmogorov complexity K is just the length function of the universal code D\u2217 . Note that D\u2217 is an\nexample of a universal code that is not (explicitly) two-part.\nExample A.1 Let us create a universal two-part code that allows us to significantly compress all binary\nstrings with frequency of 0's deviating significantly from 21 . For n0 < n1 , let Dhn,n0 i be the code that assigns\ncode words of equal (minimum) length to all strings of length n with n\u00010 zeroes, and no code words to any\nother strings. Then Dhn,n0 i is a prefix-code and Lhn,n0 i (x) = \u2308log nn0 \u2309. The universal two part code D\u0303\nrelative to the set of codes {Dhi,ji : i, j \u2208 N } then achieves the following lengths (to within 1 bit): for all\nn, all n0 \u2208 {0, . . . , n}, all x[1:n] with n0 zeroes,\n\u0012 \u0013\n\u0012 \u0013\nn\nn\nL\u0303(x[1:n] ) = log n + log n0 + 2 log log n + 2 log log n0 + log\n= log\n+ O(log n)\nn0\nn0\n\u221a\nUsing Stirling's approximation of the factorial, n! \u223c nn e\u2212n 2\u03c0n, we find that\n\u0012\n\nn\nlog\nn0\n\n\u0013\n\n= log n! \u2212 log n0 ! + log(n \u2212 n0 )! =\nn log n \u2212 n0 log n0 \u2212 (n \u2212 n0 ) log(n \u2212 n0 ) + O(log n) = nH(n0 /n) + O(log n)\n\n(A.3)\n\nNote that H(n0 /n) \u2264 1, with equality iff n0 = n. Therefore, if the frequency deviates significantly from\n1\n\u2217\n2 , D\u0303 compresses x[1:n] by a factor linear in n. In all such cases, D compresses the data by at least the\nsame linear factor. Note that (a) each individual code Dhn,n0 i is capable of exploiting a particular type of\nregularity in a sequence to compress that sequence, (b) the universal code D\u0303 may exploit many different\ntypes of regularities to compress a sequence, and (c) the code D\u2217 with lengths given by the Kolmogorov\ncomplexity asymptotically exploits all computable regularities so as to maximally compress a sequence. \u2666\nUniversal codes and Shannon: If a random variable X is distributed according to some known probability mass function f (x) = P (X = x), then the optimal (in the average sense) code to use is the Shannon-Fano\ncode. But now suppose it is only known that f \u2208 {f }, where {f } is some given (possibly very large, or\neven uncountable) set of candidate distributions. Now it is not clear what code is optimal. We may try the\nShannon-Fano code for a particular f \u2208 {f }, but such a code will typically lead to very large expected code\nlengths if X turns out to be distributed according to some g \u2208 {f }, g 6= f . We may ask whether there exists\nanother code that is 'almost' as good as the Shannon-Fano code for f , no matter what f \u2208 {f } actually\ngenerates the sequence? We now show that, provided {f } is finite or countable, then (perhaps surprisingly),\nthe answer is yes. To see this, we need the notion of an sequential information source, Section 1.2.\nSuppose then that {f } represents a finite or countable set of sequential information sources. Thus,\n(1)\n(2)\n{f } = {f1 , f2 , . . .} and fk \u2261 (fk , fk , . . .) represents a sequential information source, abbreviated to fk . To\n(n)\neach marginal distribution fk , there corresponds a unique Shannon-Fano code defined on the set {0, 1}n\n(n)\nwith lengths Lhn,ki (x) := \u2308log 1/fk (x)\u2309 and decoding function Dhn,ki .\nP\nFor given f \u2208 {f }, we define H(f (n) ) := x\u2208{0,1}n f (n) (x)[log 1/f (n)(x)] as the entropy of the distribution\nof the first n outcomes.\n\n51\n\n\fLet E be a prefix-code assigning codeword E(x) to source word x \u2208P\n{0, 1}n. The Noiseless Coding\n(n)\nTheorem 2.5 asserts that the minimal average codeword length L\u0304(f ) = x\u2208{0,1}n f (n) (x)l(E(x)) among\nall such prefix-codes E satisfies\nH(f (n) ) \u2264 L(f (n) ) \u2264 H(f (n) ) + 1.\n\nThe entropy H(f (n) ) can therefore be interpreted as the expected code length of encoding the first n bits\ngenerated by the source f , when the optimal (Shannon-Fano) code is used.\nWe look for a prefix code D\u0303 with length function L\u0303 that satisfies, for all fixed f \u2208 {f }:\nlim\n\nn\u2192\u221e\n\nEf L\u0303(X[1:n] )\n\u2264 1.\nH(f (n) )\n\n(A.4)\n\nP\nwhere Ef L\u0303(X[1:n] ) = x\u2208{0,1}n f (n) (x)L(x). Define D\u0303 as the following two-part code: first, n is encoded\nusing the standard prefix code for natural numbers. Then, among all codes Dhn,ki , the k that minimizes\nLhn,ki (x) is encoded (again using the standard prefix code); finally, x is encoded in Lhn,ki (x) bits. Then for\nall n, for all k, for every sequence x[1:n] ,\nL\u0303(x[1:n] ) \u2264 Lhn,ki (x[1:n] ) + LN (k) + LN (n)\n\n(A.5)\n\nSince (A.5) holds for all strings of length n, it must also hold in expectation for all possible distributions on\nstrings of length n. In particular, this gives, for all k \u2208 N ,\n(n)\n\nEfk L\u0303(X[1:n] ) \u2264 Efk Lhn,ki (X[1:n] ) + O(log n) = H(fk ) + O(log n),\nfrom which (A.4) follows.\nHistorically, codes satisfying (A.4) have been called universal codes relative to {f }; codes satisfying (A.2)\nhave been considered in the literature only much more recently and are usually called 'universal codes for\nindividual sequences' [19]. The two-part code D\u0303 that we just defined is universal both in an individual\nsequence and in an average sense: D\u0303 achieves code lengths within a constant of that achieved by Dhn,ki for\nevery individual sequence, for every k \u2208 N ; but D\u0303 also achieves expected code lengths within a constant\nof the Shannon-Fano code for f , for every f \u2208 {f }. Note once again that the D\u2217 based on Kolmogorov\ncomplexity does at least as well as D\u0303.\nExample A.2 Suppose our sequence is generated by independent tosses of a coin with bias p of tossing\n\"head\" where p \u2208 (0, 1). Identifying 'heads' with 1, the probability of n \u2212 n0 outcomes \"1\" in an initial\nsegment x[1:n] is then (1\u2212p)n0 pn\u2212n0 . Let {f } be the set of corresponding information sources, containing one\nelement for each p \u2208 (0, 1). {f } is an uncountable set; nevertheless, a universal code for {f } exists. In fact,\nit can be shown that the code D\u0303 with lengths (A.3) in Example A.1 is universal for {f }, i.e. it satisfies (A.4).\nThe reason for this is (roughly) as follows: if data are generated by a coin with bias p, then with probability\n1, the frequency n0 /n converges to p, so that, by (A.3), n\u22121 L\u0303(x[1:n] ) tends to n\u22121 H(f (n) ) = H(p, 1 \u2212 p).\nIf we are interested in practical data-compression, then the assumption that the data are generated by a\nbiased-coin source is very restricted. But there are much richer classes of distributions {f } for which we can\nformulate universal codes. For example, we can take {f } to be the class of all Markov sources of each order;\nhere the probability that Xi = 1 may depend on arbitrarily many earlier outcomes. Such ideas form the basis\nof most data compression schemes used in practice. Codes which are universal for the class of all Markov\nsources of each order and which encode and decode in real-time can easily be implemented. Thus, while we\ncannot find the shortest program that generates a particular sequence, it is often possible to effectively find\nthe shortest encoding within a quite sophisticated class of codes.\n\u2666\n\nReferences\n[1] H. Buhrman, H. Klauck, N.K. Vereshchagin, and P.M.B. Vit\u00e1nyi. Individual communication complexity.\nIn Proc. STACS, LNCS, pages 19\u201330, Springer-Verlag, 2004.\n[2] R.T. Cox and D. Hinkley. Theoretical Statistics. Chapman and Hall, 1974.\n[3] G.J. Chaitin. On the length of programs for computing finite binary sequences: statistical considerations.\nJ. Assoc. Comput. Mach., 16:145\u2013159, 1969.\n\n52\n\n\f[4] T.M. Cover and J.A. Thomas. Elements of Information Theory. Wiley & Sons, 1991.\n[5] R.A. Fisher. On the mathematical foundations of theoretical statistics. Philos. Trans. Royal Soc.\nLondon, Ser. A, 222:309\u2013368, 1922.\n[6] P. G\u00e1cs. On the symmetry of algorithmic information. Soviet Math. Dokl., 15:1477\u20131480, 1974. Correction, Ibid., 15:1480, 1974.\n[7] P. D. Gr\u00fcnwald. MDL Tutorial. In P. D. Gr\u00fcnwald, I. J. Myung, and M. A. Pitt (Eds.), Advances in\nMinimum Description Length: Theory and Applications. MIT Press, 2004.\n[8] P. G\u00e1cs, J. Tromp, and P.M.B. Vit\u00e1nyi. Algorithmic statistics. IEEE Trans. Inform. Theory, 47(6):2443\u2013\n2463, 2001.\n[9] D. Hammer, A. Romashchenko, A. Shen, and N. Vereshchagin. Inequalities for Shannon entropies and\nKolmogorov complexities. J. Comput. Syst. Sci., 60:442\u2013464, 2000.\n[10] A.N. Kolmogorov. Three approaches to the quantitative definition of information. Problems Inform.\nTransmission, 1(1):1\u20137, 1965.\n[11] A.N. Kolmogorov. Complexity of algorithms and objective definition of randomness. Uspekhi Mat.\nNauk, 29(4):155, 1974. Abstract of a talk at the Moscow Math. Soc. meeting 4/16/1974. In Russian.\n[12] A.N. Kolmogorov. Combinatorial foundations of information theory and the calculus of probabilities.\nRussian Math. Surveys, 38(4):29\u201340, 1983.\n[13] L.G. Kraft. A device for quantizing, grouping and coding amplitude modulated pulses. Master's thesis,\nDept. of Electrical Engineering, M.I.T., Cambridge, Mass., 1949.\n[14] S.K. Leung-Yan-Cheong and T.M. Cover. Some equivalences between Shannon entropy and Kolmogorov\ncomplexity. IEEE Transactions on Information Theory, 24:331\u2013339, 1978.\n[15] L.A. Levin. Laws of information conservation (non-growth) and aspects of the foundation of probability\ntheory. Problems Inform. Transmission, 10:206\u2013210, 1974.\n[16] L.A. Levin. Randomness conservation inequalities; information and independence in mathematical\ntheories. Inform. Contr., 61:15\u201337, 1984.\n[17] L.A. Levin. Forbidden information. In Proc. 47th IEEE Symp. Found. Comput. Sci., pages 761\u2013768,\n2002.\n[18] M. Li and P.M.B. Vit\u00e1nyi. An Introduction to Kolmogorov Complexity and Its Applications. SpringerVerlag, 1997. 2nd Edition.\n[19] N. Merhav and M. Feder. Universal prediction. IEEE Transactions on Information Theory, IT44(6):2124\u20132147, 1998. invited paper for the 1948-1998 commemorative special issue.\n[20] J.J. Rissanen. Stochastical Complexity and Statistical Inquiry. World Scientific, 1989.\n[21] J. Rissanen and I. Tabus. Kolmogorov's structure function in MDL theory and lossy data compression.\nIn P. D. Gr\u00fcnwald, I. J. Myung, and M. A. Pitt (Eds.), Advances in Minimum Description Length:\nTheory and Applications. MIT Press, 2004.\n[22] C.E. Shannon. The mathematical theory of communication. Bell System Tech. J., 27:379\u2013423, 623\u2013656,\n1948.\n[23] C.E. Shannon. Coding theorems for a discrete source with a fidelity criterion. In IRE National Convention Record, Part 4, pages 142\u2013163, 1959.\n[24] R.J. Solomonoff. A formal theory of inductive inference, part 1 and part 2. Inform. Contr., 7:1\u201322,\n224\u2013254, 1964.\n[25] V. Vovk. Competitive on-line statistics, Intern. Stat. Rev., 69:213\u2013248, 2001.\n\n53\n\n\f[26] N.K. Vereshchagin and P.M.B. Vit\u00e1nyi. Kolmogorov's structure functions and model selection. IEEE\nTrans. Informat. Theory. To appear.\n[27] N.K. Vereshchagin and P.M.B. Vit\u00e1nyi. Rate-distortion theory for individual data. Manuscript, CWI,\n2004.\n[28] Wallace, C. and P. Freeman. Estimation and inference by compact coding. Journal of the Royal\nStatistical Society, Series B 49, 240\u2013251, 1987. Discussion: pages 252\u2013265.\n[29] A.K. Zvonkin and L.A. Levin. The complexity of finite objects and the development of the concepts of\ninformation and randomness by means of the theory of algorithms. Russian Math. Surveys, 25(6):83\u2013124,\n1970.\n\n54\n\n\f"}