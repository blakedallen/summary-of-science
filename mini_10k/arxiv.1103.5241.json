{"id": "http://arxiv.org/abs/1103.5241v2", "guidislink": true, "updated": "2011-06-13T19:40:40Z", "updated_parsed": [2011, 6, 13, 19, 40, 40, 0, 164, 0], "published": "2011-03-27T18:33:12Z", "published_parsed": [2011, 3, 27, 18, 33, 12, 6, 86, 0], "title": "Integrated Impact Indicators (I3) compared with Impact Factors (IFs): An\n  alternative research design with policy implications", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1103.2548%2C1103.6201%2C1103.4037%2C1103.2221%2C1103.5241%2C1103.2349%2C1103.5346%2C1103.5593%2C1103.2511%2C1103.2344%2C1103.6270%2C1103.0272%2C1103.4822%2C1103.3915%2C1103.4700%2C1103.2251%2C1103.1330%2C1103.4029%2C1103.5312%2C1103.0884%2C1103.1011%2C1103.2813%2C1103.6272%2C1103.6157%2C1103.3888%2C1103.0549%2C1103.4266%2C1103.4205%2C1103.2969%2C1103.1927%2C1103.3173%2C1103.0916%2C1103.1010%2C1103.1802%2C1103.3592%2C1103.2720%2C1103.4548%2C1103.3756%2C1103.5906%2C1103.2454%2C1103.5384%2C1103.3154%2C1103.1289%2C1103.4975%2C1103.4491%2C1103.0734%2C1103.4734%2C1103.5333%2C1103.3990%2C1103.0969%2C1103.0571%2C1103.2271%2C1103.5151%2C1103.2843%2C1103.1753%2C1103.3718%2C1103.3088%2C1103.1209%2C1103.5631%2C1103.2455%2C1103.1786%2C1103.4264%2C1103.1975%2C1103.6161%2C1103.5336%2C1103.0677%2C1103.1772%2C1103.0259%2C1103.0227%2C1103.2867%2C1103.3768%2C1103.1660%2C1103.6241%2C1103.0999%2C1103.4299%2C1103.3851%2C1103.1001%2C1103.3847%2C1103.5086%2C1103.2310%2C1103.0559%2C1103.3845%2C1103.3635%2C1103.3818%2C1103.1128%2C1103.1796%2C1103.5112%2C1103.0632%2C1103.2679%2C1103.4452%2C1103.1659%2C1103.0728%2C1103.3392%2C1103.0427%2C1103.0910%2C1103.5236%2C1103.4440%2C1103.4669%2C1103.1551%2C1103.1586%2C1103.5947&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Integrated Impact Indicators (I3) compared with Impact Factors (IFs): An\n  alternative research design with policy implications"}, "summary": "In bibliometrics, the association of \"impact\" with central-tendency\nstatistics is mistaken. Impacts add up, and citation curves should therefore be\nintegrated instead of averaged. For example, the journals MIS Quarterly and\nJASIST differ by a factor of two in terms of their respective impact factors\n(IF), but the journal with the lower IF has the higher impact. Using percentile\nranks (e.g., top-1%, top-10%, etc.), an integrated impact indicator (I3) can be\nbased on integration of the citation curves, but after normalization of the\ncitation curves to the same scale. The results across document sets can be\ncompared as percentages of the total impact of a reference set. Total number of\ncitations, however, should not be used instead because the shape of the\ncitation curves is then not appreciated. I3 can be applied to any document set\nand any citation window. The results of the integration (summation) are fully\ndecomposable in terms of journals or instititutional units such as nations,\nuniversities, etc., because percentile ranks are determined at the paper level.\nIn this study, we first compare I3 with IFs for the journals in two ISI Subject\nCategories (\"Information Science & Library Science\" and \"Multidisciplinary\nSciences\"). The LIS set is additionally decomposed in terms of nations. Policy\nimplications of this possible paradigm shift in citation impact analysis are\nspecified.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1103.2548%2C1103.6201%2C1103.4037%2C1103.2221%2C1103.5241%2C1103.2349%2C1103.5346%2C1103.5593%2C1103.2511%2C1103.2344%2C1103.6270%2C1103.0272%2C1103.4822%2C1103.3915%2C1103.4700%2C1103.2251%2C1103.1330%2C1103.4029%2C1103.5312%2C1103.0884%2C1103.1011%2C1103.2813%2C1103.6272%2C1103.6157%2C1103.3888%2C1103.0549%2C1103.4266%2C1103.4205%2C1103.2969%2C1103.1927%2C1103.3173%2C1103.0916%2C1103.1010%2C1103.1802%2C1103.3592%2C1103.2720%2C1103.4548%2C1103.3756%2C1103.5906%2C1103.2454%2C1103.5384%2C1103.3154%2C1103.1289%2C1103.4975%2C1103.4491%2C1103.0734%2C1103.4734%2C1103.5333%2C1103.3990%2C1103.0969%2C1103.0571%2C1103.2271%2C1103.5151%2C1103.2843%2C1103.1753%2C1103.3718%2C1103.3088%2C1103.1209%2C1103.5631%2C1103.2455%2C1103.1786%2C1103.4264%2C1103.1975%2C1103.6161%2C1103.5336%2C1103.0677%2C1103.1772%2C1103.0259%2C1103.0227%2C1103.2867%2C1103.3768%2C1103.1660%2C1103.6241%2C1103.0999%2C1103.4299%2C1103.3851%2C1103.1001%2C1103.3847%2C1103.5086%2C1103.2310%2C1103.0559%2C1103.3845%2C1103.3635%2C1103.3818%2C1103.1128%2C1103.1796%2C1103.5112%2C1103.0632%2C1103.2679%2C1103.4452%2C1103.1659%2C1103.0728%2C1103.3392%2C1103.0427%2C1103.0910%2C1103.5236%2C1103.4440%2C1103.4669%2C1103.1551%2C1103.1586%2C1103.5947&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In bibliometrics, the association of \"impact\" with central-tendency\nstatistics is mistaken. Impacts add up, and citation curves should therefore be\nintegrated instead of averaged. For example, the journals MIS Quarterly and\nJASIST differ by a factor of two in terms of their respective impact factors\n(IF), but the journal with the lower IF has the higher impact. Using percentile\nranks (e.g., top-1%, top-10%, etc.), an integrated impact indicator (I3) can be\nbased on integration of the citation curves, but after normalization of the\ncitation curves to the same scale. The results across document sets can be\ncompared as percentages of the total impact of a reference set. Total number of\ncitations, however, should not be used instead because the shape of the\ncitation curves is then not appreciated. I3 can be applied to any document set\nand any citation window. The results of the integration (summation) are fully\ndecomposable in terms of journals or instititutional units such as nations,\nuniversities, etc., because percentile ranks are determined at the paper level.\nIn this study, we first compare I3 with IFs for the journals in two ISI Subject\nCategories (\"Information Science & Library Science\" and \"Multidisciplinary\nSciences\"). The LIS set is additionally decomposed in terms of nations. Policy\nimplications of this possible paradigm shift in citation impact analysis are\nspecified."}, "authors": ["Loet Leydesdorff", "Lutz Bornmann"], "author_detail": {"name": "Lutz Bornmann"}, "author": "Lutz Bornmann", "links": [{"href": "http://arxiv.org/abs/1103.5241v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1103.5241v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.soc-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1103.5241v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1103.5241v2", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Integrated Impact Indicators (I3) compared with Impact Factors (IFs):\nAn alternative research design with policy implications\nJournal of the American Society for Information Science and Technology (in press)\n\nLoet Leydesdorff i and Lutz Bornmann ii\n\nAbstract\nIn bibliometrics, the association of \"impact\" with central-tendency statistics is mistaken.\nImpacts add up, and citation curves should therefore be integrated instead of averaged.\nFor example, the journals MIS Quarterly and JASIST differ by a factor of two in terms of\ntheir respective impact factors (IF), but the journal with the lower IF has the higher\nimpact. Using percentile ranks (e.g., top-1%, top-10%, etc.), an integrated impact\nindicator (I3) can be based on integration of the citation curves, but after normalization of\nthe citation curves to the same scale. The results across document sets can be compared\nas percentages of the total impact of a reference set. Total number of citations, however,\nshould not be used instead because the shape of the citation curves is then not appreciated.\nI3 can be applied to any document set and any citation window. The results of the\nintegration (summation) are fully decomposable in terms of journals or instititutional\nunits such as nations, universities, etc., because percentile ranks are determined at the\npaper level. In this study, we first compare I3 with IFs for the journals in two ISI Subject\nCategories (\"Information Science & Library Science\" and \"Multidisciplinary Sciences\").\nThe LIS set is additionally decomposed in terms of nations. Policy implications of this\npossible paradigm shift in citation impact analysis are specified.\nKeywords: impact, percentiles, indicator, citation, significance, highly cited, papers\n\ni\n\nUniversity of Amsterdam, Amsterdam School of Communication Research (ASCoR), Klovenierburgwal\n48, 1012 CX Amsterdam, The Netherlands; loet@leydesdorff.net.\nii\nMax Planck Society, Hofgartenstrasse 8, D-80539 Munich, Germany; bornmann@gv.mpg.de.\n\n1\n\n\fIntroduction to the problem\n\nLet us introduce the problem of defining impact by taking as an example the citation\ncurves of two journals with very different impact factors (IFs):\n\nCitations (Feb. 17, 2011) \u2192\n\n100\n\n80\n\n60\n\nJASIST\nMIS Quart\nsum\n\n40\n\n20\n\n0\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nSequence numbers of citable publications in 2007 and 2008 \u2192\n\nFigure 1: Citation curves for JASIST (n = 375 publications) and MIS Quarterly (n = 66).\nFigure 1 shows the citation curves of the 66 and 375 citable items published in MIS\nQuarterly and JASIST, respectively, during 2007 and 2008. 1 These two journals are both\nattributed by Thomson Reuters-the present owner of the Institute of Scientific\nInformation (ISI)-to the ISI Subject Category of \"Library and Information Science\"\n\n1\n\nThe Journal Citation Reports (JCR) 2009 lists 370 instead of 375 citable issues for 2007 plus 2008. This\ndifference originates from the date in March that the JCR-team at Thomson Reuters decides to use for\nproducing the JCR of the year before (McVeigh, personal communication, April 7, 2010). IFs are\nnotoriously difficult to reproduce using Web-of-Science data (e.g., Brumback, 2008a and b; Rossner et al.,\n2007 and 2008; Pringle, 2008).\n\n2\n\n\f(LIS), 2 although they are very different in character (Nisonger & Davis, 2005; Zhao &\nStrotman, 2008). Within this Subject Category, MIS Quarterly had the highest IF in\n2009: 4.485. The IF of JASIST is approximately half this size: IF-2009 = 2.300. However,\nthe 66 most-highly cited publications of JASIST obtained 380 citations more than the 66\ncitable items published in MIS Quarterly (downloaded on February 17, 2011). The lower\nIF factor is entirely due to the tail of 300+ additional publications in JASIST with lower\ncitation rates.\n\nIn our opinion, this confusion finds its origin in the definition of the \"impact factor\" as a\ntwo-year average of \"impact\" (Garfield, 1972; Garfield & Sher, 1963; cf. Bensman,\n2008; Rousseau & Leydesdorff, 2011). 3 Impact (as a variable), however, is not an\naverage, but the result of the sum of the momenta of the impacting units. For example,\ntwo meteors impacting on a planet can have a combined impact larger than that of each of\nthem taken separately, but the respective velocities also matter.\n\n\uf072\nIn physics, momentum is defined as the vector of mass times velocity ( p \uf03d mv ). Using\nthe metaphor of impact, both the number of publications (the \"mass\") and their citation\ncounts (the quality of \"the velocity\") matter for the impacts. Because citations are scalar\n\uf072\ncounts, one can disregard the direction of the vectors in the summation (\u03a3 mv ). The\n\nresearch question is then how to operationalize m in terms of the numbers of publications\nand v in terms of citations in order to obtain a relevant measure of impact as a sum. The\nimpact of each subset can be expressed as a percentage impact of the set.\n2\n3\n\nThe ISI uses \"Information Science & Library Science\" as name of this category.\nMore recently, the ISI also introduced the five-year IF in the Journal Citation Reports (JCR).\n\n3\n\n\fTable 1: Comparison of MIS Quarterly and JASIST in terms of citation rates to citable\nitems in 2007 and 2008. Publication and citation data retrieved at the Web of Science\n(WoS) on February 17, 2011.\nMIS Quart\nJASIST\nsum\n\nIF 2009\n296/66 = 4.485\n851/370 = 2.300\n1147/436 = 2.631\n\n(P)ublications\nin our data\n66\n375\n441\n\n(C)itations\nin our data\n847\n1975\n2822\n\nC/P\n12.83\n5.27\n6.40\n\nMedian\n8\n3\n3\n\nIt has been argued (e.g., Bornmann & Mutz, 2011; Leydesdorff & Opthof, 2011) that the\nmedian should be used in citation analysis instead of the mean because of the skewness\nof citation distributions (e.g., Seglen, 1992). For the two journals in the example above-\nand using the two-year time window of the ISI-IF-Table 1 shows that the median is\neven more sensitive to the tails of the distributions than the mean. A more radical solution\nis therefore needed: impact has to be defined not as a distribution, but as a sum. Very\ndifferent distributions can add up to the same impact. The number of citations can be\nhighly skewed and in this situation any measure of central tendency is theoretically\nmeaningless.\n\nWhereas distributions of citations can be tested non-parametrically for the significance of\nthe differences among them, impacts are sum values. These values can be tested against\nthe expected values of the variables. For example, if the set of documents in one journal\nis twice as large as the set in another, the chance that it will contain a top-1% most\nhighly-cited document is twice as high. If the observed value, however, would be four\ntimes as high, this achievement above expectation may be statistically significant, but this\ndepends also on the sample size (N).\n\n4\n\n\fCentral tendency statistics cannot capture the increases in impact when two sets\n(\"masses\") are added; for example, when two research groups join forces or two journals\nmerge. We penciled the line for the sumtotal of JASIST and MIS Quarterly into Figure 1\nin order to show that one has to sum surfaces, and thus the citation curves have to be\nintegrated instead of averaged. Assuming the integrals, however, would lead to numbers\nequal to the \"total citations\" without qualifying the documents in terms of their citedness.\nIn order to weigh the documents, we suggest to transform the citation curves first into\ncurves of hundred percentiles as in Figure 2.\n100\n\nPercentiles in LIS\n\n80\nJASIST\nMIS Quart\n60\n\n40\n\n20\n\n0\n0\n\n100\n\n200\n\n300\n\n400\n\nSequence numbers of citable publications in 2007 and 2008 \u2192\n\nFigure 2: Distributions of 100 Percentile Ranks of JASIST and MIS Quarterly with\nreference to the 65 journals of the ISI Subject Category LIS.\nThe distributions of percentile ranks can fairly be compared across document sets, and\nthese linearly transformed distributions can be integrated. The integrals in this stepwise\nfunction are equal to\n\n\uf0e5 x * f ( x ) in which x represents the percentile rank and f(x) the\ni i\n\ni\n\n5\n\n\ffrequency of that rank; i is one hundred when using percentiles, but, for example, four\nwhen using quartiles as percentile rank classes (etc.). One can also consider hundred\npercentiles as a continuous random variable and sum these values, as we shall explain in\nmore detail in the methods section below. The function integrates both the number of\npapers (the \"mass\") and their respective quality in terms of being-cited normalized as\npercentiles with reference to a set.\n\nThe idea of using percentile rank classes was first formulated in the discussion about\nproper normalization of the citation distribution that took place last year in the Journal of\nInformetrics (e.g., Bornmann, 2010; Opthof & Leydesdorff, 2010; Van Raan et al.,\n2010a; cf. Gingras & Larivi\u00e8re, 2011). In this context, one of us proposed to assess\ncitation distributions in terms of six percentile rank classes (6PR): the top-1%, top-5%,\ntop-10%, top-25%, top-50%, and bottom-50% (Bornmann & Mutz, 2011). This\n(normative!) evaluation scheme accords with those currently used in the bi-annual\nScience & Technology Indicators of the National Science Board of the USA (2010, at\nAppendix Table 5-43). Each publication would then be weighted in accordance to its\nclass as a six for the top-1% category and a one for the bottom-50% category.\nLeydesdorff, Bornmann, Mutz & Opthof (in press) extended this approach to hundred\npercentiles which can also be weighted as classes from 1 to 100 (100PR).\n\nThe advantage of using percentile ranks is that one is thus able to compare distributions\nof citations across unequally sized document sets using a single scheme for the evaluation\nof the shape of the distribution. However, Bornmann and Mutz's (2011) approach\n\n6\n\n\fremained sensitive to the central-tendency characteristic discussed above because these\nauthors averaged over the percentile ranks using the following formula:\nRi \uf03d \uf0e5i xi * p ( xi ) . In this formula, x is the rank class and p(x) its relative frequency (or\nproportion). However, this probabilistic approach implies a division by the N of cases and\nthus normalization to the mean (albeit of the distribution of the percentiles). Leydesdorff\net al. (in press) therefore called this method the \"mean percentile rank\" approach.\n\nMeans cannot be added, whereas impacts are additive. Bensman & Wilder (1998) have\nconcluded on the basis of validation studies that the prestige of journals in chemistry is\ncorrelated with the total number of citations more than with the impact factors of journals.\nAs noted above, however, total citations do not yet qualify the shape of the distributions\nsince every citation is then counted equally. Impact factors only qualify the distribution in\nterms of the mean, but deliberately abstract from size (Bensman, 2008). Using the sum\ntotal of the frequencies (f) in each percentile, however, accounts for both the size and\nshape of the distribution. The citations are weighted in accordance with the percentile\nrank class of each publication in the Integrated Impact Indicator: I 3 \uf03d \uf0e5i xi * f ( xi ) .\n\n7\n\n\f150\nMIS Quart\nJASIST\n\nN of papers\n\n100\n\n50\n\n0\n99-100%\n\n95-99%\n\n90-95%\n\n75-90%\n\n50-75%\n\n0-50%\n\nDistribution using Six Percentile Rank Classes\n\nFigure 3: Distributions of the six percentile ranks of publications in terms of citations to\nJASIST and MIS Quarterly (with reference to all 65 of LIS).\nFigure 3 shows the distribution of the six percentile ranks of the National Science Board\n(2010) for MIS Quarterly and JASIST. Both Figures 2 and 3 show that JASIST has an\nimpact higher than MIS Quarterly using these normalized curves. Table 2 shows that this\nhigher value can be captured by the sum, but not by the means or medians of the\npercentile distributions.\nTable 2: Mean (\u00b1 s.e.m.), median and sum in the case of hundred or six percentile ranks.\n\nMIS Q\nJASIST\n\n100PR\nMean\n84.57 \u00b1 1.98\n55.50 \u00b1 1.76\n\nMedian\n90\n61\n\nSum\n5,581.4\n20,811.3\n\n6PR\nMean\n3.56 \u00b1 0.15\n2.31 \u00b1 0.07\n\nMedian\n3\n2\n\nSum\n235\n867\n\nThe sum values can be added (and subtracted), for example, for the purpose of the\naggregation or decomposition (e.g., in terms of contributing nations), and they can also\n\n8\n\n\fbe expressed as percentages of the total integrated impact of an ISI Subject Category (e.g.,\nthe 65 journals in the LIS category). For example, the sum total of the impact of all 5,737\ncitable items in the LIS category was in 2009: 213,906.2. 4 MIS Quarterly contributed\n2.61% to this total impact of the set when using 100PR, and 2.34% in the case of 6PR.\nThese percentages were 9.73% and 8.63% for JASIST, respectively. JASIST is thus to be\nconsidered as the journal with the highest impact in the set of 65 journals subsumed\nunder LIS among the ISI Subject Categories (Nisonger, 1999).\n25,000\n\nJ Am Soc Inf Sci Technol\n\nImpact I3\n\n20,000\n\nScientometrics\nJ Amer Med Inform Assoc\n\n15,000\n\nInform Process Manage\ny = 29.753x + 664.78\n2\nR = 0.3827\n\n10,000\nInform Management\nInt J Geogr Inf Sci\nMIS Quart\nJ Acad Libr\nJ Informetr\nGovt Inform Quart\nElectron Libr\nJ Inform Technol\nProf Inf\nRes Evaluat\nLibr Trends\nLibri Online\nEcontent Inf Res\n\n5,000\n\n0\n0\n\n50\n\n100\n\n150\n\nScientist\nLibr J\n\n200\n\n250\n\n300\n\n350\n\n400\n\n450\n\nNumber of citable publications in 2007 and 2008\n\nFigure 4: Regression of impact (I3) against number of citable publications in 2007 and\n2008 for the 65 journals of LIS.\nFigure 4 further elaborates this example by showing the regression line of the impacts\nthus calculated against the number of citable publications in 2007 and 2008. Unlike\ndividing sums to obtain average values-which was the core issue in the previous\n4\n\nThis sum total is equal to 39.8% of the maximally possible impact of (100 * 5,737 =) 537,700 in the case\nof 100PR. In the case of 6PR, the total is 10,049 or 29.2% of the maximally possible impact of (6 * 5,737\n=) 34,422.\n\n9\n\n\fcontroversy about the Leiden crown indicator (Gingras & Larivi\u00e8re, 2011)-this\nregression informs us that the journal set under study is heterogeneous (r2 = 0.38), both in\nsize and functions. The Scientist and the Library Journal, for example, are grouped in the\nbottom-right angle of this figure because they function as newsletters more than scholarly\njournals. Among the journals at the top right, the label for JASIST was colored red in\norder to show its leading position in this set. MIS Quarterly is a journal above the\nregression line in the set of specialized journals at the bottom left, but also colored red for\nthe purpose of this comparison.\n\nMethods\n\nData was harvested from the WoS in February 2011. Because we wished to compare our\nresults for I3 with the latest available IFs 2009, we downloaded citable items in 2007 and\n2008 in two ISI Subject Categories, namely, the one for LIS containing 65 journals and\nthe category \"Multidisciplinary Sciences\" (MS) containing 48 journals, but including\nimportant journals such as Science, Nature, and PNAS. The delineation of the ISI Subject\nCategories is beset with error (Rafols & Leydesdorff, 2009). Within this context,\nhowever, we use them pragmatically as reference sets because it is beyond our capacity-\nand perhaps illegal-to download the entire database. Only articles, proceedings papers,\nreviews, and letters are included because these categories are indicated by Thomson\nReuters-the present owner of the Institute of Scientific Information (ISI)-as citable\nitems (cf. Moed & Van Leeuwen, 1996). Note that I3 is by no means confied to this\n\n10\n\n\fdefinition of impact in terms of two previous years, but can be used for any document set\nand with any citation window.\n\nThe citation of each paper is rated in terms of its percentile in the distribution of citations\nto all items with the same document type and publication year in its ISI Subject Category\nas the respective reference sets. The percentile is determined by using the counting rule\nthat the number of items with lower citation rates than the item under study determines\nthe percentile. Tied citation numbers are thus provided with the highest values, and this\naccords with the idea of providing all papers with the highest possible ranking (in other\nwords: we wish to give the papers \"the benefit of the doubt\"). Other schemes are also\npossible. For example, Pudovkin & Garfield (2009) first averaged tied ranks.\n\nThe percentiles can be considered as a continuous random variable in the case of one\nhundred percentiles. In the case of six percentile ranks, one has to round off. Differently\nfrom Leydesdorff et al. (in press), the rounding off will in this study be based on adding\n0.9 to the count- that is: (count + 0.9)-because otherwise one can expect undesirable\neffects for datasets that are smaller than one hundred. For example, if a journal with\nmany articles publishes only 10 reviews each year, the highest possible percentile within\nthis set would be the 90th-nine out of ten-whereas this could be the 99th-that is, 9.9\nout of 10, and thus included in the top-1% percentile rank (with a value of six in the\nmentioned evaluation scheme of the NSF).\n\n11\n\n\fAs shown in Figures 2 and 3 above comparing MIS Quarterly with JASIST, the\npercentiles provide us with a scale that can be compared across document sets with\ndifferent sizes. In the case of a normative evaluation scheme such as that of the NSF, the\npercentiles are binned in six percentile rank classes. This transformation is non-linear and\none looses information, but an evaluator may gain clarity in the distinctions from a policy\nperspective (Leydesdorff et al., in press). We shall use this second set of values\nthroughout this study for the comparison, but distinguish it from I3 by denoting this\n6\n\nmeasure as I3(6PR). The formula is then specified as follows: I 3(6 PR ) \uf03d \uf0e5 xi * PRi , in\ni \uf03d1\n\nwhich PRi is the frequency value in the respective class. Other evaluation schemes are\nalso possible, but this is, in our opinion, a normative discussion which can be expected to\nchange with the policy context.\n\nThe set based on the 65 journals of LIS contained 5,737 citable items published in 2007\nor 2008. The set indicated by the ISI as journals in MS was much larger in terms of the\nnumber of documents (24,494 citable items) despite the smaller number of journals (48).\nThe two sets were brought under the control of relational database management and when\nnecessary dedicated routines were written in order to format the data for analysis in SPSS\n(v. 18) and Excel. Using the WoS, the numbers of citations were determined at the date\nof downloading, in our case February 2011.\n\nThe most relevant routine in SPSS is \"Compare Means\" using the journals (in each set,\nrespectively) as the independent (grouping) variable and the percentiles (or the six classes,\nmutatis mutandis) as the dependent variables. This routine allows for determining the\n12\n\n\fmean, the sum, the standard error of the mean, confidence levels, and other statistics in a\nsingle pass. Since we are mainly interested in the sum, the mean, and the standard error\nof the mean, this routine is sufficient for our purpose. Correlation analysis (both\nPearson's r and Spearman's rank-order correlation \u03c1) will also be pursued using SPSS in\norder to compare the new indicators with IFs.\n\nIn addition to analyzing the impact of each journal, the question can be raised of whether\nthe citation distributions are also significantly different. Non-parametric statistics enable\nus to answer this question using the citation distributions (as depicted in Figure 1)\nwithout averaging or first transforming them into percentile ranks. Among the routines\navailable for multiple comparisons in SPSS (with Bonferroni correction), Dunn's test can\nbe simulated by using LSD (\"least significant differences\") with family-wise correction\nfor Type-I error probability. In the case of N groups to be compared, the number of\ncomparisons is N * (N-1) / 2. For example, in the case of 50 journals, 50 * 49 /2 = 1,225\ncomparisons are pursued, and the significance should hence be tested at the five percent\nlevel using 0.05 / 1,225 = 0.000041 instead of 0.05 (Levine, 1991, at pp. 68 ff.).\n\nThe routine for multiple comparisons in SPSS is limited to 50 groupings at a time. In the\ncase of the MS set, 48 journals are involved, but in the case of LIS 65 journals can be\ncompared. We perform the analysis in this study using the 50 journals with the highest\nIFs among these 65 journals. (The IF was chosen as criterion in order not to bias our\nresults in favour of the proposed measure.) Alternatively, one can test any two journals\nagainst each other using the Mann-Whitney U test with Bonferroni correction. However,\n\n13\n\n\fin the case of 65 journals, these would be 2,080 one-by-one comparisons. This seemed\nnot necessary for the purpose of this study.\n\nFurthermore, the algorithm of Kamada & Kawai (1989)-as available, for example, in\nPajek-provides us with a means to visualize groups of journals as not significantly\ndifferent-or, in other words, homogenous-in terms of their citation distributions (cf.\nLeydesdorff & Bornmann, 2011, at p. 224f.). Journals which can be considered similar in\nthis respect were linked in the graphs, while in the case of significant differences the\ngrouping links were omitted. The k-core sets which are most homogeneous in terms of\ncitation distributions can thus be visualized.\n\nIn a final section, we return to the issue of performance measurement of institutional\nunits, individuals, and/or nations (but using this same data). Since the attribution of the\npercentile rank is done at the paper level, one can aggregate and decompose sub-sets in\nterms of their contribution to the reference set. We shall use country names in the address\nfield as an example. Each contribution to I3 can also be expressed as a percentage.\n\nThe observed contributions can be tested against the expected ones on the basis of the\ndistribution of citable items across units of analysis (such as journals or nations). In a\nlarger set, for example, one can expect more highly-cited papers for stochastic reasons.\nWhether a difference is statistically significant or not can be assessed for each case using\nthe binomial z-test or the standardized residuals of the \u03c72. We use the latter measure\n\n14\n\n\f[Z \uf03d\n\nobserved \uf02d exp ected\n] because this test is simpler and less conservative than the\nexp ected\n\nbinomial test. Expected values below five are discarded as unreliable.\n\nA z-value of 1.96 (that is, almost two standard deviations) can be considered as\nsignificant at the 5% level, and similarly z0.01 = 2.576. The notation of SPSS will be\nfollowed in this study using two asterisks for significance at the 1% level, and a single\nasterisk for the 5% level. However, we use the signs of the differences (++, +, -- or -)\nwhen relevant in the tables to indicate whether the observed values are significantly\nabove or below the expected values, and at which level of significance.\n\nIn summary, we distinguish between testing (1) observed impacts as sum values of\npercentiles against expected impacts of units of analysis (e.g., journals, nations, etc.)\nusing Z-statistics, and (2) differences in the citation distributions, for example, in terms of\nDunn's test. The latter test provides us with a non-parametric alternative to comparing\nthese distributions in terms of their arithmetic averages, as is done in the case of\ncomparing among IFs (cf. Leydesdorff, 2008).\n\nResults\n\nI3 for the 65 journals of LIS\nTable 3 provides the Pearson correlations (in the lower triangle) and the Spearman rankorder correlations (upper triangle) between the various indicators under discussion.\n\n15\n\n\fTable 3: Rank-order correlations (Spearman's \u03c1; upper triangle) and Pearson correlations\nr (lower triangle) for 65 journals of LIS.\nIndicator\n\nIF-2009\n\nI3 (100PR)\n\nMean\n100PR\n.924 **\n.843 **\n\nI3 (6PR)\n\nMean 6PR\n\nNumber of\npublications\n.263 *\n.670 **\n.238\n.907 **\n.271 *\n\n.804 **\n.582 **\n.936 **\nIF-2009\n.591 **\n.875 **\n.862 **\nI3 (100PR)\n.839 **\n.651 **\n.571 **\n.983 **\nMean 100PR\n.479 **\n.924 **\n.417 **\n.608 **\nI3 (6PR)\n.893 **\n.648 **\n.950**\n.506 **\nMean 6PR\n.151\n.619 **\n.042\n.861 **\n.134\nN of publications\n.685 **\n.963 **\n.631 **\n.894 **\n.713 **\n.551 **\nTotal citations\nNote: ** Correlation is significant at the 0.01 level (2-tailed); * Correlation is significant at the 0.05 level (2tailed).\n\nTotal\ncitations\n.893 **\n.974 **\n.907 **\n.817 **\n.931 **\n.562 **\n\nMost of the correlation coefficients are high (\u2265 .7), and significant at the one percent\nlevel. Using Pearson correlation coefficients, however, the numbers of publications (N) in\njournals of this set are not correlated to the IFs (r = .151; n.s.) or the mean values of\n100PR (r = .042; n.s.) and 6PR (r = .134; n.s.). These indicators have in common that\nthey are based on averages and therefore division by the N of publications in each set.\n\nThe value of I3, however, is correlated significantly to both the number of publications (r\n= .619; p < 0.01) and the total number of citations (r = .963; p < 0.01). These correlations\nare higher than the correlation between the numbers of citations and publications (r\n= .555; ; p < 0.01) which is largely a spurious correlation caused by size differences\namong the 65 journals.\n\nThe correlations with both the number of publications and citations could be expected\nbecause of the definition of I3. Like the h index (Hirsch, 2005), I3 takes both\ndimensions-the number of publications and their citation rates-into account in the\ndefinition of impact, but differently from the h index, the tails of the distributions are not\ndiscarded as irrelevant. Ceteris paribus in terms of the top-segment (e.g, h = 10), the\n\n16\n\n\fnumber of publications with less impact matters for the overall impact of two otherwise\ncomparable documents sets.\n\nFigure 5: Varimax rotated two-factor solution of the variables IF, I3, I3(6PR), number of\npublications, and citations.\n\nFigure 5 shows the plot of the (Varimax rotated) two-factor solution of the variables that\nchiefly interest us here. As can be expected, the number of publications and the IF span\northogonal coordinates (Leydesdorff, 2009). The I3 values are closest to total cites\nbecause the transformation is linear, whereas a non-linearity is involved in the case of\nI3(6PR). Unlike the total number of citations, however, the new indicator takes the\nshapes of the distributions into account by normalizing in terms of percentiles. The\n\n17\n\n\fnumber of publications has an effect on I3 independent of the latter's correlation with the\ntotal number of citations.\n\nThis can be shown as follows: the partial correlation between I3 and the number of\npublications controlled for the number of citations (rI3,N|TC ) is .391 (p = 0.01), whereas\nrIF, N|TC = -.373 (p < 0.05). For the I3(6PR) this partial correlation rI3(PR6),N|TC = .985 (p <\n0.01) indicating that the binning into six percentile rank classes uncouples relatively from\nthe citation rates and makes the publication rates therefore more important. The hundred\npercentile ranks provide a finer-grained and therefore more precise indicator of citation\nimpact than the ranking in six classes.\n\nThe correlations in Table 3 were calculated at the journal level. Although the correlation\nbetween I3 and the sum of total citations is very high (r = .963; p < 0.01), the underlying\ndata allow us also to consider the correlation between the times cited and the percentiles\nat the level of the 5,737 documents. The Pearson correlation is in this case only .639 (p <\n0.01). 5\n\nIn summary, I3 provides us with an indicator which takes both the number of\npublications and their citations into account. The normalization to percentile ranks\nappreciates the shape of the distribution; the transformation of the citation curve is linear.\nNo parametric assumptions (such as averages and standard deviations) are made. The\ndefinitions are sufficiently abstract so that impact is no longer defined in terms of a fixed\n\n5\n\nAs could be expected, the correlation between times cited and the binned values of I3(6PR) is much\nhigher (r = .815; \u03c1 = .911) because the binning reduces the variance.\n\n18\n\n\fcitation window: any document set can be so evaluated. Different from the h index, the\nfull citation curve is weighted into these non-paramatric statistics.\n\nTable 4: Rankings between 15 journals of LIS with highest values on I3 (expressed as\npercentages of the sum) compared with IFs, total citations, and % I3(6PR).\nJournal\n\nJ Am Soc Inf Sci Technol\nScientometrics\nJ Amer Med Inform Assoc\nInform Process Manage\nInform Management\nInt J Geogr Inf Sci\nMIS Quart\nJ Manage Inform Syst\nJ Health Commun\nJ Acad Libr\nJ Inform Sci\nJ Comput-Mediat Commun\nJ Informetr\nJ Med Libr Assoc\nTelecommun Policy\n\nN of\npapers\n(a)\n375\n258\n199\n221\n117\n120\n66\n82\n90\n127\n102\n108\n66\n114\n96\n\n% I3\n\n9.73\n7.24\n6.80\n6.14\n4.01\n3.14\n2.61\n2.60\n2.52\n2.51\n2.43\n2.37\n2.28\n2.21\n2.15\n\nIF 2009\n(b)\n[1] ++\n[2] ++\n[3] ++\n[4] ++\n[5] ++\n[6] ++\n[7] ++\n[8] ++\n[9] ++\n[10] ++\n[11] ++\n[12] ++\n[13] ++\n[14] ++\n[15] ++\n\n(c)\n2.300 [7]\n2.167 [10]\n3.974 [2]\n1.783 [15]\n2.282 [8]\n1.533 [17]\n4.485 [1]\n2.098 [11]\n1.344 [22]\n1.000 [26]\n1.706 [16]\n3.639 [3]\n3.379 [4]\n0.889 [31]\n0.969 [27]\n\nTotal\ncitations\n(d)\n1975 [1]\n1336 [3]\n1784 [2]\n921 [4]\n822 [6]\n446 [9]\n847 [5]\n496 [8]\n380 [10]\n252 [19]\n355 [13]\n374 [11]\n598 [7]\n248 [20]\n264 [17]\n\n% I3 (6PR)\n\n8.63\n6.37\n6.15\n4.90\n3.35\n2.55\n2.34\n2.31\n2.04\n2.05\n1.98\n2.04\n2.04\n1.93\n1.80\n\n(e)\n[1] ++\n[2] ++\n[3] ++\n[4] ++\n[5] ++\n[6] ++\n[7] ++\n[8] ++\n[10a] ++\n[9]\n[11]\n[10b] ++\n[10c]\n[12]\n[13]\n\nNote. ++ p < 0.01; above the expectation. Ranks are added between brackets.\n\nTable 4 provides the rankings for the 15 journals with the highest values for I3 in\ncomparison to rankings of the IFs-2009, the total citations, and I3(6PR) as an alternative\nclassification scheme. One can see that on all measures except the IF, JASIST is ranked in\nfirst place. MIS Quarterly holds the seventh position in terms of both I3 and I3(6PR).\n\nThe highly skewed citation distribution (in column d) cannot prevent the Journal of the\nAmerican Medical Informatics Association with 1,784 citations and an IF of 3.974 from\nranking below Scientometrics with only 1,336 citations and the lower IF of 2.167, yet\nnevertheless occupying the second position behind JASIST. Below the top segment, the\n\n19\n\n\fsix classes become less fine-grained than the hundred percentiles. This is visible in Table\n4 as the Journal of Health Communication, Journal of Computer-Mediated\nCommunication, and Journal of Informetrics are tied for the tenth position (within this set\nof 65 journals). In the case of the Journal of Informetrics, however, the I3(6PR) value is\nno longer significantly different from the expectation.\n\nWe have argued that one needs a statistic for testing the differences among citation\ndistributions for their relative significance beyond testing impacts as integrated values\nagainst expected impacts. Using the Kruskall-Wallis rank variance test, the nullhypothesis that citation distributions are the same across these 65 journals was rejected at\nthe 1% level. Given this result, we may further test between any two journals whether\ntheir citation distributions are significantly different. As noted, we used Dunn's test for\nthe comparison among the citation distributions of the fifty journals with the highest IFs\n(among the 65 in the LIS category); the results are summarized in Figure 6.\n\n20\n\n\fFigure 6: Fifty journals of LIS organized according to (dis)similarity in their being-cited\npatterns to 5,125 publications in 2007 and 2008.\nNote. Dunn's test for multiple comparisons (\u03b1 < 0.000041 = [0.05 / {(50 * 49)/2}];\nKamada & Kawai (1989) used for the visualization.\n\nFigure 6 shows that MIS Quarterly is significantly different in terms of its citation\ndistribution from all other journals in this group except for the Journal of Informetrics.\n(This exceptional distribution leads among other things to the high IF of this journal.)\nUsing measures for interdisciplinarity, Leydesdorff & Rafols (2010) have shown that\nthese two journals can be considered as relatively mono-disciplinary specialist journals\nwithin this set. JASIST and Scientometrics-both high on interdisciplinarity relative to\nthis set (!)-are positioned at another corner of the figure (at the bottom) as significantly\ndifferent from a number of journals in a major group of 37 journals that form a k = 25\n\n21\n\n\fcore set. The Journal of Computer-Mediated Communication, for example, is part of this\ncore set with an IF-2009 of 3.639, while at the lower end Interlending & Document\nSupply has an IF-2009 of 0.403. Differences in IFs of an order of magnitude do not\ninform us about the significance of differences in citation distributions, nor in terms of\ncitation impact unless, of course, one defines \"citation impact\" in these terms (e.g.,\nGarfield, 1972). Note that I3 is an indicator of the impact of document sets in terms of\ncitations, and thus the semantics are somewhat different.\n\nMultidisciplinary Sciences\nThe ISI Subject Category MS contains a heterogeneous set of 48 journal, ranging from\nScience and Nature with IFs of 29.747 and 34.480, respectively, to R&D Magazine with\nIF = 0.004 in 2009. However, 65.2% of all citable publications in this set during 2007\nand 2008 (that is, 24,494) were published in six major journals: PNAS (7,058; 28.8%),\nNature (2,285; 9.3%), Science (2,253; 9.2%), Annals of the NY Academy of Science\n(1,996; 8.2%), Current Science (1,271; 5.2%), and the Chinese Science Bulletin (1,115;\n4.6%).\n\nAmong these journals, Science and Nature seem to have a very similar profile (Figure 7).\nFor example, the number of not-cited papers in this set is 279 for Science and 282 for\nNature. In both cases, this is more than 10% of all citable items in the journal. The largest\namong these journals, PNAS, however, has a very different profile: only 58 (< 1%) of its\n7,085 citable publications had never been cited by the date of the download (Feb. 20,\n2011).\n\n22\n\n\fCitations, 20 Feb. 2011 \u2192\n\n10,000\n\n1,000\nNature\nScience\nPNAS\n\n\u2666\u2666\u2666\u2666\n\u25a0\u25a0\u25a0\u25a0\n\u25b2\u25b2\u25b2\u25b2\n\nIF 2009\n34.480\n29.747\n9.432\n\nI3\n16.31\n15.68\n43.29\n\n100\n\n10\n\n1\n0\n\n1,000\n\n2,000\n\n3,000\n\n4,000\n\n5,000\n\n6,000\n\n7,000\n\n8,000\n\nSequence numbers of citable publications in 2007 and 2008 \u2192\n\nFigure 7: Log-scaled citation distributions for the citable publications in 2007 and 2008\nin Nature, Science, and PNAS; downloaded at the WoS on Feb. 20, 2011.\n\nIn Figure 7, the numbers of citations are log-scaled in order to make the differences in\nthese skewed distributions more visible. The citation curve for Nature remains\nconsistently above the one for Science, but the one for PNAS is very differently shaped.\nThis journal has in total 27,419 more citations than Nature, whereas the latter has 24,488\nmore citations than Science (at this date), yet the IF of PNAS is less than one-third (IF2009 = 9.432). The large tail in the distribution of moderately cited papers works as\nabove to the disadvantage of the larger journal. Note that such a tail would similarly\ndisadvantage a highly productive research team or university.\n\n23\n\n\fTable 5: Fifteen MS journals with highest values on I3 compared in ranking with IFs,\ntotal citations, and I3(6PR).\nJournal\n\nProc Nat Acad Sci USA\nNature\nScience\nAnn NY Acad Sci\nCurr Sci\nChin Sci Bull\nPhilos Trans R Soc A\nJ R Soc Interface\nInt J Bifurcation Chaos\nNaturwissenschaften\nTheScientificWorldJournal\nProg Nat Sci\nSci Amer\nC R Acad Bulg Sci\nS Afr J Sci\n\nN of\npapers\n(a)\n7,058\n2,285\n2,253\n1,996\n1,271\n1,115\n451\n257\n553\n288\n358\n463\n370\n427\n191\n\n% I3\n(b)\n43.29 ++\n16.31 ++\n15.68 ++\n9.33 ++\n2.33 -2.11 -1.78 -1.18 ++\n1.09 -1.01 -0.82 -0.67 -0.37 -0.33 -0.33 --\n\nIF 2009\n(c)\n9.432\n34.480\n29.747\n2.670\n0.782\n0.898\n2.295\n4.241\n0.918\n2.316\n1.658\n0.704\n2.471\n0.204\n0.506\n\n[3]\n[1]\n[2]\n[5]\n[22]\n[20]\n[9]\n[4]\n[17]\n[8]\n[10]\n[24]\n[7]\n[37]\n[28]\n\nTotal\ncitations\n(d)\n\n% I3 (6PR)\n\n178,137\n150,718\n126,230\n14,284\n1,551\n2,239\n3,065\n2,772\n1,204\n1,697\n1,183\n710\n585\n234\n220\n\n(e)\n33.64 [1] ++\n16.46 [2] ++\n15.27 [3] ++\n8.29 [4]++\n3.40 [5] -2.55 [6] -1.46 [7] -0.83 [13] -1.34 [8] -0.75 [14] --0.91 [11]\n-1.07 [9]\n0.87 [12] -0.95 [10] -0.47 [15] --\n\nNote. ++ p < 0.01 above the expectation; -- p < 0.01 below the expectation.\nTable 5 provides the data for the 15 journals with the highest value of I3 among the 48\njournals of this Subject Category in a format similar to that of Table 4 above (for LIS),\nand Table 6 provides the correlation coefficients (as above in Table 3). Although the two\nmeasures of I3 and IF correlate again significantly over the set (Table 6), the order of the\njournals as indicated in Table 5 is very different.\n\nFor example, Current Science and the Chinese Science Bulletin were ranked at the 22nd\nand 20th place in this set with IFs of 0.782 and 0.898, respectively, but are now rated as\nthe fifth and sixth largest impact journals. The scoring in terms of I3(6PR) in the rightmost column of Table 5 shows that this is not only an effect of the large tails in the\ndistribution with infrequently cited papers, but consistent when using this evaluation\nscheme of the six classes which rewards excellence (top-1%, etc.) disproporionally.\n\n24\n\n\fTable 6: Rank-order correlations (Spearman's \u03c1; upper triangle) and Pearson correlations\nr (lower triangle) for the 48 journals of MS.\nIndicator\n\nIF-2009\n\nI3\n\nMean\n100PR\n.884 **\n.777 **\n\nI3(6PR)\n\nMean 6PR\n\nNumber of\npublications\n.479 **\n.829 **\n.364 *\n.996 **\n.605 **\n\n.798 **\n.517 **\n.844 **\nIF-2009\n.590 **\n.854 **\n.837 **\nI3\n.775 **\n.691 **\n.408 **\n.817 **\nMean 100PR\n.660 **\n.987 **\n.706 **\n.646 **\nI3(6PR)\n.956 **\n.716 **\n.875 **\n.775 **\nMean 6PR.\n.492 **\n.953 **\n.605 **\n.967 **\n.635 **\nN of publications\n.841 **\n.922 **\n.756 **\n.945 **\n.884 **\n.839 **\nTotal citations\nNote: ** Correlation is significant at the 0.01 level (2-tailed); * Correlation is significant at the 0.05 level (2tailed).\n\nTotal\ncitations\n.840 **\n.986 **\n.838 **\n.801 **\n.887 **\n.772 **\n\nTable 6 shows that in this case, the differences in size among these 48 journals are so\nimportant that all correlations are much higher. In other words, these correlations are\nspurious since they are caused by differences in size more than in the previous case since\nthere are six major journals and 42 small ones. Despite the higher correlations, the same\neffects as discussed above for the case of LIS can be found. The partial correlation\nbetween I3 and the number of publication controlled for the number of citation rI3, N|TC\n= .850 (p < 0.01), whereas rI3(6PR), N|TC = .982 (p < 0.01) is again higher. As in the\nprevious case, rIF, N|TC is negative (-.724; p < 0.01) because the IF is based on dividing by\nthe N of publications.\n\nBoth I3 and I3(6PR) correlate again significantly with the number of citable publications\nand citations. This follows from the definition of impact by analogy to the product of\n\"mass\" (number of publications) and \"velocity\" (quality of each publication). Both terms\nthus contribute to the impact, but with qualification of the citedness of each publication in\nterms of its percentile rank.\n\n25\n\n\fFigure 8: Forty-eight journals in MS organized according to (dis)similarity in their\nbeing-cited patterns to 24,494 publications in 2007 and 2008.\nNote. Dunn's test for multiple comparisons (\u03b1 < 0.000044 = [0.05 / {(48 * 47)/2}];\nKamada & Kawai (1989) used for the visualization.\n\nDunn's test applied to the citation patterns of these 48 journals are visualized in Figure 8.\nThe figure shows that 45 of the 48 journals form a k=25 core set of journals. Both Science\nand Nature are significantly different in their citation patterns from all other journals in\nthis set, and-perhaps counter-intuitively-from each other. The citation distribution of\nPNAS, however, is not significantly different from a number of other journals in the set.\n\nCurrent Science (Curr Sci) and the Chinese Science Bulletin (Chin Sci Bull), however,\nare positioned to the left within the core-set, in the neighbourhood of the New Scientist\n\n26\n\n\f(New Sci). This latter journal has an IF of only 0.333 and a contribution to the impact in\nterms of I3 of only 0.17% of the total impact of the set (1,035,332.14).\n\nIn summary, these results show that, on the one hand, two journals with similarly high\nIFs such as Science and Nature, can nevertheless differ significantly in their citation\ndistributions. Note that Dunn's test is performed directly on the raw citation scores, that\nis, before the transformation into percentile ranks. On the other hand, a journal with a\nvery high impact such as PNAS may not differ much in its citation pattern from a journal\nlike the Proceedings of the Estonian Academy of Science, although both their respective\nimpacts I3 and impact factors IFs differ by orders of magnitude.\n\nLet us recall that the IF was designed precisely with the objective to correct for these\nsize differences between otherwise similar journals such as PNAS and the Proceedings of\nthe Estonian Academy of Science (Bensman, 2008; Garfield, 1972). It completely fails to\ndo so because of the parametric assumption involved in using an arithmetic average (cf.\nRousseau & Leydesdorff, 2011).\n\nPerformance measurement\n\nBecause percentiles are attributed at the paper level, the datasets enable us to perform\naggregations and decompositions other than in terms of journals or journal sets.\nDocuments and document sets can both be analyzed in terms of disciplinary structures\nand be considered as products of authors, institutions, nations, etc. (Narin, 1976; Small &\n\n27\n\n\fGarfield, 1985). On the one side, one refers to what a journal accepts as worthy of\npublication, whereas, the other refers to how a person, say, performs and communicates\nscientific work. Whereas journals do not produce scientific knowledge the way\npeople/institions do and, of course, are evaluated, I3 is so general that one is enabled to\ncombine the two different types of evaluations (Leydesdorff, 2008). One can, for\nexample, compare the productivity and impact of an institution or nation in two different\njournal categories (and test the difference for its significance.)\n\nLet us as an example recompose the 5,737 citable documents of the LIS set using the\ncountry addresses provided in the bylines of these papers. Fractional counting of the\naddresses will be used in order to keep the total numbers consistent. In other words, if a\npaper is coauthored between two authors from country A and one from country B, the\nattribution is for one-third to country B and two-thirds to country A.\n\nTable 7 provides the results; the table is composed by first using only players in the field\nwith at least a one-percent contribution to I3, and then sorted in column c using the ratio\nof this share divided by the percentage share of publications as the expected distribution\n(column a). (The regression line is in this case less interesting since overdetermined by\nthe outliers for the USA and EU 27.)\nTable 7: Percentages shares of publications (2007 and 2008) and %I3 in the set of 65\njournals of LIS (sorted by the ratio of percentage of I3 / percentage share of publications\nin column c).\n\n28\n\n\fNetherlands\nSwitzerland\nBelgium\nSouth Korea\nTaiwan\nPeoples R China\nItaly\nEU-27\nCanada\nAustralia\nSingapore\nUK\nSweden\nUSA\nFrance\nFinland\nSpain\nGermany\n% accounted\n\nPercentage\npublications\n(a)\n2.23\n0.77\n1.15\n1.45\n2.23\n2.35\n0.83\n24.67\n3.46\n2.19\n1.16\n8.82\n0.96\n40.36\n1.10\n1.09\n4.31\n2.51\n88.72\n\n% I3\n(b)\n3.75\n1.24\n1.81\n1.88\n2.87\n2.97\n1.02\n30.21\n4.08\n2.58\n1.34\n10.18\n1.06\n42.91\n1.16\n1.10\n4.22\n2.38\n97.58\n\nRatio of (b)\nand (a)\n(c)\n1.68 ++\n1.61 ++\n1.57 ++\n1.30 ++\n1.29 ++\n1.26 ++\n1.23 ++\n1.22 ++\n1.18 ++\n1.18 ++\n1.16 ++\n1.15 ++\n1.10\n1.06 ++\n1.05 ++\n1.01\n0.98 0.95 -1.10 ++\n\n% I3(6PR)\n(d)\n3.23\n1.21\n1.56\n1.58\n2.52\n2.63\n0.86\n28.20\n3.73\n2.37\n1.17\n9.49\n1.04\n41.52\n1.13\n1.05\n4.07\n2.58\n92.91\n\nRatio of (d)\nand (a)\n(f)\n1.45 ++\n1.57 ++\n1.36 ++\n1.09 +\n1.13 ++\n1.12 ++\n1.04 ++\n1.14 ++\n1.08 ++\n1.08 +\n1.01 ++\n1.08 ++\n1.08 ++\n1.03 ++\n1.03 ++\n0.96 ++\n0.94 ++\n1.03 ++\n1.05 ++\n\nNote. ++ p < 0.01 above the expectation; -- p < 0.01 below the expectation; + p < 0.05 above\nthe expectation; - p < 0.05 below the expectation.\nOnly 5,090 (88.72%) of the 5,737 records contained (8,510) addresses with country\nnames. These records are cited more often than records without addresses, so that the\n\"world average\" given the set of 65 LIS journals would be 1.10 using I3, or 1.05 in the\ncase of I3(6PR). Because this indicator is based on a summation, the value for the EU-27\nis equal to the sum value of the 27 nations composing the EU. (Similarly, the value for\nthe UK is constructed by adding records with England, Scotland, Wales, and Northern\nIreland as country indicators in the ISI set.)\n\nUsing I3, the Netherlands scores highest with 1.68, but using I3(6PR) Switzerland which\nwas second on the scale of one hundred, is now highest with 1.57. As noted, I3(6PR) is\nsensitive to an above-average representation in the top segments of the percentile\n\n29\n\n\fdistribution. Switzerland is known to be well represented in these segments (e.g., King,\n2004). The USA outperforms all other nations (including a constructed EU-27) in terms\nof absolute numbers on both scales. The low contribution of the PR China in this set is\nnotable. Important countries such as India, Russia, and Brazil are not listed as\ncontributing because of the threshold used of more than a 1% contribution to the total\nimpact.\n\nIn summary, percentile ranks are defined at the level of each individual paper in a set.\nHow the set is composed (for example, in terms of publications in two years in this study,\nbut for the purpose of a comparison with the IFs) can still be decided on the basis of a\nresearch question. For example, one may wish to compare the impact of publications of\nrejected versus granted PIs in a competition (Bornmann et al., 2010; Van den Besselaar\n& Leydesdorff, 2009). In each such study, one can determine percentiles, test citation\ncurves against one another for the statistical signifance of differences (using Dunn's test),\nand test for each subset whether the impact is significantly above of below the\nexpectations (using the Z-test). Our method is thus most general and avoids parametric\nassumptions.\n\nConclusions and discussion\n\nWe elaborated above on the I3 values using the two-year set of citable items in order to\nfacilitate-in this study-the comparison with the IF. However, as shown above, this\nindicator is not restricted to journals, document sets, time-periods, etc., but more general:\n\n30\n\n\fonly the specification of a reference set is required from which the samples under study\nare drawn (Bornmann et al., 2008). Above, we used two ISI Subject Categories as\nreference sets, but one could also use the entire Science Citation Index, Scopus data, data\nfrom Google Scholar, or patent databases that contain citations. One can even apply this\nto a citation count in \"grey literature.\" I3 provides a general measure of citaiton that can\nbe applied across samples of different sizes; the non-parametric statistics account for the\ntypically highly-skewed citation distributions.\n\nOur first point was that impact is not captured correctly using a central tendency statistics\nsuch as the mean or the median. Lundberg (2007, at p. 148) noted that one does not have\nto average the (field-normalized) citation scores, but can also use their sum values as a\n\"total\" field-normalized citation score. Using the Leiden Rankings, the Center for\nScience and Technology Studies (CWTS) multiplied the product of the number of\npublications P with the \"old\" crown indicator CPP/FCSm in order to obtain as a result\nwhat was called the \"brute force indicator.\" In the new set of Leiden indicators,\nanalogously, a \"total normalized citation score\" is proposed (Van Raan et al., 2010b, at p.\n291). However, all these indicators are based on the parametric assumption (of the\nCentral Limit Theorem) that one is allowed to compute with the mean as a summary\nstatistics given a sufficiently large number of observations (e.g., Gl\u00e4nzel, 2010).\n\nAs with the impact factors, citation analysis has hitherto been caught in the paradigm of\nparametric statistics, although this approach is mostly not fruitful for bibliometrics (cf.\nAhlgren et al., 2003). Changing to the median, however, is not sufficient because the\n\n31\n\n\fmedian as a central-tendency measure is as sensitive-and sometimes even more so-to\nthe tails of the distributions. A finer-grained scheme of hundred percentiles can be\nenvisaged. Actually, we used the percentile ranks above as a continuous random variable\nwhich can be specified to any desirable degree of precision in terms of decimal numbers.\nThus defined, the percentile ranks are attributes of the publications which can be added in\norder to perform integration along the qualified citation curve.\n\nAdditionally, we showed that one can vary the evaluation scheme using the six percentile\nranks that are used in the Science and Engineering Indicators (National Science Board,\n2010, Appendix Table 5-43; Bornmann & Mutz, 2011). The emphasis on the morehighly-cited publications in this scheme enhances the distinctions as more significant\n(e.g., in Tables 5 and 7), but one may lose some information such as fine-grained\ndistinctions between units of analysis with tied ranks. I3 for hundred percentiles provides\nthe general scheme from which others can be derived given different policy contexts. As\nnoted, hundred percentiles can be considered as a continuous variable, and one can thus\nprovide the degree of precision in decimals.\n\nIn the meantime, the percentile rank approach is also used by the new InCites database of\nThomson Reuters that functions as an overlay to the Web of Science. Unfortunately, the\npercentile ranks are averaged in this case and one cannot escape from the scheme of ISI\nSubject Categories as the reference sets for determining the percentiles (cf. Pudovkin &\nGarfield, 2002). Using percentile ranks, however, the classification into categories can in\nthe future also be paper-based, such as using the Medical Subject Heading (MeSH) in the\n\n32\n\n\fMedline database of the NIH (Bornmann et al., 2008) or using the keywords of dedicated\ndatabases such as Chemical Abstracts (Bornmann et al., 2011). We expect the state of the\nart to change rapidly in this respect.\n\nOur suggestion to use summations for the impact may raise the question of whether\nimpact per paper was not defined above as rate of summations rather than a summation of\nrates. Last year's debate about normalization was about using \"rates of averages\" versus\n\"averages of rates,\" as Gingras & Larivi\u00e8re (2011) succinctly summarized the crucial\nissue of the controversy. However, percentile ranks are rates, albeit non-parametric ones.\nAs shown above (e.g., in Figure 4), the resulting sums for different units of analysis can\nbe regressed upon the number of publications and thus the impact/paper can be indicated.\nThis impact/paper can be tested for its significance against the distribution of papers\nunder study (using \u03c72-square statistics). The differences in underlying citation\ndistributions can be tested for their significance using, for example, Dunn's test. Using\npercentiles, the evaluation scheme for both the performance of authors and institutes, and\nthe quality of journals can methodologically be brought into a single framework. Since\nthe I3 measure in fully decomposable, multi-dimensional distinctions are also possible.\n\nIn terms of the statistics, our main message is to keep significance in differences among\ncitation distributions analytically separate from impact, which we defined-in analogy to\nthe (vector-)summation of momenta in physics-as summations of products. Thus\ndefined, the percentile rank approach of the Integrated Impact Indicator (I3) enables us to\ntake both the size and the shape of the distribution into account, and impacts among\n\n33\n\n\fdifferent units of analysis (journals, nations, universities, institutions, individuals) can be\ncompared (that is, added and substracted) as percentages. Whether these units of analysis\n(e.g., individuals, research groups, small countries such as Monaco) are large enough for\nthe comparison can be decided on statistical (instead of normative or moral) grounds.\nDifferences which are not significant can be dis-advised for usage in policy making and\nresearch management.\n\nPolicy implications\n\nThe common assumption in citation impact analysis hitherto has been normalization to\nthe mean. In our opinion, the results are then necessarily flawed because the citation\ndistributions are often highly-skewed. Highly productive units can then be disadvantaged\nbecause they publish often in addition to higher-cited papers also a number of less-cited\nones which depress their average performance. We became aware of this when we tried\nto reproduce the performance of seven PIs of the Academic Medical Center of the\nUniversity of Amsterdam whom had been evaluated by CWTS. The first and sixth\nposition among these seven were swapped when we used mean percentile ranks\n(Leydesdorff et al., 2011). Thus, the effect of the proposed change in the paradigm of\nimpact assessment can be highly significant, both in terms of the statistics and policy\nimplications. In this case, for example, the ranking in terms of impact was used as input\nto the funding scheme of these PIs; the research group of the sixth PI thus suffered a loss\nin funding because of this group's productivity other than in the top-1% (personal\ncommunication, November 2010).\n\n34\n\n\fIn this study, we generalized the non-paramatric approach. Long-standing assumptions\nsuch as Nature and Science would have higher citation impact (given higher IFs) when\ncompared with PNAS, were shown above as erroneous consequences of parametric\nassumptions. Current Science and the Chinese Science Bulletin were ranked at the 22nd\nand 20th place in the set of multidisciplinary journals with IFs of 0.782 and 0.898,\nrespectively, but were rated as the fifth and sixth largest impact journals using I3. Thus,\nthis analysis in terms of percentiles shows the increased importance of these two\nmultidisiciplinary journals, while the IFs do not.\n\nNarin's (1976) original scheme of crosstabling journals and nations in evaluative\nbibliometrics can be considered as two dimensions for the aggregation into subsets of\npapers. I3 is additive and therefore allows for the comparison of subsets which may differ\nalong both axes. Actually, one can also compute a contribution of I3 for a set that differs\nin terms of both addresses and journals (or other parameters) given that the reference sets\nfor the percentile ranks are properly set as the relevant total sum sets. Whereas the h\nindex and its derived statistics seem to allow for such comparisons, I3 does not simplify\nthe computation by discarding the bulk of the references in the tails of the distributions.\nNote that papers with zero citations do not contributed to I3, because I3 is an impact\nindicator that takes both publication quantities and (normalized) citation impact into\naccount. Using I3 for the evaluation, one is no longer \"punished\" for one's productivity.\n\n35\n\n\fAcknowledgements\n\nWe are grateful to R\u00fcdiger Mutz for comments on previous drafts.\n\nReferences\n\nAhlgren, P., Jarneving, B., & Rousseau, R. (2003). Requirement for a Cocitation\nSimilarity Measure, with Special Reference to Pearson's Correlation Coefficient.\nJournal of the American Society for Information Science and Technology, 54(6),\n550-560.\nBensman, S. J. (2007). Garfield and the impact factor. Annual Review of Information\nScience and Technology, 41(1), 93-155.\nBensman, S. J., & Wilder, S. J. (1998). Scientific and Technical Serials Holdings\nOptimization in an Inefficient Market: A LSU Serials Redesign Project Exercise.\nLibrary Resources and Technical Services, 42(3), 147-242.\nBornmann, L. (2010). Towards an ideal method of measuring research performance:\nsome comments to the Opthof and Leydesdorff (2010) paper. Journal of\nInformetrics, 4(3), 441-443\nBornmann, L., & Mutz, R. (2011). Further steps towards an ideal method of measuring\ncitation performance: The avoidance of citation (ratio) averages in fieldnormalization. Journal of Informetrics, 5(1), 228-230.\nBornmann, L., Mutz, R., Neuhaus, C., & Daniel, H. D. (2008). Citation counts for\nresearch evaluation: standards of good practice for analyzing bibliometric data\nand presenting and interpreting results. Ethics in Science and Environmental\nPolitics(ESEP), 8(1), 93-102.\nBornmann, L., Leydesdorff, L., & Van den Besselaar, P. (2010). A Meta-evaluation of\nScientific Research Proposals: Different Ways of Comparing Rejected to\nAwarded Applications. Journal of Informetrics, 4(3), 211-220.\nBornmann, L., Schier, H., Marx, W., & Daniel, H.-D. (2011). Is Interactive Open Access\nPublishing Able to Identify High-Impact Submissions? A Study on the Predictive\nValidity of Atmospheric Chemistry and Physics by Using Percentile Rank Classes,\nJournal of the American Society for Information Science and Technology, 62(1),\n61-71.\nGarfield, E. (1972). Citation Analysis as a Tool in Journal Evaluation. Science\n178(Number 4060), 471-479.\nGarfield, E. (1979). Citation Indexing: Its Theory and Application in Science, Technology,\nand Humanities. New York: John Wiley.\nGarfield, E., & Sher, I. H. (1963). New factors in the evaluation of scientific literature\nthrough citation indexing. American Documentation, 14, 195-201.\nGingras, Y., & Larivi\u00e8re, V. (2011). There are neither \"king\" nor \"crown\" in\nscientometrics: Comments on a supposed \"alternative\" method of normalization.\nJournal of Informetrics, 5(1), 226-227.\n\n36\n\n\fGl\u00e4nzel, W. (2010). On reliability and robustness of scientometrics indicators based on\nstochastic models. An evidence-based opinion paper. Journal of Informetrics, 4(3),\n313-319.\nHirsch, J. E. (2005). An index to quantify an individual's scientific research output.\nProceedings of the National Academy of Sciences of the USA, 102(46), 1656916572.\nKing, D. A. (2004). The scientific impact of nations. Nature, 430(15 July 2004), 311-316.\nLevine, G. (1991). A Guide to SPSS for Analysis of Variance. Hillsdale, NJ: Lawrence\nErlbaum.\nLeydesdorff, L. (2008). Caveats for the Use of Citation Indicators in Research and\nJournal Evaluation. Journal of the American Society for Information Science and\nTechnology, 59(2), 278-287.\nLeydesdorff, L. (2009). How are New Citation-Based Journal Indicators Adding to the\nBibliometric Toolbox? Journal of the American Society for Information Science\nand Technology, 60(7), 1327-1336.\nLeydesdorff, L., Bornmann, L., Mutz, R., & Opthof, T. (in press). Turning the tables in\ncitation analysis one more time: Principles for comparing sets of documents\nJournal of the American Society for Information Science and Technology,\nDOI: 10.1002/asi.21534.\nLeydesdorff, L., & Opthof, T. (2011). Remaining problems with the \"New Crown\nIndicator\" (MNCS) of the CWTS. Journal of Informetrics, 5(1), 224-225.\nLundberg, J. (2007). Lifting the crown - citation z-score. Journal of Informetrics, 1(2),\n145-154.\nMoed, H. F., & Van Leeuwen, T. N. (1996). Impact factors can mislead. Nature,\n381(6579), 186.\nNarin, F. (1976). Evaluative Bibliometrics: The Use of Publication and Citation Analysis\nin the Evaluation of Scientific Activity. Washington, DC: National Science\nFoundation.\nNational Science Board. (2010). Science and Engineering Indicators. Washington DC:\nNational Science Foundation; available at http://www.nsf.gov/statistics/seind10/.\nNisonger, T. E. (1999). JASIS and library and information science journal rankings: A\nreview and analysis of the last half century. Journal of the American Society for\nInformation Science, 50(11), 1004-1019.\nNisonger, T. E., & Davis, C. H. (2005). The Perception of Library and Information\nScience Journals by LIS Education Deans and ARL Library Directors: A\nReplication of the Kohl-Davis Study. College and Research Libraries, 66(4), 341377.\nOpthof, T., & Leydesdorff, L. (2010). Caveats for the journal and field normalizations in\nthe CWTS (\"Leiden\") evaluations of research performance. Journal of\nInformetrics, 4(3), 423-430.\nPudovkin, A. I., & Garfield, E. (2002). Algorithmic procedure for finding semantically\nrelated journals. Journal of the American Society for Information Science and\nTechnology, 53(13), 1113-1119.\nPudovkin, A. I., & Garfield, E. (2009). Percentile Rank and Author Superiority Indexes\nfor Evaluating Individual Journal Articles and the Author's Overall Citation\n\n37\n\n\fPerformance. CollNet Journal of Scientometrics and Information Management,\n3(2), 3-10.\nRousseau, R., & Leydesdorff, L. (2011). Simple Arithmetic versus Intuitive\nUnderstanding: The Case of the Impact Factor, ISSI Newsletter, 7(1), 10-14.\nSmall, H., & Garfield, E. (1985). The geography of science: disciplinary and national\nmappings. Journal of information science, 11(4), 147-159.\nVan den Besselaar, P., & Leydesdorff, L. (2009). Past performance, peer review, and\nproject selection: A case study in the social and behavioral sciences. Research\nEvaluation, 18(4), 273-288.\nVan Raan, A. F. J., van Leeuwen, T. N., Visser, M. S., van Eck, N. J., & Waltman, L.\n(2010a). Rivals for the crown: Reply to Opthof and Leydesdorff. Journal of\nInformetrics, 4(3), 431-435.\nVan Raan, A. F. J., Eck, N. J. van, Leeuwen, T. N. van, Visser, M. S., & Waltman, L.\n(2010b). The new set of bibliometric indicators of CWTS. Paper presented at the\n11th International Conference on Science and Technology Indicators, Leiden,\nSeptember 9-11, 2010; pp. 291-293.\nZhao, D., & Strotmann, A. (2008). Evolution of research activities and intellectual\ninfluences in information science 1996-2005: Introducing author bibliographiccoupling analysis. Journal of the American Society for Information Science and\nTechnology, 59(13), 2070-2086.\n\n38\n\n\f"}