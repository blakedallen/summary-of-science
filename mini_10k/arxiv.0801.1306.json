{"id": "http://arxiv.org/abs/0801.1306v1", "guidislink": true, "updated": "2008-01-08T19:56:00Z", "updated_parsed": [2008, 1, 8, 19, 56, 0, 1, 8, 0], "published": "2008-01-08T19:56:00Z", "published_parsed": [2008, 1, 8, 19, 56, 0, 1, 8, 0], "title": "Capacity Bounds for the Gaussian Interference Channel", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0801.2490%2C0801.3666%2C0801.0669%2C0801.4470%2C0801.4770%2C0801.4300%2C0801.4420%2C0801.3464%2C0801.1623%2C0801.0036%2C0801.0417%2C0801.4837%2C0801.4673%2C0801.4675%2C0801.2673%2C0801.4016%2C0801.2840%2C0801.2502%2C0801.3804%2C0801.0123%2C0801.0513%2C0801.0111%2C0801.1105%2C0801.4381%2C0801.2043%2C0801.1854%2C0801.2343%2C0801.2389%2C0801.3851%2C0801.2789%2C0801.1590%2C0801.3185%2C0801.2008%2C0801.2127%2C0801.0688%2C0801.1748%2C0801.0290%2C0801.2355%2C0801.0019%2C0801.1937%2C0801.3226%2C0801.3096%2C0801.1306%2C0801.4670%2C0801.4177%2C0801.2979%2C0801.2608%2C0801.1698%2C0801.4824%2C0801.4490%2C0801.3752%2C0801.4438%2C0801.2465%2C0801.3498%2C0801.1047%2C0801.1742%2C0801.1161%2C0801.4594%2C0801.2388%2C0801.0847%2C0801.3606%2C0801.4884%2C0801.3655%2C0801.0788%2C0801.0042%2C0801.0031%2C0801.4068%2C0801.2836%2C0801.0130%2C0801.4104%2C0801.4915%2C0801.4117%2C0801.4399%2C0801.4287%2C0801.3598%2C0801.0921%2C0801.1117%2C0801.3595%2C0801.4784%2C0801.0476%2C0801.0472%2C0801.2029%2C0801.2078%2C0801.0048%2C0801.1606%2C0801.4603%2C0801.3984%2C0801.2945%2C0801.4249%2C0801.1456%2C0801.3166%2C0801.2036%2C0801.0592%2C0801.3503%2C0801.0940%2C0801.0914%2C0801.2933%2C0801.1062%2C0801.1756%2C0801.4279%2C0801.2926&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Capacity Bounds for the Gaussian Interference Channel"}, "summary": "The capacity region of the two-user Gaussian Interference Channel (IC) is\nstudied. Three classes of channels are considered: weak, one-sided, and mixed\nGaussian IC. For the weak Gaussian IC, a new outer bound on the capacity region\nis obtained that outperforms previously known outer bounds. The sum capacity\nfor a certain range of channel parameters is derived. For this range, it is\nproved that using Gaussian codebooks and treating interference as noise is\noptimal. It is shown that when Gaussian codebooks are used, the full\nHan-Kobayashi achievable rate region can be obtained by using the naive\nHan-Kobayashi achievable scheme over three frequency bands (equivalently, three\nsubspaces). For the one-sided Gaussian IC, an alternative proof for the Sato's\nouter bound is presented. We derive the full Han-Kobayashi achievable rate\nregion when Gaussian codebooks are utilized. For the mixed Gaussian IC, a new\nouter bound is obtained that outperforms previously known outer bounds. For\nthis case, the sum capacity for the entire range of channel parameters is\nderived. It is proved that the full Han-Kobayashi achievable rate region using\nGaussian codebooks is equivalent to that of the one-sided Gaussian IC for a\nparticular range of channel parameters.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0801.2490%2C0801.3666%2C0801.0669%2C0801.4470%2C0801.4770%2C0801.4300%2C0801.4420%2C0801.3464%2C0801.1623%2C0801.0036%2C0801.0417%2C0801.4837%2C0801.4673%2C0801.4675%2C0801.2673%2C0801.4016%2C0801.2840%2C0801.2502%2C0801.3804%2C0801.0123%2C0801.0513%2C0801.0111%2C0801.1105%2C0801.4381%2C0801.2043%2C0801.1854%2C0801.2343%2C0801.2389%2C0801.3851%2C0801.2789%2C0801.1590%2C0801.3185%2C0801.2008%2C0801.2127%2C0801.0688%2C0801.1748%2C0801.0290%2C0801.2355%2C0801.0019%2C0801.1937%2C0801.3226%2C0801.3096%2C0801.1306%2C0801.4670%2C0801.4177%2C0801.2979%2C0801.2608%2C0801.1698%2C0801.4824%2C0801.4490%2C0801.3752%2C0801.4438%2C0801.2465%2C0801.3498%2C0801.1047%2C0801.1742%2C0801.1161%2C0801.4594%2C0801.2388%2C0801.0847%2C0801.3606%2C0801.4884%2C0801.3655%2C0801.0788%2C0801.0042%2C0801.0031%2C0801.4068%2C0801.2836%2C0801.0130%2C0801.4104%2C0801.4915%2C0801.4117%2C0801.4399%2C0801.4287%2C0801.3598%2C0801.0921%2C0801.1117%2C0801.3595%2C0801.4784%2C0801.0476%2C0801.0472%2C0801.2029%2C0801.2078%2C0801.0048%2C0801.1606%2C0801.4603%2C0801.3984%2C0801.2945%2C0801.4249%2C0801.1456%2C0801.3166%2C0801.2036%2C0801.0592%2C0801.3503%2C0801.0940%2C0801.0914%2C0801.2933%2C0801.1062%2C0801.1756%2C0801.4279%2C0801.2926&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The capacity region of the two-user Gaussian Interference Channel (IC) is\nstudied. Three classes of channels are considered: weak, one-sided, and mixed\nGaussian IC. For the weak Gaussian IC, a new outer bound on the capacity region\nis obtained that outperforms previously known outer bounds. The sum capacity\nfor a certain range of channel parameters is derived. For this range, it is\nproved that using Gaussian codebooks and treating interference as noise is\noptimal. It is shown that when Gaussian codebooks are used, the full\nHan-Kobayashi achievable rate region can be obtained by using the naive\nHan-Kobayashi achievable scheme over three frequency bands (equivalently, three\nsubspaces). For the one-sided Gaussian IC, an alternative proof for the Sato's\nouter bound is presented. We derive the full Han-Kobayashi achievable rate\nregion when Gaussian codebooks are utilized. For the mixed Gaussian IC, a new\nouter bound is obtained that outperforms previously known outer bounds. For\nthis case, the sum capacity for the entire range of channel parameters is\nderived. It is proved that the full Han-Kobayashi achievable rate region using\nGaussian codebooks is equivalent to that of the one-sided Gaussian IC for a\nparticular range of channel parameters."}, "authors": ["Abolfazl S. Motahari", "Amir K. Khandani"], "author_detail": {"name": "Amir K. Khandani"}, "author": "Amir K. Khandani", "arxiv_comment": "35 pages, 14 figures, submitted to IEEE Trans. on Inf. Theory", "links": [{"href": "http://arxiv.org/abs/0801.1306v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0801.1306v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0801.1306v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0801.1306v1", "journal_reference": null, "doi": null, "fulltext": "1\n\nCapacity Bounds for\nthe Gaussian Interference Channel\nAbolfazl S. Motahari, Student Member, IEEE, and Amir K. Khandani, Member, IEEE\nCoding & Signal Transmission Laboratory (www.cst.uwaterloo.ca)\n{abolfazl,khandani}@cst.uwaterloo.ca\n\narXiv:0801.1306v1 [cs.IT] 8 Jan 2008\n\nAbstract\nThe capacity region of the two-user Gaussian Interference Channel (IC) is studied. Three classes of channels are considered:\nweak, one-sided, and mixed Gaussian IC. For the weak Gaussian IC, a new outer bound on the capacity region is obtained that\noutperforms previously known outer bounds. The sum capacity for a certain range of channel parameters is derived. For this\nrange, it is proved that using Gaussian codebooks and treating interference as noise is optimal. It is shown that when Gaussian\ncodebooks are used, the full Han-Kobayashi achievable rate region can be obtained by using the naive Han-Kobayashi achievable\nscheme over three frequency bands (equivalently, three subspaces). For the one-sided Gaussian IC, an alternative proof for the\nSato's outer bound is presented. We derive the full Han-Kobayashi achievable rate region when Gaussian codebooks are utilized.\nFor the mixed Gaussian IC, a new outer bound is obtained that outperforms previously known outer bounds. For this case, the\nsum capacity for the entire range of channel parameters is derived. It is proved that the full Han-Kobayashi achievable rate region\nusing Gaussian codebooks is equivalent to that of the one-sided Gaussian IC for a particular range of channel parameters.\nIndex Terms\nGaussian interference channels, capacity region, sum capacity, convex regions.\n\nI. I NTRODUCTION\nNE of the fundamental problems in Information Theory, originating from [1], is the full characterization of the capacity\nregion of the interference channel (IC). The simplest form of IC is the two-user case in which two transmitters aim to\nconvey independent messages to their corresponding receivers through a common channel. Despite some special cases, such\nas very strong and strong interference, where the exact capacity region has been derived [2], [3], the characterization of the\ncapacity region for the general case is still an open problem.\nA limiting expression for the capacity region is obtained in [4] (see also [5]). Unfortunately, due to excessive computational\ncomplexity, this type of expression does not result in a tractable approach to fully characterize the capacity region. To show\nthe weakness of the limiting expression, Cheng and Verd\u00fa have shown that for the Gaussian Multiple Access Channel (MAC),\nwhich can be considered as a special case of the Gaussian IC, the limiting expression fails to fully characterize the capacity\nregion by relying only on Gaussian distributions [6]. However, there is a point on the boundary of the capacity region of\nthe MAC that can be obtained directly from the limiting expression. This point is achievable by using simple scheme of\nFrequency/Time Division (FD/TD).\nThe computational complexity inherent to the limiting expression is due to the fact that the corresponding encoding and\ndecoding strategies are of the simplest possible form. The encoding strategy is based on mapping data to a codebook constructed\nfrom a unique probability density and the decoding strategy is to treat the interference as noise. In contrast, using more\nsophisticated encoders and decoders may result in collapsing the limiting expression into a single letter formula for the\ncapacity region. As an evidence, it is known that the joint typical decoder for the MAC achieves the capacity region [7].\nMoreover, there are some special cases, such as strong IC, where the exact characterization of the capacity region has been\nderived [2], [3] where decoding the interference is the key idea behind this result.\nIn their pioneering work, Han and Kobayashi (HK) proposed a coding strategy in which the receivers are allowed to decode\npart of the interference as well as their own data [8]. The HK achievable region is still the best inner bound for the capacity\nregion. Specifically, in their scheme, the message of each user is split into two independent parts: the common part and the\nprivate part. The common part is encoded such that both users can decode it. The private part, on the other hand, can be\ndecoded only by the intended receiver and the other receiver treats it as noise. In summary, the HK achievable region is the\nintersection of the capacity regions of two three-user MACs, projected on a two-dimensional subspace.\nThe HK scheme can be directly applied to the Gaussian IC. Nonetheless, there are two sources of difficulties in characterizing\nthe full HK achievable rate region. First, the optimal distributions are unknown. Second, even if we confine the distributions to\nbe Gaussian, computation of the full HK region under Gaussian distribution is still difficult due to numerous degrees of freedom\ninvolved in the problem. The main reason behind this complexity is the computation of the cardinality of the time-sharing\nparameter.\n\nO\n\n1 An earlier version of this work containing all the results is reported in Library and Archives Canada Technical Report UW-ECE 2007-26, Aug.\n2007 (see http://www.cst.uwaterloo.ca/pub tech rep.html for details).\n\n\f2\n\nRecently, reference [9], Chong et al. has presented a simpler expression with less inequalities for the HK achievable region.\nSince the cardinality of the time-sharing parameter is directly related to the number of inequalities appearing in the achievable\nrate region, the computational complexity is decreased. However, finding the full HK achievable region is still prohibitively\ncomplex.\nRegarding outer bounds on the capacity region, there are three main results known. The first one obtained by Sato [10]\nis originally derived for the degraded Gaussian IC. Sato has shown that the capacity region of the degraded Gaussian IC is\nouter bounded by a certain degraded broadcast channel whose capacity region is fully characterized. In [11], Costa has proved\nthat the capacity region of the degraded Gaussian broadcast channel is equivalent to that of the one-sided weak Gaussian IC.\nHence, Sato outer bound can be used for the one-sided Gaussian IC as well.\nThe second outer bound obtained for the weak Gaussian IC is due to Kramer [12]. Kramer outer bound is based on the\nfact that removing one of the interfering links enlarges the capacity region. Therefore, the capacity region of the two-user\nGaussian IC is inside the intersection of the capacity regions of the underlying one-sided Gaussian ICs. For the case of weak\nGaussian IC, the underlying one-sided IC is weak, for which the capacity region is unknown. However, Kramer has used the\nouter bound obtained by Sato to derive an outer bound for the weak Gaussian IC.\nThe third outer bound due to Etkin, Tse, and Wang (ETW) is based on the Genie aided technique [13]. A genie that provides\nsome extra information to the receivers can only enlarge the capacity region. At first glance, it seems a clever genie must\nprovide some information about the interference to the receiver to help in decoding the signal by removing the interference. In\ncontrast, the genie in the ETW scheme provides information about the intended signal to the receiver. Remarkably, reference\n[13] shows that their proposed outer bound outperforms Kramer bound for certain range of parameters. Moreover, using a\nsimilar method, [13] presents an outer bound for the mixed Gaussian IC.\nIn this paper, by introducing the notion of admissible ICs, we propose a new outer bounding technique for the two-user\nGaussian IC. The proposed technique relies on an extremal inequality recently proved by Liu and Viswanath [14]. We show\nthat by using this scheme, one can obtain tighter outer bounds for both weak and mixed Gaussian ICs. More importantly, the\nsum capacity of the Gaussian weak IC for a certain range of the channel parameters is derived.\nThe rest of this paper is organized as follows. In Section II, we present some basic definitions and review the HK achievable\nregion when Gaussian codebooks are used. We study the time-sharing and the convexification methods as means to enlarge the\nbasic HK achievable region. We investigate conditions for which the two regions obtained from time-sharing and concavification\ncoincide. Finally, we consider an optimization problem based on extremal inequality and compute its optimal solution.\nIn Section III, the notion of an admissible IC is introduced. Some classes of admissible ICs for the two-user Gaussian case is\nstudied and outer bounds on the capacity regions of these classes are computed. We also obtain the sum capacity of a specific\nclass of admissible IC where it is shown that using Gaussian codebooks and treating interference as noise is optimal.\nIn Section IV, we study the capacity region of the weak Gaussian IC. We first derive the sum capacity of this channel for\na certain range of parameters where it is proved that users should treat the interference as noise and transmit at their highest\npossible rates. We then derive an outer bound on the capacity region which outperforms the known results. We finally prove\nthat the basic HK achievable region results in the same enlarged region by using either time-sharing or concavification. This\nreduces the complexity of the characterization of the full HK achievable region when Gaussian codebooks are used.\nIn Section V, we study the capacity region of the one-sided Gaussian IC. We present a new proof for the Sato outer bound\nusing the extremal inequality. Then, we present methods to simplify the HK achievable region such that the full region can be\ncharacterized.\nIn Section VI, we study the capacity region of the mixed Gaussian IC. We first obtain the sum capacity of this channel\nand then derive an outer bound which outperforms other known results. Finally, by investigating the HK achievable region for\ndifferent cases, we prove that for a certain range of channel parameters, the full HK achievable rate region using Gaussian\ncodebooks is equivalent to that of the one-sided IC. Finally, in Section VII, we conclude the paper.\nA. Notations\nThroughout this paper, we use the following notations. Vectors are represented by bold faced letters. Random variables,\nmatrices, and sets are denoted by capital letters where the difference is clear from the context. |A|, tr{A}, and At represent\nthe determinant, trace, and transpose of the square matrix A, respectively. I denotes the identity matrix. N and R are the sets\nof nonnegative integers and real numbers, respectively. The union, intersection, and Minkowski sum of two sets U and V are\nrepresented by U \u222a V , U \u2229 V , and U + V , respectively. We use \u03b3(x) as an abbreviation for the function 0.5 log2 (1 + x).\nII. P RELIMINARIES\nA. The Two-user Interference Channel\nDefinition 1 (two-user IC): A two-user discrete memoryless IC consists of two finite sets X1 and X2 as input alphabets\nand two finite sets Y1 and Y2 as the corresponding output alphabets. The channel is governed by conditional probability\ndistributions \u03c9(y1 , y2 |x1 , x2 ) where (x1 , x2 ) \u2208 X1 \u00d7 X2 and (y1 , y2 ) \u2208 Y1 \u00d7 Y2 .\n\n\f3\n\nSymmetric\nP1 = P2\n\n11111111\n00000000\n00000000\n11111111\n00000000\n11111111\n00000000\n11111111\n00000000\n11111111\n00000000\n11111111\n00000000\n11111111\n0000\n1 1111\n0000\n1111\n0000\n1111\nab = 1\n0000\n1111\na\n00001\n1111\n\nOne\u2212sided\n\nb\n\nStrong\n\nMixed\n\nDegraded\n\nWeak\n\nMixed\n\nOne\u2212sided\n\nFig. 1.\n\nClasses of the two-user ICs.\n\nDefinition 2 (capacity region of the two-user IC): A code (2nR1 , 2nR2 , n, \u03bbn1 , \u03bbn2 ) for the two-user IC consists of the following components for User i \u2208 {1, 2}:\n1) A uniform distributed message set Mi \u2208 [1, 2, ..., 2nRi ].\n2) A codebook Xi = {xi (1), xi (2), ..., xi (2nRi )} where xi (*) \u2208 Xin .\n3) An encoding function Fi : [1, 2, ..., 2nRi ] \u2192 Xi .\n4) A decoding function Gi : yi \u2192 [1, 2, ..., 2nRi ].\n5) The average probability of error \u03bbni = P(Gi (yi ) 6= Mi ).\nA rate pair (R1 , R2 ) is achievable if there is a sequence of codes (2nR1 , 2nR2 , n, \u03bbn1 , \u03bbn2 ) with vanishing average error\nprobabilities. The capacity region of the IC is defined to be the supremum of the set of achievable rates.\nLet CIC denote the capacity region of the two-user IC. The limiting expression for CIC can be stated as [5]\n\uf8f6\n\uf8eb\n\u001b\n\u001a\nn\nn\n1\n[\nI\n(X\n,\nY\n)\nR\n\u2264\n1\n1\n1\nn\n\uf8f8.\n(1)\n(R1 , R2 ) |\nCIC = lim closure \uf8ed\nR2 \u2264 n1 I (Xn2 , Yn2 )\nn\u2192\u221e\nn\nn\nP(X1 )P(X2 )\n\nIn this paper, we focus on the two-user Gaussian IC which can be represented in standard form as [15], [16]\n\u221a\ny1 = \u221a\nx1 + ax2 + z1 ,\ny2 = bx1 + x2 + z2 ,\n\n(2)\n\nwhere xi and yi denote the input and output alphabets of User i \u2208 {1, 2}, respectively, and z1 \u223c N (0, 1), z2 \u223c N (0, 1) are\nstandard Gaussian random variables. Constants a \u2265 0 and b \u2265 0 represent the gains of the interference links. Furthermore,\nTransmitter i, i \u2208 {1, 2}, is subject to the power constraint Pi . Achievable rates and the capacity region of the Gaussian IC can\nbe defined in a similar fashion as that of the general IC with the condition that the codewords must satisfy their corresponding\npower constraints. The capacity region of the two-user Gaussian IC is denoted by C . Clearly, C is a function of the parameters\nP1 , P2 , a, and b. To emphasize this relationship, we may write C as C (P1 , P2 , a, b) as needed.\nRemark 1: Since the capacity region of the general IC depends only on the marginal distributions [16], the ICs can be\nclassified into equivalent classes in which channels within a class have the same capacity region. In particular, for the Gaussian\nIC given in (2), any choice of joint distributions for the pair (z1 , z2 ) does not affect the capacity region as long as the marginal\ndistributions remain Gaussian with zero mean and unit variance.\nDepending on the values of a and b, the two-user Gaussian IC is classified into weak, strong, mixed, one-sided, and\ndegraded Gaussian IC. In Figure 1, regions in ab-plane together with their associated names are shown. Briefly, if 0 < a < 1\nand 0 < b < 1, then the channel is called weak Gaussian IC. If 1 \u2264 a and 1 \u2264 b, then the channel is called strong Gaussian\nIC. If either a = 0 or b = 0, the channel is called one-sided Gaussian IC. If ab = 1, then the channel is called degraded\nGaussian IC. If either 0 < a < 1 and 1 \u2264 b, or 0 < b < 1 and 1 \u2264 a, then the channel is called mixed Gaussian IC. Finally,\nthe symmetric Gaussian IC (used throughout the paper for illustration purposes) corresponds to a = b and P1 = P2 .\nAmong all classes shown in Figure 1, the capacity region of the strong Gaussian IC is fully characterized [3], [2]. In this\ncase, the capacity region can be stated as the collection of all rate pairs (R1 , R2 ) satisfying\nR1\nR2\nR1 + R2\n\n\u2264 \u03b3(P1 ),\n\u2264 \u03b3(P2 ),\n\u2264 min {\u03b3(P1 + aP2 ), \u03b3(bP1 + P2 )} .\n\n\f4\n\nB. Support Functions\nThroughout this paper, we use the following facts from convex analysis. There is a one to one correspondence between any\nclosed convex set and its support function [17]. The support function of any set D \u2208 Rm is a function \u03c3D : Rm \u2192 R defined\nas\n\u03c3D (c) = sup{ct R|R \u2208 D}.\n(3)\nClearly, if the set D is compact, then the sup is attained and can be replaced by max. In this case, the solutions of (3)\ncorrespond to the boundary points of D [17]. The following relation is the dual of (3) and holds when D is closed and convex\nD = {R|ct R \u2264 \u03c3D (c), \u2200 c}.\n\n(4)\n\nFor any two closed convex sets D and D\u2032 , D \u2286 D\u2032 , if and only if \u03c3D \u2264 \u03c3D\u2032 .\nC. Han-Kobayashi Achievable Region\nThe best inner bound for the two-user Gaussian IC is the full HK achievable region denoted by CHK [8]. Despite having\na single letter formula, CHK is not fully characterized yet. In fact, finding the optimum distributions achieving boundary\npoints of CHK is still an open problem. We define G as a subset of CHK where Gaussian distributions are used for codebook\ngeneration. Using a shorter description of CHK obtained in [9], G can be described as follows.\nLet us first define G0 as the collection of all rate pairs (R1 , R2 ) \u2208 R2+ satisfying\n\u0012\n\u0013\nP1\nR1 \u2264 \u03c81 = \u03b3\n,\n(5)\n1 + a\u03b2P2\n\u0013\n\u0012\nP2\n,\n(6)\nR2 \u2264 \u03c82 = \u03b3\n1 + b\u03b1P1\nR1 + R2 \u2264 \u03c83 = min {\u03c831 , \u03c832 , \u03c833 } ,\n(7)\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\nP1 + a(1 \u2212 \u03b2)P2\n\u03b1P1\n\u03b2P2 + b(1 \u2212 \u03b1)P1\n2R1 + R2 \u2264 \u03c84 = \u03b3\n+\u03b3\n+\u03b3\n,\n(8)\n1 + a\u03b2P2\n1 + a\u03b2P2\n1 + b\u03b1P1\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\nP2 + b(1 \u2212 \u03b1)P1\n\u03b1P1 + a(1 \u2212 \u03b2)P2\n\u03b2P2\n+\u03b3\n+\u03b3\n,\n(9)\nR1 + 2R2 \u2264 \u03c85 = \u03b3\n1 + b\u03b1P1\n1 + b\u03b1P1\n1 + a\u03b2P2\nfor fixed \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0, 1].1 \u03c83 is the minimum of \u03c831 , \u03c832 , and \u03c833 defined as\n\u0012\n\u0013\n\u0012\n\u0013\nP1 + a(1 \u2212 \u03b2)P2\n\u03b2P2\n\u03c831 = \u03b3\n+\u03b3\n,\n1 + a\u03b2P2\n1 + b\u03b1P1\n\u0013\n\u0012\n\u0013\n\u0012\nP2 + b(1 \u2212 \u03b1)P1\n\u03b1P1\n+\u03b3\n,\n\u03c832 = \u03b3\n1 + a\u03b2P2\n1 + b\u03b1P1\n\u0013\n\u0012\n\u0013\n\u0012\n\u03b2P2 + b(1 \u2212 \u03b1)P1\n\u03b1P1 + a(1 \u2212 \u03b2)P2\n+\u03b3\n.\n\u03c833 = \u03b3\n1 + a\u03b2P2\n1 + b\u03b1P1\n\n(10)\n(11)\n(12)\n\nG0 is a polytope and a function of four variables P1 , P2 , \u03b1, and \u03b2. To emphasize this relation, we may write G0 (P1 , P2 , \u03b1, \u03b2)\nas needed. It is convenient to represent G0 in a matrix form as G0 = {R|AR \u2264 \u03a8(P1 , P2 , \u03b1, \u03b2)} where R = (R1 , R2 )t ,\n\u03a8 = (\u03c81 , \u03c82 , \u03c83 , \u03c84 , \u03c85 )t , and\n\u0012\n\u0013t\n1 0 1 2 1\nA=\n.\n0 1 1 1 2\nEquivalently, G0 can be represented as the convex hull of its extreme points, i.e., G0 (P1 , P2 , \u03b1, \u03b2) = conv {r1 , r2 , . . . , rK },\nwhere it is assumed that G0 has K extreme points. It is easy to show that K \u2264 7.\nNow, G can be defined as a region obtained from enlarging G0 by making use of the time-sharing parameter, i.e., G is the\ncollection of all rate pairs R = (R1 , R2 )t satisfying\nAR\u2264\n\nq\nX\n\n\u03bbi \u03a8(P1i , P2i , \u03b1i , \u03b2i ),\n\n(13)\n\ni=1\n\n1 In the HK scheme, two independent messages are encoded at each transmitter, namely the common message and the private message. \u03b1 and \u03b2 are the\nparameters that determine the amount of power allocated to the common and private messages for the two users, i.e., \u03b1P1 , \u03b2P2 and (1 \u2212 \u03b1)P1 , (1 \u2212 \u03b2)P2\nof the total power is used for the transmission of the private/common messages to the first/second users, respectively.\n\n\f5\n\nwhere q \u2208 N and\n\nq\nX\n\ni=1\nq\nX\ni=1\n\n\u03bbi P1i \u2264 P1 ,\n\n(14)\n\n\u03bbi P2i \u2264 P2 ,\n\n(15)\n\nq\nX\n\n\u03bbi = 1,\n\n(16)\n\ni=1\n\n\u03bbi \u2265 0, (\u03b1i , \u03b2i )\u2208 [0, 1]2 ; \u2200i \u2208 {1, . . . , q}.\n\n(17)\n\nIt is easy to show that G is a closed, bounded and convex region. In fact, the capacity region C which contains G is inside\nthe rectangle defined by inequalities R1 \u2264 \u03b3(P1 ) and R2 \u2264 \u03b3(P2 ). Moreover, (0, 0), (\u03b3(P1 ), 0), and (0, \u03b3(P2 )) are extreme\npoints of both C and G . Hence, to characterize G , we need to obtain all extreme points of G that are in the interior of the\nfirst quadrant (the same argument holds for C ). In other words, we need to obtain \u03c3G (c1 , c2 ), the support function of G , either\nwhen 1 \u2264 c1 and c2 = 1 or when c1 = 1 and 1 \u2264 c2 .\nWe also define G1 and G2 obtained by enlarging G0 in two different manners. G1 is defined as\n[\nG0 (P1 , P2 , \u03b1, \u03b2).\n(18)\nG1 (P1 , P2 ) =\n(\u03b1,\u03b2)\u2208[0,1]2\n\nG1 is not necessarily a convex region. Hence, it can be further enlarged by the convex hull operation. G2 is defined as the\ncollection of all rate pairs R = (R1 , R2 )t satisfying\n\u2032\n\nR=\n\nq\nX\n\n\u03bbi Ri\n\n(19)\n\ni=1\n\nwhere q \u2032 \u2208 N and\nq\u2032\nX\ni=1\n\nARi \u2264 \u03a8(P1i , P2i , \u03b1i , \u03b2i ),\n\n(20)\n\n\u03bbi P1i \u2264 P1 ,\n\n(21)\n\n\u03bbi P2i \u2264 P2 ,\n\n(22)\n\n\u2032\n\nq\nX\ni=1\n\n\u2032\n\nq\nX\n\n\u03bbi = 1,\n\n(23)\n\ni=1\n\n\u03bbi \u2265 0, (\u03b1i , \u03b2i )\u2208 [0, 1]2 ; \u2200i \u2208 {1, . . . , q \u2032 }.\n\n(24)\n\nIt is easy to show that G2 is a closed, bounded and convex region. In fact, G2 is obtained by using the simple method of TD/FD.\nTo see\nthis, let us divide the available frequency band into q \u2032 sub-bands where \u03bbi represents the length of the i'th band and\nPq \u2032\nall rate pairs in G0 (P1i , P2i , \u03b1i , \u03b2i )\ni=1 \u03bbi = 1. User 1 and 2 allocate P1i and P2i in the i'th sub-band, respectively. Therefore,\nPq \u2032\nare achievable in the i'th sub-band for fixed (\u03b1i , \u03b2i ) \u2208 [0, 1]2 . Hence, all rate pairs in i=1 \u03bbi G0 (P1i , P2i , \u03b1i , \u03b2i ) are achievable\nPq \u2032\nPq\u2032\nprovided that i=1 \u03bbi P1i \u2264 P1 and i=1 \u03bbi P2i \u2264 P2 .\nClearly, the chain of inclusions G0 \u2286 G1 \u2286 G2 \u2286 G \u2286 CHK \u2286 C always holds.\nD. Concavification Versus Time-Sharing\nIn this subsection, we follow two objectives. First, we aim at providing some necessary conditions such that G2 = G .\nSecond, we bound q and q \u2032 which are parameters involved in the descriptions of G and G2 , respectively. However, we derive\nthe required conditions for the more general case where there are M users in the system. To this end, assume an achievable\nscheme for an M -user channel with the power constraint P = [P1 , P2 , . . . , PM ] is given. The corresponding achievable region\ncan be represented as\nD0 (P, \u0398) = {R|AR \u2264 \u03a8(P, \u0398)} ,\n(25)\nwhere A is a K \u00d7 M matrix and \u0398 \u2208 [0, 1]M . D0 is a polyhedron in general, but for the purpose of this paper, it suffices to\nassume that it is a polytope. Since D0 is a convex region, the convex hull operation does not lead to a new enlarged region.\nHowever, if the extreme points of the region are not a concave function of P, it is possible to enlarge D0 by using two\n\n\f6\n\ndifferent methods which are explained next. The first method is based on using the time sharing parameter. Let us denote the\ncorresponding region as D which can be written as\n)\n(\nq\nq\nq\nX\nX\nX\nM\n\u03bbi = 1, \u03bbi \u2265 0, \u0398i \u2208 [0, 1] \u2200i ,\n(26)\n\u03bbi Pi \u2264 P,\n\u03bbi \u03a8(Pi , \u0398i ),\nD = R|AR \u2264\ni=1\n\ni=1\n\ni=1\n\nwhere q \u2208 N.\nIn the second method, we use TD/FD to enlarge the achievable rate region. This results in an achievable region D2 represented\nas\n\uf8fc\n\uf8f1\nq\u2032\nq\u2032\nq\u2032\n\uf8fd\n\uf8f2\nX\nX\nX\n\u03bbi = 1, \u03bbi \u2265 0, \u0398i \u2208 [0, 1]M \u2200i ,\n(27)\n\u03bbi Pi \u2264 P,\n\u03bbi Ri |ARi \u2264 \u03a8(Pi , \u0398i ),\nD2 = R =\n\uf8fe\n\uf8f3\ni=1\n\ni=1\n\ni=1\n\nwhere q \u2032 \u2208 N. We refer to this method as concavification. It can be readily shown that D and D2 are closed and convex, and\nD2 \u2286 D. We are interested in situations where the inverse inclusion holds.\nThe support function of D0 is a function of P, \u0398, and c. Hence, we have\n\u03c3D0 (c, P, \u0398) = max{ct R|AR \u2264 \u03a8(P, \u0398)}.\n\n(28)\n\nFor fixed P and \u0398, (28) is a linear program. Using strong duality of linear programming, we obtain\n\u03c3D0 (c, P, \u0398) = min{yt \u03a8(P, \u0398)|At y = c, y \u2265 0}.\n\n(29)\n\nIn general, \u0177, the minimizer of (29), is a function of P, \u0398, and c. We say D0 possesses the unique minimizer property if\n\u0177 merely depends on c, for all c. In this case, we have\n\u03c3D0 (c, P, \u0398) = \u0177t (c)\u03a8(P, \u0398),\n\n(30)\n\nwhere At \u0177 = c. This condition means that for any c the extreme point of D0 maximizing the objective ct R is an extreme\npoint obtained by intersecting a set of specific hyperplanes. A necessary condition for D0 to possess the unique minimizer\nproperty is that each inequality in describing D0 is either redundant or active for all P and \u0398.\nTheorem 1: If D0 possesses the unique minimizer property, then D = D2 .\nProof: Since D2 \u2286 D always holds, we need to show D \u2286 D2 which can be equivalently verified by showing \u03c3D \u2264 \u03c3D2 .\nThe support function of D can be written as\n\b\n\u03c3D (c, P) = max ct R|R \u2208 D .\n(31)\n\nBy fixing P, Pi 's, \u0398i 's, and \u03bbi 's, the above maximization becomes a linear program. Hence, relying on weak duality of linear\nprogramming, we obtain\nq\nX\n\u03bbi \u03a8(Pi , \u0398i ).\n(32)\n\u03c3D (c, P) \u2264 t min yt\nA y=c,y\u22650\n\ni=1\n\nClearly, \u0177(c), the solution of (29), is a feasible point for (32) and we have\n\u03c3D (c, P) \u2264 \u0177t (c)\nUsing (30), we obtain\n\u03c3D (c, P) \u2264\n\nq\nX\n\nq\nX\n\n\u03bbi \u03a8(Pi , \u0398i ).\n\n(33)\n\n\u03bbi \u03c3D0 (c, Pi , \u0398i ).\n\n(34)\n\ni=1\n\ni=1\n\nLet us assume R\u0302i is the maximizer of (28). In this case, we have\n\u03c3D (c, P) \u2264\nHence, we have\n\nq\nX\n\nPq\n\ni=1\n\n(35)\n\ni=1\n\n\u03c3D (c, P) \u2264 ct\nBy definition,\n\n\u03bbi ct R\u0302i .\n\nq\nX\n\n\u03bbi R\u0302i .\n\n(36)\n\n\u03c3D (c, P) \u2264 \u03c3D2 (c, P).\n\n(37)\n\ni=1\n\n\u03bbi R\u0302i is a point in D2 . Therefore, we conclude\n\nThis completes the proof.\n\n\f7\n\nCorollary 1 (Han [18]): If D0 is a polymatroid, then D=D2 .\nProof: It is easy to show that D0 possesses the unique minimizer property. In fact, for given c, \u0177 can be obtained in a\ngreedy fashion independent of P and \u0398.\nIn what follows, we upper bound q and q \u2032 .\nTheorem 2: The cardinality of the time sharing parameter q in (26) is less than M + K + 1, where M and K are the\ndimensions of P and \u03a8(P), respectively. Moreover, if \u03a8(P) is a continuous function of P, then q \u2264 M + K.\nProof: Let us define E as\n)\n( q\nq\nq\nX\nX\nX\nM\n\u03bbi = 1, \u03bbi \u2265 0, \u0398i \u2208 [0, 1] \u2200i .\n(38)\n\u03bbi Pi \u2264 P,\n\u03bbi \u03a8(Pi , \u0398i )|\nE=\ni=1\n\ni=1\n\ni=1\n\nIn fact, E is the collection of all possible bounds for D. To prove q \u2264 M + K + 1, we define another region E1 as\nE1 = {(P\u2032 , S\u2032 )|0 \u2264 P\u2032 , S\u2032 = \u03a8(P\u2032 , \u0398\u2032 ), \u0398\u2032 \u2208 [0, 1]M }.\n\n(39)\n\nFrom the direct consequence of the Caratheodory's theorem [19], the convex hull of E1 denoted by conv E1 can be obtained\nby convex combinations of no more than M + K + 1 points in E1 . Moreover, if \u03a8(P\u2032 , \u0398\u2032 ) is continuous, then M + K points\nare sufficient due to the extension of the Caratheodory's theorem [19]. Now, we define the region \u00ca as\n\u00ca = {S\u2032 |(P\u2032 , S\u2032 ) \u2208 conv E1 , P\u2032 \u2264 P}.\n\n(40)\nPq\n\nSince (Pi , \u03a8(Pi , \u0398i ))\nClearly, \u00ca \u2286 E. To\nE, say S = i=1 \u03bbi \u03a8(Pi , \u0398i ).P\nPqshow the other inclusion, let us consider a point in P\nq\nq\nis a point in E1 , i=1 \u03bbi (Pi , \u03a8(Pi , \u0398i )) belongs to conv E1 . Having i=1 \u03bbi Pi \u2264 P, we conclude i=1 \u03bbi \u03a8(Pi , \u0398) \u2208 \u00ca.\nHence, E \u2286 \u00ca. This completes the proof.\nCorollary 2 (Etkin, Parakh, and Tse [20]): For the M -user Gaussian IC where users use Gaussian codebooks for data\ntransmission and treat the interference as noise, the cardinality of the time sharing parameter is less than 2M .\nProof: In this case, D0 = {R|R \u2264 \u03a8(P)} where both P and \u03a8(P) have dimension M and \u03a8(P) is a continuous\nfunction of P. Applying Theorem 2 yields the desired result.\nIn the following theorem, we obtain an upper bound on q \u2032 .\nTheorem 3: To characterize boundary points of D2 , it suffices to set q \u2032 \u2264 M + 1.\nProof: Let us assume R\u0302 is a boundary point of D2 . Hence, there exists c such that\n\u03c3D2 (c, P) = max ct R = ct R\u0302,\nR\u2208D2\n\n(41)\n\nPq \u2032\nwhere R\u0302 = i=1 \u03bb\u0302i R\u0302i and the optimum is achieved for the set of parameters \u0398\u0302i , \u03bb\u0302i , and P\u0302i . The optimization problem in\n(41) can be written as\n\u2032\n\n\u03c3D2 (c, P) =max\n\nq\nX\n\n\u03bbi g(c, Pi )\n\n(42)\n\ni=1\n\nsubject to:\n\n\u2032\n\n\u2032\n\nq\nX\ni=1\n\n\u03bbi = 1,\n\nq\nX\ni=1\n\n\u03bbi Pi \u2264 P,\n\n0 \u2264 \u03bbi , 0 \u2264 Pi , \u2200i \u2208 {1, 2, . . . , q \u2032 },\n\nwhere g(c, P) is defined as\ng(c, P) =max ct R\nsubject to: AR \u2264 \u03a8(P, \u0398), 0 \u2264 \u0398 \u2264 1,\n\n(43)\n\nIn fact, \u03c3D2 (c, P) in (42) can be viewed as the result of the concavification of g(c, P) [19]. Hence, using Theorem 2.16 in\n[19], we conclude that q \u2032 \u2264 M + 1.\nRemarkable point about Theorem 3 is that the upper bound on q \u2032 is independent of the number of inequalities involved in\nthe description of the achievable rate region.\nCorollary 3: For the M -user Gaussian IC where users use Gaussian codebooks and treat the interference as noise, we have\nD2 = D and q = q \u2032 = M + 1.\n\n\f8\n\nE. Extremal Inequality\nIn [14], the following optimization problem is studied:\nW = max h(X + Z1 ) \u2212 \u03bch(X + Z2 ),\n\n(44)\n\nQX \u2264S\n\nwhere Z1 and Z2 are n-dimensional Gaussian random vectors with the strictly positive definite covariance matrices QZ1\nand QZ2 , respectively. The optimization is over all random vectors X independent of Z1 and Z2 . X is also subject to the\ncovariance matrix constraint QX \u2264 S, where S is a positive definite matrix. In [14], it is shown that for all \u03bc \u2265 1, this\noptimization problem has a Gaussian optimal solution for all positive definite matrices QZ1 and QZ2 . However, for 0 \u2264 \u03bc < 1\nthis optimization problem has a Gaussian optimal solution provided QZ1 \u2264 QZ2 , i.e., QZ2 \u2212 QZ1 is a positive semi-definite\nmatrix. It is worth noting that for \u03bc = 1 this problem when QZ1 \u2264 QZ2 is studied under the name of the worse additive noise\n[21], [22].\nIn this paper, we consider a special case of (44) where Z1 and Z2 have the covariance matrices N1 I and N2 I, respectively,\nand the trace constraint is considered, i.e.,\nW =\n\nmax\n\ntr{QX }\u2264nP\n\nh(X + Z1 ) \u2212 \u03bch(X + Z2 ).\n\n(45)\n\nIn the following lemma, we provide the optimal solution for the above optimization problem when N1 \u2264 N2 .\nLemma 1: If N1 \u2264 N2 , the optimal solution of (45) is iid Gaussian for all 0 \u2264 \u03bc and we have\n2 +P\n1) For 0 \u2264 \u03bc \u2264 N\nN1 +P , the optimum covariance matrix is P I and the optimum solution is\nW =\n2) For\n\n3) For\n\nN2 +P\nN1 +P\n\nN2\nN1\n\n<\u03bc\u2264\n\nN2\nN1 ,\n\n\u03bcn\nn\nlog [(2\u03c0e)(P + N1 )] \u2212\nlog [(2\u03c0e)(P + N2 )] .\n2\n2\n\n(46)\n\n\u2212\u03bcN1\nI and the optimum solution is\nthe optimum covariance matrix is N2\u03bc\u22121\n\u0015\n\u0015\n\u0014\n\u0014\n\u03bcn\nn\nN2 \u2212 N1\n\u03bc(2\u03c0e)(N2 \u2212 N1 )\n\u2212\n.\nW = log (2\u03c0e)\nlog\n2\n\u03bc\u22121\n2\n\u03bc\u22121\n\n(47)\n\n< \u03bc, the optimum covariance matrix is 0 and the optimum solution is\n\n\u03bcn\nn\nlog(2\u03c0eN2 ).\n(48)\nW = log(2\u03c0eN1 ) \u2212\n2\n2\nProof: From the general result for (44), we know that the optimum input distribution is Gaussian. Hence, we need to\nsolve the following maximization problem:\n1\n\u03bc\nlog ((2\u03c0e)n |QX + N1 I|) \u2212 log ((2\u03c0e)n |QX + N2 I|)\n2\n2\nsubject to: 0 \u2264 QX , tr{QX } \u2264 nP.\n\nW =max\n\n(49)\n\nSince QX is a positive semi-definite matrix, it can be decomposed as QX = U \u039bU t , where \u039b is a diagonal matrix with\nnonnegative entries and U is a unitary matrix, i.e., U U t = I. Substituting QX = U \u039bU t in (49) and using the identities\ntr{AB} = tr{BA} and |AB + I| = |BA + I|, we obtain\n1\n\u03bc\nlog ((2\u03c0e)n |\u039b + N1 I|) \u2212 log ((2\u03c0e)n |\u039b + N2 I|)\n2\n2\nsubject to: 0 \u2264 \u039b, tr{\u039b} \u2264 nP.\n\nW =max\n\n(50)\n\nThis optimization problem can be simplified as\nn\n\nW =max\n\nnX\n[log(2\u03c0e)(\u03bbi + N1 ) \u2212 \u03bc log(2\u03c0e)(\u03bbi + N2 )]\n2 i=1\n\nsubject to: 0 \u2264 \u03bbi \u2200i,\n\nn\nX\ni=1\n\n\u03bbi \u2264 nP.\n\nBy introducing Lagrange multipliers \u03c8 and \u03a6 = {\u03c61 , \u03c62 , . . . , \u03c6n }, we obtain\nn\n\n(51)\n\nn\n\nX\nnX\nL(\u039b, \u03c8, \u03a6) = max\n\u03bbi\n[log(2\u03c0e)(\u03bbi + N1 ) \u2212 \u03bc log(2\u03c0e)(\u03bbi + N2 )] + \u03c8 nP \u2212\n2 i=1\ni=1\n\n!\n\n+\n\nn\nX\ni=1\n\n\u03c6i \u03bbi .\n\n(52)\n\n\f9\n\nVariance\n\nP\n\nN2 \u2212\u03bcN1\n\u03bc\u22121\n\n\u03bc\n1\nFig. 2.\n\nN2\nN1\n\nP +N2\nP +N1\n\nOptimum variance versus \u03bc.\n\nThe first order KKT necessary conditions for the optimum solution of (52) can be written as\n\u03bc\n1\n\u2212\n\u2212 \u03c8 + \u03c6i =0, \u2200i \u2208 {1, 2, . . . , n},\n\u03bbi + N1\n\u03bbi + N2\n!\nn\nX\n\u03bbi =0,\n\u03c8 nP \u2212\n\n(53)\n(54)\n\ni=1\n\n\u03c6i \u03bbi =0, \u2200i \u2208 {1, 2, . . . , n}.\n\nIt is easy to show that when N1 \u2264 N2 , \u03bb = \u03bb1 = . . . = \u03bbn and the\n\uf8f1\n\uf8f4\nif\n0\n\uf8f2 P,\nN2 \u2212\u03bcN1\nN2 +P\n,\nif\n\u03bb=\n\u03bc\u22121\nN1 +P\n\uf8f4\nN2\n\uf8f3 0,\nif\nN1\n\n(55)\n\nonly solution for \u03bb is\n\u2264\u03bc\u2264\n<\u03bc\u2264\n<\u03bc\n\nN2 +P\nN1 +P\nN2\nN1\n\n(56)\n\nSubstituting \u03bb into the objective function gives the desired result.\n+N2\nIn Figure 2, the optimum variance as a function of \u03bc is plotted. This figure shows that for any value of \u03bc \u2264 P\nP +N1 , we\nP +N2\nneed to use the maximum power to optimize the objective function, whereas for \u03bc > P\n+N1 , we use less power than what is\npermissible.\nLemma 2: If N1 > N2 , the optimal solution of (45) is iid Gaussian for all 1 \u2264 \u03bc. In this case, the optimum variance is 0\nand the optimum W is\nn\n\u03bcn\nW = log(2\u03c0eN1 ) \u2212\nlog(2\u03c0eN2 ).\n(57)\n2\n2\nProof: The proof is similar to that of Lemma 1 and we omit it here.\nCorollary 4: For \u03bc = 1, the optimal solution of (45) is iid Gaussian and the optimum W is\n\uf8f1\n\u0010\n\u0011\n\uf8f2 n log P +N1 , if N1 \u2264 N2\n2\nP\n+N\n\u0010 \u00112\n(58)\nW =\n\uf8f3 n log N1 ,\nif N1 > N2 .\n2\nN2\nWe frequently apply the following optimization problem in the rest of the paper:\n\u221a\n(59)\nfh (P, N1 , N2 , a, \u03bc) =\nmax h(X + Z1 ) \u2212 \u03bch( aX + Z2 ),\ntr{QX }\u2264nP\n\nwhere N1 \u2264 N2 /a. Using the identity h(AX) = log(|A|) + h(X), (59) can be written as\nfh (P, N1 , N2 , a, \u03bc) =\n\nn\nZ2\nlog a +\nmax h(X + Z1 ) \u2212 \u03bch(X + \u221a ).\n2\ntr{QX }\u2264nP\na\n\nNow, Lemma 1 can be applied to obtain\n\uf8f1 1\n\u03bc\n+ N2 )] i\n\uf8f4\n\uf8f2 2 log h[(2\u03c0e)(P + N1 )]i \u2212 2 log [(2\u03c0e)(aP\nh\nN2 /a\u2212N1\na\u03bc(2\u03c0e)(N2 /a\u2212N1 )\n\u03bc\n1\nfh (P, N1 , N2 , a, \u03bc) =\n\u2212 2 log\n2 log (2\u03c0e)\n\u03bc\u22121\n\u03bc\u22121\n\uf8f4\n\uf8f3 1\n\u03bc\nlog(2\u03c0eN\n)\n\u2212\nlog(2\u03c0eN\n)\n1\n2\n2\n2\n\nP +N2 /a\nP +N1\nP +N2 /a\nN2\n<\n\u03bc \u2264 aN\nP +N1\n1\nN2\n<\n\u03bc\naN1\n\nif 0 \u2264 \u03bc \u2264\n\nif\nif\n\n(60)\n\n(61)\n\n\f10\n\n\u1ef91\n\nx1\n\nx2\n\nFig. 3.\n\n\u03c9(\u1ef91, \u1ef92|x1 , x2)\n\n\u1ef92\n\nf1\n\n\u01771\n\n\u01772\nf2\n\nAn admissible channel. f1 and f2 are deterministic functions.\n\nIII. A DMISSIBLE C HANNELS\nIn this section, we aim at building ICs whose capacity regions contain the capacity region of the two-user Gaussian IC, i.e.,\nC . Since we ultimately use these to outer bound C , these ICs need to have a tractable expression (or a tractable outer bound)\nfor their capacity regions.\nLet us consider an IC with the same input letters as that of C and the output letters \u1ef91 and \u1ef92 for Users 1 and 2, respectively.\nThe capacity region of this channel, say C \u2032 , contains C if\nI(xn1 ; y1n ) \u2264I(xn1 ; \u1ef91n ),\n\nI(xn2 ; y2n )\n\n\u2264I(xn2 ; \u1ef92n ),\n\n(62)\n(63)\n\nfor all p(xn1 )p(xn2 ) and for all n \u2208 N.\nOne way to satisfy (62) and (63) is to provide some extra information to either one or to both receivers. This technique\nis known as Genie aided outer bounding. In [12], Kramer has used such a genie to provide some extra information to both\nreceivers such that they can decode both users' messages. Since the capacity region of this new interference channel is equivalent\nto that of the Compound Multiple Access Channel whose capacity region is known, reference [12] obtains an outer bound\non the capacity region. To obtain a tighter outer bound, reference [12] further uses the fact that if a genie provides the exact\ninformation about the interfering signal to one of the receivers, then the new channel becomes the one-sided Gaussian IC.\nAlthough the capacity region of the one-sided Gaussian IC is unknown for all ranges of parameters, there exists an outer bound\nfor it due to Sato and Costa [23], [11] that can be applied to the original channel. In [13], Etkin et al. use a different genie\nthat provides some extra information about the intended signal. Even though at first glance their proposed method appears to\nbe far from achieving a tight bound, remarkably they show that the corresponding bound is tighter than the one due to Kramer\nfor certain ranges of parameters.\nNext, we introduce the notion of admissible channels to satisfy (62) and (63).\nDefinition 3 (Admissible Channel): An IC C \u2032 with input letter xi and output letter \u1ef9i for User i \u2208 {1, 2} is an admissible\nchannel if there exist two deterministic functions \u01771n = f1 (\u1ef91n ) and \u01772n = f2 (\u1ef92n ) such that\nI(xn1 ; y1n ) \u2264I(xn1 ; \u01771n ),\nI(xn2 ; y2n ) \u2264I(xn2 ; \u01772n )\n\n(64)\n(65)\n\nhold for all p(xn1 )p(xn2 ) and for all n \u2208 N. E denotes the collection of all admissible channels (see Figure 3).\nRemark 2: Genie aided channels are among admissible channels. To see this, let us assume a genie provides s1 and s2 as\nside information for User 1 and 2, respectively. In this case, \u1ef9i = (yi , si ) for i \u2208 {1, 2}. By choosing fi (yi , si ) = yi , we\nobserve that \u0177i = yi , and hence, (64) and (65) trivially hold.\nTo obtain the tightest outer bound, we need to find the intersection of the capacity regions of all admissible channels.\nNonetheless, it may happen that finding the capacity region of an admissible channel is as hard as that of the original one (in\nfact, based on the definition, the channel itself is one of its admissible channels). Hence, we need to find classes of admissible\nchannels, say F , which possess two important properties. First, their capacity regions are close to C . Second, either their\nexact capacity regions are computable or there exist good outer bounds for them. Since F \u2286 E , we have\n\\\nC \u2286\nC \u2032.\n(66)\nF\n\nRecall that there is a one to one correspondence between a closed convex set and its support function. Since C is closed and\nconvex, there is a one to one correspondence between C and \u03c3C . In fact, boundary points of C correspond to the solutions\nof the following optimization problem\n\u03c3C (c1 , c2 ) = max c1 R1 + c2 R2 .\n(67)\n(R1 ,R2 )\u2208C\n\nSince we are interested in the boundary points excluding the R1 and R2 axes, it suffices to consider 0 \u2264 c1 and 0 \u2264 c2 where\nc1 + c2 = 1.\n\n\f11\n\nz1\n\nf1 (\u1ef91 ) = \u1ef91\n\nx1\n\n\u01771\n\n\u1ef91\n\u221a\n\nb\u2032\n\n\u221a\n\nz21\na\n\n\u221a\n\n\u1ef921\nx2\n\n\u01772\n\nz22\n\u1ef922\nAdmissible Channel\n\nFig. 4.\n\ng2\n\n1\u2212\n\n\u221a\n\ng2\n\nf2 (\u1ef922 , \u1ef921 ) = (1 \u2212\n\n\u221a\n\ng2 )\u1ef922 +\n\n\u221a\n\ng2 \u1ef921\n\nClass A1 admissible channels.\n\nSince C \u2286 C \u2032 , we have\n\n\u03c3C (c1 , c2 ) \u2264 \u03c3C \u2032 (c1 , c2 ).\n\n(68)\n\nTaking the minimum of the right hand side, we obtain\n\u03c3C (c1 , c2 ) \u2264 min\n\u03c3C \u2032 (c1 , c2 ),\n\u2032\nC \u2208F\n\n(69)\n\nwhich can be written as\n\u03c3C (c1 , c2 ) \u2264 min\n\u2032\n\nmax\n\nC \u2208F (R1 ,R2 )\u2208C \u2032\n\nc1 R1 + c2 R2 .\n\n(70)\n\nFor convenience, we use the following two optimization problems\n\u03c3C (\u03bc, 1) =\n\u03c3C (1, \u03bc) =\n\nmax\n\n\u03bcR1 + R2 ,\n\n(71)\n\nmax\n\nR1 + \u03bcR2 ,\n\n(72)\n\n(R1 ,R2 )\u2208C\n(R1 ,R2 )\u2208C\n\nwhere 1 \u2264 \u03bc. It is easy to show that the solutions of (71) and (72) correspond to the boundary points of the capacity region.\nIn the rest of this section, we introduce classes of admissible channels and obtain upper bounds on \u03c3C \u2032 (\u03bc, 1) and \u03c3C \u2032 (1, \u03bc).\nA. Classes of Admissible Channels\n1) Class A1: This class is designed to obtain an upper bound on \u03c3C (\u03bc, 1). Therefore, we need to find a tight upper bound\non \u03c3C \u2032 (\u03bc, 1). A member of this class is a channel in which User 1 has one transmit and one receive antenna whereas User 2\nhas one transmit antenna and two receive antennas (see Figure 4). The channel model can be written as\n\u221a\n\u1ef91 = x1 + \u221aax2 + z1 ,\n(73)\n\u1ef921 = x2 + b\u2032 x1 + z21 ,\n\u1ef922 = x2 + z22 ,\nwhere \u1ef91 is the signal at the first receiver, \u1ef921 and \u1ef922 are the signals at the second receiver, z1 is additive Gaussian noise\nwith unit variance, z21 and z22 are additive Gaussian noise with variances N21 and N22 , respectively. Transmitters 1 and 2\nare subject to the power constraints of P1 and P2 , respectively.\nTo investigate admissibility conditions in (64) and (65), we introduce two deterministic functions f1 and f2 as follows (see\nFigure 4)\nf1 (\u1ef91n )= \u1ef91n ,\nn\nn\n, \u1ef921\n)=\nf2 (\u1ef922\n\n\u221a n\n\u221a\nn\n,\n+ g2 \u1ef921\n(1 \u2212 g2 )\u1ef922\n\n(74)\n(75)\n\nwhere 0 \u2264 g2 . For g2 = 0, the channel can be converted to the one-sided Gaussian IC by letting N21 \u2192 \u221e and N22 = 1.\nHence, Class A1 contains the one-sided Gaussian IC obtained by removing the link between Transmitter 1 and Receiver 2.\nUsing f1 and f2 , we obtain\n\u221a\n\u01771n =xn1 + axn2 + z1n ,\n(76)\np\n\u221a\n\u221a n\nn\nn\nn\nn\n\u2032\n\u01772 = b g2 x1 + x2 + (1 \u2212 g2 )z22 + g2 z21 .\n(77)\n\n\f12\n\nHence, this channel is admissible if the corresponding parameters satisfy\nb\u2032 g2\n\u221a 2\n(1 \u2212 g2 ) N22 + g2 N21\n\n= b,\n= 1.\n\n(78)\n\nWe further add the following constraints to the conditions of the channels in Class A1:\nb\u2032\naN22\n\n\u2264 N21 ,\n\u2264 1.\n\n(79)\n\nAlthough these additional conditions reduce the number of admissible channels within the class, they are needed to get a\nclosed form formula for an upper bound on \u03c3C \u2032 (\u03bc, 1). In the following lemma, we obtain the required upper bound.\nLemma 3: For the channels modeled by (73) and satisfying (79), we have\n\u0013\n\u0012\n\u03bc1\nb\u2032 P1\nP2\n\u03bc2\n1\nN21\n\u03c3C \u2032 (\u03bc, 1) \u2264min\n(80)\n+\n+\nlog [2\u03c0e(P1 + aP2 + 1)] \u2212\nlog(2\u03c0e) + log\n2\n2\n2\nN22\nN22\nP2 + N22\n\u0013\n\u0012\n1\n+ fh (P2 , N22 , 1, a, \u03bc1 )\n+ \u03bc2 fh P1 , 1, N21 , b\u2032 ,\n\u03bc2\nsubject to: \u03bc1 + \u03bc2 = \u03bc, \u03bc1 , \u03bc2 \u2265 0.\nProof: Let us assume R1 and R2 are achievable rates for User 1 and 2, respectively. Furthermore, we split \u03bc into \u03bc1 \u2265 0\nand \u03bc2 \u2265 0 such that \u03bc = \u03bc1 + \u03bc2 . Using Fano's inequality, we obtain\nn\nn\nn(\u03bcR1 + R2 ) \u2264\u03bcI(xn1 ; \u1ef91n ) + I(xn2 ; \u1ef922\n, \u1ef921\n) + n\u01ebn\nn n\nn n\nn\nn\n=\u03bc1 I(x1 ; \u1ef91 ) + \u03bc2 I(x1 ; \u1ef91 ) + I(xn2 ; \u1ef922\n, \u1ef921\n) + n\u01ebn\n(a)\n\nn\nn\n\u2264 \u03bc1 I(xn1 ; \u1ef91n ) + \u03bc2 I(xn1 ; \u1ef91n |xn2 ) + I(xn2 ; \u1ef922\n, \u1ef921\n) + n\u01ebn\n\nn\nn\nn\n) + n\u01ebn\n|\u1ef922\n) + I(xn2 ; \u1ef922\n=\u03bc1 I(xn1 ; \u1ef91n ) + \u03bc2 I(xn1 ; \u1ef91n |xn2 ) + I(xn2 ; \u1ef921\nn\nn n\nn n\nn n\nn\n=\u03bc1 h(\u1ef91 ) \u2212 \u03bc1 h(\u1ef91 |x1 ) + \u03bc2 h(\u1ef91 |x2 ) \u2212 \u03bc2 h(\u1ef91 |x1 , x2 )\n\nn\nn\nn\nn\nn\nn\n|xn2 ) + n\u01ebn\n) + h(\u1ef922\n) \u2212 h(\u1ef922\n|xn2 , \u1ef922\n+h(\u1ef921\n|\u1ef922\n) \u2212 h(\u1ef921\n\u0003\n\u0003\n\u0002\n\u0002\nn\nn\n|xn2 , \u1ef922\n)\n= \u03bc1 h(\u1ef91n ) \u2212 \u03bc2 h(\u1ef91n |xn1 , xn2 ) + \u03bc2 h(\u1ef91n |xn2 ) \u2212 h(\u1ef921\n\u0003\n\u0003 \u0002 n\n\u0002 n n\nn\n|xn2 ) + h(\u1ef922\n) \u2212 \u03bc1 h(\u1ef91n |xn1 ) + n\u01ebn ,\n+ h(\u1ef921\n|\u1ef922 ) \u2212 h(\u1ef922\n\nxn1\n\n(81)\n\nxn2\n\nwhere (a) follows from the fact that\nand\nare independent. Now, we separately upper bound the terms within each bracket\nin (81).\nTo maximize the terms within the first bracket, we use the fact that Gaussian distribution maximizes the differential entropy\nsubject to a constraint on the covariance matrix. Hence, we have\n\u221a\n\u03bc1 h(\u1ef91n ) \u2212 \u03bc2 h(\u1ef91n |xn1 , xn2 )= \u03bc1 h(xn1 + axn2 + z1n ) \u2212 \u03bc2 h(z1n )\n\u03bc2 n\n\u03bc1 n\nlog [2\u03c0e(P1 + aP2 + 1)] \u2212\nlog(2\u03c0e).\n(82)\n\u2264\n2\n2\nSince b\u2032 \u2264 N21 , we can make use of Lemma 1 to upper bound the second bracket. In this case, we have\n\u0012\n\u0013\n1 \u221a \u2032 n\nn n\nn\nn n\nn\nn\nn\nh( b x1 + z21 )\n\u03bc2 h(\u1ef91 |x2 ) \u2212 h(\u1ef921 |x2 , \u1ef922 )= \u03bc2 h(x1 + z1 ) \u2212\n\u03bc2\n\u0013\n\u0012\n1\n,\n(83)\n\u2264 \u03bc2 nfh P1 , 1, N21 , b\u2032 ,\n\u03bc2\nwhere fh is defined in (61).\nWe upper bound the terms within the third bracket as follows [13]:\nn\n(a)X\n\nn\nn\nn\nh(\u1ef921\n|\u1ef922\n) \u2212 h(\u1ef922\n|xn2 ) \u2264\n\ni=1\nn\n(b)X\n\nn\nh(\u1ef921 [i]|\u1ef922 [i]) \u2212 h(z22\n)\n\n\u0013\u0015\n\u0014\n\u0012\n1\nn\nP2 [i]N22\n\u2212 log (2\u03c0eN22 )\nlog 2\u03c0e N21 + b\u2032 P1 [i] +\n2\nP\n[i]\n+\nN\n2\n2\n22\ni=1\n!#\n\"\nP\nn\nn\n1\n(c) n\nP\n[i]N\nn\n1X \u2032\n2\n22\ni=1\n\u2212 log (2\u03c0eN22 )\n\u2264 log 2\u03c0e N21 +\nb P1 [i] + 1 nPn\n2\nn i=1\n2\nP\n[i]\n+\nN\n22\ni=1 2\nn\n\u0013\u0015\n\u0014\n\u0012\nn\nP2 N22\nn\n\u2212 log (2\u03c0eN22 )\n\u2264 log 2\u03c0e N21 + b\u2032 P1 +\n2\nP2 + N22\n2\n\u0013\n\u0012\nb\u2032 P1\nP2\nN21\nn\n,\n+\n+\n\u2264 log\n2\nN22\nN22\nP2 + N22\n\u2264\n\n(84)\n\n\f13\n\nf1 (\u1ef911 , \u1ef912 ) = (1 \u2212\n\nz11\n\u1ef911\nx1\n\n1\u2212\n\n\u221a\n\ng1 )\u1ef911 +\n\n\u221a\n\ng1 \u1ef912\n\ng1\n\u01771\n\nz12\n\u221a\n\n\u221a\n\n\u1ef912\n\n\u221a\n\ng1\n\nb\n\n\u221a\n\na\u2032\nz2\n\nx2\n\n\u1ef92\n\n\u01772\nf2 (\u1ef92 ) = \u1ef92\n\nAdmissible Channel\n\nFig. 5.\n\nClass A2 admissible channels.\n\nwhere (a) follows from the chain rule and the fact that removing independent conditions does not decrease differential entropy,\n(b) follows from the fact that Gaussian distribution maximizes the conditional entropy for a given covariance matrix, and (c)\nfollows form Jenson's inequality.\nFor the last bracket, we again make use of the definition of fh . In fact, since aN22 \u2264 1, we have\n\u221a\nn\nn\nh(\u1ef922\n) \u2212 \u03bc1 h(\u1ef91n |xn1 )= h(xn2 + z22\n) \u2212 \u03bc1 h( axn2 + z1n )\n\u2264 nfh (P2 , N22 , 1, a, \u03bc1 ).\n(85)\nAdding all inequalities, we obtain\n\u03bcR1 + R2 \u2264\n\n\u0012\n\u0013\n\u03bc1\nN21\nb\u2032 P1\nP2\n\u03bc2\n1\n+\n+\nlog [2\u03c0e(P1 + aP2 + 1)] \u2212\nlog(2\u03c0e) + log\n2\n2\n2\nN22\nN22\nP2 + N22\n\u0012\n\u0013\n1\n+\u03bc2 fh P1 , 1, N21 , b\u2032 ,\n+ fh (P2 , N22 , 1, a, \u03bc1 ),\n\u03bc2\n\n(86)\n\nwhere the fact that \u01ebn \u2192 0 as n \u2192 \u221e is used to eliminate \u01ebn form the right hand side of the inequality. Now, by taking the\nminimum of the right hand side of (86) over all \u03bc1 and \u03bc2 , we obtain the desired result. This completes the proof.\n2) Class A2: This class is the complement of Class A1 in the sense that we use it to upper bound \u03c3C (1, \u03bc). A member of\nthis class is a channel in which User 1 is equipped with one transmit and two receive antennas, whereas User 2 is equipped\nwith one antenna at both transmitter and receiver sides (see Figure 5). The channel model can be written as\n\u1ef911 = x1 + \u221a\nz11 ,\n\u1ef912 = x1 + \u221aa\u2032 x2 + z12 ,\n\u1ef92 = x2 + bx1 + z2 ,\n\n(87)\n\nwhere \u1ef911 and \u1ef912 are the signals at the first receiver, \u1ef92 is the signal at the second receiver, z2 is additive Gaussian noise\nwith unit variance, z11 and z12 are additive Gaussian noise with variances N11 and N12 , respectively. Transmitter 1 and 2 are\nsubject to the power constraints P1 and P2 , respectively.\nFor this class, we consider two linear functions f1 and f2 as follows (see Figure 5):\n\u221a\n\u221a n\nn\nn\nn\nf1 (\u1ef911\n, \u1ef912\n)= (1 \u2212 g1 )\u1ef911\n+ g1 \u1ef912\n,\n(88)\nf2 (\u1ef92n )= \u1ef92n .\n\n(89)\n\nSimilar to Class A1, when g1 = 0, the admissible channels in Class A2 become the one-sided Gaussian IC by letting N12 \u2192 \u221e\nand N11 = 1. Therefore, we have\np\n\u221a\n\u221a n\nn\n\u01771n =xn1 + a\u2032 g1 xn2 + (1 \u2212 g1 )z11\n+ g1 z12\n,\n(90)\n\u221a\nn\nn\nn\nn\n\u01772 = bx1 + x2 + z2 .\n(91)\nWe conclude that the channel modeled by (87) is admissible if the corresponding parameters satisfy\na\u2032 g 1\n\u221a 2\n(1 \u2212 g1 ) N11 + g1 N12\n\n= a,\n= 1.\n\n(92)\n\n\f14\n\nz11\n\nf1 (\u1ef911 , \u1ef912 ) = (1 \u2212\n\u1ef911\n\nx1\n\n\u221a\n\n1\u2212\n\n\u221a\n\ng1 )\u1ef911 +\n\n\u221a\n\ng1 \u1ef912\n\ng1\n\nz12\n\n\u01771\n\u1ef912\n\n\u221a\n\ng1\n\n\u221a\nb\u2032\n\n\u221a\n\nz21\na\u2032\n\u1ef921\n\nx2\n\ng2\n\u01772\n\nz22\n\u1ef922\nAdmissible Channel\n\nFig. 6.\n\n\u221a\n\n1\u2212\n\n\u221a\n\ng2\n\nf2 (\u1ef922 , \u1ef921 ) = (1 \u2212\n\n\u221a\n\ng2 )\u1ef922 +\n\n\u221a\n\ng2 \u1ef921\n\nClass B admissible channels.\n\nSimilar to Class A1, we further add the following constraints to the conditions of Class A2 channels:\na\u2032\nbN11\n\n\u2264 N12 ,\n\u2264 1.\n\n(93)\n\nIn the following lemma, we obtain the required upper bound.\nLemma 4: For the channels modeled by (87) and satisfying (93), we have\n\u0013\n\u0012\na\u2032 P2\nP1\n\u03bc2\n1\nN12\n\u03bc1\n(94)\n+\n+\nlog [2\u03c0e(bP1 + P2 + 1)] \u2212\nlog(2\u03c0e) + log\n\u03c3C \u2032 (1, \u03bc) \u2264min\n2\n2\n2\nN11\nN11\nP1 + N11\n\u0012\n\u0013\n1\n+ \u03bc2 fh P2 , 1, N12 , a\u2032 ,\n+ fh (P1 , N11 , 1, b, \u03bc1 )\n\u03bc2\nsubject to: \u03bc1 + \u03bc2 = \u03bc, \u03bc1 , \u03bc2 \u2265 0.\nProof: The proof is similar to that of Lemma 3 and we omit it here.\n3) Class B: A member of this class is a channel with one transmit antenna and two receive antennas for each user modeled\nby (see Figure 6)\n\u1ef911 = x1 + \u221a\nz11 ,\n\u1ef912 = x1 + \u221aa\u2032 x2 + z12 ,\n(95)\n\u1ef921 = x2 + b\u2032 x1 + z21 ,\n\u1ef922 = x2 + z22 ,\nwhere \u1ef911 and \u1ef912 are the signals at the first receiver, \u1ef921 and \u1ef922 are the signals at the second receiver, and zij is additive\nGaussian noise with variance Nij for i, j \u2208 {1, 2}. Transmitter 1 and 2 are subject to the power constraints P1 and P2 ,\nrespectively. In fact, this channel is designed to upper bound both \u03c3C (\u03bc, 1) and \u03c3C (1, \u03bc).\nNext, we investigate admissibility of this channel and the conditions that must be imposed on the underlying parameters.\nLet us consider two linear deterministic functions f1 and f2 with parameters 0 \u2264 g1 and 0 \u2264 g2 , respectively, as follows (see\nFigure 6)\n\u221a n\n\u221a\nn\nn\nn\n+ g1 \u1ef912\n,\n(96)\n, \u1ef912\n)= (1 \u2212 g1 )\u1ef911\nf1 (\u1ef911\n\u221a n\n\u221a\nn\nn\nn\n(97)\nf2 (\u1ef922 , \u1ef921 )= (1 \u2212 g2 )\u1ef922 + g2 \u1ef921 .\nTherefore, we have\np\n\u221a\n\u221a n\nn\n\u01771n =xn1 + a\u2032 g1 xn2 + (1 \u2212 g1 )z11\n+ g1 z12\n,\np\n\u221a\n\u221a n\nn\nn\nn\nn\n\u2032\n\u01772 = b g2 x1 + x2 + (1 \u2212 g2 )z22 + g2 z21 .\n\n(98)\n(99)\n\nTo satisfy (64) and (65), it suffices to have\n\na\u2032 g 1\nb\u2032 g2\n\u221a 2\n(1 \u2212 g1 ) N11 + g1 N12\n\u221a\n(1 \u2212 g2 )2 N22 + g2 N21\n\n= a,\n= b,\n= 1,\n= 1.\n\n(100)\n\n\f15\n\nHence, a channel modeled by (95) is admissible if there exist two nonnegative numbers g1 and g2 such that the equalities in\n(100) are satisfied. We further add the following two constraints to the equality conditions in (100):\nb\u2032 N11\na\u2032 N22\n\n\u2264 N21 ,\n\u2264 N12 .\n\n(101)\n\nAlthough adding more constraints reduces the number of the admissible channels, it enables us to compute an outer bound on\n\u03c3C \u2032 (\u03bc, 1) and \u03c3C \u2032 (1, \u03bc).\nLemma 5: For the channels modeled by (95) and satisfying (101), we have\n\u0012\n\u0013\n\u0012\n\u0013\nP1\nP1\nP2\nP2\n\u03c3C \u2032 (\u03bc, 1) \u2264\u03bc\u03b3\n+\u03b3\n+ \u2032\n+ \u2032\nN11\na P2 + N12\nN22\nb P1 + N21\n\u03bc\n1\n\u2032\n+fh (P2 , N22 , N12 , a , \u03bc) + log((2\u03c0e)(a\u2032 P2 + N12 )) \u2212 log((2\u03c0e)(P2 + N22 )),\n(102)\n2\n2\n\u0013\n\u0012\n\u0013\n\u0012\nP1\nP2\nP2\nP1\n+ \u03bc\u03b3\n+ \u2032\n+ \u2032\n\u03c3C \u2032 (1, \u03bc) \u2264\u03b3\nN11\na P2 + N12\nN22\nb P1 + N21\n\u03bc\n1\n+fh (P1 , N11 , N21 , b\u2032 , \u03bc) + log((2\u03c0e)(b\u2032 P1 + N21 )) \u2212 log((2\u03c0e)(P1 + N11 )).\n(103)\n2\n2\nProof: We only upper bound \u03c3C \u2032 (\u03bc, 1) and an upper bound on \u03c3C \u2032 (1, \u03bc) can be similarly obtained. Let us assume R1\nand R2 are achievable rates for User 1 and User 2, respectively. Using Fano's inequality, we obtain\nn\nn\nn\nn\nn(\u03bcR1 + R2 ) \u2264\u03bcI(xn1 ; \u1ef911\n, \u1ef912\n) + I(xn2 ; \u1ef922\n, \u1ef921\n) + n\u01ebn\nn\nn\nn\n=\u03bcI(xn1 ; \u1ef912\n|\u1ef911\n) + \u03bcI(xn1 ; \u1ef911\n)\nn n\nn\nn n\n+I(x2 ; \u1ef921 |\u1ef922 , ) + I(x2 ; \u1ef922 ) + n\u01ebn\n\nn\nn\nn\nn\nn\nn\n|xn1 )\n) \u2212 \u03bch(\u1ef912\n|xn1 , \u1ef911\n) + \u03bch(\u1ef911\n) \u2212 \u03bch(\u1ef911\n=\u03bch(\u1ef912\n|\u1ef911\nn\nn\nn\nn\nn\nn\n+h(\u1ef921\n|\u1ef922\n) \u2212 h(\u1ef921\n|xn2 , \u1ef922\n) + h(\u1ef922\n) \u2212 h(\u1ef922\n|xn2 ) + n\u01ebn\n\u0003\n\u0002\n\u0003\n\u0002\nn\nn\nn\nn\nn\nn\n) \u2212 h(\u1ef922\n|xn2 )\n|\u1ef911\n) \u2212 \u03bch(\u1ef911\n|xn1 ) + h(\u1ef921\n|\u1ef922\n= \u03bch(\u1ef912\n\u0002\n\u0003\n\u0002\n\u0003\nn\nn\nn\nn\nn\nn\n+ \u03bch(\u1ef911\n) \u2212 h(\u1ef921\n|xn2 , \u1ef922\n) + h(\u1ef922\n) \u2212 \u03bch(\u1ef912\n|xn1 , \u1ef911\n) + n\u01ebn .\n\n(104)\n\nNext, we upper bound the terms within each bracket in (104) separately. For the first bracket, we have\n(a)\n\nn\nn\nn\n) \u2212 \u03bch(\u1ef911\n|xn1 ) \u2264 \u03bc\n|\u1ef911\n\u03bch(\u1ef912\n\nn\nX\ni=1\n\nh(\u1ef912 [i]|\u1ef911 [i]) \u2212\n\n\u03bcn\nlog (2\u03c0eN11 )\n2\n\nn\nX\n1\n\n\u0013\u0015\n\u0014\n\u0012\n\u03bcn\nP1 [i]N11\n\u2212\nlog 2\u03c0e N12 + a\u2032 P2 [i] +\nlog (2\u03c0eN11 )\n2\nP\n[i]\n+\nN\n2\n1\n11\ni=1\n\"\n!#\nn\n1 Pn\n(c) \u03bcn\nP\n[i]N\n1X \u2032\n\u03bcn\n1\n11\n\u2264\na P2 [i] + 1 nPn i=1\nlog 2\u03c0e N12 +\nlog (2\u03c0eN11 )\n\u2212\n2\nn i=1\n2\ni=1 P1 [i] + N11\nn\n\u0013\u0015\n\u0014\n\u0012\n\u03bcn\n\u03bcn\nP1 N11\n\u2264\n\u2212\nlog 2\u03c0e N12 + a\u2032 P2 +\nlog (2\u03c0eN11 )\n2\nP1 + N11\n2\n\u0013\n\u0012\na\u2032 P2\nP1\nN12\n\u03bcn\n,\n(105)\n+\n+\nlog\n=\n2\nN11\nN11\nP1 + N11\n(b)\n\n\u2264\u03bc\n\nwhere (a) follows from the chain rule and the fact that removing independent conditions increases differential entropy, (b)\nfollows from the fact that Gaussian distribution optimizes conditional entropy for a given covariance matrix, and (c) follows\nform Jenson's inequality.\nSimilarly, the terms within the second bracket can be upper bounded as\n\u0013\n\u0012\nn\nb\u2032 P1\nP2\nN21\nn\nn\nn\n.\n(106)\nh(\u1ef921\n|\u1ef922\n) \u2212 h(\u1ef922\n|xn2 ) \u2264 log\n+\n+\n2\nN22\nN22\nP2 + N22\nUsing Lemma 1 and the fact that N11 \u2264 N21 /b\u2032 , the terms within the third bracket can be upper bounded as\n\u0013\n\u0012\n1 \u221a\nn\nn\nn\nn\nn\n)\n\u03bch(\u1ef911\n) \u2212 h(\u1ef921\n|xn2 , \u1ef922\n)= \u03bc h(xn1 + z11\n) \u2212 h( b\u2032 xn1 + z21\n\u03bc\n\u0013\n\u0012\n\u2032 1\n.\n\u2264 \u03bcnfh P1 , N11 , N21 , b ,\n\u03bc\n\nSince 1 \u2264 \u03bc, from (61) we obtain\n\nn\nn\nn\n\u03bch(\u1ef911\n) \u2212 h(\u1ef921\n|xn2 , \u1ef922\n)\u2264\n\n\u03bcn\nn\nlog((2\u03c0e)(P1 + N11 )) \u2212 log((2\u03c0e)(b\u2032 P1 + N21 )).\n2\n2\n\n(107)\n\n(108)\n\n\f16\n\nFor the last bracket, again we use Lemma 1 to obtain\n\n\u221a\nn\nn\nn\nn\nn\n)\nh(\u1ef922\n) \u2212 \u03bch(\u1ef912\n|xn1 , \u1ef911\n)= h(xn2 + z22\n) \u2212 \u03bch( a\u2032 xn2 + z12\n\u2264 nfh (P2 , N22 , N12 , a\u2032 , \u03bc).\n\n(109)\n\nAdding all inequalities, we have\n\u0013\n\u0013\n\u0012\n\u0012\n\u03bc\n1\na\u2032 P2\nP1\nb\u2032 P1\nP2\nN12\nN21\n\u03bcR1 + R2 \u2264 log\n+ log\n+\n+\n+\n+\n2\nN11\nN11\nP1 + N11\n2\nN22\nN22\nP2 + N22\n1\n\u03bc\n(110)\n+ log((2\u03c0e)(P1 + N11 )) \u2212 log((2\u03c0e)(b\u2032 P1 + N21 )) + fh (P2 , N22 , N12 , a\u2032 , \u03bc),\n2\n2\nwhere the fact that \u01ebn \u2192 0 as n \u2192 \u221e is used to eliminate \u01ebn from the right hand side of the inequality. By rearranging the\nterms, we obtain\n\u0012\n\u0013\n\u0012\n\u0013\nP1\nP1\nP2\nP2\n\u03bcR1 + R2 \u2264\u03bc\u03b3\n+ \u2032\n+ \u2032\n+\u03b3\nN11\na P2 + N12\nN22\nb P1 + N21\n1\n\u03bc\n\u2032\n+fh (P2 , N22 , N12 , a , \u03bc) + log((2\u03c0e)(a\u2032 P2 + N12 )) \u2212 log((2\u03c0e)(P2 + N22 )).\n2\n2\nThis completes the proof.\n\u2032\n\u2032\n12 /a\n21 /b\nA unique feature of the channels within Class B is that for 1 \u2264 \u03bc \u2264 P2P+N\nand 1 \u2264 \u03bc \u2264 P1P+N\n, the upper bounds\n2 +N22\n1 +N11\nin (102) and (103) become, respectively,\n\u0013\n\u0012\n\u0013\n\u0012\nP1\nP2\nP2\nP1\n+\u03b3\n(111)\n+ \u2032\n+ \u2032\n\u03bcR1 + R2 \u2264\u03bc\u03b3\nN11\na P2 + N12\nN22\nb P1 + N21\nand\nR1 + \u03bcR2 \u2264\u03b3\n\n\u0012\n\nP1\nP1\n+ \u2032\nN11\na P2 + N12\n\n\u0013\n\n+ \u03bc\u03b3\n\n\u0012\n\nP2\nP2\n+ \u2032\nN22\nb P1 + N21\n\nOn the other hand, if the receivers treat the interference as noise, it can be shown that\n\u0012\n\u0013\nP1\nP1\nR1 = \u03b3\n+ \u2032\nN11\na P2 + N12\nand\nR2 = \u03b3\n\n\u0012\n\nP2\nP2\n+ \u2032\nN22\nb P1 + N21\n\n\u0013\n\n\u0013\n\n.\n\n(112)\n\n(113)\n\n(114)\n\nare achievable. Comparing upper bounds and achievable rates, we conclude that the upper bounds are indeed tight. In fact,\nthis property is first observed by Etkin et al. in [13]. We summarize this result in the following theorem:\nTheorem 4: The sum capacity in Class B is attained when transmitters use Gaussian codebooks and receivers treat the\ninterference as noise. In this case, the sum capacity is\n\u0012\n\u0013\n\u0012\n\u0013\nP1\nP2\nP1\nP2\n\u2032\nCsum =\u03b3\n+\u03b3\n.\n(115)\n+ \u2032\n+ \u2032\nN11\na P2 + N12\nN22\nb P1 + N21\nProof: By substituting \u03bc = 1 in (111), we obtain the desired result.\n4) Class C: Class C is designed to upper bound \u03c3C (\u03bc, 1) for the mixed Gaussian IC where 1 \u2264 b. Class C is similar to\nClass A1 (see Figure 4), however we impose different constraints on the parameters of the channels within Class C. These\nconstraints assist us in providing upper bounds by using the fact that at one of the receivers both signals are decodable.\nFor channels in Class C, we use the same model that is given in (73). Therefore, similar to channels in Class A1, this\nchannel is admissible if the corresponding parameters satisfy\nb\u2032 g2\n\u221a 2\n(1 \u2212 g2 ) N22 + g2 N21\nNext, we change the constraints in (79) as\n\nb\u2032\naN22\n\n\u2265 N21 ,\n\u2264 1.\n\n= b,\n= 1.\n\n(116)\n\n(117)\n\nThrough this change of constraints, the second receiver after decoding its own signal will have a less noisy version of the\nfirst user's signal, and consequently, it is able to decode the signal of the first user as well as its own signal. Relying on this\nobservation, we have the following lemma.\n\n\f17\n\nLemma 6: For a channel in Class C, we have\n\u0013\u0013\n\u0012\n\u0012\n1\n\u03bc\u22121\nP2 N22\n+ b\u2032 P1 + N21\nlog (2\u03c0e(P1 + aP2 + 1)) + log 2\u03c0e\n2\n2\nP2 + N22\n1\n1\n\u2212 log(2\u03c0eN21 ) \u2212 log(2\u03c0eN22 ) + fh (P2 , N22 , 1, a, \u03bc \u2212 1).\n2\n2\nProof: Since the second user is able to decode both users' messages, we have\n\u03c3C \u2032 (\u03bc, 1) \u2264\n\n1\nI(xn1 ; \u1ef91n ),\nn\n1\nn\nn\nR1 \u2264 I(xn1 ; \u1ef921\n, \u1ef922\n|xn2 ),\nn\n1\nn\nn\nR2 \u2264 I(xn2 ; \u1ef921\n, \u1ef922\n|xn1 ),\nn\n1\nn\nn\n, \u1ef922\n).\nR1 + R2 \u2264 I(xn1 , xn2 ; \u1ef921\nn\nn\nn\nn\n\u2264 1, we have I(xn1 ; \u1ef91n ) \u2264 I(xn1 ; \u1ef921\n|xn2 ) = I(xn1 ; \u1ef921\n, \u1ef922\n|xn2 ). Hence, (120) is redundant. It can be shown\nR1 \u2264\n\nFrom aN22\n\n\u03bcR1 + R2 \u2264\n\n\u03bc\u22121\n1\nn\nn\nI(xn1 ; \u1ef91n ) + I(xn1 , xn2 ; \u1ef921\n, \u1ef922\n).\nn\nn\n\n(118)\n\n(119)\n(120)\n(121)\n(122)\nthat\n(123)\n\nHence, we have\n\u03bc\u22121\n\u03bc\u22121\n1\n1\nn\nn\nn\nn\nh(\u1ef91n ) \u2212\nh(\u1ef91n |xn1 ) + h(\u1ef921\n, \u1ef922\n) \u2212 h(\u1ef921\n, \u1ef922\n|xn1 , xn2 )\nn\nn\nn\nn\n1\n1\n\u03bc\u22121\nn\nn\nn\nn\nh(\u1ef91n ) + h(\u1ef921\n|\u1ef922\n) \u2212 h(\u1ef921\n, \u1ef922\n|xn1 , xn2 )\n=\nn\nn\nn\n\u0014\n\u0015\n1\n\u03bc\u22121\nn\n+\nh(\u1ef922\n)\u2212\nh(\u1ef91n |xn1 )\nn\nn\n\n\u03bcR1 + R2 \u2264\n\nNext, we bound the different terms in (124). For the first term, we have\n\u03bc\u22121\n\u03bc\u22121\nh(\u1ef91n ) \u2264\nlog (2\u03c0e(P1 + aP2 + 1)) .\nn\n2\nThe second term can be bounded as\n\u0012\n\u0012\n\u0013\u0013\nP2 N22\n1\n1\n\u2032\nn\nn\n+ b P1 + N21\nh(\u1ef921 |\u1ef922 ) \u2264 log 2\u03c0e\n.\nn\n2\nP2 + N22\n\n(124)\n\n(125)\n\n(126)\n\nThe third term can be bounded as\n1\n1\n1\nn\nn\nh(\u1ef921\n, \u1ef922\n|xn1 , xn2 ) = log(2\u03c0eN21 ) + log(2\u03c0eN22 ).\nn\n2\n2\nThe last terms can be bounded as\n\u03bc\u22121\n1\n\u03bc\u22121 \u221a n\n1\nn\nn\nh(\u1ef922\n)\u2212\nh(\u1ef91n |xn1 )= h(xn2 + z22\n)\u2212\nh( ax2 + z1 )\nn\nn\nn\nn\n\u2264 fh (P2 , N22 , 1, a, \u03bc \u2212 1).\n\n(127)\n\n(128)\n(129)\n\nAdding all inequalities, we obtain the desired result.\nIV. W EAK G AUSSIAN I NTERFERENCE C HANNEL\nIn this section, we focus on the weak Gaussian IC. We first obtain the sum capacity of this channel for a certain range of\nparameters. Then, we obtain an outer bound on the capacity region which is tighter than the previously known outer bounds.\nFinally, we show that time-sharing and concavification result in the same achievable region for Gaussian codebooks.\n\n\f18\n\nA. Sum Capacity\nIn this subsection, we use the Class B channels to obtain the sum capacity of the weak IC for a certain range of parameters.\nTo this end, let us consider the following minimization problem:\n\u0012\n\u0013\n\u0012\n\u0013\nP1\nP1\nP2\nP2\nW =min \u03b3\n+\u03b3\n(130)\n+ \u2032\n+ \u2032\nN11\na P2 + N12\nN22\nb P1 + N21\nsubject to:\na\u2032 g 1 = a\nb\u2032 g2 = b\nb\u2032 N11 \u2264 N21\n\na\u2032 N22 \u2264 N12\n\u221a\n(1 \u2212 g1 )2 N11 + g1 N12 = 1\n\u221a\n(1 \u2212 g2 )2 N22 + g2 N21 = 1\n0 \u2264 [a\u2032 , b\u2032 , g1 , g2 , N11 , N12 , N22 , N21 ].\nThe objective function in (130) is the sum capacity of Class B channels obtained in Theorem 4. The constraints are the\ncombination of (100) and (101) where applied to confirm the admissibility of the channel and to validate the sum capacity\nresult. Since every channel in the class is admissible, we have Csum \u2264 W . Substituting S1 = g1 N12 and S2 = g2 N21 , we\nhave\n\u0012\n\u0013\n\u0012\n\u0013\n\u221a\n\u221a\n(1 \u2212 g1 )2 P1\n(1 \u2212 g2 )2 P2\ng1 P1\ng2 P2\nW =min \u03b3\n+\u03b3\n(131)\n+\n+\n1 \u2212 S1\naP2 + S1\n1 \u2212 S2\nbP1 + S2\nsubject to:\nb(1 \u2212 S1 )\n\u2264 S2 < 1\n\u221a\n(1 \u2212 g1 )2\na(1 \u2212 S2 )\n\u2264 S1 < 1\n\u221a\n(1 \u2212 g2 )2\n0 < [g1 , g2 ].\nBy first minimizing with respect to g1 and g2 , the optimization problem (131) can be decomposed as\nW =min W1 + W2\n\n(132)\n\nsubject to: 0 < S1 < 1, 0 < S2 < 1.\nwhere W1 is defined as\n\n\u0013\n\u221a\n(1 \u2212 g1 )2 P1\ng1 P1\nW1 =min \u03b3\n+\ng1\n1 \u2212 S1\naP2 + S1\nb(1 \u2212 S1 )\n\u221a\nsubject to:\n\u2264 (1 \u2212 g1 )2 , 0 < g1 .\nS2\n\u0012\n\nSimilarly, W2 is defined as\n\n\u0013\n\u221a\n(1 \u2212 g2 )2 P2\ng2 P2\nW2 =min \u03b3\n+\ng2\n1 \u2212 S2\nbP1 + S2\na(1 \u2212 S2 )\n\u221a\n\u2264 (1 \u2212 g2 )2 , 0 < g2 .\nsubject to:\nS1\n\n(133)\n\n\u0012\n\n(134)\n\nThe optimization problems (133) and (134) are easy to solve. In fact, we have\n\uf8f1 \u0010\n\u0011\n\u221a\np\nP1\n\uf8f4\nif b(1 + aP2 ) \u2264 S2 (1 \u2212 S1 )\n\uf8f2 \u03b3 1+aP\n2\n\u0012\n\u0013\n\u221a\nW1 =\n(1\u2212 b(1\u2212S1 )/S2 )2 P1\nbP1\n\uf8f4\n\u03b3\n+\nOtherwise\n\uf8f3\nS2\naP2 +S1\n\n(135)\n\n\uf8f1 \u0010\n\u0011\nP2\n\uf8f4\n\uf8f2 \u03b3 1+bP\n1\n\u0013\n\u0012\n\u221a\nW2 =\n(1\u2212 a(1\u2212S2 )/S1 )2 P2\naP2\n\uf8f4\n\uf8f3 \u03b3 S1 +\nbP1 +S2\n\nif\n\np\n\u221a\na(1 + bP1 ) \u2264 S1 (1 \u2212 S2 )\n\nOtherwise\n\n(136)\n\n\f19\n\np\n\u221a\n\u221a\nS2 (1 \u2212 S1 ) and a(1 + bP1 ) \u2264\npFrom (135) and (136), we observe that for S1 and S2 satisfying b(1 + aP2 ) \u2264\nS1 (1 \u2212 S2 ), the objective function becomes independent of S1 and S2 . In this case, we have\n\u0013\n\u0012\n\u0013\n\u0012\nP2\nP1\n+\u03b3\n,\n(137)\nW =\u03b3\n1 + aP2\n1 + bP1\nwhich is achievable by treating interference as noise. In the following theorem, we prove that it is possible to find a certain\nrange of parameters such that there exist S1 and S2 yielding (137).\nTheorem 5: The sum capacity of the two-user Gaussian IC is\n\u0012\n\u0013\n\u0012\n\u0013\nP1\nP2\nCsum = \u03b3\n+\u03b3\n,\n(138)\n1 + aP2\n1 + bP1\nfor the range of parameters satisfying\n\n\u221a\n\u221a\n\u221a\n\u221a\n1\u2212 a\u2212 b\n\u221a\n.\n(139)\nbP1 + aP2 \u2264\nab\nProof: Let us fix a and b, and define D as\n(\n)\np\np\nS1 (1 \u2212 S2 ) 1\nS2 (1 \u2212 S1 ) 1\n\u221a\n\u221a\nD = (P1 , P2 )|P1 \u2264\n\u2212 , P2 \u2264\n(140)\n\u2212 , 0 < S1 < 1, 0 < S2 < 1 .\nb a\nb\na\na b\n\u221a\np\nIn fact, if D is p\nfeasible then there exist 0 < S1 < 1 and 0 < S2 < 1 satisfying b(1 + aP2 ) \u2264\nS2 (1 \u2212 S1 ) and\n\u221a\na(1 + bP1 ) \u2264 S1 (1 \u2212 S2 ). Therefore, the sum capacity of the channel for all feasible points is attained due to (137).\nWe claim that D = D\u2032 , where D\u2032 is defined as\n(\n\u221a )\n\u221a\n\u221a\n\u221a\n1\u2212 a\u2212 b\n\u2032\n\u221a\nD = (P1 , P2 )| bP1 + aP2 \u2264\n.\n(141)\nab\nTo show D\u2032 \u2286 D, we set S1 = 1 \u2212 S2 in (140) to get\n\u001a\n\u001b\nS1\n1 \u2212 S1\n1\n1\n(P1 , P2 )|P1 \u2264 \u221a \u2212 , P2 \u2264 \u221a \u2212 , 0 < S1 < 1 \u2286 D.\na\nb a b\na b\n\n(142)\n\nIt is easy to show that the left hand side of the above equation is another representation of the region D\u2032 . Hence, we have\nD\u2032 \u2286 D.\n\u221a\n\u221a\n\u221a\n\u221a\nb\nholds. To this end, we introduce\nTo show D \u2286 D\u2032 , it suffices to prove that for any (P1 , P2 ) \u2208 D, bP1 + aP2 \u2264 1\u2212 \u221aa\u2212\nab\nthe following maximization problem:\n\u221a\n\u221a\n(143)\nJ = max\nbP1 + aP2 ,\n(P1 ,P2 )\u2208D\n\nwhich can be written as\nJ=\n\nmax\n\n(S1 ,S2 )\u2208(0,1)2\n\np\np\nS1 (1 \u2212 S2 ) + S2 (1 \u2212 S1 )\n1\n1\n\u221a\n\u2212\u221a \u2212\u221a .\na\nab\nb\n\n(144)\n\nIt is easy to show that the solution to the above optimization problem is\n1\n1\n1\nJ=\u221a \u2212\u221a \u2212\u221a .\na\nab\nb\n\n(145)\n\nHence, we deduce that D \u2286 D\u2032 . This completes the proof.\nRemark 3: The above sum capacity result for the weak Gaussian IC (see also [24]) has been established independently in\n[25] and [26].\nAs an example, let us consider the symmetric Gaussian IC. In this case, the constraint in (139) becomes\n\u221a\n1\u22122 a\n\u221a .\nP \u2264\n(146)\n2a a\n\u221a\nIn Figure 7, the admissible region for P , where treating interference as noise is optimal, versus a is plotted. For a fixed P\nand all 0 \u2264 a \u2264 1, the upper bound in (130) and the lower bound when receivers treat the interference as noise are plotted in\nFigure 8. We observe that up to a certain value of a, the upper bound coincides with the lower bound.\n\n\f20\n\na\nFig. 7.\n\nThe shaded area is the region where treating interference as noise is optimal for obtaining the sum capacity of the symmetric Gaussian IC.\n\nP2\n\n7\n\nR1 \u000e R2\n\nP1\n\na\nFig. 8.\n\nThe upper bound obtained by solving (130). The lower bound is obtained by treating the interference as noise.\n\n\f21\n\nB. New Outer Bound\nFor the weak Gaussian IC, there are two outer bounds that are tighter than the other known bounds. The first one, due to\nKramer [12], is obtained by relying on the fact that the capacity region of the Gaussian IC is inside the capacity regions of\nthe two underlying one-sided Gaussian ICs. Even though the capacity region of the one-sided Gaussian IC is unknown, there\nexists an outer bound for this channel that can be used instead. Kramers' outer bound is the intersection of two regions E1\nand E2 . E1 is the collection of all rate pairs (R1 , R2 ) satisfying\n\u0012\n\u0013\n(1 \u2212 \u03b2)P \u2032\nR1 \u2264 \u03b3\n,\n(147)\n\u03b2P \u2032 + 1/a\nR2 \u2264 \u03b3(\u03b2P \u2032 ),\n(148)\nfor all \u03b2 \u2208 [0, \u03b2max ], where P \u2032 = P1 /a + P2 and \u03b2max =\nsatisfying\n\nP2\nP \u2032 (1+P1 ) .\n\nSimilarly, E2 is the collection of all rate pairs (R1 , R2 )\n\nR1 \u2264 \u03b3(\u03b1P \u2032\u2032 ),\n\u0013\n\u0012\n(1 \u2212 \u03b1)P \u2032\u2032\n,\nR2 \u2264 \u03b3\n\u03b1P \u2032\u2032 + 1/b\n\n(149)\n(150)\n\nP1\nfor all \u03b1 \u2208 [0, \u03b1max ], where P \u2032\u2032 = P1 + P2 /b and \u03b1max = P \u2032\u2032 (1+P\n.\n2)\nThe second outer bound, due to Etkin et al. [13], is obtained by using Genie aided technique to upper bound different\nlinear combinations of rates that appear in the HK achievable region. Their outer bound is the union of all rate pairs (R1 , R2 )\nsatisfying\n\nR1 \u2264 \u03b3(P1 ),\n\nR2 \u2264 \u03b3(P2 ),\n\n(151)\n(152)\n\u0012\n\n\u0013\nP2\n,\n1 + bP1\n\u0013\n\u0012\nP1\n,\nR1 + R2 \u2264 \u03b3(P2 ) + \u03b3\n1 + aP2\n\u0013\n\u0012\n\u0013\n\u0012\nP2\nP1\n+ \u03b3 bP1 +\n,\nR1 + R2 \u2264 \u03b3 aP2 +\n1 + bP1\n1 + aP2\n\u0013\n\u0012\n\u0013\n\u0012\n1 + P1\nP2\n+ 0.5 log\n,\n2R1 + R2 \u2264 \u03b3(P1 + aP2 ) + \u03b3 bP1 +\n1 + aP2\n1 + bP1\n\u0013\n\u0012\n\u0013\n\u0012\n1 + P2\nP1\n+ 0.5 log\n.\nR1 + 2R2 \u2264 \u03b3(bP1 + P2 ) + \u03b3 aP2 +\n1 + bP1\n1 + aP2\nR1 + R2 \u2264 \u03b3(P1 ) + \u03b3\n\n(153)\n(154)\n(155)\n(156)\n(157)\n\nIn the outer bound proposed here, we derive an upper bound on all linear combinations of the rates. Recall that to obtain the\nboundary points of the capacity region C , it suffices to calculate \u03c3C (\u03bc, 1) and \u03c3C (1, \u03bc) for all 1 \u2264 \u03bc. To this end, we make\nuse of channels in A1 and B classes and channels in A2 and B classes to obtain upper bounds on \u03c3C (\u03bc, 1) and \u03c3C (1, \u03bc),\nrespectively.\nIn order to obtain an upper bound on \u03c3C (\u03bc, 1), we introduce two optimization problems as follows. The first optimization\nproblem is written as\n\u0013\n\u0012\n\u03bc1\nb\u2032 P1\nP2\n\u03bc2\n1\nN21\nW1 (\u03bc) =min\n(158)\n+\n+\nlog [2\u03c0e(P1 + aP2 + 1)] \u2212\nlog(2\u03c0e) + log\n2\n2\n2\nN22\nN22\nP2 + N22\n\u0013\n\u0012\n1\n+ fh (P2 , N22 , 1, a, \u03bc1 )\n+ \u03bc2 fh P1 , 1, N21 , b\u2032 ,\n\u03bc2\nsubject to:\n\u03bc1 + \u03bc2 = \u03bc\nb\u2032 g2 = b\nb\u2032 \u2264 N21\n\naN22 \u2264 1\n\u221a\n(1 \u2212 g2 )2 N22 + g2 N21 = 1\n\n0 \u2264 [\u03bc1 , \u03bc2 , b\u2032 , g2 , N22 , N21 ].\n\nIn fact, the objective of the above minimization problem is an upper bound on the support function of a channel within Class\nA1 which is obtained in Lemma 3. The constraints are the combination of (78) and (79) which are applied to guarantee the\n\n\f22\n\nadmissibility of the channel and to validate the upper bound obtained in Lemma 3. Hence, \u03c3C (\u03bc, 1) \u2264 W1 (\u03bc). By using a\n\u221a\nnew variable S = (1 \u2212 g2 )2 N22 , we obtain\n\u0014\n\u0015\n1\n\u221a 2 1 \u2212 S + bP1\nP2\n\u03bc1\nlog [2\u03c0e(P1 + aP2 + 1)] + log (1 \u2212 g2 ) (\n+\n)\nW1 (\u03bc) =min\n\u221a\n2\n2\ng2 S\n(1 \u2212 g2 )2 P2 + S\n\u0012\n\u0013\n1\u2212S b 1\nS\n\u03bc2\n+ fh (P2 ,\n, ,\n, 1, a, \u03bc1 ) \u2212\n+ \u03bc2 fh P1 , 1,\nlog(2\u03c0e)\n\u221a\ng2 g2 \u03bc2\n(1 \u2212 g2 )2\n2\nsubject to:\n\u03bc1 + \u03bc2 = \u03bc\n\n(159)\n\nS \u22641\u2212b\n\u221a\n(1 \u2212 g2 )2\nS\u2264\na\n0 \u2264 [\u03bc1 , \u03bc2 , S, g2 ].\nThe second optimization problem is written as\n\u0012\n\u0013\n\u0012\n\u0013\nP1\nP1\nP2\nP2\nW2 (\u03bc) =min \u03bc\u03b3\n+\u03b3\n+ fh (P2 , N22 , N12 , a\u2032 , \u03bc)\n+ \u2032\n+ \u2032\nN11\na P2 + N12\nN22\nb P1 + N21\n1\n\u03bc\n+ log((2\u03c0e)(a\u2032 P2 + N12 )) \u2212 log((2\u03c0e)(P2 + N22 ))\n2\n2\nsubject to:\na\u2032 g 1 = a\n\n(160)\n\nb\u2032 g2 = b\nb\u2032 N11 \u2264 N21\n\na\u2032 N22 \u2264 N12\n\u221a\n(1 \u2212 g1 )2 N11 + g1 N12 = 1\n\u221a\n(1 \u2212 g2 )2 N22 + g2 N21 = 1\n0 \u2264 [a\u2032 , b\u2032 , g1 , g2 , N11 , N12 , N22 , N21 ].\nFor this problem, Class B channels are used. In fact, the objective is the upper bound on the support function of channels\nwithin the class obtained in Lemma 5 and the constraints are defined to obtain the closed form formula for the upper bound\nand to confirm that the channels are admissible. Hence, we deduce \u03c3C (\u03bc, 1) \u2264 W2 (\u03bc). By using new variables S1 = g1 N12\nand S2 = g2 N21 , we obtain\n\u0013\n\u0012\n\u0013\n\u221a\n\u221a\n(1 \u2212 g2 )2 P2\n(1 \u2212 g1 )2 P1\ng1 P1\ng2 P2\n+\u03b3\n(161)\n+\n+\nW2 (\u03bc) =min \u03bc\u03b3\n1 \u2212 S1\naP2 + S1\n1 \u2212 S2\nbP1 + S2\n\u0012\n\u0013\n\u0013\n\u0013\n\u0012\n\u0012\n1 \u2212 S2\nS1 a\n\u03bc\n1\naP2 + S1\n1 \u2212 S2\n+ fh P2 ,\n,\n,\n,\n\u03bc\n+\n)\n\u2212\n)\nlog\n(2\u03c0e)(\nlog\n(2\u03c0e)(P\n+\n\u221a\n\u221a\n2\n(1 \u2212 g2 )2 g1 g1\n2\ng1\n2\n(1 \u2212 g2 )2\nsubject to:\nb(1 \u2212 S1 )\n\u2264 S2 < 1\n\u221a\n(1 \u2212 g1 )2\na(1 \u2212 S2 )\n\u2264 S1 < 1\n\u221a\n(1 \u2212 g2 )2\n0 < [g1 , g2 ].\n\u0012\n\nIn a similar fashion, one can introduce two other optimization problems, say W\u03031 (\u03bc) and W\u03032 (\u03bc), to obtain upper bounds on\n\u03c3C (1, \u03bc) by using the upper bounds on the support functions of channels in Class A2 and Class B.\nTheorem 6 (New Outer Bound): For any rate pair (R1 , R2 ) achievable for the two-user weak Gaussian IC, the inequalities\n\u03bc1 R1 + R2 \u2264 W (\u03bc1 ) = min{W1 (\u03bc1 ), W2 (\u03bc1 )},\nR1 + \u03bc2 R2 \u2264 W\u0303 (\u03bc2 ) = min{W\u03031 (\u03bc2 ), W\u03032 (\u03bc2 )},\n\n(162)\n(163)\n\nhold for all 1 \u2264 \u03bc1 , \u03bc2 .\nTo obtain an upper bound on the sum rate, we can apply the following inequality:\nCsum \u2264 min\n\n1\u2264\u03bc1 ,\u03bc2\n\n(\u03bc2 \u2212 1)W (\u03bc1 ) + (\u03bc1 \u2212 1)W\u0303 (\u03bc2 )\n.\n\u03bc1 \u03bc2 \u2212 1\n\n(164)\n\n\fR2\n\n23\n\nr5\u2032\n\nr4\u2032\n\nr3\u2032\n\nr4\nR2 = \u03c82\n\nr6\u2032\n\nr3\nR1 + 2R2 = \u03c85\nR1 + R2 = \u03c83\n\nr2\u2032\n\nr2\n\nr1\u2032\n\n2R1 + R2 = \u03c84\nr1\nR1 = \u03c81\n\nR1\nFig. 9.\n\nG0 for the weak Gaussian IC. r1 , r2 , r3 , and r4 are extreme points of G0 in the interior of the first quadrant.\n\nC. Han-Kobayashi Achievable region\nIn this sub-section, we aim at characterizing G for the weak Gaussian IC. To this end, we first investigate some properties of\nG0 (P1 , P2 , \u03b1, \u03b2). First of all, we show that none of the inequalities in describing G0 is redundant. In Figure 9, \u0010all possible extreme\n\u0011\npoints are shown. It is easy to prove that ri\u2032 \u2208\n/ G0 for i \u2208 {1, 2, . . . , 6}. For instance, we consider r6\u2032 = 2\u03c843\u2212\u03c85 , 2\u03c853\u2212\u03c84 .\nSince \u03c831 + \u03c832 + \u03c833 = \u03c84 + \u03c85 (see Section II.C), we have\n\u03c83 = min{\u03c831 , \u03c832 , \u03c833 }\n1\n\u2264 (\u03c831 + \u03c832 + \u03c833 )\n3\n1\n= (\u03c84 + \u03c85 ).\n3\nHowever, 13 (\u03c84 + \u03c85 ) is the sum of the components of r6\u2032 . Therefore, r6\u2032 violates (7) in the definition of the HK achievable\nregion. Hence, r6\u2032 \u2208\n/ G0 . As another example, let us consider r1\u2032 = (\u03c81 , \u03c83 \u2212 \u03c81 ). We claim that r1\u2032 violates (8). To this end,\nwe need to show that \u03c84 \u2264 \u03c83 + \u03c81 . However, it is easy to see that \u03c84 \u2264 \u03c831 + \u03c81 , \u03c84 \u2264 \u03c832 + \u03c81 , and \u03c84 \u2264 \u03c833 + \u03c81\nreduce to 0 \u2264 (1 \u2212 \u03b1)(1 \u2212 b + \u03b2(1 \u2212 ab)P2 ), 0 \u2264 (1 \u2212 \u03b2)(1 \u2212 a + (1 \u2212 ab)P1 ), and 0 \u2264 (1 \u2212 \u03b1)(1 \u2212 \u03b2)aP2 , respectively.\nTherefore, r1\u2032 \u2208\n/ G0 .\nWe conclude that G has four extreme points in the interior of the first quadrant, namely\nr1 = (\u03c81 , \u03c84 \u2212 2\u03c81 ),\nr2 = (\u03c84 \u2212 \u03c83 , 2\u03c83 \u2212 \u03c84 ),\n\n(165)\n(166)\n\nr4 = (\u03c85 \u2212 2\u03c82 , \u03c82 ).\n\n(168)\n\nr3 = (2\u03c83 \u2212 \u03c85 , \u03c85 \u2212 \u03c83 ),\n\n(167)\n\nMost importantly, G0 possesses the unique minimizer property. To prove this, we need to show that \u0177, the minimizer of the\noptimization problem\n\u03c3D0 (c1 , c2 , P1 , P2 , \u03b1, \u03b2)= max{c1 R1 + c2 R2 |AR \u2264 \u03a8(P1 , P2 , \u03b1, \u03b2)}\n\n= min{yt \u03a8(P1 , P2 , \u03b1, \u03b2)|At y = (c1 , c2 )t , y \u2265 0},\n\n(169)\n\nis independent of the parameters P1 , P2 , \u03b1, and \u03b2 and only depends on c1 and c2 . We first consider the case (c1 , c2 ) = (\u03bc, 1)\nfor all 1 \u2264 \u03bc. It can be shown that for 2 < \u03bc, the maximum of (169) is attained at r1 regardless of P1 , P2 , \u03b1, and \u03b2. Therefore,\nthe dual program has the minimizer \u0177 = (\u03bc \u2212 2, 0, 0, 1, 0)t which is clearly independent of P1 , P2 , \u03b1, and \u03b2. In this case, we\nhave\n(170)\n\u03c3D0 (\u03bc, 1, P1 , P2 , \u03b1, \u03b2) = (\u03bc \u2212 2)\u03c81 + \u03c84 , 2 < \u03bc.\n\nFor 1 \u2264 \u03bc \u2264 2, one can show that r2 and \u0177 = (0, 0, 2\u2212\u03bc, \u03bc\u22121, 0)t are the maximizer and the minimizer of (169), respectively.\nIn this case, we have\n(171)\n\u03c3D0 (\u03bc, 1, P1 , P2 , \u03b1, \u03b2) = (2 \u2212 \u03bc)\u03c83 + (\u03bc \u2212 1)\u03c84 , 1 \u2264 \u03bc \u2264 2.\n\nNext, we consider the case (c1 , c2 ) = (1, \u03bc) for all 1 \u2264 \u03bc. Again, it can be shown that for 2 < \u03bc and 1 \u2264 \u03bc \u2264 2,\n\u0177 = (0, \u03bc \u2212 2, 0, 0, 1)t and \u0177 = (0, 0, 2 \u2212 \u03bc, 0, \u03bc \u2212 1)t minimizes (169), respectively. Hence, we have\nif 2 < \u03bc,\n\u03c3D0 (1, \u03bc, P1 , P2 , \u03b1, \u03b2)= (\u03bc \u2212 2)\u03c82 + \u03c85 ,\n\u03c3D0 (1, \u03bc, P1 , P2 , \u03b1, \u03b2)= (2 \u2212 \u03bc)\u03c83 + (\u03bc \u2212 1)\u03c85 , if 1 \u2264 \u03bc \u2264 2.\n\n(172)\n(173)\n\n\f24\n\nFig. 10.\n\nComparison between different bounds for the symmetric weak Gaussian IC when P = 7 and a = 0.2.\n\nWe conclude that the solutions of the dual program are always independent of P1 , P2 , \u03b1, and \u03b2. Hence, G0 possesses the\nunique minimizer property.\nTheorem 7: For the two-user weak Gaussian IC, time-sharing and concavification result in the same region. In other words,\nG can be fully characterized by using TD/FD and allocating power over three different dimensions.\nProof: Since G0 possesses the unique minimizer property, from Theorem 1, we deduce that G = G2 . Moreover, using\nTheorem 3, the number of frequency bands is at most three.\nTo obtain the support function of G2 , we need to obtain g(c1 , c2 , P1 , P2 , \u03b1, \u03b2) defined in (43). Since G0 possesses the unique\nminimizer property, (43) can be simplified. Let us consider the case where (c1 , c2 ) = (\u03bc, 1) for \u03bc > 2. It can be shown that\nfor this case\ng = max 2 (\u03bc \u2212 2)\u03c81 (P1 , P2 , \u03b1, \u03b2) + \u03c84 (P1 , P2 , \u03b1, \u03b2).\n(174)\n(\u03b1,\u03b2)\u2208[0,1]\n\nSubstituting into (42), we obtain\n\u03c3G2 (\u03bc, 1, P1 , P2 ) =max\n\n3\nX\ni=1\n\n\u03bbi [(\u03bc \u2212 2)\u03c81 (P1i , P2i , \u03b1i , \u03b2i ) + \u03c84 (P1i , P2i , \u03b1i , \u03b2i )]\n\n(175)\n\nsubject to:\n3\nX\n\u03bbi = 1\ni=1\n\n3\nX\n\ni=1\n3\nX\ni=1\n\n\u03bbi P1i \u2264 P1\n\u03bbi P2i \u2264 P2\n\n0 \u2264 \u03bbi , 0 \u2264 P1i , 0 \u2264 P2i , \u2200i \u2208 {1, 2, 3}\n0 \u2264 \u03b1i \u2264 1, 0 \u2264 \u03b2i \u2264 1, \u2200i \u2208 {1, 2, 3}.\n\nFor other ranges of (c1 , c2 ), a similar optimization problem can be formed. It is worth noting that even though the number\nof parameters in characterizing G is reduced, it is still prohibitively difficult to characterize boundary points of G . In Figures\n(10) and (11), different bounds for the symmetric weak Gaussian IC are plotted. As shown in these figures, the new outer\nbound is tighter than the previously known bounds.\nV. O NE - SIDED G AUSSIAN I NTERFERENCE C HANNELS\nThroughout this section, we consider the one-sided Gaussian IC obtained by setting b = 0, i.e, the second receiver incurs no\ninterference from the first transmitter. One can further split the class of one-sided ICs into two subclasses: the strong one-sided\n\n\f25\n\nFig. 11.\n\nComparison between different bounds for the symmetric weak Gaussian IC when P = 100 and a = 0.1.\n\nIC and the weak one-sided IC. For the former, a \u2265 1 and the capacity region is fully characterized [16]. In this case, the\ncapacity region is the union of all rate pairs (R1 , R2 ) satisfying\nR1 \u2264 \u03b3(P1 ),\n\nR2 \u2264 \u03b3(P2 ),\nR1 + R2 \u2264 \u03b3(P1 + aP2 ).\nFor the latter, a < 1 and the full characterization of the capacity region is still an open problem. Therefore, we always assume\na < 1. Three important results are proved for this channel. The first one, proved by Costa in [11], states that the capacity\nregion of the weak one-sided IC is equivalent to that of the degraded IC with an appropriate change of parameters. The second\none, proved by Sato in [10], states that the capacity region of the degraded Gaussian IC is outer bounded by the capacity\nregion of a certain degraded broadcast channel. The third one, proved by Sason in [16], characterizes the sum capacity by\ncombining Costa's and Sato's results.\nIn this section, we provide an alternative proof for the outer bound obtained by Sato. We then characterize the full HK\nachievable region where Gaussian codebooks are used, i.e., G .\nA. Sum Capacity\nFor the sake of completeness, we \u0010\nfirst\u0010state the\nresult obtained by Sason.\n\u0011 sum capacity\n\u0011\nP1\nTheorem 8 (Sason): The rate pair \u03b3 1+aP2 , \u03b3(P2 ) is an extreme point of the capacity region of the one-sided Gaussian\nIC. Moreover, the sum capacity of the channel is attained at this point.\nB. Outer Bound\nIn [10], Sato derived an outer bound on the capacity of the degraded IC. This outer bound can be used for the weak one-sided\nIC as well. This is due to Costa's result which states that the capacity region of the degraded Gaussian IC is equivalent to that\nof the weak one-sided IC with an appropriate change of parameters.\nTheorem 9 (Sato): If the rate pair (R1 , R2 ) belongs to the capacity region of the weak one-sided IC, then it satisfies\n\u0011\n\u0010\n(1\u2212\u03b2)P\n,\nR1 \u2264 \u03b3 1/a+\u03b2P\n(176)\nR2 \u2264 \u03b3(\u03b2P ),\nfor all \u03b2 \u2208 [0, 1] where P = P1 /a + P2 .\nProof: Since the sum capacity is attained at the point where User 2 transmits at its maximum rate R2 = \u03b3(P2 ), other boundary points of the capacity region can be obtained by characterizing the solutions of \u03c3C (\u03bc, 1) = max {\u03bcR1 + R2 |(R1 , R2 ) \u2208 C }\n\n\f26\n\nfor all 1 \u2264 \u03bc. Using Fano's inequality, we have\n\nn(\u03bcR1 + R2 ) \u2264\u03bcI(xn1 ; y1n ) + I(xn2 ; y2n ) + n\u01ebn\n\n=\u03bch(y1n ) \u2212 \u03bch(y1n |xn1 ) + h(y2n ) \u2212 h(y2n |xn2 ) + n\u01ebn\n\u221a\n\u221a\n=[\u03bch(xn1 + axn2 + z1n ) \u2212 h(z2n )] + [h(xn2 + z2n ) \u2212 \u03bch( axn2 + z1n )] + n\u01ebn\n(a) \u03bcn\n\u221a\nn\n\u2264\nlog [2\u03c0e(P1 + aP2 + 1)] \u2212 log(2\u03c0e) + [h(xn2 + z2n ) \u2212 \u03bch( axn2 + z1n )] + n\u01ebn\n2\n2\n(b) \u03bcn\nn\nlog [2\u03c0e(P1 + aP2 + 1)] \u2212 log(2\u03c0e) + nfh (P2 , 1, 1, a, \u03bc) + n\u01ebn ,\n\u2264\n2\n2\nwhere (a) follows from the fact that Gaussian distribution maximizes the differential entropy for a given constraint on the\ncovariance matrix and (b) follows from the definition of fh in (59).\nDepending on the value of \u03bc, we consider the following two cases:\n1- For 1 \u2264 \u03bc \u2264 PP2 +1/a\n, we have\n2 +1\n\u0012\n\u0013\nP1\n\u03bcR1 + R2 \u2264 \u03bc\u03b3\n+ \u03b3(P2 ).\n(177)\n1 + aP2\n\u0011\n\u0011\n\u0010 \u0010\nP1\n,\n\u03b3(P\n)\nwhich is achievable by treating interference as noise at Receiver 1, satisfies (177) with\nIn fact, the point \u03b3 1+aP\n2\n2\nequality. Therefore, it belongs to the capacity region. Moreover, by setting \u03bc = 1, we deduce that this point corresponds to\nthe sum capacity of the one-sided Gaussian IC. This is in fact an alternative proof for Sason's result.\n2- For PP2 +1/a\n< \u03bc \u2264 a1 , we have\n2 +1\n\u0013\n\u0013\n\u0012\n\u0012\n\u03bc\n\u03bc\n1\n1/a \u2212 1\na\u03bc(1/a \u2212 1)\n\u2212 log\n.\n(178)\n\u03bcR1 + R2 \u2264 log (P1 + aP2 + 1) + log\n2\n2\n\u03bc\u22121\n2\n\u03bc\u22121\n\nEquivalently, we have\n\n\u03bc\n\u03bcR1 + R2 \u2264 log\n2\n\n\u0012\n\n(aP + 1)(\u03bc \u2212 1)\n\u03bc(1 \u2212 a)\n\n\u0013\n\n1\n+ log\n2\n\n\u0012\n\n1/a \u2212 1\n\u03bc\u22121\n\n\u0013\n\n,\n\nwhere P = P1 /a + P2 . Let us define E1 as the set of all rate pairs (R1 , R2 ) satisfying (179), i.e.\n\u0013\n\u0013\n\u001b\n\u001a\n\u0012\n\u0012\n\u03bc\n1\nP2 + 1/a\n1\n(aP + 1)(\u03bc \u2212 1)\n1/a \u2212 1\n+ log\n, \u2200\n.\nE1 = (R1 , R2 )|\u03bcR1 + R2 \u2264 log\n<\u03bc\u2264\n2\n\u03bc(1 \u2212 a)\n2\n\u03bc\u22121\nP2 + 1\na\n\n(179)\n\n(180)\n\nWe claim that E1 is the dual representation of the region defined in the statement of the theorem, see (4). To this end, we\ndefine E2 as\n\u0013\n\u001b\n\u001a\n\u0012\n(1 \u2212 \u03b2)P\n, R2 \u2264 \u03b3(\u03b2P ), \u2200\u03b2 \u2208 [0, 1]\n(181)\nE2 = (R1 , R2 )|R1 \u2264 \u03b3\n1/a + \u03b2P\nWe evaluate the support function of E2 as\n\nIt is easy to show that \u03b2 =\n\n1/a\u22121\nP (\u03bc\u22121)\n\n\u03c3E2 (\u03bc, 1) = max {\u03bcR1 + R2 |(R1 , R2 ) \u2208 E2 } .\n\nmaximizes the above optimization problem. Therefore, we have\n\u0013\n\u0013\n\u0012\n\u0012\n\u03bc\n1\n(aP + 1)(\u03bc \u2212 1)\n1/a \u2212 1\n\u03c3E2 (\u03bc, 1) = log\n+ log\n.\n2\n\u03bc(1 \u2212 a)\n2\n\u03bc\u22121\n\n(182)\n\n(183)\n\nSince E2 is a closed convex set, we can use (4) to obtain its dual representation which is indeed equivalent to (180). This\ncompletes the proof.\nC. Han-Kobayashi Achievable Region\nIn this subsection, we characterize G0 , G1 , G2 , and G for the weak one-sided Gaussian IC. G0 can be characterized as\nfollows. Since there is no link between Transmitter 1 and Receiver 2, User 1's message in the HK achievable region is only\n\n\f27\n\nthe private message, i.e., \u03b1 = 1. In this case, we have\n\u0012\n\u0013\nP1\n\u03c81 = \u03b3\n,\n1 + a\u03b2P2\n\u03c82 = \u03b3(P2 ),\n\u0012\n\u0013\nP1 + a(1 \u2212 \u03b2)P2\n\u03c831 = \u03b3\n+ \u03b3(\u03b2P2 ),\n1 + a\u03b2P2\n\u0013\n\u0012\nP1\n+ \u03b3(P2 ),\n\u03c832 = \u03b3\n1 + a\u03b2P2\n\u0013\n\u0012\nP1 + a(1 \u2212 \u03b2)P2\n+ \u03b3(\u03b2P2 ),\n\u03c833 = \u03b3\n1 + a\u03b2P2\n\u0013\n\u0012\n\u0013\n\u0012\nP1\nP1 + a(1 \u2212 \u03b2)P2\n+\u03b3\n+ \u03b3(\u03b2P2 ),\n\u03c84 = \u03b3\n1 + a\u03b2P2\n1 + a\u03b2P2\n\u0012\n\u0013\nP1 + a(1 \u2212 \u03b2)P2\n\u03c85 = \u03b3(\u03b2P2 ) + \u03b3(P2 ) + \u03b3\n,\n1 + a\u03b2P2\n\n(184)\n(185)\n(186)\n(187)\n(188)\n(189)\n(190)\n\nIt is easy to show that \u03c83 = min{\u03c831 , \u03c832 , \u03c833 } = \u03c831 , \u03c831 + \u03c81 = \u03c84 , \u03c831 + \u03c82 = \u03c85 . Hence, G0 can be represented as all\nrate pairs (R1 , R2 ) satisfying\n\u0012\n\u0013\nP1\nR1 \u2264 \u03b3\n,\n(191)\n1 + a\u03b2P2\nR2 \u2264 \u03b3(P2 ),\n(192)\n\u0013\n\u0012\nP1 + a(1 \u2212 \u03b2)P2\n+ \u03b3(\u03b2P2 ).\n(193)\nR1 + R2 \u2264 \u03b3\n1 + a\u03b2P2\nWe claim that G2 = G . To prove this, we need to show that G0 possesses the unique minimizer property. G0 is a pentagon\nwith two extreme points in the interior of the first quadrant, namely r1 and r2 where\n\u0012 \u0012\n\u0013 \u0012\n\u0013\n\u0013\nP1\n(1 \u2212 \u03b2)aP2\nr1 = \u03b3\n,\u03b3\n+ \u03b3(\u03b2P2 ) ,\n(194)\n1 + a\u03b2P2\n1 + P1 + \u03b2aP2\n\u0013\n\u0013\n\u0012 \u0012\nP1 + a(1 \u2212 \u03b2)P2\n+ \u03b3(\u03b2P2 ) \u2212 \u03b3(P2 ), \u03b3(P2 ) .\n(195)\nr2 = \u03b3\n1 + a\u03b2P2\nUsing above, it can be verified that G0 possesses the unique minimizer property.\nNext, we can use the optimization problem in (42) to obtain the support function of G . However, we only need to consider\n(c1 , c2 ) = (\u03bc, 1) for \u03bc > 1. Therefore, we have\n\u0013\n\u0012\n\u0013\n\u0012\n(1 \u2212 \u03b2)aP2\nP1\n+ \u03b3(\u03b2P2 ) + \u03b3\n.\n(196)\ng(\u03bc, 1, P1 , P2 , \u03b2) = max \u03bc\u03b3\n0\u2264\u03b2\u22641\n1 + \u03b2aP2\n1 + P1 + \u03b2aP2\nSubstituting into (42), we conclude that boundary points of G can be characterized by solving the following optimization\nproblem:\n\u0014 \u0012\n\u0013\n\u0012\n\u0013\u0015\n3\nX\nP1i\n(1 \u2212 \u03b2i )aP2i\nW =max\n\u03bbi \u03bc\u03b3\n+ \u03b3(\u03b2i P2i ) + \u03b3\n(197)\n1 + \u03b2i aP2i\n1 + P1i + \u03b2i aP2i\ni=1\nsubject to:\n3\nX\n\u03bbi = 1\ni=1\n\n3\nX\n\ni=1\n3\nX\ni=1\n\n\u03bbi P1i \u2264 P1\n\u03bbi P2i \u2264 P2\n\n0 \u2264 \u03b2i \u2264 1, \u2200i \u2208 {1, 2, 3}\n0 \u2264 [P1i , P2i , \u03bbi ], \u2200i \u2208 {1, 2, 3}.\nFor the sake of completeness, we provide a simple description for G1 in the next lemma.\n\n\f28\n\n1\n\nFig. 12.\n\nComparison between different bounds for the one-sided Gaussian IC when P1 = 1, P2 = 7, and a = 0.4.\n\nLemma 7: The region G1 can be represented as the collection of all rate pairs (R1 , R2 ) satisfying\n\u0012\n\u0013\nP1\nR1 \u2264 \u03b3\n,\n1 + a\u03b2 \u2032 P2\n\u0013\n\u0012\na(1 \u2212 \u03b2 \u2032 )P2\n,\nR2 \u2264 \u03b3(\u03b2 \u2032 P2 ) + \u03b3\n1 + P1 + a\u03b2 \u2032 P2\n\n(198)\n(199)\n\nfor all \u03b2 \u2032 \u2208 [0, 1]. Moreover, G1 is convex and any point that lies on its boundary can be achieved by using superposition\ncoding and successive decoding.\nProof: Let E denote the set defined in the above lemma. It is easy to show that E is convex and E \u2286 G1 . To prove\nthe inverse inclusion, it suffices to show that the extreme points of G0 , r1 and r2 (see (194) and (195)) are inside E for all\n\u03b2 \u2208 [0, 1]. By setting \u03b2 \u2032 = \u03b2, we see that r1 \u2208 E. To prove r2 \u2208 E, we set \u03b2 \u2032 = 1. We conclude that r2 \u2208 E if the following\ninequality holds\n\u0013\n\u0012\n\u0013\n\u0012\nP1\nP1 + a(1 \u2212 \u03b2)P2\n+ \u03b3(\u03b2P2 ) \u2212 \u03b3(P2 ) \u2264 \u03b3\n,\n(200)\n\u03b3\n1 + a\u03b2P2\n1 + aP2\nfor all \u03b2 \u2208 [0, 1]. However, (200) reduces to 0 \u2264 (1 \u2212 a)(1 \u2212 \u03b2)P2 which holds for all \u03b2 \u2208 [0, 1]. Hence, G1 \u2286 E. Using these\nfacts, it is straightforward to show that the boundary points G1 are achievable by using superposition coding and successive\ndecoding.\nFigure 12 compares different bounds for the one-sided Gaussian IC.\nVI. M IXED G AUSSIAN I NTERFERENCE C HANNELS\nIn this section, we focus on the mixed Gaussian Interference channel. We first characterize the sum capacity of this channel.\nThen, we provide an outer bound on the capacity region. Finally, we investigate the HK achievable region. Without loss of\ngenerality, we assume a < 1 and b \u2265 1.\n\n\f29\n\nA. Sum Capacity\nTheorem 10: The sum capacity of the mixed Gaussian IC with a < 1 and b \u2265 1 can be stated as\n\u001a \u0012\n\u0013 \u0012\n\u0013\u001b\nP1\nbP1\nCsum = \u03b3 (P2 ) + min \u03b3\n,\u03b3\n.\n(201)\n1 + aP2\n1 + P2\nProof: We need to prove the achievability and converse for the theorem.\nAchievability part: Transmitter 1 sends a common message to both receivers, while the first user's signal is considered as\nnoise at both receivers. In this case, the rate\n\u0013 \u0012\n\u0013\u001b\n\u001a \u0012\nbP1\nP1\n,\u03b3\n(202)\nR1 = min \u03b3\n1 + aP2\n1 + P2\nis achievable. At Receiver 2, the signal from Transmitter 1 can be decoded and removed. Therefore, User 2 is left with a\nchannel without interference and it can communicate at its maximum rate which is\nR2 = \u03b3(P2 ).\n\n(203)\n\nBy adding (202) and (203), we obtain the desired result.\nConverse part: The sum capacity of the Gaussian IC is upper bounded by that of the two underlying one-sided Gaussian\nICs. Hence, we can obtain two upper bounds on the sum rate. We first remove the interfering link between Transmitter 1 and\nReceiver 2. In this case, we have a one-sided Gaussian IC with weak interference. The sum capacity of this channel is known\n[16]. Hence, we have\n\u0012\n\u0013\nP1\nCsum \u2264 \u03b3(P2 ) + \u03b3\n.\n(204)\n1 + aP2\nBy removing the interfering link between Transmitter 2 and Receiver 1, we obtain a one-sided Gaussian IC with strong\ninterference. The sum capacity of this channel is known. Hence, we have\nCsum \u2264 \u03b3 (bP1 + P2 ) ,\nwhich equivalently can be written as\nCsum \u2264 \u03b3(P2 ) + \u03b3\n\n\u0012\n\nbP1\n1 + P2\n\n(205)\n\u0013\n\n.\n\n(206)\n\nBy taking the minimum of the right hand sides of (204) and (206), we obtain\n\u001a \u0012\n\u0013 \u0012\n\u0013\u001b\nP1\nbP1\nCsum \u2264 \u03b3 (P2 ) + min \u03b3\n,\u03b3\n.\n1 + aP2\n1 + P2\n\n(207)\n\nThis completes the proof.\nRemark 4: In an independent work [25], the sum capacity of the mixed Gaussian IC is obtained for a certain range of\nparameters, whereas in the above theorem, we characterize the sum capacity of this channel for the entire range of its parameters\n(see also [24]).\n\u0011\n\u0010\n\u0011\n\u0010\nbP1\nP1\nwith\n\u03b3\nBy comparing \u03b3 1+aP\n1+P2 , we observe that if 1 + P2 \u2264 b + abP2 , then the sum capacity corresponds to the\n2\nsum capacity of the one-sided weak Gaussian IC, whereas if 1 + P2 > b + abP2 , then the sum capacity corresponds to the\nsum capacity of the one-sided strong IC. Similar to the one-sided Gaussian IC, since the sum capacity is attained at the point\nwhere User 2 transmits at its maximum rate R2 = \u03b3(P2 ), other boundary points of the capacity region can be obtained by\ncharacterizing the solutions of \u03c3C (\u03bc, 1) = max {\u03bcR1 + R2 |(R1 , R2 ) \u2208 C } for all 1 \u2264 \u03bc.\nB. New Outer Bound\nThe best outer bound to date, due to Etkin et al. [13], is obtained by using the Genie aided technique. This bound is the\nunion of all rate pairs (R1 , R2 ) satisfying\nR1 \u2264 \u03b3(P1 ),\n\nR2 \u2264 \u03b3(P2 ),\n\n(208)\n(209)\n\u0012\n\n\u0013\nP1\n,\n1 + aP2\nR1 + R2 \u2264 \u03b3(P2 + bP1 ),\n\u0012\n2R1 + R2 \u2264 \u03b3(P1 + aP2 ) + \u03b3 bP1 +\nR1 + R2 \u2264 \u03b3(P2 ) + \u03b3\n\n(210)\n(211)\nP2\n1 + aP2\n\n\u0013\n\n+\u03b3\n\n\u0012\n\nP1\n1 + bP1\n\n\u0013\n\n.\n\n(212)\n\n\f30\n\nThe capacity region of the mixed Gaussian IC is inside the intersection of the capacity regions of the two underlying onesided Gaussian ICs. Removing the link between Transmitter 1 and Receiver 2 results in a weak one-sided Gaussian IC whose\nouter bound E1 is the collection of all rate pairs (R1 , R2 ) satisfying\n\u0012\n\u0013\n(1 \u2212 \u03b2)P \u2032\nR1 \u2264 \u03b3\n,\n(213)\n\u03b2P \u2032 + 1/a\nR2 \u2264 \u03b3(\u03b2P \u2032 ),\n(214)\n\nP2\n. On the other hand, removing the link between Transmitter\nfor all \u03b2 \u2208 [0, \u03b2max ], where P \u2032 = P1 /a + P2 and \u03b2max = P \u2032 (1+P\n1)\n2 and Receiver 1 results in a strong one-sided Gaussian IC whose capacity region E2 is fully characterized as the collection\nof all rate pairs (R1 , R2 ) satisfying\n\nR1 \u2264 \u03b3(bP1 ),\n\nR2 \u2264 \u03b3 (P2 ) ,\nR1 + R2 \u2264 \u03b3(bP1 + P2 ).\nUsing the channels in Class C, we upper bound \u03c3C (\u03bc, 1) based on the following optimization problem:\n\u0013\u0013\n\u0012\n\u0012\n\u03bc\u22121\n1\nP2 N22\nW (\u03bc) =min\n+ b\u2032 P1 + N21\nlog (2\u03c0e(P1 + aP2 + 1)) + log 2\u03c0e\n2\n2\nP2 + N22\n1\n1\n\u2212 log(2\u03c0eN21 ) \u2212 log(2\u03c0eN22 ) + fh (P2 , N22 , 1, a, \u03bc \u2212 1)\n2\n2\nsubject to:\nb\u2032 g2 = b\n\n(215)\n(216)\n(217)\n\n(218)\n\nb\u2032 \u2265 N21\naN22 \u2264 1\n\u221a\n(1 \u2212 g2 )2 N22 + g2 N21 = 1\n0 \u2264 [b\u2032 , g2 , N22 , N21 ].\n\nBy substituting S = g2 N21 , we obtain\n\u0013\u0013\n\u0012\n\u0012\n1\nP2 (1 \u2212 S)\n\u03bc\u22121\nbP1 + S\nlog (2\u03c0e(P1 + aP2 + 1)) + log 2\u03c0e\nW (\u03bc) =min\n+\n\u221a\n2\n2\n(1 \u2212 g2 )2 P2 + 1 \u2212 S\ng2\n\u0013\n\u0012\n\u0012\n\u0012\n\u0013\n\u0013\n1\n1\n2\u03c0eS\n2\u03c0e(1 \u2212 S)\n1\u2212S\n\u2212 log\n\u2212 log\n+ fh P2 ,\n, 1, a, \u03bc \u2212 1\n\u221a\n\u221a\n2\ng2\n2\n(1 \u2212 g2 )2\n(1 \u2212 g2 )2\nsubject to:\n\n(219)\n\nS<1\n\u221a\na(1 \u2212 S) \u2264 (1 \u2212 g2 )2\n0 \u2264 [S, g2 ].\n\nHence, we have the following theorem that provides an outer bound on the capacity region of the mixed Gaussian\nIC.\nT\nTheorem 11: For any rate pair (R1 , R2 ) achievable for the two-user mixed Gaussian IC, (R1 , R2 ) \u2208 E1 E2 . Moreover,\nthe inequality\n\u03bcR1 + R2 \u2264 W (\u03bc)\n(220)\nholds for all 1 \u2264 \u03bc.\nC. Han-Kobayashi Achievable Region\nIn this subsection, we study the HK achievable region for the mixed Gaussian IC. Since Receiver 2 can always decode the\nmessage of the first user, User 1 associates all its power to the common message. User 2, on the other hand, allocates \u03b2P2\n\n\f31\n\nR2\nr4\n\u03b3(P2 )\n\nr3\nG0\u2032\nr2\nAlternating Regions\n\nr1\nR1\n\nFig. 13.\n\nThe new region G0\u2032 which is obtained by enlarging G0 .\n\nand (1 \u2212 \u03b2)P2 of its total power to its private and common messages, respectively, where \u03b2 \u2208 [0, 1]. Therefore, we have\n\u0013\n\u0012\nP1\n,\n(221)\n\u03c81 = \u03b3\n1 + a\u03b2P2\n\u03c82 = \u03b3(P2 ),\n(222)\n\u0012\n\u0013\nP1 + a(1 \u2212 \u03b2)P2\n\u03c831 = \u03b3\n+ \u03b3(\u03b2P2 ),\n(223)\n1 + a\u03b2P2\n\u03c832 = \u03b3(P2 + bP1 ),\n(224)\n\u0013\n\u0012\na(1 \u2212 \u03b2)P2\n+ \u03b3(\u03b2P2 + bP1 ),\n(225)\n\u03c833 = \u03b3\n1 + a\u03b2P2\n\u0012\n\u0013\nP1 + a(1 \u2212 \u03b2)P2\n\u03c84 = \u03b3\n+ \u03b3(\u03b2P2 + bP1 ),\n(226)\n1 + a\u03b2P2\n\u0013\n\u0012\na(1 \u2212 \u03b2)P2\n.\n(227)\n\u03c85 = \u03b3(\u03b2P2 ) + \u03b3(P2 + bP1 ) + \u03b3\n1 + a\u03b2P2\nDue to the fact that the sum capacity is attained at the point where the second user transmits at its maximum rate, the\nlast inequality in the description of the HK achievable region can be removed. Although the point r5\u2032 = (\u03c83 \u2212 \u03b3(P2 ), \u03b3(P1 ))\nin Figure 9 may not be in G0 , this point is always achievable due to the sum capacity result. Hence, we can enlarge G0 by\nremoving r3 and r4 . Let us denote the resulting region as G0\u2032 . Moreover, one can show that r2\u2032 , r3\u2032 , r4\u2032 , and r6\u2032 are still outside\nG0\u2032 . However, for the mixed Gaussian IC, it is possible that r1\u2032 belongs to G0\u2032 . In Figure 13, two alternative cases for the region\nG0\u2032 along with the new labeling of its extreme points are plotted. The new extreme points can be written as\nr1 = (\u03c81 , \u03c84 \u2212 2\u03c81 ),\n\nr2 = (\u03c81 , \u03c83 \u2212 \u03c81 ),\nr3 = (\u03c84 \u2212 \u03c83 , 2\u03c83 \u2212 \u03c84 ),\nr4 = (\u03c83 \u2212 \u03c82 , \u03c82 ).\n\nIn fact, we have either G0\u2032 = conv{r1 , r3 , r4 } or G0\u2032 = conv{r2 , r4 }.\nTo simplify the characterization of G1 , we consider three cases:\nCase I: 1 + P2 \u2264 b + abP2 .\nCase II: 1 + P2 > b + abP2 and 1 \u2212 a \u2264 abP1 .\nCase III: 1 + P2 > b + abP2 and 1 \u2212 a > abP1 .\nCase I (1 + P2 \u2264 b + abP2 ): In this case, \u03c83 = \u03c831 . Moreover, it is easy to verify that \u03c831 + \u03c81 \u2264 \u03c84 which means\nredundant for the entire range of parameters. Hence, G0\u2032 = conv{r2 , r4 } consists of all rate pairs (R1 , R2 ) satisfying\n\u0013\n\u0012\nP1\n,\nR1 \u2264 \u03b3\n1 + a\u03b2P2\nR2 \u2264 \u03b3 (P2 ) ,\n\u0012\n\u0013\nP1 + a(1 \u2212 \u03b2)P2\nR1 + R2 \u2264 \u03b3\n+ \u03b3(\u03b2P2 ),\n1 + a\u03b2P2\n\n(8) is\n\n(228)\n(229)\n(230)\n\n\f32\n\nwhere \u03b2 \u2208 [0, 1]. Using a reasoning similar to the one used to express boundary points of G1 for the one-sided Gaussian IC,\nwe can express boundary points of G1 as\n\u0012\n\u0013\nP1\nR1 \u2264 \u03b3\n,\n(231)\n1 + a\u03b2P2\n\u0013\n\u0012\na(1 \u2212 \u03b2)P2\n,\n(232)\nR2 \u2264 \u03b3(\u03b2P2 ) + \u03b3\n1 + P1 + a\u03b2P2\n\nfor all \u03b2 \u2208 [0, 1].\nTheorem 12: For the mixed Gaussian IC satisfying 1 \u2264 ab, region G is equivalent to that of the one sided Gaussian IC\nobtained from removing the interfering link between Transmitter 1 and Receiver 2.\nProof: If 1 \u2264 ab, then 1 + P2 \u2264 b + abP2 holds for all P1 and P2 . Hence, G0\u2032 (P1 , P2 , \u03b2) is a pentagon defined by (228),\n(229), and (229). Comparing with the corresponding region for the one-sided Gaussian IC, we see that G0\u2032 is equivalent to G0\nobtained for the one-sided Gaussian IC. This directly implies that G is the same for both channels.\nCase II (1 + P2 > b + abP2 and 1 \u2212 a \u2264 abP\nS this case, \u03c83 = min{\u03c831 , \u03c832 }. It can be shown that G1 is the union\nS1 ): In\nof three regions E1 , E2 , and E3 , i.e, G0 = E1 E2 E3 . Region E1 is the union of all rate pairs (R1 , R2 ) satisfying\n\u0013\n\u0012\nP1\n,\n(233)\nR1 \u2264 \u03b3\n1 + a\u03b2P2\n\u0013\n\u0012\na(1 \u2212 \u03b2)P2\n.\n(234)\nR2 \u2264 \u03b3(\u03b2P2 ) + \u03b3\n1 + P1 + a\u03b2P2\nb\u22121\nfor all \u03b2 \u2208 [0, (1\u2212ab)P\n]. Region E2 is the union of all rate pairs (R1 , R2 ) satisfying\n2\n\u0012\n\u0013\nbP1\nR1 \u2264 \u03b3\n,\n1 + \u03b2P2\n\u0013\n\u0012\n\u0013\n\u0012\nbP1\nP1 + a(1 \u2212 \u03b2)P2\n+ \u03b3(\u03b2P2 ) \u2212 \u03b3\n.\nR2 \u2264 \u03b3\n1 + a\u03b2P2\n1 + \u03b2P2\n\nb\u22121\n, (b\u22121)P1 +(1\u2212a)P2 ]. Region E3 is the union of all rate pairs (R1 , R2 ) satisfying\nfor all \u03b2 \u2208 [ (1\u2212ab)P\n2 (1\u2212ab)P1 P2 +(1\u2212a)P2\n!\n1\nbP1 (1 + (1\u2212ab)P\n)\n1\u2212a\nR1 \u2264 \u03b3\n,\n1 + bP1 + P2\n\nR2 \u2264 \u03b3 (P2 ) ,\nR1 + R2 \u2264 \u03b3(bP1 + P2 ).\n\n(235)\n(236)\n\n(237)\n(238)\n(239)\n\nCase\nS III\nS (1 + P2 > b + abP2 and 1 \u2212 a > abP1 ): In this case, \u03c83 = min{\u03c831 , \u03c832 }. Similar to Case II, we have G1 =\nE1 E2 E3 , where regions E1 , E2 , and E3 are defined as follows. Region E1 is the union of all rate pairs (R1 , R2 )\nsatisfying\n\u0012\n\u0013\nP1\nR1 \u2264 \u03b3\n,\n(240)\n1 + a\u03b2P2\n\u0013\n\u0012\na(1 \u2212 \u03b2)P2\n.\n(241)\nR2 \u2264 \u03b3(\u03b2P2 ) + \u03b3\n1 + P1 + a\u03b2P2\nb\u22121\nfor all \u03b2 \u2208 [0, (1\u2212ab)P\n]. Region E2 is the union of all rate pairs (R1 , R2 ) satisfying\n2\n\u0013\n\u0012\nP1\n,\nR1 \u2264 \u03b3\n1 + a\u03b2P2\n\u0012\n\u0013\n\u0012\n\u0013\na(1 \u2212 \u03b2)P2\nP1\nR2 \u2264 \u03b3\n+ \u03b3(\u03b2P2 + bP1 ) \u2212 \u03b3\n.\n1 + P1 + a\u03b2P2\n1 + a\u03b2P2\nb\u22121\n, 1]. Region E3 is the union of all rate pairs (R1 , R2 ) satisfying\nfor all \u03b2 \u2208 [ (1\u2212ab)P\n2\n\u0012\n\u0013\nP1\nR1 \u2264 \u03b3\n,\n1 + aP2\nR2 \u2264 \u03b3 (P2 ) ,\n\nR1 + R2 \u2264 \u03b3(bP1 + P2 ).\n\n(242)\n(243)\n\n(244)\n(245)\n(246)\n\nRemark 5: Region E3 in Case II and Case III represents a facet that belongs to the capacity region of the mixed Gaussian\nIC. It is important to note that, surprisingly, this facet is obtainable when the second transmitter uses both the common message\nand the private message.\nDifferent bounds are compared for the mixed Gaussian IC for Cases I, II, and III in Figures 14, 15, and 16, respectively.\n\n\f33\n\nFig. 14.\n\nComparison between different bounds for the mixed Gaussian IC when 1 + P2 \u2264 b + abP2 (Case I) for P1 = 7, P2 = 7, a = 0.6, and b = 2.\n\nFig. 15. Comparison between different bounds for the mixed Gaussian IC when 1 + P2 > b + abP2 and 1 \u2212 a \u2264 abP1 (Case II) for P1 = 7, P2 = 7,\na = 0.4, and b = 1.5.\n\nVII. C ONCLUSION\nWe have studied the capacity region of the two-user Gaussian IC. The sum capacities, inner bounds, and outer bounds have\nbeen considered for three classes of channels: weak, one-sided, and mixed Gaussian IC. We have used admissible channels as\nthe main tool for deriving outer bounds on the capacity regions.\nFor the weak Gaussian IC, we have derived the sum capacity for a certain range of channel parameters. In this range, the\nsum capacity is attained when Gaussian codebooks are used and interference is treated as noise. Moreover, we have derived a\nnew outer bound on the capacity region. This outer bound is tighter than the Kramer's bound and the ETW's bound. Regarding\ninner bounds, we have reduced the computational complexity of the HK achievable region. In fact, we have shown that when\nGaussian codebooks are used, the full HK achievable region can be obtained by using the naive HK achievable scheme over\nthree frequency bands.\nFor the one-sided Gaussian IC, we have presented an alternative proof for the Sato's outer bound. We have also derived the\nfull HK achievable region when Gaussian codebooks are used.\nFor the mixed Gaussian IC, we have derived the sum capacity for the entire range of its parameters. Moreover, we have\npresented a new outer bound on the capacity region that outperforms ETW's bound. We have proved that the full HK achievable\nregion using Gaussian codebooks is equivalent to that of the one-sided Gaussian IC for a particular range of channel gains.\n\n\f34\n\nFig. 16. Comparison between different bounds for the mixed Gaussian IC when 1 + P2 > b + abP2 and 1 \u2212 a > abP1 (Case III) for P1 = 7, P2 = 700,\na = 0.01, and b = 1.5.\n\nWe have also derived a facet that belongs to the capacity region for a certain range of parameters. Surprisingly, this facet is\nobtainable when one of the transmitters uses both the common message and the private message.\nR EFERENCES\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[13]\n[14]\n[15]\n[16]\n[17]\n[18]\n[19]\n[20]\n[21]\n[22]\n[23]\n[24]\n\nC. E. Shannon, \"Two-way communication channels,\" in Proc. 4th Berkeley Symp. on Mathematical Statistics and Probability, vol. 1, 1961, pp. 611\u2013644.\nA. B. Carleial, \"A case where interference does not reduce capacity,\" IEEE Trans. Inform. Theory, vol. IT-21, pp. 569\u2013570, Sept. 1975.\nH. Sato, \"The capacity of the gaussian interference channel under strong interference,\" IEEE Trans. Inform. Theory, vol. IT-27, pp. 786\u2013788, Nov. 1981.\nR. Ahlswede, \"Multi-way communnication channels,\" in Proc. 2nd International Symp. on Information theory, U. Tsahkadsor, Armenia, Ed., Sep 2-8\n1971, pp. 23\u201352.\nI. Csisz\u00e1r and J. K\u00f6rner, Information Theory: Theorems for Discrete Memoryless Systems. Budapest, Hungary: Hungarian Acad. Sci., 1981.\nR. S. Cheng and S. Verd\u00fa, \"On limiting characterizations of memoryless multiuser capacity regions,\" IEEE Trans. Inform. Theory, vol. 39, pp. 609\u2013612,\nMar. 1993.\nT. Cover and J. Thomas, Elements of information theory. NY, John Wiley, 1991.\nT. S. Han and K. Kobayashi, \"A new achievable rate region for the interference channel,\" IEEE Trans. Inform. Theory, vol. IT-27, pp. 49\u20136o, Jan. 1981.\nH. Chong, M. Motani, H. Garg, and H. E. Gamal, \"On the Han-Kobayashi region for the interference channel,\" Submitted to the IEEE Trans. on Inf.,\nAug. 2006.\nH. Sato, \"On degraded gaussian two-user channels,\" IEEE Trans. Inform. Theory, vol. IT-24, pp. 637\u2013640, Sept. 1978.\nM. H. M. Costa, \"On the Gaussian interference channel,\" IEEE Trans. Inform. Theory, vol. IT-31, pp. 607\u2013615, Sept. 1985.\nG. Kramer, \"Outer bounds on the capacity of gaussian interference channels,\" IEEE Trans. Inform. Theory, vol. 50, pp. 581\u2013586, Mar. 2004.\nR. Etkin, D. Tse, and H. Wang, \"Gaussian interference channel capacity to within one bit.\" submitted to the IEEE Transactions on Information Theory.\nAvailable at http://www.eecs.berkeley.edu/ dtse/pub.html, Feb. 2007.\nT. Liu and P. Viswanath, \"An extremal inequality motivated by multi terminal information theoretic problems,\" in 2006 Internatinal Symposiun on\nInformation Theory (ISIT), Seattle, WA, July 2006, pp. 1016\u20131020.\nA. B. Carleial, \"Interference channels,\" IEEE Trans. Inform. Theory, vol. IT-24, pp. 60\u201370, Jan. 1978.\nI. Sason, \"On achievable rate regions for the gaussian interference channel,\" IEEE Trans. Inform. Theory, vol. 50, pp. 1345\u20131356, June 2004.\nS. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, U.K.: Cambridge Univ. Press, 2003.\nT. Han, \"The capacity region of general multiple-access channel with certain correlated sources,\" Inform. Contr., vol. 40, no. 1, pp. 37\u201360, 1979.\nR. T. Rockafellar and R. J.-B. Wets, Variational Analysis. Springer-Verlag, Berlin Heidelberg., 1998.\nR. Etkin, A. Parekh, and D. Tse, \"Spectrum sharing for unlicensed bands,\" IEEE Journal of Selected Area of Comm., vol. 52, pp. 1813\u20131827, April\n2007.\nS. N. Diggavi and T. Cover, \"The worst additive noise under a covariance constraint.\" IEEE Trans. Inform. Theory, vol. 47, no. 7, pp. 3072\u20133081, Nov.\n2001.\nS. Ihara, \"On the capacity of channels with additive non-Gaussian noise.\" Info. Ctrl., vol. 37, no. 1, pp. 34\u201339, Apr. 1978.\nH. Sato, \"An outer bound to the capacity region of broadcast channels,\" IEEE Trans. Inform. Theory, vol. IT-24, pp. 374\u2013377, May 1978.\nA. S. Motahari and A. K. Khandani, \"Capacity bounds for the gaussian interference channel,\" Library Archives Canada Technical Report UW-ECE 2007-26\n(Available at http://www.cst.uwaterloo.ca/ pub tech rep.html), Aug. 2007.\n\n\f35\n\n[25] X. Shang, G. Kramer, and B. Chen, \"A new outer bound and the noisy-interference sum-rate capacity for gaussian interference channels (availabe at:\nhttp://arxiv.org/abs/0712.1987),\" Submitted to IEEE Trans. Inform. Theory, Dec. 2007.\n[26] V. S. Annapureddy and V. V. Veeravalli, \"Sum capacity of the gaussian interference channel in the low interference regime (availabe at:\nhttp://arxiv.org/abs/0801.0452),\" Proceedings of ITA Workshop, San Diego, CA, Jan. 2008.\n\n\f"}