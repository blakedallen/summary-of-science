{"id": "http://arxiv.org/abs/1012.4051v1", "guidislink": true, "updated": "2010-12-18T03:25:44Z", "updated_parsed": [2010, 12, 18, 3, 25, 44, 5, 352, 0], "published": "2010-12-18T03:25:44Z", "published_parsed": [2010, 12, 18, 3, 25, 44, 5, 352, 0], "title": "Survey & Experiment: Towards the Learning Accuracy", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.2366%2C1012.5613%2C1012.3461%2C1012.4497%2C1012.2894%2C1012.1681%2C1012.4862%2C1012.4390%2C1012.1098%2C1012.1661%2C1012.5578%2C1012.2381%2C1012.2405%2C1012.2946%2C1012.1759%2C1012.4876%2C1012.4905%2C1012.5455%2C1012.1040%2C1012.0242%2C1012.4051%2C1012.2326%2C1012.0176%2C1012.5073%2C1012.3158%2C1012.3207%2C1012.4073%2C1012.0796%2C1012.1347%2C1012.3824%2C1012.1472%2C1012.0642%2C1012.2687%2C1012.3409%2C1012.2852%2C1012.6041%2C1012.0866%2C1012.3106%2C1012.0710%2C1012.4772%2C1012.0496%2C1012.5878%2C1012.3338%2C1012.2425%2C1012.4334%2C1012.3997%2C1012.3788%2C1012.5678%2C1012.5214%2C1012.0899%2C1012.2281%2C1012.4040%2C1012.4268%2C1012.1283%2C1012.4684%2C1012.1199%2C1012.4687%2C1012.1293%2C1012.5683%2C1012.1987%2C1012.2521%2C1012.3365%2C1012.3805%2C1012.1989%2C1012.5431%2C1012.4825%2C1012.5466%2C1012.2668%2C1012.2392%2C1012.3381%2C1012.0298%2C1012.3558%2C1012.5502%2C1012.0627%2C1012.3739%2C1012.1879%2C1012.0683%2C1012.2209%2C1012.3175%2C1012.1988%2C1012.3497%2C1012.5606%2C1012.2601%2C1012.1838%2C1012.4438%2C1012.1605%2C1012.2841%2C1012.0323%2C1012.3442%2C1012.2700%2C1012.1635%2C1012.2016%2C1012.0296%2C1012.0274%2C1012.2127%2C1012.2187%2C1012.1425%2C1012.3476%2C1012.5191%2C1012.1612%2C1012.5506&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Survey & Experiment: Towards the Learning Accuracy"}, "summary": "To attain the best learning accuracy, people move on with difficulties and\nfrustrations. Though one can optimize the empirical objective using a given set\nof samples, its generalization ability to the entire sample distribution\nremains questionable. Even if a fair generalization guarantee is offered, one\nstill wants to know what is to happen if the regularizer is removed, and/or how\nwell the artificial loss (like the hinge loss) relates to the accuracy.\n  For such reason, this report surveys four different trials towards the\nlearning accuracy, embracing the major advances in supervised learning theory\nin the past four years. Starting from the generic setting of learning, the\nfirst two trials introduce the best optimization and generalization bounds for\nconvex learning, and the third trial gets rid of the regularizer. As an\ninnovative attempt, the fourth trial studies the optimization when the\nobjective is exactly the accuracy, in the special case of binary\nclassification. This report also analyzes the last trial through experiments.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.2366%2C1012.5613%2C1012.3461%2C1012.4497%2C1012.2894%2C1012.1681%2C1012.4862%2C1012.4390%2C1012.1098%2C1012.1661%2C1012.5578%2C1012.2381%2C1012.2405%2C1012.2946%2C1012.1759%2C1012.4876%2C1012.4905%2C1012.5455%2C1012.1040%2C1012.0242%2C1012.4051%2C1012.2326%2C1012.0176%2C1012.5073%2C1012.3158%2C1012.3207%2C1012.4073%2C1012.0796%2C1012.1347%2C1012.3824%2C1012.1472%2C1012.0642%2C1012.2687%2C1012.3409%2C1012.2852%2C1012.6041%2C1012.0866%2C1012.3106%2C1012.0710%2C1012.4772%2C1012.0496%2C1012.5878%2C1012.3338%2C1012.2425%2C1012.4334%2C1012.3997%2C1012.3788%2C1012.5678%2C1012.5214%2C1012.0899%2C1012.2281%2C1012.4040%2C1012.4268%2C1012.1283%2C1012.4684%2C1012.1199%2C1012.4687%2C1012.1293%2C1012.5683%2C1012.1987%2C1012.2521%2C1012.3365%2C1012.3805%2C1012.1989%2C1012.5431%2C1012.4825%2C1012.5466%2C1012.2668%2C1012.2392%2C1012.3381%2C1012.0298%2C1012.3558%2C1012.5502%2C1012.0627%2C1012.3739%2C1012.1879%2C1012.0683%2C1012.2209%2C1012.3175%2C1012.1988%2C1012.3497%2C1012.5606%2C1012.2601%2C1012.1838%2C1012.4438%2C1012.1605%2C1012.2841%2C1012.0323%2C1012.3442%2C1012.2700%2C1012.1635%2C1012.2016%2C1012.0296%2C1012.0274%2C1012.2127%2C1012.2187%2C1012.1425%2C1012.3476%2C1012.5191%2C1012.1612%2C1012.5506&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "To attain the best learning accuracy, people move on with difficulties and\nfrustrations. Though one can optimize the empirical objective using a given set\nof samples, its generalization ability to the entire sample distribution\nremains questionable. Even if a fair generalization guarantee is offered, one\nstill wants to know what is to happen if the regularizer is removed, and/or how\nwell the artificial loss (like the hinge loss) relates to the accuracy.\n  For such reason, this report surveys four different trials towards the\nlearning accuracy, embracing the major advances in supervised learning theory\nin the past four years. Starting from the generic setting of learning, the\nfirst two trials introduce the best optimization and generalization bounds for\nconvex learning, and the third trial gets rid of the regularizer. As an\ninnovative attempt, the fourth trial studies the optimization when the\nobjective is exactly the accuracy, in the special case of binary\nclassification. This report also analyzes the last trial through experiments."}, "authors": ["Zeyuan Allen Zhu"], "author_detail": {"name": "Zeyuan Allen Zhu"}, "author": "Zeyuan Allen Zhu", "links": [{"href": "http://arxiv.org/abs/1012.4051v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1012.4051v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1012.4051v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1012.4051v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Survey & Experiment: Towards the Learning Accuracy\n\narXiv:1012.4051v1 [cs.LG] 18 Dec 2010\n\nZeyuan Allen Zhu\nTsinghua University\nzhuzeyuan@hotmail.com\n\nAbstract\nTo attain the best learning accuracy, people move\non with difficulties and frustrations. Though one\ncan optimize the empirical objective using a given\nset of samples, its generalization ability to the entire sample distribution remains questionable. Even\nif a fair generalization guarantee is offered, one\nstill wants to know what is to happen if the regularizer is removed, and/or how well the artificial\nloss (like the hinge loss) relates to the accuracy.\nFor such reason, this report surveys four different\ntrials towards the learning accuracy, embracing the\nmajor advances in supervised learning theory in\nthe past four years. Starting from the generic setting of learning, the first two trials introduce the\nbest optimization and generalization bounds for convex learning, and the third trial gets rid of the regularizer. As an innovative attempt, the fourth trial\nstudies the optimization when the objective is exactly the accuracy, in the special case of binary\nclassification. This report also analyzes the last\ntrial through experiments.\n\n1 Introduction\nA generic learning problem can be regarded as an optimization over parameter w \u2208 W, and the objective function is\ngiven by f (w; \u03b8) where \u03b8 is a given sample. An empirical\nobjective F\u0302 (w) can be written as\nm\n\nF\u0302 (w) = \u00ca[f (w; \u03b8)] =\n\n1 X\nf (w; \u03b8i ) ,\nm i=1\n\nhere \u03b81 , \u03b82 , . . . , \u03b8 m are a sequence of observed samples. It\nis normally assumed that these samples are i.i.d. drawn from\nsome unknown distribution D, and therefore the stochastic\nobjective F (w) is often more desirable:\nF (w) = E\u03b8\u223cD [f (w; \u03b8)] .\nFor example, if we take f (w; \u03b8 = (x, y)) = max{0, 1 \u2212\nyhw, xi} + \u03bb2 kwk22 we will arrive at the famous SVM, with\na weighted L2 norm regularizer. The kernel trick can also\nbe adopted which allows a non-linear prediction: f (w; \u03b8) =\nmax{0, 1 \u2212 yhw, \u03c6(x)i} + \u03bb2 kwk22 .\n\nTrial 1.To obtain a good learning accuracy, the first trial\nis to optimize over the empirical objective F\u0302 (w). We normally assume that f (*; \u03b8) is a convex function because we\ncan then utilize optimization techniques like interior point\nmethod or gradient descent to conquer the minimization. Recent results in for example [DSSST10, SSSS07] have shown\nthat in many cases, using the stochastic gradient descent experiences the fastest runtime in minimizing the empirical objective, and [ZCW+ 09] has generalized this in the extent of\nusing kernels and (batched) parallel computation. In Section\n2, we will briefly describe the main framework of [DSSST10],\nbecause it embraces all previously known first-order algorithms as special cases.\nTrial 2. To better understand the learning accuracy to\nfuture samples, we need to establish the connection between\nthe stochastic objective F (w) and empirical objective F\u0302 (w).\nThis is called the generalization. A recent paper [SSSSS09]\ncompleted a thorough classification over the types of learning problems, and tells us how well each of them guarantees\na good generalization error bound. The main results of this\npaper is provided in Section 3 as a good reference, but this is\nonly the second trial.\nAs a partial summary, in the above two trials, when f is\nconvex both results have an error bound proportional to \u221a1T .\nIn Step 1, this T is the number of iterations (which is also\nproportional to the runtime) and the error bound is the difference between the optimal solution and the one generated by\nthe SGD. In Step 2, this T is the number of samples and the\nerror bound is (imprecisely) the difference over the stochastic and empirical objective. However, if we further require\nf to be strongly-convex, by for instance adding a regularizer, both bounds immediately decrease to T1 instead of \u221a1T ,\nand this partially explains why we add a regularizer from a\ntheoretical point of view.\nTrial 3. Now comes to the third trial, an attempt to bound\nthe stochastic loss instead of the stochastic objective. As\nexplained above, we often add a regularizer (e.g. r(w) =\n\u03bb\n2\n2 kwk ) to the objective function f (w; \u03b8) = l(w; \u03b8)+r(w).\nTherefore, even if we achieve a close-to-optimal solution for\nthe stochastic objective F (w), it is still far away from the\nstochastic loss\nL(w) = E\u03b8\u223cD [l(w; \u03b8)] .\nTo further build a connection between these two quantities,\n[SSS08, ZCZ+ 09] used a so-called oracle inequality and de-\n\n\fduced a final bound for this stochastic loss, with respect to\nthe running time of a program, and the number of training\nsamples m. This work was recognized by the best paper\nawards of ICML 2008 and ICDM 2009, and briefly described\nin Section 4.\nTrial 4. However, how well such loss function characterizes the word \"accuracy\" remains a problem. One can feel\nfree to use any convex loss functions (like hinge loss or logistic loss), but they do not reflect the accuracy at all. In the\npaper of [SSSS10], they consider the following non-convex\nobjective:\nf (w; \u03b8 = (x, y)) =\n\n1\n1\nsgn(hw, \u03c6(x)i + \u2212 y\n2\n2\n\nwhich is exactly the definition of accuracy (for binary classification) if y \u2208 {0, 1}.\nThough incorporating the traditional Rademacher generalization bound [BM03] one can still obtain good generalization guarantee, the empirical optimization becomes hard\n(Appendix A of [SSSS10]). Realizing such difficulty, [SSSS10]\nstudied improper learning and constructed a larger class of\nclassifier. Not only the empirical optimization in the new\nconcept class is convex, the minimizer is also good enough\nso that original zero-one objective problem is learnable using such minimizer. This paper was recognized by the best\npaper award of COLT 2010, and this report is also going to\nanalyze its practical value against public data sets in Section\n5.\n1.1 Preliminary\nFor the lack of space, the definitions of strongly-convex, Lipschitz continuity, dual norm and Bregman divergence are ignored in this report. The readers who are interested in the\ntechnical details may refer to (nearly) any of the references\nattached to this report and look into its preliminary section.\n\n2 Trial 1: Empirical Optimization\nWe denote the empirical objective for the t-th sample as\nf (w; \u03b8t ) = ft (w) + r(w) ,\nin which r is a convex regularization function, and ft is a\nconvex loss function associated with example t.\nIn the online and batch learning setting, we have a sequence of T samples. At the beginning the the t-th round,\nthe algorithm must make a prediction wt and then receive a\nfunction ft . The ultimate goal is to minimize the following\nregularized regret:\nR(T ) = max\n\u2217\n\nw \u2208W\n\nT\nX\nt=1\n\n[ft (wt ) + r(wt ) \u2212 ft (w\u2217 ) \u2212 r(w\u2217 )]\n\n\u2217\n\nin which w is the optimal empirical minimizer for this sequence of samples.\nThe paper [DSSST10] defined the following update sequence\nwt+1 = argmin \u03b7h\u2202ft (wt ), wi + B\u03c8 (w, wt ) + \u03b7r(w) .\n\nHere \u03b7 is a parameter that is to be tuned later, \u2202 is the subgradient, and w1 can be set to zero vector. The Bregman\ndivergence associated with \u03c8 is defined as\nB\u03c8 (w, v) = \u03c8(w) \u2212 \u03c8(v) \u2212 h\u2207\u03c8(v), w \u2212 vi ,\nand we require \u03c8 to be \u03b1-strongly convex w.r.t. a norm k *\nk, and then B\u03c8 (w, v) \u2265 \u03b12 kw \u2212 vk2 for this \u03b1. A very\nsimple choice is to let \u03c8(w) = 12 kwk2 and then B\u03c8 (w, v) =\n1\n2\n2 kw \u2212 vk .\nOne of the main theorems in [DSSST10] proves that:\nTheorem 1 Let \u03c8 be \u03b1-strongly convex w.r.t. norm k * k.\nSuppose W is compact OR thep\nfunction ft is G-Lipschitz\n\u221a\nk\u2202ft k\u2217 \u2264 G.1 Then setting \u03b7 = 2\u03b1B\u03c8 (w\u2217 , w1 )/(G T ),\nq\n\u221a\nR\u03c8 (T ) \u2264 2T B\u03c8 (w\u2217 , w1 )G/ \u03b1 .\nNotice that this result does not use any property of the regularized term r(w), and it holds even if r(w) = 0.\nThis square root (w.r.t. T ) regret allows one to design a\nstochastic gradient descent algorithm with random sampling\nover the training data set. However, to obtain an optimization\nerror of \u01eb, one usually needs to run T = \u03a9(1/\u01eb2 ) number of\niterations. This is not good enough.\nWhen the objective function ft (w) + r(w) is stronglyconvex over w, w.l.o.g. we can let r be \u03bb strongly-convex\nand f be only of the classical convexity. In this case if we\nreplace the update sequence of Eq. 1 and let \u03b7 vary for dif1\nferent t. Specifically, we let \u03b7t = \u03bbt\n. With only little extra\neffort, one can prove the following logarithmic regret bound:\nTheorem 2 Let r be \u03bb-strongly convex w.r.t. \u03c8, and \u03c8 be \u03b1strongly convex w.r.t. norm k * k. If function ft is G-Lipschitz\nk\u2202ft k\u2217 \u2264 G. Then:\n\u0012 2\n\u0013\nG\nR\u03c8 (T ) = O\nlog T\n.\n\u03bb\u03b1\nNotice that if we set \u03c8(w) = 21 kwk22 and we immediately arrive at the online counterpart of the famous PEGASOS\nalgorithm, which is the currently best-known linear SVM\nclassifier [SSSS07], and arguably the best known kernel SVM\nclassifier at least under the parallel setting [ZCW+ 09].\nIn sum, depending on the convexity of the objective function, one may adopt either the convex or the strongly-convex\nversion of the mirror descent algorithm summarized in [DSSST10],\nwith a satisfiable theoretical bound on the regret. Regarding\nthe off-line problem given a fixed set of training samples, as\nlong as in each iteration a sample is uniformly chosen at random from this training set, a similar bound can be deduced\njust like [SSSS07, SSS08, ZCZ+ 09], using Markov inequality.\n\n3 Trial 2: Stochastic Optimization\nAs advertised in the introduction, although a sub-optimal w\nis deduced from minimizing the empirical objective F\u0302 (w),\nwe are more interested in its generalization ability to F (w).\n\nw\u2208W\n\n(1)\n\n1\n\nk * k\u2217 is the dual norm of k * k.\n\n\fFigure 1: Three classes of learnable problems. Picture borrowed from [SSSSS09].\nIn this section we follow the convention: use \u0175 to denote the\nempirical minimizer, and w\u2217 the stochastic minimizer.\nThe paper [SSSSS09] analyzed three classes of problems\n(see Figure 1). The first and the smallest class is the one that\nguarantees uniform convergence:\nsup F (w) \u2212 F\u0302 (w) \u2192 0 ,\n\nw\u2208W\n\nwhich says that the difference between two kinds of objectives tends to zero as m \u2192 \u221e. A relatively larger class is the\none that is learnable by empirical minimizer:\nF (\u0175) \u2212 F (w\u2217 ) \u2192 0 ,\nwhich guarantees point-wise convergence at the empirical\nminimizer. At last, the largest class of learnable functions\nis defined as\n\"there exists a rule for choosing w\u0303 based on samples, such that F (w\u0303) \u2212 F (w\u2217 ) \u2192 0\".\nAll of the generalized linear problems2 are included in\nthe set of uniform convergence. These are the stared rectangle in Figure 1.\nAt the same time, most of the problems (satisfying convexity, Lipschitz, boundedness, etc) are learnable (the triangle in Figure 1), but not necessarily using empirical minimization, and not necessarily guaranteeing uniform convergence. A generic way to learn such function is via online\nconvex optimization. One may refer to Section 2 for the algorithm, and its generalization guarantee is summarized in\nEq.(7) and Eq.(8) of [SSSSS09].\nRemark: All of the generalization error guarantees mentioned above are either in a factor of \u221a1m or in a factor of\n1\nm , depending on whether the objective function is convex of\nstrongly-convex.\nThe main contribution of [SSSSS09] is that they showed\nall Lipschitz-continuous strongly convex problems (dotted\n2\nSatisfying f (w; \u03b8) = g(hw, \u03c6(\u03b8)i; \u03b8) + r(w). This includes\nSVM, logistic regression and all kinds of supervised learning as\nmentioned in the summary of [SSSSS09].\n\nrectangle in Figure 1) are learnable with empirical minimization. This means that if one finds the empirical minimizer\n\u0175, F (\u0175) and F (w\u2217 ) are guaranteed to be close enough. Of\ncourse, there is also a good guarantee for any sub-optimal\nempirical minimizer w:\nr\nq\n2L2\n4L2\n\u2217\nF (w) \u2212 F (w ) \u2264\n,\nF\u0302 (w) \u2212 F\u0302 (\u0175) +\n\u03bb\n\u03b4\u03bbm\nhere L is the Lipschitz continuity constant for f , \u03bb is the\nstrong-convexity constant for f , and \u03b4 is the confidence level.\n\n4 Trial 3: Stochastic Loss Optimization\nBecause the generalization bound differs between convex and\nstrongly-convex objectives, we usually have to add a (stronglyconvex) regularizer to ensure a better error bound between\nthe empirical and stochastic objective. For instance, we can\nadd a L2 -norm regularizer to the hinge loss, resulting in the\nfamous SVM problem.\nIf we have f (w; \u03b8) = l(w; \u03b8) + r(w), we can define\nF (w) = L(w)+r(w) where L(w) = E\u03b8\u223cD [l(w; \u03b8)]. Then\nthe mathematical term we are more interested is actually:\nL(w\u0303) \u2212 L(w0 ) ,\nfor a solution w\u0303 given by the algorithm, and the loss minimizer w0 = argminw L(w). By using the oracle inequality\n[SSS08, ZCZ+ 09]:\nL(w\u0303) \u2212 L(w0 )\n= (F (w\u0303) \u2212 F (w\u2217 )) + (F (w\u2217 ) \u2212 F (w0 ))\n\u2212 r(w\u0303) + r(w0 )\n\u2264 (F (w\u0303) \u2212 F (w\u2217 )) + r(w0 ) ,\none may deduce a bound of the generalized loss error given\na generalized error F (w\u0303) \u2212 F (w\u2217 ), while the latter is already obtained in Section 3. Though this bound looks loose\n(neglecting two negative terms), through a careful selection\nof the weight hidden in the regularizer r(w), one may find\nthat the practical behavior matches this theoretical bound, in\n[SSS08, ZCZ+ 09].\n\n\f5 Trial 4: Zero-One Loss\nNot satisfied by the result in the previous section, [SSSS10]\nmade an interesting attempt towards learning 0-1 objective\nfunctions. In classification problems with a half-plane classifier, the following objective function is more desirable than\nany other (regularized or not) convex objective (e.g. hinge\nloss, logistic loss):\n1\n1\nsgn(hw, \u03c6(x)i + \u2212 y\n2\n2\n\nf (w; \u03b8 = (x, y)) =\n\nNotice that the label y \u2208 {0, 1}.\n5.1 The theory\nIf we define \u03c60\u22121 (a) = 12 (sgn(a) + 1), the above objective\nfunction is characterized by the following concept class\nH\u03c60\u22121 = {x \u2192 \u03c60\u22121 (hw, \u03c6(x)i)} ,\nand we are interested in optimizing the following stochastic\nobjective:\nF (h) = E(x,y)\u223cD [|h(x) \u2212 y|],\n\nh \u2208 H\u03c60\u22121 .\n\n(2)\n\nThe first step of this attempt requires the approximation\nto H\u03c60\u22121 using Lipschitz continuous functions. Define\n\u03c6sig (a) =\n\n1\n,\n1 + exp(\u22124La)\n\nwhich is L-Lipschitz continuous and approximates \u03c60\u22121 well.3\nNow consider the following concept class for the stochastic\nobjective (Eq. 2)\nH\u03c6sig = {x \u2192 \u03c6sig (hw, \u03c6(x)i)} .\nOne advantage of such approximation is to allow theorems like Rademacher generalization bound [BM03] to hold.\nIndeed, the empirical minimizer,\nm\n\n\u0175 = argmin F\u0302 (w) = argmin\nw\n\nw\n\n1 X\n|\u03c6sig (hw, xi i) \u2212 yi | ,\nm i=1\n\ngives a generalization error bound F (\u0175) \u2212 F (w) < \u01eb when\nm = \u03a9\u0303(L2 /\u01eb2 ). However, it has been pointed out that the\nempirical minimization is \"hard\" since the objective is not\nconvex. [SSSS10]\nTo conquer such difficulty, a new concept class is introduced:\nHB = {x \u2192 hw, \u03c8(x)i : kwk2 \u2264 B} ,\nand its difference from H\u03c60\u22121 or H\u03c6sig is twofold. First, it no\nlonger uses a 0-1 function in the prediction; the traditional\nhalf-plane classification using inner-product is adopted. Second, it enables a new kernel \u03c8, which is defined as4 :\nh\u03c8(x), \u03c8(x\u2032 )i =\n\n1\n.\n1 \u2212 \u03bdh\u03c6(x), \u03c6(x\u2032 )i\n\nChoosing B = \u03a9(exp(L log( L\u01eb ))) large enough, [SSSS10]\nproved that HB approximately includes H\u03c6sig , and therefore\n3\n\nIn [SSSS10] they also analyzed other two approximated functions, but for the lack of space they are ignored here.\n4\n\u03bd can be chosen to be 1/2 for the ease of presentation.\n\nwe can directly study the learning problem in HB . This only\nrequires the Lipschitz continuity of \u03c6sig and Chebyshev approximation technique, and is a very general proof.\nOne big benefit of such conversion is that the new problem is convex and can be empirically optimized via for instance stochastic gradient descent mentioned in Section 2.\nPay attention that due to the boundedness of HB , using Rademacher\ncomplexity bound again [BM03, KST08], a sample complexity of \u03a9\u0303(B/\u01eb2 ) can be deduced.\nThe procedure above is improper learning: to learn H\u03c60\u22121\nwe actually incorporate a larger concept class HB , and a\nclassifier h \u2208 HB will be returned which is close to the optimal classifier in H\u03c60\u22121 . Furthermore, the overall time and\nsample complexity is poly(exp(L log( L\u01eb ))). This bound is\nexponential w.r.t. L, but [SSSS10] also showed that a polynomial dependency on L is impossible, unless some NP-hard\nproblem is in P .\n5.2 The experiment\nThough the complexity depends crucially on L, however,\nwhen L is a constant (e.g. 1) there is still reason to believe\nthat the approximated 0-1 loss function using \u03c6sig is more\naccurate than the hinge loss. However, even if L is constant,\nthe sample complexity m = \u03a9(B/\u01eb2 ) = \u03a9(1/\u01eb3 ) has cubic\ndependency on \u01eb. At the same time, the kernel stochastic gradient descent algorithm (in the distribution manner) requires\na time complexity of O(m2 ). So there comes a dilemma:\nneither can we set m to be too large since otherwise the empirical optimization cannot finish in endurable time, nor can\nwe set m to be too small since otherwise the generalization\nguarantee is not good enough.\nConsider some m of medium size, just enough to be\ntrained in several minutes for example. Though zero-one loss\ndoes not have a good generalization guarantee, however, it is\na more desirable function than the hinge loss. So given training set of medium size, and we run SVM against a zero-one\nloss minimizer, who is to win this tug-of-war?\n5.2.1 Configuration\nIn the interest of fairness, the same stochastic gradient descent routine called P-packSVM [ZCW+ 09] has been adopted\nfor the experiment, on an Intel Quad CPU machine with four\ncores (4 times speed-up). Regarding the empirical optimization for the zero-one loss, two different approaches are implemented: ZeroOne refers to the classical mirror descent\nin Thm. 1, and ZeroOne-reg refers to the regularized zero\none loss objective with strongly-convex mirror descent in\nThm. 2. At last, PEGASOS refers to the empirical optimizer\nfor regularized SVM algorithm.\nThe three datasets Splice, Web and Adult presented by the\nlibSVM project team [Fan] are used.\nSplice\nWeb\nAdult\n\n# Training / Testing Samples\n1000 / 2175\n2477 / 47272\n1605 / 30956\n\n# Features\n60\n300\n123\n\n5.2.2 Results\nFrom Table 1, one can see that when m is on the magnitude\nof 1000, it is still unclear that which method outperforms\nwhich. For the dataset of Adult, the traditional SVM trainer\n\n\fTable 1: The accuracy report for three different methods on three different datasets, with the best-tuned parameters listed. The\nprogram has been run 5 times and the mean accuracy is chosen. The number of iterations is 100000 and all program runs in 30\nseconds.\n\nSplice (Gaussian)\nSplice (Linear)\nAdult (Gaussian)\nAdult (Linear)\nWeb (Gaussian)\nWeb (Linear)\n\nAccuracy\n\nPEGASOS\nrbf\n\n\u03bb\n\n0.90069\n0.846897\n0.844004\n0.842971\n0.980094\n0.976667\n\n0.02\n0.025\n0.0125\n-\n\n0.0003\n0.0006\n0.0003\n0.0003\n0.00003\n0.0003\n\nZeroOne\nAccuracy\nrbf\n0.903448\n0.885517\n0.84016\n0.838674\n0.980729\n0.981152\n\nPEGASOS significantly beats the zero one loss optimizers;\nwhile back to Splice and Web, zero-one loss does slightly\nbetter.\nAnother experiment worth conducting is the convergence\nrate of the three methods. From Figure 2 one\n\u221a can see that\nbecause ZeroOne uses a learning rate of 1/ T where T is\nthe total number of iterations, the accuracy curve is smoother\nthan that of PEGASOS and ZeroOne-reg, who uses learning rate 1/T for strongly-convexity. However, because the\ngeneralization guarantee for 0-1 loss is weaker than SVM,\nZeroOne-reg converges much slower than PEGASOS.\nNotice that Web is a highly-biased dataset and 97 percent\nof the samples are negative. This explains that why all methods seem to perform with great turbulence on this dataset.\n\n0.01\n0.0125\n0.0125\n-\n\n\u03bb\n0.08\n0.01\n0.003\n0.02\n0.001\n0.003\n\nZeroOne-reg\nAccuracy\nrbf\n\u03bb\n0.902989\n0.877701\n0.840742\n0.838448\n0.981236\n0.980411\n\n0.0006\n0.0003\n0.0002\n0.0003\n0.0001\n0.0003\n\nParallel primal gradient descent kernel svm. In\nICDM, pages 677\u2013686, 2009.\n[ZCZ+ 09] Zeyuan Allen Zhu, Weizhu Chen, Chenguang\nZhu, Gang Wang, Haixun Wang, and Zheng\nChen. Inverse time dependency in convex regularized learning. In ICDM, pages 667\u2013676,\n2009.\n\nReferences\n[BM03]\n\n0.01\n0.0125\n0.0125\n-\n\nPeter L. Bartlett and Shahar Mendelson.\nRademacher and gaussian complexities: risk\nbounds and structural results. J. Mach. Learn.\nRes., 3:463\u2013482, March 2003.\n[DSSST10] John Duchi, Shai Shalev-Shwartz, Yoram\nSinger, and Ambuj Tewari. Composite objective mirror descent. In COLT, 2010.\n[Fan]\nRong-En Fan. LIBSVM Data: Classification, Regression, and Multi-label. Available at\nhttp://www.csie.ntu.edu.tw/ \u0303cjlin/libsvmtools/datasets/.\n[KST08]\nSham M. Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In NIPS, pages 793\u2013800, 2008.\n[SSS08]\nShai Shalev-Shwartz and Nathan Srebro. Svm\noptimization: inverse dependence on training\nset size. In ICML, pages 928\u2013935, 2008.\n[SSSS07] Shai Shalev-Shwartz, Yoram Singer, and\nNathan Srebro. Pegasos: Primal estimated subgradient solver for svm. In ICML, pages 807\u2013\n814, 2007.\n[SSSS10] Shai Shalev-Shwartz, Ohad Shamir, and\nKarthik Sridharan. Learning kernel-based halfspaces with the zero-one loss. In COLT, 2010.\n[SSSSS09] Shai Shalev-Shwartz, Ohad Shamir, Nathan\nSrebro, and Karthik Sridharan. Stochastic convex optimization. In COLT, 2009.\n[ZCW+ 09] Zeyuan Allen Zhu, Weizhu Chen, Gang Wang,\nChenguang Zhu, and Zheng Chen. P-packsvm:\n\n\f0.86\n\nSplice\n\nAccuracy\n\nAccuracy\n\nAccuracy\n\n0.91\n\n0.99\n\nAdult\n\n0.85\n\n0.905\n\nWeb\n\n0.985\n\n0.84\n0.9\n\n0.98\n0.83\n\n0.895\n\n0.975\n\n0.82\n\n0.89\n\n0.81\n\n0.97\n\n0.8\n\n0.885\n\n0.965\n\n0.79\n0.88\n\n0.96\n0.78\nPEGASOS\n\n0.875\n\nZeroOne\nZeroOne-reg\n\n0.87\n0\n\n40000\n80000\nNumber of iterations\n\nPEGASOS\n\n0.77\n\nPEGASOS\n\n0.955\n\nZeroOne\nZeroOne-reg\n\n0.76\n0\n\n40000\n80000\nNumber of iterations\n\nZeroOne\nZeroOne-reg\n\n0.95\n0\n\n40000\n80000\nNumber of iterations\n\nFigure 2: The variation of the accuracy as the number of training iterations increases.\n\n\f"}