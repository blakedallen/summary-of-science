{"id": "http://arxiv.org/abs/0912.3959v1", "guidislink": true, "updated": "2009-12-20T02:19:05Z", "updated_parsed": [2009, 12, 20, 2, 19, 5, 6, 354, 0], "published": "2009-12-20T02:19:05Z", "published_parsed": [2009, 12, 20, 2, 19, 5, 6, 354, 0], "title": "Web Based Cross Language Plagiarism Detection", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0912.0430%2C0912.1571%2C0912.1741%2C0912.2630%2C0912.3991%2C0912.3420%2C0912.0389%2C0912.2065%2C0912.2352%2C0912.0312%2C0912.1660%2C0912.1779%2C0912.5363%2C0912.2637%2C0912.4541%2C0912.0797%2C0912.0792%2C0912.3747%2C0912.3854%2C0912.4995%2C0912.2015%2C0912.2104%2C0912.1242%2C0912.3309%2C0912.3975%2C0912.3165%2C0912.5318%2C0912.2054%2C0912.1226%2C0912.0479%2C0912.0740%2C0912.5228%2C0912.2140%2C0912.4295%2C0912.1180%2C0912.3085%2C0912.1742%2C0912.3100%2C0912.5155%2C0912.4037%2C0912.0256%2C0912.1375%2C0912.2271%2C0912.1692%2C0912.3148%2C0912.2187%2C0912.4847%2C0912.1353%2C0912.4631%2C0912.4872%2C0912.3776%2C0912.0688%2C0912.2790%2C0912.2154%2C0912.2635%2C0912.0910%2C0912.0261%2C0912.5026%2C0912.1650%2C0912.4431%2C0912.2204%2C0912.0009%2C0912.4889%2C0912.2546%2C0912.5491%2C0912.1763%2C0912.3959%2C0912.0893%2C0912.1081%2C0912.3684%2C0912.2769%2C0912.4191%2C0912.2585%2C0912.2760%2C0912.5503%2C0912.2951%2C0912.1879%2C0912.0801%2C0912.5191%2C0912.2643%2C0912.3832%2C0912.3611%2C0912.5296%2C0912.0802%2C0912.3172%2C0912.2476%2C0912.1579%2C0912.2400%2C0912.4285%2C0912.1523%2C0912.2071%2C0912.1331%2C0912.5266%2C0912.1541%2C0912.0401%2C0912.3922%2C0912.2529%2C0912.2085%2C0912.2900%2C0912.3342%2C0912.1687&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Web Based Cross Language Plagiarism Detection"}, "summary": "As the Internet help us cross language and cultural border by providing\ndifferent types of translation tools, cross language plagiarism, also known as\ntranslation plagiarism are bound to arise. Especially among the academic works,\nsuch issue will definitely affect the student's works including the quality of\ntheir assignments and paper works. In this paper, we propose a new approach in\ndetecting cross language plagiarism. Our web based cross language plagiarism\ndetection system is specially tuned to detect translation plagiarism by\nimplementing different techniques and tools to assist the detection process.\nGoogle Translate API is used as our translation tool and Google Search API,\nwhich is used in our information retrieval process. Our system is also\nintegrated with the fingerprint matching technique, which is a widely used\nplagiarism detection technique. In general, our proposed system is started by\ntranslating the input documents from Malay to English, followed by removal of\nstop words and stemming words, identification of similar documents in corpus,\ncomparison of similar pattern and finally summary of the result. Three\nleast-frequent 4-grams fingerprint matching is used to implement the core\ncomparison phase during the plagiarism detection process. In K-gram fingerprint\nmatching technique, although any value of K can be considered, yet K = 4 was\nstated as an ideal choice. This is because smaller values of K (i.e., K = 1, 2,\nor 3), do not provide good discrimination between sentences. On the other hand,\nthe larger the values of K (i.e., K = 5, 6, 7...etc), the better discrimination\nof words in one sentence from words in another.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0912.0430%2C0912.1571%2C0912.1741%2C0912.2630%2C0912.3991%2C0912.3420%2C0912.0389%2C0912.2065%2C0912.2352%2C0912.0312%2C0912.1660%2C0912.1779%2C0912.5363%2C0912.2637%2C0912.4541%2C0912.0797%2C0912.0792%2C0912.3747%2C0912.3854%2C0912.4995%2C0912.2015%2C0912.2104%2C0912.1242%2C0912.3309%2C0912.3975%2C0912.3165%2C0912.5318%2C0912.2054%2C0912.1226%2C0912.0479%2C0912.0740%2C0912.5228%2C0912.2140%2C0912.4295%2C0912.1180%2C0912.3085%2C0912.1742%2C0912.3100%2C0912.5155%2C0912.4037%2C0912.0256%2C0912.1375%2C0912.2271%2C0912.1692%2C0912.3148%2C0912.2187%2C0912.4847%2C0912.1353%2C0912.4631%2C0912.4872%2C0912.3776%2C0912.0688%2C0912.2790%2C0912.2154%2C0912.2635%2C0912.0910%2C0912.0261%2C0912.5026%2C0912.1650%2C0912.4431%2C0912.2204%2C0912.0009%2C0912.4889%2C0912.2546%2C0912.5491%2C0912.1763%2C0912.3959%2C0912.0893%2C0912.1081%2C0912.3684%2C0912.2769%2C0912.4191%2C0912.2585%2C0912.2760%2C0912.5503%2C0912.2951%2C0912.1879%2C0912.0801%2C0912.5191%2C0912.2643%2C0912.3832%2C0912.3611%2C0912.5296%2C0912.0802%2C0912.3172%2C0912.2476%2C0912.1579%2C0912.2400%2C0912.4285%2C0912.1523%2C0912.2071%2C0912.1331%2C0912.5266%2C0912.1541%2C0912.0401%2C0912.3922%2C0912.2529%2C0912.2085%2C0912.2900%2C0912.3342%2C0912.1687&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "As the Internet help us cross language and cultural border by providing\ndifferent types of translation tools, cross language plagiarism, also known as\ntranslation plagiarism are bound to arise. Especially among the academic works,\nsuch issue will definitely affect the student's works including the quality of\ntheir assignments and paper works. In this paper, we propose a new approach in\ndetecting cross language plagiarism. Our web based cross language plagiarism\ndetection system is specially tuned to detect translation plagiarism by\nimplementing different techniques and tools to assist the detection process.\nGoogle Translate API is used as our translation tool and Google Search API,\nwhich is used in our information retrieval process. Our system is also\nintegrated with the fingerprint matching technique, which is a widely used\nplagiarism detection technique. In general, our proposed system is started by\ntranslating the input documents from Malay to English, followed by removal of\nstop words and stemming words, identification of similar documents in corpus,\ncomparison of similar pattern and finally summary of the result. Three\nleast-frequent 4-grams fingerprint matching is used to implement the core\ncomparison phase during the plagiarism detection process. In K-gram fingerprint\nmatching technique, although any value of K can be considered, yet K = 4 was\nstated as an ideal choice. This is because smaller values of K (i.e., K = 1, 2,\nor 3), do not provide good discrimination between sentences. On the other hand,\nthe larger the values of K (i.e., K = 5, 6, 7...etc), the better discrimination\nof words in one sentence from words in another."}, "authors": ["Chow Kok Kent", "Naomie Salim"], "author_detail": {"name": "Naomie Salim"}, "author": "Naomie Salim", "links": [{"href": "http://arxiv.org/abs/0912.3959v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0912.3959v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.OH", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.OH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0912.3959v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0912.3959v1", "arxiv_comment": null, "journal_reference": "Journal of Computing, Volume 1, Issue 1, pp 39-43, December 2009", "doi": null, "fulltext": "JOURNAL OF COMPUTING, VOLUME 1, ISSUE 1, DECEMBER 2009, ISSN: 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\n39\n\nWeb Based Cross Language Plagiarism\nDetection\nChow Kok Kent, Naomie Salim\nFaculty of Computer Science and Information Systems, University Teknologi Malaysia, 81310 Skudai,\nJohor, Malaysia\n\nAbstract- As the Internet help us cross language and cultural border by providing different types of translation tools, cross language plagiarism, also known as translation plagiarism are bound to arise. Especially among the academic works, such issue will definitely affect the\nstudent's works including the quality of their assignments and paper works. In this paper, we propose a new approach in detecting cross language plagiarism. Our web based cross language plagiarism detection system is specially tuned to detect translation plagiarism by implementing different techniques and tools to assist the detection process. Google Translate API is used as our translation tool and Google\nSearch API, which is used in our information retrieval process. Our system is also integrated with the fingerprint matching technique, which is\na widely used plagiarism detection technique. In general, our proposed system is started by translating the input documents from Malay to\nEnglish, followed by removal of stop words and stemming words, identification of similar documents in corpus, comparison of similar pattern\nand finally summary of the result. Three least-frequent 4-grams fingerprint mathching is used to implement the core comparison phase during\nthe plagiarism detection process. In K-gram fingerprint matching techinique, although any value of K can be considered, yet K = 4 was stated\nas an ideal choice. This is because smaller values of K (i.e., K = 1, 2, or 3), do not provide good discrimination between sentences. On the\nother hand, the larger the values of K (i.e., K = 5, 6, 7...etc), the better discrimination of words in one sentence from words in another. However each K-gram requires K bytes of storage and hence space-consuming becomes too large for larger values of K. Hence, three leastfrequent 4-grams are the best option to represent the sentence uniquely.\n\n---------- \u008b ----------\n\n1 INTRODUCTION\nAcademic plagiarism is defined as inappropriate use by\nstudent of material originated by others. The legal and\nethical frameworks are far from clear, and the Internet has\nbrought new opportunities for plagiarists and new inves\u2010\ntigative tools for their opponents. Numerous studies\nshow that plagiarism and other types of academic fraud is\nincreasing among students. The most significant phe\u2010\nnomenon among the students is during their thesis writ\u2010\ning period. In a recent article published by the Center for\nAcademic Integrity (CAI), Professor McCabe claims that\n\u02baOn most campuses, 70% of students admit to some\ncheating\u02ba while \u02baInternet plagiarism is a growing con\u2010\ncern\u02ba because although only \u02ba10% of students admitted to\nengaging in such behavior in 1999, almost 40%\u02ba admitted\nto it in 2005\u02ba [1].\nIn general, there are various forms of plagiarism\nsuch as straight plagiarism, simple plagiarism with foot\u2010\nnote, complex plagiarism using footnote, plagiarism us\u2010\ning citation but without quotation marks, and paraphras\u2010\ning as plagiarism. The practice of plagiarism is a form of\nacademic high treason because it undermines the entire\nscholarly enterprise. Plagiarized news, magazines articles\nand web resources are the area of concern in this plagia\u2010\nrism issue.\nTranslation plagiarism is considered as transla\u2010\ntion of a sentence in the source language in the way that\n\nends up using almost the exact same words as the original\nsource used by the original author [2]. Even though trans\u2010\nlation plagiarism does not copy the original work directly,\nit still considered as plagiarism as it try to imitate and use\nother people ideas or thoughts without any citation or\nquotation marks. These incidents are much harder to de\u2010\ntect, as translation is often a fuzzy process that is hard to\nsearch for, and even harder to stop as they usually cross\ninternational border [3].\nSeveral arguments including Intellectual Prop\u2010\nerty (IP), ethics, legal constraint, and copyright has been\nraised up due to the plagiarism issue. Intellectual prop\u2010\nerty (IP) is a legal property right over the creation of the\nmind, both artistic and commercial, and the correspond\u2010\ning fields of law [4]. Where argument is provided, it is\noften along the lines that plagiarism is morally wrong\nbecause the plagiariser both claims a contribution that\nthey are not justified in claiming, and denies the origina\u2010\ntor the credit due to them. One further body of law re\u2010\nquires more detailed consideration. Copyright law pro\u2010\nvides the author or commissioner of a work with a small\nbasket of specific rights in relation to the work.\n\n\fJOURNAL OF COMPUTING, VOLUME 1, ISSUE 1, DECEMBER 2009, ISSN: 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\n2\n\nRELATED WORKS\n\nHere are a few existing plagiarism detection tools or ser\u2010\nvices that assist in plagiarism detection [5].\n\nTurnitin.com (formerly Plagiarism.org)\nhttp://www.turnitin.com\nTurnitin.com uses digital fingerprinting to match submit\u2010\nted papers against internet resources and an in\u2010house\ndatabase of previously submitted papers. According to\nSatterwhite and Gerein, Turnitin.com has the highest rate\nof detection amongst subscription detection tools. Papers\ncan be submitted individually by either students or lec\u2010\nturers. All papers are archived for future checking \u2013 a\nfeature which is particularly useful if copying of previous\nstudents' papers is suspected. [6]\n\nPlagiserve.com http://www.plagiaserve.com\nPlagiserve.com is a free service which searches the inter\u2010\nnet for duplicates of submitted papers, analyses them,\nand provides evidence of plagiarism to the lecturer. It has\nan extensive database of 90,000 papers, essays and Cliff\u2010\nNotes study guides, and papers from all known paper\nmills. Reports are generated in 12 hours. The service is\nonly available through its website, and papers must be\nsubmitted in one batch.\n\nCopyCatch Gold http://www.copycatch.freeserve.co.uk\n\n40\n\npaper that is plagiarised. Identified limitations include\nthe inability to trace documents that are not in html for\u2010\nmat, inability to trace collusion between students, the in\u2010\nability to search subscription websites.\n\nWordCheck Keyword DP\nhttp://www.wordchecksystems.com/wordcheck\u2010dp.html\nWordCheck Keyword DP is a software program which\nidentifies key word use, and matches documents based\non word use and frequency patterns. Reports showing\nkey word profiles and word frequency lists are generated.\nLimitations of the program include the time\u2010consuming\nmanual checking of each document and the inability of\nthe program to detect plagiarism from internet sources.\n\nMost of the detection softwares mentioned above use the\nwell\u2010known journal or papers database such as Pro\u2010\nQuest\u00ae, FindArticles\u00ae, and LookSmart\u00ae as their corpus.\nAlthough these tools provide an effective alternative in\nplagiarism detection, the tools not derived for translated\nplagiarism. For instance, if someone translates exactly a\ncopy of English paper into Bahasa Melayu, no tool will\never detect the translated version as plagiarized\n\n3 OPERATIONAL FRAMEWORK\nIn this paper, we focus on detecting the Malay\u2010English\nplagiarism. As a web based plagiarism detection system,\nour corpus is built up with most of the Internet resources\nthat are detectable by the Google search engine.\n\nCopyCatch Gold is a stand\u2010alone desktop software which\ncan be either installed on a single PC or on a network. It\ndetects collusion between students by checking similari\u2010\nties between words and phrases within work submitted\nby one group of students. One of the special features of\nCopyCatch is that it assists students in their writing de\u2010\nvelopment, by allowing them to see where they are re\u2010\npeating text from other sources and from their own pre\u2010\nvious assignments. Its weakness is the inability to detect\nmaterial downloaded from the web.\n\nEve2 \u2013 Essay Verification Engine\nhttp://www.canexus.com/eve/index.shtml\nEve2 is a windows based system, installed on individual\nworkstations. It is not easily installed on servers. Papers\nare submitted by cutting and pasting plain text, Microsoft\nWord, or Word Perfect documents into a text box. The\nprogram then searches internet resources for matching\ntext. Reports are provided within a few minutes, high\u2010\nlighting suspect text, and indicating the percentage of the\n\nFig.1 Cross Language Plagiarism Detection Framework\n\n3.1 Translation of Input Documents\nIn order to detect translation plagiarisms, it is essential to\ntranslate the plagiarized Malay documents into English\n\n\fJOURNAL OF COMPUTING, VOLUME 1, ISSUE 1, DECEMBER 2009, ISSN: 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\nbefore used as the query documents for further detection\nprocess. After the plagiarized documents have been trans\u2010\nlated into English, it will improve the effectiveness of the\ndetection process as the source documents are also in\nEnglish. We use Google Translate API which is a well\u2010\nknown translation tool developed by the Google and is\nfreely distributed. With this API, the language blocks of\ntext can be easily detected and translated to other pre\u2010\nferred languages. The API is designed to be a simple and\neasy to detect or translate languages when offline transla\u2010\ntion is not available.\n\n41\n\nremoval of prefixes and suffixes of a word. It does not\nrely on a lookup table that consists of inflected and roots\nform. Instead, it is a smaller list of rules that provide the\ncore of the algorithm.\nPrefix Stripping:\nIf the word begin in \"in\", remove the 'in'\nIf the word begin in \"inter\", remove the 'inter'\nSuffix Stripping:\nIf the word ends in 'ed', remove the 'ed'\nIf the word ends in 'ing', remove the 'ing'\nIf the word ends in 'ly', remove the 'ly'\n\n3.2 Removing Stop Words\nStop words are the words that frequently occurred in\ndocuments. These words do not give any hint values or\nmeanings to the content of their documents, hence they\nare eliminated from the set of index terms [7]. Salton and\nMcGill (1993) reported that such words comprise around\n40 to 50% of a collection of documents text words [8].\nEliminating the stop words in automatic indexing will\nspeed the system processing, saves a huge amount of\nspace in index, and does not damage the retrieval effec\u2010\ntiveness [9].\nIn this paper, before passing the translated\ndocuments for comparison through the Internet, it is es\u2010\nsential for us to remove the stop words in the translated\ntext. English stop words will be removed in the translated\ntexts. Currently, there are several English stop words list\nthat commonly used in the information retrieval process.\nSome of the general English stop words are shown as be\u2010\nlow.\na, an, the, ourselves, been, anywhere, any, by, did, each,\never, even, would, could, few, than, all\nBefore removing stop words:\nIn information retrieval, stop words are the words that fre\u2010\nquently occurred in the documents.\nAfter removing stop words:\ninformation retrieval, stop words words frequently occurred\ndocuments.\n\n3.3 Stemming Words\nStemming is a process to remove the affixes (prefixes and\nsuffixes) in a word in order to generate its root word. Us\u2010\ning root word in pattern matching provides a much better\neffectiveness in information retrieval. There are many\nstemmers available for the English language such as Nice\nStemmer, Text Stemmer and Porter Stemmer are the well\u2010\nknown English stemmer that commonly been used.\nAffix stripping is one of the common algorithms\nused in stemming process. Affix stripping includes the\n\nIn this paper, we propose the use of Porter Stemming al\u2010\ngorithm in our stemming process. The Porter stemming\nalgorithm (or 'Porter stemmer') is a process for removing\nthe commoner morphological and inflexional endings\nfrom words in English. Its main use is as part of a term\nnormalisation process that is usually done when setting\nup information retrieval systems. The original stemming\nalgorithm paper was written in 1979 in the Computer\nLaboratory, Cambridge (England), as part of a larger IR\nproject, and currently is widely used as a stemming algo\u2010\nrithm which is fully tested for its accuracy and effective\u2010\nness.\n\n3.4 Identifying Similar Documents in Corpus\nCorpus (collection of documents) can be either intra\u2010\ncorpus or inter\u2010corpus. Intra\u2010corpus is defined as a collec\u2010\ntion of documents which are not distributed over the het\u2010\nerogeneous network and can be found in the same stor\u2010\nage. Inter\u2010corpus is the collection of documents that lo\u2010\ncated around the World Wide Web.\nInstead of using an intra corpus, it is preferable\nto use a inter corpus which consists of a collection of\nsources through the Internet. In this case, search engine is\nan effective alternative. An internet search engine can be\nused to look for certain keywords and key sentences from\na suspected document on the World Wide Web. The de\u2010\ntection process becomes more effective as the World Wide\nWeb acts as a large collection of document (corpus) and\nenables small and characteristic fragments translation.\nQuery documents or texts are inserted into the search\nengine and a set of results are generated by the search\nengine. These results show the similar part of the query\ndocuments and their corresponding similar sources of\ndocuments.\nIn this paper, we propose the use of Google\nAJAX Search API (Google search engine) as our corpus.\nGoogle search engine is considered as one of the most\ncomplete online resources collection. Even papers pub\u2010\nlished in some free server or outside the well\u2010known\n\n\fJOURNAL OF COMPUTING, VOLUME 1, ISSUE 1, DECEMBER 2009, ISSN: 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\njournal database are also detectable using the Google\nsearch engine. The stemmed plagiarized documents are\ninserted into the search engine and a set of similar source\ndocuments can be retrieved.\n\n42\n\nwhere F(A) and F(B) are the common fingerprints in\ndocuments A and B, respectively.\n\n3.6 Summary of the Result\n3.5 Comparison of Similar Pattern\nFingerprint matching technique is widely used in the pla\u2010\ngiarism detection tools. Fingerprinting divides the docu\u2010\nment into grams of certain length k. In full fingerprinting,\ndocument fingerprint consists of the set of all possible\nsubstrings of length K. The fingerprints of two documents\ncan be compared in order to detect plagiarism. In this\npaper, we represent the fingerprints of each statement in\nthe documents by three least\u2010frequent 4\u2010grams. Although\nany value of K can be considered, yet K = 4 was stated as\nan ideal choice by Yerra and Ng (2005) [10]. This is be\u2010\ncause smaller values of K (i.e., K = 1, 2, or 3), do not pro\u2010\nvide good discrimination between sentences. On the other\nhand, the larger the values of K (i.e., K = 5, 6, 7...etc), the\nbetter discrimination of words in one sentence from\nwords in another. However each K\u2010gram requires K bytes\nof storage and hence space\u2010consuming becomes too large\nfor larger values of K.\nThree least\u2010frequent 4\u2010grams are the best option\nto represent the sentence uniquely. To illustrate the three\nleast\u2010frequent 4\u2010gram construction process, consider the\nfollowing sentence S \"soccer game is fantastic\". The 4\u2010\ngrams are socc, occe, ccer, cerg, etc. In this method, instead\nof comparing all possible 4\u2010grams, only three 4\u2010grams\nwhich have the least frequency over all 4\u2010grams will be\nchosen.\nLet the document contain J distinct n\u2010grams, with mi oc\u2010\ncurrences of n\u2010gram number i. Then the weight assigned\nto the ith n\u2010gram will be\n\nWhere\n\nThe three least\u2010frequent 4\u2010grams are concatenated to rep\u2010\nresent the fingerprint of a sentence from the query docu\u2010\nment to be compared with the three least\u2010frequent 4\u2010gram\nrepresentations of sentences in the source documents.\nFinally, two sentences are treated the same if their corre\u2010\nsponding three least frequent 4\u2010gram representations are\nthe same. A measure of resemblance for each pair of\ndocuments is computed as follows:\n\nAfter gathering the result, a summary of the plagiarism\ndetection is displayed. The plagiarized parts of the docu\u2010\nments, source of the plagiarism, and the similarity per\u2010\ncentage are all shown in the summary. Further works are\nneeded in order to determine the existence of proper cita\u2010\ntion or quotations in those plagiarized documents.\n\n4\n\nDISCUSSION AND FUTURE WORKS\n\nVolume of the Corpus\nPlagiarism detection tools concentrate on the speed and\nwidth of detection, at the cost of quality of detection. The\nspeed is the processing time and the respond time of the\nsystem once the data is inserted. The detection width re\u2010\nfers to the database available for searching the source\ndocument given the suspected plagiarized documents.\nHence, the probability in detecting the plagiarism is\nhigher if the volume of the corpus is larger. However,\ndifficulties arise when searching similar document in\nsome small sites where authors who have published their\npaper in a journal and keep their own copy in their own\nserver for free. Their papers are not published in some\nwell\u2010known journal or papers database such as Pro\u2010\nQuest\u00ae, FindArticles\u00ae, and LookSmart\u00ae. In this case, the\npapers are hardly detectable using own developed search\nengine.\n\nParaphrasing Issue\nParaphrasing is the techniques to modify the structure of\nan original sentence by changing the sentence's structure\nor replaces some of the original words with its synonym.\nWithout any proper citation or quotation marks, it also\nconsidered as plagiarism. However, it is generally be\u2010\nlieved nowadays that fingerprint\u2010based approach is quite\nweak since even slight textual modification can consid\u2010\nerably affect the fingerprint of the document. Most of the\nplagiarism tools are not stable against synonyms. If\nsomeone copies and systematically changes words by\nusing synonyms or such, all plagiarism tools we are\naware of will fail.\n\nAuthorship Identification and Stylometry Analysis\nAuthorship identification and stylometry analysis can be\nused as an alternative in detecting plagiarism indirectly.\nIt can be an indication of plagiarism if there is no essay\nchunk can be found either on Internet or inside other stu\u2010\n\n\fJOURNAL OF COMPUTING, VOLUME 1, ISSUE 1, DECEMBER 2009, ISSN: 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\ndent's submission. . If the suspected documents consist of\na large amount of text, stylometry analysis will be an ef\u2010\nfective detection techniques where each parts of the\ndocuments can be investigated based on the authorship\nand style of writing.\n\n5\n\nCONCLUSION\n\nIn this paper we presented our web based cross language\nplagiarism detection. Translation plagiarism is becoming\na major issue and concern especially in the academic\nworks. With our experiment, we feel that our system can\ndetect the translation plagiarism with high efficiency and\neffectiveness. Google API can be further utilized in our\ndetection system to improve the detection performance.\n\n6\n\nACKNOWLEDGEMENT\n\nThis project is sponsored partly by the Ministry of Sci\u2010\nence, Technology and Innovation under the E\u2010Science\ngrant 01\u201001\u201006\u2010SF\u20100502.\n\n[11] UEL Plagiarism Detection Software Group. Report on the vi\u2010\nability of CopyCatch plagiarism detection software. London:\nUniversity of East London. September 2002.\n[12] Plagiarism and Turnitin.com FAQ Adelphi, Maryland: Univer\u2010\nsity of Maryland University College. 2003.\n\nMr. Chow Kok Kent received his B.Sc. degree in Univer\u2010\nsiti Teknologi Malaysia, Malaysia in 2009. He is currently\npurchasing Master degree in Faculty of Computer Science\nand Information System, Universiti Teknologi Malaysia.\nHis current research interest includes information re\u2010\ntrieval, Plagiarism Detection and Soft Computting.\n\nDr. Naomie Salim is an Assoc.Prof. presently working as\na Deputy Dean of Postgraduate Studies in the Faculty of\nComputer Science and Information System in Universiti\nTeknologi Malaysia. She received her degree in Computer\nScience from Universiti Teknologi Malaysia in 1989. She\nreceived her Master degree from University of Illinois and\nPh.D Degree from University of Sheffield in 1992 and\n2002 respectively. Her current research interest includes\nInformation Retrieval, Distributed Database and\nChemoinformatic.\n\n.\n\n7\n[1]\n\nREFERENCES\n\nDr Don McCabe. North American Research on Academic Integ\u2010\nrity. University of British Columbia. September 2003.\n[2] Plagiarism: deliberate, inadvertent \u2013 but almost importantly \u2013\ntranslation available at\ngeo.proz.com/.../translation.../130082plagiarism:_deliberate_ina\ndvertent_but_most_importantly_translation.html.\n[3] Translation Plagiarism available at\nhttp://www.plagiarismtoday.com/2005/08/13/translation\u2010\nplagiarism/.\n[4] Richard Raysman, Edward A. Pisacreta and Kenneth A. Adler.\nIntellectual Property Licensing: Forms and Analysis. Law Jour\u2010\nnal Press, 1999\u20102008.\n[5] Plagiarism detection software, its use by universities, and stu\u2010\ndent attitudes to cheating. University of Sydney Teaching and\nLearning Committee. 2003.\n[6] Jasper\u2010Parisey. Turnitin.com Library Journal 126 (8). May 1\n2001. page 138.\n[7] C.J. van Rijsbergen. A New Theoretical Framework for Infor\u2010\nmation Retrieval. Department of Computing Scirnce University\nof Glasgow. 1979.\n[8] Salton and McGill. Query Processing and Inverted Indices in\nShared\u2010Nothing Document Information Retrieval Systems.\nDBLP Computer Science Bibliography. 1993.\n[9] W.B. Frakes and R. Baeza\u2010Yates. Information Retrieval: Data\nStructures and Algorithm. Department of Computer Science,\nUniversity of Chile. 1992.\n[10] Yerra and Ng. A Sentence\u2010Based Copy Detection Approach for\nWeb Documents. Fuzzy Systems and Knowledge Discovery.\n2005.\n\n43\n\n.\n\n\f"}