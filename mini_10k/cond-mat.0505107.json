{"id": "http://arxiv.org/abs/cond-mat/0505107v1", "guidislink": true, "updated": "2005-05-04T15:13:45Z", "updated_parsed": [2005, 5, 4, 15, 13, 45, 2, 124, 0], "published": "2005-05-04T15:13:45Z", "published_parsed": [2005, 5, 4, 15, 13, 45, 2, 124, 0], "title": "A step beyond Tsallis and Renyi entropies", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0505414%2Ccond-mat%2F0505737%2Ccond-mat%2F0505532%2Ccond-mat%2F0505246%2Ccond-mat%2F0505234%2Ccond-mat%2F0505304%2Ccond-mat%2F0505087%2Ccond-mat%2F0505603%2Ccond-mat%2F0505100%2Ccond-mat%2F0505258%2Ccond-mat%2F0505071%2Ccond-mat%2F0505330%2Ccond-mat%2F0505598%2Ccond-mat%2F0505512%2Ccond-mat%2F0505003%2Ccond-mat%2F0505256%2Ccond-mat%2F0505389%2Ccond-mat%2F0505046%2Ccond-mat%2F0505429%2Ccond-mat%2F0505665%2Ccond-mat%2F0505571%2Ccond-mat%2F0505199%2Ccond-mat%2F0505572%2Ccond-mat%2F0505569%2Ccond-mat%2F0505438%2Ccond-mat%2F0505012%2Ccond-mat%2F0505241%2Ccond-mat%2F0505627%2Ccond-mat%2F0505563%2Ccond-mat%2F0505450%2Ccond-mat%2F0505166%2Ccond-mat%2F0505233%2Ccond-mat%2F0505288%2Ccond-mat%2F0505639%2Ccond-mat%2F0505727%2Ccond-mat%2F0505102%2Ccond-mat%2F0505723%2Ccond-mat%2F0505125%2Ccond-mat%2F0505383%2Ccond-mat%2F0505590%2Ccond-mat%2F0505619%2Ccond-mat%2F0505674%2Ccond-mat%2F0505767%2Ccond-mat%2F0505070%2Ccond-mat%2F0505551%2Ccond-mat%2F0505182%2Ccond-mat%2F0505343%2Ccond-mat%2F0505108%2Ccond-mat%2F0505556%2Ccond-mat%2F0505418%2Ccond-mat%2F0505570%2Ccond-mat%2F0505435%2Ccond-mat%2F0505427%2Ccond-mat%2F0505299%2Ccond-mat%2F0505615%2Ccond-mat%2F0505151%2Ccond-mat%2F0505367%2Ccond-mat%2F0505342%2Ccond-mat%2F0505107%2Ccond-mat%2F0505120%2Ccond-mat%2F0505013%2Ccond-mat%2F0505329%2Ccond-mat%2F0505415%2Ccond-mat%2F0505197%2Ccond-mat%2F0505701%2Ccond-mat%2F0505387%2Ccond-mat%2F0505269%2Ccond-mat%2F0505183%2Ccond-mat%2F0505681%2Ccond-mat%2F0505622%2Ccond-mat%2F0505401%2Ccond-mat%2F0505386%2Ccond-mat%2F0505132%2Ccond-mat%2F0505668%2Ccond-mat%2F0505152%2Ccond-mat%2F0505596%2Ccond-mat%2F0505479%2Ccond-mat%2F0505653%2Ccond-mat%2F0505015%2Ccond-mat%2F0505041%2Ccond-mat%2F0505407%2Ccond-mat%2F0505689%2Ccond-mat%2F0505625%2Ccond-mat%2F0505732%2Ccond-mat%2F0505056%2Ccond-mat%2F0505696%2Ccond-mat%2F0505507%2Ccond-mat%2F0505492%2Ccond-mat%2F0505230%2Ccond-mat%2F0505363%2Ccond-mat%2F0505313%2Ccond-mat%2F0505765%2Ccond-mat%2F0505599%2Ccond-mat%2F0505127%2Ccond-mat%2F0505092%2Ccond-mat%2F0505550%2Ccond-mat%2F0505552%2Ccond-mat%2F0505578%2Ccond-mat%2F0505748%2Ccond-mat%2F0505336%2Ccond-mat%2F0505089&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A step beyond Tsallis and Renyi entropies"}, "summary": "Tsallis and R\\'{e}nyi entropy measures are two possible different\ngeneralizations of the Boltzmann-Gibbs entropy (or Shannon's information) but\nare not generalizations of each others. It is however the Sharma-Mittal\nmeasure, which was already defined in 1975 (B.D. Sharma, D.P. Mittal,\nJ.Math.Sci \\textbf{10}, 28) and which received attention only recently as an\napplication in statistical mechanics (T.D. Frank & A. Daffertshofer, Physica A\n\\textbf{285}, 351 & T.D. Frank, A.R. Plastino, Eur. Phys. J., B \\textbf{30},\n543-549) that provides one possible unification. We will show how this\ngeneralization that unifies R\\'{e}nyi and Tsallis entropy in a coherent picture\nnaturally comes into being if the q-formalism of generalized logarithm and\nexponential functions is used, how together with Sharma-Mittal's measure\nanother possible extension emerges which however does not obey a\npseudo-additive law and lacks of other properties relevant for a generalized\nthermostatistics, and how the relation between all these information measures\nis best understood when described in terms of a particular logarithmic\nKolmogorov-Nagumo average.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0505414%2Ccond-mat%2F0505737%2Ccond-mat%2F0505532%2Ccond-mat%2F0505246%2Ccond-mat%2F0505234%2Ccond-mat%2F0505304%2Ccond-mat%2F0505087%2Ccond-mat%2F0505603%2Ccond-mat%2F0505100%2Ccond-mat%2F0505258%2Ccond-mat%2F0505071%2Ccond-mat%2F0505330%2Ccond-mat%2F0505598%2Ccond-mat%2F0505512%2Ccond-mat%2F0505003%2Ccond-mat%2F0505256%2Ccond-mat%2F0505389%2Ccond-mat%2F0505046%2Ccond-mat%2F0505429%2Ccond-mat%2F0505665%2Ccond-mat%2F0505571%2Ccond-mat%2F0505199%2Ccond-mat%2F0505572%2Ccond-mat%2F0505569%2Ccond-mat%2F0505438%2Ccond-mat%2F0505012%2Ccond-mat%2F0505241%2Ccond-mat%2F0505627%2Ccond-mat%2F0505563%2Ccond-mat%2F0505450%2Ccond-mat%2F0505166%2Ccond-mat%2F0505233%2Ccond-mat%2F0505288%2Ccond-mat%2F0505639%2Ccond-mat%2F0505727%2Ccond-mat%2F0505102%2Ccond-mat%2F0505723%2Ccond-mat%2F0505125%2Ccond-mat%2F0505383%2Ccond-mat%2F0505590%2Ccond-mat%2F0505619%2Ccond-mat%2F0505674%2Ccond-mat%2F0505767%2Ccond-mat%2F0505070%2Ccond-mat%2F0505551%2Ccond-mat%2F0505182%2Ccond-mat%2F0505343%2Ccond-mat%2F0505108%2Ccond-mat%2F0505556%2Ccond-mat%2F0505418%2Ccond-mat%2F0505570%2Ccond-mat%2F0505435%2Ccond-mat%2F0505427%2Ccond-mat%2F0505299%2Ccond-mat%2F0505615%2Ccond-mat%2F0505151%2Ccond-mat%2F0505367%2Ccond-mat%2F0505342%2Ccond-mat%2F0505107%2Ccond-mat%2F0505120%2Ccond-mat%2F0505013%2Ccond-mat%2F0505329%2Ccond-mat%2F0505415%2Ccond-mat%2F0505197%2Ccond-mat%2F0505701%2Ccond-mat%2F0505387%2Ccond-mat%2F0505269%2Ccond-mat%2F0505183%2Ccond-mat%2F0505681%2Ccond-mat%2F0505622%2Ccond-mat%2F0505401%2Ccond-mat%2F0505386%2Ccond-mat%2F0505132%2Ccond-mat%2F0505668%2Ccond-mat%2F0505152%2Ccond-mat%2F0505596%2Ccond-mat%2F0505479%2Ccond-mat%2F0505653%2Ccond-mat%2F0505015%2Ccond-mat%2F0505041%2Ccond-mat%2F0505407%2Ccond-mat%2F0505689%2Ccond-mat%2F0505625%2Ccond-mat%2F0505732%2Ccond-mat%2F0505056%2Ccond-mat%2F0505696%2Ccond-mat%2F0505507%2Ccond-mat%2F0505492%2Ccond-mat%2F0505230%2Ccond-mat%2F0505363%2Ccond-mat%2F0505313%2Ccond-mat%2F0505765%2Ccond-mat%2F0505599%2Ccond-mat%2F0505127%2Ccond-mat%2F0505092%2Ccond-mat%2F0505550%2Ccond-mat%2F0505552%2Ccond-mat%2F0505578%2Ccond-mat%2F0505748%2Ccond-mat%2F0505336%2Ccond-mat%2F0505089&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Tsallis and R\\'{e}nyi entropy measures are two possible different\ngeneralizations of the Boltzmann-Gibbs entropy (or Shannon's information) but\nare not generalizations of each others. It is however the Sharma-Mittal\nmeasure, which was already defined in 1975 (B.D. Sharma, D.P. Mittal,\nJ.Math.Sci \\textbf{10}, 28) and which received attention only recently as an\napplication in statistical mechanics (T.D. Frank & A. Daffertshofer, Physica A\n\\textbf{285}, 351 & T.D. Frank, A.R. Plastino, Eur. Phys. J., B \\textbf{30},\n543-549) that provides one possible unification. We will show how this\ngeneralization that unifies R\\'{e}nyi and Tsallis entropy in a coherent picture\nnaturally comes into being if the q-formalism of generalized logarithm and\nexponential functions is used, how together with Sharma-Mittal's measure\nanother possible extension emerges which however does not obey a\npseudo-additive law and lacks of other properties relevant for a generalized\nthermostatistics, and how the relation between all these information measures\nis best understood when described in terms of a particular logarithmic\nKolmogorov-Nagumo average."}, "authors": ["Marco Masi"], "author_detail": {"name": "Marco Masi"}, "author": "Marco Masi", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.physleta.2005.01.094", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cond-mat/0505107v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cond-mat/0505107v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cond-mat/0505107v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cond-mat/0505107v1", "arxiv_comment": null, "journal_reference": "Physics Letters A Volume 338, Issues 3-5, 2 May 2005, Pages\n  217-224", "doi": "10.1016/j.physleta.2005.01.094", "fulltext": "arXiv:cond-mat/0505107v1 [cond-mat.stat-mech] 4 May 2005\n\nA step beyond Tsallis and R\u00e9nyi entropies\nMarco Masi \u2217\nDipartimento di Fisica G. Galilei, Padova, Italy.\n\nAbstract\nTsallis and R\u00e9nyi entropy measures are two possible different generalizations of the\nBoltzmann-Gibbs entropy (or Shannon's information) but are not generalizations\nof each others. It is however the Sharma-Mittal measure, which was already defined in 1975 (B.D. Sharma, D.P. Mittal, J.Math.Sci 10, 28) and which received\nattention only recently as an application in statistical mechanics (T.D. Frank &\nA. Daffertshofer, Physica A 285, 351 & T.D. Frank, A.R. Plastino, Eur. Phys.\nJ., B 30, 543-549) that provides one possible unification. We will show how this\ngeneralization that unifies R\u00e9nyi and Tsallis entropy in a coherent picture naturally comes into being if the q-formalism of generalized logarithm and exponential\nfunctions is used, how together with Sharma-Mittal's measure another possible extension emerges which however does not obey a pseudo-additive law and lacks of\nother properties relevant for a generalized thermostatistics, and how the relation\nbetween all these information measures is best understood when described in terms\nof a particular logarithmic Kolmogorov-Nagumo average.\nKey words: Generalized information entropy measures, Tsallis, R\u00e9nyi,\nSharma-Mittal\nPACS: 05.70, 65.50, 89.70, 05.70.L\n\n\u2217 Corresponding author.\nEmail address: marco.masi@spiro.fisica.unipd.it, marco masi2@tin.it\n(Marco Masi).\n\nPreprint submitted to Elsevier Science\n\n2 February 2008\n\n\f1\n\nIntroduction\n\nTo gain a unified understanding of the different entropy measures and how they\nrelate to each others in the frame of a generalized picture, it is first necessary\nto recall what characterizes \"classical\" entropies and emphasize some aspects\nwhich are important for the present paper.\n\n1.1 The Boltzmann-Gibbs entropy and Shannon's information measure\n\nAs it is well known, given a probability distribution P = {pi }, (i = 1, ..., N),\nwith pi representing the probability of the system to be in the i-th microstate,\nthe Boltzmann-Gibbs (BG) entropy reads\nSBG (P ) = \u2212k\n\nN\nX\n\npi log pi ,\n\ni=1\n\nwhere k is the Boltzmann constant and N the total number of possible configurations. If all states are equi-probable it leads to the famous Boltzmann\nprinciple S = k log W (N=W). BG entropy is equivalent to Shannon's expression if we set k = 1 (as we will do from now on) and use the immaterial base\nb for the logarithm function\nSS (P ) = \u2212\n\nN\nX\n\npi logb pi .\n\ni=1\n\nIt is common to use the natural base for the BG entropy, while base 2 has the\nadvantage to deliver information quantities in bits.\nWhat characterizes BG and Shannon's measure is additivity of information.\nGiven two systems, described by two independent probability distributions A\nand B (i.e. P (A \u2229 B) = P (A)P (B)), using an additive information measure\nmeans that\nSS (A \u2229 B) = SS (A) + SS (B|A) ,\nwith\nSS (B|A) =\n\nX\n\npi (A) SS (B|A = Ai ) ,\n\ni\n\nbeing the conditional entropy. In this case we are talking about extensive systems, i.e. systems where the entropy is given by the sum of all the entropies\nof their parts, as it is customary to do in standard statistical mechanics. The\nunique function which assures additivity is the logarithm. Also in the axiomatic derivation of Shannon's entropy performed by A.I. Khinchin (6), it is\nthe additive property which leads to the appearance of the logarithm function.\n2\n\n\fThis is the real reason that stands behind the ubiquitous presence of the logarithm function in information theory, and we can confidently say that every\nmodification to it reflects a deviation from the additive law.\nWe will from now on use the natural base. Shannon's entropy can be written\nin the form of a \"linear\" (the arithmetic) mean as\nSS (P ) = hIi ilin\n\n*\n\n1\n= log\npi\n\n!+\n\n,\n\n(1.1)\n\nlin\n\nwhere we will call the quantity\n1\nIi = log\npi\n\n!\n\n,\n\nthe elementary information gain associated to an event of probability pi (in\ninformation theory it is sometimes called the code length). The quantity p1i is\nalso called the surprise (less probable events are considered more \"surprising\"\nthan more probable ones), and we will see that it is this quantity which is\nreally measured in one way or another, not \u2212 log pi .\n\n1.2 Tsallis' entropy\n\nAdditivity is however not always preserved, especially in nonlinear complex\nsystems, e.g. when we have to deal with long range forces, as it is in the\ncase of the dynamic evolution of star clusters or in systems with long range\nmicroscopic memory, in fractal- or multifractal-like and self-organized critical\nsystems, etc. We are dealing in this case with non-extensive systems; a case\nwhich received much attention in the last decade.(15)\nA generalization of the BG entropy to non-extensive systems is known as\nTsallis entropy (14). C. Tsallis noted that if non-extensivity enters into the\nplay things are described better by power law distributions, piq , so called qprobabilities, i.e. by scaled probabilities where q is a real parameter. This\nintroduces the formal possibility not to set rare and common events on the\nsame footing, as in BG or Shannon statistics, but it enhances or depresses\nthem according to the parameter chosen (in complex systems rare events can\nhave dramatic effects on the overall evolution).\nWith the introduction of the normalized q-probabilities it became customary\nto define so called escort- or zooming-distribution\npq\n\u03c0i (P, q) = PN i\n\ni=1\n\npqi\n\n;\n\n3\n\nq > 0, q \u2208 R.\n\n\fIn this frame Tsallis postulated his now famous generalization of Shannon's\nentropy to non-extensivity (14):\n\nST (P, q) =\n\nPN\n\nN\npqi \u2212 1\n1 X\npi (1 \u2212 piq\u22121 ) .\n=\n1\u2212q\nq \u2212 1 i=1\n\ni=1\n\n(1.2)\n\nFor q \u2192 1, Shannon's measure is recovered, i.e.: ST (P, 1) = SS (P ) .\nTsallis entropy extends to a pseudo-additive law\nST (A \u2229 B) = ST (A) + ST (B|A) + (1 \u2212 q)ST (A)ST (B|A) ,\nwith\nST (B|A) =\n\nX\n\n(1.3)\n\n\u03c0i (A) ST (B|A = Ai ) .\n\ni\n\nLet us introduce the generalized q-logarithm function\nx1\u2212q \u2212 1\nlogq x =\n,\n1\u2212q\n\n(1.4)\n\nwhich, for q = 1, becomes again the common natural logarithm. Its inverse is\nthe generalized q-exponential function\n1\n\nexq = [1 + (1 \u2212 q)x] 1\u2212q ,\n\n(1.5)\n\nwhich becomes the exponential function for q = 1. The importance of the\nq-logarithm is that it satisfies a pseudo-additive law\nlogq xy = logq x + logq y + (1 \u2212 q)(logq x)(logq y) .\n\n(1.6)\n\nThen Tsallis entropy 1.2 can be written as the q-deformed Shannon entropy\nST (P, q) = \u2212\n\nN\nX\ni=1\n\npqi\n\nlogq pi =\n\nN\nX\n\npi logq\n\ni=1\n\n1\npi\n\n!\n\n*\n\n= logq\n\n1\npi\n\n!+\n\n= hIi ilin ,\n\nlin\n\n(1.7)\nwith the last term resulting as the q-extension of 1.1. This reflects the nonextensive character of the system on the elementary information gains.\nNote also that the classical power laws and the additivity rules for the logarithm and exponential do no longer hold in this generalized context. Except\nfor q = 1, in general logq x\u03b1 6= \u03b1 logq x, which explains why we keep\nwriting\nD\n\u0010 \u0011E\nthroughout this paper Shannon's elementary information gain as log p1i\nlin\ninstead of \u2212 hlog pi ilin . Useful for our purposes will be the equality\neqx+y+(1\u2212q)xy = exq eyq .\n4\n\n(1.8)\n\n\fWe will see how the q-deformed formalism fits naturally in the mathematical\ndescriptions of generalized entropy measures.\n1.3 R\u00e9nyi's entropy\nEither in the case of BG as for Tsallis entropy, in 1.1 and 1.7, an entropy\nmeasure is the average\n\u0010 \u0011 S obtained over many elementary information gains\n1\nIi \u2261 Ii ( pi ) = logq p1i associated to the i-th event of probability pi (if the\nsystem is extensive, q=1).\nAnother possible generalization exists and has become commonplace throughout the literature, namely R\u00e9nyi's measure (12). A. R\u00e9nyi maintained a still\nadditive measure, as in BG entropy, but considered that another form of averaging is possible. His starting point was the generalized notion of average\nof A.N. Kolmogorov and M. Nagumo ((7), (10)), who independently showed\nthat, in the frame of the Kolmogorov axioms of probability theory, the definition of the average must be extended to the quasi-arithmetic or quasi-linear\nmean defined as\n!\nN\nX\n\nS = f \u22121\n\npi f (Ii ) ,\n\n(1.9)\n\ni=1\n\nwhere f is a strictly monotone continuous and invertible function, the so called\nKolmogorov-Nagumo function (KN function). On his side, R\u00e9nyi showed that\nif we restrict to additive measures then only two possible KN functions exist.\nThe first one is the common arithmetic mean and is associated with the KN\nfunction f (x) = x, and the second is the exponential mean with\nf (x) = c1 b(1\u2212q)x + c2 ,\n\n(1.10)\n\nwhere q is a real parameter, and c1 and c2 are two arbitrary constants.\nThe exponential mean leads to R\u00e9nyi's information measure or R\u00e9nyi's entropy\nN\nX\n1\nSR (P, q) =\nlogb\npqi ,\n(1.11)\n1\u2212q\ni=1\nwith b the logarithm base (we will from now on assume the natural base, b = e,\nfor R\u00e9nyi's entropy either). For q \u2192 1 R\u00e9nyi's measure becomes Shannon's\nentropy.\nIt should be noted how P. Jizba and T.Arimitsu (5) showed that R\u00e9nyi's\nmeasure can be obtained also extending the Shannon-Khinchin axioms to a\nquasi-linear conditional information\nSR (B|A) = f\n\n\u22121\n\nX\n\n!\n\n\u03c0i (A)f (SR (B|Ai )) ,\n\ni\n\n5\n\n(1.12)\n\n\fwith f as given in 1.10.\nTherefore Shannon's information measure is an averaged information in the\nordinary sense, while R\u00e9nyi's measure represents\nan exponential mean over\n\u0010 \u0011\n1\nthe same elementary information gains log pi .\n2\n\nThe Sharma-Mittal and Supra-extensive entropy\n\n2.1 Generalizing with Kolmogorov-Nagumo means\n\nIt is important to understand that Tsallis and R\u00e9nyi entropies are two different\ngeneralizations along two different paths. Tsallis generalized to non-extensive\nsystems, while R\u00e9nyi to quasi-linear means. But we can search for an entropy\nwhich generalizes to non-extensive sets and non-linear means containing Tsallis and R\u00e9nyi measures as limiting cases.\nLet us unify the picture of all the entropies considered here through KN averages (as J.Naudts and M.Czachor did (11), tough by a slightly different\napproach).\nIt is immediate to see from 1.7 and 1.9 how for Tsallis's measure it is the KN\nfunction\nf (x) = x\n(2.1)\nwhich averages over the elementary information gain\nIi = logq\n\n1\npi\n\n!\n\n.\n\n1\npi\n\n!+\n\nThis led us to write it as\n*\n\nST (P, q) = logq\n\nWhile, for R\u00e9nyi's measure, choose in 1.10, c1 =\nthen the KN function takes the form\nf (x) = logq ex ,\nwhich, applied on\n1\nIi = log\npi\n6\n\n!\n\n,\n\n.\n\nlin\n\n1\n1\u2212q\n\n= \u2212c2 (remember 1.4),\n(2.2)\n\n\fin 1.9 (f \u22121 (x) = log exq ) leads us to rewrite 1.11 as\n*\n\n!+\n\n1\nSR (P, q) = log\npi\n\n,\n\nexp\n\nwhere, of course, h . iexp \u2261 h Ii iexp stands for the exponential mean defined by\nthe KN function 2.2 over the elementary information Ii .\nBut, what Tsallis and R\u00e9nyi measures have in common is that in both cases\n!\n\n1\npi\n\nf (Ii) = logq\n\n(2.3)\n\n.\n\nThen, for a further generalization, the simplest step beyond them would be\nthat to generalize 2.1 and 2.2 with\nf (x) = logq exr\nand set\n\n(2.4)\n\n!\n\n1\nIi = logs\n,\npi\nwhere r, s are new parameters on the generalized exponential and logarithm\nfunctions. Maintaining constraint 2.3 implies s = r. Then calculating 1.9\n(f \u22121 (x) = logr exq ), one obtains the Sharma-Mittal information measure (13)\nP\n\nSSM (P, {q, r}) = logr eq\n*\n\n= logr\n\n1\npi\n\n\uf8ee\n\ni\n\n!+\n\n1 \uf8f0 X q\npi\n=\n1\u2212r\ni\n\npi logq\n\n\u0010 \u0011\n1\npi\n\n=\n\n(2.5)\n\n=\n\nq\u2212exp\n\n! 1\u2212r\n1\u2212q\n\n\uf8f9\n\n\u2212 1\uf8fb ,\n\nwhere h * iq\u2212exp stands for an average defined by the KN function 2.4 and that\nwe will call the quasi-exponential mean.\nWe can see that for r \u2192 1 R\u00e9nyi's measure, and for r \u2192 q Tsallis measure,\nare recovered as limiting cases.\nWe will show in the next section that for two statistical independent systems\nA and B it is easy to check that\nSSM (A \u2229 B) = SSM (A) + SSM (B|A) + (1 \u2212 r)SSM (A)SSM (B|A) ,\n7\n\n\fi.e. a pseudo-additive law holds as in the case of Tsallis entropy.\nTherefore Sharma-Mittal's measure generalizes R\u00e9nyi's extensive entropy to\nnon-extensivity, characterized by the r-logarithm. It is the parameter r which\ndetermines the degree of non-extensivity, while q is the deformation parameter of the probability distribution (however, when r \u2192 q the two parameters become intertwined and in Tsallis entropy it is q which measures nonextensivity).\nOn information theoretic grounds, B.D. Sharma and D.P. Mittal (13), advanced already in 1975 this non-additive measure which shows to have a nonextensive character either. But it wasn't until recently ((2), (3), and without\nmentioning it explicitly (11)) that Sharma-Mittal's measure has been investigated in statistical mechanics.\n\n2.2 Generalizing with q-logarithms and q-exponentials\n\nAt this point let us see how by using the q-deformed logarithm and exponential formalism, one could express in a much more compact form the same\ngeneralization path.\nFirst of all recall a well known relationship which exists between Tsallis and\nR\u00e9nyi entropies, namely\nSR (P, q) =\n\n1\nlog [1 + (1 \u2212 q) ST (P, q)] .\n1\u2212q\n\n(2.6)\n\nHere we can efficiently exploit the generalized logarithm and exponential functions 1.4 and 1.5, rewriting 2.6 in the more compact form\nSR (P, q) = log eqST (P, q) ,\n\n(2.7)\n\nfrom which follows immediately\nST (P, q) = logq eSR (P, q) .\n\n(2.8)\n\nLooking at the structure of 2.7 and 2.8 we can ask if, given another parameter\nr, the following\nSSM (P, {q, r}) = logr eSq T (P, q)\n\n8\n\n\uf8ee\n\n1 \uf8f0 X q\n=\npi\n1\u2212r\ni\n\n! 1\u2212r\n1\u2212q\n\n\uf8f9\n\n\u2212 1\uf8fb ,\n\n(2.9)\n\n\fand\nSSE (P, {q, r}) =\n\nlogq eSr R (P, q)\n\n=\n\nh\n\n(1\u2212r)\n(1\u2212q)\n\n1+\n\nlog\n\nP\n\nq\ni pi\n\n1\u2212q\n\ni 1\u2212q\n\n1\u2212r\n\n\u22121\n\n(2.10)\n\n,\n\nmight then be other possible generalizations? 2.9 can be recognized immediately as Sharma-Mittal's measure 2.5 and can be already accepted as an\nextension.\n2.10 instead needs a closer look. For r \u2192 q it obviously boils down to R\u00e9nyi's\nentropy. For r \u2192 1 we obtain Tsallis' measure again. So, from a formal point\nof view it can be regarded as another generalization too. It is however not\nentirely clear what kind of statistics it expresses. Its particular status might be\nbest evidenced expressing all the measures in terms of (logarithmic averaged)\nsurprise quantities.\nIndeed, notice that we can rewrite the quantity\nX\ni\n\npqi\n\n!\n\n1\n1\u2212q\n\n=\n\n\uf8eb\n\uf8ed\n\nX\ni\n\npi\n\n1\n*\n!1\u2212q \uf8f6 1\u2212q\n1\n\uf8f8 =\n\n1\npi\n\npi\n\n1\n!1\u2212q + 1\u2212q\n\nD\n\n= eq\n\nlin\n\nlogq\n\n\u0010 \u0011E\n1\npi\n\nlin\n\n=\n\n*\n\n1\npi\n\n+\n\n,\n\nlogq\n\n(2.11)\nwhere we used the logarithmic mean h* ilogq defined by the KN function f (x) =\nlogq x. Then, from 1.7 and 1.11, and using 2.11, equations 2.7 to 2.10 can be\nrewritten as\n* +\n1\nSR (P, q) = log\n;\n(2.12)\npi log\nq\n\nST (P, q) = logq\n\n*\n\n1\npi\n\nSSM (P, {q, r}) = logr\n\n+\n\n*\n\n1\npi\n\nlog\n\nSSE (P, {q, r}) = logq er\n\n;\n\n(2.13)\n\nlogq\n\n+\n\n;\n\n(2.14)\n\nlogq\n\nD E\n1\npi\n\nlogq\n\n.\n\n(2.15)\n\nWith the q-deformed logarithm and exponential formalism we could easily\nsee the generalization path to follow and write all the measures into a more\ncompact form (2.7 to 2.10). Moreover this makes it easier to recognize the\nbehavior of the limits than in their explicit form (the r.h.s. of 2.9 and 2.10).\nWith no or only few passages it is immediate to see how 2.9 reduces to Tsallis\nentropy for r \u2192 q, and for r \u2192 1 it reduces to R\u00e9nyi's entropy (without any\n9\n\n\fneed to apply Hopital rule, first order approximations or whatever, insert 1.2\nin 1.5).\nThe limit for q \u2192 1 for Sharma-Mittal measure is\nlim SSM =\n\nq\u21921\n\nlim logr eSq T\nq\u21921\n\nSS\n\n= logr e\n\n=\n\n= logr\n\ne\u2212(1\u2212r)\n\nP\n\ni\n\n*\n\n1\npi\n\npi log pi\n\n+\n\n=\nlog\n\n\u22121\n\n,\n1\u2212r\nwhich Frank and Daffertshofer (2) used to call the gaussian entropy.\nBy the way\nlog\n\nlim SSE =\n\nq\u21921\n\nlim logq eSr R\nq\u21921\n\n=\n\nlog eSr S\n\n= log er\n\nX\n1\n=\nlog 1 \u2212 (1 \u2212 r)\npi log pi\n1\u2212r\ni\n\nD E\n\n!\n\n1\npi\nlog\n\n=\n\n.\n\nBut the point is that when compared with 2.12, 2.13, 2.14, measure 2.15 seems\nto stand apart and does not correspond to some quasi-linear mean in the style\nof 1.9.\n\n2.3 Comparing the supra-extensive entropy with Sharma-Mittal's entropy\n\nLet us then focus shortly on the separate nature of 2.15 (or 2.10) and some of\nits properties.\nFirst of all note that it can be shown how for two statistical independent\nsystems A and B, similarly to Tsallis' entropy, the Sharma-Mittal's entropy\nobeys a pseudo-additive law and can be decomposed as in 1.3. It is almost\nimmediate to see this by employing the generalized exponential formalism.\nThanks to 1.3, 1.6, 1.8, starting from the middle term of 2.9 we can write\nSSM (A \u2229 B) = logr eqST (A\u2229B) =\n\u0010\n\n\u0011\n\n= logr eq [ST (A) + ST (B|A) + (1 \u2212 q) ST (A)ST (B|A)] = logr eSq T (A) eqST (B|A) =\n= logr eSq T (A) + logr eqST (B|A) + (1 \u2212 r) logr eSq T (A) logr eqST (B|A) =\n= SSM (A) + SSM (B|A) + (1 \u2212 r)SSM (A)SSM (B|A) .\n10\n\n(2.16)\n\n\fProceeding in the same manner with 2.10 leads however not to the same\ndecomposition. Because of R\u00e9nyi's measure additive character one can't go\nfurther than\nSSE (A \u2229 B) = logq eSr R (A\u2229B) = logq er [SR (A) + SR (B|A)]\nwith SR (B|A) as given in 1.12.\nEntropy 2.15 therefore obeys a new form of non-extensivity, we call supraextensivity.\nThere are also other aspects which should be mentioned. Let us briefly recall\nthe notions of concavity and stability applied to entropy measures.\nGiven two probability distributions P = {p1 , ..., pN } and P \u2032 = {p\u20321 , ..., p\u2032N } and\ndefining an intermediate distribution P \u2032\u2032 = {p\u2032\u20321 , ..., p\u2032\u2032N } with\np\u2032\u2032i \u2261 \u03bcpi + (1 \u2212 \u03bc) p\u2032i ;\n\n\u2200\u03bc \u2208 [0, 1] ,\n\nS(P ) is said to be a concave entropic functional if and only if\nS(P \u2032\u2032) \u2265 \u03bcS(P ) + (1 \u2212 \u03bc)S(P \u2032) .\nOtherwise, S(P ) is said to be convex. Concavity implies thermodynamic stability (e.g. thermal equilibrium between two initial temperatures in BG statistical\nmechanics).\nRecall also the notion of stability (or experimental robustness, as Tsallis calls\nit, in order to avoid confusion with the previous form of thermodynamic stability) which implies that for arbitrary small variations of the probabilities pi\na statistical functional remains finite. That is, given a deformation\n||p \u2212 p\u2032 || =\n\nX\n\n|pi \u2212 p\u2032i | ,\n\ni\n\nsuch that ||p \u2212 p\u2032 || < \u03b4\u03b5 , we obtain stability of S(P ) if\n\u25b3=\n\nS(P ) \u2212 S(P \u2032)\n< \u03b5;\nSmax\n\n\u2200\u03b4\u03b5 > 0, \u2200\u01eb > 0 ,\n\nwith Smax the maximum value S can attain and for all microstates i = 1, ..., N.\nLesche claims (8) that this is a necessary condition for an entropy measure to\nbe a physical quantity and showed that, while BG entropy is always stable,\nR\u00e9nyi's measure is unstable for all q 6= 1. It is also known that BG entropy\nis always concave, while R\u00e9nyis measure is concave only for q \u2264 1, and can\nbe either concave or convex for q > 1. More recently, Abe (1) showed that\nTsallis entropy is concave and stable for all positive values of q. It might\n11\n\n\falso be worth mentioning that a physical entropy is not only expected to be\ngenerically concave and Lesche-stable but should lead also to a finite entropy\nproduction per unit time. BG and Tsallis entropies share all these properties.\nR\u00e9nyi entropy shares none. (4)\nSo, being an extension of it, it is clear that the properties of concavity and\nstability and finite entropy production per unit time are generically violated\nalso in Sharma Mittal's and the supra-extensive entropy.\nFinally, it should also be underlined how Frank and Plastino showed (3) that\nthe Sharma-Mittal entropy is the only measure that allows for a pseudoadditive decomposition and at the same time gives rise to a thermostatistics\nbased on escort mean energy values\nP\n\npiq \u03b5i\nU= P q ,\ni pi\ni\n\n(\u03b5i are the energy levels) admitting of a generalized partition function Ze defined by logr ZeSM := logr ZSM \u2212 \u03b2U with\nZSM =\n\nX\n\npiq\n\ni\n\n!\n\n1\n1\u2212q\n\n=\n\n*\n\n1\npi\n\n+\n\n(2.17)\n\n,\n\nlogq\n\n(ZSM is the partition function which takes U while ZeSM takes zero as the\nenergy reference, and where \u03b2 is an inverse temperature measure), that leads\nto the usual expressions for the free energy\n\nand the mean energy\n\n1\nF = U \u2212 T SSM = \u2212 logr ZeSM\n\u03b2\nU =\u2212\n\n\u2202\nlogr ZeSM .\n\u2202\u03b2\n\nWe saw that the new measure we considered here does not allow for a pseudoadditive decomposition like 2.16, and therefore it is to expect that the partition\nfunction describing the free and mean energy cannot have the same structure\nZeSM common to Sharma-Mittal entropy unless, as can be shown (9) applying\nD E\nlog\n\nthe maximum entropy principle, one substitutes 2.17 with ZSE = er\n\n1\npi\n\nlogq\n\n.\n\nSumming up, the supra-extensive entropy, does no longer obey a pseudoadditive statistics, if based on escort mean values the partition function must\ntake an intrinsically different form than Sharma-Mittals one, but what they\nhave in common with R\u00e9ny's entropy is that, in general, they do not possess\nthe property of concavity, Lesche-stability and finite entropy production per\nunit time.\n12\n\n\f3\n\nConclusion\n\nWe showed how the Sharma-Mittal and a new generalized entropy measure\nboth unify Tsallis and R\u00e9nyi entropies on two different paths in a way that\nappears natural and almost immediate when we make use of the generalized\nq-logarithm an q-exponentials as in 2.7, 2.8, 2.9 and 2.10. We underlined\nhow the relationship among all measures becomes particularly clear using\nthe logarithmic KN average 2.11 rewriting them as in 2.12, 2.13, 2.14 and\n2.15. This path naturally leads to the supra-extensive entropy which does not\nemerge from the KN means approach alone and does not conform to a pseudoadditive law, lacks of concavity, Lesche-stability and finite entropy production.\nHowever, because the new measure here proposed emerges so naturally as\nanother possible extension of R\u00e9nyi and Tsallis entropy it is therefore worth\nof being mentioned. It is tempting to conclude that, while it might not have\napplications in a generalized thermostatistics, it nevertheless might be of some\ninterest in the frame of information theory, cybernetics, control theory, etc.\nFinally we obtained a way of understanding all these entropy measures in a\nunified picture that can be summarized in the following table and diagram.\n\n13\n\n\fEntropy measure\n\nKN-mean form\n\nExplicit form\n(1\u2212r)\n[1+ (1\u2212q)\n\nSupra-extensive\n\n1\n1\u2212r\n\nSharma-Mittal\n\n\u0014\n\nTsallis\n\nBG-Shannon\n\n\u2212\n\nP\n\n1\u2212q\n\ni\n\npqi ] 1\u2212r \u22121\n\n1\u2212q\n\n(\n\nP\n\ni\n\n1\u2212r\n\npiq ) 1\u2212q \u2212 1\n\nPN\n\n1\n1\u2212q\n\nR\u00e9nyi\n\nlog\n\nD\n\npqi \u22121\n1\u2212q\n\n\u0015\n\nlogq er\nD\n\nlogr\n\ni=1\n\nlog\n\nPN\n\ni=1\n\nlog\n\nD\n\nq\ni=1 pi\n\n1\npi\n\nlog\n\nD\n\npi log pi\n\n1\npi\n\nlog\n\nexp\n\nlogq er\n\n\u0010 \u0011E\n\nlogq\n\nD\n\nPN\n\nKNlog -mean form\n\n\u0010 \u0011E\n\nlog\n\nlogr\n\nq\u2212exp\n\n\u0010 \u0011E\n1\npi\n\n\u0010 \u0011E\n1\npi\n\nlogq\n\nlin\n\nlog\n\nexp\n\n\u0010 \u0011E\n1\npi\n\nlog\n\nlin\n\nlogr\n\n1\npi\nlogq\n\nlogq \u00d7 expq -form\nlogq erSR (P, q)\n\nD E\n\n1\npi log\nq\n\nlogr eqST (P, q)\n\nD E\n\n1\npi log\nq\n\nlogq eSR (P, q)\n\n1\npi log\nq\n\nlog eqST (P, q)\n\nD E\n\n1\npi log\n\nlog e SS (P )\n\nD E\n\nSupra-extensive\n\nSharma-Mittal\n\u001c\n\nD E\n\nlog\n\n\u001d\n\n1\npi log\nq\n@\n@\n@\n\nHH\nH\n\nr\u2192q\n\nr\u21921\n@\n@\nR\n@\n\nr\u2192q\n\nH\n\b\nHH \b\b\n\b\nH\n\b H\nHH\n\b\b\n\b\nHH\n\b\n\b\nHH\n\b\n\u0019\n\b\nj\nH\n\nlog\n\nr\u21921\n\nTsallis\n\nR\u00e9nyi\n\u001c\n\nlogq er\n\n\b\n\b\b\n\n\u001d\n\n1\npi log\nq\n\nlogq\n\n\u001c\n\n\u001d\n1\npi log\nq\n\n@\n@\n@\n\nq\u21921\n\nq\u21921\n\n@\n@\nR\n@\n\nShannon (Boltzmann-Gibbs)\nlog\n\n\u001c\n\n\u001d\n1\npi log\n\n14\n\nD E\n\n1\npi\nlogq\n\n\fReferences\n[1] S. Abe, Stability of Tsallis entropy and instabilities of R\u00e9nyi and normalized Tsallis entropies: A basis for q-exponential distributions. Phys. Rev.\nE66, 046134 (2002).\n[2] T.D. Frank, A. Daffertshofer, Exact time-dependant solutions of the R\u00e9nyi\nFokker-Planck equation and the Fokker-Planck equations related to the entropies proposed by Sharma and Mittal, Physica A 285, 351 (2000).\n[3] T.D. Frank, A.R. Plastino, Generalized thermostatistics based on the\nSharma-Mittal entropy and escort mean values, Eur. Phys. J. B 30, 543549 (2002).\n[4] See, for instance, the Preface of Nonextensive Entropy - Interdisciplinary\nApplications. M. Gell-Mann and C. Tsallis, Oxford University Press, New\nYork (2004).\n[5] P. Jizba, T. Arimitsu, The world according to R\u00e9nyi: thermodynamics of\nmultifractal systems, Annals of Physics, Volume 312, Issue 1, 17-59 (July\n2004).\n[6] A.I. Khinchin, Mathematical Foundations of Information Theory, Dover,\nNew York (1957).\n[7] A.N. Kolmogorov Sur la notion de la moyenne, Atti Accad. Naz. Lincei\nMem. Cl. Sci. Fis. Mat. Natur. Sez. (6) 12, 388-391 (1930).\n[8] B. Lesche, J. Stat. Phys., 27, 419 (1982).\n[9] Author's paper in preparation.\n[10] M. Nagumo, \u00dcber eine Klasse der Mittelwerte, Japan. J. Math., 7, 71-79\n(1930) .\n[11] J. Naudts, M. Czachor Generalized thermostatistics and KolmogorovNagumo averages, arXiv:cond-mat/0110077 (3 Oct 2001).\n[12] A. R\u00e9nyi, Probability theory, North Holland, Amsterdam (1970); Selected\nPapers of Alfred R\u00e9nyi, Vol.2 Akademia Kiado, Budapest (1976).\n[13] B.D. Sharma and D.P. Mittal, J. Math. Sci. 10, 28 (1975). See also New\nNonadditive Measures of Relative Information, J. Comb. Inform. and Syst.\nSci., 2, 122-133 (1977).\n[14] C. Tsallis, Possible Generalization of Boltzmann-Gibbs Statistics, J. Stat.\nPhys. 52, 479 (1988).\n[15] For a general bibliography about nonextensive thermodynamics and updates see: http://tsallis.cat.cbpf.br/biblio.htm\n\n15\n\n\f"}