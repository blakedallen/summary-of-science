{"id": "http://arxiv.org/abs/0907.4249v2", "guidislink": true, "updated": "2009-09-22T19:39:10Z", "updated_parsed": [2009, 9, 22, 19, 39, 10, 1, 265, 0], "published": "2009-07-24T12:39:22Z", "published_parsed": [2009, 7, 24, 12, 39, 22, 4, 205, 0], "title": "Complanart of polynomial equations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0907.1623%2C0907.4547%2C0907.0958%2C0907.3441%2C0907.4075%2C0907.0578%2C0907.2961%2C0907.1610%2C0907.4901%2C0907.3521%2C0907.5416%2C0907.4247%2C0907.1457%2C0907.2176%2C0907.3022%2C0907.3113%2C0907.2686%2C0907.2431%2C0907.2415%2C0907.3600%2C0907.4246%2C0907.3040%2C0907.0848%2C0907.3175%2C0907.5335%2C0907.3370%2C0907.0698%2C0907.0937%2C0907.0239%2C0907.3181%2C0907.3101%2C0907.2564%2C0907.1883%2C0907.1483%2C0907.5432%2C0907.2720%2C0907.3456%2C0907.3657%2C0907.4007%2C0907.1451%2C0907.4167%2C0907.2820%2C0907.5182%2C0907.3882%2C0907.0312%2C0907.3760%2C0907.4010%2C0907.3815%2C0907.4803%2C0907.5597%2C0907.1754%2C0907.0649%2C0907.0722%2C0907.3614%2C0907.3924%2C0907.4221%2C0907.0065%2C0907.3391%2C0907.1621%2C0907.2323%2C0907.0734%2C0907.0335%2C0907.1908%2C0907.2156%2C0907.1758%2C0907.0378%2C0907.0972%2C0907.1951%2C0907.4078%2C0907.4480%2C0907.1113%2C0907.4407%2C0907.1241%2C0907.3892%2C0907.4249%2C0907.3985%2C0907.0830%2C0907.4988%2C0907.4499%2C0907.4187%2C0907.4158%2C0907.4064%2C0907.3188%2C0907.0008%2C0907.2641%2C0907.3926%2C0907.0501%2C0907.5328%2C0907.2242%2C0907.3260%2C0907.5021%2C0907.4304%2C0907.5511%2C0907.2950%2C0907.5344%2C0907.1853%2C0907.4853%2C0907.2616%2C0907.2363%2C0907.3217%2C0907.1435&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Complanart of polynomial equations"}, "summary": "In this paper we study polynomial maps of vector spaces and their\neigenvectors and eigenvalues. The new quantity called complanart is defined.\nComplanarts determine complanarity of solution vectors of systems of polynomial\nequations. Evaluation of complanart is reduced to evaluation of resultants. As\nin linear case, the pattern of eigenvectors defines the phase diagram of\nassociated differential equation. Theory of such differential equations arise\nnaturally as extension of Lyapunov's theory of stability for solutions of\ndifferential equations. The results of this work have a number of potential\napplications: from solving non-linear differential equations and calculating\nnon-linear exponents to taking non-Gaussian integrals.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0907.1623%2C0907.4547%2C0907.0958%2C0907.3441%2C0907.4075%2C0907.0578%2C0907.2961%2C0907.1610%2C0907.4901%2C0907.3521%2C0907.5416%2C0907.4247%2C0907.1457%2C0907.2176%2C0907.3022%2C0907.3113%2C0907.2686%2C0907.2431%2C0907.2415%2C0907.3600%2C0907.4246%2C0907.3040%2C0907.0848%2C0907.3175%2C0907.5335%2C0907.3370%2C0907.0698%2C0907.0937%2C0907.0239%2C0907.3181%2C0907.3101%2C0907.2564%2C0907.1883%2C0907.1483%2C0907.5432%2C0907.2720%2C0907.3456%2C0907.3657%2C0907.4007%2C0907.1451%2C0907.4167%2C0907.2820%2C0907.5182%2C0907.3882%2C0907.0312%2C0907.3760%2C0907.4010%2C0907.3815%2C0907.4803%2C0907.5597%2C0907.1754%2C0907.0649%2C0907.0722%2C0907.3614%2C0907.3924%2C0907.4221%2C0907.0065%2C0907.3391%2C0907.1621%2C0907.2323%2C0907.0734%2C0907.0335%2C0907.1908%2C0907.2156%2C0907.1758%2C0907.0378%2C0907.0972%2C0907.1951%2C0907.4078%2C0907.4480%2C0907.1113%2C0907.4407%2C0907.1241%2C0907.3892%2C0907.4249%2C0907.3985%2C0907.0830%2C0907.4988%2C0907.4499%2C0907.4187%2C0907.4158%2C0907.4064%2C0907.3188%2C0907.0008%2C0907.2641%2C0907.3926%2C0907.0501%2C0907.5328%2C0907.2242%2C0907.3260%2C0907.5021%2C0907.4304%2C0907.5511%2C0907.2950%2C0907.5344%2C0907.1853%2C0907.4853%2C0907.2616%2C0907.2363%2C0907.3217%2C0907.1435&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper we study polynomial maps of vector spaces and their\neigenvectors and eigenvalues. The new quantity called complanart is defined.\nComplanarts determine complanarity of solution vectors of systems of polynomial\nequations. Evaluation of complanart is reduced to evaluation of resultants. As\nin linear case, the pattern of eigenvectors defines the phase diagram of\nassociated differential equation. Theory of such differential equations arise\nnaturally as extension of Lyapunov's theory of stability for solutions of\ndifferential equations. The results of this work have a number of potential\napplications: from solving non-linear differential equations and calculating\nnon-linear exponents to taking non-Gaussian integrals."}, "authors": ["Andrey Vlasov"], "author_detail": {"name": "Andrey Vlasov"}, "author": "Andrey Vlasov", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s11232-010-0034-2", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0907.4249v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0907.4249v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "30 pages, 5 figures, one more particular case added", "arxiv_primary_category": {"term": "math-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "hep-th", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.MP", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0907.4249v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0907.4249v2", "journal_reference": "Theor.Math.Phys.163:438-465,2010", "doi": "10.1007/s11232-010-0034-2", "fulltext": "ITEP/TH-23/09\n\nComplanart of system of polynomial equations\nAndrey Vlasov 1\n\narXiv:0907.4249v2 [math-ph] 22 Sep 2009\n\nITEP, Moscow, Russia\n\nABSTRACT\nIn this paper we study polynomial maps of vector spaces zi \u2192 Aii1 i2 ***is zi1 zi2 zi3 * * * zis and their\neigenvectors and eigenvalues. The new quantity called complanart is de\u001cned. Complanarts determine\ncomplanarity of solution vectors of systems of polynomial equations. Evaluation of complanart is\nreduced to evaluation of resultants. As in linear case, the pattern of eigenvectors de\u001cnes the phase\ndiagram of associated di\u001berential equation \u017ci = Aii1 i2 ***is zi1 zi2 zi3 * * * zis . Theory of such di\u001berential\nequations arise naturally as extension of Lyapunov's theory of stability for solutions of di\u001berential\nequations. The results of this work have a number of potential applications: from solving non-linear\ndi\u001berential equations and calculating non-linear exponents to taking non-Gaussian integrals.\n\nContents\n1 Introduction\n\n1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2 Symmetric combinations of the roots . . . . . . . . . .\n1.2.1 Complanart . . . . . . . . . . . . . . . . . . .\n1.3 Eigenvectors and eigenvalues . . . . . . . . . . . . . .\n1.4 Applications of our approach . . . . . . . . . . . . . .\n1.4.1 Nonlinear di\u001berential equations . . . . . . . . .\n1.4.2 Non-Gaussian integrals . . . . . . . . . . . . .\n1.5 Terms and notations . . . . . . . . . . . . . . . . . . .\n1.5.1 Homogeneous and non-homogeneous equations\n1.5.2 Maps and resultants . . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n2.1 Resultant and generalization of Vieta formulas . . . . . . . . . . . . .\n2.2 Symmetric combinations . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3 Complanart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3.1 Evaluation of complanarts . . . . . . . . . . . . . . . . . . . . .\n2.3.2 When complanart is equal to 1? . . . . . . . . . . . . . . . . . .\n2.3.3 Open questions . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3.4 Examples of complanarts: n = 2, complanarts are discriminants\n2.3.5 Examples of complanarts: n = 3 . . . . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n\n2 Complanart and symmetric combinations of the roots\n\n3 Eigenvectors, eigenvalues and GLn canonical representation of non-linear maps\n3.1\n3.2\n3.3\n3.4\n3.5\n\nNon-linear eigenvectors, eigenvalues and how they can be found\nZero, nonzero and unitary eigenvectors . . . . . . . . . . . . .\nDecomposability of characteristic polynomial . . . . . . . . . .\nNumber of eigenvectors . . . . . . . . . . . . . . . . . . . . . .\nGLn canonical representation of non-linear map . . . . . . . .\n3.5.1 Example of canonical representation . . . . . . . . . . .\n3.6 Application of complanart to the theory of eigenvectors . . . .\n\n1\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n\n2\n\n2\n2\n3\n3\n4\n4\n5\n6\n6\n6\n\n6\n\n6\n7\n9\n10\n11\n11\n11\n12\n\n13\n\n13\n13\n14\n14\n15\n16\n16\n\nAlso at Moscow Institute for Physics and Technology and Institute of Astronomy of Russian Academy of Science\ne-mail: vlasov.ad@gmail.com\n\n1\n\n\f3.6.1 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.7 How to exclude \u03bb by one more method . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.7.1 Unit maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n4 Polynomial di\u001berential equations\n\n4.1 Eigenvectors as stationary points . . . . . . . . . . . . .\n4.1.1 n = 2: the equations are reduced to quadratures\n4.2 The \"eigenvector\" solution . . . . . . . . . . . . . . . .\n4.3 Non-linear condition of instability . . . . . . . . . . . .\n4.4 The example of equations with vanishing linear term . .\n4.4.1 Investigation of stability of this example . . . . .\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n5.1 General analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.1.1 Preliminary information about eigenvectors from complanart . . . . . . . .\n5.1.2 Preliminary information about eigenvectors from resultant . . . . . . . . .\n5.1.3 Preliminary information about eigenvectors from the equation xi Aj \u2212 xj Ai\n5.2 Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3 Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.4 Characteristic polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5 Phase diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5.1 a 6= 1/2, b 6= 1/2, 1 \u2212 4ab 6= 0 . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5.2 a 6= 1/2, b 6= 1/2, 1 \u2212 4ab = 0 . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5.3 a 6= 1/2, b = 1/2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5.4 a = 1/2, b = 1/2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5.5 Phase diagram without unitary eigenvectors . . . . . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n5 Example of \u001cnding degeneracies and peculiarities of non-linear map\n\n6 Acknowledgements\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n\n17\n17\n18\n\n18\n\n18\n19\n20\n20\n21\n22\n\n22\n22\n23\n23\n24\n24\n25\n25\n26\n26\n26\n26\n27\n27\n\n27\n\n1 Introduction\n1.1\n\nOverview\n\nThis paper is about non-linear algebra [1, 2], which studies non-linear maps zi \u2192 Aii1 i2 ***is zi1 zi2 zi3 * * * zis\nand systems of polynomial equations. Non-linear algebra is a direct generalization of linear algebra\n[3]. Developing non-linear algebra can help us to take non-Gaussian integrals, and can convert many\ncalculations in physics in exact ones. It can also be applied to theory of stability of di\u001berential\nequations. Instead of determinants, the main functions/objects of quantitative non-linear algebra are\ndiscriminants and resultants [1, 2]. Resultants determine solvability of system of polynomial equations,\nand discriminant determines the degeneracy of non-linear form. In this paper we consider another, new\nmeasure of degeneracy of system of polynomial equations, called complanart. Complanart determines,\nwhether there are n complanar roots of system, for details see sect 1.2.1, and sect 2.3. In sect 3 the\nproblem of \u001cnding non-linear eigenvectors/eigenvalues is discussed. In sect 4 the applications of all\nderived methods to the theory of polynomial di\u001berential equations are presented. The equations of\nsuch type arise in some degenerate cases in the theory of stability. In sect 5 all derived techniques and\nmethods are illustrated on one example of quadratic map of two variables.\n\n1.2\n\nSymmetric combinations of the roots\n\nIt is well known (see, for example, [4]), that all polynomial symmetric combinations of the roots of\nsystem of n \u2212 1 homogeneous equations of n variables in principle can be expressed polynomially in\nthe coe\u001ecients of these equations. But explicit expressions for the roots themselves, with separate\nexpression for each root, exist only in simple cases. \"Explicit\" means the expression, involving only\n2\n\n\farithmetic roots and algebraic operations. For example, for one equation of two homogeneous variables\nsuch expressions exist only for degrees from 1 to 4. So, it is sometimes easier and more convenient to\nstudy the system without \u001cnding all roots, but by studying symmetrical combinations of the roots.\nFor example, discriminant of a form determines whether the form is degenerate or not, resultant\n(see sect.1.5 and [2, 1]) of a system of polynomial equations determines whether the system has nontrivial solution. One more example of successful application of this approach to higher discriminants of\npolynomials described in [5]. Known di\u001eculties of this way of analysis is that there has been developed\nno clear methods of obtaining expression for arbitrary symmetric combinations of the roots through\ncoe\u001ecients of the system. In this paper new symmetric combination of the roots, called complanart,\nis considered. The evaluation of this quantity is reduced to evaluation of resultants. Calculation\nof elementary symmetric polynomials of roots, namely the generalization of Vieta formulas, is also\nreduced to evaluation of resultants, see 2.1.\n\n1.2.1 Complanart\nComplanart is a symmetric combination of the roots of a system of polynomial equations. Let\nf1 (x), * * * , fn\u22121 (x) be n \u2212 1 homogeneous polynomials of arbitrary degrees ri of n variables x1 , * * * , xn ,\nand let \u039b(1) , * * * , \u039b(N ) are roots of the system:\nf1 (x) = 0\n\n..\n.\n\nfn\u22121 (x) = 0\n\nThis system has in general case N = r1 r2 . . . rn\u22121 roots with at least one non-zero component up to\noverall rescaling, see, for example [4]. Double roots are counted and repeated two times, roots of third\norder - three times etc. Complanart equals:\nC=\n\nN\nY\n\n\u0010\n\n(i )\n\n(i )\n\n\u0011\n(i ) 2\n\n\u03b5j1 j2 ***jn \u039bj11 \u039bj22 * * * \u039bjnn\n\n(1)\n\ni1 ,*** ,in =1\ni1 <i2 <***<in\n\nComplanart equals\n\n0 i\u001b there is a set of n complanar roots, i. e. there is a set of roots\n\u039b(i1 ) , \u039b(i2 ) , * * * , \u039b(in ) without pair of equal indices i1 6= i2 6= i3 6= * * * =\n6 in satisfying\n(i1 ) (i2 )\n(in )\nj\nj\n***j\nn\n1\n2\n\u03b5\n\u039bj1 \u039bj2 * * * \u039bjn = 0. In particular, if there is at least one multiple root, complanart also\nequals zero. In the case n = 2 complanarity of vectors means their collinearity, so complanart reduces\nto ordinary discriminant of polynomial. If the number of distinct roots is less than n, complanart equals\n\n1. For example, complanart equals 1 if all equations are linear equations. The method of evaluating\ncomplanart is described in sect.2.2. More details about complanarts can be found in sect 2.3.\n\n1.3\n\nEigenvectors and eigenvalues\n\nEigenvector and eigenvalue of linear maps are known from linear algebra, and have direct analogues\nin non-linear algebra. Non-zero vector zi is called eigenvector of A, if it satis\u001ces (with some \u03bb):\nAii1 i2 ***is zi1 zi2 zi3 * * * zis = \u03bb(z)zi\n\u03bb, a polynomial of degree s \u2212 1, is called eigenvalue. In non-linear case one can at \u001crst \u001cnd eigenvectors\n\nand then \u001cnd eigenvalues by solving linear equations, see 3.2. Since to \u001cnd eigenvalues one have to\nsolve linear on \u03bb equations, the set of eigenvalues is a union of planes in the space of all polynomials of\ndegree s \u2212 1. This statement can be reformulated using characteristic polynomial of the map, namely:\nChA (\u03bb) \u2261 R{Ai (z) \u2212 \u03bb(z)zi }\n\n3\n\n\fChA (\u03bb) = 0 i\u001b \u03bb is an eigenvalue of A. Characteristic polynomial possesses decomposition on linear\nin the coe\u001ecients of \u03bb factors, since the set of eigenvalues is a union of planes in the space of all\npolynomials of degree s \u2212 1. This decomposability was \u001crstly stated in [2], but without a proof. The\nproof will be given in 3.3\nFinding eigenvectors is reduced to solving the system of n homogeneous equations of n + 1 variables.\nSuch systems possess complanart. Complanart can be used to determine whether the system has\ncomplanar eigenvectors. Here \"complanar\" means complanarity of vectors in extended space, i. e. in\nspace with additional homogenizing variable, see 3.6.\nn \u22121\nThe number of eigenvectors of non-degenerate map equals cn|s = ss\u22121\n, if there is no degenerations\nsuch as coinciding eigenvectors or the case when the map is unit map. The formula for cn|s was stated\nin [2], but from considerations for diagonal maps. In sect 3.4 this formula is derived in general case.\n\n1.4\n\nApplications of our approach\n\n1.4.1 Nonlinear di\u001berential equations\nThe eigenvectors of a map Aii1 i2 ***is entirely determine the phase diagram of the system of di\u001berential\nequations:\n(2)\n\n\u017ci = Aii1 i2 ***is zi1 zi2 zi3 * * * zis\n\nIf initial condition is proportional to an eigenvector, the solution is very simple, see sect 4.2.\n\nApplication in the theory of stability The equations of type (2) arise naturally when considering\n\nthe stability of the stationary point of system of di\u001berential equations. Consider a system of ordinary\ndi\u001berential equations:\n\uf8f1\ny \u03071 = f1 (y1 , y2 , * * * , yn )\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 y \u03072 = f2 (y1 , y2 , * * * , yn )\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\n..\n.\n\ny \u0307n = fn (y1 , y2 , * * * , yn )\n\nLet y1(0) , * * * , yn(0) be a stationary point of this system: f1 (y1(0) , * * * , yn(0) ) = 0, * * * , fn (y1(0) , * * * , yn(0) ) = 0.\nDenote now yi0 = yi \u2212 yi(0) . To analyse stability of this point and some quantitative properties of this\nstability/instability one can expand f1 , * * * , fn around the stationary point:\n\u1e8fi0\n\n\u0012\n=\n\u0010\n\n\u2202fi\n\u2202yj\n\n\u0013\n\nyj0 +\n0,0,*** ,0\n\n1 \u2202 2 fi 0 0\ny y + ***\n2! \u2202yj \u2202yk j k\n\n\u0011\n\n(3)\n\n\u2202fi\nfi\nThe \u001crst term in expansion \u2202y\nyj0 is linear map, the second 2!1 \u2202y\u2202j \u2202y\ny 0 y 0 is homogeneous\nj\nk j k\n0,0,*** ,0\nquadratic map, and so on. The case when there is a non-degenerate linear term in this expansion\nis well-known, this is a subject of consideration of \u0010Lyapunov\ntheory of stability (see [6]) with its\n\u0011\n\u2202fi\nLyapunov's indices equal to eigenvalues of the matrix \u2202yj\n. But for some di\u001berential equations\n2\n\n0,0,*** ,0\n\n\u0010\n\n\u0011\n\nlinear term in this expansion vanishes,\n= 0. In such cases it is necessary to consider the\n0,0,*** ,0\nterms in expansion of higher degrees. If only the term of the lowest degree is considered, one arrives\nto the system of type (2). The physically motivated example of system of di\u001berential equations with\nvanishing linear term will be given in sect.4.4. The discussion of stability/instability of points with\nvanishing linear term is given in sect.4.3. For example, if the resultant of the main non-linear term\ndoes not equal to zero, the point is unstable in its complex vicinity. If main non-linear term has a real\neigenvector, this point is unstable in its real vicinity.\n\u2202fi\n\u2202yj\n\n4\n\n\f1.4.2 Non-Gaussian integrals\nThe formula for Gaussian integral and obtained from it Wick theorem are widely used in modern science. In many cases when it is necessary to calculate something Gaussian integrals are used. Gaussian\nintegral is the integral:\n+\u221e\nr\n\u22121\ni j\nZ\n\u03c0 n \u2212 (A )ij J J\n\u2212Aij xi xj +J i xi\n4\nZ(J) \u2261\ne\ndx1 * * * dxn =\ne\ndetA\n\n(4)\n\n\u2212\u221e\n\nFor example, Feynman diagram technique uses them. In it this formula is extended from ordinary to\nfunctional integrating. One is really interested in calculating such quantities:\nR\n< \u03c6(x1 )\u03c6(x2 ) * * * \u03c6(xk ) >=\n\ni\n\n4x\n\nR\n\nD\u03c6(\u03c6(x1 )\u03c6(x2 ) * * * \u03c6(xk ))e ~ (L(\u03c6)+J\u03c6)d\nR\nR\ni\n4\nD\u03c6e ~ (L(\u03c6)+J\u03c6)d x\n\nJ=0\n\n(5)\n\nJ=0\n\nwhich are called corellators. \u03c6 is a \u001celd (or \u001celds), L(\u03c6) is a Lagrangian of this \u001celd(s) and J(x)\u03c6(x) is\ncalled source of the \u001celd. If L(\u03c6) contains only terms, quadratic on \u03c6, for example L(\u03c6) = \u2202i \u03c6\u2202 i \u03c6\u2212m2 \u03c62\n- a lagrangian for free scalar massive \u001celd, these quantities are calculated as follows:\nZ\nZ(J) \u2261\n\ni\n\nD\u03c6(\u03c6(x1 )\u03c6(x2 ) * * * \u03c6(xk ))e ~\nR\n\n< \u03c6(x1 )\u03c6(x2 ) * * * \u03c6(xk ) >f ree =\n\nR\n\n(L0 (\u03c6)+J\u03c6)d4 x\n\n\u22121\nconst\n=p\ne\u2212L0 (J,J)\ndet(L0 )\ni\n\nR\n\n(6)\n\n4\n\nD\u03c6(\u03c6(x1 )\u03c6(x2 ) * * * \u03c6(xk ))e ~ (L0 (\u03c6))d x\nR\n=\nR\ni\n4\nD\u03c6e ~ (L0 (\u03c6))d x\n\u0010\n\u0011\n\u2202\n\u2202\n\u2202\n*\n*\n*\nZ(J)\n\u2202J(x1 ) \u2202J(x2 )\n\u2202J(xm )\nJ=0\n=\nZ|J=0\n\n(7)\n\nThe formula (6) is the direct generalization of (4) to the case of functional integrating, and determinant\nin it is so-called functional determinant. L0\u22121 is called propagator of the \u001celd \u03c6. This result in another\nform is also called the Wick theorem. Now, to calculate (5) one just expands non-quadratic terms\nin the exponent and calculates only correlators in the free theory (i. e. in the theory with quadratic\nLagrangian). The quantities (5) in this approach are calculated perturbatively. It is more preferable to\ncalculate them non-perturbatively, exactly. To solve this problem, it is necessary to evaluate integrals:\nZ\n\ne(J\n\nix\n\nij\nijk x x x +*** )\ni +A xi xj +B\ni j k\n\ndx1 dx2 * * * dxn\n\n(8)\n\nin the limit n \u2192 \u221e (n is the number of xi ). Dots in the exponent substitute parts of the Lagrangian of\nhigher degrees. The integrals of type (8) are called non-Gaussian integrals. Non-Gaussian integrals are\nalso studied in [7]. Now there are no simple methods of evaluating such integrals, and the expressions\nfor these integrals were obtained in [8] only for several simple cases. The expressions for these integrals\ndepend on the number of variables (unlike the gaussian integral), therefore it is not evident how they\nbehave in the limit n \u2192 \u221e. In [8] it was shown, that discriminants of non-linear forms play an\nimportant role in the evaluation of the non-Gaussian integrals, e. g. they control singularities of these\nintegrals. The possible approach to calculating non-Gaussian integrals is to use some form of canonical\nrepresentation of non-linear form. In general case, non-linear form cannot be brought to diagonal\nrepresentation, but the free parameters of transformations can be used to \u001cx up some coe\u001ecients of\njkm\nthe form. A possible variant of such representation of the maps (Aji\n, . . . ) under GLn action is\nk , Ai\npresented in sect 3.5. Under the action of SOn the canonical representations of forms Aijk , Ajkim , . . .\njkm\nand for maps Aji\n, . . . are the same, therefore studying canonical representation of map under\nk , Ai\nSOn can help us to evaluate non-Gaussian integrals.\n\n5\n\n\f1.5\n\nTerms and notations\n\n1.5.1 Homogeneous and non-homogeneous equations\nPolynomial f (x1 , . . . , xn ) in variables x1 , . . . , xn is called homogeneous polynomial, if for any \u03bb 6= 0 :\nf (\u03bbx1 , . . . , \u03bbxn ) = \u03bbd f (x1 , . . . , xn ). Non-negative integer d is called the degree of f . Any homogeneous\npolynomial can be made non-homogeneous by dividing it by one of the variables to power d, for\nexample, by (xn )d . After such division, the ratios x1 /xn , x2 /xn , . . . , xn\u22121 /xn can be taken as new\nvariables: y1 = x1 /xn , y2 = x2 /xn , . . . , yn\u22121 = xn\u22121 /xn , so the number of variables was decreased\nby one. The variables y1 , . . . , yn\u22121 are called non-homogeneous variables, and x1 , . . . , xn are called\nhomogeneous variables. In this paper we will denote by \u039b(i)\nj j-th component of i-th root of a system\n(i)\nof equations in homogeneous variables, and by \u03bb i-th root of one equation in one non-homogeneous\nvariable. For example, if the equation were a0 (x2 )d + a1 (x2 )d\u22121 x1 + * * * + ad\u22121 x2 (x1 )d\u22121 + ad (x1 )d = 0,\n(i)\nthen \u039b(i)\n1 is x1 -component of i-th solution, \u039b2 is x2 -component of i-th solution. This equation in\nnon-homogeneous variable z = xx21 states a0 + a1 z + * * * + ad\u22121 z d\u22121 + ad z d = 0. \u03bb(i) is i-th root\n(i)\nof this equation. In sect 2 we consider only homogeneous functions of \u039b(i)\nj , because all \u039bj can be\nsimultaneously rescaled and remain the solution of the system. Symmetrical combination of roots \u039b(i)\nis an expression, which does not change under swapping of any pair \u039b(i) and \u039b(j) .\n\n1.5.2 Maps and resultants\nWe study homogeneous polynomial maps of vector spaces:\nzi \u2192 Aii1 i2 ***is zi1 zi2 zi3 * * * zis\n\nThe degree of the map is denoted by s. If s = 1, we get ordinary linear maps, which are the objects of\nconsideration of standard course of linear algebra [3]. Many objects of linear algebra can be generalized\nto describe the non-linear case. First, consider the system of equations\nAii1 i2 ***is zi1 zi2 zi3 * * * zis = 0\n\nThis is a system of n homogeneous equations of n homogeneous variables or n \u2212 1 non-homogeneous\nvariables. Such system in general case has no non-trivial solutions at all. Non-trivial solution is a\nsolution with at least one component being non-zero. For such solution to exist, the coe\u001ecients of\nthe system must satisfy one relation, because the number of variables minus the number of equations\nequals one. This relation is\nR{A} = 0\n\nwhere R{A} is polynomial of the coe\u001ecients of A, called resultant or hyperdeterminant of A. If s = 1,\ni. e. A is the linear map, the resultant reduces to ordinary determinant of matrix. Resultants play\nan increasing role in modern mathematics and physics. See, for example [2, 1, 9, 10, 11] for overview,\n[12, 13] for applications in physics and engineering, [14] for application in string theory and [15, 16]\nfor computational methods. By non-degenerate map we mean map with non-vanishing resultant.\n\n2 Complanart and symmetric combinations of the roots\n2.1\n\nResultant and generalization of Vieta formulas\n\nLet f1 (x), * * * , fn\u22121 (x) be n \u2212 1 homogeneous polynomials of arbitrary degrees ri of n variables\nx1 , * * * , xn . It is well known, that the system of equations:\n\uf8f1\n\uf8f4\n\uf8f2 f1 (x) = 0\n\uf8f4\n\uf8f3\n\n..\n.\n\nfn\u22121 (x) = 0\n\n6\n\n(9)\n\n\fin general case has N = r1 r2 * * * rn projectively-inequivalent solutions, see [4]. Sometimes they may\ncoincide, so N should account for multiplicity. For example, one polynomial of 2 homogeneous variables\nor 1 non-homogeneous variable has exactly r1 projectively-inequivalent solutions (with multiplicities).\nDenote di\u001berent solutions of (9) by \u039b(1) , \u039b(2) , * * * , \u039b(N ) . Each solution is also a vector, so \u039b(i)\nj is a j-th\nvector component of i-th solution vector. Thus, Vieta formulas:\n(\u03bc )\n\n(\u03bc )\n\n(\u03bc )\n\nsymm\u03bc1 ,*** ,\u03bcN {\u039bj1 1 \u039bj2 2 * * * \u039bjNN } \u2261\n\nX\n\n(\u03c3(1))\n\n\u039bj1\n\n(\u03c3(2))\n\n\u039b j2\n\n(\u03c3(N ))\n\n* * * \u039b jN\n\n= Vj1 ***jN\n\n(10)\n\n\u03c3\u2208PN\n\nVj1 ***jN is homogeneous polynomial of coe\u001ecients of fi . The degree of Vj1 ***jN in the coe\u001ecients of\ni-th polynomial fi equals N/ri = r1 * * * ri\u22121 ri+1 * * * ri , see [2]. In some particular cases the formula\nfor Vj1 ***jN may be obtained from simple considerations (see for detailed discussion and examples [2]),\n\nbut we consider now a general method of calculating it. This method uses a Poisson product formula\nfor resultant, see e. g. [1, 16]. Add one more homogeneous polynomial g(x) of degree r of the same\nvariables x1 , * * * , xn . Then the system of equations\n\uf8f1\nf1 (x) = 0\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n..\n\n.\n\n\uf8f4\nfn\u22121 (x) = 0\n\uf8f4\n\uf8f4\n\uf8f3\ng(x) = 0\n\nwill possess a resultant. Poisson product formula states:\nR{f1 , * * * , fn\u22121 , g} = C\n\nN\nY\n\ng(\u039b(i) )\n\n(11)\n\ni=1\n\nC is a constant, depending on the normalization of the roots; we can normalize the roots in such a way,\nthat C will be equal 1. This formula has the following meaning: the system has non-zero solution i\u001b\ng equals zero on one of the roots of other functions. With the help of this formula, one easily obtains\nthe tensor Vj1 ***jN . One can substitute for g linear function: g(x) = g i xi , then calculate the resultant\n\nof the system (11) , and (as it is easily seen from Poisson product formula):\nVj1 ***jN = the coefficient before g j1 * * * g jN\n\n2.2\n\nin R{f1 , * * * , fn\u22121 , g}\n\nSymmetric combinations\n\nThe formula (11) is easily generalized. For example, one needs to calculate:\nPg =\n\nN\nY\n\ng(\u039b(i) , \u039b(j) )\n\n(12)\n\ni,j=1\n\nAt \u001crst one calculates the resultant of the system:\n\uf8f1\ng(x, y) = 0\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2 f1 (x) = 0\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\n..\n.\n\nfn\u22121 (x) = 0\n\non the variables x1 , * * * , xn , treating y1 , * * * , yn as parameters. This resultant we will denote\nRx {g, f1 , * * * , fn\u22121 }. It is still a polynomial in the variables y1 , * * * , yn . Then one computes the\nresultant of the system:\n\uf8f1\nRx {g, f1 , * * * , fn\u22121 }(y) = 0\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\nf1 (y) = 0\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\n..\n.\n\nfn\u22121 (y) = 0\n\n7\n\n\fin variables y1 , * * * , yn . Now it is obvious, how to get\nN\nY\n\nh(\u039b(i) , \u039b(j) , \u039b(k) )\n\ni,j,k=1\nN\nY\n\nu(\u039b(i) , \u039b(j) , \u039b(k) , \u039b(m) )\n\ni,j,k,m=1\n\n..\n.\nOne should calculate resultant as many times, as there are di\u001berent arguments in desired function of\nroots. But in sect 2.3 expressions of such a type are needed:\nPg0 \u2261\n\nN\nY\n\n(13)\n\ng(\u039b(i) , \u039b(j) )\n\ni,j=1\n\ni6=j\n\nThe di\u001berence between (12) and (13) is, that there is a product over all pairs of non-coincident indices\nin (13), but in (12) there is a product over all pairs of indices. It seems, that (13) can be calculated in\nthis way:\n\nPg0 =\n\nN\nQ\n\nN\nY\n\ng(\u039b(i) , \u039b(j) )\n\ni,j=1\n\ng(\u039b(i) , \u039b(j) ) =\n\nN\nQ\n\ni,j=1\n\ni6=j\n\n(14)\n\ng(\u039b(i) , \u039b(i) )\n\ni=1\n\nNumerator in this expression can be evaluated using the formula (12), and the denominator can be\nevaluated using (11). But a problem can arise. If g(x, y) is an antisymmetric function g(x, y) =\n\u2212g(y, x), both numerator and denominator of (14) are equal to zero. So Pg0 can not be evaluated\nstraightforwardly using (14). We have found a way for evaluating Pg0 in this case. Let g0 (x, y) - some\nantisymmetric linear polynomial on x1 , * * * , xn , y1 , * * * , yn . Let us consider g(x, y) = g0 (x, y)+tg1 (x, y),\nwhere g1 (x, y) is\ng1 (x, y) =\n\nn\nX\n\nn\nX\n\nxi yj =\n\ni,j=1\n\n!\nxi\n\ni=1\n\nn\nX\n\n!\n\n(15)\n\nyi\n\ni=1\n\nThis expression seems strange because it is not invariant under GLn action. We will discuss it a bit\nlater. So:\nPg00\n\n\u2261\n\nQN\n\nN\nY\n\n(i)\n\ng0 (\u039b , \u039b\n\n(j)\n\n(i)\n(j)\ni,j=1 g(\u039b , \u039b )\n(i)\n(i)\ni=1 g(\u039b , \u039b )\n\n) = lim QN\nt\u21920\n\ni,j=1\n\n=\n\ni6=j\n\nQN\n\n=\n\n(i)\n(j)\n(i)\n(j)\ni,j=1 (g0 (\u039b , \u039b ) + tg1 (\u039b , \u039b ))\nlim QN\n(i)\n(i)\n(i)\n(i)\nt\u21920\ni=1 (g0 (\u039b , \u039b ) + tg1 (\u039b , \u039b ))\n\n(16)\n\nFor non-linear polynomials g0 (x, y) there is the same technique, the tensor g1 (x, y) should be chosen\nin other way. For example, for quadratic g0 (x, y) :\ng1 (x, y) =\n\nn\nX\ni,j,k,m=1\n\nxi xj yk ym =\n\nn\nX\n\n!\nxi\n\ni=1\n\nn\nX\ni=1\n\n8\n\n!\nxi\n\nn\nX\ni=1\n\n!\nyi\n\nn\nX\ni=1\n\n!\nyi\n\n(17)\n\n\fThis method works not only for antisymmetric function of two variables, but for any number of\nvariables. This is the formula for three variables:\n!\u0012\n\nN\nQ\n\nPg00 \u2261\n\nN\nY\n\ng(\u039b(i) , \u039b(j) , \u039b(k) )\n\n\u00132\n\ng(\u039b(i) , \u039b(i) , \u039b(i) )\n\ni=1\n\ni,j,k=1\n\ng0 (\u039b(i) , \u039b(j) , \u039b(k) ) = lim\n\nN\nQ\n\nt\u21920\n\n!3\n\nN\nQ\n\ni,j,k=1\n\ni6=j;i6=k;k6=j\n\n(18)\n\ng(\u039b(i) , \u039b(i) , \u039b(j) )\n\ni,j=1\n\nNow g0 (x, y, z) - homogeneous function, g(x, y, z) = g0 (x, y, z) + tg1 (x, y, z). For linear g0 :\nn\nX\n\ng1 (x, y, z) =\n\n(19)\n\nxi yj zk\n\ni,j,k=1\n\nThese formulas work for g0 of any degree and of any symmetry. If g(x, y, z) is of other degree, g1\nshould be chosen of the same degree. For example, for quadratic g0 we would write:\nn\nX\n\ng1 (x, y, z) =\n\nxi xj yk ym zl zp\n\n(20)\n\ni,j,k,m,l,p\n\nIf the limits (16),(18) can be evaluated, they do not depend on the choice of g1 . But sometimes, if we\nchoose degenerate g1 or simply g1 with small number of non-zero components, both numerator and\ndenominator of (16) will be zero even at t 6= 0, and we will not be able to calculate the limit. The\nexample of this phenomena will be given in s.2.3.5. Expressions (15,17,20) are simply examples of\nnon-degenerate maps with all components being nonzero. The example of application of the formulae\n(16),(18) is calculation of complanarts.\n\n2.3\n\nComplanart\n\nWhen the system (9) has coincident roots? In the case of two variables (when there is only one\npolynomial) the answer is given by discriminant of the polynomial. Discriminant equals:\nD=\n\nr1\nY\n\n\u0010\n\n\u03b5\n\nkm\n\n\u00112\n(i)\n\u039bk \u039b(j)\nm\n\ni,j=1,i<j\n\nr1\nY\n\n=\n\n(\u03bb(i) \u2212 \u03bb(j) )2\n\n(21)\n\nj,i=1,i<j\n\nr1 is the degree of a polynomial. The discriminant equals zero i\u001b there is a pair of roots, in which\none is proportional to another (\u039b(i) \u221d \u039b(j) , i 6= j , homogeneous formulation), or two equal nonhomogeneous roots (\u03bb(i) = \u03bb(j) , non-homogeneous formulation). In three-dimensional space, however,\nthe proportionality of two vectors is de\u001cned by two conditions (e. g. x2 /x1 = y2 /y1 and x3 /x1 = y3 /y1 ).\nBut there is a natural one condition for complanarity of three vectors: \u03b5ijk xi yj zk = 0. If \u03b5ijk xi yj zk = 0,\nthree vectors x, y, z lie in one plane, or they are simply complanar. So, we can formulate the condition\n\nof a system of equations to have three complanar roots:\nC\u2261\n\nN \u0010\nY\n\n(j)\n\n\u00112\n\n,\n\n(22)\n\nC=0\n\n(23)\n\n(k)\n\u03b5mlp \u039b(i)\nm \u039bl \u039bp\n\ni,j,k=1\n\ni<j<k\n\n(squared for symmetry). The condition itself is (23). (22) is some symmetrical polynomial of the\n(j) (k)\nroots of the system. To make it symmetric combination \u03b5mlp \u039b(i)\nm \u039bl \u039bp was squared. This symmetric\npolynomial of roots we call complanart. For two variables complanart reduces to ordinary discriminant,\nfor four variables:\nC=\n\nN\nY\n\n\u0010\n\n(j)\n\n(k) (q)\n\u03b5mlpr \u039b(i)\nm \u039bl \u039bp \u039br\n\ni,j,k,q=1,\n\ni<j<k<q\n\n9\n\n\u00112\n\n(24)\n\n\fand for n variables:\nN\nY\n\nC=\n\n\u0010\n\n(i )\n\n\u0011\n(i ) 2\n\n(i )\n\n(25)\n\n\u03b5j1 j2 ***jn \u039bj11 \u039bj22 * * * \u039bjnn\n\ni1 ,*** ,in =1\n\ni1 <i2 <***<in\nN!\nThe complanart has degree 2nCNn = 2n (N \u2212n)!n!\non \u039b, since every factor in product has degree 2n and\n\nthere are CNn factors. Each \u039b \u221d\n\nn\u22121\nQ\ni=1\n\n(ai )1/ri , where ai denotes coe\u001ecients of fi (see (10)). So:\nn\u22121\nY\n\nC\u221d\n\n(ai )\n\nn\u22121 N\n2CN\n\u22121 r\n\n(26)\n\ni\n\ni=1\nN\ni. e. degai C = 2CNn\u22121\n\u22121 ri . If n = 2, then complanart is discriminant, the number of solutions N = r ,\n(r\u22121)!\nand dega C = 2 1!(r\u22122)!\n= 2(r \u2212 1). It is well-known expression for degree of discriminant.\n\n2.3.1 Evaluation of complanarts\nComplanart is just a product of values of antisymmetric function over sets of all di\u001berent n roots. The\nmethod of evaluation of such quantities is given in 2.2. It is necessary just to take for g0 absolutely\nantisymmetric \u03b5-tensor of appropriate dimension. The formula for two variables:\nQN\n\nQN\n\nC=\n\n(i)\n(j)\ni,j=1 g(\u039b , \u039b )\nlim QN\n(i)\n(i)\nt\u21920\ni=1 g(\u039b , \u039b )\n\n(i)\n(j)\n(i)\n(j)\ni,j=1 (\u03b5(\u039b , \u039b ) + tg1 (\u039b , \u039b ))\n(i)\n(i)\n(i)\n(i)\ni=1 (\u03b5(\u039b , \u039b ) + tg1 (\u039b , \u039b ))\n\n= lim QN\nt\u21920\n\n(27)\n\nFor three variables:\n\u0010Q\nC 3 = lim\n\nN\n(i)\n(j)\n(k)\ni,j,k=1 g(\u039b , \u039b , \u039b )\n\n\u0010Q\n\nt\u21920\n\n\u0011 \u0010Q\n\n\u00112\n\nN\n(i)\n(i)\n(i)\ni=1 g(\u039b , \u039b , \u039b )\n\nN\n(i)\n(i)\n(j)\ni,j=1 g(\u039b , \u039b , \u039b )\n\n(28)\n\n\u00113\n\ng(x, y, z) = \u03b5(x, y, z) + tg1 (x, y, z)\n\nFor four variables:\nN\nQ\n\nC 12 = lim\n\ng(\u039b(i) , \u039b(j) , \u039b(k) , \u039b(m) )\n\n!8\ng(\u039b(i) , \u039b(i) , \u039b(i) , \u039b(j) )\n\ni,j=1\n\ni,j,k,m=1\n\nt\u21920\n\nN\nQ\n\n!3\ng(\u039b(i) , \u039b(i) , \u039b(j) , \u039b(j) )\n\ni,j=1\n\n!6 \u0012\n\nN\nQ\n\nN\nQ\n\ng(\u039b(i) , \u039b(i) , \u039b(j) , \u039b(k) )\n\nN\nQ\n\n(29)\n\n\u00136\n(i)\n(i)\n(i)\n(i)\ng(\u039b , \u039b , \u039b , \u039b )\n\ni=1\n\ni,j,k=1\n\ng(x, y, z, u) = \u03b5(x, y, z, u) + tg1 (x, y, z, u)\n\nThere is one more fact to explain. Why for three variables the limit yields to C 3 , and for four variables\nit yields for C 12 ? It is because the formulae (27),(28),(29) give us the following expressions:\nN\nY\n\nY\n\n\u03b5(\u039b(i) , \u039b(j) ) instead of\n\nN\nY\n\n(\u03b5(\u039b(i) , \u039b(j) ))2\n\ni,j=1\n\ni,j=1\n\ni6=j\n\ni<j\n\n\u03b5(\u039b(i) , \u039b(j) , \u039b(k) ) instead of\n\nN\nY\n\ni,j,k=1\n\ni,j,k=1\n\ni6=j;i6=k;j6=k\n\ni<j<k\n\n(\u03b5(\u039b(i) , \u039b(j) , \u039b(k) ))2\n\n..\n.\n10\n\n(30)\n(31)\n\n\fIn the case n = 2 these two things coincide (because there is 2 permutations and accounting for them\nadds necessary squaring). In case n = 3 there are n! = 6 permutations, but a square is again needed n!\nso appears C 3 . In case of arbitrary n this procedure yields C 2 .\nIt is now a simple exercise to write the formula for complanart analogous to (27),(28),(29) in any particular dimension: it is necessary only that all the factors, appeared in\n\nN\nQ\n\ng(\u039b(i1 ) , \u039b(i2 ) , * * * , \u039b(in ) )\n\ni1 ,i2 ,*** ,in =1\n\nwith some two or more i coincident will be cancelled by factors with explicitly equal i, for example\nN\nQ\n\ni1 ,i2 ,*** ,in =1\n\ng(\u039b(i1 ) , \u039b(i1 ) , \u039b(i3 ) , * * * , \u039b(in ) ) etc.\n\n2.3.2 When complanart is equal to 1?\nComplanart measures linear dependence of n distinct roots of polynomial system of equations (9).\nSuch system of equations in general case have N = r1 r2 * * * rn\u22121 projectively-nonequivalent roots,\nwhere r1 , r2 , * * * , rn\u22121 are degrees of equations. But what happens when N < n? It means, that there\nare no n distinct roots, so no n distinct roots can be complanar. Therefore the complanart of such a\nsystem is equal to non-zero constant, i. e. it does not depend on the coe\u001ecients of the system. What\nis this constant? Let us consider the simplest example: one linear equation, for example ax1 + bx2 = 0.\n(1)\nIt has one solution: \u039b(1)\n1 = \u2212b, \u039b2 = a. Complanart equals:\nQN\n\nC=\n\n(i)\n(j)\ni,j=1 g(\u039b , \u039b )\nlim QN\n(i)\n(i)\nt\u21920\ni=1 g(\u039b , \u039b )\n\nQN\n\n=\n\n(i)\n(j)\n(i)\n(j)\ni,j=1 (\u03b5(\u039b , \u039b ) + tg1 (\u039b , \u039b ))\nlim QN\n(i)\n(i)\n(i)\n(i)\nt\u21920\ni=1 (\u03b5(\u039b , \u039b ) + tg1 (\u039b , \u039b ))\n\n(32)\n\nBut there is only one solution, so the numerator equals g(\u039b(1) , \u039b(1) ) and denominator equals g(\u039b(1) , \u039b(1) ).\nSo complanart equals 1. Due to similar reasons, complanart equals 1 always when\nN = r1 r2 * * * rn\u22121 < n. For example, complanart equals 1 for any system of appropriate number of\nlinear equations.\n\n2.3.3 Open questions\nThe \u001crst question is about the choices of g1 : (15),(17),(20). These formulas are not GLn -invariant.\nThe second question: since we do not know appropriate canonical g1 , we may take: g(x, y, * * * ) =\n\u03b5(x, y, * * * ) + t1 g1 (x, y, * * * ) + t2 g2 (x, y, * * * ) + * * * , and now look at the same limit, but taken on di\u001berent\nt variables in di\u001berent consequence. The third question is: by taking a limit at (27-29), only term\nwith the lowest degree of t is considered. It is possible, that the terms of higher degrees contain\ninformation about higher degenerations of the system. For example, vanishing of some higher term(s)\nmay correspond to existence of two coinciding roots and so on. This point can be even more interesting\nconsidering the higher terms in the case of many t. The theory of higher complanarts would also be\nvery interesting and useful in applications.\n\n2.3.4 Examples of complanarts: n = 2, complanarts are discriminants\nQuadratic equation Our \u001crst example is:\nf (x) = a(x1 )2 + bx1 x2 + c(x2 )2 ,\n\nor in non-homogeneous variable z \u2261 x1 /x2 :\naz 2 + bz + c = 0\n\nTake g(x, y) = \u03b5(x, y) + tg1 (x, y) = x1 y2 \u2212 x2 y1 + t(x1 y1 + x1 y2 + x2 y1 + x2 y2 ). For this equation and\nthis g1 :\nY\n\ng(\u039b(i) , \u039b(j) ) = t2 (a + c \u2212 b)2 (4ac \u2212 b2 + t2 (b2 \u2212 2ab + a2 \u2212 2bc + 2ac + c2 ))\n\ni,j\n\nY\ni\n\n11\n\ng(\u039b(i) , \u039b(i) ) = t2 (a + c \u2212 b)2\n\n\fIt is easily seen, that C = 4ac \u2212 b2 , and this expression coincides with discriminant of quadratic\npolynomial. Now take another g1 = x1 y2 . For this g1 :\nY\n\ng(\u039b(i) , \u039b(j) ) = t2 ac(4ac \u2212 b2 + t(4ac \u2212 b2 ) + t2 (ac \u2212 b2 ))\n\ni,j\n\nY\n\ng(\u039b(i) , \u039b(i) ) = t2 ac\n\ni\n\nThe expressions have changed. But the value of complanart is still 4ac \u2212 b2 .\n\nCubic polynomial\n(33)\n\nf (x) = ax3 + bx2 y + cxy 2 + dy 3\n\nComplanart of this polynomial is equal 4ac3 + 4db3 + 27a2 d2 \u2212 b2 c2 \u2212 18abcd, and it again coincides\nwith the discriminant of the polynomial. Other expressions are very long in this case, and we do not\nwrite them here.\n\n2.3.5 Examples of complanarts:\nTwo quadratic polynomials, 1\n\nn=3\n\nf1 (x) = a11 (x1 )2 + a21 x1 x2 + a13 x1 x3\nf2 (x) = b22 (x2 )2 + b21 x2 x1 + b23 x2 x3\n\nTake, for example, g(x, y, z) = \u03b5(x, y, z)+tg1 (x, y, z) = (1+t)x1 y2 z3 \u2212x1 y3 z2 +(1+t)x2 y3 z1 \u2212x2 y1 z3 +\n(1 + t)x3 y1 z2 \u2212 x3 y2 z1 . Than all three terms in (28), namely\nN\nQ\n\ng(\u039b(i) , \u039b(j) , \u039b(k) ),\n\nN\nQ\n\nN\nQ\n\ng(\u039b(i) , \u039b(i) , \u039b(i) ),\n\ni=1\n\ni,j,k=1\n\nevaluated. Nevertheless, if we take, for example:\ng1 =\n\nn\nX\n\ni,j=1\n\ng(\u039b(i) , \u039b(i) , \u039b(j) ), vanish, so the limit cannot be\n\nxi yj = x1 y1 + x1 y2 + x1 y3 + x2 y1 + x2 y2 + x2 y3 + x3 y1 + x3 y2 + x3 y3 ,\n\ni,j=1\n\ni. e. choose g1 according to (19), this problem is eliminated. The limit (28) equals\n\u0010Q\nlim\n\n\u0011 \u0010Q\n\nN\n(i)\n(j)\n(k)\ni,j,k=1 g(\u039b , \u039b , \u039b )\n\n\u0010Q\n\nt\u21920\n\n\u00112\n\nN\n(i)\n(i)\n(i)\ni=1 g(\u039b , \u039b , \u039b )\n\n\u00113\n\n=\n\nN\n(i)\n(i)\n(j)\ni,j=1 g(\u039b , \u039b , \u039b )\n\n(a13 )12 (b23 )12 (a21 b23 \u2212 a13 b22 )6 (b23 a11 \u2212 a13 b21 )6 (\u2212b22 a11 a13 b23 \u2212 b21 a21 a13 b23 + b21 a213 b22 + b223 a21 a11 )6\n\nComplanart can be calculated also by bare hands, i. e. by solving equations f1 (x) = 0, f2 (x) = 0 and\nthen substituting these solutions in (22). Calculated complanart equals:\n(a13 )4 (b23 )4 (a21 b23 \u2212 a13 b22 )2 (b23 a11 \u2212 a13 b21 )2 (\u2212b22 a11 a13 b23 \u2212 b21 a21 a13 b23 + b21 a213 b22 + b223 a21 a11 )2\n\nThus, the formula (28) holds.\n\nTwo quadratic polynomials, 2\n\nThis example is just the particular case of previous one, but it is\nof great importance for further considerations of sect 5:\nf1 (x) = x2 + 2axy \u2212 xz\n2\n\nf2 (x) = y + 2bxy \u2212 yz\n\n(34)\n(35)\n\nThe complanart cubed, and the limit in the formula (28) yield:\nC 3 = (1 \u2212 2a)12 (1 \u2212 2b)12\n\n12\n\n(36)\n\n\f3 Eigenvectors, eigenvalues and GLn canonical representation of nonlinear maps\n3.1\n\nNon-linear eigenvectors, eigenvalues and how they can be found\n\nNon-zero vector zi is called eigenvector of A, if it satis\u001ces (with some \u03bb):\nAii1 i2 ***is zi1 zi2 zi3 * * * zis = \u03bb(z)zi\n\n(37)\n\nA polynomial \u03bb(z) of degree s \u2212 1 is called eigenvalue. Eigenvalue is not a number, because the\nhomogeneity constraint is imposed. The homogeneity constraint is nothing but requirement that any\neigenvector can be multiplied by some non-zero number and remain eigenvector. There is also a\nnon-linear analogue of characteristic polynomial of a map, namely:\nChA \u2261 R{Ai (z) \u2212 \u03bb(z)zi }\n\n(38)\n\nBy de\u001cnition of resultant, ChA turns to zero i\u001b exists an eigenvector, corresponding to the polynomial\n\u03bb(z). The optimal way to \u001cnd eigenvectors/eigenvalues in non-linear algebra di\u001bers from the way\nin linear algebra. Solving equation ChA = 0 with respect to \u03bb is complicated, because it is a nonn \u22121\n(n+s\u22122)!\nhomogeneous equation of Mn|s\u22121 = (n\u22121)!(s\u22121)!\ncoe\u001ecients of \u03bb of the degree cn|s = ss\u22121\n. It turns\nout, that ChA is not an arbitrary polynomial, it has a structure - namely, this polynomial is always\ndecomposable on linear in the coe\u001ecients of \u03bb factors. It is much simpler to \u001cnd these factors at\n\u001crst, and then \u001cnd eigenvalues by solving linear equations. To decompose ChA , it is necessary to\n\u001cnd eigenvectors, this is described in sect 3.3. The method of eliminating \u03bb from the equation (37) is\ndescribed in sect 3.2.\n\n3.2\n\nZero, nonzero and unitary eigenvectors\n\nOne encounters two cases considering a particular eigenvector zi : \u03bb(z) 6= 0 and \u03bb(z) = 0. Consider\nthe \u001crst case. All eigenvectors with \u03bb(z) 6= 0 we call non-zero eigenvectors. This notation can not\ncause any misinterpretation, because each eigenvector is by de\u001cnition non-zero in the sense that not\nall its components are zeros. Non-zero eigenvector can be rescaled by any non-zero number, because\nthe system of equation (37) is homogeneous system. For example, zi can be rescaled as follows:\nzi\n. Now, \u03bb(y) = \u03bb(z)s\u22121 = 1, and\nyi = \u03bb(z)1/(s\u22121)\n(\u03bb(z)) s\u22121\n\nAi (y) = yi\n\n(39)\n\nThe eigenvectors, obeying (39), are called unitary eigenvectors. \u03bb was eliminated from equation for\neigenvectors/eigenvalues. Thus, all non-zero eigenvectors can be rescaled to unitary vectors, which\nsatisfy (39).\nNow consider the case \u03bb(z) = 0. Such eigenvectors are called zero-eigenvectors. Any zero eigenvector\nsatis\u001ces the equation:\nAi (z) = 0,\n\n(40)\n\nwhich also does not contain \u03bb.\nAny solution of (39) or (40) is an eigenvector. To prove this, it is su\u001ecient to construct \u03bb satisfying\nthe equation (37) with this zi . If yi is the solution of (39), yi is the eigenvector with any \u03bb such that:\n\u03bb(yi ) = 1,\n\n(41)\n\nas it is easily seen from (37). If zi is the solution of (40), it is eigenvector with any \u03bb which satis\u001ces:\n\u03bb(zi ) = 0\n\nThus, the following statement was proven:\n13\n\n(42)\n\n\fProposition 1. All eigenvectors of A can be found among the solutions of (39) or (40), and, otherwise,\nany non-zero solution of (39) or (40) is eigenvector of A\nSo, the easy method of \u001cnding eigenvector/eigenvalues is: solve equations (39-40) to \u001cnd eigenvectors, and then \u001cnd \u03bb by algorithm, described above.\n\n3.3\n\nDecomposability of characteristic polynomial\n\nSince to \u001cnd \u03bb one has to solve linear on \u03bb equations (see previous subsection), the set of \u03bb, which are\neigenvalues of A, is a union of planes in the space of all \u03bb. Other way to reformulate this statement\nis to say: the characteristic polynomial ChA (\u03bb) is decomposable on linear on \u03bb factors. The condition\nof turning to zero one of the factors of ChA de\u001cnes the plane in the space of all \u03bb. These factors are\nthe following:\nR{Ai (z) \u2212 \u03bb(z)zi } = M (A)\n\nn1\nY\n\n(1 \u2212 \u03bb(e(\u03bc) ))\n\nn2\nY\n\n(\u03bb(z (\u03bd) )),\n\n(43)\n\n\u03bd=1\n\n\u03bc=1\n\nwhere e(\u03bc) is the \u03bc-th unitary eigenvector, and n1 is the number of them, z (\u03bd) is \u03bd -th zero eigenvector,\nand n2 is the number of them, and M (A) depends only on coe\u001ecients of A, not on the coe\u001ecients of\n\u03bb. Indeed, all possible eigenvectors are found among the solutions (39-40) modulo projective rescaling.\nTherefore all possible eigenvalues satisfy either the equation (41) with one of the unitary eigenvectors,\neither the equation (42) with one of the zero eigenvectors. If \u03bb satis\u001ces (41), r. h. s. of (43) turns to\nzero due to one of the factors (1 \u2212 \u03bb(e\u03bc )). If \u03bb satis\u001ces (42), r. h. s. (43) turns to zero due to one of\nthe factors \u03bb(z\u03bd ).\n\n3.4\n\nNumber of eigenvectors\n\nThe systems of equations (39) and (40) can be merged to one system of homogeneous equations by\nadding an auxiliary variable y :\n(44)\n\nAi (x) = (y)s\u22121 xi\n\n(44) is a system of n homogeneous equations of n + 1 variables x1 , x2 , * * * , xn , y . If a solution of (44)\nhas y 6= 0, we can consider the vector x1 /y, * * * , xn /y, 1, which is a solution both of (44) and of (39),\nand then \u001crst n components of it will be an unitary eigenvector. If a solution of (44) has y = 0, the\n\u001crst n components x1 , . . . , xn of this solution are simultaneously a solution of (40) and vector of them\nis a zero eigenvector. The examples of \u001cnding eigenvectors using (44) are presented in sect 3.5 and in\nsect 5. So, the following statement holds:\n\nProposition 2. All eigenvectors of A can be found among the solutions of (44) (these solutions should\nbe normalized, if necessary).\nConsider non-degenerate maps, R{Ai (z)} =\n6 0. Firstly, for such maps the equation (40) has no\nsolutions with at least one component being non-zero. The condition R{Ai (z)} =\n6 0 is by de\u001cnition of\nresultant the condition of non-existence of non-trivial solutions of (40). That is, non-degenerate map\ndoes not have zero eigenvectors. The same statement holds in the linear algebra. This means, that\nn2\nQ\nn2 = 0 and in (43) there is no factor\n\u03bb(z\u03bd ),\n\u03bd=1\n\nR{Ai (z) \u2212 \u03bb(z)zi } = M (A)\n\nn1\nY\n\n(1 \u2212 \u03bb(e\u03bc ))\n\n(45)\n\n\u03bc=1\n\nSetting in (45) \u03bb = 0, one obtains M (A) = R{A}. So\nR{Ai (z) \u2212 \u03bb(z)zi } = R{A}\n\nn1\nY\n\n(1 \u2212 \u03bb(e\u03bc ))\n\n\u03bc=1\n\n14\n\n(46)\n\n\fFor non-degenerate maps, n1 can be easily determined. Consider again (44). If R{A} 6= 0, it has no\nnon-zero solutions with y = 0 (the existence of such solutions would imply the existence of non-zero\nsolutions of (40), but this is prohibited by R{A} =\n6 0). As a system of n homogeneous equations (each\nof degree s, s - a degree of A) of n+1 variables, in general case (44) has sn projectively non-equivalent\nsolutions, see [4]. It has one solution which is not eigenvector:\n\u001a\n\nx1 = x2 = * * * = xn = 0\ny=1\n\n(47)\n\nThe number of solutions of (44) with at least one x-component being non-zero is sn \u22121. Some of these so(0)\n(0) (0)\nlutions yield projectively equivalent eigenvectors (in the space x1 , * * * , xn ). Let x(0)\n1 , x2 , * * * , xn , y\n(0)\n(0)\n(0)\nis a solution of (44). Then x(0)\n1 , x2 , * * * , xn , \u03c9s\u22121 y , where \u03c9s\u22121 is a root of degree s \u2212 1 from 1, is\nalso solution of (44). Since in complex plane there is s \u2212 1 distinct roots of 1, s \u2212 1 projectively inequivalent (in the space x1 , * * * , xn , y ) solutions of (44) lead to one eigenvector (or in projective equivalent\nn \u22121\nvectors in x1 , * * * , xn ). So the number of eigenvectors in general case is cn|s = ss\u22121\n. This formula was\npreviously obtained in [2], but was obtained from considerations for diagonal maps. So, characteristic\npolynomial has degree cn|s on the coe\u001ecients of \u03bb. Some of sn solutions of (44) can coincide (this\nphenomenon is controlled by complanart and higher complanarts, see 2.3). In this case there will be\nless eigenvectors than cn|s . It should be emphasized one more time, that all counting, leaded to the\nn \u22121\nformula cn|s = ss\u22121\n, was carried out under the condition R{A} 6= 0. For example, for unit maps (s.\n3.7.1) all the vectors in the space are eigenvectors.\n\n3.5\n\nGLn canonical representation of non-linear map\n\nIn linear algebra linear map (and quadratic form) have canonical representation (i. e., become diagonal)\nin the basis of its eigenvectors. The situation in non-linear algebra is similar, but there are some\ndi\u001berences. Firstly, in general case non-linear map cannot be brought to diagonal from, because the\ngroup GLn has n2 parameters, and non-linear map has more parameters. The second di\u001berence is, that\nnon-linear map in general case has cn|s eigenvectors, see 3.4. The number of eigenvectors is greater\nthan n, the dimension of the space. So there is uncertainty in canonical representation of non-linear\nmap, consisting in the freedom to choose one set of n eigenvectors from cn|s eigenvectors to be basis\nof the space. This uncertainty is yet poorly studied. To obtain canonical representation of non-linear\nmap, one should choose as basis vectors any n linear independent zero or unitary eigenvectors of the\nmap. The eigenvectors of the system can be found by solving the system of equations (44). Non-zero\neigenvectors should be normalized to make them unitary eigenvectors. Consider \u001crst what happens with\nthe components of the map, corresponding to unitary eigenvector. If unitary eigenvector zi is chosen\nto be a j -th vector of basis, zi = \u03b4ij . By de\u001cnition of unitary eigenvector, Aii1 i2 ***is zi1 zi2 zi3 * * * zis = zi .\nSubstituting zi = \u03b4ij , we obtain Ajj***j\n= \u03b4ij . If zero eigenvector yi is chosen to be k -th vector of\ni\nbasis, then yi = \u03b4ik . The equation for zero eigenvectors is: Aii1 i2 ***is yi1 yi2 yi3 * * * yis = 0. and therefore\nAkk***k\n= 0. Summarizing the statements about the canonical form:\ni\nAijj***j = \u03b4ij\nAikk***k\n\nIf\n\nj\n\n= 0 If\n\ncorresponds to unitary\nk\n\neigenvector in basis\n\ncorresponds to zero eigenvector in basis\n\n(48)\n(49)\n\nBy this method, only the components with all upper indices equal are \u001cxed, total n2 components.\nMore components can not be \u001cxed, since there are only n2 free parameters in GLn group. The rest\ncomponents of A remain arbitrary numbers. It is these numbers that determine the map in the nonlinear case. In linear case the map is de\u001cned by n eigenvectors and n eigenvalues. In non-linear case\nthe map is de\u001cned by n of its eigenvectors (not all!) and by these numbers, namely by the components\nwith not all upper indices equal. One more di\u001berence in these representations is that in non-linear case\nthe normalization of non-zero eigenvectors is important, but in linear case it is not.\n\n15\n\n\f3.5.1 Example of canonical representation\nThe example is the bringing the quadratic map of two variables to canonical representation:\n\u0012\n\nx1\nx2\n\n\u0013\n\n\u0012\n\u2192\n\n2\n12\n22\n2\nA11\n1 (x1 ) + 2A1 x1 x2 + A1 (x2 )\n2\n12\n22\n2\nA11\n2 (x1 ) + 2A2 x1 x2 + A2 (x2 )\n\n\u0013\n\n(50)\n\nTo obtain canonical representation of non-linear map, one should choose as basis vectors any n linear\nindependent zero or unitary eigenvectors of the map. As it was established in sect 3.4 and 3.2, to\nobtain the eigenvectors of the map (50), one should solve such systems of equations:\n\u001a\n\n2\n12\n22\n2\nA11\n1 (x1 ) + 2A1 x1 x2 + A1 (x2 ) = yx1\n2\n12\n22\n2\nA11\n2 (x1 ) + 2A2 x1 x2 + A2 (x2 ) = yx2\n\n(51)\n\nThis is a system of equations (44) for (50), additional homogenizing variable is also called y . Any\nsolution of the system (51), (x1 , x2 , y) with y 6= 0 is non-zero eigenvector for the map (50). Such\nsolution can be made unitary eigenvector by dividing all the components of it by y . Then the solution\nwill be (x1 /y, x2 /y, 1) and x1 /y, x2 /y are two components of unitary eigenvector. If there is a solution\nof (51) with third component equal 0, e. g. (x1 , x2 , 0), then (x1 , x2 ) are the components of zero\neigenvector. In some cases, the map can be represented in several forms of (52),(53),(54). For example,\nif the map has two unitary eigenvectors and one zero eigenvector, it can be represented either in form\n(52) or in the form (53).\n\nTwo unitary eigenvectors are chosen to be basis This choice \u001cxes the components to the\nfollowing values: A11\n1 = 1,\nthis case:\n\nA22\n1 = 0,\n\u0012\n\nx1\nx2\n\nA22\n2 = 1. Canonical representation of the map in\n\nA11\n2 = 0,\n\n\u0013\n\n\u0012\n\u2192\n\n(x1 )2 + 2A12\n1 x1 x2\n(x2 )2 + 2A12\n2 x1 x2\n\n\u0013\n\n(52)\n\nOne unitary and one zero eigenvector are chosen to be basis vectors In this case \"canonical\n\nbasis\" consists of one zero eigenvector and one unitary eigenvector. Let unitary eigenvector be the \u001crst\nA22\nA11\nvector of the basis, and zero eigenvector the second. This choice \u001cxes A11\n1 = 1,\n1 = 0,\n2 =\n22\n0, A2 = 0. Canonical representation of the map is:\n\u0012\n\nx1\nx2\n\n\u0013\n\n\u0012\n\u2192\n\n(x1 )2 + 2A12\n1 x1 x2\n2A12\nx\n2 1 x2\n\n\u0013\n\n(53)\n\nTwo zero eigenvectors are chosen to be basis vectors In this case \"canonical basis\" consists\n\nof two zero eigenvectors. This choice \u001cxes A11\n1 = 0,\nrepresentation of the map is:\n\u0012\n\n3.6\n\nx1\nx2\n\n\u0013\n\n\u0012\n\u2192\n\nA22\n1 = 0,\n\n2A12\n1 x1 x2\n2A12\n2 x1 x2\n\nA11\n2 = 0,\n\n\u0013\n\nA22\n2 = 0. Canonical\n\n(54)\n\nApplication of complanart to the theory of eigenvectors\n\nAs it was explained in 3.3, all the eigenvectors of A can be found by solving this system of equations:\n(55)\n\nAi (x) = (y)s\u22121 xi\n\nThis is a system of n homogeneous equations of n + 1 variables, so it possesses complanart (see 2.3).\nVanishing of complanart of this system means, that there are n + 1 complanar vectors in the space\n(x1 , x2 , . . . , xn , y), i. e. there exists set of n + 1 indices j1 , . . . , jn+1 with no pair of equal indices, such\nas:\n(j )\n\n(j )\n\n(j\n\n)\n\nn+1\n\u03b5i1 ...in+1 \u039bi11 \u039bi22 . . . \u039bin+1\n=0\n\n16\n\n(56)\n\n\f(j)\nHere \u039b(j) stands for j -th solution of (55), \u039b(j)\ni is i-th component of \u039b . Here i runs from 1 to n + 1,\n(j)\n\u039bn+1 is y -component of j -th solution. Let one of j indices, for example, j1 , corresponds to the solution\nof (55) of type (47), namely (x1 = 0, x2 = 0, . . . , xn = 0, y = 1). Then the sum over i1 in (56) is\nreduced to one term:\n(j )\n\n(j\n\n(j\n\n(j )\n\n)\n\n)\n\nn+1\nn+1\n1\n=\n= \u03b5(n+1)i2 ...in+1 \u039bi22 . . . \u039bin+1\n\u039bi22 . . . \u039bin+1\n\u03b5(n+1)i2 ...in+1 \u039bjn+1\n\nn\nX\n\n\u00b1\n\n(j )\n\n(j\n\nn+1\n\u03b5i2 ...in+1 \u039bi22 . . . \u039bin+1\n\n)\n\n(57)\n\ni2 ,...,in+1 =1\n1\nThe equality \u039bjn+1\n= 1, following from (47), was used. The explicit symbol of the sum in last line\nis to emphasize, that \u03b5 now is n-dimensional, not n + 1-dimensional and the sum over indices goes\nn+1 )\nto n, not to n + 1, as in the \u001crst line. Expression (57) is up to sign a condition of \u039bi(j22 ) , . . . , \u039bi(jn+1\nto be complanar in n-dimensional space of initial coordinates x1 , x2 , . . . , xn . If the solution (47) does\nnot enter in the formula (56), the formula (56) does not possess such a simple interpretation. So,\n\nvanishing of complanart of the system (55) means either that there are n eigenvectors,\ncomplanar in usual space x1 , . . . , xn , or that there are n + 1 eigenvectors, complanar in the\nspace with additional homogenizing variable, in space (x1 , . . . , xn , y).\n\n3.6.1 Example\nThis example is consideration of non-degenerate map of two variables, n = 2 and R{A} 6= 0. Since\nR{A} 6= 0,the map has only non-zero eigenvectors, which can be rescaled to be unitary eigenvectors.\nLet three unitary eigenvectors (x1 , x2 , 1), (u1 , u2 , 1), (z1 , z2 , 1) enter to the formula (56). Then\nformula (56) becomes:\n\nx1 u2 z3 \u2212 x1 u3 z2 + x2 u3 z1 \u2212 x2 u1 z3 + x3 u1 z2 \u2212 x3 u2 z1 = x1 u2 \u2212 x2 u1 + x2 z1 \u2212 x1 z2 + u1 z2 \u2212 u2 z1 = 0 (58)\n\nThe solution of type (47) equals in this case: (0, 0, 1). Let (x1 , x2 , 1), (z1 , z2 , 1) and the solution of type\n(47) enter to the formula (56). Then the formula (56) becomes:\nx1 z2 \u2212 x2 z1 = 0,\n\n(59)\n\nor simply condition of complanarity of two two-dimensional vectors (x1 , x2 ) and (y1 , y2 ) in twodimensional space. Vanishing of complanart of system (55) means, that there are such eigenvectors\n(x1 , x2 , 1), (u1 , u2 , 1), (z1 , z2 , 1), that either (58) or (59) holds. For degenerate map only the equality (58) would change. One more example of using complanart to determine degeneracy of eigenvector\npattern is given in sect 5.1.\n\n3.7\n\nHow to exclude \u03bb by one more method\n\nIn sect 3.2 the method of excluding \u03bb from equations for eigenvectors by renormalizing vectors was\nexplained. There is one more method of excluding \u03bb. This method was mentioned in [2]. The method\nis based on following statement:\n\nProposition 3. All the eigenvectors of the map\n\nequations:\n\nA can be found among the solutions of system of\n\nAi (z)zj \u2212 zi Aj (z) = 0,\n\n(60)\n\nand otherwise, any solution of (60) is an eigenvector.\n\nProof If zi is an eigenvector of Ai , Ai (z) = \u03bb(z)zi . Substituting this equality to (60) yields:\nAi (z)zj \u2212 Aj (z)zi = \u03bb(z)zi zj \u2212 \u03bb(z)zj zi = 0. Otherwise, let zi is the solution of (60). Multiply both\n17\n\n\fsides of equation (60) by a vector v i , orthogonal to zi . Then one obtains zj Ai (z)v i = Aj (z)zi v i = 0.\nSo, Ai (z) is orthogonal to any vector, orthogonal to zi , therefore Ai (z) = C * zi . One can obtain \u03bb(z),\ncorresponding to this eigenvector by solving the equation \u03bb(z) = C .\nAt \u001crst glance, in (60) there are n(n\u22121)\nnon-trivial equations. But only n \u2212 1 of them are independent:\n2\nyou can \u001cx i and get n \u2212 1 equations for j 6= i, and all other equations will follow from these. This fact\nwas previuosly noted in [2], but without a proof. This is a proposition 4 of sect. 4.1, and it is proven\nthere.\nThis is one more method to show linear decomposability of characteristic polynomial (38); one again\nhas to solve linear on \u03bb equations. For example, for (50) the system of equations (60) reduces to single\nequation:\n3\n12\n11\n2\n22\n12\n2\n22\n3\nA11\n2 (x1 ) + (2A2 \u2212 A1 )x2 (x1 ) + (A2 \u2212 2A1 )(x2 ) x1 \u2212 A1 (x2 ) = 0\n\n(61)\n\n3.7.1 Unit maps\nUsing the system of equations (60), one can establish the condition of each vector of the space to\nbe an eigenvector of the map. This condition is condition of vanishing of all the coe\u001ecients in the\nequations (60). The map, for which each vector of the space is an eigenvector, is called unit map. Any\nunit map can be represented as Ai (z) = \u03bc(z)zi , where \u03bc(z) is a homogeneous polynomial of degree\ns \u2212 1. Characteristic polynomial for unit maps is identically zero, R{Ai (z) \u2212 \u03bb(z)z i } \u2261 0. Substituting\nAi (z) = \u03bc(z)zi , one obtains: R{Ai (z) \u2212 \u03bb(z)z i } = R{\u03bc(z)zi \u2212 \u03bb(z)z i } = R{(\u03bc(z) \u2212 \u03bb(z))zi } =\nR{\u03bb0 (z)zi }. The system of equations \u03bb0 (z)zi = 0 always has non-trivial solution since \u03bb0 (z) = 0 always\nhas non-trivial solution, because it is one algebraic equation on n variables. So, R{\u03bb0 (z)zi } = 0 and\nR{Ai (z) \u2212 \u03bb(z)z i } \u2261 0. Unit map maps each vector of the space proportional by itself. This is a\ngeneralization of the notion of unit map in non-linear algebra. For example, the condition of the map\n(51) to be unit is:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\nA11\n2\n12\n2A2 \u2212 A11\n1\n12\nA22\n2 \u2212 2A1\nA22\n1\n\n=0\n=0\n=0\n=0\n\n(62)\n\nThese conditions are just the conditions of vanishing of coe\u001ecients in (61).\n\n4 Polynomial di\u001berential equations\n4.1\n\nEigenvectors as stationary points\n\nAs it was mentioned in introduction, when one considers the stability of the stationary point of a\nsystem of di\u001berential equations, one encounters such systems of equations:\n\u1e8bi = Aii1 i2 ***is xi1 xi2 xi3 * * * xis\n\n(63)\n\nIt is well known, that if the degree of the map equals 1, the solution of this equations is expressed\nthrough eigenvectors and eigenvalues of linear map Aji . In the non-linear case, eigenvectors also play\nan important role in solving these equations, for example, they entirely determine the phase diagram\nof the system. We begin by rewriting these equations in non-homogeneous variables. For doing this,\nmake the change of \"time\" variable dt0 = \u03bc(x)dt, where \u03bc(x) is a homogeneous polynomial of degree\ns \u2212 1. Now the system is:\n\u2202xi\nAi (x)\n=\n0\n\u2202t\n\u03bc(x)\n\n18\n\n(64)\n\n\fBoth the sides of equation (64) are homogeneous expressions on x with the same degree of homogeneity,\ni. e. if x is scaled x0 = kx, both the sides of equation will be scaled equally2 . Denoting \u03bei \u2261 xx1i :\n\u1e8bi x1 \u2212 \u1e8b1 xi\nAi (x)x1 \u2212 A1 (x)xi\nAi (\u03be) \u2212 A1 (\u03be)\u03bei\n\u2202\u03bei\n=\n=\n=\n0\n2\n2\n\u2202t\n(x1 )\n\u03bc(x)(x1 )\n\u03bc(\u03be)\n\n(65)\n\nThe equalities \u03be1 \u2261 1 and \u03be \u03071 \u2261 0 were used. In s.3.7 is proven a statement, that a vector xi satis\u001ces:\n(66)\n\nAi (x)xj \u2212 xi Aj (x) = 0\n\ni\u001b it is an eigenvector of A. Stationary points of (65) obey the equations:\n(67)\n\nAi (x)x1 \u2212 A1 (x)xi = 0\n\nThese equations are the equations (66) with \u001cxed j = 1.\n\nProposition 4. All\n\nn(n\u22121)\n2\n\nequations (66) follow from n \u2212 1 equations (66) with some \u001cxed j .\n\nThe system of equations (67) is the system (66) with \u001cxed j = 1. For any other j the proof is\nsimilar. Let x obey (67). One\ncan consider the solution of the system\nat\u0011 t = 0 coincides\n\u0010 \u0011\n\u0010 \u0011 (64), which\n\u0010\nxi\nxi\nxi xj\n\u2202\n\u2202\n\u2202\nwith x. Then for any i \u2202t0 x1 = 0, see (65). So for any i,j: \u2202t0 xj = \u2202t0 x1 / x1 = 0, and then,\nlooking again at (65), Ai (x)xj \u2212 xi Aj (x) = 0. From (65) it is easily seen that unit map does not move\ni\npoints in projective space at all, because for unit map Ai (x)xj \u2212 xi Aj (x) = 0 and therefore \u2202\u03be\n\u2202t0 = 0.\ns\u22121\nThe system (65) can be simpli\u001ced by choosing \u03bc(x) = (x1 ) . Then \u03bc(\u03be) \u2261 1 and the system becomes:\n\u2202\u03bei\n= Ai (\u03be) \u2212 A1 (\u03be)\u03bei\n\u2202t0\n\n4.1.1\n\nn = 2:\n\n(68)\n\nthe equations are reduced to quadratures\n\nIn the case n = 2, there is only one non-homogeneous variable: \u03be \u2261\n\u2202\u03be\n= A2 (\u03be) \u2212 A1 (\u03be)\u03be = P (\u03be),\n\u2202t0\n\nx2\nx1 .\n\nThe system (68) becomes:\n(69)\n\nwhere P (\u03be) is some polynomial of \u03be of degree s + 1 (or lower, if some coe\u001ecients of A vanish). This\nequation is then solved by separation of variables:\nZ\n\nd\u03be\n= t0 + C\nP (\u03be)\n\n(70)\n\nWhen \u03be(t0 ) is known, one can \u001cnd x1 (t0 ) by solving:\n\u2202x1\nA1 (x)\n(x1 )s A1 (\u03be)\n=\n=\n= x1 A1 (\u03be)\n\u2202t0\n\u03bc(x)\n(x1 )s\u22121\nZ\nZ\ndx1\ndt0\n=\nx1\nA1 (\u03be(t0 ))\n\n(71)\n(72)\n\nx1 (t0 ) can be found from this formula. Then:\ndt0 = (x1 (t0 ))s\u22121 dt\nZ\ndt0\nt+C =\n(x1 (t0 ))s\u22121\n\nFrom these integrals t0 (t) can be found, and, recalling that x2 = \u03bex1 , one can return to initial variables\nt, x1 (t), x2 (t). This case can be used to test some statement and predictions of general theory.\n2\n\nThis explains the choose of degree of \u03bc\n\n19\n\n\f4.2\n\nThe \"eigenvector\" solution\n\nAs it was shown in previous subsection, if xi (t) is a solution of\n\u1e8bi = Aii1 i2 ***is xi1 xi2 xi3 * * * xis\n\n(73)\n\nwith initial condition x(0)\ni , proportional to an eigenvector ei of A, then xi (t) evolves so, that\nxi (t)\n= const\nx1 (t)\n\n(74)\n\nThis means, that the solution has a form xi (t) = g(t)x(0)\n= f (t)ei . If ei is zero eigenvector, then\ni\n(0)\nxi = xi and does not change with time. If ei is unitary eigenvector, then, substituting xi (t) = f (t)ei\nin (73), one obtains the equation on f (t) with initial condition. In linear and non-linear case the\nequations are di\u001berent. In linear case:\ndf (t)\n= \u03bbi f (t)\ndt\nf (t)t=0 = f0\n\nHere \u03bbi is eigenvalue, which corresponds to this eigenvector. Solution of this equation:\nf (t) = f0 e\u03bbi t\n\nIn non-linear case:\ndf (t)\n= f (t)s\ndt\nf (t)t=0 = f0\n\n(75)\n(76)\n\nSolution:\nf (t) = \u0010\n\n1\n\n1\nf0s\u22121\n\nf0\n1\n\u0011( 1 ) =\ns\u22121\n(1 \u2212 (s \u2212 1)(f0 )s\u22121 t)( s\u22121 )\n\u2212 (s \u2212 1)t\n\n(77)\n\nFrom this formula it is seen, that in non-linear case each solution has a singularity at the moment of\n1\ntime t0 = (s\u22121)f\ns\u22121 , i. e. the solution grows in\u001cnitely during a \u001cnite time. Any stationary point, where\n0\nthe linear term vanishes (see 1.4.1), is unstable in its complex vicinity. To show this it is su\u001ecient to\ntake as initial condition such a vector, that is proportional to unitary eigenvector and f0 > 0. The\nvector of initial condition can be made arbitrary small. Then the solution with this initial condition\nwill go at in\u001cnity at some \u001cnite moment of time. If the map, corresponding to an equation, has a real\neigenvector, then this point is unstable in mentioned above sense in its real vicinity.\n\n4.3\n\nNon-linear condition of instability\n\nAny stable point, in which linear term equals zero (see sect.1.4.1) and main non-linear term has unitary\neigenvector, is unstable in its complex vicinity. To show this, it is su\u001ecient to take as initial condition\na vector, which is proportional to a unitary eigenvector and f0 > 0. This vector can be done arbitrarily\nsmall. Then the solution will go at in\u001cnity in a some \u001cnite time. If the map has real unitary eigenvector,\nthis point is unstable in its real vicinity. The question of stability of the point of equilibrium of the\nmap without unitary eigenvectors is open. This case is encountered rarely. The map should be very\ndegenerate, if it has no unitary eigenvectors. For example, the map\n\u0012\n\nx1\nx2\n\n\u0013\n\n\u0012\n\u2192\n\n0\nax1 x2\n\n\u0013\n\nhas no unitary eigenvectors, and two zero eigenvectors (0, 1) and (1, 0). This is the result of solving\nof equations (40) and (39). This map has only one non-zero component of six possible. So, here is a\nbrief summary:\n20\n\n\f1. If a map has an unitary eigenvector, the origin is unstable.\n2. If a map has non-zero resultant, the origin is unstable\n3. If a map has no unitary eigenvectors, one cannot make any de\u001cnite prediction about stability/instability of the origin.\nIf the resultant of the map does not equal to 0, the map cannot have zero eigenvectors. So, it has\nunitary eigenvectors and one comes to item 1.\n\n4.4\n\nThe example of equations with vanishing linear term\n\nConsider the system of di\u001berential equations of chemical kinetics of this system of reactions and\nreagents:\nC +D \u2192A+B\nC +A\u2194B+D+D\nA, B, C, D represent the reagents. X1 , X2 , X3 , X4 are, respectively, partial concentrations of A, B, C, D.\n\nThe condition of mass conservation in this system looks like this3 :\n\n(78)\n\nX1 + X2 + X3 + X4 = 1\n\nThe equations of chemical kinetics for this system:\n\uf8f1\n\u1e8a1\n\uf8f4\n\uf8f4\n\uf8f2\n\u1e8a2\n\uf8f4\n\u1e8a\n\uf8f4\n\uf8f3 3\n\u1e8a4\n\n= K34 X3 X4 \u2212 K31 X1 X3 + K24 X2 (X4 )2\n= K34 X3 X4 + K31 X1 X3 \u2212 K24 X2 (X4 )2\n= \u2212K34 X3 X4 \u2212 K31 X1 X3 + K24 X2 (X4 )2\n= \u2212K34 X3 X4 + 2K31 X1 X3 \u2212 2K24 X2 (X4 )2 ,\n\n(79)\n\nwhere dot denotes time derivative, K34 , K31 , K24 are rate coe\u001ecients of the reactions. Actually, they\ndepend on the temperature and pressure. If these dependencies are neglected, the equations of type\n(63) are obtained. From (78) X4 can be expressed through X1 , X2 , X3 , and there will be only three\nvariables X1 , X2 , X3 . One of stationary points of this system is:\n\uf8f1 (0)\n\uf8f4\n\uf8f2 X1 = 0\n(0)\nX2 = 1\n\uf8f4\n\uf8f3 (0)\nX3 = 0\n\n(80)\n\nDenote now \u03b41 = X1 \u2212 X1(0) , \u03b42 = X2 \u2212 X2(0) , \u03b43 = X3 \u2212 X3(0) . The linear term in the expansion of\nright-hand sides of (79) vanishes near the point (80). In the new variables, \u03b41 , \u03b42 , \u03b43 , the system (79)\nis:\n\u03b4\u03071 = K24 \u03b412 + K24 \u03b422 + (K24 \u2212 K34 )\u03b432 + 2K24 \u03b41 \u03b42 + (2K24 \u2212 K34 )\u03b42 \u03b43 + (2K24 \u2212 K34 \u2212 K31 )\u03b41 \u03b43 +\n+cubic terms\n\u03b4\u03072 =\n\n\u2212K24 \u03b412\n\n\u2212\n\nK24 \u03b422\n\n\u2212 (K24 +\n\nK34 )\u03b432\n\n\u2212 2K24 \u03b41 \u03b42 + (K31 \u2212 K34 \u2212 2K24 )\u03b43 \u03b41 \u2212 (2K24 + K34 )\u03b42 \u03b43 +\n+cubic terms\n\n\u03b4\u03073 =\n\nK24 \u03b412\n\n(81)\n\n+ 2K24 \u03b41 \u03b42 + (K34 + 2K24 \u2212 K31 )\u03b43 \u03b41 +\n\nK24 \u03b422\n\n+ (2K24 + K34 )\u03b42 \u03b43 + (K24 +\n\nK34 )\u03b432\n\n(82)\n\n+\n\n+cubic terms\n\n(83)\n\nTo analyse the stability of stationary point (80), one should consider only the term in expansion of the\nlowest degree, in this case, of second degree. After neglecting cubical terms in (81)-(83), one obtains\nthe di\u001berential equation with homogeneous quadratic right-hand side, i. e. the equation of type (63).\n3\nThe mass conservation law is not always so simple. It depends on the particular substances. It can occur that there\nare several independent equations of mass conservation, for each element. Very simpli\u001ced approximation is considered\nhere.\n\n21\n\n\f4.4.1 Investigation of stability of this example\nTo know whether the origin of the system of equations (81-83) is stable or not, one should consider\nthe system of equations:\n\uf8f6\n\uf8eb\n\uf8f6\nK24 \u03b412 + K24 \u03b422 + (K24 \u2212 K34 )\u03b432 + 2K24 \u03b41 \u03b42 + (2K24 \u2212 K34 )\u03b42 \u03b43 + (2K24 \u2212 K34 \u2212 K31 )\u03b41 \u03b43\n\u03b41\n\uf8ed \u03b42 \uf8f8 \u2192\n7 \uf8ed \u2212K24 \u03b412 \u2212 K24 \u03b422 \u2212 (K24 + K34 )\u03b432 \u2212 2K24 \u03b41 \u03b42 + (K31 \u2212 K34 \u2212 2K24 )\u03b43 \u03b41 \u2212 (2K24 + K34 )\u03b42 \u03b43 \uf8f8 (84)\nK24 \u03b412 + 2K24 \u03b41 \u03b42 + (K34 + 2K24 \u2212 K31 )\u03b43 \u03b41 + K24 \u03b422 + (2K24 + K34 )\u03b42 \u03b43 + (K24 + K34 )\u03b432\n\u03b43\n\uf8eb\n\nAll unitary eigenvectors satisfy the equation:\n\u03b41 = K24 \u03b412 + K24 \u03b422 + (K24 \u2212 K34 )\u03b432 + 2K24 \u03b41 \u03b42 + (2K24 \u2212 K34 )\u03b42 \u03b43 +\n+(2K24 \u2212 K34 \u2212 K31 )\u03b41 \u03b43\n\u03b42 =\n\n\u2212K24 \u03b412\n\n\u2212\n\nK24 \u03b422\n\n\u2212 (K24 +\n\nK34 )\u03b432\n\n\u2212 2K24 \u03b41 \u03b42 + (K31 \u2212 K34 \u2212 2K24 )\u03b43 \u03b41 \u2212\n(2K24 + K34 )\u03b42 \u03b43\n\n\u03b43 =\n\nK24 \u03b412\n\n+ 2K24 \u03b41 \u03b42 + (K34 + 2K24 \u2212 K31 )\u03b43 \u03b41 +\n\nK24 \u03b422\n\n+ (2K24 + K34 )\u03b42 \u03b43 +\n+(K24 + K34 )\u03b432\n\n(85)\n\nOne can notice, that f2 + f3 = 0, so the resultant of this map equals 0. So one cannot use the second\nitem from sect.4.3. f2 + f3 = 0, and therefore, \u03b43 = \u2212\u03b42 . Substitute it in \u001crst two equations. Then\none can exclude \u03b41 from the equations and obtain the equation with single variable \u03b42 :\n2\n2(K34\n+ K31 K34 )\u03b422 + (K24 \u2212 3K34 \u2212 K31 )\u03b42 + 1 = 0\n\n(86)\n\nThis equation always has roots, so the point (0, 1, 0) is unstable in its complex vicinity, since there are\n2 + K 1K 4) > 0,\ncomplex unitary eigenvectors. It has real roots, when (K24 \u2212 3K34 \u2212 K31 )2 \u2212 8(K34\n3\n3\nand then it is unstable in its real vicinity.\n\n5 Example of \u001cnding degeneracies and peculiarities of non-linear map\nIn this section an example of quadratic map of two variables in canonical form is considered. Nevertheless, all results, formulated in sections 5.1.1, 5.1.2, 5.1.3, can be obtained for any non-linear map in\nany coordinates. The only di\u001berence (between this example and general situation) is that the developed tools are su\u001ecient to fully investigate and classify this example, but in general situation higher\nresultants, higher complanarts, etc can be needed. Only the case when two unitary eigenvectors are\nchosen to be basis vectors is considered.\nOur example is:\nx1 \u2192 x21 + 2ax1 x2\nx2 \u2192 x22 + 2bx1 x2\n\n(87)\n\na and b are the parameters, de\u001cning a map instead of eigenvalues (see 3.5).\n\n5.1\n\nGeneral analysis\n\nBy construction of canonical form of the map, (87) has two unitary eigenvectors, namely e(1) \u2261 (1, 0)\nand e(2) \u2261 (0, 1). They are unitary eigenvectors under any values of a and b. In general case this map\nn \u22121\nhas cn|s = ss\u22121\n= 3 (s = 2, n = 2) eigenvectors. Here are the equations (39,40,44) in this case.\n\u001a\n\nx21 + 2ax1 x2 = x1\nx22 + 2bx1 x2 = x2\n\n22\n\n(88)\n\n\fThis is a system (39) in this case. One should solve this system of equations to \u001cnd unitary eigenvectors.\n\u001a\n\nx21 + 2ax1 x2 = 0\nx22 + 2bx1 x2 = 0\n\n(89)\n\nThis is a system (40) in this case. One should solve this system of equations to \u001cnd zero eigenvectors.\n\u001a\n\nx21 + 2ax1 x2 = x1 y\nx22 + 2bx1 x2 = x2 y,\n\n(90)\n\nThis is a system (44) in this case, and all eigenvectors (zero, non-zero unitary) are its solutions.\nAdditional homogenizing variable is also called y here, as in (44). A solution of (90), (x1 , x2 , y) with\ny = 0 corresponds to zero eigenvector. This zero eigenvector is (x1 , x2 ). A solution of (90), (x1 , x2 , y)\nwith y 6= 0 corresponds to non-zero eigenvector. This non-zero eigenvector is (x1 , x2 ). Since the system\n(90) is a system of homogeneous equations, the solutions of this system can be scaled by any non-zero\nnumber. To get unitary eigenvector from a non-zero eigenvector, one should rescale a solution of (90)\nwith y 6= 0 by dividing it by y : then a solution of (90) becomes (x1 /y, x2 /y, 1) and unitary eigenvector\nis (x1 /y, x2 /y).\n\n5.1.1 Preliminary information about eigenvectors from complanart\n(90) is a system of two homogeneous equations of three variables. Such system possesses a complanart.\nComplanart of this system is calculated in sect 2.3.5, and it equals\nC = (1 \u2212 2a)4 (1 \u2212 2b)4\n\nWhen it vanishes, the system (90) has three complanar in the space x1 , x2 , y roots. A system (90)\nis a system of two quadratic equations, so in general case it has 2 * 2 = 4 solutions. Two solutions\nof (90) correspond to known unitary eigenvectors, e(1) \u2261 (1, 0) and e(2) \u2261 (0, 1). These solutions\nof (90) are \u039b(1) \u2261 (1, 0, 1) and \u039b(2) \u2261 (0, 1, 1). There should be one more eigenvector in this case,\nsince the map (87) in general case has cn|s = c2|2 = 3 eigenvectors. Denote this eigenvector by\n(3) (3)\n(3) (3) (3)\ne(3) \u2261 (e1 , e2 ). Then the solution of (90), corresponding to this eigenvector, is \u039b(3) \u2261 (e1 , e2 , e3 ).\n(3) is non-zero eigenvector, e(3) 6= 0, and e(3) = 1 if e(3) is\nIf e(3) is zero eigenvector, e(3)\n3 = 0, if e\n3\n3\nunitary eigenvector. And the last solution of (90) is \u039b(4) \u2261 (0, 0, 1), this is a solution (47) for this\n1 ) (j2 ) (j3 )\nsystem. Vanishing of complanart means that \u03b5i1 i2 i3 \u039b(j\ni1 \u039bi2 \u039bi3 = 0 with at least one triple of indices\nj1 , j2 , j3 , j1 6= j2 , j1 6= j3 , j2 6= j3 . Particularly, vanishing of complanart means vanishing of at least\none of the following expressions:\n(1)\n\n(2)\n\n(3)\n\n(91)\n\n\u03b5i 1 i 2 i 3 \u039b i 1 \u039b i 2 \u039b i 3 = 1\n\n(92)\n\n(3)\n\n(3)\n\n(3)\n\n\u03b5i1 i2 i3 \u039bi1 \u039bi2 \u039bi3 = e1 \u2212 e2 \u2212 e3\n(1)\n\n(2)\n\n(4)\n\n(2)\n\n(3)\n\n(4)\n\n(3)\n\n(93)\n\n(1)\n\n(3)\n\n(4)\n\n(3)\n\n(94)\n\n\u03b5i1 i2 i3 \u039bi1 \u039bi2 \u039bi3 = e1\n\u03b5i1 i2 i3 \u039bi1 \u039bi2 \u039bi3 = e2\n\nThe solution \u039b(4) enters in (92),(93), (94), so these equations are nothing but conditions of pairs of\neigenvectors to be complanar, see discussion in sect 3.6. The equation (92) controls complanarity of\ne(1) and e(2) , (93) - of e(2) and e(3) , and (94) - of e(1) and e(3) . The condition (91) has no such a simple\ninterpretation. So, if a = 1/2 or b = 1/2, complanart vanishes, and one expects that either e(2) and\ne(3) , or e(1) and e(3) will be collinear, or the condition (91) will hold.\n\n5.1.2 Preliminary information about eigenvectors from resultant\nResultant of the map (87) equals:\nR{A} = 1 \u2212 4ab\n\n23\n\n\fAs it was stated in sect 3.4, zero eigenvectors exist i\u001b the resultant of the map equals 0. So one expects,\nthat if 1\u22124ab = 0, there will be zero eigenvectors, and if 1\u22124ab 6= 0, there will be no zero eigenvectors,\nand will be only non-zero eigenvectors.\n\n5.1.3 Preliminary information about eigenvectors from the equation xi Aj \u2212 xj Ai\nAs it was stated in sect 3.7.1, if polynomials xi Aj (x) \u2212 xj Ai (x) are identically zero, the map A is unit\nmap. For the map (87), there is only one equation:\nx1 A2 (x) \u2212 x2 A1 (x) \u2261 x1 (x22 + 2bx1 x2 ) \u2212 x2 (x21 + 2ax1 x2 ) \u2261 x1 x22 (1 \u2212 2a) \u2212 x21 y(1 \u2212 2b) = 0\n\n(95)\n\nSo, if a = 1/2 and b = 1/2, the map A will be unit map, every vector of the space will be eigenvector\nof A, every polynomial of degree s \u2212 1 \u03bc(x) will be an eigenvalue of the map A, and characteristic\npolynomial of A will be identically zero.\n\n5.2\n\nEigenvectors\n\nBesides two solutions (x1 , x2 , y) = (0, 1, 1) and (x1 , x2 ) = (1, 0, 1), the system (90) has the following\nsolution:\n\uf8eb\n\n\uf8f6 \uf8eb\n\uf8f6\nx1\n1 \u2212 2a\n\uf8ed x2 \uf8f8 = \uf8ed 1 \u2212 2b \uf8f8\ny\n1 \u2212 4ab\n\n(96)\n\nIf y value is equal to zero, (96) corresponds to zero eigenvector:\nv\n\n\u0012\n\n(1)\n\n\u2261\n\n1 \u2212 2a\n1 \u2212 2b\n\n\u0013\n\n(97)\n\n(of course, the normalization is arbitrary). In this case e(3)\n3 = 0.\nIf y value is not equal to zero, (96) corresponds to non-zero eigenvector. In this case, the solution\nvector can be renormalized:\n\uf8eb\n\uf8ed\n\n1\u22122a\n1\u22124ab\n1\u22122b\n1\u22124ab\n\n\uf8f6\n\n(98)\n\n\uf8f8\n\n1\n\nto get unitary eigenvector:\n(3)\n\ne\n\n\u0012\n\u2261\n\n1\u22122a\n1\u22124ab\n1\u22122b\n1\u22124ab\n\n\u0013\n\n(99)\n\nIn this case e(3)\n3 = 1. In agreement with predictions of sect 5.1.2, zero eigenvector exists i\u001b 1 \u2212 4ab = 0,\nor when the resultant vanishes. In agreement with predictions of sect 5.1.1, if a = 1/2 or b = 1/2, there\nare two coinciding eigenvectors. If a = 1/2, (99) coincides with e(2) , and if b = 1/2, (99) coincides\nwith e(1) . The prediction of sect 5.1.3, namely, the case a = 1/2 and b = 1/2 requires separate\nconsideration. All the components of (96) are now zero. The equation on eigenvectors/eigenvalues\nwith a = 1/2, b = 1/2:\n\u001a\n\nx21 + x1 x2 = x1 y\nx22 + x1 x2 = x2 y\n\n(100)\n\nThe case of unit map is the case of decreasing range of the system (90). Unfortunately, there have\nbeen developed no clear methods for determining whether the system of non-linear equations is degenerate. Complanart is an attempt to full this gap, but it is easily seen from this example that it does\nnot distinguish the cases when two vectors coincide and the decreasing of range of a system. Only\n24\n\n\feigenvectors, which are not equal to known (0, 1) and (1, 0), are interested in now. So, \u001crst equation\nof (100) can be divided by x1 and the second one can be divided by x2 .\n\u001a\n\nx1 + x2 = y\nx1 + x2 = y\n\nThe equation x1 + x2 = y has projective one-dimensional space of solutions: one can set y = 1 (to\nobtain unitary eigenvectors), x2 = C and x1 will be 1 \u2212 C . For any C the vector\n\u0012\n\nC\n1\u2212C\n\n\u0013\n\n(101)\n\nis unitary eigenvector. But there is also zero eigenvector, (x1 , x2 , y) = (\u22121, 1, 0). Arbitrary vector of\nthe space (x1 , x2 ) is zero eigenvector, if x1 + x2 = 0, and (x1 , x2 ) is non-zero eigenvector, if x1 + x2 6= 0.\n\n5.3\n\nEigenvalues\n\nPolynomial \u03bb, corresponding to unitary eigenvector e(i) is found from equation \u03bb(e(i) ) = 1, see sect\n3.2. This equation for e(1) is \u03bb1 = 1, i. e. to the eigenvector e(1) corresponds the line \u03bb1 = 1 on the\nplane \u03bb1 , \u03bb2 . And for e(2) : \u03bb2 = 1, i. e. to the eigenvector e(2) corresponds the line \u03bb2 = 1 on the\n1\u22122a\n1\u22122b\nplane \u03bb1 , \u03bb2 . For e(3) : \u03bb1 1\u22124ab\n+ 1\u22124ab\n\u03bb2 = 1, or\n\u03bb1 (1 \u2212 2a) + \u03bb2 (1 \u2212 2b) = 1 \u2212 4ab\n\n(102)\n\nThis is also a line at \u03bb1 , \u03bb2 plane. To \u001cnd \u03bb, corresponding to zero eigenvector, one should solve\n\u03bb(v (i) ) = 0, i. e. in this case \u03bb1 (1 \u2212 2a) + \u03bb2 (1 \u2212 2b) = 0. But (102) is equivalent to this equation,\nbecause the resultant, namely 1 \u2212 4ab, turns to zero if there is a zero eigenvector. It is not a random\ncoincidence, namely the term \u03bb1 (1 \u2212 2a) + \u03bb2 (1 \u2212 2b) \u2212 (1 \u2212 4ab) is a factor in the decomposition of\ncharacteristic polynomial. It is easily seen, that in the case of a = 1/2 and b = 1/2 (or when our map\nis unit map), any \u03bb is eigenvalue.\n\n5.4\n\nCharacteristic polynomial\n\nThe system of equations (37) for the map (87):\n\u001a\n\nx21 + 2ax1 x2 = (\u03bb1 x1 + \u03bb2 x2 )x1\nx22 + 2bx1 x2 = (\u03bb1 x1 + \u03bb2 x2 )x2\n\nCharacteristic polynomial is the resultant of the system:\n\u001a\n\nx21 + 2ax1 x2 \u2212 (\u03bb1 x1 + \u03bb2 x2 )x1 = 0\nx22 + 2bx1 x2 \u2212 (\u03bb1 x1 + \u03bb2 x2 )x2 = 0\n\nThe characteristic polynomial is equal:\nChA (\u03bb) = (1 \u2212 \u03bb1 )(1 \u2212 \u03bb2 )(1 \u2212 4ab \u2212 \u03bb1 (1 \u2212 2a) \u2212 \u03bb2 (1 \u2212 2b))\n\n(103)\n\nHere we would like to emphasize one more time: decomposability of characteristic polynomial is nontrivial property. The polynomial 1 + (\u03bb1 )2 + (\u03bb2 )2 , for example, cannot be decomposed on linear on\n\u03bb1 , \u03bb2 factors. If the resultant does not equal to 0:\nChA (\u03bb) = (1 \u2212 4ab)(1 \u2212 \u03bb(e(1) ))(1 \u2212 \u03bb(e(2) ))(1 \u2212 \u03bb(e(3) ))\n\nin full agreement with (46). If resultant of a map is equal to zero:\nChA (\u03bb) = (1 \u2212 \u03bb(e(1) ))(1 \u2212 \u03bb(e(2) ))\u03bb(v (1) )\n\nin full agreement with (43). If a = 1/2 and b = 1/2 (i. e. unit map), characteristic polynomial is\nidentically zero, in full agreement with s.3.7.1.\n25\n\n\f5.5\n\nPhase diagram\n\nDi\u001berential equation, which corresponds to the map (87) , is:\n\u001a\n\n5.5.1\n\nx \u03071 = x21 + 2ax1 x2\nx \u03072 = x22 + 2bx1 x2\n\n(104)\n\na 6= 1/2, b 6= 1/2, 1 \u2212 4ab 6= 0\n\nThis is the case of absence of any degeneracies and pecularities. There are three unitary eigenvectors,\n1\u22122a 1\u22122b\n(1, 0); (0, 1); ( 1\u22124ab\n, 1\u22124ab ). Fig.1 is the phase portrait of this system with a = \u22121, b = \u22121.\n\nFigure 1: The phase portrait with a = b = \u22121. There are no degeneracies and peculiarities, there are\nthree unitary eigenvectors:(0, 1), (1, 0), (3/5, 3/5)\n\n5.5.2\n\na 6= 1/2, b 6= 1/2, 1 \u2212 4ab = 0\n\nThe map in this case is degenerate, i. e. it has a zero eigenvector (1 \u2212 2a, 1 \u2212 2b) with full accordance\nx1\nx2\nwith sect 5.1.2. Each point, which lie on the line 1\u22122a\n= 1\u22122b\n(this is a line of zero eigenvectors), is a\nstationary point. Fig.2 is the phase portrait of this system with a = \u22121, b = \u22121/4.\n\n5.5.3\n\na 6= 1/2, b = 1/2\n\nBecause the complanart of this map equals 0, there is a double eigenvector of this map, with full\naccordance with sect 5.1.1. In this case the map has only two unitary eigenvectors, (0, 1) is \"simple\"\neigenvector (with multiplicity 1), and (1, 0) with multiplicity 2. The eigenvectors with multiplicity 2\nhave a special property: the phase trajectories tend in projective space to this eigenvector from one\nside, and they tend out of it in projective space at other side of this eigenvector. On \u001cg.3 at upper right\ncorner trajectories tend to eigenvector (1, 0) in projective space (namely, x1 \u2192 \u221e, x2 \u2192 \u221e, xx21 \u2192 0)\nand at lower right corner trajectories move out of eigenvector (1, 0) in projective space. The equation\non x2 /x1 \u2261 \u03b6 in the vicinity of 0 looks like\n\u03b6\u0307 = C\u03b6 2 ,\n\n26\n\n(105)\n\n\fFigure 2: The phase portrait with a = \u22121, b = \u22121/4. The map is degenerate, so it has zero eigenvector\ny\nx\n3 = 2 are stationary points.\n\n(3, 2). All the points of the line\n\nwhere C is a constant, depending on a and b. This projective equation explains such a behaviour: \u03b6\u0307\nhas the same signs at both sides from stationary point, so the solution approaches from one side and\nmove away from other side. Near a double eigenvectors, the linear term in projective equation always\nvanishes. Fig.3 is the phase portrait of the system with a = 1, b = 1/2. The case with a = 1/2, b 6= 1/2\nreduces to this case by substitution x1 \u2192 x2 , x2 \u2192 x1 .\n\n5.5.4\n\na = 1/2, b = 1/2\n\nAccording to predictions of 5.1.3, this map is unit map and any vector is an eigenvector. Fig.4 is the\nphase portrait in this case.\n\n5.5.5 Phase diagram without unitary eigenvectors\nThe example of map without unitary eigenvectors is considered here. There are no methods to determine whether the point which has not unitary eigenvectors is stable or not. The example is:\nx1 \u2192\n0\nx2 \u2192 bx1 x2\n\n(106)\n\nThe basis consists of two zero eigenvectors. Besides these vectors, there are no other eigenvectors. In\nthis particular case the point (0, 0) is unstable. Phase diagram is at the Fig.5.\n\n6 Acknowledgements\nWe want to thank A. Morozov and Sh. Shakirov for useful discussions. This work is supported by\ngrant for support of scienti\u001cc schools NSh-3036.2008.2. and by grant of RFBR 09-02-00393.\n\n27\n\n\fFigure 3: The phase portrait with a = 1, b = 1/2. Because the complanart of the map equals 0, there\nis double eigenvector (1, 0). Trajectories at upper right corner tend to eigenvector (1, 0) in projective\nspace and at lower right corner trajectories move out of eigenvector (1, 0) in projective space. This is\na general property of double eigenvectors.\n\nFigure 4: The phase portrait with a = 1/2, b = 1/2. Unlike all previous cases, all the phase trajectories are straight lines. This is because this map is unit map, and all vectors of the space are\neigenvectors. Vectors, proportional to (\u22121, 1) vector, are zero eigenvectors, and all other vectors are\nnon-zero eigenvectors.\n\nReferences\n[1] I.Gelfand, M.Kapranov and A.Zelevinsky, Discriminants, Resultants and Multidimensional Determinants (1994) Birkhauser\n[2] V. Dolotin and A. Morozov, Introduction to Non-Linear Algebra, hep-th/0609022\n[3] I.Gelfand, Lectures on Linear Algebra (1948) Moscow\n28\n\n\fFigure 5: Phase diagram of (106). Oy axe is vertical line, which has no arrows. The origin is unstable\nin this case.\n[4] E. Artin Galois Theory, Dover Publications, 1998, ISBN 0-486-62342-4\n[5] Sh.Shakirov, The coincident root loci and higher discriminants of polynomials, Theor.Math.Phys.\n(2007), math/0609524\n[6] A. M. Lyapunov Stability of Motion, Academic Press, New-York and London, 1966\n[7] V.Dolotin, QFT's with Action of Degree 3 and Higher and Degeneracy of Tensors, hep-th/9706001\n[8] A. Morozov, Sh. Shakirov Introduction to Integral Discriminants, math-ph, 0903.2595\n[9] A.Cayley, On the Theory of Linear Transformations, Camb.Math.J. 4 (1845) 193-209;\n[10] V.Dolotin, On Discriminants of Polylinear Forms, alg-geom/9511010\n[11] V.Dolotin, On Invariant Theory, alg-geom/9512011\n[12] L. Castellani, P. Antonio, L. Sommovio Triality Invariance in the N=2 superstring, hep-th,\n0904.2512\nL. Borsten,D. Dahanayake, M. J. Du\u001b, H. Ebrahim, W. Rubens Black Holes, Qubits and\nOctonions, hep-th,0809.4685\nP. Levay, P. Vrana , Three fermions with six single particle states can be entangled in two\ninequivalent ways, quant-ph,0806.4076\nA.Miyake and M.Wadati, Multiparticle Entaglement and Hyperdeterminants, quant-ph/02121146;\nV.Co\u001bman, J.Kundu and W.Wooters, Distributed Entaglement, Phys.Rev. A61 (2000) 52306,\nquant-ph/9907047;\n[13] D. Manocha, Algebraic and Numeric Techniques for Modeling and Robotics, PhD thesis, Computer Science Division, Department of Electrical Engineering and Computer Science, University\nof California, Berkeley.\n29\n\n\f[14] A.Morozov, String Theory, What is It?, Sov.Phys.Uspekhi, 35 (1992) 671-714 (Usp.Fiz.Nauk 162\n83-176)\n[15] A. Anokhina, A. Morozov, Sh. Shakirov Resultant as Determinant of Koszul Complex , math-ph,\n0812.5013\n[16] A. Morozov, Sh. Shakirov Resultants and Contour Integrals, math.AG, 0804.4632\nA. Morozov, Sh. Shakirov Analogue of the indentity Log Det=Trace Log for resultants, math-ph,\n0804.4632\n\n30\n\n\f"}