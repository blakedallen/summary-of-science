{"id": "http://arxiv.org/abs/1007.2669v3", "guidislink": true, "updated": "2013-03-15T14:44:30Z", "updated_parsed": [2013, 3, 15, 14, 44, 30, 4, 74, 0], "published": "2010-07-15T22:30:06Z", "published_parsed": [2010, 7, 15, 22, 30, 6, 3, 196, 0], "title": "Mixing of the symmetric exclusion processes in terms of the\n  corresponding single-particle random walk", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.1837%2C1007.2314%2C1007.4454%2C1007.3325%2C1007.3173%2C1007.3237%2C1007.1488%2C1007.0507%2C1007.3437%2C1007.5470%2C1007.3852%2C1007.5308%2C1007.4753%2C1007.1814%2C1007.4942%2C1007.5329%2C1007.2453%2C1007.1075%2C1007.4459%2C1007.3783%2C1007.2010%2C1007.4060%2C1007.4958%2C1007.0160%2C1007.1496%2C1007.2475%2C1007.0210%2C1007.2669%2C1007.0357%2C1007.0178%2C1007.0931%2C1007.2179%2C1007.0590%2C1007.0787%2C1007.2082%2C1007.2979%2C1007.3547%2C1007.3591%2C1007.4078%2C1007.2322%2C1007.2423%2C1007.3465%2C1007.3250%2C1007.4804%2C1007.3065%2C1007.2484%2C1007.1543%2C1007.3420%2C1007.1656%2C1007.3440%2C1007.4052%2C1007.2650%2C1007.1839%2C1007.3510%2C1007.1469%2C1007.5116%2C1007.2146%2C1007.3857%2C1007.3043%2C1007.1969%2C1007.0934%2C1007.4230%2C1007.4626%2C1007.1321%2C1007.5316%2C1007.4004%2C1007.4095%2C1007.2049%2C1007.4885%2C1007.2441%2C1007.3389%2C1007.2486%2C1007.0386%2C1007.2209%2C1007.2992%2C1007.3427%2C1007.4826%2C1007.1483%2C1007.5304%2C1007.2497%2C1007.1128%2C1007.0453%2C1007.4926%2C1007.0925%2C1007.3529%2C1007.5381%2C1007.1277%2C1007.0629%2C1007.2969%2C1007.0990%2C1007.4134%2C1007.4468%2C1007.2637%2C1007.3009%2C1007.1010%2C1007.4164%2C1007.4455%2C1007.3796%2C1007.3984%2C1007.2860%2C1007.2030&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Mixing of the symmetric exclusion processes in terms of the\n  corresponding single-particle random walk"}, "summary": "We prove an upper bound for the $\\varepsilon$-mixing time of the symmetric\nexclusion process on any graph G, with any feasible number of particles. Our\nestimate is proportional to $\\mathsf{T}_{\\mathsf{RW}(G)}\\ln(|V|/\\varepsilon)$,\nwhere |V| is the number of vertices in G, and $\\mathsf{T}_{\\mathsf{RW}(G)}$ is\nthe 1/4-mixing time of the corresponding single-particle random walk. This\nbound implies new results for symmetric exclusion on expanders, percolation\nclusters, the giant component of the Erdos-Renyi random graph and Poisson point\nprocesses in $\\mathbb{R}^d$. Our technical tools include a variant of Morris's\nchameleon process.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.1837%2C1007.2314%2C1007.4454%2C1007.3325%2C1007.3173%2C1007.3237%2C1007.1488%2C1007.0507%2C1007.3437%2C1007.5470%2C1007.3852%2C1007.5308%2C1007.4753%2C1007.1814%2C1007.4942%2C1007.5329%2C1007.2453%2C1007.1075%2C1007.4459%2C1007.3783%2C1007.2010%2C1007.4060%2C1007.4958%2C1007.0160%2C1007.1496%2C1007.2475%2C1007.0210%2C1007.2669%2C1007.0357%2C1007.0178%2C1007.0931%2C1007.2179%2C1007.0590%2C1007.0787%2C1007.2082%2C1007.2979%2C1007.3547%2C1007.3591%2C1007.4078%2C1007.2322%2C1007.2423%2C1007.3465%2C1007.3250%2C1007.4804%2C1007.3065%2C1007.2484%2C1007.1543%2C1007.3420%2C1007.1656%2C1007.3440%2C1007.4052%2C1007.2650%2C1007.1839%2C1007.3510%2C1007.1469%2C1007.5116%2C1007.2146%2C1007.3857%2C1007.3043%2C1007.1969%2C1007.0934%2C1007.4230%2C1007.4626%2C1007.1321%2C1007.5316%2C1007.4004%2C1007.4095%2C1007.2049%2C1007.4885%2C1007.2441%2C1007.3389%2C1007.2486%2C1007.0386%2C1007.2209%2C1007.2992%2C1007.3427%2C1007.4826%2C1007.1483%2C1007.5304%2C1007.2497%2C1007.1128%2C1007.0453%2C1007.4926%2C1007.0925%2C1007.3529%2C1007.5381%2C1007.1277%2C1007.0629%2C1007.2969%2C1007.0990%2C1007.4134%2C1007.4468%2C1007.2637%2C1007.3009%2C1007.1010%2C1007.4164%2C1007.4455%2C1007.3796%2C1007.3984%2C1007.2860%2C1007.2030&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We prove an upper bound for the $\\varepsilon$-mixing time of the symmetric\nexclusion process on any graph G, with any feasible number of particles. Our\nestimate is proportional to $\\mathsf{T}_{\\mathsf{RW}(G)}\\ln(|V|/\\varepsilon)$,\nwhere |V| is the number of vertices in G, and $\\mathsf{T}_{\\mathsf{RW}(G)}$ is\nthe 1/4-mixing time of the corresponding single-particle random walk. This\nbound implies new results for symmetric exclusion on expanders, percolation\nclusters, the giant component of the Erdos-Renyi random graph and Poisson point\nprocesses in $\\mathbb{R}^d$. Our technical tools include a variant of Morris's\nchameleon process."}, "authors": ["Roberto Imbuzeiro Oliveira"], "author_detail": {"name": "Roberto Imbuzeiro Oliveira"}, "author": "Roberto Imbuzeiro Oliveira", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/11-AOP714", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1007.2669v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1007.2669v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/11-AOP714 the Annals of\n  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.PR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1007.2669v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1007.2669v3", "journal_reference": "Annals of Probability 2013, Vol. 41, No. 2, 871-913", "doi": "10.1214/11-AOP714", "fulltext": "arXiv:1007.2669v3 [math.PR] 15 Mar 2013\n\nThe Annals of Probability\n2013, Vol. 41, No. 2, 871\u2013913\nDOI: 10.1214/11-AOP714\nc Institute of Mathematical Statistics, 2013\n\nMIXING OF THE SYMMETRIC EXCLUSION PROCESSES IN\nTERMS OF THE CORRESPONDING SINGLE-PARTICLE\nRANDOM WALK\nBy Roberto Imbuzeiro Oliveira1\nIMPA\nWe prove an upper bound for the \u03b5-mixing time of the symmetric\nexclusion process on any graph G, with any feasible number of particles. Our estimate is proportional to TRW(G) ln(|V |/\u03b5), where |V |\nis the number of vertices in G, and TRW(G) is the 1/4-mixing time\nof the corresponding single-particle random walk. This bound implies new results for symmetric exclusion on expanders, percolation\nclusters, the giant component of the Erd\u00f6s\u2013R\u00e9nyi random graph and\nPoisson point processes in Rd . Our technical tools include a variant\nof Morris's chameleon process.\n\n1. Introduction. The symmetric exclusion process is a continuous-time\nMarkov chain defined on a weighted graph G = (V, E, {we }e\u2208E ), where V is\na set of vertices, E is a set of edges and to each e \u2208 E, we assign a positive\nweight we > 0. For k \u2264 |V |, k-particle symmetric exclusion on G has the\nfollowing informal description.\nInformal description of EX(k, G): Start with k indistinguishable particles\nplaced on distinct vertices of V . Each particle moves independently according to the symmetric transition rates given by the edge weights, except that\nmoves to occupied sites are suppressed.\nThis is one of the most basic and best studied processes in the literature on\ninteracting particle systems [15, 16]. Literally hundreds of papers have been\nwritten on this process, but most of these results apply only to restricted\nclasses of infinite graphs, such as the lattices Zd .\nExclusion processes over finite graphs have also been a testbed for the\nquantitative analysis of finite Markov chains. Coupling [1], comparison arguments [8], the martingale method for log-Sobolev inequalities [12, 23] and\nvariants of the evolving sets technology [19, 21] have been variously applied\nReceived September 2010; revised August 2011.\nSupported by a Bolsa de Produtividade em Pesquisa and a Pronex project on Probability from CNPq, Brazil.\nAMS 2000 subject classifications. Primary 60J27, 60K35; secondary 82C22.\nKey words and phrases. Symmetric exclusion, interchange process, mixing time.\n1\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Probability,\n2013, Vol. 41, No. 2, 871\u2013913. This reprint differs from the original in pagination\nand typographic detail.\n1\n\n\f2\n\nR. I. OLIVEIRA\n\nto this process. Sharp results are known for some special cases, such as the\ncomplete graph [12] and discrete tori (Z/LZ)d [19, 23].\nIn this paper we consider EX(k, G) over an arbitrary finite graph and\nbound its mixing time in terms of the corresponding single-particle random\nwalk, which we denote by RW(G). Our result is very general, but we will see\nthat it nearly matches previously known mixing results for EX(k, G) for very\nspecific G and also gives new results in many interesting classes of examples.\nWe will also argue that the kind of result presented here is of conceptual\ninterest.\n1.1. The main result, and why it is interesting. Recall that the \u03b5-mixingtime of an irreducible continuous-time Markov chain Q on a finite set S,\nwith transition probabilities {qt (s, s\u2032 )}s,s\u2032 \u2208S,t\u22650 , and stationary (equilibrium)\ndistribution \u03c0, is given by the formula\nn\no\n(1.1)\nTQ (\u03b5) \u2261 inf t \u2265 0 : max dTV (qt (s, *), \u03c0) \u2264 \u03b5 ,\ns\u2208S\n\nwhere dTV is the total-variation distance; cf. (2.2.1). The 1/4 mixing time\nTQ (1/4) will also be called the mixing time of Q. Our main result follows.\n\nTheorem 1.1 (Main result; proven in Section 1.4). There exists a universal constant C > 0 such for all \u03b5 \u2208 (0, 1/2), all connected weighted graphs\nG = (V, E, {we }e\u2208E ) with |V | \u2265 2 and all k \u2208 {1, . . . , |V | \u2212 1},\nTEX(k,G) (\u03b5) \u2264 C ln(|V |/\u03b5)TRW(G) (1/4).\n\nOur bound follows quite naturally if one assumes (heuristically) that the\nmixing time of EX(k, G) is not much larger than that of k independent\nrandom walks on G, a process we denote by RW(k, G) in what follows:\n[Heuristic assumption] TEX(k,G) (\u03b5) \u2264 C0 TRW(k,G) (\u03b5),\n\nC0 > 0 universal.\n\nThis assumption, if at all true, is well beyond the reach of present techniques. However, it is at least plausible, given that RW(k, G) and EX(k, G)\nare similar.\nIt can be shown that TRW(k,G) (\u03b5) and TRW(G) (\u03b5/k) are of the same order\nif \u03b5/k \u226a 1; thus our assumption is equivalent to\n\n[Heurisitic assumption]\n\nTEX(k,G) (\u03b5) \u2264 C1 TRW(G) (\u03b5/k),\n\nC1 > 0 universal.\n\nRecall the general inequality \"TRW(G) (\u03b4) \u2264 C2 ln(1/\u03b4)TRW(G) (1/4),\" with\nC2 > 0 universal, which is valid for any 0 < \u03b4 < 1/2 [1]. Applying this to\nour assumption, we obtain\n[Heuristic conclusion] TEX(k,G) (\u03b5) \u2264 C3 ln(k/\u03b5)TRW(G) (1/4),\n\nC3 > 0 universal.\n\n|c ,\n\nc > 0 a universal constant;\nTheorem 1.1 coincides with this for k > |V\nwhereas for other k it is a strictly weaker result.\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n3\n\nWe emphasize that what we just presented is not a rigorous proof of\nTheorem 1.1, since we offer no good grounds for our heuristic assumption.\nWhat is interesting is that the theorem does give an a posteriori justification for a weakened form of the assumption. We note that the bound\n\"TEX(k,G) (\u03b5) \u2264 CTRW(G) (1/4) ln(k/\u03b5)\" is tight up to constant factors for\nsome G (e.g., discrete tori (Z/LZ)d , d fixed [19]); therefore, in some sense\nTheorem 1.1 is quite close to the best that one might hope for.\nMany other complex Markov chains are built from simpler processes that\ninteract; examples appear in, for example, [1, 7, 18]. Given our main result, it\nseems reasonable that, at least in some cases, the mixing time of these complex processes may be bounded in terms of their constituent parts. Some of\nthe techniques we use to prove Theorem 1.1 are very specific to EX(k, G), but\nit may be that some of the same ideas will turn out to be useful in other cases.\n1.2. Connections with Aldous's conjecture. Another motivation for our\npaper is a conjecture of Aldous's for the interchange process, which was\nrecently proved in [6]. The interchange process on G with k \u2264 |V | particles\ncan be informally described as follows:\nInformal description of IP(k, G): Start with k distinct vertices of V labeled\n1, 2, . . . , k all remaining vertices (if any) are labelled \"empty.\" For each edge\ne, switch the labels of the endpoints of e at rate we .\nOne can obtain EX(k, G) from IP(k, G) by \"forgetting\" the labels of the k\nparticles. In particular, the contraction principle [1] implies that TEX(k,G) (\u03b5) \u2264\nTIP(k,G) (\u03b5) for all 1 \u2264 k \u2264 |V | \u2212 1 and all \u03b5 \u2208 (0, 1).\nAldous conjectured-and Caputo et al. recently proved [6] (see also [10])-\nthat IP(k, G) and EX(k, G) always have the same spectral gap as RW(G) [or\nRW(k, G)]. This is a remarkable result, but it does not say much about\nthe mixing times of these processes, since the bounds for TIP(k,G) (\u03b5) or\nTEX(k,G) (\u03b5) that can be obtained from the spectral gap are typically very\nloose.\nTheorem 1.1 gives tighter relations between these mixing times. In the\nproof of the theorem, we will show that the bound claimed for TEX(k,G) (\u03b5)\nin the theorem statement in fact holds for TIP(k,G) (\u03b5) whenever k \u2264 |V |/2.\nOur proofs can be adapted to show that\n\u2200\u03b1 \u2208 (0, 1), \u2203C\u03b1 > 0, \u2200G, \u2200k \u2264 \u03b1|V |:\nTIP(k,G) (\u03b5) \u2264 C\u03b1 TRW(G) (1/4) ln(|V |/\u03b5).\n\nThat is, one can get a bound similar to Theorem 1.1 also for the interchange\nprocess, as long as the fraction of empty sites is bounded away from 0.\nUnfortunately, this leaves out the most interesting case of IP(|V |, G), which\nis a random walk by random transpositions in the group of permutations\nof V . Fortunately, the restriction on k does not make a difference for the\nexclusion process.\n\n\f4\n\nR. I. OLIVEIRA\nTable 1\nBounds for TEX(k,G) (1/4) via Theorem 1.1 in examples where no previous bound was\navailable. We take d as a fixed parameter and assume k \u2248 |V |/2\n\nExample\n(Z/LZ)d with nearest-neighbor bonds [19]\nTypical largest percolation cluster in (Z/LZ)d [4, 22]\nTypical Poisson process, in [0, L]d [5] (case \u03b1 > d)\nBounded-degree expanders\nGiant component of Gn,c/n , c > 1 [11]\n\nBound for TEX(k,G) (1/4)\n|V |2/d ln |V |\n|V |2/d ln |V |\n|V |2/d ln |V |\nln2 |V |\nln3 |V |\n\n1.3. Applications and comparison with previous results. It is not hard\nto apply Theorem 1.1 to specific examples: all one needs is a bound for\nthe mixing time of simple random walk on the given graph, and RW(G) is\ntypically much easier to analyse than EX(k, G). The only example where we\nknow Theorem 1.1 gives a suboptimal bound is in the case G = (Z/LZ)d\nwith the usual bonds, where the optimal bound, obtained by Morris [19], is\nof the order L2 ln k whereas ours is about L2 ln L (both for d fixed). Notice\nthat this difference is only relevant for quite small k.\nTable 1 presents the bounds given by Theorem 1.1 in examples where\nno previous bound appears explictly in the literature. The references are to\npapers where the mixing times of the corresponding graphs are computed.\nWe consider only k \u2248 |V |/2 and omit constant factors.\nIn fairness, we note that a combination of canonical paths, log Sobolev\nconstants and comparison arguments could in principle be applied to examples. This method is discussed in Section A in the Appendix. However, we\nnote that:\n\u2022 To the best of our knowledge, no good canonical paths bounds have been\nworked out for the examples in Table 1, and it might be hard or impossible\nto do so;\n\u2022 Even if such bounds were obtained, there are natural lower bounds for\nhow good they can be (cf. Section A), and Theorem 1.1 is at least as good\nas these lower bounds, up to the constants (it is actually better by a ln |V |\nfactor in the case of expanders).\n1.4. Key steps of the proof. Our proof of Theorem 1.1 can be broken\ninto two main steps. We first show that IP(2, G) always has a mixing time\ncomparable to RW(G).2\n2\n\nSince the single-particle marginal distributions of IP(2, G) are given by RW(G),\nTRW(G) (1/4) \u2264 TIP(2,G) (1/4) is immediate from the contraction principle.\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\nLemma 1.1.\n\n5\n\nFor any weighted graph G,\nTIP(2,G) (1/4) \u2264 20,000TRW(G) (1/4).\n\nWe then bootstrap the first lemma to a larger number of particles.\nLemma 1.2 (Proven in Section 6.1). There exists a universal constant\nK > 0 such that for all connected weighted graphs G = (V, E, {we }e\u2208E ), all\n\u03b5 \u2208 (0, 1/2) and all k \u2208 {1, . . . , |V |/2},\nTIP(k,G) (\u03b5) \u2264 KTIP(2,G) (1/4) ln(|V |/\u03b5).\n\nBefore we continue, we show how Theorem 1.1 easily follows from the two\nlemmas.\nProof of Theorem 1.1.\n\nCombining Lemma 1.2 with Lemma 1.1 gives\n\nTIP(k,G) (\u03b5) \u2264 CTRW(G) (1/4) ln(|V |/\u03b5)\n\nif \u03b5 \u2208 (0, 1/2) and k \u2264 |V |/2\n\nwhere C = 20,000K. The contraction principle [1] implies\n\nTEX(k,G) (\u03b5) \u2264 TIP(k,G) (\u03b5) \u2264 CTRW(G) (1/4) ln(|V |/\u03b5)\n\nif \u03b5 \u2208 (0, 1/2) and k \u2264 |V |/2.\n\nHowever, EX(k, G) and EX(|V | \u2212 k, G) are the same process with the roles of\nempty and occupied sites reversed. In particular, TEX(k,G) (\u03b5) = TEX(|V |\u2212k,G)(\u03b5)\nfor all \u03b5. \u0003\nWe now give an overview of the main ideas involved in proving the two\nlemmas. The proof of Lemma 1.1 relies on realizing that there are two classes\nof graphs. Some G are \"easy,\" in that two independent random walkers are\nlikely to meet by time O(TRW(G) (1/4)) from any pair of initial states. In\nthis case, an argument of Aldous and Fill's [1] suffices to prove Lemma 1.1\n(see Proposition 4.4).\nOn the other hand, if G is not easy, then for most initial states two independent random walkers are very unlikely to meet by time \u03a9(TRW(G) (1/4));\ncf. Proposition 4.5. Intuitively, IP(2, G) and RW(2, G) are similar in the abscence of collisions, and we will use this to prove Lemma 1.1 over noneasy\ngraphs. The negative correlation property will be crucial for this part of the\nargument; see Remark 4.3 for details.\nThe proof of Lemma 1.2 is considerably more involved. We first note\nthat there are two methods in the literature for moving from mixing of\npairs of particles to many more particles, both of which were introduced by\nMorris [19, 20]. The first one [20] gives bounds for walks on the symmetric\ngroup by random transpositions. Unfortunately, the method seems to require\ntoo much from the process to be useful in our general setting. Moreover, the\nbounds given by that method would have a factor of ln(|V |) ln(1/\u03b5) where\nours has a ln(|V |/\u03b5) term.\n\n\f6\n\nR. I. OLIVEIRA\n\nMorris's other method was introduced in his study of symmetric exclusion\nover (Z \\ LZ)d [19]. The so-called chameleon process features particles that\nchange color in a way that encodes the conditional distribution of the kth\nparticle in IP(k, G) given the other k \u2212 1 particles. It is this method that we\nwill successfully adapt to prove Lemma 1.2.\nOne way to understand Morris's construction is that it reduces the analysis of mixing to the study of pairwise collisions between particles. The\nanalysis for (Z \\ LZ)d is greatly facilitated by the explicit structure of the\ngraph, something that we lack in general. This will require certain technical\nmodifications of Morris's construction, of which we will try to make sense\nwith remarks in our proofs.\n1.5. Organization. Section 2 reviews some preliminary material. Section 3 discusses RW(G), EX(k, G) and IP(k, G), presents their joint graphical construction and reviews the negative correlation property. Section 4\npresents the proof of Lemma 1.1. The chameleon process is introduced in\nSection 5. It is then used to prove Lemma 1.2 in Section 6, but several lemmas are postponed to Sections 7\u20139. It would be pointless to describe these\nsteps now, but Section 6.2 provides an outline of those sections. Finally,\nSection 10 presents some final remarks, and the Appendix contains some\ntechnical steps that are not particularly illuminating.\n2. Preliminaries.\n2.1. Basic notation. N = {0, 1, 2, 3, . . .} is the set of nonnegative integers\nand N+ \u2261 N \\ {0}. For n \u2208 N+ , [n] \u2261 {i \u2208 N+ : i \u2264 n} = {1, . . . , n}. If S is a\nfinite set, |S| is the cardinality of S. For any k \u2208 [|S|],\n\u0012 \u0013\nS\n= {A \u2282 S : |A| = k}\nk\nis the set of all size-k subsets of S, and\n(S)k = {s = (s(1), . . . , s(k)) \u2208 S k : \u2200i, j \u2208 [k], \"i 6= j\" \u21d2 \"s(i) 6= s(j)\"}\n\nis the set of all k-tuples of distinct elements in S.\n\nNotational convention 2.1. The elements of (S)k will always be denoted by boldface letters such as x, with x(i) denoting the ith coordinate of x.\nNotice that with these symbols,\n\u0012\n\u0013\n\u0012 \u0013\n|S|\nS\n=\n,\nk\nk\n\n|(S)k | = (|S|)k .\n\nA graph\nis a couple H = (V, E) where V 6= \u2205 is the set of vertices, and\n\u0001\nE \u2282 V2 is the set of edges. For each e \u2208 E, the two elements a, b \u2208 V such\nthat e = {a, b} are called the endpoints of e.\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n7\n\nA weighted graph is a triple G = (V, E, {we }e\u2208E ), where (V, E) is a graph,\nand we > 0, the weight of edge e, is positive for each e \u2208 E. When a graph\nG is introduced without explicitly defining the edge weights, we will assume\nthat they are all equal to 1. We will assume throughout this paper that all\ngraphs we consider are connected.\n2.2. Basic probabilistic concepts. L[X] denotes the law or distribution of\nthe random variable X.\nGiven two probability distributions \u03bc, \u03bd over the same finite set S, the\ntotal variation distance between them is given by several equivalent formulas:\n(2.2.1)\n(2.2.2)\n\ndTV (\u03bc, \u03bd) \u2261 max(\u03bc(A) \u2212 \u03bd(A))\nA\u2282S\nZ\nZ\n= sup\nf d\u03bc \u2212 f d\u03bd\nf :S\u2192[0,1]\n\n(2.2.3)\n\n=\n\nX\ns\u2208S\n\n(2.2.4)\n\n=\n\n(\u03bc(s) \u2212 \u03bd(s))+\n\n1X\n|\u03bc(s) \u2212 \u03bd(s)|.\n2\ns\u2208S\n\nAnother equivalent definition of dTV is\n\ndTV (\u03bc, \u03bd) = inf P(X 6= Y ),\n\nwhere the infimum is over all pairs (X, Y ) of S-valued random variables\nwith L[X] = \u03bc and L[Y ] = \u03bd [such a pair is called a coupling of (\u03bc, \u03bd)]. This\nimplies that for any pair of S-valued random variables X, Y defined over the\nsame probability space,\ndTV (L[X], L[Y ]) \u2264 P(X 6= Y ).\n\nWe will need the following simple fact: if (for i = 1, 2) \u03bci , \u03bdi are probability\ndistributions on the finite set Si ,\n(2.2.5)\n\ndTV (\u03bc1 \u00d7 \u03bc2 , \u03bd1 \u00d7 \u03bd2 ) \u2264 dTV (\u03bc1 , \u03bd1 ) + dTV (\u03bc2 , \u03bd2 ).\n\nWe will write Unif(S) for the uniform distribution on a set S 6= \u2205. This is\nthe normalized counting measure on S, if S is finite, or normalized Lebesgue\nmeasure over S, if S \u2282 Rd .\n2.3. Markov chains and mixing times. For our purposes it is convenient\nto define a continous-time Markov chain over a finite set S as a family of\nprocesses\n{(Xts )t\u22650 : s \u2208 S}\n\ndefined on the same probability space, with the following properties:\n(1) For each s \u2208 S, X0s = s almost surely.\n\n\f8\n\nR. I. OLIVEIRA\n\n(2) Each Xts is a \"c\u00e0dl\u00e0g\" path over S: there exists a divergent sequence\n\u03c40 = 0 < \u03c41 < \u03c42 < * * *\n\nand a sequence {si }i\u22650 \u2282 S with s0 = s with Xts \u2261 si over each interval\n[\u03c4i , \u03c4i+1 ).\n(3) For each h \u2265 0 and each c\u00e0dl\u00e0g path (xu )u\u22650 taking values in S [in the\nsense of (2)],\ns\nP(Xt+h\n= s\u2032 |Xts\u2032 = xt\u2032 , 0 \u2264 t\u2032 \u2264 t) = P(Xhxt = s\u2032 )\n\nalmost surely.\n\nThe last property is the so-called Markov property. It also implies that\nxt\ns )\nthe law of (Xt+h\nh\u22650 equals that of (Xh )h\u22650 under the above conditioning.\nIt is well known that any such process is uniquely defined by its transition\nrates,\nP(X\u03b5s = s\u2032 )\n\u03b5\u05810\n\u03b5\n\nq(s, s\u2032 ) \u2261 lim\n\nor equivalenty by its generator,\nQ : f \u2208 RS 7\u2192 Qf (*) \u2261\n\nX\n\ns\u2032 \u2208S,s\u2032 6=*\n\n[(s, s\u2032 ) \u2208 S 2 , s 6= s\u2032 ],\nq(*, s\u2032 )(f (s\u2032 ) \u2212 f (*)).\n\nWe will usually make no distinction between a Markov chain and its generator in our notation.\nIn this paper we will only work with irreducible chains, that is, chains\nfor which for all A \u2282 S with A 6= \u2205, S \\ A 6= \u2205, there exist a \u2208 A, b \u2208 S \\ A\nwith q(a, b) > 0. It is well known that such Markov chains have a unique\nstationary distribution \u03c0, that is, a distribution such that if s\u2217 is picked\naccording to \u03c0 independently from the (Xts )t\u22650,s\u2208S , then L[Xts\u2217 ] = \u03c0 for all\nt \u2265 0. Moreover,\n\u2200s \u2208 S\n\ndTV (L[Xts ], \u03c0) \u0581 0\n\nas t \u2192 +\u221e.\n\n(The symbol \"\u0581\" denotes monotone convergence.) The \u03b5-mixing time of Q\nis thus defined as in the Introduction,\nn\no\nTQ (\u03b5) \u2261 inf t \u2265 0 : max dTV (L[Xts ], \u03c0) \u2264 \u03b5\n[\u03b5 \u2208 (0, 1)].\ns\u2208S\n\nWe will often need two elementary facts about Markov chains and their\nmixing times.\n\nProposition 2.1 ([13], equation (4.36), page 55). Let Q be a Markov\nchain on finite state space S. Then for all 0 < \u03b5 < 1/2,3\nTQ (\u03b5) \u2264 \u2308log2 (1/\u03b5)\u2309TQ (1/4).\n3\n\nThe result in [13] is for discrete-time chains, but the proof trivially extends to continuous time.\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n9\n\nProposition 2.2 ([1], Lemma 7 in Chapter 4). Let Q be a Markov chain\non finite state space S with symmetric transition rates. Then \u03c0 is uniform\nover S and moreover, for all 0 < \u03b5 < 1/2 and t \u2265 2TQ (\u03b5),\nP(Xts = s\u2032 ) \u2265\n\n(1 \u2212 2\u03b5)2\n,\n|S|\n\nfor all s, s\u2032 \u2208 S, with the same notation introduced above.\nWe also make the following convenient notational convention.\nNotational convention 2.2. By definition, for any c\u00e0dlag path (xt )t\u22650\nthere exists a divergent sequence t0 = 0 < t1 < t2 < * * * with xt constant over\n[ti , ti+1 ) for each i \u2265 0. For t > 0, we define xt\u2212 to be the state of xt immediately prior to time t. That is,\n\u001a\nxti\u22121 ,\nif t = ti for some i \u2265 1;\nxt\u2212 \u2261\nxt ,\notherwise.\n\nNotice that xt\u2212 = xt\u2212\u03b4 for all \u03b4 > 0 sufficiently small.\n\n3. Random walks, exclusion and interchange processes. In this section\nwe formally define the main Markov chains in this paper: RW(G), EX(k, G)\nand IP(k, G). We also present the standard graphical construction for the\nthree processes at the same time, and then discuss the negative correlation\nproperty for EX(k, G). The material in this section is quite classical: Liggett's\nbooks [15, 16] are basic references, and the manuscript by Aldous and Fill [1]\ncontains some additional facts on IP(k, G) as well as a presentation, that is,\nsomewhat closer in style to ours.\n3.1. Definitions. The three processes we are defined in terms of the same\nweighted graph G = (V, E, {we }e\u2208E ) with V finite; cf. Section 2. We will be\nimplicitly assuming that G is connected, in which case one can easily show\nthat the chains defined below are irreducible. It will be useful to define the\ntranspositions\n\uf8f1\nif x = a,\n\uf8f2 b,\nfe : x \u2208 V 7\u2192 a,\nif x = b,\n\uf8f3\nx,\notherwise.\n\u0001\nWe also write fe (A) = {fe (a) : a \u2208 A} and fe (x) = (fe (x(i)))ki=1 for A \u2208 Vk\nand x \u2208 (V )k (resp.).\nSimple random walk on G, denoted by RW(G), is the continuous-time\nMarkov chain with state space V and transition rates\n\u001a\nwe ,\nif fe (u) = v;\n[(u, v) \u2208 (V )2 ].\nq(u, v) \u2261\n0,\notherwise\n\n\f10\n\nR. I. OLIVEIRA\n\nWe will also consider the process RW(k, G) that corresponds to k such random walks performed simultaneously and independently. Since the transition rates of these process are also symmetric, it follows that the stationary\ndistribution of RW(k, G) is Unif(V k ) for all k \u2208 N+ .\nThe k-particle symmetric exclusion process on G, denoted\nby EX(k, G), is\nV\u0001\nthe continuous-time Markov chain with state space k and transition rates\n\u0014\n\u0012\u0012 \u0013\u0013 \u0015\n\u001a\nV\nwe ,\nif fe (A) = B;\n{k}\n(A, B) \u2208\n.\nq (A, B) \u2261\nk\n0,\notherwise\n2\nThe transition rates\u0001are again symmetric, and the stationary distribution of\nEX(k, G) is Unif( Vk ).\nThe k-particle interchange process on G, denoted by IP(k, G), has state\nspace (V )k . The transition rates of IP(k, G) are given by\n\u001a\nwe ,\nif fe (x) = y;\n(k)\n[(x, y) \u2208 ((V )k )2 ].\nq (x, y) \u2261\n0,\notherwise\nThis process also has symmetric transition rates, and its stationary distribution is Unif((V )k ).\n3.2. The standard graphical construction. We now present the standard\ngraphical construction of these three processes. Graphical constructions are\nstandard tools in the study of interacting particle systems [15] and are usually attributed to Harris in the literature. The basic construction presented\nhere will be elaborated upon later in the paper; see Section 5. For brevity,\nwe omit all P\nproofs in this subsection.\nSet W = e\u2208E we . We need a marked Poisson process, that is, a pair of\nindependent ingredients given as follows:\n(1) A Poisson process P = {\u03c41 \u2264 \u03c42 \u2264 \u03c43 \u2264 * * *} \u2282 [0, +\u221e) with rate W .\n(2) An i.i.d. sequence of E-valued random variables (\"markings\") {en }n\u2208N ,\nwith\n\u2200n \u2208 N\n\nP(en = e) = we /W.\n\nLet 0 \u2264 t \u2264 s < +\u221e be given. We define a random permutation I(t,s] : V \u2192\nV associated with the time interval (t, s] as follows: if P \u2229 (t, s] = \u2205, I(t,s] is\nthe identity map on V . If, on the other hand,\nP \u2229 (t, s] \u2261 {\u03c4j : m \u2264 j \u2264 n} =\n6 \u2205,\nwe set I(t,s] = fen \u25e6 fen\u22121 \u25e6 * * * \u25e6 fem ; that is, I(t,s] is the composition of each\ntransposition fej corresponding to \u03c4j \u2208 (t, s], and the transpositions are\ncomposed in the order they appear. We also set It \u2261 I(0,t] for t > 0 and\nI(t,t] = identity map over V .\n\n\f11\n\nMIXING OF SYMMETRIC EXCLUSION\n\nRemark 3.1. Strictly speaking, we should worry about what happens if\nP \u2229 (t, s] is infinite, or (more generally) some finite interval (a, b] in [0, +\u221e)\nhas infinite intersection with P. However, since the probability of any of this\nholding is 0, we will simply ignore these issues.\nNotice the following simple properties:\nProposition 3.1 (Proof omitted).\nI(s,r] \u25e6 I(t,s] .\n\nFor all 0 \u2264 t \u2264 s \u2264 r, I(t,r] =\n\nProposition 3.2 (Proof omitted).\n\u22121\n].\nL[I(t,s] ] = L[I(t,s]\n\nFor all 0 \u2264 t \u2264 s < +\u221e,\n\nProposition 3.3 (Proof omitted). Let 0 \u2264 t0 < t1 < t2 < * * * < tk . Then\nthe maps I(ti\u22121 ,ti ] , 1 \u2264 i \u2264 k, are independent.\nNotational convention 3.1. We \"lift\" the random maps I(t,s] to per\u0001\nmutations of Vk and (V )k , which we also denote by I(t,s] :\n\u0014\n\u0012 \u0013\u0015\nV\nI(t,s] (A) \u2261 {I(t,s] (a) : a \u2208 A}\nA\u2208\n,\nk\nI(t,s] (x) \u2261 (I(t,s] (x(1)), I(t,s] (x(2)), . . . , I(t,s] (x(k)))\n\n[x \u2208 (V )k ].\n\nFor brevity, we will often write xIt , AIt , xIt instead of It (x), It (A), It (x) (resp.).\nThe key property of the graphical construction follows:\nProposition 3.4 (Proof omitted).\n\nLet t0 \u2265 0. Then:\n\n(1) For each x \u2208 V , the process {I(t0 ,t+t0 ] (x)}t\u22650 is a realization of RW(G)\nwith initial state\u0001 x.\n(2) For each A \u2208 Vk , the process {I(t0 ,t+t0 ] (A)}t\u22650 is a realization of EX(k, G)\nwith initial state A.\n(3) For each x \u2208 (V )k , the process {I(t0 ,t+t0 ] (x)}t\u22650 is a realization of IP(k, G)\nwith initial state x.\n3.3. The negative correlation property. EX(k, G) enjoys important negative correlation properties. In this paper we only need a very special result,\nwhich is contained in any of [3, 14, 15].\n\u0001\nLemma 3.1. Given A \u2208 Vk , let {AIt }t\u22650 be a realization of EX(k, G)\nstarting from A. Then for all u \u2208 (V )2 -that is, for all distinct u(1), u(2) \u2208\nV -we have\nP({u(1) \u2208 AIt } \u2229 {u(2) \u2208 AIt }) \u2264 P(u(1) \u2208 AIt )P(u(2) \u2208 AIt ).\n\n\f12\n\nR. I. OLIVEIRA\n\nUsing the construction in the previous section, we can write the above\ninequality as\nP({It\u22121 (u(1)) \u2208 A} \u2229 {It\u22121 (u(2)) \u2208 A}) \u2264 P(It\u22121 (u(1)) \u2208 A)P(It\u22121 (u(2)) \u2208 A).\n\nThe following is then immediate from Proposition 3.2.\n\nCorollary 3.1 (Proof omitted). Given u \u2208 (V )2 , let {uIt }t\u22650 be a realization of IP(2, G) starting from u. Then for all A \u2282 V ,\nP({uIt (1) \u2208 A} \u2229 {uIt (2) \u2208 A}) \u2264 P(uIt (1) \u2208 A)P(uIt (2) \u2208 A).\n\n4. The dynamics of pairs of particles. The goal of this section is to prove\nLemma 1.1. We fix a weighted graph G = (V, E, {we }e\u2208E ) for the remainder\nof the section (and of the paper). The definitions of RW(G), RW(k, G),\nEX(k, G) and IP(k, G) are all relative to this graph.\n4.1. Some facts on RW(2, G) and IP(2, G). Much of this section will involve comparisons between IP(2, G) and RW(2, G). The following notational\nconvention will be useful.\nR\nR\nNotational convention 4.1. Given x \u2208 V 2 , {xR\nt \u2261 (xt (1), xt (2)) :\nt \u2265 0} denotes a realization of RW(2, G) from initial state x. That is, the\nR\ntrajectories of xR\nt (1), xt (2) are independent realizations of RW(G) with respective initial states x(1), x(2).\n\nWe collect several simple facts about RW(2, G) and IP(2, G) that we will\nneed later on. The first one is obvious, for example, from the graphical\nconstruction.\nProposition 4.1 (Proof omitted).\n\nI\nFor i = 1, 2, L[xR\nt (i)] = L[xt (i)].\n\nThe next proposition is a direct consequence of (2.2.5).\nProposition 4.2 (Proof omitted).\nisfy TRW(2,G) (\u03b5) \u2264 TRW(G) (\u03b5/2).\nProposition 4.3.\nTRW(G) (1/4).\n\nThe mixing times of RW(2, G) sat-\n\nLet k \u2208 N be given. Then TRW(2,G) (2\u2212k ) \u2264 (k + 1) \u00d7\n\nProof. Follows from the previous proposition combined with Proposition 2.1. \u0003\nThe next lemma has the following meaning. Suppose t is so large that xR\nt\nis close to equilibrium. In this case, E[\u03c6(xR\nt )] is close to the uniform average\nof \u03c6 over V 2 , for all mappings 0 \u2264 \u03c6 \u2264 1. The lemma shows that E[\u03c6(xIt )]\ncannot be much larger than that average. This will require the negative\ncorrelation property; cf. Corollary 3.1.\n\n\f13\n\nMIXING OF SYMMETRIC EXCLUSION\n\nLemma 4.1.\n\nLet \u03c6 : V 2 \u2192 [0, 1]. Then\n\nX \u03c6(v)\n\u221a\n.\nE[\u03c6(xIt )] \u2264 8 \u03b5 + 9\n|V |2\n2\n\n\u2200\u03b5 \u2208 (0, 1/16), \u2200t \u2265 TRW(G) (\u03b5), \u2200x \u2208 (V )2\n\nv\u2208V\n\nProof. Define the \"good set\" of all a \u2208 V with nearly uniform probability\n\u001a\n\u221a \u001b\n2 \u03b5\n1\n\u2264\nGood \u2261 a \u2208 V : max P(xR\n(i)\n=\na)\n\u2212\n.\nt\ni=1,2\n|V |\n|V |\nWe will show toward the end of the proof that\n\u221a\n(4.1.1)\nP(xIt \u2208\n/ Good2 ) \u2264 8 \u03b5,\nwhich (since 0 \u2264 \u03c6 \u2264 1) implies\n\n\u221a\nE[\u03c6(xIt )I(V )2 \\Good2 (xIt )] \u2264 8 \u03b5.\n\n(4.1.2)\n\nOn the other hand, notice that\nX\nE[\u03c6(xIt )IGood2 (xIt )] =\n\nP(xIt = (a(1), a(2)))\u03c6(a)\n\na\u2208(Good2 )\u2229(V )2\n\n\u2264\n(Cor. 3.1) \u2264\n\nX\n\n2\n\\\n\nP\n\nX\n\n2\nY\n\nP({xIt (i) \u2208 {a(1), a(2)}})\u03c6(a)\n\nX\n\n2\nY\n\nP({xR\nt (i) \u2208 {a(1), a(2)}})\u03c6(a)\n\na\u2208(Good2 )\u2229(V )2 i=1\n\n(Prop. 4.1) =\n\na\u2208(Good2 )\u2229(V )2 i=1\n\n[a(i) \u2208 Good] \u2264\n\n{xIt (i) \u2208 {a(1), a(2)}} \u03c6(a)\n\ni=1\n\na\u2208(Good2 )\u2229(V )2\n\n!\n\nX\n\na\u2208(Good2 )\u2229(V )2\n\n\u0012\n\n\u221a \u0013\n2+4 \u03b5 2\n\u03c6(a)\n|V |\n\nX \u03c6(a)\n\u221a\n.\n( \u03b5 \u2264 1/4) \u2264 9\n|V |2\n2\na\u2208V\n\nCombining this with (4.1.2) finishes the proof, except for (4.1.1). To prove\nthat, we let Bad = V \\ Good. Notice that\n\u001b\n\u001a\n\u221a\n1\n1\n\u03b5|Bad| X 1\nR\nR\n+ P(xt (2) = a) \u2212\n\u2264\nP(xt (1) = a) \u2212\n|V |\n2\n|V |\n|V |\na\u2208V\n\n\f14\n\nR. I. OLIVEIRA\n\nas each a \u2208 Bad contributes at least\n\n\u221a\n\n\u03b5/|V | to the sum. But the RHS equals\n\nR\ndTV (L[xR\nt (1)], Unif(V )) + dTV (L[xt (2)], Unif(V )) \u2264 2\u03b5\n\nsince t \u2265 TRW(G) (\u03b5). We deduce\n\u221a\n\u221a\n\u03b5|Bad|\n\u2264 2\u03b5 or equivalently |Good| \u2265 (1 \u2212 2 \u03b5)|V |.\n|V |\n\u221a\n\u22121 for all a \u2208 Good, hence\nMoreover, P(xR\nt (i) = a) \u2265 (1 \u2212 2 \u03b5)|V |\nP(xR\nt (i) \u2208 Good) \u2265\n\n\u221a\n\u221a\n\u221a\n|Good|\n(1 \u2212 2 \u03b5) \u2265 (1 \u2212 2 \u03b5)2 \u2265 1 \u2212 4 \u03b5.\n|V |\n\nInequality (4.1.1) now follows from\nP(xIt \u2208\n/ Good2 ) \u2264 P(xIt (1) \u2208\n/ Good) + P(xIt (2) \u2208\n/ Good)\n\n\u221a\n(Proposition 4.1) = P(xR\n/ Good) + P(xR\n/ Good) \u2264 8 \u03b5.\nt (1) \u2208\nt (2) \u2208\n\n\u0003\n\n4.2. When collisions are nearly as fast as mixing. Recalling Notational\nconvention 4.1, we define the first meeting time M (x) of RW(2, G) started\nR\nfrom x \u2208 V 2 as the smallest t0 \u2265 0 such that xR\nt0 (1) = xt0 (2) (this is a.s. finite\nby ergodicity). We will also write\nR\nM\u2265t (x) = inf{h0 \u2265 0 : xR\nt+h0 (1) = xt+h0 (2)}\n\nfor the time until the first meeting after t (this is a \"time-shifted\" meeting\ntime).\nThe following definition will be crucial for our analysis.\nDefinition 4.1.\n\nWe say that a weighted graph G is easy if\n\nsup P(M (x) > 20,000TRW(G) (1/4)) \u2264 1/8.\n\nx\u2208V 2\n\nWe note that all long enough paths and cycles are examples of easy graphs.\nNoneasy graphs include (Z/LZ)d for d \u2265 2 fixed and L sufficiently large, as\nwell as large expander graphs. The next proposition proves Lemma 1.1 for\nall easy graphs via a coupling argument due to Aldous and Fill.\nProposition 4.4.\n\nLemma 1.1 holds for all easy weighted graphs.\n\nProof sketch. Given G, Aldous and Fill [[1], Chapter 14, Section 5]\nconstruct a coupling of IP(|V |, G) started from two different states u, v.\nLetting {uIt , vtI }t\u22650 denote the coupled trajectories, the following property\nholds: for each 1 \u2264 i \u2264 |V |, uIt (i), vtI (i) behave as independent random walks\nup to their first meeting time, which we denote by Mi . After this time Mi ,\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n15\n\nuIt (i) = vtI (i), that is, the two processes move together. This implies\n\u2200t \u2265 0\n\ndTV (L[uIt ], L[vtI ]) \u2264\n\u2264\n\nP(uIt\n|V |\nX\n\n6 vtI ) \u2264\n=\n\n|V |\nX\ni=1\n\nP(uIt (i) 6= vtI (i))\n\nP(Mi > t).\n\ni=1\n\nIt is easy to adapt this to a coupling of IP(2, G) starting from given x, y \u2208\n(V )2 , so that, if {xIt , ytI }t\u22650 denotes the coupled trajectories, we have\n\u2200t \u2265 0\n\ndTV (L[xIt ], L[ytI ]) \u2264 P(M1 > t) + P(M2 > t).\n\nNow both M1 and M2 are the meeting times of independent random walkers\non G, which shows that\n\u2200t \u2265 0\n\nsup\nx,y\u2208(V )2\n\ndTV (L[xIt ], L[ytI ]) \u2264 2 sup P(M (z) > t).\nz\u2208V 2\n\nFor t = 20,000TRW(G) (1/4) and G easy, the RHS is \u2264 1/4. By convexity, this\nimplies that\n1\nsup dTV (L[xIt ], Unif((V )2 )) \u2264 .\n4\nx\u2208(V )2\nIn other words, TIP(2,G) (1/4) \u2264 20,000TRW(G) (1/4). \u0003\nRemark 4.1. Aldous and Fill's argument actually proves Theorem 1.1\nfor all easy graphs; see [1], Chapter 14, Section 5 for details.\n4.3. Long time to meet in noneasy graphs. We now consider what happens when IP(2, G) is performed on a graph, that is, not easy. Our first goal\nis to show that independent random walkers take a relatively long time to\nmeet from most initial states in V .\nProposition 4.5. Assume G = (V, E, {we }e\u2208E ) is not easy. Then\n1\n1 X\n.\nP(M (v) \u2264 20TRW(G) (1/4)) \u2264\n|V |2\n125\n2\nv\u2208V\n\nRemark 4.2. In general we cannot guarantee that P(M (v) < 20 \u00d7\nTRW(G) (1/4)) is uniformly small over all v \u2208 (V )2 . In particular, the probability of collision from adjacent v(1), v(2) might be much greater than the\nabove bound.\nProof of the Proposition. Set T = TRW(G) (1/4). Since G is not\neasy, there exists some x \u2208 V 2 with\n\n(4.3.1)\n\nP(M (x) > 20,000T ) > 1/8.\n\n\f16\n\nR. I. OLIVEIRA\n\nConsider some k \u2208 N. Using the Markov property and the notation introduced in Section 4.2, one can write\nP(M (x) > 40kT ) = E[I{M (x)>40(k\u22121)T } P(M\u226540(k\u22121)T (x) > 40T |xR\n40(k\u22121)T )].\n\nThe conditional probability in the RHS equals P(M (y) > 40T ) for y =\nxR\n40(k\u22121)T , hence\n\u0010\n\u0011\nP(M (x) > 40kT ) \u2264 sup P(M (y) > 40T ) P(M (x) > 40(k \u2212 1)T )\ny\u2208V 2\n\n(. . . induction. . . ) \u2264\n\n\u0010\n\nsup P(M (y) > 40T )\n\ny\u2208V 2\n\n\u0011k\n\n.\n\nApplying this to k = 500 and using the bound in (4.3.1) gives the following\nwith room to spare:\n497\nsup P(M (y) > 40T ) \u2265 8\u22121/500 \u2265 e\u22123/500 \u2265\n.\n500\ny\u2208V 2\nFix some y \u2208 V 2 achieving this supremum. Notice that M (y) > 40T holds if\nR\nand only if ytR (1) 6= ytR (2) for all 0 \u2264 t \u2264 40T . If, that is, the case, y20T\n+h (1) 6=\nR\ny20T +h (2) for all 0 \u2264 h \u2264 20T . Using the Markov property as before, we see\nthat\n497\n\u2264 P(M (y) > 40T ) \u2264 P(M\u226520T (y) > 20T )\n500\nX\nR\n=\nP(y20T\n= v)P(M (v) > 20T ).\nv\u2208V 2\n\nMoreover, by (2.2.2),\nX\nR\nP(y20T\n= v)P(M (v) > 20T )\nv\u2208V 2\n\n\u2264\nHence\n\nX P(M (v) > 20T )\nR\n+ dTV (L[y20T\n], Unif(V 2 )).\n2\n|V\n|\n2\n\nv\u2208V\n\nX P(M (v) > 20T )\n497\nR\n\u2212 dTV (L[y20T\n], Unif(V 2 )) \u2264\n.\n500\n|V |2\n2\nv\u2208V\n\nWe finish by noting that, by Proposition 4.3, 20T \u2265 TRW(2,G) (2\u221219 ), hence\nR\ndTV (L[y20T\n], Unif(V 2 )) \u2264 2\u221219 \u2264\n\nand therefore\n\n1\n,\n500\n\nX P(M (v) > 20T ) 496\n1\n=1\u2212\n.\n\u2265\n2\n|V |\n500\n125\n2\n\nv\u2208V\n\n\u0003\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n17\n\n4.4. If meeting takes a long time, IP(2, G) and RW(2, G) are similar. We\nhave just shown that the meeting is unlikely to be smaller than 20TRW(G) (1/4)\nfrom most initial states. We now show that IP(2, G) is similar to RW(2, G)\nuntil the first meeting time.\nProposition 4.6.\n\nFor any x \u2208 (V )2 and s \u2265 0,\n\nI\ndTV (L[xR\ns ], L[xs ]) \u2264 P(M (x) \u2264 s).\n\nWe will only need the following simple corollary (proof omitted) in what\nfollows.\nCorollary 4.1.\n\nFor any x, y \u2208 (V )2 and s \u2265 0,\n\nR\ndTV (L[xIs ], L[ysI ]) \u2264 P(M (x) \u2264 s) + P(M (y) \u2264 s) + dTV (L[xR\ns ], L[ys ]).\n\nProof of Proposition 4.6. We present a coupling of {xIt }t\u22650 and\n{xR\nt }t\u22650 such that the two processes agree up to M (x). The proposition\nthen follows from the coupling characterization of dTV (*, **); cf. Section 2.2.\nOur coupling is given by a continuous-times Markov chain on S = (V )2 \u00d7\n2\nV with transition rates given by q(*, **). The state space can be split into\ntwo parts, \u2206 \u2261 {(z, z) : z \u2208 (V )2 } and its complement \u2206c .\n\n\u2022 Transition rule 1: The transition rates from any pair (x, y) \u2208 \u2206c to any\nother pair in S are the same as those of independent realizations of\nRW(2, G) and IP(2, G).\n\u2022 Transition rule 2: The transition rates from a pair (x, x) \u2208 \u2206 are determined as follows:\n\u2013 Transition rule 2.1: For each e \u2208 E with |e \u2229 {x(1), x(2)}| = 1,\nq((x, x), (fe (x), fe (x))) = we ;\n\u2013 Transition rule 2.2: If e \u2208 E satisfies e = {x(1), x(2)},\n\u001a\n((x, x), (fe (x), (x(1), x(1)))) = we ,\nq((x, x), (x, (x(2), x(2)))) = we .\n\u2013 Transition rule 2.3: All other potential transitions have rate 0.\nInspection of the marginals reveals that this indeed gives a coupling of\nI\n{xR\nt }t\u22650 and {xt }t\u22650 when started from an initial state (x, x) \u2208 \u2206. Moreover,\nthe two processes can only differ after a transition has occurred according\nto rule 2.2. The first time when this happens is precisely the first meeting\ntime of {xR\nt }t\u22650 . \u0003\n\n\f18\n\nR. I. OLIVEIRA\n\n4.5. Proof of the mixing time bound for IP(2, G). We now use the tools\ndeveloped above in order to prove Lemma 1.1.\nProof of Lemma 1.1. The case of easy graphs is covered by Proposition 4.4, so assume G = (V, E, {we }e\u2208E ) is not easy. Let x, y be given and\nT \u2261 TRW(G) (1/4). Notice that for all A \u2282 (V )2 , if {xIt }t\u22650 , {ytI }t\u22650 are defined over the same probability space,\nI\nI\nI\nP(xI40T \u2208 A) \u2212 P(y40T\n\u2208 A) = E[P(xI40T \u2208 A|xI20T ) \u2212 P(y40T\n\u2208 A|y20T\n)]\n\nMaximizing over A yields\n(4.5.1)\n\nI\nI\n\u2208 *|y20T\n))].\n\u2264 E[dTV (P(xI40T \u2208 *|xI20T ), P(y40T\n\nI\ndTV (L[xI40T ], L[y40T\n])\nI\nI\n\u2264 E[dTV (P(xI40T \u2208 *|xI20T ), P(y40T\n\u2208 *|y20T\n))].\n\nBy the Markov property and Corollary 4.1,\n\nI\nI\ndTV (P(xI40T \u2208 *|xI20T = v), P(y40T\n\u2208 *|y20T\n= w))\nI\nI\n= dTV (L[v20T\n], L[w20T\n])\n\nR\nR\n]).\n], L[w20T\n\u2264 P(M (v) \u2264 20T ) + P(M (w) \u2264 20T ) + dTV (L[v20T\n\nProposition 4.3 implies the third term in the RHS is \u2264 2\u221219 for any v, w.\nUsing this in conjunction with (4.5.1), we obtain\n(4.5.2)\n\nI\nI\ndTV (L[xI40T ], L[y40T\n]) \u2264 E[\u03c6(xI20T )] + E[\u03c6(y20T\n)] + 2\u221219 ,\n\nwhere \u03c6(z) = P(M (z) \u2264 20T ). Notice that 0 \u2264 \u03c6 \u2264 1. We may apply Lemma 4.1\nand the fact that 20T \u2265 TRW(G) (2\u221220 ) (cf. Proposition 2.1) to deduce\nX P(M (v) \u2264 20T )\nE[\u03c6(xI20T )] \u2264 2\u22127 + 9\n(4.5.3)\n.\n|V |2\n2\nv\u2208V\n\nApplying the same reasoning to\nwe obtain\n\nI )\n\u03c6(y20T\n\nI\n(4.5.4) dTV (L[xI40T ], L[y40T\n]) \u2264 18\n\nand plugging the results into (4.5.2),\n\nX P(M (v) \u2264 20T )\n+ 2\u22126 + 2\u221219 .\n2\n|V\n|\n2\n\nv\u2208V\n\nFinally, we use the fact that G is not easy, combined with Proposition 4.5,\nto deduce\n18\nI\ndTV (L[xI40T ], L[y40T\n]) \u2264\n(4.5.5)\n+ 2\u22129 + 2\u22126 \u2264 1/4\n125\nwith room to spare. By convexity,\n(4.5.6)\n\ndTV (L[xI40T ], Unif((V )2 )) \u2264 1/4.\n\nSince this holds for all x \u2208 (V )2 , we have TIP(2,G) (1/4) \u2264 40T , which implies\nLemma 1.1 for noneasy graphs. \u0003\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n19\n\nRemark 4.3. The first inequality in (4.5.3) follows from Lemma 4.1,\nwhich is a consequence of the negative correlation property; cf. Lemma 3.1\nand Corollary 3.1. This is the first crucial use we make of negative correlation\nin this paper.\n5. The chameleon process. In the previous section we determined the\norder of magnitude of the mixing time of IP(2, G). Going beyond two particles will require an important additional idea, that is, based on Morris's\npaper [19]. His idea is to introduce the so-called chameleon process to keep\ntrack of the conditional distribution of one particle in IP(k, G). We will need\na different process, which will nevertheless call by the same name.\n5.1. A modified graphical construction. We will need consider a variant\nof the construction of IP(k, G) presented in Section 3.2. Consider three independent ingredients:\n(1) A Poisson process P = {\u03c41 \u2264 \u03c42 \u2264 \u03c43 \u2264 * * *} \u2282 [0, +\u221e) with rate 2W .\n(2) An i.i.d. sequence of E-valued random variables {en }n\u2208N , with P(en =\ne) = we /W .\n(3) An i.i.d. sequence of coin flips {cn }n\u2208N with P(cn = 1) = P(cn = 0) = 1/2.\n\nRecall the definition of fe from Section 3.1, and set fe1 = fe , fe0 = the\nidentity function. We modify the definition of the maps I(t,s] from Section 3.2\nas follows: if P \u2229 (t, s] = \u2205, I(t,s] is the identity map, as before. Otherwise,\nand we set\n\nP \u2229 (t, s] = {\u03c4n < \u03c4n+1 < * * * < \u03c4m },\nm\nn+1\n\u25e6 * * * \u25e6 fecn+1\n\u25e6 fecnn .\nI(t,s] = fecm\n\nThe thinning property of the Poisson process implies that {\u03c4n : cn = 1} is a\nPoisson process with rate W . One can use this to show that:\nProposition 5.1 (Proof omitted). The joint distribution of the maps\nI(t,s] , 0 \u2264 t < s < +\u221e, is the same as in Section 3.2.\n5.2. The chameleon process. The chameleon process is built on top of the\nmodified graphical construction. The definition of the process will depend\non a parameter T > 0 which we call the phase length, for reasons that will\nbecome clear later on.\nGiven y \u2208 (V )k\u22121 , let O(y) \u2261 {y(1), . . . , y(k \u2212 1)} denote the set of vertices that \"occupied\" by the coordinates of y. The chameleon process will\nbe a continuous-time, time-inhomogeneous Markov chain with state space\n(5.2.1)\n\nCk (V ) \u2261 {(z, R, P, W ) : z \u2208 (V )k\u22121 ;\nthe sets O(z), R, P, W partition V }.\n\n\f20\n\nR. I. OLIVEIRA\n\nNotice that we do allow any of the R, P, W to be empty in the above definition. For a given (z, R, P, W ) \u2208 Ck (V ), it will be convenient to refer to\nthe vertices in the sets O(z), R, P, W as black, red, pink and white (resp.).\nNotice that any vertex v \u2208 V will belong to one of these color classes.\nThe evolution of the process from initial state (z, R, P, W ) will be denoted\nby\nC\nC\nC\n{(zC\nt , Rt , Pt , Wt )}t\u22650 .\n\nBy definition, this process will only be updated at the times \u03c4n (n \u2208 N) given\nby the Poisson process and at deterministic times 2iT , i \u2208 N. Moreover,\nthe updates at times \u03c4n are of different kinds depending on whether \u03c4n \u2208\n((2i\u2212 2)T, (2i\u2212 1)T ] for some i \u2208 N+ , or \u03c4n \u2208 ((2i\u2212 1)T, 2iT ] for some i \u2208 N+ .\nFinally, we will define for convenience,\nC\nC\nC\n(zC\n0\u2212 , R0\u2212 , P0\u2212 , W0\u2212 ) = (z, R, P, W )\n\nand will allow an instantaneous change at time t = 0: that is,\nC\nC\nC\nit might happen that (zC\n0 , R0 , P0 , W0 ) 6= (z, R, P, W ).\n\nThe three update rules are described in Box 5.1.\nRemark 5.1. Technically, this process is not c\u00e0dl\u00e0g, as it changes at\ntime 0. We will nevertheless continue to use t\u2212 (cf. Notational convention 2.2) with the proviso for t = 0 that we have just described.\n\nRemark 5.2. We briefly note that our chamaleon process is more complicated than Morris's process [19]. In brief: his process does not have constantcolor phases and will depink right when the number of pink particles exceeds\nthe minimum of red and white. The second difference is a matter of convenience, but the first one will be fundamental at key steps of our argument.\n5.3. Two basic properties. The next two results will be useful later on.\nWe only sketch the proofs.\nLemma 5.1.\n\nLet\n\nC\nC\nC\n(\u1e91i , R\u0302i , P\u0302i , \u0174i ) = the value of (zC\n2iT\u2212 , R2iT\u2212 , P2iT\u2212 , W2iT\u2212 )\n\n(i \u2208 N).\n\nThen {(\u1e91i , R\u0302i , P\u0302i , \u0174i )}i\u2208N is a discrete-time, time-homogeneous Markov chain.\nMoreover, if Dj is the jth depinking time of the process, then D\u0302j \u2261 Dj /2T\nis a stopping-time for this discrete-time Markov chain.\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n21\n\nBox 5.1 The three kinds of updates in the chameleon process.\n\u2022 Constant-color phases: If t = \u03c4n \u2208 ((2i \u2212 2)T, (2i \u2212 1)T ] for some i \u2208 N+ ,\nupdate\nC\nC\nC\ncn C\ncn\nC\ncn\nC\ncn\nC\n(5.2.2) (zC\nt , Rt , Pt , Wt ) = (fen (zt\u2212 ), fen (Rt\u2212 ), fen (Pt\u2212 ), fen (Wt\u2212 )).\n\nThat is, the states of the endpoints of en are flipped if cn = 1, and nothing\nhappens if cn = 0.\n\u2022 Color-changing phases. If t = \u03c4n \u2208 ((2i \u2212 1)T, 2iT ], for i \u2208 N+ , update as\nabove unless:\n1. en = {w, r} has a white endpoint w \u2208 WtC\u2212 and a red endpoint r \u2208 RtC\u2212 ;\n2. |PtC\u2212 | < min{|RtC\u2212 |, |WtC\u2212 |}.\nIf (1) and (2) hold, r and w both become pink, and we call t a pinkening\ntime.\nC\nC\nC\nC\nC\nC\nC\n(5.2.3) (zC\nt , Rt , Pt , Wt ) = (zt\u2212 , Rt\u2212 \\ {r}, Pt\u2212 \u222a {r, w}, Wt\u2212 \\ {w}).\n\n\u2022 Depinking times. If t = 2iT with i \u2208 N (t = 0 or t lies at the end of a colorchanging phase) and |PtC\u2212 | \u2265 min{|WtC\u2212 |, |RtC\u2212 |} (more pink than either\nwhite or red), flip a fair coin di , and make all pink particles become red\nor white depending on whether di comes out heads or tails (resp.).\n(\nC\nC\nC\n(zC\ndi = 1;\nt\u2212 , Rt\u2212 \u222a Pt\u2212 , \u2205, Wt\u2212 ),\nC\nC\nC\nC\n(5.2.4) (zt , Rt , Pt , Wt ) =\nC\nC\nC\n(zC\ndi = 0.\nt\u2212 , Rt\u2212 , \u2205, Wt\u2212 \u222a Pt\u2212 ),\n\nProof Sketch. Markovianity and time-homogeneity are obvious. To\nprove the stopping time property, it suffices to check that (setting D0 = 0),\n\u001a\n\u001b\nDj\u22121\nDj\n= inf i >\n: |P\u0302i | \u2265 min{|R\u0302i |, |\u0174i |} ,\n\u2200j > 0\n2T\n2T\nwhere we allow the inf to be +\u221e if the set is empty or Dj\u22121 = +\u221e. \u0003\nC\nC\nC\nLemma 5.2. Suppose (zC\n2iT , R2iT , P2iT , W2iT ) is the state of the chameleon\nprocess at time 2iT (i.e., at the beginning of a constant-color phase). Then\nC\nC\nC\nC\nC\nC\nC\n(zC\n(2i+1)T , R(2i+1)T , P(2i+1)T , W(2i+1)T ) = (I(z2iT ), I(R2iT ), I(P2iT ), I(W2iT )),\n\nwhere I = I(2iT,(2i+1)T ] is the map defined in the modified graphical construction.\nProof. By inspection. \u0003\n\n\f22\n\nR. I. OLIVEIRA\n\n5.4. The chameleon process and conditional distributions. We now explain the relationship between the chameleon process and conditional distributions.\nNotational convention 5.1. x = (x(1), . . . , x(k)) \u2208 (V )k is represented as a pair (z, x), where z = (x(1), . . . , x(k \u2212 1)) \u2208 (V )k\u22121 and x =\nx(k) \u2208 V \\ O(z). [Notice that xIt = (zIt , xIt ) for all t \u2265 0.]\nProposition 5.2 (Proof omitted). Given an initial state x = (z, x) \u2208\n(V )k for IP(k, G), set R = {x}, P = \u2205 and W = V \\ (O(z) \u222a {x}). Consider the interchange process {xIt = (zIt , xIt )}t\u22650 started from state x and the\nC\nC\nC\nchameleon process {(zC\nt , Rt , Pt , Wt )}t\u22650 started from configuration (z, R,\nP, W ) \u2208 Ck (V ). Then\nP(xIt = b) = E[inkt (b)I{zC =c} ],\n\n(5.4.1) \u2200t \u2265 0, \u2200b = (c, b) \u2208 (V )k\n\nt\n\nwhere\n(5.4.2)\n\ninkt (v) \u2261 I{v\u2208RCt } +\n\nI{v\u2208P C }\nt\n\n2\n\n(v \u2208 V ).\n\nThis is almost identical (up to changes in notation) to [19, Lemma 1], and\nwe omit its proof. It will be useful to think of inkt (v) as the amount of \"red\nink\" at vertex v \u2208 V : a red vertex has one unit of red ink, a pink vertex has\nhalf a unit, and black or white vertices have no ink. We will see below that\nthe total amount of red ink in the system determines the rate of convergence\nto equilibrium of IP(k, G).\n6. From 2 to k particles via the chameleon process. In this section we\npresent the proof of Lemma 1.2, modulo several lemmas about the chameleon\nprocess that we will prove later. We then outline the remainder of the paper.\n6.1. Proof of Lemma 1.2.\nProof. We assume we have defined a chameleon process over Ck (V ) as\nin Section 5.2. We will take the notation and definitions from that section\nfor granted. We also define\n(6.1.1)\n\ninkt \u2261\n\nX\n\nv\u2208V\n\ninkt (v) = |RtC | +\n\nWe note for later reference that\n(6.1.2)\n\ninkt \u2261\n\nX\n\nv\u2208V\n\n|PtC |\n2\n\ninkt (v)\n\n\\O(zIt )\n\nsince the vertices in O(zIt ) have zero red ink.\n\n(t \u2265 0).\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n23\n\nWe have argued in Proposition 5.2 that the distribution of IP(k, G) started\nfrom x = (z, x) \u2208 (V )k corresponds to a chameleon process started from\n(z, {x}, \u2205, V \\ (O(z) \u222a {x})). Letting inkxt denote the value of inkt in that\nchameleon process, we will show that:\nLemma 6.1 (Proven in Section 8.2).\nall 1 \u2264 k \u2264 |V | \u2212 1:\n\nThe following inequality holds for\n\n\u0014\nsup dTV (L[xIt ], Unif((V )k )) \u2264 2k sup E 1 \u2212\n\nx\u2208(V )k\n\nx\u2208(V )k\n\nwhere\nFill \u2261\n\nn\n\n\u0015\ninkxt\nFill\n|V | \u2212 k + 1\n\no\nlim inkxt = |V | \u2212 k + 1 .\n\nt\u2192+\u221e\n\nThe main goal is to bound the expected value in the RHS of the inequality\nin Lemma 6.1. Fix some x \u2208 (V )k , and let Dj (x) denote the jth depinking\nc x \u2261 inkx\ntime for the chameleon process corresponding to x. Also set ink\nj\nDj (x)\nfor this process. We will show in Proposition 7.1 that there are infinitely\nmany depinking times, that is, there are infinitely many times of the form\n2iT at which the number of pink particles is at least as large as the minimum\nof the numbers of white and red. The definition of the chameleon process\nimplies that inkxt can only change at depinking times, hence for any t \u2265 0\nc x if Dj (x) \u2264 t < Dj+1 (x) for some j. We\ninkxt = 1 if t < D1 (x) and inkxt = ink\nj\ndeduce that\n\u0012\n\u0013\ncx\ninkxt\nink\nm\n1\u2212\n\u2264 sup 1 \u2212\n+ I{Dj (x)>t}\n|V | \u2212 k + 1 m\u2265j\n|V | \u2212 k + 1\n\u0013\nX\u0012\ncx\nink\nm\n\u2264\n1\u2212\n+ I{Dj (x)>t} .\n|V | \u2212 k + 1\nm\u2265j\n\nTaking expectations, we see that the RHS of the inequality in Lemma 6.1 is\nat most\n\u0015\n\u001b\n\u001aX \u0014\ncx\nink\nm\nE 1\u2212\n(6.1.3) 2k sup\nFill + P(Dj (x) \u2265 t|Fill) .\n|V | \u2212 k + 1\nx\u2208(V )k\nm\u2265j\n\nA simple (but technical) proposition proven in the Appendix will take care\nof the first term.\nProposition 6.1 (Proven in Section B). For all l \u2265 1 and x \u2208 (V )k ,\n\u0012 \u0013l\n\u0015\n\u0014\ncx\np\n71\nink\nl\n.\nFill \u2264 |V | \u2212 k + 1\nE 1\u2212\n|V | \u2212 k + 1\n72\n\n\f24\n\nR. I. OLIVEIRA\n\nWe thus have\n\n\u001aX \u0014\nE 1\u2212\n2k sup\nx\u2208(V )k\n\n(6.1.4)\n\nm\u2265j\n\n\u0015\n\u001b\ncx\nink\nm\nFill + P(Dj (x) \u2265 t|Fill)\n|V | \u2212 k + 1\n\n\u2264 C2 |V |3/2 e\u2212c1 j + 10k sup P(Dj (x) \u2265 t|Fill),\nx\u2208(V )k\n\nwhere c1 = ln(72/71) > 0 and C2 = 720 are universal constants.\nBounding P(Dj (x) \u2265 t|Fill) is the key step in the proof. Up to now all of\nour results have been valid for all values of k, |V | and of the phase length\nparameter T > 0. The next lemma will require restrictions on these values.\nLemma 6.2 (Proven in Section 9.3). There exist universal constants\nC3 , C4 > 0, such that if |V | \u2265 300, T \u2265 C3 TIP(2,G) (1/4) and k/|V | \u2264 1/2,\nthen\n\u2200x \u2208 (V )k , \u2200j \u2208 N:\n\nE[eDj (x)/(C4 T ) |Fill] \u2264 ej .\n\nIf |V | \u2265 300 Markov's inequality allows one to deduce that, for yet another\nuniversal constant L \u2261 C3 C4 ,\nP(Dj (x) > t|Fill) \u2264 ej\u2212t/(LTIP(2,G) (1/4)) .\n\nPlugging this into (6.1.4) and Lemma 6.1, we obtain\n\ndTV (L[xIt ]), Unif((V )k )) \u2264 C1 |V |3/2 e\u2212c1 j + 10|V |ej\u2212t/(LTIP(2,G) (1/4)) .\n\nSince this inequality holds for all j, we can take\n\u0016\n\u0017\nt\nj=\n2LTIP(2,G) (1/4)\nand obtain\n\ndTV (L[xIt ], Unif((V )k )) \u2264 K0 |V |3/2 e\u2212t/(2LTIP(2,G) (1/4))\n\nwith K0 > 0 universal. Comparing with the definition of mixing time in (1.1)\nand noting that Unif((V )k ) is stationary for IP(k, G) finishes the proof in\nthe case |V | \u2265 300.\nThe case |V | < 300-that is, |V | bounded by a universal constant-can be\ndealt with in several ways. For example, one may use the result of Caputo\net al. [6] for the spectral gap of IP(k, G) together with the standard lower\nbound for TRW(G) (1/4) in terms of the spectral gap and the usual upper\nbound for TIP(k,G) (\u03b5) in terms of its spectral gap (see, e.g., [17] for these\nstandard bounds). Alternatively, one may use Aldous and Fill's analysis\n(see Remark 4.1) together with the inequality\n\u0012\n\u0013i\n1\n,\nP(M (x) > 2iTRW(G) (1/4)) \u2264 1 \u2212\n4|V |\nwhich one can prove via Proposition 2.2 and a few simple calculations. \u0003\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n25\n\n6.2. Outline of the missing steps. We now summarize the main steps left\nin the proof.\n(1) In Section 7 we collect several facts about the quantity ink. The proof of\nc x /(|V | \u2212 k + 1)|Fill], presented\nProposition 6.1 on the decay of E[1 \u2212 ink\nl\nin the Appendix (see Section B), relies on results from this section.\n(2) Section 8 contains the proof of Lemma 6.1, which is based on an auxiliary\nresult on conditional distributions (Lemma 8.1).\n(3) Section 9 bounds the right tail of the first depinking time in a chameleon\nprocess, and then uses this to bound the exponential moment of the jth\ndepinking time. This leads to the key Lemma 6.2, proven in Section 9.3.\n7. A miscellany of facts on ink. In this section we prove several facts we\nwill need about the quantity inkt introduced in (6.1.1). We will use the same\nnotation introduced in the proof of Lemma 1.2 (cf. Section 6.1):\n(1) x \u2208 (V )k is some fixed state;\n(2) (z, R, P, W ) = (z, {x}, \u2205, V \\ (O(z) \u222a {x})) \u2208 Ck (V ) is the initial state\ncorresponding to x in the sense of Proposition 5.2;\n(3) inkxt is the total amount of ink in (ztC , RtC , PtC , WtC ) (with the above\ninitial state);\n(4) Dj (x) is the jth depinking time for this process;\nc x \u2261 inkx\n(5) finally, ink\nj\nDj (x) .\n\nWe will mostly omit x from the notation in what follows. Most proofs in\nthis section follow by inspection, so we will be quite brief.\nIn principle the total number of number of depinking times could be finite.\nWe begin by showing that this is not the case.\nProposition 7.1.\nfinite.\n\nThe number of depinking times is almost surely in-\n\nRemark 7.1. Notice that this only works because our definition of a\ndepinking time allows for \"trivial depinking times\" where there are only red\nand black (or only white and black) particles left. This was noted in Box 5.1.\nProof of Proposition 7.1. We use the following simple fact (proof\nomitted): there exists some \u03b4 > 0 such that each color-changing phase that\nstarts with min{|RtC |, |WtC |} > 0 will have a pinkening with probability \u2265 \u03b4,\nregardless of the past. This implies that, given s \u2265 0, the values of |PtC |\nfor t \u2208 [s, +\u221e) will have a strictly positive probability of increasing by the\nend of each color-changing phase, at least until |PtC | \u2265 min{|RtC |, |WtC |}.\nSince |PtC | can only decrease at depinking steps, this shows that |PtC | must\ncontinue to increase until |PtC | \u2265 min{|RtC |, |WtC |}, and the next time of the\n2iT will be a depinking time. \u0003\n\n\f26\n\nR. I. OLIVEIRA\n\nThe next result follows by inspection.\nProposition 7.2 (Proof omitted).\na.s.\n\nc j \u2264 |V | \u2212 k + 1 for all j \u2208 N,\n0 \u2264 ink\n\nWe now compute the amount of change of ink in each step.\nProposition 7.3.\na.s., where\n(7.0.1)\n\n\u0018\n\nc j+1 \u2208 {ink\nc j + \u2206(ink\nc j ), ink\nc j \u2212 \u2206(ink\nc j )}\nFor j \u2208 N, ink\n\nmin{r, |V | \u2212 k + 1 \u2212 r}\n\u2206(r) \u2261\n3\n\n\u0019\n\n(r \u2208 N).\n\nc l }j , each possibility is equally likely.\nMoreover, conditionally on {ink\nl=0\n\nProof. Box 5.1 shows that there are no pink particles left in the sysc j = inkD = |RC |.\ntem after depinking is performed. This implies that ink\nj\nDj\nMoreover, since the total number of nonblack particles is |V | \u2212 k + 1, there\nc j red and |V | \u2212 k + 1 \u2212 ink\nc j white particles at time Dj .\nmust be ink\nA pinkening step decreases the number of red and white particles by 1\neach and increases the number of pink particles by 2. However, no pinkenings\nare performed if the number of pink particles is at least the number of red or\nthe number of white particles. In other words, the number of pinkening steps\ncj \u2212 p\nuntil the next depinking is precisely the smallest p satisfying 2p \u2265 ink\nc\nc\nor 2p \u2265 |V | \u2212 k \u2212 1 \u2212 inkj \u2212 p, which is p = \u2206(inkj ) for \u2206 defined in (7.0.1).\nAt the depinking step, the pink particles either all become white, or they\nc j+1 = ink\nc j \u2212 \u2206(ink\nc j ) or\nall become red. These possibilities corresponds to ink\nc j+1 = ink\nc j + \u2206(ink\nc j ), respectively. Which possibility will actually occur\nink\ndepends on the value of the fair coin di , that is, flipped at the depinking\nc l }l\u2264j ,\ntime 2iT = Dj+1 . It is easy to see that the coin is independent of {ink\nand this implies that both possibilities are equally likely. \u0003\nThe next lemma summarizes the above sequence of propositions and adds\na useful remark.\nc j }j\u22650 is a Markov chain with initial state\nLemma 7.1. The sequence {ink\nc 0 = 1, absorbing states at 0 and |V | \u2212 k + 1 and transition probabilities\nink\ngiven by\n(7.0.2)\n\n1\np(a, b) \u2261 (I{b=a+\u2206(a)} + I{b=a\u2212\u2206(a)} )\n2\n\n(a, b \u2208 {0, 1, 2, . . . , |V | \u2212 k + 1}).\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n27\n\nMoreover, it is almost surely absorbed in finite time in either 0 or |V | \u2212 k + 1.\nFinally, the event\nn\no\nc j = |V | \u2212 k + 1\n(7.0.3)\nFill \u2261 lim ink\nj\u2192+\u221e\n\nhas probability 1/(|V | \u2212 k + 1).\n\nRemark 7.2. The event Fill corresponds to the number of red particles\nconverging to |V | \u2212 k + 1, that is, that there are only black and red particles\nat all large enough times, or, equivalently, to red ink filling up all available\nspace. Notice that we can rewrite\nn\no\nFill \u2261 lim inkt = |V | \u2212 k + 1 ,\nt\u2192+\u221e\n\nwhich is the form that appears in the proof of Lemma 1.1.\n\nProof of Lemma 7.1. The first sentence is obvious given the sequence\nof propositions; only notice that p(a, a) = 1 if a \u2208 {0, |V | \u2212 k + 1}. We omit\nc \u221e \u2261 limj\u2192+\u221e ink\ncj \u2208\nthe trivial proof of the next assertion, which implies ink\n{0, |V | \u2212 k + 1}.\nc j are unbiased; that implies that\nNow notice that the increments of ink\nthis process is also a martingale. We thus have\nc \u221e = |V | \u2212 k + 1)\nP(Fill) = P(ink\n\nc \u221e \u2208 {0, |V | \u2212 k + 1}) =\n(ink\n\nc j }j\u2208N bounded, cf. Prop. 7.2) =\n({ink\nc 0 = 1) =\n(mart. property + ink\n\nc \u221e]\nE[ink\n|V | \u2212 k + 1\n\nc j]\nlimj\u2192+\u221e E[ink\n|V | \u2212 k + 1\n\nc 0]\nE[ink\n1\n=\n.\n|V | \u2212 k + 1 |V | \u2212 k + 1\n\n\u0003\n\nWe will need one final lemma before we proceed.\nLemma 7.2.\n\nFor all b \u2208 (V )k\u22121 and t \u2265 0,\nP({zC\nt = b} \u2229 Fill) =\n\nP(zC\nt = b)\n.\n|V | \u2212 k + 1\n\nProof. This follows from the previous lemma if we can show that Fill\nand zC\nt are independent. To see this, simply notice that Fill is entirely determined by the coin flips di performed at the depinking times, whereas the\nvalue of zC\nt does not at all depend on these coin flips. \u0003\n\n\f28\n\nR. I. OLIVEIRA\n\nRemark 7.3. It transpires from the above that the chameleon process\nconditioned on Fill is the same as the unconditional process, except that the\ncoin flips di performed at depinking times are biased. This remark will be\nuseful in the proof of Lemma 6.2 in Section 9.3.\n8. Convergence to stationarity in terms of ink. In this section we will\nprove Lemma 6.1, used in the proof of Lemma 1.2 (cf. Section 6.1), in which\nwe show that the amount of ink in the system can be used to bound the\ndistance to the stationary distribution. We start with a preliminary result\non marginals.\n8.1. The convergence to equilibrium of conditional distributions. We will\nagain use Notational convention 5.1, whereby any x = (x(1), . . . , x(k)) \u2208\n(V )k is written as a pair x = (z, x) with z = (x(1), . . . , x(k \u2212 1)) and x = x(k).\nLet x = (z, x) \u2208 (V )k and consider the IP(k, G) process {xIt }t\u22650 . Set R =\n{x}, P = \u2205 and W = V \\ (O(z) \u222a {x}) and recall from Proposition 5.2 that\nC\nC\nC\nthe chameleon process {(zC\nt , Rt , Pt , Wt )}t\u22650 satisfies\n(8.1.1)\n\n\u2200t \u2265 0, \u2200b = (c, b) \u2208 (V )k\n\nP(xIt = b) = E[I{zCt =c} inkxt (b)],\n\nwhere (as before) we use inkxt (*) to denote the amount of ink in this chameleon\nprocess corresponding to x. The following lemma relates the total amount\nof ink in this process to the near-uniformity of xIt conditionally on zIt .\nLemma 8.1. Given x = (z, x) \u2208 (V )k , let x\u0303It = (zIt , x\u0303It ) where, conditionally on zIt , x\u0303It is uniform over V \\ O(zIt ). Then\n\u0014\n\u0015\ninkxt\nI\nI\ndTV (L[xt ]), L[x\u0303t ]) \u2264 E 1 \u2212\nFill\n|V | \u2212 k + 1\nwhere Fill is the event defined in Lemma 7.1 (see also Remark 7.2).\nProof. We have seen that zIt and zC\nt have the same distribution; cf.\nthe proof of Proposition 5.2. We deduce that\n\u2200t \u2265 0, \u2200b = (c, b) \u2208 (V )k\n\nP(zIt = c, x\u0303It = b) =\n\nP(zC\nt = c)\n|V | \u2212 k + 1\n\n= P({zC\nt = c} \u2229 Fill),\nwhere the last equality follows from Lemma 7.2. On the other hand, (8.1.1)\nimplies\n(8.1.2) \u2200t \u2265 0, \u2200b = (c, b) \u2208 (V )k\n\nP(xIt = b) \u2265 E[I{zCt =c}\u2229Fill inkxt (b)].\n\n\f29\n\nMIXING OF SYMMETRIC EXCLUSION\n\nWe deduce that\n\u2200t \u2265 0, \u2200b = (c, b) \u2208 (V )k\n\n(P(zIt = c, x\u0303It = b) \u2212 P(zIt = c, xIt = b))+\n\u2264 (E[I{zCt =c}\u2229Fill (1 \u2212 inkxt (b))])+\n= E[I{zCt =c}\u2229Fill (1 \u2212 inkxt (b))]\n\nsince the integrand is \u2265 0.\n\nWe now combine this with formula (2.2.3) for dTV (*, **).\nX\ndTV (L[xIt ], L[x\u0303It ]) \u2264\nE[I{zCt =c}\u2229Fill (1 \u2212 inkxt (b))]\nb=(c,b)\u2208(V )k\n\n=\n\nX\n\nc\u2208(V )k\u22121\n\n[sum over b + (6.1.2)] =\n\nX\n\nc\u2208(V )k\u22121\n\n\u0014\n\nE I{zCt =c}\u2229Fill\n\nX\n\nb\u2208V \\O(c)\n\n(1 \u2212 inkxt (b))\n\n\u0015\n\nE[I{zC =c}\u2229Fill (|V | \u2212 k + 1 \u2212 inkxt )]\nt\n\n(sum over c) = E[IFill (|V | \u2212 k + 1 \u2212 inkxt )]\nE[inkxt |Fill]\n.\n(apply Lemma 7.1) = 1 \u2212\n|V | \u2212 k + 1\n\n\u0003\n\n8.2. Distance to the stationary distribution in terms of ink.\nWe will prove the following stronger inequality:\n\u0014\n\u0015\ninkw\nt\nI\nI\n(8.2.1) sup dTV (L[xt ], L[yt ]) \u2264 2k sup E 1 \u2212\nFill ,\n|V | \u2212 k + 1\nx,y\u2208(V )k\nw\u2208(V )k\nProof of Lemma 6.1.\n\nwhich implies the lemma by convexity.\nDeclare two states u, v \u2208 (V )k to be adjacent (u \u223c v) if they differ at\nprecisely one coordinate: that is, there exists an i \u2208 [k] with u(i) 6= v(i) and\nu(r) = v(r) for r \u2208 [k] \\ {i}. We first bound dTV (L[xIt ], L[ytI ]) for adjacent\nx \u223c y.\nOne can assume without loss of generality that x and y differ precisely at\nthe kth coordinate. Using the notation from Section 5.4, we write x = (z, x)\nand y = (z, y) for z \u2208 (V )k\u22121 and x \u2208 V \\ O(z). Defining x\u0303It = (zIt , x\u0303It ) as\nin Section 8.1 and \u1ef9tI similarly, we see that L[x\u0303It ] = L[\u1ef9tI ] for all t \u2265 0. We\ndeduce that\ndTV (L[xIt ], L[ytI ]) \u2264 dTV (L[xIt ], L[x\u0303It ]) + dTV (L[ytI ], L[\u1ef9tI ])\n+ dTV (L[x\u0303It ], L[\u1ef9tI ])\n\n(8.2.2)\n\n(3rd. term = 0) = dTV (L[xIt ], L[x\u0303It ]) + dTV (L[ytI ], L[\u1ef9tI ])\n\n\f30\n\nR. I. OLIVEIRA\n\n\u0015\ninkxt\nFill\n(use Lemma 8.1) \u2264 E 1 \u2212\n|V | \u2212 k + 1\n\u0015\n\u0014\ninkyt\nFill\n+E 1\u2212\n|V | \u2212 k + 1\n\u0014\n\u0015\ninkw\nt\n\u2264 2 sup E 1 \u2212\nFill .\n|V | \u2212 k + 1\nw\u2208(V )k\n\u0014\n\nNow consider x, y \u2208 (V )k arbitrary. One can find a sequence {x[i]}ri=0 \u2282\n(V )k with r \u2264 2k and\nx[0] = x \u223c x[1] \u223c x[2] \u223c * * * \u223c x[r] = y.\n\nThe triangle inequality gives\n\ndTV (L[xIt ], L[ytI ]) = dTV (L[x[0]It ], L[x[r]It ]) \u2264\n\nr\nX\ni=1\n\ndTV (L[x[i \u2212 1]It ], L[x[i]It ]).\n\nApplying (8.2.3) to each adjacent pair x[i \u2212 1], x[i] gives (8.2.1). \u0003\n9. Depinkings are fast. The results in this section lead to the key Lemma 6.2. We first show that, in the first two phases of the chameleon process-\na constant color and a color-changing phase-, the number of red particles\ndecreases in expectation by a constant factor.\nLemma 9.1 (Proven in Section 9.1). Consider a modified chameleon process where one drops condition (2) for a pinkening step; cf. Box 5.1. Assume\nalso that k \u2264 |V |/2, |V | \u2265 300 and that the initial state (z, R, P, W ) \u2208 Ck (V )\nwith |P | < |R| \u2264 |W |. If the phase length parameter T satisfies\nthen\nwhere c = 1/1000 > 0.\n\nT \u2265 20TIP(2,G) (1/4),\n\nC\nE[|R2T\n|] \u2264 (1 \u2212 c)|R|,\n\u2212\n\nWith this, we will show that the first depinking time has an exponential\nmoment.\nLemma 9.2 (Proven in Section 9.2). Consider a chameleon process (without the modification in the previous lemma) with |V | \u2265 300 and k \u2264 |V |/2,\nstarted from an initial state (z, R, P, W ) \u2208 Ck (V ) with |P | = \u2205. There exists\na universal constant K > 0 such that if the phase length parameter T satisfies\nT \u2265 20TIP(2,G) (1/4), the first depinking time D1 of this process satisfies\nE[eD1 /(KT ) ] \u2264 e.\n\nIn Section 9.3 we deduce Lemma 6.2 from Lemma 9.2.\n\n\f31\n\nMIXING OF SYMMETRIC EXCLUSION\n\n9.1. Loss of red particles in the two first phases.\nProof of Lemma 9.1. Note that there is no depinking at time t =\n0, since there are less pink particles than white or red ones in the state\n(z, R, P, W ). Finally, the conditions on P, W, R and k imply\n3|W | \u2265 |R| + |P | + |W | = |V | \u2212 k + 1 \u2265 |V |/2\n\n\u21d2\n\n|W | \u2265 |V |/6.\n\nThe interval (0, T ] is a constant-color phase where black, red and white\nparticles are simply moved around. Lemma 5.2 shows that the state of the\nprocess at time T is given by\nC\nC\nC\n(zC\nT , RT , PT , WT ) = (I(z), I(R), I(P ), I(W )),\n\nwhere I = I(0,T ] = IT is the map obtained from the modified chameleon\nconstruction in Section 5. We will need the following properties later on:\nProposition 9.1 (Proven in Section 9.1.1).\nS, L \u2282 V with S \u00d7 L \u2282 (V )2 , |L| \u2265 |V |/12,\n\nFor all (a, b) \u2208 (V )2 and\n\n\u0012\n\u0013\n|S|\n\u221210\n+2\nP((a, b) \u2208 I(S) \u00d7 I(S)) \u2264 P(a \u2208 I(S))\n.\n|V |\n\nP((a, b) \u2208 I(S) \u00d7 I(L)) \u2265\n\n|S|\n|S||L|\n(1 \u2212 2\u22129 ) \u2265\n.\n2\n|V |\n13|V |\n\nRemark 9.1. The intuitive meaning of this is that (RTC , WTC ) are close\nto uniform in terms of correlations of \"pairs of particles\" at the end of\nthe constant-color phase, and this will only hold because T = \u03a9(TIP(2,G) ).\nMorris's original argument for (Z/LZ)d could instead rely on good estimates\nfor transition probabilities for single-particle random walks. We note that\nwe need the negative correlation property in the proof of this proposition.\nIn the time interval (T, 2T ), each time T < \u03c4m < 2T may or may not be a\npinkening time, depending on whether pinkening condition (1) is satisfied.\nWe will nevertheless consider the maps\n(9.1.1) I \u0303t \u2261 I(T,t] ,\nT \u2264 t < 2T ; cf. the definition in Section 5.1.\nWe emphasize that I \u0303t does not correspond directly to the evolution of the\nchameleon process in the time interval (T, 2T ). Propositions 3.3 and 5.1\nimply:\nProposition 9.2 (Proof omitted). {I \u0303t }T <t<2T is independent from I,\nand so are all the points of the Poisson process {\u03c4n }n in the interval (T, 2T )\nand all markings en , cn corresponding to these points.\nWe need a new definition before we proceed. Let a \u2208 V be given. Let \u03c6a\nbe the first time of the form \u03c4m with T < \u03c4m \u2264 2T for which a \u2208 em ; if no\n\n\f32\n\nR. I. OLIVEIRA\n\nsuch time exists, let \u03c6a = +\u221e. If \u03c6a < +\u221e, there exists a vertex b \u2208 V such\nthat the edge em just mentioned has a = I \u0303\u03c6a \u2212 (a) and I \u0303\u03c6a \u2212 (b) as endpoints\nimmediately prior to time \u03c6a . We set Fa \u2261 b in that case, or Fa \u2261 \u2217 if\n\u03c6a = +\u221e. The following simple claim is essential to what follows.\nClaim 1. The number of pinkening steps performed in time interval\n(T, 2T ) is at least the number of b \u2208 I(W ) such that Fa = b for some a\u2208I(R).\nProof. Let b \u2208 I(W ). Given the rules for color-changing phases (cf.\nBox 5.1), the particle at that location will move in the time interval (T, 2T )\naccording to I \u0303t until the first time t = \u03c4m \u2208 (T, 2T ) such that I \u0303t\u2212 (b) \u2208 em and\nthe other endpoint of em is white (if such a time exists). Now if a \u2208 I(R)\nsatisfies Fa = b and \u03c4m = \u03c6a , we have {I \u0303t\u2212 (b), a} = em and a must still be\nred at time (\u03c6a )\u2212 , since it was not contained in an edge before in this phase.\nIt follows that the particle started from b must become pink by time \u03c6a . \u0003\nThe claim implies\nC\n|R2T\n| = |R| \u2212 # of pinkening steps in (T, 2T ]\n\u2212\nX\nISa\u2208I(R) {Fa =b} .\n\u2264 |R| \u2212\n\n(9.1.2)\n(9.1.3)\n\nb\u2208I(W )\n\nThe sum in the RHS satisfies\nX\nISa\u2208I(R) {Fa =b} \u2265\n\n(9.1.4)\n\nb\u2208I(W )\n\nX\n\nI{Fa =b}\n\na\u2208I(R),b\u2208I(W )\n\n\u2212\n\nX\n\nI{Fa =b} I{Fa\u2032 =b} ,\n\n{a,a\u2032 }\u2282I(R),b\u2208I(W )\n\nand we obtain\nC\nE[|R2T\n| \u2212 |R|]\n\u2212\nX\n(9.1.5)\n\u2264\u2212\n\n(a,b)\u2208(V )2\n\n+\n\nP(a \u2208 I(R), b \u2208 I(W ), Fa = b)\n\nX\n\n{a,a\u2032 }\u2282V,b\u2208V\n\nP(a, a\u2032 \u2208 I(R), b \u2208 I(W ), Fa = b, Fa\u2032 = b).\n\nThe event {Fa = b} is entirely determined by the points of the marked\nPoisson process and by the coin flips performed in the time interval (T, 2T ),\nand therefore is independent of I; cf. Proposition 9.2. We deduce\nX\nP(a \u2208 I(R), b \u2208 I(W ), Fa = b)\n(a,b)\u2208(V )2\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\nX\n\n=\n\n(a,b)\u2208(V )2\n\n(9.1.6)\n\n(use Proposition 9.1) \u2265\n=\n\n|R|\n13|V |\n\n33\n\nP(a \u2208 I(R), b \u2208 I(W ))P(Fa = b)\nX\n\nP(Fa = b)\n\n(a,b)\u2208(V )2\n\n|R| X\nP(Fa 6= \u2217).\n13|V |\na\u2208V\n\nFor a given a \u2208 V , P(Fa = \u2217) is the probability that there is no T < \u03c4n < 2T\nwith en \u220b a. Notice that this is at most the probability that I \u03032T (a) = a:\na cannot move if there is no edge en \u220b a with T < \u03c4n \u2264 2T . We deduce\nP(Fa 6= \u2217) \u2265 1 \u2212 P(I \u03032T (a) = a) = P(aR\nT 6= a),\n\nwhere {aR\nt }t\u22650 is a realization of RW(G) started from a. By the contraction\nprinciple and Proposition 2.1,\nT \u2265 20TIP(2,G) (1/4) \u2265 20TRW(G) (1/4) \u2265 TRW(G) (2\u221220 ),\nwhich implies\nP(aR\nT 6= a) \u2265 1 \u2212\n\n1\n13\n\u2212 2\u221220 \u2265\n|V |\n14\n\nsince |V | \u2265 300.\n\nWe deduce from (9.1.6) that\nX\n\n(9.1.7)\n\n(a,b)\u2208V 2\n\nP(a \u2208 I(R), b \u2208 I(W ), Fa = b) \u2265\n\n|R|\n.\n14\n\nWe now consider the second sum in the RHS of (9.1.5). As before, we\nnotice that {Fa = b, Fa\u2032 = b} is independent of I and therefore\nX\nP(a, a\u2032 \u2208 I(R), b \u2208 I(W ), Fa = b, Fa\u2032 = b)\n{a,a\u2032 }\u2282V,b\u2208V \\{a,a\u2032 }\n\nX\n\n=\n\n{a,a\u2032 }\u2282V,b\u2208V \\{a,a\u2032 }\n\n\u2264\n\nX\n\n{a,a\u2032 }\u2282V,b\u2208V \\{a,a\u2032 }\n\nP(a, a\u2032 \u2208 I(R), b \u2208 I(W ))P(Fa = b, Fa\u2032 = b)\nP(a, a\u2032 \u2208 I(R))P(Fa = b, Fa\u2032 = b).\n\nWe claim that:\nClaim 2 (Proven in Section 9.1.2).\n\nFor all (a, a\u2032 , b) \u2208 (V )3 ,\n\nP(Fa = b, Fa\u2032 = b) \u2264 P(Fa = a\u2032 , Fa\u2032 = b) + P(Fa\u2032 = a, Fa = b).\n\n\f34\n\nR. I. OLIVEIRA\n\nSumming up over b above gives at most P(Fa = a\u2032 ) + P(Fa\u2032 = a) in the\nRHS. Therefore,\nX\nP(a, a\u2032 \u2208 I(R))P(Fa = b, Fa\u2032 = b)\n\n{a,a\u2032 }\u2282V,b\u2208V \\{a,a\u2032 }\n\n\u2264\n\nX\n\n{a,a\u2032 }\u2282V\n\n=\n\nP(a, a\u2032 \u2208 I(R))(P(Fa = a\u2032 ) + P(Fa\u2032 = a))\n\nX\n\nP(a \u2208 I(R), a\u2032 \u2208 I(R))P(Fa = a\u2032 )\n\n(a,a\u2032 )\u2208(V )2\n\n(apply Prop. 9.1) =\n\n\u0012\n\n|R|\n+ 2\u221210\n|V |\n\n\u0012[\n\u0013 \u0012\n|R|\n\u2032\n{Fa = a } = {Fa 6= \u2217} =\n|V |\na\u2032\n\u0012\n|R|\n(P(Fa 6= \u2217) \u2264 1) \u2264\n|V |\n\u0012\n|R|\n=\n|V |\n\u0012\n|R|\n=\n|V |\n\n+ 2\u221210\n\n\u0013\n\nX\n\n(a,a\u2032 )\u2208(V )2\n\n\u0013X\na\u2208V\n\n\u221210\n\n+2\n\n\u221210\n\n+2\n\n\u221210\n\n+2\n\n\u0013X\n\u0013\n\u0013\n\na\u2208V\n\nP(a \u2208 I(R))P(Fa = a\u2032 )\n\nP(a \u2208 I(R))P(Fa 6= \u2217)\nP(a \u2208 I(R))\n\nE[|I(R)|]\n|R|\n\nsince I = I(0,T ] is a bijection.\n\nPlugging this equation and (9.1.7) into (9.1.5) we obtain\n\u0012\n\u0013\n|R|\n1\nC\n\u221210\nE[|R2T | \u2212 |R|] \u2264 |R|\n+2\n\u2212\n|V |\n14\n(9.1.8)\n\u2264 \u2212|R|/30\nif |R| \u2264 |V |/28.\nIf |R| > |V |/28, we can still find a subset R0 \u2282 R of size |R0 | = \u230a|V |/28\u230b.\nSince\nX\nX\n(9.1.9)\nI{\u2203a\u2208I(R):Fa =b} \u2265\nI{\u2203a\u2208I(R0 ):Fa =b} ,\nb\u2208I(W )\n\nb\u2208I(W )\n\nwe may repeat the reasoning presented from (9.1.4) onwards, replacing R\nby R0 , to deduce that\nC\nE[|R2T\n| \u2212 |R|] \u2264 \u2212\n\u2212\n\nWe now note that, since |V | \u2265 300,\n|R0 | \u2265\n\n|R0 |\n.\n30\n\n3|V | 3|R|\n|V |\n\u22121\u2265\n\u2265\n30\n100\n100\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n35\n\nsince |R| \u2264 |V |. We deduce that\n\n|R|\nif |R| > |V |/28,\n1000\nwhich gives the lemma together with (9.1.8). \u0003\nC\nE[|R2T\n| \u2212 |R|] \u2264 \u2212\n\u2212\n\n9.1.1. Proof of the required estimates for the I map (Proposition 9.1).\nProof of Proposition 9.1. Recall that T \u2265 20TIP(2,G) (1/4), therefore\nT \u2265 2TIP(2,G) (2\u221210 ) by Proposition 2.1. By the contraction principle [1], this\nalso implies that T \u2265 TRW(G) (2\u221210 ).\nRecall that I = I(0,T ] as in the construction of the modified chameleon\nprocess. This implies that for any set S, I(S) has the law of EX(|S|, G)\nstarted from S. We deduce\nP((a, b) \u2208 I(S) \u00d7 I(S)) = P({a, b} \u2282 STI )\n\n(negative correlation, Lemma 3.1) \u2264 P(a \u2208 STI )P(b \u2208 STI )\n\n(L[I] = L[I \u22121 ], Proposition 3.2) = P(a \u2208 I(S))P(bIT \u2208 S)\n\u0012\n\u0013\n|S|\n\u221210\n\u221210\n(T \u2265 TRW(G) (2 )) \u2264 P(a \u2208 I(S))\n+2\n.\n|V |\n\nAs for the other inequality in the proposition, we have\n\nP((a, b) \u2208 I(S) \u00d7 I(L)) = P((aIT , bIT ) \u2208 S \u00d7 L)\n(take x = (a, b)) = P(xIT \u2208 S \u00d7 L)\n(*) \u2265 (1 \u2212 2\u22129 )2\n\u2265 (1 \u2212 2\u22128 )\n\n|S \u00d7 L|\n|(V )2 |\n\n|S||L|\n,\n|V |2\n\nwhere (\u2217) follows from the symmetry of the transition rates of IP(2, G), the\nfact that T \u2265 2TIP(2,G) (2\u221210 ) and Proposition 2.2. We note that |L|/|V | \u2265\n(1/12) and 1 \u2212 2\u22128 \u2265 12/13 to finish the proof. \u0003\n9.1.2. Proof of claim on Fa (Claim 2).\nProof of Claim 2.\n(9.1.10)\n\nIt suffices to show that for (a, a\u2032 , b) \u2208 (V )3 ,\n\nP(Fa = b, Fa\u2032 = b, \u03c6a \u2264 \u03c6a\u2032 ) = P(Fa = b, Fa\u2032 = a, \u03c6a \u2264 \u03c6a\u2032 ).\n\nLet Lb , Rb denote the events appearing in the LHS and RHS of (9.1.10)\n(resp.). We present a simple measure-preserving mapping \u03a6, which acts on\n(P, {en }n , {cn }n , {di }i ),\n\n\f36\n\nR. I. OLIVEIRA\n\nthat maps Lb into Rb and vice-versa. We describe \u03a6 in words: all values of\ndi , T < \u03c4j \u2264 2T and all corresponding ej and cj , except for the following\nmodification: if \u03c4m = \u03c6a , we flip the value of cm to c\u2032m = 1 \u2212 cm .\nLet us check that \u03a6 has the desired properties. \u03a6 is clearly measurepreserving, since \u03c6a is a stopping time that is independent of the value cm\nof the flipped coin.\nNow suppose {I\u02c6t }T <t\u22642T is defined precisely as {I \u0303t }T <t\u22642T , but with cm\nflipped. It is easy to see that \u03c6a , \u03c6a\u2032 retain their values and that the random\nvariable F\u0302a corresponding to Fa in the I\u02c6 process satisfies F\u0302a = Fa . The two\nprocesses coincide for any time T < t < \u03c6a . If Lb holds, we have \u03c6a = \u03c4j < 2T\nfor some j, and the endpoints of ej are a and I \u0303\u03c6a \u2212 (Fa ) = I \u0303\u03c6a \u2212 (by definition\nof Fa ). Since the coin flips used for I \u0303\u03c6a and I\u02c6\u03c6a are opposite, we have\n(I\u02c6\u03c6a (a), I\u02c6\u03c6a (b)) = (I \u0303\u03c6a (b), I \u0303\u03c6a (a))\nwhereas I \u0303\u03c6a (c) = I\u02c6\u03c6a (c) for all c \u2208 V \\ {a, Fa }.\n\n(I\u02c6t (a), I\u02c6t (b)) = (I \u0303t (b), I \u0303t (a)).\n\nUnder Lb , \u2200t \u2208 [\u03c6a , 2T ]:\n\nUnder Lb , the edge el corresponding to \u03c4l = \u03c6a\u2032 was of the form el = {a\u2032 ,\nI \u0303\u03c6a\u2032 \u2212 (b)}. This implies el = {a\u2032 , I\u02c6\u03c6a\u2032 \u2212 (a)} in the I\u02c6t process, and the latter\nmust be in the event Rb . This shows that P(Lb ) \u2264 P(Rb ). The opposite\ninequality follows from reversing roles of the two processes. \u0003\n9.2. Estimate for the first depinking time (Lemma 9.2).\nProof of Lemma 9.2. As in Lemma 9.1 we drop condition (2) for a\ndepinking time, and notice that this change does not change the value (or\nthe distribution) of D1 . The modification to the process also does not affect\nthe end result of Lemma 5.1: that is, the discrete-time process starting from\n(z, R, P, W ) with subsequent states (\u1e91i , R\u0302i , P\u0302i , \u0174i ) described in that lemma\nis a time-homogeneous Markov chain, and D\u03021 \u2261 D1 /2T is a stopping time\nfor this process.\nMoreover, we assume without loss of generality that |R| \u2264 |W |, which\nimplies that |RtC | \u2264 |WtC | for t < D1 . This implies |WtC | \u2265 |V |/6 unless there\nare more pink particles than red ones at time t < D1 ; this follows from the\nreasoning in the beginning of the proof of Lemma 9.1 in Section 9.1. Recall\nthat each pinkening step remores a red particle and creates two pink ones.\nC\nIt follows that |R2iT\n| < 2|R|/3 implies D1 \u2264 2iT , and\n\u2212\n(9.2.1)\n\n\u2200i \u2208 N+\n\nP(D\u03021 > i) \u2264 P(|R\u0302i | \u2265 2|R|/3, D\u03021 > i \u2212 1)\n\u2264\n\n3E[R\u0302i I{D\u03021 >i} ]\n2|R|\n\n\f37\n\nMIXING OF SYMMETRIC EXCLUSION\n\n(9.2.2)\n\n=\n\n3E[E[R\u0302i |F\u0302i\u22121 ]I{D\u03021 >(i\u22121)} ]\n2|R|\n\n,\n\nwhere F\u0302i\u22121 is the \u03c3-field generated by (\u1e91l , R\u0302l , P\u0302l , \u0174l ) for l \u2264 i \u2212 1.\nWe now estimate the integrand in (9.2.2). Lemma 5.1 and its proof impliy\nthat\nE[|R\u0302i ||F\u0302i\u22121 ]\n\nis the expected number of red particles after a potential depinking, a constantcolor phase and a color-changing phase for a chameleon process started from\n(\u1e91i\u22121 , R\u0302i\u22121 , P\u0302i\u22121 , \u0174i\u22121 ) \u2208 Ck (V ).\n\nBy Lemma 9.1, we can ensure that\nE[|R\u0302i ||F\u0302i\u22121 ] \u2264 (1 \u2212 c)|R\u0302i\u22121 |\n\nif |\u0174i\u22121 | \u2265 |V |/6\n\nand\n\n|P\u0302i\u22121 | < |R\u0302i\u22121 |.\n\nAs noted before, these conditions are always satisfied in the event {D\u03021 >\n(i \u2212 1)}, because there are less pink than red particles. We deduce\n\u2200i \u2208 N+\n\nThis implies\n\nE[|R\u0302i |I{D\u03021 >i} ]\n|R|\n\n\u2264\n\nE[E[|R\u0302i ||F\u0302i\u22121 ]I{D\u03021 >(i\u22121)} ]\n|R|\n\n\u001b\n\u001a E[|R\u0302 |I\ni\u22121 {D\u03021 >i\u22121} ]\n\u2264 (1 \u2212 c)\n|R|\n\n(. . . induction. . . ) \u2264 (1 \u2212 c)i .\n\n3(1 \u2212 c)i\n,\nc = 1/1000 universal.\n2\nFrom this one can easily show that E[eD1 /KT ] \u2264 e for some universal K. \u0003\nP(D1 > 2iT ) = P(D\u03021 > i) \u2264\n\n9.3. Proof of Lemma 6.2.\nProof. Fix x \u2208 (V )k . We first prove that\n(9.3.1)\n\nE[eDj (x)/(KT ) ] \u2264 ej ,\n\nK > 0 from Lemma 9.2;\n\nthis is the bound we wish to obtain except that we are not conditioning on\nFill.\nWe proceed as in the previous proof and consider the discrete-time process\n{(\u1e91i , R\u0302i , P\u0302i , \u0174i )}i\u22650 ,\n\nintroduced in Lemma 5.1, henceforth called the hat process. This time we\ntake the initial state\n(z, R, P, W ) \u2261 (z, {x}, \u2205, V \\ (O(z) \u222a {x}))\n\n\f38\n\nR. I. OLIVEIRA\n\ncorresponding to x = (z, x) in the sense of Proposition 5.2. Also recall the\ndefinition D\u0302i \u2261 Di (x)/2T and note that (9.3.1) is equivalent to\n\u2032\n\nE[eD\u0302j /K ] \u2264 ej ,\n\n(9.3.2)\n\nK \u2032 = 2K.\n\nThis is valid for j = 1 due to Lemma 9.2. For j > 1, we recall the definition\nof the \u03c3-fields F\u0302i , recall that D\u0302j\u22121 is a stopping time for the hat process (cf.\nLemma 5.1) and obtain\n(9.3.3)\n\n\u2032\n\n\u2032\n\n\u2032\n\nE[eD\u0302j /K ] \u2264 E[eD\u0302j\u22121 /K E[e(D\u0302j \u2212D\u0302j\u22121 )/K |F\u0302D\u0302j\u22121 ]].\n\nWe will apply the strong Markov property of the hat process (cf. Lemma 5.1\nagain) to bound the conditional expectation in the RHS. The conditional\nlaw of D\u0302j \u2212 D\u0302j\u22121 given F\u0302D\u0302j\u22121 is the law of the hat process started from\nstate\n(\u1e91D\u0302j\u22121 , R\u0302D\u0302j\u22121 , P\u0302D\u0302j\u22121 , \u0174D\u0302j\u22121 ).\nNotice that P\u0302D\u0302j\u22121 = PDCj\u22121 6= \u2205; in fact, since depinking occurs at time\n\u2212\n\nC\nDj\u22121 , we know that |PDCj\u22121 | \u2265 min{|RD\n|, |WDCj\u22121 |}. However, at time\nj\u22121 \u2212\n\u2212\n\u2212\nDj\u22121 all pink particles disappear: the hat process evolves as if started from\na state with no pink particles, and D\u0302j \u2212 D\u0302j\u22121 is the first depinking time for\nthe hat process with this modified initial state. We deduce from Lemma 9.2\nthat\n\u2032\n\nE[e(D\u0302j \u2212D\u0302j\u22121 )/K |F\u0302D\u0302j\u22121 ] \u2264 e\n\nalmost surely,\n\nso that\n(9.3.4)\n\n\u2032\n\n\u2032\n\nE[eD\u0302j /K ] \u2264 E[eD\u0302j\u22121 /K ]e \u2264 ej\n\nby induction.\n\nThis proves (9.3.3) and (9.3.1).\nTo prove the lemma, notice that conditioning on Fill simply biases the coin\nflips di performed at depinking times; cf. Remark 7.3. This will not change\nthe distribution of D\u03021 or the conditional distribution of D\u0302j \u2212 D\u0302j\u22121 given the\npast of the process, so the argument we presented above still applies. \u0003\n10. Final remarks. Our paper leaves many questions open. Here we\npresent a few problems that seem especially interesting:\n\u2022 Are there any other interacting particle systems whose mixing parameters\ncan be bounded solely in terms of the constituent parts? Nonsymmetric\nexclusion is an obvious candidate. Another is the zero-range process. Morris [18] used the comparison principle and a coupling argument on the\ncomplete graph to bound the spectral gap of this process on a grid. Can\none do something less indirect over an arbitrary graph?\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n39\n\n\u2022 Can we find a mixing time upper bound of IP(|V |, G) (i.e., as many particles as vertices), that is, similar to our main Theorem? Inspection of the\nchameleon process shows that it gives the conditional distribution of a\nparticle given the whole past trajectory of the other particles. This means,\nin particular, that it cannot deal with k = |V | particles.\n\u2022 Recall the heuristic assumption in the Introduction: TEX(k,G) (\u03b5) \u2264 C1 \u00d7\nTRW(G) (\u03b5/k) with C1 > 0 universal. Is this actually true? This would be\nstronger than our main theorem.\n\u2022 Combining the previous two items: is it true that TIP(|V |,G)(\u03b5) = C1 \u00d7\nTRW(G) (\u03b5/|V |)? Could it even be possible that TIP(|V |,G)(\u03b5) \u2264 TRW(|V |,G) (\u03b5),\nthat is, the interchange process mixes at least as fast as independent random walkers? This would give Aldous's (now proven) conjecture on the\nspectral gap as a corollary.\nAPPENDIX A: MIXING BOUNDS FOR EX(K, G) VIA CANONICAL\nPATHS\nWe use asymptotic notation below as shorthand; see, for example, [2] for\nprecise definitions. Let G = (V, E, {we }e\u2208E ) be a weighted graph. It seems\nthat the best general bound that was previously available (implicitly) for the\nmixing time of EX(k, G) comes from the combination of three ingredients.\nMixing time from Log-Sobolev constant. The state space of EX(k, G) has\n\u0001\ncardinality |Vk | = 2\u0398(|V |) if k = \u0398(|V |). By the results of [9], if \u03c1EX(k,G) is\nthe log-Sobolev constant of EX(k, G), then\n\u0012\n\u0013\nln |V |\nTEX(k,G) (1/4) = O\nfor k = \u0398(|V |).\n\u03c1EX(k,G)\nLog-Sobolev inequality for the Bernoulli\u2013Laplace model. Consider the complete graph K|V | where each edge has weight 1/|V |. EX(k, K|V | ) is the socalled Bernoulli\u2013Laplace model with k particles, whose log Sobolev constant\nis of the order \u0398(ln(|V |2 /k(|V |\u2212 k))). Notice that this is \u0398(1) for k = \u0398(|V |).\nComparison argument. Now consider a general weighted graph G = (V, E,\n{we }e\u2208E ). Assume that for each pair (x, y) \u2208 V 2 one has defined a path \u03b3x,y\nin G connecting x to y. For each such pair, let Ix,y (e) = 1 if e is crossed\nby \u03b3x,y and 0 otherwise, and also let lx,y denote the length of \u03b3x,y . Finally,\ndefine\nX lx,y Ix,y (e)\n\u03c6(G) \u2261 max\n.\ne\u2208E\n|V |we\n2\n(x,y)\u2208V\n\nIt is shown in the proof of [8, Theorem 3.1] that this comparison constant\nfor the Dirichlet forms of RW(G) can be \"lifted\" with no loss to EX(k, G).\n\n\f40\n\nR. I. OLIVEIRA\n\nThe comparison principle for the log Sobolev constant [9] implies \u03c1EX(k,G) =\n\u03a9(\u03c6\u22121 (G)). We deduce\n(A.0.1)\n\nTEX(k,G) (1/4) = O(\u03c6(G) ln |V |)\n\nif k = \u0398(|V |).\n\nIt can be very hard to find good upper bounds on \u03c6(G) in general, but\nthe general lower bound we will present implies that\n\u03c6(G) \u2265\n\n(A.0.2)\n\n2dist2\n,\nd\n\nwhere dist2 is the average over all (x, y) \u2208 V 2 of the square of the graphtheoretic distance between x and y, and d is the average (weighted) degree\nin G. Indeed, it suffices to see that\n\u0013\n\u0012 X\nX\nIx,y (e)lx,y\nwe\nP\n\u03c6(G) \u2265\n|V |we\nf \u2208E wf\n2\ne\u2208E\n\n[use\n\nP\n\n=P\n\n(x,y)\u2208V\n\n1\n\nf \u2208E\n\nwf\n\n\u0014 X \u0012P\n\ne\u2208E Ix,y (e)lx,y\n\n(x,y)\u2208V 2\n\n1\nP\ne Ix,y (e) = lx,y ] =\n\u22121\n|V |\nf \u2208E wf\n\n1\nP\n[use lx,y \u2265 dist(x, y)] \u2265\n|V |\u22121 f \u2208E wf\n\n\u0014 X\n\n(x,y)\u2208V\n\n|V |\nl2x,y\n|V |2\n2\n\n\u0013\u0015\n\n\u0015\n\n\u0015\ndist(x, y)2\n2dist2\n.\n=\n|V |2\nd\n(x,y)\u2208V 2\n\n\u0014 X\n\nWe note that this is a lower bound, which we do not know how to achieve\nin the examples in Table 1.\nAPPENDIX B: THE TRAJECTORY OF [\ninkJ GIVEN Fill\nWe use the facts proven in Section 7 to derive the technical estimate in\nProposition 6.1 in the proof of Lemma 1.2; cf. Section 6.1.\nProof of Proposition 6.1. We take the notation in Section 7 for\ngranted, but omit the superscript w in this proof. Our first goal will be to\nc j }j\u22650 is still a Markov chain. Repeating\nshow that, conditionally on Fill, {ink\nthe steps of the proof of Lemma 7.1, we note that\nc i )i\u2264j ) =\nP(Fill|(ink\n\nc \u221e |(ink\nc i )i\u2264j ]\ncj\nE[ink\nink\nc j.\n=\n= P(Fill)ink\n|V | \u2212 k + 1\n|V | \u2212 k + 1\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n41\n\nWe deduce from Bayes's rule and the Markovian property that\n!\n!\nj\nj\n\\\n\\\nc i = ai }|Fill = P\nc i = ai } aj\nP\n{ink\n{ink\ni=1\n\ni=1\n\nc j ) = p(1, a1 )p(a1 , a2 ) * * * p(aj\u22121 , aj )aj\n(Markov property for ink\n= q(1, a1 ) * * * q(aj\u22121 , aj ),\n\nwhere\nq(a, b) =\n\nbp(a, b)\na\n\nif a 6= 0.\n\nc j does not visit 0 in the event Fill, we do not need to\nNotice that, since ink\ndefine q(a, b) for a = 0. We have shown:\n\nc j }j\u22650 is\nProposition B.1. Conditionally on Fill, the trajectory of {ink\nthat of a Markov chain in {1, . . . , |V | \u2212 k + 1}, with transition rates q(a, b)\nc 0 = 1.\nand started from ink\n\nFor the remainder of the proof, we will use this proposition to bound\nc l /(|V | \u2212 k + 1). Actually, another quantity is easier to bound. Set\n1 \u2212 ink\nc l /(|V | \u2212 k + 1) and\nIl = ink\np\nmin{1 \u2212 Il , Il }\n.\nZl \u2261\nIl\n\nNotice that conditionally on Fill, Il > 0 always, hence Zl is a.s. well defined\nfor all l. Moreover, one can check that 1 \u2212 Il \u2264 Zl always. Therefore the\nlemma will follow from the estimate\np\nEFill [Zl ] \u2264 (71/72)l |V | \u2212 k + 1,\nwhere EFill [*] corresponds to an expectation\nwith respect to the conditional\np\ndistribution given Fill. Since Z0 = |V | \u2212 k + 1, the above estimate follows\ndirectly from the following claim.\nClaim 3.\n\u2200l \u2208 N\n\nEFill [Zl ] \u2264 (71/72)EFill [Zl\u22121 ].\n\nTherefore, proving this claim will finish the proof.\nc i,\nTo prove the claim, we first note that for all i, Zi is a function of ink\nand Zl\u22121 = 0 \u21d2 Zl = 0. We deduce\n\u0015\n\u0015\n\u0014\n\u0014\nZl\nFill\nFill\nFill\nc l\u22121 Zl\u22121 I{Z 6=0} .\nink\n(B.0.1)\nE [Zl ] = E\nE\nl\u22121\nZl\u22121\n\n\f42\n\nR. I. OLIVEIRA\n\nWe now bound the conditional expectation in the RHS. We may assume\nc l\u22121 = r with 0 < r < |V | \u2212 k + 1 (otherwise Zl\u22121 = 0). Thus we wish\nthat ink\nto bound\n\u0015\n\u0014\nZl\nFill\nc\nE\ninkl\u22121 = r ,\n1 \u2264 r \u2264 |V | \u2212 k.\nZl\u22121\n\nIf we note that\np\np\nc l\u22121\nmin{1 \u2212 Il , Il }\nmin{1 \u2212 Il , Il }\nIl\u22121\nink\nZl\n\u00d7\n\u00d7\n=p\n=p\ncl\nZl\u22121\nIl\nmin{1 \u2212 Il\u22121 , Il\u22121 }\nmin{1 \u2212 Il\u22121 , Il\u22121 }\nink\np\nand define f (a) = min{a, |V | \u2212 k + 1 \u2212 a}, we see that\n\u0015\n\u0015\n\u0014\n\u0014\nc l)\nc l\u22121\nf (ink\nZl c\nink\nc l\u22121 = r\nink\ninkl\u22121 = r = EFill\nEFill\n\u00d7\ncl\nc l\u22121 )\nZl\u22121\nink\nf (ink\nX\nf (s) r\n(use Proposition B.1) =\nq(r, s)\n\u00d7\nf (r) s\ns\n[use formula for q(*, **)] =\n\nX\ns\n\np(r, s)\n\nf (s)\nf (r)\n\nc j }j\u22650 process.\nwhere p(*, **) are the transition rates of the unconditional {ink\nUsing the formula for these, we obtain\n\u0014\n\u0015\n\u0012\n\u0013\nZl\nc l\u22121 = r = 1 f (r + \u2206(r)) + f (r \u2212 \u2206(r)) .\n(B.0.2) EFill\nink\nZl\u22121\n2\nf (r)\nRecall the formula for \u2206(r) (cf. Proposition 7.3),\n\u0019\n\u0018\nmin{r, |V | \u2212 k + 1 \u2212 r}\n.\n\u2206(r) \u2261\n3\n\nWe now split the analysis of the RHS of this in two\n\u221a cases.\nCase 1: 1 \u2264 r \u2264 (|V | \u2212 k + 1)/2. In this case p\nf (r) = r and \u2206(r) = \u2308r/3\u2309 \u2265\nr/3. We use the upper bound f (r \u00b1 \u2206(r)) \u2264 r \u00b1 \u2206(r) to obtain\n!\nr\nr\n\u0014\n\u0015\nZ\n1\n\u2206(r)\n\u2206(r)\nl\nc l\u22121 = r =\n1\u2212\n(B.0.3) EFill\n.\n+ 1+\nink\nZl\u22121\n2\nr\nr\n\u221a\n\u221a\nRecall the bound \" 1 \u2212 x + 1 + x \u2264 2(1 \u2212 x2 /8),\" valid for all 0 \u2264 x \u2264 1;\nthis can be checked by squaring both sides of the inequality. In our case, we\napply this with x = \u2206(r)/r \u2265 1/3 and deduce\n\u0015\n\u0012\n\u00132\n\u0014\n\u2206(r)\n71\n1\nZl\nFill\nc l\u22121 = r = 1 \u2212\n(B.0.4)\n\u2264 .\nink\nE\nZl\u22121\n8\nr\n72\n\n\fMIXING OF SYMMETRIC EXCLUSION\n\n43\n\nCase 2: (|V | \u2212 k + 1)/2 < r \u2264 |V | \u2212 k. In this case (B.0.3) holds with r \u2032 =\n|V | \u2212 k + 1 \u2212 r replacing r. Similar calculations imply that the conditional\nexpectation is also \u2264 71/72 in this case.\nThus we see that in both cases\n\u0015\n\u0014\n71\nZl\nFill\nc\ninkl\u22121 = r \u2264 .\nE\nZl\u22121\n72\nPlugging this into (B.0.1) gives\n\n71\n71 Fill\nE [Zl\u22121 I{Zl\u22121 6=0} ] = EFill [Zl\u22121 ],\n72\n72\nwhich completes the proof. \u0003\nEFill [Zl ] \u2264\n\nAcknowledgments. We thank Ton Dieker and Prasad Tetali for useful\ndiscussions on the exposition. We also thank an anonymous referee for a long\nlist of typos in the previous version, as well as for numerous suggestions.\nREFERENCES\n[1] Aldous, D. and Fill, J. A. Reversible Markov Chains and random walks on\ngraphs. Book draft. Available at http://www.stat.berkeley.edu/~aldous/\nRWG/book.html.\n[2] Alon, N. and Spencer, J. H. (2000). The Probabilistic Method, 2nd ed. Wiley, New\nYork. MR1885388\n[3] Andjel, E. D. (1988). A correlation inequality for the symmetric exclusion process.\nAnn. Probab. 16 717\u2013721. MR0929073\n[4] Benjamini, I. and Mossel, E. (2003). On the mixing time of a simple random\nwalk on the super critical percolation cluster. Probab. Theory Related Fields 125\n408\u2013420. MR1967022\n[5] Caputo, P. and Faggionato, A. (2007). Isoperimetric inequalities and mixing time\nfor a random walk on a random point process. Ann. Appl. Probab. 17 1707\u20131744.\nMR2358639\n[6] Caputo, P., Liggett, T. M. and Richthammer, T. (2010). Proof of Aldous' spectral gap conjecture. J. Amer. Math. Soc. 23 831\u2013851. MR2629990\n[7] Cooper, C., Frieze, A. and Radzik, T. (2009). Multiple random walks and interacting particle systems. In Automata, Languages and Programming. Part II.\nLecture Notes in Computer Science 5556 399\u2013410. Springer, Berlin. MR2544812\n[8] Diaconis, P. and Saloff-Coste, L. (1993). Comparison theorems for reversible\nMarkov chains. Ann. Appl. Probab. 3 696\u2013730. MR1233621\n[9] Diaconis, P. and Saloff-Coste, L. (1996). Logarithmic Sobolev inequalities for\nfinite Markov chains. Ann. Appl. Probab. 6 695\u2013750. MR1410112\n[10] Dieker, A. B. (2010). Interlacings for random walks on weighted graphs and the\ninterchange process. SIAM J. Discrete Math. 24 191\u2013206. MR2600660\n[11] Fountoulakis, N. and Reed, B. A. (2008). The evolution of the mixing rate of\na simple random walk on the giant component of a random graph. Random\nStructures Algorithms 33 68\u201386. MR2428978\n[12] Lee, T.-Y. and Yau, H.-T. (1998). Logarithmic Sobolev inequality for some models\nof random walks. Ann. Probab. 26 1855\u20131873. MR1675008\n\n\f44\n\nR. I. OLIVEIRA\n\n[13] Levin, D. A., Peres, Y. and Wilmer, E. L. (2009). Markov Chains and Mixing\nTimes. Amer. Math. Soc., Providence, RI. MR2466937\n[14] Liggett, T. M. (1974). A characterization of the invariant measures for an infinite\nparticle system with interactions. II. Trans. Amer. Math. Soc. 198 201\u2013213.\nMR0375531\n[15] Liggett, T. M. (1985). Interacting Particle Systems. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] 276.\nSpringer, New York. MR0776231\n[16] Liggett, T. M. (1999). Stochastic Interacting Systems: Contact, Voter and Exclusion Processes. Grundlehren der Mathematischen Wissenschaften [Fundamental\nPrinciples of Mathematical Sciences] 324. Springer, Berlin. MR1717346\n[17] Montenegro, R. and Tetali, P. (2006). Mathematical aspects of mixing times in\nMarkov chains. Found. Trends Theor. Comput. Sci. 1 x+121. MR2341319\n[18] Morris, B. (2006). Spectral gap for the zero range process with constant rate. Ann.\nProbab. 34 1645\u20131664. MR2271475\n[19] Morris, B. (2006). The mixing time for simple exclusion. Ann. Appl. Probab. 16\n615\u2013635. MR2244427\n[20] Morris, B. (2009). Improved mixing time bounds for the Thorp shuffle and L-reversal chain. Ann. Probab. 37 453\u2013477. MR2510013\n[21] Morris, B. and Peres, Y. (2005). Evolving sets, mixing and heat kernel bounds.\nProbab. Theory Related Fields 133 245\u2013266. MR2198701\n[22] Pete, G. (2008). A note on percolation on Zd : Isoperimetric profile via exponential\ncluster repulsion. Electron. Commun. Probab. 13 377\u2013392. MR2415145\n[23] Yau, H.-T. (1997). Logarithmic Sobolev inequality for generalized simple exclusion\nprocesses. Probab. Theory Related Fields 109 507\u2013538. MR1483598\nIMPA\nEstrada Dona Castorina, 110\nRio de Janeiro, RJ 22460-320\nBrazil\nE-mail: rimfo@impa.br\n\n\f"}