{"id": "http://arxiv.org/abs/0902.4838v1", "guidislink": true, "updated": "2009-02-27T14:10:26Z", "updated_parsed": [2009, 2, 27, 14, 10, 26, 4, 58, 0], "published": "2009-02-27T14:10:26Z", "published_parsed": [2009, 2, 27, 14, 10, 26, 4, 58, 0], "title": "Consistencies and rates of convergence of jump-penalized least squares\n  estimators", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0902.2374%2C0902.1746%2C0902.0498%2C0902.3250%2C0902.4017%2C0902.3403%2C0902.3461%2C0902.2443%2C0902.4122%2C0902.2411%2C0902.2428%2C0902.2211%2C0902.1717%2C0902.4755%2C0902.2361%2C0902.3483%2C0902.0199%2C0902.4191%2C0902.1416%2C0902.0715%2C0902.3279%2C0902.0322%2C0902.2643%2C0902.4656%2C0902.3422%2C0902.3742%2C0902.1528%2C0902.2152%2C0902.4779%2C0902.4838%2C0902.3836%2C0902.0487%2C0902.1496%2C0902.3998%2C0902.2235%2C0902.3048%2C0902.2682%2C0902.0204%2C0902.4235%2C0902.3984%2C0902.1437%2C0902.3363%2C0902.2776%2C0902.2334%2C0902.4601%2C0902.1296%2C0902.0719%2C0902.2340%2C0902.1227%2C0902.2818%2C0902.1051%2C0902.3865%2C0902.3631%2C0902.1707%2C0902.4651%2C0902.2399%2C0902.2668%2C0902.3497%2C0902.2728%2C0902.2782%2C0902.0753%2C0902.2768%2C0902.0643%2C0902.3854%2C0902.2473%2C0902.1635%2C0902.3552%2C0902.3699%2C0902.3006%2C0902.3379%2C0902.1819%2C0902.2706%2C0902.0481%2C0902.3840%2C0902.4893%2C0902.1027%2C0902.2699%2C0902.0171%2C0902.2175%2C0902.2652%2C0902.3968%2C0902.0741%2C0902.3747%2C0902.3553%2C0902.2790%2C0902.1314%2C0902.2856%2C0902.0276%2C0902.0757%2C0902.3902%2C0902.1874%2C0902.3777%2C0902.1345%2C0902.1082%2C0902.0697%2C0902.2891%2C0902.0314%2C0902.4212%2C0902.1507%2C0902.1989%2C0902.3205&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Consistencies and rates of convergence of jump-penalized least squares\n  estimators"}, "summary": "We study the asymptotics for jump-penalized least squares regression aiming\nat approximating a regression function by piecewise constant functions. Besides\nconventional consistency and convergence rates of the estimates in $L^2([0,1))$\nour results cover other metrics like Skorokhod metric on the space of\nc\\`{a}dl\\`{a}g functions and uniform metrics on $C([0,1])$. We will show that\nthese estimators are in an adaptive sense rate optimal over certain classes of\n\"approximation spaces.\" Special cases are the class of functions of bounded\nvariation (piecewise) H\\\"{o}lder continuous functions of order $0<\\alpha\\le1$\nand the class of step functions with a finite but arbitrary number of jumps. In\nthe latter setting, we will also deduce the rates known from change-point\nanalysis for detecting the jumps. Finally, the issue of fully automatic\nselection of the smoothing parameter is addressed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0902.2374%2C0902.1746%2C0902.0498%2C0902.3250%2C0902.4017%2C0902.3403%2C0902.3461%2C0902.2443%2C0902.4122%2C0902.2411%2C0902.2428%2C0902.2211%2C0902.1717%2C0902.4755%2C0902.2361%2C0902.3483%2C0902.0199%2C0902.4191%2C0902.1416%2C0902.0715%2C0902.3279%2C0902.0322%2C0902.2643%2C0902.4656%2C0902.3422%2C0902.3742%2C0902.1528%2C0902.2152%2C0902.4779%2C0902.4838%2C0902.3836%2C0902.0487%2C0902.1496%2C0902.3998%2C0902.2235%2C0902.3048%2C0902.2682%2C0902.0204%2C0902.4235%2C0902.3984%2C0902.1437%2C0902.3363%2C0902.2776%2C0902.2334%2C0902.4601%2C0902.1296%2C0902.0719%2C0902.2340%2C0902.1227%2C0902.2818%2C0902.1051%2C0902.3865%2C0902.3631%2C0902.1707%2C0902.4651%2C0902.2399%2C0902.2668%2C0902.3497%2C0902.2728%2C0902.2782%2C0902.0753%2C0902.2768%2C0902.0643%2C0902.3854%2C0902.2473%2C0902.1635%2C0902.3552%2C0902.3699%2C0902.3006%2C0902.3379%2C0902.1819%2C0902.2706%2C0902.0481%2C0902.3840%2C0902.4893%2C0902.1027%2C0902.2699%2C0902.0171%2C0902.2175%2C0902.2652%2C0902.3968%2C0902.0741%2C0902.3747%2C0902.3553%2C0902.2790%2C0902.1314%2C0902.2856%2C0902.0276%2C0902.0757%2C0902.3902%2C0902.1874%2C0902.3777%2C0902.1345%2C0902.1082%2C0902.0697%2C0902.2891%2C0902.0314%2C0902.4212%2C0902.1507%2C0902.1989%2C0902.3205&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study the asymptotics for jump-penalized least squares regression aiming\nat approximating a regression function by piecewise constant functions. Besides\nconventional consistency and convergence rates of the estimates in $L^2([0,1))$\nour results cover other metrics like Skorokhod metric on the space of\nc\\`{a}dl\\`{a}g functions and uniform metrics on $C([0,1])$. We will show that\nthese estimators are in an adaptive sense rate optimal over certain classes of\n\"approximation spaces.\" Special cases are the class of functions of bounded\nvariation (piecewise) H\\\"{o}lder continuous functions of order $0<\\alpha\\le1$\nand the class of step functions with a finite but arbitrary number of jumps. In\nthe latter setting, we will also deduce the rates known from change-point\nanalysis for detecting the jumps. Finally, the issue of fully automatic\nselection of the smoothing parameter is addressed."}, "authors": ["Leif Boysen", "Angela Kempe", "Volkmar Liebscher", "Axel Munk", "Olaf Wittich"], "author_detail": {"name": "Olaf Wittich"}, "author": "Olaf Wittich", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/07-AOS558", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0902.4838v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0902.4838v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/07-AOS558 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62G05, 62G20 (Primary) 41A10, 41A25 (Secondary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0902.4838v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0902.4838v1", "journal_reference": "Annals of Statistics 2009, Vol. 37, No. 1, 157-183", "doi": "10.1214/07-AOS558", "fulltext": "arXiv:0902.4838v1 [math.ST] 27 Feb 2009\n\nThe Annals of Statistics\n2009, Vol. 37, No. 1, 157\u2013183\nDOI: 10.1214/07-AOS558\nc Institute of Mathematical Statistics, 2009\n\nCONSISTENCIES AND RATES OF CONVERGENCE OF\nJUMP-PENALIZED LEAST SQUARES ESTIMATORS\nBy Leif Boysen,1 Angela Kempe,2 Volkmar Liebscher,3\nAxel Munk4 and Olaf Wittich3\nUniversit\u00e4t G\u00f6ttingen, GSF\u2013National Research Centre for Environment,\nUniversit\u00e4t Greifswald, Universit\u00e4t G\u00f6ttingen and\nTechnische Universiteit Eindhoven\nWe study the asymptotics for jump-penalized least squares regression aiming at approximating a regression function by piecewise\nconstant functions. Besides conventional consistency and convergence\nrates of the estimates in L2 ([0, 1)) our results cover other metrics like\nSkorokhod metric on the space of c\u00e0dl\u00e0g functions and uniform metrics on C([0, 1]). We will show that these estimators are in an adaptive sense rate optimal over certain classes of \"approximation spaces.\"\nSpecial cases are the class of functions of bounded variation (piecewise) H\u00f6lder continuous functions of order 0 < \u03b1 \u2264 1 and the class\nof step functions with a finite but arbitrary number of jumps. In the\nlatter setting, we will also deduce the rates known from change-point\nanalysis for detecting the jumps. Finally, the issue of fully automatic\nselection of the smoothing parameter is addressed.\n\n1. Introduction. We consider regression models of the form\n(1)\n\nn\n\nYin = f i + \u03bein ,\n\ni = 1, . . . , n,\n\nReceived September 2006; revised September 2007.\nSupported by the Georg Lichtenberg program \"Applied Statistics and Empirical Methods\" and DFG Graduate Program 1023, \"Identification in Mathematical Models.\"\n2\nSupported by DFG, Priority Program 1114, \"Mathematical Methods for Time Series\nAnalysis and Digital Image Processing.\"\n3\nSupported in part by DFG, Sonderforschungsbereich 386 \"Statistical Analysis of Discrete Structures.\"\n4\nSupported by DFG Grant \"Statistical Inverse Problems under Qualitative Shape Constraints.\"\nAMS 2000 subject classifications. Primary 62G05, 62G20; secondary 41A10, 41A25.\nKey words and phrases. Jump detection, adaptive estimation, penalized maximum likelihood, approximation spaces, change-point analysis, multiscale resolution analysis, Potts\nfunctional, nonparametric regression, regressogram, Skorokhod topology, variable selection.\n1\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2009, Vol. 37, No. 1, 157\u2013183. This reprint differs from the original in pagination\nand typographic detail.\n1\n\n\f2\n\nL. BOYSEN ET AL.\n\nwhere (\u03bein )n\u2208N,1\u2264i\u2264n is a triangular scheme of independent zero-mean subn\nGaussian random variables and f i is the mean value of a square integrable function f \u2208 L2 ([0, 1)) over an appropriate interval [xni\u22121 , xni ] [see,\ne.g., Donoho (1997)]\n(2)\n\nn\n\nf i = (xni \u2212 xni\u22121 )\u22121\n\nZ\n\nxn\ni\n\nxn\ni\u22121\n\nf (u) du.\n\nFor ease of notation, we will mostly suppress the dependency on n in the\nsequel.\nWhen trying to recover the characteristics of the regression function in applications, we frequently face situations where the most striking features are\nsharp transitions, called change points, edges or jumps [for data\nexamples see Fredkin and Rice (1992), Christensen and Rudemo (1996),\nBraun, Braun and M\u00fcller (2000)]. To capture these features, in this paper\nwe study a reconstruction of the original signal by step functions, which\nresults from a least squares approximation of Y = (Y1 , . . . , Yn ) penalized\nby the number of jumps. More precisely, we consider minimizers T\u03b3 (Y ) \u2208\narg min H\u03b3 (*, Y ) of the Potts functional\n(3)\n\nn\n1X\nH\u03b3 (u, Y ) =\n(ui \u2212 Yi )2 + \u03b3 * #J(u).\nn i=1\n\nHere J(u) = {i : 1 \u2264 i \u2264 n \u2212 1, ui 6= ui+1 } is the set of jumps of u \u2208 Rn . Note\nthat the minimizer is not necessarily unique.\nThe name Potts functional refers to a model which is well known in statistical mechanics and was introduced by Potts (1952) as a generalization\nof the Ising model [Ising (1925)] for a binary spin system to more than two\nstates. The original model was considered in the context of Gibbs fields with\nenergy equal to the above penalty.\nVarious other strategies dealing with discontinuities are known in the literature. Kernel regression as (linear) nonparametric method offers various\nways to identify jumps in the regression function, essentially by estimating modes of the derivative; see, for example, Hall and Titterington (1992),\nLoader (1996), M\u00fcller (1992) or M\u00fcller and Stadtm\u00fcller (1999). Other approaches like local M -smoothers [Chu et al. (1998)], sigma-filter\n[Godtliebsen, Spj\u00f8tvoll and Marron (1997)], chains of sigma-filters\n[Aurich and Weule (1995)] or adaptive weights smoothing [Spokoiny (1998),\nPolzehl and Spokoiny (2003)] are based on nonlinear averages which mimic\nrobust W -estimators [cf. Hampel et al. (1986)] near discontinuities. Therefore, they do not blur the jump as much as linear methods would do.\nThe case when the regression function is a step function has been studied\nfirst by Hinkley (1970) and later by Yao (1988) and Yao and Au (1989).\nGiven a known upper bound for the number of jumps, Yao and Au (1989)\n\n\fJUMP PENALIZED LEAST SQUARES\n\n3\n\nderive the optimal O(n\u22121/2 ) and O(n\u22121 ) rates for recovering the function in\nan L2 sense and detecting the jump points, respectively. Their results have\nbeen generalized to overdispersion models and applied to DNA-segmentation\nby Braun, Braun and M\u00fcller (2000). Without the constraint of a known\nupper bound for the number of jumps, Birg\u00e9 and Massart (2007) give a\nnonasymptotic bound for the MSE for a slightly different penalty.\nIn this more general setting we will deduce the same (parametric) rates\nas Yao and Au (1989) for the Potts minimizer if f is piecewise constant\nwith a finite but arbitrary number of jumps. We show that the estimate\nasymptotically reconstructs the correct number of jumps with probability\n1. Further we will give (optimal) rates in the Skorokhod topology, which\nprovides simultaneous convergence of the jump points and the graph of the\nfunction, respectively. As far as we know, this approach is new to regression\nanalysis.\nIf the true regression function is not a step function, the Potts minimizer cannot compete in terms of rate of convergence for smoothness assumptions stronger than C 1 . This is due to the nonsmooth approach of\napproximation via step functions and could be improved by fitting polynomials between estimated jumps [see Spokoiny (1998), Kohler (1999)]. For\nless smooth functions, however, we will show that it is adaptive and obtains\noptimal rates of convergence. To this end, we prove rates of convergence\nin certain classes of \"approximation spaces\" well known in approximation\ntheory [DeVore and Lorentz (1993)]. To our knowledge, these spaces have\nnot been introduced to statistics before. As special cases, we obtain (up to\na logarithmic factor) the optimal O(n\u22121/3 ) and O(n\u2212\u03b1/(2\u03b1+1) ) rates if f is\nof bounded variation or if f is (piecewise) H\u00f6lder continuous on [0, 1] of\norder 1 \u2265 \u03b1 > 0, respectively. The logarithmic factor occurs, since we give\nalmost sure bounds instead of the more commonly used stochastic or mean\nsquare error bounds. Optimality in the class of functions with bounded variation shows that the Potts minimizer has the attribute of \"local adaptivity\"\n[Donoho et al. (1995)]. Under the assumption that the error is bounded,\nKohler (1999) obtained nearly the same rates (worse by an additional logarithmic term) in these H\u00f6lder classes for the mean square error of a similar\nestimator.\nWe stress that minimizing H\u03b3 in (3) results in a step function, that is, a\nregressogram in the sense of Tukey (1961). Hence, this paper also answers the\nquestion how to choose the partition of the regressogram in an asymptotic\noptimal way [cf. Eubank (1999)] over a large scale of approximation spaces.\nSubset selection and TV penalization. Our results can be viewed as a\nresult on subset selection in a linear model Y = \u03b1 + \u03b2 T X + \u03b5 with covariates\n\n\f4\n\nL. BOYSEN ET AL.\n\nX. In this context our estimator minimizes the functional\nLn (\u03b1, \u03b2) :=\n\nn\nX\ni=1\n\nYi \u2212 \u03b1 \u2212\n\nk\nX\n\n\u03b2j Xij\n\nj=1\n\n!2\n\nsubject to\n\n#{j : \u03b2j 6= 0} \u2264 N,\n\nor (for proper N ), what is equivalent for a proper choice of \u03b3, minimization\nof\nLn (\u03b1, \u03b2) + \u03b3#{j : \u03b2j 6= 0}.\nSetting k = n \u2212 1 as well as Xij = 1 for j < i and 0 else, we obtain the Potts\nPi\u22121\n\u03b2j for 2 \u2264 i \u2264 n. In general,\nfunctional (3) with u1 = \u03b1 and ui = \u03b1 + j=1\nto select the correct variables, one requires a kind of oversmoothing, which\nis reflected by our results in the present paper. The Potts smoother in (3)\nachieves this by means of an l0 penalty and for nearly uncorrelated predictors it is well known that l1 penalization has almost the same properties as\ncomplexity-penalized least squares regression [cf. Donoho (2006a, 2006b)].\nHowever, as a variable selection problem, detection of jumps in regression\nhas a special feature, namely, the covariates Xij are highly correlated and\nthese results do not apply. A similar comment applies to TV penalized estimation, as, for example, considered by Mammen and van de Geer (1997)\nwhich aims for minimizing\nF\u03b3 (u, Y ) = \u03b3 *\n\nX\n\n1\u2264i\u2264n\u22121\n\n|ui \u2212 ui+1 | +\n\nn\nX\ni=1\n\n(ui \u2212 Yi )2 .\n\nThis can also be viewed in this context. Choosing Xik as above, it is a\nspecial case of the lasso, whichPwas introduced by Tibshirani (1996) and\nminimizes Ln (\u03b1, \u03b2) subject to kj=1 |\u03b2j | \u2264 t. Again, for (nearly) uncorrelated predictors, the lasso comes close to the l0 solution. Thus, the relation\nof the Potts functional to the total variation penalty is roughly the same\nas the relation of subset selection to the lasso. In fact, for highly correlated predictors, the relationship between l0 and l1 solutions is much less\nunderstood and this question is above the scope of the paper. However, it\nseems that in our case l1 penalization performs suboptimally. As an indication, from Mammen and van de Geer (1997), Theorem 10, we obtain an\nupper rate bound of OP (n\u2212\u03b1/3 ) for the error of the total variation penalized\nleast squares estimator of an \u03b1-H\u00f6lder continuous function in contrast to\nthe (optimal) rate of OP (n\u03b1/(2\u03b1+1) ), achieved by the Potts minimizer.\nA reason for this difference is that the Potts functional will generally lead\nto fewer but higher jumps in the reconstruction, and hence is even more\nsparse than l1 or TV based reconstructions. In general, a side phenomenon\nrelated to such sparsity of an estimator is a bad uniform risk behavior [see\nP\u00f6tscher and Leeb (2008)]. Although the conditions of that paper are not\n\n\fJUMP PENALIZED LEAST SQUARES\n\n5\n\nfulfilled in our model (basically, contiguity of the error distributions will fail),\nthis phenomenon can be observed numerically in our situation. Our estimate\nwill fail when the number of jumps grows too fast with the number of observations and small plateaus in the data will not be captured. However, our\nemphasis is on estimation of the main data features (here jumps) to obtain\na sparse description of data, similar in spirit to Davies and Kovac (2001).\nComputational issues. In general, a major burden of l0 penalization is\nthat it leads to optimization problems which are often NP hard and relaxation of this functional becomes necessary or other penalties, such as\nl1 , have to be used. Interestingly, computation of the minimizer of the\nPotts functional in (3) is a notable exception. The family (T\u03b3 (Y )))\u03b3>0 can\nbe computed in O(n3 ) and the minimizer for one \u03b3 in O(n2 ) steps [see\nWinkler and Liebscher (2002)]. At the heart of that result is the observation that the set of partitions of a discrete interval carries the structure of\na directed acyclic graph which makes dynamic programming directly applicable [see Friedrich et al. (2008)].\nThe paper is organized as follows: after introducing some notation in\nSection 2, we provide in Section 3.1 the rates and consistency results for step\nfunctions and general bounded functions in the L2 metric. In Section 3.2 we\npresent the results of convergence in Hausdorff metric for the set of jump\nfunctions and in Section 3.3 for the Skorokhod topology for the regression\nfunction. In Section 3.4 we will introduce a simple data-driven parameter\nselection strategy resulting from our previous results and compare this to a\nmultiresolution approach as in Davies and Kovac (2001). We briefly discuss\nrelations to other models such as Bayesian imaging and extensions to higher\ndimensions in Section 4. Technical proofs are given in the Appendix.\nThis paper is complemented by the work of Boysen et al. (2007) which\ncontains technical details of some of the proofs, the consistency of the estimates for more general noise conditions and the consistency of the empirical\nscale space (T\u03b3 (Y ))\u03b3>0 toward its deterministic target [cf. Chaudhuri and Marron\n(2000)].\n2. Model and notation. For a functional F : \u0398 \u2192 R \u222a {\u221e}, we denote by\narg min FPthe subset of \u0398 consisting of all minimizers of F . Let S([0, 1)) =\n{f : f = ni=1 \u03b1i 1[ti ,ti+1 ) , \u03b1i \u2208 R, 0 = t1 < * * * < tn+1 = 1, n \u2208 N} denote the\nspace of right-continuous step functions and let D([0, 1)) denote the c\u00e0dl\u00e0g\nspace of right-continuous functions on [0, 1] with left limits and left-continuous\nat 1. Both will be considered as subspaces of L2 ([0, 1)) with the obvious identification of a function with its equivalence class, which is injective for these\ntwo spaces. k * k will denote the norm of L2 ([0, 1)) and the norm on L\u221e ([0, 1))\nis denoted by k * k\u221e .\n\n\f6\n\nL. BOYSEN ET AL.\n\nMinimizers of the Potts functionals (3) will be embedded into L2 ([0, 1))\nby the map \u03b9n : Rn 7\u2212\u2192 L2 ([0, 1)),\n(4)\n\n\u03b9n ((u1 , . . . , un )) =\n\nn\nX\n\nui 1[(i\u22121)/n,i/n) .\n\ni=1\n\nUnder the regression model (1), this leads to estimates f\u02c6n = \u03b9n (T\u03b3n (Y )), that\nis,\nf\u02c6n \u2208 \u03b9n (arg min H\u03b3n (*, Y )).\n\n(5)\n\nHere and in the following (\u03b3n )n\u2208N is a (possibly random) sequence of smoothing parameters. We suppress the dependence of f\u02c6n on \u03b3n since this choice\nwill be clear from the context.\nFor the noise, we assume the following uniform sub-Gaussian condition.\nFor a discussion on how this condition can be weakened [see Boysen et al.\n(2007)].\nCondition (A). The triangular array (\u03bein )n\u2208N,1\u2264i\u2264n of random variables obeys the following properties.\n(i) For all n \u2208 N the random variables (\u03bein )1\u2264i\u2264n are independent.\n2\nn\n(ii) There is a universal constant \u03b2 \u2208 R such that Ee\u03bd\u03bei \u2264 e\u03b2\u03bd for all\n\u03bd \u2208 R, 1 \u2264 i \u2264 n, and n \u2208 N.\nFinally, we recall the definition of H\u00f6lder classes. We say that a function\nf : [0, 1] \u2192 R belongs to the H\u00f6lder class of order 0 < \u03b1 \u2264 1, if there exists\nC > 0 such that\n|f (x) \u2212 f (y)| \u2264 C|x \u2212 y|\u03b1\n\nfor all x, y \u2208 [0, 1].\n\n3. Consistency and rates. In order to extend the Potts functional in (3)\nto L2 ([0, 1)), we define for \u03b3 > 0, the continuous Potts functionals H\u03b3\u221e : L2 ([0, 1))\u00d7\nL2 ([0, 1)) \u2192 R \u222a {\u221e}:\nH\u03b3\u221e (g, f ) =\n\n\u001a\n\n\u03b3 * #J(g) + kf \u2212 gk2 ,\n\u221e,\n\nif g \u2208 S([0, 1)),\notherwise.\n\nHere J(g) = {t \u2208 (0, 1) : g(t\u2212) 6= g(t+)} is the set of jumps of g \u2208 S([0, 1)). By\ndefinition, we have for every g \u2208 arg min H\u03b3\u221e (*, f ) that H\u03b3\u221e (g, f ) \u2264 H\u03b3\u221e (0, f ) =\nkf k2 and therefore #J(g) \u2264 \u03b3 \u22121 kf k2 for \u03b3 > 0. Since a minimizer is uniquely\ndetermined by its set of jumps, minimizing H\u03b3\u221e can be reduced to a minimization problem on the compact set of jump configurations with not more\nthan \u03b3 \u22121 kf k2 jumps which implies existence of a minimizer. For \u03b3 = 0, we\nset H0\u221e (g, f ) = kf \u2212 gk2 for all g \u2208 L2 ([0, 1)), hence\n\n\f7\n\nJUMP PENALIZED LEAST SQUARES\n\nLemma 1. For any\narg min H\u03b3\u221e (*, f ) 6= \u2205.\n\nf \u2208 L2 ([0, 1))\n\nand\n\nall\n\n\u03b3 \u2265 0\n\nwe\n\nhave\n\nIn order to keep the presentation simple, we choose throughout the following an equidistant design xni = i/n in the model (1) and (2). All results given\nremain valid for designs with design density h, such that inf t\u2208[0,1] h(t) > 0\nand h is H\u00f6lder continuous on [0, 1] of order \u03b1 > 1/2. Moreover, for all theorems in this section we will assume that Y n is determined through (1) and\nthe noise \u03be n satisfies Condition (A).\n3.1. Convergence in L2 . We investigate the asymptotic behavior of the\nPotts minimizer when the sequence (\u03b3n )n\u2208N converges to a constant \u03b3 for\n\u03b3 > 0 and \u03b3 = 0, respectively. If \u03b3 > 0, we do not recover the original function\nin the limit, but a parsimonious representation at a certain scale of interest\ndetermined by \u03b3. For \u03b3 = 0 the Potts minimizer is consistent for the true\nsignal under some conditions on the sequence (\u03b3n )n\u2208N :\n(H1) (\u03b3n )n\u2208N satisfies \u03b3n \u2192 0 and \u03b3n n/ log n \u2192 \u221e P-a.s.\nFor the consistency in approximation spaces in Theorem 2, we consider\ninstead\n(H2) (\u03b3n )n\u2208N satisfies \u03b3n \u2192 0 and \u03b3n \u2265 (1 + \u03b4)12\u03b2 log n/n P-a.s. for almost\nevery n and some \u03b4 > 0. Here \u03b2 is given by the noise Condition (A).\nTheorem 1. (i) Assume that f \u2208 L2 ([0, 1)) and \u03b3 > 0 are such that f\u03b3 is\na unique minimizer of H\u03b3\u221e (*, f ). Moreover, suppose (\u03b3n )n\u2208N satisfies \u03b3n \u2192 \u03b3\nP-a.s.; then\nL2 ([0,1))\n\n\u2212\u2212\u2192 f\u03b3\nf\u02c6n \u2212\u2212\u2212\n\nP-a.s.\n\nn\u2192\u221e\n\n(ii) Let f \u2208 L2 ([0, 1)) and (\u03b3n )n\u2208N fulfill (H1). Then\nL2 ([0,1))\n\n\u2212\u2212\u2192 f\nf\u02c6n \u2212\u2212\u2212\n\nP-a.s.\n\nn\u2192\u221e\n\n(iii) Let f \u2208 S([0, 1)) and (\u03b3n )n\u2208N fulfill (H1). Then\nkf\u02c6n \u2212 f k = O\n\ns\n\u0012\n\nlog n\nn\n\n\u0013\n\nP-a.s.\n\nMoreover,\nkf\u02c6n \u2212 f k = OP\n\n\u0012r \u0013\n\n1\n.\nn\n\n\f8\n\nL. BOYSEN ET AL.\n\nWe stress that the parametric rates in Theorem 1(iii) are obtained for a\nbroad range of rates for the sequence of smoothing parameters. It is only\nrequired that \u03b3n converges to zero slower than log n/n. When trying to\nextend these results to more general function spaces, the question arises,\nwhich properties of the true regression function f determine the almost sure\nrate of convergence of the Potts estimator. It turns out that the answer lies\nin the speed of approximation of f by step functions. Let us introduce the\napproximation error\n(6)\n\n\u2206k (f ) := inf{kg \u2212 f k : g \u2208 S([0, 1)), #J(g) \u2264 k}\n\nand the corresponding approximation spaces\n\u001a\n\nA\u03b1 = f \u2208 L\u221e [0, 1] : sup k\u03b1 \u2206k (f ) < \u221e\nk\u22651\n\n\u001b\n\nfor \u03b1 > 0. The following theorem gives the almost sure rates of convergence\nfor these spaces.\nTheorem 2.\n\nIf f \u2208 A\u03b1 and (\u03b3n )n\u2208N satisfies condition (H2), then\nkf\u02c6n \u2212 f k = O(\u03b3n\u03b1/(2\u03b1+1) )\n\nP-a.s.\n\nNow we give examples of well known function spaces contained in A\u03b1 for\n\u03b1 \u2264 1.\nExample 1. Suppose f has finite total variation. Then, f \u2208 A1 holds.\nChoosing \u03b3n \u224d log n/n such that condition (H2) is fulfilled yields kf\u02c6n \u2212 f k =\nO((log n/n)1/3 ) P-a.s.\nProof. For the application of Theorem 2 we need to show that there is a\n\u03b4 > 0 such that for all k \u2208 N, k \u2265 1, there is an fk \u2208 S([0, 1)) with kf \u2212 fk k \u2264\n\u03b4/(k + 1) and #J(fk ) \u2264 k. Since each function of finite total variation is the\ndifference of two increasing functions and #J(g + g\u2032 ) \u2264 #J(g) + #J(g \u2032 ), it\nis enough to consider increasing f with f (0) = 0 and f (1) < 1. Define for\ni = 1, . . . , k intervals\nIi = f \u22121 ([(i \u2212 1)/k, i/k)).\n\nThen, fk (x) = ki=1 1Ii (x)(i\u22121/2)/k satisfies kf \u2212fk k \u2264 kf \u2212fk k\u221e \u2264 (2k)\u22121\nwhich completes the proof. \u0003\nP\n\nExample 2. Suppose f belongs to a H\u00f6lder class of order \u03b1 (with 0 <\n\u03b1 \u2264 1). Then, f \u2208 A\u03b1 holds. For \u03b3n \u224d log n/n fulfilling condition (H2), we\nget that kf\u02c6n \u2212 f k = O((log n/n)\u03b1/(2\u03b1+1) ) P-a.s.\n\n\f9\n\nJUMP PENALIZED LEAST SQUARES\n\nProof. AnalogousPto the proof above, we define for Ii = [(i \u2212 1)/k, i/k)\nthe function fk (x) = ki=1 1Ii (x)f ((i \u2212 1/2)/k). On Ii we have kf (x) \u2212\nf (y)k\u221e \u2264 Ck\u2212\u03b1 . Thus kf \u2212 fk k \u2264 kf \u2212 fk k\u221e \u2264 C(2k)\u2212\u03b1 holds. \u0003\nObviously this result still holds, if the regression function f is piecewise\nH\u00f6lder with finitely many jumps.\nRemark 1 (The case \u03b1 > 1). The characterization of the sets A\u03b1 and\nrelated questions are a prominent theme in nonlinear approximation theory\n[see, e.g., DeVore (1998), DeVore and Lorentz (1993)]. For f piecewise C 1 , it\nis known that \u03b1 > 1 implies that f is piecewise constant [Burchard and Hale\n(1975)], whereas this is still an open problem for general f . We conjecture\nthat this implication holds for any f . This would imply that stronger smoothness assumptions than in the examples above do not yield better convergence\nrates.\nChoosing \u03b3n independently of the function and the function class as in the\nexamples above yields convergence rates which are up to a logarithmic factor\nthe optimal rates in the classes A\u03b1 , 0 < \u03b1 \u2264 1 and S([0, 1)). This shows that\nthe estimate is adaptive over these classes. The additional logarithmic factor\noriginates from giving almost sure rates of convergence.\n3.2. Hausdorff convergence of the jump-sets. In this section we present\nthe rates known from change-point analysis for detecting the locations of\njumps if f is a step function. Moreover, the following theorem shows that\nwe will eventually estimate the right number of jumps almost surely. Before\nstating the results, we recall the definition of the Hausdorff metric \u03c1H on\nthe space of closed subsets contained in (0, 1). For nonempty closed sets\nA, B \u2282 (0, 1) set\n\u001a\n\n\u001b\n\n\u03c1H (A, B) = max max min |b \u2212 a|, max min |b \u2212 a|\na\u2208A b\u2208B\n\nb\u2208B a\u2208A\n\nand \u03c1H (A, \u2205) = \u03c1H (\u2205, A) = 1.\nTheorem 3.\n\nLet f \u2208 S([0, 1)) and (\u03b3n )n\u2208N fulfill (H1). Then:\n\n(i) #J(f\u02c6n ) = #J(f ) for large enough n P-a.s.,\n(ii) \u03c1H (J(f\u02c6n ), J(f )) = O(log n/n) P-a.s.,\n(iii) \u03c1H (J(f\u02c6n ), J(f )) = OP (1/n).\n\nRemark 2 (Distribution of the jump locations and estimated function\nvalues). With the help of Theorem 3(i) we can derive the asymptotic distribution of the jump locations and of the estimated function values between, obtaining the same results as Yao and Au (1989), who assumed an a\n\n\f10\n\nL. BOYSEN ET AL.\n\npriori bound of the number of jumps. To this end, note that the estimator\nof Yao and Au (1989) and the Potts minimizer coincide if they have the\nsame number of jumps. Denoting the ordered jumps of f and their estimators by (\u03c41 , . . . , \u03c4R ) and (\u03c4\u03021 , . . . , \u03c4\u0302R\u0302 ), respectively, we know by Theorem\n3(i) that asymptotically R\u0302 = R holds almost surely. For R\u0302 = R we get that\nn(\u03c4\u03021 , . . . , \u03c4\u0302R ) are asymptotically independent and the limit distribution of\nn(\u03c4\u0302r \u2212 [\u03c4r ]) is the minimum of a two-sided asymmetric random walk [cf.\nYao and Au (1989), Theorem 1]. Moreover, the\u221aestimated function values\nare asymptotically normal with the parametric n-rate.\n3.3. Convergence in Skorokhod topology. Now that we have established\nrates of convergence for the graph of the function as well as for the set of\njump points, it is natural to ask whether one can handle both simultaneously.\nTo this end, we recall the definition of the Skorokhod metric [Billingsley\n(1968), Chapter 3]. Let \u039b1 denote the set of all strictly increasing continuous\nfunctions \u03bb : [0, 1] 7\u2212\u2192 [0, 1] which are onto. We define for f, g \u2208 D([0, 1))\n\u001a\n\n\u0012\n\n\u0013\n\n\u001b\n\n\u03c1S (f, g) = inf max L(\u03bb), sup |f (\u03bb(t)) \u2212 g(t)| : \u03bb \u2208 \u039b1 ,\n0\u2264t\u22641\n\n\u03bb(t)\u2212\u03bb(s)\n|.\nt\u2212s\n\nThe topology induced by this metric\nwhere L(\u03bb) = sups6=t\u22650 | log\nis called J1 -topology.\nWe find that in the situation of Theorem 1(i) we can establish consistency\nwithout further assumptions, whereas in the situation of Theorem 1(ii), f\nhas to belong to D([0, 1)).\nTheorem 4.\n\n(i) Under the assumptions of Theorem 1(i) we have\nD([0,1))\n\n\u2212\u2212\u2192 f\u03b3\nf\u02c6n \u2212\u2212\u2212\n\nP-a.s.\n\nn\u2192\u221e\n\n(ii) If f \u2208 D([0, 1)) and (\u03b3n )n\u2208N satisfies condition (H1), then\nD([0,1))\n\n\u2212\u2212\u2192 f\nf\u02c6n \u2212\u2212\u2212\n\nP-a.s.\n\nn\u2192\u221e\n\nIf f is continuous on [0, 1]\nL\u221e ([0,1])\n\n\u2212\u2212\u2192 f\nf\u02c6n \u2212\u2212\u2212\n\nP-a.s.\n\nn\u2192\u221e\n\n(iii) If f \u2208 S([0, 1)) and (\u03b3n )n\u2208N satisfies condition (H1), then\n\u03c1S (f\u02c6n , f ) = O\nMoreover,\n\ns\n\u0012\n\nlog n\nn\n\n\u03c1S (f\u02c6n , f ) = OP\n\n\u0013\n\nP-a.s.\n\n\u0012r \u0013\n\n1\n.\nn\n\n\fJUMP PENALIZED LEAST SQUARES\n\n11\n\n3.4. Parameter choice and simulated data. In this section we assume\n\u03bein \u223c N (0, \u03c3 2 ), i = 1, . . . , n i.i.d. for all n. Note that in this case we have\n\u03b2 = \u03c3 2 /2 in Condition (A). Theorem 2 directly yields a simple data-driven\nprocedure for choosing the parameter \u03b3 which leads to optimal rates of\nconvergence. For a strongly consistent estimate \u03c3\u0302 of \u03c3, the choice \u03b3\u0302n =\nC \u03c3\u0302 2 log n/n almost surely satisfies condition (H2) for C > 6 and gives the\nrates of Theorem 2. However, in simulations it turns out that smaller choices\nof C lead to better reconstructions. A closer look at the proof of Theorem 2\nshows that the constant in condition (H2) mainly depends on the behavior of\nthe maximum of the partial sum process sup1\u2264i\u2264j\u2264n (\u03bein + * * * + \u03bejn )2 /(j \u2212 i +\n1). As we consider a triangular scheme instead of a sequence of i.i.d. random\nvariables for the error we cannot use results as in Shao (1995) to obtain an\nalmost sure bound for this process [cf. Tomkins (1974)]. But those results\ngive an upper bound in probability (cf. Lemma A.2) for the maximum. This\nallows us to refine the bound above to C \u2265 2 + \u03b4 for any \u03b4 > 0 and obtain\nthe rates of Theorem 6 in probability. We found that values of C between 2\nand 3 lead to good reconstruction for various simulation settings.\nFigure 1 shows the behavior of the Potts minimizer for the test signals\nof Donoho and Johnstone (1994) sampled at 2048 points and a choice of\nC = 2.5. In order to understand the finite sample behavior of the Potts\nminimizer, the estimates are calculated at different signal-to-noise ratios\nkf k2 /\u03c3 2 (seven, four and one). The reconstructions of the locally constant\nblocks signal (first row) differ very little from the original signal. This is\nnot surprising since the original signal is in S([0, 1)) where the estimator\nachieves parametric rates. The spikes of the bumps signal (second row) are\ncorrectly estimated for all cases. The estimator captures all relevant features\nof the Heavisine signal (third row) at the levels seven and four. Only in the\npresence of strong noise the detail of the spike right to the second maximum\nis lost. Finally, the case of the Doppler signal (fourth row) shows that the\nestimator adapts well to locally changing smoothness.\nClearly the performance depends on the particular function f . Hence one\nmight want to try different approaches to selecting the parameter. One possibility is to choose the smoothing parameter according to the multiresolution\ncriterion of Davies and Kovac (2001). If f \u2208 S([0, 1)), this criterion picks\nasymptotically the correct number of jumps.\nTheorem 5. Assume f \u2208 S([0, 1)), \u03bein \u223c N (0, \u03c3 2 ) i.i.d. and \u03b3\u0302n is chosen\naccording to the MR-criterion, that is, \u03b3\u0302n is the maximal value such that the\ncorresponding reconstruction f\u02c6nM R satisfies\n(7)\n\n\u221a\n\np\n1 X n \u02c6M R n\nYi \u2212 fn (xi ) \u2264 (1 + \u03b4)\u03c3\u0302 2 log n\n#I i\u2208I\n\n\f12\nL. BOYSEN ET AL.\nFig. 1. The left column shows signals from Donoho and Johnstone (1994). Columns 2, 4 and 6 show noisy versions with signal-to-noise\nratios of 7, 4 and 1, respectively. On the right of each noisy signal is the Potts reconstruction. The penalty was chosen as \u03b3n = 2.5\u03c3\u0302 2 log n/n,\nwhere \u03c3\u0302 2 is an estimate of the variance.\n\n\fJUMP PENALIZED LEAST SQUARES\n\n13\n\nfor all connected I \u2282 {1, . . . , n}, some \u03b4 > 0 and some consistent estimate\n\u03c3\u0302 of \u03c3. Moreover, assume \u03b3n satisfies condition (H1) and f\u02c6n is the corre\u2212\u2212\u2192 1.\nsponding reconstruction. Then P(f\u02c6nM R = f\u02c6n ) \u2212\u2212\u2212\nn\u2192\u221e\n\nNote that it is possible to derive the same result if in (7) only dyadic\nintervals [see Davies and Kovac (2001)] are considered. We conjecture that\nthe MR-criterion leads to consistent estimates in more general settings.\n4. Discussion-relation to other models. The Potts smoother falls in\nthe general framework of van de Geer (2001) which gives very general and\npowerful tools to prove rates of convergence for penalized least squares estimates. With some effort, it is possible to use the methods developed in that\npaper to derive the convergence rates given in Theorem 2. However, using\nthat method does not lead to the required constant in Section 3.4. In fact,\nthe resulting constant in condition (H2) would be substantially larger.\nMost penalized least squares methods either use a penalty which is a\nseminorm (as in spline regression) or penalizes the number or size of coefficients of an orthonormal basis reconstruction. Note that the Potts smoother\nbelongs to none of these classes. Nonetheless, it is related to various other\nstatistical procedures and we would like to close this paper by highlighting\nthese relations and shortly comment on possible extensions to two dimensions.\nBayesian interpretation and imaging. In image analysis Bayesian methods for restoration have received much attention [see, e.g., Geman and Geman\n(1984)]. The Potts functional can be interpreted as a limit of the onedimensional version of a certain MAP estimator, which has been used for\nedge-preserving smoothing, discussed by Blake and Zisserman (1987) and\nK\u00fcnsch (1994) among many others. For a detailed discussion and overview\nof related functionals in dimension 1 [see Winkler et al. (2005)].\nGeneralization to 2d. For two-dimensional data, a measure of complexity corresponding to the number of jumps is given by the number of plateaus\nor partition elements. However, it is computationally infeasible to allow for\narbitrary partitions in the reconstruction. Therefore one chooses a subclass\nof step functions with good approximation properties and seeks for effective minimization algorithms in this class. As in the one-dimensional case,\nthe rate of convergence will be determined by the approximation properties of the chosen function class. One example, complexity penalized sums\nof squares with respect to a class of \"Wedgelets\" [cf. Donoho (1999)], is\ndiscussed in the Ph.D. thesis of Friedrich (2005), and possible alternatives\nin the survey by F\u00fchr, Demaret and Friedrich (2006). We mention that the\nproof of Theorem 2 could be adapted to their setting.\n\n\f14\n\nL. BOYSEN ET AL.\n\nAPPENDIX: PROOFS\nA.1. Preliminaries. Since the consistency results are formulated in terms\nof a function space, we translate all minimization problems to equivalent\nproblems for functionals on L2 ([0, 1)). Therefore we introduce the functionals H\u0303\u03b3\u221e (g, f ) = H\u03b3\u221e (g, f ) \u2212 kf k2 and H\u0303\u03b3n (g, f ) is defined as H\u0303\u03b3\u221e (g, f )\nfor g \u2208 Sn ([0, 1)) := \u03b9n (Rn ), and \u221e, else. Clearly, the functionals are constructed in such a way that the minimization of H\u03b3 (3) on Rn is equivalent\nto the minimization of H\u0303\u03b3n if we identify the minimizers via the map \u03b9n\ndefined in (4). The constant \u2212kf k2 is just added for convenience and does\nn\nnot affect the minimization. Obviously, u \u2208 arg min H\u03b3 (*, f ) if and only if\n\u03b9n (u) \u2208 arg min H\u0303\u03b3n (*, f ) and similarly for H\u03b3 (*, y) for y \u2208 Rn . The most important property of these functionals is that the minimizers g \u2208 S([0, 1)) of\nH\u0303\u03b3n and H\u0303\u03b3\u221e for \u03b3 > 0 are determined by their jump-set J(g) and given\nby the projection onto the space of step functions which are constant outside that set. To make this precise in the course of the proofs, we introduce\nfor any J \u2282 (0, 1) the partition PJ = {[a, b) : a, b \u2208 J \u222a {0, 1}, (a, b) \u2229 J = \u2205}.\nAbbreviating by\n\u03bcI (f ) = l(I)\u22121\n\nZ\n\nf (u) du\nI\n\nthe mean of f over some interval I, this projection is then given by\nfJ =\n\nX\n\n\u03bcI (f )1I .\n\nI\u2208PJ\n\nFurther, we extend the noise in (1) to L2 ([0, 1)) by \u03be n = \u03b9n ((\u03be1n , . . . , \u03benn )) and,\nfinally, we define for f \u2208 S([0, 1)) the minimum distance between any two\njumps as\n(8)\n\nmpl(f ) := min{|s \u2212 t| : s 6= t \u2208 J(f ) \u222a {0, 1}}.\n\nThe proofs rely on properties of the noise, some a priori properties of the\nPotts minimizers and on proving epiconvergence of the functionals defined\nabove with respect to the topology of L2 ([0, 1)).\nA.2. Two properties of the noise. The behavior of \u03beJn = I\u2208PJ \u03bcI (\u03be n )1I\nfrom Condition (A) is controlled by the following two estimates which are\nproved in Boysen et al. (2007), Section 4.2.\nP\n\nLemma A.1.\n(9)\n\nLet (\u03bein )n\u2208N,1\u2264i\u2264n fulfill Condition (A). For\nCn :=\n\n(\u03bein + * * * + \u03bejn )2\n1\u2264i\u2264j\u2264n (j \u2212 i + 1) log n\nsup\n\n\fJUMP PENALIZED LEAST SQUARES\n\n15\n\nwe have that\nlim sup Cn \u2264 12\u03b2\n\nP-a.s.\n\nn\u2192\u221e\n\nMoreover, for all intervals I \u2282 [0, 1) and all n \u2208 N\n\u03bcI (\u03be n )2 \u2264 Cn\n\nlog n\nnl(I)\n\nas well as\n(10)\n\nk\u03beJnn k2 =\n\nX\n\nI\u2208PJn\n\nl(I)\u03bcI (\u03be n )2 \u2264 Cn\n\nlog n\n(#Jn + 1).\nn\n\nLemma A.2. Assume \u03bein \u223c N (0, \u03c3 2 ), i = 1, . . . , n i.i.d. for all n. Then\nfor Cn defined by (9) we have Cn = 2\u03c3 2 + oP (1).\nA.3. A priori properties of the minimizers. The following properties of\nthe minimizers are used to prove our main statements.\nLemma A.3. Let f \u2208 L2 ([0, 1)), g \u2208 arg min H\u0303\u03b3n (*, f ) and I \u2208 PJ(g) . Then,\nR\ndenoting a = \u03bcI (g) = l(I)\u22121 I g(u) du, the following statements are valid.\n(i) If I \u2032 \u2208 PJ(g) and I \u2032 \u222a I is an interval, then\n\u03b3\u2264\n\nl(I)l(I \u2032 )\n(\u03bcI (f ) \u2212 \u03bcI \u2032 (f ))2 .\n\u2032\nl(I) + l(I )\n\n(ii) If I \u2032 \u2208 Bn , I \u2032 \u2282 I, is an interval, then\n2\u03b3 \u2265 l(I \u2032 )(\u03bcI \u2032 (f ) \u2212 a)2 .\n(iii) If both I \u2032 \u2208 Bn and I \u2032 \u222a I are intervals and 1I \u2032 g = b1I \u2032 for some\nb \u2208 R, then\n\u0012\n\n(b \u2212 a) \u03bcI \u2032 (f ) \u2212\n\na+b\n\u2265 0.\n2\n\u0013\n\n(iv) If I1\u2032 , I2\u2032 , I1\u2032 \u222a I, I2\u2032 \u222a I \u2208 Bn are intervals and 1Il\u2032 f\u02c6 = bl 1Il\u2032 , l = 1, 2,\nthen for all disjoint intervals I1 , I2 \u2208 Bn , I = I1 \u222a I2 , such that I1 \u222a I1\u2032 and\nI2 \u222a I2\u2032 are intervals,\nl(I1 )(\u03bcI1 (f ) \u2212 b1 )2 + l(I2 )(\u03bcI2 (f ) \u2212 b2 )2\n\n\u2265 \u03b3 + l(I1 )(\u03bcI1 (f ) \u2212 a)2 + l(I2 )(\u03bcI2 (f ) \u2212 a)2 .\n\n\f16\n\nL. BOYSEN ET AL.\n\nProof. The inequalities are obtained by elementary calculations comparing the values of H\u0303\u03b3n (*, f ) at g and at some g\u0303 obtained from g by: joining\nthe plateaus at I and I \u2032 [for (i)], splitting the plateau at I into three plateaus\n[for (ii)], moving the jump point [for (iii)], and removing the plateau at I by\njoining each of the parts to the adjacent intervals [for (iv)].\nAs an example, we provide the calculations for (i). Determine t by {t} =\nI \u2229 I \u2032 and set g\u0303 = fJ(g)\\{t} . Then g\u0303 differs from g only on I \u2229 I \u2032 such that\n0 \u2264 H\u0303\u03b3n (g\u0303, f ) \u2212 H\u0303\u03b3n (g, f )\n\n= \u2212\u03b3 + k(\u03bcI (f ) \u2212 \u03bcI\u222aI \u2032 (f ))1I k2 + k(\u03bcI \u2032 (f ) \u2212 \u03bcI\u222aI \u2032 (f ))1I \u2032 k2\n= \u2212\u03b3 + l(I)(\u03bcI (f ) \u2212 \u03bcI\u222aI \u2032 (f ))2 + l(I \u2032 )(\u03bcI \u2032 (f ) \u2212 \u03bcI\u222aI \u2032 (f ))2\n= \u2212\u03b3 +\n\nl(I)l(I \u2032 )\n(\u03bcI (f ) \u2212 \u03bcI \u2032 (f ))2 ,\nl(I) + l(I \u2032 )\n\nwhich completes the proof of (i). \u0003\nA.4. Epiconvergence. One basic idea of the consistency proofs is to use\nthe concept of epiconvergence of the functionals [see, e.g., Dal Maso (1993),\nHess (1996)]. We say that numerical functions Fn : \u0398 7\u2192 R \u222a {\u221e}, n = 1, . . . , \u221e\non a metric space (\u0398, \u03c1) epiconverge to F\u221e if for all sequences (\u03b8n )n\u2208N with\n\u03b8n \u2192 \u03b8 \u2208 \u0398 we have F\u221e (\u03b8) \u2264 lim inf n\u2192\u221e Fn (\u03b8n ), and for all \u03b8 \u2208 \u0398 there exists a sequence (\u03b8n )n\u2208N with \u03b8n \u2192 \u03b8 such that F\u221e (\u03b8) \u2265 lim supn\u2192\u221e Fn (\u03b8n ).\nOne important property is that each accumulation point of a sequence of\nminimizers of Fn is a minimizer of F\u221e . However, that does not mean that\na sequence of minimizers has accumulation points at all. To prove this, one\nneeds to show that the minimizers are contained in a compact set. The\nfollowing lemma which is a straightforward consequence of the characterization of compact subsets of D([0, 1)) [Billingsley (1968), Theorem 14.3] will\nbe applied to this end.\nLemma A.4. A subset A \u2282 D([0, 1)) is relatively compact if the following\ntwo conditions hold:\n(C1) For all t \u2208 [0, 1] there is a compact set Kt \u2286 R such that\ng(t) \u2208 Kt\n\nfor all g \u2208 A.\n\n(C2) For all \u03b5 > 0 there exists a \u03b4 > 0 such that for all g \u2208 A there is a\nstep function g\u03b5 \u2208 S([0, 1)) such that\nsup{|g(t) \u2212 g\u03b5 (t)| : t \u2208 [0, 1]} < \u03b5\nwhere mpl is defined by (8).\n\nand\n\nmpl(g\u03b5 ) \u2265 \u03b4,\n\n\fJUMP PENALIZED LEAST SQUARES\n\n17\n\nA.5. The proof of Theorem 1(i), (ii) and Theorem 4(i), (ii). For the\nsake of brevity we just give a short outline of the proof of the first two parts\nof Theorem 1 and the proof of Theorem 4(i). The details can be found in\nBoysen et al. (2007). The proof of Theorem 1(iii) is postponed to Section\nA.7, because it requires the proof of Theorem 3.\nProof of Theorem 1(i), (ii). Note that condition (H1) automatically holds if \u03b3n \u2192 \u03b3 > 0. We can thus prove both parts at once: Use first\nH\u0303\u03b3nn (f\u02c6n , f + \u03be n ) \u2264 H\u0303\u03b3nn (0, f + \u03be n ), \u03b3n n/ log n \u2192 \u221e and (10) to obtain\n(11)\n\n#Jn \u2264\n\n2kf k + 2Cn (log n/n)\n= O(\u03b3n\u22121 ).\n\u03b3n \u2212 2Cn (log n/n)\n\nThen (10) and \u03b3n n/ log n \u2192 \u221e imply\n(12)\n\nk\u03beJnn k2 =\n\nX\n\nI\u2208PJn\n\nl(I)\u03bcI (\u03be n )2 \u2192 0\n\nP-a.s.\n\nThe map\ng 7\u2192\n\n\u001a\n\n#J(g),\n\u221e,\n\nif g \u2208 S([0, 1)),\nif g \u2208\n/ S([0, 1)),\n\nis lower semicontinuous as map from L2 to N \u222a \u221e. Using that together\nwith (11) and (12), we can verify the two inequalities from the definition of\nepiconvergence and deduce that H\u0303\u03b3nn (*, f +\u03be n ) actually converges to H\u0303\u03b3\u221e (*, f )\nfor \u03b3n \u2192 \u03b3 \u2265 0 and \u03b3n n/ log n \u2192 \u221e in that sense. Since for any f \u2208 L2 ([0, 1))\nthe set {fJ : J \u2282 (0, 1), #J < \u221e} is relatively compact in L2 ([0, 1)), a comparison of H\u0303\u03b3nn (f\u02c6n , f + \u03be n ) with H\u0303\u03b3nn (0, f + \u03be n ) and usage of (11) above\nS\nyields that the set n\u2208N arg min H\u0303\u03b3nn (*, f + \u03be n ) is relatively compact. The\nuniqueness of the minimizer of H\u0303\u03b3\u221e (*, f ) along with the epiconvergence of\nH\u0303\u03b3nn (*, f + \u03be n ) and the compactness finally imply convergence of the minimizers. \u0003\nProof of Theorem 4(i). To prove this, one can proceed in a similar\nway as above. The proof of Lemma 1 is straightforward using H\u03b3\u221e (0, f ) =\nkf k2 and the relative compactness of {fJ : #J \u2264 kf k2 /\u03b3} in L2 ([0, 1)) for\n\u03b3 > 0. \u0003\nNext, we will prove consistency in the space D([0, 1)) equipped with the\nSkorokhod J1 -topology. This part is considerably more elaborate; in particular we need some of the a priori information about the minimizers provided\nby Lemma A.3.\nProof of Theorem 4(ii). All equations in this proof hold P-almost\nsurely, which will be omitted for ease of notation. If f1 , f2 \u2208 D([0, 1)) are\n\n\f18\n\nL. BOYSEN ET AL.\n\nlimit points of the sequence of minimizers, we know by Theorem 1(ii) that\nf = f1 = f2 in L2 ([0, 1)), which implies that they are equal in D([0, 1)). Thus,\nit is enough to show that the minimizers {(f + \u03be n )Jn : n \u2208 N} are contained\nin a compact set. For this goal we use now the conditions (C1), (C2) from\nLemma A.4.\nFor the proof of (C1), consider any interval I \u2208 PJn . We know from part\n(i) of Lemma A.3, for any neighboring interval I \u2032 , that\n\u03b3n \u2264\n\u2264\n\nl(I)l(I \u2032 )\n(\u03bcI (f + \u03be n ) \u2212 \u03bcI \u2032 (f + \u03be n ))2\nl(I) + l(I \u2032 )\nlog n\nlog n\nl(I)l(I \u2032 )\n+ 3Cn\n12kf k2\u221e + 3Cn\nl(I) + l(I \u2032 )\nnl(I)\nnl(I \u2032 )\n\u0012\n\n\u2264 12kf k2\u221e l(I) + 6Cn\n\n\u0013\n\nlog n\n.\nn\n\nThis yields 1/l(I) = O(\u03b3n\u22121 ). Application of Lemma A.1 yields\nk\u03beJnn k2\u221e = max{\u03bcI (\u03be n )2 : I \u2208 PJn } = O\n\n\u0012\n\nlog n\n= o(1)\nn\u03b3n\n\u0013\n\nand k(f + \u03be n )Jn k\u221e = O(1). For the proof of (C2), let us fix \u03b5 > 0 and a step\nfunction f \u0303 with kf \u2212 f \u0303k\u221e < \u03b5/7. Further, set \u03b4 = mpl(f \u0303) > 0. Now we will\nconsider three different classes of intervals I \u2208 PJn which are characterized\nby their position relative to J(f \u0303) and estimate (f + \u03be n )Jn \u2212 f \u0303 uniformly on\nthem, separately.\nClass 1 consists of intervals I with J(f \u0303) \u2229 I = \u2205. We obtain that\n\nk1I (f \u0303 \u2212 (f + \u03be n )Jn )k\u221e \u2264 k1I (f \u0303 \u2212 fJn )k\u221e + k\u03beJnn k\u221e \u2264 kf \u0303 \u2212 f k\u221e + o(1) < \u03b5/7\n\nfor large enough n uniformly for all such I and n.\nClass 2 covers intervals I which are not in class 1 but for which there is\n \u0303 \u2265 \u03b4/6. To apply Lemma A.3(ii), choose\nsome interval I \u0303 \u2208 PJ(f \u0303) with l(I \u2229 I)\n \u0303 \u2264 1/n. We find for all\nan interval I \u2032 \u2286 I \u2229 I \u0303 from Bn such that \u03c1H (I \u2032 , I \u2229 I)\n\u2032\nt\u2208I\nn\n\nn\n\n|(f + \u03be )Jn (t) \u2212 \u03bcI \u2032 (f + \u03be )| \u2264\n\ns\n\n2\u03b3n\n\u2264\nl(I \u2032 )\n\ns\n\n2\u03b3n\n,\n\u03b4/6 \u2212 2/n\n\nhence\n|(f + \u03be )Jn (t) \u2212 f \u0303(t)| \u2264 |\u03bcI \u2032 (f ) \u2212 \u03bcI \u2032 (f \u0303)| + |\u03bcI \u2032 (\u03be n )| +\nn\n\n\u2264 \u03b5/7 +\n\ns\n\nCn log n/n\n+\n\u03b4/6 \u2212 2/n\n\ns\n\ns\n\n2\u03b3n\n\u03b4/6 \u2212 2/n\n\n2\u03b3n\n< \u03b5/6\n\u03b4/6 \u2212 2/n\n\n\fJUMP PENALIZED LEAST SQUARES\n\n19\n\nfor large enough n depending only on (\u03b3n )n\u2208N , \u03b4, \u03b5. Clearly, this implies that\nfor n large enough supI\u2229I \u2032 |(f + \u03be n )Jn \u2212 f \u0303| < \u03b5/6 uniformly in I, I \u2032 .\nClass 3 contains all intervals I \u2208 PJn which are in neither class 1 nor class\n2 such that l(I) < \u03b4/3 and I \u2229 J(f \u0303) = {t0 }. Then the neighboring intervals of\nI in PJn belong necessarily to class 1 or 2. Further, if a neighboring interval\nI \u2032 is in class 2, we know that there is I \u0303 \u2208 PJ(f \u0303) with l(I \u0303 \u2229 I \u2032 ) \u2265 \u03b4/6 and\n \u0303 = 0. In any case, we find for any interval I \u0303\nI \u0303 \u2229 I 6= \u2205 such that dist(t0 , I)\nwith endpoint t0 in PJ(f \u0303) and any interval I \u2032 neighboring I in PJn with\nn\nn\n \u0303\nI \u2032 \u2229 I \u0303 6= \u2205 that supI\u2229I\n \u0303 \u2032 |(f + \u03be )Jn \u2212 f | < \u03b5/6 and thus |\u03bcI \u2032 ((f + \u03be )Jn ) \u2212\nn\n \u0303\n\u03bcI \u0303(f \u0303)| = |\u03bcI\u2229I\n \u0303 \u2032 ((f + \u03be )Jn ) \u2212 \u03bcI\u2229I\n \u0303 \u2032 (f )| < \u03b5/6.\nWe choose t1 with nt1 \u2208 N and |t1 \u2212 t0 | < 1/n as well as I1 = I \u2229 [0, t1 ),\nI2 = I \u2229 [t1 , 1) and Ij\u2032 as neighboring intervals of Ij in PJn , j = 1, 2. Denoting\na = \u03bcI (f + \u03be n ) and bj = \u03bcIj\u2032 (f + \u03be n ), application of Lemma A.3(iv) yields\n(together with Lemma A.1) that\nl(I1 )(a \u2212 \u03bcI1 (f + \u03be n ))2 + l(I2 )(a \u2212 \u03bcI2 (f + \u03be n ))2\n\n\u2264 \u2212\u03b3n + l(I1 )(b1 \u2212 \u03bcI1 (f + \u03be n ))2 + l(I2 )(b2 \u2212 \u03bcI2 (f + \u03be n ))2 ,\n\nl(I1 )(a \u2212 \u03bcI1 (f ))2 + l(I2 )(a \u2212 \u03bcI2 (f ))2\n\n\u2264 \u2212\u03b3n + 2l(I1 )\u03bcI1 (\u03be n )(a \u2212 b1 ) + 2l(I2 )\u03bcI2 (\u03be n )(a \u2212 b2 )\n+ l(I1 )(b1 \u2212 \u03bcI1 (f ))2 + l(I2 )(b2 \u2212 \u03bcI2 (f ))2\n\n\u2264 2l(I1 )\u03bcI1 (\u03be n )(a \u2212 b1 ) + 2l(I2 )\u03bcI2 (\u03be n )(a \u2212 b2 ) + l(I)\u03b52 (1/6 + 1/7)2\nq\n\nq\n\n\u2264 2|a \u2212 b1 | l(I1 )Cn log n/n + 2|a \u2212 b2 | l(I2 )Cn log n/n + l(I)\u03b52 /9.\nFrom k\u03beJnn k = o(1) we find bi \u2212 a = O(1) such that for large n depending on\n\u03b5, \u03b4 only\nl(I1 )(a \u2212 \u03bcI1 (f ))2 + l(I2 )(a \u2212 \u03bcI2 (f ))2 \u2264 l(I)\u03b52 /9.\nThe above results yield for t\u2032 \u2208 I that\nl(I1 )((f + \u03be n )Jn (t\u2032 ) \u2212 \u03bcI1 (f ))2 + l(I2 )((f + \u03be n )Jn (t\u2032 ) \u2212 \u03bcI2 (f ))2 \u2264 l(I)\u03b52 /9\nand hence\nmin(|(f + \u03be n )Jn (t\u2032 ) \u2212 \u03bcI1 (f )|, |(f + \u03be n )Jn (t\u2032 ) \u2212 \u03bcI2 (f )|) \u2264 \u03b5/3,\n\nmin(|(f + \u03be n )Jn (t\u2032 ) \u2212 \u03bcI1 (f \u0303)|, |(f + \u03be n )Jn (t\u2032 ) \u2212 \u03bcI2 (f \u0303)|) \u2264 \u03b5/2.\n\nThis shows that either k1I\u2229[t0 ,1) (f \u0303 \u2212 (f + \u03be n )Jn )k\u221e \u2264 \u03b5/2 or k1I\u2229[0,t0 ) (f \u0303 \u2212\n(f + \u03be n )Jn )k\u221e \u2264 \u03b5/2 holds for large n, depending on \u03b5, \u03b4 only.\n\n\f20\n\nL. BOYSEN ET AL.\n\nGiven Jn we define a new partition Pn\u2032 coarser than PJn by the following\nprocedure. First we join all neighboring intervals of class 1 and denote the\nresulting intervals again as class 1. If there are class 1 intervals left of length\n< \u03b4/3, there must be a left or a right neighbor which is class 2 and has an\noverlap of length > \u03b4/3 with an interval of constancy of f \u0303. Then we join the\nclass 1 interval to that neighbor (if there are two, to the left one). At the\nend, we join each class 3 interval I to its left neighbor, if k1I\u2229[t0 ,1) (f \u0303 \u2212 (f +\n\u03be n )Jn )k\u221e \u2264 \u03b5/2, or else to its right neighbor. The collection of those joined\nintervals is Pn\u2032 .\nBy the results for class 1, 2, 3 intervals we know for all I \u2208 Pn\u2032 that l(I) \u2265\n\u03b4/3. Further, for each I \u2208 Pn\u2032 there is I \u2032 \u2208 PJ(f \u0303) such that I \u0303\u2229 I \u2032 6= \u2205 for all I \u0303 \u2208\nP , I \u0303 \u2286 I, and k1I\u2229I \u2032 (f \u0303 \u2212 (f + \u03be n )Jn )k\u221e < \u03b5/2 holds. Thus, defining f \u0303n =\nPJn\nn\nn\n \u0303\nI\u2208Pn\u2032 \u03bcI ((f + \u03be )Jn )1I we obtain that kfn \u2212 (f + \u03be )Jn k\u221e < \u03b5. Thus (C2) is\nestablished and by Lemma A.4 {(f + \u03be n )Jn : n \u2208 N} is contained in a compact\nset. This completes the proof of the first assertion. The second assertion\nfollows from the fact that convergence in D([0, 1)) implies convergence in\nL\u221e ([0, 1]) if the limit is continuous [Billingsley (1968), page 112]. \u0003\nA.6. The proof of Theorem 2. Fix numbers kn \u2265 1, the precise magnitude of which will be chosen below. Further, sets Kn \u2286 {1/n, . . . , (n \u2212 1)/n}\nare chosen such that fKn is a best approximation of f by a step function from\nSn ([0, 1)) with kn \u2265 1 jumps, which exists since the subspace of Sn ([0, 1))\ncontaining functions g with #J(g) \u2264 kn and kgk \u2264 2kf k is compact.\nLet f \u0303kn be an approximation of f in S([0, 1)) with at most kn jumps for\nwhich kf \u0303kn \u2212 f k = O( k1\u03b1 ). Further, without loss of generality, we can asn\nsume that f \u0303k = f  \u0303 which implies kf \u0303k k\u221e \u2264 kf k\u221e . Moving each jump\nn\n\nJ(fkn )\n\nn\n\nof f \u0303kn to the next t \u2208 [0, 1] with nt \u2208 N but leaving the value of f \u0303kn unchanged on each plateau, we obtain a step function f \u0303n \u2208 Sn ([0, 1)) with\n1\nkf \u0303kn \u2212 f \u0303n k2 \u2264 2knn kf k2\u221e . This shows kf \u0303n \u2212 f k2 = O( k2\u03b1\n+ knn ). Since fKn is\nn\na best approximation, we derive\nkfKn \u2212 f k2 = O\n\n\u0012\n\n1\nkn\n+\n.\n2\u03b1\nkn\nn\n\u0013\n\nBy definition f\u02c6n is a minimizer of H\u0303\u03b3nn (*, f + \u03be n ) and we get\nH\u0303\u03b3nn (f\u02c6n , f + \u03be n ) \u2264 H\u0303\u03b3nn (fKn , f + \u03be n ).\nBy #Kn = kn , this implies \u03b3n #Jn + kf\u02c6n \u2212 f \u2212 \u03be n k2 \u2264 \u03b3n kn + kfKn \u2212 f \u2212 \u03be n k2\nand hence\nkf\u02c6n \u2212 f k2 \u2264 \u03b3n (kn \u2212 #Jn ) + kfKn \u2212 f k2 + 2hf \u2212 fKn , \u03be n i + 2hf\u02c6n \u2212 f, \u03be n i\n\u2264 \u03b3n (kn \u2212 #Jn ) + kfKn \u2212 f k2 + 2hf\u02c6n \u2212 fKn , \u03be n i.\n\n\f21\n\nJUMP PENALIZED LEAST SQUARES\n\nNow observe that J(f\u02c6n \u2212 fKn ) \u2286 Jn \u222a Kn which gives\nhf\u02c6n \u2212 fKn , \u03be n i = hf\u02c6n \u2212 fKn , (\u03be n )Jn \u222aKn i\n\n\u2264 kf\u02c6n \u2212 fKn kk(\u03be n )Jn \u222aKn k\n\n\u2264 kf\u02c6n \u2212 f kk(\u03be n )Jn \u222aKn k + kf \u2212 fKn kk(\u03be n )Jn \u222aKn k\n\u2264\n\n2+\u03b4\n1\nkf\u02c6n \u2212 f k2 +\nk(\u03be n )Jn \u222aKn k2\n2+\u03b4\n4\n1\n\u03b4\n+ kf \u2212 fKn k2 + k(\u03be n )Jn \u222aKn k2 .\n\u03b4\n4\n\nThe above inequalities yield\n\u03b4\n2 + 2\u03b4\nkf\u02c6n \u2212 f k2 \u2264 \u03b3n (kn \u2212 #Jn ) +\nkfKn \u2212 f k2 + (1 + \u03b4)k(\u03be n )Jn \u222aKn k2 .\n2+\u03b4\n\u03b4\nUsing the estimate (10) with Cn from (9) we obtain for C \u2032 = \u03b4/(2 + \u03b4)\nC \u2032 kf\u02c6n \u2212 f k2\n\u2264 \u03b3n (kn \u2212 #Jn ) + C\n\u0012\n\n\u2032\u2032\n\n\u2264 kn \u03b3n + (1 + \u03b4)Cn\n+\n\n\u0012\n\nkn\n1\n+\n2\u03b1\nkn\nn\n\nlog n C \u2032\u2032\n+\nn\nn\n\nlog n\nC \u2032\u2032\n+ (1 + \u03b4)Cn\n,\n2\u03b1\nkn\nn\n\nlog n\n(#Jn + kn + 1)\nn\n\n\u0013\n\n+ (1 + \u03b4)Cn\n\n\u0013\n\n+ #Jn (1 + \u03b4)Cn\n\n\u0012\n\nlog n\n\u2212 \u03b3n\nn\n\n\u0013\n\nfor some constant C \u2032\u2032 depending on f . We get from \u03b3n \u2265 (1 + \u03b4)12\u03b2 log n/n\ntogether with the relation lim supn\u2192\u221e Cn \u2264 12\u03b2 that (1 + \u03b4)Cn log n/n \u2264\n\u03b3n and C \u2032\u2032 /n \u2264 \u03b3n for large enough n, hence C \u2032 kf\u02c6n \u2212 f k2 \u2264 \u03b3n (3kn + 1) +\n\u22121/(2\u03b1+1)\n\u230b we obtain\nC \u2032\u2032 /kn2\u03b1 . Choosing kn = \u230a\u03b3n\nkf\u02c6n \u2212 f k2 = O(\u03b3n2\u03b1/(2\u03b1+1) )\nand the proof is complete.\nA.7. The proof of Theorem 3, Theorem 1(iii) and Theorem 4(iii).\nProof of Theorem 3(ii).\n(13)\n\n\u2200t \u2208 J(f ) \u2203tn \u2208 Jn\n\n1. First we will show that\nwith |tn \u2212 t| < mpl(f )/3.\n\nFrom part (i) of Theorem 4 and S([0, 1)) \u2282 D([0, 1)) we obtain immediately\nD([0,1))\n\n\u2212\u2212\u2192 f . Therefore, there is some random integer n0 such that for\nthat f\u02c6n \u2212\u2212\u2212\nn\u2192\u221e\n\n\f22\n\nL. BOYSEN ET AL.\n\nall n \u2265 n0\n(14)\n\n\u03c1S (f\u02c6n , f )\n< min(min{|f (t) \u2212 f (t \u2212 0)| : t \u2208 J(f )}/2, |log(1 \u2212 23 mpl(f ))|).\n\nThe relation (13) is a direct consequence of inequality (14). Assume (13)\ndoes not hold. In this case, a Lipschitz function \u03bb \u2208 \u039b1 with L(\u03bb) < | log(1 \u2212\n2/3mpl(f ))| could not achieve t \u2208 J(f\u02c6n \u25e6\u03bb) and hence kf\u02c6n \u25e6\u03bb\u2212 f k\u221e \u2265 |f (t)\u2212\nf (t \u2212 0)|/2 contradicting (14).\n2. Now we will show that for all t \u2208 J(f ) there exists a sequence tn \u2208 Jn ,\nsuch that |tn \u2212 t| = O(log n/n). For any t \u2208 J(f ) let tn be a point in Jn\nclosest to t. We want to apply Lemma A.3(iii). For that goal, suppose for\nthe moment that tn < t and f (t) > f (t \u2212 0). Choose In \u2208 PJn as interval\nwith right end point tn and set In\u2032 = [tn , sn ) where nsn \u2208 N is such that\n|sn \u2212 t| < 1/n as well as an = \u03bcIn (f\u02c6n ) and bn = \u03bcIn\u2032 (f\u02c6n ). Then Lemma A.3(iii)\nshows\n\u0013\n\u0012\nan + bn\n\u2265 0.\n(bn \u2212 an ) \u03bcIn\u2032 (f + \u03be n ) \u2212\n2\nD([0,1))\n\n\u2212\u2212\u2192 f implies an \u2212\u2212\u2212\n\u2212\u2212\u2192 f (t \u2212 0) and bn \u2212\u2212\u2212\n\u2212\u2212\u2192 f (t) such that\nClearly, f\u02c6n \u2212\u2212\u2212\nn\u2192\u221e\n\nalmost surely eventually\n\nn\u2192\u221e\n\nn\u2192\u221e\n\nan + bn\n\u03bcIn\u2032 (f ) \u2212\n\u2265 \u2212\u03bcIn\u2032 (\u03be n ) \u2265 \u2212Cn\n2\n\ns\n\nlog n\n.\nnl(In\u2032 )\n\nWe know further limn\u2192\u221e \u03bcIn\u2032 (f ) = f (t \u2212 0) such that almost surely eventually\nf (t \u2212 0) \u2212 f (t)\n0>\n\u2265 \u2212Cn\n3\n\ns\n\nlog n\nnl(In\u2032 )\n\nwhich implies l(In\u2032 ) = O(log n/n) and |tn \u2212 t| = O(log n/n).\n3. Next we will prove that there exists no sequence tn \u2208 Jn which satisfies\nthe relation lim supn\u2192\u221e (n/ log n)\u03c1H ({tn }, J) = \u221e. We consider two adja \u0303 =\ncent intervals I, I \u2032 \u2208 PJn for which there is an I \u0303 \u2208 PJ(f ) with l(I \u222a I \u2032 \\ I)\nO(log n/n). Then\nR\nR\n \u0303 f (u) du \u2212 l(I)\n|l(I \u2229 I)\nI\nI\u2229I \u0303 f (u) du|\n|\u03bcI (f ) \u2212 \u03bcI\u2229I \u0303(f )| =\n \u0303\nl(I)l(I \u2229 I)\n=\n\n \u0303\n|l(I \u2229 I)\n\nR\n\n \u0303\n\\ I)\n \u0303\nl(I)l(I \u2229 I)\n\nI\\I \u0303 f (u) du \u2212 l(I\n\nR\n\nI\u2229I \u0303 f (u) du|\n\n \u0303\n \u0303\n \u0303\nl(I \\ I)\nl(I \u2229 I)l(I\n\\ I)\n= 2kf k\u221e\n \u0303\nl(I)\nl(I)l(I \u2229 I)\n\u2032\nand a similar estimate holds for I . By means of \u03bcI\u2229I \u0303(f ) = \u03bcI \u2032 \u2229I \u0303(f ) and\n\u2264 2kf k\u221e\n\n\f23\n\nJUMP PENALIZED LEAST SQUARES\n\n1/l(I) = O(1/\u03b3n ) we obtain\n(\u03bcI (f ) \u2212 \u03bcI \u2032 (f ))2 \u2264 (1/l(I)2 + 1/l(I \u2032 )2 )O\n\n\u0012\n\nlog2 n\nn2\n\nlog2 n\n\u2264 (1/l(I) + 1/l(I ))O\n\u03b3n n 2\n\u0012\n\n\u2032\n\n\u0012\n\n= (1/l(I) + 1/l(I \u2032 ))o\n\n\u0013\n\n\u0013\n\nlog n\n.\nn\n\u0013\n\nNow Lemma A.3(i) implies\n\u03b3n \u2264\n\u2264\n\nl(I)l(I \u2032 )\n(\u03bcI (f + \u03be n ) \u2212 \u03bcI \u2032 (f + \u03be n ))2\nl(I) + l(I \u2032 )\nl(I)l(I \u2032 )\n(3(\u03bcI (f ) \u2212 \u03bcI \u2032 (f ))2 + 3\u03bcI (\u03be n )2 + 3\u03bcI \u2032 (\u03be n )2 )\n\u2032\nl(I) + l(I )\n\nlog n\nlog n l(I)l(I \u2032 )\n(1/l(I) + 1/l(I \u2032 )) = O\n\u2264O\n.\n\u2032\nn\nl(I) + l(I )\nn\n\u0012\n\n\u0012\n\n\u0013\n\n\u0013\n\nThis contradicts \u03b3n n/ log n \u2192 \u221e. Thus, almost surely, there are only finitely\nmany n for which there are two adjacent intervals I, I \u2032 \u2208 PJn and I \u0303 \u2208 PJ(f )\n \u0303 = O(log n/n). Consequently, \u03c1H (Jn , J(f )) = O(log n/n),\nwith l(I \u222a I \u2032 \\ I)\nwhich implies the statement. \u0003\nProof of Theorem 3(i). 4. Suppose now there are sn , tn \u2208 Jn with\nsn \u2192 t, tn \u2192 t for t \u2208 J(f ). Then we have by the previous result that |tn \u2212\nsn | = O(log n/n) as well as 1/|tn \u2212sn | = O(1/\u03b3n ). This gives us log n/(n\u03b3n ) =\nO(1) contradicting n\u03b3n / log n \u2192 \u221e. Thus #Jn = #J(f ) eventually. \u0003\nProof of Theorem 3(iii). 5. For this statement, observe that in the\nspecial situation considered in step 2, it is not necessary to assume |sn \u2212 t| <\n1/n. Hence for any sn \u2208 [tn , t) with nsn \u2208 N we have almost surely eventually\n0>\n\nf (t \u2212 0) \u2212 f (t)\n\u2265 \u2212\u03bc[tn ,sn ) (\u03be n )\n3\n\nconditional on tn < t. Denote p the largest integer such that p/n \u2264 t \u2212 1/n.\nUsing the exponential inequality [cf. Petrov (1975), Sections 3 and 4]\n(15)\n\nP\n\nn\nX\ni=1\n\n\u03bci \u03bein\n\n!\n\n\u0012\n\n\u2265 z \u2264 exp \u2212\n\nz2\n4\u03b2\n\nPn\n\n2\ni=1 \u03bci\n\n\u0013\n\nfor all z \u2208 R,\n\nfor triangular arrays fulfilling Condition (A) and all numbers \u03bci , i = 1, . . . , n,\n\n\f24\n\nL. BOYSEN ET AL.\n\nwe obtain for all k\u2032 \u2208 N\n\nP({k\u2032 /n < (t \u2212 tn ) \u2264 (k\u2032 + 1)/n})\n\u2264P\n\n\u0012\u001a\n\n\u03bc[(p+1\u2212k\u2032 )/n,(p+1\u2212k\u2032 +i)/n) (\u03be n ) \u2265\n\nf (t) \u2212 f (t \u2212 0)\n3\n\nfor all i = 1, . . . , k\u2032\n=P\n\n\u0012\u001a\n\n\u2264P\n\n\u0012\u001a\n\nn\nn\n\u03bep\u2212k\n\u2032 +1 + * * * + \u03bep\u2212k \u2032 +i\nn\n\u03bep\u2212k\n\u2032 +1\n\n\u2265\n\n\u001b\u0013\n\nf (t) \u2212 f (t \u2212 0)\nfor all i = 1, . . . , k\u2032\n3\n\ni\n+ * * * + \u03bepn f (t) \u2212 f (t \u2212 0) \u001b\u0013\n\u2265\nk\u2032\n3\n\n\u001b\u0013\n\n\u2032\n\n\u2212z 2 k\n\u2212k\u2032 z 2\n\u2032\n=: q k ,\n= exp\n\u2264 exp\n4\u03b2\n4\u03b2\nwhere z = (f (t) \u2212 f (t \u2212 0))/3. Note that q < 1 depends on f (t) \u2212 f (t \u2212 0) and\n\u03b2 only. Clearly, we can use a similar argument if f (t \u2212 0) > f (t) or tn \u2265 t.\nSumming up these inequalities we obtain P({|t \u2212 tn | \u2265 k/n}) \u2264 2q k /(1 \u2212 q)\nand\n\u0012\n\n\u0013\n\n\u0012\n\n\u0012\n\n\u0013\u0013\n\nP({\u03c1H (Jn , J(f )) \u2265 k/n}) \u2264 2#J(f )q k /(1 \u2212 q).\n\nThis shows limk\u2192\u221e lim supn\u2192\u221e P({\u03c1H (Jn , J(f )) \u2265 k/n}) = 0, or in other\nwords \u03c1H (Jn , J(f )) = OP (n\u22121 ). \u0003\nProof of Theorem 1(iii), Theorem 4(iii). 6. By 4 and 5, we may\nchoose n so large that #Jn = #J(f ) and \u03c1H (Jn , J(f ))\nP\u2264 mpl(f )/3. Then\nthere is a unique 1\u20131 map \u03c6n : J(f ) 7\u2212\u2192 Jn for which t\u2208J(f ) |t \u2212 \u03c6n (t)| is\nminimal. We derive \u03c6n (t) \u2212 t = O(log n/n) for all t \u2208 J(f ). Extend now \u03c6n\nby \u03c6n (0) = 0 and \u03c6n (1) = 1. For [s, t) \u2208 PJ(f ) we get thus\nk1[\u03c6n (s),\u03c6n (t)) \u2212 1[s,t) k = O\n\ns\n\u0012\n\nlog n\n.\nn\n\u0013\n\np\n\nFurther, kf k\u221e < \u221e yields |\u03bc[\u03c6n (s),\u03c6n (t)) (f ) \u2212 \u03bc[s,t) (f )| = O( log n/n). Lemp\nma A.1 implies that \u03bc[\u03c6n (s),\u03c6n (t)) (\u03be n ) = O( log n/n) such that kf\u02c6n \u2212 f k =\np\nO( log n/n) which yields the first part of Theorem 1(iii) and\n\u0012s\n\nlog n\n.\nn\nWe define an extension \u03bbn \u2208 \u039b1 of \u03c6n by linear\np interpolation. From above,\n\u02c6\nwe obtain the estimate kfn \u2212 f \u25e6 \u03bbn k\u221e = O( log n/n). Furthermore,\nn\n\nk\u03bc[\u03c6n (s),\u03c6n (t)) (f + \u03be )1[\u03c6n (s),\u03c6n (t)) \u2212 \u03bc[s,t) (f )1[s,t) k = O\n\nL(\u03c6n ) =\n\nmax\n\n[s,t)\u2208PJ (f )\n\nlog\n\n\u03c6n (t) \u2212 \u03c6n (s)\n= O(log n/n)\nt\u2212s\n\n\u0013\n\n\f25\n\nJUMP PENALIZED LEAST SQUARES\n\nsuch that \u03c1S (f\u02c6n , f ) = O( log n/n).\n7. By direct calculations we obtain from (15) and Lemma A.1 that\np\n\nmax |\u03bcI (\u03be n )| = OP (n\u22121/2 ).\n\nI\u2208PJn\n\nUsing this estimate and \u03c1H (Jn , J(f )) = OP ( n1 ) in the same way as the almost\n\u02c6\n\u02c6\nsure rate\n\u221a in step 6, we obtain that \u03c1S (fn , f ) and kfn \u2212 f k are of order\nOP (1/ n). \u0003\nA.8. The proof of Theorem 5. It is sufficient to show that\nP(#J(f\u02c6nM R ) = #J(f\u02c6n )) \u2212\u2212\u2212\n\u2212\u2212\u2192 1.\nn\u2192\u221e\n\nAssume there exists some subsequence nk such that #J(f\u02c6nMk R ) < #J(f )\nfor all nk . As a step function with #J(f ) jumps cannot be approximated\nby a sequence of functions with fewer jumps, there exists a sequence of\nconnected intervals Ink with Ink \u2208 Bnk such that lim inf nk \u2192\u221e l(Ink ) \u2265 \u01eb1 > 0\nand for I \u0303nk = {i : xni k \u2208 Ink }\n1 X nk \u02c6M R nk\nf i \u2212 fnk (xi ) \u2265 \u01eb2 > 0.\n#I \u0303nk  \u0303\ni\u2208Ink\n\nConsequently by Lemma A.1 for large nk\n|\n\nP\n\ni\u2208I \u0303nk\n\nYink \u2212 f\u02c6nMk R (xni k )|\nq\n\n#I \u0303nk\n\n| i\u2208I \u0303n \u03beink |\n\u221a\n\u2265 \u01eb2 \u01eb1 n k \u2212 q k\n#I \u0303nk\np\n\u221a\n\u2265 \u01eb2 \u01eb1 nk \u2212 O( log nk )\nP\n\nP-a.s.\n\nThis implies that for large nk the MR-criterion is not satisfied. By Theorem 3(i) we have P(#J(f\u02c6n ) = #J(f\u02c6)) \u2192 1 for n \u2192 \u221e. Hence P(#J(f\u02c6nM R ) \u2265\n#J(f\u02c6n )) \u2212\u2212\u2212\n\u2212\u2212\u2192 1.\nn\u2192\u221e\n\nIt remains to show that f\u02c6nM R has asymptotically at most as many jumps\nas f\u02c6n . Observe that\nmax\n\n(16)\n\n1\u2264j\u2264k\u2264n\n\n\u2264\n\n|\n\nPk\n\ni=j\n\n\u221a\n\nmax\n\nYin \u2212 f\u02c6n (xni )|\nk\u2212j +1\n\n1\u2264j\u2264k\u2264n\n\n|\n\nPk \u02c6 n\nn\nn\ni=j fn (xi ) \u2212 f i |\ni=j \u03bei | + |\n\nPk\n\n\u221a\n\nk\u2212j+1\n\n.\n\n\f26\n\nL. BOYSEN ET AL.\n\nBy the Cauchy\u2013Schwarz inequality and Theorem 1(iii) we have for 1 \u2264 j \u2264\nk\u2264n\n|\n\nn\n\u02c6 n\ni=j fn (xi ) \u2212 f i |\n\nPk\n\n\u221a\n\nk\u2212j +1\n\nn\n\nn|hf\u02c6n \u2212 f , 1[j/n,(k+1)/n) i|\n\u221a\n=\nk\u2212j +1\n\nk1[j/n,(k+1)/n) k\nn\n\u2264n \u221a\n(kf\u02c6n \u2212 f k + kf \u2212 f k)\nk\u2212j +1\n\u221a\nn\n= n(kf\u02c6n \u2212 f k + kf \u2212 f k) = OP (1)\n\nuniformly in j, k. Lemma A.2 implies\nmax\n\n1\u2264j\u2264k\u2264n\n\nPk\n\n|\n\u221a\n\nn\ni=j \u03bei |\n\nk\u2212j+1\n\np\n\nApplying the results above to (16) we arrive at\nmax\n\n1\u2264j\u2264k\u2264n\n\n|\n\nPk\n\ni=j\n\n\u221a\n\np\n\n= \u03c3 2 log n + oP ( log n).\n\np\np\nYin \u2212 f\u02c6n (xni )|\n= \u03c3 2 log n + oP ( log n).\nk\u2212j+1\n\nSince \u03c3\u0302 is a consistent estimate of \u03c3, this implies that the probability that\nf\u02c6n satisfies the MR-criterion tends to 1 as n goes to infinity. As \u03b3\u0302n is chosen maximal such that the MR-criterion is satisfied, we can conclude P(\u03b3\u0302n \u2265\n\u03b3n ) \u2212\u2212\u2212\n\u2212\u2212\u2192 1 and consequently P(#J(f\u02c6nM R ) \u2264 #J(f\u02c6n )) \u2212\u2212\u2212\n\u2212\u2212\u2192 1 which proves\nn\u2192\u221e\n\nn\u2192\u221e\n\nthe claim. \u0003\n\nAcknowledgment. We wish to thank L. Birg\u00e9, L. Brown, L. D\u00fcmbgen,\nF. Friedrich, K. Gr\u00f6chenig, T. Hotz, E. Liebscher, E. Mammen, G. Winkler,\ntwo referees and two Associate Editors for helpful comments and bibliographic information.\nREFERENCES\nAurich, V. and Weule, J. (1995). Nonlinear Gaussian filters performing edge preserving\ndiffusion. In Proc. 17. DAGM-Symposium, Bielefeld 538\u2013545. Springer, Berlin.\nBillingsley, P. (1968). Convergence of Probability Measures. Wiley, New York.\nMR0233396\nBirg\u00e9, L. and Massart, P. (2007). Minimal penalties for Gaussian model selection.\nProbab. Theory Related Fields 138 33\u201373. MR2288064\nBlake, A. and Zisserman, A. (1987). Visual Reconstruction. MIT Press, Cambridge,\nMA. MR0919733\nBoysen, L., Liebscher, V., Munk, A. and Wittich, O. (2007). Scale space consistency\nof piecewise constant least squares estimators-another look at the regressogram. IMS\nLecture Notes Monograph Ser. 55 65\u201384. IMS, Beachwood, OH.\nBraun, J. V., Braun, R. K. and M\u00fcller, H.-G. (2000). Multiple change-point fitting\nvia quasilikelihood, with application to DNA sequence segmentation. Biometrika 87\n301\u2013314. MR1782480\n\n\fJUMP PENALIZED LEAST SQUARES\n\n27\n\nBurchard, H. G. and Hale, D. F. (1975). Piecewise polynomial approximation on\noptimal meshes. J. Approximation Theory 14 128\u2013147. MR0374761\nChaudhuri, P. and Marron, J. S. (2000). Scale space view of curve estimation. Ann.\nStatist. 28 408\u2013428. MR1790003\nChristensen, J. and Rudemo, M. (1996). Multiple change-point analysis of disease incidence rates. Prev. Vet. Med. 54\u201376.\nChu, C., Glad, I., Godtliebsen, F. and Marron, J. (1998). Edge-preserving smoothers\nfor image processing. J. Amer. Statist. Assoc. 93 526\u2013541. MR1631321\nDal Maso, G. (1993). An Introduction to \u0393-convergence. Birkh\u00e4user, Boston. MR1201152\nDavies, P. L. and Kovac, A. (2001). Local extremes, runs, strings and multiresolution.\nAnn. Statist. 29 1\u201365. MR1833958\nDeVore, R. A. (1998). Nonlinear approximation. In Acta Numerica 1998. Acta Numer.\n7 51\u2013150. Cambridge Univ. Press, Cambridge. MR1689432\nDeVore, R. A. and Lorentz, G. G. (1993). Constructive Approximation. Springer,\nBerlin. MR1261635\nDonoho, D. (2006a). For most large underdetermined systems of equations, the minimal l1 -norm near-solution approximates the sparsest near-solution. Comm. Pure Appl.\nMath. 59 907\u2013934. MR2222440\nDonoho, D. (2006b). For most large underdetermined systems of equations, the minimal l1 -norm solution is the sparsest solution. Comm. Pure Appl. Math. 59 797\u2013829.\nMR2217606\nDonoho, D. L. (1997). CART and best-ortho-basis: A connection. Ann. Statist. 25 1870\u2013\n1911. MR1474073\nDonoho, D. L. (1999). Wedgelets: Nearly minimax estimation of edges. Ann. Statist. 27\n859\u2013897. MR1724034\nDonoho, D. L. and Johnstone, I. M. (1994). Ideal spatial adaptation by wavelet shrinkage. Biometrika 81 425\u2013455. MR1311089\nDonoho, D. L., Johnstone, I. M., Kerkyacharian, G. and Picard, D. (1995).\nWavelet shrinkage: Asymptopia? J. Roy. Statist. Soc. Ser. B 57 301\u2013369. MR1323344\nEubank, R. L. (1999). Nonparametric Regression and Spline Smoothing, 2nd ed. Dekker,\nNew York. MR1680784\nFredkin, D. and Rice, J. (1992). Baysian restoration and single-channel patch clamp\nrecordings. Biometrics 48 427\u2013428.\nFriedrich, F. (2005). Complexity penalized segmentations in 2D. Ph.D. thesis, Institut\nf\u00fcr Biomathematik und Biometrie an der Gesellschaft f\u00fcr Umwelt und Gesundheit,\nM\u00fcnchen-Neuherberg.\nFriedrich, F., Kempe, A., Liebscher, V. and Winkler, G. (2008). Complexity penalized m-estimation: Fast computation. J. Comput. Graph. Statist. 17 1\u201324.\nF\u00fchr, H., Demaret, L. and Friedrich, F. (2006). Beyond wavelets: New image representation paradigms. In Document and Image Compression (M. Barni, ed.) Chapter 7\n179\u2013206. CRC Press.\nGeman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the\nBayesian restoration of images. IEEE Trans. Pattern Anal. Mach. Intell. 6 721\u2013741.\nGodtliebsen, F., Spj\u00f8tvoll, E. and Marron, J. S. (1997). A nonlinear Gaussian filter\napplied to images with discontinuities. J. Nonparametr. Statist. 8 21\u201343. MR1658111\nHall, P. and Titterington, D. M. (1992). Edge-preserving and peak-preserving\nsmoothing. Technometrics 34 429\u2013440. MR1190262\nHampel, F. R., Ronchetti, E. M., Rousseeuw, P. J. and Stahel, W. A. (1986).\nRobust Statistics. Wiley, New York. MR0829458\n\n\f28\n\nL. BOYSEN ET AL.\n\nHess, C. (1996). Epi-convergence of sequences of normal integrands and strong consistency\nof the maximum likelihood estimator. Ann. Statist. 24 1298\u20131315. MR1401851\nHinkley, D. V. (1970). Inference about the change-point in a sequence of random variables. Biometrika 57 1\u201317. MR0273727\nIsing, E. (1925). Beitrag zur theorie des ferromagnetismus. Z. Phys. 31 253.\nKohler, M. (1999). Nonparametric estimation of piecewise smooth regression functions.\nStatist. Probab. Lett. 43 49\u201355. MR1707251\nK\u00fcnsch, H. R. (1994). Robust priors for smoothing and image restoration. Ann. Inst.\nStatist. Math. 46 1\u201319. MR1272743\nLoader, C. R. (1996). Change point estimation using nonparametric regression. Ann.\nStatist. 24 1667\u20131678. MR1416655\nMammen, E. and van de Geer, S. (1997). Locally adaptive regression splines. Ann.\nStatist. 25 387\u2013413. MR1429931\nM\u00fcller, H.-G. (1992). Change-points in nonparametric regression analysis. Ann. Statist.\n20 737\u2013761. MR1165590\nM\u00fcller, H.-G. and Stadtm\u00fcller, U. (1999). Discontinuous versus smooth regression.\nAnn. Statist. 27 299\u2013337. MR1701113\nPetrov, V. V. (1975). Sums of Independent Random Variables. Springer, New York.\nMR0388499\nPolzehl, J. and Spokoiny, V. (2003). Image denoising: Pointwise adaptive approach.\nAnn. Statist. 31 30\u201357. MR1962499\nP\u00f6tscher, B. and Leeb, H. (2008). Sparse estimators and the oracle property, or the\nreturn of Hodges' estimator. J. Econometrics 142 201\u2013211. MR2394290\nPotts, R. (1952). Some generalized order-disorder transitions. Proc. Camb. Philos. Soc.\n48 106\u2013109. MR0047571\nShao, Q. M. (1995). On a conjecture of R\u00e9v\u00e9sz. Proc. Amer. Math. Soc. 123 575\u2013582.\nMR1231304\nSpokoiny, V. G. (1998). Estimation of a function with discontinuities via local polynomial\nfit with an adaptive window choice. Ann. Statist. 26 1356\u20131378. MR1647669\nTibshirani, R. (1996). Regression shrinkage and selection via the Lasso. J. Roy. Statist.\nSoc. Ser. B 58 267\u2013288. MR1379242\nTomkins, R. J. (1974). On the law of the iterated logarithm for double sequences of\nrandom variables. Z. Wahrsch. Verw. Gebiete 30 303\u2013314. MR0405555\nTukey, J. W. (1961). Curves as parameters, and touch estimation. Proc. 4th Berkeley Sympos. Math. Statist. and Probab. I 681\u2013694. Univ. California Press, Berkeley.\nMR0132677\nvan de Geer, S. (2001). Least squares estimation with complexity penalties. Math. Methods Statist. 10 355\u2013374. MR1867165\nWinkler, G. and Liebscher, V. (2002). Smoothers for discontinuous signals. J. Nonparametr. Statist. 14 203\u2013222. MR1905594\nWinkler, G., Wittich, O., Liebscher, V. and Kempe, A. (2005). Don't shed tears\nover breaks. Jahresber. Deutsch. Math.-Verein. 107 57\u201387. MR2156103\nYao, Y.-C. (1988). Estimating the number of change-points via Schwarz' criterion. Statist.\nProbab. Lett. 6 181\u2013189. MR0919373\nYao, Y.-C. and Au, S. T. (1989). Least-squares estimation of a step function. Sankhy\u0101\nSer. A 51 370\u2013381. MR1175613\n\n\fJUMP PENALIZED LEAST SQUARES\nL. Boysen\nA. Munk\nInstitute for Mathematical Statistics\nGeorg\u2013August\u2013Universit\u00e4t G\u00f6ttingen\nMaschm\u00fchlenweg 8\u201310\n37073 G\u00f6ttingen\nGermany\nE-mail: boysen@math.uni-goettingen.de\nmunk@math.uni-goettingen.de\nV. Liebscher\nDepartement of Mathematics\nand Computer Science\nUniversit\u00e4t Greifswald\nJahnstrasse 15a\n17487 Greifswald\nGermany\nE-mail: volkmar.liebscher@uni-greifswald.de\n\n29\n\nA. Kempe\nInstitute of Biomathematics and Biometry\nGSF-National Research Centre\nfor Environment\nIngolst\u00e4dter Landstrasse 1\n85764 Neuherberg\nGermany\nE-mail: kempe@gsf.de\nO. Wittich\nDepartment of Mathematics\nand Computer Science\nTechnische Universiteit Eindhoven\nDen Dolech 2\n5600 MB Eindhoven\nThe Netherlands\nE-mail: O.Wittich@tue.nl\n\n\f"}