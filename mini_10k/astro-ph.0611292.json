{"id": "http://arxiv.org/abs/astro-ph/0611292v1", "guidislink": true, "updated": "2006-11-09T12:27:04Z", "updated_parsed": [2006, 11, 9, 12, 27, 4, 3, 313, 0], "published": "2006-11-09T12:27:04Z", "published_parsed": [2006, 11, 9, 12, 27, 4, 3, 313, 0], "title": "Adaptive optics simulation performance improvements using reconfigurable\n  logic", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=astro-ph%2F0611166%2Castro-ph%2F0611263%2Castro-ph%2F0611117%2Castro-ph%2F0611754%2Castro-ph%2F0611253%2Castro-ph%2F0611010%2Castro-ph%2F0611591%2Castro-ph%2F0611267%2Castro-ph%2F0611292%2Castro-ph%2F0611469%2Castro-ph%2F0611952%2Castro-ph%2F0611560%2Castro-ph%2F0611094%2Castro-ph%2F0611362%2Castro-ph%2F0611758%2Castro-ph%2F0611871%2Castro-ph%2F0611843%2Castro-ph%2F0611426%2Castro-ph%2F0611561%2Castro-ph%2F0611336%2Castro-ph%2F0611731%2Castro-ph%2F0611940%2Castro-ph%2F0611858%2Castro-ph%2F0611517%2Castro-ph%2F0611801%2Castro-ph%2F0611171%2Castro-ph%2F0611677%2Castro-ph%2F0611102%2Castro-ph%2F0611497%2Castro-ph%2F0611137%2Castro-ph%2F0611380%2Castro-ph%2F0611080%2Castro-ph%2F0611236%2Castro-ph%2F0611735%2Castro-ph%2F0611209%2Castro-ph%2F0611722%2Castro-ph%2F0611461%2Castro-ph%2F0611491%2Castro-ph%2F0611246%2Castro-ph%2F0611644%2Castro-ph%2F0611945%2Castro-ph%2F0611647%2Castro-ph%2F0611688%2Castro-ph%2F0611738%2Castro-ph%2F0611630%2Castro-ph%2F0611149%2Castro-ph%2F0611232%2Castro-ph%2F0611446%2Castro-ph%2F0611255%2Castro-ph%2F0611040%2Castro-ph%2F0611652%2Castro-ph%2F0611023%2Castro-ph%2F0611097%2Castro-ph%2F0611194%2Castro-ph%2F0611624%2Castro-ph%2F0611690%2Castro-ph%2F0611327%2Castro-ph%2F0611014%2Castro-ph%2F0611371%2Castro-ph%2F0611621%2Castro-ph%2F0611870%2Castro-ph%2F0611642%2Castro-ph%2F0611180%2Castro-ph%2F0611437%2Castro-ph%2F0611392%2Castro-ph%2F0611869%2Castro-ph%2F0611462%2Castro-ph%2F0611607%2Castro-ph%2F0611947%2Castro-ph%2F0611057%2Castro-ph%2F0611504%2Castro-ph%2F0611154%2Castro-ph%2F0611795%2Castro-ph%2F0611403%2Castro-ph%2F0611435%2Castro-ph%2F0611615%2Castro-ph%2F0611441%2Castro-ph%2F0611266%2Castro-ph%2F0611430%2Castro-ph%2F0611949%2Castro-ph%2F0611893%2Castro-ph%2F0611217%2Castro-ph%2F0611663%2Castro-ph%2F0611820%2Castro-ph%2F0611067%2Castro-ph%2F0611004%2Castro-ph%2F0611093%2Castro-ph%2F0611686%2Castro-ph%2F0611806%2Castro-ph%2F0611848%2Castro-ph%2F0611398%2Castro-ph%2F0611664%2Castro-ph%2F0611031%2Castro-ph%2F0611703%2Castro-ph%2F0611639%2Castro-ph%2F0611365%2Castro-ph%2F0611333%2Castro-ph%2F0611520%2Castro-ph%2F0611200%2Castro-ph%2F0611852%2Castro-ph%2F0611676&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adaptive optics simulation performance improvements using reconfigurable\n  logic"}, "summary": "A technique used to accelerate an adaptive optics simulation platform using\nreconfigurable logic is described. The performance of parts of this simulation\nhave been improved by up to 600 times (reducing computation times by this\nfactor) by implementing algorithms within hardware and enables adaptive optics\nsimulations to be carried out in a reasonable timescale. This demonstrates that\nit is possible to use reconfigurable logic to accelerate computational codes by\nvery large factors when compared with conventional software approaches, and\nthis has relevance for many computationally intensive applications. The use of\nreconfigurable logic for high performance computing is currently in its infancy\nand has never before been applied to this field.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=astro-ph%2F0611166%2Castro-ph%2F0611263%2Castro-ph%2F0611117%2Castro-ph%2F0611754%2Castro-ph%2F0611253%2Castro-ph%2F0611010%2Castro-ph%2F0611591%2Castro-ph%2F0611267%2Castro-ph%2F0611292%2Castro-ph%2F0611469%2Castro-ph%2F0611952%2Castro-ph%2F0611560%2Castro-ph%2F0611094%2Castro-ph%2F0611362%2Castro-ph%2F0611758%2Castro-ph%2F0611871%2Castro-ph%2F0611843%2Castro-ph%2F0611426%2Castro-ph%2F0611561%2Castro-ph%2F0611336%2Castro-ph%2F0611731%2Castro-ph%2F0611940%2Castro-ph%2F0611858%2Castro-ph%2F0611517%2Castro-ph%2F0611801%2Castro-ph%2F0611171%2Castro-ph%2F0611677%2Castro-ph%2F0611102%2Castro-ph%2F0611497%2Castro-ph%2F0611137%2Castro-ph%2F0611380%2Castro-ph%2F0611080%2Castro-ph%2F0611236%2Castro-ph%2F0611735%2Castro-ph%2F0611209%2Castro-ph%2F0611722%2Castro-ph%2F0611461%2Castro-ph%2F0611491%2Castro-ph%2F0611246%2Castro-ph%2F0611644%2Castro-ph%2F0611945%2Castro-ph%2F0611647%2Castro-ph%2F0611688%2Castro-ph%2F0611738%2Castro-ph%2F0611630%2Castro-ph%2F0611149%2Castro-ph%2F0611232%2Castro-ph%2F0611446%2Castro-ph%2F0611255%2Castro-ph%2F0611040%2Castro-ph%2F0611652%2Castro-ph%2F0611023%2Castro-ph%2F0611097%2Castro-ph%2F0611194%2Castro-ph%2F0611624%2Castro-ph%2F0611690%2Castro-ph%2F0611327%2Castro-ph%2F0611014%2Castro-ph%2F0611371%2Castro-ph%2F0611621%2Castro-ph%2F0611870%2Castro-ph%2F0611642%2Castro-ph%2F0611180%2Castro-ph%2F0611437%2Castro-ph%2F0611392%2Castro-ph%2F0611869%2Castro-ph%2F0611462%2Castro-ph%2F0611607%2Castro-ph%2F0611947%2Castro-ph%2F0611057%2Castro-ph%2F0611504%2Castro-ph%2F0611154%2Castro-ph%2F0611795%2Castro-ph%2F0611403%2Castro-ph%2F0611435%2Castro-ph%2F0611615%2Castro-ph%2F0611441%2Castro-ph%2F0611266%2Castro-ph%2F0611430%2Castro-ph%2F0611949%2Castro-ph%2F0611893%2Castro-ph%2F0611217%2Castro-ph%2F0611663%2Castro-ph%2F0611820%2Castro-ph%2F0611067%2Castro-ph%2F0611004%2Castro-ph%2F0611093%2Castro-ph%2F0611686%2Castro-ph%2F0611806%2Castro-ph%2F0611848%2Castro-ph%2F0611398%2Castro-ph%2F0611664%2Castro-ph%2F0611031%2Castro-ph%2F0611703%2Castro-ph%2F0611639%2Castro-ph%2F0611365%2Castro-ph%2F0611333%2Castro-ph%2F0611520%2Castro-ph%2F0611200%2Castro-ph%2F0611852%2Castro-ph%2F0611676&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A technique used to accelerate an adaptive optics simulation platform using\nreconfigurable logic is described. The performance of parts of this simulation\nhave been improved by up to 600 times (reducing computation times by this\nfactor) by implementing algorithms within hardware and enables adaptive optics\nsimulations to be carried out in a reasonable timescale. This demonstrates that\nit is possible to use reconfigurable logic to accelerate computational codes by\nvery large factors when compared with conventional software approaches, and\nthis has relevance for many computationally intensive applications. The use of\nreconfigurable logic for high performance computing is currently in its infancy\nand has never before been applied to this field."}, "authors": ["Alastair Basden"], "author_detail": {"name": "Alastair Basden"}, "author": "Alastair Basden", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1364/AO.46.000900", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/astro-ph/0611292v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/astro-ph/0611292v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "19 pages with figures at end. Accepted by applied optics", "arxiv_primary_category": {"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/astro-ph/0611292v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/astro-ph/0611292v1", "journal_reference": null, "doi": "10.1364/AO.46.000900", "fulltext": "Adaptive optics simulation performance improvements\nusing reconfigurable logic\n\narXiv:astro-ph/0611292v1 9 Nov 2006\n\nAlastair Basden\nCentre for Advanced Instrumentation, Department of Physics, Durham University, South\nRoad, Durham, DH1 3LE, UK\na.g.basden@durham.ac.uk\n\nA technique used to accelerate an adaptive optics simulation platform using\nreconfigurable logic is described. The performance of parts of this simulation\nhave been improved by up to 600 times (reducing computation times by this\nfactor) by implementing algorithms within hardware and enables adaptive optics simulations to be carried out in a reasonable timescale. This demonstrates\nthat it is possible to use reconfigurable logic to accelerate computational codes\nby very large factors when compared with conventional software approaches,\nand this has relevance for many computationally intensive applications. The\nuse of reconfigurable logic for high performance computing is currently in\nits infancy and has never before been applied to this field. c 2018 Optical\nSociety of America\nOCIS codes: 010.1080, 010.7350, 100.2000\n1.\n\nIntroduction\n\nThe sensing of a corrupted optical wavefront is a key part of any astronomical adaptive optics\n(AO) system on an optical or infra-red telescope, and is carried out using a wavefront sensor\n(WFS), as described by Roddier.1 When starlight passes through the Earth's atmosphere,\nrandom perturbations are introduced which distort the wavefronts from the astronomical\nsource in a time varying fashion.2 It is then no longer possible to form a diffraction limited\nimage from these distorted wavefronts, and the effective resolution of a telescope is reduced.\nBy sensing the form of the wavefront using a WFS, and then rapidly applying corrective\nmeasures to one or more deformable mirrors, it is possible to compensate for some of the\nperturbations, and hence improve the image quality and resolution of the telescope. The\nWFS and deformable mirror together form part of an AO system. AO is a technology widely\nused in optical and infra-red astronomy, and almost all large science telescopes have an AO\nsystem. A large number of results, which would be impossible to obtain using seeing-limited\n1\n\n\f(uncorrected) observations, have been obtained using AO systems (see for example Gendron3\nand Masciadri4 ). However, there is still much room for improvement: New AO systems are\ncontinually being built and new ideas developed, for example for wide-field high resolution\nimaging5 and extra-solar planet finding.6\nThe software simulation of an AO system is an important part of the characterisation of\nthis AO system. This characterisation can be used to determine whether a given AO system\nwill meet its design requirements, thus allowing scientific goals to be met, or to model\nnew concepts. The simulated performance of different AO techniques can be compared,7\nallowing informed decisions to be made when designing or upgrading an AO system and\nwhen optimising the system design.\nA full AO simulation will typically involve several stages,8 from generation of simulated\natmospheric phase screens, image creation for wavefront sensing and system performance\ncategorisation, as well as simulation of the effect of the deformable optical path elements and\ncontrol algorithms. The computational requirements for AO simulation scale rapidly with\ntelescope size, and simulation of AO systems for the largest telescopes cannot be carried out\nwithin acceptable timescales without the use of techniques to greatly reduce computation\ntime.\n1.A.\n\nHardware acceleration\n\nThe use of reconfigurable logic to provide application acceleration for scientific applications\nis a relatively new area of research, and I have used reprogrammable logic in the form of field\nprogrammable gate arrays (FPGAs) to provide hardware acceleration for AO simulations.\nAn FPGA is a user programmable logic array, which allows a hardware programmer to link\ntogether the various elements within the FPGA in such a way that enables the desired calculations to be carried out. An FPGA can perform highly parallelised calculations, carrying\nout simultaneous independent operations in different parts of the device. This high degree\nof parallelism means that a large number of operations can be performed simultaneously.\nThe clock speed of an FPGA is typically only a tenth of that of a commodity computer\nprocessor. However, due to the massively parallel architecture, it is possible to program\nan FPGA so that a given algorithm is computed at a much greater rate than would be\npossible using a conventional central processing unit (CPU). In this paper, I show how reprogrammable logic in the form of FPGAs has been used to greatly improve the performance\nof a wavefront sensing algorithm, and has been integrated with the Durham AO simulation\nplatform.8\nIn \u00a72, I describe the wavefront sensing algorithm, and provide details of the hardware implementation. In \u00a73, I give results for the performance improvements that are seen when using\nthis hardware acceleration. In \u00a74, I describe future work, and in \u00a75 I give our conclusions.\n\n2\n\n\f2.\n\nThe wavefront sensor pipeline\n\nIn a real astronomical AO system, starlight to be used for wavefront sensing will be diverted\nfrom the main science beam using, for example, a beam splitter. This diverted beam will\nthen usually be passed through optical elements designed to allow the wavefront shape to\nbe sensed, such as a Shack-Hartmann lenslet array or a pyramid optical element. The beam\nis then imaged onto a detector, usually a CCD, converted to an electronic form and passed\nto a processing engine. The processing engine will then use the detected light to determine\nthe shape of the wavefront, for example by employing a centroiding algorithm in the case of\na Shack-Hartmann system. The computed shape of the wavefront is then used to shape an\noptical element, typically a deformable mirror, using a \"reconstruction\" process. Designs for\nfuture AO systems can require this process to be repeated at a rate of 1-5 kHz with tens of\nthousands of degrees of freedom in the reconstruction process.9 I now consider only the case\nof a Shack-Hartmann wavefront sensing system, and an overview of this wavefront sensing\nprocess is shown in Fig. 1.\n2.A.\n\nThe simulated wavefront sensor pipeline\n\nWhen simulating the wavefront sensing process, it is necessary to model many physical\nprocesses as well as the computational processes (such as the centroid location algorithm) so\nthat the result will be as accurate as possible. When simulating a Shack-Hartmann WFS with\nthe Durham AO simulation platform, I start by computing the atmospheric perturbations\nthat have been introduced into the incident starlight by the time this light has reached the\ntelescope. The wavefront sensing process then computes the following steps:\n1. A small phase map (equivalent to a phase tilt) is added to the atmospheric phase in each\nShack-Hartmann sub-aperture, so that in the un-aberrated case (with no atmospheric\nperturbations) the maximum intensity will be placed at the centre of the central four\npixels in the sub-aperture once the high light level Shack-Hartmann images are created.\n2. A telescope pupil map is used to determine which parts of the telescope aperture are\nin the optical path.\n3. The real and imaginary parts of the atmospheric phase array are computed (assuming\nan amplitude of unity) by taking the sine and cosine of the phase values.\n4. The complex phase values are Fourier transformed using a two dimensional fast Fourier\ntransform (FFT).\n5. The high light level (noiseless) image for each sub-aperture is computed by taking the\nsquare modulus of the Fourier transform.\n3\n\n\f6. Any required integration time and re-sampling of the image scale are performed on the\nhigh light level images.\n7. The light within each sub-aperture is normalized so that the total light within the\nsub-aperture is equal to that expected from a given source magnitude.\n8. A sky-brightness pattern is introduced into the high light level images.\n9. Photon shot noise is then added, by replacing the high light level (noiseless) image\nintensities in each pixel with a Poisson random variable with a mean and variance\nequal to this intensity.\n10. The CCD readout noise is simulated by adding a Gaussian random variable with a\nmean and variance defined by the level of CCD readout noise to be simulated. The\nsimulated signal is now at the stage where it would be read out from the CCD camera\nand grabbed into computer memory.\n11. Noise sources are subtracted from the signal, for example by applying a threshold value.\n12. The centroid location of light within each sub-aperture is then computed.\nThe centroid locations which are computed by this process are then passed into a software\nwavefront reconstructor which will typically use a large matrix multiplication operation to\ndetermine and update the deformable mirror shape.\n2.B. Hardware implementation of the wavefront sensing pipeline\nWhen implementing an algorithm in an FPGA, it is important to consider how the algorithm\ncan be parallelised so that the FPGA is used efficiently. I have implemented the WFS pipeline\nin a way which means that all stages of the pipeline can operate simultaneously on different\nparts of a dataset. This is demonstrated in Fig. 2. In simple terms, the phase data for\none sub-aperture is loaded into the FPGA. The FPGA then begins to compute the 2D\nFourier transform of this data while a second dataset is loading into the FPGA. After the\nFourier transform has been computed, various noise sources (e.g. photon shot noise) are\nintroduced into the sub-aperture. While this is happening, the second dataset is being Fourier\ntransformed, and the third dataset is being loaded into the FPGA. This highly parallel\noperation then continues until the results for all sub-apertures have been computed. In\nreality, the parallelization is even finer as, for example, the Fourier transform will begin\nwhile data is still being loaded, and the introduction of photon shot noise will be computed\nin stages on many different pixel values at the same time.\n\n4\n\n\fIn a CPU implementation only one stage can happen at a time and so even though the\ntime to compute each stage may be less than with the FPGA implementation, the total\ncomputation time will be greater.\n2.B.1.\n\nThe algorithm choice\n\nMany simulation codes (for example propagation codes) contain one part of the simulation\nwhere the majority of the computation time is spent computing a simple algorithm. Such\nsimulations are ideal candidates for hardware acceleration because the simple algorithm can\neasily be placed in hardware, and the resulting performance increase of this part of the\nsimulation also gives a similar performance increase to the simulation as a whole, since this\nis where the majority of computation time is spent.\nHowever, in an AO simulation, a large amount of computational time is divided between\na number of key components since an AO simulation contains many complex algorithms, for\nexample, the reconstruction algorithms, the atmospheric phase screen generation, and various\nparts of the wavefront sensing pipeline. To give a large overall performance increase for an AO\nsimulation, each of these components would require a performance increase. Amdahl's law10\nstates that the overall system speed is governed by the slowest component. As an example, a\nsimulation may consist of five algorithms, each requiring 20 percent of processor time. If one\nof these algorithms was then implemented in hardware, which reduced the computational\ntime for this algorithm by a factor of 1000, the overall simulation computational time will\nonly be reduced by about a fifth. It is therefore necessary to reduce the computational time\nof all five algorithms to achieve an overall performance increase of greater than five times.\nThis means that most parts of the AO simulation should be implemented in hardware to\ngive a significant performance increase. I have therefore implemented the wavefront sensing\nalgorithms in hardware as a first step, since these algorithms are among the most computationally intensive. Additionally, these algorithms are among the most complicated to place in\nhardware and so success implementing them gives an idea of the difficulty of implementing\nthe majority of an AO simulation within one or more FPGA.\n2.B.2.\n\nData bandwidth considerations\n\nIn many computer architectures, data flow \u2013 passing data between computational elements\n\u2013 can cause a bottleneck when processes spend significant amounts of time waiting for data.\nOn the Cray XD1 super computer where the hardware wavefront sensing pipeline has been\nimplemented, it is possible to pass data at a theoretical bandwidth of 1.6 GBs\u22121 between\nthe FPGA and processor memory in each direction. The highly parallel nature of the FPGA\nmeans that many calculations can be computed simultaneously. It is therefore important\nto ensure that all of these calculations can be fed with necessary data. To do this, I have\nensured that data is passed to and from the FPGA only once, with no intermediate results\n5\n\n\fbeing returned to the CPU memory. This ensures that the FPGA can be fed with new data\nat all times. If, for example, one intermediate result stage was returned to the CPU memory,\noperated on by the CPU, and then passed back into the FPGA, the bandwidth available for\neach data transfer stage would be halved, resulting in an increase in the computation time.\nIt is therefore essential to program the FPGA so that data flow to and from the host\nprocessor memory is minimised.\n2.B.3.\n\nAlgorithms within the wavefront sensing pipeline\n\nOptical phase data (aberrated by the atmospheric turbulence) is currently generated by the\nCPU, and read by the FPGA at every iteration (time-step) of the simulation. Once computed\nwithin the FPGA, the floating point centroid locations are written back to the CPU main\nmemory by the FPGA. I have implemented all of the algorithms described in section 2.A\nwithin the hardware implementation of the wavefront sensing pipeline, which is treated as a\nblack box, accepting floating point optical phase data, and returning floating point centroid\nlocation values. Internally, data is stored in the most appropriate format chosen to give the\nrequired precision while not consuming FPGA resources which are not needed. For example,\nduring the high light level image computation, data is stored in a floating point format with\na 22 bit mantissa and a six bit exponent, while during the introduction of photon shot noise,\nthe data is stored as a 23 bit wide fixed point number.\nIt is important to realise that each stage within the pipeline can operate simultaneously\nwith other stages on different parts of the dataset. In total, about four months of effort\nwas required to implement this algorithm in hardware. The software version on the other\nhand could be prepared in about a week or so, giving some idea of the differences between\nsoftware and hardware complexities. The hardware algorithm was implemented in VHDL, a\nlow level hardware description language. Higher level languages are available for hardware\nprogramming (such as Handel-C). However, these higher level languages take up much more\nof the FPGA resources (typically by a factor of two or much more), and can often restrict\nthe maximum clock speed of the FPGA for which the algorithm will give correct results.\nThese factors mean that the wavefront sensing pipeline would not fit into a single FPGA if\ncreated using a higher level language. Currently, about 70 percent of the FPGA resources (a\nVirtex-II Pro V2P50) are used for this pipeline.\n2.B.4.\n\nConfiguration\n\nFPGAs are usually considered to be fixed function devices, performing only one set task\n(which may be simple or complicated). In the case described here, the fixed function is the\nWFS algorithms. However, these have been implemented to be configurable:\n1. The dimensions of the input optical phase array for each sub-aperture can be selected,\n6\n\n\fup to a maximum of 32 \u00d7 32 values.\n2. The size of a two dimensional fast Fourier transform (FFT) (used to create the high\nlight level images from the atmospheric phase) can be chosen from 8\u00d78, 16\u00d716 or 32\u00d7\n32 pixels. If the FFT dimensions are not equal to the optical phase array dimensions,\nthe optical phase is zero-padded inside the FPGA before the FFT is executed.\n3. The number of iterations over which the wavefront sensor is to be integrated before\nbeing read out is also configurable (up to a maximum of 63 iterations).\n4. Pixel re-sampling (binning) can also be configured, with the FFT output being resampled to any size from 2 \u00d7 2 pixels up to the size of the FFT (including rectangular\narrays).\n5. Random number generator seeds can be configured.\n6. The shape of the telescope pupil function which is used to determine which subapertures are able to collect light can be defined.\n7. CCD readout noise parameters can be configured (mean and root-mean-square).\n8. A sky background value can be set.\n9. A threshold level to be applied to the signal after CCD readout has been simulated\ncan be set.\n10. The number of sub-apertures is configurable up to a maximum of 1024 \u00d7 1024 (more\nsub-apertures can be used when a pupil mask is not required).\n11. The magnitude of the guide star can be chosen.\nThe configurability of this hardware implementation of the wavefront sensing and centroid\nalgorithms means that the benefits of hardware acceleration can be realized for a large range\nof simulations.\n2.C. Integration with the AO simulation platform\nThe hardware accelerated wavefront sensing algorithms have been integrated with the\nDurham AO simulation platform in such a way that the user needs to specify only whether\nor not the hardware implementation should be used where possible. There are situations in\nwhich this implementation cannot be used (for example when simulating a Pyramid WFS),\nin which case, a software algorithm is used instead. When an AO simulation is running, it\nis possible to switch on or off the FPGA acceleration facility using a graphical simulation\ncontrol interface.\n7\n\n\f3.\n\nPerformance improvements\n\nAn FPGA should be operated at a clock rate which is dependent on the logic implemented\nwithin the FPGA. The synthesis tools used to compile the FPGA code give an indication\nof what the maximum clock rate should be. Improving the implementation of the logic\nwithin the FPGA will often allow a faster clock rate to be used, for example by more\nefficient pipelining. When the clock rate is set too high, the FPGA will cease to function\nin the expected way, which can cause data corruption. The FPGA performance is directly\ndependent on this clock rate.\n3.A.\n\nFPGA clock rate performance\n\nThe relative time taken by the software and hardware algorithms determines the performance\nincrease achieved by the hardware implementation. For this purpose, a slightly modified\nsoftware algorithm was used which computed only the algorithms also computed in the\nFPGA. Fig. 3 shows the performance increase obtained as a function of FPGA clock speed,\nand it can be seen that the trend is linear. The dotted line in Fig. 3 shows that at clock speeds\nabove about 170 MHz, the hardware implementation of the algorithm becomes unreliable\ndue to overclocking of the FPGA, sometimes giving incorrect results. Further work on the\nalgorithms within the FPGA to streamline the pipeline would allow the FPGA to give\naccurate results at these clock rates, though this has not yet been carried out. In the Cray\nXD1, the FPGAs can be clocked at a maximum rate of 199 MHz, which would give an extra\n10 percent increase in performance over the 170 MHz clock rate which currently gives correct\nresults. For the rest of this paper, a clock rate of 170 MHz is assumed unless otherwise stated,\nso that data integrity is maintained.\n3.B. Data quantity\nThe relative performance improvement achieved when using the FPGAs depends partly on\nthe size of the dataset which is accessed by the FPGA. When the dataset is small, the\noverhead of reading data into the FPGA, and writing the results back to the host CPU\nmemory will be large compared to the time spent computing the results. Therefore, the\nperformance improvement will be small (indeed, there are algorithms in which the FPGA\ncan be slower than the CPU when the dataset is small11 ). The pipeline itself also has some\nlatency, as there is a finite time between the last data entering the pipeline, and the last\nresult leaving the pipeline, of order 700 clock cycles, or 4 \u03bcs (at 170 MHz). Fig. 4 shows\nthe time taken to compute the WFS algorithms when using the hardware and software\nimplementations, as a function of the number of sub-apertures operated on. As can be seen,\nwhen using the FPGA implementation, the total computation time is about 20 \u03bcs for small\nnumbers of sub-apertures (up to about 4\u00d74), regardless of the number of sub-apertures used,\n8\n\n\fcorresponding to the latency in the pipeline and the latency for memory access. When larger\nnumbers of sub-apertures are used, the computation time is proportional to the number of\nsub-apertures evaluated.\nThe time taken for the hardware WFS algorithms to complete can be estimated when the\nnumber of sub-apertures is large (greater than about 100), as\nt=\n\nn2f \u00d7 ns \u00d7 ni\n+ tl\nf\n\n(1)\n\nwhere t is the time taken in seconds, nf is the size of the dimensions of the 2D fast Fourier\ntransform (FFT) used to compute the high light level images (8, 16 or 32), ns is the total\nnumber of sub-apertures to be evaluated, ni is the number of integrations carried out, f is\nthe clock frequency of the FPGA in Hz and tl is the initial latency of the pipeline, about\n20 \u00d7 10\u22126 s. The massively parallel architecture means that the computation time is simply\nthe computation time of the lowest algorithm (in this case, creating and integrating the high\nlight level images).\nWhen the CPU implementation is used, the computation time is dependent on the number\nof sub-apertures evaluated. The memory access latency is lower when the calculation is\ncarried out in the CPU, and so the computation time scales closely with the number of subapertures to be evaluated even when this is small. The performance increase when using the\nFPGA is therefore less when smaller numbers of sub-apertures are used, as shown in Fig. 5.\nA typical current AO system will contain 100 sub-apertures, while future AO systems are\nplanned with over 100, 000 sub-apertures when multiple wavefront sensors are used.\n3.C. Integrations\nWhen the optical phase data is sampled more than once for each CCD readout simulation,\n(i.e. the sub-aperture images are integrated before photon shot noise or CCD readout noise\nis added), it is necessary to pass more data into the FPGA per final centroid value. Since the\nFPGA computation is limited by the rate at which data is passed in, this will increase the\ncomputation time proportionally to the number of integrations. However, the time taken by\nthe software implementation will only be proportional to the number of integrations up to\nthe point at which the integration is carried out. After this, the remainder of the calculation\n(noise addition, centroid estimation) will be performed only once, meaning the time taken is\nindependent of the number of integrations. Therefore, the total time taken by the software\nimplementation will be less than proportional to the number of integrations, meaning that\nthe relative performance improvement realized by the FPGA will be reduced as shown in\nFig. 5.\n\n9\n\n\f3.D. Sub-aperture size\nThe computation time of the FPGA implementation of these algorithms is given by Eq. 1. For\nthe software implementation, this is not the case, since an increase in sub-aperture dimensions\nby a factor of two (i.e. four times as many phase value array elements per sub-aperture) will\ntake less than four times as long to compute, as demonstrated in Fig. 6, due to the different\nrates at which various algorithms within the pipeline take to complete. This figure shows that\nperformance increase provided by the FPGA is reduced as the sub-aperture size increases.\nAdditionally, whereas for the FPGA implementation, pixel re-sampling does not affect the\ncomputation time (due to the fine grain parallel architecture), the CPU implementation run\ntime is effected, taking shorter times to complete when greater binning is used (due to the\ncentroid algorithm then being applied to smaller sub-apertures). The performance increase\nwill therefore be less when the CPU implementation performs relatively faster as shown in\nFig. 6. In most AO simulations, small sub-apertures are used with typically 8 \u00d78 phase array\nelements.\n4.\n\nFuture work\n\nThe hardware WFS pipeline is useful for a wide range of simulations. The effect of spot\nelongation when using laser guide stars is not yet considered in this pipeline, and so this\ncould be added, Additionally, taking scintillation effects into account may be necessary for\nsome systems.\nImplementing these algorithms within the current FPGAs in the XD1 is unlikely to be\npossible, due to the finite amount of logic within the FPGAs. However, it is possible to\nupgrade the XD1 with larger FPGAs, and this would allow these extra algorithms to be\nimplemented within a single FPGA should funds become available.\nIn a typical software AO simulation at Durham, the WFS algorithms may take about\n75 percent of CPU time. By implementing these algorithms in an FPGA, they can be accelerated by over 600 times. The remaining 25 percent of the processor tasks then have full\naccess to the conventional processors. However, this will then only accelerate the AO simulation by a factor of four, not particularly impressive given the acceleration achieved for the\nWFS algorithms. It is therefore necessary to implement other parts of the simulation within\nthe FPGAs.\nThe majority of the remaining processor tasks are found within the reconstruction algorithms which map the WFS outputs onto new figures for the deformable mirror optics. It is\ntherefore necessary to implement these algorithms in hardware to accelerate the simulation\nfurther. Given the large number of combinations of natural guide stars, laser guide stars\nand conjugate deformable mirrors, it is not possible to include the reconstruction algorithms\nas part of the WFS pipeline, but rather, in a separate hardware implementation running\n10\n\n\fwithin a different FPGA. This will allow for the flexibility and reconfigurability of the AO\nsimulation to be maintained.\nThe construction of atmospheric turbulence phase screens is also processor intensive, and\nit is planned to move this algorithm into hardware also. This again will improve the overall\nperformance of the AO simulation.\n5.\n\nConclusion\n\nI have described the implementation of a WFS simulation pipeline within reconfigurable logic\nin the form of FPGAs, and this is the first time that this has been attempted. This has led to\na reduction in computation time of over 600 times over the conventional software approach,\nallowing the simulation run times to be reduced. This work demonstrates the feasibility and\nbenefits of hardware acceleration using FPGAs, and shows that hardware acceleration can\ngreatly improve the performance of calculations, far beyond that achievable by using software\nbased approaches. This has relevance for simulation of a wide range of optical systems. By\nusing a hardware accelerated AO simulation platform, it is possible to model AO systems on\nextremely large telescopes, which would be otherwise infeasible due to the long computation\ntimes.\nAcknowledgments\nThe author would like to thank R. Wilson, C. Saunter and D. Geng for their thoughtful\ncomments.\nReferences\n1. F. Roddier, Adaptive Optics in Astronomy (Cambridge University Press, 1999).\n2. V. I. Tatarski, Wavefront Propagation in a Turbulent Medium (Dover, 1961).\n3. E. Gendron, A. Coustenis, P. Drossart, M. Combes, M. Hirtzig, F. Lacombe, D. Rouan,\nC. Collin, S. Pau, A.-M. Lagrange, D. Mouillet, P. Rabou, T. Fusco, and G. Zins,\n\"VLT/NACO adaptive optics imaging of Titan,\" A&A417, L21\u2013L24 (2004).\n4. E. Masciadri, R. Mundt, T. Henning, C. Alvarez, and D. Barrado y Navascu\u00e9s, \"A Search\nfor Hot Massive Extrasolar Planets around Nearby Young Stars with the Adaptive Optics\nSystem NACO,\" Astrophys. J. 625, 1004\u20131018 (2005).\n5. E. Marchetti, R. Brast, B. Delabre, R. Donaldson, E. Fedrigo, C. Frank, N. N. Hubin,\nJ. Kolb, M. Le Louarn, J. Lizon, S. Oberti, R. Reiss, J. Santos, S. Tordo, R. Ragazzoni,\nC. Arcidiacono, A. Baruffolo, E. Diolaiti, J. Farinato, and E. Vernet-Viard, \"MAD status\nreport,\" in Advancements in Adaptive Optics. Edited by Domenico B. Calia, Brent L.\nEllerbroek, and Roberto Ragazzoni. Proceedings of the SPIE, Volume 5490, pp. 236-247\n(2004)., pp. 236\u2013247 (2004).\n11\n\n\f6. D. Mouillet, A. M. Lagrange, J.-L. Beuzit, C. Moutou, M. Saisse, M. Ferrari, T. Fusco,\nand A. Boccaletti, \"High Contrast Imaging from the Ground: VLT/Planet Finder,\" in\nASP Conf. Ser. 321: Extrasolar Planets: Today and Tomorrow, pp. 39\u2013+ (2004).\n7. C. V\u00e9rinaud, M. Le Louarn, V. Korkiakoski, and M. Carbillet, \"Adaptive optics for\nhigh-contrast imaging: pyramid sensor versus spatially filtered Shack-Hartmann sensor,\"\nMNRAS357, L26\u2013L30 (2005).\n8. A. G. Basden, T. Butterley, R. M. Myers, and R. W. Wilson, \"The Durham ELT capable\nadaptive optics simulation platform,\" Opt. Express (2006).\n9. N. N. Hubin, \"Adaptive optics status and roadmap at ESO,\" in Advancements in Adaptive Optics. Edited by Domenico B. Calia, Brent L. Ellerbroek, and Roberto Ragazzoni.\nProceedings of the SPIE, Volume 5490, pp. 195-206 (2004)., pp. 195\u2013206 (2004).\n10. G. Amdahl, \"Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities,\" in AFIPS Conference Proceedings, Volume 30, pp. 483-485 (1967),\npp. 483\u2013485 (1967).\n11. A. G. Basden, F. Ass\u00e9mat, T. Butterley, D. Geng, C. D. Saunter, and R. W. Wilson,\n\"Acceleration of adaptive optics simulations using programmable logic,\" MNRAS364,\n1413\u20131418 (2005).\n\n12\n\n\fFig. 1. A schematic diagram of the wavefront sensing process for a typical\nShack-Hartmann wavefront sensor.\n\n13\n\n\fFig. 2. A schematic diagram showing the parallelization of the wavefront sensor pipeline within the FPGA. All stages operate simultaneously. Here, NOP\nmeans no operation is computed, i.e. the input data is not valid.\n\n14\n\n\f750\n700\n\nPerformance increase\n\n650\n600\n550\n500\n450\n400\n350\n100\n\n120\n\n140\n160\nFPGA clock speed / MHz\n\n180\n\n200\n\nFig. 3. A figure showing the performance increase when using an FPGA instead\nof a CPU for wavefront sensing pipeline algorithms, as a function of the FPGA\nclock rate. The dotted line above 170 MHz shows the speeds at which the\nFPGA algorithm becomes unreliable due to overclocking. Here, the centroid\nlocations of 1024 Shack-Hartmann sub-apertures have been computed at each\nFPGA clock frequency.\n\n15\n\n\f1\n\nComputation time / s\n\n0.1\n\n0.01\n\n0.001\n\n1e-04\n\n1e-05\n1\n\n10\n\n100\n\n1000\n10000\n100000\nNumber of sub-apertures\n\n1e+06\n\n1e+07\n\nFig. 4. A figure showing the computation time for the wavefront sensing\npipeline in hardware (solid curve) and software (dotted curve) as a function\nof the number of sub-apertures to be evaluated.\n\n16\n\n\fPerformance increase (software time / hardware time)\n\n700\n\n1 integration\n2 integrations\n4 integrations\n8 integrations\n\n600\n500\n400\n300\n200\n100\n0\n1\n\n10\n\n100\n\n1000\n10000\n100000\nNumber of sub-apertures\n\n1e+06\n\n1e+07\n\nFig. 5. A figure showing the ratio of CPU to FPGA computation time for the\nwavefront sensing pipeline as a function of the number of sub-apertures to be\nevaluated. A typical current AO system will contain 100 sub-apertures, while\nfuture AO systems are designed with over 100,000 sub-apertures. The number\nof image integrations carried out before the CCD readout is simulated is 1\n(solid curve), 2 (dotted curve), 4 (dashed curve) and 8 (dot-dashed curve).\n\n17\n\n\fPerformance increase (software time / hardware time)\n\n700\n600\n500\n400\n300\n200\n100\n0\n8-4\n\n8-8\n16-4 16-8 16-16\n32-4 32-8 32-16 32-32\nSub-aperture size / (input phase size - binned CCD pixels)\n\nFig. 6. A figure showing the ratio of CPU to FPGA computation time for the\nwavefront sensing pipeline as a function of the number of pupil phase values\nper sub-aperture (first number for each bar) and the number of simulated CCD\npixels (in each dimension) per sub-aperture (second number for each bar).\n\n18\n\n\f1. A schematic diagram of the wavefront sensing process for a typical Shack-Hartmann\nwavefront sensor.\n2. A schematic diagram showing the parallelization of the wavefront sensor pipeline within\nthe FPGA. All stages operate simultaneously. Here, NOP means no operation is computed, i.e. the input data is not valid.\n3. A figure showing the performance increase when using an FPGA instead of a CPU\nfor wavefront sensing pipeline algorithms, as a function of the FPGA clock rate. The\ndotted line above 170 MHz shows the speeds at which the FPGA algorithm becomes\nunreliable due to overclocking. Here, the centroid locations of 1024 Shack-Hartmann\nsub-apertures have been computed at each FPGA clock frequency.\n4. A figure showing the computation time for the wavefront sensing pipeline in hardware\n(solid curve) and software (dotted curve) as a function of the number of sub-apertures\nto be evaluated.\n5. A figure showing the ratio of CPU to FPGA computation time for the wavefront sensing\npipeline as a function of the number of sub-apertures to be evaluated. A typical current\nAO system will contain 100 sub-apertures, while future AO systems are designed with\nover 100,000 sub-apertures. The number of image integrations carried out before the\nCCD readout is simulated is 1 (solid curve), 2 (dotted curve), 4 (dashed curve) and 8\n(dot-dashed curve).\n6. A figure showing the ratio of CPU to FPGA computation time for the wavefront\nsensing pipeline as a function of the number of pupil phase values per sub-aperture\n(first number for each bar) and the number of simulated CCD pixels (in each dimension)\nper sub-aperture (second number for each bar).\n\n19\n\n\f"}