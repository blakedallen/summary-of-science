{"id": "http://arxiv.org/abs/1012.5390v1", "guidislink": true, "updated": "2010-12-24T12:01:25Z", "updated_parsed": [2010, 12, 24, 12, 1, 25, 4, 358, 0], "published": "2010-12-24T12:01:25Z", "published_parsed": [2010, 12, 24, 12, 1, 25, 4, 358, 0], "title": "Forward Smoothing using Sequential Monte Carlo", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.4760%2C1012.3681%2C1012.3445%2C1012.4839%2C1012.0039%2C1012.0715%2C1012.4516%2C1012.0780%2C1012.5042%2C1012.5980%2C1012.1592%2C1012.3875%2C1012.2268%2C1012.0033%2C1012.0553%2C1012.4467%2C1012.5663%2C1012.3044%2C1012.0312%2C1012.3198%2C1012.5102%2C1012.2469%2C1012.0018%2C1012.2427%2C1012.3253%2C1012.5789%2C1012.3810%2C1012.5145%2C1012.5741%2C1012.5538%2C1012.4625%2C1012.2888%2C1012.0765%2C1012.4763%2C1012.0961%2C1012.0082%2C1012.4900%2C1012.4864%2C1012.3898%2C1012.4411%2C1012.0994%2C1012.1311%2C1012.3389%2C1012.2105%2C1012.4921%2C1012.4741%2C1012.3886%2C1012.5162%2C1012.1664%2C1012.4267%2C1012.1763%2C1012.5970%2C1012.1696%2C1012.5636%2C1012.4025%2C1012.1963%2C1012.3145%2C1012.0125%2C1012.3673%2C1012.1493%2C1012.0962%2C1012.1426%2C1012.1644%2C1012.1216%2C1012.5620%2C1012.5689%2C1012.5273%2C1012.4448%2C1012.1301%2C1012.3991%2C1012.3539%2C1012.5873%2C1012.2988%2C1012.2204%2C1012.0188%2C1012.1007%2C1012.0497%2C1012.5168%2C1012.3051%2C1012.0447%2C1012.2237%2C1012.3607%2C1012.5902%2C1012.3954%2C1012.5777%2C1012.5920%2C1012.4711%2C1012.0301%2C1012.4635%2C1012.0675%2C1012.2956%2C1012.0364%2C1012.5564%2C1012.0025%2C1012.3678%2C1012.3494%2C1012.5413%2C1012.1810%2C1012.0808%2C1012.5390%2C1012.3411&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Forward Smoothing using Sequential Monte Carlo"}, "summary": "Sequential Monte Carlo (SMC) methods are a widely used set of computational\ntools for inference in non-linear non-Gaussian state-space models. We propose a\nnew SMC algorithm to compute the expectation of additive functionals\nrecursively. Essentially, it is an online or forward-only implementation of a\nforward filtering backward smoothing SMC algorithm proposed in Doucet .et .al\n(2000). Compared to the standard path space SMC estimator whose asymptotic\nvariance increases quadratically with time even under favourable mixing\nassumptions, the asymptotic variance of the proposed SMC estimator only\nincreases linearly with time. This forward smoothing procedure allows us to\nimplement on-line maximum likelihood parameter estimation algorithms which do\nnot suffer from the particle path degeneracy problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1012.4760%2C1012.3681%2C1012.3445%2C1012.4839%2C1012.0039%2C1012.0715%2C1012.4516%2C1012.0780%2C1012.5042%2C1012.5980%2C1012.1592%2C1012.3875%2C1012.2268%2C1012.0033%2C1012.0553%2C1012.4467%2C1012.5663%2C1012.3044%2C1012.0312%2C1012.3198%2C1012.5102%2C1012.2469%2C1012.0018%2C1012.2427%2C1012.3253%2C1012.5789%2C1012.3810%2C1012.5145%2C1012.5741%2C1012.5538%2C1012.4625%2C1012.2888%2C1012.0765%2C1012.4763%2C1012.0961%2C1012.0082%2C1012.4900%2C1012.4864%2C1012.3898%2C1012.4411%2C1012.0994%2C1012.1311%2C1012.3389%2C1012.2105%2C1012.4921%2C1012.4741%2C1012.3886%2C1012.5162%2C1012.1664%2C1012.4267%2C1012.1763%2C1012.5970%2C1012.1696%2C1012.5636%2C1012.4025%2C1012.1963%2C1012.3145%2C1012.0125%2C1012.3673%2C1012.1493%2C1012.0962%2C1012.1426%2C1012.1644%2C1012.1216%2C1012.5620%2C1012.5689%2C1012.5273%2C1012.4448%2C1012.1301%2C1012.3991%2C1012.3539%2C1012.5873%2C1012.2988%2C1012.2204%2C1012.0188%2C1012.1007%2C1012.0497%2C1012.5168%2C1012.3051%2C1012.0447%2C1012.2237%2C1012.3607%2C1012.5902%2C1012.3954%2C1012.5777%2C1012.5920%2C1012.4711%2C1012.0301%2C1012.4635%2C1012.0675%2C1012.2956%2C1012.0364%2C1012.5564%2C1012.0025%2C1012.3678%2C1012.3494%2C1012.5413%2C1012.1810%2C1012.0808%2C1012.5390%2C1012.3411&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Sequential Monte Carlo (SMC) methods are a widely used set of computational\ntools for inference in non-linear non-Gaussian state-space models. We propose a\nnew SMC algorithm to compute the expectation of additive functionals\nrecursively. Essentially, it is an online or forward-only implementation of a\nforward filtering backward smoothing SMC algorithm proposed in Doucet .et .al\n(2000). Compared to the standard path space SMC estimator whose asymptotic\nvariance increases quadratically with time even under favourable mixing\nassumptions, the asymptotic variance of the proposed SMC estimator only\nincreases linearly with time. This forward smoothing procedure allows us to\nimplement on-line maximum likelihood parameter estimation algorithms which do\nnot suffer from the particle path degeneracy problem."}, "authors": ["Pierre Del Moral", "Arnaud Doucet", "Sumeetpal Singh"], "author_detail": {"name": "Sumeetpal Singh"}, "author": "Sumeetpal Singh", "links": [{"href": "http://arxiv.org/abs/1012.5390v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1012.5390v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1012.5390v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1012.5390v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Forward Smoothing using Sequential Monte Carlo\n\narXiv:1012.5390v1 [stat.ME] 24 Dec 2010\n\nPierre Del Moral\u2217, Arnaud Doucet\u2020, Sumeetpal S. Singh\u2021\nTechnical Report CUED/F-INFENG/TR 638, Cambridge University.\nFirst version September 27th 2009, Second version November 24th 2009, Third\nversion December 12th 2010.\n\nAbstract\nSequential Monte Carlo (SMC) methods are a widely used set of computational tools\nfor inference in non-linear non-Gaussian state-space models. We propose a new SMC\nalgorithm to compute the expectation of additive functionals recursively. Essentially, it\nis an online or \"forward-only\" implementation of a forward filtering backward smoothing\nSMC algorithm proposed in [18]. Compared to the standard path space SMC estimator whose asymptotic variance increases quadratically with time even under favourable\nmixing assumptions, the asymptotic variance of the proposed SMC estimator only increases linearly with time. This forward smoothing procedure allows us to implement\non-line maximum likelihood parameter estimation algorithms which do not suffer from\nthe particle path degeneracy problem.\nSome key words: Expectation-Maximization, Forward Filtering Backward Smoothing, Recursive Maximum Likelihood, Sequential Monte Carlo, Smoothing, State-Space\nModels.\n\n1\n\nIntroduction\n\n1.1\n\nState-space models and inference aims\n\nState-space models (SSM) are a very popular class of non-linear and non-Gaussian time\nseries models in statistics, econometrics and information engineering; see for example [7],\n[19], [20]. An SSM is comprised of a pair of discrete-time stochastic processes, {Xn }n\u22650 and\n{Yn }n\u22650 , where the former is an X -valued unobserved process and the latter is a Y-valued\nprocess which is observed. The hidden process {Xn }n\u22650 is a Markov process with initial\ndensity \u03bc\u03b8 (x) and Markov transition density f\u03b8 ( x\u2032 | x), i.e.\nX0 \u223c \u03bc\u03b8 (*) and Xn | (Xn\u22121 = xn\u22121 ) \u223c f\u03b8 ( *| xn\u22121 ) ,\n\u2217\n\nn \u2265 1.\n\n(1.1)\n\nCentre INRIA Bordeaux et Sud-Ouest & Institut de Math\u00e9matiques de Bordeaux , Universit\u00e9 de Bordeaux I, 351 cours de la Lib\u00e9ration 33405 Talence cedex, France (Pierre.Del-Moral@inria.fr)\n\u2020\nThe Institute of Statistical Mathematics, 4-6-7 Minami-Azabu, Minato-ku, Tokyo 106-8569, Japan and\nDepartment of Statistics & Department of Computer Science, University of British Columbia, 333-6356\nAgricultural Road, Vancouver, BC, V6T 1Z2, Canada (Arnaud@stat.ubc.ca)\n\u2021\nDepartment of Engineering, University of Cambridge, Trumpington Street, CB2 1PZ, United Kingdom\n(sss40@cam.ac.uk)\n\n1\n\n\fIt is assumed that the observations {Yn }n\u22650 conditioned upon {Xn }n\u22650 are statistically\nindependent and have marginal density g\u03b8 ( y| x), i.e.\n\u0011\n\u0010\n(1.2)\nYn | {Xk }k\u22650 = {xk }k\u22650 \u223c g\u03b8 ( *| xn ) .\n\nWe also assume that \u03bc\u03b8 (x), f\u03b8 ( x| x\u2032 ) and g\u03b8 ( y| x) are densities with respect to (w.r.t.)\nsuitable dominating measures denoted generically as dx and dy. For example, if X \u2286 Rp\nand Y \u2286 Rq then the dominating measures could be the Lebesgue measures. The variable\n\u03b8 in the densities of these random variables are the particular parameters of the model.\nThe set of possible values for \u03b8 is denoted \u0398. The model (1.1)-(1.2) is also referred to as a\nHidden Markov Model (HMM) in the literature.\nFor any sequence {zn }n\u2208Z and integers j \u2265 i, let zi:j denote the set {zi , zi+1 , ..., zj }.\n(When j < i this is to be understood as the empty set.) Equations (1.1) and (1.2) define\nthe joint density of (X0:n , Y0:n ),\np\u03b8 (x0:n , y0:n ) = \u03bc\u03b8 (x0 )\n\nn\nY\n\nk=1\n\nf\u03b8 ( xk | xk\u22121 )\n\nn\nY\n\nk=0\n\ng\u03b8 ( yk | xk )\n\n(1.3)\n\nwhich yields the marginal likelihood,\np\u03b8 (y0:n ) =\n\nZ\n\np\u03b8 (x0:n , y0:n ) dx0:n .\n\n(1.4)\n\nLet sk : X \u00d7 X \u2192 R, k \u2208 N, be a sequence of functions and Sn : X n \u2192 R, n \u2208 N, be the\ncorresponding sequence of additive functionals constructed from sk as follows1\nSn (x0:n ) =\n\nn\nX\n\nsk (xk\u22121 , xk ) .\n\n(1.5)\n\nk=1\n\nThere are many instances where it is necessary to be able to compute the following expectations recursively in time,\nSn\u03b8 = E\u03b8 [ Sn (X0:n )| y0:n ] .\n(1.6)\nThe conditioning implies the expectation should be computed w.r.t. the density of X0:n\ngiven Y0:n = y0:n , i.e. p\u03b8 ( x0:n | y0:n ) \u221d p\u03b8 (x0:n , y0:n ) and for this reason Sn\u03b8 is referred to as\na smoothed additive functional.\nAs the first example of the need to perform such computations, consider the problem of\ncomputing the score vector, \u2207 log p\u03b8 (y0:n ). The score is a vector in Rd and its ith component\nis\n\u2202 log p\u03b8 (y0:n )\n.\n(1.7)\n[\u2207 log p\u03b8 (y0:n )]i =\n\u2202\u03b8 i\nUsing Fisher's identity, the problem of computing the score becomes an instance of (1.6),\ni.e.\n\u2207 log p\u03b8 (y0:n ) =\n\nn\nX\nk=1\n\nE\u03b8 [ \u2207 log f\u03b8 ( Xk | Xk\u22121 )| y0:n ] +\n\n+ E\u03b8 [ \u2207 log \u03bc\u03b8 (X0 )| y0:n ] .\n\nn\nX\nk=0\n\nE\u03b8 [ \u2207 log g\u03b8 ( yk | Xk )| y0:n ]\n(1.8)\n\n1\nIncorporating dependency of sk on yk , i.e. Sn is the sums of term of the form sk (xk\u22121 , xk , yk ), is merely\na matter of redefining sk in the computations to follow.\n\n2\n\n\fAn alternative representation of the score as a smoothed additive functional based on infinitesimal perturbation analysis is given in [12]. The score has applications to Maximum\nLikelihood (ML) parameter estimation [32], [37].\nThe second example is ML parameter estimation using the Expectation-Maximization\n(EM) algorithm. Let y0:n be a batch of data and the aim is to maximise p\u03b8 (y0:n ) w.r.t. \u03b8.\nGiven a current estimate \u03b8 \u2032 , a new estimate \u03b8 \u2032\u2032 is obtained by maximizing the function\nn\nn\nX\n\u0001 X\nQ \u03b8\u2032, \u03b8 =\nE\u03b8\u2032 [ log f\u03b8 ( Xk | Xk\u22121 )| y0:n ] +\nE\u03b8\u2032 [ log g\u03b8 ( yk | Xk )| y0:n ]\nk=1\n\nk=0\n\n+ E\u03b8\u2032 [ log \u03bc\u03b8 (X0 )| y0:n ]\n\nw.r.t. \u03b8 and setting \u03b8 \u2032\u2032 to the maximising argument. A fundamental property of the EM\nalgorithm is p\u03b8\u2032\u2032 (y0:n ) \u2265 p\u03b8\u2032 (y0:n ). For linear Gaussian models and finite state-space HMM,\nit is possible to perform the computations in the definition of Q (\u03b8 \u2032 , \u03b8). For general nonlinear non-Gaussian state-space models of the form (1.1)-(1.2), we need to rely on numerical\napproximation schemes.\n\n1.2\n\nCurrent approaches to smoothing with SMC\n\nSMC methods are a class of algorithms that sequentially approximate the sequence of posterior distributions {p\u03b8 (dx0:n |y0:n )}n\u22650 using a set of N weighted random samples called\nparticles. Specifically, the SMC approximation of p\u03b8 (dx0:n |y0:n ), for n \u2265 0, is\n\n(i)\n\nwhere Wn\n\npb\u03b8 (dx0:n |y0:n ) :=\n\nN\nX\ni=1\n\nWn(i) \u03b4X (i)\n\n0:n\n\n(dx0:n ) ,\n\nWn(i)\n\n\u2265 0,\n\nN\nX\n\nWn(i) = 1,\n\n(1.9)\n\ni=1\n\n(i)\n\nis the importance weight associated to particle X0:n and \u03b4X (i) is the Dirac\n0:n\n\n(i)\n\nmeasure with an atom at X0:n . The particles are propagated forward in time using a\ncombination of importance sampling and resampling steps and there are several variants of\nboth these steps; see [14], [19] for details. SMC methods are parallelisable and flexible, the\nlatter in the sense that SMC approximations of the posterior densities for a variety of SSMs\ncan be constructed quite easily. SMC methods were popularized by the many successful\napplications to SSM.\n1.2.1\n\nPath space and fixed-lag approximations\n\nA SMC approximation of Sn\u03b8 may be constructed by replacing p\u03b8 (dx0:n |y0:n ) in Eq. (1.6)\nwith its SMC approximation in Eq. (1.9) - we call this the path space method since the\nn+1 , is used.\nSMC approximation of p\u03b8 (dx0:n |y0:n ), which is a probability distribution on X\nn\no\n(i)\nFortunately there is no need to store the entire ancestry of each particle, i.e. X0:n\n,\n1\u2264i\u2264N\n\nwhich would require a growing memory. Also, this estimate can be computed recursively.\nHowever, the reliance on the approximation of the joint distribution p\u03b8 ( dx0:n | y0:n ) is bad.\nIt is well-known in the SMC literature that the approximation of p\u03b8 ( dx0:n | y0:n ) becomes\nprogressively impoverished as n increases because of the successive resampling steps [3], [13],\n[34]. That is, the number of distinct samples representing p\u03b8 ( dx0:k | y0:n ) for any fixed k < n\n3\n\n\fdiminishes as n increases \u2013 this is known as the particle path degeneracy problem. Hence,\nwhatever being the number of particles, p\u03b8 ( dx0:k | y0:n ) will eventually be approximated by a\nsingle unique particle for all (sufficiently large) n. This has severe consequences for the SMC\nestimate Sn\u03b8 . In [13], under favourable mixing assumptions,\nthe authors established an upper\n\u221a\np\n2\nbound on the L error which is proportional to n / N . Under similar assumptions, it was\nshown in [37] that the asymptotic variance of this estimate increases at least quadratically\nwith n. To reduce the variance, alternative methods have been proposed. The technique\nproposed in [29] relies on the fact that for a SSM with \"good\" forgetting properties,\n\u0001\n(1.10)\np\u03b8 ( x0:k | y0:n ) \u2248 p\u03b8 x0:k | y0:min(k+\u2206,n)\nwhen the horizon \u2206 is large enough; that is observations collected after times k +\u2206 bring little additional information about X0:k . (See [16, Corollary 2.9] for exponential error bounds.)\nThis suggests that a very simple scheme to curb particle degeneracy is to stop updating the\nSMC estimate beyond time k + \u2206. This algorithm is trivial to implement but the main\npractical problem is that of determining an appropriate value for \u2206 such that the two densities in Eq. (1.10) are close enough and particle degeneracy is low. These are conflicting\n\u0001\nrequirements. A too small value for the horizon will result in p\u03b8 x0:k | y0:min(k+\u2206,n) being a\npoor approximation of p\u03b8 ( x0:k | y0:n ) but the particle degeneracy will be low. On the other\nhand, a larger horizon improves the approximation in Eq. (1.10) but particle degeneracy\nwill creep in. Automating the selection of \u2206 is difficult. Additionally, for any finite \u2206 the\nSMC estimate of Sn\u03b8 will suffer from a non vanishing bias even as N \u2192 \u221e. In [34], for an\noptimized value of \u2206 which is dependent on n and the typically unknown mixing properties\nof the model, the SMC estimates of Sn\u03b8 based on the approximation in Eq. (1.10) were\n\u221a shown\nto have an Lp error and bias upper bounded by quantities proportional to n log n/ N and\nn log n/N under regularity assumptions.\nThe computational cost of the SMC approximation of Sn\u03b8 computed using either the path\nspace method or the truncated horizon method of [29] is O (N ).\n1.2.2\n\nApproximating the smoothing equations\n\nA standard alternative to computing Sn\u03b8 is to use SMC approximations of fixed-interval\nsmoothing techniques such as the Forward Filtering Backward Smoothing (FFBS) algorithm [18], [26]. Theoretical results on the SMC approximations of the FFBS algorithm\nhave been recently established in [17]; this includes a central limit theorem and exponential deviation inequalities. In particular, under appropriate mixing assumptions, the authors\nhave obtained time-uniform deviation inequalities for the SMC-FFBS approximations of the\nmarginals {p\u03b8 (dxk |y0:n )}0\u2264k\u2264n [17, Section 5]; see [15] for alternative proofs and comple\u03b8\nmentary results. Let Sbn\u03b8 denote the SMC-FFBS\n\u0011 estimate of Sn . In this work it is established\n\u221a \u0010 \u03b8\nthat the asymptotic variance of N Sbn \u2212 Sn\u03b8 only grows linearly with time n; a fact which\nwas also alluded to in [15]. The main advantage of the SMC implementation of the FFBS\nalgorithm is that it does not have any tuning parameter other than the number of particles N . However, the improved theoretical properties\u0001 comes at a computational price; this\nalgorithm has a computational complexity of O N 2 compared to O (N ) for the methods\npreviously discussed. (It is possible to use fast computational methods to reduce the computational cost to O (N log N ) [30].) Another restriction is that the SMC implementation\nof the FFBS algorithm does not yield an online algorithm.\n4\n\n\f1.3\n\nContributions and organization of the article\n\nThe contributions of this article are as follows.\n\u2022 We propose an original online implementation of the SMC-FFBS estimate of Sn\u03b8 . A\nparticular case of this new algorithm was presented in [36], [37] to compute the score\nvector (1.7). However, because it was catered to estimating the score, the authors\nfailed to realise its full generality.\n\u2022 An upper bound for the non-asymptotic mean square error of the SMC-FFBS estimate\nassumptions.\nIt follows from this bound that the\nSbn\u03b8 of Sn\u03b8 is derived under regularity\n\u0011\n\u221a \u0010 \u03b8\n\u03b8\nb\nasymptotic variance of N Sn \u2212 Sn is bounded by a quantity proportional to n.\nThis complements results recently obtained in [15], [17].\n\n\u2022 We demonstrate how the online implementation of the SMC-FFBS estimate of Sn\u03b8 can\nbe applied to the problem of recursively estimating the parameters of a SSM from data.\nWe present original SMC implementations of Recursive Maximum Likelihood (RML)\n[32], [36], [39] and of the online EM algorithm [25], [22], [23], [33], [8], [28, Section\n3.2.]. These SMC implementations do not suffer from the particle path degeneracy\nproblem.\n\nThe remainder of this paper is organized as follows. In Section 2 the standard FFBS\nrecursion and its SMC implementation is presented. It is then shown how this recursion\nand its SMC implementation can be implemented exactly with only a forward pass. A\nnon-asymptotic variance bound is presented in Section 3. Recursive parameter estimation\nprocedures are presented in Section 4 and numerical results are given in Section 5. We\nconclude in Section 6 and the proof of the main theoretical result is given in the Appendix.\n\n2\n\nForward smoothing and SMC approximations\n\nWe first review the standard FFBS recursion and its SMC approximation [18], [26]. This\nis then followed by a derivation of a forward-only version of the FFBS recursion and its\ncorresponding SMC implementation. The algorithms presented in this section do not depend\non any specific SMC implementation to approximate {p\u03b8 (dxn |y0:n )}n\u22650 .\n\n2.1\n\nThe forward filtering backward smoothing recursion\n\nRecall the definition of Sn\u03b8 in Eq. (1.6). The standard FFBS procedure to compute Sn\u03b8\nproceeds in two steps. In the first step, which is the forward pass, the filtering densities\n{p\u03b8 ( xk | y0:k )}0\u2264k\u2264n are computed using Bayes' formula:\nR\ng\u03b8 ( yk+1 | xk+1 ) f\u03b8 ( xk+1 | xk ) p\u03b8 ( xk | y0:k ) dxk\n\u0001\n\u0001\n\u0001\n.\np\u03b8 ( xk+1 | y0:k+1 ) = R\ng\u03b8 yk+1 | x\u2032k+1 f\u03b8 x\u2032k+1 x\u2032k p\u03b8 x\u2032k y0:k dx\u2032k:k+1\n\nThe second step is the backward pass that computes the following marginal smoothed densities which are needed to evaluate each term in the sum that defines Sn\u03b8 :\np\u03b8 ( xk\u22121 , xk | y0:n ) = p\u03b8 ( xk | y0:n ) p\u03b8 ( xk\u22121 | y0:k\u22121 , xk ) ,\n5\n\n1 \u2264 k \u2264 n.\n\n(2.1)\n\n\fwhere\np\u03b8 ( xk\u22121 | y0:k\u22121 , xk ) =\n\nf\u03b8 ( xk | xk\u22121 ) p\u03b8 ( xk\u22121 | y0:k\u22121 )\n.\np\u03b8 ( xk | y0:k\u22121 )\n\n(2.2)\n\nWe compute Eq. (2.1) commencing at k = n and then, decrementing k each time, until\nk = 1. (Integrating Eq. (2.1) w.r.t. xk will yield p\u03b8 ( xk\u22121 | y0:n ) which is needed for the next\ncomputation.) To compute Sn\u03b8 , n backward steps must be executed and then n expectations\ncomputed. This must then be repeated at time n + 1 to incorporate the effect of the new\nobservation yn+1 on these calculations. Clearly this is not an online procedure for computing\n{Sn\u03b8 }n\u22651 .\nThe SMC implementation of the FFBS recursion is straightforward [18]. In the forward\npass, we compute and store the SMC approximation pb\u03b8 ( dxk | y0:k ) of p\u03b8 ( dxk | y0:k ) for k =\n0, 1, . . . , n. In the backward pass, we simply substitute this SMC approximation in the place\nof p\u03b8 ( dxk | y0:k ) in Eq. (2.1). Let\npb\u03b8 ( dxk | y0:n ) =\n\nN\nX\ni=1\n\n(i)\n\nWk|n \u03b4X (i) (dxk )\n\n(2.3)\n\nk\n\n(i)\n\nbe the SMC approximation of p\u03b8 ( dxk | y0:n ), k \u2264 n, initialised at k = n by setting Wn|n =\n(i)\n\nWn . By substituting pb\u03b8 ( dxk\u22121 | y0:k\u22121 ) for p\u03b8 ( xk\u22121 | y0:k\u22121 ) in Eq. (2.2), we obtain\n\u0011\n\u0010\nPN\n(i)\n(i)\nf\nx\n|X\nW\n\u03b8\nk\ni=1\nk\u22121 \u03b4X (i) (dxk\u22121 )\nk\u22121\nk\u22121\n\u0010\n\u0011\npb\u03b8 ( dxk\u22121 | y0:k\u22121 , xk ) =\n.\nPN\n(l)\n(l)\nW\nf\nx\n|X\nk\nl=1\nk\u22121 \u03b8\nk\u22121\n\nThis approximation is combined with pb\u03b8 ( dxk | y0:n ) (see Eq. (2.1)) to obtain\n\u0011\n\u0010\n(i)\n(j)\n(i)\nN\nN X\nWk\u22121 f\u03b8 Xk |Xk\u22121\nX\n(j)\n\u0010\n\u0011 \u03b4 (i) (j) (dxk\u22121:k ).\nWk|n P\npb\u03b8 (dxk\u22121:k |y0:n ) =\nXk\u22121 ,Xk\n(l)\n(j)\n(l)\nN\nW\nf\nX\n|X\ni=1 j=1\nl=1\nk\u22121 \u03b8\nk\nk\u22121\n\n(2.4)\n\n(2.5)\n\nMarginalising\nthis\no approximation will give the approximation to pb\u03b8 ( dxk\u22121 | y0:n ), that is\nn\n(i)\n(i)\nWk\u22121|n , Xk\u22121\n, where\n1\u2264i\u2264N\n\n(i)\n\nWk\u22121|n\n\n\u0010\n\u0011\n(i)\n(j)\n(i)\nWk\u22121 f\u03b8 Xk |Xk\u22121\n(j)\n\u0010\n\u0011.\nWk|n P\n=\n(l)\n(j)\n(l)\nN\nW\nf\nX\n|X\nj=1\nl=1\nk\u22121 \u03b8\nk\nk\u22121\nN\nX\n\nThe SMC estimate Sbn\u03b8 of Sn\u03b8 is then given by\nSbn\u03b8 =\n\nn Z\nX\nk=1\n\nsk (xk\u22121 , xk ) pb\u03b8 (dxk\u22121:k |y0:n ).\n\n(2.6)\n\n(2.7)\n\nThe backward recursion for the weights, given in Eq. (2.6), makes this an off-line algorithm\nfor computing Sbn\u03b8 .\n6\n\n\f2.2\n\nA forward only version of the forward filtering backward smoothing\nrecursion\n\nTo circumvent the need for the backward pass in the computation of Sn\u03b8 , the following\nauxiliary function (on X ) is introduced,\nZ\n\u03b8\nTn (xn ) := Sn (x0:n ) p\u03b8 ( x0:n\u22121 | y0:n\u22121 , xn ) dx0:n\u22121 .\n(2.8)\nIt is apparent that\nSn\u03b8 =\n\nZ\n\nTn\u03b8 (xn ) p\u03b8 ( xn | y0:n ) dxn .\n\n(2.9)\n\nThe following proposition establishes a forward recursion to compute {Tn\u03b8 }n\u22650 , which is\nhenceforth referred to as the forward smoothing recursion. For sake of completeness, the\nproof of this proposition is given.\nProposition 2.1 For n \u2265 1, we have\nZ h\ni\n\u03b8\n(xn\u22121 ) + sn (xn\u22121 , xn ) p\u03b8 ( xn\u22121 | y0:n\u22121 , xn ) dxn\u22121 ,\nTn\u22121\nTn\u03b8 (xn ) =\n\n(2.10)\n\nwhere T0\u03b8 (x0 ) := 0.\nProof. The proof is straightforward\nZ\n\u03b8\nTn (xn ) := [Sn\u22121 (x0:n\u22121 ) + sn (xn\u22121 , xn )] p\u03b8 ( x0:n\u22121 | y0:n\u22121 , xn ) dx0:n\u22121\n\u0015\nZ \u0014Z\n=\nSn\u22121 (x0:n\u22121 ) p\u03b8 ( x0:n\u22122 | y0:n\u22122 , xn\u22121 ) dx0:n\u22122 p\u03b8 ( xn\u22121 | y0:n\u22121 , xn ) dxn\u22121\nZ\n+ sn (xn\u22121 , xn ) p\u03b8 ( xn\u22121 | y0:n\u22121 , xn ) dxn\u22121 .\nThe integrand in the first equality is Sn (x0:n ) while the integrand in the first integral of the\n\u03b8\n(xn\u22121 ). \u0004\nsecond equality is Tn\u22121\nThis recursion is not new and is actually a special instance of dynamic programming for\nMarkov processes; see for example [5]. For a fully observed Markov process with transition\ndensity {f\u03b8 ( xk | xk\u22121 )}k\u22651 , the dynamic programming recursion to compute the expectation of Sn (x0:n ) with respect to the law of the Markov process is usually implemented\nusing a backward recursion going from time n to time 0. In the partially observed scenario considered here, {Xk }0\u2264k\u2264n conditional on y0:n is a \"backward\" Markov process with\nnon-homogeneous transition densities {p\u03b8 ( xk\u22121 | y0:k\u22121 , xk )}1\u2264k\u2264n . Thus (2.10) is the corresponding dynamic programming recursion to compute Sn\u03b8 with respect to p\u03b8 ( x0:n | y0:n ) for\nthis backward Markov chain. This recursion is the foundation of the online EM algorithm\nand is described at length in [21] (pioneered in [40]) where the density p\u03b8 ( xn\u22121 | y0:n\u22121 , xn )\nappearing in Tn\u03b8 (xn ) is usually written as\np\u03b8 ( xn\u22121 | y0:n\u22121 , xn ) = R\n\nf\u03b8 (xn |xn\u22121 ) p\u03b8 (xn\u22121 , y0:n\u22121 )\nf\u03b8 (xn |xn\u22121 ) p\u03b8 (xn\u22121 , y0:n\u22121 ) dxn\u22121\n7\n\n\for as in Eq. (2.2) in [8], [9], [15]. The forward smoothing recursion has been rediscovered\nindependently several times; see [27], [33] for example.\nA simple SMC scheme to approximate Sn\u03b8 can be devised by exploiting equations (2.9)\nand (2.10). This is summarised as Algorithm SMC-FS below.\nAlgorithm SMC-FS: Forward-only SMC computation of the FFBS estimate\nn\no\n(i)\n(i)\n\u2022 Assume at time n\u22121 that SMC approximations Wn\u22121 , Xn\u22121\nof p\u03b8 ( dxn\u22121 | y0:n\u22121 )\n1\u2264i\u2264N\nn\n\u0010\n\u0011o\n\u0010\nn\n\u0011o\n(i)\n(i)\n\u03b8\n\u03b8\nand Tbn\u22121\nXn\u22121\nXn\u22121\nof Tn\u22121\nare available.\n1\u2264i\u2264N\n1\u2264i\u2264N\no\nn\n(i)\n(i)\nof p\u03b8 ( dxn | y0:n ) and set\n\u2022 At time n, compute the SMC approximation Wn , Xn\n1\u2264i\u2264N\n\n\u0011\n\u0010\nTbn\u03b8 Xn(i) =\nSbn\u03b8 =\n\nPN\n\n(j)\nj=1 Wn\u22121 f\u03b8\n\nN\nX\ni=1\n\n\u0011i\n\u0010\n\u0011h\n\u0010\n\u0010\n\u0011\n(i)\n(j)\n(j)\n(j)\n(i)\n\u03b8\nXn |Xn\u22121 Tbn\u22121\nXn\u22121 + sn Xn\u22121 , Xn\n\u0010\n\u0011\n,\nPN\n(j)\n(i)\n(j)\nW\nf\nX\n|X\nn\nn\u22121 \u03b8\nn\u22121\nj=1\n\n\u0011\n\u0010\nWn(i) Tbn\u03b8 Xn(i) .\n\n1 \u2264 i \u2264 N,\n(2.11)\n(2.12)\n\n\u0010\n\u0011\n(i)\nThis algorithm is initialized by setting Tb0\u03b8 X0\n= 0 for 1 \u2264 i \u2264 N. It has a compu\u0001\ntational complexity of O N 2 which can be reduced by using fast computational methods\n[30].\nThe rationale for this algorithm is as follows. By using pb\u03b8 ( dxn\u22121 | y0:n\u22121 , xn ) defined in\nEq. (2.4) in place of p\u03b8 ( dxn\u22121 | y0:n\u22121 , xn ) in Eq. (2.10),nwe obtain\nan approximation Tbn\u03b8 (xn )\no\n(i)\nof Tn\u03b8 (xn ) which is computed at the particle locations Xn\n. The approximation of\n1\u2264i\u2264N\n\nSn\u03b8 in Eq. (2.12) now follows from Eq. (2.9) by using pb\u03b8 ( dxn | y0:n ) in place of p\u03b8 ( dxn | y0:n ).\nIt is valid to use the same notation for the estimates in Eq. (2.7) and in Eq. (2.12)\nas they are indeed the same. The verification of this assertion may be accomplished by\nunfolding the recursion in Eq. (2.11).\n\n3\n\nTheoretical results\n\nIn this section, we present a bound on the non-asymptotic mean square error of the estimate\nSbn\u03b8 of Sn\u03b8 . For sake of simplicity, the result is established for additive functionals of the type\nSn (x0:n ) =\n\nn\nX\n\nsk (xk )\n\n(3.1)\n\nk=0\n\nwhere sk : X \u2192 R, and when Algorithm SMC-FS is implemented using the bootstrap\nparticle filter; see [7], [19] for a definition of this \"vanilla\" particle filter. The result can be\ngeneralised to accommodate an auxiliary implementation of the particle filter [6], [17], [35].\nLikewise, the conclusion is also valid for additive functionals of the type in (1.5); the proof\nuses similar arguments but is more complicated.\n8\n\n\fThe following regularity condition will be assumed.\n(A) There exist constants 0 < \u03c1, \u03b4 < \u221e such that for all x, x\u2032 \u2208 X , y \u2208 Y and \u03b8 \u2208 \u0398,\n\u0001\n\u03c1\u22121 \u2264 f\u03b8 x\u2032 |x \u2264 \u03c1, \u03b4\u22121 \u2264 g\u03b8 (y|x) \u2264 \u03b4.\n\nAdmittedly, this assumption is restrictive and typically holds when X and Y are finite or\nare compact spaces. In general, quantifying the errors of SMC approximations under weaker\nassumptions is possible [17]. (More precise but complicated error bounds for the particle\nestimate of Sn\u03b8 are also presented in [15] under weaker assumptions.) However, when (A)\nholds, the bounds can be greatly simplified to the extent that they can usually be expressed\nas linear or quadratic functions of the time horizon n. These simple rates of growth are\nmeaningful as they have also been observed in numerical studies even in scenarios where\nAssumption A is not satisfied [37].\nFor a function s : X \u2192 R, let ksk = supx\u2208X |s(x)|. The oscillation of s, denoted osc(s), is\ndefined to be sup {|s(x) \u2212 s(y)| ; x, y \u2208 X }. The main result in this section is the following\nnon-asymptotic bound for the mean square error of the estimate Sbn\u03b8 of Sn\u03b8 given in Eq.\n(2.12).\nTheorem 3.1 Assume (A). Consider the additive functional Sn in (3.1) with ksk k < \u221e\nand osc(sk ) \u2264 1 for 0 \u2264 k \u2264 n. Then, for any n \u2265 0 and \u03b8 \u2208 \u0398,\nE\n\n\u0012\n\nSbn\u03b8 \u2212 Sn\u03b8\n\n2\n\n\u0013\n\n(n + 1)\n\u2264a\nN\n\n1+\n\nr\n\nn+1\nN\n\n!2\n\n(3.2)\n\nwhere a is a finite constant that is independent of time n, \u03b8 and the particular choice of\nfunctions {sk }0\u2264k\u2264n .\nThe\n\u0010 proof\u0011 is given in the Appendix. It follows that the asymptotic variance of\nN Sbn\u03b8 \u2212 Sn\u03b8 , i.e. as the number of particles N goes to infinity, is upper bounded by\na quantity proportional to (n + 1) as the bias of the estimate is O(1/N ) [15, Corollary 5.3].\nb \u03b8n denote the SMC estimate of Sn\u03b8 obtained with the standard path space method.\nLet R\nThis estimate can have a much larger asymptotic variance as is illustrated with the following\nvery simple model. Let f\u03b8 (x\u2032 |x) = \u03bc\u03b8 (x\u2032 ), i.e. {Xn }n\u22650 is an i.i.d. sequence, and let yk = y\nand sk = s for all k \u2265 1 where s is some real valued function on X , and\ns = 0. \u0011It can be\n\u221a \u0010 \u03b80\nb n \u2212 Sn\u03b8 given in\neasily established that the formula for the asymptotic variance of N R\n[11], [14, eqn. (9.13), page 304 ] simplifies to\n\u221a\n\nn\nwhere\n\nZ\n\nn (n \u2212 1)\n[\u03c0\u03b8 ( x| y) se\u03b8 (x)]2\ndx +\n\u03bc\u03b8 (x)\n2\n\nZ\n\n\u03c0\u03b8 ( x| y)2\ndx\n\u03bc\u03b8 (x)\n\nZ\n\nse\u03b8 (x)2 \u03c0\u03b8 ( x| y) dx\n\n\u03bc\u03b8 (x) g\u03b8 ( y| x)\n,\n\u03bc\u03b8 (x\u2032 ) g\u03b8 ( y| x\u2032 ) dx\u2032\nZ\nse\u03b8 (x) = s (x) \u2212 s (x) \u03c0\u03b8 ( x| y) dx.\n\n\u03c0\u03b8 ( x| y) = R\n\n9\n\n(3.3)\n\n\fThus the asymptotic variance\nincreases quadratically\nwith time n. Note though that the\n\u0011\n\u221a \u0010 \u22121 \u03b8\n\u22121\n\u03b8\nb n \u2212 n Sn converges as n tends to infinity to a positive\nasymptotic variance of N n R\n\u0002\n\u0003\nconstant. Thus path space method can provide stable estimates of E\u03b8 n\u22121 Sn (X0:n ) y0:n ,\ni.e. when the additive functionals are time-averaged. Let\nS\u03b3,n (x0:n ) = \u03b3n s(xn ) +\n\nn\u22121\nX\n\ns (xk ) \u03b3k\n\nk=1\n\nn\nY\n\n(1 \u2212 \u03b3i )\n\ni=k+1\n\nwhere {\u03b3n }n\u22651 is a positive non-increasing sequence that satisfies the following constraints:\nP\nP 2\n\u22121 then S\n\u22121\n\u03b3,n (x0:n ) = n Sn (x0:n ). One\nn \u03b3n = \u221e and\nn \u03b3n < \u221e. When \u03b3n = n\nimportant choice for recursive parameter estimation (see Section 4) is\n\u03b3n = n\u2212\u03b1 ,\n\n0.5 < \u03b1 \u2264 1.\n\n(3.4)\n\nIt is also of interest to quantify the stability of the path space method when applied to\nestimate Sn\u03b8 = E\u03b8 [ S\u03b3,n (X0:n )| y0:n ] in this more general time-averaging setting. Once again\nb \u03b8 denote the SMC estimate of E\u03b8 [ S\u03b3,n (X0:n )| y0:n ] obtained with the standard path\nlet R\nn\n\u0011\n\u221a \u0010 \u03b8\nb n \u2212 Sn\u03b8 given in\nspace method. Using the formula for the asymptotic variance of N R\n[11], [14, eqn. (9.13), page 304 ] it can be verified that this asymptotic variance is\nZ\n\n+\n\nn\nn\n[\u03c0\u03b8 ( x| y) se\u03b8 (x)]2 X 2 Y\ndx\n\u03b3k\n(1 \u2212 \u03b3i )2\n\u03bc\u03b8 (x)\nk=1\n\nZ\n\n\u03c0\u03b8 ( x| y)2\ndx\n\u03bc\u03b8 (x)\n\nZ\n\ni=k+1\n\n2\n\nse\u03b8 (x) \u03c0\u03b8 ( x| y) dx\n\nk\u22121\nn X\nX\nk=2 i=1\n\n\u03b3i2 (1 \u2212 \u03b3i+1 )2 * * * (1 \u2212 \u03b3n )2\n\nIt follows from Lemma A.4 in Appendix that any accumulation point\nof this sequence (in\n\u221a\nb\nn) has to be positive. In contrast, the asymptotic variance of N (Sn\u03b8 \u2212 Sn\u03b8 ), i.e. when\nE\u03b8 [ S\u03b3,n (X0:n )| y0:n ] is computed using Algorithm SMC-FS, will converge to zero as n tends\nto infinity.\n\n4\n\nApplication to SMC parameter estimation\n\nAn important application of the forward smoothing recursion is to parameter estimation for\nnon-linear non-Gaussian SSMs. We will assume that observations are generated from an\nunknown 'true' model with parameter value \u03b8 \u2217 \u2208 \u0398, i.e. Xn |(Xn\u22121 = xn\u22121 ) \u223c f\u03b8\u2217 (*|xn\u22121 )\nand Yn |(Xn = xn ) \u223c g\u03b8\u2217 (*|xn ). The static parameter estimation problem has generated a\nlot of interest over the past decade and many SMC techniques have been proposed to solve\nit; see [28] for a recent review.\n\n4.1\n\nBrief literature review\n\nIn a Bayesian approach to the problem, a prior distribution is assigned to \u03b8 and the sequence\nof posterior densities {p (\u03b8, x0:n |y0:n )}n\u22650 is estimated recursively using SMC algorithms\ncombined with Markov chain Monte Carlo (MCMC) steps [1], [24], [38]. Unfortunately\nthese methods suffer from the particle path degeneracy problem and will result in unreliable\n10\n\n\festimates of the model parameters; see [3], [34] for a discussion of this issue. Given a fixed\nobservation record y0:n , an alternative offline MCMC approach to estimate p (\u03b8, x0:n |y0:n )\nhas been recently proposed which relies on proposals built using the SMC approximation of\np\u03b8 ( x0:n | y0:n ) [2].\nIn a ML approach, the estimate of \u03b8 \u2217 is the maximising argument of the likelihood of\nthe observed data. The ML estimate can be calculated using a gradient ascent algorithm\neither offline for a fixed batch of data or online [32]; see Section 4.2. Likewise, the EM\nalgorithm can also be implemented offline or online. The online EM algorithm, assuming\nall calculations can be performed exactly, is presented in [25], [22], [23], [33] and [9]. For\na general SSM for which the quantities required by the online EM cannot be calculated\nexactly, an SMC implementation is possible [8], [28, Section 3.2.]; see Section 4.3.\n\n4.2\n\nGradient ascent algorithms\n\nTo maximise the likelihood p\u03b8 (y0:n ) w.r.t. \u03b8, we can use a simple gradient algorithm. Let\n{\u03b8i }i\u2208N be the sequence of parameter estimates of the gradient algorithm. We update the\nparameter at iteration i + 1 using\n\u03b8i+1 = \u03b8i + \u03b3i+1 \u2207 log p\u03b8 (y0:n )|\u03b8=\u03b8i\nwhere \u2207 log p\u03b8 (y0:n )|\u03b8=\u03b8i is the score vector computed at \u03b8 = \u03b8i and {\u03b3i }i\u22651 is a sequence of\npositive non-increasing step-sizes defined in (3.4). For a general SSM, we need to approximate \u2207 log p\u03b8 (y0:n )|\u03b8=\u03b8i . As mentioned in the introduction, the score vector admits several\nsmoothed additive functional representations; see Eq. (1.7) and [12]. Using Eq. (1.7), it is\npossible to approximate the score with Algorithm SMC-FS.\nIn the online implementation, the parameter estimate at time n + 1 is updated according\nto [4], [32]\n\u03b8n+1 = \u03b8n + \u03b3n+1 \u2207 log p\u03b80:n (yn |y0:n\u22121 )\n(4.1)\n\nUpon receiving yn , \u03b8n is updated in the direction of ascent of the predictive density of this\nnew observation. A necessary requirement for an online implementation is that the previous\nvalues of the model parameter estimates (other than \u03b8n ) are also used in the evaluation of\n\u2207\u03b8 log p\u03b8 (yn |y0:n\u22121 ) at \u03b8 = \u03b8n . This is indicated in the notation \u2207 log p\u03b80:n (yn |y0:n\u22121 ). (Not\ndoing so would require browsing through the entire history of observations.) This approach\nwas suggested by [32] for the finite state-space case and is named RML. The asymptotic\nproperties of this algorithm (i.e. the behavior of \u03b8n in the limit as n goes to infinity) have\nbeen studied in the case of an i.i.d. hidden process by [39] and for an HMM with a finite\nstate-space by [32]. Under suitable regularity assumptions, convergence to \u03b8 \u2217 and a central\nlimit theorem for the estimation error has been established.\nFor a general SSM, we can compute a SMC estimate of \u2207 log p\u03b80:n (yn |y0:n\u22121 ) using\nAlgorithm SMC-FS upon noting that \u2207 log p\u03b80:n (yn |y0:n\u22121 ) is equal to\n\u2207 log p\u03b80:n (y0:n ) \u2212 \u2207 log p\u03b80:n\u22121 (y0:n\u22121 ).\nn\no\n(i)\n(i)\nIn particular, at time n, a particle approximation Wn , Xn\n\n1\u2264i\u2264N\n\nof p\u03b80:n ( dxn | y0:n ) is\n\ncomputed using the particle approximation at time n \u2212 1 and parameter value \u03b8 = \u03b8n .\nSimilarly, the computation of Eq. (2.11) is performed using \u03b8 = \u03b8n and with\nsn (xn\u22121 , xn ) = \u2207 log f\u03b8 ( xn | xn\u22121 )|\u03b8=\u03b8n + \u2207 log g\u03b8 ( yn | xn )|\u03b8=\u03b8n .\n11\n\n\fThe estimate of \u2207 log p\u03b80:n (yn |y0:n\u22121 ) is now the difference of the estimate in Eq. (2.12)\nwith the same estimate computed at time n \u2212 1.\nUnder the regularity assumptions given in Section 3, it follows from the results in\nthe Appendix that the asymptotic variance (i.e. as N \u2192 \u221e) of the SMC estimate of\n\u2207 log p\u03b80:n (yn |y0:n\u22121 ) computed using Algorithm SMC-FS is uniformly (in time) bounded.\nOn the contrary, the standard path-based SMC estimate of \u2207 log p\u03b80:n (yn |y0:n\u22121 ) has an\nasymptotic variance that increases linearly with n.\n\n4.3\n\nEM algorithms\n\nGradient ascent algorithms are more generally applicable than the EM algorithm. However,\ntheir main drawback in practice is that it is difficult to properly scale the components of\nthe computed gradient vector. For this reason the EM algorithm is usually favoured by\npractitioners whenever it is applicable.\nLet {\u03b8i }i\u2208N be the sequence of parameter estimates of the EM algorithm. In the offline\napproach, at iteration i + 1, the function\nZ\nQ(\u03b8i , \u03b8) = log p\u03b8 (x0:n , y0:n ) p\u03b8i (x0:n |y0:n )dx0:n\nis computed and then maximized. The maximizing argument is the new estimate \u03b8i+1 .\nIf p\u03b8 (x0:n , y0:n ) belongs to the exponential family, then the maximization step is usually\nstraightforward. We now give an example of this.\nLet sl : X \u00d7 X \u00d7 Y \u2192 R, l = 1, . . . , m, be a collection of functions with corresponding\nadditive functionals\nSl,n (x0:n , y0:n ) =\n\nn\nX\n\nsl (xk\u22121 , xk , yk ) ,\n\nk=1\n\nand let\n\u03b8\nSl,n\n\n=\n\nZ\n\n1 \u2264 l \u2264 m,\n\nSl,n (x0:n , y0:n )p\u03b8 (x0:n |y0:n )dx0:n .\n\n\u03b8 }\nThe collection {Sl,n\n1\u2264l\u2264m is also referred to as the summary statistics in the literature.\nTypically, the maximising argument of Q(\u03b8i , \u03b8) can be characterised explicitly through a\nsuitable function \u039b : Rm \u2192 \u0398, i.e.\n\u0011\n\u0010\n(4.2)\n\u03b8i+1 = \u039b n\u22121 Sn\u03b8i\n\n\u03b8 . As an example of this, consider the following stochastic volatility model\nwhere [Sn\u03b8 ]l = Sl,n\n[35].\n\nExample 4.1 The stochastic volatility model is a SSM defined by the following equations:\n\u0013\n\u0012\n\u03c32\n, Xn+1 = \u03c6Xn + \u03c3Vn+1 ,\nX0 \u223c N 0,\n1 \u2212 \u03c62\nYn = \u03b2 exp (Xn /2) Wn ,\nwhere {Vn }n\u2208N and {Wn }n\u22650 are independent and identically distributed standard normal\nnoise sequences, which are also independent of each other and of the initial state X0 . The\n12\n\n\f\u0001\nmodel parameters \u03b8 , \u03c6, \u03c3 2 , \u03b2 2 \u2208 R \u00d7 (0, \u221e) \u00d7 (0, \u221e) are to be estimated. To apply the\nEM algorithm to this model, let\ns1 (xn\u22121 , xn , yn ) = xn\u22121 xn , s2 (xn\u22121 , xn , yn ) = (xn\u22121 )2 ,\ns3 (xn\u22121 , xn , yn ) = (xn )2 , s4 (xn\u22121 , xn , yn ) = yn2 exp (\u2212xn ) .\nFor large n, we can safely ignore the terms associated to the initial density \u03bc\u03b8 (x) and the\nsolution to the maximisation step is characterised by the function\n!\n\u0012 \u0013\n\u0012 \u00132\nz1\nz1\nz1\nz2 \u2212 2\n, z3 +\nz1 , z4 .\n\u039b(z1 , z2 , z3 , z4 ) =\nz2\nz2\nz2\nThe SMC implementation of the forward smoothing recursion has advantages even for\nthe batch EM algorithm. As there is no backward pass, there is no need to store the particle\napproximations of {p\u03b8 (dxk |y0:k )}k=0,...,n, which can result in a significant memory saving for\nlarge data sets.\nIn the online implementation, running averages of the sufficient statistics are computed\ninstead [8], [22], [23], [25], [28, Section 3.2.], [33]. Let {\u03b8k }0\u2264k\u2264n be the sequence of parameter\nestimates of the online EM algorithm computed sequentially based on y0:n . When yn+1 is\nreceived, for each l = 1, . . . , m, compute\nR\nSl,n+1 = \u03b3n+1 sl (xn , xn+1 , yn+1 ) p\u03b80:n (xn , x!\nn+1 |y0:n+1 )dxn:n+1\nn\nR Pn\nQ\n(1 \u2212 \u03b3i ) \u03b3k sl (xk\u22121 , xk , yk ) p\u03b80:n (x0:n |y0:n+1 )dx0:n ,\n+ (1 \u2212 \u03b3n+1 )\nk=1\ni=k+1\n\n(4.3)\n\nand then set\n\u03b8n+1 = \u039b (Sn+1 )\nwhere [Sn+1 ]l = Sl,n+1 . Here {\u03b3n }n\u22651 is a step-size sequence satisfying the same conditions\nstipulated for the RML in Section 4.2. (The recursive implementation of Sl,n+1 is standard\n[4].) The subscript \u03b80:n on p\u03b80:n\u22121 (x0:n |y0:n ) indicates that the posterior density is being\ncomputed sequentially using the parameter \u03b8k\u22121 at time k (and \u03b80 at time 0.) References\n[9], [22], [25, chapter 4] and [33] have proposed an online EM algorithm, implemented as\nabove, for finite state HMMs. In the finite state setting all computations involved can be\ndone exactly in contrast to general SSMs where numerical procedures are called for. It is\nalso possible to do all the calculations exactly for linear Gaussian models [23].\nDefine the vector valued function s : X \u00d7 X \u00d7 Y \u2192 Rm as follows: s = [s1 , . . . , sm ]T .\nComputing Sn sequentially using SMC-FS is straightforward and detailed in the following\nalgorithm.\nSMC-FS implementation of online EM\nAt time n = 0\n\u2022 Choose \u03b80 .\n(i)\n\u2022 Set T0 = 0 \u2208 Rm , i = 1, . . . , N .\n(i)\n(i)\n\u2022 Construct the SMC approximation {X0 , W0 }1\u2264i\u2264N of p\u03b80 (dx0 |y0 ).\n13\n\n\fAt times n \u2265 1\n\n(i)\n\n(i)\n\n\u2022 Construct the SMC approximation {Xn , Wn }1\u2264i\u2264N of p\u03b80:n\u22121 (dxn |y0:n ).\n\u2022 For each i = 1, . . . , N , compute\n\u0011i\n\u0010\n\u0011h\n\u0010\nPN\n(i)\n(j)\n(j)\n(j)\n(i)\n(j)\nWn\u22121 f\u03b8n\u22121 Xn |Xn\u22121 (1 \u2212 \u03b3n ) Tn\u22121 + \u03b3n s Xn\u22121 , Xn , yn\nj=1\n\u0010\n\u0011\n.\nTn(i) =\nPN\n(i)\n(j)\n(j)\nX\n|X\nW\nf\nn\nn\u22121\nn\u22121 \u03b8n\u22121\nj=1\n\u0010 \u0011\nP\n(i) (i)\n\u2022 Compute Sbn = N\nW\nT\nand\nupdate\nthe\nparameter,\n\u03b8\n=\n\u039b\nSbn .\nn\nn\nn\ni=1\n\nIt was suggested in [28, Section 3.2.] that the two other SMC methods discussed in Section 1.2 could be used to approximate Sn ; the path space approach to implement the online\nEM was also independently proposed in [8]. Doing so would yield a cheaper alternative to\nAlgorithm SMC-EM above with computational cost O (N ), but not without its drawbacks.\nThe fixed-lag approximation of [29] would introduce a bias which might be difficult to control and the path space approach suffers from the usual particle path degeneracy problem.\nConsider the step-size sequence in (3.4). If the path space method is used to estimate\nSn then the theory in Section 3 tells us that, even under strong mixing assumptions, the\nasymptotic variance of the estimate of Sn will not converge to zero for 0.5 < \u03b1 \u2264 1. Thus it\nwill not yield a theoretically convergent algorithm. Numerical experiments in [8] appear to\nprovide stable results which we attribute to the fact that this variance might be \u0001very small\nin the scenarios considered2 . In contrast, the asymptotic variance of the O N 2 estimate\nconverges to zero in time n \u0001for the entire range 0.5 < \u03b1 \u2264 1 under the same mixing conditions. The original O N 2 implementation proposed here has been recently successfully\nadopted in [31] to solve a complex parameter estimation problem arising in robotics.\n\n5\n5.1\n\nSimulations\nComparing SMC-FS with the path space method\n\nWe commence with a study of a scalar linear Gaussian SSM for which we may calculate\nsmoothed functionals analytically. We use these exact values as benchmarks for the SMC\napproximations. The model is\n\u0001\nX0 \u223c N 0, \u03c302 , Xn+1 = \u03c6Xn + \u03c3V Vn+1 ,\n(5.1)\nYn = cXn + \u03c3W Wn ,\n\ni.i.d.\n\n(5.2)\n\ni.i.d.\n\nwhere Vn \u223c N (0, 1) and Wn \u223c N (0, 1). We compared the exact values of the following\nsmoothed functionals\n\" n\n#\n\" n\n#\n\" n\n#\nX\nX\nX\n\u03b8\n2\n\u03b8\n\u03b8\nS1,n\n= E\u03b8\nXk\u22121\n= E\u03b8\ny0:n , S2,n\n= E\u03b8\nXk\u22121 y0:n , S3,n\nXk\u22121 Xk y0:n ,\nk=1\n\nk=1\n\nk=1\n\n(5.3)\n\n2\nIn a Bayesian framework where \u03b8 is assigned a prior distribution and we estimate {p ( xn , \u03b8| y0:n )}n\u22650\n[1], [24], [38], the path degeneracy problem has much more severe consequences than in the ML framework\nconsidered here as illustrated in [3]. Indeed in the ML framework, the filter {p\u03b8 ( xn | y0:n )}n\u22650 will have,\nunder regularity assumptions, exponential forgetting properties for any \u03b8 \u2208 \u0398 whereas this will never be the\ncase for p ( xn , \u03b8| y0:n ) .\n\n14\n\n\f\u2217 ) = (0.8, 0.1, 1.0, 1.0) with the bootstrap filter implementacomputed at \u03b8 \u2217 = (\u03c6\u2217 , \u03c3V\u2217 , c\u2217 , \u03c3W\ntion of Algorithm SMC-FS and the path space method. Comparisons were made after 2500,\n5000, 7500 and 10,000 observations to monitor the increase in variance and the experiment\nwas replicated 50 times to generate the box-plots in Figure 1. (All replications used the\nsame data record.) Both estimates were computed using N = 500 particles.\n\n\u03b8\n\n\u03b8\n\nO(N) method: T\u22121 \u00d7 S1,T\n\nO(N2) method: T\u22121 \u00d7 S1,T\n\n0.3\n\n0.3\n\n0.28\n\n0.28\n\n0.26\n\n0.26\n2500\n\n5000\n\n7500\n\n10000\n\n2500\n\n\u03b8\n\n7500\n\n10000\n\u03b8\n\nO(N) method: T\u22121 \u00d7 S2,T\n0.02\n0\n\u22120.02\n\u22120.04\n\n5000\n\nO(N2) method: T\u22121 \u00d7 S2,T\n0.02\n0\n\u22120.02\n\u22120.04\n\n2500\n\n5000\n\n7500\n\u22121\n\nO(N) method: T\n\n\u00d7\n\n10000\n\n2500\n\n\u03b8\nS3,T\n\n5000\n2\n\n7500\n\u22121\n\nO(N ) method: T\n\n0.25\n0.24\n0.23\n0.22\n0.21\n\n10000\n\u03b8\n\n\u00d7 S3,T\n\n0.25\n0.24\n0.23\n0.22\n0.21\n2500\n\n5000\n\n7500\n\n10000\n\n2500\n\n5000\n\n7500\n\n10000\n\nFigure 1: Box plots of SMC estimates of the smoothed additive functions in (5.3) for a\nlinear Gaussian SSM. Estimates were computed with path space method (left column) and\nAlgorithm SMC-FS (right column). The long horizontal line intersecting the box indicates\nthe true value.\nFrom Figure 1 it is evident that the SMC estimates of Algorithm SMC-FS significantly\noutperforms the corresponding SMC estimates of the path space method. However one\nshould bear in mind that the former algorithm has O(N 2 ) computational complexity while\nthe latter is O(N ). Thus a comparison that takes this difference into consideration is important. From Theorem 3.1 and the discussion after it, we expect the variance of Algorithm\nSMC-FS's estimate to grow only linearly with the time index compared to a quadratic in\ntime growth of variance for the path space method. Hence, for the same computational\neffort we argue that, for large observation records, the estimate of Algorithm SMC-FS is\nalways going to outperform the path space estimates. Specifically, for a large enough n, the\nvariance of Algorithm SMC-FS's estimate with N particles will be significantly less than\n\n15\n\n\fthe variance of the path space estimate with N 2 particles. If the number of observations\nis small then, taking into account the computational complexity, it might be better to use\nthe path space estimate as the variance benefit of using Algorithm SMC-FS may not be\nappreciable to justify the increased computational load.\n\n5.2\n\nOnline EM\n\nFigure 2 shows the parameter estimates obtained using the SMC implementation of online\nEM for the stochastic volatility\u0001 model discussed in Example 4.1. The true value of the\nparameters were \u03b8 \u2217 = \u03c6, \u03c3 2 , \u03b2 2 = (0.8, 0.1, 1) and 500 particles were used. SMC-EM was\nstarted at the initial guess \u03b80 = (0.1, 1, \u00102). \u0011For the first 100 observations, only the E-step\nwas executed. That is the step \u03b8n = \u039b Sbn , which is the M-step was skipped. SMC-EM\nwas run in its entirety for observations 101 and onwards. The step size used was \u03b3n = 0.01\nfor n \u2264 105 and 1/(n \u2212 5 \u00d7 104 )0.6 for n > 105 . Figure 2 shows the sequence of parameter\nestimates computed with a very long observation sequence.\n1.5\n\n\u03b2*= 1\n\n1.02\n\n\u03c6*= 0.8\n\n0.795\n\n2\n\n0.097\n\n(\u03c3*) = 0.1\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3\n\n(\u00d7 10 )\n\nFigure 2: Estimating the parameters of the stochastic volatility model with the SMC\nversion of online EM, Algorithm SMC-EM. Initial parameter guess \u03b80 = (0.1, 1, 2). True\nand converged values (average of the last 1000 iterations) are indicated on the left and the\nright of the plot respectively.\n\n6\n\nDiscussion\n\nWe proposed a new SMC algorithm to compute the expectation of additive functionals\nrecursively in time. Essentially, it is an online implementation of the FFBS SMC algorithm\nproposed in [18]. This algorithm has an O(N 2 ) computational complexity where N is the\nnumber of particles. It was mentioned how a standard path space SMC estimator to compute\nthe same expectations recursively in time could be developed. This would have an O(N )\ncomputational complexity. However, as conjectured in [37], it was shown here that the\nasymptotic variance of the SMC-FFBS estimator increased linearly with time whereas that\n16\n\n\fof the O(N ) method increased quadratically. The online SMC-FFBS estimator was then\nused to perform recursive parameter estimation. While the convergence of RML and online\nEM have been established when they can be implemented exactly, the convergence of the\nSMC implementation of these algorithms have yet to be established and is currently under\ninvestigation.\n\n7\n\nAcknowledgments\n\nThe authors would like to thank Olivier Capp\u00e9, Thomas Flury, Sinan Yildirim and \u00c9ric\nMoulines for comments and references that helped improve the first version of this paper.\nThe authors are also grateful to Rong Chen for pointing out the link between the forward\nsmoothing recursion and dynamic programming. Finally, we are thankful to Robert Elliott\nto have pointed out to us references [22], [23] and [25].\n\nA\n\nAppendix\n\nThe proofs in this section hold for any fixed \u03b8 and therefore \u03b8 is omitted from the notation.\nThis section commences with some essential definitions.\nConsider the measurable space (E, E). Let M(E) denote the set of all finite signed\nmeasures and P(E) the set of all probability measures on E. Let B(E) denote the Banach\nspace of Rall bounded and measurable functions f equipped with the uniform norm kf k. Let\n\u03bd(f ) =\n\u03bd(dx) f (x), i.e. \u03bd(f ) is the Lebesgue integral of the function f \u2208 B(E) w.r.t.\nthe measure\n\u03bd \u2208 M(E). If \u03bd is a density w.r.t. some dominating measure dx on E then,\nR\n\u03bd(f ) = \u03bd(x) f (x)dx. We recall that a bounded integral kernel M (x, dx\u2032 ) from a measurable\nspace (E, E) into an auxiliary measurable space (E \u2032 , E \u2032 ) is an operator f 7\u2192 M (f ) from B(E \u2032 )\ninto B(E) such that the functions\nZ\nM (x, dx\u2032 )f (x\u2032 )\nx 7\u2192 M (f )(x) :=\nE\u2032\n\nare E-measurable and bounded, for any f \u2208 B(E \u2032 ). In the above displayed formulae, dx\u2032\nstands for an infinitesimal neighborhood of a point x\u2032 in E \u2032 . Let \u03b2(M ) denote the Dobrushin\ncoefficient of M which defined by the following formula\n\u03b2(M ) := sup {osc(M (f )) ; f \u2208 Osc1 (E \u2032 )}\nwhere Osc1 (E \u2032 ) stands the set of E \u2032 -measurable functions f with oscillation less than or equal\nto 1. The kernel M also generates a dual operator \u03bd 7\u2192 \u03bdM from M(E) into M(E \u2032 ) defined\nby (\u03bdM )(f ) := \u03bd(M (f )). A Markov kernel is a positive and bounded integral operator\nM with M (1) = 1. Given a pair of bounded integral operators (M1 , M2 ), we let (M1 M2 )\nthe composition operator defined by (M1 M2 )(f ) = M\u00011 (M2 (f )). For time homogenous\nstate spaces, we denote by M m = M m\u22121 M = M M m\u22121 the m-th composition of a given\nbounded integral operator M , with m \u2265 1.\nGiven a positive function G on E, let \u03a8G : \u03bd \u2208 P(E) 7\u2192 \u03a8G (\u03bd) \u2208 P(E) be the Bayes\ntransformation defined by\n\u03a8G (\u03bd)(dx) :=\n\n1\nG(x) \u03bd(dx)\n\u03bd(G)\n17\n\n\fThe definitions above also apply if \u03bd is a density and M is a transition density. In this case\nall instances of \u03bd(dx) should be replaced with \u03bd(x)dx and M (x, dx\u2032 ) by M (x, x\u2032 )dx\u2032 where\ndx and dx\u2032 are the dominating measures.\nThe proofs below will apply to any fixed sequence of observation {yn }n\u22650 and it is\nconvenient to introduce the following transition kernels,\nQn (xn\u22121 , dxn ) = g(yn\u22121 |xn\u22121 )f (xn |xn\u22121 )dxn ,\nand\n\nQk,n = Qk+1 Qk+2 . . . Qn ,\n\n0 \u2264 k \u2264 n,\n\nn \u2265 1,\n\nwith the convention that Qn,n = Id, the identity operator. Note that Qk,n (1) = p(yk:n\u22121 |xk ).\nLet the mapping \u03a6k : P(X ) \u2192 P(X ), k \u2265 1, be defined as follows\n\u03a6k (\u03bd)(dxk ) =\n\n\u03bdQk (dxk )\n.\n\u03bdQk (1)\n\nSeveral probability densities and their SMC approximations are introduced to simplify the\nexposition. The predicted filter is denoted by\n\u03b7n (dxn ) = p(dxn |y0:n\u22121 )\nwith the understanding that \u03b70 (dx0 ) is the initial distribution of X0 . Let \u03b7nN denote its SMC\napproximation with N particles. (This notation for the SMC approximation is opted for,\ninstead of the usual \u03b7bn , to make the number of particles explicit.) The bounded integral\noperator Dk,n from X into X n+1 is defined as\n\uf8eb\n\uf8f6\nZ\nn\u22121\nY\nDk,n (Sn )(xk ) := p(dx0:k\u22121 |xk , y0:k\u22121 ) \uf8ed\ng(yq |xq )f (xq+1 |xq )\uf8f8 Sn (x0:n )dxk+1:n (A.1)\nq=k\n\nDk,n is defined for any pair of time indicesQ\nk, n satisfying 0 \u2264 k \u2264 n with the convention\nN , is\nthat p(x0:k\u22121 |xk , y0:k\u22121 ) = 1 for k = 0 and \u2205 = 1. The SMC approximation, Dk,n\n\uf8f6\n\uf8eb\nZ\nn\u22121\nY\nN\ng(yq |xq )f (xq+1 |xq )\uf8f8 Sn (x0:n )dxk+1:n\nDk,n\n(Sn )(xk ) := pN (dx0:k\u22121 |xk , y0:k\u22121 ) \uf8ed\nq=k\n\n(A.2)\nwhere\n0:k\u22121 |xk , y0:k\u22121 ) is the SMC approximation of p(dx0:k\u22121 |xk , y0:k\u22121 ) obtained\nfrom the SMC-FFBS approximation of Section 2.1, i.e.\npN (dx\n\npN (dx0:k\u22121 |xk , y0:k\u22121 ) =\n\nk\nY\n\nMq,\u03b7N (xq , dxq\u22121 )\n\nwhere the backward Markov transition kernels Mq,\u03b7N\n\nq\u22121\n\nMq,\u03b7N (xq , dxq\u22121 ) =\nq\u22121\n\n(A.3)\n\nq\u22121\n\nq=1\n\nare defined through\n\nN (dx\n\u03b7q\u22121\nq\u22121 )g(yq\u22121 |xq\u22121 )f (xq |xq\u22121 )\nN (g(y\n\u03b7q\u22121\nq\u22121 |*)f (xq |*))\n\n.\n\n(A.4)\n\nIt is easily established that the SMC-FFBS approximation of p(dxk |y0:n ), k \u2264 n, is precisely\nthe marginal of\npN (dx0:n\u22121 |xn , y0:n\u22121 )b\np(dxn |y0:n )\n18\n\n\fwhere pb(dxn |y0:n ) was defined in (1.9). Finally, we define\nPk,n\n\nDk,n\n=\nDk,n (1)\n\nand\n\nN\nPk,n\n\n=\n\nN\nDk,n\n\nN (1)\nDk,n\n\n.\n\nThe following estimates are a straightforward consequence of Assumption (A). For time\nindices 0 \u2264 k \u2264 q \u2264 n,\n\u0012\n\u0013\n\u0001(q\u2212k)\nQk,q (xk , dxq )Qq,n (1)(xq )\nQk,n (1)(xk )\n2 2\n\u2264\u03c1 \u03b4 , \u03b2\n\u2264 1 \u2212 \u03c1\u22124\n, (A.5)\nbk,n = sup\n\u2032\nQk,q (Qq,n (1))\nxk ,x\u2032k Qk,n (1)(xk )\nand for 0 < k \u2264 q,\n\n\u0011\n\u0010\n\u0001q\u2212k+1\n\u2264 1 \u2212 \u03c1\u22124\nMk,\u03b7 (x, dz) \u2264 \u03c14 Mk,\u03b7 (x\u2032 , dz) =\u21d2 \u03b2 Mq,\u03b7N . . . Mk,\u03b7N\n.\nq\u22121\n\nk\u22121\n\n(A.6)\n\nSeveral auxiliary results are now presented. For any \u03c6 \u2208 B(X ), let\nN\n)(\u03c6).\nVkN (\u03c6) = \u03b7kN (\u03c6) \u2212 \u03a6k (\u03b7k\u22121\n\n(A.7)\n\nThe following is an almost sure Kintchine type inequality [14, Lemma 7.3.3].\no\u0011\n\u0010n\n(i)\nLemma A.1 Let FnN := \u03c3 Xk ; 0 \u2264 k \u2264 n, 1 \u2264 i \u2264 N, , n \u2265 0, be the natural filtration\n\nN be the trivial sigma field. For\nassociated with the N -particle approximation model and F\u22121\nany r \u2265 1, there exist a finite (non random) constant ar such that the following inequality\nN\nN\nholds for all k \u2265 0 and Fk\u22121\nmeasurable functions \u03c6N\nk \u2208 B(X ) s.t. osc(\u03c6k ) \u2264 1,\n\nE\n\n\u0010\u221a\nN VkN (\u03c6N\nk )\n\nr\n\nN\nFk\u22121\n\n\u00111\nr\n\n\u2264 ar .\n\nThis inequality may be used to derive the following Lr error estimate [14, Theorem\n7.4.4].\nLemma A.2 For any r \u2265 1, there exists a constant ar such that the following inequality\nholds for all k \u2265 0 and \u03c6 \u2208 B(X ) s.t. osc(\u03c6) \u2264 1,\n\u221a\n\nNE\n\n\u0010\n\n[\u03b7nN\n\n\u2212 \u03b7n ](\u03c6)\n\nr\n\n\u00111\nr\n\n\u2264 ar\n\nn\nX\nk=0\n\nbk,n \u03b2\n\n\u0012\n\nQk,n\nQk,n(1)\n\n\u0013\n\n.\n\n(A.8)\n\nA time-uniform bound for (A.8) may be obtained by using the estimates in (A.5)-(A.6).\nThe final auxiliary result is the following.\nLemma A.3 For time indices 1 \u2264 k \u2264 n,\nN\nN\nN\nN\n(Sn ))\nQk )(Dk,n\n(Sn ) = (\u03b7k\u22121\nDk\u22121,n\n\u03b7k\u22121\n\n19\n\n\fProof:\nN DN\n\u03b7k\u22121\nk\u22121,n (Sn )\nR N\n(dxk\u22121 )pN (dx0:k\u22122 |xk\u22121 , y0:k\u22122 )\n= \u03b7k\u22121\nR N\n= \u03b7k\u22121\n(dxk\u22121 )Qk (xk\u22121 , dxk )\n\nn\nQ\n\n!\n\nQq (xq\u22121 , dxq ) Sn (x0:n )\n\nq=k\n\n\u00d7pN (dx0:k\u22122 |xk\u22121 , y0:k\u22122 )\n\nn\nQ\n\n!\n\nQq (xq\u22121 , dxq ) Sn (x0:n )\n\nq=k+1\n\nThe result follows upon noting that\n\nN\nN\nQk (dxk ) Mk,\u03b7N (xk , dxk\u22121 ).\n(dxk\u22121 )Qk (xk\u22121 , dxk ) = \u03b7k\u22121\n\u03b7k\u22121\nk\u22121\n\nTo prove Theorem 3.1, the same semigroup techniques of [14, Section 7.4.3] are employed.\nProof:\nThe following decomposition is central\n!\nN DN\nN (S )\nX\n(Sn )\n\u03b7k\u22121\n\u03b7kN Dk,n\nn\nk\u22121,n\ncn \u2212 Sn =\n\u2212 N\nS\nN D N (1)\nN\n\u03b7\n\u03b7k\u22121 Dk\u22121,n\n(1)\nk\nk,n\n0\u2264k\u2264n\nN DN\nwith the convention that \u03b7\u22121\n\u22121,n = \u03b70 (dx0 )\n\nn\nQ\n\nq=1\n\nstates that\n\nQq (xq\u22121 , dxq ), for k = 0. Lemma A.3\n\nN\nN\nN\nN\n(Sn ))\nQk )(Dk,n\n(Sn ) = (\u03b7k\u22121\nDk\u22121,n\n\u03b7k\u22121\n\nand therefore the decomposition can be also written as\ncn \u2212 Sn =\nS\n\nN (S )\n\u03b7kN Dk,n\nn\n\nX\n\n0\u2264k\u2264n\n\nN (1)\n\u03b7kN Dk,n\n\n\u2212\n\nN )(D N (S ))\n\u03a6k (\u03b7k\u22121\nk,n n\nN )(D N (1))\n\u03a6k (\u03b7k\u22121\nk,n\n\n!\n\n(A.9)\n\nN ) = \u03b7 , for k = 0. Let\nwith the convention \u03a60 (\u03b7\u22121\n0\nN\nSek,n\n= Sn \u2212\n\nN )(D N (S ))\n\u03a6k (\u03b7k\u22121\nk,n n\nN )(D N (1))\n\u03a6k (\u03b7k\u22121\nk,n\n\n.\n\nThen every term in the r.h.s. of (A.9) takes the following form\nN (S\neN )\n\u03b7kN Dk,n\nk,n\nN (1)\n\u03b7kN Dk,n\nN\n\n\u0011\n\u0010\n\u03b7k Qk,n (1)\nN eN\nN\n(\nS\n)\n= N\n\u00d7 Vk D k,n k,n\n\u03b7k Qk,n (1)\n\nwhere the integral operators D k,n are defined as follows,\nN (S )\nDk,n\nn\n\nN\n\nDk,n (Sn ) =\n\n\u03b7k Qk,n (1)\n\n.\n\ncn \u2212 Sn is expressed as\nFinally, using (A.9) and (A.10), S\n\u0011\n\u221a \u0010\ncn \u2212 Sn = I N (Sn ) + RN (Sn )\nN S\nn\nn\n20\n\n(A.10)\n\n\fwhere the first order term is\nInN (Sn ) :=\n\n\u0010\n\u0011\nX \u221a\nN\nN\n)\nN VkN D k,n (Sek,n\n\n0\u2264k\u2264n\n\nand the second order remainder term is\n\u0010\n\u0011\nX\n\u221a\n\u0001\n1\nN eN\nN\nN\n(\nS\n)\n.\nN\n\u03b7\n\u2212\n\u03b7\nD\n(1)\n\u00d7\nV\nD\nRnN (Sn ) :=\nk\nk,n\nk,n k,n\nk\nk\n\u03b7 N D k,n (1)\n0\u2264k\u2264n k\n\nThe non-asymptotic variance bound is based on the triangle inequality\n\u001a \u0010\n\u00112 \u001b \u0012 \b\nc\nE N Sn \u2212 Sn\n\u2264 E InN (Sn )2\n\n1\n2\n\n+E\n\n\b\n\nRnN (Sn )2\n\n1\n2\n\n\u00132\n\n,\n\n(A.11)\n\nand bounds are derived below for the individual expressions on the right-hand side of this\nequation.\n\u0010\nn\n\u0011o\nN\nN )\nUsing the fact that VkN D k,n (Sek,n\nis zero mean and uncorrelated,\n0\u2264k\u2264n\n\nInN (Sn )2\n\nE\n\n\u0001\n\n=\n\nX\n\n0\u2264k\u2264n\n\n\u001a\n\u0010\n\u00112 \u001b\nN eN\nN\nN E Vk D k,n (Sk,n )\n.\n\n(A.12)\n\nThe following results are needed to bound the right-hand side of (A.12). First, observe that\nN\nN (1) = Q\nDk,n\nk,n (1), and Dk,n (1) = D k,n (1). Now using the decomposition,\nN\nN )(x )\nD k,n (Sek,n\nk\n\n= D k,n (1)(xk ) \u00d7\nit follows that\n\nR h\n\ni\n\u2032\nN\nN (S )(x ) \u2212 P N (S )(x\u2032 ) \u03a8\nPk,n\nn\nn\nk\nQk,n (1) (\u03a6k (\u03b7k\u22121 ))(dxk ),\nk\nk,n\nN\nN\nN\n(Sn ))\n) \u2264 bk,n osc(Pk,n\nD k,n (Sek,n\n\n(A.13)\n\nFor linear functionals of the form (3.1), it is easily checked that\ni\nX h\nX\nN\nDk,n\n(Sn ) = Qk,n (1)\nMk,\u03b7N . . . Mq+1,\u03b7qN (sq ) +\nQk,q (sq Qq,n (1))\nk\u22121\n\n0\u2264q\u2264k\n\nk<q\u2264n\n\nwith the convention Mk,\u03b7N . . . Mk+1,\u03b7N = Id, the identity operator, for q = k. Recalling\nk\n\nk\u22121\n\nN (1) = Q\nthat Dk,n\nk,n (1), we conclude that\nN\n(Sn ) = sk +\nPk,n\n\nX h\n\n0\u2264q<k\n\ni\nX Qk,q (Qq,n (1) sq )\nMk,\u03b7N . . . Mq+1,\u03b7qN (sq ) +\nk\u22121\nQk,q (Qq,n (1))\nk<q\u2264n\n\nand therefore\nN\n(Sn ) =\nPk,n\n\nX h\n\n0\u2264q<k\n\ni\nX Qk,q (Qq,n (1) sq )\nMk,\u03b7N . . . Mq+1,\u03b7qN (sq ) +\nk\u22121\nQk,q (Qq,n (1))\nk\u2264q\u2264n\n\n21\n\n\fThus,\nN (S ))\nosc(Pk,n\nn \u0010\n\u0011\n\u0011\n\u0010\nP\nP\nQk,q (xk ,dxq )Qq,n (1)(xq )\nosc(sq )\n\u2264 0\u2264q<k \u03b2 Mk,\u03b7N . . . Mq+1,\u03b7qN osc(sq ) + k\u2264q\u2264n \u03b2\nQk,q (Qq,n (1))\nk\u22121\n(A.14)\nUsing the estimates in (A.5) and (A.6) for the contraction coefficients, and the estimate in\n(A.5) for bk,n , it follows that there exists some finite (non random) constant c such that the\nbound\nN\nN\n) \u2264c\n(A.15)\nDk,n (Sek,n\n\nholds for any pair of time indexes k, n satisfying 0 \u2264 k \u2264 n, particle number N and choice\nof functions {sk }0\u2264k\u2264n . The desired bound for (A.14) is now obtained by combining this\nresult with Lemma A.1:\n\u0011\n\u0010\nX\n\u0001\nN\nN\n))2\nE InN (Sn )2 =\nN E VkN (D k,n (Sek,n\n0\u2264k\u2264n\n\n\u2264 d(n + 1)\n\n(A.16)\n\nwhere d is a constant\nwhose value\n\b\nConcerning the term E RnN (Sn )2 in (A.11).\n\b\nE RnN (Sn )2\n\n1\n2\n\n\u2264\n\n\uf8f1\"\n\uf8f2\n\nX\n\n1\n1\n\u221a E\nN\nN \uf8f3 \u03b7k D k,n (1)\n0\u2264k\u2264n\n\ndoes\n\nnot\n\ndepend\n\non\n\n(n, N, Sn ).\n\n\u0010\n\u0011\n\u221a\n\u221a\n\u0001\nN\nN\n)\nN \u03b7k \u2212 \u03b7kN D k,n (1) \u00d7 N VkN D k,n (Sek,n\n\n\u001ah\n\u0010\n\u0011i2 \u001b 21\n\u221a\n\u221a\n\u0001\n1\nN eN\nN\nN\n\u221a bk,n E\nN \u03b7k \u2212 \u03b7k D k,n (1) \u00d7 N Vk Dk,n (Sk,n )\n\u2264\nN\n0\u2264k\u2264n\n\u001ah\ni4 \u001b 41\nX 1\n\u221a\n\u0001\nN\n\u221a bk,n E\nN \u03b7k \u2212 \u03b7k D k,n (1)\n\u2264\nN\n0\u2264k\u2264n\n\u001ah\n\u0010\n\u0011i4 \u001b 41\n\u221a\nN eN\nN\nN Vk D k,n (Sk,n )\n\u00d7E\nX\n\n1\n\u2264 \u221a e(n + 1)\nN\n\n#2 \uf8fc 12\n\uf8fd\n\n(A.17)\n\nwhere e is a constant whose value does not depend on (n, N, Sn ). The second line follows\nfrom (A.5) and the third by the Cauchy-Schwartz inequality. The final line was arrived at\nby the same reasoning used to derive bound (A.16) and Lemma A.2. The assertion of the\ntheorem may be verified by substituting bounds (A.16) and (A.17) into (A.11).\nIt is possible to write\nn\nX\nk=1\n\n\u03b3k2\n\nn\nY\n\n2\n\n(1 \u2212 \u03b3i ) +\n\ni=k+1\n\nn X\nk\u22121\nX\nk=2 i=1\n\nas the sum in (A.18) below.\n\n22\n\n\u03b3i2 (1 \u2212 \u03b3i+1 )2 * * * (1 \u2212 \u03b3n )2\n\n\uf8fe\n\n\fLemma A.4 Let \u03b1 \u2208 (0.5, 1] and \u03b3n = n\u2212\u03b1 for n > 0. Then\nlim inf\nn\u2192\u221e\n\n\u03b3n2 +\n\nn\u22121\nX\n(n + 1 \u2212 i)\u03b3i2 (1 \u2212 \u03b3i+1 )2 * * * (1 \u2212 \u03b3n )2 > 0.\n\n(A.18)\n\ni=1\n\nProof:\nLet \u230aa\u230b denote the largest integer less than or equal to a. Since the result is obvious for\n\u03b1 = 1, let \u03b1 \u2208 (0.5, 1).\n\u03b3n2 +\n\nn\u22121\nX\n\n(n + 1 \u2212 i)\u03b3i2 (1 \u2212 \u03b3i+1 )2 * * * (1 \u2212 \u03b3n )2\n\ni=\u230an/2\u230b\n\n\u2265 \u03b3n2 + \u03b3n2\n\uf8eb\n\nn\u22121\nX\n\ni=\u230an/2\u230b\n\nn+1\u2212\u230an/2\u230b\n\n= \u03b3n2 \uf8ed\n\nwhere \u03bbn = (1 \u2212 \u03b3\u230an/2\u230b )2 and\n\nX\nj=1\n\nX\n\n(n + 1 \u2212 i)(1 \u2212 \u03b3\u230an/2\u230b )2(n\u2212i)\n\n\uf8f6\n\u03b3n2\n1\n\uf8f8+\nj\u03bbnj\u22121 \u2212\n(1 \u2212 \u03bbn )2\n(1 \u2212 \u03bbn )2\n\nj\u03bbnj\u22121 =\n\nj>0\n\nIt may be verified that\n\n1\n.\n(1 \u2212 \u03bbn )2\n\n\u03b3n2\n= 2\u22122\u03b1\u22122\nn\u2192\u221e (1 \u2212 \u03bbn )2\nlim\n\nand\n\nlim \u03b3n2\n\nn\u2192\u221e\n\nX\n\nj\u03bbnj\u22121 = 0.\n\nj>n+1\u2212\u230an/2\u230b\n\nHence the result follows.\n\nReferences\n[1] Andrieu, C., De Freitas, J.F.G. and Doucet, A. (1999). Sequential MCMC for Bayesian\nmodel selection. Proc. IEEE Workshop on Higher Order Statistics, 130\u2013134.\n[2] Andrieu, C., Doucet, A. and Holenstein, R. (2010). Particle Markov chain Monte Carlo\n(with discussion). J. Royal Statist. Soc. B, 72, 269-342.\n[3] Andrieu, C., Doucet, A. and Tadi\u0107, V. B. (2005). On-line parameter estimation in\ngeneral state-space models. Proc. 44th IEEE Conf. on Decision and Control, 332\u2013337.\n[4] Benveniste, A., M\u00e9tivier, M. and Priouret, P. (1990). Adaptive Algorithms and Stochastic Approximation. New York: Springer-Verlag.\n[5] Bertsekas, D. and Shreve, S.E. (1978) Stochastic Optimal Control: The Discrete-Time\nCase, Academic Press.\n23\n\n\f[6] Carpenter, J., Clifford, P. and Fearnhead, P. (1999). An improved particle filter for\nnon-linear problems. IEE proceedings - Radar, Sonar and Navigation, 146, 2-7.\n[7] Capp\u00e9, O., Moulines, \u00c9. and Ryd\u00e9n, T. (2005) Inference in Hidden Markov Models.\nNew York: Springer-Verlag.\n[8] Capp\u00e9, O. (2009). Online sequential Monte Carlo EM algorithm. Proc. IEEE Workshop\non Statistical Signal Processing, Cardiff, UK.\n[9] Capp\u00e9, O. (2009). Online EM algorithm for hidden Markov models. Available at\nhttp://arxiv.org/abs/0908.2359\n[10] C\u00e9rou, F., Del Moral, P. and Guyader, A. (2008). A non asymptotic variance theorem for unnormalized Feynman-Kac particle models, Technical report INRIA-00337392.\nAvailable at http://hal.inria.fr/inria-00337392 v1/\n[11] Chopin, N. (2004). Central limit theorem for sequential Monte Carlo and its application\nto Bayesian inference. Ann. Statist., 32, 2385-2411.\n[12] Coquelin, P.A., Deguest, R. and Munos, R. (2009). Sensitivity analysis in HMMs with\napplication to likelihood maximization. Proc. Conf. NIPS, Vancouver, Canada.\n[13] Del Moral, P. and Doucet, A. (2003). On a class of genealogical and interacting Metropolis models. Lecture Notes in Mathematics 1832, Berlin: Springer-Verlag, 415-46.\n[14] Del Moral, P. (2004). Feynman-Kac Formulae: Genealogical and Interacting Particle\nSystems with Applications. New York: Springer-Verlag.\n[15] Del Moral, P., Doucet, A. and Singh. S.S. (2010). A backward particle interpretation\nof Feynman-Kac formulae. ESAIM: Math. Model. Num. Analy., 44, 947-975.\n[16] Del Moral, P. and Guionnet, A. (2001). On the stability of interacting processes with\napplications to filtering and genetic algorithms. Ann. Inst. H. Poincar\u00e9 Probab. Statist.,\n37, 155\u2013194.\n[17] Douc, R., Garivier, A., Moulines, E. and Olsson, J. (2011). Sequential Monte Carlo\nsmoothing for general state space hidden Markov models. Ann. Applied Proba., to\nappear.\n[18] Doucet, A., Godsill, S. J. and Andrieu, C. (2000). On sequential Monte Carlo sampling\nmethods for Bayesian filtering. Statistics and Computing, 10, 197\u2013208.\n[19] Doucet, A., De Freitas, J.F.G. and Gordon N.J. (eds.) (2001). Sequential Monte Carlo\nMethods in Practice. New York: Springer-Verlag.\n[20] Durbin, J. and Koopman, S.J. (2001). Time Series Analysis by State-Space Methods,\nCambridge University Press.\n[21] Elliott, R.J., Aggoun, L. and Moore, J.B. (1996). Hidden Markov Models: Estimation\nand Control, Springer-Verlag.\n\n24\n\n\f[22] Elliott, R.J., Ford, J.J. and Moore, J.B. (2000) On-line consistent estimation of hidden Markov models. Technical report, Department of Systems Engineering, Australian\nNational University.\n[23] Elliott, R.J., Ford, J.J. and Moore, J.B. (2002) On-line almost-sure parameter estimation for partially observed discrete-time linear systems with known noise characteristics.\nIntern. J. Adapt. Control Sig. Proc., 16, 435-453.\n[24] Fearnhead, P. (2002). MCMC, sufficient statistics and particle filters. J. Comp. Graph.\nStatist., 11, 848\u2013862.\n[25] Ford, J.J. (1998). Adaptive hidden Markov model estimation and applications. PhD\nthesis, Department of Systems Engineering, Australian National University.\n[26] Godsill, S.J., Doucet, A. and West, M. (2004). Monte Carlo smoothing for nonlinear\ntime series. J. Amer. Stat. Assoc., 99, 156-168.\n[27] Hernando, D., Valentino, C. and Cybenko, G. (2005) Efficient computation of the\nhidden Markov model entropy for a given observation sequence. IEEE Trans. Info.\nTheory, 51, 2681-2685.\n[28] Kantas, N., Doucet, A., Singh, S.S. and Maciejowski, J.M. (2009). An overview of\nsequential Monte Carlo methods for parameter estimation in general state-space models.\nin Proceedings IFAC System Identification (SysId) Meeting.\n[29] Kitagawa, G. and Sato, S. (2001). Monte Carlo smoothing and self-organising statespace model. In [19], 178\u2013195. New York: Springer.\n[30] Klaas, M., De Freitas, N. and Doucet, A. (2005). Toward practical N 2 Monte Carlo:\nThe marginal particle filter. Proc. Uncertainty in Artificial Intelligence, 308-15.\n[31] Le Corff, S., Fort, G. and Moulines, E. (2010) Online expectation-maximization algorithm to solve the SLAM problem. Technical report , Telecom-ParisTech.\n[32] Le Gland, F. and Mevel, M. (1997) Recursive identification in hidden Markov models,\nProceedings of the 36th IEEE Conference on Decision and Control, 3468-3473.\n[33] Mongillo, G. and Den\u00e8ve, S. (2008). Online learning with hidden Markov models. Neural\nComputation, 20, 1706-1716.\n[34] Olsson, J., Capp\u00e9, O., Douc, R. and Moulines, E. (2008). Sequential Monte Carlo\nsmoothing with application to parameter estimation in non-linear state space models.\nBernoulli, 14, 155-179.\n[35] Pitt, M.K. & Shephard, N. (1999). Filtering via simulation: auxiliary particle filter. J.\nAm. Statist. Ass., 94, 590-9.\n[36] Poyiadjis, G. (2006) Particle methods for parameter estimation in general state-space\nmodels. PhD thesis, Cambridge University.\n\n25\n\n\f[37] Poyiadjis, G., Doucet, A. and Singh, S.S. (2011). Particle approximations of the score\nand observed information matrix in state-space models with application to parameter\nestimation. Biometrika, to appear.\n[38] Storvik, G. (2002). Particle filters in state space models with the presence of unknown\nstatic parameters. IEEE. Trans. Signal Proc., 50:2, 281\u2013289.\n[39] Titterington, D.M. (1984) Recursive parameter estimation using incomplete data. J.\nRoyal Statist. Soc. B, 46, 257-267.\n[40] Zeitouni, O. and Dembo, A. (1988). Exact filters for the estimation of the number\nof transitions of finite-state continuous-time Markov processes. IEEE Trans. Inform.\nTheory, 34, 890-893.\n\n26\n\n\f"}