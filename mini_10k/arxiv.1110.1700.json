{"id": "http://arxiv.org/abs/1110.1700v1", "guidislink": true, "updated": "2011-10-08T06:05:02Z", "updated_parsed": [2011, 10, 8, 6, 5, 2, 5, 281, 0], "published": "2011-10-08T06:05:02Z", "published_parsed": [2011, 10, 8, 6, 5, 2, 5, 281, 0], "title": "Adaptive Data Stream Management System Using Learning Automata", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.0269%2C1110.1582%2C1110.0932%2C1110.0950%2C1110.3404%2C1110.1636%2C1110.1700%2C1110.3368%2C1110.1975%2C1110.3383%2C1110.1169%2C1110.2074%2C1110.2662%2C1110.0237%2C1110.2937%2C1110.0008%2C1110.4048%2C1110.6042%2C1110.4191%2C1110.3959%2C1110.1467%2C1110.1526%2C1110.1881%2C1110.1668%2C1110.4367%2C1110.6005%2C1110.4348%2C1110.1619%2C1110.4581%2C1110.5101%2C1110.6457%2C1110.2793%2C1110.3210%2C1110.6071%2C1110.5180%2C1110.0971%2C1110.1172%2C1110.0970%2C1110.4560%2C1110.3575%2C1110.4344%2C1110.1190%2C1110.1909%2C1110.3097%2C1110.6660%2C1110.4834%2C1110.4037%2C1110.3777%2C1110.5823%2C1110.0062%2C1110.3128%2C1110.2270%2C1110.3553%2C1110.2087%2C1110.4874%2C1110.4822%2C1110.4204%2C1110.4546%2C1110.3564%2C1110.3106%2C1110.5068%2C1110.6503%2C1110.5871%2C1110.1049%2C1110.0860%2C1110.3915%2C1110.6360%2C1110.4873%2C1110.0148%2C1110.1764%2C1110.0098%2C1110.1503%2C1110.0913%2C1110.2320%2C1110.6902%2C1110.6645%2C1110.4416%2C1110.2997%2C1110.3343%2C1110.4317%2C1110.2803%2C1110.2352%2C1110.6138%2C1110.6306%2C1110.6300%2C1110.0144%2C1110.0802%2C1110.6526%2C1110.2318%2C1110.3524%2C1110.5375%2C1110.0917%2C1110.0029%2C1110.2787%2C1110.2110%2C1110.2407%2C1110.1175%2C1110.3884%2C1110.2836%2C1110.0279%2C1110.2661&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Adaptive Data Stream Management System Using Learning Automata"}, "summary": "In many modern applications, data are received as infinite, rapid,\nunpredictable and time- variant data elements that are known as data streams.\nSystems which are able to process data streams with such properties are called\nData Stream Management Systems (DSMS). Due to the unpredictable and time-\nvariant properties of data streams as well as system, adaptivity of the DSMS is\na major requirement for each DSMS. Accordingly, determining parameters which\nare effective on the most important performance metric of a DSMS (i.e.,\nresponse time) and analysing them will affect on designing an adaptive DSMS. In\nthis paper, effective parameters on response time of DSMS are studied and\nanalysed and a solution is proposed for DSMSs' adaptivity. The proposed\nadaptive DSMS architecture includes a learning unit that frequently evaluates\nsystem to adjust the optimal value for each of tuneable effective. Learning\nAutomata is used as the learning mechanism of the learning unit to adjust the\nvalue of tuneable effective parameters. So, when system faces some changes, the\nlearning unit increases performance by tuning each of tuneable effective\nparameters to its optimum value. Evaluation results illustrate that after a\nwhile, parameters reach their optimum value and then DSMS's adaptivity will be\nimproved considerably.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.0269%2C1110.1582%2C1110.0932%2C1110.0950%2C1110.3404%2C1110.1636%2C1110.1700%2C1110.3368%2C1110.1975%2C1110.3383%2C1110.1169%2C1110.2074%2C1110.2662%2C1110.0237%2C1110.2937%2C1110.0008%2C1110.4048%2C1110.6042%2C1110.4191%2C1110.3959%2C1110.1467%2C1110.1526%2C1110.1881%2C1110.1668%2C1110.4367%2C1110.6005%2C1110.4348%2C1110.1619%2C1110.4581%2C1110.5101%2C1110.6457%2C1110.2793%2C1110.3210%2C1110.6071%2C1110.5180%2C1110.0971%2C1110.1172%2C1110.0970%2C1110.4560%2C1110.3575%2C1110.4344%2C1110.1190%2C1110.1909%2C1110.3097%2C1110.6660%2C1110.4834%2C1110.4037%2C1110.3777%2C1110.5823%2C1110.0062%2C1110.3128%2C1110.2270%2C1110.3553%2C1110.2087%2C1110.4874%2C1110.4822%2C1110.4204%2C1110.4546%2C1110.3564%2C1110.3106%2C1110.5068%2C1110.6503%2C1110.5871%2C1110.1049%2C1110.0860%2C1110.3915%2C1110.6360%2C1110.4873%2C1110.0148%2C1110.1764%2C1110.0098%2C1110.1503%2C1110.0913%2C1110.2320%2C1110.6902%2C1110.6645%2C1110.4416%2C1110.2997%2C1110.3343%2C1110.4317%2C1110.2803%2C1110.2352%2C1110.6138%2C1110.6306%2C1110.6300%2C1110.0144%2C1110.0802%2C1110.6526%2C1110.2318%2C1110.3524%2C1110.5375%2C1110.0917%2C1110.0029%2C1110.2787%2C1110.2110%2C1110.2407%2C1110.1175%2C1110.3884%2C1110.2836%2C1110.0279%2C1110.2661&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In many modern applications, data are received as infinite, rapid,\nunpredictable and time- variant data elements that are known as data streams.\nSystems which are able to process data streams with such properties are called\nData Stream Management Systems (DSMS). Due to the unpredictable and time-\nvariant properties of data streams as well as system, adaptivity of the DSMS is\na major requirement for each DSMS. Accordingly, determining parameters which\nare effective on the most important performance metric of a DSMS (i.e.,\nresponse time) and analysing them will affect on designing an adaptive DSMS. In\nthis paper, effective parameters on response time of DSMS are studied and\nanalysed and a solution is proposed for DSMSs' adaptivity. The proposed\nadaptive DSMS architecture includes a learning unit that frequently evaluates\nsystem to adjust the optimal value for each of tuneable effective. Learning\nAutomata is used as the learning mechanism of the learning unit to adjust the\nvalue of tuneable effective parameters. So, when system faces some changes, the\nlearning unit increases performance by tuning each of tuneable effective\nparameters to its optimum value. Evaluation results illustrate that after a\nwhile, parameters reach their optimum value and then DSMS's adaptivity will be\nimproved considerably."}, "authors": ["Shirin Mohammadi", "Ali A. Safaei", "Fatemeh Abdi", "Mostafa S. Haghjoo"], "author_detail": {"name": "Mostafa S. Haghjoo"}, "author": "Mostafa S. Haghjoo", "links": [{"href": "http://arxiv.org/abs/1110.1700v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1110.1700v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1110.1700v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1110.1700v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Advanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nADAPTIVE DATA STREAM MANAGEMENT SYSTEM\nUSING LEARNING AUTOMATA\nShirin Mohammadi1, Ali A. Safaei2, Fatemeh Abdi3 and Mostafa S. Haghjoo4\n1\n\nDepartment of Computer Engineering, Iran University of Science and Technology,\nTehran, Iran\n\n2\n\nDepartment of Computer Engineering, Iran University of Science and Technology,\nTehran, Iran\n\nsh_mohammadi@comp.iust.ac.ir\n\nsafaeei@iust.ac.ir\n3\n\nDepartment of Science, Babol-Branch, Islamic Azad University, Babol, Iran\nsulmaz_abdi@yahoo.com\n\n4\n\nDepartment of Computer Engineering, Iran University of Science and Technology,\nTehran, Iran\nhaghjoom@iust.ac.ir\n\nABSTRACT\nIn many modern applications, data are received as infinite, rapid, unpredictable and time- variant data\nelements that are known as data streams. Systems which are able to process data streams with such\nproperties are called Data Stream Management Systems (DSMS). Due to the unpredictable and timevariant properties of data streams as well as system, adaptivity of the DSMS is a major requirement for\neach DSMS. Accordingly, determining parameters which are effective on the most important performance\nmetric of a DSMS (i.e., response time) and analysing them will affect on designing an adaptive DSMS.\nIn this paper, effective parameters on response time of DSMS are studied and analysed and a solution is\nproposed for DSMSs' adaptivity. The proposed adaptive DSMS architecture includes a learning unit that\nfrequently evaluates system to adjust the optimal value for each of tuneable effective. Learning Automata is\nused as the learning mechanism of the learning unit to adjust the value of tuneable effective parameters. So,\nwhen system faces some changes, the learning unit increases performance by tuning each of tuneable\neffective parameters to its optimum value. Evaluation results illustrate that after a while, parameters reach\ntheir optimum value and then DSMS's adaptivity will be improved considerably.\n\nKEYWORDS\nData Stream, DSMS, Adaptivity, Performance Metrics, Response Time, Learning Automata\n\n1. INTRODUCTION\nDatabase management systems (DBMSs) simply store huge amounts of data. When users of\napplications need to use huge amounts of data, they once load them from a DBMS or retrieve a\nspecific part of them by a simple real time query. The DBMSs are the best in executing ad-hoc\nqueries.\nBut various applications need to process data streams. The data streams include infinite,\ncontinuous, rapid and timely variant data elements [1, 2]. These applications need a new class of\nsystems called as data stream management systems (DSMSs). The DSMSs are able to propose\nqueries on data streams. These queries are executing in a long time mode since data arrival in a\ncontinuous mode, and are called as continuous queries [1].\nDOI : 10.5121/acij.2011.2501\n\n1\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nThere are bold challenges about DSMSs. Dynamic features of system, specifications of input\ndata streams, the output and queries while executing the continuous queries, all may change\nsignificantly. When each of the above categories changes a single execution which has the best\nperformance before the change may suddenly change to the worst performance one. Thus,\nadaptivity for DSMSs is an important issue, because regardless of it the performance may\ndecrease significantly while time is going on.\nSome performance metrics of DSMSs which have to be evaluated regularly are as [3, 4]:\n1- Response time (tuple latency): the amount of time (or average time) which a tuple (or an\norder of them) has to pass to execute a query. Of course this time includes all waiting\ntimes in buffers.\n2- Memory usage: the maximum amount of memory which is used by the system.\n3- Throughput: number of output tuple in a specific unit of time.\n4- Accuracy of results: rate of accuracy or correctness of results in fault tolerant conditions.\n5- Tuple loss: ratio of lost tuples that total count of input tuples.\nThese metrics are not independent of each other. For example reducing the tuple latency usually\nincreases the amount of memory usage or reducing the memory usage leads to loss of tuples and\nfinally reduction in accuracy of results. Selecting a metric can significantly influence the other\nones. So tradeoff has to establish between performance metrics. A data stream system establishes\na rational tradeoff based on application needs [3].\nThe rest of the paper is organized as follow: effective parameters on DSMSs' response time are\nreviewed and analysed in section 2. The proposed adaptive DSMS is presented in section 3;\neffective parameters are considered to be tuneable or not, and learning automata used for\nadjusting tuneable effective parameters in the proposed adaptive DSMS is explained in detail.\nPerformance evaluation is discussed in section 4 and related work is reviewed in section 5.\nFinally, we conclude and propose some future work in section 6.\n2. BACKGROUND\nThe most important performance metric for DSMSs is the response time. Response time of a\nquery in a data stream system depends on several parameters. Some are less effective whilst some\nare more.\nAccording to the figure 1, effective parameters on response time of a DSMS are represented in\ntotal categories of: 1) Data stream properties, 2) Query properties, 3) Query execution\nproperties, 4) Output properties, 5) System properties (static conditions) and 6) System condition\n(Dynamic).\nRegistered\nQueries\n\nSystem features\n(static Conditions)\n\nData Stream(s)\n\nSystem Conditions\n(dynamic)\n\nData Stream Management System\n\n(DSMS)\n\nOutput\n\nQueries execution\n\nFigure 1. Categorizing the effective parameters on response time [23]\n\n\u2022\n\nData Stream Properties\n\nA data stream includes data elements generated in an infinite, continuous and rapid manner\nwhich varies in time. In other words data stream of S is a set of s elements with time stamp of\n\u03c4 which the elements arrive to the system in time stamp order. The time stamp specifies the\nlogical entrance time of a tuple into the data stream.\n2\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nData streams are generated by external resources or other applications and are sent to the\nDSMS. DSMSs have no direct access or any kind of control on data resources [3]. The most\nimportant features of a data stream which are effective on response time of the system\ninclude: 1) Type of elements in a data stream, 2) Domain of attributes, 3) Number of\nattributes, 4) Data stream distribution and 5) Arrival rate into the system.\n\n\u2022\n\nQuery properties\n\nContinuous queries are those which have several processes on new data to generate new\nresults. They are executed in a long lasting mode and generate the results continuously [3].\nIn this category details of effective parameters on response time are considered as: 1) Type of\nquery, 2) Number of operators in a query, 3) Arrangement of operators in query plan and 4)\nType of operators.\n\n\u2022\n\nExecution of Queries\n\nTo execute multiple queries concurrently in a DSMS, first the query selection has to be\nexecuted. In second step the manner of accurate execution of queries has to be specified. The\nfirst and second steps are discussed as scheduling queries and scheduling query operators [5].\nExecution algorithm of operators is also an important issue about the response time of the\nsystem. Effective parameters of this category include: 1) Scheduling queries, 2) Scheduling\nquery operators [6, 7] and 3) Scheduling algorithms of operators [10].\n\n\u2022\n\nOutput properties\n\nThis category includes parameters such as 1) Type of output and 2) Number of outputs.\n\n\u2022\n\nSystem properties (static conditions)\n\nDepending on total or the static conditions of the DSMS, effective parameters on response\ntime in this category can be considered as: 1) Number of registered queries, 2) Query\nregistration time [8], 3)Number of processes (logical machines) [9], 4) Processing capacity of\nprocessor(s), 5) Processor's architecture [10] and 6) The amount of available memory.\n\n\u2022\n\nSystem condition (Dynamic)\n\nDynamic status of the system includes changes which may occur while systems execution.\nThis category includes parameters such as: 1) Allocated processing capacity of processor(s)\nfor query execution, 2) Memory usage, 3) Overload in DSMS [8, 11] and 4) Occurring\ndeadlock.\n3. ADAPTIVE DSMS USING LEARNING AUTOMATA\nAt first, effective parameters which are not tuneable must be determined and removed from are\ndiscussion since we are not able to change them.\n\n3.1. Effective Parameters on Response Time of a DSMS\nEffective parameters on response time can be categorized as: 1) constant value parameters, 2)\neffective and non-tuneable parameters and 3) effective and tuneable parameters.\n\n\u2022\n\nConstant Value Parameters\n\nThis category includes parameters which are assumed with constant values of Table 1.\n\n3\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nTable 1. Parameters assumed with constant values\n\n\u2022\n\nParameter Name\n\nSymbol Parameter's Value\n\nType of data stream's elements\n\nTe\n\nTuple*\n\nDomain of attributes\n\nDA\n\nLong*\n\nNumber of attributes\n\nNA\n\nFive*\n\nType of query\n\nTQ\n\nContinues query\n\nScheduling Operators\n\nS Op\n\nFIFO\u2217*\n\nAlgorithm of operators\n\nAOp\n\nDeveloped algorithm in the system\n\nType of output\n\nTOut\n\nRelation\n\nNumber of outputs\n\nN Out\n\nOne\n\nQuery registration time\n\nRQ\n\nPre-Defined\n\n*\n\nEffective and non-tuneable Parameters\n\nParameters of table 2 are effective on response time of the DSMS but they have non-tuneable\nvalues. The reason of being non-tuneable for each one is provided in Table 2.\nTable 2. Effective and non-tuneable parameters\nSymbol\nReason\nParameter Name\nData stream distribution\n\nF\n\nArrival rate into the system\n\n\u03b1\n\nProcessing capacity of processor(s)\n\nCp\n\nSystem architecture\n\nAp\n\nThe amount of available memory\n\nMA\n\nAllocated processing capacity of\nProcessor(s) for query execution\n\nC Dp\n\nThe DSMS has no control on arrival order,\narrival rate or on distribution of the input\nstream. Because the data streams of\ndistributed resources are reaching the\nsystem remotely [2].\nIn cause of limitations in resources such as\nprocessor(s) and memory, these parameters\nare assumed as non-tuneable ones.\nConsidering that system processor(s) may\nsimultaneously be owned to other\nprocesses while executing queries, then\nalways just a part of processor(s) is\nassigned to the process of query which of\ncourse this parameter is an non-tuneable\none for the system.\n\n* The Input data set includes data of monitoring IP packets which is located in Internet Traffic Archive (ITA) [12]. One of traces,\nspecifically the \"DEC-PKT\" contains all wide-area traffic of an hour between Digital Equipment Corporation and the rest of the\nworld. This real-world data set is used in our experiments. Two types of monitored packets, the TCP and the UDP packets, are\nselected as input streams. Each TCP packet contains five items of source address, destination address, source port, destination port,\nand length. UDP packets are the same as TCP missing the length of packets.\n** considering the results of experiments on scheduling algorithms [6], regarding the simplicity of implementation and less overhead\nfor the system, the FIFO is the best choice.\n\n4\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nOverload in dSMS\n\nOccurring deadlock\n\n\u2022\n\nOv\n\nLarge number of resources is one of the\nfeatures of the DSMSs which the rate of\ndata arrival in them can unpredictably be\nhigh. While premature data arrival,\navailable demands on system resources\n(like the processor, main memory or the\nband width) may exceed of the capacity.\nThat condition is called as occurring\noverload in the system [8, 11]. This\nparameter is also an non-tuneable one.\n\nD\n\nIf three sufficient conditions for occurring\ndeadlock are established through the data\nstream system, the failure occurs and the\nresponse time will be infinite. occurring\ndeadlock is also an non-tuneable\nparameter.\n\nEffective and Tuneable Parameters\n\nTable 3 represents effective and tuneable parameters. Three parameters of number of operators in\na query, arrangement of operators in query plan and type of operators are studied in topics such as\noptimizing query plans and adaptivity of query processing [2]. About four parameters of\nscheduling queries, number of registered queries, number of processes and memory usage we can\nuse the learning unit for optimal setting of values along the execution of the system, dynamically.\nTable 3. Effective and tuneable parameters\n\nParameter Name\n\nSymbol\n\nNumber of Operators in a Query\n\nN Op\n\nArrangement of Operators in Query Plan\n\nQP\n\nType of Operators\n\nTOp\n\nReason\nCount, type and order of operators in a\nquery can set to be optimum. These\nthree parameters are fully studied in\ntopics such as query plan optimization\nand adaptivity of query process.\n\nSQ\n\nThe optimum scheduling algorithms for\nDSMSs are ones which work cyclic like\nthe Round Robin. In these algorithms\nthe most important parameter is the\nvalue of Time Quantum, which can\nsimply set to the optimum value\nregarding\nfeedbacks\nfrom\nthe\nenvironment.\n\nNumber of Registered Queries\n\nNQ\n\nConsidering the priority of registered\nqueries by the user, while reducing\nperformance factors, queries with less\npriority can temporarily suspend and\nreload in proper time.\n\nNumber of Processes (logical Machines)\n\nNp\n\nIn relation to the processing capacity of\nprocessor(s), number of in process\n\nScheduling Queries\n\n5\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nqueries and number of processes of the\nsystem can be adjusted dynamically.\n\nMU\n\nMemory Usage\n\nBy increasing the used memory, tuples\nhave to wait more in queues.\nConsequently the response time of the\nsystem will increase. On the other hand\ndecreasing the used memory forces the\nsystem to dispose tuple which finally\nleads to decrease the Accuracy of\nresults. Dynamic setting of used\nmemory can establish a proper balance\nbetween response time factor and loss of\ntuples rate.\n\n3.2. Improving DSMS Adaptivity by using Learning Automata\nLearning can be used as a solution to establish adaptivity in systems. By using the learning in a\nproper position of the system, each constructor parts of it, even in receiving incomplete and nondeterministic information, can gradually achieve the required optimum strategy of control based\non defined factors about it.\nLearning automata as an abstracted model, is able to do some limited operations. Each selected\nactivity is evaluated by a probable environment and respond the learning automata. The learning\nautomata use this response and select its activity for next step [24, 25]. Figure 2 represents the\nrelation between the learning automata and the environment.\nThe environment can be shown by triple set of E = {\u03b1 , \u03b2 , c} which set of inputs\nas \u03b1 = {\u03b1 1 , \u03b1 2 , L , \u03b1 r } , set of outputs as \u03b2 = {\u03b21 ,..., \u03b2 m } and set of fine possibilities are shown\n\nas c = {c 1 , c 2 ,..., c r } . When \u03b2 is a set with two members then the environment is in type of\nP. in such environment \u03b2 1 = 1 is considered as a fine and \u03b2 2 = 0 is considered as reward. In an\nenvironment of Q type, \u03b2 (n ) can be discretely as a value between [0, 1] and in an environment\nof type S, then \u03b2 (n ) is a random variable in range of [0, 1].\nThe c i is probability of having unfavourable results for\n\n\u03b1 i action. In a static environment\n\nc i values remain unchanged, while in a non-static environment these values change along the\ntime. A learning automaton is separated into two groups with constant and variable structures [24,\n25].\n\u03b1(n)\nEnvironment\n\nLearning Automata\n\n\u03b2 (n)\n\nFigure 2. Relation between learning automata and environment\nLearning automata with variable structure is represented by quadruple set of {\u03b1 , \u03b2 , p, T } which\n\u03b1 = {\u03b1 1,...,\u03b1 r } is set of actions for automata, \u03b2 = {\u03b2 1,...,\u03b2 m } is set of inputs for automata and\np = {p1,..., p r } is the vector of probability of selection of each of actions and pn +1 = T [\u03b1 n , \u03b2n , pn ] is\n6\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nthe learning algorithm. In these kinds of automata, if the \u03b1 i in nth stage of selection receives\ndesirable (or undesirable) response from the environment, probability of p i (n) increases (or\ndecreases) and other probabilities decrease (or increase). The changes occur so that sum of\np i (n) s is always constant and equals to one [24, 25].\n\n3.3. Architecture of the Proposed DSMS\nIn Figure 3 as the parallel query processor engine, we have k processors (logical machines) which\nare in relation to the others. These machines can be physical machines (like processors of a multiprocessing system or nodes of a clustered computer) or virtual machines (like threads which are\nexecuted on cores of multi core processor).\nFor parallel execution of query on k logical machines, for each registered query on the\nrepresented system, first query plan has to be established. Then k same copies of this query plan\nare generated and each of them are sent to one of k logical machines. The goal of this is to inform\nall machines about the query plan (operators and their order), but the issue that each machine has\nto execute which operator and how to communicate with other machines so that the query plan\nexecuted on all machines in parallel mode, is specified by the scheduling algorithm.\nResponse Time, Throughput, Tuple Loss\nNumber of Registered Queries\nNumber of Processes\n\nQueries\n\nSize Buffer\n\nLearning Unit\n\nQuery Plan Generator\n\nTime Quantum RR Algorithm\n\nStorage\nmanagement\n\nScheduler\n\nQuality\nControl\nUnit\n\nQuery processing engines\nProcess 1\nProcess 2\n\n.\n\nInput\nStream(s)\n\n.\n\n.\nOutput(s)\n\nProcess k\n\nFigure 3. Architecture of the adaptive DSMS\nFor dynamically setting of the effective and tuneable parameters as mentioned below, the learning\nunit is used in this system.\n\u2022\n\u2022\n\nTime quantum RR algorithm ( Q RR )\nNumber of registered queries ( N Q )\n\n\u2022\n\u2022\n\nNumber of processes ( N P )\nSize buffer ( S B )\n\nAs we can see in figure 4, the learning unit returns three values of below by getting feedback\nfrom the quality control unit and consequently regulate four mentioned parameters dynamically.\n\u2022\n\nResponse time ( R t )\n\n\u2022\n\u2022\n\nThroughput ( Th )\nTuple loss ( Tloss )\n7\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\nEnvironment\nQ RR ,\n\nRt ,\n\nNQ ,\n\nTh ,\n\nNP ,\n\nTloss\n\nSB\n\nLearning Unit\n\nFigure 4. Learning Unit\nLearning unit is a leaning automaton with variable structure which is defined by quadruple set\nof {\u03b1 , \u03b2 , p, T }. \u03b1 is set of automata actions as a set of quadruple elements of increase ( \u2191 ), decrease\n( \u2193 ), unchanged (-) and parameters of time quantum of RR algorithm to schedule between Queries\n(QRR), number of registered queries (NQ), number of processes (NP) and size of buffer (SB). The\nK \u2191 represents amount of increase, K \u2193 as amount of decrease and f as a factor of addition\n(subtraction) or multiplication (division), represents increase (decrease) of the X parameter. For\nexample if action of (\u2193 Q RR , N P , N Q , \u2191 SB ) is selected, it means that QRR parameter subtracts or\n\ndivides from the f Q\n\nRR\n\nin K \u2193Q units. Values of NP and NQ parameters remain unchanged and SB\nRR\n\nparameter adds or multiplies to fSB in K S\u2191B units. Then \u03b1 function is defined in form of below:\n\uf8f1\uf8f4(Act(QRR , K \u2191Q , K Q\u2193 , f Q ), Act(NP , K \u2191N , K \u2193N , f N ), Act(NQ , K \u2191N , K \u2193N , f N ), Act(SB , K S\u2191 , K S\u2193 , fS )) \uf8fc\uf8f4\n(1)\nRR\nRR\nRR\nP\nP\nP\nQ\nQ\nQ\nB\nB\nB\n\uf8fd\n\uf8f4\uf8f3Act(X, K \u2191 , K \u2193 , f ) \u2208 {\u2191 X, \u2193 X, X}\n\uf8f4\uf8fe\n\n\u03b1 =\uf8f2\n\nConsidering four parameters, each one for each of the actions of addition, subtraction,\nmultiplication and division, then \u03b1 set has 81 members as:\n\uf8f1(\u2191 Q RR , \u2191 N P , \u2191 N Q , \u2191 S B ), (\u2191 Q RR , \u2191 N P , \u2191 N Q , \u2193 S B ), (\u2191 Q RR , \u2191 N P , \u2191 N Q , S B ),\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4(\u2191 Q RR , \u2191 N P , \u2193 N Q , \u2191 S B ), (\u2191 Q RR , \u2191 N P , \u2193 N Q , \u2193 S B ), (\u2191 Q RR , \u2191 N P , \u2193 N Q , S B ),\uf8f4\n\uf8f4\n\uf8f4\n\u03b1 = \uf8f2(\u2191 Q RR , \u2193 N P , \u2193 N Q , \u2191 S B ), (\u2191 Q RR , \u2193 N P , \u2193 N Q , \u2193 S B ), (\u2191 Q RR , \u2193 N P , \u2193 N Q , S B ),\uf8fd\n\uf8f4\n\uf8f4\n\uf8f4(\u2193 Q RR , \u2193 N P , \u2193 N Q , \u2191 S B ), (\u2193 Q RR , \u2193 N P , \u2193 N Q , \u2193 S B ), (\u2193 Q RR , \u2193 N P , \u2193 N Q , S B ),\uf8f4\n\uf8f4..., (Q , N , N , S )\n\uf8f4\nRR\nP\nQ\nB\n\uf8f3\n\uf8fe\n\n(2)\n\nThe \u03b2 is the input set of the learning unit which is defined as:\n\n\u03b2 = {Rt , Tloss, Th}\n\n(3)\n\nThe p is probability vector of selecting each of the actions of the learning unit as:\n\np = {p1 , p2 ,..., p81}\n\n(4)\n\nIf the \u03b1 i action is selected in nth stage and received desired answer from the environment, then\nthe reward function of the learning algorithm is defined as:\n\uf8f1p i (n + 1) = p i (n) + a[1 - p i (n)]\nReward = \uf8f2\n\uf8f3p j (n + 1) = (1 - a) p j (n) \u2200j j \u2260 i\n\n(5)\n\n8\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nIn this formula the pi (n) is probability of selection action of \u03b1 i in nth stage and pi (n + 1) is\nprobability of selecting action of \u03b1 i in (n+1)th stage. The a is the reward parameter and if after\nselecting the \u03b1 i action in nth stage, it receives an undesired response of the environment, the\npenalty function of the algorithm will be in form of below:\n\uf8f1pi (n + 1) = (1 - b)pi (n)\n\uf8f4\nPenalty = \uf8f2\nb\n\uf8f4\uf8f3p j (n + 1) = ( r - 1) + (1 - b) p j (n)\n\n(6)\n\u2200j j \u2260 i\n\nThe b is the penalty parameter and the r is count of set for \u03b1 .\nTo specify desirability of the response from the environment, f (n) function is defined as:\n\nf (n) = \u03b11 \u00d7 k1 + \u03b1 2 \u00d7 k 2 + \u03b1 3 \u00d7 k 3\n\n(7)\n\nWhich parameters of K1, K2, K3 are defined in order of ratio of average response time than the\nresponse time of the nth stage, ratio of average tuple loss than tuple loss in nth stage and finally\nratio of operational potency in nth stage than the average operational potency. \u03b11 , \u03b1 2 , \u03b13 are\ncoefficients between 0 to 1 which represent amount of importance for each of the parameters of\nK1, K2, K3 which they are define as:\nk1 =\n\nR t (n )\nR t (n )\n\n(8)\n\nk2 =\n\nTloss (n )\nTloss (n )\n\n(9)\n\nk3 =\n\nTh(n )\n\n(10)\n\nTh(n )\n\nAverage response time R t (n ) , average tuple loss Tloss (n) and average operational potency Th(n )\nin (n+1)th stage are defined as:\nR t (n + 1) = \u03b3 \u00d7 R t (n ) + (1 \u2212 \u03b3 ) \u00d7 R t (n )\n\n,\n\nR t (0) = 0\n\n(11)\n\nTloss (n + 1) = \u03b3 \u00d7 Tloss (n ) + (1 \u2212 \u03b3 ) \u00d7 Tloss (n )\n\n,\n\nTloss (0 ) = 0\n\n(12)\n\nTh(n + 1) = \u03b3 \u00d7 Th(n ) + (1 \u2212 \u03b3 ) \u00d7 Th(n )\n\n,\n\nTh(0) = 0\n\n(13)\n\nThe F function for specifying desirability of the selected action in nth stage, works as below. If\nthe value of f (n) function is greater than or equal to f (n) value, the action is considered as\ndesired action and if it is lower than f (n) the action is considered as an undesired one. So the\nfunction of learning algorithm of T calls the reward function in case of desired action and calls\nthe Penalty function in case of undesired action.\n\uf8f1\uf8f4desirable\nF =\uf8f2\n\uf8f4\uf8f3Undesirabl e\n\nif f ( n ) \u2265 f ( n )\nif f ( n ) < f ( n )\n\n(14)\n\nThe f (n) function is also defined as:\n9\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nf ( n + 1) = \u03b3 \u00d7 f (n) + (1 \u2212 \u03b3 ) \u00d7 f (n)\n\n,\n\nf (0 ) = 0\n\n(15)\n\nAlong processing queries the learning unit starts to set values of these four parameters. After a\nwhile probability of action of (Q RR , N P , N Q , SB ) , p81 in learning unit reaches the constant value of\none. This means that for next action there is no need to change values of parameters. Of course by\nchanges in the DSMS, these values lose their optimum state and the learning unit after a while\nreached the optimum state in present condition by resetting the values. In this case resets the\nparameters adaptively and in present state.\n\n4. PERFORMANCE EVALUATION\nAs the evaluation process we developed a prototype which been implemented in the Java\nlanguage with JDK 6.0 on a machine which was equipped with a Core i7 2930 Intel processor and\n6GB of RAM in Linux environment. The Input data set includes data of monitoring IP packets\nwhich is located in Internet Traffic Archive (ITA) [12]. One of traces, specifically the \"DECPKT\" contains all wide-area traffic of an hour between Digital Equipment Corporation and the\nrest of the world. This real-world data set is used in our experiments. Two types of monitored\npackets, the TCP and the UDP packets, are selected as input streams. Each TCP packet contains\nfive items of source address, destination address, source port, destination port, and length. UDP\npackets are the same as TCP missing the length of packets.\nElements of the stream are in type of well-structures data (tuple). To schedule queries the Round\nRobin (RR) algorithm and to schedule query operators the FIFO algorithm are used. Also type of\noutput, relation is assumed. Eight registered queries are continuous and registration time of them\nin system is pre-defined. Queries do not have priority, so number of queries is considered fix.\nNumber of processes is four. Tow parameter are learned by Learning Unit: Time Quantum RR\nAlgorithm (QRR) and Size Buffer (SB).\nTime period of execution for the Learning unit is 500 milliseconds. The initial value of\nparameters which are selected for learning is represented in Table 4. The experiment is done\nthrough a 600000 milliseconds period.\nTable 4. Initial Value of Parameters\nParameter\nInitialization\nTime Quantum RR Algorithm\n500ms\nSize Buffer\n500 tuples\nAs we can see in figures 5 and 6, almost after 800 times of execution of LA, both parameters\nreach a constant value in execution. Table 5 represents achieved values for Quantum length and\nbuffer size after 600000 milliseconds of executing the system.\n\nFigure 5 \u2013 learning QRR parameter by LA\n10\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nFigure 6. learning SB parameter by LA\nTable 5. learnt values of parameters\nParameter\nLearned Values\nTime quantum RR algorithm\n50 ms\nSize buffer\n280 tuples\nIt is expected that the achieved values for parameters which are set by learning to be the optimum\nvalues in this execution for the DSMS. To study this as we can see in Table 6, three Metrics of\nresponse time, throughput and tuple loss are compared in an execution without learning and\nanother execution using the learning unit. As is seen all three factors are optimized by the\nlearning unit. Response time of the unit is almost reduced to half (Figure 7). The throughput is\nrelatively became more (Figure 8) and the tuple loss (Figure 9) is also decreased.\n\nTable 6. Comparing Performance Metrics\nPerformance Metrics\nWithout Learning\nUsing Learning\nResponse time\n465.521 ms\n220.668 ms\nThroughput\n1103.546 t/s\n1298.001 t/s\nTuple loss\n0.091\n0.078\n\nFigure 7. Comparing response time metric in DSMS without Learning and Using Learning.\n\n11\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\nFigure 8. Comparing throughput metric in DSMS without Learning and Using Learning.\n\nFigure 9. Comparing tuple loss metric in DSMS without Learning and Using Learning.\nThis comparison shows that when values of parameters of Time Quantum RR Algorithm (QRR)\nand Size Buffer (SB) are set by the learning unit, considering the features of the stream and the\nconditions of the system, performance factors of the DSMS will have significant improvements\nthan executions without using the Learning unit.\n\n5. RELATED WORKS\nLots of researches on DSMSs are done [13]. Also several primary samples of DSMSs such as the\nSTREAM [1,2], Aurora [5] and TelegraphCQ [14] are provided too.\nSome of the available approaches for adaptive query processing in DSMSs are studied. First\napproach is adaptive query processing without load shedding data. In this approach a generic\nframework is presented, called as StreaMon, for adaptive query processing in a DSMS. The\nStreaMon has three core components: Executor, Profiler and Re-optimizer. In this framework\nthree different composition of continuous type of query which needs adaptivity are represented\n[2].\nIn next approach adaptive query processing using the load shedding method is studied. in this\napproach first applications of load shedding and methods of this approach are studied and then a\ngeneral framework is provided. Finally two sample architectures of Aurora and Borealis DSMSs\nafter using load shedding for adaptivity of query processing are provided [8].\nIn next approach the quality adaptive method based on control is studied. In this approach query\nprocessing model and quality adaptive framework based on control which includes four elements\nof Plant, Monitor, Controller and Actuator are provided and then separated study on elements of\nthis framework in addition to defining some functions are provided [26].\nscheduling strategies of operators to process continuous queries on data streams varies from\nsimple ones like the Round Robin [6], chain [15] and Greedy [6] to more complex ones [16,17].\nSome of them are provided for optimizing a performance factor [18, 19], while some others try to\noptimize multiple factors or a compound one [20, 21]. Totally most of these methods are provided\nto minimize tuple latency or the response time factor. Determination and analysis of effective\nparameters on response time of DSMSs are explicitly studied in few references which [22] is one\nof them. The learning method is also used as solution to improve adaptivity in systems [24, 25].\n12\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n\n6. CONCLUSION AND FURTHER WORKS\nPerformance guarantee in a DSMS is important about static and dynamic features of the system,\nspecifications of input and output data streams, query properties and query processing algorithms.\nA DSMS can include huge amount of streams and continuous queries. When the system faces\nsome changes of the above, the total performance will decrease. DSMS needs to automatically set\nof parameters (adptivity) on runtime. The provided daptivity architecture dynamically resets\neffective and tuneable parameters along the execution using the learning technique, considering\nthe received feed backs from the quality control unit. Defining an internal learning to set time\nperiod of the learning unit is a sample of further works. The reason is that in cases which the\nsystem is in stable conditions, this unit will be executed in longer periods and while explosive\narrival or changes in conditions of the system, it will be executed in shorter periods. Setting the\nbuffer size will be done separately for each of the registered queries.\n\nREFERENCES\n[1] A. Arasu, et. al., (2003) \"STREAM: The Stanford Stream Data Manager\". In: Proc. of ACM SIGMOD,\nUSA.\n[2] B. Shivnath, (2005) \"Adaptive Query Processing in Data Stream Management Systems\", Ph.D. thesis,\nDepartment of Computer Science, Stanford University, USA.\n[3] S. Chakravarthy, et. al., (2009) \"Stream data processing: a quality of service perspective: modeling,\nscheduling, load shedding, and complex event processing\", book, springer, USA.\n[4] Y. Bai, et. al., (2008) \"Minimizing Latency and Memory in DSMS-a Unified Approach to QuasiOptimal Scheduling\", Proceedings of the 2nd international workshop on Scalable stream processing\nsystem, University of California, Los Angeles.\n[5] D. Abadi, et. al., (2003) \"Aurora: A New Model and Architecture for Data Stream Management\", In\nVLDB Journal (12)2: 120-139.\n[6] B. Babcock, et. al., (2004) \"Operator Scheduling in Data Stream Systems\", VLDB Journal, 13(4):333\u2013\n353.\n[7] D. Carney, et al., (2003) \"Operator Scheduling in a Data Stream Manager\", in Proceedings of the 29th\ninternational conference on Very large data bases, Germany, pp. 838-849.\n[8] N. Tatbul, et. al., (2007) \"Load Shedding Techniques for Data Stream Management Systems\", Ph.D.\nthesis, Brown University.\n[9] A.A.Safaei, et. al., (2010) \"Parallel Processing of Continuous Queries over Data Streams\",\nDistributed and Parallel Databases, Volume 28, Numbers 2-3, 93-118.\n[10] A Silberschatz, (2005) \"Database System Concepts\", book, 5th edition.\n[11] F. Reiss, et. al.,(2005) \"Data Triage: An Adaptive Architecture for Load Shedding in TelegraphCQ\",\nU.C. Berkeley Department of Electrical Engineering and Computer Science, And Intel Research\nBerkeley, Conference paper, Proceedings of the 21st International Conference on Data Engineering,\nICDE 2005, 5-8 April 2005, Tokyo, Japan.\n[12] Internet Traffic Archive, http://www.acm.org/sigcomm/ITA/\n[13] B. Babcock, et. al., (2002) \"Models and Issues in Data Stream Systems\", Invited paper in Proc. of\nPODS 2002.\n[14] S. Chandrasekaran, et al., (2003) \"TelegraphCQ: Continuous Dataflow Processing\", in ACM\nSIGMOD.\n[15] B Babcock, et al., (2003) \"Chain: Operator Scheduling for Memory Minimization in Data Stream\nSystems\", Proceedings of the ACM SIGMOD International conference.\n[16] M. A. Sharaf, (2005) \"Preemptive Rate-Based Operator Scheduling in a Data Stream Management\nSystem\", in IEEE/AICCSA.\n[17] M. S. Soliman, G. Tan, (2008) \"Operator-scheduling using dynamic chain for continuous-query\nprocessing\", IEEE Int. Conference on Computer Science and Software Engineering.\n[18] S. Chakravarthy, et. al., (2006) \"Scheduling Strategies and Their Evaluation in a Data Stream\nManagement System\", Springer LNCS 4042.\n[19] M. A. Sharaf, et. al., (2008) \"Scheduling Continuous Queries in Data Stream Management Systems\",\nin PVLDB.\n13\n\n\fAdvanced Computing: An International Journal ( ACIJ ), Vol.2, No.5, September 2011\n[20] B. Srivastava, et. al., (2002) \"Exploiting k-Constraints to Reduce Memory Overhead in Continuous\nQueries over Data Streams\", Technical Report.\n[21] M. Ghalambor, et. al., (2009) \"DSMS scheduling regarding complex QoS metrics\", IEEE/ACS\nInternational Conference on Computer Systems and Applications (AICCSA).\n[22] S. Chakravarthy, et. al., (2009) \"Stream data processing: a quality of service perspective: modeling,\nscheduling, load shedding, and complex event processing\", book, springer, USA.\n[23] Sh. Mohammadi , et. al., (2011) \"Effective Parameters on Response Time of Data Stream Management\nSystems\", the 2011 International Conference on Information and Knowledge Engineering (IKE'11),\nUSA.\n[24] K. S. Narendra, M. A. Thathachar,(1989) \"Learning Automata: An Introduction\", Prentice Hall, Inc..\n[25] P. Mars, J. R. Chen, R. Nambir, (1996) \"Learning Algorithms: Theory and Applications in Signal\nProcessing, Control and Communications\", CRC Press, Inc., pp. 5-24.\n[26] Yi-Cheng Tu , et. al., (2005) \"Control-Based Quality Adaptation in Data Stream Management\nSystems\", Book Chapter, Book: \"Database and Expert Systems Applications\", Publish By Springer\nBerlin / Heidelberg, U.S.A and Canada.\nAuthors\nShirin Mohammadi received her B.S. degree in computer engineering in 2007, She is currently a M.S.\nstudent of computer engineering at the Iran University of Science and Technology since 2008. Her research\ninterests include data stream management systems, adaptive query processing, query response time\nestimation, and real time scheduling.\nAli A. Safaei received his B.S. and M.S. degrees in computer engineering in 2001 and 2004, respectively.\nHe is currently a PhD student of computer engineering at the Iran University of Science and Technology\nsince 2005. His research interests include parallel and real-time query processing, quality of services and\noverload handling in data stream management systems, semantic cache and multiple-query optimization.\nFatemeh Abdi received her B.S. and M.S. degrees in computer engineering in 2001 and 2008 at the Iran\nUniversity of Science and Technology. Her research interests include data mining, mobile databases,\nstream systems, query processing and Response time estimation.\nMostafa S. Haghjoo is an associate professor at the Iran University of Science and Technology. He\nreceived his B.Sc. in mathematics and computer science from the Shiraz University in 1976. He received\nhis M.Sc. degree in computer science from the George Washington University in 1978. He obtained his\nPh.D. degree in computer science from the Australian National University in 1995.\n\n14\n\n\f"}