{"id": "http://arxiv.org/abs/0810.5728v2", "guidislink": true, "updated": "2008-11-11T23:50:54Z", "updated_parsed": [2008, 11, 11, 23, 50, 54, 1, 316, 0], "published": "2008-10-31T16:18:14Z", "published_parsed": [2008, 10, 31, 16, 18, 14, 4, 305, 0], "title": "Multi-Objective Model Checking of Markov Decision Processes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0810.2628%2C0810.1206%2C0810.0710%2C0810.0960%2C0810.0477%2C0810.3181%2C0810.2989%2C0810.2003%2C0810.2061%2C0810.4398%2C0810.3192%2C0810.0647%2C0810.1016%2C0810.2784%2C0810.1953%2C0810.5728%2C0810.1727%2C0810.0909%2C0810.3154%2C0810.4444%2C0810.4796%2C0810.4700%2C0810.1051%2C0810.2870%2C0810.2958%2C0810.0575%2C0810.3120%2C0810.5506%2C0810.4240%2C0810.1309%2C0810.4457%2C0810.4639%2C0810.4942%2C0810.1957%2C0810.2104%2C0810.2744%2C0810.1746%2C0810.4552%2C0810.3373%2C0810.1543%2C0810.1074%2C0810.1576%2C0810.2689%2C0810.1049%2C0810.0504%2C0810.5483%2C0810.0665%2C0810.5377%2C0810.0863%2C0810.0369%2C0810.5371%2C0810.1333%2C0810.1829%2C0810.2939%2C0810.1008%2C0810.4373%2C0810.2599%2C0810.2598%2C0810.3815%2C0810.0347%2C0810.3073%2C0810.1397%2C0810.2971%2C0810.4668%2C0810.0664%2C0810.4683%2C0810.2919%2C0810.5491%2C0810.4135%2C0810.3114%2C0810.0333%2C0810.5595%2C0810.4203%2C0810.3177%2C0810.0265%2C0810.0149%2C0810.4443%2C0810.2853%2C0810.1989%2C0810.0885%2C0810.2411%2C0810.3359%2C0810.5687%2C0810.2307%2C0810.0816%2C0810.0891%2C0810.1949%2C0810.3460%2C0810.2264%2C0810.0715%2C0810.5095%2C0810.0120%2C0810.3204%2C0810.2327%2C0810.5498%2C0810.4929%2C0810.4532%2C0810.2900%2C0810.0524%2C0810.5714%2C0810.1506&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Multi-Objective Model Checking of Markov Decision Processes"}, "summary": "We study and provide efficient algorithms for multi-objective model checking\nproblems for Markov Decision Processes (MDPs). Given an MDP, M, and given\nmultiple linear-time (\\omega -regular or LTL) properties \\varphi\\_i, and\nprobabilities r\\_i \\epsilon [0,1], i=1,...,k, we ask whether there exists a\nstrategy \\sigma for the controller such that, for all i, the probability that a\ntrajectory of M controlled by \\sigma satisfies \\varphi\\_i is at least r\\_i. We\nprovide an algorithm that decides whether there exists such a strategy and if\nso produces it, and which runs in time polynomial in the size of the MDP. Such\na strategy may require the use of both randomization and memory. We also\nconsider more general multi-objective \\omega -regular queries, which we\nmotivate with an application to assume-guarantee compositional reasoning for\nprobabilistic systems.\n  Note that there can be trade-offs between different properties: satisfying\nproperty \\varphi\\_1 with high probability may necessitate satisfying \\varphi\\_2\nwith low probability. Viewing this as a multi-objective optimization problem,\nwe want information about the \"trade-off curve\" or Pareto curve for maximizing\nthe probabilities of different properties. We show that one can compute an\napproximate Pareto curve with respect to a set of \\omega -regular properties in\ntime polynomial in the size of the MDP.\n  Our quantitative upper bounds use LP methods. We also study qualitative\nmulti-objective model checking problems, and we show that these can be analysed\nby purely graph-theoretic methods, even though the strategies may still require\nboth randomization and memory.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0810.2628%2C0810.1206%2C0810.0710%2C0810.0960%2C0810.0477%2C0810.3181%2C0810.2989%2C0810.2003%2C0810.2061%2C0810.4398%2C0810.3192%2C0810.0647%2C0810.1016%2C0810.2784%2C0810.1953%2C0810.5728%2C0810.1727%2C0810.0909%2C0810.3154%2C0810.4444%2C0810.4796%2C0810.4700%2C0810.1051%2C0810.2870%2C0810.2958%2C0810.0575%2C0810.3120%2C0810.5506%2C0810.4240%2C0810.1309%2C0810.4457%2C0810.4639%2C0810.4942%2C0810.1957%2C0810.2104%2C0810.2744%2C0810.1746%2C0810.4552%2C0810.3373%2C0810.1543%2C0810.1074%2C0810.1576%2C0810.2689%2C0810.1049%2C0810.0504%2C0810.5483%2C0810.0665%2C0810.5377%2C0810.0863%2C0810.0369%2C0810.5371%2C0810.1333%2C0810.1829%2C0810.2939%2C0810.1008%2C0810.4373%2C0810.2599%2C0810.2598%2C0810.3815%2C0810.0347%2C0810.3073%2C0810.1397%2C0810.2971%2C0810.4668%2C0810.0664%2C0810.4683%2C0810.2919%2C0810.5491%2C0810.4135%2C0810.3114%2C0810.0333%2C0810.5595%2C0810.4203%2C0810.3177%2C0810.0265%2C0810.0149%2C0810.4443%2C0810.2853%2C0810.1989%2C0810.0885%2C0810.2411%2C0810.3359%2C0810.5687%2C0810.2307%2C0810.0816%2C0810.0891%2C0810.1949%2C0810.3460%2C0810.2264%2C0810.0715%2C0810.5095%2C0810.0120%2C0810.3204%2C0810.2327%2C0810.5498%2C0810.4929%2C0810.4532%2C0810.2900%2C0810.0524%2C0810.5714%2C0810.1506&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study and provide efficient algorithms for multi-objective model checking\nproblems for Markov Decision Processes (MDPs). Given an MDP, M, and given\nmultiple linear-time (\\omega -regular or LTL) properties \\varphi\\_i, and\nprobabilities r\\_i \\epsilon [0,1], i=1,...,k, we ask whether there exists a\nstrategy \\sigma for the controller such that, for all i, the probability that a\ntrajectory of M controlled by \\sigma satisfies \\varphi\\_i is at least r\\_i. We\nprovide an algorithm that decides whether there exists such a strategy and if\nso produces it, and which runs in time polynomial in the size of the MDP. Such\na strategy may require the use of both randomization and memory. We also\nconsider more general multi-objective \\omega -regular queries, which we\nmotivate with an application to assume-guarantee compositional reasoning for\nprobabilistic systems.\n  Note that there can be trade-offs between different properties: satisfying\nproperty \\varphi\\_1 with high probability may necessitate satisfying \\varphi\\_2\nwith low probability. Viewing this as a multi-objective optimization problem,\nwe want information about the \"trade-off curve\" or Pareto curve for maximizing\nthe probabilities of different properties. We show that one can compute an\napproximate Pareto curve with respect to a set of \\omega -regular properties in\ntime polynomial in the size of the MDP.\n  Our quantitative upper bounds use LP methods. We also study qualitative\nmulti-objective model checking problems, and we show that these can be analysed\nby purely graph-theoretic methods, even though the strategies may still require\nboth randomization and memory."}, "authors": ["Kousha Etessami", "Marta Kwiatkowska", "Moshe Y. Vardi", "Mihalis Yannakakis"], "author_detail": {"name": "Mihalis Yannakakis"}, "author": "Mihalis Yannakakis", "links": [{"title": "doi", "href": "http://dx.doi.org/10.2168/LMCS-4(4:8)2008", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0810.5728v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0810.5728v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "21 pages, 2 figures", "arxiv_primary_category": {"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CC", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.GT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "G.3; F.2; F.3.1; F.4.1", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0810.5728v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0810.5728v2", "journal_reference": "Logical Methods in Computer Science, Volume 4, Issue 4 (November\n  12, 2008) lmcs:990", "doi": "10.2168/LMCS-4(4:8)2008", "fulltext": "Logical Methods in Computer Science\nVol. 4 (4:8) 2008, pp. 1\u201321\nwww.lmcs-online.org\n\nSubmitted\nPublished\n\nOct. 4, 2007\nNov. 12, 2008\n\nMULTI-OBJECTIVE MODEL CHECKING OF\nMARKOV DECISION PROCESSES \u2217\nKOUSHA ETESSAMI a , MARTA KWIATKOWSKA b , MOSHE Y. VARDI c ,\nAND MIHALIS YANNAKAKIS d\na\n\nLFCS, School of Informatics, University of Edinburgh, UK\ne-mail address: kousha@inf.ed.ac.uk\n\nb\n\nComputing Laboratory, Oxford University, UK\ne-mail address: Marta.Kwiatkowska@comlab.ox.ac.uk\n\nc\n\nDepartment of Computer Science, Rice University, USA\ne-mail address: vardi@cs.rice.edu\n\nd\n\nDepartment of Computer Science, Columbia University, USA\ne-mail address: mihalis@cs.columbia.edu\n\nAbstract. We study and provide efficient algorithms for multi-objective model checking\nproblems for Markov Decision Processes (MDPs). Given an MDP, M , and given multiple\nlinear-time (\u03c9-regular or LTL) properties \u03c6i , and probabilities ri \u2208 [0, 1], i = 1, . . . , k, we\nask whether there exists a strategy \u03c3 for the controller such that, for all i, the probability\nthat a trajectory of M controlled by \u03c3 satisfies \u03c6i is at least ri . We provide an algorithm\nthat decides whether there exists such a strategy and if so produces it, and which runs in\ntime polynomial in the size of the MDP. Such a strategy may require the use of both randomization and memory. We also consider more general multi-objective \u03c9-regular queries,\nwhich we motivate with an application to assume-guarantee compositional reasoning for\nprobabilistic systems.\nNote that there can be trade-offs between different properties: satisfying property \u03c61\nwith high probability may necessitate satisfying \u03c62 with low probability. Viewing this as\na multi-objective optimization problem, we want information about the \"trade-off curve\"\nor Pareto curve for maximizing the probabilities of different properties. We show that one\ncan compute an approximate Pareto curve with respect to a set of \u03c9-regular properties in\ntime polynomial in the size of the MDP.\nOur quantitative upper bounds use LP methods. We also study qualitative multiobjective model checking problems, and we show that these can be analysed by purely\ngraph-theoretic methods, even though the strategies may still require both randomization\nand memory.\n\n1998 ACM Subject Classification: G.3, F.2, F.3.1, F.4.1.\nKey words and phrases: Markov Decision Processes, Model Checking, Multi-Objective Optimization.\n\u2217\nA preliminary version of this paper appeared in the Proceedings of the 13th International Conference on\nTools and Algorithms for the Construction and Analysis of Systems (TACAS'07).\n\nl\n\nLOGICAL METHODS\nIN COMPUTER SCIENCE\n\nc\nDOI:10.2168/LMCS-4 (4:8) 2008\n\nCC\n\nK. Etessami, M. Kwiatkowska, M. Y. Vardi, and M. Yannakakis\nCreative Commons\n\n\f2\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\n1\n\ns\na1\n\na2\n\na3\n\n.6\n.5\n.4\n\n.6 .5\n\n.5\n\n.8\n\n.2\n\n3P1\n1\n\n1\n\nP0\n\nP2\n\nP1\n\nP0\n\n1\n\n1\n\n3P2 .5\n\n.8\n\n1\n\nFigure 1: An MDP with two objectives, 3P1 and 3P2 , and the associated Pareto curve.\n1. Introduction\nMarkov Decision Processes (MDPs) are standard models for stochastic optimization\nand for modelling systems with probabilistic and nondeterministic or controlled behavior\n(see [Put94, Var85, CY95, CY98]). In an MDP, at each state, the controller can choose\nfrom among a number of actions, or choose a probability distribution over actions. Each\naction at a state determines a probability distribution on the next state. Fixing an initial\nstate and fixing the controller's strategy determines a probability space of infinite runs\n(trajectories) of the MDP. For MDPs with a single objective, the controller's goal is to\noptimize the value of an objective function, or payoff, which is a function of the entire\ntrajectory. Many different objectives have been studied for MDPs, with a wide variety of\napplications. In particular, in verification research linear-time model checking of MDPs has\nbeen studied, where the objective is to maximize the probability that the trajectory satisfies\na given \u03c9-regular or LTL property ([CY98, CY95, Var85]).\nIn many settings we may not just care about a single property. Rather, we may have\na number of different properties and we may want to know whether we can simultaneously\nsatisfy all of them with given probabilities. For example, in a system with a server and two\nclients, we may want to maximize the probability for both clients 1 and 2 of the temporal\nproperty: \"every request issued by client i eventually receives a response from the server\",\ni = 1, 2. Clearly, there may be a trade-off. To increase this probability for client 1 we\nmay have to decrease it for client 2, and vice versa. We thus want to know what are\nthe simultaneously achievable pairs (p1 , p2 ) of probabilities for the two properties. More\nspecifically, we will be interested in the \"trade-off curve\" or Pareto curve. The Pareto curve\nis the set of all achievable vectors p = (p1 , p2 ) \u2208 [0, 1]2 such that there does not exist another\nachievable vector p\u2032 that dominates p, meaning that p \u2264 p\u2032 (coordinate-wise inequality) and\np 6= p\u2032 .\nConcretely, consider the very simple MDP depicted in Figure 1. Starting at state s,\nwe can take one of three possible actions {a1 , a2 , a3 }. Suppose we are interested in LTL\nproperties 3P1 and 3P2 . Thus we want to maximize the probability of reaching the two\ndistinct vertices labeled by P1 and P2 , respectively. To maximize the probability of 3P1\nwe should take action a1 , thus reaching P1 with probability 0.6 and P2 with probability 0.\nTo maximize the probability of 3P2 we should take a2 , reaching P2 with probability 0.8\nand P1 with probability 0. To maximize the sum total probability of reaching P1 or P2 , we\nshould take a3 , reaching both with probability 0.5. Now observe that we can also \"mix\"\nthese pure strategies using randomization to obtain any convex combination of these three\n\n\fMULTI-OBJECTIVE MODEL CHECKING OF\n\nMARKOV DECISION PROCESSES \u2217\n\n3\n\nvalue vectors. In the graph on the right in Figure 1, the dotted line plots the Pareto curve\nfor these two properties.\nThe Pareto curve P in general contains infinitely many points, and it can be too costly\nto compute an exact representation for it (see Section 2). Instead of computing it outright\nwe can try to approximate it ([PY00]). An \u01eb-approximate Pareto curve is a set of achievable\nvectors P(\u01eb) such that for every achievable vector r there is some vector t \u2208 P(\u01eb) which\n\"almost\" dominates it, meaning r \u2264 (1 + \u01eb)t.\nIn general, given a labeled MDP M , k distinct \u03c9-regular properties, \u03a6 = h\u03c6i | i =\n1, . . . , ki, a start state u, and a strategy \u03c3, let Pr\u03c3u (\u03c6i ) denote the probability that starting\nat u, using strategy \u03c3, the trajectory satisfies \u03c6i . For a strategy \u03c3, define the vector\nt\u03c3 = (t\u03c31 , . . . , t\u03c3k ), where t\u03c3i = Pr\u03c3u (\u03c6i ), for i = 1, . . . , k. We say a value vector r \u2208 [0, 1]k is\nachievable for \u03a6, if there exists a strategy \u03c3 such that t\u03c3 \u2265 r.\nWe provide an algorithm that given MDP M , start state u, properties \u03a6, and rational\nvalue vector r \u2208 [0, 1]k , decides whether r is achievable, and if so produces a strategy \u03c3 such\nthat t\u03c3 \u2265 r. The algorithm runs in time polynomial in the size of the MDP. The strategies\nmay require both randomization and memory. Our algorithm works by first reducing the\nachievability problem for multiple \u03c9-regular properties to one with multiple reachability\nobjectives, and then reducing the multi-objective reachability problem to a multi-objective\nlinear programming problem. We also show that one can compute an \u01eb-approximate Pareto\ncurve for \u03a6 in time polynomial in the size of the MDP and in 1/\u01eb. To do this, we use\nour linear programming characterization for achievability, and use results from [PY00] on\napproximating the Pareto curve for multi-objective linear programming problems.\nWe also consider more general multi-objective queries. Given a boolean combination\nB of quantitative predicates of the form Pr\u03c3u (\u03c6i )\u2206p, where \u2206 \u2208 {\u2264, \u2265, <, >, =, 6=}, and\np \u2208 [0, 1], a multi-objective query asks whether there exists a strategy \u03c3 satisfying B (or\nwhether all strategies \u03c3 satisfy B). It turns out that such queries are not really much more\nexpressive than checking achievability. Namely, checking a fixed query B can be reduced to\nchecking a fixed number of extended achievability queries, where for some of the coordinates\nt\u03c3i we can ask for a strict inequality, i.e., that t\u03c3i > ri . (In general, however, the number\nand size of the extended achievability queries needed may be exponential in the size of B.)\nA motivation for allowing general multi-objective queries is to enable assume-guarantee\ncompositional reasoning for probabilistic systems, as explained in Section 2.\nWhereas our algorithms for quantitative problems use LP methods, we also consider\nqualitative multi-objective queries. These are queries given by boolean combinations of\npredicates of the form Pr\u03c3u (\u03c6i )\u2206b, where b \u2208 {0, 1}. We give an algorithm using purely\ngraph-theoretic techniques that decides whether there is a strategy that satisfies a qualitative multi-objective query, and if so produces such a strategy. The algorithm runs in time\npolynomial in the size of the MDP. Even for satisfying qualitative queries the strategy may\nneed to use both randomization and memory.\nIn typical applications, the MDP is far larger than the size of the query. Also, \u03c9regular properties can be presented in many ways, and it was already shown in [CY95]\nthat the query complexity of model checking MDPs against even a single LTL property is\n2EXPTIME-complete. We remark here that, if properties are expressed via LTL formulas,\nthen our algorithms run in polynomial time in the size of the MDP and in 2EXPTIME in\nthe size of the query, for deciding arbitrary multi-objective queries, where both the MDP\nand the query are part of the input. So, the worst-case upper bound is the same as with\na single LTL objective. However, to keep our complexity analysis simple, we focus in this\n\n\f4\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\npaper on the model complexity of our algorithms, rather than their query complexity or\ncombined complexity.\nRelated work. Model checking of MDPs with a single \u03c9-regular objective has been studied\nin detail (see [CY98, CY95, Var85]). In [CY98], Courcoubetis and Yannakakis also considered MDPs with a single objective given by a positive weighted sum of the probabilities of\nmultiple \u03c9-regular properties, and they showed how to efficiently optimize such objectives\nfor MDPs. They did not consider tradeoffs between multiple \u03c9-regular objectives. We\nemploy and build on techniques developed in [CY98].\nMulti-objective optimization is a subject of intensive study in Operations Research\nand related fields (see, e.g., [Ehr05, Cl\u0131\u030197]). Approximating the Pareto curve for general\nmulti-objective optimization problems was considered by Papadimitriou and Yannakakis in\n[PY00]. Among other results, [PY00] showed that for multi-objective linear programming\n(i.e., linear constraints and multiple linear objectives), one can compute a (polynomial sized)\n\u01eb-approximate Pareto curve in time polynomial in the size of the LP and in 1/\u01eb.\nOur work is related to recent work by Chatterjee, Majumdar, and Henzinger ([CMH06]),\nwho considered MDPs with multiple discounted reward objectives. They showed that randomized but memoryless strategies suffice for obtaining any achievable value vector for these\nobjectives, and they reduced the multi-objective optimization and achievability (what they\ncall Pareto realizability) problems for MDPs with discounted rewards to multi-objective\nlinear programming. They were thus able to apply the results of [PY00] in order to approximate the Pareto curve for this problem. We work in an undiscounted setting, where\nobjectives can be arbitrary \u03c9-regular properties. In our setting, strategies may require both\nrandomization and memory in order to achieve a given value vector. As described earlier,\nour algorithms first reduce multi-objective \u03c9-regular problems to multi-objective reachability problems, and we then solve multi-objective reachability problems by reducing them\nto multi-objective LP. For multi-objective reachabilility, we show randomized memoryless\nstrategies do suffice. Our LP methods for multi-objective reachability are closely related\nto the LP methods used in [CMH06] (and see also, e.g., [Put94], Theorem 6.9.1., where a\nrelated result about discounted MDPs is established). However, in order to establish the\nresults in our undiscounted setting, even for reachability we have to overcome some new\nobstacles that do not arise in the discounted case. In particular, whereas the \"discounted\nfrequencies\" used in [CMH06] are always well-defined finite values under all strategies, the\nanalogous undiscounted frequencies or \"expected number of visits\" can in general be infinite\nfor an arbitrary strategy. This forces us to preprocess the MDPs in such a way that ensures\nthat a certain family of undiscounted stochastic flow equations has a finite solution which\ncorresponds to the \"expected number of visits\" at each state-action pair under a given\n(memoryless) strategy. It also forces us to give a quite different proof that memoryless\nstrategies suffice to achieve any achievable vector for multi-objective reachability, based on\nthe convexity of the memorylessly achievable set.\nMulti-objective MDPs have also been studied extensively in the OR and stochastic\ncontrol literature (see e.g. [Fur80, Whi82, Hen83, Gho90, WT98]). Much of this work is\ntypically concerned with discounted reward or long-run average reward models, and does not\nfocus on the complexity of algorithms. None of this work seems to directly imply even our\nresult that for multiple reachability objectives checking achievability of a value vector can\nbe decided in polynomial time, not to mention the more general results for multi-objective\nmodel checking.\n\n\fMULTI-OBJECTIVE MODEL CHECKING OF\n\nMARKOV DECISION PROCESSES \u2217\n\n5\n\n2. Basics and background\nA finite-state MDP M = (V, \u0393, \u03b4) consists of a finite set V of states, an action alphabet\n\u0393, and a transition relation \u03b4. Associated with each state v is a set of enabled actions\n\u0393v \u2286 \u0393. The transition relation is given by \u03b4 \u2286 V \u00d7 \u0393 \u00d7 [0, 1] \u00d7 V . For each state\nv \u2208 V , each enabled action \u03b3 \u2208 \u0393v , and every state v \u2032 \u2208 V , we have at most\nP one transition\n\u2032\n(v, \u03b3, p(v,\u03b3,v\u2032 ) , v ) \u2208 \u03b4, for some probability p(v,\u03b3,v\u2032 ) \u2208 (0, 1], such that v\u2032 \u2208V p(v,\u03b3,v\u2032 ) = 1.\nWhen there is no transition (v, \u03b3, p(v,\u03b3,v\u2032 ) , v \u2032 ), we may, only for notational convenience,\nsometimes assume that there is a probability 0 transition, i.e., that p(v,\u03b3,v\u2032 ) = 0. (But\nsuch redundant probability 0 transitions are not part of the actual input.) Thus, at each\nstate, each enabled action determines a probability distribution on the next state. There\nare no other transitions, so no transitions on disabled actions. We assume every state v has\nsome enabled action, i.e., \u0393v 6= \u2205, so there are no dead ends. For our complexity analysis,\nwe assume of course that all probabilities p(v,\u03b3,v\u2032 ) are rational. There are other ways to\npresent MDPs, e.g., by separating controlled and probabilistic nodes into distinct states.\nThe different presentations are equivalent and efficiently translatable to each other.\nA labeled MDP M = (V, \u0393, \u03b4, l) has, in addition, a set of propositional predicates\nQ = {Q1 , . . . , Qr } which label the states. We view this as being given by a labelling\nfunction l : V 7\u2192 \u03a3, where \u03a3 = 2Q . We define the encoding size of a (labeled) MDP M ,\ndenoted by |M |, to be the total size required to encode all transitions and their rational\nprobabilities, where rational values are encoded with numerator and denominator given in\nbinary, as well as all state labels.\nFor a labeled MDP M = (V, \u0393, \u03b4, l) with a given initial state u \u2208 V , which we denote\nby Mu , runs of Mu are infinite sequences of states \u03c0 = \u03c00 \u03c01 . . . \u2208 V \u03c9 , where \u03c00 = u and\nfor all i \u2265 0, \u03c0i \u2208 V and there is a transition (\u03c0i , \u03b3, p, \u03c0i+1 ) \u2208 \u03b4, for some \u03b3 \u2208 \u0393\u03c0i and some\n.\nprobability p > 0. Each run induces an \u03c9-word over \u03a3, namely l(\u03c0) = l(\u03c00 )l(\u03c01 ) . . . \u2208 \u03a3\u03c9 .\nA strategy is a function \u03c3 : (V \u0393)\u2217 V 7\u2192 D(\u0393), which maps a finite history of play to\na probability distribution on the next action. Here D(\u0393) denotes the set of probability\ndistributions on the set \u0393. Moreover, it must be the case that for all histories wu, \u03c3(wu) \u2208\nD(\u0393u ), i.e., the probability distribution has support only over the actions available at state\nu. A strategy is pure if \u03c3(wu) has support on exactly one action, i.e., with probability\n1 a single action is played at every history. A strategy is memoryless (stationary) if the\nstrategy depends only on the last state, i.e., if \u03c3(wu) = \u03c3(w\u2032 u) for all w, w\u2032 \u2208 (V \u0393)\u2217 . If\n\u03c3 is memoryless, we can simply define it as a function \u03c3 : V 7\u2192 D(\u0393). An MDP M with\ninitial state u, together with a strategy \u03c3, naturally induces a Markov chain Mu\u03c3 , whose\nstates are the histories of play in Mu , and such that from state s = wv if \u03b3 \u2208 \u0393v , there is\na transition to state s\u2032 = wv\u03b3v \u2032 with probability \u03c3(wv)(\u03b3) * p(v,\u03b3,v\u2032 ) . A run \u03b8 in Mu\u03c3 is thus\ngiven by a sequence \u03b8 = \u03b80 \u03b81 . . ., where \u03b80 = u and each \u03b8i \u2208 (V \u0393)\u2217 V , for all i \u2265 0. We\nassociate to each history \u03b8i = wv the label of its last state v. In other words, we overload\n.\nthe notation and define l(wv) = l(v). We likewise associate with each run \u03b8 the \u03c9-word\n.\nl(\u03b8) = l(\u03b80 )l(\u03b81 ) . . .. Suppose we are given \u03c6, an LTL formula or B\u00fcchi automaton, or any\nother formalism for expressing an \u03c9-regular language over alphabet \u03a3. Let L(\u03c6) \u2286 \u03a3\u03c9\ndenote the language expressed by \u03c6. We write Pr\u03c3u (\u03c6) to denote the probability that a\ntrajectory \u03b8 of Mu\u03c3 satistifies \u03c6, i.e., that l(\u03b8) \u2208 L(\u03c6). For generality, rather than just\nallowing an initial vertex u we allow an initial probability distribution \u03b1 \u2208 D(V ). Let\nPr\u03c3\u03b1 (\u03c6) denote the probability that under strategy \u03c3, starting with initial distribution \u03b1,\n\n\f6\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\nwe will satisfy \u03c9-regular property \u03c6. These probabilities are well defined because the set of\nsuch runs is Borel measurable (see, e.g., [Var85, CY95]).\nAs in the introduction, for a k-tuple of \u03c9-regular properties \u03a6 = h\u03c61 , . . . , \u03c6k i, given\na strategy \u03c3, we let t\u03c3 = (t\u03c31 , . . . , t\u03c3k ), with t\u03c3i = Pr\u03c3u (\u03c6i ), for i = 1, . . . , k. For MDP M\nand starting state u, we define the achievable set of value vectors with respect to \u03a6 to\nbe UMu ,\u03a6 = {r \u2208 Rk\u22650 | \u2203\u03c3 such that t\u03c3 \u2265 r}. For a set U \u2286 Rk , we define a subset\nP \u2286 U of it, called the Pareto curve or the Pareto set of U , consisting of the set of Pareto\noptimal (or Pareto efficient) vectors inside U . A vector v \u2208 U is called Pareto optimal\nif \u00ac\u2203v \u2032 (v \u2032 \u2208 U \u2227 v \u2264 v \u2032 \u2227 v 6= v \u2032 ). Thus P = {v \u2208 U | v is Pareto optimal}. We use\nPMu ,\u03a6 \u2286 UMu ,\u03a6 to denote the Pareto curve of UMu ,\u03a6 .\nIt is clear, e.g., from Figure 1, that the Pareto curve is in general an infinite set. In fact,\nit follows from our results that for general \u03c9-regular objectives the Pareto set is a convex\npolyhedral set. In principle, we may want to compute some kind of exact representation of\nthis set by, e.g., enumerating all the vertices (on the upper envelope) of the polytope that\ndefines the Pareto curve, or enumerating the facets that define it. It is not possible to do\nthis in polynomial-time in general. In fact, the following theorem holds:\nTheorem 2.1. There is a family of MDPs, hM (n) | n \u2208 Ni, where M (n) has n states and\nsize O(n), such that for M (n) the Pareto curve for two reachability objectives, 3P1 and\n3P2 , contains n\u03a9(log n) vertices (and thus n\u03a9(log n) facets).\nProof. We will adapt and build on a known construction for the bi-objective shortest path\nproblem which shows that the Pareto curve for that problem can have n\u03a9(log n) vertices.\nThis was shown in [Car83] and a simplified proof (using a similar construction) was given in\n[MS01]. (The constructions and theorems there are phrased in terms of parametric shortest\npaths, but these are equivalent to bi-objective shortest paths.) What those constructions\nshow is that, for some polynomial f , and for every n, there is a graph Gn with f (n) nodes\nand distinguished nodes s and t, and such that every edge (u, v) has two (positive) costs\nc(u, v) and d(u, v), which yield two cost functions c(*) and d(*) on the s-t paths, such that\nthe Pareto curve of the s-t paths under the two objectives has n\u03a9(log n) vertices (and edges).\nAn important property of the constructed graphs Gn is that they are acyclic and layered,\nthat is, the nodes are arranged in layers L0 = s, L1 , L2 , . . . , Ln = t, and all edges are from\nlayer Li to Li+1 for some i \u2208 {0, . . . , n \u2212 1}.\nBuilding on this construction, we now construct the following instance Mn of the MDP\nproblem with two reachability objectives. The states of Mn are the same as Gn with 2 extra\nabsorbing states: the red state R, and the blue state B, which are the two target states of\nour two reachability objectives. For each state u there is one action for each outgoing edge\n(u, v); if we choose this action then we transition with probability r(u, v) to state R, with\nprobability b(u, v) to B, with probability 1/2 to v, and with the remaining probability to\nt. The probabilities r(u, v) and b(u, v) are defined as follows. Let h be the maximum c or\nd cost over all the edges. For an edge (u, v) where u \u2208 Li (and v \u2208 Li+1 ), set\nr(u, v) :=\nand\n\n2i (2h \u2212 c(u, v))\n8h2n\n\n2i (2h \u2212 d(u, v))\n8h2n\nNote that both these quantities are in the interval [0, 1/4], so all probabilities are welldefined.\nb(u, v) :=\n\n\fMARKOV DECISION PROCESSES \u2217\n\nMULTI-OBJECTIVE MODEL CHECKING OF\n\n7\n\nThe claim is that there is a 1-1 correspondence between the vertices of the Pareto curve\nof this MDP Mn and the Pareto curve of the bi-objective shortest path on Gn . First we note\nthat the vertices of the Pareto curve for the MDP correspond to pure memoryless strategies\n(meaning that for each vertex of the Pareto curve a pure memoryless strategy can achieve\nthe value vector that the vertex defines). The reason for this is that the vertices are optima\nfor a linear combination of the two objectives, and it follows from the proof of Theorem 3.2,\nwhich we shall show later, that these objectives have pure memoryless optimal strategies.\nA pure strategy corresponds to a path from s to t. Let \u03c0 = s, u1 , u2 , ..., un\u22121 t be such a\npath/strategy. The probability that this strategy leads to the red node R is r(s, u1 ) + . . . +\nProb(reach node ui ) \u2217 r(ui , ui + 1) + . . . The probability that the process reaches node ui\nunder the strategy \u03c0 is 1/2i , independent of the path. Thus, Prob \u03c0 (reach R) = a \u2212 b \u2217 c(\u03c0),\nwhere a, b are constants independent of the path. Similarly, Prob \u03c0 (reach B) = a \u2212 b \u2217 d(\u03c0).\nIt follows that minimizing the c and d costs of the paths is equivalent to maximizing the\nprobabilities of reaching R and B, and this also holds for any positive linear combination of\nthe two respective objectives. Thus, there is a correspondence between their Pareto curves.\nSo, the Pareto curve is in general a polyhedral surface of superpolynomial size, and\nthus cannot be constructed exactly in polynomial time. We show, however, that the Pareto\nset can be efficiently approximated to any desired accuracy \u01eb > 0. An \u01eb-approximate Pareto\ncurve, PMu ,\u03a6 (\u01eb) \u2286 UMu ,\u03a6 , is any achievable set such that \u2200r \u2208 UMu ,\u03a6 \u2203t \u2208 PMu ,\u03a6 (\u01eb) such\nthat r \u2264 (1 + \u01eb)t. When the subscripts Mu and \u03a6 are clear from the context, we will drop\nthem and use U , P, and P(\u01eb) to denote the achievable set, Pareto set, and \u01eb-approximate\nPareto set, respectively.\nWe also consider general multi-objective queries. A quantitative predicate over \u03c9-regular\nproperty \u03c6i is a statement of the form Pr\u03c3u (\u03c6i )\u2206p, for some rational probability p \u2208 [0, 1],\nand where \u2206 is a comparison operator \u2206 \u2208 {\u2264, \u2265, <, >, =}. Suppose B is a boolean\ncombination over such predicates. Then, given M and u, and B, we can ask whether there\nexists a strategy \u03c3 such that B holds, or whether B holds for all \u03c3. Note that since B can\nbe put in DNF form, and the quantification over strategies pushed into the disjunction, and\nsince \u03c9-regular languages are closed under complementation, any query of the form \u2203\u03c3B (or\nof the form \u2200\u03c3B) can be transformed to a disjunction (a negated disjunction, respectively)\nof queries of the form:\n^\n^\n\u2203\u03c3\n(Pr\u03c3u (\u03c6i ) \u2265 ri ) \u2227 (Pr\u03c3u (\u03c8j ) > rj\u2032 )\n(2.1)\ni\n\nj\n\nWe call queries of the form (1) extended achievability queries. Thus, if the multiobjective query is fixed, it suffices to perform a fixed number of extended achievability\nqueries to decide any multi-objective query. Note, however, that the number of extended\nachievability queries we need could be exponential in the size of B. We do not focus on\noptimizing query complexity in this paper.\nA motivation for allowing general multi-objective queries is to enable assume-guarantee\ncompositional reasoning for probabilistic systems. Consider, e.g., a probabilistic system\nconsisting of the concurrent composition of two components, M1 and M2 , where output\nfrom M1 provides input to M2 and thus controls M2 . We denote this by M1 \u0003 M2 . M2\nitself may generate outputs for some external device, and M1 may also be controlled by\nexternal inputs. (One can also consider symmetric composition, where outputs from both\n\n\f8\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\na\nu\n\n1\n\nb\n\n1\nP1\n\nP2\n\nFigure 2: The MDP M \u2032 .\ncomponents provide inputs to both. Here, for simplicity, we restrict ourselves to asymmetric\ncomposition where M1 controls M2 .) Let M be an MDP with separate input and output\naction alphabets \u03a31 and \u03a32 , and let \u03c61 and \u03c62 denote \u03c9-regular properties over these two\nalphabets, respectively. We write h\u03c61 i\u2265r1 M h\u03c62 i\u2265r2 , to denote the assertion that \"if the\ninput controller of M satisfies \u03c61 with probability \u2265 r1 , then the output generated by M\nsatisfies \u03c62 with probability \u2265 r2 \". Using this, we can formulate a general compositional\nassume-guarantee proof rule:\nh\u03c61 i\u2265r1 M1 h\u03c62 i\u2265r2\nh\u03c62 i\u2265r2 M2 h\u03c63 i\u2265r3\n------------\nh\u03c61 i\u2265r1 M1 \u0003 M2 h\u03c63 i\u2265r3\nThus, to check h\u03c61 i\u2265r1 M1 \u0003M2 h\u03c63 i\u2265r3 it suffices to check two properties of smaller systems:\nh\u03c61 i\u2265r1 M1 h\u03c62 i\u2265r2 and h\u03c62 i\u2265r2 M2 h\u03c63 i\u2265r3 . Note that checking h\u03c61 i\u2265r1 M h\u03c62 i\u2265r2 amounts\nto checking that there does not exist a strategy \u03c3 controlling M such that Pr\u03c3u (\u03c61 ) \u2265 r1\nand Pr\u03c3u (\u03c62 ) < r2 .\nWe also consider qualitative multi-objective queries. These are queries restricted so that\nB contains only qualitative predicates of the form Pr\u03c3u (\u03c6i )\u2206b, where b \u2208 {0, 1}. These can,\ne.g., be used to check qualitative assume-guarantee conditions of the form: h\u03c61 i\u22651 M h\u03c62 i\u22651 .\nIt is not hard to see that again, via boolean manipulations and complementation of automata, we can convert any qualitative query to a number of queries of the form:\n^\n^\n(Pr\u03c3u (\u03c6) = 1) \u2227\n\u2203\u03c3\n(Pr\u03c3u (\u03c8) > 0)\n\u03c6\u2208\u03a6\n\n\u03c8\u2208\u03a8\n\nwhere \u03a6 and \u03a8 are sets of \u03c9-regular properties. It thus suffices to consider only these\nqualitative queries.\nIn the next sections we study how to decide various classes of multi-objective queries,\nand how to approximate the Pareto curve for properties \u03a6. Let us observe here a difficulty\nthat we will have to deal with. Namely, in general we will need both randomization and\nmemory in our strategies in order to satisfy even simple qualitative multi-objective queries.\nConsider the MDP, M \u2032 , shown in Figure 2, and consider the conjunctive query: B \u2261\nPr\u03c3u (23P1 ) > 0 \u2227 Pr\u03c3u (23P2 ) > 0. It is not hard to see that starting at state u in M \u2032\nany strategy \u03c3 that satisfies B must use both memory and randomization. Each predicate\nin B can be satisfied in isolation (in fact with probability 1), but, with a memoryless or\ndeterministic strategy, if we try to satisfy 23P2 with non-zero probability, we will be forced\nto satisfy 23P1 with probability 0. Note, however, that we can satisfy both with probability\n> 0 using a strategy that uses both memory and randomness: namely, upon reaching the\nstate labeled P1 for the first time, with probability 1/2 we use move a and with probability\n1/2 we use move b. Thereafter, upon encountering the state labeled P1 for the nth time,\nn \u2265 2, we deterministically pick action a. This clearly assures that both predicates are\nsatisfied with probability = 1/2 > 0.\n\n\fMARKOV DECISION PROCESSES \u2217\n\nMULTI-OBJECTIVE MODEL CHECKING OF\n\n9\n\nWe note that our results (combined with the earlier results of [CY98]) imply that\nfor general multi-objective queries a randomized strategy with a finite amount of memory\n(which depends on the MDP and query) does suffice to satisfy any satisfiable quantitative\nmulti-objective \u03c9-regular query.\n3. Multi-objective reachability\nIn this section, as a step towards quantitative multi-objective model checking problems,\nwe study a simpler multi-objective reachability problem. Specifically, we are given an MDP,\nM = (V, \u0393, \u03b4), a starting state u, and a collection of target sets Fi \u2286 V , i = 1, . . . , k. The\nsets Fi may overlap. We have k objectives: the i-th objective is to maximize the probability\nS\nof 3Fi , i.e., of reaching some state in Fi . We assume that the states F = ki=1 Fi are all\nabsorbing states with a self-loop. In other words, for all v \u2208 F , (v, a, 1, v) \u2208 \u03b4 and \u0393v = {a}.\n(The assumption that target states are absorbing is necessary for the proofs in this section,\nbut it is not a restriction in general for our results. It will follow from the model checking\nresults in Section 5, which build on this section, that multi-objective reachability problems\nfor arbitrary target states (whether absorbing or not) can also be handled with the same\ncomplexities.)\nWe first need to do some preprocessing on the MDP, to remove some useless states.\nFor each state v \u2208 V \\ F we can check easily whether there exists a strategy \u03c3 such that\nP rv\u03c3 (3F ) > 0: this just amounts to checking whether there exists a path from v to F in\nthe underlying graph of the MDP, i.e., the graph given by considering only the non-zeroprobability transitions. Let us call a state that does not satisfy this property a bad state.\nClearly, for the purposes of optimizing reachability objectives, we can compute and remove\nall bad states from an MDP. Thus, it is safe to assume that bad states do not exist.1 Let\nus call an MDP with goal states F cleaned-up if it does not contain any bad states.\nProposition 3.1. For a cleaned-up MDP, an initial distribution \u03b1 \u2208 D(V \\ F ), and a\nvector of probabilities r \u2208 [0, 1]k , there exists a (memoryless) strategy \u03c3 such that\nk\n^\n\nPr\u03c3\u03b1 (3Fi ) \u2265 ri\n\ni=1\n\nif and only if there exists a (respectively, memoryless) strategy \u03c3 \u2032 such that\nk\n^\n\ni=1\n\n\u2032\n\nPr\u03c3\u03b1 (3Fi ) \u2265 ri \u2227\n\n^\n\n\u2032\n\nPr\u03c3v (3F ) > 0 .\n\nv\u2208V\n\nProof. This is quite obvious, but we give a quick argument anyway. Suppose we have such\na strategy \u03c3. Since the MDP is cleaned-up, we know that from every state in V we can\nreach F with a positive probability. Suppose the strategy leads to a history whose last state\nis v \u2208 V \\ F , and that thereafter the strategy is such that it will never reach F on any path.\nWe simply revise \u03c3 to a strategy \u03c3 \u2032 such that, if we ever arrive at such a \"dead\" history, we\n1Technically, we would need to install a new \"dead\" absorbing state v\ndead 6\u2208 F , such that all the proba-\n\nbilities going into states that have been removed now go to vdead . For convenience in notation, instead of\nexplicitlyP\nadding vdead we treat it as implicit: we allow that for some states v \u2208 V and some action a \u2208 \u0393v\nwe have v\u2032 \u2208V p(v,\u03b3,v\u2032 ) < 1, and we implicitly assume\nP that there is an \"invisible\" transition to vdead with\nthe residual probability, i.e., with p(v,\u03b3,vdead ) = 1 \u2212 v\u2032 \u2208V p(v,\u03b3,v\u2032 ) . Of course, vdead would then be a \"bad\"\nstate, but we can ignore this implicit state.\n\n\f10\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\nObjectives (i = 1, . . . , k):\n\nMaximize\n\nSubject to:\nP\nP\ny\n\u2212 v\u2032 \u2208V \u03b3 \u2032 \u2208\u0393 \u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\u2032 ,\u03b3 \u2032 )\n\u03b3\u2208\u0393P\nv (v,\u03b3)\nv\nP\nyv \u2212 v\u2032 \u2208V \\F \u03b3 \u2032 \u2208\u0393 \u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\u2032 ,\u03b3 \u2032 )\nv\nyv\ny(v,\u03b3)\nP\n\nP\n\nv\u2208Fi\n\n=\n=\n\u2265\n\u2265\n\n\u03b1(v)\n0\n0\n0\n\nyv ;\nfor\nfor\nfor\nfor\n\nall\nall\nall\nall\n\nv\nv\nv\nv\n\n\u2208 V \\ F;\n\u2208 F;\n\u2208 F;\n\u2208 V \\ F and \u03b3 \u2208 \u0393u .\n\nFigure 3: Multi-objective LP for the multi-objective MDP reachability problem\nswitch and play according to the memoryless strategy starting at v which reaches F with\nsome positive probability. Note that if \u03c3 is memoryless then so is \u03c3 \u2032 .\nNow, consider the multi-objective LP described in Figure 3.2 The set of variables in this\nLP are as follows: for each v \u2208 F , there is a variable yv , and for each v \u2208 V \\ F and each\n\u03b3 \u2208 \u0393v there is a variable y(v,\u03b3) .\nTheorem 3.2. Suppose we are given a cleaned-up MDP, M = (V, \u0393, \u03b4), with multiple target\nS\nsets Fi \u2286 V , i = 1, . . . , k, where every target v \u2208 F = ki=1 Fi is an absorbing state. Let\n\u03b1 \u2208 D(V \\ F ) be an initial distribution (in particular V \\ F 6= \u2205). Let r \u2208 (0, 1]k be a vector\nof positive probabilities. Then the following are all equivalent:\n(1.) There is a (possibly randomized) memoryless strategy \u03c3 such that\nk\n^\n\n(Pr\u03c3\u03b1 (3Fi ) \u2265 ri )\n\ni=1\n\n(2.) There is a feasible solution\n\ny\u2032\n\nfor the multi-objective LP in Fig. 3 such that\n\uf8eb\n\uf8f6\nk\n^\nX\n\uf8ed\nyv\u2032 \u2265 ri \uf8f8\ni=1\n\nv\u2208Fi\n\n(3.) There is an arbitrary strategy \u03c3 such that\nk\n^\n\n(Pr\u03c3\u03b1 (3Fi ) \u2265 ri )\n\ni=1\n\nProof.\n(1.) \u21d2 (2.). Since the MDP is cleaned up, by Proposition 3.1 we can assume there is a\nV\nmemoryless strategy \u03c3 such that ki=1 Pr\u03c3\u03b1 (3Fi ) \u2265 ri and \u2200v \u2208 V P rv\u03c3 (3F ) > 0. Consider\nthe square matrix P \u03c3 whose size is |V \\F |\u00d7|V \\F |, and whose rows and columns are indexed\n\u03c3 , is the probability that starting in state\nby states in V \\ F . The (v, v \u2032 )'th entry of P \u03c3 , Pv,v\n\u2032\nP\n\u2032\n\u03c3 =\nv we shall in one step end up in state v . In other words, Pv,v\n\u2032\n\u03b3\u2208\u0393v \u03c3(v)(\u03b3) * pv,\u03b3,v\u2032 .\nP\nP\u221e\n\u2032\n\u2032\n\u2032\n\u03c3\nn\nFor all v \u2208 V \\ F , let y(v,\u03b3) = v\u2032 \u2208V \\F \u03b1(v ) n=0 (P )v\u2032 ,v \u03c3(v)(\u03b3). In other words y(v,\u03b3)\ndenotes the \"expected number of times that, using the strategy \u03c3, starting in the distribution\n\u03b1, we will visit the state v and upon doing so choose action \u03b3\". We don't know yet that these\n2We mention without further elaboration that this LP can be derived, using complementary slackness,\nfrom the dual LP of the standard LP for single-objective reachability obtained from Bellman's optimality\nequations, whose variables are xv , for v \u2208 V , and whose unique optimal solution is the vector x\u2217 with\nx\u2217v = max\u03c3 Pr\u03c3v (3F ) (see, e.g., [Put94, CY98]).\n\n\fMARKOV DECISION PROCESSES \u2217\n\nMULTI-OBJECTIVE MODEL CHECKING OF\n\nare finite values, but assuming they are, for v \u2208 F , let yv\u2032 =\nThis completes the definition of the entire vector y \u2032 .\n\nP\n\nv\u2032 \u2208V \\F\n\nP\n\n\u03b3 \u2032 \u2208\u0393v\u2032\n\n11\n\n\u2032\np(v\u2032 ,\u03b3 \u2032 ,v) y(v\n\u2032 ,\u03b3 \u2032 ) .\n\n\u2032\nLemma 3.3. The vector y \u2032 is well defined (i.e., all entries y(v,\u03b3)\nare finite).\n\u2032\nMoreover, y is a feasible solution to the constraints of the LP in Figure 3.\n\u2032\nProof. First, we show that for all v \u2208 V \\ F and \u03b3 \u2208 \u0393v , y(v,\u03b3)\nis a well defined finite value.\n\u2032\n\u2032\nIt then also follows from the definition of yv that yv is also finite and thus that the vector\ny \u2032 is well defined. Note that because \u03c3 has the property that \u2200v \u2208 V P rv\u03c3 (3F ) > 0, P \u03c3 is\nclearly a substochastic matrix with the property that, for some power d \u2265 1, all of the row\n\u03c3 )n \u2192 0, and thus\nsums of (P \u03c3 )d are strictly less than 1. Thus, it follows that limn\u2192\u221e\nP(P\n\u221e\n\u03c3\n\u22121\nby standard facts about matrices the inverse matrix (I \u2212 P ) = n=0 (P \u03c3 )n exists and\nis non-negative. Now observe that\n\u221e\nX\nX\n\u2032\n\u2032\n(P \u03c3 )nv\u2032 ,v \u03c3(v)(\u03b3)\ny(v,\u03b3) =\n\u03b1(v )\nv\u2032 \u2208V \\F\n\n=\n\nX\n\nn=0\n\n\u03b1(v \u2032 )\u03c3(v)(\u03b3)\n\nv\u2032 \u2208V \\F\n\n\u221e\nX\n\n(P \u03c3 )nv\u2032 ,v\n\nn=0\n\n= \u03c3(v)(\u03b3)\n\nX\n\n\u03b1(v )(I \u2212 P \u03c3 )\u22121\nv\u2032 ,v\n\u2032\n\nv\u2032 \u2208V \\F\n\nNext, we show that y \u2032 is a feasible solution to the constraints\nmulti-objective LP in\nPin theP\n\u2032\nFigure 3. Note that, for each state v \u2208 V \\ F , the expression v\u2032 \u2208V \u03b3 \u2032 \u2208\u0393 \u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\n\u2032 ,\u03b3 \u2032 )\nv\nis precisely the \"expected number of times we will take a transition\ninto\nthe\nstate\nv\"\nif\nP\n\u2032\ndefines prewe start at initial distribution \u03b1 and using strategy \u03c3, whereas \u03b3\u2208\u0393v y(v,\u03b3)\ncisely the \"expected number of times we will take a transition out of the state\nP v\". \u2032 Thus\n\u03b1(v), the probability that we will start in state v, is precisely given by\n\u03b3\u2208\u0393v y(v,\u03b3) \u2212\nP\nP\n\u2032\nv\u2032 \u2208V\n\u03b3 \u2032 \u2208\u0393 \u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\u2032 ,\u03b3 \u2032 ) = \u03b1(v). More formally, for each state v \u2208 V \\ F :\nv\n\nX X\n\n\u2032\np(v\u2032 ,\u03b3 \u2032 ,v) y(v\n=\n\u2032 ,\u03b3 \u2032 )\n\nv\u2032 \u2208V \u03b3 \u2032 \u2208\u0393v\u2032\n\nX X\n\np(v\u2032 ,\u03b3 \u2032 ,v)\n\nv\u2032 \u2208V \u03b3 \u2032 \u2208\u0393v\u2032\n\n=\n\nX\n\n\u03b1(v \u2032\u2032 )\n\nX\n\n\u03b1(v \u2032\u2032 )\n\nv\u2032\u2032 \u2208V \\F\n\n\u03b1(v \u2032\u2032 )\n\nv\u2032\u2032 \u2208V \\F\n\nX X\n\np(v\u2032 ,\u03b3 \u2032 ,v)\n\nv\u2032 \u2208V \u03b3 \u2032 \u2208\u0393v\u2032\n\nv\u2032\u2032 \u2208V \\F\n\n=\n\nX\n\n\u221e\nX\n\n\u221e\nX\n\n(P \u03c3 )nv\u2032\u2032 ,v\u2032 \u03c3(v \u2032 )(\u03b3 \u2032 )\n\n\u221e\nX\n\n(P \u03c3 )nv\u2032\u2032 ,v\u2032 \u03c3(v \u2032 )(\u03b3 \u2032 )\n\nn=0\n\nn=0\n\n(P \u03c3 )nv\u2032\u2032 ,v\n\nn=1\n\nThe last expression is easily seen to be the expected number\nwe will transition\nP of times\n\u2032\ninto state v. It is clear by linearity of expectations that \u03b3\u2208\u0393v y(v,\u03b3) gives the expected\nP\n\u2032\nnumber of times we will transition out of state v. It is thus clear that\n\u03b3\u2208\u0393v y(v,\u03b3) \u2212\nP\nP\n\u2032\nv\u2032 \u2208V\n\u03b3 \u2032 \u2208\u0393v\u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\u2032 ,\u03b3 \u2032 ) = \u03b1(v).\nP\n\u03c3\n\u2032\n\u2032\nNow we argue that\nv\u2208Fi yv = Pr\u03b1 (3Fi ). To see this, note that for v \u2208 F , yv =\nP\nP\n\u2032\nv\u2032 \u2208V \\F\n\u03b3 \u2032 \u2208\u0393v\u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\u2032 ,\u03b3 \u2032 ) is precisely the \"expected number of times that we will transition into state v for the first time\", starting at distribution \u03b1. The reason we can say \"for\n\n\f12\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\nthe first time\" is because only the states in V \\ F are included in the matrix P \u03c3 . But note\nthat this italicised statement in quotes is another way to define the probability of eventually\nreaching state v. This P\nequality can be establish formally, but we omit the formal algebraic\nderivation here. Thus v\u2208Fi yv\u2032 = Pr\u03c3\u03b1 (3Fi ) \u2265 ri . We are done with (1.) \u21d2 (2.).\n(2.) \u21d2 (1.).\nwish to show that if y \u2032\u2032 is a feasible solution to the multi-objective LP\nP We now\n\u2032\u2032\nsuch that v\u2208Fi yv \u2265 ri > 0, for all i = 1, . . . , k, then there exists a memoryless strategy \u03c3\nV\nsuch that ki=1 Pr\u03c3\u03b1 (3Fi ) \u2265 ri .\nP\n\u2032\u2032\n> 0}. Let \u03c3 be\nSuppose we have such a solution y \u2032\u2032 . Let S = {v \u2208 V \\ F | \u03b3\u2208\u0393v y(v,\u03b3)\nthe memoryless strategy, given as follows. For each v \u2208 S\n\u2032\u2032\ny(v,\u03b3)\n\u03c3(v)(\u03b3) := P\n\u2032\u2032\n\u03b3 \u2032 \u2208\u0393v yv,\u03b3 \u2032\nP\n\u2032\u2032\nNote that since\n\u03b3\u2208\u0393v y(v,\u03b3) > 0, \u03c3(v) is a well-defined probability distribution on the\nmoves at state v \u2208 S. For the remaining states v \u2208 (V \\ F ) \\ S, let \u03c3(v) be an arbitrary\ndistribution in D(\u0393v ).\nV\nLemma 3.4. This memoryless strategy \u03c3 satisfies ki=1 P r\u03b1\u03c3 (3Fi ) \u2265 ri .\nProof. Let us assume, for the sake of convenience in our analysis, that there is an extra\ndead-end absorbing state vdead 6\u2208 F available, and an extra move \u03b3dead available at each\nstate, v, with p(v,\u03b3dead ,vdead ) = 1, and for each v \u2208 (V \\ F ) \\ S, instead of letting \u03c3(v) be\narbitrary, let \u03c3(v)(\u03b3dead ) = 1. In other words, from each such state we simply move directly\nto an absorbing dead-end which is outside of F . The assumption that such a dead-end\nexists is just for convenience: clearly, without such a dead-end, we can use any (mixed)\nmove at such vertices in our strategy, and such a strategy would yield at least as high a\nvalue for Pr\u03c3\u03b1 (3Fi ), for all i = 1, . . . , k.\nLet us now explain the reason why we don't care about what moves are used at\nstates outside S in the strategy \u03c3. Let support(\u03b1) = {v \u2208 V \\ F | \u03b1(v) > 0}. We\nclaim S contains all states reachable from support(\u03b1) using strategy \u03c3.PTo see this,\n\u2032\u2032\nfirst note that support(\u03b1) \u2286 S, because for all v \u2208 support(\u03b1), since\n\u03b3\u2208\u0393v y(v,\u03b3) \u2212\nP\nP\n\u2032\u2032\nand \u03b1(v) > 0, and since yv\u2032\u2032\u2032 ,\u03b3 \u2032 \u2265 0 for all v \u2032 \u2208 V \\F and\nv\u2032 \u2208V\n\u03b3 \u2032 \u2208\u0393v\u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\u2032 ,\u03b3 \u2032 ) = \u03b1(v)\nP\n\u2032\u2032\n> 0. Thus support(\u03b1) \u2286 S. Inductively, for\n\u03b3 \u2032 \u2208 \u0393v\u2032 , it must be the case that, \u03b3\u2208\u0393v y(v,\u03b3)\nk \u2265 0, consider any state v \u2208 V \\ F , such that we can, with non-zero probability, reach v in\nk steps using strategy \u03c3 from a state in support(\u03b1), and such that we can not reach v (with\nnon-zero probability) in any fewer than k step. For the base case k = 0, we already know\nv \u2208 support(\u03b1) \u2286 S. For k > 0, we must have \u03b1(v) = 0. But note that there must be a positive probability of moving to v in one step from some other state v \u2032 which can be reached in\nk \u22121 steps from support(\u03b1). But this is so if and only if for some \u03b3 \u2032 \u2208 \u0393v\u2032 , both p(v\u2032 ,\u03b3 \u2032 ,v) > 0\nP\nP\n\u2032\u2032\n\u2032\n\u2032\n\u2032\u2032\n\u2032 ,v) y \u2032 \u2032 > 0. Thus\nand y(v\n\u2032 \u2208V\n\u2032 ,\u03b3 \u2032 ) > 0 ( and thus \u03c3(v )(\u03b3 ) > 0). Hence,\nv\n\u03b3 \u2032 \u2208\u0393v\u2032 p(v\u2032 ,\u03b3\n(v ,\u03b3 )\nP\nP\nP\nP\n\u2032\u2032\n\u2032\u2032\n\u2032\u2032\n\u2212 v\u2032 \u2208V \u03b3 \u2032 \u2208\u0393 \u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\nsince \u03b3\u2208\u0393v y(v,\u03b3)\n\u2032 ,\u03b3 \u2032 ) = 0, we must have\n\u03b3\u2208\u0393v y(v,\u03b3) > 0, and\nv\nthus v \u2208 S. Hence S contains the set of nodes reachable from nodes in the support of the\ninitial distribution, support(\u03b1), using the strategy \u03c3.\nWe will now show that Pr\u03c3\u03b1 (3Fi ) \u2265 ri , for all i = 1, . . . , k. Let us consider the underlying\ngraph of the \"flows\" defined by y \u2032\u2032 . Namely, let G = (V, E) be a graph on states of M such\n\u2032\u2032\nthat (v, v \u2032 ) \u2208 E if and only if there is some \u03b3 \u2208 \u0393v such that y(v,\u03b3)\n> 0 and p(v,\u03b3,v\u2032 ) > 0. Let\n\n\fMARKOV DECISION PROCESSES \u2217\n\nMULTI-OBJECTIVE MODEL CHECKING OF\n\n13\n\nW \u2286 V \\ F be the set of vertices in V \\ F that have a non-zero \"flow\" to F , i.e., v is in W\niff there is a path in G from vPto some vertex in F .\n\u2032\u2032\n. Note that by the constraints of the LP, for any\nFor v \u2208 V \\ F , let zv = \u03b3\u2208\u0393v y(v,\u03b3)\nvertex v \u2208 S\nX X\n\u2032\u2032\n\u03b1(v) = zv \u2212\np(v\u2032 ,\u03b3,v) y(v\n\u2032 ,\u03b3)\nv\u2032 \u2208V \\F \u03b3\u2208\u0393v\u2032\n\n= zv \u2212\n\nX X\n\n\u2032\u2032\np(v\u2032 ,\u03b3,v) y(v\n\u2032 ,\u03b3)\n\nX X\n\n\u2032\u2032\np(v\u2032 ,\u03b3,v) y(v\n\u2032 ,\u03b3) P\n\nX X\n\np(v\u2032 ,\u03b3,v) \u03c3(v \u2032 )(\u03b3)zv\u2032\n\nv\u2032 \u2208S\n\n= zv \u2212\n\n\u03b3\u2208\u0393v\u2032\n\nv\u2032 \u2208S \u03b3\u2208\u0393v\u2032\n\n= zv \u2212\n\n(because all flow into v comes from S)\nzv\u2032\n\u2032\u2032\n\u03b3 \u2032 \u2208\u0393 \u2032 y(v\u2032 ,\u03b3 \u2032 )\nv\n\nv\u2032 \u2208S \u03b3\u2208\u0393v\u2032\n\n= zv \u2212\n\nX\n\nPv\u03c3\u2032 ,v zv\u2032\n\nv\u2032 \u2208S\n\nNow, let us focus on the vertices in W . Note that, by definition, W \u2286 S. Consider the\n\u03c3\nsubmatrix PW,W\nobtained from P \u03c3 by eliminating the rows and columns whose indices are\nnot in W . Note that since there is no flow into a vertexPin W from a vertex outside of W ,\nthe above equalities yield, for each v \u2208 W , \u03b1(v) = zv \u2212 v\u2208W Pv\u03c3\u2032 ,v zv\u2032 . This can be written\n\u03c3\nin matrix notation as \u03b1T |W = z T |W (I \u2212 PW,W\n).\nNow, note that since every vertex in W has a \"flow\" to F , in terms of the underlying\n\u03c3\nMarkov chain of the substochastic matrix PW,W\n, this means that every vertex in W is\n\u03c3\ntransient, and that there is a power d \u2265 1, such that (PW,W\n)d has the property that all\n\u03c3\nits row sums are strictly less than 1. Consequently, limd\u2192\u221e (PW,W\n)d = 0 and the matrix\nP\n\u221e\n\u03c3\n\u03c3\n\u03c3\n(I \u2212 PW,W\n) is invertible, with (I \u2212 PW,W\n)\u22121 = i=0 (PW,W\n)i ), a nonnegative matrix. Thus,\nP\u221e\nT\nT\n\u03c3\n\u22121\nT\n\u03c3\ni\nz |W = \u03b1 (I \u2212 PW,W ) = \u03b1 ( i=0 (PW,W ) . From this it follows, again because no vertex\noutside of W has a flow into W , that for each v \u2208 W :\n\u221e X\nX\nX\n(P \u03c3 )nv\u2032 ,v \u03c3(v)(\u03b3)\nzv =\n\u03b1(v \u2032 )\nv\u2032 \u2208V \\F\n\n=\n\nX X\n\n\u03b3\u2208\u0393v v\u2032 \u2208W\n\nn=0 \u03b3\u2208\u0393v\n\n\u03b1(v \u2032 )\n\n\u221e\nX\n\n(P \u03c3 )nv\u2032 ,v \u03c3(v)(\u03b3)\n\nn=0\n\n(because all moves into v of strategy \u03c3 come from vertices in W )\n=\n\nX\n\n\u2032\ny(v,\u03b3)\n\n\u03b3\u2208\u0393v\n\u2032\n\u2032\u2032\nwhere, in the last expression, the values y(v,\u03b3)\n, not to be mistaken with y(v,\u03b3)\n, are values\n\u2032\nfrom the vector y which we obtained in the proof that (1.) \u21d2 (2.), from a given memoryless\nstrategy \u03c3. In this case, the strategy \u03c3 in question is precisely the memoryless strategy we\njust defined based on y \u2032\u2032 . Thus, for all v \u2208 W :\nX\nX\n\u2032\n\u2032\u2032\ny(v,\u03b3)\n(3.1)\ny(v,\u03b3)\n=\nzv =\n\u03b3\u2208\u0393v\n\n\u03b3\u2208\u0393v\n\n\f14\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\n\u2032\u2032\n\u2032\nWe next show that in fact for all v \u2208 W and \u03b3 \u2208 \u0393v , y(v,\u03b3)\n= y(v,\u03b3)\n. For v \u2208 W and \u03b3 \u2208 \u0393v ,\nwe have:\n\u221e\nX\nX\n\u2032\n\u03c3\ny(v,\u03b3)\n=\n(PW,W\n)iv\u2032 ,v \u03c3(v)(\u03b3)\n\u03b1(v \u2032 )\nv\u2032 \u2208W\n\n=\n\nX\n\nv\u2032 \u2208W\n\n=\n\ni=0\n\n\u2032\u2032\n\u221e\nX\ny(v,\u03b3)\n\u03c3\ni\n\u2032\n(PW,W )v\u2032 ,v P\n\u03b1(v )\n\u2032\u2032\n\u03b3 \u2032 \u2208\u0393v y(v,\u03b3 \u2032 )\n\ni=0\n\u2032\u2032\nX\ny(v,\u03b3)\nP\n\u2032\u2032\n\u03b3 \u2032 \u2208\u0393v y(v,\u03b3 \u2032 ) v\u2032 \u2208W\n\n\u03b1(v \u2032 )\n\n\u221e\nX\n\n\u03c3\n(PW,W\n)iv\u2032 ,v\n\ni=0\n\nBut recall that P\nthe \"expectedPnumber of times we will transition out of state v\" is given by\nP\n\u221e\n\u2032\n\u03c3\ni\n\u2032\nv\u2032 \u2208W \u03b1(v )\ni=0 (PW,W )v\u2032 ,v .\n\u03b3\u2208\u0393v y(v,\u03b3) =\n\u2032\u2032\nP\ny\n\u2032\n\u2032\nHence y(v,\u03b3)\n= P \u2032 (v,\u03b3)\n\u03b3\u2208\u0393v y(v,\u03b3) . Thus, by using equation (3.1) and canceling,\ny \u2032\u2032\n\u03b3 \u2208\u0393v\n\n(v,\u03b3 \u2032 )\n\n\u2032\n\u2032\u2032\nwe get y(v,\u03b3)\n= y(v,\u03b3)\n. Thus, since y \u2032\u2032 is a feasible solution to the LP, we have that for any\nv \u2208 F:\nX X\n\u2032\u2032\nyv\u2032\u2032 =\np(v\u2032 ,\u03b3 \u2032 ,v) y(v\n\u2032 ,\u03b3 \u2032 )\nv\u2032 \u2208V \\F \u03b3 \u2032 \u2208\u0393v\u2032\n\n=\n\nX X\n\n\u2032\u2032\np(v\u2032 ,\u03b3 \u2032 ,v) y(v\n\u2032 ,\u03b3 \u2032 )\n\nX X\n\n\u2032\np(v\u2032 ,\u03b3 \u2032 ,v) y(v\n\u2032 ,\u03b3 \u2032 )\n\nv\u2032 \u2208W\n\n=\n=\n\n(because all flow into F is from W )\n\n\u03b3 \u2032 \u2208\u0393v\u2032\n\nv\u2032 \u2208W \u03b3 \u2032 \u2208\u0393v\u2032\nPr\u03c3\u03b1 (3{v})\n\nThe\nasPwe showed\nP lastPequality holds because,\nP in the proof of\u2032 ((1.) \u21d2 (2.)), the expression\n\u2032\n\u2032\n\u2032\np\ny\n=\nv\u2032 \u2208W\n\u03b3 \u2032 \u2208\u0393v\u2032 (v ,\u03b3 ,v) (v\u2032 ,\u03b3 \u2032 )\nv\u2032 \u2208V \\F\n\u03b3 \u2032 \u2208\u0393v\u2032 p(v\u2032 ,\u03b3 \u2032 ,v) y(v\u2032 ,\u03b3 \u2032 ) is exactly the \"expected\nnumber of times that we will visit the vertex v \u2208 F for the first time\", which is precisely\nthe probability Pr\u03c3\u03b1P\n(3{v}). P\nThus, clearly, v\u2208Fi yv\u2032\u2032 = v\u2208Fi Pr\u03c3\u03b1 (3{v}) = Pr\u03c3\u03b1 (3Fi ). Thus, since we have assumed\nP\nthat v\u2208Fi yv\u2032\u2032 \u2265 ri , we have established that Pr\u03c3\u03b1 (3Fi ) \u2265 ri , for all target sets Fi .\nThis completes the proof that (2.) \u21d2 (1.).\n(3.) \u21d4 (1.). Clearly (1.) \u21d2 (3.), so we need to show that (3.) \u21d2 (1.).\nLet U be the set of achievable vectors, i.e., all k-vectors r = hr1 . . . rk i such that there\nV\nis a (unrestricted) strategy \u03c3 such that ki=1 Pr\u03c3\u03b1 (3Fi ) \u2265 ri . Let U \u2299 be the analogous set\nwhere the strategy \u03c3 is restricted to be a possibly randomized but memoryless (stationary)\nstrategy. Clearly, U and U \u2299 are both downward closed, i.e., if r \u2265 r \u2032 and r \u2208 U then also\nr \u2032 \u2208 U , and similarly with U \u2299 . Also, obviously U \u2299 \u2286 U . We characterized U \u2299 in (1.) \u21d4\n(2.), in terms of a multi-objective LP. Thus, U \u2299 is the projection of the feasible space of a\nset of linear inequalities (a polyhedral P\nset), namely the set of inequalities in the variables\ny given in Fig. 3 and the inequalities v\u2208Fi yv \u2265 ri , i = 1, . . . , k. The feasible space is a\npolyhedron in the space indexed by the y variables and the ri 's, and U \u2299 is its projection on\nthe subspace indexed by the ri 's. Since the projection of a convex set is convex, it follows\nthat U \u2299 is convex.\n\n\fMARKOV DECISION PROCESSES \u2217\n\nMULTI-OBJECTIVE MODEL CHECKING OF\n\n15\n\nSuppose that there is a point r \u2208 U \\U \u2299 . Since U \u2299 is convex, this implies that there is a\nseparating hyperplane (see, e.g., [GLS93]) that separates r from U \u2299 , and in fact since U \u2299 is\ndownward closed, there is a separating hyperplane with non-negative coefficients, i.e. there\nP\nis a non-negative \"weight\" vector w = hw1 , . . . , wk i such that wT r = ki=1 wi ri > wT x for\nevery point x \u2208 U \u2299 .\nConsider now the MDP M with the following undiscounted reward structure. There is\n0 reward for every state, action and transition, except for transitions to a state v \u2208 F from\na state in V \\ F ; i.e. a reward is produced only once, P\nin the first transition into a state of\nF . The reward for every transition to a state v \u2208 F is\n{wi | i \u2208 {1, . . . , k} & v \u2208 Fi }. By\nPk\nthe definition, the expected reward of a policy \u03c3 is i=1 wi Pr\u03c3\u03b1 (3Fi ). From classical MDP\ntheory, we know that there is a memoryless strategy (in fact even a deterministic one) that\nmaximizes the expected reward for this type of reward structure. (Namely, this is a positive\nbounded reward case: see, e.g., Theorem 7.2.11 in [Put94].) Therefore, max{wT x | x \u2208\nU } = max{wT x | x \u2208 U \u2299 }, contradicting our assumption that wT r > max{wT x | x \u2208 U \u2299 }.\nCorollary 3.5. Given an MDP M = (V, \u0393, \u03b4), a number of target sets Fi \u2286 V , i =\nSk+k\u2032\nFi is absorbing, and an initial state u (or\n1, . . . , k + k\u2032 , such that every state v \u2208 F = i=1\neven initial distribution \u03b1 \u2208 D(V )):\n(a.) Given an extended achievability query for reachability, \u2203\u03c3B, where\nB\u2261\n\nk\n^\n\n(Pr\u03c3u (\u22c4Fi )\ni=1\n\n\u2265 ri ) \u2227\n\n\u2032\nk+k\n^\n\n(Pr\u03c3u (3Fj ) > rj ),\n\nj=k+1\n\nwe can in time polynomial in the size of the input, |M | + |B|, decide whether \u2203\u03c3 B is\nsatisfiable and if so construct a memoryless strategy that satisfies it.\n(b.) For \u01eb > 0, we can compute an \u01eb-approximate Pareto curve P(\u01eb) for the multi-objective\nreachability problem with objectives 3Fi , i = 1, . . . , k, in time polynomial in |M | and\n1/\u01eb.\nProof. For (a.), consider the constraints of the LP in P\nFigure 3, and add the following\nconstraints: for each i \u2208 {1, . . . , k} add the constraint v\u2208Fi yv \u2265 ri , and for each j \u2208\nP\n{k + 1, . . . , k + k\u2032 }, add the constraint\nv\u2208Fj yv \u2265 rj + z, where z is a new variable,\nand also add the constraint z \u2265 0. Finally, consider the new objective \"Maximize z\".\nSolve this LP to find whether an optimal feasible solution y \u2217 , z \u2217 exists, and if so whether\nz \u2217 > 0. If no solution exists, or if z \u2217 \u2264 0, then the extended achievability query is not\nsatisfiable. Otherwise, if z \u2217 > 0, then a strategy that satisfies \u2203\u03c3B exists, and moreover\nwe can construct a memoryless strategy that satisfies it by using the vector y \u2032\u2032 = y \u2217 and\npicking the strategy \u03c3 constructed from y \u2032\u2032 in the proof of (2.) \u21d2 (1.) in Theorem 3.2.\nPart (b.) is immediate from Theorem 3.2, and the results of [PY00], which show we\ncan \u01eb-approximate the Pareto curve for multi-objective linear programs in time polynomial\nin the size of the constraints and objectives and in 1/\u01eb.\n4. Qualitative multi-objective model checking\nTheorem 4.1. Given an MDP M , an initial state u, and a qualitative multi-objective query\nB, we can decide whether there exists a strategy \u03c3 that satisfies B, and if so construct such\n\n\f16\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\na strategy, in time polynomial in |M |, and using only graph-theoretic methods (in particular,\nwithout linear programming).\nProof. By the discussion in Section 2, it suffices to consider the case where we are given an\nMDP, M , and two sets of \u03c9-regular properties \u03a6, \u03a8, and we want a strategy \u03c3 such that\n^\n^\nPr\u03c3u (\u03c6) = 1 \u2227\nPr\u03c3u (\u03c8) > 0\n\u03c6\u2208\u03a6\n\n\u03c8\u2208\u03a8\n\nAssume the properties in \u03a6, \u03a8 are all given by (nondeterministic) B\u00fcchi automata Ai . We\nwill use and build on results in [CY98]. In [CY98] (Lemma 4.4, page 1411) it is shown that\nwe can construct from M and from a collection Ai , i = 1, . . . , m, of B\u00fcchi automata, a new\nMDP M \u2032 (a refinement of M ) which is the \"product\" of M with the naive determinization\nof all the Ai 's (i.e., the result of applying the standard subset construction on each Ai ,\nwithout imposing any acceptance condition). Technically, we have to slightly adapt the\nconstructions of [CY98], which use the convention that MDP states are either purely controlled or purely probabilistic, to the convention used in this paper which combines both\ncontrol and probabilistic behavior at each state. But these adaptations are straightforward. For completeness, we recall the (adapted) formal definition of M \u2032 . The states of\nthe MDP M \u2032 are tuples (x, z1 , . . . , zm ), where x is a state of the MDP, M , and zi is a set\nof states of Ai . The transition relation \u03b4\u2032 of M \u2032 is as follows. There exists a transition\n\u2032 )) \u2208 \u03b4 \u2032 if and only if the transition (x, a, p, x\u2032 ) is in M and,\n((x, z1 , ..., zm ), a, p, (x\u2032 , z1\u2032 , . . . , zm\n\u2032\nfor each i = 1, . . . , m, zi is precisely the set of states in the B\u00fcchi automaton Ai that one\ncould reach with one transition, starting from some state in the set zi and reading the\nsymbol l(x\u2032 ). Technically, we also have to add a dummy initial state x0 to the MDP, M ,\nsuch that there is a single enabled action, \u03b30 , at x0 , and such that there are transitions from\nx0 on action \u03b30 to other states according to some initial probability distribution on states,\n\u03b1 \u2208 D(V ). Thus, in particular, if we assume there is just one initial state u in the MDP, M ,\nthen we would now have one transition (x0 , \u03b30 , 1, u) \u2208 \u03b4 in the new M with added dummy\nstate x0 . The reason for adding the dummy x0 is because our definition of the product M \u2032\ndoes not use the label of the initial state in defining the transitions of M \u2032 . We also assume,\nw.l.o.g., that each B\u00fcchi automaton Ai has a single initial state si0 . In this way, the initial\nstate of M \u2032 becomes the tuple v0 = (x0 , {s10 }, . . . , {sm\n0 }).\nBy Lemma 4.4 and 4.5 of [CY98], this MDP M \u2032 has the following two properties. For\nevery subset R of \u03a6 \u222a \u03a8 there is a subset TR of corresponding \"target states\" of M \u2032 (and\nwe can compute this subset efficiently, in time polynomial in the size of M \u2032 ) that satisfies\nthe following two conditions:\n(I) If a trajectory of M \u2032 hits a state in TR at some point, then we can apply from that\npoint on a strategy \u03bcR (which is deterministic but uses memory) which ensures that\nthe resulting infinite trajectory satisfies all properties in R almost surely (i.e., with\nconditional probability 1, conditioned on the initial prefix that hits TR ).\n(II) For every strategy, the set of trajectories that satisfy all properties in R and do not\ninfinitely often hit some state of TR has probability 0.\nWe now outline the algorithm for deciding qualitative multi-objective queries.\n(1) Construct the MDP M \u2032 from M and from the properties \u03a6 and \u03a8 (in other words,\nusing one automaton for each property in \u03a6 and one for each property in \u03a8).\n\n\fMULTI-OBJECTIVE MODEL CHECKING OF\n\nMARKOV DECISION PROCESSES \u2217\n\n17\n\n(2) Compute T\u03a6 , and compute for each property \u03c8i \u2208 \u03a8 the set of states TRi where Ri =\n\u03a6 \u222a {\u03c8i }.3\n(3) If \u03a6 6= \u2205, prune M \u2032 by identifying and removing all \"bad\" states by applying the\nfollowing rules.\n(a) All states v that cannot \"reach\" any state in T\u03a6 are \"bad\".4\n(b) If for a state v there is an action \u03b3 \u2208 \u0393v such that there is a transition (v, \u03b3, p, v \u2032 ) \u2208\n\u03b4\u2032 , p > 0, and v \u2032 is bad, then remove \u03b3 from \u0393v .\n(c) If for some state v, \u0393v = \u2205, then mark v as bad.\nKeep applying these rules until no more states can be labelled bad and no more actions\nremoved for any state.\n(4) Restrict M \u2032 to the reachable states (from the initial state v0 ) that are not bad, and\nrestrict their action sets to actions that have not been removed, and let M \u2032\u2032 be the\nresulting MDP.\n(5) If (M \u2032\u2032 = \u2205 or \u2203\u03c8i \u2208 \u03a8 such that M \u2032\u2032 does not contain any state of TRi )\nthen return No.\nElse return Yes.\nV\nCorrectness proof: In one direction, suppose there is a strategy \u03c3 such that \u03c6\u2208\u03a6 Pr\u03c3u (\u03c6) =\nV\n1\u2227 \u03c8\u2208\u03a8 Pr\u03c3u (\u03c8) > 0. First, note that there cannot be any finite prefix of a trajectory under\n\u03c3 that hits a state that cannot reach any state in T\u03a6 . For, if there was such a path, then all\ntrajectories that start with this prefix would go only finitely often through T\u03a6 . Hence (by\nproperty (II) above) almost all these trajectories do not satisfy all properties in \u03a6, which\ncontradicts the fact that all these properties have probability 1 under \u03c3. From the fact that\nno path under \u03c3 hits a state that cannot reach T\u03a6 , it follows by an easy induction that no\nfinite trajectory under \u03c3 hits any bad state. That is, under \u03c3 all trajectories stay in the\nsub-MDP M \u2032\u2032 . Since every property \u03c8i \u2208 \u03a8 has probability Pr\u03c3u (\u03c8i ) > 0 and almost all\ntrajectories that satisfy \u03c8i and \u03a6 must hit a state of TRi (property (II) above), it follows\nthat M \u2032\u2032 contains some state of TRi for each \u03c8i \u2208 \u03a8. Thus the algorithm returns Yes.\nIn the other direction, suppose that the algorithm returns Yes. First, note that for all\nstates v of M \u2032\u2032 , and all enabled actions \u03b3 \u2208 \u0393v in M \u2032\u2032 , all transitions (v, \u03b3, p, v \u2032 ) \u2208 \u03b4, p > 0 of\nM \u2032 must still be in M \u2032\u2032 (otherwise, \u03b3 would have been removed from \u0393v at some stage using\nrule 3(b)). On the other hand, some states may have some missing actions in M \u2032\u2032 . Next,\nnote that all bottom strongly connected components (BSCCs) of M \u2032\u2032 (to be more precise,\nin the underlying one-step reachability graph of M \u2032\u2032 ) contain a state of T\u03a6 (if \u03a6 = \u2205 then\nall states are in T\u03a6 ), for otherwise the states in these BSCCs would have been eliminated\nat some stage using rule 3(a).\nDefine the following strategy \u03c3 which works in two phases. In the first phase, the\ntrajectory stays within M \u2032\u2032 . At each control state take a random action that remains in\nM \u2032\u2032 out of the state; the probabilities do not matter, we can use any non-zero probability\nfor all the remaining actions. In addition, at each state, if the state is in T\u03a6 or it is in\nTRi for some property \u03c8i \u2208 \u03a8, then with some nonzero probability the strategy decides to\nterminate phase 1 and move to phase 2 by switching to the strategy \u03bc\u03a6 or \u03bcRi respectively,\n3 Actually these sets can all be computed together: we can compute maximal closed components of the\n\nMDP, determine the properties that each component favors (see Def. 4.1 of [CY98]), and tag each state\nwith the sets for which it is a target state.\n4By \"reach\", we mean that starting at the state v = v , there a sequence of transitions (v , \u03b3, p , v\n0\ni\ni\ni+1 ) \u2208 \u03b4,\npi > 0, such that vn \u2208 T\u03a6 for some n \u2265 0.\n\n\f18\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\nwhich it applies from that point on. (Note: a state may belong to several TRi 's, in which\ncase each one of them gets some non-zero probability - the precise value is unimportant.)\nWe claim that this strategy \u03c3 meets the desired requirements - it ensures probability\n1 for all properties in \u03a6 and positive probability for all properties in \u03a8. For each \u03c8i \u2208 \u03a8,\nthe MDP M \u2032\u2032 contains some state of TRi ; with nonzero probability the process will follow\na path to that state and then switch to the strategy \u03bcRi from that point on, in which case\nit will satisfy \u03c8i (property (I) above). Thus, all properties in \u03a8 are satisfied with positive\nprobability.\nAs for \u03a6 (if \u03a6 6= \u2205), note that with probability 1 the process will switch at some point\nto phase 2, because all BSCCs of M \u2032\u2032 have a state in T\u03a6 . When it switches to phase 2 it\napplies strategy \u03bc\u03a6 or \u03bcRi for some Ri = \u03a6 \u222a {\u03c8i }, hence in either case it will satisfy all\nproperties of \u03a6 with probability 1.\n5. Quantitative multi-objective model checking.\nTheorem 5.1.\n(1.) Given an MDP M , an initial state u, and a quantitative multi-objective query B, we\ncan decide whether there exists a strategy \u03c3 that satisfies B, and if so construct such a\nstrategy, in time polynomial in |M |.\n(2.) Moreover, given \u03c9-regular properties \u03a6 = h\u03c61 , . . . , \u03c6k i, we can construct an \u01eb-approximate Pareto curve PMu ,\u03a6 (\u01eb), for the set of achievable probability vectors UMu ,\u03a6 in time\npolynomial in M and in 1/\u01eb.\nProof. For (1.), by the discussion in Section 2, we only need to consider extended achievV\nV \u2032\nability queries, B \u2261 ki=1 Pr\u03c3u (\u03c6i ) \u2265 ri \u2227 kj=k\u2032 +1 Pr\u03c3u (\u03c6j ) > rj , where k \u2265 k\u2032 \u2265 0, and for\na vector r \u2208 (0, 1]k . Let \u03a6 = h\u03c61 , . . . , \u03c6k i. We are going to reduce this multi-objective\nproblem with objectives \u03a6 to the quantitative multi-objective reachability problem studied\nin Section 3. From our reduction, both (1.) and (2.) will follow, using Corollary 3.5. As in\nthe proof of Theorem 4.1, we will build on constructions from [CY98]: form the MDP M \u2032\nconsisting of the product of M with the naive determinizations of the automata Ai for the\nproperties \u03c6i \u2208 \u03a6. For each subset R \u2286 \u03a6 we determine the corresponding subset TR of\ntarget states in M \u2032 .5\nConstruct the following MDP M \u2032\u2032 . Add to M \u2032 a new absorbing state sR for each subset\nR of \u03a6. For each state u of M \u2032 and each maximal subset R such that u \u2208 TR add a new\naction \u03b3R to \u0393u , and a new transition (u, \u03b3R , 1, sR ) to \u03b4. With each property \u03c6i \u2208 \u03a6 we\nassociate the subset of states Fi = {sR | \u03c6i \u2208 R}. Let F = h3F1 , . . . , 3Fk i. Let u\u2217 be the\ninitial state of the product MDP M \u2032\u2032 , given by the start state u of M and the start states\nof all the naively determinized Ai 's. Recall that UMu ,\u03a6 \u2286 [0, 1]k denotes the achievable set\nfor the properties \u03a6 in M starting at u, and that UM \u2032\u2032\u2217 ,F denotes the achievable set for F\nu\nin M \u2032\u2032 starting at u\u2217 .\nLemma 5.2. UMu ,\u03a6 = UM \u2032\u2032\u2217 ,F . Moreover, from a strategy \u03c3 that achieves r in UMu ,\u03a6 , we\nu\ncan recover a strategy \u03c3 \u2032 that achieves r in UM \u2032\u2032\u2217 ,F , and vice versa.\nu\n\n5Again, we don't need to compute these sets separately. See Footnote 3.\n\n\fMARKOV DECISION PROCESSES \u2217\n\nMULTI-OBJECTIVE MODEL CHECKING OF\n\n19\n\nProof. One direction is easy. Given such a strategy \u03c3 \u2032 in M \u2032\u2032 , we follow in M \u2032 (and in\nM ) the same strategy (of course, only the first component of states of M \u2032\u2032 matters in\nM ), until just before it transitions to a state sR , at which point it must be in TR , and at\nthat point our strategy \u03c3 switches to the strategy \u03bcR . This guarantees, for every \u03c6i \u2208 \u03a6,\n\u2032\nP ru\u03c3 (\u03c6i ) \u2265 Pr\u03c3u\u2217 (3Fi ) \u2265 ri .\nFor the other direction, suppose that the claim is not true, i.e. there is a strategy \u03c3\nin M which ensures probability Pr\u03c3u (\u03c6i ) \u2265 ri , i = 1, . . . , k, but r 6\u2208 UM \u2032\u2032\u2217 ,F . Note that all\nu\n\n\u2299\nstates in F = \u222aki=1 Fi are absorbing. From Theorem 3.2 we know that UM \u2032\u2032\u2217 ,F = UM\n\u2032\u2032\nu\n\n\u2299\nwhere UM\n\u2032\u2032 ,F is the\nu\u2217\n\u2299\nUM \u2032\u2032\u2217 ,F = UM\n\u2032\u2032 ,F is\nu\nu\u2217\n\nu\u2217\n\n,F\n\nset of value vectors achievable by memoryless strategies. Recall, that\nconvex, and that it is downward-closed. Since r 6\u2208 UM \u2032\u2032\u2217 ,F , as in the\nu\n\nproof of (3.) \u21d2 (1.) in Thm. 3.2, there must be a separating hyperplane, i.e., a nonP\nnegative weight vector w = hw1 , . . . , wk i such that wT r = ki=1 wi ri > wT x for every point\nx \u2208 UM \u2032\u2032\u2217 ,F .\nu\nConsider M P\nwith the following reward structure, denoted rew(w): a trajectory \u03c4 of M\nreceives reward {wi | \u03c4 satisfies \u03c6i }. This is not the traditional type of reward structure\nwhere reward is obtained at the states and transitions of the trajectory; it is obtained\nonly at infinity when the trajectory has finished and we get a reward that depends on the\nproperties that were satisfied. In [CY98] optimization of the expected reward for MDPs\nwith this kind of reward structure was studied and solved by reducing the problem to\nan MDP with a classical type of reward. We reuse that construction here. Consider the\nMDP M \u2032\u2032 augmented with a traditional type of reward structure,\ndenoted rew\u2032\u2032 , in which\nP\neach transition of the form (u, \u03b3R , 1, sR ) produces reward {wi | \u03c6i \u2208 R}, while all other\ntransitions (and states and actions) give 0 reward. Let M\u0302 \u2032\u2032 be a subMDP of M \u2032\u2032 that\ncontains for each state u only one (at most) transition of the form (u, \u03b3R , 1, sR ), namely\nthe one that produces the maximum reward (breaking ties arbitrarily). Clearly, there is no\nreason ever to select from a state u any transition (u, \u03b3R\u2032 , 1, sR\u2032 ) that produces lower reward,\nthus, M \u2032\u2032 and M\u0302 \u2032\u2032 have the same optimal expected reward. It is shown in [CY98] that the\noptimal expected rewards in (M, rew(w)) and (M\u0302 \u2032\u2032 , rew\u2032\u2032 ), and thus also in (M \u2032\u2032 , rew\u2032\u2032 ), are\nequal to each other. Moreover, the optimum value in these MDPs is achievable, i.e., there\nare optimal strategies, and in fact a deterministic finite-memory optimal strategy can be\nconstructed.\nThe optimal expected reward in (M, rew(w)) is at least wT r (because strategy \u03c3\nachieves wT r), while the optimal expected reward in (M \u2032\u2032 , rew\u2032\u2032 ) is equal to max{wT x |\nx \u2208 UM \u2032\u2032\u2217 ,F }, because rewards are only obtained by transitioning to a state in F . Thereu\n\nfore, wT r \u2264 max{wT x | x \u2208 UM \u2032\u2032\u2217 ,F }, contradicting our hypothesis that wT r > max{wT x |\nu\nx \u2208 UM \u2032\u2032\u2217 ,F }.\nu\n\nIt follows from the lemma that: there exists a strategy \u03c3 in M such that\n\u2032\n\nk\n^\n\ni=1\n\nPr\u03c3u (\u03c6i )\n\n\u2265 ri \u2227\n\nk\n^\n\nj=k \u2032 +1\n\nPr\u03c3u (\u03c6j ) > rj\n\n\f20\n\nK. ETESSAMI, M. KWIATKOWSKA, M. Y. VARDI, AND M. YANNAKAKIS\n\nif and only if there exists a strategy \u03c3 \u2032 in M \u2032\u2032 such that\n\u2032\n\nk\n^\n\ni=1\n\nPr\u03c3u\u2217 (3Fi ) \u2265 ri \u2227\n\nk\n^\n\nPr\u03c3u\u2217 (3Fj ) > rj .\n\nj=k \u2032 +1\n\nMoreover, such strategies can be recovered from each other. Thus (1.) and (2.) follow,\nusing Corollary 3.5.\n6. Concluding remarks\nWe mention that recent results by Diakonikolas and Yannakakis [DY08] provide improved upper bounds for appoximation of convex Pareto curves, and for computing a\nsmallest such approximate convex Pareto set. These results yield significantly improved\nalgorithms, particularly in the bi-objective case, for the multi-objective LP problem, and\nthus also for the multi-objective MDP problems studied in this paper. In particular, in the\nbi-objective MDP case, [DY08] provides a polynomial time algorithm to compute a minimal\n\u01eb-approximate (convex) Pareto set (i.e., one with the fewest number of points possible).\nWe mention that, although we use LP methods to obtain our complexity upper bounds,\nin practice there is a way to combine other efficient iterative methods used for solving MDPs,\ne.g., based on value iteration or policy (strategy) iteration, with our results in order to approximate the Pareto curve for multi-objective model checking. This is because the results\nof [PY00, DY08] for multi-objective convex optimization problems only require a blackbox routine that optimizes (exactly or approximately) positive linear combinations of the\nobjectives. Specifically, in our setting the multiple MDP objectives ask to optimize the\nprobabilities of different linear-time \u03c9-regular properties. By using the results in [CY98],\nit is possible to reduce the problem of optimizing such positive linear combinations to the\nproblem of finding the optimal expected reward for a new MDP with positive rewards. The\ntask of computing or approximating this optimal expected reward can be carried out using\nany of various standard iterative methods, e.g., based on value iteration and policy iteration\n(see [Put94]). These can thus be used to answer (exactly or approximately) the black-box\nqueries required by the methods of [PY00, DY08], thereby yielding a method for approximating the Pareto curve (albeit, without the same theoretical complexity guarantees).\nAn important extension of the applications of our results is to extend the asymmetric\nassume-guarantee compositional reasoning rule discussed in Section 2 to a general compositional framework for probabilistic systems. It is indeed possible to describe symmetric\nassume-guarantee rules that allow for general composition of MDPs. A full treatment of\nthe general compositional framework requires a separate paper, and we plan to expand on\nthis in follow-up work.\nAcknowledgements. We thank the Newton Institute, where we initiated discussions\non the topics of this paper during the Spring 2006 programme on Logic and Algorithms.\nSeveral authors acknowledge support from the following grants: EPSRC GR/S11107 and\nEP/D07956X, MRL 2005-04; NSF grants CCR-9988322, CCR-0124077, CCR-0311326, and\nANI-0216467, BSF grant 9800096, Texas ATP grant 003604-0058-2003, Guggenheim Fellowship; NSF CCF-04-30946 and NSF CCF-0728736.\n\n\fMULTI-OBJECTIVE MODEL CHECKING OF\n\nMARKOV DECISION PROCESSES \u2217\n\n21\n\nReferences\n[Car83]\n\nP. Carstensen. Complexity of some parametric integer and network programming problems. Mathematical Programming, 26(1):64\u201375, 1983.\n[Cl\u0131\u030197]\nJ. Cl\u0131\u0301maco, editor. Multicriteria Analysis. Springer-Verlag, 1997.\n[CMH06] K. Chatterjee, R. Majumdar, and T. Henzinger. Markov decision processes with multiple objectives. In Proc. of 23rd Symp. on Theoretical Aspects of Computer Science, volume LNCS 3884,\npages 325\u2013336, 2006.\n[CY95] C. Courcoubetis and M. Yannakakis. The complexity of probabilistic verification. Journal of the\nACM, 42(4):857\u2013907, 1995.\n[CY98] C. Courcoubetis and M. Yannakakis. Markov decision processes and regular events. IEEE Trans.\non Automatic Control, 43(10):1399\u20131418, 1998.\n[DY08] I. Diakonikolas and M. Yannakakis. Succinct Approximate Convex Pareto Curves. In Proc. of\nACM-SIAM Symp. on Discrete Algorithms (SODA'08), 2008.\n[Ehr05] M. Ehrgott. Multicriteria optimization. Springer-Verlag, 2005.\n[Fur80] N. Furukawa. Characterization of optimal policies in vector-valued Markovian decision processes.\nMathematics of Operations Research, 5(2):271\u2013279, 1980.\n[Gho90] M. K. Ghosh. Markov decision processes with multiple costs. Oper. Res. Lett., 9(4):257\u2013260, 1990.\n[GLS93] M. Gr\u00f6tschel, L. Lov\u00e1sz, and A. Schrijver. Geometric Algorithms and Combinatorial Optimization.\nSpringer-Verlag, 2nd edition, 1993.\n[Hen83] M. I. Henig. Vector-valued dynamic programming. SIAM J. Control Optim., 21(3):490\u2013499, 1983.\n[MS01] K. Mulmuley and P. Shah. A lower bound for the shortest path problem. J. Comput. System Sci.,\n63(2):253-267, 2001.\n[Put94] M. L. Puterman. Markov Decision Processes. Wiley, 1994.\n[PY00]\nC. Papadimitriou and M. Yannakakis. On the approximability of trade-offs and optimal access\nof web sources. In Proc. of 41st IEEE Symp. on Foundations of Computer Science, pages 86\u201392,\n2000.\n[Var85] M. Vardi. Automatic verification of probabilistic concurrent finite-state programs. In Proc. of 26th\nIEEE FOCS, pages 327\u2013338, 1985.\n[Whi82] D. J. White. Multi-objective infinite-horizon discounted Markov decision processes. J. Math. Anal.\nAppl., 89(2):639\u2013647, 1982.\n[WT98] K. Wakuta and K. Togawa. Solution procedures for multi-objective Markov decision processes.\nOptimization. A Journal of Mathematical Programming and Operations Research, 43(1):29\u201346,\n1998.\n\nThis work is licensed under the Creative Commons Attribution-NoDerivs License. To view\na copy of this license, visit http:// reative ommons.org/li enses/by-nd/2.0/ or send a\nletter to Creative Commons, 559 Nathan Abbott Way, Stanford, California 94305, USA.\n\n\f"}