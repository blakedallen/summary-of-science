{"id": "http://arxiv.org/abs/1004.3105v2", "guidislink": true, "updated": "2010-04-20T01:03:00Z", "updated_parsed": [2010, 4, 20, 1, 3, 0, 1, 110, 0], "published": "2010-04-19T06:45:43Z", "published_parsed": [2010, 4, 19, 6, 45, 43, 0, 109, 0], "title": "Fast normal random number generators on vector processors", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.1253%2C1004.3115%2C1004.3507%2C1004.0184%2C1004.4460%2C1004.4902%2C1004.2076%2C1004.0274%2C1004.0772%2C1004.0362%2C1004.0626%2C1004.4513%2C1004.1596%2C1004.4316%2C1004.4553%2C1004.0246%2C1004.0326%2C1004.1446%2C1004.5007%2C1004.0660%2C1004.2194%2C1004.3937%2C1004.5497%2C1004.0813%2C1004.1950%2C1004.4076%2C1004.2692%2C1004.4467%2C1004.0615%2C1004.0988%2C1004.4118%2C1004.5285%2C1004.5160%2C1004.3647%2C1004.2266%2C1004.2276%2C1004.1358%2C1004.0848%2C1004.2691%2C1004.3046%2C1004.1295%2C1004.3272%2C1004.0958%2C1004.2666%2C1004.2258%2C1004.4195%2C1004.2697%2C1004.2413%2C1004.0542%2C1004.0313%2C1004.1192%2C1004.4745%2C1004.4640%2C1004.4067%2C1004.4212%2C1004.5071%2C1004.1371%2C1004.1071%2C1004.4644%2C1004.4176%2C1004.4009%2C1004.2764%2C1004.3581%2C1004.0255%2C1004.0334%2C1004.0431%2C1004.0681%2C1004.4853%2C1004.4868%2C1004.3106%2C1004.3105%2C1004.0829%2C1004.4588%2C1004.5320%2C1004.4201%2C1004.4682%2C1004.3015%2C1004.2814%2C1004.0808%2C1004.5305%2C1004.5588%2C1004.3658%2C1004.3165%2C1004.0426%2C1004.2109%2C1004.3099%2C1004.3751%2C1004.0924%2C1004.4979%2C1004.4423%2C1004.5181%2C1004.2617%2C1004.5393%2C1004.4863%2C1004.0468%2C1004.1096%2C1004.5283%2C1004.3014%2C1004.4556%2C1004.4216%2C1004.1969&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Fast normal random number generators on vector processors"}, "summary": "We consider pseudo-random number generators suitable for vector processors.\nIn particular, we describe vectorised implementations of the Box-Muller and\nPolar methods, and show that they give good performance on the Fujitsu VP2200.\nWe also consider some other popular methods, e.g. the Ratio method of Kinderman\nand Monahan (1977) (as improved by Leva (1992)), and the method of Von Neumann\nand Forsythe, and show why they are unlikely to be competitive with the Polar\nmethod on vector processors.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.1253%2C1004.3115%2C1004.3507%2C1004.0184%2C1004.4460%2C1004.4902%2C1004.2076%2C1004.0274%2C1004.0772%2C1004.0362%2C1004.0626%2C1004.4513%2C1004.1596%2C1004.4316%2C1004.4553%2C1004.0246%2C1004.0326%2C1004.1446%2C1004.5007%2C1004.0660%2C1004.2194%2C1004.3937%2C1004.5497%2C1004.0813%2C1004.1950%2C1004.4076%2C1004.2692%2C1004.4467%2C1004.0615%2C1004.0988%2C1004.4118%2C1004.5285%2C1004.5160%2C1004.3647%2C1004.2266%2C1004.2276%2C1004.1358%2C1004.0848%2C1004.2691%2C1004.3046%2C1004.1295%2C1004.3272%2C1004.0958%2C1004.2666%2C1004.2258%2C1004.4195%2C1004.2697%2C1004.2413%2C1004.0542%2C1004.0313%2C1004.1192%2C1004.4745%2C1004.4640%2C1004.4067%2C1004.4212%2C1004.5071%2C1004.1371%2C1004.1071%2C1004.4644%2C1004.4176%2C1004.4009%2C1004.2764%2C1004.3581%2C1004.0255%2C1004.0334%2C1004.0431%2C1004.0681%2C1004.4853%2C1004.4868%2C1004.3106%2C1004.3105%2C1004.0829%2C1004.4588%2C1004.5320%2C1004.4201%2C1004.4682%2C1004.3015%2C1004.2814%2C1004.0808%2C1004.5305%2C1004.5588%2C1004.3658%2C1004.3165%2C1004.0426%2C1004.2109%2C1004.3099%2C1004.3751%2C1004.0924%2C1004.4979%2C1004.4423%2C1004.5181%2C1004.2617%2C1004.5393%2C1004.4863%2C1004.0468%2C1004.1096%2C1004.5283%2C1004.3014%2C1004.4556%2C1004.4216%2C1004.1969&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider pseudo-random number generators suitable for vector processors.\nIn particular, we describe vectorised implementations of the Box-Muller and\nPolar methods, and show that they give good performance on the Fujitsu VP2200.\nWe also consider some other popular methods, e.g. the Ratio method of Kinderman\nand Monahan (1977) (as improved by Leva (1992)), and the method of Von Neumann\nand Forsythe, and show why they are unlikely to be competitive with the Polar\nmethod on vector processors."}, "authors": ["Richard P. Brent"], "author_detail": {"name": "Richard P. Brent"}, "author": "Richard P. Brent", "arxiv_comment": "An old Technical Report, not published elsewhere. 6 pages. For\n  details see http://wwwmaths.anu.edu.au/~brent/pub/pub141.html", "links": [{"href": "http://arxiv.org/abs/1004.3105v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1004.3105v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.NA", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "11K45", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "G.3", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1004.3105v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1004.3105v2", "journal_reference": null, "doi": null, "fulltext": "arXiv:1004.3105v2 [cs.DS] 20 Apr 2010\n\nFast Normal Random Number Generators for Vector Processors\u2217\nRichard P. Brent1\nComputer Sciences Laboratory\nAustralian National University\nCanberra, ACT 0200\nAbstract\nWe consider pseudo-random number generators\nsuitable for vector processors. In particular, we describe vectorised implementations of the Box-Muller\nand Polar methods, and show that they give good performance on the Fujitsu VP2200. We also consider\nsome other popular methods, e.g. the Ratio method and\nthe method of Von Neumann and Forsythe, and show\nwhy they are unlikely to be competitive with the Polar\nmethod on vector processors.\n\n2\n\nSome Normal Generators\n\nAssume that a good uniform random number generator which returns uniformly distributed numbers\nin the interval [0, 1) is available, and that we wish to\nsample the normal distribution with mean \u03bc and variance \u03c3 2 . We can generate two independent, normally\ndistributed numbers x, y by the following old algorithm due to Box and Muller [14] (Algorithm B1):\n1. Generate independent uniform numbers u and v.\np\n2. Set r \u2190 \u03c3 \u22122 ln(1 \u2212 u).\n3. Set x \u2190 r sin(2\u03c0v) + \u03bc and y \u2190 r cos(2\u03c0v) + \u03bc.\n\n1\n\nIntroduction\n\nSeveral recent papers [2, 4, 16] have considered the\ngeneration of uniformly distributed pseudo-random\nnumbers on vector and parallel computers. In many\napplications, random numbers from specified nonuniform distributions are required. These distributions may be continuous (e.g. normal, exponential) or\ndiscrete (e.g. Poisson). A common requirement is for\nthe normal distribution.\nThe most efficient methods for generating normally distributed random variables on sequential machines [1, 3, 6, 7, 10, 11] involve the use of different\napproximations on different intervals, and/or the use\nof \"rejection\" methods, so they often do not vectorise\nwell. Simple, \"old-fashioned\" methods may be preferable. In Section 2 we describe two such methods, and\nin Sections 3\u20134 we consider their efficient implementation on vector processors, and give the results of implementations on a Fujitsu VP2200/10. In Sections 5\u20136\nwe consider some other methods which are popular on\nserial machines, and show that they are unlikely to be\ncompetitive on vector processors.\nc 1993, R. P. Brent. Appeared as Technical\nReport TR-CS-93-04, CS Lab, ANU, March 1993.\n\u2217 Copyright\n\nTwo minor points:\na. We have written (1 \u2212 u) instead of u at step 2\nbecause of our assumption that the uniform random number generator may return exactly 0 but\nnever exactly 1. If the uniform generator never\nreturns exactly 0, then (1 \u2212 u) can be replaced\nby u. Similar comments apply below.\nb. The argument of sin and cos in step 3 is in the interval [\u2212\u03c0, \u03c0), but any interval of length 2\u03c0 would\nbe satisfactory.\nThe proof that the algorithm is correct follows on\nconsidering the distribution of (x, y) transformed to\npolar coordinates, and is similar to the proof of correctness of the Polar method, given in [10].\nAlgorithm B1 is a reasonable choice if vectorised\nsquare root, logarithm and trigonometric function routines are available. Each normally distributed number\nrequires 1 uniformly distributed number, 0.5 square\nroots, 0.5 logarithms, and 1 sin or cos evaluation. Vectorised implementations of the Box-Muller method are\ndiscussed in Section 3.\nA variation of Algorithm B1 is the Polar method\nof Box, Muller and Marsaglia (1958) (Algorithm P of\nKnuth [10]):\n1 E-mail\n\naddress: rpb@cslab.anu.edu.au\nrpb141tr typeset using LATEX\n\n\fcomponent\nln\nsqrt\nsin/cos\nother\ntotal\n\n1. Generate independent uniform random numbers\nx and y in the interval [\u22121, 1).\n2. Compute s \u2190 x2 + y 2 .\n3. If s \u2265 1 then go to step 1 (i.e. reject x and y) else\ngo to step 4.\np\n4. Set r \u2190 \u03c3 \u22122 ln(s)/s, and return rx + \u03bc and\nry + \u03bc.\n\nB2\n13.1\n8.8\n6.6\n5.6\n34.1\n\nB3\n7.1\n1.0\n6.6\n11.6\n26.3\n\nP1\n13.1\n8.8\n0.0\n11.9\n33.8\n\nP2\n7.1\n1.0\n0.0\n13.8\n21.9\n\nR1\n0.3\n0.0\n0.0\n35.1\n35.4\n\nTable 1: Cycles per normal random number\ncos y by a polynomial of the form c0 +c2 y 2 +c4 y 4 +c6 y 6 .\nThen, using the identities\n\nIt is easy to see that, at step 4, (x, y) is uniformly\ndistributed in the unit circle, so s is uniformly distributed in [0, 1). To avoid the remote possibility of\ndivision by zero at step 4, we could replace ln(s)/s by\nln(1 \u2212 s)/(1 \u2212 s).\nA proof that the values returned by Algorithm P\nare independent, normally distributed random numbers (with mean \u03bc and variance \u03c3 2 ) is given in\nKnuth [10]. On average, step 1 is executed 4/\u03c0\ntimes, so each normally distributed number requires\n4/\u03c0 \u2243 1.27 uniform random numbers, 0.5 divisions,\n0.5 square roots, and 0.5 logarithms. Compared to\nAlgorithm B1, we have avoided the sin and cos computation at the expense of more uniform random numbers, 0.5 divisions, and the cost of implementing the\nacceptance/rejection process. This can be done using\na vector gather. Vectorised implementations of the\nPolar method are discussed in Section 4.\n\n3\n\nB1\n13.1\n8.8\n13.8\n5.9\n41.6\n\nsin 2y = 2 sin y cos y, cos 2y = 1 \u2212 2 sin2 y\nfour times, we can compute sin 16y and cos 16y with\na small number of multiplications and additions. The\ncomputation is vectorizable.\nTimes, in machine cycles per normally distributed\nnumber, for methods B1, B2 (and other methods described below) are given in Table 1. In all cases\nthe generalised Fibonacci random number generator RANU4 (described in [4]) was used to generate\nthe required uniform random numbers, and a large\nnumber of random numbers were generated, so that\nvector lengths were long. RANU4 generates a uniformly distributed random number in 2.2 cycles on\nthe VP 2200/10. (The cycle time of the VP 2200/10\nat ANU is 3.2 nsec, and two multiplies and two adds\ncan be performed per clock cycle, so the peak speed is\n1.25 Gflop.)\nThe Table gives the total times and also the estimated times for the four main components:\n\nImplementation of the Box-Muller\nMethod\n\n1. ln computation (actually 0.5 times the cost of one\nln computation since the times are per normal\nrandom number generated).\n\nWe have implemented the Box-Muller method (Algorithm B1 above) and several refinements (B2, B3)\non a Fujitsu VP 2200/10 vector processor at the Australian National University. The implementations all\nreturn double-precision real results, and in cases where\napproximations to sin, cos, sqrt and/or ln have been\nmade, the absolute error is considerably less than\n10\u221210 . Thus, statistical tests using less than about\n1020 random numbers should not be able to detect any\nbias due to the approximations. The calling sequences\nallow for an array of random numbers to be returned.\nThis permits vectorisation and amortises the cost of a\nsubroutine call over the cost of generating many random numbers.\nOur method B2 is the same as B1, except that we\nreplace calls to the Fortran library sin and cos by an\ninline computation, using a fast, but sufficiently accurate, approximation. Let y = (2\u03c0v \u2212 \u03c0)/16, where\n0 \u2264 v < 1, so |y| \u2264 \u03c0/16. We approximate sin y by a\npolynomial of the form s1 y + s3 y 3 + s5 y 5 + s7 y 7 , and\n\n2. sqrt computation (actually 0.5 times).\n3. sin or cos computation.\n4. other, including uniform random number generation.\nThe results for method B1 show that the sin/cos\nand ln computations are the most expensive (65% of\nthe total time). Method B2 is successful in reducing\nthe sin/cos time from 33% of the total to 19%.\nIn Method B2, 64%\np of the time is consumed by\nthe computation of \u2212 ln(1 \u2212 u). An obvious way to\nreduce this time is to use a fast approximation to the\nfunction\np\nf (u) = \u2212 ln(1 \u2212 u),\n\njust as we used a fast approximation to sin and cos\nto speed up method B1. However, this is difficult to\n2\n\n\faccomplish with sufficient accuracy, because the function f (u) has singularities at both endpoints of the\nunit interval. Method B3 overcomes this difficulty in\nthe following way.\n1. We approximate the function\nr\ng(u) = u\u22121/2 f (u) =\n\nu is a uniformly distributed random variable on [0, 1).\nWe generate two independent uniform variables, say\nu1 and u2 , and let u \u2190 max(u1 , u2 )2 . It is easy to\nsee that u is in fact uniformly distributed on [0, 1).\nHowever, u1/2 = max(u1 , u2 ) can be computed without calling the library sqrt routine. To summarise, a\nnon-vectorised version of method B3 is:\n\n\u2212 ln(1 \u2212 u)\n,\nu\n\n1. Generate uniform random numbers u1 , u2 and u3\non [0, 1).\n\nrather than f (u). Using the Taylor series for\nln(1 \u2212 u), we see that g(u) = 1 + u/4 + * * * is\nwell-behaved near u = 0.\n\n2. Set m \u2190 max(u1 , u2 ) and u \u2190 m2 .\n\n2. The approximation to g(u) is only used in the\ninterval 0 \u2264 u \u2264 \u03c4 , where \u03c4 < 1 is suitably chosen. For \u03c4 < u < 1 we use the slow but accurate\nFortran ln and sqrt routines.\n\n3. If u > 8/9 then\n3.1. set r \u2190 \u03c3\nroutines, else\n\n3. We make a change of variable of the form v =\n(\u03b1u + \u03b2)/(\u03b3u + \u03b4), where \u03b1, . . . , \u03b4 are chosen to\nmap [0, \u03c4 ] to [\u22121, 1], and the remaining degrees\nof freedom are used to move the singularities of\nthe function h(v) = g(u) as far away as possible\nfrom the region of interest (which is \u22121 \u2264 v \u2264 1).\nTo be more precise, let \u03c1 be a positive parameter.\nThen we can choose\n\u00132\n\u0012\n\u03c1\n,\n\u03c4 =1\u2212\n\u03c1+2\n\u0012\n\u0013\n(\u03c1 + 2)u \u2212 2\nv = (\u03c1 + 1)\n,\n2(\u03c1 + 1) \u2212 (\u03c1 + 2)u\n\np\n\u2212 ln(1 \u2212 u) using Fortran library\n\n3.2. set v \u2190 (6u \u2212 4)/(4 \u2212 3u), evaluate h(v) as\ndescribed above, and set r \u2190 \u03c3mh(v).\n4. Evaluate s \u2190 sin(2\u03c0u3 \u2212\u03c0) and c \u2190 cos(2\u03c0u3 \u2212\u03c0)\nas described above.\n\u221a\n\u221a\n5. Return \u03bc + cr 2 and \u03bc + sr 2, which are independent, normal random numbers with mean \u03bc\nand standard deviation \u03c3.\nVectorization of method B3 is straightforward, and\ncan take advantage of the \"list vector\" technique on\nthe VP2200. The idea is to gather those u > 8/9\ninto a contiguous array, call the\np vectorised library routines to compute an array of \u2212 ln(1 \u2212 u) values, and\nscatter these back. The gather and scatter operations\ndo introduce some overhead, as can be seen from the\nrow labelled \"other\" in the Table. Nevertheless, on\nthe VP2200, method B3 is about 23% faster than\nmethod B2, and about 37% faster than the straightforward method B1. These ratios could be different on\nmachines with more (or less) efficient implementations\nof scatter and gather.\nPetersen [16] gives times for normal and uniform\nrandom number generators on a NEC SX-3. His\nimplementation normalen of the Box-Muller method\ntakes 55.5 nsec per normally distributed number, i.e. it\nis 2.4 times faster than our method B1, and 1.51 times\nfaster than our method B3. The model of SX-3 used\nby Petersen has an effective peak speed of 2.75 Gflop,\nwhich is 2.2 times the peak speed of the VP 2200/10.\nConsidering the relative speeds of the two machines\nand the fact that the SX-3 has a hardware square root\nfunction, our results are quite encouraging.\n\nand the singularities of h(v) are at \u00b1(\u03c1 + 1).\nFor simplicity, we choose \u03c1 = 1, which experiment\nshows is close to optimal on the VP 2200/10. Then\n\u03c4 = 8/9, v = (6u \u2212 4)/(4 \u2212 3u), and h(v) has singularities at v = \u00b12, corresponding to the singularities\nof g(u) at u = 1 and u = \u221e. A polynomial of the\nform h0 + h1 v + * * * + h15 v 15 can be used to approximate h(v) with absolute error less than 2 \u00d7 10\u221211 on\n[\u22121, 1]. About 30 terms would be needed if we attempted to approximate g(u) to the same accuracy by\na polynomial on [0, \u03c4 ]. We use polynomial approximations which are close to minimax approximations.\nThese may easily be obtained by truncating Chebyshev series, as described in [5].\nIt appears that this approach requires the computation of a square root, since we really want f (u) =\nu1/2 g(u), not g(u). However, a trick allows this square\nroot computation to be avoided, at the expense of an\nadditional uniform random number generation (which\nis cheap) and a few arithmetic operations. Recall that\n3\n\n\f4\n\nImplementation of the Polar Method\n\n4.2. set v \u2190 (6s \u2212 4)/(4 \u2212 3s), evaluate h(v) as\ndescribed in Section 3, and set r \u2190 \u03c3h(v).\n\u221a\n\u221a\n5. Return xr 2 + \u03bc and yr 2 + \u03bc, which are independent, normal random numbers with mean \u03bc\nand standard deviation \u03c3.\n\nThe times given in Table 1 for methods B1\u2013B3 can\nbe used to predict the best possible performance of the\nPolar method (Section 2). The Polar method avoids\nthe computation of sin and cos, so could gain up to 6.6\ncycles per normal random number over method B3.\nHowever, we would expect the gain to be less than\nthis because of the overhead of a vector gather caused\nby use of a rejection method. A straightforward vectorised implementation of the Polar method, called\nmethod P1, was written to test this prediction. The\nresults are shown in Table 1. 13.8 cycles are saved\nby avoiding the sin and cos function evaluations, but\nthe overhead increases by 6.0 cycles, giving an overall saving of 7.8 cycles or 19%. Thus, method P1 is\nabout the same speed as method B2, but not as fast\nas method B3.\nEncouraged by our success in avoiding most ln\nand sqrt computations in the Box-Muller method (see\nmethod B3), we considered if a similar idea would\nwork for the Polar method. In fact, it does. Step\n4 of the Polar\np method (Section 2) involves the computation of \u22122 ln(s)/s, where 0 < s < 1. The function\nhas a singularity at s = 0, but we can approximate\nit quite well on an interval such as [1/9, 1], using a\nmethod similar to that used to approximate the function g(u) of Section 3.\nInspection of the proof in Knuth [10] shows that\nstep 4 of the Polar method can be replaced by\np\n4a. Set r \u2190 \u03c3 \u22122 ln(u)/s,\nand return rx + \u03bc and ry + \u03bc\n\nTo vectorise steps 1-3, we simply generate vectors of\nxj and yj values, compute sj = x2j + yj2 , and compress\nby omitting any triple (xj , yj , sj ) for which sj \u2265 1.\nThis means that we can not predict in advance how\nmany normal random numbers will be generated, but\nthis problem is easily handled by introducing a level\nof buffering. The vectorised version of method P2 is\ncalled RANN3B, and the user-friendly routine which\nperforms the buffering and calls RANN3B is called\nRANN3.\nThe second-last column of Table 1 gives results for\nmethod P2 (actually for RANN3, since the buffering\noverhead is included). There is a saving of 11.9 cycles\nor 35% compared to method P1, and the method is\n17% faster than the fastest version of the Box-Muller\nmethod (method B3). The cost of logarithm and\nsquare root computations is only 37% of the total,\nthe remainder being the cost of generating uniform\nrandom numbers (about 13%) and the cost of the rejection step and other overheads (about 50%). On\nthe VP2200/10 we can generate more than 14 million\nnormally distributed random numbers per second (one\nper 70 nsec).\n\n5\n\nwhere u is any uniformly distributed variable on (0, 1],\nprovided u is independent of arctan(y/x). In particular, we can\nthe constant\np\n\u221a take u = 1\u2212s. Thus, omitting\nfactor \u03c3 2, we need to evaluate \u2212 ln(1 \u2212 s)/s, but\nthis is just g(s), and we can use exactly the same approximation as in Section 3. This gives us method P2.\nTo summarise, a non-vectorised version of method P2\nis:\n\nThe Ratio Method\n\nThe Polar method is one of the simplest of a class of\nrejection methods for generating random samples from\nthe normal (and other) distributions. Other examples\nare given in [1, 3, 6, 10]. It is possible to implement\nsome of these methods in a manner similar to our implementation of method P2. For example, a popular\nmethod is the Ratio Method of Kinderman and Monahan [9] (also described in [10], and improved in [11]).\nIn its simplest form, the Ratio Method is given by\nAlgorithm R:\n\n1. Generate independent uniform random numbers\nx and y in the interval [\u22121, 1).\n\n1. Generate independent uniform random numbers\nu and v in [0, 1).\np\n2. Set x \u2190 8/e(v \u2212 21 )/(1 \u2212 u).\n\n2. Compute s \u2190 x2 + y 2 .\n3. If s \u2265 1 then go to step 1 (i.e. reject x and y) else\ngo to step 4.\n\n3. If \u2212x2 ln(1\u2212u) > 4 then go to step 1 (i.e. reject x)\nelse go to step 4.\n\n4. If s > 8/9 then\np\n4.1. set r \u2190 \u03c3 \u2212 ln(1 \u2212 s)/s using Fortran library routines, else\n\n4. Return \u03c3x + \u03bc.\n4\n\n\fAlgorithm R returns a normally\n\u221a distributed random number using (on average) 8/ \u03c0e \u2243 2.74 uniform\nrandom numbers and 1.37 logarithm evaluations. The\nproof of correctness, and various refinements which reduce the number of logarithm evaluations, are given\nin [9, 10, 11]. The idea of the proof is that x is normally distributed if the point (u, v) lies inside a certain\nclosed curve\np C which\np in turn is inside the rectangle\n[0, 1] \u00d7 [\u2212 2/e, + 2/e]. Step 3 rejects (u, v) if it is\noutside C.\nThe function ln(1 \u2212 u) occurring at step 3 has a\nsingularity at u = 1, but it can be evaluated using a\npolynomial or rational approximation on some interval\n[0, \u03c4 ], where \u03c4 < 1, in much the same way as the\nfunction g(u) of Section 3.\nThe refinements added by Kinderman and Monahan [9] and Leva [11] avoid most of the logarithm\nevaluations. The following step is added:\n\n6\n\nGRAND\n\n2.5. If P1 (u, v) then go to step 4\nelse if P2 (u, v) then go to step 1\nelse go to step 3.\n\nIt is hard to see how to implement GRAND efficiently on a vector processor. There are two problems \u2013\n\nOn serial machines GRAND [3] is competitive with\nthe Ratio method. In fact, GRAND is the fastest of\nthe methods compared by Leva [11]. GRAND is based\non an idea of Von Neumann and Forsythe for generating samples from a distribution with density function\nc exp(\u2212h(x)), where 0 \u2264 h(x) \u2264 1:\n1. Generate a uniform random number x, and set\nu0 \u2190 h(x).\n2. Generate independent uniform random numbers\nu1 , u2 , . . .\nuntil the first k > 0 such that uk\u22121 < uk .\n3. If k is odd then return x,\nelse reject x and go to step 1.\nA proof of correctness is given in Knuth [10].\n\n1. k is not bounded, even though its expected value\nis small. Thus, a sequence of gather operations\nseems to be required. The result would be similar\nto Petersen's implementation [16] of a generator\nfor the Poisson distribution (much slower than his\nimplementation for the normal distribution).\n\nHere P1 (u, v) and P2 (u, v) are easily-computed conditions. Geometrically, P1 corresponds to a region R1\nwhich lies inside C, and P2 corresponds to a region\nR2 which encloses C, but R1 and R2 have almost the\nsame area. Step 3 is only executed if (u, v) lies in the\nborderline region R2 \\R1 .\nStep 2.5 can be vectorised, but at the expense of\nseveral vector scatter/gather operations. Thus, the\nsaving in logarithm evaluations is partly cancelled out\nby an increase in overheads. The last column (R1)\nof Table 1 gives the times for our implementation on\nthe VP2200. As expected, the time for the logarithm\ncomputation is now negligible, and the overheads dominate. In percentage terms the times are:\n\n2. Because of the restriction 0 \u2264 h(x) \u2264\u221a\n1, the area\nunder the normal curve exp(\u2212x2 /2)/ 2\u03c0 has to\nbe split into different regions from which samples\nare drawn with probabilities proportional to their\nareas. This complicates the implementation of\nthe rejection step.\nFor these reasons we would expect a vectorised implementation of GRAND to be even slower than our\nimplementation of the Ratio method. Similar comments apply to other rejection methods which use an\niterative rejection process and/or several different regions.\n\n1% logarithm computation (using the library routine),\n17% uniform random number computation,\n23% scatter and gather to handle borderline region,\n\n7\n\n59% step 2.5 and other overheads.\n\nConclusion\n\nWe have shown that both the Box-Muller and Polar methods vectorise well, and that it is possible to\navoid and/or speed up the evaluation of the functions (sin, cos, ln, sqrt) which appear necessary. On\nthe VP2200/10 our best implementation of the Polar\nmethod takes 21.9 machine cycles per normal random\nnumber, slightly faster than our best implementation\nof the Box-Muller method (26.3 cycles).\n\nAlthough disappointing, the result for the Ratio\nmethod is not surprising, because the computations\nand overheads are similar to those for method P2\n(though with less logarithm computations), but only\nhalf as many normal random numbers are produced.\nThus, we would expect the Ratio method to be slightly\nbetter than half as fast as method P2, and this is what\nTable 1 shows.\n5\n\n\fWe also considered the vectorisation of some other\npopular methods for generating normally distributed\nrandom numbers, such as the Ratio method and the\nmethod of Von Neumann and Forsythe, and showed\nwhy such methods are unlikely to be faster than the\nPolar method on a vector processor.\n\n[10] D. E. Knuth, The Art of Computer Programming, Volume 2: Seminumerical Algorithms (second edition). Addison-Wesley, Menlo Park, 1981,\nSec. 3.4.1.\n[11] J. L. Leva, \"A fast normal random number generator\", ACM Transactions on Mathematical Software 18 (1992), 449-453.\n\nAcknowledgements\nThanks to Dr W. Petersen for his comments and\nhelpful information on implementations of random\nnumber generators on Cray and NEC computers [15,\n16, 17]. The ANU Supercomputer Facility provided\ntime on the VP 2200/10 for the development and testing of our implementation. This work was supported\nin part by a Fujitsu-ANU research agreement.\n\n[12] G. Marsaglia, B. Narasimhan and A. Zarif, A\nrandom number generator for PC's. Computer\nPhysics Communications, 60:345\u2013349, 1990.\n[13] G. Marsaglia and A. Zaman, A new class of random number generators. The Annals of Applied\nProbability, 1:462\u2013480, 1991.\n[14] M. E. Muller, A comparison of methods for generating normal variates on digital computers.\nJ. ACM 6:376\u2013383, 1959.\n\nReferences\n[1] J. H. Ahrens and U. Dieter, \"Computer methods for sampling from the exponential and normal distributions\", Communications of the ACM\n15 (1972), 873-882.\n\n[15] W. P. Petersen, Some vectorized random number\ngenerators for uniform, normal, and Poisson distributions for CRAY X-MP, J. Supercomputing,\n1:327\u2013335, 1988.\n\n[2] S. L. Anderson, \"Random number generators on\nvector supercomputers and other advanced architectures\", SIAM Review 32 (1990), 221-251.\n\n[16] W. P. Petersen, Lagged Fibonacci Series Random Number Generators for the NEC SX-3, IPS\nResearch Report No. 92-08, IPS, ETH-Zentrum,\nZurich, April 1992.\n\n[3] R. P. Brent, Algorithm 488: A Gaussian pseudorandom number generator (G5), Comm. ACM 17\n(1974), 704-706.\n\n[17] W. P. Petersen, Random Number Generators on\nVector Architectures, preprint, 1993.\n\n[4] R. P. Brent, Uniform random number generators\nfor supercomputers, Proc. Fifth Australian Supercomputer Conference, Melbourne, December\n1992, 95-104.\n[5] C. W. Clenshaw, L. Fox, E. T. Goodwin, D. W.\nMartin, J. G. L. Michel, G. F. Miller, F. W.\nJ. Olver and J. H. Wilkinson, Modern Computing Methods, 2nd edition, HMSO, London, 1961,\nCh. 8.\n[6] L. Devroye, Non-Uniform Random Variate Generation. Springer-Verlag, New York, 1986.\n[7] P. Griffiths and I. D. Hill (editors), Applied Statistics Algorithms, Ellis Horwood, Chichester, 1985.\n[8] F. James, A review of pseudorandom number\ngenerators. Computer Physics Communications,\n60:329\u2013344, 1990.\n[9] A. J. Kinderman and J. F. Monahan, \"Computer\ngeneration of random variables using the ratio of\nuniform deviates\", ACM Transactions on Mathematical Software 3 (1977), 257-260.\n6\n\n\f"}