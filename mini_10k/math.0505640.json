{"id": "http://arxiv.org/abs/math/0505640v1", "guidislink": true, "updated": "2005-05-30T10:14:34Z", "updated_parsed": [2005, 5, 30, 10, 14, 34, 0, 150, 0], "published": "2005-05-30T10:14:34Z", "published_parsed": [2005, 5, 30, 10, 14, 34, 0, 150, 0], "title": "Data-driven rate-optimal specification testing in regression models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0505543%2Cmath%2F0505650%2Cmath%2F0505273%2Cmath%2F0505346%2Cmath%2F0505010%2Cmath%2F0505326%2Cmath%2F0505080%2Cmath%2F0505403%2Cmath%2F0505239%2Cmath%2F0505620%2Cmath%2F0505614%2Cmath%2F0505480%2Cmath%2F0505604%2Cmath%2F0505228%2Cmath%2F0505070%2Cmath%2F0505166%2Cmath%2F0505511%2Cmath%2F0505438%2Cmath%2F0505193%2Cmath%2F0505002%2Cmath%2F0505498%2Cmath%2F0505419%2Cmath%2F0505343%2Cmath%2F0505488%2Cmath%2F0505294%2Cmath%2F0505603%2Cmath%2F0505594%2Cmath%2F0505121%2Cmath%2F0505510%2Cmath%2F0505320%2Cmath%2F0505078%2Cmath%2F0505474%2Cmath%2F0505356%2Cmath%2F0505034%2Cmath%2F0505277%2Cmath%2F0505557%2Cmath%2F0505234%2Cmath%2F0505307%2Cmath%2F0505260%2Cmath%2F0505516%2Cmath%2F0505325%2Cmath%2F0505667%2Cmath%2F0505025%2Cmath%2F0505600%2Cmath%2F0505371%2Cmath%2F0505670%2Cmath%2F0505456%2Cmath%2F0505404%2Cmath%2F0505232%2Cmath%2F0505570%2Cmath%2F0505110%2Cmath%2F0505033%2Cmath%2F0505627%2Cmath%2F0505610%2Cmath%2F0505360%2Cmath%2F0505532%2Cmath%2F0505197%2Cmath%2F0505530%2Cmath%2F0505470%2Cmath%2F0505580%2Cmath%2F0505664%2Cmath%2F0505640%2Cmath%2F0505591%2Cmath%2F0505661%2Cmath%2F0505218%2Cmath%2F0505347%2Cmath%2F0505377%2Cmath%2F0505286%2Cmath%2F0505009%2Cmath%2F0505292%2Cmath%2F0505003%2Cmath%2F0505041%2Cmath%2F0505313%2Cmath%2F0505231%2Cmath%2F0505501%2Cmath%2F0505043%2Cmath%2F0505127%2Cmath%2F0505179%2Cmath%2F0505345%2Cmath%2F0505616%2Cmath%2F0505200%2Cmath%2F0505442%2Cmath%2F0505256%2Cmath%2F0505322%2Cmath%2F0505128%2Cmath%2F0505508%2Cmath%2F0505116%2Cmath%2F0505036%2Cmath%2F0505407%2Cmath%2F0505162%2Cmath%2F0505342%2Cmath%2F0505631%2Cmath%2F0505044%2Cmath%2F0505052%2Cmath%2F0505494%2Cmath%2F0505125%2Cmath%2F0505531%2Cmath%2F0505097%2Cmath%2F0505351%2Cmath%2F0505309%2Cmath%2F0505138&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Data-driven rate-optimal specification testing in regression models"}, "summary": "We propose new data-driven smooth tests for a parametric regression function.\nThe smoothing parameter is selected through a new criterion that favors a large\nsmoothing parameter under the null hypothesis. The resulting test is adaptive\nrate-optimal and consistent against Pitman local alternatives approaching the\nparametric model at a rate arbitrarily close to 1/\\sqrtn. Asymptotic critical\nvalues come from the standard normal distribution and the bootstrap can be used\nin small samples. A general formalization allows one to consider a large class\nof linear smoothing methods, which can be tailored for detection of additive\nalternatives.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0505543%2Cmath%2F0505650%2Cmath%2F0505273%2Cmath%2F0505346%2Cmath%2F0505010%2Cmath%2F0505326%2Cmath%2F0505080%2Cmath%2F0505403%2Cmath%2F0505239%2Cmath%2F0505620%2Cmath%2F0505614%2Cmath%2F0505480%2Cmath%2F0505604%2Cmath%2F0505228%2Cmath%2F0505070%2Cmath%2F0505166%2Cmath%2F0505511%2Cmath%2F0505438%2Cmath%2F0505193%2Cmath%2F0505002%2Cmath%2F0505498%2Cmath%2F0505419%2Cmath%2F0505343%2Cmath%2F0505488%2Cmath%2F0505294%2Cmath%2F0505603%2Cmath%2F0505594%2Cmath%2F0505121%2Cmath%2F0505510%2Cmath%2F0505320%2Cmath%2F0505078%2Cmath%2F0505474%2Cmath%2F0505356%2Cmath%2F0505034%2Cmath%2F0505277%2Cmath%2F0505557%2Cmath%2F0505234%2Cmath%2F0505307%2Cmath%2F0505260%2Cmath%2F0505516%2Cmath%2F0505325%2Cmath%2F0505667%2Cmath%2F0505025%2Cmath%2F0505600%2Cmath%2F0505371%2Cmath%2F0505670%2Cmath%2F0505456%2Cmath%2F0505404%2Cmath%2F0505232%2Cmath%2F0505570%2Cmath%2F0505110%2Cmath%2F0505033%2Cmath%2F0505627%2Cmath%2F0505610%2Cmath%2F0505360%2Cmath%2F0505532%2Cmath%2F0505197%2Cmath%2F0505530%2Cmath%2F0505470%2Cmath%2F0505580%2Cmath%2F0505664%2Cmath%2F0505640%2Cmath%2F0505591%2Cmath%2F0505661%2Cmath%2F0505218%2Cmath%2F0505347%2Cmath%2F0505377%2Cmath%2F0505286%2Cmath%2F0505009%2Cmath%2F0505292%2Cmath%2F0505003%2Cmath%2F0505041%2Cmath%2F0505313%2Cmath%2F0505231%2Cmath%2F0505501%2Cmath%2F0505043%2Cmath%2F0505127%2Cmath%2F0505179%2Cmath%2F0505345%2Cmath%2F0505616%2Cmath%2F0505200%2Cmath%2F0505442%2Cmath%2F0505256%2Cmath%2F0505322%2Cmath%2F0505128%2Cmath%2F0505508%2Cmath%2F0505116%2Cmath%2F0505036%2Cmath%2F0505407%2Cmath%2F0505162%2Cmath%2F0505342%2Cmath%2F0505631%2Cmath%2F0505044%2Cmath%2F0505052%2Cmath%2F0505494%2Cmath%2F0505125%2Cmath%2F0505531%2Cmath%2F0505097%2Cmath%2F0505351%2Cmath%2F0505309%2Cmath%2F0505138&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose new data-driven smooth tests for a parametric regression function.\nThe smoothing parameter is selected through a new criterion that favors a large\nsmoothing parameter under the null hypothesis. The resulting test is adaptive\nrate-optimal and consistent against Pitman local alternatives approaching the\nparametric model at a rate arbitrarily close to 1/\\sqrtn. Asymptotic critical\nvalues come from the standard normal distribution and the bootstrap can be used\nin small samples. A general formalization allows one to consider a large class\nof linear smoothing methods, which can be tailored for detection of additive\nalternatives."}, "authors": ["Emmanuel Guerre", "Pascal Lavergne"], "author_detail": {"name": "Pascal Lavergne"}, "author": "Pascal Lavergne", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/009053604000001200", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/math/0505640v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0505640v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published at http://dx.doi.org/10.1214/009053604000001200 in the\n  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62G10 (Primary) 62G08. (Secondary)", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0505640v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0505640v1", "journal_reference": "Annals of Statistics 2005, Vol. 33, No. 2, 840-870", "doi": "10.1214/009053604000001200", "fulltext": "arXiv:math/0505640v1 [math.ST] 30 May 2005\n\nThe Annals of Statistics\n2005, Vol. 33, No. 2, 840\u2013870\nDOI: 10.1214/009053604000001200\nc Institute of Mathematical Statistics, 2005\n\nDATA-DRIVEN RATE-OPTIMAL SPECIFICATION TESTING IN\nREGRESSION MODELS1\nBy Emmanuel Guerre and Pascal Lavergne\nLSTA Paris 6 and University of Toulouse GREMAQ and INRA\nWe propose new data-driven smooth tests for a parametric regression function. The smoothing parameter is selected through a\nnew criterion that favors a large smoothing parameter under the null\nhypothesis. The resulting test is adaptive rate-optimal and consistent against Pitman local alternatives approaching the parametric\n\u221a\nmodel at a rate arbitrarily close to 1/ n. Asymptotic critical values\ncome from the standard normal distribution and the bootstrap can\nbe used in small samples. A general formalization allows one to consider a large class of linear smoothing methods, which can be tailored\nfor detection of additive alternatives.\n\n1. Introduction. Consider n observations (Yi , Xi ) in R \u00d7 Rp and the heteroscedastic regression model with unknown mean m(*) and variance \u03c3 2 (*),\nYi = m(Xi ) + \u03b5i ,\n\nE[\u03b5i |Xi ] = 0 and\n\nVar[\u03b5i |Xi ] = \u03c3 2 (Xi ).\n\nWe want to test the hypothesis that the regression belongs to some parametric family {\u03bc(*; \u03b8); \u03b8 \u2208 \u0398}, that is,\n(1.1)\n\nH0 : m(*) = \u03bc(*; \u03b8)\n\nfor some \u03b8 \u2208 \u0398.\n\nTests of H0 are called lack-of-fit tests or specification tests. Based on smoothing techniques, many consistent tests of H0 have been proposed, the so-called\nsmooth tests; see Hart (1997) for a review. A fundamental issue is the choice\nof the smoothing parameter. Since this is a model selection problem, Eubank and Hart (1992), Ledwina (1994), Hart [(1997), Chapter 7] and Aerts,\nClaeskens and Hart (1999, 2000), among others, have proposed use of criteria developed by Akaike (1973) and Schwarz (1978). However, these criteria\nReceived July 2003; revised April 2004.\nSupported by LSTA and INRA.\nAMS 2000 subject classifications. Primary 62G10; secondary 62G08.\nKey words and phrases. Hypothesis testing, nonparametric adaptive tests, selection\nmethods.\n1\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2005, Vol. 33, No. 2, 840\u2013870. This reprint differs from the original in pagination\nand typographic detail.\n1\n\n\f2\n\nE. GUERRE AND P. LAVERGNE\n\nare tailored for estimation but not for testing purposes. Hence, they do not\nyield adaptive rate-optimal tests, that is, tests that detect alternatives of unknown smoothness approaching the null hypothesis at the fastest possible\nrate when the sample size grows; see Spokoiny (1996).\nMany adaptive rate-optimal specification tests are based on the maximum\napproach, which consists of choosing as a test statistic the maximum of\nStudentized statistics associated with a sequence of smoothing parameters.\nThis approach is used for testing the white noise model with normal errors\nby Fan (1996) and for testing a linear regression model with normal errors by\nFan and Huang (2001) and Baraud, Huet and Laurent (2003), who extend\nthe maximum approach. Further work on the linear model includes Spokoiny\n(2001) under homoscedastic errors and Zhang (2003) under heteroscedastic\nerrors. Finally, Horowitz and Spokoiny (2001) deal with the general case of\na nonlinear model with heteroscedastic errors.\nWe reconsider the model selection approach to propose a new test with\nsome distinctive features. First, our data-driven choice of the smoothing parameter relies on a specific criterion tailored for testing purposes. This yields\nan adaptive rate-optimal test. Second, the criterion favors a baseline statistic\nunder the null hypothesis. This results in a simple asymptotic distribution\nfor our statistic and in bounded critical values for our test. By contrast, in\nthe maximum approach, critical values diverge and must practically be evaluated by simulation for any sample size. The computational burden of this\ntask can be heavy for a large sample size and a large number of statistics.\nMoreover, diverging critical values are expected to yield some loss of power\ncompared to our test. In particular, from an asymptotic viewpoint, our test\ndetects local Pitman alternatives converging to the null at a faster rate than\nthe ones detected by a maximum test. In small samples, our simulations\nshow that our test has better power than a maximum test against irregular\nalternatives.\nIn our work we allow for a nonlinear parametric regression model with\nmutidimensional covariates, nonnormal errors and heteroscedasticity of unknown form. In Section 2 we describe the specific aspects of our testing procedure. In Section 3 we detail the practical construction of the test statistic\nfor three types of smoothing procedures. Then we give our assumptions and\nmain results, which concern the null asymptotic behavior of the test, adaptive rate-optimality, and detection of Pitman local alternatives. In Section 4\nwe prove the validity of a bootstrap method and compare the small sample\nperformances of our test with a maximum test through a simulation experiment. In Section 5 we extend our results to general linear smoothing methods. Finally, we propose a test whose power against additive alternatives is\nnot affected by the curse of dimensionality. Proofs are given in Section 6.\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n3\n\n2. Description of the procedure. Consider a collection {Tbh , h \u2208 Hn } of\nasymptotically centered statistics which measures the lack-of-fit of the null\nparametric model. The index h is a smoothing parameter, chosen in a discrete grid whose cardinality grows with the sample size n; see our examples in\nthe next section. A maximum test rejects H0 when maxh\u2208Hn Tbh /vbh \u2265 z\u03b1max ,\nwhere vbh estimates the asymptotic null standard deviation of Tbh . A test in\nthe spirit of Baraud, Huet and Laurent (2003) rejects the null if Tbh \u2265 vbh z\u03b1 (h)\nfor some h in Hn or, equivalently, if maxh\u2208Hn (Tbh /vbh \u2212 z\u03b1 (h)) > 0, where the\ncritical values are chosen to get an asymptotic \u03b1-level test, a difficult issue in\npractice. Setting z\u03b1 (h) = z\u03b1max yields a maximum test. Because the number\nh increases with n, z\u03b1max diverges.\nOn an informal basis, our approach favors a baseline statistic Tbh0 with\nlowest variance among the Tbh . In practice, Tbh0 can be designed to yield\nhigh power against parametric or regular alternatives that are of primary\ninterest for the statistician. However, this statistic may not be powerful\nenough against nonparametric or irregular alternatives. We then propose to\ncombine this baseline statistic with the other statistics Tbh in the following\nway. Let vbh,h0 be some positive estimators of the asymptotic null standard\ndeviation of Tbh \u2212 Tbh0 . We select h as\n(2.1)\n\ne\nh = arg max {Tbh \u2212 \u03b3n vbh,h0 }\nh\u2208Hn\n\n= arg max {Tbh \u2212 Tbh0 \u2212 \u03b3n vbh,h0 }\nh\u2208Hn\n\nOur test is\n\nwhere \u03b3n > 0.\n\nReject H0 when Tbeh /vbh0 \u2265 z\u03b1 ,\nwhere z\u03b1 is the quantile of order (1 \u2212 \u03b1) of a standard normal.\nThe distinctive features of our approach are as follows. First, our criterion\npenalizes each statistic by a quantity proportional to its standard deviation,\nwhile the criteria reviewed in Hart (1997) use a larger penalty proportional\nto the variance. Second, the data-driven choice of the smoothing parameter\nfavors h0 under the null hypothesis. Indeed, since Tbh \u2212 Tbh0 is of order vbh,h0\nunder H0 , e\nh = h0 asymptotically under H0 if \u03b3n diverges fast enough; see\nTheorem 1 below. Hence, the null limit distribution of the test statistic is\nthe one of Tbh0 /vbh0 , that is, the standard normal, and the resulting test\nhas bounded critical values. Third, our selection procedure allows us to\nchoose the standardization vbh0 . We could use vbeh instead, which also gives an\nasymptotic \u03b1-level test since h\u0303 = h0 asymptotically under H0 . But, because\nvbh \u2265 vbh0 asymptotically for any admissible h, our standardization gives a\nlarger critical region under the alternative. This increases power at no cost\nfrom an asymptotic viewpoint; see Fan (1996) for a similar device in wavelet\nthresholding tests. Our simulation results show that this effect is already\n(2.2)\n\n\f4\n\nE. GUERRE AND P. LAVERGNE\n\nlarge in small samples. By contrast, the maximum approach systematically\ndownweights the statistic Tbh with its standard deviation.\nThird, compared to a test using a single statistic, our test inherits the\npower properties of each of the Tbh , up to a term \u03b3n vbh,h0 . Indeed, the definition of e\nh yields\nTbeh = max (Tbh \u2212 \u03b3n vbh,h0 ) + \u03b3n vbeh,h \u2265 Tbh \u2212 \u03b3n vbh,h0\nh\u2208Hn\n\nfor any h \u2208 Hn .\n\n0\n\nAs a consequence, a lower bound for the power of the test is\n(2.3)\n\nP(Tbeh \u2265 vbh0 z\u03b1 ) \u2265 P(Tbh \u2265 vbh0 z\u03b1 + \u03b3n vbh,h0 )\n\nfor any h in Hn .\n\nUsing a penalty proportional to a standard deviation yields a better power\nbound than the selection criteria reviewed in Hart (1997). A suitable choice\nof the smoothing parameter in the latter power bound allows us to establish the adaptive rate-optimality of the test; see Theorem 2 below and the\nfollowing discussion. Fourth, combining the Tbh with our selection procedure\ngives a more powerful test than using the baseline statistic Tbh0 . Indeed, since\nvbh0 ,h0 = 0, a noteworthy implication of (2.3) is\n(2.4)\n\nP(Tbeh \u2265 vbh0 z\u03b1 ) \u2265 P(Tbh0 \u2265 vbh0 z\u03b1 ).\n\nTheorem 3 below uses the latter inequality to study detection of Pitman\nlocal alternatives approaching the null at a faster rate than in Horowitz and\nSpokoiny (2001).\n3. Main results. For any integer q and any x \u2208 Rq , |x| = max1\u2264i\u2264q |xi |.\nFor real deterministic sequences, an \u224d bn means that an and bn have the\nsame exact order, that is, there is a C > 1 with 1/C \u2264 an /bn \u2264 C for n large\nenough. For real random variables, An \u224dP Bn means that P(1/C \u2264 An /Bn \u2264\nC) goes to 1 when n grows. In such statements, uniformity with respect\nto a variable means that C can be chosen independently of it. A sequence\n{mn (*)}n\u22651 is equicontinuous if, for any \u01eb > 0, there is an \u03b7 > 0 such that\nsupn\u22651 |mn (x) \u2212 mn (x\u2032 )| \u2264 \u01eb for all x, x\u2032 with |x \u2212 x\u2032 | \u2264 \u03b7.\n3.1. Construction of the statistics and assumptions. Let \u03b8bn be the nonlinear least-squares estimator of \u03b8 in model (1.1), that is,\n(3.1)\n\n\u03b8bn = arg min\n\u03b8\u2208\u0398\n\nn\nX\ni=1\n\n(Yi \u2212 \u03bc(Xi ; \u03b8))2 ,\n\nwith an appropriate convention in case of ties. A typical statistic Tbh is an\nestimator of the mean-squared distance of the regression function from the\nparametric model\n(3.2)\n\nmin\n\u03b8\u2208\u0398\n\nn\nX\ni=1\n\n(mn (Xi ) \u2212 \u03bc(Xi ; \u03b8))2 .\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n5\n\nbi = Yi \u2212 \u03bc(Xi ; \u03b8bn ) = m(Xi ) \u2212\nFrom the estimated parametric residuals U\n\u03bc(Xi ; \u03b8bn ) + \u03b5i , i = 1, . . . , n, we can estimate the departure from the parametric regression using a leave-one-out linear nonparametric estimator \u03b4bh (Xi ) =\nPn\nb\nj=1,j6=i \u03bdij (h)Uj based on some weights \u03bdij (h) with smoothing parameter\nh. Then (3.2) can be estimated as\n\n(3.3)\n\nTbh =\n\nn\nX\ni=1\n\nbi \u03b4b (Xi ) =\nU\nh\n\nX\n\n\u03bdij (h) + \u03bdji (h) b b\nb \u2032W U\nb\nUi Uj = U\nh ,\n2\n1\u2264i6=j\u2264n\n\nb = [U\nb1 , . . . , U\nbn ]\u2032 and the generic element of Wh is wij (h) = (\u03bdij (h) +\nwhere U\n\u03bdji (h))/2 for i 6= j and wii (h) = 0. Such a Tbh is asymptotically normal under H0 ; see, for example, de Jong (1987). Examples 1a and 1b come from\nprojection methods, while Example 2 builds on kernel smoothing.\n\nExample 1a (Regression on multivariate polynomial functions). Let\nQ\n\u03c8k (x) = pl=1 xkl l , for k \u2208 Np with |k| = maxl=1,...,p kl \u2264 1/h. Let \u03a8h = [\u03c8k (Xi ), |k| \u2264\n1/h, i = 1, . . . , n] and Ph = \u03a8h (\u03a8\u2032h \u03a8h )\u22121 \u03a8\u2032h be the n \u00d7 n orthogonal projection matrix onto the linear subspace of Rn spanned by \u03a8h . The matrix Wh\nis obtained from Ph by setting its diagonal elements to zero.\nExample 1b (Regression on piecewise polynomial functions). Under\nthe assumption that the support of X is [0, 1]p , we consider\npiecewise polyQ\nnomial functions of fixed order q over bins Ik (h) = pl=1 [kl h, (kl + 1)h),\nk = (k1 , . . . , kp ), kl = 0, . . . , (1/h) \u2212 1. These functions write\n\u03c8qkh (x) =\n\np\nY\n\nl=1\n\nxql l I(x \u2208 Ik (h)),\n\n0 \u2264 |q| = max ql \u2264 q\u0304, 1 \u2264 |k| = max kl \u2264 1/h.\n1\u2264l\u2264p\n\n1\u2264l\u2264p\n\nThe particular choice q\u0304 = 0 corresponds to the regressogram. The matrix\nWh is constructed as in Example 1a.\nExample 2 (Kernel smoothing). Consider a continuous, nonnegative,\nsymmetric and bounded kernel K(*) from Rp that integrates to 1 and has\na positive integrable Fourier transform. These conditions hold for products of the triangular, normal, Laplace or Cauchy kernels. Define Kh (x) =\nK(x1 /h, . . . , xp /h). We consider\nTbh =\n\nX\n\n1\nbj\nbi qKh (Xi \u2212 Xj ) U\nU\np\n(n\n\u2212\n1)h\nb\nb\nf (X )f (X )\n1\u2264i6=j\u2264n\n\nwith fbh (Xi ) =\n\nh\n\ni\n\nh\n\nj\n\nX\n1\nKh (Xj \u2212 Xi ).\n(n \u2212 1)hp j6=i\n\n\f6\n\nE. GUERRE AND P. LAVERGNE\n\nWe now turn to variance estimation. The leave-one-out construction of\n2\nthe Tbh gives that the asymptotic conditional variances vh2 and vh,h\nof Tbh\n0\nand Tbh \u2212 Tbh0 under H0 are\nX\n\nvh2 = 2\n\n(3.4)\n\n2\nvh,h\n0\n\n=2\n\n2\nwij\n(h)\u03c3 2 (Xi )\u03c3 2 (Xj ),\n\n1\u2264i,j\u2264n\nX\n\n(wij (h) \u2212 wij (h0 ))2 \u03c3 2 (Xi )\u03c3 2 (Xj ).\n\n1\u2264i,j\u2264n\n\nFor our main examples,\nvh2 0 \u224dP h\u2212p\n0\n\nand\n\n2\nvh,h\n\u224dP h\u2212p \u2212 h\u2212p\n0 ;\n0\n\nsee Proposition 2 in Section 6. Let \u03c3 2 (*) be a nonparametric estimator of\nb 2 (*) such that\n\u03c3\nmax\n\n(3.5)\n\n1\u2264i\u2264n\n\nbn2 (Xi )\n\u03c3\n\u2212 1 = oP (1)\n\u03c3 2 (Xi )\n\nfor any equicontinuous sequence of regression functions. For instance, let\n\n(3.6)\n\nbn2 (Xi ) =\n\u03c3\n\nPn\n\n2\nj=1 Yj I(|Xj\n\n\u2212 Xi | \u2264 bn )\nj=1 I(|Xj \u2212 Xi | \u2264 bn )\n!2\nPn\nj=1 Yj I(|Xj \u2212 Xi | \u2264 bn )\nPn\n\u2212\n,\nj=1 I(|Xj \u2212 Xi | \u2264 bn )\nPn\n\nwhere bn is a bandwidth parameter chosen independently of Hn such that\n\u2032\nn1\u22124/d bpn diverges; see Proposition 3 in Section 6. Consistent estimators of\nthe variances in (3.4) are\nvbh20 = 2\n\n2\nvbh,h\n=2\n0\n\nX\n\n1\u2264i,j\u2264n\n\nX\n\nbn2 (Xi )\u03c3\nbn2 (Xj ),\nwij2 (h0 )\u03c3\n\nbn2 (Xj ).\nbn2 (Xi )\u03c3\n(wij (h) \u2212 wij (h0 ))2 \u03c3\n\n1\u2264i,j\u2264n\n\nFinally, for the sake of parsimony, and following Horowitz and Spokoiny\n(2001), Lepski, Mammen and Spokoiny (1997) and Spokoiny (2001), the\nset Hn of admissible smoothing parameters is a geometric grid of Jn + 1\nsmoothing parameters,\n(3.7) Hn = {hj = h0 a\u2212j , j = 0, . . . , Jn }\n\nfor some a > 1, Jn \u2192 +\u221e.\n\nNote that h0 can depend on an empirical measure of the dispersion of the\nXi , as in Zhang (2003), and can converge to zero very slowly, say, as 1/ ln n.\nWe assume the following:\nAssumption D.\ndensity over [0, 1]p .\n\nThe i.i.d. Xi \u2208 [0, 1]p have a strictly positive continuous\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n7\n\nAssumption M. The function \u03bc(x; \u03b8) is continuous with respect to x in\n[0, 1]p and \u03b8 in \u0398, where \u0398 is a compact subset of Rd . There is a constant \u03bc\u0307\nsuch that for all \u03b8, \u03b8 \u2032 in \u0398 and for all x in [0, 1]p , |\u03bc(x; \u03b8)\u2212 \u03bc(x; \u03b8 \u2032 )| \u2264 \u03bc\u0307|\u03b8 \u2212 \u03b8 \u2032 |.\nAssumption E. The \u03b5i are independent given X1 , . . . , Xn . For each i,\nthe distribution of \u03b5i , given the design, depends only on Xi , E[\u03b5i |Xi ] = 0 and\nVar[\u03b5i |Xi ] = \u03c3 2 (Xi ), where the unknown variance function \u03c3 2 (*) is continu\u2032\n\u2032\nous and bounded away from 0. For some d\u2032 > max(d, 4), E1/d [|\u03b5i |d |Xi ] < C1\nfor all i.\nAssumption W. (i) For any h, the matrix Wh is one from Example 1a,\n1b or 2. (ii) The set Hn is as in (3.7) with hJn \u224d (ln n)C2 /p n\u22122/(4s+p) , for\nsome C2 > 1, with s = 5p/4 in Example 1a and s = p/4 in Examples 1b and\n2. The number a is an integer for Example 1b.\nUnder Assumption M, the value of the parameter \u03b8 may not be identified, as in mixture or multiple index models. The restriction on hJn , together\nwith the definition of Hn , implies that the number Jn + 1 of smoothing parameters is of order ln n at most. Assumption W(i), which considers specific\nnonparametric methods, will be relaxed in Section 5.1, allowing us, in particular, to consider a baseline statistic Tbh0 designed for specific parametric\nalternatives.\n3.2. Limit behavior of the test under the null hypothesis.\nThe next the\u221a\norem allows for a penalty sequence \u03b3n of exact order 2 ln ln n, as Jn is of\norder ln n.\nTheorem 1. Consider a sequence {\u03bc(*, \u03b8n ), \u03b8n \u2208 \u0398}n\u22651 in H0 . Let Assumptions D, M, E and W hold and assume that the variance estimator\nsatisfies (3.5). If h0 \u2192 0 and \u03b3n \u2192 \u221e with\n(3.8)\n\np\n\n\u03b3n \u2265 (1 + \u03b7) 2 ln Jn\n\nfor some \u03b7 > 0,\n\nthe test (2.2) has level \u03b1 asymptotically given the design, that is,\nP\n\nP(Tbeh \u2265 z\u03b1 vbh0 |X1 , . . . , Xn ) \u2192 \u03b1.\n\nTheorem 1 is proved in two main steps. The first step consists in showing\nthat\n(3.9)\n\ne 6= h0 ) = P\nP(h\n\n\u0012\n\nTbh \u2212 Tbh0\n> \u03b3n\nmax\nbh,h0\nh\u2208Hn \\{h0 } v\n\n\u0013\n\ngoes to zero. This is done by first proving that (Tbh \u2212 Tbh0 )/vbh,h0 asymptotically behaves at first-order as \u03b5\u2032 (Wh \u2212 Wh0 )\u03b5/vh,h0 uniformly for h in\n\n\f8\n\nE. GUERRE AND P. LAVERGNE\n\nHn \\ {h0 }, where \u03b5 = [\u03b51 , . . . , \u03b5n ]\u2032 , and second by bounding the distribution\ntails of maxh\u2208Hn \\{h0 } \u03b5\u2032 (Wh \u2212 Wh0 )\u03b5/vh,h0 . Then we show that the limit distribution of Tbh0 /vbh0 is that of \u03b5\u2032 Wh0 \u03b5/vh0 , which converges to a standard\nnormal when h0 goes to 0.\nAs done by Horowitz and Spokoiny (2001), Theorem 1 imposes that h0\nasymptotically vanishes. This condition yields a pivotal limit distribution\nfor our test statistic. As shown by Hart [(1997), page 220] under stronger\nregularity conditions on the parametric model, considering a fixed h0 generally yields a nonpivotal limit distribution because the estimation error\n\u03bc(*; \u03b8bn ) \u2212 \u03bc(*; \u03b8) cannot be neglected. Hart (1997) then recommends the use\nof a double bootstrap procedure to estimate the critical values of the test.\n3.3. Consistency of the test. Theorem 2 below considers general alternatives with unknown smoothness. Theorem 3 considers Pitman local alternatives. For any real s, let \u230as\u230b be the lower integer part of s, that is,\n\u230as\u230b < s \u2264 \u230as\u230b + 1. Let the H\u00f6lder class Cp (L, s) be the set of maps m(*) from\n[0, 1]p to R with\nCp (L, s) = {m(*); |m(x) \u2212 m(y)| \u2264 L|x \u2212 y|s for all x, y in [0, 1]p }\nfor s \u2208 (0, 1],\nCp (L, s) = {m(*); the \u230as\u230bth partial derivatives of m(*) are in Cp (L, s \u2212 \u230as\u230b)}\nfor s > 1.\nTheorem 2. Consider a sequence of equicontinuous regression functions\n{mn (*)}n\u22651 such that for some unknown s > s and L > 0, mn (*) \u2212 \u03bc(*; \u03b8) \u2208\nCp (L, s) for all \u03b8 in \u0398 and all n. Let Assumptions D, M, E and W hold.\nAssume that the variance estimator satisfies (3.5), that 1/(C0 ln n) \u2264 h0 \u2264\nC0 for some C0 > 0 and that \u03b3n \u2264 n\u03b3 for some \u03b3 in (0, 1). If\n\"\n\n#1/2\n\nn\n1X\n(mn (Xi ) \u2212 \u03bc(Xi ; \u03b8))2\nmin\n\u03b8\u2208\u0398 n\ni=1\n(3.10)\n\u0013\n\u0012\n\u03b3n supx\u2208[0,1]p \u03c3 2 (x) 2s/(4s+p)\np/(4s+p)\n,\n\u2265 (1 + oP (1))\u03ba1 L\nn\n\nthe test (2.2) is consistent given the design, that is,\nP\n\nP(Tbeh \u2265 vbh0 z\u03b1 |X1 , . . . , Xn ) \u2192 1,\n\nprovided \u03ba1 = \u03ba1 (s) > 0 is large enough.\n\nThe proof is based upon the power bound (2.3). From this inequality, the\ntest is consistent if Tbh \u2212 z\u03b1 vbh0 \u2212 \u03b3n vbh,h0 diverges in probability for a suitable\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n9\n\nchoice of the smoothing parameter h adapted to the unknown smoothness of\nthe departure from the parametric model. Thus, combining several statistics\nin the procedure is crucial to detecting alternatives of unknown smoothness.\nA sketch of the proof is as follows. For a departure from the parametric model\nP\nin Cp (L, s), Tbh estimates min\u03b8\u2208\u0398 ni=1 (mn (Xi ) \u2212 \u03bc(Xi ; \u03b8))2 up to a multiplicative constant with a bias of order nL2 h2s . The standard deviation of Tbh\nis of order h\u2212p/2 and the order of vbh0 z\u03b1 + \u03b3n vbh,h0 is \u03b3n h\u2212p/2 supx\u2208[0,1]p \u03c3 2 (x).\nCollecting the leading terms shows that Tbh \u2212 vbh0 z\u03b1 \u2212 \u03b3n vbh,h0 diverges if\n\"\n\nn\n1X\nmin\n(mn (Xi ) \u2212 \u03bc(Xi ; \u03b8))2\n\u03b8\u2208\u0398 n\ni=1\n\n#1/2\n\nis of larger order than\n\u0014 \u0012\n\n\u0013\u00151/2\n\n1\nnL2 h2s + \u03b3n h\u2212p/2 sup \u03c3 2 (x)\nn\nx\u2208[0,1]p\n\n.\n\nFinding the minimum of this quantity with respect to h gives the rate of\n(3.10). The rate of the optimal h is (\u03b3n inf x\u2208[0,1]p \u03c3 2 (x)/L2 n)2/(4s+p) . The\nparsimonious set Hn is rich enough to contain an h of this order. Our proof\ncan be easily modified to study the selection procedures considered in Hart\n(1997), which use \u03b3n vbh2 in (2.1) instead of \u03b3n vbh,h0 . This would give the worst\ns/(2s+p) .\ndetection rate (\u03b3n /n)\n\u221a\nwith Theorem 1,\nFor \u03b3n of order ln ln n, the smallest order compatible\n\u221a\nthe test detects alternatives (3.10) with rate ( ln ln n/n)2s/(4s+p) for any\ns > s. This rate is the optimal adaptive minimax one for the idealistic white\nnoise model; see Spokoiny (1996). Horowitz and Spokoiny (2001) obtain the\nsame rate for their kernel-based test but with minimal smoothness index\ns = max(2, p/4), while we achieve s = p/4 for our piecewise polynomial or\nkernel-based tests. The value p/4 is critical for the smoothness index s, as\npreviously noted by Guerre and Lavergne (2002) and Baraud, Huet and\nLaurent (2003).\nTheorem 3. Let \u03b80 be an inner point of \u0398 and consider a sequence of\nlocal alternatives mn (*) = \u03bc(*; \u03b80 ) + rn \u03b4n (*), where {\u03b4n (*)}n\u22651 is an equicontinuous sequence from Cp (L, s) for some unknown s > s and L > 0, with\n(3.11)\n\nn\n1X\n\u03b42 (Xi ) = 1 + oP (1)\nn i=1 n\n\nand\n\nn\n1X\n\u2202\u03bc(Xi ; \u03b80 )\n= oP (1).\n\u03b4n (Xi )\nn i=1\n\u2202\u03b8\n\nAssume that for each x in [0, 1]p , \u03bc(x; \u03b8) is twice differentiable with respect\nto \u03b8 in \u0398 with second-order derivatives continuous in x and \u03b8 and that, for\n\n\f10\n\nE. GUERRE AND P. LAVERGNE\n\nsome C3 > 0,\n(3.12)\n\n(C3 + oP (1))|\u03b8 \u2212 \u03b8 \u2032 |2\nn\n1X\n(\u03bc(Xi ; \u03b8) \u2212 \u03bc(Xi ; \u03b8 \u2032 ))2\n\u2264\nn i=1\n\nfor any \u03b8, \u03b8 \u2032 in \u0398.\n\nLet Assumptions D, M, E and W hold and assume\nthat the variance esq\n\ntimator satisfies (3.5). If h0 \u2192 0, rn \u2192 0 and\nconsistent given the design.\n\np/2\n\nnh0 rn \u2192 \u221e, the test is\n\n\u221a\nThe rate rn of Theorem 3 can be made arbitrarily close to 1/ n by a\nproper choice of h0 .\u221aThis improves\nupon Horowitz and Spokoiny (2001),\n\u221a\nwho obtain the rate ln ln n/ n.\nAs stated in Lemma 5 of Section 6, conditions (3.11) and the identification\ncondition (3.12) ensure that\n(3.13)\n\n\"\n\nn\n1X\n(mn (Xi ) \u2212 \u03bc(Xi ; \u03b8))2\nmin\n\u03b8\u2208\u0398 n\ni=1\n\n#1/2\n\n= rn \u2212 oP (rn ).\n\nAs the minimum of (3.13) is achieved for \u03b8 = \u03b80 at first-order, rn \u03b4n (*)\nis asymptotically the departure from \u03bc(*; \u03b80 ). When rn converges to zero,\nthis departure becomes smoother as it belongs to the smoothness class\nCp (Lrn , s). This sharply contrasts with the departures from the parametric model in Theorem 2, which can be much more irregular. The proof of\nTheorem 3 follows from (2.4). The test is consistent as soon as Tbh0 \u2212 vbh0 z\u03b1\ndiverges in probability.\nWe show that Tbh0 is, up to a multiplicative constant,\nPn\n2\nan estimate of rn i=1 \u03b4n2 (Xi ) with a negligible bias and a standard devia\u2212p/2\n\u2212p/2\ntion of order h0 . As vbh0 is of order h0 , Tbh0 \u2212 vbh0 z\u03b1 diverges to infinity\n\u2212p/2\nas required.\nas soon as nrn2 diverges faster than h0\n4. Bootstrap implementation and small sample behavior.\n4.1. Bootstrap critical values. The wild bootstrap, initially proposed by\nWu (1986), is often used in smooth lack-of-fit tests to compute small sample critical values; see, for example, H\u00e4rdle and Mammen (1993). Here\nwe use a generalization of this method, the smooth conditional moments\nbootstrap introduced by Gozalo (1997). It consists of drawing n i.i.d. random variables \u03c9i independently from the original sample with E\u03c9i = 0,\n\u2032\nE\u03c9i2 = 1 and E|\u03c9i |d < \u221e, and generating bootstrap observations of Yi as\nbn (Xi )\u03c9i , i = 1, . . . , n. A bootstrap test statistic Tb \u2217\u2217 /vbh\u2217 0 is\nYi\u2217 = \u03bc(Xi , \u03b8bn ) + \u03c3\nh\u0303\nbuilt from the bootstrap sample, as was the original test statistic. When this\n\u2217\nscheme is repeated many times, the bootstrap critical value z\u03b1,n\nat level \u03b1 is\nthe empirical 1 \u2212 \u03b1 quantile of the bootstrapped test statistics. This critical\n\n\f11\n\nDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\nvalue is then compared to the initial test statistic. The following theorem\nestablishes the first-order consistency of this procedure.\nTheorem 4. Let Yi = mn (Xi ) + \u03b5i , i = 1, . . . , n, be the initial model,\nwhere {mn (*)}n\u22651 is any equicontinuous sequence of functions. Under the\nbn2 (Xi ) of (3.6),\nassumptions of Theorem 1 and for the variance estimator \u03c3\nP\n\nsup|P(Tbe\u2217\u2217 /vbh\u2217 0 \u2264 z|X1 , Y1 , . . . , Xn , Yn ) \u2212 P(N (0, 1) \u2264 z)| \u2192 0.\nz\u2208R\n\nh\n\n4.2. Small sample behavior. We investigated the small sample behavior\nof our bootstrap test. We generated samples of 150 observations through the\nmodel\n(4.1)\n\nY = \u03b81 + \u03b82 X + r cos(2\u03c0tX) + \u03b5,\n\nn\n\nr \u2208 0,\n\nq o\n2\n3\n\n, t \u2208 {2, 5, 10},\n\nwhere X is distributed as U [\u22121, 1]. The null hypothesis corresponds to r =\n0, while under the alternatives r 2 = 2/3 and E[r 2 cos2 (2\u03c0tX)]/E\u03b52 = 1/3\nfor any integer t, a quite small signal-to-noise ratio. When t increases, the\ndeviation from the linear model becomes more oscillating and irregular, and\nthen more difficult to detect.\nTo compute our test statistic, we used the regressogram method of Example 1b with half-binwidths in\nHn = {h0 = 2\u22122 , h1 = 2\u22123 , . . . , h5 = 2\u22127 }.\nThe smallest binwidth thus \u221a\ndefines 128 cells, which is sufficient for 150 observations. The \u03b3n was set to c 2 ln Jn , where c = 1, 1.5, 2. For each experiment\nwe ran 5000 replications under the null and 1000 under the alternative. For\neach replication the bootstrap critical values were computed from 199 bootstrap samples. For \u03c9i we used the two-point distribution\n\u221a \u0013\n\u221a \u0013\n\u221a\n\u221a\n\u0012\n\u0012\n1\u2212 5\n1+ 5\n5+ 5\n5\u2212 5\nP \u03c9i =\n,\nP \u03c9i =\n,\n=\n=\n2\n10\n2\n10\nwhich verifies the required conditions.\nIn a first stage we set (\u03b81 , \u03b82 ) = (0, 0) and performed a test for white noise,\nthat is, H0 : m(*) = 0, with homoscedastic errors following a standard normal\ndistribution (Table 1). We estimated the variance under homoscedasticity\nby\nbn2 =\n\u03c3\n\nn\u22121\nX\n1\n(Y\n\u2212 Y(i) )2 ,\n2(n \u2212 1) i=1 (i+1)\n\nwhere Y(i) denote observations ordered according to the order of the Xi . This\nestimate is consistent under the null and the alternative; see Rice (1984).\n\n\f12\n\nE. GUERRE AND P. LAVERGNE\n\nIn each cell of the tables, the first and second rows give empirical percentages of rejections at 2% and 5% nominal levels. We compare our test to\n(i) simple benchmark tests based on fixed bandwidths h0 and h5 , to evaluate the effect of a data-driven bandwidth, (ii) the maximum test based on\nMax = maxh\u2208Hn Tbh /vbh , to evaluate the gain of our approach and (iii) a test\nbased on Tbh\u0303 /vbh\u0303 , to evaluate the effect of our standardization. For each test,\nwe computed bootstrap critical values as for our test.\nUnder the null hypothesis, the bootstrap leads to accurate rejection probabilities for all tests. Under the considered alternatives, empirical power decreases for all tests when the frequency increases from t = 2 to t = 10. The\ndata-driven tests always dominate the tests based on the fixed parameter h0 ,\nwhich behaves poorly. For the low frequency alternatives, data-driven tests\nperform very well with power greater than 90% and 95% at a 2% and 5%\nnominal level, respectively, and there are no significant differences between\nthem. For higher frequency alternatives, differences are significant. Our test\nhas quite high power and rejects the null hypothesis at more than 85% and\n60% at a 5% level when t = 5 and 10, respectively. It performs better than\nor as well as does the test based on h5 designed for irregular alternatives, except for c = 2 and t = 10. It always dominates Max with differences ranging\nfrom 7.1% to 18.3%, depending on the level. The test based on Tbh\u0303 /vbh\u0303 behaves as the Max test. This suggests that the high performances of our test\nare mainly explained by our standardization choice, which is made possible\nby our selection procedure.\nTo check whether these conclusions are affected by the details of the experiments, we consider errors following a centered and standardized exponential\nTable 1\nWhite noise model-Gaussian errors\n\nT\nbh0\nb\nv h0\n\n1.9\n5.3\n\nT\nbhJn\nb\nvh\nJn\n\nt=2\n\nT\nbh\u0303\nb\nv h\u0303\n\nOur test\n\nMax\n\nc=1\n\nc = 1.5\n\nc=2\n\nc=1\n\nc = 1.5\n\nc=2\n\n2.1\n5.1\n\n2.0\n4.2\n\n2.0\n4.3\n\n2.0\n4.2\n\n2.0\n4.4\n\n1.8\n4.4\n\n1.8\n4.3\n\n1.7\n4.4\n\n5.1\n9.0\n\n60.6\n72.5\n\n90.5\n96.0\n\n90.7\n96.3\n\n90.0\n95.9\n\n90.5\n96.2\n\n91.7\n95.4\n\n91.3\n95.7\n\n91.9\n97.3\n\nt=5\n\n3.0\n7.7\n\n59.2\n73.3\n\n66.3\n79.2\n\n66.9\n79.8\n\n66.3\n79.4\n\n66.3\n79.5\n\n77.3\n88.7\n\n78.5\n88.5\n\n78.8\n87.8\n\nt = 10\n\n3.4\n7.0\n\n50.5\n66.0\n\n32.8\n49.3\n\n32.5\n50.2\n\n32.5\n49.3\n\n32.7\n48.8\n\n48.4\n65.6\n\n49.2\n65.5\n\n49.2\n59.9\n\nH0\n\nPercentages of rejection at 2% and 5% nominal levels.\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n13\n\n(Table 2), a standardized Student with five degrees of freedom (Table 3), a\nnormal distribution with conditional variance \u03c3 2 (X) = (1+3X 2 )/3 using our\nestimator (3.6) with bn = 1/8 (Table 4) and a linear model with homoscedastic normal errors and (\u03b81 , \u03b82 ) = (1, 3) (Table 5). As results for Tbh\u0303 /vbh\u0303 are very\nsimilar to the ones for Max, we do not report them. For exponential errors,\nthere is a slight tendency to overrejection. It is likely that matching thirdorder moments in the bootstrap sample generation as proposed by Gozalo\n(1997) would lead to more accurate critical values. Heteroscedasticity does\nnot adversely affect the behavior of the tests. For the linear model, there is\nsome gain in power for the Max test compared with Table 1, but differences\nwith our test remain significant for the two high-frequency alternatives.\n5. Extensions to general nonparametric methods and additive alternatives.\n5.1. General nonparametric methods. We give here some general sufficient conditions ensuring the validity of our results. These conditions could\nbe checked for other smoothing methods or other designs than the ones considered here. Indeed, different smoothing methods can be used for specification testing; see, for example, Chen (1994) for spline smoothing, Fan, Zhang\nand Zhang (2001) for local polynomials and Spokoiny (1996) for wavelets.\nAlso, our conditions allow for various constructions of the quadratic forms\nTbh ; see, for example, Dette (1999) and H\u00e4rdle and Mammen (1993).\nFor an n \u00d7\nn matrix W , let Spn [W ] be its spectral radius and Nn2 [W ] =\nP\n2 . For W symmetric, the former is its largest eigenvalue\nTr[W \u2032 W ] = i,j wij\nin absolute value and the latter is the sum of its squared eigenvalues.\nTable 2\nWhite noise model-exponential errors\nOur test\nT\nbh0\nb\nv h0\n\n2.9\n6.1\n\nT\nbhJn\nb\nvh\nJn\n\nt=2\n\nMax\n\nc=1\n\nc = 1.5\n\nc=2\n\n2.9\n6.2\n\n3.3\n6.7\n\n3.3\n6.3\n\n3.2\n5.9\n\n3.4\n6.5\n\n4.5\n9.0\n\n65.4\n77.7\n\n91.9\n95.9\n\n92.2\n96.1\n\n92.4\n96.3\n\n92.6\n97.2\n\nt=5\n\n5.6\n9.6\n\n61.4\n71.7\n\n66.5\n78.9\n\n76.7\n86.1\n\n77.0\n87.0\n\n78.6\n86.0\n\nt = 10\n\n3.6\n7.6\n\n50.6\n64.5\n\n35.4\n52.3\n\n51.3\n65.5\n\n52.8\n65.6\n\n53.7\n62.0\n\nH0\n\nPercentages of rejection at 2% and 5% nominal levels.\n\n\f14\n\nE. GUERRE AND P. LAVERGNE\nTable 3\nWhite noise model-Student errors\nOur test\nT\nbh0\nb\nv h0\n\n2.3\n5.0\n\nT\nbhJn\nb\nvh\nJn\n\nt=2\n\nMax\n\nc=1\n\nc = 1.5\n\nc=2\n\n2.1\n4.8\n\n2.0\n4.4\n\n1.8\n4.5\n\n1.7\n4.3\n\n1.9\n4.4\n\n5.2\n9.2\n\n60.4\n73.3\n\n91.8\n95.7\n\n91.9\n95.5\n\n92.2\n95.8\n\n92.1\n96.2\n\nt=5\n\n3.4\n8.4\n\n60.6\n74.6\n\n66.6\n79.3\n\n77.6\n88.2\n\n77.7\n88.2\n\n79.0\n86.9\n\nt = 10\n\n3.6\n7.8\n\n48.8\n65.1\n\n32.2\n48.1\n\n48.1\n63.1\n\n48.5\n64.2\n\n49.4\n60.0\n\nH0\n\nPercentages of rejection at 2% and 5% nominal levels.\n\nAssumption W0. Let Hn be as in (3.7) with hJn \u224d (ln n)C2 /p /n2/(4s+p)\nfor some s > 0, C2 > 1 and h0 \u2192 0. The collection of n \u00d7 n matrices {Wh , h \u2208\nHn } is such that: (i) For all h, Wh = [wij (h), 1 \u2264 i, j \u2264 n] depends only upon\nX1 , . . . , Xn and is real symmetric with wii (h) = 0 for all i. (ii) maxh\u2208Hn Spn [Wh ] =\nOP (1). (iii) Nn2 [Wh ] \u224dP h\u2212p for all h \u2208 Hn and uniformly in h \u2208 Hn \\{h0 }Nn2 [Wh \u2212\nWh0 ] \u224dP h\u2212p \u2212 h\u2212p\n0 .\nAssumption W1. Let Hn , s and hJn be as in Assumption W0. For any\nsequence hn = hjn from Hn : (i) There are some symmetric positive semidefinite matrices Phn with Spn [Whn \u2212 Phn ] = oP (1). (ii) For any s > s, there is\nTable 4\nWhite noise model-heteroscedastic errors\nOur test\nT\nbh0\nb\nv h0\n\n2.2\n5.1\n\nT\nbhJn\nb\nvh\nJn\n\nt=2\n\nMax\n\nc=1\n\nc = 1.5\n\nc=2\n\n2.2\n5.0\n\n1.8\n4.7\n\n1.7\n4.2\n\n1.5\n4.1\n\n1.6\n4.2\n\n3.0\n5.9\n\n62.3\n76.3\n\n92.6\n98.0\n\n94.1\n97.9\n\n93.9\n98.4\n\n94.9\n98.7\n\nt=5\n\n1.6\n4.2\n\n64.4\n78.9\n\n62.9\n81.9\n\n82.9\n91.9\n\n83.5\n92.8\n\n83.9\n91.6\n\nt = 10\n\n2.2\n5.6\n\n57.8\n72.8\n\n26.8\n50.3\n\n53.3\n69.5\n\n53.7\n71.3\n\n53.2\n63.5\n\nH0\n\nPercentages of rejection at 2% and 5% nominal levels.\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n15\n\nTable 5\nLinear model-Gaussian errors\nOur test\nT\nbh0\nb\nv h0\n\n2.3\n5.0\n\nT\nbhJn\nb\nvh\nJn\n\nt=2\n\nMax\n\nc=1\n\nc = 1.5\n\nc=2\n\n2.1\n5.0\n\n1.9\n4.4\n\n1.9\n4.5\n\n2.0\n4.5\n\n2.0\n5.0\n\n3.0\n6.3\n\n59.8\n71.7\n\n93.6\n96.7\n\n91.0\n95.5\n\n91.2\n95.6\n\n91.1\n96.8\n\nt=5\n\n2.7\n5.8\n\n58.2\n72.7\n\n73.2\n85.0\n\n77.7\n88.4\n\n77.9\n88.2\n\n78.5\n88.4\n\nt = 10\n\n3.0\n7.0\n\n48.2\n64.4\n\n41.9\n58.8\n\n50.4\n66.0\n\n50.6\n66.2\n\n50.0\n61.8\n\nH0\n\nPercentages of rejection at 2% and 5% nominal levels.\n\na set \u03a0s,n of functions from [0, 1]p to R such that for any L > 0 and any \u03b4(*)\nin Cp (L, s), there is a \u03c0(*) in \u03a0s,n with supx\u2208[0,1]p |\u03b4(x) \u2212 \u03c0(x)| \u2264 C4 Lhsn for\nP\nP\nsome C4 = C4 (s) > 0. (iii) Let \u039b2n = \u039b2n (s, hn ) = inf \u03c0\u2208\u03a0s,n 1\u2264i,j\u2264n \u03c0(Xi )pij (hn )\u03c0(Xj )/ ni=1 \u03c0(Xi )2 ,\nwhere pij (hn ) is the generic element of Phn . For any s > s, there is a constant\nC5 = C5 (s) > 0 such that P(\u039bn > C5 ) \u2192 1.\nAssumption W1 describes the approximation properties of the nonparametric method used to build the Wh and allows us to extend a result of\nIngster [(1993), page 253 and following]; see Lemma 6 in Section 6. The\nnext proposition shows that our main examples satisfy Assumptions W0\nand W1 under a regular i.i.d. random design.\nProposition 1. Assume that Assumption D holds, and let s be as in\nAssumption W. Then Examples 1a, 1b and 2 satisfy Assumptions W0 and\nW1.\nThe next theorem extends our main results under Assumptions W0 and\nW1. In Section 6 we actually show Theorems 1\u20134 by proving Theorem 5 and\nProposition 1.\nTheorem 5. Theorems 1 and 4 hold under Assumption W0 in place\nof Assumptions D and W. Theorems 2 and 3 hold under Assumptions W0\nand W1 in place of Assumptions D and W.\n5.2. Additive alternatives. Our general framework easily adapts to detection of specific alternatives. We focus here on additive nonparametric\n\n\f16\n\nE. GUERRE AND P. LAVERGNE\n\nregressions m(x) = m1 (x1 ) + * * * + mp (xp ). The null hypothesis is\nH0 : m(*) = \u03bc(*; \u03b8)\nfor some \u03b8 \u2208 \u0398,\nwhere \u03bc(x; \u03b8) = \u03bc1 (x1 ; \u03b8) + * * * + \u03bcp (xp ; \u03b8).\nFor ease of notation, we consider a modification of Example 1a where we\nremove cross-products of polynomial functions. Let Xi = [X1i , . . . , Xpi ]\u2032 and\nk , . . . , X k , i = 1, . . . , n, k = 0, . . . , 1/h].\nconsider the (p/h) \u00d7 n matrix \u03a8h = [X1i\npi\nLet Wh be the matrix obtained from \u03a8h (\u03a8\u2032h \u03a8h )\u22121 \u03a8\u2032h by setting the diagonal\nentries to 0 and Tbh defined as in (3.3).\nTheorem 6. Let the matrices Wh be as above and Hn be as in (3.7),\nwith hJn \u224d (ln n)C6 /n1/3 for some C6 > 1. Let Assumptions D, E and M\nhold. Consider a sequence of additive equicontinuous regression functions\n{mn (*)}n\u22651 and assume that the variance estimator satisfies (3.5).\n(i) For h0 and \u03b3n as in Theorem 1, the test is asymptotically of level \u03b1\ngiven the design.\n(ii) Assume that for some unknown s > 5/4 and L > 0, mn (*) \u2212 \u03bc(*; \u03b8) is\nin Cp (L, s) for all \u03b8 in \u0398 and all n. For h0 and \u03b3n as in Theorem 2 and\n\"\n\nn\n1X\nmin\n(mn (Xi ) \u2212 \u03bc(Xi ; \u03b8))2\n\u03b8\u2208\u0398 n\ni=1\n\n\u2265 (1 + oP (1))\u03ba2 L1/(4s+1)\n\n\u0012\n\n#1/2\n\n\u03b3n supx\u2208[0,1] \u03c3 2 (x)\nn\n\n\u00132s/(4s+1)\n\n,\n\nthe test is consistent given the design provided \u03ba2 = \u03ba2 (s) is large enough.\n2\nProof of Theorem 6 repeats the proofs of Theorems 1 and 2 with vh,h\n0\n\u2212p\n\u2212p \u2212 h ) and is therefore omitted. One\nof order (h\u22121 \u2212 h\u22121\n0 ) instead of (h\n0\ncan also show consistency of the test against Pitman\nq additive alternatives\n1/2\n\nthat approach the parametric model at rate o(1/ nh0 ). The bootstrap\nprocedure described in Section 4.1 also remains valid.\n6. Proofs. This section is organized as follows. In Section 6.1 we study\nthe quadratic forms \u03b5\u2032 (Wh \u2212 Wh0 )\u03b5 and \u03b5\u2032 Wh \u03b5 under H0 . Section 6.2 recalls some results related to variance estimation. In Section 6.3 we gather\npreliminary results on the parametric estimation error mn (*) \u2212 \u03bc(*; \u03b8bn ). In\nSections 6.4 and 6.5 we establish Theorems 1 and 4 under Assumption W0.\nIn Sections 6.6 and 6.7 we establish Theorems 2 and 3 under Assumptions\nW0 and W1. Thus, Theorem 5 is a direct consequence of Sections 6.4\u20136.7.\nSection 6.8 deals with Proposition 1.\n\n\f17\n\nDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\nWe denote Y = [Y1 , . . . , Yn ]\u2032 and \u03b5 = [\u03b51 , . . . , \u03b5n ]\u2032 . For any \u03b4(*) from Rp\nto R, \u03b4 = \u03b4(X) = [\u03b4(X1 ), . . . , \u03b4(Xn )]\u2032 and Dn (\u03b4) is the n \u00d7 n diagonal matrix\nwith entries \u03b4(Xi ). Let k * k2n and (*, *)n be the Euclidean norm and inner\nproduct on Rn divided by n, respectively, that is,\nk\u03b4k2n = k\u03b4(X)k2n =\n\nn\n1X\n\u03b42 (Xi )\nn i=1\n\nand\n(\u03b5, \u03b4)n = (\u03b5, \u03b4(X))n =\n\nn\n1X\n\u03b5i \u03b4(Xi ).\nn i=1\n\nThis gives Spn [W ] = maxkukn =1 kW ukn = maxkukn =1 |u\u2032 W u|/n for a symmetric W . Recall that Spn [AB] \u2264 Spn [A]Spn [B]. Let \u03b8n = \u03b8n,m be such that\n(6.1)\n\nmin km(X) \u2212 \u03bc(X; \u03b8)kn = km(X) \u2212 \u03bc(X; \u03b8n )kn .\n\u03b8\u2208\u0398\n\nWe use the notation Pn (A) for P(A|X1 , . . . , Xn ), En [*] and Varn [*] being the\nassociated conditional mean and variance. In what follows, C and C \u2032 are\npositive constants that may vary from line to line. An absolute constant\ndepends neither on the design nor on the distribution of the \u03b5i given the\ndesign.\n6.1. Study of quadratic forms. The proof of Lemma 1 is omitted.\nLemma 1. Let W be an n \u00d7 n symmetric matrix with zeros on the diagoP\n2 \u03c3 2 (X )\u00d7\nnal. Under Assumption E, En [\u03b5\u2032 W \u03b5] = 0 and Varn [\u03b5\u2032 W \u03b5] = 2 1\u2264i,j\u2264n wij\ni\n2\n2\n2\n\u03c3 (Xj ) = 2Nn [Dn (\u03c3)W Dn (\u03c3)] \u224d Nn [W ].\nLemma 2. Let \u03c3 = inf x\u2208[0,1]p \u03c3(x) > 0, \u03c3 = supx\u2208[0,1]p \u03c3(x) < \u221e and \u03bd \u2208\n(0, 1/2). Under Assumption E, there is an absolute constant C = C\u03bd > 0\nsuch that:\n(i) If (\u03c3 4 Sp2n [Wh ])/( \u03c3 4 Nn2 [Wh ]) \u2264 \u03bd,\n\n\u0012\n\n\u03c3 Spn [Wh ]\nsup|Pn (\u03b5 Wh \u03b5 \u2264 vh z) \u2212 P(N (0, 1) \u2264 z)| \u2264 C\n\u03c3 Nn [Wh ]\nz\u2208R\n\u2032\n\n\u00131/4\n\n.\n\n(ii) For all h \u2208 Hn \\{h0 } and any z > 0, if (\u03c3 4 Sp2n [Wh \u2212Wh0 ])/( \u03c3 4 Nn2 [Wh \u2212\nWh0 ]) < \u03bd,\n\u221a\n\u0013\n\u0012 \u2032\n\u0012\n\u0013\n\u0012\n\u0013\n\u03b5 (Wh \u2212 Wh0 )\u03b5\nz2\n2\n\u03c3 Spn [Wh \u2212 Wh0 ] 1/4\n\u2265 z \u2264 \u221a exp \u2212\n.\nPn\n+C\nvh,h0\n\u03c0z\n2\n\u03c3 Nn [Wh \u2212 Wh0 ]\n\n\f18\n\nE. GUERRE AND P. LAVERGNE\n\nProof. Let \u03b5e = Dn\u22121 (\u03c3)\u03b5, so that En [\u03b5ei ] = 0 and Varn [\u03b5ei ] = 1 for all\ni, and let W = [wij ]1\u2264i,j\u2264n be\nh \u2212 Wh0 )Dn (\u03c3),\nP Dn (\u03c3)W2h Dn\u2032(\u03c3) or Dn (\u03c3)(W\n, \u03b5e W \u03b5e/v is \u03b5\u2032 Wh \u03b5/vh or \u03b5\u2032 (Wh \u2212\nso that for v 2 = Nn2 [W ] = 1\u2264i,j\u2264n wij\nWh0 )\u03b5/vh,h0 , respectively. Let \u03bb1 , . . . , \u03bbn be the real eigenvalues of W ,\n\"\n\nn\nn\nX\nX\n1\n2\nLn = 3 6\nwij\nv\ni=1 j=1\n\n!3/2\n\n+ 36\n\nn X\nn\nX\n\ni=1 j=1\n\n3\n\n|wij |\n\n#\n\nand \u2206n =\n\nn\n1 X\n\u03bb4 .\nv 4 i=1 i\n\nConsider a vector g of n independent N (0, 1) variables, independent of the\nXi . Theorem 3 of Rotar' and Shervashidze (1985) says that there is an\nabsolute constant C > 0 such that\n\u0012 \u2032\n\u0013\n\u0013\n\u0012 \u2032\ng Wg\n\u03b5e W \u03b5e\n\u2264 z \u2212 Pn\n\u2264z\nsup Pn\nv\nv\nz\u2208R\n\u2264 C[1 \u2212 ln(1 \u2212 2\u2206n )]3/4 L1/4\nn\n\nif \u2206n < 1/2.\n\nRn }1\u2264i\u2264n\n\nLet {bi \u2208\nbe an orthonormal system of eigenvectors of W associated\nwith\nthe\neigenvalues\n\u03bbi . As En [g\u2032 W g] = 0 by Lemma 1, g \u2032 W g =\nP\nPn\nn\n\u2032 2 \u2212 E [(b\u2032 g)2 ]]. Hence, g \u2032 W g has the same con\u2032 2\nn\ni=1 \u03bbi [(b\ni=1 \u03bbi (bi g) =\ni\nPi g)\nditional distribution as ni=1 \u03bbi \u03b6i , where the \u03b6i are centered Chi-squared\nvariables with one degree of freedom, independent among themselves and of\nthe Xi . The Berry\u2013Esseen bound of Chow and Teicher [(1988), Theorem 3,\npage 304] yields that there is an absolute constant C > 0 such that\nsup Pn\nz\u2208R\n\n\u0012\n\n\u0013\n\ng\u2032 W g\n\u2264 z \u2212 P(N (0, 1) \u2264 z) \u2264 C\nv\n\nPn\n\n3\ni=1 |\u03bbi |\n.\nv3\n\nThe two above inequalities together imply that if \u2206n < 1/2,\n\u0012 \u2032\n\u0013\n\u03b5e W \u03b5e\nsup Pn\n\u2264 z \u2212 P(N (0, 1) \u2264 z)\nv\nz\u2208R\nPn\n\u0014\n(6.2)\n3\u0015\n3/4 1/4\ni=1 |\u03bbi |\n.\n\u2264 C (1 \u2212 ln(1 \u2212 2\u2206n )) Ln +\nv3\n\u221a\nLet {ei , i = 1, . . . , n} be the canonical basis of Rn , so that kei kn = 1/ n.\nThen\nn\nn\nX\nX\ni=1\n\n2\nwij\n\nj=1\n\n!3/2\n\n=\n\nn\nX\nkW ei kn\ni=1\n\nkei kn\n\n\u2264 Spn [W ] \u00d7\nX\n\n1\u2264i,j\u2264n\n\n|wij |3 =\n\u2264\n\nX\n\n1\u2264i,j\u2264n\n\nX\n\n2\nwij\n= Spn [W ]Nn2 [W ],\n\n1\u2264i,j\u2264n\n\n2\nwij\n\n|(ei , W ej )n |\nkei kn kej kn\n\n2\nwij\n\nkW ej kn\n\u2264 Spn [W ]Nn2 [W ].\nkej kn\n\n1\u2264i,j\u2264n\n\nX\n\nnkW ei k2n\n\n\f19\n\nDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\nHence, using v 2 =\n\nPn\n\n= Nn2 [W ] and |\u03bbi | \u2264 Spn [W ] for all i, we obtain\n\n2\ni=1 \u03bbi\n\n\u2206n \u2264\n\nSp2n [W ]\n,\nNn2 [W ]\n\nLn \u2264 42\n\nSpn [W ]\nNn [W ]\n\nand\nn\nX\n|\u03bbi |3\ni=1\n\nv3\n\n\u2264\n\n\u0012\n\nSpn [W ]\nSpn [W ]\n\u2264\nNn [W ]\nNn [W ]\n\n\u00131/4\n\n,\n\nsince Spn [W ]/Nn [W ] \u2264 1 for any symmetric W . The above inequalities and\n(6.2) give\n(6.3)\n\nsup Pn\nz\u2208R\n\n\u0012\n\n\u0013\n\n\u0012\n\n\u03b5e\u2032 W \u03b5e\nSpn [W ]\n\u2264 z \u2212 P(N (0, 1) \u2264 z) \u2264 C\nv\nNn [W ]\n\n\u00131/4\n\n,\n\nprovided (Spn [W ]/Nn [W ])2 \u2264 \u03bd, for an absolute constant C = C\u03bd > 0.\nPart (i) follows by setting W = Dn (\u03c3)Wh Dn (\u03c3) in (6.3) and noting that\n\u0012\n\nSpn [W ]\nNn [W ]\n\n\u00132\n\n\u2264\n\n\u0012\n\n\u03c3\n\u03c3\n\n\u00134 \u0012\n\nSpn [Wh ]\nNn [Wh ]\n\n\u00132\n\n\u2264 \u03bd < 1/2.\n\nPart (ii) follows from (6.3) with W = Dn (\u03c3)(Wh \u2212 Wh0 )Dn (\u03c3) and Mills'\nratio inequality. \u0003\n6.2. Variance estimation. The following results are proven in Guerre and\nLavergne (2003).\nProposition 2. Under Assumptions D and W, vh2 0 \u224dP h\u2212p\nand uni0\n\u2212p\n2\n\u2212p\nformly in h \u2208 Hn \\ {h0 } vh,h0 \u224dP h \u2212 h0 .\nProposition 3.\ngression functions.\n\nLet {mn (*)}n\u22651 be an equicontinuous sequence of re\u2032\n\n(i) Under Assumptions D and E, if bn \u2192 0 and n1\u22124/d bpn \u2192 \u221e, then\n(3.5) holds.\n(ii) Let {Wh , h \u2208 Hn } be any collection of nonzero n \u00d7 n symmetric matrices with zeros on the diagonal. Under (3.5),\n1| = oP (1).\n\n2\nb\nvh\n0\n\n2\nvh\n0\n\nP\n\nb\nv2\n\n0\n\u2212\n\u2192 1 and maxh\u2208Hn \\{h0 } | vh,h\n2\nh,h0\n\n6.3. The parametric estimation error.\nLemma 3. Let W be an n\u00d7n symmetric\nmatrix depending upon X1 , . . . , Xn ,\nP\n\u03b8n be as in (6.1) and Bn (R) = {\u03b8 \u2208 \u0398; n1 ni=1 (\u03bc(Xi ; \u03b8) \u2212 \u03bc(Xi ; \u03b8n ))2 \u2264 R2 }.\n\n\f20\n\nE. GUERRE AND P. LAVERGNE\n\nUnder Assumptions E and M, there is an absolute constant C = Cd\u2032 > 0\nsuch that, for any mn (*), any n and any R > 0,\nEn\n\n\u0014\n\n\u0015\n\n\u221a\nsup | n(W (\u03bc(X; \u03b8) \u2212 \u03bc(X; \u03b8n )), \u03b5)n |\n\n\u03b8\u2208Bn (R)\n\n\u2032\n\n\u2032\n\nd\n\u2264 C \u03bc\u0307Spn [W ]R max E1/d\nn [|\u03b5i | ].\n1\u2264i\u2264n\n\n\u2032\n\n\u2032\n\nProof. Without loss of generality, we can assume that max1\u2264i\u2264n E1/d [|\u03b5i |d |Xi ] =\n\u03bc\u0307 = Spn [W ] = 1. Let \u03b4W (*; \u03b8) = W (\u03bc(*; \u03b8) \u2212 \u03bc(*; \u03b8n )). The Marcinkie- wicz\u2013\nZygmund inequality, see Chow and Teicher (1988), yields, under Assumption\nE and for any \u03b8, \u03b8 \u2032 in \u0398, that there is an absolute constant C such that\n1/d\u2032\n\nEn\n\nn\n1 X\n\u221a\n(\u03b4W (Xi ; \u03b8) \u2212 \u03b4W (Xi ; \u03b8 \u2032 ))\u03b5i\nn i=1\n\nd\u2032\n\n\"\n\nn\n1X\n\u2032\nd\u2032\n(\u03b4W (Xi ; \u03b8) \u2212 \u03b4W (Xi ; \u03b8 \u2032 ))2 E2/d\n\u2264C\nn |\u03b5i |\nn i=1\n\n#1/2\n\n\u2264 CkW (\u03bc(X; \u03b8) \u2212 \u03bc(X; \u03b8 \u2032 ))kn \u2264 Ck\u03bc(X; \u03b8) \u2212 \u03bc(X; \u03b8 \u2032 )kn .\nLet Nn (t, R) be the smallest number of k\u03bc(X; \u03b8)\u2212 \u03bc(X; \u03b8 \u2032 )kn -balls of radius t\ncovering Bn (R). It follows from van der Vaart [(1998), Example 19.7] and\nAssumption M that, for some absolute constant C \u2032 > 0, Nn (t, R) \u2264 C \u2032 (R/t)d .\nThe H\u00f6lder inequality and Corollary 2.2.5 from van der Vaart and Wellner\n(1996) give, as d/d\u2032 < 1,\nEn\n\nsup\n\u03b8\u2208Bn (R)\n\nn\nn\n1 X\n1 X\n\u2032\n\u221a\n\u221a\n\u03b4W (Xi ; \u03b8)\u03b5i \u2264 E1/d\n\u03b4W (Xi ; \u03b8)\u03b5i\nsup\nn\nn i=1\nn i=1\n\u03b8\u2208Bn (R)\n\n\u2264C\n\n\u2032\n\nZ\n\n0\n\nR \u0012 R \u0013d/d\u2032\n\nt\n\ndt = Cd\u2032 R.\n\nd\u2032\n\n\u0003\n\nLemma 4. Under Assumptions E and M, there is an absolute constant\nC = Cd\u2032 > 0, such that, for any \u03c1 large enough, any mn (*) and any n,\n\u221a \u0015\n\u0014\n\u221a\n2\u03c1\nb\nPn kmn (X) \u2212 \u03bc(X; \u03b8n )kn > 3kmn (X) \u2212 \u03bc(X; \u03b8n )kn + \u221a\nn\n1/d\u2032\n\n\u2032\n\nC max1\u2264i\u2264n En [|\u03b5i |d ]\n\u2264\n.\n\u03c1\n\n\f21\n\nDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\nProof. The definition (3.1) of \u03b8bn yields, see van de Geer (2000),\nkmn (X) \u2212 \u03bc(X; \u03b8bn )k2n\n\u2264 2(\u03bc(X; \u03b8bn ) \u2212 \u03bc(X; \u03b8n ), \u03b5)n + kmn (X) \u2212 \u03bc(X; \u03b8n )k2n ,\nk\u03bc(X; \u03b8bn ) \u2212 \u03bc(X; \u03b8n )k2n\n\u2264 4(\u03bc(X; \u03b8bn ) \u2212 \u03bc(X; \u03b8n ), \u03b5)n + 4kmn (X) \u2212 \u03bc(X; \u03b8n )k2n .\n\n(6.4)\n\nConsider a fixed r > 1 and any \u03c1 \u2265 r. Let En = {kmn (X) \u2212 \u03bc(X; \u03b8n )k2n <\n(\u03bc(X; \u03b8bn )\u2212\u03bc(X;\n\u221a \u03b8n ), \u03b5)n }, so that on the complement of this event kmn (X)\u2212\nb\n\u03bc(X; \u03b8n )kn \u2264 3kmn (X) \u2212 \u03bc(X; \u03b8n )kn by (6.4). Lemma 4 follows by bounding\n\u221a J \u00132\n\u0013\n\u0012\u0012\u221a\n2r\n\u2264 kmn (X) \u2212 \u03bc(X; \u03b8bn )k2n and En\nPn\n3kmn (X) \u2212 \u03bc(X; \u03b8n )kn + \u221a\nn\n\u0012\n\n\u2264 Pn 2kmn (X) \u2212 \u03bc(X; \u03b8n )k2n +\n\n\u2264 2kmn (X) \u2212 \u03bc(X; \u03b8n )k2n\n\n2r 2J\nn\n\n+ 2k\u03bc(X; \u03b8n ) \u2212 \u03bc(X; \u03b8bn )k2n and En\n\u0013\n\n\u0012\n\n\u0013\n\nr 2J\n\u2264 k\u03bc(X; \u03b8bn ) \u2212 \u03bc(X; \u03b8n )k2n and En .\n= Pn\nn\n\u221a\n\u221a\n\u221a\nLet Sj = Sj,n = {\u03b8 \u2208 \u0398; r j / n \u2264 k\u03bc(X; \u03b8)\u2212\u03bc(X; \u03b8n )kn < r j+1 / n } \u2282 Bn (r j+1 / n )\nwith Bn (*) as in Lemma 3. Then (6.4), the definition of En , the Markov inequality and Lemma 3 with W = Idn yield\nPn\n\n\u0012\n\nr 2J\n\u2264 k\u03bc(X; \u03b8bn ) \u2212 \u03bc(X; \u03b8n )k2n and En\nn\n\u2264\n\u2264\n\u2264\n\n+\u221e\nX\n\nPn\n\n\u0012\n\nPn\n\n\u0012\n\nj=J\n\n+\u221e\nX\n\nj=J\n\n+\u221e\nX\n\nj=J\n\n\u0013\n\nr 2j\n\u2264 (\u03bc(X; \u03b8bn ) \u2212 \u03bc(X; \u03b8n ), \u03b5)n\n\u03b8bn \u2208 Sj and\n8n\n\n\u0013\n\u0013\n\n\u221a\nr 2j\n\u221a \u2264\nsup \u221a | n(\u03bc(X; \u03b8) \u2212 \u03bc(X; \u03b8n ), \u03b5)n |\n8 n \u03b8\u2208Bn (rj+1 / n )\n\n\u221a\n\u0015\n\u0014\n\u221a\n8 n\n|\nn(\u03bc(X;\n\u03b8)\n\u2212\n\u03bc(X;\n\u03b8\n),\n\u03b5)\n|\nsup\nE\nn\nn\nn\n\u221a\nr 2j\n\u03b8\u2208Bn (r j+1 / n )\n\n\u2264 C max\n\n1\u2264i\u2264n\n\n\u2032\nd\u2032\nE1/d\nn [|\u03b5i | ]\n\n+\u221e\nX\n\nj=J\n\n1/d\u2032\n\n\u221a\nr j+1 n\n\u221a\nr 2j n\n\u2032\n\nr 2 C max1\u2264i\u2264n En [|\u03b5i |d ]\n=\n.\nr\u22121\nrJ\n\n\u0003\n\n\f22\n\nE. GUERRE AND P. LAVERGNE\n\nLemma 5 is proven in Guerre and Lavergne (2003).\nLemma 5. Consider the local alternatives of Theorem 3 and let the conditions of \u221aTheorem 3 on \u03bc(*; *) hold. Under Assumptions E and M and if\nlimn\u2192+\u221e nrn = +\u221e,\nkmn (X) \u2212 \u03bc(X; \u03b8n )kn = rn \u2212 oP (rn )\n\nand\n\nk\u03bc(X; \u03b8bn ) \u2212 \u03bc(X; \u03b80 )kn = oP (rn ).\n\nProposition 4. Under Assumptions E, M and W0(ii), if h0 \u2192 0, then,\nfor any {mn (*)}n\u22651 \u2282 H0 ,\nTbh \u2212 Tbh0 \u2212 \u03b5\u2032 (Wh \u2212 Wh0 )\u03b5\n= oP (1),\n1/2\n(h\u2212p \u2212 h\u2212p\n0 )\n\nmax\n\nh\u2208Hn \\{h0 }\n\np/2\n\nh0 (Tbh0 \u2212\u03b5\u2032 Wh0 \u03b5) = oP (1).\n\nLet hn \u2208 Hn be an arbitrary sequence of smoothing parameters. Then under\nH0 or H1 ,\n\u221a\n(mn (X) \u2212 \u03bc(X, \u03b8bn ))\u2032 Wh \u03b5 = OP (1)[ nkmn (X) \u2212 \u03bc(X, \u03b8n )kn + 1].\nProof. We have\n\nTbh = (mn (X) \u2212 \u03bc(X; \u03b8bn ))\u2032 Wh (mn (X) \u2212 \u03bc(X; \u03b8bn ))\n+ 2(mn (X) \u2212 \u03bc(X; \u03b8bn ))\u2032 Wh \u03b5 + \u03b5\u2032 Wh \u03b5.\n\n(6.5)\n\nThe Cauchy\u2013Schwarz inequality, Assumptions E and W0(ii) and Lemma 4\nyield uniformly in h \u2208 Hn ,\n|(mn (X) \u2212 \u03bc(X; \u03b8bn ))\u2032 Wh (mn (X) \u2212 \u03bc(X; \u03b8bn ))|\n\u2264 n max Spn [Wh ]kmn (X) \u2212 \u03bc(X; \u03b8bn )k2n\nh\u2208Hn\n\n= OP [(1 +\n\n\u221a\n\nnkmn (X) \u2212 \u03bc(X; \u03b8n )kn )2 ] = OP (1)\n\nunder H0 , as kmn (X) \u2212 \u03bc(X; \u03b8n )kn = 0. Since for any h \u2208 Hn , h\u2212p \u2212 h\u2212p\n0 \u2265\n\u2212p\n\u2212p\n\u2212p p\nh1 \u2212 h0 = h0 (a \u2212 1) \u2192 +\u221e, we obtain that, under H0 ,\nmax\n\nh\u2208Hn \\{h0 }\n\n(mn (X) \u2212 \u03bc(X; \u03b8bn ))\u2032 (Wh \u2212 Wh0 )(mn (X) \u2212 \u03bc(X; \u03b8bn ))\n= oP (1),\n1/2\n(h\u2212p \u2212 h\u2212p\n0 )\np/2\nh0 (mn (X) \u2212 \u03bc(X; \u03b8bn ))\u2032 Wh0 (mn (X) \u2212 \u03bc(X; \u03b8bn )) = oP (1).\n\n(6.6)\nSince k\u03bc(X; \u03b8bn )\u2212\u03bc(X; \u03b8n )kn \u2264 k\u03bc(X; \u03b8bn )\u2212mn (X)kn +kmn (X)\u2212\u03bc(X; \u03b8n )kn ,\n/ B\u03c1,n ) \u2264 C/\u03c1 for any \u03c1 large\nLemma 4 and Assumption E yield Pn (\u03b8bn \u2208\nenough, any mn (*) and any n, where\n\u001a\n\nB\u03c1,n = \u03b8 \u2208 \u0398;\n\n\u221a \u001b\n\u221a\n2\u03c1\nk\u03bc(X; \u03b8) \u2212 \u03bc(X; \u03b8n )kn \u2264 ( 3 + 1)kmn (X) \u2212 \u03bc(X; \u03b8n )kn + \u221a\n.\nn\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n23\n\nLemma 3 yields\nEn\n\n(6.7)\n\n\u0014\n\n\u0015\n\nsup |(\u03bc(X, \u03b8) \u2212 \u03bc(X; \u03b8n ))\u2032 W \u03b5|\n\u03b8\u2208B\u03c1,n\n\u221a\n\u2264 C\u03c1Spn [W ]( nkmn (X) \u2212 \u03bc(X; \u03b8n )kn + 1).\n\nTaking W = Wh0 and using the Markov inequality, (6.5), (6.6), mn (X) \u2212\np/2\n\u03bc(X; \u03b8n ) = 0, Assumption W0(ii) and h0 \u2192 0 then show that h0 (Tbh0 \u2212\n\u03b5\u2032 Wh0 \u03b5) = oP (1) under H0 . Taking W = Wh \u2212 Wh0 in (6.7) and using h =\nh0 a\u2212j for some j = 0, . . . , Jn yields, under H0 ,\nPn\n\n\u0012\n\nmax\n\nh\u2208Hn \\{h0 }\n\n(\u03bc(X, \u03b8bn ) \u2212 \u03bc(X; \u03b8n ))\u2032 (Wh \u2212 Wh0 )\u03b5\n\u2265\u01eb\n1/2\n(h\u2212p \u2212 h\u2212p\n0 )\n\n\u0013\n\n\u2264 Pn (\u03b8bn \u2208\n/ B\u03c1,n )\n+\n\nX\n(\u03bc(X, \u03b8) \u2212 \u03bc(X; \u03b8n ))\u2032 (Wh \u2212 Wh0 )\u03b5\n1\nEn sup\n1/2\n\u01eb h\u2208H \\{h } \u03b8\u2208B\u03c1,n\n(h\u2212p \u2212 h\u2212p\n0 )\n0\n\nn\n\n\u2264\n\n\u221e\n1\nC \u03c1\nC \u03c1\np/2 X\np/2\n+ OP (h0 )\n= + OP (h0 ),\npj\n1/2\n\u03c1\n\u01eb\n\u03c1\n\u01eb\n(a\n\u2212\n1)\nj=1\n\nfor all \u01eb > 0. The last result follows from (6.7) with W = Wh and\nEn [((mn (X) \u2212 \u03bc(X; \u03b8n ))\u2032 Wh \u03b5)2 ] \u2264 nSp2n (Wh )\u03c3 2 kmn (X) \u2212 \u03bc(X; \u03b8n )k2n . \u0003\n6.4. Proof of Theorem 1 under Assumption W0. Under Assumptions\n1/2 uniformly in h \u2208\nW0(iii) and E, vh,h0 \u224d Nn [Wh \u2212 Wh0 ] \u224dP (h\u2212p \u2212 h\u2212p\n0 )\nHn \\ {h0 }; see Lemma 1. Therefore, Propositions 3(ii) and 4 yield\nmax\n\nh\u2208Hn \\{h0 }\n\nTbh \u2212 Tbh0\n\u03b5\u2032 (Wh \u2212 Wh0 )\u03b5\n+ oP (1).\n= (1 + oP (1)) \u00d7 max\nvh,h0\nvbh,h0\nh\u2208Hn \\{h0 }\n\nLet \u03b7 be as in (3.8). Observe that\ne 6= h ) \u2264 P\nPn (h\n0\nn\n\n\u2264 Pn\n\n\u0012\n\n\u0012\n\nmax\n\nh\u2208Hn \\{h0 }\n\nmax\n\nh\u2208Hn \\{h0 }\n\nTbh \u2212 Tbh0\n\u2265 \u03b3n\nvbh,h0\n\n\u0013\n\n\u0013\n\n\u03b5\u2032 (Wh \u2212 Wh0 )\u03b5\n\u03b3n\n\u2265\n+ oP (1).\nvh,h0\n1 + \u03b7/2\n\nApplying Lemma 2(ii) using Assumption W0(iii) and hj = h0 a\u2212j for j =\n0, . . . , Jn , we obtain\ne 6= h0 ) \u2264\nPn (h\n\nX\n\nh\u2208Hn \\{h0 }\n\nPn\n\n\u0012\n\n\u0013\n\n\u03b5\u2032 (Wh \u2212 Wh0 )\u03b5\n\u03b3n\n\u2265\n+ oP (1)\nvh,h0\n1 + \u03b7/2\n\n\f24\n\nE. GUERRE AND P. LAVERGNE\n\n\u2264\n\n\u221a\n\n\u0012\n\n\u0012\n\n1\n\u03b3n\n2(1 + \u03b7/2)\n\u221a\nexp \u2212\n\u03c0\u03b3n\n2 1 + \u03b7/2\n\n+\u221e\np/8 X\n+ OP (h0 )\nj=1\n\n(apj\n\n\u00132\n\n+ ln Jn\n\n\u0013\n\n1\n+ oP (1) = oP (1),\n\u2212 1)1/8\n\nusing (3.8), h0 \u2192 0 and \u03b3n \u2192 \u221e. Thus, Pn (Tbh\u0303 \u2265 vbh0 z\u03b1 ) = Pn (Tbh0 \u2265 vbh0 z\u03b1 ) +\noP (1). Theorem 1 then follows from Propositions 3(ii) and 4, Lemma 2(i)\nand Assumption W0.\n6.5. Proof of Theorem 4 under Assumptions D and W0. Let \u03b5\u2217 = [\u03b5\u22171 , . . . , \u03b5\u2217n ].\nWe first establish a moment bound that plays the role of Assumption E. As\n\u2032\nbn (Xi )\u03c9i , where the \u03c9i are independent of the initial sample, E[|\u03b5\u2217i |d |X1 , Y1 , . . . , Xn , Yn ] =\n\u03b5\u2217i = \u03c3\n\u2032\n\u2032\nbn (Xi )|d and\nE[|\u03c91 |d ]|\u03c3\n\n(6.8)\n\n\u2032\n\n\u2032\n\n\u0012\n\nmax E[|\u03b5\u2217i |d |X1 , Y1 , . . . , Xn , Yn ] \u2264 E[|\u03c91 |d ]\n\n1\u2264i\u2264n\n\n\u0013\n\n\u2032\n\nsup \u03c3 d (x) + oP (1) .\n\nx\u2208[0,1]p\n\nThis is sufficient to establish Theorem 4; see Guerre and Lavergne (2003).\n6.6. Proof of Theorem 2 under Assumptions W0 and W1.\nb \u2208 Cp (L, s) with s > s and L > 0.\nLemma 6. Consider a function \u03b4(*)\nConsider any sequence hn from Hn and let \u039bn = \u039bn (s, hn ) be as in Assumption W1(iii). Under Assumption W1, we have\n\u2032\nb\nb\n\u03b4(X)\nWhn \u03b4(X)\n\ns 2\n1/2\nb\n\u2265 n[(\u039bn \u2212 Sp1/2\nn [Whn \u2212 Phn ])k\u03b4(Xi )kn \u2212 (\u039bn + Spn [Phn ])C4 Lhn ] ,\n\nwhere C4 = C4 (s) is from Assumption W1(ii), provided\n1/2\n\nb\nk\u03b4(X\ni )kn \u2265\n\n(6.9)\n\n\u039bn + Spn [Phn ]\n1/2\n\u039bn \u2212 Spn [Whn\n\n\u2212 Phn ]\n\nC4 Lhsn \u2265 0.\n\nProof. We have \u03b4b\u2032 Whn \u03b4b = \u03b4b\u2032 Phn \u03b4b+ \u03b4b\u2032 (Whn \u2212Phn )\u03b4b \u2265 \u03b4b\u2032 Phn \u03b4b\u2212nSpn [Whn \u2212\ns\nb\nb 2 . Let \u03c0(*) be such that sup\nPhn ]k\u03b4k\nn\nx\u2208[0,1]p |\u03b4(x)\u2212\u03c0(x)| \u2264 C4 Lhn ; see Assumption W1(ii).\nBecause Phn is positive by Assumption W1(i), the triangle inequality and\nthe definition of \u039bn yield\n\u0012 b\u2032\n\n\u03b4 Phn \u03b4b\nn\n\n\u00131/2\n\n\u2265\n\u2265\n\n\u0012\n\n\u0012\n\n\u03c0 \u2032 Phn \u03c0\nn\n\u03c0 \u2032 Phn \u03c0\nn\n\n\u00131/2\n\u00131/2\n\n\u0012\n\n\u00131/2\n\n1 b\n( \u03b4 \u2212 \u03c0)\u2032 Phn ( \u03b4b \u2212 \u03c0)\n\u2212\nn\nb\n\u2212 Sp1/2\nn [Phn ]k\u03b4 \u2212 \u03c0kn\n\n\f25\n\nDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\nb n \u2212 Sp1/2 [P ]k\u03b4b \u2212 \u03c0kn\n\u2265 \u039bn k\u03b4b + \u03c0 \u2212 \u03b4k\nhn\nn\nb \u2212 (\u039b + Sp1/2 [P ])k\u03b4b \u2212 \u03c0k\n\u2265 \u039bn k\u03b4k\nn\nn\nn\nhn\nn\nb n \u2212 (\u039bn + Sp1/2 [P ])C4 Lhs .\n\u2265 \u039bn k\u03b4k\nhn\nn\nn\n1/2\n\n1/2\n\nb n \u2212 (\u039bn + Spn [P ])C4 Lhs \u2265 0 from (6.9),\nAs (\u039bn \u2212 Spn [Whn \u2212 Phn ])k\u03b4k\nhn\nn\n\n\u03b4b\u2032 Whn \u03b4b\nb 2\nb n \u2212 (\u039bn + Sp1/2 [P ])C4 Lhs ]2 \u2212 Sp [W \u2212 P ]k\u03b4k\n\u2265 [\u039bn k\u03b4k\nhn\nhn\nhn\nn\nn\nn\nn\nn\ns\n1/2\nb\n= [(\u039bn \u2212 Sp1/2\nn [Whn \u2212 Phn ])k\u03b4kn \u2212 (\u039bn + Spn [Phn ])C4 Lhn ]\n\ns\n1/2\nb\n\u00d7 [(\u039bn + Sp1/2\nn [Whn \u2212 Phn ])k\u03b4kn \u2212 (\u039bn + Spn [Phn ])C4 Lhn ]\n\ns 2\n1/2\nb\n\u2265 [(\u039bn \u2212 Sp1/2\nn [Whn \u2212 Phn ])k\u03b4kn \u2212 (\u039bn + Spn [Phn ])C4 Lhn ] . \u0003\n\nWe now prove Theorem 2 under Assumptions W0 and W1, using the\npower bound (2.3). Take hn = h0 a\u2212jn , where jn is the integer part of\n\u0014\n\n\u0012\n\n\u0013\n\nL2 n\n2\n1\nln\n+ ln h0\nln a 4s + p\n\u03b3n inf x\u2208[0,1]p \u03c3 2 (x)\n\u0012\n\n\u0013\n\n\u0015\n\n1\n2\nL2 n\n\u224d\nln\n,\nln a 4s + p\n\u03b3n inf x\u2208[0,1]p \u03c3 2 (x)\nusing ln h0 = O(ln ln n) and ln(n/\u03b3n ) \u2265 (1 \u2212 \u03b3) ln n for some \u03b3 \u2208 (0, 1). Note\nthat hn is in Hn for all s > s and L > 0 since hJn \u224d (ln n)C2 /p /n2/(4s+p) for\nsome C2 > 1 and \u03b3n \u2264 n\u03b3 for some \u03b3 \u2208 (0, 1). We have\nLhsn \u224d Lp/(4s+p)\n\n\u0012\n\n\u03c3 2 \u03b3n\nn\n\n\u00132s/(4s+p)\n\nand\n2 \u2212p/2\n\u224d L2p/(4s+p) (\u03c3 2 \u03b3n )4s/(4s+p) np/(4s+p) \u2192 \u221e.\nnL2 h2s\nn \u224d \u03b3n \u03c3 hn\n\nb = mn (*) \u2212 \u03bc(*; \u03b8bn ) in Lemma 6, which belongs to Cp (L, s) by\nTake now \u03b4(*)\nthe assumptions of Theorem 2. The lower bound (3.10) of Theorem 2 yields\ns\nb\nk\u03b4(X)k\nn \u2265 kmn (X) \u2212 \u03bc(X; \u03b8n )kn \u2265 C\u03ba1 Lhn (1 + oP (1)),\n\nimplying, in particular, that nkmn (X) \u2212 \u03bc(X; \u03b8n )k2n diverges in probability.\nUnder Assumptions W0(ii) and W1(i), (iii),\n\u0012\n\nP C\u03ba1 Lhsn \u2265\n\n1/2\n\n\u039bn (s, hn ) + Spn [Phn ]\n1/2\n\n\u039bn (s, hn ) \u2212 Spn [Whn \u2212 Phn ]\n\n\u0013\n\nC4 Lhsn \u2265 0 \u2192 1\n\n\f26\n\nE. GUERRE AND P. LAVERGNE\n\nb verifies (6.9) with probability tending\nfor \u03ba1 large enough, showing that \u03b4(*)\nto 1. Therefore, Lemma 6 and Assumption W1(iii) yield\n\n(mn (X) \u2212 \u03bc(X; \u03b8bn ))\u2032 Whn (mn (X) \u2212 \u03bc(X; \u03b8bn ))\nb\n= \u03b4b \u2032 (X)Whn \u03b4(X)\n\n\u2265 n[(\u039bn \u2212 Sp1/2\nn [Whn \u2212 Phn ])kmn (X) \u2212 \u03bc(X; \u03b8n )kn\n\ns 2\n\u2212 (\u039bn + Sp1/2\nn [Phn ])C4 Lhn ] (1 + oP (1))\n\n\u2265 C(1 + oP (1))nkmn (X) \u2212 \u03bc(X; \u03b8n )k2n \u2265 C(1 + oP (1))n\u03ba21 L2 h2s\nn .\nMoreover, by Proposition 4,\n\u221a\n(mn (X) \u2212 \u03bc(X; \u03b8bn ))\u2032 Whn \u03b5 = OP ( nkmn (X) \u2212 \u03bc(X; \u03b8n )kn )\n= oP (nkmn (X) \u2212 \u03bc(X; \u03b8n )k2n ).\n\n\u2212p/2\n\nFrom \u03b5\u2032 Whn \u03b5 = OP (vhn ) = OP (hn\n\n) = oP (nL2 h2s\nn ) and (6.5),\n\nTbhn \u2265 C(1 + oP (1))nkmn (X) \u2212 \u03bc(X; \u03b8n )k2n \u2265 C(1 + oP (1))n\u03ba21 L2 h2s\nn .\n\nProposition 3(ii), Lemma 1 and Assumption W0(iii) yield z\u03b1 vbh0 +\u03b3n vbhn ,h0 \u224dP\n\u2212p/2\n\u224d nL2 h2s\n\u03b3n vbhn ,h0 \u224dP \u03b3n \u03c3 2 hn\nn . Collecting the leading terms implies that,\nfor \u03ba1 large enough,\nP\n\n2\n\u2032\nTbhn \u2212 z\u03b1 vbh0 \u2212 \u03b3n vbhn ,h0 \u2265 CnL2 h2s\nn (\u03ba1 \u2212 C )(1 + oP (1)) \u2192 +\u221e.\n\n6.7. Proof of Theorem 3 under Assumptions W0 and W1. The proof\nfollows the lines of the proof of Theorem 2, using now (2.4). Since mn (X) \u2212\n\u03bc(X; \u03b8bn ) = rn \u03b4n (X) + \u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn ),\n(mn (X) \u2212 \u03bc(X; \u03b8bn ))\u2032 Wh0 (mn (X) \u2212 \u03bc(X; \u03b8bn ))\n= rn2 \u03b4n (X)\u2032 Wh0 \u03b4n (X)\n\n+ 2rn \u03b4n (X)Wh0 (\u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn ))\nBy Lemma 5,\n\n+ (\u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn ))\u2032 Wh0 (\u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn )).\n\n|rn \u03b4n (X)Wh0 (\u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn ))|\n\n\u2264 nrn Spn [Wh0 ]k\u03b4n (X)kn k\u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn )kn = oP (nrn2 ),\n\n|(\u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn ))\u2032 Wh0 (\u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn ))|\n\n\u2264 nSpn [Wh0 ]k\u03bc(X; \u03b80 ) \u2212 \u03bc(X; \u03b8bn )k2n = oP (nrn2 ).\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n27\n\nBecause {\u03b4n (*)}n\u22651 \u2282 C(L, s) with s > s, Lemma 6 yields, under (3.11) and\nh0 \u2192 0,\n\u03b4n (X)\u2032 Wh0 \u03b4n (X) \u2265 (1 + oP (1))n[(\u039bn \u2212 Sp1/2\nn [Wh0 \u2212 Ph0 ])k\u03b4n (X)kn\n\ns 2\n\u2212 C4 (\u039bn + Sp1/2\nn [Ph0 ])Lh0 ]\n\n\u2265 Cn(1 + oP (1)).\nEquation (6.5) in the proof of Proposition 4 and Lemma 5 give, since z\u03b1 vbh0 +\n\u2212p/2\np/2\n\u03b5\u2032 Wh0 \u03b5 = OP (h0 ), nrn2 h0 \u2192 +\u221e and h0 \u2192 0,\n\u2212p/2\n\nTbh0 \u2212 z\u03b1 vbh0 \u2212 \u03b3n vbh0 ,h0 \u2265 (1 + oP (1))Cnrn2 + OP (h0\n\nP\n\n) \u2192 +\u221e.\n\n6.8. Proof of Proposition 1. We only detail the case of Examples 1a and\n1b. The proof of Proposition 1 for Example 2 can be found in Guerre and\nLavergne (2003).\nThe functions \u03c8k (*) can be changed into any system generating the same\nlinear subspace of Rn : Consider the following orthonormal basis of L2 ([0, 1]p , dx):\n\u03c6k (x) =\n\np p\nY\n\n2kl + 1Qkl (xl )I(x \u2208 [0, 1]p )\n\nl=1\n\n(6.10)\n\n\u03c6qkh (x) = h\u2212p/2\n\np p\nY\n\nl=1\n\nfor Example 1a,\n\n2kl + 1Qql (kl h \u2212 xl )I(x \u2208 Ik (h))\nfor Example 1b,\n\nwhere the Qk (*) are the Legendre polynomials of degree k on [0, 1], with\nR\nR\nsupt\u2208[0,1] |Qk (t)| \u2264 1, 01 Q2k (t) dt = 1/(2k + 1), 01 Qk (t)Qk\u2032 (t) dt = 0 for k 6=\nk\u2032 ; see, for example, Davis (1975). Let \u03a6h = [\u03c6k (X), 1 \u2264 |k| \u2264 1/h] for Example 1a and \u03a6h = [\u03c6qkh (X), 1 \u2264 |q| \u2264 q\u0304, 1 \u2264 |k| \u2264 1/h] for Example 1b. Define\ndh as the number of columns of \u03a6h and note that in both examples dh is of\norder h\u2212p .\nLemma 7. If f (*) is bounded away from 0 and infinity on [0, 1]p , there\nis a C > 0 such that\nmax Spdh [(n\u22121 \u03a6\u2032h \u03a6h )\u22121 ] \u2264 C\n\nh\u2208Hn\n\nand\nmax Spdh [n\u22121 \u03a6\u2032h \u03a6h ] \u2264 C\n\nh\u2208Hn\n\nwith probability tending to 1,\n\n1/3 in Example 1a and h\u2212p = o(n/ ln n) in Example\nprovided h\u2212p\nJn\nJn = o(n/ ln n)\n1b.\n\n\f28\n\nE. GUERRE AND P. LAVERGNE\n\nProof. Consider first Example 1a. As the n\u22121 \u03a6\u2032h \u03a6h , h \u2208 Hn , are nested\nGram matrices, it is sufficient to consider the spectral radii of n\u22121 \u03a6\u2032hJn \u03a6hJn\nand its inverse. We have\n|\u03c6k (Xi )\u03c6k\u2032 (Xi )| \u2264\n\np p\nY\n\nq\n\n2kl + 1 2kl\u2032 + 1 \u2264 Ch\u2212p\nJn ,\n\nl=1\n\nVar(\u03c6k (Xi )\u03c6k\u2032 (Xi )) \u2264 E\u03c62k (Xi )\u03c62k\u2032 (Xi ) \u2264 E1/2 \u03c64k (Xi )E1/2 \u03c64k\u2032 (Xi )\n\n\u2264 sup |\u03c6k (x)| sup |\u03c6k\u2032 (x)|E1/2 \u03c62k (Xi )E1/2 \u03c62k\u2032 (Xi )\nx\u2208[0,1]p\n\nx\u2208[0,1]p\n\n\u2264 Ch\u2212p\nJn ,\nR\n\nas E\u03c62k (X) \u2264 supx\u2208[0,1]p f (x) \u03c62k (x) dx = supx\u2208[0,1]p f (x). The Bernstein inequality then yields\ns\n\nnhpJn\nln n\n\nsup\n0\u2264|k|,|k \u2032|\u22641/hJn\n\nn\n1X\n\u03c6k (Xi )\u03c6k\u2032 (Xi ) \u2212 E\u03c6k (X)\u03c6k\u2032 (X) = OP (1).\nn i=1\n\nThis gives n\u22121 \u03a6\u2032hJ \u03a6hJn = n\u22121 E\u03a6\u2032hJ \u03a6hJn + RhJn , where RhJn is a dhJn \u00d7\nq\n\nn\n\nn\n\ndhJn matrix whose elements are uniformly OP ( ln n/nhpJn ). Thus,\n1\nhpJn\n\nSpdh [RhJn ] \u2264 NdhJ [RhJn ] = OP\nJn\n\nn\n\ns\n\nln n\nnhpJn\n\n!\n\n= oP (1),\n\n1/3 . Hence, the eigenvalues of n\u22121 \u03a6\u2032\nas h\u2212p\nhJn \u03a6hJn are between\nJn = o(n/ ln n)\n\u22121\n\u2032\nthe smallest and largest eigenvalues of n E\u03a6hJ \u03a6hJn , with probability tend-\n\ning to one. But, for any a \u2208 R\n\ndhJ\n\nn\n\nn\n\n,\n\nn\u22121 a\u2032 E\u03a6\u2032hJn \u03a6hJn a = E\n\nX\n\n!2\n\nak \u03c6k (X)\n\n0\u2264|k|\u22641/hJn\n\n\u224d\n\nZ\n\n[0,1]p\n\nX\n\n!2\n\nak \u03c6k (x)\n\n0\u2264|k|\u22641/hJn\n\ndx = a\u2032 a,\n\nsince the \u03c6k (*) are orthonormal in L2 ([0, 1]p , dx). Therefore, the eigenvalues\nof the symmetric matrix n\u22121 E\u03a6\u2032hJn \u03a6hJn are bounded away from 0 and infinity when n grows. Example 1b is studied in Baraud (2002) and follows\nfrom similar arguments. \u0003\nWe now return to the proof of Proposition 1 for Example 1. Lemma 7\nimplies that, for some C > 1,\n\u0012\n\n1 \u2032\n1\n1\n\u03a6h \u03a6\u2032h \u227a Ph = \u03a6h\n\u03a6 \u03a6h\nCn\nn\nn h\n\n\u0013\u22121\n\n\u03a6\u2032h \u227a\n\nC\n\u03a6h \u03a6\u2032h ,\nn\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n29\n\nwith probability tending to 1, where \u227a is the ordering of symmetric matrices.\nBecause pii (h) = e\u2032i Ph ei , where {ei }1\u2264i\u2264n is the canonical basis of Rn , this\ngives\n\n(6.11) |pii (h)| \u2264\n\n\uf8f1\nC\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2n\n\nC\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3n\n\nX\n\n|k|\u22641/h\n\n\u03c62k (Xi ) \u2264 C/(nh2p ),\n\nX\n\n|k|\u22641/h,q\u2264q\u0304\n\n\u03c62qkh (Xi ) \u2264 C/(nhp ),\n\nfor Example 1a,\nfor Example 1b,\n\nwith probability going to 1 and uniformly in i = 1, . . . , n and h \u2208 Hn . Indeed,\n\u03c62k (*) \u2264 Ch\u2212p for all k \u2264 1/h for Example 1a, while \u03c62qkh (Xi ) vanishes except\nfor exactly one index k with \u03c62qkh (Xi ) \u2264 Ch\u2212p for Example 1b.\nTo prove Assumption W0(ii), note that Spn [Ph ] = 1 since Ph is an orthogonal projection. The triangular inequality gives maxh\u2208Hn Spn [Wh ] \u2264\n1 + maxh\u2208Hn max1\u2264i\u2264n |pii (h)| = OP (1) by (6.11) and the restriction on hJn\n= o(n) for Example 1a and h\u2212p\nwhich gives hJ\u22122p\nJn = o(n) for Example 1b. For\nn\nAssumption W0(iii), we have\nNn2 [Wh ] = Nn2 [Ph ] \u2212 Nn2 [Wh \u2212 Ph ],\n\nNn2 [Wh \u2212 Wh0 ] = Nn2 [Ph \u2212 Ph0 ] \u2212 Nn2 [(Wh \u2212 Ph ) \u2212 (Wh0 \u2212 Ph0 )].\n\nNow Nn2 [Ph ] = Rank[Ph ] and Nn2 [Ph \u2212 Ph0 ] = Rank[Ph \u2212 Ph0 ], since Ph and\nPh \u2212 Ph0 are orthogonal projections. This gives Nn2 [Ph ] \u224d h\u2212p and Nn2 [Ph \u2212\nPh0 ] \u224dP h\u2212p \u2212 h\u2212p\n0 almost surely for Example 1a, and for Example 1b, using\nnumber\nthe Bernstein inequality with h\u2212p\nJn = o(n/ ln n), ensuring that the\nPn\n2\nof Xi in each bin Ik (h) diverges. Then, since Nn [Wh \u2212 Ph ] = i=1 p2ii (h),\nAssumption W0(iii) holds if\nmax hp\n\nh\u2208Hn\n\nn\nX\n\np2ii (h) = oP (1)\n\ni=1\n\nand\n\u22121\nmax (h\u2212p \u2212 h\u2212p\n0 )\n\nh\u2208Hn \\h0\n\nn\nX\ni=1\n\n(pii (h) \u2212 pii (h0 ))2 = oP (1),\n\nwhich is a consequence of (6.11), together with h\u22123p\nJn = o(n/ ln n) for Exam\u2212p\nple 1a and hJn = o(n/ ln n) for Example 1b. To show Assumption W1(i), note\nthat the Ph are symmetric positive semidefinite with maxh\u2208Hn Spn [Wh \u2212\nPh ] = oP (1), as shown when establishing Assumption W0(ii). For Assumption W1(ii), (iii), consider first Example 1a. Let \u03a0s,h be the set of polynomial functions with order 1/h which are such that Assumption W1(ii)\nholds by the multivariate Jackson theorem; see, for example, Lorentz (1966).\nThis choice of \u03a0s,h gives \u039b2n = 1 almost surely by definition of the Ph with\n\n\f30\n\nE. GUERRE AND P. LAVERGNE\n\nh\u2212p\nJn = o(n) and Assumption D. For Example 1b, the proof of Assumtion\nW1(ii) uses the same Taylor expansion as in Guerre and Lavergne (2002) to\nbuild the \u03a0s,h . Assumption W1(iii), for any given q\u0304, is a consequence of Assumption W1(iii) for q\u0304 = 1. This can be shown using Guerre and Lavergne\n(2002) and establishing convergence of local empirical moments with repeated applications of the Bernstein inequality.\nAcknowledgments. We thank an Editor, an Associate Editor and three\nreferees for comments that were helpful to improve our paper.\nREFERENCES\nAerts, M., Claeskens, G. and Hart, J. D. (1999). Testing the fit of a parametric\nfunction. J. Amer. Statist. Assoc. 94 869\u2013879. MR1723323\nAerts, M., Claeskens, G. and Hart, J. D. (2000). Testing lack of fit in multiple\nregression. Biometrika 87 405\u2013424. MR1782487\nAkaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In Proc. Second International Symposium on Information Theory (B. N. Petrov\nand F. Csaki, eds.) 267\u2013281. Akad\u00e9miai Kiad\u00f3, Budapest. MR483125\nBaraud, Y. (2002). Model selection for regression on a random design. ESAIM Probab.\nStatist. 6 127\u2013146. MR1918295\nBaraud, Y., Huet, S. and Laurent, B. (2003). Adaptive tests of linear hypotheses by\nmodel selection. Ann. Statist. 31 225\u2013251. MR1962505\nChen, J. C. (1994). Testing goodness-of-fit of polynomial models via spline smoothing\ntechniques. Statist. Probab. Lett. 19 65\u201376. MR1253314\nChow, S. C. and Teicher, H. (1988). Probability Theory. Independence, Interchangeability, Martingales, 2nd ed. Springer, New York. MR953964\nDavis, P. J. (1975). Interpolation and Approximation. Dover, New York. MR380189\nde Jong, P. (1987). A central limit theorem for generalized quadratic forms. Probability\nTheory and Related Fields 75 261\u2013277. MR885466\nDette, H. (1999). A consistent test for the functional form of a regression based on a\ndifference of variance estimators. Ann. Statist. 27 1012\u20131040. MR1724039\nEubank, R. L. and Hart, J. D. (1992). Testing goodness-of-fit in regression via order\nselection criteria. Ann. Statist. 20 1412\u20131425. MR1186256\nFan, J. (1996). Test of significance based on wavelet thresholding and Neyman's truncation. J. Amer. Statist. Assoc. 91 674\u2013688. MR1395735\nFan, J. and Huang, L.-S. (2001). Goodness-of-fit tests for parametric regression models.\nJ. Amer. Statist. Assoc. 96 640\u2013652. MR1946431\nFan, J., Zhang, C. and Zhang, J. (2001). Generalized lihelihood ratio statistics and\nWilks phenomenon. Ann. Statist. 29 153\u2013193. MR1833962\nGozalo, P. L. (1997). Nonparametric bootstrap analysis with applications to demographic effects in demand functions. J. Econometrics 81 357\u2013393. MR1492354\nGuerre, E. and Lavergne, P. (2002). Optimal minimax rates for nonparametric specification testing in regression models. Econometric Theory 18 1139\u20131171. MR1926017\nGuerre, E. and Lavergne, P. (2003). Data-driven rate-optimal specification\ntesting in regression models. Working paper, Univ. Paris 6. Available at\nwww.ccr.jussieu.fr/lsta/R2001 3b.pdf. MR1926017\nH\u00e4rdle, W. and Mammen, E. (1993). Comparing nonparametric versus parametric regression fits. Ann. Statist. 21 1926\u20131947. MR1245774\n\n\fDATA-DRIVEN TESTS FOR REGRESSION MODELS\n\n31\n\nHart, J. D. (1997). Nonparametric Smoothing and Lack-of-Fit Tests. Springer, New York.\nMR1461272\nHorowitz, J. L. and Spokoiny, V. G. (2001). An adaptive, rate-optimal test of a parametric mean-regression model against a nonparametric alternative. Econometrica 69\n599\u2013631. MR1828537\nIngster, Y. I. (1993). Asymptotically minimax hypothesis testing for nonparametric alternatives. I, II, III. Math. Methods Statist. 2 85\u2013114, 171\u2013189 and 249\u2013268. MR1257978\nLedwina, T. (1994). Data-driven version of Neyman's smooth test of fit. J. Amer. Statist.\nAssoc. 89 1000\u20131005. MR1294744\nLepski, O. V., Mammen, E. and Spokoiny, V. G. (1997). Optimal spatial adaptation\nto inhomogeneous smoothness: An approach based on kernel estimates with variable\nbandwidth selectors. Ann. Statist. 25 929\u2013947. MR1447734\nLorentz, G. G. (1966). Approximation of Functions. Holt, Rinehart and Winston, New\nYork. MR213785\nRice, J. (1984). Bandwidth choice for nonparametric regression. Ann. Statist. 12 1215\u2013\n1230. MR760684\nRotar', V. I. and Shervashidze, T. L. (1985). Some estimates of distibutions of\nquadratic forms. Theory Probab. Appl. 30 585\u2013591. MR805309\nSchwarz, G. (1978). Estimating the dimension of a model. Ann. Statist. 6 461\u2013464.\nMR468014\nSpokoiny, V. G. (1996). Adaptive hypothesis testing using wavelets. Ann. Statist. 24\n2477\u20132498. MR1425962\nSpokoiny, V. G. (2001). Data-driven testing the fit of linear models. Math. Methods\nStatist. 10 465\u2013497. MR1887343\nvan de Geer, S. (2000). Empirical Processes in M-Estimation. Cambridge Univ. Press.\nvan der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge Univ. Press. MR1652247\nvan der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical\nProcesses. With Applications to Statistics. Springer, New York. MR1385671\nWu, C.-F. J. (1986). Jacknife, bootstrap and other resampling methods in regression\nanalysis (with discussion). Ann. Statist. 14 1261\u20131350. MR868303\nZhang, C. M. (2003). Adaptive tests of regression functions via multiscale generalized\nlikelihood ratios. Canad. J. Statist. 31 151\u2013171. MR2016225\nLaboratoire de Statistique\nTh\u00e9orique et Appliqu\u00e9e\nBo\u00eete 158, Universit\u00e9 Pierre et Marie Curie\n4 Place Jussieu\n75252 Paris Cedex 05\nFrance\ne-mail: eguerre@ccr.jussieu.fr\n\nGREMAQ\nCNRS UMR 5604\nUniversit\u00e9 Toulouse 1\nManufacture des Tabacs Bat F\n21 All\u00e9es de Brienne\n31000 Toulouse\nFrance\ne-mail: Pascal.Lavergne@univ-tlse1.fr\n\n\f"}