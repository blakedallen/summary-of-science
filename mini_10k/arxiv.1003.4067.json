{"id": "http://arxiv.org/abs/1003.4067v1", "guidislink": true, "updated": "2010-03-22T05:34:18Z", "updated_parsed": [2010, 3, 22, 5, 34, 18, 0, 81, 0], "published": "2010-03-22T05:34:18Z", "published_parsed": [2010, 3, 22, 5, 34, 18, 0, 81, 0], "title": "Computation of Reducts Using Topology and Measure of Significance of\n  Attributes", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1003.1290%2C1003.2070%2C1003.0633%2C1003.1543%2C1003.3932%2C1003.4731%2C1003.0432%2C1003.0689%2C1003.5308%2C1003.5564%2C1003.4270%2C1003.2067%2C1003.4639%2C1003.1857%2C1003.5196%2C1003.2528%2C1003.1115%2C1003.4852%2C1003.4726%2C1003.4344%2C1003.4067%2C1003.1613%2C1003.6026%2C1003.3757%2C1003.0898%2C1003.3088%2C1003.1343%2C1003.0791%2C1003.3209%2C1003.3442%2C1003.3317%2C1003.3138%2C1003.3313%2C1003.3091%2C1003.3525%2C1003.1387%2C1003.1826%2C1003.1585%2C1003.0002%2C1003.0050%2C1003.0146%2C1003.5205%2C1003.5046%2C1003.2910%2C1003.4003%2C1003.2165%2C1003.1730%2C1003.4010%2C1003.1865%2C1003.6117%2C1003.1951%2C1003.1759%2C1003.2851%2C1003.2444%2C1003.5529%2C1003.2597%2C1003.2531%2C1003.1800%2C1003.1994%2C1003.4612%2C1003.3623%2C1003.2341%2C1003.1490%2C1003.4704%2C1003.4846%2C1003.0892%2C1003.0780%2C1003.5048%2C1003.2811%2C1003.0375%2C1003.2866%2C1003.4680%2C1003.0615%2C1003.2527%2C1003.2202%2C1003.3804%2C1003.4315%2C1003.0315%2C1003.0931%2C1003.4741%2C1003.5899%2C1003.5893%2C1003.5812%2C1003.1452%2C1003.2530%2C1003.0034%2C1003.1856%2C1003.0387%2C1003.4929%2C1003.5590%2C1003.2714%2C1003.3100%2C1003.3718%2C1003.0806%2C1003.5578%2C1003.5975%2C1003.5646%2C1003.1836%2C1003.1257%2C1003.4407%2C1003.1304&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Computation of Reducts Using Topology and Measure of Significance of\n  Attributes"}, "summary": "Data generated in the fields of science, technology, business and in many\nother fields of research are increasing in an exponential rate. The way to\nextract knowledge from a huge set of data is a challenging task. This paper\naims to propose a hybrid and viable method to deal with an information system\nin data mining, using topological techniques and the significance of the\nattributes measured using rough set theory, to compute the reduct, This will\nreduce the randomness in the process of elimination of redundant attributes,\nwhich, in turn, will reduce the complexity of the computation of reducts of an\ninformation system where a large amount of data have to be processed.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1003.1290%2C1003.2070%2C1003.0633%2C1003.1543%2C1003.3932%2C1003.4731%2C1003.0432%2C1003.0689%2C1003.5308%2C1003.5564%2C1003.4270%2C1003.2067%2C1003.4639%2C1003.1857%2C1003.5196%2C1003.2528%2C1003.1115%2C1003.4852%2C1003.4726%2C1003.4344%2C1003.4067%2C1003.1613%2C1003.6026%2C1003.3757%2C1003.0898%2C1003.3088%2C1003.1343%2C1003.0791%2C1003.3209%2C1003.3442%2C1003.3317%2C1003.3138%2C1003.3313%2C1003.3091%2C1003.3525%2C1003.1387%2C1003.1826%2C1003.1585%2C1003.0002%2C1003.0050%2C1003.0146%2C1003.5205%2C1003.5046%2C1003.2910%2C1003.4003%2C1003.2165%2C1003.1730%2C1003.4010%2C1003.1865%2C1003.6117%2C1003.1951%2C1003.1759%2C1003.2851%2C1003.2444%2C1003.5529%2C1003.2597%2C1003.2531%2C1003.1800%2C1003.1994%2C1003.4612%2C1003.3623%2C1003.2341%2C1003.1490%2C1003.4704%2C1003.4846%2C1003.0892%2C1003.0780%2C1003.5048%2C1003.2811%2C1003.0375%2C1003.2866%2C1003.4680%2C1003.0615%2C1003.2527%2C1003.2202%2C1003.3804%2C1003.4315%2C1003.0315%2C1003.0931%2C1003.4741%2C1003.5899%2C1003.5893%2C1003.5812%2C1003.1452%2C1003.2530%2C1003.0034%2C1003.1856%2C1003.0387%2C1003.4929%2C1003.5590%2C1003.2714%2C1003.3100%2C1003.3718%2C1003.0806%2C1003.5578%2C1003.5975%2C1003.5646%2C1003.1836%2C1003.1257%2C1003.4407%2C1003.1304&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Data generated in the fields of science, technology, business and in many\nother fields of research are increasing in an exponential rate. The way to\nextract knowledge from a huge set of data is a challenging task. This paper\naims to propose a hybrid and viable method to deal with an information system\nin data mining, using topological techniques and the significance of the\nattributes measured using rough set theory, to compute the reduct, This will\nreduce the randomness in the process of elimination of redundant attributes,\nwhich, in turn, will reduce the complexity of the computation of reducts of an\ninformation system where a large amount of data have to be processed."}, "authors": ["P. G. JansiRani", "R. Bhaskaran"], "author_detail": {"name": "R. Bhaskaran"}, "author": "R. Bhaskaran", "links": [{"href": "http://arxiv.org/abs/1003.4067v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1003.4067v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1003.4067v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1003.4067v1", "arxiv_comment": null, "journal_reference": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "fulltext": "JOURNAL OF COMPUTING, VOLUME 2, ISSUE 3, MARCH 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n50\n\nComputation of Reducts Using Topology and\nMeasure of Significance of Attributes\nP.G. JansiRani and R.Bhaskaran\nAbstract-Data generated in the fields of science, technology, business and in many other fields of research are increasing in\nan exponential rate. The way to extract knowledge from a huge set of data is a challenging task. This paper aims to propose a\nhybrid and viable method to deal with an information system in data mining, using topological techniques and the significance of\nthe attributes measured using rough set theory, to compute the reduct, This will reduce the randomness in the process of\nelimination of redundant attributes, which, in turn, will reduce the complexity of the computation of reducts of an information\nsystem where a large amount of data have to be processed.\nKey terms - Data mining, topology, rough set theory, reducts.\n\n---------- \uf075 ----------\n\n1 INTRODUCTION\n\nF\n\nOR centuries, extracting information from data has\nbeen done manually. But extracting useful information\nfrom a huge set of data, though highly inevitable, has\nbeen highly complicating and challenging. The increasing\nvolume and complexity of data have necessitated the application tools of automatic data processing, and methods\nand models for converting data into useful information\nand knowledge. The process of data mining, in the modern\nworld, plays a vital role. Neural networks and traditional\nstatistical techniques have been compared in [14], in the\nprediction of academic performance of a specific business\nschool in India. Broad based study can be done in this\nprocess using rough set theory. The Rough set theory plays\na significant role in reducing the complexity of the process\nand automatically extracting useful information from a\nhuge set of data.\nThe paper is organized as follows: Section 1 deals with\nIntroduction, Section 2 Literature review, Section 3\npresents the theoretical background and the basic concepts\nof rough set theory and topology and Section 4 describes\nthe methodology to compute the reduct through two algorithms: one for the computation of base using indiscernity\nmatrix and the other to confirm the redundancy of an\nattribute for the given information system. Section 5 illustrates our method with an example. It concludes in section\n6 dealing with the possibility for future work.\n\nrevenue growth rate has been proposed. Analysis on HR\ndata in [8], applying data mining techniques is an efficient\nand viable method in the decision making process. The\nmethod proposed to examine the vital factors influencing\nthe sharing of the electronic information of the local agency has been analyzed in [1] and a model has been proposed in [2] for effective sharing of information system\nthat plays a significant role and can be extended globally\nto study the process of attribute reduction. Variable precision dominance-based-rough set approach to attribute\nreduction that has been proposed in [12], is effective only\nwhen the conditional attributes and the decision\nattributes are ordinal and monotonically related. In [4],\nweighted rules by using a weighted rough set based method have been extracted. In [1], rough sets have been\napplied to identify the set of significant symptoms causing rhinology and throatology diseases and to extract decision rules in Taiwan's Otolaryngology clinic data but\nthe scope is limited to data of rhinology and throatology\nonly. The essential principles and methods of rough set\ntheory for the mining classification rules to the differentiation of symptoms and signs of Traditional Chinese\nMedicine (TCM) for cancer patients have been introduced\nin [15]. There is a scope for better accuracy in the algorithm proposed to select significant features based on\ncorrelation-based feature selection and by integrating\nfeature selection techniques used in [18] [6] [19] [3]. Balanced information gain with a novel extension, to meas2 LITERATURE REVIEW\nure the contribution of each feature is analyzed in [23].\nAn association rule mining algorithm that can handle Predominant correlation and a fast filter method which\ndifferent types of data based on fuzzy techniques has can identify relevant features as well as redundancy\nbeen developed in [21]. A procedure [25] using feature among relevant features without pair wise correlation\nselection method and rough set classifier to predict firms' analysis are studied in [11]. A new filter approach to the\nfeature selection that uses a correlation-based heuristic to\nevaluate the worth of feature subsets suggested in [13], is\n----------------\nmore effective only if the features are linearly correlated.\n\uf0b7 P.G.JansiRani is with Department of Mathematics, Sethu Institute of\nTechnology, Virudhunagar-626115, Tamil Nadu, India.\nTopological structures have been used to obtain discerni\uf0b7 R.Bhaskaran is with School of Mathematics, Madurai Kamaraj University, bility matrix and discernibility function for knowledge\nMadurai-625021, Tamil Nadu, India.\nreduction and decision making in [10]. But in this process\nthe elimination of the redundant attribute, in a random\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 3, MARCH 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\nmanner, makes the process uncertain. A reduct construction method in [24], based on discernibility matrix simplification has been analyzed. To find all reducts based on\nindiscernibility-discernibility and to reduce the discernity\nfunction to a function of disjunction of conjunction discussed in [20] [17] [26] is NP hard and hence, it is necessary to find an improved method to make the process\nsimpler.\nThis paper aims to propose a hybrid method to compute the reduct of an information system by reviewing\nthe topological techniques applied in the information system in [10] and the feature selection method proposed in\n[18]. We find that the hybrid method proposed in this\nstudy is a viable approach in finding the minimal reduct\nof an information system. The process is divided into two\nstages: (i) Ranking the features according to the significance of the attributes, which is taken as a probable predicting ability of attributes, and (ii) confirmation of the\nredundancy of the ranked attributes by applying the algebraic topological techniques in the information system.\n\nsub-base of X then\nbase <A> = base< A1 U A2 >\n= base< base< A1> U base< A2> >\nbase <A> = base<G1 U G2>,\nwhere A= A1U A2,\nG1= base< A1>, G2=<base< A2>. It can also be generalized as base <A> = base< A1 U A2 ...U An > = base<\nbase< A1> U base< A2>...U base<An> > base <A> =\nbase< A1U A2 ... U An > = base<G1 U G2 ... UGn>\nwhere A= A1UA2 ... U An, G1= base< A1>, G2=<base<\nA2> ... Gn=base<An>.\n\n3.4 Basic Concepts OF Rough SET THEORY\n\n3 TEHEORETICAL BACKGROUNDS\n\nRough set theory, an extension of set theory introduced\nby Pawlak in the late 70's provides a sound basis for the\nstudy of information system to extract the vital information hidden in the data set. The main idea of rough approximation is that a set can be represented by a lower\napproximation and an upper one. One of the objectives of\nrough set data analysis is to reduce data size by finding\nthe reduct, the minimal set of attributes which preserves\nthe classification power of the original set. The main advantage of the rough set data analysis is that it does not\nuse information outside the target data sets.\n\n3.1 Basic Concepts of Topology\n\n3.5 Information System\n\nAs demonstrated in this document, the numbering for\nsections upper case Arabic numerals, then upper case\nArabic numerals, separated by periods. Initial paragraphs\nafter the section title are not indented. Only the initial,\nintroductory paragraph has a drop cap.\nTopology branch of mathematics deals with the concept of \"Unaffected by changing the size, shape and dimension\". Algebraic Topology includes the concept of set\ntheory in Topology. A topology on a set X is a collection\n\nAn information system is defined in [7], as a set of objects\nS = { U, A, V, f } where U set of objects, A , attributes ,V ,\nthe value given to the attributes defined by the, relation f\nas f , U X A \uf0e0 V\n\n3.6 Equivalence relation\nLet X be a set and let x, y, and z be elements of X. An\nequivalence relation R on X is a Relation on X such that R\nsatisfies (i)Reflexive Property, xRx for all x in X (ii)\nSymmetric Property, if xRy, then yRx and (iii) Transitive\nProperty, if xRy and yRz, then xRz.\n\n\uf0c1 subsets of X which has the following properties, \u0424\nand X are in \uf0c1 .\n3.7 Equivalence classes\n(i)The union of the elements of any sub collection of \uf0c1 is The equivalence relation R defines a partition on A; such\nin \uf0c1 .\na partition is a set of all equivalence classes of R.\nof\n\n(ii)The intersection of the elements of any finite sub collection of \uf0c1 is in \uf0c1 .\nA set X for which a topology \uf0c1 has been specified is\ncalled a topological space.\nA linearly independent subset S of the vector space L\nwhich span the whole space L is called a basis of L.\n\n3.2 Base for a Topology\nGiven a Topological space (X,\n\n\uf0c1)\n\na collection B (X) of\n\nopen subsets of X is known to be a base for topology\nB(X)\n\n\uf0c1\n\n\uf0c1\n\nif\n\nDiscernibility and Indiscernibility relation of objects in\nthe information system in rough set theory are two main\nconcepts which are very effective in classification, characterization and clustering the objects. Discernibility matrix\nconsists of entries or set of attributes which discern two\nobjects, where the indiscerniblity matrix IDxy consist entries of attributes which are common to two objects defined by\nIDxy = { a \u0404 A , f (a, x) = f(a, y)} where x, y \u0404 U }\n\n3.9 Reduct\n\nEach member of\nmember of B(X)\n\n\uf0c1\n\ncan be expressed as the union of the\n\n3.3 Sub-base\n\nIf (X, \uf0c1 ) topological space, a collection A of subsets of X\nis said to be a sub-base for T iff finite intersections of\nmembers of A form a base for\nIf (X,\n\n3.8 Discernibility and Indiscernibility\n\n\uf0c1)\n\n\uf0c1.\n\ntopological space and a collection A is a\n\nReduct is the minimal set of attributes preserving classification power on original data set A [9], which can be derived by finding the basis of the vector space.\n\n4 METHODOLOGIES\nThis section is concerned with a viable and effective method for the computation of reduct. The idea of the topological concept is used in [10], to find the reducts by find-\n\n51\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 3, MARCH 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\ning the base for the equivalence relation set taken from\nthe given information system. The minor disadvantages\nof this process are, (1) if the data is very large, the process\nto find the base by removing the set of elements corresponding to the attributes one by one without any measure on the significance of attributes randomly makes the\nprocess uncertain and (2) it will not prevent unnecessary\ncomputations. In order to avoid unnecessary computations, a modified procedure has been suggested. In the\nfirst phase, instead of checking the redundancy of\nattributes in a random manner as proposed in [18],\nattributes are checked in the descending order according\nto their measure of significance which guides in the\npromising direction and in turn reduces the uncertainty\nand saves the computational costs which is more important in process of feature filtering. This measure may not\nbe fully sufficient to decide the most significant feature.\nAnyhow, after testing a few databases this measure\nshows that definitely the first attribute having the least\nmeasure of the significance and atleast a few attributes in\nthe initial stages are most probably redundant. This will\nmake the process of elimination of redundant attributes\nin the initial stage itself which in turn reduce the unnecessary computations. In the second phase, comparing the\nbase after removing features corresponding to the most\nprobable redundant attribute, with the original sub base,\nwill confirm the redundancy of the attribute. Another\nprimary challenge in data mining is handling a large set\nof data. To meet the challenge of mining from a vast\namounts of data, a viable method using algebraic topological techniques is proposed for the computation of the\nbase from the sub base as a base of union of bases of its\ndisjoint subsets which makes the process further simple\nby using the algebraic topological property, base <A> =\nbase< A1 U A2 > = base< base<A1> U base<A2> >, where\nA=A1UA2. Based on the methodology described here two\nalgorithms have been developed. Algorithm1 describes\nthe method to find the base from a sub base using indiscernity matrix. Algorithm 2 describes the method to compute the base from the base of union of two bases which\nwill greatly reduce the complexity of the process since\nlarge data set is divided into a union of small data sets.\n\n52\n\nwill form the base G for the topology\n\n\uf0c1 on X\n\nAlgorithm 2: To compute the base (reduct) by dividing the entire data set into subsets\nStep 1: Convert the information system I into a matrix\nform.\nStep 2: Divide the entire information system A into two\ndisjoint sets A1 and A2\nStep 3: Find the base of A1 and base of A2 seperately.\nStep 4: Compute base of A from the base of union of\nbases of A1 and A2.\nBase<A>=base < base <A1> U base <A2> >\n\nAlgorithm 3: To compute the base (reduct) after\neliminating the redundant attributes\nStep 1: Convert the information system I into a matrix\nform.\nStep 2: Compute measure of significance for all the\nattributes in the information system.\nStep 3: Attributes in A are arranged according to the ascending order of their measure of significance.\nStep 4: Sub base A is divided into two disjoint sets A1, A2\nwhere the first set A1 has members corresponding to the\nattributes having minimum measure of significance\n(number of elements in A1 and A2 can be decided with\nrespect to the problem) and A2 has members corresponding to the attributes having next measure of significance.\nStep5: The attribute which have least measure of significance value is first removed from A1 and if\nbase<A>=base<base<(A1-(members corresponding to the\nremoved attribute))>U base<A2>> confirms the redundancy of that removed attribute.\nStep 6: The process is repeated until all attributes in set\nA1 are tested for their redundancy and the same process\nis repeated for A2 also.\nStep 7: The set of all attributes which cannot be removed\nfrom A1 and A2 is the reduct of the attributes of the given information system.\n\nIllustration\nAlgorithms 1, 2 and 3 are illustrated with an example discussed in [10], of seven segments display of the numbers.\n\nAlgorithm 1: To compute the base (reduct) from\nindiscernity matrix\nStep 1: Convert the information system I into a matrix\nform.\nStep 2: Equivalence relation set A is formed from the given information system I.\nStep 3: The equivalence relation set A got in step 2 is converted into a matrix B.\nStep 4: Find the indiscernity matrix for the matrix B\nStep 5: Remove the repeated indiscern elements from the\nindiscernity matrix of B\nStep 6: Again find the indiscernity matrix from the remaining set of indiscernable elements.\nStep 7: Continue this process until there is no further indiscernable elements.\nStep 8: If all the elements of the given set X are not available in the last indiscernity matrix, all missing elements\nare taken from the previous indiscernity matrices which\n\nFig.1 seven segments display of the numbers\n\n4.1 Illustration of Algorithm 1 with an example\nAlgorithm 1: To find the base from a sub base using indiscernity matrix is illustrated with an example of seven\nsegments display of the numbers.\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 3, MARCH 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\nStep 1: As an initial step of the algorithm, the matrix representation of seven segments display of the numbers\n0,1,2,3,4,5,6,7,8,9 analyzed in [10] depicted in figure 1 is\ngiven in the information Table 1.\nTABLE 1\nLed display Matrix A\nStep 2: Equivalence relation set A of the information table1 given in step 1 which is taken as a sub base of the\ninformation system is given below\nA= {U/IND(a) = { 0,2,3,5,6,7,8,9}, {1,4},\nU/IND(b)= {0,1,2,3,4,7,8,9}, {5,6},\nU/IND(c) = {0, 1, 3,4,5,6,7,8,9}, {2},\nU/IND(d)= {0, 2,3,5,6,8,9}, {1,4,7},\nU/IND(e) = {0, 2, 6, 8},{1, 3, 4, 5, 7, 9},\nU/IND(f) = {0, 4,5,6,8,9},{1, 2, 3,7},\nU/IND(g) = {0, 1, 7} ,{2, 3, 4, 5, 6, 8, 9} }\nStep 3: Equivalence relation matrix B formed from the\nequivalence relation set A from which indiscernity matrix\nto be found is given in Table 2.\nTable 2\nEquivalence relation matrix B\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n0\n\n-\n\n2\n\n3\n\n-\n\n5\n\n6\n\n7\n\n8\n\n9\n\n2\n\n-\n\n1\n\n-\n\n-\n\n4\n\n-\n\n-\n\n-\n\n-\n\n-\n\n3\n\n0\n\n1\n\n2\n\n3\n\n4\n\n-\n\n-\n\n7\n\n8\n\n9\n\n4\n\n-\n\n-\n\n-\n\n-\n\n-\n\n5\n\n6\n\n-\n\n-\n\n-\n\n5\n\n0\n\n1\n\n-\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n6\n\n-\n\n-\n\n2\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n7\n\n0\n\n-\n\n2\n\n3\n\n-\n\n5\n\n6\n\n-\n\n8\n\n9\n\n8\n\n-\n\n1\n\n-\n\n-\n\n4\n\n-\n\n-\n\n7\n\n-\n\n-\n\n9\n\n0\n\n-\n\n2\n\n-\n\n-\n\n-\n\n6\n\n-\n\n8\n\n-\n\n10\n\n-\n\n1\n\n-\n\n3\n\n4\n\n5\n\n-\n\n7\n\n-\n\n9\n\n11\n\n0\n\n-\n\n-\n\n-\n\n4\n\n5\n\n6\n\n-\n\n8\n\n9\n\n12\n\n-\n\n1\n\n2\n\n3\n\n-\n\n-\n\n-\n\n7\n\n-\n\n-\n\n13\n\n0\n\n1\n\n-\n\n-\n\n-\n\n-\n\n-\n\n7\n\n-\n\n-\n\n14\n\n-\n\n-\n\n2\n\n3\n\n4\n\n5\n\n6\n\n-\n\n8\n\n9\n\nSteps 4, 5, 6 and 7: The results of the process explained in\nalgorithm1 in steps 4, 5 and 6 to find the indiscernible\nelements, until there is no indiscernible elements could be\nfound from the indiscernity matrix.\n\nBase for the entire set X=G={0,3,5,6,8,9 U members corresponding to the missing elements taken from the previous indiscernity matrix }. Therefore the sets containing\nthe missing terms 1,2,4,7 are taken from the previous indiscernity matrix and therefore the base for the topology\nT on X =\nG={{0},{3},{5},{6},{8},{9},{1},{2},{4},{7}}\n\n4.2 Illustration of Algorithm 2, 3 with an example\nAlgorithm 2: To find the base from a sub base using indiscernity matrix is illustrated with an example of seven\nsegments display of the numbers.\nStep 1: As an initial step of the algorithm, the matrix representation of seven segments display of the numbers\n0,1,2,3,4,5,6,7,8,9 depicted in figure 1 is given in information table1 as described in [10].\nSteps 2, 3: The results of the measure of significance of\nattributes [22] are calculated difference between the positive region of the decision rules with and without the\nattribute concerned using the data mining software RSES\n(Rough Set Exploration System) are given in Table 3.\nFrom the measure of the significance of all the attributes\ngiven in ascending order in Table 3 it is found that the\nmost probable redundant attributes are identified based\non the significance of attributes. In this example the\nattributes c, d have the least and attribute a, f, g have the\nnext minimum measure.\n\n53\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 3, MARCH 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\n54\n\n5 CONCLUSIONS\nTABLE 3\nSignificance of Attributes\nSl.No.\nAttribute\nMeasure of\nsignificance\n1\nC\n0\n2\nD\n0\n3\nA\n0.2\n4\nF\n0.2\n5\nG\n0.2\n6\nB\n0.4\n7\nE\n0.4\nStep 4: The small disadvantage of the method proposed in\n[18] which is explained in step3 is rectified in step 4 by\ndividing the sub base into union of sub sets based on\nmeasure of significance and applying algebraic topological techniques in the information system. We divide the\nset A, the set of equivalence relations on X into two sets\nA1and A2 with lower and higher measure of significance\nrespectively\nA1 = { U/IND(c) = {0, 1,3,4,5,6,7,8,9} {2},\nU/IND(d) = {0,2,3,5,6,8,9} {1,4,7}\nU/IND(a) = { 0,2,3,5,6,7,8,9} {1,4},\nU/IND (f) = {0, 4, 5, 6, 8, 9} {1, 2, 3, 7}\nU/IND (g) = {0, 1, 7} {2, 3, 4, 5, 6, 8, 9} }\nA2 = {U/IND (b) = {0, 1, 2, 3, 4, 7, 8, 9} {5, 6},\nU/IND (e) = {0, 2, 6, 8} {1, 3, 4, 5, 7, 9} }\nStep 5: Form a new set B1 by removing the elements from\nA1 corresponding to the attribute having least significant\nmeasure (in this example attribute 'c').\nB1={ {0,2,3,5,6,8,9} {1,4,7}\n{0, 2, 3, 5, 6, 7, 8, 9}{1,4}, {0, 4, 5,6,8,9} {1, 2, 3,7}, {0, 1, 7} {2,\n3, 4, 5, 6, 8, 9} }\nA2 ={{ {0, 1,2,3,4,7,8,9} {5,6}, {0, 2, 6, 8} {1, 3, 4, 5, 7, 9} }\nbase<B1>=G1={{0},{1}{2,3},{4},{7}, {5,6,8,9} }\nbase < A2>= G2={{0,2,8},{5},{6},{1,3,4,7,9}}\nbase<A>= base< B1 U A2 > = base< base<B1> U\nbase<A2> >\nbase<A>=base<G1UG2>={{1},{2},{3},{4},{5},{6},{7},{8},{9},\n{10} }\nwhich is same as the base for the entire set X which\nproves that attribute 'c' is a redundant attribute. Therefore 'c' will leave the basis.\nSteps 6 and 7: The process done in step 5 is repeated for\nall the remaining attributes in the descending of its significance values, in the set B1 and A2. By proceeding like\nthis, it is proved that attribute'd' is also a redundant\nattribute. Therefore attribute'd' also left the basis. The set\nof remaining attributes {a, e, b, f, g} got in the final step\nform the reduct of the given of information system.\n\nThis paper has dealt with the challenging data mining\nproblem of feature reduction to extract the hidden information from a huge set of data in a simplified method. In the method proposed in this work to find the\nreduct-a minimal set of features that preserves the indiscernibility relation of an information system, it is\nfound that the elimination of the redundant attributes\nbased on measure of significance of attributes guides\nin the promising direction which in turn reduces the\nuncertainty and saves the computational costs which is\nmore important in feature filtering process instead of\ngoing in a random manner. Linear correlation measure\nbetween attributes will be effective only if the features\nare linearly correlated, but linear correlation cannot\nalways be expected between the features. The concept\nof measure of significance of attributes used in this\nstudy overcomes this shortcoming. Also the method\nproposed in this paper for the computation of the base\nfrom the sub base as a base of union of bases of its disjoint subsets applying algebraic topological property\nwill reduce the challenging task of handling a large set\nof data at a time. Another advantage of this method is\nthat it will derive the most required reduct of the information system. The algorithm proposed in this\nwork can be extended to a problem with uncertain and\nmissing data. It is intended to expand the model proposed in this study to improve further the certainty in\nthe filtering process of redundant attributes using\nweighted rough set model.\n\nREFERENCES\n[1]\n\n[2]\n\n[3]\n\n[4]\n\n[5]\n\n[6]\n\nAsli Yagmur Akbulut, Peter Kelle, Suzanne D. Pawlowski,\nHelmut Schneider and Clayton A. Looney. (2009) 'To share or\nnot to share? Examining the factors influencing local agency\nelectronic information sharing', Journal of information system,\nVol.4, No.2, pp 143-172.\nBikram Jit Rishi and Goyal,D.P. (2008) 'Designing a model for\nthe development of strategic information systems in Indian\npublic sector undertakings', Journal of Business Information\nSystems, Vol. 3, No. 5, pp.529-548.\nChou,T.S., Yen,K.K. and Luo,J. (2008) 'Network Intrusion Detection Design Using Feature Selection of Soft Computing Paradigms', Journal of computational intelligence,Vol.4, No.3, pp\n196-208.\nJinfu Liu, Qinghua Hu, Daren Yu. (2008) 'A weighted rough set\nbased method developed for class imbalance learning', Journal\nof Information Sciences, Vo.178, No 4, pp1235-1256.\nHsu-hao Yang, Chang-Lun Wu. (2009) 'Rough sets to help medical diagnosis-Evidence from a Taiwan's clinic', journal of Expert systems with Applications,Vol.36, No 5, pp 9293-9298\nIndraDevi, M.and Rajaram, .R and Selvakuberan, K. (2008)\n'Generation of best features for webpage classification', Journal\nof Webology, Vol.5,No.1\n\n\fJOURNAL OF COMPUTING, VOLUME 2, ISSUE 3, MARCH 2010, ISSN 2151-9617\nHTTPS://SITES.GOOGLE.COM/SITE/JOURNALOFCOMPUTING/\n\n[7]\n\n[8]\n\n[9]\n\n[10]\n\n[11]\n\n[12]\n\n[13]\n\n[14]\n\n[15]\n\n[16]\n\n[17]\n\n[18]\n\n[19]\n\n[20]\n\n[21]\n\n[22]\n\nJacek Sienkiewicz (1995) Rough sets for Boolean functions Minimization.Research Report of Institute of Computer Science.\nBulletin of the Rough Set Community EBRSC. Warsaw University of Technology, Electronic Bulletin of the Rough Set Community Vol.3 No.2\nJayanthi Ranjan and Goyal, D.P. (2008) 'Data mining techniques\nfor better decisions in human resource management systems',\nJournal of Business Information Systems, Vol. 3, No. 5, pp. 464481.\nKeyun Hu, Yuchang Lu and Chunyi Shi ( 1982) 'Feature Ranking in Rough Sets', Journal of Computer and Information\nSciences, Vo.11, No.5, pp.341-356.\nLashin, E.F.andMedha, T. (2005) 'Topological reduction of information systems', Journal of Chaos solitons and fractals,\nVol.25, No.2, pp.277 \u2013 286.\nLei Yu and Huan Liu. (2003) 'Feature selection for HighDimensional Data, A fast correlation\u2013Based Filter solution', Paper presented at the 12th, International Conference on Machine\nLearning. August 21-24, 2003. Washington.\nMasahiro Inuiguchi, Yukihiro Yoshioka, Yoshifumi Kusunki.\n(2009) 'Variable-precision dominance-based rough set approach\nand attribute reduction', Journal of Approximate reasonin, Elsevier, In Press, Corrected Proof, Available online.\nMark A.Hall.and Lloyd Smith, A. (1998) 'Feature selection for\nMachine learning, Comparing a correlation\u2013based Filter approach to the Wrapper', \u00a9 1998, American Association for Artificial Intelligence (www.aaai.org).\nMukta Paliwai Usha A.Kumar. (2009) 'A study of academic\nperformance of business school graduates using neural network and statistical techniques', Journal of Expert systems with\nApplications, Vol.36, N0.4, pp.7865-7872.\nQianchao Pang, Anji Hou, Quanping, Huaping Tang. (2007)\n'Mining Classification rules of cancer patients for traditional\nChinese medical treatments, A Rough set Based Approach'. Paper presented at the, 4th International Conference Fuzzy Systems and Knowledge Discovery. August 24-27, 2007. China.\nSarika Sharma Apeejay, Goyal,D.P. and. Mittal, R. K. (2008)\n'Data mining research for customer relationship management\nsystems, a framework and analysis', Journal of Business Information Systems, Vol. 3, No. 5, pp. 549-565.\nStarzyk, J.A., Nelson, D.E. and Sturtz, K. (1999) 'Reduct Generation in Information Systems', Bulletin of International Rough\nSet Society, Vol.3,No 3,pp.19-22.\nTe-Shun Chou, Kang K.Yen and Jun Luo. (2007) 'CorrelationBased feature selection for intrusion detection design',1-42441513-06/07 IEEE\nThangavel, K., Karnan, M.andPethalakshmi, A. (2005) 'Performance Analysis of Rough Reduct Algorithms in Mammogram\n'Journal on Graphics, vision and Image processing, Vol.5, No.8,\npp.13-21.\nYan Zhao and Yiyu Yao. (2007) 'Data analysis based on discernibility and indiscernibility', Journal of Feng Luo, Information\nsciences, Vol.177, No.1, pp.4959 \u2013 4976.\nYen-Liang Chen, Cheng-Hsiung Weng (2009), 'Mining fuzzy\nassociation rules from questionnaire data', Journal Knowledged\nBased System, Vo.22, No.1, pp.46-56.\nYeong Min Kim, Chee Kyeong Kim, Jae Cheol Lee, \"Rough set\nalgorithm for crack categary determination of reinforced concrete structures\", Advances in Engineering Software, Vol.40,\nNo.3, March 2009. Pp. 202-211.\n\n[23] Yimin Wu and Aidong Zhang, (2004) 'Feature selection for\nclassifying High-Dimensional Numerical Data', Proceedings of\nthe 2004 IEEE Computer Society Conference. Computer vision\nand Pattern Recognition. 27th June - 2nd July 2004 Washington.\n[24] Yiyu Yao,Zhao, (2009) 'Discernibility matrix simplification for\nconstructing attribute reducts', Journal of Information Sciences,\nVol.179, No.7, pp.867-882.\n[25] You-Shyang Chen, Cing-Hsue Cheng. (2009) 'Evaluating industry performance using extracted RGR rules based on feature\nselection and rough sets classifier', Journal of Expert systems\nwith Applications, Vol.36, No.5, pp.9448-9456.\n[26] Yongguang Bao, Xiaoyong Du, Mingrong Deng and Naohiro\nIshii. (2004) 'An Efficient Method for Computing All Reducts',\nTransactions of the Japanese Society for Artificial Intelligence,\nVol. 19, No. 3, pp.166-173.\n\nP.G.JansiRani - she completed her MSc (Applied Mathematics) in\n1987 and M.Phil (Mathematics)in the year 1989, Bharathidasan\nUniversity,Tiruchirapalli . She worked as lecturer in the Department\nof Mathematics at periyar Maniyammai College of Technology for\nwomen, Tamil Nadu from 1990 to 1995.Since 1995 she is working as\nProfessor in the Department of Mathematics at Sethu Institute of\nTechnology, Virudhunagar, Tamilnadu. She has 20 years of teaching\nexperience in Engineering College. She is also a research scholar at\nSchool of Mathematics, Madurai Kamaraj University, Madurai,\nTamilnadu, India. Fields of interest: Data Mining and Rough set\ntheory, Operations Research.\n\nR.Bhaskaran - He did his M.Sc., at IIT Chennai in 1973 and obtained his Ph.D at the Ramanujan Institute for Advanced study in\nMathematics, University of Madras in 1980. He joined the Madurai\nKamaraj University in 1980, as a lecturer. Now, he is working as a\nsenior professor in the School of Mathematics. His areas of interest\nare Non-archimedean functional analysis; Image processing, Data\nMining, Software development for learning Mathematics and Character Recognition.\n\n55\n\n\f"}