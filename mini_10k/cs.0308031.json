{"id": "http://arxiv.org/abs/cs/0308031v1", "guidislink": true, "updated": "2003-08-20T09:40:25Z", "updated_parsed": [2003, 8, 20, 9, 40, 25, 2, 232, 0], "published": "2003-08-20T09:40:25Z", "published_parsed": [2003, 8, 20, 9, 40, 25, 2, 232, 0], "title": "Artificial Neural Networks for Beginners", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0011022%2Ccs%2F0011033%2Ccs%2F0011006%2Ccs%2F0011023%2Ccs%2F0011002%2Ccs%2F0011042%2Ccs%2F0011020%2Ccs%2F0308009%2Ccs%2F0308010%2Ccs%2F0308042%2Ccs%2F0308003%2Ccs%2F0308020%2Ccs%2F0308006%2Ccs%2F0308023%2Ccs%2F0308017%2Ccs%2F0308022%2Ccs%2F0308040%2Ccs%2F0308025%2Ccs%2F0308021%2Ccs%2F0308012%2Ccs%2F0308014%2Ccs%2F0308013%2Ccs%2F0308044%2Ccs%2F0308001%2Ccs%2F0308035%2Ccs%2F0308034%2Ccs%2F0308038%2Ccs%2F0308039%2Ccs%2F0308026%2Ccs%2F0308007%2Ccs%2F0308005%2Ccs%2F0308002%2Ccs%2F0308043%2Ccs%2F0308033%2Ccs%2F0308027%2Ccs%2F0308008%2Ccs%2F0308004%2Ccs%2F0308019%2Ccs%2F0308041%2Ccs%2F0308015%2Ccs%2F0308024%2Ccs%2F0308011%2Ccs%2F0308037%2Ccs%2F0308018%2Ccs%2F0308030%2Ccs%2F0308031%2Ccs%2F0308016%2Ccs%2F0308032%2Ccs%2F0308028%2Ccs%2F0308036%2Ccs%2F0308029%2Ccs%2F0504015%2Ccs%2F0504024%2Ccs%2F0504027%2Ccs%2F0504017%2Ccs%2F0504099%2Ccs%2F0504075%2Ccs%2F0504083%2Ccs%2F0504093%2Ccs%2F0504084%2Ccs%2F0504088%2Ccs%2F0504012%2Ccs%2F0504058%2Ccs%2F0504005%2Ccs%2F0504097%2Ccs%2F0504044%2Ccs%2F0504041%2Ccs%2F0504063%2Ccs%2F0504016%2Ccs%2F0504043%2Ccs%2F0504033%2Ccs%2F0504034%2Ccs%2F0504053%2Ccs%2F0504071%2Ccs%2F0504004%2Ccs%2F0504064%2Ccs%2F0504068%2Ccs%2F0504030%2Ccs%2F0504032%2Ccs%2F0504065%2Ccs%2F0504013%2Ccs%2F0504090%2Ccs%2F0504104%2Ccs%2F0504046%2Ccs%2F0504035%2Ccs%2F0504080%2Ccs%2F0504092%2Ccs%2F0504111%2Ccs%2F0504110%2Ccs%2F0504055%2Ccs%2F0504091%2Ccs%2F0504095%2Ccs%2F0504074%2Ccs%2F0504089%2Ccs%2F0504052%2Ccs%2F0504085%2Ccs%2F0504079%2Ccs%2F0504029%2Ccs%2F0504067%2Ccs%2F0504019%2Ccs%2F0504076&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Artificial Neural Networks for Beginners"}, "summary": "The scope of this teaching package is to make a brief induction to Artificial\nNeural Networks (ANNs) for people who have no previous knowledge of them. We\nfirst make a brief introduction to models of networks, for then describing in\ngeneral terms ANNs. As an application, we explain the backpropagation\nalgorithm, since it is widely used and many other algorithms are derived from\nit. The user should know algebra and the handling of functions and vectors.\nDifferential calculus is recommendable, but not necessary. The contents of this\npackage should be understood by people with high school education. It would be\nuseful for people who are just curious about what are ANNs, or for people who\nwant to become familiar with them, so when they study them more fully, they\nwill already have clear notions of ANNs. Also, people who only want to apply\nthe backpropagation algorithm without a detailed and formal explanation of it\nwill find this material useful. This work should not be seen as \"Nets for\ndummies\", but of course it is not a treatise. Much of the formality is skipped\nfor the sake of simplicity. Detailed explanations and demonstrations can be\nfound in the referred readings. The included exercises complement the\nunderstanding of the theory. The on-line resources are highly recommended for\nextending this brief induction.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0011022%2Ccs%2F0011033%2Ccs%2F0011006%2Ccs%2F0011023%2Ccs%2F0011002%2Ccs%2F0011042%2Ccs%2F0011020%2Ccs%2F0308009%2Ccs%2F0308010%2Ccs%2F0308042%2Ccs%2F0308003%2Ccs%2F0308020%2Ccs%2F0308006%2Ccs%2F0308023%2Ccs%2F0308017%2Ccs%2F0308022%2Ccs%2F0308040%2Ccs%2F0308025%2Ccs%2F0308021%2Ccs%2F0308012%2Ccs%2F0308014%2Ccs%2F0308013%2Ccs%2F0308044%2Ccs%2F0308001%2Ccs%2F0308035%2Ccs%2F0308034%2Ccs%2F0308038%2Ccs%2F0308039%2Ccs%2F0308026%2Ccs%2F0308007%2Ccs%2F0308005%2Ccs%2F0308002%2Ccs%2F0308043%2Ccs%2F0308033%2Ccs%2F0308027%2Ccs%2F0308008%2Ccs%2F0308004%2Ccs%2F0308019%2Ccs%2F0308041%2Ccs%2F0308015%2Ccs%2F0308024%2Ccs%2F0308011%2Ccs%2F0308037%2Ccs%2F0308018%2Ccs%2F0308030%2Ccs%2F0308031%2Ccs%2F0308016%2Ccs%2F0308032%2Ccs%2F0308028%2Ccs%2F0308036%2Ccs%2F0308029%2Ccs%2F0504015%2Ccs%2F0504024%2Ccs%2F0504027%2Ccs%2F0504017%2Ccs%2F0504099%2Ccs%2F0504075%2Ccs%2F0504083%2Ccs%2F0504093%2Ccs%2F0504084%2Ccs%2F0504088%2Ccs%2F0504012%2Ccs%2F0504058%2Ccs%2F0504005%2Ccs%2F0504097%2Ccs%2F0504044%2Ccs%2F0504041%2Ccs%2F0504063%2Ccs%2F0504016%2Ccs%2F0504043%2Ccs%2F0504033%2Ccs%2F0504034%2Ccs%2F0504053%2Ccs%2F0504071%2Ccs%2F0504004%2Ccs%2F0504064%2Ccs%2F0504068%2Ccs%2F0504030%2Ccs%2F0504032%2Ccs%2F0504065%2Ccs%2F0504013%2Ccs%2F0504090%2Ccs%2F0504104%2Ccs%2F0504046%2Ccs%2F0504035%2Ccs%2F0504080%2Ccs%2F0504092%2Ccs%2F0504111%2Ccs%2F0504110%2Ccs%2F0504055%2Ccs%2F0504091%2Ccs%2F0504095%2Ccs%2F0504074%2Ccs%2F0504089%2Ccs%2F0504052%2Ccs%2F0504085%2Ccs%2F0504079%2Ccs%2F0504029%2Ccs%2F0504067%2Ccs%2F0504019%2Ccs%2F0504076&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The scope of this teaching package is to make a brief induction to Artificial\nNeural Networks (ANNs) for people who have no previous knowledge of them. We\nfirst make a brief introduction to models of networks, for then describing in\ngeneral terms ANNs. As an application, we explain the backpropagation\nalgorithm, since it is widely used and many other algorithms are derived from\nit. The user should know algebra and the handling of functions and vectors.\nDifferential calculus is recommendable, but not necessary. The contents of this\npackage should be understood by people with high school education. It would be\nuseful for people who are just curious about what are ANNs, or for people who\nwant to become familiar with them, so when they study them more fully, they\nwill already have clear notions of ANNs. Also, people who only want to apply\nthe backpropagation algorithm without a detailed and formal explanation of it\nwill find this material useful. This work should not be seen as \"Nets for\ndummies\", but of course it is not a treatise. Much of the formality is skipped\nfor the sake of simplicity. Detailed explanations and demonstrations can be\nfound in the referred readings. The included exercises complement the\nunderstanding of the theory. The on-line resources are highly recommended for\nextending this brief induction."}, "authors": ["Carlos Gershenson"], "author_detail": {"name": "Carlos Gershenson"}, "author": "Carlos Gershenson", "arxiv_comment": "tutorial, 8 pages", "links": [{"href": "http://arxiv.org/abs/cs/0308031v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0308031v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "C.1.3; I.5.1", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0308031v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0308031v1", "journal_reference": null, "doi": null, "fulltext": "Artificial Neural Networks for Beginners\nCarlos Gershenson\nC.Gershenson@sussex.ac.uk\n\n1. Introduction\nThe scope of this teaching package is to make a brief induction to Artificial Neural\nNetworks (ANNs) for people who have no previous knowledge of them. We first make a brief\nintroduction to models of networks, for then describing in general terms ANNs. As an\napplication, we explain the backpropagation algorithm, since it is widely used and many other\nalgorithms are derived from it.\nThe user should know algebra and the handling of functions and vectors. Differential\ncalculus is recommendable, but not necessary. The contents of this package should be\nunderstood by people with high school education. It would be useful for people who are just\ncurious about what are ANNs, or for people who want to become familiar with them, so when\nthey study them more fully, they will already have clear notions of ANNs. Also, people who\nonly want to apply the backpropagation algorithm without a detailed and formal explanation\nof it will find this material useful. This work should not be seen as \"Nets for dummies\", but of\ncourse it is not a treatise. Much of the formality is skipped for the sake of simplicity. Detailed\nexplanations and demonstrations can be found in the referred readings. The included exercises\ncomplement the understanding of the theory. The on-line resources are highly recommended\nfor extending this brief induction.\n\n2. Networks\nOne efficient way of solving complex problems is following the lemma \"divide and\nconquer\". A complex system may be decomposed into simpler elements, in order to be able\nto understand it. Also simple elements may be gathered to produce a complex system (Bar\nYam, 1997). Networks are one approach for achieving this. There are a large number of\ndifferent types of networks, but they all are characterized by the following components: a set\nof nodes, and connections between nodes.\nThe nodes can be seen as computational units. They receive inputs, and process them\nto obtain an output. This processing might be very simple (such as summing the inputs), or\nquite complex (a node might contain another network...)\nThe connections determine the information flow between nodes. They can be\nunidirectional, when the information flows only in one sense, and bidirectional, when the\ninformation flows in either sense.\nThe interactions of nodes though the connections lead to a global behaviour of the\nnetwork, which cannot be observed in the elements of the network. This global behaviour is\nsaid to be emergent. This means that the abilities of the network supercede the ones of its\nelements, making networks a very powerful tool.\n\n\fNetworks are used to model a wide range of phenomena in physics, computer science,\nbiochemistry, ethology, mathematics, sociology, economics, telecommunications, and many\nother areas. This is because many systems can be seen as a network: proteins, computers,\ncommunities, etc. Which other systems could you see as a network? Why?\n\n3. Artificial neural networks\nOne type of network sees the nodes as 'artificial neurons'. These are called artificial\nneural networks (ANNs). An artificial neuron is a computational model inspired in the\nnatural neurons. Natural neurons receive signals through synapses located on the dendrites\nor membrane of the neuron. When the signals received are strong enough (surpass a certain\nthreshold), the neuron is activated and emits a signal though the axon. This signal might be\nsent to another synapse, and might activate other neurons.\n\nFigure 1. Natural neurons (artist's conception).\nThe complexity of real neurons is highly abstracted when modelling artificial\nneurons. These basically consist of inputs (like synapses), which are multiplied by weights\n(strength of the respective signals), and then computed by a mathematical function which\ndetermines the activation of the neuron. Another function (which may be the identity)\ncomputes the output of the artificial neuron (sometimes in dependance of a certain\nthreshold). ANNs combine artificial neurons in order to process information.\n\nFigure 2. An artificial neuron\n\n\fThe higher a weight of an artificial neuron is, the stronger the input which is\nmultiplied by it will be. Weights can also be negative, so we can say that the signal is\ninhibited by the negative weight. Depending on the weights, the computation of the neuron\nwill be different. By adjusting the weights of an artificial neuron we can obtain the output\nwe want for specific inputs. But when we have an ANN of hundreds or thousands of\nneurons, it would be quite complicated to find by hand all the necessary weights. But we can\nfind algorithms which can adjust the weights of the ANN in order to obtain the desired\noutput from the network. This process of adjusting the weights is called learning or training.\nThe number of types of ANNs and their uses is very high. Since the first neural\nmodel by McCulloch and Pitts (1943) there have been developed hundreds of different\nmodels considered as ANNs. The differences in them might be the functions, the accepted\nvalues, the topology, the learning algorithms, etc. Also there are many hybrid models where\neach neuron has more properties than the ones we are reviewing here. Because of matters\nof space, we will present only an ANN which learns using the backpropagation algorithm\n(Rumelhart and McClelland, 1986) for learning the appropriate weights, since it is one of\nthe most common models used in ANNs, and many others are based on it.\nSince the function of ANNs is to process information, they are used mainly in fields\nrelated with it. There are a wide variety of ANNs that are used to model real neural\nnetworks, and study behaviour and control in animals and machines, but also there are\nANNs which are used for engineering purposes, such as pattern recognition, forecasting,\nand data compression.\n3.1. Exercise\nThis exercise is to become familiar with artificial neural network concepts. Build a\nnetwork consisting of four artificial neurons. Two neurons receive inputs to the network,\nand the other two give outputs from the network.\n\nThere are weights assigned with each arrow, which represent information flow.\nThese weights are multiplied by the values which go through each arrow, to give more or\n\n\fless strength to the signal which they transmit. The neurons of this network just sum their\ninputs. Since the input neurons have only one input, their output will be the input they\nreceived multiplied by a weight. What happens if this weight is negative? What happens if\nthis weight is zero?\nThe neurons on the output layer receive the outputs of both input neurons,\nmultiplied by their respective weights, and sum them. They give an output which is\nmultiplied by another weight.\nNow, set all the weights to be equal to one. This means that the information will flow\nunaffected. Compute the outputs of the network for the following inputs: (1,1), (1,0), (0,1),\n(0,0), (-1,1), (-1,-1).\nGood. Now, choose weights among 0.5, 0, and -0.5, and set them randomly along the\nnetwork. Compute the outputs for the same inputs as above. Change some weights and see\nhow the behaviour of the networks changes. Which weights are more critical (if you change\nthose weights, the outputs will change more dramatically)?\nNow, suppose we want a network like the one we are working with, such that the\noutputs should be the inputs in inverse order (e.g. (0.3,0.7)->(0.7,0.3)).\nThat was an easy one! Another easy network would be one where the outputs should\nbe the double of the inputs.\nNow, let's set thresholds to the neurons. This is, if the previous output of the neuron\n(weighted sum of the inputs) is greater than the threshold of the neuron, the output of the\nneuron will be one, and zero otherwise. Set thresholds to a couple of the already developed\nnetworks, and see how this affects their behaviour.\nNow, suppose we have a network which will receive for inputs only zeroes and/or\nones. Adjust the weights and thresholds of the neurons so that the output of the first output\nneuron will be the conjunction (AND) of the network inputs (one when both inputs are\none, zero otherwise), and the output of the second output neuron will be the disjunction\n(OR) of the network inputs (zero in both inputs are zeroes, one otherwise). You can see\nthat there is more than one network which will give the requested result.\nNow, perhaps it is not so complicated to adjust the weights of such a small network,\nbut also the capabilities of this are quite limited. If we need a network of hundreds of\nneurons, how would you adjust the weights to obtain the desired output? There are\nmethods for finding them, and now we will expose the most common one.\n\n4. The Backpropagation Algorithm\nThe backpropagation algorithm (Rumelhart and McClelland, 1986) is used in\nlayered feed-forward ANNs. This means that the artificial neurons are organized in layers,\nand send their signals \"forward\", and then the errors are propagated backwards. The\nnetwork receives inputs by neurons in the input layer, and the output of the network is given\nby the neurons on an output layer. There may be one or more intermediate hidden layers.\nThe backpropagation algorithm uses supervised learning, which means that we provide the\nalgorithm with examples of the inputs and outputs we want the network to compute, and\nthen the error (difference between actual and expected results) is calculated. The idea of\nthe backpropagation algorithm is to reduce this error, until the ANN learns the training\ndata. The training begins with random weights, and the goal is to adjust them so that the\nerror will be minimal.\n\n\fThe activation function of the artificial neurons in ANNs implementing the\nbackpropagation algorithm is a weighted sum (the sum of the inputs xi multiplied by their\nrespective weights wji):\n(1)\nWe can see that the activation depends only on the inputs and the weights.\nIf the output function would be the identity (output=activation), then the neuron\nwould be called linear. But these have severe limitations. The most common output\nfunction is the sigmoidal function:\n\n(2)\n\nThe sigmoidal function is very close to one for large positive numbers, 0.5 at zero,\nand very close to zero for large negative numbers. This allows a smooth transition between\nthe low and high output of the neuron (close to zero or close to one). We can see that the\noutput depends only in the activation, which in turn depends on the values of the inputs and\ntheir respective weights.\nNow, the goal of the training process is to obtain a desired output when certain\ninputs are given. Since the error is the difference between the actual and the desired\noutput, the error depends on the weights, and we need to adjust the weights in order to\nminimize the error. We can define the error function for the output of each neuron:\n(3)\nWe take the square of the difference between the output and the desired target\nbecause it will be always positive, and because it will be greater if the difference is big, and\nlesser if the difference is small. The error of the network will simply be the sum of the errors\nof all the neurons in the output layer:\n\n(4)\n\nThe backpropagation algorithm now calculates how the error depends on the\noutput, inputs, and weights. After we find this, we can adjust the weights using the method\nof gradient descendent:\n\n(5)\n\n\fThis formula can be interpreted in the following way: the adjustment of each weight\n()wji) will be the negative of a constant eta (0) multiplied by the dependance of the\nprevious weight on the error of the network, which is the derivative of E in respect to wi.\nThe size of the adjustment will depend on 0, and on the contribution of the weight to the\nerror of the function. This is, if the weight contributes a lot to the error, the adjustment will\nbe greater than if it contributes in a smaller amount. (5) is used until we find appropriate\nweights (the error is minimal). If you do not know derivatives, don't worry, you can see\nthem now as functions that we will replace right away with algebraic expressions. If you\nunderstand derivatives, derive the expressions yourself and compare your results with the\nones presented here. If you are searching for a mathematical proof of the backpropagation\nalgorithm, you are advised to check it in the suggested reading, since this is out of the scope\nof this material.\nSo, we \"only\" need to find the derivative of E in respect to wji. This is the goal of the\nbackpropagation algorithm, since we need to achieve this backwards. First, we need to\ncalculate how much the error depends on the output, which is the derivative of E in respect\nto Oj (from (3)).\n\n(6)\n\nAnd then, how much the output depends on the activation, which in turn depends\non the weights (from (1) and (2)):\n\n(7)\n\nAnd we can see that (from (6) and (7)):\n\n(8)\n\nAnd so, the adjustment to each weight will be (from (5) and (8)):\n(9)\nWe can use (9) as it is for training an ANN with two layers. Now, for training the\nnetwork with one more layer we need to make some considerations. If we want to adjust\nthe weights (let's call them vik) of a previous layer, we need first to calculate how the error\ndepends not on the weight, but in the input from the previous layer. This is easy, we would\njust need to change xi with wji in (7), (8), and (9). But we also need to see how the error of\nthe network depends on the adjustment of vik. So:\n\n\f(10)\n\nWhere:\n\n(11)\n\nAnd, assuming that there are inputs uk into the neuron with vik (from (7)):\n(12)\n\nIf we want to add yet another layer, we can do the same, calculating how the error\ndepends on the inputs and weights of the first layer. We should just be careful with the\nindexes, since each layer can have a different number of neurons, and we should not\nconfuse them.\nFor practical reasons, ANNs implementing the backpropagation algorithm do not\nhave too many layers, since the time for training the networks grows exponentially. Also,\nthere are refinements to the backpropagation algorithm which allow a faster learning.\n4.1. Exercise\nIf you know how to program, implement the backpropagation algorithm, that at least\nwill train the following network. If you can do a general implementation of the\nbackpropagation algorithm, go ahead (for any number of neurons per layer, training sets,\nand even layers).\nIf you do not know how to program, but know how to use a mathematical assistant\n(such as Matlab or Mathematica), find weights which will suit the following network after\ndefining functions which will ease your task.\nIf you do not have any computing experience, find the weights by hand.\nThe network for this exercise has three neurons in the input layer, two neurons in\na hidden layer, and three neurons in the output layer. Usually networks are trained with\nlarge training sets, but for this exercise, we will only use one training example. When the\ninputs are (1, 0.25, -0.5), the outputs should be (1,-1,0). Remember you start with random\nweights.\n\n\f5. Further reading\nThe following great books go much deeper into ANNs:\n\u2022\nRojas, R. (1996). Neural Networks: A Systematic Introduction. Springer, Berlin.\n\u2022\nRumelhart, D. and J. McClelland (1986). Parallel Distributed Processing. MIT Press,\nCambridge, Mass.\nFor further information on networks in general, and related themes, these books are\nquite useful and illustrative:\n\u2022\nBar-Yam, Y. (1997). Dynamics of Complex Systems. Addison-Wesley.\n\u2022\nKauffman, S. (1993) Origins of Order, Oxford University Press.\n\n6. Online resources\nThere is a vast amount of resources on the Internet related to Neural Networks. A\ngreat tutorial, with excellent illustrative examples using Java applets (source code\navailable), was developed at the EPFL (http://diwww.epfl.ch/mantra/tutorial/english/).\nOther two good tutorials are at the Universidad Polit\u00e9cnica de Madrid\n(http://www.gc.ssr.upm.es/inves/neural/ann1/anntutorial.html) and at the Imperial College\nof Science, Technology and Medicine University of London\n(http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html). The author also\nhas a small amount of resources related to programming neural networks in Java\n(http://jlagunez.iquimica.unam.mx/~carlos/programacione.html).\n\n7. Bibliography\nBar-Yam, Y. (1997). Dynamics of Complex Systems. Addison-Wesley.\nKauffman, S. (1993) Origins of Order, Oxford University Press.\nMcCulloch, W. and W. Pitts (1943). A Logical Calculus of the Ideas Immanent in Nervous Activity.\nBulletin of Mathematical Biophysics, Vol. 5, pp. 115-133.\nRojas, R. (1996). Neural Networks: A Systematic Introduction. Springer, Berlin.\nRumelhart, D. and J. McClelland (1986). Parallel Distributed Processing. MIT Press, Cambridge,\nMass.\nYoung, D. Formal Computational Skills Course Notes. Http://www.cogs.susx.ac.uk/users/davidy/fcs\n\n\f"}