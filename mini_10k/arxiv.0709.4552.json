{"id": "http://arxiv.org/abs/0709.4552v2", "guidislink": true, "updated": "2007-11-05T09:51:26Z", "updated_parsed": [2007, 11, 5, 9, 51, 26, 0, 309, 0], "published": "2007-09-28T08:48:31Z", "published_parsed": [2007, 9, 28, 8, 48, 31, 4, 271, 0], "title": "Distributed N-body Simulation on the Grid Using Dedicated Hardware", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0709.3138%2C0709.2985%2C0709.0326%2C0709.0766%2C0709.4435%2C0709.1258%2C0709.0297%2C0709.4084%2C0709.2758%2C0709.3723%2C0709.2135%2C0709.1196%2C0709.4668%2C0709.1850%2C0709.0844%2C0709.4119%2C0709.3240%2C0709.1113%2C0709.1630%2C0709.4163%2C0709.0584%2C0709.1923%2C0709.2461%2C0709.3634%2C0709.4503%2C0709.1261%2C0709.2395%2C0709.3369%2C0709.4342%2C0709.2860%2C0709.4263%2C0709.3869%2C0709.1516%2C0709.3053%2C0709.2870%2C0709.1889%2C0709.4602%2C0709.3561%2C0709.3856%2C0709.4238%2C0709.0913%2C0709.1502%2C0709.2188%2C0709.3452%2C0709.1618%2C0709.0396%2C0709.3242%2C0709.3885%2C0709.0800%2C0709.2311%2C0709.2270%2C0709.1593%2C0709.4287%2C0709.0302%2C0709.4614%2C0709.0459%2C0709.4617%2C0709.4336%2C0709.4337%2C0709.3249%2C0709.0398%2C0709.0605%2C0709.4088%2C0709.2392%2C0709.2942%2C0709.4502%2C0709.0036%2C0709.0603%2C0709.0777%2C0709.4150%2C0709.0573%2C0709.2089%2C0709.2228%2C0709.4348%2C0709.1259%2C0709.0714%2C0709.1859%2C0709.1943%2C0709.0756%2C0709.0474%2C0709.0236%2C0709.3563%2C0709.1525%2C0709.0094%2C0709.1793%2C0709.1068%2C0709.2090%2C0709.2101%2C0709.3997%2C0709.0229%2C0709.1592%2C0709.3217%2C0709.3463%2C0709.0539%2C0709.3913%2C0709.1460%2C0709.1758%2C0709.0335%2C0709.0617%2C0709.4237%2C0709.4552&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Distributed N-body Simulation on the Grid Using Dedicated Hardware"}, "summary": "We present performance measurements of direct gravitational N -body\nsimulation on the grid, with and without specialized (GRAPE-6) hardware. Our\ninter-continental virtual organization consists of three sites, one in Tokyo,\none in Philadelphia and one in Amsterdam. We run simulations with up to 196608\nparticles for a variety of topologies. In many cases, high performance\nsimulations over the entire planet are dominated by network bandwidth rather\nthan latency. With this global grid of GRAPEs our calculation time remains\ndominated by communication over the entire range of N, which was limited due to\nthe use of three sites. Increasing the number of particles will result in a\nmore efficient execution. Based on these timings we construct and calibrate a\nmodel to predict the performance of our simulation on any grid infrastructure\nwith or without GRAPE. We apply this model to predict the simulation\nperformance on the Netherlands DAS-3 wide area computer. Equipping the DAS-3\nwith GRAPE-6Af hardware would achieve break-even between calculation and\ncommunication at a few million particles, resulting in a compute time of just\nover ten hours for 1 N -body time unit. Key words: high-performance computing,\ngrid, N-body simulation, performance modelling", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0709.3138%2C0709.2985%2C0709.0326%2C0709.0766%2C0709.4435%2C0709.1258%2C0709.0297%2C0709.4084%2C0709.2758%2C0709.3723%2C0709.2135%2C0709.1196%2C0709.4668%2C0709.1850%2C0709.0844%2C0709.4119%2C0709.3240%2C0709.1113%2C0709.1630%2C0709.4163%2C0709.0584%2C0709.1923%2C0709.2461%2C0709.3634%2C0709.4503%2C0709.1261%2C0709.2395%2C0709.3369%2C0709.4342%2C0709.2860%2C0709.4263%2C0709.3869%2C0709.1516%2C0709.3053%2C0709.2870%2C0709.1889%2C0709.4602%2C0709.3561%2C0709.3856%2C0709.4238%2C0709.0913%2C0709.1502%2C0709.2188%2C0709.3452%2C0709.1618%2C0709.0396%2C0709.3242%2C0709.3885%2C0709.0800%2C0709.2311%2C0709.2270%2C0709.1593%2C0709.4287%2C0709.0302%2C0709.4614%2C0709.0459%2C0709.4617%2C0709.4336%2C0709.4337%2C0709.3249%2C0709.0398%2C0709.0605%2C0709.4088%2C0709.2392%2C0709.2942%2C0709.4502%2C0709.0036%2C0709.0603%2C0709.0777%2C0709.4150%2C0709.0573%2C0709.2089%2C0709.2228%2C0709.4348%2C0709.1259%2C0709.0714%2C0709.1859%2C0709.1943%2C0709.0756%2C0709.0474%2C0709.0236%2C0709.3563%2C0709.1525%2C0709.0094%2C0709.1793%2C0709.1068%2C0709.2090%2C0709.2101%2C0709.3997%2C0709.0229%2C0709.1592%2C0709.3217%2C0709.3463%2C0709.0539%2C0709.3913%2C0709.1460%2C0709.1758%2C0709.0335%2C0709.0617%2C0709.4237%2C0709.4552&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present performance measurements of direct gravitational N -body\nsimulation on the grid, with and without specialized (GRAPE-6) hardware. Our\ninter-continental virtual organization consists of three sites, one in Tokyo,\none in Philadelphia and one in Amsterdam. We run simulations with up to 196608\nparticles for a variety of topologies. In many cases, high performance\nsimulations over the entire planet are dominated by network bandwidth rather\nthan latency. With this global grid of GRAPEs our calculation time remains\ndominated by communication over the entire range of N, which was limited due to\nthe use of three sites. Increasing the number of particles will result in a\nmore efficient execution. Based on these timings we construct and calibrate a\nmodel to predict the performance of our simulation on any grid infrastructure\nwith or without GRAPE. We apply this model to predict the simulation\nperformance on the Netherlands DAS-3 wide area computer. Equipping the DAS-3\nwith GRAPE-6Af hardware would achieve break-even between calculation and\ncommunication at a few million particles, resulting in a compute time of just\nover ten hours for 1 N -body time unit. Key words: high-performance computing,\ngrid, N-body simulation, performance modelling"}, "authors": ["Derek Groen", "Simon Portegies Zwart", "Steve McMillan", "Jun Makino"], "author_detail": {"name": "Jun Makino"}, "author": "Jun Makino", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.newast.2007.11.004", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0709.4552v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0709.4552v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "(in press) New Astronomy, 24 pages, 5 figures", "arxiv_primary_category": {"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0709.4552v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0709.4552v2", "journal_reference": "NewAstron.13:348-358,2008", "doi": "10.1016/j.newast.2007.11.004", "fulltext": "arXiv:0709.4552v2 [astro-ph] 5 Nov 2007\n\nDistributed N-body Simulation on the Grid\nUsing Dedicated Hardware\nDerek Groen a,b, Simon Portegies Zwart a,b, Steve McMillan c,\nJun Makino d\na\n\nSection Computational Science, University of Amsterdam, Amsterdam, the\nNetherlands\nb\n\nAstronomical Institute \"Anton Pannekoek\", University of Amsterdam,\nAmsterdam, the Netherlands\nc\n\nDrexel University, Philadelphia, United States\nd\n\nUniversity of Tokyo, Tokyo, Japan\n\nAbstract\nWe present performance measurements of direct gravitational N -body simulation on\nthe grid, with and without specialized (GRAPE-6) hardware. Our inter-continental\nvirtual organization consists of three sites, one in Tokyo, one in Philadelphia and\none in Amsterdam. We run simulations with up to 196608 particles for a variety\nof topologies. In many cases, high performance simulations over the entire planet\nare dominated by network bandwidth rather than latency. With this global grid\nof GRAPEs our calculation time remains dominated by communication over the\nentire range of N , which was limited due to the use of three sites. Increasing the\nnumber of particles will result in a more efficient execution. Based on these timings\nwe construct and calibrate a model to predict the performance of our simulation on\nany grid infrastructure with or without GRAPE. We apply this model to predict the\nsimulation performance on the Netherlands DAS-3 wide area computer. Equipping\nthe DAS-3 with GRAPE-6Af hardware would achieve break-even between calculation and communication at a few million particles, resulting in a compute time of\njust over ten hours for 1 N -body time unit.\nKey words: high-performance computing, grid, N-body simulation, performance\nmodelling\n\n1\n\nIntroduction\n\nStar clusters are often simulated by means of direct-method N-body simulations (Aarseth, 1985). The Newtonian gravitational force on individual stars\nPreprint submitted to Elsevier\n\n28 October 2018\n\n\fin such simulations is calculated by aggregating the force contributions from\nall other particles in the system.\nTo enable faster execution of these simulations, specialized solutions such\nas GRAvity PipEs (GRAPEs) (Fukushige et al., 2005), Graphics Processing Units (GPUs) (Portegies Zwart et al., 2007; Hamada and Iitaka, 2007;\nBelleman et al., 2007) and Field-Programmable Gate Arrays (FPGAs) (Lienhart et al.,\n2002) have been successfully developed and applied. These solutions are designed or tuned specifically for optimizing force calculations, and provide dramatic speedup. For example, the GRAPE-6Af features a dedicated hardware\nimplementation that can calculate 42 force interactions simultaneously and\nwith increased efficiency. As a result, the GRAPE is able to perform force\ncalculations \u223c 130 times faster than a single PC (Makino et al., 2003a). Recently, GPUs have shown gains in speed and flexibility, and they are now\nused for simulating self gravitating systems at speeds comparable to GRAPE\n(Portegies Zwart et al., 2007; Hamada and Iitaka, 2007; Belleman et al., 2007).\nParallelization of GRAPEs appears to be an efficient way to reduce the wallclock time for individual simulations (Makino et al., 2003b; Gualandris et al.,\n2007; Harfst et al., 2007). The gravitational N-body problem has calculation\ntime complexity O(N 2 ), whereas the communication scales only with O(N).\nFor sufficiently large N, the force calculation time will therefore overtake the\ncommunication time. For a local cluster of GRAPEs with low-latency and\nhigh bandwidth network, break-even between calculation and communication\nis reached at N \u223c 104 (Harfst et al., 2007).\nGenerally, GRAPE clusters are not cheap and few institutions can afford such\ndedicated hardware solutions. Still, more than 500 GRAPE modules, where\none module is equivalent to one GRAPE-6Af, or 4 GRAPE-6 chips, are currently in use across 37 institutions in 12 countries world-wide. An alternative\nto purchasing a large GRAPE-6 or GPU cluster is provided by a computational grid. In a grid, several institutions assemble in a virtual organization,\nwithin which they share resources, and the costs for purchasing and maintaining these resources (Foster et al., 2001). Grid middleware provides a secure wide area computing environment without requiring users to register for\nindividual clusters. In addition, grid-enabled MPI implementations, such as\nMPICH-G2 (Karonis et al., 2002) or OpenMPI (Graham et al., 2006), provide\nthe ability to run MPI jobs across sites in the grid, using the existing MPI\nstandards. Applying such grid technology to clusters of GPUs is an attractive option, because there are a large number of (frequently idle) GPUs in\nconsumer machines. By connecting these consumer machines to the grid (as\nwas done in a similar fashion with regular CPUs for the SETI@home project\n(Anderson et al., 2002)) and using them for parallel N-body simulations, we\ncan increase the computational power of the grid in a cheap and convenient\nmanner.\n2\n\n\fAlthough there is a clear benefit of using grid technology in sharing financial\nburden, the real challenge is to develop new applications for astronomical\nproblems that have yet to be solved. For example, the simulation of an entire\ngalaxy, requires at least a few PFLOP/s of computational power and the\ndevelopment of a hybrid simulation environment (Hoekstra et al., 2007). Such\nan environment performs several astrophysical simulations on vastly different\ntemporal and spatial scales. For example, a hybrid simulation environment\ncould consist of a stellar evolution simulation to track how individual stars\nevolve over time, a smoothed particle hydrodynamics simulation (Monaghan,\n1992) to simulate stellar collisions or close encounters, and a direct-method\nN-body calculation to simulate the remaining dynamics between stars.\nTo facilitate these tightly-coupled multi-physics simulations on the PFLOP/s\nscale, it will no longer be sufficient to do high-performance computing (HPC)\non a local cluster, as we require an extensive grid infrastructure consisting of\nseveral of such clusters. Although grid technology has been largely applied to\nfacilitate high-throughput computing (Abramson et al., 2000), little research\nhas been done on investigating how the grid can be efficiently applied to solve\ntightly-coupled HPC problems. By using grid technology for this specific set\nof problems, we can potentially fullfill the computational requirements for\nperforming petascale multi-physics simulations.\nUsing a grid infrastructure for HPC has drawback, as the communication\nbetween grid sites dramatically increases network overhead compared to a\nlocal cluster. For intercontinental communication, the network latency can\nbecome as large as 0.3s, which is especially impractical for applications, such\nas direct-method N-body codes, that require communication over all processes\nduring every iteration. Still, even for such long communication paths there\nwill be a problem size (N) for which wall-clock time is dominated by the force\ncalculation rather than by communication. Earlier experiments indicate that a\ngrid of regular PCs across Europe improves overall performance for relatively\nsmall N (Gualandris et al., 2007). We address the question for which problem\nsize a world-wide grid has practical usage, in particular if such a cluster is\nequipped with GPUs or GRAPEs.\n\n2\n\nExperimental setup\n\nWe have constructed a heterogeneous grid of GRAPEs, which we call the\nGlobal GRAPE Grid (or G3). The G3 consists of five nodes across three sites.\nTwo nodes are located at Tokyo University (Tokyo, Japan), two are located\nat the University of Amsterdam (Amsterdam, the Netherlands) and one is at\nDrexel University (Philadelphia, United States). Each of the nodes is equipped\nwith a GRAPE-6Af special purpose computer, which allows us to test several\n3\n\n\fTable 1\nSpecifications for the nodes in G3. The first column gives the name of the computer\nfollowed by its country of residence (NL for the Netherlands, JP for Japan and US\nfor the United States). The subsequent columns give the type of processor in the\nnode, the amount of RAM, followed by the operating system, the kernel version and\nthe version of Globus installed on the PC. Each of the nodes is equipped with a\n1 Gbit/s Ethernet card and GRAPE-6Af hardware. Local nodes are interconnected\nwith Gigabit Ethernet.\nname\nlocation CPU type\nRAM\nOS kernel Globus\n[MB]\nversion\nvader\nNL\nIntel P4 2.4GHz\n1280 Ubuntu 5.10\n2.6.5\n4.0.3\npalpatine\nNL\nIntel P4 2.67GHz\n256\nRHEL 3 2.4.21\n4.0.3\nyoda\nJP\nAthlon 64 3500+\n1024\nFC 2 2.6.10\n3.2.1\nskywalker\nJP\nAthlon 64 3500+\n1024\nFC 2 2.6.10\n3.2.1\nobi-wan\nUS\n2x Xeon 3.6GHz\n2048 Gentoo 06.1 2.6.13\n4.0.4\n\ndifferent resource topologies. Local nodes are connected by Gigabit Ethernet,\nwhereas the different sites are connected with regular internet. In Table 1 we\npresent the specifications of the G3. Each of the computers in the G3 is set\nup with Globus Toolkit middleware 1 and MPICH-G2 2 .\nIn Table 2 we present the network characteristics, latency and bandwidth,\nof the connections within G3. We tested local area network (LAN) and wide\narea network (WAN) connections using the UNIX ping command to measure\nlatency. We use scp for measuring the network bandwidth, transferring a 75\nMB file, rather than referring to theoretical limits because the majority of\nbandwidth on non-dedicated WANs is used by external users. For our performance measurements, we used a standard implementation of MPICH-G2\nwithout specific optimizations for long-distance networking. As a result, the\nMPI communication makes use of only 40%-50% of the available bandwidth\n3\n. If we were to enhance MPICH-G2 with additional optimizations, or add\nsupport for grid security to already optimized MPI libraries, such as Makino's\ntcplib 4 or OpenMPI, our bandwidth use would be close to the bandwidth use\nof a regular file transfer.\nThe N-body integrator we have chosen for our experiments uses block timesteps (McMillan, 1986) with a 4th order Hermite integration scheme (Makino and Aarseth,\n1992). The time steps with which the particles are integrated are blocked in\npowers of two between a minimum of 2\u221222 and a maximum of 2\u22123 . During each\ntime step, the codes perform particle predictions, calculate forces between par1\n\nhttp://www.globus.org\nhttp://www3.niu.edu/mpi/, in the future: http://dev.globus.org/wiki/MPICH-G2\n3 for more information we refer to a research report from INRIA:\nhttp://hal.inria.fr/inria-00149411/en/\n4 see:http://grape.mtk.nao.ac.jp/\u223cmakino/softwares\n2\n\n4\n\n\fTable 2\nCharacteristics of local and wide network connections. Latency indicates the required time for sending 1 byte through the network connection. The bandwidth\nindicates the transfer capacity of the network connection. The bandwidth was measured with a 75MB scp file transfer.\nconnection\nlatency bandwidth (theory) bandwidth (real)\n[ms]\n[MB/s]\n[MB/s]\nAmsterdam LAN\n0.17 125.0\n11.0\nTokyo LAN\n0.04 125.0\n33.0\nAmsterdam - Tokyo WAN 266.0\n57.0\n0.22\nAmsterdam - Phil. WAN\n104.0\n312.5\n0.56\nPhiladelphia - Tokyo WAN 188.0\n57.0\n0.32\n\nticles and correct particles on a block of active particles. Particle corrections\ninclude updates of positions and velocities, and computation of new block time\nsteps of particles. For our experiments we use three implementations of a parallel N-body integrator. One of these codes runs on a single PC with and without GRAPE. The two others are parallelized using MPI: one of these uses the\ncopy algorithm (Makino, 2002; Dorband et al., 2003) and the other uses the\nring algorithm (Fox et al., 1988; Angus et al., 1990; Gualandris et al., 2007).\nThe copy algorithm has smaller number of communication steps whereas the\nring algorithm has lower memory usage on the nodes.\nWe initialize the simulations using Plummer (Plummer, 1911) spheres that\nwere in virial equilibrium and performed our simulations using a softening\nparameter of 2\u22128 . Since our simulations are performed over one dynamical\n(N-body) time unit (Heggie and Mathieu, 1986), the realization of the N-body\nsystem is not critical to the timing results.\n\n3\n\nResults of grid calculations\n\nWe have performed a number of simulations on local machines and on the\nG3, which consists of simulations lasting one N-body time unit and shorter\nsimulations lasting one integration time step. We measured the full wall-clock\nexecution time for the longer simulations and we profiled the shorter simulations.\n\n3.1 Timing results of N-body calculations\nWe run the N-body codes, discussed in \u00a7 2, on a single PC and across the network in parallel using N = 1024 to N = 65536 (a few additional calculations\n5\n\n\fwere performed with N > 65536). The runs were performed with and without\nGRAPE. In Figs. 1 and 2 we present the results of the copy and ring algorithms. If a simulation is run multiple times with the same problem set, the\nexecution time may be slightly different per run. This variation is relatively\nsmall, as the slowest of 4 repeated runs (using 32768 particles over two sites)\nwas a factor 1.07 slower than the fastest run. The variation can be primarily\nattributed to fluctuations in the network bandwidth.\nSingle PC\nThe performance on a single PC (represented by the thick solid line with\nbullets in Fig.1) is entirely dominated by force calculations, which scales as\nO(N 2 ). As the number of steps per N-body time unit increases with N, the\nexecution time scales slightly worse than N 2 .\nGrid of PCs\nThe performance on the G3, without using GRAPE, is given by the thin\ndashed line with triangles. For N < 24576, the performance is dominated by\nnetwork communication. Given that p indicates the number of processes, the\nnetwork communication scales as O (N log p)(see (Harfst et al., 2007)). For\nour grid-based simulation experiments without GRAPE, break-even between\ncommunications and force calculations is achieved around N \u223c 3 * 104 for the\ncopy algorithm (Fig.1), and at a somewhat higher value for the ring algorithm\n(Fig.2). For larger N, the execution time is dominated by force calculations,\nrather than network communication. For these high N, the grid speedup \u0393\n(Hoekstra and Sloot, 2005), which is the single-site execution time divided by\nthe execution time over three grid sites, increases to 1.37 for the copy and\n1.24 for the ring algorithm. As can be seen by comparing Figs. 1 and 2, the\ncopy algorithm gives overall better performance than the ring algorithm. This\ncan be explained by the smaller number of communication steps in the copy\nalgorithm.\nSingle PC with GRAPE\nThe performance on a single PC with GRAPE is dominated by force calculations, although communication between host and GRAPE, and operations on\nthe host machine have an impact on performance for N < 16384. In addition,\nthe GRAPE performs less efficiently for low N, because many blocks are too\nsmall to fill the GRAPE pipelines. For larger N, force calculations become the\nperformance bottleneck, and the scaling of the execution time becomes that\nof a single PC without GRAPE.\nGrid of PCs with GRAPE\nThe performance on the G3 (with GRAPEs) using all three sites is given by the\nthin solid line with triangles. For all problem sizes N we have measured, the\ngrid speedup \u0393 is less than 0.15, indicating that the performance is dominated\nby network communication. The network communication time scales better\n6\n\n\fthan the force calculation time, therefore, force calculation time will overtake\nthe network communication time if N is sufficiently large. However, this breakeven point lies at much higher N than for a Grid of PCs, because the use of\nGRAPE greatly decreases the time spent on force calculations.\nFor the copy algorithm (see Fig. 1), calculations between Tokyo and Philadelphia take less time than calculations between Amsterdam and Tokyo, due to\na lower network latency (see Table 2). The calculations across three sites take\nmore time than calculations across two sites. This is caused by the latency of\nall-to-all MPI communications in the copy algorithm, which scales with the\nnumber of processors.\nAccording to our profiling measurements in Fig. 3, for N < 12288, a simulation\non the G3 with GRAPEs using ring algorithm spends most of its time in network latency. For larger N more time is spent on using the network bandwidth.\nThese results indicate that network bandwidth is the primary bottleneck for\nour simulations on the G3 using ring algorithm. When we compare the results\nof the runs on the grid with GRAPEs with each other, we do not notice any\nsystematic trend. The results confirm that the wall-clock time is dominated\nby using the network bandwidth, which is bottlenecked by the transpacific\nnetwork line for all grid setups.\n\n3.2 Profiling of the N-body simulations\n\nWe have chosen one parallel algorithm (ring) and one resource topology (3\nnodes on 3 sites) to profile the simulation during one integration time step. The\nblock size n for every measurement was fixed using a formula for calculating\naverage block size (n = 0.20N 0.81 ), which has been used for the same initial\nconditions in Portegies Zwart et al. (2007). During execution, we measured the\ntime spent on individual tasks, such as force calculations or communication\nlatency between processes. We have profiled our simulations for N = 1024 up\nto N = 196608, using the timings measured on the process running in Tokyo.\nThe results of these measurements are given in Fig. 3.\nWe find that for larger N, low bandwidth of our wide area network affects\nthe outcome of the performance measurements, and that MPI calls are only\nable to use about a quarter of the available bandwidth for passing message\n> 5*105 we expect the force calculation to take more time than\ncontent. For N \u223c\nnetwork latency. If we were to use the network bandwidth more efficiently for\nsuch a large number of particles, the execution time would be dominated by\nforce calculations. The network bandwidth can be used much more efficiently,\neither by using a more efficient MPI implementation (e.g. one that supports\ncommunication over multiple tcp connections) or by using a dedicated net7\n\n\f6\n\n10\n\nCopy\n5\n\n10\n\n4\n\nTapp [s]\n\n10\n\n3\n\n10\n\n2\n\n10\n\n1 GRAPE\n2 GRAPEs (PT)\n2 GRAPEs (AT)\n3 GRAPEs (APT)\n1 PC\n3 PCs (APT)\nperf. model\n\n1\n\n10\n\n0\n\n10\n\n3\n\n10\n\n4\n\n5\n\n10\n\n10\n\n6\n\n10\n\nN\n\nFig. 1. The time for running the application for 1 N -body time unit (Tapp ) as a\nfunction of the number of stars (N ) using the copy algorithm. The two thick lines\ngive the results for a single CPU with GRAPE (lower solid curve) and without (top\ndashed curve). We make the general distinction between solid curves to present the\nresults for simulations run with GRAPE, and dashed curves to give the results without GRAPE. The results on the grid are presented with four different lines, based\non the three included locations. Each of these runs is performed with one node per\nsite. The results for the WAN connection Philadelphia\u2013Tokyo, Amsterdam\u2013Tokyo\nand Amsterdam\u2013Philadelphia\u2013Tokyo are indicated with the solid curves with filled\nsquares, open squares and filled triangles, respectively. The dashed curve with filled\ntriangles give the results for the Amsterdam\u2013Philadelphia\u2013Tokyo connection but\nwithout using GRAPE. Dotted lines indicate the performance of runs with GRAPE\naccording to the performance model.\n106\n\nRing\n\n105\n\nTapp [s]\n\n104\n\n103\n\n102\n1 GRAPE\n2 GRAPEs (PT)\n2 GRAPEs (AT)\n3 GRAPEs (APT)\n1 PC\n3 PCs (APT)\nperf. model\n\n1\n\n10\n\n100 3\n10\n\n104\n\n105\n\n106\n\nN\n\nFig. 2. The time for running the application for 1 N -body time unit (Tapp ) as a\nfunction of the number of stars (N ) for runs using the ring algorithm. See Fig.1 for\nan explanation of the lines and symbols.\n\n8\n\n\f100\n\n76\n\n133\n\n234\n\n410\n\n720\n\nblock size [n]\n1262\n2213\n\n3880\n\n6803\n\n11926\n\nnetwork bandwidth\n\nnetwork latency\n\n10-1\n\nshare of time spent\n\nforce calculations\n10-2\n\n10-3\nPC-GRAPE comm. time\ncorrector\n10\n\n-4\n\npredictor\n10-5\n103\n\n104\n\n105\n\n106\n\nN\n\nFig. 3. Share of wall-clock time spent on individual tasks during a single time-step.\nSolid lines indicate tasks performed on the local machine. The thick solid line with\nfilled circles represents time spent on force calculations, and the thin solid lines\ngive the result for time spent on communication between PC and GRAPE (open\ntriangles), particle corrections (open circles) and particle predictions (open squares)\nrespectively. Dotted lines indicate time spent on communication between nodes.\nThe thin dotted line with asterisks indicates time spent on communication latency\nbetween nodes and the thick dotted line with solid squares indicates time spent on\nusing the network bandwidth.\n\nwork. Using our current networking and MPI implementation, we expect that\n> 2 * 106 particles the force calculation time overtakes the bandwidth\nfor N \u223c\ntime.\n\n4\n\nModelling the performance of the grid\n\nIn order to further understand the results and to enable performance predictions for larger network setups, we decided to model the performance\nof the grid calculations. We model the performance of the simulation by\nadopting the parallel performance models described by Makino (2002) and\nHarfst et al. (2007) and combining it with the grid performance model described in Gualandris et al. (2007). Further extension and calibration of the\nmodel allows us to simulate the performance of our N-body simulations on a\nG3 or any other topology.\n9\n\n\f4.1 Single PC\nAn N-body simulation over one N-body time unit (Heggie and Mathieu, 1986)\nconsists of the following steps:\n(1) Read the input snapshot and initialize the N-body system.\n(2) Compute the next system time t and select the block of n active particles\nin the system.\n(3) Predict the positions and velocities of all N particles to time t.\n(4) Calculate the forces and their time derivatives between the n active particles and all N particles in the system.\n(5) Correct the positions, velocities and velocity derivatives of the n active\nparticles, and update their time steps.\n(6) Repeat from step 2 until t has exceeded one N-body time unit.\n(7) Write the output of the simulation and terminate it.\nAs relatively little time is spent on program initialization and finalization, we\nfocus on the time to integrate the system (Tintegrate ), which consists of the\ntasks performed in steps 2 to 5. Throughout this paper we use uppercase T\nto refer to the time spent in nsteps integration steps, and the lowercase (t) for\nthe time spent in a single step. Within a single step, the total execution time\nTintegrate is\n\nTintegrate =\n\nnX\nsteps\n\n(tpred + tforce + tcorr ) ,\n\n(1)\n\ni=1\n\nwith the time spent on predicting particles\ntpred = \u03c4pred N,\n\n(2)\n\nthe time spent on calculating forces\ntforce = \u03c4force nN,\n\n(3)\n\nand the time spent on correcting the active particles\ntcorr = \u03c4corr n.\n\n(4)\n\nHere \u03c4pred is the time to predict a single particle, \u03c4force is the time to calculate\nthe forces between two particles, and \u03c4corr is the time spent to correct a single\nparticle. The values for \u03c4pred , \u03c4force and \u03c4corr have been measured by using a\nsample N-body simulation with 32768 particles, and are given in table 3 for the\n10\n\n\fTable 3\nMachine performance specification and machine-specific constants. The first two\ncolumns show the name of the machine, followed by the country of residence. The\nthird column indicates machine speed in Mflops, using the Whetstone benchmark.\nThe last three columns give the time required for the CPU to perform one particle\nprediction (\u03c4pred ), the time required for one force calculation between two particles\n(\u03c4force ) and the time required for correcting one particle (\u03c4corr ) respectively, all in\nmicroseconds.\nname\nlocation speed \u03c4pred \u03c4force \u03c4corr\nMflops [\u03bcs]\n[\u03bcs]\n[\u03bcs]\nvader\nNL\n377\n0.247 0.216 4.81\npalpatine NL\n422\n0.273 0.193 2.39\nyoda\nJP\n436\n0.131 0.110 1.29\nskywalker JP\n436\n0.131 0.110 1.29\nobi-wan\nUS\n1191\n0.098 0.148 1.14\n\nvarious nodes in the G3. For a more practical comparison, we also measured\nthe compute speed (in floating point operations per second) for each of the\nnodes. These measurements were carried out using the Whetstone benchmark\n(Curnow and Wichmann, 1976).\n\n4.2 Grid of PCs with copy algorithm\n\nThe performance model for a single PC can be extended to include the parallel\noperation in the copy algorithm. In the copy algorithm, each process has a full\ncopy of the system of N particles, but only computes the active particles in\na specific subset of N/p particles. The result of this computation is sent to\nall other processes. We assume that all p processes have comparable speed,\nand every process has an equally sized subset of n/p active particles. For the\ncopy algorithm, the host computation time (Tintegrate ) also consists of the time\nspent to communicate between processes (TMPI ). Therefore,\n\nTintegrate =\n\nnX\nsteps\n\n(tpred + tforce + tcorr ) + TMPI ,\n\n(5)\n\ni=1\n\nA process computes forces for its subset of n/p active particles, and corrects\nonly these particles. Therefore, a process requires at most Nn/p force calculations per time step\nn\ntforce = \u03c4force N ,\np\n\n(6)\n\n11\n\n\fand a process corrects at most n/p particles, of which the time spent is given\nby\nn\ntcorr = \u03c4corr .\np\n\n(7)\n\nIn a parallel system, time is spent not only on integrating the system (Tintegrate ),\nbut also on exchanging messages between processes (TMPI ). This time is obtained by adding the time spent on overcoming network latency (tlatency ) and\nthe time spent transferring particles (tbandwidth )\n\nTMPI =\n\nnX\nsteps\n\n(tlatency + tbandwidth ) .\n\n(8)\n\ni=1\n\nIn our implementation tlatency is given by the sum of the latencies of each MPI\ncall in the code. The copy algorithm uses 1 MPI Allgather and 1 MPI Allgatherv\ncommand (of which the latencies both scale with log2 p (Gualandris et al.,\n2007)) per block time-step, resulting in a total time spent on latency of\ntlatency = (lMPI\n\nAllgather\n\n+ lMPI\n\nAllgatherv ) log2\n\np.\n\n(9)\n\nThe time used for transferring particle data is given by tbandwidth , which is\nobtained by taking the total size of the data that has to be communicated\n(which is assumed to scale with 2(p \u2212 1) for all-to-all communications), and\ndividing it by the network bandwidth (\u03c4bw ). Particles are stored in 58 byte\ndata structures, resulting in\n\ntbandwidth =\n\nn116(p \u2212 1)\n.\np\u03c4bw\n\n(10)\n\nFor a wide area computer tlatency and tbandwidth may be quite substantial, but\nthe separation in parts, as given here, enables us to optimize our network\ncomputer with respect to the communication characteristics.\n\n4.3 Grid of PCs with ring algorithm\nUnlike the copy algorithm, the ring algorithm (discussed in detail in Fox et al.\n(1988); Angus et al. (1990)) does not use a single all-to-all communication\noperation, and only requires the processes to have a partial copy of size N/p\nof the system. Communication occurs in a a total of p steps (or shifts). During\nevery shift, each process performs a partial force integration by calculating the\n12\n\n\fforces between their local subset of n/p active particles and the N/p particles\nstored in local memory. Then, each process sends its updated particles to their\nneighbor.\nTo model the performance for the ring algorithm, we use the model for the\ncopy algorithm and redefine the time spent on force calculations (Tforce ) and\nthe time spent to communicate between processes (tlatency and tbandwidth ), as\nthe force calculation and the MPI communications occur in multiple shifts.\nThe time spent on a partial force calculation is given by\n\ntforce,1shift = \u03c4force\n\nnN\n.\np2\n\n(11)\n\nThe total time spent on the force calculation is given by the time for a partial\nforce calculation (tforce,1shift ) multiplied by the number of shifts (p),\n\ntforce = \u03c4force n\n\nN\n.\np\n\n(12)\n\nFor every time step, our implementation of the ring algorithm uses 2 MPI Allreduce\ncommunication commands for initialization, and 1 MPI Sendrecv operation for\neach shift. The time spent overcoming network latency is then\ntlatency = 2 log2 plMPI\n\nAllreduce\n\n+ plMPI\n\nSendRecv .\n\n(13)\n\nThe ring algorithm is more bandwidth intensive than the copy algorithm, as\nall block subsets are sent and received during a ring shift, multiplying the\ntime spent on transferring particles by 2p. Per particle, 58 bytes have to be\ntransferred, therefore the time spent on transferring particles is given by\ntbandwidth = 116n/\u03c4bw .\n\n(14)\n\n4.4 Single PC with GRAPE\n\nThe GRAPE-6Af is a dedicated hardware component developed by a group of\nresearchers led by Junichiro Makino at the University of Tokyo (Fukushige et al.,\n2005). The GRAPE-6Af is the smallest commercially available GRAPE configuration, consisting of a single GRAPE module with a peak speed of about\n123 Gflops. It calculates the forces between particles, which is the bottleneck\nin the calculation, whereas the particle predictions and corrections are still\nmostly done on the host PC.\n13\n\n\fWhen a GRAPE is used, an N-body simulation over one N-body time unit\nconsists of the following steps:\n(1) Read the input snapshot and initialize the N-body system.\n(2) Compute the next system time t and select the block of active particles\nn in the system.\n(3) Predict the positions and velocities of the n active particles on the pc,\nand send the predicted values and the next system time to the GRAPE.\n(4) Predict the other particles in the system on the GRAPE.\n(5) Calculate the forces and their time derivatives, using the GRAPE, between the n active particles and all N particles in the system.\n(6) Retrieve the forces and their time derivatives from the GRAPE.\n(7) Correct the positions, velocities and velocity derivatives of the n active\nparticles, and update their time steps.\n(8) Repeat from step 2 until t has exceeded one N-body time unit.\n(9) Write the output of the simulation and terminate it.\nWhen using GRAPE\ntpc = tpred + tcorr ,\n\n(15)\n\nwhere tpred = n\u03c4pred . The time spent to integrate particles for one N-body\ntime unit is given by\n\nTintegrate =\n\nnX\nsteps\n\n(tpc + tgrape + tcomm ) .\n\n(16)\n\ni=1\n\nHere\ntgrape = \u03c4pipe nN,\n\n(17)\n\nis the time for calculating the forces on the GRAPE. The time needed by\nthe GRAPE to calculate the force between two particles is given by \u03c4pipe . The\ncommunication between host and GRAPE is given by (Fukushige et al., 2005)\ntcomm = 60ti n + 56tf n + 72tj n.\n\n(18)\n\nHere the time to respectively send or receive 1 byte of data to the GRAPE\nduring different steps is given by ti , tn and tj , respectively. During these steps\nrespectively 60, 56 and 72 bytes per particle in the block are transferred. We\nassume that ti = tf = tj . We derive tj by measuring \u03c4Gsend , which is the time\nto send one 72-byte particle to the GRAPE. Therefore, tj = \u03c4Gsend /72. By\nrewriting ti , tn and tj as factors of \u03c4Gsend , we can simplify the equation for\ntcomm to\n14\n\n\ftcomm = (60 + 56 + 72) (\u03c4Gsend /72) n.\n\n(19)\n\nTime spent on calculating forces on the GRAPE (tgrape ) cannot be directly\nmeasured by timing parts of the code, because the GRAPE force calculation\nincludes some communication between host and GRAPE as well. However, we\ncan derive \u03c4pipe from the total time of the force calculation as was done in\nHarfst et al. (2007). Therefore, we can rewrite \u03c4pipe as\n\n\u03c4pipe =\n\n1\n[tforce \u2212 (116\u03c4Gsend /72)].\nN\n\n(20)\n\nThe time spent on performing N force calculations tforce is given by,\ntforce = \u03c4Gforce N,\n\n(21)\n\nwhere \u03c4Gforce is the time spent to calculate forces between two particles. We\nthen introduce the time constant (\u03c4Gforce ) in the function for \u03c4pipe ,\n1\n.\n\u03c4pipe = \u03c4Gforce \u2212 (116\u03c4Gsend /72)\nN\n\u0012\n\n\u0013\n\n(22)\n\nUsing our derived functions for tgrape and tcomm , we are now able to model\nthe performance of the GRAPE. As mentioned in (Fukushige et al., 2005),\n\u03c4Gforce \u2248 4.3 * 10\u221210 s.\n4.5 Grid of PCs with GRAPE and copy algorithm\nWhen using GRAPE and a parallel algorithm, the time spent to integrate\nparticles for one N-body time unit is given by\n\nTintegrate =\n\nnX\nsteps\n\n(tpc + tgrape + tcomm ) + TMPI .\n\n(23)\n\ni=1\n\nWe determine time spent communicating between hosts (TMPI ) using Eq. 8. To\ndetermine the time spent on the host (tpc ) we use the equation for the single\nprocess with GRAPE (see Eq. 15). However, as we correct only n/p particles in\nparallel algorithms, we apply Eq. 7 to determine the time spent by the process\non correcting particles.\nIn a parallel setup of GRAPEs, each process needs to communicate and calculate forces for a subset of n/p particle in every block. We replace n by\n15\n\n\fn/p in our equations for tgrape as well as tcomm . Therefore, the time spent on\ncalculating forces is given by\nn\ntgrape = [N\u03c4Gforce \u2212 (116\u03c4Gsend /72)] ,\np\n\n(24)\n\nand communication between between the hosts and the GRAPEs becomes,\nn\ntcomm = (60 + 56 + 72) (\u03c4Gsend /72) .\np\n\n(25)\n\n4.6 Grid of PCs with GRAPE and ring algorithm\nIn a ring algorithm, each process computes the forces between a local set of\nn/p active particles and the local system of N/p particles during a shift. Then,\nit sends the results to its next neighbor and receives another n/p particles from\nits other neighbor. Each of the nodes spends tgrape,1shift calculating the forces\nfor n/p particles during one shift. Before the node has integrated the force on\nall n particles, a total of p shifts have passed, resulting in a total compute\ntime for this node of\ntgrape = ptgrape,1shift ,\n\n(26)\n\nwhere the time to calculate forces for a single shift (tgrape,1shift ) is\nn\ntgrape,1shift = [(N\u03c4Gforce /p) \u2212 (116\u03c4Gsend /72)] .\np\n\n(27)\n\nWhen GRAPE is used, particles are 74 bytes each because they contain two\nadditional arrays for storing the old acceleration and old jerk. Due to this\nincreased particle size,\ntbandwidth = 148n/\u03c4bw ,\n\n(28)\n\nwhereas tlatency remains unchanged. The time spent communicating between\nhosts (TMPI ) is calculated as for the ring algorithm without GRAPE.\n\n5\n\nResults of the performance model\n\nWe have applied the performance model from the previous section to the results presented in \u00a7 3. In Fig. 1 we compare the measured wall-clock time (Tapp )\n16\n\n\ffor the copy algorithm on the grid with the performance model, Fig. 2 shows\na similar comparison for the ring algorithm. To guide the eye, the results for\na single GRAPE are also presented in both figures. The performance model\ntracks the real measurements quite satisfactorily, giving a slightly lower computation time for a single GRAPE while giving a slightly higher computation\ntime for a simulation across grid sites.\nThe communication overhead of a distributed computer often renders high performance computing on a grid inefficient. However, in the N-body problem the\ncompute time scales with N 2 whereas the communication scales linearly with\nN. For sufficiently large N, there will eventually be a point where relatively\nlittle time is lost communicating, and the compute resources are efficiently\nused.\nIn figures 1 and 2 we can see that, for GRAPE-enabled simulations, breakeven between calculation and communication is reached around N \u2243 106 . For\nlarge N, a grid of two GRAPEs will outperform a single GRAPE. Our grid\nsetup included three GRAPE-enabled sites. The location of these sites (Asia,\nEurope and America) were as widely distributed as physically possible. A more\nmodest grid across a single continent, will perform considerably better than a\nglobal grid. With the performance model that we constructed in \u00a7 4, we can\nnow study various grid topologies without the need to physically build the\nenvironment and create a virtual organisation.\n\n5.1 Future Prospects\n\nWe applied the performance model to three hypothetical grids of GRAPE\nnodes. These three grids are: 1) a grid of all the available GRAPEs on the\nplanet, 2) a grid of sites with more than 1 Tflops in GRAPE speed, and 3) a\nrecently established Dutch grid (Dutch ASCII Computer, DAS-3 5 ) equipped\nwith GRAPEs.\nSince the launch of GRAPE-6, a total of 1115 GRAPE-6 modules have been\ndeployed worldwide. Japan leads the GRAPE-yard with more than 800 modules, followed by the US (119) and Germany (62). At the moment there are\n876 GRAPE-6 modules in Asia, 132 in North America and 107 in Europe. In\nTable 4 we list the sites with more than 1 Tflops peak-performance in GRAPE\nhardware and their network characteristics. The network roundtrip with the\nlongest latency is a roundtrip between Japan and Ukraine, whereas the network link with the longest latency (not given in Table 4) is the transpacific\nline between Japan and the US.\n5\n\nhttp://www.starplane.org/das3/\n\n17\n\n\fTable 4\nOverview of the major GRAPE clusters (> 1Tflops) on planet Earth, including their\nrelative network latency characteristics. The first column identifies the site, followed\nby the name of the institute, the country and the number of GRAPE modules (8\nmodules provide \u223c 1 Tflops peak performance.). The fifth column identifies the site\nwith which the latency, given in column #6, is shortest. The one but last column\n(column #7) identifies the site with which the latency, given in column #8, is\nlongest. The total number of GRAPE modules is 996.\nID institution\ncountry\n# GRAPEs nearest site farthest site\n[ms]\n[ms]\nA Nat. Astronomical Obs.\nJapan\n576\nB 2\nM 330\nA 2\nM 330\nB\nTsukuba University\nJapan\n240\nC AMNH\nUS\n40\nG 5\nB 190\nK 2\nB 280\nD Astron. Rechen Inst.\nGermany\n30\nE\nRochester Institute\nUS\n26\nF 5\nB 170\nE 5\nB 170\nF\nMcMaster University\nCanada\n13\nG Drexel University\nUS\n12\nC 5\nB 190\nD 10\nB 290\nH Max Planck Institute\nGermany\n12\nK 7\nB 266\nI\nUniversity of Amsterdam Netherlands 10\nJ\nWien University\nAustria\n10\nH 10\nB 290\nD 2\nB 280\nK Bonn University\nGermany\n9\nL\nCambridge University\nEngland\n9\nI 10\nB 260\nJ 30\nB 330\nM Main Astronomical Obs. Ukraine\n9\n\nOrganizing all the GRAPEs on the planet would be a challenging political\nproblem. Organizing only the 13 largest sites would be somewhat easier, therefore we included a performance prediction of such an infrastructure as well.\nConstructing a virtual organization within the 4 universities (University of\nAmsterdam, Free University of Amsterdam, Leiden University and Delft Technical University) that participate in the DAS-3 project (and equipping the 270\navailable DAS-3 nodes with specialized hardware) would be much easier than\ndoing this across various countries. The Dutch DAS-3 Grid is equipped with a\nfast Myrinet-10G internet and distributed across the Netherlands, connected\nby 10Gb light paths between clusters. The latency for the longest path is estimated to be 3 ms, and we estimate the bandwidth of the connections to be\n0.5 GB/s.\nIn the sequential case we do not take memory limitations into account. This\nassumption is unrealistic for predicting the performance of a GRAPE, since\nN < 262144, but GPUs have a similar performance to GRAPE, and are able\nto store up to 13 million particles (Portegies Zwart et al., 2007). In late 2007,\nthe launch of a double-precision GPU is expected, making GPUs usable for\nproduction-type direct-method N-body simulations. Additionally, in recent\nyears the amount of memory on GPUs has been steadily increasing, and we\nexpect this trend to persist in the near future.\nWe model the ring algorithm on both the G3 and on the 13 largest sites\n18\n\n\f(see Table 4). We adopted palpatine (see Table 1) as the workhorse host for\nthe GRAPEs and adopted the network characteristics as listed in Table 4.\nThe ring algorithm for our hypothetical grid experiment was assumed to be\noptimized for the use of distributed clusters of GRAPEs. The algorithm avoids\nlatency intensive network links by combining communication of local clusters\nfrom multiple shifts. Thus, communication for the local (across node) network\nis separated from the global (across sites) communication. Finally, we assume\nthat all networks between the sites have reasonable support for MPI multicast\nand gather operations, and that the latency of these operations scales with\nlog2 p\nThe results of the hypothetical global GRAPE grid are presented in Fig. 4.\nHere we see that a global grid in which all GRAPEs participate outperforms a\n> 108 particles. For a\nsingle GRAPE by about two orders of magnitude for N \u223c\n> 109 ) the total peak performance of\nsufficiently large number of particles (N \u223c\nthe global GRAPE grid approaches about 75 Tflops. Eventually, the grid with\nall the GRAPES would outperform the grid with only the largest machines by\nabout 25%, proportional to the number of GRAPEs in the two setups. When\nrunning simulations of N \u223c 106 it is faster to run on three large GRAPE sites,\nthan to use all the GRAPEs on the planet in parallel.\nThe dashed curve in Fig. 4 shows the performance of the model assuming that\nall the 270 nodes of the DAS-3 were equipped with GRAPE-6Af hardware.\nWith such a setup, the maximum performance of about 35 Tflops is achieved\nfor N \u223c 107 particles. This is an interesting number for production simulations\nfor astronomical research.\nIn Fig. 5 we present the wall-clock time for each of the different ingredients\nof a grid calculation with GRAPEs on the DAS-3, using the performance\nmodel. Break-even between calculation (straight solid curve) and communication (thick dotted curve) is achieved around N \u223c 3 * 106. For this large number\nof particles the communication between GRAPE and host, the predictor and\nthe corrector steps require little CPU time compared to the force calculation\n> 6 * 106 this setup would give an efficient use of the\non the GRAPE. For N \u223c\nspecial processors, and high performance calculations on the grid would then\nbe quite efficient.\n\n6\n\nDiscussion and Conclusions\n\nWe studied the potential use of a virtual organization in which GRAPEs are\nused in a wide area grid. For this purpose, we developed a performance model\nto simulate the behavior of a grid in which each of the nodes is equipped\nwith special purpose GRAPE hardware. We tested the performance model\n19\n\n\f6\n\n10\n\n5\n\n10\n\n4\n\n10\n\n3\n\n10\n\n2\n\nspeedup\n\n10\n\n1\n\n10\n\n0\n\n10\n\n-1\n\n10\n\n-2\n\n10\n\n-3\n\n10\n\n-4\n\n10\n\n3\n\n10\n\n4\n\n10\n\n5\n\n6\n\n10\n\n10\n\n7\n\n10\n\n8\n\n10\n\nN\n\nFig. 4. Speedup prediction of possible GRAPE grid setups compared to a single\nCPU. The solid lines indicates execution time using 1 CPU (horizontal reference\nline) or 1 GRAPE with infinite memory (curved line). The double-dotted line indicates the predicted speedup if all 1115 GRAPEs are linked together to perform\none simulation using an optimized ring algorithm. The dashed line indicates the\npredicted speedup if all GRAPE sites with more than 1 Tflops are linked together\nto perform one simulation using an optimized ring algorithm. The dotted line indicates the speedup if all 270 nodes in the Dutch DAS-3 grid would be equipped with\nGRAPEs.\n107\n\n106\n\n105\n\nt [s]\n\n104\n\n103\n\n102\n\n101\n\n100 3\n10\n\n104\n\n105\n\n106\n\n107\n\n108\n\nN\n\nFig. 5. Predicted decomposition of performance of a DAS-3 GRAPE grid. The thick\nsolid line indicates total execution time and the flat thick dashed line indicates time\nspent due to network latency. The thin dashed line indicates time spent on using\nthe network bandwidth and the steep thick dotted line indicates time spent on\ncalculating forces. The three bottom lines indicate time spent on communication\nbetween hosts and GRAPEs (upper thick dash-dotted line), correcting particles\n(middle dash-dotted line), and predicting particles (lower dotted line)\n\n20\n\n\fwith an actual grid across three sites, each of which is located on a different\ncontinent. We used GRAPE hardware in Japan, the Netherlands and the USA\nsimultaneously for calculations of 1024 up to 196608 particles.\nWith these particle numbers we were able to have a better performance than\na single computer without GRAPE. We measured a grid speedup of \u0393 \u223c 1.37\nfor a grid of PCs, and a grid of GRAPEs performs another \u223c 4 times faster.\nOn the entire range of N we were unable to reach superior speed compared\nto a single GRAPE. However, we estimate that a small intercontinental grid\n> 3 * 106 particles.\nof GRAPEs will reach superior performance for N \u223c\nWe used our grid calculations with GRAPE to construct and calibrate a performance model, with which we studied the performance of a world-wide grid\nof GRAPEs. When all the GRAPEs on the planet would participate in a virtual organization it is possible to utilize the total machine's performance, but\n> 109 . Though the total performance for\nonly for really large systems of N \u223c\nsuch a setup would be about 75 Tflops, such large N would still be impractical\nto run for production astronomical simulations.\nWe conclude that organizing all the major GRAPEs on the planet in a virtual\norganization is probably not worth the effort. Organizing a few of the largest\nsites with GRAPEs within one continent, however, appears politically doable\nand computationally favorable. For the DAS-3, for example, the GRAPEs\nwould be used at maximum performance for a feasible number of stars. Modern simulations of up to about a million stars have been done before using\nGRAPE (Portegies Zwart et al., 2004), but these calculations were performed\non a single cluster, rather than on a grid. A grid setup as proposed here would\nallow the simulation of a few million stars within a reasonable time span.\nIf we were to equip the full DAS-3 wide area computer in the Netherlands\nwith GRAPEs, maximum performance would already be achieved for N \u223c\n6 * 106 particles. Though still large, such simulations would be very doable and\nhave practical applications. We estimate that running a system of N = 106\nstars with a Salpeter mass spectrum (Salpeter, 1955) over a wide range of\nstellar masses to the moment of core collapse would take about 4 months.\nThe simulation would still be mostly dominated by network latency, but the\nhigh-throughput networking in the DAS-3 completely removes the bandwidth\nbottleneck.\nWe have mainly discussed the use of GRAPEs in a virtual organization, but\nnew developments in using graphical processing units appear to achieve similar\nspeeds as GRAPEs (Portegies Zwart et al., 2007; Hamada and Iitaka, 2007;\nBelleman et al., 2007). In addition, GPUs are equipped with a larger amount\nof memory, which allows us to exploit more memory-intensive, but also faster,\nparallel algorithms. Future grids are likely to be equipped with GPUs, as the\n21\n\n\fGPU will become part of the standard equipment for every PC.\nAlthough our proof-of-concept infrastructure was of limited size, we have\nshown that it is possible to use dedicated hardware components located across\nclusters for high-performance computing. Though the current performance\nover globally connected grids leaves a lot to be desired and much optimization remains to be done, the concept of using dedicated hardware components\nworldwide in parallel has been shown to work. It can therefore be applied\nto solving individual tightly-coupled scientific problems or as ingredient of a\ncomplex multi-physics simulation, such as simulating a full galaxy, given that\nthe problem size is sufficiently large to overcome the networking limitations.\n\n7\n\nAcknowledgements\n\nWe are grateful to Mary Inaba and Cees de Laat for discussion and support\nin realizing this grid setup, and to Alessia Gualandris for providing some of\nthe codes and feedback on this work. We are also grateful to Alfons Hoekstra,\nMarian Bubak and Stefan Harfst for fruitful discussions on the contents of this\npaper. This research is supported by the Netherlands organization for Scientific\nresearch (NWO) grant #643.200.503 and by the European Commission grant\nfor the QosCosGrid project (grant number: FP6-2005-IST-5 033883), and we\nthank SARA computing and networking services, Amsterdam for technical\nsupport.\n\nReferences\nAarseth, S. J., 1985. Direct n-body calculations. In: Goodman, J., Hut, P.\n(Eds.), Dynamics of Star Clusters. Vol. 113 of IAU Symposium. pp. 251\u2013\n258.\nAbramson, D., Giddy, J., Kotler, L., 2000. High performance parametric modeling with nimrod/g: Killer application for the global grid? ipdps 00, 520.\nAnderson, D. P., Cobb, J., Korpela, E., Lebofsky, M., Werthimer, D., 2002.\nSeti@home: an experiment in public-resource computing. Commun. ACM\n45 (11), 56\u201361.\nAngus, I. G., Fox, G. C., Kim, J. S., Walker, D. W., 1990. Solving problems\non concurrent processors: vol. 2. Prentice-Hall, Inc., Upper Saddle River,\nNJ, USA.\nBelleman, R. G., Bedorf, J., Portegies Zwart, S., Jul. 2007. High Performance\nDirect Gravitational N-body Simulations on Graphics Processing Units \u2013\nII: An implementation in CUDA. ArXiv e-prints 707.\n22\n\n\fCurnow, H. J., Wichmann, B. A., 1976. A synthetic benchmark. The Computer Journal 19 (1), 43\u201349.\nURL http://comjnl.oxfordjournals.org/cgi/content/abstract/19/1/43\nDorband, E. N., Hemsendorf, M., Merritt, D., 2003. Systolic and hyper-systolic\nalgorithms for the gravitational n-body problem, with an application to\nbrownian motion. J. Comput. Phys. 185 (2), 484\u2013511.\nFoster, I., Kesselman, C., Tuecke, S., 2001. The Anatomy of the Grid: Enabling\nScalable Virtual Organizations. International Journal of High Performance\nComputing Applications 15 (3), 200\u2013222.\nURL http://hpc.sagepub.com/cgi/content/abstract/15/3/200\nFox, G. C., Johnson, M. A., Lyzenga, G. A., Otto, S. W., Salmon, J. K.,\nWalker, D. W., 1988. Solving problems on concurrent processors. Vol. 1:\nGeneral techniques and regular problems. Prentice-Hall, Inc., Upper Saddle\nRiver, NJ, USA.\nFukushige, T., Makino, J., Kawai, A., Dec. 2005. GRAPE-6A: A\nSingle-Card GRAPE-6 for Parallel PC-GRAPE Cluster Systems.\nPublications of the Astronomical Society of Japan57, 1009\u20131021.\nGraham, R. L., Shipman, G. M., Barrett, B. W., Castain, R. H., Bosilca,\nG., Lumsdaine, A., September 2006. Open MPI: A high-performance, heterogeneous MPI. In: Proceedings, Fifth International Workshop on Algorithms, Models and Tools for Parallel Computing on Heterogeneous Networks. Barcelona, Spain.\nGualandris, A., Portegies Zwart, S., Tirado-Ramos, A., 2007. Performance\nanalysis of direct n-body algorithms for astrophysical simulations on distributed systems. Parallel Computing 33 (3), 159\u2013173.\nHamada, T., Iitaka, T., Mar. 2007. The Chamomile Scheme: An Optimized\nAlgorithm for N-body simulations on Programmable Graphics Processing\nUnits. ArXiv Astrophysics e-prints.\nHarfst, S., Gualandris, A., Merritt, D., Spurzem, R., Portegies Zwart, S.,\nBerczik, P., Jul. 2007. Performance analysis of direct N-body algorithms\non special-purpose supercomputers. New Astronomy 12, 357\u2013377.\nHeggie, D. C., Mathieu, R. D., 1986. Standardised Units and Time Scales. In:\nHut, P., McMillan, S. L. W. (Eds.), The Use of Supercomputers in Stellar\nDynamics. Vol. 267 of Lecture Notes in Physics, Berlin Springer Verlag. p.\n233.\nHoekstra, A. G., Portegies Zwart, S. F., Bubak, M., Sloot, P. M. A., Mar. 2007.\nTowards Distributed Petascale Computing. ArXiv Astrophysics e-prints.\nHoekstra, A. G., Sloot, P. M. A., 2005. Introducing grid speedup g: A scalability metric for parallel applications on the grid. In: EGC. pp. 245\u2013254.\nKaronis, N. T., Toonen, B., Foster, I., Jun. 2002. MPICH-G2: A Grid-Enabled\nImplementation of the Message Passing Interface. ArXiv Computer Science\ne-prints.\nLienhart, G., Kugel, A., R., M., 2002. Using floating-point arithmetic on fpgas\nto accelerate scientific n-body simulations. fccm 00, 182.\nMakino, J., Oct. 2002. An efficient parallel algorithm for O(N 2 ) direct sum23\n\n\fmation method and its variations on distributed-memory parallel machines.\nNew Astronomy 7, 373\u2013384.\nMakino, J., Aarseth, S. J., Apr. 1992. On a Hermite integrator\nwith Ahmad-Cohen scheme for gravitational many-body problems.\nPublications of the Astronomical Society of Japan44, 141\u2013151.\nMakino, J., Fukushige, T., Koga, M., Namura, K., Dec. 2003a. GRAPE6: Massively-Parallel Special-Purpose Computer for Astrophysical Particle\nSimulations. Publications of the Astronomical Society of Japan55, 1163\u2013\n1187.\nMakino, J., Kokubo, E., Fukushige, T., 2003b. Performance evaluation and\ntuning of grape-6 - towards 40 \"real\" tflops. sc 00, 2.\nMcMillan, S. L. W., 1986. The use of supercomputers in stellar dynamics;\nproceedings of the workshop, institute for advanced study, princeton, nj,\njune 2-4, 1986. In: Hut, P., McMillan, S. L. W. (Eds.), The Use of Supercomputers in Stellar Dynamics. Vol. 267 of Lecture Notes in Physics, Berlin\nSpringer Verlag. p. 156.\nMonaghan, J. J., 1992. Smoothed particle hydrodynamics. ARA&A30, 543\u2013\n574.\nPlummer, H. C., Mar. 1911. On the problem of distribution in globular star\nclusters. MNRAS71, 460\u2013470.\nPortegies Zwart, S. F., Baumgardt, H., Hut, P., Makino, J., McMillan,\nS. L. W., Apr. 2004. Formation of massive black holes through runaway\ncollisions in dense young star clusters. Nature428, 724\u2013726.\nPortegies Zwart, S. F., Belleman, R. G., Geldof, P. M., Nov. 2007. Highperformance direct gravitational N-body simulations on graphics processing\nunits. New Astronomy 12, 641\u2013650.\nSalpeter, E. E., Jan. 1955. The Luminosity Function and Stellar Evolution.\nApJ121, 161.\n\n24\n\n\f"}