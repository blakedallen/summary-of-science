{"id": "http://arxiv.org/abs/cs/0505085v1", "guidislink": true, "updated": "2005-05-31T08:23:32Z", "updated_parsed": [2005, 5, 31, 8, 23, 32, 1, 151, 0], "published": "2005-05-31T08:23:32Z", "published_parsed": [2005, 5, 31, 8, 23, 32, 1, 151, 0], "title": "Improving PARMA Trailing", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0405012%2Ccs%2F0405025%2Ccs%2F0405109%2Ccs%2F0405049%2Ccs%2F0405024%2Ccs%2F0405089%2Ccs%2F0405082%2Ccs%2F0405110%2Ccs%2F0405019%2Ccs%2F0405006%2Ccs%2F0405016%2Ccs%2F0405073%2Ccs%2F0405005%2Ccs%2F0405052%2Ccs%2F0405031%2Ccs%2F0405030%2Ccs%2F0405077%2Ccs%2F0405068%2Ccs%2F0405090%2Ccs%2F0405053%2Ccs%2F0405083%2Ccs%2F0405057%2Ccs%2F0405003%2Ccs%2F0405086%2Ccs%2F0405020%2Ccs%2F0405035%2Ccs%2F0405038%2Ccs%2F0405039%2Ccs%2F0405071%2Ccs%2F0405096%2Ccs%2F0405092%2Ccs%2F0405098%2Ccs%2F0405013%2Ccs%2F0405087%2Ccs%2F0405066%2Ccs%2F0405051%2Ccs%2F0405014%2Ccs%2F0405094%2Ccs%2F0405075%2Ccs%2F0405065%2Ccs%2F0405085%2Ccs%2F0405032%2Ccs%2F0405034%2Ccs%2F0405070%2Ccs%2F0405002%2Ccs%2F0405050%2Ccs%2F0405105%2Ccs%2F0405026%2Ccs%2F0405022%2Ccs%2F0405058%2Ccs%2F0405007%2Ccs%2F0405048%2Ccs%2F0405043%2Ccs%2F0405008%2Ccs%2F0405010%2Ccs%2F0405055%2Ccs%2F0405045%2Ccs%2F0505054%2Ccs%2F0505021%2Ccs%2F0505088%2Ccs%2F0505016%2Ccs%2F0505057%2Ccs%2F0505042%2Ccs%2F0505063%2Ccs%2F0505037%2Ccs%2F0505052%2Ccs%2F0505036%2Ccs%2F0505001%2Ccs%2F0505039%2Ccs%2F0505007%2Ccs%2F0505022%2Ccs%2F0505006%2Ccs%2F0505004%2Ccs%2F0505012%2Ccs%2F0505081%2Ccs%2F0505051%2Ccs%2F0505079%2Ccs%2F0505076%2Ccs%2F0505031%2Ccs%2F0505038%2Ccs%2F0505048%2Ccs%2F0505030%2Ccs%2F0505055%2Ccs%2F0505062%2Ccs%2F0505015%2Ccs%2F0505083%2Ccs%2F0505047%2Ccs%2F0505087%2Ccs%2F0505085%2Ccs%2F0505049%2Ccs%2F0505058%2Ccs%2F0505023%2Ccs%2F0505034%2Ccs%2F0505073%2Ccs%2F0505017%2Ccs%2F0505009%2Ccs%2F0505026%2Ccs%2F0505044%2Ccs%2F0505003%2Ccs%2F0505061%2Ccs%2F0505045&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Improving PARMA Trailing"}, "summary": "Taylor introduced a variable binding scheme for logic variables in his PARMA\nsystem, that uses cycles of bindings rather than the linear chains of bindings\nused in the standard WAM representation. Both the HAL and dProlog languages\nmake use of the PARMA representation in their Herbrand constraint solvers.\nUnfortunately, PARMA's trailing scheme is considerably more expensive in both\ntime and space consumption. The aim of this paper is to present several\ntechniques that lower the cost.\n  First, we introduce a trailing analysis for HAL using the classic PARMA\ntrailing scheme that detects and eliminates unnecessary trailings. The\nanalysis, whose accuracy comes from HAL's determinism and mode declarations,\nhas been integrated in the HAL compiler and is shown to produce space\nimprovements as well as speed improvements. Second, we explain how to modify\nthe classic PARMA trailing scheme to halve its trailing cost. This technique is\nillustrated and evaluated both in the context of dProlog and HAL. Finally, we\nexplain the modifications needed by the trailing analysis in order to be\ncombined with our modified PARMA trailing scheme. Empirical evidence shows that\nthe combination is more effective than any of the techniques when used in\nisolation.\n  To appear in Theory and Practice of Logic Programming.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0405012%2Ccs%2F0405025%2Ccs%2F0405109%2Ccs%2F0405049%2Ccs%2F0405024%2Ccs%2F0405089%2Ccs%2F0405082%2Ccs%2F0405110%2Ccs%2F0405019%2Ccs%2F0405006%2Ccs%2F0405016%2Ccs%2F0405073%2Ccs%2F0405005%2Ccs%2F0405052%2Ccs%2F0405031%2Ccs%2F0405030%2Ccs%2F0405077%2Ccs%2F0405068%2Ccs%2F0405090%2Ccs%2F0405053%2Ccs%2F0405083%2Ccs%2F0405057%2Ccs%2F0405003%2Ccs%2F0405086%2Ccs%2F0405020%2Ccs%2F0405035%2Ccs%2F0405038%2Ccs%2F0405039%2Ccs%2F0405071%2Ccs%2F0405096%2Ccs%2F0405092%2Ccs%2F0405098%2Ccs%2F0405013%2Ccs%2F0405087%2Ccs%2F0405066%2Ccs%2F0405051%2Ccs%2F0405014%2Ccs%2F0405094%2Ccs%2F0405075%2Ccs%2F0405065%2Ccs%2F0405085%2Ccs%2F0405032%2Ccs%2F0405034%2Ccs%2F0405070%2Ccs%2F0405002%2Ccs%2F0405050%2Ccs%2F0405105%2Ccs%2F0405026%2Ccs%2F0405022%2Ccs%2F0405058%2Ccs%2F0405007%2Ccs%2F0405048%2Ccs%2F0405043%2Ccs%2F0405008%2Ccs%2F0405010%2Ccs%2F0405055%2Ccs%2F0405045%2Ccs%2F0505054%2Ccs%2F0505021%2Ccs%2F0505088%2Ccs%2F0505016%2Ccs%2F0505057%2Ccs%2F0505042%2Ccs%2F0505063%2Ccs%2F0505037%2Ccs%2F0505052%2Ccs%2F0505036%2Ccs%2F0505001%2Ccs%2F0505039%2Ccs%2F0505007%2Ccs%2F0505022%2Ccs%2F0505006%2Ccs%2F0505004%2Ccs%2F0505012%2Ccs%2F0505081%2Ccs%2F0505051%2Ccs%2F0505079%2Ccs%2F0505076%2Ccs%2F0505031%2Ccs%2F0505038%2Ccs%2F0505048%2Ccs%2F0505030%2Ccs%2F0505055%2Ccs%2F0505062%2Ccs%2F0505015%2Ccs%2F0505083%2Ccs%2F0505047%2Ccs%2F0505087%2Ccs%2F0505085%2Ccs%2F0505049%2Ccs%2F0505058%2Ccs%2F0505023%2Ccs%2F0505034%2Ccs%2F0505073%2Ccs%2F0505017%2Ccs%2F0505009%2Ccs%2F0505026%2Ccs%2F0505044%2Ccs%2F0505003%2Ccs%2F0505061%2Ccs%2F0505045&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Taylor introduced a variable binding scheme for logic variables in his PARMA\nsystem, that uses cycles of bindings rather than the linear chains of bindings\nused in the standard WAM representation. Both the HAL and dProlog languages\nmake use of the PARMA representation in their Herbrand constraint solvers.\nUnfortunately, PARMA's trailing scheme is considerably more expensive in both\ntime and space consumption. The aim of this paper is to present several\ntechniques that lower the cost.\n  First, we introduce a trailing analysis for HAL using the classic PARMA\ntrailing scheme that detects and eliminates unnecessary trailings. The\nanalysis, whose accuracy comes from HAL's determinism and mode declarations,\nhas been integrated in the HAL compiler and is shown to produce space\nimprovements as well as speed improvements. Second, we explain how to modify\nthe classic PARMA trailing scheme to halve its trailing cost. This technique is\nillustrated and evaluated both in the context of dProlog and HAL. Finally, we\nexplain the modifications needed by the trailing analysis in order to be\ncombined with our modified PARMA trailing scheme. Empirical evidence shows that\nthe combination is more effective than any of the techniques when used in\nisolation.\n  To appear in Theory and Practice of Logic Programming."}, "authors": ["Tom Schrijvers", "Maria Garcia de la Banda", "Bart Demoen", "Peter J. Stuckey"], "author_detail": {"name": "Peter J. Stuckey"}, "author": "Peter J. Stuckey", "arxiv_comment": "36 pages, 7 figures, 8 tables", "links": [{"href": "http://arxiv.org/abs/cs/0505085v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0505085v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.PL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.PF", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "D.3.4; D.1.6; D.3.3", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0505085v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0505085v1", "journal_reference": null, "doi": null, "fulltext": "Under consideration for publication in Theory and Practice of Logic Programming\n\n1\n\narXiv:cs/0505085v1 [cs.PL] 31 May 2005\n\nImproving PARMA trailing\nTOM SCHRIJVERS\u2217 and BART DEMOEN\nDept. of Computer Science, K.U.Leuven, Belgium\n(e-mail: {toms,bmd}@cs.kuleuven.ac.be)\n\nMARIA GARCIA DE LA BANDA\nSchool of Computer Science and S.E, Monash University, Australia\n(e-mail: mbanda@csse.monash.edu.au)\n\nPETER J. STUCKEY\nNICTA Victoria Laboratory\nDepartment of Computer Science and S.E.\nUniversity of Melbourne, Australia\n(e-mail: pjs@cs.mu.oz.au)\nsubmitted 20 November 2003; revised 22 October 2004; accepted 30 May 2005\n\nAbstract\nTaylor introduced a variable binding scheme for logic variables in his PARMA system,\nthat uses cycles of bindings rather than the linear chains of bindings used in the standard\nWAM representation. Both the HAL and dProlog languages make use of the PARMA representation in their Herbrand constraint solvers. Unfortunately, PARMA's trailing scheme\nis considerably more expensive in both time and space consumption. The aim of this paper\nis to present several techniques that lower the cost.\nFirst, we introduce a trailing analysis for HAL using the classic PARMA trailing scheme\nthat detects and eliminates unnecessary trailings. The analysis, whose accuracy comes\nfrom HAL's determinism and mode declarations, has been integrated in the HAL compiler\nand is shown to produce space improvements as well as speed improvements. Second, we\nexplain how to modify the classic PARMA trailing scheme to halve its trailing cost. This\ntechnique is illustrated and evaluated both in the context of dProlog and HAL. Finally,\nwe explain the modifications needed by the trailing analysis in order to be combined with\nour modified PARMA trailing scheme. Empirical evidence shows that the combination is\nmore effective than any of the techniques when used in isolation.\nTo appear in Theory and Practice of Logic Programming.\nKEYWORDS: constraint logic programming, program analysis, trailing\n\n1 Introduction\nThe logic programming language Mercury (Somogyi et al. 1996) is considerably\nfaster than traditional implementations of Prolog due to two main reasons. First,\n\u2217 Research Assistant of the fund for Scientific Research - Flanders (Belgium)(F.W.O. - Vlaanderen)\n\n\fMercury requires the programmer to provide type, mode and determinism declarations whose information is used to generate efficient target code. And second,\nvariables can only be ground (i.e., bound to a ground term) or new (i.e., first time\nseen by the compiler and hence unconstrained). Since neither aliased variables nor\npartially instantiated structures are allowed, Mercury does not need to support full\nunification; only assignment, construction, deconstruction and equality testing for\nground terms are required. Furthermore, it does not need to perform trailing, a\ntechnique that allows an execution to resume computation from a previous program state: information about the old state is logged during forward computation\nand used to restore it during backtracking. This usually means recording the state\nof unbound variables right before they become aliased or bound. Since Mercury's\nnew variables have no run-time representation they do not need to be trailed.\nHAL (Demoen et al. 1999; de la Banda et al. 2002) is a constraint logic language\ndesigned to support the construction, extension and use of constraint solvers. HAL\nalso requires type, mode and determinism declarations and compiles to Mercury\nso as to leverage from its sophisticated compilation techniques. However, unlike\nMercury, HAL includes a Herbrand constraint solver which provides full unification. This solver uses Taylor's PARMA scheme (Taylor 1991; Taylor 1996) rather\nthan the standard WAM representation (A\u0131\u0308t-Kaci 1991). This is because, unlike\nthe WAM, the PARMA representation of ground terms does not contain reference\nchains and, hence, it is equivalent to that of Mercury. Thus, calls to the Herbrand\nconstraint solver can be replaced by calls to Mercury's more efficient routines whenever ground terms are being manipulated.\nUnfortunately, the increased expressive power of full unification comes at a cost,\nwhich includes the need to perform trailing. Furthermore, trailing in the PARMA\nscheme is more expensive than in the WAM, both in terms of time and space. We\npresent here two techniques to counter the trailing penalty of the PARMA scheme.\nThe first is a trailing analysis that detects and eliminates at compile-time unnecessary trailings and is suitable for any system based on the classic PARMA trailing\nscheme. Without other supporting information such analysis is rather inaccurate,\nsince little is known at compile-time about the way predicates are used. However,\nwhen mode and determinism information is available at compile-time, as in HAL,\nsignificant accuracy improvements can be obtained. The second technique is a modified PARMA trailing scheme which considerably reduces the required trail stack\nsize. This technique can be applied to any PARMA-based system and has been\nimplemented by us in both dProlog (Demoen and Nguyen 2000) and the Mercury\nback-end of the HAL system. Finally, we detail the modifications required by our\ntrailing analysis in order to be combined with our modified trailing scheme. The\nempirical evaluation of each technique indicates that the combination of the modified trailing scheme with the trailing analysis results in a significant reduction of\ntrail size at a negligible time cost.\nThe rest of the paper proceeds as follows. The next section provides a quick background on trailing, the classic PARMA scheme, and when trailing can be avoided.\nSection 3 summarizes the information used by our analyzer to improve its accuracy.\nSection 4 presents the notrail analysis domain. Section 5 shows how to analyze\n2\n\n\fHAL's body constructs. Section 6 shows how to use the analysis information to\navoid trailing. Section 7 presents the modified trailing scheme. Section 8 shows\nthe changes required by the analysis to deal with this modified scheme. Section 9\npresents the results from the experimental evaluation of each technique. Finally,\nfuture work is discussed in Section 10.\n2 Background\nWe begin by setting some terminology. A bound variable is a variable that is bound\nto some nonvariable term. An aliased variable is unbound and equated with some\nother variable. A free variable is unbound and unaliased. We will also refer to a\nnew variable, which is a variable in HAL (and Mercury) which has no run-time\nrepresentation, since it is yet to be constrained.\nIn the WAM, an unbound variable is represented by a linear chain. If the variable\nis free the chain has length one (a cell containing a self-reference). When two free\nvariables are unified, the younger cell is made to point to the older cell (see Section\n2.2 for a discussion of relative cell age). These two variables are now aliased. A series\nof unifications of free variables thus results in a linear chain of references of which\nthe last one is a self-reference or, in case the variable becomes instantiated, a bound\nterm. This representation implies that testing whether a (source level) variable is\nbound or unbound, requires dereferencing. Such dereferencing is necessary during\neach unification and it is thus performed quite often.\nExample 1\nConsider the execution of the goal X = Y, Z = W, X = Z, X = a when each variable is initially represented by a self-reference. Using the WAM representation, the\nfirst unification points X at Y. The second unification points Z at W. In the third\nunification we must first dereference X to get Y, dereference Z to give W, and then\npoint Y at W. In the last unification we dereference X and set W to a. The changes in\nheap states are shown in Figure 1.\n\nX\n\nY\n\nX\n\nY\n\nX\n\nY\n\nX\n\nY\n\nX\n\nZ\n\nW\n\nZ\n\nW\n\nZ\n\nW\n\nZ\n\nW\n\nZ\n\n(a) Initially\n\n(b) X = Y\n\n(c) Z = W\n\n(d) X = Z\n\nY\na\n\nW\n\n(e) X = a\n\nFig. 1. Example of binding chains using the WAM representation.\nIn his PARMA-system (Taylor 1996), Taylor introduced a different variable representation scheme that does not suffer from this dereferencing need. In this scheme\nan unbound variable is represented by a circular chain. If the variable is free the\n3\n\n\fchain has length one (a self-reference as in the WAM). Unifying two variables in\nthis scheme consists of cutting their circular chains and combining them into one\nbig circular chain. When the variable is bound, each cell in the circular chain is\nreplaced by the value to which it is bound. No dereferencing is required to verify\nwhether a cell is bound, because the tag in a cell immediately identifies the cell as\nbeing bound or not. However, as we will see later, other costs are incurred by the\nscheme.\nExample 2\nConsider the execution of the same goal X = Y, Z = W, X = Z, X = a when again\neach variable is initially represented by a self-reference. Using the PARMA representation, the first unification points X at Y and Y at X. The second unification\npoints Z at W and W at Z. In the third unification we must point X at W and Z at\nY. In the final unification each variable in the chain of X is set to a. The changes\nin heap states are shown in Figure 2. Notice how no references remain in the final\nstate, as opposed to Figure 1(e).\n\nX\n\nY\n\nX\n\nY\n\nX\n\nY\n\nX\n\nY\n\nX\n\nZ\n\nW\n\nZ\n\nW\n\nZ\n\nW\n\nZ\n\nW\n\nZ\n\n(a) Initially\n\n(b) X = Y\n\n(c) Z = W\n\n(d) X = Z\n\na\n\na\n\na\n\na\n\nY\nW\n\n(e) X = a\n\nFig. 2. Example of binding chains using the PARMA representation.\nAnother difference between the WAM and PARMA binding schemes becomes\napparent when constructing a new term containing an unbound variable X. Effectively, we are aliasing a new variable with X and, hence, this new variable must be\nadded into the variable chain of X.\nExample 3\nConsider the execution of the goal X = Y, Z = f(X) when each variable is initially\nrepresented by a self-reference.\nUsing the WAM representation, the first unification points X at Y. The second\nunification constructs a heap term f(X) with the content of X, namely Y, and points\nZ at this.\nUsing the PARMA representation, the first unification chains X and Y together.\nThe second unification has to add the copy of X in f(X), to the chain for X. The\nresulting heap states are shown in Figure 3.\nAs mentioned before, trailing is a technique that stores enough information regarding the representation state of a variable before each choice-point, to be able to\nreconstruct such state upon backtracking. For both WAM and PARMA chains the\n4\n\n\fX\n\nY\n\nX\nZ\n\nZ\n\n(a) X = Y\n\nY\n\nX\n\nf\n\nY\n\nZ\n\n(b) Z=f(X)\n\nX\nZ\n\n(c) X = Y\n\nY\nf\n\n(d) Z=f(X)\n\nFig. 3. Example of constructing a term containing an unbound variable using both\nWAM (a)(b) and PARMA (c)(d) representations.\nchange of representation state occurs at the cell level: from being a self-reference\n(when the variable represented by the cell \u2013 the associated variable \u2013 is unbound\nand unaliased), to pointing to another cell in the chain (when the associated variable gets aliased), to pointing to the final bound structure (when the variable is\nbound directly or indirectly). Thus, what needs to be trailed are the cells.\nIn the rest of the section we will discuss the PARMA trailing scheme in greater\ndetail, the orthogonal issue of conditional/unconditional trailing, and a possible\nimprovement based on compile-time detection of unnecessary trailings.\n2.1 The classic PARMA Scheme: Value trailing\nThe classic PARMA trailing scheme uses value trailing, described by the following\nC-like code:1\nvaluetrail(p) {\n*(tr++) = *p; /* store the contents of the cell p */\n*(tr++) = p; /* store the address of the cell p */\n}\nwhich takes the address p of a cell in a PARMA chain and stores in the trail stack\nfirst the (old) contents of the cell and then its address. Here, tr is a global pointer\nto the top of the trail stack.\nThe untrail operation for value trailing is straightforwardly defined by:\nuntrail_valuetrail() {\naddress = *(--tr); /* retrieve the cell address */\n*address = *(--tr); /* recover the cell contents */\n}\nwhich first pops the address of a cell and then its contents.\nIn contrast, trailing in the WAM stores only the address of the cell. The reasons\nare twofold. First, a cell is updated at most once, from a self-reference to a pointer to\n1\n\nAll code in this paper is pseudo-C code. Implementation details that obfuscate rather than\nclarify the concepts at hand, have been omitted.\n\n5\n\n\feither another cell in a linear chain or a structure. And second, for a self-referencing\ncell the address and the content of the cell are the same. Therefore, when a cell\nis updated the old content of the cell (which is the one stored during trailing) is\nalways the same as its address. This allows the WAM value trailing to be optimized\nby only storing the address of the cell, reducing by half the space cost of a single\ncell trailing.\nLet us now discuss when cells need to be trailed in the classic PARMA scheme.\nWe have seen before that trailing is only needed when the representation state of a\nvariable changes, and that this can only happen when the variable is unbound and,\ndue to a unification, it becomes either aliased or bound. Therefore, we only need\nto trail cells when their associated variables are involved in a unification or when\ncreating a new term which contains an unbound variable. The following discussion\ndistinguishes three cases: cells associated to variables involved in a variable\u2013variable\nunification, in a variable\u2013nonvariable unification, and in new term construction.\nTrailing during variable\u2013variable unification: The result of aliasing two unbound\nvariables belonging to separate chains is the merging of the two chains into a single\none. This can be done by changing the state of only two cells: those associated to\neach of the variables. Since each associated cell appears in a different chain, the\nfinal chain can be formed by simply interchanging their respective successors. One\ncan then reconstruct the previous situation by remembering which two cells have\nbeen changed and what their initial value was. This is achieved for unification X =\nY by the following (simplified) code:\nvaluetrail(X);\nvaluetrail(Y);\ntmp = *X;\n*X = *Y;\n*Y = tmp;\nNotice that X and Y are trailed independently. As only their associated cells need\nto be trailed, we will refer to this kind of trailing as shallow trailing.\nIn contrast, for this kind of unification the WAM will update and trail the last\ncell in just one of the two linear chains. Hence, the space cost is four times lower\n(one value as opposed to four).\nExample 4\nConsider the PARMA trailing that occurs during the first three unifications of the\ngoal X = Y, Z = W, X = Z, X = a from Example 2, when each variable is initially\nrepresented by a self-reference. From the first unification we trail X together with\nits initial value (which, since X is a self-reference, is also) X, and Y together with its\ninitial value Y. Similarly, for the second unification we trail Z together with its value\nZ, and W together with its value W. For the third unification, we trail X together with\nits value Y, and Z together with its value W. The resulting trail is\nX\n\nX\n\nY\n\nY\n\nZ\n\nZ\n\nW\n6\n\nW\n\nY\n\nX\n\nW\n\nZ\n\n\fThe WAM trail for the same goal illustrated in Figure 1 trails first X, then Z and\nfinally Y. The resulting trail is X Z Y .\nTrailing during variable\u2013nonvariable unification: When an unbound variable becomes bound, every single cell in its chain is set to point to the nonvariable term.\nThus, we can only reconstruct the chain if all cells in the chain are trailed. The\ncombined unification-trailing (simplified) code for unification X = Term is as follows:\nstart = X;\ndo {\nnext = *X;\nvaluetrail(X);\n*X = Term;\nX = next;\n} while (X != start);\nSince all cells in the chain of the unbound variable are trailed, we will refer to\nthis kind of trailing as deep trailing.\nIn contrast, for this kind of unification, the WAM will trail again just one cell in\nthe linear chain. Hence, the space complexity for WAM is just O(1) compared to\nO(n) for PARMA, where n is the number of cells in the chain. However, the time\ncomplexity is O(n) for both, due to the dereferencing in the WAM.\nExample 5\nConsider the PARMA trailing that happens in the last unification X = a of the goal\nfrom Example 2. The binding of all variables in the chain adds the trail elements\nW\n\nX\n\nX\n\nY\n\nY\n\nZ\n\nZ\n\nW\n\nIn contrast the WAM trailing adds a single trail element Y .\nTrailing during new term construction: As mentioned before, when a new term is\nconstructed on the heap with a copy of an unbound variable X, the cell containing\nthis copy must be added into the chain for X. This means we must trail X since\nits value (i.e., its successor in the chain) is going to change. We do not need to trail\nthe new cell since it clearly has no previous value we need to recover. The combined\nconstruction-trailing (simplified) code for constructing f(X) where X is an unbound\nvariable and th is the current top of heap pointer, is:\n*(++th) = *X;\nvaluetrail(X);\n*X = th;\nIn contrast, for this construction the WAM need not trail at all since it simply\npoints the new cell at the old unbound variable.\nIf X is either a bound or a new variable, this complexity does not arise: X will\nbe placed in the new structure pointing to either the nonvariable term or to itself,\nwith no trailing required in any case.\n7\n\n\fSummary: The major advantage of the PARMA binding scheme is that it requires\nno dereferencing, while its major disadvantages are (for a detailed account see\n(Lindgren et al. 1995)):\n1. PARMA trails more cells per unification: two in variable-variable unifications\nand all in variable-nonvariable, versus one.\n2. Trailing of an individual cell is more expensive: two slots used versus one.\n3. Unlike in the WAM, cells can be trailed more than once: every time a cell is\nupdated which can happen more than once.\n4. Copying an unbound variable into a structure involves trailing a cell.\nAs a result, the trail stack usage is expected to be much higher in the PARMA\nscheme than in the WAM. Demoen and Nguyen (Demoen and Nguyen 2000) have\nindeed observed in the dProlog system maximal trail sizes for the PARMA scheme\nthat are on average twice as large as with the WAM scheme. The techniques we\npresent in this paper attempt to counter the disadvantages. The trailing analysis\nreduces the number of trailings and thereby counters disadvantages 1, 3 and 4,\nwhile the modified trailing scheme counters disadvantage 2.\n2.2 Conditional versus unconditional trailing\nA cell that is changed only requires trailing if the cell did exist before the most\nrecent choice point since, otherwise, there is no previous state that has to be restored during backtracking. This property applies equally to the WAM and PARMA\nschemes.\nIn some systems a simple run-time test can be used to verify whether a cell is\nolder than the most recent choice point. Younger cells require no trailing. If all\ncells on the heap are kept in order of allocation, the test simply checks whether the\naddress of the cell is smaller than that of bh, the address of the top of the heap at\nthe beginning of the most recent choice point. Systems, such as dProlog, which take\nadvantage of this property use what is known as conditional trailing. Let us assume\nthe existence of function is older(p,bh) which succeeds if p < bh. Conditional\ntrailing is then described by the following code:\ncond_valuetrail(p, bh) {\nif (is_older(p,bh))\nvaluetrail(p);\n}\nthus avoiding the trailing of cells which are newer than the most recent choice point.\nThe code for variable\u2013variable and variable\u2013nonvariable unification described in the\nprevious sections using the unconditional valuetrail operation can be rewritten\nto use conditional trailing by simply substituting each call to valuetrail by a call\nto cond valuetrail. The untrail operation remains unchanged.\nIn systems where the order of cells on the heap is not guaranteed, unconditional\ntrailing is required. The Mercury back-end of the HAL system, for example, is\nsuch a system since Mercury uses the Boehm garbage collector which does not\n8\n\n\fpreserve the order of the cells on the heap between garbage collections. Other\nsystems use unconditional trailing at least during some unifications (see for instance (Van Roy and Despain 1992)). In (Demoen and Nguyen 2000) it is shown\nthat global performance is hardly affected by the choice between conditional or\nunconditional trailing, since the savings made on avoided trailings are balanced by\nthe overhead of the run-time tests.\nThe differences between conditional and unconditional trailing do not affect the\nproposed analysis. Thus, the same analysis can still be used if at some point conditional trailing becomes available in Mercury.\n2.3 Unnecessary trailing in the classic PARMA scheme:\nWhen considering the trailing of an unbound variable appearing in a unification,\nthere are at least two cases in which its trailing can be avoided:\n\u2022 If the variable is new there is no previous value to remember and, therefore,\ntrailing is not required. This is in fact a subset of the cases exploited by\nconditional trailing.\n\u2022 The cells that need to be trailed (the associated cell in the case of variable\u2013\nvariable, all cells in the case of variable\u2013nonvariable) have already been trailed\nsince the most recent choice-point. Upon backtracking only the earliest trailing\nafter the choice-point is important, since that is the one which enables the\nreconstruction of the state of the variable before the choice-point.\nIn the following sections we will see how compile-time analysis information can\nbe obtained to detect the above two cases and can therefore be used to (a) eliminate\nunnecessary trailing in the classical PARMA trailing scheme, and (b) eliminate runtime tests performed by conditional trailing on variables known at compile-time to\nhave no representation and thus be younger than the most recent choice point.\n3 Language Requirements\nThe analysis presented in this paper was designed for the HAL language. However,\nit can be useful for any language that uses PARMA representation and that provides\naccurate information regarding the following properties:\n\u2022 Instantiation state: trailing analysis can gain accuracy by taking into account\nthe instantiation state of a program variable, i.e. whether the variable is new,\nground or old. State new corresponds to program variables with no internal\nrepresentation (equivalent to Mercury's free instantiation). State ground corresponds to program variables known to be bound to ground terms. In any\nother case the state is old, corresponding to program variables which might\nbe unbound but do have a representation (a chain of length one or more) or\nbound to a term not known to be ground. Program variables with instantiation state new, ground or old will be called new, ground or old variables,\nrespectively. Note that once a new variable becomes old or ground, it can never\n9\n\n\fbecome new again. And once it is known to be ground, it remains ground.\nThus, the three states can be considered mutually exclusive. The information\nshould be available at each program point p as a table associating with each\nvariable in scope of p its instantiation state.\nWe will represent the instantiation table information at program point p as\nfollows. Let V arp denote the set of all program variables in scope at program\npoint p. The function instp : V arp \u2192 {new, ground, old} defines the instantiation state of program variable X at point p. This function allows us to\npartition V arp into three disjoint sets: N ewp , Groundp and Oldp containing\nthe set of new, ground and old variables, respectively.\n\u2022 Determinism: trailing analysis can also gain accuracy from the knowledge\nthat particular predicates have at most one solution. This information should\nbe available as a table associating with each predicate (procedure to be more\nprecise) its determinism. Herein we will refer to six main kinds of determinism:\nsemidet (minimum-maximum set of solutions: 0-1), det (1-1), multi (1-\u221e),\nnondet (0-\u221e), erroneous (1,0), and failure (0-0).\nFor our purposes we will only be interested in whether a predicate can return\nmore than one answer. We will represent the determinism table by a function\ndet : P red \u2192 {0, 1, \u221e} which maps each predicate q to its maximum number\nof solutions.\n\u2022 Sharing: trailing analysis can exploit sharing information to increase accuracy.\nThis information should be available at each program point p as a table\nassociating with each variable in scope of p the set of variables which possibly\nshare with it. Clearly, any variables that may be aliased together must possibly\nshare.\nWe will represent the sharing table at program point p by the function sharep :\nOldp \u2192 P(Oldp ) which assigns to each program variable in Oldp the set of\nprogram variables in Oldp that share with it. Note that program variables in\nN ewp and Groundp cannot share by definition.\n\n4 The notrail Analysis Domain\nThe aim of the notrail domain is to keep enough information to be able to decide\nwhether the run-time variables in a unification need to be trailed or not, so that if\npossible, optimized versions which do not perform the trailing can be used instead.\nIn order to do this, we must remember that only run-time variables which are\nunbound and have a representation (i.e., are not new) need to be trailed. This\nsuggests making use of the instantiation information mentioned in the previous\nsection. Note that, since the analysis works on the level of program variables, some\nindirection will be required.\nWe have already established that program variables in N ewp and Groundp represent run-time variables which do not need to be trailed. Thus, only variables in\nOldp need to be represented in the notrail domain. the set of new, ground and\nold program variables, respectively. Assuming that V arp contains n variables and\n10\n\n\fthe tree we have used to implement the underlying table is sufficiently balanced,\nthen, the size of the Oldp is O(n) and the complexity of instp is O(log n).\nRecall that Oldp contains all program variables representing not only run-time\nvariables which are unbound and have a representation, but also run-time variables\nbound to terms which the analysis cannot ensure to be ground. This is necessary\nto ensure correctness: even though run-time variables which are bound do not need\nto be trailed, the nonvariable terms to which they are bound might contain one\nor more unbound run-time variables. It is the trailing state of these unbound runtime variables that is represented through the domain representation of the bound\nprogram variable.\nNow that we have decided which program variables need to be represented by\nour domain, we have to decide how to represent them. We saw before that it is\nunnecessary to trail a run-time variable in a variable\u2013variable unification if its\nassociated cell has already been trailed, i.e., if the run-time variable has already\nbeen shallow trailed since the most recent choice-point. For the case of variable\u2013\nnonvariable unification this is not enough, we need to ensure all cells in the chain\nhave already been trailed, i.e, the run-time variable has already been deep trailed.\nThis suggests a domain which distinguishes between shallow and deep trailed runtime variables. This can be easily done by partitioning Oldp into three disjoint\nsets of program variables with a different trailing state: those representing runtime variables which might not have been trailed yet, those representing run-time\nvariables which have at least been shallow trailed, and those representing run-time\nvariables which have been deep trailed. It is sufficient to keep track of only two\nsets to be able to reconstruct the third. Hence, the type of the elements of our\nnotrail domain Lnotrail will be P(Oldp ) \u00d7 P(Oldp ), where the first component\ncontains the set of program variables representing run-time variables which have\nalready been shallow trailed, and the second component contains the set of program\nvariables representing run-time variables which have already been deep trailed.\nIn the following we will use l1 , l2 , . . . to denote elements of Lnotrail at program\npoints 1, 2, . . ., and s1 , s2 , . . . and d1 , d2 , . . . for the already shallow and deep trailed\ncomponents of the corresponding elements. Also, the elements of the domain will be\nreferred to as descriptions, with descriptions before and after a goal being referred\nto as the pre- and post-descriptions, respectively.\nNote that, by definition, we can state that if a run-time variable has already been\ndeep trailed, then it has also been shallow trailed (i.e., if all cells in the chain have\nalready been trailed, then the cell associated to the variable has also been trailed).\nThe partial ordering relation \u2291 on Lnotrail is thus defined as follows:\n\u2200(s1p , d1p ), (s2p , d2p )\n\n\u2208 Lnotrail :\n\n(s1p , d1p )\n\n\u2291\n\n(s2p , d2p )\n\n\u21d4\n\n\u001a\n\ns2p\nd2p\n\n\u2286\n\u2286\n\nd1p\nd1p\n\n\u222a\n\ns1p\n\nThis implies that deep trailing is stronger information than shallow trailing, and\nshallow trailing is stronger than no trailing at all. Also note that descriptions are\ncompared at the same program point only (so that the instantiation and sharing\ninformation is identical). An example of a trailing lattice is shown in Fig. 4. Clearly\n11\n\n\f(\u2205, \u2205) SS\nSSSS\nkk\nkkkk\n({Y }, \u2205)\n({X}, \u2205)\nSSSS\nRRRR\nkkkk\nllll\n(\u2205, {Y })\n({X, Y }, \u2205)\n(\u2205, {X})\nRRRR\nSSSS\nkkkk\nllll\n({Y }, {X})\n\n({X}, {Y })\n\nSSSS\n\nkkkk\n\n(\u2205, {X, Y })\n\nFig. 4. Notrail lattice Hasse diagram for variables {X, Y } where if l1 \u2291 l2 then l1\nis below l2 in the diagram\n(Lnotrail , \u2291) is a complete lattice with top description \u22a4p = (\u2205, \u2205) and bottom\ndescription \u22a5p = (\u2205, Oldp ).\nThere are two important points that need to be taken into account when considering the above domain. The first point is that the dp component of a description will\nbe used not only to represent already deep trailed variables but any variable in Oldp\nwhich, for whatever reason (e.g., it has been initialized since the last choicepoint),\ndoes not need to have any part of it trailed\nThe second point is that as soon as a deeply trailed program variable X is made\nto share with a shallow trailed program variable Y , X also must become shallow\ntrailed since some cell in some newly merged chain might come from Y and thus\nmight not have been trailed. The sharing information at each program point is used\nto define the following function which makes trailing information consistent with\nits associated sharing information:\nconsistp ((s, d)) = (s \u222a x, d \\ x)\nwhere\nx = {X \u2208 d|(sharep (X) \\ d) 6= \u2205}\nIntuitively, the function eliminates from d every program variable X which shares\nwith other variables not in d, and adds them to s. From now on we will assume that\n\u2200(s, d) \u2208 Lnotrail : consistp ((s, d)) = (s, d) and use the consist function to preserve\nthis property.2\nGiven HAL's implementation of the sharing analysis domain ASub (S\u00f8ndergaard 1986)\nthe time complexity of determining sharep (X) for a variable X is O(n2 ). Furthermore, since ASub explicitly carries the set of ground variables at each program point\n(gp ), we will use this set rather than computing a new one (Groundp ) from the instantiation information, thus increasing efficiency. The major cost of consistp is the\ncomputation of x: for each of the O(n) variables the sharep set has to be computed.\nAll other set operations are negligible in comparison. Hence, the overall time complexity is O(n3 ). We will see that the complexity of this function determines the\n2\n\nNote that the notrail domain can be seen as a \"product domain\" that also includes the mode\nand sharing information. However, for simplicity, we will consider the different elements separately, relating them only via their associated program point.\n\n12\n\n\fcomplexity of all the operations that use it. Thus, we will use it only when strictly\nnecessary.\nIn summary, each element lp = (sp , dp ) in our domain can be interpreted as\nfollows. Consider a program variable X. If X \u2208 dp , this means that all cells in\nall chains represented by X have already been trailed (if needed). Therefore, X\ndoes not need to be trailed in any unification for which lp is a pre-description.\nNote that X could be a bound variable which includes many different variable\nchains. If X \u2208 sp we have two possibilities. If X is known to be unbound, then its\nassociated cell has been shallow trailed. Therefore, it does not need to be trailed\nin any unification for which lp is a pre-description (although, in practice, we will\nonly consider optimizing variable-variable unifications). If X might be bound, then\na cell of one of its chains might not be trailed. As a result, no optimization can be\nperformed in this case.\nWe could, of course, represent bound variables more accurately, by requiring the\ndomain to keep track of the different chains contained in the structures to which\nthe program variables are bound, their individual trailing state and how these are\naffected by the different program constructs. Known techniques (see for instance\n(Janssens and Bruynooghe 1993; Van Hentenryck et al. 1995; Mulkers et al. 1994;\nLagoon and Stuckey 2001)) based on type information could be used to keep track\nof the constructor that a variable is bound to and of the trailing state of the different\narguments, thereby making this approach possible.\n5 Analyzing HAL Body Constructs with Lnotrail\nThis section defines the notrail operations required by HAL's analysis framework (Bueno et al. 2001; Nethercote 2001) to analyze the different body constructs.\nThis framework is quite similar to the well known framework of (Bruynooghe 1991)\nwhen analyzing a single module. While the analysis framework handles analysis\nof multiple module programs, it makes no extra demands on the analysis domain.\nThus, for this paper we will simply treat the program to be analyzed as a single\nmodule. For each body construct in HAL, we will show how to obtain the postdescription from the information contained in the pre-description.\nVariable initialization init(X)\nIn HAL a variable X transits from its initial instantiation new to instantiation old\nby being initialized. Since a new variable does not need to be trailed, we can simply\nadd X to the d component of the pre-description (recall that d not only represents\nalready deep trailed variables, but also any other old variable which does not need\nto be trailed). Formally, let l1 = (s1 , d1 ) be the pre-description, the post-description\nl2 can be obtained as:\nl2 = (s1 , d1 \u222a {X})\nVariable\u2013variable unification: X = Y . There are several cases to consider:\n\u2022 If one of the variables (say X) is new, it will simply be assigned a copy of\n13\n\n\f\u2022\n\u2022\n\n\u2022\n\n\u2022\n\nthe pointer of Y . After the unification is performed, the trailing state of X\nbecomes that of Y . Thus, the trailing state of X in the post-description should\nbe that of Y in the pre-description. Note that this will never require a call to\nconsist since a new variable cannot introduce any sharing.\nIf one of the variables is ground, the other one will be ground after the unification. Hence, neither of them will appear in the post-description.\nIf both variables are deep trailed, all cells in their associated chains are trailed\nand will remain trailed after unification (which is obtained by simply merging\nthe chains). Hence, all variables retain their current trailing state and the\npre-description will remain unchanged.\nIf both variables are already aliased (they belong to the same chain) nothing is\ndone by unification. Hence, they will retain the current trailing state. Hence,\nall variables retain their current trailing state and the pre-description will\nremain unchanged.\nOtherwise, at least one of the variables is not deep trailed and two unaliased\nvariables are being considered. If both variables are unbound, unification will\nmerge both chains while at the same time performing shallow trailing if necessary. Thus, after the unification both variables will be shallow trailed. If\nat least one variable is bound, the other one will become bound after the\nunification. As stated earlier, bound variables can be treated in the same\nway.\nNote that if either variable was deep trailed before the unification, all shared\nvariables must become shallow trailed as well after the unification. This requires applying the consist function.\n\nFormally, let l1 = (s1 , d1 ) be the pre-description and g2 be the set of ground\nvariables at program point 2 after the unification. Its post-description l2 can be\nobtained as:\n\uf8f1\nsame(X, Y, l1 )\nX is new\n\uf8f4\n\uf8f4\n\uf8f2\nremove ground(l1 , g2 ) X is ground\nl2 = unify(X, Y ) =\n\uf8f4\nmin(X, Y, l1 )\nX and Y are old\n\uf8f4\n\uf8f3\nunify(Y, X)\notherwise\nwith\n\n\uf8f1\n\uf8f2 (s1 \u222a {X}, d1 ) Y \u2208 s1\nsame(X, Y, (s1 , d1 )) =\n(s , d \u222a {X}) Y \u2208 d1\n\uf8f3 1 1\n(s1 , d1 )\notherwise\nremove ground(li , vi ) = (si \\ vi , di \\ vi )\n\uf8f1\n{X, Y } \u2286 d1\n\uf8f2 (s1 , d1 )\nmin(X, Y, (s1 , d1 )) =\nconsist2 ((s1 \u222a {X, Y }, d1 \\ {X, Y })) X 6\u2208 share1 (Y )\n\uf8f3\n(s1 , d1 )\notherwise\n\nHere same(X, Y, li ) gives X the same trailing state as Y , remove ground(li , vi )\nremoves all variables in vi from li , and min(X, Y, li ) distinguished between three\ncases. If X and Y are both deep trailed, nothing has to be changed. If X and Y\nare definitely not aliased (they do not share) it ensures that they move to a shallow\n14\n\n\fX\nf/1\n\nX\nf/1\n\n(a) Before.\n\n(b) After.\n\nFig. 5. Term construction example: f (X). The dashed line represents a choicepoint.\ntrailed state. Otherwise, the description must remain unchanged since unification\nmight have done nothing (and thus they might still be untrailed, so adding them\nto s1 would be a mistake). Note that there is no need to apply consist here since\nX and Y already share in the pre-description and, although sharing information\nmight have changed, it can only create sharing among variables already connected\n(through X and Y ) by the closure under union performed by consist.\nThe worst case time complexity, O(n3 ), is again due to consist.\nVariable\u2013term unification: Y = f (X1 , . . . , Xn ). There are two cases to consider: If\nY is new, the unification simply constructs the term in Y . Otherwise, we can treat\nthis for the purpose of the analysis as two unifications, Y \u2032 = f (X1 , . . . , Xn ), Y = Y \u2032\nwhere Y \u2032 is a new variable. Since unifications of the form Y \u2032 = Y have been\ndiscussed above, here we only focus on the construction into a new variable. In the\nfollowing we assume that the Y in the variable-term unification is new.\nWhen a term, e.g. f (X), is constructed with X being represented by a PARMA\nchain, the argument cell in the structure representation of f /1 is inserted in the\nchain of X (see Fig.5). While X requires shallow trailing, the cell of the term\nrequires no trailing at all as it is newly created.\nThe generalization of this to an n-ary variable term unification is as follows. If\nall arguments are deep trailed, then Y becomes deep trailed and the arguments\nremain deep trailed. Otherwise, Y and all its arguments become shallow trailed\n(since each argument is at least shallow trailed by the operation). Note that if at\nleast one argument was deep trailed, and since each argument shares with Y after\nthe unification, we must apply consist to maintain the information consistent.\nFormally, let l1 = (s1 , d1 ) be the pre-description of the unification, x be the set\nof variables {X1 , . . . , Xn } and g2 the set of ground variables after the unification.\nIts post-description l2 can be obtained as:\n\u001a\n(s1 , d1 \u222a {Y })\nx \u2286 d1\nl2 =\nconsist2 (remove ground((s1 \u222a x \u222a {Y }, d1 \\ x), g2 )) otherwise\nThe worst case time complexity is O(n3 ). This definition can be combined with\nthe previous one for the overall definition of variable\u2013term unification. The implementation can be more efficient, but the complexity will still be O(n3 ).\n15\n\n\fPredicate call: q(X1 . . . Xn ). Let l1 be the pre-description of the predicate call and\nx the set of variables {X1 , . . . , Xn }. The first step will be to project l1 onto x\nresulting in description lproj . Note that onto-projection is trivially defined as:\nonto proj(l, v) = (s \u2229 v, d \u2229 v)\nThe second step consists in extending lproj onto the set of variables local to the\npredicate call. Since these variables are known to be new (and thus they do not\nappear in Old1 ), the extension operation in our domain is trivially defined as the\nidentity. Thus, from now on we will simply disregard the extension steps required\nby HAL's framework.\nLet lanswer be the answer description resulting from analyzing the predicate's\ndefinition for calling description lproj . We will assume that the set v of variables\nlocal to q/n has already been projected out from lanswer , where out-projection is\nidentical to remove ground, which has time complexity O(n).\nIn order to obtain the post-description, we will make use of the determinism\ninformation. Thus, the post-description l2 can be derived by combining the lanswer\nand l1 , using the determinism of the predicate call as follows:\n\u2022 If the predicate has determinism multi or nondet (which can have more than\none answer), then all variables not in x become not trailed by the (possible)\nintroduction of a new choice point. Hence, l2 is equal to lanswer except for the\nfact that we have to apply the consist function in order to take into account\nthe changes in sharing involving variables not in x.\n\u2022 Otherwise, we know the trailing state of variables in l1 is unchanged except by\npossibly new introduced sharing. Thus, l2 is the result of combining lanswer\nand l1 as follows: the trailing state of variables in x is taken from lanswer ,\nwhile that of other variables is taken from l1 . Any deep trailed variables that\nshare with non-deep trailed variables must, of course, become shallow trailed.\nFormalized, the combination3 function is defined as:\nl2\n\n=\n=\n\ncomb(l1 , lanswer )\n\u001a\nconsist2 (((s1 \\ x) \u222a sanswer , (d1 \\ x) \u222a danswer ))\nconsist2 (lanswer )\n\ndet(q) \u2264 1\notherwise\n\nObviously, the complexity is O(n3 ) because of consist.\nExample 6\nAssume that the call q(X) has pre-description ({X, Y }, \u2205) and the predicate q/1\nhas answer description ({X}, \u2205). The post-description of the call depends on the\ndeterminism of the predicate. If the predicate q/1 has at most one solution, the postdescription will be (({X, Y } \\ {X}) \u222a {X}, (\u2205 \\ {X}) \u222a \u2205) = ({X, Y }, \u2205). Otherwise\nthe post-description will be equal to the answer description, ({X}, \u2205).\n3\n\nNote that the combination is not the meet of the two descriptions. It is the \"specialized combination\" introduced in (de la Banda et al. 1998) which assumes that lanswer contains the most\naccurate information about the variables in x, the role of the combination being just to propagate this information to the rest of variables in the clause.\n\n16\n\n\fDisjunction: (G1 ; G2 ; . . . ; Gn ). Disjunction is the reason why trailing becomes necessary. As mentioned before, trailing might be needed for all variables which were\nalready old before the disjunction. Thus, let l0 be the pre-description of the entire\ndisjunction. Then, \u22a4 will be the pre-description of each Gi except for Gn whose\npre-description is simply l0 (since the disjunction implies no backtracking over the\nlast branch).\nLet li = (si , di ), 1 \u2264 i \u2264 n be the post-description of goal Gi . We will again\nassume that the set vi of variables local to each Gi has already been projected out\nfrom li . The end result ln+1 of the disjunction is the least upper bound (lub) of all\nbranches,4 which is defined as:\nl1 \u2294 . . . \u2294 ln = consistn+1 (remove ground((s, d), gn+1 ))\nwhere\ns\nd\ns\u2032i\nd\u2032i\n\n=\n=\n=\n=\n\n(s\u20321 \u2229 . . . \u2229 s\u2032n ) \\ d\n(d\u20321 \u2229 . . . \u2229 d\u2032n )\nsi \u222a d\u2032i\ndi \u222a gi\n\nIntuitively, all variables which are deep trailed in all descriptions are ensured\nto remain deep trailed; all variables which are trailed in all descriptions but have\nnot always been deep trailed (i.e., are not in d) are ensured to have already been\n(at least) shallow trailed. Note that variables which are known to be ground in all\ndescriptions (those in gn+1 ) are eliminated. This is consistent with the view that\nonly old variables are represented by the descriptions and avoids adding overhead\nto the abstract operations.\nHAL also includes switches, which are disjunctions where the compiler has detected that only one branch needs to be executed. Switches are treated identically\nto disjunctions except for the fact that the pre-description for each Gi is l0 rather\nthan \u22a4.\nExample 7\nLet l0 = (\u2205, {X, Y, Z}) be the pre-description of the code fragment:\n( A = a, X = Y ; A = b, X = f(Y, Z) )\nLet us assume there is no sharing at that program point. Assuming that A is old,\nthen this is simply a disjunction. Then, the pre-descriptions of the first branch is\n(\u2205, \u2205), the \u22a4 element of our domain. The pre-description of the second branch is\n(\u2205, {X, Y, Z}), i.e., since this is the last branch in the disjunction, its pre-description\nis identical to the pre-description of the entire disjunction. Their post-descriptions\nare ({X, Y }, \u2205) and (\u2205, {X, Y, Z}), respectively. Finally, the lub of the two postdescriptions results in ({X, Y }, \u2205).\nNow assume A is ground. Then this code fragment is a switch on A. The predescription for the first branch becomes (\u2205, {X, Y, Z}) and the post description\n4\n\nNote that this is not the lub of the notrail domain alone, but that of the product domain\nwhich includes sharing (and groundness) information.\n\n17\n\n\fis the same. Finally the lub of the two post-descriptions for the two branches is\n(\u2205, {X, Y, Z}).\nThe time complexity of the joining of the branches is simply that of the lub\noperator (O(n3 )) for a fixed maximum number of branches, and it is completely\ndominated by the consistn+1 function.\nIf-then-else: I \u2192 T ; E . Although the if-then-else could be treated as (I, T ; E),\nthis is rather inaccurate since (as in the case of switches) only one branch will ever\nbe executed and, thus, there is no backtracking between the two branches.\nHence, we can do better if no old variable that exists before the if-then-else is\nbound or aliased, i.e. possibly requiring trailing and backtracking if the condition\nfails. This is not a harsh restriction, since it is ensured whenever the if-condition\nis used in a logical way, i.e., it simply inspects existing variables and does not\nchange any non-local variable. However, in general it is not possible to statically\ndetermine this property. Instead a safe approximation is used: the if-then-else is\ntreated as (I, T ; E) if the condition contains any pre-existing old variables, otherwise\nthe following stronger treatment is used.\nLet l1 be the pre-description to the if-then-else. Then l1 will also be the predescription to both I and E. Let lI be the post-description obtained for I. Then lI\nwill also be the pre-description of T . Finally, let lT and lE be the post-descriptions\nobtained for T and E, respectively. Then, the post-description for the if-then-else\ncan be obtained as the lub lT \u2294 lE .\nThe time complexity of the joining of the branches is again O(n3 ), just like the\noperation over the disjunction.\nExample 8\nLet l0 = (\u2205, \u2205) be the pre-description of the following if-then-else where N is known\nto be ground:\n( N = 1 -> X = Y\n\n; X = f(Y, Z) )\n\nAssume no variables share before the if-then-else. Then, l0 is equal to the predescription\nof\nboth\nthe\nthenand\nelse-branch.\nThe\npost-description of the then-branch is ({X, Y }, \u2205) and that of the else-branch is\n({X, Y, Z}, \u2205). The post-description finally is obtained as their lub: ({X, Y }, \u2205).\nIf the pre-description was l0 = (\u2205, {X, Y, Z}) as in Example 7, then the postdescription would be (\u2205, {X, Y, Z}), since no additional trailing will be required.\nHigher-order term construction: Y = p(X1 , . . . , Xn ). This involves the creation of\na partially evaluated predicate, i.e., we are assuming there is a predicate with name\np and arity equal or higher than n for which the higher-order construct Y is being\ncreated. In HAL, Y is required to be new. Also, it is often too difficult or even\nimpossible to know whether Y will be actually called or not and, if so, where.\nThus, HAL follows a conservative approach and requires that the instantiation of\nthe \"captured\" arguments (i.e., X1 , . . . , Xn ) remain unchanged after calling Y . It\n18\n\n\falso guarantees (through type and mode checking) that no higher-order terms are\never unified.\nThe above requirements allow us to follow a simple (although conservative) approach: Only after a call to Y will the trailing of the captured variables be affected.\nIf the call to Y might have more than one solution and thus may involve backtracking, then the involved variables will be treated safely in the analysis at the\ncall location if they are still statically live there.\nIf the call to Y does not involve backtracking but does involve unifications, then\ntrailing information might not be inferred correctly at the call location. This is\nbecause the captured variables are generally not known at the call location. To\nkeep the trailing information safe, any potential unifications have to be accounted\nfor in the higher-order unification. Since the construction of the higher-order term\ninvolves no backtracking and all unifications leave the variables they involve at\nleast shallow trailed, it is sufficient to demote all captured deep trailed variables to\nshallow trailed status, together with all sharing deep trailed variables.\nFormally, let l1 = (s1 , d1 ) be the pre-description of the higher-order term construction and x be the set of variables {X1 , . . . , Xn }. Then its post-description l2\ncan be obtained with a time complexity of O(n3 ) as:\n\u001a\nconsist2 ((s1 \u222a (x \u2229 d1 ), d1 \\ x)) x \u2229 d1 6= \u2205\nl2 =\nl1\notherwise\nHigher-order call: call(P, X1 , . . . , Xn ). The exact impact of a higher-order call is\ndifficult to determine in general. Fortunately, even if the exact predicate associated\nto variable P is unknown, the HAL compiler still knows its determinism. This can\nhelp us improve accuracy. If the predicate might have more than one solution, all\nvariables must become not trailed. Since the called predicate is typically unknown,\nno answer description is available to improve accuracy.\nOtherwise, the worst that can happen is that the deep trailed arguments of the\ncall become shallow trailed. So in the post-description we move all deep trailed\narguments to the set of shallow trailed variables, together with all variables they\nshare with. Recall that for this case the captured variables have already been taken\ncare of when constructing the higher-order term.\nThe sequence of steps is much the same as that for the predicate call. First, we\nproject the pre-description l1 onto the set x of variables {X1 , . . . , Xn }, resulting in\nlproj . Next, the answer description lanswer of the higher-order call is computed as\nindicated above:\n\u001a\n(s \u222a d, \u2205) det(P ) \u2264 1\nlanswer =\n(\u2205, \u2205)\notherwise\nThe combination of lanswer and l1 is computed to obtain the post-description l2 .\n\n6 Trailing Optimization\nThe optimization phase consists of deciding for each unification in the body of\na clause which variables need to be trailed. This decision is based on the pre19\n\n\fdescription of the unification, inferred by the trailing analysis. If some variables do\nnot need to be trailed, the general unification predicate is replaced with a variant\nthat does not trail those particular variables. Thus, we will need a different variant\nfor each possible combination of variables that do and do not need to be trailed.\n\u2022 For the unification of two unbound variables, trailing is omitted for either\nvariable if it is shallow trailed or deep trailed in the pre-description.\n\u2022 For the binding of an unbound variable X, trailing of X is omitted if it is\ndeep trailed in the pre-description.\n\u2022 In the construction of a term containing an old unbound variable X, trailing\nof X is omitted if X is either shallow or deep trailed in the pre-description.\n\u2022 For the unification of two bound variables, the trailing for chains in the structure of either is omitted if it is deep trailed in the pre-description.\nOften it is not known at compile time whether a variable is bound or not, so a\ngeneral variable-variable unification predicate is required that performs run-time\nboundness tests before selecting the appropriate kind of unification. Various optimized variants of this general predicate are needed as well.\nExperimental results for the analysis are presented in Section 9.\n7 The improved trailing scheme\nLet us now present a trailing scheme which is more sophisticated than the classic\nPARMA value trailing discussed in Section 2. We will start by considering the\nimprovements that apply to each kind of unification (variable\u2013variable and variable\u2013\nnonvariable) and finish by showing how to combine them.\nOur modified scheme must be able to apply different untrail operations depending\non the kinds of trailing that was performed. A simple tagging scheme (explained\nin detail in Section 7.3) is used to indicate the kind of untrailing required in each\ncase.\n7.1 Variable\u2013variable unification: swap trailing\nIn the classic scheme the value trailing of both cells takes up four trail stack slots\n(two for the addresses of each variable plus another two for their contents) when\ntrailing is unconditional. Undoing such variable\u2013variable unification consists of simply restoring the old values of the cells separately. However, there is a more economic\ninverse operation that undoes the swapping that happened during unification: simply swapping back. This swapping only requires the addresses of the involved cells\nand not their respective old contents. We introduce a new kind of trailing named\nswap trailing which exploits this and also the corresponding untrailing operation.\nSwap trailing is defined by the following code:\nswaptrail(p, q) {\n*(tr++) = p;\n*(tr++) = set_tag(q,SWAP_TRAIL);\n}\n20\n\n\fwhere p and q are the addresses of the two cells, tr is a pointer to the top of\nthe trailing stack, SWAP TRAIL is a tag, and the function set tag(c,t) tags cell c\nwith tag t. Note that swap trailing only consumes two slots in the trail stack, as\nopposed to the four used by (unconditional) value trailing in the classical scheme.\nThe untrail operation for swap trailing is:\nuntrail_swaptrail() {\nq = untag(*(--tr)); /* recover address q */\np = *(--tr);\n/* recover address p */\ntmp = *q;\n*q = *p;\n/* swap contents of p with q */\n*p = tmp;\n}\nThe above improvement assumes that both cells are unconditionally trailed. If conditional value trailing is available, the classic scheme would either consume zero,\ntwo or four slots if respectively none, only one or both variables are older than the\nmost recent choice point. Swap trailing can only be used in conjunction with conditional trailing to replace the four slot case, with value trailing still needed for the\ntwo slot case. As a result the code for conditional variable\u2013variable trailing looks\nlike:\ncond_varvartrail(p, q, bh) {\nif (is_older(p,bh)) {\nif (is_older(q,bh)) {\nswaptrail(p,q); /* trail both using swaptrail */\n} else {\nvaluetrail(p); /* only trail p */\n}\n} else if (is_older(q,bh)) {\nvaluetrail(q);\n/* only trail q */\n}\n}\nIt is important to note that the potential gain in space on the trail obtained by the\nabove operations comes at a cost in execution time (more run-time operations are\nneeded) and that the gain in space is not guaranteed.\n7.2 Variable\u2013nonvariable unification: chain trailing\nAs seen before, variable-nonvariable unification pulls the entire chain of the variable\napart by setting every cell in the chain to the nonvariable. In the case of classic\nvalue trailing, every address of a cell is stored twice: once as the address of a cell\nand once as the contents of the predecessor cell. This means that there is quite\nsome redundancy. The obvious improvement is to store each address only once. We\nname this chain trailing. Because the length of the chain is not known, a marker is\nneeded to indicate, for the untrailing operation, where chain trailing ends. The last\n21\n\n\fentry of the chain encountered during untrailing, is the first one actually trailed.\nWe use the CHAIN END tag to mark this entry.\nThe last address put on the trail is tagged with CHAIN BEGIN to indicate the kind\nof trailing. For chains of length one, the last and first cell coincide. The CHAIN END\ntag is used to mark this single address.\nChain trailing is defined by the code:\nchaintrail(p) {\nstart = p;\n*(tr++) = set_tag(p,CHAIN_END);\np = *p;\nonly_one = TRUE;\nwhile (p != start) {\n/*trail each cell address*/\nonly_one = FALSE;\n*(tr++) = p;\np = *p;\n}\nif (!only_one) {\n/* if more than one cell */\nlast = tr - 1;\n/* tag last one as CHAIN_BEGIN*/\n*last = set_tag(*last,CHAIN_BEGIN);\n}\n}\nThe untrail operation for reconstructing the chain is straightforward: it dispatches\nto the appropriate untrailing action depending on the tag of the first cell encountered during untrailing. If this is CHAIN BEGIN, meaning n \u2265 1, the corresponding\ncode is:\nuntrail_chaintrail() {\nhead = untag(*(--tr));\nprevious = head;\ncurrent = *(--tr);\nwhile (get_tag(current) != CHAIN_END) {\n*current = previous;\nprevious = current;\ncurrent = *(--tr);\n}\ncurrent = untag(current);\n*current = previous;\n*head = current;\n}\nIf the first tag is CHAIN END, then n = 1 and the code for untrailing is:\nuntrail_shortchain() {\ncell = untag(*(--tr));\n*cell = cell;\n}\n22\n\n\fExample 9\nConsider the trailing that occurs using the improved scheme for the goal X = Y, Z\n= W, X = Z, X = a from Example 4. The first unification is a swaptrail, trailing X\nand Y, similarly the second unification swaptrails Z and W and the third unification\nswap trails X and Z. Finally the last unification chain trails X. The resulting trail\nlooks like:\nX\n\nY\n\nsw\n\nZ\n\nW\n\nsw\n\nX\n\nZ\n\nsw\n\nX\n\nce\n\nW\n\nZ\n\nY\n\ncb\n\nwhere we use superscripts sw, cb and ce to represent the SWAP TRAIL, CHAIN BEGIN\nand CHAIN END tags respectively. This uses 10 trail entries compared to the 24\nentries in Examples 4 and 5\nThe above improvement assumes that all cells are unconditionally trailed. Let us\nassume that the chain consists of n cells, k of which are older than the most recent\nchoice point. If conditional trailing is available and 2 \u2217 k < n, our unconditional\nchain trailing will consume more space than the classic conditional value trailing.\nFortunately, a conditional variant of chain trailing is also possible:\ncond_chaintrail(p, bh) {\nstart = p;\nfirst = TRUE;\nonly_one = TRUE;\ndo {\nif (is_older(p,bh))\n/* trail each older cell in chain*/\nif (first) {\n*(tr++) = set_tag(p,CHAIN_END); /*tag if first*/\nfirst = FALSE;\n} else {\nonly_one = FALSE;\n*(tr++) = p;\n}\np = *p;\n} while (p != start);\nif (!only_one) {\n/* if more than one older cell */\nlast = tr - 1;\n/* tag last one as CHAIN_BEGIN*/\n*last = set_tag(*last,CHAIN_BEGIN);\n}\n}\nThis conditional variant uses only k slots of the stack trail, so it is clearly an\nimprovement over conditional value trailing whenever k > 0.\nNote that the untrail operation used is the same as for the unconditional chain\ntrailing. This might look wrong at first since the cond chaintrail might not trail\nall cells in the chain. However, this is simply exploiting the fact that the objective\nof trailing is to be able to reconstruct the bindings that existed at the creation time\nof a choice point. Thus, the final state of younger cells and the state of any cell\n23\n\n\fduring the intermediate steps of untrailing are irrelevant. In fact, the more general\n\u2013 and better with respect to stack trail consumption \u2013 principle behind this is that\nonly the old cells (older than the most recent choice point) in the chain pointing\nto other old cells have to be trailed (an old cell must have been made to point to a\nnew cell after the last choice-point). The kind of trailing suitable for this insight is a\nspecial kind of value trailing, where the successive equal slots on the trail stack are\noverlapped. The above cond chaintrail operation only approximates this, since\nan implementation would incur an undue time overhead because of the extra runtime tests needed to test the age of the successors. Thus, we store the addresses of\nold cells even if they neither point to nor are pointed to by old cells.\nExample 10\nFigure 6 illustrates with a small example how the above specified conditional chain\ntrailing, together with previous trailings, safely restores the state of all variables\nolder than the most recent choice point. Consider the following goal X = Z, Z =\nY, X = a, fail and let us assume that both X and Y are older than the most\nrecent choice point, Z is newer, and all three are chains of length 1 as depicted\nin Figure 6(a). The successive forward steps are shown in the Figures 6(b), 6(c)\nand 6(d). X is value trailed during X = Z, as is Y during Z = Y. The addresses of X\nand Y are stored on the trail stack with conditional chain trailing during X = a5 .\nThe cb and ce to the side of the stack trail entries represent the CHAIN BEGIN and\nCHAIN END tags respectively.\nThe execution fails immediately after X = a, and backtracks to the initial state in\nthree steps. First (Figure 6(e)), the conditional chain trailing is untrailed, creating\na chain of X and Y. Next (Figure 6(f)), the value trailing of Y is undone and finally\n(Figure 6(g)), the value trailing of X is reversed too. The final state corresponds\nto the initial state, except for Z, which is still bound to a. However, as Z did\nnot exist before the most recent choice point, its content is irrelevant at that point\nbecause it is inaccessible and will be reclaimed from the heap anyway when forward\nexecution resumes. Note the illegal intermediate state illustrated in Figure 6(f) is\nnot important since it only occurs in the middle of untrailing, and never during\nexecution.\n\n7.3 Combining the improvements\nLet us first consider the combination in the context of the modified unconditional\ntrailing scheme of the Mercury back-end of HAL. In this context, in addition to\nswap and unconditional chain trailing, function trailing is used to allow custom\ntrailings for constraint solvers. Function trailing stores a pointer to an untrailing\nfunction and to untrailing data. Thus, we need four different tags to distinguish\nthe different trailing information that can appear on the trail. Fortunately, there\nare two tag bits available (because of the aligned addressing for 32 bit machines).\nThere is one constraint on the allocation of the four different tags to the kinds of\n5\n\nThis could be avoided if X is known to have been trailed already.\n\n24\n\n\fX\n\nY\n\nX\n\nZ\n\nY\n\nX\nX\nX\n\nZ\n\n(a) Initially\n\nX\nZ\n\na\n\nY\nY\nX\nX\n\n(e) Untrail X = a\n\nY\nZ\n\n(b) X = Z\n\nY\n\nX\n\nY\na\n\n(f) Untrail Z = Y\n\na\n\nY\n\na\n\nY\nX\nY\nY\nX\nX\n\ncb\nce\n\n(d) X = a\n\nX\nX\nX\n\na\nZ\n\n(c) Z = Y\n\nX\nZ\n\nY\nY\nX\nX\n\nY\nZ\n\na\n\n(g) Untrail X = Z\n\nFig. 6. Conditional chain trailing example.\ntrailing: the CHAIN END tag should not look the same as the tag of the intermediate\naddresses in a chain trail.\nThe general untrail operation then simply looks like:\nuntrail(tr_cp) {\nwhile (tr > tr_cp) {\nswitch (get_tag(*tr)) {\ncase FUNCTION_TRAIL:\nuntrail_functiontrail();\nbreak;\ncase SWAP_TRAIL:\nuntrail_swaptrail();\nbreak;\ncase CHAIN_BEGIN:\nuntrail_chaintrail();\nbreak;\ncase CHAIN_END:\nuntrail_shortchain();\n}\n}\n}\nNote that, since we are assuming we are in a modified unconditional trailing scheme,\nvalue trailing is never used. This is because value trailing is only needed in the\nmodified scheme whenever only one of the two variables involved in a variablevariable unification is newer than the most recent choice point, and thus only that\n25\n\n\fone was trailed. Otherwise swap trailing will be used. Since no conditional trailing\nis allowed, swap trailing is always used for variable-variable unifications.\nLet us now consider the combination in the context of the modified conditional\ntrailing scheme of dProlog. In this context only value, swap and conditional chain\ntrailing are used. The remarks on the application and allocation of tags is the\nsame as for the unconditional case and the general conditional untrail operation\nlooks identical except for the fact that the FUNCTION TRAIL case is substituted by\na VALUE TRAIL case, and the call to untrail functiontrail() is substituted by a\ncall to untrail valuetrail().\nWhen looking at the value trailings of chains of length one in the example in\nthe previous section (see Figure 6), there is an obvious trailing alternative in the\nconditional system that stores no redundant information: chain trailing. Indeed, if\nsuch a variable would be chain trailed instead of value trailed, only one instead of\ntwo slots would be used on the stack. However, this would require more run-time\ntests and we have not implemented this.\nvalue_trail(p) {\nif (*p == p) /* self pointer */\n*(tr++) = set_tag(p,CHAIN_END);\nelse\n*(tr++) = *p;\n*(tr++) = p;\n}\nExperimental results for both the conditional and unconditional trailing scheme\nare presented in Section 9.\n8 Analysis for the improved trailing scheme\nTrailing analyses heavily depend on the details of the trailing scheme. The analysis\npresented in Section 4 was defined for the classic PARMA trailing scheme. In this\nsection we present the modifications needed by that analysis in order to be applied\nto our improved trailing scheme. As we will see, the improved scheme gives rise to\nfewer opportunities for trail savings.\n8.1 Unnecessary trailing in the improved trailing scheme\nThe main difference between the two schemes in terms of unnecessary trailing appears when considering cells that have been trailed since the most recent choicepoint. In the case of value- and chain-trailing, these cells do not need to be trailed\nagain since the information stored the first time allows us to reconstruct the state\nright before the choice-point.6 As we will see later in the experimental evaluation,\nthis allows our previous analysis to detect many spurious trailings.\n6\n\nThis is assuming that the semantics of function trailing is such that it does not rely on the\nintermediate state of any Herbrand variable during untrailing.\n\n26\n\n\fIn the case of swap trailing, however, cells need to be trailed even if they have\nalready been trailed since the most recent choice-point. This is because swap trailing\nis an incremental kind of trailing (the content of the cells is not stored during\nthe trailing, but only the incremental change) and thus relies on future trailings\nfor proper untrailing of cells. As a result, during the untrailing process in our\nimproved scheme, all later chain and swap trailings have to be undone before the\nswap trailing can be untrailed correctly. Thus, there is no opportunity here to\navoid future trailings between two choice points, after the first trailing has been\nperformed. Let us illustrate this with a counterexample.\nCounterexample. Let us not trail variables a second time between two choice points.\nConsider then the following code:\nX = Y, Z = W, X = Z, fail\nwhere all variables are older than the most recent choice point and, initially, they\nare represented as chains of length one, as depicted in Figure 7(a). In the first two\nsteps the four variables are aliased and swap trailed pairwise, creating two chains\nof length two (see Figure 7(b)). The s's represent SWAP TRAIL tags.\n\nX\nZ\n\nY\nW\n\n(a) Initially\n\nW\nZ\nY\nW X\n\nX\n\nY\n\nZ\n\n(b)\nZ = W\n\ns\n\nX\n\ns\n\nZ\n\nW\nZ\nY\nW X\n\nY\n\ns\n\n(c) X = Z\n\nX = Y,\n\nX\n\nY\n\nZ\n\nW\n\ns\n\n(d) Untrail\n\nFig. 7. Counterexample of incremental behavior of swap trailing: it does not eliminate the need for further trailing of the same cells.\nNext X and Z are aliased, creating one large chain (see Figure 7(c)). During\nthis step X and Z are not (swap) trailed since they have already been swap trailed\nafter the most recent choice point (and we are assuming this means trailing is not\nneeded). Finally, the execution fails and untrailing tries to restore the situation at\nthe most recent choice point. However, Figure 7(d) shows that the omission of the\nlast swap trailing was invalid, as untrailing fails to restore the correct situation.\nThus, a cell involved in swap trailing still needs trailing later in the same segment\nof the execution.\n\n8.2 The Ltrail analysis domain\nThe implications for the Ltrail analysis domain are simple: it only needs to distinguish between variables that do not have to be trailed again (deep trailed) and those\nwhich have to (rest). In other words, variables can only have one of two possible\n27\n\n\fstates at a particular program point: deep trailed or not trailed at all. Hence, the\ntype of elements of our Ltrail domain will be P(Oldp ). The ordering \u2291 is simply\n\u2287.\nAll the operations we have defined for the Lnotrail domain have to be adapted to\nthis simplification. This adaptation is rather straightforward: every description l in\nLtrail is treated as if it were the description (\u2205, l) in Lnotrail , and new descriptions l\u2032\nin Ltrail are obtained by first calculating the (s\u2032 , d\u2032 ) descriptions using the Lnotrail\noperations and then setting l\u2032 = d\u2032 .\n8.3 Optimization based on the analysis\nAgain, the pre-description of every unification is used to improve that unification.\nThe possible optimizations based on the Ltrail domain are more limited than those\nfor the Lnotrail domain, as only deep trailed variables are represented in the descriptions:\n\u2022 For the unification of two variables, a variant without (swap) trailing can be\nused if both variables are in the pre-description (i.e. deep-trailed).\n\u2022 For the binding of an unbound variable Y to a term f (X1 , ..., Xn ), a variant\nof the unification without (chain) trailing can be used if Y is in the predescription. In addition, no (swap) trailing is required for any of the Xi that\nappear in the pre-description.\n\u2022 For the unification of two bound variables, if both variables are in the predescription, or if one is in the pre-description and the other is known to be\nground, then no trailing is needed at runtime. This means that if during the\nrecursive unification process of the bound variables, unbound variables are\nunified or bound, nothing will need to be trailed for these unbound variables.\n\n9 Experimental Results\nWe first examine the effect of the trailing analysis Lnotrail and its associated optimizations on the classic PARMA trailing scheme for HAL. We then look at the\neffect of the improved PARMA trailing scheme, and at the effect of the use of the\ntrailing analysis Ltrail on the improved PARMA trailing scheme. Finally, we examine the improved PARMA trailing scheme in the context of dProlog. All timing\nresults were obtained on an Intel Pentium 4 2.00 GHz 512 MB.\n9.1 Effect of trailing analysis using Lnotrail in HAL\nThe Lnotrail analyzer has been implemented in the analysis framework of HAL and\napplied to six HAL benchmarks that use the Herbrand solver: icomp, hanoi, qsort,\nserialize, warplan and zebra. Table 1 gives a summary of these benchmarks. All\nbenchmarks make use of the Herbrand solver and cannot be executed as Mercury\nprograms (without significantly modifying the algorithm and representation).\nThe pre-descriptions inferred for the unifications of these benchmarks have been\n28\n\n\fTable 1. HAL Benchmark descriptions and lines of code\nBenchmark\nicomp\nhanoi\nqsort\nserialize\nwarplan\nzebra\n\nDescription\na cut down version of the interactive BIM compiler\nthe Hanoi puzzle using difference lists\nthe quick sort algorithm using difference lists\nthe classic Prolog palindrome benchmark\nwar planner for robot control\nthe classic five houses puzzle\n\nLines\n294\n31\n43\n74\n316\n82\n\nTable 2. Compilation statistics for notrail analysis\nBenchmark\nicomp\nhanoi\nqsort\nserialize\nwarplan\nzebra\n\nCompilation Time\nAnalysis Total Relative\n1.170 2.110\n55.5 %\n0.030 0.350\n8.6 %\n0.020 0.810\n2.5 %\n0.040 0.430\n9.3 %\n1.080 2.590\n41.7 %\n0.090 0.560\n16.1 %\n\nOld unifications\nImproved Total Relative\n314 1,542\n20.4 %\n13\n13 100.0 %\n7\n7 100.0 %\n1\n20\n5.0 %\n93 1,347\n6.9 %\n40\n177\n22.6 %\n\nSize\nRelative\n120.5 %\n100.0 %\n100.0 %\n100.2 %\n156.2 %\n108.6 %\n\nused to optimize the generated Mercury code by avoiding unnecessary trailing, as\nexplained in Section 6.\nTable 2 shows, for each benchmark, the analysis time in seconds compared to\nthe total compilation time, the number of improved unifications compared to the\ntotal number of unifications involving old variables, and the size of generated binary\nexecutable. The binary size of the optimized program is expressed as the number\nof bytes relative to the unoptimized program.\nThe high compilation times obtained for some benchmarks are due to the existence of predicates with many different pre-descriptions, something the analysis\nhas not been optimized for yet. The deterministic nature of both hanoi and qsort\nbenchmarks, allows the analysis to infer that all unifications should be replaced\nby a non-trailing alternative. In the other benchmarks a much smaller fraction of\nunifications can be improved due to the heavy use of non-deterministic predicates.\nThe last table shows that due to the multi-variant specialization, there may\nbe a considerable size blow-up. In particular, for icomp and warplan the size is\nsubstantially increased. Various approaches to limit the number generated variants,\nexplored in other work, apply to this work as well. For example, one approach is\nto use profiling information to only retain the most performance-critical variants\n(see (Ferreira and Damas 2003)). Another approach, taken in (Mazur 2001), is to\nonly generate the most and least optimized variants. The latter would reproduce\nthe optimal result for hanoi and qsort.\nTable 3 presents the execution times in seconds obtained by executing each benchmark a number of times in a loop; the iteration number in the table gives that loop\n29\n\n\fTable 3. Benchmark timings for classic PARMA: unoptimized (cparma) and optimized with trailing analysis (caparma)\nBenchmark\nicomp\nhanoi\nqsort\nserialize\nwarplan\nzebra\n\nIterations\n10,000\n10\n10,000\n10,000\n10\n200\n\ncparma\n0.834\n0.990\n0.363\n0.901\n1.293\n1.239\n\nTime\ncaparma\n0.790\n0.707\n0.303\n0.884\n1.407\n1.254\n\nrelative\n94.7 %\n71.4 %\n83.5 %\n98.1 %\n108.8 %\n101.2 %\n\nTable 4. Benchmark trail sizes for classic PARMA: unoptimized (cparma) and optimized with trailing analysis (caparma)\nBenchmark\nicomp\nhanoi\nqsort\nserialize\nwarplan\nzebra\n\nMaximum trail\ncparma caparma relative\n5,545\n4,217\n76.1 %\n61,441\n0\n0.0 %\n11,801\n0\n0.0 %\n16,569\n12,657\n76.4 %\n17\n9\n52.9 %\n209\n185\n88.5 %\n\nTrailing operations\ncparma\ncaparma relative\n1,110\n860\n77.5 %\n7,864,300\n0\n0.0 %\n1,510\n0\n0.0 %\n2,120\n1,620\n76.4 %\n102,290\n101,820\n99.5 %\n5,153,800 4,920,600\n95.5 %\n\ncount. This execution process (and the iteration number) is also used to obtain all\nother results shown for these HAL benchmarks.\nThe significant speed-up obtained for both the hanoi and qsort benchmarks\nis explained by the effects of replacing all unifications with a non-trailing version\non the maximum size of the trail stack (in kilobytes), and on the total number of\ntrailing operations, as shown in Table 4. In the non-deterministic benchmarks, a\nmuch smaller fraction of the trailing operations is removed. This results in a smaller\nspeed-up or even a slight slow-down. The slow-down shows that the optimization\ndoes not come without a cost.\nThe larger active code size due to the multi-variant specialization has an impact on the instruction cache behavior. Table 5 shows the impact on instruction\nreferences and instruction cache misses, obtained with the cachegrind skin of the\nvalgrind memory debugger (see (Nethercote and Seward 2003)). The number of instruction references is the number of times an instruction is retrieved from memory\nand the instruction cache miss rate is the percentage of instruction references in\nmain memory instead of cache.\nThe table clearly shows that the elimination of all trailing operations results in a\nconsiderable reduction of executed instructions. On the other side of the spectrum,\nthe multi-variant specialization has a negative effect on the instruction cache miss\nrate, which explains the slow-down of the warplan benchmark.\n30\n\n\fTable 5. Benchmark instruction cache misses for classic PARMA: unoptimized\n(cparma) vs. optimized with trailing analysis (caparma)\nBenchmark\nicomp\nhanoi\nqsort\nserialize\nwarplan\nzebra\n\nI1 instruction cache miss rate\ncparma caparma\nrelative\n0.85 %\n1.79 % 210.6 %\n0.00 %\n0.00 %\n-%\n0.00 %\n0.00 %\n-%\n0.00 %\n0.70 %\n\u221e%\n1.55 %\n4.44 % 286.5 %\n0.40 %\n0.10 %\n25.0 %\n\nInstruction references\ncparma\ncaparma\nrelative\n716 \u00d7106\n709 \u00d7106\n99.0 %\n991 \u00d7106\n839 \u00d7106\n84.7 %\n427 \u00d7106\n397 \u00d7106\n93.0 %\n6\n912 \u00d710\n899 \u00d7106\n98.6 %\n1,559 \u00d7106 1,560 \u00d7106 100.1 %\n1,300 \u00d7106 1,291 \u00d7106\n99.3 %\n\n9.2 Effect of the improved trailing scheme in the Mercury back-end of\nHAL\nThe improved unconditional PARMA trailing scheme has also been implemented in\nthe Mercury back-end of HAL. Since Mercury already has a tagged trail, this was\nnot too difficult. Aside from the discussed trailings for unification, this system also\nrequires trailing when a term is constructed with an old variable as an argument.\nIn this term construction, the argument cell in the term structure is inserted in\nthe variable chain. This modifies one cell in the old variable chain. In the classic\nscheme this cell is trailed with value trailing. To avoid value trailing altogether this\nhas been replaced with swap trailing in the improved trailing scheme.\nTable 6 presents the timing and maximal trail for both the classic and improved\ntrailing scheme for the six HAL benchmarks used before.\nTable 6. Timing and maximal trail for the classic (cparma) and improved (iparma)\nunconditional PARMA trailing scheme for the Mercury back-end of HAL.\nBenchmark\nicomp\nhanoi\nqsort\nserialize\nwarplan\nzebra\n\ncparma\n0.834\n0.990\n0.363\n0.901\n1.293\n1.239\n\nTime\niparma\n0.809\n0.944\n0.350\n0.836\n1.284\n1.171\n\nrelative\n97.0 %\n95.4 %\n96.4 %\n92.8 %\n99.3 %\n94.5 %\n\nMaximal trail\ncparma iparma relative\n5,545\n3,049\n55.0 %\n61,441\n40,961\n66.7 %\n11,801\n7,857\n66.6 %\n16,569\n10,233\n61.8 %\n17\n9\n52.9 %\n209\n105\n50.2 %\n\nIn all benchmarks the improved trailing scheme is faster than the classic scheme.\nThe differences are a few percentages though, with a maximum difference of slightly\nmore than 7% for the serialize benchmark. Much more important are the effects\nof the improved trailing scheme on the maximal trail size. The maximal trail is\nat least 30% and up to 50% smaller for the improved scheme than for the classic\nscheme.\n31\n\n\f9.3 Effect of the improved trailing scheme combined with trailing\nanalysis Ltrail in the Mercury back-end of HAL\nThe trailing analysis presented in Section 4 and implemented in HAL, was modified,\nas proposed in Section 8, to deal with the improved trailing scheme. Table 7 presents\nthe timing and maximal trail for the HAL benchmarks obtained under the improved\nscheme with the information inferred by the modified analysis, and compares the\nresults obtained under the same scheme without any analysis information.\nTable 7. Timing and maximal trail for the improved unconditional PARMA scheme\nwithout (iparma) and with (iaparma) Ltrail trailing analysis, relative to the classic\nscheme without trailing.\nBenchmark\nicomp\nhanoi\nqsort\nserialize\nwarplan\nzebra\n\niparma\n97.0 %\n95.4 %\n96.4 %\n92.8 %\n99.3 %\n94.5 %\n\nTime\niaparma\n93.3 %\n71.6 %\n83.5 %\n92.8 %\n99.7 %\n91.9 %\n\nrelative\n96.2 %\n75.1 %\n86.6 %\n100.0 %\n100.4 %\n97.3 %\n\niparma\n55.0 %\n66.7 %\n66.6 %\n61.8 %\n52.9 %\n50.2 %\n\nMaximal trail\niaparma\nrelative\n47.9 %\n87.1 %\n0.0 %\n0.0 %\n0.0 %\n0.0 %\n61.8 % 100.0 %\n52.9 % 100.0 %\n46.4 %\n92.4 %\n\nFor the serialize and warplan benchmarks the analysis was not able to reduce\nthe number of actual trailing operations. For the other four benchmarks the combination of the improved scheme with analysis yields better results, both for time and\nmaximal trail. For the hanoi and qsort benchmarks there is again a drastic improvement: all trailings have been avoided, with a distinctive time improvement of\n25% and 15 % respectively. For the other two benchmarks, icomp and zebra, there\nis a maximal trail improvement of about 10% together with a slightly reduced time,\n4% and 3% better respectively. Overall, the combination of the improved scheme\nwith the trailing analysis never makes the results worse. Since it drastically improves some benchmarks and shows a modest improvement of others, it is fair to\nconclude that the combination is superior to the improved system without analysis.\n\n9.4 Effect of the improved trailing scheme in dProlog\nLet us now present the experimental results of the improved conditional PARMA\ntrailing scheme in dProlog for several small benchmarks and one bigger program,\ncomp. Table 8 shows the timing and maximal trail use for each benchmark. Time is\ngiven in seconds and applies to the number of runs (iterations) given. The maximal\ntrail size is given in kilobytes and applies to a single run.\nThe time difference between the classic and the improved scheme is negligible.\nThe improved scheme is at most 8.8% slower, for the zebra benchmark, but on\n32\n\n\fTable 8. PARMA in dProlog: classic (cparma) vs. improved trailing (iparma)\nBenchmark\n\nIterations\n\nboyer\n10\nbrowse\n10\ncal\n100\nchat\n50\ncrypt\n2,000\nham\n20\nmeta qsort\n1,250\nnrev\n50,000\n100\npoly 10\nqueens 16\n20\nqueens\n100\nreducer\n200\nsdda\n12,000\nsend\n100\ntak\n100\nzebra\n300\nrelative average\ncomp\n1\ncomp relative\n\nTime\ncparma iparma\n.950\n.920\n1.010\n1.010\n1.800\n1.800\n1.020\n1.040\n1.160\n1.170\n1.160\n1.130\n1.070\n1.090\n.900\n.860\n.630\n.650\n1.810\n1.790\n3.310\n3.300\n.440\n.430\n1.000\n1.010\n.800\n.800\n1.620\n1.520\n2.510\n2.730\n100%\n99.9%\n1.930\n1.890\n100%\n97.9%\n\nMaximal trail\ncparma iparma\n450.6\n225.3\n5.2\n4.5\n0.4\n0.2\n3.6\n1.9\n0.5\n0.2\n0.8\n0.4\n12.6\n7.4\n0.4\n0.2\n52.6\n26.3\n0.7\n0.3\n0.7\n0.3\n18.9\n10.0\n1.3\n0.8\n0.5\n0.2\n373.1\n186.6\n1.6\n0.8\n100%\n51.7%\n2516.3\n1319.8\n100%\n52.4%\n\naverage both are about equally fast. The price for the lower trail usage is an increase\nin instructions executed and that is why there is no net speedup.\nThe differences in maximal trail use however are substantial. While swap trail\nand chain trail halve the trail stack consumption, value trailing is still used for\nsome cases of variable\u2013variable trailing. Yet experimental results show that that\nkind of variable\u2013variable trailing does not occur very often in most benchmarks, as\nthe maximal trail stack is effectively halved in eleven benchmarks and on average\nthe maximal trail use is 51.7% of the classical scheme.\nThe results for the smaller benchmarks are confirmed by the larger comp program.\nExecution time is nearly the same for the classic and improved trailing scheme and\nthe maximal trail shows a similar improvement of almost 50%.\n\n10 Related and future work\nAs far as we know, the modifications suggested to the classic PARMA trailing\nscheme are new.\nA somewhat similar analysis for detecting variables that do not have to be trailed\nis presented by Debray in (Debray 1992) together with corresponding optimizations.\nDebray's analysis however is for the WAM variable representation and in a traditional Prolog setting, i.e., without type, mode and determinism declarations. Also\n33\n\n\fin (Van Roy and Despain 1992) trailing is avoided, but only for variables that are\nnew in our terminology and, again, the setting is basically the WAM representation.\nTaylor too keeps track of a trailing state of variables in the global analysis of\nhis PARMA system with the classic PARMA trailing scheme (see (Taylor 1991;\nTaylor 1989)). As opposed to the Lnotrail analysis we have presented here, Taylor's\nanalysis is less precise and closer to the Ltrail analysis presented here: the trailing\nstate of a variable can only be that it has to be trailed or not, i.e. there is no\nintermediary shallow trailing state.\nThere exist also two run-time technique for preventing the multiple value trailing\nbetween two choice points. The first, described in (Noy\u00e9 1994), only works in the\nWAM scheme, because it introduces linear reference chains that PARMA does\nnot allow. The second, described in (Aggoun and Beldiceanu 1990), maintains a\ntimestamp for every cell that corresponds to the choicepoint before the last update.\nHowever, such a timestamp requires additional space, even in the case that the cell\nis never updated. In the context of PARMA, timestamps would likely consume more\nspace than is actually saved by avoiding trailing.\nFinally, there are other approaches to the reconstruction of state on backtracking\nother than trailing, using either copying (Schulte 1999) or recomputation (Van Hentenryck and Ramachandran 1995\nWhile PARMA (and for that matter WAM) bindings do not keep enough information to allow recomputation on backtracking, a copying approach to backtracking in\nPARMA is quite feasible. This remains as an interesting question for future work.\nThere is little room left for optimization of the trailing analysis for the improved\nunconditional trailing scheme. Of course, the analysis itself can be improved by\nadopting a more refined representation for bound variables. Currently, all PARMA\nchains in the structure of a bound variable are represented by the same trailing\nstate. Bound variables could be represented more accurately, by requiring the domain to keep track of the different chains contained in the structures to which\nthe program variables are bound, their individual trailing state and how these are\naffected by the different program constructs. Known techniques (see for instance\n(Janssens and Bruynooghe 1993; Van Hentenryck et al. 1995; Mulkers et al. 1994;\nLagoon and Stuckey 2001; Lagoon et al. 2003)) based on type information could\nbe used to keep track of the constructor that a variable is bound to and the trailing state of the different arguments, thereby making this approach possible. This\napplies equally to the analysis of the classical scheme.\nAdditionally, it would be interesting to see how much extra gain analysis can add\nto the improved conditional trailing scheme as implemented in dProlog or in the\nMercury back-end of HAL that supports conditional trailing. Such analysis would\ncertainly not improve the maximal trail, but it would remove the overhead of the\nrun-time test. This will most likely also result in a small speed-up.\nThough experimental results show that the improved scheme with analysis is\nbetter than the classic scheme with analysis, this need not be true for all programs.\nRecall that between two choice points all value trailings of a cell but the first can be\neliminated in the classic scheme, while no swap trailings could be eliminated in the\nimproved scheme. A hybrid scheme would be possible using analysis to decide on a\nsingle unification basis if either swap trailing or value trailing is better at minimizing\n34\n\n\fthe amount of trailing and the cost of untrailing. This analysis would require a more\nglobal view of all the trailings in between two choice points. Moreover, some trailings\ncould be common to different pairs of choice points and optimality would depend\non where execution spends most of its time.\nAlso the untrailing operation can be improved: when analysis is able to determine\nfor instance that the only trailing that happened was a swap trailing, no tags need\nto be set and tested.\nAcknowledgements\nWe would like to thank the referees for their detailed and insightful reports which\nhave significantly improved the paper.\nReferences\nAggoun, A. and Beldiceanu, N. 1990. Time Stamps Techniques for the Trailed Data in\nConstraint Logic Programming Systems. In SPLT'90: 8\u00e8me S\u00e9minaire Programmation\nen Logique, S. Bourgault and M. Dincbas, Eds. CNET, Tr\u00e9gastel, France, 487\u2013510.\nA\u0131\u0308t-Kaci, H. 1991. Warren's Abstract Machine: A Tutorial Reconstruction. MIT Press.\nBruynooghe, M. 1991. A Practical Framework for the Abstract Interpretation of Logic\nPrograms. Journal of Logic Programming 10, 1/2/3&4, 91\u2013124.\nBueno, F., de la Banda, M. J. G., Hermenegildo, M. V., Marriott, K., Puebla,\nG., and Stuckey, P. J. 2001. A Model for Inter-module Analysis and Optimizing\nCompilation. In LOPSTR '00: Selected Papers form the 10th International Workshop\non Logic Based Program Synthesis and Transformation, K.-K. Lau, Ed. Lecture Notes\nin Computer Science, vol. 2042. Springer Verlag, London, UK, 86\u2013102.\nde la Banda, M. J. G., Demoen, B., Marriott, K., and Stuckey, P. J. 2002. To the\nGates of HAL: A HAL Tutorial. In FLOPS 2002: Proceedings of the 6th International\nSymposium on Functional and Logic Programming, Z. Hu and M. Rodr\u0131\u0301guez-Artalejo,\nEds. Lecture Notes in Computer Science, vol. 2441. Springer Verlag, Aizu, Japan, 47\u201366.\nde la Banda, M. J. G., Marriott, K., Stuckey, P. J., and S\u00f8ndergaard, H. 1998.\nDifferential Methods in Logic Program Analysis. Journal of Logic Programming 35, 1,\n1\u201337.\nDebray, S. 1992. A Simple Code Improvement Scheme for Prolog. Journal of Logic\nProgramming 13, 1 (May), 57\u201388.\nDemoen, B., de la Banda, M. J. G., Harvey, W., Marriott, K., and Stuckey, P. J.\n1999. An Overview of HAL. In CP'99: Proceedings of the 5th International Conference\non Principles and Practice of Constraint Programming, J. Jaffar, Ed. Lecture Notes in\nComputer Science, vol. 1713. Springer Verlag, Alexandria, Virginia, USA, 174\u2013188.\nDemoen, B. and Nguyen, P.-L. 2000. So Many WAM Variations, so Little Time.\nIn CL2000: Proceedings of the 1st International Conference on Computational Logic,\nJ. Lloyd, V. Dahl, U. Furbach, M. Kerber, K.-K. Lau, C. Palamidessi, L. Moniz Pereira,\nY. Sagiv, and P. J. Stuckey, Eds. Lecture Notes in Artificial Intelligence, vol. 1861. ALP,\nSpringer Verlag, London, UK, 1240\u20131254.\nFerreira, M. and Damas, L. 2003. Controlling Code Expansion in a Multiple Specialization Prolog Compiler. In Proceedings of CICLOPS 2003: Colloquium on Implementation\nof Constraint and LOgic Programming Systems, R. Lopes and M. Ferreira, Eds. University of Porto, Mumbai, India, 75\u201387. Technical Report DCC-2003-05, DCC - FC &\nLIACC, Univeristy of Porto, December 2003.\n\n35\n\n\fJanssens, G. and Bruynooghe, M. 1993. Deriving Descriptions of Possible Value of\nProgram Variables by means of Abstract Interpretation. Journal of Logic Programming 13, 205\u2013258.\nLagoon, V., Mesnard, F., and Stuckey, P. J. 2003. Termination Analysis with Types\nIs More Accurate. In ICLP 2003: Proceedings of the 19th International Conference on\nLogic Programming, C. Palamidessi, Ed. Lecture Notes in Computer Science, vol. 2916.\nSpringer Verlag, Mumbai, India, 254\u2013268.\nLagoon, V. and Stuckey, P. 2001. A Framework for Analysis of Typed Logic Programs.\nIn FLOPS 2001: Proceedings of the 5th International Symposium on Functional and\nLogic Programming, H. Kuchen and K. Ueda, Eds. Lecture Notes in Computer Science,\nvol. 2024. Springer Verlag, Tokyo, Japan, 296\u2013310.\nLindgren, T., Mildner, P., and Bevemyr, J. 1995. On Taylor's Scheme for Unbound\nVariables. Tech. rep., Computer Science Department, Uppsala University. Oct.\nMazur, N. 2001. Compile-time Garbage Collection for the Declarative Language Mercury.\nPh.D. thesis, Department of Computer Science, K.U.Leuven, Leuven, Belgium.\nMulkers, A., Winsborough, W. H., and Bruynooghe, M. 1994. Live-Structure\nDataflow Analysis for Prolog. ACM Transactions on Programming Languages and Systems 16, 2, 205\u2013258.\nNethercote, N. 2001. The Analysis Framework of HAL. M.S. thesis, University of\nMelbourne.\nNethercote, N. and Seward, J. 2003. Valgrind: A Program Supervision Framework.\nElectronic Notes in Theoretical Computer Science 89, 2, 1\u201323.\nNoy\u00e9, J. 1994. Elagage de contexte, retour arri\u00e8re superficiel, modifications r\u00e9versibles\net autres: une \u00e9tude approfondie de la WAM. Ph.D. thesis, Universit\u00e9 de Rennes I.\nSchulte, C. 1999. Comparing Trailing and Copying for Constraint Programming. In\nProceedings of the Sixteenth International Conference on Logic Programming, D. De\nSchreye, Ed. MIT Press, Las Cruces, NM, USA, 275\u2013289.\nSomogyi, Z., Henderson, F., and Conway, T. 1996. The Execution Algorithm of\nMercury, an Efficient Purely Declarative Logic Programming Language. Journal of\nLogic Programming 29, 1\u20133, 17\u201364.\nS\u00f8ndergaard, H. 1986. An Application of Abstract Interpretation of Logic Programs:\nOccur Check Reduction. In ESOP 86: Proceedings of the European Symposium on\nProgramming, B. Robinet and R. Wilhelm, Eds. Lecture Notes in Computer Science,\nvol. 213. Springer Verlag, Saarbr\u00fccken, Germany, 327\u2013338.\nTaylor, A. 1989. Removal of Dereferencing and Trailing in Prolog Compilation. In\nProceedings of the 6th Internation Conference on Logic Programming, G. Levi and\nM. Martelli, Eds. MIT Press, Lisbon, Portugal, 48\u201360.\nTaylor, A. 1991. High Performace Prolog Implementation. Ph.D. thesis, Basser Department of Computer Science.\nTaylor, A. 1996. Parma - Bridging the Performance GAP Between Imperative and Logic\nProgramming. Journal of Logic Programming 29, 1-3, 5\u201316.\nVan Hentenryck, P., Cortesi, A., and Charlier, B. L. 1995. Type analysis of Prolog\nusing Type Graphs. Journal of Logic Programming 22, 3, 179\u2013209.\nVan Hentenryck, P. and Ramachandran, V. 1995. Backtracking without Trailing in\nCLP(Rlin). ACM Transactions on Programming Languages and Systems 17, 4 (July),\n635\u2013671.\nVan Roy, P. and Despain, A. 1992. High-Performance Logic Programming with the\nAquarius Prolog Compiler. IEEE Computer 25, 1, 54\u201368.\n\n36\n\n\f"}