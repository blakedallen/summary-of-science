{"id": "http://arxiv.org/abs/1004.0456v1", "guidislink": true, "updated": "2010-04-03T16:28:47Z", "updated_parsed": [2010, 4, 3, 16, 28, 47, 5, 93, 0], "published": "2010-04-03T16:28:47Z", "published_parsed": [2010, 4, 3, 16, 28, 47, 5, 93, 0], "title": "Exploratory Analysis of Functional Data via Clustering and Optimal\n  Segmentation", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.2432%2C1004.4457%2C1004.2288%2C1004.3209%2C1004.0920%2C1004.2307%2C1004.2672%2C1004.3206%2C1004.4821%2C1004.2372%2C1004.0685%2C1004.3224%2C1004.4172%2C1004.0876%2C1004.0545%2C1004.3862%2C1004.1863%2C1004.2966%2C1004.1843%2C1004.4400%2C1004.2416%2C1004.2792%2C1004.5083%2C1004.4481%2C1004.0483%2C1004.4441%2C1004.2311%2C1004.0128%2C1004.4861%2C1004.4456%2C1004.4860%2C1004.4574%2C1004.0304%2C1004.4695%2C1004.0094%2C1004.4607%2C1004.4251%2C1004.1262%2C1004.2029%2C1004.3019%2C1004.3391%2C1004.3256%2C1004.3522%2C1004.0650%2C1004.4892%2C1004.2414%2C1004.0812%2C1004.1877%2C1004.3156%2C1004.3775%2C1004.0669%2C1004.4022%2C1004.4854%2C1004.4659%2C1004.4484%2C1004.4804%2C1004.2351%2C1004.5601%2C1004.2894%2C1004.4443%2C1004.3771%2C1004.0272%2C1004.0175%2C1004.4948%2C1004.1387%2C1004.3057%2C1004.1168%2C1004.1770%2C1004.3513%2C1004.4887%2C1004.1893%2C1004.3666%2C1004.2516%2C1004.0514%2C1004.5518%2C1004.2543%2C1004.1521%2C1004.3188%2C1004.4570%2C1004.2491%2C1004.2799%2C1004.3876%2C1004.5263%2C1004.3810%2C1004.3840%2C1004.4875%2C1004.2734%2C1004.0456%2C1004.1336%2C1004.2779%2C1004.4592%2C1004.1043%2C1004.4987%2C1004.2331%2C1004.3609%2C1004.1357%2C1004.4120%2C1004.3419%2C1004.4712%2C1004.3939%2C1004.4367&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Exploratory Analysis of Functional Data via Clustering and Optimal\n  Segmentation"}, "summary": "We propose in this paper an exploratory analysis algorithm for functional\ndata. The method partitions a set of functions into $K$ clusters and represents\neach cluster by a simple prototype (e.g., piecewise constant). The total number\nof segments in the prototypes, $P$, is chosen by the user and optimally\ndistributed among the clusters via two dynamic programming algorithms. The\npractical relevance of the method is shown on two real world datasets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.2432%2C1004.4457%2C1004.2288%2C1004.3209%2C1004.0920%2C1004.2307%2C1004.2672%2C1004.3206%2C1004.4821%2C1004.2372%2C1004.0685%2C1004.3224%2C1004.4172%2C1004.0876%2C1004.0545%2C1004.3862%2C1004.1863%2C1004.2966%2C1004.1843%2C1004.4400%2C1004.2416%2C1004.2792%2C1004.5083%2C1004.4481%2C1004.0483%2C1004.4441%2C1004.2311%2C1004.0128%2C1004.4861%2C1004.4456%2C1004.4860%2C1004.4574%2C1004.0304%2C1004.4695%2C1004.0094%2C1004.4607%2C1004.4251%2C1004.1262%2C1004.2029%2C1004.3019%2C1004.3391%2C1004.3256%2C1004.3522%2C1004.0650%2C1004.4892%2C1004.2414%2C1004.0812%2C1004.1877%2C1004.3156%2C1004.3775%2C1004.0669%2C1004.4022%2C1004.4854%2C1004.4659%2C1004.4484%2C1004.4804%2C1004.2351%2C1004.5601%2C1004.2894%2C1004.4443%2C1004.3771%2C1004.0272%2C1004.0175%2C1004.4948%2C1004.1387%2C1004.3057%2C1004.1168%2C1004.1770%2C1004.3513%2C1004.4887%2C1004.1893%2C1004.3666%2C1004.2516%2C1004.0514%2C1004.5518%2C1004.2543%2C1004.1521%2C1004.3188%2C1004.4570%2C1004.2491%2C1004.2799%2C1004.3876%2C1004.5263%2C1004.3810%2C1004.3840%2C1004.4875%2C1004.2734%2C1004.0456%2C1004.1336%2C1004.2779%2C1004.4592%2C1004.1043%2C1004.4987%2C1004.2331%2C1004.3609%2C1004.1357%2C1004.4120%2C1004.3419%2C1004.4712%2C1004.3939%2C1004.4367&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose in this paper an exploratory analysis algorithm for functional\ndata. The method partitions a set of functions into $K$ clusters and represents\neach cluster by a simple prototype (e.g., piecewise constant). The total number\nof segments in the prototypes, $P$, is chosen by the user and optimally\ndistributed among the clusters via two dynamic programming algorithms. The\npractical relevance of the method is shown on two real world datasets."}, "authors": ["Georges H\u00e9brail", "Bernard Hugueney", "Yves Lechevallier", "Fabrice Rossi"], "author_detail": {"name": "Fabrice Rossi"}, "author": "Fabrice Rossi", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1016/j.neucom.2009.11.022", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1004.0456v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1004.0456v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1004.0456v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1004.0456v1", "arxiv_comment": null, "journal_reference": "Neurocomputing, Volume 73, Issues 7-9, March 2010, Pages 1125-1141", "doi": "10.1016/j.neucom.2009.11.022", "fulltext": "Exploratory Analysis of Functional Data via Clustering\nand Optimal Segmentation\nGeorges H\u00e9brailb , Bernard Hugueneya , Yves Lechevallierc , Fabrice Rossi\u2217,b\na LAMSADE,\n\narXiv:1004.0456v1 [stat.ML] 3 Apr 2010\n\nUniversit\u00e9 Paris Dauphine, Place du Mar\u00e9chal de Lattre de Tassigny, 75016\nParis \u2013 France\nb BILab, T\u00e9l\u00e9com ParisTech, LTCI - UMR CNRS 5141 46, rue Barrault, 75013 Paris \u2013\nFrance\nc Projet AxIS, INRIA, Domaine de Voluceau, Rocquencourt, B.P. 105 78153 Le Chesnay\nCedex \u2013 France\n\nAbstract\nWe propose in this paper an exploratory analysis algorithm for functional data.\nThe method partitions a set of functions into K clusters and represents each\ncluster by a simple prototype (e.g., piecewise constant). The total number of\nsegments in the prototypes, P , is chosen by the user and optimally distributed\namong the clusters via two dynamic programming algorithms. The practical\nrelevance of the method is shown on two real world datasets.\nKey words: Functional Data, Multiple time series, Exploratory analysis,\nClustering, Segmentation, Dynamic programming\n\n1. Introduction\nFunctional Data Analysis [26] addresses problems in which the observations\nare described by functions rather than finite dimensional vectors. A well known\nreal world example of such data is given by spectrometry in which each object\nis analysed via one spectrum, that is a function which maps wavelengths to\nabsorbance values. Online monitoring of hardware is also a good example of\nsuch data: each object is described by several time series associated to physical\nquantities monitored at specified sampling rates.\nAnother application domain is related to electric power consumption curves,\nalso called (electric) load curves. Such data describes the electric power consumption over time of one household or one small or large industry. Load curves\ncan be very voluminous since there are many consumers (over 35 millions in\nFrance) which can be observed during a long period (often several years) and\nat a rate up to one point every minute. Consequently, there is a strong need\nfor applying unsupervised learning methods to summarize such datasets of load\ncurves. A typical way to do so is to split each curve into daily (or weekly)\nperiods and perform a clustering of such daily curves. The motivation of such\nanalyses is related to several practical problems: understanding the consumption\n\u2217 Corresponding\n\nauthor\nEmail addresses: Georges.Hebrail@telecom-paristech.fr (Georges H\u00e9brail),\nYves.Lechevallier@inria.fr (Yves Lechevallier), Fabrice.Rossi@telecom-paristech.fr\n(Fabrice Rossi)\n\nPreprint submitted to Neurocomputing\n\nOctober 24, 2018\n\n\fbehaviour of consumers in relation to their equipment or weather conditions,\ndefining new prices, optimizing the production of electric power, and in the next\nyears monitoring the household consumption to face high peaks.\nWe focus in this paper on the exploratory analysis of a set of curves (or time\nseries). The main idea is to provide the analyst with a summary of the set with\na manageable complexity. A classical solution for multivariate data consists in\nusing a prototype based clustering approach: each cluster is summarized by its\nprototype. Standard clustering methods such as K means and Self Organizing\nMap have been adapted to functional data and could be used to implement\nthis solution [1, 6, 8, 29]. Another possibility comes from the symbolization\napproaches in which a time series is represented by a sequence of symbols. In the\nSAX approach [24], the time series is transformed into a piecewise representation\nwith contiguous time intervals of equal size: the value associated with each\ninterval is the average of actual values in the interval. This approach is very\nfast but does not give any guarantee on the associated error. In [15], a piecewise\nconstant approximation of a time series is constructed via a segmentation of the\ntime domain into contiguous intervals on which the series is represented by its\naverage value, which can be turned into a label in a subsequent quantization\nstep. When we are given a set of curves, an unique segmentation can be found in\norder to represent all the curves on a common piecewise constant basis (see [28]\nfor an optimal solution). This was used as a preprocessing step in e.g. [19, 30].\nWe propose in this paper to merge the two approaches: we build a K means\nlike clustering of a set of functions in which each prototype is given by a simple\nfunction defined in a piecewise way. The input interval of each prototype is partitioned into sub-intervals (segments) on which the prototype assumes a simple\nform (e.g., constant). Using dynamic programming, we obtain an optimal segmentation for each prototype while the number of segments used in each cluster\nis also optimally chosen with respect to a user specified total number segments.\nIn the case of piecewise constant prototypes, a set of functions is summarized\nvia 2P \u2212 K real values, where K is the number of prototypes and P the total\nnumber of segments used to represent the prototypes.\nThe rest of this paper is organized as follows. Section 2 introduces the problem of finding a simple summary of a single function, links this problem with\noptimal segmentation and provides an overview of the dynamic programming\nsolution to optimal segmentation. Section 3 shows how single function summarizing methods can be combined to clustering methods to give a summary\nof a set of functions. It also introduces the optimal resource allocation algorithm that computes an optimal number of segments for each cluster. Section\n4 illustrates the method on two real world datasets.\n2. Summarizing one function via optimal segmentation: a state-ofthe-art\nThe proposed solution is built on two elements that are mostly independent: any standard clustering algorithm that can handle functional data and\na functional summarizing technique that can build an appropriate low complexity representation of a set of homogeneous functions. The present section\ndescribes the summarizing technique for a single function; the extension to a\nset of functions is described in Section 3.\n\n2\n\n\fBuilding a summary of a function is deeply connected to building an optimal segmentation of the function, a problem which belongs to the general task\nof function approximation. Optimal segmentation has been studied under different point of views (and different names) in a large and scattered literature\n(see [2, 3, 16, 21, 22, 25, 32] and references therein). The goal of the present\nsection is not to provide new material but rather to give a detailed exposition\nof the relations between summary and segmentation on the one hand, and of\nthe optimal segmentation framework itself on the other hand.\n2.1. Simple functions\nWe consider a function s sampled in M distinct points (tk )M\nk=1 from the\ninterval [t1 , tM ] (points are assumed to be ordered, i.e., t1 < t2 < . . . < tM ).\nOur goal is to approximate s by a simpler function g on [t1 , tM ]. The simplicity\nconcept targeted in this article is not based on smoothness as in regularization\n[33] or on capacity as in learning theory (see e.g., [9]). It corresponds rather\nto an informal measure of the simplicity of the visual representation of g for a\nhuman analyst.\nThe difference between those three aspects (smoothness, capacity and visual\nsimplicity) can be illustrated by an elementary example: let us assume that g\nis chosen as a linear combination of N fixed functions, i.e.,\ng(t) =\n\nN\nX\n\n\u03b2i \u03c6i (t).\n\n(1)\n\ni=1\n\nIt is well known that the set of indicator functions based on functions of the\nprevious form has a VC dimension of N , as long as the functions (\u03c6i )N\ni=1 are\nlinearly independent (see e.g., [9]). If we consider now the L2 norm of the\nsecond derivative of g as a roughness measure, it is quite clear that the actual\nsmoothness will depend both on the functions (\u03c6i )N\ni=1 and on the values of the\ncoefficients (\u03b2i )N\ni=1 . As an extreme case, one can consider \u03c61 (t) = sign(t) while\nthe (\u03c6i )N\ni=2 are smooth functions. Then any g with \u03b21 6= 0 is non smooth while\nall other functions are.\nThe visual complexity of g will also clearly depend on both the basis functions and the coefficients, but in a way that will be generally quite opposite to\nthe smoothness. Let us again consider an extreme case with N = 2, the interval\n[0, 1] and two choices of basis functions, (\u03c61 , \u03c62 ) and (\u03c81 , \u03c82 ) defined as follows:\n(\n(\n0 if t < 12\n1 if t < 21\n\u03c6\n(t)\n=\n(2)\n\u03c61 (t) =\n2\n0 if t \u2265 12\n1 if t \u2265 12\nand\n\u0012\n\n1\n\u03c81 (t) = exp \u221216 t \u2212\n4\n\n\u00132 !\n\n\u0012\n\u03c82 (t) = exp \u22128\n\n\u00132 !\n3\n\u2212t\n.\n4\n\n(3)\n\nObviously, functions generated by (\u03c61 , \u03c62 ) are far less smooth than functions\ngenerated by (\u03c81 , \u03c82 ) (the former are piecewise constant). However, as shown\non Figure 1, functions generated by (\u03c61 , \u03c62 ) are much easier to describe and\nto understand than functions generated by (\u03c81 , \u03c82 ). Indeed, piecewise constant\nfunctions admit simple textual description: the function on the left part of\n3\n\n\f1.0\n0.0\n\n0.2\n\n0.4\n\ng(t)\n\n0.6\n\n0.8\n\n1.0\n0.8\n0.6\n0.0\n\n0.2\n\n0.4\n\ng(t)\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n0.0\n\nt\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nt\n\nFigure 1: Left: function generated by (\u03c61 , \u03c62 ); right: function generated by (\u03c81 , \u03c82 )\n\nFigure 1 takes values 0.5 for the first half of the period of study and then\nswitches to 1 for the last part. In the contrary, smooth functions are inherently\nmore complex to describe: the function on the right part of Figure 1 needs a\nlong textual description specifying its evolution trough time. In addition, no\nreasonable textual description will give a way to reconstruct exactly the original\nfunction.\nTo summarize, classes of functions with identical capacity (as measured by\nthe VC dimension) can contain functions of arbitrary smoothness and arbitrary\nunrelated complexity. We need therefore a specific solution to induce a simple\napproximation of a function. Experimental studies in information visualization\n(see, e.g., [13]) have shown that different tasks (e.g., clustering, change detection,\netc.) have different type of adapted visual features. In our context, we target\napplications in which a textual (written or spoken) description of behavior of\nthe function is useful. For instance in mass spectrometry, peaks positions are\nvery informative and correspond to descriptions such as \"there is a peak of this\nheight at that position\". In the case of electrical consumption, the mean level\nconsumption during a time frame is an interesting aspect and corresponds to\ndescription such as \"the outlet consumes this amount of power from time a to\ntime b\". Hardware monitoring provides another example in which quantities of\ninterest are generally constant on some time frames and switch linearly from\none stable value to another.\nTo implement this idea, we rely on a segmentation approach detailed in the\nnext section (and covered in a large body of literature, as recalled in the introduction). A simple function in our context is given by a partition of [t1 , tM ]\ninto a small number of segments on which the function takes a simple application dependent parametric form. If the parametric form is reduced to a single\nvalue, the simple function is piecewise constant and is easy to describe in text.\nOther convenient parametric forms are affine functions (to provide less jumpy\ntransitions) and peaks. This approach is closely related to (and inspired by)\nsymbolization techniques used for instance in time series indexing [15]: the idea\nis to build a piecewise constant approximation of a function and then to quantize\nthe levels (the constants) into a small number of values. The obtained values\nare replaced by symbols (e.g., letters) and the time series is therefore translated\n\n4\n\n\finto a sequence of symbols.\n2.2. Segmentation\nLet us analyze first an example of the general scheme described in the previous section. We consider a piecewise constant approximation given by P interP\nP\nvals (Ip )P\np=1 and the corresponding P values (ap )p=1 (we assume that (Ip )p=1\nis a partition of [t1 , tM ]). The approximating function g is defined by g(t) = ap\nwhen t \u2208 Ip . If we consider the L2 error criterion, we obtain a particular case\nof the Stone approximation problem [32] as we have to minimize\n\u0001\nP\nE s, (Ip )P\np=1 , (ap )p=1 =\n\nZ\n\ntM\n\n(s(t) \u2212 g(t))2 dt =\n\nt1\n\nXZ\np\n\n(s(t) \u2212 ap )2 dt.\n\n(4)\n\nIp\n\nThe integral can be approximated by a quadrature scheme. We consider the\nsimplest setting in which a summation over (tk )M\nk=1 provides a sufficient approximation, but the generalization to weighted schemes such as [23] is straightforward. In practice, we optimize therefore\n\u0001 X X\nP\nE s, (Ip )P\n(s(tk ) \u2212 ap )2 .\n(5)\np=1 , (ap )p=1 =\np tk \u2208Ip\n\nThe difficulty lies in the segmentation, i.e., in the choice of the partition (Ip )P\np=1 ,\nas, given this partition, the optimal values of the (ap )P\nare\nequal\nto\nthe\n\u03bc\np =\np=1\nP\n1\ns(t\n),\nwhere\n|I\n|\ndenotes\nthe\nnumber\nof\nt\nin\nI\n.\nk\np\nk\np\ntk \u2208Ip\n|Ip |\nMore generally, the proposed method is built upon a parametric approximation on each segment. Let us denote Q(s, I) the minimal error made by this\nparametric approximation of s on interval I. The piecewise constant approximation described above corresponds to\nQ(s, I) =\n\nX\nt\u2208I\n\n!2\n1 X\ns(t) \u2212\ns(v) .\n|I|\n\n(6)\n\nv\u2208I\n\nSimilar expressions can be derived for a piecewise linear approximation or any\nother reasonable solution. For instance\nX\nQ(s, I) =\n|s(t) \u2212 m(s, I)| ,\n(7)\nt\u2208I\n\nwhere m(s, I) is the median of {s(tk )|tk \u2208 I}, provides a more robust solution.\nGiven Q, the optimal segmentation of s is obtained by minimizing\n\u0001 X\nE s, (Ip )P\nQ(s, Ip ),\n(8)\np=1 =\np\n\nover the partitions (Ip )P\np=1 of [t1 , tM ]. This formulation emphasizes the central\nhypothesis of the method: the optimal approximation on a segment depends\nonly on the values taken by s in the segment. While a more general segmentation\nproblem, in which this constraint is removed, can be easily defined, its resolution\nis much more computationally intensive than the one targeted here.\n\n5\n\n\f2.3. Optimal segmentation\nBellman showed in [3] that the Stone approximation problem [32] problem\ncan be solved exactly and efficiently by dynamic programming: this is a consequence of the independence between the approximation used in each segment.\nThis has been rediscovered and/or generalized in numerous occasions (see e.g.,\n[2, 16, 25]).\nA different point of view is given in [21, 22]. As the function s is known only\nthrough its values at (tk )M\nk=1 , defining a partition of [t1 , tM ] into P intervals\n(Ip )P\nis\nnot\nneeded.\nA\npartition\n(Cp )P\np=1\np=1 of t1 < t2 < . . . < tM is sufficient, as\nlong as it is ordered : if tk \u2208 Cp and tl \u2208 Cp , then {tk , tk+1 , . . . , tl } \u2282 Cp . The\ngoal of the segmentation is then to find an optimal ordered partition of (tk )M\nk=1\naccording to the error criterion defined in equation (5) (where Ip is replaced by\nCp and Q is modified accordingly). As shown in [21, 22], this problem can be\nsolved by dynamic programming as long as the error criterion is additive, i.e.,\nof the general form\n\u0001 X\nE s, (Cp )P\nQ(s, Cp ).\n(9)\np=1 =\np\n\nMoreover, the summation operator can be replaced by any commutative\naggregation operator, e.g., the maximum. As already mentioned in the previous\nsection, the crucial point is that the error criterion is a combination of values\nobtained independently on each cluster.\nThe dynamic programming algorithm for minimizing the cost of equation\n(9) is obtained as follows. We define first a set of clustering problems:\nX\nF (s, k, j) =\nmin\nQ(s, Cp ).\n(10)\nj\n(Cp )p=1 , ordered partition of {tk ,tk+1 ,...,tM } p\n\nThe basic idea is to compute F (s, k, j) using F (s, ., j \u2212 1), as the best partition\ninto j clusters is obtained with\nF (s, k, j) =\n\nmin\n\nk\u2264l\u2264M \u2212j+1\n\nQ (s, {tk , . . . , tl }) + F (s, l + 1, j \u2212 1).\n\n(11)\n\nIndeed the partition is ordered and therefore, up to a renumbering of the clusters, there is l such that C1 = {t1 , . . . , tl }. Moreover, the quality measure is\nadditive and therefore finding the best partition in j clusters with the constraint\nthat C1 = {t1 , . . . , tl } corresponds to finding the best partition of {tl+1 , . . . , tM }\nin j \u2212 1 clusters. This leads to Algorithm 1, in which W (k, j) is the winner split,\ni.e., the l that realizes the minimum in equation (11). The final loop is the backtracking phase in which the split positions are assembled to produce the result\nof the algorithm, C, which gives the last indexes of the P \u2212 1 first clusters of\nthe partition. It should be noted that while Algorithm 1 outputs only the optimal partition in P clusters, an actual implementation will of course provide in\naddition the optimal model. Moreover, as the algorithm produces all values of\nF (., .), it can provide at once all optimal partitions in p clusters for p ranging\nfrom 1 to P . This is done via P backtracking loops for a total cost in O(P 2 ).\nFigures 2 and 3 show an example of the results obtained by the algorithm.\nThe original function on the left side of Figure 2 is a spectrum from the Tecator\n\n6\n\n\fAlgorithm 1 Dynamic programming summarizing of a single function\n1: for k = 1 to M do\n2:\nF (s, k, 1) \u2190 Q (s, {tk , tk+1 , . . . , tM })\n3:\nW (k, 1) \u2190 NA {no meaningful value at for j = 1}\n4: end for\n5: for j = 2 to P do\n6:\nfor k = 1 to M \u2212 j + 1 do\n7:\nF (s, k, j) \u2190 Q(s, {tk }) + F (s, k + 1, j \u2212 1)\n8:\nW (k, 1) \u2190 k\n9:\nfor l = k + 1 to M \u2212 j + 1 do\n10:\nif Q(s, {tk , . . . , tl }) + F (s, l + 1, j \u2212 1) < F (s, k, j) then\n11:\nF (s, k, j) \u2190 Q({s, tk , . . . , tl }) + F (s, l + 1, j \u2212 1)\n12:\nW (k, 1) \u2190 l\n13:\nend if\n14:\nend for\n15:\nend for\n16: end for\n17: C \u2190 (NA, . . . , NA)\n18: C(P \u2212 1) \u2190 W (1, P )\n19: for j = P \u2212 2 to 2 do\n20:\nC(j) \u2190 W (C(j + 1) + 1, j + 1)\n21: end for\n\n4\n5\n6\n\nnumber of segments\n\n7\n10\n\n9\n\n2.8\n\n8\n\n3.0\n\nabsorbance\n\n3.2\n\n3\n\n2\n\n3.4\n\n1\n\ndataset1 for which M = 100. The spectrum is segmented into 1 to 10 segments\n(positions of the optimal segments are given on the right side of Figure 2). The\nresulting approximations for 4 and 6 segments are given on Figure 3.\n\n850\n\n900\n\n950\n\n1000\n\n1050\n\n850\n\nwavelength\n\n900\n\n950\n\n1000\n\n1050\n\nwavelength\n\nFigure 2: Segmentation of a single function: original function on the left, segment positions\non the right\n\n2.4. Algorithmic cost\nGiven all the Q (s, {tk , . . . , tl }), Algorithm 1 runs in O(P M 2 ). This is far\nmore efficient that a naive approach in which all possible ordered partitions\n1 Data\n\nare available on statlib at http://lib.stat.cmu.edu/datasets/tecator and consist\nin near infrared absorbance spectrum of meat samples recorded on a Tecator Infratec Food\nand Feed Analyzer.\n\n7\n\n\f3.4\nabsorbance\n\n2.8\n\n3.0\n\n3.2\n\n3.4\n3.2\nabsorbance\n\n3.0\n2.8\n850\n\n900\n\n950\n\n1000\n\n1050\n\n850\n\nwavelength\n\n900\n\n950\n\n1000\n\n1050\n\nwavelength\n\nFigure 3: Piecewise constant approximations with 4 segments on the left and 6 segments on\nthe right\n!\nwould be evaluated (there are (MM\n\u2212P )! such partitions). However, an efficient\nimplementation is linked to a fast evaluation of the Q (s, {tk , . . . , tl }). For instance, a naive implementation based on equation (6) leads to a O(M 3 ) cost\nthat dominates the cost of Algorithm 1.\nFortunately, recursive formulation can be used to reduce in some cases the\ncomputation cost associated to the Q (s, {tk , . . . , tl }) to O(M 2 ). As an illustration, Algorithm 2 computes Q for equation (6) (piecewise constant approximation) in O(M 2 ). Similar algorithms can be derived for other choices of the\n\nAlgorithm 2 Recursive calculation of Q (s, {tk , . . . , tl })\n1: for k = 1 to M do\n2:\n\u03bc(s, {tk }) \u2190 s(tk )\n3:\nQ(s, {tk }) \u2190 0\n4: end for\n5: for l = 2 to M do\n6:\n\u03bc(s, {t1 , . . . , tl }) \u2190 1l [(l \u2212 1)\u03bc(s, {t1 , . . . , tl\u22121 }) + s(tl )]\n2\nl\n7:\nQ(s, {t1 , . . . , tl }) \u2190 Q(s, {t1 , . . . , tl\u22121 }) + l\u22121\n[s(tl ) \u2212 \u03bc(s, {t1 , . . . , tl })]\n8: end for\n9: for k = 2 to M \u2212 1 do\n10:\nfor l = k + 1 to M do\n1\n11:\n\u03bc(s, {tk , . . . , tl }) \u2190 l\u2212k+1\n[(l \u2212 k + 2)\u03bc(s, {tk\u22121 , . . . , tl }) \u2212 s(tk\u22121 )]\nl\u2212k+1\n12:\nQ(s, {tk , . . . , tl })\n\u2190\nQ(s, {tk\u22121 , . . . , tl }) \u2212 l\u2212k+2\n[s(tk\u22121 ) \u2212\n2\n\u03bc(s, {tk , . . . , tl })]\n13:\nend for\n14: end for\napproximation solution used in each segment. The memory usage can also be\nreduced from O(M 2 ) in Algorithms 1 and 2 to O(M ) (see, e.g., [21, 22]).\n2.5. Extensions and variations\nAs mentioned before, the general framework summarized in equation (9) is\nvery flexible and accommodates numerous variations. Those variations include\nthe approximation model (constant, linear, peak, etc.), the quality criterion\n(quadratic, absolute value, robust Huber loss [14], etc.) and the aggregation\noperator (sum or maximum of the point wise errors).\n8\n\n\f3.4\n3.2\nabsorbance\n\n3.0\n2.6\n\n2.8\n\n3.0\n2.6\n\n2.8\n\nabsorbance\n\n3.2\n\n3.4\n\nFigure 4 gives an application example: the spectrum from Figure 2 is approximated via a piecewise linear representation (on the right hand side of the\nFigure): as expected, the piecewise linear approximation is more accurate than\nthe piecewise constant one, while the former uses less numerical parameters\nthan the latter. Indeed, the piecewise linear approximation is easier to describe\n\n850\n\n900\n\n950\n\n1000\n\n1050\n\n850\n\nwavelength\n\n900\n\n950\n\n1000\n\n1050\n\nwavelength\n\nFigure 4: Segmentation of a single function: piecewise constant with 10 segments (left) versus\npiecewise linear with 5 segments (right)\n\nthan the piecewise constant approximation: one needs only to specify the 4\nbreakpoints between the segments together with 10 extremal values, while the\npiecewise constant approximation needs 10 numerical values together with 9\nbreakpoints.\nHowever, this example is particularly favorable for the piecewise linear approximation. Indeed, the independence hypothesis embedded in equation (9)\nand needed for the dynamic programming approach prevents us from introducing non local constraints in the approximating function. In particular, one\ncannot request for the piecewise linear approximation to be continuous. This\nis emphasized on Figure 4 by the disconnected drawing of the approximation\nfunction: in fact, the function g is not defined between tk and tk+1 if those two\npoints belong to distinct clusters (in the case of Bellman's solution [3], the function is not well defined at the boundaries of the intervals). Of course, one can\nuse linear interpolation to connect g(tk ) with g(tk+1 ), but this can introduce\nin practice P \u2212 1 additional very short segments, leading to an overly complex\nmodel.\nIn the example illustrated on Figure 4, the piecewise approximation does\nnot suffer from large jumps between segments. Would that be the case, one\ncould use an enriched description of the function specifying the starting and\nending values on each segment. This would still be simpler than the piecewise\nconstant description. However, if the original function is continuous, using a\nnon continuous approximation can lead to false conclusion.\nTo illustrate the flexibility of the proposed framework beyond the simple\nvariations listed above, we derive a solution to this continuity problem [22]. Let\nus first define\n\u00132\nl \u0012\nX\n(s(tk ) \u2212 s(tl ))t + (tk s(tl ) \u2212 tl s(tk ))\nQ(s, {tk , . . . , tl }) =\ns(tj ) \u2212\n. (12)\ntk \u2212 tl\nj=1\nThis corresponds to the total quadratic error made by the linear interpolation of\ns on {tk , . . . , tl } based on the two interpolation points (tk , s(tk )) and (tl , s(tl )).\n9\n\n\fThen we replace the quality criterion of equation (9) by the following one:\nE((kl )P\nl=0 ) =\n\nP\nX\n\nQ(s, {tkl\u22121 , . . . , tkl }),\n\n(13)\n\nl=1\n\n3.4\n3.2\nabsorbance\n\n3.0\n2.6\n\n2.8\n\n3.0\n2.6\n\n2.8\n\nabsorbance\n\n3.2\n\n3.4\n\ndefined for an increasing series (kl )P\nl=0 with k0 = 1 and kP = M . Obviously,\nE((kl )P\n)\nis\nthe\ntotal\nquadratic\nerror\nmade by the piecewise linear interpolation\nl=0\nof s on [t1 , tM ] based on the P + 1 interpolation points (tkl , s(tkl ))P\nl=0 . While\n(kl )P\ndoes\nnot\ncorrespond\nstrictly\nto\na\npartition\nof\n{t\n,\n.\n.\n.\n,\nt\n}\n(because\neach\n1\nM\nl=0\ninterpolation point belongs to two segments, expect for the first and the last\nones), the criterion E is additive and has to be optimized on an ordered structure. As such, it can be minimized by a slightly modified version of Algorithm\n1. Figure 5 shows the differences between the piecewise linear approach and the\n\n850\n\n900\n\n950\n\n1000\n\n1050\n\n850\n\nwavelength\n\n900\n\n950\n\n1000\n\n1050\n\nwavelength\n\nFigure 5: Segmentation of a single function: piecewise linear with 5 segments (left) versus\nlinear interpolation with 5 segments (right)\n\nlinear interpolation approach. For continuous functions, the latter seems to be\nmore adequate than the former: it does not introduce arbitrary discontinuity\nin the approximating function and provides a simple summary of the original\nfunction.\nThe solutions explored in this section target specifically the summarizing\nissue by providing simple approximating function. As explained in Section 2.1,\nthis rules out smooth approximation. In other applications context not addressed here, one can trade simplicity for smoothness. In [4], a model based\napproach is used to build a piecewise polynomial approximation of a function.\nThe algorithm provides a segmentation useful for instance to identify underlying\nregime switching as well as a smooth approximation of the original noisy signal. While the method and framework are related, they oppose on the favored\nquality: one favors smoothness, the other favors simplicity.\n3. Summarizing several functions\nM\nLet us now consider N functions (si )N\ni=1 sampled in M distinct points (tk )k=1\nfrom the interval [t1 , tM ] (as in Section 2, the points are assumed to be ordered).\nOur goal is to leverage the function summarizing framework presented in Section\n2 to build a summary of the whole set of functions. We first address the case of\na homogeneous set of functions and tackle after the general case.\n\n10\n\n\f3.1. Single summary\nHomogeneity is assumed with respect to the chosen functional metric, i.e.,\nto the error criterion used for segmentation in Section 2, for instance the L2 .\nThen a set of functions is homogeneous if its diameter is small compared to the\ntypical variations of a function from the set. In the quadratic case, we request\nfor instance to the maximal quadratic distance between two functions of the set\nto be small compared to the variances of functions of the set around their mean\nvalues.\nThen, if the functions (si )N\ni=1 are assumed to be homogeneous, finding a\nsingle summary function seems natural. In practice, this corresponds to finding\na simple function g that is close to each of the si as measured by a functional\nnorm: one should choose a summary type (e.g., piecewise constant), a norm\n(e.g., L2 ) and a way to aggregate the individual comparison of each function to\nthe summary (e.g., the sum of the norms).\nLet us first consider the case of a piecewise constant function g defined by P\nP\nintervals (Ip )P\np=1 (a partition of [t1 , tM ]) and P values (ap )p=1 (with g(t) = ap\nwhen t \u2208 Ip ). If we measure the distance between g and si via the L2 distances\nand consider the sum of L2 distances as the target error measure, we obtain the\nfollowing segmentation error:\nP\nP\n(si )N\ni=1 , (Ip )p=1 , (ap )p=1\n\nE\n\n\u0001\n\n=\n\nN X\nP X\nX\n\n2\n\n(si (tk ) \u2212 ap ) ,\n\n(14)\n\ni=1 p=1 tk \u2208Ip\n\nusing the same quadrature scheme as in Section 2.2. Huygens' theorem gives\nN\nX\n\n2\n\n2\n\n(si (tk ) \u2212 ap ) = N (\u03bc(tk ) \u2212 ap ) +\n\ni=1\n\nN\nX\n\n2\n\n(si (tk ) \u2212 \u03bc(tk )) ,\n\n(15)\n\ni=1\n\nPN\nwhere \u03bc = N1 i=1 si is the mean function. Therefore minimizing E from\nequation (14) is equivalent to minimizing\nP X\n\u0001 X\n2\nP\nP\nE 0 (si )N\n,\n(I\n)\n,\n(a\n)\n=\n(\u03bc(tk ) \u2212 ap ) .\np p=1\np p=1\ni=1\n\n(16)\n\np=1 tk \u2208Ip\n\nIn other words, the problem consists simply in building an optimal summary\nof the mean curve of the set and is solved readily with Algorithm 1. The\nadditional computational cost induced by the calculation of the mean function\nis in O(N M ). This will generally be negligible compared to the O(P M 2 ) cost\nof the dynamic programming.\nThe reasoning above applies more generally to piecewise models used with\nthe sum of L2 distances. Indeed if g(t) is such that g(t) = gp (t) when t \u2208 Ip ,\nthen we have\nN\nX\n\n2\n\n2\n\n(si (tk ) \u2212 gp (tk )) = N (\u03bc(tk ) \u2212 gp (tk )) +\n\ni=1\n\nN\nX\n\n2\n\n(si (tk ) \u2212 \u03bc(tk )) ,\n\n(17)\n\ni=1\n\nwhich leads to the same solution: g can be obtained as the summary of the\nmean curve. The case of linear interpolation proposed in Section 2.5 is a bit\ndifferent in the sense that interpolating several curves at the same time is not\n11\n\n\f5.5\n5.0\n4.5\n\nabsorbance\n\n4.0\n3.5\n850\n\n900\n\n950\n\n1000\n\n1050\n\nwavelength\n\nFigure 6: Group summary via an linear interpolation of the mean; the grey dashed/dotted\ncurves are the original ones, the grey solid curve is the mean curve and the black curve is the\npiecewise linear interpolation curve that provides the group summary\n\npossible. Therefore, the natural way to extend the proposal to a set of curves\nit to interpolate the mean curve. An example of the result obtained by this\nmethod is given in Figure 6.\nIn the general case, we assume given an error criterion Q that applies to\na set of contiguous evaluation points {tk , . . . , tl }: it gives the error done by\nthe local model on {tk , . . . , tl } as an approximation of all the functions (si )N\ni=1 .\nThen given an ordered partition of {t1 , . . . , tM }, (Cp )P\n,\nwe\nuse\nthe\nsum\nof\np=1\nthe Q((si )N\n,\nC\n)\nas\na\nglobal\nerror\nmeasure\nand\nobtain\nan\nadditive\nform\nas\nin\np\ni=1\nequation (9). Algorithm 1 applies straightforwardly (it applies also to taking,\ne.g., the maximum of the Q((si )N\ni=1 , Cp ) over p).\nThe only difficulty compared to the case of a unique function is the efficient\ncalculation of the Q((si )N\ni=1 , {tk , . . . , tl }). As shown above, the L2 case is very\nfavorable as it amounts to summarizing the mean curve. However, more complex\nnorms and/or aggregation rules lead to difficulties. If we use for instance\nQ((si )N\ni=1 , {tk , . . . , tl }) = min max\na\n\n1\u2264i\u2264N\n\nl\nX\n\n(si (tj ) \u2212 a)2 ,\n\n(18)\n\nj=k\n\nthe problem cannot be formulated as a single curve summarizing problem, while\nis consists simply in considering the maximal L2 distance rather than the sum\nof the L2 distances: if the curves have different variances on {tk , . . . , tl }, the\noptimal value a will not be the mean on the set of evaluation points of the\nmean curve. More generally, the computation of all Q((si )N\ni=1 , {tk , . . . , tl }) might\nscale much faster than in O(M (M + N )) and thus might dominate the total\ncomputation time. Indeed, if there is no closed form expression for Q, then the\nvalue has to be computed from scratch for each pair (k, l), which implies O(M 2 )\nproblem solving. Then, if no special structure or preprocessing can be used to\n12\n\n\fsimplify the problem, computing one value of Q implies clearly to look at all the\ninvolved functions, i.e, to O(N ) observations. Therefore, if no simplification can\nbe done, computing all values of Q will imply at least at O(N M 2 ) calculation\ncost. Care must therefore be exercised when choosing the quality criterion\nand the summarizing model to avoid excessive costs. That said, the dynamic\nprogramming approach applies and leads to an optimal segmentation according\nto the chosen criterion.\n3.2. Clustering\nAssuming that a set of functions is homogeneous is too strong in practice,\nas shown on Figure 6: some of the curves have a small bump around 940 nm\nbut the bump is very small in the mean curve. As a consequence, the summary\nmisses this feature while the single curve summary used in Figure 5 was clearly\npicking up the bump.\nIt is therefore natural to rely on a clustering strategy to produce homogeneous clusters of curves and then to apply the method described in the previous\nsection to summarize each cluster. However, the direct application of this strategy might lead to suboptimal solutions: at better solution should be obtained\nby optimizing at once the clusters and the associated summaries. We derive\nsuch an algorithm in the present section.\nIn a similar way to the previous section, we assume given an error measure\nQ(G, {tk , . . . , tl }) (where G is a subset of {1, . . . , N }): it measures the error\nmade by the local model on the {tk , . . . , tl } as an approximation of the functions\nsi for i \u2208 G. For instance\nQ(G, {tk , . . . , tl }) = min\na\n\nl\nXX\n(si (tj ) \u2212 a)2 ,\n\n(19)\n\ni\u2208G j=k\n\nfor a constant local model evaluated with respect to the sum of L2 distances. Let\nus assume for now that each cluster is summarized via a segmentation scheme\nthat uses a cluster independent number of segments, P/K (this value is assumed\nto be an integer). Then, given a partition of {1, . . . , N } into K clusters (Gk )K\nk=1\nP/K\nand given for each cluster an ordered partition of {t1 , . . . , tM }, (Cpk )p=1 , the\nglobal error made by the summarizing scheme is\n\u0012\nK P/K\n\u0010\n\u0011K \u0013 X\nX\nK\nk P/K\nE (si )N\n,\n(G\n)\n,\n(C\n)\n=\nQ(Gk , Cpk ).\nk k=1\ni=1\np p=1\nk=1\n\n(20)\n\nk=1 p=1\n\nBuilding an optimal summary of the set of curves amounts to minimizing this\nglobal error with respect to the function partition and, inside each cluster, with\nrespect to the segmentation.\nA natural way to tackle the problem is to apply an alternating minimization scheme (as used for instance in the K-means algorithm): we optimize successively the partition given the segmentation and the segmentation given the\npartition. Given the function partition (Gk )K\nk=1 , the error is a sum of independent terms: one can apply dynamic programming to build an optimal summary\nof each of the obtained clusters, as shown in the previous section. Optimizing\nwith respect to the partition given the segmentation is a bit more complex as\n\n13\n\n\fthe feasibility of the solution depends on the actual choice of Q. In general however, Q is built by aggregating via the maximum or the sum operators distances\nbetween the functions assigned to a cluster and the corresponding summary.\nIt is therefore safe to assume that assigning a function to the cluster with the\nclosest summary in term of the distance used to define Q will give the optimal\npartition given the summaries. This leads to Algorithm 3. The computational\nAlgorithm 3 Simultaneous clustering and segmentation algorithm (uniform\ncase)\n1:\n2:\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n\ninitialize the partition (Gk )K\nk=1 randomly\nrepeat\nfor k = 1 to K do\napply Algorithm 1 to the find the optimal summary gk of the set of\ncurves {si |i \u2208 Gk } in P/K segments\nend for\nfor i = 1 to N do\nassign si to Gk such that the distance between si and gk is minimal\nover k\nend for\nuntil (Gk )K\nk=1 is stable\n\ncost depends strongly on the availability of an efficient calculation scheme for\nQ. If this is the case, then the cost of the K dynamic programming applications\ngrows in O(K(P/K)M 2 ) = O(P M 2 ). With standard Lp distances between\nfunctions, the cost of the assignment phase is in O(N KM ). Computing the\nmean function in each cluster (or similar quantities such as the median function\nthat might be needed to compute efficiently Q) has a negligible cost compared\nto the other costs (e.g., O(N M ) for the mean functions).\nConvergence of Algorithm 3 is obvious as long as both the assignment phase\n(i.e., the minimization with respect to the partition given the segmentation) and\nthe dynamic programming algorithm use deterministic tie breaking rules and\nlead to unique solutions. While the issue of ties between prototypes is generally\nnot relevant in the K-means algorithm, for instance, it plays an important role in\nour context. Indeed, the summary of a cluster can be very simple and therefore,\nseveral summaries might have identical distance to a given curve. Then if ties\nare broken at random, the algorithm might never stabilize. Algorithm 1 is\nformulated to break ties deterministically by favoring short segments at the\nbeginning of the segmentation (this is implied by the strict inequality test used\nat line 10 of the Algorithm). Then the optimal segmentation is unique given\nthe partition, and the optimal partition is unique given the segmentation. As a\nconsequence, the alternative minimization scheme converges.\nFigure 7 gives an example of the results obtained by the algorithm in the case\nof a piecewise linear summary with five segments per cluster (the full Tecator\ndataset contains N = 240 spectra with M = 100 for each spectrum).\nIn practice, we will frequently use a piecewise constant model whose quality\nis assessed via the sum of the L2 distances, as in equation (19). Then equation\n\n14\n\n\f1050\n\n950\n\n1000\n\n1050\n\n5.5\n4.5\n4.0\n3.5\n3.0\n2.5\n2.0\n850\n\n900\n\n850\n\n900\n\n950\n\n1000\n\n1050\n\n950\n\n1000\n\n1050\n\n900\n\n850\n\n900\n\n950\n\n1000\n\n1050\n\n950\n\n1000\n\n1050\n\n5.0\n2.0\n\n2.5\n\n3.0\n\n3.5\n\n4.0\n\n4.5\n\n5.0\n2.0\n\n2.5\n\n3.0\n\n3.5\n\n4.0\n\n4.5\n\n5.0\n4.5\n4.0\n3.5\n2.5\n2.0\n\n850\n\n5.5\n\n900\n\n1000\n\n5.5\n850\n\n950\n\n3.0\n\nabsorbance\n\n5.0\n\n5.5\n2.0\n\n2.5\n\n3.0\n\n3.5\n\n4.0\n\n4.5\n\n5.0\n\n5.5\n5.0\n4.5\n4.0\n3.5\n\nabsorbance\n\n3.0\n2.5\n2.0\n\n900\n\n5.5\n\n850\n\nwavelength\n\nwavelength\n\nwavelength\n\nFigure 7: Summary of the full Tecator dataset with 6 clusters and 5 segments per cluster:\ngrey curves are the original spectra, dark curves are the summaries for each cluster\n\n(20) can be instantiated as follows:\n\u0012\nK X P/K\n\u0010\n\u0011K \u0013 X\nX X\n2\nK\nk P/K\nE (si )N\n,\n(G\n)\n,\n(C\n)\n=\n(si (tj ) \u2212 \u03bck,p ) ,\nk\ni=1\nk=1\np p=1\nk=1\n\nk=1 i\u2208Gk p=1 tj \u2208Cpk\n\n(21)\nwith\n\u03bck,p =\n\nX X\n1\nsi (tj ).\n|Gk ||Cpk |\nk\n\n(22)\n\ni\u2208Gk tj \u2208Cp\n\nLet us denote si = (si (t1 ), . . . , si (tM )) the RM vector representation of the\nsampled functions and gk = (gk (t1 ), . . . , gk (tM )) the vector representation of\nthe piecewise constant functions defined by gk (ti ) = \u03bck,p when ti \u2208 Cpk . Then\nequation (21) can be rewritten\nK X\n\u0010\n\u0001K \u0011 X\nK\nk P\nE (si )N\n,\n(G\n)\n,\n(C\n)\n=\nksi \u2212 gk k2 .\nk k=1\ni=1\np p=1 k=1\n\n(23)\n\nk=1 i\u2208Gk\n\nThis formulation emphasizes the link between the proposed approach and the\nK means algorithm. Indeed the criterion is the one optimized by the K means\nalgorithm for the vector data si with a constraint on the prototypes: rather\nthan allowing arbitrary prototypes as in the standard K means, we use simple\nprototypes in the sense of Section 2.1. Any piecewise model whose quality is\nassessed via the sum of the L2 distances leads to the same formulation, but with\ndifferent constraints on the shape of the functional prototypes.\nEquation (23) opens the way to many variations on the same theme: most of\nthe prototype based clustering methods can be modified to embed constraints\n15\n\n\fon the prototypes, as Algorithm 3 does for the K means. Interesting candidates\nfor such a generalization includes the Self Organizing Map in its batch version\n[18], Neural Gas also in its batch version [7] and deterministic annealing based\nclustering [27], among others.\n3.3. Optimal resources allocation\nA drawback of Algorithm 3 is that it does not allocate resources in an optimal\nway: we assumed that each summary will use P/K segments, regardless of the\ncluster. Fortunately, dynamic programming can be used again to remove this\nconstraint. Let us considerPindeed a resource assignment, that is K positive\nK\nintegers (Pk )K\nk=1 such that\nk=1 Pk = P . The error made by the summarizing\nscheme when cluster Gk uses Pk segments for its summary is given by\n\u0012\nE\n\nK\nK\n(si )N\ni=1 , (Gk )k=1 , (Pk )k=1 ,\n\n\u0010\n\nk\n(Cpk )P\np=1\n\n\u0011K \u0013\n\n=\n\nk=1\n\nPk\nK X\nX\n\nQ(Gk , Cpk ).\n\n(24)\n\nk=1 p=1\n\nAs for the criterion given by equation (20) in the previous section, we rely on\nan alternating minimization scheme. Given the function partition (Gk )K\nk=1 , the\nchallenge is to optimize E with respect to both the resource assignment and the\nsegmentation. For a fixed partition (Gk )K\nk=1 , let us denote\nRk (Pk ) =\n\nPk\nX\n\nmin\n\nP\n\nk ordered partition of {t ,...,t }\n(Cpk )p=1\n1\nM p=1\n\nQ(Gk , Cpk )\n\n(25)\n\nThen minimizing E from equation (24) for a fixed partition (Gk )K\nk=1 corresponds\nto minimizing:\nK\n\u0001 X\nK\nRk (Pk )\n(26)\nU (Pk )k=1 =\nk=1\n\nwith respect to the resource assignment. This formulation emphasizes two major\npoints.\nFirstly, as already mentioned in Section 2.3, given a number of segments P\nAlgorithm 1 and its variants provide with a negligible additional cost all the\noptimal segmentations in p = 1 to P segments. Therefore, the calculation of\nRk (p) for p \u2208 {1, . . . , P } can be done with a variant of Algorithm 1 in O(P M 2 )\nprovided an efficient calculation of Q(Gk , Cpk ) is possible (in fact, one needs only\nvalues of Rk (p) for p \u2264 P \u2212 K + 1 as we request at least one segment per cluster,\nbut to ease the calculation we assume that we compute values up to P ).\nSecondly, U as defined in equation (26) is an additive measure and can\ntherefore be optimized by dynamic programming. Indeed let us define S(l, p) as\nS(l, p) =\n\nmin\n(Pk )lk=1 such that\n\nl\nX\nPl\n\nk=1\n\nPk =p\n\nRk (Pk )\n\n(27)\n\nk=1\n\nThe S(1, p) are readily obtained from the optimal segmentation algorithm\n(S(1, p) = R1 (p)). Then the additive structure of equation (26) shows that\nS(l, p) =\n\nmin\n\n1\u2264u\u2264p\u2212l+1\n\nS(l \u2212 1, p \u2212 u) + Rl (u).\n\n16\n\n(28)\n\n\fAlgorithm 4 Simultaneous clustering and segmentation algorithm with optimal\nresource assignment\n1: initialize the partition (Gk )K\nk=1 randomly\n2: repeat\n3:\nfor k = 1 to K do\n4:\napply Algorithm 1 to compute Rk (p) for all p \u2208 {1, . . . , P \u2212 K + 1}\n5:\nend for\n6:\nfor p = 1 to P \u2212K +1 do {initialisation of the resource allocation phase}\n7:\nS(1, p) \u2190 R1 (p)\n8:\nend for\n9:\nfor l = 2 to K do {dynamic programming for resource allocation}\n10:\nfor p = l to P do\n11:\nW (l, p) \u2190 1\n12:\nS(l, p) \u2190 S(l \u2212 1, p \u2212 1) + Rl (1)\n13:\nfor u = 2 to p \u2212 l + 1 do\n14:\nif S(l \u2212 1, p \u2212 u) + Rl (u) < S(l, p) then\n15:\nW (l, p) \u2190 u\n16:\nS(l, p) \u2190 S(l \u2212 1, p \u2212 u) + Rl (u)\n17:\nend if\n18:\nend for\n19:\nend for\n20:\nend for\n21:\nPopt \u2190 (NA, . . . , NA) {backtracking phase}\n22:\nPopt (K) \u2190 W (K, P )\n23:\navailabe \u2190 P \u2212 W (K, P )\n24:\nfor k = K \u2212 1 to 1 do\n25:\nPopt (k) \u2190 W (k, available)\n26:\navailabe \u2190 availabe \u2212 W (k, available)\n27:\nend for\n28:\nfor k = 1 to K do {summary calculation}\n29:\nbuild gk with Popt (k) segments\n30:\nend for\n31:\nfor i = 1 to N do\n32:\nassign si to Gk such that the distance between si and gk is minimal\nover k\n33:\nend for\n34: until (Gk )K\nk=1 is stable\n\n17\n\n\fGiven all the Rl (p) the calculation of S(K, P ) has therefore a cost of O(KP 2 ).\nThis calculation is plugged into Algorithm 3 to obtain Algorithm 4.\nGiven the values of Q, the total computational cost of the optimal resource\nassignment and of the optimal segmentation is therefore in O(KP (M 2 + P )).\nAs we target simple descriptions, it seems reasonable to assume that P \u001c\nM 2 and therefore that the cost is dominated by O(KP M 2 ). Then, using the\noptimal resource assignment multiplies by K the computational cost compared\nto a uniform use of P/K segments per cluster (which computational cost is in\nO(P M 2 )). The main source of additional computation is the fact that one has\nto study summaries using P segments for each cluster rather than only P/K\nin the uniform case. This cost can be reduced by introducing an arbitrary\n(user chosen) maximal number of segments in each clusters, e.g., \u03bbP/K. Then\nthe running time of the optimal assignment is \u03bb times the running time of the\nuniform solution. As we aim at summarizing the data, using a small \u03bb such 2\nseems reasonable.\nThe computation of the values of Q remains the same regardless of the way\nthe number of segments is chosen for each cluster. The optimisation of the\nerror with respect to the partition (Gk )K\nk=1 follows also the same algorithm in\nboth cases. If we use for instance piecewise constant approximation evaluated\nwith the sum of the L2 distances, then, as already mentioned previously, the\nsummaries are computed from the cluster means: computing the means costs\nO(N M ), then the calculation of Q for all functional clusters is in O(KM 2 )\nand the optimization with respect to (Gk )K\nk=1 is in O(N KM ). In general, the\ndominating cost will therefore scale in O(KP M 2 ). However, as pointed out\nin Section 3.1, complex choices for the Q error criterion may induce larger\ncomputational costs.\n3.4. Two phases approaches\nThe main motivation of the introduction of Algorithm 3 was to avoid relying on a simpler but possibly suboptimal dual phases solution. This alternative\nsolution simply consists applying a standard K means to the vector representation of the functions (si )N\ni=1 to get homogeneous clusters and then to summarize\neach cluster via a simple prototype. While this approach should intuitively give\nhigher errors for the criterion of equation (20), the actual situation is more\ncomplex.\nAlgorithms 3 and 4 are both optimal in their segmentation phase and will\ngenerally be also optimal for the partition determination phase if Q is a standard\ncriterion. However, the alternating optimization scheme itself is not optimal (the\nK means algorithm is also non optimal for the same reason). We have therefore\nno guarantee on the quality of the local optimum reached by those algorithms,\nexactly as in the case of the K means. In addition, even if the two phase\napproach is initialized with the same random partition as, e.g., Algorithm 3, the\nfinal partitions can be distinct because of the distinct prototypes used during\nthe iterations. It is therefore difficult to predict which approach is more likely to\ngive the best solution. In addition a third possibility should be considered: the K\nmeans algorithm is applied to the vector representation of the functions (si )N\ni=1\nto get homogeneous clusters and is followed by an application of Algorithm 3\nor 4 using the obtained clusters for its initial configuration.\nTo assess the differences between these three possibilities, we conducted a\nsimple experiment: using the full Tecator dataset, we compared the value of\n18\n\n\fE from equation (20) for the final summary obtained by the three solutions\nusing the same random starting partitions with K = 6, P = 30 and piecewise\nconstant summaries. We used 50 random initial configurations and compared\nthe methods in the case of uniform resource allocation (5 segments per cluster)\nand in the case of optimal resource allocation.\nIn the case of uniform resources assignment the picture is rather simple. For\nall the 50 random initialisations both two phases algorithms gave exactly the\nsame results. In 44 cases, using Algorithm 3 gave exactly the same results as\nthe two phases algorithms. In 5 cases, its results were of lower quality and in\nthe last case, its results were slightly better. The three algorithms manage to\nreach the same overall optimum (for which E = 472).\nThe case of optimal resources assignment is a bit more complex. For 15\nout of 50 initial configurations, using Algorithm 4 after the K means improves\nthe results over a simple optimal summary (with optimal resource assignment)\napplied to the results of the K means (results were identical for the 35 other\ncases). In 34 cases, K means followed by Algorithm 4 gave the same results as\nAlgorithm 4 alone. In the 16 remaining cases, results are in favor of K means\nfollowed by Algorithm 4 which wins 13 times. Nevertheless, the three algorithms\nmanage to reach the same overall optimum (for which E = 467).\nWhile there is no clear winner in this (limited scope) experimental analysis,\nconsidering the computational cost of the three algorithms helps choosing the\nmost appropriate one in practice. In favorable cases, the cost of an iteration of\nAlgorithm 4 is O(KM N +KP M 2 )) while the cost of an iteration of the K means\non the same dataset is O(KM N ). In functional data, M is frequently of the\nsame order as N . For summarizing purposes, P should remain small, around 5K\nfor instance. This means that one can perform several iterations of the standard\nK means for the same computational price than one iteration of Algorithm 4.\nFor instance, the Tecator dataset values, P = 30, M = 100 and N = 240, give\n1 + P M/N = 13.5. In the experiments described above, the median number\nof iterations for the K means was 15 while it was respectively 15 and 16 for\nAlgorithms 3 and 4. We can safely assume therefore that Algorithm 4 is at\nleast 10 times slower than the K means on the Tecator dataset. However, when\ninitialized with the K means solution, Algorithms 3 and 4 converges very quickly,\ngenerally in two iterations (up to five for the optimal assignment algorithm in\nthe Tecator dataset). In more complex situations, as the ones studied in Section\n4 the number of iterations is larger but remains small compared to what would\nbe needed when starting from a random configuration.\nBased on the previous analysis, we consider that using the K means algorithm followed by Algorithm 4 is the best solution. If enough computing resources are available, one can run the K means algorithm from several starting\nconfigurations, keep the best final partition and use it as an initialization point\nfor Algorithm 4. As the K means provides a good starting point to this latter\nalgorithm, the number of iterations remains small and the overall complexity\nis acceptable, especially compared to using Algorithm 4 alone from different\nrandom initial configurations. Alternatively, the analyst can rely on more robust clustering scheme such as Neural Gas [7] and deterministic annealing based\nclustering [27], or on the Self Organizing Map [18] to provide the starting configuration of Algorithm 4.\n\n19\n\n\f4. Experimental results\nWe give in this section two examples of the type of exploratory analyses that\ncan be performed on real world datasets with the proposed method. In order to\nprovide the most readable display for the user, we build the initial configuration\nof Algorithm 4 with a batch Self Organizing Map. We optimize the initial radius\nof the neighborhood influence in the SOM with the help of the Kaski and Lagus\ntopology preservation criterion [17] (the radius is chosen among 20 radii).\n4.1. Topex/Poseidon satellite\nWe study first the Topex/Poseidon satellite dataset2 . The Topex/Poseidon\nradar satellite has been used over the Amazonian basin to produce N = 472\nwaveforms sampled at M = 70 points. The curves exhibit a quite large variability induced by differences in ground type below the satellite during data\nacquisition. Details on the dataset can be found in e.g. [11, 12]. Figure 8\ndisplays 20 curves from the dataset chosen randomly.\n\nFigure 8: 20 curves from the Topex/Poseidon radar satellite dataset\n\nWe conduct first a hierarchical clustering (based on Euclidean distance between the functions and the Ward criterion) to get a rough idea of the potential\nnumber of clusters in the dataset. Both the dendrogram (Figure 9) and the\ntotal within class variance evolution (Figure 10) tend to indicate a rather small\nnumber of clusters, around 12. As the visual representation obtained via the\nSOM is more readable than a randomly arranged set of prototypes, we can use\na slightly oversized SOM without impairing the analyst's work. We decided\ntherefore to use a 4 \u00d7 5 grid: the rectangular shape has been preferred over a\n2 Data\n\nare available at http://www.lsp.ups-tlse.fr/staph/npfda/npfda-datasets.html\n\n20\n\n\f40000\n30000\n20000\n\nHeight\n\n10000\n0\n0\n\n10000\n\n20000\n\n30000\n\nFigure 9: Dendrogram of the hierarchical clustering for the Topex/Poseidon radar satellite\ndataset\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10 11 12 13 14 15 16 17 18 19 20\n\nFigure 10: Total within class variance decrease for the 20 first merges in the dendrogram\ndepicted by Figure 9\n\n21\n\n\fsquare one according to results from [34] that show some superiority in topology\npreservation for anisotropic maps compared to isotropic ones.\n\nFigure 11: Prototypes obtained by SOM algorithm on the Topex/Poseidon radar satellite\ndataset (grey curves are the original curves as assigned to the clusters)\n\nThe results of the SOM are given in Figure 11 that displays the prototype\nof each cluster arranged on the SOM grid. Each cell contains also all the curves\nassigned to the associated clusters. As expected, the SOM prototypes are well\norganized on the grid: the horizontal axis seems to encode approximately the\nposition of the maximum of the prototype while the vertical axis corresponds\nto the width of the peak. However, the prototypes are very noisy and inferring\nthe general behavior of the curves of a cluster remains difficult.\n\n22\n\n\fFigure 12: Summarized prototypes obtained by Algorithm 4 on the Topex/Poseidon radar\nsatellite dataset (grey curves are the original curves as assigned to the clusters)\n\n23\n\n\fFigure 12 represents the results obtained after applying Algorithm 4 to the\nresults of the SOM. In order to obtain a readable summary, we set P to 80,\ni.e., an average of 4 segments for each cluster. We use the linear interpolation\napproach described in Section 2.5. Contrarily to results obtained on the Tecator\ndataset and reported in Section 3.4, Algorithm 4 used a significant number\nof iterations (17). Once the new clusters and the simplified prototypes were\nobtained, they were displayed in the same way as for the SOM results. The\nresulting map is almost as well organized as the original one (with the exception\nof a large peak on the bottom line), but the simplified prototypes are much more\ninformative than the original ones: they give an immediate idea of the general\nbehavior of the curves in the corresponding cluster. For example, the differences\nbetween the two central prototypes of the second line (starting from the top)\nare more obvious with summaries: in the cluster on the left, one can identify\na slow increase followed by a two phases slow decrease, while the other cluster\ncorresponds to a sharp increase followed by a slow decrease. Those differences\nare not as obvious on the original map.\n\n0\n\n2\n\n4\n\n6\n\n8\n\n4.2. Electric power consumption\nThe second dataset3 consists in the electric power consumption recorded in\na personal home during almost one year (349 days). Each curve consists in\n144 measures which give the power consumption of one day at a 10 minutes\nsampling rate. Figure 13 displays 20 randomly chosen curves.\n\n0\n\n5\n\n10\n\n15\n\n20\n\nh\n\nFigure 13: 20 electric power consumption curves\n\nWe analyse the curves in a very similar way as for the Topex/Poseidon\ndataset The results of the hierarchical clustering displayed by Figures 14 and\n3 This\n\ndataset is available at http://bilab.enst.fr/wakka.php?wiki=HomeLoadCurve.\n\n24\n\n\f300\n200\n0\n\n100\n\nHeight\n\n400\n\n500\n\n15 lead to the same conclusion as in the previous case: the number of clusters\nseems to be small, around 13. As before, we fit a slightly larger SOM (4 \u00d7 5\nrectangular grid) to the data and obtain the representation provided by Figure\n16. The map is well organized: the vertical axis seems to encode the global\npower consumption while the horizontal axis represents the overall shape of\nthe load curve. However, the shapes of the prototypes are again not easy to\ninterpret, mainly because of their noisy nature.\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\nFigure 14: Dendrogram of the hierarchical clustering for the load curves\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10 11 12 13 14 15 16 17 18 19 20\n\nFigure 15: Total within class variance decrease for the 20 first merges in the dendrogram\ndepicted by Figure 14\n\nFigure 17 shows the results obtained after applying Algorithm 4 to the results of the SOM (Algorithm 4 used 9 iterations to reach a stable state). In order\nto obtain a readable summary, we set P to 80, i.e., an average of 4 segments\nfor each cluster. We depart from the previous analysis by using here a piecewise constant summary: this is more adapted to load curves as they should be\nfairly constant on significant time periods, because many electrical appliances\nhave stable power consumption once switched on. As for the Topex/Poseidon\ndataset, the summarized prototypes are much easier to analyze than their noisy\ncounterpart. Even the global organization of the map is more obvious: the top\n25\n\n\fFigure 16: Prototypes obtained by SOM algorithm for the load curves (grey curves are the\noriginal curves as assigned to the clusters)\n\n26\n\n\fright of the map for instance, gathers days in which the power consumption\nin the morning (starting at 7 am) is significant, is followed by a period of low\nconsumption and then again by a consumption peak starting approximately\nat 7 pm. Those days are week days in which the home remains empty during\nwork hours. Other typical clusters include holidays with almost no consumption\n(second cluster on the first line), more constant days which correspond to week\nends, etc. In addition, the optimal resource assignment tends to emphasize the\ndifference between the clusters by allocating less resources to simple ones (as\nexpected). This is quite obvious in the case of piecewise constant summaries\nused in Figure 17 (and to a lesser extent on Figure 12).\n\nFigure 17: Summarized prototypes (P = 80) obtained by Algorithm 4 for the load curves\n(grey curves are the original curves as assigned to the clusters)\n\n27\n\n\fIn order to emphasize the importance of a drastic reduction in prototype\ncomplexity, Algorithm 4 was applied to the same dataset, using the same initial\nconfiguration, with P = 160, i.e., an average of 8 segments in each cluster.\nResults are represented on Figure 18. As expected, prototypes are more complex\n\nFigure 18: Summarized prototypes (P = 160) obtained by Algorithm 4 for the load curves\n(grey curves are the original curves as assigned to the clusters)\n\nthan on Figure 17. While they are not as rough as the original ones (see Figure\n16), their analysis is more difficult than the one of prototypes obtained with\nstronger summarizing constraints. In particular, some consumption peaks are\nno more roughly outlined as in Figure 17 but more accurately approximated\n(see for instance the last cluster of the second row). This leads to series of\n\n28\n\n\fshort segments from which expert inference is not easy. It should be noted in\naddition, that because of the noisy nature of the data, increasing the number\nof segments improves only marginally the approximation quality. To show this,\nwe first introduce a relative approximation error measure given by\nPK P\nPPk P\n2\nk=1\ni\u2208Gk\np=1\ntj \u2208Cpk (si (tj ) \u2212 \u03bck,p )\n,\n(29)\nPN PM\n2\ni=1\nj=1 (si (tj ) \u2212 \u03bci )\nusing notations from Section 3.2, and where \u03bci denotes the mean of si . This relative error compares the total squared approximation error to the total squared\ninternal variability inside the dataset. Table 1 gives the dependencies between\nthe number of segments and the relative approximation error. It is quite clear\nthat the relative loss in approximation precision is acceptable even with strong\nsummarizing constraints. This type of numerical quality assessment should be\nSummary\nRelative approximation error\n\nNo summary\n0.676\n\nP = 160\n0.696\n\nP = 80\n0.740\n\nTable 1: Approximation errors for the load curves\n\nprovided to the analyst either as a way to control the relevance of the summarized prototypes, or as a selection guide for choosing the value of P .\n5. Conclusion\nWe have proposed in this paper a new exploratory analysis method for functional data. This method relies on simple approximations of functions, that\nis on models that are piecewise constant or piecewise linear. Given a set of\nhomogeneous functions, a dynamic programming approach computes efficiently\nand optimally a simple model via a segmentation of the interval on which the\nfunctions are evaluated. Given several groups of homogeneous functions and a\ntotal number of segments, another dynamic programming algorithm allocates\noptimally to each cluster the number of segments used to build the summary.\nFinally, those two algorithms can be embedded in any classical prototype based\nalgorithm (such as the K means) to refine the groups of functions given the\noptimal summaries and vice versa in an alternating optimization scheme.\nThe general framework is extremely flexible and accommodates different\ntypes of quality measures (L2 , L1 ), aggregation strategies (maximal error, mean\nerror), approximation model (piecewise constant, linear interpolation) and clustering strategies (K means, Neural gas). Experiments show that it provides\nmeaningful summaries that help greatly the analyst in getting quickly a good\nidea of the general behavior of a set of curves.\nWhile we have listed many variants of the method to illustrate its flexibility,\nnumerous were not mentioned as they are more distant from the main topic of\nthis paper. It should be noted for instance that rather than building a unique\nsummary for a set of curves, one could look for a unique segmentation supporting curve by curve summaries. In other words, each curve will be summarized\nby e.g., a piecewise constant model, but all models will share the same segmentation. This strategy is very interesting for analyzing curves in which internal\nchanges are more important than the actual values taken by the curves. It is\n29\n\n\fclosely related to the so called best basis problem in which a functional basis is\nbuilt to represent efficiently a set of curves (see e.g., [5, 28, 31]). This is also\nrelated to variable clustering methods (see e.g., [19]). This final link opens an\ninteresting perspective for the proposed method. It has been shown recently\n[10, 20] that variable clustering methods can be adapted to supervised learning:\nvariables are grouped in a way that preserves as much as possible the explanatory power of the reduced set of variables with respect to a target variable. It\nwould be extremely useful to provide the analyst with such a tool for exploring a\nset of curves: she would be able to select an explanatory variable and to obtain\nsummaries of those curves that mask details that are not relevant for predicting\nthe chosen variable.\nAcknowledgment\nThe authors thank the anonymous referees for their valuable comments that\nhelped improving this paper.\nReferences\n[1] C. Abraham, P.-A. Cornillon, E. Matzner-Lober, and N. Molinari. Unsupervised curve clustering using b-splines. Scandinavian Journal of Statistics,\n30(3):581\u2013595, September 2003.\n[2] I. E. Auger and C. E. Lawrence. Algorithms for the optimal identification\nof segment neighborhoods. Bulletin of Mathematical Biology, 51(1):39\u201354,\n1989.\n[3] R. Bellman. On the approximation of curves by line segments using dynamic programming. Communication of the ACM, 4(6):284, 1961.\n[4] F. Chamroukhi, A. Sam\u00e9, G. Govaert, and P. Aknin. A regression model\nwith a hidden logistic process for signal parametrization. In Proceedings of\nXVIth European Symposium on Artificial Neural Networks (ESANN 2009),\npages 503\u2013508, Bruges, Belgique, April 2009.\n[5] R. R. Coifman and M. V. Wickerhauser. Entropy-based algorithms for best\nbasis selection. IEEE Transactions on Information Theory, 38(2):713\u2013718,\nMarch 1992.\n[6] M. Cottrell, B. Girard, and P. Rousset. Forecasting of curves using a\nkohonen classification. Journal of Forecasting, 17:429\u2013439, 1998.\n[7] M. Cottrell, B. Hammer, A. Hasenfuss, and T. Villmann. Batch and median\nneural gas. Neural Networks, 19(6\u20137):762\u2013771, July\u2013August 2006.\n[8] A. Debr\u00e9geas and G. H\u00e9brail. Interactive interpretation of kohonen maps\napplied to curves. In Proc. International Conference on Knowledge Discovery and Data Mining (KDD'98), pages 179\u2013183, New York, August 1998.\n[9] L. Devroye, L. Gy\u00f6rfi, and G. Lugosi. A Probabilistic Theory of Pattern\nRecognition, volume 21 of Applications of Mathematics. Springer, 1996.\n\n30\n\n\f[10] D. Fran\u00e7ois, C. Krier, F. Rossi, and M. Verleysen. Estimation de redondance pour le clustering de variables spectrales. In Actes des 10\u00e8mes\njourn\u00e9es Europ\u00e9ennes Agro-industrie et M\u00e9thodes statistiques (Agrostat\n2008), pages 55\u201361, Louvain-la-Neuve, Belgique, January 2008.\n[11] F. Frappart. Catalogue des formes d'onde de l'altim\u00e8tre topex/pos\u00e9idon sur\nle bassin amazonien. Rapport technique, CNES, Toulouse, France, 2003.\n[12] F. Frappart, S. Calmant, M. Cauhop\u00e9, F. Seyler, and A. Cazenave. Preliminary results of envisat ra-2-derived water levels validation over the amazon\nbasin. Remote Sensing of Environment, 100(2):252 \u2013 264, 2006. ISSN\n0034-4257.\n[13] C. G. Healey, K. S. Booth, and J. T. Enns. Visualizing real-time multivariate data using preattentive processing. ACM Transactions on Modeling\nand Computer Simulation, 5(3):190\u2013221, July 1995.\n[14] P. J. Huber. Robust estimation of a location parameter. Annals of Mathematical Statistics, 35(1):73\u2013101, 1964.\n[15] B. Hugueney. Adaptive segmentation-based symbolic representations of\ntime series for better modeling and lower bounding distance measures. In\nProceedings of 10th European Conference on Principles and Practice of\nKnowledge Discovery in Databases, PKDD 2006, volume 4213 of Lecture\nNotes in Computer Science, pages 545\u2013552, September 2006.\n[16] B. Jackson, J. Scargle, D. Barnes, S. Arabhi, A. Alt, P. Gioumousis,\nE. Gwin, P. Sangtrakulcharoen, L. Tan, and T. T. Tsai. An algorithm\nfor optimal partitioning of data on an interval. IEEE Signal Processing\nLetters, 12(2):105\u2013108, Feb. 2005.\n[17] S. Kaski and K. Lagus. Comparing self-organizing maps. In C. von der\nMalsburg, W. von Seelen, J. Vorbr\u00fcggen, and B. Sendhoff, editors,\nProceedings of International Conference on Artificial Neural Networks\n(ICANN'96), volume 1112 of Lecture Notes in Computer Science, pages\n809\u2013814. Springer, Berlin, Germany,, July 16-19 1996.\n[18] T. Kohonen. Self-Organizing Maps, volume 30 of Springer Series in Information Sciences. Springer, third edition, 1995. Last edition published in\n2001.\n[19] C. Krier, F. Rossi, D. Fran\u00e7ois, and M. Verleysen. A data-driven functional\nprojection approach for the selection of feature ranges in spectra with ica\nor cluster analysis. Chemometrics and Intelligent Laboratory Systems, 91\n(1):43\u201353, March 2008.\n[20] C. Krier, M. Verleysen, F. Rossi, and D. Fran\u00e7ois. Supervised variable\nclustering for classification of nir spectra. In Proceedings of XVIth European\nSymposium on Artificial Neural Networks (ESANN 2009), pages 263\u2013268,\nBruges, Belgique, April 2009.\n[21] Y. Lechevallier. Classification automatique optimale sous contrainte d'ordre\ntotal. Rapport de recherche 200, IRIA, 1976.\n\n31\n\n\f[22] Y. Lechevallier. Recherche d'une partition optimale sous contrainte d'ordre\ntotal. Rapport de recherche RR-1247, INRIA, June 1990. http://www.\ninria.fr/rrrt/rr-1247.html.\n[23] J. A. Lee and M. Verleysen. Generalization of the lp norm for time series\nand its application to self-organizing maps. In Proceedings of the 5th Workshop on Self-Organizing Maps (WSOM 05), pages 733\u2013740, Paris (France),\nSeptember 2005.\n[24] J. Lin, E. J. Keogh, S. Lonardi, and B. Y. chi Chiu. A symbolic representation of time series, with implications for streaming algorithms. In M. J.\nZaki and C. C. Aggarwal, editors, DMKD, pages 2\u201311. ACM, 2003.\n[25] F. Picard, S. Robin, E. Lebarbier, and J.-J. Daudin. A segmentation/clustering model for the analysis of array cgh data. Biometrics, 63\n(3):758\u2013766, 2007.\n[26] J. Ramsay and B. Silverman. Functional Data Analysis. Springer Series in\nStatistics. Springer Verlag, June 1997.\n[27] K. Rose. Deterministic annealing for clustering, compression,classification,\nregression, and related optimization problems. Proceedings of the IEEE, 86\n(11):2210\u20132239, November 1998.\n[28] F. Rossi and Y. Lechevallier. Constrained variable clustering for functional data representation. In Proceedings of the first joint meeting of the\nSoci\u00e9t\u00e9 Francophone de Classification and the Classification and Data Analysis Group of the Italian Statistical Society (SFC-CLADAG 2008), Caserta,\nItaly, June 2008.\n[29] F. Rossi, B. Conan-Guez, and A. El Golli. Clustering functional data with\nthe SOM algorithm. In Proceedings of XIIth European Symposium on Artificial Neural Networks (ESANN 2004), pages 305\u2013312, Bruges (Belgium),\nApril 2004.\n[30] F. Rossi, D. Fran\u00e7ois, V. Wertz, and M. Verleysen. Fast selection of spectral\nvariables with b-spline compression. Chemometrics and Intelligent Laboratory Systems, 86(2):208\u2013218, April 2007.\n[31] N. Saito and R. R. Coifman. Local discriminant bases and their applications. Journal of Mathematical Imaging and Vision, 5(4):337\u2013358, 1995.\n[32] H. Stone. Approximation of curves by line segments. Mathematics of\nComputation, 15(73):40\u201347, 1961.\n[33] A. N. Tikhonov and V. Y. Arsenin. Solutions of ill-posed problems. SIAM\nReview, 21(2):266\u2013267, April 1979.\n[34] A. Ultsch and L. Herrmann. The architecture of emergent self-organizing\nmaps to reduce projection errors. In Proceedings of the 13th European Symposium on Artificial Neural Networks (ESANN 2005), pages 1\u20136, Bruges\n(Belgium), April 2005.\n\n32\n\n\f"}