{"id": "http://arxiv.org/abs/math/0605740v1", "guidislink": true, "updated": "2006-05-30T05:49:04Z", "updated_parsed": [2006, 5, 30, 5, 49, 4, 1, 150, 0], "published": "2006-05-30T05:49:04Z", "published_parsed": [2006, 5, 30, 5, 49, 4, 1, 150, 0], "title": "Sharp thresholds for high-dimensional and noisy recovery of sparsity", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0204135%2Cmath%2F0204310%2Cmath%2F0204247%2Cmath%2F0204077%2Cmath%2F0204249%2Cmath%2F0204278%2Cmath%2F0204125%2Cmath%2F0204336%2Cmath%2F0204088%2Cmath%2F0204213%2Cmath%2F0204018%2Cmath%2F0204008%2Cmath%2F0204039%2Cmath%2F0204256%2Cmath%2F0204059%2Cmath%2F0204242%2Cmath%2F0204047%2Cmath%2F0204322%2Cmath%2F0204168%2Cmath%2F0204143%2Cmath%2F0204293%2Cmath%2F0204267%2Cmath%2F0204042%2Cmath%2F0204069%2Cmath%2F0204309%2Cmath%2F0204237%2Cmath%2F0204258%2Cmath%2F0204117%2Cmath%2F0204269%2Cmath%2F0204233%2Cmath%2F0204176%2Cmath%2F0204131%2Cmath%2F0204298%2Cmath%2F0204196%2Cmath%2F0204311%2Cmath%2F0605682%2Cmath%2F0605249%2Cmath%2F0605463%2Cmath%2F0605735%2Cmath%2F0605712%2Cmath%2F0605327%2Cmath%2F0605588%2Cmath%2F0605320%2Cmath%2F0605615%2Cmath%2F0605117%2Cmath%2F0605750%2Cmath%2F0605505%2Cmath%2F0605740%2Cmath%2F0605692%2Cmath%2F0605001%2Cmath%2F0605427%2Cmath%2F0605667%2Cmath%2F0605504%2Cmath%2F0605335%2Cmath%2F0605582%2Cmath%2F0605218%2Cmath%2F0605325%2Cmath%2F0605340%2Cmath%2F0605011%2Cmath%2F0605671%2Cmath%2F0605170%2Cmath%2F0605500%2Cmath%2F0605238%2Cmath%2F0605540%2Cmath%2F0605550%2Cmath%2F0605113%2Cmath%2F0605198%2Cmath%2F0605539%2Cmath%2F0605457%2Cmath%2F0605188%2Cmath%2F0605119%2Cmath%2F0605253%2Cmath%2F0605653%2Cmath%2F0605030%2Cmath%2F0605283%2Cmath%2F0605270%2Cmath%2F0605728%2Cmath%2F0605308%2Cmath%2F0605751%2Cmath%2F0605631%2Cmath%2F0605090%2Cmath%2F0605361%2Cmath%2F0605721%2Cmath%2F0605106%2Cmath%2F0605581%2Cmath%2F0605659%2Cmath%2F0605544%2Cmath%2F0605136%2Cmath%2F0605029%2Cmath%2F0605559%2Cmath%2F0605286%2Cmath%2F0605753%2Cmath%2F0605684%2Cmath%2F0605345%2Cmath%2F0605601%2Cmath%2F0605640%2Cmath%2F0605403%2Cmath%2F0605163%2Cmath%2F0605070%2Cmath%2F0605215%2Cmath%2F0605014&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Sharp thresholds for high-dimensional and noisy recovery of sparsity"}, "summary": "The problem of consistently estimating the sparsity pattern of a vector\n$\\betastar \\in \\real^\\mdim$ based on observations contaminated by noise arises\nin various contexts, including subset selection in regression, structure\nestimation in graphical models, sparse approximation, and signal denoising. We\nanalyze the behavior of $\\ell_1$-constrained quadratic programming (QP), also\nreferred to as the Lasso, for recovering the sparsity pattern. Our main result\nis to establish a sharp relation between the problem dimension $\\mdim$, the\nnumber $\\spindex$ of non-zero elements in $\\betastar$, and the number of\nobservations $\\numobs$ that are required for reliable recovery. For a broad\nclass of Gaussian ensembles satisfying mutual incoherence conditions, we\nestablish existence and compute explicit values of thresholds $\\ThreshLow$ and\n$\\ThreshUp$ with the following properties: for any $\\epsilon > 0$, if $\\numobs\n> 2 (\\ThreshUp + \\epsilon) \\log (\\mdim - \\spindex) + \\spindex + 1$, then the\nLasso succeeds in recovering the sparsity pattern with probability converging\nto one for large problems, whereas for $\\numobs < 2 (\\ThreshLow - \\epsilon)\n\\log (\\mdim - \\spindex) + \\spindex + 1$, then the probability of successful\nrecovery converges to zero. For the special case of the uniform Gaussian\nensemble, we show that $\\ThreshLow = \\ThreshUp = 1$, so that the threshold is\nsharp and exactly determined.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0204135%2Cmath%2F0204310%2Cmath%2F0204247%2Cmath%2F0204077%2Cmath%2F0204249%2Cmath%2F0204278%2Cmath%2F0204125%2Cmath%2F0204336%2Cmath%2F0204088%2Cmath%2F0204213%2Cmath%2F0204018%2Cmath%2F0204008%2Cmath%2F0204039%2Cmath%2F0204256%2Cmath%2F0204059%2Cmath%2F0204242%2Cmath%2F0204047%2Cmath%2F0204322%2Cmath%2F0204168%2Cmath%2F0204143%2Cmath%2F0204293%2Cmath%2F0204267%2Cmath%2F0204042%2Cmath%2F0204069%2Cmath%2F0204309%2Cmath%2F0204237%2Cmath%2F0204258%2Cmath%2F0204117%2Cmath%2F0204269%2Cmath%2F0204233%2Cmath%2F0204176%2Cmath%2F0204131%2Cmath%2F0204298%2Cmath%2F0204196%2Cmath%2F0204311%2Cmath%2F0605682%2Cmath%2F0605249%2Cmath%2F0605463%2Cmath%2F0605735%2Cmath%2F0605712%2Cmath%2F0605327%2Cmath%2F0605588%2Cmath%2F0605320%2Cmath%2F0605615%2Cmath%2F0605117%2Cmath%2F0605750%2Cmath%2F0605505%2Cmath%2F0605740%2Cmath%2F0605692%2Cmath%2F0605001%2Cmath%2F0605427%2Cmath%2F0605667%2Cmath%2F0605504%2Cmath%2F0605335%2Cmath%2F0605582%2Cmath%2F0605218%2Cmath%2F0605325%2Cmath%2F0605340%2Cmath%2F0605011%2Cmath%2F0605671%2Cmath%2F0605170%2Cmath%2F0605500%2Cmath%2F0605238%2Cmath%2F0605540%2Cmath%2F0605550%2Cmath%2F0605113%2Cmath%2F0605198%2Cmath%2F0605539%2Cmath%2F0605457%2Cmath%2F0605188%2Cmath%2F0605119%2Cmath%2F0605253%2Cmath%2F0605653%2Cmath%2F0605030%2Cmath%2F0605283%2Cmath%2F0605270%2Cmath%2F0605728%2Cmath%2F0605308%2Cmath%2F0605751%2Cmath%2F0605631%2Cmath%2F0605090%2Cmath%2F0605361%2Cmath%2F0605721%2Cmath%2F0605106%2Cmath%2F0605581%2Cmath%2F0605659%2Cmath%2F0605544%2Cmath%2F0605136%2Cmath%2F0605029%2Cmath%2F0605559%2Cmath%2F0605286%2Cmath%2F0605753%2Cmath%2F0605684%2Cmath%2F0605345%2Cmath%2F0605601%2Cmath%2F0605640%2Cmath%2F0605403%2Cmath%2F0605163%2Cmath%2F0605070%2Cmath%2F0605215%2Cmath%2F0605014&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The problem of consistently estimating the sparsity pattern of a vector\n$\\betastar \\in \\real^\\mdim$ based on observations contaminated by noise arises\nin various contexts, including subset selection in regression, structure\nestimation in graphical models, sparse approximation, and signal denoising. We\nanalyze the behavior of $\\ell_1$-constrained quadratic programming (QP), also\nreferred to as the Lasso, for recovering the sparsity pattern. Our main result\nis to establish a sharp relation between the problem dimension $\\mdim$, the\nnumber $\\spindex$ of non-zero elements in $\\betastar$, and the number of\nobservations $\\numobs$ that are required for reliable recovery. For a broad\nclass of Gaussian ensembles satisfying mutual incoherence conditions, we\nestablish existence and compute explicit values of thresholds $\\ThreshLow$ and\n$\\ThreshUp$ with the following properties: for any $\\epsilon > 0$, if $\\numobs\n> 2 (\\ThreshUp + \\epsilon) \\log (\\mdim - \\spindex) + \\spindex + 1$, then the\nLasso succeeds in recovering the sparsity pattern with probability converging\nto one for large problems, whereas for $\\numobs < 2 (\\ThreshLow - \\epsilon)\n\\log (\\mdim - \\spindex) + \\spindex + 1$, then the probability of successful\nrecovery converges to zero. For the special case of the uniform Gaussian\nensemble, we show that $\\ThreshLow = \\ThreshUp = 1$, so that the threshold is\nsharp and exactly determined."}, "authors": ["Martin J. Wainwright"], "author_detail": {"name": "Martin J. Wainwright"}, "author": "Martin J. Wainwright", "arxiv_comment": "Appeared as Technical Report 708, Department of Statistics, UC\n  Berkeley", "links": [{"href": "http://arxiv.org/abs/math/0605740v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0605740v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0605740v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0605740v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:math/0605740v1 [math.ST] 30 May 2006\n\nSharp thresholds for high-dimensional and noisy\nrecovery of sparsity\n\nMartin J. Wainwright\nDepartment of Statistics, and\nDepartment of Electrical Engineering and Computer Sciences\nUniversity of California, Berkeley\nwainwrig@{eecs,stat}.berkeley.edu\nTechnical Report, UC Berkeley, Department of Statistics\nMay 2006\nAbstract\nThe problem of consistently estimating the sparsity pattern of a vector \u03b2 \u2217 \u2208 Rp based\non observations contaminated by noise arises in various contexts, including subset selection in\nregression, structure estimation in graphical models, sparse approximation, and signal denoising.\nWe analyze the behavior of l1 -constrained quadratic programming (QP), also referred to as the\nLasso, for recovering the sparsity pattern. Our main result is to establish a sharp relation\nbetween the problem dimension p, the number s of non-zero elements in \u03b2 \u2217 , and the number of\nobservations n that are required for reliable recovery. For a broad class of Gaussian ensembles\nsatisfying mutual incoherence conditions, we establish existence and compute explicit values of\nthresholds \u03b8l and \u03b8u with the following properties: for any \u03bd > 0, if n > 2 (\u03b8u +\u03bd) log(p\u2212s)+s+1,\nthen the Lasso succeeds in recovering the sparsity pattern with probability converging to one\nfor large problems, whereas for n < 2 (\u03b8l \u2212 \u03bd) log(p \u2212 s) + s + 1, then the probability of successful\nrecovery converges to zero. For the special case of the uniform Gaussian ensemble, we show that\n\u03b8l = \u03b8u = 1, so that the threshold is sharp and exactly determined.\n\nKeywords: Quadratic programming; Lasso; subset selection; consistency; thresholds; sparse approximation; signal denoising; sparsity recovery; l0 -regularization; model selection.\n\n1\n\nIntroduction\n\nThe problem of recovering the sparsity pattern of an unknown vector \u03b2 \u2217 -that is, the positions\nof the non-zero entries of \u03b2 \u2217 - based on noisy observations arises in a broad variety of contexts,\nincluding subset selection in regression [29], structure estimation in graphical models [28], sparse\napproximation [8, 30], and signal denoising [6]. A natural optimization-theoretic formulation of\nthis problem is via l0 -minimization, where the l0 \"norm\" of a vector corresponds to the number of\nnon-zero elements. Unfortunately, however, l0 -minimization problems are known to be NP-hard in\ngeneral [30], so that the existence of polynomial-time algorithms is highly unlikely. This challenge\nmotivates the use of computationally tractable approximations or relaxations to l0 minimization.\nIn particular, a great deal of research over the past decade has studied the use of the l1 -norm as a\ncomputationally tractable surrogate to the l0 -norm.\n1\n\n\fIn more concrete terms, suppose that we wish to estimate an unknown but fixed vector \u03b2 \u2217 \u2208 Rp\non the basis of a set of n observations of the form\nYk = xTk \u03b2 \u2217 + Wk ,\n\nk = 1, . . . n,\n\n(1)\n\nwhere xk \u2208 Rp , and Wk \u223c N (0, \u03c3 2 ) is additive Gaussian noise. In many settings, it is natural to\nassume that the vector \u03b2 \u2217 is sparse, in that its support\nS := {i \u2208 {1, . . . p} | \u03b2i\u2217 6= 0}\n\n(2)\n\nhas relatively small cardinality s = |S|. Given the observation model (1) and sparsity assumption (2), a reasonable approach to estimating \u03b2 \u2217 is by solving the l1 -constrained quadratic program\n(QP)\n(\n)\nn\n1 X\n2\nT\nmin\n(3)\nkYk \u2212 xk \u03b2k2 + \u03bbn k\u03b2k1 ,\n\u03b2\u2208Rp\n2n\nk=1\n\nwhere \u03bbn \u2265 0 is a regularization parameter. Of interest are conditions on the ambient dimension\np, the sparsity index s, and the number of observations n for which it is possible (or impossible) to\nrecover the support set S of \u03b2 \u2217 .\n\n1.1\n\nOverview of previous work\n\nGiven the substantial literature on the use of l1 constraints for sparsity recovery and subset selection, we provide only a very brief (and hence necessarily incomplete) overview here. In the noiseless\nversion (\u03c3 2 = 0) of the linear observation model (1), one can imagine estimating \u03b2 \u2217 by solving the\nproblem\n(4)\nsubject to xTk \u03b2 = Yk , k = 1, . . . , n.\nminp k\u03b2k1\n\u03b2\u2208R\n\nThis problem is in fact a linear program (in disguise), and corresponds to a method in signal\nprocessing known as basis pursuit, pioneered by Chen et al. [6]. For the noiseless setting, the\ninteresting regime is the underdetermined setting (i.e., n < p). With contributions from a broad\nrange of researchers [e.g., 3, 6, 12, 10, 14, 15, 26, 33], there is now a fairly complete understanding\nof conditions on deterministic vectors {xk } and sparsity index s for which the true solution \u03b2 \u2217\ncan be recovered exactly. Without going into technical details, the rough idea is that the mutual\nincoherence of the vectors {xk } must be large relative to the sparsity index s, and indeed we\nimpose similar conditions to derive our results (e.g., conditions (14a) and (18) in the sequel). Most\nclosely related to the current paper-as we discuss in more detail in the sequel-are recent results\nby Donoho [9], as well as Candes and Tao [4] that provide high probability results for random\nensembles. More specifically, as independently established by both sets of authors using different\nmethods, for uniform Gaussian ensembles (i.e., xk \u223c N (0, Ip )) with the ambient dimension p scaling\nlinearly in terms of the number of observations (i.e., p = \u03b3n, for some \u03b3 > 1), there exists a constant\n\u03b1 > 0 such that all sparsity patterns with s \u2264 \u03b1p can be recovered with high probability.\nThere is also a substantial body of work focusing on the noisy setting (\u03c3 2 > 0), and the use\nof quadratic programming techniques for sparsity recovery [e.g., 6, 17, 18, 34, 11, 16, 28, 35]. The\nl1 -constrained quadratic program (3), also known as the Lasso [32, 13], has been the focus of considerable research in recent years. Knight and Fu [23] analyze the asymptotic behavior of the optimal\n2\n\n\fsolution, not only for l1 regularization but for lp -regularization with p \u2208 (0, 2]. Fuchs [17, 18] investigates optimality conditions for the constrained QP (3), and provides deterministic conditions,\nof the mutual incoherence form, under which a sparse solution, which is known to be within \u01eb of\nthe observed values, can be recovered exactly. Among a variety of other results, both Tropp [34]\nand Donoho et al. [11] also provide sufficient conditions for the support of the optimal solution to\nthe constrained QP (3) to be contained within the true support of \u03b2 \u2217 . Most directly related to the\ncurrent paper is recent work by both Meinshausen and Buhlmann [28], focusing on Gaussian noise,\nand extensions by Zhao and Yu [35] to more general noise distributions, on the use of the Lasso\nfor model selection. For the case of Gaussian noise, both papers established that under mutual\nincoherence conditions and appropriate choices of the regularization parameter \u03bbn , the Lasso can\nrecover the sparsity pattern with probability converging to one for particular regimes of n, p and s,\nwhen xk drawn randomly from random Gaussian ensembles. We discuss connections to our results\nat more length in the the sequel.\n\n1.2\n\nOur contributions\n\nRecall the linear observation model (1). For compactness in notation, let us use X to denote\nthe n \u00d7 p matrix formed with the vectors xk = (xk1 , xk2 , . . . , xkp ) \u2208 Rp as rows, and the vectors\nXj = (x1j , x2j , . . . , xnj )T \u2208 Rn as columns, as follows:\n\uf8ee T\uf8f9\nx1\n\uf8efxT \uf8fa\n\u0003\n\u0002\n\uf8ef 2\uf8fa\nX := \uf8ef . \uf8fa = X1 X2 * * * Xp .\n(5)\n\uf8f0 .. \uf8fb\nxTn\n\nConsider the (random) set S(X, \u03b2 \u2217 , W, \u03bbn ) of optimal solutions to this constrained quadratic program (3). By convexity and boundedness of the cost function, the solution set is always non-empty.\nFor any vector \u03b2 \u2208 Rp , we define the sign function\n\uf8f1\n\uf8f4\n\uf8f2+1 if \u03b2i > 0\n(6)\nsgn(\u03b2i ) :=\n\u22121 if \u03b2i < 0\n\uf8f4\n\uf8f3\n0\nif \u03b2i = 0.\n\nOf interest is the event that the Lasso (3) succeeds in recovering the sparsity pattern of the unknown\n\u03b2\u2217:\nProperty R(X, \u03b2 \u2217 , W, \u03bbn ): There exists an optimal solution \u03b2b \u2208 S(X, \u03b2 \u2217 , W, \u03bbn ) with the property\nb = sgn(\u03b2 \u2217 ).\nsgn(\u03b2)\nOur main result is that for a broad class of random Gaussian ensembles based on covariance matrices\nsatisfying mutual incoherence conditions, there exist fixed constants 0 < \u03b8l \u2264 1 and 1 \u2264 \u03b8u < +\u221e\nsuch that for all \u03bd > 0, property R(X, \u03b2 \u2217 , W, \u03bbn ) holds with high probability (over the choice of\nnoise vector W and random matrix X) whenever\nn > 2(\u03b8u + \u03bd) s log(p \u2212 s) + s + 1,\n\n(7)\n\nand conversely, fails to hold with high probability whenever\nn < 2(\u03b8l \u2212 \u03bd) s log(p \u2212 s) + s + 1.\n3\n\n(8)\n\n\fMoreover, for the special case of the uniform Gaussian ensemble (i.e., xk \u223c N (0, Ip )), we show that\n\u03b8l = \u03b8u = 1, so that the threshold is sharp. This threshold result has a number of connections to\nprevious work in the area that focuses on special forms of scaling. More specifically, as we discuss\nin more detail in Section 3.2, in the special case of linear scaling (i.e., n = \u03b3p for some \u03b3 > 0), this\ntheorem provides a noisy analog of results previously established for basis pursuit in the noiseless\ncase [9, 4]. Moreover, our result can also be adapted to an entirely different scaling regime for n, p\nand s, as considered by a separate body of recent work [28, 35] on the high-dimensional Lasso.\nThe remainder of this paper is organized as follows. We begin in Section 2 with some necessary\nand sufficient conditions, based on standard optimality conditions for convex programs, for property\nR(X, \u03b2 \u2217 , W, \u03bbn ) to hold. We then prove a consistency result for the case of deterministic design\nmatrices X. Section 3 is devoted to the statement and proof of our main result on the asymptotic\nbehavior of the lasso for random Gaussian ensembles. We illustrate this result via simulation in\nSection 4, and conclude with a discussion in Section 5.\n\n2\n\nSome preliminary analysis\n\nIn this section, we provide necessary and sufficient conditions for property R(X, \u03b2 \u2217 , W, \u03bbn ) to hold.\nBased on these conditions, we then define collections of random variables that play a central role\nin our analysis. In particular, the study of R(X, \u03b2 \u2217 , W, \u03bbn ) is reduced to the study of the extreme\norder statistics of these random variables. We then state and prove a result about the behavior of\nthe Lasso for the case of a deterministic design matrix X.\n\n2.1\n\nNecessary and sufficient conditions\n\nWe begin with a simple set of necessary and sufficient conditions for property R(X, \u03b2 \u2217 , W, \u03bbn ) to\nhold. We note that this result is not essentially new (e.g., see [17, 18, 28, 34, 35] for variants),\nand follows in a straightforward manner from optimality conditions for convex programs [21]; see\nAppendix A for further details. We define S := {i \u2208 {1, . . . , p} | \u03b2i\u2217 6= 0} to be the support of \u03b2 \u2217 ,\nand let S c be its complement. For any subset T \u2286 {1, 2, . . . , p}, let XT be the n \u00d7 |T | matrix with\nthe vectors {Xi , i \u2208 T } as columns.\nLemma 1. Assume that the matrix XST XS is invertible. Then, for any given \u03bb > 0 and noise\nvector w \u2208 Rn , property R(X, \u03b2 \u2217 , w, \u03bbn ) holds if and only if\nXSTc XS\n\n\u0015\n1\n1 T\n\u2217\nXS w \u2212 \u03bb sgn(\u03b2S ) \u2212 XSTc w\nn\nn\n\u0013\u22121 \u0014\n\u0015\n\u0012\n1 T\n1 T\nXS XS\nXS w \u2212 \u03bb sgn(\u03b2S\u2217 )\n\u03b2S\u2217 +\nn\nn\n\n\u0001\u22121\nXST XS\n\n\u0014\n\n\u2264 \u03bb,\n> 0,\n\nand\n\n(9a)\n(9b)\n\nwhere both of these vector inequalities should be taken elementwise.\nFor shorthand, define ~b := sgn(\u03b2S\u2217 ), and denote by ei \u2208 Rs the vector with 1 in the ith position,\nand zeroes elsewhere. Motivated by Lemma 1, much of our analysis is based on the collections of\n4\n\n\frandom variables, defined each index i \u2208 S and j \u2208 S c as follows:\nUi :=\nVj\n\neTi\n\n\u0012\n\n\u0015\n1 T\nX W \u2212 \u03bbn~b\nn S\n)\niW\nh\n\u0001\u22121 T\n\u0001\u22121\nT\nT\n.\nXS \u2212 In\u00d7n\n\u03bbn~b \u2212 XS XS XS\nXS XS\nn\n\n1 T\nX XS\nn S\n(\n\n:= XjT XS\n\n\u0013\u22121 \u0014\n\n(10a)\n(10b)\n\nRecall that s = |S| and N = |S c | = p \u2212 s. From Lemma 1, the behavior of R(X, \u03b2 \u2217 , W, \u03bbn ) is\ndetermined by the behavior of maxj\u2208S c |Vj | and maxi\u2208S |Ui |. In particular, condition (9a) holds if\nand only if the event\nM(V ) :=\n\n\u001a\n\nmaxc |Vj | \u2264 \u03bbn\nj\u2208S\n\n\u001b\n\n(11)\n\nholds. On the other hand, if we define \u03c1n := mini\u2208S |\u03b2i\u2217 |, then the event\nM(U ) :=\n\n\u001a\n\nmax |Ui | \u2264 \u03c1n\ni\u2208S\n\n\u001b\n\n(12)\n\nis sufficient to guarantee that condition (9b) holds. Consequently, our proofs are based on analyzing\nthe asymptotic probability of these two events.\n\n2.2\n\nRecovery of sparsity: deterministic design\n\nWe now show how Lemma 1 can be used to analyze the behavior of the Lasso for the special case of\na deterministic (non-random) design matrix X. To gain intuition for the conditions in the theorem\nstatement, it is helpful to consider the zero-noise condition w = 0, in which each observation\nYk = xTk \u03b2 \u2217 is uncorrupted. In this case, the conditions of Lemma 1 reduce to\n\u0001\u22121\nXSTc XS XST XS\nsgn(\u03b2S\u2217 )\n\u0013\u22121\n\u0012\n1 T\n\u2217\nX XS\nsgn(\u03b2S\u2217 )\n\u03b2S \u2212 \u03bb\nn S\n\n\u2264 1\n\n(13a)\n\n> 0.\n\n(13b)\n\nOf course, if the conditions of Lemma 1 fail to hold in the zero-noise setting, then there is little\nhope of succeeding in the presence of noise.\nThe zero-noise conditions motivate imposing the following set of conditions on the design matrix:\nXSTc XS XST XS\n\n\u0001\u22121\n\n\u221e\n\n\u2264 (1 \u2212 \u01eb) for some \u01eb \u2208 (0, 1], and\n\n1\n\u039bmin ( XST XS ) \u2265 Cmin > 0,\nn\n\n(14a)\n(14b)\n\nwhere \u039bmin denotes the minimal eigenvalue. Under these conditions, we have the following:\n\n5\n\n\fProposition 1. Suppose that we observe Y = X\u03b2 \u2217 + W , where each column Xj of X is normalized to l2 -norm n, and W \u223c N (0, \u03c3 2 I). Assume \u03b2 \u2217 and X satisfy conditions (14), and define\n\u03c1n := mini\u2208S |\u03b2i\u2217 |. If \u03bbn \u2192 0 is chosen such that\n)\n(r\n1\n1 T\nlog s\nn\u03bb2n\n\u22121\n(15)\n\u2192 +\u221e,\nand (b)\n+ \u03bbn k( XS XS ) k\u221e \u2192 0,\n(a)\nlog(p \u2212 s)\n\u03c1n\nn\nn\nthen P(R(X, \u03b2 \u2217 , W, \u03bbn ) \u2192 1 as n \u2192 +\u221e.\nBefore proving the proposition, we pause to make a number of comments. First, conditions of\nthe form (14a) have been considered in previous work on the lasso [17, 18, 28, 34, 35]. In particular,\nvarious authors [34, 28, 35] provide examples and results on matrix families that satisfy this type\nof condition. Moreover, previous work [28, 35] provides asymptotic results for particular scalings\nof p, s and n for random design matrices, as we discuss in more detail in Section 3. To the best of\nour knowledge, Proposition 1 is the first result to provide sufficient conditions for exact recovery\nin deterministic designs with general scaling of p, s and n.\nSecond, it is worthwhile to consider Proposition 1 in the classical setting (i.e., in which the\nnumber of samples n \u2192 +\u221e with p and s remaining fixed). In this setting, the quantity \u03c1n =\nmini\u2208S |\u03b2i\u2217 | does not depend on n. Hence, in addition to the condition (14), the requirements reduce\n\u221a n is one suitable choice. This classical case is also\nto \u03bbn \u2192 0 and n\u03bb2n \u2192 +\u221e. Note that \u03bbn = log\nn\ncovered by previous work [23, 28, 35].\nLast, consider the more general setting where all three parameters (n, p, s) grow to infinity,\nand suppose for simplicity that \u03c1n stays bounded away from 0. The conditions \u03bb2n \u2192 0 and\nn\n\u2192 +\u221e imply that the number of observations n must grow at a rate faster than\n\u03bb2n log(p\u2212s)\nlog(p \u2212 s). In the following section, in which we consider the more general case of random Gaussian\nensembles, we will see that for ensembles satisfying mutual incoherence conditions, we in fact require\nn\nthat log(p\u2212s)\n= \u0398(s) \u2192 +\u221e.\n\n2.3\n\nProof of Proposition 1\n\nRecall the events M(V ) and M(U ) defined in equations (11) and (12) respectively. To establish\nthe claim, we must show that that P[M(V )c or M(U )c ] \u2192 0, where M(V )c and M(U )c denote the\ncomplements of these events. By union bound, it suffices to show both P[M(V )c ] and P[M(U )c ]\nconverge to zero, or equivalently that P[M(V )] and P[M(U )] both converge to one.\nAnalysis of M(V ): We begin by establishing that P[M(V )] \u2192 1. Throughout the proof, we use\nthe shorthand ~b := sgn(\u03b2 \u2217 ) and N := p \u2212 s = |S c |.\nRecalling the definition (10b) of the random variables Vj , note that M(V ) holds holds if and\nminj\u2208S c Vj\nmaxj\u2208S c Vj\nonly\n\u2265 \u22121 and\n\u2264 1. Moreover, we note that each Vj is Gaussian with mean\n\u03bbn\n\u03bbn\n\u03bcj = E[Vj ] = \u03bbn XjT XS XST XS\n\n\u0001\u22121\n\n~b .\n\nUsing condition (14a), we have |\u03bcj | \u2264 (1 \u2212 \u01eb) \u03bbn for all indices j = 1, . . . , N , from which we obtain\nthat\nminj\u2208S c Vj\nmaxj\u2208S c Vj\n1\n1\n\u2264 (1 \u2212 \u01eb) +\nmax Vej ,\n\u2265 |; \u2212(1 \u2212 \u01eb) +\nmin Vej ,\nand\n\u03bbn\n\u03bbn j\n\u03bbn\n\u03bbn j\n6\n\n\fh\n\u0001\u22121 T i\nXS W are zero-mean (correlated) Gaussian variables.\nwhere Vej := XjT In\u00d7n \u2212 XS XST XS\nHence, in order to establish condition (9a) of Lemma 1, we need to show that\n\u0014\n\n1\nP\nmin Vej < \u2212\u01eb,\n\u03bbn j\u2208S c\n\nor\n\n\u0015\n1\ne\nmax Vj > \u01eb \u2192 0.\n\u03bbn j\u2208S c\n\n(16)\nmax\n\nc\n\n|Ve |\n\nj\nj\u2208S\nIn fact, using Lemma 11 (see Appendix C), it is sufficient to show that P[\n> \u01eb] \u2192 0. By\n\u03bbn\napplying Markov's inequality and Gaussian comparison results [25] (see Lemma 9 in Appendix B),\nwe obtain\n#\n\"\n\u221a\nq\nE[maxj\u2208S c |Vej |]\nmaxj\u2208S c |Vej |\n3 log N\n>\u01eb \u2264\n\u2264\nmax E[Vej2 ].\nP\nj\n\u03bbn\n\u03bbn\n\u03bbn\n\nStraightforward computation yields that\nE[Vej2 ] =\n\n\u0001\u22121 T i\n\u03c32\n\u03c32\n\u03c32 T h\nT\n2\nX\n\u2264\nX\nX\nX\nI\n\u2212\nX\nX\nkX\nk\n=\n,\nj\nS\nn\u00d7n\nS\nj\nS\nS\nn2 j\nn2\nn\n\n\u0001\u22121 T\nXS has maximum eigenvalue equal to one, and kXj k22 = n\nsince the matrix In\u00d7n \u2212 XS XST XS\nN\nby construction. Consequently, condition (a) in the theorem statement-namely, that log\n\u21920\nn\u03bb2n\nis sufficient to ensure that E[Ve(N ) ]/\u03bbn \u2192 0. Thus, we have established P(M(V )) \u2192 1 (i.e., that\ncondition (9a) holds w.p. one as n \u2192 +\u221e).\nAnalysis of M(U ): We now show that P(M(U )) \u2192 1. Beginning with the triangle inequality,\nwe upper bound maxi |Ui | := k( n1 XST XS )\u22121 [ n1 XST W \u2212 \u03bbn sgn(\u03b2S\u2217 )]k\u221e as\nmax |Ui | \u2264\ni\n\n1\n1\n( XST XS )\u22121 XST W\nn\nn\n\n1\n+ ( XST XS )\u22121\nn\n\u221e\n\n\u03bbn\n\u221e\n\nLet ei denote the unit vector with one in position i and zeroes elsewhere. Now define, for each index\ni \u2208 S, the Gaussian random variable Zi := eTi ( n1 XST XS )\u22121 n1 XST W . Each such Zi is a zero-mean\nGaussian with variance given by\nvar(Zi ) =\n\n\u03c32 T 1 T\n\u03c32\nei ( XS XS )\u22121 ei \u2264\nn\nn\nCmin n\n\nHence, by a standard Gaussian comparison theorem [25] (in particular, see Lemma 9 in Appendix B), we have\n\u0014\n1\n1\nE[ max |Zi |] = E ( XST XS )\u22121 XST W\n1\u2264i\u2264s\nn\nn\ns\n\u03c3 2 log s\n.\n\u2264 3\nnCmin\n7\n\n\u221e\n\n\u0015\n\n\fThus, recalling the defining \u03c1n := mini\u2208S |\u03b2i\u2217 |, we apply Markov's inequality to conclude that\n#\n\"\n\u0014\n\u0015\n\u0015\n\u0013\u22121 \u0014\n\u0012\n1\n1\n1\nT\nT\n\u2217\n\u2217\n\u2264 P\nX XS\nX w \u2212 \u03bb sgn(\u03b2S ) > 0\nmax |Ui | > 1\n1 \u2212 P \u03b2S +\nn S\nn S\n\u03c1n 1\u2264i\u2264s\n\u0014 \u001a\n\u001b\n\u0015\n1\n1\n\u2264 P\nmax |Zi | + \u03bbn k( XST XS )\u22121 k\u221e > 1\n\u03c1n 1\u2264i\u2264s\nn\n\u001a \u0014\n\u0015\n\u001b\n1 T\n1\n\u22121\nE max |Zi | + \u03bbn k( XS XS ) k\u221e\n\u2264\n1\u2264i\u2264s\n\u03c1n\nn\n\uf8fc\n\uf8f1 s\n\uf8fd\n1 \uf8f2\n1 T\n\u03c3 2 log s\n\u2264\n+ \u03bbn k( XS XS )\u22121 k\u221e ,\n3\n\uf8fe\n\u03c1n \uf8f3\nnCmin\nn\nwhich converges to zero as n \u2192 +\u221e, using condition (b) in the theorem statement.\n\n3\n\nRecovery of sparsity: random Gaussian ensembles\n\nWe now turn to the analysis of random design matrices X, in which each row xk is chosen as an\ni.i.d. Gaussian random vector with covariance matrix \u03a3. In particular, we prove the existence\nof thresholds that provide a sharp description of the failure/success of the Lasso as a function of\n(n, p, s). We begin by setting up and providing a precise statement of the main result, and then\ndiscussing its connections to previous work. In the later part of this section, we provide the proof.\n\n3.1\n\nStatement of main result\n\nConsider a covariance matrix \u03a3 with unit diagonal, and with its minimum and maximum eigenvalues\n(denoted \u039bmin and \u039bmax respectively) bounded as\n\u039bmin (\u03a3SS ) \u2265 Cmin ,\n\nand\n\n\u039bmax (\u03a3) \u2264 Cmax\n\n(17)\n\nfor constants Cmin > 0 and Cmax < +\u221e. Given a vector \u03b2 \u2217 \u2208 Rp , define its support S = {i \u2208\n{1, . . . , p} | \u03b2i\u2217 6= 0}, as well as the complement S c of its support. Suppose that \u03a3 and S satisfy\nthe conditions k(\u03a3SS )\u22121 k\u221e \u2264 Dmax for some Dmax < +\u221e, and\nk\u03a3S c S (\u03a3SS )\u22121 k\u221e \u2264 (1 \u2212 \u01eb)\n\n(18)\n\nfor some \u01eb \u2208 (0, 1]. Under these conditions, we consider the observation model\nYk = xTk \u03b2 \u2217 + Wk ,\n\nk = 1, . . . , n,\n\n(19)\n\nwhere xk \u223c N (0, \u03a3) and Wk \u223c N (0, \u03c3 2 ) are independent Gaussian variables for k = 1, . . . , n.\nFurthermore, we define \u03c1n := mini\u2208S |\u03b2i\u2217 |, and the sparsity index s = |S|.\n\nTheorem 1. Consider a sequence of covariance matrices {\u03a3[p]} and solution vectors {\u03b2 \u2217 [p]} satisfying conditions (17) and (18). Under the observation model (19), consider a sequence (n, p(n), s(n))\nsuch that s, (n \u2212 s) and (p \u2212 s) tend to infinity. Define the thresholds\nq\n\u221a\n1\n)2\n( Cmax \u2212 Cmax \u2212 Cmax\nCmax\n\u2264 1,\nand\n\u03b8u := 2\n\u2265 1.\n(20)\n\u03b8l :=\n2\nCmax (2 \u2212 \u01eb)\n\u01eb Cmin\n\nThen for any constant \u03bd > 0, we have the following\n8\n\n\f(a) If n < 2(\u03b8l \u2212 \u03bd) s log(p \u2212 s) + s + 1, then P[R(X, \u03b2 \u2217 , W, \u03bbn )] \u2192 0 for any non-increasing\nsequence \u03bbn > 0.\n(b) Conversely, if n > 2(\u03b8u + \u03bd) s log(p \u2212 s) + s, and \u03bbn \u2192 0 is chosen such that\nr\n1h\nn\u03bb2n\nlog s i\n\u2192 0,\n\u2192 +\u221e,\nand\n\u03bbn +\nlog(p \u2212 s)\n\u03c1n\nn\n\n(21)\n\nthen P[R(X, \u03b2 \u2217 , W, \u03bbn )] \u2192 1.\n\nRemark: Suppose for simplicity that \u03c1n remains bounded away from 0. In this case, the require,\nments on \u03bbn reduce to \u03bbn \u2192 0, and \u03bb2n n/ log(p\u2212s) \u2192 +\u221e. One suitable choice is \u03bb2n = log(s) log(p\u2212s)\nn\nwith which we have\n\u0012\n\u0012\n\u0013\n\u0013\ns log(p \u2212 s) log(s)\nlog s\n2\n\u03bbn =\n= O\n\u2192 0,\nn\ns\ns\nand\nn\u03bb2n\nlog(p \u2212 s)\n\n= log(s) \u2192 +\u221e.\n\nWithout a bound on \u03c1n , the second condition in equation (21) constrains the rate of decrease of\nthe minimum \u03c1n = mini\u2208S |\u03b2i\u2217 |.\n\n3.2\n\nSome consequences\n\nTo develop intuition for this result, we begin by stating certain special cases as corollaries, and\ndiscussing connections to previous work.\n3.2.1\n\nUniform Gaussian ensembles\n\nFirst, we consider the special case of the uniform Gaussian ensemble, in which \u03a3 = Ip\u00d7p . Previous\nwork by Donoho [9] as well as Candes and Tao [4] has focused on the uniform Gaussian ensemble in\nthe the noiseless (\u03c3 2 = 0) and underdetermined setting (n = \u03b3p for some \u03b3 \u2208 (0, 1)). Analyzing the\nasymptotic behavior of the linear program (4) for recovering \u03b2 \u2217 , the basic result is that there exists\nsome \u03b1 > 0 such that all sparsity patterns with s \u2264 \u03b1p can be recovered with high probability.\nApplying Theorem 1 to the noisy version of this problem, the uniform Gaussian ensemble means\nthat we can choose \u01eb = 1, and Cmin = Cmax = 1, so that the threshold constants reduce\nq\n\u221a\n1\n)2\n( Cmax \u2212 Cmax \u2212 Cmax\nCmax\n= 1\nand\n\u03b8u = 2\n= 1.\n\u03b8l =\n2\nCmax (2 \u2212 \u01eb)\n\u01eb Cmin\nConsequently, Theorem 1 provides a sharp threshold for the behavior of the Lasso, in that failure/success is entirely determined by whether or not n > 2s log(p \u2212 s) + s + 1. Thus, if we consider\nthe particular linear scaling analyzed in previous work on the noiseless case [9, 4], we have:\nCorollary 1 (Linearly underdetermined setting). Suppose that n = \u03b3p for some \u03b3 \u2208 (0, 1).\nThen\n9\n\n\f(a) If s = \u03b1p for any \u03b1 \u2208 (0, 1), then P [R(X, \u03b2 \u2217 , W, \u03bbn )] \u2192 0 for any positive sequence \u03bbn > 0.\n(b) On the other hand, if s = O( logp p ), then P [R(X, \u03b2 \u2217 , W, \u03bbn )] \u2192 1 for any sequence {\u03bbn }\nsatisfying the conditions of Theorem 1(a).\nConversely, suppose that the size s of the support of \u03b2 \u2217 scales linearly with the number of parameters\np. The following result describes the amount of data required for the l1 -constrained QP to recover\nthe sparsity pattern in the noisy setting (\u03c3 2 > 0):\nCorollary 2 (Linear fraction support). Suppose that s = \u03b1p for some \u03b1 \u2208 (0, 1). Then we\nrequire n > 2\u03b1p log[(1 \u2212 \u03b1) p] + \u03b1p in order to obtain exact recovery with probability converging to\none for large problems.\nThese two corollaries establish that there is a significant difference between recovery using basis\npursuit (4) in the noiseless setting versus recovery using the Lasso (3) in the noisy setting. When the\namount of data n scales only linearly with ambient dimension p, then the presence of noise means\nthat the recoverable support size drops from a linear fraction (i.e., s = \u03b1p as in the work [9, 4]) to\na sublinear fraction (i.e., s = O( logp p ), as in Corollary 1).\n3.2.2\n\nNon-uniform Gaussian ensembles\n\nWe now consider more general (non-uniform) Gaussian ensembles that satisfy conditions (17)\nand (18). As mentioned earlier, previous papers by both Meinshausen and Buhlmann [28] as\nwell as Zhao and Yu [35] treat model selection with the high-dimensional Lasso. For suitable covariance matrices (e.g., satisfying conditions (17) and (18)), both sets of authors proved that the\nsparsity pattern can be recovered exactly under scaling conditions of the form\ns = O(nc1 ),\n\nc2\n\nand p = O(en ),\n\nwhere c1 + c2 < 1.\n\n(22)\n\nApplying Theorem 1 in this scenario, we have the following:\nCorollary 3. Under the scaling (22), the Lasso will recover the sparsity pattern with probability\nconverging to one.\nProof. Substituting the conditions (22) into the threshold condition (7), we obtain that the RHS\ntakes the form\n\u0003\n\u0002\nc2\n2s log(p \u2212 s) + s + 1 = O(nc1 ) log O(en ) \u2212 O(nc1 ) + O(nc1 )\n= O(nc1 +c2 ) \u226a n,\n\nsince c1 + c2 < 1 by assumption. Thus, we see that under these conditions, our threshold condition (7) is satisfied a fortiori.\nIn fact, under this stronger scaling (22), both papers [28, 35] proved that the probability of exact\nrecovery converges to one at a rate exponential in some polynomial function of n. Interestingly, our\nresults show that the Lasso can recover the sparsity pattern for a much broader range of (n, p, s)\nscaling.\n10\n\n\f3.3\n\nProof of Theorem 1(b)\n\nWe now turn to the proof of part (b) of our main result. As with the proof of Proposition 1, the\nproof is based on analyzing the collections of random variables {Vj | j \u2208 S c } and {Ui | i \u2208 S},\nas defined in equations (10a) and (10b) respectively. We begin with some preliminary results that\nserve to set up the argument.\n3.3.1\n\nSome preliminary results\n\nWe first note that for s < n, the random Gaussian matrix XS will have rank s with probability\none, whence the matrix XST XS is invertible with probability one. Accordingly, the necessary and\nsufficient conditions of Lemma 1 are applicable. Our first lemma, proved in Appendix D.1, concerns\nthe behavior of the random vector V = (V1 , . . . , VN ), when conditioned on XS and W . Recalling\nthe shorthand notation ~b := sgn(\u03b2 \u2217 ), we summarize in the following\nLemma 2. Conditioned on XS and W , the random vector (V | W, XS ) is Gaussian. Its mean\nvector is upper bounded as\n|E[V | W, XS ]| \u2264 \u03bbn (1 \u2212 \u01eb) 1.\n\n(23)\n\nMoreover, its conditional covariance takes the form\n\nwhere\n\n\u0002\n\u0003\ncov[V | W, XS ] = Mn \u03a3(S c | S) = Mn \u03a3S c S c \u2212 \u03a3S c S (\u03a3SS )\u22121 \u03a3SS c ,\nMn := \u03bb2n~b T (XST XS )\u22121~b +\n\nis a random scaling factor.\n\n\u0001\u22121 T i\n1 Th\nT\nXS W\nX\nX\nI\n\u2212\nX\nW\nS\nn\u00d7n\nS\nS\nn2\n\n(24)\n\n(25)\n\nThe following lemma, proved in Appendix D.2, captures the behavior of the random scaling\nfactor Mn defined in equation (25):\nLemma 3. The random variable Mn has mean\nE[Mn ] =\n\n2\n\u03bb2n\n~b T (\u03a3SS )\u22121~b + \u03c3 (n \u2212 s) .\nn\u2212s\u22121\nn2\n\n(26)\n\nMoreover, it is sharply concentrated in that for any \u03b4 > 0, we have\n\u0002\n\u0003\nP Mn \u2212 E[Mn ] \u2265 \u03b4E[Mn ] \u2192 0\n3.3.2\n\nas n \u2192 +\u221e.\n\n(27)\n\nMain argument\n\nWith these preliminary results in hand, we now turn to analysis of the collections of random\nvariables {Ui , i \u2208 S} and {Vj , j \u2208 S c }.\n11\n\n\fAnalysis of M(V ): We begin by analyzing the behavior of maxj\u2208S c |Vj |. First, for a fixed but\narbitrary \u03b4 > 0, define the event T (\u03b4) := {|Mn \u2212 E[Mn ]| \u2265 \u03b4E[Mn ]}. By conditioning on T (\u03b4) and\nits complement [T (\u03b4)]c , we have the upper bound\n\u0014\n\u0015\nc\nP[maxc |Vj | > \u03bbn ] \u2264 P maxc |Vj | > \u03bbn | [T (\u03b4)] + P[T (\u03b4)].\nj\u2208S\n\nj\u2208S\n\nBy the concentration statement in Lemma 3, we have P[T (\u03b4)] \u2192 0, so that it suffices to analyze\nthe first term. Set \u03bcj = E[Vj |XS ], and let Z be a zero-mean Gaussian vector with cov(Z) =\ncov(V | XS , W ).\nmaxc |Vj | = maxc |\u03bcj + Zj |\nj\u2208S\n\nj\u2208S\n\n\u2264 maxc [|\u03bcj | + |Zj |]\nj\u2208S\n\n\u2264 (1 \u2212 \u01eb)\u03bbn + maxc |Zj |,\nj\u2208S\n\nwhere we have used the upper bound (23) on the mean. This inequality establishes the inclusion\nof events\n{maxc |Zj | \u2264 \u01eb\u03bbn } \u2286 {maxc |Vj | \u2264 \u03bbn },\nj\u2208S\n\nj\u2208S\n\nthereby showing that it suffices to prove that P[maxj\u2208S c |Zj | > \u01eb\u03bbn | [T (\u03b4)]c ] \u2192 0.\nNote that conditioned on [T (\u03b4)]c , the maximum value of Mn is v \u2217 := (1 + \u03b4)E[Mn ]. Since\nGaussian maxima increase with increasing variance, we have\n\u0014\n\u0015\n\u0014\n\u0015\nP maxc |Zj | > \u01eb\u03bbn | [T (\u03b4)]c\n\u2264 P maxc |Zej | > \u01eb\u03bbn ,\nj\u2208S\n\nj\u2208S\n\ne is zero-mean Gaussian with covariance v \u2217 \u03a3(S c |S).\nwhere Z\nUsing Lemma 11, it suffices to show that P[maxj\u2208S c Zej > \u01eb\u03bbn ] converges to zero. Accordingly,\nwe complete this part of the proof via the following two lemmas, both of which are proved in\nAppendix D:\nLemma 4. Under the stated assumptions of the theorem, we have\n\nv\u2217\n\u03bb2n\n\n\u2192 0 and\n\n1\nej ] \u2264 \u01eb.\nE[maxc Z\nn\u2192+\u221e \u03bbn\nj\u2208S\nlim\n\nLemma 5. For any \u03b7 > 0, we have\n\u0014\n\u0015\n\u0012\n\u0013\n2\n\u03b7\nP maxc Zej > \u03b7 + E[maxc Zej ] \u2264 exp \u2212 \u2217 .\nj\u2208S\nj\u2208S\n2v\n\n(28)\n\nLemma 4 implies that for all \u03b4 > 0, we have E[maxj\u2208S c Zej ] \u2264 (1 + 2\u03b4 )\u01eb\u03bbn for all n sufficiently\nlarge. Therefore, setting \u03b7 = 2\u03b4 \u03bbn \u01eb in the bound (28), we have for fixed \u03b4 > 0 and n sufficiently\nlarge:\n\u0015\n\u0014\n\u0015\n\u0014\n\u03b4\ne\ne\ne\nP maxc Zj > (1 + \u03b4)\u03bbn \u01eb \u2264 P maxc Zj > \u03bbn \u01eb + E[maxc Zj ]\nj\u2208S\nj\u2208S\nj\u2208S\n2\n\u0012 2 2 2\u0013\n\u03b4 \u03bbn \u01eb\n\u2264 2 exp \u2212\n.\n8v \u2217\n12\n\n\fFrom Lemma 4, we have \u03bb2n /v \u2217 \u2192 +\u221e, which implies that P[maxj\u2208S c Zej > (1 + \u03b4)\u03bbn \u01eb] \u2192 0 for all\nej \u2264 \u01eb\u03bbn ] \u2192 1, thereby establishing\n\u03b4 > 0. By the arbitrariness of \u03b4 > 0, we thus have P[maxj\u2208S c Z\nthat property (9a) of Lemma 1 holds w.p. one asymptotically.\nAnalysis of {Ui }: Next we prove that maxi\u2208S |Ui | < \u03c1n := mini\u2208S |\u03b2i\u2217 | with probability one\nas n \u2192 +\u221e. Conditioned on XS , the only random component in Ui is the noise vector W . A\nstraightforward calculation yields that this conditioned RV is Gaussian, with mean and variance\n\u0013\u22121\n\u0012\n1 T\nT\n~b ,\nX XS\nYi := E[Ui | XS ] = \u2212\u03bbn ei\nn S\n\u0014\n\u0015\u22121\n\u03c32 T 1 T\n\u2032\nYi := var[Ui | XS ] =\ne\nX XS\nei ,\nn i n S\nrespectively. The following lemma, proved in Appendix D.5, is key to our proof:\nLemma 6. (a) The random variables Yi and Yi\u2032 have means\nE[Yi ] =\n\n\u2212\u03bbn n T\ne (\u03a3SS )\u22121 ~b ,\nn\u2212s\u22121 i\n\nand\n\nE[Yi\u2032 ] =\n\nrespectively, which are bounded as\n|E[Yi ]| \u2264\n\n2Dmax n\u03bbn\n,\nn\u2212s\u22121\n\nand\n\n\u03c32\neT (\u03a3SS )\u22121 ei ,\nn\u2212s\u22121 i\n\n\u03c32\n\u03c3 2 Dmax\n\u2264 E[Yi\u2032 ] \u2264\n.\nCmax (n \u2212 s \u2212 1)\nn\u2212s\u22121\n\n(b) Moreover, each pair (Yi , Yi\u2032 ) is sharply concentrated, in that we have\n\"\n#\n6Dmax n\u03bbn\nK\n\u2032\n\u2032\nP |Yi | \u2265\n, or |Yi | \u2265 2E[Yi ] \u2264\n,\nn\u2212s\u22121\nn\u2212s\n\n(29)\n\n(30)\n\n(31)\n\nwhere K is a fixed constant independent of n and s.\nWe exploit this lemma as follows. First define the event\n)\n(\ns\n[\n6Dmax n\u03bbn\n, or |Yi\u2032 | \u2265 2E[Yi\u2032 ] .\n|Yi | \u2265\nT (\u03b4) :=\nn\u2212s\u22121\ni=1\n\nBy the union bound and Lemma 6(b), we have\nP[T (\u03b4)] \u2264 s\n\nK\n=\nn\u2212s\n\nn\ns\n\nK\n\u2192 0,\n\u22121\n\nsince ns \u2192 +\u221e as n \u2192 +\u221e. For convenience in notation, for any a \u2208 R and b \u2208 R+ , we use Ui (a, b)\nto denote a Gaussian random variable with mean a and variance b. Conditioning on the event T (\u03b4)\nand its complement, we have\nP[max Ui > \u03c1n ] \u2264 P[max Ui > \u03c1n | T (\u03b4)c ] + P[T (\u03b4)]\ni\u2208S\n\ni\u2208S\n\n\u2264 P[max Ui (\u03bc\u2217i , vi\u2217 ) > \u03c1n ] +\ni\u2208S\n\n13\n\nn\ns\n\nK\n,\n\u22121\n\n(32)\n\n\fn\nand variance vi\u2217 := 2E[Yi\u2032 ]\nwhere each Ui (\u03bc\u2217i , vi\u2217 ) is Gaussian with mean \u03bc\u2217i := 6Dmax \u03bbn n\u2212s\u22121\nrespectively. In asserting the inequality (32), we have used the fact that the probability of the event\n{maxi\u2208S Yi > \u03c1n } increases as the mean and variance of Yi increase. Continuing the argument, we\nhave\n\nP[max Ui (\u03bc\u2217i , vi\u2217 ) > \u03c1n ] \u2264 P[max |Ui (\u03bc\u2217i , vi\u2217 )| > \u03c1n ]\ni\u2208S\ni\u2208S\n\u0014\n\u0015\n1\n\u2264\nE max |Ui (\u03bc\u2217i , vi\u2217 )| ,\ni\u2208S\n\u03c1n\nd\n\nn\n+\nwhere the last step uses Markov's inequality. We now decompose Ui (\u03bc\u2217i , vi\u2217 ) = 2Dmax \u03bbn n\u2212s\u22121\nei (0, v \u2217 ), and write\nU\ni\n\u0014\n\u0015\n\u0014\n\u0015\nn\n\u2217 \u2217\n\u2217\ne\nE max |Ui (\u03bci , vi )| \u2264 2Dmax \u03bbn\n+ E max |U (0, vi )| .\ni\u2208S\ni\u2208S\nn\u2212s\u22121\n\nWith this decomposition, we use the bound (30) on vi\u2217 := 2E[Yi\u2032 ] and Lemma 9 on Gaussian maxima\n(see Appendix B) to conclude that\n\"\n#\nr\n\u0014\n\u0015\n2\u03c3 2 Dmax log s\n1\n1\nn\n\u2217 \u2217\nE max |Ui (\u03bci , vi )| \u2264\n+3\n2Dmax \u03bbn\n,\ni\u2208S\n\u03c1n\n\u03c1n\nn\u2212s\u22121\nn\u2212s\u22121\nwhich converges to zero by the second condition (21) in the theorem statement.\n\n3.4\n\nProof of Theorem 1(a)\n\nWe establish the claim by proving that under the stated conditions, maxj\u2208S c |Vj | > \u03bbn with probability one, for any positive sequence \u03bbn > 0. We begin by writing Vj = E[Vj ] + Vej , where Vej is\nzero-mean. Now\nmaxc |Vj | \u2265 maxc |Vej | \u2212 maxc |E[Vj ]|\nj\u2208S\n\nj\u2208S\n\nj\u2208S\n\n\u2265 maxc |Vj | \u2212 (1 \u2212 \u01eb)\u03bbn\nj\u2208S\n\nwhere have used Lemma 2. Consequently, the event {maxj\u2208S c |Vej | > (2 \u2212 \u01eb)\u03bbn } implies the event\n{maxj\u2208S c |Vj | > \u03bbn }, so that\nP[maxc |Vj | > \u03bbn ] \u2265 P[maxc |Vej | > (2 \u2212 \u01eb) \u03bbn ].\nj\u2208S\n\nj\u2208S\n\nFrom the preceding proof of Theorem 1(b), we know that conditioned on XS and W , the random\nvector (V1 , . . . , VN ) is Gaussian with covariance of the form Mn [\u03a3S c S c \u2212 \u03a3S c S (\u03a3SS )\u22121 \u03a3SS c ]; thus,\nthe zero-mean version (Ve1 , . . . , VeN ) has the same covariance. Moreover, Lemma 3 guarantees that\nthe random scaling term Mn is sharply concentrated. In particular, defining for any \u03b4 > 0 the event\nT (\u03b4) := { |Mn \u2212 E[Mn ]| \u2265 \u03b4E[Mn ]}, we have P[T (\u03b4)] \u2192 0, and the bound\n\u0014\n\u0015\nc\nP[maxc |Vej | > (2 \u2212 \u01eb) \u03bbn ] \u2265 (1 \u2212 P[T (\u03b4)]) P maxc |Vej | > (2 \u2212 \u01eb) \u03bbn | T (\u03b4)\nj\u2208S\nj\u2208S\n\u0014\n\u0015\n\u2217\n\u2265 (1 \u2212 P[T (\u03b4)]) P maxc |Zj (v )| > (2 \u2212 \u01eb) \u03bbn ,\nj\u2208S\n\n14\n\n\fwhere each Zj \u2261 Zj (v \u2217 ) is the conditioned version of Vej with the scaling factor Mn fixed to\nv \u2217 := (1 \u2212 \u03b4)E[Mn ]. (Here we have used the fact that the probability of Gaussian maxima decreases\nas the variance decreases, and that var(Vej ) \u2265 v \u2217 when conditioned on T (\u03b4)c .)\nOur proof proceeds by first analyzing the expected value, and then exploiting Gaussian concentration of measure. We summarize the key results in the following:\nLemma 7. Under the stated conditions, one of the following two conditions must hold:\n2\n\n(a) either \u03bbv\u2217n \u2192 +\u221e, and there exists some \u03b3 > 0 such that\nfor all sufficiently large n, or\n(b) there exist constants \u03b1, \u03b3 > 0 such that\nsufficiently large n.\n\nv\u2217\n\u03bb2n\n\n\u2264 \u03b1 and\n\n1\nc\n\u03bbn E[maxj\u2208S\n\n1\nc\n\u03bbn E[maxj\u2208S\n\nZj ] \u2265 (2 \u2212 \u01eb) [1 + \u03b3]\n\n\u221a\nZj ] \u2265 \u03b3 log N for all\n\nLemma 8. For any \u03b7 > 0, we have\n\u0012\n\n\u03b72\nP[maxc Zj (v ) < E[maxc Zj (v )] \u2212 \u03b7] \u2264 exp \u2212 \u2217\nj\u2208S\nj\u2208S\n2v\n\u2217\n\n\u2217\n\n\u0013\n\n.\n\n(33)\n\nUsing these two lemmas, we complete the proof as follows. First, if condition (a) of Lemma 7\nholds, then we set \u03b7 = (2\u2212\u01eb)2 \u03b3\u03bbn in equation (33) to obtain that\n\u0012\n\u0013\n(2 \u2212 \u01eb)2 \u03b3 2 \u03bb2n\n\u03b3\n1\n\u2217\n.\nP[ maxc Zj (v ) \u2265 (2 \u2212 \u01eb) (1 + ) ] \u2265 1 \u2212 exp \u2212\n\u03bbn j\u2208S\n2\n8v \u2217\n2\n\nThis probability converges to 1 since \u03bbv\u2217n \u2192 +\u221e from Lemma 7(a).\nOn the other hand, if condition (b) holds, then we use the bound\nand set \u03b7 =\n\n\u221a\n\n\u03b3\u03bbn log N\n2\n\n1\nc\n\u03bbn E[maxj\u2208S\n\nin equation (33) to obtain\n\n\u221a\nZj ] \u2265 \u03b3 log N\n\n\u221a\n1\n1\n\u03b3 log N\n\u2217\n\u2217\nP[ maxc Zj (v ) > 2 (2 \u2212 \u01eb) ] \u2265 P[ maxc Zj (v ) \u2265\n]\n\u03bbn j\u2208S\n\u03bbn j\u2208S\n2\n\u0012 2 2\n\u0013\n\u03b3 \u03bbn log N\n\u2265 1 \u2212 exp \u2212\n.\n8v \u2217\n2\n\nThis probability also converges to 1 since \u03bbvn\u2217 \u2265 1/\u03b1 and log N \u2192 +\u221e. Thus, in either case, we\nhave shown that limn\u2192+\u221e P[ \u03bb1n maxj\u2208S c Zj (v \u2217 ) > (2 \u2212 \u01eb)] = 1, thereby completing the proof of\nTheorem 1(a).\n\n4\n\nIllustrative simulations\n\nIn this section, we provide some simulations to confirm the threshold behavior predicted by Theorem 1. We consider the following three types of sparsity indices:\n(a) linear sparsity, meaning that s(p) = \u03b1p for some \u03b1 \u2208 (0, 1);\n(b) sublinear sparsity, meaning that s(p) = \u03b1p/(log(\u03b1p)) for some \u03b1 \u2208 (0, 1), and\n(c) fractional power sparsity, meaning that s(p) = \u03b1p\u03b3 for some \u03b1, \u03b3 \u2208 (0, 1).\n15\n\n\fFor all three types of sparsity indices, we investigate the success/failure of the Lasso in recovering\nthe sparsity pattern, where the number of observations scales as n = 2 \u03b8 s log(p \u2212 s) + s + 1. The\ncontrol parameter \u03b8 is varied in the interval (0, 2.4). For all results shown here, we fixed \u03b1 = 0.40\nfor allqthree ensembles, and set \u03b3 = 0.75 for the fractional power ensemble. In addition, we set\n\nlog(s)\nin all cases.\n\u03bbn = log(p\u2212s)\nn\nWe begin by considering the uniform Gaussian ensemble, in which each row xk is chosen in an\ni.i.d. manner from the multivariate N (0, Ip\u00d7p ) distribution. Recall that for the uniform Gaussian\nensemble, the critical value is \u03b8u = \u03b8l = 1. Figure 1 plots the control parameter \u03b8 versus the\nprobability of success, for linear sparsity (a), sublinear sparsity pattern (b), and fractional power\nsparsity (c), for three different problem sizes (p \u2208 {128, 256, 512}). Each point represents the\naverage of 200 trials. Note how the probability of success rises rapidly from 0 around the predicted\nIdentity; Linear\n\nIdentity; Sublinear\n1\n\nn = 128\nn = 256\nn = 512\n\n0.8\nProb. of success\n\nProb. of success\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\nIdentity; Fractional power\n1\n\nn = 128\nn = 256\nn = 512\n\n0.6\n\n0.4\n\n0.2\n\n0.5\n\n1\n1.5\nControl parameter \u03b8\n\n2\n\n0\n0\n\nn = 128\nn = 256\nn = 512\n\n0.8\nProb. of success\n\n1\n\n0.6\n\n0.4\n\n0.2\n\n0.5\n\n1\n1.5\nControl parameter \u03b8\n\n(a)\n\n0\n0\n\n2\n\n(b)\n\n0.5\n\n1\n1.5\nControl parameter \u03b8\n\n(c)\n\nFigure 1. Plots of the number of data samples (indexed by the control parameter \u03b8 versus the\nprobability of success in the Lasso for the uniform Gaussian ensemble. Each panel shows three curves,\ncorresponding to the problem sizes p \u2208 {128, 256, 512}, and each point on each curve represents the\naverage of 200 trials. (a) Linear sparsity index: s(p) = \u03b1p. (b) Sublinear sparsity index s(p) =\n\u03b1p/ log(\u03b1p). (c) Fractional power sparsity index s(p) = \u03b1p\u03b3 with \u03b3 = 0.75.\n\nthreshold point \u03b8 = 1, with the sharpness of the threshold increasing for larger problem sizes.\nWe now consider a non-uniform Gaussian ensemble-in particular, one in which the covariance\nmatrices \u03a3 are Toeplitz with the structure\n\uf8ee\n\uf8f9\n1\n\u03c1 \u03c12 * * * \u03c1p\u22121 \u03c1p\n\uf8ef\u03c1\n1\n\u03c1 \u03c12 * * * \u03c1p\u22121 \uf8fa\n\uf8ef\n\uf8fa\np\u22122 \uf8fa\n\uf8ef\u03c12 \u03c1\n1\n\u03c1\n*\n*\n*\n\u03c1\n\u03a3 = \uf8ef\n(34)\n\uf8fa,\n\uf8ef ..\n..\n..\n..\n..\n.. \uf8fa\n\uf8f0.\n\uf8fb\n.\n.\n.\n.\n.\n\u03c1p * * *\n\n\u03c13\n\n\u03c12\n\n\u03c1\n\n1\n\nfor some \u03c1 \u2208 (\u22121, +1). As shown by Zhao and Yu [35], this family of Toeplitz matrices satisfy condition (18). Moreover, the maximum and minimum eigenvalues (Cmin and Cmax ) can be computed\nusing standard asymptotic results on Toeplitz matrix families [20]. Figure 2 shows representative\nresults for this Toeplitz family with \u03c1 = 0.10. Panel (a) corresponds to linear sparsity s = \u03b1p\nwith \u03b1 = 0.40), and panel (b) corresponds to sublinear sparsity (s = \u03b1p/ log(\u03b1p) with \u03b1 = 0.40).\nEach panel shows three curves, corresponding to the problem sizes p \u2208 {128, 256, 512}, and each\n16\n\n2\n\n\f\u03c1 = 0.10; Linear\n1\n\n\u03c1 = 0.10; Sublinear\n1\n\nn = 128\nn = 256\nn = 512\n\n0.6\n\n0.4\n\n0.2\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n0.5\n\n1\n1.5\nControl parameter \u03b8\n\n2\n\n0\n0\n\nn = 128\nn = 256\nn = 512\n\n0.8\nProb. of success\n\n0.8\nProb. of success\n\nProb. of success\n\n0.8\n\n\u03c1 = 0.10; Fractional power\n1\n\nn = 128\nn = 256\nn = 512\n\n0.6\n\n0.4\n\n0.2\n\n0.5\n\n1\n1.5\nControl parameter \u03b8\n\n(a)\n\n(b)\n\n2\n\n0\n0\n\n0.5\n\n1\n1.5\nControl parameter \u03b8\n\n(c)\n\nFigure 2. Plots of the number of data samples (indexed by the control parameter \u03b8 versus the\nprobability of success in the Lasso for the Toeplitz family (34) with \u03c1 = 0.10. Each panel shows\nthree curves, corresponding to the problem sizes p \u2208 {128, 256, 512}, and each point on each curve\nrepresents the average of 200 trials. (a) Linear sparsity index: s(p) = \u03b1p. (b) Sublinear sparsity\nindex s(p) = \u03b1p/ log(\u03b1p). (c) Fractional power sparsity index s(p) = \u03b1p\u03b3 with \u03b3 = 0.75.\n\npoint on each curve represents the average of 200 trials. The vertical lines to the left and right of\n\u03b8 = 1 represent the theoretical upper and lower bounds on the threshold (\u03b8u \u2248 1.84 and \u03b8l \u2248 0.46\nrespectively in this case). Once again, these simulations show good agreement with the theoretical\npredictions.\n\n5\n\nDiscussion\n\nThe problem of recovering the sparsity pattern of a high-dimensional vector \u03b2 \u2217 from noisy observations has important applications in signal denoising, graphical model selection, sparse approximation, and subset selection. This paper focuses on the behavior of l1 -regularized quadratic\nprogramming, also known as the Lasso, for estimating such sparsity patterns in the noisy and\nhigh-dimensional setting. The main contribution of this paper is to establish a set of general and\nsharp conditions on the observations n, the sparsity index s (i.e., number of non-zero entries in\n\u03b2 \u2217 ), and the ambient dimension p that characterize the success/failure behavior of the Lasso in\nthe high-dimensional setting, in which n, p and s all tend to infinity. For the uniform Gaussian\nensemble, our threshold result is sharp, whereas for more general Gaussian ensembles, it should be\npossible to tighten the analysis given here.\n\nAcknowledgements\nWe would like to thank Noureddine El Karoui and Bin Yu for helpful comments and pointers.\nThis work was partially supported by an Alfred P. Sloan Foundation Fellowship, and an Intel\nCorporation Equipment Grant.\n17\n\n2\n\n\fA\n\nProof of Lemma 1\n\nBy standard conditions for optimality in a convex program [21], the point \u03b2b \u2208 Rp is optimal if and\nb such that\nonly if there exists a subgradient zb \u2208 \u2202l1 (\u03b2)\n1 T b 1 T\nX X \u03b2 \u2212 X y + \u03bbb\nz = 0.\nn\nn\nHere the subdifferential of the l1 norm takes the form\nn\no\nb = zb \u2208 Rp | zbi = sgn(\u03b2bi ) for \u03b2bi 6= 0,\n\u2202l1 (\u03b2)\n|b\nzj | \u2264 1 otherwise .\n\nSubstituting our observation model y = X\u03b2 \u2217 + w and re-arranging yields\n1 T\n1\nX X(\u03b2b \u2212 \u03b2 \u2217 ) \u2212 X T w + \u03bbb\nz = 0.\nn\nn\nNow condition R(X, \u03b2 \u2217 , w, \u03bb) holds if and only we have\n\u03b2bS c = 0,\n\n\u03b2bS 6= 0,\n\nzbS = sgn(\u03b2S\u2217 ),\n\nand\n\n(35)\n\n(36)\n\n|b\nzS c | \u2264 1.\n\nFrom these conditions and using equation (36), we conclude that the condition R(X, \u03b2 \u2217 , w, \u03bb) holds\nif and only if\n\u0011 1\n\u0010\n1 T\nXS c XS \u03b2bS \u2212 \u03b2S\u2217 \u2212 XSTc w = \u2212\u03bbb\nzS c .\nn\nn\n\u0011 1\n\u0010\n1 T\nXS XS \u03b2bS \u2212 \u03b2S\u2217 \u2212 XST w = \u2212\u03bb sgn(\u03b2S\u2217 ).\nn\nn\nT\nUsing the invertibility of XS XS , we may solve for \u03b2bS and zbS c to conclude that\n\u0015\n\u0014\n\u0001\u22121 1 T\n1\n\u2217\nT\nT\nc\nXS w \u2212 \u03bb sgn(\u03b2S ) \u2212 XSTc w\n\u03bb zbS = XS c XS XS XS\nn\nn\n\u0013\u22121 \u0014\n\u0015\n\u0012\n1 T\n1 T\nXS XS\nX w \u2212 \u03bb sgn(\u03b2S\u2217 ) .\n\u03b2bS = \u03b2S\u2217 +\nn\nn S\n\nFrom these relations, the conditions |b\nzS c | \u2264 1 and \u03b2bS 6= 0 yield conditions (9a) and (9b) respectively.\n\nB\n\nSome Gaussian comparison results\n\nWe state here (without proof) some well-known comparison results on Gaussian maxima [25]. We\nbegin with a crude but useful bound:\nLemma 9. For any Gaussian random vector (X1 , . . . , Xn ), we have\nq\np\nE max |Xi | \u2264 3 log n max EXi2 .\n1\u2264i\u2264n\n\n1\u2264i\u2264n\n\nNext we state (a version of) the Sudakov-Fernique inequality [25, 5]:\n\nLemma 10. Let X = (X1 , . . . , Xn ) and Y = (Y1 , . . . , Yn ) be Gaussian random vectors such that\nfor all i, j\nE[(Yi \u2212 Yj )2 ] \u2264 E[(Xi \u2212 Xj )2 ].\nThen E[ max Yi ] \u2264 E[ max Xi ].\n1\u2264i\u2264n\n\n1\u2264i\u2264n\n\n18\n\n\fC\n\nAuxiliary lemma\n\nFor future use, we state formally the following elementary\nLemma 11. Given a collection {Z1 , Z2 , . . . , ZN } of zero-mean random variables, for any constant\na > 0 we have\nP[ max |Zj | \u2264 a] \u2264 P[ max Zj \u2264 a],\n1\u2264j\u2264N\n\n1\u2264j\u2264N\n\nand\n\n(39a)\n\nP[ max |Zj | > a] \u2264 2P[ max Zj > a].\n1\u2264j\u2264N\n\n(39b)\n\n1\u2264j\u2264N\n\nProof. The first inequality is trivial. To establish the inequality (39b), we write\nP[ max |Zj | > a] = P[( max Zj > a) or ( min Zj < \u2212a)]\n1\u2264j\u2264N\n\n1\u2264j\u2264N\n\n1\u2264j\u2264N\n\n\u2264 P[ max Zj > a] + P[ min Zj < \u2212a]\n1\u2264j\u2264N\n\n1\u2264j\u2264N\n\n= 2P[ max Zj > a],\n1\u2264j\u2264N\n\nwhere we have used the union bound, and the symmetry of the events {max1\u2264j\u2264N Zj > a} and\n{min1\u2264j\u2264N Zj < \u2212a}.\n\nD\nD.1\n\nLemma for Theorem 1\nProof of Lemma 2\n\nConditioned on both XS and W , the only random component in Vj is the column vector Xj . Using\nstandard LLSE formula [e.g., 2] (i.e., for estimating XS c on the basis of XS ), the random variable\n(XS c | XS , W ) \u223c (XS c | XS ) is Gaussian with mean and covariance\nE[XSTc | XS , W ] = \u03a3S c S (\u03a3SS )\u22121 XST ,\n\n(40a)\n\u22121\n\nvar(XS c | XS ) = \u03a3(S c |S) = \u03a3S c S c \u2212 \u03a3S c S (\u03a3SS )\n\n\u03a3SS c .\n\n(40b)\n\nConsequently, we have\n|E[Vj | XS , W ]| =\n=\n\n(\n\n\u03a3S c S (\u03a3SS )\u22121 XST XS XST XS\n\u03a3S c S (\u03a3SS )\u22121 \u03bbn~b\n\n\u0001\u22121\n\niW\nh\n\u0001\u22121 T\nXS \u2212 In\u00d7n\n\u03bbn~b \u2212 XS XST XS\nn\n\n)\n\n\u2264 \u03bbn (1 \u2212 \u01eb)1,\nas claimed.\nSimilarly, we compute the elements of the conditional covariance matrix as follows\ncov(Vj , Vk XS , W ) =\ncov(Aji , Aki | XS , W )\n\n\u001a\n\u001b\nh\n\u0001\u22121 T i\n1\n\u03bb2n~b T (XST XS )\u22121~b + 2 W T In\u00d7n \u2212 XS XST XS\nXS W .\nn\n19\n\n\fD.2\n\nProof of Lemma 3\n\nWe begin by computing the expected value. Since XST XS is Wishart with matrix \u03a3SS , the random\n\u22121\nSS )\nmatrix (XST XS )\u22121 is inverse Wishart with mean E[(XST XS )\u22121 ] = (\u03a3\nn\u2212s\u22121 (see Lemma 7.7.1 of\nAnderson [1]). Hence we have\nh\n\u0001\u22121 i\n~b =\nE \u03bb2n~b T XST XS\n\n\u03bb2n\n~b T (\u03a3SS )\u22121~b .\nn\u2212s\u22121\n\n(41)\n\nNow define the random matrix R = In\u00d7n \u2212 XS (XST XS )\u22121 XST . A straightforward calculation yields\nthat R2 = R, so that all the eigenvalues of R are either 0 or 1. In particular, for any vector z = XS u\nin the range of XS , we have\n\u0002\n\u0003\nRz = In\u00d7n \u2212 XS (XST XS )\u22121 XST XS u = 0.\n\n(42)\n\nHence dim(ker R) = dim(range XS ) = s. Since R is symmetric and positive semidefinite, there\nexists an orthogonal matrix U such that R = U T DU , where D is diagonal with (n \u2212 s) ones, and s\nzeros. The random matrices D and U are both independent of W , since XS is independent of W .\nHence we have\n\u0003\n1 \u0002 T\n=\nE\nW\nRW\n|\nX\nS\nn2\n\n\u0003\n1 \u0002 T T\nE\nW\nU\nDU\nW\n|\nX\nS\nn2\n\u0003\n\u0002\n1\n=\ntrace DU U T E W W T | XS\n2\nn\nn\u2212s\n= \u03c32\nn2\n\nsince E[W W T ] = \u03c3 2 I. Consequently, we have established that E[Mn ] =\n\u03c32\n\n\u03bb2n\nn\u2212s\u22121\n\n(43)\n~b T (\u03a3SS )\u22121~b +\n\n(n\u2212s)\nn2\n\nas claimed.\nWe now compute the expected value of the squared variance\ni\nh\n2 h\n\u0001\n\u0001\n\u0001\n\u0001\u22121 i2\n~b + 2 \u03bbn ~b T X T XS \u22121 ~b W T RW + 1 W T RW 2\nMn2 = \u03bb4n ~b T XST XS\nS\n2\n4\n{z\n} |n\n|\n{z\n} |n\n{z\n}\nT1\nT2\nT3\n\nFirst, conditioning on XS and using the eigenvalue decomposition D of R, we have\nE[T3 |XS ] =\n=\n=\nwhence E[T3 ] =\n\n2(n\u2212s)\u03c34\nn4\n\n+\n\n(n\u2212s)2 \u03c34\nn4\n\n1\nE[(W T DW )2 ]\nn4 \"\n#\nn\u2212s\nX\n1\nWi )2\nE (\nn4\ni=1\ns)\u03c3 4\n\n2(n \u2212\nn4\n\nas well.\n20\n\n+\n\n(n \u2212 s)2 \u03c3 4\n.\nn4\n\n(44)\n\n\fSimilarly, using conditional expectation and our previous calculation (43) of E[W T RW | XS ],\nwe have\n#\n\"\ni\nh\n2\u03bb2n\nE E ~b T (XST XS )\u22121~b (W T RW ) | XS\nE[T2 ] =\nn2\ni\n2\u03bb2n (n \u2212 s)\u03c3 2 h~ T T\n\u22121~\nE\nb\n(X\nX\n)\nb\n=\nS S\nn2\n2\n2\n2\u03bbn (n \u2212 s) \u03c3 ~ T\n=\nb (\u03a3SS )\u22121 ~b ,\n(45)\nn2 (n \u2212 s \u2212 1)\nwhere the final step uses Lemma 7.7.1 of Anderson [1] on the expectation of inverse Wishart\nmatrices.\nLastly, since (XST XS )\u22121 is inverse Wishart with matrix (\u03a3SS )\u22121 , we can use formula for second\nmoments of inverse Wishart matrices (see, e.g., Siskind [31]) to write, for all n > s + 3,\n\u001b\nh\ni2 \u001a\n1\n\u03bb4n\n~b T (\u03a3SS )\u22121~b\n1+\n.\nE[T1 ] =\n(n \u2212 s) (n \u2212 s \u2212 3)\nn\u2212s\u22121\nConsequently, combining our results, we have\nvar(Mn ) = E[Mn2 ] \u2212 (E[Mn ])2\n(\n\u0012\n\u00132 )\n3\n2\nX\n\u03c3 4 (n \u2212 s)2\n\u03bb2n\n\u03bb\n\u03c3 2 (n \u2212 s)\nn\nT\n\u22121\nT\n\u22121\n~b (\u03a3SS ) ~b +\n~b (\u03a3SS ) ~b\nE[Ti ] \u2212\n=\n+2\nn4\nn2\nn\u2212s\u22121\nn\u2212s\u22121\ni=1\n\u001a\n\u001b\n1\nn \u2212 s \u2212 1 (n \u2212 s \u2212 3)\n\u03bb4n [~b T (\u03a3SS )\u22121~b ]2\n2(n \u2212 s)\u03c3 4\n+\n\u2212\n+\n.\n(46)\n=\n4\ns)\n(n \u2212 s)\n(n \u2212 s \u2212 1)\n| n\n{z\n} |(n \u2212 s \u2212 1) (n \u2212 s \u2212 3) (n \u2212{z\n}\nH1\nH2\nFinally, we establish the concentration result. Using Chebyshev's inequality, we have\nP [|Mn \u2212 E[Mn ]| \u2265 \u03b4E[Mn ]] \u2264\n\nvar(Mn )\n,\n2\n\u03b4 (E[Mn ])2\n\nso that it suffices to prove that var(Mn )/(E[Mn ])2 \u2192 0 as n \u2192 +\u221e. We deal with each of the two\nvariance terms H1 and H2 in equation (46) separately. First, we have\nH1\n(E[Mn ])2\n\n\u2264\n\n2(n \u2212 s)\u03c3 4\n2\nn4\n\u2192 0.\n=\n4\n2\n4\nn\n(n \u2212 s) \u03c3\nn\u2212s\n\nSecondly, denoting A = (~b T (XST XS )\u22121~b ) for short-hand, we have\n\u001a\n\u001b\nH2\n\u03bb4n A2\n1\n(n \u2212 s \u2212 1)2\nn \u2212 s \u2212 1 (n \u2212 s \u2212 3)\n\u2264\n+\n\u2212\n(E[Mn ])2\n\u03bb4n A2\n(n \u2212 s \u2212 1) (n \u2212 s \u2212 3) (n \u2212 s)\n(n \u2212 s)\n(n \u2212 s \u2212 1)\n\u001a\n\u001b\nn \u2212 s \u2212 1 (n \u2212 s \u2212 3)\n1\n(n \u2212 s \u2212 1)\n+\n\u2212\n,\n=\n(n \u2212 s \u2212 3) (n \u2212 s)\n(n \u2212 s)\n(n \u2212 s \u2212 1)\nwhich also converges to 0 as (n \u2212 s) \u2192 0.\n21\n\n\fD.3\n\nProof of Lemma 4\n\nRecall that the Gaussian random vector (Z1 , . . . , ZN ) is zero-mean with covariance v \u2217 \u03a3(S c |S) , where\n\u03a3(S c |S) := \u03a3S c S c \u2212 \u03a3S c S (\u03a3SS )\u22121 \u03a3SS c . For any index i, let ei \u2208 RN be equal to 1 in position i, and\nzero otherwise. For any two indices i 6= j, we have\nE[(Zi \u2212 Zj )2 ] = v \u2217 (ei \u2212 ej )T \u03a3(S c |S) (ei \u2212 ej )\n\u2264 2v \u2217 \u03bbmax (\u03a3(S c |S) )\n\n\u2264 2Cmax v \u2217 ,\n\nsince \u03a3(S c |S) \u0016 \u03a3S c S c by definition, and \u039bmax (\u03a3S c S c ) \u2264 \u039bmax (\u03a3) \u2264 Cmax .\nLetting (X1 , . . . , XN ) \u223c N (0, Cmax v \u2217 IN \u00d7N ), we have E[(Xi \u2212Xj )2 ] = 2Cmax v \u2217 . Hence, applying\nthe Sudakov-Fernique inequality [25] yields E[maxj Zj ] \u2264 E[maxj Xj ]. By asymptotic behavior of\nE[max Xj ]\n= 1. Consequently, for all \u03b4\u2032 > 0, there\ni.i.d. Gaussians [19, 7], we have limN \u2192\u221e \u221a2C vj\u2217 log\nN\nmax\nexists an N (\u03b4\u2032 ) such that for all N \u2265 N (\u03b4\u2032 ), we have\n1\nE[max Zj (v \u2217 )] \u2264\nj\n\u03bbn\n\n1\nE[max Xj ]\nj\n\u03bbn\ns\n2Cmax v \u2217 log N\n\u2264 (1 + \u03b4\u2032 )\n\u03bb2n\ns\n\u221a\n2Cmax \u03c3 2 (1 \u2212 ns ) log N\n2Cmax log N ~ T\n= (1 + \u03b4\u2032 ) 1 + \u03b4\nb (\u03a3SS )\u22121~b +\nn\u2212s\u22121\nn\u03bb2n\ns\n\u221a\n2Cmax s log N 1\n2Cmax \u03c3 2 log N\n\u2264 (1 + \u03b4\u2032 ) 1 + \u03b4\n.\n+\nn \u2212 s \u2212 1 Cmin\nn\u03bb2n\n\nNow, applying our condition bounding n, N via \u03bd and \u03b8u , we have\ns \u0012\n\u0013\n\u221a\n\u03bd log N\n1\n2Cmax \u03c3 2 log N\n\u2217\n\u2032\n2\nE[max Zj (v )] < (1 + \u03b4 ) 1 + \u03b4 \u01eb 1 \u2212\n.\n+\nj\n\u03bbn\nn\u2212s\u22121\nn\u03bb2n\nlog N\nN\nand n\u2212s\u22121\nRecall that by assumption, as n, N \u2192 +\u221e, we have that log\nconverge to zero. Conn\u03bb2n\np\n\u2032\nsequently, the RHS converges to (1 + \u03b4 ) (1 + \u03b4)\u01eb as n, N \u2192 \u221e. Hence, we have\n\n\u221a\n1\nE[max Zj (v \u2217 )] < (1 + \u03b4\u2032 ) 1 + \u03b4 \u01eb.\nn\u2192+\u221e \u03bbn\nj\nlim\n\nSince \u03b4\u2032 > 0 and \u03b4 > 0 were arbitrary, the result follows.\n\nD.4\n\nProof of Lemma 5\n\nConsider the function f : RN \u2192 R given by\nf (w) :=\n\nmax\n\n1\u2264j\u2264N\n\nhq\n\n22\n\ni\nv \u2217 \u03a3(S c |S) w ,\n\n\fwhere \u03a3(S c |S) := \u03a3S c S c \u2212 \u03a3S c S (\u03a3SS )\u22121 \u03a3SS c . By construction, for a Gaussian random vector V \u223c\nd\nej .\nN (0, I), we have f (V ) = maxj\u2208S c Z\np\nWe now bound the Lipschitz constant of f . Let R = \u03a3(S c |S). For each w, v \u2208 RN and index\nj = 1, . . . , N , we have\n\u221a\n\u221a\n[ v \u2217 Rw]j \u2212 [ v \u2217 Rv]j\n\n\u2264\n\u2264\n\nP\n\n\u2264\n\n\u221a\n\nv\u2217\n\nX\nk\n\n\u221a\n\u221a\n\nv\u2217\n\nRjk [wk \u2212 vk ]\n\nsX\nk\n\n2 kw \u2212 vk\nRjk\n2\n\nv \u2217 kw \u2212 vk2 ,\n\n2\nwhere the last inequality follows since\nk Rjk = [\u03a3(S c |S) ]jj \u2264 1. Therefore, by Gaussian concentration of measure for Lipschitz functions [24, 27], we conclude that for any \u03b7 > 0, it holds\nthat\n\u0012\n\u0013\n\u03b72\nP[f (W ) \u2265 E[f (W )] + \u03b7] \u2264 exp \u2212 \u2217 ,\nand\n2v\n\u0013\n\u0012\n\u03b72\nP[f (W ) \u2264 E[f (W )] \u2212 \u03b7] \u2264 exp \u2212 \u2217 .\n2v\n\nD.5\n\nProof of Lemma 6\n\nSince the matrix XST XS is Wishart with n degrees of freedom, using properties of the inverse\n\u22121\nSS )\nWishart distribution, we have E[(XST XS )\u22121 ] = (\u03a3\nn\u2212s\u22121 (see Lemma 7.7.1 of Anderson [1]). Thus,\nwe compute\n\u2212\u03bbn n T\nE[Yi ] =\ne (\u03a3SS )\u22121 ~b ,\nand\nn\u2212s\u22121 i\n\u03c32\nn\n\u03c32\nE[Yi\u2032 ] =\neTi (\u03a3SS )\u22121 ei =\neT (\u03a3SS )\u22121 ei .\nn n\u2212s\u22121\nn\u2212s\u22121 i\nMoreover, using formulae for second moments of inverse Wishart matrices (see, e.g., Siskind [31]),\nwe compute for all n > s + 3\n\u0015\n\u0014\u0010\n\u0010\n\u0011\n\u00112\n\u0001\n1\n\u03bb2n n2\nT\n\u22121~\nT\n\u22121~\nT\n\u22121\n2\n~\nb (\u03a3SS ) b ei (\u03a3SS ) ei\nei (\u03a3SS ) b +\nE[Yi ] =\n(n \u2212 s) (n \u2212 s \u2212 3)\nn\u2212s\u22121\n\u0014\n\u0015\n\u00012\n\u03c3 4 n2\n1\n\u2032 2\nT\n\u22121\nE[(Yi ) ] =\ne (\u03a3SS ) ei\n1+\n.\n(n \u2212 s \u2212 1)2 (n \u2212 s) (n \u2212 s \u2212 3) i\nn\u2212s\u22121\nWe now compute and bound the variance of Yi . Setting Ai = eTi (\u03a3SS )\u22121~b and Bi = eTi (\u03a3SS )\u22121~b\nfor shorthand, we have\n\u0015\n\u0014\n\u03bb2n n2\n\u03bb2n n2\n1\nvar(Yi ) =\nAi Bi \u2212\nA2\nA2i +\n(n \u2212 s) (n \u2212 s \u2212 3)\nn\u2212s\u22121\n(n \u2212 s \u2212 1)2 i\n\u0013\n\u0014\n\u0015\n\u0012\n1\n(n \u2212 s) (n \u2212 s \u2212 3)\n\u03bb2n n2\n2\nAi Bi\n+\nAi 1 \u2212\n=\n(n \u2212 s) (n \u2212 s \u2212 3)\n(n \u2212 s \u2212 1)2\nn\u2212s\u22121\n\u0015\n\u0014\nAi Bi\n3A2i\n2\n+\n\u2264 2\u03bbn\nn\u2212s n\u2212s\u22121\n23\n\n\ffor n sufficiently large. Using the bound k(\u03a3SS )\u22121 k\u221e \u2264 Dmax , we see that the quantities Ai and\nBi are uniformly bounded for all i. Hence, we conclude that, for n sufficiently large, the variance\nis bounded as\nvar(Yi ) \u2264\n\nK\u03bb2n\nn\u2212s\n\n(49)\n\nfor some fixed constant K independent of s and n.\nmax \u03bbn n\n, we have\nNow since |E[Yi ]| \u2264 2Dn\u2212s\u22121\n|Yi \u2212 E[Yi ]| \u2265 |Yi | \u2212 |E[Yi ]| \u2265 |Yi | \u2212\n\n2Dmax \u03bbn n\n.\nn\u2212s\u22121\n\nConsequently, making use of Chebyshev's inequality, we have\nP[|Yi | \u2265\n\n6Dmax \u03bbn n\n2Dmax \u03bbn n\n4Dmax \u03bbn n\n] = P[|Yi | \u2212\n\u2265\n]\nn\u2212s\u22121\nn\u2212s\u22121\nn\u2212s\u22121\n4Dmax \u03bbn n\n\u2264 P[|Yi \u2212 E[Yi ]| \u2265\n]\nn\u2212s\u22121\nvar(Yi )\n\u2264\n2 \u03bb2\n16Dmax\nn\nK\n\u2264\n,\n16Dmax (n \u2212 s)\n\nwhere the final step uses the bound (49).\nWe now compute and bound the variance of Yi\u2032 . We have\n\u0012\n\u0015\u0013\n\u0014\n\u03c34\n\u03c3 4 n2\n1\n2\n\u2032\nA2\nA\n\u2212\nvar(Yi ) =\n1\n+\ni\n(n \u2212 s \u2212 1)2 (n \u2212 s) (n \u2212 s \u2212 3)\nn\u2212s\u22121\n(n \u2212 s \u2212 1)2 i\n\u0015\u0013\n\u0012\n\u0014\n(n \u2212 s) (n \u2212 s \u2212 3)\n1\n\u03c3 4 n2\n2\n\u2212\nAi 1 +\n=\n(n \u2212 s \u2212 1)2 (n \u2212 s) (n \u2212 s \u2212 3)\nn\u2212s\u22121\nn2\nK\u03c3 4\n\u2264\n(n \u2212 s \u2212 1)3\nfor some constant K independent of s and n. Consequently, applying Chebyshev's inequality, we\nhave\nP[Yi\u2032 \u2265 2E[Yi\u2032 ]] = P[Yi\u2032 \u2212 E[Yi\u2032 ] \u2265 E[Yi\u2032 ]] \u2264\n\u2264\n\u2264\n\u2264\nfor some constant K \u2032 independent of s and n.\n24\n\nvar(Yi\u2032 )\n(E[Yi\u2032 ])2\nK\n(n \u2212 s \u2212 1)3\n\n1\n\u03c34 T\n\u22121\nn2 ei (\u03a3SS ) ei\n\nKn2 Cmax\n\u03c3 4 (n \u2212 s \u2212 1)3\nK\u2032\nn\u2212s\u22121\n\n\fD.6\n\nProof of Lemma 7\n\nAs in the proof of Lemma 4, we define and bound\n\u2206Z (i, j) := E[(Zi \u2212 Zj )2 ] \u2264 2Cmax v \u2217 .\nNow let (X1 , . . . , XN ) be an i.i.d. zero-mean Gaussian vector with var(Xi ) = Cmax v \u2217 , so that\n\u2206X (i, j) := E[(Xi \u2212 Xj )2 ] = 2Cmax v \u2217 . If we set\n\u2206\u2217 :=\n\nmax |\u2206X (i, j) \u2212 \u2206Z (i, j)| ,\n\ni,j\u2208S c\n\nthen, by applying a known error bound for the Sudakov-Fernique inequality [5], we are guaranteed\nthat\np\nE[maxc Zj ] \u2265 E[maxc Xj ] \u2212 \u2206\u2217 log N .\n(50)\nj\u2208S\n\nj\u2208S\n\nWe now show that the quantity \u2206\u2217 is upper bounded by\n\u2206\u2217 \u2264 2v \u2217 (Cmax \u2212\n\n1\nCmax\n\n).\n\nUsing the inversion formula for block-partitioned matrices [22], we have\n\u0002\n\u0003\n\u03a3(S c |S) := \u03a3S c S c \u2212 \u03a3S c S (\u03a3SS )\u22121 \u03a3SS c = \u03a3\u22121 S c S c .\n\nConsequently, we have the lower bound\n\nE[(Zi \u2212 Zj )2 ] = v \u2217 (ei \u2212 ej )T \u03a3(S c |S) (ei \u2212 ej )\n\u2265 2v \u2217 \u039bmin (\u03a3(S c |S) )\n\n\u2265 2v \u2217 \u039bmin (\u03a3\u22121 )\n2v \u2217\n.\n=\nCmax\nIn turn, this leads to the upper bound\n\u2206\u2217 =\n\nmax |\u2206X (i, j) \u2212 \u2206Z (i, j)|\n\ni,j\u2208S c\n\nmax [2v \u2217 Cmax \u2212 \u2206Z (i, j)]\n\u0012\n\u0013\n1\n\u2217\n\u2264 2v\nCmax \u2212\n.\nCmax\n=\n\ni,j\u2208S c\n\nWe now analyze the behavior of E[maxj\u2208S c Xj ]. Using asymptotic results on the extrema of\nc Xj ]\nE[max\ni.i.d. Gaussian sequences [19, 7], we have limN \u2192+\u221e \u221a2C j\u2208S\n= 1. Consequently, for all \u03b4\u2032 > 0,\n\u2217\nmax v log N\nthere exists an N (\u03b4\u2032 ) such that for all N \u2265 N (\u03b4\u2032 ), we have\np\nE[maxc Xj ] \u2265 (1 \u2212 \u03b4\u2032 ) 2Cmax v \u2217 log N .\nj\u2208S\n\n25\n\n\fApplying this lower bound to the bound (50), we have\ni\np\np\n1\n1 h\nE[maxc Zj ] \u2265\n(1 \u2212 \u03b4\u2032 ) 2Cmax v \u2217 log N \u2212 \u2206\u2217 log N\n\u03bbn j\u2208S\n\u03bbn\nr\n\u0015\n\u0014\np\n1\n1\n\u2032\n\u2217\n\u2217\n\u2265\n) log N\n(1 \u2212 \u03b4 ) 2Cmax v log N \u2212 2 v (Cmax \u2212\n\u03bbn\nCmax\nr\n\u0014\n\u0015 s \u2217\np\n1\nv\n\u2032\n2 2 log N .\n= (1 \u2212 \u03b4 ) Cmax \u2212 Cmax \u2212\nCmax\n\u03bbn\n\u03bb2n\nv\u2217\n\n(51)\n\nFirst, assume that \u03bb2n /v \u2217 does not diverge to infinity. Then, there exists some \u03b1 > 0 such that\n\u2264 \u03b1 for all sufficiently large n. In this case, we have from the bound (51) that\n\np\n1\nE[maxc Zj ] \u2265 \u03b3 log N\n\u03bbn j\u2208S\nq\ni\nh\n\u221a\n1\n\u221a1 > 0. (Note that by choosing \u03b4 \u2032 > 0 sufficiently\nwhere \u03b3 := (1 \u2212 \u03b4\u2032 ) Cmax \u2212 Cmax \u2212 Cmax\n\u03b1\nsmall, we can always guarantee that \u03b3 > 0, since Cmax \u2265 1.) This completes the proof of condition\n(b) in the lemma statement.\nOtherwise, we may assume that \u03bb2n /v \u2217 \u2192 +\u221e. We compute\ns\np\n\u221a\n2\u03c3 2 (1 \u2212 ns ) log N\n1\n2 log N ~ T\n\u2217\n\u22121\n~\n2v log N =\n1\u2212\u03b4\nb (\u03a3SS ) b +\n\u03bbn\nn\u2212s\u22121\nn\u03bb2n\nr\n\u221a\n2 log N ~ T\n1\u2212\u03b4\n\u2265\nb (\u03a3SS )\u22121~b\nn\u2212s\u22121\nr\nr\n1\u2212\u03b4\n2s log N\n.\n\u2265\nCmax n \u2212 s \u2212 1\nWe now apply the condition\n2s log N\nn\u2212s\u22121\nto obtain that\n\n>\n\n1\n= Cmax (2 \u2212 \u01eb)2 /\n\u03b8l \u2212 \u03bd\n\n\"\u0014\np\n\nCmax \u2212\n\nr\n\nCmax \u2212\n\n1\nCmax\n\n\u00152\n\n2\n\n\u2212 \u03bdCmax (2 \u2212 \u01eb)\n\nq\n\u221a\n1\n\u2032) C\n\u2212\nCmax \u2212 Cmax\n(1\n\u2212\n\u03b4\np\nmax\n1\n(2 \u2212 \u01eb)\nE[max Zj ] \u2265\n(1 \u2212 \u03b4) rh\nq\ni2\n\u03bbn j\u2208S c\n\u221a\n1\n2\nCmax \u2212 Cmax \u2212 Cmax \u2212 \u03bdCmax (2 \u2212 \u01eb)\n\n#\n\n(52)\n\nRecall that \u03bdCmax (2 \u2212 \u01eb)2 > 0 is fixed, and moreover that \u03b4, \u03b4\u2032 > 0 are arbitrary. Let F (\u03b4, \u03b4\u2032 )\nbe the lower bound on the RHS (52). Note that F is a continuous function, and moreover that\nq\n\u221a\n1\nCmax \u2212 Cmax \u2212 Cmax\nF (0, 0) = rh\n(2 \u2212 \u01eb) > (2 \u2212 \u01eb).\nq\ni2\n\u221a\n1\n2\nCmax \u2212 Cmax \u2212 Cmax \u2212 \u03bdCmax (2 \u2212 \u01eb)\n\nTherefore, by the continuity of F , we can choose \u03b4, \u03b4\u2032 > 0 sufficiently small to ensure that for some\n\u03b3 > 0, we have \u03bb1n E[maxj\u2208S c Zj ] \u2265 (2 \u2212 \u01eb) (1 + \u03b3) for all sufficiently large n.\n26\n\n\fD.7\n\nProof of Lemma 8\n\nThis claim follows from the proof of Lemma 5.\n\nReferences\n[1] T. W. Anderson. An Introduction to Multivariate Statistical Analysis. Wiley Series in Probability and Mathematical Statistics. Wiley, New York, 1984.\n[2] P. J. Bickel and K. A. Doksum. Mathematical statistics: basic ideas and selected topics.\nPrentice Hall, Upper Saddle River, N.J., 2001.\n[3] E. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction\nfrom highly incomplete frequency information. Technical report, Applied and Computational\nMathematics, Caltech, 2004.\n[4] E. Candes and T. Tao. Decoding by linear programming.\n51(12):4203\u20134215, December 2005.\n\nIEEE Trans. Info Theory,\n\n[5] S. Chatterjee. An error bound in the Sudakov-Fernique inequality. Technical report, UC\nBerkeley, October 2005. arXiv:math.PR/0510424.\n[6] S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM\nJ. Sci. Computing, 20(1):33\u201361, 1998.\n[7] H. A. David and H. N. Nagaraja. Order Statistics. Wiley Series in Probability and Statistics.\nWiley, New York, 2003.\n[8] R. A. DeVore and G. G. Lorentz. Constructive Approximation. Springer-Verlag, New York,\nNY, 1993.\n[9] D. Donoho. For most large undetermined system of linear equations the minimal l1 -norm\nnear-solution is also the sparsest solution. Technical report, Statistics Department, Stanford\nUniversity, 2004.\n[10] D. Donoho. Compressed sensing. IEEE Trans. Info Theory, 52(4):1289\u20131306, April 2006.\n[11] D. Donoho, M. Elad, and V. M. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Trans. Info Theory, 52(1):6\u201318, January 2006.\n[12] D. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Trans.\nInfo Theory, 47(7):2845\u20132862, 2001.\n[13] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of\nStatistics, 32(2):407\u2013499, 2004.\n[14] M. Elad and A. M. Bruckstein. A generalized uncertainty principle and sparse representation\nin pairs of bases. IEEE Trans. Info Theory, 48(9):2558\u20132567, September 2002.\n[15] A. Feuer and A. Nemirovski. On sparse representation in pairs of bases. IEEE Trans. Info\nTheory, 49(6):1579\u20131581, 2003.\n27\n\n\f[16] A. K. Fletcher, S. Rangan, V. K. Goyal, and K. Ramchandran. Denoising by sparse approximation: Error bounds based on rate-distortion theory. Journal on Applied Signal Processing,\n10:1\u201319, 2006.\n[17] J. J. Fuchs. Recovery of exact sparse representations in the presence of noise. In ICASSP,\nvolume 2, pages 533\u2013536, 2004.\n[18] J. J. Fuchs. Recovery of exact sparse representations in the presence of noise. IEEE Trans.\nInfo. Theory, 51(10):3601\u20133608, October 2005.\n[19] J. Galambos. The Asymptotic Theory of Extreme Order Statistics. Wiley Series in Probability\nand Mathematical Statistics. Wiley, New York, 1978.\n[20] R. M. Gray. Toeplitz and Circulant Matrices: A Review. Technical report, Stanford University,\nInformation Systems Laboratory, 1990.\n[21] J. Hiriart-Urruty and C. Lemar\u00e9chal. Convex Analysis and Minimization Algorithms, volume 1.\nSpringer-Verlag, New York, 1993.\n[22] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge,\n1985.\n[23] K. Knight and W. J. Fu. Asymptotics for lasso-type estimators. Annals of Statistics, 28:1356\u2013\n1378, 2000.\n[24] M. Ledoux. The Concentration of Measure Phenomenon. Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI, 2001.\n[25] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.\nSpringer-Verlag, New York, NY, 1991.\n[26] D. M. Malioutov, M. Cetin, and A. S. Willsky. Optimal sparse representations in general\novercomplete bases. In Int. Conf. on Acoustics, Speech, and Signal Processing, volume 2,\npages II\u2013793\u2013796, May 2004.\n[27] P. Massart. Concentration Inequalties and Model Selection. Ecole d'Et\u00e9 de Probabilit\u00e9s, SaintFlour. Springer, New York, 2003.\n[28] N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the\nlasso. Annals of Statistics, 2006. To appear.\n[29] A. J. Miller. Subset selection in regression. Chapman-Hall, New York, NY, 1990.\n[30] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Computing,\n24(2):227\u2013234, 1995.\n[31] V. Siskind. Second moments of inverse Wishart-matrix elements. Biometrika, 59(3):690\u2013691,\nDecember 1972.\n[32] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\nSociety, Series B, 58(1):267\u2013288, 1996.\n28\n\n\f[33] J. Tropp. Greed is good: algorithmic results for sparse approximation. IEEE Trans. Info\nTheory, 50(10):2231\u20132242, 2004.\n[34] J. Tropp. Just relax: Convex programming methods for identifying sparse signals in noise.\nIEEE Trans. Info Theory, 52(3):1030\u20131051, March 2006.\n[35] P. Zhao and B. Yu. Model selection with the lasso. Technical report, UC Berkeley, Department\nof Statistics, March 2006. Accepted to Journal of Machine Learning Research.\n\n29\n\n\f"}