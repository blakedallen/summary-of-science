{"id": "http://arxiv.org/abs/1110.1391v1", "guidislink": true, "updated": "2011-10-06T20:48:58Z", "updated_parsed": [2011, 10, 6, 20, 48, 58, 3, 279, 0], "published": "2011-10-06T20:48:58Z", "published_parsed": [2011, 10, 6, 20, 48, 58, 3, 279, 0], "title": "A Comparison of Different Machine Transliteration Models", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.2840%2C1110.2868%2C1110.1006%2C1110.2572%2C1110.3765%2C1110.1741%2C1110.3918%2C1110.2750%2C1110.0472%2C1110.3287%2C1110.3005%2C1110.1194%2C1110.3503%2C1110.6556%2C1110.2889%2C1110.5355%2C1110.5625%2C1110.1265%2C1110.4147%2C1110.0266%2C1110.5098%2C1110.1820%2C1110.1501%2C1110.2984%2C1110.4854%2C1110.4353%2C1110.0905%2C1110.4169%2C1110.6569%2C1110.1554%2C1110.6658%2C1110.6355%2C1110.0630%2C1110.6591%2C1110.4798%2C1110.5468%2C1110.5861%2C1110.3845%2C1110.3281%2C1110.4650%2C1110.0655%2C1110.3505%2C1110.0964%2C1110.2713%2C1110.5329%2C1110.0613%2C1110.0447%2C1110.5427%2C1110.1391%2C1110.4222%2C1110.0845%2C1110.4969%2C1110.3133%2C1110.4628%2C1110.1485%2C1110.1805%2C1110.3874%2C1110.0043%2C1110.4626%2C1110.6739%2C1110.4132%2C1110.4977%2C1110.1420%2C1110.1201%2C1110.2782%2C1110.3772%2C1110.4188%2C1110.6442%2C1110.0712%2C1110.0620%2C1110.6689%2C1110.0410%2C1110.1347%2C1110.0962%2C1110.3267%2C1110.5956%2C1110.6866%2C1110.3250%2C1110.5886%2C1110.2120%2C1110.5116%2C1110.1255%2C1110.5014%2C1110.6459%2C1110.0368%2C1110.4912%2C1110.6450%2C1110.3348%2C1110.3434%2C1110.5004%2C1110.0537%2C1110.1213%2C1110.0471%2C1110.0752%2C1110.1344%2C1110.6814%2C1110.3842%2C1110.5719%2C1110.2224%2C1110.3203%2C1110.5379&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Comparison of Different Machine Transliteration Models"}, "summary": "Machine transliteration is a method for automatically converting words in one\nlanguage into phonetically equivalent ones in another language. Machine\ntransliteration plays an important role in natural language applications such\nas information retrieval and machine translation, especially for handling\nproper nouns and technical terms. Four machine transliteration models --\ngrapheme-based transliteration model, phoneme-based transliteration model,\nhybrid transliteration model, and correspondence-based transliteration model --\nhave been proposed by several researchers. To date, however, there has been\nlittle research on a framework in which multiple transliteration models can\noperate simultaneously. Furthermore, there has been no comparison of the four\nmodels within the same framework and using the same data. We addressed these\nproblems by 1) modeling the four models within the same framework, 2) comparing\nthem under the same conditions, and 3) developing a way to improve machine\ntransliteration through this comparison. Our comparison showed that the hybrid\nand correspondence-based models were the most effective and that the four\nmodels can be used in a complementary manner to improve machine transliteration\nperformance.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1110.2840%2C1110.2868%2C1110.1006%2C1110.2572%2C1110.3765%2C1110.1741%2C1110.3918%2C1110.2750%2C1110.0472%2C1110.3287%2C1110.3005%2C1110.1194%2C1110.3503%2C1110.6556%2C1110.2889%2C1110.5355%2C1110.5625%2C1110.1265%2C1110.4147%2C1110.0266%2C1110.5098%2C1110.1820%2C1110.1501%2C1110.2984%2C1110.4854%2C1110.4353%2C1110.0905%2C1110.4169%2C1110.6569%2C1110.1554%2C1110.6658%2C1110.6355%2C1110.0630%2C1110.6591%2C1110.4798%2C1110.5468%2C1110.5861%2C1110.3845%2C1110.3281%2C1110.4650%2C1110.0655%2C1110.3505%2C1110.0964%2C1110.2713%2C1110.5329%2C1110.0613%2C1110.0447%2C1110.5427%2C1110.1391%2C1110.4222%2C1110.0845%2C1110.4969%2C1110.3133%2C1110.4628%2C1110.1485%2C1110.1805%2C1110.3874%2C1110.0043%2C1110.4626%2C1110.6739%2C1110.4132%2C1110.4977%2C1110.1420%2C1110.1201%2C1110.2782%2C1110.3772%2C1110.4188%2C1110.6442%2C1110.0712%2C1110.0620%2C1110.6689%2C1110.0410%2C1110.1347%2C1110.0962%2C1110.3267%2C1110.5956%2C1110.6866%2C1110.3250%2C1110.5886%2C1110.2120%2C1110.5116%2C1110.1255%2C1110.5014%2C1110.6459%2C1110.0368%2C1110.4912%2C1110.6450%2C1110.3348%2C1110.3434%2C1110.5004%2C1110.0537%2C1110.1213%2C1110.0471%2C1110.0752%2C1110.1344%2C1110.6814%2C1110.3842%2C1110.5719%2C1110.2224%2C1110.3203%2C1110.5379&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Machine transliteration is a method for automatically converting words in one\nlanguage into phonetically equivalent ones in another language. Machine\ntransliteration plays an important role in natural language applications such\nas information retrieval and machine translation, especially for handling\nproper nouns and technical terms. Four machine transliteration models --\ngrapheme-based transliteration model, phoneme-based transliteration model,\nhybrid transliteration model, and correspondence-based transliteration model --\nhave been proposed by several researchers. To date, however, there has been\nlittle research on a framework in which multiple transliteration models can\noperate simultaneously. Furthermore, there has been no comparison of the four\nmodels within the same framework and using the same data. We addressed these\nproblems by 1) modeling the four models within the same framework, 2) comparing\nthem under the same conditions, and 3) developing a way to improve machine\ntransliteration through this comparison. Our comparison showed that the hybrid\nand correspondence-based models were the most effective and that the four\nmodels can be used in a complementary manner to improve machine transliteration\nperformance."}, "authors": ["K. Choi", "H. Isahara", "J. Oh"], "author_detail": {"name": "J. Oh"}, "author": "J. Oh", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1613/jair.1999", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1110.1391v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1110.1391v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1110.1391v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1110.1391v1", "arxiv_comment": null, "journal_reference": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  119-151, 2006", "doi": "10.1613/jair.1999", "fulltext": "Journal of Artificial Intelligence Research 27 (2006) 119\u2013151\n\nSubmitted 1/06; published 10/06\n\nA Comparison of Different Machine Transliteration Models\nJong-Hoon Oh\n\nrovellia@nict.go.jp\n\narXiv:1110.1391v1 [cs.CL] 6 Oct 2011\n\nComputational Linguistics Group\nNational Institute of Information and Communications Technology (NICT)\n3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan\n\nKey-Sun Choi\n\nkschoi@cs.kaist.ac.kr\n\nComputer Science Division, Department of EECS\nKorea Advanced Institute of Science and Technology (KAIST)\n373-1 Guseong-dong, Yuseong-gu, Daejeon 305-701 Republic of Korea\n\nHitoshi Isahara\n\nisahara@nict.go.jp\n\nComputational Linguistics Group\nNational Institute of Information and Communications Technology (NICT)\n3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan\n\nAbstract\nMachine transliteration is a method for automatically converting words in one language into phonetically equivalent ones in another language. Machine transliteration plays\nan important role in natural language applications such as information retrieval and machine translation, especially for handling proper nouns and technical terms. Four machine\ntransliteration models \u2013 grapheme-based transliteration model, phoneme-based transliteration model, hybrid transliteration model, and correspondence-based transliteration model \u2013\nhave been proposed by several researchers. To date, however, there has been little research\non a framework in which multiple transliteration models can operate simultaneously. Furthermore, there has been no comparison of the four models within the same framework and\nusing the same data. We addressed these problems by 1) modeling the four models within\nthe same framework, 2) comparing them under the same conditions, and 3) developing a\nway to improve machine transliteration through this comparison. Our comparison showed\nthat the hybrid and correspondence-based models were the most effective and that the\nfour models can be used in a complementary manner to improve machine transliteration\nperformance.\n\n1. Introduction\nWith the advent of new technology and the flood of information through the Web, it has\nbecome increasingly common to adopt foreign words into one's language. This usually entails adjusting the adopted word's original pronunciation to follow the phonological rules\nof the target language, along with modification of its orthographical form. This phonetic\n\"translation\" of foreign words is called transliteration. For example, the English word\ndata is transliterated into Korean as 'de-i-teo'1 and into Japanese as 'de-e-ta'. Transliteration is particularly used to translate proper names and technical terms from languages\n1. In this paper, target language transliterations are represented in their Romanized form with single\nquotation marks and hyphens between syllables.\nc 2006 AI Access Foundation. All rights reserved.\n\n\fOh, Choi, & Isahara\n\nusing Roman alphabets into ones using non-Roman alphabets such as from English to\nKorean, Japanese, or Chinese. Because transliteration is one of the main causes of the\nout-of-vocabulary (OOV) problem, transliteration by means of dictionary lookup is impractical (Fujii & Tetsuya, 2001; Lin & Chen, 2002). One way to solve the OOV problem is\nto use machine transliteration. Machine transliteration is usually used to support machine\ntranslation (MT) (Knight & Graehl, 1997; Al-Onaizan & Knight, 2002) and cross-language\ninformation retrieval (CLIR) (Fujii & Tetsuya, 2001; Lin & Chen, 2002). For CLIR, machine\ntransliteration bridges the gap between the transliterated localized form and its original form\nby generating all possible transliterations from the original form (or generating all possible\noriginal forms from the transliteration)2 . For example, machine transliteration can assist\nquery translation in CLIR, where proper names and technical terms frequently appear in\nsource language queries. In the area of MT, machine transliteration helps preventing translation errors when translations of proper names and technical terms are not registered in\nthe translation dictionary. Machine transliteration can therefore improve the performance\nof MT and CLIR.\nFour machine transliteration models have been proposed by several researchers: grapheme3 -based transliteration model (\u03c8G ) (Lee & Choi, 1998; Jeong, Myaeng, Lee, &\nChoi, 1999; Kim, Lee, & Choi, 1999; Lee, 1999; Kang & Choi, 2000; Kang & Kim, 2000;\nKang, 2001; Goto, Kato, Uratani, & Ehara, 2003; Li, Zhang, & Su, 2004), phoneme4 based transliteration model (\u03c8P ) (Knight & Graehl, 1997; Lee, 1999; Jung, Hong, &\nPaek, 2000; Meng, Lo, Chen, & Tang, 2001), hybrid transliteration model (\u03c8H ) (Lee,\n1999; Al-Onaizan & Knight, 2002; Bilac & Tanaka, 2004), and correspondence-based\ntransliteration model (\u03c8C ) (Oh & Choi, 2002). These models are classified in terms of\nthe units to be transliterated. The \u03c8G is sometimes referred to as the direct method because\nit directly transforms source language graphemes into target language graphemes without\nany phonetic knowledge of the source language words. The \u03c8P is sometimes referred to as\nthe pivot method because it uses source language phonemes as a pivot when it produces\ntarget language graphemes from source language graphemes. The \u03c8P therefore usually\nneeds two steps: 1) produce source language phonemes from source language graphemes;\n2) produce target language graphemes from source phonemes5 . The \u03c8H and \u03c8C make use\nof both source language graphemes and source language phonemes when producing target\nlanguage transliterations. Hereafter, we refer to a source language grapheme as a source\n2. The former process is generally called \"transliteration\", and the latter is generally called \"backtransliteration\" (Knight & Graehl, 1997)\n3. Graphemes refer to the basic units (or the smallest contrastive units) of a written language: for example,\nEnglish has 26 graphemes or letters, Korean has 24, and German has 30.\n4. Phonemes are the simplest significant unit of sound (or the smallest contrastive units of a spoken language); for example, /M/, /AE/, and /TH/ in /M AE TH/, the pronunciation of math. We use the\nARPAbet symbols to represent source phonemes. ARPAbet is one of the methods used for coding source\nphonemes into ASCII characters (http://www.cs.cmu.edu/~ laura/pages/arpabet.ps). Here we denote\nsource phonemes and pronunciation with two slashes, as in /AH/, and use pronunciation based on The\nCMU Pronunciation Dictionary and The American Heritage(r) Dictionary of the English Language.\n5. These two steps are explicit if the transliteration system produces target language transliterations after\nproducing the pronunciations of the source language words; they are implicit if the system uses phonemes\nimplicitly in the transliteration stage and explicitly in the learning stage, as described elsewhere (Bilac\n& Tanaka, 2004)\n\n120\n\n\fA Comparison of Machine Transliteration Models\n\ngrapheme, a source language phoneme as a source phoneme, and a target language grapheme\nas a target grapheme.\nThe transliterations produced by the four models usually differ because the models use\ndifferent information. Generally, transliteration is a phonetic process, as in \u03c8P , rather\nthan an orthographic one, as in \u03c8G (Knight & Graehl, 1997). However, standard transliterations are not restricted to phoneme-based transliterations. For example, the standard\nKorean transliterations of data, amylase, and neomycin are, respectively, the phonemebased transliteration 'de-i-teo', the grapheme-based transliteration 'a-mil-la-a-je', and 'neo-ma-i-sin', which is a combination of the grapheme-based transliteration 'ne-o' and the\nphoneme-based transliteration 'ma-i-sin'. Furthermore, if the unit to be transliterated is\nrestricted to either a source grapheme or a source phoneme, it is hard to produce the correct\ntransliteration in many cases. For example, \u03c8P cannot easily produce the grapheme-based\ntransliteration 'a-mil-la-a-je', the standard Korean transliteration of amylase, because \u03c8P\ntends to produce 'a-mil-le-i-seu' based on the sequence of source phonemes /AE M AH\nL EY S/. Multiple transliteration models should therefore be applied to better cover the\nvarious transliteration processes. To date, however, there has been little published research\nregarding a framework in which multiple transliteration models can operate simultaneously.\nFurthermore, there has been no reported comparison of the transliteration models within\nthe same framework and using the same data although many English-to-Korean transliteration methods based on \u03c8G have been compared to each other with the same data (Kang\n& Choi, 2000; Kang & Kim, 2000; Oh & Choi, 2002).\nTo address these problems, we 1) modeled a framework in which the four transliteration models can operate simultaneously, 2) compared the transliteration\nmodels under the same conditions, and 3) using the results of the comparison,\ndeveloped a way to improve the performance of machine transliteration.\nThe rest of this paper is organized as follows. Section 2 describes previous work relevant\nto our study. Section 3 describes our implementation of the four transliteration models.\nSection 4 describes our testing and results. Section 5 describes a way to improve machine\ntransliteration based on the results of our comparison. Section 6 describes a transliteration ranking method that can be used to improve transliteration performance. Section 7\nconcludes the paper with a summary and a look at future work.\n\n2. Related Work\nMachine transliteration has received significant research attention in recent years. In most\ncases, the source language and target language have been English and an Asian language, respectively \u2013 for example, English to Japanese (Goto et al., 2003), English to Chinese (Meng\net al., 2001; Li et al., 2004), and English to Korean (Lee & Choi, 1998; Kim et al., 1999;\nJeong et al., 1999; Lee, 1999; Jung et al., 2000; Kang & Choi, 2000; Kang & Kim, 2000;\nKang, 2001; Oh & Choi, 2002). In this section, we review previous work related to the four\ntransliteration models.\n2.1 Grapheme-based Transliteration Model\nConceptually, the \u03c8G is direct orthographical mapping from source graphemes to target\ngraphemes. Several transliteration methods based on this model have been proposed, such\n121\n\n\fOh, Choi, & Isahara\n\nas those based on a source-channel model (Lee & Choi, 1998; Lee, 1999; Jeong et al.,\n1999; Kim et al., 1999), a decision tree (Kang & Choi, 2000; Kang, 2001), a transliteration\nnetwork (Kang & Kim, 2000; Goto et al., 2003), and a joint source-channel model (Li et al.,\n2004).\nThe methods based on the source-channel model deal with English-Korean transliteration. They use a chunk of graphemes that can correspond to a source phoneme. First,\nEnglish words are segmented into a chunk of English graphemes. Next, all possible chunks of\nKorean graphemes corresponding to the chunk of English graphemes are produced. Finally,\nthe most relevant sequence of Korean graphemes is identified by using the source-channel\nmodel. The advantage of this approach is that it considers a chunk of graphemes representing a phonetic property of the source language word. However, errors in the first step\n(segmenting the English words) propagate to the subsequent steps, making it difficult to\nproduce correct transliterations in those steps. Moreover, there is high time complexity\nbecause all possible chunks of graphemes are generated in both languages.\nIn the method based on a decision tree, decision trees that transform each source\ngrapheme into target graphemes are learned and then directly applied to machine transliteration. The advantage of this approach is that it considers a wide range of contextual\ninformation, say, the left three and right three contexts. However, it does not consider any\nphonetic aspects of transliteration.\nKang and Kim (2000) and Goto et al. (2003) proposed methods based on a transliteration network for, respectively, English-to-Korean and English-to-Japanese transliteration.\nTheir frameworks for constructing a transliteration network are similar \u2013 both are composed\nof nodes and arcs. A node represents a chunk of source graphemes and its corresponding\ntarget graphemes. An arc represents a possible link between nodes and has a weight showing\nits strength. Like the methods based on the source-channel model, their methods consider\nthe phonetic aspect in the form of chunks of graphemes. Furthermore, they segment a chunk\nof graphemes and identify the most relevant sequence of target graphemes in one step. This\nmeans that errors are not propagated from one step to the next, as in the methods based\non the source-channel model.\nThe method based on the joint source-channel model simultaneously considers the source\nlanguage and target language contexts (bigram and trigram) for machine transliteration.\nIts main advantage is the use of bilingual contexts.\n2.2 Phoneme-based Transliteration Model\nIn the \u03c8P , the transliteration key is pronunciation or the source phoneme rather than\nspelling or the source grapheme. This model is basically source grapheme-to-source phoneme\ntransformation and source phoneme-to-target grapheme transformation.\nKnight and Graehl (1997) modeled Japanese-to-English transliteration with weighted\nfinite state transducers (WFSTs) by combining several parameters including romaji-tophoneme, phoneme-to-English, English word probabilities, and so on. A similar model was\ndeveloped for Arabic-to-English transliteration (Stalls & Knight, 1998). Meng et al. (2001)\nproposed an English-to-Chinese transliteration method based on English grapheme-to-phoneme\nconversion, cross-lingual phonological rules, mapping rules between English phonemes and\nChinese phonemes, and Chinese syllable-based and character-based language models. Jung\n122\n\n\fA Comparison of Machine Transliteration Models\n\net al. (2000) modeled English-to-Korean transliteration with an extended Markov window.\nThe method transforms an English word into English pronunciation by using a pronunciation dictionary. Then it segments the English phonemes into chunks of English phonemes;\neach chunk corresponds to a Korean grapheme as defined by handcrafted rules. Finally, it\nautomatically transforms each chunk of English phonemes into Korean graphemes by using\nan extended Markov window.\nLee (1999) modeled English-to-Korean transliteration in two steps. The English graphemeto-English phoneme transformation is modeled in a manner similar to his method based\non the source-channel model described in Section 2.1. The English phonemes are then\ntransformed into Korean graphemes by using English-to-Korean standard conversion rules\n(EKSCR) (Korea Ministry of Culture & Tourism, 1995). These rules are in the form of\ncontext-sensitive rewrite rules, \"PA PX PB \u2192 y\", meaning that English phoneme PX is\nrewritten as Korean grapheme y in the context PA and PB , where PX , PA , and PB represent English phonemes. For example, \"PA = \u2217, PX = /SH/, PB = end \u2192 'si'\" means\n\"English phoneme /SH/ is rewritten into Korean grapheme 'si' if it occurs at the end of\nthe word (end) after any phoneme (\u2217)\". This approach suffers from both the propagation\nof errors and the limitations of EKSCR. The first step, grapheme-to-phoneme transformation, usually results in errors, and the errors propagate to the next step. Propagated errors\nmake it difficult for a transliteration system to work correctly. In addition, EKSCR does\nnot contain enough rules to generate correct Korean transliterations since its main focus is\nmapping from an English phoneme to Korean graphemes without taking into account the\ncontexts of the English grapheme.\n2.3 Hybrid and Correspondence-based Transliteration Models\nAttempts to use both source graphemes and source phonemes in machine transliteration\nled to the correspondence-based transliteration model (\u03c8C ) (Oh & Choi, 2002) and the\nhybrid transliteration model (\u03c8H ) (Lee, 1999; Al-Onaizan & Knight, 2002; Bilac & Tanaka,\n2004). The former makes use of the correspondence between a source grapheme and a source\nphoneme when it produces target language graphemes; the latter simply combines \u03c8G and\n\u03c8P through linear interpolation. Note that the \u03c8H combines the grapheme-based transliteration probability (P r(\u03c8G )) and the phoneme-based transliteration probability (P r(\u03c8P ))\nusing linear interpolation.\nOh and Choi (2002) considered the contexts of a source grapheme and its corresponding source phoneme for English-to-Korean transliteration. They used EKSCR as the basic rules in their method. Additional contextual rules are semi-automatically constructed\nby examining the cases in which EKSCR produced incorrect transliterations because of\na lack of contexts. These contextual rules are in the form of context-sensitive rewrite\nrules, \"CA CX CB \u2192 y\", meaning \"CX is rewritten as target grapheme y in the context\nCA and CB \". Note that CX , CA , and CB represent the correspondence between the English grapheme and phoneme. For example, we can read \"CA = (\u2217 : /V owel/), CX =\n(r : /R/), CB = (\u2217 : /Consonant/) \u2192 NULL\" as \"English grapheme r corresponding to\nphoneme /R/ is rewritten into null Korean graphemes when it occurs after vowel phonemes,\n(\u2217 : /V owel/), before consonant phonemes, (\u2217 : /Consonant/)\". The main advantage of\nthis approach is the application of a sophisticated rule that reflects the context of the source\n123\n\n\fOh, Choi, & Isahara\n\ngrapheme and source phoneme by considering their correspondence. However, there is lack\nof portability to other languages because the rules are restricted to Korean.\nSeveral researchers (Lee, 1999; Al-Onaizan & Knight, 2002; Bilac & Tanaka, 2004) have\nproposed hybrid model-based transliteration methods. They model \u03c8G and \u03c8P with WFSTs or a source-channel model and combine \u03c8G and \u03c8P through linear interpolation. In\ntheir \u03c8P , several parameters are considered, such as the source grapheme-to-source phoneme\nprobability, source phoneme-to-target grapheme probability, and target language word probability. In their \u03c8G , the source grapheme-to-target grapheme probability is mainly considered. The main disadvantage of the hybrid model is that the dependence between the source\ngrapheme and source phoneme is not taken into consideration in the combining process; in\ncontrast, Oh and Choi's approach (Oh & Choi, 2002) considers this dependence by using\nthe correspondence between the source grapheme and phoneme.\n\n3. Modeling Machine Transliteration Models\nIn this section, we describe our implementation of the four machine transliteration models\n(\u03c8G , \u03c8P , \u03c8H , and \u03c8C ) using three machine learning algorithms: memory-based learning,\ndecision-tree learning, and the maximum entropy model.\n3.1 Framework for Four Machine Transliteration Models\nFigure 1 summarizes the differences among the transliteration models and their component\nfunctions. The \u03c8G directly transforms source graphemes (S) into target graphemes (T).\nThe \u03c8P and \u03c8C transform source graphemes into source phonemes and then generate target\ngraphemes6 . While \u03c8P uses only the source phonemes, \u03c8C uses the correspondence between\nthe source grapheme and the source phoneme when it generates target graphemes. We\ndescribe their differences with two functions, \u03c6P T and \u03c6(SP )T . The \u03c8H is represented as the\nlinear interpolation of P r(\u03c8G ) and P r(\u03c8P ) by means of \u03b1 (0 \u2264 \u03b1 \u2264 1). Here, P r(\u03c8P ) is the\nprobability that \u03c8P will produce target graphemes, while P r(\u03c8G ) is the probability that \u03c8G\nwill produce target graphemes. We can thus regard \u03c8H as being composed of component\nfunctions of \u03c8G and \u03c8P (\u03c6SP , \u03c6P T , and \u03c6ST ). Here we use the maximum entropy model\nas the machine learning algorithm for \u03c8H because \u03c8H requires P r(\u03c8P ) and P r(\u03c8G ), and\nonly the maximum entropy model among memory-based learning, decision-tree learning,\nand the maximum entropy model can produce the probabilities.\nTo train each component function, we need to define the features that represent training\ninstances and data. Table 1 shows five feature types, fS , fP , fStype , fP type , and fT . The\nfeature types used depend on the component functions. The modeling of each component\nfunction with the feature types is explained in Sections 3.2 and 3.3.\n3.2 Component Functions of Each Transliteration Model\nTable 2 shows the definitions of the four component functions that we used. Each is defined\nin terms of its input and output: the first and last characters in the notation of each\ncorrespond respectively to its input and output. The role of each component function in\n6. According to (g\u25e6f )(x) = g(f (x)), we can write (\u03c6(SP )T \u25e6\u03c6SP )(x) = \u03c6(SP )T (\u03c6SP (x)) and (\u03c6P T \u25e6\u03c6SP )(x) =\n\u03c6P T (\u03c6SP (x)).\n\n124\n\n\fA Comparison of Machine Transliteration Models\n\n\u03c6ST\n\nS\n\nT\n\u03c6(SP)T\n\n\u03c6SP\n\nP\nSS:: Source graphemes\nP\nP:: Source Phonemes\nTT:: Target graphemes\n\n\u03c6PT\n\u03c8 G : \u03c6ST\n\u03c8 P : \u03c6PT o \u03c6SP\n\u03c8 C : \u03c6( SP )T o \u03c6SP\n\u03c8 H : \u03b1 \u00d7 Pr(\u03c8 P )\n+ (1 \u2212 \u03b1 ) \u00d7 Pr(\u03c8 G )\n\nFigure 1: Graphical representation of each component function and four transliteration\nmodels: S is a set of source graphemes (e.g., letters of the English alphabet), P is\na set of source phonemes defined in ARPAbet, and T is a set of target graphemes.\n\nFeature type\nfS\nfS,Stype\nfStype\nfP\nfP,P type\nfP type\nfT\n\nDescription and possible values\nSource graphemes in S:\n26 letters in English alphabet\nSource grapheme types:\nConsonant (C) and Vowel (V)\nSource phonemes in P\n(/AA/, /AE/, and so on)\nSource phoneme types: Consonant (C), Vowel (V),\nSemi-vowel (SV), and silence (/\u223c/)\nTarget graphemes in T\n\nTable 1: Feature types used for transliteration models: fS,Stype indicates both fS and fStype ,\nwhile fP,P type indicates both fP and fP type .\n\neach transliteration model is to produce the most relevant output from its input. The\nperformance of a transliteration model therefore depends strongly on that of its component\nfunctions. In other words, the better the modeling of each component function, the better\nthe performance of the machine transliteration system.\nThe modeling strongly depends on the feature type. Different feature types are used\nby the \u03c6(SP )T , \u03c6P T , and \u03c6ST functions, as shown in Table 2. These three component\nfunctions thus have different strengths and weaknesses for machine transliteration. The\n\u03c6ST function is good at producing grapheme-based transliterations and poor at producing\n125\n\n\fOh, Choi, & Isahara\n\nNotation\n\u03c6SP\n\u03c6(SP )T\n\u03c6P T\n\u03c6ST\n\nFeature types used\nfS,Stype , fP\nfS,Stype , fP,P type , fT\nfP,P type, fT\nfS,Stype , fT\n\nInput\nsi , c(si )\nsi , pi , c(si ), c(pi )\npi , c(pi )\nsi , c(si )\n\nOutput\npi\nti\nti\nti\n\nTable 2: Definition of each component function: si , c(si ), pi , c(pi ), and ti respectively represent the ith source grapheme, the context of si (si\u2212n , * * * , si\u22121 and si+1 , * * * , si+n ),\nthe ith source phoneme, the context of pi (pi\u2212n , * * * , pi\u22121 and pi+1 , * * * , pi+n ), and\nthe ith target grapheme.\n\nphoneme-based ones. In contrast, the \u03c6P T function is good at producing phoneme-based\ntransliterations and poor at producing grapheme-based ones. For amylase and its standard\nKorean transliteration, 'a-mil-la-a-je', which is a grapheme-based transliteration, \u03c6ST tends\nto produce the correct transliteration; \u03c6P T tends to produce wrong ones like 'ae-meol-le-iseu', which is derived from /AE M AH L EY S/, the pronunciation of amylase. In contrast,\n\u03c6P T can produce 'de-i-teo', which is the standard Korean transliteration of data and a\nphoneme-based transliteration, while \u03c6ST tends to give a wrong one, like 'da-ta'.\nThe \u03c6(SP )T function combines the advantages of \u03c6ST and \u03c6P T by utilizing the correspondence between the source grapheme and source phoneme. This correspondence enables \u03c6(SP )T to produce both grapheme-based and phoneme-based transliterations. Furthermore, the correspondence provides important clues for use in resolving transliteration\nambiguities7 . For example, the source phoneme /AH/ produces much ambiguity in machine transliteration because it can be mapped to almost every vowel in the source and\ntarget languages (the underlined graphemes in the following example corresponds to /AH/:\nholocaust in English, 'hol-lo-ko-seu-teu' in its Korean counterpart, and 'ho-ro-ko-o-su-to' in\nits Japanese counterpart). If we know the correspondence between the source grapheme and\nsource phoneme, we can more easily infer the correct transliteration of /AH/ because the\ncorrect target grapheme corresponding to /AH/ usually depends on the source grapheme\ncorresponding to /AH/. Moreover, there are various Korean transliterations of the source\ngrapheme a: 'a', 'ae', 'ei', 'i', and 'o'. In this case, the English phonemes corresponding\nto the English grapheme can help a component function resolve transliteration ambiguities, as shown in Table 3. In Table 3, the a underlined in the example words shown in\nthe last column is pronounced as the English phoneme in the second column. By looking\nat English grapheme and its corresponding English phoneme, we can find correct Korean\ntransliterations more easily.\nThough \u03c6(SP )T is more effective than both \u03c6ST and \u03c6P T in many cases, \u03c6(SP )T sometimes works poorly when the standard transliteration is strongly biased to either graphemebased or phoneme-based transliteration. In such cases, either the source grapheme or source\nphoneme does not contribute to the correct transliteration, making it difficult for \u03c6(SP )T\nto produce the correct transliteration. Because \u03c6ST , \u03c6P T , and \u03c6(SP )T are the core parts\n7. Though contextual information can also be used to reduce ambiguities, we limit our discussion here to\nthe feature type.\n\n126\n\n\fA Comparison of Machine Transliteration Models\n\nKorean Grapheme\n'a'\n'ae'\n'ei'\n'i'\n'o'\n\nEnglish Phoneme\n/AA/\n/AE/\n/EY/\n/IH/\n/AO/\n\nExample usage\nadagio, safari, vivace\nadvantage, alabaster, travertine\nchamber, champagne, chaos\nadvantage, average, silage\nallspice, ball, chalk\n\nTable 3: Examples of Korean graphemes derived from English grapheme a and its corresponding English phonemes: the underlines in the example words indicate the\nEnglish grapheme corresponding to English phonemes in the second column.\n\nof \u03c8G , \u03c8P , and \u03c8C , respectively, the advantages and disadvantages of the three component\nfunctions correspond to those of the transliteration models in which each is used.\nTransliteration usually depends on context. For example, the English grapheme a can\nbe transliterated into Korean graphemes on the basis of its context, like 'ei' in the context\nof -ation and 'a' in the context of art. When context information is used, determining\nthe context window size is important. A context window that is too narrow can degrade\ntransliteration performance because of a lack of context information. For example, when\nEnglish grapheme t in -tion is transliterated into Korean, the one right English grapheme is\ninsufficient as context because the three right contexts, -ion, are necessary to get the correct\nKorean grapheme, 's'. A context window that is too wide can also degrade transliteration\nperformance because it reduces the power to resolve transliteration ambiguities. Many\nprevious studies have determined that an appropriate context window size is 3. In this\npaper, we use a window size of 3, as in previous work (Kang & Choi, 2000; Goto et al.,\n2003). The effect of the context window size on transliteration performance will be discussed\nin Section 4.\nTable 4 shows how to identify the most relevant output in each component function using\ncontext information. The L3-L1, C0, and R1-R3 represent the left context, current context\n(i.e., that to be transliterated), and right context, respectively. The \u03c6SP function produces\nthe most relevant source phoneme for each source grapheme. If SW = s1 * s2 * . . . * sn is\nan English word, SW 's pronunciation can be represented as a sequence of source phonemes\nproduced by \u03c6SP ; that is, PSW = p1 * p2 * . . . * pn , where pi = \u03c6SP (si , c(si )). \u03c6SP transforms\nsource graphemes into phonemes in two ways. The first one is to search in a pronunciation\ndictionary containing English words and their pronunciation (CMU, 1997). The second one\nis to estimate the pronunciation (or automatic grapheme-to-phoneme conversion) (Andersen, Kuhn, Lazarides, Dalsgaard, Haas, & Noth, 1996; Daelemans & van den Bosch, 1996;\nPagel, Lenzo, & Black, 1998; Damper, Marchand, Adamson, & Gustafson, 1999; Chen,\n2003). If an English word is not registered in the pronunciation dictionary, we must estimate its pronunciation. The produced pronunciation is used for \u03c6P T in \u03c8P and \u03c6(SP )T in\n\u03c8C . For training the automatic grapheme-to-phoneme conversion in \u03c6SP , we use The CMU\nPronouncing Dictionary (CMU, 1997).\nThe \u03c6ST , \u03c6P T , and \u03c6(SP )T functions produce target graphemes using their input. Like\n\u03c6SP , these three functions use their previous outputs, which are represented by fT . As\n127\n\n\fOh, Choi, & Isahara\n\n\u03c6SP\n\n\u03c6ST\n\n\u03c6P T\n\n\u03c6(SP )T\n\nT ype\nfS\nfStype\nfP\nfS\nfStype\nfT\nfP\nfP type\nfT\nfS\nfP\nfStype\nfP type\nfT\n\nL3\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n\nL2\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n\nL1\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n$\n\nC0\nb\nC\n\nR1\no\nV\n\nR2\na\nV\n\nR3\nr\nC\n\nOutput\n\u2192\n\n/B/\n\na\nV\n\nr\nC\n\n\u2192\n\n'b'\n\n/\u223c/\n/\u223c/\n\n/R/\nC\n\n\u2192\n\n'b'\n\na\n/\u223c/\nV\n/\u223c/\n\nr\n/R/\nC\nC\n\n\u2192\n\n'b'\n\n\u01eb\nb\nC\n\no\nV\n\u01eb\n\n/B/\nC\n\n/AO/\nV\n\u01eb\n\nb\n/B/\nC\nC\n\no\n/AO/\nV\nV\n\u01eb\n\nTable 4: Framework for each component function: $ represents start of words and \u01eb means\nunused contexts for each component function.\n\nshown in Table 4, \u03c6ST , \u03c6P T , and \u03c6(SP )T produce target grapheme 'b' for source grapheme\nb and source phoneme /B/ in board and /B AO R D/. Because the b and /B/ are the\nfirst source grapheme of board and the first source phoneme of /B AO R D/, respectively,\ntheir left context is $, which represents the start of words. Source graphemes (o, a, and r )\nand their type (V: vowel, V: vowel, and C: consonant) can be the right context in \u03c6ST and\n\u03c6(SP )T . Source phonemes (/AO/, /\u223c/, and /R/) and their type (V: vowel, /\u223c/: silence,\nV: vowel) can be the right context in \u03c6P T and \u03c6(SP )T . Depending on the feature type\nused in each component function and described in Table 2, \u03c6ST , \u03c6P T , and \u03c6(SP )T produce\na sequence of target graphemes, TSW = t1 * t2 * . . . * tn , for SW = s1 * s2 * . . . * sn and\nPSW = p1 * p2 * . . . * pn . For board, SW , PSW , and TSW can be represented as follows. The\n/\u223c/ represents silence (null source phonemes), and the '\u223c' represents null target graphemes.\n\u2022 SW = s1 * s2 * s3 * s4 * s5 = b * o * a * r * d\n\u2022 PSW = p1 * p2 * p3 * p4 * p5 = /B/ * /AO/ * / \u223c / * /R/ * /D/\n\u2022 TSW = t1 * t2 * t3 * t4 * t5 = 'b'* 'o' * '\u223c' * '\u223c' * 'deu'\n3.3 Machine Learning Algorithms for Each Component Function\nIn this section we describe a way to model component functions using three machine learning algorithms (the maximum entropy model, decision-tree learning, and memory-based\nlearning)8 . Because the four component functions share a similar framework, we limit our\nfocus to \u03c6(SP )T in this section.\n8. These three algorithms are typically applied to automatic grapheme-to-phoneme conversion (Andersen\net al., 1996; Daelemans & van den Bosch, 1996; Pagel et al., 1998; Damper et al., 1999; Chen, 2003).\n\n128\n\n\fA Comparison of Machine Transliteration Models\n\n3.3.1 Maximum entropy model\nThe maximum entropy model (MEM) is a widely used probability model that can incorporate heterogeneous information effectively (Berger, Pietra, & Pietra, 1996). In the\nMEM, an event (ev) is usually composed of a target event (te) and a history event (he);\nsay ev =< te, he >. Event ev is represented by a bundle of feature functions, f ei (ev),\nwhich represent the existence of certain characteristics in event ev. A feature function is\na binary-valued function. It is activated (f ei (ev) = 1) when it meets its activating condition; otherwise it is deactivated (f ei (ev) = 0) (Berger et al., 1996). Let source language\nword SW be composed of n graphemes. SW, PSW , and TSW can then be represented as\nSW = s1 , * * * , sn , PSW = p1 , * * * , pn , and TSW = t1 , * * * , tn , respectively. PSW and TSW\nrepresent the pronunciation and target language word corresponding to SW, and pi and ti\nrepresent the source phoneme and target grapheme corresponding to si . Function \u03c6(SP )T\nbased on the maximum entropy model can be represented as\nP r(TSW |SW, PSW ) = P r(t1 , * * * , tn |s1 , * * * , sn , p1 , * * * , pn )\n\n(1)\n\nWith the assumption that \u03c6(SP )T depends on the context information in window size k, we\nsimplify Formula (1) to\nP r(TSW |SW, PSW ) \u2248\n\nY\n\nP r(ti |ti\u2212k , * * * , ti\u22121 , pi\u2212k , * * * , pi+k , si\u2212k , * * * , si+k )\n\n(2)\n\ni\n\nBecause t1 , * * * , tn , s1 , * * * , sn , and p1 , * * * , pn can be represented by fT , fS,Stype , and fP,P type ,\nrespectively, we can rewrite Formula (2) as\nP r(TSW |SW, PSW ) \u2248\n\nY\n\nP r(ti |fT(i\u2212k,i\u22121) , fP,P type(i\u2212k,i+k) , fS,Stype(i\u2212k,i+k) )\n\n(3)\n\ni\n\nwhere i is the index of the current source grapheme and source phoneme to be transliterated\nand fX(l,m) represents the features of feature type fX located from position l to position m.\nAn important factor in designing a model based on the maximum entropy model is\nto identify feature functions that effectively support certain decisions of the model. Our\nbasic philosophy of feature function design for each component function is that the context\ninformation collocated with the unit of interest is important. We thus designed the feature\nfunction with collocated features in each feature type and between different feature types.\nFeatures used for \u03c6(SP )T are listed below. These features are used as activating conditions\nor history events of feature functions.\n\u2022 Feature type and features used for designing feature functions in \u03c6(SP )T (k = 3)\n\u2013 All possible features in fS,Stypei\u2212k,i+k , fP,P typei\u2212k,i+k , and fTi\u2212k,i\u22121 (e.g., fSi\u22121 ,\nfPi\u22121 , and fTi\u22121 )\n\u2013 All possible feature combinations between features of the same feature type (e.g.,\n{fSi\u22122 , fSi\u22121 , fSi+1 }, {fPi\u22122 , fPi , fPi+2 }, and {fTi\u22122 , fTi\u22121 })\n\u2013 All possible feature combinations between features of different feature types (e.g.,\n{fSi\u22121 , fPi\u22121 }, {fSi\u22121 , fTi\u22122 } , and {fP typei\u22122 , fPi\u22123 , fTi\u22122 })\n\u2217 between fS,Stypei\u2212k,i+k and fP,P typei\u2212k,i+k\n129\n\n\fOh, Choi, & Isahara\n\nf ej\n\nte\nti\n\nf e1\nf e2\nf e3\nf e4\nf e5\n\n'b'\n'b'\n'b'\n'b'\n'b'\n\nhe\nfT(i\u2212k,i\u22121)\n\nfS,Stype(i\u2212k,i+k)\n\n\u2013\n\u2013\nfTi\u22121 = $\n\u2013\nfTi\u22122 = $\n\nfSi+1\n\nfP,P type(i\u2212k,i+k)\n\nf Si = b\nfSi\u22121 = $\n= o and fStypei+2 = V\n\u2013\nfSi+3 = r\n\nfPi = /B/\n\u2013\nfPi = /B/\nfPi+1 = /AO/\nfP typei = C\n\nTable 5: Feature functions for \u03c6(SP )T derived from Table 4.\n\n\u2217 between fS,Stypei\u2212k,i+k and fTi\u2212k,i\u22121\n\u2217 between fP,P typei\u2212k,i+k and fTi\u2212k,i\u22121\nGenerally, a conditional maximum entropy model that gives the conditional probability\nP r(y|x) is represented as Formula (4) (Berger et al., 1996).\nP r(y|x) =\nZ(x) =\n\nX\n1\nexp( \u03bbi f ei (x, y))\nZ(x)\ni\n\nX\n\nX\n\nexp(\n\ny\n\n(4)\n\n\u03bbi f ei (x, y))\n\ni\n\nIn \u03c6(SP )T , the target event (te) is target graphemes to be assigned, and the history event\n(he) can be represented as a tuple < fT(i\u2212k,i\u22121) , fS,Stype(i\u2212k,i+k) , fP,P type(i\u2212k,i+k) >. Therefore,\nwe can rewrite Formula (3) as\nP r(ti |fT(i\u2212k,i\u22121) , fS,Stype(i\u2212k,i+k) , fP,P type(i\u2212k,i+k) )\nX\n1\nexp( \u03bbi f ei (he, te))\n= P r(te|he) =\nZ(he)\ni\n\n(5)\n\nTable 5 shows example feature functions for \u03c6(SP )T ; Table 4 was used to derive the\nfunctions. For example, f e1 represents an event where he (history event) is \"fSi is b and\nfPi is /B/\" and te (target event) is \"fTi is 'b'\". To model each component function based\non the MEM, Zhang's maximum entropy modeling tool is used (Zhang, 2004).\n3.3.2 Decision-tree learning\nDecision-tree learning (DTL) is one of the most widely used and well-known methods for\ninductive inference (Quinlan, 1986; Mitchell, 1997). ID3, which is a greedy algorithm\nthat constructs decision trees in a top-down manner, uses the information gain, which is a\nmeasure of how well a given feature (or attribute) separates training examples on the basis of\ntheir target class (Quinlan, 1993; Manning & Schutze, 1999). We use C4.5 (Quinlan, 1993),\nwhich is a well-known tool for DTL and an implementation of Quinlan's ID3 algorithm.\nThe training data for each component function is represented by features located in L3L1, C0, and R1-R3, as shown in Table 4. C4.5 tries to construct a decision tree by looking\nfor regularities in the training data (Mitchell, 1997). Figure 2 shows part of the decision\n130\n\n\fA Comparison of Machine Transliteration Models\n\ntree constructed for \u03c6(SP )T in English-to-Korean transliteration. A set of the target classes\nin the decision tree for \u03c6(SP )T is a set of the target graphemes. The rectangles indicate the\nleaf nodes, which represent the target classes, and the circles indicate the decision nodes.\nTo simplify our examples, we use only fS and fP . Note that all feature types for each\ncomponent function, as described in Table 4, are actually used to construct decision trees.\nIntuitively, the most effective feature from among L3-L1, C0, and R1-R3 for \u03c6(SP )T may be\nlocated in C0 because the correct outputs of \u03c6(SP )T strongly depend on the source grapheme\nor source phoneme in the C0 position. As we expected, the most effective feature in the\ndecision tree is located in the C0 position, that is, C0(fP ). (Note that the first feature\nto be tested in decision trees is the most effective feature.) In Figure 2, the decision tree\nproduces the target grapheme (Korean grapheme) 'o' for the instance x(SP T ) by retrieving\nthe decision nodes from C0(fP ) = /AO/ to R1(fP ) = / \u223c / represented by '\u2217'.\n\nC0(f\nC0(fPP): /AO/ (*)\nC0(fSS): e\n\nC0(fSS): a\n\n'o'\n\n'a'\n\nC0(f\nC0(fSS): o(*)\n\nx(SPT)\n\nC0(fSS): others\n......\n'eu'\n\nR1(fPP): /R/\n\nL2(fSS): $\n\nC0(fSS): i\n\nL2(fSS): a\n\nR1(f\nR1(fPP): /~/(*)\n\nR1(fPP): others\n\n''o'\no' (*)\n(*)\n\n'o'\n\n...... L2(fSS): r\n\nFeature type L3 L2\nfS\n\n$\n\n$\n\nfP\n\n$\n\n$\n\nL1\n\nC0\n\nR1\n\nR2\n\nR3\n\nb\n\no\n\na\n\nr\n\nd\n\n\u03c6(SP)T\n\u2192\n\n'o'\n\n/B/ /AO/ /~/ /R/ /D/\n\nFigure 2: Decision tree for \u03c6(SP )T .\n\n3.3.3 Memory-based learning\nMemory-based learning (MBL), also called \"instance-based learning\" and \"case-based learning\", is an example-based learning method. It is based on a k-nearest neighborhood algorithm (Aha, Kibler, & Albert, 1991; Aha, 1997; Cover & Hart, 1967; Devijver & Kittler.,\n1982). MBL represents training data as a vector and, in the training phase, it places all\ntraining data as examples in memory and clusters some examples on the basis of the knearest neighborhood principle. Training data for MBL is represented in the same form\nas training data for a decision tree. Note that the target classes for \u03c6(SP )T , which MBL\noutputs, are target graphemes. Feature weighting to deal with features of differing importance is also done in the training phase9 . It then produces an output using similarity-based\n9. TiMBL (Daelemans, Zavrel, Sloot, & Bosch, 2004) supports gain ratio weighting, information gain\nweighting, chi-squared (\u03c72 ) weighting, and shared variance weighting of the features.\n\n131\n\n\fOh, Choi, & Isahara\n\nreasoning between test data and the examples in memory. If the test data is x and the\nset of examples in memory is Y , the similarity between x and Y can be estimated using\ndistance function \u2206(x, Y )10 . MBL selects an example yi or the cluster of examples that are\nmost similar to x and then assigns the example's target class to x's target class. We use\nan MBL tool called TiMBL (Tilburg memory-based learner) version 5.0 (Daelemans et al.,\n2004).\n\n4. Experiments\nWe tested the four machine transliteration models on English-to-Korean and English-toJapanese transliteration. The test set for the former (EKSet) (Nam, 1997) consisted of\n7,172 English-Korean pairs \u2013 the number of training items was about 6,000 and that of the\nblind test items was about 1,000. EKSet contained no transliteration variations, meaning\nthat there was one transliteration for each English word. The test set for the latter (EJSet)\ncontained English-katakana pairs from EDICT (Breen, 2003) and consisted of 10,417 pairs\n\u2013 the number of training items was about 9,000 and that of the blind test items was about\n1,000. EJSet contained transliteration variations, like <micro, 'ma-i-ku-ro'>, and <micro,\n'mi-ku-ro'>; the average number of Japanese transliterations for an English word was 1.15.\nEKSet and EJSet covered proper names, technical terms, and general terms. We used\nThe CMU Pronouncing Dictionary (CMU, 1997) for training pronunciation estimation (or\nautomatic grapheme-to-phoneme conversion) in \u03c6SP . The training for automatic graphemeto-phoneme conversion was done ignoring the lexical stress of vowels in the dictionary (CMU,\n1997). The evaluation was done in terms of word accuracy (W A), the evaluation measure\nused in previous work (Kang & Choi, 2000; Kang & Kim, 2000; Goto et al., 2003; Bilac &\nTanaka, 2004). Here, W A can be represented as Formula (6). A generated transliteration\nfor an English word was judged to be correct if it exactly matched a transliteration for that\nword in the test data.\nWA =\n\nnumber of correct transliterations output by system\nnumber of transliterations in blind test data\n\n(6)\n\nIn the evaluation, we used k-fold cross-validation (k=7 for EKSet and k=10 for EJSet). The\ntest set was divided into k subsets. Each was used in turn for testing while the remainder was\nused for training. The average W A computed across all k trials was used as the evaluation\nresults presented in this section.\nWe conducted six tests.\n\u2022 Hybrid Model Test: Evaluation of hybrid transliteration model by changing value of \u03b1\n(the parameter of the hybrid transliteration model)\n\u2022 Comparison Test I: Comparison among four machine transliteration models\n\u2022 Comparison Test II: Comparison of four machine transliteration models to previously\nproposed transliteration methods\n10. Modified value difference metric, overlap metric, Jeffrey divergence metric, dot product metric, etc. are\nused as the distance function (Daelemans et al., 2004).\n\n132\n\n\fA Comparison of Machine Transliteration Models\n\n\u2022 Dictionary Test: Evaluation of transliteration models on words registered and not\nregistered in pronunciation dictionary to determine effect of pronunciation dictionary\non each model\n\u2022 Context Window-Size Test: Evaluation of transliteration models for various sizes of\ncontext window\n\u2022 Training Data-Size Test: Evaluation of transliteration models for various sizes of training data sets\n4.1 Hybrid Model Test\nThe objective of this test was to estimate the dependence of the performance of \u03c8H on\nparameter \u03b1. We evaluated the performance by changing \u03b1 from 0 to 1 at intervals of\n0.1 (i.e., \u03b1=0, 0.1, 0.2, * * *, 0.9, 1.0). Note that the hybrid model can be represented as\n\"\u03b1 \u00d7 P r(\u03c8P ) + (1 \u2212 \u03b1) \u00d7 P r(\u03c8G )\". Therefore, \u03c8H is \u03c8G when \u03b1 = 0 and \u03c8P when \u03b1 = 1.\nAs shown in Table 6, the performance of \u03c8H depended on that of \u03c8G and \u03c8P . For example,\nthe performance of \u03c8G exceeded that of \u03c8P for EKSet. Therefore, \u03c8H tended to perform\nbetter when \u03b1 \u2264 0.5 than when \u03b1 > 0.5 for EKSet. The best performance was attained\nwhen \u03b1 = 0.4 for EKSet and when \u03b1 = 0.5 for EJSet. Hereinafter, we use \u03b1 = 0.4 for\nEKSet and \u03b1 = 0.5 for EJSet as the linear interpolation parameter for \u03c8H .\n\u03b1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n\nEKSet\n58.8%\n61.2%\n62.0%\n63.0%\n64.1%\n63.4%\n61.1%\n59.6%\n58.2%\n57.0%\n55.2%\n\nEJSet\n58.8%\n60.9%\n62.6%\n64.1%\n65.4%\n65.8%\n65.0%\n63.4%\n62.1%\n61.2%\n59.2%\n\nTable 6: Results of Hybrid Model Test.\n\n4.2 Comparison Test I\nThe objectives of the first comparison test were to compare performance among the four\ntransliteration models (\u03c8G , \u03c8P , \u03c8H , and \u03c8C ) and to compare the performance of each model\nwith the combined performance of three of the models (\u03c8G+P +C ). Table 7 summarizes the\nperformance of each model for English-to-Korean and English-to-Japanese transliteration,\n133\n\n\fOh, Choi, & Isahara\n\nwhere DTL, MBL11 and MEM represent decision-tree learning, memory-based learning,\nand maximum entropy model.\nThe unit to be transliterated was restricted to either a source grapheme or a source\nphoneme in \u03c8G and \u03c8P ; it was dynamically selected on the basis of the contexts in \u03c8H\nand \u03c8C . This means that \u03c8G and \u03c8P could produce an incorrect result if either a source\nphoneme or a source grapheme, which, respectively, they do not consider, holds the key to\nproducing the correct transliteration result. For this reason, \u03c8H and \u03c8C performed better\nthan both \u03c8G and \u03c8P .\nTransliteration Model\n\u03c8G\n\u03c8P\n\u03c8H\n\u03c8C\n\u03c8G+P +C\n\nDTL\n53.1%\n50.8%\nN/A\n59.5%\n72.0%\n\nEKSet\nMBL\n54.6%\n50.6%\nN/A\n60.3%\n71.4%\n\nMEM\n58.8%\n55.2%\n64.1%\n65.5%\n75.2%\n\nDTL\n55.6%\n55.8%\nN/A\n64.0%\n73.4%\n\nEJSet\nMBL\n58.9%\n56.1%\nN/A\n65.8%\n74.2%\n\nMEM\n58.8%\n59.2%\n65.8%\n69.1%\n76.6%\n\nTable 7: Results of Comparison Test I.\nIn the table, \u03c8G+P +C means the combined results for the three transliteration models,\n\u03c8G , \u03c8P , and \u03c8C . We exclude \u03c8H from the combining because it is implemented only\nwith the MEM (the performance of combining the four transliteration models are discussed\nin Section 5). In evaluating \u03c8G+P +C , we judged the transliteration results to be correct\nif there was at least one correct transliteration among the results produced by the three\nmodels. Though \u03c8C showed the best results among the three transliteration models due to\nits ability to use the correspondence between the source grapheme and source phoneme, the\nsource grapheme or the source phoneme can create noise when the correct transliteration\nis produced by the other one. In other words, when the correct transliteration is strongly\nbiased to either grapheme-based or phoneme-based transliteration, \u03c8G and \u03c8P may be more\nsuitable for producing the correct transliteration.\nTable 8 shows example transliterations produced by each transliteration model. The\n\u03c8G produced correct transliterations for cyclase and bacteroid, while \u03c8P did the same for\ngeoid and silo. \u03c8C produced correct transliterations for saxhorn and bacteroid, and \u03c8H\nproduced correct transliterations for geoid and bacteroid. As shown by these results, there\nare transliterations that only one transliteration model can produce correctly. For example,\nonly \u03c8G , \u03c8P , and \u03c8C produced the correct transliterations of cyclase, silo, and saxhorn,\nrespectively. Therefore, these three transliteration models can be used in a complementary\nmanner to improve transliteration performance because at least one can usually produce the\ncorrect transliteration. This combination increased the performance by compared to \u03c8G ,\n\u03c8P , and \u03c8C (on average, 30.1% in EKSet and 24.6% in EJSet). In short, \u03c8G , \u03c8P , and \u03c8C are\ncomplementary transliteration models that together produce more correct transliterations,\n11. We tested all possible combinations between \u2206(x, Y ) and a weighting scheme supported by\nTiMBL (Daelemans et al., 2004) and did not detect any significant differences in performance for the\nvarious combinations. Therefore, we used the default setting of TiMBL (Overlap metric for \u2206(x, Y ) and\ngain ratio weighting for feature weighting).\n\n134\n\n\fA Comparison of Machine Transliteration Models\n\nso combining different transliteration models can improve transliteration performance. The\ntransliteration results produced by \u03c8G+P +C are analyzed in detail in Section 5.\n\ncyclase\nbacteroid\ngeoid\nsilo\nsaxhorn\ncyclase\nbacteroid\ngeoid\nsilo\nsaxhorn\n\n\u03c8G\n'si-keul-la-a-je'\n'bak-te-lo-i-deu'\n\u2217'je-o-i-deu'\n\u2217'sil-lo'\n\u2217'saek-seon'\n\u03c8H\n\u2217'sa-i-keul-la-a-je'\n'bak-te-lo-i-deu'\n'ji-o-i-deu'\n\u2217'sil-lo'\n\u2217'saek-seon'\n\n\u03c8P\n\u2217'sa-i-keul-la-a-je'\n\u2217'bak-teo-o-i-deu'\n'ji-o-i-deu'\n'sa-il-lo'\n\u2217'saek-seu-ho-leun'\n\u03c8C\n\u2217'sa-i-keul-la-a-je'\n'bak-te-lo-i-deu'\n\u2217'ge-o-i-deu'\n\u2217'sil-lo'\n'saek-seu-hon'\n\nTable 8: Example transliterations produced by each transliteration model (\u2217 indicates an\nincorrect transliteration).\n\nIn our subsequent testing, we used the maximum entropy model as the machine learning\nalgorithm for two reasons. First, it produced the best results of the three algorithms we\ntested12 . Second, it can support \u03c8H .\n4.3 Comparison Test II\nIn this test, we compared four previously proposed machine transliteration methods (Kang\n& Choi, 2000; Kang & Kim, 2000; Goto et al., 2003; Bilac & Tanaka, 2004) to the four\ntransliteration models (\u03c8G , \u03c8P , \u03c8H , and \u03c8C ), which were based on the MEM. Table 9 shows\nthe results. We trained and tested the previous methods with the same data sets used for\nthe four transliteration models. Table 10 shows the key features of the methods and models\nfrom the viewpoint of information type and usage. Information type indicates the type of\ninformation considered: source grapheme, source phoneme, and correspondence between\nthe two. For example, the first three methods use only the source grapheme. Information\nusage indicates the context used and whether the previous output is used.\nIt is obvious from the table that the more information types a transliteration model\nconsiders, the better its performance. Either the source phoneme or the correspondence \u2013\nwhich are not considered in the methods of Kang and Choi (2000), Kang and Kim (2000),\nand Goto et al. (2003) \u2013 is the key to the higher performance of the method of Bilac and\nTanaka (2004) and the \u03c8H and \u03c8C .\nFrom the viewpoint of information usage, the models and methods that consider the\nprevious output tended to achieve better performance. For example, the method of Goto et\nal. (2003) had better results than that of Kang and Choi (2000). Because machine translit12. A one-tail paired t-test showed that the results with the MEM were always significantly better (except\nfor \u03c6G in EJSet) than those of DTL and MBL (level of significance = 0.001).\n\n135\n\n\fOh, Choi, & Isahara\n\nMethod/Model\nKang and Choi (2000)\nKang and Kim (2000)\nPrevious methods\nGoto et al. (2003)\nBilac and Tanaka (2004)\n\u03c8G\n\u03c8P\nMEM-based models\n\u03c8H\n\u03c8C\n\nEKSet\n51.4%\n55.1%\n55.9%\n58.3%\n58.8%\n55.2%\n64.1%\n65.5%\n\nEJSet\n50.3%\n53.2%\n56.2%\n62.5%\n58.8%\n59.2%\n65.8%\n69.1%\n\nTable 9: Results of Comparison Test II.\n\nMethod/Model\nKang and Choi (2000)\nKang and Kim (2000)\nGoto et al. (2003)\nBilac and Tanaka (2004)\n\u03c8G\n\u03c8P\n\u03c8H\n\u03c8C\n\nInfo. type\nS P C\n+ \u2013 \u2013\n+ \u2013 \u2013\n+ \u2013 \u2013\n+ + \u2013\n+ \u2013 \u2013\n\u2013 + \u2013\n+ + \u2013\n+ + +\n\nInfo. usage\nContext\n< \u22123 \u223c +3 >\nUnbounded\n< \u22123 \u223c +3 >\nUnbounded\n< \u22123 \u223c +3 >\n< \u22123 \u223c +3 >\n< \u22123 \u223c +3 >\n< \u22123 \u223c +3 >\n\nPO\n\u2013\n+\n+\n\u2013\n+\n+\n+\n+\n\nTable 10: Information type and usage for previous methods and four transliteration models, where S, P, C, and PO respectively represent the source grapheme, source\nphoneme, correspondence between S and P, and previous output.\n\neration is sensitive to context, a reasonable context size usually enhances transliteration\nability. Note that the size of the context window for the previous methods was limited to 3\nbecause a context window wider than 3 degrades performance (Kang & Choi, 2000) or does\nnot significantly improve it (Kang & Kim, 2000). Experimental results related to context\nwindow size are given in Section 4.5.\nOverall, \u03c8H and \u03c8C had better performance than the previous methods (on average,\n17.04% better for EKSet and 21.78% better for EJSet), \u03c8G (on average, 9.6% better for\nEKSet and 14.4% better for EJSet), and \u03c8P (on average, 16.7% better for EKSet and\n19.0% better for EJSet). In short, a good machine transliteration model should 1) consider\neither the correspondence between the source grapheme and the source phoneme or both\nthe source grapheme and the source phoneme, 2) have a reasonable context size, and 3)\nconsider previous output. The \u03c8H and \u03c8C satisfy all three conditions.\n136\n\n\fA Comparison of Machine Transliteration Models\n\n4.4 Dictionary Test\nTable 11 shows the performance of each transliteration model for the dictionary test. In this\ntest, we evaluated four transliteration models according to a way of pronunciation generation\n(or grapheme-to-phoneme conversion). Registered represents the performance for words\nregistered in the pronunciation dictionary, and Unregistered represents that for unregistered\nwords. On average, the number of Registered words in EKSet was about 600, and that in\nEJSet was about 700 in k-fold cross-validation test data. In other words, Registered words\naccounted for about 60% of the test data in EKSet and about 70% of the test data in\nEJSet. The correct pronunciation can always be acquired from the pronunciation dictionary\nfor Registered words, while the pronunciation must be estimated for Unregistered words\nthrough automatic grapheme-to-phoneme conversion. However, the automatic graphemeto-phoneme conversion does not always produce correct pronunciations \u2013 the estimated rate\nof correct pronunciations was about 70% accuracy.\n\n\u03c8G\n\u03c8P\n\u03c8H\n\u03c8C\nALL\n\nEKSet\nRegistered Unregistered\n60.91%\n55.74%\n66.70%\n38.45%\n70.34%\n53.31%\n73.32%\n54.12%\n80.78%\n68.41%\n\nEJSet\nRegistered Unregistered\n61.18%\n50.24%\n64.35%\n40.78%\n70.20%\n50.02%\n74.04%\n51.39%\n81.17%\n62.31%\n\nTable 11: Results of Dictionary Test: ALL means \u03c8G+P +H+C .\n\nAnalysis of the results showed that the four transliteration models fall into three categories. Since the \u03c8G is free from the need for correct pronunciation, that is, it does not use\nthe source phoneme, its performance is not affected by pronunciation correctness. Therefore,\n\u03c8G can be regarded as the baseline performance for Registered and Unregistered. Because\n\u03c8P (\u03c6P T \u25e6 \u03c6SP ), \u03c8H (\u03b1\u00d7 P r(\u03c8P )+(1 \u2212 \u03b1)\u00d7 P r(\u03c8G )), and \u03c8C (\u03c6(SP )T \u25e6 \u03c6SP ) depend on\nthe source phoneme, their performance tends to be affected by the performance of \u03c6SP .\nTherefore, \u03c8P , \u03c8H , and \u03c8C show notable differences in performance between Registered\nand Unregistered. However, the performance gap differs with the strength of the dependence. \u03c8P falls into the second category: its performance strongly depends on the correct\npronunciation. \u03c8P tends to perform well for Registered and poorly for Unregistered. \u03c8H\nand \u03c8C weakly depend on the correct pronunciation. Unlike \u03c8P , they make use of both\nthe source grapheme and source phoneme. Therefore, they can perform reasonably well\nwithout the correct pronunciation because using the source grapheme weakens the negative\neffect of incorrect pronunciation in machine transliteration.\nComparing \u03c8C and \u03c8P , we find two interesting things. First, \u03c8P was more sensitive to\nerrors in \u03c6SP for Unregistered. Second, \u03c8C showed better results for both Registered and\nUnregistered. Because \u03c8P and \u03c8C share the same function, \u03c6SP , the key factor accounting\nfor the performance gap between them is the component functions, \u03c6P T and \u03c6(SP )T . From\nthe results shown in Table 11, we can infer that \u03c6(SP )T (in \u03c8C ) performed better than\n\u03c6P T (in \u03c8P ) for both Registered and Unregistered. In \u03c6(SP )T , the source grapheme corre137\n\n\fOh, Choi, & Isahara\n\nsponding to the source phonemes, which \u03c6P T does not consider, made two contributions\nto the higher performance of \u03c6(SP )T . First, the source grapheme in the correspondence\nmade it possible to produce more accurate transliterations. Because \u03c6(SP )T considers the\ncorrespondence, \u03c6(SP )T has a more powerful transliteration ability than \u03c6P T , which uses\njust the source phonemes, when the correspondence is needed to produce correct transliterations. This is the main reason \u03c6(SP )T performed better than \u03c6P T for Registered. Second,\nsource graphemes in the correspondence compensated for errors produced by \u03c6SP in producing target graphemes. This is the main reason \u03c6(SP )T performed better than \u03c6P T for\nUnregistered. In the comparison between \u03c8C and \u03c8G , the performances were similar for Unregistered. This indicates that the transliteration power of \u03c8C is similar to that of \u03c8G , even\nthough the pronunciation of the source language word may not be correct. Furthermore, the\nperformance of \u03c8C was significantly higher than that of \u03c8G for Registered. This indicates\nthat the transliteration power of \u03c8C is greater than that of \u03c8G if the correct pronunciation\nis given.\nThe behavior of \u03c8H was similar to that of \u03c8C . For Unregistered, P r(\u03c8G ) in \u03c8H made\nit possible for \u03c8H to avoid errors caused by P r(\u03c8P ). Therefore, it worked better than \u03c8P .\nFor Registered, P r(\u03c8P ) enabled \u03c8H to perform better than \u03c8G .\nThe results of this test showed that \u03c8H and \u03c8C perform better than \u03c8G and \u03c8P while\ncomplementing \u03c8G and \u03c8P (and thus overcoming their disadvantage) by considering either\nthe correspondence between the source grapheme and the source phoneme or both the\nsource grapheme and the source phoneme.\n4.5 Context Window-Size Test\nIn our testing of the effect of the context window size, we varied the size from 1 to 5.\nRegardless of the size, \u03c8H and \u03c8C always performed better than both \u03c8G and \u03c8P . When\nthe size was 4 or 5, each model had difficulty identifying regularities in the training data.\nThus, there were consistent drops in performance for all models when the size was increased\nfrom 3 to 4 or 5. Although the best performance was obtained when the size was 3, as shown\nin Table 12, the differences in performance were not significant in the range of 2-4. However,\nthere was a significant difference between a size of 1 and a size of 2. This indicates that\na lack of contextual information can easily lead to incorrect transliteration. For example,\nto produce the correct target language grapheme of t in -tion, we need the right three\ngraphemes (or at least the right two) of t, -ion (or -io). The results of this testing indicate\nthat the context size should be more than 1 to avoid degraded performance.\n4.6 Training Data-Size Test\nTable 13 shows the results of the Training Data-Size Test using MEM-based machine\ntransliteration models. We evaluated the performance of the four models and ALL while\nvarying the size of the training data from 20% to 100%. Obviously, the more training data\nused, the higher the system performance. However, the objective of this test was to determine whether the transliteration models perform reasonably well even for a small amount\nof training data. We found that \u03c8G was the most sensitive of the four models to the amount\nof training data; it had the largest difference in performance between 20% and 100%. In\ncontrast, ALL showed the smallest performance gap. The results of this test shows that\n138\n\n\fA Comparison of Machine Transliteration Models\n\nContext Size\n1\n2\n3\n4\n5\n\n\u03c8G\n44.9%\n57.3%\n58.8%\n56.1%\n53.7%\n\nContext Size\n1\n2\n3\n4\n5\n\n\u03c8G\n46.4%\n58.2%\n58.8%\n56.4%\n53.9%\n\nEKSet\n\u03c8P\n44.9%\n52.8%\n55.2%\n54.6%\n52.6%\nEJSet\n\u03c8P\n52.1%\n59.5%\n59.2%\n58.5%\n56.4%\n\n\u03c8H\n51.8%\n61.7%\n64.1%\n61.8%\n60.4%\n\n\u03c8C\n52.4%\n64.4%\n65.5%\n64.3%\n62.5%\n\nALL\n65.8%\n74.4%\n75.8%\n74.4%\n73.9%\n\n\u03c8H\n58.0%\n65.6%\n65.8%\n64.4%\n62.9%\n\n\u03c8C\n62.0%\n68.7%\n69.1%\n68.2%\n66.3%\n\nALL\n70.4%\n76.3%\n77.0%\n76.0%\n75.5%\n\nTable 12: Results of Context Window-Size Test: ALL means \u03c8G+P +H+C .\n\ncombining different transliteration models is helpful in producing correct transliterations\neven if there is little training data.\n\nTraining Data Size\n20%\n40%\n60%\n80%\n100%\nTraining Data Size\n20%\n40%\n60%\n80%\n100%\n\nEKSet\n\u03c8G\n\u03c8P\n46.6% 47.3%\n52.6% 51.5%\n55.2% 53.0%\n58.9% 54.0%\n58.8% 55.2%\nEJSet\n\u03c8G\n\u03c8P\n47.6% 51.2%\n52.4% 55.1%\n55.2% 57.3%\n57.9% 58.8%\n58.8% 59.2%\n\n\u03c8H\n53.4%\n58.7%\n61.5%\n62.6%\n64.1%\n\n\u03c8C\n57.0%\n62.1%\n63.3%\n64.6%\n65.5%\n\nALL\n67.5%\n71.6%\n73.0%\n74.7%\n75.8%\n\n\u03c8H\n56.4%\n60.7%\n62.9%\n65.4%\n65.8%\n\n\u03c8C\n60.4%\n64.8%\n66.6%\n68.0%\n69.1%\n\nALL\n69.6%\n72.6%\n74.7%\n76.7%\n77.0%\n\nTable 13: Results of Training Data-Size Test: ALL means \u03c8G+P +H+C .\n\n5. Discussion\nFigures 3 and 4 show the distribution of the correct transliterations produced by each\ntransliteration model and by the combination of models, all based on the MEM. The \u03c8G ,\n139\n\n\fOh, Choi, & Isahara\n\n\u03c8P , \u03c8H , and \u03c8C in the figures represent the set of correct transliterations produced by each\nmodel through k-fold validation. For example, |\u03c8G | = 4,220 for EKSet and |\u03c8G | = 6,121\nfor EJSet mean that \u03c8G produced 4,220 correct transliterations for 7,172 English words\nin EKSet (|KT G| in Figure 3) and 6,121 correct ones for 10,417 English words in EJSet\n(|JT G| in Figure 4). An important factor in modeling a transliteration model is to reflect the\ndynamic transliteration behaviors, which means that a transliteration process dynamically\nuses the source grapheme and source phoneme to produce transliterations. Due to these\ndynamic behaviors, a transliteration can be grapheme-based transliteration, phoneme-based\ntransliteration, or some combination of the two. The forms of transliterations are classified\non the basis of the information upon which the transliteration process mainly relies (either\na source grapheme or a source phoneme or some combination of the two). Therefore, an\neffective transliteration system should be able to produce various types of transliterations\nat the same time. One way to accommodate the different dynamic transliteration behaviors\nis to combine different transliteration models, each of which can handle a different behavior.\nSynergy can be achieved by combining models so that one model can produce the correct\ntransliteration when the others cannot. Naturally, if the models tend to produce the same\ntransliteration, less synergy can be realized from combining them. Figures 3 and 4 show the\nsynergy gained from combining transliteration models in terms of the size of the intersection\nand the union of the transliteration models.\n\u03c8G\n407\n\n\u03c8P\n82\n\n680 3,051\n344\n\n207\n624\n\n\u03c8C\n\n|KTG-(\u03c8G \u222a \u03c8P \u222a \u03c8C )|\n=1,777\n\n(a) \u03c8G +\u03c8P +\u03c8C\n\n\u03c8G\n188\n\n7\n\n899 3,126\n119\n\n\u03c8H\n\n\u03c8P\n\n305\n\n374\n\n\u03c8P\n267 129\n\n713 3,423\n457\n311\n\n\u03c8H\n\n|KTG-(\u03c8G \u222a \u03c8P \u222a \u03c8H )|\n=2,002\n\n(b) \u03c8G +\u03c8P +\u03c8H\n\n252\n\n\u03c8C\n\n|KTG-(\u03c8H \u222a \u03c8P \u222a \u03c8C )|\n=1,879\n\n(c) \u03c8P +\u03c8H +\u03c8C\n\n\u03c8G\n393\n\n\u03c8H\n340 369\n\n46 3,685\n763\n\n451\n\n\u03c8C\n\n|KTG-(\u03c8G \u222a\u03c8H \u222a\u03c8C )|\n=1,859\n\n(d) \u03c8G +\u03c8H +\u03c8C\n\nFigure 3: Distributions of correct transliterations produced by models for English-toKorean transliteration. KTG represents \"Korean Transliterations in the Gold\nstandard\". Note that |\u03c8G \u222a \u03c8P \u222a \u03c8H \u222a \u03c8C | = 5,439, |\u03c8G \u2229 \u03c8P \u2229 \u03c8H \u2229 \u03c8C | =\n3,047, and |KT G| = 7,172.\n\nThe figures show that, as the area of intersection between different transliteration models\nbecomes smaller, the size of their union tends to become bigger. The main characteristics\nobtained from these figures are summarized in Table 14. The first thing to note is that\n|\u03c8G \u2229 \u03c8P | is clearly smaller than any other intersection. The main reason for this is that\n\u03c8G and \u03c8P use no common information (\u03c8G uses source graphemes while \u03c8P uses source\nphonemes). However, the others use at least one of source grapheme and source phoneme\n(source graphemes are information common to \u03c8G , \u03c8H , and \u03c8C while source phonemes\nare information common to \u03c8P , \u03c8H , and \u03c8C ). Therefore, we can infer that the synergy\nderived from combining \u03c8G and \u03c8P is greater than that derived from the other combinations.\n140\n\n\fA Comparison of Machine Transliteration Models\n\n\u03c8G\n379\n\n\u03c8P\n141\n\n805 4,796\n628\n\n261\n963\n\n\u03c8C\n\n\u03c8G\n378\n\n\u03c8H\n\n\u03c8P\n12\n\n806 4,925\n202\n\n222\n\n308\n\n\u03c8P\n267 135\n\n786 5,574\n916\n647\n\n\u03c8H\n\n185\n\n\u03c8C\n\n\u03c8G\n207\n\n\u03c8H\n313 176\n\n183 5,418\n649\n\n942\n\n\u03c8C\n\n|JTG-(\u03c8G \u222a \u03c8P \u222a \u03c8C )|\n=2,444\n\n|JTG-(\u03c8G \u222a \u03c8P \u222a \u03c8H )|\n=2,870\n\n|JTG-(\u03c8H \u222a \u03c8P \u222a \u03c8C )|\n=2,601\n\n|JTG-(\u03c8G \u222a\u03c8H \u222a\u03c8C )|\n=2,529\n\n(a) \u03c8G +\u03c8P +\u03c8C\n\n(b) \u03c8G +\u03c8P +\u03c8H\n\n(c) \u03c8P +\u03c8H +\u03c8C\n\n(d) \u03c8G +\u03c8H +\u03c8C\n\nFigure 4: Distributions of correct transliterations produced by models for English-toJapanese transliteration. JTG represents \"Japanese Transliterations in the Gold\nstandard\". Note that |\u03c8G \u222a \u03c8P \u222a \u03c8H \u222a \u03c8C |=8,021, |\u03c8G \u2229 \u03c8P \u2229 \u03c8H \u2229 \u03c8C |=4,786,\nand |JT G| = 10,417.\n\n|\u03c8G |\n|\u03c8P |\n|\u03c8H |\n|\u03c8C |\n|\u03c8G \u2229 \u03c8P |\n|\u03c8G \u2229 \u03c8C |\n|\u03c8G \u2229 \u03c8H |\n|\u03c8C \u2229 \u03c8H |\n|\u03c8P \u2229 \u03c8C |\n|\u03c8P \u2229 \u03c8H |\n|\u03c8G \u222a \u03c8P |\n|\u03c8G \u222a \u03c8C |\n|\u03c8G \u222a \u03c8H |\n|\u03c8C \u222a \u03c8H |\n|\u03c8P \u222a \u03c8C |\n|\u03c8P \u222a \u03c8H |\n\nEKSet\n4,202\n3,947\n4,583\n4,680\n3,133\n3,731\n4,025\n4,136\n3,675\n3,583\n5,051\n5,188\n4,796\n5,164\n4,988\n4,982\n\nEJSet\n6,118\n6,158\n6,846\n7,189\n4,937\n5,601\n5,731\n6,360\n5,759\n5,841\n7,345\n7,712\n7,239\n7,681\n7,594\n7,169\n\nTable 14: Main characteristics obtained from Figures 3 and 4.\n\nHowever, the size of the union between the various pairs of transliteration models in Table 14\nshows that |\u03c8C \u222a \u03c8H | and |\u03c8G \u222a \u03c8C | are bigger than |\u03c8G \u222a \u03c8P |. The main reason for this\nmight be the higher transliteration power of \u03c8C and \u03c8H compared to that of \u03c8G and \u03c8P\n\u2013 \u03c8C and \u03c8H cover more of the KTG and JTG than both \u03c8G and \u03c8P . The second thing\nto note is that the contribution of each transliteration model to |\u03c8G \u222a \u03c8P \u222a \u03c8H \u222a \u03c8C | can\nbe estimated from the difference between |\u03c8G \u222a \u03c8P \u222a \u03c8H \u222a \u03c8C | and the union of the three\nother transliteration models. For example, we can measure the contribution of \u03c8H from the\n141\n\n\fOh, Choi, & Isahara\n\ndifference between |\u03c8G \u222a \u03c8P \u222a \u03c8H \u222a \u03c8C | and |\u03c8G \u222a \u03c8P \u222a \u03c8C |. As shown in Figures 3(a)\nand 4(a)), \u03c8H makes the smallest contribution while \u03c8C (Figures 3(b) and 4(b)) makes the\nlargest contribution. The main reason for \u03c8H making the smallest contribution is that it\ntends to produce the same transliteration as the others, so the intersection between \u03c8H and\nthe others tends to be large.\nIt is also important to rank the transliterations produced by a transliteration system for\na source language word on the basis of their relevance. While a transliteration system can\nproduce a list of transliterations, each reflecting a dynamic transliteration behavior, it will\nfail to perform well unless it can distinguish between correct and wrong transliterations.\nTherefore, a transliteration system should be able to produce various kinds of transliterations depending on the dynamic transliteration behaviors and be able to rank them on the\nbasis of their relevance. In addition, the application of transliteration results to natural\nlanguage applications such as machine translation and information retrieval requires that\nthe transliterations be ranked and assigned a relevance score.\nIn summary, 1) producing a list of transliterations reflecting dynamic transliteration behaviors (one way is to combine the results of different transliteration models,\neach reflecting one of the dynamic transliteration behaviors) and 2) ranking the transliterations in terms of their relevance are both necessary to improve the performance of\nmachine transliteration. In the next section, we describe a way to calculate the relevance\nof transliterations produced by a combination of the four transliteration models.\n\n6. Transliteration Ranking\nThe basic assumption of transliteration ranking is that correct transliterations are more\nfrequently used in real-world texts than incorrect ones. Web data reflecting the real-world\nusage of transliterations can thus be used as a language resource to rank transliterations.\nTransliterations that appear more frequently in web documents are given either a higher\nrank or a higher score. The goal of transliteration ranking, therefore, is to rank correct\ntransliterations higher and rank incorrect ones lower. The transliterations produced for a\ngiven English word by the four transliteration models (\u03c8G , \u03c8P , \u03c8H , and \u03c8C ), based on the\nMEM, were ranked using web data.\nOur transliteration ranking relies on web frequency (number of web documents). To\nobtain reliable web frequencies, it is important to consider a transliteration and its corresponding source language word together rather than the transliteration alone. This is\nbecause our aim is to find correct transliterations corresponding to a source language word\nrather than to find transliterations that are frequently used in the target language. Therefore, the best approach to transliteration ranking using web data is to find web documents\nin which transliterations are used as translations of the source language word.\nA bilingual phrasal search (BPS) retrieves the Web with a Web search engine query,\nwhich is a phrase composed of a transliteration and its source language word (e.g., {'a-milla-a-je' amylase}). The BPS enables the Web search engine to find web documents that\ncontain correct transliterations corresponding to the source language word. Note that a\nphrasal query is represented in brackets, where the first part is a transliteration and the\nsecond part is the corresponding source language word. Figure 5 shows Korean and Japanese\nweb documents retrieved using a BPS for amylase and its Korean/Japanese transliterations,\n142\n\n\fA Comparison of Machine Transliteration Models\n\nRetrieved\nRetrievedKorean\nKoreanweb\nwebpages\npagesfor\nforquery\nquery\n{'a-mil-la-a-je'\namylase}\n{'a-mil-la-a-je' amylase}\n\nRetrieved\nRetrievedJapanese\nJapaneseweb\nwebpages\npagesfor\nforquery\nquery\n{'a-mi-ra-a-je'\namylase}\n{'a-mi-ra-a-je' amylase}\n\nQuery\n\n\u30a2\u30df\u30e9\u30fc\u30bc\namylase\n\u30a2\u30df\u30e9\u30fc\u30bcamylase\n(\namylase)\n)\n\u30a2\u30df\u30e9\u30fc\u30bc\namylase\n\u30a2\u30df\u30e9\u30fc\u30bc (amylase)\n[\namylase]\n\u30a2\u30df\u30e9\u30fc\u30bc\namylase]\n\u30a2\u30df\u30e9\u30fc\u30bc [amylase]\n'a-mi-ra-a-je'\n'a-mi-ra-a-je'amylase\namylase\namylase)\n'a-mi-ra-a-je'\namylase)\n'a-mi-ra-a-je'((amylase)\namylase]\n'a-mi-ra-a-je'\namylase]\n'a-mi-ra-a-je'[[amylase]\n\n\uc544\ubc00\ub77c\uc544\uc81c\namylase\n\uc544\ubc00\ub77c\uc544\uc81camylase\n(\namylase)\n)\n\uc544\ubc00\ub77c\uc544\uc81c\namylase\n\uc544\ubc00\ub77c\uc544\uc81c (amylase)\n[\namylase]\n\uc544\ubc00\ub77c\uc544\uc81c\namylase]\n\uc544\ubc00\ub77c\uc544\uc81c [amylase]\n'a-mil-la-a-je'\n'a-mil-la-a-je'amylase\namylase\namylase)\n'a-mil-la-a-je'\namylase)\n'a-mil-la-a-je'((amylase)\namylase]\n'a-mil-la-a-je'\namylase]\n'a-mil-la-a-je'[[amylase]\n\nFigure 5: Desirable retrieved web pages for transliteration ranking.\n\n'a-mil-la-a-je' and 'a-mi-ra-a-je'. The web documents retrieved by a BPS usually contain a\ntransliteration and its corresponding source language word as a translation pair, with one\nof them often placed in parentheses, as shown in Figure 5.\nA dilemma arises, though, regarding the quality and coverage of retrieved web documents. Though a BPS generally provides high-quality web documents that contain correct\ntransliterations corresponding to the source language word, the coverage is relatively low,\nmeaning that it may not find any web documents for some transliterations. For example, a BPS for the Japanese phrasal query {'a-ru-ka-ro-si-su' alkalosis} and the Korean\nphrasal query {'eo-min' ermine} found no web documents. Therefore, alternative search\nmethods are necessary when the BPS fails to find any relevant web documents. A bilingual\nkeyword search (BKS) (Qu & Grefenstette, 2004; Huang, Zhang, & Vogel, 2005; Zhang,\nHuang, & Vogel, 2005) can be used when the BPS fails, and a monolingual keyword search\n(MKS) (Grefenstette, Qu, & Evans, 2004) can be used when both the BPS and BKS fail.\nLike a BPS, a BKS makes use of two keywords, a transliteration and its source language\nword, as a search engine query. Whereas a BPS retrieves web documents containing the\ntwo keywords as a phrase, a BKS retrieves web documents containing them anywhere in\nthe document. This means that the web frequencies of noisy transliterations are sometimes\nhigher than those of correct transliterations in a BKS, especially when the noisy transliterations are one-syllable transliterations. For example, 'mok', which is a Korean transliteration\nproduced for mook and a one-syllable noisy transliteration, has a higher web frequency than\n'mu-keu', which is the correct transliteration for mook, because 'mok' is a common Korean\n143\n\n\fOh, Choi, & Isahara\n\nnoun that frequently appears in Korean texts with the meaning of neck. However, a BKS\ncan improve coverage without a great loss of quality in the retrieved web documents if the\ntransliterations are composed of two or more syllables.\nThough a BKS has higher coverage than a BPS, it can fail to retrieve web documents\nin some cases. In such cases, an MKS (Grefenstette et al., 2004) is used. In an MKS,\na transliteration alone is used as the search engine query. A BPS and a BKS act like a\ntranslation model, while an MKS acts like a language model. Though an MKS cannot give\ninformation as to whether the transliteration is correct, it does provide information as to\nwhether the transliteration is likely to be a target language word. The three search methods\nare used sequentially (BPS, BKS, MKS). If one method fails to retrieve any relevant web\ndocuments, the next one is used. Table 15 summarizes the conditions for applying each\nsearch method.\nAlong with these three search strategies, three different search engines are used to obtain\nmore web documents. The search engines used for this purpose should satisfy two conditions: 1) support Korean/Japanese web document retrieval and 2) support both phrasal\nand keyword searches. Google13 , Yahoo14 , and MSN15 satisfy these conditions, and we used\nthem as our search engines.\nSearch method\nBPS\nBKS\nMKS\n\nCondition\n\nP P\nW FBP Sj (e, ck )) 6= 0\nPj Pck \u2208C\nW FBP Sj (e, ck )) = 0\nP j P ck \u2208C\nW\nFBKSj (e, ck )) 6= 0\nPj Pck \u2208C\nW FBP Sj (e, ck ) = 0\nP j P ck \u2208C\nW\nFBKSj (e, ck ) = 0\nP j P ck \u2208C\nj\n\nck \u2208C\n\nW FM KSj (e, ck ) 6= 0\n\nTable 15: Conditions under which each search method is applied.\n\nRF (e, ci ) =\n\nX\n\nN W Fj (e, ci ) =\n\nX\nj\n\nj\n\nW Fj (e, ci )\nck \u2208C W Fj (e, ck )\n\nP\n\n(7)\n\nWeb frequencies acquired from these three search methods and these three search engines were used to rank transliterations on the basis of Formula (7), where ci is the ith\ntransliteration produced by the four transliteration models, e is the source language word\nof ci , RF is a function for ranking transliterations, W F is a function for calculating web\nfrequency, N W F is a function for normalizing web frequency, C is a set of produced transliterations, and j is an index for the j th search engine. We used the normalized web frequency\nas a ranking factor. The normalized web frequency is the web frequency divided by the\ntotal web frequency of all produced transliterations corresponding to one source language\nword. The score for a transliteration is then calculated by summing up the normalized\n13. http://www.google.com\n14. http://www.yahoo.com\n15. http://www.msn.com\n\n144\n\n\fA Comparison of Machine Transliteration Models\n\nweb frequencies of the transliteration given by the three search engines. Table 16 shows an\nexample ranking for the English word data and its possible Korean transliterations, 'de-iteo', 'de-i-ta', and 'de-ta', which web frequencies are obtained using a BPS. The normalized\nW FBP S (N W FBP S ) for search engine A was calculated as follows.\n\u2022 N W FBP S (data, 'de-i-teo') = 94,100 / (94,100 + 67,800 + 54) = 0.5811\n\u2022 N W FBP S (data, 'de-i-ta') = 67,800 / (94,100 + 67,800 + 54) = 0.4186\n\u2022 N W FBP S (data, 'de-ta') = 54 / (94,100 + 67,800 + 54) = 0.0003\nThe ranking score for 'de-i-teo' was then calculated by summing up N W FBP S (data, 'de-iteo') for each search engine:\n\u2022 RFBP S (data, 'de-i-teo') = 0.5810 + 0.7957 + 0.3080 = 1.6848\n\nSearch Engine\nA\nB\nC\nRF\n\nc1 = 'de-i-teo'\nWF\nNW F\n94,100 0.5811\n101,834 0.7957\n1,358\n0.3080\n1.6848\n\ne=data\nc2 = 'de-i-ta'\nWF\nNW F\n67,800 0.4186\n26,132 0.2042\n3,028 0.6868\n1.3096\n\nc3 = 'de-ta'\nW F NW F\n54\n0.0003\n11\n0.0001\n23\n0.0052\n0.0056\n\nTable 16: Example transliteration ranking for data and its transliterations; W F , N W F ,\nand RF represent W FBP S , N W FBP S , and RFBP S , respectively.\n\n6.1 Evaluation\nWe tested the performance of the transliteration ranking under two conditions: 1) with all\ntest data (ALL) and 2) with test data for which at least one transliteration model produced\nthe correct transliteration (CTC). Testing with ALL showed the overall performance of the\nmachine transliteration while testing with CTC showed the performance of the transliteration ranking alone. We used the performance of the individual transliteration models\n(\u03c8G , \u03c8P , \u03c8H , and \u03c8C ) as the baseline. The results are shown in Table 17. \"Top-n\" means\nthat the correct transliteration was within the Top-n ranked transliterations. The average\nnumber of produced Korean transliterations was 3.87 and that of Japanese ones was 4.50;\nnote that \u03c8P and \u03c8C produced more than one transliteration because of pronunciation\nvariations. The results for both English-to-Korean and English-to-Japanese transliteration\nindicate that our ranking method effectively filters out noisy transliterations and positions\nthe correct transliterations in the top rank; most of the correct transliterations were in\nTop-1. We see that transliteration ranking (in Top-1) significantly improved performance\nof the individual models for both EKSet and EJSet16 . The overall performance of the\n16. A one-tail paired t-test showed that the performance improvement was significant (level of significance\n= 0.001.\n\n145\n\n\fOh, Choi, & Isahara\n\ntransliteration (for ALL) as well that of the ranking (for CTC) were relatively good. Notably, the CTC performance showed that web data is a useful language resource for ranking\ntransliterations.\nTest data\nALL\n\nALL\n\nCTC\n\n\u03c8G\n\u03c8P\n\u03c8H\n\u03c8C\nTop-1\nTop-2\nTop-3\nTop-1\nTop-2\nTop-3\n\nEKSet\n58.8%\n55.2%\n64.1%\n65.5%\n71.5%\n75.3%\n75.8%\n94.3%\n99.2%\n100%\n\nEJSet\n58.8%\n59.2%\n65.8%\n69.1%\n74.8%\n76.9%\n77.0%\n97.2%\n99.9%\n100%\n\nTable 17: Results of Transliteration ranking.\n\n6.2 Analysis of Results\nWe defined two error types: production errors and ranking errors. A production error\nis when there is no correct transliteration among the produced transliterations. A ranking\nerror is when the correct transliteration does not appear in the Top-1 ranked transliterations.\nWe examined the relationship between the search method and the transliteration ranking. Table 18 shows the ranking performance by each search method. The RTC represents\ncorrect transliterations ranked by each search method. The NTC represents test data\nranked, that is, the coverage of each search method. The ratio of RTC to NTC represents\nthe upper bound of performance and the difference between RTC and NTC is the number\nof errors.\nThe best performance was with a BPS. A BPS handled 5,270 out of 7,172 cases for\nEKSet and 8,829 out of 10,417 cases for EJSet. That is, it did the best job of retrieving\nweb documents containing transliteration pairs. Analysis of the ranking errors revealed\nthat the main cause of such errors in a BPS was transliteration variations. These variations\ncontribute to ranking errors in two ways. First, when the web frequencies of transliteration\nvariations are higher than those of the standard ones, the variations are ranked higher than\nthe standard ones, as shown by the examples in Table 19. Second, when the transliterations\ninclude only transliteration variations (i.e., there are no correct transliterations), the correct\nranking cannot be. In this case, ranking errors are caused by production errors. With a\nBPS, there were 603 cases of this for EKSet and 895 cases for EJSet.\nNTC was smaller with a BKS and an MKS because a BPS retrieves web documents\nwhenever possible. Table 18 shows that production errors are the main reason a BPS fails\nto retrieve web documents. (When a BKS or MKS was used, production errors occurred in\n146\n\n\fA Comparison of Machine Transliteration Models\n\nTop-1\nTop-2\nTop-3\nRTC\nNTC\n\nBPS\n83.8%\n86.6%\n86.6%\n4,568\n5,270\n\nEKSet\nBKS\nMKS\n55.1% 16.7%\n58.4% 27.0%\n58.2% 31.3%\n596\n275\n1,024\n878\n\nBPS\n86.2%\n88.3%\n88.35%\n7,800\n8,829\n\nEJSet\nBKS\n19.0%\n22.8%\n22.9%\n188\n820\n\nMKS\n2.7%\n4.2%\n4.3%\n33\n768\n\nTable 18: Ranking performance of each search method.\n\ncompact \u2192 Korean\npathos \u2192 Korean\ncohen \u2192 Japanese\ncriteria \u2192 Japanese\n\nTransliteration\n'kom-paek-teu'\n'keom-paek-teu'\u2217\n'pa-to-seu'\n'pae-to-seu'\u2217\n'ko-o-he-n'\n'ko-o-e-n'\u2217\n'ku-ra-i-te-ri-a'\n'ku-ri-te-ri-a'\u2217\n\nWeb Frequency\n1,075\n1,793\n1,615\n14,062\n23\n112\n104\n1,050\n\nTable 19: Example ranking errors when a BPS was used (\u2217 indicates a variation).\n\nall but 87117 cases for EKSet and 22118 cases for EJSet). The results also show that a BKS\nwas more effective than an MKS.\nThe trade-off between the quality and coverage of retrieved web documents is an important factor in transliteration ranking. A BPS provides better quality rather than wider\ncoverage, but is effective since it provides reasonable coverage.\n\n7. Conclusion\nWe tested and compared four transliteration models, grapheme-based transliteration\nmodel (\u03c8G ), phoneme-based transliteration model (\u03c8P ), hybrid transliteration\nmodel (\u03c8H ), and correspondence-based transliteration model (\u03c8C ), for English-toKorean and English-to-Japanese transliteration. We modeled a framework for the four\ntransliteration models and compared them within the framework. Using the results, we\nexamined a way to improve the performance of machine transliteration.\nWe found that the \u03c8H and \u03c8C are more effective than the \u03c8G and \u03c8P . The main reason\nfor the better performance of \u03c8C is that it uses the correspondence between the source\ngrapheme and the source phoneme. The use of this correspondence positively affected\ntransliteration performance in various tests.\n17. 596 (RTC of BKS in EKSet) + 275 (RTC of MKS in EKSet) = 871\n18. 188 (RTC of BKS for EJSet) + 33 (RTC of MKS for EJSet) = 221\n\n147\n\n\fOh, Choi, & Isahara\n\nWe demonstrated that \u03c8G , \u03c8P , \u03c8H , and \u03c8C can be used as complementary transliteration models to improve the chances of producing correct transliterations. A combination of\nthe four models produced more correct transliterations both in English-to-Korean transliteration and English-to-Japanese transliteration compared to each model alone. Given these\nresults, we described a way to improve machine transliteration that combines different\ntransliteration models: 1) produce a list of transliterations by combining transliterations produced by multiple transliteration models; 2) rank the transliterations\non the basis of their relevance.\nTesting showed that transliteration ranking based on web frequency is an effective way\nto calculate the relevance of transliterations. This is because web data reflects real-world\nusage, so it can be used to filter out noisy transliterations, which are not used as target\nlanguage words or are incorrect transliterations for a source language word.\nThere are several directions for future work. Although we considered some transliteration variations, our test sets mainly covered standard transliterations. In corpora or\nweb pages, however, we routinely find other types of transliterations \u2013 misspelled transliterations, transliterations of common phrases, etc. \u2013 along with the standard transliterations and transliteration variations. Therefore, further testing using such transliterations\nis needed to enable the transliteration models to be compared more precisely. To achieve\na machine transliteration system capable of higher performance, we need a more sophisticated transliteration method and a more sophisticated ranking algorithm. Though many\ncorrect transliterations can be acquired through the combination of the four transliteration\nmodels, there are still some transliterations that none of the models can produce. We need\nto devise a method that can produce them. Our transliteration ranking method works well,\nbut, because it depends on web data, it faces limitations if the correct transliteration does\nnot appear in web data. We need a complementary ranking method to handle such cases.\nMoreover, to demonstrate the effectiveness of these four transliteration models, we need to\napply them to various natural language processing applications.\n\nAcknowledgments\nWe are grateful to Claire Cardie and the anonymous reviewers for providing constructive\nand insightful comments to earlier drafts of this paper.\n\nReferences\nAha, D. W. (1997). Lazy learning: Special issue editorial. Artificial Intelligence Review,\n11:710.\nAha, D. W., Kibler, D., & Albert, M. (1991). Instance-based learning algorithms. Machine\nLearning, 6 (3766).\nAl-Onaizan, Y., & Knight, K. (2002). Translating named entities using monolingual and\nbilingual resources. In Proceedings of ACL 2002, pp. 400\u2013408.\nAndersen, O., Kuhn, R., Lazarides, A., Dalsgaard, P., Haas, J., & Noth, E. (1996). Comparison of two tree-structured approaches for grapheme-to-phoneme conversion. In\nProceedings of ICSLP 1996, pp. 1808\u20131811.\n148\n\n\fA Comparison of Machine Transliteration Models\n\nBerger, A. L., Pietra, S. D., & Pietra, V. J. D. (1996). A maximum entropy approach to\nnatural language processing. Computational Linguistics, 22 (1), 39\u201371.\nBilac, S., & Tanaka, H. (2004). Improving back-transliteration by combining information\nsources. In Proceedings of IJCNLP2004, pp. 542\u2013547.\nBreen, J. (2003). EDICT Japanese/English dictionary .le. The Electronic Dictionary Research and Development Group, Monash University. http://www.csse.monash.edu.\nau/~jwb/edict.html.\nChen, S. F. (2003). Conditional and joint models for grapheme-to-phoneme conversion. In\nProceedings of Eurospeech, pp. 2033\u20132036.\nCMU (1997). The CMU pronouncing dictionary version 0.6. http://www.speech.cs.cmu.\nedu/cgi-bin/cmudict.\nCover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classification. Institute of\nElectrical and Electronics Engineers Transactions on Information Theory, 13 (2127).\nDaelemans, W., Zavrel, J., Sloot, K. V. D., & Bosch, A. V. D. (2004). TiMBL: Tilburg\nMemory-Based Learner - version 5.1 reference guide. Tech. rep. 04-02, ILK Technical\nReport Series.\nDaelemans, W., & van den Bosch, A. (1996). Language-independent data-oriented\ngrapheme-to-phoneme conversion. In J. Van Santen, R. Sproat, J. O., & Hirschberg,\nJ. (Eds.), Progress in Speech Synthesis, pp. 77\u201390. Springer Verlag, New York.\nDamper, R. I., Marchand, Y., Adamson, M. J., & Gustafson, K. (1999). Evaluating the\npronunciation component of text-to-speech systems for English: A performance comparison of different approaches. Computer Speech and Language, 13 (2), 155\u2013176.\nDevijver, P. A., & Kittler., J. (1982). Pattern recognition: A statistical approach. PrenticeHall.\nFujii, A., & Tetsuya, I. (2001). Japanese/English cross-language information retrieval: Exploration of query translation and transliteration. Computers and the Humanities,\n35 (4), 389\u2013420.\nGoto, I., Kato, N., Uratani, N., & Ehara, T. (2003). Transliteration considering context\ninformation based on the maximum entropy method. In Proceedings of MT-Summit\nIX, pp. 125\u2013132.\nGrefenstette, G., Qu, Y., & Evans, D. A. (2004). Mining the web to create a language\nmodel for mapping between English names and phrases and Japanese. In Proceedings\nof Web Intelligence, pp. 110\u2013116.\nHuang, F., Zhang, Y., & Vogel, S. (2005). Mining key phrase translations from web corpora. In Proceedings of Human Language Technology Conference and Conference on\nEmpirical Methods in Natural Language Processing, pp. 483\u2013490.\nJeong, K. S., Myaeng, S. H., Lee, J. S., & Choi, K. S. (1999). Automatic identification and\nback-transliteration of foreign words for information retrieval. Information Processing\nand Management, 35 (1), 523\u2013540.\n149\n\n\fOh, Choi, & Isahara\n\nJung, S. Y., Hong, S., & Paek, E. (2000). An English to Korean transliteration model of\nextended markov window. In Proceedings of the 18th conference on Computational\nlinguistics, pp. 383 \u2013 389.\nKang, B. J. (2001). A resolution of word mismatch problem caused by foreign word transliterations and English words in Korean information retrieval. Ph.D. thesis, Computer\nScience Dept., KAIST.\nKang, B. J., & Choi, K. S. (2000). Automatic transliteration and back-transliteration by\ndecision tree learning. In Proceedings of the 2nd International Conference on Language\nResources and Evaluation, pp. 1135\u20131411.\nKang, I. H., & Kim, G. C. (2000). English-to-Korean transliteration using multiple unbounded overlapping phoneme chunks. In Proceedings of the 18th International Conference on Computational Linguistics, pp. 418\u2013424.\nKim, J. J., Lee, J. S., & Choi, K. S. (1999). Pronunciation unit based automatic EnglishKorean transliteration model using neural network. In Proceedings of Korea Cognitive\nScience Association, pp. 247\u2013252.\nKnight, K., & Graehl, J. (1997). Machine transliteration. In Proceedings of the 35th Annual\nMeetings of the Association for Computational Linguistics, pp. 128\u2013135.\nKorea Ministry of Culture & Tourism (1995). English to Korean standard conversion rule.\nhttp://www.hangeul.or.kr/nmf/23f.pdf.\nLee, J. S. (1999). An English-Korean transliteration and retransliteration model for Crosslingual information retrieval. Ph.D. thesis, Computer Science Dept., KAIST.\nLee, J. S., & Choi, K. S. (1998). English to Korean statistical transliteration for information\nretrieval. Computer Processing of Oriental Languages, 12 (1), 17\u201337.\nLi, H., Zhang, M., & Su, J. (2004). A joint source-channel model for machine transliteration.\nIn Proceedings of ACL 2004, pp. 160\u2013167.\nLin, W. H., & Chen, H. H. (2002). Backward machine transliteration by learning phonetic similarity. In Proceedings of the Sixth Conference on Natural Language Learning\n(CoNLL), pp. 139\u2013145.\nManning, C., & Schutze, H. (1999). Foundations of Statistical natural language Processing.\nMIT Press.\nMeng, H., Lo, W.-K., Chen, B., & Tang, K. (2001). Generating phonetic cognates to\nhandle named entities in English-Chinese cross-language spoken document retrieval.\nIn Proceedings of Automatic Speech Recognition and Understanding, 2001. ASRU '01,\npp. 311\u2013314.\nMitchell, T. M. (1997). Machine learning. New-York: McGraw-Hill.\nNam, Y. S. (1997). Foreign dictionary. Sung An Dang.\nOh, J. H., & Choi, K. S. (2002). An English-Korean transliteration model using pronunciation and contextual rules. In Proceedings of COLING2002, pp. 758\u2013764.\nPagel, V., Lenzo, K., & Black, A. W. (1998). Letter to sound rules for accented lexicon compression. In Proceedings of International Conference on Spoken Language Processing,\npp. 2015\u20132018.\n150\n\n\fA Comparison of Machine Transliteration Models\n\nQu, Y., & Grefenstette, G. (2004). Finding ideographic representations of Japanese names\nwritten in Latin script via language identification and corpus validation.. In Proc. of\nACL, pp. 183\u2013190.\nQuinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81\u2013106.\nQuinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kauffman.\nStalls, B. G., & Knight, K. (1998). Translating names and technical terms in arabic text.\nIn Proceedings of COLING/ACL Workshop on Computational Approaches to Semitic\nLanguages, pp. 34\u201341.\nZhang, L. (2004). Maximum entropy modeling toolkit for python and C++. http://\nhomepages.inf.ed.ac.uk/s0450736/software/maxent/manual.pdf.\nZhang, Y., Huang, F., & Vogel, S. (2005). Mining translations of OOV terms from the web\nthrough cross-lingual query expansion. In Proceedings of the 28th annual international\nACM SIGIR conference on Research and development in information retrieval, pp.\n669\u2013670.\n\n151\n\n\f"}