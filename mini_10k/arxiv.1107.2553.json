{"id": "http://arxiv.org/abs/1107.2553v1", "guidislink": true, "updated": "2011-07-13T14:01:50Z", "updated_parsed": [2011, 7, 13, 14, 1, 50, 2, 194, 0], "published": "2011-07-13T14:01:50Z", "published_parsed": [2011, 7, 13, 14, 1, 50, 2, 194, 0], "title": "Learning Hypergraph Labeling for Feature Matching", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.4649%2C1107.4729%2C1107.2553%2C1107.0164%2C1107.1501%2C1107.1197%2C1107.2726%2C1107.5467%2C1107.4439%2C1107.5774%2C1107.1560%2C1107.4909%2C1107.1954%2C1107.0637%2C1107.4426%2C1107.2660%2C1107.3611%2C1107.5021%2C1107.4362%2C1107.4671%2C1107.1836%2C1107.4873%2C1107.2486%2C1107.4484%2C1107.5145%2C1107.5279%2C1107.5581%2C1107.1318%2C1107.5855%2C1107.5923%2C1107.2037%2C1107.4744%2C1107.5386%2C1107.2604%2C1107.2735%2C1107.0787%2C1107.4358%2C1107.5497%2C1107.5989%2C1107.1259%2C1107.2087%2C1107.3490%2C1107.3574%2C1107.5893%2C1107.3749%2C1107.3000%2C1107.0108%2C1107.0588%2C1107.5164%2C1107.2937%2C1107.4775%2C1107.5305%2C1107.2296%2C1107.4361%2C1107.4590%2C1107.0538%2C1107.5012%2C1107.3826%2C1107.0451%2C1107.0868%2C1107.0831%2C1107.3792%2C1107.3104%2C1107.3130%2C1107.0689%2C1107.0488%2C1107.5781%2C1107.5995%2C1107.0001%2C1107.2095%2C1107.1531%2C1107.1643%2C1107.4924%2C1107.2675%2C1107.1619%2C1107.0891%2C1107.3122%2C1107.1304%2C1107.1955%2C1107.2589%2C1107.5727%2C1107.4156%2C1107.2573%2C1107.0413%2C1107.1399%2C1107.0045%2C1107.2465%2C1107.1961%2C1107.5719%2C1107.4011%2C1107.3503%2C1107.2025%2C1107.0324%2C1107.5801%2C1107.2806%2C1107.0978%2C1107.2266%2C1107.1239%2C1107.5865%2C1107.5541%2C1107.6002&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Learning Hypergraph Labeling for Feature Matching"}, "summary": "This study poses the feature correspondence problem as a hypergraph node\nlabeling problem. Candidate feature matches and their subsets (usually of size\nlarger than two) are considered to be the nodes and hyperedges of a hypergraph.\nA hypergraph labeling algorithm, which models the subset-wise interaction by an\nundirected graphical model, is applied to label the nodes (feature\ncorrespondences) as correct or incorrect. We describe a method to learn the\ncost function of this labeling algorithm from labeled examples using a\ngraphical model training algorithm. The proposed feature matching algorithm is\ndifferent from the most of the existing learning point matching methods in\nterms of the form of the objective function, the cost function to be learned\nand the optimization method applied to minimize it. The results on standard\ndatasets demonstrate how learning over a hypergraph improves the matching\nperformance over existing algorithms, notably one that also uses higher order\ninformation without learning.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1107.4649%2C1107.4729%2C1107.2553%2C1107.0164%2C1107.1501%2C1107.1197%2C1107.2726%2C1107.5467%2C1107.4439%2C1107.5774%2C1107.1560%2C1107.4909%2C1107.1954%2C1107.0637%2C1107.4426%2C1107.2660%2C1107.3611%2C1107.5021%2C1107.4362%2C1107.4671%2C1107.1836%2C1107.4873%2C1107.2486%2C1107.4484%2C1107.5145%2C1107.5279%2C1107.5581%2C1107.1318%2C1107.5855%2C1107.5923%2C1107.2037%2C1107.4744%2C1107.5386%2C1107.2604%2C1107.2735%2C1107.0787%2C1107.4358%2C1107.5497%2C1107.5989%2C1107.1259%2C1107.2087%2C1107.3490%2C1107.3574%2C1107.5893%2C1107.3749%2C1107.3000%2C1107.0108%2C1107.0588%2C1107.5164%2C1107.2937%2C1107.4775%2C1107.5305%2C1107.2296%2C1107.4361%2C1107.4590%2C1107.0538%2C1107.5012%2C1107.3826%2C1107.0451%2C1107.0868%2C1107.0831%2C1107.3792%2C1107.3104%2C1107.3130%2C1107.0689%2C1107.0488%2C1107.5781%2C1107.5995%2C1107.0001%2C1107.2095%2C1107.1531%2C1107.1643%2C1107.4924%2C1107.2675%2C1107.1619%2C1107.0891%2C1107.3122%2C1107.1304%2C1107.1955%2C1107.2589%2C1107.5727%2C1107.4156%2C1107.2573%2C1107.0413%2C1107.1399%2C1107.0045%2C1107.2465%2C1107.1961%2C1107.5719%2C1107.4011%2C1107.3503%2C1107.2025%2C1107.0324%2C1107.5801%2C1107.2806%2C1107.0978%2C1107.2266%2C1107.1239%2C1107.5865%2C1107.5541%2C1107.6002&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This study poses the feature correspondence problem as a hypergraph node\nlabeling problem. Candidate feature matches and their subsets (usually of size\nlarger than two) are considered to be the nodes and hyperedges of a hypergraph.\nA hypergraph labeling algorithm, which models the subset-wise interaction by an\nundirected graphical model, is applied to label the nodes (feature\ncorrespondences) as correct or incorrect. We describe a method to learn the\ncost function of this labeling algorithm from labeled examples using a\ngraphical model training algorithm. The proposed feature matching algorithm is\ndifferent from the most of the existing learning point matching methods in\nterms of the form of the objective function, the cost function to be learned\nand the optimization method applied to minimize it. The results on standard\ndatasets demonstrate how learning over a hypergraph improves the matching\nperformance over existing algorithms, notably one that also uses higher order\ninformation without learning."}, "authors": ["Toufiq Parag", "Vladimir Pavlovic", "Ahmed Elgammal"], "author_detail": {"name": "Ahmed Elgammal"}, "author": "Ahmed Elgammal", "links": [{"href": "http://arxiv.org/abs/1107.2553v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1107.2553v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1107.2553v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1107.2553v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Learning Hypergraph Labeling for Feature Matching\n\narXiv:1107.2553v1 [cs.CV] 13 Jul 2011\n\nToufiq Parag, Vladimir Pavlovic and Ahmed Elgammal\nDept of Computer Science, Rutgers University, NJ\n{tparag, vladimir, elgammal}@cs.rutgers.edu\n\nAbstract\nThis study poses the feature correspondence problem\nas a hypergraph node labeling problem. Candidate feature matches and their subsets (usually of size larger\nthan two) are considered to be the nodes and hyperedges of a hypergraph. A hypergraph labeling algorithm,\nwhich models the subset-wise interaction by an undirected graphical model, is applied to label the nodes (feature correspondences) as correct or incorrect. We describe a method to learn the cost function of this labeling algorithm from labeled examples using a graphical\nmodel training algorithm. The proposed feature matching algorithm is different from the most of the existing learning point matching methods in terms of the\nform of the objective function, the cost function to be\nlearned and the optimization method applied to minimize it. The results on standard datasets demonstrate\nhow learning over a hypergraph improves the matching\nperformance over existing algorithms, notably one that\nalso uses higher order information without learning.\n\n1. Introduction\nIdentifying feature correspondence is an important\nproblem in computer vision (see references in [8]). In\ngeneral, matching features using only the appearance\ndescriptor values can often result in many incorrect\nmatches. To address this problem, most algorithms\nfor feature correspondence combine information about\nboth appearance and geometric structure among the\nfeature locations. Several methods [2, 18, 15, 7, 10]\nutilize the pairwise geometric consistency, along with\nthe pointwise descriptor similarity, to design a matching cost function which is minimized using various optimization algorithms. For example, [18, 7, 15] uses spectral techniques to compute a 'soft' assignment vector\nthat is later discretized to produce the correct assignment of features. These works model the appearance\nand pairwise geometric similarity using a graph, either\nexplicitly or implicitly, and are commonly known as\n\ngraph matching algorithms. The soft assignment vector is typically computed by an eigen-decomposition of\nthe compatibility or the match quality matrix. Several\nstudies applied graph matching algorithms for various\nvision problems [16].\nCaetano et.al. [5] discusses how the parameters of\nthe matching cost function (primarily the match compatibility scores) can be learned from pairs with labeled correspondences to maximize the matching accuracy. A more recent work [16] proposes to learn similar\nmatching scores in an unsupervised fashion by repeatedly refining the soft assignment vector.\nHigher order relationship among the feature points\nhave also been investigated as the means of improving\nthe matching accuracy. Zass et.al. [21] assumes two\nseparate hypergraphs among the feature points on two\nimages and propose an iterative algorithm to match\nthe the two hypergraphs. On the other hand, Olivier\net.al. [8] generalize the pairwise spectral graph matching methods for higher order relationships among the\npoint matches. The pairwise score matrix is generalized to a high order compatibility tensor. The eigenvectors of this tensor are used as the soft assignment\nmatrix to recover the matches.\nIn our framework, each feature correspondence is\nconsidered as a datapoint and we assume a hypergraph\nstructure among these datapoints (similar to [8]). That\nis, we conceive a subset of candidate feature matches\nas a hyperedge of the hypergraph. For subsets of such\ndatapoints, we assume that the relationship among features of one image follows the same geometrical model\nas that present among the corresponding features in the\nother image. We compute the likelihood, using this geometrical model, for every subset of datapoints and use\nit as weight of the hyperedge. The objective is to label\nthe datapoints, i.e., matches to be correct or incorrect,\ngiven this hypergraph structure among them.\nWe adopt a hypergraph node labeling algorithm proposed in [17]. Given a hypergraph, where the hyperedge weights are computed using a model, this algorithm produces the optimal labeling of the nodes that\n\n\fmaximally conforms with the hyperedge weights or\nlikelihood values. Within the framework, the higher\norder interaction among subsets of datapoints is modeled using a higher order undirected graphical model\nor the Markov network (see [17] for details). The labels are computed by solving the inference problem on\nthis graphical model where a labeling cost or energy\nfunction is minimized to produce the optimal labeling.\nIn this paper, we show that the framework of hypergraph node labeling of [17] can be applied for feature\nmatching. In addition, we show how it is possible, and\nin fact advantageous, to learn (a parametric form of)\nthe)cost function for matching given several labeled examples of feature correspondences. The learned forms\nof cost functions are able to appropriately weight the\nlabel disagreement cost for different subsets. For example, if the number of subsets containing more accurate matches than the inaccurate ones, the associated\npenalty function will attain a higher weight to balance\nthe relative importance. The learning procedure is general, i.e., in addition to the feature matching, it can be\nutilized for any application of the labeling problem [17].\nPoint matching problem was addressed by a probabilistic graphical model before, in [4, 13], enforcing a\ngraph among the points for spatial consistency. The\nrequired potential (cost) functions in these two studies\nwere pre-selected and not learned from the data. Our\napproach can handle match interaction in larger sets\nand demonstrates the advantage of learning the cost\nfunctions from the data. Feature matching problem\nhas also been cast as an energy minimization problem\nin [19].\n\n1.1. Contribution:\nAt this point, we would like to clarify what aspect\nof learning (hypergraph labeling for) point matching is\ndifferent from earlier works. Let us suppose xi \u2208 {0, 1}\nis the label for i-th candidate feature match, xi = 1\nimplies a correct match and xi = 0 implies an incorrect one. Let HV k be the match compatibility score\nof a subset V k of matches of size k. The popular\ngraph and tensor matching algorithms maximize the\nfollowing overall matching score to retrieve the correct\nmatches [15, 5, 7, 8].\nS(X) =\n\nX\nVk\n\nHV k\n\nk\nY\n\nsuch label agreement function (or, conversely a disagreement cost function) can be learned from labeled matches. We believe it is particularly useful\nto learn this function for higher order (k > 2) methods. To illustrate the necessity of such learning, we\nshow two images in Figure 1 with candidate feature\nmatches (D1 , F1 ), (D2 , F2 ), (D3 , F3 ) and (D3 , F4 ) , all\nwith equal matching probability, overlaid on them.\n\nD1\n\nF\n\nF1\n\n2\n\nD2\n\nF4\n\nD3\nF\n\n3\n\nFigure 1. Triangle pairs with overlapping matches.\n\nIt is assumed that the geometrical arrangement\namong matching features can be encoded by triangle. Clearly, the similarity between triangles D1 D2 D3\n(red) and F1 F2 F3 (green) will be high resulting\nin a large match compatibility HV 3 (where V 3 =\n{(D1 , F1 ), (D2 , F2 ), (D3 , F3 )}). Notice that the triangle F1 F2 F4 (blue dashed) would also have relatively\nlarge similarity with D1 D2 D3 . Though this subset\n{(D1 , F1 ), (D2 , F2 ), (D3 , F4 )} of matches contain one\nincorrect match (D3 , F4 ), it still provides us significant\ngeometric information about the two correct matches\n(D1 , F1 ) and (D2 , F2 ) . Incorporating this information in the algorithm should assist establishing more\ncorrect correspondences\nQk among the features. However,\nthe form of s(V k ) = l=1 xil does not explicitly handle\nthis situation, even when xil is relaxed to take values in\nreal domain1 . One needs to learn an appropriate label\nagreement (or disagreement cost) function to explicitly\ninclude this information in the framework. Learning\nthe cost function can also counteract the uneven ratio\nof subsets with more correct matches and those with\nmore incorrect matches.\nAs it will be explained in details later, to determine\nthe correspondence, we in fact minimize a cost function\nof the form as follows.\n\u1ebc(X) =\n\nX\n\nHV k g\u03031 (xi1 , . . . , xik )\n\nVk\n\n+ (1 \u2212 HV k ) g\u03030 (xi1 , . . . , xik ).\nxil .\n\n(2)\n\n(1)\n\nl=1\n\nThe score function is a weighted summation\nQk of subsetwise label concurrence function, s(V k ) = l=1 xil . Notice that, s(V k ) is a binary valued function: s(V k ) = 1\nonly when all labels xi1 , . . . , xik are equal to 1 and\n0 otherwise. Instead of using this predefined binary valued function, we investigate whether or not\n\nThis paper describes how to learn appropriate subsetwise label disagreement cost functions (also referred\nas penalty functions) g\u03031 and g\u03030 from labeled matches.\nOur approach is significantly different in concept from\nprevious learning algorithms for correspondence. The\n1 For binary x , s(V 3 ) = 0 with one incorrect match in V 3\nil\nand therefore the compatibility score is ignored.\n\n\falgorithms of [5, 16] aim to learn a match compatibility function HV k from the data to optimally reflect\naccurate correspondences among the features. On the\ncontrary, our algorithm learns the label disagreement\ncost functions g\u03031 and g\u03030 to minimize the total label disagreements within the subsets given the subset matching qualities HV k . The next section describes how feature correspondence can be cast as a hypergraph labeling problem as defined in [17]\n\n2. Matching as hypergraph labeling\nGiven two images IL and IR , we denote al and ar\nto be the indices of feature points from IL and IR respectively. In general, the number nL of features in\nIL is different from the number nR of features in IR .\nEach candidate match (al , ar ) is considered to be a datapoint vi , i = 1, . . . , n, in our approach. The goal is\nto partition the dataset V = {v1 , . . . , vn } into subset\nA comprising correct correspondences and to B comprising incorrect ones. This is a data labeling problem\nwhere the binary label xi \u2208 {0, 1} of vi needs to be\nassigned xi = 1 if vi belongs A and to 0 otherwise.\nWe wish to exploit the information about subsets of datapoints to enforce geometric consistency\nin matching. More specifically, for a subset V k =\n{vi1 , . . . , vik } = {(al1 , ar1 ), . . . , (alk , ark )} of size k of\nmatching points, we assume the geometric relationship among {al1 , . . . , alk } to be similar to that among\n{ar1 , . . . , ark }. This similarity value (computed by a\nsuitable function) is denoted by \u03bb(V k ) \u2208 [0, 1]. Notice that, we are effectively dealing with a hypergraph\nwith datapoints vi as the nodes and the subsets V k\nas the hyperedges. Given such hypergraph, the labeling algorithm is supposed to partition the set of nodes\ninto two sets A and B, corresponding to correct and\nincorrect matches respectively. We will use the term\nlikelihood value and weight interchangeably when referring to similarity value \u03bb(V k ).\nThe work in [17] models the higher order interactions in this hypergraph by a Markov network (by a\nConditional Random Field (CRF) to be precise) [9].\nThe optimal labeling can then be achieved by solving\nthe inference for this CRF model. We follow this representation which is described in the next section.\n\n3. The cost function\nLet V k be the set of all hyperedges V k in this hypergraph. Let X = {x1 , . . . , xn } be a label assignment\nof the nodes V of the hypergraph. The cost function\nthat asserts discrepancy of node assignments X in the\nhypergraph nodes V can be written as\n\nX\n\nE(X, V ) =\nV\n\nE k (X k , V k ),\n\n(3)\n\nk \u2208V k\n\nwhere X k is the set labels of member nodes of subset V k and E k is the local discrepancy, i.e., the cost\nof assignment X k in V k . We assume functionally homogeneous local costs, E k = E. Given this representation, it is possible to construct an equivalent CRF\nwith clique potentials E k (see [17]) and formulate the\noptimal assignment task as the inference in this CRF.\nFollowing [17] , each clique potential E is represented as\nE(X k ; V k ) = \u03b21 \u03bb(V k ) g1 (\u03b70 ) + \u03b20 (1 \u2212 \u03bb(V k )) g0 (\u03b71 ).\n(4)\n\nHere, gc , c = 0, 1 represent a penalty function :\nthe cost of assigning clique nodes to an incorrect class\n(eg, match to non-match and vice-versa). The penalty\nfunction is defined as a function of \u03b71\u2212c , the number of\nnodes in the clique whose label differs from the clique\nhypothesis c. \u03b2c are non-negative balancing parameters\nand \u03b70 + \u03b71 = k.\nIntuitively, this potential penalizes, via functions gc ,\nthe label assignments incompatible with one of the two\nhypotheses, matching and non-matching features. To\nachieve this, the penalties gc should be non-decreasing\nin \u03b71\u2212c . If the likelihood of matching, \u03bb(V k ), is high,\nthe potential seeks to decrease \u03b70 , the number of assignments to \"not-matching\" hypothesis. In the opposite case, with high non-matching likelihood 1 \u2212 \u03bb(V k ),\nthe potential attempts to decrease the number of labels\nincompatible with this hypothesis, \u03b71 .\nPenalty functions gc could be directly modeled as\nlinear and nonlinear functions of number of label disagreement \u03b71\u2212c in the clique. However, as it will become clear later, it is advantageous to learn a nonlinear mappings gc from labeled data. The next section\ndescribes how the functions gc can be learned from labeled matches/mismatches.\n\n4. Learning penalty functions\nGiven J hypergraphs with hyperedges Vjk , j =\n1, . . . , J, along with the weights and labels Xj of the\ndatapoints (or correspondences), we wish to learn the\nparametric form of the gc functions. We first describe\ntwo parametric forms of the penalty functions so that\nthe clique potentials, as defined in Equation 4 become\nlog-linear models. In particular, we seek to express the\npotential as a linear combination of factors defined over\neach clique) [9]\nE(X k ; V k ) =\n\nX\n\nwl \u03c6l (X k ; V k ).\n\n(5)\n\nl\n\nIn this definition, \u03c6l (X k ; V k ) are the factors and wl\nare the mixing weights. The following sections explain\n\n\fhow restating the penalty functions in this manner facilitates learning using CRF training algorithms.\n\n4.1. Discrete gc\nFirst, we express gc as a discrete function. Observe\nthat, penalty functions gc are defined on \u03b7(1\u2212c) values, which are integers in our case. Therefore, it suffices to learn a set of discrete mapping gc (\u03b7(1\u2212c) ) for\nall c \u2208 {0, 1} and 0 \u2264 \u03b7(1\u2212c) \u2264 k. Let us introduce two\nquantities as follows\nwc\u03b1 = \u03b2c gc (\u03b1),\n\n(6)\n\n\u03c6c (\u03b1; V k ) = \u2212\u03bbc (V k ) I(\u03b7(1\u2212c) , \u03b1),\n\n(7)\n\nwhere I(s, t) is an indicator function which equals to 1\nonly when s is equal to t and 0 otherwise. Furthermore,\nthe likelihood weights are denoted by \u03bb1 (V k ) = \u03bb(V k )\nand \u03bb0 (V k ) = 1 \u2212 \u03bb(V k ) for notational convenience.\nNotice that, in this case, \u03c6c functions are the factors\n(for each clique) that assume nonzero values only when\n\u03b71\u2212c = \u03b1. The clique cost function defined in Equation 4 can be rewritten as follows\n\n(e)\n\nFor polynomial gc , we learn the values of gc for all\ne = 0, 1, 2 and c = 0, 1. This redefinition of gc has the\nbenefit of regulating the learned form to be of some\nspecific type. Also, regardless of the size k or data\nsubset, we only need to learn 3 \u00d7 C parameters, where\nC is the total number of classes. Next section briefly\ndiscusses existing techniques for learning CRFs.\n\n4.3. Learning algorithms\nIn last two sections we have shown that the clique\npotential function of the proposed framework can be\nexpressed as a linear combination of features or factors.\nThe joint probability of any label configuration for a\nCRF, with discrete form of gc , can be stated as follows\np(X | V ) =\n\n1\nexp\nZ(V )\n\n\u001a\n\n1 X\nk\nX X\n\n\u001b\n\nwc\u03b1 \u03c6c (\u03b1, V k )\n\nV k \u2208V k c=0 \u03b1=0\n\n(14)\n\nThis definition of gc expresses the joint probability\nof any assignment as log-linear model. For this form of\ngc , the values of wc\u03b1 are learned for all \u03b1 = 1, . . . , \u03b7(1\u2212c)\nand c = 0, 1.\n\nwhere Z(V ) is a normalizing term, Z(V ) =\nP\nX p(X | V ). The joint probability will be similar\nfor second order polynomial gc and we are omitting\nthe derivation for it here. There are two types of algorithms to estimate the parameters wc\u03b1 from data: one\nthat aims at determining the parameters by maximizing the log-likelihood [9] and the other that maximizes\nthe separation, or the label margin, between classes of\ndatapoints [1].\n\n4.2. Second order polynomial gc\n\n4.3.1\n\nE(X k ; V k ) =\n\nk\nXX\nc\n\nwc\u03b1 \u03c6c (\u03b1; V k ).\n\n(8)\n\n\u03b1=0\n\nUnconstrained forms of gc may be prone to overfitting. We thus propose a more constrained gc by assuming a second order polynomial form for it. In this\ncase, this function can be expressed using the Taylor\nexpansion around reference point 0:\ngc (\u03b1) =\n(0)\n\ngc(0)\n\n+\n\n\u03b1gc(1)\n\n\u03b12 (2)\ngc .\n+\n2\n\n(9)\n\n(2)\n\n(1)\n\nIn Equation 9, gc , gc and gc are the 0, 1st and 2nd\norder derivatives of gc at 0. The features for this case\ncan be defined as\n\u03c8c0 (\u03b1; V k ) = \u2212\n\nk\nX\n\n\u03bbc (V k ) I(\u03b7(1\u2212c) , \u03b3),\n\n(10)\n\n\u03b1 \u03bbc (V k ) I(\u03b7(1\u2212c) , \u03b3),\n\n(11)\n\n\u03b3=0\n\n\u03c8c1 (\u03b1; V k ) = \u2212\n\nk\nX\n\u03b3=1\n\n\u03c8c2 (\u03b1; V k ) = \u2212\n\nk\nX\n\u03b3=1\n\nThe log-likelihood function for the training data is\ngiven by\nl(w) =\n\n1 X\nk\nJ\nX\nX X\n\n2\n\n\u03b1\n\u03bbc (V k ) I(\u03b7(1\u2212c) , \u03b3).\n2\n\n(12)\n\n2\nXX\nc\n\ne=0\n\ngc(e) \u03c8ce (\u03b1; V k ).\n\n(13)\n\nwc\u03b1 \u03c6c (\u03b1; V k ) \u2212 log Z(V ). (15)\n\nj=1 V k \u2208V k c=0 \u03b1=0\nj\n\nIt has been shown that l(w) is concave [9]. Therefore, a\nGradient Ascent algorithm is able to produce the globally optimal values for wc\u03b1 . It is straightforward to see\nthat the gradient with respect to wc\u03b1 is the difference\nbetween summation of observed and expected \u03c6c (\u03b1)\nvalues\nJ\nX\nX\n\u2202l\n=\n\u03c6c (\u03b1; V k )\n\u03b1\n\u2202wc\nk\nj=1 k\nV \u2208Vj\n\n\u2212\n\nThen, the cost function in Equation 4 can be expressed as linear combination of features \u03c8ce (\u03b1), e =\n0, . . . , 2\nE(X k ; V k ) =\n\nLikelihood Maximization\n\nJ\nX\nX X\n\n\u03c6c (\u03b1; V k ) p(X k | V k ).\n\n(16)\n\nj=1 V k \u2208V k X k\nj\n\nWe used a sum-product belief propagation algorithm [14] to compute the marginal posteriors\np(X k | V k ). A regularizer term was added to the\nlikelihood function to penalize large parameter values.\nApart from Gradient Ascent, other algorithms such as\nConjugate Gradient and L-BFGS have also been for\nthis maximization problem [9].\n\n\f4.3.2\n\nMargin maximization\n\nThe second type of algorithms try to estimate the parameters by maximizing the class margin of the labeled\nexamples. Margin maximization is useful if the data\ndistribution is biased to one of the classes or there are\nmany noisy samples in the data. Bartlett et.al. [1] proposed a constrained optimization problem, in terms of\nprimal variables wc\u03b1 , for parameter learning in maximal margin setting. Their formulation minimizes a\nloss function, defined in terms of the number of incorrectly labeled examples, and a regularizer term. An\nexponentiated gradient (EG) algorithm is applied to\nminimize the objective that updates the primal variables wc\u03b1 similarly as in Equation 16. In addition, the\nEG algorithm also updates the the dual variables to\nminimize the subset-wise mislabeling error. Furthermore, the marginal terms are different from those in\nlikelihood maximization \u2013 in [1], they are calculated\nfrom a Markov network where the dual variables act as\npotential functions.\nMore efficient version of both these algorithms have\nbeen described in [6]. In our experiments, parameters\nwere learned by standard Gradient Ascent optimization\nto maximize the likelihood for a discrete gc .\n\n5. Inference\nOnce gc (*), c \u2208 {0, 1}, are learned, problems with\nnonlinear gc (*) can be solved using any efficient Markov\nnetwork inference algorithm, See [11, 20], and references therein. We adopted the sum-product belief\npropagation [14] since we also use it for computing the\nmarginal probabilities p(X k |V k ) required to learn the\nparameters. The output of this algorithm is belief (approximate marginal probability) bi (1) and bi (0) that\nany datapoint vi belong to class 1 and 0 respectively.\nThe belief values for each datapoint vi could be used\nto determine the hard one to one assignment for any\nfeature al of image IL to its unique match ar on image\nIR . To do this, for each al , we select the match corresponding to the datapoint with the largest ratio of two\n(1)\namong all the datapoints associated with\nbeliefs bbii (0)\nal . The accompanying feature ar on the right image is\nselected as the resultant match for al . This method of\ndiscretization is similar to [15].\n\n6. Experiments and Results\nThis section describes different matching experiments conducted on standard datasets to test the proposed method and compares the performances with\npast studies. For all the experiments, the penalty functions were learned using Gradient Ascent to maximize\nthe likelihood for a discrete mapping gc (Section 4.1).\n\n6.1. House, Hotel and Horse data\nWe conduct our first experiment on the standard\nHouse and Hotel datasets. Each of these datasets contains a sequence of (around 100) images of a toy house\n(or hotel) seen from increasingly varying viewpoint.\nLocations of a set of keypoints, that appear on each\nof the image of the sequence, are available for both\nthese sequences.\nAnother synthetic dataset, namely the silhouette\nimages of a Horse as used in [5], were also included\nin this experiment. From a single silhouette image,\ntwo sequences of 200 images were generated by shearing and rotating. The width of the image is sheared\nto twice of its height at most and the maximum angle\nof rotation was 90 degrees. These image transformations are different from those present in House and Hotel datasets. The feature locations are extracted by a\nsampling method as in [5].\nFor the proposed algorithm, the Geometric Blur\n(GB) [3] descriptor is used to represent each feature.\nFor each keypoint al in image IL , m = 3 candidate\nmatches, denoted by the set \u03bc(al ), are chosen based\non largest normalized correlation between the GB descriptors. Each of the candidate matches is considered\nto be a datapoint vi .\nWe construct a hypergraph of edge cardinality k = 3\nwith these datapoints. For each feature point al in\nimage IL , all possible triangles are generated among\nal and kN N = 5 nearest neighbors. Any such triangle among {al1 , . . . , alk }, has k m possible matching triangles in image IR induced by the set of candidate\nmatches {\u03bc(al1 ), . . . , \u03bc(alk )}. This construction of hypergraphs among matches follows that of [8] and [21],\nexcept [8] searches all possible triangles in image IR\ninstead of searching the ones induced by candidate\nmatches. The geometric similarity of these triangle\npairs are evaluated by the sum of squared difference of\nthe angles similar to the tensor matching algorithm [8].\nThe parametric difference \u000f between triangles is converted to geometric similarity weight using 1 \u2212 \u03b4\u000f where\n\u03b4 = 0.5 for all experiments in this section2 .\nThe appearance similarity value is the normalized\ncorrelation between two GB descriptors computed for\npotential matching features. Each candidate match is\nassigned a weight that reflects the quality of the match\ncomputed by normalized correlation [15]. To compute\nthe overall similarity \u03bb1 (V k ) between two triangles, the\nweight of corresponding matches {\u03bc(al1 ), . . . , \u03bc(alk )} is\nmultiplied with the geometric similarity weight computed from parametric difference between two triangles.\n2 Triangle\n\npairs with \u000f > \u03b4 are discarded.\n\n\fhouse:Incorrect Matches\n\nhotel:Incorrect Matches\n\n15\n10\n5\n0\n0\n\n20\n15\n10\n\n40\n\n60\ninterval\n\n80\n\n100\n\n0\n0\n\n40\n\n100\nProposed\u2212learn\nGraph Matching\nTensor Matching\nLinear+learn\nQuad+learn\n\n30\n\n20\n\n10\n\n5\n\n20\n\nrotate:Incorrect Matches\n\n50\nProposed\u2212learn\nGraph Matching\nTensor Matching\nLinear+learn\nQuad+learn\n\n% incorrect points/pair\n\n20\n\n25\n% incorrect points/pair\n\n% incorrect points/pair\n\n25\n\nshear:Incorrect Matches\n\n30\nProposed\u2212learn\nGraph Matching\nTensor Matching\nLinear+learn\nQuad+learn\n\n% incorrect points/pair\n\n30\n\n20\n\n40\n\n60\n\n80\n\n100\n\n0\n0\n\nProposed\u2212learn\nGraph Matching\nTensor Matching\nLinear+learn\nQuad+learn\n\n80\n\n60\n\n40\n\n20\n\n20\n\ninterval\n\n40\n\n60\n\n80\n\n100\n\n0\n0\n\n20\n\n40\n\ninterval\n\n60\n\n80\n\n100\n\ninterval\n\nFigure 2. (Left to right) House, Hotel, Horse-Shear, Horse-Rotate: Mean and std deviation of incorrect matches.\n\nWe consider four sets of image pairs where, in each\npair, the two images are {20, 40, 60, 80} frames apart\nfrom the other (also 100 for Horse datasets). For each\nset of image pairs, first five pairs were selected to learn\nthe parameters for the proposed matching algorithm.\nWe learned the parameters for a discrete gc by maximum likelihood (ML) method (refer to Section 4).\nThe performance of our algorithm is compared\nagainst the following algorithms:\n1. Tensor matching method [8](implementation\navailable at author's website): The parameter\nvalues such as number of triangles to be generated, number of nearest neighbors of each triangle\nand the distances are tuned to produce the best\nresults in each of the experiments.\n2. Graph matching of [15]: We used the exact same\nprocedure as described in the paper with the same\nm = 3 candidate matches for each keypoint and\nthe used 3 as the distance threshold to determine\nthe neighboring keypoints (also tuned for best result).\n3. Learning graph matching [5]: The results of learning both the linear and quadratic assignments have\nbeen used for comparison.\n\nthe descriptor similarity would be too low in rotated\nimages for a weight vector to generate a correct match.\nThis observation supports the claim made in [16] that,\nin general, Linear Assignment alone can not result in\naccurate matches. The proposed algorithm and Graph\nmatching [15] could not identify the correct matches\nfor larger rotational angles (>80 degrees) due to inferior initial candidate matches.\nThese results attest the advantage of using higher\norder information and learning the cost function\nfor matching.\nUtilizing higher order information\nconsistently produced higher accuracy than learning\nQuadratic Assignment in all but one dataset. The Tensor matching algorithm, which uses higher order information but does not lear from data, was not robust\neither on different datasets3 . The reason for this behavior was surmised in the introduction: the number\nof subsets generated by higher order algorithm is usually large with imbalanced ratio of useful subsets. One\nneeds to learn the appropriate cost functions for accurate labeling of the members of these subsets. However, it is interesting to see that both Quadratic Assignment [5] and Tensor matching [8] produced a perfect matching for rotated images (Figure 2, rightmost\nplot). Indeed, [8] also reports similar matching results\non synthetic 2D points.\nw1: house, 80, LearnML\n\nw0: house, 80, LearnML\n\n1.4\n\n1\n\n1.2\n\n0.9\n0.8\n\n1\n\n0.7\n0.8\nw0\n\nw1\n\nFigure 2 shows the percentage of incorrect matches produced by these and proposed method. Some qualitative\nresults are supplied as supplementary material.\nThe results show that none of the spectral Graph\nmatching and Tensor matching techniques was able to\nperform well on all of these datasets. On the other\nhand, the proposed method, with learned cost functions is more robust and accurate than all other methods in House, Hotel and Horse-shear datasets. The\nresult of learned Linear Assignment procedure of [5]\nclosely follows that of our method. However, learning linear assignment produces unacceptably high error rates (much higher than the proposed method) for\nHorse-rotate dataset. This is due to the fact that Linear Assignment learns the weight vector for descriptor\nsimilarity for a candidate match. Unless the window\u2013\nin which the descriptor is computed\u2013 is also rotated,\n\n0.6\n\n0.6\n0.5\n0.4\n\n0.4\n\n0.2\n0\n0\n\n0.3\n1\n\n\u03b70\n\n2\n\n3\n\n0.2\n0\n\n1\n\n\u03b71\n\n2\n\n3\n\nFigure 3. Parameters learned by ML method, left: w1 , right\n: w0 .\n\nIn Figure 3 , we show the discrete gc learned by the\nML algorithm for c = 0, 1. As expected, the learned\npenalty functions resembles strongly to smooth concave\n(w1 , left in Figure 3) and convex (w0 ,right in Figure 3)\n3 In\n\n[8], the authors did not report the results on all possible\npairs of images. Results for one pair of images for each interval\non House dataset were reported are these values are the same as\nthe minimum error rates of our result.\n\n\ffunctions. The forms of gc functions also provides some\ninsight about the subsets generated for matching. A\nconvex penalty imposes 'lenient' penalties on lower\nvalues of \u03b71 , number of label variables assuming the\nopposite class, class 1. This penalty function would\nbe effective when there are many subsets comprising\nvery few (e.g., one) correct matches. For these subsets, a convex g0 would allow to let few datapoints\nwithin the subset to assume the opposite label 1. Examining the matching triangles used for matching, one\ncan verify that there are indeed many subsets that contains one correct matches and two incorrect matches in\nthem. On the other hand, the triangles with all correct\nmatches are rare and therefore the penalty function is\n'strict' (i.e., concave) on the value of \u03b70 .\nMore plots of such learned penalty functions, as\nwell as non-discretized belief values (i.e., the soft assignment vector) generated by inference algorithm and\nsome qualitative matching results are presented as supplementary material.\n\n6.2. KTH Activity\nWe applied our method on some KTH activity recognition data [5]. For this dataset, we chose three activities, walking, jogging and hand waving and for each\nof these activities we randomly selected two sequences.\nThe experimental setup is almost same as above except\nthe features are detected using Kadir-Brady (KB) keypoint detector algorithm [12] on both the images, i.e.,\nwe do not manually select keypoints on image. For\neach keypoint selected by the feature detector (KB) on\nthe left image, the goal is to find its best match on the\nright image.\nOne of the objectives of this experiment is to show\nthe necessity of learning the penalty function instead\nof employing predefined (linear) ones. We applied the\nlabeling algorithm with predefined linear penalty functions and compared the results to show the improvement achieved by learning gc . For the learning algorithms, discrete gc functions are learned using the\nML estimation procedure as before. All parameters\nfor both methods are the same for all the experiments\nin this section. Sample output matches are shown in\nFigure 4. The top row shows the output produced by\nthe proposed method using linear penalties, and the\nbottom row shows the results produced by discrete\ngc trained from data. The matching algorithm with\nlearned penalty function were able to extract more accurate matches than that with linear penalties.\nTable 1 summarizes the quantitative matching performances of these two methods. The results clearly\nshow that hypergraph labeling with learned penalty\nfunction consistently produces better results than the\n\nsame method with predefined linear penalties. It is\nworth mentioning here that the proposed matching algorithm was applied to the (spatially clustered) keypoint locations detected by the KB detector leading\nto variable number of feature locations in different images. We manually counted the number of correct and\nincorrect matches from the output for quantitative performance evaluations.\nThe learned penalty functions for each of these\ndatasets resemble closely to those shown in Figure 3,\nplease refer to the supplementary material specific\nplots. These learned optimal penalty functions are\nclearly non-linear which explains why predefined linear\npenalty functions produce inferior matching results.\n\n6.3. Caltech Aeroplane and Motorbike\nFinally, we are showing some more qualitative results on Caltech objects, such as airplanes and motorbikes, in Figure 5. The experimental setup is exactly\nsame as that described in the last section. Notice that,\nin this experiment, we are establishing correspondences\nbetween two different instances of same object category, unlike the experiments described before.\n\n7. Discussion\nIn this paper, we propose a novel feature matching\nalgorithm based on higher order information among\nthem. The feature correspondence problem is formulated as a hypergraph node labeling problem. A recent algorithm that models the higher order interaction among the datapoints using a Markov network\nis applied to address the labeling problem. We describe how the associated cost function can be learned\nfrom labeled data using existing graphical model training algorithm. The results show that learning the cost\nfunction makes the proposed matching algorithm more\nrobust than other pairwise and higher order methods.\nThis paper presents methods to learn the appropriate cost functions (in terms of the penalty functions)\nof a hypergraph node labeling algorithm [17]. Feature\ncorrespondence is one significant application of the supervised hypergraph labeling algorithm, but the learning procedure can benefit any applications of it. We\nstrongly believe learning penalty functions will improve\nthe performances of model estimation and object localization demonstrated in [17].\nHypergraph labeling method could potentially be\napplied to other problems where learning cost functions\ncould be advantageous. One such problem is object\nboundary detection or image segmentation. We performed a small experiment on natural images of Berkeley dataset. The description of the procedure and sample results are shown in the supplementary material to\n\n\fFigure 4. Improvement achieved by learning. Top: results of [17] using a predefined linear penalty, bottom: matches after\nlearning. More correct correspondences are recovered by learned penalty function.\nmethod\nJog1\nJog4\nWalk1\nWalk4\nWave4\nWave7\nTrue False True False True False True False True False True False\nLinear gc\n4.33\n0.83\n5.5\n1.83\n4.89\n0.78\n3.86\n1.43\n5\n0.5\n6.67\n2\nLearned gc\n4.83\n0.67\n6\n1.5\n6.89\n0.89\n5.71\n1\n7.5\n0.67\n7.17\n1.33\nTable 1. Average number of correct and incorrect matches (NOT percentages) found on the image pairs. The proposed\nalgorithm with learned penalty functions consistently produces more true positives with less false positives on all the\nsequences.\n\navoid confusion. These results suggest the method can\nbe used for segmentation problems, at least for specific\ndomain if not for natural images, with appropriately\nchosen image features and model.\n\nReferences\n[1] P. L. Bartlett, B. Taskar, M. Collins, and\nD. Mcallester. Exponentiated gradient algorithms for\nlarge-margin structured classification. In NIPS, 2005.\n[2] A. Berg, T. Berg, and J. Malik. Shape matching\nand object recognition using low distortion correspondences. In CVPR, 2005.\n[3] A. Berg and J. Malik. Geometric blur for template\nmatching. In CVPR, 2001.\n[4] T. S. Caetano, T. Caelli, D. Schuurmans, and D. A.\nBarone. Graphical models and point pattern matching.\nPAMI, 28:1646\u20131663, 2006.\n[5] T. S. Caetano, J. J. McAuley, L. Cheng, Q. V. Le,\nand A. J. Smola. Learning graph matching. PAMI,\n31:1048\u20131058, 2009.\n[6] M. Collins, A. Globerson, T. Koo, X. Carreras, and\nP. Bartlett. Exponentiated gradient algorithms for\nconditional random fields and max-margin markov\nnetworks. JMLR, 9:1775\u20131822, 2008.\n[7] T. Cour, P. Srinivasan, and J. Shi. Balanced graph\nmatching. In NIPS 19, 2007.\n[8] O. Duchenne, F. Bach, I. Kweon, and J. Ponce. A\ntensor-based algorithm for high-order graph matching.\nIn CVPR, 2009.\n[9] L. Getoor and B. Taskar. Introduction to Statistical\nRelational Learning. MIT Press, 2007.\n[10] S. Gold and A. Rangarajan. A graduated assignment\nalgorithm for graph matching. TPAMI, 1996.\n\n[11] H. Ishikawa. Higher order clique reduction in binary\ngraph cut. In CVPR, 2009.\n[12] T. Kadir and M. Brady. Saliency, scale and image\ndescription. IJCV, 45(2):83\u2013105, 2001.\n[13] N. Komodakis and N. Paragios. Beyond loose lprelaxations: Optimizing mrfs by repairing cycles. In\nECCV, 2008.\n[14] F. R. Kschischang, B. J. Frey, and H. andrea\nLoeliger. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory,\n47:498\u2013519, 1998.\n[15] M. Leordeanu and M. Hebert. A spectral technique for\ncorrespondence problems using pairwise constraints.\nIn ICCV, 2005.\n[16] M. Leordeanu and M. Hebert. Unsupervised learning\nfor graph matching. In CVPR, 2009.\n[17] T. Parag and A. Elgammal. Supervised hypergraph\nlabeling. In CVPR, 2011.\n[18] L. S. Shapiro and J. M. Brady. Feature-based correspondence: an eigenvector approach. Image and Vision Computing, 10(5):283 \u2013 288, 1992.\n[19] L. Torresani, V. Kolmogorov, and C. Rother. Feature correspondence via graph matching: Models and\nglobal optimization. In ECCV, 2008.\n[20] T. Werner. A linear programming approach to maxsum problem: A review. PAMI, 29(7):1165\u20131179, 2007.\n[21] R. Zass and A. Shashua. Probabilistic graph and hypergraph matching. In CVPR, 2008.\n\n\fFigure 5. Qualitative results on Caltech aeroplanes and motorbikes.\n\n\f"}