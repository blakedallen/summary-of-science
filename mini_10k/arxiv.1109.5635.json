{"id": "http://arxiv.org/abs/1109.5635v1", "guidislink": true, "updated": "2011-09-26T16:48:20Z", "updated_parsed": [2011, 9, 26, 16, 48, 20, 0, 269, 0], "published": "2011-09-26T16:48:20Z", "published_parsed": [2011, 9, 26, 16, 48, 20, 0, 269, 0], "title": "Approximating Edit Distance in Near-Linear Time", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1109.5608%2C1109.5720%2C1109.1618%2C1109.3918%2C1109.1021%2C1109.5708%2C1109.5064%2C1109.4543%2C1109.4498%2C1109.3522%2C1109.6071%2C1109.0814%2C1109.6521%2C1109.6727%2C1109.3299%2C1109.1521%2C1109.4118%2C1109.2124%2C1109.4248%2C1109.1255%2C1109.1179%2C1109.1062%2C1109.4157%2C1109.5968%2C1109.0196%2C1109.5129%2C1109.2934%2C1109.2453%2C1109.4517%2C1109.1996%2C1109.6349%2C1109.2071%2C1109.3366%2C1109.3601%2C1109.1173%2C1109.4335%2C1109.2262%2C1109.4063%2C1109.1695%2C1109.4976%2C1109.6309%2C1109.2373%2C1109.3370%2C1109.2251%2C1109.6055%2C1109.3163%2C1109.1804%2C1109.0221%2C1109.3289%2C1109.1372%2C1109.5911%2C1109.5685%2C1109.3061%2C1109.1320%2C1109.5867%2C1109.1343%2C1109.0263%2C1109.2774%2C1109.2350%2C1109.0129%2C1109.4106%2C1109.3237%2C1109.3109%2C1109.5298%2C1109.4635%2C1109.4987%2C1109.2098%2C1109.5902%2C1109.0768%2C1109.4746%2C1109.6777%2C1109.5512%2C1109.6088%2C1109.5686%2C1109.6452%2C1109.6451%2C1109.4042%2C1109.0721%2C1109.6832%2C1109.6220%2C1109.2523%2C1109.5565%2C1109.2660%2C1109.1445%2C1109.2429%2C1109.6042%2C1109.1623%2C1109.1565%2C1109.1892%2C1109.1692%2C1109.4286%2C1109.2708%2C1109.1677%2C1109.4099%2C1109.0410%2C1109.5293%2C1109.5666%2C1109.5635%2C1109.0857%2C1109.4494%2C1109.5873&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Approximating Edit Distance in Near-Linear Time"}, "summary": "We show how to compute the edit distance between two strings of length n up\nto a factor of 2^{\\~O(sqrt(log n))} in n^(1+o(1)) time. This is the first\nsub-polynomial approximation algorithm for this problem that runs in\nnear-linear time, improving on the state-of-the-art n^(1/3+o(1)) approximation.\nPreviously, approximation of 2^{\\~O(sqrt(log n))} was known only for embedding\nedit distance into l_1, and it is not known if that embedding can be computed\nin less than quadratic time.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1109.5608%2C1109.5720%2C1109.1618%2C1109.3918%2C1109.1021%2C1109.5708%2C1109.5064%2C1109.4543%2C1109.4498%2C1109.3522%2C1109.6071%2C1109.0814%2C1109.6521%2C1109.6727%2C1109.3299%2C1109.1521%2C1109.4118%2C1109.2124%2C1109.4248%2C1109.1255%2C1109.1179%2C1109.1062%2C1109.4157%2C1109.5968%2C1109.0196%2C1109.5129%2C1109.2934%2C1109.2453%2C1109.4517%2C1109.1996%2C1109.6349%2C1109.2071%2C1109.3366%2C1109.3601%2C1109.1173%2C1109.4335%2C1109.2262%2C1109.4063%2C1109.1695%2C1109.4976%2C1109.6309%2C1109.2373%2C1109.3370%2C1109.2251%2C1109.6055%2C1109.3163%2C1109.1804%2C1109.0221%2C1109.3289%2C1109.1372%2C1109.5911%2C1109.5685%2C1109.3061%2C1109.1320%2C1109.5867%2C1109.1343%2C1109.0263%2C1109.2774%2C1109.2350%2C1109.0129%2C1109.4106%2C1109.3237%2C1109.3109%2C1109.5298%2C1109.4635%2C1109.4987%2C1109.2098%2C1109.5902%2C1109.0768%2C1109.4746%2C1109.6777%2C1109.5512%2C1109.6088%2C1109.5686%2C1109.6452%2C1109.6451%2C1109.4042%2C1109.0721%2C1109.6832%2C1109.6220%2C1109.2523%2C1109.5565%2C1109.2660%2C1109.1445%2C1109.2429%2C1109.6042%2C1109.1623%2C1109.1565%2C1109.1892%2C1109.1692%2C1109.4286%2C1109.2708%2C1109.1677%2C1109.4099%2C1109.0410%2C1109.5293%2C1109.5666%2C1109.5635%2C1109.0857%2C1109.4494%2C1109.5873&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We show how to compute the edit distance between two strings of length n up\nto a factor of 2^{\\~O(sqrt(log n))} in n^(1+o(1)) time. This is the first\nsub-polynomial approximation algorithm for this problem that runs in\nnear-linear time, improving on the state-of-the-art n^(1/3+o(1)) approximation.\nPreviously, approximation of 2^{\\~O(sqrt(log n))} was known only for embedding\nedit distance into l_1, and it is not known if that embedding can be computed\nin less than quadratic time."}, "authors": ["Alexandr Andoni", "Krzysztof Onak"], "author_detail": {"name": "Krzysztof Onak"}, "author": "Krzysztof Onak", "arxiv_comment": "Preliminary version appeared in STOC 2009", "links": [{"href": "http://arxiv.org/abs/1109.5635v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1109.5635v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1109.5635v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1109.5635v1", "journal_reference": null, "doi": null, "fulltext": "Approximating Edit Distance in Near-Linear Time\u2217\nAlexandr Andoni\u2020\nMicrosoft Research SVC\n\nKrzysztof Onak\u2021\nCarnegie Mellon University\n\narXiv:1109.5635v1 [cs.DS] 26 Sep 2011\n\nSeptember 27, 2011\n\nAbstract\nWe\u221ashow how to compute the edit distance between two strings of length n up to a factor\nof 2\u00d5( log n) in n1+o(1) time. This is the first sub-polynomial approximation algorithm for this\nproblem that runs in near-linear time,\nimproving on the state-of-the-art n1/3+o(1) approximation.\n\u221a\nPreviously, approximation of 2\u00d5( log n) was known only for embedding edit distance into l1 , and\nit is not known if that embedding can be computed in less than quadratic time.\n\n1\n\nIntroduction\n\nThe edit distance (or Levenshtein distance) between two strings is the number of insertions, deletions, and substitutions needed to transform one string into the other [Lev65]. This distance\nis of fundamental importance in several fields such as computational biology and text processing/searching, and consequently, problems involving edit distance were studied extensively (see [Nav01],\n[Gus97], and references therein). In computational biology, for instance, edit distance and its slight\nvariants are the most elementary measures of dissimilarity for genomic data, and thus improvements\non edit distance algorithms have the potential of major impact.\nThe basic problem is to compute the edit distance between two strings of length n over some\nalphabet. The text-book dynamic programming runs in O(n2 ) time (see [CLRS01] and references\ntherein). This was only slightly improved by Masek and Paterson [MP80] to O(n2 / log2 n) time for\nconstant-size alphabets1 . Their result from 1980 remains the best algorithm to this date.\nSince near-quadratic time is too costly when working on large datasets, practitioners tend to\nrely on faster heuristics (see [Gus97], [Nav01]). This leads to the question of finding fast algorithms\nwith provable guarantees, specifically: can one approximate the edit distance between two strings\nin near-linear time [Ind01, BEK+ 03, BJKK04, BES06, CPSV00, Cor03, OR07, KN06, KR06] ?\n\u2217\nA preliminary version of this paper appeared in Proceedings of the 41st Annual ACM Symposium on Theory of\nComputing (STOC 2009), Bethesda, MD, USA, 2009, pp. 199\u2013204.\n\u2020\nThis work was done when the author was at Massachusetts Institute of Technology, while supported in part by\nDavid and Lucille Packard Fellowship and by MADALGO (Center for Massive Data Algorithmics, funded by the\nDanish National Research Association) and by NSF grant CCF-0728645.\n\u2021\nSupported in part by a Symantec research fellowship, NSF grant 0728645, and NSF grant 0732334. This work\nwas done when the author was a graduate student at Massachusetts Institute of Technology.\n1\nThe result has been only recently extended to arbitrarily large alphabets by Bille and Farach-Colton [BFC08]\nwith a O(log log n)2 factor loss in time.\n\n1\n\n\f\u221a\nPrior results on approximate algorithms2 . A linear-time n-approximation algorithm immediately follows from the O(n + d2 )-time exact algorithm (see Landau, Myers, and Schmidt\n[LMS98]), where d is the edit distance between the input strings. Subsequent research improved\nthe approximation first to n3/7 , and then to n1/3+o(1) , due to, respectively, Bar-Yossef, Jayram,\nKrauthgamer, and Kumar [BJKK04], and Batu, Erg\u00fcn, and Sahinalp [BES06].\nA sublinear time algorithm was obtained by Batu, Erg\u00fcn, Kilian, Magen, Raskhodnikova, Rubinfeld, and Sami [BEK+ 03]. Their algorithm distinguishes the cases when the distance is O(n1\u2212\u01eb )\nvs. \u03a9(n) in \u00d5(n1\u22122\u01eb + n(1\u2212\u01eb)/2 ) time3 for any \u01eb > 0. Note that their algorithm cannot distinguish\ndistances, say, O(n0.1 ) vs. \u03a9(n0.9 ).\nOn a related front, in 2005, the breakthrough\nresult of Ostrovsky and Rabani gave an embedding\n\u221a\nof the edit distance metric into l1 with 2\u00d5( log n) distortion [OR07] (see preliminaries for definitions).\nThis result vastly improved related applications, namely nearest neighbor search and sketching.\nHowever, it did not have implications for computing edit distance between two strings in subquadratic time. In particular, to the best of our knowledge it is not known whether it is possible\nto compute their embedding in less than quadratic time.\nThe best approximation to this date remains the 2006 result of Batu, Erg\u00fcn, and Sahinalp [BES06],\nachieving n1/3+o(1) approximation. Even for n2\u2212\u01eb time, their approximation is n\u01eb/3+o(1) .\n\u221a\n\nOur result. We obtain 2\u00d5( log n) approximation in near-linear time. This is the first subpolynomial approximation algorithm for computing the edit distance between two strings running\nin strongly subquadratic time.\nTheorem\n1.1. The edit distance\nbetween two strings x, y \u2208 {0, 1}n can be computed up to a factor\n\u221a\n\u221a\nlog\nn\nlog\nlog\nn)\nO(\nlog\nn\nlog\nlog n) time.\nO(\nin n * 2\nof 2\nOur result immediately extends to two more related applications. The first application is to\nsublinear-time algorithms. In this scenario, the goal is to compute the distance between two strings\nx, y of the same length n in o(n) time. For this problem, for any \u03b1 < \u03b2 \u2264 1, we can distinguish\ndistance O(n\u03b1 ) from distance \u03a9(n\u03b2 ) in O(n\u03b1+2(1\u2212\u03b2)+o(1) ) time.\nThe second application is to the problem of pattern matching with errors. In this application,\none is given a text T of length N and a pattern P of length n, and the goal is to report the\nsubstring of T that minimizes the edit distance\nto P . Our result\n\u221a immediately gives an algorithm\n\u221a\nfor this problem running in O(N log N )*2\u00d5( log n) time with 2\u00d5( log n) approximation. We note that\nthe best exact algorithm for this problem runs in time O(N n/ log2 n) [MP80]. Better algorithms\nmay be obtained if we restrict the minimal distance between the pattern and best substring of\nT or for relatives of the edit distance. In particular, Sahinalp and Vishkin [SV96] and Cole and\nHariharan [CH02] showed linear-time algorithms for finding all substrings at distance at most nc ,\nwhere c is a constant in (0, 1). Moreover, Cormode and Muthukrishnan gave a near-linear time\n\u00d5(log n)-approximation algorithm when the distance is the edit distance with moves.\n\n1.1\n\nPreliminaries and Notation\n\nBefore describing our general approach and the techniques used, we first introduce a few definitions.\n2\nWe make no attempt at presenting a complete list of results for restricted problems, such as average case edit\ndistance, weakly-repetitive strings, bounded distance regime, or related problems, such as pattern matching/nearest\nneighbor, sketching. However, for a very thorough survey, if only slightly outdated, see [Nav01].\n3\nWe use \u00d5(f (n)) to denote f (n) * logO(1) f (n).\n\n2\n\n\fWe write ed(x, y) to denote the edit distance between strings x and y. We use the notation\n[n] = {1, 2, 3, . . . n}. For a string x, a substring starting at i, of length m, is denoted x[i : i + m \u2212 1].\nWhenever we say with high probability (w.h.p.) throughout the paper, we mean \"with probability\n1 \u2212 1/p(n)\", where p(n) is a sufficiently large polynomial function of the input size n.\nEmbeddings. For a metric (M, dM ), and another metric (X, \u03c1), an embedding is a map \u03c6 : M \u2192\nX such that, for all x, y \u2208 M , we have dM (x, y) \u2264 \u03c1(\u03c6(x), \u03c6(y)) \u2264 \u03b3 * dM (x, y) where \u03b3 \u2265 1 is the\ndistortion of the embedding. In particular, all embeddings in this paper are non-contracting.\nWe say embedding \u03c6 is oblivious if for any subset S \u2282 M of size n, the distortion guarantee\nholds for all pairs x, y \u2208 S with high probability. The embedding \u03c6 is non-oblivious if it holds for\na specific set S (i.e., \u03c6 is allowed to depend on S).\nMetrics.\nset of points living in Rk under the distance kx \u2212\nPk The k-dimensional l1 metric is the\nk\nyk1 = i=1 |xi \u2212 yi |. We also denote it by l1 .\nWe define thresholded Earth-Mover Distance, denoted TEMDt for a fixed threshold t > 0, as\nthe following distance on subsets A and B of size s \u2208 N of some metric (M, dM ):\nX\n\b\nmin dM (a, \u03c4 (a)), t\n(1)\nTEMDt (A, B) = 1s min\n\u03c4 :A\u2192B\n\na\u2208A\n\nwhere \u03c4 ranges over all bijections between sets A and B. TEMD\u221e is the simple Earth-Mover\nDistance (EMD). We will always use t = s and thus drop the subscript t; i.e., TEMD = TEMDs .\nA graph (tree) metric is a metric induced by a connected weighted graph (tree) G, where the\ndistance between two vertices is the length of the shortest path between them. We denote an\narbitrary tree metric by TM.\nSemimetric spaces. We define a semimetric to be a pair (M, dM ) that satisfies all the properties\nof a metric space except the triangle inequality. A \u03b3-near metric is a semimetric (M, dM ) such that\nthere exists some metric (M, d\u2217M ) (satisfying the triangle inequality) with the property that, for\nany x, y \u2208 M , we have that d\u2217M (x, y) \u2264 dM (x, y) \u2264 \u03b3 * d\u2217M (x, y).\nLk\nProduct spaces. A sum-product over a metric M = (M, dM ), denoted\nl1 M, is a derived\nk\nmetric over the set M , where the distance between two points x = (x1 , . . . xk ) and y = (y1 , . . . yk )\nis equal to\nX\nd1,M (x, y) =\ndM (xi , yi ).\ni\u2208[k]\n\nLk\n\nFor example the space l1 R is just the k-dimensional l1 .\nLk\nk\nAnalogously, a min-product over M = (M, dM ), denoted\nmin M, is a semimetric over M ,\nwhere the distance between two points x = (x1 , . . . xk ) and y = (y1 , . . . yk ) is\n\b\ndmin,M (x, y) = min dM (xi , yi ) .\ni\u2208[k]\n\nWe also slightly abuse the notation by writing\nmetrics (that could differ from each other).\n3\n\nLk\n\nmin TM\n\nto denote the min-product of k tree\n\n\f1.2\n\nTechniques\n\nOur starting point is the Ostrovsky-Rabani embedding [OR07]. For strings x, y, as well as for all\nsubstrings \u03c3 of specific lengths, we compute some vectors v\u03c3 living in low-dimensional l1 such that\nthe distance between two such vectors approximates the edit distance between the associated (sub)strings. In this respect, these vectors can be seen as an embedding of the considered strings into\nl1 of polylogarithmic dimension. Unlike the Ostrovsky-Rabani embedding, however, our embedding\nis non-oblivious in the sense that the vectors v\u03c3 are computed given all the relevant strings \u03c3. In\ncontrast, Ostrovsky and Rabani give an oblivious embedding \u03c6n : {0, 1}n \u2192 l1 such that k\u03c6n (x) \u2212\n\u03c6n (y)k1 approximates ed(x, y). However, the obliviousness comes at a high price: their embedding\nrequires a high dimension, of order \u03a9(n), and a high computation time, of order \u03a9(n2 ) (even\nwhen allowing randomized embedding, and a constant probability of a correctness). We further\nnote that reducing the dimension of this embedding seems unlikely as suggested by the results on\nimpossibility of dimensionality reduction within l1 [CS02, BC03, LN04]. Nevertheless, the general\nrecursive approach of the Ostrovsky-Rabani embedding is the starting point of the algorithm from\nthis paper.\nThe heart of our algorithm is a near-linear time algorithm that, given a sequence of lowO(log2 n)\ndimensional vectors v1 , . . . vn \u2208 l1 and an integer s < n, constructs new vectors q1 , . . . qm \u2208 l1\n,\nwhere m = n \u2212 s + 1, with the following property. For all i, j \u2208 [m], the value kqi \u2212 qj k1 approximates the Earth-Mover Distance (EMD)4 between the sets Ai = {vi , vi+1 , . . . vi+s\u22121 } and\nAj = {vj , vj+1 , . . . vj+s\u22121 }. To accomplish this (non-oblivious) embedding, we proceed in two\nstages. First, we embed (obliviously) the EMD metric into a min-product of l1 's of low dimension.\nIn other words, for a set A, we associate a matrix L(A), ofP\npolylogarithmic size, such that the EMD\ndistance between sets A and B is approximated by minr t |L(A)rt \u2212 L(B)rt |. Min-products help\nus simultaneously on two fronts: one is that we can apply a weak dimensionality reduction in l1 ,\nusing the Cauchy projections, and the second one enables us to accomplish a low-dimensional EMD\nembedding itself. Our embedding L(*) is not only low-dimensional, but it is also linear, allowing us\nto compute matrices L(Ai ) in near-linear time by performing\nP one pass over the sequence v1 , . . . vn .\nLinearity is crucial here as even the total size of Ai 's is i |Ai | = (n \u2212 s + 1) * s, which can be as\nhigh as \u03a9(n2 ), and so processing each Ai separately is infeasible.\nIn the second stage, we show how to embed a set of n points lying in a low-dimensional minproduct of l1 's back into a low-dimensional l1 with only small distortion. We note that this is\nnot possible in general, with any bounded distortion, because such a set of points does not even\nform a metric. We show that this is possible when we assume that the semi-metric induced by\nthe set of points approximates some metric (in our case, the set of points approximates the initial\nEMD metric). The embedding from this stage starts by embedding a min-product of l1 's into a\nlow-dimensional min-product of tree metrics. We further embed the latter into an n-point metric\nsupported by the shortest-path metric of a sparse graph. Finally, we observe that we can implement\nBourgain's embedding on a sparse graph metric in near-linear time. These last two steps make our\nembedding non-oblivious.\n\n1.3\n\nRecent Work\n\nWe note that the recent work [AKO10] has shown that one can approximate the edit distance\nbetween two strings up to a multiplicative factor of (log n)O(1/\u01eb) in n1+\u01eb time, for any desired\n4\n\nIn fact, our algorithm does this for thresholded EMD, TEMD, but the technique is precisely the same.\n\n4\n\n\f\u01eb > 0. Although the new result obtains polylogarithmic approximation, the running time is slightly\nhigher\nthan the algorithm presented here. For a comparable approximation, obtained for \u01eb =\np\nlog log n/ log n, the algorithm of [AKO10] does not improve the running time (up to constants\nhidden by the big O notation). We further remark that the techniques of [AKO10] are disjoint\nfrom the techniques presented here, and are based on asymmetric sampling of one of the strings.\n\n2\n\nShort Overview of the Ostrovsky-Rabani Embedding\n\nWe now briefly describe the embedding of Ostrovsky and Rabani [OR07]. Some notions introduced\nhere are used in our algorithm described in the next section.\nThe embedding of Ostrovsky and Rabani is recursive. For a fixed n, they construct the\nembedding of edit distance over \u221a\nstrings of length n using the embedding of edit distance over\nstrings of shorter lengths l \u2264 n/2 log n log log n . We denote their embedding of length-n strings by\n\u03c6n : {0, 1}n \u2192 l1 , and let dOR\nbe the resulting distance: dOR\nn\nn (x, y) = k\u03c6n (x) \u2212 \u03c6n (y)k1 . For\nn\nOR\ntwo strings x, y \u2208 {0, 1} , the embedding is such that dn = k\u03c6n (x) \u2212 \u03c6n (y)k1 approximates an\n\"idealized\" distance d\u2217n (x, y), which itself approximates the edit distance between x and y.\nBefore\ndescribing the \"idealized\" distance d\u2217n , we introduce some notation. Partition x into\n\u221a\nlog\nn\nlog\nlog n blocks called x(1) , . . . x(b) of length l = n/b. Next, fix some j \u2208 [b] and s \u2264 l. We\nb=2\nconsider the set of all substrings of x(j) of length l \u2212 s + 1, embed each one recursively via \u03c6l\u2212s+1 ,\nand define Sjs (x) \u2282 l1 to be the set of resulting vectors (note that |Sjs | = s). Formally,\n\b\nSjs (x) = \u03c6l\u2212s+1 (x[(j \u2212 1)l + z : (j \u2212 1)l + z + l \u2212 s]) | z \u2208 [s] .\nTaking \u03c6l\u2212s+1 as given (and thus also the sets Sjs (x) for all x), define the new \"idealized\" distance\nd\u2217n approximating the edit distance between strings x, y \u2208 {0, 1}n as\nd\u2217n (x, y)\n\n=c\n\nb\nX\nX\nj=1\n\nTEMD(Sjs (x), Sjs (y))\n\n(2)\n\nf \u2208N\ns=2f \u2264l\n\nwhere TEMD is the thresholded Earth-Mover Distance (defined in Equation (1)), and c is a sufficiently large normalization constant (c \u2265 12 suffices). Using the terminology from the preliminaries,\nthe distance function d\u2217n can be viewed as the distance function of the sum-product of TEMDs,\nLb LO(log n)\nTEMD, and the embedding into this product space is attained by the natural\ni.e.,\nl1\nl1\nidentity map (on sets Sjs ).\nThe key idea is that the distance d\u2217n (x, y) approximates edit distance well, assuming that \u03c6l\u2212s+1\napproximates edit distance well, for all s = 2f where f \u2208 {1, 2, . . . \u230alog2 l\u230b}. Formally, Ostrovsky\nand Rabani show that:\nFact 2.1 ([OR07]). Fix n and b < n, and let l = n/b. Let Dn/b be an upper bound on distortion of\n\u03c6l\u2212s+1 viewed as an embedding of edit distance on strings {x[i : i+l\u2212s], y[i : i+l\u2212s] | i \u2208 [n\u2212l+s]},\nfor all s = 2f where f \u2208 {1, 2, . . . \u230alog2 l\u230b}. Then,\n\u0001\ned(x, y) \u2264 d\u2217n (x, y) \u2264 ed(x, y) * Dn/b + b * O(log n).\n\nTo obtain a complete embedding, it remains to construct an embedding approximating d\u2217n up to\na small factor. In fact, if one manages to approximate d\u2217n up to a poly-logarithmic factor, then the\n5\n\n\f\u221a\n\nfinal distortion comes out to be 2O( log n log log n) . This follows from the following recurrence on the\ndistortion factor Dn . Suppose \u03c6n is an embedding that approximates d\u2217n up to a factor logO(1) n.\nThen, if Dn is the distortion\nof \u03c6n (as an embedding of edit distance), then Fact 2.1 immediately\n\u221a\nimplies that, for b = 2 log n log log n ,\n\u221a\n\nDn \u2264 Dn/2\u221alog n log log n * logO(1) n + 2O(\n\nlog n log log n)\n\n.\n\n\u221a\n\nThis recurrence solves to Dn \u2264 2O( log n log log n) as proven in [OR07].\nConcluding, to complete a step of the recursion, it is sufficient to embed the metric given\nby d\u2217n into l1 with a polylogarithmic distortion. Recall that d\u2217n is the distance of the metric\nLb LO(log n)\nTEMD, and thus, one just needs to embed TEMD into l1 . Indeed, Ostrovsky and\nl1\nl1\nRabani show how to embed a relaxed (but sufficient) version of TEMD into l1 with O(log n)\ndistortion, yielding the desired embedding \u03c6n , which approximates d\u2217n up to a O(log n) factor at\neach level of recursion. We note that the required dimension is \u00d5(n).\n\n3\n\nProof of the Main Theorem\n\nWe now describe our general approach. Fix x \u2208 {0, 1}n . For each substring \u03c3 of x, we construct a\nlow-dimensional vector v\u03c3 such that, for any two substrings \u03c3, \u03c4 of the same length, the edit distance\nbetween \u03c3 and \u03c4 is approximated by the l1 distance between the vectors v\u03c3 and v\u03c4 . We note that\nthe embedding is non-oblivious: to construct vectors v\u03c3 we need to know all the substrings of x\nin advance (akin to Bourgain's embedding guarantee). We also note that computing such vectors\nis enough to solve the problem of approximating the edit distance between two strings, x and y.\nSpecifically, we apply this procedure to the string x\u2032 = x \u25e6 y, the concatenation of x and y, and\nthen compute the l1 distance between the vectors corresponding to x and y, substrings of x\u2032 .\nMore precisely, for each length m \u2208 W , for some set W \u2282 [n] specified later, and for each\n(m)\nsubstring x[i : i + m \u2212 1], where i = 1, . . . n \u2212 m + 1, we compute a vector vi\nin l\u03b11 , where\n\u221a\n(m)\n(l)\n\u03b1 = 2\u00d5( log n) . The construction is inductive: to compute vectors vi , we use vectors vi for\nl \u226a m and l \u2208 W . The general approach of our construction is based on the analysis of the\n(m)\nrecursive step of Ostrovsky and Rabani, described in Section 2. In particular, our vectors vi \u2208 l1\nwill also approximate the d\u2217m distance (given in Equation (2)) with sets Sis defined using vectors\n(l)\nvi with l \u226a m.\n(m)\nThe main challenge is to process one level (vectors vi for a fixed m) in near-linear time. Besides\nthe computation time itself, a fundamental difficulty in applying the approach of Ostrovsky and\nRabani directly is that their embedding would give a much higher dimension \u03b1, proportional to\n\u00d5(m). Thus, if we were to use their embedding, even storing all the vectors would take quadratic\nspace.\nTo overcome this last difficulty, we settle on non-obliviously embedding the set of substrings\nO(1)\nn distortion (formally,\nx[i : i + m \u2212 1] for i \u2208 [n \u2212 m + 1] under the \"ideal\" distance d\u2217m with log\no\nn\n(l\u2212s+1)\n\u2217\ns\nunder the distance dm from Equation (2), when Sj (x[i : i + m \u2212 1]) = vi+(j\u22121)l+z\u22121 | z \u2208 [s] for\n\u221a\n\n(m)\n\n2\n\nl = m/2 log n log log n ). Existentially, we know that there exist vectors wi \u2208 RO(log n) such that\n(m)\n(m)\nkwi \u2212 wj k1 approximates d\u2217m (x[i : i + m \u2212 1], x[j : j + m \u2212 1]) for all i and j - this follows by\n(m)\n\nthe standard Bourgain's embedding [Bou85]. The vectors vi\n6\n\nthat we compute approximate the\n\n\f(m)\n\nproperties of the ideal vectors wi . Their efficient computability comes at the cost of an additional\npolylogarithmic loss in approximation.\nThe main building block is the following theorem. It shows how to approximate the TEMD\ndistance for the desired sets Sjs .\nTheorem 3.1. Let n \u2208 N and s \u2208 [n]. Let v1 , . . . vn be vectors in {\u2212M, . . . M }\u03b1 , where M = nO(1)\nand \u03b1 \u2264 n. Define sets Ai = {vi , vi+1 , . . . vi+s\u22121 } for i \u2208 [n \u2212 s + 1].\nLet t = O(log2 n). We can compute (randomized) vectors qi \u2208 lt1 for i \u2208 [n \u2212 s + 1] such that\nfor any i, j \u2208 [n \u2212 s + 1], with high probability, we have\nTEMD(Ai , Aj ) \u2264 kqi \u2212 qj k1 \u2264 TEMD(Ai , Aj ) * logO(1) n.\nFurthermore, computing all vectors qi takes \u00d5(n\u03b1) time.\nTo map the statement of this theorem to the\u0010 above description,\nwe mention that, for each l =\n\u0011\n(l\u2212s+1)\nfor each s = 1, 2, 4, 8, . . . 2\u230alog2 l\u230b .\nm/b for m \u2208 W , we apply the theorem to vectors vi\ni\u2208[n\u2212l+s]\n\nWe prove Theorem 3.1 in later sections. Once we have Theorem 3.1, it becomes relatively\nstraight-forward (albeit a bit technical) to prove the main theorem, Theorem 1.1. We complete the\nproof of Theorem 1.1 next, assuming Theorem 3.1.\n\nof Theorem 1.1. We\nstart by appending y to the end of x; we will work with the new version of\n\u221a\nlog\nn log log n and \u03b1 = O(b log 3 n). We construct vectors v (m) \u2208 R\u03b1 for m \u2208 W ,\nx only. Let b = 2\ni\n\u221a\nwhere W \u2282 [n] is a carefully chosen set of size 2O( log n log log n) . Namely, W is the minimal set\nsuch that: n \u2208 W , and, for each i \u2208 W with i \u2265 b, we have that i/b \u2212\u221a2j + 1 \u2208 W for all integers\nj \u2264 \u230alog2 i/b\u230b. It is easy to show by induction that the size of W is 2O( log n log log n) . We construct\n(m)\nthe vectors vi inductively in a bottom-up manner. We use vectors for small m to build vectors\nfor large m. W is exactly the set of lengths\u221am that we need in the process.\n(m)\nFix an m \u2208 W such that m \u2264 b2 = 22 log n log log n . We define the vector vi to be equal to\nhm (x[i : i + m \u2212 1]), where hm : {0, 1}m \u2192 {0, 1}\u03b1 is a randomly chosen function. It is readily seen\n\u221a\n(m)\n(m)\nthat kvi \u2212 vj k1 approximates ed(x[i : i + m \u2212 1], x[j : j + m \u2212 1]) up to b2 = 22 log n log log n\napproximation factor, for each i, j \u2208 [n \u2212 m + 1].\nNow consider m \u2208 Wnsuch that m > b2 . Let l =\no m/b. First we construct vectors approximating\n(l\u2212s+1)\nm,s\nTEMD on sets Ai = vi+z\n| z = 0, . . . s \u2212 1 , where s = 1, 2, 4, 8, . . . , l and i \u2208 [n \u2212 l + s].\nIn\nfor a fixed s \u2208 [l] equal\u0010to a power\nof 2, we apply Theorem 3.1 to the set of vectors\n\u0011\n\u0011\n\u0010 particular,\n(m,s)\n(l\u2212s+1)\n. Theorem 3.1 guarantees that, for each i, j \u2208\nobtaining vectors qi\nvi\ni\u2208[n\u2212l+1]\n(m,s)\n(m,s)\nm,s\nO(1)\n[n\u2212l+1], the value kqi\n\u2212qj\nk1 approximates TEMD(Am,s\nn. We\ni , Aj ) up to a factor of log\n(m,s)\n(m)\n\u03b1\ncan then use these vectors qi\nto obtain the vectors vi \u2208 R that approximate the \"idealized\"\n(m)\n\u2217\ndistance dm on substrings x[i : i + m \u2212 1], for i \u2208 [n \u2212 m + 1]. Specifically, we let the vector vi\n(m,s)\nbe a concatenation of vectors qi+(j\u22121)l , where j \u2208 [b], and s goes over all powers of 2 less than l:\ni\u2208[n\u2212l+s]\n\n(m)\n\nvi\n\n\u0011\n\u0010\n(m,s)\n= qi+(j\u22121)l\n\nj\u2208[b]\n\ns=2f \u2264l,f \u2208N\n\n7\n\n.\n\n\f(m)\n\nThen, the vectors vi\napproximate the distance d\u2217m (given in Equation (2)) up to a logO(1) n\napproximation factor, with the sets Sjs (x[i : i + m \u2212 1]) taken as\no\nn\n(l\u2212s+1)\n|\nz\n=\n0,\n.\n.\n.\ns\n\u2212\n1\n,\n=\nv\nSjs (x[i : i + m \u2212 1]) = Am,s\ni+(j\u22121)l+z\ni+(j\u22121)l\n\nfor i \u2208 [n \u2212 m + 1] and j \u2208 [b].\n(n)\n(n)\nThe algorithm finishes by outputting kv1 \u2212 vn+1 k, which is an approximation to the edit\nO(1) *logO(1) n) =\ndistance\n\u221a between x[1 : n] and x[n+1 : 2n] = y. The total running time is O(|W |*n*b\nn * 2O( log n log log n) .\nIt remains to analyze the resulting approximation. Let Dm be the approximation achieved by\n(k)\nvectors vi \u2208 l1 for substrings of x of lengths k, where k \u2208 W and k \u2264 m. Then, using Fact 2.1\n(m)\nand the fact that vectors vi \u2208 l1 approximate d\u2217m , we have that\n\u0011\n\u0010\n\u221a\nDm \u2264 logO(1) n * Dm/b + 2 log n log log n .\nSince the total number of recursion levels is bounded by logb n =\nDn =\n\n\u221a\n2O( log n log log n) .\n\n3.1\n\nProof of Theorem 3.1\n\nq\n\nlog n\nlog log n ,\n\nwe deduce that\n\nThe proof proceeds in two stages. In the first stage we show an embedding of the TEMD metric\ninto a low-dimensional space. Specifically, we show an (oblivious)\nembedding of TEMD into a\nLl\na semi-metric where\nmin-product of l1 . Recall that the min-product of l1 , denoted min lk1 , is\nnP\no the\nl\u00d7k\ndistance between two l-by-k vectors x, y \u2208 R\nis dmin,1 (x, y) = mini\u2208[l]\nj\u2208[k] |xi,j \u2212 yi,j | . Our\nmin-product of l1 's has dimensions l = O(log n) and k = O(log3 n). The min-product can be seen\nas helping us on two fronts: one is the embedding of TEMD into l1 (of initially high-dimension),\nand another is a weak dimensionality reduction in l1 , using Cauchy projections. Both of these\nembeddings are of the following form: consider a randomized embedding f into (standard) l1 that\nhas no contraction (w.h.p.) but the expansion is bounded only in the expectation (as opposed to\nw.h.p.). To obtain a \"w.h.p.\" expansion, one standard approach is to sample f many times and\nconcentrate the expectation. This approach, however, will necessitate a high number of samples of\nf , and thus yield a high final dimension. Instead, the min-product allows us to take only O(log n)\nindependent samples of f .\nWe note\nP that our embedding of TEMD into min-product of l1 , denoted \u03bb, is linear in the sets A:\n\u03bb(A) = a\u2208A \u03bb({a}). The linearity allows us to compute the embedding of sets Ai in a streaming\nfashion: the embedding of Ai+1 is obtained from the embedding of Ai with logO(1) n additional\nprocessing. This stage appears in Section 3.1.1.\nIn the second stage, we show that, given a set of n points in min-product of l1 's, we can (nonobliviously) embed these points into low-dimensional l1 with O(log n) distortion. The time required\nis near-linear in n and the dimensions of the min-product of l1 's.\nTo accomplish this step, we start by embedding the min-product of l1 's into a min-product of\ntree metrics. Next, we show that n points in the low-dimensional min-product of tree metrics can\nbe embedded into a graph metric supported by a sparse graph. We note that this is in general\nnot possible, with any (even non-constant) distortion. We show that this is possible when we\n8\n\n\fassume that our subset of the min-product of tree metrics approximates some actual metric (in\nour case, the min-product approximates the TEMD metric). Finally, we observe that we can\nimplement Bourgain's embedding in near-linear time on a sparse graph metric. This stage appears\nin Section 3.1.2.\nWe conclude with the proof of Theorem 3.1 in Section 3.1.3.\n3.1.1\n\nEmbedding EMD into min-product of l1\n\nIn the next lemma, we show how to embed TEMD into a min-product of l1 's of low dimension.\nMoreover, when the sets Ai are obtained from a sequence of vectors v1 , . . . vn , by taking Ai =\n{vi , . . . vi+s\u22121 }, we can compute the embedding in near-linear time.\nLemma 3.2. Fix n, M \u2208 N and s \u2208 [n]. Suppose we have n vectors v1 , . . . vn in {\u2212M, \u2212M +\n1, . . . , M }\u03b1 for some \u03b1 \u2264 n. Consider the sets Ai = {vi , vi+1 , . . . vi+s\u22121 }, for i \u2208 [n \u2212 s + 1].\nLet k = O(log3 n). We can compute (randomized) vectors qi \u2208 lk1 for i \u2208 [n \u2212 s + 1] such that,\nfor any i, j \u2208 [n \u2212 s + 1] we have that\nh\ni\n\u2022 Pr kqi \u2212 qj k1 \u2264 TEMD(Ai , Aj ) * O(log2 n) \u2265 0.1 and\n\u2022 kqi \u2212 qj k1 \u2265 TEMD(Ai , Aj ) w.h.p.\nThe computation time is \u00d5(n\u03b1).\nLl\nk\nThus, we can embed the TEMD metric over sets Ai into\nmin l1 , for l = O(log n), such that\n2\nthe distortion is O(log n) w.h.p. The computation time is \u00d5(n\u03b1).\nProof. First, we show how to embed TEMD metric over the sets Ai into l1 of dimension M O(\u03b1) *\nO(log n). For this purpose, we use a slight modification of the embedding of [AIK08] (it can also\nbe seen as a strengthening of the TEMD embedding of Ostrovsky and Rabani).\nThe embedding of [AIK08] constructs m = O(log s) embeddings \u03c8i , each of dimension h =\nM O(\u03b1) , and then the final embedding is just the concatenation \u03c8 = \u03c81 \u25e6\u03c82 . . .\u25e6\u03c8m . For i = 1, . . . m,\nwe impose a randomly shifted grid of side-length Ri = 2i\u22122 . That is, let \u2206i = (\u03b4i,1 , . . . , \u03b4i,\u03b1 ) be\nselected uniformly at random from [0, 1)\u03b1 . A specific vector vj falls into the cell (c1 , . . . , c\u03b1 ), where\nct = \u230avj,t /Ri + \u03b4i,t \u230b for t = 1, . . . , \u03b1. Then \u03c8i has a coordinate for each cell (c1 , . . . , c\u03b1 ), where\n0 \u2264 ct \u2264 2M/Ri + 1 for t = 1, . . . , \u03b1. These are the only cells that can be non-empty, and there\nis at most (2M/Ri + 1)\u03b1 = M O(\u03b1) of them. The value of a specific coordinate, for a set A, equals\nthe number of vectors from A falling into the corresponding cell times Ri . Now, if we scale \u03c8 up\nby a factor of \u0398( 1s log n), Theorem 3.1 from [AIK08]5 says that the vectors qi\u2032 = \u03c8(Ai ) satisfy the\ncondition that, for any i, j \u2208 [n \u2212 s + 1], we have:\ni\nh\n\u2022 E kqi\u2032 \u2212 qj\u2032 k1 \u2264 TEMD(Ai , Aj ) * O(log2 n) and\n\u2022 kqi\u2032 \u2212 qj\u2032 k1 \u2265 TEMD(Ai , Aj ) w.h.p.\nThus, the vectors qi\u2032 satisfy the promised properties except they have a high dimension.\nTo reduce the dimension of qi\u2032 's, we apply a weak l1 dimensionality reduction via 1-stable\n(Cauchy) projections. Namely, we pick a random matrix P of size k = O(log3 n) by mh = O(log s) *\n5\n\nNote that Theorem 3.1 from [AIK08] is stated for EMD, and here we are concerned with TEMD. Nevertheless,\nthe whole statement still applies, because the side of the largest grid is bounded by O(s) .\n\n9\n\n\fM O(\u03b1) , the dimension of \u03c8, where each entry is distributed according to the Cauchy distribution,\n1\n\u2032\nk\nwhich has probability distribution function f (x) = \u03c01 * 1+x\n2 . Now define qi = P * qi \u2208 l1 . Standard\nproperties of the l1 dimensionality reduction guarantee that the vectors qi satisfy the properties\npromised in the lemma statement, after an appropriate rescaling (see Theorem 5 of [Ind06] with\n\u01eb = 1/2, \u03b3 = 1/6, and \u03b4 = n\u2212O(1) ).\nIt remains to show that we can compute the vectors qi in \u00d5(n\u03b1)Ptime. To this end, observe\nthat the resulting embedding P * \u03c8(A) is linear, namely P * \u03c8(A) = a\u2208A P * \u03c8({a}). Moreover,\neach P * \u03c8({vi }) can be computed in \u03b1 * logO(1) n time, because \u03c8({vi }) has exactly one non-zero\ncoordinate, which can be computed in O(\u03b1) time, and then P * \u03c8({vi }) is simply the corresponding\ncolumn of P multiplied by the non-empty coordinate of \u03c8({vi }). To obtain the first vector q1 , we\ncompute the summation of all corresponding P * \u03c8({vi }). To compute the remaining vectors qi\niteratively, we use the idea of a sliding window over the sequence v1 , . . . vn . Specifically, we have\nqi+1 = P * \u03c8(Ai+1 ) = P * \u03c8(Ai \u222a {vi+s } \\ {vi }) = qi + P * \u03c8({vi+s }) \u2212 P * \u03c8({vi }),\nwhich implies that qi+1 can be computed in \u03b1 * logO(1) n time, given the value of qi . Therefore, the\ntotal time required to compute all qi 's is O(n\u03b1 * logO(1) n).\nFinally, we show how we obtain an efficient embedding of TEMD into min-product of l1 's. We\n(z)\napply the above procedure l = O(log n) times. Let qi be the resulting vectors, for i \u2208 [n \u2212 s + 1]\n(z)\nand z \u2208 [l]. The embedding of a set Ai is the concatenation of the vectors qi , namely Qi =\nL\n(1) (2)\n(l)\n(qi , qi , . . . qi ) \u2208 lmin lk1 . The Chernoff bound implies that w.h.p., for any i, j \u2208 [n \u2212 s + 1], we\nhave that\n(z)\n(z)\ndmin,1 (Qi , Qj ) = min kqi \u2212 qj k \u2264 TEMDs (Ai , Aj ) * O(log2 n).\nz\u2208[l]\n\nAlso, dmin,1 (Qi , Qj ) \u2265 TEMDs (Ai , Aj ) w.h.p. trivially. Thus the vectors Qi are an embedding of\nL\nthe TEMD metric on Ai 's into lmin lk1 with distortion O(log2 n) w.h.p.\n3.1.2\n\nEmbedding of min-product of l1 into low-dimensional l1\n\nL\nIn this section, we show that n points Q1 , . . . Qn in the semi-metric space lmin lk1 can be embedded\ninto l1 of dimension O(log2 n) with distortion logO(1) n. The embedding works under the assumption\nthat the semi-metric on Q1 , . . . Qn is a logO(1) n approximation of some metric. We start by showing\nthat we can embed a min-product of l1 's into a min-product of tree metrics.\nLl\nk\nLemma 3.3. Fix n, M \u2208 N such that M = nO(1) . Consider n vectors v1 , . . . vn in\nmin l1 , for\nsome l, k \u2208 N, where each coordinate of each vi lies in the set {\u2212M, . . . , M }. We can embed these\nLO(l log2 n)\nTM, incurring distortion\nvectors into a min-product of O(l * log2 n) tree metrics, i.e.,\nmin\nO(log n) w.h.p. The computation time is \u00d5(n * kl).\nProof. We consider all thresholds 2t , for t \u2208 {0, 1, . . . , log M }. For each threshold 2t , and for\neach coordinate of the min-product (i.e., lk1 ), we create O(log n) tree metrics. Each tree metric is\nindependently created as follows. We again use randomly shifted grids. Specifically, we define a\nhash function h : lk1 \u2192 Zk as\n\u0012\u0016\n\u0017 \u0016\n\u0017\n\u0016\n\u0017\u0013\nx 1 + u1\nx 2 + u2\nx k + uk\nh(x1 , . . . , xk ) =\n,\n,\n.\n.\n.\n,\n,\n2t\n2t\n2t\n10\n\n\fwhere each ut is chosen at random from [0, 2t ). We create each tree metric so that the nodes\ncorresponding to the points hashed by h to the same value are at distance 2t (this creates a set of\nstars), and each pair of points that are hashed to different values are at distance 2M k (we connect\nthe roots of the stars).\nFor two points x, y \u2208 lk1 , the probability that they are separated by the grid in the i-th dimension\nis at most |xi \u2212 yi |/2t , which implies by the union bound that\nPr[h(x) = h(y)] \u2265 1 \u2212\nh\n\nX |xi \u2212 yi |\n2t\n\ni\n\n=1\u2212\n\nkx \u2212 yk1\n.\n2t\n\nOn the other hand, the probability that x and y are not separated by the grid in the i-th dimension\nt\nis max{1 \u2212 |xi \u2212 yi |/2t , 0} \u2264 e\u2212|xi \u2212yi |/2 . Since the grid is shifted independently in each dimension,\nPr[h(x) = h(y)] \u2264\nh\n\nk\nY\n\nt\n\ne\u2212|xi \u2212yi |/2 = e\u2212\n\nPk\n\ni=1\n\n|xi \u2212yi |/2t\n\nt\n\n= e\u2212kx\u2212yk1 /2 .\n\ni=1\n\nBy the Chernoff bound, if x, y \u2208 lk1 are at distance at most 2t for some t, they will be at distance\nat most 2t+1 in one of the tree metrics with high probability. On the other hand, let vi and vj be\ntwo input vectors at distance greater than 2t . The probability that they are at distance smaller\nthan 2t /c log n in any of the O(log2 n) tree metrics, is at most n\u2212c+1 for any c > 0, by the union\nbound.\nTherefore, we multiply the weights of all edges in all trees by O(log n) to achieve a proper\n(non-contracting) embedding.\nWe now show that we can embed a subset of the min-product of tree metrics into a graph\nmetric, assuming the subset is close to a metric.\nLl\nLemma 3.4. Consider a semi-metric M = (X, \u03be) of size n in\nmin TM for some l \u2208 N, where\neach tree metric in the product is of size O(n). Suppose M is a \u03b3-near metric (i.e., it is embeddable\ninto a metric with \u03b3 distortion). Then we can embed M into a connected weighted graph with O(nl)\nedges with distortion \u03b3 in O(nl) time.\nProof. We consider l separate trees each on O(n) nodes, corresponding to each of l dimensions of the\nmin-product. We identify the nodes of trees that correspond to the same point in the min-product,\nand collapse them into a single node. The graph we obtain has at most O(nl) edges. Denote\nthe shortest-path metric it spans with M\u2032 = (V, \u03c1), and denote our embedding with \u03c6 : X \u2192 V .\nClearly, for each pair u, v of points in X, we have \u03c1(\u03c6(u), \u03c6(v)) \u2264 \u03be(u, v). If the distance between\ntwo points shrinks after embedding, then there is a sequence of points w0 = u, w1 , . . . , wk\u22121 ,\nwk = v such that \u03c1(\u03c6(u), \u03c6(v)) = \u03be(w0 , w1 ) + \u03be(w1 , w2 ) + * * * + \u03be(wk\u22121 , wk ). Because M is a \u03b3-near\nmetric, there exists a metric \u03be \u22c6 : X \u00d7 X \u2192 [0, \u221e), such that \u03be \u22c6 (x, y) \u2264 \u03be(x, y) \u2264 \u03b3 * \u03be \u22c6 (x, y), for all\nx, y \u2208 X. Therefore,\n\u03c1(\u03c6(u), \u03c6(v)) =\n\nk\u22121\nX\ni=0\n\n\u03be(wi , wi+1 ) \u2265\n\nk\u22121\nX\ni=0\n\n\u03be \u22c6 (wi , wi+1 ) \u2265 \u03be \u22c6 (w0 , wk ) = \u03be \u22c6 (u, v) \u2265 \u03be(u, v)/\u03b3.\n\nHence, it suffices to multiply all edge weights of the graph by \u03b3 to achieve a non-contractive\nembedding. Since there was no expansion before, it is now bounded by \u03b3.\n11\n\n\fWe now show how to embed the shortest-path metric of a graph into a low dimensional l1 -space\nin time near-linear in the graph size. For this purpose, we implement Bourgain's embedding [Bou85]\nin near-linear time. We use the following version of Bourgain's embedding, which follows from the\nanalysis in [Mat02].\nLemma 3.5 (Bourgain's embedding [Mat02]). Let M = (X, \u03c1) be a finite metric on n points.\nThere is an algorithm that computes an embedding f : X \u2192 lt1 of M into lt1 for t = O(log2 n) such\nthat, with high probability, for each u, v \u2208 X, we have \u03c1(u, v) \u2264 kf (u) \u2212 f (v)k1 \u2264 \u03c1(u, v) * O(log n).\nSpecifically, for coordinate i \u2208 [k] of f , the embedding associates a nonempty set Ai \u2286 X such\nthat f (u)i = \u03c1(u, Ai ) = mina\u2208Ai \u03c1(u, a). Each Ai is samplable in linear time.\nThe running time of the algorithm is O(g(n)*log 2 n), where g(n) is the time necessary to compute\nthe distance of all points to a given fixed subset of points.\nLemma 3.6. Consider a connected graph G = (V, E) on n nodes with m edges and a weight\nfunction w : E \u2192 [0, \u221e). There is a randomized algorithm that embeds the shortest path metric of\nO(log2 n)\nG into l1\nwith O(log n) distortion, with high probability, in O(m log3 n) time.\nO(log2 n)\n\nProof. Let \u03c8 : V \u2192 l1\nbe the embedding given by Lemma 3.5. For any nonempty subset\nA \u2286 V , we can compute \u03c1(v, A) for all v \u2208 V by Dijkstra's algorithm in O(m log n) time. The total\nrunning time is thus O(m log3 n).\n3.1.3\n\nFinalization of the proof of Theorem 3.1\nLO(log n) k\nl1 with distortion at most O(log2 n)\nWe first apply Lemma 3.2 to embed the sets Ai into min\nwith high probability, where k = O(log3 n). We write vi , i \u2208 [n \u2212 s + 1], to denote the embedding\nof Ai . Note that the TEMD distance between two different Ai 's is at least 1/s \u2265 1/n, and so is\nthe distance between two different vi 's. We multiply all coordinates of vi 's by 2kn = \u00d5(n) and\nround them to the nearest integer. This way we obtain vectors vi\u2032 with integer coordinates in\n{\u22122knM \u2212 1, . . . , 2knM + 1}. Consider two vectors vi and vj . Let D be their distance, and let\nD \u2032 be the distance between the corresponding vi\u2032 and vj\u2032 . We claim that knD \u2264 D \u2032 \u2264 3knD, and\nit suffices to show this claim for vi 6= vj , in which case we know that D \u2265 1/n. Each coordinate\nof the min-product is lk1 , and we know that in each of the coordinates the distance is at least D.\nConsider a given coordinate of the min-product, and let d and d\u2032 be the distance before and after\nthe scaling and rounding, respectively. On the one hand,\n2knd \u2212 k\nk\nd\u2032\n\u2265\n\u2265 2kn \u2212\n\u2265 2kn \u2212 kn = kn,\nd\nd\nD\nand on the other,\n\nd\u2032\n2knd + k\nk\n\u2264\n\u2264 2kn +\n\u2264 2kn + kn = 3kn.\nd\nd\nD\nTherefore, in each coordinate, the distance gets scaled by a factor in the range [kn, 3kn]. We now\napply Lemma 3.3 to vi\u2032 's and obtain their embedding into a min-product of tree metrics. Then, we\ndivide all distances in the trees by kn, and achieve an embedding of vi 's into a min-product of trees\nwith distortion at most 3 times larger than that implied by Lemma 3.3, which is O(log n).\nThe resulting min-product of tree metrics need not be a metric, but it is a \u03b3-near metric, where\n\u03b3 = O(log3 n) is the expansion incurred so far. We therefore embed the min-product of tree metrics\n12\n\n\finto the shortest-path metric of a weighted graph by using Lemma 3.4 with expansion at most \u03b3.\nFinally, we embed this metric into a low dimensional l1 metric space with distortion O(log2 n) by\nusing Lemma 3.6.\n\n4\n\nApplications\n\nWe now present two applications mentioned in the introduction: sublinear-time approximation of\nedit distance, and approximate pattern matching under edit distance.\n\n4.1\n\nSublinear-time approximation\n\nWe now present a sublinear-time algorithm for distinguishing pairs of strings with small edit distance from pairs with large edit distance. Let x and y be the two strings. The algorithm partitions\nthem into blocks x\nei and yei of the same length such that x = x\ne1 . . . x\neb and y = ye1 . . . yeb . Then it\nselects a few random i, and for each of them, it compares x\nei to yei . If it finds an i for which x\nei and\nyei are very different, the distance between x and y is likely to be large. Otherwise, if no such i is\ndetected, the edit distance between x and y is likely to be small. Our edit distance algorithm is\nused for approximating the distance between specific x\nei and yei .\n\nTheorem 4.1. Let \u03b1 and \u03b2 be two constants such that 0 \u2264 \u03b1 < \u03b2 \u2264 1. There is an algorithm\nthat distinguishes pairs of strings with edit distance O(n\u03b1 ) from those with distance \u03a9(n\u03b2 ) in time\nn\u03b1+2(1\u2212\u03b2)+o(1) .\n\u221a\n\nProof. Let f (n) = 2O( log n log log n) be a non-decreasing function that bounds the approximation\nn\u03b2\u2212\u03b1\nfactor of the algorithm given by Theorem 1.1. Let b = f (n)*log\nn . We partition the input strings x\nand y into b blocks, denoted x\nei and yei for i \u2208 [b], of length n/b each.\nIf ed(x, y) = O(n\u03b1 ), then maxi ed(e\nxi , yei ) \u2264 ed(x, y) = O(n\u03b1 ). On the other hand, if ed(x, y) =\n\u03b2\n\u03a9(n ), then maxi ed(e\nxi , yei ) \u2265 ed(x, y)/b = \u03a9(n\u03b1 * f (n) * log n). Moreover, the number of blocks i\nsuch that ed(e\nxi , yei ) \u2265 ed(x, y)/2b = \u03a9(n\u03b1 * f (n) * log n) is at least\ned(x, y) \u2212 b * ed(x, y)/2b\n= \u03a9(n\u03b2\u22121 * b).\nn/b\n\nTherefore, we can tell the two cases apart with constant probability by sampling O(n1\u2212\u03b2 ) pairs\nof blocks (e\nxi , yei ) and checking if any of the pairs is at distance \u03a9(n\u03b1 * f (n) * log n). Since for\neach such pair of strings, we only have to tell edit distance O(n\u03b1 ) from \u03a9(n\u03b1 * f (n) * log n), we\ncan use the algorithm of Theorem 1.1. We amplify the probability of success of that algorithm\nin the standard way by running it O(log n) times. The total running time of the algorithm is\nO(n1\u2212\u03b2 ) * O(log n) * (n/b)1+o(1) = O(n\u03b1+2(1\u2212\u03b2)+o(1) ).\n\n4.2\n\nPattern matching\n\nOur algorithm can be used for approximating the edit distance between a pattern P of length n\nand all length-n substrings of a text T . Let N = |T |. For every s \u2208 [N \u2212 2n + 1] of the form\nin + 1, we concatenate T 's length-2n substring that starts at index s with P , and compute an\nembedding\nof edit distance between all length-n substrings of the newly created string into l\u03b11 for\n\u221a\n\u03b1 = 2O( log n log log n) . We routinely amplify the probability of success of each execution of the\n13\n\n\falgorithm by running it O(log N ) times and selecting\nthe median of the returned values. The\n\u221a\nlog\nn\nlog\nlog n) .\nO(\nrunning time of the algorithm is O(N log N ) * 2\nThe\n\u221a distance between each of the substrings and the pattern is approximate up to a factor\nof 2O( log n log log n) , and can be used both for finding approximate occurrences of P in T , and for\nfinding a substring of T that is approximately closest to P .\n\nAcknowledgment\nThe authors thank Piotr Indyk for helpful discussions, and Robert Krauthgamer, Sofya Raskhodnikova, Ronitt Rubinfeld, and Rahul Sami for early discussions on near-linear algorithms for edit\ndistance.\n\nReferences\n[AIK08]\n\nAlexandr Andoni, Piotr Indyk, and Robert Krauthgamer. Earth mover distance over highdimensional spaces. In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms\n(SODA), pages 343\u2013352, 2008.\n\n[AKO10]\n\nAlexandr Andoni, Robert Krauthgamer, and Krzysztof Onak. Polylogarithmic approximation for edit distance and the asymmetric query complexity. In Proceedings of the Symposium on Foundations of Computer Science (FOCS), 2010. A full version is available at\nhttp://arxiv.org/abs/1005.4033.\n\n[BC03]\n\nBo Brinkman and Moses Charikar. On the impossibility of dimension reduction in l1 . In Proceedings of the Symposium on Foundations of Computer Science (FOCS), 2003.\n\n[BEK+ 03] Tu\u011fkan Batu, Funda Erg\u00fcn, Joe Kilian, Avner Magen, Sofya Raskhodnikova, Ronitt Rubinfeld,\nand Rahul Sami. A sublinear algorithm for weakly approximating edit distance. In Proceedings\nof the Symposium on Theory of Computing (STOC), pages 316\u2013324, 2003.\n[BES06]\n\nTu\u011fkan Batu, Funda Erg\u00fcn, and Cenk Sahinalp. Oblivious string embeddings and edit distance\napproximations. In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),\npages 792\u2013801, 2006.\n\n[BFC08]\n\nPhilip Bille and Martin Farach-Colton. Fast and compact regular expression matching. Theoretical\nComputer Science, 409(28):486\u2013496, 2008.\n\n[BJKK04] Ziv Bar-Yossef, T. S. Jayram, Robert Krauthgamer, and Ravi Kumar. Approximating edit distance efficiently. In Proceedings of the Symposium on Foundations of Computer Science (FOCS),\npages 550\u2013559, 2004.\n[Bou85]\n\nJean Bourgain. On Lipschitz embedding of finite metric spaces into Hilbert space. Israel Journal\nof Mathematics, 52:46\u201352, 1985.\n\n[CH02]\n\nRichard Cole and Ramesh Hariharan. Approximate string matching: A simpler faster algorithm.\nSIAM J. Comput., 31(6):1761\u20131782, 2002. Previously appeared in SODA'98.\n\n[CLRS01] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to\nAlgorithms. MIT Press, 2nd edition, 2001.\n[Cor03]\n\nGraham Cormode. Sequence Distance Embeddings. Ph.D. Thesis, University of Warwick. 2003.\n\n[CPSV00] Graham Cormode, Mike Paterson, Suleyman Cenk Sahinalp, and Uzi Vishkin. Communication\ncomplexity of document exchange. In Proceedings of the ACM-SIAM Symposium on Discrete\nAlgorithms (SODA), pages 197\u2013206, 2000.\n\n14\n\n\f[CS02]\n\nMoses Charikar and Amit Sahai. Dimension reduction in the l1 norm. In Proceedings of the\nSymposium on Foundations of Computer Science (FOCS), pages 551\u2013560, 2002.\n\n[Gus97]\n\nDan Gusfield. Algorithms on strings, trees, and sequences. Cambridge University Press, Cambridge, 1997.\n\n[Ind01]\n\nPiotr Indyk. Algorithmic aspects of geometric embeddings (tutorial). In Proceedings of the\nSymposium on Foundations of Computer Science (FOCS), pages 10\u201333, 2001.\n\n[Ind06]\n\nPiotr Indyk. Stable distributions, pseudorandom generators, embeddings and data stream computation. J. ACM, 53(3):307\u2013323, 2006. Previously appeared in FOCS'00.\n\n[KN06]\n\nSubhash Khot and Assaf Naor. Nonembeddability theorems via Fourier analysis. Math. Ann.,\n334(4):821\u2013852, 2006. Preliminary version appeared in FOCS'05.\n\n[KR06]\n\nRobert Krauthgamer and Yuval Rabani. Improved lower bounds for embeddings into L1 . In\nProceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1010\u20131017,\n2006.\n\n[Lev65]\n\nVladimir I. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals\n(in russian). Doklady Akademii Nauk SSSR, 4(163):845\u2013848, 1965. Appeared in English as:\nV. I. Levenshtein, Binary codes capable of correcting deletions, insertions, and reversals. Soviet\nPhysics Doklady 10(8), 707\u2013710, 1966.\n\n[LMS98]\n\nGad M. Landau, Eugene W. Myers, and Jeanette P. Schmidt. Incremental string comparison.\nSIAM J. Comput., 27(2):557\u2013582, 1998.\n\n[LN04]\n\nJames Lee and Assaf Naor. Embedding the diamond graph in Lp and dimension reduction in L1 .\nGeometric and Functional Analysis (GAFA), 14(4):745\u2013747, 2004.\n\n[Mat02]\n\nJi\u0159\u0131\u0301 Matou\u0161ek. Lectures on Discrete Geometry. Springer, 2002.\n\n[MP80]\n\nWilliam J. Masek and Mike Paterson. A faster algorithm computing string edit distances. J.\nComput. Syst. Sci., 20(1):18\u201331, 1980.\n\n[Nav01]\n\nGonzalo Navarro. A guided tour to approximate string matching. ACM Comput. Surv., 33(1):31\u2013\n88, 2001.\n\n[OR07]\n\nRafail Ostrovsky and Yuval Rabani. Low distortion embedding for edit distance. J. ACM, 54(5),\n2007. Preliminary version appeared in STOC'05.\n\n[SV96]\n\nCenk Sahinalp and Uzi Vishkin. Efficient approximate and dynamic matching of patterns using\na labeling paradigm. In Proceedings of the Symposium on Foundations of Computer Science\n(FOCS), pages 320\u2013328, 1996.\n\n15\n\n\f"}