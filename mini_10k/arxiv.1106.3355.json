{"id": "http://arxiv.org/abs/1106.3355v2", "guidislink": true, "updated": "2012-03-05T21:36:51Z", "updated_parsed": [2012, 3, 5, 21, 36, 51, 0, 65, 0], "published": "2011-06-16T21:32:26Z", "published_parsed": [2011, 6, 16, 21, 32, 26, 3, 167, 0], "title": "On epsilon-optimality of the pursuit learning algorithm", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1106.2732%2C1106.5532%2C1106.2159%2C1106.4537%2C1106.4130%2C1106.3405%2C1106.3530%2C1106.1222%2C1106.2126%2C1106.6163%2C1106.0842%2C1106.1635%2C1106.1794%2C1106.5580%2C1106.5464%2C1106.5440%2C1106.4538%2C1106.1512%2C1106.5009%2C1106.3811%2C1106.0447%2C1106.1343%2C1106.0396%2C1106.4749%2C1106.4522%2C1106.2643%2C1106.1363%2C1106.2911%2C1106.6018%2C1106.4576%2C1106.5795%2C1106.4181%2C1106.3768%2C1106.1631%2C1106.5750%2C1106.5746%2C1106.3355%2C1106.1406%2C1106.3771%2C1106.2318%2C1106.0102%2C1106.2141%2C1106.1160%2C1106.4362%2C1106.0159%2C1106.6096%2C1106.6284%2C1106.2898%2C1106.6199%2C1106.0700%2C1106.0025%2C1106.2002%2C1106.4841%2C1106.3258%2C1106.5635%2C1106.3798%2C1106.2384%2C1106.0990%2C1106.5955%2C1106.4157%2C1106.2606%2C1106.1489%2C1106.2667%2C1106.4258%2C1106.1883%2C1106.2908%2C1106.6017%2C1106.5477%2C1106.5053%2C1106.3286%2C1106.2983%2C1106.0033%2C1106.4268%2C1106.3305%2C1106.0096%2C1106.4185%2C1106.5055%2C1106.0462%2C1106.2415%2C1106.3036%2C1106.6034%2C1106.0834%2C1106.2472%2C1106.0155%2C1106.3150%2C1106.2975%2C1106.0256%2C1106.5044%2C1106.0719%2C1106.0072%2C1106.5002%2C1106.4756%2C1106.3913%2C1106.0124%2C1106.0347%2C1106.1703%2C1106.5438%2C1106.5743%2C1106.5868%2C1106.4530%2C1106.4257&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "On epsilon-optimality of the pursuit learning algorithm"}, "summary": "Estimator algorithms in learning automata are useful tools for adaptive,\nreal-time optimization in computer science and engineering applications. This\npaper investigates theoretical convergence properties for a special case of\nestimator algorithms: the pursuit learning algorithm. In this note, we identify\nand fill a gap in existing proofs of probabilistic convergence for pursuit\nlearning. It is tradition to take the pursuit learning tuning parameter to be\nfixed in practical applications, but our proof sheds light on the importance of\na vanishing sequence of tuning parameters in a theoretical convergence\nanalysis.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1106.2732%2C1106.5532%2C1106.2159%2C1106.4537%2C1106.4130%2C1106.3405%2C1106.3530%2C1106.1222%2C1106.2126%2C1106.6163%2C1106.0842%2C1106.1635%2C1106.1794%2C1106.5580%2C1106.5464%2C1106.5440%2C1106.4538%2C1106.1512%2C1106.5009%2C1106.3811%2C1106.0447%2C1106.1343%2C1106.0396%2C1106.4749%2C1106.4522%2C1106.2643%2C1106.1363%2C1106.2911%2C1106.6018%2C1106.4576%2C1106.5795%2C1106.4181%2C1106.3768%2C1106.1631%2C1106.5750%2C1106.5746%2C1106.3355%2C1106.1406%2C1106.3771%2C1106.2318%2C1106.0102%2C1106.2141%2C1106.1160%2C1106.4362%2C1106.0159%2C1106.6096%2C1106.6284%2C1106.2898%2C1106.6199%2C1106.0700%2C1106.0025%2C1106.2002%2C1106.4841%2C1106.3258%2C1106.5635%2C1106.3798%2C1106.2384%2C1106.0990%2C1106.5955%2C1106.4157%2C1106.2606%2C1106.1489%2C1106.2667%2C1106.4258%2C1106.1883%2C1106.2908%2C1106.6017%2C1106.5477%2C1106.5053%2C1106.3286%2C1106.2983%2C1106.0033%2C1106.4268%2C1106.3305%2C1106.0096%2C1106.4185%2C1106.5055%2C1106.0462%2C1106.2415%2C1106.3036%2C1106.6034%2C1106.0834%2C1106.2472%2C1106.0155%2C1106.3150%2C1106.2975%2C1106.0256%2C1106.5044%2C1106.0719%2C1106.0072%2C1106.5002%2C1106.4756%2C1106.3913%2C1106.0124%2C1106.0347%2C1106.1703%2C1106.5438%2C1106.5743%2C1106.5868%2C1106.4530%2C1106.4257&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Estimator algorithms in learning automata are useful tools for adaptive,\nreal-time optimization in computer science and engineering applications. This\npaper investigates theoretical convergence properties for a special case of\nestimator algorithms: the pursuit learning algorithm. In this note, we identify\nand fill a gap in existing proofs of probabilistic convergence for pursuit\nlearning. It is tradition to take the pursuit learning tuning parameter to be\nfixed in practical applications, but our proof sheds light on the importance of\na vanishing sequence of tuning parameters in a theoretical convergence\nanalysis."}, "authors": ["Ryan Martin", "Omkar Tilak"], "author_detail": {"name": "Omkar Tilak"}, "author": "Omkar Tilak", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1239/jap/1346955334", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1106.3355v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1106.3355v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1106.3355v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1106.3355v2", "arxiv_comment": null, "journal_reference": "Journal of Applied Probability, 49(3), 795-805, 2012", "doi": "10.1239/jap/1346955334", "fulltext": "On \u03b5-optimality of the pursuit learning algorithm\n\narXiv:1106.3355v2 [cs.LG] 5 Mar 2012\n\nRyan Martin\nDepartment of Mathematics, Statistics, and Computer Science\nUniversity of Illinois at Chicago\nrgmartin@math.uic.edu\nOmkar Tilak\nDepartment of Computer and Information Sciences\nIndiana University\u2013Purdue University Indianapolis\notilak@cs.iupui.edu\nSeptember 11, 2018\nAbstract\nEstimator algorithms in learning automata are useful tools for adaptive, realtime optimization in computer science and engineering applications. This paper investigates theoretical convergence properties for a special case of estimator\nalgorithms-the pursuit learning algorithm. In this note, we identify and fill a gap\nin existing proofs of probabilistic convergence for pursuit learning. It is tradition\nto take the pursuit learning tuning parameter to be fixed in practical applications,\nbut our proof sheds light on the importance of a vanishing sequence of tuning\nparameters in a theoretical convergence analysis.\nKeywords and phrases: Convergence in probability; indirect estimator algorithms; learning automata.\n\n1\n\nIntroduction\n\nA learning automaton consists of an adaptive learning agent operating in unknown random environment (Narendra and Thathachar 1989). In a nutshell, a learning automaton\nhas a choice among a finite set of actions to take, with one such action being optimal\nin the sense that it has the highest probability of producing a reward from the environment. This optimal action is unknown and the automaton uses feedback from the\nenvironment to try to identify the optimal action. Applications of learning automata include game theory, pattern recognition, computer vision, and routing in communications\nnetworks. Recently, learning automata have been used for call routing in ATM networks\n(Atlasis et al. 2000), multiple access channel selection (Zhong et al. 2010), congestion\navoidance in wireless networks (Misra et al. 2009), channel selection in radio networks\n\n1\n\n\f(Tuan et al. 2010), modeling of students' behavior (Oommen and Hashem 2010), clustering and backbone formation in ad-hoc wireless networks (Torkestania and Meybodi\n2010a,b), power system stabilizers (Kashki et al. 2010), and spectrum allocation in cognitive networks (Lixia et al. 2010). The simplest type of learning automata applies a direct algorithm, such as the linear reward-inaction algorithm (Narendra and Thathachar\n1989), which uses only the environmental feedback at iteration t to update the preference ordering of the actions. A drawback to using direct algorithms is their slow rate of\nconvergence. Attention recently has focused on the faster indirect estimator algorithms.\nWhat sets indirect algorithms apart from their direct counterparts is that they use the\nentire history of environmental feedback, i.e., from iteration 1 to t, to update the action preference ordering at iteration t. It is this more efficient use of the environmental\nfeedback which leads to faster convergence.\nHere we consider a special case of indirect estimator algorithms-the pursuit learning\nalgorithm-and, in particular, the version presented by Rajaraman and Sastry (1996).\nStarting with vacuous information about the unknown reward probabilities, pursuit\nlearning adaptively samples actions and tracks the empirical reward probabilities for\neach action. As the algorithm progresses, the sampling probabilities for the set of actions are updated in a way consistent with the relative magnitudes of the empirical\nreward probabilities; see Section 2.1. Simulations demonstrate that the algorithm is fast\nto converge in a number of different estimation scenarios (Lanct\u00f4t and Oommen 1992;\nOommen and Lanct\u00f4t 1990; Sastry 1985; Thathachar and Sastry 1985). Theoretically,\nthe algorithm is said to converge if the sampling probability for the action with the\nhighest reward probability becomes close to 1 as the number of interations increases.\nIn the learning automata literature, \u03b5-optimality is the gold standard for theoretical\nconvergence. But there seems to be two different notions of \u03b5-optimality that appears\nin the estimator algorithm literature. The version that appears in the direct estimator\ncontext (e.g., the linear reward-inaction algorithms) is in some sense weaker than that\nwhich appears in the indirect algorithm context. The latter is essentially convergence in\nprobability of the dominant action sampling probability to 1 as the number of iterations\napproaches infinity. Section 2.2 describes these two modes of stochastic convergence in\nmore detail, but our focus is on the latter convergence in probability version.\nThe main goal of this paper is to identify and fill a gap in existing proofs of \u03b5-optimality\nfor pursuit learning. We believe that it is important to throw light on this gap because\nthere are relatively recent papers proposing new algorithms that simply copy verbatim\nthese incomplete arguments. Specifically, in many proofs, the weak law of large numbers\nis incorrectly interpreted as giving a bound on the probability that the sample path stays\ninside a fixed neighborhood of its target forever after some fixed iteration. It is true that\nany finite-dimensional properties of the sample path can be handled via the weak law\nof large numbers, but the word \"forever\" implies that countably many time instances\nmust be dealt with and, hence, more care must be taken. A detailed explanation of the\ngap in existing proofs is presented in Section 2.3. In Section 3 we give a new proof of\nconvergence in probability for pursuit learning with some apparently new arguments. A\nfurther consequence of our analysis relates to the algorithm's tuning parameter. Indeed, it\nstandard to assume, in both theory and practice, that the algorithm's tuning parameter is\na small but fixed quantity. However, our analysis suggests that it is necessary to consider\na sequence of tuning parameters that vanish at a certain rate.\n2\n\n\f2\n2.1\n\nPursuit learning algorithm\nNotation and statement of the algorithm\n\nSuppose a learning automaton has a finite set of actions A = {a1 , . . . , ar }. If the automaton plays action ai , then it earns a reward with probability di ; otherwise, it gets a\npenalty. An estimator algorithm tracks this reward/penalty information with the goal if\nidentifying the optimal action-the one having the largest reward probability d. Pursuit\nlearning, described below, is one such algorithm.\nAt iteration t, the automaton selects an action \u03b1(t) \u2208 A with respective probabilities\n\u03c0(t) = {\u03c01 (t), . . . , \u03c0r (t)}. When this action is played, the environment produces an\noutcome X(t) \u2208 {0, 1} that satisfies\ndi = E{X(t) | \u03b1(t) = ai },\n\ni = 1, . . . , r.\n\nAs the algorithm proceeds and the various actions are tried, the automaton acquires more\nand more information about the d's indirectly through the X's. In other words, estimates\n\u02c6 of d at time t can be used to update the sampling probabilities \u03c0(t) in such a way\nd(t)\n\u02c6 are more likely to be chosen again in the next iteration.\nthat those actions with large d(t)\nAlgorithm 1 gives the details.\nFor comparison, the direct linear reward-inaction algorithm updates \u03c0(t) according\nto the following rule: If \u03b1(t) = ai , then\n(\n\u03c0j (t \u2212 1) + \u03bbX(t)[1 \u2212 \u03c0j (t \u2212 1)] if j = i,\n\u03c0j (t) =\n\u03c0j (t \u2212 1) \u2212 \u03bbX(t)\u03c0j (t \u2212 1)\nif j 6= i.\nIt is clear that this direct linear reward-inaction algorithm does not make efficient use\nof the full environmental history X(1), . . . , X(t) up to and including iteration t. For\nthis reason, it suffers from slower convergence than that of the indirect pursuit learning\nalgorithm. In fact, Thathachar and Sastry (1985) demonstrate, via simulations, that\nan indirect algorithm requires roughly 87% fewer iterations than a direct algorithm to\nachieve the same level of precision.\nOne might also notice that the pursuit learning algorithm is not unlike the popular stochastic approximation methods introduced in Robbins and Monro (1951) and\ndiscussed in detail in Kushner and Yin (2003). But a convergence analysis of pursuit\nlearning using the powerful ordinary differential equation techniques seems particularly\nchallenging due to the discontinuity of the \u03b4m(t) component in the Step 2(c) update.\nThe internal parameter \u03bb controls the size of steps that can be made in moving from\n\u03c0(t \u2212 1) to \u03c0(t). In general, small values of \u03bb correspond to slower rates of convergence,\nand vice versa. In our asymptotic results, we follow Tilak et al. (2011) and actually take\n\u03bb = \u03bbt to change with t. They argue that a changing \u03bb is consistent with the usual notion\nof convergence (see also Section 2.2), and does not necessarily conflict with the practical\nchoice of small fixed \u03bb. In what follows, we will assume that\n\u03bbt = 1 \u2212 \u03b81/t ,\n\nfor some fixed \u03b8 \u2208 (e\u22121 , 1),\n\nalthough all that is necessary is that \u03bbt \u224d 1 \u2212 \u03b81/t as t \u2192 \u221e.\n3\n\n(1)\n\n\fAlgorithm 1 \u2013 Pursuit Learning.\n1. For i = 1, . . . , r, set\n\u03c0i (0) = 1/r and Ni (0) = 0,\nand initialize d\u02c6i (0) by playing action ai a few times and recording the proportion\nof rewards. Set t = 1.\n2. (a) Sample \u03b1(t) according to \u03c0(t\u22121), and observe X(t) drawn from its conditional\ndistribution given \u03b1(t).\n(b) For i = 1, . . . , r, update\n(\nNi (t \u2212 1) + 1 if \u03b1(t) = ai\nNi (t) =\nNi (t \u2212 1)\nif \u03b1(t) 6= ai ,\nwhich denotes the number of times action ai has been tried up to and including\niteration t, and\n(\nd\u02c6i (t\u22121)\nd\u02c6i (t \u2212 1) + X(t)\u2212\nif \u03b1(t) = ai\nN\n(t)\n\u02c6\ni\ndi (t) =\nd\u02c6i (t \u2212 1)\nif \u03b1(t) 6= ai ,\nand then compute\nm(t) = arg max{d\u02c61 (t), . . . , d\u02c6r (t)}.\n(c) Update\n\u03c0(t) = (1 \u2212 \u03bb)\u03c0(t \u2212 1) + \u03bb\u03b4m(t) ,\nwhere \u03b4j is an r-vector whose j th entry is 1 and the others 0.\n3. Set t \u2190 t + 1 and return to Step 2.\n\n2.2\n\nConvergence and \u03b5-optimality\n\nConvergence of an estimator algorithm like pursuit learning implies that, eventually, the\nautomaton will always play the optimal action. In other words, if d1 is the largest among\nthe d's, then \u03c01 (t) gets close to 1, in some sense, as t \u2192 \u221e. This convergence is typically\ncalled \u03b5-optimality, although there appears to be no widely agreed upon definition.\nIn the context of indirect estimator algorithms, the following is perhaps the most\ncommon definition of \u03b5-optimality. We shall henceforth assume, without loss of generality,\nthat action a1 is the unique dominant action, i.e., d1 is the largest of the d's.\nDefinition 1. The pursuit learning algorithm is \u03b5-optimal if, for any \u03b5, \u03b4 > 0, there exists\nT \u22c6 = T \u22c6 (\u03b5, \u03b4) and \u03bb\u22c6 = \u03bb\u22c6 (\u03b5, \u03b4) such that\nP{\u03c01 (t) > 1 \u2212 \u03b5} > 1 \u2212 \u03b4,\n\n(2)\n\nfor all t > T \u22c6 and \u03bb < \u03bb\u22c6 . Simply put, the algorithm has the \u03b5-optimality property if\n\u03c01 (t) \u2192 1 in probability as (t, \u03bb) \u2192 (\u221e, 0).\n4\n\n\fThis is the definition of \u03b5-optimality that appears in Agache and Oommen (2002) and\nthe references mentioned in Section 1; Thathachar and Sastry (1985) say an algorithm\nthat satisfies Definition 1 is optimal in probability, arguably a better adjective. However, a different notion of \u03b5-optimality can be found in other contexts. This one says\nthat the algorithm is \u03b5-optimal if, for any \u03b5 > 0, there exists a fixed \u03bb > 0 so that\nlim inf t\u2192\u221e \u03c01 (t) > 1 \u2212 \u03b5 with probability 1. Compared to Definition 1, this latter definition is, on one hand, stronger because the condition is \"with probability 1\" but, on\nthe other hand, weaker because it does not even require \u03c01 (t) to converge. Since one\nwill not, in general, imply the other, it is unclear which definition is to be preferred.\nOommen and Lanct\u00f4t (1990) and others have recognized the difference between the two,\nbut apparently no explanation has been given for choosing one over the other.\nSince both T \u22c6 and \u03bb\u22c6 in Definition 1 are linked together through the choice of (\u03b5, \u03b4),\nit is intuitively clear that \u03bb should decrease with t. In fact, allowing \u03bb to change with\nt appears to be necessary in the proof presented in Section 3.2. So, throughout this\npaper, our notion of \u03b5-optimality will be that (2) holds for all t > T \u22c6 with the particular\n(vanishing) sequence of tuning parameters {\u03bbt } in (1).\n\n2.3\n\nExisting proofs of \u03b5-optimality\n\nHere we shall identify the gap in existing proofs of \u03b5-optimality for pursuit learning.\nFocus will fall primarily on the proof in Rajaraman and Sastry (1996), but this is just for\nconcreteness and not to single out these particular authors. In fact, essentially the same\ngap appears in Papadimitriou et al. (2004); there is a similar mis-step in other papers\nwhich we mention briefly below. The outline of these proofs goes roughly as follows:\nStep 1. Show that Ni (t) \u2192 \u221e in probability for each i = 1, . . . , r as t \u2192 \u221e. That is,\nshow that for any large n and small \u03b4, there exists T \u22c6 such that\nn\no\nP min Ni (t) > n > 1 \u2212 \u03b4, \u2200 t > T \u22c6 .\n(3)\ni=1,...,r\n\nStep 2. Show that for any small \u03b4 and \u03c1, there exists n such that\nn\no\nP max |d\u02c6i (t) \u2212 di | < \u03c1 min Ni (t) > n > 1 \u2212 \u03b4.\ni=1,...,r\n\ni=1,...,r\n\n(4)\n\nRajaraman and Sastry (1996) apply the famous inequality of Hoeffding (1963) to\nget an expression on the right-hand side that approaches 1 exponentially fast in n.\nA similar idea is used in Section 3.2.\nStep 3. Reason from (4) that, for sufficiently small \u03c1 and for all t larger than some T \u22c6 ,\nd\u02c61 (t) will be the largest among the d\u02c6i (t)'s with probability at least 1 \u2212 \u03b4.\nStep 4. Apply the monotonicity property (Lanct\u00f4t and Oommen 1992) to show that\n\u03c01 (t) increases monotonically to 1 starting from some t > T \u22c6 and must, therefore,\neventually cross the 1 \u2212 \u03b5 threshold.\nThe trouble with this line of reasoning emerges in Step 3, and is a consequence of an\nincorrect interpretation of the law of large numbers. Roughly speaking, what is needed\n5\n\n\f\u02c6 process over an infinite time horizon, t > T \u22c6 , but\nin Step 3 is a control of the entire d(t)\nthe law of large numbers alone can provide control at only finitely many time instances.\nMore precisely, from Steps 1 and 2 and the law of large numbers one can reason that\n\b\n\u02c6\nP d\u02c61 (t) is the largest of the d(t)'s\n> 1 \u2212 \u03b4, \u2200 t \u2265 T \u22c6 .\n(5)\n\nBut even though the left-hand side above is monotone increasing in t, one cannot conclude\ndirectly from this fact that\n\b\n\u02c6\nP d\u02c61 (t) is the largest of the d(t)'s\nfor all t \u2265 T \u22c6 > 1 \u2212 \u03b4.\n(6)\n\nRajaraman and Sastry (1996) implicitly assume that (5) implies (6) in their proof of\n\u03b5-optimality. A slightly different oversight is made in Thathachar and Sastry (1987),\nOommen and Lanct\u00f4t (1990), and Lanct\u00f4t and Oommen (1992). They assume that\n\b\n\u02c6\nP \u03c01 (t) > 1 \u2212 \u03b5 | d\u02c61 (t) is the largest of the d(t)'s\n\ncan be made arbitrarily close to 1 for large enough t. However, the knowledge that d\u02c61 (t)\n\u02c6\nis the largest of the d(t)'s\nonly at time t provides no control over how close \u03c01 (t) is to 1.\n\u02c6\nThe monotonicity property in Step 4 requires that the d(t)'s\nbe properly ordered forever,\nnot just at a single point in time.\nIt will be insightful to have a clearer picture of what the problem is mathematically.\nFirst, the left-hand side of (6) is, in general, much smaller than the left-hand side of (5),\nso the claim \"(5) \u21d2 (6)\" immediately seems questionable. In fact, if Et is the event that\n\u02c6\nd\u02c61 (t) is the largest of the d(t)'s\nat time t, then from (5) we can conclude that\nlim inf P{Et } \u2265 1 \u2212 \u03b4.\n\n(7)\n\nt\u2192\u221e\n\nBut the event inside P{* * * } in (6) is\n\\\n[ \\\nEt \u2282\nEt =: lim inf Et ,\nt\u2265T \u22c6\n\nT \u22c6 \u22651 t\u2265T \u22c6\n\nt\u2192\u221e\n\nand it follows from Fatou's lemma that\n\b\nleft-hand side of (6) \u2264 P lim inf Et \u2264 lim inf P{Et }.\nt\u2192\u221e\n\nt\u2192\u221e\n\n(8)\n\nSo, from (7) and (8), we can conclude only that the left-hand side (6) is bounded from\nabove by something greater than 1 \u2212 \u03b4 and, hence, (5) need not imply (6). Therefore,\nsome pursuit learning-specific considerations are needed and, to the authors' knowledge,\nthere is no obvious way to fill this gap. In the next section we give a proof of \u03b5-optimality\nbased on some apparently new arguments.\n\n3\n3.1\n\nA refined analysis of pursuit learning\nAn infinite-series result\n\nHere we state an infinite-series result which will be useful in our analysis in Section 3.2.\nFor completeness, a proof is given in Appendix A.\n6\n\n\fLemma 1. Given a, b \u2208 (0, 1), let \u03b6(t) = (1 \u2212 a/tb )t , t \u2265 1. Then\n\nP\u221e\n\nt=1\n\n\u03b6(t) < \u221e.\n\nIt is easy to see that the condition b \u2208 (0, 1) is necessary. Indeed, if b > 1, then the\nsequence itself converges to e\u2212a and the series cannot hope to converge. In our pursuit\nlearning application below, this condition will be taken care of in our choice of tuning\nparameter sequence \u03bbt .\n\n3.2\n\nMain results\n\nWe start by summarizing a few known results from the literature (see, e.g., Tilak et al.\n2011) which will be needed in the proof of the main theorem. Recall the notation Ni (t)\nused for the number of times, up to iteration t, that action i has been tried, i = 1, . . . , r.\nThe first result is that all of the N(t)'s are unbounded in probability as the number of\niterations t increases to \u221e.\nLemma 2. Suppose \u03bbt satisfies (1) with e\u22121 < \u03b8 < 1. Then for any small \u03b4 > 0 and any\nK > 0, there exists T1\u22c6 such that, for each i = 1, . . . , r,\nP{Ni (t) \u2264 K} < \u03b4,\n\n\u2200 t > T1\u22c6 .\n\nAs the number of times each action is played is increasing to \u221e, it is reasonable\n\u02c6\nto think that the estimates, namely the d(t)'s,\nshould be approaching their respective\ntargets, the d's. It turns out that this intuition is indeed correct.\nLemma 3. Suppose \u03bbt satisfies (1) with e\u22121 < \u03b8 < 1. Then for any small \u03b4 > 0 and any\nsmall \u03b7 > 0, there exists T2\u22c6 such that, for each i = 1, . . . , r,\n\b\nP |d\u0302i (t) \u2212 di | > \u03b7 < \u03b4, \u2200 t > T2\u22c6 .\nAn alternative way to phrase the previous two lemmas is that, under the stated\nconditions, Ni (t) and d\u02c6i (t) converge in probability to \u221e and di , respectively, as t \u2192 \u221e.\nNext is the main \u03b5-optimality result.\n\nTheorem 1. Suppose \u03bbt satisfies (1) with e\u22121 < \u03b8 < 1. Then for any small \u03b5, \u03b4 > 0,\nthere exists T \u22c6 such that\n\u2200 t > T \u22c6.\n\nP{\u03c01 (t) > 1 \u2212 \u03b5} > 1 \u2212 \u03b4,\n\nTo prove Theorem 1, we shall initially follow the argument of Rajaraman and Sastry\n(1996). To simplify notation, define the events\nA\u03b5 (t) = {\u03c01 (t) > 1 \u2212 \u03b5},\n\nt \u2265 1.\n\nNext, we observe that, since the reward probabilities are fixed, there is a number \u03b7 > 0\n\u02c6 at\nsuch that, if |d\u02c61 (t) \u2212 d1 | < \u03b7, then d\u02c61 (t) must be the largest of the estimates d(t)\niteration t. For this \u03b7, define the two sequences of events\n\b\nB(t) = |d\u02c61 (t) \u2212 d1 | < \u03b7 , t > 0,\n\\\n\b\nB(T ) = sup |d\u02c61(t) \u2212 d1 | < \u03b7 =\nB(t), T > 0.\nt\u2265T\n\nt\u2265T\n\n7\n\n\fThen, for any positive integers t and T , the law of total probability gives\nP{A\u03b5 (t + T )} \u2265 P{A\u03b5 (t + T ) | B(T )}P{B(T )}.\nMoreover, from the monotonicity property of pursuit learning, it follows that there exists\nT3\u22c6 such that\nP{A\u03b5 (t + T ) | B(T )} = 1, \u2200 t > T3\u22c6 ,\nTherefore, to complete the proof, it remains to show that there exists T > 0 such that\nP{B(T )} > 1 \u2212 \u03b4. But by DeMorgan's law,\nn\\\no\nn[\no\n\b\nP B(T ) = P\nB(t) = 1 \u2212 P\nB(t)c ,\nt\u2265T\n\nt\u2265T\n\nso we are done if we can find T > 0 such that\nn[\no\nc\nP\nB(t) < \u03b4.\n\n(9)\n\nt\u2265T\n\nTowards this, write N(t) = N1 (t) and note that\nn[\no X\nc\nP\nB(t) \u2264\nP{B(t)c }\nt\u2265T\n\nt\u2265T\n\n=\n\nt\nX\u0010X\nt\u2265T\n\nn=0\n\n\u0011\nP{B(t)c | N(t) = n}P{N(t) = n} .\n\nIt follows easily from Hoeffding's inequality that\n\b\nP{B(t)c | N(t) = n} = P |d\u02c61 (t) \u2212 d1 | \u2265 \u03b7 | N(t) = n \u2264 e\u2212hn ,\nwhere h = \u03b7 2 /8 > 0 is a constant independent of n. Therefore,\nP\n\nn[\n\nt\u2265T\n\nB(t)\n\nc\n\no\n\n\u2264\n\nt\nX\u0010 X\nt\u2265T\n\n\u2264\n\nn=0\n\nt\nX\u0010 X\nt\u2265T\n\nP{B(t)c | N(t) = n}P{N(t) = n}\n\nn=0\n\n\u0011\n\n\u0011\ne\u2212hn P{N(t) = n} ,\n\nand the inner-most sum is easily seen to be the moment generating function, call it \u03c8t (u),\nof the random variable N(t) evaluated at u = \u2212h. To prove that this sum is finite, we\nmust show that \u03c8t (\u2212h) vanishes sufficiently fast in t.\nFormulae for moment generating functions of standard random variables are readily\navailable. But N(t) is not a standard random variable; it is like a Bernoulli convolution\n(Klenke and Mattner 2010; Proschan and Sethuraman 1976) but the summands are only\nconditionally Bernoulli. In Lemma 4 below we show that \u03c8t (u), for u \u2264 0, is bounded\nabove by a certain binomial random variable's moment generating function.\nLemma 4. Consider P\na binomial random variable with parameters (t, \u03c9t ), where \u03c9t =\n\u03b3(t)\n\u03c01 (0)\u03b8\nand \u03b3(t) = ts=1 s\u22121 . If \u03c6t is the corresponding moment generating function,\nthen \u03c8t (u) \u2264 \u03c6t (u) for u \u2264 0.\n8\n\n\fP\nProof. Let N(t) = ts=1 \u03be(s), where \u03be(s) = 1 if the optimal action is sampled at iteration\ns and 0 otherwise. If As denotes the \u03c3-algebra generated by the history of the algorithm\nup to and including iteration s, then \u03be(s) satisfies\nE{\u03be(s) | As\u22121 } = \u03c0(s \u2212 1).\nFor u \u2264 0, the moment generating function \u03c8t (u) of N(t) satisfies\n\u03c8t (u) = E{euN (t) } = E{eu\u03be(1)+***+u\u03be(t) }\nt\nt\no\nnY\nnY\n\u0002\n\u0003o\nu\u03be(s)\n=E\nE(e\n| As\u22121 ) = E\n1 \u2212 (1 \u2212 eu )\u03c0(s \u2212 1) .\ns=1\n\ns=1\n\nBut Rajaraman and Sastry (1996) show that \u03c9t \u2264 min{\u03c0(1), . . . , \u03c0(t)}, so\n\u03c8t (u) \u2264\n\nt\nY\n\u0002\ns=1\n\n\u0003 \u0002\n\u0003t\n1 \u2212 (1 \u2212 eu )\u03c9t = 1 \u2212 (1 \u2212 eu )\u03c9t .\n\nBut the right-hand side above is exactly \u03c6t (u), completing the proof.\nBack to our main discussion, we now have that\nn[\no X\nX\nc\nP\n\u03c8t (\u2212h) \u2264\n\u03c6t (\u2212h),\nB(t) \u2264\nt\u2265T\n\nt\u2265T\n\nt\u2265T\n\nand it is well-known that the moment generating function \u03c6t for the binomial random\nvariable satisfies\n\u0002\n\u0003t \u0002\n\u0003t\n\u03c6t (\u2212h) = 1 \u2212 \u03c9t (1 \u2212 e\u2212h ) = 1 \u2212 \u03c01 (0)(1 \u2212 e\u2212h )\u03b8\u03b3(t) .\nBut the sequence \u03b3(t) grows like ln(t), and \u03b8ln(t) = tln(\u03b8) , so for large t\nh\n\u03c01 (0)(1 \u2212 e\u2212h ) it\n.\n\u03c6t (\u2212h) \u223c 1 \u2212\nt\u2212 ln(\u03b8)\n\nThe right-hand side above is just the sequence \u03b6(t) defined in Lemma 1, with\na = \u03c01 (0)(1 \u2212 e\u2212h ) and b = \u2212 ln(\u03b8).\nP\nTherefore,\nthe\nseries\nt\u22651 \u03c6t (\u2212h) converges so, for any \u03b4, there exists T such that\nP\n(\u2212h) < \u03b4, thus proving (9). To put everything together, let T4\u22c6 be the smallest\nt\u2265T \u03c6tP\nT with t\u2265T \u03c6t (\u2212h) < \u03b4. Then Theorem 1 follows by taking T \u22c6 = T3\u22c6 + T4\u22c6 .\nA natural question is if one can give a deterministic bound for T \u22c6 in terms of the\nuser-specfied \u03b5, \u03b4, and \u03b8. An affirmative answer to this question is given in Appendix B.\nWe choose not to give much emphasis to this result, as the bound we obtain appears to\nbe quite conservative. For example, for numerical experiments run under the setup in\nSimulation 1 of Thathachar and Sastry (1985), we find that more than 95% of sample\npaths converge in roughly 25\u2013250 iterations, while our conservative theoretical bounds\nare, for moderate \u03b8, orders of magnitude greater.\n9\n\n\f4\n\nDiscussion\n\nIn this paper we have taken a closer look at convergence properties of pursuit learning.\nIn particular, we have identified a gap in existing proofs of \u03b5-optimality and provided\na new argument to fill this gap. An important consequence of our theoretical analysis\nis that it seems necessary to explicitly specify the rate at which the tuning parameter\nsequence \u03bb = \u03bbt vanishes with t.\nPIn fact, if \u03c9t defined in Lemma 4 vanishes too quickly,\nwhich it would if \u03bbt \u2261 \u03bb, then t \u03c6t (\u2212h) = \u221e and the proof fails. But we should also\nreiterate that a theoretical analysis that requires vanishing \u03bbt need not conflict with the\ntradition of running the algorithm with fixed small \u03bb in practical applications. In fact, the\nparticular \u03bbt vanishes relatively fast so, for applications, we recommend running pursuit\nlearning with, say,\n+ \u22121\n\u03bbt = 1 \u2212 \u03b8[1+(t\u2212t0 ) ] ,\nwhere t0 is some fixed cutoff, and x+ = max{0, x}. This effectively keeps \u03bbt constant for\na fixed finite period of time, after which it vanishes like that in (1). Alternatively, one\nmight consider \u03bbt = 1 \u2212 \u03b8v(t) , where v(t) vanishes more slowly than t\u22121 . This choice of\n\u03bbt vanishes more slowly than that in (1), thus giving the algorithm more opportunities\nto adjust to the environment. We believe that an analysis similar to ours can be used to\nshow that the corresponding pursuit learning converges in the sense of Definition 1.\nIt is also worth mentioning that the results of Tilak et al. (2011), for \u03bbt as in (1), can\nbe applied to show that \u03c01 (t) \u2192 1 with probability 1 as (t, \u03bbt ) \u2192 (\u221e, 0). This, of course,\nimmediately implies \u03b5-optimality in the sense of Definition 1. However, this indirect\nargument does not give any insight as to how to bound the number of iterations needed\nto be sufficiently close to convergence, as we do-albeit conservatively-in Appendix B.\n\u02c6\nBut the result proved in Tilak et al. (2011) that d\u02c61 (t) is largest among the d(t)'s\ninfinitely\noften with probability 1, together with the formula in (10) in Appendix B can perhaps\nbe used to reason towards and almost sure rate of convergence for pursuit learning.\n\nAcknowledgements\nA portion of this work was completed while the first author was affiliated with the Department of Mathematical Sciences at Indiana University\u2013Purdue University Indianapolis. The authors thank Professors Snehasis Mukhopadhyay and Chuanhai Liu for sharing\ntheir insight, and the Editor and two anonymous referees for some helpful suggestions.\n\nA\n\nProof of Lemma 1\n\nTo start, write \u03b6(t) as\na \u0011tb it1\u2212b\na \u0011t h\u0010\n.\n\u03b6(t) = 1 \u2212 b = 1 \u2212 b\nt\nt\n\u0010\n\nIf f (t) = (1 \u2212 at )t , then ordinary calculus reveals that\n\n\u0010\n\u0010\na\na\na\u0011\na \u0011\nd\n+\n+\nln f (t) = ln 1 \u2212\n= \u2212 ln 1 +\n> 0.\ndt\nt\nt\u2212a\nt\u2212a\nt\u2212a\n10\n\n\fTherefore, we have shown that ln f (t) and, hence, f (t) and, hence, f (tb ) are monotone\nincreasing. Moreover, f (tb ) \u2191 e\u2212a < 1. Thus, \u03b6(t) \u2264 exp{\u2212at1\u2212b }. So to show that\nP\n\u221e\nt=1 \u03b6(t) is finite, it suffices to show that, for c = 1 \u2212 b,\nZ \u221e\nc\ne\u2212at dt < \u221e.\n1\n\nMaking a change-of-variable x = tc , the integral becomes\nZ\nZ \u221e\nZ \u221e\n1 \u221e 1/c\u22121 \u2212ax\n1\n\u2212atc\n\u2212ax\nx\ne\ndx.\ne\ndt =\ne\ndx =\ncx1\u22121/c\nc 1\n1\n1\nSince 1/c > 1 and a > 0, the integral is finite, completing the proof.\nMaking one more change-of-variables (y = ax), one finds that the last integral above\ncan be expressed as\nZ \u221e\nZ \u221e\n1\n1\n\u2212atc\ny 1/c\u22121 e\u2212y dy = 1/c \u0393(c\u22121 ; a),\ne\ndt = 1/c\nca\nca\na\n1\nR \u221e s\u22121 \u2212u\nwhere \u0393(s, x) = x u e du is the incomplete gamma function.\n\nB\n\nA bound on the number of iterations\n\nAs a follow-up to the proof of Theorem 1, we give a conservative upper bound on the\nnumber of iterations T \u22c6 needed to be sufficiently close to convergence.\nTheorem 2. For given \u03b5, \u03b4 \u2208 (0, 1) and \u03b8 \u2208 (e\u22121 , 1), a deterministic bound on the\nnecessary number of iterations T \u22c6 in Theorem 1 can be found numerically.\nIn the proof that follows, we are assuming h to be a known constant, while it actually\ndepends on the \u03b7 used above which, in turn, depends on the unknown d's. The bounds\nobtained in Rajaraman and Sastry (1996) also depend on \u03b7, called the size of the problem.\nTo use this bound in practice, users must estimate \u03b7 by some other means.\nProof of Theorem 2. As stated above, the desired\nT \u22c6 is actually a sum T3\u22c6 + T4\u22c6 . Let's\nP\nbegin with T4\u22c6 , the smallest T = T (\u03b4) such that t\u2265T \u03c6t (\u2212h) < \u03b4. From the proof of the\nclassical integral test for convergence of infinite series in calculus, it follows that\nZ \u221e\nX\n\u03c6t (\u2212h) \u2264 \u03c6T (\u2212h) +\n\u03c6t (\u2212h) dt.\nT\n\nt\u2265T\n\nA modification of the argument presented in Appendix A shows that\nZ \u221e\nb\n\u03c6t (\u2212h) dt \u2264 b \u0393(b; aT ),\na\nT\nwhere a = \u03c01 (0)(1 \u2212 e\u2212h ), b = (1 + ln \u03b8)\u22121 , and \u0393(s, x) is the incomplete gamma function\n(defined in Appendix A). Since \u03c6T (\u2212h) and \u0393(b; aT ) are both decreasing functions of T ,\nit is possible to solve the equation\n\u03c6T (\u2212h) +\n\nb\n\u0393(b; aT ) = \u03b4\nab\n11\n\n\ffor T numerically to obtain the bound\nTowards bounding T3\u22c6 we note that\nsample paths in a set \u03a9 of probability\n(2011) prove that\n\nT4\u22c6 in terms of the user-specified inputs.\n\u02c6 for all t \u2265 1 for\nd\u02c61 (t + T4\u22c6 ) is the largest of the d's\n> 1 \u2212 \u03b4. For sample paths in this \u03a9, Tilak et al.\n\u22c6\n\n\u03c01 (t + T4\u22c6 ) = 1 \u2212 \u03b8\u03b3(t+T4 ) {1 \u2212 \u03c01 (T4\u22c6 )},\nwhere \u03b3(t) =\n\nPt\n\ns=1\n\nt \u2265 1,\n\n(10)\n\ns\u22121 . Since \u03c01 (T4\u22c6 ) \u2208 (0, 1), it easily follows that\n\u22c6\n\n1 \u2212 \u03c01 (t + T4\u22c6 ) \u2264 \u03b8\u03b3(t+T4 ) ,\n\nt \u2265 1.\n\u22c6\n\nGiven \u03b5 and T4\u22c6 , it is easy to calculate T3\u22c6 such that \u03b8\u03b3(t+T4 ) \u2264 \u03b5 for all t > T3\u22c6 .\n\nReferences\nAgache, M. and Oommen, B. J. (2002), \"Generalized pursuit learning schemes: New\nfamilies of condinuous and discrete learning automata,\" IEEE Trans. Systems Man\nCybernet., 32, 738\u2013749.\nAtlasis, A., Loukas, A., and Vasilakos, A. V. (2000), \"Call admission control algorithm\nin ATM networks: A methodology,\" Computer Networks, 34, 341\u2013353.\nHoeffding, W. (1963), \"Probability inequalities for sums of bounded random variables,\"\nJ. Amer. Statist. Assoc., 58, 13\u201330.\nKashki, M., Abido, M., and Abdel-Magid, Y. (2010), \"Pole placement approach for\nrobust optimum design of pss and tcsc-based stabilizers using reinforcement learning\nautomata,\" Electrical Engineering, 383\u2013394.\nKlenke, A. and Mattner, L. (2010), \"Stochastic ordering of classical discrete distributions,\" Adv. Appl. Probab., 42, 392\u2013410.\nKushner, H. J. and Yin, G. G. (2003), Stochastic approximation and recursive algorithms\nand applications, New York: Springer-Verlag, 2nd ed.\nLanct\u00f4t, J. K. and Oommen, B. J. (1992), \"Discretized estimator learning automata,\"\nIEEE Trans. Systems Man Cybernet., 22, 1473\u20131483.\nLixia, L., Gang, H., Ming, X., and Yuxing, P. (2010), \"Learning automata based spectrum allocation in cognitive networks,\" in IEEE International Conference on Wireless\nCommunications, Networking, and Information Security, pp. 503\u2013508.\nMisra, S., Tiwari, V., and Obaidat, M. (2009), \"Lacas: learning automata-based congestion avoidance scheme for healthcare wireless sensor networks,\" IEEE Journal on\nSelected Areas in Communications, 27, 466\u2013479.\nNarendra, K. S. and Thathachar, M. A. L. (1989), Learning automata: An introduction,\nEnglewood Cliffs, NJ: Prentice Hall.\n\n12\n\n\fOommen, B. J. and Hashem, M. K. (2010), \"Modeling a student's behavior in a tutoriallike system using learning automata,\" IEEE Trans. Systems Man Cybernet B, 40,\n481\u2013492.\nOommen, B. J. and Lanct\u00f4t, J. K. (1990), \"Discretized pursuit learning automata,\" IEEE\nTrans. Systems Man Cybernet., 20, 931\u2013938.\nPapadimitriou, G. I., Sklira, M., and Pomportsis, A. S. (2004), \"A new class of \u01eb-optimal\nlearning automata,\" IEEE Trans. Systems Man Cybernet B, 34, 246\u2013254.\nProschan, F. and Sethuraman, J. (1976), \"Stochastic comparisons of order statistics from\nheterogeneous populations, with applications in reliability,\" J. Multivariate Anal., 6,\n608\u2013616.\nRajaraman, K. and Sastry, P. S. (1996), \"Finite time analysis of the pursuit algorithm\nfor learning automata,\" IEEE Trans. Systems Man Cybernet B, 26, 590\u2013598.\nRobbins, H. and Monro, S. (1951), \"A stochastic approximation method,\" Ann. Math.\nStatistics, 22, 400\u2013407.\nSastry, P. S. (1985), \"Systems of Learning Automata: Estimator Algorithms and Applications,\" Ph.D. thesis, Indian Institute of Science.\nThathachar, M. A. L. and Sastry, P. S. (1985), \"A new approach to the design of reinforcement schemes for learning automata,\" IEEE Trans. Systems Man Cybernet., 15,\n168\u2013175.\n- (1987), \"Learning optimal discriminant functions through a cooperative game of automata,\" IEEE Trans. Systems Man Cybernet., 17, 73\u201385.\nTilak, O., Martin, R., and Mukhopadhyay, S. (2011), \"A decentralized, indirect method\nfor learning automata games,\" IEEE Trans. Systems Man Cybernet B, 41, 1213\u20131223.\nTorkestania, J. and Meybodi, M. (2010a), \"Clustering the wireless ad hoc networks: A\ndistributed learning automata approach,\" Journal of Parallel and Distributed Computing, 70, 394\u2013405.\n- (2010b), \"An intelligent backbone formation algorithm for wireless ad hoc networks\nbased on distributed learning automata,\" Computer Networks, 54, 826\u2013843.\nTuan, T., Tong, L., and Premkumar, A. (2010), \"An adaptive learning automata algorithm for channel selection in cognitive radio network,\" in 2010 International Conference on Communications and Mobile Computing, vol. 2, pp. 159\u2013163.\nZhong, W., Xu, Y., and Tao, M. (2010), \"Precoding strategy selection for cognitive\nmimo multiple access channels using learning automata,\" in 2010 IEEE International\nConference on Communications, pp. 23\u201327.\n\n13\n\n\f"}