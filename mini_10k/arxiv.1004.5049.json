{"id": "http://arxiv.org/abs/1004.5049v3", "guidislink": true, "updated": "2012-04-19T07:29:27Z", "updated_parsed": [2012, 4, 19, 7, 29, 27, 3, 110, 0], "published": "2010-04-28T14:54:53Z", "published_parsed": [2010, 4, 28, 14, 54, 53, 2, 118, 0], "title": "The Burbea-Rao and Bhattacharyya centroids", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.3995%2C1004.0914%2C1004.4418%2C1004.0546%2C1004.1095%2C1004.3906%2C1004.2374%2C1004.0436%2C1004.4174%2C1004.2982%2C1004.3834%2C1004.3465%2C1004.3373%2C1004.5273%2C1004.2360%2C1004.0601%2C1004.4403%2C1004.2171%2C1004.2683%2C1004.1474%2C1004.2913%2C1004.1162%2C1004.1029%2C1004.1158%2C1004.4557%2C1004.2561%2C1004.0308%2C1004.3350%2C1004.1849%2C1004.2743%2C1004.2595%2C1004.3829%2C1004.5218%2C1004.1783%2C1004.1767%2C1004.0581%2C1004.0176%2C1004.4219%2C1004.4136%2C1004.2100%2C1004.1606%2C1004.5049%2C1004.0158%2C1004.2321%2C1004.0253%2C1004.2507%2C1004.3541%2C1004.1409%2C1004.4530%2C1004.4664%2C1004.4162%2C1004.2304%2C1004.4037%2C1004.5387%2C1004.3445%2C1004.4331%2C1004.2005%2C1004.3617%2C1004.2946%2C1004.3097%2C1004.0756%2C1004.1177%2C1004.4368%2C1004.2578%2C1004.1337%2C1004.4147%2C1004.3395%2C1004.0895%2C1004.2735%2C1004.5302%2C1004.2055%2C1004.3874%2C1004.1570%2C1004.0103%2C1004.1097%2C1004.4656%2C1004.5140%2C1004.5494%2C1004.0722%2C1004.5318%2C1004.5533%2C1004.3497%2C1004.2887%2C1004.0843%2C1004.0015%2C1004.5442%2C1004.1091%2C1004.1719%2C1004.5325%2C1004.1403%2C1004.2072%2C1004.3090%2C1004.1771%2C1004.3276%2C1004.3054%2C1004.0833%2C1004.4095%2C1004.1083%2C1004.1128%2C1004.2751%2C1004.1873&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Burbea-Rao and Bhattacharyya centroids"}, "summary": "We study the centroid with respect to the class of information-theoretic\nBurbea-Rao divergences that generalize the celebrated Jensen-Shannon divergence\nby measuring the non-negative Jensen difference induced by a strictly convex\nand differentiable function. Although those Burbea-Rao divergences are\nsymmetric by construction, they are not metric since they fail to satisfy the\ntriangle inequality. We first explain how a particular symmetrization of\nBregman divergences called Jensen-Bregman distances yields exactly those\nBurbea-Rao divergences. We then proceed by defining skew Burbea-Rao\ndivergences, and show that skew Burbea-Rao divergences amount in limit cases to\ncompute Bregman divergences. We then prove that Burbea-Rao centroids are\nunique, and can be arbitrarily finely approximated by a generic iterative\nconcave-convex optimization algorithm with guaranteed convergence property. In\nthe second part of the paper, we consider the Bhattacharyya distance that is\ncommonly used to measure overlapping degree of probability distributions. We\nshow that Bhattacharyya distances on members of the same statistical\nexponential family amount to calculate a Burbea-Rao divergence in disguise.\nThus we get an efficient algorithm for computing the Bhattacharyya centroid of\na set of parametric distributions belonging to the same exponential families,\nimproving over former specialized methods found in the literature that were\nlimited to univariate or \"diagonal\" multivariate Gaussians. To illustrate the\nperformance of our Bhattacharyya/Burbea-Rao centroid algorithm, we present\nexperimental performance results for $k$-means and hierarchical clustering\nmethods of Gaussian mixture models.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1004.3995%2C1004.0914%2C1004.4418%2C1004.0546%2C1004.1095%2C1004.3906%2C1004.2374%2C1004.0436%2C1004.4174%2C1004.2982%2C1004.3834%2C1004.3465%2C1004.3373%2C1004.5273%2C1004.2360%2C1004.0601%2C1004.4403%2C1004.2171%2C1004.2683%2C1004.1474%2C1004.2913%2C1004.1162%2C1004.1029%2C1004.1158%2C1004.4557%2C1004.2561%2C1004.0308%2C1004.3350%2C1004.1849%2C1004.2743%2C1004.2595%2C1004.3829%2C1004.5218%2C1004.1783%2C1004.1767%2C1004.0581%2C1004.0176%2C1004.4219%2C1004.4136%2C1004.2100%2C1004.1606%2C1004.5049%2C1004.0158%2C1004.2321%2C1004.0253%2C1004.2507%2C1004.3541%2C1004.1409%2C1004.4530%2C1004.4664%2C1004.4162%2C1004.2304%2C1004.4037%2C1004.5387%2C1004.3445%2C1004.4331%2C1004.2005%2C1004.3617%2C1004.2946%2C1004.3097%2C1004.0756%2C1004.1177%2C1004.4368%2C1004.2578%2C1004.1337%2C1004.4147%2C1004.3395%2C1004.0895%2C1004.2735%2C1004.5302%2C1004.2055%2C1004.3874%2C1004.1570%2C1004.0103%2C1004.1097%2C1004.4656%2C1004.5140%2C1004.5494%2C1004.0722%2C1004.5318%2C1004.5533%2C1004.3497%2C1004.2887%2C1004.0843%2C1004.0015%2C1004.5442%2C1004.1091%2C1004.1719%2C1004.5325%2C1004.1403%2C1004.2072%2C1004.3090%2C1004.1771%2C1004.3276%2C1004.3054%2C1004.0833%2C1004.4095%2C1004.1083%2C1004.1128%2C1004.2751%2C1004.1873&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study the centroid with respect to the class of information-theoretic\nBurbea-Rao divergences that generalize the celebrated Jensen-Shannon divergence\nby measuring the non-negative Jensen difference induced by a strictly convex\nand differentiable function. Although those Burbea-Rao divergences are\nsymmetric by construction, they are not metric since they fail to satisfy the\ntriangle inequality. We first explain how a particular symmetrization of\nBregman divergences called Jensen-Bregman distances yields exactly those\nBurbea-Rao divergences. We then proceed by defining skew Burbea-Rao\ndivergences, and show that skew Burbea-Rao divergences amount in limit cases to\ncompute Bregman divergences. We then prove that Burbea-Rao centroids are\nunique, and can be arbitrarily finely approximated by a generic iterative\nconcave-convex optimization algorithm with guaranteed convergence property. In\nthe second part of the paper, we consider the Bhattacharyya distance that is\ncommonly used to measure overlapping degree of probability distributions. We\nshow that Bhattacharyya distances on members of the same statistical\nexponential family amount to calculate a Burbea-Rao divergence in disguise.\nThus we get an efficient algorithm for computing the Bhattacharyya centroid of\na set of parametric distributions belonging to the same exponential families,\nimproving over former specialized methods found in the literature that were\nlimited to univariate or \"diagonal\" multivariate Gaussians. To illustrate the\nperformance of our Bhattacharyya/Burbea-Rao centroid algorithm, we present\nexperimental performance results for $k$-means and hierarchical clustering\nmethods of Gaussian mixture models."}, "authors": ["Frank Nielsen", "Sylvain Boltz"], "author_detail": {"name": "Sylvain Boltz"}, "author": "Sylvain Boltz", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TIT.2011.2159046", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1004.5049v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1004.5049v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "13 pages", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1004.5049v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1004.5049v3", "journal_reference": "IEEE Transactions on Information Theory 57(8):5455-5466, 2011", "doi": "10.1109/TIT.2011.2159046", "fulltext": "IEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n1\n\nThe Burbea-Rao and Bhattacharyya centroids\n\narXiv:1004.5049v3 [cs.IT] 19 Apr 2012\n\nFrank Nielsen, Senior Member, IEEE, and Sylvain Boltz, Nonmember, IEEE\n\nAbstract-We study the centroid with respect to the class of\ninformation-theoretic Burbea-Rao divergences that generalize the\ncelebrated Jensen-Shannon divergence by measuring the nonnegative Jensen difference induced by a strictly convex and\ndifferentiable function. Although those Burbea-Rao divergences\nare symmetric by construction, they are not metric since they\nfail to satisfy the triangle inequality. We first explain how a\nparticular symmetrization of Bregman divergences called JensenBregman distances yields exactly those Burbea-Rao divergences.\nWe then proceed by defining skew Burbea-Rao divergences, and\nshow that skew Burbea-Rao divergences amount in limit cases to\ncompute Bregman divergences. We then prove that Burbea-Rao\ncentroids are unique, and can be arbitrarily finely approximated\nby a generic iterative concave-convex optimization algorithm with\nguaranteed convergence property. In the second part of the paper,\nwe consider the Bhattacharyya distance that is commonly used to\nmeasure overlapping degree of probability distributions. We show\nthat Bhattacharyya distances on members of the same statistical\nexponential family amount to calculate a Burbea-Rao divergence\nin disguise. Thus we get an efficient algorithm for computing\nthe Bhattacharyya centroid of a set of parametric distributions\nbelonging to the same exponential families, improving over\nformer specialized methods found in the literature that were\nlimited to univariate or \"diagonal\" multivariate Gaussians. To\nillustrate the performance of our Bhattacharyya/Burbea-Rao\ncentroid algorithm, we present experimental performance results\nfor k-means and hierarchical clustering methods of Gaussian\nmixture models.\nIndex Terms-Centroid, Kullback-Leibler divergence, JensenShannon divergence, Burbea-Rao divergence, Bregman divergences, Exponential families, Bhattacharrya divergence, Information geometry.\n\nI. I NTRODUCTION\nA. Means and centroids\nIn Euclidean geometry, the centroid c of a point\nPn set P =\n{p1 , ..., pn } is defined as the center of mass n1 i=1 pi , also\ncharacterized as the center point that minimizes\nPn the average\nsquared Euclidean distances: c = arg minp i=1 n1 kp \u2212 pi k2 .\nThis basic notion of Euclidean centroid can be extended to\ndenote a mean point M (P) representing the centrality of a\ngiven point set P. There are basically two complementary\napproaches to define mean values of numbers: (1) by axiomatization, or (2) by optimization, summarized concisely as\nfollows:\n\u2022 By axiomatization. This approach was first historically\npioneered by the independent work of Kolmogorov [1]\nand Nagumo [2] in 1930, and simplified and refined later\nF. Nielsen is with the Department of Fundamental Research of Sony\nComputer Science Laboratories, Inc., Tokyo, Japan, and the Computer Science Department (LIX) of \u00c9cole Polytechnique, Palaiseau, France. e-mail:\nFrank.Nielsen@acm.org\nS. Boltz is with the Computer Science Department (LIX) of \u00c9cole Polytechnique, Palaiseau, France. e-mail: boltz@lix.polytechnique.fr\nManuscript received April 2010, revised April 2012. This revision includes\nin the appendix a proof of the uniqueness of the centroid.\n\nby Acz\u00e9l [3]. Without loss of generality we consider\nthe mean of two non-negative numbers x1 and x2 , and\npostulate the following expected behaviors of a mean\nfunction M (x1 , x2 ) as axioms (common sense):\n\u2013 Reflexivity. M (x, x) = x,\n\u2013 Symmetry. M (x1 , x2 ) = M (x2 , x1 ),\n\u2013 Continuity and strict monotonicity. M (*, *) continuous and M (x1 , x2 ) < M (x01 , x2 ) for x1 < x01 , and\n\u2013 Anonymity. M (M (x11 , x12 ), M (x21 , x22 ))\n=\nM (M (x11 , x21 ), M (x12 , x22 ))\n(also\ncalled\nbisymmetry expressing the fact that the mean\ncan be computed as a mean on the row means or\nequivalently as a mean on the column means).\nThen one can show that the mean function M (*, *) is\nnecessarily written as:\nM (x1 , x2 ) = f\n\n\u22121\n\n\u0012\n\nf (x1 ) + f (x2 )\n2\n\n\u0013\n\ndef\n\n= Mf (x1 , x2 ),\n\n(1)\n2\nfor a strictly increasing function f . The arithmetic x1 +x\n,\n2\n\u221a\n2\ngeometric x1 x2 and harmonic means 1 + 1 are inx1\n\nx2\n\nstances of such generalized means obtained for f (x) = x,\nf (x) = log x and f (x) = x1 , respectively. Those generalized means are also called quasi-arithmetic means, since\nthey can be interpreted as the arithmetic mean on the sequence f (x1 ), ..., f (xn ), the f -representation of numbers.\nTo get geometric centroids, we simply consider means\non each coordinate axis independently. The Euclidean\ncentroid is thus interpreted as the Euclidean arithmetic\nmean. Barycenters (weighted centroids) are similarly\nobtained\nusing non-negative weights (normalized so that\nPn\nw\ni=1 i = 1):\n\nMf (x1 , ..., xn ; w1 , ..., wn ) = f\n\n\u22121\n\nn\nX\n\n!\nwi f (xi )\n\n(2)\n\ni=1\n\nThose generalized means satisfy the inequality property:\nMf (x1 , ..., xn ; w1 , ..., wn ) \u2264 Mg (x1 , ..., xn ; w1 , ..., wn ),\n(3)\nif and only if function g dominates f : That is, \u2200x, g(x) >\nf (x). Therefore the arithmetic mean (f (x) = x) dominates the geometric mean (f (x) = log x) which in turn\ndominates the harmonic mean f (x) = x1 . Note that it\nis not a strict inequality in Eq. 3 as the means coincide\nfor all identical elements: if all xi are equal to x then\nMf (x1 , ..., xn ) = f \u22121 (f (x)) = x = g \u22121 (g(x)) =\nMg (x1 , ..., xn ). All those quasi-arithmetic means further\nsatisfy the \"interness\" property\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n\u2022\n\n2\n\nmin(x1 , ..., xn ) \u2264 Mf (x1 , ..., xn ) \u2264 max(x1 , ..., xn ),\n(4)\nderived from limit cases p \u2192 \u00b1\u221e of power means1 for\nf (x) = xp , p \u2208 R\u2217 = (\u2212\u221e, \u221e)\\{0}, a non-zero real\nnumber.\nBy optimization. In this second alternative approach, the\nbarycenter c is defined according to a distance function\nd(*, *) as the optimal solution of a minimization problem\n(OPT) : min\nx\n\nn\nX\ni=1\n\nwi d(x, pi ) = min L(x; P, d),\nx\n\n(5)\n\nwhere the non-negative weights wi denote multiplicity\nor relative importance of points (by default, the centroid\nis defined by fixing all wi = n1 ). Ben-Tal et al. [4]\nconsidered an information-theoretic class of distances\ncalled f -divergences [5], [6]:\n\u0012 \u0013\nx\n,\n(6)\nIf (x, p) = pf\np\nfor a strictly convex differentiable function f (*) satisfying\nf (1) = 0 and f 0 (1) = 0. Although those f -divergences\nwere primarily investigated for probability measures,2 we\ncan extend the f -divergence to positive measures. Since\nprogram (OPT) is strictly convex in x, it admits a unique\nminimizer M (P; If ) = arg minx L(x; P, If ), termed the\nentropic mean by Ben-Tal et al. [4]. Interestingly, those\nentropic means are linear scale-invariant:3\nM (\u03bbp1 , ..., \u03bbpn ; If ) = \u03bbM (p1 , ..., pn ; If )\n\n(7)\n\nNielsen and Nock [7] considered another class of\ninformation-theoretic distortion measures BF called\nBregman divergences [8], [9]:\nBF (x, p) = F (x) \u2212 F (p) \u2212 (x \u2212 p)F 0 (p),\n\n(8)\n\nfor a strictly convex differentiable function F . It follows\nthat (OPT) is convex, and admits a unique minimizer\nM (p1 , ..., pn ; BF ) = MF 0 (p1 , ..., pn ), a quasi-arithmetic\nmean for the strictly increasing and continuous function F 0 , the derivative of F . Observe that informationtheoretic distances may be asymmetric (i.e., d(x, p) 6=\nd(p, x)), and therefore one may also define a right-sided\ncentroid M 0 as the minimizer of\n(OPT0 ) : min\nx\n\nn\nX\n\nwi d(pi , x),\n\n(9)\n\ni=1\n\nIt turns out that for f -divergences, we have:\nIf (x, p) = If \u2217 (p, x),\n1 Besides\n\n(10)\n\nthe min/max operators interpreted as extremal power means, the\n1\nQ\np p\ngeometric mean itself can also be interpreted as a power mean ( n\ni=1 xi )\nin the limit case p \u2192 0.\n2 In that context, a d-dimensional point is interpreted as a discrete and finite\nprobability measure lying in the (d \u2212 1)-dimensional unit simplex.\n3 That is, means of homogeneous degree 1.\n\nfor f \u2217 (x) = xf (1/x) so that (OPT') is solved as a (OPT)\nproblem for the conjugate function f \u2217 (*). In the same\nspirit, we have:\nBF (x, p) = BF \u2217 (F 0 (p), F 0 (x))\n\n(11)\n\nfor Bregman divergences, where F \u2217 denotes the Legendre\nconvex conjugate [8], [9].4 Surprisingly, although (OPT')\nmay not be convex in x for Bregman divergences (e.g.,\nF (x) = \u2212 log x), (OPT') admits nevertheless a unique\nminimizer, independent of thePgenerator function F : the\nn\ncenter of mass M 0 (P; BF ) = i=1 n1 pi . Bregman means\nare not homogeneous except for the power generators\nF (x) = xp which yields entropic means, i.e. means\nthat can also be interpreted5 as minimizers of average f divergences [4]. Amari [11] further studied those power\nmeans (known as \u03b1-means in information geometry [12]),\nand showed that they are linear-scale free means obtained as minimizers of \u03b1-divergences, a proper subclass of f -divergences. Nielsen and Nock [13] reported\nan alternative simpler proof of \u03b1-means by showing\nthat the \u03b1-divergences are Bregman divergences in disguise (namely, representational Bregman divergences for\npositive measures, but not for normalized distribution\nmeasures [10]). To get geometric centroids, we simply\nconsider multivariate extensions of the optimization task\n(OPT). In particular, one may consider separable divergences that are divergences that can be assembled\ncoordinate-wise:\nd(x, p) =\n\nd\nX\n\ndi (x(i) , p(i) ),\n\n(12)\n\ni=1\n\nwith x(i) denoting the ith coordinate. A typical non\nseparable divergence is the squared Mahalanobis distance [14]:\nd(x, p) = (x \u2212 p)T Q(x \u2212 p),\n\n(13)\n\na Bregman divergence called generalized quadratic distance, defined for the generator F (x) = xT Qx, where\nQ is a positive-definite matrix (Q \u001f 0). For separable\ndistances, the optimization problem (OPT) may then be\nreinterpreted as the task of finding the projection [15] of\na point p (of dimension d \u00d7 n) to the upper line U :\n(PROJ) : inf d(u, p)\nu\u2208U\n\n(14)\n\nwith u1 = ... = ud\u00d7n > 0, and p the (n\u00d7d)-dimensional\npoint obtained by stacking the d coordinates of each of\nthe n points.\nIn geometry, means (centroids) play a crucial role in centerbased clustering (i.e., k-means [16] for vector quantization\napplications). Indeed, the mean of a cluster allows one to\naggregate data into a single center datum. Thus the notion\n4 Legendre dual convex conjugates F and F \u2217 have necessarily reciprocal\ngradients: F \u22170 = (F 0 )\u22121 . See [7].\n5 In fact, Amari [10] proved that the intersection of the class of f divergences with the class of Bregman divergences are \u03b1-divergences.\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n3\n\nof means are encapsulated into the broader theory of mathematical aggregators [17].\nResults on geometric means can be easily transfered to the\nfield of Statistics [4] by generalizing the optimization problem\ntask to a random variable X with distribution F as:\nZ\n(OPT) : min E[Xd(x, X)] = min\nx\n\nx\n\ntd(x, t)dF (t),\n\n(15)\n\nt\n\nwhere E[*] denotes the expectation defined with respect to\nthe Lebesgue-Stieltjes integral. Although this approach is\ndiscussed in [4] and important for defining various notions\nof centrality in statistics, we shall not cover this extended\nframework here, for sake of brevity.\nB. Burbea-Rao divergences\nIn this paper, we focus on the optimization approach\n(OPT) for defining other (geometric) means using the class of\ninformation-theoretic distances obtained by Jensen difference\nfor a strictly convex and differentiable function F :\nd(x, p) =\n\nF (x) + F (p)\n\u2212F\n2\n\n\u0012\n\nx+p\n2\n\n\u0013\n\ndef\n\n= BRF (x, p) \u2265 0.\n\n(16)\nSince the underlying differential geometry implied by those\nJensen difference distances have been seminally studied in\npapers of Burbea and Rao [18], [19], we shall term them\nBurbea-Rao divergences, and point out to them as BRF . In\nthe remainder, we consider separable Burbea-Rao divergences.\nThat is, for d-dimensional points p and q, we define\nBRF (p, q) =\n\nd\nX\n\nBRF (p(i) , q (i) ),\n\n(17)\n\ni=1\n\nand study the Burbea-Rao centroids (and barycenters) as the\nminimizers of the average Burbea-Rao divergences. Those\nBurbea-Rao divergences generalize the celebrated JensenShannon divergence [20]\n\u0012\n\u0013\np+q\nH(p) + H(q)\nJS(p, q) = H\n\u2212\n(18)\n2\n2\nby choosing F (x) = \u2212H(x), the negative Shannon entropy\nH(x) = \u2212x log x. Generators F (*) of parametric distances\nare convex functions representing entropies which are concave\nfunctions. Burbea-Rao divergences contain all generalized\nquadratic distances (F (x) = xT Qx = hQx, xi for a positive\ndefinite matrix Q \u001f 0, also called squared Mahalanobis\ndistances):\n\nBRF (p, q)\n\n=\n=\n=\n=\n\n\u0012\n\u0013\nF (p) + F (q)\np+q\n\u2212F\n2\n2\n2hQp, pi + 2hQq, qi \u2212 hQ(p + q), p + qi\n4\n1\n(hQp, pi + hQq, qi \u2212 2hQp, qi)\n4\n1\n1\nhQ(p \u2212 q), p \u2212 qi = kp \u2212 qk2Q .\n4\n4\n\nAlthough the square root of the Jensen-Shannon divergence yields a metric (a Hilbertian metric), it is not true\nin general for Burbea-Rao divergences. The closest work to\nour paper is a 1-page symposium6 paper [21] discussing\nabout Ali-Silvey-Csisz\u00e1r f -divergences [5], [6] and Bregman\ndivergences [22], [8] (two entropy-based divergence classes).\nThose information-theoretic distortion classes are compared\nusing quadratic differential metrics, mean values and projections. The notion of skew Jensen differences intervene in the\ndiscussion.\nC. Contributions and paper organization\nThe paper is articulated into two parts: The first part studies\nthe Burbea-Rao centroids, and the second part shows some\napplications in Statistics. We summarize our contributions as\nfollows:\n\u2022 We define the parametric class of (skew) Burbea-Rao\ndivergences, and show that those divergences naturally\narise when generalizing the principle of the JensenShannon divergence [20] to Jensen-Bregman divergences.\nIn the limit cases, we further prove that those skew\nBurbea-Rao divergences yield asymptotically Bregman\ndivergences.\n\u2022 We show that the centroids with respect to the (skew)\nBurbea-Rao divergences are unique. Besides centroids\nfor special cases of Burbea-Rao divergences (including\nthe squared Euclidean distances), those centroids are\nnot available in closed-form equations. However, we\nshow that any Burbea-Rao centroid can be estimated\nefficiently using an iterative convex-concave optimization\nprocedure. As a by-product, we find Bregman sided\ncentroids [7] in closed-form in the extremal skew cases.\nWe then consider applications of Burbea-Rao centroids in\nStatistics, and show the link with Bhattacharyya distances. A\nwide class of statistical parametric models can be handled in\na unified manner as exponential families [23]. The classes of\nexponential families contain many of the standard parametric\nmodels including the Poisson, Gaussian, multinomial, and\nGamma/Beta distributions, just to name a few prominent\nmembers. However, only a few closed-form formulas for the\nstatistical Bhattacharyya distances between those densities are\nreported in the literature.7\nFor the second part, our contributions are reviewed as\nfollows:\n\u2022 We show that the (skew) Bhattacharyya distances calculated for distributions belonging to the same exponential\nfamily in statistics, are equivalent to (skew) BurbeaRao divergences. We mention corresponding closed-form\nformula for computing Chernoff coefficients and \u03b1divergences of exponential families. In the limit case, we\nobtain an alternative proof showing that the KullbackLeibler divergence of members of the same exponential\n6 In the nineties, the IEEE International Symposium on Information Theory\n(ISIT) published only 1-page papers. We are grateful to Prof. Mich\u00e8le\nBasseville for sending us the corresponding slides.\n7 For instance, the Bhattacharyya distance between multivariate normal\ndistributions is given here [24].\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\nfamily is equivalent to a Bregman divergence calculated\non the natural parameters [14].\n\u2022 We approximate iteratively the Bhattacharyya centroid of\nany set of distributions of the same exponential family\n(including multivariate Gaussians) using the Burbea-Rao\ncentroid algorithm. For the case of multivariate Gaussians, we design yet another tailored iterative scheme\nbased on matrix differentials, generalizing the former\nunivariate study of Rigazio et al. [25]. Thus we get either\nthe generic way or the tailored way for computing the\nBhattacharrya centroids of arbitrary Gaussians.\n\u2022 As a field application, we show how to simplify Gaussian mixture models using hierarchical clustering, and\nshow experimentally that the results obtained with the\nBhattacharyya centroids compare favorably well with\nformer results obtained for Bregman centroids [26]. Our\nnumerical experiments show that the generic method outperforms the alternative tailored method for multivariate\nGaussians.\nThe paper is organized as follows: In section II, we introduce Burbea-Rao divergences as a natural extension of the\nJensen-Shannon divergence using the framework of Bregman\ndivergences. It is followed by Section III which considers\nthe general case of skew divergences, and reveals asymptotic\nbehaviors of extreme skew Burbea-Rao divergences as Bregman divergences. Section IV defines the (skew) Burbea-Rao\ncentroids, show they are unique, and present a simple iterative\nalgorithm with guaranteed convergence. We then consider\napplications in Statistics in Section V: After briefly recalling\nexponential distributions in \u00a7V-A, we show that Bhattacharyya\ndistances and Chernoff/Amari \u03b1-divergences are available in\nclosed-form equations as Burbea-Rao divergences for distributions of the same exponential families. Section V-C presents\nan alternative iterative algorithm tailored to compute the Bhattacharyya centroid of multivariate Gaussians, generalizing the\nformer specialized work of Rigazio et al. [25]. In section V-D,\nwe use those Bhattacharyya/Burbea-Rao centroids to simplify\nhierarchically Gaussian mixture models, and comment both\nqualitatively and quantitatively our experiments on a color\nimage segmentation application. Finally, section VI concludes\nthis paper by describing further perspectives and hinting at\nsome information geometrical aspects of this work.\nII. B URBEA -R AO DIVERGENCES FROM SYMMETRIZATION\nOF B REGMAN DIVERGENCES\nLet R+ = [0, +\u221e) denote the set of non-negative reals. For\na strictly convex (and differentiable) generator F , we define\nthe Burbea-Rao divergence as the following non-negative\nfunction:\nBRF\n\n:\n\n(p, q) 7\u2192\n\nX \u00d7 X \u2192 R+\n\u0012\n\u0013\np+q\nF (p) + F (q)\n\u2212F\n\u22650\nBRF (p, q) =\n2\n2\n\nThe non-negative property of those divergences follows\nstraightforwardly from Jensen inequality. Although BurbeaRao distances are symmetric (BRF (p, q) = BRF (q, p)), they\n\n4\n\nF (p)+F (q)\n( p+q\n)\n2 ,\n2\n\n(p, F (p))\n\n(q, F (q))\n\nBRF (p, q)\np+q\n( p+q\n2 , F ( 2 ))\n\np\n\nq\n\np+q\n2\n\nFig. 1. Interpreting the Burbea-Rao divergence BRF (p, q) as the vertical\ndistance between the midpoint\nof segment\n\u0010\n\u0010\n\u0011\u0011[(p, F (p)), (q, F (q))] and the\nmidpoint of the graph plot p+q\n, F p+q\n.\n2\n2\n\nF\nHq0\np\u0302\nBF (p, q) = Hq \u2212 Hq0\nHq\n\nq\u0302\nq\n\np\n\nFig. 2. Interpreting the Bregman divergence BF (p, q) as the vertical distance\nbetween the tangent plane at q and its translate passing through p (with\nidentical slope \u2207F (q)).\n\nare not metrics since they fail to satisfy the triangle inequality.\nA geometric interpretation of those divergences is given in\nFigure 1. Note that F is defined up to an affine term ax + b.\nWe show that Burbea-Rao divergences extend the JensenShannon divergence using the broader concept of Bregman\ndivergences instead of the Kullback-Leibler divergence. A\nBregman divergence [22], [8], [9] BF is defined as the positive\ntail of the first-order Taylor expansion of a strictly convex and\ndifferentiable convex function F :\nBF (p, q) = F (p) \u2212 F (q) \u2212 hp \u2212 q, \u2207F (q)i,\n\n(19)\n\nwhere \u2207F denote the gradient of F (the vector of partial\n\u2202F\nderivatives { \u2202x\n}i ), and hx, yi = xT y the inner product (dot\ni\nproduct for vectors). A Bregman divergence is interpreted\ngeometrically [14] as the vertical distance between the tangent\nplane Hq at q of the graph plot F = {x\u0302 = (x, F (x)) |x \u2208 X }\nand its translates Hq0 passing through p\u0302 = (p, F (p)). Figure 2 depicts graphically the geometric interpretation of the\nBregman divergence (to be compared with the Burbea-Rao\ndivergence in Figure 1).\nBregman divergences are never metrics, and symmetric\nonly for the generalized quadratic distances [14] obtained by\nchoosing F (x) = xT Qx, for some positive definite matrix\nQ \u001f 0. Bregman divergences allow one to encapsulate both\nstatistical distances with geometric distances:\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n5\n\n\u2022\n\nKullback-Leibler divergence obtained for F (x) =\nx log x:\nd\nX\np(i)\n(20)\nKL(p, q) =\np(i) log (i)\nq\ni=1\n\n\u2022\n\nsquared Euclidean distance obtained for F (x) = x2 :\nL22 (p, q) =\n\nd\nX\ni=1\n\n(p(i) \u2212 q (i) )2 = kp \u2212 qk2\n\n(21)\n\n\u2022\n\nBF (p, q) + BF (q, p)\n(22)\n2\n1\n=\nhp \u2212 q, \u2207F (p) \u2212 \u2207F (q)i, (23)\n2\nExcept for the generalized quadratic distances, this symmetric distance cannot be interpreted as a Bregman\ndivergence [14].\nJensen-Bregman divergences. We consider the JeffreysBregman divergences from the source parameters to the\naverage parameter p+q\n2 as follows:\nJF (p; q)\n\n=\n=\n\n=\n\np+q\nBF (p, p+q\n2 ) + BF (q, 2 )\n(24)\n2\nF (p) + F (q)\np+q\n\u2212 F(\n) = BRF (p, q)\n2\n2\n\nNote that even for the negative Shannon entropy F (x) =\nx log x \u2212 x (extended to positive measures), those two symmetrizations yield different divergences: While SF uses the\ngradient \u2207F , JF relies only on the generator F . Both JF\nand SF have always finite values.8 The first symmetrization\napproach was historically studied by Jeffreys [29].\nThe second way to symmetrize Bregman divergences generalizes the spirit of the Jensen-Shannon divergence [20]\n\u0012\n\u0012\n\u0013\n\u0012\n\u0013\u0013\n1\np+q\np+q\nJS(p, q) =\nKL p,\n+ KL q,\n(25)\n,\n2\n2\n2\n\u0012\n\u0013\np+q\nH(p) + H(q)\n= H\n\u2212\n(26)\n2\n2\nwith non-negativity that can be derived from Jensen's inequality, hence its name. The Jensen-Shannon divergence is\nalso called the total divergence to the average, a generalized\nmeasure of diversity from the population distributions p and q\nto the average population p+q\n2 . Those Jensen difference-type\ndivergences are by definition Burbea-Rao divergences. For the\nShannon entropy, those two different information divergence\nsymmetrizations (Jensen-Shannon divergence and Jeffreys J\ndivergence) satisfy the following inequality:\nJ(p, q) \u2265 4 JS(p, q) \u2265 0.\n8 This\n\nIII. S KEW B URBEA -R AO DIVERGENCES\nWe further generalize Burbea-Rao divergences by introducing a positive weight \u03b1 \u2208 (0, 1) when averaging source\nparameters p and q as follows:\n(\u03b1)\n\nBasically, there are two ways to symmetrize Bregman divergences (see also work on Bregman metrization [27], [28]):\n\u2022 Jeffreys-Bregman divergences. We consider half of the\ndouble-sided divergences:\nSF (p; q)\n\nNielsen and Nock [7] investigated the centroids with respect\nto Jeffreys-Bregman divergences (the symmetrized KullbackLeibler divergence).\n\nBRF\n(\u03b1)\n\nBRF (p, q)\n\n:\n=\n\nX \u00d7 X \u2192 R+\n\n\u03b1F (p) + (1 \u2212 \u03b1)F (q) \u2212 F (\u03b1p + (1 \u2212 \u03b1)q)\n\nWe consider the open interval (0, 1) since otherwise the\ndivergence has no discriminatory power (indeed, for \u03b1 \u2208\n(\u03b1)\n{0, 1}, BRF (p, q) = 0, \u2200p, q). Although skewed divergences\n(\u03b1)\n(\u03b1)\nare asymmetric BRF (p, q) 6= BRF (q, p), we can swap\narguments by replacing \u03b1 by 1 \u2212 \u03b1:\n(\u03b1)\n\nBRF (p, q)\n\n=\n=\n\n\u03b1F (p) + (1 \u2212 \u03b1)F (q) \u2212 F (\u03b1p + (1 \u2212 \u03b1)q)\n(1\u2212\u03b1)\n\nBRF\n\n(q, p)\n\nThose skew Burbea-Rao divergences are similarly found using a skew Jensen-Bregman counterpart (the gradient terms\n\u2207F (\u03b1p + (1 \u2212 \u03b1)q) perfectly cancel in the sum of skew\nBregman divergences):\ndef\n\n\u03b1BF (p, \u03b1p + (1 \u2212 \u03b1)q) + (1 \u2212 \u03b1)BF (q, \u03b1p + (1 \u2212 \u03b1)q) =\n(\u03b1)\n\nBRF (p, q)\n(\u03b1)\n\nIn the limit cases, \u03b1 \u2192 0 or \u03b1 \u2192 1, we have BRF (p, q) \u2192\n0 \u2200p, q. That is, those divergences loose their discriminatory\npower at extremities. However, we show that those skew\nBurbea-Rao divergences tend asymptotically to Bregman divergences:\nBF (p, q)\nBF (q, p)\n\n1\n(\u03b1)\nBRF (p, q)\n\u03b1\n1\n(\u03b1)\n= lim\nBRF (p, q)\n\u03b1\u21921 1 \u2212 \u03b1\n=\n\nlim\n\n\u03b1\u21920\n\n(29)\n(30)\n\nThe limit in the right-hand-side of Eq. 30 can be expressed\nalternatively as the following one-sided limit:\nlim\n\u03b1\u21911\n\n1\n1\n(\u03b1)\n(\u03b1)\nBRF (p, q) = lim BRF (q, p),\n\u03b1\u21930 \u03b1\n1\u2212\u03b1\n\n(31)\n\nwhere the arrows \u2191 and \u2193 denote the limit from the left and\nthe limit from the right, respectively (see [30] for notations).\n0\n(x) =\nThe right derivative of a function f at x is defined as f+\n(0)\nf (y)\u2212f (x)\nlimy\u2193x y\u2212x . Since BRF (p, q) = 0 \u2200p, q, it follows that\nthe right-hand-side limit of Eq. 31 is the right derivative (see\nTheorem 1 of [30] that gives a generalized Taylor expansion\nof convex functions) of the map\n\n(27)\n\nmay not be the case of Bregman/Kullback-Leibler divergences that\ncan potentially be unbounded.\n\n(28)\n\n(\u03b1)\n\nL(\u03b1) : \u03b1 7\u2192 BRF (q, p)\ntaken at \u03b1 = 0. Thus we have\n\n(32)\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\nlim\n\u03b1\u21930\n\n1\n(\u03b1)\nBRF (q, p) = L0+ (0).,\n\u03b1\n\n6\n\n(33)\n\nwith\nL0+ (0)\n\n=\n=\n=\n\nd+\n(\u03b1F (q) + (1 \u2212 \u03b1)F (p) \u2212 F (\u03b1q + (1 \u2212 \u03b1)p))\nd\u03b1\nF (q) \u2212 F (p) \u2212 hq \u2212 p, \u2207F (p)i\n(34)\n\nBF (q, p)\n\nconcave functions is concave). We can thus solve iteratively\nthis optimization problem using the Convex-ConCave Procedure [32], [33] (CCCP), byP\nstarting from an initial position c0\nn\n(say, the barycenter c0 = i=1 wi pi ), and iteratively update\nthe barycenter as follows:\n\u2207F (ct+1 ) = Pn\n\nn\nX\n\n1\n\ni=1\n\nwi \u03b1i\n\n(35)\n\nLemma 1: Skew Burbea-Rao divergences tend asymptotically to Bregman divergences (\u03b1 \u2192 0) or reverse Bregman\ndivergences (\u03b1 \u2192 1).\nThus we may scale skew Burbea-Rao divergences so that\nBregman divergences belong to skew Burbea-Rao divergences:\n\nct+1 = \u2207F \u22121\n\ni=1\n\nn\nX\n\n1\nPn\n\ni=1\n\nwi \u03b1i \u2207F (\u03b1i ct + (1 \u2212 \u03b1i )pi )\n\nwi \u03b1i\n\ni=1\n\n(40) !\nwi \u03b1i \u2207F (\u03b1i ct + (1 \u2212 \u03b1i )pi )\n\n(41)\nSince F is convex, the second-order derivative \u22072 F is\nalways positive definite, and \u2207F is strictly monotone increasing. Thus we can interpret Eq. 41 as a fixed-point\nequation by considering the \u2207F -representation. Each iteration\n(\u03b1)\nsBRF (p, q) =\nis interpreted as a quasi-arithmetic mean. This proves that the\n1\nBurbea-Rao centroid is always well-defined and unique (see\n(\u03b1F (p) + (1 \u2212 \u03b1)F (q) \u2212 F (\u03b1p + (1 \u2212 \u03b1)q))\nAppendix A for a detailed proof), since there is (at most) a\n\u03b1(1 \u2212 \u03b1)\n(36) unique fixed point for x = g(x) with a function g(*) strictly\nmonotone increasing.\nMoreover, \u03b1 is now not anymore restricted to (0, 1) but\nIn some cases, like the squared Euclidean distance (or\nto the full real line: \u03b1 \u2208 R, as also noticed in [31]. Setting squared Mahalanobis distances), we find closed-form solutions\n0\n\u03b1 = 1\u2212\u03b1\n(that is, \u03b10 = 1 \u2212 2\u03b1), we get\n2\nfor the Burbea-Rao barycenters. For example, P\nconsider the\nd\n(i) 2\n(negative) quadratic entropy F (x) = hx, xi =\ni=1 (x )\n0\n1\n(\u03b1 )\nwith weights wi and all \u03b1i = 2 (non-skew symmetric BurbeasBRF (p, q) =\n\u0012\n\u0013\u0013\n\u0012\nRao\n0\n0\n0\n4\n1\u2212\u03b1\n1\u2212\u03b1\n1+\u03b1\n1 + \u03b10 divergences). We have:\nF\n(p)\n+\nF\n(q)\n\u2212\nF\np\n+\nq\n1 \u2212 \u03b102\n2\n2\n2\n2\n\u0012\n\u0013\nn\nX\npi + x\n(37)min E(x) = F (x) \u2212\nwi F\n,\n(42)\n2\n2\ni=1\nn\n\nIV. B URBEA -R AO CENTROIDS\nLet P = {p1 , ..., pn } denote a d-dimensional point set.\nTo each point, let us further associate a positive weight wi\n(accounting for arbitrary multiplicity) and a positive scalar\n(\u03b1 )\n\u03b1i \u2208 (0, 1) to define an anchored distance BRF i (*, pi ).\nDefine the skew Burbea-Rao9 barycenter (or centroid) c as\nthe minimizer of the following optimization task:\n\nOPT : c = arg min\nx\n\nn\nX\n\n(\u03b1i )\n\nwi BRF\n\ni=1\n\n(x, pi ) = arg min L(x)\nx\n\n(38)\nWithout loss of generality, we consider argument x on the\nleft argument position (otherwise, we change all \u03b1i \u2192 1\u2212\u03b1i to\nget the right-sided Burbea-Rao centroid). Removing all terms\nindependent of x, the minimization program (OPT) amounts\nto minimize equivalently the following energy function:\nn\nn\nX\nX\nE(c) = (\nwi \u03b1i )F (c) \u2212\nwi F (\u03b1i c + (1 \u2212 \u03b1i )pi ) (39)\ni=1\n\ni=1\n\nObserve that the energy P\nfunction is decomposable in the\nn\nsum of a convex\nfunction\n(\ni=1 wi \u03b1i )F (c) with a concave\nPn\nfunction \u2212 i=1 wi F (\u03b1i c + (1 \u2212 \u03b1i )pi ) (since the sum of n\n9 We\n\nalso call them skew Jensen barycenters or centroids since they are\ninduced by a divergence using the Jensen inequality.\n\n=\n\nmin\n\nhx, xi 1 X\nwi (hx, xi + 2hx, pi i + hpi , pi i)\n\u2212\n2\n4 i=1\n\nThe minimum is obtained\nPn when the gradient \u2207E(x) = 0,\nthat is when x = p\u0304 = i=1 wi pi , the barycenter of the point\nset P. For most Burbea-Rao divergences, Eq. 42 can only be\nsolved numerically.\nObserve that for extremal skew cases (for \u03b1 \u2192 0 or \u03b1 \u2192 1),\nwe obtain the Bregman centroids in closed-form solutions (see\nEq. 30). Thus skew Burbea-Rao centroids allow one to get a\nsmooth transition from the right-sided centroid (the center of\nmass) to the left-sided centroid (a quasi-arithmetic mean Mf\nobtained for f = \u2207F , a continuous and strictly increasing\nfunction).\nTheorem 1: Skew Burbea-Rao centroids are unique. They\ncan be estimated iteratively using the CCCP iterative algorithm. In extremal skew cases, the Burbea-Rao centroids tend\nto Bregman left/right sided centroids, and have closed-form\nequations in limit cases.\nTo describe the orbit of Burbea-Rao centroids linking the\nleft to right sided Bregman centroids, we compute for \u03b1 \u2208\n[0, 1] the skew Burbea-Rao centroids with the following update\nscheme:\n\nct+1 = \u2207F\n\n\u22121\n\nn\nX\ni=1\n\n!\nwi \u2207F (\u03b1ct + (1 \u2212 \u03b1)pi )\n\n(43)\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n7\n\nWe may further consider various convex generators Fi for\neach point, and consider the updating scheme\n\nThe vector t(x) denote the sufficient statistics, that is the\nset of linear independent functions that allows to concentrate\nwithout any loss all information about the parameter \u03b8 carried\nct+1 =\n!\u22121\nin! the iid. observations x1 , x2 , ..., . The inner product hp, qi is\nn\nX\nX\n1\nPn\nwi \u03b1i \u2207Fi (\u03b1i ct + (1 \u2212 \u03b1i )pi )defined according to the primitive type of \u03b8. Namely, it is a\nwi \u2207Fi\ni=1 wi \u03b1i i=1\ni\nmultiplication hp, qi = pq for scalars, a dot product hp, qi =\npT q for vectors, a matrix trace hp, qi = tr(pT \u00d7q) = tr(p\u00d7q T )\nfor matrices, etc. For composite types such as p being defined\nby both a vector part and a matrix part, the composite inner\nA. Burbea-Rao divergences of a population\nproduct is defined as the sum of inner products on the primitive\nConsider now the Burbea-Rao divergence of a populatypes. Finally, k(x) represents the carrier measure according to\ntion p1 , ..., pn with respective positive normalized weights\nthe counting or Lebesgue measures. Decompositions for most\nw1 , ..., wn . The Burbea-Rao divergence is defined by:\ncommon exponential family distributions are given in [23].\nAn exponential family EF = {pF (x; \u03b8) |\u03b8 \u2208 \u0398} is the set of\nn\nn\nX\nX\nprobability distributions obtained for the same log-normalizer\nw\nBRF (p1 , ..., pn ) =\nwi F (pi ) \u2212 F (\nwi pi ) \u2265 0 (44)\nfunction F . Information geometry considers EF as a manifold\ni=1\ni=1\nentity, and study its differential geometric properties [12].\nThis family of diversity measures includes the JensenFor example, consider the family of Poisson distributions\nR\u00e9nyi divergencesP\n[34], [35] for F (x) = \u2212R\u03b1 (x), where E with mass function:\nF\nd\n1\nlog j=1 p\u03b1\nR\u03b1 (x) = 1\u2212\u03b1\nj is the R\u00e9nyi entropy of order \u03b1.\n\u03bbx\n(R\u00e9nyi entropy is concave for \u03b1 \u2208 (0, 1) and tend to Shannon\nexp(\u2212\u03bb),\n(47)\np(x; \u03bb) =\nentropy for \u03b1 \u2192 1.)\nx!\nV. B HATTACHARYYA DISTANCES AS B URBEA -R AO\nDISTANCES\n\nWe first briefly recall the versatile class of exponential family distributions in Section V-A. Then we show in Section V-B\nthat the statistical Bhattacharyya/Chernoff distances between\nexponential family distributions amount to compute a BurbeaRao divergence.\nA. Exponential family distribution in Statistics\nMany usual statistical parametric distributions p(x; \u03bb) (e.g.,\nGaussian, Poisson, Bernoulli/multinomial, Gamma/Beta, etc.)\nshare common properties arising from their common canonical\ndecomposition of probability distribution [9]:\n\nfor x \u2208 N+ = N \u222a {0} a positive integer. Poisson distributions\nare univariate exponential families (x \u2208 N+ ) of order 1\n(parameter \u03bb). The canonical decomposition yields\n\u2022 the sufficient statistic t(x) = x,\n\u2022 \u03b8 = log \u03bb, the natural parameter,\n\u2022 F (\u03b8) = exp \u03b8, the log-normalizer,\n\u2022 and k(x) = \u2212 log x! the carrier measure (with respect to\nthe counting measure).\nSince we deal with applications using multivariate normals in the following, we also report explicitly that canonical decomposition for the multivariate Gaussian family\n{pF (x; \u03b8) |\u03b8 \u2208 \u0398}. We rewrite the usual Gaussian density\nof mean \u03bc and variance-covariance matrix \u03a3:\np(x; \u03bb)\n\np(x; \u03bb) = pF (x; \u03b8) = exp (ht(x), \u03b8i \u2212 F (\u03b8) + k(x)) . (45)\n10\n\n=\n=\n\np(x; \u03bc, \u03a3)\n(48)\n\u0012\n\u0013\n1\n(x \u2212 \u03bc)T \u03a3\u22121 (x \u2212 \u03bc))\n\u221a\nexp \u2212\n(49)\n2\n2\u03c0 det \u03a3\n\nThose distributions are said to belong to the exponential\nfamilies (see [23] for a tutorial). An exponential family is\ncharacterized by its log-normalizer F (\u03b8), and a distribution in\nthat family by its natural parameter \u03b8 belonging to the natural\nspace \u0398. The log-normalizer F is strictly convex and C \u221e , and\ncan also be expressed using the source coordinate system \u03bb\nusing the 1-to-1 map \u03c4 : \u039b \u2192 \u0398 that converts parameters\nfrom the source coordinate system \u03bb to the natural coordinate\nsystem \u03b8:\n\nin the canonical form of Eq. 45 with,\n\u22121\n\u2022 \u03b8 = (\u03a3\n\u03bc, 12 \u03a3\u22121 ) \u2208 \u0398 = Rd \u00d7 Kd\u00d7d , with Kd\u00d7d\ndenotes the cone of positive definite matrices,\n\u22121\n1\n1\nd\nT\n\u2022 F (\u03b8) = 4 tr(\u03b82 \u03b81 \u03b81 ) \u2212 2 log det \u03b82 + 2 log \u03c0,\nT\n\u2022 t(x) = (x, \u2212x x),\n\u2022 k(x) = 0.\nIn this case, the inner product is composite and is calculated\nas the sum of a dot product and a matrix trace as follows:\n\nF (\u03b8) = F (\u03c4 (\u03bb)) = (F \u25e6 \u03c4 )(\u03bb) = F\u03bb (\u03bb),\n\nh\u03b8, \u03b80 i = \u03b81T \u03b810 + tr(\u03b82T \u03b820 ).\n\n(46)\n\nwhere F\u03bb = F \u25e6 \u03c4 denotes the log-normalizer function\nexpressed using the \u03bb-coordinates instead of the natural \u03b8coordinates.\n10 The distributions can either be discrete or continuous. We do not introduce\nthe unifying framework of probability measures in order to not burden the\npaper.\n\n(50)\n\nThe coordinate transformation \u03c4 : \u039b \u2192 \u0398 is given for \u03bb =\n(\u03bc, \u03a3) by\n\u0013\n\u0012\n1 \u22121\n\u22121\n,\n(51)\n\u03c4 (\u03bb) = \u03bb2 \u03bb1 , \u03bb2\n2\nand its inverse mapping \u03c4 \u22121 : \u0398 \u2192 \u039b by\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n\u03c4\n\n\u22121\n\n\u0012\n(\u03b8) =\n\n\u0013\n1 \u22121\n1 \u22121\n.\n\u03b8 \u03b81 , \u03b82\n2 2\n2\n\n8\n\n(52)\n\nB\u03b1 (p, q)\n\nB. Bhattacharyya/Chernoff coefficients and \u03b1-divergences as\nskew Burbea-Rao divergences\nFor arbitrary probability distributions p(x) and q(x) (parametric or not), we measure the amount of overlap between\nthose distributions using the Bhattacharyya coefficient [36]:\nZ p\np(x)q(x)dx,\n(53)\nC( p, q) =\nClearly, the Bhattacharyya coefficient (measuring the affinity\nbetween distributions [37]) falls in the unit range:\n\nZ\n\np\u03b1 (x)q 1\u2212\u03b1 (x)dx = \u2212 ln C\u03b1 (p, q)\n(58)\n\u0012\n\u0013\u03b1\nZ\np(x)\ndx\n(59)\n= \u2212 ln q(x)\nq(x)\nx\n= \u2212 ln Eq [L\u03b1 (x)]\n(60)\n=\n\n\u2212 ln\n\nx\n\ndefined for some \u03b1 \u2208 (0, 1) (the Bhattacharyya divergence\nis obtained for \u03b1 = 21 ), where E[*] denote the expectation, and L(x) = p(x)\nq(x) the likelihood ratio. The term\nR \u03b1\n1\u2212\u03b1\np\n(x)q\n(x)dx\nis\ncalled the Chernoff coefficient. The\nx\nBhattacharyya/Chernoff distance of members of the same\nexponential family yields a weighted asymmetric Burbea-Rao\ndivergence (namely, a skew Burbea-Rao divergence):\n(\u03b1)\n\n0 \u2264 C(p, q) \u2264 1.\n\nIn fact, we\np may interpret\np this coefficient geometrically by considering p(x) and q(x) as unit vectors. The Bhattacharyya\ndistance is then the dot product, representing the cosine of\nthe angle made by the two unit vectors. The Bhattacharyya\ndistance B : X \u00d7 X \u2192 R+ is derived from its coefficient [36]\nas\nB(p, q) = \u2212 ln C(p, q).\n\nsuch that 0 \u2264 H(p, q) \u2264 1. It follows that\n\n(56)\n\n(61)\n\nwith\n(\u03b1)\n\nBRF (\u03b8p , \u03b8q ) = \u03b1F (\u03b8p )+(1\u2212\u03b1)F (\u03b8q )\u2212F (\u03b1\u03b8p +(1\u2212\u03b1)\u03b8q )\n(62)\nChernoff coefficients are also related to \u03b1-divergences, the\ncanonical divergences in \u03b1-flat spaces in information geometry [12] (p. 57):\n\n(55)\n\nThe Bhattacharyya distance allows one to get both upper\nand lower bound the Bayes' classification error [38], [39],\nwhile there are no such results for the symmetric KullbackLeibler divergence. Both the Bhattacharyya distance and the\nsymmetric Kullback-Leibler divergence agrees with the Fisher\ninformation at the infinitesimal level. Although the Bhattacharyya distance is symmetric, it is not a metric. Nevertheless, it can be metrized by transforming it into to the following\nHellinger metric [40]:\ns Z\np\np\n1\nH(p, q) =\n( p(x) \u2212 q(x))2 dx,\n2\n\nB\u03b1 (pF (x; \u03b8p ), pF (x; \u03b8q )) = BRF (\u03b8p , \u03b8q )\n\n(54)\n\nD\u03b1 (p||q) =\n\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\n4\n1\u2212\u03b12\n\n\u0010\n\n1\u2212\n\nR\n\np(x)\n\n1\u2212\u03b1\n2\n\nq(x)\n\n1+\u03b1\n2\n\n\u0011\ndx ,\n\nR\np(x) log p(x)\ndx = KL(p, q),\nq(x)\n\uf8f4\nR\n\uf8f4\n\uf8f3 q(x) log q(x) dx = KL(q, p),\np(x)\n\n\u03b1 6= \u00b11,\n\u03b1 = \u22121,\n\u03b1 = 1,\n(63)\n\nThe class of \u03b1-divergences satisfy the following reference\nduality: D\u03b1 (p||q) = D\u2212\u03b1 (q||p). Remapping \u03b10 = 1\u2212\u03b1\n(\u03b1 =\n2\n1 \u2212 2\u03b10 ), we transform Amari \u03b1-divergences to Chernoff \u03b10 divergences:12\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\n1\n\u03b10 (1\u2212\u03b10 )\n\n\u0010\n\n1\u2212\n\nR\n\n\u0011\n0\n0\np(x)\u03b1 q(x)1\u2212\u03b1 dx ,\n\nR\nD\u03b10 (p, q) =\np(x) log p(x)\ndx = KL(p, q),\nq(x)\n\uf8f4\n\uf8f4\n\uf8f3 R q(x) log q(x) dx = KL(q, p),\np(x)\n\n\u03b10 6\u2208 {0, 1},\n\u03b10 = 1,\n\u03b10 = 0,\n(64)\n\nTheorem 2: The Chernoff \u03b10 -divergence (\u03b1 6= \u00b11) of\ndistributions belonging to the same exponential family is\ngiven in closed-form by means of a skewed0 Burbea-Rao\n\u0013 divergence as: D\u03b10 (p, q) = 0 1 0 (1 \u2212 e\u2212BR\u03b1F (\u03b8p ,\u03b8q ) ), with\n\u03b1 (1\u2212\u03b1 )\n\nH(p, q) =\ns \u0012Z\nZ\nZ p\np\n1\n(\u03b1)\np(x)dx + q(x)dx \u2212 2\np(x) q(x)dx\nBRF (\u03b8p , \u03b8q ) = (\u03b1F (\u03b8p ) \u2212 (1 \u2212 \u03b1)F (\u03b8q )) \u2212 F (\u03b1\u03b8p \u2212\n2\np\n(1 \u2212 \u03b1)\u03b8q ). Amari \u03b1-divergence for members of the same\n=\n1 \u2212 C(p, q).\n(57) exponential families amount to compute D (p, q) = 4 (1\u2212\n\u03b1\n1\u2212\u03b12\n1\u2212\u03b1\n(\n2 )\n(\u03b8p ,\u03b8q )\nHellinger metric is also called Matusita metric [37] in the e\u2212BRF\n)\nliterature. The thesis of Hellinger was emphasized in the work\nWe get the following theorem for Bhattacharyya/Chernoff\nof Kakutani [41].\ndistances:\nWe consider a direct generalization of Bhattacharyya coef12 Chernoff coefficients are also related to R\u00e9nyi \u03b1-divergence generalizing\nR\nficients and divergences called Chernoff divergences11\n1\n\u03b1 1\u2212\u03b1\n11 In\nthe literature,R Chernoff information is also defined\nas\n\u2212 log inf \u03b1\u2208[0,1] p\u03b1 (x)q 1\u2212\u03b1 (x)dx.\nSimilarly,\nChernoff\ncoefficients R C\u03b1 (p, q) are defined as the supremum: C\u03b1 (p, q) =\n\u03b1\n1\u2212\u03b1\nsup\u03b1\u2208[0,1] p (x)q\n(x)dx.\n\nthe Kullback-Leibler divergence: R\u03b1 (p||q) = \u03b1\u22121 log x p(x) q\n(x)dx\nR \u03b1\n1\n\u03b1 (p) =\nbuilt on R\u00e9nyi entropy HR\nlog(\np\n(x)dx\n\u2212\n1).\nThe\nTsallis\nx\n1\u2212\u03b1\nR\n1\nentropy HT\u03b1 (p) = \u03b1\u22121\n(1\u2212 p(x)\u03b1 dx) can also be obtained from the R\u00e9nyi\n\u03b1\n1\nentropy (and vice-versa) via the mappings: HT\u03b1 (p) = 1\u2212\u03b1\n(e(1\u2212\u03b1)HR (p) \u2212\n1\n\u03b1\n\u03b1\n1) and HR (p) = 1\u2212\u03b1 log(1 + (1 \u2212 \u03b1)HT (p)).\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n9\n\nLet us compute the Chernoff coefficient for distributions belonging to the same exponential families. Without loss of generality,\nlet us consider the reduced canonical form of exponential families pF (x; \u03b8) = exphx, \u03b8i \u2212 F (\u03b8). Chernoff coefficients C\u03b1 (p, q)\nof members p = pF (x; \u03b8p ) and q = pF (x; \u03b8q ) of the same exponential family EF :\nZ\nZ\n(\u03b1)\nC\u03b1 (p, q) =\np\u03b1 (x)q 1\u2212\u03b1 (x)dx = pF (x; \u03b8p )p1\u2212\u03b1\n(x; \u03b8q )dx\nF\nZ\n=\nexp(\u03b1(hx, \u03b8p i \u2212 F (\u03b8p ))) \u00d7 exp((1 \u2212 \u03b1)(hx, \u03b8q i \u2212 F (\u03b8q )))dx\nZ\n=\nexp (hx, \u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q i \u2212 (\u03b1F (\u03b8p ) + (1 \u2212 \u03b1)F (\u03b8q )) dx\nZ\n= exp \u2212(\u03b1F (\u03b8p ) + (1 \u2212 \u03b1)F (\u03b8q )) \u00d7 exp (hx, \u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q i \u2212 F (\u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q ) + F (\u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q )) dx\nZ\n= exp (F (\u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q ) \u2212 (\u03b1F (\u03b8p ) + (1 \u2212 \u03b1)F (\u03b8q )) \u00d7 exphx, \u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q i \u2212 F (\u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q )dx\nZ\n= exp (F (\u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q ) \u2212 (\u03b1F (\u03b8p ) + (1 \u2212 \u03b1)F (\u03b8q )) \u00d7 pF (x; \u03b1\u03b8p + (1 \u2212 \u03b1)\u03b8q )dx\n{z\n}\n|\n=1\n\n=\n\n(\u03b1)\nexp(\u2212BRF (\u03b8p , \u03b8q ))\n\n\u2265 0.\n\nTheorem 3: The\nskew\nBhattacharyya\ndivergence\nB\u03b1 (p, q) is equivalent to the Burbea-Rao divergence\nfor\nmembers\nof\nthe\nsame\nexponential\nfamily\nEF : B\u03b1 (p, q)\n=\nB\u03b1 (pF (x; \u03b8p ), pF (x; \u03b8q ))\n=\n(\u03b1)\n\u2212 log C\u03b1 (pF (x; \u03b8p ), pF (x; \u03b8q )) = BRF (\u03b8p , \u03b8q ) \u2265 0.\nIn particular, for \u03b1 = \u00b11, the Kullback-Leibler divergence\nof those exponential family distributions amount to compute\na Bregman divergence [14] (by taking the limit as \u03b1 \u2192 1 or\n\u03b1 \u2192 0).\nCorollary 1: In the limit case \u03b10 \u2208 {0, 1}, the \u03b10 divergences amount to compute a Kullback-Leibler divergence, and is equivalent to compute a Bregman divergence\nfor the log-normalized on the swapped natural parameters:\nKL(pF (x; \u03b8p ), pF (x; \u03b8q )) = BF (\u03b8q , \u03b8p ).\nProof: The proof relies on the equivalence of BurbeaRao divergences to Bregman divergences for extremal values\nof \u03b1 \u2208 {0, 1}.\nKL(p, q)\n\n=\n\nKL(pF (x; \u03b8p ), pF (x; \u03b8q ))\n\n(65)\n\n=\n\nlim\nD\u03b10 (pF (x; \u03b8p ), pF (x; \u03b8q ))\n0\n\n(66)\n\n=\n\n=\n\n=\n\n\u03b1 \u21921\n\nC. Direct method for calculating the Bhattacharyya centroids\nof multivariate normals\nTo the best of our knowledge, the Bhattacharyya centroid\nhas only been studied for univariate Gaussian or diagonal\nmultivariate Gaussian distributions [42] in the context of\nspeech recognition, where it is reported that it can be estimated\nusing an iterative algorithm (no convergence guarantees are\nreported in [42]).\nIn order to compare this scheme on multivariate data with\nour generic Burbea-Rao scheme, we extend the approach of\nRigazio et al. [42] to multivariate Gaussians. Plugging the\nBhattacharyya distance of Gaussians in the energy function\nof the optimization problem (OPT), we get\n\u0012\n\u0013\u22121\nn\nX\n1\n\u03a3c + \u03a3 i\nT\nL(c) =\n(\u03bcc \u2212 \u03bci )\n(\u03bcc \u2212 \u03bci )\n8\n2\ni=1\n\u0001 !\ni\ndet \u03a3c +\u03a3\n1\n2\n+\n.\n(69)\nlog \u221a\n2\ndet \u03a3c det \u03a3i\nThis is equivalent to minimize the following energy:\nn\nX\nT\n\u22121\nF (c) =\n(\u03bcc \u2212 \u03bci ) (\u03a3c + \u03a3i ) (\u03bcc \u2212 \u03bci )\ni=1\n\n1\n+ 2 log (det(\u03a3c + \u03a3i )) \u2212 log (det \u03a3c )\nlim\n(1 \u2212 C\u03b1 (pF (x; \u03b8p ), pF (x; \u03b8q )))\n\u0001\n\u03b10 \u21921 \u03b10 (1 \u2212 \u03b10 )\n|\n{z\n}\n\u2212 log 22d det \u03a3i .\n(70)\nsince exp x'x'0 1+x\nIn order to minimize F (c), let us differentiate with respect to\n1\n\u22121\n\u03b10\nlim\nBR\n(\u03b8\n,\n\u03b8\n)\n(67)\n\u03bcc . let Ui denote (\u03a3c + \u03a3i ) . Using matrix differentials [43]\np\nq\nF\n0\n0\n\u03b10 \u21921 \u03b1 (1 \u2212 \u03b1 ) |\n{z\n}\n(p.10 Eq. 73), we get:\n(1\u2212\u03b10 )BF (\u03b8q ,\u03b8p )\nn\nX\n\u0002\n\u0003\n\u2202L\n1\nT\n=\nU\n+\nU\n[\u03bcc \u2212 \u03bci ]\n(71)\nB\n(\u03b8\n,\n\u03b8\n)\n=\nB\n(\u03b8\n,\n\u03b8\n)\n(68)\nlim\ni\nF\nq\np\nF\nq\np\ni\n0\n\u03b10 \u21921 \u03b1\n\u2202\u03bcc\ni=1\n\nSimilarly, we have lim\u03b10 \u21920 D\u03b10 (pF (x; \u03b8p ), pF (x; \u03b8q )) =\nKL(pF (x; \u03b8q ), pF (x; \u03b8p )) = BF (\u03b8p , \u03b8q ).\nTable I reports the Bhattacharyya distances for members of\nthe same exponential families.\n\nThen one can estimate iteratively \u03bcc , since Ui depends on\n\u03a3c which is unknown. We update \u03bcc as follows:\n\" n\n#\u22121 \" n\n#\nX\u0002\nX\u0002\n\u0003\n\u0003\nT\nT\n\u03bcc (t + 1) =\nUi + Ui\nUi + Ui \u03bci (72)\ni=1\n\ni=1\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n\u03c4 :\u03bb\u2192\u03b8\n\nExponential family\n\n10\n\nF (\u03b8) (up to a constant)\nPd\u22121\nlog(1 + i=1\nexp \u03b8i )\n\npi\n)\npd i\n\nMultinomial\n\n(log\n\nPoisson\n\nlog \u03bb\n\nexp \u03b8\n\nGaussian\n\n(\u03b81 = \u03bc, \u03b82 = \u03c3 2 )\n\n\u2212 4\u03b81 +\n\nMultivariate Gaussian\n\n(\u03b8 = \u03a3\u22121 \u03bc, \u0398 =\n\n1 \u22121\n\u03a3 )\n2\n\n\u03b82\n\n2\n\n1\n2\n\n2\n1 (\u03bcp \u2212\u03bcq )\n2 +\u03c3 2\n4 \u03c3p\nq\n\nlog(\u2212 \u03b8\u03c0 )\n2\n\n1\ntr(\u0398\u22121 \u03b8\u03b8 T )\n4\n\n\u2212\n\n1\n2\n\nBhattacharyya/Burbea-Rao BRF (\u03bbp , \u03bbq ) = BRF (\u03c4 (\u03bbp ), \u03c4 (\u03bbq ))\nP\n\u221a\n\u2212 ln di=1 pi qi\n\u221a\n\u221a\n1\n( \u03bcp \u2212 \u03bcq )2\n2\n\nlog det \u0398\n\n1\n(\u03bcp\n8\n\n+ 21 ln\n\u0010\n\n\u2212 \u03bcq )T\n\n2\n2\n+\u03c3q\n\u03c3p\n2\u03c3p \u03c3q\n\n\u03a3p +\u03a3q\n2\n\n\u0011\u22121\n\n\u03a3 +\u03a3\n\n(\u03bcp \u2212 \u03bcq ) +\n\n1\n2\n\nln\n\ndet p 2 q\ndet \u03a3p det \u03a3q\n\nTABLE I\nC LOSED - FORM B HATTACHARYYA DISTANCES FOR SOME CLASSES OF EXPONENTIAL FAMILIES ( EXPRESSED IN SOURCE PARAMETERS FOR EASE OF\nUSE )).\n\nNow let us estimate \u03a3c . We used matrix differentials [43] (p.9\nEq. 55 for the first term, and Eq. 51 p.8 for the two others):\n\u2202L\n\u2202\u03a3c\n\n=\n+\n\nn\nX\n\nT\n\n\u2212UiT (\u03bcc \u2212 \u03bci ) (\u03bcc \u2212 \u03bci ) UiT\n\ni=1\nn\nX\n\n2\n\ni=1\n\nUiT \u2212\n\nn\nX\n\n\u03a3\u2212T\nc .\n\n(73)\n\ni=1\n\nTaken into account the fact that \u03a3c is symmetric, differential\ncalculus on symmetric matrices can be simply estimate:\n\u0014\n\u0015T\n\u0012\n\u0013\n\u2202L\n\u2202L\n\u2202L\ndL\n=\n+\n\u2212 diag\n.\n(74)\nd\u03a3c\n\u2202\u03a3c\n\u2202\u03a3c\n\u2202\u03a3c\nThus, if one notes\nA=\n\nn\nX\ni=1\n\nT\n\n2UiT \u2212 UiT (\u03bcc \u2212 \u03bci ) (\u03bcc \u2212 \u03bci ) UiT\n\n(75)\n\nand recalling that \u03a3c is symmetric, one has to solve\n\u22121\nT\nn(2\u03a3\u22121\nc \u2212 diag(\u03a3c )) = A + A \u2212 diag(A).\n\n(76)\n\nLet\nB = A + AT \u2212 diag(A)\nThen one can estimate \u03a3c iteratively as follows:\nh\ni\u22121\n\u03a3(k+1)\n= 2n (B (k) + diag(B (k) ))\nc\n\n(77)\n\n(78)\n\nLet us now compare the two generic Burbea-Rao/tailored\nGaussian methods for computing the Bhattacharyya centroids\non multvariate Gaussians.\nD. Applications to mixture simplification in statistics\nSimplifying Gaussian mixtures is important in many applications arising in signal processing [26]. Mixture simplification is also a crucial step when one wants to study the Riemannian geometry induced by the Rao distance with respect\nto the Fisher metric: The set of mixture models need to have\nthe same number of components, so that we simplify source\nmixtures to get a set of Gaussian mixtures with prescribed\nsize. We adapt the hierarchical clustering algorithm of Garcia\net al. [26] by replacing the symmetrized Bregman centroid\n(namely, the Jeffreys-Bregman centroid) by the Bhattacharyya\ncentroid. We consider the task of color image segmentation\nby learning a Gaussian mixture model for each image. Each\nimage is represented as a set of 5D points (color RGB and\nposition xy).\n\nThe first experimental results depicted in Figure 3 demonstrates the qualitative stability of the clustering performance.\nIn particular, the hierarchical clustering with respect to the\nBhattacharrya distance performs qualitatively much better on\nthe last colormap image.13\nThe second experiment focuses on characterizing the numerical convergence of the generic Burbea-Rao method compared to the tailored Gaussian method. Since we presented\ntwo novel different schemes to compute the Bhattacharyya\ncentroids of multivariate Gaussians, one wants to compare\nthem, both in terms of stability and accuracy. Whenever the\nratio of Bhattacharyya distance energy function between those\nestimated centroids is greater than 1%, we consider that one\nof the two estimation methods is beaten (namely, the method\nthat gives the highest Bhattacharyya distance). Among the 760\ncentroids computed to generate Figures 3, 100% were correct\nwith the Burbea-Rao approach, while only 87% were correct\nwith the tailored multivariate Gaussian matrix optimization\nmethod. The average number of iterations to reach the 1%\naccuracy is 4.1 for the Burbea-Rao estimation algorithm, and\n5.2 for the alternative method.\nThus we experimentally checked that the generic CCCP\niterative Burbea-Rao algorithm described for computing the\nBhattacharrya centroids always converge, and moreover beats\nanother ad-hoc iterative method tailored for multivariate Gaussians.\nVI. C ONCLUDING REMARKS\nIn this paper, we have shown that the Bhattacharrya distance\nfor distributions of the same statistical exponential families\ncan be computed equivalently as a Burbea-Rao divergence\non the corresponding natural parameters. Those results extend to skew Chernoff coefficients (and Amari \u03b1-divergences)\nand skew Bhattacharyya distances using the notion of skew\nBurbea-Rao divergences. We proved that (skew) Burbea-Rao\ncentroids are unique, and can be efficiently estimated using\nan iterative concave-convex procedure with guaranteed convergence. We have shown that extremally skewed BurbeaRao divergences amount asymptotically to evaluate Bregman\ndivergences. This work emphasizes on the attractiveness of\nexponential families in Statistics. Indeed, it turns out that for\nmany statistical distances, one can evaluate them in closedform. For sake of brevity, we have not mentioned the recent\n13 See reference images and segmentation using Bregman centroids at http:\n//www.informationgeometry.org/MEF/\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n11\n\n(a)\n\n(b)\n\n(c)\nFig. 3. Color image segmentation results: (a) source images, (b) segmentation with k = 48 5D Gaussians, and (c) segmentation with k = 16 5D Gaussians.\n\n\u03b2-divergences and \u03b3-divergences [44], although their distances\non exponential families are again available in closed-form.\nThe differential Riemannian geometry induced by the class\nof such Jensen difference measures was studied by Burbea\nand Rao [18], [19] who built quadratic differential metrics\non probability spaces using Jensen differences. The JensenShannon divergence is also an instance of a broad class of\ndivergences called the f -divergences. A f -divergence If is a\nstatistical measure\nof dissimilarity defined by the functional\nR\nq(x)\nIf (p, q) = p(x)f ( p(x)\n)dx. It turns out that the JensenShannon divergence is a f -divergence for the generator\n\u0012\n\u0013\n1\n2\nf (x) =\n(x + 1) log\n+ x log x .\n(79)\n2\nx+1\n\nleft-sided\nBregman\ncentroids\nis\navailable\nhttp://www.informationgeometry.org/BurbeaRao/\n\nf -divergences preserve the information monotonicity [44], and\ntheir differential geometry was studied by Vos [45]. However,\nthis Jensen-Shannon divergence is a very particular case of\nBurbea-Rao divergences since the squared Euclidean distance\n(another Burbea-Rao divergence) does not belong to the class\nof f -divergences.\n\nP ROOF OF UNIQUENESS OF THE B URBEA -R AO CENTROIDS\n\nS OURCE CODE\nThe generic Burbea-Rao barycenter estimation algorithm\nshall be released in the J MEF open source library:\nhttp://www.informationgeometry.org/MEF/\nAn\napplet\nvisualizing\nthe\nskew\nBurbeaRao centroids ranging from the right-sided to\n\nat:\n\nACKNOWLEDGMENTS\nWe gratefully acknowledge financial support from French\nagency DIGITEO (GAS 2008-16D) and French National Research Agency (ANR GAIA 07-BLAN-0328-01), and Sony\nComputer Science Laboratories, Inc. We are very grateful to\nthe reviewers for their thorough and thoughtful comments and\nsuggestions. In particular, we are thankful to the anonymous\nReferee that pointed out a rigorous proof of Lemma 1.\nA PPENDIX\nConsider without loss of generality the Burbea-Rao centroid\n(also called Jensen centroid) defined as the minimizer of\nc = arg min\nx\n\nn\nX\n1\nJF (pi , x)\nn\ni=1\n\n(q)\nwhere JF (p, q) = F (p)+F\n\u2212 F ( p+q\n2\n2 ) \u2265 0. For sake of\nsimplicity, let us consider univariate generators.\nThe Jensen\n0\ndivergence may not be convex as J 0 (x, p) = F 2(x) \u2212 12 F ( x+p\n2 )\nand J 00 (x, p) = 21 F 00 (x) \u2212 14 F 00 ( x+p\n)\ncan\nbe\nalternatively\n2\npositive/negative (see [46]). In general, minimizing the average\nnon-convex divergence may a priori yield to many local\nminima [47]. It is remarkable to observe that the centroid\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\ninduced by a Jensen divergence is unique although the problem\nmay not be convex.\nThe proof of uniqueness of the Burbea-Rao centroid and\nthe convergence of the CCCP approximation algorithm rely on\nthe \"interness\" property (called compensativeness14 in [48]) of\nquasi-arithmetic means:\n\n12\n\nR EFERENCES\n\n[1] A. N. Kolmogorov, \"Sur la notion de la moyenne,\" Accad. Naz. Lincei\nMem. Cl. Sci. Fis. Mat. Natur. Sez., vol. 12, pp. 388\u2013391, 1930.\n[2] M. Nagumo, \"\u00dcber eine Klasse der Mittelwerte,\" Japanese Journal of\nMathematics, vol. 7, pp. 71\u201379, 1930, see Collected papers, Springer\n1993.\n[3] J. D. Acz\u00e9l, \"On mean values,\" Bulletin of the American Mathematical\nSociety, vol. 54, no. 4, pp. 392\u2013400, 1948, http://www.mta.hu/.\nn\nn\n[4] A. Ben-Tal, A. Charnes, and M. Teboulle, \"Entropic means,\" Journal\nmin pi \u2264 M\u2207F (p1 , ..., pn ) \u2264 max pi ,\ni=1\ni=1\nof Mathematical Analysis and Applications, vol. 139, no. 2, pp. 537 \u2013\n551, 1989.\nwith\n!\n[5] S. M. Ali and S. D. Silvey, \"A general class of coefficients of divergence\nn\nX1\nof one distribution from another,\" Journal of the Royal Statistical Society,\n\u2207F (pi )\nM\u2207F (p1 , ..., pn ) = (\u2207F )\u22121\nSeries B, vol. 28, pp. 131\u2013142, 1966.\nn\n[6] I. Csisz\u00e1r, \"Information-type measures of difference of probability distrii=1\nbutions and indirect observation,\" Studia Scientiarum Mathematicarum\nfor a strictly convex function F (and hence, strictly monotone\nHungarica, vol. 2, p. 229318, 1967.\nincreasing gradient \u2207F ). The interness property of quasi[7] F. Nielsen and R. Nock, \"Sided and symmetrized Bregman centroids,\"\nIEEE Transactions on Information Theory, vol. 55, no. 6, pp. 2048\u2013\narithmetic means ensures that it is indeed a mean value\n2059, June 2009.\ncontained within the extremal values.\n[8] Y. Censor and S. A. Zenios, Parallel Optimization: Theory, Algorithms,\nFor sake of simplicity, let us first consider a univariate\nand Applications. Oxford University Press, 1997.\nconvex generator F with the pi 's following the increasing\n[9] M. J. Wainwright and M. I. Jordan, \"Graphical models, exponential\n(0)\n(0)\nfamilies, and variational inference,\" Foundational Trends in Machine\norder: p1 \u2264 ... \u2264 pn . Let initially c0 \u2208 [p1 = p1 , pn = pn ].\nLearning, vol. 1, pp. 1\u2013305, January 2008.\n(1)\n(1)\npn +c0\np1 +c0\nSince c1 = M\u2207F (p1 = 2 , ..., pn = 2 ) is a quasi- [10] S.-I. Amari, \"\u03b1-divergence is unique, belonging to both f -divergence\n(1) (1)\nand bregman divergence classes,\" IEEE Trans. Inf. Theor., vol. 55,\narithmetic mean, we necessarily have c1 \u2208 [p1 , pn ] and\nno. 11, pp. 4925\u20134931, 2009.\n(1)\n(1)\nc0 +pn \u2212c0 \u2212p1\npn \u2212p1\npn \u2212p1 =\n= 2 . Thus the CCCP iterations [11] S.-i. Amari, \"Integration of stochastic models by minimizing \u03b12\ndivergence,\" Neural Comput., vol. 19, no. 10, pp. 2780\u20132796, 2007.\ninduce a sequence of iterated quasi-arithmetic means ct such\n[12] S. Amari and H. Nagaoka, Methods of Information Geometry, A. M.\nthat\nSociety, Ed. Oxford University Press, 2000.\n![13] F. Nielsen and R. Nock, \"The dual voronoi diagrams with respect to\n(t\u22121)\n(t\u22121)\nrepresentational bregman divergences,\" in International Symposium on\n+ ct\u22121\npn\np\n+ ct\u22121\n(t)\n, ..., p(t)\n, Voronoi Diagrams (ISVD). DTU Lyngby, Denmark: IEEE, June 2009.\nct = M\u2207F p1 = 1\nn =\n2\n2\n[14] J.-D. Boissonnat, F. Nielsen, and R. Nock, \"Bregman Voronoi diagrams,\" Discrete & Computational Geometry, 2010, accepted, extend\n(t) (t)\nct \u2208 [p1 , pn ]\nACM-SIAM SODA 2007.\n[15] I. Csisz\u00e1r, \"Generalized projections for non-negative functions,\" Acta\nwith\nMathematica Hungarica, vol. 68, no. 1-2, pp. 161\u2013185, 1995.\n(t\u22121)\n(t\u22121)\n[16] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh, \"Clustering with\n\u2212\nc\n\u2212\np\nc\n+\np\nn\nt\u22121\nt\u22121\n(t)\n1\nBregman divergences,\" J. Mach. Learn. Res., vol. 6, pp. 1705\u20131749,\np(t)\n=\n,\nn \u2212 p1\n2005.\n2\n(t\u22121)\n(t\u22121)\n[17] M. Detyniecki, \"Mathematical aggregation operators and their applica\u2212 p1\npn\ntion to video querying,\" Ph.D. dissertation, 2000.\n=\n,\n2\n[18] J. Burbea and C. R. Rao, \"On the convexity of some divergence measures\nbased on entropy functions,\" IEEE Transactions on Information Theory,\n1\n=\n(pn \u2212 p1 ).\nvol. 28, no. 3, pp. 489\u2013495, 1982.\n2t\n[19] --, \"On the convexity of higher order Jensen differences based on\nIt follows that the sequence of centroid approximation ct\nentropy functions,\" IEEE Transactions on Information Theory, vol. 28,\nno. 6, pp. 961\u2013, 1982.\nconverges in the limit to a unique centroid c\u2217 . That is, the\nBurbea-Rao centroids exist and are unique for any strictly [20] J. Lin, \"Divergence measures based on the Shannon entropy,\" IEEE\nTransactions on Information Theory, vol. 37, pp. 145\u2013151, 1991.\nconvex generator F . The centroid can be approximated within [21] M. Basseville and J.-F. Cardoso, \"On entropies, divergences and mean\n1\nvalues,\" in Proceedings of the IEEE Workshop on Information Theory,\n2t relative precision after t iterations (linear convergence\n1995.\nof the CCCP). Since the CCCP iterations yield both an\n[22] L. M. Bregman, \"The relaxation method of finding the common point\n(t) (t)\napproximation ct and a range [p1 , pn ] where ct should be at\nof convex sets and its application to the solution of problems in convex\nprogramming,\" USSR Computational Mathematics and Mathematical\nthe t-iteration, we choose in practice to stop iterating whenever\n(t)\nPhysics, vol. 7, pp. 200\u2013217, 1967.\np(t)\nn \u2212p1\ngoes below a prescribed threshold (for example, [23] F. Nielsen and V. Garcia, \"Statistical exponential families: A digest with\n(t)\npn\nflash cards,\" 2009, arXiv.org:0911.4863.\ntaking \u2207F = log x, we find in about 50 iterations the centroid\nFukunaga, Introduction to statistical pattern recognition (2nd ed.).\nwith machine precision 10\u221212 ). The CCCP algorithm with b [24] K.\nAcademic Press Professional, Inc., 1990.\nbits precision require O(nb) time to approximate.\n[25] L. Rigazio, B. Tsakam, and J. C. Junqua, \"An optimal bhattacharyya\nPt\n(t)\ncentroid algorithm for gaussian clustering with applications in\nNote that limt\u2192\u221e p1 = limt\u2192\u221e 21 i=0 21i p1 = p1 (and\nautomatic speech recognition,\" in Acoustics, Speech, and Signal\n(t)\n\u2217\nsimilarly, we have limt\u2192\u221e pn = pn ). It follows that c \u2208\nProcessing, 2000. ICASSP '00. Proceedings. 2000 IEEE International\nConference on, vol. 3, 2000, pp. 1599\u20131602 vol.3. [Online]. Available:\n[p1 , pn ] as expected (all the initial extremal range is possible,\nhttp://ieeexplore.ieee.org/xpls/abs all.jsp?arnumber=861998\nand the center shall depend on the chosen generator F ). The\n[26] V. Garcia, F. Nielsen, and R. Nock, \"Hierarchical gaussian mixture\nproof extends naturally to separable multivariate functions by\nmodel,\" in International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), March 2010.\ncarrying the analysis on each dimension independently.\n[27] P. Chen, Y. Chen, and M. Rao, \"Metrics defined by Bregman diver14 A fact following from the monotonicity of the generator function \u2207F .\ngences: Part I,\" Commun. Math. Sci., vol. 6, pp. 9915\u2013926, 2008.\n\n\fIEEE TRANSACTIONS ON INFORMATION THEORY 57(8):5455-5466, 2011.\n\n[28] --, \"Metrics defined by Bregman divergences: Part II,\" Commun.\nMath. Sci., vol. 6, pp. 927\u2013948, 2008.\n[29] H. Jeffreys, \"An invariant form for the prior probability in estimation\nproblems,\" Proceedings of the Royal Society of London, vol. 186, no.\n1007, pp. 453\u2013461, March 1946.\n[30] F. Liese and I. Vajda, \"On Divergences and Informations in Statistics\nand Information Theory,\" IEEE Transactions on Information Theory,\nvol. 52, no. 10, pp. 4394\u20134412, October 2006.\n[31] J. Zhang, \"Divergence function, duality, and convex analysis,\" Neural\nComputation, vol. 16, no. 1, pp. 159\u2013195, 2004.\n[32] A. Yuille and A. Rangarajan, \"The concave-convex procedure,\" Neural\nComputation, vol. 15, no. 4, pp. 915\u2013936, 2003.\n[33] B. Sriperumbudur and G. Lanckriet, \"On the convergence of the\nconcave-convex procedure,\" in Neural Information Processing Systems,\n2009.\n[34] Y. He, A. B. Hamza, and H. Krim, \"An information divergence measure\nfor ISAR image registration,\" in Automatic target recognition XI (SPIE),\nvol. 4379, 2001, pp. 199\u2013208.\n[35] A. O. Hero, B. Ma, O. Michel, and J. D. Gorman, \"Alpha-divergence\nfor classification, indexing and retrieval,\" Comm. and Sig. Proc. Lab.\n(CSPL), Dept. EECS, University of Michigan, Ann Arbor, Tech. Rep.\n328, July, 2001, presented at Joint Statistical Meeting.\n[36] A. Bhattacharyya, \"On a measure of divergence between two statistical populations defined by their probability distributions,\" Bulletin of\nCalcutta Mathematical Society, vol. 35, pp. 99\u2013110, 1943.\n[37] K. Matusita, \"Decision rules based on the distance, for problems of\nfit, two samples, and estimation,\" Annal of Mathematics and Statistics,\nvol. 26, pp. 631\u2013640, 1955.\n[38] T. Kailath, \"The divergence and Bhattacharyya distance measures in\nsignal selection,\" IEEE Transactions on Communication Technology,\nvol. 15, no. 1, pp. 52\u201360, 1967.\n[39] F. Aherne, N. Thacker, and P. Rockett, \"The Bhattacharyya metric as\nan absolute similarity measure for frequency coded data,\" Kybernetika,\nvol. 34, no. 4, pp. 363\u2013368, 1998.\n[40] E. D. Hellinger, \"Die orthogonalinvarianten quadratischer formen von\nunendlich vielen variablen,\" 1907, thesis of the university of G\u00f6ttingen.\n[41] S. Kakutani, \"On equivalence of infinite product measures,\" Annals of\nMathematics, vol. 49, no. 214-224, 1948.\n[42] L. Rigazio, B. Tsakam, and J. Junqua, \"Optimal Bhattacharyya centroid\nalgorithm for Gaussian clustering with applications in automatic speech\nrecognition,\" in IEEE International Conference on Acoustics, Speech,\nand Signal Processing, vol. 3, 2000, pp. 1599\u20131602.\n[43] K. B. Petersen and M. S. Pedersen, The Matrix Cookbook.\nTechnical University of Denmark, oct 2008. [Online]. Available:\nhttp://www2.imm.dtu.dk/pubdb/p.php?3274\n[44] A. Cichocki and S. ichi Amari, \"Families of alpha- beta- and gammadivergences: Flexible and robust measures of similarities,\" Entropy,\n2010, review submitted.\n[45] P. Vos, \"Geometry of f -divergence,\" Annals of the Institute of Statistical\nMathematics, vol. 43, no. 3, pp. 515\u2013537, 1991.\n[46] F. Nielsen, R. Nock, \"Skew Jensen-Bregman Voronoi Diagrams.\" Transactions on Computational Science, no. 14, pp. 102\u2013128, 2011.\n[47] P. Auer, M. Herbster and M. Warmuth, \"Exponentially many local\nminima for single neurons,\" Advances in Neural Information Processing\nSystems, vol. 8, pp. 316-317, 1995.\n[48] J.-L. Marichal, \"Aggregation Operators for Multicriteria Decision Aid,\"\nInstitute of Mathematics, University of Li\u00e8ge, Belgium, 1998.\n\nFrank Nielsen received the BSc (1992) and MSc\n(1994) degrees from Ecole Normale Superieure\n(ENS Lyon, France). He prepared his PhD on\nadaptive computational geometry at INRIA SophiaPLACE\nAntipolis (France) and defended it in 1996. As a\nPHOTO\ncivil servant of the University of Nice (France),\nHERE\nhe gave lectures at the engineering schools ESSI\nand ISIA (Ecole des Mines). In 1997, he served in\nthe army as a scientific member in the computer\nscience laboratory of Ecole Polytechnique. In 1998,\nhe joined Sony Computer Science Laboratories Inc.,\nTokyo (Japan) where he is senior researcher. He became a professor of the CS\nDept. of Ecole Polytechnique in 2008. His current research interests include\ngeometry, vision, graphics, learning, and optimization. He is a senior ACM\nand senior IEEE member.\n\n13\n\nPLACE\nPHOTO\nHERE\n\nSylvain Boltz Sylvain Boltz received the M.S. degree and the Ph.D. degree in computer vision from\nthe University of Nice-Sophia Antipolis, France, in\n2004 and 2008, respectively. Since then, he has been\na postdoctoral fellow at the VisionLab, University\nof California, Los Angeles and a LIX-Qualcomm\npostdoctoral fellow in Ecole Polytechnique, France.\nHis research spans computer vision and image, video\nprocessing with a particular interest in applications\nof information theory and compressed sensing to\nthese areas.\n\n\f"}