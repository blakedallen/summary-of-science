{"id": "http://arxiv.org/abs/1112.3110v1", "guidislink": true, "updated": "2011-12-14T03:46:46Z", "updated_parsed": [2011, 12, 14, 3, 46, 46, 2, 348, 0], "published": "2011-12-14T03:46:46Z", "published_parsed": [2011, 12, 14, 3, 46, 46, 2, 348, 0], "title": "GPU-based Image Analysis on Mobile Devices", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1112.3051%2C1112.3336%2C1112.0197%2C1112.4229%2C1112.3142%2C1112.4226%2C1112.5214%2C1112.1550%2C1112.2612%2C1112.4256%2C1112.0405%2C1112.1474%2C1112.0962%2C1112.1347%2C1112.4716%2C1112.0614%2C1112.1093%2C1112.1978%2C1112.0538%2C1112.2654%2C1112.0823%2C1112.3575%2C1112.4259%2C1112.0308%2C1112.0064%2C1112.0203%2C1112.4480%2C1112.0907%2C1112.4492%2C1112.1551%2C1112.3056%2C1112.2589%2C1112.4634%2C1112.2441%2C1112.5401%2C1112.2189%2C1112.3995%2C1112.0128%2C1112.2872%2C1112.2179%2C1112.4520%2C1112.4086%2C1112.3699%2C1112.2842%2C1112.6346%2C1112.3110%2C1112.6340%2C1112.0630%2C1112.0010%2C1112.1767%2C1112.4244%2C1112.3483%2C1112.3822%2C1112.2257%2C1112.1253%2C1112.3821%2C1112.0786%2C1112.0476%2C1112.1965%2C1112.2629%2C1112.3744%2C1112.3438%2C1112.1338%2C1112.3325%2C1112.5300%2C1112.4015%2C1112.3779%2C1112.5882%2C1112.2450%2C1112.1183%2C1112.4787%2C1112.5034%2C1112.2359%2C1112.5045%2C1112.2909%2C1112.3478%2C1112.2864%2C1112.2777%2C1112.0834%2C1112.3985%2C1112.3315%2C1112.3817%2C1112.5223%2C1112.4854%2C1112.3387%2C1112.1534%2C1112.0813%2C1112.5565%2C1112.3289%2C1112.6118%2C1112.5584%2C1112.3579%2C1112.0480%2C1112.3705%2C1112.3165%2C1112.2938%2C1112.2568%2C1112.3858%2C1112.5841%2C1112.0270%2C1112.0980&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "GPU-based Image Analysis on Mobile Devices"}, "summary": "With the rapid advances in mobile technology many mobile devices are capable\nof capturing high quality images and video with their embedded camera. This\npaper investigates techniques for real-time processing of the resulting images,\nparticularly on-device utilizing a graphical processing unit. Issues and\nlimitations of image processing on mobile devices are discussed, and the\nperformance of graphical processing units on a range of devices measured\nthrough a programmable shader implementation of Canny edge detection.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1112.3051%2C1112.3336%2C1112.0197%2C1112.4229%2C1112.3142%2C1112.4226%2C1112.5214%2C1112.1550%2C1112.2612%2C1112.4256%2C1112.0405%2C1112.1474%2C1112.0962%2C1112.1347%2C1112.4716%2C1112.0614%2C1112.1093%2C1112.1978%2C1112.0538%2C1112.2654%2C1112.0823%2C1112.3575%2C1112.4259%2C1112.0308%2C1112.0064%2C1112.0203%2C1112.4480%2C1112.0907%2C1112.4492%2C1112.1551%2C1112.3056%2C1112.2589%2C1112.4634%2C1112.2441%2C1112.5401%2C1112.2189%2C1112.3995%2C1112.0128%2C1112.2872%2C1112.2179%2C1112.4520%2C1112.4086%2C1112.3699%2C1112.2842%2C1112.6346%2C1112.3110%2C1112.6340%2C1112.0630%2C1112.0010%2C1112.1767%2C1112.4244%2C1112.3483%2C1112.3822%2C1112.2257%2C1112.1253%2C1112.3821%2C1112.0786%2C1112.0476%2C1112.1965%2C1112.2629%2C1112.3744%2C1112.3438%2C1112.1338%2C1112.3325%2C1112.5300%2C1112.4015%2C1112.3779%2C1112.5882%2C1112.2450%2C1112.1183%2C1112.4787%2C1112.5034%2C1112.2359%2C1112.5045%2C1112.2909%2C1112.3478%2C1112.2864%2C1112.2777%2C1112.0834%2C1112.3985%2C1112.3315%2C1112.3817%2C1112.5223%2C1112.4854%2C1112.3387%2C1112.1534%2C1112.0813%2C1112.5565%2C1112.3289%2C1112.6118%2C1112.5584%2C1112.3579%2C1112.0480%2C1112.3705%2C1112.3165%2C1112.2938%2C1112.2568%2C1112.3858%2C1112.5841%2C1112.0270%2C1112.0980&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "With the rapid advances in mobile technology many mobile devices are capable\nof capturing high quality images and video with their embedded camera. This\npaper investigates techniques for real-time processing of the resulting images,\nparticularly on-device utilizing a graphical processing unit. Issues and\nlimitations of image processing on mobile devices are discussed, and the\nperformance of graphical processing units on a range of devices measured\nthrough a programmable shader implementation of Canny edge detection."}, "authors": ["Andrew Ensor", "Seth Hall"], "author_detail": {"name": "Seth Hall"}, "author": "Seth Hall", "arxiv_comment": "Proceedings of Image and Vision Computing New Zealand 2011", "links": [{"href": "http://arxiv.org/abs/1112.3110v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1112.3110v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.GR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.GR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.3.1; I.4.8", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1112.3110v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1112.3110v1", "journal_reference": null, "doi": null, "fulltext": "arXiv:1112.3110v1 [cs.GR] 14 Dec 2011\n\nGPU-based Image Analysis on Mobile Devices\nAndrew Ensor\n\nSeth Hall\n\nSchool of Computing and Mathematical Sciences\nAUT University\nAuckland, New Zealand\nEmail: andrew.ensor@aut.ac.nz\n\nSchool of Computing and Mathematical Sciences\nAUT University\nAuckland, New Zealand\nEmail: sehall@aut.ac.nz\n\nAbstract-With the rapid advances in mobile technology many\nmobile devices are capable of capturing high quality images\nand video with their embedded camera. This paper investigates\ntechniques for real-time processing of the resulting images,\nparticularly on-device utilizing a graphical processing unit. Issues\nand limitations of image processing on mobile devices are\ndiscussed, and the performance of graphical processing units on\na range of devices measured through a programmable shader\nimplementation of Canny edge detection.\n\nI. I NTRODUCTION\nMobile phone technology is virtually ubiquitous and rapidly\nevolving, giving rise to new and exciting application domains through the convergence of communication, camera and\ncomputing technologies. Many of these applications, such as\nthose for mobile augmented reality, utilize the device camera\nfor image recognition or visual tag identification [1], [2],\n[3], [4]. Mobile devices have quite distinct capabilities and\nlimitations from desktop computers, so many of the usual\napproaches for application development must be reworked to\nbe made suitable for deployment to actual mobile devices.\nFor instance, the procedure for capturing images varies from\ndevice to device, and the quality, contrast, resolution and rates\nof image capture can be substantially different. The central\nprocessing unit capabilities of many devices is a significant\ninhibiting factor for realizing some applications, as can be the\nnetwork communication bandwidth, latency, and cost, as well\nas demands on the finite battery charge.\nHowever, mobile computational capabilities and memory\nspecifications are rapidly evolving making more processorintensive applications possible that were considered infeasible\neven two years ago. For instance, the Nokia N series of\nmultimedia devices commenced with the release of the Nokia\nN70 in 2005, which included a 2 megapixel rear camera (and\n0.3 megapixel front camera), 32 MB memory, and a 220 MHz\nARM-926 CPU. In 2007 the Nokia N95 was released with a\n5 megapixel rear camera, 160 MB memory, and a 330 MHz\nARM-11 CPU. More recently, the Nokia N8 was released in\n2010 with a 12 megapixel rear camera, 256 MB memory, and\nboth a 680 MHz ARM-11 CPU and a BCM2727 GPU capable\nof 32 MPoly/s. It is now common for newer smart phones to\ninclude a 1 GHz CPU and a GPU such as a PowerVR SGX\n(Imagination Technologies), Adreno (Qualcomm, formerly of\nAMD), Mali (ARM), or Tegra 2 (NVIDIA).\n\nII. I MAGE C APTURE\n\nAND\n\nA NALYSIS\n\nON\n\nM OBILE D EVICES\n\nImages can be obtained by an application from a mobile\ncamera by taking a photograph snapshot. However, this can\nbe a notoriously slow process, requiring between 520 ms and\n8 s for some N-series devices [5]. Instead, it is far preferable to\nobtain preview frames from the video. On Java ME supported\nmobiles the commonly available Multimedia API provides\naccess to video data. However, device implementations of this\nAPI usually require that the video capture be stopped to obtain\nand then separately decode the video segment (typically in\n3GPP format) in order to obtain any frames. Some platforms,\nsuch as Android, allow both RGB and greyscale preview\nframes to be captured (with typical rates for a 640\u00d7480 image\nof 26 frames per second on a Google Nexus One and 30 frames\nper second on an HTC Desire HD), whereas others, such as\niOS, only return RGB frames by default (with typical rates of\n29 frames per second on an Apple iPhone 4) which can then\nbe converted by software to greyscale if necessary for further\nanalysis.\nOnce captured there are two (non-exclusive) choices for\nprocessing an image:\n\u2022 off-device utilizing the network capabilities of the mobile,\neither a localized network technology such as Bluetooth\nor Wi-Fi, or a cellular network to off-load the image\nprocessing to a more powerful machine,\n\u2022 on-device utilizing the computing capabilities of the mobile to itself perform the processing via the CPU or GPU.\nFor instance, the Shoot & Copy application [6] utilizes Bluetooth to pass a captured image to a Bluetooth server for\nidentification and contextual information about the image. The\nTouch Projector application [7] passes video and touch events\nvia Wi-Fi to a computer connected to a projector. However,\noff-device processing has some significant disadvantages. Although many devices support Bluetooth 2.0 with enhanced\ndata rates providing a theoretical data transfer rate of 2.1\nMbps, the authors found that in practice on most devices the\nrate was closer to 430 kbps upload and 950 kbps download,\nwhich can result in a significant communication latency when\ntransmitting image frames. Wi-Fi improves the bandwidth\nand reduces latency but it has somewhat less support on\nolder mobile devices and can be quite demanding on the\nbattery. Whereas both Bluetooth and Wi-Fi are only suitable\nfor localized processing solutions, utilizing a cellular network\n\n\fwith a persistent but mostly idle TCP connection to a processing server can provide a more suitable off-device solution.\nHowever, this too can result in significant network-specific\nbandwidth limitations (a 3G network has typical speeds of\n150 kbps upload and 2 Mbps download), latencies, and usage\ncharges. The eventual availability of LTE promises to reduce\nthis issue with 50 Mbps upload, 100 Mbps download, and\nround trip latencies reduced to around 10 ms.\nWith the evolving specifications of mobile devices there\nis a growing list of literature and applications that choose\nto perform image processing on-device. On-device processing\nwas used by [8] for edge-based tracking of the camera pose\nby a tablet PC in an outdoor environment. PhoneGuide [9]\nperformed object recognition computations on a mobile phone.\nSURF [10] was implemented on a Nokia N95 to match camera\nimages against a database of location-tagged images [11]\nproviding image matches in 2.8 seconds. Variants of SIFT and\nFerns algorithms were used in [12], and [13] tested them on\nan Asus P552W with a 624 MHz Marvell PXA 930 CPU\nwith the algorithms processing a 240 \u00d7 320 frame in 40\nms. Studierstube ES [14] is a marker tracking API that is\na successor to ARToolKitPlus and available for Windows CE,\nSymbian, and iOS, but it is closed source. Junaio 3.0 [15] is a\nfree augmented reality browser for iOS and Android platforms\nthat utilizes image tracking to display objects from a locationbased channel (showing points of interest in surroundings) or\na Junaio GLUE channel (attaching virtual 3D models to up to\nseven visible markers). Most other mobile applications, such as\nGoogle Goggles [16] for Android and iOS have entirely webbased pattern matching so no image analysis is performed on\nthe device. From version 2.2 the popular OpenCV API [17]\nhas been available for Android and Maemo/Meego platforms,\nand it also can be built for iOS. NVidia has contributed\n(non-mobile) GPU implementations of some computer vision\nalgorithms, and has contributed optimizations for the Android\nCPU implementation.\nIt is now commonplace for applications to utilize GPU for\nprocessing beyond only graphics rendering, particularly for\ntasks that are highly parallel and have high arithmetic intensity,\nfor which GPU are well suited. As most computer vision\nalgorithms take an array of pixel data as input and output\na variable-length representation of the image (the reverse of\ngraphics rendering for which GPU were originally designed)\ntheir implementation on GPU has been somewhat slower than\nby some other fields. Some examples of computer vision\nalgorithms implemented on GPU can be found in [18], [19],\nand [20]. However, mobile devices containing programmable\nGPU only became widely available in 2009 with the use of the\nPowerVR SGX535 processor, so to date there has been very\nlittle literature available on mobile-specific GPU implemented\nalgorithms. Several recent articles and potential power savings\nby utilizing GPU rather than CPU on mobiles are discussed in\n[21]. In particular, [22] implements a Harris corner detection\non a OMAP ZOOM Mobile Development Kit equipped with a\nPowerVR SGX 530 GPU using four render passes (greyscale\nconversion, gradient calculations, Gaussian filtering and corner\n\nstrength calculation, and local maxima), reporting 6.5fps for\na 640 \u00d7 480 video image.\nIII. O PEN GL ES\nWith the notable exception of Windows Phone devices the\nvast majority of modern mobile devices support OpenGL ES, a\nversion of the OpenGL API that is intended for embedded systems. From version 2.0 OpenGL ES supports programmable\nshaders, so parts of an application can be written in GLSL and\nexecuted directly in the GPU pipeline.\nAs with all shaders branching is discouraged as it carries a\nperformance penalty, particularly when it involves dynamic\nflow control on a condition computed within each shader,\nalthough the shader compiler may be able to compile out\nstatic flow control and unroll loops computed on compile-time\nconstant conditions or uniform variables. The reason for this\nis that GPU don't have the branch-prediction circuitry that is\ncommon in CPU, and many GPU execute shader instances in\nparallel in lock-step, so one instance caught inside a condition\nwith a substantial amount of computation can delay all the\nother instances from progressing. The same holds for dependent texture reads, where the shader itself computes texture\ncoordinates rather than directly using unmodified texture coordinates passed into the shader. The graphics hardware cannot\nthen prefetch texel data before the shader executes to reduce\nmemory access latency. Unfortunately, many computer vision\nalgorithms require dependent texture reads when implemented\non a GPU. Another issue that must be considered is the latency\nin creating and transferring textures. Ideally, all texture data for\na GPU should be loaded during initialization and preferably\nnot changed while the shaders execute, to reduce the dataflow\nbetween memory and the GPU. However, for real-time image\nanalysis to be feasible on a GPU image data captured from the\ncamera should preferably be loaded into a preallocated texture\nat 30 fps, quite contrary to GPU recommended practices.\nThis can be partially compensated for by reducing the image\nresolution or changing its format from RGB vector float values\nto integer or compressed.\nOpenGL ES 2.0 allows byte, unsigned byte, short, unsigned\nshort, float, and fixed data types for vertex shader attributes,\nbut vertex shaders always expect attributes to be float so all\nother types are converted, resulting in a compromise between\nbandwidth/storage and conversion costs. It requires that a GPU\nmust allow at least two texture units to be available to fragment\nshaders, which is not an issue for many image processing\nalgorithms, although most GPU support eight texture units.\nTextures might not be available to vertex shaders and there\nare often tight limits on the number of vertex attributes and\nvarying variables that can be used (16 and 8 respectively in\nthe case of the PowerVR SGX series of GPU).\nUnlike the full version OpenGL ES uses precision hints for\nall shader values:\n\u2022 lowp for 10 bit values between \u22122 and 1.999 with\na precision of 1/256 (which for graphics rendering is\nmainly used for colours and reading from low precision\ntextures such as normals from a normal map),\n\n\fmediump for 16 bit values between -65520 and 65520\nconsisting of a sign bit, 5 exponent bits, and 10 mantissa\nbits (which can be useful for reducing storage requirements),\n\u2022 highp for 32 bit (mostly adhering to the IEEE754 standard).\nFurthermore, the GPU on a mobile device is most likely\nto be a scalar rather than vector processor. This means that\nthere is typically no advantage vectorizing highp operations, as\neach highp component will be computed sequentially, although\nlowp and mediump values can be processed in parallel. It is\nalso common for GPU on mobiles to use tile-based deferred\nrendering, where the framebuffer is divided into tiles and\ncommands get buffered and processed together as a single\noperation for each tile. This helps the GPU to more effectively\ncache framebuffer values and allows it to discard some fragments before they get processed by a fragment shader (for this\nto work correctly fragment shaders should themselves avoid\ndiscarding fragments).\nThere are performance benchmarks for the GPU commonly\nfound in mobile devices [23]. However, the benchmarks typically only compare the performance for graphics rendering\nthroughput, not for other tasks such as image processing, so\ndo not significantly test the implications of effects such as\nfrequent texture reloading and dependent texture reads.\n\u2022\n\nIV. C ANNY S HADER I MPLEMENTATION\nCanny edge detection [24] is one of the most commonly\nused image processing algorithms, and it illustrates many of\nthe issues associated with implementing image processing\nalgorithms on GPU. It has a texture transfer for each frame\ncaptured, a large amount of conditionally executed code, and\ndependent texture reads. As such it might not be considered\nan ideal candidate for implementation on a GPU.\nThe Canny edge detection algorithm is based on the gradient\nvector and can give excellent edge detection results in practice.\nStarting with a single channel (greyscale) image it proceeds\nin four steps to produce an image whose pixels with non-zero\nintensity represent the edges in the original image:\n\u2022 First the image is smoothed using a Gaussian filter to\nreduce some of the noise.\n\u2022 At each pixel in the smoothed image the gradient vector\nis calculated using the two Sobel operators. The length\n|\u2207f | of the gradient vector is calculated or approximated,\nand its direction is classified into one of the four directions horizontal, vertical, forward diagonal, or backward\ndiagonal (depending to which direction \u2207f is closest).\n\u2022 At each pixel non-maximum suppression is applied to the\nvalue of |\u2207f | by comparing the value of |\u2207f | at the pixel\nwith its value at each of the two opposite neighbouring\npixels in either direction. If its value is smaller than\nthe value at either of those two pixels then the pixel is\ndiscarded as not a potential edge pixel (value is set to 0 as\nthe neighbouring pixel has a greater change in intensity\nso it better represents the edge). This results in thin lines\nfor the edges.\n\n\u2022\n\nAt each remaining pixel a double threshold (or hysteresis\nthreshold) is applied using both an upper and a lower\nthreshold, with a ratio upper:lower typically between 2:1\nand 3:1. If the pixel has a value of |\u2207f | above the upper\nthreshold then it is accepted as an edge pixel (and referred\nto as a strong pixel), whereas any pixel for which |\u2207f | is\nbelow the lower threshold is rejected. For any pixel whose\nvalue of |\u2207f | is between the upper and lower thresholds,\nit is accepted as an edge pixel if and only if one of its\neight neighbours is above the threshold (it has a strong\npixel neighbour, in which case the pixel is referred to as\na weak pixel).\n\nCanny edge detection was implemented in [25] using CUDA\non a Tesla C1060 GPU with 240 1.3 GHz cores. The GPU\nimplementation achieved a speedup factor of 50 times over a\nconventional implementation on a 2 GHz Intel Xeon E5520\nCPU, although both these GPU and CPU were far more powerful than the processors currently found in mobile devices.\nIn this work the authors have created a purely GPUbased implementation of the Canny edge detection algorithm\nand tested its performance across a range of popular mobile\ndevices that support OpenGL ES 2.0 using the camera on\neach device. The purpose is to determine whether it is yet\nadvantageous to utilize the GPU in these devices for image\nanalysis instead of the usual approach of having the processing\nperformed entirely by the CPU. To achieve this the algorithm\nwas implemented in GLSL via a total of five render passes\nusing four distinct fragment shaders all having mediump\nprecision:\n\u2022\n\n\u2022\n\nGaussian smoothing using either a 3 \u00d7 3 or a 5 \u00d7 5\nconvolution kernel. Since a Gaussian kernel is separable\nit can be applied as two one-dimensional convolutions\nso the Gaussian smoothing is performed in two passes,\ntrading the overhead of a second render pass against the\nlower number of texture reads. Even for a 3 \u00d7 3 kernel\nusing two render passes rather than one was found to\nbenefit performance on actual devices.\nThe gradient vector is calculated and its direction is\nclassified. First the nine smoothed pixel intensities are\nobtained in the neighbourhood of a pixel, and used by\nthe Sobel X and Y operators to obtain the gradient\nvector. Then IF statements are avoided by multiplying\n1\n-turn rotation matrix and\nthe gradient vector by a 2 \u00d7 2 16\nthen its angle relative to horizontal is doubled so that it\nfalls into one of four quadrants. A combination of step\nand sign functions is then used to classify the resulting\nvector as one of the eight primary directions (\u2206x , \u2206y )\nwith \u2206x and \u2206y each being either \u22121, 0, or 1. These\neight directions correspond to the four directions in the\nusual Canny edge detection algorithm along with their\nopposite directions. The shader then outputs the length\nof the gradient vector and the vector (\u2206x , \u2206y ). This\napproach to classifying the direction was found to take\nas little as half the time of several alternative approaches\nthat utilized conditional statements.\n\n\fNon-maximal suppression and the double threshold are\napplied together. Non-maximal suppression is achieved\nby obtaining the length of the gradient vector from the\nprevious pass for the pixel with the length of the gradient\nvector for the two neighbouring pixels in directions\n(\u2206x , \u2206y ) and (\u2212\u2206x , \u2212\u2206y ). The length at the pixel is\nsimply multiplied by a step function that returns either\n0.0 or 1.0 depending whether its length is greater than\nthe maximum of the two neighbouring lengths. For the\ndouble threshold a smoothstep is used with the two\nthresholds to output an edge strength measurement for\nthe pixel between 0.0 (reject) and 1.0 (accept as a strong\npixel).\n\u2022 The final shader handles the weak pixels differently from\nCanny's original algorithm. Rather than simply accepting\na pixel as a weak pixel if one of its neighbouring eight\npixels is a strong pixel, since the previous render pass has\nprovided an edge strength measurement for each pixel\nmore information is available. This shader obtains the\nnine edge strength measurements in the neighbourhood\nof a pixel, and takes a linear combination of the edge\nstrength measurement at the pixel with a step function\nthat accepts a weak pixel if the sum of the nine edge\nstrength measurements is at least 2.0. This avoids the\nusual IF statement with eight OR conditions, greatly\nincreasing performance of this render pass and giving a\nsmall improvement in the weak pixel criterion.\nIn effect, the entire Canny edge detection algorithm is implemented without any conditional statements whatsoever, ideal\nfor a GPU shader-based implementation on OpenGL ES. The\nshader code is available from the authors upon request.\n\u2022\n\nV. P ERFORMANCE R ESULTS\nThe GPU version of the Canny edge detection described\nin Section IV was implemented on the following devices,\nchosen as they were all released within the same year and\nnow commonplace:\n\u2022 Google Nexus One, released January 2010, operating\nsystem Android 2.3, CPU 1 GHz Qualcomm QSD8250\nSnapdragon, GPU Adreno 200, memory 512 MB RAM,\ncamera 5 megapixel, video 720\u00d7480 at minimum 20 fps.\n\u2022 Apple iPhone 4, released June 2010, operating system\niOS 4.3.5, CPU Apple A4 ARM Cortex A8, GPU\nPowerVR SGX 535, memory 512 MB RAM, camera 5\nmegapixel, video 720p (1280 \u00d7 720) at 30 fps.\n\u2022 Samsung Galaxy S, released June 2010, operating system Android 2.3, CPU 1 GHz Samsung Hummingbird\nS5PC110 ARM Cortex A8, GPU PowerVR SGX 540\nwith 128 MB GPU cache, memory 512 MB RAM,\ncamera 5 megapixel, video 720p at 30 fps.\n\u2022 Nokia N8, released September 2010, operating system\nSymbian\u02c63, CPU 680 MHz Samsung K5W4G2GACAAL54 ARM 11, GPU Broadcom BCM2727, memory 256\nMB RAM, camera 12 megapixel, video 720p at 25 fps.\n\u2022 HTC Desire HD, released October 2010, operating system Android 2.3, CPU 1 GHz Qualcomm MSM8255\n\nTABLE I\nR ENDER PASS AND I MAGE R ELOADING T EXTURE T IMES ( MS )\nOperation\nGreyscale\nGaussian X\nGaussian Y\nGradient\nNon-max Sup\nWeak Pixels\nReload texture\n\nNexus One\n\niPhone 4\n\nDesire HD\n\nn/a\n29.9 \u00b1 4.9\n29.0 \u00b1 4.5\n138.2 \u00b1 3.9\n50.1 \u00b1 6.0\n78.8 \u00b1 2.5\n86.6 \u00b1 12.8\n\n8.9 \u00b1 3.0\n12.2 \u00b1 0.8\n12.0 \u00b1 0.1\n60.2 \u00b1 0.4\n25.1 \u00b1 2.7\n28.9 \u00b1 4.4\n36.8 \u00b1 4.3\n\nn/a\n11.1 \u00b1 3.3\n11.2 \u00b1 3.7\n22.5 \u00b1 1.4\n11.2 \u00b1 1.8\n19.7 \u00b1 1.0\n5.2 \u00b1 4.8\n\nSnapdragon, GPU Adreno 205, memory 768 MB RAM,\ncamera 8 megapixel, video 720p at 30 fps.\n\u2022 Google Nexus S, released December 2010, operating\nsystem Android 2.3, CPU 1 GHz Samsung Hummingbird\nS5PC110 ARM Cortex A8, GPU PowerVR SGX 540,\nmemory 512 MB RAM, camera 5 megapixel, video\n800 \u00d7 480 at 30 fps (not 720p).\nThe Android devices directly supported obtaining the video\npreview in YUV format, and the Y component could be used\nas input as a greyscale image without the requirement for\nany preliminary processing. However, the iOS and Symbian\u02c63\ndevices only supported obtaining the preview in RGB, so\nthey required an additional preliminary render pass to convert\nthe RGB image to greyscale. An additional point worth\nmentioning for the iPhone is that any pending OpenGL ES\ncommands must be flushed before the application is put into\nthe background, otherwise the application gets terminated by\nthe operating system.\nTable I lists the times in milliseconds for each of the render\npasses for some of the devices. To obtain these times the\nOpenGL ES glFinish command was used to flush any queued\nrendering commands and wait until they have finished. Note\nthis removes the ability of the GPU to commence further commands, so although useful for comparing the times required\nfor each render pass, their sum only gives an upper bound on\nthe total algorithm time. The two Gaussian smoothing render\npasses were timed using a 3 \u00d7 3 convolution kernel. Using\ninstead a Gaussian 5 \u00d7 5 kernel was found to add between\nan extra 3 ms (for iPhone 4 and Desire HD) and an extra 10\nms (Nexus One) to each of the two Gaussian render passes,\nbut did not have any visibly noticeable effect on the edge\ndetection results. The calculation of the gradient vector is the\nmost burdensome render pass, explained by the nine texture\nreads it performs and relatively complex computation used\nto classify its direction. This number of texture reads is also\nperformed in the weak pixels render pass, whereas the other\ntwo render passes only require three texture reads. The table\nalso gives the time required to copy captured image data to the\ntexture, which is an important quantity for real-time processing\nof images captured from the device camera, and dictated by\nthe GPU memory bandwidth. A 640 \u00d7 480 (VGA, non-powerof-two) image was used, a common resolution available for\nvideo preview on all the devices, although most supported\ngreater resolutions as well. No texture compression was used\n\n\fTABLE II\nF RAME R ATES FOR I MAGE C APTURE AND E DGE D ETECTION ( FPS )\nDevice\nNexus One\niPhone 4\nGalaxy S\nNokia N8\nDesire HD\nNexus S\n\nCPU+Android Cam\n\nCPU+Native Cam\n\nGPU Shaders\n\n7.5 \u00b1 1.8\nn/a\n9.1 \u00b1 0.5\nn/a\n7.1 \u00b1 1.3\n8.2 \u00b1 0.9\n\n9.7 \u00b1 0.7\n7.4 \u00b1 0.4\n14.8 \u00b1 0.1\nn/a\n10.7 \u00b1 0.8\n15.5 \u00b1 0.8\n\n3.9 \u00b1 0.2\n7.6 \u00b1 0.0\n11.3 \u00b1 0.2\n14.5 \u00b1 0.1\n15.4 \u00b1 0.2\n8.9 \u00b1 0.4\n\nwhich would introduce conversion latency but assist texture\ndata to better fit on the memory bus and in a texture cache.\nThe results in Table II show the actual overall frame\nrates that were achieved in practice on each device. As the\nOpenGL ES glTexImage2D command used to update a texture\nwith new image data blocks until all the texture data has\nbeen transferred, for efficiency the (non-blocking) render pass\ncommands were performed before glTexImage2D was called\nto set the texture with a image capture for the next set of render\npasses - this was found to help increase frame rates. To\nprovide some comparison with the CPU performance on each\ndevice, an OpenCV version of Canny edge detection was also\ntimed (unlike the iOS build of OpenCV, the Android version\ncurrently has an optimized platform-specific build available).\nNo specific Symbian\u02c63 release of OpenCV was available\nduring testing. As the OpenCV edge detection relies on the\nperformance of the CPU, wherever practical any applications\nrunning in the background on the device were stopped. On\nthe Android devices it was found that the burden on the\nCPU associated with obtaining an image capture could be\nsignificantly reduced by using a native camera capture API\nrather than the default Android API, hence the two sets of\nCPU results reported.\nVI. D ISCUSSION\n\nAND\n\nC ONCLUSIONS\n\nPerhaps the most interesting conclusion that can be drawn\nfrom the results in Section V is the great variation in the\nability of different GPU in the mobile market for performing\nimage processing. The Nexus One with an Adreno 200 GPU\ndisplayed quite poor performance, due to the time to transfer\ntexture data and its slower execution of shader code. However,\nthe Desire HD with the newer Adreno 205 GPU provided\nsurprisingly good results, receiving at least a 50% performance\nbenefit by offloading edge detection to the GPU rather than\nCPU. Both these devices use Snapdragon CPU which were\nseen to execute OpenCV code slower than their competing\nHummingbird CPU, found on the Galaxy S and Nexus S. For\nthese two devices the benefit of running the edge detection\non the GPU is less definitive, although doing so would free\nup the CPU for other processor-intensive tasks that might be\nrequired by an application. The GPU results for the N8 with its\nBroadcom GPU were encouraging as its processor hardware\nis common across Symbian\u02c63 devices of the era, whereas the\nGPU results for the iPhone 4 are not surprising, it uses an older\nPowerVR SGX535 rather than the newer PowerVR SGX540\n\nfound in the Galaxy S and Nexus S. It should be reiterated\nthat the iPhone CPU results were taken using an OpenCV\nbuild that was not optimized for that platform.\nIt is worthwhile to compare the frame rates with some\nof the OpenGL ES rendering benchmarks that are available.\nFor instance, [23] reports comparative benchmark results for\nNexus One (819), iPhone 4 (1361), Galaxy S (2561), Desire\nHD (2377), and Nexus S (2880). These results do depart\nsomewhat from the GPU fps results in Section V, indicating\ndifferences between benchmarking GPU for typical graphics\nrendering versus performing an image processing algorithm\nsuch as Canny edge detection.\nThe general pattern in the GPU ability for image processing\nappears to have reached a tipping point during the 2010 release\nperiod of the investigated devices, with some devices clearly\nbeing able to benefit from offloading processing to the GPU.\nAs GPU continue to rapidly evolve, with the release of Adreno\n220 and PowerVR SGX543, along with new GPU such as\nthe Mali and the Tegra 2 for mobile devices available on\ndevices in 2011, this benefit is only continuing to increase.\nFor instance, modest performance improvements are observed\nin the Sony Ericsson Xperia Arc, released in April 2011 with\nsame CPU and GPU as the Desire HD, with the CPU+Android\nCamera tests achieving 10.0\u00b11fps and GPU shaders achieving\n17.5\u00b10.1fps. More impressive are the results for the Samsung\nGalaxy S2, first released in May 2011 with a 1.5 GHz\nSnapdragon S3 CPU and Mali-400 GPU. Its CPU+Android\nCamera tests achieved 14.2 \u00b1 0.7fps, which were dwarfed by\nthe GPU shader results of 33.8 \u00b1 3.6fps.\nR EFERENCES\n[1] de Santos Siera, A., Casanova, J.G., Avila, C.S., and Vera, V.J., Silhouettebased Hand Recognition on Mobile Devices, 43rd Annual International\nCarnahan Conference on Security Technology, 2009, pp. 160\u2013166.\n[2] Karodia, R., Lee, S., Mehta, A., and Mbogho, A., CipherCode: A Visual\nTagging SDK with Encryption and Parameterisation IEEE Workshop on\nAutomatic Identification Advanced Technologies, 2007, pp. 186\u2013191.\n[3] Lee, J.A. and Kin Choong Yow, Image Recognition for Mobile Applications IEEE International Conference on Image Processing, 2007, pp.\n177\u2013180.\n[4] Human Interface Technology Lab, \"ARToolKit 2.65\" 2011,\nhttp://www.hitl.washington.edu/artoolkit/.\n[5] Gu, J., Mukundan, R., and Billinghurst, M., Developing Mobile Phone AR\nApplications Using J2ME IVCNZ 23rd International Conference Image\nand Vision Computing New Zealand, 2008.\n[6] Boring, S., Altendorfer, M., Broll, G., Hilliges, O., and Butz, A., Shoot\n& Copy: Phonecam-based Information Transfer from Public Displays\nonto Mobile Phones Mobility '07 Proceedings of the 4th International\nConference on Mobile Technology, Applications, and Systems, 2007, pp.\n24\u201331.\n[7] Boring, S., Baur, D., Butz, A., Gustafson, S., and Baudisch, P., Touch\nProjector: Mobile Interaction through Video CHI '10: Proceedings of the\n28th International Conference on Human Factors in Computing Systems,\n2010, pp. 2287\u20132296.\n[8] Reitmayr, G. and Drummond, T., Going out: Robust Model-based Tracking for Outdoor Augmented Reality ISMAR '06 Proceedings of the 5th\nIEEE and ACM International Symposium on Mixed and Augmented\nReality, 2006, pp. 109\u2013118.\n[9] Bruns, E. and Bimber, O., Adaptive Training of Video Sets for Image\nRecognition on Mobile Phones Journal of Personal and Ubiquitous\nComputing, Volume 13 Issue 2, 2009, pp. 165\u2013178.\n[10] Bay, H., Ess, A., Tuytelaars, T., Gool, L, SURF: Speeded Up Robust\nFeatures Computer Vision and Image Understanding, Vol. 110, No. 3,\n2008, pp. 346\u2013359.\n\n\f[11] Takacs, G et.al., Outdoors Augmented Reality on Mobile Phone using\nLoxel-based Visual Feature Organization MIR '08 Proceeding of the 1st\nACM International Conference on Multimedia Information Retrieval ,\n2008, pp. 427\u2013434.\n[12] Wagner, D., Reitmayr, G., Mulloni, A., Drummond, T., Schmalstieg, D,\nPose Tracking from Natural Features on Mobile Phones 7th IEEE/ACM\nInternational Symposium on Mixed and Augmented Reality, 2008, pp.\n125\u2013134.\n[13] Wagner, D., Reitmayr, G., Mulloni, A., Drummond, T., Real-Time\nDetection and Tracking for Augmented Reality on Mobile Phones IEEE\nTransactions on Visualization and Computer Graphics, Volume 16, Issue\n3, 2010, pp. 355\u2013368.\n[14] Schmalstieg, D. and Wagner, D., Experiences with Handheld Augmented\nReality ISMAR '07 Proceedings of the 2007 6th IEEE and ACM\nInternational Symposium on Mixed and Augmented Reality, 2007.\n[15] Metaio Inc, \"Junaio 3.0\" 2011, http://www.junaio.com/.\n[16] Google\nInc,\n\"Google\nGoggles\n1.6\"\n2011,\nhttp://www.google.com/mobile/goggles/.\n[17] Willow\nGarage,\n\"OpenCV\n2.3.1\"\n2011,\nhttp://opencv.willowgarage.com/.\n[18] Fung, J. and Mann, S., OpenVIDIA: Parallel GPU Computer Vision\nMULTIMEDIA '05 Proceedings of the 13th Annual ACM International\nConference on Multimedia, 2005, pp. 849\u2013852.\n[19] Allusse, Y, Horain, P., Agarwal, A., Saipriyadarshan, C., GpuCV: An\nOpen Source GPU-accelerated Framework For Image Processing and\nComputer Vision MM '08 Proceeding of the 16th ACM International\nConference on Multimedia, 2008, pp. 1089\u20131092.\n[20] Junchul, K., Eunsoo, P., Xuenan, C., Hakil, K., Gruver, W., A Fast\nFeature Extraction in Object Recognition using Parallel Processing on\nCPU and GPU IEEE International Conference on Systems, Man and\nCybernetics, 2009, pp. 3842\u20133847.\n[21] Kwang-Ting, C. and Yi-Chu, W. Using Mobile GPU for GeneralPurpose Computing A Case Study of Face Recognition on Smartphones\nInternational Symposium on VLSI Design, Automation and Test (VLSIDAT), 2011, pp. 1\u20134.\n[22] Singhal, N., Park, I., Cho, S. Implementation and Optimization of Image\nProcessing Algorithms on Handheld GPU IEEE International Conference\non Image Processing (ICIP), 2010, pp. 4481\u20134484.\n[23] Kishonti Informations Ltd, \"GLBenchmark 2.1 Egypt\", 2011,\nhttp://www.glbenchmark.com/.\n[24] Canny, J., A Computational Approach To Edge Detection IEEE Transactions on Pattern Analysis and Machine Intelligence, 1986, pp. 679\u2013698.\n[25] Ogawa, K., Ito, Y., Nakano, K., Efficient Canny Edge Detection Using\na GPU First International Conference on Networking and Computing\n(ICNC), 2010, pp. 279\u2013280.\n\n\f"}