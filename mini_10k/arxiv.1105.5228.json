{"id": "http://arxiv.org/abs/1105.5228v3", "guidislink": true, "updated": "2012-03-27T13:42:11Z", "updated_parsed": [2012, 3, 27, 13, 42, 11, 1, 87, 0], "published": "2011-05-26T08:23:44Z", "published_parsed": [2011, 5, 26, 8, 23, 44, 3, 146, 0], "title": "Theory and computation of covariant Lyapunov vectors", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1105.6317%2C1105.4672%2C1105.0705%2C1105.3325%2C1105.3069%2C1105.3669%2C1105.3490%2C1105.1628%2C1105.1881%2C1105.3086%2C1105.3735%2C1105.1186%2C1105.5225%2C1105.0951%2C1105.3592%2C1105.6055%2C1105.3223%2C1105.3902%2C1105.1443%2C1105.2653%2C1105.5249%2C1105.1330%2C1105.0470%2C1105.1362%2C1105.4308%2C1105.5519%2C1105.1502%2C1105.1076%2C1105.5503%2C1105.3773%2C1105.2216%2C1105.5377%2C1105.2285%2C1105.5228%2C1105.2791%2C1105.2095%2C1105.1679%2C1105.5229%2C1105.0006%2C1105.1927%2C1105.0342%2C1105.0141%2C1105.1444%2C1105.4665%2C1105.4417%2C1105.5691%2C1105.0539%2C1105.2487%2C1105.5308%2C1105.0587%2C1105.5583%2C1105.0695%2C1105.1312%2C1105.1891%2C1105.1917%2C1105.4072%2C1105.1568%2C1105.0865%2C1105.2902%2C1105.1056%2C1105.3088%2C1105.2865%2C1105.2877%2C1105.4222%2C1105.1990%2C1105.4176%2C1105.6037%2C1105.4817%2C1105.3101%2C1105.4642%2C1105.1095%2C1105.5364%2C1105.4717%2C1105.2234%2C1105.0067%2C1105.4594%2C1105.2186%2C1105.1472%2C1105.1425%2C1105.6309%2C1105.2970%2C1105.6208%2C1105.3246%2C1105.1988%2C1105.1452%2C1105.4751%2C1105.3083%2C1105.0919%2C1105.4876%2C1105.5216%2C1105.5395%2C1105.5577%2C1105.4182%2C1105.1528%2C1105.4618%2C1105.3717%2C1105.1691%2C1105.2331%2C1105.3582%2C1105.5174%2C1105.2651&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Theory and computation of covariant Lyapunov vectors"}, "summary": "Lyapunov exponents are well-known characteristic numbers that describe growth\nrates of perturbations applied to a trajectory of a dynamical system in\ndifferent state space directions. Covariant (or characteristic) Lyapunov\nvectors indicate these directions. Though the concept of these vectors has been\nknown for a long time, they became practically computable only recently due to\nalgorithms suggested by Ginelli et al. [Phys. Rev. Lett. 99, 2007, 130601] and\nby Wolfe and Samelson [Tellus 59A, 2007, 355]. In view of the great interest in\ncovariant Lyapunov vectors and their wide range of potential applications, in\nthis article we summarize the available information related to Lyapunov vectors\nand provide a detailed explanation of both the theoretical basics and numerical\nalgorithms. We introduce the notion of adjoint covariant Lyapunov vectors. The\nangles between these vectors and the original covariant vectors are\nnorm-independent and can be considered as characteristic numbers. Moreover, we\npresent and study in detail an improved approach for computing covariant\nLyapunov vectors. Also we describe, how one can test for hyperbolicity of\nchaotic dynamics without explicitly computing covariant vectors.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1105.6317%2C1105.4672%2C1105.0705%2C1105.3325%2C1105.3069%2C1105.3669%2C1105.3490%2C1105.1628%2C1105.1881%2C1105.3086%2C1105.3735%2C1105.1186%2C1105.5225%2C1105.0951%2C1105.3592%2C1105.6055%2C1105.3223%2C1105.3902%2C1105.1443%2C1105.2653%2C1105.5249%2C1105.1330%2C1105.0470%2C1105.1362%2C1105.4308%2C1105.5519%2C1105.1502%2C1105.1076%2C1105.5503%2C1105.3773%2C1105.2216%2C1105.5377%2C1105.2285%2C1105.5228%2C1105.2791%2C1105.2095%2C1105.1679%2C1105.5229%2C1105.0006%2C1105.1927%2C1105.0342%2C1105.0141%2C1105.1444%2C1105.4665%2C1105.4417%2C1105.5691%2C1105.0539%2C1105.2487%2C1105.5308%2C1105.0587%2C1105.5583%2C1105.0695%2C1105.1312%2C1105.1891%2C1105.1917%2C1105.4072%2C1105.1568%2C1105.0865%2C1105.2902%2C1105.1056%2C1105.3088%2C1105.2865%2C1105.2877%2C1105.4222%2C1105.1990%2C1105.4176%2C1105.6037%2C1105.4817%2C1105.3101%2C1105.4642%2C1105.1095%2C1105.5364%2C1105.4717%2C1105.2234%2C1105.0067%2C1105.4594%2C1105.2186%2C1105.1472%2C1105.1425%2C1105.6309%2C1105.2970%2C1105.6208%2C1105.3246%2C1105.1988%2C1105.1452%2C1105.4751%2C1105.3083%2C1105.0919%2C1105.4876%2C1105.5216%2C1105.5395%2C1105.5577%2C1105.4182%2C1105.1528%2C1105.4618%2C1105.3717%2C1105.1691%2C1105.2331%2C1105.3582%2C1105.5174%2C1105.2651&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Lyapunov exponents are well-known characteristic numbers that describe growth\nrates of perturbations applied to a trajectory of a dynamical system in\ndifferent state space directions. Covariant (or characteristic) Lyapunov\nvectors indicate these directions. Though the concept of these vectors has been\nknown for a long time, they became practically computable only recently due to\nalgorithms suggested by Ginelli et al. [Phys. Rev. Lett. 99, 2007, 130601] and\nby Wolfe and Samelson [Tellus 59A, 2007, 355]. In view of the great interest in\ncovariant Lyapunov vectors and their wide range of potential applications, in\nthis article we summarize the available information related to Lyapunov vectors\nand provide a detailed explanation of both the theoretical basics and numerical\nalgorithms. We introduce the notion of adjoint covariant Lyapunov vectors. The\nangles between these vectors and the original covariant vectors are\nnorm-independent and can be considered as characteristic numbers. Moreover, we\npresent and study in detail an improved approach for computing covariant\nLyapunov vectors. Also we describe, how one can test for hyperbolicity of\nchaotic dynamics without explicitly computing covariant vectors."}, "authors": ["Pavel V. Kuptsov", "Ulrich Parlitz"], "author_detail": {"name": "Ulrich Parlitz"}, "author": "Ulrich Parlitz", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/s00332-012-9126-5", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1105.5228v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1105.5228v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "21 pages, 5 figures", "arxiv_primary_category": {"term": "nlin.CD", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "nlin.CD", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1105.5228v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1105.5228v3", "journal_reference": "J Nonlinear Sci, 2012", "doi": "10.1007/s00332-012-9126-5", "fulltext": "Theory and computation of covariant Lyapunov vectors\nPavel V. Kuptsov1, \u2217 and Ulrich Parlitz2, 3\n\narXiv:1105.5228v3 [nlin.CD] 27 Mar 2012\n\n1\nDepartment of Instrumentation Engineering, Saratov State\nTechnical University, Politekhnicheskaya 77, Saratov 410054, Russia\n2\nBiomedical Physics Group, Max Planck Institute for Dynamics\nand Self-Organization, Am Fassberg 17, 37077 G\u00f6ttingen, Germany\n3\nInstitute for Nonlinear Dynamics, Georg\u2013August\u2013Universit\u00e4t G\u00f6ttingen, Am Fassberg 17, 37077 G\u00f6ttingen, Germany\n( \u0307Dated: March 28, 2012)\n\nLyapunov exponents are well-known characteristic numbers that describe growth rates of perturbations applied to a trajectory of a dynamical system in different state space directions. Covariant\n(or characteristic) Lyapunov vectors indicate these directions. Though the concept of these vectors\nhas been known for a long time, they became practically computable only recently due to algorithms\nsuggested by Ginelli et al. [Phys. Rev. Lett. 99, 2007, 130601] and by Wolfe and Samelson [Tellus\n59A, 2007, 355]. In view of the great interest in covariant Lyapunov vectors and their wide range of\npotential applications, in this article we summarize the available information related to Lyapunov\nvectors and provide a detailed explanation of both the theoretical basics and numerical algorithms.\nWe introduce the notion of adjoint covariant Lyapunov vectors. The angles between these vectors\nand the original covariant vectors are norm-independent and can be considered as characteristic\nnumbers. Moreover, we present and study in detail an improved approach for computing covariant\nLyapunov vectors. Also we describe, how one can test for hyperbolicity of chaotic dynamics without\nexplicitly computing covariant vectors.\nKeywords: covariant Lyapunov vectors; characteristic Lyapunov vectors; forward and backward Lyapunov\nvectors; Lyapunov exponents; Lyapunov analysis; tangent space; high-dimensional chaos\n\nINTRODUCTION\n\nHigh-dimensional nonlinear systems like coupled oscillators, dynamical networks, or extended excitable media often exhibit very complex dynamics that is difficult to analyze and to characterize. From a practical\npoint of view there only a few concepts have been developed for studying low-dimensional systems that can\nefficiently be applied to high-dimensional attractors, too.\nAn important example are Lyapunov exponents that describe growth rates of perturbations applied to a trajectory in different state space directions. These exponents\nare a central point in the investigation of chaotic dynamical systems. They are related to a number of different\nphysical properties such as sensitivity to initial conditions or local entropy production and can be used to estimate the (Kaplan\u2013Yorke) dimension of (even very highdimensional) attractors [1].\nMathematically, Lyapunov exponents are defined in\ntangent space. This space is spanned by all possible infinitesimal perturbations that can be applied to a state of\nthe system. The dimension of the tangent space is equal\nto the dimension of the original phase space. In general,\nthe tangent space is an inner product space, but often the\ntangent space is defined as an Euclidean space where the\ninner product is just the ordinary scalar (dot) product.\nThe dynamics in this space is generated by linear operators, that determine the evolution of perturbation vectors\nfrom one point on the trajectory to another. These operators are called tangent linear propagators or resolvents.\n\n\u2217\n\nCorresponding author. Electronic address:p.kuptsov@rambler.ru\n\nThe tangent space is a very important subject of study.\nOn the one hand, the tangent space dynamics is closely\nrelated to the dynamics of the original system. One can\nobtain key characteristics of the original system observing the associated tangent space dynamics. On the other\nhand, the tangent space is linear and the dynamics in\nthis space is determined by the action of linear operators. This means that analysis methods as well as results\nare universal for a wide class of systems.\nBesides the growth rates of perturbations the directions of this growth are also important. There are various concepts identifying these directions including bred\nvectors [2, 3], which are finite-amplitude perturbations\ninitialized and periodically rescaled within the original\nphase space, singular or optimal vectors [4, 5], which\nare the singular vectors of a finite-time propagator, or\nfinite-time normal modes [6], defined as eigenvectors of\nthe propagator.\nOrthogonal sets of singular vectors related to the propagators operating on infinite time intervals were referred\nto by Legras and Vautard as forward and backward Lyapunov vectors [7]. These vectors can be computed in parallel with the Lyapunov exponents [7, 8], and, thus, are\nclosely related to them. Unlike the exponents, the forward and backward Lyapunov vectors depend on time,\ni.e., they are different for different trajectory points. Analyzing the orientation of these vectors, one can expect\nto recover the local structure of an attractor. But unfortunately, the forward and backward Lyapunov vectors\nprovide only limited information. They always remain\northogonal and thus cannot indicate directions of stable and unstable manifolds as well as their tangencies.\nThese vectors are not invariant under time reversal and\n\n\f2\nare not covariant with the dynamics. The latter means\nthat forward (or backward) vectors at a given point are\nnot mapped by tangent propagators to the forward (backward) vectors at the image point. Another drawback of\nthese vectors is their norm-dependence, i.e., they depend\non the definition of the inner products and norms in the\ntangent space [7].\nThe concept of norm-independent Lyapunov vectors ha\nbeen known for a long time [1, 7, 9, 10]. However, only\nrecently two efficient algorithms for computing these vectors were suggested almost simultaneously by Wolfe and\nSamelson [11] and by Ginelli et al. [12]. After Ginelli et al.\nwe call these vectors covariant Lyapunov vectors. Note\nthat these vectors are also referred to as characteristic\nLyapunov vectors [7, 11]. These vectors are not orthogonal, they are invariant under time reversal and covariant\nwith the dynamics in the sense that they may, in principle, be computed once and then determined for all times\nusing the tangent propagator. (Note that this is the case\nonly for exact covariant vectors, while those computed\nnumerically do not demonstrate perfect covariance due\nto the accumulation of numerical errors.) The covariant\nLyapunov vectors can be considered as a generalization of\nthe notion of \"normal modes.\" They are reduced to Floquet vectors if the flow is time periodic and to stationary\nnormal modes if the flow is stationary [11].\nIn view of potential wide applications to the analysis of\ncomplex, high-dimensional dynamics, the covariant Lyapunov vectors receive a lot of interest of researchers [13\u2013\n19]. For these extensive studies to be productive, it is\nimportant to analyze the Lyapunov vectors systematically. In this paper we summarize features of forward,\nbackward and covariant Lyapunov vectors and provide a\ndetailed explanation of both the theoretical basics and\nnumerical algorithms. We present and study in detail an\nefficient method for computing covariant Lyapunov vectors, which can be considered as a modification of the\nmethod by Wolfe and Samelson. Moreover, our general\napproach reveals the existence of adjoint covariant Lyapunov vectors. This is not an independent type of characteristic vectors, because given the covariant vectors,\none can always compute the adjoint ones. However, the\nangles between corresponding covariant and adjoint covariant vectors provide a compact representation of the\ninformation contained in the covariant vectors and can\nbe used as characteristic numbers. In particular, the\npresence of homoclinic tangencies is indicated by orthogonality of corresponding original and adjoint covariant\nvectors. Since the covariant as well as the adjoint covariant vectors are norm-independent their angles also are\ninvariant with respect to the norm.\nThe structure of the article is as follows. In Sec. I\nwe present the theory of Lyapunov exponents and forward and backward Lyapunov vectors, and in Sec. II we\ndescribe numerical methods for computing them. Section III presents the theoretical aspects of covariant Lyapunov vectors, and in Sec. IV we describe different methods of computing covariant vectors. Finally, in Sec. V a\n\nsimple illustrative example is presented. In Sec. VI we\nsummarize the results presented.\nI.\n\nLYAPUNOV EXPONENTS, FORWARD AND\nBACKWARD LYAPUNOV VECTORS\nA.\n\nBasic definitions\n\nConsider a system whose dynamics can be described\nby an ordinary differential equation\nu\u0307 = g(u, t),\n\n(1)\n\nwhere u \u2261 u(t) \u2208 Rm is an m-dimensional state vector that changes in time t, and g(u, t) \u2208 Rm is a nonlinear vector function. We are primarily interested in\nhigh-dimensional systems, so m is assumed to be large.\nEquation (1) can model a system with many interacting point-wise dynamical elements, or it can be a finite\nstep size approximation of a spatially extended system\nthat appears after discretization of spatial derivatives.\nInfinitesimal perturbations to a trajectory of this system\nare described by the following equation:\nv\u0307 = J(u, t)v,\n\n(2)\n\nwhere J(u, t) \u2208 Rm\u00d7m is the Jacobian matrix composed\nof derivatives of the vector function g(u, t) with respect\nto components of the vector u. The fundamental matrix\nM \u2208 Rm\u00d7m for Eq. (2) can be found as a solution of the\nmatrix equation\n\u1e40 = J(u, t)M,\n\n(3)\n\nwhere any non-singular matrix can be used as an initial\ncondition.\nThe tangent linear propagator or resolvent is defined\nas\nF (t1 , t2 ) = M(t2 )M(t1 )\u22121 ,\n\n(4)\n\nand can be represented by a non-singular m \u00d7 m matrix.\nThe propagator evolves solutions of Eq. (2) from time t1\nto time t2 :\nv(t2 ) = F (t1 , t2 )v(t1 ),\n\n(5)\n\nwhere v(t1 ) and v(t2 ) are tangent vectors at times t1 and\nt2 , respectively, computed along the same trajectory of\nthe base system (1). According to Eq. (4), the propagator is always non-singular and F (t1 , t2 ) = F (t2 , t1 )\u22121 .\nFurthermore we define the adjoint tangent propagator:\nG(t1 , t2 ) = F (t1 , t2 )\u2212T ,\n\n(6)\n\nwhere \"\u2212T\" denotes matrix inversion and transposition.\nIn general, a non-Euclidean norm can be defined in the\ntangent space, so that instead of the transposition a generalized adjoint with respect to the chosen norm has to\nbe used. In this paper we do not consider such cases.\n\n\f3\nAs follows from Eq. (5), the growth of the Euclidean\nnorm of tangent vectors in forward-time dynamics is\ndetermined by the matrix F (t1 , t2 )T F (t1 , t2 ). We denote its eigenvectors and eigenvalues as f +\ni (t1 , t2 ) and\n\u03c3i (t1 , t2 )2 , respectively, where \u03c31 (t1 , t2 ) \u2265 \u03c32 (t1 , t2 ) \u2265\n* * * \u2265 \u03c3m (t1 , t2 ) \u2265 0. The eigenvectors are termed optimal vectors because the maximal growth ratio is equal\nto \u03c31 (t1 , t2 ) and is achieved if the initial vector v(t1 ) coincides with f +\n1 (t1 , t2 ). The same role for the backwardtime dynamics plays the matrix F (t1 , t2 )\u2212T F (t1 , t2 )\u22121\nwith the reciprocal eigenvalues and the eigenvectors\nf\u2212\ni (t1 , t2 ).\nThe eigenvectors and eigenvalues can be found via singular value decompositions (SVD) [20] of the propagator\nmatrix and its inverse, thus:\n\u2212\nF (t1 , t2 )f +\ni (t1 , t2 ) = f i (t1 , t2 )\u03c3i (t1 , t2 ),\n\nF (t1 , t2 )\u22121 f \u2212\ni (t1 , t2 )\n\n=\n\n\u22121\nf+\n.\ni (t1 , t2 )\u03c3i (t1 , t2 )\n\n(7)\n(8)\n\nHere \u03c3i (t1 , t2 ) are called singular values, and f +\ni (t1 , t2 )\nand f \u2212\n(t\n,\nt\n)\nare\nright\nand\nleft\nsingular\nvectors\nof\n1 2\ni\nF (t1 , t2 ), respectively. The singular vectors are orthonormal. They are norm-dependent, i.e., they have different\norientations with respect to different norms [7, 11]. Taking into account Eqs. (6), (7), and (8), one can write the\nSVD for the adjoint propagator G(t1 , t2 ) and its inverse\nas\n\u2212\n\u22121\nG(t1 , t2 )f +\n,\ni (t1 , t2 ) = f i (t1 , t2 )\u03c3i (t1 , t2 )\n\u22121\n\nG(t1 , t2 )\n\nf\u2212\ni (t1 , t2 )\n\n=\n\nf+\ni (t1 , t2 )\u03c3i (t1 , t2 ).\n\n(9)\n(10)\n\nthat are stretched/contracted by factors \u03c3i (t1 , t2 ), see\nFig. 1(a). The volume at t2 is equal to the product of the\nfirst k singular values. Alternatively, we can consider a\nk-dimensional ball of unit radius at t1 . At t2 this ball is\ntransformed into an ellipsoid with axes along the vectors\nf\u2212\ni and lengths \u03c3i . One can describe this transformation\nof volumes by\n!\nk\nX\n\u03bc\u0303i (t1 , t2 ) ,\n(11)\nVk (t2 ) = Vk (t1 ) exp (t2 \u2212 t1 )\ni=1\n\nwhere Vk (t) is the k-dimensional volume, and \u03bc\u0303i (t1 , t2 ) =\nln \u03c3i (t1 , t2 )/(t2 \u2212t1 ) are stretch ratios that can be considered as local Lyapunov exponents. (Note that there are\nalternative definitions of local Lyapunov exponents that\nshall be considered below.)\nThe backward transformation with F (t1 , t2 )\u22121 is symmetric. At t = t2 we construct a unit volume using\nthe first k left singular vectors f \u2212\ni (t1 , t2 ). According to\nEq. (8), the right singular vectors span this volume at t =\nt1 , and the edges of this volume are stretched/contracted\nby factors \u03c3i\u22121 , see Fig. 1(b). In a similar manner we can\nconsider a unit ball at t2 that is transformed into an ellipsoid at t1 . Therefore, the volumes are again transformed\nin accordance with Eq. (11).\nThis discussion is also valid for the adjoint propagator G(t1 , t2 ). But because the singular values are now\nreciprocal, the volumes are transformed as\n!\nk\nX\n\u03bc\u0303i (t1 , t2 ) , (12)\nVk (t2 ) = Vk (t1 ) exp \u2212(t2 \u2212 t1 )\ni=1\n\nComparing Eq. (7) with (9) and Eq. (8) with (10) we see\nthat the propagators F (t1 , t2 ) and G(t1 , t2 ) have identical\nsingular vectors and reciprocal singular values.\nIf all \u03c3i (t1 , t2 ) are distinct, the singular vectors are\nunique up to a simultaneous change of signs of elements\n\u2212\nof f +\ni (t1 , t2 ) and f i (t1 , t2 ). In the presence of degeneracy, we still can find a set of orthonormal right singular\nvectors that are mapped according to Eq. (7) onto a set\nof orthonormal left singular vectors, but these sets are\nnot unique and can be selected arbitrarily.\nStrictly speaking, propagators and singular vectors as\nwell as the Lyapunov vectors considered below can depend on time both explicitly, and implicitly via state\nvectors u(t). To avoid complicated notation, we shall\nuse a compact form, like F (t1 , t2 ).\nB.\n\nProperties of propagators. Transformation of\nvolumes built on singular vectors\n\nLet us discuss how F (t1 , t2 ) transforms volumes of\ndifferent dimensions: segments, squares, cubes and so\non. Being at a trajectory point at t1 we construct a kdimensional unit volume using the first k right singular vectors f +\ni (t1 , t2 ). According to Eq. (7) F (t1 , t2 )\ntransforms these vectors into the left singular vectors\nf\u2212\ni (t1 , t2 ) associated with the trajectory point at t2\n\nC.\n\nFar-past and far-future operators. Forward and\nbackward Lyapunov vectors\n\nFor infinitely large time intervals we can expect to obtain limits for the stretch ratios and singular vectors.\nThe Oseledec multiplicative ergodic theorem [21] and its\ncorollaries state that the limit indeed exists for t2 \u2192 \u221e,\nand also a limit can be reached for t1 \u2192 \u2212\u221e. When\nt2 \u2192 \u221e, the far-future operator is defined as\n\u0002\n\u00031/(2(t2 \u2212t))\nW+ (t) = lim F (t, t2 )T F (t, t2 )\n(13)\nt2 \u2192\u221e\ni\nh\n= lim F+ (t, t2 )\u03a3(t, t2 )1/(t2 \u2212t) F+ (t, t2 )T ,\nt2 \u2192\u221e\n\n+\nwhere F+ (t, t2 ) = [f +\n1 (t, t2 ), . . . , f m (t, t2 )] and \u03a3(t, t2 ) =\ndiag[\u03c31 (t, t2 ), . . . , \u03c3m (t, t2 )] are matrices of singular vectors and values, respectively. The eigenvectors of the\nfar-future operator are the limits of vectors f +\ni (t, t2 ).\nWe denote them as \u03c6+\ni (t) and refer to them as forward\nLyapunov vectors. They are orthonormal and depend on\nt [7]. The convergence of the singular vectors to the Lyapunov vectors is considered in Ref. [22]. Logarithms of\neigenvalues of W+ (t), \u03bb1 \u2265 \u03bb2 \u2265 * * * \u2265 \u03bbm , are called\nLyapunov exponents. Regardless of time dependence of\nW+ (t), they do not depend on time.\n\n\f4\nF (t1 , t2 )\n\u03c31 f \u2212\n1\n\nf+\n1\n\n\u03c32 f \u2212\n2\n\nf+\n2\na)\n\nF (t1 , t2 )\u22121\n\u03c31\u22121 f +\n1\n\nf\u2212\n1\n\n\u03c32\u22121 f +\n2\nb)\n\nf\u2212\n2\n\nFigure 1. Transformation of a volume. (a) Forward step by the propagator F (t1 , t2 ), (b) backward step via F (t1 , t2 )\u22121 .\n\nThe far-past operator is defined as\nW\u2212 (t) =\n=\n\n\u00031/(2(t\u2212t1 ))\nF (t1 , t)\u2212T F (t1 , t)\u22121\n(14)\ni\nh\nlim F\u2212 (t1 , t)\u03a3(t1 , t)\u22121/(t\u2212t1 ) F\u2212 (t1 , t)T ,\n\nlim\n\nt1 \u2192\u2212\u221e\n\n\u0002\n\nt1 \u2192\u2212\u221e\n\n\u2212\nwhere F\u2212 (t1 , t) = [f \u2212\n1 (t1 , t), . . . , f m (t1 , t)]. The eigenvectors of this matrix are the limits of the left singular\nvectors f \u2212\ni (t1 , t) for t1 \u2192 \u2212\u221e. They are called backward Lyapunov vectors. These vectors are also referred\nto as Gram\u2013Schmidt vectors, because they can be computed in the course of a procedure, which includes Gram\u2013\nSchmidt orthogonalizations; see. Sec. II. We denote them\nby \u03c6\u2212\ni (t). Similar to the forward vectors, the backward\nLyapunov vectors are orthonormal, and depend on t [7].\nAs well as singular vectors, forward and backward Lyapunov vectors are norm-dependent [7, 11]. The logarithms of the eigenvalues of W\u2212 (t) are equal to the Lyapunov exponents with opposite signs.\nIn analogy with the finite-time case, the k-dimensional\nvolumes can be built on the forward Lyapunov vectors\n\n\u03c6+\ni (t). Modifying Eq. (11) we find that average growth\nrates of these volumes are the sums of Lyapunov exponents,\n\u0013\n\u0012\nk\nX\nVk (t2 )\n1\n.\n(15)\nln\n\u03bbi = lim\nt2 \u2192\u221e t2 \u2212 t1\nVk (t1 )\ni=1\n\nAs we shall see below, this formula is valid for almost\nany k-dimensional volume in the tangent space, not necessarily related to the forward Lyapunov vectors.\nThe Lyapunov exponents may not be all distinct.\nTo take possible degeneracy into account we introduce\nan additional notation. Let s be a number of distinct Lyapunov exponents (1 \u2264 s \u2264 m), and let \u03bb(i)\n(i = 1, 2, . . . , s) denote the ith distinct Lyapunov exponent with the multiplicity\n\u03bd (i) . So, we have \u03bb(1) > \u03bb(2) >\nPs\n(i)\n(s)\n* * * > \u03bb , and i=1 \u03bd = m. In what follows, to address the whole spectrum of Lyapunov exponents as well\nas related vectors, we shall employ lower indices while\npaying special attention to the multiplicity, we shall use\nupper indices. The notation \u03c6\u00b1\nwill stand for a set of\n\u03bb(i)\nvectors, related to the ith distinct Lyapunov exponent,\n\n\f5\nand \u03c6\u00b1\n, where j = 1, 2, . . . , \u03bd (i) , will denote the jth\n\u03bb(i) ,j\nvector related to \u03bb(i) .\nIn presence of the degeneracy forward and backward\nLyapunov vectors are not unique. But as we already\nmentioned for singular vectors, this is not an obstacle,\nbecause it is always sufficient to choose any orthonormal\nset of these vectors.\nThe adjoint propagator G can also be used to define\nfar-past and far-future operators and forward and backward vectors, respectively. The Lyapunov exponents are\nthe logarithms of the eigenvalues of the far-past operator, while the far-future operator is associated with the\nLyapunov exponents with inverted signs.\n\nD.\n\nOseledec subspaces. Asymptotic behavior of\narbitrary vectors and volumes\n\nLet us now discuss what happens with arbitrary vectors. The framework that helps to understand it is provided by the following set of subspaces:\n\b\n+\nSj+ (t) = span \u03c6+\n(t) i = j, j + 1, . . . , s , Ss+1\n(t) = \u2205,\n\u03bb(i)\n+\nSs+ (t) \u2282 Ss\u22121\n(t) \u2282 * * * \u2282 S1+ (t) = Rm .\n\n(16)\n\nIn other words, Sj+ (t) is spanned by forward Lyapunov\nvectors \u03c6+\n(t) (i \u2265 j) related to the distinct Lyapunov\n\u03bb(i)\nexponents starting from the jth\nPs one. Dimensions of these\nsubspaces are dim Sj+ (t) = i=j \u03bd (i) , where \u03bd (i) is the\nmultiplicity of \u03bb(i) . Analogous subspaces spanned by the\nbackward Lyapunov vectors \u03c6\u2212\n(t) are defined by\n\u03bb(i)\n\b\n(t) i = 1, 2, . . . , j , S0\u2212 (t) = \u2205,\nSj\u2212 (t) = span \u03c6\u2212\n\u03bb(i)\nS1\u2212 (t) \u2282 S2\u2212 (t) \u2282 * * * \u2282 Ss\u2212 (t) = Rm ,\n\n(17)\n\nP\nand their dimensions are dim Sj\u2212 (t) = ji=1 \u03bd (i) . These\nsets of subspaces are referred to as Oseledec splitting [7,\n21, 23].\nRecall that the propagator F (t1 , t2 ) maps each right\nsingular vector onto the corresponding left singular vector and stretching rates are determined by singular values, see Eq. (7). When (t2 \u2212 t1 ) \u2192 \u221e, the right and left\nsingular vectors converge to forward and backward Lyapunov vectors, respectively, and the stretching rates converge to the Lyapunov exponents. Hence, the Oseledec\nsubspace Sj+ (t) consists of vectors that asymptotically\ngrow or decay with rate \u03bb \u2264 \u03bb(j) . In turn, the vectors\nfrom Oseledec subspace Sj\u2212 (t) grow or decay with exponential rates \u03bb \u2265 \u03bb(j) backward in time.\n+\nConsider a vector v (j) (t) \u2208 Sj+ (t)\\Sj+1\n(t). This vector\n+\nis orthogonal to each \u03c6\u03bb(i) (t), where i < j, and obligatory\nhas a nonzero projection onto at least one of the vectors\n\u03c6+\n(t), related to the jth distinct Lyapunov exponent.\n\u03bb(j)\nIt means that being iterated for infinitely long time with\nthe propagator F , the vector v (j) (t) exponentially grows\n\nwith the average rate \u03bb(j) [21, 23, 24],\n+\nv (j) (t1 ) \u2208 Sj+ (t1 ) \\ Sj+1\n(t1 ) \u21d2\n\n(j)\n\nkF (t1 , t1 + t)v (j) (t1 )k \u223c e\u03bb\n\nt\n\n.\n\n(18)\n\n\u2212\nThe vectors v (j) (t) \u2208 Sj\u2212 (t) \\ Sj\u22121\n(t) behave analogously\nin backward time:\n\u2212\nv (j) (t1 ) \u2208 Sj\u2212 (t1 ) \\ Sj\u22121\n(t1 ) \u21d2\n\n(j)\n\nkF (t1 \u2212 t, t1 )\u22121 v (j) (t1 )k \u223c e\u2212\u03bb\n\nt\n\n.\n\n(19)\n\nVectors v (1) (t) \u2208 S1+ (t) \\ S2+ (t) fill almost the whole tangent space, because the excluded subspace S2+ (t) has a\nmeasure zero in Rm . It means that under the action of F\nalmost any vector, i.e., 1-dimensional volume, asymptotically grows or decays with the exponent \u03bb(1) , and its image tends to the subspace span{\u03c6\u2212\n(t)} = S1\u2212 (t). Con\u03bb(1)\nsider now a square, i.e., a 2-dimensional volume. First\nwe assume that \u03bb(1) is not degenerate so that \u03bd (1) = 1.\nAlmost any such square has a 1-dimensional intersection\nwith the subspace S2+ (t) \\ S3+ (t) of vectors v (2) (t) that\nare dominated by the \u03bb(2) [7, 23\u201325]. (Here \"almost\"\nmeans that there is a measure zero set of squares fully\nbelonging to subspaces with j > 1.) Thus, the area of the\nsquare asymptotically grows or decays with the exponent\n\u03bb(1) +\u03bb(2) . All segments within this square except a single\none approach the subspace span{\u03c6\u2212\n(t)}, while that one\n\u03bb(1)\n(t)}.\nAs\na\nresult,\nthis square tends\ngoes into span{\u03c6\u2212\n\u03bb(2)\ninto the subspace S2\u2212 . When \u03bd (1) = 2, the area of the\nsquare grows/decays with 2\u03bb(1) = \u03bb1 + \u03bb2 and the whole\nsquare is embedded into S1\u2212 . But when we take a cube,\nits volume grows or decays with 2\u03bb(1) +\u03bb(2) = \u03bb1 +\u03bb2 +\u03bb3\nand its image goes into S2\u2212 . In general this can be\nformulated as follows. Under the action of F almost\nany k-dimensional volume asymptotically grows or dePk\ncays with average exponential rate i=1 \u03bbi and tends to\n\u2212\nsettle down inside the subspace Si , where i is defined\n\u2212\nfrom the inequalities dim Si\u22121\n< k \u2264 dim Si\u2212 . In the\n\u2212\nsame way considering vectors v (j) (t) \u2208 Sj\u2212 (t)\\Sj\u22121\n(t) we\nsee that almost any k-dimensional volume being iterated\nin backward time with the propagator F (t1 , t2 )\u22121 grows\nP\nor decays with the exponential rate ki=1 \u03bbi and settles\n+\ndown in Si+ (t), such that dim Si+1\n(t) < k \u2264 dim Si+ (t).\nFormally, these asymptotic embeddings can be described\nas:\nF (t1 , t)Vk (t1 )\n\n\u2282\n\nt1 \u2192\u2212\u221e\n\nSj\u2212 (t),\n\n\u2212\ndim Sj\u22121\n(t) < k \u2264 dim Sj\u2212 (t),\n\nF (t, t2 )\u22121 Vk (t2 )\n\n\u2282\n\nt2 \u2192+\u221e\n\nSj+ (t),\n\n+\ndim Sj+1\n(t) < k \u2264 dim Sj+ (t).\n\n(20)\n\n(21)\n\nLet us now turn to the adjoint propagator G(t1 , t2 ). We\nrecall that its singular vectors coincide with the singular\n\n\f6\nvectors for F , while its singular values are reciprocal.\nHence the adjoint Oseledec subspaces can be defined as\n\b\nHj+ (t) = span \u03c6+\n(t) i = 1, 2, . . . , j , H0+ (t) = \u2205,\n\u03bb(i)\nH1+ (t) \u2282 H2+ (t) \u2282 * * * \u2282 Hs+ (t) = Rm ,\n\nHj\u2212 (t)\n\n= span\n\n\u2212\n\u03c6\u2212\n(t) i = j, j + 1, . . . , s , Hs+1\n(t)\n\u03bb(i)\n\u2212\n\u2282 Hs\u22121\n(t) \u2282 * * * \u2282 H1\u2212 (t) = Rm .\n\n= \u2205,\n\n(23)\n\n+\n\u2212\nNote that Hj\u22121\n(t) \u22a5 Sj+ (t) and Hj+1\n(t) \u22a5 Sj\u2212 (t). Reasoning in the same way as above, we find that the adjoint\npropagator G generates the following asymptotic behavior as t \u2192 \u221e:\n+\nv (j) (t1 ) \u2208 Hj+ (t1 ) \\ Hj\u22121\n(t1 ) \u21d2\n\n(j)\n\nt\n\n, (24)\n\n(j)\n\nt\n\n, (25)\n\nkG(t1 , t1 + t)v (j) (t1 )k \u223c e\u2212\u03bb\n\u2212\nv (j) (t1 ) \u2208 Hj\u2212 (t1 ) \\ Hj+1\n(t1 ) \u21d2\n\nkG(t1 \u2212 t, t1 )\u22121 v (j) (t1 )k \u223c e\u03bb\nand the asymptotic embeddings read:\nG(t1 , t)Vk (t1 )\n\n\u2282\n\nt1 \u2192\u2212\u221e\n\nHj\u2212 (t),\n\n\u2212\ndim Hj+1\n(t) < k \u2264 dim Hj\u2212 (t),\n\nG(t, t2 )\u22121 Vk (t2 )\n\n\u2282\n\nt2 \u2192+\u221e\n\nHj+ (t),\n\n+\ndim Hj\u22121\n(t) < k \u2264 dim Hj+ (t).\n\nE.\n\n+\n+\n\u03a6+ (t) = [\u03c6+\n1 (t), \u03c62 (t), . . . , \u03c6m (t)]\n\n(22)\n\n\b\n\nHs\u2212 (t)\n\nLet us first assume that there is no degeneracy i.e.,\nall Lyapunov exponents are distinct. In matrix form we\nhave F (t1 , t2 )\u03a6+ (t1 ) = \u03a8+ (t2 ), where\n\n(26)\n\n(27)\n\nFinite-time evolution of forward and backward\nLyapunov vectors\n\nNow we need to discuss how orthogonal Lyapunov vectors are transformed in finite time intervals. First consider the action of F (t1 , t2 ) on forward Lyapunov vectors. For any such vector related to the jth distinct Lyapunov exponent \u03bb(j) we can write: \u03c6+\n(t ) \u2208 Sj+ (t1 ) \\\n\u03bb(j) ,i 1\n\n+\nSj+1\n(t1 ), where i = 1, 2, . . . , \u03bd (j) , see Eq. (16). It means\nthat this vector shows the asymptotic behavior (18), i.e.,\nit grows or decays with the exponent \u03bb(j) forward in time.\n(t ) \u2208\nIn turn, it means that F (t1 , t2 )\u03c6+\n(t ) = \u03c8 +\n\u03bb(j) ,i 2\n\u03bb(j) ,i 1\n+\n+\n+\nSj (t2 ) \\ Sj+1 (t2 ). We see that the image of \u03c6\u03bb(j) ,i (t1 )\n(t ) with n < j. But\nat t2 is orthogonal to vectors \u03c6+\n\u03bb(n) 2\nthis is not a forward Lyapunov vector anymore, because\n+\nthe subspaces span{\u03c6+\n(t )} and Sj+ (t2 ) \\ Sj+1\n(t2 ) are\n\u03bb(j) 2\n+\n+\nnot identical. Vectors from Sj (t2 ) \\ Sj+1 (t2 ) obligatory\nhave a nonzero projection inside span{\u03c6+\n(t )} but typ\u03bb(j) 2\nically do not belong to it and also have projections onto\nforward vectors with n > j.\n\n(28)\n\nis the matrix consisting of the forward Lyapunov vectors.\nAccording to the above discussion of \u03c8 +\n(t ), the first\n\u03bb(j) ,i 2\n+\nvector-column of \u03a8 (t2 ) is collinear with \u03c6+\n1 (t2 ). The\nsecond one is orthogonal to \u03c6+\n(t\n),\nbut\ncan\nhave\nnonzero\n2\n1\nprojections onto all others forward vectors. The third one\n+\nis orthogonal both to \u03c6+\n1 (t2 ) and to \u03c62 (t2 ) and so on.\nThus we can write\n\u03a8+ (t2 ) = \u03a6+ (t2 )L,\n\n(29)\n\nwhere L is a lower triangular matrix.\nWhen the spectrum of Lyapunov exponents is degenerate, the matrix \u03a6+ (t2 ) is not unique. There exist\nsubspaces span{\u03c6+\n(t )} corresponding to each unique\n\u03bb(j) 2\nLyapunov exponent, such that any vector from these subspaces can be treated as a forward Lyapunov vector. This\nmeans that the decomposition (29) is also not unique,\nbecause there exists a variety of non-triangular matrices L satisfying this equation. But the representation of\n\u03a8+ (t2 ) as a product of an orthogonal and a lower triangular matrices exists and is unique regardless of the\ndegeneracy of Lyapunov exponents. In fact, this is the\nwell-known QL factorization [20]. The analysis of the\ndetails of the factorization procedure shows that the orthogonal matrix can always be treated as a matrix of\nforward Lyapunov vectors. Hence, regardless of the degeneracy, Eq. (29) remains valid.\nAltogether, the propagator F maps forward Lyapunov\nvectors onto new vectors that are not Lyapunov vectors. In other words, forward Lyapunov vectors are noncovariant with the dynamics. To recover forward Lyapunov vectors, we have to perform a QL factorization.\nFor the subsequent analysis it is convenient to represent\nit as a mapping backward in time:\nF (t1 , t2 )\u22121 \u03a6+ (t2 ) = \u03a6+ (t1 )LF (t1 , t2 ),\n\n(30)\n\nwhere LF (t1 , t2 ) \u2208 Rm\u00d7m is a lower triangular matrix.\nBecause the propagator is non-singular and QL factorization is unique (if one requires for all diagonal elements\nof LF (t1 , t2 ) to be positive), this equation determines\n\u03a6+ (t1 ) via \u03a6+ (t2 ) in a unique way. By definition, the\ndiagonal elements of LF (t1 , t2 ) do not vanish, i.e, this\nmatrix is non-singular.\nRepeating the above discussion for the backward Lyapunov vectors, we see that regardless of the degeneracy,\nthe following relation is always valid:\nF (t1 , t2 )\u03a6\u2212 (t1 ) = \u03a6\u2212 (t2 )RF (t1 , t2 ),\n\n(31)\n\n\u2212\n\u2212\n\u03a6\u2212 (t) = [\u03c6\u2212\n1 (t), \u03c62 (t), . . . , \u03c6m (t)],\n\n(32)\n\nwhere\n\n\f7\nv3\n\nq3\n\nv2\nq2\n\nq1\n\nv1\n\nFigure 2. The idea of orthogonalization. The vectors v i are the result of mapping (5) and vectors q i are their orthogonalization:\nq 1 is collinear to v 1 , q 2 belongs to the plane spanned by v 1 and v 2 , and q 3 belongs to the space spanned by vectors v 1 , v 2 ,\nand v 3 .\n\nand RF (t1 , t2 ) is an upper triangular matrix with a\nnonzero diagonal. Like the forward vectors, the backward vectors are non-covariant with the dynamics.\nFor the adjoint propagator G(t1 , t2 ) we obtain:\nG(t1 , t2 )\u22121 \u03a6+ (t2 ) = \u03a6+ (t1 )RG (t1 , t2 ),\n\u2212\n\n\u2212\n\nG\n\nG(t1 , t2 )\u03a6 (t1 ) = \u03a6 (t2 )L (t1 , t2 ),\n\n(33)\n(34)\n\nwhere RG (t1 , t2 ) and LG (t1 , t2 ) are upper and lower nonsingular triangular matrices, respectively.\nII. NUMERICAL COMPUTATION OF\nLYAPUNOV EXPONENTS AND FORWARD AND\nBACKWARD VECTORS\n\nThe definition of the Lyapunov exponents and vectors\ncannot be implemented directly as a numerical algorithm.\nIt is impossible to solve Eq. (3) for a sufficiently long time\ninterval t2 \u2212 t1 , to calculate the propagator F (t1 , t2 ), and\nthen to find a good approximation for the limit matrix\nW+ . As we already discussed above, when we move away\nfrom the starting point t1 almost any vector approaches\nthe first backward Lyapunov vector \u03c6\u2212\n1 (t), i.e., falls into\nsubspace S1\u2212 (t). Hence, in this way we can compute only\nthe largest Lyapunov exponent and the corresponding\nvector.\nEquation (31) determines a mapping of backward Lyapunov vectors at t1 onto backward Lyapunov vectors at\nt2 . A set of all backward vectors at different times can\nbe considered as a kind of limit set, attracting or repelling, and the mapping (31) can be treated as stationary dynamics on this set. This gives an idea for\nan iterative computation of the backward Lyapunov vectors. One can initialize an arbitrary orthogonal matrix and start iterations including mapping by F and\nQR factorization as described by Eq. (31). These iterations converge to the backward Lyapunov vectors where\nconvergence is guaranteed by Eq. (20). One sees that\n\nthe forward in time mapping embeds an arbitrary volume into the subspace spanned by backward Lyapunov\nvectors. It means that in the course of forward iterations F (tn , tn+1 )Q(tn ) = Q(tn+1 )RF (tn+1 , tn ) columns\nof Q(tn ) \u2208 Rm\u00d7m converge to backward Lyapunov vectors. In fact this idea was suggested almost simultaneously by Benettin et al. [23, 26] and by Shimada and\nNagashima [24] to compute the Lyapunov exponents.\nThe convergence of these iterations towards the backward Lyapunov vectors is discussed in Refs. [7, 8].\nConsider the iterations in more detail, see Fig. 2. Suppose we have an orthogonal matrix Q(tn ). First we determine F (tn , tn+1 ) for some interval tn+1 \u2212 tn , which\ntypically is not very large, and perform the mapping\nV(tn+1 ) = F (tn , tn+1 )Q(tn ). The first vector-column\nv 1 of V(tn+1 ) behaves as we need, namely it approaches\nthe subspace S1\u2212 . So, we only normalize it to prevent\noverflow or underflow: v 1 \u2192 q 1 , kq1 k = 1. The plane\nspanned by vectors v 1 and v 2 approaches the subspace\nS2\u2212 if \u03bb1 6= \u03bb2 , or it goes into S1\u2212 otherwise. In the first\ncase we need to prevent the collapse of the plane due to\nthe alignment of v 2 along \u03c6\u2212\n1 , and also the orientation\nof the plane has to be preserved to support the convergence. These two goals can be achieved by finding a new\nvector q 2 which is orthogonal to q 1 and belongs to the\nplane originally spanned by v 1 and v 2 . This vector is\nalso normalized. In the second case, when \u03bb1 = \u03bb2 , there\nis no alignment and, in principle, there are more options\nhow to define q 2 . But it is allowed anyway to compute\nq 2 as if the degeneracy was absent, and this is the most\nreasonable choice making the procedure most transparent. In a similar manner we find the third normalized\nvector q 3 that is orthogonal to q 1 and q 2 and belongs to\nthe space spanned by v 1 , v 2 and v 3 . Doing so for all the\nremaining columns of V(tn+1 ) we compose the matrix\nQ(tn+1 ) whose columns are vectors q i . Then we use this\nQ(tn+1 ) as an initial value for the next mapping with\nF (tn+1 , tn+2 ) and repeat the procedure. After many recursions the columns of Q(tn ) converge to the backward\n\n\f8\nF (tn , tn+1 )\n\nq2\n\nq2\nr22\n\nv2\n\nq1\nr12\n\nq1\n\nv 1 = r11 q 1\n\nFigure 3. Computation of a volume after the mapping by F (tn , tn+1 ).\n\nLyapunov vectors. This procedure works not only for the\nwhole set of vectors, but allows to compute any number\nof the first backward Lyapunov vectors.\nThe described procedure eliminates the ambiguity of\nbackward Lyapunov vectors that emerge when not all\nLyapunov exponents are distinct. Particular directions\nof backward Lyapunov vectors corresponding to each degenerate Lyapunov exponent \u03bb(j) depend on the choice\nof the initial matrix Q(t0 ). But these variations remain\nwithin subspace span{\u03c6\u2212\n} so that any choice is appro\u03bb(j)\npriate. Moreover, in practical computations the degeneracy manifests itself very weakly, because typically the\ndegenerate Lyapunov exponents converge to identical values very slowly. In fact, dealing with a high-dimensional\nsystem one needs to know in advance which of the exponents are expected to be identical to identify them in the\ncomputed spectrum.\nThe computation of the Lyapunov exponents is illustrated in Fig. 3. An initial unit square composed of vectors q i (tn ) is transformed into the parallelogram spanned\nby the vectors v i (tn+1 ). After the orthogonalization we\nobtain q i (tn+1 ). To compute the area of the parallelogram we can construct a rectangle with identical area\nby projecting v j (tn+1 ) onto q i (tn+1 ): rij = q i v j . As\nwe see from the figure, the area is r11 r22 . Similarly, a\nk-dimensional unit volume after the mapping is equal to\nr11 r22 . . . rkk . Thus, we can define the local Lyapunov\nexponents as\n\u03bb\u0303i = ln(rii )/(tn+1 \u2212 tn ).\n\n(35)\n\nIn the course of the mapping/orthogonalization iterations\nwe need to accumulate and average \u03bb\u0303i to obtain the Lyapunov exponents.\nBy construction, the first vector v 1 has only one\nnonzero projection onto q 1 , the second vector v 2 has\ntwo nonzero projections, onto q 1 and q 2 , the third vector v 3 has three nonzero projections onto first three\nvectors q i and so on. It means that rij are elements\nof an upper triangular matrix. So, the procedure described above represents the matrix V as the product\n\nV = QR. Here Q is an orthogonal matrix such that\nspan{q 1 , q 2 , . . . q k } = span{v1 , v 2 , . . . v k } for any k \u2264 m,\nand R is an upper triangular matrix consisting of the projections of columns of V onto columns of Q. This procedure is called QR factorization [20]. There are different\nnumerical algorithms of the QR factorization. Note that\nthe often used Gram\u2013Schmidt algorithm as well as its\nmodified version are not very accurate when the dimension of the tangent space is large [27]. Most high precision QR algorithms are based on so called Householder\ntransformations [20, 28].\nAnother way to compute backward Lyapunov vectors\nis based on the adjoint propagator G. Equation (34) determines the stationary dynamics, and Eq. (26) indicates\nthat the forward iterations converge to this dynamics.\nBecause G(tn , tn+1 ) has reciprocal singular values, the\nvalue \u03c3m (tn , tn+1 )\u22121 dominates in the course of forward\niterations with the adjoint propagator. It means that\ncolumns of Q converge to the backward Lyapunov vectors in the reverse order. If we rearrange columns of \u03a6\u2212\nin Eq. (34) in the reverse order, we also have to transpose LG with respect to its diagonal and with respect\nto the antidiagonal. As a result we obtain an upper triangular matrix. Thus, the algorithm is identical to the\none previously described. We perform the mapping by\nG(tn , tn+1 ), find a QR factorization of the resulting matrix, take Q(tn+1 ), and do the next recursion.\nConsider now the computation of the forward Lyapunov vectors. The first algorithm is based on Eqs. (27)\nand (33). We need to move backward in time alternating mappings with G(tn , tn+1 )\u22121 and QR factorizations.\nThe matrices Q converge to \u03a6+ , and the forward Lyapunov vectors come up in the correct order. Note that\nG \u22121 is merely the transposition of F , see Eq. (6). In\nthe course of this procedure we can compute local Lyapunov exponents as logarithms of diagonal elements of\ntriangular matrices per unit time. For short time intervals these local exponents will differ from those given by\nEq. (35), but being averaged over many times steps they\nalso converge to the Lyapunov exponents.\n\n\f9\nAnother algorithm for the forward Lyapunov vectors is\nbased on Eqs. (30) and (21). The procedure is the same\nas above except using the inverted propagator F \u22121 . This\nmethod computes the vectors in the reversed order, and,\nhence, the previous one is usually more applicable. The\nidea to apply the transposed propagator instead of the\ninverted one was suggested in Ref. [7].\nThe implementation of the algorithm with the transposed propagator F T is straightforward for discrete time\nsystems (e.g. coupled map lattices), where the action\nof F T on a set of (Lyapunov) vectors can be computed\nusing the transposed Jacobian matrix of the system. In\nprinciple, one can do the same with continuous systems,\nbut in that case one would have to compute the full\npropagator F first by solving m copies of the linearized\nODE (2) and then use its transpose F T to evolve the\ndesired number of tangent vectors. This implementation is inefficient if the system is very high dimensional\n(m \u226b 1) and if only a few Lyapunov vectors are to be\ncomputed. As an alternative, the action of F T can reformulated as follows. Using the Magnus expansion [29],\nwe can represent the propagator of Eq. (2) via matrix exponential functions as F (t1 , t2 ) = exp[\u03a9F (t1 , t2 )]. Here\n\u03a9F (t1 , t2 ) is a matrix that is given as a series expanP\u221e\nF\n(t1 , t2 ), with \u03a9F\nsion \u03a9F (t1 , t2 ) =\n1 (t1 , t2 ) =\ni=1 \u03a9i R\nR t2\nR \u03c41\nF\n1 t2\nt1 J(\u03c41 )d\u03c41 , \u03a92 (t1 , t2 ) = 2 t1 d\u03c41 t1 d\u03c42 [J(\u03c41 ), J(\u03c42 )],\nand so on, see [29], J(\u03c4 ) \u2261 J(u, \u03c4 ) is the Jacobian\nmatrix, and [*, *] denotes the matrix commutator. The\nadjoint propagator reads: G(t1 , t2 ) = F (t1 , t2 )\u2212T =\nexp{\u2212[\u03a9F (t1 , t2 )]T } = exp[\u03a9G (t1 , t2 )]. The matrix\n\u03a9G (t1 , t2 ) = \u2212[\u03a9F (t1 , t2 )]T generating G(t1 , t2 ) is obtained with a Magnus expansion where the Jacobian matrix J(u, t) is replaced by \u2212J(u, t)T . So, to compute the\naction of G(t1 , t2 ) on a tangent vector we have to solve\nthe following linear ODE\nv\u0307 = \u2212J(u, t)T v\n\n(36)\n\nforward in time (from t1 to t2 > t1 , because the action of\nthe adjoint propagator G(t1 , t2 ) corresponds to moving\nforward in time). To compute forward Lypaunov vectors\nusing F (t1 , t2 )T = G(t1 , t2 )\u22121 we have to invert G(t1 , t2 ).\nThis can be done by integrating the required number of\ncopies of Eq. (36) and the basic system (1) backward in\ntime (from t2 to t1 ).\nAll four algorithms compute the dominating Lyapunov\nexponents and corresponding vectors with the highest\nprecision, while the remaining part of the spectrum is\nnot very accurate. Namely, F - and G \u22121 -algorithms do\nthe best for the first Lyapunov exponent and vectors,\nwhile F \u22121 - and G-algorithms achieve the highest accuracy for the mth exponent and vectors. One can perform F - and G-algorithms in parallel, and then construct\nweighted sums of computed exponents and backward vectors to obtain the whole spectrum with very high precision. Similarly, performing backward iterations simultaneously with F \u22121 and G \u22121 one can compute the forward\nLyapunov vectors with improved accuracy.\n\nIII.\n\nCOVARIANT LYAPUNOV VECTORS\n\nOrthogonal matrices computed according to QR decomposition preserve subspaces spanned by each first k\ncolumns of a factorized matrix. The QL decomposition\npreserves subspaces spanned by each last k columns of\na factorized matrix. It means that considering Eqs. (30)\nand (31), we can conclude, that Oseledec subspaces (16)\nand (17) are preserved under the tangent flow [1, 7]. The\nsame conclusion follows from Eqs. (33) and (34) for the\nsubspaces (22) and (23):\nF (t1 , t2 )Sj+ (t1 ) = Sj+ (t2 ),\nF (t1 , t2 )Sj\u2212 (t1 ) = Sj\u2212 (t2 ),\nG(t1 , t2 )Hj+ (t1 ) = Hj+ (t2 ),\nG(t1 , t2 )Hj\u2212 (t1 ) = Hj\u2212 (t2 ).\n\n(37)\n\n(38)\n\nSo, the Oseledec subspaces are invariant under time reversal and covariant with the dynamics. But this is not\nthe case for the forward and backward Lyapunov vectors\nthemselves. Being multiplied by F and G they also have\nto be multiplied by lower or upper triangular matrices\nto be mapped to new forward and backward Lyapunov\nvectors, see Eqs. (30), (31), (33), and (34).\nGiven the covariant subspaces, it is natural to search\nfor some vectors inside these subspaces that are also covariant with the dynamics and are invariant with respect\nto time reversal. These vectors are referred to as covariant Lyapunov vectors [12]. We denote them by \u03b3 j (t).\nThe basic property of these vectors (which are covariant\nwith respect to the propagator F ) can be written as\nkF (t1 , t1 \u00b1 t)\u03b3 j (t1 )k \u223c exp(\u00b1\u03bbj t)\n\n(39)\n\nfor any t1 and t \u2192 \u221e. The covariant Lyapunov vectors are norm-independent [7, 11]. Also we can introduce\nnorm-independent adjoint vectors \u03b8j (t) that are covariant with respect to the adjoint dynamics:\nkG(t1 , t1 \u00b1 t)\u03b8 j (t1 )k \u223c exp(\u2213\u03bbj t).\n\n(40)\n\nEquation (39) means that Eqs. (18) and (19) are fulfilled\nsimultaneously, and Eq. (40) implies the simultaneous\nvalidity of Eqs. (24) and (25). It means that the covariant Lyapunov vectors belong to the intersection of the\nOseledec subspaces [1, 7, 30], and the adjoint covariant\nvectors can be found within the intersections of the adjoint subspaces:\n\u03b3 j (t) \u2208 Sj+ (t) \u2229 Sj\u2212 (t),\n\u03b8j (t) \u2208\n\nHj+ (t)\n\n\u2229\n\nHj\u2212 (t).\n\n(41)\n(42)\n\nThese intersections are always nonempty because the\nsum of dimensions of Oseledec subspaces is always higher\nthan the dimension of the whole tangent space.\n+\nConsider arbitrary vectors v (j) (t1 ) \u2208 Sj+ (t1 )\\Sj+1\n(t1 ),\nwhere j = 1, 2, . . . , s, and s is the number of distinct Lyapunov exponents. There are \u03bd (j) linearly independent\n\n\f10\nvectors corresponding to the jth Lyapunov\n\u03bb(j) ,\nPexponent\ns\n(j)\nand the total number of such vectors is j=1 \u03bd = m.\nRepresenting the whole set of these vectors as a matrix\nV, we obtain V = \u03a6+ A+ , where A+ is a lower triangular matrix, and \u03a6+ is a matrix of forward Lyapunov\nvectors (28). As follows from Eq. (18), when the forward propagator F is applied to these vectors, the first\n\u03bd (1) of them grow or decay asymptotically with the exponent \u03bb(1) , the next \u03bd (2) vectors grow / decay with the\nexponent \u03bb(2) and so on. In a similar manner we can\n\u2212\nconsider arbitrary vectors v (j) (t1 ) \u2208 Sj\u2212 (t1 ) \\ Sj\u22121\n(t1 ).\nThe matrix of these vectors V can be found as V =\n\u03a6\u2212 A\u2212 , where A\u2212 is an upper triangular matrix, and\n\u03a6\u2212 is defined by Eq. (32). According to Eq. (19), acting upon these vectors by the inverted propagator F \u22121 ,\nwe can observe that the first \u03bd (1) of them grow or decay asymptotically with the exponent \u2212\u03bb(1) , the next\n\u03bd (2) vectors grow / decay with the exponent \u2212\u03bb(2) and\nso on. Let \u0393(t) = [\u03b3 1 (t), \u03b3 2 (t), . . . , \u03b3 m ] be a matrix\nconsisting of the covariant Lyapunov vectors, and let\n\u0398(t) = [\u03b81 (t), \u03b8 2 (t), . . . , \u03b8 m ] be a matrix of adjoint covariant vectors. As follows from Eq. (39), the covariant\nvectors have to demonstrate both forward (18) and backward (19) asymptotic behavior. It means that there exist\nan upper triangular matrix A\u2212 and a lower triangular\nmatrix A+ , such that\n\nseveral couples A\u2212 (t) and A+ (t), resulting in different\nmatrices \u0393, and, hence, in different covariant Lyapunov\nvectors. As an example one can consider a matrix \u03a6+\nthat consists of columns of \u03a6\u2212 arranged in the reverse\norder. Ambiguity of A\u2212 (t) and A+ (t) means that there\nare Lyapunov exponents associated with several covariant vectors. But on the other hand, the total number\nof covariant vectors is equal to the total number of Lyapunov exponents m, and there are no exponents without\nvectors. It means that the ambiguity can occur if and\nonly if the Lyapunov exponents are degenerate. The covariant vectors associated with a k times degenerate Lyapunov exponent can have arbitrary orientation within a\nk-dimensional subspace corresponding to this exponent.\nBut because any set of linearly independent covariant\nvectors from the subspace corresponding to the degenerate exponent is as good as any other, this ambiguity\ncan be ignored: we just need to have any linear independent set of vectors. (We recall that though forward\nand backward vectors are also subject to the degeneracy,\ntheir ambiguity is eliminated in the course of the computations, see Sec. II.)\nLet us find how \u0393(t1 ) is transformed by F (t1 , t2 ). In\ngeneral we can write\n\n\u0393(t) = \u03a6\u2212 (t)A\u2212 (t) = \u03a6+ (t)A+ (t).\n\nwhere CF (t1 , t2 ) is a matrix whose structure should be\ndetermined. When the Lyapunov spectrum is not degenerate, Eq. (41) immediately implies that CF (t1 , t2 )\nis diagonal. To show that this is the case regardless of\nthe degeneracy, we substitute \u0393(t) = \u03a6+ (t)A+ (t), see\nEq. (43), in Eq. (48) and, taking into account Eq. (30),\nwe obtain\n\n(43)\n\nReasoning in a similar manner one obtains for the adjoint\nvectors:\n\u0398(t) = \u03a6+ (t)B+ (t) = \u03a6\u2212 (t)B\u2212 (t),\n\n(44)\n\nwhere B+ (t) and B\u2212 (t) are upper and lower triangular\nmatrices, respectively. Note that Eqs. (43) and (44) convey, in fact, the same property of covariant vectors as\nEq. (41) and (42), respectively. Multiplying Eq. (43) by\n[\u03a6+ (t)]T and Eq. (44) by [\u03a6\u2212 (t)]T we obtain the relations between triangular matrices that will be required\nlater:\nP(t)A\u2212 (t) = A+ (t),\n\n(45)\n\nP(t)T B+ (t) = B\u2212 (t),\n\n(46)\n\nP(t) = [\u03a6+ (t)]T \u03a6\u2212 (t)\n\n(47)\n\nwhere\n\nis a m \u00d7 m orthogonal matrix.\nIf the Lyapunov exponents are degenerate, the covariant vectors are not unique. Let us discuss what Eq. (43)\nimplies in this case (Eq. (44) can be considered in the\nsame way). If \u0393(t) is known, then we can compute \u03a6\u2212 (t)\nand A\u2212 (t), and \u03a6+ (t) and A+ (t) via QR and QL decompositions, respectively, in a unique way. However,\nEq. (43) does not determine \u0393(t) via \u03a6+ (t) and \u03a6\u2212 (t)\nin a unique way. In principle, there exist orthogonal matrices \u03a6\u2212 and \u03a6+ that allow one to fulfill Eq. (43) with\n\nF (t1 , t2 )\u0393(t1 ) = \u0393(t2 )CF (t1 , t2 ),\n\nLF (t1 , t2 )A+ (t2 )CF (t1 , t2 ) = A+ (t1 ).\n\n(48)\n\n(49)\n\nSince all known matrices here are lower triangular,\nCF (t1 , t2 ) is also lower triangular. Analogously substituting \u0393(t) = \u03a6\u2212 (t)A\u2212 (t) in Eq. (48) and using Eq. (31)\nwe obtain:\nRF (t1 , t2 )A\u2212 (t1 ) = A\u2212 (t2 )CF (t1 , t2 ),\n\n(50)\n\ni.e., CF (t1 , t2 ) is an upper triangular matrix. Simultaneous upper and lower triangular structure has only a diagonal matrix: CF (t1 , t2 ) =\ndiag[c1 (t1 , t2 ), c2 (t1 , t2 ), . . . , cm (t1 , t2 )].\nHence, the\nvectors \u03b3 j can freely evolve under the tangent flow (48)\nso that the tangent flow preserves their directions. The\ndirection, represented by \u03b3 j (t1 ) at t1 is mapped onto\nthe direction pointed by \u03b3 j (t2 ) at t2 , and the backward\nstep maps \u03b3 j (t2 ) onto the direction of \u03b3 j (t1 ). The\nvectors themselves are stretched or contracted by factors\ncF\ni (t1 , t2 ). (Recall, that the directions of the forward\nand the backward Lyapunov vectors are not preserved).\nThe adjoint vectors freely evolve under the tangent flow\ngenerated by the adjoint propagator:\nG(t1 , t2 )\u0398(t1 ) = \u0398(t2 )[CG (t1 , t2 )]\u22121 .\n\n(51)\n\n\f11\nOne can say that the vectors \u03b3 j are covariant with the\ntangent dynamics generated by F and the adjoint vectors\n\u03b8j are covariant with the tangent dynamics of G. This is\nthe reason why these vectors are referred to as covariant\nvectors.\nSince the covariant vectors are defined up to an arbitrary length, the diagonal elements of CF can be defined in various ways. In particular, to fulfill Eq. (39)\nwe should not normalize the vectors, and CF \u2261 I in\nthis case. However, in the course of numerical computations we need to avoid overflows and underflows.\nHence, constant lengths of \u03b3 j (t) have to preserved with\nrespect to the chosen norm. In this case cF\nj (t1 , t2 ) =\n||F (t1 , t2 )\u03b3 j (t1 )||/||\u03b3 j (t1 )||, and\nln[cF\nj (t1 , t2 )]/(t2 \u2212 t1 )\n\n(52)\n\ncan be treated as local Lyapunov exponent. The values of\nthese local Lyapunov exponents depend on the norm, but\nbeing averaged over many / long time intervals (t1 , t2 ),\nregardless of the norm they converge to the Lyapunov exponents \u03bbj . Consider an important particular case. As\nfollows from the discussions in Sec. II, one can build unit\nvolumes using the covariant Lyapunov vectors when the\ndiagonal elements of the upper triangular matrix A\u2212 are\nequal to 1, see Eq. (43). Equation (50) describes the dynamics of A\u2212 corresponding to the tangent dynamics of\nthe covariant Lyapunov vectors. When two upper triangular matrices are multiplied, the resulting matrix is also\nupper triangular and its diagonal elements are the products of the diagonal elements of the multipliers. Thus, if\nthe covariant Lyapunov vectors are rescaled to preserve\nones on the diagonal of A\u2212 , then the cF\nj are equal to\nF\nthe diagonal elements of R , and the local Lyapunov\nexponents (52) coincide with those defined by Eq. (35):\nln[cF\nj (t1 , t2 )]/(t2 \u2212 t1 ) = \u03bb\u0303j (t1 , t2 ).\nLet us now discuss what it means if covariant vectors\nmerge. The phase space of dynamical systems can contain structures called \"wild hyperbolic sets\" that are responsible for the existence of structurally stable and unavoidable homoclinic tangencies between stable and unstable manifolds. In turn, the presence of these tangencies results in formation of non-hyperbolic chaotic attractors [31]. Since covariant vectors are associated with invariant manifolds of trajectories, in points of tangencies\nthe corresponding vectors become collinear [1, 11, 12, 16].\nThe same happens with the corresponding adjoint covariant vectors. Collinear vectors result in a singularity\nof the matrices \u0393(t), and \u0398(t). The triangular matrices\nA\u00b1 (t) and B\u00b1 (t) also become singular. Note, that this\nproperty is time-invariant: as follows from Eq. (48) and\n(51) if some of covariant vectors are identical at t = t1 ,\nthey remain identical for all time. In practice, selecting an arbitrary trajectory we almost never hit exactly\nthe trajectory with the tangencies. But if a trajectory\nwith tangencies exists, the arbitrarily selected orbit will\npass infinitely close to it and we will encounter with a\nnonzero frequency ill-conditioned matrices of covariant\n\nvectors. Note that this is not the case for orthogonal\nforward and backward vectors, which are not affected by\ntangencies.\nNow we consider how covariant and adjoint covariant vectors are related to each other. First of all notice that given \u0393(t), one can always compute \u03a6\u2212 (t) and\nA\u2212 (t) as its QR decomposition and \u03a6+ (t) and A+ (t)\nas a QL decomposition. Then one can construct the matrix P(t) = [\u03a6+ (t)]T \u03a6\u2212 (t) and compute \u0398(t) via the LU\nmethod as described below in Sec. IV B. It means that\nthese two sets of vectors are not independent from each\nother. However, the mutual orientation of these vectors\ncan help to recover some new data.\nTransposing Eq. (44) and multiplying it with Eq. (43),\nwe obtain: B+ (t)T A+ (t) = B\u2212 (t)T A\u2212 (t). The left hand\nside of this equation is a lower triangular matrix, while\nthe matrix on the right hand side is upper triangular.\nHence,\nB\u00b1 (t)T A\u00b1 (t) = A\u00b1 (t)T B\u00b1 (t) = D(t),\n\n(53)\n\nwhere D(t) is a diagonal matrix. Again take into account\nEqs. (44) and (43) to write:\n\u0398(t)T \u0393(t) = \u0393(t)T \u0398(t) = D(t).\n\n(54)\n\nThe diagonal structure of D indicates that each adjoint\ncovariant vector \u03b8j (t), j = 1, 2, . . . , m is always orthogonal to the covariant vectors \u03b3 i (t), where i 6= j. In presence of the tangency \u03b3 j (t) = \u03b3 j+1 (t) the jth and the\n(j + 1)th diagonal elements of D vanish, i.e., corresponding adjoint and original vectors also become orthogonal:\n\u03b3 j+i (t) \u22a5 \u03b8 j+i (t), where i = 0, 1. It means that given\nthe vectors \u03b3 i (t), one can find the adjoint vectors \u03b8 j (t)\nas null vectors of the matrix consisting of all \u03b3 i (t) except\nthe jth one. Notice that even if a tangency occurs, one\nstill can compute \u03b8j (t) in this way. To find how D(t) is\nvarying in time, we transpose Eq. (48), multiply it with\nEq. (51), and take into account Eq. (6): \u0393(t1 )T \u0398(t1 ) =\nCF (t1 , t2 )\u0393(t2 )T \u0398(t2 )[CG (t1 , t2 )]\u22121 . Hence, D(t1 ) =\nCF (t1 , t2 )D(t2 )[CG (t1 , t2 )]\u22121 (recall that all matrices\nhere are diagonal). Altogether, the elements of the diagonal matrix D are cosines of angles between corresponding\ncovariant and adjoint covariant vectors. Since these angles are affected by tangencies, their time averages as well\nas their temporal fluctuations; i.e., the first and other\nmoments, can be considered as characteristic numbers\ndescribing the structure of an attractor. The angles are\nnorm-independent, because they are defined in terms of\ncovariant and adjoint covariant Lyapunov vectors which\nshare this property.\nIf the covariant vectors are computed with a non-ideal\naccuracy, the errors will grows in course of the tangent\ndynamics. The same is the case for the adjoint covariant\nvectors. In particular, it means that if we have found\nnumerically covariant vectors at t1 , we cannot compute\nthem at t > t1 via Eq. (48) because numerical errors\nresults in the divergence from the true directions. But\nnevertheless, Paz\u00f3 in Ref. [32] shows that this divergence\n\n\f12\nis actually sufficiently slow. Hence, Eq. (48) can be used\nto find an estimate for the covariant vectors at t > t1\nwhen t \u2212 t1 is not very large.\nThe covariant Lyapunov vectors are defined locally, according to Eqs. (43) and (44), and asymptotically, as\nfollows from Eqs. (39) and (40). These equations provide two basic ideas for computing these vectors. The\nfirst one is to find backward and forward Lyapunov vectors for some point of the trajectory and compute an\nintersection of corresponding Oseledec subspaces. The\nstraightforward implementation of this approach though\npossible, takes a lot of computational resources. We discuss it in Sec. IV A. In Secs. IV B and IV C more \"clever\"\nimplementations are considered.\nThe second approach is to try to arrive at asymptotic\nbehavior described by Eq. (39) or (40). If we initialize\na vector, satisfying Eq. (19) and start iterations backward in time, after a long time we closely approach the\nlimiting vectors that evolve as F (tn , tn+1 )\u22121 v j (tn+1 ) =\nv j (tn )cj (tn , tn+1 )\u22121 , where cj (tn , tn+1 ) are related to the\nlocal Lyapunov exponents (52). This equation is reversible, so that when the limit is reached, we can turn\nforward and arrive the opposite limit too. It means that\nthe limiting vectors v j found in this way satisfy Eq. (39)\nand coincide with \u03b3 j . The forward iterations defined\nby Eqs. (18) also converge to the covariant Lyapunov\nvectors. Similarly, the iterations initialized according to\nEqs. (24) and (25) converge to the adjoint covariant vectors. The straightforward numerical implementation of\nthis approach is impossible. Due to numerical noise, vectors v (j) cannot be initialized exactly as required, and the\nnumerical routines always converge to the single dominating vector. But a way to avoid this obstacle is known,\nand we consider it in Sec. IV D.\n\nIV. NUMERICAL METHODS FOR\nCOMPUTING COVARIANT LYAPUNOV\nVECTORS\nA.\n\nIntersection of Oseledec subspaces\n\nA straightforward way to find covariant Lyapunov vectors is based on Eq. (41). Given forward and backward\nLyapunov vectors, one can construct intersections of the\nOseledec subspaces and find the covariant vectors. To\ncompute the intersection of two subspaces one can compute so called principle angles between subspaces [20, 33].\nIn brief, this method is associated with computation of\nthe singular values and vectors of submatrices of the matrix (47).\nTo compute the jth covariant vector one needs the first\nj backward vectors and m \u2212 j + 1 last forward vectors.\nThe first backward vectors can be computed in the course\nof the iterations with the propagator F , and the last\nforward vectors are the result of the iterations with the\ninverted propagator F \u22121 , see Sec. II.\nRegardless of j, m+1 forward and backward Lyapunov\n\nvectors are always required. So, this method is applicable\nfor computation of the whole spectrum, but this is not an\nefficient approach if one needs only a few first covariant\nvectors. Because the forward Lyapunov vectors are computed in the reverse order, this method has a \"flattened\"\naccuracy along the spectrum: the backward vectors have\nhigher accuracy in first part of the spectrum, and the\nforward one are more accurate in its last part. So, the\nresulting covariant vectors have approximately the same\naccuracy for the whole spectrum.\n\nB.\n\nMethod of LU factorization\n\nIt is possible to avoid computation of the whole spectrum of the forward or backward Lyapunov vectors to\nget only a few first covariant vectors. Two original ideas,\nwhich were reported in Refs. [11, 12], are discussed in\nSecs. IV C and IV D. In the current section we present a\nnew approach to this problem.\nConsider Eq. (45). Matrices A+ and A\u2212 are lower and\nupper triangular, respectively. If A\u2212 is non-singular, we\ncan rewrite Eq. (45) as P = A+ (A\u2212 )\u22121 . This equation\ncan be considered as an LU factorization of P, i.e., representation of a matrix as a product of a lower and an\nupper triangular matrix [20]. If the factorization exists,\nit is unique up to the diagonal elements of one of the matrices (factors). For us it means that if we find the LU\ndecomposition of P, we find the covariant vectors up to\narbitrary lengths.\nThere are many well developed standard routines computing the LU factorization. But for us the serious disadvantage is that they work well only as long as the assumption of non-singularity of A\u2212 remains valid. If matrices A\u00b1 are singular, the straightforward factorization\nof P does not exist. The standard routines for LU decomposition avoid this obstacle performing preliminary\npermutations of rows and columns of P. This is not suitable for us, because the order of rows and columns in P\nis essential. Moreover, the standard routines find both\nA\u2212 , and A+ , while it is enough for us to have only A\u2212 .\nLet us return to Eq. (45). We shall demonstrate now\nthat the required elements of A\u2212 can be found from this\nequation regardless of a possible singularity of A\u00b1 . To\ncompute the jth covariant vector we need to find the top\nj elements of the jth column of A\u2212 . This fragment of\nthe column can be denoted as A\u2212 (1 : j, j). The remaining fragment A\u2212 (j + 1 : m, j) contains zeros. Note that\nhere we omit the time dependence and use parentheses to\nindicate submatrices. The matrix equation for nonzero\nelements reads: P(1 : j, 1 : j)A\u2212 (1 : j, j) = A+ (1 : j, j),\nwhere P(1 : j, 1 : j) is the top left square submatrix of P.\nBecause A+ is lower triangular, the fragment A+ (1 : j, j)\nof its jth column contains zeros except for the diagonal\nelement A+ (j, j). As already mentioned above, the LU\ndecomposition is unique up to diagonal elements of one of\nthe matrices. It means that we can eliminate the equation, corresponding to the jth row of P(1 : j, 1 : j) and\n\n\f13\nwrite the following homogeneous matrix equation\nP(1 : j \u2212 1, 1 : j)A\u2212 (1 : j, j) = 0.\n\n(55)\n\nThis equation allows to compute nonzero elements of the\njth column of A\u2212 as the null space of the rectangular\nsubmatrix P(1 : j \u2212 1, 1 : j). To obtain covariant unit vectors the solutions have to be normalized.\nEquation (55) can, in principle, have multiple solutions\nfor A\u2212 (1 : j, j). (In this case the rank of P(1 : j \u2212 1, 1 : j)\nis less than (j \u2212 1).) As we discussed above, this ambiguity can occur only due to the degeneracy of the Lyapunov\nexponents, and we can arbitrarily choose one of the multiple solutions.\nAs follows from Eq. (46), the adjoint covariant vectors\ncan be computed analogously, using the equation\n(PT )(1 : j \u2212 1, 1 : j)B+ (1 : j, j) = 0.\n\n(56)\n\nLet us now consider the submatrix P(1 : j, 1 : j). If this\nis singular, then Eq. (55) provides for the (j+1)th column\nthe solution A\u2212 (1 : m, j + 1) = A\u2212 (1 : m, j), i.e., the jth\nand (j + 1)th covariant vectors coincide. The inverse is\nalso true, and, hence, the singularity of the submatrix\nP(1 : j, 1 : j) is a sufficient and necessary condition for\nmerging of the jth and (j + 1)th covariant vectors.\nAs discussed above, the merging of covariant Lyapunov vectors indicates tangencies of invariant manifolds of an attractor that, in particular, occur when\nthe attractor is chaotic and non-hyperbolic [31]. To detect the violation of hyperbolicity, one usually studies a\ndistribution of angles between expanding and contracting subspaces spanned by corresponding covariant vectors [12, 14, 34, 35]. (Another method for a numerical\ntest of hyperbolicity, which does not employ covariant\nvectors, is based on the so called cone criterion [36].)\nAnalyzing properties of submatrices of P one can test\nfor hyperbolicity without explicit computation of covariant vectors. Let the number of positive Lyapunov exponents be k. Moving along a trajectory, we need to\ncompute some characteristic number whose small value\nindicates the nearness of P(1 : k, 1 : k) to singularity. It\ncan be, for instance, the determinant or the smallest singular value. A small characteristic number means that\nthe trajectory passes close to the tangency. So, if the distribution of characteristic numbers computed for many\ntrajectory points is well separated from the origin, then\nthe chaos is hyperbolic, and if it approaches the origin\nviolations of hyperbolicity occur.\nOne can also study the statistics of nearness to singularity of all submatrices P(1 : j, 1 : j), where j =\n1, 2, . . . , m \u2212 1. This can provide detailed information\nconcerning properties of various limit sets embedded in\nan attractor.\nAnother way to characterize an attractor is to compute the matrix D containing cosines of angles between\ncovariant and adjoint covariant vectors. As discussed\nabove, each merged couple of vectors, i.e., each tangency,\nis represented as a couple of zeros of the corresponding\n\nmatrix elements. To compute D, first we find the matrix A\u2212 , then using Eq. (45) compute only the diagonal elements of A+ , and after that compute B+ using\nEq. (56). (Though only its diagonal elements are required, we cannot get them without computing the rests\nof the columns.) Finally, we obtain the elements of D\nas products of diagonal elements of A+ and B+ ; see\nEq. (53). Note, that it is not required to compute the\nwhole matrix D. The method allows one to find only a\nfew first elements.\nNormally, one has to compute the covariant Lyapunov\nvectors for a series of subsequent points of a trajectory. A\npractical implementation of the algorithm in this case can\nbe the following. We start the procedure for Lyapunov\nexponents forward in time including the iterations with\nF (t1 , t2 ) and QR factorizations, and perform it as long as\nrequired for the orthogonal matrices Q(t) to converge to\nthe matrices of the backward Lyapunov vectors \u03a6\u2212 (t).\nDenote the end of the preliminary stage as tA . After\nthis point the iterations are continued, but now we store\ntrajectory points of the basic system and the backward\nvectors \u03a6\u2212 (tn ), see the diagram in Fig. 4(a). The duration of this stage depends on the number of points where\nwe need to know the covariant vectors. At tB we stop the\nstoring of \u03a6\u2212 (tn ) and, moreover, stop the procedure for\nLyapunov exponents and continue to solve only the basic\nsystem saving the trajectory points. This stage lasts from\ntB to tC . Its duration must be long enough for the subsequent backward procedure to converge. At tC we start\nmoving back along the saved trajectory performing the\nbackward procedure for Lyapunov exponents including\niterations with the adjoint propagator G \u22121 and QR factorizations. Upon the arrival at tB we have the forward\nLyapunov vectors \u03a6+ (t). Now we pass the interval from\ntB to tA given both the backward vectors \u03a6\u2212 (tn ), that\nwere saved in the course of the forward pass, and the\nforward vectors \u03a6+ (tn ). These vectors can be used to\ncompute the matrices A\u2212 (tn ) by means of P (Eq. (47)),\nas explained before. In turn, these matrices can be used\nto find the covariant vectors \u0393(tn ), according Eq. (43).\nNote that it is not necessary to perform this procedure\nwith the whole set of vectors. To compute j first covariant vectors we need j first backward vectors and j \u2212 1\nfirst forward vectors. In the appendix we provide a pseudocode implementation of the presented algorithm.\nColumns of A\u2212 (tn ) can also be considered as covariant Lyapunov vectors written with respect to the basis\n\u03a6\u2212 (tn ). The covariant vectors in the form of A\u2212 (tn )\nhave a mutual orientation that is identical to \u0393(tn ).\nTherefore, if for example the angles between covariant\nLyapunov vectors are required, they can be computed\nwith respect to columns of A\u2212 (tn ). This allows us to\nsave some machine time.\nThe numerical implementation of the described procedure includes well established numerical routines. To\nperform the forward procedure for Lyapunov exponents,\nbesides of numerically solving the dynamical equations,\none also needs to compute QR decompositions. For high-\n\n\f14\na)\nBenettin steps, store \u03a6\u2212 and\n\nSolve the basic system only,\n\ntrajectory points\n\nstore trajectory points\n\nBackward Benettin steps,\n\nBackward Benettin steps\n\ncompute CLVs\n\ntA\n\ntB\n\ntC\n\nb)\nBenettin steps, store RF\n\nBenettin steps, store\n\u03a6\u2212 and RF\n\nIterate with (RF )\u22121\n\nIterate with (RF )\u22121 ,\ncompute CLVs\n\ntA\n\ntB\n\ntC\n\nFigure 4. Computation of covariant Lyapunov vectors (CLVs). a) Method of LU factorization (see Sec. IV B), and orthogonal\ncomplement method of Wolfe and Samelson (see Sec. IV C). b) Iterative method of Ginelli et al. (see Sec. IV D).\n\ndimensional systems good results are obtained with algorithms based on Householder transformations [20, 28].\nThe backward steps may in addition require an interpolation of the stored trajectory to find a solution of variational equations with variable time steps. Finally, each\ncolumn of A\u2212 (tn ) is the null space of a corresponding\nrectangular submatrix of P. One of the most reliable\nmethods of computation of the null space is based on the\nSVD [20]. The null vector is identified as a right singular vector corresponding to the vanishing singular value.\nAbove we discussed that in principle in the case of degeneracy of Lyapunov exponents one can obtain more than\none null vector for one column A\u2212 (tn ). But exactly identical Lyapunov exponents are unlikely to occur in numerical computations, and, hence, multiple null vectors can\n(practically) never appear. It means that among right\nsingular vectors we always have a preferable candidate\nwith the smallest singular value.\nImplementations of QR decomposition and SVD in\nFortran can, for example, be found in the well-known\nLAPACK library [37]. For a C++ implementations we\nrefer to the ALGLIB NET library [38]. Also this library\nprovides implementations for many other platforms, such\nas Delphi and VBA.\n\nC.\n\nOrthogonal complement method of Wolfe and\nSamelson\n\nOne of two first methods for the efficient computation\nof covariant Lyapunov vectors was suggested by Wolfe\nand Samelson [11]. Just as the LU method, their approach utilizes the local property of the covariant vectors\ndetermined by Eq. (43). This equation can be written for\nthe jthe vector as\n\u03b3j =\n\u03b3j =\n\nj\nX\n\ni=1\nm\nX\n\n\u2212\n\u03c6\u2212\ni \u03b1ij ,\n\n(57)\n\n+\n\u03c6+\ni \u03b1ij .\n\n(58)\n\ni=j\n\nAs above, the time dependence is not explicitly shown.\nEquating Eqs. (58) and (57) and multiplying them by \u03c6+\nk\nwe can find\n\u03b1+\nkj =\n\nj\nX\n\n\u2212\n\u2212\nh\u03c6+\nk \u03c6n i\u03b1nj .\n\n(59)\n\nn=1\n\nNow we substitute this \u03b1+\nkj in Eq. (58) and multiply\nthe resulting equation by \u03c6\u2212\nk . Taking into account that\n\n\f15\n\u2212\nh\u03c6\u2212\nk \u03b3 j i = \u03b1kj , we obtain:\n\uf8f6\n\uf8eb\nj\nm\nX\nX\n\uf8ed\npik pin \uf8f8 \u03b1\u2212\n\u03b1\u2212\nnj , k \u2264 j.\nkj =\nn=1\n\nChanging the order of sums in Eq. (63), we can write\nit in matrix form as\n(60)\n\ni=j\n\n\u2212\nwhere pik = h\u03c6+\ni \u03c6k i are elements of the matrix P (47).\nIn principle, this equation allows one to compute \u03b1\u2212\nkj\nand to find the covariant vectors via Eq. (57). But this\nstraightforward approach is not efficient. To compute\nthe jth covariant vector, the coefficients \u03b1\u2212\nkj are required,\nwhere k = 1, 2, . . . , j. These coefficients depend on pik =\n\u2212\nh\u03c6+\ni \u03c6k i, where i = j, j + 1, . . . , m. So, we need m \u2212 j + 1\nlast vectors \u03c6+ , and j first vectors \u03c6\u2212 . The total number\nis always m + 1.\nThe key idea of Wolfe and Samelson to avoid this obstacle utilizes the orthogonality of P [11, 39]. One can\nobtain the needed subspace spanned by the last (m\u2212j+1)\nvectors by taking the orthogonal complement to the subspace of the first (j \u2212 1) vectors. In more\nP detail, columns\nof P are orthogonal to each other, i.e., m\ni=1 pik pin = \u03b4kn ,\nwhere \u03b4kn = 1 if k = n and 0 otherwise. This sum can\nbe split at i = j as follows:\nm\nX\ni=j\n\npik pin = \u03b4kn \u2212\n\nj\u22121\nX\n\n(61)\n\npik pin .\n\ni=1\n\nThe sum at the left hand side of this equation includes\nelements from the last rows of P, while the sum at the\nright hand side consists of the elements of the first rows.\nSo, the sum in parentheses in Eq. (60) can be substituted\nas:\n!\nj\u22121\nj\nX\nX\n\u2212\npik pin \u03b1\u2212\n\u03b4kn \u2212\n\u03b1kj =\nnj\nn=1\n\ni=1\n\n=\n\n\u03b1\u2212\nkj\n\n\u2212\n\nj\u22121\nj\nX\nX\n\nn=1\n\ni=1\n\npik pin\n\n!\n\n\u03b1\u2212\nnj . (62)\n\nThus, to compute j unknown coefficients \u03b1\u2212\nnj , where\nn \u2264 j, we have to solve a set of j linear homogeneous\nequations\n!\nj\u22121\nj\nX\nX\npik pin \u03b1\u2212\nnj = 0 (j = 1, 2, . . . , m, k \u2264 j).\nn=1\n\ni=1\n\n(63)\n(We remind the reader that \u03b1\u2212\n=\n0\nfor\nn\n>\nj.)\nEquanj\ntion (63) was suggested by Wolfe and Samelson to compute A\u2212 . It does not depend on the last rows of P, so\nthat one needs j first backward vectors and j \u2212 1 first\nforward vectors to compute j first covariant vectors.\nLater the method of Wolfe and Samelson was modified\nby Paz\u00f3 et al. [15] using the standard approach of computation of the forward and backward Lyapunov vectors,\nbased on QR factorizations and on the backward iterations with the transposed propagator (these ideas were\ndiscussed in Sec. II).\n\nP(1 : j \u2212 1, 1 : j)T P(1 : j \u2212 1, 1 : j)A\u2212 (1 : j, j) = 0. (64)\nCompare this equation with Eq. (55). We can see that\nsolutions of Eq. (55) constitute a subset of solutions of\nEq. (64). But because we need only one solution at each\nj, and because our LU method finds such solution, we\ncan conclude that the LU method works in the same way\nas the Wolfe and Samelson method, avoiding redundant\nmatrix multiplication.\n\nD.\n\nBackward iterations, method of Ginelli et al.\n\nAlmost simultaneously with Wolfe and Samelson,\nGinelli et al. [12] suggested a method based on asymptotic properties of covariant vectors (39). The underlying idea of this method was described in the end of\nSec. III, but it cannot be directly implemented. Assume\nthat we have backward Lyapunov vectors at t1 . Theoretically we can initialize v j (t1 ) satisfying Eq. (19), and\nstart the backward iterations using F \u22121 . But in practice,\ndue to numerical noise all these vectors shall belong to\n\u2212\n\u2212\nSm\n(t1 ) \\ Sm\u22121\n(t1 ), because this set has the largest measure. Hence, these iterations can provide only \u03b3 m . Due\nto the same reasons the forward iterations converge to\n\u03b3 1 . The same is also true for the adjoint propagator.\nThe key idea of Ginelli et al. is to perform the iterations in the space of projections onto backward Lyapunov\nvectors \u03a6\u2212 (t). For a set of vectors initialized according to\nEq. (19), the matrix of projections onto \u03a6\u2212 (t) is upper\ntriangular and the iterations converge in the backward\ntime. As follows from Eq. (50), the backward iterations\nwith F (t1 , t2 )\u22121 in the space of projections onto \u03a6\u2212 (t)\nare equivalent to backward iterations with the upper triangular matrix RF (t1 , t2 )\u22121 . This mapping preserves the\ntriangular structure of the matrix of projections, and we\ncan perform as many backward iterations as we need al\u2212\nways staying within subspaces Sj\u2212 (t1 )\\Sj\u22121\n(t1 ). In other\nwords, any upper triangular matrix iterated backward in\ntime with RF (t1 , t2 )\u22121 converges to A\u2212 (t). Note that\nsince the subspaces Sj\u2212 (t) are spanned by the first j backward Lyapunov vectors, we are allowed to compute only\nj first covariant vectors without computing the rest of\nthem.\nIn a similar way we can compute the first j adjoint covariant vectors, using the forward-time asymptotic (24).\nWe start the procedure moving backward in time with\nthe transposed propagator and computing forward Lyapunov vectors as described in Sec. II. The triangular matrices RG (t1 , t2 ) have to be stored. Then we turn round\nand start forward iterations RG (tn , tn+1 )\u22121 B(tn ) =\nB(tn+1 )[CG (tn , tn+1 )]\u22121 that converge to B+ (t).\nA practical implementation of the method of Ginelli\net al. might be the following; see the illustration in\nFig. 4(b). First, we perform the procedure for Lyapunov\n\n\f16\nexponents including forward iterations with F (t1 , t2 ) and\nQR factorizations. This stage is preliminary and it is\nfinished at tA when we decide that the orthogonal matrices Q(t) have converged to the matrices of backward\nLyapunov vectors \u03a6\u2212 (t). Starting from tA , we continue\nthe procedure, but now all the matrices \u03a6\u2212 (tn ) and\nRF (tn , tn+1 ), see Eq. (31), are stored. This stage continues until tB . The length of this stage depends on the\nnumber of points where we later want to compute the\ncovariant vectors. After tB we still proceed with the procedure, but store only RF (tn , tn+1 ). This stage must be\nsufficiently long to provide the convergence of the subsequent backward procedure and it finishes at tC . At this\npoint we initialize a set of arbitrary vectors, for which\nthe property (19) is fulfilled. In fact we just generate a\nrandom upper triangular matrix A. Using the stored matrices RF (tn , tn+1 ), we perform the backward iterations\non the interval from tC to tB .\nRF (tn , tn+1 )\u22121 A(tn+1 ) = A(tn )C(tn , tn+1 )\u22121 ,\n\n(65)\n\nwhere the diagonal matrix C(tn , tn+1 )\u22121 contains column\nnorms of A. If tC \u2212 tB is sufficiently large, A(tn ) converges to A\u2212 (tn ). Now we pass the stage from tB to tA\ncomputing the covariant Lyapunov vectors via Eq. (43)\nand using them as we need. Note, that this procedure\nallows one to compute not only the whole set of m covariant vectors, but also as many of them as we want.\nAs we already mentioned above, the columns of A\u2212 (tn )\ncan also be considered as covariant Lyapunov vectors, so\nthat in some cases it is enough to consider these vectors\nwithout computation of \u0393(tn ). In this case the matrices\n\u03a6\u2212 (tn ) do not have to be stored.\nThe algorithm of backward iterations can suffer from\nill-conditioned RF , which manifests itself if one computes\nmany (i.e., not just a few first) covariant Lyapunov vectors for a system with strong contraction. Typically,\nhigh-dimensional chaotic dissipative systems have several positive Lyapunov exponents of moderate magnitude\nwhile negative exponents can have large absolute values. Because logarithms of diagonal elements of RF are\nproportional to local Lyapunov exponents, they can be\nsufficiently small. So, if a lot of covariant vectors corresponding to negative Lyapunov exponents are computed,\nthe diagonal elements of RF can become small, and the\nwhole matrix RF , whose determinant is the product of\nits diagonal elements, can potentially be ill-conditioned.\nIn turn this can influence the accuracy of computations.\nTo avoid or at least minimize this problem one should\nfirst try to decrease the interval between QR orthogonalizations. Another, also almost obvious recommendation\nis not to employ Eq. (65) as it is, but compute iterations\nimplicitly. Note that the implicit method is preferable regardless of the presence of ill-conditioned RF . Namely,\nnonzero elements of the ith column of A(tn ) can be computed as a solution of equation\nRF (1 : i, 1 : i)An (1 : i, i) = An+1 (1 : i, i),\n\n(66)\n\nwhere RF (1 : i, 1 : i) is a top left submatrix of RF and\nAn (1 : i, i) top fragment of the ith column of A(tn ).\nComputed in this way An ( : , i) then has to be normalized. We see that the ith column of A(tn ) is influenced\nonly by the submatrix RF (1 : i, 1 : i) that remains wellconditioned until i is sufficiently small. It means that\neven if RF has some small diagonal elements, errors that\nthey can produce are not spread along the whole spectrum, but influence only minor covariant vectors from its\nright part.\nWhen the trajectory passes close to tangencies of invariant manifolds of an attractor, A(tn ) becomes illconditioned, i.e., small values can appear on its diagonal.\nBecause A(tn ) is used to compute A(tn\u22121 ), small values\ncan accumulate and vanish due to the numerical underflow. Then the zeros will be preserved in the course of\niterations even if the trajectory goes far from the tangency points. This false indication of an exact tangency\ncan be cured by adding a small amount of noise to the\ndiagonal elements.\nE.\n\nComparison of the methods\n\nComputation of covariant vectors requires saving of intermediate matrices. We estimate the amount of the required memory for the \"worst\" case when the whole set\nof m covariant vectors is computed. Let KAB be the\nnumber of trajectory points where we are going to compute covariant vectors, i.e., the number of steps in the\nstage AB in Fig. 4. It is reasonable to assume that this\nvalue depends on m, KAB = KAB (m), where m is the dimension of the phase space. Denote the number of steps\nin the transient stage BC by KBC . The convergence of\ncolumns of matrices to their asymptotic form during the\ntransient stage is exponential with rates equal to differences between corresponding Lyapunov exponents [11].\nFor extensive chaotic systems these differences are proportional to 1/m; thus, the convergence time is proportional to m. Altogether, the length of the transient stage\ncan be estimated as KBC = kBC m, where kBC is an empirical constant, which depends on the particular system\nunder consideration.\nFor the LU method, Sec. IV B, and for the method\nof orthogonal complement, Sec. IV C, the estimates are\nidentical. Namely, we need KAB matrices \u03a6\u2212 , each of the\nsize m2 , and KAB + KBC trajectory vectors of the size\nm, see Fig. 4(a). Hence, the total amount of memory\n(in bytes) is BLU = (m2 (KAB (m) + kBC ) + mKAB (m))b,\nwhere b is the number of bytes required to store one real\nnumber. For large m we have\nBLU \u2248 m2 KAB (m)b.\n\n(67)\n\nFor example if the dimension is m = 100 and we want to\ncompute KAB = 1000 covariant vectors using double precision numbers, i.e., b = 8, we need BLU \u2248 76 megabytes.\nFor the method of backward iterations, Sec. IV D, we\nneed to save KAB + KBC triangular matrices RF , each\n\n\f17\nof the size (m2 + m)/2, and KAB matrices \u03a6\u2212 of the\nsize m2 , see Fig. 4(b). The total amount of memory\ncan be estimated as BBI = (m2 (3KAB (m) + kBC m) +\nm(KAB (m)+kBC m))b/2. Keeping only the leading terms\nfor large m we obtain:\n\nBBI \u2248 m2 (3KAB (m) + kBC m)b/2.\n\n(68)\n\nFor the same numerical values as in the example for LU\nmethod and at kBC = 1 we obtain, though higher, but\nclose estimate: BBI \u2248 118 megabytes. Note however,\nthat the amount of memory for the transient stage grows\nwith m as kBC m3 b/2 for the backward iterations method,\nwhile for two other methods it grows as kBC m2 b. Hence,\nthe efficient application of the backward iterations requires closer attention to the minimization of the transient stage length, otherwise, one can easily exhaust the\navailable memory.\nIn principle, all methods may suffer from a shortage\nof memory. One possible way to handle this problem\nis to save intermediate data to binary files. The disadvantage of this approach is deceleration of computations\ndue to the slowness of file operations. Alternatively, see\nRef. [12], instead of keeping all necessary matrices moving\nforward in time, one can periodically (and sufficiently seldom to fit in the available memory) save snapshots of the\nprocedure for Lyapunov exponents (i.e., the trajectory\npoints of the basic system together with corresponding\nmatrices \u03a6\u2212 ). Then, moving backward, one periodically\nuses these snapshots to recompute forward steps and obtain missing data. Of course, this approach also slows\ndown the computations, now due to the recomputations.\nTo choose the preferable way one has to compare the average time for writing to file and subsequent reading of\none matrix with the time needed to recompute it. The\nresult of comparison depends on the particular computer\nsystem. Note also that using the method of backward iterations one can reduce the memory consumption if only\nthe angles between covariant vectors are needed. As we\nalready mentioned in Sec. IV B, the triangle matrices A\u2212\nare suitable for finding the angles, and hence, in this case\none does not need to save matrices \u03a6\u2212 .\nLet us estimate the computation speed of the methods presented (the straightforward intersection of the\nOseledec subspaces is not taken into account). If all\nthe methods have enough memory to avoid either using\nfiles or performing recomputing, the backward iterations\nare the fastest. Local methods of LU factorization and\northogonal complement loose the race on the backward\nstage B-A, see Fig. 4. Each iteration is simultaneously\na time step and also a computation of the covariant vectors. The time steps for local methods are performed\nvia the procedure for Lyapunov exponents and also some\ntime is required to compute the covariant vectors.\n\nV.\nA.\n\nEXAMPLES\n\nSystem with constant Jacobian matrix\n\nConsider a system with a constant Jacobian matrix\n\uf8eb\n\uf8f6\n1 \u22122 0\nJ = \uf8ed0 \u22121 0 \uf8f8 .\n(69)\n0 2 \u22123\n\nSince J is time-independent and has real eigenvalues, the\nLyapunov exponents for this system simply coincide with\nthe magnitude of its eigenvalues, \u03bb1,2,3 = 1, \u22121, \u22123. The\ncorresponding eigenvectors are simultaneously the covariant Lyapunov vectors, and the eigenvectors of (\u2212JT ) are\nthe adjoint covariant vectors:\n\uf8eb p\n\uf8f6\n\uf8f6\n\uf8ebp\n1 p1/3 0\np1/2 0 p0\n\u0393 = \uf8ed0 p1/3 0\uf8f8 , \u0398 = \uf8ed\u2212 1/2 1 \u2212p 1/2\uf8f8 . (70)\n1/3 1\n1/2\n0\n0\n0\np\np\np\nD = \u0398T \u0393 = diag[ 1/2, 1/3, 1/2]. The propagator\nreads:\n\uf8eb \u03c4 \u2212\u03c4\n\uf8f6\ne e (1 \u2212 e2\u03c4 )\n0\ne\u2212\u03c4\n0 \uf8f8 , (71)\nF (t1 , t2 ) = \u0393L\u0393\u22121 = \uf8ed 0\n\u22123\u03c4 2\u03c4\n0 e\n(e \u2212 1) e\u22123\u03c4\n\nwhere \u03c4 = t2 \u2212 t1 , and L = diag[e\u03bb1 \u03c4 , e\u03bb2 \u03c4 , e\u03bb3 \u03c4 ]. Forward and backward Lyapunov vectors can be computed\nas eigenvectors of far-future and far-past operators, respectively, directly from Eqs. (13) and (14) (finding the\nlimits one has to keep constant norms of vectors):\np\n\uf8eb\n\uf8f6\n\uf8f6\n\uf8ebp\n1 p0\n0\n1/2 p1/2 0\np\np\n\u03a6\u2212 = \uf8ed0 p1/2 \u2212p 1/2\uf8f8 , \u03a6+ = \uf8ed\u2212 1/2\n1/2 0\uf8f8 .\n1/2\n1/2\n0\n0\n0\n1\n(72)\nNote, that in accordance with Eq. (43), the first backward vector {1, 0, 0} and the last forward Lyapunov vector {0, 0, 1}, coincide with the first and the last covariant\nvectors, i.e., with eigenvectors of J. One can also check\nthat the logarithms of eigenvalues of the limit operators,\ni.e., the Lyapunov exponents, indeed coincides with the\nmagnitude of the eigenvalues of J. The matrix P, as\ndefined by Eq. (47), reads:\n\uf8f6\n\uf8ebp\np1/2 \u22121/2 1/2\nP = \uf8ed 1/2 p\n(73)\n1/2 p\n\u22121/2 \uf8f8 .\n1/2\n1/2\n0\n\nTo compute covariant vectors via the LU method, we\nhave to find the matrix A\u2212 . As follows from Eq. (55), the\nfirst column of this matrix is always\n{1, 0, 0} while for\n\u221a\n\u221a the\n\u2212\n\u2212\nother elements we have a\u2212\n2\n\u2212\na\n2\u2212\n/2\n=\n0,\na\n/\n12 /\n22\n13\n\u221a\n\u2212\n\u2212\n\u2212\n\u2212\na\u2212\n/2\n+\na\n/2\n=\n0,\nand\na\n/\n2\n+\na\n/2\n\u2212\na\n/2\n=\n0.\nFor\n23\n33\n13\n23\n33\nthe matrix B+ , needed to compute the adjoint covariant\nvectors, we construct\nequations\naccording\n\u221a\n\u221a to Eq.\u221a(56) us\u221a\ning PT : b12 / 2 + b22 / 2 = 0, b13 / 2 + b23 / 2 = 0,\n\n\f18\n\u221a\n\u2212b13 /2 + b23 /2 + b33 / 2 = 0. Both of these equation\nsets have to be solved with the additional requirement of\nunit column norms:\np\n\uf8eb p\n\uf8f6\n\uf8f6\n\uf8eb\n1 p1/3 p0\n1 \u2212p 1/2 1/2\nA\u2212 = \uf8ed0\n2/3 p1/2\uf8f8 , B+ = \uf8ed0\n1/2 p\n\u22121/2 \uf8f8 .\n1/2\n1/2\n0\n0\n0\n0\n(74)\nOne can check that Eqs. (43) and (44) are fulfilled, i.e.,\n\u0393 = \u03a6\u2212 A\u2212 and \u0398 = \u03a6+ B+ .\nThe method of Wolfe and Samelson does essentially the\nsame job. Computing A\u2212 we have to multiply submatrices of P by the transposed submatrices and construct\nequations; see Eq. (64). Similarly one can get B+ and\nverify that the results coincide with Eq. (74).\nFor the method of Ginelli et al. we find RF (t1 , t2 ) =\n[\u03a6\u2212 ]T F (t1 , t2 )\u03a6\u2212 ; see Eq. (31). Since the iterations (65)\nconverge in backward time, consider RF (t1 , t2 )\u22121 :\n\uf8eb\n\u221a\n\u221a \uf8f6\ne\u2212\u03c4 (e\u03c4 \u2212 e\u2212\u03c4 )/ 2 (e\u2212\u03c4 \u2212 e\u03c4 )/ 2\n\uf8f8.\nRF (t1 , t2 )\u22121 = \uf8ed 0\ne\u03c4\ne3\u03c4 \u2212 e\u03c4\n0\n0\ne3\u03c4\n(75)\nAs follows from Eq. (65), at \u03c4 \u2192 \u221e the column norms\nof RF (t1 , t2 )\u22121 have to grow as e\u2212\u03bbi \u03c4 . Indeed, it can\nbe checked that the column norms of this matrix are\nasymptotically dominated by the terms e\u2212\u03c4 , e\u03c4 , and e3\u03c4 ,\nrespectively. If we normalize columns to the unit, the\nelements of this matrix converge to A\u2212 , see Eq. (74),\ni.e., we again obtain the covariant vectors.\n\nB.\n\nGeneralized H\u00e9non map\n\nAs second example we consider a generalized threedimensional H\u00e9non map [40]\nxn+1\n= a \u2212 [xn2 ]2 \u2212 bxn3\n1\n\nxn+1\n= xn1\n2\n\n(76)\n\nxn+1\n= xn2 .\n3\nFor a = 1.76 and b = 0.1 this system generates a hyperchaotic attractor with Lyapunov exponents \u03bb1 = 0.225,\n\u03bb2 = 0.188, and \u03bb3 = \u22122.716.\nFigure 5 shows the chaotic attractor, where the\ncolor of the points corresponds to det[P(1 : 2, 1 : 2)] (see\nSec. IV B). Dark (red) colors indicate locations of the attractor where (almost) tangent CLVs occur and the submatrix P(1 : j, 1 : j) with j = 2 is (almost) singular.\n\nVI.\n\nCONCLUSION\n\nWe presented an extensive description of modern\nachievements of Lyapunov analysis. The Lyapunov exponents, the forward and backward Lyapunov vectors as\n\nwell as covariant Lyapunov vectors were discussed in detail.\nThe systematic approach allowed us to reveal a symmetry in the structure of the tangent space and to introduce the concept of adjoint covariant vectors. There\nare tangent linear propagators that can be characterized\nby left and right singular vectors. When the propagators\nare considered on asymptotically growing time intervals\nthese singular vectors converge to backward and forward\nLyapunov vectors. One can also define adjoint propagators that are associated with the same singular vectors,\nbut have reciprocal singular values. The backward and\nforward Lyapunov vectors can be used as frameworks for\ntwo sets of Oseledec subspaces and for two adjoint Oseledec subspaces that are orthogonal to the Oseledec subspaces. The main feature of these subspaces is the covariance with the tangent dynamics: the propagator maps\neach Oseledec subspace onto the corresponding Oseledec\nsubspace associated with the image point of the trajectory, and the adjoint propagator does the same with the\nadjoint subspaces. Within these subspaces one can find\nvectors with the same property of covariance. There are\ncovariant Lyapunov vectors whose exponential growth\nunder the action of the propagators is characterized by\nLyapunov exponents, and there are also adjoint covariant\nLyapunov vectors that grow under the action of adjoint\npropagators with Lyapunov exponents of opposite signs.\nThe adjoint covariant vectors are not independent\ncharacteristic vectors, because in principle one can always compute them using the original covariant Lyapunov vectors. Important are the norm-independent angles between corresponding covariant and adjoint vectors.\nThey provide a compact representation of the information provided by covariant vectors. In particular, homoclinic tangencies between stable and unstable manifolds\n(characteristic for non-hyperbolic chaos) are indicated by\northogonality of corresponding original and adjoint vectors.\nAn important result of our detailed analysis is an efficient method for computing covariant Lyapunov vectors.\nThe basic idea of the method is an optimized LU decomposition of the matrix P consisting of scalar products of\nforward and backward Lyapunov vectors. Our approach\nis very close to the method by Wolfe and Samelson [11],\nbut its advantages are a more transparent explanation,\nand the explicit formulation of the matrix P which is\ninteresting by itself. Moreover our approach is slightly\nmore efficient because we avoid some redundant computations.\nUsing the matrix P, we present a method for detecting\nnon-hyperbolicity of chaotic dynamics without explicit\ncomputation of the covariant vectors. In brief, the violation indicator is the singularity of a j \u00d7 j submatrix of P,\nwhere j is the number of positive Lyapunov exponents.\nThe chaotic dynamics is non-hyperbolic if moving along\na trajectory we encounter nearly singular submatrices.\nIn presence of degenerate Lyapunov exponents all\ntypes of Lyapunov vector are not unique. We provide an\n\n\f19\n\n2\n\nx3\n\n1\n\n0\n\n\u22121\n\n\u22122\n2\n\n2\n1\n\n0\n\n0\n\n\u22121\n\nx2\n\n\u22122\n\n\u22122\n\nx1\n\nFigure 5. Attractor of the generalized H\u00e9non map Eq. (76). Dark (red) colors indicate closeness to homoclinic tangencies.\n(Color figure online)\n\nanalysis of this case. As for the forward and backward\nLyapunov vectors, the standard algorithms can be used\nwithout modifications. Selection of an orthogonal initial\nmatrix eliminates the ambiguity. Starting from different\nseed matrices, we can obtain different sets of vectors, but\nany one of them is appropriate. Moreover, in practical\ncomputations the degeneracy of the Lyapunov exponents\nmanifests itself very weakly, especially for systems of high\ndimension. Typically, due to numerical errors all computed exponents are distinct, and one cannot identify\ndegenerate exponents just by examining the computed\nspectrum. The same is true for the covariant vectors.\nTheoretically the degeneracy of the Lyapunov exponents\ncan result in multiple sets of covariant vectors, but in\npractice the computations can be organized in a such\nway that one always obtains a unique appropriate solution regardless of the degeneracy.\n\nACKNOWLEDGMENTS\n\nThe research leading to the results has received funding from the European Community's Seventh Framework\nProgramme FP7/2007\u20132013 under grant agreement No.\nHEALTH-F2-2009-241526, EUTrigTreat. P. V. K. acknowledges support from RFBR-DFG under Grant No.\n08-02-91963.\n\nAppendix: Pseudocode for the LU method\n\nInput: nclv, number of computed covariant Lyapunov vectors; nstore, number of trajectory points\nwhere the covariant vectors are computed; m, dimension\nof the tangent space; dt, time interval between orthogonalizations (normally, a multiple of time discretization\nstep); nspend_att, nspend_fwd, nspend_bkw, steps to\nconverge to the attractor, forward and backward vectors,\nrespectively.\nSubroutines: solve_bas(), solving of the basic system; solve_lin_fwd(),\nsolve_lin_trp(), action of forward and transposed\npropagators, respectively (see Sec. II); null_vect(),\ncomputing a null vector (in the case of multiple\nsolutions, an arbitrary null-vector can be taken);\northog(), QR-orthogonalization (matrix R is abandoned); transpose(), transpose of a matrix; random(),\ngenerate random matrix or vector; A.B, multiplication of\nmatrices A and B.\nResult: Gamma, array of nstore matrices m by nclv,\nwhose columns are the covariant Lyapunov vectors.\nBEGIN clv_lu\n// *** ARRIVE AT THE ATTRACTOR ***\nCREATE u[1:m]=random(1,m)\nu=solve_bas(u,dt*nspend_att)\n// *** PRELIMINARY STAGE ***\nCREATE Q[1:m][1:nclv]=random(1,m,1,nclv)\nQ=orthog(Q)\nFOR i=1 TO nspend_fwd\nQ=solve_lin_fwd(Q,u,dt)\n\n\f20\nQ=orthog(Q)\nu=solve_bas(u,dt)\nNEXT i\n// *** STAGE A-B ***\nCREATE PhiMns[1:nstore][1:m][1:nclv]\nCREATE traj[1:nstore+nspend_bkw][1:m]\nFOR i=1 TO nstore\nQ=solve_lin_fwd(Q,u,dt)\nQ=orthog(Q)\nu=solve_bas(u,dt)\ntraj[i]=u\nPhiMns[i]=Q\nNEXT i\n// *** STAGE B-C ***\nFOR i=1 TO nspend_bkw\nu=solve_bas(u,dt)\ntraj[nstore+i]=u\nNEXT i\n// *** STAGE C-B ***\n// Now we use one column less\nRECREATE Q[1:m][1:nclv-1]=random(1,m,1,nclv-1)\nQ=orthog(Q)\n// We leave this cycle at\n// the (nstore+1)th trajectory point!\nFOR i=nspend_bkw TO 2 STEP -1\n\n[1] J.-P. Eckmann and D. Ruelle, \"Ergodic theory of chaos\nand strange attractors,\" Rev. Mod. Phys. 57, 617\u2013656\n(1985).\n[2] Z. Toth and E. Kalnay, \"Ensemble forecasting at NMC:\nThe generation of perturbations,\" Bull. Amer. Met. Soc.\n74, 2317\u20132330 (1993).\n[3] Z. Toth and E. Kalnay, \"Ensemble forecasting at NCEP\nand the breeding method,\" Monthly Weather Review\n125, 3297\u20133319 (1997).\n[4] R. Buizza, J. Tribbia, F. Molteni, and T. Palmer, \"Computation of optimal unstable structures for a numerical\nweather prediction model,\" Tellus A 45, 388\u2013407 (1993).\n[5] R. Buizza and T. Palmer, \"The singular-vector structure\nof the atmospheric global circulation,\" Journal of the Atmospheric Sciences 52, 1434\u20131456 (1995).\n[6] J. S. Frederiksen, \"Adjoint sensitivity and finite-time normal mode disturbances during blocking,\" Journal of the\nAtmospheric Sciences 54, 1144\u20131165 (1997).\n[7] B. Legras and R. Vautard, \"A guide to Lyapunov vectors,\" in Predictability Seminar Proc., ECWF Seminar, Vol. 1, edited by T. Palmer (European Centre\nfor Medium-Range Weather Forecasts, Reading, United\nKingdom, 1996) pp. 135\u2013146.\n[8] S. V. Ershov and A. B. Potapov, \"On the concept of stationary Lyapunov basis,\" Physica D 118, 167\u2013198 (1998).\n[9] J. A. Vastano and R. D. Moser, \"Shorttime\nLyapunov\nexponent\nanalysis\nand\nthe\ntransition to chaos in Taylor\u2013Couette flow,\"\nJournal of Fluid Mechanics 233, 83\u2013118 (1991).\n[10] A. Trevisan and F. Pancotti, \"Periodic orbits, Lyapunov\nvectors, and singular vectors in the Lorenz system,\" Journal of the Atmospheric Sciences 55, 390\u2013398 (1998).\n\nu=traj[nstore+i]\nQ=solve_lin_trp(Q,u,dt)\nQ=orthog(Q)\nNEXT i\n// *** STAGE B-A ***\nCREATE P[1:nclv-1][1:nclv]\nCREATE Gamma[1:nstore][1:m][1:nclv]\nCREATE a[1:nclv]\n// We come into this cycle being at\n// the (nstore+1)th point\n// and take traj[i+1], but not traj[i].\nFOR i=nstore TO 1 STEP -1\nu=traj[i+1]\nQ=solve_lin_trp(Q,u,dt)\nQ=orthog(Q)\nP=transpose(Q).PhiMns[i]\nGamma[i][1:m][1]=PhiMns[i][1:m][1]\nFOR j=2 TO nclv\na[1:j]=null_vect(P[1:j-1][1:j])\nGamma[i][1:m][j]=PhiMns[i][1:m][1:j].a[1:j]\nNEXT j\nNEXT i\nEND\n\n[11] C. L. Wolfe and R. M. Samelson, \"An efficient method\nfor recovering Lyapunov vectors from singular vectors,\"\nTellus A 59A, 355\u2013366 (2007).\n[12] F. Ginelli, P. Poggi, A. Turchi, H. Chat\u00e9, R. Livi, and\nA. Politi, \"Characterizing dynamics with covariant Lyapunov vectors,\" Phys. Rev. Lett. 99, 130601 (2007).\n[13] I. G. Szendro, D. Paz\u00f3, M. A. Rodr\u00edguez,\nand\nJ. M. L\u00f3pez, \"Spatiotemporal structure of Lyapunov vectors in chaotic coupled-map lattices,\"\nPhys. Rev. E 76, 025202 (2007).\n[14] P. V. Kuptsov and S. P. Kuznetsov, \"Violation of hyperbolicity in a diffusive medium with local hyperbolic\nattractor,\" Physical Review E 80, 016205 (2009).\n[15] D. Paz\u00f3, I. G. Szendro, J. M. L\u00f3pez, and M. A. Rodr\u00edguez, \"Structure of characteristic Lyapunov vectors in\nspatiotemporal chaos,\" Phys. Rev. E 78, 016209 (2008).\n[16] H.-L. Yang, K. A. Takeuchi, F. Ginelli, H. Chat\u00e9, and\nG. Radons, \"Hyperbolicity and the effective dimension of\nspatially-extended dissipative systems,\" Phys. Rev. Lett.\n102, 074102 (2009).\n[17] P. V. Kuptsov and U. Parlitz, \"Strict and fussy mode\nsplitting in the tangent space of the Ginzburg-Landau\nequation,\" Phys. Rev. E 81, 036214 (2010).\n[18] H.-L. Yang and G. Radons, \"Comparison between covariant and orthogonal Lyapunov vectors,\"\nPhys. Rev. E 82, 046204 (2010).\n[19] D. Paz\u00f3 and J. M. L\u00f3pez, \"Characteristic Lyapunov vectors in chaotic time-delayed systems,\"\nPhys. Rev. E 82, 056201 (2010).\n[20] G. H. Golub and C. F. van Loan, Matrix computations,\n3rd ed. (The Johns Hopkins University Press, Baltimore,\nMD, 1996).\n\n\f21\n[21] V. I. Oseledec, \"A multiplicative ergodic theorem. Lyapunov characteristic numbers for dynamical systems,\"\nTrudy Mosk. Mat. Obsc. 19, 197\u2013231 (1968), [Moscow\nMath. Soc. 19, 197-231 (1968)].\n[22] C. A. Reynolds and R. M. Errico, \"Convergence of singular vectors toward Lyapunov vectors,\" Monthly Weather\nReview 127, 2309\u20132323 (1999).\n[23] G. Benettin, L. Galgani, A. Giorgilli, and J. M. Strelcyn,\n\"Lyapunov characteristic exponents for smooth dynamical systems and for Hamiltonian systems: A method for\ncomputing all of them. Part I: Theory. Part II: Numerical\napplication,\" Meccanica 15, 9\u201330 (1980).\n[24] I. Shimada and T. Nagashima, \"A numerical approach to\nergodic problem of dissipative dynamical systems,\" Prog.\nTheor. Phys. 61, 1605\u20131616 (1979).\n[25] T. S. Parker and L. O. Chua, Practical numerical algorithms for chaotic systems (Springer-Verlag, 1989) p.\n348.\n[26] G. Benettin, L. Galgani, A. Giorgilli, and J. M. Strelcyn, \"All Lyapunov characteristic numbers are effectively\ncomputable,\" C. R. Acad. Sci. Paris, S\u00e9r. A 286, 431\u2013433\n(1978).\n[27] P. V. Kuptsov, \"Vychislenie pokazateley Lyapunova dlya\nraspredelennyh sistem: preimuschestva i nedostatki razlichnyh chislennyh metodov,\" Izv. vuzov Prikladnaya\nNelineynaya Dinamika 5 (2010), [Computation of Lyapunov exponents for spatially extended systems: advantages and limitations of various numerical methods,\nAppl. Nonlin. Dynam, 5 (2010), (in Russian)].\n[28] K. Geist, U. Parlitz, and W. Lauterborn, \"Comparision of different methods for computing Lyapunov exponents,\" Prog. Theor. Phys. 83, 875\u2013893 (1990).\n[29] W. Magnus, \"On the exponential solution of\ndifferential equations for\na linear operator,\"\nComm. Pure and Appl Math 7, 649\u2013673 (1954).\n\n[30] D. Ruelle, \"Ergodic theory of differentiable dynamical\nsystems,\" Publ. Math. de L'IH\u00c9S 50, 27\u201358 (1979).\n[31] J. Guckenheimer and P. Holmes, Nonlinear oscillations, dynamical dystems and bifurcations of vector fields\n(Springer-Verlag, 1983).\n[32] D. Paz\u00f3, M. A. Rodr\u00edguez, and J. M. L\u00f3pez, \"Spatiotemporal evolution of perturbations in ensembles initialized by bred, lyapunov and singular vectors,\" Tellus 62A,\n10\u201323 (2010).\n[33] A. V. Knyazev and M. E. Argentati, \"Principal angles\nbetween subspaces in A-based scalar product: algorithms\nand perturbation estimates,\" SIAM H. Sci. Comput. 23,\n2008\u20132040 (2002).\n[34] Y.-C. Lai, C. Grebogi, J. A. Yorke, and I. Kan, \"How\noften are chaotic saddles nonhyperbolic?\" Nonlinearity 6,\n779 (1993).\n[35] V. S. Anishchenko, A. S. Kopeikin, J. Kurths,\nT. E. Vadivasova,\nand G. I. Strelkova,\n\"Studying\nhyperbolicity\nin\nchaotic\nsystems,\"\nPhysics Letters A 270, 301 \u2013 307 (2000).\n[36] S. P. Kuznetsov and I. R. Sataev, \"Hyperbolic attractor\nin a system of coupled non-autonomous van der Pol oscillators: Numerical test for expanding and contracting\ncones,\" Phys. Lett. A 365, 97\u2013104 (2007).\n[37] E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel,\nJ. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling,\nA. McKenney, and D. Sorensen, \"LAPACK users' guide,\"\n(1999).\n[38] S. Bochkanov and V. Bystritsky, \"ALGLIB NET,\" Electronic resource, http://www.alglib.net (1999).\n[39] R. M. Samelson and C. L. Wolfe, \"Lyapunov\nvectors\nfor\nlarge\nsystems,\"\nin\nExploring Complex Dynamics in High-Dimensional Chaotic Systems:\n(MPIPKS Dresden, Germany, 2010).\n[40] G. Baier and M. Klein, \"Maximum hyperchaos in generalized H\u00e9non map,\" Phys. Lett. A 151, 281\u2013284 (1990).\n\n\f"}