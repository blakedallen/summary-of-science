{"id": "http://arxiv.org/abs/1011.4059v1", "guidislink": true, "updated": "2010-11-17T20:58:25Z", "updated_parsed": [2010, 11, 17, 20, 58, 25, 2, 321, 0], "published": "2010-11-17T20:58:25Z", "published_parsed": [2010, 11, 17, 20, 58, 25, 2, 321, 0], "title": "Image Coaddition with Temporally Varying Kernels", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.3049%2C1011.4253%2C1011.5860%2C1011.6513%2C1011.1134%2C1011.3989%2C1011.6612%2C1011.4958%2C1011.2078%2C1011.1856%2C1011.0992%2C1011.2810%2C1011.0808%2C1011.6135%2C1011.0798%2C1011.1112%2C1011.0543%2C1011.2329%2C1011.1921%2C1011.1093%2C1011.4795%2C1011.4900%2C1011.6243%2C1011.5383%2C1011.5125%2C1011.1788%2C1011.5400%2C1011.5884%2C1011.5910%2C1011.2629%2C1011.3419%2C1011.1851%2C1011.1457%2C1011.1251%2C1011.4059%2C1011.3978%2C1011.4088%2C1011.5862%2C1011.0061%2C1011.1175%2C1011.1714%2C1011.1375%2C1011.1055%2C1011.3414%2C1011.5504%2C1011.2494%2C1011.3335%2C1011.2903%2C1011.0290%2C1011.5906%2C1011.2821%2C1011.3881%2C1011.4558%2C1011.6493%2C1011.3361%2C1011.3211%2C1011.3691%2C1011.0791%2C1011.3369%2C1011.4221%2C1011.3252%2C1011.1448%2C1011.2505%2C1011.1414%2C1011.4486%2C1011.4299%2C1011.2837%2C1011.4148%2C1011.0952%2C1011.1682%2C1011.1956%2C1011.6008%2C1011.5604%2C1011.5142%2C1011.5730%2C1011.1688%2C1011.0599%2C1011.1014%2C1011.0983%2C1011.2239%2C1011.3804%2C1011.1805%2C1011.2324%2C1011.6623%2C1011.0345%2C1011.2841%2C1011.5118%2C1011.2438%2C1011.0311%2C1011.4283%2C1011.6546%2C1011.1890%2C1011.1923%2C1011.2339%2C1011.3112%2C1011.3290%2C1011.2244%2C1011.4675%2C1011.2229%2C1011.1768%2C1011.6550&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Image Coaddition with Temporally Varying Kernels"}, "summary": "Large, multi-frequency imaging surveys, such as the Large Synaptic Survey\nTelescope (LSST), need to do near-real time analysis of very large datasets.\nThis raises a host of statistical and computational problems where standard\nmethods do not work. In this paper, we study a proposed method for combining\nstacks of images into a single summary image, sometimes referred to as a\ntemplate. This task is commonly referred to as image coaddition. In part, we\nfocus on a method proposed in previous work, which outlines a procedure for\ncombining stacks of images in an online fashion in the Fourier domain. We\nevaluate this method by comparing it to two straightforward methods through the\nuse of various criteria and simulations. Note that the goal is not to propose\nthese comparison methods for use in their own right, but to ensure that\nadditional complexity also provides substantially improved performance.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.3049%2C1011.4253%2C1011.5860%2C1011.6513%2C1011.1134%2C1011.3989%2C1011.6612%2C1011.4958%2C1011.2078%2C1011.1856%2C1011.0992%2C1011.2810%2C1011.0808%2C1011.6135%2C1011.0798%2C1011.1112%2C1011.0543%2C1011.2329%2C1011.1921%2C1011.1093%2C1011.4795%2C1011.4900%2C1011.6243%2C1011.5383%2C1011.5125%2C1011.1788%2C1011.5400%2C1011.5884%2C1011.5910%2C1011.2629%2C1011.3419%2C1011.1851%2C1011.1457%2C1011.1251%2C1011.4059%2C1011.3978%2C1011.4088%2C1011.5862%2C1011.0061%2C1011.1175%2C1011.1714%2C1011.1375%2C1011.1055%2C1011.3414%2C1011.5504%2C1011.2494%2C1011.3335%2C1011.2903%2C1011.0290%2C1011.5906%2C1011.2821%2C1011.3881%2C1011.4558%2C1011.6493%2C1011.3361%2C1011.3211%2C1011.3691%2C1011.0791%2C1011.3369%2C1011.4221%2C1011.3252%2C1011.1448%2C1011.2505%2C1011.1414%2C1011.4486%2C1011.4299%2C1011.2837%2C1011.4148%2C1011.0952%2C1011.1682%2C1011.1956%2C1011.6008%2C1011.5604%2C1011.5142%2C1011.5730%2C1011.1688%2C1011.0599%2C1011.1014%2C1011.0983%2C1011.2239%2C1011.3804%2C1011.1805%2C1011.2324%2C1011.6623%2C1011.0345%2C1011.2841%2C1011.5118%2C1011.2438%2C1011.0311%2C1011.4283%2C1011.6546%2C1011.1890%2C1011.1923%2C1011.2339%2C1011.3112%2C1011.3290%2C1011.2244%2C1011.4675%2C1011.2229%2C1011.1768%2C1011.6550&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Large, multi-frequency imaging surveys, such as the Large Synaptic Survey\nTelescope (LSST), need to do near-real time analysis of very large datasets.\nThis raises a host of statistical and computational problems where standard\nmethods do not work. In this paper, we study a proposed method for combining\nstacks of images into a single summary image, sometimes referred to as a\ntemplate. This task is commonly referred to as image coaddition. In part, we\nfocus on a method proposed in previous work, which outlines a procedure for\ncombining stacks of images in an online fashion in the Fourier domain. We\nevaluate this method by comparing it to two straightforward methods through the\nuse of various criteria and simulations. Note that the goal is not to propose\nthese comparison methods for use in their own right, but to ensure that\nadditional complexity also provides substantially improved performance."}, "authors": ["Darren Homrighausen", "Christopher Genovese", "Andy Connolly", "Andy Becker", "Russell Owen"], "author_detail": {"name": "Russell Owen"}, "author": "Russell Owen", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1086/662075", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1011.4059v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.4059v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "astro-ph.IM", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "astro-ph.IM", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.4059v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.4059v1", "arxiv_comment": null, "journal_reference": null, "doi": "10.1086/662075", "fulltext": "Image Coaddition with Temporally Varying Kernels\n\narXiv:1011.4059v1 [astro-ph.IM] 17 Nov 2010\n\nHomrighausen, D; Genovese, C\nCarnegie Mellon University, Department of Statistics. Pittsburgh, PA 15232\ndhomrigh@stat.cmu.edu\nand\nConnolly, A; Becker, A.C; Owen, R\nUniversity of Washington, Department of Astronomy. Seattle, WA\nReceived\n\n;\n\naccepted\n\n\f\u20132\u2013\nABSTRACT\n\nLarge, multi-frequency imaging surveys, such as the Large Synaptic Survey\nTelescope (LSST), need to do near-real time analysis of very large datasets. This\nraises a host of statistical and computational problems where standard methods\ndo not work. In this paper, we study a proposed method for combining stacks of\nimages into a single summary image, sometimes referred to as a template. This\ntask is commonly referred to as image coaddition. In part, we focus on a method\nproposed in Kaiser (2004), which outlines a procedure for combining stacks of\nimages in an online fashion in the Fourier domain. We evaluate this method by\ncomparing it to two straightforward methods through the use of various criteria\nand simulations. Note that the goal is not to propose these comparison methods\nfor use in their own right, but to ensure that additional complexity also provides\nsubstantially improved performance.\nSubject headings: Sequential Estimation, Inverse Problems, Deconvolution\n\n\f\u20133\u2013\n1.\n\nIntroduction\n\nIn astronomy today, the standard technique for acquiring deep astronomical images is\nthrough a series of short, often dithered, exposures. Applying this procedure enables the\nremoval of cosmic rays, the filtering of bad pixels, and the masking of device defects. It\nalso increases the dynamic range of the resulting image (as bright sources in single, long\nexposures saturate) and provides better control of the underlying point spread function\n(PSF) as long as the PSF is fully sampled.\nTo achieve the depth of a long exposure from a series of exposures requires that we\ncombine the individual images, accounting for the variation in seeing, sky transparency,\nand background variability. This process is typically referred to as \"image coaddition\" and\nis at the heart of many image processing systems. The criteria that we optimize in the\nimage coaddition depends on the science in question at hand. For example, in faint galaxy\nsurveys (e.g. the Hubble Ultra Deep Field, Beckwith (2006)) photometric accuracy might\nbe paramount, but for weak lensing surveys (e.g. the Canada France Hawaii Telescope\nLegacy Survey, Parker (2007)), the preservation of the underlying image resolution may be\nthe primary concern. Given these differing objectives two open questions remain; how do\nwe optimize the coaddition of astronomical images and is there is an optimal criterion that\nis relevant to all scientific goals?\nWith a new generation of deep imaging surveys coming on-line throughout this decade,\nincluding the Panoramic Survey Telescope & Rapid Response System (Pan-STARRS1 ), the\nDark Energy Survey (DES2 ), and the Large Synoptic Sky Survey (LSST3 ), each producing\n1\n\nhttp://pan-starrs.ifa.hawaii.edu\n\n2\n\nhttp://www.darkenergysurvey.org\n\n3\n\nhttp://www.lsst.org\n\n\f\u20134\u2013\npetabytes of data, image coaddition cannot just be assed by the statistical analysis; it is\nalso a question of the computational efficiency of the different approaches. In this paper,\nwe compare several different approaches for image coaddition, taking into account their\nstatistical properties and their computational cost. We consider in detail the Fourier-domain\nmethods proposed in Kaiser (2004) along with several straightforward methods, such as\nrunning means or medians. We compare these methods using two performance criteria: flux\nconservation (i.e., pixelwise photometric accuracy) and image resolution (using a metric\nwe call Image Quality). Using these comparisons we can make statements about whether\nthe additional complexity outweighs the extra costs they incur. Our goal is to provide a\nframework for assessing the magnitude of the practical advantages more complex methods\nmight provide.\nIn Section 2, we set up the statistical model underlying our analysis and outline the\nproperties of the various techniques. In Section 3, we describe the simulations and define\nthe performance criteria, and, in Section 4, we present the results of our simulations.\n\n1.1.\n\nMathematical Setup\n\nConsider a single image of a particular patch of sky. The object of interest is the\ntrue scene, which we represent as a function g that takes a two-dimensional argument\ncorresponding to position in the image and returns an intensity. For ground-based viewing,\nhowever, we observe only a blurred, discretized, and noisy version of g. We represent the\nblurring (caused by atmospheric seeing and instrument artifacts) by an operator K. In this\ncase, K is associated with a PSF k such that the action of K on g is to integrate g against\nk.\nNow, suppose instead of getting one image we get a stack of L images, all of the same\n\n\f\u20135\u2013\npatch of sky with science g. We assume for this discussion that the images have been\nregistered. Then, due to atmospheric conditions changing over time, for each image i there\nis a possibly different operator Ki with associated PSF ki . Then, if we define a regular grid\nof pixel centers x1 , x2 , . . ., the observation on the ith image at the j th pixel is4\nYij =\n\nZ\n\nki (xj , y)g(y) dy + \u01ebij ,\n\n(1)\n\nwhere the \u01ebij 's are noise random variables with suitable distributions. Note that equation\n(1) also defines the operator Ki as integrating g against the PSF ki . The statistical problem\nis to estimate g given observation of Y = (Y1 , Y2 , . . .).\nWhat makes the problem challenging is that the operators Ki are ill conditioned,\nmeaning that large changes in g can make relatively small changes in Yij , making recovery\nof g from Yij unstable. This is so because blurring dampens high frequency structure, so\nthe singular values of this integral operator decay quickly to zero. Thus, even small levels\nof noise can critically obscure structure in g.\nTo see this in a simple example, assume that the PSF does not vary in shape across\nthe image, which makes k(x, y) = k(x \u2212 y) a convolution kernel. It follows that by using\nthe Fourier transform F , we can recover g exactly in the absence of noise by\ng=F\n\n\u22121\n\n\u0012\n\nF (Kg)\nFk\n\n\u0013\n\n(2)\n\nbecause F (Kg) = F k F g. While the inverse exists in the absence of noise, the decay of\nF k at high frequencies can produce catastrophically high variances in the presence of noise,\n4\n\nTo simplify notation, we are eschewing ordered pairs for representing two dimensional\n\ncoordinates in favor of single variables. Thus we are using a linear list of pixels and are using\nsingle two-dimensional arguments (e.g., x) for points in the sky/image. Note that integrals\nover such two dimensional arguments are actually double integrals.\n\n\f\u20136\u2013\nwhich produces nontrivial and random contribution at high frequencies (so the numerator\nis non-zero but the denominator goes to zero). Figure 1 demonstrates this effect in a\n1-dimensional analog of the problem, comparing noise free reconstruction via equation (2)\nwith reconstruction under noise with a 10e9:1 signal-to-noise ratio (SNR). The estimate in\nthe latter case is wholly untenable.\n\n1.2.\n\nStatistical Model\n\nWe assume that each observation is drawn from a distribution with mean\n\u03b8ij = (Ki g)(xj ) =\n\nZ\n\nki (xj , y)g(y) dy.\n\n(3)\n\nBecause the observation process in the telescope involves counting photons, the distribution\nof Yij is well modeled by a Poisson distribution (Hu 2007; Scully 1969). Thus, we assume\nthat the Yij 's are independent Poissonh\u03b8ij i random variables. If the mean counts \u03b8ij are\nlarge enough, the Gaussian approximation to the Poisson distribution is accurate, and we\ncan model the Yij 's as independent Normalh\u03b8ij , \u03b8ij i random variables.\nThe statistical problem is to form an estimator \u011d of g given observations Yij . In the\npresent context, \u011d is the template or coadd we are attempting to create. For each \u011d we need\nways to measure its misfit relative to the true scene g. In this paper we consider two. The\nfirst criteria corresponds to flux conservation, which we measure through Mean Integrated\nSquared Error (MISE), and the second criteria we refer to as Image Quality. We give a\nbrief description of these here. See section 3 for a more thorough discussion.\nFlux conservation gives an indication about how well a method maintains the spatial\nlocation of a given amount flux. Mean Integrated Squared Error (MISE), which corresponds\nto the expected value of the integrated squared difference between any recovered image and\nthe true image g is the measure of flux conservation we use. Specifically, suppose we form a\n\n\f\u20137\u2013\ntemplate \u011d. Then\nMISE(\u011d, g) = E\n\nZ\n\n(\u011d(x) \u2212 g(x))2 dx.\n\nSee Silverman (1985) for a thorough overview of MISE.\nWhile MISE measures the degree to which \u011d maintains flux in the correct location,\nit doesn't distinguish between different scatterings of the remaining misspecified flux. We\ndefine a second comparison, which we call Image Quality, that is designed to measure how\nspread out the source is, as this isn't captured by MISE. For Image Quality, we do a very\nrough PSF estimation on a source in the coadded image \u011d. The relative size of these PSFs\ngive us an indication about how spread out an object gets by the coaddition. See Fig. 2 for\nan example of proceedures with combinations of good and bad MISE and Image Quality.\nOne significant complication in practice is that the PSFs are unknown and must be\nestimated as part of the procedure. While it is ideal to simultaneously estimate the PSF\nand g, it is common, and typically effective, to first estimate the PSF and then use the\nestimate as a 'known' PSF in the coaddition procedure. Because our focus is on the relative\nperformance of techniques for estimating g, we will follow standard practices and assume\nthat the PSFs are known. This produces a comparison of the techniques at their best\nbut does not account for how well the procedures tolerate mis-estimation of the kernels.\nNote that the straightforward comparison methods do not require PSF estimates. So, any\nmethod that does require knowledge of the PSF would need to be substantially better,\ngiven it has access to much more information.\n\n2.\n\nMethods\n\nIn this section, we describe a variety of methods that have been applied to the\ncoaddition problem, with particular focus on the Fourier domain approach we are analyzing.\n\n\f\u20138\u2013\nNote that we restrict our attention to non-iterative methods.\nLucky Imaging. In lucky imaging, a large number of images are observed and only the\n'best,' according to some criterion such as seeing, are retained. Fried (1978) and Tubbs\n(2003) describe such implementations in detail. An advantage of this approach is that the\nreconstruction of the true scene is based entirely on high quality data. A disadvantage is\nthat the method requires storing many images to determine which are best. Moreover, the\nimages that are discarded can contain useful information about the scene that is, in effect,\nwasted.\nPixelwise Statistics. If a stack of images is aligned, the values at a particular pixel\nin each image represent a random sample from a common distribution. The aligned pixel\nstacks can thus be used to estimate the parameters of these distributions and in turn the\ntrue scene. Basic choices would be a mean, median, or perhaps a trimmed mean to account\nfor heavy-tailed noise, but more sophisticated estimators tailored to particular distributional\nassumptions can be constructed. The advantages of these approaches are computational\nand conceptual simplicity. Moreover, the mean can be computed sequentially with a fixed\namount of storage, although this is not true of the median or trimmed mean, for which the\nentire image stack needs to be maintained. A big disadvantage is that pixelwise methods\ntend to create rough, discontinuous images as no information sharing is permitted between\npixels.\nFourier Deconvolution. In spatially constant seeing case is that equation (1) has a\nconvolutional structure ki (x, y) \u2261 ki (x \u2212 y). The advantage of Fourier based methods is\nthat the operator Ki decomposes nicely in the Fourier domain. An inherent disadvantage\nto the Fourier approach is that great care must be used to avoid outcomes such as Fig. 1.\nThe approach outlined in Kaiser (2004) is to Fourier transform each image and estimate\nthe uth Fourier coefficient of g by a weighted average of the uth Fourier coefficient of each\n\n\f\u20139\u2013\nimage. The weighting is accomplished so that the images with better seeing are weighted\nmore heavily in the average. This method has some associated optimality properties.\nHowever, in practice, these properties turn out to not be useful. See below for a more\nthorough description.\nNote that, as presented, this method makes three nontrivial assumptions. First, it\nassumes that each ki (x, y) \u2261 ki (x \u2212 y), that is, spatially constant seeing. As we describe\nin Section 4, while this assumption could have significant consequences when seeing varies\nspatially, in practice, it does not appear to cause much problem. The second assumption is\nthat the \u03b8ij 's are large enough that the Gaussian approximation to the Poisson is accurate\nacross the image. The third assumption is that the variance of the Gaussian is a known\nvalue \u03c3i2 that depends only on the image i. These last two assumptions make the analysis\neasier, and while they are often reasonable, they need not hold in practice.\nTo define the Fourier Deconvolution estimator, first expand g into the Fourier basis,\nwhich we write as (\u03c6u ),\ng=\n\nX\n\ng\u0303(u)\u03c6u .\n\n(4)\n\nu\n\nIn general, we use the notation f \u0303 for the Fourier transform of the function f .\nUsing the above assumptions, and the resulting form of the \u03b8ij 's, Kaiser (2004) proposes\ntwo estimators of g\u0303(u). See Table 1 for these estimators and some of their cumulants. The\nd has the smallest variance of all unbiased estimators of\nfirst estimator in the table, g\u0303(u),\n\ng\u0303(u). Also, the resulting estimator of g, defined to be\n\u011d =\n\nX\nu\n\nis the best linear unbiased estimator of g.\n\nd u,\ng\u0303(u)\u03c6\n\n(5)\n\nHowever, as alluded to previously, these optimality properties are not useful. To see\nd Also, since the\nthis, observe that the variance of \u011d is the sum of the variances of g\u0303(u).\n\n\f\u2013 10 \u2013\nEstimator\n\n\uf8eb\n\nd\ng\u0303(u)\n\n=\n\n\uf8ec\n\uf8ec\n\uf8ed\n\nExpectation & Variance\n\n\uf8f6\n\nL\nX\nk\u0303l (u)\u1ef8l (u) \uf8f7\n\nl=1\n\uf8eb\nL\nX\n\uf8ec\n\uf8ec\n\uf8ed\ni=1\n\n\u03c3l2\n\n\uf8f7\n\uf8f8\n\n|k\u0303i (u)|2 \uf8f7\n\uf8f7\n\u03c3i2 \uf8f8\n\nh\ni\nd\nVg g\u0303(u)\n\nL\nX\nk\u0303l (u)\u1ef8l (u) \uf8f7\n\uf8ec\n\uf8ec\n\uf8ed\n\nl=1\n\n\u03c3l2\n\nv\n\uf8ebv\nu\nu L\nL\nuX\nuX\n2\nu\n\uf8ecu\n|\nk\u0303\n(u)|\ni\nu\n\uf8ecu\nu\n\uf8ect\n\uf8ed\n\u03c3i2 t j=1\ni=1\n\n\uf8f7\n\uf8f8\n\uf8f6\n\n1 \uf8f7\n\uf8f7\n\uf8f7\n\u03c3j s2 \uf8f8\n\nh\ni\n[\nEg g\u0303\u2217 (u)\n\nVg\n\nTable 1:: k\u0303 \u2217 (u) :=\n\ntion.\n\n= g\u0303(u)\n\n=\n\n1\n\uf8f6\nL\nX\n|k\u0303i (u)|2 \uf8f7\n\uf8ec\n\uf8eb\n\uf8ec\n\uf8ed\n\ni=1\n\n\u03c3i2\n\n\uf8f7\n\uf8f8\n\n\uf8f6\n\n\uf8eb\n\ng\u0303[\n\u2217 (u) =\n\nh\ni\nd\nEg g\u0303(u)\n\n\uf8f6\n\nv\nu L\nuX\nu\nu\n|k\u0303i (u)|2/\u03c3i2\nt\ni=1\nv\nu\nL\nuX\nu\nu\nu\n1/\u03c3j2\nt\nj=1\n\nh\n\ni\n[\ng\u0303\u2217 (u)\n\n= k \u2217 (u)g\u0303(u)\n\nL\nX\n1\n=\n\u03c32\ni=1 i\n\n, which is the result of a variance stabilizing transforma-\n\nK \u2032 s are smoothing operators, |k\u0303(u)| is small for large |u|. In fact, the worse the seeing,\nthe smaller |k\u0303(u)| becomes and can be smaller than machine error in many cases. Hence,\nd depends on the reciprocal of |k\u0303(u)|, the variance of \u011d can be arbitrarily large.\nas Vg [g\u0303(u)]\n\nNote that we have seen a case of this already in Figure 1 where this large variance property\n\n\f\u2013 11 \u2013\ncauses unusable results.\nGenerally speaking, demanding unbiased estimators is less effective than choosing\nan estimator with some bias but smaller variance. In this case, the distinction is crucial\nbecause the unbiased estimator is unusable. For the interested reader, Chapter 7 of\nWasserman (2006) provides a good introduction to this surprisingly broad and deep issue5 .\nSee Figure 3 for an example of equation (5) in a simple simulated situation. The ability\nof \u011d to reconstruct even a simple g decays rapidly as the full width at half maximum6 , or\nFWHM, of the PSF increases. In particular, even the small amount of seeing (FWHM of\n1.1 pixels) in the right most column makes the estimator unusable7 .\nKaiser (2004) provides a correction that prevents the variance from exploding for large\nd by a term that is proportional to the reciprocal of its standard\n|u| by multiplying g\u0303(u)\n\ndeviation. See the second row of Table 1. This multiplication biases the estimator but\nbounds the variance. We define\n\u011d\u2217 =\n\nX\n\ng\u0303[\n\u2217 (u)\u03c6u\n\n(6)\n\nu\n\nand we spend the balance of the paper investigating its properties. We refer to \u011d\u2217 as the\nFourier Deconvolution (FD) method. As an aside, since the correction corresponds to\nmultiplication in Fourier space, we see that the bias of \u011d\u2217 is given by (k \u2217 \u2217 g) \u2212 g, where the\nFourier transform of k \u2217 is defined in the caption of Table 1 and (a \u2217 b) is the convolution of\na and b.\n5\n\nBetter performance under many different criteria comes from giving up some bias for\n\nimproved variance. One well used example of this compromise is Tikhonov regularization.\n6\n\nThe full width at half maximum is defined to be FWHM(k) := supx1 ,x2 \u2208Pk |x1 \u2212x2 | where\nn\no\nPk = x : k(x) = ||k||2 \u221e .\n7\n\nThe fact that this PSF is undersampled is not important as we numerically applied a\n\nknown PSF to a known image.\n\n\f\u2013 12 \u2013\n2.1.\n\nComputational and Space Complexities\n\nThe computational complexity of the FD method is dominated by the need to Fourier\ntransform the data and kernel. If we suppose the images are m by n and N := mn, then\nwe can perform a Fourier transform in O(N log N) FLOPs8 via the fast Fourier transform.\nThis is the dominating computation in the FD method. However, it also has several order\nO(N) computations9 . The mean method requires 2N FLOPs and the median method\nrequires N comparisons to update for each new image.\nThe FD method must maintain two N pixel images and the mean method requires one\nN pixel image, each of which get updated after each new image and kernel is recorded.\nThe median method requires the entire stack must be maintained at all times, though not\nnecessary in RAM. Hence, after L images have been observed, all L images must be kept\nfor possible updating of the median.\n\n3.\n\nMethods: Images and Evaluation Criteria\n\nFor our evaluation metrics, we need to know the true sky before any distortions. Hence,\nwe simulate an idealized view of the sky (above the atmosphere) using a catalog of point\nand extend sources. Extended sources are represented by Sersic profiles and the density of\npoint and extended sources is designed to match observations to a depth of r\u223c28. Figure\n4 shows a representation of one such image. From these true scenes, we apply a blurring\n8\n\nFLOPs stands for Floating-point Operations and corresponds loosely to additions and\n\nmultiplications.\n9\n\nThe square root operation takes a variable number of FLOPs. This is the cost for\n\nupdating the method with each new image. The exact number depends on the specific\ncomputing floating point unit.\n\n\f\u2013 13 \u2013\noperator K and noise with variance \u03c3 2 to represent the effect of the atmosphere, telescope,\nand instrument.\nUnder the assumptions of the FD method, we assume the PSFs are convolutional (by\nthat we mean spatially constant seeing), but this leaves open the specific functional form\nfor the kernel k. We choose to set k to be the two dimensional Gaussian probability density\nfunction. Our results are, however, robust against more complicated parametrization of k\n(e.g., a mixture of Gaussians).\nFor evaluating the effectiveness of the FD method, we compare it to the pixel-wise\nmean and pixel-wise median. Our goal is to assess the value added by the FD method and\nevaluate whether the improved performance overcomes the added cost and complexity of\nthe method. The mean and median procedures are intended as extreme comparisons rather\nthan serious proposals and we aren't endorsing their use in this paper. However, if the value\nadded of more sophisticated methods relative to even these simplistic methods is not great,\nthen it suggests that simple methods may suffice in practice. Note that the computational\ncomplexity of the methods is not a purely academic concern. In large imaging surveys such\nas the LSST, a near constant data stream requires efficient, near real-time processing, and\nthe cost of generating template images is an important factor.\nFor this comparison, we use the two general criteria introduced in section 1.2\ncorresponding to MISE and Image Quality. In principle, at least two different errors can\nbe made when comparing an estimated template \u011d to the true scene g. These errors can\nmost easily be thought of in the context of a point source such as a star. Our estimated\nimage can remove some of the flux from the point source and/or it can smooth the image\n(degrading the photometric accuracy and the resolution of the resulting template). Figure\n2 shows examples of a pixelated point source that has been reconstructed by four made-up\nmethods.\n\n\f\u2013 14 \u2013\nIn the MISE comparison we compute the expected value of the integrated squared\ndifference between the true image and our estimate, assuming zero background. Under\nthe non-Gaussian distributions, a larger variance results in more total flux in the image.\nThis, in turn, results in a large MISE. To compensate for this increase, and to make all the\ncases more directly comparable, we rescale the MISE by the total signal of each image to\ncompensate for this additional flux.\nFor the evaluation of Image Quality we fit a two dimensional, spherical10 Gaussian\ndensity to a source using least squares. The covariance matrix of this fitted Gaussian is\nthen used as a metric for the width of the point source.\n\n4.\n\nResults\n\nIn this section, we present the results of our simulations. The section is divided into\ntwo parts, corresponding to the two evaluation criteria outlined above.\n\n4.1.\n\nMISE\n\nWhen looking at equation (1) we see two major influences on the quality of the\nobservation. One is the type and severity of the seeing, determined by the PSF k. The\nother is the distribution of the noise \u01eb. In this section, we look at several different scenarios\nfor comparing the mean or median to the FD method by changing these two factors.\n10\n\nBy spherical, we mean that the covariance matrix is proportional to the identity matrix.\n\n\f\u2013 15 \u2013\n4.1.1. Increasing FWHM Comparison\nOverall, we wish to understand the impact of seeing and noise on the performance of\nthe methods. To do this, we simulate a stack of images with each image having a FWHM\ndrawn from a distribution with mean, \u03bcF W HM . We do this procedure for \u03bcF W HM ranging\nfrom 2 pixels to 7 pixels (0.4\" to 1.4\"). This interval is chosen so that it contains the\nexpected seeing width for the LSST. This procedure creates a sequence of stacks of images.\nWe apply this increasing FWHM to six different noise parameterizations. For both\nhigh variance (5:1 SNR) and low variance (20:1 SNR) we have the noise term be distributed\nGaussian, heavy-tailed shot, and inhomogeneous Poisson noise. In all six combinations, a\nlower MISE indicates that the method conserved the flux better, on average. See Figure 5\nfor the results.\nIn all cases, the mean severity of seeing, \u03bcF W HM , does not impact the ranking of the\nmethods. In the Gaussian case, the methods are ranked, from best to worst, as FD, mean,\nand then median. In both the low and high noise cases, absolute difference between the\nmean and the median is constant for all levels of \u03bcF W HM due to the well-known efficiency\ngains of the mean over the median in the Gaussian case. However, in the high noise case,\nthe difference in MISE between the FD and mean methods increases 11% as \u03bcF W HM ranges\nfrom 2 to 7 pixels. As the FD method uses the additional information of a known seeing\nkernel, this non-constant difference is expected.\nIn the inhomogeneous Poisson or heavy-tailed shot noise cases, the median outperforms\nboth other methods. This owes to the particular and well known feature of the median\nto be more robust against heavier-tailed distributions. In the shot-noise case, the median\nignores the random noise spikes with overwhelming probability and we get a noise free\nversion of the true scene g with the median amount of seeing. The very small advantage\nof the FD over the mean method owes to the FD's slightly narrower impulse response\n\n\f\u2013 16 \u2013\nfunction, which we discuss in section 4.2 and Figure 7. In the inhomogeneous Poisson noise\ncase, the observed image is created by simulating independently from a Poissonh\u03b8ij + \u03c3 2 i\nfrom equation (3). This creates a very noisy image, especially near sources, and hence the\nmedian's ability to ignore large, transient spikes results in the large MISE advantage.\nUnder all three noise distributions, the FD method displays a small but increasing\nbenefit in flux conservation over the mean as \u03bcF W HM goes from 2 to 7. It should be kept in\nmind that the FD method uses the known kernel assumption. Hence, as the seeing becomes\nmore severe this informational advantage should become more pronounced. In reality, the\nkernel would need to be estimated and this advantage would most likely decay.\n\n4.2.\n\nImage Quality\n\nTo do this comparison we use an image where the function g is a \u03b4-function (i.e.\na point source). We use the expected distribution of seeing at the LSST stack to draw\n10 independent and identically distributed Gaussian PSFs and apply them to the point\nsource. This mimics the forward process of an image getting blurred by the atmosphere\nand corresponds to operating on g with Ki for i = 1, 2, . . . , 10. For this stack of images we\napply the three methods described earlier, FD, mean, and median, to derive a template\nimage. We then fit a spherical Gaussian kernel to the template via least squares and use the\ndiagonal element of the covariance matrix as a measure of the width of the method's PSF.\nWe make 1000 draws from the seeing distribution and compute this statistic for each\nmethod and each draw. The results are summarized in Figure 6 with boxplots of the\nFWHM of the fitted Gaussian kernel. Boxplots are a graphical tool for quickly conveying\nthe distribution of observed data and are composed of a 'box' and 'whiskers.' The 'box' is\n\n\f\u2013 17 \u2013\nthe main rectangle, which has horizontal lines, increasing with FWHM, at the 25th, 50th11 ,\nand 75th percentiles of the data. The 'whiskers' correspond to the dotted vertical lines with\nhorizontal lines at the end and the plus signs. These horizontal lines are at the 1st and 99th\npercentile. The pluses are at extreme data points.\nThe boxplots are very similar and show little difference in the FWHM of the fitted\nkernels. The FD method has longer 'whiskers' than the other methods, indicating a wider\nrange of fitted FWHM that can be expected. It also has a very slightly lower 'box' than the\nother two methods, indicating that the majority of draws result in a slightly better fitted\nFWHM. For instance, the 50th percentile of the FD method's fitted FWHM is 1.2% lower\nthen the mean method's fitted FWHM.\nInterestingly, one of the draws resulted in six very good seeing images and four poor\nimages. This resulted in the median method having the smallest fitted FWHM recorded\nfor any method on any draw. This is part of a more general property of the median to\nbehave non-continuously with the composition of the stack. For instance, if there were\ninstead 4 very good seeing images and 6 poor seeing images, the result would be a large\nfitted FWHM.\n\n4.2.1. Differences in Expectation\nThe mean and FD methods are very similar in that they are both based around\nsumming the images and hence are both linear filters on the observed images. Let's denote\nthe estimated coadds of the two methods as \u011dmean and \u011d\u2217 , respectively. We can understand\nthe filtering that we are applying to the true image g by considering E[\u011dmean ] and E[\u011d\u2217 ].\n11\n\nThe 50th percentile is the median of a data set.\n\n\f\u2013 18 \u2013\nNote that these are\n1X\nKi\nL i=1\nL\n\nE[\u011dmean ] =\n\n!\n\ng=\n\nZ\n\nkmean (x \u2212 y)g(y)dy\n\nand\n\nE[\u011d\u2217 ] =\n\nZ\n\nk \u2217 (x \u2212 y)g(y)dy\n(7)\n\nwhere the Fourier transform of the function k \u2217 is defined in the caption under Table 1.\nBy examining the impulse response functions (IRFs) of these filters we can find the\naverage width of the PSFs that will result from these methods. Note that in this case this\ncorresponds to the bias of each method. See Figure 7 for a one dimensional representation\nof the IRFs of the expected filters. The mean IRF and the Fourier Deconvolution IRF are\nvirtually identical and would create no visible difference in the image quality.\n\n5.\n\nDiscussion\n\nA variety of template generation/image coaddition techniques have been applied\nin current imaging surveys, including the CFHTLS (Gwyn (2008); mean technique),\nPanSTARRS (Price (2007), mean technique), MOPEX for SIRTF images (Makovoz\n(2005), mean and median technique), and the VIRMOS survey (Radovich (2004), median\ntechnique). In this paper, we address the broad question of whether sophisticated methods\nof template generation and image coaddition are worth the added cost and complexity, in\nthe context of modern image surveys with large and nearly constant data streams. To this\nend, we compare Fourier domain reconstruction techniques which have a variety of nice\ntheoretical properties to straightforward pixelwise statistics. We consider two cost metrics\nto evaluate the image reconstructions; Image Quality by way of the resulting Gaussian\nFWHM of an image and the conservation of flux, measured by MISE.\nUnder the assumptions that motivate the FD method, namely Gaussian noise and\nspatially constant seeing, the FD and mean methods both have lower MISE than the\n\n\f\u2013 19 \u2013\nmedian. However, in the heavy-tailed shot and inhomogeneous Poisson noise cases, the\nmedian outperforms the FD method.\nIn situations when there is a small amount of seeing in an image stack, corresponding\nto \u03bcF W HM \u2248 2 pixels, we see the mean and FD methods have nearly the same MISE\nproperties (less than 0.7% difference across all considered noise distributions). However, as\nthe severity of the seeing increases, the disparity between the methods increases as well. As\nthe FD method incorporates knowledge of the seeing kernel, this property is to be expected.\nHowever, the MISE for the high noise Gaussian, shot, and inhomogeneous Poisson cases are\nonly 5.4%, 1.2%, and 0.13% lower, respectively, at the most severe levels of seeing.\nThese benefits are rather small, particularly in the more realistic heavy-tailed and\nPoisson noise cases. This is especially so considering the extra computational complexities\ninvolved in the FD method over the mean and median methods.\nFor comparing image quality of the templates, we find that the resolution of the\nresulting images is very similar as the 50th percentile of the fitted FWHM of the FD method\nis less than 2% smaller than the 50th percentile of the fitted FWHM of the mean method.\nWe find that the value added by the FD method over the mean or median procedures\ndoes not overcome its added cost and complexity. This leads us to the conclusion that\nthere is room for improvement over all three methods. We are currently exploring various\npossible improvements and theoretical justifications for some of the approaches outlined in\nthis paper.\n\n\f\u2013 20 \u2013\n\n0.4\n\n0.4\n\n0.3\n\n0.3\n\ng\n\ng\n\nKg\n\nY\n\n0.2\n\n0.2\n\n0.1\n\n0.1\n\n0.0\n\n0.0\n\n\u0003\n\n0.1\n\n\u0002\n\u0001\n\n\u0007\n\n0.1\n\n0.2\n\n\u0006\n\n0.2\n\n0.3\n\n\u0005\n\n0.4\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n120\n\n\u0004\n\n0.3\n\n0.4\n0\n\n(a) No Noise: Example\n\n40\n\n60\n\n80\n\n100\n\n120\n\n(b) 10e9:1 SNR: Example\n\n0.4\n\n0.4\n\n0.3\n\n0.3\n\ng\nEstimate\n\n0.2\n\n0.1\n\n0.0\n\n0.0\n\n0.1\n\n\u000f0.1\n\n0.2\n\n\u000e0.2\n\n0.3\n\n0.3\n20\n\n40\n\n60\n\n80\n\n100\n\n(c) No Noise: Estimate\n\ng\nEstimate\n\n0.2\n\n0.1\n\n\b0.40\n\n20\n\n120\n\n0.40\n\n20\n\n40\n\n60\n\n80\n\n100\n\n120\n\n(d) 10e9:1 SNR: Estimate\n\nFig. 1.-: We simulate a very simple case of a g that is a mixture of two Gaussians,\nrepresented as blue, solid curves. We apply constant Gaussian seeing as well, which results\nin a new function Kg, displayed as a green, dashed curve. Then, we make observations\nat discrete, regular points, plotted as solid green dots. In (a) we observe Kg directly and\nin (b) we observe Kg under a miniscule amount of noise, 10e9:1 SNR. Using equation (2),\nwe attempt to recover g using these observations. These recoveries are in (c) and (d),\nrespectively. Notice that we can perfectly reconstruct g in absence of noise. However, even\nthough Y and Kg are visually identical, the same inverse approach fails spectacularly in the\nsecond case.\n\n\f\u2013 21 \u2013\n\n(a) Good MISE, Good Image Quality\n\n(b) Bad MISE, Good Image Quality\n\n(c) Good MISE, Bad Image Quality\n\n(d) Bad MISE, Bad Image Quality\n\nFig. 2.-: The dashed line represents the single pixel \u03b4-function of g. The solid line corresponds to a possible reconstruction. The first column has estimates that have good MISE\nand the first row has estimates that have good Image Quality. The second column and row\nhave poor MISE and Image Quality, respectively.\n\n\f\u2013 22 \u2013\n\nFig. 3.-: This is an example of the use of \u011d. The true scene g is a one pixel delta function\nrepresenting a star (not pictured). Top Row: An example of an input, or observed, image.\nBottom Row: Reconstruction using \u011d. Left to Right: We increase the FWHM from 0.5\npixels to 1.1 pixels. Note that this means that some of the images have an undersampled\nPSF. This isn't an issue in this case as we are applying the seeing ourselves and can without\nloss of generality assume the star is at the centroid of the pixel spike in g. The performance\ndecays rapidly as the width of the seeing increases. In particular, even the mild amount of\nseeing in the right most column makes the estimator unusable. The magnified high frequency\ninformation has swamped the signal we wish to recover and left checkerboard pattern seen\nin the bottom right image..\n\n\f\u2013 23 \u2013\n\nFig. 4.-: The simulated image after being z-scaled and power transformed. The pixel width\nof this simulation is 0.2 arcsec (0.2\").\n\n\f\u2013 24 \u2013\n\n(a) Gaussian, Low Noise\n\n(b) Gaussian, High Noise\n\n(c) Shot, Low Noise\n\n(d) Shot, High Noise\n\n(e) Inhomogeneous Poisson, Low Noise\n\n(f) Inhomogeneous Poisson, High Noise\n\nFig. 5.-: Results for the MISE comparison. The \"Low Noise\" and \"High Noise\" are 20:1\nand 5:1 signal to noise, respectively. Notice that the FD method slightly outperforms the\nmean and median under the Gaussian assumption. However, the median, being more robust\n\n\f\u2013 25 \u2013\n\n2.4\n\n2.3\n\nFWHM\n\n2.2\n\n2.1\n\n2.0\n\n1.9\n\n1.8\n\n1.7\n\nFourier\n\nMean\n\nMedian\n\nFig. 6.-: Boxplots of the FWHM of the fitted Gaussian kernel for stacks created from draws\nfrom the expected seeing distribution of the LSST site. The three methods considered in\nthis paper are shown. The boxplots are very similar and show little difference in the FWHM\nof the fitted kernels.\n\n\f\u2013 26 \u2013\n\nFig. 7.-: This figure shows the IRF of the expected filter for the mean (dashed line) and\nFD (solid line) methods. Though k \u2217 has more mass at lag = 0, bear in mind that it uses\na weighting scheme that depends on knowledge of both the true PSFs and true variances.\nTaking this into consideration, the difference is slight.\n\n\f\u2013 27 \u2013\nREFERENCES\nBeckwith, Steven et al., 2006, AJ, 132, 5, pg. 1729\nFried, D.L., 1978, J. Optical Soc. Amer, 86, pgs. 1651-1657\nGwyn, S. D. J., 2008, PASP, Feb., 120, pgs. 212-223\nHu, Yuchng et al., 2007, Physics Letters A, 367, pgs. 173-176\nKaiser, Nick, 2004, Private communication\nMokovoz, D. et al., 2005, Astronomical Data Analysis Software and Systems XIV, 347, Dec,\npg. 81\nParker, Laura et al., 2007, ApJ, 669, 1, pg. 21\nPrice, P.A and Pan-STARRS Team, Bulletin of the American Astronomical Society, 38,\nMay, pg. 183\nRadovich M. et al., 2004, A&A, Apr., 417, pgs. 51-60\nScully, Marlan et al., 1969, Physics Review. 179, pgs. 368-374\nSilverman, Brian, 1985, Chapman & Hall/CRC, Monographs on Statistics and Applied\nProbability, Vol 26\nTubbs, R.N., 2003, PhD thesis, Cambridge Univ\nWasserman, Larry, 2006, Chapter 7, Springer Science\n\nThis manuscript was prepared with the AAS LATEX macros v5.2.\n\n\f"}