{"id": "http://arxiv.org/abs/1111.0712v1", "guidislink": true, "updated": "2011-11-03T01:58:45Z", "updated_parsed": [2011, 11, 3, 1, 58, 45, 3, 307, 0], "published": "2011-11-03T01:58:45Z", "published_parsed": [2011, 11, 3, 1, 58, 45, 3, 307, 0], "title": "Online Learning with Preference Feedback", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1111.2385%2C1111.3313%2C1111.6487%2C1111.0188%2C1111.0833%2C1111.0988%2C1111.1451%2C1111.4776%2C1111.4199%2C1111.1971%2C1111.2637%2C1111.6476%2C1111.1572%2C1111.5030%2C1111.1343%2C1111.1076%2C1111.1171%2C1111.4790%2C1111.1541%2C1111.0785%2C1111.1032%2C1111.5358%2C1111.4219%2C1111.2457%2C1111.5176%2C1111.0665%2C1111.0793%2C1111.6558%2C1111.5700%2C1111.0830%2C1111.0269%2C1111.3475%2C1111.7305%2C1111.6100%2C1111.4710%2C1111.6236%2C1111.2921%2C1111.1395%2C1111.4343%2C1111.3603%2C1111.7121%2C1111.2308%2C1111.4846%2C1111.3104%2C1111.2887%2C1111.4623%2C1111.4208%2C1111.7230%2C1111.4005%2C1111.3301%2C1111.5456%2C1111.3392%2C1111.2331%2C1111.6081%2C1111.3132%2C1111.1150%2C1111.2942%2C1111.5970%2C1111.5082%2C1111.3875%2C1111.1239%2C1111.4111%2C1111.5145%2C1111.6724%2C1111.6452%2C1111.6282%2C1111.1028%2C1111.0448%2C1111.5714%2C1111.3335%2C1111.5944%2C1111.3807%2C1111.3391%2C1111.6426%2C1111.7026%2C1111.0560%2C1111.4670%2C1111.2174%2C1111.3186%2C1111.0635%2C1111.0953%2C1111.2962%2C1111.4329%2C1111.0107%2C1111.5653%2C1111.0031%2C1111.0608%2C1111.0712%2C1111.1853%2C1111.6802%2C1111.6337%2C1111.0673%2C1111.5368%2C1111.1481%2C1111.2842%2C1111.4663%2C1111.1519%2C1111.0722%2C1111.2402%2C1111.4137%2C1111.6691&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Online Learning with Preference Feedback"}, "summary": "We propose a new online learning model for learning with preference feedback.\nThe model is especially suited for applications like web search and recommender\nsystems, where preference data is readily available from implicit user feedback\n(e.g. clicks). In particular, at each time step a potentially structured object\n(e.g. a ranking) is presented to the user in response to a context (e.g.\nquery), providing him or her with some unobserved amount of utility. As\nfeedback the algorithm receives an improved object that would have provided\nhigher utility. We propose a learning algorithm with provable regret bounds for\nthis online learning setting and demonstrate its effectiveness on a web-search\napplication. The new learning model also applies to many other interactive\nlearning problems and admits several interesting extensions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1111.2385%2C1111.3313%2C1111.6487%2C1111.0188%2C1111.0833%2C1111.0988%2C1111.1451%2C1111.4776%2C1111.4199%2C1111.1971%2C1111.2637%2C1111.6476%2C1111.1572%2C1111.5030%2C1111.1343%2C1111.1076%2C1111.1171%2C1111.4790%2C1111.1541%2C1111.0785%2C1111.1032%2C1111.5358%2C1111.4219%2C1111.2457%2C1111.5176%2C1111.0665%2C1111.0793%2C1111.6558%2C1111.5700%2C1111.0830%2C1111.0269%2C1111.3475%2C1111.7305%2C1111.6100%2C1111.4710%2C1111.6236%2C1111.2921%2C1111.1395%2C1111.4343%2C1111.3603%2C1111.7121%2C1111.2308%2C1111.4846%2C1111.3104%2C1111.2887%2C1111.4623%2C1111.4208%2C1111.7230%2C1111.4005%2C1111.3301%2C1111.5456%2C1111.3392%2C1111.2331%2C1111.6081%2C1111.3132%2C1111.1150%2C1111.2942%2C1111.5970%2C1111.5082%2C1111.3875%2C1111.1239%2C1111.4111%2C1111.5145%2C1111.6724%2C1111.6452%2C1111.6282%2C1111.1028%2C1111.0448%2C1111.5714%2C1111.3335%2C1111.5944%2C1111.3807%2C1111.3391%2C1111.6426%2C1111.7026%2C1111.0560%2C1111.4670%2C1111.2174%2C1111.3186%2C1111.0635%2C1111.0953%2C1111.2962%2C1111.4329%2C1111.0107%2C1111.5653%2C1111.0031%2C1111.0608%2C1111.0712%2C1111.1853%2C1111.6802%2C1111.6337%2C1111.0673%2C1111.5368%2C1111.1481%2C1111.2842%2C1111.4663%2C1111.1519%2C1111.0722%2C1111.2402%2C1111.4137%2C1111.6691&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We propose a new online learning model for learning with preference feedback.\nThe model is especially suited for applications like web search and recommender\nsystems, where preference data is readily available from implicit user feedback\n(e.g. clicks). In particular, at each time step a potentially structured object\n(e.g. a ranking) is presented to the user in response to a context (e.g.\nquery), providing him or her with some unobserved amount of utility. As\nfeedback the algorithm receives an improved object that would have provided\nhigher utility. We propose a learning algorithm with provable regret bounds for\nthis online learning setting and demonstrate its effectiveness on a web-search\napplication. The new learning model also applies to many other interactive\nlearning problems and admits several interesting extensions."}, "authors": ["Pannagadatta K. Shivaswamy", "Thorsten Joachims"], "author_detail": {"name": "Thorsten Joachims"}, "author": "Thorsten Joachims", "links": [{"href": "http://arxiv.org/abs/1111.0712v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1111.0712v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1111.0712v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1111.0712v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:1111.0712v1 [cs.LG] 3 Nov 2011\n\nOnline Learning with Preference Feedback\n\nThorsten Joachims\nDepartment of Computer Science\nCornell University, Ithaca NY\ntj@cs.cornell.edu\n\nPannagadatta K. Shivaswamy\nDepartment of Computer Science\nCornell University, Ithaca NY\npannaga@cs.cornell.edu\n\nAbstract\nWe propose a new online learning model for learning with preference feedback.\nThe model is especially suited for applications like web search and recommender\nsystems, where preference data is readily available from implicit user feedback\n(e.g. clicks). In particular, at each time step a potentially structured object (e.g. a\nranking) is presented to the user in response to a context (e.g. query), providing\nhim or her with some unobserved amount of utility. As feedback the algorithm\nreceives an improved object that would have provided higher utility. We propose\na learning algorithm with provable regret bounds for this online learning setting\nand demonstrate its effectiveness on a web-search application. The new learning\nmodel also applies to many other interactive learning problems and admits several\ninteresting extensions.\n\n1 Introduction\nOur new learning model is motivated by how users interact with a web-search engine or a recommender system. At each time step, the user issues a query and the system responds by supplying a\nlist of results. The user views some of the results and selects those that he or she prefers. Here are\ntwo such examples:\nWeb Search: In response to a query, the search engine presents the ranking [A, B, C, D, E, ...] and\nobserves that the user clicks on documents C and D.\nMovie Recommendation: An online service recommends movie A to a user. However, the user\nignores the recommendation and instead rents another movie B after some browsing.\nIn both cases the user feedback comes in the form of a preference. In the web search example,\nwe can infer that the user would have preferred the ranking [C, D, A, B, E, ...] over the one we\npresented [6]. In the recommendation example, movie B was preferred over movie A. The cardinal\nutilities of the predictions, however, are never observed, and the algorithm typically does not get the\noptimal ranking/movie as feedback.\nThis preference feedback is different from conventional online learning models. In the simplest\nform of the multi-armed bandit problem [2, 1, 3], an algorithm chooses an action (out of K possible\nactions) and observes reward only for that action. Conversely, rewards of all possible actions are\nrevealed in the case of learning with expert advice [3]. Our model, where the ordering of two arms\nis revealed (the one we presented and the one we receive as feedback), sits between the expert and\nthe bandit setting. A similar relationship holds for online convex optimization [9] and online convex\noptimization in the bandit setting [5], which can be viewed as continuous extensions of the expert\nand the bandit problems respectively, since they rely on observing either a full convex function or\nthe value of a convex functions after each iteration. Most closely related to our work is the dueling\nbandits setting [7, 8], but existing algorithms are known to converge rather slowly.\n1\n\n\fIn the following, we formally define the online preference learning model and a notion of regret, propose a simple algorithm for which we prove a regret bound, and empirically evaluate the algorithm\non a web-search problem.\n\n2 Online Preference Learning Model\nThe online preference learning model is defined as follows. At each round t, the learning algorithm\nreceives a context xt \u2208 X and presents a (possibly structured) object yt \u2208 Y. In response, the user\nreturns an object \u0233t \u2208 Y which the algorithm receives as feedback. For example, in web-search,\na user issues a query and is presented with a ranked list of URL's (yt ). The user interacts with the\nranking that was provided to her by clicking on results that are relevant to her. This user interaction\nallows us to infer a better ranking \u0233t to this user.\nWe assume that the user evaluates rankings according to a utility function U (x, y) that is unknown\nto the learning algorithm. A natural way to define regret in this model is based on the difference\nin utility U (xt , yt\u2217 ) \u2212 U (xt , yt ) between the object yt we present and the best possible objects\nyt\u2217 = argmaxy U (xt , y) that could have been presented. The goal of an algorithm is to minimize\nREGRETT :=\n\nT\n1X\n(U (xt , yt\u2217 ) \u2212 U (xt , yt )) .\nT t=1\n\n(1)\n\nTo prove bounds on the regret, we specify the properties of the user's preference feedback more\nprecisely. We say that user feedback is \u03b1-informative, if for some \u03b1 \u2208 (0, 1] and \u03bet \u2265 0\n(U (xt , \u0233t ) \u2212 U (xt , yt )) = \u03b1 (U (xt , yt\u2217 ) \u2212 U (xt , yt )) \u2212 \u03bet .\n\n(2)\n\nIntuitively, the above definition describes the quality of feedback by how much the utility of the user\nfeedback \u0233t is higher than that of the algorithm's prediction yt in terms of an (unknown) fraction \u03b1\nof the maximum possible utility range. Note that \u03bet \u2265 0 is a slack variable that captures noise in the\nfeedback.\nIn the following, we use a linear model for the utility function\nU (x, y) = w\u2217\u22a4 \u03c6(x, y),\n\u2217\n\nN\n\n(3)\nN\n\nwhere w \u2208 R is an unknown parameter vector and \u03c6 : X \u00d7 Y \u2192 R is a joint feature map such\nthat k\u03c6(x, y)k \u2264 R for any x \u2208 X and y \u2208 Y.\n\n3 Algorithm\nWe propose the algorithm in Figure 1 for the\nonline preference learning problem. It maintains a vector wt and predicts the object with\nthe highest utility according to wt in each iteration t. It then receives feedback \u0233t and updates wt in the direction \u03c6(xt , \u0233t )\u2212\u03c6(xt , yt ).\n\nInitialize w1 \u2190 0\nfor t = 1 to T do\nObserve xt\nPresent yt \u2190 argmaxy\u2208Y wt\u22a4 \u03c6(xt , y)\nObtain feedback \u0233t\nUpdate: wt+1 \u2190 wt + \u03c6(xt , \u0233t ) \u2212 \u03c6(xt , yt )\nend for\nFigure 1: Preference Perceptron.\n\nTheorem 1 Under \u03b1-informative feedback\nthe algorithm in Figure 1 has regret\n2Rkw\u2217 k\n1 X\n\u221a\n.\n\u03bet +\n\u03b1T t=1\n\u03b1 T\nT\n\nREGRETT \u2264\n\n(4)\n\nProof of the above theorem is provided in the Appendix A. When the user feedback is noise free,\nthe first term on the right hand\n\u221a side of the above bound vanishes. The average regret in this case\napproaches zero at the rate 1/ T . In addition to this result, we have the following extensions which\nwe cannot provide here due to space limitations:\n\u2022 It is possible to further weaken the requirement on the feedback. Instead of requiring \u03b1informative feedback, the user is required to give \u03b1-informative feedback in expectation.\nWe can show a result similar to that in Theorem 1 in this case.\n2\n\n\f\u2022 It is also possible to show that an algorithm different from Algorithm 1 can minimize any convex loss (under mild assumptions) defined on the utility difference\nw\u2217\u22a4 (\u03c6(xt , yt ) \u2212 \u03c6(xt , yt\u2217 )).\n\n4 Experiments\n1.4\n\n3\n\n\u03b1 = 0.1\n\u03b1 = 1.0\n\n1.2\n\navg. dcg* regret\n\navg. util regret\n\n1\n0.8\n0.6\n\n\u03b1 = 0.1\n\u03b1 = 1.0\n\n2.5\n\n2\n\n0.4\n0.2\n0 0\n10\n\n1.5\n1\n\n10\n\n2\n\n3\n\n10\n\n10\n\n4\n\n0\n\n10\n\n10\n\n1\n\n10\n\nt\n\n2\n\n3\n\n10\n\n10\n\n4\n\n10\n\nt\n\nFigure 2: Average regret versus time based on noise free \u03b1-informative feedback.\nWe applied our Preference Perceptron algorithm to the Yahoo! learning to rank dataset [4]. This\ndataset consists of query-url features (denoted as xqi for query q and URL i for that particular query)\nwith a relevance rating riq which ranges from zero (irrelevant) to four (perfectly relevant). We first\ncomputed the best least squares fit to the relevance labels from the features using the entire dataset\nand all the utilities in our experiment are reported with respect to this w\u2217 .\nTo pose ranking as a structured prediction problem, we defined our joint feature map as follows:\nw\u22a4 \u03c6(q, y) =\n\n5\nX\nw\u22a4 xqyi\nlog(i + 1)\ni=1\n\n(5)\n\nIn the above equation, y denotes a ranking. In particular, yi is the index of the URL which is placed\nat position i in the ranking. Thus, the above measure considers the top five URLs for a query q and\ncomputes a score based on a graded relevance. The above feature-map and utility are motivated from\nq\nP5\nry\ni\nthe definition: DCG@5(q, y) = i=1 log(i+1)\n. Effectively, our utility score (5) mimics DCG@5\nby replacing the relevance label with a linear prediction based on the features.\nFor query qt at time step t, the Preference Perceptron algorithm present the ranking ytq that maximizes wt\u22a4 \u03c6(q, y). Note that this merely amounts to sorting documents by the scores wt\u22a4 xqi t , which\ncan be done very efficiently. Once a ranking (yqt ) was presented to a user, the user returns a ranking\n\u0233qt . The exact nature of user feedback differed in the two experiments; the details of feedback can\nbe found below. Query ordering was randomly permuted twenty times and all the results reported\nare an average over the runs.\nThe utility regret in Eqn.\n(1), based on the definition of utility in (5), is given by\nPT\n1\nqt \u2217\n\u2217\u22a4\nqt \u2217\nqt\ndenotes the optimal ranking with respect to w\u2217 .\n(w\n\u03c6(q\n,\ny\n)\n\u2212\n\u03c6(q\n,\nt\nt y )). Here y\nt=1\nT\nWe also present our results on another quantity which we refer to as the DCG* regret. Since for\nevery query-URL pair there is a manual relevance judgment in the dataset, optimal DCG can be\ncomputed by sorting the relevance score. In DCG* regret, we measure the difference between the\nDCG of the optimal ranking and that of the rankings we present in each step.\n\u03b1-informative feedback The goal of the first experiment was to see how the regret of the algorithm changes with \u03b1, assuming \u03b1-informative feedback without noise. Once a ranking was presented, the feedback was obtained as follows: given a ranked list, the simulated user would go down\nthe list and would stop when she found five URL's such that, when they are placed at the top of\nthe list (in the order of their utilities), gave noise free \u03b1-informative feedback (i.e. \u03bet = 0) based\non w\u2217 . Figure 2 shows the results for this experiment for two different \u03b1 values. As expected, the\nregret with \u03b1 = 1.0 is lower compared to the regret with respect \u03b1 = 0.1. Note, however, that\nthe difference between the two curves is much smaller than a factor of ten. This is because, strictly\n\u03b1-informative feedback is also strictly \u03b2-informative feedback for any \u03b2 \u2264 \u03b1. So, there could be\n3\n\n\fseveral instances where user feedback was much stronger than what was required. Since the slack\nvariables are zero, the average utility regret approaches zero as expected.\n1.6\n\n3.2\n\nSVM\nPref. Perceptron\n\navg. dcg* regret\n\n1.4\navg. util regret\n\nSVM\nPref. Perceptron\n\n3\n\n1.2\n1\n0.8\n\n2.8\n2.6\n2.4\n2.2\n\n0.6\n0.4 0\n10\n\n2\n\n1\n\n2\n\n10\n\n3\n\n10\n\n10\n\n1.8 0\n10\n\n4\n\n10\n\nt\n\n1\n\n10\n\n2\n\n3\n\n10\n\n10\n\n4\n\n10\n\nt\n\nFigure 3: Regret versus time based on actual relevance labels.\nRelevance label feedback In this experiment, feedback was based on the actual relevance labels\nin the dataset as follows: given a ranking for a query, the user would go down the list inspecting the\ntop 25 (or all the URLs if the list is shorter) URLs. Five URL's with the highest relevance labels (riq )\nare placed at the top five locations in the user feedback. Note that this is a noisy version of feedback\nsince the linear fit cannot describe the labels exactly in this dataset.\nAs a baseline, a ranking SVM was trained repeatedly. In the first iteration, a random ranking was\npresented, the feedback ranking (as mentioned in the paragraph above) was obtained. An SVM\nwas trained based on the pair of examples ((q1 , yq1 ), (q1 , \u0233q1 )). From then on, a ranking was presented based on the prediction from the previously trained ranking SVM. The user always returned\na ranking based on the relevance labels as mentioned above; the pairs of examples were stored after\nevery iteration. Note that training a ranking SVM after each iteration would be prohibitive since it\ninvolves cross-validating a parameter C that trades-off between the margin and the slacks. Thus,\nwe trained an SVM whenever 10% more examples were added to the training set after the previous\ntraining. The value of the parameter C was obtained via a five-fold cross-validation.1 Once a C\nvalue was determined, SVM was trained on all the training examples available at that time and used\nit to predict rankings until the next training.\nResults of this experiment are presented in Figure 3. We have provided both the mean regret as well\nas one standard deviation for this experiment. Since the feedback is now based on relevance labels\n(and not on a linear fit), the utility regret converges to a non-zero value. It can also be noticed that\nour preference perceptron performs significantly better compared to the SVM. It might be possible\nto improve the performance of the SVM by training it more often. However, this would be extremely\nprohibitive. For instance, the perceptron algorithm took around 30 minutes to run (which was mostly\ninefficient Python IO), whereas the SVM version took about 20 hours (on the same machine).\n\n5 Conclusions\nWe proposed a new model of online learning with preferences that is especially suitable for implicit\nuser feedback. An efficient algorithm was proposed that provably minimizes regret. Experiments\nshowed its effectiveness for web-search ranking.\n\nReferences\n[1] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine\nLearning, 47(2-3):235\u2013256, 2002.\n[2] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire. The non-stochastic multi-armed bandit problem.\nSIAM Journal on Computing, 32(1):48\u201377, 2002.\n[3] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.\n[4] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. JMLR - Proceedings Track,\n14:1\u201324, 2011.\n1\n\nIt was fixed at 100 when there were less than 50 examples.\n\n4\n\n\f[5] A. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient\ndescent without a gradient. In SODA, 2005.\n[6] T. Joachims, L. Granka, Bing Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of\nimplicit feedback from clicks and query reformulations in web search. ACM Transactions on Information\nSystems (TOIS), 25(2), April 2007.\n[7] Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. In COLT, 2009.\n[8] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In ICML, 2009.\n[9] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML, 2003.\n\nA\n\nProof of theorem 1\n\nProof First, consider the inner product of wT +1 with itself. We have,\nwT\u22a4+1 wT +1 = wT\u22a4 wT + 2wT\u22a4 (\u03c6(xT , \u0233T ) \u2212 \u03c6(xT , yT ))\n\n+ (\u03c6(xT , \u0233T ) \u2212 \u03c6(xT , yT ))\u22a4 (\u03c6(xT , \u0233T ) \u2212 \u03c6(xT , yT ))\n\u2264 wT\u22a4 wT + 4R2\n\n\u2264 4R2 T.\n\nOn the first line, we simply used our update rule from algorithm 1. On the second line, we used\nthe fact that wT\u22a4 (\u03c6(xT , \u0233T ) \u2212 \u03c6(xT , yT )) \u2264 0 from the choice of yT in Algorithm 1 and that\nk\u03c6(xT , \u0233T ) \u2212 \u03c6(xT , yT )k2 \u2264 4R2 . We obtain the last line inductively.\nFurther, from the update rule in algorithm 1, we have,\nwT\u22a4+1 w\u2217 = wT\u22a4 w\u2217 + w\u2217\u22a4 (\u03c6(xT , \u0233T ) \u2212 \u03c6(xT , yT ))\n=\n\nT\nX\n\n=\n\nT\nX\n\nt=1\n\nt=1\n\nw\u2217\u22a4 (\u03c6(xt , \u0233t ) \u2212 \u03c6(xt , yt ))\n(U (xt , \u0233t ) \u2212 U (xt , yt )) .\n\nWe now use the fact that wT\u22a4+1 w\u2217 \u2264 kw\u2217 kkwT +1 k (Cauchy-Schwarz inequality) which implies,\nT\nX\nt=1\n\n\u221a\n(U (xt , \u0233t ) \u2212 U (xt , yt )) \u2264 2R T kw\u2217 k.\n\nThe above inequality, along with the \u03b1-informative feedback (Eqn. (2)) gives,\n\u03b1\n\nT\nX\nt=1\n\n(U (xt , yt\u2217 ) \u2212 U (xt , yt )) \u2212\n\nfrom which the claimed result follows.\n\n5\n\nT\nX\nt=1\n\n\u221a\n\u03bet \u2264 2R T kw\u2217 k.\n\n\f"}