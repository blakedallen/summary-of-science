{"id": "http://arxiv.org/abs/1002.0832v1", "guidislink": true, "updated": "2010-02-03T20:42:49Z", "updated_parsed": [2010, 2, 3, 20, 42, 49, 2, 34, 0], "published": "2010-02-03T20:42:49Z", "published_parsed": [2010, 2, 3, 20, 42, 49, 2, 34, 0], "title": "K-Dimensional Coding Schemes in Hilbert Spaces", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.4045%2C1002.0587%2C1002.4261%2C1002.4764%2C1002.1263%2C1002.0225%2C1002.1614%2C1002.2865%2C1002.1108%2C1002.4735%2C1002.0639%2C1002.4310%2C1002.2576%2C1002.1039%2C1002.2372%2C1002.1833%2C1002.0381%2C1002.3079%2C1002.3724%2C1002.2944%2C1002.3378%2C1002.1787%2C1002.4334%2C1002.4550%2C1002.0536%2C1002.4360%2C1002.4370%2C1002.1239%2C1002.3303%2C1002.2983%2C1002.0099%2C1002.2158%2C1002.1441%2C1002.1187%2C1002.0197%2C1002.0889%2C1002.3970%2C1002.0832%2C1002.3613%2C1002.0622%2C1002.0217%2C1002.0220%2C1002.0206%2C1002.5000%2C1002.0690%2C1002.3088%2C1002.2463%2C1002.1487%2C1002.4103%2C1002.3184%2C1002.0966%2C1002.3031%2C1002.3444%2C1002.3987%2C1002.3287%2C1002.3167%2C1002.0091%2C1002.3629%2C1002.3390%2C1002.0654%2C1002.2151%2C1002.0195%2C1002.1788%2C1002.0802%2C1002.1284%2C1002.0218%2C1002.2478%2C1002.3050%2C1002.2526%2C1002.3782%2C1002.2266%2C1002.0227%2C1002.2678%2C1002.4621%2C1002.3524%2C1002.1251%2C1002.3840%2C1002.3077%2C1002.4728%2C1002.2134%2C1002.4634%2C1002.3920%2C1002.1493%2C1002.3280%2C1002.0870%2C1002.3317%2C1002.4438%2C1002.4992%2C1002.4282%2C1002.4487%2C1002.0530%2C1002.1264%2C1002.3726%2C1002.0290%2C1002.2381%2C1002.3937%2C1002.1303%2C1002.2900%2C1002.4989%2C1002.1819%2C1002.3677&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "K-Dimensional Coding Schemes in Hilbert Spaces"}, "summary": "This paper presents a general coding method where data in a Hilbert space are\nrepresented by finite dimensional coding vectors. The method is based on\nempirical risk minimization within a certain class of linear operators, which\nmap the set of coding vectors to the Hilbert space. Two results bounding the\nexpected reconstruction error of the method are derived, which highlight the\nrole played by the codebook and the class of linear operators. The results are\nspecialized to some cases of practical importance, including K-means\nclustering, nonnegative matrix factorization and other sparse coding methods.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1002.4045%2C1002.0587%2C1002.4261%2C1002.4764%2C1002.1263%2C1002.0225%2C1002.1614%2C1002.2865%2C1002.1108%2C1002.4735%2C1002.0639%2C1002.4310%2C1002.2576%2C1002.1039%2C1002.2372%2C1002.1833%2C1002.0381%2C1002.3079%2C1002.3724%2C1002.2944%2C1002.3378%2C1002.1787%2C1002.4334%2C1002.4550%2C1002.0536%2C1002.4360%2C1002.4370%2C1002.1239%2C1002.3303%2C1002.2983%2C1002.0099%2C1002.2158%2C1002.1441%2C1002.1187%2C1002.0197%2C1002.0889%2C1002.3970%2C1002.0832%2C1002.3613%2C1002.0622%2C1002.0217%2C1002.0220%2C1002.0206%2C1002.5000%2C1002.0690%2C1002.3088%2C1002.2463%2C1002.1487%2C1002.4103%2C1002.3184%2C1002.0966%2C1002.3031%2C1002.3444%2C1002.3987%2C1002.3287%2C1002.3167%2C1002.0091%2C1002.3629%2C1002.3390%2C1002.0654%2C1002.2151%2C1002.0195%2C1002.1788%2C1002.0802%2C1002.1284%2C1002.0218%2C1002.2478%2C1002.3050%2C1002.2526%2C1002.3782%2C1002.2266%2C1002.0227%2C1002.2678%2C1002.4621%2C1002.3524%2C1002.1251%2C1002.3840%2C1002.3077%2C1002.4728%2C1002.2134%2C1002.4634%2C1002.3920%2C1002.1493%2C1002.3280%2C1002.0870%2C1002.3317%2C1002.4438%2C1002.4992%2C1002.4282%2C1002.4487%2C1002.0530%2C1002.1264%2C1002.3726%2C1002.0290%2C1002.2381%2C1002.3937%2C1002.1303%2C1002.2900%2C1002.4989%2C1002.1819%2C1002.3677&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "This paper presents a general coding method where data in a Hilbert space are\nrepresented by finite dimensional coding vectors. The method is based on\nempirical risk minimization within a certain class of linear operators, which\nmap the set of coding vectors to the Hilbert space. Two results bounding the\nexpected reconstruction error of the method are derived, which highlight the\nrole played by the codebook and the class of linear operators. The results are\nspecialized to some cases of practical importance, including K-means\nclustering, nonnegative matrix factorization and other sparse coding methods."}, "authors": ["Andreas Maurer Massimiliano Pontil"], "author_detail": {"name": "Andreas Maurer Massimiliano Pontil"}, "author": "Andreas Maurer Massimiliano Pontil", "arxiv_comment": "17 pages", "links": [{"href": "http://arxiv.org/abs/1002.0832v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1002.0832v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1002.0832v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1002.0832v1", "journal_reference": "IEEE Transactions on Information Theory, 56(11): 5839-5846, 2010", "doi": null, "fulltext": "K-Dimensional Coding Schemes in Hilbert\nSpaces\nAndreas Maurer1 and Massimiliano Pontil2\n\narXiv:1002.0832v1 [stat.ML] 3 Feb 2010\n\n1\n\nAdalbertstrasse 55\nD-80799 M\u00fcnchen, Germany\nam@andreas-maurer.eu\n2\nDept. of Computer Science\nUniversity College London\nMalet Pl., WC1E, London, UK\nm.pontil@cs.ucl.ac.uk\n\nAbstract. This paper presents a general coding method where data in\na Hilbert space are represented by finite dimensional coding vectors. The\nmethod is based on empirical risk minimization within a certain class of\nlinear operators, which map the set of coding vectors to the Hilbert space.\nTwo results bounding the expected reconstruction error of the method\nare derived, which highlight the role played by the codebook and the\nclass of linear operators. The results are specialized to some cases of\npractical importance, including K-means clustering, nonnegative matrix\nfactorization and other sparse coding methods.\n\nIndex Terms: Empirical risk minimization, estimation bounds, K-means clustering and vector quantization, statistical learning.\n\n1\n\nIntroduction\n\nWe study a general class of K-dimensional coding methods for data drawn from\na distribution \u03bc on the unit ball of a Hilbert space H. These methods encode a\ndata point x \u223c \u03bc as a vector \u0177 \u2208 RK , according to the formula\n2\n\n\u0177 = arg min kx \u2212 T yk ,\ny\u2208Y\n\nwhere Y \u2286 RK is a prescribed set of codes (called the codebook ), which we can\nalways assume to span RK , and T : RK \u2192 H is a linear map, which defines a\nparticular implementation of the codebook. It embeds the codebook Y in H and\nyields the set T (Y ) of exactly codable patterns. If \u0177 is the code found for x then\nx\u0302 = T \u0177 is the reconstructed data point. The quantity\n2\n\nfT (x) = min kx \u2212 T yk\ny\u2208Y\n\nis called the reconstruction error.\n\n\f2\n\nGiven a codebook Y and a finite number of independent observations x1 , . . . , xm \u223c\n\u03bc, a common sense approach searches for an implementation T\u0302 which is optimal\non average over the observed points, that is\nm\n\nT\u0302 = arg min\nT \u2208T\n\n1 X\nfT (xi ) ,\nm i=1\n\n(1)\n\nwhere T denotes some class of linear maps T : RK \u2192 H. As we shall see, this\nframework is general enough to include principal component analysis, K-means\nclustering, non-negative matrix factorization [10] and the sparse coding method\nas proposed in [14].\nWhenever the codebook Y is compact and T is bounded in the operator\nnorm this approach is justified by the following high-probability, uniform bound\non the expected reconstruction error.\nTheorem 1. Suppose that Y is a closed subset of the unit ball of RK , that there\nis c \u2265 1 such that kT k\u221e \u2264 c for all T \u2208 T and that \u03b4 \u2208 (0, 1). Then with\nprobability at least 1 \u2212 \u03b4 in the observed data x1 , . . . , xm \u223c \u03bc we have for every\nT \u2208 T that\nr\nr\nm\n\u03c0\n8 ln 1/\u03b4\n1 X\n2 2\n2\nEx\u223c\u03bc fT (x) \u2212\nfT (xi ) \u2264 6c K\n+c\n.\nm i=1\nm\nm\nThe bound is two-sided in the sense that also with probability at least 1 \u2212 \u03b4 we\nhave for every T \u2208 T that\nr\nr\nm\n\u03c0\n8 ln 1/\u03b4\n1 X\n2 2\n2\nfT (xi ) \u2212 Ex\u223c\u03bc fT (x) \u2264 6c K\n+c\n.\nm i=1\nm\nm\nAny compact subset of RK can of course be down-scaled to be contained in\nthe unit ball, and the scaling factor can be absorbed in c, so that the above\nresult is applicable to any compact codebook.\nThe theorem implies a bound on the excess risk: let T0 \u2208 T be a minimizer of\nthe expected reconstruction error within the set T . It follows from the definition\nof T\u0302 and the above result that the expected\nreconstruction error of T\u0302 is with\n\u221a\nhigh probability not more than O (1/ m) worse than that of T0 .\nThis order in m is optimal, as we know from existing lower bounds for Kmeans clustering [3]. The above dependence on K is, however, generally not\noptimal, and can be considerably improved with apmore careful analysis, if we\nare prepared to accept the slightly inferior rate of ln m/m in the sample size.\nTo state this improvement define\nkT kY = sup kT kY = sup sup kT yk .\nT \u2208T\n\nWe then have the following result.\n\nT \u2208T y\u2208Y\n\n\f3\n\nTheorem 2. Assume that kT kY \u2265 1 and that the functions fT for T \u2208 T , when\nrestricted to the unit ball of H, have range contained in [0, b]. Fix \u03b4 > 0.\nThen with probability at least 1 \u2212 \u03b4 in the observed data x1 , . . . , xm \u223c \u03bc we\nhave for every T \u2208 T that\n! r\nr \u0010\nm\n\u0011\nK\nb\n1 X\nln 1/\u03b4\n2\nln 16m kT kY\n+b\nfT (xi ) \u2264 \u221a\n.\n14 kT kY +\nEx\u223c\u03bc fT (x)\u2212\nm i=1\n2\n2m\nm\nThe bound is two sided in the same sense as the previous result.\nBoth results immediately imply uniform convergence in probability. We are\nnot aware of other results for nonnegative matrix factorization [10] or the sparse\ncoding techniques as in [14].\nBefore proving our results, we will illustrate their implications in some cases\nof interest. It turns out that the dependence on K in Theorem 2 adapts to the\nspecific situation under consideration.\nA preliminary version of this paper appeared in the proceedings of the 2008\nAlgorithmic Learning Theory Conference [12]. The new version contains Theorem 1 and a simplified proof of Theorem 2 with improved constants.\n\n2\n\nExamples of coding schemes\n\nSeveral coding schemes can be expressed in our framework. We describe some of\nthese methods and how our result applies.\n2.1\n\nPrincipal component analysis\n\nPrincipal component analysis (PCA) seeks a K-dimensional orthogonal projection which maximizes the projected variance and then uses this projection to\nencode future data. A projection P can be expressed as T T \u2217 where T is an\nisometry which maps RK to the range of P . Since\n2\n\n2\n\n2\n\nkP xk = kxk \u2212 kx \u2212 P xk2 = kxk2 \u2212 min kx \u2212 T yk\ny\u2208RK\n\nfinding P to maximize the true or empirical expectation of kP xk2 is equivalent to\n2\nfinding T to minimize the corresponding expectation of miny\u2208RK kx \u2212 T yk . We\nsee that PCA is described by our framework upon the identifications Y = RK\nand T is restricted to the class of isometries T : RK \u2192 H. Given T \u2208 T and\nx \u2208 H the reconstruction error is\nfT (x) = min kx \u2212 T yk2 .\ny\u2208RK\n\nIf the data are constrained to be in the unit ball of H, as we generally assume,\nthen it is easily seen that we can take Y to be the unit ball of RK without\nchanging any of the encodings. We can therefore apply Theorem 2 with kT kY = 1\n\n\f4\n\nand b = 1. This is besides the point however, because in the simple case of PCA\nmuch better bounds are available (see [15], [19] and Lemma 6 below). In [19] local\nRademacher averages are used to give faster rates under certain circumstances.\nAn objection to PCA is, that generic codes have K nonzero components,\nwhile for practical and theoretical reasons sparse codes with much less than K\nnonzero components may be preferable [14].\n2.2\n\nK-means clustering or vector quantization\n\nHere Y = {e1 , . . . , eK }, where the vectors ek form an orthonormal basis of\nRK . An implementation T now defines a set of centers {T e1, . . . , T eK }, the\n2\nreconstruction error is minK\nk=1 kx \u2212 T ek k and a data point x is coded by the ek\nsuch that T ek is nearest to x. The algorithm (1) becomes\nm\n\nT\u0302 = arg min\nT \u2208T\n\n1 X K\n2\nmin kxi \u2212 T ek k .\nm i=1 k=1\n\nIt is clear that every center T ek has at most unit norm, so that kT kY = 1. Since\n2\nall data points are in the unit ball we have kx \u2212 T ek k \u2264 4 so we can set b = 4\nand the bound in Theorem 2 becomes\nr\n\u0010\n\u0011 K\np\n8 ln (1/\u03b4)\n14 + 2 ln (16m) \u221a +\n.\nm\nm\n\u221a\nThe order of this bound matches up to ln m the order given in [4] or [16].\nTo illustrate our method we will also prove the bound\nr\n\u221a\n8 ln (1/\u03b4)\nK\n18\u03c0 \u221a +\nm\nm\n(Theorem 6), which is\npessentially the same as those in [4] or [16]. There is a\nlower bound of order K/m in [3], and it is unknown which of the two bounds\n(upper or lower) is tight.\nIn K-means clustering every code has only one nonzero component, so that\nsparsity is enforced in a maximal way. On the other hand this results in a weaker\napproximation capability of the coding scheme.\n2.3\n\nNonnegative matrix factorization\n\nHere Y is the positive orthant in RK , that is the cone\nY = {y : y = (y1 , . . . , yK ), yk \u2265 0, 1 \u2264 k \u2264 K} .\nA chosen map T generates a cone T (Y ) \u2282 H onto which incoming data is\nprojected. In the original formulation by Lee and Seung [10] it is postulated\nthat both the data and the vectors T ek be contained in the positive orthant\n\n\f5\n\nof some finite dimensional space, but we can drop most of these restrictions,\nkeeping only the requirement that hT ek , T el i \u2265 0 for 1 \u2264 k, l \u2264 K.\nNo coding will change if we require that kT ek k = 1 for all 1 \u2264 k \u2264 K by a\nsuitable normalization. The set T is then given by\nT = {T : T \u2208 L(RK , H), kT ek k = 1, hT ek , T el i \u2265 0, 1 \u2264 k, l \u2264 K}.\nWe can restrict Y to its intersection\nwith the unit ball in RK (see Lemma 2\n\u221a\nbelow). We obtain that kT kY = K. Hence, Theorem 2 yields the bound\nK\n\u221a\nm\n\n\u0013 r\n\u0012\n\u221a\n1p\nln (1/\u03b4)\nln (16mK) +\n14 K +\n2\n2m\n\non the estimation error. We do not know of any other generalization bounds for\nthis coding scheme.\nNonnegative matrix factorization appears to encourage sparsity, but cases\nhave been reported where sparsity was not observed [11]. In fact this undesirable behavior should be generic for exactly codable data. Various authors have\ntherefore proposed additional constraints ([11], [7]). It is clear that additional\nconstraints on T can only improve estimation and that the passage from Y to a\nsubset can only improve our bounds, because the quantity kT kY would decrease.\n2.4\n\nSparse coding\n\nAnother method arises by choosing the lp -unit ball as a codebook. Let Y = {y :\ny \u2208 RK , kykp \u2264 1} and T = {T : RK \u2192 H : kT ek k \u2264 1, 1 \u2264 k \u2264 K}. We have\nkT yk = k\n\nK\nX\n\nk=1\n\nyk T ek k \u2264\n\nK\nX\n\nk=1\n\n|yk |kT ek k \u2264\n\nK\nX\n\nk=1\n\nkT ek k\n\nq\n\n!1/q\n\n\u2264 K 1/q = K 1\u22121/p\n\nimplying that kT kY \u2264 K 1\u22121/p .\nBy the same argument as above all the fT have range contained in [0, 1], so\nTheorem 2 can be applied with b = 1 to yield the bound\n\u0012\n\u0013 r\nq\n\u0001\n1\nK\nln (1/\u03b4)\n1\u22121/p\n\u221a\n14K\n+\nln 16mK 2\u22122/p +\n2\n2m\nm\n\non the estimation error. The best bound is obtained when p = 1, and the order\nin K matches that of the bound for K-means clustering described earlier.\nThe method for p = 1 is similar to the sparse-coding method proposed by\nOlshausen and Field [14], with the difference that the term kyk1 is used as a\npenalty term instead of the hard constraint kyk1 \u2264 1. The method of Olshausen\nand Field [14] approximates with a compromise of geometric proximity and sparsity and our result asserts that the observed value of this compromise generalizes\nto unseen data if enough data have been observed.\n\n\f6\n\n3\n\nProofs\n\nWe first introduce some notation, conventions and auxiliary results. Then we set\nabout to prove Theorems 1 and 2.\n3.1\n\nNotation, definitions and auxiliary results\n\nThroughout H denotes a Hilbert space. The term norm and the notation k*k\nand h*, *i always refer to the Euclidean norm and inner product on RK or on H.\nOther norms are characterized by subscripts. If H1 and H2 are any Hilbert spaces\nL (H1 , H2 ) denotes the vector space of bounded linear transformations from H1\nto H2 . If H1 = H2 we just write L (H1 ) = L (H1 , H1 ). With U (H1 , H2 ) we denote\nthe set of isometries in L (H1 , H2 ), that is maps U satisfying kU xkH2 = kxkH1\nfor all x \u2208 H1 .\nWe use L2 (H) for the set of Hilbert-Schmidt operators on H, which becomes itself a Hilbert space with the inner product hT, Si2 =tr(T \u2217 S) and the\ncorresponding (Frobenius) norm k*k2 .\nFor x \u2208 H the rank-one operator Qx is defined by Qx z = hz, xi x. For any\nT \u2208 L2 (H) the identity\nhT \u2217 T, Qx i2 = kT xk2\nis easily verified.\nSuppose that Y \u2286 RK spans RK . It is easily verified that the quantity\nkT kY = sup kT yk\ny\u2208Y\n\n\u0001\ndefines a norm on L RK , H .\nWe use the following well known result on covering numbers (see, for example,\nProposition 5 in [5]).\nProposition 1. Let B be a ball of radius r in an N -dimensional Banach space\nN\nand \u01eb > 0. There exists a subset B\u01eb \u2282 B such that |B\u01eb | \u2264 (4r/\u01eb) and \u2200z \u2208\n\u2032\n\u2032\nB, \u2203z \u2208 B\u01eb with d(z, z ) \u2264 \u01eb, where d is the metric of the Banach space.\nThe following concentration inequality, known as the bounded difference inequality [13], goes back to the work of Hoeffding [6].\nTheoremQ3. Let \u03bci be a probability measure on a space Xi , for i = 1, . . . , m.\nm\nLet X = i=1 Xi and \u03bc = \u2297m\ni=1 \u03bci be the product space and product measure\nrespectively. Suppose the function \u03a8 : X \u2192 R satisfies\n|\u03a8 (x) \u2212 \u03a8 (x\u2032 )| \u2264 ci\nwhenever x and x\u2032 \u2208 X differ only in the i-th coordinate, where c1 , . . . , cm are\nsome positive parameters. Then\n\u0013\n\u0012\n\u22122t2\n\u2032\nPr {\u03a8 (x) \u2212 Ex\u2032 \u223c\u03bc \u03a8 (x ) \u2265 t} \u2264 exp Pm 2 .\nx\u223c\u03bc\ni=1 ci\n\n\f7\n\nThroughout \u03c3 i will denote a sequence of mutually independent random variables, uniformly distributed on {\u22121, 1} and \u03b3 i , \u03b3 ij will be (multiple indexed)\nsequences of mutually independent Gaussian random variables, with zero mean\nand unit standard deviation.\nIf F is a class of real-valued functions on a space X and \u03bc a probability\nmeasure on X then for m \u2208 N the Rademacher and Gaussian complexities of F\nw.r.t. \u03bc are defined ([9],[2]) as\nm\n\nRm (F , \u03bc) =\n\u0393m (F , \u03bc) =\n\nX\n2\n\u03c3 i f (xi ) ,\nEx\u223c\u03bcm E\u03c3 sup\nm\nf \u2208F i=1\n2\nE\nm\n\nx\u223c\u03bcm\n\nE\u03b3 sup\n\nm\nX\n\nf \u2208F i=1\n\n\u03b3 i f (xi )\n\nrespectively.\nAppropriately scaled Gaussian complexities can be substituted for Rademacher\ncomplexities, by virtue of the next Lemma. For a proof see, for example, [9, p.\n97].\np\nLemma 1. For Y \u2286 Rk we have R (Y ) \u2264 \u03c0/2 \u0393 (Y ).\nThe next result is known as Slepian's lemma ([17], [9]).\n\nTheorem 4. Let \u03a9 and \u039e be mean zero, separable Gaussian processes indexed\nby a common set S, such that\nE (\u03a9s1 \u2212 \u03a9s2 )2 \u2264 E (\u039es1 \u2212 \u039es2 )2 for all s1 , s2 \u2208 S.\nThen\nE sup \u03a9s \u2264 E sup \u039es .\ns\u2208S\n\ns\u2208S\n\nThe following result, which generalizes Theorem 8 in [2], plays a central role\nin our proof.\nTheorem 5. Let {Fn : 1 \u2264 n \u2264 N } be a finite collection of [0, b]-valued function\nclasses on a space X , and \u03bc a probability measure on X . Then \u2200\u03b4 \u2208 (0, 1) we\nhave with probability at least 1 \u2212 \u03b4 that\n#\n\"\nr\nm\nln N + ln (1/\u03b4)\n1 X\nf (xi ) \u2264 max Rm (Fn , \u03bc) + b\n.\nmax sup Ex\u223c\u03bc f (x) \u2212\nn\u2264N\nn\u2264N f \u2208Fn\nm i=1\n2m\nProof. Denote with \u03a8n the function on X m defined by\n\"\n#\nm\n1 X\n\u03a8n (x) = sup Ex\u223c\u03bc f (x) \u2212\nf (xi ) , x \u2208 X m .\nm i=1\nf \u2208Fn\nBy standard symmetrization (see, for example, [18]) we have Ex\u223c\u03bcm \u03a8n (x) \u2264\nRm (Fn , \u03bc) \u2264 maxn\u2264N Rm (Fn , \u03bc). Modifying one of the xi can change the value\n\n\f8\n\nof any \u03a8n (x) by at most b/m, so that by a union bound and the bounded\ndifference inequality (Theorem 3)\n\u001a\n\u001b X\n2\nPr max \u03a8n > max Rm (Fn , \u03bc) + t \u2264\nPr {\u03a8n > E\u03a8n + t} \u2264 N e\u22122m(t/b) .\nn\u2264N\n\nn\u2264N\n\nn\n\n2\n\nSolving \u03b4 = N e\u22122m(t/b) for t gives the result.\n\n\u2293\n\u2294\n\nNotice that replacing the functions f \u2208 Fn by b \u2212 f does not affect the\nRademacher complexities, so the above result can be used in a two-sided way.\nThe following lemma was used in Section 2.3.\nLemma 2. Suppose kxk \u2264 1, kck k = 1, hck , cl i \u2265 0, y \u2208 RK , yi \u2265 0. If y\nminimizes\n2\nK\nX\nh (y) = x \u2212\ny k ck ,\nk=1\n\nthen kyk \u2264 1.\nProof. Assume that y is a minimizer of h and kyk > 1.Then\nK\nX\n\n2\n2\n\ny k ck\n\n= kyk +\n\nk=1\n\nX\nk6=l\n\nyk yl hck , cl i > 1.\n\nLet the real-valued function f be defined by f (t) = h (ty). Then\n\uf8eb\n+\uf8f6\n* K\n2\nK\nX\nX\nf \u2032 (1) = 2 \uf8ed\nyk ck \u2212 x,\ny k ck \uf8f8\nk=1\n\n\uf8eb\n\n\u2265 2\uf8ed\n=2\n\nK\nX\n\ny k ck\n\nk=1\nK\nX\n\nk=1\n\n> 0.\n\nk=1\n\n2\n\n\u2212\n\ny k ck \u2212 1\n\nK\nX\n\nk=1\n\n!\n\n\uf8f6\n\ny k ck \uf8f8\n\nK\nX\n\ny k ck\n\nk=1\n\nSo f cannot have a minimum at 1, whence y cannot be a minimizer of h.\n3.2\n\n\u2293\n\u2294\n\nProof of the main results\n\n\u0001\nWe now fix a spanning codebook Y \u2286 RK and recall that, for T \u2208 L RK , H ,\nwe had introduced the notation\n2\n\nfT (x) = inf kx \u2212 T yk , x \u2208 H.\ny\u2208Y\n\n\f9\n\nOur principal object of study is the function class\nF = {fT : T \u2208 T } ,\nwhere T \u2282 L RK , H is some fixed set of candidate implementations of our\ncoding scheme. We first address the rather general Theorem 1 which can be\ntreated in parallel to the case of K-means clustering. We begin with a technical\nlemma.\n\u0001\n\nLemma 3. Suppose that\n1.\n2.\n3.\n4.\n\n(ek : 1 \u2264 k \u2264 K) is an orthonormal basis of RK ;\nT is the class of linear operators T : RK \u2192 H with kT ek k \u2264 c;\n(xi : 1 \u2264 i \u2264 m) is a sequence xi \u2208 H, kxi k \u2264 1;\n(\u03b3 ik : 1 \u2264 i \u2264 m, 1 \u2264 k \u2264 K) and (\u03b3 ikl : 1 \u2264 i \u2264 m, 1 \u2264 k, l \u2264 K) are orthogaussian sequences.\nThen the following three inequalities hold\nE\u03b3 sup\n\nK\nm X\nX\n\nT \u2208T i=1\nk=1\n\nE\u03b3 sup\n\n\u221a\n\u03b3 ik hxi , T ek i \u2264 cK m\n\nm X\nK\nX\n\nT \u2208T i=1\nk=1\n\nE\u03b3 sup\n\nm X\nK\nX\n\nT \u2208T i=1\nk,l=1\n\n\u221a\n2\n\u03b3 ik kT ek k \u2264 c2 K m\n\n\u221a\n\u03b3 ikl hT ek , T el i \u2264 c2 K 2 m.\n\nProof. Using Cauchy-Schwarz' and Jensen's inequalities and the orthogaussian\nproperties of the \u03b3 ik , we get\nE\u03b3 sup\nT \u2208T\n\nK X\nm\nX\nk=1 i=1\n\n\u03b3 ik hxi , T ek i \u2264 cE\u03b3\n\nm\nK\nX\nX\nk=1\n\ni=1\n\n\u221a\n\u03b3 ik xi \u2264 cK m\n\nwhich is the first inequality. Similarly we obtain\nE\u03b3 sup\nE\u03b3 sup\nT \u2208T\n\nK X\nm\nX\n\nT \u2208T\n\nk=1 i=1\n\nK\nX\n\nm\nX\n\nk,l=1 i=1\n\n\u03b3 ik kT ek k2 \u2264 c2 E\u03b3\n\n\u03b3 ikl hT ek , T el i \u2264 c2 E\u03b3\n\nK X\nm\nX\n\nk=1 i=1\n\n\u221a\n\u03b3 ik \u2264 c2 K m\n\nK\nm\nX\nX\n\nk,l=1 i=1\n\n\u221a\n\u03b3 ikl \u2264 c2 K 2 m.\n\u2293\n\u2294\n\nProposition 2. Suppose that the probability measure \u03bc is supported on the unit\nball of H, that {ek : 1 \u2264 k \u2264 K} is an orthonormal basis of RK and that T is\n\n\f10\n\na class of linear operators T : RK \u2192 H with kT ek k \u2264 c for 1 \u2264 k \u2264 K, with\nc \u2265 1. Let Y be a nonempty closed subset of the unit ball in RK and\nFY =\n\n\u001a\n\u001b\n2\nx \u2208 H 7\u2192 min kx \u2212 T yk : T \u2208 T .\ny\u2208Y\n\nThen\nR (FY , \u03bc) \u2264 6c2 K 2\n\nr\n\n\u03c0\n.\nm\n\nand if Y = {ek : 1 \u2264 k \u2264 K} then the bound improves to\n2\n\nR (FY , \u03bc) \u2264 c K\n\nr\n\n18\u03c0\n.\nm\n\nProof. By Lemma 1 it suffices to bound the corresponding Gaussian averages,\nwhich we shall do using Slepian's Lemma (Theorem 4). First fix a sample x and\ndefine Gaussian processes \u03a9 and \u039e indexed by T\n\u03a9T =\n\nX\ni\n\n\u03b3 i min kxi \u2212 T yk2 and\ny\n\n\u221a X\n\u221a X\n\u03b3 ik hxi , T ek i + 2\n\u03b3 ilk hT el , T ek i .\n\u039eT = 8\nik\n\nilk\n\nSuppose T1 , T2 \u2208 T . For any x \u2208 H we have, using (a + b)2 \u2264 2a2 + 2b2 and\nCauchy-Schwarz\n\u00132\n\u0012\n2\n2\nmin kx \u2212 T1 yk \u2212 min kx \u2212 T2 yk\ny\n\ny\u2208Y\n\n\u00132\n\u0012\n2\n2\n\u2264 max kx \u2212 T1 yk \u2212 kx \u2212 T2 yk\ny\u2208Y\n\n\u2264 8 max\ny\u2208Y\n\n\u22648\n\nX\nk\n\nX\nk\n\nyk hx, (T1 \u2212 T2 ) ek i\n\n!2\n\n2\n\n(hx, T1 ek i \u2212 hx, T2 ek i) + 2\n\n+ 2 max\ny\u2208Y\n\nX\nkl\n\nX\nkl\n\nyk yl hek , (T1\u2217 T1\n\n\u2212\n\nT2\u2217 T2 ) el i\n\n!2\n\n2\n\n(hT1 ek , T1 el i \u2212 hT2 ek , T2 el i) .\n\nWe therefore have\nE (\u03a9T1\n\n\u00132\nX\u0012\n2\n2\nmin kxi \u2212 T1 yk \u2212 min kxi \u2212 T2 yk\n\u2212 \u03a9T2 ) =\n2\n\ny\n\ni\n\n\u22648\n\nX\nik\n\ny\n\n2\n\n(hxi , T1 ek i \u2212 hxi , T2 ek i) + 2\n\n= E (\u039eT1 \u2212 \u039eT2 )2 .\n\nX\nikl\n\n2\n\n(hT1 ek , T1 el i \u2212 hT2 ek , T2 el i)\n\n\f11\n\nSo, by Slepian's Lemma and the first and last inequalities in Lemma 3\nE sup \u03a9T \u2264 E sup \u039eT\nT \u2208T\n\nT \u2208T\n\nX\nX\n\u221a\n\u221a\n\u03b3 ik hxi , T ek i + 2E sup\n\u03b3 ilk hT el , T ek i\n\u2264 8E sup\nT \u2208T\n\nT \u2208T\n\nik\n\n\u221a\n\u221a\n\u2264 cK 8m + c2 K 2 2m.\n\nMultiply by\n\nilk\n\n\u221a\n2\u03c0/m to get a bound on the Rademacher complexity of\nr\nr\nr\n\u03c0\n\u03c0\n\u03c0\n2 2\n2 2\nR (FY , \u03bc) \u2264 4cK\n+ 2c K\n\u2264 6c K\n.\nm\nm\nm\n\nTo obtain the second conclusion we improve the bound on the Gaussian average.\nWith \u03a9T as above we set\n\u039eT =\n\nm X\nK\nX\ni=1 k=1\n\n2\n\n\u03b3 ik kxi \u2212 T ek k .\n\nNow we have for T1 , T2 \u2208 T that\nE (\u03a9T1\n\n\u00132\nm \u0012\nX\nK\nK\n2\n2\nmin kxi \u2212 T1 ek k \u2212 min kxi \u2212 T2 ek k\n\u2212 \u03a9T2 ) =\n2\n\n\u2264\n\u2264\n\ni=1\nm\nX\ni=1\n\nk=1\n\nk=1\n\n\u00112\n\u0010\nK\n2\n2\nmax kxi \u2212 T1 ek k \u2212 kxi \u2212 T2 ek k\nk=1\n\nm X\nK \u0010\nX\ni=1 k=1\n\n2\n\n2\n\nkxi \u2212 T1 ek k \u2212 kxi \u2212 T2 ek k\n2\n\n\u00112\n\n= E (\u039eT1 \u2212 \u039eT2 ) .\nAgain with Slepian's Lemma and the triangle inequality\nE\u03b3 sup \u03a9T \u2264 E\u03b3 sup \u039eT = E\u03b3 sup\nT \u2208T\n\nT \u2208T\n\n\u2264 2E\u03b3 sup\n\nm X\nK\nX\n\nT \u2208T i=1\nk=1\n\nm X\nK\nX\n\nT \u2208T i=1\nk=1\n\n\u221a\n\u2264 3c2 K m,\n\n2\n\n\u03b3 ik kxi \u2212 T ek k\n\n\u03b3 ik hxi , T ek i + E\u03b3 sup\n\nm X\nK\nX\n\nT \u2208T i=1\nk=1\n\n2\n\n\u03b3 ik kT ek k\n\nwhere the last\n\u221a inequality follows from the first two inequalities in Lemma 3.\nMultiply by 2\u03c0/m as above\n\u2293\n\u2294\n\u0002\n\u0003\nTheorem 1 follows from observing that the functions in F map to 0, 4c2\nand combining the above bound on the Rademacher complexity with Theorem\n5 with N = 1 and b = 4.\n\n\f12\n\nThe second conclusion of the proposition yields a bound for K-means clustering, corresponding to the choices Y = {e1 , . . . , eK } and T = {T : kT ek k \u2264 1, 1 \u2264 k \u2264 K}.\nAs already noted in Section 2.2 the vectors T ek define the cluster centers. With\nTheorem 5 we obtain\nTheorem 6. For every \u03b4 > 0 with probability greater 1\u2212\u03b4 in the sample x \u223c \u03bcm\nwe have for all T \u2208 T\nr\nr\nm\nK\n18\u03c0\n8 ln (1/\u03b4)\n1 X K\n2\n2\nmin kxi \u2212 T ek k + K\n+\n.\nEx\u223c\u03bc min kx \u2212 T ek k \u2264\nk=1\nm i=1 k=1\nm\nm\n\nTo prove Theorem 2 a more subtle approach is necessary. The idea is the\nfollowing: every implementing \u0001map T \u2208 T can be factored as T = U S, where S\nis a K \u00d7 K matrix, S \u2208 L RK , and U is an isometry, U \u2208 U(RK , H). Suitably\nbounded K \u00d7 K matrices form a compact, finite dimensional set, the complexity\nof which can be controlled using covering numbers, while the complexity arising\nfrom the set of isometries can be controlled with Rademacher and Gaussian\naverages. Theorem 5 then\n\u0001 combines these complexity estimates.\nFor fixed S \u2208 L RK we denote\n\b\n\u0001\nGS = fUS : U \u2208 U RK , H .\n\nRecall the notation kT kY = supT \u2208T kT kY = supT \u2208T supy\u2208Y kT yk. With S we\ndenote the set of K \u00d7 K matrices\n\b\n\u0001\nS = S \u2208 L RK : kSkY \u2264 kT kY .\n\nLemma 4. Assume kT kY \u2265 1, that the functions in F , when restricted to the\nunit ball of H, have range contained in [0, b], and that the measure \u03bc is supported\non the unit ball of H. Then with probability at least 1 \u2212 \u03b4 we have for all T \u2208 T\nthat\nm\n\nEx\u223c\u03bc fT (x) \u2212\n\n1 X\nfT (xi )\nm i=1\n\n\u2264 sup Rm (GS , \u03bc) +\nS\u2208S\n\nv \u0010\n\u0011\nu\nu ln 16m kT k2\nt\nY\nbK\n2\n\nm\n\n8 kT k\n+ \u221a Y +b\nm\n\nr\n\nln (1/\u03b4)\n.\n2m\n\nProof. Fix \u01eb > 0. The\u0001set S \u0001is the ball of radius kT kY in the K 2 -dimensional\nBanach space L RK , k.kY so by Proposition 1 we can find a subset S\u01eb \u2282\nK2\n\nS, of cardinality |S\u01eb | \u2264 (4 kT kY /\u01eb)\nsuch that every member of S can be\napproximated by a member of S\u01eb up to distance \u01eb in the norm k.kY .\nWe claim that for all T \u2208 T there exist U \u2208 U(RK , H) and S\u01eb \u2208 S\u01eb such that\n|fT (x) \u2212 fUS\u01eb (x)| < 4 kT kY \u01eb,\n\n\f13\n\nfor all x in the unit ball of H. To see this write T = U S with U \u2208 U(RK , H)\nand S \u2208 L(RK ). Then, since U is an isometry, we have\nkSkY = sup kSyk = sup kT yk = kT kY \u2264 kT kY\ny\u2208Y\n\ny\u2208Y\n\nso that S \u2208 S. We can therefore choose S\u01eb \u2208 S\u01eb such that kS\u01eb \u2212 SkY < \u01eb. Then\nfor x \u2208 H, with kxk \u2264 1, we have\n\u0011\n\u0010\n\u0011\n\u0010\n2\n2\n|fT (x) \u2212 fUS\u01eb (x)| = inf kx \u2212 U Syk \u2212 inf kx \u2212 U S\u01eb yk\ny\u2208Y\ny\u2208Y\n\u0011\n\u0010\n2\n2\n\u2264 sup kx \u2212 U Syk \u2212 kx \u2212 U S\u01eb yk\ny\u2208Y\n\n= sup |hU S\u01eb y \u2212 U Sy, 2x \u2212 (U Sy + U S\u01eb y)i|\ny\u2208Y\n\n\u2264 (2 + 2 kT kY ) sup k(S\u01eb \u2212 S) yk \u2264 4 kT kY \u01eb.\ny\u2208Y\n\nApply Theorem 5 to the finite collection of function classes {GS : S \u2208 S\u01eb } to see\nthat with probability at least 1 \u2212 \u03b4\nm\n\nsup Ex\u223c\u03bc fT (x) \u2212\n\nT \u2208T\n\n\u2264 max\n\nsup\n\nS\u2208S\u01eb U\u2208U (RK ,H)\n\n1 X\nfT (xi )\nm i=1\n\nEx\u223c\u03bc fUS (x) \u2212\nr\n\nm\n\n1 X\nfUS (xi ) + 8 kT kY \u01eb\nm i=1\n\nln |S\u01eb | + ln (1/\u03b4)\n+ 8 kT kY \u01eb\nS\u2208S\u01eb\n2m\nv \u0010\n\u0011\nu\nr\n2\nu\n8 kT kY\nln (1/\u03b4)\nbK t ln 16m kT kY\n+b\n\u2264 sup Rm (GS , \u03bc) +\n+ \u221a\n,\n2\nm\n2m\nm\nS\u2208S\n\n\u2264 max Rm (GS , \u03bc) + b\n\nwhere the last line follows from the\n\u221a known bound on |S\u01eb |, subadditivity of the\nsquare root and the choice \u01eb = 1/ m.\n\u2293\n\u2294\nRemark 1. If H is finite dimensional the above result may be improved to\nv\n\u0010\n\u0011\nu\nr\n2\nu\n8 kT kY\nb t dK ln 16m kT kY\nln (1/\u03b4)\nEfT \u2212 \u00cafT \u2264\n+ \u221a\n.\n(2)\n+b\n2\nm\n2m\nm\nTo see this, follow the same lines as in Lemma 4 to note that\n\nsup EfT \u2212 \u00cafT \u2264 max EfT \u2212 \u00cafT + 8kT kY \u01eb,\n\nT \u2208T\n\nT \u2208T\u01eb\n\nwhere T\u01eb is a subset of T such that every member of T can be approximated by\na member of T\u01eb up to distance \u01eb in the norm k*kY .\ndK\nBy Proposition 1, |T\u01eb | \u2264 (4 kT k\u221a\n. Inequality (2) now follows from\nY /\u01eb)\nTheorem 5 with N = |T\u01eb | and \u01eb = 1/ m.\n\n\f14\n\nTo complete the proof of Theorem 2 we now fix some S \u2208 S and focus on\nthe corresponding function class GS .\n\u0001\nLemma 5. For any S \u2208 L RK we have\n\u221a\nK\nR (GS , \u03bc) \u2264 2 2\u03c0 kSkY \u221a .\nm\n\nProof. Let kxi k \u2264 1 and define Gaussian processes \u03a9U and \u039eU indexed by\nU(RK , H)\n\u03a9U =\n\nm\nX\ni=1\n\n2\n\n\u03b3 i inf kxi \u2212 U Syk\ny\u2208Y\n\n\u039eU = 2 kSkY\n\nK X\nm\nX\nk=1 i=1\n\n\u03b3 ik hxi , U ek i ,\n\nwhere the ek are the canonical basis of RK . For U1 , U2 \u2208 U(RK , H) we have\n\u00132\nm \u0012\nX\nsup kxi \u2212 U1 Syk2 \u2212 kxi \u2212 U2 Sk2\nE (\u03a9U1 \u2212 \u03a9U2 )2 \u2264\n\u2264\n\ni=1\nm\nX\n\nsup 4hxi , (U2 \u2212 U1 )Syi2\n\ni=1 y\u2208Y\nm\nX\n\n\u22644\n=\n\ny\u2208Y\n\nsup kU2\u2217 xi \u2212 U1\u2217 xi k2 kSyk2\n\ni=1 y\u2208Y\n\n2\n4 kSkY\n\nm X\nK\nX\ni=1 k=1\n\n2\n\n(hxi , U1 ek i \u2212 hxi , U2 ek i)\n\n= E (\u039eU1 \u2212 \u039eU2 )2 .\nIt follows from Lemma 1 and Slepians lemma (Theorem 4) that\nr\n2 \u03c0\nRm (GS , \u03bc) \u2264 Ex\u223c\u03bcm\nE\u03b3 sup \u039eU ,\nm 2\nU\nso the result follows from the following inequalities, using Cauchy-Schwarz' and\nJensen's inequality, the orthonormality of the \u03b3 ik and the fact that kxi k \u2264 1 on\nthe support of \u03bc.\n+\n*m\nK\nX\nX\n\u03b3 ik xi , U ek\nE\u03b3 sup \u039eU = 2 kSkY E sup\nU\n\nU\n\n\u2264 2 kSkY\n\nK\nX\n\nk=1\n\nk=1\n\nE\n\n\u221a\n\u2264 2 kSkY K m.\n\nm\nX\n\ni=1\n\n\u03b3 ik xi\n\ni=1\n\n\u2293\n\u2294\n\n\f15\n\n\u221a Substitution of the last result in Lemma 4 and noting that, for K \u2265 1,\n2 2\u03c0K + 8 \u2264 14K, gives Theorem 2.\nObserve that when the set S contains only the identity matrix, the function\nclass GS is the class of reconstruction errors of PCA. In this case, the result can\nbe improved as shown by the next lemma.\np\nLemma 6. R (D, \u03bc) \u2264 2 K/m.\n\nProof. Recall, for every z \u2208 H, that the outer product operator Qz is defined by\nQz x = hx, zi z. With h*, *i2 and k*k2 denoting the Hilbert-Schmidt inner product\nand norm respectively we have for kxi k \u2264 1\nE\u03c3 sup\n\nm\nX\n\nf \u2208D i=1\n\n\u03c3 i f (xi ) = E\u03c3 sup\n\nm\nX\n\n\u0011\n\u0010\n2\n2\n\u03c3 i kxi k \u2212 kU U \u2217 xi k\n\nU\u2208U i=1\n*m\nX\n\n= E\u03c3 sup\n\nU\u2208U\n\n\u2264 E\u03c3\n\u2264\n\n\u221a\n\nm\nX\ni=1\n\n\u03c3 i Qxi , U U\n\ni=1\n\n\u03c3 i Qxi\n2\n\n\u2217\n\n+\n\n2\n\nsup kU U \u2217 k2\n\nU\u2208U\n\nmK,\n\n\u221a\nsince the Hilbert-Schmidt norm of a K-dimensional projection is K. The result\nfollows upon multiplication with 2/m and taking the expectation in \u03bcm .\n\u2293\n\u2294\nAn application of Theorem\n5 with N = 1 and b = 1 also give a generalization\np\nbound for PCA of order K/m.\n\n4\n\nConcluding remarks\n\nWe have analyzed a general method to encode random vectors in a Hilbert space\nH. The method searches for an operator T : RK \u2192 H which minimizes, within\nsome prescribed class T , the empirical average of the reconstruction error, which\nis defined as the minimum distance between a given point in H and an image of\nthe operator T acting on a prescribed codebook Y .\nWe have presented two approaches to upper bound the estimation error of the\nmethod in terms of the parameter K, the sample size m and the properties of the\nsets T and Y . The first approach is based on a direct bound for the Rademacher\naverage of the loss class induced by the reconstruction error. The bound matches\nthe best known bound for K-means clustering in a Hilbert space [4] but also\napplies to other interesting coding techniques such as sparse coding and nonnegative matrix factorization. The second approach uses a decomposition of the\nfunction class as a union of function classes parameterized by K-dimensional\nisometries. The main idea is to approximate the union with a finite union via\ncovering numbers and then bound the complexity of each class under the union\nwith Rademacher averages. This second result is more complicated than the first\n\n\f16\n\none, however it provides in certain cases a better dependency of the bound on\nthe parameter K at the expense of an additional logarithmic factor in m.\nWe conclude with some open problems and possible extensions which are\nsuggested by this study. Firstly, it would be valuable to investigate the possibility\nof removing the logarithmic term in m in the bound of Theorem 2. Secondly, it\nwould be important to elucidate whether the dependency in K in the same bound\nis optimal. The latter problem is also mentioned in [4] in the case of K-means\nclustering. Finally, in would be interesting to study possible improvements of our\nresults in the case that additional assumptions on the probability measure \u03bc are\nintroduced. For example, in the case of K-means clustering in a finite dimensional\nHilbert space [1] shows that for certain classes of probability measures the rate of\nconvergence can be improved to O(log(m)/m) and it may be possible to obtain\nsimilar improvements in our general framework.\nAcknowledgments\nThis work was supported by EPSRC Grants GR/T18707/01 and EP/D071542/1.\n\nReferences\n1. A. Antos, L. Gyorfi, A. Gyorgy. Individual convergence rates in empirical vector\nquantizer design. IEEE Transactions on Information Theory, 51(11):4013\u20134022,\n2005.\n2. P. L. Bartlett and S. Mendelson. Rademacher and Gaussian Complexities: Risk\nBounds and Structural Results. Journal of Machine Learning Research, 3: 463\u2013\n482, 2002.\n3. P. Bartlett, T. Linder, G. Lugosi. The minimax distortion redundancy in empirical\nquantizer design. IEEE Transactions on Information Theory, 44: 1802\u20131813, 1998.\n4. G. Biau, L. Devroye, G. Lugosi. On the performance of clustering in Hilbert spaces.\nIEEE Transactions on Information Theory, 54:781\u2013790, 2008.\n5. F. Cucker and S. Smale. On the mathematical foundations of learning, Bulletin of\nthe American Mathematical Society, 39 (1):1\u201349, 2001.\n6. W. Hoeffding. Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association, 58:13\u201330, 1963.\n7. P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. Journal\nof Machine Learning Research, 5:1457\u20131469, 2004.\n8. V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding\nthe generalization error of combined classifiers, The Annals of Statistics, 30(1):\n1\u201350, 2002.\n9. M. Ledoux, M. Talagrand. Probability in Banach Spaces, Springer, 1991.\n10. D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix\nfactorization. Nature 401, 788\u2013791, 1999.\n11. S. Z. Li, X. Hou, H. Zhang, and Q. Cheng. Learning spatially localized parts-based\nrepresentations. Proc. IEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), Vol. I, pages 207\u2013212, Hawaii, USA, 2001.\n12. A. Maurer and M. Pontil. Generalization bounds for K-dimensional coding schemes\nin Hilbert spaces. Proceedings of the 19th international conference on Algorithmic\nLearning Theory, pages 91, 2008,\n\n\f17\n13. C. McDiarmid. Concentration, in Probabilistic Methods of Algorithmic Discrete\nMathematics, p195-248, Springer, Berlin, 1998.\n14. B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive field properties\nby learning a sparse code for natural images. Nature, 381:607\u2013609, 1996.\n15. J. Shawe-Taylor, C. K. I. Williams, N. Cristianini, J. S. Kandola. On the eigenspectrum of the Gram matrix and the generalization error of kernel-PCA. IEEE\nTransactions on Information Theory 51(7): 2510\u20132522, 2005.\n16. O. Wigelius, A. Ambroladze, J. Shawe-Taylor. Statistical analysis of clustering\nwith applications. Preprint, 2007.\n17. D. Slepian. The one-sided barrier problem for Gaussian noise. Bell System Tech.\nJ., 41: 463\u2013501, 1962.\n18. A.W. van der Vaart and J.A. Wallner. Weak Convergence and Empirical Processes,\nSpringer Verlag, 1996.\n19. L. Zwald, L., O. Bousquet, and G. Blanchart. Statistical properties of kernel principal component analysis. Machine Learning 66(2-3): 259\u2013294, 2006.\n\n\f"}