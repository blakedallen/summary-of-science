{"id": "http://arxiv.org/abs/1006.1543v1", "guidislink": true, "updated": "2010-06-08T13:03:45Z", "updated_parsed": [2010, 6, 8, 13, 3, 45, 1, 159, 0], "published": "2010-06-08T13:03:45Z", "published_parsed": [2010, 6, 8, 13, 3, 45, 1, 159, 0], "title": "Efficient Discovery of Large Synchronous Events in Neural Spike Streams", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1006.5209%2C1006.2220%2C1006.4642%2C1006.3742%2C1006.1047%2C1006.4888%2C1006.2072%2C1006.4342%2C1006.4707%2C1006.3766%2C1006.3610%2C1006.4890%2C1006.3506%2C1006.0084%2C1006.0108%2C1006.0134%2C1006.4986%2C1006.0903%2C1006.3664%2C1006.0780%2C1006.5662%2C1006.5880%2C1006.3487%2C1006.3126%2C1006.2540%2C1006.0756%2C1006.3879%2C1006.3535%2C1006.0858%2C1006.0627%2C1006.1892%2C1006.4215%2C1006.3071%2C1006.2486%2C1006.5394%2C1006.2231%2C1006.5955%2C1006.5270%2C1006.0163%2C1006.2391%2C1006.3960%2C1006.5929%2C1006.3333%2C1006.1931%2C1006.2084%2C1006.3213%2C1006.2862%2C1006.4812%2C1006.0253%2C1006.0916%2C1006.1046%2C1006.3183%2C1006.0760%2C1006.3745%2C1006.0439%2C1006.2860%2C1006.2846%2C1006.1190%2C1006.4928%2C1006.2047%2C1006.3681%2C1006.4511%2C1006.1543%2C1006.0829%2C1006.0488%2C1006.0867%2C1006.4726%2C1006.3378%2C1006.5630%2C1006.5849%2C1006.1407%2C1006.5108%2C1006.2499%2C1006.3819%2C1006.3565%2C1006.2539%2C1006.2119%2C1006.2138%2C1006.3673%2C1006.1034%2C1006.0819%2C1006.4788%2C1006.1882%2C1006.5567%2C1006.4615%2C1006.4044%2C1006.5418%2C1006.4013%2C1006.4529%2C1006.4510%2C1006.4085%2C1006.1888%2C1006.4690%2C1006.5688%2C1006.0485%2C1006.0458%2C1006.2858%2C1006.5144%2C1006.2749%2C1006.3554%2C1006.5706&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Efficient Discovery of Large Synchronous Events in Neural Spike Streams"}, "summary": "We address the problem of finding patterns from multi-neuronal spike trains\nthat give us insights into the multi-neuronal codes used in the brain and help\nus design better brain computer interfaces. We focus on the synchronous firings\nof groups of neurons as these have been shown to play a major role in coding\nand communication. With large electrode arrays, it is now possible to\nsimultaneously record the spiking activity of hundreds of neurons over large\nperiods of time. Recently, techniques have been developed to efficiently count\nthe frequency of synchronous firing patterns. However, when the number of\nneurons being observed grows they suffer from the combinatorial explosion in\nthe number of possible patterns and do not scale well. In this paper, we\npresent a temporal data mining scheme that overcomes many of these problems. It\ngenerates a set of candidate patterns from frequent patterns of smaller size;\nall possible patterns are not counted. Also we count only a certain well\ndefined subset of occurrences and this makes the process more efficient. We\nhighlight the computational advantage that this approach offers over the\nexisting methods through simulations.\n  We also propose methods for assessing the statistical significance of the\ndiscovered patterns. We detect only those patterns that repeat often enough to\nbe significant and thus be able to automatically fix the threshold for the\ndata-mining application. Finally we discuss the usefulness of these methods for\nbrain computer interfaces.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1006.5209%2C1006.2220%2C1006.4642%2C1006.3742%2C1006.1047%2C1006.4888%2C1006.2072%2C1006.4342%2C1006.4707%2C1006.3766%2C1006.3610%2C1006.4890%2C1006.3506%2C1006.0084%2C1006.0108%2C1006.0134%2C1006.4986%2C1006.0903%2C1006.3664%2C1006.0780%2C1006.5662%2C1006.5880%2C1006.3487%2C1006.3126%2C1006.2540%2C1006.0756%2C1006.3879%2C1006.3535%2C1006.0858%2C1006.0627%2C1006.1892%2C1006.4215%2C1006.3071%2C1006.2486%2C1006.5394%2C1006.2231%2C1006.5955%2C1006.5270%2C1006.0163%2C1006.2391%2C1006.3960%2C1006.5929%2C1006.3333%2C1006.1931%2C1006.2084%2C1006.3213%2C1006.2862%2C1006.4812%2C1006.0253%2C1006.0916%2C1006.1046%2C1006.3183%2C1006.0760%2C1006.3745%2C1006.0439%2C1006.2860%2C1006.2846%2C1006.1190%2C1006.4928%2C1006.2047%2C1006.3681%2C1006.4511%2C1006.1543%2C1006.0829%2C1006.0488%2C1006.0867%2C1006.4726%2C1006.3378%2C1006.5630%2C1006.5849%2C1006.1407%2C1006.5108%2C1006.2499%2C1006.3819%2C1006.3565%2C1006.2539%2C1006.2119%2C1006.2138%2C1006.3673%2C1006.1034%2C1006.0819%2C1006.4788%2C1006.1882%2C1006.5567%2C1006.4615%2C1006.4044%2C1006.5418%2C1006.4013%2C1006.4529%2C1006.4510%2C1006.4085%2C1006.1888%2C1006.4690%2C1006.5688%2C1006.0485%2C1006.0458%2C1006.2858%2C1006.5144%2C1006.2749%2C1006.3554%2C1006.5706&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We address the problem of finding patterns from multi-neuronal spike trains\nthat give us insights into the multi-neuronal codes used in the brain and help\nus design better brain computer interfaces. We focus on the synchronous firings\nof groups of neurons as these have been shown to play a major role in coding\nand communication. With large electrode arrays, it is now possible to\nsimultaneously record the spiking activity of hundreds of neurons over large\nperiods of time. Recently, techniques have been developed to efficiently count\nthe frequency of synchronous firing patterns. However, when the number of\nneurons being observed grows they suffer from the combinatorial explosion in\nthe number of possible patterns and do not scale well. In this paper, we\npresent a temporal data mining scheme that overcomes many of these problems. It\ngenerates a set of candidate patterns from frequent patterns of smaller size;\nall possible patterns are not counted. Also we count only a certain well\ndefined subset of occurrences and this makes the process more efficient. We\nhighlight the computational advantage that this approach offers over the\nexisting methods through simulations.\n  We also propose methods for assessing the statistical significance of the\ndiscovered patterns. We detect only those patterns that repeat often enough to\nbe significant and thus be able to automatically fix the threshold for the\ndata-mining application. Finally we discuss the usefulness of these methods for\nbrain computer interfaces."}, "authors": ["Raajay Viswanathan", "P. S. Sastry", "K. P. Unnikrishnan"], "author_detail": {"name": "K. P. Unnikrishnan"}, "author": "K. P. Unnikrishnan", "links": [{"href": "http://arxiv.org/abs/1006.1543v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1006.1543v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1006.1543v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1006.1543v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:1006.1543v1 [cs.NE] 8 Jun 2010\n\nEfficient Discovery of Large Synchronous Events in Neural\nSpike Streams\nV. Raajay\n\nP. S. Sastry\n\nK. P. Unnikrishnan\n\nIndian Institute of Science\nBangalore, India. 560025\n\nIndian Institute of Science\nBangalore, India. 560025\n\nUniversity of Michigan\nAnn Arbor, USA.\n\nraajay.v@gmail.com\n\nsastry@ee.iisc.ernet.in\n\nABSTRACT\nWe address the problem of finding patterns from multineuronal spike trains that give us insights into the multineuronal codes used in the brain and help us design better\nbrain computer interfaces. We focus on the synchronous firings of groups of neurons as these have been shown to play\na major role in coding and communication ([6]). With large\nelectrode arrays, it is now possible to simultaneously record\nthe spiking activity of hundreds of neurons over large periods\nof time. Recently, techniques have been developed to efficiently count the frequency of synchronous firing patterns.\nHowever, when the number of neurons being observed grows\nthey suffer from the combinatorial explosion in the number\nof possible patterns and do not scale well. In this paper,\nwe present a temporal data mining scheme that overcomes\nmany of these problems. It generates a set of candidate\npatterns from frequent patterns of smaller size; all possible\npatterns are not counted. Also we count only a certain well\ndefined subset of occurrences and this makes the process\nmore efficient. We highlight the computational advantage\nthat this approach offers over the existing methods through\nsimulations.\nWe also propose methods for assessing the statistical significance of the discovered patterns. We detect only those\npatterns that repeat often enough to be significant and thus\nbe able to automatically fix the threshold for the data-mining\napplication. Finally we discuss the usefulness of these methods for brain computer interfaces ([4, 11]).\n\nKeywords\nmulti-neuronal spike trains,Data mining,Neural code,Frequent\nEpisodes, Synchrony\n\n1.\n\nINTRODUCTION\n\nNeurons form the basic computing elements of brain and\nhence, gaining the understanding of the coordinated behavior of groups of neurons is essential for gaining a principled\n\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee.\nCopyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$10.00.\n\nkpuk@umich.edu\n\nunderstanding of the brain function. Thus, one of the important problems in neuroscience is that of understanding\nthe functioning of a neural tissue in terms of interactions\namong its neurons .\nThe neurons communicate with one another by means of\nvoltage fluctuations called action potential or spikes. We\ncan study the activity of a specific neural tissue by gathering data in the form of sequences of action potentials or\nspikes generated by each of a group of potentially interconnected neurons. Recent techniques, like Micro Electrode\nArray (MEA), imaging of ionic concentrations etc., have enabled us in recording the activity of hundreds of neurons simultaneously. Such recorded data, known as multi-neuronal\nspike train data, is a mixture of the stochastic spiking of activities of individual neurons as well as correlated spiking\nactivity due to interactions or connections among neurons.\nOne way to find out the interactions among neurons is to\nfind patterns from the spike train data([2]). The patterns\nhelp in understanding the relation between the spiking times\nof neurons which in turn can throw light on the interaction\namong neurons performing a specific function.\nVarious algorrithms have been developed to find interesting patterns in spike train data. All the algorithms essentially find frequent (or less-frequent) occurences of specific\npatterns and try to establish the significance of their occurence in the data, that is, statistically show that these\npatterns have not occurred by chance and have occurred\nbecause of interaction among the constituent neurons.\nPredominantly, two kinds of patterns have been explored,\nnamely, 1. Sequential firing patterns 2. Synchronous firing\npatterns. Sequential firing patterns are used to represent a\nchain of neurons firing one after the other after a certain\namount of delay in time. Such patterns have been found in\ndata recorded from the hippocampus circuit.\nSynchronous firing patterns, on the other hand, represent\na group of neurons firing very close to each other in time.\nThe difference in the their spiking times is in the order of\nmilliseconds. Some algorithms ([5]) use time binning technique to find such patterns. The spiking times of neurons\nare binned into time intervals equal to time span of interest.\nThen the occurence of every possible pattern is checked in\neach time bin. Such binning techniques affect the time resolution of the spike times. Also some patterns that occur\nacross time bins will be missed. Recently techniques [13]\nhave been developed that avoid time-binning and count the\nfrequency of patterns more efficiently. However, these algorithms are all essentially correlation based and also count\nthe frequency of all possible patterns. When the number of\n\n\fneurons being observed grows they suffer from the combinatorial explosion in the number of possible patterns and do\nnot scale well.\nIn this paper, we view this problem of finding synchronous\npatterns from a temporal datamining perspective. The patterns are represented as parallel episodes (with expiry times)\ndiscussed in the frequent episode discovery framework [12].\nWe use the parallel mining algorithm to discover frequent\npatterns whose frequency in the data is above a user specified threshold. The algorithm is apriori based. It generates\na set of candidate patterns from frequent patterns of smaller\nsize. All possible patterns are not counted. Also we count\nonly a certain well defined subset of occurrences and this\nmakes the process more efficient. We highlight the computational advantage that this approach offers over the existing\nmethods through simulations.\nWe also develop statistical techniques to establish the significance of the discovered patterns.\nThe rest of the paper is organised as follows. Section 2\ndescribes the parallel episode mining algorithm that we use\nto discover synchronous patterns. Section 3 presents a significance test for the parallel episodes. In Section 4 we\npresent simulation results that shows the effectiveness of our\nmethod. Concluding remarks are provided in Section 5.\n\n2.\n\nFREQUENT EPISODE FRAMEWORK FOR\nDISCOVERY OF SYNCHRONOUS PATTERNS\n\nTemporal datamining is concerned with analyzing symbolic time series data to discover 'interesting' patterns of\ntemporal dependencies ([7, 10]). Recently we have proposed\nthat some datamining techniques, based on the so called\nfrequent episodes framework, are well suited for analyzing\nmulti-neuronal spike train data [12, 3, 14]. Patterns of interest in spike data such as synchronous firings by groups of\nneurons, the sequential patterns, and synfire chains which\nare a combination of synchrony and ordered firings, can be\nefficiently discovered from the data using these datamining\ntechniques. In this section we first briefly outline the frequent episodes framework and then qualitatively describe\nthis datamining technique for discovering frequently occurring synchronous patterns.\nIn the frequent episodes framework of temporal datamining. the data to be analyzed is a sequence of events denoted\nby h(E1 , t1 ), (E2 , t2 ), . . .i where Ei represents an event type\nand ti the time of occurrence of the ith event. Ei 's are drawn\nfrom a finite set of event types, \u03b6. The sequence is ordered\nwith respect to time of occurrences of the events so that,\nti \u2264 ti+1 , \u2200i. The following is an example event sequence\ncontaining 11 events with 5 event types.\nh(A, 1), (B, 3), (D, 5), (A, 5), (C, 6), (A, 10),\n(E, 15), (B, 15), (B, 17), (C, 18), (C, 19)i\n\n(1)\n\nA parallel episode is an ordered tuple of event types. For\nexample, (A B C) is a 3-node parallel episode. Such an\nepisode is said to occur in an event sequence if there are\ncorresponding events in the data sequence. In sequence (1),\nthe events {(A, 1), (B, 3), (C, 6)} and {(B, 3), (C, 6), (A, 10)}\nconstitute an occurrence of the parallel episode (A B C) .\nWe note here that occurrence of an episode does not require\nthe associated event types to occur consecutively; there can\n\nbe other intervening events between them.\nThe objective in frequent episode discovery is to detect\nall frequent episodes (of different lengths) from the data.\nA frequent episode is one whose frequency exceeds a (user\nspecified) frequency threshold. The frequency of an episode\ncan be defined in many ways. It is intended to capture\nsome measure of how often an episode occurs in an event\nsequence. One chooses a measure of frequency so that frequent episode discovery is computationally efficient and, at\nthe same time, higher frequency would imply that an episode\nis occurring often. In our algorithm we use the maximum\nnon-overlapped occurences as the frequency measure. This\ndefinition of frequency results in very efficient counting algorithms with some interesting theoretical properties ([8, 9]).\nIn analyzing neuronal spike data, it is useful to consider\nmethods, where, while counting the frequency, we include\nonly those occurrences which satisfy some additional temporal constraints. Here we are interested in what we call\nexpiry time constraint which is specified by giving a time\nspan \u03c4 . The constraint requires that span of occurence of\nthe parallel episode is less that \u03c4 . For example in sequence\n(1), with an expiry time \u03c4 = 5, the occurrence of parallel\nepisode {(A, 1), (B, 3), (C, 6)} is valid where as the occurrence {(B, 3), (C, 6), (A, 10)} is not. As is easy to see, a\nparallel episode with expiry time constraints corresponds to\nwhat we called a synchronous pattern in the previous section. These are the temporal patterns of interest in this\npaper. To represent a parallel episode (A B C) with expiry\ntime \u03c4 we use the notation (A B C)\u03c4 .\nEfficient algorithm to count such episodes exist ([12]).\nThe algorithm counts the non-overlapped occurrence of an\nepisode (say, (A B C)\u03c4 ) as follows. While going down the\ndata stream it remembers the latest time of occurrence of\nall its constituent events. Once all the events are seen at\nleast once, it checks if the span of the latest occurrences of\nall the events is less than \u03c4 . If the expiry time constraint is\nsatisfied, then frequency counter is incremented and all the\nevents are marked as not seen. The algorithm then proceeds\nfurther to look for more occurrences. It is easy to see that\nsuch a method counts only non-overlapped occurrences of\nthe parallel episodes.\nHowever, an efficient counting algorithms alone is not sufficient. This is because at higher levels the number of parallel episodes to be counted increases exponentially. The problem of exploding number of candidates is tackled through\nthe classic apriori method that is popular in datamining.\nAt each level, the number of parallel episodes that have to\ncounted is generated from the frequent candidates at lower\nlevels.\nBased on this idea, we have the following structure for\nthe algorithm. We first get frequent 1-node episodes which\nare then used to make candidate 2-node episodes. Then, by\none more pass over data, we find frequent 2-node episodes\nwhich are then used to make candidate 3-node episodes and\nso on. Such a technique is quite effective in controlling combinatorial explosion and the number of candidates comes\ndown drastically as the size increases. This is because, as\nthe size increases, many of the combinatorially possible parallel episodes of that size would not be frequent. This allows\nthe algorithm to find large size frequent episodes efficiently.\nAt each stage of this process, we count frequencies of not\none but a whole set of candidate episodes (of a given size)\nthrough one sequential pass over the data. We do not actu-\n\n\fally traverse the time axis in time ticks once for each pattern\nwhose occurrences we want to count. We traverse the timeordered data stream. As we traverse the data we remember\nenough from the data stream to correctly take care of all the\noccurrence possibilities of all episodes in the candidate set\nand thus compute all the frequent episodes of a given size\nthrough one pass over the data. The complete details of the\nalgorithm are available in ([12]).\n\nof length L.\nLet p be the probability that we find an occurence of an\nepisode at any given time instant. Then from the above\ndescription of counting we can say that, from any given time\ninstant we move ahead by T time units with a probability\np and move ahead by one time unit with probability 1 \u2212 p.\nThis leads to the recurrence relation,\nF (L, T, p) = (1\u2212p)F (L\u22121, T, p)+p(1+F (L\u2212T, L, p)) (2)\n\n3.\n\nSIGNIFICANCE OF DISCOVERED SYNCHRONOUS FIRING PATTERNS\n\nIn the previous section we discussed effective algorithms to\ndiscover synchronous patterns. Here, we present significance\ntests to show that the obtained patterns are significant and\nhave not occurred by chance.\nThere have been many approaches for assessing the significance of detected firing patterns ([1, 13]). In the current\nanalytical approaches, one generally employs a Null hypothesis that the different spike trains are generated by independent processes. In many cases one also assumes (possibly\ninhomogeneous) Bernoulli or Poisson processes. Then one\ncan calculate the probability of observing the given number\nof repetitions of the pattern (or of any other statistic derived\nfrom such counts) under the null hypothesis of independent\nprocesses and hence calculate a minimum number of repetitions needed to conclude that a pattern is significant in the\nsense of being able to reject the null hypothesis. There are\nalso some empirical approaches, which may be called the jitter methods, suggested for assessing significance. Here one\ncreates many surrogate data streams from the experimentally observed data by perturbing (or jittering) the individual spikes while keeping certain statistics same. Then, by\ncalculating the empirical distribution of pattern counts on\nthe sample of surrogate data, one assesses the significance\nof the observed patterns.\nRecently, significance tests for sequential patterns had\nbeen developed [14]. These tests involve estimating the expected frequency of serial episodes under a given null hypothesis by modelling the counting process of the algorithm.\nWe also take a similar approach and modify the method to\nsuit synchronous patterns. The Null hypothesis we assume\nis that all the neurons fire independently of each other.\n\n3.1 Modelling the counting process\nSuppose, we are operating at a time resolution of \u2206T .\n(That is, the times of events or spikes are recorded to a\nresolution of \u2206T ). Then we discretize the time axis into\nintervals of length \u2206T . For a parallel episode with expiry\ntimes, the span of any occurrence should be less than the\nexpiry time, T (in steps of \u2206T ). But initially let us assume\nthat the span of each occurrence of a parallel episode is exactly T time steps. (We can modify the counting to skip T\ntime units once an occurrence is found.) Now the counting\nprocess explained in the previous section can be viewed in\nthe following way. For each episode (say (A B C)T ) whose\nfrequency we want to find, we do the following. We start\nat time instant 1. We check to see whether there is an occurrence of the episode starting from the current instant. If\nwe find an occurrence we increment the counter and move\nahead by T steps and again start looking for another occurrence. If we do not find an occurrence we move ahead one\nstep. We do this since we reach the end of the data stream\n\nwhere, F (L, T, p) is the expected frequency of a episode.\nThe notation F (L, T, p) denotes that the mean is a function\nof L,T and p.\nThe boundary conditions for this recurrence are:\nF (x, y, p) = 0, if x < y and \u2200p.\n\n(3)\n\nSimilarly, the mean of the square of the frequency, G(L, T, p),\nof the episode can be obtained as\nG(L, T, p) = (1 \u2212 p)G(L \u2212 1, T, p) +\np(1 + G(L \u2212 T, T, p) + 2F (L \u2212 T, T, p))\n\n(4)\n\nHence, the variance V (L, T, p) is,\nV (L, T, p) = G(L, T, p) \u2212 (F (L, T, p))2\n\n(5)\n\nSince the neurons fire independently of each other the\nprobability of an occurence of an episode at any time instant is given by,\np = \u03c1n (\u2206T )n\n\nn\u22121\nX\n\n(T \u2212 1)n\u22121\u2212i T i\n\n(6)\n\ni=0\n\nwhere, \u03c1 be the unconditional probability that a neuron\nfires at any given time instant. We obtain \u03c1 by estimating the average rate of firing for this neuron from the data\n(or we may know it from other prior knowledge). Using\nthe value of p, we can calculate values of F (L, T, p) and\nV (L, T, p) from equations 2, 4 and 5. For a given type-I error\n\u01eb, using the Chebyshev inequality, the frequency\nthreshold\np\ncan then be obtained as F (L, T, p) + k V (L, T, p), where\nk is the smallest integer such that k2 \u2265 1\u01eb . We use this\nfrequency threshold for mining significant parallel episodes\n(synchronous firing patterns).\nBy using this frquency threshold, we ensure that the chances\nof a random episode being reported as frequent is less than\n\u01eb. So for low values of \u01eb, we can confidentally say that the\npatterns reported as frequent are not random patterns.\n\n4. SIMULATION RESULTS\nIn this section we compare the parallel episode mining\nalgorithm with a popular existing tool, NeuroXidence, in\nterms of running times, scalability and false positive rates.\nNeuroXidence ([13]) is used to detect an excess or a lack of\nsynchronous firing in spike train data. This is done by counting the number of occurrences of synchronous firing patterns\nsatisfying a given expiry time. Unlike our non-overlapped\ncounts, NeuroXidence counts all occurrences of a pattern.\nFor example, for the pattern, (A B C)T any set of spikes\nof A,B and C that satisfy the time constraint is considered\nas an occurrence. NeuroXidence counts the frequency of all\npatterns that occur at least once in the data. The counting\nprocess is essentially a correlation based technique.\n\n\fL\n50000\n100000\n200000\n\nAvg. Run\nPE\n0.2\n0.375\n0.8\n\nTime (s)\nNX\n51\n134\n270\n\nF.P.R.\nPE\nNX\n15% 31%\n21% 47%\n48% 79%\n\nTable 1: Comparison of NeuroXidence (NX) and\nParallel Episode Mining Algorithm (PE) : Average\nrunning time (in seconds) and False Positive Rates\n(F.P.R.)comparison for varying data lengths (L) .\n(Parameters: \u03c1 = 5 Hz, T = 5, Number of Neurons\n= 20.)\n\nNeuroXidence employs a non-parametric method to assess the significance of the observed counts. The Null hypothesis is that the patterns occur by chance. The estimate\nof the chance frequency under null hypothesis is obtained\nby generating surrogate data. Surrogate data is created by\njittering the spikes of the neurons independently of one another. This way the temporal cross structure in the data\nis destroyed while retaining the auto-structure of the spike\ntrains. For every trial of the data obtained, around 25 surrogates are created. The patterns frequencies are found out in\nthe surrogate data set. From the values so obtained, we get\nan empirical distribution of the chance frequencies. Using\nthat the significance of the observed frequency counts are\nobtained.\nNeuroXidence is found to be very effective in finding synchronous firing patterns [13].\nFor the results provided in this section we use spike train\ndata generated by using the Poisson simulator described in\n[14]. Each neuron is modelled as an inhomogenous poisson\nprocess. Strong interactions among neurons\ncan be input to the simulator by means of conditional\nprobabilities. For example, if we want the spiking of A at\nany time t to affect the spiking of B at time t + \u03c4 , we represent it by a conditional probality P (B/(A, \u03c4 )) = p. Since\nour null hypothesis is of independence no strong connections\nare embedded into the simulator. The neurons fire independently of one another. We include correlated firing in the\ndata by means of external stimulation, that is, we embed\nsynchronous firing in the data generated by the simulator.\nDifferent sized patterns (upto 7 nodes) with various expiry\ntimes are embedded in the data.\nEffectiveness of both the methods are assessed with respect to the running times and false positive rates. The\nmethods are tested for varying parameters like random firing rate, different expiry times, different number of neurons.\nThe running times and false positives rate reported for the\nparallel episode algorithm are average values obtained from\n100 realizations of the data. In case of NeuroXidence, the\nvalues are averaged over 20 iterations. The results are reported in Tables 1-4.\nNeuroXidence requires input data from various trials. For\nour experiments we split a single long data into 20 portions\nand give it as an input. For better statistical analysis, the\nnumber of surrogates for determing the empirical probability\ndistribution is set at 25.\nBoth the methods were found to be very effective in mining the embedded patterns. All the patterns that are embedded in that data were discovered by the both the methods.\nHowever, the parallel episode mining algorithm has huge\n\n\u03c1\n5\n10\n\nAvg. Run Time (s)\nPE\nNX\n0.38\n51\n0.50\n309\n\nF.P.R.\nPE\nNX\n22% 31%\n22% 49%\n\nTable 2: Comparison of NeuroXidence (NX) and\nParallel Episode Mining Algorithm (PE) : Average\nrunning time (in seconds) and False Positive Rates\n(F.P.R.)comparison for varying random firing frequency (\u03c1) . (Parameters: L = 50000, T = 5, Number of Neurons = 20.)\ncomputational advantage over NeuroXidence (refer Table 1)\nSuch difference in times are because the NeuroXidence calculates the frequencies of all possible patterns in the data.\nBut the parallel episode mining algorithms uses an efficient\nlevel wise procedure to count candidates generated out of\nfrequent sub-episodes. Also, the statistical test required for\nNeuroXidence requires it to find the frequencies of pattern\nin the surrogate data. If the number of surrogates is 25, then\neffectively NeuroXidence has to calculate the frequencies in\ndata that is 25 times longer than the input data. This is\nthe reason for the marked difference in running times of the\nalgorithms.\nThe change in expiry time of mining does not affect the\nrunning times of the episodes mining algorithms(see Table 4).\nM\n20\n30\n40\n\nAvg. Run Time (s)\nPE\nNX\n0.38\n51\n0.44\n233\n0.54\n1193\n\nF.P.R.\nPE\nNX\n22% 31%\n27% 49%\n40% 59%\n\nTable 3: Comparison of NeuroXidence (NX) and\nParallel Episode Mining Algorithm (PE) : Average\nrunning time (in seconds) and False Positive Rates\n(F.P.R.)comparison for varying number of participation neurons (M ). (Parameters: \u03c1 = 5 Hz, T = 5,\nL = 50000)\n\nT\n3\n5\n8\n10\n\nAvg. Run Time (s)\nPE\nNX\n0.38\n21\n0.38\n51\n0.37\n122\n0.37\n189\n\nF.P.R.\nPE\nNX\n29% 23%\n22% 31%\n15% 51%\n14% 54%\n\nTable 4: Comparison of NeuroXidence (NX) and\nParallel Episode Mining Algorithm (PE) : Average\nrunning time (in seconds) and False Positive Rates\n(F.P.R.)comparison for varying expiry times (T ) .\n(Parameters: \u03c1 = 5 Hz, L = 50000, Number of Neurons = 20.)\nThe running time of NeuroXidence increases drastically\nwith expiry times. Similar effects can also be seen because\nof increase in background firing rates(see Table 2). The\nnumber of false positives increase because more and more\npatterns start occurring more than once.\nThe increase in number of neurons has a huge effect on\n\n\fthe running times of NeuroXidence(see Table 3). Infact for\na network with 40 neurons the running time is as high as\n20 minutes for only 50 sec data at 5 Hz. This is because\nof the exponential increase in the number of patterns to be\ncounted.\nFrom Tables 2 and 4, it is clear that change in random\nfiring frequency and expiry times does not affect the running\ntimes very much. The algorithm will be able to scale up for\nlonger data with many interacting neurons.\n\n5.\n\nCONCLUSIONS\n\nFast decoding of information-bearing patterns are critical\nto the success of brain-computer interfaces. Data mining\napproaches, combined with statistical significance tests that\ndoes not require a huge amount of surrogate data, may provide some of the answers. In this paper we have presented\nan approach that significantly more efficient than existing\nmethods and should lay the foundations for more efficient\ndecoding of neural signals and hence achieve better braincomputer interfaces.\n\n6.\n\nACKNOWLEDGEMENTS\n\nUnnikrishnan's work was supported in part by NIH grant\nU54DA021519.\n\n7.\n\nREFERENCES\n\n[1] M. Abeles and I. Gat. Detecting precise firing\nsequences in experimental data. Journal of\nNeuroscience Methods, 107:141\u2013154, May 2001.\n[2] E. N. Brown, R. E. Kass, and P. P. Mitra. Multiple\nneural spike train data analysis: state-of-the-art and\nfuture challenges. Nature Neuroscience, 7:456\u2013461,\nMay 2004.\n[3] C. Diekman, P. S. Sastry, and K. P. Unnikrishnan.\nStatistical significance of sequential firing patterns in\nmulti-neuronal spike trains. Journal of Neuroscience\nMethods, 182.\n[4] J. P. Donoghue. Bridging the brain to the world: A\nperspective on neural interface systems. Neuron,\n60:511.\n[5] S. Gr\u00fcn, M. Diesmann, and A. Aertsen. Unitary\nevents in multiple single-neuron spiking activity: I.\ndetection and significance. Neural Computation, 14.\n[6] S. Grun. Data-driven significance estimation for\nprecise spike correlation. Journal of Neurophysiology,\n101:1126.\n[7] S. Laxman and P. S. Sastry. A survey of temporal\ndata mining. SADHANA, Academy Proceedings in\nEngineering Sciences, 31(2):173\u2013198, 2006.\n[8] S. Laxman, P. S. Sastry, and K. P. Unnikrishnan.\nDiscovering frequent episodes and learning hidden\nmarkov models: A formal connection. IEEE\nTransactions on Knowledge and Data Engineering,\n17(11):1505\u20131517, 2005.\n[9] S. Laxman, P. S. Sastry, and K. P. Unnikrishnan. A\nfast algorithm for finding frequent episodes in event\nstreams. In Proc. ACM SIGKDD International\nConference on Knowledge Discovery and Datamining,\nSan Jose, USA, August 2007.\n[10] F. Morchen. Unsupervised pattern mining from\nsymbolic temporal data. SIGKDD Exploration,\n9:41\u201355, 2007.\n\n[11] C. Omar, A. Akce, M. Johnson, T. Bretl, R. Ma,\nE. Maclin, M. McCormick, and T. P. Coleman. A\nfeedback information-theoretic approach to the design\nof brain-computer interfaces. International Journal on\nHuman-Computer Interaction, special issue on\n\"Current Trends in Brain-Computer Interface (BCI)\nResearch and Development\".\n[12] D. Patnaik, P. S. Sastry, and K. P. Unnikrishnan.\nInferring neuronal network connectivity from spike\ndata: A temporal datamining approach. Scientific\nProgramming, 16(1):49\u201377, 2008.\n[13] G. Pipa, D. W. Wheeler, W. Singer, and D. Nikolic.\nNeuroxidence: Reliable and efficient analysis of an\nexcess or deficiency of joint spike events. Journal of\nComputational Neuroscience, 25(1):64\u201388, 2008.\n[14] P. S. Sastry and K. P. Unnikrishnan. Conditional\nprobability-based significance tests for sequential\npatterns in multineuronal spike trains. Neural\nComputation, 22:1025\u20131059, 2010.\n\n\f"}