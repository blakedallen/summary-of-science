{"id": "http://arxiv.org/abs/1108.5703v1", "guidislink": true, "updated": "2011-08-26T07:02:35Z", "updated_parsed": [2011, 8, 26, 7, 2, 35, 4, 238, 0], "published": "2011-08-26T07:02:35Z", "published_parsed": [2011, 8, 26, 7, 2, 35, 4, 238, 0], "title": "Web Pages Clustering: A New Approach", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.1388%2C1108.5427%2C1108.5345%2C1108.1465%2C1108.0193%2C1108.2659%2C1108.2904%2C1108.2318%2C1108.2867%2C1108.1733%2C1108.5799%2C1108.0897%2C1108.0666%2C1108.0682%2C1108.2883%2C1108.1674%2C1108.0774%2C1108.4146%2C1108.1407%2C1108.1340%2C1108.1476%2C1108.2109%2C1108.1386%2C1108.0158%2C1108.0925%2C1108.5929%2C1108.5870%2C1108.3411%2C1108.3391%2C1108.5135%2C1108.4359%2C1108.2386%2C1108.5220%2C1108.5360%2C1108.4682%2C1108.5703%2C1108.4929%2C1108.3436%2C1108.5219%2C1108.3561%2C1108.4626%2C1108.2798%2C1108.4812%2C1108.3083%2C1108.0033%2C1108.6142%2C1108.1637%2C1108.3549%2C1108.5148%2C1108.4411%2C1108.4925%2C1108.3267%2C1108.1318%2C1108.1040%2C1108.4614%2C1108.2641%2C1108.0828%2C1108.5021%2C1108.3980%2C1108.1337%2C1108.1612%2C1108.2238%2C1108.5275%2C1108.1025%2C1108.0056%2C1108.0885%2C1108.0909%2C1108.5898%2C1108.1811%2C1108.4522%2C1108.2536%2C1108.0454%2C1108.4351%2C1108.5272%2C1108.5676%2C1108.5458%2C1108.4752%2C1108.5932%2C1108.2626%2C1108.5563%2C1108.6300%2C1108.1046%2C1108.2546%2C1108.5837%2C1108.3388%2C1108.3402%2C1108.1425%2C1108.1524%2C1108.1793%2C1108.2994%2C1108.1253%2C1108.2746%2C1108.4362%2C1108.3406%2C1108.3032%2C1108.6117%2C1108.2416%2C1108.3982%2C1108.3706%2C1108.3858%2C1108.4027&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Web Pages Clustering: A New Approach"}, "summary": "The rapid growth of web has resulted in vast volume of information.\nInformation availability at a rapid speed to the user is vital. English\nlanguage (or any for that matter) has lot of ambiguity in the usage of words.\nSo there is no guarantee that a keyword based search engine will provide the\nrequired results. This paper introduces the use of dictionary (standardised) to\nobtain the context with which a keyword is used and in turn cluster the results\nbased on this context. These ideas can be merged with a metasearch engine to\nenhance the search efficiency.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1108.1388%2C1108.5427%2C1108.5345%2C1108.1465%2C1108.0193%2C1108.2659%2C1108.2904%2C1108.2318%2C1108.2867%2C1108.1733%2C1108.5799%2C1108.0897%2C1108.0666%2C1108.0682%2C1108.2883%2C1108.1674%2C1108.0774%2C1108.4146%2C1108.1407%2C1108.1340%2C1108.1476%2C1108.2109%2C1108.1386%2C1108.0158%2C1108.0925%2C1108.5929%2C1108.5870%2C1108.3411%2C1108.3391%2C1108.5135%2C1108.4359%2C1108.2386%2C1108.5220%2C1108.5360%2C1108.4682%2C1108.5703%2C1108.4929%2C1108.3436%2C1108.5219%2C1108.3561%2C1108.4626%2C1108.2798%2C1108.4812%2C1108.3083%2C1108.0033%2C1108.6142%2C1108.1637%2C1108.3549%2C1108.5148%2C1108.4411%2C1108.4925%2C1108.3267%2C1108.1318%2C1108.1040%2C1108.4614%2C1108.2641%2C1108.0828%2C1108.5021%2C1108.3980%2C1108.1337%2C1108.1612%2C1108.2238%2C1108.5275%2C1108.1025%2C1108.0056%2C1108.0885%2C1108.0909%2C1108.5898%2C1108.1811%2C1108.4522%2C1108.2536%2C1108.0454%2C1108.4351%2C1108.5272%2C1108.5676%2C1108.5458%2C1108.4752%2C1108.5932%2C1108.2626%2C1108.5563%2C1108.6300%2C1108.1046%2C1108.2546%2C1108.5837%2C1108.3388%2C1108.3402%2C1108.1425%2C1108.1524%2C1108.1793%2C1108.2994%2C1108.1253%2C1108.2746%2C1108.4362%2C1108.3406%2C1108.3032%2C1108.6117%2C1108.2416%2C1108.3982%2C1108.3706%2C1108.3858%2C1108.4027&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The rapid growth of web has resulted in vast volume of information.\nInformation availability at a rapid speed to the user is vital. English\nlanguage (or any for that matter) has lot of ambiguity in the usage of words.\nSo there is no guarantee that a keyword based search engine will provide the\nrequired results. This paper introduces the use of dictionary (standardised) to\nobtain the context with which a keyword is used and in turn cluster the results\nbased on this context. These ideas can be merged with a metasearch engine to\nenhance the search efficiency."}, "authors": ["Jeevan H E", "Prashanth P P", "Punith Kumar S N", "Vinay Hegde"], "author_detail": {"name": "Vinay Hegde"}, "author": "Vinay Hegde", "arxiv_comment": "Clustering, concept mining, information retrieval, metasearch engine", "links": [{"href": "http://arxiv.org/abs/1108.5703v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1108.5703v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1108.5703v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1108.5703v1", "journal_reference": null, "doi": null, "fulltext": "INTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\n\nWeb Pages Clustering: A New Approach\nJeevan H E #1, Prashanth P P #2, Punith Kumar S N #3, Vinay Hegde #4\n#\n\nDept. of Computer Science and Engineering, RV College of Engineering,\nBangalore, Karnataka, India\n\nAbstract-The rapid growth of web has resulted in vast\nvolume of information. Information availability at a\nrapid speed to the user is vital. English language (or\nany for that matter) has lot of ambiguity in the usage of\nwords. So there is no guarantee that a keyword based\nsearch engine will provide the required results. This\npaper introduces the use of dictionary (standardised)\nto obtain the context with which a keyword is used and\nin turn cluster the results based on this context. These\nideas can be merged with a metasearch engine to\nenhance the search efficiency.\n\nthree other search engines, including Google, Yahoo,\nand Bing. The results from these search engines are\ncombined to find the most relevant pages. The\nadvantage is obvious. People can fast identify the\ninformation they need.\nIn this paper we propose a simple and effective\nmethod to cluster web pages and extract concepts\nfrom a keyword. We also introduce an improved\nranking algorithm for metasearch engines.\n\n\u0003\nII.\n\nClustering can be considered the most important\nunsupervised learning problem; so, as every other\nproblem of this kind, it deals with finding a structure in\na collection of unlabeled data.\nA loose definition of clustering could be \"the\nprocess of organizing objects into groups whose\nmembers are similar in some way\".\nA cluster is therefore a collection of objects which\nare \"similar\" between them and are \"dissimilar\" to the\nobjects belonging to other clusters as defined in [2].\nWeb pages clustering, in particular, mean\nremoving irrelevant links from the obtained results.\nThe result from multiple search engines is processed\nto obtain the final search result page. The result\nwhich appears in results of more search engines will\nbe listed above the others.\n\nINTRODUCTION\nAs information availability increases with the\ngrowth of the web, the number of users who want to\nretrieve that information also increases. This has led\nto the rise of search engines. A search engine\ntypically is based on a keyword as a query, uses this\nto search its indexed database which has data about\ndifferent web sites and their content and presents the\nresults to the user. But users still find it fairly difficult\nto find the exact information required by them, even\nthough it may be present in the web. There are\nvarious reasons for this.\nOne reason for this is that many users search the\nInternet with keywords that are ambiguous to certain\ndegree.\nFor example : If one searches for \"keyboard\" in a\nsearch engine expecting sites containing information\nabout the musical instrument, he gets a list that is a\nmix of links to pages containing information about\ntyping keyboard and musical instrument.\nToday we have many sophisticated search engines\nlike Google, Yahoo, Bing etc. But still we are not\nguaranteed of accurate search results. Apart from the\nabove mentioned reason, it may also be due to the\nfact that a single search engine may not be able to\nindex the entire web which has grown to such a large\nextent. Every day thousands of new web sites are\ncreated and millions of existing pages get updated.\nTo keep track of every such detail is impossible.\nIn order to solve this problem, many meta-search\nengines emerge such as, Excite, WebCrawler and so\non, which make further processing of search results\ngathered from many existing search engines as\nexplained in [1]. For example Excite issues queries to\nI.\n\nB. Concept Mining\nConcept mining is an activity that results in the\nextraction of concepts from artefacts. Solutions to the\ntask typically involve aspects of artificial intelligence\nand statistics, such as data mining and text mining.\nBecause artefacts are typically a loosely structured\nsequence of words and other symbols (rather than\nconcepts), the problem is nontrivial, but it can provide\npowerful insights into the meaning, provenance and\nsimilarity of documents.\nThe idea is to use the dictionary available in the\nInternet to determine the different contexts in which\nthe keyword can appear, that is, the same keyword\nexplaining different concepts.\nIII.\n\n42\n\n\u0003\n\nWEB PAGES CLUSTERING AND CONCEPT MINING\n\nA. Web Pages Clustering\n\nKeywords: Clustering, concept mining, information\nretrieval, metasearch engine\n\nUSE OF DICTIONARY\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nprovided it is sophisticated enough to provide the\nresults with minimum delay and can be updated\nregularly.\nOne problem with this is the use of multiword\nqueries. In this case, it may still be possible to get the\nmeaning of each word of the query from the\ndictionary, but constructing a new query from that will\nbe a problem. Different solutions can be provided for\nthe same. The algorithm may be designed to select\nonly one word for querying, based on the number of\nmeanings retrieved for each word in the multiword\nquery. The word with maximum number of different\nmeanings can be used. Another solution is to perform\na quick concept mining from the multi word query and\nobtain a single word query. For Example, a query\nsuch as \"Where is Bangalore\", can be reduced to just\n\"Bangalore\"\nAnother problem with the use of dictionary is for\nthe queries that involve proper nouns. The dictionary\nis not expected to provide results for these. Even\nproper nouns can be ambiguous to some extent. For\nexample consider \"Sachin\". This could refer to cricket\nplayer Sachin Tendulkar or any other individual with\nthe same name ( music director Sachin Dev\nBurman), resolving such ambiguities is non-trivial and\nmay require more input from the user itself. One\napproach to remove such ambiguities is to use the\nhistory of searches by the same user [5]. This can\ninherently point to a certain context. In this case if the\nuser had earlier searched things about sports, then\nthe probability is more that the query \"Sachin\" meant\n\"Sachin Tendulkar\". This requires data mining and\nstatistical analysis of previous data available.\n\nConcept mining as mentioned earlier involves\nArtificial Intelligence. Extracting concepts from short\ntext snippets retrieved from the search results may\nnot be accurate enough. To achieve good amount of\naccuracy, we may require the entire text to be\navailable. Hence it can be computationally intensive\nand consume high bandwidth to function at an\nacceptable speed [3]. For the internet environment, a\nbetter solution can be to use a dictionary. A dictionary\ncan be used for the queries that the user gives. Each\nambiguous word will lead to multiple meanings\nobtained from the dictionary. Based on these multiple\nmeanings clusters can be done for each type of\nresult.\nThis clustering can be done in two ways. One is to\nprocess the search results. Compare the context of\nthe results with the meanings retrieved from the\ndictionary. This is again not straightforward and\nrequires considerable data mining techniques [4].\nHence we propose a simple alternative but an\nefficient technique. The technique is to submit the\nmeanings retrieved itself as queries to the search\nengine. This eliminates the need for any data mining\nalgorithm. Each result retrieved already belongs to a\nparticular cluster (the meaning used for searching).\nSo this eliminates the need for a clustering algorithm.\nNow consider a query such as \"Bank\". The dictionary\ncan provide meanings such as financial institution,\nsides of a water body and rely upon. The search\nengine can resolve the ambiguity by forming three\nclusters of results, one for each meaning. The\nmeaning itself is sent to the search engine as a\nquery. Further, the results can be improved by\nconcatenating the user query and the meaning and\nmaking it a single new query. In this case it can be\n\"Bank financial institution\".\n\nMETASEARCH ENGINE\nA metasearch engine is a search tool that sends\nuser requests to several other search engines and/or\ndatabases and aggregates the results into a single\nlist and provides it to the user in way similar to any\nother search engine. The concept of metasearch\nengine arises from the fact that the web is too large\nfor one search engine to index it completely and\nmore comprehensive results can be obtained by\ncombining the results of various search engines [6].\nThe obvious advantage of this technique is that the\nsearch space is more i.e. more web pages are\ncovered. Since a metasearch engine has to deal with\ndifferent search engines, it requires a parsing stage\nto convert the results from all the search engines into\na uniform manner. The implementation can typically\ninvolve XML and HTML parsing.\nThe usage of a metasearch engine must be done\nin an intelligent manner to extract the maximum\nbenefit out of it. The ranking of results is very crucial\nto provide the user with the required information in\nminimum time. A straightforward algorithm that can\nbe adopted to provide a well refined search result is\ngiven below. The underlying assumption is that a few\nIV.\n\n//Module to retrieve meanings from a dictionary\n//Input- user query \u2013 string\n//Output- list of meanings\nDictionary (String query)\ndo\nmeanings = getFromDictionary (query);\nfor each meaning from the dictionary\ndo\nAddToList (list, meaning)\nend\nif (list is NULL) // no meaning found\n// query may be a noun\nAddToList (list, query)\nreturn list\nend\nWhen it comes to implementation of the same, the\ndictionary can be maintained either online or offline.\nAn online dictionary such as that of the WorldNet is a\nbetter choice, since it is updated regularly and is\nwidely accepted standard dictionary. On the other\nhand an offline, local dictionary is also possible,\n43\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nresults will be same, from all the search engines.\nHere we consider the count of each result link from\nall the search engines used. Then rank it, based on\nthe decreasing order of the count.\n\nneeded upon querying. We also did an experimental\nimplementation of the same ideas, which performed\nto meet our expectations of speed and efficiency.\nIt can be said that providing context sensitive\nresults increases the efficiency of the user, so that he\ncan easily find the document he is searching for in\nthe web.\nCurrent keyword based search engines rank the\nweb pages based on frequency of the keywords,\ninbound link count etc. Hence these results require\nuser to go through all the returned links for finding the\nright one. With the use of a metasearch engine the\nrelevance of results is also high, since it uses multiple\nsearch engines like Google, Yahoo and Bing. The\nlinks that appear in most of search engines' results\nare given higher priority.\nFurther enhancements include support for queries\nfrom languages other than English, enabling caching\nmechanism for recently queried keywords and\nmoving forward to implement the above idea for\nimage searching as well as video searching.\n\n//Module to search and unify the results\n//Performs ranking based on the count\n//Input: user query \u2013 string\n//Output: list of browsable search results\nMetaSearchEngine (query)\ndo\nSubmit the search query to the search engines\nfor each search engine\ndo\nfor each result_link from the given\nsearch engine\ndo\nif (Final_Results has result_link)\n// increment count\nSetCount (result_link, getCount\n(result_link) +1)\nelse\n//add it to result list and set count to 1\nAddToFinalResults(result_link)\nSetCount(result_link, 1)\nend\nend\nSort the Final_Results in the decreasing order of\nthe\ncount of the result\nDisplay the search results in this order\n\nACKNOWLEDGMENT\nWe would like to thank. Dr. T. M. Rangaswamy,\nProfessor, IEM department, R.V College of\nEngineering for providing support and guidance for\nthe study and research regarding the subject.\nREFERENCES\n[1] Fang Li, Martin Mehlitz, Li Feng and Huange Sheng,\"Web\n[2]\n\nend\n\n[3]\n\nThe use of this approach provides a far more\nefficient ranking than simply performing a union of all\nthe results. Moreover it's a simple approach and\neasily implementable. This ranking can also be done\non client side (using client side scripting). Hence it\nprovides a flexible approach for implementation.\nExperimental implementation of the same technique\nhas been done, with a good amount of success.\n\n[4]\n[5]\n[6]\n\nCONCLUSION\nThe paper proposed a new basis for web pages\nclustering and concept extraction from a keyword\nbased on results of multiple search engines on the\nInternet. It will help user to get relevant information\nV.\n\n44\n\n\u0003\n\nPages Clustering and Concept\nMining- An approach\ntowards intelligent information retrieval\", 2006.\nOren Zamir and Oren Etzioni, Department of Computer\nScience and Engineering, University of Washington,\" Web\nDocument Clustering: A Feasibility Demonstration\".\nDavid A Grossman and Ophir Frieder,\" Information Retrieval\n\u2013 Algorithms and Heuristics\", 2004.\nJiawei Han and Micheline Kamber, \"Data mining: concepts\nand techniques\", 2006.\nY Taher H. Haveliwala, Aristides Gionis and Piotr\nIndyk,\"Scalable Techniques for Clustering the Web\".\nMike Perkowitz and Oren Etzioni, Department of Computer\nScience and Engineering, Box 352350, University of\nWashington, Seattle,\"Towards adaptive Web sites:\nConceptual framework and case study\".\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\n\nA Performance Study of Data Mining\nTechniques: Multiple Linear Regression vs.\nFactor Analysis\n\u0003\n\u0004\u010f\u015a\u015d\u0190\u015a\u011e\u016c\u0003d\u0102\u0176\u011e\u0169\u0102\u0355\u0003Z\u0358<\u0358\u0012\u015a\u0102\u01b5\u015a\u0102\u0176\u0003\n\u0004\u0190\u0190\u015d\u0190\u019a\u0102\u0176\u019a\u0003W\u018c\u017d\u0128\u011e\u0190\u0190\u017d\u018c\u0355\u0003\u0018\u011e\u0189\u019a\u0358\u0003\u017d\u0128\u0003\u0012\u017d\u0175\u0189\u01b5\u019a\u011e\u018c\u0003^\u0110\u0358\u0003\u0398\u0003\u0004\u0189\u0189\u016f\u015d\u0110\u0102\u019a\u015d\u017d\u0176\u0190\u0355\u0003\u0018/Dd\u0355\u0003<\u01b5\u018c\u01b5\u016c\u0190\u015a\u011e\u019a\u018c\u0102\u0003\nW\u018c\u017d\u0128\u011e\u0190\u0190\u017d\u018c\u0355\u0003\u0018\u011e\u0189\u019a\u0358\u0003\u017d\u0128\u0003\u0012\u017d\u0175\u0189\u01b5\u019a\u011e\u018c\u0003^\u0110\u0358\u0003\u0398\u0003\u0004\u0189\u0189\u016f\u015d\u0110\u0102\u019a\u015d\u017d\u0176\u0190\u0355<\u01b5\u018c\u01b5\u016c\u0190\u015a\u011e\u019a\u018c\u0102\u0003h\u0176\u015d\u01c0\u011e\u018c\u0190\u015d\u019aLJ\u0355\u0003<\u01b5\u018c\u01b5\u016c\u0190\u015a\u011e\u019a\u018c\u0102\u0003\nKeywords: Data mining, Multiple Linear Regression,\nFactor Analysis, Principal Component Regression,\nMaximum Liklihood Regression, Generalized Least\nSquare Regression\n\nAbstract:The growing volume of data usually\ncreates an interesting challenge for the need of data\nanalysis tools that discover regularities in these\ndata. Data mining has emerged as disciplines that\ncontribute tools for data analysis, discovery of\nhidden knowledge, and autonomous decision\nmaking in many application domains. The purpose\nof this study is to compare the performance of two\ndata mining techniques viz., factor analysis and\nmultiple linear regression for different sample sizes\non three unique sets of data. The performance of the\ntwo data mining techniques is compared on\nfollowing parameters like mean square error (MSE),\nR-square, R-Square adjusted, condition number,\nroot mean square error(RMSE), number of variables\nincluded in the prediction model, modified\ncoefficient of efficiency, F-value, and test of\nnormality. These parameters have been computed\nusing various data mining tools like SPSS, XLstat,\nStata, and MS-Excel. It is seen that for all the given\ndataset, factor analysis outperform multiple linear\nregression. But the absolute value of prediction\naccuracy varied between the three datasets\nindicating that the data distribution and data\ncharacteristics play a major role in choosing the\ncorrect prediction technique.\n\n1. Data Introduction\nA basic assumption concerned with general linear\nregression model is that there is no correlation (or\nno multi-collinearity) between the explanatory\nvariables. When this assumption is not satisfied,\nthe least squares estimators have large variances\nand become unstable and may have a wrong sign.\nTherefore, we resort to biased regression\nmethods, which stabilize the parameter estimates\n[17]. The data sets we have chosen for this study\nhave\na\ncombination\nof\u0003 \u019a\u015a\u011e\u0003 \u0128\u017d\u016f\u016f\u017d\u01c1\u015d\u0176\u0150\u0003\ncharacteristics: few predictor variables, many\npredictor variables, highly collinear variables, very\nredundant variables and presence of outliers.\nThe three data sets used in this paper viz.,\nmarketing, bank and parkinsons telemonitoring\ndata set are taken from [8],[9], and [10]\nrespectively.\n\nFrom the foregoing, it can be observed that each of these three sets has unique properties. The marketing\ndataset consists of 14 demographic attributes. The dataset is a good mixture of categorical and continuous\nvariables with a lot of missing data. This is characteristic for data mining applications.\n\nFig 1 Box Plot of Marketing Dataset\n\nFig 2: Box Plot of Parkinson Dataset\n\nThe bank dataset is synthetically generated from a simulation of how bank-customers choose their banks.\nTasks are based on predicting the fraction of bank customers who leave the bank because of full queues.\n45\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nEach bank has several queues, that open and close according to demand. The tellers have various\naffectivities, and customers may change queue, if their patience expires.\n\nFig 3: Box Plot of Bank Dataset\nIn the rej prototasks, the object is to predict the rate\nof rejections, i.e., the fraction of customers that are\nturned away from the bank because all the open\ntellers have full queues. This dataset consists of 32\ncontinuous attributes and having 4500 records.\nThe parkinsons telemonitoring data set is composed\nof a range of biomedical voice measurements from\n42 people with early-stage Parkinson's disease\nrecruited to a six-month trial of a telemonitoring\ndevice for remote symptom progression monitoring.\nThe recordings were automatically captured in the\npatient's homes.\nColumns in the table contain\nsubject number, subject age, subject gender, time\ninterval from baseline recruitment date, motor\nUPDRS, total UPDRS, and 16 biomedical voice\nmeasures. Each row corresponds to one of 5,875\nvoice recording from these individuals. The main aim\nof the data is to predict the total UPDRS scores\n('total_UPDRS') from the 16 voice measures. This is\na multivariate dataset with 26 attributes and 5875\ninstances. All the attributes are either integer or real\nwith lots of missing and outlier values.\nThe box plot of the three datasets (fig 1 to fig.3)\nshown above display measure of dispersion between\nthese variables, compares the mean of different\nvariables, and also shows the outliers in three\ndatasets. In this regard, it becomes necessary to\nscale these three datasets to reduce the measure of\ndispersion and bring all the variables of all datasets\nto the same unit of measure.\n2. Prediction Techniques\nThere are many prediction techniques (association\nrule analysis, neural networks, regression analysis,\ndecision tree, etc.) but in this study only two linear\nregression techniques have been compared.\n\nor, equivalently, in more compact matrix terms:\nY = Xb + E\nwhere, for all the n considered observations, Y is a\ncolumn vector with n rows containing the values of\nthe response variable; X is a matrix with n rows and k\n+ 1 columns containing for each column the values of\nthe explanatory variables for the n observations, plus\na column (to refer to the intercept) containing n\nvalues equal to 1; b is a vector with k + 1 rows\ncontaining all the model parameters to be estimated\non the basis of the data: the intercept and the k slope\ncoefficients relative to each explanatory variable.\nFinally E is a column vector of length n containing the\nerror terms. In the bivariate case the regression\nmodel was represented by a line, now it corresponds\nto a (k + 1)-dimensional plane, called the regression\nplane. This plane is defined by the equation\n\u0177i= a + b1xi1 + b2xi2 + \u30fb\u30fb \u30fb+bkxik+\u03bci\nWhere \u0177i is dependent variable. Xi's are independent\nvariables, and \u03bci is stochastic error term. We have\ncompared three basic methods under this multiple\nlinear regression technique. They are full method\n(which uses the least square approach), forward\nmethod, and stepwise approach (which used\ndiscriminant approach or all possible subsets) [5].\n2.2 Factor Analysis\nFactor analysis attempts to represent a set of\nobserved variables X1, X2 .... Xn in terms of a number\nof 'common' factors plus a factor which is unique to\neach variable. The common factors (sometimes\ncalled latent variables) are hypothetical variables\nwhich explain why a number of variables are\n\n2.1 Multiple Linear Regression\nMultiple linear regression model maps a group of\npredictors x to a response variable y [4]. The multiple\nlinear regression is defined by the following\nrelationship, for i = 1, 2, n:\nyi = a + b1xi1 + b2xi2 + \u30fb\u30fb \u30fb+bkxik + ei\n46\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\ncorrelated with each other- it is because they have\none or more factors in common [7].\n\ncomponent analysis, generalized least square and\nmaximum likelihood estimation.\n\nFactor analysis is basically a one-sample procedure\n[6]. We assume a random sample y1, y2, yn from a\nhomogeneous population with mean vector \u03bc and\n\n3. Related Work\nThere are many data mining techniques (decision\ntree, neural networks, regression, clustering etc.) but\nin this paper we have compared two linear\ntechniques viz., multiple linear regression, and factor\nanalysis. In this domain there have been many\nresearchers and authors who compared various data\nmining techniques from varied aspects.\n\ncovariance matrix \u2211 . The factor analysis model\nexpresses each variable as a linear combination of\nunderlying common factors f1, f2, . . . , fm, with an\naccompanying error term to account for that part of\nthe variable that is unique (not in common with the\nvariables). For y1, y2, yp in any observation vector y,\nthe model is as follows:\n\nIn year 2004 Munoz et. al did a comparison of three\ndata mining methods: linear statistical methods,\nneural network method, and non-linear multivariate\nmethods [11]. In 2008, Saikat and Jun Yan\ncompared PCA and PLS on simulated data [12].\nMunoz et.al compared logistic regression, principal\ncomponent regression, and classification and\nregression tree with multivariate adaptive regression\nspines [16]. In 1999, Manel et.al compared\ndiscriminate analysis, neural networks, and logistic\nregression for predicting species distribution [13]. In\nyear 2005, Orsalya et. al compared ridge regression,\npair wise correlation method, forward selection, best\nsubset selection, on quantitative structure retention\nrelationship study based on multiple linear regression\non predicting the retention indices for aliphatic\nalcohols[14]. In year 2002 Huang et. al compared\nleast square regression, ridge and partial least\nsquare in the context of the varying calibration data\nsize using only squared prediction errors as the only\nmodel comparison criteria [15].\n\ny1 \u2212 \u03bc1 = \u03bb11 f1 + \u03bb12 f2 +* * *+\u03bb1m fm + \u03b51\ny2 \u2212 \u03bc2 = \u03bb21 f1 + \u03bb22 f2 +* * *+\u03bb2m fm + \u03b52\n...\nyp \u2212 \u03bcp = \u03bbp1 f1 + \u03bbp2 f2 +* * *+\u03bbpm fm + \u03b5p.\n\nIdeally, m should be substantially smaller than p;\notherwise we have not achieved a parsimonious\ndescription of the variables as functions of a few\nunderlying factors. We might regard the f's in\nequations above as random variables that engender\nthe y's. The coefficients \u03bbij are called loadings and\nserve as weights, showing how each yi individually\ndepends on the f 's. With appropriate assumptions, \u03bbij\nindicates the importance of the jth factor fj to the ith\nvariable yi and can be used in interpretation of fj. We\ndescribe or interpret f2, for example, by examining its\ncoefficients, \u03bb12, \u03bb22, \u03bbp2. The larger loadings relate f2\nto the corresponding y's. From these y's, we infer a\nmeaning or description of f2. After estimating the \u03bbij\n's, it is hoped they will partition the variables into\ngroups corresponding to factors. There is superficial\nresemblance to the multiple linear regression, but\nthere are fundamental differences. For example,\nfirstly f's in above equations are unobserved,\nsecondly\nequations\nabove\nrepresents\none\nobservational vector, whereas multiple linear\nregression depicts all n observations.\n\n4. Preparation and Methodology\nBoth the techniques under study are linear in nature\nand the choice of technique is vital for getting\nsignificant results. When a nonlinear data are fitted to\na linear technique, the results obtained are biased\nand when linear data are fitted to a non-linear\ntechnique, the results have increased variance. As\nthe techniques undertaken for this study are both\nlinear, so to get significant results we need to apply\nthe same on linear data sets. Both the techniques\nare linear regression techniques, we mean that they\nare linear in parameters [1] [2]; the \u03b2 's (that is, the\nparameters are raised to the first power only. It may\nor may not be linear in explanatory variables, the X's.\nTo make our data sets linear it is preprocessed by\ntaking natural log of all the instances of the data sets\nor normalized using z-score [3] normalization. After\n\nThere are a number of different varieties of factor\nanalysis: the comparison here is limited to principal\n47\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nscaling and standardizing the three datasets, it is\nfound that skewness is reduced that is shown by\nhistogram diagram of all three datasets. For proving\nlinearity of these data sets box-plot, histogram and\nJB Test (Jarque Bera Test) with p-value (exact\nsignificance level or probability value of committing\ntype-I error) have been used.\nAfter scaling and standardizing the data sets are\ndivided into two parts, taking 70% observations as\nthe \"training set\" and the remaining 30%\nobservations as the \"test validation set\"[3]. For each\ndata set training set is used to build the model and\nvarious methods of that technique are employed. For\nexample in Multiple Linear Regression (MLR), three\nmethods are associated in this study: the full model,\nforward model and stepwise model. The model is\nvalidated using test validation data set and the\nresults are presented using ten goodness of fit\ncriteria. Both the techniques are intra and inter\n\ncompared for their performance on the underlying\nthree unique datasets.\n5. Interpretation and Findings\nRefer to table 1and table 2 given below.\n5.1 Interpreting Marketing Dataset\n2\n\n2\n\nIn marketing dataset, the value of R and Adj.R , of\nfull model was found with good explanatory power\ni.e., 0.47, which is higher than both stepwise and\nforward model.\nOn the behalf of this explanatory power value we can\nsay that among all methods of multiple linear\nregression, full model was found best method for\ndata mining purpose, since 47% change in variation\nin dependent variable was explained by independent\n\nTable 1\nThe inclusion of some other independent variables\n(either relevant or irrelevant) in multiple regression\nmodel\nmostly\ngenerate\nnon-decreasing\nexplanatory value or R2 value. In this case we can\n2\n2\nuse anther good measure of R i.e., Adj. R , which\naccounts for the effect of new explanatory\nvariables in the model, since it incorporate degree\nof freedom of the model, or denominator of the\nexplained and unexplained variation[18]. The\n\nvariables. But 0.47 value of explanatory power is\nnot significant up-to the mark which requires\nanother regression model than multiple regression\nmodel for reporting data set, since 0.53 means\n53% of the total variation was found unexplained.\nSo, within multiple regression techniques full\nmodel was found best but not up-to the mark.\n2\nValue of R suggest for using another regression\nmodel.\n48\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nexpression for the adjusted multiple determination\nis:\n2\n\nn \u22121\nn\u2212k\n\n2\n\nAdj. R = 1-(1-r )\n\n\uf8ee \u2211 ei\n\uf8ef\n\uf8ef\uf8f0 \u2211 y\n\n2\n\nAdj. R = 1-\n\nis large in relation to the sample observations Adj.\n2\n2\nR will be much smaller than R and can even\nassume negative values in which case Adj. R2\nshould be interpreted as being equal to zero.\n\n2\n\n2\n\n2\n\nFor marketing data set, all methods of multiple\nlinear regression Adj. R2 was found similar to R2\nvalue which means sample size is sufficiently large\nas required for data mining purpose [19].\n\nn \u2212 k)\uf8f9\n\uf8fa\n/( n \u2212 1) \uf8fa\n\uf8fb\n\n/(\n\n2\n\nIf n is large Adj. R and R will not differ much. But\nwith small samples, if the number of regressors X's\n\nTable 2\nThe R2 in case of marketing dataset for factor\nanalysis was found around 0.58. So, all methods\nhave equal explanatory power under factor\nanalysis. More over, under all methods viz., PCR,\nMaximum Likelihood, and GLS, explained variation\nis 58% out of total variation in the dependent\nvariable which signifies that factor analysis\nextraction is better than multiple linear regression.\n2\nR can also be estimated through the following\n2\n\nnotations:R =\n\nTSS = Explained Sum Square(ESS)+\nResidual Sum Square(RSS)\n2\n\nThe Adj. R i.e., adjusted for inclusion of new\nexplanatory variable was also found 0.56 less than\n2\nR . The 58% variation was captured due to\nregression, it explains the overall goodness of fit of\nthe regression line to marketing dataset due to use\nof factor analysis.\n\nESS\nTSS\n\nSo, on the behalf of first order statistical test (R2),\nwe can conclude that factor analysis technique is\n49\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nbetter than multiple regression technique due to\nexplanatory power.\n\nall b's are unbiased but with large variance. Due to\nlarge variance in factor analysis techniques the\nprobability value of unbiased-ness increases that\ngenerates a contradictory result about the\nexplanatory power of the factor analysis methods.\nBut factor analysis methods may have\nquestionable values of MSE, due to this reason\nnew measure of MSE that is RMSE (root mean\nsquare error) was used in the study.\n\nMean Square Error (MSE) criteria is a combination\nof unbiased-ness and the minimum variance\nproperty. An estimator is a minimum MSE\nestimator if it has smallest MSE, defined as the\nexpected value of the squared differences of the\nestimator around the true population parameter b.\nMSE( b\u0302 ) =E( b\u0302 -b)2 . It can be proved that it is\nequal to\n\nRMSE was found considerably similar in methods\nof both the techniques. Due to less variation in\nRMSE of both MLR and factor analysis of\nmarketing dataset it can be stated that both\ntechniques have equal weights for consideration.\n\nMSE( b\u0302 )'s\n=Var( b\u0302 )'s+bias ( b\u0302 )\n2\n\nA common measure used to compare the\nprediction performance of different models is Mean\nAbsolute Error (MAE).\n\nThe MSE criteria for unbiased-ness and minimum\nvariance were found increasing under multiple\nlinear regression models. It signifies that full\nmethod MSE is less than all model's MSE, which\nfurther means that under full model of multiple\nlinear regression of marketing dataset there is less\nunbiased-ness and less variance.\n\nIf Yp be the predicted dependent variable and Y be\nthe actual dependent variable then the MAE can\nbe computed by\n\nThe minimum variance also increases the\nprobability of unbiased-ness and gives better\nexplanatory power like R2 in marketing dataset.\n\nMAE=\n\n\u2211 Y \u2212Y p\n\nn\n\nY\n\nIn marketing dataset MAE was found less under\nfull model, which is less than stepwise and forward\nmodel. MAE signifies that full model under MLR\ntechniques give better prediction than other mode\n\nThe inter comparison of two techniques multiple\nlinear regression and factor analysis generated\nthat in factor analysis models MSE is significantly\ndifferent which signifies that under factor analysis\n\nTest Vs. Actual\n\nStepsise Predicted Vs. Actual\n\n14\n\n12\n\n12\nRange of Income\n\n1\n\n10\n\n10\n\n8\n\n8\n\nPredicted Income\n\n6\n\nActual Income\n\nStepwise predicted\n\n6\n\n4\n\nActual\n\n4\n\n2\n\n2\n\n0\n1\n\n209 417 625\n\n0\n\n833 1041 1249 1457 1665 1873 2081 2289 2497\n\n1\n\nNumber of Test Data Observations\n\nFig 5: MLR-Stepwise Model (Marketing)\n\nFig 4: MLR-Full Model (Marketing)\n\n50\n\n\u0003\n\n193 385 577 769 961 1153 1345 1537 1729 1921 2113 2305 2497 2689\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nForward Predicted V. Actual\n\n0\n-1\n\n14\n12\n\n108 215 322 429 536 643 750 857 964 1071 1178 1285\n\n-3\n\n10\n8\n\nForward predicted\nActual\n\n6\n\n-4\n\nrej_Actual\n\n-5\n\nPredicted\n\n-6\n\n4\n\n-7\n\n2\n\n-8\n\n0\n1\n\n-9\n\n251 501 7511001125115011751200122512501\n\nFig 6: MLR-Forward Model (Marketing)\n\nFig 7: MLR-Full Model (Bank Dataset)\n\n25\n\n25\n\n20\n\n20\n\n15\n\n15\n\n10\n\n10\n\n5\n\nPredicted Rej\n\n0\n\nrej\n\n-5\n\n1\n\n-2\n\n1\n\n5\n\nPredicted Backward\nrej\n\n0\n\n116 231 346 461 576 691 806 921 1036 1151 1266\n\n-5\n\n-10\n\n-10\n\n-15\n\n-15\n\n-20\n\n-20\n\n1\n\n129 257 385 513 641 769 897 1025 1153 1281\n\nFig 8: MLR-Forward Model (Bank Dataset)\n\nFig 9: MLR-Stepwise Model (Bank Dataset)\n\nFig 10: MLR-Full Model (Parkinson Dataset)\n\nFig 11: MLR-Forward Model (Parkinson Dataset)\n20\n\nStepwise Predicted Vs. Actual\n\n18\n16\n\n2\n\n14\n\n1\n\n12\n10\n\n0\n1\n-1\n\n112 223 334 445 556 667 778 889 1000 1111 1222 1333 1444 1555 1666\n\nPredicted\n\n8\n\nActual\n\n6\n\nGLS Actual\n\n4\n2\n\n-2\n-3\n\n0\n1\n\n-4\n\nFig 12: MLR-Stepwise Model (Parkinson Dataset)\n\n183 365 547 729 911 1093 1275 1457 1639 1821 2003 2185 2367 2549\n\nFig 13: Factor Analysis-GLS Model (Marketing Dataset)\n\n51\n\n\u0003\n\nGLS Predicted\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\n20\n\n18\n\n18\n16\n\n16\n14\n\n14\n\n12\n\n12\n10\n\nPCR Predicted\n\n10\n\n8\n\nMLHood Predicted\n\n8\n\nPCR Actual\n\nMLHood Actual\n\n6\n\n6\n\n4\n\n4\n2\n\n2\n\n0\n\n0\n1\n\n183 365 547 729 911 1093 1275 1457 1639 1821 2003 2185 2367 2549\n\n1\n\n192 383 574 765 956 1147 1338 1529 1720 1911 2102 2293 2484 2675\n\nFig 14: Factor Analysis-PCR Model (Marketing Dataset) Fig 15: Factor Analysis-Maximum\nLikelihood Model (Marketing Dataset)\n\n6\n\n20\n\n4\n\n15\n\n2\n\n10\n\n0\n-2 1\n\n93\n\n185 277 369 461 553 645 737 829 921 1013 1105 1197 1289\n\n5\n\nGLS Predicted\nGLS Actual\n\n-4\n\nPCR Predicted\nPCR Actual\n\n0\n\n-6\n\n-5\n\n1\n\n93\n\n185 277 369 461 553 645 737 829 921 1013 1105 1197 1289\n\n-8\n\n-10\n\n-10\n\n-15\n\n-12\n\nFig 16: Factor Analysis-GLS Model (Bank Dataset)\n\nFig 17: Factor Analysis-PCR Model (Bank Dataset)\n\n6\n\n2\n\n4\n\n1.5\n\n2\n\n1\n\n0\n-2\n\n0.5\n\n1\n\n100 199 298 397 496 595 694 793 892 991 1090 1189 1288\n\nMaximum Predicted\n-0.5\n\n-6\n\n-1\n\n-8\n\n-1.5\n\n-10\n\n-2\n\nFig 18: Factor Analysis-Maximum Likelihood\n\nActual Total UPDRS\n\n0\n\nMaximum Actual\n\n-4\n\n1\n\n140 279 418 557 696 835 974 1113 1252 1391 1530 1669\n\nPredicted PCR T.UPDRS\n\nFig 19: Factor Analysis-PCR Model (Parkinson\n\nModel (Bank Dataset)\n\nDataset)\n\n2\n\n2\n\n1.5\n1\n\n1\n0.5\n0\n-0.5 1\n-1\n\n0\n140 279 418 557 696 835 974 1113 1252 1391 1530 1669\n\n1\n\nActual Total UPDRS\n\n-1\n\nPredicted GLS T.UPDRS\n\n144 287 430 573 716 859 1002 1145 1288 1431 1574 1717\n\nActual Total UPDRS\nPredicted MLHood T.UPDRS\n\n-2\n\n-1.5\n-2\n\n-3\n\n-2.5\n-3\n\n-4\n\nFig 20: Factor Analysis-GLS Model (Parkinson Dataset)\n\nFig 21: Factor Analysis-Maximum\nLikelihood Model (Parkinson Dataset)\n\nUnder factor analysis marketing dataset MAE in all\nmodels was found considerably similar but higher\nthan multiple regression techniques, therefore we\ncan say factor analysis models for such kind of\ndatasets generate poor prediction performance.\n\nThe diagnosis index of multi collinearity was found\nsignificantly below 100 under MLR methods in\nmarketing dataset, which means there is no scope\nfor high and severe multi collinearity. In case of\nsame dataset condition number was found lower\n52\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nthan factor analysis technique. This means factor\nanalysis is better technique to diagnosis the effect\nof multi collinearity. But in marketing dataset both\nfactor analysis and MLR techniques were found\nwith less multi collinearity in regressors than\nsevere level of multi collinearity.\n\nsignificant, but MLR techniques parameters are\nsignificant with high variance.\nThe RMSE is also satisfactory and upto the mark\nin case of factor analysis. Therefore, we can say\nthat factor analysis parameters have low variance\nand unbiasness.\n\nThe F value in case of marketing dataset was\nfound more than critical value with respect to\ndF(degree of freedom), in both techniques, which\nsignifies that overall regression model is\nsignificantly estimated but stepwise model of MLR\ntechnique was found high F corresponding to its\ndF which means overall significance of the\nregression model was up-to the mark in case of\nstepwise method. The prediction plots of two\ntechniques on marketing dataset better represent\nabove discussion visually (see fig. 4-fig. 6 and\nfig. 13- fig. 15)\n\nThe prediction power of the regression model is\nalso found good fit in all factor analysis models. In\ncase of bank dataset MLR is having more MAE\ndue to test dataset skewness.\nModified coefficient of efficiency was found low in\ncase of factor analysis model in case of bank\ndataset, since this dataset does not satisfy the\ncenter limit theorem due to constant number of\nvariables; but in MLR model modifies coefficient of\nefficiency was found considerably significant for all\nmodels. This may be due to the successful\nimplementation of center limit theorem.\n\n5.2 Interpreting Bank Dataset\n\nIn case bank dataset the diagnosis index of multicollinearity was found higher in factor analysis than\nMLR, which signifies that factor analysis is better\ntechnique to identify multi-colinearity problem.\n\nIn case full model of bank dataset explanatory\npower (R2) was found considerably low due to\nresidual, whereas in stepwise and forward model\nMLR generated satisfactory explanatory power.\nDue to stepwise and forward model 56% variation\nin dependent variable was explained with respect\nto independent variables. Another measure of\nexplanatory power was also found satisfactory in\ncase of stepwise and forward model but not in full\nmodel.\n\nThe F value in case of bank dataset was found\nsignificant under MLR model but F value was\nfound very low rather in case of factor analysis\nwas found 200 times more than the critical value,\nwhich means overall significance of all factor\nanalysis model is higher than MLR model. The\nprediction plots of the two techniques (see fig. 7fig. 9 and fig. 16- fig. 18) corroborate our\ndiscussion.\n\nOn the other hand factor analysis models on bank\n2\ndataset generated higher value of both R and\n2\nadjusted R , which signifies that the explanatory\npower of factor analysis in case of bank dataset is\nmore than MLR technique. Overall one drastic\npoint was found that in all models of factor analysis\nand MLR, full model of MLR generated very poor\nR2 value, which means this dataset is not having\nproper specification according to magnitude\nchange.\n\n5.3 Interpreting of Parkinson Dataset\nIn case of Parkinson dataset forward model of\nMLR was found very low explanatory power, it is\ndue to hetroscedasticity in stochastic error term of\nthe model, but the full and the stepwise model was\nfound to have 90% explanatory power of the\nmodel. In all models of factor analysis R2 was\nfound to have 60%, which is considerably sufficient\nfor satisfactory explanatory power of the model.\nMoreover adjusted R2 was found similar in both\ntechniques i.e., MLR and factor analysis, due to no\nintrapolation.\n\nThe MSE criteria for unbiasness and minimum\nvariance for all parameters is found increasing\nunder both factor analysis and MLR techniques,\nbut all models of factor analysis are found with low\nunbiasness and variance than all models of MLR.\nIt means both the technique parameters are\n\n53\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\nIn case of MLR models on Parkinson dataset MSE\nwas found low and up-to the mark, which signifies\nthat MLR technique is better technique for the\nextraction of structural parameters with unbiasness\nand low variance. On the other hand factor\nanalysis was found having high biasness and high\nvariance for extracting structural parameters of the\nmodel.\n\nThe analysis of linear techniques (MLR and Factor\nAnalysis) suggests that factor analysis is\nconsiderably better technique than MLR. The\nprincipal component model extracted good\nperformance on all datasets of the study. The good\nperformance is said on the basis of higher\nexplanatory power, higher goodness of fit, and\nhigher prediction power.\n\nRMSE was found similar in all models of MLR and\nfactor analysis which signifies the same\nconsideration for unbiasness and variance.\n\nIn diagnosis of multi-colinearity PCR model of\nfactor analysis was found better model. However,\nfull model of MLR also extracted satisfactory\nresult. All other models of both the techniques\nwere found with high explanatory power but with\nmoderate prediction power.\n\nThe prediction power (MAE) of two models of\nfactor analyis viz. PCR and maximum likelihood\nwas found significant but GLS model prediction\npower was found considerably higher than PCR\nand maximum likelihood methods. On the other\nhand MLR prediction power was found significantly\ndifferent in all three models. In case of stepwise\nand forward models prediction power increased\nmore than full model.\n\nAll models are best fit from the point of view of\nlinearity and unbiased ness due to moderate\nvariance and heteroscedasticity, distribution of\nresidual term. Their prediction power was found\nconsiderably moderate fit.\nFrom the point of view of structural parameters\nand overall significance of regression model again\nfactor analysis was found significantly up-to the\nmark.\n\nThe center limit theorem for getting efficiency of\nthe model was found incompatible, but in case of\nfactor analysis it was found satisfactory to the\ncenter limit theorem. Overall inn case of factor\nanalysis modified coefficient of efficiency was\nfound increasing.\n\nFrom overall analysis of regression technique we\ncan say that data with high skew ness and large\nstructural\nobservations\nshould\nbe\nestimated/treated with principal component model\nof factor analysis. The dataset with high multicolinearity should also be treated through\nfactors/components according to relevancy. The\nsmall dataset on the other hand should be\nextracted through full model of multiple regression.\n\nIn Parkinson dataset multi-colinearity extraction\nindex was found higher under all models of MLR\ntechniques except forward model. In factor\nanalysis on the same dataset, this index was found\nlower than MLR model. This means MLR is better\ntechnique\nfor\ndiagnosing\nmulti-colinearity\nparticularly with full and stepwise methods.\n\nThe compatibility of a technique on particular\ndataset also depends on particular dataset's\ndistribution of residual term of the model. In our\nstudy marketing or Parkinson dataset are having\nnormal distribution of the residual term, on the\nother hand bank dataset residual term was found\nnon normally distributed considerably. The\nviolation of this residual assumption is affecting the\nprediction power for removing heteroscedastic\nvariance of residual term. The method GLS should\nbe adopted to estimate the structural parameters\nwith suitable suggested forms of the regression\nmodel.\n\nThe significance of overall model was found higher\nin two models of MLR viz. full and stepwise\nmethods but in case of factor analysis, overall\nsignificance of regression model was found similar\nin all methods. The forward method of MLR\ngenerated considerably low F value, which means\noverall significance is poor than another models of\nboth technique. The prediction plots of two\ntechniques on Parkinson dataset is given in figure\n10 to figure 12 and figure 19 to figure 21.\n6. Conclusion and Future Work\n\n54\n\n\u0003\n\n\fINTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY & CREATIVE ENGINEERING (ISSN:2045-8711)\nVOL.1 NO.4 APRIL 2011\n\n\u0003\n\u0003\n[13] Manel, S., J. M. Dias, and S. J. Ormerod, \"Comparing\nDiscriminant Analysis,\n\nThe techniques in which estimators satisfy BLUE\n(best, linear, unbiased, and efficient) properties of\nstructural parameters estimates and stochastic\nrandom error term are considered better than\nothers.\n\nNeural Networks and Logistic Regression for Predicting\nSpecies\nDistribution: A Case Study with a Himalayian River\nBird,\" Ecol. Model.\n\nThe skewness of predictors and random term in\nthe linear regression model is creating obstacles to\nsatisfy BLUE properties. Reducing skewness with\nsome advance data mining tool and then\ncomparing performance of said techniques can\nfurther enlighten us, which is an area that can be\nfurther explored.\n\n120 (1999), pp. 337-347.\n[14] Farkas, Orsolya, and Heberger Karoly, \"Comparison of\nRidge Regression, PLS,\nPairwise Correlation,\nSelection methods for\n\nForward\n\nand\n\nBest\n\nSubset\n\nPrediction of Retention indices for Aliphatic Alcohols,\"\nJournal of\n\nReferences\n\nInformation and Modeling, 45:2 (2005) pp. 339-346.\n\n[1] Gujarati N. Damodar, Sangeetha, \"Basic Econometrics\" 4th\nedition, New York: McGraw Hill, (2007).\n\n[15] Huang, J. et al., \"A Comparison of Calibration Methods\nBased on Calibration\n\n[2] Walpole, R.E, S.L Myers, and K. Ye., Probability and\nStatistics for Engineers and Scientists, 7th edition.\nEnglewood Cliffs, NJ: Prentice Hall (2002).\n\nData size and Robustness,\" Journal of Chemometrics\nand Intelligent Lab.\n\n[3] Myatt J. Glenn, \"Making Sense of Data-A practical guide to\nexploratory data analysis and data mining\" New Jersy:\nWiley-Interscience (2007).\n\nSystems, 62:1 (2002) pp. 25-35.\n[16] Specht, D. F., \"A General Regression Neural Network,\"\nIEEE Transactions on\n\n[4] Giudici Paolo, \"Applied Data Mining-Statistical methods for\nbusiness and industry\" wiley, (2003)\n\nNeural Networks, 2:6 (1991), pp. 568-576.[17] AlKassab M, \"A Monte Carlo Comparison between Ridge and\nPrincipal\nComponents\nRegression\nMethods\" Applied\nMathematical Sciences, Vol. 3, 2009, no. 42, 2085 - 2098\n\n[5] Dash, M., and H. Liu, \"Feature Selection for Classification,\"\nIntelligent Data Analysis. 1:3 (1997) pp. 131-156.\n[6] Rencher C. Alvin, \"Methods of Multivariate Analysis\" 2nd\nEdition, Wiley Interscience, (2002).\n\n[18] Larose T. Daniel, \"Data Mining-Methods and Models\" Wiley\nInterscience, (2002), pp 114.\n\n[7] Kim, Jae-on.; Mueller, Charles W., \"Introduction to Factor\nAnalysis-What it is and how to do it.\", Sage Publications,\nInc., (1978).\n\n[19] Han Jiawei and Kamber Micheline \"Data Mining Concepts\nand Techniques\" Morgan Kaufmann Publishers,\n2006, PP6.\n\n[8] http://www-stat.stanford.edu/~tibs/ElemStatLearn/,\n[9] http://www.cs.toronto.edu/~delve/data/bank/desc.html\n[10] http://archive.ics.uci.edu/ml/datasets.html\n[11] Munoz, Jesus, and Angel M. Felicisimo, \"Comparison of\nStatistical Methods\nCommonly used in Predictive Modeling,\" Journal for\nVegetations Science\n15 (2004), pp. 285-292.\n[12] Maitra Saikat and Yan Jun,\" Principle Component Analysis\nand Partial Least Squares: Two Dimension Reduction\nTechniques for Regression\" Casualty Actuarial Society,\n2008 Discussion Paper Program, pp. 79-90\n55\n\n\u0003\n\n\f"}