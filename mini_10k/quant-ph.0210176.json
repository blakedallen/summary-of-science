{"id": "http://arxiv.org/abs/quant-ph/0210176v2", "guidislink": true, "updated": "2002-12-04T14:42:47Z", "updated_parsed": [2002, 12, 4, 14, 42, 47, 2, 338, 0], "published": "2002-10-25T13:42:53Z", "published_parsed": [2002, 10, 25, 13, 42, 53, 4, 298, 0], "title": "Quantum Pattern Recognition", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=quant-ph%2F0703143%2Cquant-ph%2F0703241%2Cquant-ph%2F0703225%2Cquant-ph%2F0703034%2Cquant-ph%2F0703001%2Cquant-ph%2F0703227%2Cquant-ph%2F0703087%2Cquant-ph%2F0703057%2Cquant-ph%2F0703226%2Cquant-ph%2F0703245%2Cquant-ph%2F0703118%2Cquant-ph%2F0703044%2Cquant-ph%2F0703091%2Cquant-ph%2F0703009%2Cquant-ph%2F0703022%2Cquant-ph%2F0703121%2Cquant-ph%2F0703138%2Cquant-ph%2F0703020%2Cquant-ph%2F0703139%2Cquant-ph%2F0703083%2Cquant-ph%2F0703251%2Cquant-ph%2F0703052%2Cquant-ph%2F0703275%2Cquant-ph%2F0703169%2Cquant-ph%2F0703041%2Cquant-ph%2F0703016%2Cquant-ph%2F0703155%2Cquant-ph%2F0703068%2Cquant-ph%2F0703215%2Cquant-ph%2F0703104%2Cquant-ph%2F0703132%2Cquant-ph%2F0703025%2Cquant-ph%2F0703171%2Cquant-ph%2F0703157%2Cquant-ph%2F0703043%2Cquant-ph%2F0703125%2Cquant-ph%2F0703088%2Cquant-ph%2F0703265%2Cquant-ph%2F0703221%2Cquant-ph%2F0703248%2Cquant-ph%2F0703152%2Cquant-ph%2F0703277%2Cquant-ph%2F0703204%2Cquant-ph%2F0703109%2Cquant-ph%2F0703030%2Cquant-ph%2F0703042%2Cquant-ph%2F0703266%2Cquant-ph%2F0703170%2Cquant-ph%2F0703213%2Cquant-ph%2F0703235%2Cquant-ph%2F0703190%2Cquant-ph%2F0703018%2Cquant-ph%2F0703212%2Cquant-ph%2F0703220%2Cquant-ph%2F0703195%2Cquant-ph%2F0703127%2Cquant-ph%2F0703112%2Cquant-ph%2F0703224%2Cquant-ph%2F0703134%2Cquant-ph%2F0703028%2Cquant-ph%2F0703055%2Cquant-ph%2F0703165%2Cquant-ph%2F0703141%2Cquant-ph%2F0703113%2Cquant-ph%2F0703192%2Cquant-ph%2F0703187%2Cquant-ph%2F0703210%2Cquant-ph%2F0703067%2Cquant-ph%2F0703097%2Cquant-ph%2F0703081%2Cquant-ph%2F0703148%2Cquant-ph%2F0703053%2Cquant-ph%2F0703236%2Cquant-ph%2F0703024%2Cquant-ph%2F0703252%2Cquant-ph%2F0703214%2Cquant-ph%2F0703168%2Cquant-ph%2F0703100%2Cquant-ph%2F0703201%2Cquant-ph%2F0703039%2Cquant-ph%2F0703205%2Cquant-ph%2F0703114%2Cquant-ph%2F0703062%2Cquant-ph%2F0703150%2Cquant-ph%2F0703122%2Cquant-ph%2F0210202%2Cquant-ph%2F0210071%2Cquant-ph%2F0210009%2Cquant-ph%2F0210176%2Cquant-ph%2F0210005%2Cquant-ph%2F0210199%2Cquant-ph%2F0210124%2Cquant-ph%2F0210002%2Cquant-ph%2F0210155%2Cquant-ph%2F0210207%2Cquant-ph%2F0210054%2Cquant-ph%2F0210110%2Cquant-ph%2F0210131%2Cquant-ph%2F0210101%2Cquant-ph%2F0210012%2Cquant-ph%2F0210128&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Quantum Pattern Recognition"}, "summary": "I review and expand the model of quantum associative memory that I have\nrecently proposed. In this model binary patterns of n bits are stored in the\nquantum superposition of the appropriate subset of the computational basis of n\nqbits. Information can be retrieved by performing an input-dependent rotation\nof the memory quantum state within this subset and measuring the resulting\nstate. The amplitudes of this rotated memory state are peaked on those stored\npatterns which are closest in Hamming distance to the input, resulting in a\nhigh probability of measuring a memory pattern very similar to it. The accuracy\nof pattern recall can be tuned by adjusting a parameter playing the role of an\neffective temperature. This model solves the well-known capacity shortage\nproblem of classical associative memories, providing an exponential improvement\nin capacity. The price to pay is the probabilistic nature of information\nretrieval, a feature that, however, this model shares with our own brain.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=quant-ph%2F0703143%2Cquant-ph%2F0703241%2Cquant-ph%2F0703225%2Cquant-ph%2F0703034%2Cquant-ph%2F0703001%2Cquant-ph%2F0703227%2Cquant-ph%2F0703087%2Cquant-ph%2F0703057%2Cquant-ph%2F0703226%2Cquant-ph%2F0703245%2Cquant-ph%2F0703118%2Cquant-ph%2F0703044%2Cquant-ph%2F0703091%2Cquant-ph%2F0703009%2Cquant-ph%2F0703022%2Cquant-ph%2F0703121%2Cquant-ph%2F0703138%2Cquant-ph%2F0703020%2Cquant-ph%2F0703139%2Cquant-ph%2F0703083%2Cquant-ph%2F0703251%2Cquant-ph%2F0703052%2Cquant-ph%2F0703275%2Cquant-ph%2F0703169%2Cquant-ph%2F0703041%2Cquant-ph%2F0703016%2Cquant-ph%2F0703155%2Cquant-ph%2F0703068%2Cquant-ph%2F0703215%2Cquant-ph%2F0703104%2Cquant-ph%2F0703132%2Cquant-ph%2F0703025%2Cquant-ph%2F0703171%2Cquant-ph%2F0703157%2Cquant-ph%2F0703043%2Cquant-ph%2F0703125%2Cquant-ph%2F0703088%2Cquant-ph%2F0703265%2Cquant-ph%2F0703221%2Cquant-ph%2F0703248%2Cquant-ph%2F0703152%2Cquant-ph%2F0703277%2Cquant-ph%2F0703204%2Cquant-ph%2F0703109%2Cquant-ph%2F0703030%2Cquant-ph%2F0703042%2Cquant-ph%2F0703266%2Cquant-ph%2F0703170%2Cquant-ph%2F0703213%2Cquant-ph%2F0703235%2Cquant-ph%2F0703190%2Cquant-ph%2F0703018%2Cquant-ph%2F0703212%2Cquant-ph%2F0703220%2Cquant-ph%2F0703195%2Cquant-ph%2F0703127%2Cquant-ph%2F0703112%2Cquant-ph%2F0703224%2Cquant-ph%2F0703134%2Cquant-ph%2F0703028%2Cquant-ph%2F0703055%2Cquant-ph%2F0703165%2Cquant-ph%2F0703141%2Cquant-ph%2F0703113%2Cquant-ph%2F0703192%2Cquant-ph%2F0703187%2Cquant-ph%2F0703210%2Cquant-ph%2F0703067%2Cquant-ph%2F0703097%2Cquant-ph%2F0703081%2Cquant-ph%2F0703148%2Cquant-ph%2F0703053%2Cquant-ph%2F0703236%2Cquant-ph%2F0703024%2Cquant-ph%2F0703252%2Cquant-ph%2F0703214%2Cquant-ph%2F0703168%2Cquant-ph%2F0703100%2Cquant-ph%2F0703201%2Cquant-ph%2F0703039%2Cquant-ph%2F0703205%2Cquant-ph%2F0703114%2Cquant-ph%2F0703062%2Cquant-ph%2F0703150%2Cquant-ph%2F0703122%2Cquant-ph%2F0210202%2Cquant-ph%2F0210071%2Cquant-ph%2F0210009%2Cquant-ph%2F0210176%2Cquant-ph%2F0210005%2Cquant-ph%2F0210199%2Cquant-ph%2F0210124%2Cquant-ph%2F0210002%2Cquant-ph%2F0210155%2Cquant-ph%2F0210207%2Cquant-ph%2F0210054%2Cquant-ph%2F0210110%2Cquant-ph%2F0210131%2Cquant-ph%2F0210101%2Cquant-ph%2F0210012%2Cquant-ph%2F0210128&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "I review and expand the model of quantum associative memory that I have\nrecently proposed. In this model binary patterns of n bits are stored in the\nquantum superposition of the appropriate subset of the computational basis of n\nqbits. Information can be retrieved by performing an input-dependent rotation\nof the memory quantum state within this subset and measuring the resulting\nstate. The amplitudes of this rotated memory state are peaked on those stored\npatterns which are closest in Hamming distance to the input, resulting in a\nhigh probability of measuring a memory pattern very similar to it. The accuracy\nof pattern recall can be tuned by adjusting a parameter playing the role of an\neffective temperature. This model solves the well-known capacity shortage\nproblem of classical associative memories, providing an exponential improvement\nin capacity. The price to pay is the probabilistic nature of information\nretrieval, a feature that, however, this model shares with our own brain."}, "authors": ["Carlo A. Trugenberger"], "author_detail": {"name": "Carlo A. Trugenberger"}, "author": "Carlo A. Trugenberger", "arxiv_comment": "Invited Talk at the 1st Feynman Festival, Univ. of Maryland, College\n  Park, August 2002", "links": [{"href": "http://arxiv.org/abs/quant-ph/0210176v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/quant-ph/0210176v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.dis-nn", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "nlin.AO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "q-bio.NC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/quant-ph/0210176v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/quant-ph/0210176v2", "journal_reference": null, "doi": null, "fulltext": "Quantum Pattern Recognition\nC. A. Trugenberger\n\narXiv:quant-ph/0210176v2 4 Dec 2002\n\nInfoCodex SA, av. Louis-Casai 18, CH-1209 Geneva, Switzerland\nTheory Division, CERN, CH-1211 Geneva 23, Switzerland\ne-mail: ca.trugenberger@InfoCodex.com\n(November 2, 2018)\n\nnition for complex tasks, the shortcomings of RAM memories were addressed by introducing models of associative\n(or content-addressable) memories [6]. Here, recall of information is possible on the basis of partial knowledge\nof their content, without knowing the storage location.\nThese are examples of collective computation on neural\nnetworks [6], the best known example being the Hopfield\nmodel [7] and its generalization to a bidirectional associative memory [8].\nWhile these models solve the problem of recalling incomplete or noisy inputs, they suffer from a severe capacity shortage. Due to the phenomenon of crosstalk, which\nis essentially a manifestation of the spin glass transition\n[9] in the corresponding spin systems, the maximum number of binary patterns that can be stored in a Hopfield\nnetwork of n neurons is pmax \u2243 0.14 n [6] . While various\npossible improvements can be introduced [6], the maximum number of patterns remains linear in the number\nof neurons, pmax = O(n).\nQuantum mechanics offers a way out from the impossibility of reconciling the association power of contentaddressable memories with the requirement of large storage capacity. Indeed, quantum mechanical entanglement\nprovides a natural mechanism for both improving dramatically the storage capacity of associative memories\nand retrieving corrupted or incomplete information.\nThe basic idea is to store the given p binary patterns of\nn bits in a quantum superposition of the corresponding\nsubset of the computational basis of n qbits. The number\nof binary patterns that can be stored in such a quantum\nassociative memory is exponential in the number n of\nqbits, pmax = 2n , i.e. it is optimal in the sense that all\nbinary patterns that can be formed with n bits can be\nstored.\nThe basic idea of the information retrieval mechanism\nis very simple. Given an input pattern, the memory\nquantum state is rotated within the subspace defined\nby the stored patterns so that the resulting amplitudes\nare peaked on the stored patterns which are closest in\nHamming distance to the input. A measurement of the\nrotated memory quantum state provides the output pattern.\nAn efficient way to perform this rotation is to embed\nthe memory quantum state in a larger Hilbert space by\nadding b control qbits. The full state is then rotated\nin the enlarged Hilbert space. After this rotation one\nis interested only in the projection of the rotated state\nonto a specific subspace of the enlarged Hilbert space.\n\nI review and expand the model of quantum associative\nmemory that I have recently proposed. In this model binary patterns of n bits are stored in the quantum superposition of the appropriate subset of the computational basis of n\nqbits. Information can be retrieved by performing an inputdependent rotation of the memory quantum state within this\nsubset and measuring the resulting state. The amplitudes of\nthis rotated memory state are peaked on those stored patterns which are closest in Hamming distance to the input,\nresulting in a high probability of measuring a memory pattern very similar to it. The accuracy of pattern recall can be\ntuned by adjusting a parameter playing the role of an effective temperature. This model solves the well-known capacity\nshortage problem of classical associative memories, providing\nan exponential improvement in capacity. The price to pay\nis the probabilistic nature of information retrieval, a feature\nthat, however, this model shares with our own brain.\n\nI. INTRODUCTION\n\nThe power of quantum computation [1] is mostly associated with the speed-up in computing time it can\nprovide with respect to its classical counterparts, the\nparamount examples being Shor's factoring algorithm [2]\nand Grover's search algorithm [3]. There is, however, another aspect of quantum computation which represents a\nbig improvement upon its classical counterpart [4]. This\nleads to an exponential increase in a particular memory\ncapacity rather than speed. In this paper I will review\nand expand the main aspects of this new application of\nquantum information theory. Further aspects of it can\nbe found in [5].\nIn traditional computers the storage of information requires setting up a lookup table (RAM). The main disadvantage of this address-oriented memory system lies in\nits rigidity. Retrieval of information requires a precise\nknowledge of the memory address and, therefore, incomplete or corrupted inputs are not permitted.\nThis is definitely not how our own brain works. When\ntrying to recognize a person from a blurred photo it is\ntotally useless to know that it is the 17384th person you\nmet in your life. Rather, the recognition process is based\non our strong power of association with stored memories\nthat resemble the given picture. Association is what we\nuse every time we solve a crossword puzzle and is distinctive of the human brain.\nGiven the superior power of associative pattern recog-\n\n1\n\n\fsingle-qbit gate is the Hadamard gate H, with the matrix\nrepresentation\n\u0013\n\u0012\n1\n1 1\n\u221a\nH=\n.\n(1)\n1 \u22121\n2\n\nThis projection can be obtained either by repeated measurement or by rotating the state (approximately) to the\ndesired subspace using the amplitude amplification technique [10]. Either way one has to repeat a certain algorithm a number of times and measure the control register to check if the desired projection has been obtained.\nThe information retrieval mechanism is thus probabilistic, with postselection of the measurement result. This\nmeans that one has to repeat an algorithm until a threshold T is reached or the measurement of a control register\nyields a given result. In the former case the input is not\nrecognized. In the latter case, instead, the output is determined itself by a probability distribution on the memory which is peaked around the stored patterns closest in\nHamming distance to the input.\nThe accuracy of this information retrieval mechanism depends on the distribution of the stored patterns.\nRecognition efficiency is best when the number of stored\npatterns is very large while identification efficiency is best\nfor isolated patterns which are very different from all\nother ones, both very intuitive features. Both efficiencies\ncan be tuned to prescribed accuracy levels. The recognition efficiency can be varied by changing the threshold\nT : the higher T , the larger the number of qbits that can\nbe corrupted without affecting recognition. The identification efficiency, instead, can be tuned by varying the\nnumber b of control qbits in the memory. As we shall see,\nb = 1/t plays the role of an inverse effective temperature\nt. The lower t, the more concentrated is the corresponding effective Boltzmann distribution on the states closest\n(in Hamming distance) to the input and the better becomes the identification.\nBy averaging over the distribution of stored patterns\none can eliminate the dependence on the stored pattern\ndistribution and derive the effective statistical mechanics of quantum associative memories by introducing the\nusual thermodynamic potentials. In particular, the free\nenergy F (t) describes the average behaviour of the recall mechanism at temperature t and provides concrete\ncriteria to tune the accuracy of the associative memory.\nBy increasing b (lowering t), the associative memory undergoes a phase transition from a disordered phase with\nno correlation between input and output to an ordered\nphase with minimal Hamming distance bewteen the input and the output. This extends to quantum information theory the relation with Ising spin systems known\nin error-correcting codes [11] and in public key cryptography [12].\n\nThen, I will use extensively the two-qbit XOR (exclusive\nOR) gate, which performs a NOT on the second qbit if\nand only if the first one is in state |1i. In matrix notation\nthis gate is represented as XOR = diag (1, \u03c31 ), where 1\ndenotes a two-dimensional identity matrix and \u03c31 acts on\nthe components |01i and |11i of the Hilbert space. The\n2XOR, or Toffoli gate is the three qbit generalization of\nthe XOR gate: it performs a NOT on the third qbit if and\nonly if the first two are both in state |1i. In matrix notation it is given by 2XOR = diag (1, 1, \u03c31 ). In the storage\nalgorithm I shall make use also of the nXOR generalization of these gates, in which there are n control qbits.\nThis gate is also used in the subroutines implementing\nthe oracles underlying Grover's algorithm [1] and can be\nrealized using unitary maps affecting only few qbits at\na time [13], which makes it feasible. All these are standard gates. In addition to them I introduce the two-qbit\ncontrolled gates\nCS i = |0ih0| \u2297 1 + |1ih1| \u2297 S i ,\n\uf8ebq\n\uf8f6\nS i= \uf8ed\n\ni\u22121\ni\n\n\u22121\n\u221a\ni\n\n1\n\u221a\ni\n\nq\n\ni\u22121\ni\n\n\uf8f8 ,\n\n(2)\n\nfor i = 1, .\u0001. . , p. These have the matrix notation CS i =\ndiag 1, S i . For all these gates I shall indicate by subscripts the qbits on which they are applied, the control\nqbits coming always first.\nGiven p binary patterns pi of length n, it is not difficult to imagine how a quantum memory can store them.\nIndeed, such a memory is naturally provided by the following superposition of n entangled qbits:\np\n1 X i\n|mi = \u221a\n|p i .\np i=1\n\n(3)\n\nThe only real question is how to generate this state unitarily from a simple initial state of n qbits. In [4] I presented an algorithm which achieves this by loading sequentially the classical patterns into an auxiliary quantum register from which they are then copied into the\nactual memory register. Here I will generalize this algorithm by constructing also the unitary operator which\ngenerates the memory state (3) directly from the state\n|0, . . . , 0i.\nLet me begin by reviewing the sequential algorithm of\nref. [4]. I shall use three registers: a first register p of\nn qbits in which I will subsequently feed the patterns pi\nto be stored, a utility register u of two qbits prepared in\nstate |01i, and another register m of n qbits to hold the\nmemory. This latter will be initially prepared in state\n|01 , . . . , 0n i. The full initial quantum state is thus\n\nII. STORING INFORMATION\n\nLet me start by describing the elementary quantum\ngates [1] that I will use in the rest of the paper. First\nof all there are the single-qbit gates represented by the\nPauli matrices \u03c3i , i = 1 . . . 3. The first Pauli matrix\n\u03c31 , in particular, implements the NOT gate. Another\n\n2\n\n\f|\u03c801 i = |p11 , . . . p1n ; 01; 01, . . . , 0n i .\n\ndescribed. At the end of the whole process, the m-register\nis exactly in state |mi, eq. (3).\nAny quantum state can be generically obtained by a\nunitary transformation of the initial state |0, . . . , 0i. This\nis true also for the memory state |mi. In the following\nI will explicitly construct the unitary operator M which\nimplements the transformation |mi = M |0, . . . , 0i.\nTo this end I introduce first the single-qbit unitary\ngates\n\u0010\u03c0 \u0011\n\u0010\u03c0 \u0011\nUji = cos\n(11)\npij 1 + i sin\npi \u03c32 ,\n2\n2 j\n\n(4)\n\nThe idea of the storage algorithm is to separate this state\ninto two terms, one corresponding to the already stored\npatterns, and another ready to process a new pattern.\nThese two parts will be distinguished by the state of the\nsecond utility qbit u2 : |0i for the stored patterns and |1i\nfor the processing term.\nFor each pattern pi to be stored one has to perform\nthe operations described below:\nn\nY\n\n|\u03c81i i =\n\nj=1\n\n2XORpij u2 mj |\u03c80i i .\n\n(5)\n\nwhere \u03c32 is the second Pauli matrix. These operators\nare such that their product over the n qbits generates\npattern pi out of |0, . . . , 0i:\n\nThis simply copies pattern pi into the memory register\nof the processing term, identified by |u2 i = |1i.\n|\u03c82i i=\n\nn\nY\n\nj=1\n\n|pi i= P i |0, . . . , 0i ,\nn\nY\ni\nUji .\nP \u2261\n\nN OTmj XORpij mj |\u03c81i i ,\n\n|\u03c83i i= nXORm1 ...mn u1 |\u03c82i i .\n\n(6)\n\nI now introduce, in addition to the memory register\nproper, the same two utility qbits as before, also initially\nin the state |0i. The idea is, exactly as in the sequential\nalgorithm, to split the state into two parts, a storage term\nwith |u2 i = |0i and a processing term with |u2 i = |1i.\nTherefore I generalize the operators P i defined above to\n\nThe first of these operations makes all qbits of the memory register |1i's when the contents of the pattern and\nmemory registers are identical, which is exactly the case\nonly for the processing term. Together, these two operations change the first utility qbit u1 of the processing\nterm to a |1i, leaving it unchanged for the stored patterns\nterm.\n|\u03c83i i .\n|\u03c84i i = CSup+1\u2212i\n1 u2\n\nCPui 2 \u2261\n\n(7)\n\nn\nY\n\nCUui 2 j ,\n\n(13)\n\nj=1\n\nwhich loads pattern pi into the memory register only for\nthe processing term. It is then easy to check that\n\nThis is the central operation of the storing algorithm. It\nseparates out the new pattern to be stored, already with\nthe correct normalization factor.\n|\u03c85i i= nXORm1 ...mn u1 |\u03c84i i ,\n1\nY\ni\nXORpij mj N OTmj |\u03c85i i .\n|\u03c86 i=\n\n(12)\n\nj=1\n\n|m; 00i = M |0, . . . , 0; 00i ,\np h\ni\nY\n\u0001\u22121\ni\nCPui 2\nCP\nXOR\nN OTu1 CSup+1\u2212i\nM=\nu\nu\n2 1\nu2 \u00d7\n1 u2\ni=1\n\n(8)\n\n\u00d7 N OTu2 .\n\nj=n\n\n(14)\n\nThe storage algorithm is thus efficient in the sense that\nthe number p(2n+ 3)+ 1 of elementary one- and two-qbit\ngates needed to implement it, is linear in n for fixed p.\nNote that, by construction, there are no restrictions on\nthe loading factor p/n. However, the storage algorithm\nis efficient in an absolute sense only for p polynomial in\nn.\n\nThese two operations are the inverse of eqs.(6) and restore the utility qbit u1 and the memory register m to\ntheir original values. After these operations on has\ns\ni\n1 X i\np\u2212i i\ni\nk\n|\u03c86 i = \u221a\n|p ; 01; pi i . (9)\n|p ; 00; p i +\np\np\nk=1\n\nWith the last operation,\n|\u03c87i i =\n\n1\nY\n\nj=n\n\n2XORpij u2 mj |\u03c86i i ,\n\nIII. REMEMBERING\n\n(10)\n\nA memory is of real value only if it can be used repeatedly. This poses a problem since, as we shall see in\nthe next section, an output of the memory is obtained\nby measuring the memory register and the rules of quantum mechanics imply that, when the memory register is\nmeasured, all the information about the entangled superposition of stored patterns is lost. If one does not want\n\none restores the third register m of the processing term,\nthe second term in eq.(9) above, to its initial value\n|01 , . . . , 0n i. At this point one can load a new pattern\ninto register p and go through the same routine as just\n3\n\n\fThe quantum network for the probabilistic cloner of\ntwo states has been developed in [18]. It can be constructed exclusively out of the two simple distinguishability tranfer (D) and state separation (S) gates. Note that\nthese gates embody information about the two states to\nbe cloned. Part of the memory, therefore, actually resides in the cloning network, which is unavoidable if one\nwants to use a quantum memory repeatedly. This is then\na mixed solution in which part of the information resides\nin a quantum state and another part in a unitary operator implemented as a probabilistic cloning network.\nAt the other end of the spectrum one can store the\ninformation about the p patterns entirely in the unitary\noperator M in eq.(14). Each time one needs to retrieve\ninformation one prepares then an initial quantum state\n|0, . . . , 0i and one transforms it into |mi by applying M\nto it. In this case one needs pn bits of information to\nstore the p patterns, exactly as in the classical Hopfield\nmodel. Indeed, this way of storing the patterns is very\nclose in spirit to the Hopfield model since a unitary operator can always be represented as the exponential of a\nHamiltonian operator, which is the quantum generalization of an energy functional. As I now show, however, in\nthe quantum case there are no restrictions on the number\nof patterns that can be stored and retrieved.\n\nto forget everything after the first information retrieval\none must therefore find a way to store the information\nfor repeated use.\nIn quantum mechanics there are many choices to do\nthis, since information can be stored, with various degrees of compression, both in quantum states and in unitary operators. The most compressed storage would be\na quantum state: in this case up to 2n patterns can\nbe stored using only n (quantum) degrees of freedom.\nTo this end, however, one would have to keep a master\ncopy of the memory and produce copies out of it when\nneeded. Unfortunately, this is impossible since the linearity of quantum mechanics forbids exact universal cloning\nof quantum states [14]. Universal cloning [15] has two\ndisadvantages: first of all the copies to be used for information retrieval are imperfect, though optimal [16];\nsecondly, the quality of the master copy decreases with\neach recall, i.e. the memory is quickly washed out.\nThis leaves state-dependent cloning as the only viable\noption if one wants to store at least part of the information in a quantum state. State-dependent cloners are\ndesigned to reproduce only a finite number of states and\nthis is definitely enough for our purposes. The simplest\noption in this setting is to use a probabilistic cloning machine [17]. To this end it is sufficient to consider any\ndummy state |di different from |mi (for more than two\nstates the condition would be linear independence) and\nto construct a probabilistic cloning machine for these two\nstates. This machine reproduces |mi with probability pm\nand |di with probability pd ; a flag tells us exactly when\nthe desired state |mi has been obtained. In order to obtain an exact copy of |mi one needs then 1/pm trials on\naverage. The master copy is exactly preserved.\nThe cloning efficiencies of the probabilistic cloner of\ntwo states are bounded as follows [17]:\npm + pd \u2264\n\n2\n.\n1 + hd|mi\n\nIV. RETRIEVING INFORMATION\n\nAssume now one is given a binary input i, which might\nbe, e.g. a corrupted version of one of the patterns stored\nin the memory. The retrieval algorithm requires also\nthree registers. The first register i of n qbits contains\nthe input pattern; the second register m, also of n qbits,\ncontains the memory |mi; finally there is a control register c with b qbits all initialized in the state |0i.\nThe full initial quantum state is thus:\n\n(15)\n\np\n\n1 X\n|\u03c80 i = \u221a\n|i; pk ; 01 , . . . , 0b i\np\n\nThis bound can be made large by choosing |di as nearly\northogonal to |mi as possible. A simple way to achieve\nthis for a large number of patterns is to encode also the\nstate\n1\n|di = \u221a\np\n\np\nX\ni=1\n\n(\u22121)i+1 |pi i\n\nwhere |ii = |i1 , . . . , in i denotes the input qbits, the second register, m, contains the memory (3) and all b control\nqbits are in state |0i. Applying the Hadamard gate to\nthe first control qbit one obtains\n\n(16)\n\np\n1 X\n|\u03c81 i= \u221a\n|i; pk ; 01 , . . . , 0b i\n2p\n\ntogether with |mi when storing information. This can\nbe done simply by using alternately the operators S i and\n\u0001\u22121\nSi\nin the storing algorithm of section 2. For binary\npatterns which are all different from one another one has\nthen\nhd|mi= 0 ,\n1\nhd|mi= ,\np\n\np even ,\n\n(18)\n\nk=1\n\nk=1\n\np\n1 X\n+\u221a\n|i; pk ; 11 , . . . , 0b i .\n2p\n\n(19)\n\nk=1\n\n(17)\n\nI now apply to this state the following combination of\nquantum gates:\n\np odd ,\n\nand the bound for the cloning efficiencies is very close to\nits maximal value 2 in both cases.\n\n|\u03c82 i =\n4\n\nn\nY\n\nj=1\n\nN OTmj XORij mj |\u03c81 i ,\n\n(20)\n\n\fp\nb\n\u0010\u03c0\n\u0001\u0011\n1 XX\ndH i, pk \u00d7\n|\u03c8fin i= \u221a\ncosb\u2212l\np\n2n\nk=1 l=0\n\u0010\u03c0\n\u0011 X\n\u0001\nsinl\ndH i, pk\n|i; pk ; J l i,\n2n\nl\n\nAs a result of the above operation the memory register\nqbits are in state |1i if ij and pkj are identical and |0i\notherwise:\np\n1 X\n|i; dk ; 01 , . . . , 0b i\n|\u03c82 i= \u221a\n2p\nk=1\n\n1\n+\u221a\n2p\n\np\nX\n\nk=1\n\n|i; dk ; 11 , . . . , 0b i ,\n\n\b\nwhere J l denotes the set of all binary numbers of b\nbits with exactly l bits 1 and (b \u2212 l) bits 0.\nAs in the case of the storing algorithm, there is a version of the information retrieval algorithm in which the\ninput is not loaded into an auxiliary quantum register\nbut rather into a unitary operator. Indeed, the auxiliary quantum register is needed only by the operator (20)\nleading from (19) to (21). The same result (apart from\nan irrelevant overall sign) can be obtained by applying\n\n(21)\n\nwhere dkj = 1 if and only if ij = pkj and dkj = 0 otherwise.\nConsider now the following Hamiltonian:\nH= (dH )m \u2297 (\u03c33 )c1 ,\n\u0013\nn \u0012\nX\n\u03c33 + 1\n,\n(dH )m =\n2\nmj\nj=1\n\n(22)\nI=\n\n\u03c0\n\n1\n+\u221a\n2p\n\nUj = sin\n\n(23)\n\nk\n\u03c0\ne\u2212i 2n dH (i,p ) |i; dk ; 11 , . . . , 0b i ,\n\n\u0001\nk\n\nwhere dH i, p denotes the Hamming distance bewteen\nthe input i and the stored pattern pk .\nIn the final step I restore the memory gate to the state\n|mi by applying the inverse transformation to eq. (20)\nand I apply the Hadamard gate to the control qbit c1 ,\nthereby obtaining\n|\u03c84 i= Hc1\n1\n|\u03c84 i= \u221a\np\n\n1\nY\n\nj=n\np\nX\nk=1\n\nXORij mj N OTmj |\u03c83 i ,\ncos\n\nUj ,\n\u0010\u03c0 \u0011\n\u0010\u03c0 \u0011\nij 1 + i cos\nij \u03c32 ,\n2\n2\n\n(26)\n\ndirectly on the memory state |mi. The rest of the algorithm is the same, apart the reversing of the operator\n(20) which needs now the operator I \u22121 .\nThe end effect of the information retrieval algorithm\nrepresents a rotation of the memory quantum state in\nthe enlarged Hilbert space obtained by adding b control\nqbits. Note that the overall effect of this rotation is an\noverall amplitude concentration on memory states similar to the input if there is a large number of |0i control\nqbits and an amplitude concentration on states different\nto the input if there is a large number of |1i control qbits.\nAs a consequence, the most interesting state for information retrieval purposes is the projection of |\u03c8fin i onto the\nsubspace with all control qbits in state |0i.\nThere are two ways of obtaining this projection. The\nfirst is to repeat the above algorithm and measure the\ncontrol register several times, until exactly the desired\nstate for the control register is obtained. If the number\nof such repetitions exceeds a preset threshold T the input is classified as \"non-recognized\" and the algorithm\nis stopped. Otherwise, once |c1 , . . . , cb i = |01 , . . . , 0b i is\nobtained, one proceeds to a measurement of the memory register m, which yields the output pattern of the\nmemory.\nThe second method is to apply T steps of the amplitude amplification algorithm [10] rotating |\u03c8fin i towards\nits projection onto the \"good\" subspace formed by the\nstates with all control qbits in state |0i. To this end it is\nbest to use the versions of the storing and retrieving algorithms which do not need any auxiliary quantum register\nfor inputs. Let me define as R(i) the input-dependent\noperator which rotates the memory state in the Hilbert\nspace enlarged by the b control qbits towards the final\nstate |\u03c8fin i in eq. (25) (where I now omit the auxiliary\nregister for the input):\n\nk=1\np\nX\nk=1\n\nn\nY\n\nj=1\n\nwhere \u03c33 is the third Pauli matrix. H measures the number of 0's in register m, with a plus sign if c1 is in state\n|0i and a minus sign if c1 is in state |1i. Given how I have\nprepared the state |\u03c82 i, this is nothing else than the number of qbits which are different in the input and memory\nregisters i and m. This quantity is called the Hamming\ndistance and represents the (squared) Euclidean distance\nbetween two binary patterns.\nEvery term in the superposition (21) is an eigenstate of\nH with a different eigenvalue. Applying thus the unitary\noperator exp(i\u03c0H/2n) to |\u03c82 i one obtains\n|\u03c83 i= ei 2n H |\u03c82 i ,\np\nk\n\u03c0\n1 X i 2n\ne dH (i,p ) |i; dk ; 01 , . . . , 0b i\n|\u03c83 i= \u221a\n2p\n\n(25)\n\n{J }\n\n(24)\n\n\u0001\n\u03c0\ndH i, pk |i; pk ; 01 , . . . , 0b i\n2n\n\np\n\u0001\n\u03c0\n1 X\nsin\ndH i, pk |i; pk ; 11 , . . . , 0b i.\n+\u221a\np\n2n\nk=1\n\nThe idea is now to repeat the above operations sequentially for all b control qbits c1 to cb . This gives\n\n|\u03c8fin i = R(i) |m; 01 , . . . , 0b i .\n5\n\n(27)\n\n\fThis new parameter b controls the identification efficiency of the quantum memory\n\u0001 since, increasing b, the\nprobability distribution Pb \u0001pk becomes more and more\npeaked on the low dH i, pk states, until\n\nBy adding also the two utility qbits needed for the storing\nalgorithm one can then obtain |\u03c8fin i as a unitary transformation of the initial state with all qbits in state |0i:\n|\u03c8fin ; 00i = R(i)M |0, . . . , 0; 01 , . . . , 0b ; 00i .\n\n(28)\n\n\u0001\nlim Pb pk = \u03b4kkmin ,\n\nThe amplitude amplification rotation of |\u03c8fin ; 00i towards\nits \"good\" subspace in which all b control qbits are in\nstate |0i is then obtained [10] by repeated application of\nthe operator\nQ = \u2212R(i)M S0 M \u22121 R\u22121 (i)S\n\nb\u2192\u221e\n\nwhere kmin is the index of the pattern (assumed unique\nfor convenience) with the smallest Hamming distance to\nthe input.\nThe probability of recognition is determined by comparing (even) powers of cosines and sines of the distances\nto the stored patterns. It is thus clear that the worst\ncase for recognition is the situation in which there is an\nisolated pattern, with the remaining patterns forming a\ntight cluster spanning all the largest distances to the first\none. As a consequence, the threshold needed to recognize all patterns diminishes when the number of stored\npatterns becomes very large, since, in this case, the distribution of patterns becomes necessarily more homogeneous. Indeed, for the maximal number of stored patterns p = 2n one has Pbrec = 1/2b and the recognition\nefficiency becomes also maximal, as it should be.\nWhile the recognition efficiency depends on comparing powers of cosines and sines of the same distances in\nthe distribution, the identification efficiency depends on\ncomparing the (even) powers of cosines of the different\ndistances in the distribution. Specifically, it is best when\none of the distances is zero, while all others are as large\nas possible, such that the probability of retrieval is completely peaked on one pattern. As a consequence, the\nidentification efficiency is best when the recognition efficiency is worst and viceversa.\nThe role of the parameter b becomes familiar upon\na closer examination of eq.( 31). Indeed, the quantum\ndistribution described by this equation is equivalent to\na canonical Boltzmann distribution with (dimensionless)\ntemperature t = 1/b and (dimensionless) energy levels\n\u0010\u03c0\n\u0001\u0011\nE k = \u22122 log cos\n,\n(34)\ndH i, pk\n2n\n\n(29)\n\non the state |\u03c8fin ; 00i. Here S conditionally changes the\nsign of the amplitude of the \"good\" states with the b\ncontrol qbits in state |0i, while S0 changes the sign of\nthe amplitude if and only if the state is the zero state\n|0, . . . , 0; 01 , . . . , 0b ; 00i. As before, if a measurement of\nthe control register after the T iterations of the amplitude\namplification rotation yields |01 , . . . , 0b i one proceeds to a\nmeasurement of the memory register; otherwise the input\nis classified as \"non-recognized\".\nSince the expected number of repetitions needed to\nmeasure the desired control register state is 1/Pbrec , with\nPbrec =\n\np\n\u0010\u03c0\n\u0001\u0011\n1 X\ndH i; pk\ncos2b\np\n2n\n\n(30)\n\nk=1\n\nthe probability of measuring |c1 , . . . , cn i = |01 , . . . , 0n i,\nthe threshold T governs the recognition efficiency of the\ninput patterns. Note, however, that amplitude amplification provides a quadratic\np boost [10] to the recognition\nefficiency since only 1/ Pbrec steps are required to rotate\n|\u03c8fin i onto the desired subspace.\nAccordingly, the thresh\u221a\nold T can be lowered to T with respect to the method\nof projection by measurement.\nOnce the input pattern i is recognized, the measurement of the memory register yields the stored pattern pk\nwith probability\n\u0010\u03c0\n\u0001 1\n\u0001\u0011\nPb pk =\n,\ncos2b\ndH i, pk\nZ\n2n\np\n\u0010\u03c0\nX\n\u0001\u0011\nZ= pPbrec =\ncos2b\n.\ndH i, pk\n2n\n\n(33)\n\n(31)\n\nwith Z playing the role of the partition function.\nThe appearance of an effective thermal distribution\nsuggests studying the average behaviour of quantum\nassociative memories via the corresponding thermodynamic potentials. Before this can be done, however, one\nmust deal with the different distributions of stored patterns characterizing each individual memory. To this end\nI propose to average also over this distribution, by keeping as a tunable parameter only the minimal Hamming\ndistance d between the input and the stored patterns. In\ndoing so, one obtains an average description of the average memory. This is essentially the replica trick used\nto derive the behaviour of spin glasses [9] and classical\nHopfield models [6].\nAs a first step it is useful to normalize the pattern representation by adding (modulo 2) to all patterns, input\n\n(32)\n\nk=1\n\nClearly, this probability is peaked around those patterns\nwhich have the smallest Hamming distance to the input.\nThe highest probability of retrieval is thus realized for\nthat pattern which is most similar to the input. This\nis always true, independently of the number of stored\npatterns. In particular there are never spurious memories: the probability of obtaining as output a non-stored\npattern is always zero. As a consequence there are no\nrestrictions on the loading factor p/n coming from the\ninformation retrieval algorithm.\nIn addition to the threshold T , there is a second tunable parameter, namely the number b of control qbits.\n\n6\n\n\fThe function D(b, d) provides a complete description\nof the behaviour of quantum associative memories in the\nlimit p/n \u226a 1. This can be used to tune their performance. Indeed, suppose that one wants the memory to\nrecognize and identify inputs with up to \u01ebn corrupted inputs with an efficiency of \u03bd (0 \u2264 \u03bd \u2264 1). Then one must\nchoose a number b of control qbits sufficiently large that\n(D(b, \u01ebn) \u2212 \u01eb) \u2264 (1 \u2212 \u03bd) and a threshold\nT of repetitions\n\u0001\nsatisfying T \u2265 1/cos2b \u03c02 D(b, \u01ebn) , as illustrated in Fig.\n1 below.\nA first hint about the general behaviour of the effective\ndistance function D(b, d) can be obtained by examining\ncloser the energy eigenvalues (34). For small Hamming\ndistance to the input these reduce to\n\u0001 !2\n\u0001\ndH i, pk\n\u03c0 2 dH i, pk\nk\nE \u2243\n,\n\u226a 1 . (40)\n4\nn\nn\n\nincluded, the input pattern i. This clearly preserves all\nHamming distances and has the effect of normalizing the\ninput to be the state with \u0001all qbits in state |0i. The\nHamming distance dH i, pk becomes thus simply the\nnumber of qbits in pattern pk with value |1i. For loading\nfactors p/n \u2192 0 in the limit n \u2192 \u221e the partition function for the average memory takes then a particularly\nsimple form:\nZav\n\n\u0012\n\u0013\nn\n\u03c0j\np X X\n2b\n,\n\u03bbj cos\n=\nN\u03bb\n2n\n\n(35)\n\n{\u03bb} j=d\n\nwhere \u03bbj describes\nPn an unconstrained probability distribution such that j=d \u03bbj = 1, {\u03bb} is the set of such distributions and N\u03bb the corresponding normalization factor.\nFor finite loading factors, instead, the probabilities \u03bbj\nbecome subject to constraints which make things more\ncomplicated.\nI now introduce the free energy F (b, d) by the usual\ndefinition\nZav = p e\u2212bF (b,d) = Zav (b = 0) e\u2212bF (b,d) ,\n\nChoosing again the normalization in which |ii = |0 . . . 0i\nand introducing a \"spin\" ski with value ski = \u22121/2 if qbit\ni in pattern pk has value |0i and ski = +1/2 if qbit i\nin pattern pk has value |1i, one can express the energy\nlevels for dH /n \u226a 1 as\n\n(36)\n\nwhere I have chosen a normalization such that exp(\u2212bF )\ndescribes the deviation of the partition function from\nits value for b = 0 (high effective temperature). Since\nZ/p, and consequently also Zav /p posses a finite, nonvanishing large-n limit, this normalization ensures that\nF (b, d) is intensive, exactly like the energy levels (34),\nand scales as a constant for large n. This is the only difference with respect to the familiar situation in statistical\nmechanics.\nThe free energy describes the equilibrium of the system at effective temperature t = 1/b and has the usual\nexpression in terms of the internal energy U and the entropy S:\nF (t, d)= U (t, d) \u2212 tS(t, d) ,\nU (t, d)= hEit ,\n\nS(t, d) =\n\n\u2212\u2202F (t, d)\n.\n\u2202t\n\nEk =\n\n\u2212F (t,d)\n2\n.\narccos e 2\n\u03c0\n\n(41)\n\nApart from a constant, this is the Hamiltonian of an\ninfinite-range antiferromagnetic Ising model in presence\nof a magnetic field. The antiferromagnetic term favours\nconfigurations\nhalf the spins up and half down, so\nPk with\nk\nk\n= \u03c0 2 /16.The magthat sktot =\ni si = 0, giving E\nnetic field, however, tends to align the spins so that\nsktot = \u2212n/2, giving E k = 0. Since this is lower than\n\u03c0 2 /16, the ground state configuration is ferromagnetic,\nwith all qbits having value |0i. At very low temperature (high b), where the energy term dominates the free\nenergy, one expects thus an ordered phase of the quantum associative memory with D(t, d) = d/n. This corresponds to a perfect identification of the presented input. As the temperature is raised (b decreased) however,\nthe thermal energy embodied by the entropy term in the\nfree energy begins to counteract the magnetic field. At\nvery high temperatures (low b) the entropy approaches\nits maximal value S(t = \u221e) = 0 (with the normalization\nchosen here). If this value is approached faster than 1/t,\nthe free energy will again be dominated by the internal\nenergy . In this case, however, this is not any more determined by the ground state but rather equally distributed\non all possible states, giving\nZ 1\n\u0010\u03c0 \u0011\n\u22121\nF (t = \u221e)= U (t = \u221e) =\ndx\n2\nlog\ncos\nx\n2\n1 \u2212 nd nd\n\u0013\n\u0012 \u00132 !\n\u0012\nd\nd\n2 log2 + O\n,\n(42)\n= 1+\nn\nn\n\n(37)\n\nNote that, with the normalization I have chosen in (36),\nthe entropy S is always a negative quantity describing\nthe deviation from its maximal value Smax = 0 at t = \u221e.\nBy inverting eq.(34) with F substituting E one can\nalso define an effective (relative) input/output Hamming\ndistance D at temperature t:\nD(t, d) =\n\n\u03c02 X k k \u03c02 X k\n\u03c02\ns .\ns s +\n+ 2\n16 4n i,j i j\n4n i i\n\n(38)\n\nThis corresponds exactly to representing the recognition\nprobability of the average memory as\n\u0010\u03c0\n\u0011\n(Pbrec )av = cos2b\nD(b, d) ,\n(39)\n2\nwhich can also be taken as the primary definition of the\neffective Hamming distance.\n\nand leading to an effective distance\n7\n\n\f2 2 log2 d\nD(t = \u221e, d) = \u2212 \u221a\n+O\n3\n\u03c0 3 n\n\n\u0012 \u00132 !\nd\n.\nn\n\nHamiltonian (22) to\n\n(43)\n\n(dH )m =\n\nThis value corresponds to a disordered phase with no\ncorrelation between input and output of the memory.\nA numerical study of the thermodynamic potentials in\n(37) and (38) indeed confirms a phase transition from the\nordered to the disordered phase as the effective temperature is raised. In Fig. 1 I show the effective distance\nD and the entropy S for 1 Mb (n = 8 \u00d7 106 ) patterns\nand d/n = 1% as a function of the inverse temperature\nb (the entropy is rescaled to the interval [0,1] for ease of\npresentation). At high temperature there is indeed a disordered phase with S = Smax = 0 and D = 2/3. At low\ntemperatures, instead, one is in the ordered phase with\nS = Smin and D = d/n = 0.01. The effective Hamming\ndistance plays thus the role of the order parameter for\nthe phase transition.\n\n\u0013\nq \u0012\nX\n\u03c33 + 1\n,\n2\nmki\ni=1\n\n(44)\n\nso that the Hamming distances to the stored patterns are\ncomputed on the basis of the known qbits only. After this\nthe pattern recall process continues exactly as described\nabove. This second possibility has the advantage that it\ndoes not introduce random noise in the similarity measure but it has the disadvantage that the operations of\nthe memory have to be adjusted to the inputs.\nV. EFFICIENCY, COMPLEXITY AND MEMORY\nTUNING\n\nAs anticipated in section 4, the effective i/o Hamming\ndistance can be used to tune the quantum associative\nmemory to prescribed accuracy levels. Typically, it is to\nbe expected that increasing this accuracy will lead to an\nenhanced complexity level. Before I even begin addressing this issue, however, I will show that the information\nretrieval algorithm is efficient.\nFirst of all I would like to point out that, in addition to the standard NOT, H (Hadamard), XOR, 2XOR\n(Toffoli) and nXOR gates [1] I have introduced only the\ntwo-qbit gates CS i in eq. (2) and the unitary operator\nexp (i\u03c0H/2n). This latter can, however also be realized\nby simple gates involving only one or two qbits. To this\nend I introduce the single-qbit gate\n\u0012 i\u03c0\n\u0013\ne 2n 0\nU=\n,\n(45)\n0\n1\n24.03.02\n\nOrder-Disorder Transition of QAMs\n\nEffective I/O distance and entropy (rescaled)\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nand the two-qbit controlled gate\n\neffective I/O distance\nentropy (rescaled to [0,1])\n\nCU \u22122 = |0ih0| \u2297 1 + |1ih1| \u2297 U \u22122 .\n\n0.0\n10 -5\n\n10 -4\n\n10 -3\n\n10 -2\n\n10 -1\n\n10 0\n\n10 1\n\n10 2\n\n10 3\n\n10 4\n\n10 5\n\n10 6\n\nNumber b of control qbits\n\n(46)\n\nIt is then easy to check that exp (i\u03c0H/2n) in eq. (21)\ncan be realized as follows:\n\nFIG. 1. Effective input/output distance and entropy\n(rescaled to [0,1]) for 1Mb patterns and d/n = 1%.\n\n\u03c0\n\nei 2n H |\u03c82 i =\n\nThe phase transition occurs at bcr \u2243 10\u22121 . The physical regime of the quantum associative memory (b = positive integer) begins thus just above this transition. For a\ngood accuracy of pattern recognition one should choose\na temperature low enough to be well into the ordered\nphase.\nHaving described at length the information retrieval\nmechanism for complete, but possibly corrupted patterns, it is easy to incorporate also incomplete ones.\nTo this end assume that only q < n qbits of the input are known and let me denote these by the indices\n{k1, . . . , kq}. After assigning the remaining qbits randomly, there are two possibilities. One can just treat the\nresulting complete input as a noisy one and proceed as\nabove or, better, one can limit the operator (dH )m in the\n\nn\nY\n\ni=1\n\nCU \u22122\n\n\u0001\n\ncmi\n\nn\nY\n\nj=1\n\nUmj |\u03c82 i ,\n\n(47)\n\nwhere c is the control qbit for which one is currently repeating the algorithm. Essentially, this means that one\nimplements first exp (i\u03c0dH /2n) and then one corrects by\nimplementing exp (\u2212i\u03c0dH /n) on that part of the quantum state for which the control qbit |ci is in state |1i.\nUsing this representation for the Hamming distance\noperator one can count the total number of simple gates\nthat one must apply in order to implement one step of the\ninformation retrieval algorithm. This is given by (6n + 2)\nusing the auxiliary register for the input and by (4n + 2)\notherwise. This retrieval step has then to be repeated\nfor each of the b control qbits. Therefore, implementing the projection by repeated measurements, the overall\ncomplexity C of information retrieval is bounded by\n8\n\n\fC \u2264 T b(6n + 2)Cclon ,\n\nby the memory energy functional. Spurious memories,\ni.e. spurious metastable minima not associated with any\nof the original patterns become important for loading\nfactors p/n > 0.14 and wash out completely the memory.\nSo, in the low p/n phase the memory works perfectly in\nthe sense that it outputs always the stored pattern which\nis most similar to the input. For p/n > 0.14, instead,\nthere is an abrupt transition to total amnesia caused by\nspurious memories.\nQuantum associative memories work better than classical ones since they are free from spurious memories.\nThe easiest way to see this is in the formulation\n\n(48)\n\nwhere Cclon is the complexity of the (probabilistic)\ncloning machine that prepares a copy of the memory\nstate.\nThe computation of the complexity is easier for the\ninformation retrieval algorithm which uses the amplitude\namplification technique. In this case the initial memory\nis prepared only once by a product of the operators M ,\nwith complexity p(2n + 3) + 1 and R(i), with complexity\nb(4n+ 2). Then one applies T times the operator Q, with\ncomplexity p(4n+ 6)+ b(8n+ 4)+2 +CS + CS0 , where CS\nand CS0 are the polynomial complexities of the oracles\nimplementing S and S0 . This gives\n\n|mi = M |0i .\n\n(50)\n\nAll the information about the stored patterns is encoded\nin the unitary operator M . This is such that all quantum\nstates that do not correspond to stored patterns have\nexactly vanishing amplitudes.\nAn analogy with the classical Hopfield model [6] can\nbe established as follows. Instead of generating the memory state from the initial zero state one can start from a\nuniform superposition of the computational basis. This\nis achieved by the operator M W defined by\n\nC= T [p(4n + 6) + b(8n + 4) + 2 + CS + CS0 ] +\n+p(2n + 3) + b(4n + 2) + 1 .\n(49)\nAs expected, this result depends on both T and b, the parameters governing the recognition and identification efficiencies. This represents exactly the unavoidable tradeoff\nbetween accuracy and complexity.\nSuppose now one would like to recognize on average\ninputs with up to 1% of corrupted or missing bits and\nidentify them with high accuracy. The effective i/o Hamming distance D shown in Fig. 1 can then be used to\ndetermine the values of the required parameters T and\nb needed to reach this accuracy for the average memory\nwith p/n \u226a 1. For b = 104 e.g., one has D = 0.018,\nwhich gives the average i/o distance (in percent of total\nqbits) if the minimum possible i/o distance is 0.01. For\nthis value of b the recognition probability is 1.5 10\u22124 .\nWith the measurement repetition technique one should\nthus set the threshold T = 0.6 104 . Using amplitude\namplification, however, one needs only around T = 80\nrepetitions.\nI would like to conclude by stressing that the values\nof b and T obtained by tuning the memory with the\neffective i/o Hamming distance become n-independent\nfor large values of n. This is because they are intensive variables unaffected by this \"thermodynamic limit\".\nFor any p polynomial in n the information retrieval can\nthen be implemented efficiently and the overall complexity is determined by the accuracy requirements via the\nn-independent parameters T and b.\n\nn\n\n2 \u22121\n1 X\n|mi= M W \u221a\n|ji ,\n2n j=0\n\nW\u2261\n\nn\nY\n\nHj .\n\n(51)\n\nj=1\n\nNow, the same result can also be obtained by Grover's\nalgorithm, or better by its generalization with zero failure\nrate [19]. Here the state |mi is obtained by applying to\nthe uniform superposition of the computational basis q\ntimes the search operator X defined by\nn\n\n2 \u22121\n1 X\n|mi= X \u221a\n|ji ,\n2n j=0\nq\n\nX\u2261 \u2212W J0 W J ,\n\n(52)\n\nwhere J rotates the amplitudes of the states corresponding to the patterns to be stored by a phase \u03c6 which is\nvery close to \u03c0 (the original Grover value) for large n and\nJ0 does the same on the zero state. Via the two equations (51) and (52), the memory operator M provides an\nimplicit realization of the phase shift operator J. Being\na unitary operator, this can always be written as an exponential of an hermitian Hamiltonian H, which is the\nquantum generalization of a classical energy functional.\nBy defining J \u2261 exp(\u2212iH) one obtains an energy operator which is diagonal in the computational basis. The\nenergy eigenvalues of this operator are such that the patterns to be stored have energy E = \u2212\u03c6 \u2243 \u2212\u03c0 while all\nothers have energy E = 0. This formulation is the exact\nquantum generalization of the Hopfield model; the important point is that the operator M realizes efficiently a\n\nVI. CONCLUSION\n\nI would like to conclude this review by stressing the\nreason why a quantum associative memory works better\nthan its classical counterpart.\nIn classical associative memories, the information\nabout the patterns to recall is typically stored in an energy functional. When retrieving information, the input\nconfiguration evolves to the corresponding output, driven\nby the memory functional. The capacity shortage is due\nto a phase transition in the statistical ensemble governed\n9\n\n\fdynamics in which the patterns to be stored are always,\nfor all numbers p of patterns, the exact global minima\nof a quantum energy landscape, without the appearance\nof any spurious memories. The price to pay is the probabilistic nature of the information retrieval mechanism.\nAs always in quantum mechanics, the dynamics determines only the evolution of probability distributions and\nthe probabilistic aspect is brought in by the collapse of\nthis probability distributions upon measurement. Therefore, contrary to the classical Hopfield model in the low\np/n phase, one does not always have the absolute guarantee that an input is recognized, and identified correctly\nas the stored pattern most similar to the input, even if\nthis state has the highest probability of being measured.\nBut, after all, the same happens also to the human brain.\n\n[17] L.-M. Duan and G.-C. Guo, Phys. Rev. Lett. 80, 4999\n(1998).\n[18] A. Chefles and S. M. Barnett, Phys. Rev. A60, 136 (1999).\n[19] G. L. Long, \"Grover Algorithm with Zero Theoretical Failure Rate\", quant-ph/0106071.\n\n[1] For a review see: M. A. Nielsen and I. L. Chuang, \"Quantum Computation and Quantum Information\", Cambridge\nUniversity Press, Cambridge (2000); A. O. Pittenger,\n\"An Introdcution to Quantum Computing Algorithms\",\nBirkh\u00e4user, Boston (2000).\n[2] P. W. Shor, SIAM J. Computing 26 (1997) 1484.\n[3] L. Grover, Phys. Rev. Lett. 79 (1997) 325.\n[4] C. A. Trugenberger, Phys. Rev. Lett. 87.067901 (2001),\nC. A Trugenberger, quant-ph/0204115, to appear in Phys.\nRev. Lett..\n[5] M. Sasaki, A. Carlini and R. Jozsa, Phys. Rev.\nA64.022317 (2001); M Sasaki and A. Carlini, quantph/0202173; R. Sch\u00fctzhold, quant-ph/0208063.\n[6] For a review see: B. M\u00fcller and J. Reinhardt, \"Neural Networks\", Springer-Verlag, Berlin (1990); T. Kohonen, \"SelfOrganization and Associative Memory\", Springer-Verlag,\nBerlin (1984).\n[7] J.J. Hopfield, Proc. Natl. Acad. Scie. USA 79, 2554 (1982).\n[8] B. Kosko, IEEE Trans. on Systems, Man and Cybernetics\n18, 49 (1988).\n[9] See e.g. M. Mezard, G. Parisi and M. A. Virasoro, \"Spin\nGlass Theory and Beyond\", World Scientific, Singapore\n(1987).\n[10] G. Brassard, P. Hoyer, M. Mosca and A. Tapp, \"Amplitude\nAmplification and Estimation\", quant-ph/0005055.\n[11] N. Sourlas, Nature 339, 693 (1989); I. Kanter and D. Saad,\nPhys. Rev. Lett. 83, 2660 (1999); Y. Kabashima, T. Murayama and D. Saad, Phys. Rev. Lett. 84, 1355 (2000).\n[12] Y. Kabashima, T. Murayama and D. Saad, Phys. Rev.\nLett. 84 2030 (2000).\n[13] See e.g. A. Barenco, C. Bennet, R. Cleve, D. DiVincenzo,\nN. Margolus, P. Shor, T. Sleator, J. Smolin and H. Weinfurter, Phys. Rev. A52, 3457 (1995).\n[14] W. Wootters and W. Zurek, Nature 299, 802 (1982).\n[15] V. Buzek and M. Hillery, Phys. Rev. A54, 1844 (1996).\n[16] N. Gisin and S. Massar, Phys. Rev. Lett. 79, 2153 (1997);\nD. Bruss, A. K. Ekert and C. Macchiavello, Phys. Rev.\nLett. 81, 2598 (1998).\n\n10\n\n\f"}