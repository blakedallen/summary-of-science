{"id": "http://arxiv.org/abs/1011.2234v2", "guidislink": true, "updated": "2010-11-24T16:42:42Z", "updated_parsed": [2010, 11, 24, 16, 42, 42, 2, 328, 0], "published": "2010-11-09T23:22:24Z", "published_parsed": [2010, 11, 9, 23, 22, 24, 1, 313, 0], "title": "Strong rules for discarding predictors in lasso-type problems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.5942%2C1011.2319%2C1011.5924%2C1011.4893%2C1011.5517%2C1011.3850%2C1011.1573%2C1011.4737%2C1011.0707%2C1011.3995%2C1011.4575%2C1011.1866%2C1011.1166%2C1011.6594%2C1011.3871%2C1011.2811%2C1011.0456%2C1011.2761%2C1011.4517%2C1011.5731%2C1011.4217%2C1011.6590%2C1011.5306%2C1011.3143%2C1011.4188%2C1011.2234%2C1011.6248%2C1011.5265%2C1011.1741%2C1011.5506%2C1011.4143%2C1011.3367%2C1011.4007%2C1011.5773%2C1011.5905%2C1011.1798%2C1011.1435%2C1011.4213%2C1011.0332%2C1011.4083%2C1011.6323%2C1011.2835%2C1011.6113%2C1011.3744%2C1011.2008%2C1011.5365%2C1011.1683%2C1011.1279%2C1011.4727%2C1011.0294%2C1011.3130%2C1011.5351%2C1011.2044%2C1011.3566%2C1011.5499%2C1011.1548%2C1011.5902%2C1011.5391%2C1011.4204%2C1011.1870%2C1011.2209%2C1011.4604%2C1011.4386%2C1011.0408%2C1011.4856%2C1011.3236%2C1011.5286%2C1011.0698%2C1011.5514%2C1011.5303%2C1011.5342%2C1011.6293%2C1011.4457%2C1011.2600%2C1011.1935%2C1011.3100%2C1011.1984%2C1011.3537%2C1011.2994%2C1011.5885%2C1011.1401%2C1011.3439%2C1011.0565%2C1011.3887%2C1011.2391%2C1011.3653%2C1011.1972%2C1011.0879%2C1011.0999%2C1011.2979%2C1011.5868%2C1011.6120%2C1011.5463%2C1011.1686%2C1011.4699%2C1011.5025%2C1011.5937%2C1011.6086%2C1011.5069%2C1011.0681%2C1011.6041&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Strong rules for discarding predictors in lasso-type problems"}, "summary": "We consider rules for discarding predictors in lasso regression and related\nproblems, for computational efficiency. El Ghaoui et al (2010) propose \"SAFE\"\nrules that guarantee that a coefficient will be zero in the solution, based on\nthe inner products of each predictor with the outcome. In this paper we propose\nstrong rules that are not foolproof but rarely fail in practice. These can be\ncomplemented with simple checks of the Karush- Kuhn-Tucker (KKT) conditions to\nprovide safe rules that offer substantial speed and space savings in a variety\nof statistical convex optimization problems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.5942%2C1011.2319%2C1011.5924%2C1011.4893%2C1011.5517%2C1011.3850%2C1011.1573%2C1011.4737%2C1011.0707%2C1011.3995%2C1011.4575%2C1011.1866%2C1011.1166%2C1011.6594%2C1011.3871%2C1011.2811%2C1011.0456%2C1011.2761%2C1011.4517%2C1011.5731%2C1011.4217%2C1011.6590%2C1011.5306%2C1011.3143%2C1011.4188%2C1011.2234%2C1011.6248%2C1011.5265%2C1011.1741%2C1011.5506%2C1011.4143%2C1011.3367%2C1011.4007%2C1011.5773%2C1011.5905%2C1011.1798%2C1011.1435%2C1011.4213%2C1011.0332%2C1011.4083%2C1011.6323%2C1011.2835%2C1011.6113%2C1011.3744%2C1011.2008%2C1011.5365%2C1011.1683%2C1011.1279%2C1011.4727%2C1011.0294%2C1011.3130%2C1011.5351%2C1011.2044%2C1011.3566%2C1011.5499%2C1011.1548%2C1011.5902%2C1011.5391%2C1011.4204%2C1011.1870%2C1011.2209%2C1011.4604%2C1011.4386%2C1011.0408%2C1011.4856%2C1011.3236%2C1011.5286%2C1011.0698%2C1011.5514%2C1011.5303%2C1011.5342%2C1011.6293%2C1011.4457%2C1011.2600%2C1011.1935%2C1011.3100%2C1011.1984%2C1011.3537%2C1011.2994%2C1011.5885%2C1011.1401%2C1011.3439%2C1011.0565%2C1011.3887%2C1011.2391%2C1011.3653%2C1011.1972%2C1011.0879%2C1011.0999%2C1011.2979%2C1011.5868%2C1011.6120%2C1011.5463%2C1011.1686%2C1011.4699%2C1011.5025%2C1011.5937%2C1011.6086%2C1011.5069%2C1011.0681%2C1011.6041&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We consider rules for discarding predictors in lasso regression and related\nproblems, for computational efficiency. El Ghaoui et al (2010) propose \"SAFE\"\nrules that guarantee that a coefficient will be zero in the solution, based on\nthe inner products of each predictor with the outcome. In this paper we propose\nstrong rules that are not foolproof but rarely fail in practice. These can be\ncomplemented with simple checks of the Karush- Kuhn-Tucker (KKT) conditions to\nprovide safe rules that offer substantial speed and space savings in a variety\nof statistical convex optimization problems."}, "authors": ["Robert Tibshirani", "Jacob Bien", "Jerome Friedman", "Trevor Hastie", "Noah Simon", "Jonathan Taylor", "Ryan J. Tibshirani"], "author_detail": {"name": "Ryan J. Tibshirani"}, "author": "Ryan J. Tibshirani", "arxiv_comment": "5", "links": [{"href": "http://arxiv.org/abs/1011.2234v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.2234v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "62J07 62G08", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.2234v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.2234v2", "journal_reference": null, "doi": null, "fulltext": "arXiv:1011.2234v2 [math.ST] 24 Nov 2010\n\nStrong Rules for Discarding Predictors in\nLasso-type Problems\nRobert Tibshirani,\nTrevor Hastie,\n\n\u2217\n\nJacob Bien,\nNoah Simon,\nRyan Tibshirani\n\nJerome Friedman,\nJonathan Taylor,\n\nNovember 25, 2010\n\nAbstract\nWe consider rules for discarding predictors in lasso regression and\nrelated problems, for computational efficiency. El Ghaoui et al. (2010)\npropose \"SAFE\" rules, based on univariate inner products between each\npredictor and the outcome, that guarantee a coefficient will be zero in\nthe solution vector. This provides a reduction in the number of variables\nthat need to be entered into the optimization. In this paper, we propose\nstrong rules that are not foolproof but rarely fail in practice. These are\nvery simple, and can be complemented with simple checks of the KarushKuhn-Tucker (KKT) conditions to ensure that the exact solution to the\nconvex problem is delivered. These rules offer a substantial savings in both\ncomputational time and memory, for a variety of statistical optimization\nproblems.\n\n1\n\nIntroduction\n\nOur focus here is on statistical models fit using l1 regularization. We start\nwith penalized linear regression. Consider a problem with N observations and\np predictors, and let y denote the N -vector of outcomes, and X be the N \u00d7 p\nmatrix of predictors, with jth column xj and ith row xi . For a set of indices\nA = {j1 , . . . jk }, we write XA to denote the N \u00d7k submatrix XA = [xj1 , . . . xjk ],\nand also bA = (bj1 , . . . bjk ) for a vector b. We assume that the predictors and\noutcome are centered, so we can omit an intercept term from the model.\nThe lasso Tibshirani (1996) solves the optimization problem\n\u03b2\u0302 = argmin\n\u03b2\n\n1\nky \u2212 X\u03b2k22 + \u03bbk\u03b2k1 ,\n2\n\n(1)\n\n\u2217 Departments of Statistics and Health Research and Policy, Stanford University, Stanford\nCA 94305, USA. E-mail: tibs@stanford.edu\n\n1\n\n\fwhere \u03bb \u2265 0 is a tuning parameter. There has been considerable work in the\npast few years deriving fast algorithms for this problem, especially for large\nvalues of N and p. A main reason for using the lasso is that the l1 penalty\ntends to give exact zeros in \u03b2\u0302, and therefore it performs a kind of variable\nselection. Now suppose we knew, a priori to solving (1), that a subset of the\nvariables S \u2286 {1, . . . p} will have zero coefficients in the solution, that is, \u03b2\u0302S = 0.\nThen we could solve problem (1) with the design matrix replaced by XS c , where\nS c = {1, . . . p} \\ S, for the remaining coefficients \u03b2\u0302S c . If S is relatively large,\nthen this could result in a substantial computational savings.\nEl Ghaoui et al. (2010) construct such a set S of \"screened\" or \"discarded\"\nvariables by looking at the inner products |xTj y|, j = 1, . . . p. The authors\nuse a clever argument to derive a surprising set of rules called \"SAFE\", and\nshow that applying these rules can reduce both time and memory in the overall\ncomputation. In a related work, Wu et al. (2009) study l1 penalized logistic\nregression and build a screened set S based on similar inner products. However,\ntheir construction does not guarantee that the variables in S actually have zero\ncoefficients in the solution, and so after fitting on XS c , the authors check the\nKarush-Kuhn-Tucker (KKT) optimality conditions for violations. In the case\nof violations, they weaken their set S, and repeat this process. Also, Fan &\nLv (2008) study the screening of variables based on their inner products in the\nlasso and related problems, but not from a optimization point of view. Their\nscreening rules may again set coefficients to zero that are nonzero in the solution,\nhowever, the authors argue that under certain situations this can lead to better\nperformance in terms of estimation risk.\nIn this paper, we propose strong rules for discarding predictors in the lasso\nand other problems that involve lasso-type penalties. These rules discard many\nmore variables than the SAFE rules, but are not foolproof, because they can\nsometimes exclude variables from the model that have nonzero coefficients in\nthe solution. Therefore we rely on KKT conditions to ensure that we are indeed\ncomputing the correct coefficients in the end. Our method is most effective\nfor solving problems over a grid of \u03bb values, because we can apply our strong\nrules sequentially down the path, which results in a considerable reduction in\ncomputational time. Generally speaking, the power of the proposed rules stems\nfrom the fact that:\n\u2022 the set of discarded variables S tends to be large and violations rarely\noccur in practice, and\n\u2022 the rules are very simple and can be applied to many different problems,\nincluding the elastic net, lasso penalized logistic regression, and the graphical lasso.\nIn fact, the violations of the proposed rules are so rare, that for a while a group\nof us were trying to establish that they were foolproof. At the same time, others\nin our group were looking for counter-examples [hence the large number of coauthors!]. After many flawed proofs, we finally found some counter-examples to\n\n2\n\n\fthe strong sequential bound (although not to the basic global bound). Despite\nthis, the strong sequential bound turns out to be extremely useful in practice.\nHere is the layout of this paper. In Section 2 we review the SAFE rules of El\nGhaoui et al. (2010) for the lasso. The strong rules are introduced and illustrated\nin Section 3 for this same problem. We demonstrate that the strong rules rarely\nmake mistakes in practice, especially when p \u226b N . In Section 4 we give a\ncondition under which the strong rules do not erroneously discard predictors\n(and hence the KKT conditions do not need to be checked). We discuss the\nelastic net and penalized logistic regression in Sections 5 and 6. Strong rules for\nmore general convex optimization problems are given in Section 7, and these are\napplied to the graphical lasso. In Section 8 we discuss how the strong sequential\nrule can be used to speed up the solution of convex optimization problems,\nwhile still delivering the exact answer. We also cover implementation details of\nthe strong sequential rule in our glmnet algorithm (coordinate descent for lasso\npenalized generalized linear models). Section 9 contains some final discussion.\n\n2\n\nReview of the SAFE rules\n\nThe basic SAFE rule of El Ghaoui et al. (2010) for the lasso is defined as follows:\nfitting at \u03bb, we discard predictor j if\n|xTj y| < \u03bb \u2212 kxj k2 kyk2\n\n\u03bbmax \u2212 \u03bb\n,\n\u03bbmax\n\n(2)\n\nwhere \u03bbmax = maxi |xTi y| is the smallest \u03bb for which all coefficients are zero.\nThe authors derive this bound by looking at a dual of the lasso problem (1).\nThis is:\n\u03b8\u0302 = argmax G(\u03b8) =\n\u03b8\n\n1\n1\nkyk22 \u2212 ky + \u03b8k22\n2\n2\n\n(3)\n\nsubject to |xTj \u03b8| \u2264 \u03bb for j = 1, . . . p.\nThe relationship between the primal and dual solutions is \u03b8\u0302 = X\u03b2\u0302 \u2212 y, and\n\uf8f1\n\uf8f4\nif \u03b2\u0302j > 0\n\uf8f2{+\u03bb}\n(4)\nxTj \u03b8\u0302 \u2208 {\u2212\u03bb}\nif \u03b2\u0302j < 0\n\uf8f4\n\uf8f3\n[\u2212\u03bb, \u03bb] if \u03b2\u0302j = 0\n\nfor each j = 1, . . . p. Here is a sketch of the argument: first we find a dual feasible\npoint of the form \u03b80 = sy, (s is a scalar), and hence \u03b3 = G(sy) represents a\nlower bound for the value of G at the solution. Therefore we can add the\nconstraint G(\u03b8) \u2265 \u03b3 to the dual problem (3) and nothing will be changed. For\neach predictor j, we then find\nmj = argmax |xTj \u03b8| subject to G(\u03b8) \u2265 \u03b3.\n\u03b8\n\n3\n\n\fIf mj < \u03bb (note the strict inequality), then certainly at the solution |xTj \u03b8\u0302| < \u03bb,\nwhich implies that \u03b2\u0302j = 0 by (4). Finally, noting that s = \u03bb/\u03bbmax produces a\ndual feasible point and rewriting the condition mj < \u03bb gives the rule (2).\nIn addition to the basic SAFE bound, the authors also derive a more complicated but somewhat better bound that they call \"recursive SAFE\" (RECSAFE).\nAs we will show, the SAFE rules have the advantage that they will never discard\na predictor when its coefficient is truly nonzero. However, they discard far fewer\npredictors than the strong sequential rule, introduced in the next section.\n\n3\n3.1\n\nStrong screening rules\nBasic and strong sequential rules\n\nOur basic (or global) strong rule for the lasso problem (1) discards predictor j\nif\n|xTj y| < 2\u03bb \u2212 \u03bbmax ,\n(5)\nwhere as before \u03bbmax = maxj |xTj y|.\nWhen the predictors are standardized (kxj k2 = 1 for each j), it is not difficult\nto see that the right hand side of (2) is always smaller than the right hand side\nof (5), so that in this case the SAFE rule is always weaker than the basic strong\nrule. This follows since \u03bbmax \u2264 kyk2 , so that\n\u03bb \u2212 kyk2\n\n\u03bbmax \u2212 \u03bb\n\u2264 \u03bb \u2212 (\u03bbmax \u2212 \u03bb) = 2\u03bb \u2212 \u03bbmax .\n\u03bbmax\n\nFigure 1 illustrates the SAFE and basic strong rules in an example.\nWhen the predictors are not standardized, the ordering between the two\nbounds is not as clear, but the strong rule still tends to discard more variables\nin practice unless the predictors have wildly different marginal variances.\nWhile (5) is somewhat useful, its sequential version is much more powerful.\nSuppose that we have already computed the solution \u03b2\u0302(\u03bb0 ) at \u03bb0 , and wish to\ndiscard predictors for a fit at \u03bb < \u03bb0 . Defining the residual r = y \u2212 X\u03b2\u0302(\u03bb0 ),\nour strong sequential rule discards predictor j if\n|xTj r| < 2\u03bb \u2212 \u03bb0 .\n\n(6)\n\nBefore giving a detailed motivation for these rules, we first demonstrate their\nutility. Figure 2 shows some examples of the applications of the SAFE and\nstrong rules. There are four scenarios with various values of N and p; in the\nfirst three panels, the X matrix is dense, while it is sparse in the bottom right\npanel. The population correlation among the feature is zero, positive, negative\nand zero in the four panels. Finally, 25% of the coefficients are non-zero, with\na standard Gaussian distribution. In the plots, we are fitting along a path\nof decreasing \u03bb values and the plots show the number of predictors left after\nscreening at each stage. We see that the SAFE and RECSAFE rules only exclude\npredictors near the beginning of the path. The strong rules are more effective:\n4\n\n\f0.2\n\nBasic strong bound\n8\n\n0.1\n0.0\n\n5\n2\n9\n7\n4\n6\n10\n\n\u22120.1\n\nInner product with residual\n\nSAFE bound\n\n\u22120.2\n\nSAFE bound\n1\n\nBasic strong bound\n\n3\n0.00\n\n0.05\n\n0.10\n\n0.15\n\n0.20\n\n0.25\n\n\u03bb\n\nFigure 1: SAFE and basic strong bounds in an example with 10 predictors,\nlabelled at the right. The plot shows the inner product of each predictor with the\ncurrent residual, with the predictors in the model having maximal inner product\nequal to \u00b1\u03bb. The dotted vertical line is drawn at \u03bbmax ; the broken vertical line\nis drawn at \u03bb. The strong rule keeps only predictor #3, while the SAFE bound\nkeeps predictors #8 and #1 as well.\n\n5\n\n\fremarkably, the strong sequential rule discarded almost all of the predictors that\nhave coefficients of zero. There were no violations of any of rules in any of the\nfour scenarios.\nIt is common practice to standardize the predictors before applying the lasso,\nso that the penalty term makes sense. This is what was done in the examples\nof Figure 2. But in some instances, one might not want to standardize the\npredictors, and so in Figure 3 we investigate the performance of the rules in this\ncase. In the left panel the population variance of each predictor is the same; in\nthe right panel it varies by a factor of 50. We see that in the latter case the\nSAFE rules outperform the basic strong rule, but the sequential strong rule is\nstill the clear winner. There were no violations in any of rules in either panel.\n\n3.2\n\nMotivation for the strong rules\n\nWe now give some motivation for the strong rule (5) and later, the sequential\nrule (6). We start with the KKT conditions for the lasso problem (1). These\nare\n(7)\nxTj (y \u2212 X\u03b2\u0302) = \u03bb * sj\nfor j = 1, . . . p, where sj is a subgradient\n\uf8f1\n\uf8f4\n\uf8f2{+1}\nsj \u2208 {\u22121}\n\uf8f4\n\uf8f3\n[\u22121, 1]\n\nof \u03b2\u0302j :\nif \u03b2\u0302j > 0\nif \u03b2\u0302j < 0\nif \u03b2\u0302j = 0.\n\n(8)\n\nLet cj (\u03bb) = xTj {y\u2212X\u03b2\u0302(\u03bb)}, where we emphasize the dependence on \u03bb. Suppose\nin general that we could assume\n|c\u2032j (\u03bb)| \u2264 1,\n\n(9)\n\nwhere c\u2032j is the derivative with respect to \u03bb, and we ignore possible points of\nnon-differentiability. This would allow us to conclude that\nZ \u03bbmax\n|cj (\u03bbmax ) \u2212 cj (\u03bb)| =\nc\u2032j (\u03bb) d\u03bb\n(10)\n\u03bb\n\n\u2264\n\nZ\n\n\u03bbmax\n\n\u03bb\n\n|c\u2032j (\u03bb)| d\u03bb\n\n(11)\n\n\u2264 \u03bbmax \u2212 \u03bb,\nand so\n|cj (\u03bbmax )| < 2\u03bb \u2212 \u03bbmax \u21d2 |cj (\u03bb)| < \u03bb \u21d2 \u03b2\u0302j (\u03bb) = 0,\n\nthe last implication following from the KKT conditions, (7) and (8). Then the\nstrong rule (5) follows as \u03b2\u0302(\u03bbmax ) = 0, so that |cj (\u03bbmax )| = |xTj y|.\nWhere does the slope condition (9) come from? The product rule applied to\n(7) gives\nc\u2032j (\u03bb) = sj (\u03bb) + \u03bb * s\u2032j (\u03bb),\n(12)\n6\n\n\fDense Nocorr n= 200 p= 5000\n\n100\n\n1000\n\n3000\n\n5000\n\n0.16 0.25 0.33 0.4\n\n0\n\n3000\n1000\n\n50\n\n0 0.04\nNumber of predictors left after filtering\n\n0.48\n\n5000\n\n0.35\n\nSAFE\nstrong/global\nstrong/seq\nRECSAFE\n\n0\n\n150\n\n0\n\n20\n\n40\n\n60\n\n80\n\n120\n\nNumber of predictors in model\n\nNumber of predictors in model\n\nDense Neg n= 200 p= 5000\n\nSparse Nocorr n= 500 p= 50000\n\n50\n\n100\n\n150\n\nNumber of predictors in model\n\n0.5 0.79\n\n0.9 0.96 0.98\n\n0\n\n200\n\n600\n\n15000\n\n5000\n3000\n1000\n0\n0\n\n0\n25000\n\n0.5\n\n5000\n\n0.4\n\n0\n\n0.09 0.21 0.3\n\nNumber of predictors left after filtering\n\n0\nNumber of predictors left after filtering\n\n0.23\n\n0\n\nNumber of predictors left after filtering\n\n0 0.09\n\nDense Pos n= 200 p= 5000\n\n1000\n\nNumber of predictors in model\n\nFigure 2: Lasso regression: results of different rules applied to four different\nscenarios. There are four scenarios with\n7 various values of N and p; in the first\nthree panels the X matrix is dense, while it is sparse in the bottom right panel.\nThe population correlation among the feature is zero, positive, negative and zero\nin the four panels. Finally, 25% of the coefficients are non-zero, with a standard\nGaussian distribution. In the plots, we are fitting along a path of decreasing \u03bb\nvalues and the plots show the number of predictors left after screening at each\nstage. A broken line with unit slope is added for reference. The proportion of\nvariance explained by the model is shown along the top of the plot. There were\n\n\fEqual population variance\n\n0\n\n50\n\n100\n\n150\n\n1000 2000 3000 4000 5000\n0\n\nNumber of predictors in model\n\n0.82 0.93 0.97\n\n0\n\nSAFE\nstrong/global\nstrong/seq\nRECSAFE\n\n0 0.29 0.5 0.67\nNumber of predictors left after filtering\n\n1000 2000 3000 4000 5000\n\n0.82 0.93 0.97\n\n0\n\nNumber of predictors left after filtering\n\n0 0.29 0.5 0.67\n\nUnequal population variance\n\n50\n\n100\n\n150\n\nNumber of predictors in model\n\nFigure 3: Lasso regression: results of different rules when the predictors are not\nstandardized. The scenario in the left panel is the same as in the top left panel\nof Figure 2, except that the features are not standardized before fitting the lasso.\nIn the data generation for the right panel, each feature is scaled by a random\nfactor between 1 and 50, and again, no standardization is done.\n\n8\n\n\fand as |sj (\u03bb)| \u2264 1, condition (9) can be obtained if we simply drop the second term above. For an active variable, that is \u03b2\u0302j (\u03bb) 6= 0, we have sj (\u03bb) =\nsign{\u03b2\u0302j (\u03bb)}, and continuity of \u03b2\u0302j (\u03bb) with respect to \u03bb implies s\u2032j (\u03bb) = 0. But\ns\u2032j (\u03bb) 6= 0 for inactive variables, and hence the bound (9) can fail, which makes\nthe strong rule (5) imperfect. It is from this point of view-writing out the\nKKT conditions, taking a derivative with respect to \u03bb, and dropping a term-\nthat we derive strong rules for l1 penalized logistic regression and more general\nproblems.\nIn the lasso case, condition (9) has a more concrete interpretation. From\nEfron et al. (2004), we know that each coordinate of the solution \u03b2\u0302j (\u03bb) is a\npiecewise linear function of \u03bb, hence so is each inner product cj (\u03bb). Therefore\ncj (\u03bb) is differentiable at any \u03bb that is not a kink, the points at which variables\nenter or leave the model. In between kinks, condition (9) is really just a bound\non the slope of cj (\u03bb). The idea is that if we assume the absolute slope of cj (\u03bb) is\nat most 1, then we can bound the amount that cj (\u03bb) changes as we move from\n\u03bbmax to a value \u03bb. Hence if the initial inner product cj (\u03bbmax ) starts too far\nfrom the maximal achieved inner product, then it cannot \"catch up\" in time.\nAn illustration is given in Figure 4.\nThe argument for the strong bound (intuitively, an argument about slopes),\nuses only local information and so it can be applied to solving (1) on a grid of \u03bb\nvalues. Hence by the same argument as before, the slope assumption (9) leads\nto the strong sequential rule (6).\nIt is interesting to note that\n|xTj r| < \u03bb\n\n(13)\n\nis just the KKT condition for excluding a variable in the solution at \u03bb. The\nstrong sequential bound is \u03bb\u2212 (\u03bb0 \u2212 \u03bb) and we can think of the extra term \u03bb0 \u2212 \u03bb\nas a buffer to account for the fact that |xTj r| may increase as we move from \u03bb0\nto \u03bb. Note also that as \u03bb0 \u2192 \u03bb, the strong sequential rule becomes the KKT\ncondition (13), so that in effect the sequential rule at \u03bb0 \"anticipates\" the KKT\nconditions at \u03bb.\nIn summary, it turns out that the key slope condition (9) very often holds,\nbut can be violated for short stretches, especially when p \u2248 N and for small\nvalues of \u03bb in the \"overfit\" regime of a lasso problem. In the next section we\nprovide an example that shows a violation of the slope bound (9), which breaks\nthe strong sequential rule (6). We also give a condition on the design matrix\nX under which the bound (9) is guaranteed to hold. However in simulations\nin that section, we find that these violations are rare in practice and virtually\nnon-existent when p >> N .\n\n9\n\n\f1.0\n0.8\n0.6\n0.4\n\ncj = xTj (y \u2212 X\u03b2\u0302)\n\n0.2\n\n\u03bbmax \u2212\u03bb1\n\n\u03bbmax\n\n0.0\n\n\u03bb1\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\n\u03bb\nFigure 4: Illustration of the slope bound (9) leading to the strong rule (6). The\ninner product cj is plotted in red as a function of \u03bb, restricted to only one\npredictor for simplicity. The slope of cj between \u03bbmax and \u03bb1 is bounded in\nabsolute value by 1, so the most it can rise over this interval is \u03bbmax \u2212 \u03bb1 .\nTherefore, if it starts below \u03bb1 \u2212 (\u03bbmax \u2212 \u03bb1 ) = 2\u03bb1 \u2212 \u03bbmax , it can not possibly\nreach the critical level by \u03bb1 .\n\n4\n4.1\n\nSome analysis of the strong rules\nViolation of the slope condition\n\nHere we demonstrate a counter-example of both the slope bound (9) and of\nthe strong sequential rule (6). We believe that a counter-example for the basic\nstrong rule (5) can also be constructed, but we have not yet found one. Such an\nexample is somewhat more difficult to construct because it would require that\nthe average slope exceed 1 from \u03bbmax to \u03bb, rather than exceeding 1 for short\nstretches of \u03bb values.\nWe took N = 50 and p = 30, with the entries of y and X drawn independently from a standard normal distribution. Then we centered y and the\n\n10\n\n\fcolumns of X, and standardized the columns of X. As Figure 5 shows, the\nslope of cj (\u03bb) = xTj {y \u2212 X\u03b2\u0302(\u03bb)} is c\u2032j (\u03bb) = \u22121.586 for all \u03bb \u2208 [\u03bb1 , \u03bb0 ], where\n\u03bb1 = 0.0244, \u03bb0 = 0.0259, and j = 2. Moreover, if we were to use the solution\nat \u03bb0 to eliminate predictors for the fit at \u03bb1 , then we would eliminate the 2nd\npredictor based on the bound (6). But this is clearly a problem, because the\n2nd predictor enters the model at \u03bb1 . By continuity, we can choose \u03bb1 in an\ninterval around 0.0244 and \u03bb0 in an interval around 0.0259, and still break the\nstrong sequential rule (6).\n\n4.2\n\nA sufficient condition for the slope bound\n\nTibshirani & Taylor (2010) prove a general result that can be used to give the\nfollowing sufficient condition for the unit slope bound (9). Under this condition,\nboth basic and strong sequential rules are guaranteed not toPfail.\nRecall that a matrix A is diagonally dominant if |Aii | \u2265 j6=i |Aij | for all i.\nTheir result gives us the following:\nTheorem 1. Suppose that X is N \u00d7 p, with N \u2265 p, and of full rank. If\n(XT X)\u22121 is diagonally dominant,\n\n(14)\n\nthen the slope bound (9) holds at all points where cj (\u03bb) is differentiable, for\nj = 1, . . . p, and hence the strong rules (5), (6) never produce violations.\nProof. Tibshirani & Taylor (2010) consider a generalized lasso problem\nargmin\n\u03b2\n\n1\nky \u2212 X\u03b2\u0302k22 + \u03bbkD\u03b2k1 ,\n2\n\n(15)\n\nwhere D is a general m \u00d7 p penalty matrix. In the proof of their \"boundary\nlemma\", Lemma 1, they show that if rank(X) = p and D(XT X)\u22121 DT is diagonally dominant, then the dual solution \u00fb(\u03bb) corresponding to problem (15)\nsatisfies\n|\u00fbj (\u03bb) \u2212 \u00fbj (\u03bb0 )| \u2264 |\u03bb \u2212 \u03bb0 |\nfor any j = 1, . . . m and \u03bb, \u03bb0 . By piecewise linearity of \u00fbj (\u03bb), this means that\n|\u00fb\u2032j (\u03bb)| \u2264 1 at all \u03bb except the kink points. Furthermore, when D = I, problem\n(15) is simply the lasso, and it turns out that the dual solution \u00fbj (\u03bb) is exactly\nthe inner product cj (\u03bb) = xTj {y \u2212 X\u03b2\u0302(\u03bb)}. This proves the slope bound (9)\nunder the condition that (XT X)\u22121 is diagonally dominant.\nFinally, the kink points are countable and hence form a set of Lebesgue measure zero. Therefore cj (\u03bb) is differentiable almost everywhere and the integrals\nin (10) and (11) make sense. This proves the strong rules (5) and (6).\nWe note a similarity between condition (14) and the positive cone condition\nused in Efron et al. (2004). It is not hard to see that the positive cone condition\nimplies (14), and actually (14) is a much weaker condition because it doesn't\nrequire looking at every possible subset of columns.\n11\n\n\f0.00\n\u22120.01\n\ncj = xTj (y \u2212 X\u03b2\u0302)\n\n0.01\n\n0.02\n\n2\u03bb1 \u2212 \u03bb0\n\n\u22120.02\n\n\u03bb1 \u03bb0\n\n0.000\n\n0.005\n\n0.010\n\n0.015\n\n0.020\n\n\u03bb\nFigure 5: Example of a violation of the slope bound (9), which breaks the strong\nsequential rule (6). The entries of y and X were generated as independent,\nstandard normal random variables with N = 50 and p = 30. (Hence there is no\nunderlying signal.) The lines with slopes \u00b1\u03bb are the envelop of maximal inner\nproducts achieved by predictors in the model for each \u03bb. For clarity we only\nshow a short stretch of the solution path. The rightmost vertical line is drawn\nat \u03bb0 , and we are considering the new value \u03bb1 < \u03bb0 , the vertical line to its\nleft. The horizontal line is the bound (9). In the top right part of the plot, the\ninner product path for the predictor j = 2 is drawn in red, and starts below the\nbound, but enters the model at \u03bb1 . The slope of the red segment between \u03bb0 and\n\u03bb1 exceeds 1 in absolute value. A dotted line with slope -1 is drawn beside the\nred segment for reference.\n12\n\n0.025\n\n\fA simple model in which diagonal dominance holds is when the columns\nof X are orthonormal, because then XT X = I. But the diagonal dominance\ncondition (14) certainly holds outside of the orthogonal design case. We give\ntwo such examples below.\n\u2022 Equi-correlation model. Suppose that kxj k2 = 1 for all j, and xTj xk = r\nfor all j 6= k. Then the inverse of XT X is\n\u0012\n\u0013\n1\n11T\n1\nT\n\u22121\n(X X) = I *\n,\n\u2212\n1 \u2212 r 1 \u2212 r 1 + r(p \u2212 1)\nwhere 1 is the vector of all ones. This is diagonally dominant as along as\nr \u2265 0.\n\u2022 Haar basis model. Suppose that the columns\nsimplest example being\n\uf8eb\n1\n\uf8ec 1 1\n\uf8ec\nX=\uf8ec .\n\uf8ed ..\n1\n\n1 ... 1\n\nof X form a Haar basis, the\n\uf8f6\n\n\uf8f7\n\uf8f7\n\uf8f7,\n\uf8f8\n\n(16)\n\nthe lower triangular matrix of ones. Then (XT X)\u22121 is diagonally dominant. This arises, for example, in the one-dimensional fused lasso where\nwe solve\nN\nN\nX\n1X\n|\u03b2i \u2212 \u03b2i\u22121 |.\n(yi \u2212 \u03b2i )2 + \u03bb\nargmin\n2 i=1\n\u03b2\ni=2\nIf we transform this problem to the parameters \u03b11 = 1, \u03b1i = \u03b2i \u2212 \u03b2i\u22121 for\ni = 2, . . . N , then we get a lasso with design X as in (16).\n\n4.3\n\nConnection to the irrepresentable condition\n\nThe slope bound (9) possesses an interesting connection to a concept called the\n\"irrepresentable condition\". Let us write A as the set of active variables at \u03bb,\nA = {j : \u03b2\u0302j (\u03bb) 6= 0},\nand kbk\u221e = maxi |bi | for a vector b. Then, using the work of Efron et al.\n(2004), we can express the slope condition (9) as\nkXTAc XA (XTA XA )\u22121 sign(\u03b2\u0302A )k\u221e \u2264 1,\n\n(17)\n\nwhere by XTA and XTAc , we really mean (XA )T and (XAc )T , and the sign is\napplied element-wise.\nOn the other hand, a common condition appearing in work about model\nselection properties of lasso, in both the finite-sample and asymptotic settings,\nis the so called \"irrepresentable condition\" ?, which is closely related to the\n13\n\n\fconcept of \"mutual incoherence\" ?. Roughly speaking, if \u03b2T denotes the nonzero\ncoefficients in the true model, then the irrepresentable condition is that\nkXTT c XT (XTT XT )\u22121 sign(\u03b2T )k\u221e \u2264 1 \u2212 \u01eb\n\n(18)\n\nfor some 0 < \u01eb \u2264 1.\nThe conditions (18) and (17) appear extremely similar, but a key difference\nbetween the two is that the former pertains to the true coefficients that generated the data, while the latter pertains to those found by the lasso optimization\nproblem. Because T is associated with the true model, we can put a probability\ndistribution on it and a probability distribution on sign(\u03b2T ), and then show\nthat with high probability, certain designs X are mutually incoherent (18). For\nexample, Candes & Plan (2009) suppose that k is sufficiently small, T is drawn\nfrom the uniform distribution on k-sized subsets of {1, . . . p}, and each entry of\nsign(\u03b2T ) is equal to +1 or \u22121 with probability 1/2, independent of each other.\nUnder this model, they show that designs X with maxj6=k |xTj xk | = O(1/ log p)\nsatisfy the irrepresentable condition with very high probability.\nUnfortunately the same types of arguments cannot be applied directly to\n(17). A distribution on T and sign(\u03b2T ) induces a different distribution on A\nand sign(\u03b2\u0302A ), via the lasso optimization procedure. Even if the distributions of\nT and sign(\u03b2T ) are very simple, the distributions of A and sign(\u03b2\u0302A ) can become\nquite complicated. Still, it does not seem hard to believe that confidence in (18)\ntranslates to some amount of confidence in (17). Luckily for us, we do not need\nthe slope bound (17) to hold exactly or with any specified level of probability,\nbecause we are using it as a computational tool and can simply revert to checking\nthe KKT conditions when it fails.\n\n4.4\n\nA numerical investigation of the strong sequential rule\nviolations\n\nWe generated Gaussian data with N = 100, varying values of the number of\npredictors p and pairwise correlation 0.5 between the predictors. One quarter\nof the coefficients were non-zero, with the indices of the nonzero predictors\nrandomly chosen and their values equal to \u00b12. We fit the lasso for 80 equally\nspaced values of \u03bb from \u03bbmax to 0, and recorded the number of violations of\nthe strong sequential rule. Figure 6 shows the number of violations (out of p\npredictors) averaged over 100 simulations: we plot versus the percent variance\nexplained instead of \u03bb, since the former is more meaningful. Since the vertical\naxis is the total number of violations, we see that violations are quite rare\nin general never averaging more than 0.3 out of p predictors. They are more\ncommon near the right end of the path. They also tend to occur when p is fairly\nclose to N . When p \u226b N (p = 500 or 1000 here), there were no violations. Not\nsurprisingly, then, there were no violations in the numerical examples in this\npaper since they all have p \u226b N .\nLooking at (13), it suggests that if we take a finer grid of \u03bb values, there\nshould be fewer violations of the rule. However we have not found this to be\n14\n\n\f0.5\n0.4\n0.3\n0.2\n0.1\n0.0\n\nTotal number of violations\n\np=20\np=50\np=100\np=500\np=1000\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nPercent variance explained\n\nFigure 6: Total number of violations (out of p predictors) of the strong sequential\nrule, for simulated data with N = 100 and varying values of p. A sequence of\nmodels is fit, with decreasing values of \u03bb as we move from left to right. The\nfeatures are uncorrelated. The results are averages over 100 simulations.\n\n15\n\n\ftrue numerically: the average number of violations at each grid point \u03bb stays\nabout the same.\n\n5\n\nScreening rules for the elastic net\n\nIn the elastic net we solve the problem\nminimize\n\n1\n\n1\n1\n||y \u2212 X\u03b2||2 + \u03bb2 ||\u03b2||2 + \u03bb1 ||\u03b2||1\n2\n2\n\n(19)\n\nLetting\n\u0013\n\u0012 \u0013\n\u0012\nX\ny\n\u2217\n\u221a\n; y =\n,\nX =\n0\n\u03bb2 * I\n\n(20)\n\n1\nminimize ||y\u2217 \u2212 X\u2217 \u03b2||2 + \u03bb1 ||\u03b2||1 .\n2\n\n(21)\n\n\u2217\n\nwe can write (19) as\n\nIn this form we can apply the SAFE rule p\n(2) to obtain a rule for discarding\npredictors. Now |x\u2217j T y\u2217 | = |xTj y|, ||x\u2217j || = ||xj ||2 + \u03bb2 , ||y\u2217 || = ||y||. Hence\nthe global rule for discarding predictor j is\nq\n\u03bb1max \u2212 \u03bb1\n(22)\n|xTj y| < \u03bb1 \u2212 ||y|| * ||xj ||2 + \u03bb2 *\n\u03bb1max\nNote that the glmnet package uses the parametrization ((1 \u2212 \u03b1)\u03bb, \u03b1\u03bb) rather\nthan (\u03bb2 , \u03bb1 ). With this parametrization the basic SAFE rule has the form\nq\n\u0010\n\u03bbmax \u2212 \u03bb \u0011\n(23)\n|xTj y| < \u03b1\u03bb \u2212 ||y|| * ||xj ||2 + (1 \u2212 \u03b1)\u03bb *\n\u03bbmax\nThe strong screening rules turn out to be the same as for the lasso. With\nthe glmnet parametrization the global rule is simply\n|xTj y| < \u03b1(2\u03bb \u2212 \u03bbmax )\n\n(24)\n\nwhile the sequential rule is\n|xTj r| < \u03b1(2\u03bb \u2212 \u03bb0 ).\n\n(25)\n\nFigure 7 show results for the elastic net with standard independent Gaussian\ndata, n = 100, p = 1000, for 3 values of \u03b1. There were no violations in any of\nthese figures, i.e. no predictor was discarded that had a non-zero coefficient\nat the actual solution. Again we see that the strong sequential rule performs\nextremely well, leaving only a small number of excess predictors at each stage.\n1 This differs from the original form of the \"naive\" elastic net in Zou & Hastie (2005) by\nthe factors of 1/2, just for notational convenience.\n\n16\n\n\f\u03b1 = 0.1\n\n\u03b1 = 0.5\n\n0 100\n\n300\n\n500\n\nNumber of predictors in model\n\n0\n\n50\n\n\u03b1 = 0.9\n\n0.7 0.82 0.98\n\n0 0.31 0.57 0.87 0.99\n\n800\n600\n400\n200\n0\n\n200\n\n400\n\n600\n\n800\n\nNumber of predictors left after filtering\n\n1000\n\n0.33\n\n0\n\nNumber of predictors left after filtering\n\n800\n600\n400\n200\n\nSAFE\nstrong global\nstrong/seq\n\n0\n\nNumber of predictors left after filtering\n\n0\n1000\n\n0.86 0.97\n\n1000\n\n0 0.4 0.69\n\n100 150 200\n\nNumber of predictors in model\n\n0\n\n40\n\n80\n\nNumber of predictors in model\n\nFigure 7: Elastic net: results for different rules for three different values of the\nmixing parameter \u03b1. In the plots, we are fitting along a path of decreasing \u03bb\nvalues and the plots show the number of predictors left after screening at each\nstage. The proportion of variance explained by the model is shown along the\ntop of the plot is shown. There were no violations of any of the rules in the 3\nscenarios.\n\n17\n\n120\n\n\f6\n\nScreening rules for logistic regression\n\nHere we have a binary response yi = 0, 1 and we assume the logistic model\nPr(Y = 1|x) = 1/(1 + exp(\u2212\u03b20 \u2212 xT \u03b2))\n\n(26)\n\nLetting pi = Pr(Y = 1|xi ), the penalized log-likelihood is\nX\n[yi log pi + (1 \u2212 yi ) log(1 \u2212 pi )] + \u03bb||\u03b2||1\nl(\u03b20 , \u03b2) = \u2212\n\n(27)\n\ni\n\nEl Ghaoui et al. (2010) derive an exact global rule for discarding predictors,\nbased on the inner products between y and each predictor, using the same kind\nof dual argument as in the Gaussian case.\nHere we investigate the analogue of the strong rules (5) and (6). The subgradient equation for logistic regression is\nXT (y \u2212 p(\u03b2)) = \u03bb * sign(\u03b2)\n\n(28)\n\nThis leads to the global rule: letting p\u0304 = 1\u0233, \u03bbmax = max|xTj (y \u2212 p\u0304)|, we\ndiscard predictor j if\n|xTj (y \u2212 p\u0304)| < 2\u03bb \u2212 \u03bbmax\n\n(29)\n\nThe sequential version, starting at \u03bb0 , uses p0 = p(\u03b2\u03020 (\u03bb0 ), \u03b2\u0302(\u03bb0 )):\n|xTj (y \u2212 p0 )| < 2\u03bb \u2212 \u03bb0 .\n\n(30)\n\nFigure 8 show the result of various rules in an example, the newsgroup\ndocument classification problem (Lang 1995). We used the training set cultured\nfrom these data by Koh et al. (2007). The response is binary, and indicates\na subclass of topics; the predictors are binary, and indicate the presence of\nparticular tri-gram sequences. The predictor matrix has 0.05% nonzero values.\n2\nResults for are shown for the new global rule (29) and the new sequential\nrule (30). We were unable to compute the logistic regression global SAFE rule\nfor this example, using our R language implementation, as this had a very long\ncomputation time. But in smaller examples it performed much like the global\nSAFE rule in the Gaussian case. Again we see that the strong sequential rule\n(30), after computing the inner product of the residuals with all predictors at\neach stage, allows us to discard the vast majority of the predictors before fitting.\nThere were no violations of either rule in this example.\nSome approaches to penalized logistic regression such as the glmnet package\nuse a weighted least squares iteration within a Newton step. For these algorithms, an alternative approach to discarding predictors would be to apply one\nof the Gaussian rules within the weighted least squares iteration.\n2 This\ndataset\nis\navailable\nas\na\nhttp://www-stat.stanford.edu/ hastie/glmnet\n\n18\n\nsaved\n\nR\n\ndata\n\nobject\n\nat\n\n\f1e+05\n1e+03\n1e+01\n\nNumber of predictors left after filtering\n\nNewsgroup data\n\nstrong/global\nstrong/seq\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\nNumber of predictors in model\n\nFigure 8: Logistic regression: results for newsgroup example, using the new\nglobal rule (29) and the new sequential rule (30). The broken black curve is the\n45o line, drawn on the log scale.\n\n19\n\n\fWu et al. (2009) used |xTj (y \u2212 p\u0304)| to screen predictors (SNPs) in genomewide association studies, where the number of variables can exceed a million.\nSince they only anticipated models with say k < 15 terms, they selected a small\nmultiple, say 10k, of SNPs and computed the lasso solution path to k terms.\nAll the screened SNPs could then be checked for violations to verify that the\nsolution found was global.\n\n7\n\nStrong rules for general problems\n\nSuppose that we have a convex problem of the form\nK\nh\ni\nX\ng(\u03b2j )\nminimize\u03b2 f (\u03b2) + \u03bb *\n\n(31)\n\nk=1\n\nwhere f and g are convex functions, f is differentiable and \u03b2 = (\u03b21 , \u03b22 , . . . \u03b2K )\nwith each \u03b2k being a scalar or vector. Suppose further that the subgradient\nequation for this problem has the form\nf \u2032 (\u03b2) + \u03bb * sk = 0; k = 1, 2, . . . K\n\n(32)\n\nwhere each subgradient variable sk satisfies ||sk ||q \u2264 A, and ||sk ||q = A when\nthe constraint g(\u03b2j ) = 0 is satisfied (here ||*||q is a norm). Suppose that we have\ntwo values \u03bb < \u03bb0 , and corresponding solutions \u03b2\u0302(\u03bb), \u03b2\u0302(\u03bb0 ). Then following the\nsame logic as in Section 3, we can derive the general strong rule\n||\n\nf (\u03b2\u03020k\n)||q < (1 + A)\u03bb \u2212 A\u03bb0\nd\u03b2k\n\n(33)\n\nThis can be applied either globally or sequentially. In the lasso regression setting, it is easy to check that this reduces to the rules (5),(6) where A = 1.\nThe rule (33) has many potential applications. For example in the graphical\nlasso for sparse inverse covariance estimation (Friedman et al. 2007), we observe\nN multivariate normal observations of dimension p, with mean 0 and covariance\n\u03a3, with observed empirical covariance matrix S. Letting \u0398 = \u03a3\u22121 , the problem\nis to maximize the penalized log-likelihood\nlog det \u0398 \u2212 tr(S\u0398) \u2212 \u03bb||\u0398||1 ,\n\n(34)\n\nover non-negative definite matrices \u0398. The penalty ||\u0398||1 sums the absolute\nvalues of the entries of \u0398; we assume that the diagonal is not penalized. The\nsubgradient equation is\n\u03a3 \u2212 S \u2212 \u03bb * \u0393 = 0,\n(35)\nwhere \u0393ij \u2208 sign(\u0398ij ). One could apply the rule (33) elementwise, and this\nwould be useful for an optimization method that operates elementwise. This\ngives a rule of the form |Sij \u2212 \u03a3\u0302(\u03bb0 )| < 2\u03bb \u2212 \u03bb0 . However, the graphical lasso\nalgorithm proceeds in a blockwise fashion, optimizing one whole row and column\n20\n\n\f300\n250\n200\n150\n100\n50\n0\n\nNumber rows/cols left after filtering\n\nstrong global\nstrong/seq\n\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\nNumber rows/cols in model\n\nFigure 9: Strong global and sequential rules applied to the graphical lasso. A\nbroken line with unit slope is added for reference.\nat a time. Hence for the graphical lasso, it is more effective to discard entire\nrows and columns at once. For each row i, let s12 , \u03c312 , and \u039312 denote Si,\u2212i ,\n\u03a3i,\u2212i , and \u0393i,\u2212i , respectively. Then the subgradient equation for one row has\nthe form\n\u03c312 \u2212 s12 \u2212 \u03bb * \u039312 = 0,\n(36)\nNow given two values \u03bb < \u03bb0 , and solution \u03a3\u03020 at \u03bb0 , we form the sequential\nrule\n0\nmax|\u03c3\u030212\n\u2212 s12 | < 2\u03bb \u2212 \u03bb0 .\n(37)\nIf this rule is satisfied, we discard the entire ith row and column of \u0398, and\nhence set them to zero (but retain the ith diagonal element). Figure 9 shows an\nexample with N = 100, p = 300, standard independent Gaussian variates. No\nviolations of the rule occurred.\nFinally, we note that strong rules can be derived in a similar way, for other\nproblems such as the group lasso (Yuan & Lin 2007). In particular, if Xl denotes\nthe n \u00d7 pl block of the design matrix corresponding to the features in the lth\ngroup, then the strong sequential rule is simply\n||XTl r||2 < 2\u03bb \u2212 \u03bbmax .\nWhen this holds, we set \u03b2 l = 0.\n21\n\n\f8\n\nImplementation and numerical studies\n\nThe strong sequential rule (6) can be used to provide potential speed improvements in convex optimization problems. Generically, given a solution \u03b2\u0302(\u03bb0 ) and\nconsidering a new value \u03bb < \u03bb0 , let S(\u03bb) be the indices of the predictors that\nsurvive the screening rule (6): we call this the strong set. Denote by E the\neligible set of predictors. Then a useful strategy would be\n1. Set E = S(\u03bb).\n2. Solve the problem at value \u03bb using only the predictors in E.\n3. Check the KKT conditions at this solution for all predictors. If there are\nno violations, we are done. Otherwise add the predictors that violate the\nKKT conditions to the set E, and repeat steps (b) and (c).\nDepending on how the optimization is done in step (b), this can be quite effective. Now in the glmnet procedure, coordinate descent is used, with warm\nstarts over a grid of decreasing values of \u03bb. In addition, an \"ever-active\" set\nof predictors A(\u03bb) is maintained, consisting of the indices of all predictors that\nhave a non-zero coefficient for some \u03bb\u2032 greater than the current value \u03bb under\nconsideration. The solution is first found for this active set: then the KKT\nconditions are checked for all predictors. if there are no violations, then we have\nthe solution at \u03bb; otherwise we add the violators into the active set and repeat.\nThe two strategies above are very similar, with one using the strong set\nS(\u03bb) and the other using the ever-active set A(\u03bb). Figure 10 shows the active\nand strong sets for an example. Although the strong rule greatly reduces the\ntotal number of predictors, it contains more predictors than the ever-active set;\naccordingly, violations occur more often in the ever-active set than the strong\nset. This effect is due to the high correlation between features and the fact\nthat the signal variables have coefficients of the same sign. It also occurs with\nlogistic regression with lower correlations, say 0.2.\nIn light of this, we find that using both A(\u03bb) and S(\u03bb) can be advantageous.\nFor glmnet we adopt the following combined strategy:\n1. Set E = A(\u03bb).\n2. Solve the problem at value \u03bb using only the predictors in E.\n3. Check the KKT conditions at this solution for all predictors in S(\u03bb). If\nthere are violations, add these predictors into E, and go back to step (a)\nusing the current solution as a warm start.\n4. Check the KKT conditions for all predictors. If there are no violations,\nwe are done. Otherwise add these violators into A(\u03bb), recompute S(\u03bb)\nand go back to step (a) using the current solution as a warm start.\nNote that violations in step (c) are fairly common, while those in step (d) are\nrare. Hence the fact that the size of S(\u03bb) is \u226a p can make this an effective\nstrategy.\n22\n\n\f1000\n600\n200\n0\n\nNumber of predictors\n\n15000\n5000\n0\n\nNumber of predictors\n\never active\nstrong/seq\n\n0\n\n20\n\n40\n\n60\n\n80\n\nNumber of predictors in model\n\n0\n\n20\n\n40\n\n60\n\n80\n\nNumber of predictors in model\n\nFigure 10: Gaussian lasso setting, N = 200, p = 20, 000, pairwise correlation\nbetween features of 0.7. The first 50 predictors have positive, decreasing coefficients. Shown are the number of predictors left after applying the strong\nsequential rule (6) and the number that have ever been active (i.e. had a nonzero coefficient in the solution) for values of \u03bb larger than the current value.\nA broken line with unit slope is added for reference. The right-hand plot is a\nzoomed version of the left plot.\n\n23\n\n\fWe implemented this strategy and compare it to the standard glmnet algorithm in a variety of problems, shown in Tables 1\u20133. Details are given in the\ntable captions. We see that the new strategy offers a speedup factor of five or\nmore in some cases, and never seems to slow things down.\nThe strong sequential rules also have the potential for space savings. With a\nlarge dataset, one could compute the inner products {xTj r}p1 offline to determine\nthe strong set of predictors, and then carry out the intensive optimization steps\nin memory using just this subset of the predictors.\n\n9\n\nDiscussion\n\nIn this paper we have proposed strong global and sequential rules for discarding\npredictors in statistical convex optimization problems such as the lasso. When\ncombined with checks of the KKT conditions, these can offer substantial improvements in speed while still yielding the exact solution. We plan to include\nthese rules in a future version of the glmnet package.\nThe RECSAFE method uses the solution at a given point \u03bb0 to derive a rule\nfor discarding predictors at \u03bb < \u03bb0 . Here is another way to (potentially) apply\nthe SAFE rule in a sequential manner. Suppose that we have \u03b2\u03020 = \u03b2\u0302(\u03bb0 ), and\nr = y \u2212 X\u03b2\u03020 , and we consider the fit at \u03bb < \u03bb0 , with r = y \u2212 X\u03b2\u03020 . Defining\n\u03bb0\n\n= maxj (|xTj r|);\n\n(38)\n\n\u03bb0 \u2212 \u03bb\n\u03bb0\n\n(39)\n\nwe discard predictor j if\n|xTj r| < \u03bb \u2212 ||r|||xj ||\n\nWe have been unable to prove the correctness of this rule, and do not know if it\nis infallible. At the same time, we have been not been able to find a numerical\nexample in which it fails.\nAcknowledgements: We thank Stephen Boyd for his comments, and Laurent El Ghaoui and his co-authors for sharing their paper with us before publication, and for helpful feedback on their work. The first author was supported\nby National Science Foundation Grant DMS-9971405 and National Institutes of\nHealth Contract N01-HV-28183.\n\nReferences\nCandes, E. J. & Plan, Y. (2009), 'Near-ideal model selection by l1 minimization',\nAnnals of Statistics 37(5), 2145\u20132177.\nEfron, B., Hastie, T., Johnstone, I. & Tibshirani, R. (2004), 'Least angle regression', Annals of Statistics 32(2), 407\u2013499.\n\n24\n\n\fEl Ghaoui, L., Viallon, V. & Rabbani, T. (2010), Safe feature elimination in\nsparse supervised learning, Technical Report UC/EECS-2010-126, EECS\nDept., University of California at Berkeley.\nFan, J. & Lv, J. (2008), 'Sure independence screening for ultra-high dimensional\nfeature space', Journal of the Royal Statistical Society Series B, to appear\n.\nFriedman, J., Hastie, T., Hoefling, H. & Tibshirani, R. (2007), 'Pathwise coordinate optimization', Annals of Applied Statistics 2(1), 302\u2013332.\nFuchs, J. (2005), 'Recovery of exact sparse representations in the presense of\nnoise', IEEE Transactions on Information Theory 51(10), 3601\u20133608.\nKoh, K., Kim, S.-J. & Boyd, S. (2007), 'An interior-point method for large-scale\nl1-regularized logistic regression', Journal of Machine Learning Research\n8, 1519\u20131555.\nLang, K. (1995), Newsweeder: Learning to filter netnews., in 'Proceedings of\nthe Twenty-First International Conference on Machine Learning (ICML)',\npp. 331\u2013339.\nMeinhausen, N. & Buhlmann, P. (2006), 'High-dimensional graphs and variable\nselection with the lasso', Annals of Statistics 34, 1436\u20131462.\nTibshirani, R. (1996), 'Regression shrinkage and selection via the lasso', Journal\nof the Royal Statistical Society Series B 58(1), 267\u2013288.\nTibshirani, R. & Taylor, J. (2010), The solution path of the generalized lasso.\nSubmitted.\n*http://www-stat.stanford.edu/~ryantibs/papers/genlasso.pdf\nTropp, J. (2006), 'Just relax: Convex programming methods for identifying sparse signals in noise', IEEE Transactions on Information Theory\n3(52), 1030\u20131051.\nWainwright, M. (2006), Sharp thresholds for high-dimensional and noisy sparsity recovery using l1 -constrained quadratic programming (lasso), Technical report, Statistics and EECS Depts., University of California at Berkeley.\nWu, T. T., Chen, Y. F., Hastie, T., Sobel, E. & Lange, K. (2009), 'Genomewide\nassociation analysis by lasso penalized logistic regression', Bioinformatics\n25(6), 714\u2013721.\nYuan, M. & Lin, Y. (2007), 'Model selection and estimation in regression\nwith grouped variables', Journal of the Royal Statistical Society, Series\nB 68(1), 49\u201367.\nZhao, P. & Yu, B. (2006), 'On model selection consistency of the lasso', Journal\nof Machine Learning Research 7, 2541\u20132563.\n25\n\n\fZou, H. & Hastie, T. (2005), 'Regularization and variable selection via the elastic\nnet', Journal of the Royal Statistical Society Series B. 67(2), 301\u2013320.\n\n26\n\n\fTable 1: Glmnet timings (seconds) for fitting a lasso problem in the Gaussian\nsetting. In the first four columns, there are p = 100, 000 predictors, N = 200\nobservations, 30 nonzero coefficients, with the same value and signs alternating; signal-to-noise ratio equal to 3. In the rightmost column, the data matrix\nis sparse, consisting of just zeros and ones, with 0.1% of the values equal to\n1. There are p = 50, 000 predictors, N = 500 observations, with 25% of the\ncoefficients nonzero, having a Gaussian distribution; signal-to-noise ratio equal\nto 4.3.\nMethod\nPopulation correlation\n0.0 0.25\n0.5\n0.75 Sparse\nglmnet\n4.07 6.13 9.50 17.70\n4.14\n2.98\n2.52\nwith seq-strong 2.50 2.54 2.62\n\nTable 2: Glmnet timings (seconds) for fitting an elastic net problem. There are\np = 100, 000 predictors, N = 200 observations, 30 nonzero coefficients, with the\nsame value and signs alternating; signal-to-noise ratio equal to 3\nMethod\n\u03b1\n1.0\n0.5\n0.2\n0.1 0.01\nglmnet\n9.49 7.98 5.88 5.34 5.26\nwith seq-strong 2.64 2.65 2.73 2.99 5.44\n\nTable 3: Glmnet timings (seconds) fitting a lasso/logistic regression problem.\nHere the data matrix is sparse, consisting of just zeros and ones, with 0.1% of\nthe values equal to 1. There are p = 50, 000 predictors, N = 800 observations,\nwith 30% of the coefficients nonzero, with the same value and signs alternating;\nBayes error equal to 3%.\nMethod\nPopulation correlation\n0.0\n0.5\n0.8\nglmnet\n11.71 12.41 12.69\nwith seq-strong\n6.31 9.491 12.86\n\n27\n\n\f"}