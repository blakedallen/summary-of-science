{"id": "http://arxiv.org/abs/cs/0502086v1", "guidislink": true, "updated": "2005-02-22T09:51:16Z", "updated_parsed": [2005, 2, 22, 9, 51, 16, 1, 53, 0], "published": "2005-02-22T09:51:16Z", "published_parsed": [2005, 2, 22, 9, 51, 16, 1, 53, 0], "title": "The Self-Organization of Speech Sounds", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0502079%2Ccs%2F0502077%2Ccs%2F0502084%2Ccs%2F0502024%2Ccs%2F0502058%2Ccs%2F0502035%2Ccs%2F0502008%2Ccs%2F0502004%2Ccs%2F0502048%2Ccs%2F0502031%2Ccs%2F0502089%2Ccs%2F0502065%2Ccs%2F0502074%2Ccs%2F0502064%2Ccs%2F0502071%2Ccs%2F0502053%2Ccs%2F0502055%2Ccs%2F0502082%2Ccs%2F0502039%2Ccs%2F0502033%2Ccs%2F0502047%2Ccs%2F0502060%2Ccs%2F0502088%2Ccs%2F0502036%2Ccs%2F0502078%2Ccs%2F0502093%2Ccs%2F0502056%2Ccs%2F0502057%2Ccs%2F0502038%2Ccs%2F0502050%2Ccs%2F0502007%2Ccs%2F0502029%2Ccs%2F0502015%2Ccs%2F0502037%2Ccs%2F0502090%2Ccs%2F0502075%2Ccs%2F0502067%2Ccs%2F0502069%2Ccs%2F0502061%2Ccs%2F0502044%2Ccs%2F0502086%2Ccs%2F0502010%2Ccs%2F0502070%2Ccs%2F0502045%2Ccs%2F0502066%2Ccs%2F0502040%2Ccs%2F0502043%2Ccs%2F0604089%2Ccs%2F0604047%2Ccs%2F0604087%2Ccs%2F0604053%2Ccs%2F0604041%2Ccs%2F0604030%2Ccs%2F0604110%2Ccs%2F0604019%2Ccs%2F0604076%2Ccs%2F0604108%2Ccs%2F0604105%2Ccs%2F0604012%2Ccs%2F0604063%2Ccs%2F0604092%2Ccs%2F0604083%2Ccs%2F0604022%2Ccs%2F0604046%2Ccs%2F0604018%2Ccs%2F0604077%2Ccs%2F0604071%2Ccs%2F0604034%2Ccs%2F0604069%2Ccs%2F0604027%2Ccs%2F0604090%2Ccs%2F0604096%2Ccs%2F0604060%2Ccs%2F0604081%2Ccs%2F0604024%2Ccs%2F0604009%2Ccs%2F0604064%2Ccs%2F0604082%2Ccs%2F0604036%2Ccs%2F0604080%2Ccs%2F0604073%2Ccs%2F0604043%2Ccs%2F0604072%2Ccs%2F0604003%2Ccs%2F0604084%2Ccs%2F0604068%2Ccs%2F0604061%2Ccs%2F0604101%2Ccs%2F0604004%2Ccs%2F0604055%2Ccs%2F0604113%2Ccs%2F0604005%2Ccs%2F0604066%2Ccs%2F0604037%2Ccs%2F0604050%2Ccs%2F0604007%2Ccs%2F0604094%2Ccs%2F0604008%2Ccs%2F0604059%2Ccs%2F0604111%2Ccs%2F0604035&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Self-Organization of Speech Sounds"}, "summary": "The speech code is a vehicle of language: it defines a set of forms used by a\ncommunity to carry information. Such a code is necessary to support the\nlinguistic interactions that allow humans to communicate. How then may a speech\ncode be formed prior to the existence of linguistic interactions? Moreover, the\nhuman speech code is discrete and compositional, shared by all the individuals\nof a community but different across communities, and phoneme inventories are\ncharacterized by statistical regularities. How can a speech code with these\nproperties form? We try to approach these questions in the paper, using the\n\"methodology of the artificial\". We build a society of artificial agents, and\ndetail a mechanism that shows the formation of a discrete speech code without\npre-supposing the existence of linguistic capacities or of coordinated\ninteractions. The mechanism is based on a low-level model of sensory-motor\ninteractions. We show that the integration of certain very simple and non\nlanguage-specific neural devices leads to the formation of a speech code that\nhas properties similar to the human speech code. This result relies on the\nself-organizing properties of a generic coupling between perception and\nproduction within agents, and on the interactions between agents. The\nartificial system helps us to develop better intuitions on how speech might\nhave appeared, by showing how self-organization might have helped natural\nselection to find speech.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0502079%2Ccs%2F0502077%2Ccs%2F0502084%2Ccs%2F0502024%2Ccs%2F0502058%2Ccs%2F0502035%2Ccs%2F0502008%2Ccs%2F0502004%2Ccs%2F0502048%2Ccs%2F0502031%2Ccs%2F0502089%2Ccs%2F0502065%2Ccs%2F0502074%2Ccs%2F0502064%2Ccs%2F0502071%2Ccs%2F0502053%2Ccs%2F0502055%2Ccs%2F0502082%2Ccs%2F0502039%2Ccs%2F0502033%2Ccs%2F0502047%2Ccs%2F0502060%2Ccs%2F0502088%2Ccs%2F0502036%2Ccs%2F0502078%2Ccs%2F0502093%2Ccs%2F0502056%2Ccs%2F0502057%2Ccs%2F0502038%2Ccs%2F0502050%2Ccs%2F0502007%2Ccs%2F0502029%2Ccs%2F0502015%2Ccs%2F0502037%2Ccs%2F0502090%2Ccs%2F0502075%2Ccs%2F0502067%2Ccs%2F0502069%2Ccs%2F0502061%2Ccs%2F0502044%2Ccs%2F0502086%2Ccs%2F0502010%2Ccs%2F0502070%2Ccs%2F0502045%2Ccs%2F0502066%2Ccs%2F0502040%2Ccs%2F0502043%2Ccs%2F0604089%2Ccs%2F0604047%2Ccs%2F0604087%2Ccs%2F0604053%2Ccs%2F0604041%2Ccs%2F0604030%2Ccs%2F0604110%2Ccs%2F0604019%2Ccs%2F0604076%2Ccs%2F0604108%2Ccs%2F0604105%2Ccs%2F0604012%2Ccs%2F0604063%2Ccs%2F0604092%2Ccs%2F0604083%2Ccs%2F0604022%2Ccs%2F0604046%2Ccs%2F0604018%2Ccs%2F0604077%2Ccs%2F0604071%2Ccs%2F0604034%2Ccs%2F0604069%2Ccs%2F0604027%2Ccs%2F0604090%2Ccs%2F0604096%2Ccs%2F0604060%2Ccs%2F0604081%2Ccs%2F0604024%2Ccs%2F0604009%2Ccs%2F0604064%2Ccs%2F0604082%2Ccs%2F0604036%2Ccs%2F0604080%2Ccs%2F0604073%2Ccs%2F0604043%2Ccs%2F0604072%2Ccs%2F0604003%2Ccs%2F0604084%2Ccs%2F0604068%2Ccs%2F0604061%2Ccs%2F0604101%2Ccs%2F0604004%2Ccs%2F0604055%2Ccs%2F0604113%2Ccs%2F0604005%2Ccs%2F0604066%2Ccs%2F0604037%2Ccs%2F0604050%2Ccs%2F0604007%2Ccs%2F0604094%2Ccs%2F0604008%2Ccs%2F0604059%2Ccs%2F0604111%2Ccs%2F0604035&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The speech code is a vehicle of language: it defines a set of forms used by a\ncommunity to carry information. Such a code is necessary to support the\nlinguistic interactions that allow humans to communicate. How then may a speech\ncode be formed prior to the existence of linguistic interactions? Moreover, the\nhuman speech code is discrete and compositional, shared by all the individuals\nof a community but different across communities, and phoneme inventories are\ncharacterized by statistical regularities. How can a speech code with these\nproperties form? We try to approach these questions in the paper, using the\n\"methodology of the artificial\". We build a society of artificial agents, and\ndetail a mechanism that shows the formation of a discrete speech code without\npre-supposing the existence of linguistic capacities or of coordinated\ninteractions. The mechanism is based on a low-level model of sensory-motor\ninteractions. We show that the integration of certain very simple and non\nlanguage-specific neural devices leads to the formation of a speech code that\nhas properties similar to the human speech code. This result relies on the\nself-organizing properties of a generic coupling between perception and\nproduction within agents, and on the interactions between agents. The\nartificial system helps us to develop better intuitions on how speech might\nhave appeared, by showing how self-organization might have helped natural\nselection to find speech."}, "authors": ["Pierre-Yves Oudeyer"], "author_detail": {"name": "Pierre-Yves Oudeyer"}, "author": "Pierre-Yves Oudeyer", "links": [{"href": "http://arxiv.org/abs/cs/0502086v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0502086v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.RO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.DS", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0502086v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0502086v1", "arxiv_comment": null, "journal_reference": "Journal of Theoretical Biology 233 (2005) Issue 3, Pages 435-449", "doi": null, "fulltext": "The Self-Organization of Speech Sounds\n\narXiv:cs/0502086v1 [cs.LG] 22 Feb 2005\n\nPierre-Yves Oudeyer\nSony CSL Paris,\n6, rue Amyot,\n75005 Paris, France\n\nAbstract\nThe speech code is a vehicle of language: it defines a set of forms used by a\ncommunity to carry information. Such a code is necessary to support the linguistic\ninteractions that allow humans to communicate. How then may a speech code be\nformed prior to the existence of linguistic interactions? Moreover, the human speech\ncode is discrete and compositional, shared by all the individuals of a community\nbut different across communities, and phoneme inventories are characterized by\nstatistical regularities. How can a speech code with these properties form?\nWe try to approach these questions in the paper, using the \"methodology of\nthe artificial\". We build a society of artificial agents, and detail a mechanism that\nshows the formation of a discrete speech code without pre-supposing the existence\nof linguistic capacities or of coordinated interactions. The mechanism is based on\na low-level model of sensory-motor interactions. We show that the integration of\ncertain very simple and non language-specific neural devices leads to the formation\nof a speech code that has properties similar to the human speech code. This result\nrelies on the self-organizing properties of a generic coupling between perception and\nproduction within agents, and on the interactions between agents. The artificial\nsystem helps us to develop better intuitions on how speech might have appeared,\nby showing how self-organization might have helped natural selection to find speech.\nKey words:\norigins of speech sounds, self-organization, evolution, forms, artificial systems,\nagents, phonetics, phonology\n\nEmail address: py@csl.sony.fr (Pierre-Yves Oudeyer).\nURL: www.csl.sony.fr/\u223cpy (Pierre-Yves Oudeyer).\n\n\f1\n\nThe origins of language: a growing field of research\n\nA very long time ago, human vocalizations were inarticulate grunts. Now,\nhumans speak. The question of how they came to speak is one of the most\ndifficult that science has to tackle. After its ban from scientific inquiry during\nmost of the 20th century, because of the Soci\u00e9t\u00e9 Linguistique de Paris declared in its constitution that is was not a scientific question, it is now again\nthe centre of research of a growing scientific community. The diversity of the\nproblems which are implied requires a high pluri-disciplinarity: linguists, anthropologists, neuroscientists, primatologists, psychologists but also physicists\nand computer scientists belong to this community. Indeed, a growing number\nof researchers on the origins of language consider that a number of properties\nof language can only be explained by the dynamics of the complex interactions between the entities which are involved (the interaction between neural\nsystems, the vocal tract, the ear, but also the interactions between individuals\nin a real environment). This is the contribution of the theory of complexity (Nicolis and Prigogine, 1977), developed in the 20th century, which tells\nus that there are many natural systems in which macroscopic properties can\nnot be deduced directly from the microscopic properties. This is what is called\nself-organization. Self-organization is a property of systems composed of many\ninteracting sub-systems, where the patterns and dynamics at the global level\nare qualitatively different from the patterns and dynamics of the sub-systems.\nThis is for example the case of the fascinating structures of termite nests\n(Bonabeau et al., 1999), whose shape is neither coded nor known by the individual termites, but appears in a self-organized manner when termites interact. This type of self-organized dynamics is very difficult to grasp intuitively.\nThe computer happens to be the most suited tool for their exploration and\ntheir understanding (Steels, 1997). It is now an essential tool in the domain\nof human sciences and in particular for the study of the origins of language\n(Cangelosi and Parisi, 2002). One of the objectives of this paper is to illustrate\nhow it can help our understanding to progress.\nWe will not attack the problem of the origins of language in its full generality,\nbut rather we will focus on the question of the origins of one of its essential\ncomponents : speech sounds, the vehicle and physical carriers of language.\n\n2\n\nThe speech code\n\nHuman vocalization systems are complex. Though physically continuous acousticomotor trajectories, vocalizations are cognitively discrete and compositional:\nthey are built with the re-combination of units which are systematically reused. These units are present at several levels (Browman and Goldstein, 1986):\n2\n\n\fgestures, coordination of gestures or phonemes, morphemes. While the articulatory space which defines the space of physically possible gestures is continuous, each language discretizes this space in its own way, and only uses a small\nand finite set of constriction configurations when vocalizations are produced\nas opposed to using configurations which span all the continuous articulatory\nspace: this is what we call is the discreteness of the speech code. While there\nis a great diversity across the repertoires of these units in the world languages,\nthere are also strong regularities (e.g. the frequency of the vowel system /a, e,\ni, o, u/ as shown in (Schwartz et al., 1997)).\nMoreover, speech is a conventional code. Whereas there are strong statistical\nregularities across human languages, each linguistic community possess its\nown way of categorizing sounds. For example, the Japanese do not hear the\ndifference between the [r] of \"read\" and the [l] of \"lead\". How can a code,\nshared by all the members of a community, appear without centralised control?\nIt is true that since the work of de Boer (de Boer, 2001) or Kaplan (Kaplan,\n2001), we know how a new sound or a new word can propagate and be accepted\nin a given population. But this is based on mechanisms of negotiation which\npre-suppose the existence of conventions and of linguistic interactions. These\nmodels are dealing with the cultural evolution of languages, but do not say\nmuch about the origins of language. Indeed, when there were no conventions\nat all, how could the first speech conventions have appeared?\n\n3\n\nHow did the first speech codes appear?\n\nIt is then natural to ask where this organization comes from, and how a shared\nspeech code could have formed in a society of agents who did not already possess conventions. Two types of answers must be provided. The first type is\na functional answer: it establishes the function of sound systems, and shows\nthat human sound systems have an organization which makes them efficient\nfor achieving this function. This has for example been proposed by Lindblom\n(Lindblom, 1992) who showed that statistical regularities of vowel systems\ncould be predicted by searching for the vowel systems with quasi-optimal perceptual distinctiveness. This type of answer is necessary, but not sufficient: it\ndoes not explain how evolution (genetic or cultural) may have found these optimal structures, and how a community may choose a particular solution among\nthe many good ones. In particular, it is possible that \"naive\" Darwinian search\nwith random variations is not efficient enough for finding complex structures\nlike those of speech: the search space is too big (Ball, 2001). This is why a second type of answer is necessary: we have to account for how natural selection\nmay have found these structures. A possible way to do that is to show how\nself-organization can constrain the search space and help natural selection.\nThis may be done by showing how a much simpler system can self-organize\n3\n\n\fFig. 1. The cells in the honey-bees nests (figure on the left) have a perfect hexagonal\nshape. Packed water bubbles take spontaneously this shape under the laws of physics\n(figure on the right). This lead D'Arcy Thompson to think that these same laws of\nphysics might be of great help in the building of their hexagonal wax cells.\n\nspontaneously and form the structure we want to explain.\nThe structure of our argumentation about the origins of speech is the same\nas the one of D'Arcy Thompson (Thompson, 1932) about the explanation of\nhexagonal cells in honey-bees nests (see Figure 1). The cells in the honey-bees\nnests have a perfect hexagonal shape. How did bees came to build such structures? A first element of answer appears if one remarks that the hexagon is\nthe shape which necessitates the minimum amount of wax in order to cover a\nplane with cells of a given surface. So, the hexagon makes the bees spend less\nmetabolic energy, and so they are more efficient for survival and reproduction\nthan if they would build other shapes. One can then propose the classical\nneo-Darwinian explanation : the bees must have begun by constructing random shapes, then with random mutations and selections, more efficient shapes\nwere progressively found, until one day the perfect hexagon was found. Now,\na genome which would lead a bee to build exactly hexagons must be rather\ncomplex and is really a needle in a haystack. And it seems that the classical version of the neo-Darwinian mechanism with random mutations is not\nefficient enough for natural selection to have found such a genome. So the\nexplanation is not sufficient. D'Arcy Thompson completed it. He remarked\nthat when wax cells, with a shape not too twisted, were heated as they actually are by the working bees, then they have approximately the same physical\nproperties as water droplets packed one over the other. And it happens that\nwhen droplets are packed, they spontaneously take the shape of hexagons. So,\nD'Arcy Thompson shows that natural selection did not have to find genomes\nwhich pre-program precisely the construction of hexagons, but only genomes\nwho made bees pack cells whose shape should not be too twisted, and then\nphysics would do the rest 1 . He showed how self-organized mechanisms (even\n1\n\nThis does not mean that nowadays honey bees have not a precise innate hard\n\n4\n\n\fif the term did not exist at the time) could constrain the space of shapes and\nfacilitate the action of natural selection. We will try to show in this paper how\nthis could be the case for the origins of speech sounds.\nSome work in this direction has already been developed in (Browman and Goldstein,\n2000), (de Boer, 2001), and (Oudeyer, 2001b) concerning speech, and in (Steels,\n1997), (Kirby, 2001) or (Kaplan, 2001), concerning lexicons and syntax. These\nworks provide an explanation of how a convention like the speech code can\nbe established and propagated in a society of contemporary human speakers.\nThey show how self-organization helps in the establishment of society-level\nconventions only with local cultural interactions between agents. But all these\nworks deal rather with the cultural evolution of languages than with the origins of language. Indeed, the mechanisms of convention propagation that they\nuse necessitate already the existence of very structured and conventionalised\ninteractions between individuals. They pre-suppose in fact a number of conventions whose complexity is already \"linguistic\".\nLet us illustrate this point with the work of de Boer (de Boer, 2001). He proposed a mechanism for explaining how a society of agents may come to agree\non a vowel system. This mechanism is based on mutual imitations between\nagents and is called the \"imitation game\". He built a simulation in which\nagents were given a model of the vocal tract as well as a model of the ear.\nAgents played a game called the imitation game. Each of them had a repertoire of prototypes, which were associations between a motor program and its\nacoustic image. In a round of the game, one agent called the speaker, chose\nan item of its repertoire, and uttered it to the other agent, called the hearer.\nThen the hearer would search in its repertoire for the closest prototype to the\nspeaker's sound, and produce it (he imitates). Then the speaker categorizes\nthe utterance of the hearer and checks if the closest prototype in its repertoire\nis the one he used to produce its initial sound. He then tells the hearer whether\nit was \"good\" or \"bad\". All the items in the repertoires have scores that are\nused to promote items which lead to successful imitations and prune the other\nones. In case of bad imitations, depending on the scores of the item used by\nthe hearer, either this item is modified so as to match better the sound of\nthe speaker, or a new item is created, as close as possible to the sound of the\nspeaker.\nwired neural structure which allows them to build precisely hexagonal shapes, as has\nbeen suggested in further studies such as those of (von Frisch, 1974). The argument\nof D'Arcy Thompson just says that initially the honey bees might have just relied\non the self-organization of heated packed wax cells, which would have lead them\nto \"find\" the hexagon, but later on in their evolutionary history, they might have\nincorporated in their genome schemata for building directly those hexagons, in a\nprocess similar to the Baldwin effect (Baldwin, 1896), in which cultural evolution\nis replaced here by the self-organization of coupled neural maps.\n\n5\n\n\fThis model is obviously very interesting since it was the first to show a process\nof formation of vowel systems within a population of agents (which was then\nextended to syllables by Oudeyer in (Oudeyer, 2001b)). Yet, one has also to\nremark that the imitation game that agents play is quite complex and requires\na lot of assumptions about the capabilities of agents. From the description\nof the game, it is clear that to perform this kind of imitation game, a lot of\ncomputational/cognitive power is needed. First of all, agents need to be able to\nplay a game, involving successive turn-taking and asymmetric changing roles.\nSecond, they need to have the ability to try to copy the sound production\nof others, and be able to evaluate this copy. Finally, when they are speakers,\nthey need to recognize that they are being imitated intentionally, and give\nfeedback/reinforcement to the hearer about the success or not. The hearer\nhas to be able to understand the feedback, i.e. that from the point of view of\nthe other, he did or did not manage to imitate successfully.\nIt seems that the level of complexity needed to form speech sound systems in\nthis model is characteristic of a society of agents which has already some complex ways of interacting socially, and has already a system of communication\n(which allows them for example to know who is the speaker and who is the\nhearer, and which signal means \"good\" and which signal means \"bad\"). The\nimitation game is itself a system of conventions (the rules of the game), and\nagents communicate while playing it. It requires the transfer of information\nfrom one agent to another, and so requires that this information be carried by\nsome shared \"forms\". So it pre-supposes that there is already a shared system\nof forms. The vowel systems that appear do not really appear \"from scratch\".\nThis does not mean at all that there is a flaw in de Boer's model, but rather\nthat it deals with the cultural evolution of speech rather than with the origins\n(or, in other terms it deals with the formation of languageS - \"les langues\" in\nFrench - rather than with the formation of language - \" le langage\" in French).\nIndeed, de Boer presented interesting results about sound change, provoked by\nstochasticity and learning by successive generations of agents. But the model\ndoes not address the bootstrapping question: how the first shared repertoire of\nforms appeared, in a society with no communication and language-like interaction patterns? In particular, the question of why agents imitate each other\nin the context of de Boer's model (this is programmed in) is open.\nThe \"naming game\" described in (Kaplan, 2001), or the \"iterated learning model\" described in (Kirby, 2001), are based on similar strong assumptions concerning the cognitive capabilities of the agents. (Kaplan, 2001) presupposes the capacity to play a game with rules even more complex than in\nthe \"imitation game\". (Kirby, 2001) pre-supposes complex parsing capabilities\nas well as non-trivial generalization mechanisms which seem to be language\nspecific, even if its simulation does not use an explicit functional pressure for\ncommunication. And, most importantly, both pre-suppose the existence of a\nspeech convention: their agents can transmit and recognize \"labels\" or lists of\n6\n\n\fletters directly (what they learn is what these labels mean for the others in\nthe case of (Kaplan, 2001), or how these streams of letters are syntactically\norganized by the others in the case of (Kirby, 2001)).\nThis shows that existing work relies on agents whose innate cognitive capacities are already very complex and \"quasi-linguistic\", and which possess\nalready a number of conventions. So far, we do not know how these capacities\nand these conventions, especially the speech convention, might have appeared\nif we do not pre-suppose that speech already exists. This is why we need either\nto provide the explanation of their origins, or we need to provide a mechanism\nof the origins of speech which does not necessitate them and relies on much\nsimpler capacities whose origins we can understand without pre-supposing the\nexistence of speech.\nWe are going to present in this paper the second option: we will build an\nartificial system that will put forward the idea that indeed, much simpler\nmechanisms can account for the formation of shared acoustic codes, which\nmay later on be recruited for speech communication. This mechanism relies\nheavily on self-organization, in the same manner as in the explanation of\nthe hexagonal shape of honey bee's cells, where the self-organization due to\nthe physics of packed wax cells does most of the job. Before presenting this\nartificial system, we will briefly describe our methodology.\n\n4\n\nThe method of the artificial\n\nThe \"method of the artificial\" consists in building a society of formal agents\n(Steels, 2001). The scientific logic is abductive. These agents are computer\nprograms implemented in robots which possess for example an artificial vocal\ntract, an artificial ear, and artificial neural networks that connect them. These\ncomponents are inspired by what we know of their human counterpart, but we\ndo not necessarily try to reproduce faithfully what we know of the human brain\nstructures. We then study the dynamics resulting from their interactions, and\nwe try to determine in what conditions they reproduce phenomena analogous\nto those of human speech. This does not aim to show directly what were the\nmechanisms which gave rise to human speech, but the aim is to show what\ntypes of mechanisms are plausible candidates. The building of this artificial\nsystem provides constraints to the space of possible theories, in particular by\nshowing examples of mechanisms which are sufficient, and examples of mechanisms which are not necessary (e.g. we will show that imitation or feedback\nare not necessary to explain the formation of shared discrete speech codes).\nSome criticisms are sometimes put forward about this approach of the origins\nof language through the building of artificial systems. The opposition is often\n7\n\n\fbased on the argument that computer models are based on strong assumptions\nwhich are remote from reality or very difficult to validate or refute. This comes\nfrom a misunderstanding of the methodology and of the aim of the researchers\nwho build these artificial systems. It must be stated clearly that this kind of\ncomputer simulation does not intend to provide directly an explanation about\nthe origins of some aspects of the human language. Rather, they are used\nto organize the thinking and the conceptualisation of the problematic of the\norigins of language, by shaping the search space of possible theories. They are\nused to evaluate the internal coherence of existing theories, and to explore\nnew theoretical ideas. Then of course, these computational models need to be\nextended and selected so as to fit the observations, and become actual scientific\nhypotheses of the origins of language. But because the phenomena involved in\nthe origins of language are complex, we must first develop and conceptualise\nour intuitions about the possible dynamics, before trying to formulate actual\nhypotheses. Building abstract computer simulations is so far the best tool for\nthis purpose.\n\nAnother opposition is the argument that says that too many aspects are modelled at the same time, at the price of modelling each of them over simplistically. This criticism is related to the first one. It should be answered again\nthat this might still be useful because of complexity: some phenomena are\nunderstandable only through the interactions of many components. Yet, most\nresearch projects studying human speech focus on very particular isolated\ncomponents like the study of the electro-mechanical properties of the cochlea,\nthe architecture of the auditory cortex, the acoustics of the vocal tract, the\nsystemic properties of vowels systems, etc. Of course, having detailed knowledge and understanding of each of these components is fundamental. But\nfocusing on each of them individually might prevent us from understanding\nmajor phenomena of speech and language (and might possibly prevent us from\nunderstanding some of the aspects of each module). It is necessary to study\ntheir interactions in a parallel track. Because, it is practically impossible to\nincorporate all the knowledge that we have of each component in a simulation, and because for most of them there exist no real agreement on how they\nwork, we can only use simplistic models so far. Besides the fact that simulations incorporating the interactions of many components can provide insights\non the phenomena of speech, it is quite possible that using these simplistic\nmodels might also shed light on the functioning of some of the components by\nopening new conceptual dimensions and new experiments in vivo. In return,\nthe simplistic models will then be made more realistic, which will then help\nthe understanding of components, forming a virtuous circle.\n8\n\n\f5\n\nThe artificial system\n\nThe system is a generalization of the one we described in (Oudeyer, 2001a),\nwhich was used to model the phenomenon called the \"perceptual magnet effect\". It is based on the building of an artificial system, composed of agents 2\nendowed with working models of the vocal tract, of the cochlea and of some\nparts of the brain. The complexity and degree of reality of these models can\nbe varied to investigate which aspects of the results are due to which aspects\nof the model.\nAs explained in (Oudeyer, 2001a), this system contains neural maps which are\nsimilar to those used in (Guenther and Gjaja, 1996) and (Damper and Harnad,\n2000). What is different is that on the one hand, motor and perceptual neural maps are coupled so that the learning of sounds affects the production\nof sounds, and on the other hand, these two other works used single agents\nthat learnt an existing sound system, while here we use several agents that\nco-create a sound system.\n\n5.1 Overview\nEach agent has one ear which takes measures of the vocalizations that it\nperceives, which are then sent to its brain. It also has a vocal tract, whose\nshape is controllable and is used to produce sounds. Typically, the vocal tract\nand the ear define three spaces: the motor space (which will be for example\n3-dimensional in the vowel simulations with tongue body position, tongue\nheight and lip rounding); the acoustic space (which will be 4-dimensional in\nthe vowel simulation with the first four formants) and the perceptual space\n(which corresponds to the information the ear sends to the brain, and will be\n2-dimensional in the vowel simulations with the first formant and the second\neffective formant).\nThe ear and the vocal tract are connected to the brain, which is basically a\nset of interconnected artificial neurons (the use of artificial neurons in computational models of the human brain is described for example in (Anderson,\n1995)). This set of artificial neurons is organized into two neural topological maps: one perceptual map and one motor map. Topological neural maps\nhave been widely used for many models of cortical maps ((Kohonen, 1982),\n(Morasso et al., 1998)), which are the neural devices that humans have to represent parts of the outside world (acoustic, visual, touch etc.). Figure 2 gives\n2\n\nThe term 'agent' is used in artificial intelligence as an abbreviation of 'artificial\nsoftware agent', and denotes a software entity which is functionally equivalent to a\nrobot (this is like a virtual robot in the virtual environment of the computer)\n\n9\n\n\fFig. 2. Architecture of the artificial system : agents are given an artificial ear,\nan artificial vocal tract, and an artificial \"brain\" which couples these two organs.\nAgents are themselves coupled through their common environment : they perceive\nthe vocalizations of their neighbours.\n\nan overview of the architecture. We will now describe the technical details of\nthe architecture.\n\n5.2 Motor neurons, vocal tract and production of vocalizations\nStructure. A motor neuron j is characterized by a preferred vector vj which\ndetermines the vocal tract configuration which is to be reached when it is\nactivated and when the agent sends a GO signal to the motor neural map.\nThis GO signal is sent at random times by the agent to the motor neural\nmap. As a consequence, the agent produces vocalizations at random times,\nindependently of any events.\nWhen an agent produces a vocalization, the neurons which are activated are\nchosen randomly. Typically, 2, 3 or 4 neurons are chosen and activated in sequence. Each activation of a neuron specifies, through its preferred vector, a\nvocal tract configuration objective that a sub-system takes care of reaching by\nmoving continuously the articulators. In this paper, this sub-system is simply\na linear interpolator, which produces 10 intermediate configurations between\neach articulatory objective, which is an approximation of a dynamic continuous vocalization and that we denote ar1 , ar2 , ..., arN . We did not use realistic\nmechanisms like the propagation techniques of population codes proposed in\n(Morasso et al., 1998), because these would have been rather computationally inefficient for this kind of experiment. Figure 3 illustrates this process\nin the case of the abstract 2-dimensional articulatory space that we will now\n10\n\n\fFig. 3. When an agent produces a vocalization, several motor neurons are activated\nin sequence. Each of them corresponds to an articulatory configuration which has\nto be reached from the current configuration. A sub-control system takes care of\ninterpolating between the different configurations.\n\ndescribe.\nIndeed, the articulatory configurations will be coded in an abstract space in\na first set of simulations, and coded in a realistic space in a second more\nrealistic set of simulations. Also, in each case, we use an artificial vocal tract\nto compute an acoustic image of the dynamic articulation.\nIn the abstract simulations, the articulatory configurations ari = (d1i , d2i )\nare just points in [0, 1]2 . The vocal tract is here a random linear mapping: in\norder to compute the acoustic image of an articulatory trajectory defined by\nthe sequence of articulations ar1 , ar2 , ..., arN , we compute the trajectory of the\nacoustic images of each articulation in the acoustic space with the formula:\naci = (r1 .d1i + r2 .d2i )/2\nwhere aci is the acoustic image of ari and r1 as well as r2 are fixed random\nnumbers.\nIn the more realistic simulations, we use a vocal tract model of vowel production designed by (de Boer, 2001). We use vowel production only because\nthere exists this computationally efficient and rather accurate model, but one\ncould do simulations with a vocal tract model which models consonants if\nefficient ones were available. The three major vowel articulatory parameters\n(Ladefoged and Maddieson, 1996) are used: lip rounding, tongue height and\ntongue position. The values within these dimensions are between 0 and 1, and\na triplet of values ari = (r, h, p) defines an articulatory configuration. The\nacoustic image of one articulatory configuration is a point in the 4-dimensional\nspace defined by the first four formants, which are the frequencies of the peaks\n11\n\n\fin the frequency spectrum, and is computed with the formula :\nF1 = ((\u2212392 + 392r)h2 + (596 \u2212 668r)h + (\u2212146 + 166r))p2 + ((348 \u2212 348r)h2\n+ (\u2212494 + 606r)h + (141 \u2212 175r))p + ((340 \u2212 72r)h2 + (\u2212796 + 108r)h\n+ (708 \u2212 38r))\nF2 = ((\u22121200+1208r)h2 +(1320\u22121328r)h+(118\u2212158r))p2+((1864\u22121488r)h2\n+ (\u22122644 + 1510r)h + (\u2212561 + 221r))p + ((\u2212670 + 490r)h2 + (1355 \u2212 697r)h\n+ (1517 \u2212 117r))\nF3 = ((604 \u2212604r)h2 + (1038 \u22121178r)h+ (246 + 566r))p2 + ((\u22121150 + 1262r)h2\n+ (\u22121443 + 1313r)h + (\u2212317 \u2212 483r))p + ((1130 \u2212 836r)h2 + (\u2212315 + 44r)h\n+ (2427 \u2212 127r))\nF4 = ((\u22121120 + 16r)h2 + (1696 \u2212 180r)h + (500 + 522r))p2 + ((\u2212140 + 240r)h2\n+ (\u2212578 + 214r)h + (\u2212692 \u2212 419r))p + ((1480 \u2212 602r)h2 + (\u22121220 + 289r)h\n+ (3678 \u2212 178r))\nThese were derived from polynomial interpolation based on a database of\nreal vowels presented in (Vallee, 1994). Details are given in (de Boer, 2001).\n\nPlasticity. The preferred vector of each neuron in the motor map is updated\neach time the motor neurons are activated (which happens both when the\nagent produces a vocalization and when it hears a vocalization produced by\nanother agent, as we will explain below). This update is made in two steps :\n1) one computes which neuron m is most activated and takes the value vm of\nits preferred vector ; 2) the preferred vectors of all neurons are modified with\nthe formula:\n\nvj,t+1 = vj,t + 0.001.Gj,t(s).(v \u2212 vj,t )\n\nwhere Gj,t (s) is the activation of neuron j at time t with the stimulus s (as we\nwill detail later on) and vj,t denotes the value of vj at time t. This law of adaptation of the preferred vectors has the consequence that the more a particular\nneuron is activated, the more the agent will produce articulations which are\nsimilar to the one coded by this neuron. This is because geometrically, when\nvm is the preferred vector of the most active neuron, the preferred vectors of\nthe neurons which are also highly activated are shifted a little bit towards vm .\nThe initial value of all the preferred vectors of the motor neurons is random\nand uniformly distributed. There are in this paper 500 neurons in the motor\nneural map (above a certain number of neurons, which is about 150 in all the\ncases presented in the paper, nothing changes if this number varies).\n12\n\n\f5.3 Ear, perception of vocalizations and perceptual neurons\n\nWe describe here the perceptual system of the agents, which is used when they\nperceive a vocalization. As explained in the previous paragraphs, this perceived\nvocalization takes the form of an acoustic trajectory, i.e. a sequence of points\nwhich approximate the continuous sounds. In the abstract simulations, these\npoints are in the abstract 2-D space which we described above. In this case,\nthe acoustic space and the perceptual space are equal. In the more realistic\nsimulations, these points are in the 4-D space whose dimensions are the first\nfour formants of the acoustic signal. In this case, we use also a model of our\near which transforms this 4-D acoustic representation in a 2-D perceptual\nrepresentation that we know is close to the way humans represent vowels.\nThis model was used in (Boe et al., 1995) and (de Boer, 2001). It is based\non the observations by (Carlson et al., 1970) who showed that the human ear\nis not able to distinguish the frequency peaks with narrow bands in the high\nfrequencies. The first dimension is the first formant, and the second dimension\nis the second effective formant:\n\n\u2032\n\nF2 =\n\n\uf8f1\n\uf8f4\n\uf8f4\nF2 , if F3 \u2212 F2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4 (2\u2212w1 )F2 +w1 F3\n\uf8f2\n,\n2\n\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\n>c\nif F3 \u2212 F2 \u2264 c and F4 \u2212 F2 \u2265 c\n\nw2 F2 +(2\u2212w2 )F3\n2\n\n\u2212 1, if F4 \u2212 F2 \u2264 c and F3 \u2212 F2 \u2264 F4 \u2212 F3\n\n(2+w2 )F3 \u2212w2 F4\n2\n\n\u2212 1, if F4 \u2212 F2 \u2264 c and F3 \u2212 F2 \u2265 F4 \u2212 F3\n\nwith\nw1 =\n\nc \u2212 (F3 \u2212 F 2)\nc\n\n(F4 \u2212 F3 ) \u2212 (F3 \u2212 F2 )\nF4 \u2212 F2\nwhere c is a constant of value 3.5 Barks.\nw2 =\n\nIn both cases (abstract and realistic simulations), the agent gets as input to\nits perceptual neural system a trajectory of perceptual points. Each of these\nperceptual points is then presented in sequence to its perceptual neural map\n(this models a discretization of the acoustic signal by the ear due to its limited\ntime resolution).\nThe neurons i in the perceptual map have a gaussian tuning function which\nallows us to compute the activation of the neurons upon the reception of an\ninput stimulus. If we denote by Gi,t the tuning function of neuron i at time t,\ns is a stimulus vector, then the form of the function is:\nGi,t (s) =\n\n1\n2\n2\n\u221a 1 e\u2212 2 (vi,t .s) /\u03c3\n2\u03c0\u03c3\n\n13\n\n\fwhere the notation v1 .v2 denotes the scalar product between vector v1 and\nvector v2 , and vi,t defines the center of the gaussian at time t and is called the\npreferred vector of the neuron. This means that when a perceptual stimulus\nis sent to a neuron i, then this neuron will be activated maximally if the\nstimulus has the same value as vi,t . The parameter \u03c3 determines the width\nof the gaussian, and so if it is large the neurons are broadly tuned (a value\nof 0.05, which is used in all simulations here, means that a neuron responds\nsubstantially to 10 percent of the input space).\nWhen a neuron in the perceptual map is activated because of a stimulus, then\nits preferred vector is changed. The mathematical formula of the new tuning\nfunction is:\nGi,t+1 (s) =\n\n1\n2\n2\n\u221a 1 e\u2212 2 (vi,t+1 .s) /\u03c3\n2\u03c0\u03c3\n\nwhere s is the input, and vi,t+1 the preferred vector of neuron i after the\nprocessing of s:\nvi,t+1 = vi,t + 0.001.Gi,t(s).(s \u2212 vi,t )\nThis formula makes that the distribution of preferred vectors evolves so as to\napproximate the distribution of sounds which are heard.\nThe initial value of the preferred vectors of all perceptual neurons follows a\nrandom and uniform distribution. There are 500 neurons in the perceptual\nmap in the simulations presented in this paper.\n\n5.4 Connections between the perceptual map and the motor map\n\nEach neuron i in the perceptual map is connected unidirectionally to all the\nneurons j in the motor map. The connection between the perceptual neuron\ni and the motor neuron j is characterized by a weight wi,j , which is used to\ncompute the activation of neuron j when a stimulus s has been presented to\nthe perceptual map, with the formula :\nGj,t (s) =\n\n\u221a1\n2\u03c0\u03c3\n\n\u2217 e\u2212\n\nP\n\ni\n\nwi,j Gi,t (s)/\u03c32\n\nThe weights wi,j are initially set to a small random value, and evolve so as\nto represent the correlation of activity between neurons. This is how agents\nwill learn the perceptual/articulatory mapping. The learning rule is hebbian\n(Sejnowsky, 1977):\n\u03b4wi,j = c2 (Gi \u2212 < Gi >)(Gj \u2212 < Gj >)\n14\n\n\fwhere Gi denotes the activation of neuron i and < acti > the mean activation\nof neuron i over a certain time interval (correlation rule). c2 denotes a small\nconstant. This learning rule applies only when the motor neural map is already\nactivated before the activations of the perceptual map have been propagated,\ni.e. when an agent hears a vocalization produced by itself. This amounts to\nlearning the perceptual/motor mapping through vocal babbling.\nNote that this means that the motor neurons can be activated either through\nthe activation of the perceptual neurons when a vocalization is perceived, or\nby direct activation when the agent produces a vocalization (in this case, the\nactivation of the chosen neuron is set to 1, and the activation of the other neurons is set to 0). Because the connections are unidirectional, the propagation\nof activations only takes place from the perceptual to the articulatory map\n(this does not mean that a propagation in the other direction would change\nthe dynamics of the system, but we did not study this variant).\nThis coupling between the motor map and the perceptual map has an important dynamical consequence: the agents will tend to produce more vocalizations composed of sounds that they have already heard. Said another way,\nwhen a vocalization is perceived by an agent, this increases the probability\nthat the sounds that compose this vocalization will be re-used by the agent\nin its future vocalizations. It is interesting to note that this phenomenon of\nphonological attunement is observed in very young babies (Vihman, 1996).\n\n5.5 Recurrence of the perceptual map and of the motor map\n\nHere we present an addition to the architecture presented in the previous\nparagraphs which is not crucial for the system 3 but which allows us both to\nmodel the additional feature of categorization and to visualize the dynamics\nof the rest of the system.\nThis addition is based on the concept of population vector developed by\n(Georgopoulos et al., 1988), and used in a similar setup by (Guenther and Gjaja,\n1996). It proposes that the stimuli which are stored in the neural maps through\nthe distributed activations of neurons, can be decoded or re-constructed by\nsome other parts of the brain by computing the sum of all preferred vectors of\nthe neurons weighted by their activity and normalized. Technically, the population vector corresponding to the pattern of activations of the neurons i of\na neural map when they have been activated by the stimulus s is:\n3\n\nThis means that we do not need this addition for the main results of the system:\n1) the formation of shared discrete speech codes; 2) the formation of statistical\nregularities similar to those of humans in the formed phonemic repertoires.\n\n15\n\n\fP\n\nG (s)\u2217v\npop(s) = Pi Gi i (s) i\ni\n\nIn general, pop(s) is not exactly the same point as s, because this decoding\nscheme is imprecise. But this imprecision can be exploited usefully. Indeed,\nnow we add a re-entrance of this re-constructed stimulus pop(s): it is fed\nback as an input to the neural map. And this gives rise to a new pattern of\nactivations, which is re-decoded, and the result is again fed back as input, and\nthis is iterated until a fixed point is reached. Indeed, this recurrent system has\nproperties which can be shown to be equivalent to Hopfield neural networks\n(Hopfield, 1982) and is very similar to the system of (Morasso et al., 1998):\nwhatever the initial pattern of activations due to the perception of a stimulus,\nthe cycle coding-decoding always converges on a fixed point (fixed pattern\nof activation). This process models a categorizing behaviour, and the fixed\npoint is the category which has been recognized by the system. Note that\nthis process is applied to each neural map only after all activations have been\npropagated and after the learning rules have been applied (so this extension\ndoes not modify the dynamics induced by the mechanisms presented in the\nprevious paragraphs).\nA nice property is that the fixed pattern of activation when the system has\nconverged represents a particular stimulus which is basically the prototype of\na category. Indeed, if the preferred vectors of a neural map self-organize in\nclusters as will happen in the simulations, each cluster, coding for a discrete\n\"mode\" or phoneme, will have its center which coincides with the fixed point\nwhich is reached when a stimulus close to this cluster is perceived. This center and associated fixed point also represent the point of maximal density of\nneurons in the vicinity of the cluster. More generally, the properties of this\nrecurrent system make that its dynamics is easy to represent. Indeed, if we use\na stimulus space with 2 dimensions, then each cycle coding/decoding can also\nbe represented in 2-D : the input point is represented by the beginning of an\narrow, and the re-constructed point is represented by the end of an arrow. By\nplotting all the arrows corresponding to the first iteration of this process for all\nthe points on a regular grid, we can have a general view of the different basins\nof attractions and their fixed points, which are at the same time the zones of\nmaximal density of the clusters. We will use this kind of plot to visualize the\nresults of all our simulations, as explained below.\n\n5.6 Coupling of agents\nThe agents are put in a world where they move randomly. At random times,\na randomly chosen agent sends a GO signal and produces a vocalization. The\nagents which are close to it can perceive this vocalization. Here, we fix the\nnumber of agents who can hear the vocalization of another to 1 (we pick the\n16\n\n\fclosest one). This is a non-crucial parameter of the simulations, since basically\nnothing changes when we tune this parameter, except the speed of convergence\nof the system (and this speed is lowest when the parameter is 1). Technically,\nthis amounts to having a list of agents, and in sequence picking up randomly\ntwo of them, have one produce a vocalization, and the other hear it. Typically,\nthere are 20 agents in the system. This is also a non-crucial parameter of the\nsimulation : nothing changes except the speed of convergence.\n\n5.7 What the system does not assume\n\nIt is crucial to note that as opposed to most of simulations on the origins of\nlanguage that exist in the literature (Cangelosi and Parisi, 2002)), our agents\ndo not play here a \"language game\", in the sense that there is no need to\nsuppose an extra-linguistic protocol of interaction such as who should give\nfeedback to whom and at what particular moment and for what particular\npurpose. In particular agents do not play the \"imitation game\" which is for\nexample used in (de Boer, 2001). Indeed, it is crucial to note that agents\nDO NOT imitate each other in the simulations we present. Indeed, imitation\ninvolves at least the reproduction of another's vocalization now or later: here,\none agent which hears another one never produces a vocalization in response,\nand does not store the heard vocalization so as to reproduce it later. The only\nconsequence of hearing a vocalization is that it increases the probability, for\nthe agent which hears it, of producing later on vocalizations whose parts are\nsimilar to those of the heard vocalization. Of course it might happen, specially\nwhen the system has converged on a few modes, that an agent produces a\nvocalization that it has already heard, but this is no more an imitation than a\nhuman producing the same vowels as another when responding to a question\nfor example. The interactions of agents are not structured, there are no roles\nand no coordination. In fact, they have no social skill at all. They do not\ndistinguish between their own vocalizations and those of others. They do not\ncommunicate. Here, \"communication\" refers to the emission of a signal by an\nindividual with the aim of modifying the state of at least one other agent,\nwhich does not happen here. Indeed, agents do not even have means to detect\nor represent other agents around them, so it would be difficult to say that\nthey communicate. Finally, not only there are no social force which act as a\npressure to distinguish sounds, but there are no internal force which act as a\npressure to have a repertoire of different discrete sounds : indeed, there are\nno repulsive forces in the dynamics which update the preferred vectors of the\nneural maps.\n17\n\n\f6\n\nDynamics\n\nWe will study the dynamics of the artificial system in two different cases: the\nfirst one is when the abstract linear articulatory synthesizer is used, while the\nsecond one is when the realistic articulatory synthesizer is used.\n\n6.1 Using the abstract linear articulatory/perceptual mapping\n\nThe present experiment used a population of 20 agents. Let us describe first\nwhat we obtain when agents use the abstract linear articulatory synthesizer.\nInitially, as the preferred vectors of neurons are randomly and uniformly distributed across the space, the different targets that compose the vocalizations\nof agents are also randomly and uniformly distributed. Figure 4 shows the\npreferred vectors of the neurons of the perceptual map of two agents. We see\nthat they cover the whole space uniformly. They are not organized. Figure\n5 shows the basins of attraction associated with the coding/decoding recurrent process that we described earlier. The beginning of an arrow represents\na pattern of activations at time t generated by presenting a stimulus whose\ncoordinates correspond to the coordinates of this point. The end of the arrow\nrepresents the pattern of activations of the neural map after one iteration of\nthe process. The set of all arrows provides a visualization of several iterations:\nstart somewhere on the figure, and follow the arrows. At some point, for every\ninitial point, you get to a fixed point. This corresponds to one attractor of\nthe network dynamic, and the fixed point to the category of the stimulus that\ngave rise to the initial activation. The zones defining stimuli which fall in the\nsame category are visible on the figure, and are called basins of attractions.\nWith initial preferred vectors uniformly spread across the space, the number of\nattractors as well as the boundaries of their basins of attractions are random.\nThe learning rule of the acoustic map is such that it evolves so as to approximate the distribution of sounds in the environment (but remember this is not\ndue to imitation). All agents produce initially complex sounds composed of\nuniformly distributed targets. Hence, this situation is in equilibrium. Yet, this\nequilibrium is unstable, and fluctuations ensure that at some point, the symmetry of the distributions of the produced sounds breaks: from time to time,\nsome sounds get produced a little more often than others, and these random\nfluctuations may be amplified through positive feedback loops. This leads to a\nmulti-peaked distribution: agents get in a situation like that of Figure 6 which\ncorresponds to Figure 4 after 2000 interactions in a population of 20 agents.\nFigure 6 shows that the distribution of preferred vectors is no longer uniform\nbut clustered (the same phenomenon happens in the motor maps of the agents,\n18\n\n\fFig. 4. Perceptual neural maps of two agents at the beginning (the two agents are\nchosen randomly among a set of 20 agents). Units are arbitrary. Each of both square\nrepresents the perceptual map of one agent.\n\nFig. 5. Representation of the same two agent's attractor field initially.\n\nso we represent here only the perceptual maps, as in the rest of the paper).\nYet, it is not so easy to visualize the clusters with the representation in Figure\n6, since there are a few neurons which have preferred vectors not belonging to\nthese clusters. They are not statistically significant, but introduce noise into\nthe representation. Furthermore, in the clusters, basically all points have the\nsame value so that they appear as one point. Figure 7 shows better the clusters using the attractor landscape that is associated with them. We see that\nthere are now three well-defined attractors or categories, and that there are\nthe same in the two agents represented (they are also the same in the 18 other\nagents in the simulation). This means that the targets the agents use now\nbelong to one of several well-defined clusters, and moreover can be classified\nautomatically as such by the recurrent coding/decoding process of the neural\nmap. The continuum of possible targets has been broken, sound production\n19\n\n\fFig. 6. Neural maps after 2000 interactions, corresponding to the initial state of\nfigure 4 The number of points that one can see is fewer than the number of neurons,\nsince clusters of neurons have the same preferred vectors and this is represented by\nonly one point.\n\nFig. 7. Representation of the attractor fields of 2 agents after 2000 interactions.\nThe number of attractors is fewer that the number of points in the last figure. This\nis because in the previous figure, some points corresponded to clusters and other to\nsingle points. The broad width of the tuning function makes that the landscape is\nsmoothed and individual point which are not too far from clusters do not manage\nto form their own basin of attraction.\n\nis now discrete. Moreover, the number of clusters that appear is low, which\nautomatically brings it about that targets are systematically re-used to build\nthe complex sounds that agents produce: their vocalizations are now compositional. All the agents share the same speech code in any one simulation. Yet,\nin each simulation, the exact set of modes at the end is different. The number\nof modes also varies with exactly the same set of parameters. This is due to\nthe inherent stochasticity of the process. We will illustrate this later in the\npaper.\n20\n\n\fIt is very important to note that this result of crystallization holds for any\nnumber of agents (experimentally), and in particular with only one agent\nwhich adapts to its own vocalizations. This means that the interaction with\nother agents (i.e. the social component) is not necessary for discreteness and\ncompositionality to arise. But what is interesting is that when agents do interact, then they crystallize in the same state, with the same categories. To\nsummarize, there are so far two results in fact: on the one hand discreteness\nand compositionality arise thanks to the coupling between perception and\nproduction within agents, on the other hand shared systems of phonemic categories arise thanks to the coupling between perception and production across\nagents.\nWe also observe that the attractors that appear are relatively well spread\nacross the space. The prototypes that their centres define are thus perceptually\nquite distinct. In terms of Lindblom's framework, the energy of these systems\nis high. Yet, there was no functional pressure to avoid close prototypes. They\nare distributed in that way thanks to the intrinsic dynamics of the recurrent\nnetworks and their rather large tuning functions: indeed, if two neuron clusters\njust get too close, then the summation of tuning functions in the iterative\nprocess of coding/decoding smoothes their distribution locally and only one\nattractor appears.\nA last point to make is that what we call \"crystallization\" here is not exactly a\nmathematical convergence, but a practical convergence of the system. Indeed,\nas we explained in the previous sections, there are only attractive forces that\nact on the preferred vectors of neurons. No repulsive force is present. As a\nconsequence, as these forces are always strictly positive because of the gaussian\ntuning function, the point of mathematical convergence of the system is when\nall preferred vectors are clustered in one single point. Yet, this mathematical\nconvergence can not be reached in practice. Indeed, because we use a gaussian\ntuning function, this attractive force becomes exponentially low as stimuli get\nfurther from a given preferred vector. This has the consequence that there\nis a first phase in the system during which a number of clusters form, and\nsometimes \"melt\", until a state is reached in which the attraction between\nclusters is so small that no new melting of clusters happens before billions of\ntime steps : in practice it is impossible to wait this amount of time, which\nis much longer than the lifetime of agents. This evolution can be illustrated\nby plotting the evolution of the entropy of the distribution of the preferred\nvectors of all agents, as on Figure 8. We see a first phase of sharp decrease in\nthe entropy, and then a plateau. We use the term crystallization and stop the\nsimulations when this entropy plateau has been reached (i.e. when the entropy\nvalue does not change for several thousands time steps).\nFinally, it has to be noted that a crucial parameter of the simulation is the\nparameter \u03c3 which defines the width of the tuning functions. All the results\n21\n\n\fFig. 8. Evolution of the entropy of the distributions of the preferred vectors of the\nacoustic neurons of all agents.\n\npresented are with a value 0.05. In (Oudeyer, 2003), we present a study of\nwhat happens when we tune this parameter. This study shows that the simulation is quite robust to this parameter: indeed, there is a large zone of values\nin which we get a practical convergence of the system in a state where agents\nhave a multi-peaked preferred vector distribution, as in the examples we presented. What changes is the mean number of these peaks in the distributions:\nfor example, with \u03c3 = 0.05, we obtain between 3 and 10 clusters, and with\n\u03c3 = 0.01, we obtain between 6 and 15 clusters. If \u03c3 becomes too small, then\nthe initial equilibrium of the system becomes stable and nothing changes:\nagents keep producing inarticulate and holistic vocalizations. If \u03c3 is too large,\nthen the practical convergence of the system is the same as the mathematical\nconvergence: only one cluster appears.\n\n6.2 Using the realistic articulatory/acoustic mapping\nIn the previous paragraph, we supposed that the mapping from articulations\nto perceptions was linear. In other words, constraints from the vocal apparatus due to non-linearities were not taken into account. This was interesting\nbecause it showed that no initial asymmetry in the system was necessary to\nget discreteness (which is very asymmetrical). In other words, this shows that\nthere is no need to have sharp natural discontinuities in the mapping from the\narticulations to the acoustic signals and to the perceptions in order to explain\nthe existence of discreteness in speech sounds (we are not saying that the\nnon-linearities of the mapping do not help, just that they are not necessary).\nYet, this mapping has a particular shape which introduces a bias into the\npattern of speech sounds. Indeed, with the human vocal tract, there are ar22\n\n\fticulatory configurations for which a small change gives a small change in the\nproduced sound, but there are also articulatory configurations for which a\nsmall change gives a large change in the produced sound. While the neurons\nin the neural maps have initially random preferred vectors with a uniform\ndistribution, this distribution will soon become biased: the consequence of\nnon-linearities will be that the learning rule will have different consequences\nin different parts of the space. For some stimuli for which there are many articulatory configurations which produce similar sounds, a lot of motor neurons\nwill have their preferred vectors shifted a lot, and for other stimuli, very few\nneurons will have their preferred vectors shifted. This will very quickly lead\nto non-uniformities in the distribution of preferred vectors in the motor map,\nwith more neurons in the parts of the space for which small changes give small\ndifferences in the produced sounds, and with fewer neurons in the parts of the\nspace for which small changes give large differences in the produced sounds.\nAs a consequence, the distribution of the targets that compose vocalizations\nwill be biased, and the learning of the neurons in the perceptual maps will\nensure that the distributions of the preferred vectors of these neurons will also\nbe biased.\nWe are going to study the consequence of using such a realistic vocal tract\nand cochlear model in the system. We use the models described earlier. To get\nan idea of the bias imposed by this mapping, Figure 9 shows the state of the\nacoustic neural maps of one agent after a few interactions (200) between the\nagents.\n\nFig. 9. Neural map and attractor field of one agent within a population of\n20 agents, after 200 interactions. Here the realistic articulatory synthesizer is\nused. The triangle which appears correspond to the so-called \"vocalic triangle\"\n(Ladefoged and Maddieson, 1996).\n\nA series of 500 simulations was run with the same set of parameters, and\neach time the number of vowels as well as the structure of the system was\nchecked. Each vowel system was classified according to the relative position of\n23\n\n\fFig. 10. Neural map and attractor field of the agent of figure 9 after 2000 interactions\nwith other 20 agents. The corresponding figures of other agents are nearly identical.\nThe produced vowel system is here an instantiation the most frequent vowel system\nin human languages: /a, e, i, o, u/.\n\nthe vowels, as opposed to looking at the precise location of each of them. This\nis inspired by the work of Crothers (Crothers, 1978) on universals in vowel systems, and is identical to the type of classification performed in (de Boer, 2001).\nThe first result shows that the distribution of vowel inventory sizes is very similar to that of human vowel systems (Ladefoged and Maddieson, 1996): Figure\n11 shows the 2 distributions (in plain line the distribution corresponding to\nthe emergent systems of the experiment, in dotted line the distribution in human languages), and in particular the fact that there is a peak at 5 vowels,\nwhich is remarkable since 5 is neither the maximum nor the minimum number\nof vowels found in human languages. The prediction made by the model is\neven more accurate than the one provided by de Boer (de Boer, 2001) since\nhis model predicted a peak at 4 vowels. Then the structure of the emergent\nvowel systems was compared to the structure of vowel systems in human languages as reported in (Schwartz et al., 1997). More precisely, the distributions\nof structures in the 500 emergent systems were compared to the distribution\nof structures in the 451 languages of the UPSID database (Maddieson, 1984).\nThe results are shown in Figure 12. We see that the predictions are rather\naccurate, especially in the prediction of the most frequent system for each\nsize of vowel system (less than 8). Figure 10 shows an instance of the most\nfrequent system in both emergent and human vowel systems. In spite of the\npredictions of one 4-vowel system and one 5-vowel system which appear frequently (9.1 and 6 percent of systems) in the simulations and never appear\nin UPSID languages, these results compare favourably to those obtained in\n(de Boer, 2001). In particular, we obtain all this diversity of systems with the\nappropriate distributions with the same parameters, whereas de Boer had to\nmodify the level of noise to increase the sizes of vowel systems. Yet, like de\nBoer, we are not able to predict systems with many vowels (which are admittedly rare in human languages, but do exist). This is certainly a limit of our\n24\n\n\fFig. 11. Distribution of vowel inventories sizes in emergent and UPSID human\nvowel systems\n\nFig. 12. Distribution of vowel inventories structures in artificial and UPSID\nhuman vowel systems. This diagram uses the same notations than the one in\n(Schwartz et al., 1997). Note that here, the vertical axis is also F2, but oriented\ndownwards.\n\nmodel. Functional pressure to develop efficient communication systems might\nbe necessary here.\n25\n\n\f7\n\nConclusion\n\nThis paper has presented a mechanism which provides a possible explanation\nof how a speech code may form in a society of agents which do not already possess means to communicate and coordinate in a language-like manner (as opposed to the agents described in (de Boer, 2001), (Kaplan, 2001) or (Oudeyer,\n2001b)), and which do not already possess a convention and complex cognitive skills for linguistic processing (as opposed to the agents in (Kirby, 2001)\nfor example). The agents in this paper have in fact no social skills at all. We\nbelieve that the value of the mechanism we presented resides in its quality of\nexample of the kind of mechanism that might solve the language bootstrapping problem. We show how one crucial pre-requisite, i.e. the existence of an\norganized medium which can carry information in a conventional code shared\nby a population, may appear without linguistic features being already there.\nThe self-organized mechanism of this system appears as a necessary complement to the classical neo-Darwinian account of the origins of speech sounds.\nIt is compatible with the classical neo-Darwinian scenario in which the environment favours the replication of individuals capable of speech. In this\nscenario, our artificial system plays the same role as the laws of the physics\nof droplets in the explanation of the hexagonal shape of wax cells: it shows\nhow self-organized mechanisms can facilitate the work of natural selection by\nconstraining the shape space. Indeed, we show that natural selection did not\nnecessarily have to find genomes which pre-programmed the brain in precise\nand specific ways so as to be able to create and learn discrete speech systems.\nThe capacity of coordinated social interactions and the behaviour of imitation\nare also examples of mechanisms which are not necessarily pre-required for\nthe creation of the first discrete speech systems, as our system demonstrates.\nThis draws the contours of a convincing classical neo-Darwinian scenario, by\nfilling the conceptual gaps that made it stay an idea rather than a real working\nmechanism.\nFurthermore, this same mechanism accounts for properties of the speech code\nlike discreteness, compositionality, universal tendencies, sharing and diversity.\nWe believe that this account is original because: 1) only one mechanism is\nused to account for all these properties and 2) we need neither a pressure for\nefficient communication nor innate neural devices specific to speech (the same\nneural devices used in the paper can be used to learn hand-eye coordination\nfor example). In particular, having made simulations both with and without\nnon-linearities in the articulatory/perceptual mapping allows us to say that\nin principle, whereas the particular phonemes which appear in human languages are under the influence of the properties of this mapping, their mere\nexistence, which means the phenomenon of phonemic coding, does not require\nnon-linearities in this mapping but can be due to the sensory-motor coupling\n26\n\n\fdynamics. This contrasts with the existing views that the existence of phonemic coding necessarily need either non-linearities, as defended by (Stevens,\n1972) or (Mrayati et al., 1988), or an explicit functional pressure for efficient\ncommunication, as defended by (Lindblom, 1992).\nModels like the one of de Boer (de Boer, 2001) are to be seen as describing\nphenomena occurring later in the evolutionary history of language. More precisely, de Boer's model, as well as for example the one presented in (Oudeyer,\n2001b) for the formation of syllable systems, deals with the recruitment of\nspeech codes like those that appear in this paper, and studies how they are\nfurther shaped and developed under functional pressure for communication.\nIndeed, if we have here shown that one can already go a long way without\nsuch pressure, some properties of speech can only be accounted for with it.\nAn example is the existence of large vowel inventories (Schwartz et al., 1997).\n\n8\n\nAcknowledgements\n\nI would like to thank Michael Studdert-Kennedy, Bart de Boer, Jim Hurford\nand Louis Goldstein for their motivating feedback and their useful help in the\nwriting of this paper. I would also like to thank Luc Steels for supporting the\nresearch presented in the paper.\n\nReferences\nAnderson, J., 1995. An Introduction to Neural Networks. Cambridge, MA:\nMIT Press.\nBaldwin, J., 1896. A new factor in evolution. American Naturalist 30, 441\u2013451.\nBall, P., 2001. The self-made tapestry, Pattern formation in nature. Oxford\nUniversity Press.\nBoe, L., Schwartz, J., Valle, N., 1995. The prediction of vowel systems: perceptual contrast and stability. In: E., K. (Ed.), Fundamentals of Speech\nSynthesis and Recognition. Chichester:John Wiley, pp. 185\u2013213.\nBonabeau, E., Dorigo, M., Theraulaz, G., 1999. Swarm intelligence. From\nnatural to artificial systems. Santa Fee Institute studies in the sciences of\ncomplexity.\nBrowman, C., Goldstein, L., 1986. Towards an articulatory phonology. Phonology Yearbook 3, 219\u2013252.\nBrowman, C., Goldstein, L., 2000. Competing constraints on intergestural\ncoordination and self-organization of phonological structures. Bulletin de la\nCommunication Parle 5, 25\u201334.\n27\n\n\fCangelosi, A., Parisi, D., 2002. Simulating the evolution of language. Springer\nVerlag.\nCarlson, R., Granstrom, B., Fant, G., 1970. Some studies concerning perception of isolated vowels. Quarterly Progress Status Reports STL-QPSR 2/3,\nSpeech Transmission Laboratory.\nCrothers, J., 1978. Typology and universals of vowels systems. Phonology 2,\n93\u2013152.\nDamper, R., Harnad, S., 2000. Neural network modelling of categorical perception. Perception and Psychophysics 62, 843\u2013867.\nde Boer, B., 2001. The origins of vowel systems. Oxford Linguistics. Oxford\nUniversity Press.\nGeorgopoulos, A., Kettner, R., Schwartz, A., 1988. Primate motor cortex and\nfree arm movement to visual targets in three-dimensional space : coding\nof the direction of movement by a neuronal population. Journal of Neurosciences 8, 2928\u20132937.\nGuenther, F., Gjaja, M., 1996. The perceptual magnet effect as an emergent property of neural map formation. Journal of the Acoustical Society of\nAmerica 100, 1111\u20131121.\nHopfield, J., 1982. Neural networks and physical systems with emergent collective abilities. Proceedings of the National Academy of Sciences of the\nUSA - Biological Sciences 79 (8), 2554\u20132558.\nKaplan, F., 2001. La naissance d' une langue chez les robots.\nKirby, S., 2001. Spontaneous evolution of linguistic structure - an iterated\nlearning model of the emergence of regularity and irregularity. IEEE Transactions on Evolutionary Computation 5 (2), 102\u2013110.\nKohonen, T., 1982. Self-organized formation of topologically correct feature\nmaps. Biological Cybernetics 43 (1), 59\u201369.\nLadefoged, P., Maddieson, I., 1996. The Sounds of the World's Languages.\nBlackwell Publishers, Oxford.\nLindblom, B., 1992. Phonological units as adaptive emergents of lexical development. In: Ferguson, Menn, Stoel-Gammon (Eds.), Phonological Development: Models, Research, Implications. York Press, Timonnium, MD, pp.\n565\u2013604.\nMaddieson, I., 1984. Patterns of sound. Cambridge university press.\nMorasso, P., Sanguinetti, V., Frisone, F., Perico, L., 1998. Coordinate-free sensorimotor processing: computing with population codes. Neural Networks\n11, 1417\u20131428.\nMrayati, M., Carr, R., Gurin, B., 1988. Distinctive region and modes: A new\ntheory of speech production. Speech Communication 7, 257\u2013286.\nNicolis, G., Prigogine, I., 1977. Self-Organization in Nonequilibrium Systems:\nFrom Dissipative Structures to Order through Fluctuations. Wiley.\nOudeyer, P.-Y., 2001a. Coupled neural maps for the origins of vowel systems. In: Dorffner G., Bischof H., H. K. (Ed.), Artificial Neural Networks ICANN. LNCS 2130. Springer Verlag, pp. 1171\u20131176.\nOudeyer, P.-Y., 2001b. Origins and learnability of syllable systems, a cultural\n28\n\n\fevolutionary model. In: P. Collet, C. Fonlupt, J. H. E. L. M. S. (Ed.), in\nArtificial Evolution. LNCS 2310. pp. 143\u2013155.\nOudeyer, P.-Y., 2003. L'auto-organisation de la parole. Ph.D. thesis, Universit\u00e9\nParis VI.\nSchwartz, J., Bo, L., Valle, N., Abry, C., 1997. Major trends in vowel systems\ninventories. Journal of Phonetics 25, 255\u2013286.\nSejnowsky, T., 1977. Storing covariance with non-linearly interacting neurons.\nJournal of mathematical biology 4, 303\u2013312.\nSteels, L., 1997. The synthetic modeling of language origins. Evolution of\nCommunication 1.\nSteels, L., 2001. The methodology of the artificial. Behavioural and brain\nsciences 24 (6).\nStevens, K., 1972. The quantal nature of speech: evidence from articulatoryacoustic data. New-York: Mc Graw-Hill, pp. 51\u201366.\nThompson, D., 1932. On Growth and Form. Cambridge University Press.\nVallee, N., 1994. Systemes vocaliques: de la typologie aux predictions. Ph.D.\nthesis, Universite Stendhal, Grenoble, France.\nVihman, M., 1996. Phonological Development: The Origins of Language in\nthe Child. Oxford, UK: Blackwell Publishers.\nvon Frisch, K., 1974. Animal Architecture. London: Hutchinson.\n\n29\n\n\f"}