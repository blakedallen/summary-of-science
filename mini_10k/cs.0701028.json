{"id": "http://arxiv.org/abs/cs/0701028v2", "guidislink": true, "updated": "2008-05-30T21:26:09Z", "updated_parsed": [2008, 5, 30, 21, 26, 9, 4, 151, 0], "published": "2007-01-05T06:11:16Z", "published_parsed": [2007, 1, 5, 6, 11, 16, 4, 5, 0], "title": "Statistical keyword detection in literary corpora", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0701171%2Ccs%2F0701047%2Ccs%2F0701184%2Ccs%2F0701012%2Ccs%2F0701037%2Ccs%2F0701076%2Ccs%2F0701134%2Ccs%2F0701149%2Ccs%2F0701113%2Ccs%2F0701092%2Ccs%2F0701176%2Ccs%2F0701163%2Ccs%2F0701038%2Ccs%2F0701155%2Ccs%2F0701101%2Ccs%2F0701108%2Ccs%2F0701055%2Ccs%2F0701094%2Ccs%2F0701178%2Ccs%2F0701200%2Ccs%2F0701139%2Ccs%2F0701146%2Ccs%2F0701127%2Ccs%2F0701103%2Ccs%2F0701058%2Ccs%2F0701168%2Ccs%2F0701129%2Ccs%2F0701141%2Ccs%2F0701093%2Ccs%2F0701023%2Ccs%2F0701046%2Ccs%2F0701126%2Ccs%2F0701120%2Ccs%2F0701005%2Ccs%2F0701061%2Ccs%2F0701125%2Ccs%2F0701078%2Ccs%2F0701144%2Ccs%2F0701083%2Ccs%2F0701131%2Ccs%2F0701095%2Ccs%2F0701151%2Ccs%2F0701096%2Ccs%2F0701187%2Ccs%2F0701034%2Ccs%2F0701183%2Ccs%2F0701062%2Ccs%2F0701122%2Ccs%2F0701110%2Ccs%2F0701027%2Ccs%2F0701194%2Ccs%2F0701069%2Ccs%2F0701091%2Ccs%2F0701124%2Ccs%2F0701028%2Ccs%2F0701191%2Ccs%2F0701041%2Ccs%2F0701085%2Ccs%2F0701123%2Ccs%2F0701185%2Ccs%2F0701040%2Ccs%2F0701024%2Ccs%2F0701161%2Ccs%2F0701169%2Ccs%2F0701057%2Ccs%2F0701080%2Ccs%2F0701052%2Ccs%2F0701090%2Ccs%2F0701030%2Ccs%2F0701167%2Ccs%2F0701119%2Ccs%2F0701016%2Ccs%2F0701150%2Ccs%2F0701043%2Ccs%2F0701107%2Ccs%2F0701195%2Ccs%2F0701033%2Ccs%2F0701025%2Ccs%2F0701111%2Ccs%2F0701068%2Ccs%2F0701014%2Ccs%2F0701186%2Ccs%2F0701035%2Ccs%2F0701019%2Ccs%2F0701196%2Ccs%2F0701180%2Ccs%2F0701106%2Ccs%2F0701152%2Ccs%2F0701198%2Ccs%2F0701193%2Ccs%2F0701158%2Ccs%2F0701159%2Ccs%2F0701188%2Ccs%2F0701077%2Ccs%2F0701026%2Ccs%2F0701031%2Ccs%2F0701173%2Ccs%2F0701142%2Ccs%2F0701048%2Ccs%2F0701145%2Ccs%2F0701015&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Statistical keyword detection in literary corpora"}, "summary": "Understanding the complexity of human language requires an appropriate\nanalysis of the statistical distribution of words in texts. We consider the\ninformation retrieval problem of detecting and ranking the relevant words of a\ntext by means of statistical information referring to the \"spatial\" use of the\nwords. Shannon's entropy of information is used as a tool for automatic keyword\nextraction. By using The Origin of Species by Charles Darwin as a\nrepresentative text sample, we show the performance of our detector and compare\nit with another proposals in the literature. The random shuffled text receives\nspecial attention as a tool for calibrating the ranking indices.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0701171%2Ccs%2F0701047%2Ccs%2F0701184%2Ccs%2F0701012%2Ccs%2F0701037%2Ccs%2F0701076%2Ccs%2F0701134%2Ccs%2F0701149%2Ccs%2F0701113%2Ccs%2F0701092%2Ccs%2F0701176%2Ccs%2F0701163%2Ccs%2F0701038%2Ccs%2F0701155%2Ccs%2F0701101%2Ccs%2F0701108%2Ccs%2F0701055%2Ccs%2F0701094%2Ccs%2F0701178%2Ccs%2F0701200%2Ccs%2F0701139%2Ccs%2F0701146%2Ccs%2F0701127%2Ccs%2F0701103%2Ccs%2F0701058%2Ccs%2F0701168%2Ccs%2F0701129%2Ccs%2F0701141%2Ccs%2F0701093%2Ccs%2F0701023%2Ccs%2F0701046%2Ccs%2F0701126%2Ccs%2F0701120%2Ccs%2F0701005%2Ccs%2F0701061%2Ccs%2F0701125%2Ccs%2F0701078%2Ccs%2F0701144%2Ccs%2F0701083%2Ccs%2F0701131%2Ccs%2F0701095%2Ccs%2F0701151%2Ccs%2F0701096%2Ccs%2F0701187%2Ccs%2F0701034%2Ccs%2F0701183%2Ccs%2F0701062%2Ccs%2F0701122%2Ccs%2F0701110%2Ccs%2F0701027%2Ccs%2F0701194%2Ccs%2F0701069%2Ccs%2F0701091%2Ccs%2F0701124%2Ccs%2F0701028%2Ccs%2F0701191%2Ccs%2F0701041%2Ccs%2F0701085%2Ccs%2F0701123%2Ccs%2F0701185%2Ccs%2F0701040%2Ccs%2F0701024%2Ccs%2F0701161%2Ccs%2F0701169%2Ccs%2F0701057%2Ccs%2F0701080%2Ccs%2F0701052%2Ccs%2F0701090%2Ccs%2F0701030%2Ccs%2F0701167%2Ccs%2F0701119%2Ccs%2F0701016%2Ccs%2F0701150%2Ccs%2F0701043%2Ccs%2F0701107%2Ccs%2F0701195%2Ccs%2F0701033%2Ccs%2F0701025%2Ccs%2F0701111%2Ccs%2F0701068%2Ccs%2F0701014%2Ccs%2F0701186%2Ccs%2F0701035%2Ccs%2F0701019%2Ccs%2F0701196%2Ccs%2F0701180%2Ccs%2F0701106%2Ccs%2F0701152%2Ccs%2F0701198%2Ccs%2F0701193%2Ccs%2F0701158%2Ccs%2F0701159%2Ccs%2F0701188%2Ccs%2F0701077%2Ccs%2F0701026%2Ccs%2F0701031%2Ccs%2F0701173%2Ccs%2F0701142%2Ccs%2F0701048%2Ccs%2F0701145%2Ccs%2F0701015&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Understanding the complexity of human language requires an appropriate\nanalysis of the statistical distribution of words in texts. We consider the\ninformation retrieval problem of detecting and ranking the relevant words of a\ntext by means of statistical information referring to the \"spatial\" use of the\nwords. Shannon's entropy of information is used as a tool for automatic keyword\nextraction. By using The Origin of Species by Charles Darwin as a\nrepresentative text sample, we show the performance of our detector and compare\nit with another proposals in the literature. The random shuffled text receives\nspecial attention as a tool for calibrating the ranking indices."}, "authors": ["Juan P. Herrera", "Pedro A. Pury"], "author_detail": {"name": "Pedro A. Pury"}, "author": "Pedro A. Pury", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1140/epjb/e2008-00206-x", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cs/0701028v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0701028v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published version. 11 pages, 7 figures. SVJour for LaTeX2e", "arxiv_primary_category": {"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CL", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "physics.soc-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0701028v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0701028v2", "journal_reference": "Eur. Phys. J. B 63, 135-146 (2008)", "doi": "10.1140/epjb/e2008-00206-x", "fulltext": "EPJ manuscript No.\n(will be inserted by the editor)\n\narXiv:cs/0701028v2 [cs.CL] 30 May 2008\n\nStatistical Keyword Detection in Literary Corpora\nJuan P. Herreraa and Pedro A. Puryb\nFacultad de Matem\u00e1tica, Astronom\u0131\u0301a y F\u0131\u0301sica, Universidad Nacional de C\u00f3rdoba,\nCiudad Universitaria, X5000HUA C\u00f3rdoba, Argentina\nReceived: 1st May 2007 / Received in final form 15 February 2008\nc EDP Sciences, Societ\u00e0 Italiana di Fisica, Springer-Verlag 2008\nAbstract. Understanding the complexity of human language requires an appropriate analysis of the statistical distribution of words in texts. We consider the information retrieval problem of detecting and ranking\nthe relevant words of a text by means of statistical information referring to the spatial use of the words.\nShannon's entropy of information is used as a tool for automatic keyword extraction. By using The Origin\nof Species by Charles Darwin as a representative text sample, we show the performance of our detector and\ncompare it with another proposals in the literature. The random shuffled text receives special attention as\na tool for calibrating the ranking indices.\nPACS.\n\u2013 89.70.+c Information theory and communication theory\n\u2013 05.45.Tp Time series analysis\n\u2013 89.75.-k Complex systems\n\n1 Introduction\nData mining for texts is a well-established area of natural\nlanguage processing [1]. Text mining is the computerised\nextraction of useful answers from a mass of textual information by machine methods, computer-assisted human\nones, or a combination of both. A key problem in text\nmining is the extraction of keywords from texts for which\nno a priori information is available. The problem of unsupervised extraction of relevant words from their statistical\nproperties was first addressed by Luhn [2], who based his\nmethod on Zipf's analysis of frequencies [3]. This analysis consists of counting the number of occurrences of each\ndistinct word in a given text, and then generating a list\nof all these words ordered by decreasing frequency. In this\nlist, each word is identified by its position or Zipf 's rank\nin the list. The empirical observation of Zipf was that the\nfrequency of occurrence of the r\u2013th rank in the list is proportional to r\u22121 (Zipf 's law). Luhn proposed the crude\napproach of excluding the words at both ends of the Zipf's\nlist and considering as keywords the remaining cases. The\nlimitations of Luhn's approach are known in the literature [4].\nThe main goal of this work is to investigate unsupervised statistical methods for detecting keywords in literacy\ntexts beyond the simple counting of word occurrences. In\na\nPresent address: Argentina Software Development Center (ASDC), Intel Software, C\u00f3rdoba, Argentina (e-mail:\njuan.herrera@intel.com).\nb\nCorresponding author (e-mail: pury@famaf.unc.edu.ar).\n\norder to obtain statistically significant results we restrict\nour work to a large book, which can be used as a corpus what is thematically consistent throughout its entire\nlength. We are searching for relevance according to the\ntext's context, but we will only use statistical information\nabout the spatial use of the words in a text.\nParticularly, the measure of content of information for\neach word can be made by Shannon's entropy. In the\nphysics literature, we can find several applications of the\nentropy concept to linguistics and natural language like\nDNA sequences analysis [5,6,7], long-range correlations\nmeasurements [8,9], language acquisition [10], authorship\ndisputes [11,12], communication modelling [13], and statistical analysis of the linguistic role of words in corpora [14].\nThe organisation of the remainder of the article is as\nfollows. In Sec. 2 we first introduce the corpus used as\na representative sample throughout this work. Later, in\nSec. 3 we review the algorithms proposed in the literature based on the analysis of the statistical distribution of\nwords in a text. Then, in Sec. 4 we discuss the behaviour of\nthe indices in random texts. By using Shannon's entropy,\nin Sec. 5 we propose another index based on the information content of the sequence of occurrences of each word\nin the text. In Sec. 6 we use the glossary of the corpus for\nmeasuring the performace of each index as keyword detector. Finally, in Sec. 7 we present a summary of the work.\nBesides, mathematical details are given in appendices. In\nAppendix A we review the geometrical distribution, useful to random texts, and in Appendix B we calculate the\nentropy of a random text.\n\n\f2\n\nJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n50\n\n2 Representative Corpus Sample\nFor our study, we will use a prototypical real text, i.e.,\n\"On the Origin of Species by Means of Natural Selection,\nor The Preservation of Favoured Races in the Struggle for\nLife\" [15] (usually abbreviated to The Origin of Species)\nby Charles Darwin (1859). The book was written with the\nvocabulary of a nineteenth-century naturalist but with\nfluid prose, combining first\u2013person narrative with scholarly analysis.\nFor the preparation of our working corpus we first\nwithdrew any punctuation symbol from the text, mapped\nall words to uppercase and then used the simple tokenization method based on whitespaces [1]. We draw a distinction between a word token versus a word type. For our\nconvenience, we define a word type as any different string\nof letters between two whitespaces. Thus, for our elementary analysis, words like INSTINCT and INSTINCTS correspond to different word types in our corpus. On the other\nhand, a word token is each individual occurrence of a given\nword type. When the context refers to a particular word\ntype, we will use indistinctly \"word token\" or simply \"token\" to refer to an individual occurrence of the word type\nin the text.\nThe relevant words have not been explicitly defined in\nDarwin's book, with exception of a glossary appended at\nthe end of the work. Therefore, the table of contens in the\nbeginning, the glossary and the analytical index, also inserted at the end, were removed from our corpus. By doing\nthis, we avoid introducing obvious bias for the words used\nin these parts. Thus, the prepared corpus includes 94% of\nmaterial from the original Darwin's book and has 192, 665\nword tokens and 8, 294 word types. In addition, the corpus\ncontains 842 paragraphs distributed in 16 chapters.\nThe glossary of the principal scientific terms used in\nthe book, prepared by Mr. W.S. Dallas, and the analytical index, both appended at the end of the book, were\nwritten using 2, 418 word types. If we do not consider the\nfunction words, still remain 1, 679 word types (20% of the\nbook's lexicon). With this information,, we prepared by\nhand a customed version of the glossary, by selecting 283\nword types (3.4% of the lexicon) with frequencies of occurrence greater than 9. We have avoided word types with less\nthan 9 occurrences because we cannot extract any significant statistics from data obtained using such small sets.\nThus, the criterion for selection was rather more arbitrary,\nbut we think that all selected words are pertinent to the\nbook's context. Our prepared version of the glossary will\nbe used later to evaluate the retrieval capabilities of different keyword extractors.\n\n3 Clustering as criterion for relevance of\nwords\nThe attraction between words is a phenomenon that plays\nan important role in both language processing and acquisition, and it has been modeled for information retrieval\nand speech recognition purposes [16,17]. Empirical data\n\noriginal text\nrandom text\n\n40\n\nNATURAL (rank: 53)\n30\n\nf\n20\n\n10\n\n0\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\ninter-token distance\nFig. 1. Histogram of frequencies of distances between occurrences of NATURAL (Zipf's rank 53) in Darwin's corpus.\n\nreveals that the attraction between words decays exponentially, while stylistic and syntactic constraints create\na repulsion between words that discourages close occurrences. In Fig. (1) we have plotted the histogram of absolute frequencies of distances between nearest neighbour\ntokens of the word type NATURAL in Darwin's corpus.\nFor long distances, Fig. (1) qualitatively suggests an exponential tail, but for very short distances the frequencies\ndecay abruptly. Also in Fig. (1) we have superimposed\nthe histogram of a random shuffled version of the corpus\nwhere we can qualitatively see an exponential decay for all\ndistances. The attraction\u2013repulsion phenomenon is more\nemphasized for relevant words than for common words,\nwhich have less syntactic penalties for close co-occurrence.\nTherefore, the spatial distributions of relevant words in\nthe text are inhomogeneous and these words gather together in some portions of the text forming clusters. The\nclustering phenomenon can be visualised in Fig. 2 where\nwe have plotted the absolute positions of four different\nword types from Darwin's corpus in a \"bar code\" arrangement. The clustering becomes manifest in the patterns of\nNATURAL, LIFE, and INSTINCT in spite of their different numbers of occurrences. In contrast, THE (the more\nfrequent word in the English language) has no apparent\nclustering.\nRecently, the assumption that highly relevant words\nshould be concentrated in some portions of the text was\nused for searching relevant words in a given text. In the\nfollowing two subsections, we briefly review the indices of\nrelevance of words proposed by Ortu\u00f1o et al. [18] and Zhou\nand Slater [19], which are based on the spatial distribution\nof words in the text.\n\n\fJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\ndefinition\n\nINSTINCT (rank: 405)\n\nv\nu\nu\ns=t\nLIFE (rank: 72)\n\nNATURAL (rank: 53)\n\nTHE (rank: 1)\n\n0\n\n50 k\n\n100 k\n\nt (\u2715 1000)\n\n150 k\n\n200 k\n\nFig. 2. Absolute positions (t) in the text, counted from the\nbeginning of Darwin's corpus, of the word types: THE (13, 414\noccurrences), NATURAL (475 occurrences), LIFE (326 occurrences), and INSTINCT (64 occurrences). To draw the picture,\nwe set a very thin vertical line (of arbitrary height) at the position of each occurrence.\n\n3.1 \u03c3\u2013index\nTo study the spatial distribution of a given word type in\na text, we can map the occurrences of the corresponding\nword tokens into a time series. For this task, we denote\nby ti the absolute position in the corpus of the i\u2013th occurrence of a word token. Thus, we obtain the sequence\n{t0 , t1 , . . . , tn , tn+1 }, where we are assuming that there are\nn word tokens. We have additionally included the boundaries of the corpus, defining t0 = 0 and tn+1 = N + 1,\nwhere N is the total number of tokens in the corpus, in\norder to take into account the space before the first occurrence of a word token and the space after the last occurrence of a token [19].\nGiven the sequence of inter\u2013token distances\n{t1 \u2212 t0 , t2 \u2212 t1 , . . . , tn \u2212 tn\u22121 , tn+1 \u2212 tn } ,\nthe average distance between two successive word tokens\nis given by\n\u03bc=\n\n3\n\nn\nN +1\n1 X\n,\n(ti+1 \u2212 ti ) =\nn + 1 i=0\nn+1\n\n(1)\n\nand the sample standard deviation of the set of spacings\nbetween nearest neighbour word tokens (ti+1 \u2212 ti ) is by\n\nn\n1 X\n((ti+1 \u2212 ti ) \u2212 \u03bc)2 .\nn \u2212 1 i=0\n\n(2)\n\nTo eliminate the dependence on the frequency of occurrence for different word types, in Ref. [18] the authors\nsuggest to normalise the token spacings, i.e., to measure\nthem in units of their corresponding mean value. Thus, we\ndefine\ns\n(3)\n\u03c3= .\n\u03bc\nGiven that the standard deviation grows rapidly when\nthe inhomogeneity of the distribution of spacing ti+1 \u2212 ti\nincreases, Ortu\u00f1o et al. [18] proposed \u03c3 as an indicator of\nthe relevance of the words in the analysed text. In many\ncases, empirical evidence vindicates that large \u03c3 values\ngenerally correspond to terms relevant to the text considered, and that common words have associated low values\nof \u03c3. However, Zhou and Slater [19] pointed out that \u03c3index has some weaknesses. First, several obviously common (relevant) words have relative high (low) \u03c3 values in\nseveral texts. Second, the index is not stable in the sense\nthat it can be strongly affected by the change of a single\noccurrence position. Third, high values of \u03c3 do not always imply a cluster concentration. A big cluster of words\ncan be splitted into smaller clusters without substantial\nchange in the \u03c3 value.\n3.2 \u0393 \u2013index\nThe \u03c3-index is only based on the spacing between nearestneighbour word tokens. To improve the performance in the\nsearching for relevance, Zhou and Slater [19] introduced a\nnew index that uses more information from the sequence\nof occurrences {t0 , t1 , . . . , tn , tn+1 }. For this task, these\nauthors consider the spacings wi = ti \u2212 ti\u22121 , with i =\n1, . . . , n + 1, and define the average separation around the\noccurrence at ti as\nti+1 \u2212 ti\u22121\nwi+1 + wi\n=\n,\ni = 1, . . . , n . (4)\nd(ti ) =\n2\n2\nThe position ti is said to be a cluster point if d(ti ) < \u03bc.\nThe new suggestion is that the relevance of a word in a\ngiven text is related to the number of cluster points found\nin it. Thus, in order to measure the degree of clusterization, the local cluster index at position ti is defined by\n\uf8f1\n\uf8f2 \u03bc \u2212 d(ti )\nif ti is a cluster point\n\u03b3(ti ) =\n. (5)\n\uf8f30 \u03bc\notherwise\n\nFinally, a new index to measure relevance is obtained from\nthe average of all cluster indices corresponding to a given\nword type\nn\n1X\n\u03b3(ti ) .\n(6)\n\u0393 =\nn i=1\n\u0393 -index is more stable than \u03c3, but it is still based on local\ninformation and is computationally more time consuming\nto evaluate than \u03c3.\n\n\f4\n\nJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\n4 Random text and shuffled words\nIn a completely random text we have an uncorrelated sequence of tokens, and a word type w is only characterised\nby its relative frequency of occurrence (pw ). Thus, a random text can be generated by picking successively tokens\nby chance in such a way that at each position the probability of finding a token,\nP corresponding to the word type\nw, is pw . Obviously, w pw = 1. For the word type w, we\nhave in this manner defined a binomial experiment where\nthe probability of success (occurrence) at each site in the\ntext is pw , and the probability of failure (non-occurrence)\nis (1 \u2212 pw ). Therefore, the distribution of distances between nearest neighbour tokens corresponding to the same\nword type is geometrical. In Appendix A, we have compiled some results of the geometrical distribution that are\nuseful for our next analyses.\nBesides, its worth as comparative standard, the theoretical random text has the virtue of being analytically\ntractable. Also, from an empirical point of view, there is\na workable fashion for building a random version of a corpus. In an actual corpus the probabilities of occurrence\np are estimated from the relative frequencies n/N , where\nn is the number of tokens corresponding to a given word\ntype and N is the total number of tokens in the corpus.\nA random version of the text can be obtained by shuffling\nor permuting all the tokens. The random shuffling of all\nthe words has the effect of rescasting the corpus into a\nnonsensical realization, keeping the same original tokens\nwithout discernible order at any level. However, both the\nZipf's list of ranks and the frequency of occurrence of each\nword type are kept intact.\nThe important point that we want to stress here is that\nthe indices of relevance defined in the previous section are\nfunctions of the frequencies of occurrence of each word\ntype. Thus, in a random text the values of these indices\nchange with p, which has nonsense. In a truly random\ntext, there are not relevant words. Therefore, to eliminate\ncompletely the dependence on frequency we need to renormalise the indices with their values in the random version\nof the corpus.\n4.1 Renormalised \u03c3\u2013index\nFor a given probability distribution, \u03c3 is defined from the\n\u221a\nsecond\u2013 (\u03bc2 ) and first\u2013order (\u03bc1 ) cumulant by \u03bc2 /\u03bc1 .\nThus, from Eq. (20) in Appendix A we find that in a\nrandom text the value of \u03c3\u2013index is given by\np\n(7)\n\u03c3ran = 1 \u2212 p .\nHence, we renormalise the index to eliminate this dependence on relative frequency defining\n\u03c3nor =\n\n1\ns\n\u221a\n.\n\u03bc 1\u2212p\n\n(8)\n\nFor texts as large as corpora the importance of normalisation factor given by Eq. (7) becomes negligible. For example, in Darwin's corpus, N = 192, 665, and for the most\n\nFig. 3. Renormalised \u03c3\u2013index vs. Zipf's rank for each word\nin Darwin's corpus (the first 4000 ranks). We have also plotted superposed the random version of the text (grey) and we\nhave marked by open circles the words corresponding to our\nprepared glossary (red online).\n\nfrequent word type (THE) we have n = 13, 414 (n/N =\n0.0696). Thus, in the less significant case (the lowest value\nfor \u03c3ran ) \u03c3ran = 0.965, whereas \u03c3ran = 1 for p = 0. However, for shorter texts the significance of the normalisation\nmay become critical and the values of \u03c3 and \u03c3nor may be\nvery different for any word type.\nIn Fig. 3 we plot the values of \u03c3nor for the first 4000\nranks in the Zipf's list of Darwin's corpus. The random\nversion of the corpus is also plotted in the same graph.\nThe \"cloud of points\" corresponding to the random text\nis distributed around the unitary value of \u03c3nor , but the\nwidth of the \"cloud\" grows with rank. This behaviour is\ndue to the fact that the frequency of occurrence decreases\nas the rank increases (Zipf's law), therefore the statistics\nget worse. The words of our preparated version of the\nglossary are marked by open circles in Fig. 3. From Fig. 3,\nit is appreciable that most of the glossary words have high\nvalues of \u03c3nor .\n4.2 Renormalised skewness\nAs in the case of \u03c3, any cumulant contains partial information of the spatial distribution of words. Skewness is\na parameter that describes the asymmetry of a distribution. Mathematically, the skewness is measured using the\nsecond\u2013 (\u03bc2 ) and third\u2013order (\u03bc3 ) cumulant of the distri3/2\nbution according to \u03ba = \u03bc3 /\u03bc2 . Given that the distances\nbetween nearest neighbour tokens are positive defined, the\ncorresponding distribution has positive skew, i.e., the upper tail is longer than the the lower tail (see Fig. 1).\n\n\fJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\n5\n\nFrom Eq. (20), we find that in a random text the skewness of the distribution of distances between nearest neighbour tokens is given by\n2\u2212p\n;\n\u03baran = \u221a\n1\u2212p\n\n(9)\n\nThus, the skewness also depends on the relative frequency\nof occurrence, p, in the random case. However, this dependence is also negligible for a corpora. In Darwin's corpus\nwe obtain \u03baran = 2.001 for the largest value p = 0.0696\n(the relative frequency of the word type THE), whereas\n\u03baran = 2 for p = 0.\nAs a consequence, we can define another renormalised\nquantity as we did with the \u03c3\u2013index. Thus, to eliminate\nthe dependence on the relative frequency of occurrence in\nthe random case, we write\n\u221a\n1\u2212p\n\u03bc3\n.\n(10)\n\u03banor = 3/2\n2\n\u2212p\n\u03bc2\n\u03banor can also be used for measuring relevance. However,\nthe finite-size effects of the texts are more pronounced\nfor higher order cumulants. We now use both cumulants\n\u03c3nor and \u03banor to construct a bi-dimensional graph for\nthe corpus. In this manner, in Fig. 4 we plot the the\npairs (\u03c3nor , \u03banor ) for all words in Darwin's corpus. In this\ngraph, the \"cloud of points\" corresponding to the random\ntext is distributed around the pair of values (1, 1), while\nthe region defined by \u03c3nor > 2 and \u03banor > 2 has almost\nnone. The upper right corner of the graph concentrates almost all the points corresponding to the glossary. Figure 4\ngives us immediate insight into the distribution of distances between nearest neighbour tokens, and provides us\na powerful tool for determining keywords.\n4.3 Renormalised \u0393 \u2013index\nAs we did with the \u03c3\u2013index, we need to calculate \u0393 for a\nword type which appears in a random text with relative\nfrequency p. For this task, we calculate the average of the\nrandom variable \u03b3 defined in Eq. (5) in a random text.\nFrom Eq. (30) in Appendix A we obtain\n\u0393ran =\n\n1\nh (h \u2212 1) (1 \u2212 p)h ((1 \u2212 p) + (1 \u2212 p)\u22121 \u2212 2) , (11)\n2\n\nwhere h = Int[2/p]. In this case, the dependence on p is\neven more complicated than previous cases. This observation is absent from Ref. [19]. Zhou and Slater only calculate the value of \u0393 for the Poisson distribution: \u0393 = 2 e\u22122\n(see Eq. (33) in Appendix A), which is constant (\u2248 0.271).\nAlso in this case, the dependence on p is negligible for corpora. In Darwin's corpus we obtain \u0393ran = 0.261 for the\nlargest value of p = 0.0696 (the relative frequency of the\nword type THE), whereas \u0393ran \u2248 0.271 in the limit p \u2192 0\n(see Appendix A).\nNow, as in the other cases, we define from Eqs. (6)\nand (11) a renormalised index by \u0393nor = \u0393/\u0393ran . In\nFig. 5 we plot the values of \u0393nor for the first 4000 ranks in\n\nFig. 4. Renormalised \u03ba\u2013index vs. \u03c3\u2013index for all words in\nDarwin's corpus. We have also plotted superposed the random\nversion of the text (grey) and we have marked by open circles\nthe words corresponding to our prepared glossary (red online).\n\nthe Zipf's list of Darwin's corpus. The \"cloud of points\"\ncorresponding to the random text is distributed around\nthe unitary value, but the width of the \"cloud\" grows\nwith rank faster than in the case of \u03c3nor . The words corresponding to the glossary have systematically high values\nof \u0393nor .\n\n5 Entropy of token distributions\nClaude Shannon introduced the concept of entropy of information in 1948 [20]. Mapping a discrete information\nsource on a set of possible events whose probabilities of occurrences are p1 , p2 , . . . , pP , Shannon constructed a measure of information and uncertainty, S(p1 , p2 , . . . , pP ), requiring the following properties:\n1. S should be continuous in the {pi }.\n2. For the iso-probability case, pi = 1/P , S should be a\nmonotonic increasing function of P .\n3. If the set p1 , p2 , . . . , pP is broken down into two subsets\nwith probabilities w1 = p1 + . . . + pk and w2 = pk+1 +\n. . . + pP , then we must have the following composition\nlaw S(p1 , . . . pN ) = S(w1 , w2 )+ w1 S(p1 /w1 , . . . pk /w1 )\n+ w2 S(pk+1 /w2 , . . . pP /w2 ).\nThe only S satisfying the three above assumptions is of\nthe form\nS(p1 , p2 , . . . , pP ) = \u2212K\n\nP\nX\ni=1\n\npi log pi ,\n\n(12)\n\n\f6\n\nJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\nS(w) < 1. In this manner, when a type word is uniformly\ndistributed (pi = 1/P , for all i), Eq. (14) yields S = 1.\nConversely, the other extreme case, S = 0, is when a word\ntype appears only in part j, thus we have pj = 1 and pi = 0\nfor i 6= j. Therefore, words with frequent grammatical use\nlike function words (prepositions, adverbs, adjectives, conjunctions, and pronouns) will have high values of entropy,\nmeanwhile keywords will have low values of entropy. Empirical evidence [14] shows a tendency of the entropy to\nincrease with n. It implies that, on average, the more frequent word types are more uniformly used.\nAs we did with preceding indices, we need to calculate the average of the entropy of a mock word type that\nappears n times in a random corpus. From Eq. (39) in\nAppendix B, we obtain\n(1 \u2212 S)ran \u2248\n\nFig. 5. Renormalised \u0393 \u2013index vs. Zipf's rank for each word\nin Darwin's corpus (the first 4000 ranks). We have also plotted superposed the random version of the text (grey) and we\nhave marked by open circles the words corresponding to our\nprepared glossary (red online).\n\nwhere K is a positive constant.\nA literary corpus can be divided in parts using natural\npartitions such as parts, sections, chapters, paragraphs or\nsentences. Thus, we consider the corpus as a composite of\nP parts. For the i\u2013th part of the corpus we can reckon up\nthe total number Ni of tokens and the number ni (w) of\noccurrence of the word type w inside this part. Then, the\nfraction fi (w) = ni (w)/Ni (i = 1, . . . , P ) is the relative\nfrequency of occurrence of the word type w in the part\nPP\ni. Obviously, i=1 Ni = N is the total number of tokens\nPP\nin the corpus and i=1 ni (w) = n(w) is the number of\ntokens corresponding to the word type w. Therefore, it\nis possible to define a probability measure over the partitions [14] as\nfi (w)\n.\n(13)\npi (w) = P\nX\nfj (w)\nj=1\n\nThe quantity pi (w) results more complex than the conditional probability fi (w)/(n(w)/N ), of finding the word\ntype w in the part i given that it is present in the corpus.\nFollowing Shannon's arguments, the information entropy associated with the discrete distribution pi (w) is\nP\n\nS(w) = \u2212\n\n1 X\npi (w) ln(pi (w)) .\nln(P ) i=1\n\n(14)\n\nThe value 1/ ln(P ) for the constant K was selected to\ntake the maximum value of S equal to one. Thus, 0 <\n\nP \u22121\n,\n2 n ln P\n\n(15)\n\nfor n >> 1 and if all the parts of the random text have\nthe same number of tokens. Empirical evidence [14] shows\nthat the agreement of Eq. (15) with random shuffling of\ntexts using natural partitions is very good, in spite of the\nlimitation of the last assumption. From Eq. (15), we can\nsee that the dependence on the absolute frequency, n, is\ncritical for (1 \u2212 S)ran and it could not be ignored even if\nthe text is as large as a corpus.\nMontemurro and Zanette [14] proposed Eqs. (13) and\n(14) to study the distribution of words according to their\nlinguistic role. For this task, they found that the suitable coordinates whereby words can be categorized are\nn (1 \u2212 S) and n. In the same way, we will use these ideas\nfor detecting relevance of words. We cannot use directly\nthe entropy as index because all tokens with only one occurrence have zero entropy. Thus, we define a normalised\nindex freed from the dependence on absolute frequency\n(n) in random texts by\n2 ln P\n(1 \u2212 S(w)) .\nP \u22121\n(16)\nFigure 6 shows the values of Enor for all word types of\nDarwin's corpus versus its number of occurrence, n, on a\ndouble logarithmic scale. The individual deviations from\nthe bulk trend for each value of n are related to the particular usage nuances of words. To stress these deviations,\nwe have used the 16 chapters of the corpus as natural partitions for our entropic analysis (i.e. P = 16). In this way,\nwe obtain a remarkable scattering of higher values of Enor\nin the full range of number of occurrences. A same entropic\nanalysis using the 842 paragraphs of Darwin's corpus as\npartitions (i.e. P = 842) generates a similar graph that\nstresses the bulk trend, but the fluctuations are completely\nsmoothed. Using the chapters as partitions (P = 16) in\nFig. 6, the \"cloud of points\" corresponding to the random\nversion of the corpus is distributed around the unitary\nvalue and the corpus appears clearly more separated from\nthe random text than with previous indices. Additionally,\nthe words corresponding to the glossary have systematically high values of the index Enor .\nEnor (w) = n(w) (1 \u2212 S(w))nor = n(w)\n\n\fJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\n7\n\nTable 1. Recall and precision of each index. N G is the number of glossary's word types among the first 283 entries of each\nranking. LP is the last position in each ranking in which appears a word type of the glossary. Thus, recall = N G/283 and\nprecision = 283/LP .\nIndex\nEnor\n\u03c3nor\n\u03banor\n\u0393nor\n\nNG\n118\n114\n107\n72\n\nrecall\n0.417\n0.403\n0.378\n0.254\n\nLP\n2, 790\n5, 689\n4, 181\n4, 312\n\nprecision\n0.101\n0.050\n0.068\n0.066\n\nlast word\nFLOWERING\nSCARCELY\nINDIAN\nOSTRICH\n\n0.11\n\nEnor\n0.1\n\nFig. 6. Enor vs. number of occurrence (n) for each word in\nDarwin's corpus. We have also plotted superposed the random\nversion of the corpus (grey) and we have marked by open circles\nthe words corresponding to our prepared glossary (red online).\n\nTo reinforce our graphical findings, in the following\nsection we perform a quantitative comparison among the\nindices \u03c3nor , \u03banor , \u0393nor , and Enor based on the power of\neach index for discriminating the glossary from the bulk\nof words.\n\nprecision\n\n0.09\n\n0.08\n\n0.07\n\n\u03banor\n\n\u0393nor\n\n0.06\n\n\u03c3nor\n\n0.05\n\n0.04\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\nrecall\n\n6 Glossary as benchmark\n\nFig. 7. Comparison of information retrieval performance of\nthe indices (see Table 1).\n\nEvaluation in information retrieval makes frequent use of\nthe notions of recall and precision [1,4]. Recall is defined as\nthe proportion of the target items that a system recover.\nPrecision is defined as a measure of the proportion of selected items that are targets. Remembering that our prepared glossary has 283 word types, we denote by N G the\nnumber of the glossary's word types among the first top\n283 ranked word types of the corpus. For our purposes,\nwe define recall of an index of relevance as the fraction\nN G/283. Thus, recall for the index Enor results 41%. On\nthe other hand, precision can be built looking for the last\nword type of our prepared glossary in the global ranking of each index. For our convenience, we denote by LP\nthe position in the ranking of the last word type of the\nglossary, and we define precision of a keyword extractor\nas the fraction 283/LP . Thus, for example, the last entry\nof the glossary according to the index Enor is FLOWERING and is ranked in the position 2, 790. Remembering\nthat the corpus has 8, 294 word types, we obtain that the\ncomplete prepared glossary is allocated by Enor in the\nfirst third part of the ranked lexicon and the precision\nof the index results 10%. Recall and precision are use-\n\nful benchmarks for measuring the index's performance. In\nparticular, recall and precision of each index analysed in\nthis work are given in Table 1. We want to stress that the\nvalues of recall and precision of the indices \u03c3 and \u0393 are\nexactly the same as those obtained for \u03c3nor and \u0393nor ,\nrespectively. This fact is due to the normalisation factors\ngiven by Eqs. (7) and (11), which are almost constant for\na corpus. Therefore, the pair of indices \u03c3 and \u03c3nor (or \u0393\nand \u0393nor ) yield identical rankings of keywords. In order to\ncompare the performance of all indices, in Fig. 7 we have\ndrawn a precision\u2013recall plot where we can see the significant improvement performed by the index Enor , both in\nrecall and precision. Also, in Fiq. (7) we see that \u03banor has\na recall slightly worse than \u03c3nor and precision as good\nas \u0393nor . Thus, we find that the skewness of the distribution of occurrences of a word type has a significant part of\ninformation about the relevance of the word in the text.\nIn Table 2 we show the first top 50 word types of the\nprepared glossary ranked by the index Enor . We also show\nthe rank position of each word type by the others indices.\nA false positive is when the system identifies a keyword\n\n\f8\n\nJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\nTable 2. First top 50 word types of the prepared glossary ranked by the index Enor . The numerical values correspond to the\npositions in the ranking of each word type, not to the actual values of the indices.\nWord type\nHYBRIDS\nSTERILITY\nSPECIES\nFORMS\nVARIETIES\nINSTINCTS\nBREEDS\nFERTILITY\nFORMATIONS\nCROSSED\nSELECTION\nORGANS\nNEST\nINSTINCT\nRUDIMENTARY\nFORMATION\nBEES\nPLANTS\nCELLS\nPOLLEN\nNATURAL\nGROUPS\nCROSSES\nWATER\nSTERILE\n\nEnor\n1\n3\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n16\n17\n18\n19\n21\n22\n23\n24\n25\n26\n27\n29\n31\n\n\u03c3nor\n2\n1\n447\n185\n39\n3\n38\n8\n20\n9\n212\n61\n22\n5\n25\n144\n6\n113\n18\n12\n460\n79\n60\n75\n19\n\n\u0393nor\n13\n7\n1312\n667\n384\n19\n142\n33\n78\n82\n858\n433\n18\n32\n130\n341\n29\n776\n50\n74\n1288\n393\n81\n400\n109\n\nWord type\nSEA\nSEEDS\nFERTILE\nORGAN\nMOUNTAINS\nGLACIAL\nGARTNER\nHYBRID\nCUCKOO\nLAND\nEGGS\nSTRUGGLE\nBREED\nGEOLOGICAL\nCROSS\nHABITS\nSTRUCTURE\nINHABITANTS\nFLOWERS\nANTS\nRACES\nOFFSPRING\nSEXUAL\nVARIABLE\nWILD\n\nEnor\n33\n35\n37\n39\n40\n41\n43\n44\n47\n48\n50\n51\n52\n54\n62\n63\n65\n67\n68\n75\n78\n81\n85\n87\n89\n\n\u03c3nor\n65\n64\n54\n14\n120\n51\n36\n46\n13\n106\n109\n829\n332\n129\n125\n278\n105\n95\n35\n41\n566\n400\n89\n138\n235\n\n\u0393nor\n309\n279\n135\n218\n94\n113\n20\n59\n3\n613\n215\n571\n367\n456\n205\n1260\n1451\n556\n250\n35\n542\n884\n285\n467\n269\n\nTable 3. First 40 false positives word types ranked by the index Enor and its numbers of occurrences n. The numerical values\nin the Enor column correspond to the positions in the ranking of each word types, not to the actual values of the index.\nWord type\nI\nISLANDS\nCHARACTERS\nGENERA\nWAX\nISLAND\nDOMESTIC\nYOUNG\nTEMPERATE\nSLAVES\nNEW\nMY\nINCREASE\nINTERMEDIATE\nPERIOD\nMIVART\nTHROUGH\nHE\nF\nPARTS\n\nEnor\n2\n4\n15\n20\n28\n30\n32\n34\n36\n38\n42\n45\n46\n49\n53\n55\n56\n57\n58\n59\n\nn\n947\n154\n192\n215\n42\n69\n131\n127\n40\n34\n278\n99\n82\n164\n245\n34\n249\n236\n37\n230\n\n*\n*\n*\n\n*\n\n*\n*\n\n*\n\nWord type\nNORTHERN\nDESCENT\nFRESH\nITS\nDIFFERENCES\nCELL\nEXTINCT\nEUROPE\nFERTILISED\nDIAGRAM\nSHALL\nWE\nDEVELOPED\nBEDS\nADULT\nTWO\nBETWEEN\nNUMBER\nOCEANIC\nTHEORY\n\nEnor\n60\n61\n64\n66\n69\n70\n71\n72\n73\n74\n76\n77\n79\n80\n82\n83\n84\n86\n88\n90\n\nn\n41\n80\n50\n497\n168\n30\n116\n81\n34\n40\n105\n1320\n146\n35\n46\n456\n367\n255\n42\n131\n\n*\n*\n\n*\n*\n\n*\n\n*\n\n\fJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\nthat really is not one. In Table 3 we show the first top\n40 ranked (by Enor ) word types not included in our prepared glossary. We can immediately see that several terms\nare not necessarily false positives. We have marked with\nan asterisk (*) in the table those word types that were\nnot previously selected in the prepared glossary, but that\nappeared in the main entries of the original glossary of\nDarwin's book. Indeed, several more word types like these\ncould have been included in our prepared glossary, too.\nMoreover, we could say that the word type I is relevant for\na text that uses the first\u2013person narrative, like Darwin's\nbook. ISLAND and SLAVES were not used neither in the\nbook's glossary nor in its index; however Enor ranks it\nadequately as a keyword. The word type F is also meaningful to the text. It appear in the proper nouns \"Mr.\nF. Smith\" and \"Dr. F. Muller\", and in the collocations\n\"F. sanguinea\", \"F. rufescens\", \"F. fusca\", \"F. flava\", and\n\"F. rufescens\" which denote species. The observations in\nthe last paragraphs induce us to consider that the performance of the index Enor is better than what can be\ninferred from Table 1.\nMoreover, the index Enor requires less computational\nefforts that the others. Knowing the number of occurrences of a word type, the implementation of the algorithm\nfor the variance or the skewness requires of one accumulator plus a counter for reckoning the number of tokens\nbetween nearest neighbour occurrences of the word type.\nWhile, for the entropic index, we only need one counter (of\nnumber of occurrences) for each partition per word type.\nOn the other hand, the algorithm for \u0393 requires three\naccumulators and for each occurrence of a word type we\nneed to determine if it corresponds to a cluster point.\n\n7 Concluding remarks\nIn summary, in this work we addressed the issue of statistical distribution of words in texts. Particularly, we have\nconcentrated on the statistical methods for detecting keywords in literacy text. We reviewed two indices (\u03c3 and \u0393 )\npreviously proposed [18,19] for measuring relevance and\nwe improved them by considering their values in random\ntexts. Additionally, we introduced \u03banor based on the skewness of the distribution of occurrences of a word and we\nproposed another index for keyword detection based on\nthe information entropy. Our proposals are very easy to\nimplement numerically and have performances as detectors as good as or better than the other indices. The ideas\nof this work can be applied to any natural language with\nwords clearly identified, without requiring any previous\nknowledge about semantics or syntax.\n\nAcknowledgements\nContributions to Appendix B by Marcelo Montemurro\nare gratefully acknowledged. This work was partially supported by grant from \"Secretar\u0131\u0301a de Ciencia y Tecnolog\u0131\u0301a\nde la Universidad Nacional de C\u00f3rdoba\" (Code: 05/B370).\n\n9\n\nA The Geometrical distribution\nIn this Appendix we briefly review the basic results of the\ngeometrical distribution, scattered in the literature, that\nare useful for this work. First, we consider an experiment\nwith only two possible outcomes for each trial (binomial\nexperiment). Repeated independent trials of the binomial\nexperiment are called Bernoulli trials if their probabilities\nremain constant throughout the trials. We denote by p the\nprobability of the \"successful\" outcome. Now, we are interested in the probability of success on the j\u2013th trial after\na given success. Given that the trials are independent, we\nimmediately obtain the geometrical distribution\nP (j) = (1 \u2212 p)j\u22121 p ,\n\nfor j \u2265 1 .\n\n(17)\n\nA.1 Moments and cumulants\nThe characteristic function P\nof a stochastic variable X is\ndefined by G(k) = ekX = j\u22651 P (j) exp(kj). Thus, for\nthe geometrical distribution we obtain\nG(k) =\n\np ek\n.\n1 \u2212 (1 \u2212 p) ek\n\n(18)\n\nThis function is also the moment generating function\nhX n i =\n\ndn G\ndk n\n\n.\n\n(19)\n\nk=0\n\nTherefore, the first three cumulants of the geometrical distribution are given by\n\u03bc1 = hXi =\n\n1\n,\np\n2\n\n\u03bc2 = X 2 \u2212 hXi =\n\n1\u2212p\n,\np2\n3\n\n\u03bc3 = X 3 \u2212 3 X 2 hXi + 2 hXi =\n\n(2 \u2212 p) (1 \u2212 p)\n.\np3\n(20)\n\nA.2 Addition of two geometrical variables\nIf X1 e X2 are geometrical distributed independent random variables, the distribution of the addition Y = X1 +\nX2 is\nX\nP (m1 , m2 ) ,\nfor j = 2, 3, . . . ,\nPY (j) =\nm1 +m2 =j\n\n(21)\nwhere the joint probability distribution of the variables\nX1 e X2 , P (m1 , m2 ), is given by\nP (m1 , m2 ) = p2 (1\u2212p)m1 +m2 \u22122 , for m1 \u2265 1, and m2 \u2265 1 .\n(22)\nIn this manner,\nPY (j) =\n\nj\u22121\nX\n\nm=1\n\nP (m, j \u2212 m) =\n\nj\u22121\nX\n\nm=1\n\np2 (1 \u2212 p)j\u22122 .\n\n(23)\n\n\f10\n\nJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\nB Entropy of a random text\n\nTherefore\nPY (j) = (j \u2212 1) p2 (1 \u2212 p)j\u22122 , for j = 2, 3, . . . .\n\n(24)\n\nNow, we are interested in the average of the random\nvariable (recall Eq. (5))\n\uf8f1\nY\n\uf8f2\n, Y < 2\u03bc\n1\u2212\n\u03b3=\n,\n(25)\n2\u03bc\n\uf8f30,\nY \u2265 2\u03bc\n\nwhere Y is the addition of two independent geometrical\ndistributed random variables with mean \u03bc = 1/p. By definition we have that\n\u0013\nh \u0012\nX\nj\n1\u2212\nPY (n) ,\nh\u03b3i =\n2\u03bc\nj=2\n\n(26)\n\nwhere PY (n) is given by Eq. (24) and h = Int[2\u03bc]. Defining\nq = 1 \u2212 p and using the identity\nN\nX\n\nqn =\n\nn=1\n\nq \u2212 q N +1\n1\u2212q\n\nS=\u2212\n\nh\nd X k\u22121\nq\n= 1 \u2212 h q h\u22121 + (h \u2212 1) q h ,\nPY (j) = p\ndq\nn=j\n2\n\nk=2\n\n(28)\nand\np\n\nh\nX\n\nj PY (j) = p3\n\nj=2\n\nh\nd2 X k\nq = 2 \u2212 h (h + 1) q h\u22121\ndq 2\n(29)\nk=2\n\n+2 (h + 1) (h \u2212 1) q h \u2212 h (h \u2212 1) q h+1 .\nTherefore\nh\u03b3i =\n\n1\nh (h \u2212 1) q h (q + q \u22121 \u2212 2) .\n2\n\n(30)\n\nThe Poisson distribution can be obtained from the geometrical distribution in the limit p \u2192 0. Expanding q z\ninto a Taylor series up to fourth order we obtain\n1\n(2 \u2212 3h + h2 ) p4 .\n2\n(31)\nGiven that for p \u2192 0 we have h >> 1, the last equation\ncan be recast as\n\nq h+1 + q h\u22121 \u2212 2 q h \u2248 p2 + (1 \u2212 h) p3 +\n\nq h+1 + q h\u22121 \u2212 2 q h \u2248 p2 1 \u2212 h p + 21 (hp)2\n\n\u2248 p2 exp (\u2212hp) .\n\nP\nwhere n = P\nj=1 nj is the absolute frequency (number of\ntokens) of the word type w.\nFor reasons of simplicity, in this Appendix we consider\nthe particular case in which all the parts have exactly\nthe same number of tokens, i.e. Ni = N/P . Hence, the\nprobability measure defined by Eq. (13) can be simply\nwritten as pi = ni /n and the information entropy defined\nby Eq. (14) results\n\n(27)\n\nwe immediately obtain\nh\nX\n\nHere, we derive the entropy of a random text in a more\ndetailed way that is described in Ref. [14].\nWe consider a corpus of N tokens as a composite of\nP parts, with Ni tokens in the i\u2013th part (i = 1, 2, . . . , P ).\nIn a random corpus, the probability that a word type w\nappears in the part j is Nj /N . Thus, the probability that\nw appears n1 times in part 1, n2 times in part 2, and so\non, is the multinomial distribution\n\u0012 \u0013nj\nP\nY\nNj\n1\n,\n(34)\npw (n1 , n2 , . . . , nP ) = n!\nn ! N\nj=1 j\n\n\u0001\n\n(32)\n\nP\n1 X ni \u0010 ni \u0011\n.\nln\nln P i=1 n\nn\n\n(35)\n\nNow, we are interested in the average value of the entropy over the distribution given by Eq. (34). We only\nneed to compute the average of each term of Eq. (35)\nusing the marginal distributions, pw (ni ), obtained from\nEq. (34). All marginal distributions result binomials with\nmean n/P and variance n/P (1 \u2212 1/P ). Thus, we obtain\nfor the average entropy\n\u0013n\u2212m\n\u0012\nn\n1\nP X m \u0010m\u0011 \u0010 n \u0011 1\n1\u2212\n.\nln\nhSi = \u2212\nln P m=0 n\nn\nP\nm Pm\n(36)\nFor highly frequent word types, n >> 1, we can approximate the binomial distribution by a Gaussian probability function (G(x; \u03bc, \u03c3)) with mean \u03bc = 1/P and variance \u03c3 2 = (1/n)(P \u2212 1)/P 2 . Thus, Eq. (36) can be recast\nas\nZ 1\nP\nx ln x G(x; \u03bc, \u03c3)dx .\n(37)\nhSi \u2248 \u2212\nln P 0\nIn the limit n >> 1, \u03c3 \u2192 0 and the Gaussian probability\nfunction concentrates around its mean value \u03bc. Using the\nexpansion of the function x ln x around \u03bc,\nx ln x \u2248 \u03bc ln \u03bc + (1 + ln \u03bc)(x \u2212 \u03bc) +\n\n11\n(x \u2212 \u03bc)2 ,\n2\u03bc\n\n(38)\n\nin Eq. (37) and remembering that\nZ \u221e\n(x \u2212 \u03bc)2 G(x; \u03bc, \u03c3)dx = \u03c3 2 ,\n\u2212\u221e\n\nFinally, using that hp \u2248 2, we obtain that the average of\nthe random variable \u03b3 for a Poisson distribution [19] is\nh\u03b3i = 2 e\u22122 .\n\n(33)\n\nwe finally obtain for a random text [14] that\nhSi \u2248 1 \u2212\n\nP \u22121\n.\n2 n ln P\n\n(39)\n\n\fJuan P. Herrera, Pedro A. Pury: Statistical Keyword Detection in Literary Corpora\n\nReferences\n1. C. Manning and H. Sch\u00fctze, Foundations of Statistical\nNatural Language Processing, (MIT Press, Cambridge,\nMA, 1999).\n2. H. P. Luhn, The automatic creation of literature abstracts,\nIBM J. Res. Devel. 2, 159\u2013165 (1958).\n3. G. K. Zipf, Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology, (Addison-Wesley,\nCambridge, MA, 1949).\n4. G. Salton and M. J. McGill, Introduction to Modern Information Retrieval, (McGraw-Hill, New York, 1983).\n5. R. N. Mantegna, S. V. Buldyrev, A. L. Goldberger,\nS. Havlin, C.-K. Peng, M. Simons and H. E. Stanley, Systematic analysis of coding and noncoding DNA sequences\nusing methods of statistical linguistic, Phys. Rev. E 52,\n2939\u20132950 (1995).\n6. H. Stanley, S. Buldyrev, A. Goldberger, S. Havlin, C.-K.\nPeng and M. Simons, Scaling features of noncoding DNA,\nPhysica A 273, 1\u201318 (1999).\n7. I. Grosse, P. Bernaola-Galv\u00e1n, P. Carpena, R. Rom\u00e1nRold\u00e1n, J. Oliver and H. E. Stanley, Analysis of symbolic\nsequences using the Jensen-Shannon divergence, Phys.\nRev. E 65, 041905 (2002).\n8. W. Ebeling and T. P\u00f6schel, Entropy and long range correlations in literary English, Europhys. Lett. 26, 241\u2013246\n(1994).\n9. W. Ebeling, T. P\u00f6schel and K.-F. Albrecht, Entropy,\ntransinformation and word distribution of information\u2013\ncarrying sequences, Int. J. Bifurcation and Chaos 5, 51\u201361\n(1995) .\n10. M. Cassandro, P. Collet, A. Galves and C. Galves, A\nstatistical\u2013physics approach to language acquisition and\nlanguage change, Physica A 263, 427\u2013437 (1999).\n11. A. Cohen, R. N. Mantegna and S. Havlin, Numerical analysis of word frequencies in artificial and natural language\ntexts, Fractals 5, 95\u2013104 (1997).\n12. A. C.-C. Yang, C.-K. Peng, H.-W. Yien and A. L. Goldberger, Information categorization approach to literary authorship disputes, Physica A 329, 473\u2013483 (2003).\n13. R. F. i Cancho, Decoding least effort and scaling in signal\nfrequency distributions, Physica A 345, 275\u2013284 (2005).\n14. M. A. Montemurro and D. H. Zanette, Entropic analysis of\nthe role of words in literary texts, Adv. Complex Systems\n5, 7\u201317 (2002).\n15. The digital text file was obtained from Project Gutenberg:\nhttp://promo.net/pg\n16. D. Beeferman, A. Berger and J. Lafferty, A model of lexical\nattraction and repulsion, in Proceedings of the ACL-EACL\nJoint Conferences (Madrid, Spain, 1997), pp. 373\u2013380.\n17. T. Niesler and P. Woodland, Modelling word-pair relations\nin a category-based language model, in Proceedings of the\nIEEE International Conference on Acoustics, Speech and\nSignal Processing, Vol. 2 (Munich, Germany, 1997), pp.\n795\u2013798.\n18. M. Ortu\u00f1o, P. Carpena, P. Bernaola-Galv\u00e1n, E. Mu\u00f1oz\nand A. M. Somoza, Keyword detection in natural languages\nand DNA, Europhys. Lett. 57, 759\u2013764 (2002).\n19. H. Zhou and G. W. Slater, A metric to search for relevant\nwords, Physica A 329, 309\u2013327 (2003).\n20. C. E. Shannon and W. Weaver, The Mathematical Theory\nof Communication, (University of Illinois Press, Urbana,\nIllinois, 1949), reprinted with corrections from The Bell\n\n11\n\nSystem Technical Journal 27, pp. 379\u2013423, 623\u2013656, July,\nOctober, (1948).\n\n\f"}