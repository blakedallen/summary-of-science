{"id": "http://arxiv.org/abs/1011.6369v5", "guidislink": true, "updated": "2012-07-20T22:33:42Z", "updated_parsed": [2012, 7, 20, 22, 33, 42, 4, 202, 0], "published": "2010-11-29T20:54:05Z", "published_parsed": [2010, 11, 29, 20, 54, 5, 0, 333, 0], "title": "Detection of sparse additive functions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.0460%2C1011.6438%2C1011.4687%2C1011.1505%2C1011.3366%2C1011.0966%2C1011.4852%2C1011.1352%2C1011.0087%2C1011.5011%2C1011.4732%2C1011.4320%2C1011.3712%2C1011.1437%2C1011.2899%2C1011.2986%2C1011.1106%2C1011.5920%2C1011.2978%2C1011.5849%2C1011.6532%2C1011.4985%2C1011.5601%2C1011.2615%2C1011.2290%2C1011.2431%2C1011.1896%2C1011.2965%2C1011.5629%2C1011.4017%2C1011.1369%2C1011.1431%2C1011.1260%2C1011.0919%2C1011.0900%2C1011.6208%2C1011.1842%2C1011.3733%2C1011.2133%2C1011.2763%2C1011.4913%2C1011.5914%2C1011.2889%2C1011.5564%2C1011.0616%2C1011.2906%2C1011.6566%2C1011.1998%2C1011.6376%2C1011.3640%2C1011.4587%2C1011.6197%2C1011.2369%2C1011.1667%2C1011.0220%2C1011.3434%2C1011.1655%2C1011.1666%2C1011.6369%2C1011.0855%2C1011.2579%2C1011.5457%2C1011.3760%2C1011.6264%2C1011.2090%2C1011.6300%2C1011.5038%2C1011.4008%2C1011.0326%2C1011.0671%2C1011.2297%2C1011.0662%2C1011.3809%2C1011.1883%2C1011.4490%2C1011.0594%2C1011.5776%2C1011.5821%2C1011.1160%2C1011.2355%2C1011.4957%2C1011.2346%2C1011.0011%2C1011.0911%2C1011.2656%2C1011.5137%2C1011.2132%2C1011.4643%2C1011.6277%2C1011.0737%2C1011.4871%2C1011.2118%2C1011.1887%2C1011.1464%2C1011.0978%2C1011.4155%2C1011.6140%2C1011.4370%2C1011.4918%2C1011.0403%2C1011.3008&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Detection of sparse additive functions"}, "summary": "We study the problem of detection of a high-dimensional signal function in\nthe white Gaussian noise model. As well as a smoothness assumption on the\nsignal function, we assume an additive sparse condition on the latter. The\ndetection problem is expressed in terms of a nonparametric hypothesis testing\nproblem and it is solved according to the asymptotical minimax approach. The\nminimax test procedures are adaptive in the sparsity parameter for high\nsparsity case. We extend to the functional case the known results in the\ndetection of sparse high-dimensional vectors. In particular, our asymptotic\ndetection boundaries are derived from the same asymptotic relations as in the\nvector case.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1011.0460%2C1011.6438%2C1011.4687%2C1011.1505%2C1011.3366%2C1011.0966%2C1011.4852%2C1011.1352%2C1011.0087%2C1011.5011%2C1011.4732%2C1011.4320%2C1011.3712%2C1011.1437%2C1011.2899%2C1011.2986%2C1011.1106%2C1011.5920%2C1011.2978%2C1011.5849%2C1011.6532%2C1011.4985%2C1011.5601%2C1011.2615%2C1011.2290%2C1011.2431%2C1011.1896%2C1011.2965%2C1011.5629%2C1011.4017%2C1011.1369%2C1011.1431%2C1011.1260%2C1011.0919%2C1011.0900%2C1011.6208%2C1011.1842%2C1011.3733%2C1011.2133%2C1011.2763%2C1011.4913%2C1011.5914%2C1011.2889%2C1011.5564%2C1011.0616%2C1011.2906%2C1011.6566%2C1011.1998%2C1011.6376%2C1011.3640%2C1011.4587%2C1011.6197%2C1011.2369%2C1011.1667%2C1011.0220%2C1011.3434%2C1011.1655%2C1011.1666%2C1011.6369%2C1011.0855%2C1011.2579%2C1011.5457%2C1011.3760%2C1011.6264%2C1011.2090%2C1011.6300%2C1011.5038%2C1011.4008%2C1011.0326%2C1011.0671%2C1011.2297%2C1011.0662%2C1011.3809%2C1011.1883%2C1011.4490%2C1011.0594%2C1011.5776%2C1011.5821%2C1011.1160%2C1011.2355%2C1011.4957%2C1011.2346%2C1011.0011%2C1011.0911%2C1011.2656%2C1011.5137%2C1011.2132%2C1011.4643%2C1011.6277%2C1011.0737%2C1011.4871%2C1011.2118%2C1011.1887%2C1011.1464%2C1011.0978%2C1011.4155%2C1011.6140%2C1011.4370%2C1011.4918%2C1011.0403%2C1011.3008&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study the problem of detection of a high-dimensional signal function in\nthe white Gaussian noise model. As well as a smoothness assumption on the\nsignal function, we assume an additive sparse condition on the latter. The\ndetection problem is expressed in terms of a nonparametric hypothesis testing\nproblem and it is solved according to the asymptotical minimax approach. The\nminimax test procedures are adaptive in the sparsity parameter for high\nsparsity case. We extend to the functional case the known results in the\ndetection of sparse high-dimensional vectors. In particular, our asymptotic\ndetection boundaries are derived from the same asymptotic relations as in the\nvector case."}, "authors": ["Ghislaine Gayraud", "Yuri Ingster"], "author_detail": {"name": "Yuri Ingster"}, "author": "Yuri Ingster", "links": [{"href": "http://arxiv.org/abs/1011.6369v5", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1011.6369v5", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1011.6369v5", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1011.6369v5", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Detection of sparse additive functions\n\narXiv:1011.6369v5 [math.ST] 20 Jul 2012\n\nGhislaine Gayraud\u2217\nUniversit\u00e9 de Technologie de Compi\u00e8gne & CREST,\nBP 20529, 60205 Compi\u00e8gne, France\nemail: ghislaine.gayraud@utc.fr\n\nand\nYuri Ingster\u2020\nSt. Petersburg State Electrotechnical University, 5,\nProf. Popov str., 197376 St.Petersburg, Russia\nemail: yurii ingster@mail.ru\nAbstract: We study the problem of detection of high-dimensional signal functions in the\nGaussian white noise model. We assume that, in addition to a smoothness assumption, the\nsignal function has an additive sparse structure. The detection problem is expressed in terms\nof a nonparametric hypothesis testing problem and is solved using asymptotically minimax\napproach. We provide minimax test procedures that are adaptive in the sparsity parameter\nin the high sparsity case. We extend some known results related to the detection of sparse\nhigh-dimensional vectors to the functional case. In particular, our derivation of asymptotic\ndetection rates is based on same detection boundaries as in the vector case.\nAMS 2000 subject classifications: 62H15, 60G15, 62G10, 62G20, 60C20.\nKeywords and phrases: High-dimensional setting, sparsity, asymptotic minimax approach,\ndetection boundary, Gaussian white noise model.\n\nContents\n1\n2\n3\n4\n5\n\nIntroduction . . . . . . . . . . . . . . . . . . . . . . .\nDetection boundaries in a vectorial Gaussian model\nTransformation of the statistical testing problem . .\nExtremal problem . . . . . . . . . . . . . . . . . . .\nMain results . . . . . . . . . . . . . . . . . . . . . . .\n5.1 Moderate sparsity . . . . . . . . . . . . . . . .\n5.2 High sparsity . . . . . . . . . . . . . . . . . . .\n6 Extended problem . . . . . . . . . . . . . . . . . . .\n7 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . .\n7.1 Properties of test statistics . . . . . . . . . . .\n7.2 Upper bound . . . . . . . . . . . . . . . . . . .\n7.3 Lower Bound . . . . . . . . . . . . . . . . . . .\n7.4 Appendix . . . . . . . . . . . . . . . . . . . . .\n7.4.1 Proof of Lemma 7.2. . . . . . . . . . . .\n7.4.2 Proof of Lemma 7.3. . . . . . . . . . . .\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\n1\n4\n4\n6\n9\n10\n11\n12\n13\n14\n17\n21\n28\n28\n29\n31\n\n1. Introduction\nOver the past years, boosted by applications and computer performance, problems in highdimensions have been explored in a number of statistical studies. If no additional structure is\nassumed, high-dimensional data processing suffers from some intrinsic difficulties such as the curse\nof dimensionality that results in a loss in the efficiency of statistical procedures, and inconsistency\n\u2217 Ghislaine\n\u2020 Yuri\n\nGayraud's research was partially supported by the ANR-blanc 'SP Bayes'\nIngster's research was partially supported by RFBR Grant 11-01-00577 and by Grant NSh\u20134472.2010.1\n\n1\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n2\n\nof classical statistical procedures \u2013 even in the linear regression model \u2013 unless the dimension of\nvariables is less than the sample size.\nIn order to overcome the curse of dimensionality in a nonparametric framework, where typical\nfunctional classes are Sobolev, Holder, or Besov balls, some additional conditions, including\nadditivity or tensor product structure, are assumed, see, for instance, [20, 6, 18, 14, 15, 16] and\nreferences therein. Even if one of these conditions is assumed, yet it is required that the sample\nsize is to be larger than the data dimension. One way to free oneself from the latter condition is\nto impose an additional sparsity constraint.\nIn this paper we focus on the problem of detection of high-dimensional signal functions in\nthe Gaussian white noise model. To avoid difficulties stemming from high-dimensional settings,\nwe suppose that a signal function satisfies an additional structural condition. Specifically, it is\nassumed to be sparse additive. This means that a high-dimensional function of interest is a sum\nof few univariate functions. Formally, we consider an d-dimensional (d \u2208 IN and d > 0) Gaussian\nwhite noise model\ndX(t) = f (t)dt + \u01ebdW (t), t \u2208 [0, 1]d,\n\n(1.1)\n\nwhere W (t) is the Wiener process, \u01eb > 0 is the noise level, and f , the quantity of interest, is the\nsignal function. The additive sparse structure means that f is the sum of d univariate functions\nfj :\nf (t) =\n\nd\nX\n\n\u03bej fj (tj ),\n\nj=1\n\ntj \u2208 [0, 1],\n\n(1.2)\n\nwhere the \u03bej 's are unknown but deterministic taking their values in {0, 1} : \"0\" means that the\njth component fj is non active whereas \"1\" means that fj is active. Denote by K the positive\nd\nX\n\u03bej , and assume that K = d1\u2212b , where b \u2208 (0, 1) is\nnumber of active components, that is, K =\nj=1\n\nthe sparsity index. If d1\u2212b is not an integer then take K as its integer part. Denote by Fd,b the\nfunctional class of additive sparse signals f of the form (1.2) with K = d1\u2212b active components\nand db non-active components. Model (1.1) with the sparse additive structure (1.2) is a natural\ngeneralization of the sparse linear model: the nonparametric nature of the problem suggests to\nconsider more flexible models.\nThere is a huge statistical literature on estimation in sparse models, see, for instance, [1, 2, 3]\nand references therein. In particular, there are many works related to the well-known Lasso introduced by Tibshirani [21] in 1996. There are also a number of papers that deal with nonparametric\nestimation in sparse additive models. For a complete review of these topics, we refer to [19], where\nminimax estimation rates in sparse additive models are obtained, to [5], where the Lasso-type\nestimate in sparse additive models is studied, and to [20], where various structural assumptions\non models in high dimensions are discussed.\nBack to our study, the detection problem at hand can be expressed in terms of a nonparametric\nhypothesis testing problem with the null hypothesis stating that \"the signal is a constant\", and\n\"there is no signal\" being a particular case of the null hypothesis. In order to specify an alternative\nhypothesis, recall that, within the minimax framework, it is impossible to detect signal functions\nthat are \"too close\" to the null one, as well as to test the null and alternative hypotheses for too\nlarge alternative classes. Therefore, we are interested in the following nonparametric hypothesis\ntesting problem:\nH0 : f = const0\nwhere\n\nversus\n\nH1 : f = const1 + f 1 , f 1 \u2208 Fd (\u03c4, r\u01eb , b),\n\n\uf8f1\n\uf8f2 const0 , const1 are some constants,\nFd (\u03c4, r\u01eb , b) ={f \u2208 Fd,b : \u2200j, fj \u2208 S\u0303\u03c4 and kfj k2 \u2265 r\u01eb }, \u03c4 > 0, r\u01eb > 0,\nR1\n\uf8f3\n(\u03c4 )\nS\u0303\u03c4 = {f \u2208 L2 ([0, 1]) : 0 f (t)dt = 0, kf k2 \u2264 1}.\n\n(1.3)\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n3\n\nThe L2 -norm k * k2 is used to separate the nonparametric alternative from the null hypothesis.\n(\u03c4 )\nThe functional class S\u0303\u03c4 is the Sobolev ball, expressed via the Sobolev semi-norm k * k2 , that\ncontains \u03c4 -smooth functions, which are assumed 1-periodic and orthogonal to a constant. Due to\n(\u03c4 )\nthe periodic constraints, it is possible to express k * k2 in terms of Fourier coefficients; this will\nbe done in Section 2. The quantity \u03c4 is the smoothness parameter. Both the smoothness condition\nand the separation condition between H0 and H1 are expressed in terms of the components fj that\nare linked to the whole signal f via (1.2): each active component fj is smooth and is separated\nfrom the null hypothesis in the L2 -norm by a positive value r\u01eb .\nIn Section 6, we generalize the hypothesis testing problem (1.3) by considering a more general\nclass of alternatives that consists of signals f equal, up to a constant, to a function f 1 \u2208 Fd,b , which\nis separated from the null hypothesis in the L2 ([0, 1]K )-norm, and whose smoothness is expressed\nin terms of the whole function f .\nFor these two hypothesis testing problems, the main questions are: what are the separation rates\nin the problem, i.e., what are the asymptotics for the minimal r\u01eb such that one can distinguish\nbetween H0 and H1 ? And, also, what are the optimal test procedures that provide distinguishability?\nTo answer these questions, we use asymptotically minimax approach that provides detection\nboundaries or distinguishability conditions, i.e., necessary and sufficient conditions for the possibility of successful detection; these detection boundaries yield asymptotics for the minimal r\u01eb\nseparating the areas of distinguishability and non-distinguishability (between H0 and H1 ). The\nasymptotics for the minimal values of r\u01eb are called either the (minimax) separation rates or the\nminimax rates of testing; in the present paper, the separation rates are denoted by r\u01eb\u22c6 .\nIn connection with the current study, a number of works on detection and classification boundaries in Gaussian sequence models could be mentioned, see, for example,\n[7, 8, 9, 13, 12, 4, 15, 16, 11]. Also, in [17], rather than considering a Gaussian sequence\nmodel, the authors generalize the problem of finding a detection boundary in the linear regression\nmodel. Another paper [10] deals with the signal detection problem in a multichannel model in\nthe functional framework. At the end of the next paragraph, we explain what are the differences\nbetween the results in [10] and our study.\nThe main contribution of this paper consists of extending the results on detection boundaries\nobtained for d-dimensional sparse Gaussian vectors, see, for instance, [12], to the functional case.\nIn particular, we obtain the same detection boundaries as in the vectorial case. However, in the\ncase of high sparsity when b > 1/2, an additional assumption on the growth of d as a function\nof \u01eb is required. Distinguishability is possible when the sum of the type I error probability and\nthe maximum over alternatives of the type II error probability vanishes asymptotically, and distinguishability is not possible when this sum tends to one. Boundary conditions depend on the\nquantity a(r\u01eb ) = a(r\u01eb , d, \u03c4 ), which is a solution of a certain extremal problem stated in Section 4.\nIn the vectorial case, the quantity a(r\u01eb ) corresponds to the energy of a signal (see [12] and [10]). In\nthe functional case, this quantity characterizes the distinguishability in a one-variable hypotheses\ntesting problem. The minimax separation rates obtained in this paper depend on the value of b:\nfor large b they are worse than for small b. Such a behaviour is expected because, with large b,\nonly few components are active, and hence the problem of distinguishing between the alternative\nand null hypothesis becomes more difficult.\nFor the most difficult case of b \u2208 (1/2, 1), not only separation rates, but also sharp separation\nrates, that include both rates and constants, are obtained. We also provide optimal test procedures\nfor which minimax rates of testing are achieved asymptotically. Depending on the value of b, we\npropose two types of test procedures: one is of a \u03c72 type, the other one is related to a HigherCriticism statistic introduced in [4] and based on the Tukey's ideas. In the case of b \u2208 (1/2, 1), our\ntest procedure is adaptive in the sparsity index b, see Remark 5.3.\nIn the paper [10], which is focused on a similar problem of multichannel signal detection, the\noptimal rates are obtained. In our study, we obtain sharp separation rates for b \u2208 (1/2, 1). The\nmain difference between the study of [10] and our work is in the quantity a(r\u01eb ) that characterizes\nthe distinguishability: in our work, it is just a solution of a certain extremal problem, whereas in\n[10], it is obtained directly from the use of the respective test procedures.\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n4\n\nThe rest of the paper is organized as follows. Section 2 is concerned with the problem of finding\ndetection boundary in a sparse Gaussian d-vectors model. In Section 3, we give a new formulation\nof the problem (1.3) in terms of sequence spaces. Section 4 is devoted to the description of the\nextremal problem that gives the distinguishability characteristics. The main results are stated in\nSection 5. In Section 6, we generalize the hypothesis testing problem (1.3) by considering more\ngeneral alternatives. The proofs are given in Section 7.\n2. Detection boundaries in a vectorial Gaussian model\nHypothesis testing problems for d-dimensional vectors, under the sparse conditions similar to the\nones we use, were studied in [7, 12, 4]. Namely, let X = (X1 , . . . , Xd ) be a random vector of the\ni.i.d.\nform Xj = vj + \u03b7j , where \u03b7j \u223c N (0, 1), j = 1, . . . , d, and\nvj = \u03bej a,\n\na > 0,\n\n\u03bej \u2208 {0, 1},\n\nK=\n\nd\nX\n\n\u03bej = d1\u2212b ,\n\nj=1\n\nb \u2208 (0, 1).\n\n(2.1)\n\nLet Vd (a, b) \u2282 Rd be the set of all vectors v = (v1 , . . . , vd ) of the form (2.1). Then, the testing\nproblem is stated as follows: it is required to test H0 : v = 0 against the alternative H1 : v \u2208 Vd (a, b).\nHere the questions of interest are: what are the asymptotics for a = ad as d \u2192 +\u221e for which the\nhypotheses H0 and H1 separate asymptotically? Also, what are the optimal test procedures that\nprovide the distinguishability (or separation) of H0 and H1 ?\nThe answer to each question depends essentially on the sparsity index b \u2208 (0, 1), see [7, 12, 4]. The\ndetection boundaries are expressed in terms of a, d and b: if b \u2264 1/2 (moderate sparsity), then the\ndistinguishability is impossible when ad1/2\u2212b = o(1), and it is possible when ad1/2\u2212b \u2192 +\u221e. This is\nP\nachieved by the test procedure based on a simple linear statistic t = d\u22121/2 di=1 Xi . If b > 1/2 (high\nsparsity), then the distinguishability conditions look as follows: the distinguishability p\nis impossible\nwhen lim sup a/Td < \u03c6(b), and it is possible when lim inf a/Td > \u03c6(b), where Td = log(d) and\nthe function \u03c6(b), b \u2208 (1/2, 1) is defined by\n(\n\u221a\n1/2 < b \u2264 3/4,\n\u03c61 (b) = 2b \u2212 1,\n\u221a\n\u221a\n(2.2)\n\u03c6(b) =\n\u03c62 (b) = 2(1 \u2212 1 \u2212 b), 3/4 < b < 1.\nObserve that the function \u03c6 is positive, continuous, and increasing in b \u2208 (0, 1].\nThe test procedure that provides distinguishability in the high-sparsity case is based on the\nHigher-Criticism statistics introduced in [4]. It is defined as Ld = maxLd (s), for any s0 > 0, with\ns>s0\n\nd\n\nX\n1\nLd (s) = p\n(1I(Xi >s) \u2212 \u03a6(\u2212s)),\nd \u03a6(s)\u03a6(\u2212s) i=1\n\n(2.3)\n\nwhere, here and later, \u03a6 stands for the standard Gaussian cumulative distribution function. Note\nthat it suffices to take the\u221amaximum of Ld over a discrete grid of the form sl = ul Td , ul = \u03b4d l, l =\n1, . . . , L, such that uL \u2264 2 and \u03b4d = o(1) is small enough.\n3. Transformation of the statistical testing problem\nConsider the tensor structure of the space L2 ([0, 1]d ), i.e., L2 ([0, 1]d ) = L2 ([0, 1]) \u2297 . . . \u2297 L2 ([0, 1]).\nThen, the corresponding orthonormal basis (\u03c6\u0303dl )l\u2208ZZd of L2 ([0, 1]d ) has the form\n\u03c6\u0303dl (t) =\n\nd\nY\n\nj=1\n\nZd ,\n\u03c61lj (tj ), t = (t1 , . . . , td ) \u2208 [0, 1]d, l = (l1 , . . . , ld ) \u2208 Z\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n5\n\nwhere (\u03c61k )k\u2208ZZ is an orthonormal basis of L2 ([0, 1]). It is assumed that \u03c610 = 1. For any (j, k) \u2208\n{1, . . . , d} \u00d7 Z\nZ, let us define \u03c6\u0304dj,k as\n\u03c6\u0304dj,k (t) = \u03c6\u0303dl (t) = \u03c61k (tj ), l = (0, . . . , k, 0, . . . , 0),\nwhere k is the j-th component of l. Observe that \u03c6\u0304dj,0 = 1. Using the orthonormal system\n(\u03c6\u0304dj,k )(j,k)\u2208{1,...,d}\u00d7ZZ , consider the statistics (xj )1\u2264j\u2264d = {xj,k ; k \u2208 Z\nZ}1\u2264j\u2264d defined by\nxj,k\n\n=\n\nZ\n\n[0,1]d\n\n=\n\n\u03bej\n\n\u03c6\u0304dj,k (t)dX(t)\n\nZ\n\n[0,1]\n\n\u03c61k (tj )fj (tj )dtj + \u01eb\u03b7j,k\n\n= \u03bej \u03b8j,k + \u01eb\u03b7j,k ,\n(3.1)\nR\nwhere the random variables \u03b7j,k = [0,1]d \u03c6\u0304dj,k (t)dW (t) are i.i.d. real standard Gaussian random\nR\nvariables and \u03b8j,k = [0,1] \u03c61k (tj )fj (tj )dtj . Set \u03b8j = (\u03b8j,k )k\u2208ZZ and \u03b8 = (\u03b8j )1\u2264j\u2264d .\nThanks to the periodic constraints, we may consider (\u03c61k )k\u2208ZZ as the standard Fourier basis.\nThen the Sobolev X\nsemi-norm of fj can be expressed in terms of its Fourier coefficients as follows:\n(\u03c4 )\n2 1/2\nkfj k2 = ((2\u03c0)2\u03c4\n|k|2\u03c4 \u03b8j,k\n) . Therefore, the functional class Fd (\u03c4, r\u01eb , b) can be equivalently\nk\u2208Z\nZ\n\nrepresented as the sequence space \u0398d (\u03c4, r\u01eb , b):\n\u0398d (\u03c4, r\u01eb , b) = {\u03b8 = (\u03b81 \u03be1 , . . . , \u03b8d \u03bed ) :\n\nd\nX\nj=1\n\nwhere\n\u0398(\u03c4, r\u01eb ) = {\u03b8 \u2208 l2 (Z\nZ) : (2\u03c0)2\u03c4\n\n\u03bej = d1\u2212b ; \u2200j \u2208 {1, . . . , d}, \u03b8j \u2208 \u0398(\u03c4, r\u01eb )},\nX\n\nk\u2208Z\nZ\n\n|k|2\u03c4 \u03b8k2 \u2264 1;\n\nX\n\nk\u2208Z\nZ\n\n\u03b8k2 \u2265 r\u01eb2 }.\n\nThe testing problem of interest (1.3) can be rewritten in the form\nH0 : \u03b8 = 0\n\nversus\n\nH1 : \u03b8 \u2208 \u0398d (\u03c4, r\u01eb , b).\n\nDenote by IP0 and IP\u03b8 the distributions under the null and alternative hypotheses, respectively.\nAlso, denote by IE0 , Var0 , IE\u03b8 , and Var\u03b8 the expectations and variances with respect to IP0 and IP\u03b8 ,\nrespectively. The notation IP\u03b8j , IE\u03b8j and Var\u03b8j also will be used: they are related to the distribution\nof the observations xj = (xj,k )k\u2208ZZ .\nFor any test procedure \u03c8, that is, for any function measurable with respect to the observations\nand taking its values on the interval [0, 1], let \u03c9(\u03c8) = IE0 (\u03c8) be the type I error probability and\nIE\u03b8 (1 \u2212 \u03c8) be the maximal type II error probability over the set\nlet \u03b2(\u03c8, \u0398d (\u03c4, r\u01eb , b)) =\nsup\n\u03b8\u2208\u0398d (\u03c4,r\u01eb ,b)\n\n\u0398d (\u03c4, r\u01eb , b). Also, consider the total error probability \u03b3(\u03c8, \u0398d (\u03c4, r\u01eb , b)) = \u03c9(\u03c8) + \u03b2(\u03c8, \u0398d (\u03c4, r\u01eb , b)),\nand denote by \u03b3 or \u03b3(\u0398d (\u03c4, r\u01eb , b)) the minimax total error probability over \u0398d (\u03c4, r\u01eb , b), that is,\n\u03b3\n\n=\n\n\u03b3(\u0398d (\u03c4, r\u01eb , b)) = inf \u03b3(\u03c8, \u0398d (\u03c4, r\u01eb , b)),\n\u03c8\n\n(3.2)\n\nwhere the infimum is taken over all test procedures. One can not distinguish between H0 and H1\nif \u03b3 \u2192 1, and distinguishability occurs if it exists \u03c8 such that either \u03b3(\u03c8, \u0398d (\u03c4, r\u01eb , b)) \u2192 0 or\n\u03b2(\u03c8, \u0398d (\u03c4, r\u01eb , b)) = o(1) once \u03c8 has a given asymptotic level.\nThe aim of this paper is to provide separation rates for the alternatives \u0398d (\u03c4, r\u01eb , b) and to\ndetermine statistical procedures \u03c8 and/or \u03c8\u03b1 asymptotically of level \u03b1, i.e., \u03c9(\u03c8\u03b1 ) \u2264 \u03b1 + o(1), for\nwhich these separation rates are achieved.\nBy the separation rates we mean a family r\u01eb\u22c6 such that\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n\uf8f1\n\u03b3\u21921\n\uf8f4\n\uf8f4\n\uf8f2\n\n6\n\nif\n\n\uf8f4\n\uf8f4\n\uf8f3 \u03b3(\u03c8, \u0398d (\u03c4, \u01eb, b)) \u2192 0,\n\nand/or\n\n\u2200 \u03b1 \u2208 (0, 1) \u03b2(\u03c8\u03b1 , \u0398d (\u03c4, r\u01eb , b)) \u2192 0 if\n\nr\u01eb\n\u2192 0,\nr\u01eb\u22c6\nr\u01eb\n\u2192 +\u221e.\nr\u01eb\u22c6\n\nBy the sharp separation rates, we mean a family r\u01eb\u22c6 such that\n\uf8f1\n\u03b3\u21921\n\uf8f4\n\uf8f4\n\uf8f2\n\n\uf8f4\n\uf8f4\n\uf8f3 \u03b3(\u03c8, \u0398d (\u03c4, r\u01eb , b)) \u2192 0,\n\nand/or\n\n\u2200 \u03b1 \u2208 (0, 1) \u03b2(\u03c8\u03b1 , \u0398d (\u03c4, r\u01eb , b)) \u2192 0\n\nif\n\nlim sup\n\nif\n\nlim inf\n\nr\u01eb\n< 1,\nr\u01eb\u22c6\n\nr\u01eb\n> 1.\nr\u01eb\u22c6\n\nTypically, asymptotics for models like model (1.1) are given as \u01eb \u2192 0. However, we are mainly\ninterested in high-dimensional settings when d \u2192 +\u221e. Therefore, here and later, asymptotics and\nsymbols o, O, \u223c and \u224d are used when \u01eb \u2192 0 and d \u2192 +\u221e, except for the cases when it is explicitly\n\u2206\nspecified, say, od is used when d \u2192 +\u221e. The notation A = B means that we use notation A for\nquantity B.\n4. Extremal problem\nIn this section, we explain what is the quantity a(r\u01eb ) that corresponds to the energy of a signal in\nthe vectorial case. Only in this section, we assume that the observations have the form xk = \u03b8k +\u01eb\u03b7k\nfor k \u2208 Z\nZ, where the \u03b7k 's are i.i.d. real standard Gaussian random variables. The quantity a(r\u01eb )\ndenotes the solution of the extremal problem\n\u001a\nP\n2\u03c4\n2\u03c4 2\nX\n1\n(2\u03c0)\nk\u2208Z\nZ |k| \u03b8k \u2264 1\nP\n\u03b8k4 subject to\n(4.1)\na2 (r\u01eb ) = 4 inf\n2\n2\n2\u01eb \u03b8\u2208l2 (ZZ)\nk\u2208Z\nZ \u03b8k \u2265 r\u01eb\nk\u2208Z\nZ\n\nand characterizes distinguishability in the minimax detection problem for one-variable functions\nlying inX\nS\u0303\u03c4 and separated from the null hypothesis in L2 by positive values r\u01eb , i.e., for t \u2208 [0, 1],\nf (t) =\n\u03b8k \u03c61k (t) with f \u2208 S\u0303\u03c4 and kf k2 \u2265 r\u01eb .\nk\u2208Z\nZ\n\nNamely, if a(r\u01eb ) \u2192 0 then the minimax total error probability \u03b3(\u0398(\u03c4, r\u01eb )) \u2192 1, and if a(r\u01eb ) \u2192\n+\u221e, then \u03b3(\u0398(\u03c4, r\u01eb )) \u2192 0.\n\u2206\n\nFurthermore, let \u03b8\u22c6 = \u03b8\u22c6 (r\u01eb ) be a sequence in l2 (Z\nZ) that provides solution to the extremal\nproblem (4.1). Set\nwk (r\u01eb )\n\n=\n\n1 (\u03b8k\u22c6 (r\u01eb ))2\n, k\u2208Z\nZ.\n2 a(r\u01eb )\u01eb2\n\n(4.2)\n\nsup wk (r\u01eb ) = o(1).\n\n(4.3)\n\nSuppose that\na(r\u01eb ) \u224d 1,\n\nk\u2208Z\nZ\n\nThen, we get the sharp asymptotics\n\u03b3(\u0398(\u03c4, r\u01eb )) = 2\u03a6(\u2212a(r\u01eb )/2) + o(1).\nFor the reader's convenience, we give a sketch of the proofs of these results. The proofs are based\non the methods and results of Sections 3.1, 3.3, 4.3 in [13]. In the vectorial case in hand, we also\ndescribe the structure of asymptotically minimax tests.\nIn order to obtain lower bounds, we consider the Bayesian hypothesis testing problem\nwith\nY\nthe product prior distribution on \u03b8, using the symmetric two-point factors: \u03c0 =\n\u03c0k , \u03c0k =\nk\u2208Z\nZ\n\n1\n(\u03b4\u2212\u03b8k + \u03b4\u03b8k ) for \u03b8 \u2208 \u0398(\u03c4, r\u01eb ), and \u03b4 is the Dirac mass. Let IP\u03c0 be the mixture of measures IP\u03b8\n2\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n7\n\nover \u03c0. Observe that\nY dIP\u03c0\nY\ndIP\u03c0\nk\n((xk )k\u2208ZZ ) =\n(xk ) =\nexp(\u2212\u03b8k2 /2\u01eb2 ) cosh(xk \u03b8k /\u01eb2 ).\ndIP0\ndIP0\nk\u2208Z\nZ\n\nFor the sake of simplicity, denote\nProposition 2.12 in [13],\n\nk\u2208Z\nZ\n\ndIP\u03c0 \u2206 dIP\u03c0\n((xk )k\u2208ZZ ). Since \u03c0(\u0398(\u03c4, r\u01eb )) = 1, we have, see\n=\ndIP0\ndIP0\n\n1\n1\n1\n\u03b3(\u0398(\u03c4, r\u01eb )) \u2265 1\u2212 IE0 |dIP\u03c0 /dIP0 \u22121| \u2265 1\u2212 (IE0 (dIP\u03c0 /dIP0 \u22121)2 )1/2 = 1\u2212 ((IE0 (dIP\u03c0 /dIP0 )2 )\u22121)1/2 .\n2\n2\n2\nThis yields \u03b3(\u0398(\u03c4, r\u01eb )) \u2192 1 as soon as IE0 (dIP\u03c0 /dIP0 )2 \u2192 1. Simple calculations and the inequality\ncosh(x) \u2264 exp(x2 /2) give\n!\nY\nY\n1 X 4\n2\n2\n2\n\u03b8k .\nIE0 (dIP\u03c0 /dIP0 ) =\nIE0 (dIP\u03c0k /dIP0 ) =\ncosh((\u03b8k /\u01eb) ) \u2264 exp\n2\u01eb4\nk\u2208Z\nZ\n\nk\u2208Z\nZ\n\nk\u2208Z\nZ\n\nTherefore, providing the \"asymptotically least favorable prior\" of the type under consideration\nleads to the problem (4.1).\nUnder assumption (4.3), taking the prior based on the extremal sequence in the problem (4.1),\none can show that the Bayesian log-likelihood ratio is asymptotically Gaussian:\n\u0013\nX \u0012 (\u03b8\u22c6 )2\nlog(dIP\u03c0 /dIP0 ) =\n\u2212 k 2 + log(cosh(xk \u03b8k\u22c6 /\u01eb2 )) = \u2212a2 (r\u01eb )/2 + a(r\u01eb )\u03b7\u01eb + \u03c1\u01eb ,\n2\u01eb\nk\u2208Z\nZ\n\nwhere \u03b7\u01eb \u2192 \u03b7 \u223c N (0, 1) and \u03c1\u01eb \u2192 0 in IP0 -probability. The proof is based on Taylor's expansion,\nsee Section 4.3.1 of [13]. This yields the sharp lower bounds.\nP 2\nIn order to obtain upper bounds, take a sequence q = (qk )k\u2208ZZ such that qk \u2265 0,\nk qk = 1/2,\nand consider tq , a centered and normalized (under IP0 ) statistic of a weighted \u03c72 -type:\n\u0011\nX \u0010 xk\ntq =\nqk ( )2 \u2212 1 .\n\u01eb\nk\u2208Z\nZ\n\nConsider also the test procedures \u03c8H,q = 1Itq >H . Observe that IE0 tq = 0, Var0 tq = 1, and tq are\nasymptotically standard Gaussian under IP0 . These observations imply w(\u03c8H,q ) = \u03a6(\u2212H) + o(1).\nDenote by \u03ba(\u03b8, q) and \u03ba(q) the following functions:\nX\n\u03ba(\u03b8, q) =\nqk \u03b8k2 , \u03ba(q) = \u03ba(\u0398(\u03c4, r\u01eb ), q) = inf \u03ba(\u03b8, q).\n(4.4)\n\u03b8\u2208\u0398(\u03c4,r\u01eb )\n\nk\u2208Z\nZ\n\nThen,\nIE\u03b8 tq = \u01eb\u22122 \u03ba(\u03b8, q),\n\nVar\u03b8 tq = 1 + 4\u01eb\u22122\n\nX\n\nqk2 \u03b8k2 = 1 + O((max qk )IE\u03b8 tq ),\nk\n\nk\n\nand hence, by Chebyshev's inequality, \u03b2(\u03c8H,q , \u0398(\u03c4, r\u01eb )) \u2192 0 when \u01eb\u22122 \u03ba(q) \u2192 +\u221e and H \u2264\nc\u01eb\u22122 \u03ba(q), c \u2208 (0, 1). Under assumption (4.3), one can check that the statistic t\u0302q = tq \u2212 IE\u03b8 tq is\nasymptotically standard Gaussian under IP\u03b8 such that IE\u03b8 tq = O(1). Therefore\n\u03b2(\u03c8H,q , \u0398(\u03c4, r\u01eb )) \u2264 \u03a6(H \u2212 \u01eb\u22122 \u03ba(q)) + o(1).\nIn order to determine \"asymptotically the best sequence\" (qk )k\u2208ZZ , it suffices to find a solution of\nthe following maximin problem:\n\u00e3(r\u01eb ) = \u01eb\u22122 P\n\nsup\nk\n\n\u03ba(q).\n\n(4.5)\n\n2\nqk\n=1/2,qk \u22650\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n8\n\n\u221a\n\u221a\nFirst, we change the variables for v = (vk )k\u2208ZZ and (pk )k\u2208ZZ , where vk = \u03b8k2 / 2, pk = 2qk . Then,\nby convexity of the set\nX\nX\nV + = {v \u2208 l1 (Z\nZ) : vk \u2265 0; (2\u03c0)2\u03c4\nk 2\u03c4 vk \u2264 2\u22121/2 ;\nvk \u2265 2\u22121/2 r\u01eb2 },\n(4.6)\nk\u2208Z\nZ\n\nk\u2208Z\nZ\n\nand using the minimax theorem, we get\n\u00e3(r\u01eb ) =\n=\n=\n\n\u01eb\u22122 P\n\nsup\n\nk\n\np2k =1,pk \u22650 v\u2208V\n\n\u01eb\u22122 inf+ P\nv\u2208V\n\n1\n\u221a\n2 \u01eb2\n\ninf\n\nsup\n\nk\n\n+\n\nX\nk\n\nX\n\np2k \u22641,pk \u22650 k\n\ninf\n\n\u03b8\u2208\u0398(\u03c4,r\u01eb )\n\npk vk = \u01eb\u22122 P\n\nsup\n\nk\n\ninf\n\np2k \u22641,pk \u22650 v\u2208V\n\nX\npk vk = \u01eb\u22122 inf+ (\nvk2 )1/2\nv\u2208V\n\n+\n\nX\n\npk vk\n\nk\n\nk\n\nX\n(\n\u03b8k4 )1/2 = a(r\u01eb ).\nk\n\n\u2206\n\nThus, asymptotically the best sequence (qk )k\u2208ZZ is the sequence w(r\u01eb ) = (wk (r\u01eb ))k\u2208ZZ of the form\n(4.2), and the value of the problem (4.5) coincides with the value of the problem (4.1). Setting\nH = a(r\u01eb )/2, we get the upper bounds and the structure of asymptotically minimax tests.\nNote that the above evaluations entail (see also Proposition 4.1 in [13]) that\n1\n\ninf\n\n\u03b8\u2208\u0398(\u03c4,r\u01eb ) \u01eb2\n\nMoreover if (\n\nX\n\n\u03ba(\u03b8, w(r\u01eb )) \u2265 a(r\u01eb ).\n\n(4.7)\n\n\u03b8k2 )1/2 is larger than r\u01eb , then \u03ba(\u03b8, w(r\u01eb )) becomes rather large. Namely, let us\n\nk\u2208Z\nZ\n\ndenote\n\n\u03ba(r\u01eb , B) =\n\ninf\n\n\u03b8\u2208\u0398(\u03c4,Br\u01eb )\n\n\u03ba(\u03b8, w(r\u01eb )), B > 0\n\nProposition 4.1. Let B \u2265 1, then\n1\n\u03ba(r\u01eb , B) \u2265 B 2 a(r\u01eb ).\n\u01eb2\nProof of Proposition 4.1.\nP\nSet \u0398(\u03c4, A, r\u01eb ) = {\u03b8 \u2208 l2 (Z\nZ) : (2\u03c0)2\u03c4 k\u2208ZZ |k|2\u03c4 \u03b8k2 \u2264 A2 ,\n\u0398(\u03c4, Br\u01eb ) \u2282 \u0398(\u03c4, B, Br\u01eb ), we have\ninf\n\n\u03b8\u2208\u0398(\u03c4,Br\u01eb )\n\n\u03ba(\u03b8, w(r\u01eb )) \u2265\n\ninf\n\n\u03b8\u2208\u0398(\u03c4,B,Br\u01eb )\n\n\u03ba(\u03b8, w(r\u01eb )) = B 2\n\ninf\n\n\u03b8\u2208\u0398(\u03c4,r\u01eb )\n\nP\n\n2\nk\u2208Z\nZ \u03b8k\n\n\u2265 r\u01eb2 }, A > 0. Since\n\n\u03ba(\u03b8, w(r\u01eb )) \u2265 B 2 \u01eb2 a(r\u01eb ),\n\nwhere the last inequality follows from (4.7). This completes the proof.\nThe solution of the extremal problem (4.1) is obtained in Ingster and Suslina [13], Section\n4.3. Adapting the derivations on pages 146\u2013147 of Section 4.3.2. in [13] to our case, we set c3 =\n1\n1\n1\n1\nB(a, b), c2 =\nB(b, c) and c0 =\nB(a, d), where B(*, *) is the Euler Beta function, a =\n,\n4\u03c4\n4\u03c4\n8\u03c4\n2\u03c4\n1\n, c = 2 and d = 3.\nb=1+\n2\u03c4\nLemma 4.1. The solution of the extremal problem (4.1) is given by\na(r\u01eb ) \u223c (c1 (\u03c4 ))1/2 r\u01eb2+1/(2\u03c4 ) \u01eb\u22122 as r\u01eb \u2192 0,\nwhere\n\nc1 (\u03c4 )\n\n= c0 \u03c0c\u22122\n2 (\n\nc2 (4\u03c4 +1)/2\u03c4\n)\nis a positive constant.\nc3\n\n(4.8)\n(4.9)\n\nRemark 4.1. One must note that r\u01eb \u2192 0 is the only condition we need to obtain the asymptotic\nsolution of (4.1). In particular, it is not required that \u01eb \u2192 0 and Lemma 4.1 is valid whatever the\nvalue of \u01eb > 0 is.\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n9\n\nSketch of proof of Lemma 4.1.\n\u221a\nFollowing Chapter 4 in [13], observe that by setting vk = \u03b8k2 / 2 for all k \u2208 Z\nZ, one can transform\nthe minimization problem under constraints (4.1) into the following one:\nX\nv\u01eb2 =\ninf +\nvk2 ,\n(vk )k\u2208Z\nZ \u2208V\n\nk\u2208Z\nZ\n\nwhere V + is defined by equation (4.6). The space l1+ (Z\nZ) contains non-negative sequences lying\nin l1 (Z\nZ). Note that v\u01eb2 = \u01eb4 a2 (r\u01eb ). The convexity of the set V + assures the uniqueness of v\u01eb2 . In\norder to determine the solution, rewrite as in Section 4.3. in [13] the sequence (vk )k\u2208ZZ as follows:\nvk = v0 \u03b6(k/m), where \u03b6(y) = (1 \u2212 |y|2\u03c4 )1I(|y|\u22641) and m > 0. By using the Lagrange multipliers\nrule, it is possible to obtain the following relations, as r\u01eb \u2192 0 and m \u2192 +\u221e:\nc3 v0 m \u223c 2\u22121/2 r\u01eb2 , v\u01eb2 \u223c c0 v02 m, c2 v0 m2\u03c4 +1 \u223c 2\u22121/2 (2\u03c0)\u22122\u03c4 ,\n\n(4.10)\n\n4+1/\u03c4\n\n4+1/\u03c4\n\nwhich entail the existence of v\u01eb2 satisfying v\u01eb2 \u223c c1 (\u03c4 )r\u01eb\n, and thus a2 (r\u01eb ) \u223c c1 (\u03c4 )\u01eb\u22124 r\u01eb\nIf r\u01eb \u2192 0, then the first and second relations in (4.10) entail that\nv0 \u224d v\u01eb2 r\u01eb\u22122 \u224d r\u01eb2+1/\u03c4 ,\n\n.\n\n(4.11)\n\u22121/(2\u03c4 +1)\n\nwhich implies that m \u2192 +\u221e since the third relation in (4.10) yields m \u224d v0\n\n\u22121/\u03c4\n\n\u224d r\u01eb\n\n.\n\nRemark 4.2. The form of function \u03b6 and relation (4.11) imply that sup vk \u2264 v0 = o(1).\nk\n\n5. Main results\nDepending on the values of b, we distinguish between two types of sparsity: the moderate sparsity\ncase with b \u2208 (0, 1/2] and the high sparsity case with b \u2208 (1/2, 1). In each case, although being\nof different types, the \"best\" test procedures that achieve the separation rates are based on the\n\u03c72 -type statistics (tj )1\u2264j\u2264d determined in the same way as the \"best statistic\" tq of a weighted\n\u03c72 -type in Section 4.\nLet us introduce a general version of the \u03c72 -type statistics of interest. For j in {1, . . . , d}, put\n\u0010 x\n\u0011\nX\nj,k 2\ntj =\nwk (\n) \u22121 ,\n(5.1)\n\u01eb\nk\u2208Z\nZ\n\nwhere (wk )k\u2208ZZ is the sequence of weights such that wk \u2265 0 for all k in Z\nZ and\nalso\n\u0010 x\n\u0011\nj,k 2\n) \u22121 ,\ntj,k = wk (\n\u01eb\nX\nso that tj =\ntj,k .\n\nP\n\nk\u2208Z\nZ\n\nwk2 = 12 . Set\n(5.2)\n\nk\u2208Z\nZ\n\u221a\n\u221a\nRecall that Td = log d (see Section ??). Similarly to (2.3) and for any u \u2208 (0, 2], let us define\nthe statistics L(u) on which the Higher-Criticism type test procedure is built:\n\nL(u) = Cu\n\nd\nX\nj=1\n\n(1I(tj >uTd ) \u2212 \u03a6\u03030 (uTd )),\n\n(5.3)\n\nwhere\n\u03a6\u03030 (x)\n\n=\n\nIP0 (tj > x)\n\n(5.4)\n\nCu\n\n=\n\n(d\u03a6\u03030 (uTd )(1 \u2212 \u03a6\u03030 (uTd )))\u22121/2 .\n\n(5.5)\n\nTaking into account the sparsity condition, we consider a particular sequence of weights\n\u2206\n(wk (r\u01eb\u22c6 ))k\u2208ZZ defined by equation (4.2) with r\u01eb\u22c6 = r\u01eb\u22c6 (b) being the separations rates depending\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n10\n\non b in (0, 1). Then, for all j \u2208 {1, . . . , d}, we consider the statistics tj,b as in (5.1) with the weight\nsequence (wk (r\u01eb\u22c6 ))k\u2208ZZ , that is,\n\u0011\n\u0010 x\nX\nj,k 2\n) \u22121 .\ntj,b =\nwk (r\u01eb\u22c6 ) (\n\u01eb\nk\u2208Z\nZ\n\nAlso, denote by tb the normalized empirical mean of the tj,b 's:\nd\n1 X\ntb = \u221a\ntj,b .\nd j=1\n\n(5.6)\n\nSimilarly, replacing tj by tj,b , consider the statistics L(u, b), Cu,b , and \u03a6\u03030,b defined by equations\n(5.3), (5.5) and (5.4) respectively, that is,\nL(u, b) =\n\nCu,b\n\nd\nX\nj=1\n\nCu,b\n\n=\n\n\u03a6\u03030,b (x)\n\n=\n\n(1I(tj,b >uTd ) \u2212 \u03a6\u03030,b (uTd )),\n\n(5.7)\n\n(d\u03a6\u03030,b (uTd )(1 \u2212 \u03a6\u03030,b (uTd )))\u22121/2 ,\n\nIP0 (tj,b > x).\n\n5.1. Moderate sparsity\nIn case of moderate sparsity, for any \u03b1 \u2208 (0, 1), consider the \u03c72 -type test procedure:\n2\n\n\u2206\n\n2\n\n\u03c7\n\u03c8\u03b1\u03c7 = \u03c8\u03b1,b\n\n=\n\n1I(tb >T\u03b1 ) ,\n\n(5.8)\n\nwhere tb is defined in (5.6) and T\u03b1 is the (1 \u2212 \u03b1)-quantile of a real standard Gaussian random\nvariable.\nTheorem 5.1. Assume that r\u01eb \u2192 0 and let a(r\u01eb ) be given by (4.8). Then, the following results\nhold true.\n\u2022 (i) Lower bound.\n\nIf a(r\u01eb )d1/2\u2212b = o(1), then \u03b3 \u2192 1.\n\nIf a(r\u01eb )d1/2\u2212b = O(1), then lim inf \u03b3 > 0.\n2\n\n\u2022 (ii) Upper bound. Let r\u01eb\u22c6 = r\u01eb\u22c6 (b) be determined by the relation a(r\u01eb\u22c6 ) \u224d db\u22121/2 and \u03c8\u03b1\u03c7 be\ndefined by (5.8). Then,\n2\n\nType I error: \u2200\u03b1 \u2208 (0, 1), \u03c9(\u03c8\u03b1\u03c7 ) = \u03b1 + o(1).\n\n2\n\nType II error: if a(r\u01eb )d1/2\u2212b \u2192 +\u221e, then \u03b2(\u03c8\u03b1\u03c7 , \u0398d (\u03c4, r\u01eb , b)) = o(1).\nRemark 5.1. Note that we obtain the same detection boundaries as in the vectorial case (see Section 2): the areas of distinguishability and non-distinguishability depend on the limit of d1/2\u2212b a(r\u01eb ).\nThe condition d1/2\u2212b a(r\u01eb ) \u224d a(r\u01eb )/a(r\u01eb\u22c6 ) \u2192 +\u221e is equivalent to r\u01eb /r\u01eb\u22c6 \u2192 +\u221e where by (4.8)\nr\u01eb\u22c6 \u224d (\u01eb4 d2b\u22121 )\u03c4 /(4\u03c4 +1) .\n\n(5.9)\n\nIn order to use Lemma 4.1, the condition r\u01eb \u2192 0 is required. Note that the requirement r\u01eb\u22c6 \u2192 0 is\nalways fulfilled for b \u2208 (0, 1/2) whatever the value of \u01eb > 0 is as soon as d \u2192 +\u221e. For b = 1/2,\nthe condition r\u01eb\u22c6 \u2192 0 holds when \u01eb \u2192 0.\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n11\n\n5.2. High sparsity\nLet us define the Higher-Criticism type test procedure. Let r\u01eb\u22c6 = r\u01eb\u22c6 (b) be\n\u221a determined by the\nrelation a(r\u01eb\u22c6 ) \u223c \u03c6(b)Td , where\u221a\u03c6(b) is given by (2.2). Set u(b) = min(2\u03c6(b), 2), i.e., u(b) = 2\u03c6(b)\nfor b \u2208 (1/2, 3/4], and u(b) = 2 for b \u2208 (3/4, 1]. Consider the test\n\u03c8L\n\n=\n\n1I{ max L(u , b ) > H} ,\nl l\n\nul = u(bl ),\n\n1\u2264l\u2264N \u22121\n\nwhere the function L is defined in (5.7) and (bl )1\u2264l\u2264N consists of a regular grid on (1/2, 1], that is,\nbl = 1/2 + l\u03b4, where \u03b4 is a positive parameter that satisfies \u03b4 = od (1), Td \u03b4 \u2192 +\u221e and N \u03b4 = 1/2.\nThis entails that N = Od (\u03b4 \u22121 ) and thus N = od (Td ). Take a positive H such that H \u223c (log d)C\nfor some positive constant C satisfying C > 14 .\n\u221a\nFor a constant D > 2, consider also the test\n\u03c8 max\n\n=\n\n1I{ max max t\n.\nj,bl > DTd }\n1\u2264j\u2264d 1\u2264l\u2264N\n\nL\n\nFinally, combining \u03c8 and \u03c8\n\nmax\n\n, we define the test procedure\n\u03c8 HC = \u03c8 L \u03c8 max ,\n\n(5.10)\n\nthat rejects H0 if both \u03c8 L and \u03c8 max reject H0 .\nFor the high sparsity case, not only separation rates but also sharp asymptotics are obtained;\ntwo ranges of b should be distinguished: the range of b in (1/2, 3/4], called the intermediate sparsity\ncase, and the range of b in (3/4, 1), called the highest sparsity case.\nTheorem 5.2. Assume that r\u01eb \u2192 0 and that log d = o(\u01eb\u22122/(2\u03c4 +1) ). Let a(r\u01eb ) be given by (4.8) and\nlet \u03c6 be given by (2.2).\n\u2022 (i) Lower bound. If lim sup a(r\u01eb )/Td < \u03c6(b), then lim inf \u03b3 \u2192 1.\n\u2022 (ii) Upper bound: errors of \u03c8 HC defined by (5.10).\n\n\u2013 Type I error: \u03c9(\u03c8 HC ) = o(1).\n\u2013 Type II error: if lim inf a(r\u01eb )/Td > \u03c6(b), then \u03b2(\u03c8 HC , \u0398d (\u03c4, r\u01eb , b)) = o(1).\n\nRemark 5.2.\n\u2022 Set a(r\u01eb\u22c6 ) = Td \u03c6(b). In our sparse functional framework, the distinguishability conditions are the same as for a d-dimensional sparse vector (see, e.g., [12]), with\nthe only difference that in our case the assumption log d = o(\u01eb\u22122/(2\u03c4 +1) ) is required. Under this assumption, the result of Theorem 5.2 means that distinguishability is impossible\nif lim sup a(r\u01eb )/a(r\u01eb\u22c6 ) < 1 and it is possible if lim inf a(r\u01eb )/a(r\u01eb\u22c6 ) > 1. Due to (4.8), these\nconditions provide sharp separation rates since they are equivalent to lim sup r\u01eb /r\u01eb\u22c6 < 1 and\nlim inf r\u01eb /r\u01eb\u22c6 > 1, respectively, where\nr\u01eb\u22c6\n\n\u223c (\u01eb4 Td2 (c1 (\u03c4 ))\u22121 \u03c62 (b))\u03c4 /(4\u03c4 +1) ,\n\n(5.11)\n\nr\u01eb\u22c6\n\nand c1 (\u03c4 ) is defined by (4.9). Note that the condition\n\u2192 0 is fulfilled under the assumption\nlog d = o(\u01eb\u22122/(2\u03c4 +1) ).\nThe values r\u01eb\u22c6 mark the border between the areas of distinguishability and nondistinguishability. Indeed, for r\u01eb \u2192 0 such that lim sup r\u01eb /r\u01eb\u22c6 < 1, the alternatives separated\nfrom the null hypothesis by r\u01eb are not distinguishable and, on the other side, for r\u01eb \u2192 0\nsuch that lim inf r\u01eb /r\u01eb\u22c6 > 1, the alternatives separated from the null hypothesis by r\u01eb are\ndistinguishable.\n\u2022 Actually, the assumption log d = o(\u01eb\u22122/(2\u03c4 +1) ) is equivalent to\n(r\u01eb\u22c6 )1/(2\u03c4 ) Td = o(1),\n\n(5.12)\n\nwhich is required when dealing with the asymptotic behavior of the tail distribution of tj,b (see\nLemma 7.1) since Td supwk (r\u01eb\u22c6 ) \u2264 (r\u01eb\u22c6 )1/(2\u03c4 ) Td . Relation (5.12) follows from the relations\nk\n\nin (4.10). Concerning the lower bound, condition (5.12) is necessary when we evaluate the\nsecond moment of the Bayesian likelihood ratio under the null hypothesis.\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n12\n\n\u2022 Note that the condition log d = o(\u01eb\u22122/(2\u03c4 +1) ) is essential for b \u2208 (1/2, 1). Namely, it follows\nfrom Theorem 2 \u221a\nin [10] that if lim inf (log d \u01eb2/(2\u03c4 +1) ) > 0, then the separation rates are of\n\u22c6\nthe form r\u01eb = \u01eb log d for any b \u2208 (1/2, 1). Observe that if log d \u2265 c\u01eb\u22122 for some c > 0,\nthen the separation rates are bounded away from zero, i.e., it is impossible to detect functions\nlying in \u0398d (\u03c4, r\u01eb , b) with small enough r\u01eb > 0.\nRemark 5.3. Adaptation.\nIn the high sparsity case, a family of test procedures \u03c8 HC provides the distinguishability for all\nb \u2208 (1/2, 1). Moreover, it follows from the proofs that our result is uniform over b \u2208 (1/2 + \u03c1, 1 \u2212 \u03c1)\nfor any \u03c1 \u2208 (0, 1/4), i.e., the results are adaptive over b \u2208 (1/2 + \u03c1, 1 \u2212 \u03c1) for any \u03c1 \u2208 (0, 1/4),\nwithout a loss in separation rates.\n2\n\u03c72\nFor the moderate sparsity case, it is worth noting that the family of test procedures \u03c8\u03b1\u03c7 = \u03c8\u03b1,b\ndepends on b \u2208 (0, 1/2] since the sequence of weights w(r\u01eb\u22c6 (b)) does. It is shown in Theorem\n3 of [10] that \"adaptive\" separation rates for unknown b \u2208 (0, 1/2) are of the form r\u01eb\u22c6 \u224d\n(\u01eb4 d2b\u22121 log log d)\u03c4 /(4\u03c4 +1) , i.e., the adaptive case leads to an unavoidable log log-loss in separation rates compared to non-adaptive setting. Using the Bonferroni method, it is possible to prove\n2\nthat the test procedures based on a grid of tests of the form \u03c8\u03b1\u03c7d ,bl are adaptive rate-optimal test\nprocedures. Since this result is similar to the one stated in [10], we omit it.\n6. Extended problem\nIn this section, we generalize the hypothesis testing problem stated in (1.3) to more general alternatives. The null hypothesis H0 is still characterized by some constant const0 and, as in (1.3), under\nthe alternative, the signal function f is, up to some constant, equal to f 1 , i.e., f = const1 + f 1 . The\nadditive sparse structure on f 1 is still assumed, i.e., f 1 \u2208 Fd,b , as well as every component fj1 is asPd\nd\nsumed 1-periodic and orthogonal to a constant (recall that for any t \u2208 [0, 1] f 1 (t) = j=1 \u03bej fj1 (tj )\nwhere \u03bej \u2208 {0, 1} and tj \u2208 [0, 1] for any j \u2208 {1, . . . , d} ). We then denote by F\u0303d,b the set of signal\nfunctions in Fd,b whose components are 1-periodic and orthogonal to a constant. Rather than imposing smoothness constrains component-wise, we now study the alternative classes for which the\nsmoothness and separation conditions are expressed in terms of the whole signal function f 1 . In\nother words, the main difference between the extended and initial detection problems is that the\ndistinguishability problem is studied with respect to a global signal.\nThen, given the alternatives that include signal functions f as in (1.3), where f 1 belongs to the\nfunctional class Fdext (\u03c4, L, r\u01eb , b), the testing problem of interest is stated as follows:\nH0 : f = const0\n\nversus\n\nH1 : f = const1 + f 1 , f 1 \u2208 Fdext (\u03c4, L, r\u01eb , b),\n\n(6.1)\n\nwhere\nFdext (\u03c4, L, r\u01eb , b) =\n(\u03c4 )\n\nin which (kf 1 k2 )2 =\n\nPd\n\nn\no\n(\u03c4 )\nf 1 \u2208 F\u0303d,b : kf 1 k2 \u2265 r\u01eb , kf 1 k2 \u2264 L ,\n\n1 (\u03c4 ) 2\nj=1 \u03bej (kfj k2 ) .\n\nDue to the periodic constraint, we consider the stan(\u03c4 )\n\ndard Fourier basis. This allows to express the semi-norm k * k2 in terms of Fourier coefficients.\nAs in Section 3, we then transform the functional space Fdext (\u03c4, r\u01eb , L, b) to the sequence space\n\u0398ext\nd (\u03c4, L, r\u01eb , b), which consists of sequences \u03b8 = (\u03bej \u03b8j,k )j,k such that\nd\nX\n\n\u03bej = d1\u2212b = K,\n\nj=1\n\nd\nX\nj=1\n\n\u03bej (2\u03c0)2\u03c4\n\nX\n\nk\u2208Z\nZ\nd\nX\nj=1\n\n2\n|k|2\u03c4 \u03b8j,k\n\u2264 L2 ,\n\n\u03bej\n\nX\n\nk\u2208Z\nZ\n\n2\n\u03b8j,k\n\u2265 r\u01eb2 .\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n13\n\nNote that if L2 = K and r\u0303\u01eb2 = Kr\u01eb2 , then we have\n\u0398ext\nd (\u03c4, L, r\u0303\u01eb , b) \u2283 \u0398d (\u03c4, r\u01eb , b).\nThis implies that the results on the lower bound continue to hold for \u0398ext\nd (\u03c4, L, r\u0303\u01eb , b) with the\nseparation rates (r\u0303\u01eb\u22c6 )2 = K(r\u01eb\u22c6 )2 , where r\u01eb\u22c6 is defined by either (5.9) or (5.11) depending on the\nvalues of b. Here, the quantity of interest is \u00e3(r\u01eb ), the solution of the following extremal problem:\n\uf8f1\nd\nX\n\uf8f4\n\uf8f4\n\uf8f4\n\u03bej = d1\u2212b = K\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\nj=1\n\uf8f4\n\uf8f4\n\uf8f4\nd\nd\n\uf8f2\nX\nX\nX\nX\n1\n2\n2\n4\n\u03bej (2\u03c0)2\u03c4\n|k|2\u03c4 \u03b8j,k\n\u2264K\n(6.2)\n\u00e3 (r\u01eb ) = 4 inf\n\u03bej\n\u03b8j,k subject to\n\uf8f4\n2\u01eb \u03b8\u2208l2 j=1\n\uf8f4\nj=1\nk\u2208Z\nZ\nk\u2208Z\nZ\n\uf8f4\n\uf8f4\n\uf8f4\nd\n\uf8f4\n\uf8f4\n\uf8f4 X\u03be X \u03b82 \u2265 Kr2\n\uf8f4\nj\n\uf8f4\nj,k\n\u01eb\n\uf8f3\nj=1\n\nk\u2208Z\nZ\n\nAs follows from Section 4.3 in [13], the solution of the extremal problem (6.2) is given by\n\u00e3(r\u01eb )\n\n\u223c (c1 (\u03c4 ))1/2 Kr\u01eb2+1/(2\u03c4 ) \u01eb\u22122 as r\u01eb \u2192 0,\n\nwhere c1 (\u03c4 ) is defined in (4.9). That is, \u00e3(r\u01eb ) = Ka(r\u01eb ), where a(r\u01eb ) is the solution (4.8) of the\nextremal problem (4.1).\nRemark 6.1. Consider the function \u03ba defined by (4.4), for which the sequence of weights w(r\u01eb ) =\n(wk (r\u01eb ))k is defined as in (4.2). Then we obtain from (4.7) that\nd\n1 X\n\u03bej \u03ba(\u03b8j , w(r\u01eb )) \u2265\n1/2 ,K 1/2 r ,b) \u01eb2\n\u03b8\u2208\u0398ext\n\u01eb\nd (\u03c4,K\nj=1\n\n\u00e3(r\u01eb ) = Ka(r\u01eb ),\n\ninf\n\n(6.3)\n\nand similarly to Proposition 4.1 for any D \u2265 1,\nd\n1 X\n\u03bej \u03ba(\u03b8j , w(r\u01eb ))\ninf\n1/2 ,DK 1/2 r ,b) \u01eb2\n\u03b8\u2208\u0398ext\n\u01eb\nd (\u03c4,K\nj=1\n\n\u2265\n\nD2 \u00e3(r\u01eb ) = D2 Ka(r\u01eb ).\n\n(6.4)\n\nNow, as in Section 3, with the use of the orthonormal system, instead of considering the random\nprocess X(t) defined in model (1.1), we observe a family of random sequences (xj,k )k\u2208ZZ,j\u2208{1,...,d}\n2\ndefined by (3.1). Finally, the remained question is: do the families of test procedures \u03c8\u03b1\u03c7 given by\n(5.8) and \u03c8 HC given by (5.10) provide distinguishability? The answer is affirmative and is given\nbelow. Note that it is then sufficient to study the type II error probability of these tests since their\ntype I error probability has been already studied for the hypothesis testing problem (1.3).\nTheorem 6.1. Assume that r\u01eb \u2192 0 and let a(r\u01eb ) and \u03c6 be given by (4.8) and (2.2), respectively.\nThen, the following results hold true.\n2\n\n\u2022 (i) Moderate sparsity-Type II error probability of \u03c8\u03b1\u03c7 defined by (5.8).\n2\n1/2\nIf a(r\u01eb )d1/2\u2212b \u2192 +\u221e, then \u03b2(\u03c8\u03b1\u03c7 , \u0398ext\n, K 1/2 r\u01eb , b)) = o(1).\nd (\u03c4, K\n\u2022 (ii) High sparsity-Type II error probability of \u03c8 HC defined by (5.10).\nAssume that log d = o(\u01eb\u22122/(2\u03c4 +1) ).\n1/2\nIf lim inf a(r\u01eb )/Td > \u03c6(b), then \u03b2(\u03c8 HC , \u0398ext\n, K 1/2 r\u01eb , b)) = o(1).\nd (\u03c4, K\nRemark 6.2. One should note that the detection boundaries are the same for the hypothesis testing\nproblems (1.3) and (6.1), the initial one and its generalization.\n7. Proofs\nProofs of our main results require some preliminary results that are stated below both under the\nnull and alternative hypotheses. Specifically, we establish asymptotic tail distributions of the test\nstatistics in hand and find their first and second moments.\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n14\n\n7.1. Properties of test statistics\nIn this section, we consider the statistics tj defined by (5.1) with any sequence of weights w =\nP\n(wk )k\u2208ZZ such that wk \u2265 0, \u2200k \u2208 Z\nZ and k wk2 = 1/2. Therefore the quantities L(u), C(u), and \u03a6\u03030\nare those defined by (5.3), (5.5) and (5.4).\nProposition 7.1. Asymptotic tail distribution of tj defined by (5.1).\nAssume T maxk wk = o(1), then\nlog IP0 (tj > T ) \u223c \u2212\nlog IP\u03b8j (tj > T ) \u223c \u2212\n\nT2\nas T \u2192 +\u221e,\n2\n(T \u2212 IE\u03b8j (tj ))2\nT \u2192+\u221e\n, as (T \u2212 IE\u03b8j (tj )) \u2212\u2192 +\u221e.\n2\n\nProof of Proposition 7.1.\nWe consider only the distribution IP\u03b8j since IP0 is a particular case of IP\u03b8j . The proof consists\nof bounding IP\u03b8j (tj > T ) from above and below. This is done by using the cumulant-generating\nfunction of tj under IP\u03b8j which is defined by \u03c6\u03b8j (h) = log(IE\u03b8j (exp(htj ))) for any h. Let us consider\ndIP\u03b8j ,h\nonly positive h and let us introduce a new family of probability measures IP\u03b8j ,h such that\n=\ndIP0\nexp(htj ) exp(\u2212\u03c6\u03b8j (h)). This yields\nIP\u03b8j (tj > T ) =\n=\n\nIE\u03b8j ,h [1I(tj >T ) exp(\u2212(htj \u2212 \u03c6\u03b8j (h)))]\n\nexp(\u2212(hT \u2212 \u03c6\u03b8j (h))) IE\u03b8j ,h [1I(tj >T ) exp(\u2212h(tj \u2212 T ))].\n\n(7.1)\n\nLet us start with the upper bound.\nUpper bound. The second term o the right-hand side of (7.1) is less than 1. Hence there is a\nstraightforward upper bound on IP\u03b8j (tj > T ):\nIP\u03b8j (tj > T ) \u2264\n\nexp(\u2212(hT \u2212 \u03c6\u03b8j (h))).\n\n(7.2)\n\nTo complete this part of the proof, it remains to determine the minimum value of a positive value\nh on the right-hand side of (7.2). The minimum is attained for positive h such that\nIE\u03b8j ,h (tj )\nsince\n\n\u001a\n\u2032\n\n(\u03c6\u03b8j (h) \u2212 hT )\u2032\n\u2032\u2032\n(\u03c6\u03b8j (h) \u2212 hT )\n\n=\n=\n\n= T\n\n(7.3)\n\nIE\u03b8j ,h (tj ) \u2212 T,\nVar\u03b8j ,h (tj ) \u2265 0,\n\n\u2032\u2032\n\nwhere (*) and (*) denote the first and second derivatives with respect to h, respectively, and,\nIE\u03b8j ,h and Var\u03b8j ,h are the expectation and variance with respect to IP\u03b8j ,h .\n\u03b8j,k\nIn order to find h that solves equation (7.3), we need to determine \u03c6\u03b8j . For this, set \u03bdj,k =\n.\n\u01eb\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n15\n\nThen for any positive h such that h \u2192 +\u221e and hmaxwk = o(1), we obtain\nk\n\n\u03c6\u03b8j (h) =\n\nlog\n\nY\nk\n\n=\n\nX\nk\n\n=\n\nX\nk\n\n=\n\nIE\u03b8j [exp(hwk ((\u03bdj,k + \u03b7j,k )2 \u2212 1))]\n\n{\u2212hwk +\n\n2\nhwk \u03bdj,k\n1\n\u2212 log(1 \u2212 2hwk )}\n(1 \u2212 2hwk ) 2\n\n2\n{\u2212hwk + hwk \u03bdj,k\n(1 + 2hwk + o(hwk ))\n\n1\n(2hwk )2\n\u2212 (\u22122hwk \u2212\n+ o(h2 wk2 ))}\n2\n2\nX\n2\n2\n{hwk \u03bdj,k\n(1 + o(h max wk )) + h2 wk2 (2\u03bdj,k\n+ 1) + o(h2 wk2 )}\nk\n\nk\n\n=\n\nhIE\u03b8j (tj )(1 + o(h max wk )) +\nk\n\nh2\n(1 + o(1)) + o(h2 ),\n2\n\n(7.4)\n\nwhere the last equality sign in (7.4) follows from (T \u2212 IE\u03b8j (tj )) \u2192 +\u221e and T maxwk = o(1) as\nk\n\nT \u2192 +\u221e. Next, differentiating the right-hand side of (7.4) with respect to h yields\n(\u03c6\u03b8j (h) \u2212 hT )\u2032\n\n=\n\n0\n\n\u21d2\n\nas T \u2212 IE\u03b8j (tj ) goes to infinity.\n\nh \u223c T \u2212 IE\u03b8j (tj ),\n\nT \u2192+\u221e\n\n\u2212\u2192 +\u221e, this leads to the following optimal upper bound for right-hand side\nAs (T \u2212 IE\u03b8j (tj ))\nof (7.2):\n\u0012\n\u0013\n(T \u2212IE\u03b8j (tj ))2\n(T \u2212IE\u03b8j (tj ))2\nexp (T \u2212IE\u03b8j (tj ))IE\u03b8j (tj )+\n\u2212T (T \u2212IE\u03b8j (tj ) \u223c exp(\u2212\n).\n2\n2\nSince by assumption T maxwk = o(1), the condition h maxwk = o(1) with (T \u2212 IE\u03b8j (tj )) in place\nk\n\nk\n\nof h is fulfilled.\n2\nBy assumption T maxwk = o(1), hence the optimal upper bound under IP0 is exp(\u2212 T2 ) as T goes\nk\n\nto infinity. This completes the proof of the upper bound.\nLower Bound. We are interested in obtaining a lower bound for (7.1). This is done by first\nconsidering a new family of probability distributions under which the normalized statistics tj are\nproved to be asymptotically Gaussian.\nFor h > 0 satisfying equation (7.3), let us introduce the following probability measures IP\u03b8j ,h,k :\ndIP\u03b8j ,h,k\n= exp(htj,k ) exp(\u2212\u03c6\u03b8j,k (h)),\ndIP0\nwith tj,k defined in (5.2), \u03c6\u03b8j,k (h) = log IE\u03b8j,k (exp(htj,k )) and where IE\u03b8j,k stands for the expectation\nwith respect to the observations (xj,k )j,k of (3.1). Denote by IE\u03b8j ,h,k and Var\u03b8j ,h,k the expectation\nand variance with respect to IP\u03b8j ,h,k .\nTo establish the asymptotic normality of tj , we will check\nthe Lyapunov condition is\nP that\n2\n2\n2\n.\nsatisfied. To this end, set \u03c3j,h,k\n= Var\u03b8j ,h,k (tj,k ) and \u03c3j,h\n= k \u03c3j,h,k\n(2)\n\n(4)\n\nDenote by \u03c6\u03b8j,k and \u03c6\u03b8j,k the second and fourth derivatives of \u03c6\u03b8j,k with respect to h, respectively.\nUsing well-known relations between moments of tj under IP\u03b8j ,h,k and the successive derivatives of\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n2\n\u03c6\u03b8j,k (h) with respect to h, in particular, \u03c3j,h\n=\n\nP\n\nk\n\nIE\u03b8j ,h,k (tj,k \u2212 IE\u03b8j ,h,k (tj,k ))4\nP 2\n)2\n( k \u03c3j,h,k\n\nP\n\n16\n\n(2)\n\nk\n\n\u03c6\u03b8j,k , we get\nP (4)\n(2)\n2\nk \u03c6\u03b8j,k (h)\nk (\u03c6\u03b8j,k (h)) +\nP (2)\n( k \u03c6\u03b8j,k (h))2\nP\n4 max(wk2 ) k wk2 (1 + o(1)) + o(1)\n3\n\n=\n\u2264\n\nP\n\n1\n\n=\n\no(1),\n\nwhere the last relation follows from max wk = o(1) and relation (7.4), since by (7.4) we get\nP (4)\n(4)\nk \u03c6\u03b8j,k (h) = \u03c6\u03b8j (h) = o(1). The Lyapunov condition is then satisfied. This implies that under\ntj \u2212 IE\u03b8j ,h (tj )\nIP\u03b8j ,h , Zj,h =\nis asymptotically a real standard Gaussian random variable.\n\u03c3j,h\nLet us return to relation (7.1), where h is chosen to have IE\u03b8j ,h (tj ) = T , and observe that\nIE\u03b8j ,h [1I(tj >T ) exp(\u2212h(tj \u2212 T ))] = IE\u03b8j ,h [1I(Zj,h >0) exp(\u2212hZj,h \u03c3j,h )].\nDue to the asymptotic normality of tj , for any \u03b4 > 0,\nIE\u03b8j ,h [1I(Zj,h >0) exp(\u2212hZj,h \u03c3j,h )] = IE\u03b8j ,h [1I(Zj,h \u2208(0,\u03b4)) exp(\u2212hZj,h \u03c3j,h )] +\n\nIE\u03b8j ,h [1I(Zj,h >\u03b4) exp(\u2212hZj,h \u03c3j,h )]\n> (IP\u03b8j ,h (Zj,h \u2208 (0, \u03b4)) + o(1)) exp(\u2212h\u03b4\u03c3j,h ).\n\n(7.5)\n\nBy choosing \u03b4 = o(h) in relation (7.5) implies that\nlog(IP\u03b8j (tj > T )) \u2265\n\n\u03c6\u03b8j (h) \u2212 hT \u2212 o(h2 ).\n\n(7.6)\n\nUp to o(h2 ), the right-hand side of (7.6) corresponds to the argument of the exponential function\non the right-hand side of (7.2). This entails that the right-hand side of (7.6) is equivalent to\n(T \u2212 IE\u03b8j (tj ))2\n. This completes the proof of the lower bound, and thus Proposition 7.1 is proved.\n\u2212\n2\nLemma 7.1.\n\u2022 (i) Expectation and variance of tj defined by (5.1).\nIE\u03b8j (tj ) =\nVar\u03b8j (tj ) =\n\n\u03bej \u01eb\u22122 \u03ba(\u03b8j , w),\n\n(7.7)\n\n1 + O((max wk ) IE\u03b8j (tj )).\n\n(7.8)\n\nk\u2208Z\nZ\n\n\u2022 (ii) Expectation and variance of L(u) defined by (5.3). Assume that Td max wk = o(1) and\nP\nconsider any \u03b8 = (\u03be1 \u03b81 , . . . , \u03bed \u03b8d ) such that dj=1 \u03bej = d1\u2212b . Moreover, assume that for all\nnonzero \u03bej , IE\u03b8j (tj ) \u2265 cTd , with some positive c, and max IE\u03b8j (tj ) = O(Td ). Then, for all\nj:\u03bej =1\n\u221a\nu \u2208 (0, 2],\n1\n\nd 2 \u2212b+(\n\nIE\u03b8 (L(u)) \u2265\n\n\u03b7\n\nVar\u03b8 (L(u)) =\n\nu2\n4\n\n\u2212\n\n((u\u2212c)+ )2\n2\n\n)(1+o(1))\n\no(d IE\u03b8 (L(u))) + o(1),\n\n(1 + o(1)),\n\n\u03b7 = o(1),\n\nwhere x+ = max(0, x).\nRemark 7.1. Under IP0 , the statistics tj and L(u) have zero mean and unit variance. Moreover, under IP0 and the assumption max wk = o(1), the statistics tj are asymptotically stank\n\ndard Gaussian. Under IP\u03b8j , the statistics tj \u2212 IE\u03b8j tj are asymptotically standard Gaussian if\nmax wk IE\u03b8j tj = o(1), see Lemma 3.1 in [13].\nk\n\n7.1.\nProof of LemmaX\nxj,k 2\n(i) Recall that\nwk2 = 1/2. For each index j satisfying \u03bej = 1, the random variable (\n) is a\n\u01eb\nk\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n17\n\n2 \u22122\n\u01eb ). From this relation (7.7) is easily obtained. Relation (7.8) is deduced\nIP\u03b8j -noncentral \u03c72 (1, \u03b8j,k\nfrom the following calculations:\nX\n2\nVar\u03b8j (tj ) =\nwk2 (2 + 4\u01eb\u22122 \u03bej \u03b8j,k\n)\nk\u2208Z\nZ\n\nX\n\n2\nwk2 4\u01eb\u22122 \u03bej \u03b8j,k\n\n=\n\n1+\n\n=\n\n1 + O(max wk \u01eb\u22122 \u03bej \u03ba(\u03b8j , w))\n\n=\n\n1 + O(max wk IE\u03b8j (tj )).\n\nk\u2208Z\nZ\nk\u2208Z\nZ\n\nk\u2208Z\nZ\n\n\u221a\n(ii) For any u \u2208 (0, 2], as Td \u2192 +\u221e, Proposition 7.1 gives a control over Cu defined by (5.5):\nCu2\n\n= d\u22121 exp(\n= d\u22121+\n\nu2\n2\n\n\u2212u2 Td2\nu2 Td2\n(1 + o(1)))(1 \u2212 exp(\n(1 + o(1))))\u22121\n2\n2\n\n(1+o(1))\n\n.\n\u221a\nSince u \u2264 2, the exponent of d in Cu is o(1).\nCase 1: for the nonzero \u03bej 's, assume that lim sup(uTd \u2212IE\u03b8j (tj )) < +\u221e. In this case, the probability\nIP\u03b8j (tj > uTd ) = IP\u03b8j (tj \u2212 IE\u03b8j (tj ) > uTd \u2212 IE\u03b8j (tj )) is bounded away from zero. This follows from\nthe asymptotic normality of tj \u2212 IE\u03b8j (tj ) for IE\u03b8j (tj ) = O(Td ) (see Remark 7.1)\nCase 2: for the nonzero \u03bej 's, assume that uTd \u2212 IE\u03b8j (tj ) \u2192 +\u221e. Then, for any nonzero \u03bej ,\nProposition 7.1 implies that\nlog IP\u03b8j (tj > uTd ) \u2265\n\n\u2212\n\n(uTd \u2212 cTd )2\n(1 + o(1)).\n2\n\nRecall that the number of nonzero \u03bej is equal to K = d1\u2212b and that for all nonzero \u03bej , IE\u03b8j (tj ) \u2265\ncTd for some positive c such that maxj:\u03bej =1 IE\u03b8j (tj ) = O(Td ). To sum up, the cases 1 and 2 entail\nthat\n\u0011\nX \u0010\nIE\u03b8 (L(u)) = Cu\nIP\u03b8j (tj > uTd ) \u2212 \u03a6\u03030 (uTd )\nj:\u03bej =1\n\n\u2265\n=\n=\n\n\u0012\n\u0013\n((u\u2212c)+ )2\nu2\n\u2212\n(1+o(1))\n\u2212\n(1+o(1))\n2\nCu K d\n\u2212d 2\n\u0012\n\u0013\n((u\u2212c)+ )2\n2\n2\n\u2212\n(1+o(1))\n\u2212 u2 (1+o(1))\n\u2212 12 + u4 (1+o(1))+1\u2212b\n2\nd\n(1 + o(1))\n\u2212d\nd\n1\n\nd 2 \u2212b+(\n\nu2\n4\n\n\u2212\n\n((u\u2212c)+ )2\n2\n\n)(1+o(1))\n\n(1 + o(1)).\n\nSimilarly, let us study the variance of L(u). Using Proposition 7.1, we obtain\nX\nX\nVar\u03b8 (L(u)) = Cu2\nIP\u03b8j (tj > uTd)IP\u03b8j (tj \u2264 uTd ) + Cu2\n\u03a6\u03030 (uTd )(1 \u2212 \u03a6\u03030 (uTd ))\nj:\u03bej =1\n\n=\n\nCu2 KIP\u03b8j (tj\n\nj:\u03bej =0\n\nb\u22121\n\n> uTd )(1 + o(1)) + (d\nb\u22121\n\n=\n\n(Cu IE\u03b8 (L(u)) + d\n\n=\n\no(d\u03b7 IE\u03b8 (L(u))) + o(1),\n\n\u2212b\n\n+d\n\n)(1 + o(1))\n\n)(1 + o(1))\n\u03b7 = o(1).\n\n7.2. Upper bound\nRemark 7.2. Note that the condition Td maxwk (r\u01eb\u22c6 ) = o(1) follows from assumption log d =\nk\n\no(\u01eb\u22122/(2\u03c4 +1) ). Indeed, Remark 4.2 and relations (4.10) imply that Td max wk (r\u01eb\u22c6 ) \u2264 (r\u01eb\u22c6 )1/(2\u03c4 ) Td ,\nwhere the term on the right-hand side goes to zero as soon as log d = o(\u01eb\u22122/(2\u03c4 +1) ). Therefore,\nassumption log d = o(\u01eb\u22122/(2\u03c4 +1) ) allows us to apply Proposition 7.1 and Lemma 7.1.\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n18\n\nProof of (ii)\u2013Theorem 5.1.\n2\nType I error probability of \u03c8\u03b1\u03c7 . It follows from the Central Limit Theorem that, under the\nnull hypothesis, tb is asymptotically a standard normal random variable. Therefore\nIP0 (tb > T\u03b1 ) =\n\n\u03a6(\u2212T\u03b1 ) + o(1) = \u03b1 + o(1).\n\n2\n\nType II error probability of \u03c8\u03b1\u03c7 uniformly over \u0398d (\u03c4, r\u01eb , b) for r\u01eb \u2265 Br\u01eb\u22c6 , B \u2265 1. Thanks\nto Lemma 7.1, uniformly over \u03b8 \u2208 \u0398d (\u03c4, r\u01eb , b), we have\nd\n\nVar\u03b8 (tb ) =\n\nIE\u03b8 (tb ) =\n\n1X\n(1 + O(IE\u03b8j (tj,b )))\nd j=1\nd\u22121/2\n\nd\nX\n\nIE\u03b8j (tj,b ).\n\nj=1\n\nThis implies that Var\u03b8 (tb ) = o((IE\u03b8 (tb ))2 ) provided that IE\u03b8 (tb ) \u2192 +\u221e. Let us study IE\u03b8 (tb ):\nfrom Proposition 4.1, Lemma 7.1, and relation (4.7), we get uniformly over \u0398d (\u03c4, r\u01eb , b) with r\u01eb \u2265\nBr\u01eb\u22c6 , B \u2265 1:\nIE\u03b8 (tb ) \u2265 d1/2\u2212b B 2 a(r\u01eb\u22c6 ) \u2192 +\u221e as soon as B 2 d1/2\u2212b a(r\u01eb\u22c6 ) \u224d B 2 \u2192 +\u221e, i.e., as soon as r\u01eb /r\u01eb\u22c6 \u2192 +\u221e, (7.9)\nwhere r\u01eb\u22c6 \u224d (\u01eb4 d2b\u22121 )\u03c4 /(4\u03c4 +1) .\nDue to (7.9), using Markov's inequality and Lemma 7.1, for all \u03b8 in \u0398d (\u03c4, r\u01eb , b),\nIP\u03b8 (tb \u2264 T\u03b1 ) =\n\u2264\n\u2264\n\nIP\u03b8 (tb \u2212 IE\u03b8 (tb ) \u2264 T\u03b1 \u2212 IE\u03b8 (tb ))\nIP\u03b8 (|tb \u2212 IE\u03b8 (tb )| \u2265 IE\u03b8 (tb ) \u2212 T\u03b1 )\nVar\u03b8 (tb )\n= o(1).\n(IE\u03b8 (tb ) \u2212 T\u03b1 )2\n\n2\n\nThis entails that \u03b2(\u03c8\u03b1\u03c7 , \u0398d (\u03c4, r\u01eb , b)) goes to zero as soon as d1/2\u2212b a(r\u01eb ) \u2192 +\u221e, i.e., as soon as\na(r\u01eb )\n\u2192 +\u221e where a(r\u01eb\u22c6 ) \u224d db\u22121/2 .\na(r\u01eb\u22c6 )\nProof of (ii)\u2013Theorem 5.2.\nType I error probability of \u03c8 HC . Observe that w(\u03c8 HC ) \u2264 w(\u03c8 L ) + w(\u03c8 max ). The assumption\nlog(d) = o(\u01eb\u22122/(2\u03c4 +1) ) implies that Td maxk wk (r\u01eb\u22c6 ) = o(1). Therefore the application of Proposition\n7.1 and the fact that D2 > 2 and N = o(Td ) yield\nw(\u03c8\n\nmax\n\n) =\n\u2264\n\nIP0 ( max max tj,bl > DTd ) \u2264\n1\u2264j\u2264d 1\u2264l\u2264N\n\nd X\nN\nX\n\nIP0 (tj,bl > DTd )\n\nj=1 l=1\n\nN d exp(\u2212D2 Td2 /2(1 + od (1))) = N d1\u2212D\n\n2\n\n/2(1+od (1))\n\n\u2192 0.\n\nBy Lemma 7.1 and applying Markov's inequality,\nw(\u03c8 L ) = IP0 ( max\n\n1\u2264l\u2264N \u22121\n\nL(ul , bl ) > H) \u2264\n\u2264\n\u2264\n\nN\n\u22121\nX\n\nIP0 (L(ul , bl ) > H)\n\nl=1\n\nN\n\u22121\nX\nl=1\n\nVar0 (L(ul , bl ))\nH2\n\n(N \u2212 1)\n,\nH2\n\nwhich goes to zero as d \u2192 +\u221e since H \u223c (log d)C , with C >\n\n1\n4\n\nand N = od (Td ).\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n19\n\nType II error probability of \u03c8 HC uniformly over \u0398d (\u03c4, r\u01eb , b). For any \u03b8 \u2208 \u0398d (\u03c4, r\u01eb , b), we\nobtain\nIE\u03b8 (1 \u2212 \u03c8 HC )\nIE\u03b8 (1 \u2212 \u03c8 max )\n\n\u2264 min(IE\u03b8 (1 \u2212 \u03c8 max ), IE\u03b8 (1 \u2212 \u03c8 L )),\n\u2264 min min IP\u03b8j (tj,bl \u2264 DTd ).\nj:\u03bej =1 1\u2264l\u2264N\n\n(7.10)\n(7.11)\n\nFirst, let us consider the alternatives \u03b8 \u2208 \u0398d (\u03c4, r\u01eb , b) such that for a nonzero \u03bej , there exists l \u2208\n{1, . . . , N } for which IE\u03b8j tj,bl \u2265 D1 Td with D1 > D. From Lemma 7.1(i) and Markov's inequality,\nwe obtain\nIP\u03b8j (tj,bl \u2264 DTd ) \u2264\n\u2264\n\nIP\u03b8j (|tj,bl \u2212 IE\u03b8j (tj,bl )| \u2265 IE\u03b8j (tj,bl ) \u2212 DTd )\nVar\u03b8j (tj,bl )\n= o(1).\n(IE\u03b8j (tj,bl ) \u2212 DTd )2\n\n(7.12)\n\nSecond, in view of (7.10), (7.11), (7.12), it suffices to study the test procedures \u03c8 L under the\nalternatives \u03b8 \u2208 \u0398d (\u03c4, r\u01eb , b) such that max max IE\u03b8j tj,bl = O(Td ). Then we obtain\nj:\u03bej =1 1\u2264l\u2264N\n\nIE\u03b8 (1 \u2212 \u03c8 L ) =\n\nIP\u03b8 ( max\n\n1\u2264l\u2264N \u22121\n\nL(ul , bl ) \u2264 H) \u2264\n\nmin\n\n1\u2264l\u2264N \u22121\n\nIP\u03b8 (L(ul , bl ) \u2264 H).\n\nFor any l \u2208 {1, . . . , N \u2212 1},\nIP\u03b8 (L(ul , bl ) \u2264 H) \u2264\n\u2264\n\u2264\n\u2264\n\nIP\u03b8 (L(ul , bl ) \u2212 IE\u03b8 (L(ul , bl )) \u2264 H \u2212 IE\u03b8 (L(ul , bl )))\nIP\u03b8 (\u2212|L(ul , bl ) \u2212 IE\u03b8 (L(ul , bl ))| \u2264 H \u2212 IE\u03b8 (L(ul , bl )))\nIP\u03b8 (|L(ul , bl ) \u2212 IE\u03b8 (L(ul , bl ))| \u2265 \u2212H + IE\u03b8 (L(ul , bl )))\nVar\u03b8 (L(ul , bl ))\n.\n(IE\u03b8 (L(ul , bl )) \u2212 H)2\n\nFor any bl \u2208 (1/2, 1), if we prove that\n\n(7.13)\n\nIE\u03b8 (L(ul , bl )) goes to infinity as a power of d\n\ninf\n\n\u03b8\u2208\u0398d (\u03c4,r\u01eb ,b)\n\n(d \u2192 +\u221e), then Lemma 7.1 and the choice of H (recall H = Od ((log d)C ), with C > 1/4) yield\nthe result since in this case the right-hand side of relation (7.13) goes to zero.\nThird, for b \u2208 (1/2, 1), take an index l in {1, . . . , N \u2212 1} such that bl \u2264 b \u2264 bl+1 . This, combined\nwith the continuity of \u03c6, yields\nbl = b + o(1),\n\nr\u01eb\u22c6 (bl ) \u2264 r\u01eb\u22c6 (b) \u223c r\u01eb\u22c6 (bl ),\n\na(r\u01eb\u22c6 (bl )) \u2264 a(r\u01eb\u22c6 (b)) \u223c a(r\u01eb\u22c6 (bl )).\n\nLet \u03b8 \u2208 \u0398d (\u03c4, r\u01eb , b) with b \u2208 (1/2, 1) and lim inf(a(r\u01eb )/a(r\u01eb\u22c6 (b)) > 1. Then r\u01eb \u2265 (1 + \u03b4)r\u01eb\u22c6 (bl ) for\nsome \u03b4 > 0. Proposition 4.1 entails that for j such that \u03bej = 1 we have\nIE\u03b8j tj,bl \u2265 (1 + \u03b4)2 a(r\u01eb\u22c6 (bl )) \u223c (1 + \u03b4)2 a(r\u01eb\u22c6 (b)) \u223c (1 + \u03b4)2 \u03c6(b)Td .\nWe then derive from Lemma 7.1 with c = c(b) = (1 + \u03b4)2 \u03c6(b) that\ninf\n\u03b8\u2208\u0398d (\u03c4,r\u01eb ,b)\n\nIE\u03b8 (L(ul , bl )) >\n\n1\n\nd2+\n\nu2\nl\n4\n\n\u2212b\u2212\n\n((ul \u2212c(b))+ )2\n2\n\n(1+o(1))\n\n(1 + o(1)).\n\n(7.14)\n\nFinally, denote the main term in the exponent of d in (7.14) by\nM=\n\n((u(b) \u2212 c(b))+ )2\n1 u(b)2\n+\n\u2212b\u2212\n.\n2\n4\n2\n\nTo obtain the result, it is sufficient to prove that M is positive and bounded away from zero for\nany \u03b4 > 0.\nIntermediate sparsity case. This case corresponds to b \u2208 (1/2, 3/4]. Recall that u(b) = 2\u03c61 (b),\nwhere \u03c61 is defined in (2.2). Then\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n20\n\n\u221a\n\u001a\n(\u03c621 (b)/2)((1 + \u03b4)2 \u2212 1)(3 \u2212 (1 + \u03b4)2 ) > 0 for 0 < \u221a\n\u03b4 < 2\u22121\n.\nM >0\u21d4\n\u03c621 (b)/2 > 0\nfor \u03b4 \u2265 2 \u2212 1\nThe latter inequalities are obviously satisfied. This leads to\n\u221a the result.\nHighest sparsity\ncase.\nIn\nthis\ncase\nb\n\u2208\n(3/4,\n1)\nand\nu(b)\n=\n2. Then\n\u221a\n\u001a\n((1 + \u03b4)2 \u2212 1)\u03c62 (b) > 0 for (1 + \u03b4)2 \u03c62 (b) \u2264 \u221a2\n.\nM >0\u21d4\n1\u2212b>0\nfor (1 + \u03b4)2 \u03c62 (b) > 2\nAgain, the latter inequalities are satisfied, and the result follows.\nProof of (i)\u2013Theorem 6.1.\nSimilar to the proof of part (ii) of Theorem 5.1, due to (6.3) and (6.4), uniformly over \u03b8 \u2208\n2\n1/2\n\u0398ext\n, K 1/2 r\u01eb , b), the type II error probability of \u03c8\u03b1\u03c7 goes to zero as soon as r\u01eb /r\u01eb\u22c6 \u2192 +\u221e.\nd (\u03c4, K\nProof of (ii)\u2013Theorem 6.1.\nThe proof of the fact that the type II error probability of \u03c8 HC goes to zero as d \u2192 +\u221e is similar to\nthe one of Theorem 5.2. Recall that K = d1\u2212b is the number of nonzero \u03bej 's and suppose without\nloss of generality that \u03bej = 1, \u2200j \u2208 {1, . . . , K} and \u03bej = 0, \u2200j \u2208 {K + 1, . . . , d}. Note that relations\n1/2\n, K 1/2 r\u01eb , b).\n(7.10) and (7.11) remain valid for any \u03b8 \u2208 \u0398ext\nd (\u03c4, K\next\n1/2\nFirst, similarly to (7.12), for any \u03b8 \u2208 \u0398d (\u03c4, K , K 1/2 r\u01eb , b) such that for the nonzero \u03bej 's,\nthere exists l \u2208 {1, . . . , N } for which IE\u03b8j tj,bl \u2265 D1 Td with D1 > D, the type II error probability\nof \u03c8 HC vanishes asymptotically. Therefore, it suffices to study the test procedures \u03c8 L under the\n1/2\n, K 1/2 r\u01eb , b) such that max max IE\u03b8j tj,bl = O(Td ). Therefore, let us\nalternatives \u03b8 \u2208 \u0398ext\nd (\u03c4, K\nj:\u03bej =1 1\u2264l\u2264N\n\ntake \u03b4 > 0 and consider the alternatives that are as far away from the null hypothesis as r\u01eb such\nthat r\u01eb \u2265 (1 + \u03b4)r\u01eb\u22c6 (b), where r\u01eb\u22c6 (b) is determined by a(r\u01eb\u22c6 (b)) \u223c Td \u03c6(b).\nSecond, for any l \u2208 {1, . . . , N }, observe that the only difference between the proofs of the\nextended and initial problems lies in the study of\nK\nX\n\ninf\n\n1/2 ,K 1/2 r ,b)\n\u03b8\u2208\u0398ext\n\u01eb\nd (\u03c4,K\nj=1\n\nIP\u03b8j (tj,bl \u2212 IE\u03b8j (tj,bl ) > ul Td \u2212 IE\u03b8j (tj,bl )).\n\n(7.15)\n\nNow it is no more possible to control (7.15) by using Lemma 7.1 (ii) because the condition\nIE\u03b8j (tj ) \u2265 cTd is not necessarily satisfied for all nonzero \u03bej 's. In fact, the only condition we have is\nK\nX\nIE\u03b8j (tj ) \u2265 cKTd with some constant c > 1.\nj=1\n\nLet us now explain why the current proof is reduced to the study of (7.15). As in (7.13), we get\n1/2\nfor any \u03b8 in \u0398ext\n, K 1/2 r\u01eb , b),\nd (\u03c4, K\nIP\u03b8 ( max L(ul , bl ) \u2264 H) \u2264\n1\u2264l\u2264N\n\nmin\n\n1\u2264l\u2264N\n\nVar\u03b8 (L(ul , bl ))\n.\n(IE\u03b8 (L(ul , bl )) \u2212 H)2\n\nDue to Lemma 7.1 and the fact that H = Od ((log d)C ) with C > 1/4, in order to obtain the result,\nit remains to prove that for any l such that bl \u2264 b \u2264 bl+1 ,\n\ninf\n\n1/2 ,K 1/2 r ,b)\n\u03b8\u2208\u0398ext\n\u01eb\nd (\u03c4,K\n\nd\u2192+\u221e\n\nIE\u03b8 (L(ul , bl )) \u2212\u2192\n\n+\u221e as a positive power of d. Finally, recall that\nIE\u03b8 (L(ul , bl ))\n\n= Cul ,bl\n\nK \u0010\n\u0011\nX\nIP\u03b8j (tj,bl \u2212 IE\u03b8j (tj,bl ) > ul Td \u2212 IE\u03b8j (tj,bl )) \u2212 \u03a6\u03030,bl (ul Td ) (, 7.16)\nj=1\n\nwhere Cul ,bl = (d\u03a6\u03030,bl (ul Td )(1 \u2212 \u03a6\u03030,bl (ul Td )))1/2 and \u03a6\u03030,bl (x) = IP0 (tj,bl > x). The term on the\nright-hand side of (7.16) corresponds to the product of (7.15) and Cul ,bl . The quantity Cul ,bl is\ncontrolled by Lemma 7.1 and Proposition 7.1. Thus it remains to study (7.15).\nThird, the application of Proposition 7.1 gives the following approximation of (7.15),\nK\nX\nj=1\n\nIP\u03b8j (tj,bl \u2212 IE\u03b8j (tj,bl ) > ul Td \u2212 IE\u03b8j (tj,bl ))\n\n=\n\nK\nX\nj=1\n\nexp(\u2212\n\n((ul Td \u2212 IE\u03b8j (tj,bl ))+ )2\n)O(1).\n2\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n21\n\nRecall that a(r\u01eb ) given by (4.8) is the solution of the extremal problem (4.1). Set \u03b7j = IE\u03b8j (tj,bl ),\n2\n\n) \u2200\u03b7 \u2208 [0, R], where R = R(T ) >\n\u03b70 = (1+\u03b4)2 a(r\u01eb\u22c6 (b)) \u223c (1+\u03b4)2 a(r\u01eb\u22c6 (bl )) and fT (\u03b7) = exp(\u2212 (T \u2212\u03b7)\n2\n0 will be specified later on. Consider also\n\u2206\n\nFK,T (\u03b70 ) = inf\n\nK\nX\n\nfT (\u03b7j ) subject to\n\nj=1\n\nK\nX\nj=1\n\n\u03b7j \u2265 K\u03b70 .\n\nDue to relation (6.4), we have for the sequence w(r\u01eb\u22c6 (bl )) that\nK\nX\n\n\u03b7j =\n\nj=1\n\nK\nX\n\nIE\u03b8j (tj,bl ) =\n\nj=1\n\nK\n1 X\n\u03ba(\u03b8j , wl ) \u2265 K(1 + \u03b4)2 a(r\u01eb\u22c6 (bl )) \u223c K\u03b70 .\n\u01eb2 j=1\n\nThen, in order to obtain the same right-hand side as in (7.14), it is sufficient to show that for\nany l in {1, . . . , N } such that T = ul Td , relation (7.17) which is stated below, holds:\nFK,T (\u03b70 ) = KfT (\u03b70 ).\n\n(7.17)\n\nThis is handled by a technical result similar to the one stated in Lemma 7.4 and Lemma 7.5 in\nIngster et al. [17]. The proof of Lemma 7.2 is postponed to Section 7.4.\nLemma 7.2. Set \u03bb = (T \u2212 \u03b70 )fT (\u03b70 ).\nIf 0 < \u03b70 < T \u2212 1 and T < R < T + ((T \u2212 \u03b70 )2 \u2212 2 log(1 + 2(T \u2212 \u03b70 )2 ))1/2 ,\n\n(7.18)\n\ninf (fT (\u03b7) \u2212 \u03bb\u03b7) = fT (\u03b70 ) \u2212 \u03bb\u03b70 ,\n\n(7.19)\n\nFK,T (\u03b70 ) = KfT (\u03b70 ).\n\n(7.20)\n\nthen\n\u03b7\u2208[0,R]\n\nwhich implies that\n\nAs d \u2192 +\u221e, for any l \u2208 {1, . . . , N } such that T = ul Td with ul > (1+\u03b4)2 \u03c6(b) and R = pTd with\n2\n\u03c6(b)\nul < p < ul + ul \u2212(1+\u03b4)\n, the conditions in (7.18) are then satisfied. Therefore the application\n2\n1/2\nof Lemma 7.2 yields the results since for all \u03b8 \u2208 \u0398ext\n, K 1/2 r\u01eb , b),\nd (\u03c4, K\n\u0013\n\u0012\nul Td\n((ul Td \u2212 (1 + \u03b4)2 a(r\u01eb\u22c6 (bl )))+ )2\n)O(1) \u2212 exp(\u2212\n(1 + o(1)) ,\nIE\u03b8 (L(ul , bl )) > Cul ,bl K exp(\u2212\n2\n2\nwhich corresponds to the right-hand side of (7.14).\n7.3. Lower Bound\nThe prior distribution we consider is a classical one for a functional Gaussian model. In Section\n4.3 of [13] it is referred to as the symmetric Three-point Factors.\nPrior. Before defining the prior \u03a0d formally, we shall start with an informal discussion.\nThe prior \u03a0d adds mass on (\u03bej \u03b8j )1\u2264j\u2264d : the components are i.i.d. and \u03bej and \u03b8j are supposed\nto be independent. A natural choice for \u03bej is a Bernoulli with a parameter pd \u2208 (0, 1) such that\nPd\nIE( j=1 \u03bej ) \u223c K. The \u03b8j 's are binary random variables (with probability 1/2) such that \u03b8j2 = (\u03b8\u22c6 )2\nwhere the sequence \u03b8\u22c6 is a solution of the extremal problem (3.1); this guarantees that \u03b8j belongs\nto \u0398(\u03c4, r\u01eb ).\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n22\n\nNow, we define the prior distribution more precisely. Let \u03c1d be any sequence of positive numbers\nd\u2192+\u221e\nd\u2192+\u221e\nsuch that \u03c1d \u2212\u2192 0 and d1\u2212b \u03c1sd \u2212\u2192 +\u221e, \u2200b \u2208 (0, 1), \u2200s > 0. Consider two sequences (\u03bej )j and\n(\u03b8j,k )j,k of independent random variables whose distributions are the following:\n\u001a\n\u03bej \u223c Bernoulli B(pd ) with pd = d\u2212b (1 + \u03c1d ), j \u2208 {1, . . . , d},\nZ.\n\u03b8j,k = \u03b5j,k \u01eb zk , with IP(\u03b5j,k = 1) = IP(\u03b5j,k = \u22121) = 21 , j \u2208 {1, . . . , d}, k \u2208 Z\nThe sequence (zk )k\u2208ZZ is deterministic and is defined as follows: (\u01eb zk )k = (\u03b8k\u22c6 )k\u2208ZZ = \u03b8\u22c6 where \u03b8\u22c6\nis the sequence that leads to the solution (4.8) of the extremal problem (4.1). In particular, this\nentails that\nX z4\nk\n= a2 (r\u01eb ),\n(7.21)\n2\nk\u2208Z\nZ\nX\n(2\u03c0)2\u03c4\n|k|2\u03c4 (\u01ebzk )2 \u2264 1,\n(7.22)\nk\u2208Z\nZ\n\nX\n\n(\u01eb zk )2\n\nk\u2208Z\nZ\n\n\u2265\n\nr\u01eb2 .\n\n(7.23)\n\nThe sequences (\u03bej )j and (\u03b8j,k )j,k are also taken mutually independent. For each j in {1, . . . , d}, we\ndefine the prior distribution \u03c0jd on (\u03bej , \u03b8j ) as follows:\nY\n\u03c0jd = (1 \u2212 pd )\u03b40 + pd\n\u03c0j,k = (1 \u2212 pd )\u03b40 + pd \u03c0j ,\n(7.24)\nk\u2208Z\nZ\n\nQ\n\nwhere \u03c0j = k\u2208ZZ \u03c0j,k , \u03c0j,k = 21 (\u03b4(\u2212\u01eb zk ) +\u03b4(\u01eb zk ) ) puts mass on \u03b8j,k and \u03b4 is the Dirac mass. Finally,\nwe define the global prior \u03a0d by\n\u03a0d =\n\nd\nY\n\n\u03c0jd .\n\nj=1\n\nMinimax and Bayesian risks. Denote by IP\u03a0d the mixture of the measures IP\u03b8 over the prior\n\u03a0d , and let \u03b3(Q) be the minimal total error probability for testing a simple null hypothesis H0 :\nIP = IP0 against a simple alternative H1 : IP = Q regarding the measure IP of our observations\n(xj,k )k\u2208ZZ,1\u2264j\u2264d .\nProposition 7.2.\n\u03b3 \u2265 \u03b3(IP\u03a0d ) + o(1),\n\n(7.25)\n\nwhere \u03b3 is the minimax total error probability over \u0398d (\u03c4, r\u01eb , b) (see (3.2)).\nProof of Proposition 7.2.\nConsider two sets \u039e(s) and \u039e+ (s) defined by\n\u039e(s) = {\u03b6 = \u01eb (\u03be1 (\u03b51,k zk )k , . . . , \u03bed (\u03b5d,k zk )k ) :\n\nd\nX\n\n\u03bej = s},\n\nj=1\n\n0 \u2264 s \u2264 d,\n\n\u039e+ (s) =\n\n[\n\n\u039e(l).\n\ns\u2264l\u2264d\n\nFirst, due to relations (7.22) and (7.23), \u039e(K) is included in \u0398d (\u03c4, r\u01eb , b). This entails that\n\u03b3 \u2265 \u03b3(\u039e(K)).\n\n(7.26)\n\nSecond, Q\nlet us introduce some additional priors: for any subset u \u2282 {1, . . . , d}, define \u03c0u =\nQ\nj \u2208u\n/ \u03b40 , where \u03c0j is as in (7.24). Note that \u03c0u has a support on the collections\nj\u2208u \u03c0j\n\u03b6 = \u01eb (\u03be1 (\u03b51,k zk )k , . . . , \u03bed (\u03b5d,k zk )k ) with \u03bej = 1 if and only if j \u2208 u. For any integer s such\nthat 0 \u2264 s \u2264 d, let Gd,s be the set of all subsets u \u2282 {1, . . . , d} of cardinality s, and define \u03c0(s) as\nthe uniform distribution on Gd,s :\n1 X\n\u03c0u .\n\u03c0(s) = d\u0001\ns\n\nu\u2208Gd,s\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n23\n\nP\nObserve that the prior \u03a0d is of the form \u03a0d = ds=0 rs \u03c0(s) where rs = psd (1 \u2212 pd )d\u2212s . Clearly,\n\u03c0(K) (\u039e(K)) = 1, which implies\n(7.27)\n\u03b3(\u039e(K)) \u2265 \u03b3(IP\u03c0(K) ).\nThird, consider the conditional prior of the form \u03a0d+ with respect to \u039e(K)+ , i.e.,\nd\nX\n\u03a0d (A \u2229 \u039e(K)+ )\nrs\nd\n, K \u2264 s \u2264 q.\n\u03a0d+ (A) =\nwhich\nis\nof\nthe\nform\n\u03a0\n=\nqs \u03c0(s) with qs = Pd\n+\n\u03a0d (\u039e(K)+ )\ns=K rs\ns=K\nLet us prove that\n(7.28)\n\u03b3(IP\u03c0(K) ) \u2265 \u03b3(IP\u03a0d+ ).\ndIP\u03c0(K)\n((xj,k )j,k ) < 1} the admissible set of the optimal test for\ndIP0\ntesting H0 : IP = IP0 against H1 : IP = IP\u03c0(K) . Since\n\nDenote by XK = {(xj,k )j,k :\n\n\u03b3(IP\u03c0(K) ) = 1 \u2212 IP0 (XK ) + IP\u03c0(K) (XK )\n\nand\n\n\u03b3(IP\u03a0d+ ) \u2264 1 \u2212 IP0 (XK ) + IP\u03a0d+ (XK ),\n\nproving (7.28) is then reduced to checking that\nIP\u03c0(K) (XK ) \u2265 IP\u03a0d+ (XK ).\n\n(7.29)\n\nIn view of Proposition 2.5 in [13], XK is a convex set. Also, the set XK is sign-invariant and\ninvariant with respect to all permutations of the xj,k 's; the measures IP\u03c0(s) , 0 \u2264 s \u2264 d have the\nsame property of invariance with respect to all permutations of the xj,k 's. These observations imply\nIP\u03c0(K) (XK ) = IP\u03b8 K (XK ),\n\nIP\u03a0d+ (XK ) =\n\nd\nX\n\nqs IP\u03b8s (XK ),\n\ns=K\ns\n\ns\nK\nwhere \u03b8 = \u01eb(z, . . . , z , 0, . . . 0), z = (zk )k\u2208ZZ . Since \u03b8j,k\n\u2265 \u03b8j,k\n\u2265 0, \u2200 j, k, s \u2265 K, the application\n| {z }\ns\n\nof Lemma 2.4 in [13] entails that IP\u03b8\u0304K (XK ) \u2265 IP\u03b8\u0304s (XK ), s \u2265 K. This yields relation (7.29) and\nhence relation (7.28).\nFinally, in view of Proposition 2.11 in [13], it remains to check that\n\u03b3(IP\u03a0d+ ) = \u03b3(IP\u03a0d ) + o(1).\n\n(7.30)\n\nSimilarly to the proof of Proposition 2.9. in [13], it is easily seen that (7.30) follows from the\nrelation\nd\u2192+\u221e\n\n\u03a0d (\u039e+ (K)) \u2212\u2192 1.\n\n(7.31)\n\nActing as in the proof of Proposition 3 in [12], we obtain by Chebyshev's inequality,\nX\n1 \u2212 \u03a0d (\u039e+ (K)) = \u03a0d (\n\u03bej < d1\u2212b )\nX\n= \u03a0d (dpd \u2212\n\u03bej > dpd \u2212 d1\u2212b )\n\u2264\n\nd1\u2212b (1 + \u03c1d )(1 \u2212 d\u2212b (1 + \u03c1d ))\n,\n(d1\u2212b \u03c1d )2\n\nwhere the ratio on the right-hand side tends to zero as d goes to infinity. Relation (7.31) is then\nproved.\nAs relations (7.26), (7.27), (7.28), and (7.30) imply (7.25), the proof of Proposition 7.2 is completed.\n\u2206\n\nDue to Proposition 7.2. the proof of the lower bound is reduced to bounding \u03b3 \u22c6 = \u03b3(IP\u03a0d ) from\nbelow.\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n24\n\nBefore studying \u03b3 \u22c6 , we introduce some useful notation and make some helpful remarks. Denote\nby k * kT V and k * k2 the distance in variation and the L2 -distance between any pair of probabilities\n(P, Q); the latter one is defined by\n\u001a\n+\u221e\nif P does not dominate Q,\n2\nkP \u2212 Qk2 =\n(7.32)\nIEP (L \u2212 1)2 if P dominates Q,\ndQ\nis the Radon-Nikodym derivative of Q with respect to P .\ndP\nRemark 7.3. Note that\nwhere L =\n\n\u2022 k * kT V = k * k1 , where k * k1 is the L1 -distance.\n\u2022 If P dominates Q, then kP \u2212 Qk22 = IEP (L2 ) \u2212 1.\n\u2022 As stated in Proposition 2.12 of [13],\nIf kP \u2212 Qk2 = o(1), then kP \u2212 Qk1 = o(1),\n\nIf kP \u2212 Qk2 is bounded, then lim sup kP \u2212 Qk1 < 2.\nUsing Remark 7.3, one has\nIf kIP0 \u2212 IP\u03a0d k2 = o(1) then\n\nIf kIP0 \u2212 IP\u03a0d k2 = O(1)\n\nthen\n\nkIP0 \u2212 IP\u03a0d k1 = o(1) and \u03b3 \u22c6 \u2192 1.\n\n(7.33)\n\u22c6\n\nlim sup kIP0 \u2212 IP\u03a0d k1 < 2 and lim inf \u03b3 > 0.\n\n(7.34)\n\nTherefore, if needed, the L2 -distance can be conveniently used instead of the total variation distance.\nDue to (7.33) and (7.34), it remains to study kIP0 \u2212 IP\u03a0d k2 which is expressed in terms of the\ndIP\u03a0d\n(see relation (7.32)) .\nBayesian likelihood ratio L\u03a0d =\ndIP0\nLikelihood Ratios. Here and below, when it is not absolutely necessary, we omit the arguments\nof the likelihood ratios. Then, observe that L\u03a0d is defined by:\nL \u03a0d\n\n=\n\n=\n\n=\n\nZ Y\nd\ndIP\u03b8j\n) d\u03a0d\n(\ndI\nP\n0\nj=1\n\nd Z\nY\ndIP\u03b8j\n(\n) d\u03c0jd\ndI\nP\n0\nj=1\nd\nY\n\n(1 \u2212 pd + pd Lj ),\n\nj=1\n\nwhere Lj is the likelihood ratio between IP\u03c0j and IP0 . Denote also by L\u03c0jd the likelihood ratio\nbetween IP\u03c0jd and IP0 , i.e., L\u03c0jd = (1 \u2212 pd + pd Lj ). Then Lj is such that\nLj (xj )\n\nZ Y\ndIP\u03b8j,k\n(xj,k ))d\u03c0j\n(\ndIP0\nk\u2208Z\nZ\n\u0013\nY 1\u0012\nz2\nz2\n=\nexp(\u2212 k + zk xj,k /\u01eb) + exp(\u2212 k \u2212 zk xj,k /\u01eb)\n2\n2\n2\n\n=\n\nk\u2208Z\nZ\n\n=\n\nY\n\nk\u2208Z\nZ\n\nexp(\u2212\n\nzk2\n) cosh(zk xj,k /\u01eb),\n2\n\n(7.35)\n\nwhere cosh is the hyperbolic cosine. Using routine calculations, in particular, using twice the\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n25\n\ninequality 1 + x \u2264 exp(x), \u2200x \u2208 IR, we obtain\nIE0 (L2\u03c0d )\nj\n\n= 1 + p2d {IE0 (L2j ) \u2212 1}\n= 1 + p2d {\n\nY\n\n(1 + 2(sinh(\n\nk\u2208Z\nZ\n\n\u2264 1 + p2d {exp(\n\nX\n\n2(sinh(\n\nk\u2208Z\nZ\n\n\u2264 exp(p2d {exp(\n\nX\n\nzk2 2\n)) ) \u2212 1}\n2\nzk2 2\n)) ) \u2212 1}\n2\n\n2(sinh(\n\nk\u2208Z\nZ\n\nzk2 2\n)) ) \u2212 1}),\n2\n\nwhere sinh denotes the hyperbolic sine. In view of Remark 7.3, in order to study kIP0 \u2212 IP\u03a0d k2 , it\nsuffices to study IE0 (L\u03a0d \u2212 1)2 . The latter includes the quantity IE0 (L2\u03a0d ) that satisfies\n!\nd\n2\nX\nY\nz\nIE0 (L2\u03c0d ) \u2264 exp dp2d {exp(\n2(sinh( k ))2 ) \u2212 1} .\n(7.36)\nIE0 (L2\u03a0d ) =\nj\n2\nj=1\nk\u2208Z\nZ\n\nAs d goes to infinity, the right-hand side of (7.36) goes to one provided that\n\nd\u2192+\u221e\n\nd p2d (exp(A) \u2212 1) \u2212\u2192 0\n\nwith A =\n\nX\n\n2(sinh(\n\nk\u2208Z\nZ\n\nzk2 2\n)) .\n2\n\n(7.37)\n\nProof of (i)\u2013Theorem 5.1.\nRecall that by assumption b \u2208 (0, 1/2]. We shall distinguish between two cases depending on the\nvalues of r\u01eb with respect to r\u01eb\u22c6 defined in (5.9).\nCase 1: r\u01eb /r\u01eb\u22c6 = O(1). Since dp2d (a(r\u01eb\u22c6 ))2 = O(1), it follows that dp2d a2 (r\u01eb ) = O(1). Since dp2d is\nbounded away from zero, a2 (r\u01eb ) = O(1), and, due to Remark 4.2 and relations (4.10), we have\nP zk4\nz4\nz2\nsupk zk2 = o(1). This entails that sinh2 ( k ) \u223c k , which, due to (7.21), implies that A \u223c\n2 \u223c\n2\n4\na2 (r\u01eb ), and hence A = O(1). It now follows that exp(A) \u2212 1 \u224d A. Finally, we get\ndp2d (exp(A) \u2212 1) \u224d dp2d a2 (r\u01eb ) = O(1),\n\n(7.38)\n\nand the second part of (i) in Theorem 5.1 is proved.\nCase 2: r\u01eb /r\u01eb\u22c6 = o(1). Due to (7.38), we have dp2d (exp(A) \u2212 1) \u224d dp2d a2 (r\u01eb ), and since\no(1), relation (7.37) is trivially fulfilled.\n\na2 (r\u01eb )\n=\na2 (r\u01eb\u22c6 )\n\nProof of (i)\u2013Theorem 5.2.\nNow by assumption b \u2208 (1/2, 1). Due to the condition log(d) = o(\u01eb\u22122/(2\u03c4 +1) ), Remark 4.2, and\nrelations (4.10), supk zk2 = o(1). As in the moderate case, this yields A \u223c a2 (r\u01eb ), and thus we obtain\nd p2d (exp(A) \u2212 1) = dp2d exp(a2 (r\u01eb )(1 + o(1))).\n\n(7.39)\n\nAgain, we shall consider two cases depending on the values of r\u01eb with respect to r\u01eb\u22c6 , where r\u01eb\u22c6 is\nnow defined by (5.11).\nCase 1: Suppose that r\u01eb /r\u01eb\u22c6 = o(1). Then a(r\u01eb ) = o(Td ). Due to equation (7.39), this implies that\nrelation (7.37) is fulfilled.\nCase 2: Suppose that r\u01eb /r\u01eb\u22c6 = O(1) and let c(r\u01eb ) be a positive constant satisfying c2 (r\u01eb ) log(d) =\na2 (r\u01eb ). Then the right-hand side of (7.39) can be rewritten as follows:\ndp2d exp(a2 (r\u01eb )) =\n=\n\nd1\u22122b (1 + \u03c1d )2 exp(log(d)c2 (r\u01eb )(1 + o(1)))\nd1\u22122b+c\n\n2\n\n(r\u01eb )(1+o(1))\n\n(1 + \u03c1d )2 .\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n26\n\n\u221a\nTherefore, relation (7.37) is fulfilled provided that c(r\u01eb ) < 2b \u2212 1 = \u03c61 (b), where \u03c61 is defined\nin (2.2). This means that a successful detection is impossible if c(r\u01eb ) < \u03c61 (b), which corresponds\nto the intermediate sparsity case; in fact, the inequality c(r\u01eb ) < \u03c61 (b) is valid for any b \u2208 (1/2, 1)\nbut it could be improved for b \u2208 (3/4, 1). Indeed, for b \u2208 (3/4, 1), one can show that a successful\ndetection is impossible if c(r\u01eb ) is such that c(r\u01eb ) < \u03c62 (b), where the function \u03c62 is defined in (2.2),\nand for b \u2208 (3/4, 1), \u03c61 (b) < \u03c62 (b). That is why the improvement is possible and is achieved by\ndealing with a truncated version of the Bayesian likelihood ratio L\u03a0d . From now, let us consider\n\u221a\n\u221a\n1\n1\na(r\u01eb ) = c(r\u01eb ) log d with \u221a < c(r\u01eb ) < 2. The case c(r\u01eb ) \u2264 \u221a coincides with the intermediate\n2\n2\nsparsity case when b \u2208 (1/2, 3/4].\nThus, for some positive v, let us define L\u0302\u03a0d , the truncated likelihood ratio of L\u03a0d :\nL\u0302\u03a0d =\n\nd\nY\n\nL\u0302\u03c0jd =\n\nd\nY\n\n(L\u03c0jd )1I(l\u0303\n\nj=1\n\nj=1\n\nj \u2264a(r\u01eb )\n\n\u221a\n\n(2+v) log d)\n\n,\n\n(7.40)\n\nwhere\nl\u0303j = log(Lj ) +\n\n1 2\na (r\u01eb ).\n2\n\n(7.41)\n\nAlso put\nlj = log(Lj ),\n\n(7.42)\n\nwhere Lj is defined by (7.35). Now we introduce two new probability measures IP\u03bdj and IP\u03bcj\nexpressed in terms of IP0 as follows:\ndIP\u03bdj\ndIP0\ndIP\u03bcj\ndIP0\n\n=\n=\n\nexp(lj )\n,\nIE0 (Lj )\nexp(2lj )\n.\nIE0 (L2j )\n\n(7.43)\n(7.44)\n\nIn order to get a lower bound for the minimax total error probability, it is sufficient to prove (see\nthe proof of Theorem 4.1 in [11]) that IE0 ((L\u0302\u03a0d \u2212 1)2 ) = o(1), where L\u0302\u03a0d is defined in (7.40)\nprovided that\nIP0 (\n\nd\n\\\n\nj=1\n\np\n{l\u0303j \u2264 a(r\u01eb ) (2 + v) log d}) \u2192 1.\n\n(7.45)\n\nIn fact, it is enough to prove that\nd\nX\nj=1\n\np\nIP0 (l\u0303j > a(r\u01eb ) (2 + v) log d) \u2192 0.\n\n(7.46)\n\nRelation (7.46), and hence relation (7.45), follows from relation (7.47), which is a part of the next\nlemma whose proof is postponed to Section 7.4.\nLemma 7.3. Assume that r\u01eb \u2192 0 and log d = o(\u01eb\u22122/(2\u03c4 +1) ). If T > 0 is such that T = O(a2 (r\u01eb )),\nthen\n\u0012\n\u0013\nT2\nIP0 (l\u0303j > T ) \u2264 exp \u2212 2\n+ o(a2 (r\u01eb )) .\n(7.47)\n2a (r\u01eb )\nMoreover, if lim inf(T /a2 (r\u01eb )) > 1, then\n\u0012\n\u0013\n(T \u2212 a2 (r\u01eb ))2\n2\nIP\u03bdj (l\u0303j > T ) \u2264 exp \u2212\n+\no(a\n(r\n))\n,\n\u01eb\n2a2 (r\u01eb )\n\n(7.48)\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n27\n\nand if lim sup(T /a2 (r\u01eb )) < 2, then\n\u0013\n\u0012\n(T \u2212 2a2 (r\u01eb ))2\n2\n+ o(a (r\u01eb )) .\nIP\u03bcj (l\u0303j \u2264 T ) \u2264 exp \u2212\n2a2 (r\u01eb )\n\n(7.49)\n\nNext, it remains to prove that IE0 (L\u0302\u03a0d ) \u2192 1 and IE0 ((L\u0302\u03a0d )2 ) \u2192 1. This will entail the expected\nresult that IE0 ((L\u0302\u03a0d \u2212 1)2 ) = o(1).\nFirst, consider the term IE0 (L\u0302\u03a0d ):\nIE0 (L\u0302\u03a0d ) = \u03a0dj=1 IE0 (L\u0302\u03c0jd )\n= \u03a0dj=1 IE0 (1 + pd (Lj \u2212 1) \u2212 1IDj (pd (Lj \u2212 1) + 1))\n\u0010\n\u0011\n= \u03a0dj=1 1 \u2212 pd (IE0 (Lj 1IDj )) + (\u22121 + pd )IP0 (Dj )\n\nd\n\u0010\n\u0011\nX\nlog 1 \u2212 pd (IE0 (Lj 1IDj )) + (\u22121 + pd )IP0 (Dj ) ,\n= exp(\n\n(7.50)\n\nj=1\n\np\nwhere Dj = {l\u0303j \u2264 a(r\u01eb ) (2 + v) log d} and Dj denotes the complement of Dj . Relation (7.46)\nentails the convergence to zero of the second term in the log term of the right-hand side of (7.50).\nTherefore, in order to obtain IE0 (L\u0302\u03a0d ) \u2192 1, it is sufficient to prove that\ndpd (IE0 (Lj 1IDj )) = o(1).\n\n(7.51)\n\n\u221a\n\u221a\n2+v\n\u2212 1 is positive (c(r\u01eb ) < 2) for any positive v, we\nNote that IE0 (Lj 1IDj ) = IP\u03bdj (Dj ). Since\nc(r\u01eb )\ncan applied relation (7.48) of Lemma 7.3 to get\n\u0012\n\u0013\n\u221a\n1\ndpd IP\u03bdj (Dj ) \u2264 dpd exp \u2212 log(d)(( 2 + v \u2212 c(r\u01eb ))2 + o(1))\n2\n1\n\nd1\u2212b (1 + \u03c1d ) d\u2212 2 (\n\n\u221a\n2+v\u2212c(r\u01eb ))2 +o(1)\n\n,\n(7.52)\np\n\u221a\nwhere the right-hand side of (7.52) goes to zero as soon as c(r\u01eb ) < 2 + v \u2212 2(1 \u2212 b). This yields\nrelation (7.51).\nSecond, we need to study IE0 (L\u03022\u03a0d ):\n=\n\nIE0 (L\u03022\u03a0d )\n\n=\n\nd\nY\n\nj=1\n\nIE0 ((1 \u2212 pd (1 \u2212 Lj ))2 1IDj )\n\uf8eb\n\n= exp \uf8ed\n\nd\nX\nj=1\n\n\uf8f6\n\nlog(1 \u2212 2pd IE0 ((1 \u2212 Lj )1IDj ) + IE0 (p2d (1 \u2212 Lj )2 1IDj \u2212 1IDj ))\uf8f8 .\n\nSince the relations dIP0 (Dj ) = o(1) and dpd IE0 ((1 \u2212 Lj )1IDj ) = o(1) have been already proved, it\nis sufficient to show that dp2d IE0 ((1 \u2212 Lj )2 1IDj ) = o(1). To this end, observe that\n\u0001\n(7.53)\ndIE0 (p2d (1 \u2212 Lj )2 1IDj ) \u2264 2 dp2d IP0 (Dj ) + dp2d IE0 (L2j 1IDj ) .\n\nThe first term on the right-hand side of (7.53) tends to zero as d goes to infinity since dp2d =\ndd\u22122b (1 + \u03c1d )2 for b \u2208 (3/4, 1).\nTo study of the second term on the right-hand side of (7.53), we take into account the following\ntwo points:\n(i) since supk zk2 = o(1), we can apply Lemma 7.4 of Section 7.4 with h = 2, X = xj,k /\u01eb, and\nz = zk , and obtain\nexp(2l \u0303j ) =\n\nexp(2a2 (r\u01eb )).\n\n(7.54)\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n28\n\n\u221a\n2\n2\n\np\nwith T = a(r\u01eb ) (2 + v) log d, we\n(ii) since lim sup(T /a2 (r\u01eb )) < 2 is satisfied as soon as c(r\u01eb ) >\ncan applied relation (7.49) of Lemma 7.3, which jointly with relation (7.54) leads to\np\np\ndp2d IE0 (L2j 1IDj ) = dd\u22122b (1 + \u03c1d )2 IP\u03bcj (l\u0303j \u2264 a(r\u01eb ) log d (2 + v)) exp(2l \u0303j \u2212 a2 (r\u01eb ))\n\ndd\u22122b (1 + \u03c1d )2 \u00d7\n\u221a\n\u221a\n\u0012 2\n\u0013\na (r\u01eb )( 2 + v log d \u2212 2a(r\u01eb ))2\n2\n2\nexp \u2212\n+\na\n(r\n)\n+\no(a\n(r\n))\n\u01eb\n\u01eb\n2a2 (r\u01eb )\nlog d \u221a\n= dd\u22122b (1 + \u03c1d )2 exp(\u2212\n(( 2 + v \u2212 2c(r\u01eb ))2 + c2 (r\u01eb ) + o(1)))\n2\n\u221a\n2\n2\n1\n= dd\u22122b (1 + \u03c1d )2 d\u2212 2 ( (2+v)\u22122c(r\u01eb )) +c (r\u01eb )+o(1) .\n(7.55)\np\n\u221a\n2 + v \u2212 2(1 \u2212 b).\nThe expression on the right-hand side of (7.55) goes to zero as soon as c(r\u221a\n\u01eb) <\nThe last inequality is obtained by resolving the inequality 1 \u2212 2b \u2212 12 ( 2 + v \u2212 2x)2 + x2 < 0,\n\u221a\nwhere x is constrained to be larger than 22 . This implies that a successful detection is impossible\nas soon as c(r\u01eb ) < \u03c62 (b), where \u03c62 is defined by (2.2).\n<\n\n7.4. Appendix\n7.4.1. Proof of Lemma 7.2.\nIf there exists \u03bb such that (7.19) is valid, then equation (7.20) is obtained in adapting Lemma\nPK\n7.4.'s proof of [17]. Indeed, due to (7.19) and using the fact that j=1 \u03b7j \u2265 K\u03b70 , we obtain for all\n\u03b7j \u2208 [0, R], j \u2208 {1, . . . , K}:\nK\nX\n\nfT (\u03b7j )\n\nj=1\n\n\u2265\n\nK\nX\nj=1\n\ninf{fT (\u03b7j ) \u2212 \u03bb\u03b7j } + \u03bbK\u03b70\n\n\u2265 K(fT (\u03b70 ) \u2212 \u03bb\u03b70 ) + \u03bbK\u03b70\n\n= KfT (\u03b70 ).\n\n(7.56)\n\nOn the other hand,\nFK,T (\u03b70 )\n\n=\n\ninf\nP\n\n{(\u03b71 ,...,\u03b7K ):\n\n\u2264 KfT (\u03b70 ).\n\n\u03b7j \u2265K\u03b70 }\n\nK\nX\n\nfT (\u03b7j )\n\nj=1\n\n(7.57)\n\nRelations (7.56) and (7.57) yield relation (7.20).\nNow, let us prove that (7.18) implies (7.19). For this, set gT (\u03b7) = fT (\u03b7) \u2212 \u03bb\u03b7 and denote by gT\u2032\n(2)\nand gT the first and second derivatives of gT , respectively. Note that gT\u2032 (\u03b7) = (T \u2212 \u03b7)fT (\u03b7) \u2212 \u03bb,\nand we choose \u03bb = (T \u2212 \u03b70 )fT (\u03b70 ) to have gT\u2032 (\u03b70 ) = 0.\n(2)\n(2)\n(2)\nThe study of gT yields that gT > 0 for |T \u2212 \u03b7| > 1 and gT < 0 for |\u03b7 \u2212 T | < 1. Since\n0 < \u03b70 < T \u2212 1, this implies that gT\u2032 < 0 on [0, \u03b70 [, gT\u2032 (\u03b70 ) = 0, gT\u2032 > 0 on ]\u03b70 , T \u2212 1], gT\u2032 is\ndecreasing on ]T \u2212 1, T + 1], and gT\u2032 is increasing on ]T + 1, +\u221e[. Moreover, gT\u2032 (T \u2212 1) > 0 and\ngT\u2032 (T ) = \u2212\u03bb < 0, so that there exists t \u2208]T \u2212 1, T [ such that gT\u2032 (t) = 0. This yields that \u03b70 is a\nlocal minimum of gT . In order to prove that \u03b70 is a global minimum of gT , it is sufficient to show\nthat gT (R) \u2212 gT (\u03b70 ) > 0. Let us set R = T + x, with a positive real x. We already know that\nx < T \u2212 \u03b70 since gT (T + (T \u2212 \u03b70 )) = fT (\u03b70 ) \u2212 \u03bb(T + T \u2212 \u03b70 ) = fT (\u03b70 ) \u2212 \u03bb\u03b70 \u2212 2\u03bb(T \u2212 \u03b70 ) < gT (\u03b70 ),\nwhere the last inequality is valid because of the choice of \u03bb and T \u2212 \u03b70 . For x < (T \u2212 \u03b70 ), we obtain\ngT (R) \u2212 gT (\u03b70 )\n\nx2\n) \u2212 (T \u2212 \u03b70 )fT (\u03b70 )(T + x) \u2212 fT (\u03b70 ) + (T \u2212 \u03b70 )fT (\u03b70 )\u03b70\n2\nx2\n(7.58)\n> exp(\u2212 ) \u2212 fT (\u03b70 )(2(T \u2212 \u03b70 )2 + 1) > 0,\n2\n\n= exp(\u2212\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n29\n\nwhere inequality (7.58) is valid as soon as\nexp(\u2212\n\nx2\n(T \u2212 \u03b70 )2\n) > exp(\u2212\n)(2(T \u2212 \u03b70 )2 + 1) \u21d4 x < ((T \u2212 \u03b70 )2 \u2212 2 log(2(T \u2212 \u03b70 )2 + 1))1/2 .\n2\n2\n\nSince (7.18) implies (7.19), this completes the proof of Lemma 7.2.\n7.4.2. Proof of Lemma 7.3.\nThe proof of Lemma 7.3 requires an additional result stated as Lemma 7.4 below. For any j \u2208\n{1, . . . , d}, recall that lj and  \u0303\nlj are given by (7.42) and (7.41), respectively. For any j \u2208 {1, . . . , d}\nz4\n\nz2\n\nz2\n\nand k \u2208 Z\nZ, set l\u0303j,k = 4k \u2212 2k + log(cosh(zk xj,k /\u01eb)) and lj,k = \u2212 2k + log(cosh(zk xj,k /\u01eb)). Denote by\n\u039bj , \u039b\u0303j , and \u039b\u0303j,k the moment-generating functions of lj , l\u0303j , and  \u0303lj,k under IP0 , respectively. From\nequations (7.35) and (7.41), it turns out that for any h,\nY\n\u039b\u0303j (h) =\n\u039b\u0303j,k (h),\n(7.59)\nk\u2208Z\nZ\n\n\u039b\u0303j (h)\n\n= \u039bj (h) exp(h\n\na2 (r\u01eb )\n).\n2\n\n(7.60)\n\nz4 z2\nNext, define the function g\u0303 : (z, y) \u2192\n\u2212\n+ log(cosh(zy)), and observe that the following\n4\n2\nrelations hold:\nl\u0303j,k\n\n=\n\n\u039b\u0303j,k (h) =\n\ng\u0303(zk , xj,k /\u01eb),\nIE0 (exp(hg\u0303(zk , xj,k /\u01eb))).\n\n(7.61)\n\nLemma 7.4. Let X be a real standard Gaussian random variable. For any z = o(1) and any\nh = O(1),\nlog(IE(exp(hg\u0303(z, X)))) = h2\n\nz4\n+ o(z 4 ).\n4\n\nProof of Lemma 7.4.\nFor some \u03b4 > 0, consider the event E = {|zX| < \u03b4} and denote by E its complement in\nIR. We shall study the expectations G1 (h, \u03b4) = IE(exp(h log(cosh(zX)))1IE ) and G2 (h, \u03b4) =\nIE(exp(h log(cosh(zX)))1IE ) separately. At this point, we choose \u03b4 small enough (\u03b4 = o(1)) to\nsatisfy z\u03b4 \u22121 = o(1).\nFirst, let us study the term G2 (h, \u03b4). With the use of the inequality cosh(x) \u2264 exp(|x|), \u2200x \u2208 IR,\nand the fact that h = O(1), the routine calculations of exponential moments of a real Gaussian\nrandom variable lead to\nG2 (h, \u03b4) \u2264\n\n=\n\n=\n\u2264\n\u2264\n\nIE(exp(h|zX|)1I(|zX|\u2265\u03b4) )\n2IE(exp(hzX)1I(X\u2265\u03b4/z) )\nZ\n1\n1\n2\n\u221a\nexp(\u2212 (x \u2212 hz)2 ) 1I(x\u2265 \u03b4 ) dx exp( h2 z 2 )\nz\n2\n2\n+\n2\u03c0 IR\nz2\n1 \u03b4\n2 exp(h2 ) exp(\u2212 ( \u2212 hz)2 )\n2 \u0013z\n\u0012 2 2\n1\u03b4\n2 exp \u2212 2 + o(1) ,\n2z\n\n(7.62)\n\nwhere, with our choice of \u03b4, the right-hand side of (7.62) is small.\nNow, we move on to the term G1 (h, \u03b4). If \u03b4 is small enough, then |zX| is also small and the\nfollowing relation holds:\nlog(cosh(zX)) =\n\nz2 2 z4 4\nX \u2212 X + o(z 4 X 4 ).\n2\n12\n\n(7.63)\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n30\n\nThen the routine calculations of exponential moments as above lead to the following:\n\u0013 \u0013\n\u0012\n\u0012\nz4\nz2\nG1 (h, \u03b4) = IE exp h( X 2 \u2212 X 4 (1 + o(1))) 1IE\n2\n12\n\u0012\n\u0013\nz2 2\nz4\n= IE exp(h( X ))(1 \u2212 h X 4 (1 + o(1)))1IE\n2\n12\nh\n1\n= exp(\u2212 log(1 \u2212 hz 2 )) exp(\u2212 z 4 (1 + o(1)))\n2\n4\nh 2 h2 4\nh\n= exp( z + z (1 + o(1))) exp(\u2212 z 4 (1 + o(1)))\n2\n4\n4\nh 2 h2 4 h 4\n= exp( z + z \u2212 z + o(z 4 )).\n2\n4\n4\n\n(7.64)\n\nTaking h = O(1), z = o(1), \u03b4 = o(1) and z\u03b4 \u22121 = o(1) in relations (7.62) and (7.64) entails that\nG1 (h, \u03b4) = O(1), G2 (h, \u03b4) = O(exp(\u2212\u03b4 2 /(2z 2 )) = o(1), and therefore G2 (h, \u03b4)(G1 (h, \u03b4))\u22121 = o(1).\nNext, due to (7.62), (7.64) and using the fact that h = O(1), z = o(1), for small \u03b4 such that\nz0 \u03b4 \u22121 = o(1) and \u03b4 = o(1), we obtain\nlog(IE(exp(hg\u0303(z, X)))) = log(G1 (h, \u03b4) + G2 (h, \u03b4)) \u2212\n\nh 2 z4\n(z \u2212 )\n2\n2\n\nG2 (h, \u03b4)\nh 2 z4\n(z \u2212 )) + log(1 +\n)\n2\n2\nG1 (h, \u03b4)\nG2 (h, \u03b4)\nz4\n(1 + o(1))\n= h2 + o(z 4 ) +\n4\nG1 (h, \u03b4)\nG2 (h, \u03b4)(1 + o(1))\nz4\n)\n= (h2 + o(z 4 ))(1 +\n4\n4\nG1 (h, \u03b4)(h2 z4 + o(z 4 ))\n= (log G1 (h, \u03b4) \u2212\n\n= h2\n\nz4\n+ o(z 4 ),\n4\n\n(7.65)\n\nwhere relation (7.65) holds provided that\nG2 (h, \u03b4)\n= o(1).\n4\nG1 (h, \u03b4)(h2 z4 + o(z 4 ))\n\n(7.66)\n\nIt is then sufficient to prove (7.66) since (7.65) is the expected result of Lemma 7.4. Recall that\nh = O(1) and z = o(1) entail that G1 (h, \u03b4) = O(1) and G2 (h, \u03b4) = O(exp(\u2212\u03b4 2 /(2z 2 ))). Then,\n1 \u03b42\nit is sufficient to establish that exp(\u2212 2 )z \u22124 = o(1). The latter holds if we choose \u03b4 such that\n2z\np\n\u03b4 \u22121 = o((z log(z \u22121 ))\u22121 ).\nProof of Lemma 7.3.\nRemark 4.2 and relations (4.10) imply that supzk2 \u2264 z02 = o(1) as soon as log(d) = o(\u01eb\u22122/(2\u03c4 +1) ).\nk\n\nDue to (7.61), for any h such that h = O(1), Lemma 7.4 can be applied to the moment-generating\nfunction \u039bj,k (h).\nHere and later, we consider any j \u2208 {1, . . . , d} and any k \u2208 Z\nZ. Due to relations (7.59), (7.61),\n(7.21), (7.41), by applying Lemma 7.4 and using the exponential Chebyshev's inequality, we obtain\nfor any positive h such that h = O(1),\nIP0 (l\u0303j > T ) \u2264 \u039b\u0303j (h) exp(\u2212hT )\nh2\n\u2264 exp( a2 (r\u01eb ) \u2212 hT + o(a2 (r\u01eb ))).\n2\nThe minimum on the right-hand side of (7.67) is attained for h =\n\nT\na2 (r\u01eb )\n\n(7.67)\nwhich is positive and of\n\norder 1; this allows us to prove relation (7.47).\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n31\n\nDue to relations (7.61), (7.59), (7.21), (7.41), (7.43), by applying again Lemma 7.4 and using\nthe exponential Chebyshev's inequality, we obtain for any positive h such that h = O(1),\nIP\u03bdj (l\u0303j > T ) \u2264 IE\u03bdj (exp(l\u0303j h)) exp(\u2212hT )\n\na2 (r\u01eb )\n= \u039b\u0303j (h + 1) exp(\u2212\n\u2212 hT )\n2\n\u0013\n\u0012\na2 (r\u01eb )\n(h + 1)2 2\n2\na (r\u01eb ) \u2212\n\u2212 hT + o(a (r\u01eb )) ,\n= exp\n2\n2\n\nwhere the minimum in the right-hand side of (7.68) is attained for h =\n\nT\na2 (r\u01eb )\n\n(7.68)\n\n\u2212 1 which is positive\n\nand of order 1; this yields relation (7.48).\nRecall that under the assumption of Lemma 7.3, the quantity 2a2 (r\u01eb ) \u2212 T is positive. Therefore,\nfrom (7.61), (7.59), (7.21), (7.44), (7.41), and (7.60), applying Lemma 7.4 and using the exponential\nChebyshev's inequality, we get for any positive h such that h = O(1),\nIP\u03bcj (l\u0303j \u2264 T ) = IP\u03bcj (\u2212l\u0303j \u2265 \u2212T )\n\n= IE\u03bcj (exp(\u2212l\u0303j h)) exp(hT ))\n= IE0 (exp(\u2212l\u0303j h) exp(2l\u0303j )) exp(\u2212a2 (r\u01eb ))(\u039bj (2))\u22121 exp(hT )\n= \u039b\u0303j (2 \u2212 h)(\u039bj (2))\u22121 exp(\u2212a2 (r\u01eb ) + T h)\n\n= \u039b\u0303j (2 \u2212 h)(\u039b\u0303j (2))\u22121 exp(a2 (r\u01eb )) exp(\u2212a2 (r\u01eb ) + T h)\n\u0012\n\u0013\n1\n2 2\n2\n2\n= exp\n(2 \u2212 h) a (r\u01eb ) \u2212 2a (r\u01eb ) + T h + o(a (r\u01eb )) ,\n2\nwhere the minimum in the right-hand side of (7.69) is achieved for h = \u2212\n\nT\na2 (r\u01eb )\n\n(7.69)\n+ 2 which is\n\npositive and of order O(1); this yields relation (7.49). The proof of Lemma 7.3 is completed.\n\nAcknowledgements: We thank one of our colleagues for his contribution to improve the English\nlanguage of the paper.\nReferences\n[1] Bickel, P.J., Ritov, Y. and Tsybakov, A.B. (2009). Simultaneous analysis of Lasso and\nDantzig selector. Ann. Statist. 37 1705\u20131732.\n[2] Cai, T., Jin, J. and Low, M. (2007). Estimation and confidence sets for sparse normal\nmixtures. Ann. Statist. 35 2421\u20132449.\n[3] Donoho, D.L. (2006). Compressed Sensing. IEEE Transactions on Information Theory 52\n1289\u20131306.\n[4] Donoho, D. and Jin, J. (2004). Higher criticism for detecting sparse heterogeneous mixtures.\nAnn. Statist. 32 962\u2013994.\n[5] Huang, J., Horowitz, J.L. and Wei, F. (2010). Variable selection in nonparametric additive models. Ann. Statist. 38 2282\u20132313.\n[6] Ibragimov, I.A. and Khasminskii, R.Z. (1997). Some estimation problems on Infinite dimensional Gaussian white noise. In: Festschrift for Lucien Le Cam. Research papers in Probability and Statistics, 275\u2013296. Springer-Verlag, New York.\n[7] Ingster, Yu.I. (1997). Some problems of hypothesis testing leading to infinitely divisible\ndistributions. Math. Methods of Statist. 6 47\u201369.\n[8] Ingster, Yu.I. (2001). Adaptive detection of a signal of growing dimension. I. Math. Methods\nStatist. 10 395\u2013421.\n[9] Ingster, Yu.I. (2002). Adaptive detection of a signal of growing dimension. II. Math. Methods\nStatist. 11 37\u201368.\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\fG. Gayraud et al./Detection of sparse functional signals\n\n32\n\n[10] Ingster, Yu.I. and Lepski, O. (2003). On multichannel signal detection. Math. Methods\nStatist. 12 247\u2013275.\n[11] Ingster, Yu.I., Pouet, Ch. and Tsybakov, A.B. (2009). Classification of sparse highdimensional vectors. Phi. Trans. R. Soc. A. 367 4427\u20134448.\n[12] Ingster, Yu.I. and Suslina, I.A. (2002). On a detection of a signal of known shape in\nmultichannel system. Zapiski Nauchn. Sem. POMI 294 88\u2013112 (Transl. J. Math. Sci. (2005)\n127 1723\u20131736).\n[13] Ingster, Yu.I. and Suslina, I.A. (2003). Nonparametric goodness-of-fit testing under gaussian models. Lectures Notes in Statistics. vol. 169., Springer-Verlag, New York.\n[14] Ingster, Yu.I. and Suslina, I.A. (2005). On estimation and detection of smooth function\nof many variables. Math. Methods Statist. 14 299\u2013331.\n[15] Ingster, Yu.I. and Suslina, I.A. (2007). Estimation and detection of high-variable functions\nfrom Sloan-Wo\u017aniakowski space. Math. Methods Statist. 16 318\u2013353.\n[16] Ingster, Yu.I. and Suslina, I.A. (2007). On estimation and detection of a function from\ntensor product spaces. (In Russian). Zapiski Nauchn. Sem. POMI 351 180\u2013218. (Translation\nin: J. Math. Sci. (2008), 152 897\u2013920).\n[17] Ingster, Yu.I., Tsybakov, A.B. and Verzelen, N. (2010). Detection boundary in sparse\nregression. Electronic J. of Statistics 4 1476\u20131526.\n[18] Lin, Y. (2000). Tensor product space ANOVA model. Ann. Statist. 28 734\u2013755.\n[19] Raskutti, G., Wainwright, M.J. and Yu, B. (2011). Minimax-optimal rates for sparse\nadditive models over kernel classes via convex programming.\nhttp://arxiv.org/abs/1008.3654\n[20] Stone, Ch. (1985). Additive regression and other nonparametric models. Ann. Statist. 13\n689\u2013705.\n[21] Tibshirani, R. (1996). Regression shrinkage and selection via Lasso. J. Roy. Statist. Soc.\nSer. B. 58 267\u2013288.\n\nimsart-generic ver. 2011/11/15 file: Corrected-Gayraud-Ingster-Second-Round-Submitted.tex date: May 22, 2018\n\n\f"}