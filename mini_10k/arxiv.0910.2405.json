{"id": "http://arxiv.org/abs/0910.2405v1", "guidislink": true, "updated": "2009-10-13T14:19:01Z", "updated_parsed": [2009, 10, 13, 14, 19, 1, 1, 286, 0], "published": "2009-10-13T14:19:01Z", "published_parsed": [2009, 10, 13, 14, 19, 1, 1, 286, 0], "title": "Generating Concise and Readable Summaries of XML Documents", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0910.1606%2C0910.0407%2C0910.1806%2C0910.5113%2C0910.2852%2C0910.0920%2C0910.2117%2C0910.3267%2C0910.3222%2C0910.1299%2C0910.1322%2C0910.3933%2C0910.0857%2C0910.2251%2C0910.0084%2C0910.4168%2C0910.0733%2C0910.5128%2C0910.2127%2C0910.1985%2C0910.2623%2C0910.0513%2C0910.1617%2C0910.1165%2C0910.4744%2C0910.1018%2C0910.0047%2C0910.1292%2C0910.5805%2C0910.4534%2C0910.1994%2C0910.1882%2C0910.2066%2C0910.0097%2C0910.2555%2C0910.2314%2C0910.2220%2C0910.4554%2C0910.0245%2C0910.4700%2C0910.4081%2C0910.4064%2C0910.5862%2C0910.0800%2C0910.2109%2C0910.0576%2C0910.5216%2C0910.4513%2C0910.0795%2C0910.5076%2C0910.4181%2C0910.0373%2C0910.2808%2C0910.3104%2C0910.1661%2C0910.0158%2C0910.2418%2C0910.2811%2C0910.2161%2C0910.5120%2C0910.2828%2C0910.1622%2C0910.3742%2C0910.4087%2C0910.0069%2C0910.5263%2C0910.1623%2C0910.2718%2C0910.5689%2C0910.1534%2C0910.4506%2C0910.1142%2C0910.4561%2C0910.3960%2C0910.4196%2C0910.4608%2C0910.4942%2C0910.2139%2C0910.2495%2C0910.2265%2C0910.0332%2C0910.4271%2C0910.0844%2C0910.5616%2C0910.4025%2C0910.1346%2C0910.1934%2C0910.1797%2C0910.4141%2C0910.0483%2C0910.3394%2C0910.4395%2C0910.1194%2C0910.2056%2C0910.2405%2C0910.5741%2C0910.1339%2C0910.5261%2C0910.2073%2C0910.1430%2C0910.1209&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Generating Concise and Readable Summaries of XML Documents"}, "summary": "XML has become the de-facto standard for data representation and exchange,\nresulting in large scale repositories and warehouses of XML data. In order for\nusers to understand and explore these large collections, a summarized, bird's\neye view of the available data is a necessity. In this paper, we are interested\nin semantic XML document summaries which present the \"important\" information\navailable in an XML document to the user. In the best case, such a summary is a\nconcise replacement for the original document itself. At the other extreme, it\nshould at least help the user make an informed choice as to the relevance of\nthe document to his needs. In this paper, we address the two main issues which\narise in producing such meaningful and concise summaries: i) which tags or text\nunits are important and should be included in the summary, ii) how to generate\nsummaries of different sizes.%for different memory budgets. We conduct user\nstudies with different real-life datasets and show that our methods are useful\nand effective in practice.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0910.1606%2C0910.0407%2C0910.1806%2C0910.5113%2C0910.2852%2C0910.0920%2C0910.2117%2C0910.3267%2C0910.3222%2C0910.1299%2C0910.1322%2C0910.3933%2C0910.0857%2C0910.2251%2C0910.0084%2C0910.4168%2C0910.0733%2C0910.5128%2C0910.2127%2C0910.1985%2C0910.2623%2C0910.0513%2C0910.1617%2C0910.1165%2C0910.4744%2C0910.1018%2C0910.0047%2C0910.1292%2C0910.5805%2C0910.4534%2C0910.1994%2C0910.1882%2C0910.2066%2C0910.0097%2C0910.2555%2C0910.2314%2C0910.2220%2C0910.4554%2C0910.0245%2C0910.4700%2C0910.4081%2C0910.4064%2C0910.5862%2C0910.0800%2C0910.2109%2C0910.0576%2C0910.5216%2C0910.4513%2C0910.0795%2C0910.5076%2C0910.4181%2C0910.0373%2C0910.2808%2C0910.3104%2C0910.1661%2C0910.0158%2C0910.2418%2C0910.2811%2C0910.2161%2C0910.5120%2C0910.2828%2C0910.1622%2C0910.3742%2C0910.4087%2C0910.0069%2C0910.5263%2C0910.1623%2C0910.2718%2C0910.5689%2C0910.1534%2C0910.4506%2C0910.1142%2C0910.4561%2C0910.3960%2C0910.4196%2C0910.4608%2C0910.4942%2C0910.2139%2C0910.2495%2C0910.2265%2C0910.0332%2C0910.4271%2C0910.0844%2C0910.5616%2C0910.4025%2C0910.1346%2C0910.1934%2C0910.1797%2C0910.4141%2C0910.0483%2C0910.3394%2C0910.4395%2C0910.1194%2C0910.2056%2C0910.2405%2C0910.5741%2C0910.1339%2C0910.5261%2C0910.2073%2C0910.1430%2C0910.1209&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "XML has become the de-facto standard for data representation and exchange,\nresulting in large scale repositories and warehouses of XML data. In order for\nusers to understand and explore these large collections, a summarized, bird's\neye view of the available data is a necessity. In this paper, we are interested\nin semantic XML document summaries which present the \"important\" information\navailable in an XML document to the user. In the best case, such a summary is a\nconcise replacement for the original document itself. At the other extreme, it\nshould at least help the user make an informed choice as to the relevance of\nthe document to his needs. In this paper, we address the two main issues which\narise in producing such meaningful and concise summaries: i) which tags or text\nunits are important and should be included in the summary, ii) how to generate\nsummaries of different sizes.%for different memory budgets. We conduct user\nstudies with different real-life datasets and show that our methods are useful\nand effective in practice."}, "authors": ["Maya Ramanath", "Kondreddi Sarath Kumar", "Georgiana Ifrim"], "author_detail": {"name": "Georgiana Ifrim"}, "author": "Georgiana Ifrim", "links": [{"href": "http://arxiv.org/abs/0910.2405v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0910.2405v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.DB", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0910.2405v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0910.2405v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:0910.2405v1 [cs.IR] 13 Oct 2009\n\nGenerating Concise and\nReadable Summaries of XML\nDocuments\n\nMaya Ramanath, Kondreddi Sarath\nKumar, Georgiana Ifrim\n\nMPI\u2013I\u20132009\u20135-002\n\nMay 2009\n\n\fAuthors' Addresses\nMaya Ramanath\nMax-Planck Institute for Informatics\n66123 Saarbr\u00fccken, Germany\nKondreddi Sarath Kumar\nMax-Planck Institute for Informatics\n66123 Saarbr\u00fccken, Germany\nGeorgiana Ifrim\nBioinformatics Research Center\nAarhus University\nDK-8000 Aarhus C\n\n\fAbstract\nXML has become the de-facto standard for data representation and exchange,\nresulting in large scale repositories and warehouses of XML data. In order for\nusers to understand and explore these large collections, a summarized, bird's eye\nview of the available data is a necessity. In this paper, we are interested in semantic\nXML document summaries which present the \"important\" information available\nin an XML document to the user. In the best case, such a summary is a concise\nreplacement for the original document itself. At the other extreme, it should at\nleast help the user make an informed choice as to the relevance of the document to\nhis needs. In this paper, we address the two main issues which arise in producing\nsuch meaningful and concise summaries: i) which tags or text units are important\nand should be included in the summary, ii) how to generate summaries of different\nsizes.We conduct user studies with different real-life datasets and show that our\nmethods are useful and effective in practice.\n\nKeywords\nSummarization, XML\n\n\fContents\n1\n\nIntroduction\n1.1 Challenges in XML Summarization . . . . . . . . . . . . . . . .\n1.2 Our Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.1 Contributions and Organization . . . . . . . . . . . . . .\n\n2\n3\n4\n4\n\n2\n\nRelated Work\n\n5\n\n3\n\nSummarization Model\n3.1 A First Attempt . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2 Our Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2.1 Summarization Framework . . . . . . . . . . . . . . . . .\n\n7\n7\n8\n9\n\n4\n\nRanking Model\n4.1 Ranking Tag Units . . . . . . . . . . . . . . . . . . .\n4.2 Ranking Text Units . . . . . . . . . . . . . . . . . . .\n4.2.1 Text with Redundancy in its Tag Context . . .\n4.2.2 Text with Almost no Redundancy at Tag Level\n4.3 Handling Co-occurring Tags (and Text Values) . . . . .\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n12\n12\n14\n14\n16\n16\n\n5\n\nGenerating the Summary\n\n18\n\n6\n\nExperiments\n6.1 Datasets . . . . .\n6.2 Metrics . . . . .\n6.3 Evaluation Setup\n6.4 Results . . . . . .\n6.4.1 Analysis\n\n20\n20\n20\n21\n23\n23\n\n7\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\nConclusions and Future Work\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n.\n.\n.\n.\n.\n\n26\n\nAppendices\n\n28\n\nA\n\n29\n1\n\n\f1 Introduction\nWith the ubiquity of XML as the format of storage and exchange of data, we\ncan expect to see ever-growing repositories of XML documents. Exploration of\nthese collections requires the use of a diverse set of tools ranging from classifiers,\nclustering tools, data visualizers to mining software. One of the ways in which\nhuman-centric exploration can be made easier is to provide the user with a concise, summarized view of the information contained in an individual or in a set of\ndocuments. Consider the following scenario. Suppose there is a large corpus of\nXML documents, each of which describes a movie released in the last 30 years\n(for example, extracted from IMDB). A movie enthusiast wants to make a list of\ninteresting movies based on various criteria, such as, the genre, lead actors, directors, etc. She first decides to narrow the focus to just thrillers. However, she then\nhas to look into each document individually, since only then is it possible for her\nto tell whether the combination of actors, directors, etc. interests her. This would\nbe time-consuming if the documents in question contain hundreds of tags each.\nInstead, if short summaries of each document could be presented to her, she could\nuse these summaries to filter out movies she certainly would not be interested in.\nThe generation of such summaries is the problem we address in this paper.\nA generic summary summarizes the entire contents of the document by identifying and possibly rewriting the most important content. The implicit assumption\nregarding the user's information need is that she is interested in knowing \"what is\nin the document\" without having to read the document in its entirety. In this paper\nwe propose techniques to automatically generate concise, readable summaries of\nXML documents subject to size constraints.\nAs a concrete example of the type of summaries we are interested in, consider Figure 1.1. The original document in Figure 1.1(a) describes the movie\n\"2001: A Space Odyssey\" (from IMDB). All the tags in this snippet have semantics associated with them (they are not tags for formatting or display), and\nthere are short pieces of information at the leaf level, such as the title of the\nmovie, its director, genres, etc. A concise summary of this snippet is shown in\nFigure 1.1(b) where only the \"important\" information has been retained \u2013 some\n2\n\n\ftags and their text values have been dropped completely (several actors and\nproduction locations for example). The resulting summary is shorter but\nconveys all the important information from the original document.\ntitle\n\n2001: A Space Odyssey\n\nprod_year\n\n1968\nname\n\nproducer\n\nKubrick, Stanley\n\njob\n\nproducer\nKubrick, Stanley\n\ndirector\nactor\n\nDullea, Keir\n\nrole\n\nDavid Bowman\n\ncasting\n\ncast\nmovie\n\nactor\ncast\n\ncast\n\ncasting\n\n...\n\nactor\ncasting\n\nLockwood, Gary\nDr. Frank Poole\n\nrole\n\nrole\n\nSylvester, William\nDr. Haywood Floyd\n\nprod_location\n\nShepperton Studios, England\n\nprod_location\n\nBorehamwood, England\n\nprod_location\n\nIsle of Harris, Scotland\n\nprod_location\n\nMonument Valley, Utah, USA\nThis movie is concerned with\nintelligence as the division\nbetween animal and human...\n\nplot\n\n(a) Original Document\ntitle\n\n2001: A Space Odyssey\n\nprod_year\n\n1968\nname\n\nproducer\n\nKubrick, Stanley\n\njob\n\nproducer\nKubrick, Stanley\n\ndirector\nactor\ncast\n\ncasting\n\nmovie\n\nrole\nactor\n\ncast\n\ncasting\n\nrole\nactor\n\ncast\n\ncasting\n\nrole\n\nDullea, Keir\nDavid Bowman\nLockwood, Gary\nDr. Frank Poole\nSylvester, William\nDr. Haywood Floyd\n\nproduction_language\n\nEnglish\n\nplot\n\nThis movie is concerned with\nintelligence ...\n\ngenres\n\ngenre\n\nSci\u2212fi\n\n(b) Summary\n\nFigure 1.1: \"2001: A Space Odyssey\" \u2013 Snippet of original document and a possible summary\n\n1.1 Challenges in XML Summarization\nInformally, a summary is useful if, at the very least, it helps the user decide\nwhether a particular document is worth looking into in its entirety or not. The\n3\n\n\fbest summary would encapsulate most or all the salient points of the document\nand could, in many cases, serve as a replacement for the original document. However, generating such a summary may often involve a trade-off between coverage\nand importance. Important content may have to be sacrificed in order to improve\ncoverage. And coverage may have to be reduced to ensure all important content\nis included. The right balance between the two is required, and this balance has\nto be achieved given a limit on the summary size. While importance and coverage are factors that need to be considered in both text and XML summarization,\nthere is one additional source of complexity in XML summarization. That is, importance and coverage have to extend to both structure as well as content. And\nsince structure and content play different roles in the document and have different\ncharacteristics, different techniques may be needed to deal with them.\n\n1.2 Our Approach\nWe regard the problem of generating XML summaries as a two-stage problem.\nFirst, we separately rank tags and text according to a notion of their importance.\nNext, we construct the summary based on the tag and text scores. The choice of\ntag-text pairs is made such that the summary reflects only their relative importance in the document, thus achieving a balance between including importance\nand coverage.\n\n1.2.1 Contributions and Organization\nOur contributions are the following:\n\u2022 We present a formal model for the generation of XML summaries.\n\u2022 We propose techniques for ranking tags and text values based on their importance in the document and the corpus.\n\u2022 We propose an algorithm which takes the ranked tags and text values and\nconstructs a summary while strictly adhering to a size limit.\n\u2022 Finally, we test our techniques for summary generation with a user study\nusing real-life datasets.\nThe rest of this paper is organized as follows. Related work is discussed in\nSection 2. Section 3 discusses the data characteristics and our model for summarization. Section 4 discusses various techniques to prioritize tags and text values\nto be included in the summary. Section 5 describes the algorithm for generating\na summary given a size limit. Section 6 reports on our user study. Finally, we\nconclude in Section 7.\n4\n\n\f2 Related Work\nText summarization is a well-developed field (see, for example, [7] for an overview),\ndedicated to developing techniques for summarizing text documents. In a nutshell, the aim is to present important and non-redundant information contained in\na document to the user in a concise and readable manner. One way to tackle the\nproblem of summarization is to look at it as a ranking problem \u2013 text spans (say,\nsentences) are ranked according to a certain set of features and only the top-ranked\nspans are included in the summary. We follow this approach in our work. That\nis, we rank both tags and text values and consider them for inclusion in the summary according to their rank. However, the techniques for text summarization are\nnot directly applicable in our context for two reasons: i) the structure of an XML\ndocument may be as important as the text (the tags and tag hierarchy in XML are\nmeant for providing additional semantics) ii) the textual values which appear in\nXML documents are not always free-flowing text, for which text summarizers are\nmost suitable.\nXML has been used as markup in text documents to make them more feature\nrich and enable better text summarization (see for example, [11] for an overview).\nSummarization techniques are described in [1] for such feature-rich XML documents. However, the goal of these techniques is still to rank and extract the best\nsentences to be included in a summary. And the structural (XML) features of the\ndocument are specifically made use of to improve upon previous techniques for\nsummarizing text spans. Hence, the kind of XML document that is being summarized is still predominantly text while certain parts of the text are tagged. In\ncontrast, our work deals with documents which may or may not have free-flowing\ntext.\nThe work presented in [15] deals with XML Schema summarization, where\nthe goal is to present important schema elements to make large XML schemas\neasily readable by the user. Our work, on the other hand, is interested in summarizing XML documents of which structure is only one part and the data the other.\nAxPRE summaries, proposed in [3] generates summaries of the structure of a corpus of XML documents. The user can then interactively explore the repository\n5\n\n\fby selectively expanding parts of the summary. We differ from this work in two\nways - first, we do single document summaries and second, we take into account\nthe text values present in the document. In contrast, AxPRE can be regard as a\nmulti-document structural summarizer.\nOther approaches to representing XML data in a concise manner include compression (for example, [5]) and statistical summaries (for example, [6]). However,\nthe many tools for compression focus on efficient query processing and not on the\nreadability for the end-user. And statistical summaries are used mainly for cardinality estimation which feed into the query optimizer. Work on building tools for\ndatabase exploration (for example, [10, 14]) is also relevant in our context. However, our work looks at data-oriented XML while the focus of these tools is on\ndeveloping techniques to summarize the entire database (of relational tuples). On\nthe other hand, a document-oriented view is taken, for example, in [4]. But, there\nthe aim is to extract facets from a database consisting of text documents while\nwe aim at presenting the user with a concise and readable summary of individual\nXML documents.\nGenerating snippets of XML query results [8] is close to our work, but our\nsetting is different in that we consider stand-alone XML documents and rank elements without any query bias. Finally, our own previous work [13] presented\nideas for generic XML document summarization and a user study illustrating its\neffectiveness. The current paper provides a summarization model and formalizes\nthe scoring functions. In addition, we present a more extensive user evaluation\nand analysis of the results.\n\n6\n\n\f3 Summarization Model\n3.1 A First Attempt\nOne appealing scenario for generating summaries is to represent a document D\nas a set of its tag-text pairs sij , thus D = {sij |i \u2208 {1 . . . |T ags|} and j \u2208\n{1 . . . |T exts in T agi |}}. Let D be a document such that |D| = n. Let S,\nthe summary of D, have size m, i.e. it contains m tag-text pairs\u0001selected from\nn\nD. The potential number of candidate summaries for D is then m\nwhich makes\nit prohibitively expensive to generate each summary, score it and return the topranked summary. A simple alternative would be to estimate the m most likely\ntag-text pairs in D, and return that as its summary S. This method, however does\nnot work well, as shown in the following example.\nLet document D, chosen from a movie corpus, have the distribution shown in\nTable 3.1 on its tags and their corresponding text. Without a proper scoring mechanism for text values (all text values have equal probability, given the tag), we note\nthat: i) simply computing the joint probability tells us absolutely nothing about\nthe relative importance of the tag-text pairs, ii) we would have to rank actor,\nkeyword and trivia above title, even though the movie title is probably\nthe \"must-have\" tag in any movie summary. This is the direct consequence of\ncomputing the probability based on the \"local\" frequency of occurrence \u2013 that is,\nTags\n\ntitle\nactor\nkeyword\ntrivia\n\nTotal\n#tags\nin doc\n1\n4\n3\n2\n\nProb. of\nTags\n0.1\n0.4\n0.3\n0.2\n\nProb. of\nText given\nTag\n1\n0.25, 0.25 . . .\n0.33, 0.33 . . .\n0.5, 0.5\n\nJoint prob.\n\n0.1\n0.1, 0.1, . . .\n0.1, 0.1 . . .\n0.1, 0.1\n\nTable 3.1: Probability distribution on tag-text pairs in document with 10 elements.\n\n7\n\n\fCorpus\nStatistics\n\nSummary\nSize\n\nRanker\nTag\n\nXML\nDocument\n\nInfo Unit\nUnits\nGenerator\nText\nUnits\n\nTag\nRanker\n\nRanked\nTag Units\n\nText\nRanker\n\nRanked\nText Units\n\nSummary\nSummarizer\n\nFigure 3.1: Steps in Summarization\n\nthe number of times a tag occurs in a document may not directly correspond to its\nimportance, and finally, iii) choosing the summary with the maximum likelihood,\nof say, size 4 elements, would choose only actors, thus completely ignoring\ncoverage.\n\n3.2 Our Approach\nIn order to address the above problems, we start by first defining scoring functions\nfor both tags and text values separately. Since clearly, the frequency of occurrence\nof a tag in the document does not correlate with its importance, our scoring functions are based on a closer examination of the role of tags in XML and includes\nthe corpus statistics. Second, we provide methods to score text values which occur under the same tag. This is based on the premise that we can only compare\napples with apples \u2013 that is, it makes more sense to compare an actor with another\nactor (for example, \"Kate Winslet\" with \"Billy Zane\" in Titanic) and say which\nof them is more important (\"Kate Winslet\"), than comparing an actor with a keyword (\"Kate Winslet\" with \"Iceberg\"). Hence, our text ranking is \"local\" (within\nthe tag context), while our tag ranking is \"global\" (within the entire document).\nThe scoring functions for both take into account both the document as well as the\ncorpus statistics. Finally, we note from the previous discussion that choosing a\nsummary which maximizes likelihood does not ensure coverage. Instead, we approach the summary generation problem as a two step process: first, we constrain\nthe structure of the summary based on an importance distribution inferred from\nthe structure of the document and the corpus; second, given the fixed structure we\ncan focus on selecting the most important text associated with that structure.\n\n8\n\n\f3.2.1 Summarization Framework\nThe various components involved in our summarization framework are shown in\nFigure 3.1. The XML Document is taken as input into an Information Unit Generator module. This module generates two types of information units \u2013 tag information units and text information units. Following text summarization techniques,\nthese sets of information units are ranked according to importance by the Ranker\nmodule which also takes the corpus statistics as input to its ranking functions.\nThe Summarizer module takes as input the ranked lists of tag and text information\nunits, along with the size constraint. It chooses tag-text pairs to be included and\nrewrites them appropriately (for example, to reflect document order) to produce\nthe final summary.\nNext, we explain the functionality of these components in more detail.\n\nInformation Unit Generation\nAn XML document has two different types of content \u2013 tags and text. They play\ndistinct roles. Tags can be regarded as the metadata for a set of documents \u2013 that\nis, tags, nesting of tags and their value types are defined to express a specific class\nof information (for example, movies). This information could be encoded into\nschemas or DTDs which are typically much smaller than the corpus data. On the\nother hand, text values are required to \"instantiate\" a specific document. Hence,\nthe statistical properties of tags and text differ considerably. Unlike tags which\nare highly redundant in the context of a corpus, text values are much less so. For\nthis reason, we need to use different techniques to identify important tags and\nimportant text values. Our first step toward this goal is to generate separate sets\nof \"information units\" for tags and text from the document. We then rank each set\nof information units using different scoring functions.\nFigure 3.2 shows how the movie \"2001: A Space Odyssey\" is decomposed.\nFirst, the tag information units are identified \u2013 that is, each unique path from\nthe root to a leaf (without the text value). Second, the text information units are\nconstructed by putting together text values corresponding to each tag information\nunit. Thus text information units are always associated with a tag context. The\nranking of tag information units gives a global ordering on tags, while the ranking\nof text units gives a local ordering, within a tag context.\nFigure 3.2 shows how the tag units (on the left of the figure) and the text units\n(on the right of the figure) are related to each other. When no distinction between\ntag and text units is required, they are together termed information units.\n\n9\n\n\f1\n\nmovie\n\ntitle\n\nmovie\n\nprod_year\n\nmovie\n\nproducer\n\nname\n\nmovie\n\nproducer\n\njob\n\nmovie\n\ndirector\n\nmovie\n\ncast\n\ncasting\n\nactor\n\nmovie\n\ncast\n\ncasting\n\nrole\n\nmovie\n\nprod_location\n\nmovie\n\nplot\n\n2001: A Space Odyssey\n\n2\n\n1968\n\n3\n\nKubrick, Stanley\n\n4\n\nproducer\n\n5\n\nKubrick, Stanley\n\n8\n\n6 Dullea, Keir\n\nLockwood, Gary\nSylvester, William\n...\nDavid Bowman\nDr. Frank Poole\nDr. Haywood Floyd\n...\n\n7\n\n1\n2\n3\n4\n\n9\n\nShepperton Studios, England\nBorehamwood, England\nIsle of Harris, Scotland\nMonument Valley, Utah, USA\nThis movie is concerned with\nintelligence as the division\nbetween animal and human...\n\nFigure 3.2: Example document decomposed into its tag and text information units\n\nRanker\nThe ranker module takes the tag and text information units as input and scores\nthem according to an importance measure. Corpus statistics form a crucial input\nto the scoring functions. A document by itself may not give us enough information about the importance of tags or text units, as shown in the previous section.\nInstead, a measure of how tags and text values are distributed in a large corpus\ncontaining similar information gives us insights into what could be considered\nimportant. Note that in the ideal case, we would have an expert who would study\nthe corpus and tell us how to measure the importance of tags and text units. However, this is neither scalable nor often practical. Hence our aim is to define some\ngeneral principles which would work for many different kinds of documents and\ncorpora. And a crucial part of these principles is the use of corpus statistics. The\nscoring mechanisms are described in more detail in the next section.\nSummary Generation\nUsing the ranker module, we basically estimate a distribution of importance on\ntags and text units, from the document and the background corpus. Our summary\n10\n\n\fgeneration module takes these distributions as input in order to generate the summary of the required size. As previously mentioned, the summary is a sample of\nthe document based on the importance distribution computed by the ranker module. In addition to this sample, the summary generation unit may also rewrite\nthe summary to make it more readable. We currently support only one kind of\nrewrite function: the order of tags and text values in the summary will reflect the\ndocument order (and siblings in the summary will also be siblings in the original document). However, more complex rewritings are possible and are briefly\ndiscussed in Section 7.\nIn the next section we provide details on our design of the scoring functions\nfor both tags and text units. We then use the ranked lists to construct the summary\nand describe the process in Section 5.\n\n11\n\n\f4 Ranking Model\n4.1 Ranking Tag Units\nWe consider 2 criteria for considering a tag t important:\nTypicality: If t is salient in the corpus, then it is very likely that it defines the\ncontext of the documents. For example, title is the most salient tag in the\ncorpus, since it is present in all documents. And clearly, it sets the context for the\nrest of the document \u2013 that the given document is about the movie with the given\ntitle.\nSpecialty: If t is more or less frequent than in the \"average\" document in the corpus, then it is likely to denote a special aspect of the current document. For example, production location may occur once typically, but if the current document has 10 of those, then it implies that the film was shot in an unusually large\nnumber of locations. Also, if the current document contains oscar winner\nwhile the average document does not, then that too should be considered special.\nOur scoring function is a mixture model of two components (typicality and\nspecialty) with a parameter \u03b1 controlling the influence of each and is defined as\nfollows.\nP (Ti ) = \u03b1Ptyp (Ti ) + (1 \u2212 \u03b1)Pspe(Ti )\n\n(4.1)\n\nwhere P (Ti ) is the probability of choosing tag Ti , Ptyp (Ti ) and Pspe (Ti ) are\nthe probabilities of choosing Ti based on its typicality and specialty respectively,\nthe parameter \u03b1, 0 \u2264 \u03b1 \u2264 1 is set by the user or learned through examples. It now\nremains for us to describe how to compute the probabilities Ptyp and Pspe.\nTypicality As mentioned before, the typicality of the tag unit refers to \"common\nknowledge\" in the corpus. If it occurs in most or all documents, then the tag unit\nis considered very typical and ranked high. We quantify the typicality of a tag unit\nby measuring the fraction of documents in which the tag unit occurs (document\n\n12\n\n\ffrequency). That is, we define the typicality of tag unit Ti as,\n|D|Ti \u2208 D|\n|D|\nwhere the numerator is the document frequency of Ti (number of documents in\nwhich Ti occurs) and the denominator is the total number of documents in the\ncorpus C.\nThe tags can now be ranked in order of their typicality values \u2013 the higher\nthe typicality, the higher the rank. We normalize the typicality of tags to get a\nprobability distribution on the typicality values as follows:\ntyp(Ti ) =\n\nPtyp (Ti ) =\n\ntyp(Ti )\n\u03a3j typ(Tj )\n\n(4.2)\n\nSpecialty The specialty of a tag is characterized by how different the frequency\nof the tag in the current document is from an average document in the corpus. The\ncurrent document could contain a larger number or a smaller number of instances\nof a particular tag than the average document.\nIn order to construct the average document, we simply estimate for each tag,\nits average number of occurrences per document in the corpus:\n|Ti |Ti \u2208 C|\n|D|\nwhere the numerator is the number of times Ti occurs in the corpus and the denominator is the number of documents in the corpus. Now, in order to compute\nhow much a tag Ti deviates from this average document we use the following:\n\u001a\n\u001b\n|Ti |Ti \u2208 D| countavg (Ti |C)\ndev(Ti ) = max\n,\ncountavg (Ti |C) |Ti |Ti \u2208 D|\nwhere dev(Ti ) is the maximum of the ratio between the number of tags in the\ncurrent document and the number of tags in the average document and its reciprocal. We use the maximum value so that tags which occur less number of\ntimes as well as those which occur a greater number times than the average document are given equal consideration (for example, an unusually large number of\nproduction locations must be as important as an unusually small cast\ncompared to the average document).\nWe then compute the specialty as follows, where the numerator denotes the deviation of tag Ti and the denominator is the normalizing factor to get a probability\ndistribution of the specialty of tags.\ncountavg (Ti |C) =\n\nPspe(Ti ) =\n\ndev(Ti )\n\u03a3j dev(Tj )\n\n13\n\n(4.3)\n\n\f4.2 Ranking Text Units\nThe problem of ranking text units is more complex than that of tags. This is\nmainly because of the many different forms of text that can occur in a document.\nFor example, a document could contain free-flowing long text values, short text\nvalues, entities, etc. For each of these kinds of text, a different ranking mechanism\nwould make sense. We divide text into the following categories: i) entities and ii)\nregular text. Entities are treated holistically and our system currently supports\nproper names. Regular text could be long or short and can be reduced to a set of\nterms. In addition to these two types of text, we make another distinction with\nrespect to their occurrence. Ideally, we should rank text values of a given text unit\nwith respect to the other text values in the same unit. However, this is possible\nonly if the terms (or the entity) occur multiple times in the text unit. When such\nredundancy is not to be found within the context of the tag unit, we need to change\nthe context to take into consideration the document and the corpus. Examples of\nsuch text units could include the list of actors in a movie or the genres of the\nmovie, etc. Examples of text units which are redundant within the context of their\ntag unit include trivia items, plots, goofs etc.\nOur general model for text units, regardless of whether or not they have redundancy in the tag, document or corpus context is a mixture model defined as\nfollows:\nP (tj |D, Ti ) = \u03bbP (tj |D, c(Ti )) + \u03bcP (tj |D) + (1 \u2212 \u03bc \u2212 \u03bb)P (tj |C)\n\n(4.4)\n\nwhere the first term, P (tj |D, c(Ti )) denotes the probability of choosing text\nvalue tj within the context of Ti (denoted c(Ti )). The second and third terms,\nP (tj |D) and P (tj |C) denote the probability of tj in the document and the corpus\nrespectively.\nThe probability P (tj |C) mainly comes into play when tj has little or no redundancy in the tag context Ti 1 . In these cases \u03bb and \u03bc have to be set empirically\nor learned through examples. These values can be tuned depending on the corpus.\nWe next discuss how to estimate each of the above three probability distributions. Note that it is fairly easy to determine whether or not a text unit is redundant\nwithin its tag unit by examining the document.\n\n4.2.1 Text with Redundancy in its Tag Context\nWe would like to choose a set of text values which are representative of the text\nunit, while also being as diverse as possible. That is, we should choose values\n1\n\nNote that without any redundancy, the best conclusion we can come to is that each value is\nequally important\n\n14\n\n\fwhich are important while simultaneously increasing coverage. For the first goal\nof extracting the most representative or important of the text values, we use the\ncentroid query method. For the second goal of ensuring diversity, we utilize the\nconcept of maximal marginal relevance (MMR) proposed in [2].\nCentroid query method Let TEXT = {ti }, 1 \u2264 i \u2264 n be the text unit for the\ntag T . Let, TERM = {trmi |trmi \u2208 tj , 1 \u2264 j \u2264 n} be the set of terms occurring\nin any of the ti 's. Let F = (trmi ) be the sequence of terms from TERM sorted\nby their frequencies of occurrence in TEXT, where i denotes the rank of trm.\nWe now choose the top m terms from F to be the centroid query Q. That is,\nQ = {trmi |1 \u2264 i \u2264 m}. The set Q contains terms which are representative of the\ntext unit. We now compute the relevance (or similarity) of each text unit ti with\nrespect to Q:\nR(ti ) =\n\nf req(qj |qj \u2208 ti )\nX\ncount(tk |qj \u2208 tk )\n1\u2264j\u2264m\nX\n\n(4.5)\n\nk\n\nwhere the numerator is the term frequency of qj in text value ti while the\ndenominator is the number of text values tj which contain the term qj . The final\nscore is the sum over all terms of Q2 .\nDiversity The above relevance gives us a ranking of text values from the most\nrelevant to the least relevant. However, as stated before, our aim is to increase\ndiversity, while simply using the ranking above would give us values which are\n\"more of the same\". In order to increase diversity, we use the MMR metric proposed in [2]. The idea of the MMR metric is to do a re-ranking of the text units\nonce a particular text unit has been included in the summary. The re-ranking considers the text units not yet included in the summary and calculates a new ranking\nfor these text units based on their similarity to the already included text units and\ntheir relevance rank. In order to calculate the similarity between two text values, we first eliminate stop words in both text values and stem all the terms. The\nnumber of common terms between the two values gives us an estimate of their\nsimilarity.\nLet T = {t1 , t2 , ..., tm } be the set of text values already included and let T \u2032 =\n{t1 , t2 , ..., tk } be the set of text values yet to be included. To compute a new score\nfor the elements of T \u2032 , we use the following formula:\nS(ti ) = \u03b2R(ti ) \u2212 (1 \u2212 \u03b2) max\n(sim(ti , tj ))\nj\nt \u2208T\n\n2\n\nThis is analogous to the tf.idf scoring.\n\n15\n\n(4.6)\n\n\fwhere R(ti ) is calculated as shown above and sim(x, y) is calculated as\n|terms(x) \u2229 terms(y)| where terms(x) and terms(y) are the set of terms in text\nvalues x and y respectively.\nIn order to get a final ranked list of text values, we need to repeat the process\nn \u2212 1 times. That is, we first choose the highest ranked text value according to\nR(ti ). Then compute S(.) for the remaining text values to choose the second text\nvalue. Similarly, we repeat to choose the third text value and so on until we get\na final ranked list. Let T = (t1 , t2 , ..., tn ) be the n text values in text unit T in\nranked order. If there are negative scores in T , we perform the normalizing step\nof adding the minimum score in T plus 1 to all scores to convert them into positive\nvalues. We can then define,\nS(ti )\nP (ti |D, c(Tj )) = X\nS(tj )\n\n(4.7)\n\nj\n\n4.2.2 Text with Almost no Redundancy at Tag Level\nWhen there is no redundancy of text values at the tag level, then we need to look\nat the document and possibly the corpus in order to rank them. The documentcontext probabilities are calculated as,\n|ti |ti \u2208 D|\nP (ti |D) = X\n|tk |tk \u2208 D|\n\n(4.8)\n\nk\n\nwhere the numerator is the number of occurrence of ti in the document and\nthe denominator is the sum total of occurrences of all text values in this text information unit.\nAnalogously, at the corpus level, we simply count the number of occurrences\nof ti and normalize it as above to get P (ti |C).\n\n4.3 Handling Co-occurring Tags (and Text Values)\nSo far, we have described the ranking of tag units assuming that they occur independently of one another. However, since XML has a tree structure, it is often\nthe case that we find related tag units \u2013 tag units which are siblings of one another. An example of such an occurrence is a role occurring along with actor.\nClearly, the text values corresponding to these two tags should co-occur in the\nsummary. For example, we would like a role to appear with the actor who played\nthat role, rather than simply pair up the top-ranked actor value with the top-ranked\n16\n\n\frole value. One further aspect to consider is whether it makes sense to include all\nco-occurring tag units in the summary or only a subset of them is still acceptable.\nFor example, it is perfectly acceptable for an actor to appear without the corresponding role that he played, but would likely make no sense when a role\nappears without the corresponding actor.\nWe address these issues with our rankings of tag and text units as follows. Let\nthe co-occurring siblings under consideration for inclusion in the summary be:\nTsib = (T1 , T2 , ..., Tk ). Let rank(T1 ) \u2265 rank(T2 ) \u2265 ... \u2265 rank(Tk ). Then,\nCase 1: If rank(T1 ) = rank(T2 ) = ... = rank(Tj ), j \u2264 k, then all of\n{T1 , ..., Tj } should be included in the summary at one shot. Moreover, for the\ninclusion of text values corresponding to these siblings, we choose a Ti at random from among {T1 , ..., Tj } and include its best ranked text value ti . Then the\ncorresponding ts of the remaining tag units, regardless of their rank are chosen\nfor inclusion. If the desired size of the summary is exceeded because of this inclusion, then we can decide to either include only a subset and satisfy the size\nrequirements or to exceed the size limit. We currently consider size to be a hard\nconstraint and only include a subset.\nCase 2: If rank(T1 ) > rank(T2 ) (implying that it is also greater than the\nrest of the Ti s), then only T1 is included in the summary at the current time along\nwith its best ranked text value t1 . At a later stage in the summary construction, if\nwe also include T2 , then we include T2 as a sibling of T1 and choose a text value\nwhich co-occurs with t1 . The principle is repeated for the rest of the siblings. The\nreasoning is that because of its higher rank, T1 plays a more dominant role among\nthe siblings and can occur by itself (which indeed it has in the document and/or\ncorpus, otherwise, it would not have a higher score than the others).\nWe have now described techniques to rank tag and text units. The next step is\nto construct a summary of the required size, given these rankings.\n\n17\n\n\f5 Generating the Summary\nWe discussed the ranking of tags and text values and the rationale for generating\na summary S by the process of sampling its structure and its content from the\ncorresponding \"importance\" distributions for tag and text units. In this section,\nwe discuss summary generation in detail and outline practical issues that arise\nduring this process and our solutions to overcome them.\nTag\n\nProb.\n\nactor\nkeyword\ntrivia\n\n0.5\n0.3\n0.2\n\nNo. of tags\nin summary\n15\n9\n6\n\nTable 5.1: Number of tag units in a summary of size 30 spans\nAs a first step, we compute the number of tags of each type which should occur\nin the summary based on the estimated distribution on tags presented in Section 4.\nTo give a simple example, suppose we want a summary of 30 spans of a document\ncontaining just 3 tags \u2013 actor, keyword and trivia. Let their probabilities\nbe as shown in Table 5.1. Hence, the summary should contain 15 actors, 9\nkeywords and 6 trivia items. Once the structure of the summary is fixed,\nwe select the most likely text units for each of the tag types, in order to build the\nnecessary spans.\nHowever, we encounter a first problem in this setting. The document may not\ncontain the required number of tags (more exactly tag-text pairs). In our example, suppose the document contains just 2 keywords as opposed to the required 9,\nwe would not be able to sample according to the original distribution. In order\nto address this problem, we propose re-distributing the remaining \"tag-budget\"\nto the other tag types in the summary structure, by repeated sampling and renormalization of the importance distribution on tags, up to the desired summary\nsize.\n18\n\n\fAn example is shown in Table 5.2. Let the desired summary size |S| = 30. In\nRound 1, the initial summary size |S| = 0. In step 1.1, multiplying the desired\nsummary size with the probability of actor gives us 15 instances of this tag to\nbe added into the summary. Since the number of actors in the document is 30, we\ncan include the top 15 actors into the summary. However, in step 1.2, the number\nof keyword tags to be included turns out to be 9, while we have only 2 keyword\ntags in the document. We include both keywords into the summary and note that\na probability mass of 0.3 is available for redistribution. Continuing in step 1.3, we\ninclude 6 trivia into the summary. Since we still require 7 more tags, we need\nto continue to sample from the top. However, before we do so, we redistribute the\nprobability mass of keyword (0.3) in proportion to the remaining available tags.\nIn this case, actor, the top-ranked tag is still available for inclusion as is the\nbottom-ranked tag trivia. Hence, the probability is distributed in proportion\nto their existing probability mass before round 2. In round 2, we again start with\nactor in step 2.1. Repeating the calculations as in round 1, we end up with 5\nadditional actor and 2 trivia tags. Overall, the final summary consists of 20\nactor, 2 keyword and 8 trivia tags. Once the tags have been chosen, they\nneed to be filled with the appropriate text values. The top-ranked text values are\npreferred except when a co-occurring set of tags need to be populated as described\nin the previous section.\nStep\n\nTag\n\n1.1\n1.2\n1.3\n\nactor\nkeyword\ntrivia\n\n2.1\n2.2\n\nactor\nkeyword\ntrivia\n\nProb.\n\n#tags\nremaining\n\nRound 1: |S| = 0\n0.5\n30\n0.3\n2\n0.2\n15\nRound 2: |S| = 23\n5/7 \u2248 0.7\n15\n0\n0\n2/7 \u2248 0.3\n9\n|S| = 30\n\n#tags\nto be\nadded to\nS\n\n#tags\nactually\nadded\n(total #tags\nin S\n\n15\n9\n6\n\n15 (15)\n2 (2)\n6 (6)\n\n5\n0\n2\n\n5 (20)\n0 (2)\n2 (8)\n\nTable 5.2: Generating the summary with |S| = 30\n\n19\n\n\f6 Experiments\nThe goal of our experiments was to determine how good our techniques are in\ngenerating summaries of various sizes. We describe our datasets and metrics in\nmore detail in the following.\n\n6.1 Datasets\nWe used two datasets for both set of experiments which are summarized in Table\n6.1. Both datasets \u2013 Movie and People \u2013 were extracted from the IMDB corpus\n(available from http://www.imdb.com). Out of the corpus of available documents, 8 documents from each dataset were chosen for summarization. The list\nof these documents for each dataset is shown in Table 6.2.\nDataset\nMovie\n\n#files\n(corpus)\n200,000\n\nPeople\n\n150,000\n\nExample\ntags\ntitle,director\nactor, role\ngoofs, alt versions\nbirthdate, spouse\nacted in, biography\n\n#tags\n(unique)\n39\n\n11\n\nTable 6.1: Description of Datasets\n\n6.2 Metrics\nWe conducted an intrinsic evaluation of summaries of various sizes. That is, evaluators were asked to judge a summary in and of itself (see [12] for a more detailed\nexplanation of intrinsic evaluations). The evaluators were asked to provide a grade\nto the summary ranging from 1 to 7 (1 \u2013 extremely bad, 2 \u2013 pretty bad, 3 \u2013 bad,\n20\n\n\fDataset\n\nFilename\n\nMovie\n\nAmerican Beauty\nOcean's Eleven\nKill Bill Part II\nSaving Private Ryan\nThe Last Samurai\nThe Usual Suspects\nTitanic\n2001: A Space Odyssey\nMatt Damon\nBen Affleck\nTom Cruise\nLeonardo DiCaprio\n\nPeople\n\n#tags\ntotal\n832\n795\n153\n1121\n429\n617\n1681\n1107\n116\n136\n150\n79\n\nTable 6.2: Documents used for Summarization\n\n4 \u2013 ok, 5 \u2013 good, 6 \u2013 pretty good, 7 \u2013 perfect) based on whether the summary\nreflected its source. They were asked to consider the importance of the tag-text\npairs selected, the coverage offered, and to also take into account the hard restriction of summary size. In effect, they were asked to grade the summary based on\nhow well it made use of the space available.\nWe used a total of 6 evaluators to conduct the evaluations and each summary\nhad at least 3 evaluations by 3 separate evaluators (the setup is described in more\ndetail in the next section). Once they submitted their evaluations, we conducted\na short survey to understand how they arrived at their grade and their general\nimpressions. We report on both the grades of the evaluators as well as their impressions and infer some trends.\n\n6.3 Evaluation Setup\nAutomatically Generated Summaries The set of summaries generated for evaluation is tabulated in Table 6.3. A total of 8 summaries per document were generated for the movie documents while a total of 4 summaries per document were\ngenerated for the people documents. We experimented with 3 values of the parameter \u03b1 for choosing tags, from 1.0 (typicality only), 0.8 (0.8Ptyp + 0.2Pspe )\nand 0.6 (0.6Ptyp + 0.4Pspe ).\nWe did not generate a 5-element summary with \u03b1 = 0.6 for the movie documents since this value of \u03b1 eliminated the intuitively most important tag, title\n21\n\n\fMovie\nPeople\nTOTAL\n\nSize\n\u03b1\n5\n1, 0.8\n10 1, 0.8, 0.6\n20 1, 0.8, 0.6\n5\n1, 0.6\n10\n1, 0.6\n64+16 = 80\n\nTable 6.3: Summaries used for evaluation.\n\nfrom the summary, which made it not worth evaluating. For the people documents, we did not generate summaries for \u03b1 = 0.8 since the same summary was\ngenerated as for \u03b1 = 1.0.\nFor text selection, we chose approximately the same parameter values \u03bb =\n0.49 and \u03bc = 0.48 for short text and entities. This was in effect giving the tag and\ndocument context approximately the same importance, while the corpus context\nwas used as a last resort (often to resolve importance in the case of ties between\ntwo values). For long text, we used only the centroid query method to generate text\nvalues (recall that for long text, we only take the tag context into consideration).\nHuman-generated Summaries An important thing that we realized early on\nis that there is no single summary which can be considered the best. In fact,\nthere have been studies conducted for text summarization which show that even\nhuman-generated summaries may not agree on a majority of the content and that\nthe same person generating a summary at two different times may not generate the\nsame summary [12, 9]. Therefore, while we instructed the evaluators that more\nthan one summary of the same size could have the same grade, we also added\nan additional human-generated summary to each evaluation batch. The humangenerated summaries were constructed by people who did not participate in the\nfinal evaluation. Additionally, the evaluators were not told that human-generated\nsummaries were included in the evaluation and so, would not be biased by its\npresence. We were able to gain additional understanding of the potential of our\ntechniques with human-generated summaries as a basis for comparison.\nIn total, 112 summaries were evaluated by 6 different evaluators. Each summary was evaluated by at least 3 evaluators. Among the 112 summaries, 80 were\ngenerated by our techniques and 32 were human-generated summaries. A total of\n336 evaluations were collected (264 for the Movie dataset and 72 from the People\ndataset).\n\n22\n\n\f6.4 Results\nDataset\n\nMovie\n\nPeople\n\n1.0\n\n\u03b1 values\n0.8\n\n0.6\n\n8/8 (100%)\n8/8 (100%)\n7/8 (87.5%)\n23/24 (95.8%)\n\n5/8 (62.5%)\n7/8 (87.5%)\n7/8 (87.5%)\n19/24 (79.1%)\n\n\u2013\n1/8 (12.5%)\n4/8 (50%)\n5/16 (31.2%)\n\nTotal\n(across \u03b1)\n13/16 (81.25%)\n16/24 (66.6%)\n18/24 (75%)\n47/64 (73.4%)\n\n3/4 (75%)\n4/4 (100%)\n7/8 (87.5%)\n\n\u2013\n\u2013\n\u2013\n\n1/4 (62.5%)\n4/4 (100%)\n5/8 (62.5%)\n\n4/8 (50%)\n8/8 (100%)\n12/16 (75%)\n\nSize\n\n5\n10\n20\nTotal\n(across sizes)\n5\n10\nTotal\n(across sizes)\n\nTable 6.4: Tabulation of average and above average grades (4,5,6,7) across all\ndocuments. Grades are reported only if at least 2 evaluators agreed on it.\n\nIn the following, we report on the grades provided by the evaluators for various\nclasses of summaries only if the relevant grade has been provided by at least 2\nout of 3 evaluators. For specific examples of grades, please refer to Tables 6.5\nand 6.6, which tabulate the XML documents and the best, worst and as-goodas-human-generated summaries. For examples of summaries generated by our\nsystem, please refer to the Appendix.\nTable 6.4 presents the summary of our results. It shows the number of summaries graded average and above average for various values of \u03b1 and different\nsummary sizes. Each cell contains an entry of the form x/y (z%), where y is the\ntotal number summaries in that category (for example, 8 is the number 10-element\nsummaries with \u03b1 = 1.0, in the Movie dataset), x is the number summaries in that\ncategory which were graded average and above by at least 2 evaluators, while z\nshows the percentage. The \"Totals\" rows and column provide aggregated numbers\nacross all \u03b1 values and across all sizes for both datasets.\n\n6.4.1 Analysis\nImpact of \u03b1 It is clear from Table 6.4, that while \u03b1 = 1, 0.8 result in good\nsummaries, an \u03b1 = 0.6 value results in low quality summaries (especially for the\nMovie dataset). For the Movie dataset, the total (across all \u03b1), despite being well\nover the 50% mark, suffers because of the low grades for summaries with \u03b1 = 0.6.\nIf we eliminate these low quality summaries from our computation (for the Movie\n23\n\n\fdataset only), we find that a total of 15/16 (93.7%) of 10-element summaries and\n14/16 (87.5%) of 20-element summaries score average and above average grades.\nFor the People dataset, a lower value of \u03b1 slightly reduces the effectiveness of\nthe summaries. In total, 5/8 (62.5%) summaries were given average and above\naverage grades for this dataset.\nWe thus conclude that our evaluators preferred highly typical tags for the given\ndatasets. In order to understand why specialty was not playing a bigger role in\nthe grading process, we questioned the evaluators. Many evaluators expressed\nthe opinion that, while it was nice to see a special tag (in the Movie dataset,\ntrivia, goof, had low typicality and high specialty), they did not need to see\nmore than one or two of them. Whenever they felt that there were too many\nspecial tags, those summaries were ranked lower. We concluded from this that it\nwas not specifically the specialty component of our model that was at fault, but\nthe special tags in our current datasets. These tags were not all that appealing to\nthe evaluators. For example, an oscar winner tag, occurring multiple times\nmay have been much more appealing than trivia, but we did not have such a\ntag in our dataset.\nImpact of summary size It is clear from Table 6.4 that the best \u03b1 (in our\ncase \u03b1 = 1.0 for both datasets) results in consistently good summaries across\nall sizes (95.8% of summaries for Movie dataset and 87.5% of summaries for\nPeople dataset). This is an important point favoring our techniques. The larger\nthe summary, the larger the options to choose from and the larger the chance of\njunk being selected. The consistent good grades across the different sizes shows\nthat our techniques succeed in choosing the right elements for inclusion in the\nsummary as the desired size increases.\nImpact of text values Our summaries contained both long as well as short text\nvalues and entities. In order to understand how the choice of text values impacted the grades, we again questioned the evaluators. For long text values such\nas trivia, plot and goof, they were not particularly interested in the exact\nvalue chosen, but were happy to see that they were present in the summary. Evaluators who hand-generated the summaries also had a similar opinion \u2013 that they\ndidn't really see a good criterion to choose one text value over another, and that\nany of them would be acceptable.\nFor the short text values and entities, which are more easily readable, higher\nimportance was given. For values such as those for actor (in Movies) and\nacts in (in People), it was mandatory that the most important values (lead actors, famous movies) be chosen. In the case of less typical tags, such as keyword,\nonly the relevance of the keyword to the movie was taken into account, rather than\n24\n\n\fthe best keyword among those available in the source.\nHence, we conclude that it is extremely important to have robust techniques\nto choose the best values for entities and short text. And our techniques seem to\nwork well for entities and short text. For longer text, it does not seem to be all\nthat important. However, retaining the flexibility to generate diverse values may\nbe essential for other datasets.\nFile\n\nBest (grades 6,7)\nSize\n(\u03b1 = 1)\n\nAmerican Beauty\nKill Bill - 2\nOcean's Eleven\nSaving Private Ryan\nThe Last Samurai\nTitanic\nUsual Suspects\nCruise\n\n10\n5\n10\n5, 10, 20\n5, 10, 20\n10\n5, 10\n5\n\nWorst (grades 1\u20133)\nSize\n(\u03b1 = 0.6)\n(in most cases)\nAmerican Beauty\n10, 20\nKill Bill - 2\n5, 10, 20\nOcean's Eleven\n5, 10, 20\nSaving Private Ryan\n10\n2001: A Space Odyssey 10, 20\nTitanic\n5, 10\nUsual Suspects\n10\nFile\n\nTable 6.5: Best and Worst Summaries.\n\nAs-good-as human-generated\nFile\nSize\n(\u03b1 = 1)\nKill Bill - 2\n5\nSaving Private Ryan 5, 10, 20\nThe Last Samurai\n5, 10, 20\nUsual Suspects\n10, 20\nAmerican Beauty\n20\n\nTable 6.6: As-good-as (human-generated) Summaries.\n\n25\n\n\f7 Conclusions and Future Work\nOur focus in this paper was to provide general-purpose techniques to generate concise, generic summaries automatically for a given XML document. We proposed\na framework and model for ranking tags and text. We described an algorithm for\ngenerating size-constrained summaries. Finally, we showed through a user study\nthat our techniques are able to generate good summaries for a range of different\nsummary sizes and made recommendations on how to set the tuning parameters.\nThere are at least a couple of directions for future work. First, many evaluators\nwere of the opinion that the text values were sometimes too long (for tags such as\nplot, trivia, etc.). One direction of future work is to use text summarizers to\nshorten these values. We experimented with this in our previous work [13], but\nthere is a need for a more comprehensive model for rewriting both structure as\nwell as text. Second, it would be interesting to develop ranking functions for different kinds of text. In this work, we only considered entities and regular text. In\naddition, we may also consider numbers and special methods for ranking them.\n\n26\n\n\fBibliography\n[1] M. Amini, A. Tombros, N. Usunier, and M. Lalmas. Learning-based summarisation\nof XML documents. Information Systems, 2007.\n[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR, 1998.\n[3] M. Consens, F. Rizzolo, and A. Vaisman. AxPRE summaries: Exploring the (semi)structure of xml web collections. In Proc. of ICDE, 2008.\n[4] W. Dakka and P.G. Ipeirotis. Automatic extraction of useful facet hierarchies from\ntext databases. In Proc. of ICDE, 2008.\n[5] P. Ferragina, F. Luccio, G. Manzini, and S. Muthukrishnan.\nsearching XML data via two zips. In Proc. of WWW, 2006.\n\nCompressing and\n\n[6] J. Freire, J. Haritsa, M. Ramanath, P. Roy, and J. Sim\u00e9on. StatiX: Making XML\ncount. In Proc. of SIGMOD, 2002.\n[7] U. Hahn and I. Mani. The challenges of automatic summarization. IEEE Computer,\n11(33), 2000.\n[8] Y. Huang, Z. Liu, and Y. Chen. Query biased snippet generation in XML search. In\nProc. of SIGMOD, 2008.\n[9] H. Jing, R. Barzilay, K. McKeown, and M. Elhadad. Summarization evaluation\nmethods: experiments and analysis. In AAAI Sym. on Intelligent Summarization,\n1998.\n[10] L.V.S. Lakshmanan, J. Pei, and Y. Zhao. Qc-trees: An efficient summary structure\nfor semantic OLAP. In Proc. of SIGMOD, 2003.\n[11] K. Litkowski. Evolving XML summarization strategies in DUC 2005. In Proc. of\nDocument Understanding Workshop at the HLT/NAACL Annual Meeting, 2005.\n[12] I. Mani. Summarization evaluation: An overview. In Proc. of NTCIR Workshop,\n2001.\n\n27\n\n\f[13] M. Ramanath and K. Sarath Kumar. A rank-rewrite framework for summarizing\nXML documents. In Proc. of DBRank, 2008.\n[14] R. Saint-Paul, G. Raschia, and N. Mouaddib. General purpose database summarization. In Proc. of VLDB, 2005.\n[15] C. Yu and H.V. Jagadish. Schema summarization. In Proc. of VLDB, 2006.\n\n28\n\n\fAppendix A\nAll evaluated summaries and grade tabulation are available from\nhttp://mpi-inf.mpg.de/\u223cramanath/Summarization.\nExample summaries for The Last Samurai and The Usual Suspects (\u03b1 = 1.0, 5-element,\n10-element).\n<movie>\n<title> Last Samurai, The </title>\n<prod year> 2003 </prod year>\n<director> Zwick, Edward </director>\n<colorinfo> Color </colorinfo>\n<cast><casting>\n<actor> Cruise, Tom </actor>\n</casting></cast>\n</movie>\n<movie>\n<title> Usual Suspects, The </title>\n<prod year> 1995 </prod year>\n<prod lang> English </prod lang>\n<director> Singer, Bryan </director>\n<genres>\n<genre> Crime </genre>\n<genre> Thriller </genre>\n</genres>\n<colourinfo> Color (Technicolor) </colourinfo>\n<cast>\n<casting>\n<actor> Spacey, Kevin </actor>\n<role> Roger'Verbal'Kint </role>\n</casting>\n<casting>\n<actor> Byrne, Gabriel </actor>\n</casting>\n</cast>\n</movie>\n\nThe summary for Benjamin Affleck (\u03b1 = 0.8, 10-element).\n<person>\n<name> Ben Affleck </name>\n<produced>\n<movie> Crossing Cords </movie>\n</produced>\n<acts in>\n<movie> third wheel, the </movie>\n<role> Michael </role>\n\n29\n\n\f</acts in>\n<acts in>\n<movie> good will hunting </movie>\n<role> Chuckie Sullivan </role>\n</acts in>\n<acts in><movie> voyage of the mimi, the </movie>\n</acts in>\n<acts in><movie> pearl harbor </movie>\n</acts in>\n<biography>\n<author> trendekid at aol.com </author>\n<text> benjamin geza affleck was . . . </text>\n</acts in>\n</person>\n\n30\n\n\f"}