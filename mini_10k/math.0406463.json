{"id": "http://arxiv.org/abs/math/0406463v1", "guidislink": true, "updated": "2004-06-23T12:36:13Z", "updated_parsed": [2004, 6, 23, 12, 36, 13, 2, 175, 0], "published": "2004-06-23T12:36:13Z", "published_parsed": [2004, 6, 23, 12, 36, 13, 2, 175, 0], "title": "Discussion of \"Least angle regression\" by Efron et al", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0406153%2Cmath%2F0406460%2Cmath%2F0406498%2Cmath%2F0406355%2Cmath%2F0406075%2Cmath%2F0406434%2Cmath%2F0406240%2Cmath%2F0406433%2Cmath%2F0406517%2Cmath%2F0406077%2Cmath%2F0406168%2Cmath%2F0406105%2Cmath%2F0406428%2Cmath%2F0406563%2Cmath%2F0406144%2Cmath%2F0406273%2Cmath%2F0406104%2Cmath%2F0406235%2Cmath%2F0406245%2Cmath%2F0406005%2Cmath%2F0406571%2Cmath%2F0406302%2Cmath%2F0406087%2Cmath%2F0406366%2Cmath%2F0406617%2Cmath%2F0406062%2Cmath%2F0406292%2Cmath%2F0406155%2Cmath%2F0406338%2Cmath%2F0406261%2Cmath%2F0406039%2Cmath%2F0406468%2Cmath%2F0406491%2Cmath%2F0406165%2Cmath%2F0406512%2Cmath%2F0406177%2Cmath%2F0406382%2Cmath%2F0406337%2Cmath%2F0406575%2Cmath%2F0406138%2Cmath%2F0406043%2Cmath%2F0406408%2Cmath%2F0406463%2Cmath%2F0406602%2Cmath%2F0406298%2Cmath%2F0406056%2Cmath%2F0406021%2Cmath%2F0406102%2Cmath%2F0406128%2Cmath%2F0406234%2Cmath%2F0406402%2Cmath%2F0406432%2Cmath%2F0406196%2Cmath%2F0406504%2Cmath%2F0406232%2Cmath%2F0406183%2Cmath%2F0406211%2Cmath%2F0406096%2Cmath%2F0406190%2Cmath%2F0406569%2Cmath%2F0406375%2Cmath%2F0406026%2Cmath%2F0406358%2Cmath%2F0406539%2Cmath%2F0406380%2Cmath%2F0406330%2Cmath%2F0406474%2Cmath%2F0406188%2Cmath%2F0406266%2Cmath%2F0406281%2Cmath%2F0406598%2Cmath%2F0406118%2Cmath%2F0406423%2Cmath%2F0406620%2Cmath%2F0406391%2Cmath%2F0406385%2Cmath%2F0406209%2Cmath%2F0406596%2Cmath%2F0406178%2Cmath%2F0406246%2Cmath%2F0406309%2Cmath%2F0406424%2Cmath%2F0406414%2Cmath%2F0406393%2Cmath%2F0406336%2Cmath%2F0406405%2Cmath%2F0406483%2Cmath%2F0406505%2Cmath%2F0406506%2Cmath%2F0406392%2Cmath%2F0406249%2Cmath%2F0406524%2Cmath%2F0406095%2Cmath%2F0406437%2Cmath%2F0406264%2Cmath%2F0406566%2Cmath%2F0406024%2Cmath%2F0406149%2Cmath%2F0406472%2Cmath%2F0406260%2Cmath%2F0406471&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Discussion of \"Least angle regression\" by Efron et al"}, "summary": "Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456]", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=math%2F0406153%2Cmath%2F0406460%2Cmath%2F0406498%2Cmath%2F0406355%2Cmath%2F0406075%2Cmath%2F0406434%2Cmath%2F0406240%2Cmath%2F0406433%2Cmath%2F0406517%2Cmath%2F0406077%2Cmath%2F0406168%2Cmath%2F0406105%2Cmath%2F0406428%2Cmath%2F0406563%2Cmath%2F0406144%2Cmath%2F0406273%2Cmath%2F0406104%2Cmath%2F0406235%2Cmath%2F0406245%2Cmath%2F0406005%2Cmath%2F0406571%2Cmath%2F0406302%2Cmath%2F0406087%2Cmath%2F0406366%2Cmath%2F0406617%2Cmath%2F0406062%2Cmath%2F0406292%2Cmath%2F0406155%2Cmath%2F0406338%2Cmath%2F0406261%2Cmath%2F0406039%2Cmath%2F0406468%2Cmath%2F0406491%2Cmath%2F0406165%2Cmath%2F0406512%2Cmath%2F0406177%2Cmath%2F0406382%2Cmath%2F0406337%2Cmath%2F0406575%2Cmath%2F0406138%2Cmath%2F0406043%2Cmath%2F0406408%2Cmath%2F0406463%2Cmath%2F0406602%2Cmath%2F0406298%2Cmath%2F0406056%2Cmath%2F0406021%2Cmath%2F0406102%2Cmath%2F0406128%2Cmath%2F0406234%2Cmath%2F0406402%2Cmath%2F0406432%2Cmath%2F0406196%2Cmath%2F0406504%2Cmath%2F0406232%2Cmath%2F0406183%2Cmath%2F0406211%2Cmath%2F0406096%2Cmath%2F0406190%2Cmath%2F0406569%2Cmath%2F0406375%2Cmath%2F0406026%2Cmath%2F0406358%2Cmath%2F0406539%2Cmath%2F0406380%2Cmath%2F0406330%2Cmath%2F0406474%2Cmath%2F0406188%2Cmath%2F0406266%2Cmath%2F0406281%2Cmath%2F0406598%2Cmath%2F0406118%2Cmath%2F0406423%2Cmath%2F0406620%2Cmath%2F0406391%2Cmath%2F0406385%2Cmath%2F0406209%2Cmath%2F0406596%2Cmath%2F0406178%2Cmath%2F0406246%2Cmath%2F0406309%2Cmath%2F0406424%2Cmath%2F0406414%2Cmath%2F0406393%2Cmath%2F0406336%2Cmath%2F0406405%2Cmath%2F0406483%2Cmath%2F0406505%2Cmath%2F0406506%2Cmath%2F0406392%2Cmath%2F0406249%2Cmath%2F0406524%2Cmath%2F0406095%2Cmath%2F0406437%2Cmath%2F0406264%2Cmath%2F0406566%2Cmath%2F0406024%2Cmath%2F0406149%2Cmath%2F0406472%2Cmath%2F0406260%2Cmath%2F0406471&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456]"}, "authors": ["Hemant Ishwaran"], "author_detail": {"name": "Hemant Ishwaran"}, "author": "Hemant Ishwaran", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/009053604000000067", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/math/0406463v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/math/0406463v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/math/0406463v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/math/0406463v1", "arxiv_comment": null, "journal_reference": "Annals of Statistics 2004, Vol. 32, No. 2, 452-458", "doi": "10.1214/009053604000000067", "fulltext": "arXiv:math/0406463v1 [math.ST] 23 Jun 2004\n\nThe Annals of Statistics\n2004, Vol. 32, No. 2, 452\u2013458\nDOI: 10.1214/009053604000000067\nc Institute of Mathematical Statistics, 2004\n\nDISCUSSION OF \"LEAST ANGLE REGRESSION\" BY EFRON\nET AL.\nBy Hemant Ishwaran\nCleveland Clinic Foundation\nBeing able to reliably, and automatically, select variables in linear regression models is a notoriously difficult problem. This research attacks this\nquestion head on, introducing not only a computationally efficient algorithm\nand method, LARS (and its derivatives), but at the same time introducing\ncomprehensive theory explaining the intricate details of the procedure as\nwell as theory to guide its practical implementation. This is a fascinating\npaper and I commend the authors for this important work.\nAutomatic variable selection, the main theme of this paper, has many\ngoals. So before embarking upon a discussion of the paper it is important\nto first sit down and clearly identify what the objectives are. The authors\nmake it clear in their introduction that, while often the goal in variable\nselection is to select a \"good\" linear model, where goodness is measured\nin terms of prediction accuracy performance, it is also important at the\nsame time to choose models which lean toward the parsimonious side. So\nhere the goals are pretty clear: we want good prediction error performance\nbut also simpler models. These are certainly reasonable objectives and quite\njustifiable in many scientific settings. At the same, however, one should\nrecognize the difficulty of the task, as the two goals, low prediction error\nand smaller models, can be diametrically opposed. By this I mean that\ncertainly from an oracle point of view it is true that minimizing prediction\nerror will identify the true model, and thus, by going after prediction error\n(in a perfect world), we will also get smaller models by default. However, in\npractice, what happens is that small gains in prediction error often translate\ninto larger models and less dimension reduction. So as procedures get better\nat reducing prediction error, they can also get worse at picking out variables\naccurately.\nUnfortunately, I have some misgivings that LARS might be falling into\nthis trap. Mostly my concern is fueled by the fact that Mallows' Cp is the\ncriterion used for determining the optimal LARS model. The use of Cp\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Statistics,\n2004, Vol. 32, No. 2, 452\u2013458. This reprint differs from the original in pagination\nand typographic detail.\n1\n\n\f2\n\nDISCUSSION\n\noften leads to overfitting, and this coupled with the fact that LARS is a\nforward optimization procedure, which is often found to be greedy, raises\nsome potential flags. This, by the way, does not necessarily mean that LARS\nper se is overfitting, but rather that I think Cp may be an inappropriate\nmodel selection criterion for LARS. It is this point that will be the focus\nof my discussion. I will offer some evidence that Cp can sometimes be used\neffectively if model uncertainty is accounted for, thus pointing to ways for its\nmore appropriate use within LARS. Mostly I will make my arguments by way\nof high-dimensional simulations. My focus on high dimensions is motivated\nin part by the increasing interest in such problems, but also because it is\nin such problems that performance breakdowns become magnified and are\nmore easily identified. Note that throughout my discussion I will talk only\nabout LARS, but, given the connections outlined in the paper, the results\nshould also naturally apply to the Lasso and Stagewise derivatives.\n1. Is Cp the correct stopping rule for LARS? The Cp criterion was\nintroduced by Mallows (1973) to be used with the OLS as an unbiased\nestimator for the model error. However, it is important to keep in mind that\nit was not intended to be used when the model is selected by the data as this\ncan lead to selection bias and in some cases poor subset selection [Breiman\n(1992)]. Thus, choosing the model with lowest Cp value is only a heuristic\ntechnique with sometimes bad performance. Indeed, ultimately, this leads to\nan inconsistent procedure for the OLS [Shao (1993)]. Therefore, while I think\nit is reasonable to assume that the Cp formula (4.10) is correct [i.e., that\nb k ) \u2248 k under a wide variety of settings],\nit is reasonable to expect that df (\u03bc\nthere is really no reason to expect that minimizing the Cp value will lead to\nan optimal procedure for LARS.\nIn fact, using Cp in a Forward Stagewise procedure of any kind seems to\nme to be a risky thing to do given that Cp often overfits and that Stagewise procedures are typically greedy. Figure 5 of the paper is introduced\n(partly) to dispel these types of concerns about LARS being greedy. The\nb a performance measurement related to predicmessage there is that pe(\u03bc),\ntion error, declines slowly from its maximum value for LARS compared to\nthe quick drop seen with standard forward stepwise regression. Thus, LARS\nacts differently than well-known greedy algorithms and so we should not\nbe worried. However, I see the message quite differently. If the maximum\nproportion explained for LARS is roughly the same over a large range of\nsteps, and hence models of different dimension, then this implies that there\nis not much to distinguish between higher-and lower-dimensional models.\nCombine this with the use of Cp which could provide poor estimates for the\nprediction error due to selection bias and there is real concern for estimating\nmodels that are too large.\n\n\fDISCUSSION OF \"LEAST ANGLE REGRESSION\" BY EFRON ET AL.\n\n3\n\nTo study this issue, let me start by reanalyzing the diabetes data (which\nwas the basis for generating Figure 5). In this analysis I will compare LARS\nto a Bayesian method developed in Ishwaran and Rao (2000), referred to as\nSVS (short for Stochastic Variable Selection). The SVS procedure is a hybrid\nof the spike-and-slab model approach pioneered by Mitchell and Beauchamp\n(1988) and later developed in George and McCulloch (1993). Details for SVS\ncan be found in Ishwaran and Rao (2000, 2003). My reason for using SVS\nas a comparison procedure is that, like LARS, its coefficient estimates are\nderived via shrinkage. However, unlike LARS, these estimates are based on\nmodel averaging in combination with shrinkage. The use of model averaging\nis a way of accounting for model uncertainty, and my argument will be that\nmodels selected via Cp based on SVS coefficients will be more stable than\nthose found using LARS thanks to the extra benefit of model averaging.\nFigures 1 and 2 present the Cp values for the main effects model and the\nquadratic model from both procedures (the analysis for LARS was based on\nS-PLUS code kindly provided by Trevor Hastie). The Cp values for SVS were\ncomputed by (a) finding the posterior mean values for coefficients, (b) ranking covariates by the size of their absolute posterior mean coefficient values\n(with the top rank going to the largest absolute mean) and (c) computing\ne k ) = ky \u2212 \u03bc\ne k k/\u03c3 2 \u2212 n + 2k, where \u03bc\ne k is the OLS estimate\nthe Cp value Cp (\u03bc\nbased on the k top ranked covariates. All covariates were standardized. This\ntechnique of using Cp with SVS was discussed in Ishwaran and Rao (2000).\nWe immediately see some differences in the figures. In Figure 1, the final\nmodel selected by SVS had k = 6 variables, while LARS had k = 7 variables.\nMore interesting, though, are the discrepancies for the quadratic model seen\nin Figure 2. Here the optimal SVS model had k = 8 variables in contrast to\nthe much higher k = 15 variables found by LARS. The top eight variables\nfrom SVS (some of these can be read off the top of the plot) are bmi, ltg, map,\nhdl, sex, age.sex, bmi.map and glu.2. The last three variables are interaction\neffects and a squared main effects term. The top eight variables from LARS\nare bmi, ltg, map, hdl, bmi.map, age.sex, glu.2 and bmi.2. Although there\nis a reasonable overlap in variables, there is still enough of a discrepancy to\nbe concerned. The different model sizes are also cause for concern. Another\nworrisome aspect for LARS seen in Figure 2 is that its Cp values remain\nbounded away from zero. This should be compared to the Cp values for\nSVS, which attain a near-zero mininum value, as we would hope for.\n2. High-dimensional simulations. Of course, since we do not know the\ntrue answer in the diabetes example, we cannot definitively assess if the\nLARS models are too large. Instead, it will be helpful to look at some simulations for a more systematic study. The simulations I used were designed\nfollowing the recipe given in Breiman (1992). Data was simulated in all\ncases by using i.i.d. N(0, 1) variables for \u03b5i . Covariates xi , for i = 1, . . . , n,\n\n\f4\n\nDISCUSSION\n\nFig. 1. Cp values from main effects model for diabetes data: thick line is values from\nSVS; thin dashed line is from LARS. Covariates listed at the top of the graph are ordered\nby importance as measured by their absolute posterior mean.\n\nwere generated independently from a multivariate normal distribution with\nzero mean and with covariance satisfying E(xi,j xi,k ) = \u03c1|j\u2212k| . I considered\ntwo settings for \u03c1: (i) \u03c1 = 0 (uncorrelated); (ii) \u03c1 = 0.90 (correlated). In all\nsimulations, n = 800 and m = 400. Nonzero \u03b2j coefficients were in 15 clusters of 7 adjacent variables centered at every 25th variable. For example,\nfor the variables clustered around the 25th variable, the coefficient values\nwere given by \u03b225+j = |h \u2212 j|1.25 for |j| < h, where h = 4. The other 14\nclusters were defined similarly. All other coefficients were set to zero. This\ngave a total of 105 nonzero values and 295 zero values. Coefficient values\nwere adjusted by multiplying by a common constant to make the theoretical\nR2 value equal to 0.75 [see Breiman (1992) for a discussion of this point].\nPlease note that, while the various parameters chosen for the simulations\nmight appear specific, I also experimented with other simulations (not reported) by considering different configurations for the dimension m, sample\nsize n, correlation \u03c1 and the number of nonzero coefficients. What I found\nwas consistent with the results presented here.\nFor each \u03c1 correlation setting, simulations were repeated 100 times independently. Results are recorded in Table 1. There I have recorded what I\ncall TotalMiss, FDR and FNR. TotalMiss is the total number of misclassified\nvariables, that is, the total number of falsely identified nonzero \u03b2j coefficients\nand falsely identified zero coefficients; FDR and FNR are the false discovery\n\n\fDISCUSSION OF \"LEAST ANGLE REGRESSION\" BY EFRON ET AL.\n\n5\n\nFig. 2. Cp values from quadratic model: best model from SVS is k = 8 (thick line) compared with k = 15 from LARS (thin dashed line). Note how the minimum value for SVS is\nnearly zero.\n\nand false nondiscovery rates defined as the false positive and false negative\nrates for those coefficients identified as nonzero and zero, respectively. The\nTotalMiss, FDR and FNR values reported are the averaged values from the\nb the average number of\n100 simulations. Also recorded in the table is m,\nb\nvariables selected by a procedure, as well as the performance value pe(\u03bc)\n[cf. (3.17)], again averaged over the 100 simulations.\nTable 1 records the results from various procedures. The entry \"svsCp\"\nrefers to the Cp -based SVS method used earlier; \"Step\" is standard forward\nstepwise regression using the Cp criterion; \"svsBMA\" is the Bayesian model\naveraged estimator from SVS. My only reason for including svsBMA is to\ngauge the prediction error performance of the other procedures. Its variable\nselection performance is not of interest. Pure Bayesian model averaging leads\nto improved prediction, but because it does no dimension reduction at all it\ncannot be considered as a serious candidate for selecting variables.\nThe overall conclusions from Table 1 are summarized as follows:\n1. The total number of misclassified coefficients and FDR values is high\nin the uncorrelated case for LARS and high in the correlated case for\nstepwise regression. Their estimated models are just too large. In comparison, svsCp does well in both cases. Overall it does the best in terms\nof selecting variables by maintaining low FDR and TotalMiss values. It\nalso maintains good performance values.\n\n\f6\n\nDISCUSSION\nTable 1\nBreiman simulation: m = 400, n = 800 and 105 nonzero \u03b2j\n\nLARS\nsvsCp\nsvsBMA\nStep\n\nm\nb\n\n210.69\n126.66\n400.00\n135.53\n\n\u03c1 = 0 (uncorrelated X )\npe(\u03bc\nb ) TotalMiss FDR FNR\n0.907\n0.887\n0.918\n0.876\n\n126.63\n61.14\n295.00\n70.35\n\n0.547\n0.323\n0.737\n0.367\n\n0.055\n0.072\n0.000\n0.075\n\nm\nb\n\n99.51\n58.86\n400.00\n129.24\n\n\u03c1 = 0.9 (correlated X)\npe(\u03bc\nb ) TotalMiss FDR FNR\n0.962\n0.952\n0.966\n0.884\n\n75.77\n66.38\n295.00\n137.10\n\n0.347\n0.153\n0.737\n0.552\n\n0.135\n0.164\n0.000\n0.208\n\nFig. 3. Cp values from simulations where \u03c1 = 0 ( left) and \u03c1 = 0.9 ( right): bottom curves\nare from SVS; top curves are from LARS. The lines seen on each curve are the mean Cp\nvalues based on 100 simulations. Note how the minimum value for SVS is near zero in\nboth cases. Also superimposed on each curve are error bars representing mean values plus\nor minus one standard deviation.\n\n2. LARS's performance values are good, second only to svsBMA. However,\nlow prediction error does not necessarily imply good variable selection.\n3. LARS Cp values in orthogonal models. Figure 3 shows the Cp values\nfor LARS from the two sets of simulations. It is immediately apparent that\nthe Cp curve in the uncorrelated case is too flat, leading to models which are\ntoo large. These simulations were designed to reflect an orthogonal design\nsetting (at least asymptotically), so what is it about the orthogonal case\nthat is adversely affecting LARS?\nWe can use Lemma 1 of the paper to gain some insight into this. For\nthis argument I will assume that m is fixed (the lemma is stated for m = n\nbut applies in general) and I will need to assume that Xn\u00d7m is a random\northogonal matrix, chosen so that its rows are exchangeable. To produce such\nan X, choose m values ei1 , . . . , eim without replacement from {e1 , . . . , en },\nwhere ej is defined as in Section 4.1, and set X = [ei1 , . . . , eim ]. It is easy to\nsee that this ensures row-exchangeability. Hence, \u03bc1 , . . . , \u03bcn are exchangeable\n\n\fDISCUSSION OF \"LEAST ANGLE REGRESSION\" BY EFRON ET AL.\n\n7\n\nand, therefore, Yi = \u03bci + \u03b5i are exchangeable since \u03b5i are i.i.d. I will assume,\nas in (4.1), that \u03b5i are independent N(0, \u03c3 2 ) variables.\nFor simplicity take \u03c3 2 = \u03c3 2 = 1. Let Vj , for j = 0, . . . , m \u2212 1, denote the\n(j + 1)st largest value from the set of values {|Yi1 |, . . . , |Yim |}. Let k0 denote\nthe true dimension, that is, the number of nonzero coordinates of the true\n\u03b2, and suppose that k is some dimension larger than k0 such that 1 \u2264 k0 <\nk \u2264 m \u2264 n. Notice that Vk \u2264 Vk0 , and thus, by Lemma 1 and (4.10),\nb k ) \u2212 Cp ( \u03bc\nb k0 ) = (Vk2 \u2212 Vk20 )\nCp (\u03bc\n\n\u2212\n\nm\nX\n\nm\nX\n\n1{|Yij | > Vk0 } + Vk2\n\nm\nX\n\n1{Vk < |Yij | \u2264 Vk0 }\n\nj=1\n\nj=1\n\nYi2j 1{Vk < |Yij | \u2264 Vk0 } + 2(k \u2212 k0 )\n\nj=1\n\n\u2264 \u2212\u2206k Bk + 2(k \u2212 k0 ),\nP\n\nwhere \u2206k = Vk20 \u2212 Vk2 \u2265 0 and Bk = m\nj=1 1{|Yij | > Vk0 }. Observe that by\nexchangeability Bk is a Binomial(m, k0 /m) random variable. It is a little\nmessy to work out the distribution for \u2206k explicitly. However, it is not\nhard to see that \u2206k can be reasonably large with high probability. Now if\nk0 > k \u2212 k0 and k0 is large, then Bk , which has a mean of k0 , will become\nthe dominant term in \u2206k Bk and \u2206k Bk will become larger than 2(k \u2212 k0 )\nwith high probability. This suggests, at least in this setting, that Cp will\noverfit if the dimension of the problem is high. In this case there will be\ntoo much improvement in the residual sums of squares when moving from\nk0 to k because of the nonvanishing difference between the squared order\nstatistics Vk20 and Vk2 .\n4. Summary. The use of Cp seems to encourage large models in LARS,\nespecially in high-dimensional orthogonal problems, and can adversely affect variable selection performance. It can also be unreliable when used with\nstepwise regression. The use of Cp with SVS, however, seems better motivated due to the benefits of model averaging, which mitigates the selection\nbias effect. This suggests that Cp can be used effectively if model uncertainty is accounted for. This might be one remedy. Another remedy would\nbe simply to use a different model selection criteria when using LARS.\nREFERENCES\nBreiman, L. (1992). The little bootstrap and other methods for dimensionality selection\nin regression: X-fixed prediction error. J. Amer. Statist. Assoc. 87 738\u2013754. MR1185196\nGeorge, E. I. and McCulloch, R. E. (1993). Variable selection via Gibbs sampling. J.\nAmer. Statist. Assoc. 88 881\u2013889.\nIshwaran, H. and Rao, J. S. (2000). Bayesian nonparametric MCMC for large variable\nselection problems. Unpublished manuscript.\n\n\f8\n\nDISCUSSION\n\nIshwaran, H. and Rao, J. S. (2003). Detecting differentially expressed genes in microarrays using Bayesian model selection. J. Amer. Statist. Assoc. 98 438\u2013455. MR1995720\nMallows, C. (1973). Some comments on Cp . Technometrics 15 661\u2013675.\nMitchell, T. J. and Beauchamp, J. J. (1988). Bayesian variable selection in linear\nregression (with discussion). J. Amer. Statist. Assoc. 83 1023\u20131036. MR997578\nShao, J. (1993). Linear model selection by cross-validation. J. Amer. Statist. Assoc. 88\n486\u2013494. MR1224373\nDepartment of Biostatistics/Wb4\nCleveland Clinic Foundation\n9500 Euclid Avenue\nCleveland, Ohio 44195\nUSA\ne-mail: ishwaran@bio.ri.ccf.org\n\n\f"}