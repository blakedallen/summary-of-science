{"id": "http://arxiv.org/abs/1104.1550v2", "guidislink": true, "updated": "2011-12-22T10:43:39Z", "updated_parsed": [2011, 12, 22, 10, 43, 39, 3, 356, 0], "published": "2011-04-08T11:30:49Z", "published_parsed": [2011, 4, 8, 11, 30, 49, 4, 98, 0], "title": "A bio-inspired image coder with temporal scalability", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.4740%2C1104.1976%2C1104.1095%2C1104.0687%2C1104.2291%2C1104.3988%2C1104.2587%2C1104.0148%2C1104.0772%2C1104.4949%2C1104.4270%2C1104.5705%2C1104.1312%2C1104.1969%2C1104.2263%2C1104.4669%2C1104.3279%2C1104.0303%2C1104.2645%2C1104.2148%2C1104.1217%2C1104.1096%2C1104.5701%2C1104.4575%2C1104.3775%2C1104.3489%2C1104.4945%2C1104.4279%2C1104.1595%2C1104.3058%2C1104.3039%2C1104.3020%2C1104.4007%2C1104.1304%2C1104.1515%2C1104.2725%2C1104.1418%2C1104.3727%2C1104.5547%2C1104.4968%2C1104.3807%2C1104.3912%2C1104.1214%2C1104.3245%2C1104.0957%2C1104.5103%2C1104.3720%2C1104.4416%2C1104.1518%2C1104.0984%2C1104.3671%2C1104.2839%2C1104.5004%2C1104.0459%2C1104.2224%2C1104.5021%2C1104.1963%2C1104.4921%2C1104.0380%2C1104.1084%2C1104.0066%2C1104.5283%2C1104.5022%2C1104.3909%2C1104.4727%2C1104.2870%2C1104.5089%2C1104.2741%2C1104.1813%2C1104.5676%2C1104.3018%2C1104.5165%2C1104.0728%2C1104.3182%2C1104.3876%2C1104.4134%2C1104.0203%2C1104.2521%2C1104.0240%2C1104.0522%2C1104.4923%2C1104.3806%2C1104.1964%2C1104.2092%2C1104.3708%2C1104.0478%2C1104.4787%2C1104.0467%2C1104.4383%2C1104.2035%2C1104.3731%2C1104.4223%2C1104.0026%2C1104.5094%2C1104.4096%2C1104.3850%2C1104.5401%2C1104.1425%2C1104.2247%2C1104.1550%2C1104.1076&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A bio-inspired image coder with temporal scalability"}, "summary": "We present a novel bio-inspired and dynamic coding scheme for static images.\nOur coder aims at reproducing the main steps of the visual stimulus processing\nin the mammalian retina taking into account its time behavior. The main novelty\nof this work is to show how to exploit the time behavior of the retina cells to\nensure, in a simple way, scalability and bit allocation. To do so, our main\nsource of inspiration will be the biologically plausible retina model called\nVirtual Retina. Following a similar structure, our model has two stages. The\nfirst stage is an image transform which is performed by the outer layers in the\nretina. Here it is modelled by filtering the image with a bank of difference of\nGaussians with time-delays. The second stage is a time-dependent\nanalog-to-digital conversion which is performed by the inner layers in the\nretina. Thanks to its conception, our coder enables scalability and bit\nallocation across time. Also, our decoded images do not show annoying artefacts\nsuch as ringing and block effects. As a whole, this article shows how to\ncapture the main properties of a biological system, here the retina, in order\nto design a new efficient coder.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1104.4740%2C1104.1976%2C1104.1095%2C1104.0687%2C1104.2291%2C1104.3988%2C1104.2587%2C1104.0148%2C1104.0772%2C1104.4949%2C1104.4270%2C1104.5705%2C1104.1312%2C1104.1969%2C1104.2263%2C1104.4669%2C1104.3279%2C1104.0303%2C1104.2645%2C1104.2148%2C1104.1217%2C1104.1096%2C1104.5701%2C1104.4575%2C1104.3775%2C1104.3489%2C1104.4945%2C1104.4279%2C1104.1595%2C1104.3058%2C1104.3039%2C1104.3020%2C1104.4007%2C1104.1304%2C1104.1515%2C1104.2725%2C1104.1418%2C1104.3727%2C1104.5547%2C1104.4968%2C1104.3807%2C1104.3912%2C1104.1214%2C1104.3245%2C1104.0957%2C1104.5103%2C1104.3720%2C1104.4416%2C1104.1518%2C1104.0984%2C1104.3671%2C1104.2839%2C1104.5004%2C1104.0459%2C1104.2224%2C1104.5021%2C1104.1963%2C1104.4921%2C1104.0380%2C1104.1084%2C1104.0066%2C1104.5283%2C1104.5022%2C1104.3909%2C1104.4727%2C1104.2870%2C1104.5089%2C1104.2741%2C1104.1813%2C1104.5676%2C1104.3018%2C1104.5165%2C1104.0728%2C1104.3182%2C1104.3876%2C1104.4134%2C1104.0203%2C1104.2521%2C1104.0240%2C1104.0522%2C1104.4923%2C1104.3806%2C1104.1964%2C1104.2092%2C1104.3708%2C1104.0478%2C1104.4787%2C1104.0467%2C1104.4383%2C1104.2035%2C1104.3731%2C1104.4223%2C1104.0026%2C1104.5094%2C1104.4096%2C1104.3850%2C1104.5401%2C1104.1425%2C1104.2247%2C1104.1550%2C1104.1076&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present a novel bio-inspired and dynamic coding scheme for static images.\nOur coder aims at reproducing the main steps of the visual stimulus processing\nin the mammalian retina taking into account its time behavior. The main novelty\nof this work is to show how to exploit the time behavior of the retina cells to\nensure, in a simple way, scalability and bit allocation. To do so, our main\nsource of inspiration will be the biologically plausible retina model called\nVirtual Retina. Following a similar structure, our model has two stages. The\nfirst stage is an image transform which is performed by the outer layers in the\nretina. Here it is modelled by filtering the image with a bank of difference of\nGaussians with time-delays. The second stage is a time-dependent\nanalog-to-digital conversion which is performed by the inner layers in the\nretina. Thanks to its conception, our coder enables scalability and bit\nallocation across time. Also, our decoded images do not show annoying artefacts\nsuch as ringing and block effects. As a whole, this article shows how to\ncapture the main properties of a biological system, here the retina, in order\nto design a new efficient coder."}, "authors": ["Khaled Masmoudi", "Marc Antonini", "Pierre Kornprobst"], "author_detail": {"name": "Pierre Kornprobst"}, "author": "Pierre Kornprobst", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1007/978-3-642-23687-7_41", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1104.1550v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1104.1550v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "12 pages; Advanced Concepts for Intelligent Vision Systems (ACIVS\n  2011)", "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.NE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1104.1550v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1104.1550v2", "journal_reference": null, "doi": "10.1007/978-3-642-23687-7_41", "fulltext": "A bio-inspired image coder with temporal\nscalability\nKhaled Masmoudi, Marc Antonini1 , and Pierre Kornprobst2\n\narXiv:1104.1550v2 [cs.CV] 22 Dec 2011\n\n1\n\n2\n\nI3S laboratory\u2013UNS\u2013CNRS\nSophia-Antipolis, France\nkmasmoud@i3s.unice.fr,\nhttp://www.i3s.unice.fr/~kmasmoud\nNeuroMathComp Team Project\u2013INRIA\nSophia-Antipolis, France\n\nAbstract. We present a novel bio-inspired and dynamic coding scheme\nfor static images. Our coder aims at reproducing the main steps of the\nvisual stimulus processing in the mammalian retina taking into account\nits time behavior. The main novelty of this work is to show how to\nexploit the time behavior of the retina cells to ensure, in a simple way,\nscalability and bit allocation. To do so, our main source of inspiration will\nbe the biologically plausible retina model called Virtual Retina. Following\na similar structure, our model has two stages. The first stage is an image\ntransform which is performed by the outer layers in the retina. Here it\nis modelled by filtering the image with a bank of difference of Gaussians\nwith time-delays. The second stage is a time-dependent analog-to-digital\nconversion which is performed by the inner layers in the retina. Thanks\nto its conception, our coder enables scalability and bit allocation across\ntime. Also, our decoded images do not show annoying artefacts such as\nringing and block effects. As a whole, this article shows how to capture\nthe main properties of a biological system, here the retina, in order to\ndesign a new efficient coder.\n\nKeywords: Static image compression, bio-inspired signal coding, retina\n\n1\n\nIntroduction\n\nIntensive efforts have been made during the past two decades for the design of\nlossy image coders yielding several standards such as JPEG and JPEG2000 [1,3].\nThese compression algorithms, mostly, followed the same conception schema,\nthough, improving considerably the performances in terms of cost and quality.\nYet, it became clear now that little is still to be gained if no shift is made in the\nphilosophy underlying the design of coders.\nIn this paper, we propose a novel image codec based on visual system properties: Our aim is to set a new framework for coder design. In this context,\nneurophysiologic studies \"have demonstrated that our sensory systems are remarkably efficient at coding the sensory environment\" [8], and we are convinced\nthat an interdisciplinary approach would improve coding algorithms.\n\n\fWe focused on the complex computations that the mammalian retina operates to transform the incoming light stimulus into a set of uniformly-shaped\nimpulses, also called spikes. Indeed, recent studies such as [7] confirmed that the\nretina is doing non-trivial operations to the input signal before transmission, so\nthat our goal here is to capture the main properties of the retina processing for\nthe design of our new coder.\nSeveral efforts in the literature reproduced fragments of this retina processing\nthrough bio-inspired models and for various vision tasks, for example: object detection and robot movement decision [9], fast categorization [19,20], and regions\nof interest detection for bit allocation [13]. But most of these approaches do not\naccount for the precise retina processing. Besides, these models overlooked the\nsignal recovery problem which is crucial in the coding application. Attempts in\nthis direction were done making heavy simplifications at the expense of biological relevance [14] or restricting the decoding ability within a set of signals in a\ndictionary [15]. Here, the originality of our work is twofold: we focus explicitly on\nthe coding application and we keep our design as close as possible to biological\nreality considering most of the mammalian retina processing features.\nOur main source of inspiration will be the biologically plausible Virtual\nRetina model [23] whose goal was to find the best compromise between the biological reality and the possibility to make large-scale simulations. Based on this\nmodel, we propose a coding scheme following the architecture and functionalities\nof the retina, doing some adaptations due to the application.\nThis paper is organized as follows. In Section 2 we revisit the retina model\ncalled Virtual Retina [23]. In Section 3, we show how this retina model can\nbe used as the basis of a novel bio-inspired image coder. The coding pathway\nis presented in a classical way distinguishing two stages: the image transform\nand the analog-to-digital (A/D) converter. In Section 4 we present the decoding\npathway. In Section 5 we show the main results that demonstrate the properties\nof our model. In Section 6 we summarize our main conclusions.\n\n2\n\nVirtual Retina: a bio-inspired retina model\n\nThe motivation of our work is to investigate the retina functional architecture\nand use it as a design basis to devise new codecs. So, it is essential to understand\nwhat are the main functional principles of the retina processing. The literature in\ncomputational neuroscience dealing with the retina proposes different models.\nThese models are very numerous, ranking from detailed models of a specific\nphysiological phenomenon, to large-scale models of the whole retina.\nIn this article, we focused on the category of large-scale retina models as we\nare interested in a model that gathers the main features of mammalian retina.\nWithin this category, we considered the retina model called Virtual Retina [23].\nThis model is one of the most complete ones in the literature, as it encompasses\nthe major features of the actual mammalian retina. This model is mostly stateof-the-art and the authors confirmed its relevance by reproducing accurately real\ncell recordings for several experiments.\n\n\fFig. 1. (a) Schematic view of the Virtual Retina model proposed by [23]. (b) and\n(c): Overview of our bio-inspired codec. Given an image, the static DoG-based multiscale transform generates the sub-bands {Fk }. DoG filters are sorted from the lowest\nfrequency-band filter DoG0 to the highest one DoGN \u22121 . Each sub-band Fk is delayed\nusing a time-delay circuit Dtk , with tk < tk+1 . The time-delayed multi-scale output is\nthen made available to the subsequent coder stages. The final output of the coder is\na set of spike series, and the coding feature adopted will be the spike count nkij (tobs )\nrecorded for each neuron indexed by (kij) at a given time tobs .\n\nThe architecture of the Virtual Retina model follows the structure of mammalian retina as schematized in Figure 1(a). The model has several interconnected layers and three main processing steps can be distinguished:\n\u2013 Outer layers: The first processing step is described by non-separable spatiotemporal filters, behaving as time-dependent edge detectors. This is a classical step implemented in several retina models.\n\u2013 Inner layers: A non-linear contrast gain control is performed. This step models mainly bipolar cells by control circuits with time-varying conductances.\n\u2013 Ganglionic layer: Leaky integrate and fire neurons are implemented to model\nthe ganglionic layer processing that finally converts the stimulus into spikes.\nGiven this model as a basis, our goal is to adapt it to conceive the new codec\npresented in the next sections.\n\n\f3\n\nThe coding pathway\n\nThe coding pathway is schematized in Figure 1(b). It follows the same architecture as Virtual Retina. However, since we have to define also a decoding pathway,\nwe need to think about the invertibility of each processing stage. For this reason\nsome adaptations are required and described in this section.\n3.1\n\nThe image transform: The outer retina layers\n\nIn Virtual Retina, the outer layers were modelled by a non-separable spatiotemporal filtering. This processing produces responses corresponding to spatial\nor temporal variations of the signal because it models time-dependent interactions between two low-pass filters: this is termed center-surround differences.\nThis stage has the property that it responds first to low spatial frequencies and\nlater to higher frequencies. This time-dependent frequency integration was shown\nfor Virtual Retina (see [24]) and it was confirmed experimentally (see, e.g., [17]).\nThis property is interesting as a large amount of the total signal energy is contained in the lower frequency sub-bands, whereas high frequencies bring further\ndetails. This idea already motivated bit allocation algorithms to concentrate the\nresources for a good recovery on lower frequencies.\nHowever, it appears that inverting this non-separable spatio-temporal filtering is a complex problem [24,25]. To overcome this difficulty, we propose to model\ndifferently this stage while keeping its essential features. To do so, we decomposed this process into two steps: The first one considers only center-surround\ndifferences in the spatial domain (through differences of Gaussians) which is justified by the fact that our coder here gets static images as input. The second\nstep reproduces the time-dependent frequency integration by the introduction\nof time-delays.\nCenter-surround differences in the spatial domain: DoG Neurophysiologic experiments have shown that, as for classical image coders, the retina\nencodes the stimulus representation in a transform domain. The retinal stimulus transform is performed in the cells of the outer layers, mainly in the outer\nplexiform layer (OPL). Quantitative studies such as [6,16] have proven that the\nOPL cells processing can be approximated by a linear filtering. In particular,\nthe authors in [6] proposed the largely adopted DoG filter which is a weighted\ndifference of spatial Gaussians that is defined as follows:\nDoG(x, y) = wc G\u03c3c (x, y) \u2212 ws G\u03c3s (x, y),\n\n(1)\n\nwhere wc and ws are the respective weights of the center and surround components of the receptive fields, and \u03c3c and \u03c3s are the standard deviations of the\nGaussian kernels G\u03c3c and G\u03c3s .\nIn terms of implementation, as in [20], the DoG cells can be arranged in a\ndyadic grid to sweep all the stimulus spectrum as schematized in Figure 2(a).\nEach layer k in the grid, is tiled with DoGk cells having a scale sk and generating\n\n\fa transform sub-band Fk , where \u03c3sk+1 = 12 \u03c3sk . So, in order to measure the degree\nopl\nof activation I \u0304kij\nof a given DoGk cell at the location (i, j) with a scale sk , we\ncompute the convolution of the original image f by the DoGk filter:\nopl\nI \u0304kij\n=\n\n\u221e\nX\n\nDoGk (i \u2212 x, j \u2212 y) f (x, y).\n\n(2)\n\nx,y=\u2212\u221e\n\nThis generates a set of 34 N 2 \u22121 coefficients for an N 2 -sized image, as it works\nin the same fashion as a Laplacian pyramid [2]. An example of such a bio-inspired\nmulti-scale decomposition is shown in Figure 2(b). Note here that we added to\nthis bank of filters a Gaussian low-pass scaling function that represents the state\nopl\nof the OPL filters at the time origin. This yields a low-pass coefficient I \u0304000\nand\nenables the recovery of a low-pass residue at the reconstruction level [5,12].\n\n(a)\n\n(b)\n\n(c)\n\nFig. 2. (a) Input Lena Image. (b) Example of a dyadic grid of DoG's used for the\nimage analysis (from [20]). (c) Example on image (a) of DoG coefficients generated by\nthe retina model (the sub-bands are shown in the logarithmic scale)\n\nIntegrating time dynamics through time-delay circuits Of course, the\nmodel described in (2) has no dynamical properties. In the actual retina, the\nsurround G\u03c3s in (1) appears progressively across time driving the filter passband\nfrom low frequencies to higher ones. Our goal is to reproduce this phenomenon\nthat we called time-dependent frequency integration. To do so, we added in the\ncoding pathway of each sub-band Fk a time-delay circuit Dtk . The value of tk is\nspecific to Fk and is linearly increasing as a function of k. The tk -delay causes\nthe sub-band Fk to be transmitted to the subsequent stages of the coder starting\nopl\nfrom the time tk . The time-delayed activation coefficient Ikij\n(t) computed at the\nlocation (i, j) for the scale sk at time t is now defined as follows:\nopl\nopl\nIkij\n(t) = I \u0304kij\n1{t>tk } (t),\n\n(3)\n\nwhere 1{t>tk } is the indicator function such that, 1{t>tk } (t) = 0 if t < tk and 1\notherwise.\n\n\f3.2\n\nThe A/D converter: inner and ganglionic layers\n\nThe retinal A/D converter is defined based on the processing occurring in the\ninner and ganglionic layers, namely a contrast gain control, a non-linear rectification and a discretization based on leaky integrate and fire (LIF) neurons [10].\nA different treatment will be performed for each delayed sub-band, and this\nproduces a natural bit allocation mechanism. Indeed, as each sub-band Fk is\npresented at a different time tk , it will be subject to a transform according to\nthe state of our dynamic A/D converter at tk .\nContrast gain control Retina adjust its operational range to match the input stimuli magnitude range. This is done by an operation called contrast gain\ncontrol mainly performed in the bipolar cells. Indeed, real bipolar cells conductance is time varying, resulting in a phenomenon termed shunting inhibition.\nThis shunting avoids the system saturation by reducing high magnitudes.\nopl\nIn Virtual Retina, given the scalar magnitude I \u0304kij\nof the input step current\nopl\nIkij\n(t), the contrast gain control is a non-linear operation on the potential of the\nbipolar cells. This potential varies according to both the time and the magnitude\nopl\nopl\nb\nvalue I \u0304kij\n; and will be denoted by Vkij\n(t, I \u0304kij\n).\nopl\nThis phenomenon is modelled, for a constant value of I \u0304kij\n, by the following\ndifferential equation:\n\uf8f1\nopl\nb\n\uf8f4\n\uf8f2 b dVkij (t, I \u0304kij )\nopl\nopl\nb\n+ g b (t)Vkij\n(t, I \u0304kij\n) = Ikij\n(t), for t > 0,\nc\n(4)\ndt\n\uf8f4\n\uf8f3 g b (t) = E b \u2217t Q(V b (t, I \u0304opl )),\n\u03c4\nkij\nkij\n\n\u0010\n\u00112\n\u2212t\n1\nb\nb\nwhere Q(Vkij\n) = g0b + \u03bbb Vkij\n(t) and E\u03c4 b = b exp \u03c4 b , for t > 0. Figure 3(a)\n\u03c4\nopl\nopl\nb\nshows the time behavior of Vkij\n(t, I \u0304kij\n) for different magnitude values I \u0304kij\nof\nopl\nIkij\n(t).\n\nopl\nb\nNon-linear rectification Then, the potential Vkij\n(t, I \u0304kij\n) is subject to a nonlinear rectification yielding the so-called ganglionic current I g (t, I \u0304opl ). Virtual\nkij\n\nkij\n\nopl\nRetina models it, for a constant scalar value I \u0304kij\n, by:\n\n\u0010\n\u0011\ng\nopl\nopl\nb\nIkij\n(t, I \u0304kij\n) = N Twg ,\u03c4 g (t) \u2217 Vkij\n(t, I \u0304kij\n) ,\n\nfor t > 0,\n\n(5)\n\nwhere wg and \u03c4 g are constant scalar parameters, Twg ,\u03c4 g is the linear transient\nfilter defined by Twg ,\u03c4 g = \u03b40 (t) \u2212 wg E\u03c4 g (t), and N is defined by:\n\uf8f1\nig0\n\uf8f2\n, if v < v0g\nN (v) = ig0 \u2212 \u03bbg (v \u2212 v0g )\n\uf8f3 g\ni0 + \u03bbg (v \u2212 v0g ), if v > v0g ,\n\n\f(a)\n\n(b)\n\n(c)\n\n(d)\n\ng\nb\nFig. 3. 3(a): Vkij\n(t) as a function of time for different values of I \u0304opl ; 3(b): Ikij\nas\ng\nopl\nopl\n \u0304\n \u0304\na function of time for different values of I ; 3(c): The functions ftk that map Ikij\ng\nr\ninto nkij for\ninto Ikij\nfor different values of tk ; 3(d): The functions ftnobs that map I \u0304kij\ndifferent values of tobs\n\nwhere ig0 , v0g , and \u03bbg are constant scalar parameters. Figure 3(b) shows the time\ng\nopl\nopl\nbehavior of Ikij\n(t, I \u0304kij\n) for different values of I \u0304kij\n.\nopl\nAs the currents I \u0304kij are delayed with times {tk }, our goal is to catch the\ninstantaneous behavior of the inner layers at these times {tk }. This amounts to\nopl\nopl\ninfer the transforms Itgk (I \u0304kij\n) that maps a given scalar magnitude I \u0304kij\ninto a\nr\n \u0304\nrectified current Ikij as the modelled inner layers would generate it at tk . To do\ng\nopl\nso, we start from the time-varying curves of Ikij\n(t, I \u0304kij\n) in Figure 3(b) and we\ndo a transversal cut at each time tk : We show in Figure 3(c) the resulting maps\ng\nopl\nopl\nftgk such that Ikij\n(tk , I \u0304kij\n) = ftgk (I \u0304kij\n).\nopl\nAs for Ikij\n(t) (see (3)), we introduce the time dimension using the indicator\nfunction 1{t>tk } (t). The final output of this stage is the set of step functions\nr\nIkij\n(t) defined by:\nopl\nr\nr\nr\nIkij\n(t) = I \u0304kij\n1{t>tk } (t), with I \u0304kij\n= ftgk (I \u0304kij\n).\n\n(6)\n\n\fThis non-linear rectification is analogous to a widely-used telecommunication\ntechnique: the companding [4]. Companders are used to make the quantization\nsteps unequal after a linear gain control stage. Though, unlike A \u2212 law or \u03bc \u2212\nlaw companders that amplify low magnitudes, the inner layers emphasize high\nmagnitudes in the signal. Besides, the inner layers stage have a time dependent\nbehavior, whereas a usual gain controller/compander is static, and this makes\nour A/D converter go beyond the standards.\nLeaky integrate-and-fire quantization: The ganglionic layer is the deepest\nr\none tiling the retina: it transforms a continuous signal Ikij\n(t) into discrete sets\nof spike trains. As in Virtual Retina, this stage is modelled by leaky integrate\nand fire neurons (LIF) which is a classical model. One LIF neuron is associated\nto every position in each sub-band Fk . The time-behavior of a LIF neuron is\ngoverned by the fluctuation of its voltage Vkij (t). Whenever Vkij (t) reaches a\npredefined \u03b4 threshold, a spike is emitted and the voltage goes back to a resting\n(l)\n(l+1)\npotential VR0 . Between two spike emission times, tkij and tkij , the potential\nevolves according to the following differential equation:\ncl\n\ndVkij (t)\nr\n+ g l Vkij (t) = Ikij\n(t),\ndt\n\n(l)\n\n(l+1)\n\nfor t \u2208 [tkij , tkij ],\n\n(7)\n\nwhere g l is a constant conductance, and cl is a constant capacitance. In the literature, neurons activity is commonly characterized by the count of spikes emitted\nduring an observation time bin [0, tobs ], which we denote by nkij (tobs ) [22]. Obvir\nously, as nkij (tobs ) encodes for the value of Ikij\n(t), there is a loss of information\nas nkij (tobs ) is an integer. The LIF is thus performing a quantization. If we observe the instantaneous behavior of the ganglionic layer at different times tobs ,\nwe get a quasi-uniform scalar quantizer that refines in time. We can do this by\na similar process to the one described in the previous paragraph. We show in\nr\nFigure 3(d) the resulting maps ftnobs such that nkij (tobs ) = ftnobs (I \u0304kij\n).\nBased on the set {nkij (tobs )}, measured at the output of our coder, we describe in the next section the decoding pathway to recover the initial image\nf (x, y).\n\n4\n\nThe decoding pathway\n\nThe decoding pathway is schematized in Figure 1(c). It consists in inverting,\nstep by step, each coding stage described in Section 3. At a given time tobs , the\ncoding data is the set of ( 34 N 2 \u2212 1) spike counts nkij (tobs ), this section describes\nhow we can recover an estimation f \u0303tobs of the N 2 -sized input image f (x, y).\nNaturally, the recovered image f \u0303tobs (x, y) depends on the time tobs which ensures\ntime-scalability: the quality of the reconstruction improves as tobs increases. The\nganglionic and inner layers are inverted using look-up tables constructed off-line\nand the image is finally recovered by a direct reverse transform of the outer\nlayers processing.\n\n\fRecovering the input of the ganglionic layer: First, given a spike count\nr\nr\nnkij (tobs ), we recover I \u0303kij\n(tobs ), the estimation of Ikij\n(tobs ). To do so, we comr\npute off-line the look-up table ntobs (I \u0304kij ) that maps the set of current magnitude\nr\nvalues I \u0304kij\ninto spike counts at a given observation time tobs (see Figure 3(d)).\nThe reverse mapping is done by a simple interpolation in the reverse-look up\ntable denoted LU TtLIF\n. Here we draw the reader's attention to the fact that,\nobs\nas the input of the ganglionic layer is delayed, each coefficient of the sub-band\nFk is decoded according to the reverse map LU TtLIF\n. Obviously, the recovobs \u2212tk\nered coefficients do not match exactly the original ones due to the quantization\nperformed in the LIF's.\nRecovering the input of the inner layers: Second, given a rectified current\nopl\nopl\nr\nvalue I \u0303kij\n(tobs ), we recover I \u0303kij\n(tobs ), the estimation of Ikij\n(tobs ). In the same way\nas for the preceding stage, we infer the reverse \"inner layers mapping\" through\nopl\nthe pre-computed look up table LU TtCG\n. The current intensities I \u0303kij\n(tobs ),\nobs\ncorresponding to the retinal transform coefficients, are passed to the subsequent\nretinal transform decoder.\nRecovering the input stimulus: Finally, given the set of 34 N 2 \u2212 1 coefficients\nopl\n{I \u0303kij\n(tobs )}, we recover f \u0303tobs (x, y), the estimation of the original image stimulus\nf (x, y). Though the dot product of every pair of DoG filters is approximately\nequal to 0, the set of filters considered is not strictly orthonormal. We proved\nin [11] that there exists a dual set of vectors enabling an exact reconstruction.\nHence, the reconstruction estimate f \u0303 of the original input f can be obtained as\nfollows:\nX opl\n] k (i \u2212 x, j \u2212 y),\nI \u0303kij (tobs ) DoG\nf \u0303tobs (x, y) =\n(8)\n{kij}\n\nwhere {kij} is the set of possible scales and locations in the considered dyadic\n] k are the duals of the DoGk filters obtained as detailed in [11].\ngrid and DoG\nEquation (8) defines a progressive reconstruction depending on tobs . This feature\nmakes the coder be time-scalable.\n\n5\n\nResults\n\nWe show examples of image reconstruction using our bio-inspired coder at different times3 . Then, we study these results in terms of quality and bit-cost.\nQuality is assessed by classical image quality criteria (PSNR and mean SSIM [21]).\nThe cost is measured by the Shannon entropy H(tobs ) upon the population of\n{nkij (tobs )}. The entropy computed in bits per pixel (bpp), for an N 2 -sized image,\n3\n\nIn\ng0b\nig0\ngL\n\nall experiments, the model parameters are set to biologically realistic values:\n= 8 10\u2212 10 S, \u03c4 b = 12 10\u22123 s, \u03bbb = 9 10\u22127 , cb = 1.5 10\u221210 F , v0g = 4 10\u22123 V ,\n= 15 10\u2212 12 A, wg = 8 10\u2212 1, \u03c4 g = 16 10\u22123 s; \u03bbg = 12 10\u22129 S, \u03b4 = 2 10\u22123 V,\n= 2 10\u22129 S, VR0 = 0 V , tk = 5 10\u22123 + k 10\u22123 s.\n\n\f\u0010n\no\u0011\nPK\u22121\n2\nis defined by: H(tobs ) = N12 k=0 22k H\nnsk ij (tobs ), (i, j) \u2208 J0, 2k \u2212 1K\n,\nwhere K is the number of analyzing sub-bands.\nFigure 4 shows two examples of progressive reconstruction obtained with\nour new coder. The new concept of time scalability is an interesting feature as it\nintroduces time dynamics in the design of the coder. This is a consequence of the\nmimicking of the actual retina. We also notice that, as expected, low frequencies\nare transmitted first to get a first approximation of the image, then details are\nadded progressively to draw its contours. The bit-cost of the coded image is\nslightly high. This can be explained by the fact that Shannon entropy is not\nthe most relevant metric in our case as no context is taken into consideration,\nespecially the temporal context. Indeed, one can easily predict the number of\nspikes at a given time t knowing nkij (t \u2212 dt). Note also that no compression\ntechniques, such that bit-plane coding, are yet employed. Our paper aims mainly\nat setting the basis of new bio-inspired coding designs.\nFor the reasons cited above, the performance of our coding scheme in terms\nof bit-cost have still to be improved to be competitive with the well established JPEG and JPEG2000 standards. Thus we show no comparison in this\npaper. Though primary results are encouraging, noting that optimizing the bitallocation mechanism and exploiting coding techniques as bit-plane coding [18]\nwould improve considerably the bit-cost. Besides, the image as reconstructed\nwith our bio-inspired coder shows no ringing and no block effect. Finally our\ncodec enables scalability in an original fashion through the introduction of time\ndynamics within the coding mechanism.\nNote also that differentiation in the processing of sub-bands, introduced\nthrough time-delays in the retinal transform, enables implicit but still not optimized bit-allocation. In particular the non-linearity in the inner layers stage\namplifies singularities and contours, and these provide crucial information for\nthe analysis of the image. The trade-off between the emphasize made on high\nfrequencies and the time-delay in the starting of their coding process is still an\nissue to investigate.\n\n6\n\nConclusion\n\nWe proposed a new bio-inspired codec for static images. The image coder is\nbased on two stages. The first stage is the image transform as performed by\nthe outer layers of the retina. In order to integrate time dynamics, we added to\nthis transform time delays that are sub-band specific so that, each sub-band is\nprocessed differently. The second stage is a succession of two dynamic processing\nsteps mimicking the deep retina layers behavior. The latter perform an A/D\nconversion and generate a spike-based, invertible, retinal code for the input image\nin an original fashion.\nOur coding scheme offers interesting features such as (i) time-scalability, as\nthe choice of the observation time of our codec enables different reconstruction\nqualities, and (ii) bit-allocation, as each sub-band of the image transform is\nseparately mapped according to the corresponding state of the inner layers.\n\n\fFig. 4. Progressive image reconstruction of Lena and Cameraman using our new bioinspired coder. The coded/decoded image is shown at: 20 ms, 30 ms, 40 ms, and 50\nms. Rate/Quality are computed for each image in terms of the triplet (bit-cost in bpp/\nPSNR quality in dB/ mean SSIM quality). Upper line: From left to right (0.07 bpp/\n20.5 dB/ 0.59), (0.38 bpp/ 24.4 dB/ 0.73), (1.0 bpp/ 29.1 dB/ 0.86), and (2.1 bpp/ 36.3\ndB/ 0.95). Lower line: From left to right (0.005 bpp/ 15.6 dB/ 0.47), (0.07 bpp/ 18.9\ndB/ 0.57), (0.4 bpp/ 23 dB/ 0.71), and (1.2 bpp/ 29.8 dB/ 0.88).\n\nPrimary results are encouraging, noting that optimizing the bit-allocation and\nusing coding techniques as bit-plane coding would improve considerably the cost.\nThis work is at the crossroads of diverse hot topics in the fields of neurosciences, brain-machine interfaces, and signal processing and tries to lay the\ngroundwork for future efforts, especially concerning the design of new biologically inspired coders.\n\nReferences\n1. Antonini, M., Barlaud, M., Mathieu, P., Daubechies, I.: Image coding using wavelet\ntransform. IEEE Transactions on Image Processing (1992)\n2. Burt, P., Adelson, E.: The Laplacian pyramid as a compact image code. IEEE\nTransactions on communications 31(4), 532\u2013540 (1983)\n3. Christopoulos, C., Skodras, A., Ebrahimi, T.: The JPEG2000 still image coding\nsystem: An overview. IEEE Transactions on Consumer Electronics 16(4), 1103\u2013\n1127 (2000)\n4. Clark, A., et al: Electrical picture-transmitting system. US Patent assigned to AT&\nT (1928)\n5. Crowley, J., Stern, R.: Fast computation of the difference of low-pass transform.\nIEEE Transactions on Pattern Analysis and Machine Intelligence (2), 212\u2013222\n(2009)\n6. Field, D.: What is the goal of sensory coding? Neural Computation 6(4), 559\u2013601\n(1994)\n\n\f7. Gollisch, T., Meister, M.: Eye smarter than scientists believed: Neural computations in circuits of the retina. Neuron 65(2), 150\u2013164 (2010)\n8. Graham, D., Field, D.: Efficient coding of natural images. New Encyclopedia of\nNeuroscience (2007)\n9. Linares-Barranco, A., Gomez-Rodriguez, F., Jimenez-Fernandez, A., Delbruck, T.,\nLichtensteiner, P.: Using FPGA for visuo-motor control with a silicon retina and\na humanoid robot. In: Proceedings of ISCAS 2007. pp. 1192\u20131195. IEEE (2007)\n10. Masmoudi, K., Antonini, M., Kornprobst, P.: Another look at the retina as an\nimage scalar quantizer. In: Proceedings of ISCAS 2010. pp. 3076\u20133079. IEEE (2010)\n11. Masmoudi, K., Antonini, M., Kornprobst, P.: Exact reconstruction of the rank\norder coding using frames theory. ArXiv e-prints (2011), http://arxiv.org/abs/\n1106.1975v1\n12. Masmoudi, K., Antonini, M., Kornprobst, P., Perrinet, L.: A novel bio-inspired\nstatic image compression scheme for noisy data transmission over low-bandwidth\nchannels. In: Proceedings of ICASSP 3506\u20133509. IEEE (2010)\n13. Ouerhani, N., Bracamonte, J., Hugli, H., Ansorge, M., Pellandini, F.: Adaptive\ncolor image compression based on visual attention. In: Proceedings of IEEE ICIAP.\npp. 416\u2013421. IEEE (2002)\n14. Perrinet, L.: Sparse Spike Coding: applications of Neuroscience to the processing of natural images. Proceedings of SPIE, the International Society for Optical\nEngineering, number ISSN (2008)\n15. Pillow, J., Shlens, J., Paninski, L., Sher, A., Litke, A., Chichilnisky, E., Simoncelli, E.: Spatio-temporal correlations and visual signalling in a complete neuronal\npopulation. Nature 454(7207), 995\u2013999 (2008)\n16. Rodieck, R.: Quantitative analysis of the cat retinal ganglion cells response to\nvisual stimuli. Vision Research 5(11), 583\u2013601 (1965)\n17. Sterling, P., Cohen, E., Smith, R., Tsukamoto, Y.: Retinal circuits for daylight:\nwhy ballplayers don't wear shades. Analysis and Modeling of Neural Systems pp.\n143\u2013162 (1992)\n18. Taubman, D.: High performance scalable image compression with ebcot. IEEE\ntransactions on Image Processing 9(7), 1158\u20131170 (2000)\n19. Thorpe, S., Gautrais, J.: Rank order coding. Computational Neuroscience: Trends\nin Research 13, 113\u2013119 (1998)\n20. Van Rullen, R., Thorpe, S.: Rate coding versus temporal order coding: What the\nretinal ganglion cells tell the visual cortex. Neural Computation 13, 1255\u20131283\n(2001)\n21. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: From\nerror visibility to structural similarity. IEEE Transactions on Image Processing\n13(4), 600\u2013612 (2004), http://www.cns.nyu.edu/~zwang/\n22. W.Gerstner, W.Kistler: Spiking Neuron Models : Single Neurons, Populations,\nPlasticity. Cambridge University Press (2002)\n23. Wohrer, A., Kornprobst, P.: Virtual retina : A biological retina model and simulator, with contrast gain control. Journal of Computational Neuroscience 26(2),\n219\u2013249 (2009)\n24. Wohrer, A., Kornprobst, P., Antonini, M.: Retinal filtering and image reconstruction. Research Report RR-6960, INRIA (2009), http://hal.inria.fr/\ninria-00394547/en/\n25. Zhang, Y., Ghodrati, A., Brooks, D.: An analytical comparison of three spatiotemporal regularization methods for dynamic linear inverse problems in a common\nstatistical framework. Inverse Problems 21, 357 (2005)\n\n\f"}