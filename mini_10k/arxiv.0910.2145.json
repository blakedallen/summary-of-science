{"id": "http://arxiv.org/abs/0910.2145v2", "guidislink": true, "updated": "2011-01-07T09:02:24Z", "updated_parsed": [2011, 1, 7, 9, 2, 24, 4, 7, 0], "published": "2009-10-12T12:12:46Z", "published_parsed": [2009, 10, 12, 12, 12, 46, 0, 285, 0], "title": "Node harvest", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0910.2756%2C0910.2317%2C0910.0331%2C0910.1342%2C0910.0926%2C0910.1155%2C0910.5803%2C0910.5158%2C0910.3831%2C0910.2477%2C0910.1488%2C0910.2821%2C0910.2796%2C0910.4773%2C0910.5507%2C0910.3809%2C0910.3010%2C0910.0761%2C0910.1957%2C0910.4939%2C0910.3847%2C0910.4699%2C0910.3475%2C0910.4491%2C0910.1401%2C0910.4675%2C0910.4015%2C0910.1688%2C0910.1638%2C0910.1472%2C0910.0776%2C0910.5917%2C0910.3160%2C0910.2333%2C0910.1313%2C0910.4683%2C0910.0451%2C0910.0831%2C0910.4921%2C0910.4986%2C0910.1248%2C0910.1265%2C0910.0656%2C0910.1196%2C0910.4918%2C0910.5921%2C0910.3909%2C0910.0416%2C0910.5321%2C0910.1849%2C0910.5453%2C0910.3512%2C0910.0720%2C0910.2928%2C0910.3697%2C0910.1702%2C0910.4934%2C0910.5455%2C0910.0106%2C0910.4157%2C0910.3245%2C0910.3437%2C0910.5902%2C0910.3919%2C0910.3815%2C0910.1999%2C0910.4275%2C0910.4324%2C0910.5319%2C0910.2816%2C0910.4640%2C0910.5183%2C0910.3088%2C0910.4868%2C0910.2145%2C0910.4745%2C0910.2334%2C0910.2715%2C0910.2426%2C0910.0328%2C0910.3825%2C0910.3799%2C0910.5172%2C0910.3058%2C0910.3530%2C0910.0916%2C0910.4990%2C0910.4354%2C0910.0037%2C0910.0104%2C0910.0212%2C0910.5042%2C0910.1418%2C0910.1045%2C0910.1451%2C0910.4343%2C0910.4027%2C0910.1848%2C0910.0034%2C0910.3349%2C0910.0364&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Node harvest"}, "summary": "When choosing a suitable technique for regression and classification with\nmultivariate predictor variables, one is often faced with a tradeoff between\ninterpretability and high predictive accuracy. To give a classical example,\nclassification and regression trees are easy to understand and interpret. Tree\nensembles like Random Forests provide usually more accurate predictions. Yet\ntree ensembles are also more difficult to analyze than single trees and are\noften criticized, perhaps unfairly, as `black box' predictors. Node harvest is\ntrying to reconcile the two aims of interpretability and predictive accuracy by\ncombining positive aspects of trees and tree ensembles. Results are very sparse\nand interpretable and predictive accuracy is extremely competitive, especially\nfor low signal-to-noise data. The procedure is simple: an initial set of a few\nthousand nodes is generated randomly. If a new observation falls into just a\nsingle node, its prediction is the mean response of all training observation\nwithin this node, identical to a tree-like prediction. A new observation falls\ntypically into several nodes and its prediction is then the weighted average of\nthe mean responses across all these nodes. The only role of node harvest is to\n`pick' the right nodes from the initial large ensemble of nodes by choosing\nnode weights, which amounts in the proposed algorithm to a quadratic\nprogramming problem with linear inequality constraints. The solution is sparse\nin the sense that only very few nodes are selected with a nonzero weight. This\nsparsity is not explicitly enforced. Maybe surprisingly, it is not necessary to\nselect a tuning parameter for optimal predictive accuracy. Node harvest can\nhandle mixed data and missing values and is shown to be simple to interpret and\ncompetitive in predictive accuracy on a variety of data sets.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0910.2756%2C0910.2317%2C0910.0331%2C0910.1342%2C0910.0926%2C0910.1155%2C0910.5803%2C0910.5158%2C0910.3831%2C0910.2477%2C0910.1488%2C0910.2821%2C0910.2796%2C0910.4773%2C0910.5507%2C0910.3809%2C0910.3010%2C0910.0761%2C0910.1957%2C0910.4939%2C0910.3847%2C0910.4699%2C0910.3475%2C0910.4491%2C0910.1401%2C0910.4675%2C0910.4015%2C0910.1688%2C0910.1638%2C0910.1472%2C0910.0776%2C0910.5917%2C0910.3160%2C0910.2333%2C0910.1313%2C0910.4683%2C0910.0451%2C0910.0831%2C0910.4921%2C0910.4986%2C0910.1248%2C0910.1265%2C0910.0656%2C0910.1196%2C0910.4918%2C0910.5921%2C0910.3909%2C0910.0416%2C0910.5321%2C0910.1849%2C0910.5453%2C0910.3512%2C0910.0720%2C0910.2928%2C0910.3697%2C0910.1702%2C0910.4934%2C0910.5455%2C0910.0106%2C0910.4157%2C0910.3245%2C0910.3437%2C0910.5902%2C0910.3919%2C0910.3815%2C0910.1999%2C0910.4275%2C0910.4324%2C0910.5319%2C0910.2816%2C0910.4640%2C0910.5183%2C0910.3088%2C0910.4868%2C0910.2145%2C0910.4745%2C0910.2334%2C0910.2715%2C0910.2426%2C0910.0328%2C0910.3825%2C0910.3799%2C0910.5172%2C0910.3058%2C0910.3530%2C0910.0916%2C0910.4990%2C0910.4354%2C0910.0037%2C0910.0104%2C0910.0212%2C0910.5042%2C0910.1418%2C0910.1045%2C0910.1451%2C0910.4343%2C0910.4027%2C0910.1848%2C0910.0034%2C0910.3349%2C0910.0364&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "When choosing a suitable technique for regression and classification with\nmultivariate predictor variables, one is often faced with a tradeoff between\ninterpretability and high predictive accuracy. To give a classical example,\nclassification and regression trees are easy to understand and interpret. Tree\nensembles like Random Forests provide usually more accurate predictions. Yet\ntree ensembles are also more difficult to analyze than single trees and are\noften criticized, perhaps unfairly, as `black box' predictors. Node harvest is\ntrying to reconcile the two aims of interpretability and predictive accuracy by\ncombining positive aspects of trees and tree ensembles. Results are very sparse\nand interpretable and predictive accuracy is extremely competitive, especially\nfor low signal-to-noise data. The procedure is simple: an initial set of a few\nthousand nodes is generated randomly. If a new observation falls into just a\nsingle node, its prediction is the mean response of all training observation\nwithin this node, identical to a tree-like prediction. A new observation falls\ntypically into several nodes and its prediction is then the weighted average of\nthe mean responses across all these nodes. The only role of node harvest is to\n`pick' the right nodes from the initial large ensemble of nodes by choosing\nnode weights, which amounts in the proposed algorithm to a quadratic\nprogramming problem with linear inequality constraints. The solution is sparse\nin the sense that only very few nodes are selected with a nonzero weight. This\nsparsity is not explicitly enforced. Maybe surprisingly, it is not necessary to\nselect a tuning parameter for optimal predictive accuracy. Node harvest can\nhandle mixed data and missing values and is shown to be simple to interpret and\ncompetitive in predictive accuracy on a variety of data sets."}, "authors": ["Nicolai Meinshausen"], "author_detail": {"name": "Nicolai Meinshausen"}, "author": "Nicolai Meinshausen", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1214/10-AOAS367", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0910.2145v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0910.2145v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "Published in at http://dx.doi.org/10.1214/10-AOAS367 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "arxiv_primary_category": {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.AP", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0910.2145v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0910.2145v2", "journal_reference": "Annals of Applied Statistics 2010, Vol. 4, No. 4, 2049-2072", "doi": "10.1214/10-AOAS367", "fulltext": "The Annals of Applied Statistics\n2010, Vol. 4, No. 4, 2049\u20132072\nDOI: 10.1214/10-AOAS367\nc Institute of Mathematical Statistics, 2010\n\narXiv:0910.2145v2 [stat.ML] 7 Jan 2011\n\nNODE HARVEST\nBy Nicolai Meinshausen\nUniversity of Oxford\nWhen choosing a suitable technique for regression and classification with multivariate predictor variables, one is often faced with\na tradeoff between interpretability and high predictive accuracy. To\ngive a classical example, classification and regression trees are easy to\nunderstand and interpret. Tree ensembles like Random Forests provide usually more accurate predictions. Yet tree ensembles are also\nmore difficult to analyze than single trees and are often criticized,\nperhaps unfairly, as 'black box' predictors.\nNode harvest is trying to reconcile the two aims of interpretability\nand predictive accuracy by combining positive aspects of trees and\ntree ensembles. Results are very sparse and interpretable and predictive accuracy is extremely competitive, especially for low signal-tonoise data. The procedure is simple: an initial set of a few thousand\nnodes is generated randomly. If a new observation falls into just a\nsingle node, its prediction is the mean response of all training observation within this node, identical to a tree-like prediction. A new\nobservation falls typically into several nodes and its prediction is then\nthe weighted average of the mean responses across all these nodes.\nThe only role of node harvest is to 'pick' the right nodes from the initial large ensemble of nodes by choosing node weights, which amounts\nin the proposed algorithm to a quadratic programming problem with\nlinear inequality constraints. The solution is sparse in the sense that\nonly very few nodes are selected with a nonzero weight. This sparsity is not explicitly enforced. Maybe surprisingly, it is not necessary\nto select a tuning parameter for optimal predictive accuracy. Node\nharvest can handle mixed data and missing values and is shown to\nbe simple to interpret and competitive in predictive accuracy on a\nvariety of data sets.\n\n1. Introduction. Let Y = (Y1 , . . . , Yn ) be a vector of n observations of\na univariate real-valued response and X be the n \u00d7 p-dimensional matrix,\nwhere the row-vector Xi* \u2208 X is the p-dimensional covariate for the ith\nReceived October 2009; revised April 2010.\nKey words and phrases. Trees, tree ensembles, machine learning, Random Forests,\nsparsity, quadratic programming.\n\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Applied Statistics,\n2010, Vol. 4, No. 4, 2049\u20132072. This reprint differs from the original in pagination\nand typographic detail.\n1\n\n\f2\n\nN. MEINSHAUSEN\n\nobservation for i = 1, . . . , n. When trying to predict a new response, given\ncovariates, regression trees [Breiman et al. (1984)] are attractive since they\nare very simple to build and understand. They are one example of a wider\nrange of recursive partitioning methods. For the sake of notational simplicity,\nlet the notion of a node in a tree and the corresponding subspace of X be\nidentical. Let Q be a collection of q nodes, where a node Qg \u2208 Q, g = 1, . . . , q,\nis defined by a rectangular subspace of X ,\n(g)\n\nQg = {x \u2208 X : xk \u2208 Ik\n\nfor k = 1, . . . , p},\n\n(g)\n\nand each interval Ik is a subset of the support of the kth covariate.\nThe leaf nodes of a tree form a partition of X in that their union is\nidentical to X and all pairwise intersections are empty. If each leaf node is\nan element of Q, the partition corresponding to a tree can be expressed by\na weight vector w \u2208 {0, 1}q , where wg = 0 means that node g is not used in\nthe partition, while wg = 1 means that node g is used in the partition. The\ntree-style prediction \u0176 (x) at a point x \u2208 X is then the observed mean over\nall training observations in the same node,\n(1)\n\n\u0176 (x) =\n\nq\nX\n\n\u03bcg 1{x \u2208 Qg }wg ,\n\ng=1\n\nwhere \u03bcg is the mean over all observations falling into node Qg ,\nPn\ni=1 1{Xi*\u2208Qg }Yi\n\u03bcg = P\n.\nn\ni=1 1{Xi*\u2208Qg }\n\nThe predictions on the n observed samples can be conveniently written\nas Mw, where M is the n \u00d7 q-dimensional matrix, with row entries for\ni = 1, . . . , n given by\n\u001a\n\u03bcg ,\nif Xi* \u2208 Qg\n(2)\nfor g = 1, . . . , q = |Q|.\nMig =\n0,\nif Xi* \u2208\n/ Qg\nThe empirical squared error loss on the training samples is then\n(3)\n\nkY \u2212 Mwk2\n\nand trees try to pick a partitioning by a tree (and a weight vector w equivalently) that minimizes this empirical loss (3), under certain complexity\nconstraints on the tree. These complexity constraints can, for example, entail a penalty on tree size or a lower bound on the number of observations in\neach node [Breiman et al. (1984)]; for an alternative approach see Blanchard\net al. (2007). The optimal values of the complexity constraints are typically\ndetermined by cross-validation.\nCompared to single regression trees, predictive accuracy is often improved\nby tree ensembles. Boosting [Freund and Schapire (1996); Friedman, Hastie\n\n\fNODE HARVEST\n\n3\n\nand Tibshirani (2000)], bagging [Breiman (1996a)] and Random Forests\n[Breiman (2001)] are popular techniques to create these ensembles. Predictions are weighted averages over the output of all trees in the ensemble.\nThey thus effectively allow an observation to be part of more than one node.\nFor Random Forests [Breiman (2001)], each of m trees in the ensemble receives equal weight 1/m. If all leaf nodes of the Random Forest are part\nof the set Q above, the empirical loss can again be written as in (3) with\nthe only difference that now wg \u2208 {0, 1/m, 2/m, . . . , 1} instead of the binary\nweights wg = {0, 1} for trees. If a node appears only once in the ensemble,\nits weight is 1/m. If it appears more than once, the associated weight is the\ncorresponding multiple of 1/m, up to a maximum of 1 if the node appears\nin every tree of the ensemble.\nHere, we explore the possibility of allowing arbitrary weights wg \u2208 [0, 1].\nRather than growing trees greedily, we start from a large set Q of potential\nnodes that are either obtained by random splits or picked from an initial\ntree ensemble, just as in 'Rule ensembles' [Friedman and Popescu (2008)].\nWhile 'Rule ensembles' uses the nodes as binary indicator variable in a\nlinear model with an l1 -penalty on coefficients, node harvest retains treelike predictions of the form (1). The only task of node harvest is finding\nsuitable weights on nodes. Minimizing the empirical loss (3) under suitable\nconstraints on the weights turns out to be a quadratic program with linear\ninequality constraints, which can be solved efficiently.\nThe goal of the proposed node harvest procedure is two-fold: On the one\nhand, a very competitive predictive accuracy (with practically no adjustment\nof tuning parameters). On the other hand, simple, interpretable results and\npredictions.\nRandom Forests satisfy the first of these demands but not necessarily the\nlatter since hundreds of large trees with thousands of nodes are involved\nin the final decision. Marginal importance measures can be calculated as\nproposed in [Breiman (2001)], but they only describe some limited characteristics of the fitted function and certainly do not explain the whole fit.\nTrees, on the other hand, satisfy the second constraint but fall short of optimal predictive accuracy. Moreover, if tree size is chosen by cross-validation,\nthe interaction order (tree depth) can be very high, lowering interpretability. Node harvest has the advantage of delivering very accurate results while\nusing in general only main effects and two-factor interactions.\nNode harvest is introduced in Section 2. An extension to binary classification, dealing with missing values and additional regularization of the\nestimator are covered in Section 3, while numerical results are shown in\nSection 4.\n2. Node harvest. Node harvest (NH) is introduced, along with an efficient algorithm to solve the involved quadratic programming problem. Some\nbasic properties of the estimator are established.\n\n\f4\n\nN. MEINSHAUSEN\n\n2.1. Optimal partitioning. The starting point of NH is loss function (3).\nSuppose one would like to obtain a partitioning of the space that minimizes\nthe empirical loss (3). One could collect a very large number of nodes into\nthe set Q that satisfy desired complexity criteria. Typical complexity criteria\nare a minimal node size or maximal interaction depth (tree depth). An\nempirically optimal partitioning would search for a weight vector such that\nthe empirical loss is minimal,\n\u0175 = argmin kY \u2212 Mwk2\nw\n\n(4)\n\nsuch that w \u2208 {0, 1}q and {Qg : wg = 1} is a partition of X .\n\nThe selected set {Qg : wg = 1} \u2282 Q of nodes is understood to form a partition\niff the intersection between all selected nodes is empty and their union is\nthe entire space X . Even if given a collection Q of nodes, the optimization\nproblem above is very difficult to solve. The constraint w \u2208 {0, 1}q does not\ncorrespond to a convex feasible region. Moreover, the constraint that the\nselected set of nodes form a partition of the space is also awkward to handle\ncomputationally.\nThe latter problem can be circumvented by demanding instead that the\npartition is a proper partitioning for the empirically observed data only in\nthe sense that each data point is supposed to be part of exactly one node.\nThis loosening of the constraint will be very helpful at a later stage. It might\ncreate the situation that a new observation will not belong to any node, but\nthis will turn out to be not a problem in the NH approach since every\nobservation will be a member of the root node and the root node always\nreceives a small positive weight, which is discussed further below.\nTo form such an empirical partitioning, let I be the n \u00d7 q matrix indicating\nwhether or not an observation falls into a given leaf. For all rows i = 1, . . . , n,\n\u001a\n1,\nif Xi* \u2208 Qg\n(5)\nIig =\nfor g = 1, . . . , q.\n0,\nif Xi* \u2208\n/ Qg\nThe constraint that each data point be part of one and exactly one node is\nequivalent to demanding that Iw = 1, understood componentwise. Since w \u2208\n{0, 1}q , this simple linear equality constraint ensures that each observation\nis part of exactly one selected node.\nGiven a collection Q of nodes, a weight vector w could thus be found by\nthe constrained optimization\n(6)\n\n\u0175 = argmin kY \u2212 Mwk2\n\nsuch that Iw = 1 and w \u2208 {0, 1}q .\n\nw\n\nFor the n observed data points, this problem is equivalent to (4), yet it is\nstill NP-hard to solve in general due to the nonconvex feasible region of\nthe constraint w \u2208 {0, 1}q . Tree ensembles relax this constraint and average\n\n\fNODE HARVEST\n\n5\n\nover several trees, implicitly allowing weights to take on values in the interval\n[0, 1]. It thus seems natural to relax the nonconvex constraint w \u2208 {0, 1}q\nand only ask for nonnegativity of the weights.\n2.2. Node harvest. The main idea of NH is that it becomes computationally feasible to solve the optimal empirical partitioning problem (6) if the\nweights are only constrained to be nonnegative. The weights across all nodes\nfor a single observation still have to sum to 1 (as they do for all weighted\ntree ensembles), but this constraint is equivalent to Iw = 1, and we can relax\n(6) to the convex optimization problem\n(7)\n\n\u0175 = argmin kY \u2212 Mwk2\n\nsuch that Iw = 1 and w \u2265 0.\n\nw\n\nThis estimator is called the node harvest (NH) estimator since a small subset\nof nodes is 'picked' or selected from a large initial ensemble of nodes. It\nwill turn out that the vast majority of nodes in this large ensemble will\nreceive a zero weight, without the sparsity being enforced explicitly other\nthan through the constraint Iw = 1. Nodes g which receive a zero weight\n(\u0175g = 0) can be ignored for further analysis.\nThe constraints in (7) are satisfied, for example, by setting the weight of\nthe root node, which is always included in Q and contains all observations,\nequal to 1 and all other weights to 0. The set of solutions is thus always\nnonempty. The solution to (7) is also either unique or the set of solutions is a\nconvex set. In the latter case, we define \u0175 for definiteness to be the solution\nthat has minimal l2 -norm among all solutions in this convex set, which\namounts to adding a small ridge penalty \u03bdkwk22 to the objective function in\n(7) and letting \u03bd \u2192 0. Other solutions are possible, but adding a very small\nridge penalty guarantees, moreover, positive definiteness of the quadratic\nform and facilitates computation of (7) even if the solution is unique.\nThe prediction for new data is then simply a weighted average over node\nmeans. For the training data, this is still the vector Mw. The prediction\n\u0176 (x) for a new data point x \u2208 X is the weighted average over all nodes that\nx falls into,\nP\ng\u2208G \u0175g \u03bcg\n(8)\n,\n\u0176 (x) = P x\ng\u2208Gx \u0175g\nwhere Gx := {g : x \u2208 Qg } is the collection of nodes that observation x falls\ninto.\nThe denominator in (8) is constrained to be 1 for all n training samples\nsince Iw = 1 is enforced. For new observations outside the training set, the\nweights in the denominator do not necessarily sum to 1. We always let the\nroot node be a member of the set Q, where the root node is defined as\ncontaining the entire predictor space X . We demand that the weight of the\n\n\f6\n\nN. MEINSHAUSEN\n\nroot node is bounded from below not by 0 as for all other nodes, but by a very\nsmall weight chosen here as 0.001 and converging to 0 for increasing sample\nsizes. The set Gx in (8) is then always nonempty and the denominator in (8)\nis bounded from below by 0.001, although it will typically be in the region\nof 1 for new observations. In the unlikely event that a new observation is\nnot part of any node except the root node, its prediction will, according\nto (8), be the node mean of the root node. This is identical to the mean\nresponse over all observations in the training data, a reasonable answer if a\nnew observation should fail to fall into any selected node.\n2.3. Tuning parameters. The NH procedure requires only an initial set of\nnodes Q. Once this set is specified, there are no further tuning parameters.\nIt will turn out that results are very insensitive to the actual choice of the\nset of nodes as long as q = |Q| is sufficiently large and some complexity\nconstraints, such as maximal interaction order and minimal node size, are\nfollowed.\nThere are three essential characteristics of the set Q: the number of nodes,\nmaximal interaction order and minimal node size. We discuss these constraints in the following, but an advantageous aspect of the proposed method\nis that the method is competitive in terms of predictive accuracy for the default choices proposed below. In fact, all numerical results are computed\nwith the same defaults parameters for maximal interaction order, which is\nset to 1, and minimal node size, which is set to 5.\nNumber of nodes. It will be shown empirically for many data sets that the\nperformance is continuously improving the more nodes q = |Q| are added to\nthe initial set of nodes. Solving (7) gets clearly more costly as q increases.\nOne should thus use as many nodes as can be afforded computationally.\nTypically, q ranges in the hundreds or thousands. All examples are calculated with q = 1000 nodes. It is maybe surprising that there is practically\nno overfitting if q is chosen very large. A first attempt at explaining this\nphenomenon can be found in Proposition 1.\nMaximal interaction order. The maximal interaction order of node Qg is\nthe number of variables that are necessary to determine whether an observation is part of a node or not. Main effects have thus an interaction order 1.\nTo keep results as interpretable as possible, a maximal interaction order of\n2 (equivalent to a two-factor interaction) is chosen for almost all examples.\nMinimal node size. The minimal node size ming |{i : Xi* \u2208 Qg }| has an\ninfluence on the amount of smoothing. Allowing nodes with just a single\nobservation, the algorithm could simply interpolate all observed data by\nassigning weights of 1 to the n nodes that contain each exactly one of the\nn observations. This is clearly undesirable and a minimal node size of 5 is\nimposed throughout. Again, results could be improved for some data sets\nby tuning this choice, yet the results show that a choice of 5 gives very\ncompetitive results across a remarkably wide range of data sets.\n\n\f7\n\nNODE HARVEST\n\n2.4. Node generation. To generate the desired nodes, one can generate\nnodes at random, without use of the response variable. Alternatively, one\ncan use a data-adaptive choice by using nodes from a fitted tree ensemble.\nResults seem very insensitive to this choice, but the latter method requires\nin general fewer nodes in the initial set Q for a close to optimal predictive\naccuracy. We thus follow the latter approach. The set Q is initially empty. A\nnew tree is grown as proposed in Breiman (2001) for each tree in a Random\nForest (RF) ensemble. To speed up computation and increase diversity of\nthe set, the trees are fitted on subsamples of the data of size n/10 rather\nthan bootstrap samples. All the nodes of the tree that satisfy the maximal\ninteraction order and minimal node size constraint are added to the set Q,\nprovided that they are not already present in the set. While the size of Q\nis less than the desired number q, the procedure is repeated. If two or more\nnodes in Q contain exactly the same set of training observations, only a\nrandomly chosen one of them is kept.\n2.5. Algorithm and dimensionality reduction. As stated above, the initial\nset of nodes Q is generated with a Random Forests approach. After the\ndesired number q of nodes have been obtained, it only remains to solve (7).\nThis is a quadratic program (QP) with linear constraints and could be solved\nwith standard QP solvers. However, the specific structure of the problem can\nbe used to reduce dimensionality and make the computation more efficient.\nWe suppose that the root node, containing all observations, is the first\namong all q = |Q| nodes. Let wroot be the vector wroot = (1, 0, 0, . . . , 0).\nClearly, Iwroot = 1 componentwise, so the equality constraint in (7) is fulfilled for wroot . This means that the difference \u0175 \u2212 wroot between the actual\nsolution and the 'root' solution wroot lies in the nullspace NI \u2286 Rq of I.\nLet q\u0303 be the dimension of NI . Since I is of rank at most min{q, n}, we have\nq\u0303 \u2265 q \u2212 min{q, n}, and the nullspace NI is guaranteed to be nontrivial (q\u0303 > 0)\nfor q > n, that is, if there are more nodes than actual observations, which\nwe can always satisfy by generating sufficiently many nodes. If the nullspace\nis nontrivial, then let B be the q \u00d7 q\u0303-dimensional matrix, where the kth column, with k = 1, . . . , q\u0303, contains the kth basis vector of an arbitrarily chosen\northonormal basis of NI . The solution to (7) can then be written, using the\nargument above, for some d\u0302 \u2208 Rq\u0303 as \u0175 = wroot + Bd\u0302, and, to get the same\nsolution as in (7), d\u0302 is the solution to\nd\u0302 = argmin \u22122dT (MB)T (Y \u2212 Y) + dT (MB)T (MB)d\n(9)\n\nd\n\nsuch that Bd \u2265 \u2212wroot ,\n\nwhere it was used that Mwroot = Y by definition of wroot . If a small ridge\npenalty \u03bdkwk22 on w is added to guarantee uniqueness of the solution, a\n\n\f8\n\nN. MEINSHAUSEN\n\nterm \u03bdk(wroot + Bd)k22 is added to the objective function in (9), where here\nalways \u03bd = 0.001 under a standardized response with Var(Y) = 1. To also\nensure that the weight of the root node is bounded from below by the small\nchosen value 0.001 instead of 0, the constraint Bd \u2265 \u2212wroot in (9) needs to\nbe replaced by Bd \u2265 \u22120.999wroot .\nThus, the original q-dimensional problem is reduced to a q\u0303 \u2265 q \u2212min{q, n}dimensional one. A price to pay for this is the computation of a basis for\nthe nullspace NI of I, which is achieved by a SVD of I. Compared to the\nsavings in the QP solution, computation of the SVD is, however, very much\nworthwhile. The remaining QP problem (9) is solved with the QP solver of\nGoldfarb and Idnani (1983), as implemented in the package quadprog of the\nR-programming language [R Development Core Team (2005)]. It is conceivable that an alternative interior-point algorithm and especially explicit use\nof the sparse structure of the matrixes M and I would generate additional\ncomputational savings, but, even so, it took less than 10 seconds to solve\n(9) on data sets with less than 103 observations, using a 2.93 GHz processor\nand 8 GB of RAM.\n2.6. Smoothing. NH can be seen as a smoothing operation in that \u0176 =\nSY for a data-adaptive choice of the smoothing matrix S. The smoothing\nmatrix is doubly stochastic, symmetric and has nonnegative entries.\nLemma 1. The fitted values \u0176 are obtained as a linear transformation\n\u0176 = SY of the original\ndata, where S is a doubly stochastic\nand symmetric\nP\nP\nmatrix in that\nj Sij = 1 for all i = 1, . . . , n and\ni Sij = 1 for all j =\n1, . . . , n. Moreover, Sij \u2265 0 for all i, j = 1, . . . , n.\nProof. The fitted values are for the n training P\nobservations given by\n\u0176 = M\u0175, with M defined in (2). Therefore, \u0176i = qg=1 1{i \u2208 Qg }\u0175g \u03bcg ,\nwhere i \u2208 Qg is a shorthand notation for Xi* \u2208 Qg P\n. Let ng = |{j : j \u2208 Qg }| be\nthe number of samples in node g. Then \u03bcg = n\u22121\ng\nj\u2208Qg Yj by definition of\nthe node means and, hence, putting together,\n\u0176i =\n\nq\nX\ng=1\n\n1{i \u2208 Qg }\u0175g n\u22121\ng\n\nn\nX\nj=1\n\n1{j \u2208 Qg }Yj =\n\nq\nn X\nX\n\u0175g 1{i, j \u2208 Qg }\nj=1 g=1\n\nng\n\nYj .\n\nP\nDefining matrix S \u2208 Rn\u00d7n by its entries Sij = g \u0175g n\u22121\ng 1{i, j \u2208 Qg }, it follows that (a) \u0176 = SY, (b) S is symmetric\nand\n(c)\nthat\nall entries are nonP\nnegative. It remains to show that j Sij = 1 for all i = 1, . . . , n. The colP\nP P\nj \u2208 Qg } =\numn sums follow by symmetry. Now, j Sij = j g \u0175g n\u22121\ng 1{i,P\nP\n\u0175\n1{i\n\u2208\nQ\n}.\nBy\ndefinition\nof\nthe\nmatrix\nI,\nthe\nright-hand\nside\ng\ng \u0175g 1{i \u2208\ng g\nQg } is identical to the ith coefficient in I\u0175. Since, componentwise, I\u0175 = 1\n\n\f9\n\nNODE HARVEST\n\nby (7), it follows that indeed\nthe proof. \u0003\n\nP\n\nj\n\nSij = 1 for all i = 1, . . . , n, which completes\n\nFrom the lemma above, one can immediately derive that the mean \u0176 of\nthe fitted values is identical to the mean Y of the observed values. And\nthe lemma above also ensures that, irrespective of the size q of the initial\nensemble, it is impossible to fit the response exactly by interpolation if the\nminimal node size is strictly larger than 1.\nProposition 1. The mean of the fitted and observed values agree, \u0176 =\nY. Moreover, if the minimal node size is larger than 1, the weight of the\nroot node is strictly positive and Var(Y) 6= 0, it holds for any strictly convex\nreal-valued function f that\nn\nX\n\n(10)\n\ni=1\n\nf (\u0176i ) <\n\nn\nX\n\nf (Yi ).\n\ni=1\n\nProof. The\nPn first claim\nPn follows directly\nPn from Lemma 1 since \u0176 = SY\nand, hence,\n\u0176\n=\nS\nY\n=\ni=1 i\ni,j=1 ij j\nj=1 Yj , where the last equality\nP\nfollows by the fact that\ni Sij = 1 for all j = 1, . . . , n from Lemma 1.\nLikewise, observe that Sij < 1 for all i, j = 1, . . . , n if the minimal node\nsize isPlarger than 1. This follows from the definition of S by the entries\nSij = g \u0175g n\u22121\ng 1{i, j \u2208 Qg } since more than 1 entry in each row-vector Si* ,\niP= 1, . . . , n, has to be nonzero. Since the sum of the row is constrained to\nj Sij = 1 and all entries in S are nonnegative, all entries have got to be\nstrictly less than 1. Moreover, if the weight of the root node is positive, all\nentries Si,j are strictly positive. Hence, for a strictly convex function f ,\n!\nn\nn\nn X\nn\nn\nn\nX\nX\nX\nX\nX\nf (Yj ),\nSij f (Yj ) =\nSij Yj <\nf\nf (\u0176i ) =\ni=1\n\ni=1\n\nj=1\n\ni=1 j=1\n\nj=1\n\nhaving used Var(Y)\nP 6= 0 and the strict positivity of all entries of S in the\ninequality and\nj Sij = 1 for all i = 1, . . . , n, from Lemma 1 in the last\nequality. \u0003\nThe second part of the result can be obtained if the condition that the\nweight of the root nodes is positive is replaced with the following weaker\ncondition: there exists a pair of observations Yi , Yj with Yi 6= Yj such that\nboth i and j are members of a node Qg and the weight \u0175g is strictly positive.\nThe proposition implies that the observed data cannot be interpolated\nexactly by NH even though the number q of nodes might greatly exceed\nsample size n.\n\n\f10\n\nN. MEINSHAUSEN\n\n2.7. Related work. There has been substantial interest in the Random\nForests framework for classification and regression [Breiman (2001)], which\nbuilds partly upon the randomized tree idea in Amit and Geman (1997). Lin\nand Jeon (2006) interpreted Random Forests as an adaptive nearest neighbor scheme, with the distance metric given by the grown tree ensemble. The\nsame interpretation is maybe even more imminent for NH since predictions\nare explicitly averages over node means. Both bagging [Breiman (1996a)]\nand boosting [Freund and Schapire (1996); Friedman, Hastie and Tibshirani\n(2000)] are possible alternative and powerful techniques for growing multiple trees. If using either of these, predictions are formed by averaging in a\npossibly weighted form across all grown trees. Results are often difficult to\ninterpret, though, as each of possibly hundreds of grown trees consists in\nturn of multiple nodes and all variables in the data set are often involved\nsomewhere in the ensemble. The influence of individual variables can only\nbe measured indirectly for such tree ensembles; see Strobl et al. (2007) for a\nmore involved discussion. Despite a similar sounding name, 'tree harvesting'\n[Hastie et al. (2001)], a regression technique commonly used in computational biology, is not closely related to NH. An interesting machine learning\ntechnique is 'stacking' [Wolpert (1992); Breiman (1996b)], which is weighting various classifiers and weights are chosen by minimizing the error on\nweighted leave-one-out predictions. In contrast to stacked trees, however,\nNH is not weighting whole trees but is working at the level of individual\nnodes by reweighting each node. In a similar spirit, the 'Rule Ensemble' algorithm by Friedman and Popescu (2008) simplifies interpretability of tree\nensembles by selecting just a few nodes across all trees. Each node is seen to\nform a binary indicator variable and the prediction is a linear combination\nof all these indicator variables. In fact, for a given collection Q of nodes, the\nmatrix whose columns correspond to the binary indicator variables is exactly\nthe matrix defined as I in (5). The linear combination \u03b2 of nodes is then\nsought in a Lasso-style way by putting a constraint on the l1 -norm of the\ncoefficient vector [Tibshirani (1996); Chen, Donoho and Saunders (2001)],\n(11)\n\n\u03b2\u0302 \u03bb = argmin kY \u2212 I\u03b2k2\n\nsuch that k\u03b2k1 \u2264 \u03bb.\n\n\u03b2\n\nThe original variables can be added to the matrix I of binary indicator\nvariables. Despite the superficial similarity of 'Rule Ensembles' with NH,\nthere are fundamental differences to the NH procedure (7). Choosing the\nright tuning parameter \u03bb is essential in (11), but no such tuning is necessary\nfor NH. The inherent reason for this is that NH imposes much stronger\nregularization by requiring in (8) that predictions are weighted node means.\nNH is only selecting the weights w in (7), whereas the vector \u03b2 in (11)\ncannot be interpreted as the weight attached to a particular node or rule.\nThe sign and magnitude of the coefficient \u03b2g is thus not directly related to\n\n\fNODE HARVEST\n\n11\n\nthe average response of observations in node g. A possible advantage of NH\nis thus the interpretability of the predictions as weighted node means. An\nexample is shown in the breast cancer example in Figure 3. If a new patient\nfalls into only a single node, the NH prediction is simply the average response\nin the group of patients, which is very easy to communicate and relate to the\nactually observed data. If he or she falls into several groups, the prediction\nis the weighted average across these groups. In terms of predictive power,\nrule ensembles seem to be often better than NH and also Random Forests\nin our experience if the signal-to-noise is high [Meinshausen (2009)]. The\nstrength of NH is its ability to cope well with very low signal-to-noise ratio\ndata and the two approaches seem complementary in this regard. Both 'Rule\nEnsembles' and NH can make use of a dictionary of rules, which is currently\nbuilt either randomly or by harvesting nodes from existing tree ensembles\nsuch as Random Forests. More general nodes, such as spheres under various\nmetrics that are centered at training observations, could conceivably help\nimprove both methods.\n3. Extensions. Node harvest (NH) can be extended and generalized in\nvarious ways, as briefly outlined below. NH is shown to be directly applicable\nto binary classification. Missing values can easily be dealt with, without using imputation techniques or surrogate splits when predicting the response\nfor new observations with missing values. Finally, a regularization is proposed that can reduce the number of selected nodes.\n3.1. Classification. For binary classification with Y \u2208 {0, 1}, the nonconvex misclassification loss is typically replaced with a convex majorant\nof this loss function [Bartlett, Jordan and McAuliffe (2003)]. One of these\npossible convex loss functions is the L2 -loss, as used for classification in Yu\nand B\u00fchlmann (2003).\nSimply applying the previous QP problem (7) on binary data leads to a\nprediction \u0176 (x) at a new data point x which is identical to (8). The node\nmeans \u03bcg , g = 1, . . . , q, are now equivalent to the fraction of samples in class\n\"1\" among all samples in node Qg ,\n\u03bcg =\n\n|{i : Xi* \u2208 Qg and Yi = 1}|\n.\n|{i : Xi* \u2208 Qg }|\n\nThe NH predictions are naturally in the interval [0, 1]. Use of the L2 -loss as\na convex surrogate for misclassification error is thus not only appropriate for\nNH, it is even beneficial since it allows for an interpretation of the predictions\n\u0176 (x) as weighted empirical node means.\n\n\f12\n\nN. MEINSHAUSEN\n\n3.2. Missing values. An interesting property of NH is its natural ability\nto cope with missing values. Once a fit is obtained, predictions for new data\ncan be obtained without use of imputation techniques or surrogate splits. To\nfit the node harvest estimator with missing data, we replace missing values\nin the matrix X by the imputation technique described in Breiman (2001)\nand Liaw and Wiener (2002) and proceed just as previously.\nSuppose then that the node harvest estimator is available and one would\nlike to get a prediction for a new observation Xi* that has missing values\nin some variables. We still calculate the prediction as the weighted mean\n(8) over all nodes of which the new observation is a member. The question\nis whether observation i is part of node Qg \u2208 Q if it has missing values\nin variables that are necessary to evaluate group membership of node Qg .\nThe simplest and, as it turns out, effective solution is to say that i is not a\nmember of a node if it has missing values in variables that are necessary to\nevaluate membership of this node. To make this more precise, let Qg be a\nnode\n(g)\n\nQg = {x \u2208 X : xk \u2208 Ik\n\nfor all k \u2208 {1, . . . , p}},\n\nand let Kg \u2286 {1, . . . , p} be the set of variables that are necessary and suffi(g)\ncient to evaluate node membership [sufficient in the sense that Ik is identical to the entire support of xk for all k \u2208\n/ Kg and necessary in the sense that\n(g)\nIk is not identical to the support of xk for all k \u2208 Kg ]. If x has missing\nvalues, we define\n(g)\n\nx \u2208 Qg if and only if, for all k \u2208 Kg , xk is not missing and xk \u2208 Ik .\nSince we usually only work with main effects and two-factor interactions,\nall nodes require only one or two variables to evaluate node membership.\nEven with missing values in Xi,* , observation i can still be a member of\nmany nodes in Q, namely, those that involve only variables where the ith\nobservation has nonmissing values. In the most extreme case, all variables\nare missing from a new observation. The observation will then only be a\nmember of the root node and the prediction is the node mean of the root\nnode, which is the mean of the response variable across all training observations, maybe not an unreasonable answer in the absence of any information.\nIn more realistic cases, the new observation will have some nonmissing variables and be a member of more than the root node and the prediction will be\nmore refined. With trees, a similar idea would amount to dropping a new observation down a tree and stopping at the first node where the split-variable\nis missing. The prediction would then naturally be the mean response of\nobservations within this node. However, if the variables on which the root\nnode is split are missing, the predicted response will be the mean across\nall observations. This situation occurs for NH only typically if all variables\nare missing. The use of surrogate variables [Breiman et al. (1984)] is thus\nparamount for trees, while NH can take a more direct approach.\n\n\f13\n\nNODE HARVEST\n\n3.3. Regularization. There is so far no tuning parameter in the NH procedure apart from the choice of the large initial set Q of nodes. And results\nare rather insensitive to the choice of Q as long as it is chosen large enough,\nas shown in the next section with numerical results.\nEven though often not necessary from the point of predictive accuracy,\nthe method can be regularized to further improve interpretability. Here it\nis proposed to constrain the average number of samples in each node. From\nthe outset, the minimal node size of 5 ensures that the average fraction of\nsamples in each node is above 5/n. Even so, one might not like to select\nmany nodes that contain only a handful of observations. The fraction of\nsamples in node g is ng /n and the weighted mean across all nodes is\nP\ng \u0175g (ng /n)\nP\n(12)\n,\n\u0175g\n\nwhere ng = |{j : j \u2208 Qg }| is again the number of samples in node g. Since\nI\u0175 = 1 by (7), we have, by summing over the rows of this equality,\nn=\n\nq\nn X\nX\n\nIig \u0175g =\n\nq\nX\ng=1\n\ni=1 g=1\n\n\u0175g\n\nn\nX\n\nIig =\n\ni=1\n\nq\nX\n\n\u0175g ng ,\n\ng=1\n\nwhere the last equality stems from the definition of matrix I in (5). The\nnominator in (12) is thus 1 and the weighted average fraction of samples\n(12) within nodes is, maybe surprisingly, equal to the inverse of the l1 -norm\nof the weight vector \u0175,\nP\n1\ng \u0175g (ng /n)\nP\n(13)\n=P\n= k\u0175k\u22121\n1 .\n\u0175\n\u0175\ng\ng\ng\ng\n\nConstraining the l1 -norm of \u0175 to be less than a positive value of \u03bb \u2208 [1, \u221e]\nconstrains thus the average fraction of samples (13) to be at least 1/\u03bb. For\n\u03bb = 1, every node with nonzero weight has to contain all n samples and only\nthe root node is thus selected for \u03bb = 1. At the other extreme, let m be the\nminimal node size (here m = 5). For \u03bb > n/m, the constraint will have no\neffect at all, since all nodes have ng \u2265 m anyhow and the average weighted\nfraction (13) is thus bounded from below by m/n for all weight vectors. The\nregularized estimator \u0175\u03bb solves then\n\u0175\u03bb = argmin kY \u2212 Mwk2\n(14)\n\nw\n\nsuch that Iw = 1 and w \u2265 0 and kwk1 \u2264 \u03bb\n\ninstead of (7). The interesting region is \u03bb \u2208 [1, m/n], where m is the enforced\nlower bound on node size. From the point of predictive accuracy, constraining\n\u03bb is usually not beneficial unless the signal-to-noise ratio is very low. There\nis thus a tradeoff between sparsity (number of selected nodes) and predictive\npower, as shown in the next section with numerical results.\n\n\f14\n\nN. MEINSHAUSEN\n\n4. Numerical results. For various data sets, we look at the predictive\naccuracy of node harvest (NH) and various related aspects like sensitivity\nto the size of the initial set of nodes, interpretability and predictive power\nof results under additional regularization as in (14).\n4.1. Example I: Two-dimensional sinusoidal reconstruction. As a very\nsimple first example, assume that the random predictor variable x = (x1 , x2 )\nis two-dimensional and distributed uniformly on [0, 1]2 . and the response is\ngenerated as\n(15)\n\nY = sin(2\u03c0x1 ) sin(2\u03c0x2 ) + \u03b5,\n\nwhere \u03b5 follows a normal distribution with mean 0 and variance 1/4 and the\nnoise is independent between observations. Taking n = 103 samples from\n(15), a regression tree [Breiman et al. (1984)] is fitted to the data, using a\ncross-validated choice of tree size penalty. The fit is constant on rectangular\nregions of the two-dimensional space, as shown in Figure 1. Each of these\nregions corresponds to a node in the tree. The fit is rather poor, however,\nand the structure of the problem is not well captured. Random Forests is\nfitted with the default parameters proposed in Breiman (2001). It improves\nin terms of predictive accuracy on trees, yet the contour plot appears very\nnoisy since the trees are grown until almost pure (keeping only 10 observations in each node) and the variability of the Random Forests approach\nmanifests itself here in a high spatial variability of the fitted function. NH\nis fitted with the default parameters used throughout (1000 random nodes\ngenerated picked from a Random Forest fit, two-factor interactions and minimal node size of 10). It gives a comparably clean contour plot, as seen in the\nrightmost panel of Figure 1 and forms a compromise between trees and Random Forests. In contrast to trees, the fitted function is not constant across\nrectangular-shaped subspaces since each observation can fall into more than\none node.\n4.2. Example II: Importance sampling in climate modeling. The climateprediction.net project [Allen (1999)] is, broadly speaking, concerned with\nuncertainty analysis of climate models, using a distributed computing environment. A climate model contains typically several parameters whose\nprecise values are only known up to a certain precision. The project analyzes the behavior of a coarse resolution variant of the HadCM3 climate\nmodel [Johns et al. (2003)] under thousands of small perturbations of the\ndefault parameters. Once a certain number of models has been sampled,\nthe behavior of the underlying climate model can be better understood and\nimportance sampling can be used to sample only in relevant sections of the\nparameter space. While Gaussian process emulation is widely used in this\ncontext [Oakley and O'Hagan (2004)], we note that the data here are not\n\n\f15\n\nNODE HARVEST\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFig. 1. (a) Contour plot of E(Y ) under model (15) in the two-dimensional predictor\nspace, with contour lines at values \u22121 to 1 with step sizes of 0.2. The contour plot for\nthe fit of a regression tree (b), a Random Forest fit (c) and node harvest (d). The three\nmethods are fitted using the same 103 observations from (15).\n\nnoise free since the outcome depends on the random initial conditions and\na standard regression analysis of the model is hence useful. Without giving\na full explanation, we show an example of a data set containing 250 models, each run with a different combination of 29 parameters. The response\nvariable is mean temperature change over a 50 year period under a given\nemissions scenario.\nFollowing the approach laid out above, 1000 nodes are generated with a\nRandom Forest type approach. All of these nodes are constrained to contain\nat least 10 observations and have at most two-factor interactions. Then the\nquadratic program (7) is applied. Only 14 of the originally 1000 nodes receive\na nonzero weight and these nodes are shown in Figure 2.\nThe plot is very interpretable: the position of each node on the x-axis\ncorresponds to the mean of all training observations in this node. And pre-\n\n\f16\n\nN. MEINSHAUSEN\n\nFig. 2. The 14 nodes selected by node harvest for the climateprediction.net data. The\narea of each node g is proportional to the weight \u0175g it received in (7). The 4986 nodes that\nreceived a zero weight are not shown. The position on the x-axis shows for each node g the\nmean \u03bcg of all training observations that fall into it, while the position on the y-axis shows\nhow many observations it contains. If observations of a node are a subset of observations\nof another node, a line between the two nodes is drawn. The node \"entcoef \u2265 2\" contains a\nsubset of the observations of the node \"entcoef \u2265 0.8.\" A single new observation was chosen\nat random and the 5 nodes that the new observation falls into are lighter and annotated.\nThe prediction for the new observation is then simply the weighted mean across the x-axis\npositions of the annotated nodes.\n\ndictions for new data are simply the weighted mean across all nodes the new\nobservation falls into. The weight of each node is proportional to the area\nwith which it is plotted.\nTo give an example of a prediction, a new observation is sampled at\nrandom. It happens to fall into five nodes, whose respective weights and\nnode means are as follows:\nNode g\nMean \u03bcg\nWeight \u0175g\n\nentcoef \u2265 2\nentcoef \u2265 2\nnum star \u2264 5.5 * 105\n\u22125\nct \u2265 7.5 * 10\nrhcrit \u2265 1.5 entcoef \u2265 2\nrhcrit \u2265 2.5\nentcoef \u2265 .8\n1.98\n1.97\n1.94\n2.30\n2.14\n0.37\n0.24\n0.21\n0.11\n0.06\n\n\fNODE HARVEST\n\n17\n\nFour of these nodes contain the entrainment coefficient (entcoef ) as a split\nvariable, which is maybe unsurprising since the entrainment coefficient is\nknown to be the parameter to which the model is most sensitive.\nThe new observation belongs also to the root node (as do all observations),\nwith the minimal imposed weight 0.001 for this node, but this influence is\nnegligible and ignored here. The predicted response for this new observation\nis then the weighted mean across these nodes, which is 2.014. A graphical\nvisualization of this weighted averaging is immediate from Figure 2. The\nprediction for this new observation (or rather model) is simply the weighted\nhorizontal position of the 5 selected and annotated nodes, with weights proportional to node size. As will be seen further below, the predictive accuracy\nof NH is for this data set better than cross-validated trees, even though no\ntuning was used in the NH approach and the result is at least as interpretable\nand simple as a tree. To get optimal predictive performance, a tree needs\nto employ interactions up to fourth order while NH gets a better accuracy\nwith only two-factor interactions.\n4.3. Example III: Wisconsin breast cancer data. As an example of binary\nclassification, take the Wisconsin breast cancer data [Mangasarian, Street\nand Wolberg (1995)]. There are 10 clinical variables to predict whether a\ntumor is benign or malignant. Applying NH again with 1000 RF-generated\nnodes, with at most two-factor interactions and a minimal node size of 10,\nthe results in Figure 3 are obtained. The root node is again not shown,\ndespite its small enforced positive weight of 0.001. The position on the xaxis gives for each node the percentage of people within this group that\nhad a malignant tumor (Y = 1). The y-axis position is proportional to the\nnumber of people within this node in the training sample. A new patient\nfalls into one or several of these nodes and the predicted probability of class\nY = 1 for this patient is simply the weighted average over the means \u03bcg of\nall nodes g the patient is part of, as shown for a randomly chosen example\npatient in Figure 3. A prediction (or risk assessment in the example) is thus\neasy to communicate and can be related to the empirical outcome in relevant\ngroups of patients with similar characteristics.\nIf splitting the data into two equally large parts and taking one part\nas training and the other part as test data, and averaging over 20 splits,\nthe misclassification test error with NH is 3.6%, compared with 3.3% for\nRandom Forests and 5.5% for cross-validated classification trees. NH seems\nto perform better in a low signal-to-noise ratio setting. If changing 20% of\nall labels in the training set, the performance of Random Forests drops to\n6.0% while NH maintains an accuracy of 4.4%. This behavior is completely\nanalogous to regression, as shown below.\n\n\f18\n\nN. MEINSHAUSEN\n\nFig. 3. Node harvest (NH) estimator for the Wisconsin Breast Cancer study. 22 nodes\nare selected, where the number of patients within each node is shown on the vertical scale.\nThe percentage of patients with a malignant tumor (Y = 1) is shown for each node on the\nhorizontal scale. The number of patients within each node is shown on the vertical scale.\nThe size of nodes is again plotted proportional to the weights chosen by NH. A new patient\nwas randomly selected and belongs to the 6 lighter and annotated nodes. Among these, there\nare 5 'main effect' nodes, with the addition of one 'two-factor interaction' node. All of the\n6 selected groups of patients contain a large fraction of people with a malignant tumor, with\nactual proportions varying between 83% for node \"Bare.nuclei \u2265 3.5; Marg.adhesion \u2264 3.5\"\nto above 97% for node \"bare.nuclei \u2265 5.5.\" The estimated probability for having a malignant tumor for this new patient is the weighted mean across the percentages of people with\na malignant tumor in these 6 groups of patients.\n\n4.4. Further data sets. Besides these examples, the method is applied to\nmotif regression [Conlon et al. (2003)], where the task is to identify transcription factor binding sites from gene expression measurements. The data\nset consist of n = 2588 samples and p = 660 genes and the response variable\nis the concentration of the transcription factor. In addition, the well-known\nabalone data [Nash et al. (1994)], with p = 8, are considered, as are the diabetes data from Efron et al. (2004) ('diabetes,' p = 10, n = 442) and the LA\nOzone data ('ozone,' p = 9, n = 203), bone mineral density data ('bones,'\np = 4, n = 485), fuel efficiency data ('mpg,' p = 7, n = 392), median house\nprices in the Boston area ('housing,' p = 13, n = 506), CPU performance data\n('machine,' p = 7, n = 209), crime rate data from the US census ('crime,'\n\n\fNODE HARVEST\n\n19\n\np = 101, n = 1993), and a data set about prediction of Parkinson's symptoms from voice measurements ('parkinson,' p = 19, n = 5875). The latter\ndata sets are all available at the UCI machine learning repository [Asuncion\nand Newman (2007)]. We also consider a data set about radial velocity of\ngalaxies ('galaxy,' p = 4, n = 323) and prostate cancer analysis ('prostate,'\np = 8, n = 97), the latter all from Hastie, Friedman and Tibshirani (2001),\nwhich contains more details, and, finally, a gene expression data set, kindly\nprovided by DSM nutritional products (Switzerland). For n = 115 samples,\nthere is a continuous response variable measuring the logarithm of riboavin\n(vitamin B2) production rate of Bacillus Subtilis, and there are p = 4088\ncontinuous covariates measuring the logarithm of gene expressions from essentially the whole genome of Bacillus Subtilis. Certain mutations of genes\nare thought to lead to higher vitamin concentrations and the challenge is\nto identify those relevant genes via regression, possibly using also interaction between genes. Observations with missing values are removed from the\ndata sets. Even though NH could deal with these, as alluded to above, it\nfacilitates comparison with other techniques.\nEach data set is split 10 times into two equally large parts. On the half\nused as a training set, NH is employed as well as Random Forests (RF),\na CART regression tree (TREE), Rule Ensembles (RE) and L2 -boosted\nregression trees (L2 B). For NH, we select 1000 nodes from the Random\nForest ensemble as described above, keeping only main-effect and two-factor\ninteraction nodes and a minimal node size of 5. Then (7) is applied to this\nensemble and exactly the same procedure is followed for all data sets without\nany tuning of these parameters. The same initial set of nodes is used for Rule\nEnsembles with a 5-fold CV-choice of the tuning parameter. We remark that\nboth NH and RE could perform better for some data sets if higher order\ninteractions were allowed in the nodes. For Random Forests, one could fine\ntune the minimal node size or the value of mtry, which is the size of the\nrandom number of variables used to find the optimal split point at each\nnode. However, they are kept at the default values (which are known to give\nnearly optimal results), as proposed in Breiman (2001) and Liaw and Wiener\n(2002), to give an equal comparison between the two essentially 'tuning'free algorithms NH and RF. The size of the regression trees [Breiman et al.\n(1984)] is chosen by 10-fold CV on the training data. Boosting is using\nregression trees of depth two as weak learners and a CV-optimized stopping\ntime. The predictions on the test data (the second part of the data) are\nthen recorded for all three methods and the fraction of the variance that is\nunexplained is averaged across all 10 sample splits. The number of training\nobservations available for each data set is shown in Table 1, together with\nthe average unexplained fraction of the variance.\nOn most data sets, Random Forests has the highest predictive accuracy\nwith the exception of 'servo' and 'bones,' where NH is coming on top. A single tree is, maybe unsurprisingly, consistently the worst performing method.\n\n\f20\n\nN. MEINSHAUSEN\n\nTable 1\nAverage proportion of unexplained variance on test data, rounded to two significant\nfigures for Random Forests (RF), CART trees (TREE), Node Harvest without\nregularization, \u03bb = \u221e, (NH\u221e ), Rule Ensembles (RE) and L2 -boosted regression trees\n(L2 B)\nWith additional\nobservational noise\nData set\nOzone\nMpg\nServo\nProstate\nHousing\nDiabetes\nMachine\nGalaxy\nAbalone\nBones\nCpdn\nMotifs\nVitamin\nCrime\nParkinson\n\nn\n\np\n\n203 12\n392\n7\n166\n4\n97\n8\n506 13\n442 10\n209\n7\n323\n4\n4177\n8\n485\n3\n493 29\n2587 666\n115 4088\n1993 101\n5875 19\n\nRF\n\nTREE\n\nRE\n\nL2 B\n\nNH\u221e\n\nRF TREE RE L2 B NH\u221e\n\n0.27\n0.15\n0.32\n0.53\n0.14\n0.55\n0.16\n0.036\n0.45\n0.71\n0.52\n0.67\n0.35\n0.34\n0.20\n\n0.41\n0.24\n0.38\n0.68\n0.30\n0.71\n0.58\n0.094\n0.56\n0.79\n0.66\n0.87\n0.55\n0.47\n0.33\n\n0.31\n0.16\n0.20\n0.61\n0.18\n0.58\n0.86\n0.045\n0.52\n0.73\n0.55\n0.72\n0.40\n0.38\n0.53\n\n0.33\n0.16\n0.37\n0.63\n0.19\n0.57\n0.43\n0.049\n0.48\n0.73\n0.68\n0.71\n0.38\n0.36\n0.63\n\n0.34\n0.20\n0.26\n0.58\n0.25\n0.59\n0.27\n0.065\n0.60\n0.70\n0.66\n0.78\n0.37\n0.42\n0.69\n\n0.55\n0.54\n0.61\n>1\n0.46\n0.74\n0.84\n0.53\n0.64\n0.83\n0.98\n0.83\n0.78\n0.46\n0.43\n\n>1\n>1\n0.94\n>1\n>1\n>1\n>1\n0.81\n0.65\n>1\n>1\n>1\n>1\n0.70\n0.68\n\n>1\n0.47\n0.73\n>1\n0.66\n>1\n>1\n0.35\n0.59\n0.98\n0.98\n>1\n>1\n0.49\n0.60\n\n0.67\n0.39\n0.88\n>1\n0.48\n0.74\n0.56\n0.33\n0.56\n0.88\n0.98\n0.84\n0.99\n0.46\n0.76\n\n0.47\n0.36\n0.57\n>1\n0.39\n0.65\n0.54\n0.26\n0.61\n0.85\n0.77\n0.80\n0.98\n0.45\n0.69\n\nNotes: The best performing method is shown in bold, while the worst performing method\nis shown in italics. A result '>1' indicates that the prediction is worse on test data than\nthe best constant prediction.\n\nThe picture changes if additional noise is added to the training observations.\nTo this end, the response vector Y is replaced on the training observations\nwith the response Y + \u03b5, where \u03b5 = (\u03b51 , . . . , \u03b5n ) contains i.i.d. standard normal noise with variance three times the variance of Y, cutting the correlation\nbetween the true unknown signal and the response exactly in half. As can be\nseen in the right part of the table, NH is now the best performing method\non the clear majority of these low signal-to-noise ratio data, sometimes outperforming all other approaches by a substantial margin.\nFigure 4 shows the impact that the number of nodes in the initial set Q\nhas on predictive accuracy: the more nodes in Q, the better the predictive\naccuracy on test data. Even though Figure 4 shows this phenomenon only up\nto a few thousands of nodes, it holds well beyond this point. In other words,\nNH does not seem to overfit if more and more nodes are added to the initial\nset of nodes and it is ideal to include as many nodes as computationally\nfeasible in Q, even though a few hundred seem to be sufficient for most data\nsets.\n\n\fNODE HARVEST\n\n21\n\nFig. 4. The unexplained variance on test data as a function of the number q of nodes in\nthe initial set of nodes (x-axis in log-scale). Each line corresponds to one data set. Close\nto optimal performance is reached after a few hundred nodes, with results continuing to\nimprove slightly thereafter.\n\nA crude measure for the complexity of a tree or tree ensembles is the total\nnumber of nodes of the predictor, which is equivalent to the total number\nof leaf nodes for tree ensembles and the total number of nodes with nonzero\nweights or coefficients for NH and RE respectively. Table 2 shows that NH\n(with \u03bb = \u221e) and RE use roughly a similar amount of nodes in the final fit,\ntypically a few dozen, while NH with regularization yields the sparsest results\nin general, with the obvious exception of single trees, as seen in the following\nTable 3. Boosting leads to hundreds and Random Forests to thousands or\neven hundreds of thousands of final leaf nodes. The greater sparsity of NH\nand RE comes at a higher computational price. Starting from the same\nnumber of initial nodes, NE and RE are more computationally intensive to\ncompute than all other methods, with a slight edge for NH, especially for\ndata sets with a larger sample size. While it is faster to fit RF than either\nRE or NH, it should be emphasized that, due to much fewer used nodes, NH\nand RE are clearly very fast for predicting the response of new observations,\nwhich can be of importance in an online prediction setting, where RF can\nbe too slow for some applications.\nLast, the effect of regularization (14) on the sparsity of the solution and\npredictive accuracy is examined. Results are summarized in Table 3, where\nthe unconstrained estimator is compared for all previous data sets with the\nregularized estimator at \u03bb = 3. Unsurprisingly, regularization always im-\n\n\f22\n\nN. MEINSHAUSEN\n\nTable 2\nAverage number of nodes for each tree-based predictor (left half ) and the average\ncomputational time necessary to fit the predictor in seconds (right half ), rounded to two\nsignificant figures\nNumber of leaf nodes\nData set\nOzone\nMpg\nServo\nProstate\nHousing\nDiabetes\nMachine\nGalaxy\nAbalone\nBones\nCpdn\nMotifs\nVitamin\nCrime\nParkinson\n\nn\n\np\n\n203\n392\n166\n97\n506\n442\n209\n323\n4177\n485\n493\n2587\n115\n1993\n5875\n\n12\n7\n4\n8\n13\n10\n7\n4\n8\n3\n29\n666\n4088\n101\n19\n\nRF TREE RE L2 B NH\u221e\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n>104\n\n6.7\n8\n2.9\n3.9\n8.3\n12\n3.6\n5.8\n11\n10\n13\n15\n4.7\n11\n24\n\n32\n38\n21\n20\n60\n36\n62\n49\n74\n27\n44\n68\n43\n59\n97\n\n98\n150\n110\n71\n130\n96\n420\n170\n150\n67\n82\n120\n230\n140\n100\n\n97\n56\n20\n52\n74\n75\n47\n52\n53\n30\n24\n64\n70\n71\n15\n\nComputational time (s)\nRF\n\nTREE RE\n\n0.15 0.016\n0.17 0.015\n0.074 0.0098\n0.22 0.01\n0.36 0.025\n0.26 0.019\n0.24 0.011\n0.19 0.0098\n5.8 0.16\n0.15 0.011\n0.42 0.041\n140\n11\n2.5 0.92\n12\n0.83\n16\n0.53\n\nL2 B NH\u221e\n\n11\n0.57\n28\n0.53\n3.8 0.27\n2.8 0.27\n84\n0.53\n57\n0.5\n9\n0.34\n19\n0.35\n520\n0.76\n20\n0.45\n35\n0.71\n470 100\n4.6 170\n220\n3\n920\n1.4\n\n25\n8.6\n3.2\n8.8\n24\n31\n6.7\n8.6\n38\n4.6\n4.8\n86\n75\n21\n44\n\nproves the sparsity of the solution. The average number of selected nodes\ncan decrease by a potentially substantial amount if applying the additional\nregularization, improving interpretability. Predictive accuracy is typically\nvery similar between the two estimators, with an advantage for the unconstrained estimator for the original data sets. Regularization seems to\nimprove the already very good performance of NH in the low signal-to-noise\nratio setting where additional noise is applied to the training data. Overall,\nthe unconstrained estimator seems a very good default choice. Applying the\nadditional regularization is worthwhile if the results are desired to be very\nsparse or the signal in the data is extremely weak.\n5. Discussion. The aim of node harvest (NH) is to combine positive aspects of trees on the one hand and tree ensembles such as Random Forests\non the other hand.\nNH shares with trees the ease of interpretability and simplicity of results.\nAs with trees, only a few nodes are used. For trees, every observation falls\nexactly into one such node and the predicted response is the corresponding node mean. With NH, nodes can overlap and an observation can be a\nmember of a few nodes. While trees often have to include higher order interactions to achieve their optimal predictive performance, it is often sufficient\nfor NH to include main effects and two-factor interactions. While tree size\n\n\f23\n\nNODE HARVEST\n\nTable 3\nAverage proportion of unexplained variance and average number of selected nodes for the\nunrestricted node harvest estimator (\u03bb = \u221e) and the regularized estimator (\u03bb = 3), where\nthe average fraction of samples in each node has to be larger than \u03bb\u22121 = 1/3. The better\nperforming method is again shown in bold\nWith additional noise\n\nData set\nOzone\nMpg\nServo\nProstate\nHousing\nDiabetes\nMachine\nGalaxy\nAbalone\nBones\nCpdn\nMotifs\nVitamin\nCrime\nParkinson\n\nUnexpl. variance\n\nNo. selected nodes\n\nUnexpl. variance\n\nNo. selected nodes\n\n\u03bb=\u221e\n\n\u03bb=3\n\n\u03bb=\u221e\n\n\u03bb=3\n\n\u03bb=\u221e\n\n\u03bb=3\n\n\u03bb=\u221e\n\n\u03bb=3\n\n0.34\n0.20\n0.26\n0.58\n0.25\n0.59\n0.27\n0.065\n0.60\n0.70\n0.66\n0.78\n0.37\n0.42\n0.69\n\n0.34\n0.24\n0.27\n0.57\n0.28\n0.61\n0.26\n0.097\n0.63\n0.70\n0.68\n0.78\n0.39\n0.44\n0.71\n\n97\n56\n20\n52\n74\n75\n47\n52\n53\n30\n24\n64\n70\n71\n15\n\n73\n37\n11\n47\n40\n55\n35\n32\n38\n22\n18\n46\n60\n44\n12\n\n0.47\n0.36\n0.57\n>1\n0.39\n0.65\n0.54\n0.26\n0.61\n0.85\n0.77\n0.80\n1.00\n0.45\n0.69\n\n0.48\n0.34\n0.49\n0.98\n0.41\n0.66\n0.48\n0.24\n0.63\n0.83\n0.79\n0.80\n0.85\n0.48\n0.73\n\n100\n35\n23\n53\n69\n96\n42\n44\n39\n30\n48\n54\n71\n59\n22\n\n95\n34\n17\n47\n46\n96\n40\n33\n28\n24\n36\n42\n66\n46\n18\n\nis determined by cross-validation, essentially no tuning parameter and no\ncross-validation is necessary for NH.\nThe lack of a very important tuning parameter is thus a common feature of\nboth NH and Random Forests. Predictive accuracy also seems comparable.\nFor high signal-to-noise ratio data, Random Forests seems to have an edge\nwhile NH delivers typically a smaller loss if the signal-to-noise ratio drops to\nlower values. The general advantage of NH over Random Forests is simplicity\nand arguably much better interpretability of results.\nIn common with both trees and tree ensembles, NH can handle mixed\ndata very well and is invariant under monotone transformations of the data.\nNH is, moreover, able to deal with missing values without explicit use of\nimputation or surrogate splits. Both regression and classification are handled\nnaturally and it is conceivable that the method can also be extended to\ncensored data, in particular, survival analysis, in analogy to the extension\nof Random Forests to Random Survival Forests [Ishwaran et al. (2006)].\nMost of the functionality of node harvest is implemented in the package\nnodeHarvest for the R-programming language [R Development Core Team\n(2005)].\n\n\f24\n\nN. MEINSHAUSEN\n\nAcknowledgments. I would like to thank a referee, an Associate Editor\nand the editor Michael Stein for their helpful comments on an earlier version\nof the manuscript.\nREFERENCES\nAllen, M. (1999). Do-it-yourself climate prediction. Nature 401 642\u2013642.\nAmit, Y. and Geman, D. (1997). Shape quantization and recognition with randomized\ntrees. Neural Comput. 9 1545\u20131588.\nAsuncion, A. and Newman, D. (2007). UCI Machine Learning Repository. Univ. California, Irvine, CA.\nBartlett, P., Jordan, M. and McAuliffe, J. (2003). Convexity, classification, and\nrisk bounds. Technical report, Dept. Statistics, UC Berkeley.\nBlanchard, G., Sch\u00e4fer, C., Rozenholc, Y. and M\u00fcller, K. (2007). Optimal dyadic\ndecision trees. Mach. Learn. 66 209\u2013241.\nBreiman, L. (1996a). Bagging predictors. Mach. Learn. 24 123\u2013140.\nBreiman, L. (1996b). Stacked regressions. Mach. Learn. 24 49\u201364.\nBreiman, L. (2001). Random forests. Mach. Learn. 45 5\u201332.\nBreiman, L., Friedman, J., Olshen, R. and Stone, C. (1984). Classification and Regression Trees. Wadsworth, Belmont, CA. MR0726392\nChen, S., Donoho, S. and Saunders, M. (2001). Atomic decomposition by basis pursuit.\nSIAM Rev. 43 129\u2013159. MR1854649\nConlon, E., Liu, X., Lieb, J. and Liu, J. (2003). Integrating regulatory motif discovery\nand genome-wide expression analysis. Proc. Natl. Acad. Sci. 100 3339\u20133344.\nEfron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angle regression. Ann. Statist. 32 407\u2013451. MR2060166\nFreund, Y. and Schapire, R. (1996). Experiments with a new boosting algorithm. In\nMachine Learning: Proceedings of the Thirteenth International Conference 148\u2013156.\nMorgan Kauffman, San Francisko, CA.\nFriedman, J., Hastie, T. and Tibshirani, R. (2000). Additive logistic regression: A\nstatistical view of boosting. Ann. Statist. 28 337\u2013407. MR1790002\nFriedman, J. and Popescu, B. (2008). Predictive learning via rule ensembles. Ann. Appl.\nStatist. 2 916\u2013954. MR2522175\nGoldfarb, D. and Idnani, A. (1983). A numerically stable dual method for solving\nstrictly convex quadratic programs. Math. Program. 27 1\u201333. MR0712108\nHastie, T., Friedman, J. and Tibshirani, R. (2001). The Elements of Statistical Learning. Springer, New York. MR1851606\nHastie, T., Tibshirani, R., Botstein, D. and Brown, P. (2001). Supervised harvesting\nof expression trees. Genome Biol. 2 0003\u20131.\nIshwaran, H., Kogalur, U., Blackstone, E. and Lauer, M. (2006). Random survival\nforests. Ann. Appl. Statist. 2 841\u2013860. MR2516796\nJohns, T., Gregory, J., Ingram, W., Johnson, C., Jones, A., Lowe, J., Mitchell,\nJ., Roberts, D., Sexton, D., Stevenson, D. et al. (2003). Anthropogenic climate\nchange for 1860 to 2100 simulated with the HadCM3 model under updated emissions\nscenarios. Climate Dynamics 20 583\u2013612.\nLiaw, A. and Wiener, M. (2002). Classification and regression by random forest. R News\n2 18\u201322.\nLin, Y. and Jeon, Y. (2006). Random forests and adaptive nearest neighbors. J. Amer.\nStatist. Assoc. 101 578\u2013590. MR2256176\n\n\fNODE HARVEST\n\n25\n\nMangasarian, O., Street, W. and Wolberg, W. (1995). Breast cancer diagnosis and\nprognosis via linear programming. Oper. Res. 43 570\u2013577. MR1356410\nMeinshausen, N. (2009). Forest Garrote. Electron. J. Statist. 3 1288\u20131304. MR2566188\nNash, W., Sellers, T., Talbot, S., Cawthorn, A. and Ford, W. (1994). The population biology of abalone in Tasmania. Technical report, Sea Fisheries Division.\nOakley, J. and O'Hagan, A. (2004). Probabilistic sensitivity analysis of complex models:\nA Bayesian approach. J. Roy. Statist. Soc. Ser. B 66 751\u2013769. MR2088780\nR Development Core Team (2005). R: A Language and Environment for Statistical\nComputing. R Foundation for Statistical Computing, Vienna, Austria.\nStrobl, C., Boulesteix, A., Zeileis, A. and Hothorn, T. (2007). Bias in random\nforest variable importance measures: Illustrations, sources and a solution. BMC Bioinformatics 8 25.\nTibshirani, R. (1996). Regression shrinkage and selection via the Lasso. J. Roy. Statist.\nSoc. Ser. B 58 267\u2013288. MR1379242\nWolpert, D. (1992). Stacked generalization. Neural Networks 5 241\u2013259.\nYu, B. and B\u00fchlmann, P. (2003). Boosting with the L2 loss: Regression and classification.\nJ. Amer. Statist. Assoc. 98 324\u2013339. MR1995709\nDepartment of Statistics\nUniversity of Oxford\n1 South Parks Road\nOxford OX1 3TG\nUK\nE-mail: meinshausen@stats.ox.ac.uk\n\n\f"}