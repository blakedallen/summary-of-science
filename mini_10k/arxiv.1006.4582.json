{"id": "http://arxiv.org/abs/1006.4582v2", "guidislink": true, "updated": "2013-01-28T13:44:39Z", "updated_parsed": [2013, 1, 28, 13, 44, 39, 0, 28, 0], "published": "2010-06-23T16:19:57Z", "published_parsed": [2010, 6, 23, 16, 19, 57, 2, 174, 0], "title": "The Poisson Compound Decision Problem Revisited", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1006.1934%2C1006.3389%2C1006.1667%2C1006.5807%2C1006.3834%2C1006.1143%2C1006.2218%2C1006.1582%2C1006.2290%2C1006.3955%2C1006.0892%2C1006.1559%2C1006.3166%2C1006.3817%2C1006.5084%2C1006.2333%2C1006.3591%2C1006.3847%2C1006.2659%2C1006.0731%2C1006.2976%2C1006.3185%2C1006.1674%2C1006.2105%2C1006.2485%2C1006.4163%2C1006.2547%2C1006.3559%2C1006.1620%2C1006.0632%2C1006.5644%2C1006.0410%2C1006.5685%2C1006.5826%2C1006.2758%2C1006.1409%2C1006.2691%2C1006.2197%2C1006.3606%2C1006.4575%2C1006.5739%2C1006.1612%2C1006.0816%2C1006.4842%2C1006.4583%2C1006.3949%2C1006.5106%2C1006.4340%2C1006.3192%2C1006.3382%2C1006.3901%2C1006.3103%2C1006.2336%2C1006.3439%2C1006.1932%2C1006.2800%2C1006.3175%2C1006.1672%2C1006.2475%2C1006.0480%2C1006.4200%2C1006.3887%2C1006.4227%2C1006.1831%2C1006.1421%2C1006.1369%2C1006.3541%2C1006.1887%2C1006.0137%2C1006.4753%2C1006.0970%2C1006.0668%2C1006.3484%2C1006.4605%2C1006.3562%2C1006.4979%2C1006.3740%2C1006.0334%2C1006.5604%2C1006.0800%2C1006.0751%2C1006.2981%2C1006.4361%2C1006.4582%2C1006.4037%2C1006.3779%2C1006.3250%2C1006.2512%2C1006.5788%2C1006.5380%2C1006.1897%2C1006.4637%2C1006.4666%2C1006.5370%2C1006.3552%2C1006.2788%2C1006.4237%2C1006.4501%2C1006.5080%2C1006.1288%2C1006.1055&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The Poisson Compound Decision Problem Revisited"}, "summary": "The compound decision problem for a vector of independent Poisson random\nvariables with possibly different means has half a century old solution.\nHowever, it appears that the classical solution needs smoothing adjustment even\nwhen there are many observations and relatively small means such that the\nempirical distribution is close to its mean. We discuss three such adjustments.\nWe also present another approach that first transforms the problem into the\nnormal compound decision problem.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1006.1934%2C1006.3389%2C1006.1667%2C1006.5807%2C1006.3834%2C1006.1143%2C1006.2218%2C1006.1582%2C1006.2290%2C1006.3955%2C1006.0892%2C1006.1559%2C1006.3166%2C1006.3817%2C1006.5084%2C1006.2333%2C1006.3591%2C1006.3847%2C1006.2659%2C1006.0731%2C1006.2976%2C1006.3185%2C1006.1674%2C1006.2105%2C1006.2485%2C1006.4163%2C1006.2547%2C1006.3559%2C1006.1620%2C1006.0632%2C1006.5644%2C1006.0410%2C1006.5685%2C1006.5826%2C1006.2758%2C1006.1409%2C1006.2691%2C1006.2197%2C1006.3606%2C1006.4575%2C1006.5739%2C1006.1612%2C1006.0816%2C1006.4842%2C1006.4583%2C1006.3949%2C1006.5106%2C1006.4340%2C1006.3192%2C1006.3382%2C1006.3901%2C1006.3103%2C1006.2336%2C1006.3439%2C1006.1932%2C1006.2800%2C1006.3175%2C1006.1672%2C1006.2475%2C1006.0480%2C1006.4200%2C1006.3887%2C1006.4227%2C1006.1831%2C1006.1421%2C1006.1369%2C1006.3541%2C1006.1887%2C1006.0137%2C1006.4753%2C1006.0970%2C1006.0668%2C1006.3484%2C1006.4605%2C1006.3562%2C1006.4979%2C1006.3740%2C1006.0334%2C1006.5604%2C1006.0800%2C1006.0751%2C1006.2981%2C1006.4361%2C1006.4582%2C1006.4037%2C1006.3779%2C1006.3250%2C1006.2512%2C1006.5788%2C1006.5380%2C1006.1897%2C1006.4637%2C1006.4666%2C1006.5370%2C1006.3552%2C1006.2788%2C1006.4237%2C1006.4501%2C1006.5080%2C1006.1288%2C1006.1055&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The compound decision problem for a vector of independent Poisson random\nvariables with possibly different means has half a century old solution.\nHowever, it appears that the classical solution needs smoothing adjustment even\nwhen there are many observations and relatively small means such that the\nempirical distribution is close to its mean. We discuss three such adjustments.\nWe also present another approach that first transforms the problem into the\nnormal compound decision problem."}, "authors": ["L. Brown", "E. Greenshtein", "Y. Ritov"], "author_detail": {"name": "Y. Ritov"}, "author": "Y. Ritov", "links": [{"href": "http://arxiv.org/abs/1006.4582v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1006.4582v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "math.ST", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.TH", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1006.4582v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1006.4582v2", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "arXiv:1006.4582v2 [math.ST] 28 Jan 2013\n\nThe Poisson Compound Decision\nProblem Revisited\nLawrence D. Brown,\u2217\nUniversity of Pennsylvania; e-mail: lbrown@wharton.upenn.edu\n\nEitan Greenshtein\nIsrael Census Bureau of Statistics; e-mail: eitan.greenshtein@gmail.com\n\nand\nYa'acov Ritov\u2020\nThe Hebrew University of Jerusalem; e-mail: yaacov.ritov@gmail.com\nAbstract: The compound decision problem for a vector of independent\nPoisson random variables with possibly different means has a half-century\nold solution. However, it appears that the classical solution needs smoothing\nadjustment. We discuss three such adjustments. We also present another\napproach that first transforms the problem into the normal compound decision problem. A simulation study shows the effectiveness of the procedures\nin improving the performance over that of the classical procedure. A real\ndata example is also provided. The procedures depend on a smoothness\nparameter, that can be selected using a non-standard cross-validation step\nwhich is of independent interest. Finally, we mention some asymptotic results.\n\n1. Introduction\nIn this paper we consider the problem of estimating a vector \u03bb = (\u03bb1 , . . . , \u03bbn ),\nbased on observations Y1 , . . . , Yn , where Yi \u223c P o(\u03bbi ) are independent. The\nperformance of an estimator \u03bb\u0302 is evaluated based on the risk\nE\u03bb ||\u03bb\u0302 \u2212 \u03bb||2 ,\nwhich corresponds to the loss function\nL2 (\u03bb, \u03bb\u0302) =\n\u2217 Research\n\u2020 Research\n\nX\n\nsupported by an NSF grant.\nsupported by an ISF grant.\n1\n\n(\u03bbi \u2212 \u03bb\u0302i )2 .\n\n(1)\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n2\n\nEmpirical Bayes (EB) is a general approach to handle compound decision\nproblems. It was suggested by Robbins, see (1951, 1955); see Copas (1969)\nand Zhang (2003) for review papers. The improvement that empirical Bayes\nmethods yield over more classical, e.g, mle, methods is especially prominent\nin inference for high dimensional data. Thus the empirical Bayes method has\nbecome especially relevant in recent years; see e.g., the enthusiastic advocation\nfor Empirical Bayes usage and relevance in Efron (2003).\nAssume that \u03bbi , i = 1, . . . , n are realizations of i.i.d. \u039bi , i = 1, . . . , n, where\n\u039bi \u223c G. Then a natural approach is to use the Bayes procedure:\n\u03b4 G = argmin EG (\u03b4(Y ) \u2212 \u039b)2 ,\n\n(2)\n\n\u03b4\n\nand estimate \u03bb by \u03bb\u0302 = (\u03b4 G (Y1 ), . . . , \u03b4 G (Yn )). When G is completely unknown,\nbut it is assumed that \u03bb1 , . . . , \u03bbn are i.i.d., then it may be possible to estimate\n\u03b4 G from the data Y1 , . . . , Yn , and replace it by some \u03b4\u0302 G .\nOptimal frequentist properties of \u03b4 G in the context of the compound decision problem, are described in terms of optimality within the class of simple\nsymmetric decision functions. See the recent paper by Brown and Greenshtein\n(2009) for a review of the topic. The optimality of empirical Bayes decision rules\nwithin the larger class of permutational invariant decision functions is shown in\nGreenshtein and Ritov (2009).\nThe Bayes procedure \u03b4 G has an especially simple form in the Poisson setup.\nIn this case there is also a simple and straightforward estimator \u03b4\u0302 G for \u03b4 G .\nDenote by P the joint distribution of (\u039b, Y ), which is induced by G. The Bayes\nestimator of \u03bbi given an observation Yi = y, is:\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\nR\n\u03bbP (Yi = y|\u039bi = \u03bb)dG(\u03bb)\n\u03b4 (y) \u2261 E(\u039bi |Yi = y) = R\nP (Yi = y|\u039bi = \u03bb)dG(\u03bb)\n(y + 1)PY (y + 1)\n=\n,\nPY (y)\n\n3\n\nG\n\n(3)\n\nwhere PY is the marginal distribution of Y under P . Given Y1 , . . . , Yn , we may\nestimate PY (y) trivially by the empirical distribution: P\u0302Y (y) = #{i|Yi = y}/n.\nWe obtain the following Empirical Bayes procedure\n\u03b4\u0302 G (y) =\n\n(y + 1)P\u0302Y (y + 1)\nP\u0302Y (y)\n\n.\n\n(4)\n\nThis estimator was originally proposed in Robbins (1955). It is still the \"default\"/\"classical\" empirical Bayes estimator in the Poisson situation. Various\ntheoretical results established in the above-mentioned papers and many other\npapers imply that as n \u2192 \u221e, the above procedure will have various optimal\nproperties. This is very plausible, since as n \u2192 \u221e, P\u0302Y \u2192 PY and thus \u03b4\u0302 G \u2192 \u03b4 G .\nHowever, the convergence may be very slow, even in common situations as\ndemonstrated in the following example, and one might want to improve the\nabove \u03b4\u0302 G . This is the main purpose of this work.\nExample 1: Consider the case where n = 500 and \u03bbi = 10, i = 1, . . . , 500.\nThe Bayes risk of \u03b4 G for a distribution/prior G with all its mass concentrated\nat 10 is, of course, 0. The risk of the naive procedure which estimates \u03bbi by Yi ,\nequals the sum of the variances, that is, 10 \u00d7 500 = 5000. In 100 simulations we\nobtained an average loss of 4335 for the procedure (4), which is not a compelling\nimprovement over the naive procedure, and is very far from the Bayes risk.\nWe will improve \u03b4\u0302 G mainly through \"smoothing\". A non-trivial improvement\nis also obtained by imposing monotonicity on the estimated decision function.\nBy imposing monotonicity without any further smoothing step, the average total\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n4\n\nloss in the above example in 100 simulations is reduced to 301. By implementing\nthe procedure of Section 2 with a suitable smoothing parameter (h = 3) and\nimposing monotonicity the average loss is reduced further to 30. Early attempts\nto improve (4) through smoothing, including imposing monotonicity, may be\nfound in Maritz (1969) and references there, see also Park (2011) for further\nreferences and for an interesting application.\nThe rest of the paper is organized as follows. In Section 2 we will suggest\nadjustments and improvements of \u03b4\u0302 G . In Section 3 we describe the alternative approach of transforming the Poisson EB problem to a normal EB problem, using a variance stabilizing transformation. In Section 4 we discuss some\ndecision-theoretic background, and in particular we examine loss functions other\nthan squared-error loss. In Section 5 we discuss the above mentioned two approaches and compare them in a simulation study. Both approaches involve a\nchoice of a \"smoothing-parameter\". For our new approach a choice based on\ncross-validation is suggested in Section 6. In Section 7 we present an analysis of\nreal data describing frequency of car accidents. In Section 8 a further approach\nwhich estimates \u03b4 G using a nonparametric MLE is discussed. Finally, in Section\n9, we study some asymptotic properties of the classical Robbins' estimator.\n\n2. Adjusting the classical Poisson empirical Bayes estimator\nSection 1 describes the Bayes decision function \u03b4 G and its straightforward estimator \u03b4\u0302 G . Surprisingly, it was found empirically (see, Example 1) that even for\nn relatively large, when the empirical distribution is close to its expectation, the\nestimated decision function should be smoothed. We discuss in this section how\nthis estimator can be improved. The improvement involves three steps, which\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n5\n\nfinally define an adjusted Robbins estimator.\n\n2.1. Step 1\nRecall the joint probability space defined on (Y, \u039b). We introduce a r.v. N \u223c\nP o(h), where N is independent of Y and \u039b. Let Z = Y + N . Consider the\nsuboptimal decision function\n\u03b4h,1 (z) \u2261 E(\u039b|Z = z) = E(\u039b + h|Z = z) \u2212 h.\n\n(5)\n\nThe above is the optimal decision rule, when obtaining the corrupted observations Zi = Yi + Ni , i = 1, . . . , n instead of the observations Y1 , . . . , Yn . The\n\"corruption parameter\" h is a selected parameter, referred to as the \"smoothing\nparameter\". In general, we will select a smaller h as n becomes larger. See Section 6 for further discussion on the choice of h. Motivated by (5) and reasoning\nsimilar to (4), we define \u03b4\u0302h,1 as:\n\u03b4\u0302h,1 (z) =\n\n(z + 1)P\u0303Z (z + 1)\n\u2212 h,\nP\u0303Z (z)\n\n(6)\n\nwhen P\u0303Z (z) > 0; \u03b4\u0302h,1 (z) = 0 otherwise.\nHere the distribution P\u0303Z (z) is defined by\nP\u0303Z (z) =\n\nz\nX\ni=0\n\nP\u0302Y (i) \u00d7 exp(\u2212h)\n\nhz\u2212i\n.\n(z \u2212 i)!\n\n(7)\n\nNote that P\u0303Z (z) as defined in (7) involves observation of Y through the quantity\nP\u0302Y (y) that appears inside its definition. It is-in general-a much better estimate of PZ (z) compared to the empirical distribution function #{i : Zi = z}.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n6\n\n2.2. Step 2.\nThere is room for considerable improvement of \u03b4h,1 . Note that \u03b4h,1 is applied\nto the randomized observation Zi . Therefore, the natural next adjustment is\nRao-Blackwellization of the estimator. Define\n\u03b4\u0302h,2 (y) = EN (\u03b4\u0302h,1 (y + N )),\n\n(8)\n\nfor N \u223c P o(h), which is independent of the observations Yi , i = 1, . . . , n. That\nis,\n\u03b4\u0302h,2 (y) = e\u2212h\n\n\u221e\nX\nhj\nj=0\n\nj!\n\n\u03b4\u0302h,1 (y + j).\n\nNote that for a given y, the value of \u03b4\u0302h,2 (y) depends on all of P\u0302Y (0), P\u0302Y (1), . . . ,\nalthough mainly on the values in the nearby neighborhood of y.\n\n2.3. Step 3\nFinally after applying adjustments 1 and 2 we obtain a decision function which\nis not necessarily monotone. However, because of the monotone likelihood ratio\nproperty of the Poisson model, \u03b4 G is monotone. A final adjustment is to impose\nmonotonicity on the decision function \u03b4\u0302h,2 . We do it through applying isotonic\nregression by the pooling adjacent violators, cf. Robertson, Wright, and Dykstra (1988). Note, the monotonicity is imposed on \u03b4\u0302h,2 confined to the domain\nD(Y ) \u2261 {y : Yi = y for some i = 1, . . . , n}. To be more explicit, an estimator\nis isotonic if\nyi , yj \u2208 D(Y ) and yi \u2264 yj \u21d2 \u03b4(yi ) \u2264 \u03b4(yj ),\nand \u03b4h,3 is isotonic and satisfies\nn\nX\ni=1\n\nn\no\nnX\n\u00012\n\u00012\n\u03b4\u0302(yi ) \u2212 \u03b4\u0302h,2 (yi ) : \u03b4 satisfies (9) .\n\u03b4\u0302h,3 (yi ) \u2212 \u03b4\u0302h,2 (yi ) = min\ni=1\n\n(9)\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n7\n\nWe obtain the final decision function \u03b4\u0302h,3 , after this third step.\nIn order to simplify notations we denote: \u2206h \u2261 \u03b4\u0302h,3 . This is our adjusted\nRobbins estimator.\n\n2.4. Discussion\nWe now further discuss the above approach. Step 1 of this approach transforms\nthe original problem of estimating the decision function in the Bayesian problem\nwhere \u039b \u223c G, to an auxiliary problem of estimating the decision function in a\nproblem with \u039b\u2032 \u223c G\u2032 , where G\u2032 (\u03bb + h) = G(\u03bb). The estimation of the decision\nfunction in the auxiliary problem is done through an adaptation of Robbins'\nclassical estimator. Indeed, note that (\u03b4\u0302h,1 + h) is an estimator of the Bayes\nprocedure in the auxiliary problem, using (an adapted) Robbins' method.\nLet B(G) and B(G\u2032 ) be the Bayes risk in the original and in the auxiliary\nproblem. Obviously B(G\u2032 ) \u2265 B(G) since the original experiment dominates the\nauxiliary one. Furthermore, as precisely argued in the final section, in both the\noriginal and the auxiliary problems the difference between the average risk per\ncoordinate of Robbins' procedure and the Bayes risk is of order o(log(n)2 /n).\nHence, the average risk per coordinate, of our final procedure \u2206h , is bounded\nbelow by B(G) and bounded above by B(G\u2032 ) + o(log(n)2 /n). For a fixed h in\n\u2032\n\nnon-trivial situations \u03b4 G \u2212 \u03b4 G does not converge to zero, and thus for large\nenough n our adjusted Robbins' procedure performs worse than the original\nRobbins' procedure. However, our simulations show that the asymptotics might\n'kick-in' only for a very large n, and adjusting Robbins' procedure may be very\nhelpful even for large values of n. Estimating the decision function in the auxiliary problem could be much more efficient, compared to estimating the decision\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n8\n\nfunction in the original problem, even for large n. The above heuristically implies, i) as n grows we should use smaller h ii) for distributions G closer to 0\n(i.e., with smaller values) we might want to apply smaller values of h, since we\nexpect a larger difference between B(G) and the upper bound B(G\u2032 ).\nThe Rao-Blackwellization in Step 2 is especially needed when h is not small.\n\u2032\n\nNote again, that (\u03b4\u0302h,1 + h) is an estimator of the decision function \u03b4 G , which\nmight be very different than \u03b4 G when h is not small. In Step 2 we transform the\noriginal observations Y1 , ..., Yn , to Z1 , ..., Zn which are distributed according to\n\u2032\n\nthe observations in the auxiliary problem to which \u03b4 G corresponds, it is then\naveraged over all possible Zi , i = 1, ..., n, in order to obtain a Rao-Blackwell\nimprovement.\nThe choice of the PAV algorithm for the smoothing Step 3 is heuristically\nnatural and convenient. See, for example, Mammen (1991). But there could\nbe other ways to carry out this step. See Koenker and Mizera (2012) for a\nrecently proposed and interesting approach for monotonization and estimation.\nOur experience is that monotonization is particularly useful when h is small since\nfor larger h the smoothing in the first two steps typically yields an estimator\nthat is already very close to being monotone.\n\nFinally we remark on a curious discontinuity property of \u2206h . The function \u2206h\nis a random function, which depends on the realization y = (y1 , . . . , yn ). In order\nto emphasize it we write here \u2206y,h \u2261 \u2206h . Consider the collection of functions\nparameterized by h, denoted {\u2206y,h (y)}. It is evident from the definition of (6),\nthat \u2206y,h (y) does not (necessarily) converge to \u2206y,0 (y) as h approaches 0, even\nfor y in the range y1 , . . . , yn . This will happen whenever there is a gap in the\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n9\n\nrange of y. Suppose, for simplicity that P\u0302Y (y) = 0, while P\u0302Y (y \u2212 1), P\u0302Y (y + 1) >\n0. Then, limh\u21920 \u03b4\u0302h,1 (y \u2212 1) = 0, and limh\u21920 h\u03b4\u0302h,1 (y) = (y + 1)P\u0302Y (y + 1)/P\u0302Y (y \u2212\n1). Hence\nlim \u03b4\u0302h,2 (y \u2212 1) =\n\nh\u21920\n\n=\n=\n\n\u0011\n\u0010\nlim E \u03b4\u0302h,1 (y \u2212 1 + N ) y1 , . . . , yn\n\nh\u21920\n\n\u0001\nlim (1 \u2212 h)\u03b4\u0302h,1 (y \u2212 1) + h\u03b4\u0302h,1 (y)\n\nh\u21920\n\n(y + 1)P\u0302Y (y + 1)/P\u0302Y (y \u2212 1),\n\nwhich is strictly different from \u03b4\u03020,2 (y) = 0. Suppose, more generally, that P\u0302y (y) >\n0 and P\u0302Y (y +j0 ) > 0 for some j0 > 1, but P\u0302 (y +j) = 0 for j = 1, . . . , j0 \u22121. Then\none can check directly from the definition that limh\u21920 \u03b4\u0302h,2 = (y + j0 )P\u0302Y (y +\nj0 )/P\u0302Y (y). Note that in such a situation \u03b4\u0302 G (y) = 0. Hence \u03b4\u0302h,2 (y) for small to\nmoderate h seems preferable to \u03b4\u0302 G (y) = \u03b4\u03020,2 (y) in such gap situations.\nThis phenomena is reflected in our simulations in Section 5, especially in\nTable 5.\nAnother curious feature of our estimator is when applied on ymax = max{Y1 , ..., Yn }.\nIt may be checked that: \u03b4\u0302h,2 (ymax ) = (ymax + 1)h + O(h2 ). When h is small so\nthat (ymax + 1)h \u226a ymax , this would introduce a significant bias. Hence, choosing very small h, might be problematic, though this bias is partially corrected\nthrough the isotonic regression.\n\n3. Transforming the data to normality.\nThe emprical Bayes approach for the analogous normal problem has also been\nstudied for a long time. See the recent papers of Brown and Greenshtein (2009)\nand of Wenhua and Zhang (2009) and references there. The Poisson problem\nand the derivation of (4) are simpler and were obtained by Robbins at a very\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n10\n\nearly stage, before the problem of density estimation, used in the normal empirical Bayes procedure, was addressed. In what follows we will describe the\nmodification of the normal method to the Poisson problem.\nIn the normal problem we observe Zi \u223c N (Mi , \u03c3 2 ), i = 1, . . . , n where\nM1 , . . . , Mn are i.i.d. random variables sampled from G and the purpose is to estimate \u03bc1 , . . . , \u03bcn the realizations of M1 , . . . , Mn . The application of the normal\nEB procedure to the Poisson problem has a few simple steps. First transform the\n\u221a\nPoisson variables Y1 , . . . , Yn to the variables Zi = 2 \u2217 Yi + q. Various recommenations for q are given in the literature, the simplest and most common one is\nq = 0, but the choice q = 0.25 was recommended by Brown et. al. (2005, 2009).\nThus treat Zi 's as (approximate) normal variables with variance \u03c3 2 = 1 and\n\u221a\nmean 2 \u2217 \u03bbi , and estimate their means by \u03bc\u0302i , by applying normal empirical\nBayes technique; specifically, \u03bc\u0302i = \u03b4N,h (Zi ), as defined in (11) below. Finally\nestimate \u03bbi = EYi , by \u03bb\u0302i = 41 \u03bc\u03022i .\nWe will follow the approach of Brown and Greenshtein (2009). Let\ng(z) =\n\nZ\n\n1 \u0010z \u2212 \u03bc\u0011\ndG(\u03bc).\n\u03c6\n\u03c3\n\u03c3\n\nG\nIt may be shown that the normal Bayes procedure denoted \u03b4N\n, satisfies:\n\nG\n\u03b4N\n(z) = z + \u03c3 2\n\ng \u2032 (z)\n.\ng(z)\n\n(10)\n\nThe procedure studied in Greenshtein and Brown (2009), involves an estimation\nG\nof \u03b4N\n, by replacing g and g \u2032 in (10) by their kernel estimators which are derived\n\nthrough a normal kernel with bandwidth h. Denoting the kernel estimates by\n\u011dh and \u011dh\u2032 we obtain the decision function, (Z1 , . . . , Zn ) \u00d7 z 7\u2192 R:\n\u03b4N,h (z) = z + \u03c3 2\n\n\u011dh\u2032 (z)\n.\n\u011dh (z)\n\n(11)\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n11\n\nOne might expect this approach to work well in setups where \u03bbi are large,\n\u221a\nand hence, the normal approximation to Zi = Yi + q is good. In extensive\nsimulations the above approach was found to also work well for configurations\nwith moderate and small values of \u03bb. In many cases it gave results comparable\nto the adjusted Poisson EB procedure.\nRemark In the paper of Brown and Greenshtein the estimator \u03b4N,h as defined\nin (11) was studied. However, just as in the Poisson case, it is natural to impose\nmonotonicity. In the simulations of this paper we make this adjustment using\nisotonic regression. Again, the monotonicity is imposed on \u03b4N,h confined to the\nrange {y1 , ..., yn }. We denote the adjusted estimator by\n\u2206N,h .\n\n4. The loss functions.\nThe estimator \u03b4N,h (Zi ) = \u03bc\u0302i , may be interpreted as an approximation of the\n\u221a\nnonparametric EB estimator for \u03bci \u2261 2 \u03bbi , based on the (transformed) observations Zi under the loss L(\u03bc, a) = ||\u03bc \u2212 a||2 , for the decision a = (a1 , . . . , an ).\nThus,\n\n1 2\n4 \u03bc\u0302i\n\nmay be interpreted as the approximation of the empirical Bayes\n\nestimator for \u03bbi , under the loss\nLH (\u03bb, a) =\n\nXp\n\u221a\n2\n),\n( \u03bbi \u2212 ai )2 = \u22122 log(1 \u2212 DH\n\nwhere DH is to the Hellinger distance between the distributions\nQ\nP o(ai ).\n\nQ\n\nP o(\u03bbi ) and\n\nSome papers that discuss the problem of estimating a vector of Poisson means\n\nare Clevenson and Zidek (1975), Johnstone (1984), Johnstone and Lalley (1984)\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n12\n\nand Fourdinier and Robert(1995). Those and other works suggest that a particularly natural loss function in addition to LH and L2 , denoted LKL is\nLKL (\u03bb, \u03bb\u0302) =\n\nX (\u03bbi \u2212 \u03bb\u0302i )2\n\u03bbi\n\n.\n\nNote, LKL also corresponds to the local Kulback-Leibler distance between the\ndistributions.\nFrom an empirical Bayes perspective, the optimal decisions that correspond\nto those three loss functions may have more and less similarity, depending on the\nconfiguration. For example, when the prior G is concentrated on a point mass,\nthe Bayes procedures corresponding to those 3 loss functions are obviously the\nsame. Since the LKL loss is of a special importance, we will briefly describe how\nour analysis can be modified to handle it. As in the case of L2 loss, one may\nobtain that the Bayes decision under the LKL loss is given for y \u2265 1 by:\nyPY (y)\n.\nPY (y \u2212 1)\nThe decision for y = 0 denoted \u03bb\u0302(0), is:\n\u03bb\u0302(0) =\n=\n\n(\u03bb \u2212 a)2 \u2212\u03bb\narg min\ne dG(\u03bb)\na\n\u03bb\nR \u2212\u03bb\ne dG(\u03bb)\nR\n.\n\u03bb\u22121 e\u2212\u03bb dG(\u03bb)\nZ\n\nIn particular, \u03bb\u0302(0) = 0 if G gives a positive probability to any neighborhood of\n0.\nThe decision for y \u2265 1 may be estimated as in (4) together with the three\nadjustments suggested in Section 2, along the same lines. However, we still need\nto approximate the Bayes decision \u03bb\u0302(0). Note however, that if G has a point\nmass at 0, however small, the risk will be infinite unless \u03bb\u0302(0) = 0. This is the\nonly safe decision, since we cannot ascertain that there is no mass at 0.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n13\n\nNote, defining Z = Y + N , N \u223c P o(h) under the KL loss as in Step 1 in\nthe squared loss, might introduce instability due to small values of P\u0303Z (z \u2212 1) in\nthe denominator of P\u0303Z (z)/P\u0303Z (z \u2212 1), e.g., for z = min{Z1 , ..., Zn }. One might\nwant to define the \"corrupted\" variable alternatively, as Z \u223c B(Y, p). Then\nZ \u223c P o(p\u03bb), when Y \u223c P o(\u03bb). Our smoothing/corrupting parameter is p. We\nskip the details of the analogs of steps 1-3.\nThroughout the rest of the paper, we consider and evaluate procedures explicitly only under the L2 loss.\n\n5. Simulations\nIn this section we provide some simulation results which approximate the risk\nof various procedures as defined in (1). Specifically for various fixed vectors\nP\nP\n\u03bb = (\u03bb1 , ..., \u03bbn ), we estimate E\u03bb (\u2206h (Yi ) \u2212 \u03bbi )2 and E\u03bb (\u2206N,h (Yi ) \u2212 \u03bbi )2 ,\nfor various values of h. The results are reported in tables bellow, each entry in\n\nthose tables is based on 1000 simulations.\nIt is known that for fixed vector \u03bb = (\u03bb1 , ..., \u03bbn ) a good benchmark and a\nlower bound for the risk of our suggested procedures is nB(\u03bb); here B(\u03bb) is the\nBayes risk for the problem where we observe \u039b \u223c G, where G is the empirical\ndistribution which is defined by \u03bb1 , ..., \u03bbn . See Greenshtein and Ritov (2009) for\na general investigation and discussion of this relation.\nAs already seen in Example 1, adjusting the classical non parametric empirical Bayes estimator can yield a significant improvement in the risk. Significant\nimprovement also occurs in a range of parameter configurations, as exemplified\nby those in the following tables. The normal empirical Bayes method of Section\n3 works nearly as well in many of those configurations, but seems less suited to\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n14\n\ntightly clumped configurations like those in Tables 3 and 4. We were somewhat\nsurprised to find that this normal method does compare reasonably well even\nwhen there are some small values of \u03bb as in Table 2. Simulations for the normal\nmethod were performed with both q = 0 and q = 1/4, as variance stabilizers. In\nevery case the results for q = 1/4 were between 2% and 5% better than those\nfor q = 0. So, we report only on those with q = 1/4.\nWe elaborate on Table 1. The reading of the other tables is similar. In Table\n1 we study risks of our procedure, \u2206h , and of of \u2206N,h for various values of\nh. The risks for this table are computed when \u03bb1 , ..., \u03bb200 are equally spaced\nbetween 5 and 15. In practical settings the smoothing parameter, h, should be\nselected according to cross-validation or other method. In Section 6 we describe\na new cross-validation method that seems to work well in the present context.\nIn Table 1 the risks of \u2206h and \u2206N,h under the perspective best choices of h\nare shown in bold-face. The second row of the table shows the risk of \u03b4\u0302h,2 . This\nprocedure does not involve the isotonic monotonization step. This is included\nfor the purpose of comparison in order to show the beneficial effect of this final\nstep of our procedure.\nNote that \u03b4\u03020,2 is the classic Robbins' procedure. Its risk is much larger than\nis available from \u2206h or \u2206N,h . The risk of \u22060 is is that of the classic procedure\nfollowed by the monotonization step and, as can be seen, this step considerably\nreduces the risk. However, as h increases, the procedure \u03b4\u0302h,2 becomes more\nnearly monotone and as can be seen from the table the monotonization step\nbecomes less important in decreasing the risk.\nFor purposes of comparison we note that the risk of the naive procedure is\n1500 and the risk of the Bayes procedure for the setting of the table is approx-\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n15\n\nTable 1\nDifferent EB procedures for \u03bb1 , . . . , \u03bb200 that are evenly spaced between 5 and 15\n\n\u2206h\n\u03b4\u0302h,2\n\n\u2206N,h\n\nh\nrisk\nrisk\n\n0\n1114\n6714\n\n0.2\n1049\n2656\n\n0.4\n1017\n1623\n\n0.8\n994\n1162\n\n1.8\n965\n994\n\n3\n958\n964\n\nh\nrisk\n\n0.2\n1230\n\n0.3\n1099\n\n0.5\n1013\n\n0.7\n997\n\n0.9\n1046\n\n1.2\n1138\n\nTable 2\nDifferent EB procedures for \u03bb1 , . . . , \u03bb200 that are evenly spaced between 0 and 5\n\n\u2206h\n\u03b4\u0302h,2\n\n\u2206N,h\n\nh\nrisk\nrisk\n\n0\n248\n556\n\n0.5\n229\n305\n\n1\n232\n233\n\n1.8\n242\n243\n\n2.4\n249\n250\n\n3\n258\n259\n\nh\nrisk\n\n0.2\n308\n\n0.3\n267\n\n0.5\n245\n\n0.8\n242\n\n1.0\n254\n\n1.4\n291\n\nimately 880.\nOur simulations were done using R (2008) software; monotonicity is imposed\non all the estimators, as described in Step 3, through the 'isoreg' R-procedure.\nAn observed advantage, of the adjusted Robbins' method over the transformed normal method, is its stability with respect to the chosen smoothing\nparameter h. This appears in Table 1 and is even more apparent in some of the\nsubsequent tables.\nThe model studied in Table 2 is of \u03bbi , i = 1, . . . , 200 evenly spaced between 0\nand 5. Comparing the two halves of the table, one may see how well the normal\nmodification works even for such small value of \u03bbi .\nNext, in Table 3, we study the case where \u03bb1 = * * * = \u03bb200 = 10. Here the\nadvantage of the adjusted Poisson over the modified normal is clear.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n16\n\nTable 3\nDifferent EB procedures for \u03bb1 = * * * = \u03bb200 = 10.\n\n\u2206h\n\u03b4\u0302h,2\n\n\u2206N,h\n\nh\nrisk\nrisk\n\n0\n253\n3904\n\n0.2\n121\n1215\n\n0.4\n90\n570\n\n1\n54\n160\n\n2\n38\n72\n\n3\n28\n47\n\nh\nrisk\n\n0.2\n330\n\n0.3\n197\n\n0.5\n180\n\n0.7\n265\n\n0.9\n442\n\n1.3\n808\n\nTable 4\nDifferent EB procedures for \u03bb1 = * * * = \u03bb200 = 5, while \u03bb201 = * * * = \u03bb220 = 15.\n\n\u2206h\n\u03b4\u0302h,2\n\n\u2206N,h\n\nh\nrisk\nrisk\n\n0\n665\n10382\n\n0.2\n476\n3488\n\n0.4\n471\n1761\n\n1.2\n449\n720\n\n2.0\n462\n623\n\n3\n483\n599\n\nh\nrisk\n\n0.2\n819\n\n0.3\n613\n\n0.5\n550\n\n0.9\n653\n\n1.1\n732\n\n1.4\n823\n\nNext we study the following situation where we have a few large \u03bbi values:\n\u03bb1 = * * * = \u03bb200 = 5, while \u03bb201 = * * * = \u03bb220 = 15. There is still a clear\nadvantage of the adjusted Poisson over the modified normal. See Table 4. It\nseems that in this situation the advantage of the modified Robbins procedure\nover the normal is due to the poor tail approximation of the latter.\nFinally we investigate a configuration with only n = 30 observations spread\nover a larger interval. The \u03bbi are evenly spread between 0 and 20. For this configuration there is a slight advantage of the modified normal procedure. In order to\ndemonstrate the discontinuity of \u2206h mentioned in Remark 1, we approximated\nthe risk of \u2206h for h = 0.01, based on 1000 simulations. The approximated risk\nis 244, compared to 867, for h = 0, this is also the minimal approximated risk\nfrom the values of h that we tried in Table 5.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n17\n\nTable 5\nDifferent EB procedures for \u03bb1 , . . . , \u03bb30 that are evenly spread between 0 and 20.\n\n\u2206h\n\u03b4\u0302h,2\n\n\u2206N,h\n\nh\nrisk\nrisk\n\n0\n867\n3190\n\n0.2\n256\n1452\n\n0.4\n249\n924\n\n1.2\n256\n384\n\n2.0\n262\n320\n\n3\n260\n281\n\nh\nrisk\n\n0.2\n316\n\n0.3\n302\n\n0.5\n280\n\n0.9\n243\n\n1.2\n236\n\n1.4\n239\n\nFinally, the standard error of the estimated risk in the range of smoothing\nparameters h, is about 3 in Experiment 1, about 1 in experiments 2-4, and about\n2.5 in Experiment 5.\n\n6. Choosing the smoothing-parameter by Cross-validation\nIn this section we suggest a non-standard cross validation method, and study\nits performance. This method is explained in the Poisson context, and then\nin the normal context. The same general idea works for other cases where an\nobservation may be factorized, e.g., for infinitely divisible experiments. About\nfactorization of experiments, see Greenshtein (1996) and references there.\nLet p \u2208 (0, 1), p \u2248 1, and let U1 , . . . , Un be independent given Y1 , . . . , Yn ,\nwhere Ui \u223c B(Yi , p), i = 1, . . . , n, are binomial variables. As is well known,\none of the features of the Poisson distribution is that Ui \u223c P o(p\u03bbi ), and Vi \u2261\nYi \u2212 Ui \u223c P o((1 \u2212 p)\u03bbi ), and they are independent given \u03bb1 , . . . , \u03bbn . We will use\nthe main sub-sample U1 , . . . , Un for the construction of the family of estimators\n(parameterized by h), while the auxiliary sub sample V1 , . . . , Vn is used for\nvalidation. The choice p \u2248 1 is in order that the distribution of Ui will be close\nto that of Yi , i = 1, ..., n, thus estimation based on Ui is similar to estimation\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n18\n\nbased on Yi . Let \u03b4\u0302h\u2217 (*), h \u2208 H be a family of estimators, based on U1 , . . . , Un\nsuch that \u03b4\u0302h\u2217 (Ui ) estimates p\u03bbi , i = 1, . . . , n. Consider:\n\u03c1(h; U , V )\nn\n\u00112\n1 X\u0010 \u2217\n\u03b4\u0302h (Ui ) \u2212 p(1 \u2212 p)\u22121 Vi\n=\nn i=1\n=\n\nn\n\u0001\u00112\n\u0001\n1 X\u0010 \u2217\n\u03b4\u0302h (Ui ) \u2212 p\u03bbi \u2212 p(1 \u2212 p)\u22121 Vi \u2212 (1 \u2212 p)\u03bbi\nn i=1\n\n=\n\n\u00012\n1X \u2217\n\u03b4\u0302h (Ui ) \u2212 p\u03bbi + Rn (h) + An ,\nn i=1\n\n(12)\n\nn\n\nwhere An is a random quantity that does not depend on h, and has no importance to the selection of h, while\nn\n\nRn (h) =\n\nX\n\u0001\n\u0001\n2p\n\u03b4\u0302h\u2217 (Ui ) \u2212 p\u03bbi Vi \u2212 (1 \u2212 p)\u03bbi .\n(1 \u2212 p)n i=1\n\n(13)\n\nSince V1 , . . . , Vn are independent and independent of U1 , . . . , Un given \u03bb1 , . . . , \u03bbn :\nE(Rn2 (h)|U , \u03bb) =\n\nn\nX\n\u00012\n4p2\n\u03b4\u0302h\u2217 (Ui ) \u2212 p\u03bbi \u03bbi .\n2\n(1 \u2212 p)n i=1\n\n(14)\n\nWe conclude that if (1 \u2212 p)n/ max{\u03bbi }|H| \u2192 \u221e, then\n\u03c1(h; U , V ) = L(\u03b4\u0302h\u2217 , p\u03bb) + op (1),\n\n(15)\n\nuniformly in h \u2208 H. Recall that the decision function \u03b4\u0302h\u2217 used in the above\nresult, is the non-parametric empirical Bayes procedure based on U1 , . . . , Un\nand \u03b4\u0302h\u2217 (Ui ) is estimating p\u03bbi . If also p \u2192 1, we suggest to use the value h that\nminimizes \u03c1(h; U , V ), to construct a similar estimator based on the original\nsample Y1 , . . . , Yn , estimating \u03bb1 , . . . , \u03bbn .\n\u03c1(h; U , V ), given the sample Y1 , . . . , Yn is a randomized estimator of the loss\nfunction. Once again we suggest in this paper to replace a randomized estimator\n\u0010\n\u0011\nby its expectation given the sample E \u03c1(h; U , V ) Y . This expectation can be\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n19\n\nestimated by a Monte Carlo integration-sampling K i.i.d. samples of U and\nV.\nFor the normal model, Zi \u223c N (\u03bci , 1), i = 1, . . . , n, let \u01ebi \u223c N (0, 1) be\nauxiliary i.i.d. variables, independent of Y1 , . . . , Yn . Define Ui = Yi + \u03b1\u01ebi , Vi =\nYi \u2212 (1/\u03b1)\u01ebi . Then Ui and Vi are independent both with mean \u03bci , and with\nvariances 1 + \u03b12 and 1 + (1/\u03b12 ) correspondingly. Again, U may be used for\nestimation and V for validation, where \u03b1 > 0, \u03b1 \u2192 0.\n6.1. Numerical Study.\nExample 2: Consider the configuration \u03bb1 = * * * = \u03bb200 = 10, simulated in\nTable 3 Section 5. In that table h = 3 is recommended with a noticeable advantage over h \u2264 0.4. We applied the above cross validation procedure with\np = 0.9 on a single realization of Yi , i = 1, . . . , 200. We repeated the crossvalidation process K = 10000 times on this single realization for the values\nh \u2208 {0, 0.5, 1, 1.5, 2, 2.5, 3}. The corresponding numbers \u03c1(h, U, V) (scaled by\n(1 \u2212 p)2 ) were: 165.834, 164.862, 164.736, 164.457, 164.421, 164.286, 164.340.\nNote that, the last numbers represent mainly the variance of our validation\nvariable, but the success of the corresponding estimator is also a factor. The\nnumbers indicate that the choices h = 0, 0.5, 1 are inferior, the formal recommended choice is h = 2.5, the second best is h = 3.\nWe repeated the simulation on another single realization, again K = 10000,\nthis time we took p = 0.85. The corresponding numbers are: 220.562, 217.986,\n217.706, 217.374, 217.209, 217.272, 217.247. Again, the numbers indicate that\nthe choices h = 0, 0.5, 1 are inferior. The formal recommended choice is h = 2,\nthe second best is again h = 3.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n20\n\nFinally, we extended the five experiments, studied in the previous section.\nFor each experiment we repeated 100 times the following simulation. We took\nthe six values of h which are reported in the corresponding table in Section 5,\nand in each of the 100 runs we chose the smoothing parameter among the six\ncandidates through implementing the above cross validation method with K =\n10000 and p = 0.9. Hence different values of h were used for different realizations.\nThe results we obtained for experiments 1-5 are correspondingly: 944, 246, 30,\n453, 258. The simulated risks that correspond to the best individual smoothing\nparameter in each experiment are: 958, 229, 28, 449, 249. The performance of\nthe CV is quite impressive.\nNote that in Experiment 1 the simulated risk of the CV is actually smaller\nthan all the risks that correspond to the individual six smoothing parameters.\nThis improvement could be an artifact of the simulation and not a real one,\nour simulations were too slow to make a confident statement. However, such an\nimprovement could be real since the CV method might choose a different 'more\nsuitable' smoothing parameter depending on the realization.\n\n7. Real Data Example.\nIn the following we study an example based on real data about car accidents with\ninjuries in 109 towns in Israel in July 2008. The 109 towns are those that had at\nleast one accident with injuries in that period of time; in the following we ignore\nthis selection bias. There were 5 Tuesdays, Wednesdays and Thursdays, in that\nmonth. For Town i, let Yi be the total number of accidents with injuries in those\n5 Wednesdays. Similarly, for Town i, let Zi be half of the number of accidents\nwith injuries in the corresponding Tuesdays and Thursdays. We modelled Yi\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n21\n\nTable 6\nEB applied to traffic accident by city\n\n\u2206h\n\nh\nR\u0302\n\n0\n140\n\n0.5\n163\n\n1\n172\n\n1.5\n168\n\n2\n166\n\n3\n159\n\n\u2206N,h\n\nh\nR\u0302\n\n0.2\n262\n\n0.6\n185\n\n1\n174\n\n2\n170\n\n3\n183\n\n4\n202\n\nas independently distributed P o(\u03bbi ). In the following we will report on the\nperformance of our empirical-Bayes estimator for various smoothing parameters\nh. It is evaluated through the predictive squared error\nR\u0302 =\n\nX\n\n(Zi \u2212 \u2206h (Yi ))2 .\n\nThe towns Tel-Aviv and Jerusalem had a heavy impact on the risk and thus we\nexcluded them from the analysis. The remaining data seems to have relatively\nlow values of \u03bbi , a case where the classical Poisson-EB procedure is expected to\nP\nperform well, and indeed it does. The range of Yi is 0-14, while\nYi = 135, and\nP 2\nYi = 805. In this example, the classical Poisson-EB adjusted for monotonicity\n(i.e., h = 0), gave the best result. Applying a smoothing parameter h > 0 is\n\nslightly inferior based on the above empirical risk. Yet, it is re-assuring to see\nhow stable is the performance of \u2206h , as h varies. The empirical loss for the\nnaive procedure estimating \u03bbi by Yi , is 240. The modified normal estimators\nwith q =\n\n1\n4\n\nand various values of h was applied to the data as well. Again\n\na clear advantage of our class of adjusted Poisson procedures over the class\nof modified normal procedures was observed. In particular, the former class is\nmuch more stable with respect to the choice of the smoothing parameter h. The\nresults are summarized in Table 6.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n22\n\n8. The nonparametric MLE\nThe nonparametric maximum-likelihood (NPMLE), as suggested by Kiefer and\nWolfowitz (1956), is an alternative approach for estimating \u03b4 G . It yields, automatically, a monotone and smooth decision function. See Jiang and Zhang\n(2009) for the normal model. To simplify the discussion, we will assume that\n\u03bb1 , . . . , \u03bbn are realizations of i.i.d. random variables sampled from the distribution G. Obtaining a NPMLE \u011c for G, induces the estimator \u03b4 \u011c for \u03b4 G . We will\nrefer to \u03b4 \u011c also as \u03b4KW .\nNote that the NPMLE maximizes with respect to G, the likelihood function:\n\n\u221e\n\nn\n\nX\n1X\nPn (i) log pG (i)\nlog pG (yi ) =\nn i=1\ni=0\n=\n\n\u221e\nX\ni=0\n\n\u0001\nF\u0304n (i \u2212 1) \u2212 F\u0304n (i) log pG (i)\n\n= log pG (0) +\n\n\u221e\nX\n\nF\u0304n (i) log\n\n= log pG (0) +\n\n\u221e\nX\n\nF\u0304n (i) log \u03b4 G (i) + C(y).\n\ni=0\n\npG (i + 1)\npG (i)\n\ni=0\n\nwhere Pn is the empirical process, Pn (i) = Pn ({i}), and F\u0304n (i) =\n\nP\u221e\n\nj=i+1\n\nPn (j)\n\n(F\u0304n (\u22121) = 1). That is, the likelihood function can be written as a direct function\nof the Bayes procedure.\nSuppose G is supported on [a, b]. Extend\n\u03bby+1 e\u2212\u03bb dG(\u03bb)\n\u03b4 (y) = R y \u2212\u03bb\n,\n\u03bb e dG(\u03bb)\nG\n\nR\n\ny \u2208 R+ .\n\nThen, clearly, \u03b4 G (y) \u2208 [a, b]. Moreover, it is monotone non-decreasing with\n\u2032\n\nderivative \u03b4 G (y) = cov(\u03bb, log \u03bb) \u2208 [0, b log b \u2212 a log a] (where the covariance\nis with respect to measure \u03bby e\u2212\u03bb dG(\u03bb) normalized)\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n23\n\nIt is well known that the NPMLE is discrete with point mass g1 , . . . , gk on\n\u03bb1 , . . . , \u03bbk say. It is easy to see that it satisfies\nn\nX\ni=1\n\n\u03bbyj i\n= e\u03bbj\nyi !pG (yi )\n\n, j = 1, . . . , k.\n\nSince the left hand side is a polynomial in \u03bb of degree max yi , and a polynomial\nof degree q in \u03bb can be equal to exp{\u03bb} only q times, we conclude that k < max yi\n(a more careful argument can reduce the bound on the support size). Hence, it\nis feasible to approximate algorithmically the NPMLE. Pursuing the asymptotic\nproperties of the NPMLE estimator is beyond the scope of this paper. We should\nmention that as we argue in Section 9, Robbins' estimator is weak only when G\nis sparse and discrete, exactly where the NPMLE seems to excel.\nKoenker and Mizera (2012) further developed this idea for the normal case.\nThey approximated \u03b4KW directly (i.e., not through approximating \u011c first), utilizing the monotonicity property/constraint of \u03b4KW to define a corresponding\nconvex optimization problem. Then, using interior point methods and available\nsoftwares they derived algorithmically very efficient approximations of \u03b4KW .\nWe are indebted to the AE for the following Table 7 provided to us. In the first\nline of the table, the risk of the approximated \u03b4KW is given for the 5 simulated\nnumerical experiments presented in our simulation section. Those results are\nbased on 1000 simulations for each example. The second line in the table gives\nthe simulated risk of \u2206h for the best value of h among those reported in Tables\n1-5, the third line gives the estimator obtained through cross-validation, as given\nand described in Section 6.\nThe performances of the methods are very similar. An advantage of our suggested procedure is that it is rather elementary and does not require more\nsophisticated optimization methods and software. Also, as described in Section\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n24\n\nTable 7\nComparison with Kiefer and Wolfowitz estimator\n\nKW estimator\nBest-h\nCV selection\n\nExp1\n\nExp2\n\nExp3\n\nExp4\n\nExp5\n\n958\n958\n944\n\n228\n229\n246\n\n39\n28\n30\n\n434\n449\n453\n\n263\n249\n258\n\n4 our method may be modified and specialized to deal with other loss functions.\nIt may also prove to be more adaptable for generalizations involving additional\ncovariates such as were studied in the normal case by Jiang and Zhang (2010),\nCohen, Greenshtein and Ritov (2012), Koenker and Mizera (2012). We hope to\nstudy this issue in the future.\nAn advantage of (the approximation of) \u03b4KW is that it does not involve a\nchoice of a smoothing parameter h, and does not require cross validation.\n\n9. Asymptotics for Robbins' Estimator.\nIn this section we will investigate theoretically the performance of Robbins'\nmethod. It will be shown that in the usual asymptotical EB setup, where we\nobserve i.i.d. \u039b1 , ..., \u039bn , \u039bi \u223c G, and G is non-degenerate, Robbins' procedure\n\u03b4\u0302 G is very efficient. This is because that its risk is within O((log n/ log log n)2 )\nof the risk of the Bayes procedure which is of order O(n). Note however, that\nif G is degenerate the risk of the Bayes procedure is zero, and achieving a\nrisk of order (log n/ log log n)2 rather than a zero risk might not be considered\na \"success\", in particular the ratio of the risks in that case is infinity. More\ngenerally when the sequence \u03bb1 , ..., \u03bbn of the realized \u039bi i = 1, ..., n is very\n\"sparse\", in the sense that only a very small fraction of it does not equal to\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n25\n\n\u03bb0 , then Robbins' procedure, whose risk will be shown to be larger than the\nBayes risk by \u03ba(log n/ log log n)2 for appropriate \u03ba > 0, might not be considered\nefficient. Note, we use the term sparse for \u03bb0 which does not equal necessarily\nzero; in fact, the case \u03bb0 = 0 is excluded from the following theorem and from\nthe discussion, to avoid technical difficulties.\nIn order to formally study asymptotics for such sparse setups we will consider\na triangular array where at stage k, G = Gk . Typically we consider Gk \u2192 G0\nweakly, where G0 may be degenerate at \u03bb0 , where the support of Gk is bounded\nuniformly in k = 1, 2, ...\nFor simplicity we assume further that the sample size M is a Poisson random\nvariable with mean \u03bd = \u03bd k . Asymptotic results will hold as \u03bd k \u2192 \u221e. This\nassumption simplifies considerably the proof, and has little significance for the\ninterpretation of the result. Let N\u03bd (y) = #{i : 1 \u2264 i \u2264 M, Yi = y}, y =\n0, 1, . . . . Note that they are independent under the Poisson sample size, N\u03bd (y) \u223c\nP o(\u03bdP (y)), where P (*) denotes the marginal probabilities of Y . A proof for a\nfixed sample size would involve the binomial distribution B(P (y), n) for Nn (y)\n\u0001\nand conditional on Nn (y), Nn (y + 1) \u223c B n \u2212 Nn (y), P (y + 1)/(1 \u2212 P (y) , but\notherwise would be very similar, though more cumbersome. Let\nk\n\n\u03b4 G (y) = (y + 1)\nk\n\n\u03b4\u0302 G (y) = (y + 1)\n\nP (y + 1)\n,\nP (y)\nNk\u03bd (y + 1)\n,\nNk\u03bd (y)\n\ny = 0, 1, . . .\ny = 0, 1, . . .\n\nbe the Bayes procedure and its Robbins' approximation.\nIn the sequel we will occasionally drop the superscript k for simplicity.\nLet r(G, \u03b4) be the total Bayes risk of the estimator \u03b4 when \u03bb1 , . . . , \u03bbM are\n(given M ) simple random sample from G.\nOur main result in this section is the following theorem.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n26\n\n\u0001\n\u0001\nTheorem 1. Suppose that lim inf Gk (\u03bb1 , \u221e) > 0 and lim inf Gk [0, \u03bb2 ) = 1\nk\n\nk\n\nfor some 0 < \u03bb1 < \u03bb2 < \u221e . Then (r(Gk , \u03b4\u0302 G ) \u2212 r(Gk , \u03b4 G ))(log log \u03bd/ log \u03bd)2\nis bounded from above and away from 0.\n\nProof. The risk of Robbins' procedure \u03b4\u0302 G is given by\nr(G, \u03b4\u0302 G ) = E\n\n\u221e\nX\n\nN\u03bd (y)E\n\n=E\n\n\u221e\nX\n\n\u221e\n\u0010\n\u0011\nX\n\u00012\nN\u03bd (y) \u03b4\u0302 G (y) \u2212 \u03b4 G (y) + E\nN\u03bd (y) var \u039b|Y = y\n\ny=0\n\n=E\n\ny=0\n\u221e \u0010\nX\ny=0\n\n\u0010\n\n\u0011\n\u00012\n\u03b4\u0302 G (y) \u2212 \u039b |Y = y\n\ny=0\n\n2\n2 N\u03bd (y\n\nN\u03bd (y + 1)P (y + 1)\n+ 1)\n\u2212 2(y + 1)2\nN\u03bd (y)\nP (y)\n\u0011\n2\n+ N\u03bd (y)\u03b4 G (y) 1(N\u03bd (y) > 0) + r(G, \u03b4 G )\n(y + 1)\n\n= r(G, \u03b4 G ) + E\n\n\u221e\nX\n\n(y + 1)2\n\ny=0\n\nN2\u03bd (y + 1)\n2\n1(N\u03bd (y) > 0) \u2212 \u03bdE\u03b4 G (Y ).\nN\u03bd (y)\n\nIn the above we used the facts that N\u03bd (y + 1) and N\u03bd (y) are independent,\nand that if X \u223c P o(\u03b8) then EX 2 = \u03b8 + \u03b82 .\nIn order to evaluate R(G, \u03b4\u0302 G ) \u2212 R(G, \u03b4 G ) we need the following.\n\u221e\n\nE\u03b8\n\nX \u03b8i\n1(X > 0)\n= e\u2212\u03b8\n,\nX\ni!i\ni=1\n\nhence\n\u221e\n\nE\n\nX \u03b8i\n1(X > 0)\n= e\u2212\u03b8\nX\ni!i\ni=1\n= ce\u2212\u03b8\n\n\u221e\nX\ni=1\n\n\u03b8i\n(i + 1)!\n\n= ce\u2212\u03b8 \u03b8\u22121 (e\u03b8 \u2212 1 \u2212 \u03b8),\nwhere c \u2208 (1, 2).\n\n(16)\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n27\n\nAlso,\n\u221e\n\nE\n\nX \u03b8i\n1\n1(X > 0) 1\n\u2212 = e\u2212\u03b8\n\u2212\nX\n\u03b8\ni!i\n\u03b8\ni=1\n= e\u2212\u03b8\n\n\u221e\nX\n\n\u221e\n\nX\n\u03b8i\n\u03b8i\n1\n\u2212 + e\u2212\u03b8\n(i + 1)! \u03b8\n(i + 1)!i\ni=1\n\ni=1\n\n\u221e\nX\n\n\u221e\nX\n\u03b8i\n\u03b8i\n1\n\u2212 + 3e\u2212\u03b8\n(i + 1)! \u03b8\n(i + 2)!\ni=1\ni=1\n\u0010\n\u0010\n\u0011\n3\n1 \u0011\n1\n1\n= e\u2212\u03b8 e\u03b8 \u2212 1 \u2212 \u03b8 \u2212 + 2 e\u2212\u03b8 e\u03b8 \u2212 1 \u2212 \u03b8 \u2212 \u03b82\n\u03b8\n\u03b8 \u03b8\n2\n1 + \u03b8 \u2212\u03b8\n1 2\u0011\n3 \u2212\u03b8 \u0010 \u03b8\n=\u2212\ne \u22121\u2212\u03b8\u2212 \u03b8 .\ne + 2e\n\u03b8\n\u03b8\n2\n\n\u2264 e\u2212\u03b8\n\n(17)\n\nNow\nr(G, \u03b4\u0302 G ) = r(G, \u03b4 G ) +\n\n\u221e\nX\n\n(y + 1)2 E\n\ny=0\n\n+\n\n\u221e\nX\n\n(y + 1)2 E\n\ny=0\n\n\u03bdP (y + 1)\n1(N\u03bd (y) > 0)\nN\u03bd (y)\n\n\u0010 \u03bd 2 P 2 (y + 1)\nN\u03bd (y)\n\n= r(G, \u03b4 G ) + I + II,\n\n1(N\u03bd (y) > 0) \u2212 \u03bd\n\nP 2 (y + 1) \u0011\nP (y)\n\nsay.\n\nIn the following c1 , . . . , c5 \u2208 (a, b) are some constants for some universal constants 0 < a < b < \u221e. Now,\nI = c1\n\n\u221e\nX\n\n(y + 1)2\n\ny=0\n\n\u0001\nP (y + 1)\n1 \u2212 e\u2212\u03bdP (y) (1 + \u03bdP (y))\nP (y)\nk\n\nIf Gk has a compact support, then \u03b4 G (y) is increasing and bounded by \u03bbU \u2261\n\u03bbkU < \u03bb2 , the upper support of Gk . Using this observation and (16), we obtain\nfor \u03bd large enough\nI = c1 \u03bbU\n\n\u221e\nX\n\n\u0001\n(y + 1) 1 \u2212 e\u2212\u03bdP (y) (1 + \u03bdP (y))\n\ny=0\n\n= c1 \u03bbU\n\nX\n\n\u0001\n(y + 1) 1 \u2212 e\u2212\u03bdP (y) (1 + \u03bdP (y))\n\n\u03bdP (y)>1/2\n\n+ c1 \u03bbU\n\nX\n\n\u0001\n(y + 1) 1 \u2212 e\u2212\u03bdP (y) (1 + \u03bdP (y))\n\n\u03bdP (y)\u22641/2\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n28\n\nNote that for \u03b8 > 0, 1 \u2212 (1 + \u03b8)e\u2212\u03b8 is monotone increasing from 0 to 1:\nX\nX\n\u00012\n(y + 1) \u03bdP (y)\n(y + 1) + c3\nI = c2 \u03bbU\n\u03bdP (y)\u22641/2\n\n\u03bdP (y)>1/2\n\n2\n\n= c2 \u03bbU max{(y + 1) : P (y) > 1/2\u03bd} + c3 \u03bbU\n\nX\n\n\u00012\n(y + 1) \u03bdP (y)\n\n\u03bdP (y)\u22641/2\n\nNow, for z > 2\u03bbU ,\n\nP\n\ny\u2265z\n\ny k P (y) \u2264 2P (z), k = 0, 1, and e\u2212\u03bb \u03bby /y! = \u01eb implies\n\nthat y log | log \u01eb|/| log \u01eb| \u2192 1 as \u01eb \u0581 0 and y \u2192 \u221e for any \u03bb1 \u2264 \u03bb \u2264 \u03bb2 . Hence\nX\n\u00012\n(y + 1)P (y)\nI = c2 \u03bbU log \u03bd/ log log \u03bd + c3 \u03bd\n\u03bdP (y)\u22641/2\n\n\u00012\n\n= c4 \u03bbU log \u03bd/ log log \u03bd .\nBounding II is similar, noting that there is \u03b3 > 0 such that the RHS of (17)\nis negative for \u03b8 < \u03b3:\nII \u2264\n\n\u221e\nX\n\n(y + 1)2 \u03bd 2 P 2 (y + 1)E\n\ny\u22120\n\n\u22643\n\nX\n\n(y + 1)2\n\n\u03bdP (y)>\u03b3\n\n\u2264 c5\n\nX\n\n\u0011\n1 2 2\nP 2 (y + 1) \u0010\n\u2212\u03bdP (y)\n1\n\u2212\ne\n(1\n+\n\u03bdP\n(y)\n+\n\u03bd\nP\n(y))\nP 2 (y)\n2\n\n(y + 1)2\n\n\u03bdP (y)>\u03b3\n\n\u0010 1(N (y) > 0)\n1 \u0011\n\u03bd\n\u2212\nN\u03bd (y)\n\u03bdP (y)\n\nP 2 (y + 1)\nP 2 (y)\n\n\u2264 c6 max{y : \u03bdP (y) > \u03b3}\n= c6 log \u03bd/ log log \u03bd.\n\nRemarks:\n1. The asymptotics in the above theorem implies that in a non-sparse situation, asymptotically there is a room for only a negligible improvement\non Robbins' classical estimator. However, in light of Example 1 and our\nsimulations, the asymptotic presented in this section may be somewhat\nmisleading. This is since the above asymptotics often seems to 'kick-in'\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n29\n\nonly for very large n and are thus irrelevant for moderately large values\nof n, that appear in practice.\n2. Nevertheless, our asymptotics suggests that there are limitations and possible room for improvement of Robbins' classical procedure in a triangular array setup of sparse problems in which the risk may be of order\nO((log(n)/ log log(n))2 )), for arbitrarily large n.\nAcknowledgement. We are grateful to the reviewers for their comments\nand suggestions and in particular for Table 7 that was provided to us.\n\nReferences\nBrown, L. D., Gans, N., Mandelbaum, A., Sakov, A., Shen, H., Zeltyn, S.,\nand Zhao, L. H. (2005). Statistical analysis of a telephone call center: a\nqueing science perspective. Jour. Amer. Stat. Asoc. 100 36-50.\nBrown, L.D., Cai, T., Zhang, R., Zhao, L., Zhou, H. (2010). The root-unroot\nalgorithm for density estimation as implemented via wavelet block thresholding. Probability and Related Fields, 146, 401-433.\nBrown, L.D. and Greenshtein, E. (2009). Non parametric empirical Bayes\nand compound decision approaches to estimation of high dimensional vector of normal means. Ann. Stat. 37, No 4, 1685-1704.\nClevenson, L. and Zidek, J.V. (1975). Simultaneous estimation of the mean\nof independent Poisson laws. Jour. Amer. Stat. Asoc. 70 698-705.\nCohen, N., Greenshtein E., and Ritov, Y. (2012). Empirical Bayes in the\npresence of explanatory variables. To appear in Statistica Sinica.\nCopas, J.B. (1969). Compound decisions and empirical Bayes (with discus-\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n30\n\nsion). JRSSB 31 397-425.\nEfron, B. (2003). Robbin, empirical Bayes, and microarrays (invited paper).\nAnn.Stat. 31, 364-378.\nFourdrinier, D. and Robert, C. P. (1995). Intrinsic losses for empirical Bayes\nestimation: A note on normal and Poisson cases. Stat. and Prob. Letters\n23, 35-44.\nGreenshtein, E. (1996). Comparison of sequential experiments. Ann. Stat.\n24, No 1, 436-448.\nGreenshtein, E. and Ritov, Y. (2008). Asymptotic efficiency of simple decisions for the compound decision problem. The 3'rd Lehmann Symposium.\nIMS Lecture Notes Monograph Series, J.Rojo, editor. 266-275.\nHengartner, N. W. (1997). Adaptive Demixing in Poisson Mixture Models.\nAnn. of Statist. 25, 917\u2013928.\nJiang, W. and Zhang, C.-H. (2009). General maximum likelihood empirical\nBayes estimation of normal means. Ann. Stat. 37, No 4, 1647-1684.\nJiang, W. and Zhang, C.-H. (2010). Empirical Bayes in-season prediction of\nbaseball batting average. Borrowing Strength: Theory Powering ApplicationA festschrift for L.D. Brown J.O. Berger, T.T. Cai, I.M. Johnstone, eds.\nIMS collections 6, 263-273.\nJohnstone, I. (1984). Admissibility, difference equations and recurrence in\nestimating a Poisson mean. Ann. Stat. 12, 1173-1198.\nJohnstone, I. and Lalley, S. (1984). On independent statistical decision problems and products of diffusions. Z. fur Wahrsch. 68, 29-47.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n31\n\nKoenker, R. and Mizera, I. (2012). Shape constraints , compound decisions\nand empirical Bayes rules. Manuscript.\nKiefer, J. and Wolfowitz, J. (1956). Consistency of the maximum likelihood estimator in the presence of infinitely many incidental parameters.\nAnn.Math.Stat. 27 , 887-906.\nMammen, E. (1991). Estimating a smooth monotone Regression function.\nAnn.Stat. 19, No. 2, 724-740.\nMaritz, J.S. (1969). Empirical Bayes estimation for the Poisson distribution.\nBiometrika 56, N0.2, 349-359.\nPark, J. (2011). Non parametric empirical Bayes estimator in simultaneous\nestimation of Poisson means with application to mass spectrometry data.\nJournal of Nonparametric Statistics.\nR Development Core Team (2008). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\nISBN 3-900051-07-0, URL http://www.R-project.org.\nRobbins, H. (1951). Asymptotically subminimax solutions of compound decision problems. Proc. Second Berkeley Symp. 131-148.\nRobbins, H. (1955). An Empirical Bayes approach to statistics. Proc. Third\nBerkeley Symp. 157-164.\nRobbins, H. (1964). The empirical Bayes approach to statistical decision\nproblems. Ann.Math.Stat. 35, 1-20.\nRobertson, T., Wright, F. T. and Dykstra, R. L. (1988). Order Restricted\nStatistical Inference. Wiley, New York.\n\n\fBrown, Greenshtein, Ritov/Poisson Compound\n\n32\n\nZhang, C.-H.(2003). Compound decision theory and empirical Bayes methods.(invited paper). Ann. Stat. 31 379-390.\n\n\f"}