{"id": "http://arxiv.org/abs/cond-mat/0105345v1", "guidislink": true, "updated": "2001-05-17T13:50:21Z", "updated_parsed": [2001, 5, 17, 13, 50, 21, 3, 137, 0], "published": "2001-05-17T13:50:21Z", "published_parsed": [2001, 5, 17, 13, 50, 21, 3, 137, 0], "title": "Learning to coordinate in a complex and non-stationary world", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0504069%2Ccond-mat%2F0504074%2Ccond-mat%2F0504482%2Ccond-mat%2F0504338%2Ccond-mat%2F0504454%2Ccond-mat%2F0504153%2Ccond-mat%2F0504744%2Ccond-mat%2F0504333%2Ccond-mat%2F0504446%2Ccond-mat%2F0504286%2Ccond-mat%2F0504500%2Ccond-mat%2F0504070%2Ccond-mat%2F0504263%2Ccond-mat%2F0504545%2Ccond-mat%2F0105044%2Ccond-mat%2F0105513%2Ccond-mat%2F0105310%2Ccond-mat%2F0105471%2Ccond-mat%2F0105509%2Ccond-mat%2F0105046%2Ccond-mat%2F0105199%2Ccond-mat%2F0105059%2Ccond-mat%2F0105573%2Ccond-mat%2F0105042%2Ccond-mat%2F0105440%2Ccond-mat%2F0105118%2Ccond-mat%2F0105433%2Ccond-mat%2F0105101%2Ccond-mat%2F0105294%2Ccond-mat%2F0105078%2Ccond-mat%2F0105625%2Ccond-mat%2F0105411%2Ccond-mat%2F0105456%2Ccond-mat%2F0105511%2Ccond-mat%2F0105178%2Ccond-mat%2F0105368%2Ccond-mat%2F0105186%2Ccond-mat%2F0105394%2Ccond-mat%2F0105548%2Ccond-mat%2F0105067%2Ccond-mat%2F0105279%2Ccond-mat%2F0105366%2Ccond-mat%2F0105096%2Ccond-mat%2F0105506%2Ccond-mat%2F0105336%2Ccond-mat%2F0105095%2Ccond-mat%2F0105345%2Ccond-mat%2F0105375%2Ccond-mat%2F0105237%2Ccond-mat%2F0105313%2Ccond-mat%2F0105193%2Ccond-mat%2F0105385%2Ccond-mat%2F0105225%2Ccond-mat%2F0105492%2Ccond-mat%2F0105019%2Ccond-mat%2F0105276%2Ccond-mat%2F0105177%2Ccond-mat%2F0105191%2Ccond-mat%2F0105584%2Ccond-mat%2F0105434%2Ccond-mat%2F0105392%2Ccond-mat%2F0105609%2Ccond-mat%2F0105180%2Ccond-mat%2F0105437%2Ccond-mat%2F0105022%2Ccond-mat%2F0105329%2Ccond-mat%2F0105349%2Ccond-mat%2F0105391%2Ccond-mat%2F0105172%2Ccond-mat%2F0105365%2Ccond-mat%2F0105024%2Ccond-mat%2F0105505%2Ccond-mat%2F0105089%2Ccond-mat%2F0105439%2Ccond-mat%2F0105196%2Ccond-mat%2F0105055%2Ccond-mat%2F0105466%2Ccond-mat%2F0105370%2Ccond-mat%2F0105248%2Ccond-mat%2F0105075%2Ccond-mat%2F0105141%2Ccond-mat%2F0105260%2Ccond-mat%2F0105569%2Ccond-mat%2F0105403%2Ccond-mat%2F0105147%2Ccond-mat%2F0105182%2Ccond-mat%2F0105362%2Ccond-mat%2F0105308%2Ccond-mat%2F0105086%2Ccond-mat%2F0105080%2Ccond-mat%2F0105498%2Ccond-mat%2F0105287%2Ccond-mat%2F0105574%2Ccond-mat%2F0105404%2Ccond-mat%2F0105062%2Ccond-mat%2F0105285%2Ccond-mat%2F0105138%2Ccond-mat%2F0105425%2Ccond-mat%2F0105427%2Ccond-mat%2F0105229%2Ccond-mat%2F0105088&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Learning to coordinate in a complex and non-stationary world"}, "summary": "We study analytically and by computer simulations a complex system of\nadaptive agents with finite memory. Borrowing the framework of the Minority\nGame and using the replica formalism we show the existence of an equilibrium\nphase transition as a function of the ratio between the memory $\\lambda$ and\nthe learning rates $\\Gamma$ of the agents. We show that, starting from a random\nconfiguration, a dynamic phase transition also exists, which prevents the\nsystem from reaching any Nash equilibria. Furthermore, in a non-stationary\nenvironment, we show by numerical simulations that agents with infinite memory\nplay worst than others with less memory and that the dynamic transition\nnaturally arises independently from the initial conditions.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cond-mat%2F0504069%2Ccond-mat%2F0504074%2Ccond-mat%2F0504482%2Ccond-mat%2F0504338%2Ccond-mat%2F0504454%2Ccond-mat%2F0504153%2Ccond-mat%2F0504744%2Ccond-mat%2F0504333%2Ccond-mat%2F0504446%2Ccond-mat%2F0504286%2Ccond-mat%2F0504500%2Ccond-mat%2F0504070%2Ccond-mat%2F0504263%2Ccond-mat%2F0504545%2Ccond-mat%2F0105044%2Ccond-mat%2F0105513%2Ccond-mat%2F0105310%2Ccond-mat%2F0105471%2Ccond-mat%2F0105509%2Ccond-mat%2F0105046%2Ccond-mat%2F0105199%2Ccond-mat%2F0105059%2Ccond-mat%2F0105573%2Ccond-mat%2F0105042%2Ccond-mat%2F0105440%2Ccond-mat%2F0105118%2Ccond-mat%2F0105433%2Ccond-mat%2F0105101%2Ccond-mat%2F0105294%2Ccond-mat%2F0105078%2Ccond-mat%2F0105625%2Ccond-mat%2F0105411%2Ccond-mat%2F0105456%2Ccond-mat%2F0105511%2Ccond-mat%2F0105178%2Ccond-mat%2F0105368%2Ccond-mat%2F0105186%2Ccond-mat%2F0105394%2Ccond-mat%2F0105548%2Ccond-mat%2F0105067%2Ccond-mat%2F0105279%2Ccond-mat%2F0105366%2Ccond-mat%2F0105096%2Ccond-mat%2F0105506%2Ccond-mat%2F0105336%2Ccond-mat%2F0105095%2Ccond-mat%2F0105345%2Ccond-mat%2F0105375%2Ccond-mat%2F0105237%2Ccond-mat%2F0105313%2Ccond-mat%2F0105193%2Ccond-mat%2F0105385%2Ccond-mat%2F0105225%2Ccond-mat%2F0105492%2Ccond-mat%2F0105019%2Ccond-mat%2F0105276%2Ccond-mat%2F0105177%2Ccond-mat%2F0105191%2Ccond-mat%2F0105584%2Ccond-mat%2F0105434%2Ccond-mat%2F0105392%2Ccond-mat%2F0105609%2Ccond-mat%2F0105180%2Ccond-mat%2F0105437%2Ccond-mat%2F0105022%2Ccond-mat%2F0105329%2Ccond-mat%2F0105349%2Ccond-mat%2F0105391%2Ccond-mat%2F0105172%2Ccond-mat%2F0105365%2Ccond-mat%2F0105024%2Ccond-mat%2F0105505%2Ccond-mat%2F0105089%2Ccond-mat%2F0105439%2Ccond-mat%2F0105196%2Ccond-mat%2F0105055%2Ccond-mat%2F0105466%2Ccond-mat%2F0105370%2Ccond-mat%2F0105248%2Ccond-mat%2F0105075%2Ccond-mat%2F0105141%2Ccond-mat%2F0105260%2Ccond-mat%2F0105569%2Ccond-mat%2F0105403%2Ccond-mat%2F0105147%2Ccond-mat%2F0105182%2Ccond-mat%2F0105362%2Ccond-mat%2F0105308%2Ccond-mat%2F0105086%2Ccond-mat%2F0105080%2Ccond-mat%2F0105498%2Ccond-mat%2F0105287%2Ccond-mat%2F0105574%2Ccond-mat%2F0105404%2Ccond-mat%2F0105062%2Ccond-mat%2F0105285%2Ccond-mat%2F0105138%2Ccond-mat%2F0105425%2Ccond-mat%2F0105427%2Ccond-mat%2F0105229%2Ccond-mat%2F0105088&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We study analytically and by computer simulations a complex system of\nadaptive agents with finite memory. Borrowing the framework of the Minority\nGame and using the replica formalism we show the existence of an equilibrium\nphase transition as a function of the ratio between the memory $\\lambda$ and\nthe learning rates $\\Gamma$ of the agents. We show that, starting from a random\nconfiguration, a dynamic phase transition also exists, which prevents the\nsystem from reaching any Nash equilibria. Furthermore, in a non-stationary\nenvironment, we show by numerical simulations that agents with infinite memory\nplay worst than others with less memory and that the dynamic transition\nnaturally arises independently from the initial conditions."}, "authors": ["M. Marsili", "R. Mulet", "F. Ricci-Tersenghi", "R. Zecchina"], "author_detail": {"name": "R. Zecchina"}, "author": "R. Zecchina", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1103/PhysRevLett.87.208701", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cond-mat/0105345v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cond-mat/0105345v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "4 pages, 3 figures", "arxiv_primary_category": {"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat.stat-mech", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cond-mat.dis-nn", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cond-mat/0105345v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cond-mat/0105345v1", "journal_reference": "Phys. Rev. Lett 87, 208701 (2001)", "doi": "10.1103/PhysRevLett.87.208701", "fulltext": "Learning to coordinate in a complex and non-stationary world\nM. Marsili1 , R. Mulet2\u2020 , F. Ricci-Tersenghi2 and R. Zecchina2\n\narXiv:cond-mat/0105345v1 [cond-mat.stat-mech] 17 May 2001\n\n1\n\nIstituto Nazionale per la Fisica della Materia (INFM), Unit\u00e0 di Trieste\u2013SISSA, I-34014 Trieste, Italy\n2\nThe Abdus Salam International Center for Theoretical Physics, Condensed Matter Group\nStrada Costiera 11, P.O. Box 586, I-34100 Trieste, Italy\n(October 24, 2018)\nWe study analytically and by computer simulations a complex system of adaptive agents with\nfinite memory. Borrowing the framework of the Minority Game and using the replica formalism we\nshow the existence of an equilibrium phase transition as a function of the ratio between the memory\n\u03bb and the learning rates \u0393 of the agents. We show that, starting from a random configuration, a\ndynamic phase transition also exists, which prevents the system from reaching any Nash equilibria.\nFurthermore, in a non-stationary environment, we show by numerical simulations that agents with\ninfinite memory play worst than others with less memory and that the dynamic transition naturally\narises independently from the initial conditions.\n\nSocial interactions pose many coordination problems\nto individuals. Generally social agents face problems of\nsharing and distributing limited resources in an optimal\nway. Examples range from the use of public roads and\nthe Internet, to exchanging what we produce with what\nwe consume. A solution to problem of this kind invokes\nthe intervention of a public authority who finds the social\noptimum and imposes or suggests the optimal behavior\nto agents. While such a solution may be easy to find, its\nimplementation may be difficult to enforce in practical\nsituations.\nSelf-enforcing solutions \u2013 where agents achieve optimal\nallocation of resources while pursuing their self-interests,\nwithout explicit communication or agreement with others\n\u2013 are of great practical importance. Competitive markets\nare the prototypical example of such a solution: With\neverybody maximizing his own profit and no one really\ncaring for global optimality, competitive markets perform\nthe remarkable task of leading to system wide optimality.\nMicro-economics and Game Theory have gone quite\nfar in explaining what equilibria can one expect in social\ninteractions. However most of these studies deal with unrealistic cases with either few players or with many, but\nidentical, agents. Secondly the analysis is restricted to\nthe equilibria which deductively rational players would\nagree upon. Such an approach seems unrealistic in cases\ninvolving many individuals with different goals and characteristics. The computational complexity required by\ndeductive rationality may easily go far beyond the capabilities of agents. Inductive thinking, as suggested by\nArthur [1], may be a more suited model of how real\npeople behave. A growing effort has indeed been put\nin recent years in understanding under what conditions\nbounded inductively rational agents may reach optimal\noutcomes [2,3]. Several learning rules have been found\nto lead to optimal outcomes when a single agent \"plays\"\nagainst nature [4]. Similar results hold for games with\nfew players, even though non-trivial dynamical effects\ncan also arise [2].\n\nIn this letter we address the problem of how many\nheterogeneous adaptive agents learn to coordinate in a\ncomplex, eventually non-stationary, world. We draw inspiration from recent work on the Minority Game [5], in\norder to model a typical situation where a large number of agents pursue different individual goals, using a\ncertain number of distributed resources. Optimal use of\nresources becomes then a complex coordination problem.\nWe focus on agents with finite memory and finite learning rates. We find that, when agents need to \"learn\"\ncollectively a fixed structure of interactions, they can attain a close to optimal coordination, provided that their\nmemory extends far enough into the past. As the memory decreases, the system undergoes a phase transition\nto a state where agents are unable to learn and play in a\nrandom way.\nMore interestingly we find situations where the agents\nare unable to coordinate and to converge to a Nash equilibrium. Thus the game ends in a stationary regime with\nno cooperation. This is a completely dynamical effect\nwhich prevents the system from a proper convergence\nto equilibrium and makes useless the standard analysis\nbased on Nash equilibria. This is a further clear evidence\nof the relevance of tools and ideas of statistical mechanics\nin the study of complex socio-economic systems, indeed\ndynamical transitions are very well known in statistical\nmechanics [6].\nThe model we study is closely related to the Minority\nGame (MG). The reason for this choice is that this allows\nus to benefit from the detailed understanding which has\nbeen recently uncovered by the statistical mechanics approach [7,8]. On one hand we can make reference to exact\nresults, on the other we can extend our understanding of\nthis keystone model of complex adaptive systems.\nThe model is precisely defined as follows [5,7]: Agents\nlive in a world which can be in one of P states, labelled\nby an integer \u03bc = 1, . . . , P . Each agent i = 1, . . . , N can\nchoose between two personal strategies, labeled by a spin\nvariable si , which prescribe an action a\u03bcsi ,i for each state\n1\n\n\f\u03bc. These actions are drawn from a bimodal distribution\nfor all i, s and \u03bc, such that there are two possible actions,\ndo something (a\u03bcsi ,i = 1) or do the opposite (a\u03bcsi ,i = \u22121).\nThe payoff received by an agent who plays strategy si ,\nwhile her opponents take strategies s\u2212i = {sj , \u2200j 6= i},\nis, in the state \u03bc,\n\ndescribed by a replica symmetric theory, Nash equilibria\nare described by a full replica symmetry broken (RSB)\nphase [8]. Our aim is precisely that of studying the coordination of adaptive agents in a complex world with\nexponentially many optimal states (Nash equilibria).\nThe second key feature is that previous work has only\nexplored the dynamics of learning with an infinite memory \u2013 i.e. with \u03bb = 0 in Eq. (3) \u2013 and for a fixed structure\nof interactions \u2013 i.e. with fixed (quenched) disorder a\u03bcs,i .\nOur goal is to clarify the role of different time-scales involved in the learning dynamics. We shall first study the\ncase where the structure of interactions is fixed \u2013 which\ncorresponds to a\u03bcs,i being the usual quenched disorder \u2013\nand then move to the more realistic case where the structure of interactions changes over long time-scales.\nFollowing the lines of reasoning of Refs. [7,13], we\nintroduce a continuum time \u03c4 = \u0393t/P and variables\nyi (\u03c4 ) = \u0393[U+,i (t) \u2212 U\u2212,i (t)]/2 in terms of which the dynamics reads\nX\n\u03bb\ndyi\n= \u2212 y i \u2212 hi \u2212\nJi,j tanh(yj ) + \u03b7i (\u03c4 ) ,\n(4)\nd\u03c4\n\u0393\n\nu\u03bci (si , s\u2212i ) = \u2212a\u03bcsi ,i A\u03bc ,\n(1)\nP\nwhere A\u03bc = j a\u03bcsj ,j . The total payoff to agents is always negative: The majority of agents receives a negative\npayoff whereas only the minority of them gain.\nThe game is repeated many times; the state \u03bc is drawn\nfrom a uniform distribution \u03c1\u03bc = 1/P at each time and\nagents try to estimate, on the basis of past observations,\nwhich of their strategies is the best one. More precisely,\nif si (t) is the strategy played by agent i at time t, we\nassume as in [9] that\nProb[si (t) = s] \u221d exp [\u0393Us,i (t)]\n\n,\n\n(2)\n\nwhere Us,i (t) is the score of strategy s at time t and \u0393 is\na positive constant [10]. Each agent monitors the scores\nUs,i (t) of each of her strategies s by\nUs,i (t + 1) = (1 \u2212 \u03bb/P )Us,i (t) + u\u03bci [s, s\u2212i (t)]/P\n\nj6=i\n\nhi =\n\n, (3)\n\nwhere the last term is the payoff agent i would have received if she had played strategy s at time t \u2013 see Eq. (1)\n\u2013 against the strategies s\u2212i (t) = {sj (t), \u2200j 6= i} played\nby her opponents at that time.\nIn words, Eqs. (2,3) model agents who play more\nlikely strategies which have performed better in the past.\nEqs. (2,3) belong to a class of learning models which has\nreceived much attention recently [3].\nThe relevant parameter [11] is the ratio \u03b1 = P/N between the \"information complexity\" P and the number\nof agents, and the key quantity we shall look at is the\nglobal efficiency defined as \u03c3 2 = hA2 i.\nThis model differs from the MG [5] for two important\naspects: First agents compute correctly the payoff for\nstrategies s 6= si (t) which they did not play. In the MG\nagents only account for the explicit dependence of u\u03bci on\ns which arises from a\u03bcs,i \u2013 see Eq. (1) \u2013 whereas they neglect the fact that if they had taken a different decision\nalso A\u03bc would have changed. This seems reasonable at\nfirst sight because A\u03bc is an aggregate quantity and its\ndependence on each individual agent is weak. A more\ncareful analysis [7,8] however shows that if agents properly account for their impact on A\u03bc as in Eq. (3) a radically different scenario arises: Rather than converging to\nan unique stationary state as in the MG, the dynamics\n(with \u03bb = 0) converges to one of exponentially many (in\nN ) states \u2013 which are Nash equilibria [12] \u2013 characterized by an optimal coordination. This change emerges in\nthe statistical mechanics approach with the breakdown\nof replica symmetry (RS): While the Minority Game is\n\nJi,j =\n\n\u03bc\n\u03bc\n\u03bc\n\u03bc\nP\nN\n1 X X a+,i \u2212 a\u2212,i a+,j + a\u2212,j\nP \u03bc=1 j=1\n2\n2\n\u03bc\n\u03bc\n\u03bc\n\u03bc\nP\n1 X a+,i \u2212 a\u2212,i a+,j \u2212 a\u2212,j\nP \u03bc=1\n2\n2\n\n,\n\n,\n\nwith \u03b7i (\u03c4 ) a white noise with zero mean and correlations\nh\u03b7i (\u03c4 )\u03b7j (\u03c4 \u2032 )i \u2243\n\n\u0393\u03c3 2\n\u03b4i,j \u03b4(\u03c4 \u2212 \u03c4 \u2032 )\n\u03b1N\n\n.\n\nRefs. [7,13] have shown that, for \u03bb = 0, the stationary\nstates of this dynamics are related to the local minima of\nX\nX\nh i mi +\nJi,j mi mj ,\n\u03c3 2 = H0 + 2\ni\n\nj6=i\n\nwhere H0 is a constant and mi = htanh(yi )i. These states\nare also Nash equilibria [12], which means that agents\nachieve an optimal coordination. Since \u03c3 2 takes its minima for mi = \u00b11 \u2013 which correspond to yi \u2192 \u00b1\u221e \u2013 the\nstochastic force \u03b7i (t) is irrelevant in the late stages of the\ndynamics, which is dominated by the deterministic drift\ntowards the Nash equilibrium.\nFor \u03bb/\u0393 > 0 we expect the stochastic force \u03b7i (\u03c4 ),\nwhose strength is itself proportional to \u03c3 2 , to compete\nwith the deterministic drift. Indeed the distribution of\nyi will be cutoff for |yi | \u226b \u0393/\u03bb: For small \u03bb we expect\n(\u221e)\nwhich minithat htanh(yi )i is close to the values mi\nmize \u03c3 2 , and a spread in the distribution of yi around\nits average which is maintained by the stochastic force.\nWhen \u03bb increases we expect a transition to a phase where\nagents are unable to coordinate because their memory is\ntoo short for learning correctly the interaction structure:\n2\n\n\fThe dynamics is dominated by the stochastic force \u03b7i ,\nwhich is made even stronger by the fact that \u03c3 2 /N \u2243 1\nis much larger than in the coordinated state. This transition is captured by the statistical mechanics approach\nof Ref. [7]. Neglecting stochastic fluctuations induced by\n\u03b7i , which is legitimate only for \u0393 \u226a 1, one can easily\n(\u03bb)\nprove, following Ref. [7], that mi = htanh yi i are given\nby the solution of the minimization of the function\n\u0003\n\u03bb X\u0002\nlog(1 \u2212 m2i ) + 2mi tanh\u22121 (mi ) . (5)\nH = \u03c32 +\n\u0393 i\n\n2.5\n\nQ\n\nIII\n\n2\n\u03c3 2/N\n\nII\n\u03bb\n\n1.5\n\n1\n0.8\n0.6\n0.4\n0.2\n0\n0\n\n0.2\n\n1\n\n0.5\n\nIn order to study the ground state properties of H we follow the same steps of Ref. [7]: We introduce an inverse\ntemperature \u03b2, we compute the partition function and\nthe free energy per agent and then we take averages over\nthe disordered variables a\u03bcs,i with the replica method [14].\nThe free energy, within the RS Ansatz, reads\n\u0015\n\u0014\n\u03b1\n1+q\n\u03b2(Q \u2212 q)\n\u03b1\n+\nf (q, r, Q, R) = ln 1 +\n\u03b2\n\u03b1\n2 \u03b1 + \u03b2(Q \u2212 q)\nZ 1\n1\n\u03b1\u03b2\n1\u2212Q\n\u2212 hln\ndm e\u2212\u03b2Vz (m) i +\n(RQ \u2212 rq) , (6)\n+\n2\n\u03b2\n2\n\u22121\nP\nwhere Q = N1 i (mi )2 and q = hmai mbi i with a 6= b labelling different replicas of the systems;\n\u221a R and r arise\nas Lagrange multipliers and Vz (m) = \u2212 \u03b1rmz + \u03b1\u03b2\n2 (r \u2212\n\u22121\n2 \u03bb\n2\nR)m + \u0393 [log(1\u2212m )+2m tanh (m)]. The ground state\nproperties of H are obtained solving the saddle point\nequations [14] in the limit \u03b2 \u2192 \u221e.\nIn the inset of Fig. 1 we compare the analytical predictions for \u03c3 2 and Q with simulations results. We focus on\nsmall \u03b1 (i.e. \u03b1 = 0.1) where the effects we wish to discuss\nare more evident. Little discrepancies between numerical data and analytical curves are maybe due to RSB effects. Note that a phase transition occurs at \u03bbc \u2243 0.46\u0393\nwhere both \u03c3 2 and Q change their analytical behaviour.\nWe have studied this equilibrium phase transition in the\n(\u03bb, 1/\u0393) plane, confirming the critical line \u03bbc = 0.46\u0393:\nOpen symbols in Fig. 1 refer to a static experiment where\nwe let the system equilibrate to a Nash equilibrium for\n\u03bb = 0 and then we move it slowly along lines \u03bb\u0393 = const.\nThe situation changes when the system starts from\nscratch [Us,i (0) = 0 \u2200{s, i}] in each run. Depending on \u03bb\nand \u0393, the dynamics may lead the system to a stationary regime (different from the static one) which is characterized by larger fluctuations (i.e. larger \u03c3 2 ). These\ndynamical effects make the phase diagram more complex\nin the \u03bb < \u03bbc region (see Fig. 1): In I the system always\nrelaxes to the static equilibrium, in II it sometimes converges to equilibrium and sometimes get trapped in the\nmetastable regime with large fluctuations, while in III it\nnever reaches equilibrium. The presence of this dynamical transition implies that the analysis in terms of Nash\nequilibria is no longer enough to predict the collective behavior of the system in a large part of the phase diagram,\ni.e. for high learning rates and short memory.\n\n0.4 0.6\n\u03bb/\u0393\n\n0.8\n\n1\n\nI\n\n0\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n\u0393 -1\n\nFIG. 1. Phase diagram: static (\u25e6) and dynamic (\u2022) critical\nlines obtained from the simulation. The full line represents\nthe RS critical line. The dashed lines are guide to the eyes.\nInset: Q (\u2022) and \u03c3 2 /N (\u25e6) as a function of \u03bb/\u0393 obtained for\nthe simulation. The lines represent the RS solution.\n\nWhen the external world is non-stationary, i.e. changes\nwith time, the adaptation task becomes still harder. We\nmimic the external world modification as follows: Every \u03c4 time steps a randomly chosen state of the world is\nremoved and a new one replaces it (in order to keep P\nconstant). Actually we randomly choose a \u03bc index and\nwe re-extract the strategies a\u03bcs,i for all i and s.\nHere we focus on the results of the simulations done\nwith \u03c4 = 103 , \u0393 = \u221e, N P = 104 and many \u03bb values.\nThe results do not dependent on the initial conditions.\n1\n10-1\n10-2\n\u03c3 2/N\n\n\u03bb = 2.5\n\u03bb = 3.5\n1\n10-1\n10-2\n102\n\n103\n\n104\nt\n\n105\n\n106\n\nFIG. 2. In a non-stationary world (\u03c4 = 103 ) the evolution\nof \u03c3 2 /N with simulation time for 50 different samples and two\nvalues of \u03bb (N P = 104 , \u03b1 = 0.1 and \u0393 = \u221e).\n\nIn the upper panel of Fig. 2 we show the relaxation\nof \u03c3 2 /N for \u03bb = 2.5: As expected, it starts from 1 and\nconverges to its equilibrium value. Note that \u03c4 = 103\nhas been chosen in order to allow the system to reach a\ncooperative behaviour before the world starts changing.\nFor this value of \u03bb the system is robust with respect to\n3\n\n\fchanges of the world: Apart from occasional excursions\nto states with large \u03c3 2 , agents are able to adapt themselves to the evolving interaction structure.\nIn the lower panel we present the evolution of \u03c3 2 /N\nfor \u03bb = 3.5 (i.e. with shorter memory) in 50 different\nsamples. The behaviour is now completely different: After having reached a low value of \u03c3 2 /N (cooperation) the\nsystem undergoes a sharp transition and \u03c3 2 /N jumps to\na high value. The players are no longer able to adapt\nto the changing world and they start playing in a wrong\nway. Occasionally agents may achieve a good coordination with small \u03c3 2 , but they eventually always go back\nto uncoordinated states with large \u03c3 2 .\nFor large times, the instantaneous values of \u03c3 2 /N have\na roughly bimodal distribution: They are either low\n(\u223c 10\u22122 ) or high (\u223c 1). In Fig. 3 we plot the average\nof the low (\u25e6) and of the high (\u2737) values (these averages\ncan be defined in an unambiguous way thanks to the gap\nbetween low and high \u03c3 2 values). In the inset we report\nthe fraction of samples that spend the last decade in the\nhigh \u03c3 2 regime. In a whole intermediate range around\n\u03bbc \u2248 3.3 we find that coordinated states with small \u03c3 2\ncoexist with wildly fluctuating states (\u03c3 2 > 1).\n\na fixed world \u2013 shows features of first order transitions\nsuch as discontinuities and phase coexistence.\nIn conclusion, we have extended the replica solution of\nthe Minority Game to the case where agents have finite\nmemory and finite learning rates. We have proven that a\nphase transition between phases with low and high \u03c3 2 exists as a function of \u03bb/\u0393. We have also shown, by means\nof computer simulations, that a dynamical phase transition exists for high values of \u03bb (short memories), and that\nthis dynamic phase transition is responsible for a noncooperative behaviour of agents. Furthermore we have\nshown, by numerical simulation, that when the structure\nof the interactions is non-stationary, agents with infinite\nmemory behave worst than agents with a finite memory.\nUnder these conditions we recover again a scenario where\nagents with too short memory display a first order transition from a cooperative to a non-cooperative phase.\n\n\u2020\n\n[1]\n[2]\n\n10\n\n[3]\n[4]\n\n1\n0.8\n\n\u03c32/N\n\nProb\n\n1\n\n[5]\n[6]\n\n0.6\n0.4\n0.2\n0\n\n0.1\n\n2\n\n2.5\n\n3\n\u03bb\n\n3.5\n\n[7]\n\n4\n\n[8]\n\n0.01\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\u03bb\n\n2.5\n\n3\n\n3.5\n\n[9]\n\n4\n\nFIG. 3. Average low (\u25e6) and high (\u2737) \u03c3 2 /N as a function of \u03bb (N P = 104 , \u03b1 = 0.1, \u0393 = \u221e and \u03c4 = 103 ).\nThe arrow indicates a transition from the cooperative to the\nnon-cooperative regime. The horizontal dotted line is the\n\u03c3 2 /N value with fixed world (\u03c4 = \u221e). Inset: Probability of\nbeing in a non-cooperative regime as a function of \u03bb.\n\n[10]\n\nIs worth noticing some facts in Fig. 3. The minimum\nof \u03c3 2 , corresponding to the best cooperation, is no longer\nlocated in \u03bb = 0 (i.e. infinite memory). In other words,\nin a non-stationary environment the agents play better\nwith a finite memory, which allows them to take decision based more on the recent past rather than on the\nfar past. The minimum they can attain is very near to\nthe \u03c3 2 /N value in an unchanging world (shown with a\nhorizontal line in Fig. 3). The second remarkable fact\nis that the transition from a coordinated state to a high\n\u03c3 2 regime when \u03bb increases \u2013 which was continuous in\n\n[11]\n[12]\n\n[13]\n[14]\n\n4\n\nPermanent address: Supercond. Lab., Fac. F\u0131\u0301sica IMRE, Univ. Havana, CP 10400, La Habana, Cuba.\nW.B. Arthur, Am. Econ. Assoc. Papers Proc. 84, 406\n(1994).\nD. Fudenberg and D.K. Levine, The theory of learning in\ngames (MIT Press, 1998).\nC. Camerer and T.-H. Ho, Econometrica 67, 827 (1999).\nA. Rustichini, Games and Economic Behavior 29, 244\n(1999).\nD. Challet and Zhang Y.-C., Physica A 246, 407 (1997).\nJ.P. Bouchaud, L.F. Cugliandolo, J. Kurchan and M.\nM\u00e9zard, in Spin Glasses and Random Fields, A.P. Young\ned. (World Scientific, Singapore, 1998).\nD. Challet, M. Marsili and R. Zecchina, Phys. Rev. Lett.\n84, 1824 (2000); M. Marsili, D. Challet and R. Zecchina,\nPhysica A 280, 522 (2000).\nA. De Martino and M. Marsili, J. Phys. A 34, 2525\n(2001).\nA. Cavagna, J.P. Garrahan, I. Giardina, and D. Sherrington, Phys. Rev. Lett. 83, 4429 (1999).\nThe probabilistic nature of agent's choice does not necessarily imply that agents randomize their behavior on purpose. Mc Fadden [Ann. Econ. Soc. Measurement, 5, 363\n(1976)] has indeed shown that Eq. (2) models individuals\nwho maximize an \"utility\" which has an implicit random\nidiosyncratic part. The constant \u0393 is then the relative\nweight which agents assign to the empirical evidence accumulated in Us,i with respect to random idiosyncratic\nshocks: If \u0393 \u2192 \u221e agents always play their best strategy\naccording to the scores, while if \u0393 decreases agents take\nless into account past performances.\nR. Savit, R. Manuca and R. Riolo, Phys. Rev. Lett. 82,\n2203 (1999).\n{s\u2217i , \u2200i} is a Nash equilibrium if for all i, ui (s, s\u2217\u2212i ) \u2264\nui (s\u2217i , s\u2217\u2212i ) holds \u2200s. See D. Fudenberg and J. Tirole,\nGame Theory (MIT Press, 1991).\nM. Marsili and D. Challet, e-print cond-mat/0102257.\nM. M\u00e9zard, G. Parisi, M. A. Virasoro, Spin glass theory\nand beyond (World Scientific, Singapore, 1987).\n\n\f"}