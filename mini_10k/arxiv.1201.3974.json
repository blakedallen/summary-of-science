{"id": "http://arxiv.org/abs/1201.3974v3", "guidislink": true, "updated": "2012-04-10T17:06:23Z", "updated_parsed": [2012, 4, 10, 17, 6, 23, 1, 101, 0], "published": "2012-01-19T05:43:20Z", "published_parsed": [2012, 1, 19, 5, 43, 20, 3, 19, 0], "title": "Perfect Sampling with Unitary Tensor Networks", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.0862%2C1201.4037%2C1201.5847%2C1201.0355%2C1201.0032%2C1201.1415%2C1201.3553%2C1201.0550%2C1201.2914%2C1201.4530%2C1201.4339%2C1201.2491%2C1201.2245%2C1201.3719%2C1201.0979%2C1201.5582%2C1201.3913%2C1201.4632%2C1201.1148%2C1201.6319%2C1201.2309%2C1201.6672%2C1201.0450%2C1201.4737%2C1201.3787%2C1201.1976%2C1201.2942%2C1201.5981%2C1201.2408%2C1201.4726%2C1201.6176%2C1201.5185%2C1201.3713%2C1201.3985%2C1201.3565%2C1201.2010%2C1201.5247%2C1201.0787%2C1201.2276%2C1201.2539%2C1201.1997%2C1201.4145%2C1201.2588%2C1201.0655%2C1201.1559%2C1201.5118%2C1201.1193%2C1201.5611%2C1201.4752%2C1201.2826%2C1201.1837%2C1201.0480%2C1201.6275%2C1201.3364%2C1201.3709%2C1201.1578%2C1201.5168%2C1201.4260%2C1201.4669%2C1201.1968%2C1201.4611%2C1201.2376%2C1201.6239%2C1201.3974%2C1201.4658%2C1201.3393%2C1201.2368%2C1201.3834%2C1201.1108%2C1201.4714%2C1201.2038%2C1201.6049%2C1201.5453%2C1201.3269%2C1201.1819%2C1201.0319%2C1201.0953%2C1201.2630%2C1201.5479%2C1201.2762%2C1201.0797%2C1201.3995%2C1201.3129%2C1201.1217%2C1201.3408%2C1201.4943%2C1201.3145%2C1201.1468%2C1201.1013%2C1201.3371%2C1201.6016%2C1201.4971%2C1201.0965%2C1201.0098%2C1201.2725%2C1201.6594%2C1201.0482%2C1201.6241%2C1201.1021%2C1201.0856%2C1201.5573&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Perfect Sampling with Unitary Tensor Networks"}, "summary": "Tensor network states are powerful variational ans\\\"atze for many-body ground\nstates of quantum lattice models. The use of Monte Carlo sampling techniques in\ntensor network approaches significantly reduces the cost of tensor\ncontractions, potentially leading to a substantial increase in computational\nefficiency. Previous proposals are based on a Markov chain Monte Carlo scheme\ngenerated by locally updating configurations and, as such, must deal with\nequilibration and autocorrelation times, which result in a reduction of\nefficiency. Here we propose a perfect sampling scheme, with vanishing\nequilibration and autocorrelation times, for unitary tensor networks -- namely\ntensor networks based on efficiently contractible, unitary quantum circuits,\nsuch as unitary versions of the matrix product state (MPS) and tree tensor\nnetwork (TTN), and the multi-scale entanglement renormalization ansatz (MERA).\nConfigurations are directly sampled according to their probabilities in the\nwavefunction, without resorting to a Markov chain process. We also describe a\npartial sampling scheme that can result in a dramatic (basis-dependent)\nreduction of sampling error.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1201.0862%2C1201.4037%2C1201.5847%2C1201.0355%2C1201.0032%2C1201.1415%2C1201.3553%2C1201.0550%2C1201.2914%2C1201.4530%2C1201.4339%2C1201.2491%2C1201.2245%2C1201.3719%2C1201.0979%2C1201.5582%2C1201.3913%2C1201.4632%2C1201.1148%2C1201.6319%2C1201.2309%2C1201.6672%2C1201.0450%2C1201.4737%2C1201.3787%2C1201.1976%2C1201.2942%2C1201.5981%2C1201.2408%2C1201.4726%2C1201.6176%2C1201.5185%2C1201.3713%2C1201.3985%2C1201.3565%2C1201.2010%2C1201.5247%2C1201.0787%2C1201.2276%2C1201.2539%2C1201.1997%2C1201.4145%2C1201.2588%2C1201.0655%2C1201.1559%2C1201.5118%2C1201.1193%2C1201.5611%2C1201.4752%2C1201.2826%2C1201.1837%2C1201.0480%2C1201.6275%2C1201.3364%2C1201.3709%2C1201.1578%2C1201.5168%2C1201.4260%2C1201.4669%2C1201.1968%2C1201.4611%2C1201.2376%2C1201.6239%2C1201.3974%2C1201.4658%2C1201.3393%2C1201.2368%2C1201.3834%2C1201.1108%2C1201.4714%2C1201.2038%2C1201.6049%2C1201.5453%2C1201.3269%2C1201.1819%2C1201.0319%2C1201.0953%2C1201.2630%2C1201.5479%2C1201.2762%2C1201.0797%2C1201.3995%2C1201.3129%2C1201.1217%2C1201.3408%2C1201.4943%2C1201.3145%2C1201.1468%2C1201.1013%2C1201.3371%2C1201.6016%2C1201.4971%2C1201.0965%2C1201.0098%2C1201.2725%2C1201.6594%2C1201.0482%2C1201.6241%2C1201.1021%2C1201.0856%2C1201.5573&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Tensor network states are powerful variational ans\\\"atze for many-body ground\nstates of quantum lattice models. The use of Monte Carlo sampling techniques in\ntensor network approaches significantly reduces the cost of tensor\ncontractions, potentially leading to a substantial increase in computational\nefficiency. Previous proposals are based on a Markov chain Monte Carlo scheme\ngenerated by locally updating configurations and, as such, must deal with\nequilibration and autocorrelation times, which result in a reduction of\nefficiency. Here we propose a perfect sampling scheme, with vanishing\nequilibration and autocorrelation times, for unitary tensor networks -- namely\ntensor networks based on efficiently contractible, unitary quantum circuits,\nsuch as unitary versions of the matrix product state (MPS) and tree tensor\nnetwork (TTN), and the multi-scale entanglement renormalization ansatz (MERA).\nConfigurations are directly sampled according to their probabilities in the\nwavefunction, without resorting to a Markov chain process. We also describe a\npartial sampling scheme that can result in a dramatic (basis-dependent)\nreduction of sampling error."}, "authors": ["Andrew J. Ferris", "Guifre Vidal"], "author_detail": {"name": "Guifre Vidal"}, "author": "Guifre Vidal", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1103/PhysRevB.85.165146", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/1201.3974v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1201.3974v3", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "11 pages, 9 figures, renamed partial sampling to incomplete sampling\n  for clarity, extra references, plus a variety of minor changes", "arxiv_primary_category": {"term": "cond-mat.str-el", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cond-mat.str-el", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "quant-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1201.3974v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1201.3974v3", "journal_reference": "Phys. Rev. B 85, 165146 (2012)", "doi": "10.1103/PhysRevB.85.165146", "fulltext": "Perfect Sampling with Unitary Tensor Networks\nAndrew J. Ferris1, 2 and Guifre Vidal1, 3\n\narXiv:1201.3974v3 [cond-mat.str-el] 10 Apr 2012\n\n1\n\nThe University of Queensland, School of Mathematics and Physics, Queensland 4072, Australia\n2\nD\u00e9partement de Physique, Universit\u00e9 de Sherbrooke, Qu\u00e9bec, J1K 2R1, Canada\n3\nPerimeter Institute for Theoretical Physics, Waterloo, Ontario, N2L 2Y5, Canada\n(Dated: May 22, 2018)\n\nTensor network states are powerful variational ans\u00e4tze for many-body ground states of quantum lattice models. The use of Monte Carlo sampling techniques in tensor network approaches significantly reduces the cost\nof tensor contractions, potentially leading to a substantial increase in computational efficiency. Previous proposals are based on a Markov chain Monte Carlo scheme generated by locally updating configurations and, as\nsuch, must deal with equilibration and autocorrelation times, which result in a reduction of efficiency. Here we\npropose perfect sampling schemes, with vanishing equilibration and autocorrelation times, for unitary tensor\nnetworks \u2013 namely tensor networks based on efficiently contractible, unitary quantum circuits, such as unitary\nversions of the matrix product state (MPS) and tree tensor network (TTN), and the multi-scale entanglement\nrenormalization ansatz (MERA). Configurations are directly sampled according to their probabilities in the\nwave-function, without resorting to a Markov chain process. We consider both complete sampling, involving\nall the relevant sites of the system, as well as incomplete sampling, which only involves a subset of those sites,\nand which can result in a dramatic (basis-dependent) reduction of sampling error.\nPACS numbers: 05.10.\u2013a, 02.50.Ng, 03.67.\u2013a, 74.40.Kb\n\nI.\n\nINTRODUCTION\n\nTo the computational physicist interested in onedimensional quantum lattice models, the density matrix\nrenormalization group (DMRG)1,2 is a dream come true.\nIt provides an essentially unbiased, extremely accurate\nvariational approach to ground state properties of a large\nclass of local Hamiltonians in one dimensional lattices.\nDMRG operates by approximating the ground state of the\nsystem with a matrix product state (MPS)3\u20136 , which is a\nsimple tensor network with tensors connected according to\na one-dimensional array. In recent years, the success and\nbroad applicability of DMRG has been understood to follow\nfrom (i) the existence of a characteristic, universal pattern of\nentanglement common to most ground states in one spatial\ndimension; and (ii) the ability of the MPS to reproduce this\nuniversal pattern of entanglement, thanks to having its tensors\nconnected into a one-dimensional geometry.\nThe above insight has since then guided the development\nof new tensor network approaches that aim to repeat, in other\ngeometries or physical regimes of interest, the unprecedented\nsuccess of DMRG1,2,7,8 in one dimension. The recipe is quite\nsimple: first, identify a pattern of entanglement common to\na large class of ground states; then, connect tensors so that\nthey can reproduce this pattern, and use the resulting tensor\nnetwork as a variational ansatz. In this way the multi-scale,\nlayered pattern of entanglement observed in ground states\nnear a continuous quantum phase transition motivated the proposal of the multi-scale entanglement renormalization ansatz\n(MERA)9,10 to address quantum critical phenomena. Similarly, the characteristic spatial pattern of entanglement in the\nground states in two and higher dimensions motivated higherdimensional generalizations of both the MPS (known as projected entangled-pair states, PEPS11\u201317 ) and the MERA18\u201321 .\nThe cost of simulating a lattice of L sites with any of the\nabove tensor networks is roughly proportional to L, which un-\n\nderlies the efficiency of the approaches22. Importantly, however, this cost also grows as O(\u03c7p ), that is as a power p of the\ndimension \u03c7 of the indices connecting the tensors into a network. On the one hand, this bond dimension \u03c7 determines the\nsize of the tensors and therefore the number of variational parameters contained in the tensor network ansatz. On the other,\n\u03c7 is also a measure of how much entanglement the tensor network can carry. It then follows that the cost of simulations\nincreases with the amount of entanglement in the ground state\nof the system. Entanglement is indeed the key factor limiting\nthe range of applicability of tensor network approaches.\nMore specifically, for an MPS, a small power p, namely\np MPS = 3, implies that very large values of \u03c7 (of up to a\nfew thousands) can be considered even with a high-end desktop computer. Correspondingly, DMRG can address onedimensional systems with robustly entangled ground states. In\ncontrast, the cost of two dimensional simulations with PEPS\nand MERA scales with a much larger power p of \u03c7, e.g.\np PEPS = 12 in Ref. 14 and p MERA = 16 in Ref. 21, and this considerably reduces the affordable values of \u03c7. In other words,\nPEPS and MERA calculations have so far been restricted to\nsystems with relatively small amounts of ground state entanglement. A major present challenge for these approaches is\nto obtain more efficient tensor contraction schemes that could\nlower their cost.\nA possible route to reducing the scaling of computational\ncost with \u03c7 in tensor network algorithms is by using Monte\nCarlo sampling techniques, as proposed in Refs. 23\u201325. As\nreviewed in the next section, the cost of manipulating the tensor network (for a single sample) is reduced to O(\u03c7q ), where\nq is significantly smaller than p (typically of the order of p/2).\nThe proposals in Refs. 23,24 are best suited for tensor networks, such as MPS and PEPS, where the coefficients in the\ntensors are unconstrained. However, in the MERA, as well as\nin other unitary tensor networks such as unitary versions of\nMPS (uMPS) and of tree tensor network26,27 (uTTN), tensors\n\n\f2\nare subject to unitary constraints.\nThe purpose of this paper is to address the use of Monte\nCarlo sampling in the context of unitary tensor networks, including uMPS, uTTN and MERA. [Notice that this excludes\ntensor networks such as a periodic MPS or PEPS, which cannot be generically re-expressed as a unitary tensor network].\nAn important difference with respect to Refs. 23,24 is that\nin a unitary tensor network, sampling is performed on an effective lattice corresponding to the past causal cone of the local operator whose expectation value is being computed. This\nmeans that sampling typically occurs over some reduced number of sites (less than the system size L). A second difference\nis that in unitary tensor networks there is no need to use a\nMarkov chain Monte Carlo scheme. Indeed, our main result\nis the proposal and benchmark of perfect sampling schemes\nfor unitary tensor networks, by means of which one can obtain completely uncorrelated samples directly according to the\ncorrect probability. Therefore, one can sample without incurring additional computational costs due to equilibration and\nautocorrelations times. This is particularly of interest near a\nquantum phase transition, where equilibration and autocorrelation times diverge with system size L. We consider both\ncomplete (perfect) sampling and incomplete (perfect) sampling schemes. In the former, the indices for all sites of the\neffective lattice are sampled. In the latter, only the indices of a\nsubset of sites is sampled, while the indices of the rest of sites\nare contracted exactly, with an insignificant or minor increase\nof computational cost as far as the scaling O(\u03c7q ) is concerned.\nImportantly, the statistical variance (due to sampling) of an\nexpectation value obtained with incomplete sampling can decrease dramatically with a proper chose of sampling basis, as\nillustrated in Fig. 9 with a drop of 10\u22127 in error.\nThe paper is organized in sections as follows. First, in section II we briefly review the use of Monte Carlo sampling\ntechniques to evaluate the expectation value of local operators in context of tensor networks, and introduce the notions\nof complete and incomplete sampling. Then in section III we\nexplain how the proposals of Refs. 23,24 can be adapted to\nthe case of a unitary tensor network by sampling within the\npast causal cone of the local operator. In section IV we propose a complete perfect sampling scheme for unitary tensor\nnetworks. Its performance is demonstrated for a uMPS with\nthe quantum Ising chain at criticality. In section V we then\npresent an incomplete perfect sampling scheme. We discuss\ncomputational costs in section VI. The conclusions in Section VII and an Appendix analyzing the variance in different\nschemes close the paper.\nWe emphasize that this paper is only concerned with the\nevaluation of local expectation values from a unitary tensor\nnetwork. That is, here we assume that the unitary tensor network has already been optimized and focus on how to extract\ninformation from it. The optimization of unitary tensor networks using variational Monte Carlo is discussed in Ref. 29.\n\nII.\n\nBACKGROUND MATERIAL: SAMPLING IN TENSOR\nNETWORK ALGORITHMS\n\nLet us start by introducing our notation and by reviewing\nsome basic concepts.\n\nA. Exact contraction versus sampling\n\nLet L be a lattice made of L sites, with vector space\nVL \u2261 \u2297 L\ni=1 V, where V is the d-dimensional vector space\nof one site. Let |\u03a8i \u2208 VL denote the wave-function encoded\nin the tensor network and let \u00c2 be a local operator on VL . An\nimportant task in tensor network algorithms is to compute the\nexpectation value h\u03a8|\u00c2|\u03a8i, which can be expressed as\nh\u03a8|\u00c2|\u03a8i =\n\nX\ns\u2208S\n\nh\u03a8|sihs|\u00c2|\u03a8i,\n\n(1)\n\nwhere |si \u2261 |s1 i \u2297 |s2 i \u2297 * * * \u2297 |sL i denotes a product\nstate of the L sites of the lattice, with si = 1, 2, * * * , d labelling the elements of an orthonormal basis {|si i} on site i,\ni = 1, 2, * * * , L. Here, S is the set of all dL possible configurations s = (s1 , s2 , * * * , sL ) of the system. The expectation value of Eq. (1) can be obtained exactly by contracting\nthe corresponding tensor network. However, a large computational cost motivates the search for an alternative approach\nbased on sampling.\nIn preparation for an approximate evaluation of the expectation value h\u03a8|\u00c2|\u03a8i, let us first introduce the probability\nQ(s) \u2261 |hs|\u03a8i|2 of projecting state |\u03a8i into the product state\n|si, and the estimator A(s) \u2261 hs|\u00c2|\u03a8i/hs|\u03a8i, and rewrite\nEq. (1) as\nh\u03a8|\u00c2|\u03a8i =\n\nX\n\nQ(s)A(s).\n\n(2)\n\ns\u2208S\n\nThis expression emphasizes that h\u03a8|\u00c2|\u03a8i can be regarded as a\nprobabilistic average of estimatorPA(s) according to the probabilities Q(s), where Q(s) \u2265 0, s\u2208S Q(s) = 1.\nLet us replace the sum over the set S of all |S| = dL configurations s with a sum over some subset S\u0303 \u2286 S containing\nN \u2261 |S\u0303| configurations s, where N < dL , that is\nh\u03a8|\u00c2|\u03a8i \u2248\n\n1 X\nQ(s)A(s),\nZ\n\n(3)\n\ns\u2208S\u0303\n\nP\nwhere Z \u2261\ns\u2208S\u0303 Q(s) is a normalization factor. Eq. (3)\nstates that an approximate evaluation of h\u03a8|\u00c2|\u03a8i is obtained\nby considering a probabilistic sum over N configurations s. If\nthe N configurations in S\u0303 have been randomly chosen from S\naccording to the probability Q(s), then importance sampling\nallow us to replace the previous expression with\nh\u03a8|\u00c2|\u03a8i \u2248\n\n1 X\nA(s).\nN\ns\u2208S\u0303\n\n(4)\n\n\f3\n\u221a\nstandard deviation \u03c3A / N of N independent samples, scales\nwith N as\ns\n2\n\u03c3\u00c2\n.\n(10)\n\u01ebA (N ) \u2248\nN\nLet us analyze in which sense the above Monte Carlo sampling strategy could be of interest. The cost (i.e. computational time) of an exact contraction, Eq. (1), scales as O(\u03c7p )\nwith the bond dimension \u03c7. On the other hand, notice that for\neach specific configuration s, the contribution h\u03a8|sihs|\u00c2|\u03a8i\nto h\u03a8|\u00c2|\u03a8i consists of two tensor networks, namely one for\nh\u03a8|si and another for hs|\u00c2|\u03a8i, whose contraction can be accomplished with a cost O(\u03c7q ), for some q < p, see Fig. 1.\n[This is also the cost of computing Q(s) and A(s) in Eq. (2)].\nIf the number of samples required to obtain an acceptably\n\u2032\nsmall error \u01ebA (N ) is N \u2248 O(\u03c7q ), the use of sampling in\u2032\ncurs a computational cost of O(\u03c7q+q ) instead of O(\u03c7p ). We\nconclude that if q + q \u2032 < p, then (for large \u03c7) the sampling\nstrategy will have a lower computational cost than the exact\ncontraction.\nFIG. 1: (Color online) Contraction of a tensor network. (a) Tensor\nnetwork corresponding to the expectation value h\u03a8|\u00c2|\u03a8i, with a sum\nover (or exact contraction of) indices s1 , s2 , * * * , s6 (exact contraction). Contracting this tensor network has a cost that scales as O(\u03c7p )\nwith the bond index \u03c7, for some power p. (b) Tensor networks corresponding to h\u03a8|sihs|\u00c2|\u03a8i for a given configuration s, corresponding to a single sample. The cost of contracting these two networks\nscales as O(\u03c7q ) with the bond index \u03c7, where power q is smaller\nthan power p. (c) Tensor network corresponding to h\u03a8|s\u22c4 ihs\u22c4 |\u00c2|\u03a8i\nfor a given incomplete configuration s\u22c4 \u2261 (s1 , s2 , s3 ) (these three\nindices are being sampled), where in addition there is a sum over (or\nexact contraction of) indices s4 , s5 and s6 . The cost of contracting\n\u2032\nthis tensor network scales as O(\u03c7q ), with q \u2032 somewhere between q\nand p.\n\nEquation (4) estimates h\u03a8|\u00c2|\u03a8i by means of N independent samples of a random variable (A(s), Q(s)). By construction, the mean \u0100 of this random variable,\nX\n\u0100 \u2261\nQ(s)A(s),\n(5)\ns\u2208S\n\nis given by the expectation value h\u03a8|\u00c2|\u03a8i of operator \u00c2, see\n2\nEq. (2). Notice that, in addition, its variance \u03c3A\n, defined by\nX\n2\n\u03c3A\n\u2261\nQ(s)|A(s) \u2212 \u0100|2\n(6)\ns\n\n=\n\nX\ns\n\nQ(s)|A(s)|2 \u2212 |\u0100|2 ,\n\n(7)\n\n2\nalso equals the variance \u03c3\u00c2\nof operator \u00c2,\n\u0011\n\u0010\n2\n\u03c3\u00c2\n\u2261 h\u03a8| |\u00c2 \u2212 h\u03a8|\u00c2|\u03a8i|2 |\u03a8i\n2\n\n2\n\n= h\u03a8|(|\u00c2| )|\u03a8i \u2212 |h\u03a8|\u00c2|\u03a8i| ,\n\n(8)\n(9)\n\n2\n2\nthat is \u03c3A\n= \u03c3\u00c2\n, see Appendix. It follows that the error\n\u01ebA (N ) in the approximation of Eq. (4), as measured by the\n\nB. Combining exact contraction with sampling:\nIncomplete sampling\n\nMore generally, one can consider a hybrid strategy which\ncombines exact contraction and sampling. This is accomplished by sampling over only a subset of the L indices corresponding to the L sites of lattice L, while performing an\nexact contraction on the remaining sites. For instance, Fig.\n1(c) considers a lattice L made of L = 6 sites where the first\nthree sites are being sampled, with configuration (s1 , s2 , s3 ),\nwhereas the remaining three of sites are being addressed with\nan exact contraction.\nIf we denote by s\u22c4 \u2208 S \u22c4 a configuration of the L\u22c4 indices\nto be sampled (L\u22c4 < L), then Eq. 1 is replaced with\nX\nh\u03a8|\u00c2|\u03a8i =\nh\u03a8|s\u22c4 ihs\u22c4 |\u00c2|\u03a8i,\n(11)\ns\u22c4 \u2208S \u22c4\n\nWe can again rewrite Eq. (11) as a probabilistic sum of an\nestimator A\u22c4 (s\u22c4 ) \u2261 h\u03a8|s\u22c4 ihs\u22c4 |\u00c2|\u03a8i/|h\u03a8|s\u22c4 i|2 according to\nprobabilities Q(s\u22c4 ) \u2261 |h\u03a8|s\u22c4 i|2 ,\nX\nh\u03a8|\u00c2|\u03a8i =\nQ(s\u22c4 )A\u22c4 (s\u22c4 ).\n(12)\ns\u22c4 \u2208S \u22c4\n\nSimilarly, we could generalize Eqs. 3-4 and apply importance\n2\nsampling. We note that in this case the variance \u03c3A\n\u22c4 , defined\nby\nX\n2\nQ(s\u22c4 )|A\u22c4 (s\u22c4 ) \u2212 \u0100|2\n(13)\n\u03c3A\n\u22c4 \u2261\ns\u22c4\n\n=\n\nX\ns\u22c4\n\nQ(s\u22c4 )|A(s\u22c4 )|2 \u2212 |\u0100|2 ,\n\n(14)\n\n2\nmight be smaller than the variance \u03c3\u00c2\nof operator \u00c2 (Eq.\n\u22c4\n9), since a single incomplete sample s corresponds to many\n\n\f4\ncomplete samples s. [For instance, in the example of Fig.\n1(c), the incomplete sample s\u22c4 = (s1 , s2 , s3 ) corresponds to\nall complete samples s = (s1 , s2 , s3 , s4 , s5 , s6 ) that coincide\nwith s\u22c4 in the first three sites.] In other words, the statistical\nerror might be reduced. This should not come as a surprise.\nAfter all, in the extreme case where no sampling at all is performed (L\u22c4 = 0) but all indices are exactly contracted, there\nis no statistical error left.\n\nC.\n\nMarkov chain Monte Carlo\n\nIn Refs. 23,24 the random configurations s were generated\nby means of a Markov chain process based on local updates.\nGiven a stored configuration s, let us denote s\u2032i a configuration\nobtained from s by replacing in site i the value si with s\u2032i .\nThen, visiting the sites sequentially, i = 1, 2, * * * , L, in what\nis known as a sweep, a change on site i is introduced according\nto the Metropolis probability\nPchange = min[\n\nQ(s\u2032i )\n, 1].\nQ(s)\n\n(15)\n\nIn this way, after one sweep a new configuration s\u2032 is obtained\nfrom s, and by iteration a sequence of configurations\ns \u2192 s\u2032 \u2192 s\u2032\u2032 \u2192 * * *\n\n(16)\n\nis produced. However, these configurations will in general be\ncorrelated. The number \u03c4 of sweeps required between configurations s and s\u2032 in order for them to be essentially independent to be independent is known as the autocorrelation time.\nSweeping \u03c4 times between samples is necessary in order for\nthe error \u01ebA (N ) to scale as in Eq. (10), since that expression\nfor the error assumed the samples to be independent. (If only\na single sweep mediates the samples, the statistical error in\nEq. (10) increases by a factor which scales as \u03c4 1/2 due to autocorrelations). In addition, the first sample s will be obtained\nafter applying \u03c4 \u2032 sweeps to some random initial configuration. The equilibration time \u03c4 \u2032 is necessary in order to guarantee that the first sample is picked-up according to the correct\nprobability distribution. The autocorrelation time \u03c4 and the\nequilibration time \u03c4 \u2032 are known to diverge with systems size\nL for critical systems.\nLarge equilibration and autocorrelation times, e.g. near or\nat a critical point, increase the cost of simulations. This increase can be prevented if somehow independent configurations s can be directly generated according to probabilities\nQ(s). In section IV we show how this is possible for a specific class of tensor networks, namely unitary tensor networks,\nwhich are introduced next.\n\nIII.\n\nSAMPLING OF UNITARY TENSOR NETWORKS\n\nLet us specialize to the particular case of unitary tensor networks, namely tensor networks that are based on a unitary\nquantum circuit. Examples include the MERA and unitary\n\nFIG. 2: (Color online) Sampling in a unitary matrix product state\n(uMPS). (a) uMPS for a state |\u03a8i of lattice L. Notice the (fictitious)\ntime direction, which provides each tensor with a sense of which\nindices are incoming and which are outgoing. (b) The past causal\ncone C of a local operator \u00c2 acting on a single site of L (denoted by\na discontinuous circle) defines an effective lattice LC , which is found\nin state |\u03a8C i. Notice that the effective lattice LC is made of two types\nof sites, namely sites already present in the original lattice L and one\nsite not present in L, with d-dimensional and \u03c7-dimensional vector\nspaces, respectively. (c) Tensor networks representing h\u03a8|\u00c2|\u03a8i and\nh\u03a8C |\u00c2|\u03a8C i. The inset shows unitarity reductions [Eq. (17)] used to\ntransform h\u03a8|\u00c2|\u03a8i into h\u03a8C |\u00c2|\u03a8C i.\n\nversions of MPS (with open boundary conditions) and TTN,\nwhich we will refer as uMPS and uTTN32 .\nUnitary tensor networks are special in that each tensor u\nis constrained to be unitary/isometric. Figs. 2 and 3 exemplify the discussion for uMPS and uTTN respectively. Specifically, we first note that in one such tensor network there is a\nwell-defined direction of time throughout, see e.g. Figs. 2(a)\nand 3(a). Each index of a tensor u is either an incoming index (if time flows towards the tensor) or an outgoing index\n(if time flows away from the tensor). The constraint on u can\nbe expressed in the following way. Let us group all incoming\nindices of u into a composite incoming index \u03b1 and all outgoing indices of u into a composite outgoing index \u03b2, so that\ntensor u becomes a matrix u\u03b2\u03b1 . Then the unitary/isometric\nconstraint on u reads\nX\n(17)\n(u\u2020 )\u03b1\u03b2 u\u03b2\u03b1\u2032 = \u03b4\u03b1\u03b1\u2032 .\n\u03b2\n\nA direct implication of this property is that the tensor network corresponding to the expectation value h\u03a8|\u00c2|\u03a8i can be\nreplaced with a simplified tensor network where the pairs of\ntensors (u, u\u2020 ) outside the so-called past causal cone C of \u00c2\nhave been removed, see Figs. 2(c) and 3(c). This new tensor\nnetwork can be interpreted to represent the expectation value\nh\u03a8C |\u00c2|\u03a8C i of the local operator \u00c2 on a state |\u03a8C i \u2208 VLC\nof an effective lattice LC defined by the causal cone C of the\noperator \u00c2, see Figs. 2(b) and 3(b), where by construction\n\n\f5\n\nFIG. 3: (Color online) Sampling in a unitary tree tensor network\n(uTTN). (a) uTTN for a state |\u03a8i of lattice L. (b) Effective lattice\nLC . (c) Tensor networks for h\u03a8|O|\u03a8i and h\u03a8C |A|\u03a8C i. The inset\nshows a reduction due to the unitary constrain of tensors in the uTTN.\n\nh\u03a8|\u00c2|\u03a8i = h\u03a8C |\u00c2|\u03a8C i. The effective lattice LC is made of\nLC sites that can be of two types: those already contained in\nthe original lattice L, which are described by a d-dimensional\nvector space, and those which did not belong to L, which\nare described by a \u03c7-dimensional vector space. We use r =\n(r1 , r2 , * * * , rLC ) to denote a configuration of the effective lattice LC , and |ri \u2261 |r1 i \u2297 |r2 i \u2297 * * * \u2297 |rLC i the corresponding\nproduct vector, where for some sites ri = 1, 2, * * * , d and for\nsome others ri = 1, 2, * * * , \u03c7. We denote R the set of all\nconfigurations r.\nThe exact contraction of the tensor network corresponding\nto h\u03a8C |\u00c2|\u03a8C i, may still be very expensive and again we might\nbe interested in exploring the use of sampling to lower the\ncomputational cost. For that purpose, we repeat the discussion\nin section II. First we write the expectation value h\u03a8|\u00c2|\u03a8i as\nX\nh\u03a8|\u00c2|\u03a8i =\nh\u03a8C |rihr|\u00c2|\u03a8C i,\n(18)\nr\u2208R\n\nsee Fig. 4 for uMPS and uTTN. Then we rewrite Eq. (18) in\nterms of the estimator AC (r) \u2261 hr|\u00c2|\u03a8C i/hr|\u03a8C i and probabilities P (r) \u2261 |hr|\u03a8C i|2 ,\nX\nh\u03a8|\u00c2|\u03a8i =\nP (r)AC (r).\n(19)\nr\u2208R\n\nWe can again limit the sum over configurations r to a subset\nR\u0303 containing just N configurations which, when chosen from\nR randomly according to the probabilities P (r), results in\nh\u03a8|\u00c2|\u03a8i \u2248\n\n1 X C\nA (r).\nN\n\n(20)\n\nr\u2208R\u0303\n\nThe error in the approximation scales with N as in Eq. (10).\n\nFIG. 4: (Color online) Graphical representation of h\u03a8C |\u00c2|\u03a8C i =\nP\nC\nC\nr\u2208R h\u03a8 |rihr|\u00c2|\u03a8 i. In (a), the original state |\u03a8i was represented with an uMPS, see Fig. 2. In (b), the original state |\u03a8i was\nrepresented with an uTTN, see Fig. 3. However, in both cases the\nstate |\u03a8C i is represented by an uMPS that runs through the causal\ncone.\n\nIV.\n\nPERFECT SAMPLING\n\nIn this section we describe how to randomly draw configurations r according to probability P (r) in a unitary tensor network. We refer to this scheme as perfect sampling because, in\ncontrast with Markov chain Monte Carlo, the present scheme\nproduces perfectly uncorrelated samples. We will also refer\nto this scheme as complete perfect sampling, to distinguish it\nfrom the incomplete perfect sampling scheme discussed in the\nnext section, where sampling is performed only on a subset of\nsites.\n\nA.\n\nAlgorithm\n\nRecall that as a quantum circuit, the tensor network is\nequipped with a notion of (fictitious) time. From now on we\nassume that the labeling of the sites in the effective lattice LC\nhas been chosen so as to progress forward with respect to this\nnotion of time. Thus, site 1 corresponds to the earliest time,\nsite 2 corresponds to a later time, and so on, until site LC corresponds to the latest time (when two sites correspond to the\nsame time, e.g. sites 4 and 5 in Fig. 4 (a), we order them\narbitrarily).\nOur perfect sampling algorithm consists of sequentially\ncomputing a series of conditional single-site density matrices {\u03c11 , \u03c12 (r1 ), * * * } and conditional single-site probabilities\n{P (r1 ), P (r2 |r1 ), * * * }. First we compute the reduced density\n\n\f6\nand the conditional probabilities\nP (r3 |r1 , r2 ) \u2261 hr3 |\u03c13 (r1 , r2 )|r3 i,\n\n(27)\n\nand so on for the rest of sites in the effective lattice LC . In this\nway, and since\nP (r) = P (r1 )P (r2 |r1 ) * * * P (rLC |r1 , r2 , * * * , rLC \u22121 ), (28)\n\nFIG. 5: (Color online) Perfect sampling with a uMPS. The figure\nshows a sequence of the tensor networks corresponding (up to a proportionality constant) to \u03c11 , P (r1 ), \u03c12 (r2 ), P (r2 |r1 ), and so on,\nsee Eqs. (21\u201328). Importantly, all these tensor networks can be contracted with a cost that scales as O(\u03c72 ) with the bond dimension \u03c7,\nand are therefore computational less expensive than an exact contraction, which has cost O(\u03c73 ).\n\nmatrix \u03c11 for site 1 exactly, i.e. without sampling,\n\b\n\u03c11 \u2261 tr 2***LC |\u03a8C ih\u03a8C |\n\n(21)\n\nfrom which we can compute the probabilities\nP (r1 ) \u2261 hr1 |\u03c11 |r1 i.\n\n(22)\n\nWe can then randomly choose a value for r1 according to\nprobability P (r1 ), and compute (exactly) the conditional reduced density matrix \u03c12 (r1 ) for site 2, which is obtained from\nthe state hr1 |\u03a8C i of sites 2 to LC ,\n\u03c12 (r1 ) \u2261\n\n\b\n1\ntr 3***LC hr1 |\u03a8C ih\u03a8C |r1 i .\nP (r1 )\n\n(23)\n\nAgain, we can use the reduced density matrix to compute the\nconditional probabilities\nP (r2 |r1 ) \u2261 hr2 |\u03c12 (r1 )|r2 i,\n\n(24)\n\nand we can therefore randomly select a value of r2 according\nto probabilities P (r2 |r1 ). Let us notice at this point that so far\nwe have randomly chosen values for r1 and r2 according to\nthe probability\nP (r1 , r2 ) = P (r1 )P (r2 |r1 ) = ||hr1 , r2 |\u03a8C i||2 .\n\n(25)\n\nWe can now iterate the above process, that is, compute the\nconditional density matrix\n\u03c13 (r1 , r2 ) \u2261\n\n\b\n1\ntr\nC\nhr1 , r2 |\u03a8C ih\u03a8C |r1 , r2 i\nP (r1 , r2 ) 4***L\n(26)\n\nwe end up indeed randomly choosing a configuration r =\n(r1 , r2 , * * * , rLC ) with probability given precisely by P (r) \u2261\n|hr|\u03a8C i|2 .\nFig. 5 illustrates the sequence of computations in the case\nof a one-site operator \u00c2 specifically for a uMPS, assuming as\nin Figs. 2 and 4(a) that the operator \u00c2 is supported on the\nfourth site of the original chain. This algorithm is similar to\none used for thermal state sampling with MPS25 described in\nRef. 28. Analogous computations for a uTTN are very similar,\nsince the causal cone of a single-site operator \u00c2 is described\nalso by a uMPS, see Fig. 3(b). For the case of a MERA, more\ndetails on the implementation of Eqs. (21\u201328) can be found in\nRef. 29.\nA key point is that, for unitary tensor networks such as\nuMPS, uTTN, and MERA, the computational cost of generating the above sequence of density matrices and probabilities\noften does not exceed (to leading order in \u03c7 and effective size\nLC ) the cost of a single sweep in Markov chain Monte Carlo33 .\nB. Benchmark\n\nTo illustrate the performance of the perfect sampling\nscheme and compare it to Markov chain Monte Carlo, we have\nconsidered a duly optimized uMPS for the ground state |\u03a8i\nof the quantum Ising model with critical transverse magnetic\nfield,\nX\nX\n\u03c3\u0302ix ,\n(29)\n\u03c3\u0302iz \u03c3\u0302jz \u2212\n\u0124Ising \u2261 \u2212\nhi,ji\n\ni\n\non an open chain of L spins.35 The two sampling schemes are\nthen used in order to compute the expectation value of local\noperators.\nFig. 6(a) and (b) show a history of 150 configurations of\na chain of L = 50 spins obtained with perfect sampling and\nMarkov chain Monte Carlo, respectively. The existence of\ncorrelations in the second case is manifest.\nFig. 6(c) and (d) show the error in the expectation value\nz\nz\nh\u03a8|\u03c3\u030225\n|\u03a8i and h\u03a8|\u03c3\u030225\n|\u03a8i for the local operators \u03c3 z and \u03c3 x\non site 25, as a function of the number of samples N . In both\ncases, the effect of autocorrelations in Markov chain Monte\nCarlo results in an error larger than the error obtained with\nperfect sampling, which is given by Eq. (10). The ratio between statistical\n\u221a errors, as given in terms of the autocorrelation\ntime \u03c4 by 2\u03c4 + 1, is seen to depend on the choice of local\nz\noperator \u2013 this autocorrelation time is larger for h\u03a8|\u03c3\u030225\n|\u03a8i\nx\nthan for h\u03a8|\u03c3\u030225\n|\u03a8i.\nFinally, Fig. 6 (e) and (f) explore the autocorrelation time \u03c4\nfor \u03c3\u0302 z as a function of the size L of the spin chain. In particular, Fig. 6 (f) reveals that \u03c4 grows linearly in L. This means36\n\n\f7\n(b)\n20\n\n20\n\n40\n\n40\n\nSample number\n\nSample number\n\n(a)\n\n60\n80\n100\n\n80\n100\n120\n\n120\n\n140\n\n140\n1\n\n25\nSite\n\n50\n\n(c)\n\n1\n\n25\nSite\n\n50\n\nFIG. 7: (Color online) Graphical representation of h\u03a8C |\u00c2|\u03a8C i =\nP\nC\n\u22c4\nC\nr\u22c4 \u2208R\u22c4 h\u03a8 |rihr |\u00c2|\u03a8 i for a uMPS, to be compared with Fig.\n4(a). Notice that sampling does not affect two of the indices, over\nwhich an exact contraction is still performed.\n\n(d)\n0\n\n0\n\n10\n\n\u2206 \u03c3\u0302 z\n\n\u2206 \u03c3\u0302 x\n\n10\n\n\u22121\n\n\u22121\n\n10\n\n10\n0\n\n10\n\n1\n\n2\n\n0\n\n10\n10\nNumber of samples\n\n10\n\n1\n\n2\n\n10\n10\nNumber of samples\n\n0.4\n0.2\n0\n0\n\n10\n20\n30\nNumber of sweeps j\n\nAutocorrelation time \u03c4\n\n(f)\n\n(e)\nh \u03c3\u0302 z ( i ) \u03c3\u0302 z ( i + j ) i\n\n60\n\n10\n5\n0\n\n0\n\n100\n200\nSystem size\n\nFIG. 6: (Color online) Sampling of the ground state of the critical\ntransverse Ising model in the z basis. Comparison between configurations obtained using (a) the presented perfect sampling scheme and\n(b) a Markov chain scheme (single sweep) on 50 sites. Blue sites represent spin up and yellow for spin down. The correlations between\nconfigurations obtained using a Markov chain scheme are evidenced\nby the appearance of domains of well defined color that extend vertically. In (c) we have calculated the expected statistical error on\nthe estimate of h\u03c3\u0302 z i for the perfect sampling (blue line) and Markov\nchain sampling (blue dots). While with perfect sampling the error\ndecreases with the usual N \u22121/2 factor, correlations between subsequent samples increase the error on the estimate in the Markov chain\nscheme. In (d) we plot the same for h\u03c3\u0302 x i by projecting all the spins\ninto the x basis. In this case the Markov scheme used utilizes a 2-site\nupdate so as to be compatible with the wave-function symmetry34 . In\n(e) we present the correlations on the centre site (in the z basis) after\nj Markov chain sweeps using 106 samples for 50 sites (blue dots)\nand 250 sites (black crosses). In the perfect sampling scheme (blue\nline), there are no correlations between configurations. In (f) we plot\nthe estimated autocorrelation time for different system sizes.\n\nz\nthat in order to achieve a fixed accuracy in h\u03a8|\u03c3\u0302L/2\n|\u03a8i, the\nnumber of samples N with Markov chain Monte Carlo has to\ngrow linearly in L, whereas a constant number of samples is\nenough with perfect sampling.\n\nIt is important to stress, however, that the Markov chain\nMonte Carlo update scheme discussed here, based on single\nspin updates, is used as a reference only \u2013 more sophisticated\nMarkov chain Monte Carlo schemes, based e.g. on global spin\nupdates, could lead to smaller autocorrelation times.\n\nFIG. 8: (Color online) Incomplete perfect sampling with a uMPS.\nThe figure shows a complete sequence of the tensor networks corresponding (up to a proportionality constant) to \u03c11 , P (r1 ), \u03c12 (r2 ),\nP (r2 |r1 ), \u03c13 (r1 , r2 ) and P (r1 , r2 , r3 ) necessary in order to generate a configuration r\u22c4 = (r1 , r2 , r3 ) with probability P (r\u22c4 ) =\n|h\u03a8C |r\u22c4 i|2 . Notice that the cost still scales as O(\u03c72 ), as in the complete (perfect) sampling scheme.\n\nV.\n\nINCOMPLETE PERFECT SAMPLING\n\nSo far we have considered perfect sampling over the whole\ncausal cone, that is, over the indices associated to all the sites\nof the effective lattice LC . However, it is also possible to use\nan incomplte perfect sampling scheme, which combines perfect sampling over most of the sites of LC and an exact contraction over a small set of sites, without altering the scaling\nO(\u03c7q ) of the cost of a single sample. Because we are sampling over fewer indices, we can expect a decrease in the statistical error with little change in the cost. In some cases the\nreduction in statistical uncertainty can be dramatic.\n\n\f8\nA.\n\nIncomplete perfect sampling scheme\n\nThe incomplete perfect sampling scheme is illustrated in\nFig. 7 for a uMPS. The first step is to rewrite the expectation\nvalue h\u03a8C |\u00c2|\u03a8C i = h\u03a8|\u00c2|\u03a8i as\nX\nh\u03a8|\u00c2|\u03a8i =\nh\u03a8C |r\u22c4 ihr\u22c4 |\u00c2|\u03a8C i,\n(30)\nr\u22c4 \u2208R\u22c4\n\nwhere R\u22c4 is the set of incomplete configurations r\u22c4 \u2261\n(r1 , r2 , * * * , rL\u22c4 ), where L\u22c4 is the number of sites over which\nsampling takes place, with L\u22c4 < LC . For the case of the\nuMPS illustrated in Fig. 7, one can perform an exact contraction on two sites of LC , namely the site on which the local\noperator \u00c2 is supported and the effective, \u03c7-dimensional site\ncorresponding to the bond index of the uMPS. Notice that now\nthe term h\u03a8C |r\u22c4 ihr\u22c4 |\u00c2|\u03a8C i does not factorize into two terms,\nsince hr\u22c4 |\u03a8C i and hr\u22c4 |\u00c2|\u03a8C i are no longer complex numbers\nbut d\u03c7-dimensional vectors.\nWe can still rewrite Eq. (30) as a probabilitistic sum of an\nestimator A\u22c4 (r\u22c4 ) \u2261 h\u03a8C |r\u22c4 ihr\u22c4 |\u00c2|\u03a8C i/|h\u03a8C |r\u22c4 i|2 according to probabilities P (r\u22c4 ) \u2261 |h\u03a8C |r\u22c4 i|2 ,\nX\nh\u03a8|\u00c2|\u03a8i =\nP (r\u22c4 )A\u22c4 (r\u22c4 ),\n(31)\nr\u22c4 \u2208R\u22c4\n\nlimiting the sum over configurations r\u22c4 to a subset R\u0303\u22c4 containing just N configurations, and use (perfect) importance\nsampling to obtain the estimate\n1 X \u22c4 \u22c4\nA (r ).\n(32)\nh\u03a8|\u00c2|\u03a8i \u2248\nN \u22c4 \u22c4\nr \u2208R\u0303\n\nAn important difference between the incomplete perfect sampling scheme and the comnplete perfect sampling scheme of\nEqs. (18\u201320) is that the estimator A\u22c4 , whose mean is A \u0304\u22c4 =\n2\nh\u03a8|\u00c2|\u03a8i as indicated in Eq. (31), has a variance \u03c3A\n\u22c4,\nX\n2\n\u03c3A\nP (r\u22c4 )|A\u22c4 (r\u22c4 ) \u2212 \u0100\u22c4 |2\n(33)\n\u22c4 \u2261\nr\u22c4 \u2208R\u22c4\n\n=\n\nX\n\nr\u22c4 \u2208R\u22c4\n\nP (r\u22c4 )|A\u22c4 (r\u22c4 )|2 \u2212 |A \u0304\u22c4 |2 ,\n\n(34)\n\n2\nthat is no longer necessarily equal to the variance \u03c3\u00c2\nof\n2\n2\nEq. (9), but is instead upper bounded by it, \u03c3A\u22c4 \u2264 \u03c3\u00c2 , see\nthe Appendix. In other words, the error \u01ebA\u22c4 (N ) in the approximation of Eq. (32), given by\nr\n2\n\u03c3A\n\u22c4\n,\n(35)\n\u01ebA\u22c4 (N ) \u2248\nN\n\ncan be smaller than the error \u01ebA (N ) of a complete sampling\nscheme.\nB. Algorithm\n\nWe have implemented the incomplete perefect sampling\nscheme in conjunction with the complete perfect sampling\n\nscheme described in section IV. We notice, however, that\nincomplete sampling can also be incorporated into Markov\nchain Monte Carlo.\nAs in section IV, we proceed by constructing a sequence of conditional single-site reduced density matrices {\u03c11 , \u03c12 (r1 ), * * * } and conditional probabilities\n{P (r1 ), P (r2 |r1 ), * * * }.\nHowever, in this occasion the\nsequence concludes at site L\u22c4 , after which we can already\nevaluate the estimator A\u22c4 (r\u22c4 ). This is illustrated for the case\nof a uMPS in Fig. 8, which is to be compared with Fig. 5.\n\nC.\n\nBenchmark\n\nAs in section IV, we use sampling to compute the expectation value of local observables from a uMPS with \u03c7 = 30\nthat has been previously optimized to approximate the ground\nstate of the quantum Ising chain at criticality, Eq. (29). The\nexact structure that we sample can bee seen in Fig. 7. Figure 9 shows the sampling error, as a function of the number of\nx\nz\n|\u03a8i\n|\u03a8i and h\u03a8|\u03c3\u030225\nsamples N , in the computation of h\u03a8|\u03c3\u030225\nin a chain of L = 50 spins. The error is seen to depend on two\nfactors. On the one hand, it depends on which operator (\u03c3\u0302 z\nor \u03c3\u0302 x ) is being measured, as it did in section IV. In addition,\nnow it also drastically depends on which product basis {|r\u22c4 i}\nis used. In particular, we see that a very substantial reduction\nof sampling error, of seven orders of magnitude, is obtained\nz\nby measuring on the x basis while computing h\u03a8|\u03c3\u030225\n|\u03a8i. It\nshould be noted that the two-site Markov chain update scheme\nused for the x-basis calculations,34 although appears competitive, is more computationally demanding than the perfect sampling scheme and runs approximately 2\u20133 times slower.\n\nVI.\n\nCOMPUTATIONAL COSTS\n\nFor completeness, we include a brief summary of the computational costs incurred in extracting, from a given unitary\ntensor network, the expectation value of a local operator by\nusing (i) exact contraction, (ii) Markov chain Monte Carlo\nand (iii) a perfect sampling scheme. For simplicity, we consider only one-site local operators. The scaling of the costs\nin the bond dimension \u03c7 is presented in Table I. We emphasize that in the sampling schemes, we only consider the cost\nof obtaining one sample. A fair comparison of costs with an\nexact contraction should also take into account the number of\nsamples required in order to approximate the exact result with\nsome pre-agreed accuracy.\nThe table shows that for both a uMPS and the MERA, the\ncost of Markov chain Monte Carlo and perfect sampling scale\nwith the same power. Instead, for the uTTN, the of Markov\nchain Monte Carlo is one power smaller than that of perfect\nsampling. [The same would happen with uMPS if the local dimension of each site was also \u03c7]. More significant speed-ups\ncan be seen with the MERA, both for the computation of twopoint correlators, and in systems in two dimensions (not in the\nTable), where sampling techniques to increase computational\n\n\f9\n(a)\n\nTABLE I: The leading-order costs of contracting unitary tensor networks with and without sampling techniques, with the goal of estimating the expectation value of a one-site operator. For the MERA\nwe have also included the cost calculating arbitrary (long-range) twopoint correlators.29\n\ncomplete sampling\n\n0\n\n10\n\npartial sampling, z\u2212\n\n\u2206 \u03c3\u0302 z\n\nbasis\n\n\u22125\n\n10\n\npartial sampling, x\u2212\n\nbasis\n\n0\n\n10\n\n(b)\n\n1\n\n10\n\n0\n\n10\n\ncomp\n\nlete s\n\n\u2206 \u03c3\u0302 x\n\n2\n\n10\nNumber of samples\n\nTensor\nExact\nMarkov- Perfect\nnetwork\ncontraction chain MC sampling\nuMPS (open BC)\nO(\u03c73 )\nO(\u03c72 )\nO(\u03c72 )\n4\n2\nuTTN (binary)\nO(\u03c7 )\nO(\u03c7 )\nO(\u03c73 )\nMERA (1D binary)\nO(\u03c79 )\nO(\u03c75 )\nO(\u03c75 )\n12\n7\n\u2192 2-point correlators O(\u03c7 )\nO(\u03c7 )\nO(\u03c78 )\n\n\u22121\n\nampl\n\nparti\n\n10\n\nal sam\n\npling\n\nparti\n\n, x\u2212b\n\nal sam\n\n0\n\n10\n\nasis\n\npling\n\n\u22122\n\n10\n\n1\n\nthe other hand, although we have focused our analysis on the\nevaluation of local expectation values, more complex tasks involving a uMPS, such as the computation of entanglement entropy, can exploit the perfect sampling schemes presented in\nthis paper at a cost significantly lower than that of an exact\ncontraction (see e.g. Ref. 31).\n\ning\n\n, z\u2212b\n\n10\nNumber of samples\n\nasis\n2\n\n10\n\nFIG. 9: (Color online) Sampling errors with the incomplete perfect\nsampling scheme for a 50 site critical Ising chain, using both perfect\nsampling (continuous lines) and Markov chain Monte Carlo samz\npling (dots). (a) Sampling errors in the computation of h\u03a8|\u03c3\u030225\n|\u03a8i.\nWith perfect sampling, errors in the incomplete perfect sampling\nscheme are upper-bounded by the errors in a complete sampling\nscheme, as proven in the Appendix. Interestingly, for estimates of\nh\u03c3\u0302z i the incomplete perfect sampling scheme obtains an error 10\u22127\ntimes smaller by measuring in the x basis on sites 1, 2, * * * , L\u22c4 . (b)\nx\nSampling errors in the computation of h\u03a8|\u03c3\u030225\n|\u03a8i. Again, the errors\nwith incomplete perfect sampling are smaller than those with complete perfect sampling, and depend on the choice of product basis.\n\nefficiency are required most. The authors present an in-depth\nanalysis of perfect sampling with the MERA in Ref. 29.\nA further remark is in order. The above analysis assumes\nthat a tensor network has been provided in a unitary circuit\nform. In particular, the costs in Table I do not include operations such as converting a non-unitary version of the tensor network into its unitary form (typically through the QRdecomposition). In particular, the cost of QR-decompositions\nrequired to turn an MPS into a uMPS scales as O(\u03c73 ) \u2013 that\nis, the same scaling as an exact contraction. What is then the\npractical interest in a perfect sampling scheme for a uMPS?\nOn the one hand, the uMPS might conceivably have been generated through some procedure (e.g. along the lines of the\nalgebraic Bethe Ansatz MPS constructions described in Ref.\n30), with a cost O(\u03c72 ) (notice that a uMPS tensor only contains O(\u03c72 ) coefficients). In this case, the perfect sampling\nscheme would allow for a very efficient, approximate evaluation of expectation values without increasing this cost. On\n\nVII.\n\nCONCLUSIONS\n\nWe have explained how to perform Monte Carlo sampling\non unitary tensor networks such as the MERA, uMPS and\nuTTN. In order to compute the expectation value h\u03a8|\u00c2|\u03a8i of\na local operator \u00c2, sampling is performed on the past causal\ncone C of operator \u00c2. In addition, by exploiting the unitary character of the tensors, it is possible to directly sample\nconfigurations r of the causal cone according to their weight\nin the wave-function, resulting in uncorrelated samples and\nthus avoiding the equilibration and autocorrelation times of\nMarkov chain Monte Carlo schemes. This last property makes\nthe perfect sampling scheme particularly interesting to study\ncritical systems.\nIn principle, one can also proceed as in Eqs. (21\u201328) for\nnon-unitary tensor networks, e.g. PEPS, and obtain perfect\nsampling. However, in non-unitary tensor networks the cost\nof computing e.g. \u03c11 is already the same as that of computing\nthe expectation value h\u03a8|\u00c2|\u03a8i without sampling. Therefore\nperfect sampling in non-unitary tensor networks seems to be\nof very limited interest.\nHere we have only considered sampling in the context of\ncomputing expectation values. However, the same approach\ncan also be applied in order to optimize the variational ansatz,\nas discussed in full detail in Ref. 29 for the MERA.\nThe authors thank Glen Evenbly for useful discussions.\nSupport from the Australian Research Council (FF0668731,\nDP0878830, DP1092513), the visitor programme at Perimeter Institute, NSERC and FQRNT is acknowledged.\n\n\f10\nAppendix A: Variance with complete and incomplete sampling\n\nGiven a vector |\u03a8i \u2208 VL and a local operator \u00c2, the expectation value of \u00c2 is given by h\u03a8|\u00c2|\u03a8i and its variance is\n\u0011\n\u0010\n2\n(A1)\n\u03c3\u00c2\n\u2261 h\u03a8| |\u00c2 \u2212 h\u03a8|\u00c2|\u03a8i|2 |\u03a8i\n\u0011\n\u0010\n(A2)\n= h\u03a8| |\u00c2|2 |\u03a8i \u2212 |h\u03a8|\u00c2|\u03a8i|2 .\n1.\n\n2. Mean and variance with incomplete sampling\n\nConsider now a new complex random\n(A(s), Q(s)), where A(s) is the estimator\nA(s) \u2261\n\nA(s) \u2261\n\nhs|\u00c2|\u03a8i\nh\u03a8|sihs|\u00c2|\u03a8i\n=\nh\u03a8|sihs|\u03a8i\nhs|\u03a8i\n\n(A3)\n\nand Q(s) is the probability\nQ(s) \u2261 h\u03a8|sihs|\u03a8i.\n\nX\n\n\u0100 \u2261\n\nQ(s)A(s) =\n\ns\n\nX\n\n=\n\ns\n\n(A4)\n\nX\ns\n\nh\u03a8|sihs|\u03a8i\n\nh\u03a8|sihs|\u00c2|\u03a8i\nh\u03a8|sihs|\u03a8i\n\nh\u03a8|sihs|\u00c2|\u03a8i = h\u03a8|\u00c2|\u03a8i.\n\n(A5)\n\n2\nIn turn, its variance \u03c3A\n,\nX\n2\n\u03c3A\n\u2261\nQ(s)|A(s) \u2212 \u0100|2\n\nQ(s) \u2261 h\u03a8|\u03c0(s)|\u03a8i.\n(A10)\nHere {\u03c0(s)} denotes a complete set ofP\nprojectors on the vector\nspace VL , that is \u03c0(s)2 = \u03c0(s),\nP and s \u03c0(s) is a resolution\nof the identity in VL , so that s Q(s) = h\u03a8|\u03a8i = 1. Notice\nthat if all the projectors \u03c0(s) have rank one, then we recover\nthe situation analyzed in the previous subsection. Notice also\nthat this more general setting includes the case addressed in\nSect. V in the context of incomplete sampling.\nThe mean \u0100 is again given by the expectation value\nh\u03a8|\u00c2|\u03a8i,\n\n=\n\nX\ns\n\ns\n\nX\n=\n\n(A7)\n=\n\n4\n\n6\n\ns\n\nh\u03a8|\u03c0(s)|\u03a8i\n\nh\u03a8|\u03c0(s)\u00c2|\u03a8i\nh\u03a8|\u03c0(s)|\u03a8i\n\nh\u03a8|\u03c0(s)\u00c2|\u03a8i = h\u03a8|\u00c2|\u03a8i.\n\n(A11)\n\nQ(s)|A(s)|2\n\nh\u03a8|sihs|\u03a8i\n\nS.R. White, Phys. Rev. Lett. 69, 2863 (1992).\nS.R. White, Phys. Rev. B, 48, 10345 (1993).\nM. Fannes, B. Nachtergaele, and R. F. Werner, Commun. Math.\nPhys. 144, 443 (1992).\nS. Ostlund and S. Rommer, Phys. Rev. Lett. 75, 3537 (1995).\nG. Vidal, Phys. Rev. Lett., 91, 147902 (2003)\nD. Perez-Garcia, F. Verstraete, M. M.Wolf, and J. I. Cirac, Quant.\nInf. Comput. 7, 401 (2007).\n\nX\nh\u03a8|\u00c2\u2020 \u03c0(s)|\u03a8i h\u03a8|\u03c0(s)\u00c2|\u03a8i\nh\u03a8|\u03c0(s)|\u03a8i\nh\u03a8|\u03c0(s)|\u03a8i h\u03a8|\u03c0(s)|\u03a8i\ns\nX h\u03a8|\u00c2\u2020 \u03c0(s)|\u03a8ih\u03a8|\u03c0(s)\u00c2|\u03a8i\n\nh\u03a8|\u03c0(s)|\u03a8i\ns\n\u0011\n\u0010\nX\n\u2264\nh\u03a8|\u00c2\u2020 \u03c0(s)\u00c2|\u03a8i = h\u03a8| |\u00c2|2 |\u03a8i.\n\n(A12)\n\ns\n\n\u2020\n\ns\n\n5\n\nX\n\n2\nHowever, this time the variance \u03c3A\nis only upper bounded by\n2\nthe variance \u03c3\u00c2 of operator \u00c2. This follows from,\n\n(A6)\n\nh\u03a8|\u00c2 |sihs|\u00c2|\u03a8i\nh\u03a8|sihs|\u03a8i\ns\ns\n\u0011\n\u0010\nX\n(A8)\n=\nh\u03a8|\u00c2\u2020 |sihs|\u00c2|\u03a8i = h\u03a8| |\u00c2|2 |\u03a8i.\n\n3\n\ns\n\nX\n\n=\n\n2\nequals the variance \u03c3\u00c2\nof operator \u00c2, as can be seen from\n\nQ(s)|A(s)|2 =\n\nQ(s)A(s) =\n\ns\n\nQ(s)|A(s)|2 \u2212 |\u0100|2 ,\n\nX\n\nX\n\n\u0100 \u2261\n\ns\n\n1\n\n(A9)\n\nand Q(s) is the probability\n\nHere {|si} denotes\nP an orthonormal basis in the vector space\nVL . Notice that\nP s |sihs| is a resolution of the identity in VL\nand therefore s Q(s) = h\u03a8|\u03a8i = 1.\nThe mean \u0100 is given by the expectation value h\u03a8|\u00c2|\u03a8i,\n\n2\n\nh\u03a8|\u03c0(s)\u00c2|\u03a8i\nh\u03a8|\u03c0(s)|\u03a8i\n\nMean and variance with complete sampling\n\nConsider the complex random variable (A(s), P (s)), where\nA(s) is the estimator\n\nX\n\nvariable\n\nHere, the inequality follows from hx|yihy|xi \u2264 hx|xihy|yi\nwith the identifications |xi \u2261 \u03c0(s)\u00c2|\u03a8i and |yi \u2261 \u03c0(s)|\u03a8i.\n\n7\n8\n9\n\n10\n\n11\n\nU. Schollwoeck, Rev. Mod. Phys., 77, 259 (2005).\nU. Schollwoeck, Ann. of Phys. 326, 96 (2011).\nG. Vidal, Phys. Rev. Lett., 99, 220405 (2007); G. Vidal, Phys.\nRev. Lett., 101, 110501 (2008).\nG. Vidal, in Understanding Quantum Phase Transitions, edited\nby L. D. Carr (Taylor & Francis, Boca Raton, 2010),\narXiv:0912.1651v2.\nF. Verstraete, and J. I. Cirac, arXiv:cond-mat/0407066v1 (2004).\n\n\f11\n12\n\n13\n14\n\n15\n\n16\n\n17\n\n18\n19\n20\n\n21\n22\n\n23\n\n24\n25\n26\n\n27\n\n28\n\n29\n30\n\nG. Sierra and M.A. Martin-Delgado, arXiv:cond-mat/9811170v3\n(1998).\nT. Nishino and K. Okunishi, J. Phys. Soc. Jpn., 67, 3066, 1998.\nV. Murg, F. Verstraete, and J. I. Cirac, Phys. Rev. A, 75, 033605\n(2007).\nJ. Jordan, R. Orus, G. Vidal, F. Verstraete, and J. I. Cirac, Phys.\nRev. Lett., 101, 250602 (2008).\nZ.-C. Gu, M. Levin, and X.-G. Wen, Phys. Rev. B, 78, 205116\n(2008).\nH. C. Jiang, Z. Y. Weng, and T. Xiang, Phys. Rev. Lett., 101,\n090603 (2008).\nG. Evenbly and G. Vidal, Phys. Rev. B, 81, 235102 (2010).\nG. Evenbly and G. Vidal, New J. Phys., 12, 025007 (2010).\nL. Cincio, J. Dziarmaga, and M. M. Rams Phys. Rev. Lett., 100,\n240603 (2008).\nG. Evenbly and G. Vidal, Phys. Rev. Lett., 102, 180406 (2009).\nExploitation of space symmetries (e.g. translation invariance and\nscale invariance) can significantly reduce the computational cost\nof simulations from O(L) to O(log L) or even to a constant (independent of L), allowing to reach the thermodynamic limit.\nN. Schuch, M.M. Wolf, F. Verstraete, J.I. Cirac, Phys. Rev. Lett.\n100, 040501 (2008)\nA. W. Sandvik, G. Vidal, Phys. Rev. Lett. 99, 220602 (2007).\nS. R. White, Phys. Rev. Lett. 102, 190601 (2009).\nY. Y. Shi, L.-M. Duan and G. Vidal, Phys. Rev. A, 74, 022320\n(2006).\nL. Tagliacozzo, G. Evenbly, and G. Vidal, Phys. Rev. B 80,\n235127 (2009).\nE. M. Stoudenmire and S. R. White, New J. Phys. 12, 055026\n(2010).\nA. J. Ferris, G. Vidal, e-print arXiv:1201.3975 (2012).\nH. Katsura, I. Maruyama, J. Phys. A: Math. Theor. 43, 175003\n\n31\n32\n\n33\n\n34\n\n35\n\n36\n\n(2010). V. Murg, V. E. Korepin, F. Verstraete, arXiv:1201.5636\n(2012) and arXiv:1201.5627 (2012).\nL. Cincio, G. Vidal, in preparation.\nFrom an MPS (TTN) for a state |\u03a8i, with given bond dimension\n\u03c7, one can always use the gauge freedom in these tensor networks\nto obtain a unitary MPS (respectively TTN) for the same state\n|\u03a8i and with the same bond dimension \u03c7, by writing the tensor\nnetwork in its canonical form5,26 .\nWe have found some tensor networks for which perfect sampling costs one more power of \u03c7 than a Markov chain sweep, see\nRef. 29. Further, for local operators supported on two or more\nsites, perfect sampling may incur a slightly larger cost than that of\na single sweep of Markov chain Monte Carlo. For example, this\noccurs with a TTN, where perfect sampling of a two-site operator\ncosts \u03c74 instead of \u03c73 , but not with MPS or MERA.\nThe Q\ntransverse Ising model contains a Z2 symmetry as the operator i X\u0302i commutes with the Hamiltonian. For a wave-function\nchosen from one of the \u00b11 sectors of this operator, after making a\nprojective measurement of all the spins in the x-basis, one always\ngets an even (odd) number number of spin downs (actually, lefts).\nThe overlap after flipping a single spin of such a configuration is\nalways zero, and thus a two-site Markov chain update scheme that\ncan preserve parity is required.\nOur Hamiltonian is actually defined in terms of bond operators,\nleading to effectively half the magnetic field at the ends of the\nchain. This change does not affect any of the critical properties of\nthe system.\nz\nz\nWe assume that the variance h\u03a8|(\u03c3\u0302L/2\n)2 |\u03a8i \u2212 (h\u03a8|\u03c3\u0302L/2\n|\u03a8i)2 ,\nwhich is upper bounded by 1, is essentially constant as a function\nof the system size L.\n\n\f"}