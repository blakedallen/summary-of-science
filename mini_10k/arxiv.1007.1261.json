{"id": "http://arxiv.org/abs/1007.1261v1", "guidislink": true, "updated": "2010-07-07T23:26:22Z", "updated_parsed": [2010, 7, 7, 23, 26, 22, 2, 188, 0], "published": "2010-07-07T23:26:22Z", "published_parsed": [2010, 7, 7, 23, 26, 22, 2, 188, 0], "title": "MalStone: Towards A Benchmark for Analytics on Large Data Clouds", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.2138%2C1007.4768%2C1007.0223%2C1007.2904%2C1007.0857%2C1007.1831%2C1007.0549%2C1007.3744%2C1007.1882%2C1007.0327%2C1007.0077%2C1007.3554%2C1007.1425%2C1007.1744%2C1007.0371%2C1007.3300%2C1007.3081%2C1007.3562%2C1007.2952%2C1007.3013%2C1007.4357%2C1007.0927%2C1007.2658%2C1007.3171%2C1007.0108%2C1007.0312%2C1007.4438%2C1007.1575%2C1007.0963%2C1007.2057%2C1007.1722%2C1007.1504%2C1007.1261%2C1007.3266%2C1007.2656%2C1007.5494%2C1007.2965%2C1007.2431%2C1007.2301%2C1007.2769%2C1007.3827%2C1007.0958%2C1007.2454%2C1007.4283%2C1007.0911%2C1007.3379%2C1007.3817%2C1007.2102%2C1007.5146%2C1007.5399%2C1007.0437%2C1007.1903%2C1007.1977%2C1007.1542%2C1007.1958%2C1007.4883%2C1007.2344%2C1007.0431%2C1007.5009%2C1007.5483%2C1007.3104%2C1007.0130%2C1007.1418%2C1007.1727%2C1007.0058%2C1007.3034%2C1007.3533%2C1007.3848%2C1007.5410%2C1007.0594%2C1007.4924%2C1007.3178%2C1007.1050%2C1007.0439%2C1007.4179%2C1007.4168%2C1007.2506%2C1007.5282%2C1007.0051%2C1007.4091%2C1007.0859%2C1007.1658%2C1007.0152%2C1007.0634%2C1007.0113%2C1007.4823%2C1007.4644%2C1007.2811%2C1007.0337%2C1007.1861%2C1007.1923%2C1007.2106%2C1007.5377%2C1007.2165%2C1007.5314%2C1007.4100%2C1007.2360%2C1007.1759%2C1007.1210%2C1007.4961%2C1007.2409&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "MalStone: Towards A Benchmark for Analytics on Large Data Clouds"}, "summary": "Developing data mining algorithms that are suitable for cloud computing\nplatforms is currently an active area of research, as is developing cloud\ncomputing platforms appropriate for data mining. Currently, the most common\nbenchmark for cloud computing is the Terasort (and related) benchmarks.\nAlthough the Terasort Benchmark is quite useful, it was not designed for data\nmining per se. In this paper, we introduce a benchmark called MalStone that is\nspecifically designed to measure the performance of cloud computing middleware\nthat supports the type of data intensive computing common when building data\nmining models. We also introduce MalGen, which is a utility for generating data\non clouds that can be used with MalStone.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1007.2138%2C1007.4768%2C1007.0223%2C1007.2904%2C1007.0857%2C1007.1831%2C1007.0549%2C1007.3744%2C1007.1882%2C1007.0327%2C1007.0077%2C1007.3554%2C1007.1425%2C1007.1744%2C1007.0371%2C1007.3300%2C1007.3081%2C1007.3562%2C1007.2952%2C1007.3013%2C1007.4357%2C1007.0927%2C1007.2658%2C1007.3171%2C1007.0108%2C1007.0312%2C1007.4438%2C1007.1575%2C1007.0963%2C1007.2057%2C1007.1722%2C1007.1504%2C1007.1261%2C1007.3266%2C1007.2656%2C1007.5494%2C1007.2965%2C1007.2431%2C1007.2301%2C1007.2769%2C1007.3827%2C1007.0958%2C1007.2454%2C1007.4283%2C1007.0911%2C1007.3379%2C1007.3817%2C1007.2102%2C1007.5146%2C1007.5399%2C1007.0437%2C1007.1903%2C1007.1977%2C1007.1542%2C1007.1958%2C1007.4883%2C1007.2344%2C1007.0431%2C1007.5009%2C1007.5483%2C1007.3104%2C1007.0130%2C1007.1418%2C1007.1727%2C1007.0058%2C1007.3034%2C1007.3533%2C1007.3848%2C1007.5410%2C1007.0594%2C1007.4924%2C1007.3178%2C1007.1050%2C1007.0439%2C1007.4179%2C1007.4168%2C1007.2506%2C1007.5282%2C1007.0051%2C1007.4091%2C1007.0859%2C1007.1658%2C1007.0152%2C1007.0634%2C1007.0113%2C1007.4823%2C1007.4644%2C1007.2811%2C1007.0337%2C1007.1861%2C1007.1923%2C1007.2106%2C1007.5377%2C1007.2165%2C1007.5314%2C1007.4100%2C1007.2360%2C1007.1759%2C1007.1210%2C1007.4961%2C1007.2409&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Developing data mining algorithms that are suitable for cloud computing\nplatforms is currently an active area of research, as is developing cloud\ncomputing platforms appropriate for data mining. Currently, the most common\nbenchmark for cloud computing is the Terasort (and related) benchmarks.\nAlthough the Terasort Benchmark is quite useful, it was not designed for data\nmining per se. In this paper, we introduce a benchmark called MalStone that is\nspecifically designed to measure the performance of cloud computing middleware\nthat supports the type of data intensive computing common when building data\nmining models. We also introduce MalGen, which is a utility for generating data\non clouds that can be used with MalStone."}, "authors": ["Collin Bennett", "Robert L. Grossman", "David Locke", "Jonathan Seidman", "Steve Vejcik"], "author_detail": {"name": "Steve Vejcik"}, "author": "Steve Vejcik", "links": [{"href": "http://arxiv.org/abs/1007.1261v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1007.1261v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.DC", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1007.1261v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1007.1261v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "MalStone: Towards A Benchmark for\nAnalytics on Large Data Clouds\nCollin Bennett\n\narXiv:1007.1261v1 [cs.DC] 7 Jul 2010\n\nOpen Data Group\n400 Lathrop Ave Suite 90\nRiver Forest IL 60305\n\nOpen Data Group\n400 Lathrop Ave Suite 90\nRiver Forest IL 60305\n\nDavid Locke\n\nJonathan Seidman\n\nSteve Vejcik\n\nOpen Data Group\n400 Lathrop Ave Suite 90\nRiver Forest IL 60305\n\nOpen Data Group\n400 Lathrop Ave Suite 90\nRiver Forest IL 60305\n\nOpen Data Group\n400 Lathrop Ave Suite 90\nRiver Forest IL 60305\n\nABSTRACT\nDeveloping data mining algorithms that are suitable for cloud\ncomputing platforms is currently an active area of research,\nas is developing cloud computing platforms appropriate for\ndata mining. Currently, the most common benchmark for\ncloud computing is the Terasort (and related) benchmarks.\nAlthough the Terasort Benchmark is quite useful, it was not\ndesigned for data mining per se. In this paper, we introduce\na benchmark called MalStone that is specifically designed\nto measure the performance of cloud computing middleware\nthat supports the type of data intensive computing common\nwhen building data mining models. We also introduce MalGen, which is a utility for generating data on clouds that\ncan be used with MalStone.\n\n1.\n\n\u2217\n\nRobert L. Grossman\n\nINTRODUCTION\n\nClouds based on the Hadoop system and associated Apache\nsystems, such as Hbase, Apache Pig, Hive and ZooKeeper,\nhave proved effective for processing large scale data for data\nmining and related applications over racks of commodity\ncomputers [11]. This type of architecture is sometimes called\na large data cloud [9], and was popularized in a series of\nGoogle technical reports that described the Google File System (GFS) [7], MapReduce [5], and BigTable [3]. In a large\ndata cloud, the data is stored over many loosely coupled distributed disks, such as you would find in racks of commodity\ncomputers. A common architecture is for the computers in\neach rack to communicate using a switch located at the top\nof each rack and for different racks to communicate using a\nlarger switch that connects racks within a data center.\n\u2217This author is the corresponding author. He is also a faculty member at the University of Illinois at Chicago.\n\nIt is an important requirement now for many industry and\ngovernment applications to evaluate the applicability and\nscalability of different large data cloud architectures and\nsystems. This can be difficult without standardized architectures and benchmarks. In this paper, we take a first step\ntowards a benchmark that is designed to measure in part the\nability of a large data cloud system to prepare data for data\nmining and to build statistical and data mining models.\nAs motivation, think of the role that the TPC Benchmarks\nhave played in understanding performance differences between different databases and transaction processing systems. Currently, there are no similar benchmarks for comparing two large data clouds that support building analytic\nmodels on large datasets. In this paper, we take a first in\nthis direction by introducing a benchmark called MalStone.\nWe also describe the implementation of a data generator for\nMalStone called MalGen as well as several experimental\nstudies using MalStone that compare three different large\ndata cloud middleware stacks.\nAlthough using clouds for data mining and data intensive\ncomputing is an area of active research [6], [4], there is no\nbenchmark that we are aware of for understanding the impact of different cloud middleware on the performance of a\nparticular algorithm. MalStone is a first step in this direction.\nAnother way to view MalStone is as a stylized analytic computation of a type that is common in data intensive computing. MalStone computes a ratio in moving window over\naggregated data. We call MalStone stylized since it is typical of the type of derived attributes or features that are\ncomputed as part of the modeling process. For data small\nenough to fit in memory, in a disk, or in a network attached\nstorage system, it is straightforward to compute the MalStone statistic. On the other hand, if the log data is so\nlarge that it requires large numbers of disks to manage it, as\nis the case in a large data cloud, then computing something\nas simple as this ratio can be computationally challenging.\nFor example, if the data spans 100 disks, then the computation cannot be done easily with any of the databases that\n\n\fare common today. On the other hand, if the data fits into\na database, then this statistic can be computed easily using\na few lines of SQL.\nThe open source MalGen code to generate data for MalStone\nand a technical report describing some illustrative implementations of MalStone is available from\nmalgen.googlecode.com.\nMalStone was designed to give some insight into different\nlarge data cloud systems. It was not designed to compare\na large data cloud system (in which data is typically stored\nover multiple distributed disks) to a traditional database,\ndatabases and systems that utilize proprietary hardware, or\nhybrid database/large data cloud systems. Other benchmarks must be designed for this purpose. For example,\nMalStone does not measure the efficiency of joins and does\nnot take into account the cost by TB of data managed, and\nsimilar considerations, all of which are important when comparing these different types of systems.\nThis paper is organized as follows: in Section 2 motivates\nthe MalStone benchmark. Section 3 describes the abstract\nmodel motivating the statistic behind the MalStone benchmark. Section 4 describes the benchmark. Section 5 describes MalGen. Section 6 describes three illustrative implementations of MalStoneand shows the sometimes quite significant differences that can arise with different large data\ncloud middleware stacks. Section 7 contains some experimental studies. Section 8 contains some discussion. Section 9 describes related work. Section 10 is the summary\nand conclusion.\nThis emerging application and technology paper paper makes\nthe following contributions:\n1. There is currently no benchmark that we are aware\nfor measuring the performance of cloud middleware designed to support building data mining models on large\ndatasets. MalStone is such a benchmark. MalStone is\ndefined in Section 4 and is useful for quantifying differences in system architectures and in quantifying their\nscalability. See Tables 4 and 5 for some examples.\n2. There are currently very few data generators that we\nare aware for generating data records that can be used\nfor testing data mining algorithms designed for cloud\ncomputing platforms. MalGen is such a data generator. MalGen is described in Section 5.\n3. Through three experimental studies using MalStone,\nwe have shown that are substantial differences (\u2248 20\u00d7)\nbetween different cloud computing platforms designed\nto support building data mining models on very large\ndatasets. This is discussed in Section 7.\n4. The abstraction described in Section 3 covers a number of interesting examples as summarized in Table 1.\nViewing these types of problems from this point of view\nhas not received a lot attention in the data mining literature to date, but represents an interesting class of\n\nproblems that occur not infrequently. As the implementations demonstrate, the type of log files that these\ntypes of problems produce can be analyzed easily using\nMapReduce style parallel programming frameworks.\n\n2.\n\nMOTIVATING EXAMPLE\n\nWe introduce MalStone with a simple motivating example.\nConsider visitors to web sites. As described in the paper\n\"The Ghost in the Browser\" by Provos et. al. [13], approximately 10% of web pages have exploits installed that\ncan infect certain computers when users visit the web pages.\nSometimes these are called \"drive-by exploits.\"\nThe MalStone benchmark assumes that there are log files\nthat record the date and time that users visited web pages.\nAssume that the log files of visits have the following fields:\nTimestamp | Web Site ID | User ID\nThere is a further assumption that if the computers become\ninfected, at perhaps a later time, then this is known. That\nis for each computer, which we assume is identified by the\nID of the corresponding user, it is known whether at some\nlater time that computer has become compromised:\nUser ID | Compromise Flag\nHere the Compromise field is a flag, with 1 denoting a compromise. A very simple statistic that provides some insight\ninto whether a web page is a possible source of compromises\nis to compute for each web site the ratio of visits in which\nthe computer subsequently becomes compromised to those\nin which the computer remains uncompromised. This statistic is defined in Section 3. Also, see Figure 2.\nWe call MalStone stylized since we do not argue that this\nis a useful or effective algorithm for finding compromised\nsites. Rather, we point out that if the log data is so large\nthat it requires large numbers of disks to manage it, then\ncomputing something as simple as this ratio can be computationally challenging. For example, if the data spans\n100 disks, then the computation cannot be done easily with\nany of the databases that are common today. On the other\nhand, if the data fits into a database, then this statistic can\nbe computed easily using a few lines of SQL.\nWe abstract this problem by abstracting web sites by sites,\nusers by entities, and visits by events. When entity visits\na marked site, it may become marked at some time in the\nfuture. With this generalization, we assume that we have\nlog files containing events records describing an event associated with an entity and a site and that some of these sites\nmark some of the entities that are associated with them. We\nassume that not all entities become marked and that there\nmay be a time delay in the marking.\n\n3.\n\nSITES, ENTITIES & MARKS\n\nIn this section, we abstract and formalize the example described in the previous section.\n\n\fExample\ndrive-by exploits\n\nSite\nweb site\n\ncompromised login service\n\ncomputer providing\ncompromised service\n\nthe\n\nEntity\ncomputer identified by IP\nwith browser\nuser providing credentials\n\nTable 1: Some examples of scenarios producing site-entity logs. The problem of interest in these examples is\nto identify the site or sites that are the source of the marks assuming that we know which entities are marked.\nThe problem is difficult since: i) the site-entity log files may be very large; ii) there may be a background\nprocess that marks some entities independent of the marked sites; and iii) not all entities that visit a marked\nsite become marked.\nWe define the SPM statistic as follows:\n\n1. Fix an exposure window ExpW and a monitor window\nMonW. See Figure 1.\n2. Fix a site sj .\n3. Let Aj be the set of all entities ei that: i) transact\nat site sj at any time during the exposure window\nExpW; and ii), in the case that the entity is marked,\nthe transaction occurs before the entity is marked.\n\nFigure 1: To define the SPM \u03c1j statistic requires fixing an exposure window and a monitor window. To\ndefine the SPM \u03c1j,t statistic requires fixing an exposure window and a sequence of increasing monitor\nwindows.\n\n3.1\n\n4. Let Bj be the set of all entities ei \u2208 Aj that become\nmarked at any time in the monitor window MonW.\n\nDefinition 1. Define the subsequent proportion of marks\n\u03c1j by:\n\u03c1j =\n\nThe Model\n\nThe model we use contains abstract sites and abstract entities. There are two types of activities: entities can visit sites\nand sites can mark entities. There is a log file that records\neach type of activity. The first type of log file records the\ntimes at which entities visit sites. The second type of log file\nrecords the times at which entities become marked. More\nprecisely, some of the entities that visit sites became marked\nat some time in the future after the visit. The second type\nof log file records the times at which this happens.\nWe use the following notation:\n\u2022 We usually let e denote an entity and let s denote a\nsite; we also use ei for an entity and sj for a site.\n\u2022 A, B refer to sets of entities, Aj , Bj refer to sets of\nentities that depend upon a site sj .\n\u2022 S refers to a set of sites and Si refers to a set of sites\nassociated with an entity ei .\n\nWe close this section with some remarks:\n\u2022 Note that Bj \u2286 Aj .\n\u2022 It is important to note that Aj depends upon the exposure window, and Bj depends upon the monitor window, and through the relation Bj \u2286 Aj upon the exposure window.\n\u2022 Note that the MonW may: 1) start after the ExpW, 2)\nbefore the ExpW, or 3) include the entire data available.\n\u2022 As an example, in Figure 2, for time dk , there are no\nevents for site sj , but for the window starting at time\ndk , extending backward to time dk\u22122 (time zero in this\nexample), the statistic is (1 + 0 + 0)/(1 + 1 + 0) = 12 .\n\n3.3\n3.2\n\nSPM for Fixed Windows\n\nIn this section, we define a statistic associated with sites, entities, and marks called the subsequent proportion of marks\nor SPM. We first define SPM-based scores for a fixed window. In the next subsection, we will consider moving windows.\n\n|Bj |\n.\n|Aj |\n\nSPM for Moving Windows\n\nIn general, we have a sequence of monitor windows\nMonWt1 , MonWt2 , MonWt3 , . . .\ndepending upon time t. For example, the sequence may all\nhave a common start time, but the end time increases by a\nweek for each monitor window in the sequence. See Figure 1.\n\n\fBenchmark\nMalStone A-10\nMalStone A-100\nMalStone A-1000\nMalStone B-10\nMalStone B-100\nMalStone B-1000\n\nStatistic\n\u03c1j\n\u03c1j\n\u03c1j\n\u03c1j,t\n\u03c1j,t\n\u03c1j,t\n\n# records\n10 billion\n100 billion\n1 trillion\n10 billion\n100 billion\n1 trillion\n\nData Size\n1 TB\n10 TB\n100 TB\n1 TB\n10 TB\n100 TB\n\nTable 2: The MalStone benchmarks use 100 byte\nrecords, with a fixed field width.\n\n5.\n\nMalGen\n\nAs mentioned above, we have developed an open source program called MalGen for generating site-entity log files for all\nthe nodes in a cluster. MalGen uses a power law distribution to model the number of entities associated with a site.\nMost sites have few entities associated with them, while a\nfew sites have a large number of entities. This is the case, for\nexample, with web sites: most sites have only a few visitors,\na few sites have a lot of visitors, and a power law distribution\nis often used to model this distribution.\nMalGen are 100 bytes in size with five fixed width fields:\n\nUsing this sequence of monitor windows, we can define a\nsequence\n\u03c1j,t\n\n|Bj,t |\n=\n,\n|Aj |\n\nwhere Bj,t is the set of entities ej \u2208 Aj that become marked\nat any time during the monitor window MonWt .\n\n4.\n\nMalStone A & B\n\nIn this section, we define the MalStone A and B Benchmarks.\nAssume that we have a collection of log files. For simplicity,\nwe assume that the log files that describe visits of entities\nto sites has been joined to the log file that describes which\nentities are marked (and when). With this assumption, log\nfiles contain the following fields:\nEvent ID | Timestamp | Site ID |\nEntity ID | Mark Flag |\nWe interpret these as recording the fact that at the time\nindicted by the timestamp, the entity with the entity ID\nvisited the site with the Site ID. The Mark Flag indicates\nwhether at the time of visit the entity was marked.\nRemark. It is important to note that if the Mark Flag is 1\nindicating the entity is marked, we do not necessarily know\nthat the site identified by the Site ID marked the entity.\nInstead, all that we know is that either the site, or any site\nthat the entity has visited in the past during the exposure\nwindow has marked the entity.\nIt is for this reason, that the statistic is called the subsequent\nproportion of marks.\nMalStone A computes \u03c1j for all sites j in the log files. MalStone B computes \u03c1j,t for sites j in the log files and for a\nsequence of moving windows that begin at time t0 and end\nat time t equal to:\nt1 < t 2 < t 3 < . . .\nMalStone records are 100 byte records, with a fixed width\nfields. Both MalStone A and B use 1 year's worth of data.\nMalStone A uses a single window for the entire year, while\nMalStone B uses a window that begins at the beginning of\nthe year and ends at week 1, week 2, . . ., week 52.\n\nEvent ID | Timestamp | Site ID |\nEntity ID | Mark\nThe following is a description of each field:\n\u2022 Event ID - The Event ID consists of an ID for each\nrecord that is sequential and unique when restricted\nto a single node followed by a hash of the hostname to\ncreate a globally unique Event ID.\n\u2022 Timestamp - The date and time of the event. This\nis a uniformly distributed random value over a userspecified number of days. The default is to generate\ndata distributed over a period of one year.\n\u2022 Site ID - This is the ID of the site associated with\nthe event.\n\u2022 Entity ID \u2013 This is the ID of the entity associated with\nthe event.\n\u2022 Mark - The field is either 0 or 1 and indicates whether\nthe entity is marked at the time. Note that, as discussed above, the fact that the mark is 1 does not indicate that the site with Site ID is responsible for the\nmark, simply that at some time prior to the Timestamp. the entity visited some marked site.\nSite-entity log files are generated by MalGen in several steps.\nSeveral steps are used since the MalStone SPMstatistic requires aggregating data from different distributed nodes and\ncomputing a statistic that satisfies both certain statistical\nproperties and certain consistency requirements. The first\nstep generates certain seed information about the marked\nsites and scatters this information to all the nodes in the\nlarge data cloud. The algorithm is designed to keep certain\ninformation required for the first step in memory in order\nto improve the overall speed of MalGen . The subsequent\nsteps are done independently by each of the nodes. We report on the memory utilization of the first step of MalGen\nbelow since keeping the memory utilization relatively low is\nimportant so that enough seed information is available for\ngenerating the 10 billion, 100 billion and 1 trillion records\nthat MalGen requires.\nIn the first step, MalGen generates events associated with\nmarked sites. For each marked site, a random date is generated. For a particular site, the number of events is randomly\ngenerated using a power law distribution and a set of entity\n\n\fFigure 2: The diagram shows an example of how the SPM statistic is computed. Here sites sj are represented\nby small rectangles and marked sites are represented by shaded rectangles. Specifically, for each site sj at\ntime tk , MalStone B collects all the transactions (represented by arrows) that are associated with the site\nat time tk or earlier. Notice there are no transactions associated with sj at time tk , but that there are two\ntransactions associated with the site at earlier times tk\u22121 and tk\u22122 . Entity e2 was associated with the site\nat tk\u22122 and entity e1 at time tk\u22121 . Entity e1 became marked at the site sj at time sk\u22121 (represented by red\nentity arrow with an \"X\"). Therefore 21 of the transactions are marked for site sj with respect to the window\n(tk\u22122 , tk\u22121 , tk ).\nIDs is randomly generated from the pool of available entity\nIDs. The power law distribution is constructed so that most\nsites are associated with a relatively few number of entity\nIDs (a few hundred a day), but with a long tail so that\nthere are a small number of sites with a very large number\nof events. The Entity IDs are sampled until the number of\nevents for each site is complete.\nFor the marked sites, a visit by an entity subjects the entity\nto a probability (e.g. 70%) of being marked. If an entity is\nmarked, it is tagged as being such with a timestamp that\noccurs after a delay period (e.g. one week) If an entity is\nalready marked when it visits a marked site again, it is subject to mark if the date of the current event precedes that\nof the event that marked it. In this case, the date-time of\nthe mark is updated accordingly.\nThe initial seeding and the generation of marked entities\nis done on a single node. This information is then pushed\nout to all the nodes in the large data cloud and each local\nnode then generates records for entities that are not marked.\nTable 3 shows the times required to generate 2 billion, 6\nbillion and 10 billion events in this way on a 20 node cloud.\nThe time required for seeding the process is in the table\nbelow:\nAfter all event histories for the requested number of marked\nsites are complete, subsequent sites are assumed to be unmarked with no possibility of being marked. This is the third\nstep, which uses the same process that was just described to\nconstruct visits for non-marked sites.\n\nRecords/node\n100 M\n300 M\n500 M\nRecords/node\n100 M\n300 M\n500 M\n\nRAM\n16 GB\n16 GB\n16 GB\n\nTotal Records\n2B\n6B\n10 B\n\nTime\n60 min\n142 min\n190 min\nRAM\n4 GB\n4 GB\n4 GB\n\nTime\n54 min\n157 min\n275 min\n\nTable 3: The first table shows the time required\nin minutes for MalGen to seed the data generation\nand to generate the marked entities. This was run\non a head node with 16 GB of memory. The second\ntable shows the time required to copy the required\ndata from the head node to each of 20 local nodes\nand for the local nodes to generate all the required\nunmarked events. For example, the time required to\ngenerate 10 billion events distributed over 20 nodes\nis 190 + 275 = 465 minutes.\n\n\f\u2022 Mapper - Reads the records and groups them using\nthe Site ID as the key. The corresponding value is the\ntimestamp and the mark flag.\n\u2022 Reducer - For each key, the Reducer tracks the total\nnumber of events seen with the mark flag equal to one\nand the total number of events and stores them by the\ndate. All saved values are then processed in order by\nthe date.\n\u2022 Partitioner - The Site Id is taken modulo the number\nof reducers.\n\nFigure 3: The memory usage of MalGen as it generated approximately 500,000,000 log records for visits to approximately 120,000 different sites, where\nthe number of visits to a web site follows a power\nlaw. Approximately 8.5 GB of an available 12 GB of\nmemory were used during the 30 minute generation\nof data. The bottom line shows the memory used,\nwhile the top line shows the available memory.\n\nWe close this section with two remarks about MalGen.\nMalGen is designed to generate large datasets that span all\nthe nodes in a cluster. To create consistent data in parallel\nover all the nodes in the cluster, MalGen: i) generates the\ndata describing all marked sites on one machine in the cluster; ii) scatters the information to all the other nodes in the\ncluster; iii) all the other nodes in the cluster then generate\nthe data for all the unmarked sites.\nBy keeping information about sites in memory (vs on disk),\nMalGen can improve its performance. The default parameters for MalGen can generate approximately 500,000,000\nevents for approximately 120,000 sites in about 30 minutes\nusing a Dell 1435 2.0GHz dual-core AMD Opteron 2212 processor and 16 GB of memory. Figure 3 contains a graph\nshowing MalGen's memory usage.\n\n6.\n\nTHREE IMPLEMENTATIONS\n\nEach record is parsed and then transformed into a key-value\npair. The key is the Site Id. The value is the Flag and the\nbucket the time stamp is put in. The time stamp in each\nrecord can be bucketed arbitrarily. MalStone B requires that\nthe statistic be computed for each week; we used the ISO\nweek number (www.iso.org) for convenience.\nThe operation performed on each group of data is to count\nthe number of events and the number of events with the\nmark equal to one each time t. For each site s, this is stored\nin a Java Collections Map using t as the key.\nWhen all records for a site id are processed, the stored values are accessed in chronological order and running totals\ncomputed.\nThe output is the Site ID (key) j and a list of the times t\nand associated SPM statistics \u03c1j,t\n\n6.2\n\nHDFS, Hadoop Streams and Python\n\nThe second implementation used Hadoop Streams [11] and\nPython. The mapper method reads the records from Standard Input and sends the mapped data to Standard Output.\nThe same key and value structure as described in Section 6.1\nis used.\nThe reducer reads the mapped data from Standard Input\nand for each Site ID, stores the aggregated number of events\nseen and those seen with the mark equal to 1 in a Python\ndictionary keyed by the time t.\n\nWe have implemented the MalStone A and B benchmarks\nin three different ways:\n\nWhen all the records for a site id are processed, the stored\nvalues are accessed in chronological order and running totals\ncomputed.\n\n1. using the Hadoop Distributed File System (HDFS) [2]\nand Hadoop's implementation of MapReduce [11];\n\nThe output is the Site ID (key) and a list of the times t's and\nthe associated SPMstatistic \u03c1j,t is sent to Standard Output\n(value).\n\n2. using HDFS, Hadoop Streams [11] and coding MalStone A and B in Python; and\n3. 3) using the Sector Distributed File System and Sphere\nUser Defined Functions [10].\n\n6.1\n\nHadoop HDFS and MapReduce\n\nWe used MalGen to generate data which we stored in HDFS\nand then implemented MalStone B using a Mapper, Reducer\nand Partitioner as follows:\n\n6.3\n\nSector and Sphere UDFs\n\nSector provides two methods for implementing processing\n[10]:\n\n\u2022 Using indexed data - when using indexed data each\ninput data file has an accompanying index file containing the offsets of each record in the data file. This index\nallows Sector to segment the data during processing.\n\n\f\u2022 Using non-indexed data - when using non-indexed\ndata, the processing code must manually segment the\ninput the input data during processing.\nUsing non-indexed data requires somewhat more code to\nimplement, but seems to improve processing time. MalStone\nB was implemented using non-indexed data.\nThe MalStone B code was implemented in two stages:\n\u2022 In the first stage, each record in the input data is read,\nassigned to a bucket based upon the site ID, and each\nbucket file is written to disk. After this stage is completed, all records for a particular site will be in a single\nfile.\n\u2022 In the second stage, for each site, for each site j the\ncardinality of the sets Aj and Bj is computed. After\nall the records for a site j have been processed, the\nresulting record is saved to a file.\n\n8.\n\nDISCUSSION\n\nOne of the pleasant surprises is the power of Hadoop streams,\nwhich does not require the MapReduce framework. Hadoop\nis now a relatively mature distributed file system that can\nscale to over a thousand nodes and manage petabytes of\ndata. As Tables 4 and 5 show, Python programs can be\ninvoked by Hadoop streams and be used to efficiently process large data sets without the MapReduce framework and\nthis approach can be faster when computing certain statistics (such as \u03c1j,t ) than performing the computation using\nMapReduce. We stress that this is a positive outcome and\nsimply shows (as is obvious in hindsight) that certain statistical qualities can be computed more efficiently directly\nwith Python over the data managed by the HDFS than by\nusing MapReduce and the HDFS.\nAnother pleasant surprise is that once we abstracted the\nMalStone statistic as the Site-Entity-Mark Model, we found\nthat other applications could also be modeled in this way,\nas Table 1 shows.\n\nThe experimental studies used a rack of 30 Dell 1435 computers. Each computer had 12 GB memory, 1TB disk, and a\n2.0GHz dual dual-core AMD Opteron 2212. Each computer\nhad a 1 Gb/s network interface cards and were networked\ntogether with a Cisco 3750E switch.\n\nIn practice, once the MalStone SPM statistic is computed,\nrelatively effective statistical models can be computed by\nlooking for changes over time t in the \u03c1j,t statistic using\nCUSUM, GLR and related change detection models [12].\nAlthough outside the scope of this paper, if segmented models are used for each site j, the Reducer in MapReduce can\nbe used to organize the computation so that each node in\na large data cloud contains all the data required to build a\nchange detection model for a site j [1].\n\n7.2\n\n9.\n\n7. EXPERIMENTAL STUDIES\n7.1 Testbed\n\nMalStone Benchmarks\n\nFor the experimental studies reported below, we used 20\nnodes. Each node was populated with 500 million records\nusing MalGen for a total of 10 billion records. Each record\nwas 100 bytes for a total of 1 TB of data. The results are\nreported in Tables 4 and 5.\nNote that as measured by these benchmarks, storing the\ndata using HDFS and implementing the benchmark using\nHadoop streams and Python was substantially faster than\nusing HDFS and Hadoop's MapReduce.\nNote also that managing the data using Sector and implementing the benchmark using Spheres UDFs was about 2.5\ntimes faster than the Hadoop streams implementation.\n\nRELATED WORK\n\nThe CloudStone Benchmark [14] is a first step towards a\nbenchmark for clouds designed to support Web 2.0 type applications. In this note, we describe the MalStone Benchmark, which is a first step towards a benchmark for clouds,\nsuch as Hadoop and Sector, designed to support data intensive computing.\nOne of the motivations for choosing 10 billion 100-byte records\nis that the TeraSort Benchmark [8] (sometimes called the\nTerabyte Sort Benchmark) also uses 10 billion 100-byte records.\nWe note that in 2008, Hadoop became the first open source\nprogram to hold the record for the TeraSort Benchmark.\nIt was able to sort 1 TB of data using using 910 nodes in\n209 seconds, breaking the previous record of 297 seconds.\nHadoop set a new record in 2009 by sorting 100 TB of data\nat 0.578 TB/minute using 3800 nodes.\nThe TeraSort Benchmark is now deprecated and has been\nreplaced by the Minute Sort Benchmark. Currently, 1 TB of\ndata can be sorted in about a minute given the right software\nand sufficient hardware.\nThe paper by Provos et. al. [13] describes a system for detecting drive-by malware that uses MapReduce. Specifically,\nMapReduce is used to extract links from a large collection\nof crawled web pages. These links are then analyzed using\nheuristics to identify a relatively small number of suspect\nweb sites. These suspect web sites are then tested using Internet Explorer to retrieve web pages in a virtual machine\nthat is instrumented. This allows those web sites resulting\n\n\fRun 1\nRun 2\nRun 3\nAverage\n\nHadoop HDFS\nwith Streams &\nPython\n82m 21s\n90m 31s\n89m 35s\n87m 29s\n\nHadoop HDFS\nwith\nMapReduce\n458m 7s\n450m 21s\n454m 12s\n454m 13s\n\nSector\nwith\nSphere UDFs\n33m\n33m\n33m\n33m\n\n44s\n26s\n51s\n40s\n\nTable 4: This table summarizes an experimental study running MalStone A on 20 nodes. Each node had 500\nmillion 100-byte MalStone records. The tests used version 0.18.3 of Hadoop and version 1.20 of Sector.\n\nRun 1\nRun 2\nRun 3\nAverage\n\nHadoop HDFS\nwith Streams &\nPython\n144m 10s\n146m 23s\n137m 4s\n142m 32s\n\nHadoop HDFS\nwith\nMapReduce\n799m 0s\n861m 40s\n861m 51s\n840m 50s\n\nSector\nwith\nSphere UDFs\n43m\n43m\n43m\n43m\n\n57s\n52s\n24s\n44s\n\nTable 5: This table summarizes running MalStone B on 20 nodes. Each node had 500 million 100-byte\nMalStone records. The tests used version 0.18.3 of Hadoop and version 1.20 of Sector.\nin drive-by infections to be directly monitored. In contrast,\nthe work described in this paper is quite different. The work\nhere uses Hadoop and MapReduce to compute the MalStone\nstatistic from a collection of log files generated by MalGen\nin one of the illustrative implementations of MalStone.\nThe paper [4] describes how several standard data mining\nalgorithms can be implemented using MapReduce, but this\npaper does not describe a computation similar to the MalStone statistic.\n\n10.\n\nSUMMARY\n\nIn this paper, we have introduced a benchmark called MalStone for measuring the performance of cloud middleware\ndesigned for data mining and data intensive computing. Currently, gaining access to large amounts of nonproprietary\ndata to use for benchmarking cloud middleware can be challenging. For this reason, we have developed an application\ncalled MalGen that is designed to generate synthetic logentity files that can be used by MalStone. We have used\nMalGen to generate tens of billions of events on clouds with\nover 100 nodes.\nThe MalStone benchmark computes a statistic that is a stylized analytic on log files consisting of records of visits by entities to sites. Sometimes, after these visits, entities become\nmarked at some time in the future. Note that this analytic\nis related to identifying the sites that are the sources of the\nmarks, not the marked entities themselves.\nAs Tables 4 and 5 show, there can be substantial differences\nin performance, depending upon which cloud middleware is\nused to compute the MalStone statistic.\n\n11.\n\nAVAILABILITY\n\nMalGen is open source and available from:\nmalgen.googlecode.com.\n\nThe current version of MalGen is 0.9.\n\n12.\n\nREFERENCES\n\n[1] C. Bennett, D. Locke, R. L. Grossman, and S. Vejcik.\nSawmill: Building segmented models in large data\nclouds. to appear, 2010.\n[2] D. Borthaku. The Hadoop distributed file system:\nArchitecture and design. retrieved from\nlucene.apache.org/hadoop, 2007.\n[3] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A.\nWallach, M. Burrows, T. Chandra, A. Fikes, and R. E.\nGruber. Bigtable: A distributed storage system for\nstructured data. In OSDI'06: Seventh Symposium on\nOperating System Design and Implementation, 2006.\n[4] C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. Bradski,\nA. Y. Ng, and K. Olukotun. Map-Reduce for machine\nlearning on multicore. In NIPS, volume 19, 2007.\n[5] J. Dean and S. Ghemawat. MapReduce: Simplified\ndata processing on large clusters. In OSDI'04: Sixth\nSymposium on Operating System Design and\nImplementation, 2004.\n[6] J. Dean and S. Ghemawat. MapReduce: Simplified\ndata processing on large clusters. Communications of\nthe ACM, 51(1):107\u2013113, 2008.\n\n\f[7] S. Ghemawat, H. Gobioff, and S.-T. Leung. The\nGoogle file system. In SOSP '03: Proceedings of the\nnineteenth ACM symposium on Operating systems\nprinciples, pages 29\u201343, New York, NY, USA, 2003.\nACM.\n[8] J. Gray. Sort benchmark home page.\nhttp://research.microsoft.com/barc/SortBenchmark/,\n2008.\n[9] R. L. Grossman and Y. Gu. On the varieties of clouds\nfor data intensive computing. Bulletin of the Technical\nCommittee on Data Engineering, 32(1):44\u201350, March\n2009.\n[10] Y. Gu and R. L. Grossman. Sector and sphere:\nTowards simplified storage and processing of large\nscale distributed data. Philosophical Transactions of\nthe Royal Society A, also arXiv:0809.1181, 2009.\n[11] Hadoop Wiki. Apache Hadoop. retrieved from\nhttp://wiki.apache.org/hadoop/, 2010.\n[12] H. V. Poor and O. Hadjiliadis. Quickest Detection.\nCambridge University Press, 2008.\n[13] N. Provos, D. McNamee, P. Mavrommatis, K. Wang,\nand N. Modadugu. The ghost in the browser: Analysis\nof web-based malware. In HotBot '07, 2007.\n[14] W. Sobel, S. Subramanyam, A. Sucharitakul,\nJ. Nguyen, H. Wong, A. Klepchukov, S. Patil, A. Fox,\nand D. Patterson. Cloudstone: Multi-platform,\nmulti-language benchmark and measurement tools for\nweb 2.0. In Proceedings of Cloud Computing and its\nApplications 2008, 2008.\n\n\f"}