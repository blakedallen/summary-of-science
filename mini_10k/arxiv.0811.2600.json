{"id": "http://arxiv.org/abs/0811.2600v2", "guidislink": true, "updated": "2009-03-23T17:56:38Z", "updated_parsed": [2009, 3, 23, 17, 56, 38, 0, 82, 0], "published": "2008-11-16T20:07:10Z", "published_parsed": [2008, 11, 16, 20, 7, 10, 6, 321, 0], "title": "A Unified Framework for Photometric Redshifts", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0811.4004%2C0811.1173%2C0811.2623%2C0811.0878%2C0811.3350%2C0811.3078%2C0811.3448%2C0811.4460%2C0811.3330%2C0811.4752%2C0811.2790%2C0811.2377%2C0811.0961%2C0811.0233%2C0811.0403%2C0811.0384%2C0811.2136%2C0811.1163%2C0811.3534%2C0811.3500%2C0811.3432%2C0811.1906%2C0811.0692%2C0811.2617%2C0811.2600%2C0811.0795%2C0811.3450%2C0811.3127%2C0811.0722%2C0811.4287%2C0811.1903%2C0811.1840%2C0811.1962%2C0811.2870%2C0811.1878%2C0811.2954%2C0811.3020%2C0811.2731%2C0811.2615%2C0811.0508%2C0811.0066%2C0811.4049%2C0811.1951%2C0811.4470%2C0811.3272%2C0811.0228%2C0811.3611%2C0811.1477%2C0811.3422%2C0811.3929%2C0811.3775%2C0811.3291%2C0811.0993%2C0811.0349%2C0811.1457%2C0811.2821%2C0811.1411%2C0811.4367%2C0811.2576%2C0811.3879%2C0811.1017%2C0811.1270%2C0811.0829%2C0811.0928%2C0811.3670%2C0811.4361%2C0811.3273%2C0811.3583%2C0811.3935%2C0811.3871%2C0811.2895%2C0811.1987%2C0811.2560%2C0811.1581%2C0811.1065%2C0811.2285%2C0811.1828%2C0811.0212%2C0811.0720%2C0811.2746%2C0811.4165%2C0811.1156%2C0811.2371%2C0811.1881%2C0811.0451%2C0811.3607%2C0811.0292%2C0811.3596%2C0811.3213%2C0811.2880%2C0811.0171%2C0811.3690%2C0811.1733%2C0811.2729%2C0811.1982%2C0811.2048%2C0811.0687%2C0811.4384%2C0811.1916%2C0811.0473%2C0811.1617&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "A Unified Framework for Photometric Redshifts"}, "summary": "We present a rigorous mathematical solution to photometric redshift\nestimation and the more general inversion problem. The challenge we address is\nto meaningfully constrain unknown properties of astronomical sources based on\ngiven observables, usually multicolor photometry, with the help of a training\nset that provides an empirical relation between the measurements and the\ndesired quantities. We establish a formalism that blurs the boundary between\nthe traditional empirical and template-fitting algorithms, as both are just\nspecial cases that are discussed in detail to put them in context. The new\napproach enables the development of more sophisticated methods that go beyond\nthe classic techniques to combine their advantages. We look at the directions\nfor further improvement in the methodology, and examine the technical aspects\nof practical implementations. We show how training sets are to be constructed\nand used consistently for reliable estimation.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0811.4004%2C0811.1173%2C0811.2623%2C0811.0878%2C0811.3350%2C0811.3078%2C0811.3448%2C0811.4460%2C0811.3330%2C0811.4752%2C0811.2790%2C0811.2377%2C0811.0961%2C0811.0233%2C0811.0403%2C0811.0384%2C0811.2136%2C0811.1163%2C0811.3534%2C0811.3500%2C0811.3432%2C0811.1906%2C0811.0692%2C0811.2617%2C0811.2600%2C0811.0795%2C0811.3450%2C0811.3127%2C0811.0722%2C0811.4287%2C0811.1903%2C0811.1840%2C0811.1962%2C0811.2870%2C0811.1878%2C0811.2954%2C0811.3020%2C0811.2731%2C0811.2615%2C0811.0508%2C0811.0066%2C0811.4049%2C0811.1951%2C0811.4470%2C0811.3272%2C0811.0228%2C0811.3611%2C0811.1477%2C0811.3422%2C0811.3929%2C0811.3775%2C0811.3291%2C0811.0993%2C0811.0349%2C0811.1457%2C0811.2821%2C0811.1411%2C0811.4367%2C0811.2576%2C0811.3879%2C0811.1017%2C0811.1270%2C0811.0829%2C0811.0928%2C0811.3670%2C0811.4361%2C0811.3273%2C0811.3583%2C0811.3935%2C0811.3871%2C0811.2895%2C0811.1987%2C0811.2560%2C0811.1581%2C0811.1065%2C0811.2285%2C0811.1828%2C0811.0212%2C0811.0720%2C0811.2746%2C0811.4165%2C0811.1156%2C0811.2371%2C0811.1881%2C0811.0451%2C0811.3607%2C0811.0292%2C0811.3596%2C0811.3213%2C0811.2880%2C0811.0171%2C0811.3690%2C0811.1733%2C0811.2729%2C0811.1982%2C0811.2048%2C0811.0687%2C0811.4384%2C0811.1916%2C0811.0473%2C0811.1617&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We present a rigorous mathematical solution to photometric redshift\nestimation and the more general inversion problem. The challenge we address is\nto meaningfully constrain unknown properties of astronomical sources based on\ngiven observables, usually multicolor photometry, with the help of a training\nset that provides an empirical relation between the measurements and the\ndesired quantities. We establish a formalism that blurs the boundary between\nthe traditional empirical and template-fitting algorithms, as both are just\nspecial cases that are discussed in detail to put them in context. The new\napproach enables the development of more sophisticated methods that go beyond\nthe classic techniques to combine their advantages. We look at the directions\nfor further improvement in the methodology, and examine the technical aspects\nof practical implementations. We show how training sets are to be constructed\nand used consistently for reliable estimation."}, "authors": ["Tamas Budavari"], "author_detail": {"name": "Tamas Budavari"}, "author": "Tamas Budavari", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1088/0004-637X/695/1/747", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/0811.2600v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0811.2600v2", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "9 pages, 2 figures, accepted to the ApJ", "arxiv_primary_category": {"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "astro-ph", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0811.2600v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0811.2600v2", "journal_reference": "Astrophys.J.695:747-754,2009", "doi": "10.1088/0004-637X/695/1/747", "fulltext": "The Astrophysical Journal; received 2008 November 16, accepted 2009 January 14\nPreprint typeset using LATEX style emulateapj v. 08/22/09\n\nA UNIFIED FRAMEWORK FOR PHOTOMETRIC REDSHIFTS\nTam\u00e1s Budav\u00e1ri\n\narXiv:0811.2600v2 [astro-ph] 23 Mar 2009\n\nDept. of Physics and Astronomy, The Johns Hopkins University, 3400 North Charles Street, Baltimore, MD 21218; budavari@jhu.edu\nDraft version November 3, 2018\n\nABSTRACT\nWe present a rigorous mathematical solution to photometric redshift estimation and the more general\ninversion problem. The challenge we address is to meaningfully constrain unknown properties of\nastronomical sources based on given observables, usually multicolor photometry, with the help of a\ntraining set that provides an empirical relation between the measurements and the desired quantities.\nWe establish a formalism that blurs the boundary between the traditional empirical and templatefitting algorithms, as both are just special cases that are discussed in detail to put them in context.\nThe new approach enables the development of more sophisticated methods that go beyond the classic\ntechniques to combine their advantages. We look at the directions for further improvement in the\nmethodology, and examine the technical aspects of practical implementations. We show how training\nsets are to be constructed and used consistently for reliable estimation.\nSubject headings: galaxies: statistics - methods: statistical\n1. MOTIVATION\n\nThe concept of photometric redshift estimation is over\nfour decades old. Since Baum (1962) the methodology\nhas changed only incrementally but its role in astronomy\nhas completely spun around. The astronomy community originally received the idea with serious skepticism,\nwhich, over time, thanks to a series of breakthroughs in\nthe field (e.g., Koo 1985; Connolly et al. 1995a), slowly\nfaded. Today the next generation telescopes plan to perform photometric observations only, and completely rely\non these kind of estimation techniques for most of their\nkey science projects including cosmology and large-scale\nstructure.\nWhile getting ready for extracting most of our new scientific knowledge from photometric measurements, we\nhave to examine the current limitations of the various\ntechniques and understand the underlying assumptions.\nEssentially all currently existing implementation can be\ncategorized into two classes of methods: empirical estimators and template fitting. Reviewing the history of\nthe research area is outside the scope this study; see\nWeymann et al. (1999) for a rich cross section of the field\ninstead; now we look at the basic concepts and the differences in the traditional methodologies. Empirical methods map the relation of the observed and desired properties using a training set; e.g., piecewise linear or polynomial fitting, or via other regression methods like artificial\nneural nets, support vector machines, etc. Templatefitting techniques rely on prior knowledge encoded in the\nmodel's spectral energy distributions (SEDs) that can\nbe matched to observations. Why are the current implementations of these two so different? There is no fundamental reason, e.g., one could generate training sets\nfrom model templates. Why do only template-fitting algorithms use photometric uncertainties and not the empirical ones? Why do people estimate the redshifts independently from other physical properties, e.g., often use\nempirical redshift estimates and then template spectra\nfor type determination? We know these quantities are\ncorrelated and should be dealt with in a consistent way.\n\nThe answers to these questions are usually direct consequences of limitations in the models and the measurements. If the model SEDs matched all the observations,\nwe would know everything about all the objects in the\nUniverse. The uncertainties would be used more often if\nthey provided reliable extra information.\nThe \"Photo-Z\" label currently associated with the\nabove methods, should gain a new meaning. We should\nexpect more from the codes than a single estimate per\nobject. The implementations need to provide the full\njoint probability density functions of all desired physical\nparameters, so we can develop new statistical tools that\nutilize all the information available.\nIn this paper, we are not concerned with what observables are the best to use or which filter set is optimal\nfor special cases of the generalized photometric inversion\nproblem, which depend on the specific science cases, instead we derive a probabilistic formalism to address the\ncommon issues. In Section 2, we introduce the methodology and derive the formulas for determining the photometric constraints on physical properties. Section 3,\ndescribes the traditional empirical and template-fitting\nalgorithms as special cases of the proposed framework,\nand the advanced techniques that go beyond their limits. In Section 4, we illustrate the concepts and detail\nthe practical aspects. Section 5 concludes our study.\nThroughout the paper, we use the capital P letter for\nprobabilities and the lower-case p letter for probability\ndensity functions, or PDFs for short.\n2. METHODOLOGY\n\nWe start by formulating the problem as general as possible. The challenge is to constrain physical properties of\nsources with some observables in a data set denoted by\nQ, hereafter the query set. Since model spectra would\nnever be perfectly suitable for all desired parameters, one\nwill need a training set, T . In fact there is no reason to\ndemand that these data sets have the same observables.\nThe mapping is provided by some model, M . For example, magnitudes of different photometric systems can be\nmapped on to one another, say, UJFN observations to\n\n\f2\n\nTAM\u00c1S BUDAV\u00c1RI\n\nugriz (Fukugita et al. 1995) by empirical formulas. In\ngeneral, let x be a set of observables in the training set\nT that also contains extra information about the physical properties \u03be, and let y denote the observables of the\nquery set Q. Our model is parameterized by a vector \u03b8;\nT : {xt , \u03be t }t\u2208T\n\b\nQ:\ny q q\u2208Q\n\nM:\n\n\u03b8\n\nThe model M can predict the observables x and y for\na given parameter via the density p(x, y|\u03b8, M ) and has\na prior on its parameters p(\u03b8|M ). For example, one can\nbuild models based on the Coleman, Wu & Weedman\n(1980) or Bruzual & Charlot (2003) templates that can\nbe used to calculate the colors of sources at a given redshift in any particular photometric system. However, the\nmodeling goes beyond just estimating the values for a\ngiven parameter, because the observational uncertainties\nalso enter the formula. Later on, we will discuss in details how to establish various models; for now, the above\nfunctions are assumed to be known. Furthermore, let us\nassume that the training set samples the entire space of\nthe observables, and discuss the selection effects later.\nOur goal is to derive the probability density function\n(PDF) of the physical properties \u03be for a given query point\nq with y q observations using our model M . This function, p(\u03be|y q , M ), is the solution of the generalized photometric inversion problem and the subject of this section.\nThe next two paragraphs discuss probabilistic concepts\nanalogous to elements of template fitting and empirical\nestimation, respectively, in the context of our probabilistic formalism. Next we address the burning issues of\nselection effects and feasibility.\n2.1. Mapping the Observables\nThe first step is to make the connection between the\nobservables. It can be done formally by calculating the\nprobability density of x for the query point q. We do\nthis via the equality of\n\np(x|y q , M ) =\n\np(x, y q |M )\np(y q |M )\n\n(1)\n\nwhere the right-hand side contains integrals of known\nfunctions over the model's parameter domain\nZ\np(x, y q |M ) = d\u03b8 p(\u03b8|M ) p(x, y q |\u03b8, M )\n(2)\n\none has no prior knowledge about the model parameters, and wishes to use a noninformative prior, e.g., flat\np(\u03b8|M ) = 1, formally he/she is allowed to do so; see more\non the priors later on.\n2.2. Physical Properties\n\nNext we establish the relation between the observable\nand the desired physical parameters. The traditional way\nis to assume the properties of interest to be a function\nof the observables. Some of the existing methods utilize\nexplicit functions such as a polynomial or piecewise linear, while others use more obscure mappings such as a\ndecision tree or an artificial neural net. Conceptually,\nthey are just assuming a fitting function\n\u03be = \u03be\u0302(x)\n\n(4)\n\nwhich is tuned to reproduce the elements of the training\nset as best as possible. The problem with this assumption is that there is no guarantee that the same x observables always correspond to the same \u03be properties. In\nfact, we know that degeneracies are present in most data\nsets. Clearly, the above assumption is an unnecessary\nrestriction over the general relation of x and \u03be denoted\nby p(\u03be|x). In other words, the traditional model is\np(\u03be|x) = \u03b4(|\u03be \u2212 \u03be\u0302(x)|)\n\n(5)\n\nusing Dirac's \u03b4 symbol.\nA better way is not to restrict the distribution arbitrarily to an unknown surface but to leave the formula\ngeneral. We can establish the proper relation by observing the fact that\np(\u03be|x) =\n\np(\u03be, x)\np(x)\n\n(6)\n\nThe right-hand side is a ratio of two densities that\n(both) can be estimated from the training set, e.g., using\nVoronoi tessellation or kernel density estimation (KDE).\nHaving derived the above relation, one can compute\nthe final PDF of interest as the integral over the possible\nobservables in the training set\nZ\np(\u03be|y q , M ) = dx p(\u03be|x) p(x|y q , M )\n(7)\n\n(3)\n\nWhen it is possible to accurately characterize this distribution by a Gaussian function or some mixture model,\none can compress the numerical results into a few parameters. When the PDF is unimodal, which is often not the\ncase, the expectation value should suffice for an estimate\nZ\n\u03be\u0304(y q ) = d\u03be \u03be p(\u03be|y q , M )\n(8)\n\nWe see how this is superior to the techniques analogous\nto the traditional way. The usual solution involves fitting for the best-match model parameter using, for example, maximum likelihood estimation (MLE), and accepting that parameter at face value to derive the estimates.\nHere, we consider all possible model parameters and add\nup their contributions.\nWe note that the above general mapping formula is\nvalid in case of improper priors, too, in the sense that\nthe posterior is always properly normalized to unity. If\n\nThe above equation is similar to kernel regression\n(Nadaraya 1964) in case of using KDE, except it is a generalization to incorporate the uncertainties in the data\nsets.\nPhotometric redshifts and other such properties are often used in statistical studies for their availability for a\nlarge number of sources, even though they provide relatively loose constraints on individual objects. The full\nPDFs of the sources are best suited to derive the ensemble properties of entire catalogs or even specific subsamples. The distribution of the properties over a set of\n\nand over x for the marginalization\nZ\np(y q |M ) = dx p(x, y q |M )\n\n\fA UNIFIED FRAMEWORK FOR PHOTOMETRIC REDSHIFTS\nmeasurements Q is given by the average\np(\u03be|Q, M ) = p(\u03be|y q , M )\n\nq\u2208Q\n\n(9)\n\nHence there is no need for an extra deconvolution step\nto recover the underlying distribution of the objects in\na sample, because their average PDF is exactly that. A\ncommon example is the estimation of the redshift distribution dN/dz for various subsamples, say, at different distances. When selection bias is not an issue for\nthe scientific analysis, e.g., lensing studies, one can even\nchoose the subsets to optimize the contrast of the averaged PDFs.\n2.3. Selection Effects\n\nThe inherent limitations of a finite training set pose\na serious problem for any estimator, which is often neglected. Our formalism introduced earlier is no exception, hence we now turn to examine the effects. The\nselection function is the probability of a source, with observables x making it into the training set, P (T |x). The\nregion that the training set can sample is the window\nfunction P (W |x), which takes the value of 1 where the\nselection function is nonzero, and 0 otherwise. For example,\n\u001a\n1\nif V(x) < 22\nP (W |x) = 0\n(10)\notherwise\nfor a survey that has a magnitude limit of 22 in a V band.\nThe selection function is expected to enter our method\nat two separate places: the marginalization over x and\nvia the density estimates used for the relation p(\u03be|x).\nThe former appears to be inevitable but causes problems\nonly at the boundaries of the selection criteria. If the\nintegrand p(x|y q , M ) in equation (7) vanishes within the\nintegration domain of the window function P (W |x), the\nresults are valid. Otherwise the estimated PDF is biased\nin an unknown way. The probability of q being inside\nthe window function is the right indicator of the problem\noccurring\nZ\nP (W |y q , M ) = dx P (W |x) p(x|y q , M )\n(11)\nWhen this probability is close to 1, the training set provides good support for the photometric inversion problem, but when the value is low, the query point is known\nto be outside the regime of the training set.\nThe relation between the desired properties and the\nobservables is the other issue as it is only probed on the\ntraining set. The relation as seen on the training set\ndepends on the true relation and the selection function\nvia the equation\np(\u03be|x, T ) =\n\np(\u03be|x) P (T |x, \u03be)\nP (T |x)\n\n(12)\n\nIf the selection function strictly depends only on x, we\nhave P (T |x, \u03be) = P (T |x) and find that the empirical relation is identical to the true one on the selection domain.\nIf the sampling frequency is low, the measured relation\nis noisier and less robust numerically.\nThis is a critical point, which is worth emphasizing\nonce again: the p(\u03be|x, T ) = p(\u03be|x) equality holds only if\n\u03be does not influence the selection in any way, not even indirectly via some hidden parameter. A counter example\n\n3\n\nis the common case of cutting on morphological parameters in the selection function, while only considering the\nfluxes for x. Another interesting consequence is that one\ncannot use only the colors to estimate, say, photometric\nredshifts, if a magnitude cut was involved in the selection\nof the training set. Yet another issue is cosmic variance,\nwhich might cause the relation to depend on the position in the sky. The solution in all cases is to revise the\nselection of the training set, if possible, or to add the hidden observables into x, and extend the model to include\nthem.\n3. MODELS IN THE TRADITIONAL LIMITS AND BEYOND\n\nPreviously we have hinted at how models can be constructed but, until now, they have just been assumed to\nbe known. A model is a combination of the limitations\nin our observations, both in the training and query sets,\nand the parameterization of the observables. From discussing the topic in the most general way, we now turn to\nthe practicalities of real-life astronomical observations.\nToday the errors of extracted fluxes of photometric\nmeasurements are independent estimates of the uncertainties in the separate passbands. Typically, Gaussian\nerrors are assumed, and the catalogs would quote 1\u03c3 values for every source. Analyzing the repeated observations in the Sloan Digital Sky Survey (SDSS; York et al.\n2000), Scranton et al. (2005) have shown that this simple picture is wrong, and the off-diagonal elements of the\ncovariance matrix are significant. This is not surprising.\nOne of the major components in the photometric uncertainty is the error in the determination of the aperture.\nIf the multicolor measurements share a common aperture, e.g., SDSS model magnitudes that are best suited\nfor colors, the flux measurements will be inevitably correlated. Thus an improved error model of the photometric observations is described by a multivariate normal\ndistribution, N (x|x\u0304, Cx ), with a mean of x\u0304 and covariance matrix Cx . The next generation survey telescopes\nthat plan to visit the sources on multiple occasions will\nbe able to better determine the full covariance matrices\nfrom actual observations to improve our understanding\nof the errors. Hence, for now it is general enough to\nconsider error estimates that are fully described by the\ncovariances.\nIn this reasonable approximation, the p(x, y|\u03b8, M )\nmapping is also a normal distribution with a full covariance matrix that includes cross-catalog terms, if necessary, that go beyond the calibration work on the individual catalogs. If the apertures are locked together for\nbetter color determination, one has to obtain the dependencies via a data set that contains sources with all\nx and y measurements. However, when the processing\npipelines are independent, one can assume that the uncertainties in x and y are also independent, and write a\nrealistic M as the product of the two Gaussians:\np(x, y|\u03b8, M ) = Nx (x|x\u0304(\u03b8), Cx (\u03b8))\n\u00d7 Ny (y|\u0233(\u03b8), Cy (\u03b8))\n\n(13)\n\nThe dependences in the means x\u0304(\u03b8) and \u0233(\u03b8) are\nstraightforward to model and, even in the most complicated case, are similar, in spirit, to the traditional\ntemplate-fitting procedures. For example, when considering a synthetic model of galaxies, one has to vary the\n\n\f4\n\nTAM\u00c1S BUDAV\u00c1RI\n\nredshift, age, optical depth, and so on, to derive highresolution model spectra for different parameters, and\nthen convolve them with the broadband filters to get the\nfluxes.\nClearly modeling the covariance matrices is more complicated and would require many more parameters to\nmodel accurately. If \u03b8 is a minimal set of parameters\nthat is enough to describe x\u0304(\u03b8) and \u0233(\u03b8), there are some\nother hidden parameters or hyperparameters that are\nalso needed for the covariances. The fully Bayesian way\nis to establish the relation of the covariance matrix and\nthe hyperparameters along with a hyperprior (the prior\non the hyperparameters), and to marginalize over the extra dependence. Even though, this relation between the\nelements of the covariance matrix and the observables\ncould, in principle, be modeled based on the catalogs, it\nmay prove impractical. The empirical Bayes approach,\nadmittedly more optimistic but easily quantifiable, is to\nfind the most likely hyperparameter and substitute it into\nthe dependence. In practice, for every parameter \u03b8, one\ncan find the values of x\u0304(\u03b8) and \u0233(\u03b8) and the closest measurement points, whose covariance matrices are good estimates. If the covariance matrix changes slowly with\nx compared to its widths, one can safely calculate the\nvalues at the catalog points by using the corresponding\nerror matrices,\np(xt , y q |\u03b8, M ) = Nx (xt |x\u0304(\u03b8), Ct )\n\u0001\n\u00d7 Ny y q |\u0233(\u03b8), Cq\n\n(14)\n\nThe only concern with this approximation is the noise\non the elements of the covariance. If needed, one could\nimprove on the stability by smoothing or fitting locally\nover the catalog entries.\nThe consequences of the model approximation in equation (14) are most intriguing from the implementation\naspect of the methodology. As long as we only evaluate the PDFs at the observed locations, the calculations\nare more straightforward and computationally less expensive.\n3.1. Numerical Evaluation\nThe field of numerical evaluation of complicated multidimensional integrals that usually emerge in Bayesian\nanalysis such as ours is well studied. The solution typically involves some randomized algorithms that range\nfrom simple direct sampling from the prior to adaptive strategies often based on Markov chain Monte Carlo\n(MCMC) methods, e.g., Gibbs sampling. Although this\ntopic is beyond the scope of the present discussion, we\nbriefly touch on the basic idea to illustrate the concepts\nand provide some insight on how to derive the final results numerically, namely the value of P (W |y q , M ) and\nthe function p(\u03be|y q , M ).\nThe clever construction of the chain in the MCMC algorithm yields model parameters {\u03b8i } that can be considered independent random realization drawn from the\nposterior distribution, p(\u03b8|y q , M ) in our case. With the\nchain in hand, one can readily approximate the integral\nby the average over the MCMC samples. The mapping\nof the observables then becomes\n\np(x|y q , M ) = Nx (x|x\u0304(\u03b8 i ),Ct )\n\n(15)\n\nwhere t is the index of the training point xt closest to\nx\u0304(\u03b8 i ). When the query point is well within the regime\nof the training set, this approximation is valid. What\nhappens otherwise? Often the uncertainties are larger\noutside the selection criteria, e.g., the photometric errors beyond the flux limit. By using the covariance\nmatrix of the closest training point, one actually artificially decreases the contribution to the integral making\np(x|y q , M ) tighter. While the accuracy of the calculation\nis affected, the change is such that it reduces the value\nof the integral in P (W |y q , M ), which is the measure of\nreliability. Hence, if we measure a large value, we can be\nconfident of the result. Having said that we note that\nin practice the covariances probably do not change fast\nenough to pose a significant problem in this calculation\nfor the objects along the edge of the selection function,\nand farther away the probabilities are very small anyway.\nOnce we know that the estimation is in the safe regime,\nwe can compute the p(\u03be|y q , M ) integral ignoring the window function completely by summing up at preset \u03be r\npoints in our region of interest, e.g., a fine redshift grid,\nas\nX\np(xt |y q , M )\n(16)\np(\u03be r |y q , T, M ) \u221d\np(\u03be r |xt , T )\np(xt |T )\nt\u2208T\n\nwhere the p(xt |T ) densities and the matrix p(\u03be r |xt , T )\nare obtained from the numerical density estimates once\nfor the training set; see equation (6). Here, we made use\nof the fact that the {xt } points are (naturally) drawn\nfrom the distribution p(x|T ).\nIn order to perform these summations efficiently for\nmany query points, one has to utilize fast searching mechanisms in the space of the observables. The situation is\ncomplicated by the strong correlation in the observables\nand the varying Mahalanobis metric, yet, a significant\nspeedup can be achieved by adequate multidimensional\nindexing of the color\u2013space as described in Csabai et al.\n(2007).\n3.2. Template Fitting\n\nIn classical SED-fitting approaches, one does not technically have a training set. Although, formally it can\nbe generated from a grid of model parameters {\u03b8t } as\n{xt , \u03be t } = {x\u0304(\u03b8 t ), \u03be\u0304(\u03b8 t )}, where \u03be\u0304(\u03b8) is often simply a\nsubset of \u03b8, e.g., the redshift is just one of the parameters in the models of SEDs. Traditionally, this artificial\ntraining set has no errors associated with the reference\npoints, hence we have\np(x|\u03b8, M ) = \u03b4(|x \u2212 x\u0304(\u03b8)|)\n\n(17)\n\nand, assuming x\u0304(\u03b8) has an inverse,\np(\u03be|xt , M ) = \u03b4(|\u03be \u2212 \u03be t |)\n\n(18)\n\nThe analytical calculation yields an intuitive result,\nwhere the grid points are weighted by their likelihood\nmultiplied by the corresponding prior\nX\n\u0001\np(\u03be|y q , M ) \u221d\n\u03b4(|\u03be\u2212\u03bet |) p(\u03b8 t |M ) N y q |\u0233(\u03b8t ),Cq\nt\u2208T\n\n(19)\nIn the limit of a flat prior, this is the classic MLE case,\nwhich is equivalent to the \u03c72 minimization techniques\n\n\fA UNIFIED FRAMEWORK FOR PHOTOMETRIC REDSHIFTS\nused in most SED-fitting implementations today, where\nthe measurements are compared to the simulated observations at the grid points to select the optimum. One obvious exception is the algorithm of Ben\u0131\u0301tez (2000), which\nactually applies an explicit empirical redshift prior in this\nequation, hence it is often referred to as \"Bayesian.\"\nThe selection of a set of templates is another simple\nprior but on the spectral type, even if well hidden, implicit, and not often admitted. Researchers routinely\nseek for templates that provide the best redshift estimates. Strictly speaking, this is cheating. The selection\nshould be based on how well the templates represent the\ndata in the space of the observables, and not based on\ntheir performance in the estimation. Naturally, there\nis a connection, but not in that direction. The templates that follow the data will likely provide better estimates; however, templates that yield good estimates are\nnot guaranteed to match the data. The development\nof a class of methods by Budav\u00e1ri et al. (1999, 2000,\n2001) and Csabai et al. (2000) can be considered early\nattempts to achieve a better SED prior. Here, the templates are statistically modified to represent the observations more accurately, while not optimized for redshift\nestimation whatsoever. Clearly, these are just the first\nsteps in this direction. Instead of just assigning 1 and\n0 weights to the templates by either including them or\nnot (respectively) as typically done today, one can explicitly formalize more realistic priors over a broader range\nof SEDs that are driven by scientific knowledge and/or\nensemble statistics.\nAn obvious but rather important improvement in the\nnew framework is the ability to naturally introduce and\nutilize the uncertainties of the template spectra. We\nknow that the models are not perfect, and this can be easily characterized. As an example, one can use the same\nprescription for the spectral synthesis, but build on a\nvarious stellar libraries to analyze the differences. When\nusing empirical templates, the implementation is even\nmore evident. We fold in the uncertainties by abandoning the simplified relation in equation (17) and creating\na more realistic model with the estimated finite errors.\n3.3. Empirical Method\nThe new methodology in the limit of the classic empirical algorithms goes well beyond the usual techniques,\nwhich consist of simply establishing the fitting function\nin equation (4). We can utilize those fits (or preferably\nestimate the densities numerically to map the full relation), but we can also properly consider the uncertainties.\nThe parameterization of a minimalist model is done\nby a position in the space of the observables, i.e., \u03b8 is\nthe same type of quantity as x and y, e.g., UBVI fluxes.\nNamely, we choose x\u0304(\u03b8) = \u03b8 and \u0233(\u03b8) = \u03b8. Even though\nthe observables in x and y are the same quantities, the\nmapping is still required to fold in the photometric errors.\nWith an improper flat prior p(\u03b8|M ) = 1, the mapping of\nthe observables is integrated analytically\nZ\np(xt |y q , M ) = d\u03b8 N (xt |\u03b8, Ct ) N (\u03b8|y q , Cq )\n(20)\n\nWhile this model is clearly very simple, it is quite powerful and conceptually more sound than a number of traditional methods. We will use it for illustrations in the\nupcoming discussions.\n\n5\n\nOther simple forms of priors can also be handled analytically, e.g., linear and Gaussian, that may be reasonable approximations at least locally. Otherwise we resort\nto the numerical evaluation.\n3.4. Advanced Methods of the Future\n\nThe problem with the classic empirical methods is the\nrequirement of having the same set of observables for\nboth the training and the query sets. The limitations of\nthe SED-fitting techniques come from the fact that the\nmodels cannot perfectly describe the relation of observables and the physical properties.\nIn the realm of our unified framework, we can have\nmore advanced methods that combine these two previously separate classes of techniques. We can introduce\nnew algorithms to take advantage of the training points\neven if their photometric observables differ from those\nin the query set. The idea is the following: True to the\nspirit of empirical methods, we utilize the training set to\nprovide the relation between the physical properties we\nwish to constrain and some observables; see equation (6).\nIn addition to this empirical relation, we apply a mapping from the observables of the query set to that of the\ntraining set based on SED modeling, like in the template fitting procedures. For example, if the training set\ncontains UJFN magnitudes, one can map them to ugriz\nusing equation (1).\nThe intriguing observation to make here is that one\ndoes not even need realistic physical models to start with,\nbecause the physics is in the training set and not the\nmodel. Let us consider a model M , which is a complete\nbasis on the observed wavelength range, e.g., Legendre\npolynomials or Fourier series with the parameterization\nby their coefficients. The manifold of the physical spectra is naturally contained within. In practice, this model\nneeds to be only sufficiently complete and band-limited\nso that real SEDs can be well described; this is a weak\nprior that we can set up based on all the spectra we\nobserved and simulated before. A model spectrum corresponding to a certain parameter value in M can be\nconvolved with the appropriate transmission curves to\nyield the observables x\u0304(\u03b8) and \u0233(\u03b8), even if they are unphysical. Hence, formally we have the basis of our mapping, p(x, y|\u03b8, M ). As long as the data provide good\nenough constraints on the model parameters, the mapping is valid and the algorithm follows the routine. When\nthe observations barely constrain the model parameters\nand large volumes of unphysical SEDs have significant\nlikelihood, the mapping will be wrong. The solution is\nto apply a prior to consider only the physical SEDs. Using the entire catalog, one can derive an empirical physical prior statistically, which we will discuss in the next\nsection.\nThese new advanced methods overcome the usual difficulties in photometric redshift estimation, and offer a\nway out of the half-century-old dilemma. They are a\nnatural extension of everything that has worked before\nin the field: a straightforward combination of the two\npreviously separate methodologies.\n4. DISCUSSION\n\nNext we demonstrate the new framework in action by\napplying a simple model to real-life data, which is followed by discussions of the qualities of training sets and\n\n\f6\n\nTAM\u00c1S BUDAV\u00c1RI\n\nFig. 1.- The probability density as a function of the redshift for early- and late-type galaxies (upper and lower panels, respectively) at\ndifferent distances marked by the vertical dashed lines. For every object, the dotted line shows the empirical relation of p(z|x = mq ), and\nthe solid line illustrates the final result of p(z|y = mq , M ) after properly folding in the photometric uncertainties via the mapping in the\nmodel.\n\nthe prior.\n4.1. A Case Study\nTo illustrate the concepts introduced earlier, we apply the aforementioned minimalist empirical model to\na sample of galaxies. We choose SDSS sources for\ntheir well-studied photometric uncertainties. Following\nScranton et al. (2005), we estimate the full covariance\nmatrix for all objects, and utilize them in the subsequent analysis. We randomly select a quarter of the entire Main Galaxy Sample (MGS; Strauss et al. 2002) of\nDR6 to be the training set, roughly 100 thousand objects. Our query set is a smaller disjoint random subset\nfor illustration purposes. First, we map the observables\n(magnitudes to magnitudes) analytically using our simple model in equation (20). The calculation is done inside\nthe DR6 database by SQL User-Defined Functions.\nNext, we compute the conditional PDFs by a dual-tree\nKDE implementation (Gray & Moore 2003; Lee & Gray\n2006) at preset locations defined by the T training and\nQ query sets in magnitude space and a uniform highresolution redshift grid. The practical complication\nwith any density estimation is the fact that it is scaledependent and changes with the metric. We are further\nlimited in our applications to fix bandwidths for the conditional density estimation in the current implementation\nof the estimator. We adopt a bandwidth of h = 0.004 in\na metric that scales the magnitudes to the redshift. In\nother words, the resolution in redshift space is set by h,\nthe full width half maximum of the normal distributions,\nand we re-scale the magnitudes by a factor of f = 0.08 to\nreasonably match the density of the sources in the separate subspaces. This simple technique is expected perform reasonably well within the regime where the sources\n\nare suitably dense but not in the outskirts where a larger\nvariable bandwidth is needed in magnitude space. The\ntheory of more sophisticated conditional density estimation is well-studied (e.g., Fan et al. 1996), and advanced\nadaptive implementations are in the works to help out\n(Lee & Gray, 2008; private communication).\nFigure 1 illustrates the nature of the x \u2212 \u03be relation, in this case the multicolor measurements and redshift p(z|mq ), as well as the final redshift distribution,\np(z|mq , M ), incorporating the photometric uncertainties\nin our model. We see that the redshift is really not a simple function of the magnitudes but rather a more general\nrelation. This is even more so for observables that constrain the physical properties less than the ugriz measurements. The relation itself (shown as a dotted line)\nmight provide an overly optimistic view of the uncertainties at times and usually much noisier than the final\nPDF (shown in solid) that sums up these relations with\nappropriate weights. The top panels show intrinsically\nred galaxies at three different redshifts, which were selected based on the mixing angle of the first two principal\ncomponents, also known as the eClass in the SDSS terminology. The bottom panels show the more problematic blue galaxies at similar redshifts. Note the consistent performance of the estimator on the red sources as a\nfunction redshift in comparison to the blue galaxies that\nhave broader PDFs at higher redshifts and are noisier,\nespecially at the largest distances.\nIn the bottom rightmost panel, the distribution is\nnot even centered around the spectroscopic redshift, but\nskewed toward lower values. This object is very close to\nthe edge of the training set, and the result would be considered unreliable due to the lack of calibrators at higher\nredshift that would still be within the sources photomet-\n\n\fA UNIFIED FRAMEWORK FOR PHOTOMETRIC REDSHIFTS\nric uncertainties.\n4.2. Sampling Frequency\n\nA very attractive feature of the new PDF estimator\nderived earlier in equation (8) is its conceptual independence from the sampling of the calibrators. Many statistical tools rely heavily on having a representative training\nset and only provide unbiased results in that limit. In our\ncase, the training points simply provide locations where\nthe evaluation is feasible and their density is essentially\njust a resolution factor. The sampling frequency of the\ntraining set only affects the accuracy of the numerical\nintegral in equation (16) but not in a systematic way as\nlong as the query point is well within the boundaries of\nthe window function. A denser training set will provide\nhigher resolution in the summation, but there is a practical limit beyond which one expects no improvement. The\nreason is that the new calibrator sources are essentially\nidentical to the ones already in the training set.\nThe number of spectroscopic measurements to be carried out for calibration purposes is limited by finite resources. It is vital to acquire reliable training sets for the\nnew generation photometric studies. A good training set\nhas a well-defined selection function, using criteria based\non only the observables one plans to model for the estimation, and, within that, a smart adaptive sampling\nstrategy to optimize the coverage in observable space.\nClearly, the densest regions can be subsampled, but one\nneeds all training points in the outskirts of the manifold\nfor broad support. For this reason, the simplest random\nsubsampling of the underlying population will not suffice.\nInstead, a stratified sampling strategy is to be pursued.\nTo demonstrate that the methodology is robust to this\nkind of systematic changes in the training set, we create\na stratified subset and perform the previous analysis the\nsame way. The sampling is done by including sources\nrandomly based on their local density p(x|T ) in magnitude space. A galaxy is included in the training set only\nif the ratio of some constant p0 and the local density is\nlarger than a randomly generated real number, U01 , uniform between 0 and 1, i.e., p0 /p(x|T ) > U01 . We set the\nvalue of p0 to yield a subsample that is half the size of the\noriginal data set. Figure 2 shows the results for the previously selected sources based on the smaller stratified\nsubset. The basic shape of the curves is practically the\nsame in most cases, only somewhat noisier but without\nsystematics. One exception is the blue galaxy at around\nz = 0.1, where the subsampling somewhat amplifies the\neffect of the large wall in SDSS at z = 0.08. The blue\ngalaxy at the highest redshift is essentially unchanged\n(except for the part at the lowest redshift where the density in magnitude space is larger to start with) because\nthe stratified sampling (by construction) has no effect on\nits already very sparse neighborhood.\nOptimal sampling is difficult to achieve. In fact, it is\ndifficult even to define. In addition to the photometric\nuncertainties, the desired resolution of the physical quantities also sets limits on the sampling frequency. This is\nprominent in the case of degenerate regions where an extended part of the physical parameter space is cramped\ninto a small volume of observables. Simulations built on\nrealistic SED models can help cross-check these factors,\nand evaluate the performance of the estimator ahead\nof time. In the ideal case, one would create stratified\n\n7\n\ntraining sets in the space of the physical parameters instead of the observables, which should be more feasible\nin the near future with improved spectral modeling (e.g.,\nCharlot & Bruzual 2009).\n4.3. Empirical Priors\n\nThe distribution of sources in a training set may be\nartificial and, as we just argued, should be optimized for\ncoverage with a practical upper bound on the density\ntuned to the photometric inaccuracies and source diversity. However, the distribution in the query set is often\nphysical and can be used to derive an empirical prior\nfor our model. The basic observation is that the density\nof sources in the query set, p(y|Q), should match the\npredicted density of the model, p(y|M ). The latter is\ncalculated for any prior as the convolution,\nZ\np(y|M ) = d\u03b8 p(y|\u03b8, M ) p(\u03b8|M )\n(21)\nIf we substitute p(y|Q) measured from the sources on the\nleft-hand side of the equation, the only unknown is the\nprior, which we can solve for using the elegant deconvolution technique of Richardson (1972) and Lucy (1974).\nTo see why a physically sensible prior is important,\nlet us consider the density of sources in the training set\nwithin the window function. Since the density is proportional to the product of the underlying p(x) density and\nthe selection function,\np(x|T ) \u221d p(x) P (T |x)\n\n(22)\n\na significant volume of the window function is not sampled by the training set, where p(x) is zero. Without\na reasonable prior, the mapping p(x|y q , M ) could yield\nwrong weights for unphysical observables in the summation of equation (7). Hence, any model needs some physical information. Even if one is hesitant to take the empirical prior at face value, the domain of the model parameters should be carefully considered. In case of template\nfitting, this happens implicitly, even if not optimally, via\nthe selection of the set or manifold of template spectra,\nbut can be also done for even the empirical algorithms.\n5. CONCLUSIONS\n\nStarting from first principles of Bayesian probability\ntheory, we built a description and obtained the solution\nof the generic photometric inversion problem, where the\nphysical properties of sources are constrained based on\nobservational measurements. The new approach yields a\nformalism that encapsulates the field of photometric redshift estimation, and contains the traditional methods as\nspecial cases. In our systematic analysis of the mathematical problem, we put previous techniques in context\nand pointed out the directions for improvement in each.\nThe proposed extensions to the current methods represent significant progress in more respects. We avoid\nthe common assumption of the physical properties being\na single-valued function of the observables by treating\ntheir relation in a more general way. Thus the formalism\nis not prone to fail in regions, where the data sets are\ndegenerate. We showed how to estimate the corresponding probability density of this relation. In addition, the\nuncertainties of the observables are propagated all the\n\n\f8\n\nTAM\u00c1S BUDAV\u00c1RI\n\nFig. 2.- Results obtained from the stratified training set look much like the those from the full sample, which shows that the representativeness does not matter; instead the training sets should be optimized for the broad coverage of the observable volume with highest\nsampling rates in the outskirts.\n\nway to the results via explicit modeling of the accuracies. We discussed various aspects of the modeling from\nthe simplest empirical case to the application of SEDs.\nThis general framework allows for the construction\nof novel, more advanced methods that combine the attractive qualities of empirical and template-fitting algorithms. One can build empirical estimators based on\ntraining sets that have different observables from the\nquery set, e.g., UJFN photometry to ugriz, via SED\nmodeling. We can improve the methods by creating\nmore and more realistic models that include, for example,\nthe strengths of the emission lines in galaxies (following\nGy\u0151ry et al. 2009) and their inclination angles (based on\nYip et al. 2009) among the model parameters to properly marginalize over the nuance parameters for a more\nreliable mapping of the observables.\nThe current limitations come from the lack of good\nunderstanding of the photometric uncertainties. From\nprevious studies, we know that the flux measurements\nin various passbands are correlated, yet, most catalogs\nonly quote errors on the individual fluxes. For more\nprecise scientific measurements via tighter photometric\nconstraints, we need better photometric error models in\nthe future. Upcoming survey telescopes will observe all\nsources multiple times, hence will be able to get a better handle on the errors and their covariances. Understadning these systematics is probably one of the highest\npriority tasks in the preparation for the upcoming era of\n\nphotometric science.\nThe proper solution of the generalized photometric inversion problem may be straightforward on paper, but efficient implementations of realistic models with appropriate priors involve many advanced concepts in statistics,\nand can only be built on the most recent and on-going developments in computer science, e.g., multi-dimensional\nindexing in databases. Even then the computations are\nnot trivial to carry out, and have significantly higher demand for compute power than previous methods. The\nimmediate future work is to have such a unified framework developed and ready for the next generation imaging surveys.\nThe author is grateful to Alexander S. Szalay, Istv\u00e1n\nCsabai and Andrew J. Connolly for originally introducing the problem, and for the countless inspiring discussions on various aspects of the topic over the course of\nmany years ever since. The presented work has grown\nout of their collaboration on Hubble Space Telescope and\nSloan Digital Sky Survey data. Also the author thanks\nDongryeol Lee and Alexander G. Gray of the FastLab\nTeam at the Georgia Institute of Technology for making\ntheir dual-tree KDE implementation available for this\nwork, and for their continuing support to the astronomy\ncommunity. This study was supported by the Gordon\nand Betty Moore Foundation via GBMF 554.\n\nREFERENCES\nBaum, W.A., 1962, IAU Symposium, 15 (New York:IAU), 390\nBen\u0131\u0301tez, N. 2000, ApJ, 536, 571\n\nBudav\u00e1ri, T., Szalay, A.S., Connolly, A.J., Csabai, I. &\nDickinson, M.E., 1999, in ASP Conf. Proc., 191, Photometric\nRedshifts and High Redshift Galaxies, ed. R. J. Weymann, L.\nJ. Storrie\u2013Lombardi, M. Sawicki, & R. Brunner, (San\nFrancisco, CA: ASP), 19\n\n\fA UNIFIED FRAMEWORK FOR PHOTOMETRIC REDSHIFTS\nBudav\u00e1ri, T., Szalay, A.S., Connolly, A.J., Csabai, I., &\nDickinson, M.E., 2000, AJ, 120, 1588\nBudav\u00e1ri, T., et al., 2001, AJ, 122, 1163\nBruzual, G., & Charlot, S. 2003, MNRAS, 344, 1000\nCharlot, S., & Bruzual, G. 2009, in preparation\nColeman, G.D., Wu., C.-C., & Weedman, D.W., 1980, ApJS, 43,\n393\nConnolly, A.J., Csabai, I., Szalay, A.S., Koo, D.C., Kron, R.G., &\nMunn, J.A., 1995a, AJ, 110, 2655\nCsabai, I., Connolly, A.J., Szalay, A.S., & Budav\u00e1ri, T., 2000, AJ,\n119, 69\nCsabai, I., Dobos, L., Trencs\u00e9ni, M., Herczegh, G., J\u00f3zsa, P.,\nPurger, N., Budav\u00e1ri, T., & Szalay, A. S. 2007, Astron. Nachr.,\n328, 852\nFan, J., Yao, Q., & Tong, H. 1996, Biometrika, 83, 1, 189-206\nFukugita, M., Shimasaku, K., & Ichikawa, T. 1995, PASP, 107,\n945\nGray, A., & Moore, A.W. 2003, SIAM Int. Conf. on Data Mining\n(Philadelphia: SIAM)\nGy\u0151ry, Z., et al. 2009, in preparation\n\nKoo, D.C. 1985, AJ, 90, 148\nLee, D., & Gray, A. 2006, Proceedings of the 22nd Annual\nConference of Uncertainty in Artificial Intelligence (UAI-06),\nArlington, Virginia\nLucy, L.B. 1974, AJ, 79, 745\nNadaraya, E.A. 1964, Theory of Probab. Appl., 9, 141\nRichardson, W.H. 1972, J. Opt. Soc. Am. , 62, 55\nScranton, R., Connolly, A.J., Szalay, A.S., Lupton, R.H.,\nJohnston, D., Budav\u00e1ri, T., Brinkman, J., & Fukugita, M.\n2005, arXiv:astro-ph/0508564\nStrauss, M.A., et al. 2002, AJ, 124, 1810\nWeymann, R. J., Storrie\u2013Lombardi, L. J., Sawicki, M., &\nBrunner, R., (ed.) 1999, in ASP Conf. Proc., Photometric\nRedshifts and High\u2013Redshift Galaxies (San Francisco, CA:\nASP)\nYip, C.-W., et al., 2009, in preparation\nYork, D.G., et al. 2000, AJ, 120, 1579\n\n9\n\n\f"}