{"id": "http://arxiv.org/abs/0902.1324v2", "guidislink": true, "updated": "2009-02-09T22:30:36Z", "updated_parsed": [2009, 2, 9, 22, 30, 36, 0, 40, 0], "published": "2009-02-08T18:38:06Z", "published_parsed": [2009, 2, 8, 18, 38, 6, 6, 39, 0], "title": "Using the Eigenvalue Relaxation for Binary Least-Squares Estimation\n  Problems", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0902.3860%2C0902.1158%2C0902.2264%2C0902.0992%2C0902.1724%2C0902.0268%2C0902.0293%2C0902.3988%2C0902.3611%2C0902.2267%2C0902.3618%2C0902.3124%2C0902.3481%2C0902.1916%2C0902.1115%2C0902.1900%2C0902.2192%2C0902.3384%2C0902.4503%2C0902.4505%2C0902.4034%2C0902.0707%2C0902.2202%2C0902.3464%2C0902.1204%2C0902.4029%2C0902.3780%2C0902.2910%2C0902.3912%2C0902.4132%2C0902.1979%2C0902.3924%2C0902.1843%2C0902.4188%2C0902.4157%2C0902.0779%2C0902.4521%2C0902.3183%2C0902.2436%2C0902.3775%2C0902.4461%2C0902.2933%2C0902.4714%2C0902.2311%2C0902.0391%2C0902.3908%2C0902.4482%2C0902.4557%2C0902.0208%2C0902.3896%2C0902.0150%2C0902.2513%2C0902.3688%2C0902.1178%2C0902.1006%2C0902.4025%2C0902.4237%2C0902.1381%2C0902.1772%2C0902.4668%2C0902.0900%2C0902.2283%2C0902.2359%2C0902.1987%2C0902.3570%2C0902.2894%2C0902.0634%2C0902.2491%2C0902.4269%2C0902.3181%2C0902.0681%2C0902.3444%2C0902.0746%2C0902.2502%2C0902.0476%2C0902.1740%2C0902.4328%2C0902.1458%2C0902.3226%2C0902.3160%2C0902.0239%2C0902.3787%2C0902.0549%2C0902.1386%2C0902.3852%2C0902.2409%2C0902.0780%2C0902.3402%2C0902.4136%2C0902.3294%2C0902.4105%2C0902.3458%2C0902.2553%2C0902.3677%2C0902.2369%2C0902.2106%2C0902.1034%2C0902.1369%2C0902.3283%2C0902.2998%2C0902.1324&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Using the Eigenvalue Relaxation for Binary Least-Squares Estimation\n  Problems"}, "summary": "The goal of this paper is to survey the properties of the eigenvalue\nrelaxation for least squares binary problems. This relaxation is a convex\nprogram which is obtained as the Lagrangian dual of the original problem with\nan implicit compact constraint and as such, is a convex problem with polynomial\ntime complexity. Moreover, as a main pratical advantage of this relaxation over\nthe standard Semi-Definite Programming approach, several efficient bundle\nmethods are available for this problem allowing to address problems of very\nlarge dimension. The necessary tools from convex analysis are recalled and\nshown at work for handling the problem of exactness of this relaxation. Two\napplications are described. The first one is the problem of binary image\nreconstruction and the second is the problem of multiuser detection in CDMA\nsystems.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0902.3860%2C0902.1158%2C0902.2264%2C0902.0992%2C0902.1724%2C0902.0268%2C0902.0293%2C0902.3988%2C0902.3611%2C0902.2267%2C0902.3618%2C0902.3124%2C0902.3481%2C0902.1916%2C0902.1115%2C0902.1900%2C0902.2192%2C0902.3384%2C0902.4503%2C0902.4505%2C0902.4034%2C0902.0707%2C0902.2202%2C0902.3464%2C0902.1204%2C0902.4029%2C0902.3780%2C0902.2910%2C0902.3912%2C0902.4132%2C0902.1979%2C0902.3924%2C0902.1843%2C0902.4188%2C0902.4157%2C0902.0779%2C0902.4521%2C0902.3183%2C0902.2436%2C0902.3775%2C0902.4461%2C0902.2933%2C0902.4714%2C0902.2311%2C0902.0391%2C0902.3908%2C0902.4482%2C0902.4557%2C0902.0208%2C0902.3896%2C0902.0150%2C0902.2513%2C0902.3688%2C0902.1178%2C0902.1006%2C0902.4025%2C0902.4237%2C0902.1381%2C0902.1772%2C0902.4668%2C0902.0900%2C0902.2283%2C0902.2359%2C0902.1987%2C0902.3570%2C0902.2894%2C0902.0634%2C0902.2491%2C0902.4269%2C0902.3181%2C0902.0681%2C0902.3444%2C0902.0746%2C0902.2502%2C0902.0476%2C0902.1740%2C0902.4328%2C0902.1458%2C0902.3226%2C0902.3160%2C0902.0239%2C0902.3787%2C0902.0549%2C0902.1386%2C0902.3852%2C0902.2409%2C0902.0780%2C0902.3402%2C0902.4136%2C0902.3294%2C0902.4105%2C0902.3458%2C0902.2553%2C0902.3677%2C0902.2369%2C0902.2106%2C0902.1034%2C0902.1369%2C0902.3283%2C0902.2998%2C0902.1324&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "The goal of this paper is to survey the properties of the eigenvalue\nrelaxation for least squares binary problems. This relaxation is a convex\nprogram which is obtained as the Lagrangian dual of the original problem with\nan implicit compact constraint and as such, is a convex problem with polynomial\ntime complexity. Moreover, as a main pratical advantage of this relaxation over\nthe standard Semi-Definite Programming approach, several efficient bundle\nmethods are available for this problem allowing to address problems of very\nlarge dimension. The necessary tools from convex analysis are recalled and\nshown at work for handling the problem of exactness of this relaxation. Two\napplications are described. The first one is the problem of binary image\nreconstruction and the second is the problem of multiuser detection in CDMA\nsystems."}, "authors": ["Stephane Chretien", "Franck Corset"], "author_detail": {"name": "Franck Corset"}, "author": "Franck Corset", "links": [{"href": "http://arxiv.org/abs/0902.1324v2", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0902.1324v2", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "stat.ME", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0902.1324v2", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0902.1324v2", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Using the Eigenvalue Relaxation for Binary Least-Squares\nEstimation Problems\n\narXiv:0902.1324v2 [stat.ME] 9 Feb 2009\n\nSt\u00e9phane Chr\u00e9tien \u2217and Franck Corset\n\n\u2020\n\nFebruary 9, 2009\n\nAbstract\nThe goal of this paper is to survey the properties of the eigenvalue relaxation for least squares binary\nproblems. This relaxation is a convex program which is obtained as the Lagrangian dual of the original problem with an implicit compact constraint and as such, is a convex problem with polynomial time complexity.\nMoreover, as a main pratical advantage of this relaxation over the standard Semi-Definite Programming\napproach, several efficient bundle methods are available for this problem allowing to address problems of\nvery large dimension. The necessary tools from convex analysis are recalled and shown at work for handling\nthe problem of exactness of this relaxation. Two applications are described. The first one is the problem\nof binary image reconstruction and the second is the problem of multiuser detection in CDMA systems.\n\n1\n\nIntroduction\n\nSeveral problems in engineering and in particular signal and image processing necessitate to estimate binary\nvectors corrupted by some noise and can be simply addressed using the least squares principle under binarity\nconsraints. The resulting problem is a minimization of a quadratic form over {\u22121, 1}n, a problem which is\nknown to be N P -Hard in general. One of the main approaches to relax this problem into a convex one is the\nSemi-Definite Programming relaxation which has been extensively used in classification, pattern recognition\nand communication systems. Some of the main achievements in the study of the SDP relaxation were obtained\nby Goemans and Williamson [13] and [11]. However, solving a SemiDefinite Program in practice relies on\ninterior point methods which although enjoying nice theoretical convergence properties are limited to problems\nof size up to 500 \u00d7500. On the other hand, very pratically efficient bundle methods are available for the\neigenvalue relaxation of the same binary quadratic optimization problems. We refer the reader to [1] for a\ndiscussion of the pratical superiority of bundle methods for solving certain semi-definite programs such as the\nones appearing in the present paper. Despite this empirical fact in favor of the eigenvalue relaxation, one of\nthe main reasons most users prefer the SDP relaxation is that good primal binary solutions can be recovered\nusing Goemans and Williamson's randomized algorithm. The main motivation of the present paper is to show\nhow a solution of the SDP can be recovered from a solution of the eigenvalue relaxation. As a by product, a\nnew geometric interpretation of the randomized algorithm is proposed.\nPenalized binary least squares estimation problems are problems of the form\nmin ky \u2212 Axk2 + \u03bdxt P x s.t. x \u2208 {\u22121, 1}n,\n\nx\u2208Rn\n\n(1.0.1)\n\nwhere the vector y \u2208 Rm is the observed data, the matrix A \u2208 Rm\u00d7n represents the \"filter\", the vector x \u2208 Rn\nis the signal, or parameter vector, that has to be estimated, and the term \u03bdxt P x is a penalization term that\ncan often be interpreted as an a priori information in terms of Bayesian statistics.\nThis problem belongs to the larger class of minimization of quadratic forms over binary vectors which is\nknown to be N P -hard. Much work has been devoted to constructing Semi Definite Programming (SDP) based\nrelaxations for general quadratic binary problems. Semi-Definite programs are linear optimization problems over\nsymmetric matrices with real coefficients and with the additional convex constraint of positive semidefiniteness;\nsee for instance [6] or [2] for excellent introductions to convex programming and in particular SDP. SDP methods\nhave already played an important role in various topics inside signal processing problems and we refer to [3]\n\u2217 M. Chr\u00e9tien is with the Laboratoire de Math\u00e9matiques, UMR CNRS 6623 and Universit\u00e9 de Franche Comt\u00e9, 16 route de Gray,\n25030 Besan\u00e7on Cedex, France. Email: stephane.chretien@math.univ-fcomte.fr\n\u2020 M. Corset is with LabSAD Universit\u00e9 Pierre Mendes France, 1251 Avenue centrale. BP47 38040 Grenoble cedex 9, France.\nEmail: franck.corset@upmf-grenoble.fr\n\n\ffor a nice survey on possible applications. A common feature of essentially all the existing relaxations is that\nthey can be obtained using Lagrange duality which is a general methodology for obtaining lower bounds to\nhard minimization problems, as overviewed in [4] and [5].\nThe goal of the paper is to survey what is known about another Lagrangian duality based relaxation,\nnamely the eigenvalue relaxation, for this problem. This relaxation was first proposed by Delorme and Poljak\n[19] for the max-cut problem. See also the work of Poljak, Rendl and Wolkowics [7] for more details. The\nmain advantage of the eigenvalue relaxation over the SDP relaxation is that the eigenvalue relaxation can be\nsolved much faster than the SDP relaxation, as reported for instance in [1], [8], [9] and [10]. This remarkable\ncomputational tractability of the eigenvalue relaxation is the main motivation for writing this detailed survey.\nThe content of the paper is as follows. The second section is devoted to a rapid presentation of the relaxation\nand its relationship with Lagrangian duality. We also recall a simple and well known certificate for exactness\nof the relaxation, i.e. the fact that a globally optimal binary solution is obtained.\nThe third section details the relationships between the Semi-Definite Relaxation and the eigenvalue relaxation. The main result of this section is the following: a solution of the SDP relaxation can be recovered from\nthe solution of the eigenvalue relaxation. The case of inexact solutions to the eigenvalue relaxation is also\nstudied.\nThe forth section deals with the problem of recovering binary primal solutions from the dual scheme. We\nfirst give sufficient conditions under which strong duality holds and the eigenvectors of norm n + 1 associated\nto the maximum eigenvalue at optimality are binary solutions. Next, in the case where strong duality does not\nhold, we show that Goemans and Williamson's randomized algorithm has a very natural meaning when viewed\nin terms of the optimal eigenspace associated to the maximal eigenvalue in the eigenvalue relaxation.\nIn the last section, we propose simulation experiments in the case of binary image denoising and CDMA\nMultiuser Detection problems. The first of these problems has been previously approached by stochastic\nmethods based on Markov chains like simulated annealing and Metropolis Hastings schemes; see for instance\n[17] and the more recent work of Gibbs [14]. The approach discussed here was presented in [15]. Recently\na lot more problems have been addressed using the SDP relaxation in [16]. The results obtained so far are\nquite encouraging and the approach performs well on very dirty images. We prove hat strong duality holds\nfor the immage denoising problem, thus recovering back the polynomial solvability result of Greig, Porteous\nand Seheult as a special case and by a very different path. Passing to the second problem, our Monte Carlo\nexperiments show that the average computational effort for solving the eigenvalue relaxation as a function of\nthe number of users grows slowlier than for the SDP relaxation with the standard implementations available\nwith scilab.\nNotations. In the sequel we will use the following notations. The inner product on Rn is denoted by h*, *i,\nthe set of real symmetric matrices of order n are denoted by Sn . The partial order < denotes the Loewner\nordering, i.e. for A and B in Sn , A < B means that A \u2212 B is positive semidefinite. For a set S in Rn , conv(S)\ndenotes the convex hull of S and S denotes its closure. For a matrix A in Sn , d(A) denotes its diagonal vector\nand for a in Rn , D(a) denotes the diagonal matrix whose diagonal vector is a. If an equation number #\ncorresponds to an optimization problem, then opt(#) will denote the optimum value for this problem.\n\n2\n\nThe eigenvalue relaxation\n\nWe first introduce the eigenvalue relaxation and at the same time, we propose a quick refresher on Lagrangian\nduality, collecting all the results that will play an essential role in the sequel. The proofs of almost all the\nresults presented here can be found in [18].\n\n2.1\n\nThe Lagrangian dual and the eigenvalue relaxation\n\nThe binary least-squares estimation problem is in fact equivalent to the homogenized problem\n\u0014 t\n\u0015\nA A + \u03bdP \u2212At y\nt\nx s.t. x \u2208 {\u22121, 1}n+1.\n(BLS)\nmaxx\u2208Rn+1 \u2212 x\n\u2212y t A\nyty\nIndeed, if we add the constraint xn+1 = 1 in (BLS), we obtain exactly the binary least squares problem. Now,\nif x\u2217 is a solution of (BLS), then \u2212x\u2217 is again a solution of of (BLS), thus adding the constraint xn+1 = 1 is\nin fact redundant, which proves the claimed equivalence. Set\n\u0014 t\n\u0015\nA A + \u03bdP \u2212At y\nM=\n.\n\u2212y t A\nyty\n\n\fNotice further that the constraint xi \u2208 {\u22121, 1} is equivalent to x2i = 1 for all i = 1, * * * , n + 1. Thus, to\nproblem (BLS), we can associate the Lagrangian function\nP\n2\nL(x, u) = \u2212xt M x + n+1\ni=1 ui (xi \u2212 1)\nt\nt\n= x (D(u) \u2212 M )x \u2212 u e.\nNow we can add to the problem the implicit spherical constraint\nSn+1 = {x \u2208 Rn+1 | xt x = n + 1},\nwhich is redundant with the binary constraints. Then, optimizing over this sphere, we obtain the Lagrangian\ndual function, i.e.\n\u03b8(u) = maxx\u2208Sn+1 xt (D(u) \u2212 M )x \u2212 ut e\nut e t\n(D(u) \u2212 M )x \u2212 n+1\n= maxx\u2208Sn+1 xt \u0010\n\u0011x x\n= maxx\u2208Sn+1 xt D(u) \u2212 M \u2212\n\nut e\nn+1 I\n\nx\n\nwhich, using Raleigh-Ritz variational formulation of the largest eigenvalue of symmetric matrices, can be written\n\u0010\nut e \u0011\n\u03b8(u) = (n + 1)\u03bbmax D(u) \u2212 M \u2212\nI .\nn+1\n\n(2.1.1)\n\nFinally, the dual problem, i.e. the eigenvalue relaxation, is given by\nmin \u03b8(u).\n\n(2.1.2)\n\nu\u2208Rn+1\n\n2.2\n2.2.1\n\nProperties of the dual relaxation\nConvexity\n\nIt is important to notice first that the dual function \u03b8(u) is convex, since it is the maximum over a family\nparametrized by x \u2208 Sn+1 of linear functions in the variable u.\n2.2.2\n\nWeak duality\n\nThe main classical property of the Lagrangian dual is weak duality, i.e.\nmin \u03b8(u) \u2265 opt(BLS),\n\nu\u2208Rn+1\n\nwhere opt denotes the optimal value.\nThis property explains in part why Lagrange duality is used : it provides a bound on the primal optimal\nvalue. When equality holds in the weak duality property, we say that strong duality holds. Sometimes, like in\nthe case of the Max-Cut problem, the bound can be proved to be proportional to the optimal original value.\nMore precisely, Goemans and Williamson proved that the optimum value of the eigenvalue relaxation (in fact\nthe equivalent SDP formulation; see the original paper and Section 3 below) is greater than or equal to the\noptimal original value (this is just weak duality), which itself is always greater than or equal to .876 times the\neigenvalue relaxation's optimal value. A quite similar but less tight bound, proved by Nesterov applies directly\nto the present problem. We will recall this bound in section 4.2.1 below.\n2.2.3\n\nExistence of dual solutions\n\nIt is well known that there exists an optimal dual solution. This was proved by Poljak and Wolkowicz in [20].\nThe proof given here is more direct.\nProposition 2.2.1 The dual function admits a minimizer.\nProof. Let \u03b8\u2217 = inf u\u2208Rn+1 \u03b8(u). Make the change of variable v = u \u2212\n\n1\nn+1\n\n\u03b7(v) = (n + 1)\u03bbmax (D(v) \u2212 M ) = \u03b8(u).\n\nPn+1\ni=1\n\nui , i.e. define\n\nPn+1\nk\nWe now have the property that\ni=1 vi = 0. We prove that \u03b7 is coercive. Take any sequence (v )k\u2208N\nk\nwith kvk k \u2192 +\u221e as k \u2192 +\u221e. We can assume that vi \u2192 +\u221e for some i because otherwise, the fact that\nP\nkvk k \u2192 +\u221e implies that there must exists a sequence (vjk )k\u2208N with vjk \u2192 \u2212\u221e and the fact that n+1\ni=1 vi = 0\n\n\fgives a contradiction. Now, the Gershgorin circle around the diagonal element Mi,i + vik has a constant radius,\nsay r and its center goes to +\u221e. Since |Mi,i + vik \u2212 \u03bbmax (D(v) \u2212 M )|, this implies that \u03bbmax (D(v) \u2212 M ) \u2192 +\u221e.\nThus \u03b7 is coercive and\nsince it is continuous, it admits a minimizer that we will denote by v \u2217 . Now, for all\nPn+1\n1\nu \u2208 R, v = u \u2212 n+1 i=1 ui , we have\n\u03b8\u2217 \u2264 \u03b8(v \u2217 )\nBut, on the other hand, \u03b8(v \u2217 ) = \u03b7(v \u2217 ) \u2264 \u03b7(v) = \u03b8(v) = \u03b8(u). Therefore,\n\u03b8(v \u2217 ) \u2264 \u03b8\u2217\nand the proof is complete.\n2.2.4\n\n\u0003\n\nSubdifferential's description and exactness criterion\n\nThe subdifferential \u2202\u03b8(u) of the eigenvalue relaxation has been much studied. Recall that for any convex\nfunction f : Rm 7\u2192 R, the subdifferential \u2202f (u) is defined by\nn\no\n\u2202f (u) = g \u2208 Rm | f (u\u2032 ) \u2265 f (u) + g t (u\u2032 \u2212 u) .\nThe analysis of \u2202\u03b8(u) is based on the following general theorem.\nTheorem 2.2.2 [18] Let A : Rm 7\u2192 Sn be an affine operator defined by A(u) = Au+B for some linear operator\nA : Rm 7\u2192 Sn and some matrix B \u2208 Sn . Then, we have\n\u2202(\u03bbmax \u25e6 A)(u)) = A\u2217 \u2202\u03bbmax (A(u))\nwith\n\n\u2202\u03bbmax (X) =\no\nn\nt\nEmax Z \u2208 Srmax | Z < 0 and trace(Z) = 1 Emax\n\nwhere A\u2217 is the adjoint of A, rmax denotes the multiplicity of \u03bbmax at X \u2208 Sn and Emax is a matrix whose\ncolumns form any orthonormal basis of the eigenspace of X associated to \u03bbmax .\nt\n\nt\n\nu e\n1\nu e\nI, we get B = \u2212M , Au = D(u)\u2212 n+1\nI and A\u2217 X = d(X)\u2212 n+1\ntrace(X)e.\nNow, if we set A(u) = D(u)\u2212M \u2212 n+1\nFor d \u2208 N, let Zd be defined by\nn\no\nZd = Z \u2208 Sd | Z < 0 and trace(Z) = 1 .\n\nUsing the previous theorem, we obtain\nCorollary 2.2.3 The subdifferential \u2202\u03b8(u) of the dual function \u03b8 is given by\nt\nt\n\u2202\u03b8(u) = (n + 1)d(Emax ZEmax\n) \u2212 trace(Emax ZEmax\n)e\n\nFollowing Oustry [8], the formula for \u2202\u03bbmax (X) in theorem 2.2.2 is proved by showing that the maximum\neigenvalue function \u03bbmax (X) on Sn is nothing but the support function \u03c3Zn (X) of Zn , defined by\n\u03c3Zn (X) = sup hX, Zi\nZ\u2208Zn\n\nwith the scalar product defined by hX, Zi = trace(X, Z). By definition, the face FZn (X) of Zn exposed by X\nis the set of maximizers in (2.2.1), i.e.\nn\no\nFZn (X) = Z \u2208 Zn | \u03bbmax (X) = hX, Zi .\nKnowing that the subdifferential of a support function of a set is exactly the exposed face of this set, we finally\nget\nn\no\n\u2202\u03bbmax (X) = Z \u2208 Zn | \u03bbmax (X) = hX, Zi\n\nthe formula follows after some linear algebra.\nThere is a different path to the subdifferential's formula, which is perhaps more a propos in the context of\nduality: it is proved in [18, Chapter XII] that\nn\no\n(2.2.1)\n\u2202\u03b8(u) = conv (x21 \u2212 1, * * * , x2n+1 \u2212 1)t | L(x, u) = \u03b8(u) ,\n\n\fwhere conv denotes the closure of the convex hull. This fact is in fact true for general continuous constrained\nproblems in the case where the underlying space is compact (for example) 1 and the associated technical\ncondition is called the filling property. The following proposition provides a useful sufficient condition for\nproving that the relaxation is exact, i.e. strong duality applies.\nProposition 2.2.4 Let u\u2217 be a minimizer of the dual eigenvalue relaxation. Then, if \u03bbmax (A(u\u2217 )) has multiplicity one, then\nmin \u03b8(u\u2217 ) = opt(BLS)\nu\u2208Rn+1\n\n\u2217\n\nand any eigenvector x of A(u ) whose squared norm is n + 1 is a binary solution of (BLS).\nThe proof is a direct consequence of [18, Theorem XII.2.3.4.]. We provide a specialized proof here because it\nis short and instructive.\nProof. Since the multiplicity of \u03bbmax (A(u\u2217 )) is one, the subdifferential of \u03bbmax \u25e6 A at u\u2217 is a single vector.\nThus, \u03b8 is differentiable at u\u2217 and its gradient is simply\n\u2207\u03b8(u\u2217 ) = (x\u22171 2 \u2212 1, * * * , x\u2217n+1 2 \u2212 1)t\nfor any x\u2217 in Sn+1 such that \u03b8(u\u2217 ) = L(x\u2217 , u\u2217 ). Since, u\u2217 minimizes \u03b8, we must have \u2207\u03b8(u\u2217 ) = 0. This implies\nthat x\u2217i 2 = 1 for all i = 1, * * * , n + 1. Thus, using weak duality\nopt(BLS) \u2264 \u03b8(u\u2217 ) = x\u2217 t (\u2212M )x\u2217 \u2264 opt(BLS)\nwhich proves that x\u2217 solves the original problem (BLS).\n\u0003\nWe now have a nice criterion for deciding whether our relaxation was exact and if so, we also know how to\nrecover a binary solution from an optimal eigenvector. This approach works for any quadratic binary problem\nand is extensively used for approximating combinatorial problems. However, the question remains on what to\ndo when the relaxation is not exact, i.e. when the multiplicity at the optimum is greater than one. The next\ntwo sections will help answer this crucial question.\n\n3\n\nFrom eigenvectors to SDP solutions\n\nThe purpose of the next two sections is to describe how to recover primal binary solutions from the eigenvector\nsolutions of the dual eigenvalue problem. It was first shown that good binary solution can be generated at\nrandom using the SDP solution by Goemans and Williamson [13] in the case of the Max-Cut problem in graph\ntheory. Their results were then extended by Nesterov to the case of indefinite quadratic binary programming\n[11]. Those results allowed to conclude that both eigenvalue and SDP relaxations are in a certain precise sense\nvery efficient. However, both relaxations are not equivalent from the computational point of view. Recall that\none of the main motivations for using the eigenvalue relaxation is its manageable practical complexity which\nis often favorable compared to the one of solving the SDP relaxation. But what is not clear is how to generate\ngood (primal) binary solutions in average with the eigenvalue relaxation only ? The first natural approach to\nthis question is of course to try and recover an optimal SDP solution from the eigenvalue relaxation. Thus,\nwe devote this section to this problem. It can be solved as follows : an appropriate convex combination of\nrank one matrices obtained from a set of optimal eigenvectors is shown to be a solution we are looking for.\nOur approach simplifies the presentation of [21]. The adaptation of the randomized algorithm of Goemans and\nWilliamson and the associated bound established by Nesterov will be discussed in the next section.\n\n3.1\n\nThe SDP relaxation\n\nIn order to obtain the Semi-Definite Programming (SDP) relaxation of the the homogenized problem (BLS),\nwe begin with the following equivalence relating our problem to a problem on symmetric matrices. We have 2\nopt(BLS) = max\ntrace(\u2212M xxt ) s.t. d(xxt ) = e.\nn+1\nx\u2208R\n\nThis last problem is itself equivalent to\nmax trace(\u2212M X) s.t. d(X) = e, X < 0, rankX = 1.\n\nX\u2208Sn+1\n1 which\n2 Here,\n\nis the case here since we optimize over the sphere Sn+1\nwe use the fact that xt M x = trace(xt M x) = trace(M xxt )\n\n\fThis problem being nonconvex, we drop the rank constraint and obtain the following SDP (convex) relaxation\nmax trace(\u2212M X) s.t. d(X) = e, X < 0\n\nX\u2208Sn+1\n\n(SDP)\n\nwhose value is obviously greater than or equal to val(BLS).\nAn important result of Pataki [36, Theorem 2.1] gives a bound on the rank of solutions to Semi-Definite\nPrograms. In the case of our Semi-Definite relaxation, this theorem implies that the rank r\u2217 of an optimal\nmatrix X \u2217 satisfies 21 r\u2217 (r\u2217 + 1) \u2264 n.\n\n3.2\n\nSDP versus maximal eigenvalue : theoretical equivalence\n\nIt follows from the subdifferential's formula given in Corollary 2.2.3 that at any minimizer u\u2217 , we have\n(n +\n\n0 \u2208 \u2202\u03b8(u\u2217 ) =\nt\n\u2217\n\u2217\n\u2217\n\u2212 trace(Emax\nZrmax\nEmax\n)e.\n\nt\n\u2217\n\u2217\n\u2217\n1)d(Emax\nZrmax\nEmax\n)\n\n\u2217\nSuppose we have in hand a matrix Z \u2217 \u2208 Zrmax\nsuch that\n\nt\n\u2217\n\u2217 t\n\u2217\n\u2217\n0 = (n + 1)d(Emax\nZ \u2217 Emax\n) \u2212 trace(Emax\nZ \u2217 Emax\n)e.\n\n(3.2.1)\n\nIt appears that a good guess for a candidate solution X \u2217 to the SDP relaxation in the general case is\nt\n\u2217\n\u2217\nX \u2217 = (n + 1)Emax\nZ \u2217 Emax\n.\n\nWe just need to check the details to see how it works. This result was initially proved in [21] but the proof\ngiven here is more direct.\nTheorem 3.2.1 [21] Let u\u2217 be the optimal solution of the eigenvalue relaxation let Emax be a matrix whose\ncolumns for an orthonormal basis of the eigenspace associated to \u03bbmax (A(u\u2217 )) and let Z \u2217 be as in (3.2.1). Then\nt\n\u2217\n\u2217\nZ \u2217 Emax\nis an optimal solution of the SDP relaxation.\nthe matrix X \u2217 = (n + 1)Emax\nRemark 3.2.2 We would like to underline at this point that a more elegant proof of the theorem could be\nobtained using conic duality but we preferred to keep on with elementary arguments since this is possible in the\npresent context.\n\u2217\nProof. Compute the eigenvalue/eigenvector decomposition Z \u2217 = U \u2206U t , set F = Emax\nU , \u03b4 = d(\u2206), let r be\n\u2217\nthe multiplicity\nof\nA(u\n)\nand\nlet\nf\n,\n*\n*\n*\n,\nf\ndenote\nthe\ncolumns\nof\nF\n.\nRecall\nthat\nfrom\nthe\ndefinition of Z \u2217 , we\n1\nr\nPr\nhave j=1 \u03b4j = 1. Then, we get\n\n0 = d(F \u2206F t ) \u2212\n\n1\ntrace(F \u2206F t )e.\nn+1\n\nThus,\n\u0011\n\u0010\n1\n(u\u2217 )t eI)F \u2206F t =\ntrace (D(u\u2217 ) \u2212 n+1\n1\ntrace(F \u2206F t )e = 0.\n(u\u2217 )t d(F \u2206F t ) \u2212 (u\u2217 )t n+1\nUsing this fact, we obtain\ntrace(\u2212M X \u2217 ) \u0010\n\n1\n= (n + 1)trace (\u2212M + D(u\u2217 ) \u2212 n+1\n(u\u2217 )t eI)F \u2206F t\n= (n + 1)trace(A(u\u2217 )FP\u2206F t )\nr\n= (n + 1)trace(A(u\u2217 ) j=1 \u03b4j fj fjt )\nPr\nt\n\u2217\n= (n + 1) j=1 \u03b4j fj A(u )fj\nP\n= (n + 1) rj=1 \u03b4j \u03bbmax (A(u\u2217 ))\n= (n + 1)\u03bbmax (A(u\u2217 )),\n\n\u0011\n\nPr\nsince j=1 \u03b4j = 1. Thus, the optimal value of the SDP is greater than or equal to the optimal value of the\neigenvalue relaxation. On the other hand, it is well known that the optimal value of the eigenvalue relaxation\nis greater than or equal to the one of the SDP relaxation. We provide a proof here for the sake of completeness.\nLet X \u2217\u2217 be an optimal solution to the SDP relaxation. Now, for all u in Rn+1 , we have\n\u0010\net u \u0011\ntrace X \u2217\u2217 (D(u) \u2212\nI) = 0\nn+1\n\n\fby using the fact that D(X \u2217\u2217 ) = e. Now, compute the eigenvalue/eigenvector decomposition \u2212M + D(u) \u2212\nPn+1\net u\nt\ni=1 \u03bbi vi vi and let \u03bbmax be the greatest of these eigenvalues. Then,\nn+1 I =\n\u0011\n\u0010\net u\nI)\ntrace(\u2212M X \u2217\u2217 ) = trace X \u2217\u2217 (\u2212M + D(u) \u2212 n+1\nP\n= n+1\n\u03bbi vit X \u2217\u2217 vi\ni=1P\nt \u2217\u2217\n\u2264 \u03bbmax n+1\ni=1 vi XP vi\nn+1\n\u2217\u2217\nt\n= \u03bbmax trace(X\ni=1 vi vi )\n\u2217\u2217\n= \u03bbmax trace(X I)\n= (n + 1)\u03bbmax\nSince this is true for all u, we obtain that the eigenvalue relaxation majorates the SDP relaxation. Thus, both\noptimal values are equal and this completes the proof of the proposition.\n\u0003\n\n3.3\n\nSDP versus maximal eigenvalue: practical implementation\n\n\u2217\nOf course, it can be hard to find a matrix Z \u2217 \u2208 Zrmax\nS that works. We will now try to overcome this problem.\nWe first have to specify how the subgradients are obtained in practice. At each point u \u2208 Rn+1 , choose an\neigenvector x of squared norm equal to n + 1 associated to \u03bbmax (A(u)). Then, using the alternative representation of the subdifferential (2.2.1), a subgradient of \u03b8 at u is obtained by setting g = [x1 2 \u2212 1, . . . , xn+1 2 \u2212 1]t .\n2\n2\nAssume that we have a set of subgradients gj = [xj1 \u2212 1, . . . , xjn+1 \u2212 1]t \u2208 \u2202\u03b8(uj ) for some uj , j = 1, . . . , p\nand such that\np\nX\n\u03b1j gj k \u2264 \u01eb,\n(\u01ebOPT)\nk0 \u2212\n\ni=1\n\nPp\nfor some nonnegative \u03b1j 's with j=1 \u03b1j = 1. This can be performed for \u01eb as small as we want by using a\nbundle method. Such a method will construct in a finite number of iterations, say k, an iterate uk and a family\nof uj 's with the desired property, all of them lying in a small neighborhood of uk . This is one very nice feature\nof the bundle mechanism which is extensively described in [18, Volume II]. Moreover, it is a well known fact,\ncalled Caratheodory's theorem, that only p = n + 2 subgradients are sufficient in the expression (\u01ebOPT).\nSet\np\nX\nt\n\u03b1j xj xj .\nX\u01eb\u2217 =\nj=1\n\nThen, we have the following result.\nProposition 3.3.1 For any \u01eb > 0, the matrix X\u01eb\u2217 defined above satisfies\ntrace(M X\u01eb\u2217 ) \u2264 min\n\u03b8(u) \u2212 O(\u01eb).\nn+1\nu\u2208R\n\n\u2217\n\nProof. Let u be any minimizer of \u03b8. Then, for each j = 1, . . . , p, we have by the definition of the subdifferential\n\u03b8(u\u2217 ) \u2265 \u03b8(uj ) + gjt (u\u2217 \u2212 uj ).\nBut \u03b8(uj ) is given by\n\u03b8(uj ) = xj\n\nt\n\n\u0010\n\nD(uj ) \u2212 M \u2212\n\net u j \u0011 j\nI x .\nn+1\n\nt\n\nOn the other hand, since xj xj = n + 1,\n\u0010\n\n\u0011\nt j\nu\nD(uj ) \u2212 M \u2212 en+1\nI xj\nPn+1\nPn+1\n2\nt\n= xj M xj + i=1 ui xji \u2212 i=1 ui\nPn+1\n2\nt\n= xj M xj + i=1 ui (xji \u2212 1)\nt\n= xj M xj + gjt uj .\nxj\n\nThus, we obtain\n\nt\n\nt\n\n\u03b8(u\u2217 ) \u2265 trace(M xj xj ) + gjt u\u2217\n\nwhich implies, after multiplying by \u03b1j and summing over j = 1, . . . , p\n\np\nX\n\u03b1j gj )t u\u2217 .\n\u03b8(u\u2217 ) \u2265 trace(M X\u01eb\u2217 ) + (\nj=1\n\n\fUsing Cauchy-Schwartz inequality, this gives\n\u03b8(u\u2217 ) \u2265 trace(M X\u01eb\u2217 ) + \u01ebku\u2217 k.\nSince the eigenvalue and the SDP relaxation have equal optimal values, we finally obtain\nopt(SDP) \u2265 trace(M X\u01eb\u2217 ) + \u01ebku\u2217 k\nwhich implies the desired result.\n\n3.4\n\n\u0003\n\nComments\n\nIt is a common idea that the SDP relaxation contains more information than the eigenvalue relaxation. We\nhope that the results of this section managed to convince the reader that this is in fact not the case and a good\napproximate solution can be recovered quite easily using subgradient information at the optimum.\n\n4\n\nRecovering primal binary solutions\n\nWe now are in position to answer our main question of how to recover a satisfactory although sometimes\nsuboptimal primal binary solution. In the first part of this section, we show that optimal binary solutions\ncan actually be exactly recovered using the eigenvalue relaxation, i.e. strong duality holds, under some simple\nconditions. Then, in the case where the problem does not satisfy these necessary conditions for strong duality,\nwe develop a randomized algorithm based on the optimal eigenspace of the maximum eigenvalue dual function\nand show that this procedure is equivalent to Goemans and Williamson's randomized algorithm for Max-Cut.\nThis provides a new interpretation of Goemans and Williamson's procedure.\n\n4.1\n\nA sufficient conditions for strong duality\n\nWe have the following theorem.\nTheorem 4.1.1 For almost all A in the sense of the Lebesgue measure, such that At A + \u03bdP is componentwise\nnegative outside the diagonal. Then the eigenvalue relaxation is exact, i.e. strong duality holds.\nProof. Fix u \u2208 Rn+1 . Let un1 be the vector of the first n components of u. The fact that At A + \u03bdP is\ncomponentwise negative outside the diagonal implies that \u2212At A \u2212 \u03bdP + D(un1 ) \u2212 min(un1 )I is componentwise\npositive. Thus, the Perron-Frobenius theorem implies that the maximum eigenvalue of \u2212At A \u2212 \u03bdP + D(un1 ) \u2212\nmin(un1 )I has multiplicity one. From this, we deduce that the maximum eigenvalue of \u2212At A \u2212 \u03bdP + D(un1 )\nalso has multiplicity one. Let Vun1 Dun1 Vutn1 be an eigenvalue decomposition of At A + \u03bdP + D(un1 ), where we\nused the subscript un1 in order to remember that whatever the chosen decomposition, it is a nonlinear and non\nnecessarily continuous function of u. Moreover, since the maximum eigenvalue has multiplicity one, Corollary 4\nin [22] says that it is possible to choose the eigenvector associated to the maximum eigenvalue as a continuously\ndifferentiable function of un1 . We will denote by vumax\nthis eigenvector. Using this parametrization, the matrix\nn\n1\n\u0014 t\n\u0015\nA A + \u03bdP \u2212At y\n\u2212 M + D(u) = \u2212\n+ D(u)\n\u2212y t A\nyty\ncan be rewritten as\n\u2212 M + D(u) =\n\n\u0014\n\nVun1\n0\n\n0\n1\n\n\u0015\u0014\n\nDun1\n\u2212y t AVun1\n\n\u2212Vutn1 At y\ny t y + un+1\n\n\u0015\u0014\n\nVun1\n0\n\n0\n1\n\n\u0015t\n\n,\n\nwhere all dimensions can easily be guessed from the previous knowledge on the involved submatrices.\nLet V be the codimension one differentiable submanifold defined by\nV = {(A, u) \u2208 Rm\u00d7n \u00d7 Rn+1 | y t Avumax\n= 0}.\nn\n1\nLet W be the optimal set defined by\nW = {(A, u) \u2208 Rm\u00d7n \u00d7 Rn+1 | 0 \u2208 \u2202\u03b8(u)}.\nDue to the representation\n\u2202\u03b8(u)\n\nt\n= {Vmax ZVmax\n| A \u2208 Rm\u00d7n , u \u2208 Rn+1 , V \u2208 R(n+1)\u00d7rmax , Z \u2208 Srmax , Z \u0017 0,\nt\n(\u2212M + D(u))Vmax = \u03bbVmax , Vmax\nVmax = I, trace(Z) = 1},\n\n\fthe set W is the projection onto the cartesian product {(A, u) \u2208 Rm\u00d7n \u00d7 Rn+1 } of the set \u222aR\nR is\nr=1 W\u0303r where\n\u221a\nthe upper bound of Pataki (see Section 3.1) on the optimal rank of the SDP relaxation3 (here R \u2264 2n for n\nlarge) and where W\u0303r is the set\nW\u0303r\n\n= {(A, u, V, \u03bb, Z) | A \u2208 Rm\u00d7n , u \u2208 Rn+1 , V \u2208 R(n+1)\u00d7r , Z \u2208 Sr , (\u2212M + D(u))V = \u03bbV,\nV t V = I, trace(Z) = 1, (n + 1)d(V ZV t ) + trace(V ZV t )e = 0},\n\nwhose intersection with {(A, u, V, \u03bb, Z) | A \u2208 Rm\u00d7n , u \u2208 Rn+1 , V \u2208 Rn\u00d7r , Z \u2208 Sr , Z \u0017 0} corresponds to the\nparameter set allowing for zero to belong to the subdifferential of the dual function \u03b8 in the case where u =\n\u03bbmax (\u2212M +D(u)). Now, since the constraint (\u2212M +D(u))V = \u03bbV is described by (N +1)r equations, V t V = I\nby r (r+1)\nequations, trace(Z) = 1 by one equation and (n + 1)d(V ZV t ) + trace(V ZV t )e = 0, the dimension of\n2\n(r+1)\nW\u0303r is greater than or equal to m\u00d7n+(n+1)+(n+1)\u00d7r+1+r\u00d7 (r+1)\n2 \u2212(n+1)\u00d7r\u2212r\u00d7 2 \u22121+(n+1) = m\u00d7n.\nFurthermore, notice that since the eigenvalues are continuous fonctions of the entries of \u2212M + D(u), the subset\nof \u222aR\nr=1 W\u0303r for which u = \u03bbmax (\u2212M + D(u)) is open in the topology induced by the ambiant space. Therefore\nits projection set onto the cartesian product {(A, u) \u2208 Rm\u00d7n \u00d7 Rn+1 } is of dimension at least m \u00d7 n which\ngarantees that the projection onto the A-space {A \u2208 Rm\u00d7n } of its intersection with V is a set of null Lebesgue\nmeasure. And thus, for almost all A, such that At A + \u03bdP is componentwise negative outside the diagonal,\n6= 0.\ny t Avumax\nn\n1\nUsing this result, Theorem A about the interlacing property of the eigenvalues for arrow matrices in the\nAppendix implies that the maximum eigenvalue of M + D(u) is greater than the maximum diagonal element\nof Dun1 which nothing by \u03bbmax (\u2212(At A + \u03bdP ) + D(un1 )) and all n other eigenvalues are less than \u03bbmax (\u2212(At A +\n\u03bdP ) + D(un1 )). This implies that for allmost all A, the maximum eigenvalue of M + D(u) has multiplicity one\nat the optimum, which implies that \u03b8 is differentiable at the optimum. Therefore, using Proposition 2.2.4 we\nobtain that strong duality holds for allmost all A such that At A + \u03bdP is componentwise negative outside the\ndiagonal.\n\u0003\n\n4.2\n\nWhen strong duality fails: the randomized algorithm\n\nWe start this section with some recalls on Goemans and Williamson's algorithm and Nesterov's bound.\n4.2.1\n\nGoemans and Williamson's algorithm and Nesterov's bound\n\nThe method relies on the Cholesky factorization of the optimal solution X \u2217 of the SDP relaxation,\nX \u2217 = V t V.\nFrom Theorem 3.2.1 we see that V \u2208 R(n+1)\u00d7rmax where rmax is the multiplicity of \u03bbmax (A(u\u2217 )) at the chosen\ncorresponding solution u\u2217 of the eigenvalue relaxation. This factorization is important, since it allows to write\n\u2217\nXij\n= vit vj where vi is the transpose of ith row vector of V . Let \u03be be a random variable with uniform distribution\non the unit sphere in Rrmax .\nProcedure 4.2.1 (Goemans and Williamson's algorithm)\n1. Find the Cholesky factorization X \u2217 = V t V .\nLet \u03b6 be a random vector with uniform distribution on the unit sphere of S(0, 1). The random cut is defined\nby\n\u0010\n\u0011\nZ = sign V t \u03b6 .\nwhere the sign function is defined coordinate-wise.\n2. Draw n samples from Z, say z 1 , . . . , z n and choose the sample giving the best value of the objective\nfunction z t M z.\nThe key result is that, in average, the vector Z gives a good binary solution to the original problem. Since the\nbest sample will have greater cut value than the average with overwhelming probability, the above procedure\nshould work well. This is made precise by Nesterov's theorem.\nTheorem 4.2.2 (Nesterov) Define\nf \u2217 = max xt M x s.t. x \u2208 {\u22121, 1}n+1\nx\u2208Rn+1\n\n3 which\n\nalso holds for the eigenvalue relaxation due to the complete equivalence between these two problems\n\n\fand\nf\u2217 = min xt M x s.t. x \u2208 {\u22121, 1}n+1\nx\u2208Rn+1\n\nthen, we have\n\n2\nf \u2217 \u2212 E[z t M z]\n\u2264 .\nf \u2217 \u2212 f\u2217\n\u03c0\n\nThis result is remarkable despite the fact that the bound \u03c02 is rather large. An important issue for future\nresearch is to study such type of bounds for particular subclasses of problems in hope of improving Nesterov's\nresult.\n4.2.2\n\nThe eigenvector viewpoint\n\nThe main drawback of the former presentation is that using the uniform variable \u03be is quite hard to motivate\nfrom an optimization viewpoint. Let us take a slightly different perspective. Assume that we have a solution\nu\u2217 of the eigenvalue relaxation. As before, let Emax be a matrix whose columns form an orthonormal bases of\nthe eigenspace associated to \u03bbmax (A(u\u2217 )). Moreover, we may require that\nt\n0 = A\u2217 (Emax \u2206Emax\n),\n(4.2.1)\nPrmax\n\u03b1i = 1. In the case where the multiplicity\nwhere \u2206 is some diagonal matrix with \u03b1 = d(\u2206), \u03b1 \u2265 0 and i=1\nat the optimum is one, the\u221aoptimal eigenbasis reduces to a unique vector and we saw in Proposition 2.2.4 that\nmultiplying this vector by n + 1 gives a binary solution. Now let us turn to the case where there are rmax > 1\neigenvectors. To each unit norm eigenvector ej , we associate a subgradient gj = [(n + 1)(ej1 )2 \u2212 1, . . . , (n +\n1)(ejn+1 )2 \u2212 1]t . Then, (4.2.1) implies that\nrX\nmax\n\u03b1j gj .\n0=\nj=1\n\nNow one natural strategy might be the following: pick the best eigenvector, i.e. the eigenvector\n\n\u221a\nn + 1ej0\n\ng1\nb\n\nb\n\n0\n\ng3\nb\n\nb\n\ng2\n\nFigure 1: Three subgradients in R2 at the optimal dual solution, one convex combination of which gives zero.\nwhose associated coefficient \u03b1j0 in expression (4.2.2) is the greatest and round its\u221acoordinates to the nearest\nbinary values. There is a second strategy : draw random linear combinations of the n + 1ej 's giving preference\nto the components with higher associated coefficient in (4.2.2). This can be done by sampling vectors of the\ntype\nrX\nmax\n\u221a\n\u03b6j n + 1ej\nj=1\n\nwhere the \u03b6j 's are independent random variables with distribution N (0, \u03b1j ). For each sample, a feasible solution\nis obtained by rounding off the components to the nearest binary. We sum up this procedure as follows.\nProcedure 4.2.3 (Randomized algorithm based on optimal eigenvectors) 1. Find the matrix Emax\nwhose columns form an\neigenbasis associated to \u03bbmax (A(u\u2217 )) such that (4.2.2) holds for some \u03b1j 's\nProrthonormal\nmax\nsatisfying \u03b1 \u2265 0 and j=1 \u03b1j = 1.\n2. Let \u03b6 be a random vector with distribution N (0, D(\u03b1)). The random cut is defined by\n\u0011\n\u0010\u221a\nZ = sign n + 1Emax \u03b6 .\n3. Draw n samples from Z, say Z 1 , . . . , Z n and choose the sample giving the best value of the objective\nfunction z t M z.\n\n\fThe important result is that this second strategy is equivalent to Goemans and Williamson's randomized\nprocedure.\nProposition 4.2.4 Procedure 4.2.3 is equivalent to Goemans and Williamson's algorithm.\n1\n\nProof. Set W = Emax D(\u03b1) 2 . Then Theorem 3.2.1 and equation 4.2.2 imply that X \u2217 = V t V with V t = W ,\n1\nthus retrieving the Cholesky factorization of X \u2217 . Let \u03be = D(\u03b1)\u2212 2 \u03b6. It is clear that \u03be has distribution N (0, I).\nThis proves that the cut Z obtained by Procedure 4.2.3 is exactly the output of Goemans and Williamson's\nprocedure.\n\u0003\nThe eigenvalue point of view thus allowed us to provide an alternative and geometric explanation for taking\na random cut using a uniformly distributed variable on the sphere in Goemans and Williamson's methodology.\n\n5\n\nTwo application examples\n\nIn this section, we provide some results for the concrete problems of image denoising and show how this\nrelaxation applies to the problem of multiuser detection in CDMA systems.\n\n5.1\n5.1.1\n\nImage denoising\nPresentation of the problem\n\nThe first set of simulations is devoted to the denoising problem, in which A is simply the identity matrix.\nThis is the problem considered in [23], [14] and [17] for instance. The original binary image as 26 rows and 62\ncolumns which gives a total number of 1612 variables.\nFor this problem, the penalization matrix P is chosen so as to smooth the image. This is achieved by\nrequiring neighboring pixels to be similar in the sense that if i and j are indices of neighbor pixels, then, we\nwould like the least square cost to be penalized by the quantity |xi \u2212 xj |2 . Thus, P is the matrix associated to\nthe quadratic form\nX\n\u03b6ij |xi \u2212 xj |2 ,\n(5.1.1)\ni\u223cj\n\nwhere i \u223c j denotes the property of being neighbor indices and the \u03b6ij are nonnegative. The neighborhood of\neach pixel is usually chosen to be the north, south, east and west pixels.\n5.1.2\n\nExactness of the relaxation\n\nThe following theorem is the main result of this section.\nTheorem 5.1.1 For A = I, the identity matrix and P the matrix associated to the quadratic form (5.1.1), the\neigenvalue relaxation is exact.\nProof. The eigenvalue relaxation of the optimization problem corresponding to this binary least square denoising problem is as before\nmin (n + 1)\u03bbmax (\u2212(M + \u03bdP +\n\nu\u2208Rn+1\n\net u\nI) + D(un1 )).\nn+1\n\n(Denoise)\n\nConsider now the perturbed optimization problem\nmin\n(n + 1)\u03bbmax (\u2212(M + \u2206M +\nn+1\n\nu\u2208R\n\net u\nI) + D(un1 )+)\nn+1\n\n(P erturbed)\n\nwhere \u2206M is negative outside the diagonal. Since the \u03b6ij are nonnegative, the matrix P has only nonpositive\noff diagonal terms and thus, Theorem 4.1.1 proves that strong duality holds for this problem and there exists\na binary eigenvector that achieves optimality. Assume that \u2206M is chosen so that k\u2206M k \u2264 \u01eb. Then, the\n\u2217\noptimum value \u03b8\u2217 of problem (Denoise) and the optimum value \u03b8\u2206M\nof problem (Perturbed) satisfy\n\u2217\n\u2217\n\u03b8\u2206M\n\u2212 (n + 1)\u01eb \u2264 \u03b8\u2217 \u2264 \u03b8\u2206M\n+ (n + 1)\u01eb.\n\nMoreover, by weak duality, we have\nmax\n\nx\u2208{\u22121,1}n\n\n\u2217\n\u2212xt (I + \u03bdP )x \u2264 \u03b8\u2206M\n.\n\n\fSince strong duality holds for problem (Perturbed), denoting by x\u2217\u2206M a solution of maxx\u2208{\u22121,1}n \u2212xt (I +\u2206M +\n\u03bdP )x we have\n\u2217\n\u03b8\u2206M\n= \u2212x\u2217\u2206M t (I + \u2206M + \u03bdP )x\u2217\u2206M \u2264 max n \u2212xt (I + \u03bdP )x.\nx\u2208{\u22121,1}\n\nTherefore, we obtain\n\u2212 x\u2217\u2206M t (I + \u2206M + \u03bdP )x\u2217\u2206M \u2264\n\nmax\n\nx\u2208{\u22121,1}n\n\n\u2212xt (I + \u03bdP )x \u2264 \u2212x\u2217\u2206M t (I + \u2206M + \u03bdP )x\u2217\u2206M + (n + 1)\u01eb,\n\nwhich implies\n\u2212 x\u2217\u2206M t (I + \u03bdP )x\u2217\u2206M \u2212 (n + 1)\u01eb \u2264\n\nmax\n\nx\u2208{\u22121,1}n\n\n\u2212xt (I + \u03bdP )x \u2264 \u2212x\u2217\u2206M t (I + \u03bdP )x\u2217\u2206M + 2(n + 1)\u01eb,\n\nNow, since {\u22121, 1}n is finite, the image I of {\u22121, 1}n by the function \u2212xt (I + \u03bdP )x is a finite set. Let \u03b4 denote\nthe closest number to maxx\u2208{\u22121,1}n \u2212xt (I + \u03bdP )x in I. Now, choosing 2(n + 1)\u01eb < \u03b4, we obtain\n\u2212 x\u2217\u2206M t (I + \u03bdP )x\u2217\u2206M =\n\nmax\n\nx\u2208{\u22121,1}n\n\n\u2212xt (I + \u03bdP )x\n\nwhich proves that the denoising problem is polynomial time solvable by solving problem (Perturbed).\n\u0003\nThis theorem is to be compared with the results of D. M. Greig, B. T. Porteous and A. H. Seheult [34] which\nformulates the binary denoising problem as a minimization problem with cost given at the top of page 273. The\nobjective to be minimized in [34] can be rearranged so as to minimize a linear cost with same penalization as the\none given by (5.1.1). The main contribution of [34] is to say that this problem can be solved in polynomial time\nusing a network flow algorithm. Notice that our proof works for At A = 0 and any additional linear term added\nto the penalized objective function to be optimized. Since the eigenvalue relaxation can also be optimized in\npolynomial time, this confirms that the eigenvalue relaxation performs at least as good as previous approaches\non a well known problem. On the other hand, the eigenvalue relaxation can be a flexible approach in more\ncomplicated cases where A is not equal to the identify or other quadratic constraints have to be incorporated\nsuch as in [16].\n5.1.3\n\nA numerical experiment\n\nThe experiments reported on below were performed for the case of quite noisy original images. The noise\nwas taken to be additive, independent identically distributed and Gaussian N (0, 2) and was applied to the\nsymmetrized image with pixel values in {\u22121, 1}. In order to show the influence of the smoothing parameter \u03bd,\nwe displayed the percentage of misspecified bits vs values of \u03bd. The recovered image is the one with the choice\nof \u03bd giving the best percentage of bits recovered.\nWe found the results very encouraging. Indeed, even when the observed image is very noisy, we still recover\nan image which is readable. This suggested that an appropriate postprocessing might easily allow to recover\nthe original written words, by comparing the letters to a given dictionary. Cross validation can be used to\nestimate \u03bd. We will not discuss this problem here. Instead, it seems reasonable to argue that the choice of \u03bd can\njust be made a posteriori since it consists of tuning the method until a satisfactory solution is obtained. This\nreduces the hard combinatorial initial problem to a simpler one parameter knobing procedure. The displayed\nexperiment and the numerous simulations not presented here confirm that robust intervals for the values of \u03bd\nare not very difficult to identify in practice.\n\n5.2\n5.2.1\n\nMultiuser detection in CDMA systems\nPresentation of the problem\n\nThis problem was studied by [24] using the maximum likelihood approach. As we will see, the resulting\noptimization problem is of the same form as the binary least squares problem. The main difference here is that\nA 6= I and P = 0.\nA synchronous K users DS-CDMA system is considered with a common single path additive white Gaussian\nnoise (AWGN) channel. The signature waveform of the kth user is denoted by sk (t), a function taking nonzero\nvalues in [0, T ] and being equal to zero outside this interval, and xk is the information bit transmitted by user\nk. The overall received signal is therefore of the form\ny(t) =\n\nK\nX\n\nk=1\n\nak xk sk (t) + n(t)\n\n\fwhere ak is the amplitude of the kth user's signal and n(t) is an additive white Gaussian white noise with zero\nmean and variance \u03c3 2 . The signal y is then filtered using a bank of K matched filters. The output of the kth\nmatched filter is given by\nZ T\ny(t)sk (t)dt.\nyk =\n0\n\nIn matrix form, this can be written\ny = RAx + \u03bd\nRT\nwhere y = [y1 , . . . , yk ]t , R is the correlation matrix whose components are given by Rij = 0 si (t)sj (t)dt,\nRT\nA = D(a) and \u03bd is the vector with components \u03bdk = 0 n(t)sj (t)dt.\nSince the gaussian vector has a correlation matrix equal to \u03c3 2 R, the ML estimator is obtained by simply\nsolving the following combinatorial optimization problem.\nminx\u2208Rn xt ARAx \u2212 2y t Ax\n\n(5.2.1)\n\ns.t. xi \u2208 {\u22121, 1}, i = 1, . . . , K.\n5.2.2\n\nSome comments\n\nThe SDP approach seems to have been first applied for the DS-CDMA detection problem in [26]. Since then\nnumerous contributions have appeared using the SDR and comparing it to other methods as in [28] and [29].\nExtension to M-ary phase shift keying symbol constellations is proposed in [30]. The issue of accelerating\nthe speed of the method is addressed in [31]. However, as for the former problem, the main drawback of the\nstandard primal semidefinite relaxation is that the size of the problem is greatly increased by using K \u00d7 K\nmatrices instead of vectors of size K. In order to overcome this problem, a better approach using semidefinite\nprogramming duality was recently proposed in [32].\nThe analysis of the previous sections proves that the eigenvalue relaxation is equally applicable to this\nproblem and maybe a good competitor to the SDP relaxation. The most important point of our analysis is\nthe following: Theorem 4.1.1 proves that if the correlation matrix R is componentwise negative outside the\ndiagonal, then strong duality holds, i.e. the detection problem can be solved exactly in polynomial time. The\nconstruction of efficient signatures is the current subject of an active research activity. For instance, the theory\nof frames allows to consider the problem from an interesting viewpoint as developed in [35]. Our findings suggest\nin particular that the componentwise negativity of the correlation matrix may be an interesting constraint to\nlook at in future investigations on this problem.\ns1\n1\n2\u03c0/3\n0\n\nb\n\ns3\n-1\n\ns2\n-1\n\n0\n\n1\n\nFigure 2: Three vectors in R2 with correlation matrix having negative off-diagonal components.\nFinally, the eigenvalue relaxation can also be useful even for general signatures because of the weak duality\nproperty. Indeed, several recent publications prove that clever heuristics can perform better than the SDP\nrelaxation. However, in real situations it is hard to certify that a primal solution provided by such a heuristic\nis indeed the optimal solution because the original signal is unknown. Comparing the dual optimal value to a\nprimal value given by a heuristic can give a precise idea of the error without prior information on the signal.\n5.2.3\n\nA numerical experiment\n\nIn order to verify this point, we performed Monte Carlo simulations over 1000 random problems for a number\na users varying from 10 to 35. These computational experiments are reported in Figure 7 where the number of\nusers is on the x-axis and the average computation time is on the y-axis. The computations where performed\nusing the Scilab software [33]. The SDP solver called Semidef interfaces Boyd and Vandenberghe's sp.c program.\nThe eigenvalue relaxation was solved using the solver Optim with the \"nd\" option for possibly nondifferentiable\ncosts as is the case here. The curves in Figure 7 interpolate the average computation times for messages taken\nto be sequences of uniform and independant variables taking values in {0, 1} vs. the number of users. The\ncurve with dashed style is for the results of the SDP relaxation while the curve with plain style is for the\n\n\feigenvalue relaxation. Our computations suggest that the eigenvalue relaxation has lower complexity growth\nas the number of users increases exactly as expected. The reader should be warned that this experiment does\nnot prove that the complexity of the eigenvalue relaxation is lower than the SDP relaxation. The experiment\nonly shows that when a widely used routine for SDP is used, the eigenvalue relaxation, solved using a general\npurpose bundle method available through a free a well established software, has a lower complexity growth on\nthis problem.\n\n6\n\nAppendix: Arrow matrices and strict interlacing of eigenvalues\n\nArrow matrices are matrices A of the form\nA=\n\n\u0014\n\nD(a)\nbt\n\nb\nc\n\n\u0015\n\n,\n\nThe properties of the eigenvalues of such matrices have been well studied in the past. Some of them are\nsummarized in the following theorem. Theorem A. Let A be an arrow matrix, with a1 \u2264 a2 \u2264 . . . \u2264 an .\nMoreover, assume that all the components of b are different from zero. Let \u03bb1 \u2264 \u03bb2 \u2264 ... \u2264 \u03bbn+1 be its\neigenvalues considered in increasing order. Then, the characteristic polynomial of A is given by\npA (\u03bb) = (c \u2212 \u03bb)\n\nn\nY\n\ni=1\n\n(ai \u2212 \u03bb) \u2212\n\nn Y\nX\n(aj \u2212 \u03bb)b2i .\ni=1 j6=i\n\nThen, we have \u03bb1 < a1 and an < \u03bbn+1 . Moreover,if ai = ai+1 we have ai = \u03bbi+1 = ai+1 and if ai < ai+1 , we\nhave ai < \u03bbi+1 < ai+1 .\nThe properties of the eigenvalues of arrow matrices are part of the folkore, especially in the realm of\nmathematical physics. We give a sketch of the proof of this theorm below in order to give the main ideas\nunderlying the results.\nProof of Theorem A. The formula for the characteristic polynomial pA (\u03bb) = det(A\u2212\u03bbI) is easily obtained\nby reccurence on the dimension. We have to consider two cases:\n\u2022 for some i, ai = ai+1 ,\n\u2022 a1 < a2 < . . . < an\nQ\nIn the first case ai is a root of pA . In the second case pA (ai ) = j6=i (aj \u2212 ai )b2i which is different from zero\nsince we assumed all the bi 's to be different from zero. In this case, the eigenvalues of A are the zeros of the\nfunction\nn\nX\nb2i\nqA (\u03bb) = c \u2212 \u03bb +\n.\n\u03bb \u2212 ai\ni=1\nFrom this formula, we deduce that there is a root in each interval (\u2212\u221e, a1 ), (ai , ai+1 ), for all i = 1, . . . , n and\n(an , +\u221e).\nThe final conclusions are easily derived by combining the results in the two simple cases discussed above.\n\u0003\n\n7\n\nConclusion\n\nIn this paper, we surveyed the main properties of the eigenvalue relaxation for binary least squares problem. A\nfull connection with the standard SDP relaxation was presented and we showed how to recover a solution of the\nSemi-Definite program from the solution of the eigenvalue minimization problem. The problem of recovering\nprimal binary solution was also addressed and we gave simple sufficient conditions for strong duality. In the case\nwhere these conditions are not satisfied, the randomized procedure adapted from Goemans and Williamson's\nallows to recover binary solutions with garanteed relative approximation ratio due to Nesterov's bound. Two\napplications were presented: binary image denoising and detection in multiuser CDMA systems. In the case\nof image denoising, we show that strong duality holds. For the multiuser detection problem, our results prove\nthat strong duality holds when the signature covariance matrix has nonpositive off diagonal components.\n\n\fReferences\n[1] Lemar\u00e9chal C. and Oustry F., Nonsmooth algorithms to solve semidefinite programs, Recent Advances on\nLMI methods in Control, L. EL Ghaoui and S-I. Niculescu editors, SIAM (1999).\n[2] Boyd, Stephen and Vandenberghe, Lieven Convex optimization. Cambridge University Press, Cambridge,\n2004\n[3] Luo, Zhi-Quan, Applications of convex optimization in signal processing and digital communication. ISMP,\n2003 (Copenhagen). Math. Program. 97 (2003), no. 1-2, Ser. B, 177\u2013207.\n[4] Lemar\u00e9chal, Claude and Oustry, Fran\u00e7ois SDP relaxations in combinatorial optimization from a Lagrangian\nviewpoint. Advances in convex analysis and global optimization (Pythagorion, 2000), 119\u2013134, Nonconvex\nOptim. Appl., 54, Kluwer Acad. Publ., Dordrecht, 2001.\n[5] Wolkowicz, Henry and Anjos, Miguel F. Semidefinite programming for discrete optimization and matrix\ncompletion problems. Workshop on Discrete Optimization, DO'99 (Piscataway, NJ). Discrete Appl. Math.\n123 (2002), no. 1-3, 513\u2013577.\n[6] Ben-Tal, Aharon and Nemirovski, Arkadi Lectures on modern convex optimization. Analysis, algorithms,\nand engineering applications. MPS/SIAM Series on Optimization. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA; Mathematical Programming Society (MPS), Philadelphia, PA, 2001.\n[7] Poljak, S., Rendl, F. and Wolkowicz, H. A recipe for semidefinite relaxation for (0, 1)-quadratic programming.\nJ. Global Optim. 7 (1995), no. 1, 51\u201373.\n[8] Oustry, Fran\u00e7ois A second-order bundle method to minimize the maximum eigenvalue function. Math. Program. 89 (2000), no. 1, Ser. A, 1\u201333\n[9] Helmberg, Christoph and Oustry, Fran\u00e7ois Bundle methods to minimize the maximum eigenvalue function.\nHandbook of semidefinite programming, 307\u2013337, Internat. Ser. Oper. Res. Management Sci., 27, Kluwer\nAcad. Publ., Boston, MA, 2000\n[10] Helmberg, C. and Rendl, F. A spectral bundle method for semidefinite programming. SIAM J. Optim. 10\n(2000), no. 3, 673\u2013696\n[11] Nesterov, Yu. Semidefinite relaxation and nonconvex quadratic optimization. Optim. Methods Softw. 9\n(1998), no. 1-3, 141\u2013160.\n[12] Nesterov, Yuri, Wolkowicz, Henry and Ye, Yinyu Semidefinite programming relaxations of nonconvex\nquadratic optimization. Handbook of semidefinite programming, 361\u2013419, Internat. Ser. Oper. Res. Management Sci., 27, Kluwer Acad. Publ., Boston, MA, 2000.\n[13] Goemans, Michel X. and Williamson, David P. Improved approximation algorithms for maximum cut and\nsatisfiability problems using semidefinite programming. J. Assoc. Comput. Mach. 42 (1995).\n[14] Gibbs, Alison L. Bounding the convergence time of the Gibbs sampler in Bayesian image restoration.\nBiometrika 87 (2000), no. 4, 749\u2013766.\n[15] Chr\u00e9tien, St\u00e9phane; Corset, Franck Least squares reconstruction of binary images using eigenvalue optimization. COMPSTAT 2002 (Berlin), 419\u2013424, Physica, Heidelberg, 2002.\n[16] Keuchel, J.; Schnorr, C.; Schellewald, C.; Cremers, D.; Binary partitioning, perceptual grouping, and\nrestoration with semidefinite programming, Pattern Analysis and Machine Intelligence, IEEE Transactions\non 25, (2003), no. 11, 1364\u20131379.\n[17] Besag, Julian On the statistical analysis of dirty pictures. J. Roy. Statist. Soc. Ser. B 48 (1986), no. 3,\n259\u2013302.\n[18] Hiriart-Urruty, J.-B.; Lemar\u00e9chal, C. Convex analysis and minimization algorithms. II. Advanced theory\nand bundle methods. Grundlehren der Mathematischen Wissenschaften, 306. Springer-Verlag, Berlin, 1993.\n[19] Delorme, C.; Poljak, S. Laplacian eigenvalues and the maximum cut problem. Math. Programming 62\n(1993), no. 3, Ser. A, 557\u2013574.\n\n\f[20] Poljak, Svatopluk and Wolkowicz, Henry Convex relaxations of (0, 1)-quadratic programming. Math. Oper.\nRes. 20 (1995), no. 3, 550\u2013561.\n[21] Poljak, Svatopluk and Rendl, Franz Nonpolyhedral relaxations of graph-bisection problems. SIAM J. Optim.\n5 (1995), no. 3, 467\u2013487.\n[22] C. D. Meyer and G. W. Stewart, \"Derivatives and perturbations of eigenvectors\", SIAM Journal on\nNumerical Analysis, 25 (1988), no. 3, 679\u2013691.\n[23] Nikolova M., Estimation of binary images using convex criteria, Proc. of IEEE Int. Conf. on Image Processing, Oct. 1998.\n[24] Verd\u00f9, Sergio Minimum probability of error for asynchronous Gaussian multiple-access channels. IEEE\nTrans. Inform. Theory 32 (1986), no. 1, 85\u201396.\n[25] Goemans, Michel X.and Williamson, David P. Improved approximation algorithms for maximum cut and\nsatisfiability problems using semidefinite programming. J. Assoc. Comput. Mach. 42 (1995), no. 6, 1115\u2013\n1145.\n[26] Peng Hui Tan and Lars K. Rasmussen, The application of semidefinite programming for detection in\nCDMA, IEEE J. Select. Areas in Comm. 18 (2001), no. 8, 1442\u20131448.\n[27] Garey, M. R. and Johnson, D. S. Computers and intractability. A guide to the theory of NP-completeness.\nA Series of Books in the Mathematical Sciences. W. H. Freeman and Co., San Francisco, Calif., 1979.\n[28] Fumihiro Hasegawa, Jie Luo, Krishna R. Pattipati, Peter Willett and David Pham, Speed and accuracy\ncomparison of techniques for multiuser detection in synchronous CDMA, IEEE Trans. Comm. 52 (2004),\nno. 4, 540\u2013545.\n[29] Peng Hui Tan and Lars K. Rasmussen, Multiuser detection in CDMA\u2013A comparison of relaxations, exact\nand Heuristic search methods, IEEE Trans. Wireless Comm. 3, (2004), no. 5, 1802\u20131809.\n[30] Wing-Kin Ma, Pak-Chung Ching and Zhi Ding, Semidefinite relaxation based multiuser detection for M-ary\nPSK multiuser systems, IEEE Trans. Sig. Proc. 52, (2004), no. 10, 2862\u20132872.\n[31] Moussa Abdi, Hassan El Nahas, Alexandre Jard and Eric Moulines, Semidefinite positive relaxation of\nthe maximum likelihood criterion applied to the multiuser detection in a CDMA context, IEEE Sig. Proc.\nLetters, 9, (2002), no. 6, 165\u2013167.\n[32] X. M. Wang, W. S. Lu and A. Antoniou, A near optimal multiuser detector for DS-CDMA systems using\nsemidefinite programming relaxation, IEEE Trans. Sig. Proc. 51, (2003), no. 9, 2446\u20132450.\n[33] http://www.scilab.org\n[34] D. M. Greig, B. T. Porteous, and A. H. Seheult, Exact maximum a posteriori estimation for binary images,\nJ. Roy. Statist. Soc. Ser. B 51 (1989), no. 2, 271\u2013 279.\n[35] J. Tropp, I. S. Dhillon, R. W. Heath, Jr., and T. Strohmer Designing Structured Tight Frames Via an\nAlternating Projection Method, IEEE Trans. Info. Theory, vol. 51, (2005) no. 1, 188\u2013209.\n[36] G. Pataki, On the Rank of Extreme Matrices in Semidefinite Programs and the Multiplicity of Optimal\nEigenvalues, Math. Op. Research, Vol. 23, (1998), no. 2, 339\u2013358.\n\n\f40\n36\n32\n28\n24\n20\n16\n12\n8\n4\n0\n0\n\n10\n\n20\n\n30\n\nFigure 3: Original image\n\n40\n\n50\n\n\f40\n36\n32\n28\n24\n20\n16\n12\n8\n4\n0\n0\n\n10\n\n20\n\n30\n\nFigure 4: Noisy image: i.i.d. N (0, 2)\n\n40\n\n50\n\n\f0.226\n0.224\n0.222\n0.220\n0.218\n0.216\n0.214\n0.212\n0.210\n0.208\n0.206\n1\n\n3\n\n5\n\n7\n\n9\n\n11\n\nFigure 5: Percentage of misspecified bits v.s. \u03bd\n\n13\n\n\f40\n36\n32\n28\n24\n20\n16\n12\n8\n4\n0\n0\n\n10\n\n20\n\n30\n\nFigure 6: Recovered image\n\n40\n\n50\n\n\fFigure 7: Comparison of SDP and eigenvalue relaxations for CDMA multiuser detection\n\n\f"}