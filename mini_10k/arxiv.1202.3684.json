{"id": "http://arxiv.org/abs/1202.3684v1", "guidislink": true, "updated": "2012-02-16T20:08:11Z", "updated_parsed": [2012, 2, 16, 20, 8, 11, 3, 47, 0], "published": "2012-02-16T20:08:11Z", "published_parsed": [2012, 2, 16, 20, 8, 11, 3, 47, 0], "title": "Generalized Boundaries from Multiple Image Interpretations", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.3346%2C1202.2672%2C1202.0999%2C1202.2790%2C1202.6207%2C1202.4530%2C1202.5203%2C1202.0531%2C1202.1840%2C1202.2071%2C1202.6005%2C1202.2402%2C1202.0022%2C1202.4820%2C1202.0710%2C1202.0096%2C1202.5884%2C1202.4055%2C1202.3357%2C1202.2414%2C1202.0803%2C1202.2909%2C1202.4611%2C1202.3666%2C1202.2218%2C1202.2219%2C1202.4113%2C1202.3684%2C1202.5494%2C1202.3620%2C1202.4260%2C1202.3637%2C1202.0326%2C1202.4211%2C1202.3122%2C1202.2786%2C1202.5824%2C1202.3116%2C1202.5815%2C1202.2276%2C1202.5919%2C1202.3457%2C1202.4697%2C1202.1929%2C1202.2704%2C1202.4258%2C1202.3175%2C1202.0010%2C1202.0936%2C1202.0677%2C1202.3068%2C1202.2811%2C1202.0699%2C1202.2657%2C1202.0114%2C1202.1908%2C1202.5081%2C1202.4855%2C1202.1253%2C1202.6093%2C1202.4117%2C1202.0384%2C1202.5381%2C1202.0330%2C1202.6039%2C1202.2302%2C1202.0459%2C1202.2802%2C1202.6490%2C1202.3185%2C1202.1926%2C1202.1736%2C1202.0779%2C1202.4232%2C1202.2027%2C1202.2542%2C1202.4106%2C1202.2050%2C1202.5460%2C1202.2599%2C1202.5637%2C1202.4456%2C1202.2905%2C1202.1371%2C1202.6515%2C1202.1778%2C1202.0366%2C1202.0962%2C1202.6254%2C1202.3212%2C1202.3579%2C1202.2774%2C1202.2392%2C1202.4619%2C1202.6518%2C1202.4048%2C1202.0420%2C1202.2863%2C1202.5798%2C1202.6457%2C1202.5691&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Generalized Boundaries from Multiple Image Interpretations"}, "summary": "Boundary detection is essential for a variety of computer vision tasks such\nas segmentation and recognition. In this paper we propose a unified formulation\nand a novel algorithm that are applicable to the detection of different types\nof boundaries, such as intensity edges, occlusion boundaries or object category\nspecific boundaries. Our formulation leads to a simple method with\nstate-of-the-art performance and significantly lower computational cost than\nexisting methods. We evaluate our algorithm on different types of boundaries,\nfrom low-level boundaries extracted in natural images, to occlusion boundaries\nobtained using motion cues and RGB-D cameras, to boundaries from\nsoft-segmentation. We also propose a novel method for figure/ground\nsoft-segmentation that can be used in conjunction with our boundary detection\nmethod and improve its accuracy at almost no extra computational cost.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1202.3346%2C1202.2672%2C1202.0999%2C1202.2790%2C1202.6207%2C1202.4530%2C1202.5203%2C1202.0531%2C1202.1840%2C1202.2071%2C1202.6005%2C1202.2402%2C1202.0022%2C1202.4820%2C1202.0710%2C1202.0096%2C1202.5884%2C1202.4055%2C1202.3357%2C1202.2414%2C1202.0803%2C1202.2909%2C1202.4611%2C1202.3666%2C1202.2218%2C1202.2219%2C1202.4113%2C1202.3684%2C1202.5494%2C1202.3620%2C1202.4260%2C1202.3637%2C1202.0326%2C1202.4211%2C1202.3122%2C1202.2786%2C1202.5824%2C1202.3116%2C1202.5815%2C1202.2276%2C1202.5919%2C1202.3457%2C1202.4697%2C1202.1929%2C1202.2704%2C1202.4258%2C1202.3175%2C1202.0010%2C1202.0936%2C1202.0677%2C1202.3068%2C1202.2811%2C1202.0699%2C1202.2657%2C1202.0114%2C1202.1908%2C1202.5081%2C1202.4855%2C1202.1253%2C1202.6093%2C1202.4117%2C1202.0384%2C1202.5381%2C1202.0330%2C1202.6039%2C1202.2302%2C1202.0459%2C1202.2802%2C1202.6490%2C1202.3185%2C1202.1926%2C1202.1736%2C1202.0779%2C1202.4232%2C1202.2027%2C1202.2542%2C1202.4106%2C1202.2050%2C1202.5460%2C1202.2599%2C1202.5637%2C1202.4456%2C1202.2905%2C1202.1371%2C1202.6515%2C1202.1778%2C1202.0366%2C1202.0962%2C1202.6254%2C1202.3212%2C1202.3579%2C1202.2774%2C1202.2392%2C1202.4619%2C1202.6518%2C1202.4048%2C1202.0420%2C1202.2863%2C1202.5798%2C1202.6457%2C1202.5691&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Boundary detection is essential for a variety of computer vision tasks such\nas segmentation and recognition. In this paper we propose a unified formulation\nand a novel algorithm that are applicable to the detection of different types\nof boundaries, such as intensity edges, occlusion boundaries or object category\nspecific boundaries. Our formulation leads to a simple method with\nstate-of-the-art performance and significantly lower computational cost than\nexisting methods. We evaluate our algorithm on different types of boundaries,\nfrom low-level boundaries extracted in natural images, to occlusion boundaries\nobtained using motion cues and RGB-D cameras, to boundaries from\nsoft-segmentation. We also propose a novel method for figure/ground\nsoft-segmentation that can be used in conjunction with our boundary detection\nmethod and improve its accuracy at almost no extra computational cost."}, "authors": ["Marius Leordeanu", "Rahul Sukthankar", "Cristian Sminchisescu"], "author_detail": {"name": "Cristian Sminchisescu"}, "author": "Cristian Sminchisescu", "links": [{"href": "http://arxiv.org/abs/1202.3684v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1202.3684v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.CV", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1202.3684v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1202.3684v1", "arxiv_comment": null, "journal_reference": null, "doi": null, "fulltext": "Technical Report, INS, University of Bonn, November 2011\n\nGeneralized Boundaries from Multiple Image Interpretations\nMarius Leordeanu1\n\nRahul Sukthankar3,4\n\nCristian Sminchisescu2,1\n\nmarius.leordeanu@imar.ro\n\nrahuls@cs.cmu.edu\n\ncristian.sminchisescu@ins.uni-bonn.de\n\n1\n\narXiv:1202.3684v1 [cs.CV] 16 Feb 2012\n\n2\n\nInstitute of Mathematics of the Romanian Academy\nFaculty of Mathematics and Natural Science, University of Bonn\n3\nGoogle Research\n4\nCarnegie Mellon University\n\nAbstract\nBoundary detection is essential for a variety of computer\nvision tasks such as segmentation and recognition. In this\npaper we propose a unified formulation and a novel algorithm that are applicable to the detection of different types\nof boundaries, such as intensity edges, occlusion boundaries or object category specific boundaries. Our formulation leads to a simple method with state-of-the-art performance and significantly lower computational cost than\nexisting methods. We evaluate our algorithm on different\ntypes of boundaries, from low-level boundaries extracted\nin natural images, to occlusion boundaries obtained using motion cues and RGB-D cameras, to boundaries from\nsoft-segmentation. We also propose a novel method for figure/ground soft-segmentation that can be used in conjunction with our boundary detection method and improve its\naccuracy at almost no extra computational cost.\n\n1. Introduction\nBoundary detection is a fundamental problem in computer vision and has been studied since the early days of the\nfield. The majority of papers on boundary detection have focused on using only low-level cues, such as pixel intensity\nor color [3, 14, 16, 18, 19]. Recent work has started exploring the problem of boundary detection using higher-level\nrepresentations of the image, such as motion, surface and\ndepth cues [9, 22, 24], segmentation [1], as well as category\nspecific information [8, 13].\nIn this paper we propose a general formulation for\nboundary detection that can be applied, in principle, to the\nidentification of any type of boundaries, such as general\nboundaries from low-level static cues, motion boundaries or\ncategory-specific boundaries (Figures 1, 6, 7). Our method\ncan be seen both as a generalization of the early view of\nboundaries as step edges [11], and as a unique closed-form\n\nFigure 1. Detection of occlusion and motion boundaries using the\nproposed generalized boundary detection method (Gb). First two\nrows: the input layers consist of color (C), soft-segmentation (S)\n[the first three dimensions are shown as RGB], and optical\nflow (OF). Last two rows: input layers are color (C), depth (D)\nand optical flow (OF). The same implementation is used for both;\ncombining multiple input layers using Gb improves boundary detection. Best viewed in color.\n\nsolution to current boundary detection problems, based on\na straightforward mathematical formulation.\nWe generalize the classical view of boundaries from\n1\n\n\fsudden signal changes on the original low-level image input [3, 5, 6, 10, 14, 16, 18], to a locally linear (planar or stepwise) model on multiple layers of the input. The layers\nare interpretations of the image at different levels of visual\nprocessing, which could be high-level (e.g., object category\nsegmentation) or low-level (e.g., color or grey level intensity).\nDespite the abundance of research on boundary detection, there is no general formulation of this problem. In\nthis paper, we make the popular but implicit intuition of\nboundaries explicit: boundary pixels mark the transition\nfrom one relatively constant region to another, in appropriate interpretations of the image. Thus, while the region\nconstancy assumption may only apply weakly for low-level\ninput such as pixel intensity, it will also be weakly observed\nin higher-level interpretation layers of the image. Generalized boundary detection aims to exploit such weak signals\nacross multiple layers in a principled manner. We could say\nthat boundaries do not exist in the raw image, but rather\nin the multiple interpretation layers of that image. We can\nsummarize our assumptions as follows:\n1. A boundary separates different image regions, which\nin the absence of noise are almost constant, at some\nlevel of image interpretation or processing. For example, at the lowest level, a region could have a constant\nintensity. At a higher-level, it could be a region delimitating an object category, in which case the output of\na category-specific classifier would be constant.\n2. For a given image, boundaries in one layer often coincide, in terms of position and orientation, with boundaries in other layers. For example, discontinuities in\nintensity are typically correlated with discontinuities\nin optical flow, texture or other cues. Moreover, the\nboundaries that align across multiple layers often correspond to the semantic boundaries that are primarily of interest to humans: the so-called \"ground-truth\nboundaries\".\nBased on these observations, we develop a unified model,\nwhich can simultaneously consider both low-level and\nhigher-level information.\nClassical vector-valued techniques on multi-images [6,\n10,11] can be simultaneously applied to several image channels, but differ from the proposed approach in a fundamental way: they are specifically designed for low-level input, by using first or second-order derivatives of the image\nchannels, with edge models limited to very small neighborhoods of only a few pixels (for approximating the derivatives). We argue that in order to correctly incorporate\nhigher-level information, one must go beyond a few pixels, to much larger neighborhoods, in line with more recent\nmethods [1, 15, 17, 19]. First, even though boundaries from\none layer coincide with edges from a different layer, they\n\ncannot be required to match perfectly in location. Second,\nboundaries, especially in higher-level layers, do not have to\ncorrespond to sudden changes. They could be smooth transitions over larger regions and exhibit significant noise that\nwould corrupt any local gradient computation. That is why\nwe advocate a linear boundary model rather than one based\non noisy estimation of derivatives, as discussed in the next\nsection.\nAnother drawback of traditional multi-image techniques\nis the issue of channel scaling, where the algorithms require\nconsiderable manual tuning. Consistent with current machine learning based approaches [1,7,15], the parameters in\nour proposed method are automatically learned using realworld datasets. However, our method has better computational complexity and employs far fewer parameters. This\nallows us to learn efficiently from limited quantities of data\nwithout overfitting.\nAnother important advantage of our approach over current methods is in the closed-form computation of the\nboundary orientation. The idea behind P b [15] is to classify each possible boundary pixel based on the histogram\ndifference in color and texture information between the two\nhalf disks on either side of a potential orientation, for a fixed\nnumber of candidate angles (e.g., 8). The separate computation for each orientation significantly increases the computational cost and limits orientation estimates to a particular\ngranularity.\nWe summarize our contributions as follows: 1) we\npresent a closed-form formulation of generalized boundary\ndetection that is computationally efficient; 2) we recover exact boundary normals through direct estimation rather than\nevaluating coarsely sampled orientation candidates; 3) as\nopposed to current approaches [1, 24], our unified framework treats both low-level pixel data and higher-level interpretations equally and can easily incorporate outputs from\nnew image interpretation algorithms; and 4) our method requires learning only a single parameter per layer, which enables efficient training with limited data. We demonstrate\nthe strength of our method on a variety of real-world tasks.\n\n2. Problem Formulation\nFor a given Nx \u00d7 Ny image I, let the k-th layer Lk be\nsome real-valued array, of the same size, associated with\nI, whose boundaries are relevant to our task. For example,\nLk could contain, at each pixel, the real-valued output of\na patch-based binary classifier trained to detect man-made\nstructures or respond to a particular texture or color distribution.1 Thus, Lk will consist of relatively constant regions\n(modulo classifier error) separated by boundaries. Note that\nthe raw pixels in the corresponding regions of the original\nimage may not be constant.\n1 The output of a discrete-valued multi-class classifier can be encoded\nas multiple input layers, with each layer representing a given label.\n\n\fUnlike some previous approaches, we expect that boundaries in different layers may not precisely align. Given a\nset of layers, each corresponding to a particular interpretation level of the image, we wish to identify the most consistent boundaries across multiple layers. The output of our\nmethod for each point p on the Nx \u00d7 Ny image grid is a\nreal-valued probability that p lies on a boundary, given the\ninformation in all multiple image interpretations Lk centered at p.\nWe model a boundary point in layer Lk as a transition\n(either sudden or gradual) in the corresponding values of\nLk along the normal to the boundary. If several K such layers are available, let L be a three-dimensional array of size\nNx \u00d7 Ny \u00d7 K, such that L(x, y, k) = Lk (x, y), for each\nk. Thus, L contains all the relevant information for the current boundary detection problem, given multiple interpretations of the image or video. Figure 1 illustrates how we\nimprove the accuracy of boundary detection by combining\ndifferent useful layers of information, such as color, softsegmentation and optical flow, in a single representation\nL,\n\u221a\nN\n\u00d7\nLet\np\nbe\nthe\ncenter\nof\na\nwindow\nW\n(p\n)\nof\nsize\nW\n0\n0\n\u221a\nNW . For each image-location p0 we want to evaluate the\nprobability of boundary using the information from L, limited to that particular window. For any p within the window,\nwe make the following approximation, which gives our locally linear boundary model:\nT\n\nLk (p) \u2248 Ck (p0 ) + bk (p0 )(p\u0302\u000f \u2212 p0 ) n(p0 ).\n\n(1)\n\nHere bk is nonnegative and corresponds to the boundary\n\"height\" for layer k at location p0 ; p\u0302\u000f is the closest point to\np (projection of p) on the disk of radius \u000f centered at p0 ;\nn(p0 ) is the normal to the boundary and Ck (p0 ) is a constant over the window W (p0 ). This constant is useful for\nconstructing our model (see Figure 2), but its value is unimportant, since it cancels out, as shown below. Note that if\nwe set Ck (p0 ) = Lk (p0 ) and use a sufficiently large \u000f such\nthat p\u0302\u000f = p, our model reduces to the first-order Taylor expansion of Lk (p) around the current p0 .\n\nFigure 2. Simplified 1-dimensional view of our generalized boundary model. \u000f controls the region where the model is linear. For\npoints outside that region the layer is assumed to be roughly constant.\n\nAs shown in Figures 2 and 3, \u000f controls the steepness of\nthe boundary, going from completely planar when \u000f is large\n\nFigure 3. Our boundary model for different values of \u000f relative to\nthe window size W : a) \u000f > W ; b) \u000f = W/2 ; c) \u000f = W/1000.\nWhen \u000f approaches zero the boundary model becomes a step\n(along the normal direction passing through the window center).\n\n(first-order Taylor expansion) to a sharp step-wise discontinuity through the window center p0 , as \u000f approaches zero.\nMore precisely, when \u000f is very small we have a step along\nthe normal through the window center, and a sigmoid which\nflattens as we get farther from the center, along the boundary normal. As \u000f increases, the model flattens to become a\nperfect plane for any \u000f that is larger than the window radius.\nWhen the window is far from any boundary, the value\nof bk will be near zero, since the only variation in the layer\nvalues is due to noise. If we are close to a boundary, then bk\nT\nwill become positive and large. The term (p\u0302\u000f \u2212 p0 ) n(p0 )\napproximates the sign which indicates the side of the boundary: it does not matter on which side we are, as long as a\nsign change occurs when the boundary is crossed.\nWhen a true boundary is present within several layers at\nthe same position - i.e., bk (p0 ) is non-zero and possibly\ndifferent, for several k - the normal to the boundary should\nbe consistent. Thus, we model the boundary normal n as\ncommon across all layers.\nWe can now write the above equation in matrix form for\nall layers, with the same window size and location as follows. Let X be a NW \u00d7 K matrix with a row i for each\nlocation pi of the window and a column for each layer k,\nsuch that Xi;k = Lk (pi ). Similarly, we define NW \u00d7 2\nposition matrix P: on its i-th row we store the x and y\ncomponents of (p\u0302\u000f \u2212 p0 ) for the i-th point of the window. Let n = [nx , ny ] denote the boundary normal and\nb = [b1 , b2 , . . . , bK ] the step sizes for layers 1, 2, . . . , K.\nAlso, let us define the rank-1 2 \u00d7 K matrix J = nT b. We\nalso define matrix C of the same size as X, with each column k constant and equal to Ck (p0 ).\nWe can then rewrite Equation 1 as follows (dropping\nthe dependency on p0 for notational simplicity), with unknowns J and C:\nX \u2248 C + PJ.\n\n(2)\n\nSince C is a matrix with constant columns, and each column of P sums to 0, we have PT C = 0. Thus, by multiplying both sides of the equation above by PT we can eliminate the unknown C. Moreover, it can be easily shown that\nPT P = \u03b1I, i.e., the identity matrix scaled by a factor \u03b1,\n\n\fwhich can be computed since P is known. We finally obtain a simple expression for the unknown J (since both P\nand X are known):\nJ\u2248\n\n1 T\nP X.\n\u03b1\n\n(3)\nT\n\nSince J = nT b it follows that JJT = kbk2 n n is symmetric and has rank 1. Then n can be estimated as the\nprincipal eigenvector of M = JJT and kbk2 as its largest\neigenvalue. kbk, which is obtained as the square root of the\nlargest eigenvalue of M, is the norm of the boundary steps\nvector b = [b1 , b2 , ..., bK ]. This norm captures the overall strength of boundaries from all layers simultaneously. If\nlayers are properly scaled, then kbk could be used as a measure of boundary strength. Besides the intuitive meaning of\nkbk, the spectral approach to boundary estimation is also\nrelated to the gradient of multi-images previously used for\nlow-level color edge detection from classical papers such\nas [6, 10]. However, it is important to notice that unlike\nthose methods, we do not compute derivatives, as they are\nnot appropriate for higher-level layers and can be noisy for\nlow-level layers. Instead, we fit a model, which, by controlling \u000f, can vary from planar to sigmoid/step-wise. For\nsmoother-looking results, in practice we weigh the rows of\nmatrices X and P by a 2D Gaussian with the mean set to\nthe window center p0 and the standard deviation equal to\nhalf of the window radius.\nOnce we identify kbk, we pass it through a onedimensional logistic model to obtain the probability of\nboundary, similarly to recent classification approaches to\nboundary detection [1, 15]. The parameters of the logistic regression model are learned using standard procedures.\nThe normal to the boundary n is then used for non-maxima\nsuppression.\n\n3. Algorithm and Numerical Considerations\nBefore applying the main algorithm we scale each layer\nin L according to its importance, which may be problem dependent. For example, in Figure 1, it is clear that when recovering occlusion boundaries, the optical flow layer (OF)\nshould contribute more than the raw color (C) and colorbased soft segmentation (S) layers. The images displayed\nare from the dataset of Stein and Hebert [22]. The optical\nflow shown is an average between the flow [23] computed\nover two pairs of images: (reference frame, first frame), and\n(reference frame, last frame). We learn the correct scaling of the layers from training data using a standard unconstrained nonlinear optimization procedure (e.g., fminsearch routine in MATLAB) on the average F -measure of\nthe training set. We apply the same learning procedure in all\nof our experiments. This is computationally feasible since\nthere is only one parameter per layer in the proposed model.\nAlgorithm 1 (referred to as Gb1) summarizes the proposed approach. The overall complexity of our method is\n\nAlgorithm 1 Gb1: Fast Generalized Boundary Detection\nInitialize L, scaled appropriately.\nInitialize w0 and w1 .\nfor all pixels p do\nT\nT\nT\nM \u2190 (P Xp )(P Xp )\n(v, \u03bb) \u2190 principal eigenpair of M\nbp \u2190 1+exp(w1 +w \u221a\u03bb)\n0\n\n1\n\n\u03b8p \u2190 atan2(vy , vx )\nend for\nreturn b, \u03b8\nrelatively straightforward to compute. For each pixel p,\nthe most expensive step is the computation of the matrix\nM, which takes O((NW + 2)K) steps (NW is the number of pixels in the window, and K is the number of layers). Since M is always 2 \u00d7 2, computing its eigenpair\n(v, \u03bb) is a closed-form operation, with a small fixed cost.\nIt follows that for a fixed window size NW and a total of\nN pixels per image the overall complexity of our algorithm\nis O(KNW N ). If NW is a constant fraction f of N , then\ncomplexity becomes O(f KN 2 ).\nThus, the running time of Gb1 compares very favorably\nto that of the P b algorithm [1, 15], which in its exact form\nhas complexity O(f KNo N 2 ), where No is a discrete number of candidate orientations. An approximation is proposed in [1] with O(f KNo Nb N ) complexity where Nb is\nthe number of histogram bins for the different image channels. However, No Nb is large in practice and significantly\naffects the overall running time.\nWe also propose a faster version of our algorithm, Gb2,\nwith complexity O(f KN ), that is linear in the number of\nimage pixels. The speed-up is achieved by computing M\nat a constant cost (independent of the number of pixels in\nthe window). When \u000f is large and no Gaussian weighing is\nT\napplied, we have PT Xp = PT\np Xp \u2212 P0 Xp , where Pp is\nthe matrix of absolute positions for each pixel p and P0 is\na matrix with two constant columns equal to the 2D coordinates of the window center. Upon closer inspection, we note\nthat both PTp X and P0 T X can be computed in constant\ntime by using integral images, for each layer separately. We\nimplemented the faster version of our algorithm, Gb2, and\nverified experimentally that it is linear in the number of pixels per image, independent of the window size (Figure 4).\nThe output of Gb2 is similar to Gb1 (see Table 1), and provably identical when \u000f is larger than the window radius and\nno Gaussian weighting is applied. The weighting can be\napproximated by running Gb2 at multiple scales and combining the results.\nIn Figure 4 we present a comparison of the running times\nof edge detection in MATLAB of the three algorithms (Gb1,\nGb2 and P b [15]) vs. the number of pixels per image.2\n2 Our optimized C++ implementation of Gb1 is an order of magnitude\nfaster than its MATLAB version.\n\n\fformed by a composition of regions of uniform color distributions, then we can consider c to be a multi-dimensional\nrandom variable drawn from a mixture (linear combination) of color distributions hi corresponding to the image\nregions:\nc\u223c\n\nX\n\n\u03c0i hi .\n\n(4)\n\ni\n\nFigure 4. Edge detection running times on a 3.2 GHz desktop\nof our non-optimized MATLAB implementation of Gb1 and Gb2\nvs. the publicly available code of P b [15]. Each algorithm uses the\nsame window radius, whose number of pixels is a constant fraction\nof the total number of image pixels. Gb2 is linear in the number\nof image pixels (independent of the window size). The accuracy\nof all algorithms is similar.\n\nIt is important to note that while our algorithm is fast,\nobtaining some of the layers may be slow, depending on the\nimage processing required. If we only use low-level interpretations, such as raw color or depth (e.g., from an RGBD camera) then the total execution time is small, even for\na MATLAB implementation. In the next section, we propose an efficient method for color-based soft-segmentation\nof images that works well with our algorithm. More complex, higher-level inputs, such as class-specific segmentations naturally increase the total running time.\n\n4. An Efficient Soft-Segmentation Method\nIn this section we present a novel method to rapidly generate soft figure/ground image segmentations. Its soft continuous output is similar to the eigenvectors computed by\nnormalized cuts [21] or the soft figure/ground assignment\nobtained by alpha-matting [12], but it is much faster than\nmost existing segmentation methods. We describe it here\nbecause it serves as a fast mid-level interpretation of the\nimage that significantly improves accuracy over raw color\nalone.\nWhile we describe our approach in the context of color\ninformation, the proposed method is general enough to handle a variety of other types of low-level information as well.\nThe method is motivated by the observation that regions of\nsemantic interest (such as objects) can often be modeled\nwith a relatively uniform color distribution. Specifically,\nwe assume that the colors of any image patch are generated\nfrom a distribution that is a linear combination (or mixture)\nof a finite number of color probability distributions belonging to the regions of interest/objects in the image.\nLet c be an indicator vector associated with some patch\nfrom the image, such that ci = 1 if color i is present in\nthe patch and 0 otherwise. If we assume that the image is\n\nThe linear subspace of color distributions can be automatically discovered by performing PCA on collections\nof such indicator vectors c, sampled uniformly from the\nimage. This idea deserves a further in-depth discussion\nbut, due to space limitations, in this paper we outline just\nthe main idea, without presenting our detailed probabilistic\nanalysis.\nOnce the subspace is discovered using PCA, for any\npatch sampled from the image and its associated indicator\nvector c, its generating distribution (considered to be the\ndistribution of the foreground) can be reconstructed from\nthe linear subspace using the usual PCA reconstruction apP\nT\nproximation: hF (c) \u2248 h0 + i (c \u2212 h0 ) vi . The distribution of the background is also obtained from the PCA\nmodel using the same coefficients, but with opposite sign.\nAs expected, we obtain a background distribution that is as\nfar as possible (in the subspace) from the distribution of the\nP\nT\nforeground: hB (c) \u2248 h0 \u2212 i (c \u2212 h0 ) vi .\nUsing the figure/ground distributions obtained in this\nmanner, we classify each point in the image as either belonging or not to the same region as the current patch. If we\nperform the same classification procedure for ns (\u2248 150)\nlocations uniformly sampled on the image grid, we obtain\nns figure/ground segmentations for the same image. At\na final step, we again perform PCA on vectors collected\nfrom all pixels in the image; each vector is of dimension\nns and corresponds to a certain image pixel, such that its\ni-th element is equal to the value at that pixel in the i-th\nfigure/ground segmentation. Finally we perform PCA reconstruction using the first 8 principal components, and obtain a set of 8 soft-segmentations which are a compressed\nversion of the entire set of ns segmentations. These softsegmentations are used as input layers to our boundary detection method, and are similar in spirit to the normalized\ncuts eigenvectors computed for gP b [1].\nIn Figure 5 we show examples of the first three such softsegmentations on the RGB color channels. This method\ntakes less than 3 seconds in MATLAB on a 3.2GHz desktop\ncomputer for a 300 \u00d7 200 color image.\n\n5. Experimental analysis\nTo evaluate the generality of our proposed method, we\nconduct experiments on detecting boundaries in image,\nvideo and RGB-D data on both standard and new datasets.\nFirst, we test our method on static color images for which\n\n\fFigure 5. Soft-segmentation examples using our method. The first three dimensions of the soft-segmentations, reconstructed using PCA,\nare shown on the RGB channels. Total computation time for segmentation is less than 3 seconds in MATLAB per image. Best viewed in\ncolor.\nTable 1. Comparisons of accuracy (F-measure) and computational\ntime between our method and two other popular methods on BSDS\ndataset. We use two versions of the proposed method: Gb1 (S)\nuses color and soft-segmentations as input layers, while Gb1 uses\nonly color. Color layers are represented in CIE Lab space.\n\nAlgorithm\n\nGb1 (S)\n\nGb1\n\nGb2\n\nPb [15]\n\nCanny [3]\n\nF-measure\nTime (sec)\n\n0.67\n8\n\n0.65\n3\n\n0.64\n2\n\n0.65\n20\n\n0.58\n0.1\n\nwe only use the local color information. Second, we perform experiments on occlusion boundary detection in short\nvideo clips. Multiple frames, closely spaced in time, provide significantly more information about dynamic scenes\nand make occlusion boundary detection possible, as shown\nin recent work [9,20,22,24]. Third, we also experiment with\nRGB-D images of people and show that the depth layer can\nbe effectively used for detecting occlusions. In the fourth\nset of experiments we use the CPMC method [4] to generate figure/ground category segments on the PASCAL2011\ndataset. We show how it can be effectively used to generate image layers that can produce high-quality boundaries\nwhen processed using our method.\n\n5.1. Boundaries in Static Color Images\nWe evaluate our proposed method on the well-known\nBSDS300 benchmark [15]. We compare the accuracy and\ncomputational time of Gb with P b [15] and Canny [3] edge\ndetector. All algorithms use only local information at a\nsingle scale. Canny uses brightness information, Gb uses\nbrightness and color, while P b uses brightness, color and\ntexture information. Table 1 summarizes the results. Note\n\nTable 2. Performance comparison on the CMU Motion Dataset of\ncurrent techniques for occlusion boundary detection.\n\nAlgorithm\n\nF-measure\n\nGb1\nHe et al. [9]\nSargin et al. [20]\nStein et al. [22]\nSundberg et al. [24]\n\n0.63\n0.47\n0.57\n0.48\n0.62\n\nthat our method is much faster than P b (times are averages in Matlab on the same 3.2 GHz desktop computer).\nWhen no texture information is used for P b, its accuracy\ndrops significantly while the computational time remains\nhigh (\u2248 16 seconds).\n\n5.2. Occlusion Boundaries in Video\nOcclusion boundary detection is an important problem\nand has received increasing attention in computer vision.\nCurrent state-of-the-art techniques are based on the computation of optical flow combined with a global processing\nphase [9,20,22,24]. We evaluate our approach on the CMU\nMotion Dataset [22] and compare our method with published results on the same dataset (summarized in Table 2).\nOptical flow is an important cue for detecting occlusions in\nvideo; we use Sun et al.'s publicly available code [23]. In\naddition to optical flow, we provided Gb-1 with two additional layers: color and our soft segmentation (Section 4).\nIn contrast to the other methods [9, 20, 22, 24], which require significant time for processing and optimization, Gb\nrequires less than 4 seconds on average (aside from the external optical flow routine) to process images (230 \u00d7 320)\n\n\fTable 3. Average F-measure on 100 test RGB-D frames of Gb1\nalgorithm, using different layers: color (C), depth (D) and optical\nflow (OF). The performance improves as more layers are combined. Note: the reported time for C+OF and C+D+OF does not\ninclude that of generating optical flow using an external module.\n\nLayers\nF-measure\nTime (sec)\n\nC+OF\n\nC+D\n\nC+D+OF\n\n0.41\n5\n\n0.58\n4\n\n0.61\n6\n\nfrom the CMU dataset.\n\n5.3. Occlusion Boundaries in RGB-D Video\nThe third set of experiments uses RGB-D video clips\nof people performing different actions. We combine the\nlow-level color and depth input with large-displacement optical flow [2], which is useful for large inter-frame body\nmovements. Figure 1 shows an example of the input layers and the output of our method. The depth layer was\npre-processed to retain the largest connected component of\npixels at a similar depth, so as to cover the main subject performing actions. Table 3 summarizes boundary detection in\nRGB-D on our dataset of 74 training and 100 testing images.3 We see that Gb can effectively combine information\nfrom color (C), optical flow (OF) and depth (D) layers to\nachieve better results. Figure 6) shows sample qualitative\nresults for Gb using only the basic color and depth information (without pre-processing of the depth layer). Without\noptical flow, the total computation time for boundary detection is less than 4 seconds per image in MATLAB.\n\n5.4. Boundaries from soft-segmentations\nOur previous experiments use our soft-segmentation\nmethod as one of the input layers for Gb. In all of our experiments, we find the mid-level layer information provided by\nsoft-segmentations significantly improves the accuracy of\nGb.\nThe PCA reconstruction procedure described in Section 4 can also be applied to a large pool of figure/ground segments, such as those generated by the CPMC\nmethod [4]. This enables us to achieve an F-measure of 0.70\non BSDS300, which matches the performance of gP b [1].\nCPMC+Gb also gives very promising results on the PASCAL2011 dataset, as evidenced by the examples in Figure 7. These preliminary results indicate that fusing evidence from color and soft-segmentation using Gb is a\npromising avenue for further research.\n\n6. Conclusions\nWe present Gb, a novel model and algorithm for generalized boundary detection. Our method effectively combines\n3\n\nWe will release this dataset to enable direct comparisons.\n\nFigure 7. Qualitative results using Gb on PASCAL2011 images,\nfrom color and soft-segmentations obtained from the output of\nCPMC [4]. Best viewed on the screen.\n\nmultiple low- and high-level interpretation layers of an input image in a principled manner to achieve state-of-theart accuracy on standard datasets at a significantly lower\ncomputational cost than competing methods. Gb's broad\nreal-world applicability is demonstrated through qualitative\nand quantitative results on detecting semantic boundaries\nin natural images, occlusion boundaries in video and object boundaries in RGB-D data. We also propose a second,\neven more efficient variant of Gb, with asymptotic computational complexity that is linear with image size. Additionally, we introduce a practical method for fast generation of\nsoft-segmentations, using either PCA dimensionality reduction on data collected from image patches or a large pool of\nfigure/ground segments. We also demonstrate experimentally that our soft-segmentations are valuable mid-level interpretations for boundary detection.\n\nReferences\n[1] P. Arbelaez, M. Maire, C. Fawlkes, and J. Malik. Contour\ndetection and hierarchical image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(5),\n2011. 1, 2, 4, 5, 7\n[2] T. Brox, C. Bregler, and J. Malik. Large displacement optical flow. In Proceedings of Computer Vision and Pattern\n\n\fFigure 6. Example results of Gb1 using different input layers: a) color and soft-segmentation on BSDS300; b) color, soft-segmentation and\noptical flow on CMU Motion Dataset; c) color and depth from RGB-D images.\n\nRecognition, 2009. 7\n[3] J. Canny. A computational approach to edge detection. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n8(6):679\u2013698, 1986. 1, 2, 6\n[4] J. Carreira and C. Sminchisescu. Constrained parametric\nmin-cuts for automatic object segmentation. In Proceedings\nof Computer Vision and Pattern Recognition, 2010. 6, 7\n[5] A. Cumani. Edge detection in multispectral images. Computer Vision, Graphics, and Image Processing, 53(1), 1991.\n2\n[6] S. Di Senzo. A note on the gradient of a multi-image. Computer Vision, Graphics, and Image Processing, 33(1), 1986.\n2, 4\n[7] P. Dollar, Z. Tu, and S. Belongie. Supervised learning of\nedges and object boundaries. In Proceedings of Computer\nVision and Pattern Recognition, 2006. 2\n[8] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik.\nSemantic contours from inverse detectors. In Proceedings of\nInternational Conference on Computer Vision, 2011. 1\n[9] X. He and A. Yuille. Occlusion boundary detection using\npseudo-depth. In Proceedings of European Conference on\nComputer Vision, 2010. 1, 6\n[10] T. Kanade. Image understanding research at CMU. In Image\nUnderstanding Workshop, 1987. 2, 4\n[11] M. Koschan and M. Abidi. Detection and classification of\nedges in color images. Signal Processing Magazine, Special\nIssue on Color Image Processing, 22(1), 2005. 1, 2\n\n[12] A. Levin, D. Lischinski, and Y. Weiss. A closed form solution to natural image matting. In Proceedings of Computer\nVision and Pattern Recognition, 2006. 5\n[13] J. Mairal, M. Leordeanu, F. Bach, M. Hebert, and J. Ponce.\nDiscriminative sparse image models for class-specific edge\ndetection and image interpretation. In Proceedings of European Conference on Computer Vision, 2008. 1\n[14] D. Marr and E. Hildtreth. Theory of edge detection. In Proceedings of Royal Society of London, 1980. 1, 2\n[15] D. Martin, C. Fawlkes, and J. Malik. Learning to detect natural image boundaries using local brightness, color, and texture cues. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(5), 2004. 2, 4, 5, 6\n[16] J. Prewitt. Object enhancement and extraction. In Picture\nProcessing and Psychopictorics, pages 75\u2013149. Academic\nPress, New York, 1970. 1, 2\n[17] X. Ren. Multi-scale improves boundary detection in natural\nimages. In Proceedings of European Conference on Computer Vision, 2008. 2\n[18] L. Roberts. Machine perception of three-dimensional solids.\nIn J. Tippett et al., editors, Optical and Electro-Optical Information Processing, pages 159\u2013197. MIT Press, 1965. 1,\n2\n[19] M. Ruzon and C. Tomasi. Edge, junction, and corner detection using color distributions. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 23(11), 2001. 1, 2\n[20] M. Sargin, L. Bertelli, B. Manjunath, and K. Rose. Probabilistic occlusion boundary detection on spatio-temporal lat-\n\n\f[21]\n\n[22]\n\n[23]\n\n[24]\n\ntices. In Proceedings of International Conference on Computer Vision, 2009. 6\nJ. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 22(8):888\u2013905, 2000. 5\nA. Stein and M. Hebert. Occlusion boundaries from motion:\nLow-level detection and mid-level reasoning. International\nJournal of Computer Vision, 82(3), 2009. 1, 4, 6\nD. Sun, S. Roth, and M. Black. Secrets of optical flow estimation and their principles. In Proceedings of Computer\nVision and Pattern Recognition, 2010. 4, 6\nP. Sundberg, T. Brox, M. Maire, P. Arbelaez, and J. Malik.\nOcclusion boundary detection and figure/ground assignment\nfrom optical flow. In Proceedings of Computer Vision and\nPattern Recognition, 2011. 1, 2, 6\n\n\f"}