{"id": "http://arxiv.org/abs/1204.3972v3", "guidislink": true, "updated": "2013-03-13T21:55:59Z", "updated_parsed": [2013, 3, 13, 21, 55, 59, 2, 72, 0], "published": "2012-04-18T04:43:24Z", "published_parsed": [2012, 4, 18, 4, 43, 24, 2, 109, 0], "title": "EigenGP: Sparse Gaussian process models with data-dependent\n  eigenfunctions", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.5242%2C1204.2174%2C1204.5292%2C1204.1383%2C1204.6477%2C1204.5524%2C1204.5786%2C1204.0738%2C1204.1019%2C1204.5191%2C1204.2468%2C1204.1045%2C1204.3387%2C1204.1684%2C1204.5855%2C1204.0218%2C1204.1009%2C1204.3518%2C1204.0070%2C1204.2947%2C1204.2525%2C1204.1369%2C1204.2441%2C1204.4721%2C1204.1592%2C1204.4347%2C1204.4830%2C1204.1433%2C1204.5985%2C1204.5144%2C1204.3447%2C1204.3290%2C1204.3375%2C1204.1657%2C1204.4914%2C1204.4130%2C1204.2716%2C1204.1668%2C1204.6547%2C1204.1723%2C1204.0043%2C1204.2454%2C1204.4047%2C1204.5135%2C1204.4038%2C1204.2739%2C1204.2993%2C1204.4533%2C1204.2373%2C1204.4718%2C1204.0362%2C1204.0289%2C1204.6126%2C1204.4484%2C1204.5246%2C1204.0022%2C1204.5430%2C1204.3252%2C1204.1122%2C1204.2211%2C1204.1874%2C1204.3972%2C1204.5543%2C1204.5705%2C1204.0721%2C1204.5825%2C1204.2119%2C1204.4161%2C1204.1161%2C1204.2481%2C1204.2263%2C1204.6134%2C1204.2273%2C1204.0399%2C1204.1566%2C1204.0366%2C1204.1810%2C1204.3271%2C1204.1568%2C1204.3186%2C1204.3793%2C1204.2367%2C1204.2292%2C1204.1238%2C1204.5553%2C1204.6425%2C1204.4575%2C1204.0302%2C1204.3998%2C1204.1361%2C1204.4231%2C1204.2644%2C1204.5340%2C1204.6226%2C1204.0833%2C1204.0912%2C1204.1352%2C1204.5197%2C1204.4732%2C1204.6454%2C1204.1607&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "EigenGP: Sparse Gaussian process models with data-dependent\n  eigenfunctions"}, "summary": "Gaussian processes (GPs) provide a nonparametric representation of functions.\nHowever, classical GP inference suffers from high computational cost and it is\ndifficult to design nonstationary GP priors in practice. In this paper, we\npropose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve\n(KL) expansion of a GP prior. We use the Nystrom approximation to obtain data\ndependent eigenfunctions and select these eigenfunctions by evidence\nmaximization. This selection reduces the number of eigenfunctions in our model\nand provides a nonstationary covariance function. To handle nonlinear\nlikelihoods, we develop an efficient expectation propagation (EP) inference\nalgorithm, and couple it with expectation maximization for eigenfunction\nselection. Because the eigenfunctions of a Gaussian kernel are associated with\nclusters of samples - including both the labeled and unlabeled - selecting\nrelevant eigenfunctions enables EigenGP to conduct semi-supervised learning.\nOur experimental results demonstrate improved predictive performance of EigenGP\nover alternative state-of-the-art sparse GP and semisupervised learning methods\nfor regression, classification, and semisupervised classification.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=1204.5242%2C1204.2174%2C1204.5292%2C1204.1383%2C1204.6477%2C1204.5524%2C1204.5786%2C1204.0738%2C1204.1019%2C1204.5191%2C1204.2468%2C1204.1045%2C1204.3387%2C1204.1684%2C1204.5855%2C1204.0218%2C1204.1009%2C1204.3518%2C1204.0070%2C1204.2947%2C1204.2525%2C1204.1369%2C1204.2441%2C1204.4721%2C1204.1592%2C1204.4347%2C1204.4830%2C1204.1433%2C1204.5985%2C1204.5144%2C1204.3447%2C1204.3290%2C1204.3375%2C1204.1657%2C1204.4914%2C1204.4130%2C1204.2716%2C1204.1668%2C1204.6547%2C1204.1723%2C1204.0043%2C1204.2454%2C1204.4047%2C1204.5135%2C1204.4038%2C1204.2739%2C1204.2993%2C1204.4533%2C1204.2373%2C1204.4718%2C1204.0362%2C1204.0289%2C1204.6126%2C1204.4484%2C1204.5246%2C1204.0022%2C1204.5430%2C1204.3252%2C1204.1122%2C1204.2211%2C1204.1874%2C1204.3972%2C1204.5543%2C1204.5705%2C1204.0721%2C1204.5825%2C1204.2119%2C1204.4161%2C1204.1161%2C1204.2481%2C1204.2263%2C1204.6134%2C1204.2273%2C1204.0399%2C1204.1566%2C1204.0366%2C1204.1810%2C1204.3271%2C1204.1568%2C1204.3186%2C1204.3793%2C1204.2367%2C1204.2292%2C1204.1238%2C1204.5553%2C1204.6425%2C1204.4575%2C1204.0302%2C1204.3998%2C1204.1361%2C1204.4231%2C1204.2644%2C1204.5340%2C1204.6226%2C1204.0833%2C1204.0912%2C1204.1352%2C1204.5197%2C1204.4732%2C1204.6454%2C1204.1607&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Gaussian processes (GPs) provide a nonparametric representation of functions.\nHowever, classical GP inference suffers from high computational cost and it is\ndifficult to design nonstationary GP priors in practice. In this paper, we\npropose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve\n(KL) expansion of a GP prior. We use the Nystrom approximation to obtain data\ndependent eigenfunctions and select these eigenfunctions by evidence\nmaximization. This selection reduces the number of eigenfunctions in our model\nand provides a nonstationary covariance function. To handle nonlinear\nlikelihoods, we develop an efficient expectation propagation (EP) inference\nalgorithm, and couple it with expectation maximization for eigenfunction\nselection. Because the eigenfunctions of a Gaussian kernel are associated with\nclusters of samples - including both the labeled and unlabeled - selecting\nrelevant eigenfunctions enables EigenGP to conduct semi-supervised learning.\nOur experimental results demonstrate improved predictive performance of EigenGP\nover alternative state-of-the-art sparse GP and semisupervised learning methods\nfor regression, classification, and semisupervised classification."}, "authors": ["Yuan Qi", "Bo Dai", "Yao Zhu"], "author_detail": {"name": "Yao Zhu"}, "author": "Yao Zhu", "arxiv_comment": "10 pages, 19 figures", "links": [{"href": "http://arxiv.org/abs/1204.3972v3", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/1204.3972v3", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.CO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "stat.ML", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/1204.3972v3", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/1204.3972v3", "journal_reference": null, "doi": null, "fulltext": "EigenGP: Sparse Gaussian process models with data-dependent\neigenfunctions\n\nYuan Qi\nDepartments of CS and Statistics\nPurdue University\nWest Lafayette, IN 47906\nalanqi@purdue.edu\n\nBo Dai\nDepartment of Computer Science\nPurdue University\nWest Lafayette, IN 47906\ndaibo@purdue.edu\n\nAbstract\nGaussian processes (GPs) provide a nonparametric representation of functions. However, classical GP inference suffers from high\ncomputational cost and it is difficult to design nonstationary GP priors in practice.\nIn this paper, we propose a sparse Gaussian process model, EigenGP, based on the\nKarhunen-Lo\u00e8ve (KL) expansion of a GP\nprior. We use the Nystr\u00f6m approximation\nto obtain data dependent eigenfunctions and\nselect these eigenfunctions by evidence maximization. This selection reduces the number\nof eigenfunctions in our model and provides\na nonstationary covariance function. To handle nonlinear likelihoods, we develop an efficient expectation propagation (EP) inference\nalgorithm, and couple it with expectation\nmaximization for eigenfunction selection. Because the eigenfunctions of a Gaussian kernel are associated with clusters of samples\n\u2013 including both the labeled and unlabeled\n\u2013 selecting relevant eigenfunctions enables\nEigenGP to conduct semi-supervised learning. Our experimental results demonstrate\nimproved predictive performance of EigenGP\nover alternative state-of-the-art sparse GP\nand semisupervised learning methods for regression, classification, and semisupervised\nclassification.\n\n1\n\nIntroduction\n\nGaussian processes (GPs) are powerful nonparametric\nBayesian models with numerous applications in statistics and machine learning. However, we face two limitations when using GPs in practice. First, it is difficult to construct nonstationary covariance functions\nfor GPs because it is statistically and computationally challenging to parameterize positive definite co-\n\nYao Zhu\nDepartment of Computer Science\nPurdue University\nWest Lafayette, IN 47906\nzhu36@purdue.edu\n\nvariance matrices as a function of the input space. In\npractice, while nonstationary GPs have been developed and applied to real world applications, they are\noften limited to low-dimensional problems, such as applications in spatial statistics (Paciorek & Schervish,\n2004; Higdon et al., 1998). Second, GP inference\nis computationally challenging. Even for the regression case where the GP prediction formula is analytic, training the exact GP model with N points\ndemands an O(N 3 ) computational cost for inverting\nthe covariance matrix. To reduce the computational\ncost, a variety of approximate sparse GP inference\napproaches have been developed (Williams & Seeger,\n2001; Csat\u00f3 & Opper, 2002; Snelson & Ghahramani,\n2006; L\u00e1zaro-Gredilla et al., 2010; Williams & Barber,\n1998; Qi et al., 2010) \u2013 for example, using the Nystr\u00f6m\nmethod to approximate covariance matrices (Williams\n& Seeger, 2001) or grounding the GP on a small set of\n(blurred) basis points (Snelson & Ghahramani, 2006;\nQi et al., 2010). An elegant unifying view for various\napproximate sparse GP regression models is given in\nQui\u00f1onero-Candela & Rasmussen (2005). Note that\nall these sparse GP approaches gain computational efficiency with certain approximations \u2013 possibly degenerating prediction accuracy.\nIn this paper, we propose a new approach, EigenGP,\nthat addresses these two issues in a principled framework. Specifically, we project the GP prior into a space\nspanned by eigenfunctions, and add white noise to it\nto handle prediction uncertainty at infinity. The eigenfunctions depend on data input so that the covariance\nchanges in the input space. Furthermore, based on the\nobserved data, we select a small number of eigenfunctions by maximizing model marginal likelihood. The\nprojection of GPs into a small eigensubspace can remove noise in function values; this is similar to what\nprinciple component analysis does in noise reduction,\nbut we do it in a functional space for the output. Furthermore, with only a few eigenfunctions in the model,\nwe can greatly reduce the computational cost; it is\nO(N L2 ) \u2013 rather than O(N 3 ) \u2013 where L is the number\n\n\fof the selected eigenfunctions. This selection also enables semi-supervised learning based on a commonlyused clustering assumption. This assumption states\nthat if points are in the same cluster, they are likely\nto be of the same class. Because eigenfunctions of a\nGaussian covariance function correspond to clusters of\ndata points, we can choose clusters \u2013 based on both\nlabeled and unlabeled data points \u2013 relevant for the\npredictions.\nThe rest of the paper is organized as follows. Section 2 describes the background of GPs. Section 3\nand 4 present the EigenGP model, EP inference for\nEigenGP, and expectation maximization updates for\nsparsification. In Section 5, we discuss related works\n\u2013 in particular, the difference between EigenGP and\nthe Nystr\u00f6m method (Williams & Seeger, 2001) and\nrelevance vector machine (Tippings, 2000). Section\n6 shows experimental results on regression, classification and semi-supervised classification, demonstrating improved predictive performance of EigenGP over\nstate-of-the-art approaches, including support vector\nmachines, GPs and sparse GPs.\n\n2\n\nBackground of Gaussian processes\n\nWe denote N independent and identically distributed\nsamples as D = {(x1 , y1 ), . . . , (xn , yn )}N , where xi is\na d dimensional input (i.e., explanatory variables) and\nyi is a scalar output (i.e., a response), which we assume\nis the noisy realization of a latent function f at xi .\nA Gaussian process places a prior distribution over the\nlatent function f . Its projection fx at {xi }N\ni=1 defines\na joint Gaussian distribution: p(fx ) = N (f |m0 , K),\nwhere, without any prior preference, the mean m0\nare set to 0 and the covariance function k(xi , xj ) \u2261\nK(xi , xj ) encodes the prior notion of smoothness. A\ntypical choice of k is Gaussian covariance (or kernel)\n! ||x\u2032 \u2212 x||2 \u0001\n,\n(1)\nk(x, x\u2032 ) = exp \u2212\n2\u03b7 2\nwhere \u03b7 controls the smoothness of the function. Note\nthat this covariance function has the same value as\nlong as ||x\u2032 \u2212x|| remains the same \u2013 regardless where x\u2032\nand x are. Thus this leads to a stationary GP model.\nFor regression, we use a Gaussian likelihood function\np(yi |f ) = N (yi |f (xi ), vy ),\n\n(2)\n\ny\n\n\u03b8 (or w)\n\u03c61\n\nf\n\u03c6j\n\n\u03c6L\n\nki\n\nkQ\n\nU\nk1\n\nFigure 1: Deep structure of EigenGP.\nGiven the Gaussian process prior over f and the data\nlikelihood, the exact posterior process is\np(f |D, y) \u221d GP (f |0, k)\n\nN\nY\n\ni=1\n\np(yi |f )\n\n(4)\n\nFor the regression problem, the posterior process has\nan analytical form. But to make a prediction on a\nnew sample, we need to invert a N by N matrix. If\nthe training set is big, this matrix inversion will be too\ncostly. For classification or other nonlinear problems,\nthe computational cost is even higher because we do\nnot have an analytical solution to the posterior process and the complexity of the process grows with the\nnumber of training samples.\n\n3\n\nModel\n\nTo obtain a nonstationary covariance function and enable fast inference, EigenGP projects the GP prior in\nan eigensubspace and add a white noise Gaussian process \u03b80 (x) of constant variance w0 so that its prediction\nuncertainty does not shrink to zero at infinity. Specifically, we set the latent function f\nf (x) =\n\nL\nX\n\n\u03b8j \u03c6j (x) + \u03b80 (x)\n\n(5)\n\nj=1\n\nwhere \u03c6j (x) are eigenfucntions of the GP prior. We\nassign a Gaussian prior over \u03b8 = [\u03b81 , . . . , \u03b8L ], \u03b8 \u223c\nN (0, diag(w)), so that f follows a GP prior with zero\nmean and the following covariance function\nk\u0303(x, x\u2032 ) =\n\nL\nX\n\nwj \u03c6j (x)\u03c6j (x\u2032 ) + w0 \u03b4x,x\u2032\n\n(6)\n\nj=1\n\n(3)\n\nwhere \u03b4x,x\u2032 = 1 if x is the same as x\u2032 and \u03b4x,x\u2032 = 0\notherwise. We choose L in (5) to be a reasonably small\nnumber so that we can conduct efficient inference for\nthis model as shown later.\n\nwhere \u01eb models the labeling error and \u03c3(*) is a cumulative distribution function (cdf) of a standard Gaussian\n(i.e., the probit model).\n\nTo obtain the eigenfunctions \u03c6j (x) of a GP prior, we\ncan use the Galekin projection to approximate them by\nHermite polynomials (Marzouk & Najm, 2009). But\n\nwhere vy is the observation noise. For classification,\nthe data likelihood has the form\np(yi |f ) = (1 \u2212 \u01eb)\u03c3(f (xi )yi ) + \u01eb\u03c3(\u2212f (xi )yi )\n\n\ffor high dimensional problems, this approach requires\na tensor product of univariate Hermite polynomials\nthat dramatically increases the number of parameters.\nTo avoid this problem, we use the Nystr\u00f6m method\n(Williams & Seeger, 2001) that allows us to efficiently\nobtain an approximation to the eigenfunctions in a\nhigh dimensional space. Specifically, assuming basis\npoints B = [b1 , . . . , bQ ] (Q \u2265 L) are i.i.d. samples\nfrom the probability density p(x), we can replace\nZ\nk(x, x\u2032 )\u03c6j (x)p(x)dx = \u03bbj \u03c6j (x)\n(7)\nby its Monte Carlo approximation\nQ\n\n1 X\nk(x, bi )\u03c6j (bi ) \u2248 \u03bbj \u03c6j (x)\nQ i=1\n\n(8)\n\nThen, with simple derivations, we obtain the j-th\neigenfunction \u03c8 j (x) as follows\n\u221a\nQ\n(9)\n\u03c6j (x) = Q k(x)\u0169j = k(x)uj\n\u03bbj\nwhere k(x) , [k(x, b1 ), . . . , k(x, bQ )], \u03bbQ\nj and \u0169j are\nthe j-th eigenvalue and eigenvector\u221aof the covariance\nfunction evaluated at B, and uj = \u03bbQQ \u0169j . In practice,\n\nour model. In this fashion, EigenGP can be viewed as\nthe method of sieves (Geman & Hwang, 1982), which\nhas been applied to semi-nonparametric models with\ngreat success (Chen, 2007).\nNote that given w and U = [u1 , . . . , uL ], the prior\nover f is nonstationary because its covariance function\nEigenGP in (6) varies at different regions of x. This\ncomes at no surprise since the eigenfunctions are tied\nwith p(x) in (7). This nonstationarity reflects the fact\nthat our model is adaptive to the distribution of the\nexplanatory variables x.\nIf we use the Gaussian kernel (1) whose eigenfunctions\ncorrespond to clusters of data, we can use the cluster\nassumption for semi-supervised learning. More specifically, we first use both labeled and unlabeled data to\nlearn clusters in the whole dataset. Assuming that\nmost data points in the same cluster share the same\nlabel, we can then propagate the labels of points in a\ncluster to unlabeled data points in the same cluster.\nHowever, when labeled data points in the same cluster\nhave different signs, our ARD sparsification allows us\nto automatically choose appropriate eigenfunctions to\naccommodating the sign change in a principled way.\n3.1\n\nAn illustrative example\n\nj\n\nwe often select the basis points B as a random subset\nof X. We can also first estimate p(x) based on X\nand then sample multiple B from p(x) so that we can\nobtain estimation uncertainty in \u03c6j (x) (i.e., uj ).\n\nNow we give a toy example in Figure 2 to illustrate\nwhy the selection of eigenfunctions removes noise from\nfunction values and leads to easy classification. We\nalso visualizes the nonstationarity of our model.\n\nInserting (9) into (5), we obtain\n\nThis equation reveals a two-layer structure of our\nmodel as visualized in Figure 1 (for simplicity, we do\nnot shown \u03b80 (x) in this figure). Thus our model can be\nviewed as a deep Bayesian kernel machine. The deep\nstructure highlights the difference between our model\nand the relevance vector machine (Tippings, 2000),\nwhich links the kernel function ki to f directly and\ndoes not have the additional white noise w0 .\n\nGiven 300 samples from a mixture of four Gaussian\ncomponents (See green \"+\" markers in the top panel\nof 2.a), we estimate the eigenfunctions based on a\nGaussian covariance function with the kernel width\n\u03b7 = 0.2. The 1st, 10th, 20th and 30th eigenfuntions\nare shown in Figure 2.(a). The eigenfunctions control\nthe smoothness of the model. Using more eigenfunctions, we can capture more variability in function values. However, too much modeling flexibility does not\nhelp prediction accuracy; in Figure 2.(a), we can see\nthe 10th, 20th and 30th eigenfunctions are not useful in\ndiscriminating samples from two classes, represented\nby red circles and black \"x\" markers.\n\nTo learn the structure of the second layer in EigenGP,\nwe use an empirical Bayesian technique, automatic relevance determination (ARD) (MacKay, 1992), which\nprunes edges (elements of w) by maximizing model evidence (See Section 4 for more details). Since L < N\nand w is sparsified, estimating the posterior process\nof f is computationally efficient. Also, we constrain\nf in the eigensubspace and therefore reduce noise in\nfunction values, which robustifies the model. If we\nhave more training samples and allow a longer training time,we can increase L, i.e., the eigensubpsace for\n\nFigure 2.(b) shows that our method identifies four\ndiscriminative eigenfunctions for classification; the\nweights of the selected eigenfunctions are shown in Figure 3.(a). The first, third and forth eigenfunctions are\nselected and they separates samples in the first, second and third data clusters from each other while each\ncluster contains samples with the same labels. What is\ninteresting is that the second eigenfunction that covers the forth cluster is not selected, while this cluster\ncontains labels from two classes. Instead, the ninth\neigenfunction is selected and it separates samples in\n\nf (x) =\n\nL\nX\nj=1\n\n\u03b8j\n\nQ\nX\n\nuij k(x, bi ) + \u03b80 (x)\n\n(10)\n\ni=1\n\n\f0.2\n\n0.2\n\n0\n\n0.1\n\n\u22120.2\n\u22121\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\u22121\n\n7\n\n0.2\n\n0.1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n0\n\u22121\n\n7\n\n0.2\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n\u22120.2\n\u22121\n\n7\n\n0.5\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n\u22120.2\n\u22121\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0.5\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n\u22120.2\n\u22121\n\n7\n\n(a) 1st, 10th, 20th and 30th eigenfuntions\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n(b) Selected eigenfunctions when a\ncluster having samples from two\nclasses\n\n\u22120.2\n\u22121\n\n5\n\n5\n\n4\n\n4\n\n4\n\nx'\n\n5\n\nx'\n\n6\n\n3\n2\n\n2\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n\u22121\n\n0\n\n1\n\n2\n\nx\n(d) Stationary Gaussian covariance\nfunction based on all the eigenfunctions\n\n7\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n3\n\n2\n\n0\n\n6\n\n(c) Selected eigenfunctions when a\ncluster contains a mislabeled datapoint\n\n6\n\n\u22121\n\n5\n\n0\n\n0\n\n6\n\n3\n\n4\n\n0.2\n\n0\n\n0\n\n3\n\n0\n\u22120.2\n\u22121\n\n0.2\n\n\u22120.5\n\u22121\n\n2\n\n0.2\n\n0\n\u22120.2\n\u22121\n\n1\n\n0\n\n0\n\n0.2\n\n\u22120.5\n\u22121\n\n0\n\n0.2\n\n0\n\n\u22120.2\n\u22121\n\nx'\n\n0.2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n\u22121\n\n0\n\n1\n\nx\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nx\n\n(e) Nonstationary covariance function\nbased on the selected eigenfunctions\nin (b)\n\n(f) Nonstationary covariance function\nbased on the selected eigenfunctions\nin (c)\n\nFigure 2: A toy example. Subfigure (a) shows the eigenfunctions of the GP model with a Gaussian covariance.\n(b) and (c) depict the selected eigenfunctions learned with different labeled samples. (d) shows the Gaussian\ncovariance function evaluated at x and x\u2032 (both ranging from -1 to 7). It has a constant value along the\ndiagonal direction. (e) and (f) visualize the nonstationary covariance functions corresponding to the selected\neigenfunctions in (b) and (c).\n\nFigures 2.(d)-(f) show the values of the covariance\nfunctions corresponding to 2, demonstrating the nonstationarity of our model.(d)-(f). As shown in 2.(d),\nthe stationary Gaussian covariance function in (1) has\n\n0\n\nw\n\nIn Figure 2.(c), the samples in the forth cluster belong\nto one class, except a single outlier (the red circle). In\nthis case, EigenGP automatically selects the top four\neigenfunctions whose weights are shown in 3.(b). Since\nthe second eigenfunction covers the forth cluster with\nthe same sign, EigenGP removes the impact of the outlier, demonstrating the robustness of our model. This\nis similar to noise reduction in principle component\nanalysis but in a functional space for the output.\n\nw\n\nthe forth cluster \u2013 in other words, the samples with\nthe same label correspond to the same sign in this\neigenfunction. Therefore, based on these four selected\neigenfunctions we can accurately classify the samples.\n\n10\n20\nrank of eigenfunctions\n\n30\n\n0\n\n10\n20\nrank of eigenfunctions\n\n30\n\n(a) Based on data in Fig. (b) Based on data in Fig.\n2.(b)\n2.(c)\n\nFigure 3: Estimated w for the classification of two data\nsets in Figure 2.(b) and (c). Only a few eigenfunctions\nare associated with nonzero weights.\n\n\fa constant value along the diagonal direction in the\nimage. This is because the covariance value remains\nthe same when kx \u2212 x\u2032 k is a constant regardless where\nthe two samples x and x\u2032 are. In contrast, the covariance functions of EigenGP, defined by 6, change their\nvalues based on the values of x and x\u2032 along the diagonal direction as shown in (e) and (f). Note that\nthe upper-right corner of (e) corresponds to the ninth\neigenfunction (the bottom blue curve in (b)).\n\n4\n\nEP based evidence maximization\n\nFor Gaussian likelihoods (i.e., regression), we can use\nthe matrix inverse lemma to efficiently compute the\nposterior process in (4) and the marginal likelihood.\nFor general nonlinear likelihoods such as the probit\nmodel for classification, we can use expectation propagation (Minka, 2001) to map the nonlinear likelihood\ninto a linear Gaussian form N (gi |f (xi ), \u03c4i ) and approximate the posterior process in (4) by\nN\nY\nN (gi |f (xi ), \u03c4i ).\n(11)\nq(f ) \u221d GP (f |0, k\u0303)\ni=1\n\nLet g = [g1 , . . . , gN ], \u03c4 = [\u03c41 , . . . , \u03c4N ], and \u03a6D (a N by\nL matrix) represent the values of the L basis functions\nat the N training points. Then combining (6) and (11)\nwith equations (6.66) and (6.67) in (Bishop, 2006), we\nobtain the following proposition.\nProposition 1 The posterior process q(f ) defined in\n(11) has the mean function m(x) and covariance function V (x, x\u2032 ):\nm(x) = k(x)U\u03b1\nV (x, x\u2032 ) = k\u0303(x, x\u2032 ) \u2212 k(x)U\u03b2UT k(x\u2032 )\n\n(12)\n(13)\n\nwhere W = diag(w), \u03b2 = W\u03a6D T (K\u0303 + w0 I +\ndiag(\u03c4 ))\u22121 \u03a6D W, K\u0303 = \u03a6D W\u03a6D T and \u03b1 = \u03b2g.\nFrom this proposition, we see the additional white\nnoise plays a role equivalent to the Gaussian approximation of the likelihoods so that we can absorb it in\nthe likelihoods. For the probit model, this amounts to\nincreasing the variance of the Gaussian in the cdf.\nTo estimate (\u03b1, \u03b2) (equivalently, (g, \u03c4 )), the expectation propagation inference repeats the following\nthree steps: message deletion, projection, and message update. In the message deletion step, we compute the partial belief q \\i (f ; \u03b1\\i , \u03b2 \\i ) by removing a\nmessage t\u0303i from the approximate posterior q(f ; \u03b1, \u03b2):\nq \\i (f ; \u03b1\\i , \u03b2 \\i ) \u221d q(f ; \u03b1, \u03b2)/t\u0303i . In the projection\nstep, we minimize the KL divergence between p\u0303(f ) \u221d\np(yi |f )q(f ; \u03b1\\i , \u03b2 \\i ) and the new approximate posterior q(f ; \u03b1, \u03b2), such that the information from the i-th\ndata point is incorporated into the model. Finally,\n\nthe message t\u0303i is updated based on the new and old\nposteriors: t\u0303i \u221d q(f ; \u03b1, \u03b2)/q \\i (f ; \u03b1\\i , \u03b2 \\i ).\n4.1\n\nProjection\n\nWe start with the projection step, since it is the most\ncrucial step of EP. From (11), we see that the posterior GP of EigenGP defines an exponential family with\nfeatures {fx , fx fxT }, fx = (f (x1 ), . . . , f (xNl ))T . Therefore, the minimization of the KL divergence between\np\u0303(f ) and its posterior process q(f ) is achieved by moment matching on the mean mx and the covariance\nVx of fx . The moment matching equations are\nd log Z\n\\i\nmx = m\\i\n(14)\nx + V (X, xi )\ndm\\i (xi )\nVx = Vx\\i + V\\i (X, xi )\n\nd log2 Z\nV\\i (xi , X)\ndm\\i (xi )2\n\n(15)\n\nR\nwhere Z = q \\i (f )p(yi |f ) df , and m\\i (xi ) is the mean\nof q \\i (f (xi )) and V\\i (xi , X) is covariance matrix between xi and X.\nGiven mx and Vx , we want to compute \u03b1 and \u03b2. To\ndo so, first note that, from Proposition 1, it follows\nthat\nmx = Kx,B U\u03b1 Vx = K\u0303 \u2212 Kx,B U\u03b2UT KT\nx,B (16)\nwhere K\u0303 = Kx,B UWUT KT\nx,B , and the (i, j) element\nof the matrix Kx,B is k(xi , bj ).\nThen combining (14) and (16), we obtain\n\u03b1 = \u03b1\\i + h\n\nd log Z\ndm\\i (xi )\n\n(17)\n\nwhere h , K\u22121 V\\i (X, xi ) = pi \u2212 \u03b2 \\i \u03c6i , pi ,\ndiag(w)\u03c6i and \u03c6i , k(xi )U is precomputed before\nthe EP inference.\nInserting (16) to (15), we obtain the update for \u03b2\nd2 log Z\n(18)\n\u03b2 = \u03b2 \\i \u2212 hhT\ndm\\i (xi )2\nThe above equations define the projection step.\nFor the classification likelihood (3) where \u03c3(*) is the\nprobit function, the quantities in the projection step\n(17) and (18) are\nm\\i (xi )yi\nz=p\nv \\i (xi , xi )\n\nZ = \u01eb + (1 \u2212 2\u01eb)\u03c3(z)\nd log Z\n= \u03b3yi\ndm\\i (xi )\nd2 log Z\n\u03b3(m\\i (xi )yi + v \\i (xi )\u03b3)\n=\u2212\n\\i\n2\n(dm (xi ))\nv \\i (xi )\nwhere \u03b3 =\n\n(1\u22122\u01eb)N (z|0,1)\nZ\n\n\u221a\n\nv \\i (xi )\n\n.\n\n(19)\n(20)\n(21)\n(22)\n\n\f4.2\n\nMessage update\n\nGiven the new q(f ), we will update the message t\u0303i (f )\naccording to the ratio q(f )/q \\i (f ):\n1\n\nt\u0303i (f ) =\n\n|Vx |\u2212 2 exp(\u2212 21 (fx \u2212 mx )Vx\u22121 (fx \u2212 mx ))\n\\i\n\n\\i\n\n1\n\n\\i\n\n\\i\n\n|Vx |\u2212 2 exp(\u2212 21 (fx \u2212 mx )(Vx )\u22121 (fx \u2212 mx ))\n\n= N (f (xi )|gi , \u03c4i )\n! d2 log Z \u0001\u22121\n\u03c4i , \u2212\n\u2212 v \\i (xi )\n(dm\\i (xi ))2\n! d2 log Z \u0001\u22121 d log Z\ngi , m\\i (xi ) \u2212\n(dm\\i (xi ))2\ndm\\i (xi )\n\n(23)\n\n(24)\n(25)\n\nT \\i\n\\i\nwhere v\\i (xi ) = \u03c6T\ni h and m (xi ) = \u03c6i \u03b1 . The\nfunction t\u0303i (f ) can be viewed as a message from the\ni-th data point to q(f ).\n\nIf we want to approximate the marginal likelihood of\nthe model, we need to scale N (f (xi )|gi , \u03c4i ) so that the\nmessage t\u0303i (f ) = si exp(f (xi |gi , \u03c4i )) = Zq(f )/q \\i (f )\npreserves\nthe local\nR\nR \"evidence\" - in other words,\nt\u0303i (f )q \\i (f )df = ti (f )q \\i (f )df . It is easy to show\nlog s = log Z \u2212\n\n1\n((log Z)\u2032 )2\nlog(\u2212(log Z)\u2032\u2032 \u03c4i ) \u2212\n2\n2(log Z)\u2032\u2032\n\nwhere (log Z)\u2032 and (log Z)\u2032\u2032 are the first- and the\nsecond- order derivatives of log Z over m\\i (xi ).\n4.3\n\nNote that we compute m(xi ) = \u03c6T\ni \u03b1 and v(xi ) =\nT \\i\nh\n.\n(diag(w)\n\u2212\n\u03b2)\u03c6\n=\n\u03c6\n\u03c6T\ni\ni\ni\n4.4\n\nEigenfunction selection\n\nEigenGP uses a small subset of eigenfunctions. But\nunlike principle component analysis, we can improve\nthe modeling power by not simply picking the top few\neigenfunctions, as illustrated in Figure 2.(b) where\nthe ninth, instead of the second, eigenfunction is selected. To select relevant eigenfunctions, we maximize\nthe model marginal likelihood obtained from expectation progagation:\np(y|w, X) =\n\nTo delete a message t\u0303i (f ), we need to compute q (f ) \u221d\nq(f )/t\u0303i (f ). Instead of computing this ratio directly,\nwe can equivalently multiply its reciprocal with the\ncurrent q(f ). Then we can solve the multiplication\nby minimizing KL(q(f )kq \\i (f )t\u0303i (f )) over q \\i (f ). Since\nq(f ), q \\i (f ), and t\u0303i (f ) all have the form of the exponential family, the minimal value of this KL-divergence\nis 0, so that q \\i (f ) \u221d q(f )/t\u0303i (f ). This KL minimization can be easily done by the moment matching equations similar to (17) and (18):\n(26)\n2\n\\i\n\\i d log Z\u0303d\n\\i\n\\i d log Z\u0303d\n(h\\i )T\n, \u03b2 =\u03b2\u2212h\n\u03b1 =\u03b1+h\ndm(xi )\n(dm(xi ))2\nwhere\nh\\i , pi \u2212 \u03b2\u03c6i\nZ\n1 \\i\nZ\u0303d =\nq (f )df\nt\u0303i (f )\n\\i\n\u221d N (ui |pT\ni mx , \u2212\u03c4i + v(xi ))\n\nd2 log Z\u0303d\n= \u03c4i \u2212 v(xi )\ndm(xi )2\n\nd log Z\u0303d\nd2 log Z\n(m(xi ) \u2212 gi )\n=\ndm(xi )\ndm(xi )2\n\nN (fx |0, K\u0303)\n\nY\n\nt\u0303i (f )dfx\n\ni\n\nY\nY 1\n1\n= ( si )( wj2 )|W \u2212 \u03b2| 2 *\ni\n\nj\n\n(27)\nX\n1 T\n\u22121\n2\n* exp( (\u03b1 (W \u2212 \u03b2) \u03b1 \u2212\ng (i)/\u03c4i ))\n2\ni\nWe maximize the marginal likelihood via automatic\nrelevance determination (ARD) (MacKay, 1992). To\ndo so, we use an EP-EM approach. In the E-step, we\ncompute q(f ) via EP. In the M-step, we maximize the\nexpected likelihood Eq [log p(y, fx |w)] over w, which\nleads to the following update:\n\nMessage deletion\n\\i\n\nZ\n\nwjnew = wj \u2212 \u03b2j,j + \u03b1j2 .\n\n(28)\n\nInstead of the EM updates, we can use an active-set\nmethod (Faul & Tipping, 2002; Qi et al., 2004) to obtain efficient updates of w. We can certainly explore\nother sparsification approaches, like l1 penalty (i.e, a\nLaplace prior), an elastic net penalty, or spike and slab\npriors to sparsify w. But a thorough comparison of\nvarious sparse priors is out of the scope of this paper.\n4.5\n\nComputational complexity\n2\n\nd log Z\nSince (dm\n\\i (x ))2 is a scalar and h is a L by 1 veci\ntor, it takes O(L2 ) to update \u03b2 via (18). Similarly, it\ntakes O(L2 ) to update \u03b2 \\i via (26). Therefore, given\nNl labeled training points, the computational cost is\nO(L2 Nl ) per EP iteration over all the data points.\nSince in practice the number of EP iterations is small\n(e.g., 10), the overall cost is O(L2 Nl ). Because of using the active-set method, the computation cost will\nbe further significantly reduced to O(r2 Nl ) where r is\nthe actual number of eigenfunctions used in the EP\niterations and r < L. In addition, we have the cost of\ncomputing the top L eigenvectors U of the Q by Q kernel matrix KB ; using efficient iterative algorithms such\nas the Lanzcos method, it takes O(Q2 L). In summary,\nthe complexity of the EP inference is O(Q2 L + Nl L2 ).\n\n\fRelated work\n\nOur model also bears similarity to relevance vector machine (RVM) (Tippings, 2000) and sparse spectrum\nGaussian process (SSGP) by L\u00e1zaro-Gredilla et al.\n(2010). But ours differs from these methods in four\naspects. First, based on the K-L expansion of a covariance function, the basis functionsn of ours is quite\ndifferent those used in RVM or SSGP. Second, with\nthe white noise \u03b80 in the model, ours gives nonzero\nprediction uncertainty when a test sample is far from\nthe training samples. In contrast, for this case the\nprediction of RVM shrinks to zero. Third, Figure 1\nshows that our model has a two-layer structure, while\nRVM and SSGP do not. Note that while we estimate\nthe parameters u and w (or equivalently \u03b8) associated\nwith the two layers, our first layer training does not\ndepend on the label y \u2013 in this sense, it echos the idea\nof unsupervised first layer training used in deep learning. Forth, our method can conduct semi-supervised\nlearning while RVM and SSGP cannot.\n\n6\n\nExperiments\n\nIn this section we test EigenGP on regression, classification, and semi-supervised classification tasks.\n6.1\n\nRegression\n\nFor regression, we compared EigenGP with the full\nGP and two sparse approximate GP algorithms: the\nNystr\u00f6m method (Williams & Seeger, 2001), and the\npseudo-input approach, also known as Fully Independent Training Conditional (FITC) approximation\n(Snelson & Ghahramani, 2006). We used the Gaussian covariance (1) for all the competing methods. We\n\nFull\u2212GP\nFITC\nEigenGP\nEigenGP*\n\n4.2\n4\nRMSE\n\nOur work is built upon the seminal work by Williams\n& Seeger (2001). But they differ in three critical aspects. First, we define a valid probabilistic model\nbased on an eigen-decomposition of the GP prior. By\ncontrast, the previous approach by Williams & Seeger\n(2001) aims at a low-rank approximation to the finite\ncovariance/kernel matrix used in GP training \u2013 purely\nfrom a numerical approximation perspective \u2013 and its\npredictive distribution is not well-formed in a probabilistic framework (e.g., it may give a negative variance\nof the predictive distribution). Accordingly, both predictive means and variances of these two methods are\ndifferent. Second, while the previous Nystr\u00f6m method\nsimply uses the first few eigenvectors, we maximize the\nmodel marginal likelihood to select eigenfunctions and\nadjust their weights, learning a nonstationary covariance function. Third, exploring the clustering property of the eigenfunctions of the Gaussian kernel, our\napproach can conduct semi-supervised learning, while\nthe previous one cannot.\n\n4.4\n\n3.8\n3.6\n3.4\n3.2\n3\n\n50\n\n100\n150\nNumber of Basis/Rank\n\n200\n\n(a) Boston Housing\nFull\u2212GP\nFITC\nNystrom\nEigenGP\nEigenGP*\n\n1.7\n1.6\n1.5\nRMSE\n\n5\n\n1.4\n1.3\n1.2\n1.1\n1\n0.9\n\n50\n\n100\n150\nNumber of Basis/Rank\n\n200\n\n(b) Pumadyn-8nm\n\nFigure 4: Regression results on Boston Housing and\nPumadyn-8nm. The results of the Nystr\u00f6m method\non Boston Housing are not included in the figure, since\ntheir errors are much larger than the others.\noptimized the kernel width \u03b7 for the full GP and used\nit for the Nystr\u00f6m method and EigenGP for a simple fair comparison. For the FITC, we used the optimization code from http://www.gatsby.ucl.ac.uk/\n~snelson/SPGP_dist.tgz to learn the kernel width\nfor each dimension and the basis points. Because the\noptimization is sensitive to local optima (as observed\nby (Snelson & Ghahramani, 2006) and (Qi et al.,\n2010)), we tried different initial values to improve the\nresults of FITC and presented here the best results we\nobtained (which are better than those of FITC using\nthe kernel parameters learned from the full GP). For\nEigenGP, we set w0 = 0.1 to have small white noise.\nWe compared the prediction accuracies of these alternative methods with the same computational complexity. Let Q1 and L be the number of basis points and\nthe total number of eigenfunctions of EigenGP and\nthe Nystr\u00f6m method, and Q2 be the number of basis points of FITC. The computational complexities of\nEigenGP and the Nystr\u00f6m method is O(Q21 L + N L2 ),\nwhile FITC takes O(N Q22 ). To make these algorithms\n\n\fhave the same cost, we required O(Q21 L + N L2 ) =\nO(N Q22 ). To achieve\u221athis, we could set L = Q2 and\nQ1 is smaller than N Q2 . In our experiments, we\nsimply set Q1 = Q2 as well (This gave FITC more\ntraining time because of N > Q2 ).\nThe results on two benchmark datasets, Boston Housing and Pumadyn-8nm, are reported in Figure 4.\nThese two datasets contain 506 and 8192 data points.\nFrom these two datasets, we randomly selected 400\nand 2000 points for training, respectively. And we used\nthe rest for testing. We repeated these experiments 10\ntimes and reported the average root mean square errors (RMSE) as well as the standard errors in Figure 4.\nFor EigenGP, we not only ran the version with sparsification over w as described before, but also tested a version of EigenGP that simply chose the top L eigenfunctions. We denote the later version as EigenGP*. The\nRMSE of the Nystr\u00f6m method on Boston housing are\n312.5, 41.68, 15.17 and 6.480 with 50, 100, 150 and 200\nbasis points, respectively. The corresponding standard\nerrors are 70.45, 11.23, 6.194 and 1.502. Despite the\nsimilarity between the two versions of EigenGPs and\nthe previous Nystr\u00f6m method, EigenGPs significantly\noutperformed the Nystr\u00f6m method on Boston Housing consistently and on Pumadyn-8nm when the number of basis points is small the identical experimental setting \u2013 such as the same hyperparameters and\nthe basis points between EigenGP and the Nystr\u00f6m\nmethod \u2013 in our experiments (because EigenGP* did\nnot select eigenfunctions and estimate w via ARD, it\neven used the same weights as the Nystr\u00f6m method).\nThis shows that when the number of basis points is\nsmall, the Nystr\u00f6m method suffers severely as the\nquality of the numerical approximation of the covariance matrix degenerates, while by contrast EigenGP\nand EigenGP*, as valid probabilistic models, degrade\ntheir performance smoothly.\n6.2\n\nSupervised classification\n\nFor supervised classification, we compared EigenGP\nwith the full GP and three sparse GP algorithms:\nthe Nystr\u00f6m method, Sparse Online Gaussian Processes (SOGP) (Csat\u00f3 & Opper, 2002), and FITC approximations (Snelson & Ghahramani, 2006; NaishGuzman & Holden, 2008).\nWe used the Gaussian covariance function for all these algorithms and\ntuned their kernel width by cross-validation. For the\nNystr\u00f6m method, we used Laplace's method to approximate the posterior distribution as described in\n(Williams & Seeger, 2001). A better comparison with\nthe Nystr\u00f6m method would be using EP, instead of\nthe less effective Laplace's approximation. However,\nthere is no previous work that combines EP with the\nNystr\u00f6m method and the development of this algo-\n\nrithm is out of the scope of this paper. For the FITC\nmodel, we used EP for the posterior approximation\nas proposed by Naish-Guzman & Holden (2008). Although we did not maximize the model evidence to\nlearn the kernel parameters for the FITC model (thus\nwe may not obtain their best results), we found in\npractice that our extensive cross-validation of kernel\nparameters often gave prediction accuracies at least\ncomparable to those based on evidence maximization.\nWe tested these algorithms on two benchmark\ndatasets: Spambase and USPS. We randomly split the\nSpambase dataset into 2300 training and 2300 test\nsamples 10 times and ran all the competing methods on each partition. For each partition, we chose\nthe centers from K-means clustering of a randomly selected subset of the whole dataset as basis points for\nall the sparse GP methods. For the USPS dataset, we\nconducted three digit classification tasks: 8 vs 9, 3 vs\n8, and 5 vs 8. Each digit in USPS was treated as a\n256 dimensional vector. We randomly selected 1200\ntraining and 1000 test samples and repeated the partition 10 times. In USPS, we used the same procedure\nto select basis points when the number of basis points\nis smaller than 500; when it is bigger than 500, we\nselected the basis points randomly.\nAs shown in Figure 5.(a), by sparsifying w, EigenGP\nconsistently outperforms EigenGP* (which does not\nsparsify w) on the classification of digits 8 and 9.\nThis improvement verifies the benefit of selecting relevant eigenfunctions for classification. Figures 5.(b)-(d)\ndemonstrate that, with the same or less computational\ncost, EigenGP achieved lower classification error rates\nthan the other sparse GP algorithms. Interestingly,\nEigenGP even outperforms the full GP. Two possible\nreasons are i) that by eigenfunctions selection, we obtain a nonstationary GP model that can better reflect\nthe local smoothness of the latent function and remove\nlabeling noise just like PCA for noise reduction, and\nii) that the clustering property of eigenfunctions helps\nimprove classification accuracy.\n6.3\n\nSemi-supervised classification\n\nWe compared EigenGP with three well-known semisupervised learning algorithms: the graph regularization (GR) approach (Zhou et al., 2004), the Laplacian support vector machine (LAPSVM) (Belkin et al.,\n2006), and the sparse eigenfunction bases approach\n(SEB) (Sinha & Belkin, 2010). We did not include\nEigenGP* here because, without eigenfunction selection, it is not designed for semisupervised learning.\nFor semi-supervised learning, we used both the labeled\nand unlabeled samples as basis points to generate the\neigenfunctions for EigenGP. We also tested supervised\nSVM as a baseline for comparison. For all these al-\n\n\f1.0\n\n0.5\n\n50\n150\n250\nRank or Number of Basis Points\n\n3\n\n11\n\n2.5\n\n2.5\n\n10\n\n2\n1.5\n1\n0.5\n\n(a) Digits 8 vs. 9\n\n300 400\n600\n800\n1000\nRank or Number of Basis Points\n\nError Rate (%)\n\n1.5\n\n3\nError Rate (%)\n\nEigenGP*\nEigenGP\nError Rate (%)\n\nError Rate (%)\n\n2.0\n\n2\n1.5\n1\n0.5\n\n(b) Digits 3 vs. 8\n\nFull\u2212GP\nFITC\nNystrom\nSOGP\nEigenGP\n\n9\n8\n7\n\n6\n300400\n600\n800 1000\nRank or Number of Basis Points\n\n300 400\n600\n800\n1000\nRank or Number of Basis Points\n\n(c) Digits 5 vs. 8\n\n(d) Spambase\n\nFigure 5: Classification results on Spambase and USPS. The results are averaged on 10 random splits of the data\nand the error bars represent the standard errors.\n\n25\n\n20\n\n50\n100\n200\nNumber of Labeled Points\n\n(a) Diabetes\n\n20\n\n15\n\n10\n\n50\n100\n200\nNumber of Labeled Points\n\n(b) Ionosphere\n\n40\nError Rate (%)\n\n30\n\n15\nError Rate (%)\n\n25\nError Rate (%)\n\nError Rate (%)\n\n35\n\n10\n\n5\n\n0\n\n50\n\n100\n200\n300\nNumber of Labeled Points\n\n(c) TDT2\n\nSVM\nSEB\nGR\nLAPSVM\nEigenGP\n\n30\n20\n10\n0\n\n50\n\n100\n200\n300\nNumber of Labeled Points\n\n(d) 20 Newsgroup\n\nFigure 6: Semi-supervised classification results on Diabetes, Ionosphere, TDT2, and 20 Newsgroups. The results\nof SVM on Ionosphere are not reported here since they are much worse than the others.\ngorithms, we used the Gaussian covariance function.\nThe same kernel width was tuned by cross-validation.\nWe used two UCI datasets, Diabetes and Ionosphere,\nand two text datasets, TDT2 and 20 Newsgroups.\nDiabetes and Ionosphere contain 768 and 351 samples, respectively. For the TDT2 dataset, we selected two biggest categories for classification; for\nthe 20 Newsgroup dataset, we chose two categories,\ncomp.sys.ibm.pc.hardware and rec.sport.baseball, in\nour comparison. After the selection, we obtained\n1976 and 3672 documents, respectively, from these two\ndatasets. We then represented each document by the\ntf-idf term weights of the most frequent 1000 words.\nWe varied the number of randomly selected labeled\nsamples and repeated the experiments 10 times. Figure 6 shows the averaged prediction error rates and the\nstandard errors; clearly, EigenGP consistently outperformed the alternative approaches.\n\n7\n\nclassification tasks, it can outperform the full GP that\nuses infinite eigenfunctions. We believe the improvement in both training speed and prediction accuracy\ncomes from the selection of relevant eigenfunctions and\nthe nonstationarity in the covariance function associated with this selection.\nWe have used cross-validation to tune the kernel width\nof the covariance function. A future work is to learn\nthe hyperparameters automatically from data. Although addressing this issue is out of the scope of this\npaper, we expect learning hyparameter for EigenGP\nis feasible by adopting a sampling method, for example, the slice sampling method proposed by Murray &\nAdams (2011).\nFinally, we want to point out that the eigenfunctions\nin EigenGP can be viewed as dictionary elements. By\nsharing the dictionary elements across related tasks,\nEigenGP can be easily extended for multi-task learning.\n\nConclusions\nAcknowledgments\n\nWe have presented a new GP model, EigenGP, which\nselects eigenfunctions learned from data. Experiments\ndemonstrated EigenGP's superior performance for regression, classification and semisupervised classification on several benchmark datasets. What is a little\nsurprising is that, with lower computational cost, for\n\nThis work was supported by NSF IIS-0916443, NSF\nECCS-0941043, NSF CAREER award IIS-1054903,\nand the Center for Science of Information, an NSF Science and Technology Center, under grant agreement\nCCF-0939370.\n\n\fReferences\nBelkin, M., Niyogi, P., and Sindhwani, V. Manifold\nregularization: a geometric framework for learning\nfrom labeled and unlabeled examples. Journal of\nMachine Learning Research, 7:2399\u20132434, 2006.\nBishop, C. M. Pattern Recognition and Machine\nLearning. Springer, 2006.\nChen, X-H. Large sample sieve estimation of seminonparametric models. In Heckman, J. J. and\nLeamer, E. E. (eds.), Handbook of Econometrics,\nvolume 6 of Handbook of Econometrics, chapter 76.\nElsevier, June 2007.\nCsat\u00f3, L. and Opper, M. Sparse online Gaussian\nprocesses. Neural Computation, 14:641\u2013668, March\n2002.\nFaul, A. C. and Tipping, M. E. Analysis of sparse\nBayesian learning. In Advances in Neural Information Processing Systems 14. MIT Press, 2002.\nGeman, S. and Hwang, C-R. Nonparametric maximum\nlikelihood estimation by the method of sieves. The\nAnnals of Statistics, 10(2):401\u2013414, 1982.\n\nQi, Y., Minka, T. P., and Picard, R. W. Predictive automatic relevance determination by expectation propagation. In Proceedings of 21st International Conference on Machine Learning, pp. 671\u2013\n678, 2004.\nQi, Y., Abdel-Gawad, A., and Minka, T. Sparseposterior Gaussian processes for general likelihoods.\nIn Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, 2010.\nQui\u00f1onero-Candela, J. and Rasmussen, C. E. A unifying view of sparse approximate Gaussian process\nregression. Journal of Machine Learning Research,\n6:1935\u20131959, 12 2005.\nSinha, K. and Belkin, M. Semi-supervised learning using sparse eigenfunction bases. In Advances in Neural Information Processing Systems 22. MIT Press,\n2010.\nSnelson, E. and Ghahramani, Z. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18. MIT press,\n2006.\n\nHigdon, D., Swall, J., and Kern, J. Nonstationary\nspatial modeling. Bayesian Statistics, 6, 1998.\n\nTippings, M. E. The relevance vector machine. In\nAdvances in Neural Information Processing Systems\n12. MIT Press, 2000.\n\nL\u00e1zaro-Gredilla, M., Quinonero-Candela, J., Rasmussen, C. E., and Figueiras-Vidal, A.R. Sparse\nspectrum Gaussian process regression. Journal of\nMachine Learning Research, 11:1865\u20131881, 2010.\n\nWilliams, C. K. I. and Seeger, M. Using the Nystr\u00f6m\nmethod to speed up kernel machines. In Advances\nin Neural Information Processing Systems 13, volume 13. MIT Press, 2001.\n\nMacKay, D. J. C. Bayesian interpolation. Neural Computation, 4(3):415\u2013447, 1992.\n\nWilliams, Christopher K. I. and Barber, David.\nBayesian classification with Gaussian processes.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342\u20131351, 1998.\n\nMarzouk, Y. M. and Najm, H. N. Dimensionality reduction and polynomial chaos acceleration of\nBayesian inference in inverse problems. Journal of\nComputational Physics, 228(6):1862 \u2013 1902, 2009.\nISSN 0021-9991.\nMinka, T. P. Expectation propagation for approximate Bayesian inference. In Proceedings of the 17th\nConference in Uncertainty in Artificial Intelligence,\npp. 362\u2013369, 2001.\nMurray, Iain and Adams, Ryan P. Slice sampling covariance hyperparameters of latent Gaussian models. In Advances in Neural Information Processing\nSystems 23, 2011.\nNaish-Guzman, A. and Holden, S. The generalized\nFITC approximation. In Platt, J.C., Koller, D.,\nSinger, Y., and Roweis, S. (eds.), Advances in Neural Information Processing Systems 20. MIT Press,\n2008.\nPaciorek, Christopher J. and Schervish, Mark J. Nonstationary covariance functions for Gaussian process\nregression. In Advances in Neural Information Processing Systems 16. MIT Press, 2004.\n\nZhou, D., Bousquet, O., T. N. Lal, J. Weston, and\nSchlkopf., B. Learning with local and global consistency. In Advances in Neural Information Processing Systems 16. MIT Press, 2004.\n\n\f"}