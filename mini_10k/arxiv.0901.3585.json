{"id": "http://arxiv.org/abs/0901.3585v1", "guidislink": true, "updated": "2009-01-23T05:29:09Z", "updated_parsed": [2009, 1, 23, 5, 29, 9, 4, 23, 0], "published": "2009-01-23T05:29:09Z", "published_parsed": [2009, 1, 23, 5, 29, 9, 4, 23, 0], "title": "Resource Adaptive Agents in Interactive Theorem Proving", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0901.0163%2C0901.2284%2C0901.0244%2C0901.4681%2C0901.1427%2C0901.3984%2C0901.3195%2C0901.1409%2C0901.2857%2C0901.1633%2C0901.3670%2C0901.4332%2C0901.3585%2C0901.0052%2C0901.3344%2C0901.4608%2C0901.1904%2C0901.3980%2C0901.0553%2C0901.3800%2C0901.4351%2C0901.3366%2C0901.2825%2C0901.4049%2C0901.3611%2C0901.4192%2C0901.3910%2C0901.1938%2C0901.0773%2C0901.2322%2C0901.2418%2C0901.4399%2C0901.2121%2C0901.2503%2C0901.3617%2C0901.2716%2C0901.0293%2C0901.0546%2C0901.1206%2C0901.0211%2C0901.1896%2C0901.0770%2C0901.2476%2C0901.2963%2C0901.4067%2C0901.2145%2C0901.2791%2C0901.3689%2C0901.3221%2C0901.0559%2C0901.3572%2C0901.4272%2C0901.0700%2C0901.1122%2C0901.0909%2C0901.0621%2C0901.1848%2C0901.3080%2C0901.4288%2C0901.2450%2C0901.1568%2C0901.3816%2C0901.1361%2C0901.1760%2C0901.3076%2C0901.4809%2C0901.0273%2C0901.1800%2C0901.4173%2C0901.4907%2C0901.0492%2C0901.4447%2C0901.0096%2C0901.4277%2C0901.3851%2C0901.4461%2C0901.1961%2C0901.4466%2C0901.3289%2C0901.1193%2C0901.0832%2C0901.3561%2C0901.1441%2C0901.0214%2C0901.0603%2C0901.4379%2C0901.3635%2C0901.1854%2C0901.0522%2C0901.4856%2C0901.3839%2C0901.3974%2C0901.3648%2C0901.1108%2C0901.3047%2C0901.1716%2C0901.2780%2C0901.0753%2C0901.0996%2C0901.3296%2C0901.4137&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Resource Adaptive Agents in Interactive Theorem Proving"}, "summary": "We introduce a resource adaptive agent mechanism which supports the user in\ninteractive theorem proving. The mechanism uses a two layered architecture of\nagent societies to suggest appropriate commands together with possible command\nargument instantiations. Experiments with this approach show that its\neffectiveness can be further improved by introducing a resource concept. In\nthis paper we provide an abstract view on the overall mechanism, motivate the\nnecessity of an appropriate resource concept and discuss its realization within\nthe agent architecture.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=0901.0163%2C0901.2284%2C0901.0244%2C0901.4681%2C0901.1427%2C0901.3984%2C0901.3195%2C0901.1409%2C0901.2857%2C0901.1633%2C0901.3670%2C0901.4332%2C0901.3585%2C0901.0052%2C0901.3344%2C0901.4608%2C0901.1904%2C0901.3980%2C0901.0553%2C0901.3800%2C0901.4351%2C0901.3366%2C0901.2825%2C0901.4049%2C0901.3611%2C0901.4192%2C0901.3910%2C0901.1938%2C0901.0773%2C0901.2322%2C0901.2418%2C0901.4399%2C0901.2121%2C0901.2503%2C0901.3617%2C0901.2716%2C0901.0293%2C0901.0546%2C0901.1206%2C0901.0211%2C0901.1896%2C0901.0770%2C0901.2476%2C0901.2963%2C0901.4067%2C0901.2145%2C0901.2791%2C0901.3689%2C0901.3221%2C0901.0559%2C0901.3572%2C0901.4272%2C0901.0700%2C0901.1122%2C0901.0909%2C0901.0621%2C0901.1848%2C0901.3080%2C0901.4288%2C0901.2450%2C0901.1568%2C0901.3816%2C0901.1361%2C0901.1760%2C0901.3076%2C0901.4809%2C0901.0273%2C0901.1800%2C0901.4173%2C0901.4907%2C0901.0492%2C0901.4447%2C0901.0096%2C0901.4277%2C0901.3851%2C0901.4461%2C0901.1961%2C0901.4466%2C0901.3289%2C0901.1193%2C0901.0832%2C0901.3561%2C0901.1441%2C0901.0214%2C0901.0603%2C0901.4379%2C0901.3635%2C0901.1854%2C0901.0522%2C0901.4856%2C0901.3839%2C0901.3974%2C0901.3648%2C0901.1108%2C0901.3047%2C0901.1716%2C0901.2780%2C0901.0753%2C0901.0996%2C0901.3296%2C0901.4137&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "We introduce a resource adaptive agent mechanism which supports the user in\ninteractive theorem proving. The mechanism uses a two layered architecture of\nagent societies to suggest appropriate commands together with possible command\nargument instantiations. Experiments with this approach show that its\neffectiveness can be further improved by introducing a resource concept. In\nthis paper we provide an abstract view on the overall mechanism, motivate the\nnecessity of an appropriate resource concept and discuss its realization within\nthe agent architecture."}, "authors": ["Christoph Benzmueller", "Volker Sorge"], "author_detail": {"name": "Volker Sorge"}, "author": "Volker Sorge", "arxiv_comment": "13 pages", "links": [{"href": "http://arxiv.org/abs/0901.3585v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/0901.3585v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LO", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.AI", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "I.2.11; I.2.3; F.4.1; D.4.7; H.3.4", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/0901.3585v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/0901.3585v1", "journal_reference": "SEKI Report (ISSN 1437-4447), Saarland University, 1999", "doi": null, "fulltext": "SEKI-REPORT ISSN 1437-4447\n\nWWW: http://www.ags.uni-sb.de/\n\nUNIVERSIT\u00c4T DES SAARLANDES\nFACHRICHTUNG INFORMATIK\nD\u201366123 SAARBR\u00dcCKEN\nGERMANY\n\narXiv:0901.3585v1 [cs.LO] 23 Jan 2009\n\nAGS\n2004\n\nResource Adaptive Agents in Interactive\nTheorem Proving\nChristoph Benzm\u00fcller and Volker Sorge\nFR Informatik, Saarland Univiversity, Germany\n{chris|sorge}@ags.uni-sb.de\n\nSEKI Report SR\u201399\u201302\n\n\fThis SEKI Report was internally reviewed by:\nClaus-Peter Wirth\nE-mail: cp@ags.uni-sb.de\nWWW: http://www.ags.uni-sb.de/~cp/welcome.html\nEditor of SEKI series:\nClaus-Peter Wirth\nFR Informatik, Universit\u00e4t des Saarlandes, D\u201366123 Saarbr\u00fccken, Germany\nE-mail: cp@ags.uni-sb.de\nWWW: http://www.ags.uni-sb.de/~cp/welcome.html\n\n\f1\nAbstract\nWe introduce a resource adaptive agent mechanism which supports the user in interactive\ntheorem proving. The mechanism, an extension of [4], uses a two layered architecture of\nagent societies to suggest appropriate commands together with possible command argument\ninstantiations. Experiments with this approach show that its effectiveness can be further\nimproved by introducing a resource concept. In this paper we provide an abstract view on the\noverall mechanism, motivate the necessity of an appropriate resource concept and discuss its\nrealization within the agent architecture.\n\n1 Introduction\nInteractive theorem provers have been developed to overcome the shortcomings of purely automatic systems by enabling the user to guide the proof search and by directly importing expert\nknowledge into the system. For large proofs, however, this task becomes difficult if the system\ndoes not provide a sophisticated mechanism to suggest possible next steps in order to minimize\nthe necessary interactions.\nSuggestion mechanisms in interactive theorem proving systems such as H OL [7], T PS [2] or\n\u03a9MEGA [3] are rather limited in their functionality as they usually\n(i)\nuse inflexible sequential computation strategies,\n(ii) do not have anytime character,\n(iii) do not work steadily and autonomously in the background of a\nsystem, and\n(iv) do not exhaustively use available computation resources\nIn order to overcome these limitations, [4] proposed - within the context1 of tactical theorem\nproving based on ND-calculus [5] - a new, flexible support mechanism with anytime character.\nIt suggests commands, applicable in the current proof state - more precisely commands that\ninvoke some ND-rule or tactic - together with a suitable argument instantiations. It is based\non two layers of societies of autonomous, concurrent agents which steadily work in the background of a system and dynamically update their computational behavior to the state of the proof\nand/or specific user queries to the suggestion mechanism. By exchanging relevant results via\nblackboards the agents cooperatively accumulate useful command suggestions which can then be\nheuristically sorted and presented to the user.\nA first Lisp-based implementation of the support mechanism in the \u03a9MEGA-system yielded\npromising results. However as we could not exploit concurrency in the Lisp implementation\nexperience showed that we had to develop a resource adapted concept for the agents in order to\nallow for efficient suggestions even in large examples. In the current reimplementation of major\nparts of \u03a9MEGA and the command suggestion mechanism in Oz, a concurrent constraint logic\nprogramming language [12], the first impression is that the use of concurrency provides a good\nopportunity to switch from a static to a dynamic, resource adaptive control of the mechanism's\ncomputational behavior. Thereby, we can exploit both knowledge on the prior performance of the\nmechanism as well as knowledge on classifying the current proof state and single agents in order\nto distribute resources.\n1 We\n\nwant to point out that this mechanism is in no way restricted to a specific logic or calculus and can easily\nbe adapted to other interactive theorem contexts as well.\n\n\f2\nIn this paper we first sketch our two layered agent mechanism and introduce the static approach of handling resources. We then extend this approach into a resource adaptive2 one where\nthe agents monitor their own contributions and performance in the past in order to estimate their\npotential contribution and performance within the next step and where the adequacy of all agents\ncomputations with respect to the complexity of the current proof goal is analyzed by a special\nclassification agent. Thus, the agents have a means to decide whether or not they should pursue\ntheir own intentions in a given state. Moreover, the overall mechanism can dynamically change\nresource allocations thereby enabling or disabling computations of single agents or complete societies of agents. And finally, the classification agent sends deactivation signals to single agents\nor agent societies as soon as evidence or definite knowledge is available that these agents can\ncurrently not compute any appropriate contributions (e.g. when an agent belongs to a first-order\ncommand whereas the current proof goal can be classified as propositional).\n\n2 Reference Example\nThe following example will be used throughout this paper: (po\u2192o (ao \u2227 bo )) \u21d2 (p (b \u2227 a)).\nWhereas it looks quite simple at a first glance this little higher-order (HO) problem cannot be\nsolved by most automatic HO theorem provers known to the authors, since it requires the application of the extensionality principles which are generally not built-in in HO theorem proving\nsystems. Informally this example states: If the truth value of a \u2227 b is element of the set p of truth\nvalues, then b \u2227 a is also in p. In the mathematical assistant \u03a9MEGA [3], which employs a variant\nof Gentzen's natural deduction calculus (ND) [5] enriched by more powerful proof tactics, the\nfollowing proof can easily be constructed interactively3:\nL1\nL4\nL3\nL2\nC\n\n(L1 ) \u22a2\n(L1 ) \u22a2\n(L1 ) \u22a2\n(L1 ) \u22a2\n() \u22a2\n\n(p (a \u2227 b))\n(b \u2227 a) \u21d4 (a \u2227 b)\n(b \u2227 a) = (a \u2227 b)\n(p (b \u2227 a))\n(p (a \u2227 b)) \u21d2 (p (b \u2227 a))\n\nHyp\nOTTER\n\u21d42= : (L4 )\n=subst : (h1i)(L1 L3 )\n\u21d2I : (L2 )\n\nThe idea of the proof is to show that the truth value of a \u2227 b equals that of b \u2227 a (lines L3 and\nL4 ) and then to employ equality substitution (line L2 ). The equation (b \u2227 a) = (a \u2227 b), i.e. by\nboolean extensionality the equivalence (b \u2227 a) \u21d4 (a \u2227 b) is proven here with the first-order prover\nOTTER [9] and the detailed subproof is hidden at the chosen presentation level. Our agent mechanism is able to suggest all the single proof steps together with the particular parameter instantiations to the user. In this paper we use parts of this example to demonstrate the working scheme\nof the mechanism and to motivate the use of resources.\n\n2 In\n\nthis paper we adopt the notions of resource adapted and resource adaptive as defined in [13], where the\nformer means that agents behave with respect to some initially set resource distribution. According to the latter\nconcept agents have an explicit notion of resources themselves, enabling them to actively participate in the dynamic\nallocation of resources.\n3 Linearized ND proofs are presented as described in [1]. Each proof line consists of a label, a set of hypotheses,\nthe formula and a justification.\n\n\f3\n\nCommands\n=subst (s:L2 ,u:L1 ,pl:(1))\nL EO(conc:L2 ,prems:(L1 ))\n\nCommand\nBlackboard\n\nInterface\n\n\u2732\n\n...\n\n\u274d\n\u2768\n\u274d\u274d\n\nmessage: goal is HO\n\nCommand\nAgents\n\n\u2704\n\n\u2704\u2704\u2717\n\nC=subst C\u2200E\n\u2744\n\n=subst\nSuggestion\nBlackboards\n\n(s:L2 ,u:L1 )\n(s:L2 ,u:L1 ,pl:(1))\n\n...\ngoal is HO\n\nSocieties of\nArgument\nAgents\n\n\u273b\n\n\u2702\u2702\u270d \u273b\u2747\u25bc\u2747 \u2748\u2756\u2748\neq\n\nA0s,u\n/\n\n\u2748\u2756\u2748\n\u2748\n\nClassif. Agent\n\nCL\n\nEO\n\n\u2747\u2747\u25c6\n\n\u2744\n\nL EO\n\n\u2200E\n\n...\n...\n...\ngoal is HO\n\ngoal is HO\n\n\u273b \u2747\u25bc\u2747 \u2751\u2746\n\u2746\n\u2747 \u2746\n\n...\n\nA0/ \u2748\nA0c/ Ac\npl\n{p}\nt\nA\nAeq\nA\n{s,u} {s,u}\n{p,c}\n\nFigure 1: The two layered suggestion mechanism.\n\n3 Suggesting Commands\nThe general suggestion mechanism is based on a two layered agent architecture displayed in Fig. 1\nwhich shows the actual situation after the first proof step in the example, where the backward\napplication of \u21d2I introduces the hypothesis line L1 and L2 as the new open goal. The task of the\nbottom layer of agents (cf. the lower part of Fig. 1) is to compute possible argument instantiations\nfor the provers commands in dependence of the dynamically changing partial proof tree. The task\nof the top layer (cf. the upper part of Fig. 1) is to collect the most appropriate suggestions from\nthe bottom layer, to heuristically sort them and to present them to the user.\nThe bottom layer consists of societies of argument agents where each society belongs to\nexactly one command associated with a proof tactic4 (cf. below). On the one hand each argument\nagent has its own intention, namely to search in the partial proof for a proof line that suits a certain\nspecification. On the other hand argument agents belonging to the same society also pursue a\ncommon goal, e.g. to cooperatively compute most complete argument suggestions (cf. concept\nof PAI's below) for their associated command. Therefore the single agents of a society exchange\ntheir particular results via a suggestion blackboard and try to complete each others suggestions.\nThe top layer consists of a single society of command agents which steadily monitor the\nparticular suggestion blackboards on the bottom layer. For each suggestion blackboard there\nexists one command agent whose intention is to determine the most complete suggestions and to\nput them on the command blackboard.\nThe whole distributed agent mechanism runs always in the background of the interactive theorem proving environment thereby constantly producing command suggestions that are dynamically adjusted to the current proof state. At any time the suggestions on the command blackboard\nare monitored by an interface component which presents them heuristically sorted to the user via\na graphical user interface. As soon as the user executes a command the partial proof is updated\nand simultaneously the suggestion and command blackboards are reinitialized.\n4 For\n\nour approach it is not necessary to distinguish between proof rules, proof tactics and proof methods and\ntherefore we just use the phrase \"tactic\" within this article.\n\n\f4\n\n3.1 Partial Argument Instantiations (PAI)\nThe data that is exchanged within the blackboard architecture heavily depends on a concept called\na partial argument instantiation (PAI) of a command. In order to clarify our mechanism we need\nto introduce this concept in detail.\nIn an interactive theorem prover such as \u03a9MEGA or H OL one has generally one command\nassociated with each proof tactic that invokes the application of this tactic to a set of proof lines.\nIn \u03a9MEGA these tactics have a fixed outline, i.e. a set of premise lines, conclusion lines and\nadditional parameters, such as terms or term-positions. Thus the general instance of a tactic T\ncan be formalized in the following way:\nP1 * * * Pl\nT (Q1 * * * Qn )\nC1 * * *Cm\n,\nwhere we call the Pi ,C j , Qk the formal arguments of the tactic T (we give an example below).\nWe can now denote the command t invoking tactic T formally in a similar fashion as\npi1 * * * pil \u2032\nc j1 * * * c jm\u2032 t(qk1 * * * qkn\u2032 ),\nwhere the formal arguments pi , c j , qk of t correspond to a subset of the formal arguments of the\ntactic. To successfully execute the command some, not necessarily all, formal arguments have\nto be instantiated with actual arguments, e.g. proof lines. A set of pairs relating each formal\nargument of the command to an (possibly empty) actual argument is called a partial argument\ninstantiation (PAI).\nWe illustrate the idea of a PAI using the tactic for equality substitution =Subst and its corresponding command =Subst as an example.\n\u03a6[x] x = y\n=Subst (P\u2217 )\n\u03a6\u2032 [y]\n\n\u2212\u2192\n\nu eq\ns =Subst(pl)\n\nHere \u03a6[s] is an arbitrary higher order formula with at least one occurrence of the term s, P\u2217 is a\nlist of term-positions representing one or several occurrences of s in \u03a6, and \u03a6\u2032 [t] represents the\nterm resulting from replacing s by t at all positions P\u2217 in \u03a6. u, eq, s and pl are the corresponding\nformal arguments of the command associated with the respective formal arguments of the tactic.\nWe observe the application of this tactic to line L2 of our example:\nL1 (L1 ) \u22a2\n...\nL2 (L1 ) \u22a2\n\n(p (a \u2227 b)) Hyp\n(p (b \u2227 a)) Open\n\nAs one possible PAI for =Subst we get the set of pairs (u:L1 , eq:\u03b5 , s:L2 , pl:\u03b5 ), where \u03b5 denotes\nthe empty or unspecified actual argument. We omit writing pairs containing \u03b5 and, for instance,\nwrite the second possible PAI of the above example as (u:L1 , s:L2 , pl:(h1i)). To execute =Subst\nwith the former PAI the user would have to at least provide the position list, whereas using the\nlatter PAI results in the line L3 of the example containing the equality.\n\n\f5\n\n3.2 Argument Agents\nThe idea underlying our mechanism to suggest commands is to compute PAIs as complete as\npossible for each command, thereby gaining knowledge on which tactics can be applied combined\nwith which argument instantiations in a given proof state.\nThe main work is done by the societies of cooperating Argument Agents at the bottom layer\n(cf. Fig. 1). Their job is to retrieve information from the current proof state either by searching\nfor proof lines appropriate to the agents specification or by computing some additional parameter\n(e.g. a list of sub-term positions) with already given information. Sticking to our example we can\npl\neq\neq\ninformally specify the agents A0u,s\n/ , A0/ , A{u,s} , and A{u,s} for the =Subst command (cf. [4] for a\nformal specification):\n)\n(\nfind open line and a support line that differ\n\nAs,u\n0/ = only wrt. occurences of a single proper subterm\n\neq\n\nA0/ ={find line with an equality}\neq\nA{u,s}={find equality line suitable for s and u}\npl\n\nA{u,s}={compute positions where s and u differ}\nThe attached superscripts specify the formal arguments of the command for which actual arguments are computed, whereas the indices denote sets of formal arguments that necessarily have\nto be instantiated in some PAI, so that the agent can carry out its own computations. For example\neq\nagent A{u,s} only starts working when it detects a PAI on the blackboard where actual arguments\neq\nfor u and s have been instantiated. On the contrary A0/ does not need any additional knowledge\nin order to pursue its task to retrieve an open line containing an equality as formula.\nThe agents themselves are realized as autonomous processes that concurrently compute their\nsuggestions whereas they are triggered by the PAIs on the blackboard, i.e. the results of other\neq\npl\nagents of their society. For instance both agents A{u,s} and A{u,s} would simultaneously start\ntheir search as soon as As,u\n0/ has returned a result. The agents of one society cooperate in the sense\nthat they activate each other (by writing new PAIs to the blackboard) and furthermore complete\neach others suggestions.\nConflicts between agents do not arise, as agents that add actual parameters to some PAI always write a new copy of the particular PAI on the blackboard, thereby keeping the original less\ncomplete PAI intact. The agents themselves watch their suggestion blackboard (both PAI entries\nand additional messages, cf. 4) and running agents terminate as soon as the associated suggestion\nblackboard is reinitialized, e.g. when a command has been executed by the user.\nThe left hand side of Fig. 1 illustrates our above example: The topmost suggestion blackboard\ncontains the two PAIs: (u:L1 , s:L2 ) computed by agent As,u\n0/ and (u:L1 , s:L2 , pl:(h1i)) completed\neq\nby agent A{u,s} .\n\n3.3 Command Agents\nIn the society of command agents every agent is linked to a command and its task is to initialize\nand monitor the associated suggestion blackboards. Its intention is to select among the entries\n\n\f6\nof the associated blackboard the most complete and appropriate one and to pass it, enriched by\nthe corresponding command name to the command blackboard. That is, as soon as a PAI is\nwritten to the related blackboard that has at least one actual argument instantiated, the command\nagent suggests the command as applicable in the current proof state, providing also the PAI as\npossible argument instantiations. It then updates this suggestion, whenever a better PAI has been\ncomputed. In this context better generally means a PAI containing more actual arguments. In the\ncase of our example the current PAI suggested by command agent C=Subst is (u:L1 , s:L2 , pl:(h1i)).\nThese suggestions are accumulated on a command blackboard, that simply stores all suggested commands together with the proposed PAI, continuously handles updates of the latter,\nsorts and resorts the single suggestions and provides a means to propose them to the user. In\nthe case of the \u03a9MEGA-system this is achieved in a special command suggestion window within\nthe graphical user interface L \u03a9U I [10]. The sorting of the suggestions is done according to\nseveral heuristic criteria, one of which is that commands with fully instantiated PAIs are always\npreferred as their application may conclude the whole subproof.\n\n3.4 Experiences\nThe mechanism sketched here has some obvious advantages. Firstly, it has anytime character\nand does not waste system resources, as it constantly works in a background process and steadily\nimproves its suggestions wrt. its heuristic sorting criteria. Secondly, in contrast to conventional\ncommand and argument suggestion mechanisms, our approach provides more flexibility in the\nsense that agents can be defined independently from the actual command and the user can choose\nfreely among several given suggestions. Thirdly, we can employ concurrent programming techniques to parallelize the process of computing suggestions.\nUnfortunately, computations of single agents themselves can be very costly. Reconsider the\nagents of command =Subst: In \u03a9MEGA we have currently 25 different argument agents defined\nfor =Subst where some are computationally highly expensive. For example, while the agent\neq\nA0/ only tests head symbols of formulas during its search for lines containing an equality and is\ntherefore relatively inexpensive, the agent A0u,s\n/ performs computationally expensive matching operations. In large proofs agents of the latter type might not only take a long time before returning\nany useful result, but also will absorb a fair amount of system resources, thereby slowing down\nthe computations of other argument agents.\n[4] already tackles this problem partially by introducing a focusing technique that explicitly\npartitions a partial proof into subproblems in order to guide the search of the agents. This focusing\ntechnique takes two important aspects into account: (i) A partial proof often contains several\nopen subgoals and humans usually focus on one such subgoal before switching to the next. (ii)\nHypotheses and derived lines belonging to an open subgoal are chronologically sorted where the\ninterest focuses on the more recently introduced lines. Hence, the agents restrict their search to\nthe actual subgoal (actual focus) and guide their search according to the chronological order of\nthe proof lines.\n\n\f7\n\n4 Resource Adapted Approach\nApart from this we use a concept of static complexity ratings where a rating is attached to each\nargument and each command agent, that roughly reflects the computational complexity involved\nfor its suggestions. A global complexity value can then be adjusted by the user permitting to\nsuppress computations of agents, whose ratings are larger than the specified value. Furthermore\ncommands can be completely excluded from the suggestion process. For example, the agent A0u,s\n/\neq\nhas a higher complexity rating than A0/ from the =Subst example, since recursively matching\nterms is generally a harder task than retrieving a line containing an equality. The overall rating\nof a command agent is set to the average rating of its single argument agents. This rating system\nincreased the effectiveness of the command suggestions for the simulation of the architecture\nin LISP. (In the simulation a user was always forced to wait for all active agents to finish their\ncomputations.) However, the system is very inflexible as ratings are assigned by the programmer\nof a particular agent and cannot be adjusted by the user at runtime. Furthermore, the ratings do\nnot necessarily reflect the actual relative complexity of the agents.\nShifting to the Oz implementation the latter problem no longer arises since a user can interrupt the suggestion process by choosing a command at any time without waiting for all possible\nsuggestions to be made. In the Oz implementation every agent is an independent thread. It either\nquits its computations regularly or as soon as it detects that the blackboard it works for has been\nreinitialized, i.e. when the user has executed a command. It then performs all further computations with respect to the reinitialized blackboard.\nHowever, with increasing size of proofs some agents never have the chance to write meaningful suggestions to a blackboard. Therefore, these agents should be excluded from the suggestion\nprocess altogether, especially if their computations are very costly which deprives other agents of\nresources. But while in the sequential simulation in LISP it was still possible for a user to monitor\nwhich agent is computationally very expensive and to subsequently adjust the global complexity\nvalue accordingly, this is no longer possible in the concurrent setting as the user in general is not\naware of which agents produced useful results. Furthermore, the user should be as far as possible\nspared from fine-tuning a mechanism designed for his/her own support.\n\n5 Resource Adaptive Approach\nIn this section we extend the resource adapted approach into a resource adaptive one. While we\nretain the principle of activation/deactivation by comparing the particular complexity ratings of\nthe argument agents with the overall deactivation threshold, we now allow the individual complexity ratings of argument agents to be dynamically adjusted by the system itself. Furthermore,\nwe introduce a special classification agent which analyses and classifies the current proof goal in\norder to deactivate those agents which are not appropriate wrt. the current goal.\n\n5.1 Dynamic Adjustment of Ratings\nThe dynamic adjustment takes place on both layers: On the bottom layer we allow the argument\nagents to adjust their own ratings by reflecting their performance and contributions in the past. On\nthe other hand the command agents on the top layer adjust the ratings of their associated argument\n\n\f8\nagents. This is motivated by the fact that on this layer it is possible to compare the performance\nand contribution of agent societies of the bottom layer.\nTherefore, agents need an explicit concept of resources enabling them to communicate and\nreason about their performance. The communication is achieved by propagating resource informations from the bottom to the top layer and vice versa via the blackboards. The actual information is gathered by the agents on the bottom layer of the architecture. Currently the argument\nagents evaluate their effectiveness with respect to the following two measures:\n1. the absolute cpu time the agents consume, and\n2. 'the patience of the user', before executing the next command.\n(1) is an objective measure that is computed by each agent at runtime. Agents then use these values to compute the average cpu time for the last n runs and convert the result into a corresponding\ncomplexity rating.\nMeasure (2) is rather subjective which expresses formally the ability of an agent to judge\nwhether it ever makes contributions for the command suggesting process in the current proof state.\nWhenever an agent returns from a computation without any new contribution to the suggestion\nblackboard, or even worse, whenever an agent does not return before the user executes another\ncommand (which reinitializes the blackboards), the agent receives a penalty that increases its\ncomplexity rating. Consequently, when an agent fails to contribute several times in a row, its\ncomplexity rating quickly exceeds the deactivation threshold and the agent retires.\nWhenever an argument agent updates its complexity rating this adjustment is reported to the\ncorresponding command agent via a blackboard entry. The command agent collects all these\nentries, computes the average complexity rating of his argument agents, and reports the complete resource information on his society of argument agents to the command blackboard. The\ncommand blackboard therefore steadily provides information on the effectiveness of all active\nargument agents, as well as information on the retired agents and an estimation of the overall\neffectiveness every argument agent society.\nAn additional resource agent uses this resource information in order to reason about a possibly\noptimal resource adjustment for the overall system, taking the following criteria into account:\n\u2022 Assessment of absolute cpu times.\n\u2022 A minimum number of argument agents should always be active. If the number of active\nagents drops below this value the global complexity value is readjusted in order to reactivate\nsome of the retired agents.\n\u2022 Agent societies with a very high average complexity rating and many retired argument\nagents should get a new chance to improve their effectiveness. Therefore the complexity\nratings of the retired agents is lowered beneath the deactivation threshold.\n\u2022 In special proof states some command agent (together with their argument agents) are excluded. For example, if a focused subproblem is a propositional logic problem, commands\ninvoking tactics dealing with quantifiers are needless.\nResults from the resource agent are propagated down in the agent society and gain precedence\nover the local resource adjustments of the single agents.\n\n\f9\n\n5.2 Informed Activation & Deactivation\nMost tactics in an interactive theorem prover are implicitly associated with a specific logic (e.g. propositional, first-order, or higher -order logic) or even with a specific mathematical theory (e.g. natural numbers, set theory). This obviously also holds for the proof problems examined in a mathematical context. Some systems \u2013 for instance the \u03a9MEGA-System \u2013 do even explicitly maintain\nrespective knowledge by administering all rules, tactics, etc. as well as all proof problems within\na hierarchically structured theory database. This kind of classification knowledge can fruitfully\nbe employed by our agent mechanism to activate appropriate agents and especially to deactivate\nnon-appropriate ones. Even if a given proof problem can not be associated with a very restrictive\nclass (e.g. propositional logic) from the start, some of the subproblems subsequently generated\nduring the proof probably can. This can be nicely illustrated with our example: The original\nproof problem belonging to higher-order logic gets transformed by the backward application of\n\u21d2I , =Subst and \u22612= into a very simple propositional logic problem (cf. line L4 ). In this situation agents associated with a command from first- or higher-order logic (like =Subst, \u2200E, or\nL EO5 ) should be disabled.\nTherefore we add a classification agent to our suggestion mechanism whose only task is to\ninvestigate each new subgoal in order to classify it wrt. to the known theories or logics. As soon as\nthis agent is able to associate the current goal with a known class or theory it places an appropriate\nentry on the command blackboard (cf. \"HO\" entry in Fig. 1). This entry is then broadcasted to\nthe lower layer suggestion blackboards by the command agents where it becomes available to all\nargument agents. Each argument agent can now compare its own classification knowledge with\nthe particular entry on the suggestion blackboard and decide whether it should perform further\ncomputations within the current system state or not.\nThe motivation for designing the subgoal classifying component as an agent itself is clear:\nIt can be very costly to examine whether a given subgoal belongs to a specific theory or logic.\nTherefore this task should be performed concurrently by the suggestion mechanism and not within\neach initialization phase of the blackboard mechanism. Whereas our current architecture provides\none single classification agent only, the single algorithms and tests employed by this component\ncan generally be further distributed by using a whole society of classification agents.\n\n6 Conclusion\nIn this paper we reported on the extension of the concurrent command suggestion mechanism [4]\nto a resource adaptive approach. The resources that influence the performance of our system are:\n(i) The available computation time and memory space. (ii) Classification knowledge on the single\nagents and the agent societies. (iii) Criteria and algorithms available to the classification agent.\nOur approach can be considered as an instance of a boundedly rational system [13, 11]. The\nwork is also related to [6] which presents an abstract resource concept for multi-layered agent\narchitectures. [8] describes a successful application of this framework within the Robocup simulation. Consequently some future work should include a closer comparison of our mechanism\nwith this work.\nThe presented extensions are currently implemented and analyzed in \u03a9MEGA. This might\n5A\n\nhigher-order theorem prover integrated to \u03a9MEGA.\n\n\f10\nyield further possible refinements of the resource concepts to improve the performance of the\nmechanism. Another question in this context is, whether learning techniques can support our\nresource adjustments on the top layer, as it seems to be reasonable that there even exist appropriate\nresource patterns for the argument agents in dependence of the focused subproblem.\nAcknowledgements\n\nWe thank Serge Autexier and Christoph Jung for stimulating discussions.\n\nReferences\n[1] P. B. Andrews. An Introduction To Mathematical Logic and Type Theory: To Truth Through\nProof. Academic Press, San Diego, CA, USA, 1986.\n[2] Peter B. Andrews, Matthew Bishop, Sunil Issar, Dan Nesmith, Frank Pfenning, and Hongwei Xi. T PS: A Theorem Proving System for Classical Type Theory. Journal of Automated\nReasoning, 16(3):321\u2013353, 1996.\n[3] C. Benzm\u00fcller, L. Cheikhrouhou, D. Fehrer, A. Fiedler, X. Huang, M. Kerber, M. Kohlhase,\nK. Konrad, E. Melis, A. Meier, W. Schaarschmidt, J. Siekmann, and V. Sorge. \u03a9Mega: Towards a Mathematical Assistant. In W. McCune, editor, Proceedings of CADE\u201314, volume\n1249 of LNAI, pages 252\u2013255. Springer Verlag, 1997.\n[4] Christoph Benzm\u00fcller and Volker Sorge. A Blackboard Architecture for Guiding Interactive\nProofs. In F. Giunchiglia, editor, Proceedings of AIMSA'98, number 1480 in LNAI, pages\n102\u2013114. Springer Verlag, 1998.\n[5] G. Gentzen. Untersuchungen \u00fcber das Logische Schliessen I und II.\nZeitschrift, 39:176\u2013210, 405\u2013431, 1935.\n\nMathematische\n\n[6] C. Gerber and C. G. Jung. Resource management for boundedly optimal agent societies. In\nProceedings of the ECAI\u00c2t'98 Workshop on Monitoring and Control of Real-Time Intelligent\nSystems, pages 23\u201328, 1998.\n[7] M. J. C. Gordon and T. F. Melham. Introduction to HOL \u2013 A theorem proving environment\nfor higher order logic. Cambridge University Press, 1993.\n[8] C. G. Jung. Experimenting with layered, resource-adapting agents in the robocup simulation. In Proc. of the ROBOCUP'98 Workshop, 1998.\n[9] William McCune and Larry Wos. Otter CADE-13 competition incarnations. Journal of\nAutomated Reasoning, 18(2):211\u2013220, 1997. Special Issue on the CADE-13 Automated\nTheorem Proving System Competition.\n[10] J. Siekmann, S. M. Hess, C. Benzm\u00fcller, L. Cheikhrouhou, D. Fehrer, A. Fiedler,\nM. Kohlhase, K. Konrad, E. Melis, A. Meier, and V. Sorge. L\u03a9UI: A Distributed Graphical User Interface for the Interactive Proof System \u03a9MEGA. In Proceedings of UITP-98,\nEindhoven, The Netherlands, July 13\u201315 1998.\n[11] H. A. Simon. Models of Bounded Rationality. MIT Press, Cambridge, 1982.\n\n\f11\n[12] Gert Smolka. The Oz Programming Model. In J. v. Leeuwen, editor, Computer Science\nToday, volume 1000 of LNCS, pages 324\u2013343. Springer-Verlag, 1995.\n[13] S. Zilberstein. Models of Bounded Rationality. In AAAI Fall Symposium on Rational\nAgency, Cambridge, Massachusetts, November 1995.\n\n\f"}