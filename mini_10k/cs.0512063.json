{"id": "http://arxiv.org/abs/cs/0512063v1", "guidislink": true, "updated": "2005-12-15T14:51:36Z", "updated_parsed": [2005, 12, 15, 14, 51, 36, 3, 349, 0], "published": "2005-12-15T14:51:36Z", "published_parsed": [2005, 12, 15, 14, 51, 36, 3, 349, 0], "title": "Complex Random Vectors and ICA Models: Identifiability, Uniqueness and\n  Separability", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0604099%2Ccs%2F0604021%2Ccs%2F0604015%2Ccs%2F0604051%2Ccs%2F0604006%2Ccs%2F0604088%2Ccs%2F0604107%2Ccs%2F0604095%2Ccs%2F0604048%2Ccs%2F0604078%2Ccs%2F0604045%2Ccs%2F0604075%2Ccs%2F0604042%2Ccs%2F0604070%2Ccs%2F0604013%2Ccs%2F0604049%2Ccs%2F0604056%2Ccs%2F0604054%2Ccs%2F0604097%2Ccs%2F0604044%2Ccs%2F0604032%2Ccs%2F0604039%2Ccs%2F0604106%2Ccs%2F0604085%2Ccs%2F0604079%2Ccs%2F0604103%2Ccs%2F0604029%2Ccs%2F0604065%2Ccs%2F0604112%2Ccs%2F0604017%2Ccs%2F0604038%2Ccs%2F0604098%2Ccs%2F0604100%2Ccs%2F0604025%2Ccs%2F0604057%2Ccs%2F0604014%2Ccs%2F0604058%2Ccs%2F0604062%2Ccs%2F0604028%2Ccs%2F0604102%2Ccs%2F0604091%2Ccs%2F0604010%2Ccs%2F0604093%2Ccs%2F0604026%2Ccs%2F0604002%2Ccs%2F0604031%2Ccs%2F0604023%2Ccs%2F0604001%2Ccs%2F0604104%2Ccs%2F0604040%2Ccs%2F0604109%2Ccs%2F0604016%2Ccs%2F0604033%2Ccs%2F0604011%2Ccs%2F0604086%2Ccs%2F0604052%2Ccs%2F0604067%2Ccs%2F0512102%2Ccs%2F0512104%2Ccs%2F0512001%2Ccs%2F0512073%2Ccs%2F0512089%2Ccs%2F0512034%2Ccs%2F0512026%2Ccs%2F0512043%2Ccs%2F0512085%2Ccs%2F0512097%2Ccs%2F0512010%2Ccs%2F0512091%2Ccs%2F0512060%2Ccs%2F0512088%2Ccs%2F0512079%2Ccs%2F0512044%2Ccs%2F0512051%2Ccs%2F0512074%2Ccs%2F0512069%2Ccs%2F0512032%2Ccs%2F0512025%2Ccs%2F0512048%2Ccs%2F0512055%2Ccs%2F0512004%2Ccs%2F0512030%2Ccs%2F0512045%2Ccs%2F0512070%2Ccs%2F0512035%2Ccs%2F0512066%2Ccs%2F0512023%2Ccs%2F0512056%2Ccs%2F0512059%2Ccs%2F0512063%2Ccs%2F0512065%2Ccs%2F0512037%2Ccs%2F0512009%2Ccs%2F0512105%2Ccs%2F0512016%2Ccs%2F0512049%2Ccs%2F0512033%2Ccs%2F0512027%2Ccs%2F0512021%2Ccs%2F0512082%2Ccs%2F0512024&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "Complex Random Vectors and ICA Models: Identifiability, Uniqueness and\n  Separability"}, "summary": "In this paper the conditions for identifiability, separability and uniqueness\nof linear complex valued independent component analysis (ICA) models are\nestablished. These results extend the well-known conditions for solving\nreal-valued ICA problems to complex-valued models. Relevant properties of\ncomplex random vectors are described in order to extend the Darmois-Skitovich\ntheorem for complex-valued models. This theorem is used to construct a proof of\na theorem for each of the above ICA model concepts. Both circular and\nnoncircular complex random vectors are covered. Examples clarifying the above\nconcepts are presented.", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=&id_list=cs%2F0604099%2Ccs%2F0604021%2Ccs%2F0604015%2Ccs%2F0604051%2Ccs%2F0604006%2Ccs%2F0604088%2Ccs%2F0604107%2Ccs%2F0604095%2Ccs%2F0604048%2Ccs%2F0604078%2Ccs%2F0604045%2Ccs%2F0604075%2Ccs%2F0604042%2Ccs%2F0604070%2Ccs%2F0604013%2Ccs%2F0604049%2Ccs%2F0604056%2Ccs%2F0604054%2Ccs%2F0604097%2Ccs%2F0604044%2Ccs%2F0604032%2Ccs%2F0604039%2Ccs%2F0604106%2Ccs%2F0604085%2Ccs%2F0604079%2Ccs%2F0604103%2Ccs%2F0604029%2Ccs%2F0604065%2Ccs%2F0604112%2Ccs%2F0604017%2Ccs%2F0604038%2Ccs%2F0604098%2Ccs%2F0604100%2Ccs%2F0604025%2Ccs%2F0604057%2Ccs%2F0604014%2Ccs%2F0604058%2Ccs%2F0604062%2Ccs%2F0604028%2Ccs%2F0604102%2Ccs%2F0604091%2Ccs%2F0604010%2Ccs%2F0604093%2Ccs%2F0604026%2Ccs%2F0604002%2Ccs%2F0604031%2Ccs%2F0604023%2Ccs%2F0604001%2Ccs%2F0604104%2Ccs%2F0604040%2Ccs%2F0604109%2Ccs%2F0604016%2Ccs%2F0604033%2Ccs%2F0604011%2Ccs%2F0604086%2Ccs%2F0604052%2Ccs%2F0604067%2Ccs%2F0512102%2Ccs%2F0512104%2Ccs%2F0512001%2Ccs%2F0512073%2Ccs%2F0512089%2Ccs%2F0512034%2Ccs%2F0512026%2Ccs%2F0512043%2Ccs%2F0512085%2Ccs%2F0512097%2Ccs%2F0512010%2Ccs%2F0512091%2Ccs%2F0512060%2Ccs%2F0512088%2Ccs%2F0512079%2Ccs%2F0512044%2Ccs%2F0512051%2Ccs%2F0512074%2Ccs%2F0512069%2Ccs%2F0512032%2Ccs%2F0512025%2Ccs%2F0512048%2Ccs%2F0512055%2Ccs%2F0512004%2Ccs%2F0512030%2Ccs%2F0512045%2Ccs%2F0512070%2Ccs%2F0512035%2Ccs%2F0512066%2Ccs%2F0512023%2Ccs%2F0512056%2Ccs%2F0512059%2Ccs%2F0512063%2Ccs%2F0512065%2Ccs%2F0512037%2Ccs%2F0512009%2Ccs%2F0512105%2Ccs%2F0512016%2Ccs%2F0512049%2Ccs%2F0512033%2Ccs%2F0512027%2Ccs%2F0512021%2Ccs%2F0512082%2Ccs%2F0512024&start=0&max_results=1000&sortBy=relevance&sortOrder=descending", "value": "In this paper the conditions for identifiability, separability and uniqueness\nof linear complex valued independent component analysis (ICA) models are\nestablished. These results extend the well-known conditions for solving\nreal-valued ICA problems to complex-valued models. Relevant properties of\ncomplex random vectors are described in order to extend the Darmois-Skitovich\ntheorem for complex-valued models. This theorem is used to construct a proof of\na theorem for each of the above ICA model concepts. Both circular and\nnoncircular complex random vectors are covered. Examples clarifying the above\nconcepts are presented."}, "authors": ["Jan Eriksson", "Visa Koivunen"], "author_detail": {"name": "Visa Koivunen"}, "author": "Visa Koivunen", "links": [{"title": "doi", "href": "http://dx.doi.org/10.1109/TIT.2005.864440", "rel": "related", "type": "text/html"}, {"href": "http://arxiv.org/abs/cs/0512063v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/cs/0512063v1", "rel": "related", "type": "application/pdf"}], "arxiv_comment": "To appear in IEEE TR-IT March 2006", "arxiv_primary_category": {"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.CE", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.IR", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}, {"term": "math.IT", "scheme": "http://arxiv.org/schemas/atom", "label": null}], "pdf_url": "http://arxiv.org/pdf/cs/0512063v1", "affiliation": "None", "arxiv_url": "http://arxiv.org/abs/cs/0512063v1", "journal_reference": "Information Theory, IEEE Transactions on , vol.52, no.3pp. 1017-\n  1029, March 2006", "doi": "10.1109/TIT.2005.864440", "fulltext": "IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\n1\n\nComplex Random Vectors and ICA Models:\nIdentifiability, Uniqueness and Separability\n\narXiv:cs/0512063v1 [cs.IT] 15 Dec 2005\n\nJan Eriksson Member, IEEE and Visa Koivunen Senior Member, IEEE\n\nAbstract- In this paper the conditions for identifiability, separability and uniqueness of linear complex valued independent\ncomponent analysis (ICA) models are established. These results extend the well-known conditions for solving real-valued\nICA problems to complex-valued models. Relevant properties\nof complex random vectors are described in order to extend\nthe Darmois-Skitovich theorem for complex-valued models. This\ntheorem is used to construct a proof of a theorem for each of\nthe above ICA model concepts. Both circular and noncircular\ncomplex random vectors are covered. Examples clarifying the\nabove concepts are presented.\nIndex Terms- Blind methods, circularity, complex linear models, complex Darmois-Skitovich theorem, differential entropy,\nindependent component analysis (ICA), noncircular complex\nrandom vectors, properness.\n\nI. I NTRODUCTION\nIndependent component analysis (ICA) [1] is a relatively\nnew signal processing and data analysis technique. It may be\nused, for example, in blind source separation (BSS) and identifying or equalizing instantaneous multiple-input multipleoutput (I-MIMO) models. It has found applications, e.g., in\nwireless communications, biomedical signal processing and\ndata mining (see [2] for references). In instantaneous complexvalued ICA problem\n~x = A~s,\n(1)\nthe goal is to recover the original source signal vectors ~s from\nthe observation vectors ~x blindly without explicit knowledge\nof the sources or the linear mixing system A. ICA is based\non the crucial assumption that the underlying unknown source\nsignals are statistically independent. Recent textbooks provide\nan interesting tutorial material and a partial review on ICA\n[2], [3].\nThe theorems for linear combinations of real-valued random\nvectors and theoretical conditions on separation for realvalued signals are now well-known [1], [4], [5]. Even though\nalgorithms for separation of complex-valued signals have been\ndeveloped, for example [1], [6], the conditions when the\nseparation is possible have not been established. Also recent\npapers, e.g., [7]\u2013[10], proposing ICA algorithms for complexvalued data ignore this important issue.\nIn this paper we construct theorems stating the conditions\nfor identifiability, separability, and uniqueness of complexvalued linear ICA models. These results extend the theorems\nManuscript received March xx, 2004; revised December xx, 2005. This\nwork was supported in part by the Academy of Finland and GETA Graduate\nSchool.\nThe authors are with the SMARAD CoE, Signal Processing Laboratory,\nDepartment of Electrical Engineering, Helsinki University of Technology,\nFIN-02015 HUT, Finland (e-mail: {jan.eriksson,visa.koivunen}@hut.fi).\n\nproved for the real-valued instantaneous ICA model [1], [5]\nto the complex case. Both circular (proper) and noncircular\ncomplex random vectors are covered by the theorems. These\nconditions depend not only on the probabilistic structure of the\nsources but also the linear space structure of the mixing. In\norder to prove the theorems, the celebrated Darmois-Skitovich\ntheorem [4] needs to be extended to linear combinations\nof complex random variables. A good number of statistical\nproperties of circular and noncircular complex vectors have\nto be considered in the process of constructing the proof.\nThis is due to the special operator structure that may be\nused for complex random vectors. In addition, the second\norder statistical properties of noncircular complex vectors may\nnot be defined using the covariance matrix alone [11]\u2013[13].\nGeneral complex Gaussian random vectors is an important\nclass of random vectors that need to be addressed in detail.\nThere are relatively few papers where noncircular complex\nrandom vectors are studied [11]\u2013[16]. Hence, many of the\nkey results needed in proving the theorems are included in\nthis paper and presented in a unified manner. This also allows\na direct derivation of some fundamental information-theoretic\nquantities like the entropy of a complex normal random vector.\nThe paper is organized as follows. In Section II relevant\nproperties that distinguish complex random vectors from real\nrandom vectors are described in detail. Especially, the correlation structure is used to study complex normal random vectors.\nThese properties are needed in proving the Darmois-Skitovich\ntheorem for the complex case. This theorem plays a key role\nin establishing the conditions for identifiablity, separability\nand uniqueness of complex linear ICA models in Section III.\nFinally, some concluding remarks are given. Most of the proofs\nare presented in appendices.\nII. R ELEVANT P ROPERTIES\n\nOF\nVECTORS\n\nC OMPLEX\n\nRANDOM\n\nThe traditional probability theory is concerned with realvalued random variables (r.v.s) and random vectors (r.vc.s).\nThe theory has been generalized to various algebraic structures. Main studies are in the frameworks of locally compact\nspaces and complete separable metric spaces (see, e.g., [17]\u2013\n[20] and references therein). However, the most natural extension from the engineering point of view is the complex Hilbert\nspace. It seems to have gained relatively little attention. Some\nresults on complex normal r.vc.s can be found in [21], [22].\nThe second-order structure of complex r.vc.s has been studied\nin [11]\u2013[13], [15], and a general framework for higher-order\nstatistics can be found from [23]. Some research has been\nconducted on complex elliptically symmetric distributions [24]\n\n\f2\n\nand on complex stable distributions [25]. Polya's theorem to\ncomplex case is presented in [26]. The only systematic Hilbert\nspace approach known to the authors is [14]. This may be\ndue to the fact that the additive structure of the complex\nHilbert space is the same as that of the real Hilbert space.\nHowever, the multiplicative structure and the operator structure\nare different giving r.vc.s in a complex Hilbert space distinct\nproperties. Even though many results from the general abstract\ntheory apply directly to the complex Hilbert space case, the\nsystematic treatment considering both the additive and the\nmultiplicative structure seems to be missing.\nIn Section II-B the finite dimensional Hilbert space is\nreviewed by constructing an isomorphism into a real-valued\nHilbert space. This isomorphism shows essentially the difference between the real and complex Hilbert spaces. In\nSection II-C some basic properties of r.vc.s in the complex\nHilbert space are stated, the second-order structure of complex\nr.vc.s is studied in Section II-D. Complex normal r.vc.s are\nstudied is Section II-E and, finally the complex DarmoisSkitovich theorem is proved in Section II-F.\nA. Notation\nLet us begin with some definitions and notations. We have\nused typewriter font for all random objects, e.g. x, in order to\ndistinguish them from deterministic ones, e.g. x. For random\nvectors, e.g. ~x, we have used the vec symbol in order to\nseparate them from scalar random variables. For deterministic\nobjects, the bold face lower case letters are used for vectors,\ne.g. z, and the bold face upper case letters are used for\nmatrices, e.g. W .\nThe modulus\u221aof a complex\np number z = zR + \uf6bezI \u2208 C is\u2217\n2 + z 2 , where the superscript\ndenoted |z| = z \u2217 z = zR\nI\n\u221a\ndenotes the complex conjugate, z \u2217 = zR \u2212 \uf6bezI , and \uf6be = \u22121\nis the imaginary unit. Recall that any nonzero complex number\nz can be given in polar form z = \u03b1e\uf6be\u03b8 , where \u03b1 > 0, \u03b8 \u2208 R.\nThe number \u03b8 is called an argument of the complex number\nz, and the argument \u03b8 = Arg(z) such that \u2212\u03c0 \u2264 \u03b8 < \u03c0 is\ncalled the principal argument. The real part of a p-dimensional\ncomplex vector (z1 z2 * * * zp )T = z \u2208 Cp , where T is\nthe ordinary transpose, is denoted by zR and the imaginary\npart by zI . The Euclidean norm of a vector z is denoted\nk z k2 = hz, zi = zH z, where h*, *i is the inner product and\nthe superscript H denotes the conjugate transpose, i.e., the\nHermitian adjoint. A complex matrix C \u2208 Cp\u00d7p is termed\n[27] symmetric if C T = C and Hermitian if C H = C.\nFurthermore, the matrix C is orthogonal if C T C = CC T =\nI p and unitary if C H C = CC H = I p , where I p denotes the\np \u00d7 p identity matrix.\nB. Complex Hilbert space isomorphism\nLet C = C R + \uf6beC I \u2208 Cm\u00d7p and z = zR + \uf6bezI \u2208 Cp . We\nuse the following notations\n\u0013\n\u0012 \u0013\n\u0012\nzR\nC R \u2212C I\n(2)\nand zR =\nCR =\nzI\nCI CR\nfor the associated 2m \u00d7 2p real matrix and 2p-variate real\nvector, respectively. The mapping z 7\u2192 zR gives naturally a\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\ngroup isomorphism between the additive Abelian groups Cp\nand R2p . In the case m = p = 1, the mapping given by\nC 7\u2192 C R defines a field isomorphism (e.g., [14], [22]) between\nthe complex numbers and a subset of real two dimensional\nmatrices. Therefore, one can construct real structures where\nthe role of complex multiplication is played by the special\nmatrices.\nNow consider the mapping\nCz 7\u2192 (Cz)R = C R zR .\n\n(3)\n\nIt is continuous and therefore preserves the topological properties, i.e., it is a homeomorphism [19]. Let diag(z) (as in\nMatlab) denote the diagonal matrix with components of z in\nits main diagonal and zeros elsewhere. Since Cp is a vector\nspace, where the scalar multiplication for c \u2208 C is given by\n\uf8eb \uf8f6\ncz1\n\u0001\n\uf8ec .. \uf8f7\ncz , \uf8ed . \uf8f8 = diag( c * * * c )z,\n(4)\nczp\nthe mapping (3) defines a vector space isomorphism between\nthe standard p-dimensional complex vector space and a 2pdimensional real-valued vector space given by the mapping. It\nis important to realize that this associated real-valued vector\nspace is not isomorphic to the standard real vector space R2p .\nFurthermore, by equating zH\n1 with C in (3) it is easily verified\nH\nthat the mapping C \u2192 R2 : zH\n1 z2 7\u2192 (z1 )R (z2 )R associates\n2p\na (complex) inner product for R . Therefore, the mapping\n(3) is also a Hilbert space isomorphism. Again, it should be\nemphasized that the inner product given by the mapping is\nnot the standard Euclidean inner product in R2p . However,\nthe vector norms, and hence metrics, are equivalent in both.\nThe following properties are easily established.\nLemma 1: Let C \u2208 Cp\u00d7p and z \u2208 Cp .\n\u0001\n\u0001\n(i) | det C |2 = det C R .\n\u00012\n(ii) C is Hermitian\niff C R is \u0001symmetric. Then\n\u0001\n\u0001 det C =\ndet C R and 2 \u00d7 rank C = rank C R .\n(iii) C is nonsingular iff C R is nonsingular.\n(iv) C is unitary iff C R is orthogonal.\n(v) zH Cz = zTR C R zR\n(vi) C is Hermitian positive definite iff C R is symmetric\npositive definitive.\n(vii) Any polynomial with complex coefficients in variables\nzR can be equivalently given in variables (z, z\u2217 ).\nProof: These properties are direct consequences of the\nisomorphism, see, e.g., [22], [24]. The last property follows\n\u2217\nfrom the identities zR = 12 (z + z\u2217 ) and zI = \u2212\uf6be\n2 (z \u2212 z ).\n\u2217\nSince the variables (z, z ) in Lemma 1(vii) are dependent,\nwe call such complex polynomials wide sense polynomials.\nThe idea of using also the complex conjugate variable has\nturned out to be highly useful in, e.g., complex parameter\nestimation [28] and blind channel equalization [16].\nC. Complex random vectors\nA p-variate complex random vector (r.vc.) ~x is defined as\nan r.vc. of the form\n~x = ~xR + \uf6be~xI ,\n\n(5)\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nwhere ~xR and ~xI are p-variate real r.vc.s, i.e., ~xR and ~xI are\nmeasurable functions from a probability space to Rp . This\nis equivalent for ~x to be measurable from the probability\nspace into Cp due to the separability of the complex space.\nTherefore, the probabilistic structure of the r.vc.s in Cp and the\nprobabilistic structure of the r.vc.s in R2p is the same. However, the operator structure is different as it is evident from\nthe previous section. This gives distinct properties to the r.vc.s\nwith complex values, and justifies studying them separately.\nThroughout this paper all complex r.vc.s are assumed to be\nfull. This means that the support of the induced measure of a\np-dimensional r.vc. is not contained in any lower dimensional\ncomplex subspace.\nSince the probabilistic structures of r.vc.s in Cp and in R2p\nare the same, also the operator structure of r.vc.s in Cp can be\nstudied by first using the isomorphism (3) and then applying\nthe concepts associated with the real r.vc.s. However, we define\nthese associated concepts directly on Cp , since this approach\nis notationally more convenient.\nThe expectation E[*] of a complex r.vc. ~x is defined as\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n(6)\nE~x ~x = E~xR ~xR + \uf6be E~xI ~xI ,\n\nand the distribution function F~x is given as F~x (z) , F~xR (zR ),\nwhere z = (z1 , . . . , zp )T \u2208 Cp and F~xR denotes the distribution function of real-valued r.vc. ~xR . Then for independent\nr.v.s (s1 , . . . , sp )T = ~s, we have\nF~s (z) = F~sR (zR ) =\n\np\nY\n\nF(sk )R ((zk )R ) =\n\nk=1\n\np\nY\n\nFsk (zk ). (7)\n\nk=1\n\nThe same way we define the probability density function f~x\n(if it exists) of a p-dimensional complex r.vc. ~x as f~x (z) ,\nf~xR (zR ), and the characteristic function (c.f.) [14] as\n\u0002\n\u0001\u0003\n\u03c6~x (z) , \u03c6~xR (zR ) = E~xR exp \uf6behzR ,~xR i\n\u0002\n\b\n\u0001\u0003\n(8)\n= E~x exp \uf6be Re hz,~xi .\n\nIt follows directly from Eq. (7) that for independent complex\nr.v.s (s1 , . . . , sp )T = ~s,\n\u03c6~s (z) =\n\np\nY\n\n\u03c6sk (zk ).\n\n(9)\n\nk=1\n\nUsing a standard property of real c.f.s and the properties of\nthe isomorphism (3), we have a useful relation for the c.f. of\nan r.vc. ~x and the c.f. of the linearly transformed r.vc. C~x.\nNamely, for any complex matrix C, we have\n\u03c6C~x (z) =\u03c6(C~x)R (zR ) = \u03c6C R~xR (zR ) = \u03c6~xR ((C R )T zR )\n=\u03c6~xR ((C H )R zR ) = \u03c6~xR ((C H z)R ) = \u03c6~x (C H z).\n(10)\nFinally, a c.f. \u03c6~x (z) is called analytic if \u03c6~xR (zR ) is an analytic\nc.f. [29], i.e., the real c.f. \u03c6~xR (zR ) has a regular extension\ndefined on C2p in some neighborhood of the origin.\nD. Second-order statistics of complex random vectors\nAn r.vc. ~x has\norder or weak second order [14]\n\u0002 finite second\n\u0003\nstatistics if E~x |h~x, zi|2 < \u221e for all z \u2208 Cp . This is clearly\nequivalent to the existence of finite second order statistics for\n\n3\n\nboth real r.vc.s ~xR and ~xI . All r.vc.s in this section are assumed\nto have finite second order statistics. Such r.vc.s are in general\ncalled second-order complex r.vc.s.\nThe second-order statistics between two real r.vc.s may be\ndescribed by\nmatrix. The complex covariance\n\u0003\n\u0002 the covariance\nmatrix cov ~x1 ,~x2 of two complex r.vc.s ~x1 and ~x2 may be\ndefined as\n\u0002 \u0003 \u0003\n\u0002 \u0003\n\u0002\n\u0003\n\u0002\ncov ~x1 ,~x2 , E~x1 ,~x2 (~x1 \u2212 E~x1 ~x1 )(~x2 \u2212 E~x2 ~x2 )H . (11)\nHowever, considering the real representations of the complex\nr.vc.s, it can be seen that the complex covariance matrix does\nnot give complete second order description.\n\u0003For that we define\n\u0002\nthe pseudo-covariance matrix1 pcov ~x1 ,~x2 [11] as\n\u0002 \u0003 \u0003\n\u0002 \u0003\n\u0002\n\u0003\n\u0002\npcov ~x1 ,~x2 , E~x1 ,~x2 (~x1 \u2212 E~x1 ~x1 )(~x2 \u2212 E~x2 ~x2 )T\n\u0003\n\u0002\n= cov ~x1 ,~x\u22172 .\n(12)\n\nTwo complex r.vc.s ~x1 and ~x2 are uncorrelated\nif real r.vc.s\n\u0002\n\u0003\n(~x1 )R and (~x2 )R are uncorrelated, i.e., cov (~x1 )R , (~x2 )R =\n02p\u00d72p , where 02p\u00d72p denotes the 2p \u00d7 2p matrix of zeros.\nThen, by using the properties from the previous section, the\nfollowing lemma [11] follows directly.\nLemma 2:\nr.vc.s\n\u0002 Complex\n\u0003\n\u0002 ~x1 and\n\u0003 ~x2 are uncorrelated if and\nonly if cov ~x1 ,~x2 = pcov ~x1 ,~x2 = 0p\u00d7p .\nAs it is the case with real r.vc.s, the internal correlation\nstructure of a single r.vc. ~x may be of interest in addition\n\u0002 \u0003\nto correlation\nbetween\n\u0002 \u0003\n\u0002 \u0003two r.vc.s. \u0002Then\u0003 we define cov ~x ,\ncov ~x,~x and pcov ~x , pcov ~x,~x , and call them the\ncovariance matrix and the pseudo-covariance matrix of an\nr.vc. ~x, respectively.\nIt is easily seen that the covariance\n\u0002 \u0003\nmatrix cov ~x is Hermitian and the pseudo-covariance matrix\nis symmetric. Since all\n\u0002 \u0003 r.vc.s are assumed to be full, the\ncovariance matrix cov ~x is also positive definite. R.vc. ~x is\nsaid to have uncorrelated components if all its marginal r.v.s\nxk and xl , k 6= l, are uncorrelated. The following lemma is a\nsimple consequence of Lemma 2.\nLemma 3: A complex r.vc. ~x has uncorrelated components\nif and only if its covariance matrix and pseudo-covariance\nmatrix are diagonal.\n\u0002 \u0003\nAn r.vc. ~x is said to be\u0002 spatially\nwhite, if cov ~x = \u03c3 2 I p\n\u0003\nfor some \u03c3 2 > 0. If pcov ~x = 0p\u00d7p , then the r.vc. is called\nsecond order circular (or circularly symmetric). Some authors\nprefer the term proper [11], [14]. Circular r.vc.s have gained\nmost of the attention in the literature of complex r.vc.s. This is\nlikely due to the fact that all the second order information of\ncircular r.vc.s is contained in the covariance matrix, which, on\nthe other hand, behaves like the covariance matrix for the real\nr.vc.s. However, in this paper we need the complete secondorder description to be derived next. Our approach is to our\nbest knowledge novel, mainly based on the following theorem.\nFor alternative characterizations, see [12]\u2013[14].\nTheorem 1: Any full complex p-dimensional r.vc. ~x with\nfinite second order statistics can be transformed by using\na nonsingular square matrix C such that the r.vc. ~s =\n(s1 , . . . , sp )T = C~x has the following properties:\n1 The pseudo-covariance matrix is called the relation matrix in [12] and the\ncomplementary covariance matrix in [13].\n\n\f4\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\n\u0002 \u0003\n(i) cov ~s\u0002 \u0003= I p\n\u0002 \u0003\n\u0002 \u0003\n(ii) pcov ~s = diag(\u03bb ~s ), where \u03bb ~s = (\u03bb1 , . . . , \u03bbp )T\ndenotes a vector such that \u03bb1 \u2265 * *\u0002* \u2265 \u0003\u03bbp .\n\u0002 \u0003\nProof:\ncov C~x = C cov ~x C H\n\u0002 It\u0003is easily verified\n\u0002 \u0003 that\nand pcov C~x = C pcov ~x C T . By Corollary 4.6.12(b) in\n[27], if a matrix A is Hermitian and positive definite and a\nmatrix B is symmetric, then there exists a nonsingular matrix\nC such that CAC H = I p and CBC T is a diagonal matrix\nwith nonnegative diagonal entries. Since the covariance matrix\nis Hermitian and positive definitive and the pseudo-covariance\nmatrix is symmetric, the proof is completed by noticing that\nthe diagonal entries can be ordered by permutating the rows\nof C.\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\nSince\n\u0002 cov x\u0003I and pcov x =\n\u0002 \u0003 cov x \u0002 =\u0003 cov xR +\ncov xR \u2212 cov xI + 2\uf6be cov xR , xI for any complex\nr.v. \u0002 x \b = xR \b+ \uf6bex\n\u0003 I , it follows that in \u0002Theorem\n\b\n\u0003 1\ncov\u0002Re\bsk ,\u0003Im sk = 0 and 1 \u2265 \u03bbk = cov Re sk \u2212\n\u2265 0, k = 1, . . . , p. The r.vc.s satisfying the\ncov Im sk\nproperties of Theorem 1 have a special structure, and they are\nhere called strongly uncorrelated.\nAny strongly uncorrelated\n\u0002 \u0003\nr.vc. is white with cov ~s = I p , but the converse is not true.\nIn general, for a given r.vc. ~x, the strongly uncorrelated r.vc. ~s\nand the strong-uncorrelating transform C given by Theorem 1\nare not unique. However, we have the following.\u0002 \u0003\nTheorem 2: For a given r.vc. ~x, the vector \u03bb ~s in Theorem 1 is unique.\nProof: Suppose there exist two nonsingular transformations C 1 and C 2 such that r.vc.s ~s1 = C 1~x and ~s2 = C 2~x\nsatisfy the properties in Theorem 1. Let C 1 = U 1 \u039b1 V H\n1\nand C 2 = U 2 \u039b2 V H\n2 be the singular value decompositions\n(SVD) \u0002(see\nmatrices. Now \u0002I p\u0003 =\n\u0003 [27]) of the \u0002transform\n\u0003 H\nC 1 cov ~x C H\n=\nC\ncov\n~\nx\nC\n,\nand\n2\n1\n2\n\u0002 \u0003therefore cov ~x =\n\u22122 H\n\u22122 H\nV 1 \u039b1 V 1 = V 2 \u039b2 V 2 . Since cov ~x is positive definite,\nH\nit follows V 1 \u039b1 V H\n1 = V 2 \u039b2 V 2 . Now\n\u0002 \u0003\n\u0002\n\u0003\npcov ~s1 =U 1 \u039b1 V H\nx V \u22171 \u039b1 U T1\n1 pcov ~\n\u0002 \u0003 \u2217\nH\n=U 1 (V H\nx V 1 \u039b1 (V T1 V \u22171 )U T1\n1 V 1 )\u039b1 V 1 pcov ~\n\u0002\n\u0003\nH\n=U 1 V H\nx (V \u22171 \u039b1 V T1 )V \u22171 U T1\n1 (V 1 \u039b1 V 1 ) pcov ~\n\u0002 \u0003 \u2217\nH\n=U 1 V H\nx (V 2 \u039b2 V T2 )V \u22171 U T1\n1 (V 2 \u039b2 V 2 ) pcov ~\n\u0002 \u0003\nH\nH\n=U 1 V H\nx\n1 V 2 (U 2 U 2 )\u039b2 V 2 pcov ~\nV \u22172 \u039b2 (U T2 U \u22172 )V T2 V \u22171 U T1\n\n\u0002 \u0003\nH\nH\n=U 1 V H\nx\n1 V 2 U 2 (U 2 \u039b2 V 2 pcov ~\n\nV \u22172 \u039b2 U T2 )U \u22172 V T2 V \u22171 U T1\n\u0002 \u0003 \u2217 T \u2217 T\nH\ns2 U 2 V 2 V 1 U 1 ,\n=U 1 V H\n1 V 2 U 2 pcov ~\n\n(13)\n\u0002 \u0003\n\u0002 \u0003\nis unitary, pcov ~s1 and pcov ~s2\nand since U 1 V\nhave \u0002the\u0003 same singular\n\u0002 \u0003 values. Since by the assumption\npcov ~s1 and\u0002 pcov\n\u0003 ~s2 are\n\u0002 \u0003diagonal with sorted entries, it\nfollows pcov ~s1 = pcov ~s2 .\nRemark 1: The proof of Theorem 2 gives a way to construct\na strong-uncorrelating transform C as follows:\n\u0002 \u0003\u2212 1\n(i) Find the usual whitening transform D = cov\u0002 ~x\u0003 2 , i.e.,\nthe inverse of the matrix square root of cov ~x .\n(ii) Any symmetric matrix B has a special form of SVD\nknown as Takagi's factorization (see [27]). The factorizaH\nH\n1 V 2U 2\n\ntion is given as B = U \u039bU T , where U is unitary and \u039b\nis a diagonal matrix with real nondecreasing nonnegative\nmain diagonal entries. An example of\u0002 the \u0003factorization\nis given in Eq. (13). Hence, find pcov D~x = U \u039bU T .\n(iii) Set C = U H D.\n\u0002 \u0003\nNotice also that the vector \u03bb ~s contains the singular values\nof the pseudo-covariance matrix of a white r.vc. with unit\nvariances.\nThe previous theorems lead to a useful characterization of\nsecond-order complex r.vc.s. \u0002 \u0003\n\u0002 \u0003\nDefinition 1: The vector \u03bb ~x , \u03bb ~s = (\u03bb1 , . . . , \u03bbp )T in\nTheorem 1 is called the circularity spectrum of an r.vc. ~x. An\nelement of the circularity spectrum corresponding to an r.v. is\ncalled a circularity coefficient.\nAny r.vc. ~x is clearly second order circular \u0002if\u0003and only if\nits circularity spectrum is a zero vector, i.e., \u03bb ~x = 0p\u00d71 .\nCorollary 1: If the circularity spectrum of an r.vc. has\ndistinct elements, all rows corresponding to nonzero circularity\ncoefficients of the strong-uncorrelating transform are unique\nup to multiplication of the row by \u22121. A row corresponding\nto the zero coefficient is unique up to multiplication of the\nrow by e\uf6be\u03b8 , \u03b8 \u2208 R.\nProof: The left unitary factor in the SVD of a block\nmatrix with distinct singular values is determined up to right\nmultiplication by the matrix \u039b = diag(e\uf6be\u03b81 , . . . , e\uf6be\u03b8p ) and the\nright unitary factor is determined by the left unitary factor [27].\nIn the special form for a symmetric matrix (Takagi's factorization), \u03b8k = 0 or \u03b8k = \u03c0 for the values of k corresponding\nH\nto nonzero singular values. Therefore, U 1 V H\n1 V 2 U 2 = \u039b in\nEq. (13), and\nH\nH\nC 1 =U 1 \u039b1 V H\n1 = U 1 V 1 (V 1 \u039b1 V 1 )\nH\nH\n=\u039bU 2 V H\n2 (V 2 \u039b2 V 2 ) = \u039bU 2 \u039b2 V 2 = \u039bC 2\n\n(14)\n\nby the proof of Theorem 2.\nSome properties of the circularity coefficient are listed in\nthe following lemma, whose proof is given in Appendix I.\nLemma 4: Let x and y be uncorrelated second-order complex r.v.s. Then\n\u0002\u0003\n\u0002 \u0003\n\u0002 \u0003\n| pcov x |\n\u0002\n\u0003 \u2264 1 for any nonzero\n(i) 0 \u2264 \u03bb cx = \u03bb x =\ncov x\n\nconstant\nc \u2208 C,\n\u0002 \u0003\n(ii) \u03bb x = 1 if and only if x = c(sR + \uf6be\u03b1) for some unit\nvariance real r.v. sR and deterministic constants 0 6= c \u2208\nC, \u03b1 \u2208 R,\n\u0002\u0003\n\u0002\u0003\n\u0002\n\u0003\n\u0002 \u0003 \u0002 \u0003\n| pcov x +pcov y |\n\u0002\u0003\n\u0002\u0003\n(iii) \u03bb x + y =\n\u2264 max{\u03bb x , \u03bb y }\ncov x +cov y\n\u0002 \u0003\n\u0002 \u0003\nwith the \u0002equality\nif and only\n\u0003\n\u0002 \u0003 if \u03bb\u0002 x\u0003 = \u03bb y and\nArg(pcov x ) = Arg(pcov y ) if \u03bb x 6= 0.\n\nE. Complex normal random vectors\nThere are no commonly agreed definitions of what is meant\nby complex normal r.vc.s. It is natural to require that a r.vc.\n~x is normal (Gaussian) if the real r.vc. ~xR is multivariate\nnormal. Such r.vc.s are generally called wide sense normal\nr.vc.s [14]. Since the real complex normal r.vc. is completely\ncharacterized by its mean vector and covariance, the results\nfrom the previous section show that a wide sense complex\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nnormal r.vc. is completely specified by its mean, covariance\nmatrix, and pseudo-covariance matrix.\nHowever, all wide sense normal r.vc.s do not possess all the\nproperties that real normal r.vc.s do. Only a special subclass\nof wide sense normal r.vc.s has a density function similar to\nthe real r.vc.s [21], [22], maximizes the entropy [11], or has\nthe 2-stability property (Polya's characterization) [26]. Such\nr.vc.s are called narrow sense normal r.vc.s [14]. They are\nwide sense normal r.vc.s such that the real and imaginary parts\nof any linear projection of the r.vc. are independent and have\nequal variances. This condition is equivalent to the requirement\nthat a wide sense normal r.vc. is second order circular (see,\ne.g., [11]).\nIn order to establish the properties of the complex ICA\nmodel of Eq. (1), neither wide sense normal in its full generality nor narrow sense normal is adequate, and a more specific\ncharacterization of complex normal r.vc.s is needed. This is\ndone next. From now on, we will use the term \"complex\nnormal\" to mean wide sense complex normal r.vc.\nThe main result is the following decomposition theorem for\ncomplex normal random vectors.\nTheorem 3: An r.vc. ~n is complex normal with circularity\nspectrum \u03bb if and only if\n~n = C(~\n\u03b7R + \uf6be~\n\u03b7I ) + \u03bc\n\n(15)\n\nfor some nonsingular matrix C, a complex constant vector \u03bc,\nand multinormal\nreal independent r.vc.s ~\u03b7R \u223c N 0p\u00d71\u0001, 12 I p +\n\u0001\n1\nand ~\u03b7I \u223c\nN 0p\u00d71 , 21 I p \u2212 12 diag(\u03bb)\n2 diag(\u03bb)\n\u0002 \u0003\n\u0002\n\u0003\n\u0002 \u0003. Also\nH\nT\ncov ~n = CC , pcov ~n = C diag(\u03bb)C , and E~n ~n = \u03bc.\nProof: \u0002It is\nthat the r.vc.\n\u0003 obvious H\n\u0002 \u0003 ~n in Eq. (15) is complex\nnormal,\ncov\n~\nn\n=\nCC\n,\npcov\n~n = C diag(\u03bb)C T , and\n\u0002 \u0003\nE~n ~n = \u03bc. Thus, it remains to show that any complex normal\nr.vc. can be given the form (15).\nLet ~n be a complex normal r.vc. Without loss of generality\nassume it is zero mean. By Theorem\n\u0002 \u0003 1, there exists a\u0002 nonsin\u0003\ngular matrix D such that cov D~n = I p and pcov\nD~n =\n\u0001\n1\n1\ndiag(\u03bb). Let ~\u03b7R \u223c N 0p\u00d71\n\u0001 , 2 I p + 2 diag(\u03bb) and ~\u03b7I \u223c\n1\n1\nN 0\u0002 p\u00d71 , 2 I p \u0003\u2212 2 diag(\u03bb) be real independent r.vc.s. Now\ncov ~\u03b7R +\u0002 \uf6be~\n\u03b7I = 12\u0003I p + 21 diag(\u03bb) + 12 I p \u2212 12 diag(\u03bb) = I p\n\u03b7R + \uf6be~\n\u03b7I have\nand pcov ~\u03b7R + \uf6be~\n\u03b7I = diag(\u03bb). Hence D~n and ~\nthe same second order structure. Since a zero mean complex\nnormal r.vc. is completely characterized by the covariance and\nthe pseudo-covariance matrices, it follows D~n = ~\u03b7R +\uf6be~\n\u03b7I , and\nthe claim follows by setting C = D\u22121 .\nA complex normal r.vc. ~\u03b7 such that C = I p and \u03bc = 0p\u00d71\nin the representation (15), i.e., ~\n\u03b7=~\n\u03b7R +\uf6be~\n\u03b7I , is called standard\ncomplex normal with the circularity spectrum \u03bb. Clearly any\ncentered and strongly uncorrelated complex normal r.vc. is\nstandard. Also, it is seen that any complex normal r.vc. may\nbe alternatively specified by the mean, the circularity spectrum,\nand the (inverse of) strong-uncorrelating matrix C.\nThe previous decomposition allows the derivation of differential entropy of a complex normal r.vc. in a closed form.\nEntropy h(~n) of an r.vc. ~x is defined as the entropy [30] of the\nreal r.vc. ~xR . The following result has been implicitly derived\nin [31] without reference to circularity coefficients.\nCorollary 2: The differential entropy h(~n) of a zero-mean\ncomplex normal r.vc. ~n with the circularity coefficients \u03bbk 6=\n\n5\n\n1, k = 1, . . . , p, is given by\np\n\u0002 \u0003\u0001 1X\nlog(1 \u2212 \u03bb2k ).\n(16)\nh(~n) = log det(\u03c0e cov ~n ) +\n2\nk=1\nProof: Let ~n = C~\u03b7 \u0002be \u0003the decomposition\ngiven by\nQp\nTheorem 3. Now det(2 cov ~\u03b7R ) = k=1 (1 \u2212 \u03bb2k ), and the\ndifferential entropy of real-valued normal r.vc. [30] simplifies\nas\n\u0002 \u0003\u0001\n1\nh(~n) = log det(2\u03c0e cov ~nR )\n2\n\u0002\n\u0003\u0001\n1\n= log det(2\u03c0e cov C R ~\u03b7R )\n2\n\u0001\n\u0002 \u0003\n1\n= log det(2\u03c0eC R cov ~\u03b7R C TR )\n2\n\u0001 1\n\u0002 \u0003\u0001\n1\n= log det(\u03c0eC R C TR ) + log det(2 cov ~\u03b7R )\n2\n2\np\nY\n\u0001 1\n\u0001\n1\n(1 \u2212 \u03bb2k )\n= log (\u03c0e)2p det((CC H )R ) + log\n2\n2\nk=1\n\np\n\u0002 \u0003 \u0001 1X\n1\n= log (\u03c0e)2p det(cov ~n R ) +\nlog(1 \u2212 \u03bb2k )\n2\n2\nk=1\n\np\n1X\n\n\u0002 \u0003 \u0001\n1\n= log (\u03c0e)2p det(cov ~n )2 +\n2\n2\n\nk=1\n\nlog(1 \u2212 \u03bb2k )\n\np\n\u0002 \u0003 \u0001 1X\n= log det(\u03c0e cov ~n ) +\nlog(1 \u2212 \u03bb2k )\n2\nk=1\n\n(17)\n\nby the properties of Lemma 1.\nSince the summation term on the right of Eq. (16) is\nalways nonpositive and the entropy of real r.vc.s with the\ngiven covariance is maximized for Gaussian r.vc.s [30], it\nmay be seen that the entropy of complex r.vc.s with the given\ncovariance is maximized for a narrow sense complex normal\nr.vc. [11], i.e., for a complex normal r.vc. with zero pseudocovariance. Theorem 3 allows also an easy derivation of the\nc.f. of a complex normal r.vc. [12], [14].\nCorollary 3: The c.f. of a complex normal r.vc. ~n is given\nby\n\u0002 \u0003\n\u0002 \u0003\n1 \b\n1\n\u03c6~n (z) = exp \u2212 zH cov ~n z \u2212 Re zH pcov ~n z\u2217\n4 \b\n4\n\u0002 \u0003 \u0001\n+ \uf6be Re zH E~n ~n\n\u0002 \u0003\n\u0002 \u0003\n1 \b\n= exp \u2212 Re hz, cov ~n z + pcov ~n z\u2217 i\n4 \b\n\u0002 \u0003 \u0001\n+ \uf6be Re hz, E~n ~n i .\n(18)\nProof: By Theorem 3, ~n = C(~\u03b7R + \uf6be~\n\u03b7I ) + \u03bc. Let z =\nzR + \uf6bezI \u2208 Cp , and ~\u03b7 = ~\u03b7R + \uf6be~\n\u03b7I . Now\n\u0002 \u0003 \u0001\n1\n\u03c6\u03b7~ (z) =\u03c6\u03b7~R (zR ) = exp \u2212 (zTR cov ~\u03b7R zR )\n2\n1 T\n= exp \u2212 (zR (I p + diag(\u03bb))zR\n4\n\u0001\n+ zTI (I p \u2212 diag(\u03bb))zI )\n(19)\n1\n= exp \u2212 (zTR zR + zTI zI + zTR diag(\u03bb)zR\n4\n\u0001\n\u2212 zTI diag(\u03bb)zI )\n\b\n\u0001\n1\n= exp \u2212 (zH z + Re zT diag(\u03bb)z ) ,\n4\n\n\f6\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\nand by Eq. (10)\n\b\n\u0001\n\u03c6~n (z) =\u03c6C~\u03b7+\u03bc (z) = \u03c6C~\u03b7 (z) exp \uf6be Re hz, \u03bci\n\b\n\u0001\n=\u03c6\u03b7~ (C H z) exp \uf6be Re hz, \u03bci\n\b\n\u0001\n1\n= exp \u2212 (zH CC H z + Re zT C \u2217 diag(\u03bb)C H z )\n4\b\n\u0001\nexp \uf6be Re zH \u03bc\n\b\n1\n= exp \u2212 (zH CC H z + Re zH C diag(\u03bb)C T z\u2217 )\n\b4\n\u0001\n+ \uf6be Re zH \u03bc .\n(20)\nCorollary 3 shows in particular that the second characteristic function \u03c8~x , log \u03c6~x of a complex r.vc. ~x is a second-order\nwide sense polynomial in variables (z, z\u2217 ). Theorem 3 can be\nalso used to derive the density function of a complex normal\nr.vc. However, unlike the c.f., the density function of a wide\nsense normal r.vc. does not appear to have a simple form. See\n[12] for expressions for the density function in terms of the\ncovariance and the pseudo-covariance matrices. The following\nexample essentially shows that in some cases the distribution\nof a standard complex normal r.vc. is invariant to orthogonal\ntransformations.\nExample 1: Let the components of ~n be uncorrelated complex normal r.v.s with the same circularity coefficient \u03bb.\nNow for a diagonal matrix \u039b the r.vc. \u039b~n is standard complex normal with the circularity spectrum (\u03bb * * *\u0002 \u03bb)T , \u0003and\nfor any\northonormal matrix O, cov\u0002 O\u039b~n\u0003 =\n\u0002 (real-valued)\n\u0003 H\nT\nO cov \u039b~\nn\nO\n=\nOI\n= I p and pcov O\u039b~n =\npO\n\u0002 \u0003 T\nO pcov \u039b~n O = O(\u03bbI p )O T = \u03bbI p . Therefore, the r.vc.\nO\u039b~n is also standard complex normal.\n\nr.v.s)\nx1 =\n\nn\nX\n\n\u03b1k sk and x2 =\n\nn\nX\n\n\u03b2k s k ,\n\n(21)\n\nk=1\n\nk=1\n\nwhere \u03b1k , \u03b2k \u2208 C, k = 1, . . . , n, are independent, then r.v.s\nsk for which \u03b1k \u03b2k 6= 0 are complex normal.\nSketch of the proof: The complete proof is given in Appendix II and it follows the proof of the real-valued DarmoisSkitovich theorem (see [4]) with appropriate extensions to\ncomplex field. The idea is to consider two forms of the\nlogarithm of the joint c.f. of x1 and x2 following from\nindependence. This functional equation is only satisfied for\nwide sense polynomials showing that the r.v. x1 is complex\nnormal. This is only possible if r.v.s sk are complex normal.\nAlthough narrow sense complex normal r.v.s had to be\nadmitted to the complex Darmois-Skitovich theorem, it may\nstill appear in the view of Corollary 1 that complex normal\nr.v.s appearing in the theorem can not be completely arbitrary.\nThat is, it may appear that some of the circularity coefficients\nof normal r.v.s should be equal. It is true if n = 2. However,\nit is not generally true as it is shown in the next example.\nExample 2: Let ~\u03b71 = (n1 , n2 , n3 )T be \u0002standard\ncomplex\n\u0003\n1 1 1 T\n=\n(\nnormal r.vc. with the circularity\nspectrum\n\u03bb\n~\n\u03b7\n1\n3, 5, 8) .\n\u0001\n1\n3 5 4\n~\n\u03b7\nis\nalso\nstandard\ncomplex\nnormal\nThen ~\u03b72 = 5\u221a\n1\n3\n\u22125\n4\n2\n\u0002 \u0003\nr.vc. with the circularity spectrum \u03bb ~\u03b72 = ( 15 , 15 )T . Thus\nmarginals of ~\u03b72 are independent, and the Darmois-Skitovich\ntheorem applies. However, the circularity spectrum of ~\u03b71 is\ndistinct. Notice also that by Example 1, the r.vc. obtained from\n~\u03b72 by multiplying with any orthogonal matrix is also standard\ncomplex normal r.vc. with the same circularity spectrum.\nIII. C OMPLEX ICA M ODELS\n\nF. Darmois-Skitovich theorem for complex random variables\nOne of the main characterization theorems for real r.v.s\nis the well-known Darmois-Skitovich theorem (see [4]). The\ntheorem is fundamental for proving the identifiability of real\nICA models [1], [5]. Here we extend the theorem to complex\nr.v.s.\nThe proofs of the complex Darmois-Skitovich theorem and\nthe proof of a closely related characterization theorem (Theorem 5 in Section 5) are both based on a complex functional\nequation (Lemma 5 in Appendix II). The functional equation is\nan extension of the corresponding equation for real variables\n(see, e.g., Lemma 1.5.1 in [4]) to complex variables. Using\nthe mapping (3) Lemma 5 may be easily seen to be a direct\nconsequence of the real multivariate theorem [32] (see also\n[4], [33]). A direct proof is given in Appendix II for the sake\nof completeness.\nThe complex extension of Darmois-Skitovich theorem has\nexactly the same form as the real theorem with the wide\nsense complex normal r.v.s taking the role of real normal r.v.s.\nHence, this theorem is an example where the analogy [22]\nbetween theories of narrow sense complex normal r.v.s and\nreal normal r.v.s is broken.\nTheorem 4 (Complex Darmois-Skitovich): Let sk , . . . , sn\nbe mutually independent complex r.v.s. If the linear forms (the\n\nIn this section, we show that complex ICA is actually a\nwell-defined concept, and we establish theoretical conditions\nsimilar to the real-valued case [5]. In Section III-A the main\ndefinitions along with some illustrative examples are given.\nAlso a crucial characterization theorem giving a connection between vector coefficients and complex normal r.v.s is proved.\nFinally, in sections III-B, III-C, and III-D the conditions for\nseparability, identifiability, and uniqueness of complex ICA\nmodels, respectively, are derived.\nA. Definitions and problem statement\nA general linear instantaneous complex-valued ICA model\nmay be described by the equation\n~x = A~s,\nT\n\n(22)\n\nwhere (s1 , . . . , sm ) = ~s are unknown complex-valued independent non-degenerate r.v.s, i.e., sources, A is a complex\nconstant p \u00d7 m unknown mixing matrix, p \u2265 2, and ~x =\n(x1 , . . . , xp )T are mixtures, i.e., the observed complex r.vc.\n(sensor array output). The couple (A,~s) is called a representation of r.vc. ~x. If no column in the mixing matrix A is collinear\nwith another column in the matrix, i.e., all columns are pairwise linearly independent, the representation is called reduced.\nAll representations are assumed to be reduced throughout\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nthis paper. Furthermore, a reduced representation for the r.vc.\n~x in the model (22) is called proper, if it satisfies all the\nassumptions made about the model.\nThe model of Eq. (22) is defined to be\n(i) identifiable, or the mixing matrix is (essentially) unique,\nif in every proper representations (A,~s) and (B,~r) of\n~x, every column of complex matrix A is collinear with\na column of complex matrix B and vice versa,\n(ii) unique if the model is identifiable and furthermore the\nsource r.vc.s ~s and ~r in different proper representations\nhave the same distribution for some permutation up to\nchanges of location and complex scale, and\n(iii) separable, if for every complex matrix W such that W~x\nhas m independent components, we have \u039bP~s = W~x\nfor some diagonal matrix \u039b with nonzero diagonals and\npermutation matrix P . Moreover, such a matrix W has\nto always exist.\nIt is completely possible for the model (22) to be identifiable\nbut not unique nor separable as it is shown in the next example.\nExample 3: As an example of a model which is identifiable\nbut is not separable nor unique, consider independent nonnormal r.v.s sk , k = 1, . . . 4. Let \u03b71 , \u03b72 , and \u03b73 be independent\nstandard normal r.v.s with the same circularity coefficient.\nThen also r.v.s \u03b71 + \u03b72 and \u03b71 \u2212 \u03b72 are independent. Now\n\uf8f6\ns1\n\uf8f7\n0 1 1 \uf8ec\n\uf8ec s2 \uf8f7\n\uf8ed\n1 1 \u22121\ns3 + \u03b71 \uf8f8\ns4 + \u03b72\n\uf8f6\n\uf8eb\ns\n+\n\u03b7\n1\n1 + \u03b72\n\u0013\n\uf8f7\n1 1 \uf8ec\n\uf8ecs2 + \u03b71 \u2212 \u03b72 \uf8f7 ,\n\uf8f8\n1 \u22121 \uf8ed\ns3\ns4\n(23)\n\n\u0013 \u0012\n\u0012\n1\ns1 + s3 + s4 + \u03b71 + \u03b72\n=\n0\ns2 + s3 \u2212 s4 + \u03b71 \u2212 \u03b72\n\n=\n\n\u0012\n\n1 0\n0 1\n\n\u0013\n\n\uf8eb\n\nwhich shows that the corresponding model can not be unique.\nHowever, it is identifiable. R.v.s of the form s + n, where n\nis a normal r.v. independent of s, are said to have a normal\ncomponent.\nIt follows from the reduction assumption that the number\nof columns, i.e., the number of sources or the model order,\nis the same in every proper representation of ~x in identifiable\nmodels. If W is a separating matrix, then linear manifolds\u0001 of\n\u039bP and W\n\u0001 must coincide, and therefore p \u2265 rank W =\nrank \u039bP = m, i.e., there has to be at least as many mixtures\nas sources in a separable model. This fact also emphasizes that\nidentifiability of the model (22) depends also on the linear\noperator structure, and since the linear operators defined on\nR2p and Cp are not isomorphic, one can not simply consider\nreal-valued model with twice the observation dimension when\nstudying the complex ICA model (22). This is illustrated in\nthe following example.\nExample 4: By simply considering real-valued models with\ntwice the dimension, it may actually seem that the complex\nseparation is possible only under very strict conditions. Indeed,\nlet rk , k = 1 . . . , 4, be independent real-valued r.v.s, and let\nA1 , A2 , B 1 , and B 2 be 2 \u00d7 2 nonsingular real matrices.\n\n7\n\nDefine ~s1 = A1 (r1 r2 )T and ~s2 = A2 (r3 r4 )T . Now ~s1 and\n~s2 are independent, but so are also ~y1 and ~y2 ,\n\u0013 \u0012 \u22121\n\u0013\u0012 \u0013\n\u0012 \u0013 \u0012\nA1\n02\u00d72\n~s1\nB 1 02\u00d72\n~y1\n, (24)\nP\n=\n~s2\n02\u00d72 B 2\n~y2\n02\u00d72 A\u22121\n2\nfor any permutation matrix P . However, ~y1 and ~y2 are\nmixtures of ~s1 and ~s2 for many permutations P .\nThe previous example is easily generalized to the ICA\nmodels that have multidimensional independent sources, i.e.,\none is looking for independent multidimensional subspaces.\nThe example shows that such models can not be identified\nor separated without additional constraints on the internal\ndependency structure of the sources or the allowed mixing\nmatrices.\nSince linear operators in complex and real spaces are\nnot isomorphic, the classes of separable source r.v.s are not\nthe same. That is, some source r.v.s considered in complex\nmixtures can be separated although their real-valued representations in real mixtures can not. This is shown in the next\nexample.\nExample 5: Let \u03b71 , . . . , \u03b72m be independent standard zero\nmean unit variance real Gaussian r.v.s. Define\n\u221a\n1 \u221a\n1\n( m\u03b71 + \uf6be\u03b7m+1 ), \u221a ( m \u2212 1\u03b72 + \uf6be\u03b7m+2 ),\n~\u03b7 = \u221a\nm\nm+1\n\u0001\n1\n. . ., \u221a (\u03b7m + \uf6be\u03b72m ) .\n2\n(25)\nNow it is easily seen that ~\u03b7 is a\u0002 standard\nnormal r.vc. with the\n\u0003\nm\u22122\nT\ndistinct circularity spectrum \u03bb ~\u03b7 = ( m\u22121\nm+1 , m , . . . , 0) . If\n~\u03b7R is taken as the source r.vc. in the real-valued ICA model,\ni.e., ~y = B~\u03b7R and B is a 2p \u00d7 2m real-valued matrix, p \u2265 m,\nthe model is not separable [5]. However, the complex model\ninvolving ~\u03b7 itself, i.e., ~x = A~\u03b7 and A is a p \u00d7 m complexvalued matrix, is separable by Corollary 1.\nThe following characterization theorem is the base of the\nidentifiablility and uniqueness theorems. It is an extension of\na real theorem [4, Theorem 10.3.1] to the complex case. The\nidea of the proof is similar to the proof of Darmois-Skitovich\ntheorem, and the proof given follows loosely that of the real\ncounterpart with appropriate complex extensions.\nTheorem 5: Let (A,~s) and (B,~r) be two reduced representations of a p-dimensional complex r.vc. ~x, where A and B\nare constant complex matrices of dimensions p \u00d7 m and p \u00d7 n,\nrespectively, and ~s = (s1 , . . . , sm )T and ~r = (r1 , . . . , rn )T\nare complex r.vc.s with independent components. Then the\nfollowing properties hold.\n(i) If the kth column of A is not collinear with any column\nof B, then the r.v. sk is complex normal.\n(ii) If the kth column of A is collinear with the lth column\nof B, then the logarithms of the c.f.s of r.v.s sk and rl\ndiffer by a wide sense polynomial in a neighborhood of\nthe origin.\nProof:\n(i) By Lemma 7 (see Appendix III), there exists a 2 \u00d7 p\nmatrix C such that the kth column of D 1 = CA is\nnot collinear with any other column of D 1 , or with any\ncolumn of D2 = CB. Then C~x = D1~s = D 2~r, and\n\n\f8\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\napplying Lemma 8(i) (see Appendix III) it is seen that\nthe r.v. sk is complex normal.\n(ii) By definitions the kth column of A, say \u03b1, is collinear\nonly with the lth column of B, say \u03b2. Therefore by\nLemma 7 (see Appendix III), there exists a 2 \u00d7 p matrix\nC such that the kth column of D1 = CA is not collinear\nwith any other columns of D1 , or with any column of\nD 2 = CB except possibly the lth. Furthermore, since\nC\u03b1 = C(c\u03b2) = c(C\u03b2) for some c \u2208 C, it is seen that\n(D 1 ,~s) and (D 2 ,~r) are reduced representations of C~x\nsuch that Lemma 8(ii) gives the claim.\n\nB. Separability\nICA is commonly used as a Blind Source Separationmethod, where the problem is to extract the original signals\nfrom the observed linear mixture. Therefore, separability of\nthe ICA model is an important issue. The separability theorem\nfor the complex ICA model below may be surprising, since it\nallows also separation of some complex normal mixtures.\nTheorem 6 (Separability): The model of Eq. (22) is separable if and only if the complex mixing matrix A is of full\ncolumn rank and there are no two complex normal source r.v.s\nwith the same circularity coefficient.\nProof:\u0001 Suppose the\u0001 model is separable. Since m =\nrank W A \u2264 rank A \u2264 m, the mixing matrix A is\nof full column rank m. If there were two complex normal\nsource r.v.s with the same circularity coefficient, by Example 1\nin Section II-E, there would exist matrices that produce m\nindependent components but which are not diagonal matrices\nfor any permutation of the columns.\nTo the other direction, suppose the mixing matrix A is of\nfull column rank and there are no two complex normal source\nr.v.s with the same circularity coefficient. Now A# , where the\nsuperscript # denotes the Moore-Penrose generalized inverse\n[27], is a separating matrix. Suppose W is a matrix such\nthat W~x has m independent components. If W A is not of\nthe form \u039bP , then there exist at least two columns such\nthat they both contain at least two nonzero elements. By\nLemma 10 (see Appendix III) there can not exist only one\nsuch column since the sources are nondegenerate. Assume\nwithout loss of generality that the first l columns \u03b2k , k =\n1, . . . , l \u2264 m, of W A are columns with at least two nonzero\nelements, and denote the corresponding matrix of rank l by\nB = (\u03b2 1 * * * \u03b2l ). By Theorem 4 the r.v. sk corresponding\nto the column \u03b2 k , k = 1, . . . , l, is complex normal, and\nwe assume, without loss of generality, that the r.vc. ~\u03b71 =\n(s1 * * * sl )T is standard complex normal. By Theorem 10\n(see Appendix II) all components of ~n2 = B~\n\u03b71 are complex\nnormal, and by Lemma 9 (see Appendix III) all components\nof ~n2 are independent. Choose any l rows of B such that\nthe corresponding submatrix B\u0302 is of rank l, and B\u0302 contains\na row with two nonzero elements. Since B\u0302 is not diagonal\nfor any permutation by construction, ~\n\u03b71 is standard, and ~n2\nhas independent components, it follows from Corollary 1 that\n\u03b71 can not have a distinct circularity spectrum, which is a\n~\ncontradiction. Therefore, W A is of the form \u039bP , and the\nmodel is separable.\n\nRemark 2: If the source ~s has finite\n\u0002 \u0003 second order statistics and the circularity spectrum \u03bb ~s is distinct, then the\nseparation can be achieved by simply performing the stronguncorrelating transform by Corollary 1. In this case, there is\nno additional restrictions on the distribution of the source r.v.s,\nand therefore some normal r.v.s can be also separated. An\nexample of such a mixture is seen in Example 5.\nC. Identifiability\nIdentifiability considers reconstruction of the mixing matrix.\nThis is useful in some problems, where the immediate interest\nmay not be in the sources themselves but in how they were\nmixed (e.g., channel matrix in MIMO communications).\nTheorem 7 (Identifiability): The model of eq. (22) is identifiable, if\n(i) no source r.v. is complex normal, or\n(ii) A is of full column rank and there are no two complex\nnormal source r.v.s with the same circularity coefficient.\nProof:\n(i) Since there are no complex normal r.v.s, by Theorem 5(i),\nevery column has to be collinear with exactly a column\nin another proper representation, i.e., the model is identifiable.\n(ii) Let(A,~s) and (B,~r) be proper representations of ~x.\nSince the model is separable by Theorem 6 and A#\nis a separating matrix, A# B = P \u039b for a permutation\nmatrix P and a diagonal matrix \u039b. By the uniqueness\nof the generalized inverse, it follows AP \u039b = B.\nThere is a striking contrast between the two cases in\nTheorem 7. Namely, if there are more sources than mixtures\nnot a single normal r.v. is allowed whereas in the other case\nall source r.v.s can be normal. The following example shows\nthe reason why we can not allow a single normal r.v. for\nidentifiability when there are more sources than sensors.\nExample 6: Consider independent non-normal r.v.s s1 , s2 ,\nand standard normal r.v.s \u03b71 and \u03b72 with the same circularity\ncoefficient. Now\n\uf8eb\n\uf8f6\n\u0013 \u0012\n\u0012\n\u0013\ns1\n1 1 0 \uf8ed\ns1 + s2 + 2\u03b71\ns2 + 2\u03b71 \uf8f8\n=\n~x =\n1 0 1\ns1 + 2\u03b72\n2\u03b72\n\uf8eb\n\uf8f6\n\u0012\n\u0013 s1 + \u03b71 + \u03b72\n1 1 1 \uf8ed\n\uf8f8,\ns2\n=\n1 0 \u22121\n\u03b71 \u2212 \u03b72\n(26)\nand the last column shows that the model is not identifiable.\nIt is evident from the previous example and from the\nseparation theorem that another identifiability condition could\nbe formulated by essentially allowing a single normal r.v. and\nnot allowing other source r.v.s to have normal components\nwith the same circularity coefficient. However, this condition\nis unnecessarily complicated. Therefore, it is not stated in a\nformal manner.\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nD. Uniqueness\nUniqueness considers the case where one is interested not\nonly in the mixing matrix but also in the distribution of the\nsources.\nTheorem 8 (Uniqueness): The model of Eq. (22) is unique\nif either of the following properties hold.\n(i) The model is separable.\n(ii) All c.f.s of source r.v.s are analytic (or all c.f.s are\nnon-vanishing), and none of the c.f.s has an exponential\nfactor with a wide sense polynomial of degree at least\ntwo, i.e., no source r.v. has the c.f. \u03c6 such that \u03c6(z) =\n\u03c61 (z) exp(P(z, z \u2217)) for a c.f. \u03c61 (z) and for some wide\nsense polynomial P(z, z \u2217) of degree at least two.\nProof:\n(i) Let(A,~s) and (B,~r) be proper representations of ~x. By\nTheorem 7(ii) the model is identifiable, and therefore\nAP \u039b = B for a permutation matrix P and a diagonal\nmatrix \u039b. Now ~s = A#~x = A# B~r = P \u039b~r.\n(ii) There can not be any complex normal r.v.s, and therefore\nthe model is identifiable by Theorem 7(i). Now the\nlogarithms of the c.f.s of the source variables in two\nproper representations differ by a wide sense polynomial\nby Theorem 5(ii). However, by the assumption this wide\nsense polynomial can be at most of degree 1, i.e., the\nsource variables have the same distribution up to changes\nof location and complex scale.\nA nonunique but identifiable mixture was described in Example 3. By slightly restricting the allowed mixing matrices,\nit is possible in the real case to obtain more classes of unique\nmodels [5]. Further work is needed to determine if those\ntheorems can be extended to the complex case.\nIV. C ONCLUSION\nIn this paper conditions for separability, identifiablity, and\nuniqueness of complex-valued linear ICA models are established. Both circular and noncircular complex random vectors\nare covered by the results. So far these conditions have\nbeen known for real random vectors only. The conditions for\nidentifiablity, and uniqueness are sufficient and the separability\ncondition is also found to be necessary. In order to show these\nresults, a proof of complex extension of the Darmois-Skitovich\nTheorem is constructed. Some second-order properties and\ncharacterizations of linear forms of complex random vectors\nare reviewed and new results found in the process of proving\nthe theorem. As a by-product of establishing the conditions,\na theorem on differential entropy for complex normal random\nvectors is proved and a slightly surprising result about separating complex Gaussian sources is found.\nACKNOWLEDGMENT\nThe authors wish to thank the anonymous reviewers for their\nvaluable comments and suggestions.\n\n9\n\nA PPENDIX I\nP ROOF OF L EMMA 4\nProof of Lemma 4: By Theorem 1 there exist nonzero\nconstants a, b \u2208 C such that r.v.s s = ax and r = by are\nstrongly uncorrelated.\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n(i) Since\n\u0003 sR \u0002+ \u0003cov sI = 1,\u0002 0 \u0003\u2264 \u03bb x = a\u03bb s =\n\u0002 cov\ncov sR \u2212 cov sI = 1 \u2212 2 cov\u0002 sI\u0003 \u2264 1. \u0002Also\n\u0003 c (cx)\n\u0002 =\n\u0003\ns, and thus by uniqueness \u03bb cx = \u03bb s = \u03bb x .\nFurthermore\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003 | pcov s |\n| pcov ax |\n\u0002\n\u0003\n\u0002\n\u0003\n=\n\u03bb x = pcov s =\ncov s\ncov ax\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n|a2 pcov x |\n|a2 || pcov x |\n| pcov x |\n\u0002 \u0003 =\n\u0002 \u0003 =\n\u0002 \u0003 .\n= 2\n|a| cov x\n|a|2 cov x\ncov x\n(27)\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n(ii) \u03bb x = 1 \u2212\u00022\u0003cov sI \u0002=\u00031 if and only if cov sI = 0.\n(iii) Suppose \u03bb x \u2265 \u03bb y . Using the first part of the\nlemma for an r.v. x + y, uncorrelateness, and the triangle\ninequality, we have\n\u0002 \u0003\n\u0002 \u0003\n\u0002\n\u0003\n\u0002\n\u0003 | pcov x + y |\n| pcov x + pcov y |\n\u0002\n\u0003 =\n\u0002 \u0003\n\u0002 \u0003\n\u03bb x+y =\ncov x + y\ncov x + cov y\n\u00021 \u0003\n\u00021 \u0003\n| pcov a s + pcov b r |\n\u0002 \u0003\n\u0002 \u0003\n=\ncov a1 s + cov 1b r\n\u0002\n\u0003\n\u0002 \u0003\n| a12 pcov s + b12 pcov r |\n=\n1\n1\n|a|2 + |b|2\n\u0002\n\u0003\n\u0002\n\u0003\n| a12 \u03bb x + b12 \u03bb y |\n=\n1\n1\n|a|2 + |b|2\n\u0002\n\u0003\n\u0002 \u0003\n1\n1\n\u0002 \u0003\n|a|2 \u03bb x + |b|2 \u03bb y\n\u2264\n\u2264\u03bb x ,\n1\n1\n|a|2 + |b|2\n(28)\nwhich proves the inequality.\nIf both r.v.s x and y are second order circular, then clearly\nthe equality holds in (28). Now suppose the condition for\nthe\n\u0002 \u0003equality\n\u0002 \u0003holds in the noncircular\n\u0002 \u0003 case, and let \u0002\u03bb \u0003=\n\u03bb x = \u03bb y and \u03b8 = Arg(pcov x ) = Arg(pcov y ).\nThen\n\u0002 \u0003\n\u0002 \u0003\n\u0002\n\u0003 | pcov x + pcov y |\n\u0002 \u0003\n\u0002 \u0003\n\u03bb x+y =\ncov x + cov y\n\u0002 \u0003\n\u0002 \u0003\n|\u03bb cov x e\uf6be\u03b8 + \u03bb cov y e\uf6be\u03b8 |\n\u0002 \u0003\n\u0002 \u0003\n(29)\n=\ncov x + cov y\n\u0002\n\u0003\n\u0002\n\u0003\n|\u03bbe\uf6be\u03b8 || cov x + cov y |\n\u0002 \u0003\n\u0002 \u0003\n= \u03bb.\n=\ncov x + cov y\n\nTo the other direction,\n\u0002 the\n\u0003 last\n\u0002 \u0003inequality \u0002in \u0003(28) holds\nwith the equality iff \u03bb x = \u03bb y . If now \u03bb x 6= 0, then\nthe triangle inequality in (28) holds with the equality iff\n\u0002 \u0003\n\u0002 \u0003\nb2 pcov s\npcov x\nb2\n\u0002 \u0003=\n\u0002 \u0003.\n(30)\n0< 2 = 2\na\na pcov r\npcov y\n\u0002 \u0003\n\u0002 \u0003\nHence Arg(pcov\ny ) by the polar\n\u0002 \u0003 x ) = Arg(pcov\n\u0002 \u0003\nforms of pcov x and pcov y .\n\n\f10\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\nP ROOF\n\nOF\n\nA PPENDIX II\nTHE COMPLEX DARMOIS -S KITOVICH\n\nTHEOREM\n\n\u0002\u0003\nwhere \u2206 * is the general difference operator defined by\n1\u0002\n\u0003\n\u2206 f (z) =f (z + a) \u2212 f (z)\n\nAND RELATED THEOREMS\n\na\n\nThe following theorem is a direct consequence of the\nmultivariate version of the real Marcinkiewicz theorem. The\ntheorem shows essentially that a complex normal r.v. is the\nonly r.v. whose second c.f. is a wide sense polynomial.\nTheorem 9 (Complex Marcinkiewicz): If in some neighborhood of zero the c.f. \u03c6x of a complex r.v. x admits the\nrepresentation\n\u0001\n\u03c6x (z) = exp P(z, z \u2217) ,\n(31)\n\nwhere P is a wide sense polynomial, then the r.v. x is complex\nnormal.\nProof: Fix\u0001z0 \u2208 C, and define a c.f. \u03c60 (t) , \u03c6x (tz0 ) =\nexp P(tz0 , tz0\u2217 ) for t \u2208 R. Then for some \u03b5 > 0, log \u03c60 (t)\nis a polynomial in t, |t| < \u03b5. Therefore, by a version of \u03b1decomposition theorem (see [34, Theorem 7.4.2]) the relation\nis valid for all t and \u03c60 (t) is normal. Since z0 is assumed to\nbe arbitrary, it follows that the equation (31) is valid for all\nz. By the last property of Lemma 1, P(z, z \u2217) is a polynomial\nin zR , and the claim follows from the multivarite (bivariate)\nMarcinkiewicz's theorem (e.g., [29, Theorem 3.4.3]).\nAlso the well-known Cramer's theorem has a direct complex\ncounterpart.\nTheorem 10 (Complex Cramer): If s1 and s2 are independent r.v.s such that s1 + s2 is a complex normal r.v., then each\nof the r.v.s s1 and s2 is complex normal.\nProof: This is a direct corollary to the real multivariate\nCramer's theorem (e.g., [34, Theorem 6.3.2]).\nLemma 5: Consider the equation, assumed valid for\n|z1 |, |z2 | < \u03b5,\np\nX\n\n\u03c8k (z1 + ck z2 ) = h1 (z1 ) + h2 (z2 ),\n\n(32)\n\nk=1\n\nwhere \u03c8k , k = 1, . . . , p, h1 , and h2 are continuous complexvalued functions of complex variables and the nonzero complex numbers ck , k = 1, . . . , p, are distinct. Then all the\nfunctions in (32) are wide sense polynomials in (z, z \u2217 ) of\ndegree not exceeding p.\n(1)\nProof: Let dk = (1 \u2212 cckp )b1 . Now, for small enough b1 ,\nwe have\np\nX\n\nk=1\n\np\n\nX\nb1\n(1)\n\u03c8k (z1 + b1 + ck (z2 \u2212 )) =\n\u03c8k (z1 + dk + ck z2 )\ncp\nk=1\n\n=h1 (z1 + b1 ) + h2 (z2 \u2212\nby substituting (z1 + b1 ) for z1 and (z2 \u2212\nSubtracting (32) from (33), we obtain\np\u22121\nX\n\nb1\ncp )\n\nb1\n)\ncp\n(33)\n\nfor z2 in (32).\n\n(1)\n\nb1\n\n\u2206\n\na0 ,...,an\n\nand\n\u0002\n\u0003\nf (z) =\n\nn\n\n\u2206\n\na0 ,...,an\u22121\n\n(35)\n\n\u0002\n\u0003\nf (z + an ) \u2212 f (z)\n\nfor any constants ak \u2208 C. Equation (34) is of the same form\nas (32) except the number of the terms in the sum is lower. Let\n(2)\nck\n)b2 . Again by substituting and subtracting,\ndk = (1 \u2212 cp\u22121\nwe obtain from (34) the equation\np\u22122\nX\n\n2\n\n\u2206\n\n(1)\n\n(2)\n\nk=1 dk ,dk\n\n\u0002\n\n2 \u0002\n\u0003\n\u0003\n\u03c8k (z1 +ck z2 ) = \u2206 h1 (z1 ) +\nb1 ,b2\n\n2\n\n\u2206\n\u2212b1\ncp\n\n\u2212b2\np\u22121\n\n,c\n\n\u0002\n\u0003\nh2 (z2 ) .\n(36)\n\nContinuing the process, we end up with the equation\np\u22121\n\n\u2206\n\n(1)\n(p\u22121)\nd1 ,...,d1\n\n\u0002\n\u0003\n\u03c81 (z1 + c1 z2 )\n\np\u22121\n\n=\n\n\u2206\n\nb1 ,...,bp\u22121\n\n\u0002\n\u0003\nh1 (z1 ) +\n\n(37)\n\np\u22121\n\n\u2206\n\u2212b1\ncp\n\n,...,\n\n\u2212bp\u22121\nc2\n\n\u0002\n\n\u0003\nh2 (z2 ) .\n\nThis is the generalized Cauchy's equation\nfor\n\u0002\n\u0003 complex \u2217variables [35] showing that \u2206p\u22121\nfor\n(1)\n(p\u22121) \u03c81 (z) = az + bz\nd1 ,...,d1\nsome constants a, b \u2208 C. Since coefficients bk are arbitrary\nin the neighborhood of zero, and by continuity, the difference\noperator structure [36] shows that \u03c81 (z) is a wide sense polynomial in (z, z \u2217 ) of degree not exceeding p. By renumbering,\nthe same is obtained for \u03c8k (z), k = 1, . . . , p, and thus also\nfor h1 (z) and h2 (z).\nProof of Theorem 4: The joint c.f. of (x1 , x2 )T is given\nas\n\u0001\n\u03c6x1 ,x2 z1 , z2\n\u0002\n\b\n\u0001\u0003\n= Ex1 ,x2 exp \uf6be Re h(z1 , z2 )T , (x1 , x2 )T i\nn\nX\n\u0002\n\b\n\u0001\u0003\n= Ex1 ,x2 exp \uf6be Re h(z1 , z2 )T ,\n(\u03b1k sk , \u03b2k sk )T i\nk=1\n\n\u0002\n= Ex1 ,x2 exp \uf6be\n=\n=\n\nn\nY\n\nk=1\nn\nY\n\nn\nX\n\nk=1\n\n\b\n\u0001\u0003\nRe (\u03b1k z1 + \u03b2k z2 )sk\n\n(38)\n\n\u0002\n\b\n\u0001\u0003\nEsk exp \uf6be Re (\u03b1k z1 + \u03b2k z2 )sk\n\u03c6sk (\u03b1k z1 + \u03b2k z2 ),\n\nk=1\n\nz1 , z2 \u2208 C, by independence of r.v.s sk , k = 1, . . . , n. On the\nother hand, by independence of x1 and x2 , we have\n\u0001\n\u03c6x1 ,x2 z1 , z2 =\u03c6x1 (z1 )\u03c6x2 (z2 )\nn\nn\nY\nY\n(39)\n=\n\u03c6sk (\u03b1k z1 )\n\u03c6sk (\u03b2k z2 ).\nk=1\n\nk=1\n\nThus by combining equations (38) and (39), we get\n\n1 \u0002\n1 \u0002\n1\u0002\n\u0003\n\u0003\n\u0003\n\u2206 \u03c8k (z1 + ck z2 ) = \u2206 h1 (z1 ) + \u2206 h2 (z2 ) , (34)\n\nk=1 dk\n\nn+1\n\n\u2212b1\ncp\n\nn\nY\n\nk=1\n\n\u03c6sk (\u03b1k z1 + \u03b2k z2 ) =\n\nn\nY\n\nk=1\n\n\u03c6sk (\u03b1k z1 )\n\nn\nY\n\nk=1\n\n\u03c6sk (\u03b2k z2 ). (40)\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nAs always, there exists a neighborhood of zero such that all\nc.f.s in Eq. (40) are nonzero. Let rk = \u03b1\u2217k sk and ck = \u03b2k /\u03b1k\nfor \u03b1k 6= 0, and ck = \u03b2k for \u03b1k = 0. Then, by Eq. (10), we\ncan rewrite Eq. (40) for some positive \u03b5 > |z1 |, |z2 | by setting\n\u03c8k = log \u03c6rk as\nl\nX\n\n\u03c8k (z1 + ck z2 ) =\n\nk=1\n\nl\nX\n\n\u03c8k (z1 ) +\n\nk=1\n\nl\nX\n\n\u03c8k (ck z2 ),\n\n(41)\n\nk=1\n\nwhere it is assumed without loss of generality that l first r.v.s\nrk , k = 1, . . . , l, are such that \u03b1k \u03b2k 6= 0, and therefore\ncomponents \u03c8k , k > l, cancel out. By combining functions\n\u03c8k with the equal arguments to a single function \u03c8\u0303 and\nrenumbering, Eq. (41) may be rewritten as\nq\nX\n\n\u03c8\u0303k (z1 + ck z2 ) =\n\nk=1\n\nl\nX\n\n\u03c8k (z1 ) +\n\nk=1\n\nq\nX\n\n\u03c8\u0303k (ck z2 )\n\n(42)\n\nk=1\n\nsuch\nPl that numbers ck , k = 1, . . . , q \u2264 l, are distinct. Therefore,\nsense polynomial by Lemma 5. By\nk=1 \u03c8k (z1 ) is a wide\nPl\nTheorem 9, the r.v.\nk=1 rk is complex normal. Thus by\nTheorem 10 each r.v. rk , and hence each r.v. sk , k = 1, . . . , l,\nis complex normal.\n\nA DDITIONAL\n\nA PPENDIX III\nCHARACTERIZATION LEMMAS\n\nLemma 6: Let \u03b11 , . . . , \u03b1m be given nonzero vectors of an\ninner product space. Then there exist a vector \u03b2, which is not\northogonal to any of the given vectors.\nProof: Suppose \u03b2 is not orthogonal to any \u03b1l , l =\n1, . . . , k \u2212 1, but is orthogonal to \u03b1k . Then a scalar c \u2208 C can\nbe chosen such that h\u03b2, \u03b1l i =\n6 \u2212ch\u03b1k , \u03b1l i for all l \u2264 k. Now\nthe vector \u03b2\u0302 = \u03b2 + c\u03b1k is not orthogonal to any \u03b1l , l \u2264 k.\nSince \u03b11 is nonzero, \u03b2 1 = \u03b11 is not orthogonal to \u03b11 .\nChoose \u03b2 2 = \u03b2 1 + c2 \u03b12 , where c2 is a scalar as above if \u03b2 1\nis orthogonal to \u03b12 , and c2 = 0 otherwise. By iterating the\nprocedure m \u2212 1 times, it is seen that \u03b2m is a required type\nof vector.\nLemma 7: Let \u03b11 , . . . , \u03b1m be given p-dimensional nonzero\ncomplex vectors such that \u03b11 is not collinear with any \u03b1k ,\nk 6= 1. Then there exists a 2 \u00d7 p matrix C such that C\u03b11 is\nnot collinear with any C\u03b1k , k 6= 1.\nProof: Denote \u03b1k = (\u03b1k1 , . . . , \u03b1kp )T , k = 1, . . . , m.\nWithout loss of generality we assume that the coefficients \u03b1k1 ,\nk = 1, . . . , m, are either zero or one. Furthermore, we may\ntake \u03b111 = 1 by permutating the original indices.\nSuppose \u03b11 is not collinear with \u03b1k , i.e., \u03b11 6= \u03b1k , for\nany k 6= 1. Define\n\u0013\n\u0012\n1\n0 *** 0\n,\n(43)\nC=\n\u03b21 \u03b22 * * * \u03b2p\nwhere \u03b2 = (\u03b21 , . . . , \u03b2p )T is a vector such that\nh\u03b2, (\u03b11 \u2212 \u03b1k )\u2217 i 6= 0,\n\nk = 2 . . . , m.\n\n11\n\nC\u03b11 can be collinear with another vector C\u03b1k only if \u03b1k1 =\n1. But then the difference\n\u0012\n\u0013 \u0012\n\u0013 \u0012\n\u0013\n1\n1\n0\nC\u03b11 \u2212 C\u03b1k =\n\u2212\n=\nh\u03b2, (\u03b11 \u2212 \u03b1k )\u2217 i\n\u03b2 T \u03b11\n\u03b2 T \u03b1k\n(45)\nis not zero by construction. Thus C\u03b11 is not collinear with\nany C\u03b1k , k 6= 1, and C is a required type of matrix.\nLemma 8: Let (A,~s) and (B,~r) be two reduced representations of a 2-dimensional complex r.vc. ~x, where A and B\nare constant complex matrices of dimensions 2 \u00d7 m and 2 \u00d7 n\nrespectively, and ~s = (s1 , . . . , sm )T and ~r = (r1 , . . . , rn )T\nare complex r.vc.s with independent components. Then the\nfollowing properties hold.\n(i) If the kth column of A is not collinear with any column\nof B, then the r.v. sk is complex normal.\n(ii) If the kth column of A is collinear with the lth column\nof B, then the logarithms of the c.f.s of sk and rl differ\nby a wide sense polynomial in a neighborhood of the\norigin.\nProof:\n(i) Without loss of generality we assume that matrices A\nand B are scaled such that the first rows consist only of\nzeros and ones. This amounts only to the scale of r.v.s\nsl and r.v.s rl . Furthermore, since the components of ~x\ncan be interchanged if necessary, the first entry of the\nkth column of A can be taken to be one.\nAs always, there exists a neighborhood \u03b5 > 0 of zero\nsuch that all c.f.s are nonzero, and the logarithms of\nc.f.s are well-defined. Therefore for z = (z1 , z2 )T \u2208 C2 ,\n|z1 | < \u03b5, |z2 | < \u03b5, we have using the properties (10) and\n(9) that\nlog \u03c6~x (z) = log \u03c6~s (AH z) = log \u03c6~r (B H z)\nm\nX\nlog \u03c6sl (\u03b1\u22171l z1 + \u03b1\u22172l z2 )\n=\n=\n\nl=1\nn\nX\n\n\u2217\n\u2217\nlog \u03c6rl (\u03b21l\nz1 + \u03b22l\nz2 ),\n\n(46)\n(47)\n\nl=1\n\nwhere A = (\u03b1ql ), B = (\u03b2ql ). Let q be the number of\ndifferent noncollinear columns with nonzero coefficients\nin A and B other than the kth column of A. Now\nsubstituting (47) from (46), and combining the terms with\nequal nonzero coefficient arguments to functions hl , and\nwith one zero coefficient to f and g, respectively, we get\nan equation of the form\nlog \u03c6sk (z1 + \u03b1\u22172k z2 ) +\n\nq\nX\n\nhl (z1 + \u03b3l z2 ) = f (z1 ) + g(z2 )\n\nl=1\n\n(48)\n\nif \u03b12k 6= 0, and of the form\nq\nX\n\nhl (z1 + \u03b3l z2 ) = log \u03c6sk (z1 ) + g(z2 )\n\n(49)\n\nl=1\n\n(44)\n\nBy Lemma 6 such a vector \u03b2 exists. Now vectors C\u03b1k are\nagain such that the first component is either zero or one. Thus\n\nif \u03b12k = 0. Numbers \u03b12k , \u03b31 , . . . , \u03b3q are now distinct,\nand then by Lemma 5, log \u03c6sk must be a wide sense\npolynomial in (z, z \u2217 ) of degree not exceeding q. Thus\nby Theorem 9, the r.v. sk is complex normal.\n\n\f12\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\n(ii) By definitions of representations, kth column of A is\ncollinear only with the lth column of B. Thus one of the\nh's in the proof of part (i) is the difference the logarithms\nof the c.f.s of sk and rl , and the claim follows from\nLemma 5.\nLemma 9: Suppose independent complex r.v.s s1 and s2\nare independent of complex normal r.v.s n1 and n2 . If s1 + n1\nis independent of s2 +n2 , then also n1 and n2 are independent.\nProof: Since the r.vc. (s1 , s2 )T is independent of the\nr.vc. (n1 , n2 )T , the joint c.f. can be written as\n\u0001\n\u03c6s1 +n1 ,s2 +n2 z1 , z2\n\u0001\n\u0001\n(50)\n=\u03c6s1 ,s2 z1 , z2 \u03c6n1 ,n2 z1 , z2\n\u0001\n=\u03c6s1 (z1 )\u03c6s2 (z2 )\u03c6n1 ,n2 z1 , z2 .\n\nOn the other hand, using the independence of s1 + n1 and\ns2 + n2 , we have\n\u0001\n\u03c6s1 +n1 ,s2 +n2 z1 , z2 =\u03c6s1 +n1 (z1 )\u03c6s2 +n2 (z2 )\n(51)\n=\u03c6s1 (z1 )\u03c6n1 (z1 )\u03c6s2 (z2 )\u03c6n2 (z2 ),\nand therefore\n\u0001\n\u03c6s1 (z1 )\u03c6s2 (z2 )\u03c6n1 ,n2 z1 , z2\n=\u03c6s1 (z1 )\u03c6s2 (z2 )\u03c6n1 (z1 )\u03c6n2 (z2 ).\n\n(52)\n\nThen, in some neighborhood of zero, all c.f.s in (52) are\nnonzero, and we have\n\u0001\n\u03c6n1 ,n2 z1 , z2 = \u03c6n1 (z1 )\u03c6n2 (z2 )\n(53)\n\nin the neighborhood. By the \u03b1-decomposition theorem [34,\nTheorem 7.4.2], the equation if valid for all z1 and z2 , i.e., n1\nand n2 are independent.\nLemma 10: If complex r.v.s n and s are independent and\nn+s is independent of n, then n is degenerate (i.e., a constant).\nProof: By Theorem 4 the r.v. n is complex normal. As\nin the proof of Lemma 9, it follows that the equation\n\u03c6n (z1 + z2 ) = \u03c6n (z1 )\u03c6n (z2 )\n\n(54)\n\nis satisfied in a neighborhood of zero. This is only possible if\nn is a degenerate complex normal r.v., i.e., a complex normal\nr.v. with zero variance.\nR EFERENCES\n[1] P. Comon, \"Independent component analysis, a new concept?\" Signal\nProcessing, vol. 36, no. 3, pp. 287\u2013314, Apr. 1994.\n[2] A. Hyv\u00e4rinen, J. Karhunen, and E. Oja, Independent Component Analysis. John Wiley & Sons, 2001.\n[3] A. Cichocki and S. Amari, Adaptive Blind Signal and Image Processing:\nLearning Algorithms and Applications. John Wiley & Sons, 2002.\n[4] A. Kagan, Y. Linnik, and C. Rao, Characterization Problems in Mathematical Statistics, ser. Probability and Mathematical Statistics. New\nYork, NY: John Wiley & Sons, 1973.\n[5] J. Eriksson and V. Koivunen, \"Identifiability, separability and uniqueness\nof linear ICA models,\" IEEE Signal Processing Lett., vol. 11, no. 7, pp.\n601\u2013604, July 2004.\n[6] J.-F. Cardoso, \"An efficient technique for the blind separation of complex\nsources,\" in Proc. HOS'93, South Lake Tahoe, CA, June 1993, pp. 275\u2013\n279.\n[7] E. Bingham and A. Hyv\u00e4rinen, \"A fast fixed-point algorithm for independent component analysis of complex valued signals,\" Int. J. Neural\nSystems, vol. 10, no. 1, pp. 1\u20138, Feb. 2000.\n\n[8] V. Calhoun and T. Adali, \"Complex infomax: convergence and approximation of infomax with complex nonlinearities,\" in Proc. NNSP 2002,\nMartigny, Switzerland, Sept. 2002, pp. 307\u2013316.\n[9] S. Fiori, \"Extended hebbian learning for blind separation of complexvalued sources,\" IEEE Trans. Circuits Syst. II, vol. 50, no. 4, pp. 195\u2013\n202, Apr. 2003.\n[10] J. Anem\u00fcller, T. Sejnowski, and S. Makeig, \"Complex independent\ncomponent analysis of frequency-domain electroencephalographic data,\"\nNeural Networks, vol. 16, no. 9, pp. 1311\u20131323, Nov. 2003.\n[11] F. Neeser and J. Massey, \"Proper complex random processes with\napplications to information theory,\" IEEE Trans. Inform. Theory, vol. 39,\nno. 4, pp. 1293\u20131302, July 1993.\n[12] B. Picinbono, \"Second-order complex random vectors and normal distributions,\" IEEE Trans. Signal Processing, vol. 44, no. 10, pp. 2637\u20132640,\nOct. 1996.\n[13] P. Schreier and L. Scharf, \"Second-order analysis of improper complex\nrandom vectors and processes,\" IEEE Trans. Signal Processing, vol. 51,\nno. 3, pp. 714\u2013725, Mar. 2003.\n[14] N. Vakhania and N. Kandelaki, \"Random vectors with values in complex\nHilbert spaces,\" Theory Probab. Appl., vol. 41, no. 1, pp. 116\u2013131, Feb.\n1996.\n[15] B. Picinbono and P. Bondon, \"Second-order statistics of complex signals,\" IEEE Trans. Signal Processing, vol. 45, no. 2, pp. 411\u2013420, Feb.\n1997.\n[16] O. Grellier, P. Comon, B. Mourrain, and P. Trebuchet, \"Analytical blind\nchannel identification,\" IEEE Trans. Signal Processing, vol. 50, no. 9,\npp. 2196\u20132207, Sept. 2002.\n[17] N. Vakhania, V. Tarieladze, and S. Chobanyan, Probability Distributions\non Banach Spaces. Dordrecht, Netherlands: Reidel, 1987.\n[18] I. Ruzsa and G. Sz\u00e9kely, Algebraic Propbability Theory. John Wiley\n& Sons, 1988.\n[19] R. Dudley, Real analysis and probability. Chapman & Hall, 1989.\n[20] G. Feldman, Arithmetic of Probability Distributions, and Characterization Problems on Abelian Groups, ser. Translations of mathematical\nmonographs. Providence, RI: AMS, 1993, vol. 116.\n[21] R. Wooding, \"The multivariate distribution of complex normal variables,\" Biometrika, vol. 43, no. 1/2, pp. 212\u2013215, June 1956.\n[22] N. Goodman, \"Statistical analysis based on certain multivariate complex\nGaussian distribution (An introduction),\" Ann. Math. Stat., vol. 34, no. 1,\npp. 152\u2013177, Mar. 1963.\n[23] P. Amblard, M. Gaeta, and J. Lacoume, \"Statistics for complex variables\nand signals \u2013 Part I: Variables,\" Signal Processing, vol. 53, pp. 1\u201313,\n1996.\n[24] P. Krishnaiah and J. Lin, \"Complex elliptically symmetric distributions,\"\nCommun. Statist. A, vol. 15, no. 12, pp. 3693\u20133718, 1986.\n[25] W. Hudson and J. Veeh, \"Complex stable sums of complex stable\nrandom variables,\" J. Mult. Anal., vol. 77, pp. 229\u2013238, 2001.\n[26] N. Vakhania, \"Polya's characterization theorem for complex random\nvariables,\" J. Complexity, vol. 13, pp. 480\u2013488, 1997.\n[27] R. Horn and C. Johnson, Matrix Analysis. New York, NY: Cambridge\nUniversity Press, 1985.\n[28] B. Picinbono and P. Chevalier, \"Widely linear estimation with complex\ndata,\" IEEE Trans. Signal Processing, vol. 43, no. 8, pp. 2030\u20132033,\nAug. 1995.\n[29] R. Cuppens, Decomposition of Multivariate Probabilities, ser. Probability and Mathematical Statistics. Academic Press, 1975, vol. 29.\n[30] T. Cover and J. Thomas, Elements of Information Theory. John Wiley\n& Sons, 1991.\n[31] G. Taub\u00f6ck, \"Rotationally variant complex channels,\" in Proc. 23rd\nSymp. on Inform. Theory in the Benelux, Louvain-la-Neuve, Belgium,\nMay 2002.\n[32] S. Ghurye and I. Olkin, \"A characterization of the multivariate normal\ndistribution,\" Ann. Math. Stat., vol. 33, no. 2, pp. 533\u2013541, June 1962.\n[33] A. Mathai and G. Pederzoli, Characterizations of the Normal Probability\nLaw. New Delhi, India: Wiley Eastern Limited, 1977.\n[34] Y. Linnik and I. Ostrovski\u01d0, Decomposition of Random Variables and\nVectors, ser. Translation of Mathematical Monographs. AMS, 1977,\nvol. 48.\n[35] J. Acz\u00e9l and J. Dhombres, Functional Equations in Several Variables,\nser. Encyclopedia of Mathematics and its Applications. Cambridge,\nGreat Britain: Cambridge University Press, 1989, vol. 31.\n[36] J. Acz\u00e9l, Lectures on Functional Equations and their Applications, ser.\nMathematics in Science and Engineering. New York, NY: Academic\nPress, 1966, vol. 19.\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nJan Eriksson (M'04) received the M.Sc. degree\nin mathematics from University of Turku, Finland,\nin 2000, and the D.Sc.(Tech) degree (with honors)\nin signal processing from Helsinki University of\nTechnology (HUT), Finland, in 2004. He is currently\nworking as a postdoctoral researcher of Academy of\nFinland.\nHis research interest are in blind signal processing, stochastic modeling, digital communication, and\ninformation theory.\n\nVisa Koivunen (Senior Member, IEEE) received his\nD.Sc. (Tech) degree with honors from the University\nof Oulu, Dept. of Electrical Engineering. From 1992\nto 1995 he was a visiting researcher at the University\nof Pennsylvania, Philadelphia, USA. Year 1996 he\nheld a faculty position at the Department of Electrical Engineering, University of Oulu, Finland. From\n1997 to 1999 he was an Associate Professor at the\nSignal Processing Labroratory, Tampere University\nof Technology. Since 1999 he has been a Professor\nof Signal Processing at the Department of Electrical\nand Communications Engineering, Helsinki University of Technology (HUT),\nFinland. He is one of the Principal Investigators in SMARAD Center of\nExcellence in Radio and Communications Engineering nominated by the\nAcademy of Finland. Since year 2003 he has been also adjunct professor\nat the University of Pennsylvania, Philadelphia, USA.\nDr. Koivunen's research interest include statistical, communications and\nsensor array signal processing. He received the best paper award (co-authored\nby C. Ribeiro and A. Richter) from IEEE PIMRC 2005 for his work on MIMO\nchannel propagation parameter estimation. He has published more than 170\npapers in international scientific conferences and journals. He has served as\nan associate editor for IEEE Signal Processing Letters. He is a member of the\neditorial board for the Signal Processing journal. He is also a member of the\nIEEE Signal Processing for Communication Technical Committee (SPCOMTC).\n\n13\n\n\f"}